{"hands_on_practices": [{"introduction": "从零开始构建一个精确的伽马分布生成器是理解高级蒙特卡洛方法的基础。这项实践要求你亲手实现一个完整的伽马随机变量生成器，覆盖不同的参数范围 [@problem_id:3056410]。通过这个过程，你不仅能掌握 Marsaglia-Tsang 算法等高效采样技术的精髓，还能加深对分布理论及其在算法验证中应用的理解。", "problem": "实现一个针对伽马分布的统计精确的随机变量生成器，该生成器使用接受-拒绝采样和增广法，并通过一组基于基本分布关系的测试来验证其正确性。具体来说，构建一个程序，用于模拟来自形状参数为$k$、尺度参数为$\\theta$的伽马分布（记作$\\text{Gamma}(k,\\theta)$）的独立同分布样本，且仅依赖于以下基础：(i) 伽马分布的定义，即一个在$(0,\\infty)$上具有概率密度函数的分布族，(ii) 用于验证样本矩的大数定律，(iii) 带有有效提议分布的接受-拒绝法，(iv) 贝塔分布的定义，即两个具有相同尺度的独立伽马随机变量之比的分布，以及(v) 卡方分布的定义，即作为伽马分布的一个特例。不要使用任何预构建的伽马、贝塔或卡方采样器。\n\n您的实现必须满足以下规范。\n- 对于$k > 1$，使用一个基于标准正态提议分布的变换和一个“挤压”接受区域的接受-拒绝采样器。该方法必须完全从第一性原理出发，使用独立的均匀和标准正态变量进行编码。\n- 对于$0  k  1$，使用一种增广方法。", "solution": "用户提供的问题经评估有效，因为它具有科学依据、问题明确、客观且内部一致。它提出了一个计算统计学中标准但并非微不足道的问题：从第一性原理出发实现伽马分布的随机变量生成器，并验证其性质。问题陈述为唯一且可验证的解法提供了所有必要的定义、算法和测试条件。\n\n该解决方案的实现首先是为标准伽马分布$\\text{Gamma}(k,1)$构建一个生成器，然后利用尺度性质将其扩展到一般的$\\text{Gamma}(k,\\theta)$分布：如果$X \\sim \\text{Gamma}(k,1)$，则$\\theta X \\sim \\text{Gamma}(k,\\theta)$。标准伽马变量的生成由三种不同的方法处理，根据形状参数$k$的值进行选择。\n\n所需的标准正态变量$Z \\sim N(0,1)$是使用Box-Muller变换从成对的独立均匀变量$U_1, U_2 \\sim \\operatorname{Uniform}(0,1)$合成的：$Z_1 = \\sqrt{-2 \\ln(U_1)} \\cos(2\\pi U_2)$ 和 $Z_2 = \\sqrt{-2 \\ln(U_1)} \\sin(2\\pi U_2)$。此变换为每对均匀样本生成两个独立的$N(0,1)$样本。\n\n形状参数$k$的三种情况如下：\n\n1.  **对于$k > 1$**：使用Marsaglia和Tsang (2000)的接受-拒绝法。这是一种高效的算法。\n    设目标分布为标准伽马分布$\\text{Gamma}(k,1)$。该方法使用标准正态变量$Z \\sim N(0,1)$的巧妙变换来提出候选值$X$。算法如下：\n    a. 预计算常数$d = k - 1/3$和$c = 1/\\sqrt{9d}$。\n    b. 生成一个标准正态变量$Z$和一个均匀变量$U \\sim \\operatorname{Uniform}(0,1)$。\n    c. 计算$V = (1+cZ)^3$。如果$V \\le 0$，则拒绝并返回步骤(b)。\n    d. 提议的伽马分布值为$X = dV$。\n    e. 如果满足以下条件，则接受$X$：$\\ln(U)  \\frac{Z^2}{2} + d - X + d \\ln(V)$。否则，拒绝$X$并返回步骤(b)。\n    重复此过程，直到生成所需数量的样本。\n\n2.  **对于$0  k  1$**：采用Ahrens和Dieter (1974)描述的增广法。该方法利用了形状参数大于1的生成器。它基于这样一个性质：如果$Y \\sim \\text{Gamma}(k+1,1)$和$U \\sim \\operatorname{Uniform}(0,1)$是独立的，则随机变量$X = Y \\cdot U^{1/k}$服从$\\text{Gamma}(k,1)$分布。\n    由于$0  k  1$，所需变量$Y$的形状参数$k+1$将在$(1,2)$范围内。因此，我们可以使用上面描述的针对$k > 1$的接受-拒绝法来生成$Y$。这是一种直接方法（无拒绝），因此一个$Y$和一个$U$产生一个$X$。\n\n3.  **对于$k = 1$**：这是一个边界情况。$\\text{Gamma}(1,1)$分布是标准指数分布$\\operatorname{Exp}(1)$。可以使用逆变换法高效、精确地对其进行采样。如果$U \\sim \\operatorname{Uniform}(0,1)$，则$X = -\\ln(U)$服从$\\operatorname{Exp}(1)$分布。\n\n最终的实现将这些方法整合到一个生成器函数中。然后使用此生成器执行五个指定的测试。对于每个测试，都会生成大量样本（$n=200000$），并将其样本矩（均值和方差）与从伽马、贝塔和卡方分布的性质推导出的理论值进行比较。这些比较的结果是布尔值，它们被收集起来并以指定的格式打印。为随机数生成器使用固定种子可确保结果是可复现的。", "answer": "```python\nimport numpy as np\n\ndef _my_normal_sampler(size: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Generates standard normal variates using the Box-Muller transform.\"\"\"\n    if size == 0:\n        return np.array([])\n    \n    num_pairs = (size + 1) // 2\n    u1 = rng.uniform(size=num_pairs)\n    # Avoid u1=0 for log\n    u1[u1 == 0] = 1e-100 \n    u2 = rng.uniform(size=num_pairs)\n    \n    r = np.sqrt(-2.0 * np.log(u1))\n    theta = 2.0 * np.pi * u2\n    \n    z1 = r * np.cos(theta)\n    z2 = r * np.sin(theta)\n    \n    all_z = np.empty(2 * num_pairs)\n    all_z[0::2] = z1\n    all_z[1::2] = z2\n    \n    return all_z[:size]\n\ndef _gamma_sampler_gt1(k: float, n: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generates n samples from a standard Gamma(k, 1) distribution for k  1.\n    Uses the Marsaglia and Tsang (2000) acceptance-rejection method.\n    \"\"\"\n    d = k - 1.0/3.0\n    c = 1.0 / np.sqrt(9.0 * d)\n    \n    samples = np.empty(n)\n    count = 0\n    \n    while count  n:\n        needed = n - count\n        \n        # Generate candidate Zs and Us\n        z = _my_normal_sampler(needed, rng)\n        u = rng.uniform(size=needed)\n        \n        v = (1.0 + c * z)**3\n        \n        # Filter for valid V  0\n        mask_valid_v = v > 0\n        z_valid = z[mask_valid_v]\n        u_valid = u[mask_valid_v]\n        v_valid = v[mask_valid_v]\n        \n        if len(v_valid) == 0:\n            continue\n            \n        x_cand = d * v_valid\n        \n        # Squeeze test and main acceptance condition\n        log_u_valid = np.log(u_valid)\n        accept_mask = log_u_valid  (0.5 * z_valid**2 + d - x_cand + d * np.log(v_valid))\n        \n        accepted_samples = x_cand[accept_mask]\n        num_accepted = len(accepted_samples)\n        \n        if num_accepted > 0:\n            fill_count = min(num_accepted, needed)\n            samples[count:count + fill_count] = accepted_samples[:fill_count]\n            count += fill_count\n            \n    return samples\n\ndef _gamma_sampler_eq1(n: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generates n samples from a standard Gamma(1, 1) distribution (Exponential).\n    Uses the inverse transform method.\n    \"\"\"\n    u = rng.uniform(size=n)\n    # Avoid u=0 for log\n    u[u == 0] = 1e-100\n    return -np.log(u)\n\ndef _gamma_sampler_lt1(k: float, n: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generates n samples from a standard Gamma(k, 1) distribution for 0  k  1.\n    Uses augmentation method: Y * U^(1/k) where Y ~ Gamma(k+1, 1).\n    \"\"\"\n    # Generate Y ~ Gamma(k+1, 1)\n    # Since k+1 is in (1, 2), we use the k>1 sampler.\n    y_samples = _gamma_sampler_gt1(k + 1.0, n, rng)\n    \n    # Generate U ~ Uniform(0, 1)\n    u_samples = rng.uniform(size=n)\n    \n    return y_samples * (u_samples ** (1.0/k))\n\ndef gamma_sampler(k: float, theta: float, n: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generates n samples from a Gamma(k, theta) distribution.\n    \"\"\"\n    if k = 0 or theta = 0 or not isinstance(n, int) or n  0:\n        raise ValueError(\"k and theta must be positive, n must be a non-negative integer.\")\n\n    if k > 1.0:\n        std_samples = _gamma_sampler_gt1(k, n, rng)\n    elif k == 1.0:\n        std_samples = _gamma_sampler_eq1(n, rng)\n    else:  # 0  k  1\n        std_samples = _gamma_sampler_lt1(k, n, rng)\n        \n    return std_samples * theta\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    seed = 123456789\n    n = 200000\n    rng = np.random.default_rng(seed)\n    \n    results = []\n\n    # Test 1: Gamma, shape greater than one\n    k, theta = 3.5, 2.0\n    samples_1 = gamma_sampler(k, theta, n, rng)\n    m_star_1, v_star_1 = k * theta, k * theta**2\n    m_hat_1, v_hat_1 = np.mean(samples_1), np.var(samples_1, ddof=1)\n    results.append(np.abs(m_hat_1 - m_star_1)  0.03)\n    results.append(np.abs(v_hat_1 - v_star_1)  0.30)\n    \n    # Test 2: Gamma, shape less than one (augmentation)\n    k, theta = 0.7, 1.5\n    samples_2 = gamma_sampler(k, theta, n, rng)\n    m_star_2, v_star_2 = k * theta, k * theta**2\n    m_hat_2, v_hat_2 = np.mean(samples_2), np.var(samples_2, ddof=1)\n    results.append(np.abs(m_hat_2 - m_star_2)  0.03)\n    results.append(np.abs(v_hat_2 - v_star_2)  0.10)\n\n    # Test 3: Beta via ratio of independent gammas\n    a, b = 2.1, 3.3\n    x_samples_3 = gamma_sampler(a, 1.0, n, rng)\n    y_samples_3 = gamma_sampler(b, 1.0, n, rng)\n    u_samples_3 = x_samples_3 / (x_samples_3 + y_samples_3)\n    u_mean_star_3 = a / (a + b)\n    u_mean_hat_3 = np.mean(u_samples_3)\n    results.append(np.abs(u_mean_hat_3 - u_mean_star_3)  0.01)\n\n    # Test 4: Chi-squared as a gamma\n    nu = 10\n    k, theta = nu / 2.0, 2.0\n    samples_4 = gamma_sampler(k, theta, n, rng)\n    m_star_4, v_star_4 = nu, 2.0 * nu\n    m_hat_4, v_hat_4 = np.mean(samples_4), np.var(samples_4, ddof=1)\n    results.append(np.abs(m_hat_4 - m_star_4)  0.03)\n    results.append(np.abs(v_hat_4 - v_star_4)  0.20)\n\n    # Test 5: Boundary case k=1 (exponential)\n    k, theta = 1.0, 0.8\n    samples_5 = gamma_sampler(k, theta, n, rng)\n    m_star_5, v_star_5 = k * theta, k * theta**2\n    m_hat_5, v_hat_5 = np.mean(samples_5), np.var(samples_5, ddof=1)\n    results.append(np.abs(m_hat_5 - m_star_5)  0.02)\n    results.append(np.abs(v_hat_5 - v_star_5)  0.05)\n\n    print(f\"[{','.join(map(str, results))}]\".lower())\n\nsolve()\n```", "id": "3056410"}, {"introduction": "在掌握了生成器的基本实现后，下一个关键步骤是学习如何优化其性能。这项练习将引导你为接受-拒绝采样设计一个最优的提议分布 [@problem_id:3309202]。你将通过分析指数倾斜（exponential tilting）这一重要技巧，并运用凸对偶等理论工具，来最小化接受指示变量的方差，从而最大化采样效率。", "problem": "考虑一个目标伽马分布，其形状参数为 $k \\in (0,1)$，率参数为 $r>0$，其概率密度函数为\n$$\nf(x) \\;=\\; \\frac{r^{k}}{\\Gamma(k)} \\, x^{k-1} \\exp(-r x), \\quad x>0.\n$$\n您的任务是设计一个接受-拒绝方案，使用一个在原点附近保留幂律行为的指数倾斜提议分布，从 $f$ 中生成随机变量。具体来说，考虑提议分布族\n$$\nq_{t}(x) \\;=\\; \\frac{t^{k}}{\\Gamma(k)} \\, x^{k-1} \\exp(-t x), \\quad x>0,\n$$\n其倾斜参数为 $t>0$，因此 $q_{t}$ 是一个具有相同形状参数 $k$ 和率参数 $t$ 的伽马分布。对于接受-拒绝法，您必须选择一个有限常数 $M(t)$，使得对于所有 $x>0$ 都有 $f(x) \\leq M(t)\\, q_{t}(x)$，并且您以概率 $f(X)/(M(t) q_{t}(X))$ 接受一次抽样 $X \\sim q_{t}$。令 $I_{t}$ 表示接受指示变量，它是一个伯努利随机变量，其成功概率为 $p(t) = 1/M(t)$。\n\n从接受-拒绝法和伽马分布族的基本定义出发，且不使用任何快捷公式，执行以下操作：\n\n- 推导 $M(t)$ 作为 $t$ 的函数的显式表达式，并刻画使 $M(t)$ 为有限的 $t$ 的取值范围。\n\n- 用 $t$、$r$ 和 $k$ 表示 $p(t)$ 和 $\\operatorname{Var}(I_{t})$。\n\n- 通过分析对数似然比的上确界并利用矩匹配来解释解，使用凸对偶来确定最优倾斜参数 $t^{\\ast}$，该参数在所有容许的 $t$ 上最小化 $\\operatorname{Var}(I_{t})$。\n\n您的最终答案必须是用 $r$ 表示的最优倾斜参数 $t^{\\ast}$。不需要数值近似；请提供精确的符号表达式。如果您找到多个候选解，请选择在您推导的容许性约束下实现 $\\operatorname{Var}(I_{t})$ 全局最小值的那个解。最终答案应为单个闭式表达式。", "solution": "用户要求我解决并验证一个关于伽马分布的接受-拒绝方案的问题。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n\n*   **目标分布：** 形状参数为 $k \\in (0,1)$、率参数为 $r>0$ 的伽马分布。\n*   **目标概率密度函数（PDF）：** 对于 $x>0$，$f(x) = \\frac{r^{k}}{\\Gamma(k)} x^{k-1} \\exp(-r x)$。\n*   **提议分布族：** 形状参数为 $k$、率参数为 $t>0$ 的伽马分布。\n*   **提议概率密度函数（PDF）：** 对于 $x>0$，$q_{t}(x) = \\frac{t^{k}}{\\Gamma(k)} x^{k-1} \\exp(-t x)$。\n*   **接受-拒绝条件：** 对于所有 $x>0$，$f(x) \\leq M(t) q_{t}(x)$。\n*   **接受指示变量：** $I_t$，一个伯努利随机变量。\n*   **成功概率：** $p(t) = 1/M(t)$。\n\n**任务：**\n1.  推导 $M(t)$ 的表达式，并找到使其为有限的 $t$ 的值。\n2.  用 $t$、$r$ 和 $k$ 表示 $p(t)$ 和 $\\operatorname{Var}(I_t)$。\n3.  通过使用凸对偶和矩匹配解释，确定最小化 $\\operatorname{Var}(I_t)$ 的最优倾斜参数 $t^{\\ast}$。\n\n**步骤2：使用提取的已知条件进行验证**\n\n该问题具有科学依据，提法恰当，且是客观的。这是一个计算统计学和蒙特卡洛方法中的标准问题，具体涉及接受-拒绝抽样中提议分布的优化。\n\n1.  **科学或事实上的不健全：** 无。该问题基于概率论（伽马分布、伯努利试验）和模拟（接受-拒绝法）的既定原理。\n2.  **非形式化或不相关：** 无。该问题以精确的数学术语陈述，与指定主题直接相关。\n3.  **不完整或矛盾的设置：** 无。所有必要的参数和定义（$k, r, t, f(x), q_t(x)$）均已提供。\n4.  **不切实际或不可行：** 无。该问题纯粹是数学问题。\n5.  **提法不当或结构不良：** 无。目标函数 $\\operatorname{Var}(I_t)$ 定义明确，且优化是针对指定参数 $t$ 进行的。结构逻辑清晰。\n6.  **伪深刻、琐碎或同义反复：** 无。尽管最终答案可能看起来很简单，但所要求的涉及凸对偶和矩匹配的论证过程需要对抽样算法的基本理论有深刻的理解。该问题并非琐碎。\n7.  **超出科学可验证范围：** 无。该推导过程可以通过严格的数学证明来验证。\n\n**步骤3：结论与行动**\n\n问题有效。将提供完整解答。\n\n### 解答\n\n接受-拒绝算法需要一个常数 $M(t)$，使得对于所有 $x > 0$，都有 $f(x) \\leq M(t) q_t(x)$。$M(t)$ 的最有效选择是满足该条件的最小常数，它由密度函数的比值的上确界给出。\n$$\nM(t) = \\sup_{x>0} \\frac{f(x)}{q_t(x)}\n$$\n我们来计算这个比值：\n$$\n\\frac{f(x)}{q_t(x)} = \\frac{\\frac{r^{k}}{\\Gamma(k)} x^{k-1} \\exp(-r x)}{\\frac{t^{k}}{\\Gamma(k)} x^{k-1} \\exp(-t x)} = \\left(\\frac{r}{t}\\right)^{k} \\exp(-r x + t x) = \\left(\\frac{r}{t}\\right)^{k} \\exp((t-r)x)\n$$\n为了找到该表达式在 $x>0$ 上的上确界，我们分析指数项 $\\exp((t-r)x)$ 的行为，这取决于系数 $t-r$ 的符号。\n\n1.  如果 $t > r$，则 $t-r > 0$。函数 $\\exp((t-r)x)$ 在 $x$ 上是严格递增的。当 $x \\to \\infty$ 时，该函数无界增长。因此，$M(t) = \\infty$。\n2.  如果 $t = r$，则 $t-r = 0$。比值对于所有 $x>0$ 恒为 $(r/r)^k \\exp(0) = 1$。因此，$M(r) = 1$。\n3.  如果 $0  t  r$，则 $t-r  0$。函数 $\\exp((t-r)x)$ 在 $x$ 上是严格递减的。上确界在 $x \\to 0^{+}$ 时取得。\n    $$\n    \\sup_{x>0} \\left(\\frac{r}{t}\\right)^{k} \\exp((t-r)x) = \\lim_{x \\to 0^{+}} \\left(\\frac{r}{t}\\right)^{k} \\exp((t-r)x) = \\left(\\frac{r}{t}\\right)^{k}\n    $$\n\n综合这些情况，我们发现有限常数 $M(t)$ 仅在 $t \\in (0, r]$ 时存在。对于这个容许的 $t$ 的范围，最优常数为：\n$$\nM(t) = \\left(\\frac{r}{t}\\right)^{k}, \\quad t \\in (0, r]\n$$\n\n接受指示变量 $I_t$ 是一个伯努利随机变量。接受一个提议值 $X \\sim q_t(x)$ 的概率是 $f(X)/(M(t)q_t(X))$。总接受概率 $p(t)$ 是这个量的期望。\n$$\np(t) = \\mathbb{E}_{X \\sim q_t}\\left[ \\frac{f(X)}{M(t) q_t(X)} \\right] = \\frac{1}{M(t)} \\int_0^{\\infty} \\frac{f(x)}{q_t(x)} q_t(x) \\,dx = \\frac{1}{M(t)} \\int_0^{\\infty} f(x) \\,dx = \\frac{1}{M(t)}\n$$\n因为 $f(x)$ 是一个概率密度函数。对于 $t \\in (0, r]$，这给出：\n$$\np(t) = \\frac{1}{(r/t)^k} = \\left(\\frac{t}{r}\\right)^{k}\n$$\n伯努利指示变量 $I_t$ 的方差是 $\\operatorname{Var}(I_t) = p(t)(1-p(t))$。\n$$\n\\operatorname{Var}(I_t) = \\left(\\frac{t}{r}\\right)^{k} \\left(1 - \\left(\\frac{t}{r}\\right)^{k}\\right)\n$$\n我们的目标是找到在容许范围 $(0, r]$ 内使该方差最小化的 $t$ 值。令 $P = p(t) = (t/r)^k$。当 $t$ 在 $(0, r]$ 内变化时，$P$ 在 $(0, 1]$ 内变化。我们希望最小化函数 $g(P) = P(1-P)$，其中 $P \\in (0, 1]$。这个函数是一个开口向下的抛物线，在 $P=1/2$ 处有最大值，在其定义域的边界 $P=0$ 和 $P=1$ 处有最小值。在区间 $(0, 1]$ 上，$g(P)$ 的最小值是 $0$，在 $P=1$ 时取得。\n设置 $p(t)=1$，我们得到：\n$$\n\\left(\\frac{t}{r}\\right)^{k} = 1 \\implies \\frac{t}{r} = 1 \\implies t = r\n$$\n由于 $t=r$ 在容许集合 $(0, r]$ 内，因此它是最优值。当 $t=r$ 时，方差为 $\\operatorname{Var}(I_r) = 1(1-1) = 0$。\n\n我们现在使用问题陈述中要求的概念来为这个结果提供论证。最小化 $\\operatorname{Var}(I_t)$ 等价于使 $p(t)$ 尽可能接近 $0$ 或 $1$。为了抽样效率，我们寻求最大化接受概率 $p(t)$，这对应于 $p(t)=1$ 的情况。最大化 $p(t)$ 等价于最小化其倒数 $M(t)$。因此，问题等价于求解：\n$$\n\\min_{t \\in (0, r]} M(t) \\quad \\text{即} \\quad \\min_{t \\in (0, r]} \\sup_{x>0} \\frac{f(x)}{q_t(x)}\n$$\n这是一个极小化极大问题。通过取对数（一个单调函数），我们可以等价地求解：\n$$\n\\min_{t \\in (0, r]} \\sup_{x>0} \\log\\left(\\frac{f(x)}{q_t(x)}\\right)\n$$\n令 $L(x,t) = \\log(f(x)/q_t(x)) = k\\log(r/t) + (t-r)x$。函数 $L(x,t)$ 关于 $t$ 是凸的，关于 $x$ 是线性的（因此是凹的）。由于 $t$ 和 $x$ 的定义域都是凸集，我们可以应用极小化极大定理并交换最小化和最大化的顺序（这是凸对偶的一个应用）。\n$$\n\\min_{t \\in (0, r]} \\sup_{x>0} L(x,t) = \\sup_{x>0} \\min_{t \\in (0, r]} L(x,t)\n$$\n我们来分析内部的最小化问题：对于一个固定的 $x>0$，求 $\\min_{t \\in (0, r]} L(x,t)$。我们将 $L(x,t)$ 对 $t$ 求导：\n$$\n\\frac{\\partial L}{\\partial t} = -\\frac{k}{t} + x\n$$\n令导数为零得到 $t = k/x$。这个临界点对应一个最小值，因为二阶导数 $\\frac{\\partial^2 L}{\\partial t^2} = k/t^2 > 0$。\n条件 $t=k/x$ 提供了矩匹配的解释。提议分布 $q_t(x)$（一个伽马($k,t$)分布）的均值是 $\\mathbb{E}_{q_t}[X] = k/t$。条件 $t = k/x$ 等价于 $\\mathbb{E}_{q_t}[X] = x$。这意味着对于一个给定的点 $x$，局部最优的提议分布是其期望值为 $x$ 的那个分布。\n\n解 $t=k/x$ 仅当它位于容许范围 $(0,r]$ 内时才有效。\n- 如果 $k/x \\in (0,r]$，即 $x \\ge k/r$，则最小值在 $t=k/x$ 处取得。\n- 如果 $k/x > r$，即 $x  k/r$，则导数 $\\partial L / \\partial t = x - k/t$ 对于 $t \\in (0,r]$ 总是负的，因为 $t \\le r  k/x$。因此，$L(x,t)$ 在 $t$ 上是递减的，最小值在边界 $t=r$ 处取得。\n\n现在，将这个结果代回到外部问题中可以得到极大化极小值。然而，直接求解原始问题 $\\min_{t \\in (0, r]} \\sup_{x>0} L(x,t)$ 更为直接。如前所示，对于 $t \\in (0,r]$，$\\sup_{x>0} L(x,t) = k\\log(r/t)$。函数 $k\\log(r/t)$ 是 $t$ 的递减函数。它在 $(0,r]$ 上的最小值在 $t$ 的最大可能值处取得，即 $t=r$。\n因此，最优倾斜参数是 $t^*=r$。\n\n矩匹配的解释给出了最终的洞见。全局最优的提议分布是通过将提议分布的均值与目标分布的均值相匹配来找到的。\n目标分布 $f(x) \\sim \\text{Gamma}(k,r)$ 的均值是 $\\mathbb{E}_f[X] = k/r$。\n提议分布 $q_t(x) \\sim \\text{Gamma}(k,t)$ 的均值是 $\\mathbb{E}_{q_t}[X] = k/t$。\n令两个均值相等以找到最优的 $t$：\n$$\n\\mathbb{E}_{q_t}[X] = \\mathbb{E}_{f}[X] \\implies \\frac{k}{t} = \\frac{k}{r} \\implies t = r\n$$\n这证实了在所要求的框架下，$t^*=r$ 是最优选择。当 $t=r$ 时，提议分布和目标分布相同 ($q_r = f$)，导致 $M(r)=1$，$p(r)=1$，以及 $\\operatorname{Var}(I_r)=0$，这是可能的最有效的结果。", "answer": "$$\\boxed{r}$$", "id": "3309202"}, {"introduction": "除了精确采样方法，马尔可夫链蒙特卡洛（MCMC）提供了另一种强大的生成器构建范式。这项实践任务是设计并分析一个混合式 Metropolis-Hastings 伽马生成器，该生成器在对数尺度上使用正态提议分布 [@problem_id:3309192]。通过这个练习，你将探究提议分布的方差失配如何影响接受率，并理解单步 MCMC 方法为何会引入偏差，从而深入认识近似采样算法的理论权衡与实际挑战。", "problem": "要求您设计并分析一个用于 Gamma 分布的单步混合生成器，该生成器结合了正态提议与单步 Metropolis-Hastings (MH) 校正。目标分布是形状参数为 $k$、尺度参数为 $\\theta$ 的 Gamma 分布，记为 $\\text{Gamma}(k,\\theta)$，其在 $x \\in (0,\\infty)$ 上的密度由下式给出\n$$\n\\pi_X(x) = \\frac{1}{\\Gamma(k)\\,\\theta^k}\\,x^{k-1}\\,e^{-x/\\theta}.\n$$\n为在使用正态提议时确保正值性，请在对数尺度上进行操作。定义 $Y = \\log X$。通过变量替换，$Y$ 的目标密度为\n$$\n\\pi_Y(y) = \\pi_X(e^y)\\,e^y = \\frac{1}{\\Gamma(k)\\,\\theta^k}\\,e^{k y - e^y/\\theta}, \\quad y \\in \\mathbb{R}.\n$$\n您的混合生成器在对数尺度上使用一个具有正态密度的独立性提议，\n$$\nq_Y(y) = \\mathcal{N}(m, s^2),\n$$\n其中均值参数 $m$ 和方差参数 $s^2$ 是通过对 $\\text{Gamma}(k,\\theta)$ 进行对数尺度矩近似来选择的。在目标分布下，$Y=\\log X$ 的确切均值和方差由双伽马函数和三伽马函数给出：如果 $\\psi(\\cdot)$ 是双伽马函数，$\\psi_1(\\cdot)$ 是三伽马函数，则\n$$\n\\mathbb{E}[Y] = \\psi(k) + \\log \\theta, \\qquad \\mathrm{Var}(Y) = \\psi_1(k).\n$$\n在本问题中，设置 $m = \\psi(k) + \\log \\theta$，并通过 $s^2 = c \\,\\psi_1(k)$（其中缩放因子 $c  0$）来研究方差误设的影响。从确定性初始状态 $y_0 = m$ 开始，执行一个以 $\\pi_Y$ 为目标、以 $q_Y$ 为提议的单步独立 Metropolis-Hastings (MH) 步骤。给定一个提议 $y' \\sim q_Y$，接受概率为\n$$\n\\alpha(y_0, y') = \\min\\left\\{1,\\ \\frac{\\pi_Y(y')\\,q_Y(y_0)}{\\pi_Y(y_0)\\,q_Y(y')}\\right\\}.\n$$\n如果提议被接受，则设置 $Y_{\\text{out}} = y'$；否则设置 $Y_{\\text{out}} = y_0$。最后，返回 $X_{\\text{out}} = \\exp(Y_{\\text{out}})$ 作为生成的变量。\n\n您的任务是：\n- 实现上述混合生成器，并用它对每个参数集经验性地估计两个量：接受率和生成均值的偏差。具体来说，对于每个参数集，独立运行单步生成器 $M$ 次，每次都从相同的 $y_0 = m$ 开始，并计算：\n  1. 接受率，定义为 $M$ 次试验中被接受的提议所占的比例。\n  2. 均值偏差，定义为 $\\widehat{\\mathbb{E}}[X_{\\text{out}}] - k\\theta$，其中 $\\widehat{\\mathbb{E}}[X_{\\text{out}}]$ 是生成的 $X_{\\text{out}}$ 值的经验均值，$k\\theta$ 是 $\\text{Gamma}(k,\\theta)$ 的真实均值。\n- 从第一性原理出发，描述在离散不足 ($c  1$)、离散度正确 ($c = 1$) 和过度离散 ($c  1$) 的情况下，接受率如何随方差误设因子 $c$ 变化，并定性解释当 $c \\neq 1$ 时，生成器的单步特性如何导致均值产生偏差。\n\n您的推导应基于上述核心定义和关于 Metropolis-Hastings 算法的标准事实。除这些之外，不要假设任何专门的快捷公式。\n\n实现要求：\n- 在对数尺度上使用如上定义的独立 Metropolis-Hastings (MH) 步骤。\n- 使用精确的对数目标 $\\log \\pi_Y(y) = k y - e^y/\\theta - \\log \\Gamma(k) - k \\log \\theta$ 和精确的对数提议 $\\log q_Y(y) = -\\tfrac{1}{2}\\log(2\\pi s^2) - \\tfrac{(y-m)^2}{2s^2}$，其中 $m = \\psi(k) + \\log \\theta$ 且 $s^2 = c\\,\\psi_1(k)$。\n- 为保证可复现性，使用固定的随机种子 $123456789$。\n- 对每个测试用例，运行 $M = 200000$ 次独立的单步试验。\n\n测试套件：\n为以下参数集 $(k,\\theta,c)$ 提供结果：\n- 情况 A (理想路径，离散度正确)：$(k,\\theta,c) = (2, 1, 1)$。\n- 情况 B (过度离散的提议)：$(k,\\theta,c) = (2, 1, 4)$。\n- 情况 C (离散不足的提议)：$(k,\\theta,c) = (2, 1, 0.25)$。\n- 情况 D (形状小于 1，接近零的边界行为)：$(k,\\theta,c) = (0.5, 1, 1)$。\n- 情况 E (大形状，原始尺度上接近正态的目标，误设方差)：$(k,\\theta,c) = (20, 2, 2)$。\n\n答案规格：\n- 对每个情况，报告一个包含两个浮点数的列表：上面定义的接受率和均值偏差。\n- 将五个情况的结果按 A、B、C、D、E 的顺序汇总到一个列表中。\n- 您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，其中每个情况的结果本身就是一个双元素列表，例如：“[[accA,biasA],[accB,biasB],...,[accE,biasE]]”。\n- 将每个报告的浮点数四舍五入到 $6$ 位小数。\n\n角度和物理单位不适用于此问题。所有输出必须是不带任何单位的纯数字。", "solution": "问题陈述已经过验证，并被认为是合理的。它在科学上基于随机模拟和 Metropolis-Hastings 算法的原理。该问题定义明确、客观，并提供了一套完整且一致的定义和参数，从而能够得到唯一且可验证的解。\n\n任务是为伽马分布 $\\text{Gamma}(k, \\theta)$ 实现并分析一个单步 Metropolis-Hastings (MH) 生成器。该算法在对数尺度 $Y = \\log X$ 上运行，以确保生成的变量为正值。\n\n$Y$ 的目标概率密度函数由下式给出\n$$\n\\pi_Y(y) = \\frac{1}{\\Gamma(k)\\,\\theta^k}\\,e^{k y - e^y/\\theta}.\n$$\n该生成器采用一个独立正态提议，$q_Y(y) = \\mathcal{N}(m, s^2)$，其参数被选择来近似对数伽马分布的矩：\n- 均值：$m = \\mathbb{E}[Y] = \\psi(k) + \\log \\theta$\n- 方差：$s^2 = c \\cdot \\mathrm{Var}(Y) = c \\cdot \\psi_1(k)$\n这里，$\\psi(\\cdot)$ 和 $\\psi_1(\\cdot)$ 分别是双伽马函数和三伽马函数，$c$ 是一个方差误设因子。\n\n算法从确定性初始状态 $y_0 = m$ 开始，通过一个单步 MH 步骤进行。从提议分布中抽取一个提议 $y'$，$y' \\sim q_Y(y)$。该提议以如下概率被接受\n$$\n\\alpha(y_0, y') = \\min\\left\\{1,\\ \\frac{\\pi_Y(y')\\,q_Y(y_0)}{\\pi_Y(y_0)\\,q_Y(y')}\\right\\}.\n$$\n如果接受，输出为 $Y_{\\text{out}} = y'$；否则为 $Y_{\\text{out}} = y_0$。最终的变量是 $X_{\\text{out}} = \\exp(Y_{\\text{out}})$。\n\n为了高效地实现接受-拒绝步骤，我们处理接受比的对数。完整的比率为\n$$\nr = \\frac{\\pi_Y(y')}{\\pi_Y(y_0)} \\cdot \\frac{q_Y(y_0)}{q_Y(y')}.\n$$\n对数比率 $\\log r$ 可以使用非归一化密度计算，因为归一化常数会抵消。令 $\\log \\tilde{\\pi}_Y(y) = ky - e^y/\\theta$ 和 $\\log \\tilde{q}_Y(y) = -\\frac{(y-m)^2}{2s^2}$。则对数比率为：\n$$\n\\log r = \\left( \\log \\tilde{\\pi}_Y(y') - \\log \\tilde{\\pi}_Y(y_0) \\right) + \\left( \\log \\tilde{q}_Y(y_0) - \\log \\tilde{q}_Y(y') \\right).\n$$\n给定初始状态 $y_0 = m$，提议密度的项简化为：\n- $\\log \\tilde{q}_Y(y_0) = -\\frac{(m-m)^2}{2s^2} = 0$。\n- $\\log \\tilde{q}_Y(y') = -\\frac{(y'-m)^2}{2s^2}$。\n因此，对于一个提议 $y'$，对数接受比为\n$$\n\\log r = \\left( (ky' - e^{y'}/\\theta) - (km - e^{m}/\\theta) \\right) + \\left( 0 - \\left(-\\frac{(y'-m)^2}{2s^2}\\right) \\right) = (ky' - e^{y'}/\\theta) - (km - e^{m}/\\theta) + \\frac{(y'-m)^2}{2c\\,\\psi_1(k)}.\n$$\n如果一个均匀随机变量 $u \\sim U(0,1)$ 满足 $u  r$（等价于 $\\log u  \\log r$），则提议 $y'$ 被接受。\n\n**接受率分析**\n\n接受率是接受概率的期望值，$\\mathbb{E}_{y' \\sim q_Y}[\\alpha(y_0, y')]$。当 $c=1$ 时，提议分布 $q_Y$ 是对目标密度 $\\pi_Y$ 的一个二阶矩近似。在所有均值为 $m$ 的正态分布中，它在方差方面提供了最佳的“拟合”。\n- **离散度正确 ($c=1$)**：提议分布 $q_Y$ 的方差与目标分布 $\\pi_Y$ 的方差相同。这个选择使得 $q_Y$ 成为 $\\pi_Y$ 的一个良好全局近似，最大化了比率 $\\pi_Y(y')/q_Y(y')$ 接近 $\\pi_Y(y_0)/q_Y(y_0)$ 的区域。因此，接受比 $r$ 经常接近于 1，从而导致该提议族中可能达到的最高接受率。\n- **过度离散 ($c > 1$)**：提议方差 $s^2$ 大于目标方差。提议分布比目标分布更平坦且尾部更重。这种策略会频繁地在 $\\pi_Y$ 的尾部提出点 $y'$，在这些地方目标密度 $\\pi_Y(y')$ 极小。尽管提议密度 $q_Y(y')$ 在尾部也较小，但衰减率的不匹配导致比率 $\\pi_Y(y') / q_Y(y')$ 对于这些点非常小，从而导致接受概率 $r$ 很小，整体接受率很低。\n- **离散不足 ($c  1$)**：提议方差 $s^2$ 小于目标方差。提议分布比目标分布在均值 $m$ 处更尖锐。虽然大多数提出的点 $y'$ 落在目标 $\\pi_Y$ 的高概率区域，但比率 $q_Y(y_0)/q_Y(y') = \\exp((y'-m)^2 / (2s^2))$ 对于离均值 $m$ 中等距离的点也可能变得非常大。接受比中的这一项可能占主导地位，并将 $r$ 推向零，从而降低接受率。本质上，提议未能充分探索目标的尾部。\n\n**均值偏差分析**\n\n单步生成器不直接从目标分布 $\\text{Gamma}(k,\\theta)$ 中产生样本。相反，样本 $X_{\\text{out}}$ 是从一个混合分布中抽取的。设 $A_a$ 为接受率。输出分布在初始值 $x_0 = \\exp(y_0)$ 处有一个点质量，其概率为 $(1-A_a)$，并以概率 $A_a$ 从一个已接受提议的分布中抽取。\n输出的期望值为：\n$$\n\\mathbb{E}[X_{\\text{out}}] = (1 - A_a) \\cdot \\exp(y_0) + A_a \\cdot \\mathbb{E}[ \\exp(y') \\mid \\text{提议 } y' \\text{ 被接受} ].\n$$\n偏差为 $\\mathbb{E}[X_{\\text{out}}] - k\\theta$。关键的观察点是：\n1. 原始尺度上的初始状态为 $x_0 = \\exp(y_0) = \\exp(m) = \\exp(\\psi(k) + \\log\\theta) = \\theta \\exp(\\psi(k))$。\n2. 对严格凹函数 $\\log(\\cdot)$ 和一个非退化的随机变量 $Z \\sim \\text{Gamma}(k,1)$ 应用琴生不等式，我们有 $\\mathbb{E}[\\log Z]  \\log(\\mathbb{E}[Z])$。这可以转化为 $\\psi(k)  \\log(k)$。\n3. 因此，$\\exp(\\psi(k))  \\exp(\\log(k)) = k$。这意味着起始点 $x_0 = \\theta \\exp(\\psi(k))$ 系统性地小于目标分布的真实均值 $\\mathbb{E}[X] = k\\theta$。\n4. 当一个提议被拒绝时，算法会输出这个系统性偏低的值 $x_0$。任何增加拒绝率（即降低 $A_a$）的条件都会增加 $x_0$ 在最终混合分布中的权重，将经验均值 $\\widehat{\\mathbb{E}}[X_{\\text{out}}]$ 向下拉低，使其偏离真实均值 $k\\theta$。\n5. 由于过度离散 ($c>1$) 和离散不足 ($c1$) 都会导致比最优情况 ($c=1$) 更低的接受率，它们都会在生成样本的均值中引入更显著的负偏差。即使对于 $c=1$ 的情况，由于非零的拒绝率和采样器的单步特性，也预期会存在一些偏差。\n\n实现将遵循向量化的方法以提高效率。对于每个测试用例，同时生成 $M=200000$ 个提议。为所有提议计算对数接受比，并通过单次向量化比较来确定接受情况。生成的样本用于计算接受率和均值偏差。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import psi, polygamma\n\ndef solve():\n    \"\"\"\n    Implements and analyzes a one-step hybrid generator for the Gamma distribution.\n    \"\"\"\n    # Define fixed parameters from the problem statement.\n    M = 200000\n    SEED = 123456789\n    rng = np.random.default_rng(SEED)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (k, theta, c)\n        (2.0, 1.0, 1.0),\n        # Case B\n        (2.0, 1.0, 4.0),\n        # Case C\n        (2.0, 1.0, 0.25),\n        # Case D\n        (0.5, 1.0, 1.0),\n        # Case E\n        (20.0, 2.0, 2.0),\n    ]\n\n    results = []\n\n    for k, theta, c in test_cases:\n        # Calculate parameters for the Normal proposal distribution on the log-scale\n        # m = E[log(X)], s^2 = Var(log(X))\n        m = psi(k) + np.log(theta)\n        s2 = c * polygamma(1, k)\n        s = np.sqrt(s2)\n\n        # The initial state is fixed at the mean of the proposal distribution.\n        y0 = m\n\n        # --- Run M independent one-step trials in a vectorized manner ---\n\n        # 1. Generate M proposals from q_Y and M uniform variates for acceptance check.\n        y_prop = rng.normal(loc=m, scale=s, size=M)\n        u = rng.random(size=M)\n\n        # 2. Calculate the log of the Metropolis-Hastings acceptance ratio.\n        # We use unnormalized log-densities as the normalization constants cancel.\n        # log(r) = (log pi(y') - log pi(y0)) + (log q(y0) - log q(y'))\n        \n        # Unnormalized log target density: log(pi_tilde(y)) = k*y - exp(y)/theta\n        log_pi_y_prop = k * y_prop - np.exp(y_prop) / theta\n        log_pi_y0 = k * y0 - np.exp(y0) / theta\n\n        # Unnormalized log proposal density: log(q_tilde(y)) = -0.5 * (y-m)^2 / s^2\n        # Since y0 = m, log_q_y0 is 0.\n        log_q_y_prop = -0.5 * np.square(y_prop - m) / s2\n        log_q_y0 = 0.0\n\n        log_ratio = (log_pi_y_prop - log_pi_y0) + (log_q_y0 - log_q_y_prop)\n\n        # 3. Determine which trials are accepted.\n        # Accept if u  r, which is equivalent to log(u)  log(r).\n        accepted_mask = np.log(u)  log_ratio\n\n        # 4. Compute the empirical acceptance rate.\n        num_accepted = np.sum(accepted_mask)\n        acceptance_rate = num_accepted / M\n\n        # 5. Construct the final sample Y_out on the log scale.\n        # If accepted, Y_out = y', otherwise Y_out = y0.\n        y_out = np.where(accepted_mask, y_prop, y0)\n\n        # 6. Transform back to the original scale X_out and calculate the mean bias.\n        x_out = np.exp(y_out)\n        empirical_mean = np.mean(x_out)\n        true_mean = k * theta\n        mean_bias = empirical_mean - true_mean\n\n        # 7. Store the formatted results for this case.\n        results.append([round(acceptance_rate, 6), round(mean_bias, 6)])\n\n    # Final print statement in the exact required format.\n    # The replace(\" \", \"\") is to match the compact list format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3309192"}]}