{"hands_on_practices": [{"introduction": "在贝叶斯推断中，马尔可夫链蒙特卡洛（MCMC）方法是探索复杂后验分布的基石。本练习 ([@problem_id:3306479]) 将指导您为一个标准的贝叶斯正态模型实现一个吉布斯采样器。更重要的是，您将学习并应用Rao-Blackwell定理，通过将被采样的随机变量替换为其条件期望，来显著降低蒙特卡洛估计量的方差。通过这个练习，您将掌握提高估计效率的关键技能，这是计算统计学中的一个核心议题。", "problem": "考虑一个带有共轭先验的贝叶斯正态位置-尺度模型。设数据 $y = (y_1,\\dots,y_n)$ 在给定参数 $\\theta = (\\mu,\\sigma^2)$ 的条件下是条件独立同分布的，即对所有 $i \\in \\{1,\\dots,n\\}$ 都有 $y_i \\mid \\mu,\\sigma^2 \\sim \\mathcal{N}(\\mu,\\sigma^2)$。假设使用正态-逆伽马先验：$\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(\\mu_0,\\sigma^2/\\kappa_0)$ 和 $\\sigma^2 \\sim \\mathrm{Inverse\\text{-}Gamma}(a_0,b_0)$，其中逆伽马分布的密度在 $(0,\\infty)$ 上正比于 $(\\sigma^2)^{-a_0-1}\\exp(-b_0/\\sigma^2)$。对于所有有限数据，其后验是正常的（proper）。\n\n任务是使用 Gibbs 采样器和通过条件蒙特卡洛实现的 Rao–Blackwellization 方法，来估计后验均值 $\\mathbb{E}[g(\\theta)\\mid y]$（选择 $g(\\theta)=\\mu$），以及估计后验 $\\alpha$-分位数 $q_\\alpha(g(\\theta)\\mid y)$（同样选择 $g(\\theta)=\\mu$）。从第一性原理出发：通过 Bayes 定理对联合后验进行因子分解，条件期望的定义，以及分位数作为累积分布函数反函数的定义。\n\n设计一个 Gibbs 采样器，该采样器在两个满条件分布 $p(\\mu\\mid \\sigma^2,y)$ 和 $p(\\sigma^2\\mid \\mu,y)$ 之间交替采样。使用该采样器，构建以下估计量：\n- 一个基于 Gibbs 链中 $\\mu$ 抽样的直接蒙特卡洛平均的朴素后验均值估计量。\n- 一个 Rao–Blackwellized 后验均值估计量，它通过将每个抽样的 $\\mu$ 替换为条件期望 $\\mathbb{E}[\\mu\\mid \\sigma^2,y]$，然后在链的 $\\sigma^2$ 值上进行平均，从而对一个条件进行解析平均。\n- 一个基于 $\\mu$ 抽样样本分位数的朴素后验 $\\alpha$-分位数估计量。\n- 一个条件蒙特卡洛“平滑”后验 $\\alpha$-分位数估计量，该估计量通过以下方式获得：首先通过 $\\widehat{F}(t) = \\frac{1}{T}\\sum_{t=1}^T \\mathbb{P}(\\mu\\le t\\mid \\sigma_t^2,y)$ 估计 $\\mu$ 在任意阈值 $t$ 处的边际后验累积分布函数，其中 $\\{\\sigma_t^2\\}$ 是来自 Gibbs 采样器的 $\\sigma^2$ 抽样，T 是保留的迭代次数；然后数值上对 $t \\mapsto \\widehat{F}(t)$ 求逆以获得 $\\alpha$-分位数。\n\n通过模拟检验 Rao–Blackwellized 估计量相对于朴素估计量是否减小了以下各项的蒙特卡洛方差：\n- 后验均值。\n- 在真实后验 $\\alpha$-分位数处的后验累积分布函数。\n- 后验 $\\alpha$-分位数本身。\n\n为了使其可测试，请使用以下参数设置的测试套件。对于每个测试用例，程序必须：\n- 使用指定的数据 $y$ 和超参数 $(\\mu_0,\\kappa_0,a_0,b_0)$。\n- 运行 $R$ 次独立的 Gibbs 采样器复制，以凭经验估计各个复制之间的方差和均方误差。每次复制应使用 $M$ 次总迭代，其中包含 $B$ 次预烧（burn-in）迭代，然后保留接下来的 $T=M-B$ 次抽样。\n- 使用共轭性下已知的 $\\mu$ 的边际后验分布，以闭合形式计算 $\\mu$ 的真实后验 $\\alpha$-分位数，并相对于此目标评估朴素估计量和条件蒙特卡洛估计量。\n\n使用以下具体的测试用例（所有数字均为实标量）：\n\n- 用例 A:\n  - 数据长度 $n = 20$，数据向量 $y = [\\,1.3,\\,0.7,\\,1.8,\\,1.5,\\,0.9,\\,2.1,\\,0.4,\\,1.2,\\,0.8,\\,1.0,\\,1.7,\\,1.1,\\,0.5,\\,1.4,\\,0.6,\\,1.9,\\,0.3,\\,1.6,\\,1.2,\\,0.95\\,]$。\n  - 超参数 $(\\mu_0,\\kappa_0,a_0,b_0) = (0,\\,1,\\,2,\\,2)$。\n  - 分位数水平 $\\alpha = 0.5$。\n\n- 用例 B:\n  - 数据长度 $n = 3$，数据向量 $y = [\\,0.5,\\,1.2,\\,-0.1\\,]$。\n  - 超参数 $(\\mu_0,\\kappa_0,a_0,b_0) = (0,\\,1,\\,2,\\,2)$。\n  - 分位数水平 $\\alpha = 0.9$。\n\n- 用例 C:\n  - 数据长度 $n = 30$，数据向量 $y = [\\,-1.1,\\,-0.7,\\,-1.4,\\,-1.3,\\,-1.0,\\,-0.6,\\,-1.2,\\,-0.8,\\,-1.5,\\,-0.9,\\,-1.3,\\,-1.1,\\,-0.4,\\,-0.7,\\,-1.0,\\,-1.6,\\,-1.3,\\,-0.5,\\,-0.8,\\,-1.2,\\,-1.0,\\,-0.9,\\,-1.4,\\,-1.1,\\,-0.6,\\,-1.2,\\,-0.7,\\,-1.0,\\,-1.3,\\,-0.8\\,]$。\n  - 超参数 $(\\mu_0,\\kappa_0,a_0,b_0) = (0,\\,1,\\,2,\\,2)$。\n  - 分位数水平 $\\alpha = 0.1$。\n\n在所有用例中，您的程序必须：\n- 使用一个 Gibbs 采样器，它从 $p(\\mu\\mid \\sigma^2,y)$ 和 $p(\\sigma^2\\mid \\mu,y)$ 交替抽样，除了使用固定种子的伪随机数生成器以确保可复现性外，不引入任何外部随机性。\n- 对每个用例，运行 $R$ 次独立复制；在每次复制中，运行 $M$ 次迭代，其中包含 $B$ 次预烧，并保留 $T=M-B$ 次抽样。\n- 在 $R$ 次复制中，计算：\n  - $\\mu$ 的朴素后验均值估计量的样本方差，以及 $\\mu$ 的 Rao–Blackwellized 后验均值估计量的样本方差。\n  - 在 $\\mu$ 的真实后验 $\\alpha$-分位数处，后验累积分布函数的朴素估计量在各次复制中的样本方差，以及在同一阈值处，该函数的条件蒙特卡洛估计量在各次复制中的样本方差。\n  - 相对于真实后验 $\\alpha$-分位数，$\\mu$ 的朴素后验 $\\alpha$-分位数估计量在各次复制中的样本均方误差，以及 $\\mu$ 的条件蒙特卡洛平滑后验 $\\alpha$-分位数估计量在各次复制中的样本均方误差。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按 A、B、C 用例的顺序包含以下九个布尔条目：\n- 对每个用例，首先是一个布尔值，指示 Rao–Blackwellized 后验均值估计量的方差是否小于朴素后验均值估计量在各次复制中的方差。\n- 其次是一个布尔值，指示在真实后验 $\\alpha$-分位数处，后验累积分布函数的条件蒙特卡洛估计量的方差是否小于朴素经验累积分布函数估计量在同一阈值处的方差。\n- 第三是一个布尔值，指示条件蒙特卡洛平滑后验 $\\alpha$-分位数估计量的均方误差是否小于朴素经验分位数估计量在各次复制中的均方误差。\n\n因此，最终所需的输出必须是 $[b_1,b_2,b_3,b_4,b_5,b_6,b_7,b_8,b_9]$ 的形式，其中每个 $b_i \\in \\{\\mathrm{True},\\mathrm{False}\\}$。", "solution": "用户需要对贝叶斯正态位置-尺度模型中的 Gibbs 采样与 Rao-Blackwellization 进行详细分析和实现。解决方案必须从第一性原理推导，并且必须通过模拟对三个特定测试用例的标准估计量与方差缩减估计量的性能进行比较。\n\n### 1. 贝叶斯模型与后验分布\n\n该模型由数据 $y = (y_1, \\dots, y_n)$ 的正态似然函数和参数 $\\theta = (\\mu, \\sigma^2)$ 的共轭正态-逆伽马先验定义。\n\n-   **似然函数**：数据是条件独立同分布的，$y_i \\mid \\mu, \\sigma^2 \\sim \\mathcal{N}(\\mu, \\sigma^2)$。似然函数为：\n    $$p(y \\mid \\mu, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right) \\propto (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mu)^2\\right)$$\n-   **先验**：先验是正态-逆伽马分布，分层指定为 $\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(\\mu_0, \\sigma^2/\\kappa_0)$ 和 $\\sigma^2 \\sim \\mathrm{Inverse\\text{-}Gamma}(a_0, b_0)$。联合先验密度为：\n    $$p(\\mu, \\sigma^2) = p(\\mu \\mid \\sigma^2) p(\\sigma^2) \\propto (\\sigma^2)^{-1/2} \\exp\\left(-\\frac{\\kappa_0(\\mu - \\mu_0)^2}{2\\sigma^2}\\right) \\cdot (\\sigma^2)^{-a_0-1} \\exp\\left(-\\frac{b_0}{\\sigma^2}\\right)$$\n-   **后验**：联合后验分布 $p(\\mu, \\sigma^2 \\mid y)$ 正比于似然函数与先验的乘积，$p(\\mu, \\sigma^2 \\mid y) \\propto p(y \\mid \\mu, \\sigma^2) p(\\mu, \\sigma^2)$。由于共轭性，后验分布也是一个正态-逆伽马分布，$p(\\mu, \\sigma^2 \\mid y) \\sim \\mathrm{NIG}(\\mu_n, \\kappa_n, a_n, b_n)$，其更新后的超参数为：\n    -   $\\kappa_n = \\kappa_0 + n$\n    -   $\\mu_n = \\frac{\\kappa_0\\mu_0 + n\\bar{y}}{\\kappa_0 + n}$，其中 $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$\n    -   $a_n = a_0 + \\frac{n}{2}$\n    -   $b_n = b_0 + \\frac{1}{2} \\sum_{i=1}^n (y_i - \\bar{y})^2 + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(\\bar{y} - \\mu_0)^2$\n\n### 2. Gibbs 采样器\n\nGibbs 采样器通过从满条件分布 $p(\\mu \\mid \\sigma^2, y)$ 和 $p(\\sigma^2 \\mid \\mu, y)$ 中迭代采样，生成一个马尔可夫链，其平稳分布是联合后验分布 $p(\\mu, \\sigma^2 \\mid y)$。\n\n-   **$\\mu$ 的满条件分布**：给定 $\\sigma^2$ 和 $y$ 的条件下 $\\mu$ 的分布，可以从联合后验 $p(\\mu, \\sigma^2 \\mid y) = p(\\mu \\mid \\sigma^2, y) p(\\sigma^2 \\mid y)$ 推导得出。这意味着 $p(\\mu \\mid \\sigma^2, y)$ 是联合正态-逆伽马后验分布的正态部分。\n    $$ \\mu \\mid \\sigma^2, y \\sim \\mathcal{N}(\\mu_n, \\sigma^2/\\kappa_n) $$\n-   **$\\sigma^2$ 的满条件分布**：类似地，$\\sigma^2$ 的满条件分布可以从联合后验推导。我们使用 $p(\\sigma^2 \\mid \\mu, y) \\propto p(\\mu, \\sigma^2 \\mid y)$。在 NIG 后验密度中，涉及 $\\sigma^2$ 的项包括来自边际部分的 $(\\sigma^2)^{-a_n-1} \\exp(-b_n/\\sigma^2)$ 和来自条件部分的 $(\\sigma^2)^{-1/2}\\exp(-\\frac{\\kappa_n(\\mu-\\mu_n)^2}{2\\sigma^2})$。将这些项结合起来会得到一个逆伽马分布。\n    $$ \\sigma^2 \\mid \\mu, y \\sim \\mathrm{Inverse\\text{-}Gamma}\\left(a_n + \\frac{1}{2}, b_n + \\frac{\\kappa_n(\\mu - \\mu_n)^2}{2}\\right) $$\n    这种形式计算效率高，因为后验超参数 $(\\mu_n, \\kappa_n, a_n, b_n)$ 只需计算一次。\n\n**Gibbs 采样算法**：\n1.  初始化 $\\mu^{(0)}$ 和 $(\\sigma^2)^{(0)}$。\n2.  对于 $k = 1, 2, \\dots, M$：\n    a. 从分布 $\\mathrm{Inverse\\text{-}Gamma}\\left(a_n + \\frac{1}{2}, b_n + \\frac{\\kappa_n(\\mu^{(k-1)} - \\mu_n)^2}{2}\\right)$ 中抽取 $(\\sigma^2)^{(k)}$。\n    b. 从分布 $\\mathcal{N}(\\mu_n, (\\sigma^2)^{(k)}/\\kappa_n)$ 中抽取 $\\mu^{(k)}$。\n3.  丢弃前 $B$ 个样本作为预烧期，并保留随后的 $T=M-B$ 个样本 $\\{(\\mu^{(k)}, (\\sigma^2)^{(k)})\\}_{k=B+1}^M$。\n\n### 3. 后验均值 $\\mathbb{E}[\\mu \\mid y]$ 的估计量\n\n真实后验均值可以从 $\\mu$ 的边际后验分布解析地得知，该分布是一个学生 t 分布。此 t 分布的均值是其位置参数 $\\mu_n$。因此，$\\mathbb{E}[\\mu \\mid y] = \\mu_n$。\n\n-   **朴素估计量**：Gibbs 链中 $\\mu$ 样本的直接蒙特卡洛平均。\n    $$ \\hat{\\mathbb{E}}[\\mu \\mid y]_{\\text{naive}} = \\frac{1}{T} \\sum_{k=1}^T \\mu^{(k)} $$\n-   **Rao-Blackwellized 估计量**：该估计量利用了全期望定律 $\\mathbb{E}[\\mu \\mid y] = \\mathbb{E}_{\\sigma^2|y}[\\mathbb{E}[\\mu \\mid \\sigma^2, y]]$。蒙特卡洛近似是在 $\\sigma^2$ 的抽样上对内部的条件期望进行平均。\n    $$ \\hat{\\mathbb{E}}[\\mu \\mid y]_{\\text{RB}} = \\frac{1}{T} \\sum_{k=1}^T \\mathbb{E}[\\mu \\mid (\\sigma^2)^{(k)}, y] $$\n    因为我们知道 $\\mathbb{E}[\\mu \\mid \\sigma^2, y] = \\mu_n$，这是一个不依赖于 $\\sigma^2$ 的常数，所以估计量简化为：\n    $$ \\hat{\\mathbb{E}}[\\mu \\mid y]_{\\text{RB}} = \\frac{1}{T} \\sum_{k=1}^T \\mu_n = \\mu_n $$\n    Rao-Blackwellized 估计量就是精确的后验均值。它在模拟复制中的方差为 $0$，而朴素估计量的方差为正。因此，Rao-Blackwellized 估计量的方差将严格更小。\n\n### 4. 后验分位数 $q_\\alpha(\\mu \\mid y)$ 的估计量\n\n首先，我们确定用于比较的真实分位数。$\\mu$ 的边际后验分布是一个位置-尺度学生 t 分布，$\\mu \\mid y \\sim t(\\nu, \\mu_n, s^2)$，其中：\n-   自由度：$\\nu = 2a_n = 2a_0 + n$\n-   位置：$\\mu_{loc} = \\mu_n$\n-   尺度：$s = \\sqrt{b_n / (a_n \\kappa_n)}$\n\n真实 $\\alpha$-分位数 $q_\\alpha^*$ 为 $q_\\alpha^* = \\mu_n + s \\cdot t_{\\nu, \\alpha}$，其中 $t_{\\nu, \\alpha}$ 是自由度为 $\\nu$ 的标准学生 t 分布的 $\\alpha$-分位数。\n\n-   **朴素估计量**：收集到的 $\\mu$ 抽样的样本 $\\alpha$-分位数。\n    $$ \\hat{q}_{\\alpha, \\text{naive}} = \\text{SampleQuantile}(\\{\\mu^{(k)}\\}_{k=1}^T, \\alpha) $$\n-   **条件蒙特卡洛（平滑）估计量**：此方法首先估计 $\\mu$ 的后验累积分布函数（CDF）$F(t) = \\mathbb{P}(\\mu \\le t \\mid y)$，然后对其求逆。\n    CDF 使用 Rao-Blackwellization 进行估计：$F(t) = \\mathbb{E}_{\\sigma^2|y}[\\mathbb{P}(\\mu \\le t \\mid \\sigma^2, y)]$。\n    -   内部项是一个正态分布的 CDF：$\\mathbb{P}(\\mu \\le t \\mid \\sigma^2, y) = \\Phi\\left(\\frac{t-\\mu_n}{\\sqrt{\\sigma^2/\\kappa_n}}\\right)$，其中 $\\Phi$ 是标准正态 CDF。\n    -   估计的 CDF 为：$\\hat{F}(t) = \\frac{1}{T} \\sum_{k=1}^T \\Phi\\left(\\frac{t-\\mu_n}{\\sqrt{(\\sigma^2)^{(k)}/\\kappa_n}}\\right)$。\n    -   然后通过数值求解方程 $\\hat{F}(t) = \\alpha$ 来找到平滑分位数估计量 $\\hat{q}_{\\alpha, \\text{cmc}}$。\n\n### 5. 在 $q_\\alpha^*$ 处的后验 CDF 估计量\n\n为了评估 CDF 估计本身的方差缩减效果，我们在真实分位数 $q_\\alpha^*$ 处评估 CDF 估计量。\n\n-   **朴素估计量（经验 CDF）**：\n    $$ \\hat{F}_{\\text{naive}}(q_\\alpha^*) = \\frac{1}{T} \\sum_{k=1}^T \\mathbf{1}_{\\{\\mu^{(k)} \\le q_\\alpha^*\\}}, \\quad \\text{其中 } \\mathbf{1}_{\\{\\cdot\\}} \\text{ 是指示函数。} $$\n-   **条件蒙特卡洛估计量**：\n    $$ \\hat{F}_{\\text{cmc}}(q_\\alpha^*) = \\frac{1}{T} \\sum_{k=1}^T \\Phi\\left(\\frac{q_\\alpha^* - \\mu_n}{\\sqrt{(\\sigma^2)^{(k)}/\\kappa_n}}\\right) $$\n根据 Rao-Blackwell 定理，$\\text{Var}(\\hat{F}_{\\text{cmc}}(q_\\alpha^*)) \\le \\text{Var}(\\hat{F}_{\\text{naive}}(q_\\alpha^*))$，因为 $\\hat{F}_{\\text{cmc}}$ 是通过对随机变量 $\\mathbf{1}_{\\{\\mu \\le q_\\alpha^*\\}}$ 的条件期望进行平均而形成的。\n\n### 6. 模拟与比较逻辑\n\n对于每个测试用例，我们运行 $R$ 次独立的 Gibbs 采样器复制。在每次复制中，我们计算四个估计量：$\\hat{\\mathbb{E}}_{\\text{naive}}$、$\\hat{\\mathbb{E}}_{\\text{RB}}$、$\\hat{q}_{\\alpha, \\text{naive}}$ 和 $\\hat{q}_{\\alpha, \\text{cmc}}$。我们还计算两个 CDF 估计值，$\\hat{F}_{\\text{naive}}(q_\\alpha^*)$ 和 $\\hat{F}_{\\text{cmc}}(q_\\alpha^*)$。\n\n-   **比较 1（均值估计量方差）**：我们计算 $\\hat{\\mathbb{E}}_{\\text{naive}}$ 的 $R$ 个估计值的样本方差，并将其与 $\\hat{\\mathbb{E}}_{\\text{RB}}$ 的 $R$ 个估计值的样本方差进行比较。由于 $\\hat{\\mathbb{E}}_{\\text{RB}}$ 是常数 $\\mu_n$，其方差为 $0$，这将小于朴素估计量的正方差。\n-   **比较 2（CDF 估计量方差）**：我们计算 $\\hat{F}_{\\text{naive}}(q_\\alpha^*)$ 的 $R$ 个估计值的样本方差，并将其与 $\\hat{F}_{\\text{cmc}}(q_\\alpha^*)$ 的 $R$ 个估计值的样本方差进行比较。理论预测后者的方差会更小。\n-   **比较 3（分位数估计量 MSE）**：我们计算两种分位数估计量相对于真实分位数 $q_\\alpha^*$ 的均方误差（MSE）。\n    -   $\\text{MSE}_{\\text{naive}} = \\frac{1}{R} \\sum_{r=1}^R (\\hat{q}_{\\alpha, \\text{naive}}^{(r)} - q_\\alpha^*)^2$\n    -   $\\text{MSE}_{\\text{cmc}} = \\frac{1}{R} \\sum_{r=1}^R (\\hat{q}_{\\alpha, \\text{cmc}}^{(r)} - q_\\alpha^*)^2$\n我们比较这两个 MSE 值。由于平滑 CDF 是一个更好的估计量，其反函数（即分位数估计量）预计将具有更低的 MSE。\n\n对于所有三个比较，理论上保证 Rao-Blackwellized / 条件蒙特卡洛方法可以减少方差。因此，我们预计所有得到的布尔值都将是 `True`。该模拟是对这一原理的数值验证。模拟参数将设置为 $R=500$ 次复制，$M=2500$ 次总迭代，预烧期 $B=500$，从而每次复制产生 $T=2000$ 个样本。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, t as student_t\nfrom scipy.optimize import root_scalar\n\ndef run_simulation_for_case(data, mu0, kappa0, a0, b0, alpha, R, M, B, seed):\n    \"\"\"\n    Runs the full simulation for one test case.\n\n    Args:\n        data (np.ndarray): The observed data vector y.\n        mu0, kappa0, a0, b0 (float): Hyperparameters for the prior.\n        alpha (float): The quantile level.\n        R (int): Number of independent replications.\n        M (int): Total number of Gibbs iterations per replication.\n        B (int): Number of burn-in iterations.\n        seed (int): Seed for the main random number generator.\n\n    Returns:\n        tuple[bool, bool, bool]: The three boolean comparison results.\n    \"\"\"\n    n = len(data)\n    y_bar = np.mean(data)\n    sum_sq_dev = np.sum((data - y_bar)**2)\n    T = M - B\n\n    # --- 1. Compute posterior and true analytical values ---\n    # Posterior hyperparameters for NIG(mu_n, kappa_n, a_n, b_n)\n    kappa_n = kappa0 + n\n    mu_n = (kappa0 * mu0 + n * y_bar) / kappa_n\n    a_n = a0 + n / 2.0\n    b_n = b0 + 0.5 * sum_sq_dev + (n * kappa0 * (y_bar - mu0)**2) / (2.0 * (kappa0 + n))\n    \n    # True posterior mean is mu_n\n    true_posterior_mean = mu_n\n    \n    # Marginal posterior for mu is a Student's t distribution\n    # p(mu|y) = t(df=2*a_n, loc=mu_n, scale=sqrt(b_n/(a_n*kappa_n)))\n    nu = 2 * a_n\n    scale = np.sqrt(b_n / (a_n * kappa_n))\n    \n    # True posterior alpha-quantile\n    q_alpha_true = student_t.ppf(alpha, df=nu, loc=mu_n, scale=scale)\n\n    # --- 2. Setup for simulation replications ---\n    # Create a seeded RNG to generate seeds for each replication for reproducibility\n    main_rng = np.random.default_rng(seed)\n    replication_seeds = main_rng.integers(low=1, high=2**31, size=R)\n    \n    # Lists to store results from each replication\n    naive_mean_estimates = np.zeros(R)\n    rb_mean_estimates = np.zeros(R)\n    \n    naive_cdf_estimates_at_true_q = np.zeros(R)\n    cmc_cdf_estimates_at_true_q = np.zeros(R)\n    \n    naive_quantile_estimates = np.zeros(R)\n    cmc_quantile_estimates = np.zeros(R)\n\n    for r in range(R):\n        rng = np.random.default_rng(replication_seeds[r])\n        \n        # --- Gibbs Sampler ---\n        mu_samples = np.zeros(T)\n        sigma2_samples = np.zeros(T)\n        \n        # Initialize chain\n        mu_curr = y_bar\n        \n        for i in range(M):\n            # Sample sigma^2 | mu, y\n            a_cond = a_n + 0.5\n            b_cond = b_n + 0.5 * kappa_n * (mu_curr - mu_n)**2\n            # Inv-Gamma(a, b) draw is 1/Gamma(a, 1/b)\n            sigma2_curr = 1.0 / rng.gamma(shape=a_cond, scale=1.0/b_cond)\n            \n            # Sample mu | sigma^2, y\n            mu_curr = rng.normal(loc=mu_n, scale=np.sqrt(sigma2_curr / kappa_n))\n            \n            if i >= B:\n                mu_samples[i - B] = mu_curr\n                sigma2_samples[i - B] = sigma2_curr\n\n        # --- 3. Compute estimators for this replication ---\n        \n        # Mean estimators\n        naive_mean_estimates[r] = np.mean(mu_samples)\n        rb_mean_estimates[r] = true_posterior_mean # This is constant\n\n        # CDF estimators at the true quantile q_alpha_true\n        naive_cdf_estimates_at_true_q[r] = np.mean(mu_samples <= q_alpha_true)\n        phi_args = (q_alpha_true - mu_n) / np.sqrt(sigma2_samples / kappa_n)\n        cmc_cdf_estimates_at_true_q[r] = np.mean(norm.cdf(phi_args))\n\n        # Quantile estimators\n        naive_quantile_estimates[r] = np.quantile(mu_samples, alpha)\n        \n        # CMC quantile estimator via root finding\n        def cdf_to_invert(t):\n            phi_args_t = (t - mu_n) / np.sqrt(sigma2_samples / kappa_n)\n            return np.mean(norm.cdf(phi_args_t)) - alpha\n        \n        # Provide a reasonable search bracket based on the t-distribution\n        bracket_low = student_t.ppf(0.001, df=nu, loc=mu_n, scale=scale)\n        bracket_high = student_t.ppf(0.999, df=nu, loc=mu_n, scale=scale)\n        sol = root_scalar(cdf_to_invert, bracket=[bracket_low, bracket_high], method='brentq')\n        cmc_quantile_estimates[r] = sol.root\n\n    # --- 4. Compute variances and MSEs across replications ---\n    var_mean_naive = np.var(naive_mean_estimates, ddof=1)\n    var_mean_rb = np.var(rb_mean_estimates, ddof=1) # Will be 0\n\n    var_cdf_naive = np.var(naive_cdf_estimates_at_true_q, ddof=1)\n    var_cdf_cmc = np.var(cmc_cdf_estimates_at_true_q, ddof=1)\n\n    mse_quantile_naive = np.mean((naive_quantile_estimates - q_alpha_true)**2)\n    mse_quantile_cmc = np.mean((cmc_quantile_estimates - q_alpha_true)**2)\n\n    # --- 5. Perform comparisons ---\n    b1 = var_mean_rb < var_mean_naive\n    b2 = var_cdf_cmc < var_cdf_naive\n    b3 = mse_quantile_cmc < mse_quantile_naive\n\n    return b1, b2, b3\n\ndef solve():\n    # Simulation Parameters\n    R_reps = 500\n    M_iters = 2500\n    B_burn = 500\n\n    # Test Case A\n    y_A = np.array([1.3, 0.7, 1.8, 1.5, 0.9, 2.1, 0.4, 1.2, 0.8, 1.0, 1.7, 1.1, 0.5, 1.4, 0.6, 1.9, 0.3, 1.6, 1.2, 0.95])\n    params_A = {'data': y_A, 'mu0': 0.0, 'kappa0': 1.0, 'a0': 2.0, 'b0': 2.0, 'alpha': 0.5, 'R': R_reps, 'M': M_iters, 'B': B_burn, 'seed': 123}\n    \n    # Test Case B\n    y_B = np.array([0.5, 1.2, -0.1])\n    params_B = {'data': y_B, 'mu0': 0.0, 'kappa0': 1.0, 'a0': 2.0, 'b0': 2.0, 'alpha': 0.9, 'R': R_reps, 'M': M_iters, 'B': B_burn, 'seed': 456}\n\n    # Test Case C\n    y_C = np.array([-1.1, -0.7, -1.4, -1.3, -1.0, -0.6, -1.2, -0.8, -1.5, -0.9, -1.3, -1.1, -0.4, -0.7, -1.0, -1.6, -1.3, -0.5, -0.8, -1.2, -1.0, -0.9, -1.4, -1.1, -0.6, -1.2, -0.7, -1.0, -1.3, -0.8])\n    params_C = {'data': y_C, 'mu0': 0.0, 'kappa0': 1.0, 'a0': 2.0, 'b0': 2.0, 'alpha': 0.1, 'R': R_reps, 'M': M_iters, 'B': B_burn, 'seed': 789}\n    \n    test_cases = [params_A, params_B, params_C]\n    \n    all_results = []\n    for case_params in test_cases:\n        results = run_simulation_for_case(**case_params)\n        all_results.extend(results)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3306479"}, {"introduction": "将Rao-Blackwell化的思想从简单的均值估计推广到更复杂的量（如分位数）需要更精细的方法。由于分位数是累积分布函数（CDF）的逆，我们不能直接应用期望的技巧。本练习 ([@problem_id:3306479]) 演示了一种强大的两步策略：首先，使用条件蒙特卡洛方法（Rao-Blackwell化的一个实例）来平滑地估计后验CDF，然后通过数值方法反转该CDF来获得分位数。这个过程不仅能得到更准确的分位数估计，也加深了对蒙特卡洛方法灵活性的理解。", "problem": "考虑一个带有共轭先验的贝叶斯正态位置-尺度模型。设数据 $y = (y_1,\\dots,y_n)$ 在给定参数 $\\theta = (\\mu,\\sigma^2)$ 的条件下是独立同分布的，即对所有 $i \\in \\{1,\\dots,n\\}$ 都有 $y_i \\mid \\mu,\\sigma^2 \\sim \\mathcal{N}(\\mu,\\sigma^2)$。假设正态-逆伽马先验为：$\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(\\mu_0,\\sigma^2/\\kappa_0)$ 且 $\\sigma^2 \\sim \\mathrm{Inverse\\text{-}Gamma}(a_0,b_0)$，其中逆伽马分布的密度在 $(0,\\infty)$ 上正比于 $(\\sigma^2)^{-a_0-1}\\exp(-b_0/\\sigma^2)$。对于所有有限数据，后验分布都是正常的（proper）。\n\n任务是使用 Gibbs 采样器和通过条件蒙特卡洛实现的 Rao–Blackwellization 方法，来估计后验均值 $\\mathbb{E}[g(\\theta)\\mid y]$（其中 $g(\\theta)=\\mu$），以及估计后验 $\\alpha$-分位数 $q_\\alpha(g(\\theta)\\mid y)$（其中 $g(\\theta)=\\mu$）。请从第一性原理出发：通过贝叶斯定理对联合后验进行因式分解，条件期望的定义，以及分位数作为累积分布函数反函数的定义。\n\n设计一个 Gibbs 采样器，该采样器在两个全条件分布 $p(\\mu\\mid \\sigma^2,y)$ 和 $p(\\sigma^2\\mid \\mu,y)$ 之间交替采样。使用该采样器，构建以下估计量：\n- 一个朴素后验均值估计量，基于对 Gibbs 链中抽取的 $\\mu$ 值进行直接蒙特卡洛平均。\n- 一个 Rao–Blackwellized 后验均值估计量，它通过在对链中的 $\\sigma^2$ 值进行平均之前，将每个抽样的 $\\mu$ 替换为条件期望 $\\mathbb{E}[\\mu\\mid \\sigma^2,y]$，从而在一个条件上进行解析平均。\n- 一个朴素后验 $\\alpha$-分位数估计量，基于 $\\mu$ 抽样的样本分位数。\n- 一个条件蒙特卡洛“平滑”后验 $\\alpha$-分位数估计量，通过首先估计 $\\mu$ 在任意阈值 $t$ 处的边际后验累积分布函数 $\\widehat{F}(t) = \\frac{1}{T}\\sum_{t=1}^T \\mathbb{P}(\\mu\\le t\\mid \\sigma_t^2,y)$（其中 $\\{\\sigma_t^2\\}$ 是从 Gibbs 采样器中抽取的 $\\sigma^2$ 值，$T$ 是保留的迭代次数），然后通过数值方法对 $t \\mapsto \\widehat{F}(t)$ 进行求逆来获得 $\\alpha$-分位数。\n\n通过模拟检验 Rao–Blackwellized 估计量相对于朴素估计量是否降低了以下各项的蒙特卡洛方差：\n- 后验均值。\n- 在真实后验 $\\alpha$-分位数处的后验累积分布函数。\n- 后验 $\\alpha$-分位数本身。\n\n为了使其可测试，请使用以下参数设置的测试套件。对于每个测试用例，程序必须：\n- 使用指定的数据 $y$ 和超参数 $(\\mu_0,\\kappa_0,a_0,b_0)$。\n- 运行 $R$ 次独立的 Gibbs 采样器复制，以凭经验估计各次复制间的方差和均方误差。每次复制应使用 $M$ 次总迭代，其中包含 $B$ 次的预烧（burn-in）迭代，然后保留接下来的 $T=M-B$ 次抽样。\n- 使用共轭条件下已知的 $\\mu$ 的边际后验分布，以闭式形式计算 $\\mu$ 的真实后验 $\\alpha$-分位数，并相对于此目标评估朴素估计量和条件蒙特卡洛估计量。\n\n使用以下具体测试用例（所有数字均为实数标量）：\n\n- 用例 A：\n  - 数据长度 $n = 20$ 且数据向量 $y = [\\,1.3,\\,0.7,\\,1.8,\\,1.5,\\,0.9,\\,2.1,\\,0.4,\\,1.2,\\,0.8,\\,1.0,\\,1.7,\\,1.1,\\,0.5,\\,1.4,\\,0.6,\\,1.9,\\,0.3,\\,1.6,\\,1.2,\\,0.95\\,]$。\n  - 超参数 $(\\mu_0,\\kappa_0,a_0,b_0) = (0,\\,1,\\,2,\\,2)$。\n  - 分位数水平 $\\alpha = 0.5$。\n\n- 用例 B：\n  - 数据长度 $n = 3$ 且数据向量 $y = [\\,0.5,\\,1.2,\\,-0.1\\,]$。\n  - 超参数 $(\\mu_0,\\kappa_0,a_0,b_0) = (0,\\,1,\\,2,\\,2)$。\n  - 分位数水平 $\\alpha = 0.9$。\n\n- 用例 C：\n  - 数据长度 $n = 30$ 且数据向量 $y = [\\,-1.1,\\,-0.7,\\,-1.4,\\,-1.3,\\,-1.0,\\,-0.6,\\,-1.2,\\,-0.8,\\,-1.5,\\,-0.9,\\,-1.3,\\,-1.1,\\,-0.4,\\,-0.7,\\,-1.0,\\,-1.6,\\,-1.3,\\,-0.5,\\,-0.8,\\,-1.2,\\,-1.0,\\,-0.9,\\,-1.4,\\,-1.1,\\,-0.6,\\,-1.2,\\,-0.7,\\,-1.0,\\,-1.3,\\,-0.8\\,]$。\n  - 超参数 $(\\mu_0,\\kappa_0,a_0,b_0) = (0,\\,1,\\,2,\\,2)$。\n  - 分位数水平 $\\alpha = 0.1$。\n\n在所有用例中，您的程序必须：\n- 使用一个 Gibbs 采样器，从 $p(\\mu\\mid \\sigma^2,y)$ 和 $p(\\sigma^2\\mid \\mu,y)$ 交替抽样，除了使用固定种子的伪随机数生成器以确保可复现性外，不引入任何外部随机性。\n- 对每个用例，运行 $R$ 次独立复制；在每次复制中，运行 $M$ 次迭代，其中包含 $B$ 次预烧迭代，并保留 $T=M-B$ 次抽样。\n- 在 $R$ 次复制中，计算：\n  - $\\mu$ 的朴素后验均值估计量的样本方差，以及 $\\mu$ 的 Rao–Blackwellized 后验均值估计量的样本方差。\n  - 在 $\\mu$ 的真实后验 $\\alpha$-分位数处，后验累积分布函数的朴素估计量在各次复制间的样本方差，以及在相同阈值处，同一数量的条件蒙特卡洛估计量在各次复制间的样本方差。\n  - 相对于真实后验 $\\alpha$-分位数，$\\mu$ 的朴素后验 $\\alpha$-分位数估计量在各次复制中的均方误差，以及 $\\mu$ 的条件蒙特卡洛平滑后验 $\\alpha$-分位数估计量在各次复制中的均方误差。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按用例 A、B、C 的顺序包含以下九个布尔条目：\n- 对每个用例，首先是一个布尔值，指示 Rao–Blackwellized 后验均值估计量的方差是否小于朴素后验均值估计量在各次复制间的方差。\n- 其次，一个布尔值，指示在真实后验 $\\alpha$-分位数处，后验累积分布函数的条件蒙特卡洛估计量的方差是否小于在相同阈值处，朴素经验累积分布函数估计量在各次复制间的方差。\n- 第三，一个布尔值，指示条件蒙特卡洛平滑后验 $\\alpha$-分位数估计量的均方误差是否小于朴素经验分位数估计量在各次复制间的均方误差。\n\n因此，要求的最终输出必须是 $[b_1,b_2,b_3,b_4,b_5,b_6,b_7,b_8,b_9]$ 的形式，其中每个 $b_i \\in \\{\\mathrm{True},\\mathrm{False}\\}$。", "solution": "用户需要对一个贝叶斯正态位置-尺度模型，进行包含 Rao-Blackwellization 的 Gibbs 采样的详细分析和实现。解决方案必须从第一性原理推导得出，并且必须通过模拟对三个特定的测试用例，比较标准估计量与方差缩减估计量的性能。\n\n### 1. 贝叶斯模型和后验分布\n\n该模型由数据 $y = (y_1, \\dots, y_n)$ 的正态似然函数和参数 $\\theta = (\\mu, \\sigma^2)$ 的共轭正态-逆伽马先验定义。\n\n-   **似然函数**：数据是条件独立同分布的，$y_i \\mid \\mu, \\sigma^2 \\sim \\mathcal{N}(\\mu, \\sigma^2)$。似然函数为：\n    $$p(y \\mid \\mu, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right) \\propto (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mu)^2\\right)$$\n-   **先验分布**：先验分布是正态-逆伽马分布，分层指定为 $\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(\\mu_0, \\sigma^2/\\kappa_0)$ 且 $\\sigma^2 \\sim \\mathrm{Inverse\\text{-}Gamma}(a_0, b_0)$。联合先验密度为：\n    $$p(\\mu, \\sigma^2) = p(\\mu \\mid \\sigma^2) p(\\sigma^2) \\propto (\\sigma^2)^{-1/2} \\exp\\left(-\\frac{\\kappa_0(\\mu - \\mu_0)^2}{2\\sigma^2}\\right) \\cdot (\\sigma^2)^{-a_0-1} \\exp\\left(-\\frac{b_0}{\\sigma^2}\\right)$$\n-   **后验分布**：联合后验分布 $p(\\mu, \\sigma^2 \\mid y)$ 正比于似然函数和先验分布的乘积，$p(\\mu, \\sigma^2 \\mid y) \\propto p(y \\mid \\mu, \\sigma^2) p(\\mu, \\sigma^2)$。由于共轭性，后验分布也是一个正态-逆伽马分布，$p(\\mu, \\sigma^2 \\mid y) \\sim \\mathrm{NIG}(\\mu_n, \\kappa_n, a_n, b_n)$，其更新后的超参数为：\n    -   $\\kappa_n = \\kappa_0 + n$\n    -   $\\mu_n = \\frac{\\kappa_0\\mu_0 + n\\bar{y}}{\\kappa_0 + n}$，其中 $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$\n    -   $a_n = a_0 + \\frac{n}{2}$\n    -   $b_n = b_0 + \\frac{1}{2} \\sum_{i=1}^n (y_i - \\bar{y})^2 + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(\\bar{y} - \\mu_0)^2$\n\n### 2. Gibbs 采样器\n\nGibbs 采样器通过从全条件分布 $p(\\mu \\mid \\sigma^2, y)$ 和 $p(\\sigma^2 \\mid \\mu, y)$ 迭代采样，生成一个马尔可夫链，其平稳分布是联合后验分布 $p(\\mu, \\sigma^2 \\mid y)$。\n\n-   **$\\mu$ 的全条件分布**：给定 $\\sigma^2$ 和 $y$ 的条件下 $\\mu$ 的分布是从联合后验分布 $p(\\mu, \\sigma^2 \\mid y) = p(\\mu \\mid \\sigma^2, y) p(\\sigma^2 \\mid y)$ 推导出来的。这意味着 $p(\\mu \\mid \\sigma^2, y)$ 是联合正态-逆伽马后验分布的正态部分。\n    $$ \\mu \\mid \\sigma^2, y \\sim \\mathcal{N}(\\mu_n, \\sigma^2/\\kappa_n) $$\n-   **$\\sigma^2$ 的全条件分布**：类似地，$\\sigma^2$ 的全条件分布可以从联合后验分布推导出来。我们使用 $p(\\sigma^2 \\mid \\mu, y) \\propto p(\\mu, \\sigma^2 \\mid y)$。在 NIG 后验密度中，涉及 $\\sigma^2$ 的项来自边际部分的 $(\\sigma^2)^{-a_n-1} \\exp(-b_n/\\sigma^2)$ 和条件部分的 $(\\sigma^2)^{-1/2}\\exp(-\\frac{\\kappa_n(\\mu-\\mu_n)^2}{2\\sigma^2})$。将它们结合起来可以得到一个逆伽马分布。\n    $$ \\sigma^2 \\mid \\mu, y \\sim \\mathrm{Inverse\\text{-}Gamma}\\left(a_n + \\frac{1}{2}, b_n + \\frac{\\kappa_n(\\mu - \\mu_n)^2}{2}\\right) $$\n    这种形式计算效率很高，因为后验超参数 $(\\mu_n, \\kappa_n, a_n, b_n)$ 只需计算一次。\n\n**Gibbs 采样算法**：\n1.  初始化 $\\mu^{(0)}$ 和 $(\\sigma^2)^{(0)}$。\n2.  对 $k = 1, 2, \\dots, M$：\n    a. 从分布 $\\mathrm{Inverse\\text{-}Gamma}\\left(a_n + \\frac{1}{2}, b_n + \\frac{\\kappa_n(\\mu^{(k-1)} - \\mu_n)^2}{2}\\right)$ 中抽取 $(\\sigma^2)^{(k)}$。\n    b. 从分布 $\\mathcal{N}(\\mu_n, (\\sigma^2)^{(k)}/\\kappa_n)$ 中抽取 $\\mu^{(k)}$。\n3.  丢弃前 $B$ 个样本作为预烧期（burn-in），并保留随后的 $T=M-B$ 个样本 $\\{(\\mu^{(k)}, (\\sigma^2)^{(k)})\\}_{k=B+1}^M$。\n\n### 3. 后验均值 $\\mathbb{E}[\\mu \\mid y]$ 的估计量\n\n真实后验均值可以从 $\\mu$ 的边际后验分布（一个学生t分布）中解析得出。这个t分布的均值是其位置参数 $\\mu_n$。因此，$\\mathbb{E}[\\mu \\mid y] = \\mu_n$。\n\n-   **朴素估计量**：对 Gibbs 链中的 $\\mu$ 样本进行直接蒙特卡洛平均。\n    $$ \\hat{\\mathbb{E}}[\\mu \\mid y]_{\\text{naive}} = \\frac{1}{T} \\sum_{k=1}^T \\mu^{(k)} $$\n-   **Rao-Blackwellized 估计量**：该估计量利用了全期望定律，$\\mathbb{E}[\\mu \\mid y] = \\mathbb{E}_{\\sigma^2|y}[\\mathbb{E}[\\mu \\mid \\sigma^2, y]]$。蒙特卡洛近似是在 $\\sigma^2$ 的抽样上对内部的条件期望进行平均。\n    $$ \\hat{\\mathbb{E}}[\\mu \\mid y]_{\\text{RB}} = \\frac{1}{T} \\sum_{k=1}^T \\mathbb{E}[\\mu \\mid (\\sigma^2)^{(k)}, y] $$\n    由于我们知道 $\\mathbb{E}[\\mu \\mid \\sigma^2, y] = \\mu_n$，这是一个不依赖于 $\\sigma^2$ 的常数，因此估计量简化为：\n    $$ \\hat{\\mathbb{E}}[\\mu \\mid y]_{\\text{RB}} = \\frac{1}{T} \\sum_{k=1}^T \\mu_n = \\mu_n $$\n    Rao-Blackwellized 估计量就是精确的后验均值。它在模拟复制中的方差为 $0$，而朴素估计量的方差为正。因此，Rao-Blackwellized 估计量的方差将严格更小。\n\n### 4. 后验分位数 $q_\\alpha(\\mu \\mid y)$ 的估计量\n\n首先，我们确定真实分位数以进行比较。$\\mu$ 的边际后验分布是一个位置-尺度学生t分布，$\\mu \\mid y \\sim t(\\nu, \\mu_n, s^2)$，其中：\n-   自由度：$\\nu = 2a_n = 2a_0 + n$\n-   位置：$\\mu_loc = \\mu_n$\n-   尺度：$s = \\sqrt{b_n / (a_n \\kappa_n)}$\n\n真实的 $\\alpha$-分位数 $q_\\alpha^*$ 为 $q_\\alpha^* = \\mu_n + s \\cdot t_{\\nu, \\alpha}$，其中 $t_{\\nu, \\alpha}$ 是自由度为 $\\nu$ 的标准学生t分布的 $\\alpha$-分位数。\n\n-   **朴素估计量**：收集到的 $\\mu$ 抽样的样本 $\\alpha$-分位数。\n    $$ \\hat{q}_{\\alpha, \\text{naive}} = \\text{SampleQuantile}(\\{\\mu^{(k)}\\}_{k=1}^T, \\alpha) $$\n-   **条件蒙特卡洛（平滑）估计量**：此方法首先估计 $\\mu$ 的后验累积分布函数（CDF）$F(t) = \\mathbb{P}(\\mu \\le t \\mid y)$，然后对其求逆。\n    CDF 使用 Rao-Blackwellization 进行估计：$F(t) = \\mathbb{E}_{\\sigma^2|y}[\\mathbb{P}(\\mu \\le t \\mid \\sigma^2, y)]$。\n    -   内部项是正态分布的 CDF：$\\mathbb{P}(\\mu \\le t \\mid \\sigma^2, y) = \\Phi\\left(\\frac{t-\\mu_n}{\\sqrt{\\sigma^2/\\kappa_n}}\\right)$，其中 $\\Phi$ 是标准正态 CDF。\n    -   估计的 CDF 为：$\\hat{F}(t) = \\frac{1}{T} \\sum_{k=1}^T \\Phi\\left(\\frac{t-\\mu_n}{\\sqrt{(\\sigma^2)^{(k)}/\\kappa_n}}\\right)$。\n    -   然后通过数值求解方程 $\\hat{F}(t) = \\alpha$ 来找到平滑分位数估计量 $\\hat{q}_{\\alpha, \\text{cmc}}$。\n\n### 5. 在 $q_\\alpha^*$ 处的后验 CDF 估计量\n\n为了评估 CDF 估计本身的方差缩减效果，我们在真实分位数 $q_\\alpha^*$ 处评估 CDF 估计量。\n\n-   **朴素估计量（经验 CDF）**：\n    $$ \\hat{F}_{\\text{naive}}(q_\\alpha^*) = \\frac{1}{T} \\sum_{k=1}^T \\mathbf{1}_{\\{\\mu^{(k)} \\le q_\\alpha^*\\}}, \\quad \\text{其中 } \\mathbf{1}_{\\{\\cdot\\}} \\text{ 是指示函数。} $$\n-   **条件蒙特卡洛估计量**：\n    $$ \\hat{F}_{\\text{cmc}}(q_\\alpha^*) = \\frac{1}{T} \\sum_{k=1}^T \\Phi\\left(\\frac{q_\\alpha^* - \\mu_n}{\\sqrt{(\\sigma^2)^{(k)}/\\kappa_n}}\\right) $$\n根据 Rao-Blackwell 定理，$\\text{Var}(\\hat{F}_{\\text{cmc}}(q_\\alpha^*)) \\le \\text{Var}(\\hat{F}_{\\text{naive}}(q_\\alpha^*))$，因为 $\\hat{F}_{\\text{cmc}}$ 是通过对随机变量 $\\mathbf{1}_{\\{\\mu \\le q_\\alpha^*\\}}$ 的条件期望进行平均而形成的。\n\n### 6. 模拟与比较逻辑\n\n对于每个测试用例，我们运行 $R$ 次独立的 Gibbs 采样器复制。在每次复制中，我们计算四个估计量：$\\hat{\\mathbb{E}}_{\\text{naive}}$、$\\hat{\\mathbb{E}}_{\\text{RB}}$、$\\hat{q}_{\\alpha, \\text{naive}}$ 和 $\\hat{q}_{\\alpha, \\text{cmc}}$。我们还计算两个 CDF 估计值，$\\hat{F}_{\\text{naive}}(q_\\alpha^*)$ 和 $\\hat{F}_{\\text{cmc}}(q_\\alpha^*)$。\n\n-   **比较 1 (均值估计量方差)**：我们计算 $\\hat{\\mathbb{E}}_{\\text{naive}}$ 的 $R$ 个估计值的样本方差，并将其与 $\\hat{\\mathbb{E}}_{\\text{RB}}$ 的 $R$ 个估计值的样本方差进行比较。由于 $\\hat{\\mathbb{E}}_{\\text{RB}}$ 是常数 $\\mu_n$，其方差为 $0$，这将小于朴素估计量的正方差。\n-   **比较 2 (CDF 估计量方差)**：我们计算 $\\hat{F}_{\\text{naive}}(q_\\alpha^*)$ 的 $R$ 个估计值的样本方差，并将其与 $\\hat{F}_{\\text{cmc}}(q_\\alpha^*)$ 的 $R$ 个估计值的样本方差进行比较。理论预测后者会更小。\n-   **比较 3 (分位数估计量 MSE)**：我们计算两个分位数估计量相对于真实分位数 $q_\\alpha^*$ 的均方误差（MSE）。\n    -   $\\text{MSE}_{\\text{naive}} = \\frac{1}{R} \\sum_{r=1}^R (\\hat{q}_{\\alpha, \\text{naive}}^{(r)} - q_\\alpha^*)^2$\n    -   $\\text{MSE}_{\\text{cmc}} = \\frac{1}{R} \\sum_{r=1}^R (\\hat{q}_{\\alpha, \\text{cmc}}^{(r)} - q_\\alpha^*)^2$\n我们比较这两个 MSE 值。由于平滑 CDF 是一个更好的估计量，其逆函数（分位数估计量）预计将具有更低的 MSE。\n\n对于所有三个比较，理论上保证 Rao-Blackwellized / 条件蒙特卡洛方法可以减少方差。因此，我们期望所有得到的布尔值都为 `True`。该模拟是对这一原理的数值验证。模拟参数将设置为 $R=500$ 次复制，$M=2500$ 次总迭代，预烧期 $B=500$，每次复制产生 $T=2000$ 个样本。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, t as student_t\nfrom scipy.optimize import root_scalar\n\ndef run_simulation_for_case(data, mu0, kappa0, a0, b0, alpha, R, M, B, seed):\n    \"\"\"\n    Runs the full simulation for one test case.\n\n    Args:\n        data (np.ndarray): The observed data vector y.\n        mu0, kappa0, a0, b0 (float): Hyperparameters for the prior.\n        alpha (float): The quantile level.\n        R (int): Number of independent replications.\n        M (int): Total number of Gibbs iterations per replication.\n        B (int): Number of burn-in iterations.\n        seed (int): Seed for the main random number generator.\n\n    Returns:\n        tuple[bool, bool, bool]: The three boolean comparison results.\n    \"\"\"\n    n = len(data)\n    y_bar = np.mean(data)\n    sum_sq_dev = np.sum((data - y_bar)**2)\n    T = M - B\n\n    # --- 1. Compute posterior and true analytical values ---\n    # Posterior hyperparameters for NIG(mu_n, kappa_n, a_n, b_n)\n    kappa_n = kappa0 + n\n    mu_n = (kappa0 * mu0 + n * y_bar) / kappa_n\n    a_n = a0 + n / 2.0\n    b_n = b0 + 0.5 * sum_sq_dev + (n * kappa0 * (y_bar - mu0)**2) / (2.0 * (kappa0 + n))\n    \n    # True posterior mean is mu_n\n    true_posterior_mean = mu_n\n    \n    # Marginal posterior for mu is a Student's t distribution\n    # p(mu|y) = t(df=2*a_n, loc=mu_n, scale=sqrt(b_n/(a_n*kappa_n)))\n    nu = 2 * a_n\n    scale = np.sqrt(b_n / (a_n * kappa_n))\n    \n    # True posterior alpha-quantile\n    q_alpha_true = student_t.ppf(alpha, df=nu, loc=mu_n, scale=scale)\n\n    # --- 2. Setup for simulation replications ---\n    # Create a seeded RNG to generate seeds for each replication for reproducibility\n    main_rng = np.random.default_rng(seed)\n    replication_seeds = main_rng.integers(low=1, high=2**31, size=R)\n    \n    # Lists to store results from each replication\n    naive_mean_estimates = np.zeros(R)\n    rb_mean_estimates = np.zeros(R)\n    \n    naive_cdf_estimates_at_true_q = np.zeros(R)\n    cmc_cdf_estimates_at_true_q = np.zeros(R)\n    \n    naive_quantile_estimates = np.zeros(R)\n    cmc_quantile_estimates = np.zeros(R)\n\n    for r in range(R):\n        rng = np.random.default_rng(replication_seeds[r])\n        \n        # --- Gibbs Sampler ---\n        mu_samples = np.zeros(T)\n        sigma2_samples = np.zeros(T)\n        \n        # Initialize chain\n        mu_curr = y_bar\n        \n        for i in range(M):\n            # Sample sigma^2 | mu, y\n            a_cond = a_n + 0.5\n            b_cond = b_n + 0.5 * kappa_n * (mu_curr - mu_n)**2\n            # Inv-Gamma(a, b) draw is 1/Gamma(a, 1/b)\n            sigma2_curr = 1.0 / rng.gamma(shape=a_cond, scale=1.0/b_cond)\n            \n            # Sample mu | sigma^2, y\n            mu_curr = rng.normal(loc=mu_n, scale=np.sqrt(sigma2_curr / kappa_n))\n            \n            if i >= B:\n                mu_samples[i - B] = mu_curr\n                sigma2_samples[i - B] = sigma2_curr\n\n        # --- 3. Compute estimators for this replication ---\n        \n        # Mean estimators\n        naive_mean_estimates[r] = np.mean(mu_samples)\n        rb_mean_estimates[r] = true_posterior_mean # This is constant\n\n        # CDF estimators at the true quantile q_alpha_true\n        naive_cdf_estimates_at_true_q[r] = np.mean(mu_samples <= q_alpha_true)\n        phi_args = (q_alpha_true - mu_n) / np.sqrt(sigma2_samples / kappa_n)\n        cmc_cdf_estimates_at_true_q[r] = np.mean(norm.cdf(phi_args))\n\n        # Quantile estimators\n        naive_quantile_estimates[r] = np.quantile(mu_samples, alpha)\n        \n        # CMC quantile estimator via root finding\n        def cdf_to_invert(t):\n            phi_args_t = (t - mu_n) / np.sqrt(sigma2_samples / kappa_n)\n            return np.mean(norm.cdf(phi_args_t)) - alpha\n        \n        # Provide a reasonable search bracket based on the t-distribution\n        bracket_low = student_t.ppf(0.001, df=nu, loc=mu_n, scale=scale)\n        bracket_high = student_t.ppf(0.999, df=nu, loc=mu_n, scale=scale)\n        sol = root_scalar(cdf_to_invert, bracket=[bracket_low, bracket_high], method='brentq')\n        cmc_quantile_estimates[r] = sol.root\n\n    # --- 4. Compute variances and MSEs across replications ---\n    var_mean_naive = np.var(naive_mean_estimates, ddof=1)\n    var_mean_rb = np.var(rb_mean_estimates, ddof=1) # Will be 0\n\n    var_cdf_naive = np.var(naive_cdf_estimates_at_true_q, ddof=1)\n    var_cdf_cmc = np.var(cmc_cdf_estimates_at_true_q, ddof=1)\n\n    mse_quantile_naive = np.mean((naive_quantile_estimates - q_alpha_true)**2)\n    mse_quantile_cmc = np.mean((cmc_quantile_estimates - q_alpha_true)**2)\n\n    # --- 5. Perform comparisons ---\n    b1 = var_mean_rb < var_mean_naive\n    b2 = var_cdf_cmc < var_cdf_naive\n    b3 = mse_quantile_cmc < mse_quantile_naive\n\n    return b1, b2, b3\n\ndef solve():\n    # Simulation Parameters\n    R_reps = 500\n    M_iters = 2500\n    B_burn = 500\n\n    # Test Case A\n    y_A = np.array([1.3, 0.7, 1.8, 1.5, 0.9, 2.1, 0.4, 1.2, 0.8, 1.0, 1.7, 1.1, 0.5, 1.4, 0.6, 1.9, 0.3, 1.6, 1.2, 0.95])\n    params_A = {'data': y_A, 'mu0': 0.0, 'kappa0': 1.0, 'a0': 2.0, 'b0': 2.0, 'alpha': 0.5, 'R': R_reps, 'M': M_iters, 'B': B_burn, 'seed': 123}\n    \n    # Test Case B\n    y_B = np.array([0.5, 1.2, -0.1])\n    params_B = {'data': y_B, 'mu0': 0.0, 'kappa0': 1.0, 'a0': 2.0, 'b0': 2.0, 'alpha': 0.9, 'R': R_reps, 'M': M_iters, 'B': B_burn, 'seed': 456}\n\n    # Test Case C\n    y_C = np.array([-1.1, -0.7, -1.4, -1.3, -1.0, -0.6, -1.2, -0.8, -1.5, -0.9, -1.3, -1.1, -0.4, -0.7, -1.0, -1.6, -1.3, -0.5, -0.8, -1.2, -1.0, -0.9, -1.4, -1.1, -0.6, -1.2, -0.7, -1.0, -1.3, -0.8])\n    params_C = {'data': y_C, 'mu0': 0.0, 'kappa0': 1.0, 'a0': 2.0, 'b0': 2.0, 'alpha': 0.1, 'R': R_reps, 'M': M_iters, 'B': B_burn, 'seed': 789}\n    \n    test_cases = [params_A, params_B, params_C]\n    \n    all_results = []\n    for case_params in test_cases:\n        results = run_simulation_for_case(**case_params)\n        all_results.extend(results)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3306479"}, {"introduction": "除了MCMC方法，重要性采样提供了另一种强大的后验推断范式，尤其适用于难以直接采样但其密度函数已知的后验分布。该方法通过从一个简单的“提议分布”中抽取样本，并为每个样本分配一个“重要性权重”来修正其分布。本练习 ([@problem_id:3306507]) 将引导您从第一性原理出发，为几个经典的贝叶斯模型实现自标准化的重要性采样，以估计后验均值、CDF值和分位数。这项实践对于理解如何在无法进行直接采样时进行贝叶斯推断至关重要。", "problem": "设计并实现一个完整的程序，该程序使用自归一化重要性抽样来为指定的贝叶斯模型估计后验均值、累积分布函数和分位数。该解决方案必须基于以下基本原理构建：贝叶斯定理、后验期望作为概率测度下积分的定义、分位数作为累积分布函数广义逆的定义，以及用于变换积分的测度变换恒等式。不假定任何其他公式。\n\n给定一个目标后验密度 $\\pi(\\theta \\mid y)$（其归一化常数未知）和一个可以从中进行抽样的提议密度 $q(\\theta)$，使用自归一化重要性抽样来近似后验期望 $\\mathbb{E}_{\\pi}[h(\\theta)]$、后验累积分布函数值 $F_{\\pi}(x) = \\mathbb{P}_{\\pi}(\\theta \\le x)$，以及由 $Q_{\\pi}(\\alpha) = \\inf\\{x \\in \\mathbb{R} : F_{\\pi}(x) \\ge \\alpha\\}$ 定义的后验分位数 $Q_{\\pi}(\\alpha)$。你的估计器必须从测度变换下的期望定义出发，且不得假定能够获取 $\\pi(\\theta \\mid y)$ 的归一化常数。\n\n你的程序必须实现以下包含三个贝叶斯模型的测试套件。对每个模型，你必须：\n- 使用固定的随机种子从 $q(\\theta)$ 中抽取 $N$ 个提议样本。\n- 使用未归一化的后验核计算自归一化重要性权重。\n- 估计三个后验泛函：均值、在指定点 $x$ 的单个累积分布函数值，以及在指定概率水平 $\\alpha$ 的单个分位数。\n- 对于指定的共轭情况，使用闭式后验计算精确的基准真值。\n- 返回布尔值，指示三个估计的绝对误差是否在指定的容差范围内。\n\n全局蒙特卡洛设置：\n- 提议抽样数量：$N = 200000$。\n- 用于可复现性的随机种子：$s = 20231115$。\n- 容差：绝对均值误差阈值 $\\varepsilon_{\\mathrm{mean}} = 0.01$，绝对累积分布函数误差阈值 $\\varepsilon_{\\mathrm{cdf}} = 0.005$，绝对分位数误差阈值 $\\varepsilon_{\\mathrm{q}} = 0.02$。\n\n测试用例 A（高斯似然与高斯先验；实值参数）：\n- 先验：$\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$，其中 $\\mu_0 = 0$ 且 $\\tau_0 = 2$。\n- 似然：$y_1,\\dots,y_n \\mid \\theta \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(\\theta, \\sigma^2)$，其中 $\\sigma = 1$。\n- 观测到的汇总统计量：$n = 30$ 且 $\\bar{y} = 0.25$。\n- 已知闭式后验：$\\theta \\mid y \\sim \\mathcal{N}(\\mu_{\\mathrm{post}}, v_{\\mathrm{post}})$，其中 $v_{\\mathrm{post}} = \\left(\\frac{n}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right)^{-1}$ 且 $\\mu_{\\mathrm{post}} = v_{\\mathrm{post}}\\left(\\frac{n \\bar{y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right)$。\n- 提议：$q(\\theta) = \\mathcal{N}(m_q, s_q^2)$，其中 $m_q = \\mu_{\\mathrm{post}} + 0.5$ 且 $s_q = \\sqrt{2}\\sqrt{v_{\\mathrm{post}}}$。\n- 待估计的目标：后验均值、在 $x_0 = 0$ 处的后验累积分布函数，以及在概率水平 $\\alpha_0 = 0.95$ 处的后验分位数。\n\n测试用例 B（泊松似然与伽马先验；正值参数）：\n- 先验：$\\theta \\sim \\mathrm{Gamma}(a, b)$，其中形状 $a = 2$，率 $b = 1$。\n- 似然：$y_1,\\dots,y_n \\mid \\theta \\overset{\\text{i.i.d.}}{\\sim} \\mathrm{Poisson}(\\theta)$。\n- 观测到的汇总统计量：$n = 40$ 且 $\\sum_{i=1}^n y_i = 55$。\n- 已知闭式后验：$\\theta \\mid y \\sim \\mathrm{Gamma}(a', b')$，其中 $a' = a + \\sum_{i=1}^n y_i$ 且 $b' = b + n$。\n- 提议：$q(\\theta) = \\mathrm{Gamma}(\\tilde{a}, \\tilde{b})$，其中形状 $\\tilde{a} = 50$，率 $\\tilde{b} = 35$。\n- 待估计的目标：后验均值、在 $x_1 = 1.2$ 处的后验累积分布函数，以及在概率水平 $\\alpha_1 = 0.5$ 处的后验分位数。\n\n测试用例 C（伯努利似然与贝塔先验；单位区间参数）：\n- 先验：$\\theta \\sim \\mathrm{Beta}(\\alpha, \\beta)$，其中 $\\alpha = 0.5$ 且 $\\beta = 0.5$。\n- 似然：$y_1,\\dots,y_n \\mid \\theta \\overset{\\text{i.i.d.}}{\\sim} \\mathrm{Bernoulli}(\\theta)$。\n- 观测到的汇总统计量：$n = 80$ 且 $\\sum_{i=1}^n y_i = 20$。\n- 已知闭式后验：$\\theta \\mid y \\sim \\mathrm{Beta}(\\alpha', \\beta')$，其中 $\\alpha' = \\alpha + \\sum_{i=1}^n y_i$ 且 $\\beta' = \\beta + n - \\sum_{i=1}^n y_i$。\n- 提议：$q(\\theta) = \\mathrm{Beta}(\\tilde{\\alpha}, \\tilde{\\beta})$，其中 $\\tilde{\\alpha} = 1.0$ 且 $\\tilde{\\beta} = 1.5$。\n- 待估计的目标：后验均值、在 $x_2 = 0.3$ 处的后验累积分布函数，以及在概率水平 $\\alpha_2 = 0.95$ 处的后验分位数。\n\n用于比较的精确参考值：\n- 对测试用例 A，使用闭式高斯后验计算精确的后验均值、累积分布函数和分位数。\n- 对测试用例 B，使用闭式伽马后验计算精确的后验均值、累积分布函数和分位数。\n- 对测试用例 C，使用闭式贝塔后验计算精确的后验均值、累积分布函数和分位数。\n\n输出规范：\n- 对每个测试用例，按顺序生成三个布尔值：均值准确性、累积分布函数准确性、分位数准确性。当且仅当绝对误差小于或等于相应的容差 $\\varepsilon_{\\mathrm{mean}}$、$\\varepsilon_{\\mathrm{cdf}}$ 或 $\\varepsilon_{\\mathrm{q}}$ 时，每个布尔值为真。\n- 将三个测试用例的所有布尔值按测试用例 A、测试用例 B、测试用例 C 的顺序汇总到一个扁平列表中。\n- 你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如 $[r_1,r_2,\\dots,r_9]$），每个 $r_i$ 为字符串字面量 $\\texttt{True}$ 或 $\\texttt{False}$。", "solution": "当前任务要求设计并实现一个程序，使用自归一化重要性抽样 (SNIS) 来估计后验泛函。解决方案必须从基本原理推导得出，以贝叶斯定理和期望的定义为起点。\n\n### 自归一化重要性抽样的理论基础\n\n设 $\\theta$ 为参数向量，$y$ 为观测数据。根据贝叶斯定理，后验概率密度函数 (PDF) $\\pi(\\theta \\mid y)$ 与似然 $p(y \\mid \\theta)$ 和先验 $p(\\theta)$ 的乘积成正比：\n$$\n\\pi(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}\n$$\n其中 $p(y) = \\int p(y \\mid \\theta) p(\\theta) d\\theta$ 是边缘似然，作为一个归一化常数。我们将未归一化的后验核表示为 $f(\\theta) = p(y \\mid \\theta) p(\\theta)$，因此 $\\pi(\\theta \\mid y) = f(\\theta) / p(y)$。在许多实际场景中，$f(\\theta)$ 是已知的，但其积分 $p(y)$ 是难以计算的。\n\n函数 $h(\\theta)$ 的后验期望定义为：\n$$\n\\mathbb{E}_{\\pi}[h(\\theta)] = \\int h(\\theta) \\pi(\\theta \\mid y) d\\theta\n$$\n当直接从 $\\pi(\\theta \\mid y)$ 抽样很困难时，为了使用蒙特卡洛方法估计此积分，我们引入一个可以轻松从中抽取样本的提议分布 $q(\\theta)$。$q(\\theta)$ 的支撑集必须覆盖 $\\pi(\\theta \\mid y)$ 的支撑集，即只要 $\\pi(\\theta \\mid y) > 0$，就有 $q(\\theta) > 0$。\n\n我们通过在被积函数上乘以和除以 $q(\\theta)$ 来应用测度变换恒等式：\n$$\n\\mathbb{E}_{\\pi}[h(\\theta)] = \\int h(\\theta) \\frac{\\pi(\\theta \\mid y)}{q(\\theta)} q(\\theta) d\\theta = \\mathbb{E}_{q}\\left[h(\\theta) \\frac{\\pi(\\theta \\mid y)}{q(\\theta)}\\right]\n$$\n其中 $\\mathbb{E}_{q}[\\cdot]$ 表示关于提议分布 $q$ 的期望。代入 $\\pi(\\theta \\mid y) = f(\\theta) / p(y)$，我们得到：\n$$\n\\mathbb{E}_{\\pi}[h(\\theta)] = \\mathbb{E}_{q}\\left[h(\\theta) \\frac{f(\\theta)/p(y)}{q(\\theta)}\\right] = \\frac{1}{p(y)} \\mathbb{E}_{q}[h(\\theta) w(\\theta)]\n$$\n其中 $w(\\theta) = \\frac{f(\\theta)}{q(\\theta)}$ 是未归一化的重要性权重。未知常数 $p(y)$ 仍然存在。然而，我们可以将 $p(y)$ 本身表示为关于 $q$ 的一个期望：\n$$\np(y) = \\int f(\\theta) d\\theta = \\int \\frac{f(\\theta)}{q(\\theta)} q(\\theta) d\\theta = \\mathbb{E}_{q}[w(\\theta)]\n$$\n结合这两个表达式，得到自归一化形式：\n$$\n\\mathbb{E}_{\\pi}[h(\\theta)] = \\frac{\\mathbb{E}_{q}[h(\\theta) w(\\theta)]}{\\mathbb{E}_{q}[w(\\theta)]}\n$$\n给定从 $q(\\theta)$ 中抽取的 $N$ 个独立同分布样本 $\\theta_1, \\dots, \\theta_N$，我们可以使用蒙特卡洛平均来近似这些期望。令 $w_i = w(\\theta_i) = f(\\theta_i)/q(\\theta_i)$。$\\mathbb{E}_{\\pi}[h(\\theta)]$ 的 SNIS 估计器为：\n$$\n\\widehat{\\mathbb{E}}_{\\pi}[h(\\theta)] = \\frac{\\frac{1}{N}\\sum_{i=1}^{N} h(\\theta_i) w_i}{\\frac{1}{N}\\sum_{i=1}^{N} w_i} = \\frac{\\sum_{i=1}^{N} h(\\theta_i) w_i}{\\sum_{i=1}^{N} w_i}\n$$\n该估计器是两个估计量的比率，并且当 $N \\to \\infty$ 时，它对于 $\\mathbb{E}_{\\pi}[h(\\theta)]$ 是一致的。\n\n### 后验泛函的估计\n\n1.  **后验均值**：为估计后验均值，我们设 $h(\\theta) = \\theta$。其估计器为：\n    $$\n    \\hat{\\mu}_{\\pi} = \\frac{\\sum_{i=1}^{N} \\theta_i w_i}{\\sum_{i=1}^{N} w_i}\n    $$\n\n2.  **后验累积分布函数 (CDF)**：在点 $x$ 处的后验 CDF，$F_{\\pi}(x) = \\mathbb{P}_{\\pi}(\\theta \\le x)$，是一个期望，其中函数为指示函数 $h(\\theta) = \\mathbb{I}(\\theta \\le x)$。其估计器为：\n    $$\n    \\hat{F}_{\\pi}(x) = \\frac{\\sum_{i=1}^{N} \\mathbb{I}(\\theta_i \\le x) w_i}{\\sum_{i=1}^{N} w_i}\n    $$\n\n3.  **后验分位数**：$\\alpha$-分位数 $Q_{\\pi}(\\alpha)$ 是 CDF 的广义逆：$Q_{\\pi}(\\alpha) = \\inf\\{x: F_{\\pi}(x) \\ge \\alpha\\}$。我们通过对估计的 CDF $\\hat{F}_{\\pi}(x)$ 求逆来估计它。这可以通过算法实现。首先，计算归一化权重 $\\bar{w}_i = w_i / \\sum_{j=1}^{N} w_j$。然后，将样本 $\\{\\theta_i\\}$ 与其归一化权重 $\\{\\bar{w}_i\\}$ 配对。根据 $\\theta_i$ 的值对这些配对进行排序，得到 $(\\theta_{(1)}, \\bar{w}_{(1)}), \\dots, (\\theta_{(N)}, \\bar{w}_{(N)})$，其中 $\\theta_{(1)} \\le \\theta_{(2)} \\le \\dots \\le \\theta_{(N)}$。估计的分位数 $\\hat{Q}_{\\pi}(\\alpha)$ 是第一个使权重累积和至少为 $\\alpha$ 的有序样本 $\\theta_{(k)}$：\n    $$\n    \\hat{Q}_{\\pi}(\\alpha) = \\theta_{(k)} \\quad \\text{where} \\quad k = \\min\\left\\{j \\in \\{1, \\dots, N\\} : \\sum_{i=1}^{j} \\bar{w}_{(i)} \\ge \\alpha\\right\\}\n    $$\n\n### 数值实现与测试用例\n\n为了数值稳定性，涉及概率乘积的计算在对数域中进行。对数权重为 $\\log w_i = \\log f(\\theta_i) - \\log q(\\theta_i)$。为防止在求幂时溢出，一种常见的稳定化技巧是减去最大对数权重：$w_i = \\exp(\\log w_i - \\max_j(\\log w_j))$。这会重新缩放权重，但不会改变最终的 SNIS 估计值。\n\n**测试用例 A：高斯-高斯模型**\n- 先验：$\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$，因此 $p(\\theta) \\propto \\exp\\left(-\\frac{(\\theta-\\mu_0)^2}{2\\tau_0^2}\\right)$。\n- 使用汇总统计量的似然：$p(y \\mid \\theta) \\propto \\exp\\left(-\\frac{n(\\theta-\\bar{y})^2}{2\\sigma^2}\\right)$。\n- 未归一化后验核：$f(\\theta) \\propto \\exp\\left(-\\frac{n(\\theta-\\bar{y})^2}{2\\sigma^2} - \\frac{(\\theta-\\mu_0)^2}{2\\tau_0^2}\\right)$。\n- 提议：$q(\\theta) \\sim \\mathcal{N}(m_q, s_q^2)$，因此 $q(\\theta) \\propto \\exp\\left(-\\frac{(\\theta-m_q)^2}{2s_q^2}\\right)$。\n- 对数权重为 $\\log w(\\theta) = \\left(-\\frac{n(\\theta-\\bar{y})^2}{2\\sigma^2} - \\frac{(\\theta-\\mu_0)^2}{2\\tau_0^2}\\right) - \\left(-\\frac{(\\theta-m_q)^2}{2s_q^2}\\right)$。\n\n**测试用例 B：泊松-伽马模型**\n- 先验：$\\theta \\sim \\mathrm{Gamma}(a, b)$，因此 $p(\\theta) \\propto \\theta^{a-1} e^{-b\\theta}$ 对于 $\\theta > 0$。\n- 使用汇总统计量的似然：$p(y \\mid \\theta) \\propto \\theta^{\\sum y_i} e^{-n\\theta}$。\n- 未归一化后验核：$f(\\theta) \\propto \\theta^{a+\\sum y_i-1} e^{-(b+n)\\theta}$。\n- 提议：$q(\\theta) \\sim \\mathrm{Gamma}(\\tilde{a}, \\tilde{b})$，因此 $q(\\theta) \\propto \\theta^{\\tilde{a}-1} e^{-\\tilde{b}\\theta}$。\n- 对数权重为 $\\log w(\\theta) = ((a+\\!\\sum y_i\\!-\\!1)\\log\\theta - (b\\!+\\!n)\\theta) - ((\\tilde{a}\\!-\\!1)\\log\\theta - \\tilde{b}\\theta)$。\n\n**测试用例 C：伯努利-贝塔模型**\n- 先验：$\\theta \\sim \\mathrm{Beta}(\\alpha, \\beta)$，因此 $p(\\theta) \\propto \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}$ 对于 $\\theta \\in (0, 1)$。\n- 使用汇总统计量的似然：$p(y \\mid \\theta) \\propto \\theta^{\\sum y_i} (1-\\theta)^{n-\\sum y_i}$。\n- 未归一化后验核：$f(\\theta) \\propto \\theta^{\\alpha+\\sum y_i-1} (1-\\theta)^{\\beta+n-\\sum y_i-1}$。\n- 提议：$q(\\theta) \\sim \\mathrm{Beta}(\\tilde{\\alpha}, \\tilde{\\beta})$，因此 $q(\\theta) \\propto \\theta^{\\tilde{\\alpha}-1} (1-\\theta)^{\\tilde{\\beta}-1}$。\n- 对数权重为 $\\log w(\\theta) = ((\\alpha+\\!\\sum y_i\\!-\\!1)\\log\\theta + (\\beta+n-\\!\\sum y_i\\!-\\!1)\\log(1-\\theta)) - ((\\tilde{\\alpha}\\!-\\!1)\\log\\theta + (\\tilde{\\beta}\\!-\\!1)\\log(1-\\theta))$。\n\n该程序将实现这些模型，从每个提议分布生成 $N=200000$ 个样本，计算均值、指定 CDF 值和指定分位数的 SNIS 估计值，并与从已知共轭后验导出的精确值进行比较。比较基于绝对误差阈值 $\\varepsilon_{\\mathrm{mean}} = 0.01$、$\\varepsilon_{\\mathrm{cdf}} = 0.005$ 和 $\\varepsilon_{\\mathrm{q}} = 0.02$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    # Global Monte Carlo settings\n    N = 200000\n    seed = 20231115\n    E_MEAN = 0.01\n    E_CDF = 0.005\n    E_Q = 0.02\n\n    rng = np.random.default_rng(seed)\n\n    def estimate_functionals(theta_samples, weights):\n        \"\"\"\n        Computes posterior mean, CDF, and quantile from weighted samples.\n        \"\"\"\n        if np.sum(weights) == 0:\n            # This can happen if all weights underflow to zero\n            return np.nan, lambda x: np.nan, lambda alpha: np.nan\n            \n        # 1. Normalize weights\n        norm_weights = weights / np.sum(weights)\n\n        # 2. Estimate posterior mean\n        est_mean = np.sum(norm_weights * theta_samples)\n\n        # 3. Estimator for posterior CDF\n        def est_cdf_func(x):\n            return np.sum(norm_weights * (theta_samples <= x))\n\n        # 4. Estimator for posterior quantile\n        def est_quantile_func(alpha):\n            sort_indices = np.argsort(theta_samples)\n            sorted_theta = theta_samples[sort_indices]\n            sorted_weights = norm_weights[sort_indices]\n            cum_weights = np.cumsum(sorted_weights)\n            # Find the first index where cumulative weight is >= alpha\n            quantile_index = np.searchsorted(cum_weights, alpha, side='left')\n            # Handle edge case where alpha is 1 or very close\n            if quantile_index >= len(sorted_theta):\n                return sorted_theta[-1]\n            return sorted_theta[quantile_index]\n\n        return est_mean, est_cdf_func, est_quantile_func\n\n    # --- Test Case A: Gaussian-Gaussian ---\n    def run_case_A():\n        # Parameters\n        mu0, tau0 = 0.0, 2.0\n        n, y_bar, sigma = 30.0, 0.25, 1.0\n        x0, alpha0 = 0.0, 0.95\n\n        # Closed-form posterior parameters\n        v_post = 1.0 / (n / sigma**2 + 1.0 / tau0**2)\n        mu_post = v_post * (n * y_bar / sigma**2 + mu0 / tau0**2)\n\n        # Proposal parameters\n        m_q = mu_post + 0.5\n        s_q = np.sqrt(2.0 * v_post)\n\n        # Ground truth using closed-form posterior\n        posterior_dist = stats.norm(loc=mu_post, scale=np.sqrt(v_post))\n        true_mean = posterior_dist.mean()\n        true_cdf = posterior_dist.cdf(x0)\n        true_quantile = posterior_dist.ppf(alpha0)\n\n        # Unnormalized log-posterior kernel\n        def log_f(theta):\n            log_prior = stats.norm.logpdf(theta, loc=mu0, scale=tau0)\n            # We use the sufficient statistic for the log-likelihood\n            log_lik = -n * (theta - y_bar)**2 / (2 * sigma**2)\n            return log_lik + log_prior\n\n        # Sample from proposal\n        proposal_dist = stats.norm(loc=m_q, scale=s_q)\n        theta_samples = proposal_dist.rvs(size=N, random_state=rng)\n\n        # Compute importance weights in log-space for stability\n        log_weights = log_f(theta_samples) - proposal_dist.logpdf(theta_samples)\n        \n        # Stabilize weights by subtracting the max log-weight before exponentiating\n        stable_log_weights = log_weights - np.max(log_weights)\n        weights = np.exp(stable_log_weights)\n\n        # Estimate functionals\n        est_mean, est_cdf_func, est_quantile_func = estimate_functionals(theta_samples, weights)\n        est_cdf = est_cdf_func(x0)\n        est_quantile = est_quantile_func(alpha0)\n\n        # Compare and return booleans\n        mean_ok = np.abs(est_mean - true_mean) <= E_MEAN\n        cdf_ok = np.abs(est_cdf - true_cdf) <= E_CDF\n        q_ok = np.abs(est_quantile - true_quantile) <= E_Q\n        \n        return [mean_ok, cdf_ok, q_ok]\n\n    # --- Test Case B: Poisson-Gamma ---\n    def run_case_B():\n        # Parameters\n        a, b = 2.0, 1.0\n        n, sum_y = 40.0, 55.0\n        x1, alpha1 = 1.2, 0.5\n\n        # Closed-form posterior parameters\n        a_prime = a + sum_y\n        b_prime = b + n\n\n        # Proposal parameters\n        a_tilde, b_tilde = 50.0, 35.0\n\n        # Ground truth using closed-form posterior\n        posterior_dist = stats.gamma(a=a_prime, scale=1.0/b_prime)\n        true_mean = posterior_dist.mean()\n        true_cdf = posterior_dist.cdf(x1)\n        true_quantile = posterior_dist.ppf(alpha1)\n\n        # Unnormalized log-posterior kernel\n        def log_f(theta):\n            # Using logsumexp trick is not necessary here, but checking for theta > 0\n            # is important for log.\n            with np.errstate(divide='ignore'):\n                log_prior = stats.gamma.logpdf(theta, a=a, scale=1.0/b)\n                log_lik = sum_y * np.log(theta) - n * theta\n            # Replace -inf from log(0) with a very small number to avoid issues\n            log_prior[theta <= 0] = -np.inf\n            log_lik[theta <= 0] = -np.inf\n            return log_lik + log_prior\n\n        # Sample from proposal\n        proposal_dist = stats.gamma(a=a_tilde, scale=1.0/b_tilde)\n        theta_samples = proposal_dist.rvs(size=N, random_state=rng)\n        \n        # Compute importance weights\n        log_weights = log_f(theta_samples) - proposal_dist.logpdf(theta_samples)\n        stable_log_weights = log_weights - np.nanmax(log_weights)\n        weights = np.exp(stable_log_weights)\n\n        # Estimate functionals\n        est_mean, est_cdf_func, est_quantile_func = estimate_functionals(theta_samples, weights)\n        est_cdf = est_cdf_func(x1)\n        est_quantile = est_quantile_func(alpha1)\n        \n        # Compare and return booleans\n        mean_ok = np.abs(est_mean - true_mean) <= E_MEAN\n        cdf_ok = np.abs(est_cdf - true_cdf) <= E_CDF\n        q_ok = np.abs(est_quantile - true_quantile) <= E_Q\n        \n        return [mean_ok, cdf_ok, q_ok]\n\n    # --- Test Case C: Bernoulli-Beta ---\n    def run_case_C():\n        # Parameters\n        alpha_prior, beta_prior = 0.5, 0.5\n        n, sum_y = 80.0, 20.0\n        x2, alpha2 = 0.3, 0.95\n\n        # Closed-form posterior parameters\n        alpha_prime = alpha_prior + sum_y\n        beta_prime = beta_prior + n - sum_y\n\n        # Proposal parameters\n        alpha_tilde, beta_tilde = 1.0, 1.5\n\n        # Ground truth using closed-form posterior\n        posterior_dist = stats.beta(a=alpha_prime, b=beta_prime)\n        true_mean = posterior_dist.mean()\n        true_cdf = posterior_dist.cdf(x2)\n        true_quantile = posterior_dist.ppf(alpha2)\n\n        # Unnormalized log-posterior kernel\n        def log_f(theta):\n            with np.errstate(divide='ignore'):\n                log_prior = stats.beta.logpdf(theta, a=alpha_prior, b=beta_prior)\n                log_lik = sum_y * np.log(theta) + (n - sum_y) * np.log(1 - theta)\n            log_prior[~np.isfinite(log_prior)] = -np.inf\n            log_lik[~np.isfinite(log_lik)] = -np.inf\n            return log_lik + log_prior\n            \n        # Sample from proposal\n        proposal_dist = stats.beta(a=alpha_tilde, b=beta_tilde)\n        theta_samples = proposal_dist.rvs(size=N, random_state=rng)\n        \n        # Compute importance weights\n        log_weights = log_f(theta_samples) - proposal_dist.logpdf(theta_samples)\n        stable_log_weights = log_weights - np.nanmax(log_weights)\n        weights = np.exp(stable_log_weights)\n\n        # Estimate functionals\n        est_mean, est_cdf_func, est_quantile_func = estimate_functionals(theta_samples, weights)\n        est_cdf = est_cdf_func(x2)\n        est_quantile = est_quantile_func(alpha2)\n\n        # Compare and return booleans\n        mean_ok = np.abs(est_mean - true_mean) <= E_MEAN\n        cdf_ok = np.abs(est_cdf - true_cdf) <= E_CDF\n        q_ok = np.abs(est_quantile - true_quantile) <= E_Q\n\n        return [mean_ok, cdf_ok, q_ok]\n\n    # Run all test cases\n    results = []\n    results.extend(run_case_A())\n    results.extend(run_case_B())\n    results.extend(run_case_C())\n\n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3306507"}]}