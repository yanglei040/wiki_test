{"hands_on_practices": [{"introduction": "在许多贝叶斯推断问题中，即使我们无法直接从后验分布中抽样，我们仍然希望能够估计其某些性质，例如参数的期望值。本实践将引导你使用一种基础而强大的蒙特卡洛方法——重要性采样（Importance Sampling）——来解决这个问题。通过在一个我们已知精确解（共轭先验模型）的场景中应用该技术，你将亲手量化近似估计的准确性，特别是其在有限样本量下的偏差，这是评估和理解任何蒙特卡洛方法性能的基本功。 [@problem_id:3289041]", "problem": "给定一个由泊松似然建模的单次观测，其未知率参数服从Gamma先验分布。您的任务是从第一性原理出发，推导精确的后验分布，计算指定泛函的精确后验期望，然后使用对数正态提议分布量化自归一化重要性抽样（SNIS）近似的有限样本偏差。您的程序必须实现完整的流程，并为提供的测试套件生成一行输出，汇总偏差估计值。\n\n您可以使用的基本原理包括：\n- 贝叶斯定理：对于先验密度 $p(\\lambda)$、似然 $p(y \\mid \\lambda)$ 和后验 $p(\\lambda \\mid y)$，恒等式 $p(\\lambda \\mid y) \\propto p(y \\mid \\lambda) p(\\lambda)$ 成立。\n- 泊松概率质量函数：$p(y \\mid \\lambda) = \\exp(-\\lambda) \\lambda^{y} / y!$，其中 $y \\in \\{0,1,2,\\dots\\}$ 且 $\\lambda > 0$。\n- 形状-率参数化的Gamma概率密度函数：$p(\\lambda \\mid \\alpha, \\beta) = \\beta^{\\alpha} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda) / \\Gamma(\\alpha)$，其中 $\\alpha > 0$, $\\beta > 0$ 且 $\\lambda > 0$。\n- 参数为 $(\\mu, \\sigma)$ 的对数正态概率密度函数：$q(\\lambda \\mid \\mu, \\sigma) = \\left[ \\lambda \\sigma \\sqrt{2 \\pi} \\right]^{-1} \\exp\\left( - \\frac{(\\log \\lambda - \\mu)^2}{2 \\sigma^2} \\right)$，其中 $\\sigma > 0$ 且 $\\lambda > 0$。\n\n需要完成的任务：\n1) 仅从贝叶斯定理以及泊松和Gamma分布的定义出发，为一个单一观测 $y \\in \\{0,1,2,\\dots\\}$、先验 $\\lambda \\sim \\mathrm{Gamma}(\\alpha, \\beta)$（形状参数为 $\\alpha$，率参数为 $\\beta$）以及似然 $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$ 的模型，推导精确的后验分布 $p(\\lambda \\mid y, \\alpha, \\beta)$。实现一个函数，以相同的形状-率形式返回后验参数。\n2) 对于步骤1中的后验分布，考虑泛函 $h_1(\\lambda) = \\lambda$ 和 $h_2(\\lambda) = \\log \\lambda$。使用可被视为已充分验证的Gamma分布性质，计算在精确后验下的精确期望 $\\mathbb{E}[h_1(\\lambda) \\mid y, \\alpha, \\beta]$ 和 $\\mathbb{E}[h_2(\\lambda) \\mid y, \\alpha, \\beta]$。\n3) 使用对数正态提议分布 $q(\\lambda \\mid \\mu, \\sigma)$ 实现自归一化重要性抽样（SNIS）来近似 $\\mathbb{E}[h(\\lambda)]$，其中 $h \\in \\{h_1, h_2\\}$。给定从 $q$ 中抽取的 $N$ 个独立同分布（IID）的样本 $\\{\\lambda_i\\}_{i=1}^N$，使用精确到乘法常数的后验密度形成未归一化的重要性权重 $w_i \\propto \\frac{p(\\lambda_i \\mid y, \\alpha, \\beta)}{q(\\lambda_i \\mid \\mu, \\sigma)}$，将权重归一化 $\\tilde{w}_i = \\frac{w_i}{\\sum_{j=1}^N w_j}$，并计算SNIS估计值 $\\hat{I}_N(h) = \\sum_{i=1}^N \\tilde{w}_i h(\\lambda_i)$。在对数域中进行数值稳定的权重计算。\n4) 通过蒙特卡洛复制来量化SNIS估计器对 $h_1$ 和 $h_2$ 的有限样本偏差。具体来说，通过 $R$ 次SNIS过程的IID复制，计算经验偏差估计 $\\widehat{\\mathrm{Bias}}_R(h) = \\left( \\frac{1}{R} \\sum_{r=1}^R \\hat{I}_{N,r}(h) \\right) - \\mathbb{E}[h(\\lambda) \\mid y, \\alpha, \\beta]$，其中 $\\hat{I}_{N,r}(h)$ 是第 $r$ 次复制中的SNIS估计值。使用基础随机种子 $s_0$，对于测试用例索引 $k \\in \\{0,1,2\\}$，使用种子 $s_0 + k$ 来初始化该测试用例的随机数生成器。所有随机性必须从由 $(\\mu, \\sigma)$ 参数化的对数正态分布（即提议分布 $q$）生成。\n5) 实现以下测试套件。对于每个测试用例，按顺序 $[ \\widehat{\\mathrm{Bias}}_R(h_1), \\widehat{\\mathrm{Bias}}_R(h_2) ]$ 计算两个经验偏差，并按用例1、用例2、用例3的顺序将它们聚合到一个扁平化的列表中。\n- 使用 $N = 2000$ 和 $R = 400$。\n- 使用基础种子 $s_0 = 20251010$。\n- 测试用例1：$y = 12$, $\\alpha = 2.5$, $\\beta = 1.3$, $\\mu = \\log\\left( \\frac{\\alpha + y}{\\beta + 1} \\right)$, $\\sigma = 0.7$。\n- 测试用例2：$y = 1$, $\\alpha = 0.6$, $\\beta = 0.2$, $\\mu = -0.5$, $\\sigma = 1.1$。\n- 测试用例3：$y = 100$, $\\alpha = 10.0$, $\\beta = 5.0$, $\\mu = \\log\\left( \\frac{\\alpha + y}{\\beta + 1} \\right) - 0.5$, $\\sigma = 0.5$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须包含6个浮点数，顺序为 $[\\widehat{\\mathrm{Bias}}_R(h_1)\\_\\mathrm{case1}, \\widehat{\\mathrm{Bias}}_R(h_2)\\_\\mathrm{case1}, \\widehat{\\mathrm{Bias}}_R(h_1)\\_\\mathrm{case2}, \\widehat{\\mathrm{Bias}}_R(h_2)\\_\\mathrm{case2}, \\widehat{\\mathrm{Bias}}_R(h_1)\\_\\mathrm{case3}, \\widehat{\\mathrm{Bias}}_R(h_2)\\_\\mathrm{case3}]$。", "solution": "我们已严格审查问题陈述，并确定其有效。该问题具有科学依据，定义明确，客观，自洽且一致。所有给定的数据和定义都清晰且足以推导出唯一且有意义的解。此问题是贝叶斯推断和蒙特卡洛模拟中的一个标准且全面的练习。我们现在开始求解。\n\n### 步骤1：后验分布的推导\n\n该模型由单次观测 $y$ 的泊松似然和率参数 $\\lambda$ 的Gamma先验定义。\n先验分布为 $\\lambda \\sim \\mathrm{Gamma}(\\alpha, \\beta)$，其概率密度函数（PDF）为：\n$$\np(\\lambda \\mid \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda}\n$$\n一次观测 $y$ 的似然为 $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$，其概率质量函数（PMF）为：\n$$\np(y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{y}}{y!}\n$$\n根据贝叶斯定理，后验分布 $p(\\lambda \\mid y, \\alpha, \\beta)$ 正比于似然与先验的乘积：\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto p(y \\mid \\lambda) p(\\lambda \\mid \\alpha, \\beta)\n$$\n代入似然和先验的函数形式，我们得到：\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\left( \\frac{e^{-\\lambda} \\lambda^{y}}{y!} \\right) \\left( \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda} \\right)\n$$\n我们可以将所有不依赖于 $\\lambda$ 的项归入比例常数中。这些项是 $\\frac{1}{y!}$、$\\beta^{\\alpha}$ 和 $\\frac{1}{\\Gamma(\\alpha)}$。合并作为 $\\lambda$ 函数的项：\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\lambda^{y} e^{-\\lambda} \\cdot \\lambda^{\\alpha - 1} e^{-\\beta \\lambda}\n$$\n利用指数的性质，我们合并 $\\lambda$ 的幂和指数函数的参数：\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\lambda^{y + \\alpha - 1} e^{-(\\lambda + \\beta\\lambda)}\n$$\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\lambda^{(\\alpha + y) - 1} e^{-(\\beta + 1)\\lambda}\n$$\n这个结果表达式是Gamma分布的核。通过观察，我们可以识别出这个后验Gamma分布的新参数。设后验参数为 $\\alpha'$ 和 $\\beta'$。我们可以看到：\n$$\n\\alpha' = \\alpha + y\n$$\n$$\n\\beta' = \\beta + 1\n$$\n因此，给定观测 $y$ 和先验参数 $\\alpha$ 和 $\\beta$，$\\lambda$ 的后验分布是一个更新了参数的Gamma分布：\n$$\n\\lambda \\mid y, \\alpha, \\beta \\sim \\mathrm{Gamma}(\\alpha' = \\alpha + y, \\beta' = \\beta + 1)\n$$\n这证明了Gamma先验与泊松似然的共轭性。\n\n### 步骤2：精确后验期望的计算\n\n在确定后验分布为 $\\lambda \\mid y \\sim \\mathrm{Gamma}(\\alpha', \\beta')$ 后，我们可以计算指定泛函 $h_1(\\lambda) = \\lambda$ 和 $h_2(\\lambda) = \\log\\lambda$ 的精确期望。\n\n对于一个随机变量 $X \\sim \\mathrm{Gamma}(k, \\theta)$，其形状参数为 $k$，尺度参数为 $\\theta$，均值为 $\\mathbb{E}[X] = k\\theta$。在我们的形状-率参数化中，率参数为 $\\beta_p = 1/\\theta$，均值为 $\\mathbb{E}[X] = k/\\beta_p$。\n对于我们的后验分布 $\\mathrm{Gamma}(\\alpha', \\beta')$，$h_1(\\lambda) = \\lambda$ 的期望为：\n$$\n\\mathbb{E}[h_1(\\lambda) \\mid y, \\alpha, \\beta] = \\mathbb{E}[\\lambda \\mid y] = \\frac{\\alpha'}{\\beta'} = \\frac{\\alpha + y}{\\beta + 1}\n$$\n对于泛函 $h_2(\\lambda) = \\log\\lambda$，一个服从Gamma分布的变量 $X \\sim \\mathrm{Gamma}(\\alpha_p, \\beta_p)$ 的期望由 $\\mathbb{E}[\\log X] = \\psi(\\alpha_p) - \\log(\\beta_p)$ 给出，其中 $\\psi(\\cdot)$ 是双伽玛函数，定义为伽玛函数的对数导数，$\\psi(z) = \\frac{d}{dz}\\log\\Gamma(z)$。\n将此应用于我们的后验分布：\n$$\n\\mathbb{E}[h_2(\\lambda) \\mid y, \\alpha, \\beta] = \\mathbb{E}[\\log \\lambda \\mid y] = \\psi(\\alpha') - \\log(\\beta') = \\psi(\\alpha + y) - \\log(\\beta + 1)\n$$\n\n### 步骤3：自归一化重要性抽样（SNIS）\n\n目标是使用一个提议分布 $q(\\lambda)$ 来估计后验期望 $\\mathbb{E}[h(\\lambda) \\mid y] = \\frac{\\int h(\\lambda) p(y|\\lambda)p(\\lambda) d\\lambda}{\\int p(y|\\lambda)p(\\lambda) d\\lambda}$。令 $\\tilde{p}(\\lambda) = p(y|\\lambda)p(\\lambda)$ 为未归一化的后验。\n基于从提议分布 $q(\\lambda)$ 中抽取的 $N$ 个样本 $\\{\\lambda_i\\}_{i=1}^N$ 的泛函 $h(\\lambda)$ 的SNIS估计器为：\n$$\n\\hat{I}_N(h) = \\sum_{i=1}^{N} \\tilde{w}_i h(\\lambda_i)\n$$\n其中归一化权重 $\\tilde{w}_i$ 由以下公式给出：\n$$\n\\tilde{w}_i = \\frac{w_i}{\\sum_{j=1}^{N} w_j} \\quad \\text{其中原始权重为} \\quad w_i = \\frac{\\tilde{p}(\\lambda_i)}{q(\\lambda_i)}\n$$\n我们的目标分布（未归一化的后验）是 $\\tilde{p}(\\lambda) \\propto \\lambda^{\\alpha' - 1} e^{-\\beta'\\lambda}$，而我们的提议分布是对数正态分布 $q(\\lambda \\mid \\mu, \\sigma)$。\n为了防止数值下溢或上溢，计算在对数域中进行。原始权重的对数为：\n$$\n\\log w_i = \\log \\tilde{p}(\\lambda_i) - \\log q(\\lambda_i)\n$$\n我们只需要目标密度的核，所以我们使用 $\\log \\tilde{p}_{\\text{kernel}}(\\lambda) = (\\alpha' - 1)\\log\\lambda - \\beta'\\lambda$。提议分布PDF的对数为 $\\log q(\\lambda \\mid \\mu, \\sigma) = -\\log\\lambda - \\log\\sigma - \\frac{1}{2}\\log(2\\pi) - \\frac{(\\log\\lambda - \\mu)^2}{2\\sigma^2}$。所以，\n$$\n\\log w_i = \\left( (\\alpha' - 1)\\log\\lambda_i - \\beta'\\lambda_i \\right) - \\left( -\\log\\lambda_i - \\log\\sigma - \\frac{1}{2}\\log(2\\pi) - \\frac{(\\log\\lambda_i - \\mu)^2}{2\\sigma^2} \\right)\n$$\n为了归一化，我们使用log-sum-exp技巧。令 $L_i = \\log w_i$ 和 $L_{\\max} = \\max_i \\{L_i\\}$。归一化权重为：\n$$\n\\tilde{w}_i = \\frac{e^{L_i}}{\\sum_{j=1}^N e^{L_j}} = \\frac{e^{L_i - L_{\\max}}}{\\sum_{j=1}^N e^{L_j - L_{\\max}}}\n$$\n这种稳定的计算方法可以避免浮点误差。\n\n### 步骤4：量化有限样本偏差\n\nSNIS估计器对于有限的 $N$ 通常是有偏的。偏差定义为 $\\mathrm{Bias}(\\hat{I}_N(h)) = \\mathbb{E}[\\hat{I}_N(h)] - \\mathbb{E}[h(\\lambda) \\mid y]$。我们使用蒙特卡洛模拟来估计这个偏差。通过生成 $R$ 个独立的SNIS估计的复制品 $\\{\\hat{I}_{N,r}(h)\\}_{r=1}^R$，我们可以用样本均值 $\\frac{1}{R}\\sum_{r=1}^R \\hat{I}_{N,r}(h)$ 来近似期望 $\\mathbb{E}[\\hat{I}_N(h)]$。经验偏差则为：\n$$\n\\widehat{\\mathrm{Bias}}_R(h) = \\left( \\frac{1}{R} \\sum_{r=1}^R \\hat{I}_{N,r}(h) \\right) - \\mathbb{E}[h(\\lambda) \\mid y]\n$$\n此过程将为问题中指定的每个测试用例的泛函 $h_1(\\lambda)$ 和 $h_2(\\lambda)$ 实现。对于测试用例 $k$，随机数生成器使用种子 $s_0 + k$ 以确保可复现性。", "answer": "```python\nimport numpy as np\nfrom scipy.special import digamma\n\ndef solve():\n    \"\"\"\n    Implements the full pipeline to derive posterior, compute exact expectations,\n    and quantify the finite-sample bias of a Self-Normalized Importance Sampling\n    (SNIS) approximation for a Poisson-Gamma model.\n    \"\"\"\n\n    # Global parameters from the problem statement\n    N = 2000\n    R = 400\n    s0 = 20251010\n\n    def calculate_biases(y, alpha, beta, mu, sigma, seed):\n        \"\"\"\n        Calculates the SNIS bias for a single test case.\n\n        Args:\n            y (int): The observed Poisson count.\n            alpha (float): The shape parameter of the Gamma prior.\n            beta (float): The rate parameter of the Gamma prior.\n            mu (float): The mean parameter of the Lognormal proposal (on the log scale).\n            sigma (float): The standard deviation of the Lognormal proposal (on the log scale).\n            seed (int): The random seed for this test case.\n\n        Returns:\n            A tuple (bias_h1, bias_h2) containing the empirical biases for\n            h1(lambda) = lambda and h2(lambda) = log(lambda).\n        \"\"\"\n        # Set the seed for this specific test case to ensure reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Derive posterior parameters\n        # Posterior is Gamma(alpha', beta')\n        alpha_post = alpha + y\n        beta_post = beta + 1\n\n        # Step 2: Compute exact posterior expectations\n        exact_exp_h1 = alpha_post / beta_post\n        exact_exp_h2 = digamma(alpha_post) - np.log(beta_post)\n\n        # Accumulators for the mean of the SNIS estimates over R replicates\n        total_snis_h1 = 0.0\n        total_snis_h2 = 0.0\n\n        # Step 4: Quantify bias via Monte Carlo replication\n        for _ in range(R):\n            # Step 3: Implement one replicate of the SNIS estimator\n\n            # Draw N samples from the Lognormal proposal distribution q(lambda | mu, sigma)\n            # numpy.random.lognormal uses mu and sigma of the underlying normal distribution.\n            lambda_samples = rng.lognormal(mean=mu, sigma=sigma, size=N)\n            log_lambda_samples = np.log(lambda_samples)\n\n            # Calculate log of the unnormalized posterior (target) density kernel\n            # log p(lambda|y) \\propto (alpha_post - 1) * log(lambda) - beta_post * lambda\n            log_target_unnorm = (alpha_post - 1) * log_lambda_samples - beta_post * lambda_samples\n\n            # Calculate log of the Lognormal proposal density\n            # log q(lambda) = -log(lambda) - log(sigma) - 0.5*log(2*pi) - (log(lambda)-mu)^2 / (2*sigma^2)\n            log_proposal = -log_lambda_samples - np.log(sigma) - 0.5 * np.log(2 * np.pi) - \\\n                           (log_lambda_samples - mu)**2 / (2 * sigma**2)\n\n            # Calculate log of the unnormalized importance weights\n            log_weights = log_target_unnorm - log_proposal\n\n            # Normalize weights in a numerically stable way (log-sum-exp trick)\n            # This prevents underflow/overflow when exponentiating.\n            log_weights_max = np.max(log_weights)\n            weights = np.exp(log_weights - log_weights_max)\n            normalized_weights = weights / np.sum(weights)\n\n            # Compute the SNIS estimates for this replicate for h1 and h2\n            snis_h1 = np.sum(normalized_weights * lambda_samples)\n            snis_h2 = np.sum(normalized_weights * log_lambda_samples)\n\n            # Accumulate the estimates\n            total_snis_h1 += snis_h1\n            total_snis_h2 += snis_h2\n\n        # Calculate the average SNIS estimates over all R replicates\n        avg_snis_h1 = total_snis_h1 / R\n        avg_snis_h2 = total_snis_h2 / R\n\n        # Compute the final empirical bias estimates\n        bias_h1 = avg_snis_h1 - exact_exp_h1\n        bias_h2 = avg_snis_h2 - exact_exp_h2\n\n        return bias_h1, bias_h2\n\n    # Step 5: Implement the test suite\n    test_cases_params = [\n        # Test Case 1\n        {'y': 12, 'alpha': 2.5, 'beta': 1.3, 'mu_func': lambda a, y, b: np.log((a + y) / (b + 1)), 'sigma': 0.7},\n        # Test Case 2\n        {'y': 1, 'alpha': 0.6, 'beta': 0.2, 'mu_func': lambda a, y, b: -0.5, 'sigma': 1.1},\n        # Test Case 3\n        {'y': 100, 'alpha': 10.0, 'beta': 5.0, 'mu_func': lambda a, y, b: np.log((a + y) / (b + 1)) - 0.5, 'sigma': 0.5},\n    ]\n\n    all_biases = []\n    for i, params in enumerate(test_cases_params):\n        y_val = params['y']\n        alpha_val = params['alpha']\n        beta_val = params['beta']\n        mu_val = params['mu_func'](alpha_val, y_val, beta_val)\n        sigma_val = params['sigma']\n        seed_val = s0 + i\n\n        bias_h1, bias_h2 = calculate_biases(y_val, alpha_val, beta_val, mu_val, sigma_val, seed_val)\n        all_biases.extend([bias_h1, bias_h2])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_biases))}]\")\n\nsolve()\n```", "id": "3289041"}, {"introduction": "许多近似方法（如拉普拉斯近似）的理论基石是后验分布在数据量充足时会趋近于正态分布，这一性质被称为伯恩斯坦-冯·米塞斯定理（Bernstein-von Mises theorem）。然而，这一定理并非普遍成立。本实践将通过构建一个经典的反例，让你亲手诊断并理解该定理失效的情景，从而深刻体会其成立所需的“正则性条件”。通过计算并比较正态与非正态后验的形状诊断指标，你将对近似方法背后的理论假设建立起更深刻的直觉。 [@problem_id:3289088]", "problem": "考虑在一个非正则模型与一个正则基线模型的对比中，诊断 Bernstein–von Mises theorem（后验分布的渐近正态性）失效的任务。\n\n请从第一性原理出发：用于后验构造的贝叶斯法则、似然函数的定义以及基本的概率变换。不要先验地假设任何渐近正态性结果。\n\n您的目标是实现一个程序，该程序针对一小组测试用例，构建后验分布，从中生成独立同分布的样本，并计算定量诊断指标，以揭示即使在样本量很大时，后验分布的形状是接近高斯分布还是明显非高斯分布。\n\n模型和任务：\n\n1) 非正则模型（支撑集依赖于参数）：\n- 数据模型：对于参数 $\\theta \\in (0, b)$，其中 $b > 0$ 是固定且已知的，观测值 $X_1, \\dots, X_n$ 是独立同分布的，且 $X_i \\mid \\theta \\sim \\mathrm{Uniform}(0,\\theta)$。\n- 先验分布：$\\pi(\\theta)$ 在 $(0, b)$ 上为常数，在区间外为零。\n- 使用的基本原理：贝叶斯法则以及独立同分布观测值似然函数的基本形式。\n- 任务：\n  - 使用贝叶斯法则推导后验密度 $\\pi(\\theta \\mid x_{1:n})$。\n  - 证明 $\\pi(\\theta \\mid x_{1:n})$ 仅通过样本最大值 $X_{(n)} = \\max_i X_i$ 依赖于数据。\n  - 设计并论证一种方法，通过一个解析推导出的逆累积分布函数，变换独立的 $\\mathrm{Uniform}(0,1)$ 变量，从而从 $\\pi(\\theta \\mid x_{1:n})$ 中生成独立同分布的样本。您的方法必须是精确的（无马尔可夫链蒙特卡洛）。\n  - 使用您的采样器，生成 $S$ 个独立同分布的后验样本，使用经验后验均值 $\\hat{\\mu}$ 和经验后验标准差 $\\hat{\\sigma}$ 将它们标准化为零均值和单位方差，即计算 $Z_j = (\\Theta_j - \\hat{\\mu})/\\hat{\\sigma}$，并计算以下两者：\n    - 标准化的三阶中心矩（偏度），定义为 $Z_j^3$ 的经验平均值；\n    - $\\{Z_j\\}$ 的经验累积分布函数与标准正态累积分布函数之间的 Kolmogorov–Smirnov 上确界范数距离，即 $\\sup_z \\lvert \\hat{F}_S(z) - \\Phi(z) \\rvert$。\n  - 在您的解决方案中解释为什么这个模型是后验渐近正态性的一个反例，以及为什么即使对于大的 $n$，上述两个诊断指标仍然远离其高斯理想值。\n\n2) 正则基线模型：\n- 数据模型：对于参数 $\\theta \\in \\mathbb{R}$，观测值 $Y_1, \\dots, Y_n$ 是独立同分布的，且 $Y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, 1)$。\n- 先验分布：$\\theta \\sim \\mathcal{N}(0, \\tau^2)$，其中 $\\tau^2$ 是一个很大但有限的值。\n- 使用的基本原理：针对高斯似然的共轭高斯先验的贝叶斯法则。\n- 任务：\n  - 推导后验分布 $\\pi(\\theta \\mid y_{1:n})$。\n  - 以封闭形式从此后验分布中生成 $S$ 个独立同分布的样本。\n  - 使用经验后验均值和经验后验标准差将其标准化为零均值和单位方差，并计算与上述相同的两个诊断指标。\n\n需计算并返回的诊断指标：\n- 对于下面的每个测试用例，返回两个数字：\n  - 标准化后验样本的经验偏度（标准化的三阶中心矩）；\n  - 标准化后与标准正态累积分布函数的 Kolmogorov–Smirnov 上确界范数距离。\n\n实现细节：\n- 仅使用下面提供的测试套件。对于所有随机模拟，请将伪随机数生成器的种子设置为 $20231011$ 以确保可复现性。\n- 对于所有后验模拟，使用 $S = 120000$ 个独立同分布的样本。\n- 对于 Kolmogorov–Smirnov 距离，通过在排序后的样本点上比较经验累积分布函数和标准正态累积分布函数，来计算标准化样本上的 $\\sup_z \\lvert \\hat{F}_S(z) - \\Phi(z) \\rvert$。\n\n测试套件：\n- 用例 1（非正则，大 $n$）：$\\theta^\\star = 1.0$, $n = 1000$, $b = 10.0$。\n- 用例 2（非正则，更大 $n$）：$\\theta^\\star = 1.0$, $n = 10000$, $b = 10.0$。\n- 用例 3（正则基线）：$\\theta^\\star = 1.0$, $n = 1000$, $\\tau^2 = 10^6$。\n\n数据生成：\n- 对于用例 1 和 2，独立同分布地生成数据 $X_i \\sim \\mathrm{Uniform}(0, \\theta^\\star)$；基于实现的 $X_{(n)}$ 构建后验分布。\n- 对于用例 3，独立同分布地生成数据 $Y_i \\sim \\mathcal{N}(\\theta^\\star, 1)$；基于实现的数据构建后验分布。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个测试用例的结果，形式为一个包含三对数值的逗号分隔列表，每对数值保留小数点后六位，并用方括号括起来。具体来说，打印一行格式如下：\n  - [[skew1,ks1],[skew2,ks2],[skew3,ks3]]\n- 不应打印任何额外文本。\n\n角度和物理单位：\n- 此问题中没有物理单位或角度。\n\n答案类型：\n- 每个报告的数字都是一个实数（浮点数）。最终输出是一个包含三个列表的单一列表，每个子列表包含两个浮点数，如上所述。\n\n您的程序必须是自包含的，并且不得要求任何用户输入。它必须遵守最终答案部分指定的执行环境。", "solution": "目标是分析两种不同模型的后验分布的渐近行为：一种是数据的支撑集依赖于参数的非正则模型，另一种是标准的正则模型。我们将推导后验分布，实现精确采样方法，并计算诊断指标（偏度和 Kolmogorov-Smirnov 距离）以检验正则模型下 Bernstein-von Mises (BvM) theorem 所预测的渐近正态性。\n\n### 模型 1：非正则均匀模型\n\n该模型是 BvM theorem 的一个典型反例。\n\n**1. 数据设定与先验分布**\n- 数据模型：$X_1, \\dots, X_n$ 是来自 $\\mathrm{Uniform}(0,\\theta)$ 分布的独立同分布 (i.i.d.) 抽样，即 $X_i \\mid \\theta \\sim \\mathcal{U}(0,\\theta)$。参数 $\\theta$ 未知，但被限制在区间 $(0, b)$ 内，其中 $b > 0$ 是一个已知的常数。\n- 先验分布：为 $\\theta$ 选择一个无信息先验，它在其支撑集上是常数：$\\pi(\\theta) \\propto \\mathbb{I}(0  \\theta  b)$。这是一个 $\\mathrm{Uniform}(0,b)$ 分布，因此先验密度为 $\\pi(\\theta) = \\frac{1}{b} \\mathbb{I}(0  \\theta  b)$，其中 $\\mathbb{I}(\\cdot)$ 是指示函数。\n\n**2. 后验分布的推导**\n我们使用贝叶斯法则，该法则指出后验密度与似然和先验的乘积成正比：\n$$\n\\pi(\\theta \\mid x_{1:n}) \\propto p(x_{1:n} \\mid \\theta) \\pi(\\theta)\n$$\n对于 i.i.d. 观测值，似然函数 $p(x_{1:n} \\mid \\theta)$ 是各个密度的乘积：\n$$\np(x_{1:n} \\mid \\theta) = \\prod_{i=1}^n p(x_i \\mid \\theta) = \\prod_{i=1}^n \\left( \\frac{1}{\\theta} \\mathbb{I}(0  x_i  \\theta) \\right)\n$$\n指示函数的乘积 $\\prod_{i=1}^n \\mathbb{I}(0  x_i  \\theta)$ 等于 $1$ 当且仅当所有的 $x_i$ 都小于 $\\theta$。这个条件可以用样本最大值 $X_{(n)} = \\max\\{X_1, \\dots, X_n\\}$ 来简洁地表示。该条件成立当且仅当 $X_{(n)}  \\theta$。我们假设所有 $x_i  0$。因此，似然函数为：\n$$\np(x_{1:n} \\mid \\theta) = \\left(\\frac{1}{\\theta}\\right)^n \\mathbb{I}(\\theta  X_{(n)}) = \\theta^{-n} \\mathbb{I}(\\theta  X_{(n)})\n$$\n如上所示，似然函数以及后验分布都仅通过单一的充分统计量 $X_{(n)}$ 依赖于数据 $x_{1:n}$。\n\n结合似然和先验，未归一化的后验分布为：\n$$\n\\pi(\\theta \\mid x_{1:n}) \\propto \\left( \\theta^{-n} \\mathbb{I}(\\theta  X_{(n)}) \\right) \\times \\left( \\frac{1}{b} \\mathbb{I}(0  \\theta  b) \\right)\n$$\n这可以简化为：\n$$\n\\pi(\\theta \\mid x_{1:n}) \\propto \\theta^{-n} \\mathbb{I}(X_{(n)}  \\theta  b)\n$$\n为了找到归一化的后验密度，我们计算归一化常数 $C$：\n$$\nC = \\int_{X_{(n)}}^b \\theta^{-n} d\\theta = \\left[ \\frac{\\theta^{-n+1}}{-n+1} \\right]_{X_{(n)}}^b = \\frac{1}{n-1} \\left( X_{(n)}^{-n+1} - b^{-n+1} \\right) \\quad (\\text{对于 } n1)\n$$\n精确的后验密度是一个截断的 Pareto 分布：\n$$\n\\pi(\\theta \\mid x_{1:n}) = \\frac{\\theta^{-n}}{C} = \\frac{(n-1)\\theta^{-n}}{X_{(n)}^{-n+1} - b^{-n+1}} \\mathbb{I}(X_{(n)}  \\theta  b)\n$$\n\n**3. 逆 CDF 采样方法**\n为了从此后验分布中生成精确的 i.i.d. 样本，我们使用逆变换采样法。首先，我们推导后验累积分布函数 (CDF)，$F(\\theta_0) = P(\\theta \\le \\theta_0 \\mid x_{1:n})$：\n$$\nF(\\theta_0) = \\int_{X_{(n)}}^{\\theta_0} \\pi(\\theta \\mid x_{1:n}) d\\theta = \\frac{1}{C} \\int_{X_{(n)}}^{\\theta_0} \\theta^{-n} d\\theta = \\frac{\\frac{1}{n-1}(X_{(n)}^{-n+1} - \\theta_0^{-n+1})}{\\frac{1}{n-1}(X_{(n)}^{-n+1} - b^{-n+1})} = \\frac{X_{(n)}^{-n+1} - \\theta_0^{-n+1}}{X_{(n)}^{-n+1} - b^{-n+1}}\n$$\n对于 $\\theta_0 \\in [X_{(n)}, b]$。我们令 $F(\\theta_0) = u$，其中 $u \\sim \\mathcal{U}(0,1)$，然后解出 $\\theta_0$：\n$$\nu = \\frac{X_{(n)}^{-n+1} - \\theta_0^{-n+1}}{X_{(n)}^{-n+1} - b^{-n+1}} \\implies u(X_{(n)}^{-n+1} - b^{-n+1}) = X_{(n)}^{-n+1} - \\theta_0^{-n+1}\n$$\n$$\n\\theta_0^{-n+1} = X_{(n)}^{-n+1} - u(X_{(n)}^{-n+1} - b^{-n+1}) = (1-u)X_{(n)}^{-n+1} + u b^{-n+1}\n$$\n$$\n\\theta_0 = \\left( (1-u)X_{(n)}^{-n+1} + u b^{-n+1} \\right)^{\\frac{1}{1-n}}\n$$\n这就是逆 CDF，$F^{-1}(u)$。一个用于计算的数值稳定形式可以通过提取 $X_{(n)}$ 因子得到：\n$$\n\\theta_0 = X_{(n)} \\left( 1 - u \\left(1 - \\left(\\frac{X_{(n)}}{b}\\right)^{n-1}\\right) \\right)^{\\frac{-1}{n-1}}\n$$\n生成一个均匀分布的随机变量 $u$ 并应用此变换，可以得到一个来自后验分布的精确样本。\n\n**4. Bernstein-von Mises Theorem 的失效**\nBvM theorem 指出，在某些“正则性条件”下，随着样本量 $n \\to \\infty$，后验分布的形状会收敛于高斯分布。一个关键的正则性条件是数据分布 $p(x|\\theta)$ 的支撑集不能依赖于参数 $\\theta$。在我们的模型中，支撑集是 $(0, \\theta)$，这违反了此条件。\n\n因此，后验分布不会变成高斯分布。当 $n \\to \\infty$ 时，$X_{(n)}$ 收敛于真实参数值 $\\theta^{\\star}$，而项 $b^{-n+1}$（其中 $b  X_{(n)}$）比 $X_{(n)}^{-n+1}$ 快得多地趋近于零。后验分布变得越来越集中在 $X_{(n)}$ 的正上方。密度 $\\pi(\\theta | x_{1:n}) \\propto \\theta^{-n}$ 在其支撑集的下界 $\\theta=X_{(n)}$ 处达到最大值，并急剧下降。这种形状高度不对称，类似于一个反向（且截断）的幂律分布。随着 $n$ 的增加，这种不对称性不会消失。对后验进行适当的重新缩放，例如 $n( \\theta - X_{(n)} )$，可以证明其收敛于指数分布，而非高斯分布。因此，旨在测量高斯性的诊断指标，如偏度（对于高斯分布为 $0$）和与正态 CDF 的 K-S 距离，将不会收敛到它们的理想值。它们反而会收敛到这种非高斯极限形状的特征值，即使对于非常大的 $n$ 也仍然远离高斯理想值。\n\n### 模型 2：正则高斯模型\n\n该模型满足 BvM 正则性条件，可作为比较的基线。\n\n**1. 数据设定与先验分布**\n- 数据模型：$Y_1, \\dots, Y_n$ 是来自 $\\mathcal{N}(\\theta, 1)$ 分布的 i.i.d. 抽样。\n- 先验分布：高斯分布均值的共轭先验是高斯分布。我们使用 $\\theta \\sim \\mathcal{N}(0, \\tau^2)$，其中方差 $\\tau^2$ 很大，以使其成为弱信息先验。\n\n**2. 后验分布的推导**\n我们再次使用贝叶斯法则：$\\pi(\\theta \\mid y_{1:n}) \\propto p(y_{1:n} \\mid \\theta) \\pi(\\theta)$。\n- 似然函数为：$p(y_{1:n} \\mid \\theta) \\propto \\exp\\left(-\\frac{1}{2} \\sum_{i=1}^n (y_i - \\theta)^2 \\right)$。\n- 先验分布为：$\\pi(\\theta) \\propto \\exp\\left(-\\frac{\\theta^2}{2\\tau^2}\\right)$。\n后验分布与两者的乘积成正比：\n$$\n\\pi(\\theta \\mid y_{1:n}) \\propto \\exp\\left(-\\frac{1}{2} \\sum_{i=1}^n (y_i - \\theta)^2 - \\frac{\\theta^2}{2\\tau^2}\\right)\n$$\n指数中的项是关于 $\\theta$ 的二次式：$\\sum(y_i - \\theta)^2 + \\frac{\\theta^2}{\\tau^2} = \\sum y_i^2 - 2\\theta \\sum y_i + n\\theta^2 + \\frac{\\theta^2}{\\tau^2}$。忽略不含 $\\theta$ 的项，我们关注于：\n$$\n-\\frac{1}{2} \\left[ \\left(n + \\frac{1}{\\tau^2}\\right)\\theta^2 - 2(n\\bar{y})\\theta \\right]\n$$\n其中 $\\bar{y} = \\frac{1}{n}\\sum y_i$。对 $\\theta$ 配方表明这是一个高斯密度的核。因此，$\\theta$ 的后验分布也是一个高斯分布，即 $\\mathcal{N}(\\mu_n, \\sigma_n^2)$，其参数为：\n$$\n\\sigma_n^2 = \\left(n + \\frac{1}{\\tau^2}\\right)^{-1} \\qquad \\mu_n = \\sigma_n^2 (n\\bar{y}) = \\frac{n\\bar{y}}{n + 1/\\tau^2}\n$$\n从此后验分布中采样是直接的：只需从具有计算出的均值 $\\mu_n$ 和方差 $\\sigma_n^2$ 的正态分布中生成抽样即可。由于这个后验分布本身就是完美的高斯分布，生成样本、将其标准化并与标准正态分布进行比较，应该会得到一个非常接近 $0$ 的偏度和一个非常小的 K-S 距离，其误差仅受限于有限后验抽样次数 $S$ 带来的蒙特卡洛误差。\n\n### 诊断指标计算\n\n对于两个模型，在生成 $S$ 个后验样本 $\\{\\Theta_j\\}_{j=1}^S$ 后，我们计算：\n1.  经验均值 $\\hat{\\mu} = \\frac{1}{S} \\sum_{j=1}^S \\Theta_j$ 和标准差 $\\hat{\\sigma} = \\sqrt{\\frac{1}{S-1} \\sum_{j=1}^S (\\Theta_j - \\hat{\\mu})^2}$。\n2.  标准化样本 $Z_j = (\\Theta_j - \\hat{\\mu})/\\hat{\\sigma}$。\n3.  **偏度**：标准化的三阶中心矩，$\\mathrm{Skew} = \\frac{1}{S} \\sum_{j=1}^S Z_j^3$。\n4.  **Kolmogorov-Smirnov 距离**：$D_S = \\sup_z \\lvert \\hat{F}_S(z) - \\Phi(z) \\rvert$，其中 $\\hat{F}_S$ 是 $\\{Z_j\\}$ 的经验 CDF，$\\Phi$ 是标准正态 CDF。这是通过在排序后的样本点上找到最大偏差来计算的。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the problem of diagnosing the failure of the Bernstein–von Mises theorem\n    by comparing a nonregular Uniform model with a regular Gaussian model.\n    \"\"\"\n    RNG_SEED = 20231011\n    S = 120000  # Number of posterior draws\n\n    rng = np.random.default_rng(RNG_SEED)\n\n    test_cases = [\n        {'model': 'nonregular', 'theta_star': 1.0, 'n': 1000, 'b': 10.0},\n        {'model': 'nonregular', 'theta_star': 1.0, 'n': 10000, 'b': 10.0},\n        {'model': 'regular', 'theta_star': 1.0, 'n': 1000, 'tau_sq': 1e6}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        if case['model'] == 'nonregular':\n            # Parameters for the nonregular case\n            theta_star = case['theta_star']\n            n = case['n']\n            b = case['b']\n\n            # 1. Generate data\n            # X_i ~ Uniform(0, theta_star)\n            data = rng.uniform(0, theta_star, n)\n            x_n_max = np.max(data)\n\n            # 2. Generate samples from the posterior using inverse CDF sampling\n            u = rng.uniform(0, 1, S)\n            \n            # Numerically stable inverse CDF formula:\n            # theta = x_n_max * (1 - u * (1 - (x_n_max/b)**(n-1)))**(-1/(n-1))\n            power_term = (x_n_max / b)**(n - 1)\n            base = 1 - u * (1 - power_term)\n            exponent = -1 / (n - 1)\n            theta_samples = x_n_max * (base**exponent)\n\n        elif case['model'] == 'regular':\n            # Parameters for the regular case\n            theta_star = case['theta_star']\n            n = case['n']\n            tau_sq = case['tau_sq']\n\n            # 1. Generate data\n            # Y_i ~ Normal(theta_star, 1)\n            data = rng.normal(theta_star, 1, n)\n            y_bar = np.mean(data)\n\n            # 2. Calculate posterior parameters (Normal-Normal conjugate model)\n            post_var = 1 / (n + 1 / tau_sq)\n            post_mean = post_var * (n * y_bar)\n            post_std = np.sqrt(post_var)\n\n            # 3. Generate samples from the posterior\n            theta_samples = rng.normal(post_mean, post_std, S)\n\n        # 4. Compute diagnostics for all cases\n        # Standardize samples to zero mean and unit variance\n        mu_hat = np.mean(theta_samples)\n        # Using ddof=0 for population standard deviation of the sample\n        sigma_hat = np.std(theta_samples) \n        z_samples = (theta_samples - mu_hat) / sigma_hat\n\n        # Compute empirical skewness (standardized third central moment)\n        skewness = np.mean(z_samples**3)\n\n        # Compute Kolmogorov-Smirnov distance\n        z_sorted = np.sort(z_samples)\n        \n        # Empirical CDF values at each sorted sample point\n        ecdf = np.arange(1, S + 1) / S\n        \n        # Standard normal CDF values at the same points\n        norm_cdf_vals = norm.cdf(z_sorted)\n\n        # The KS statistic is the max difference between ECDF and the true CDF.\n        # The difference can be maximal just before or at the ECDF jump points.\n        dist1 = np.abs(ecdf - norm_cdf_vals)\n        dist2 = np.abs((ecdf - 1/S) - norm_cdf_vals)\n        ks_dist = np.max(np.maximum(dist1, dist2))\n\n        results.append([skewness, ks_dist])\n\n    # Format the final output as specified\n    formatted_results = [f\"[{s:.6f},{k:.6f}]\" for s, k in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3289088"}, {"introduction": "在复杂的贝叶斯推断中，后验分布常常呈现出多个被低概率区域隔开的“峰”（即多峰性），这对马尔可夫链蒙特卡洛（MCMC）采样构成了巨大挑战。本实践将引入一个源自统计物理学的直观视角，将后验分布的负对数视为一个“能量函数”，从而将采样难题转化为在能量地貌上翻越“能垒”的物理过程。你将通过分析推导，量化模式之间的能垒高度，并探究温度调节（tempering）技术是如何通过“加热”系统来显著降低跨越能垒的期望时间，从而提升采样效率的。 [@problem_id:3289096]", "problem": "考虑一个由非高斯先验和高斯似然引导出双阱结构的一维贝叶斯后验。设先验由密度 $p(\\theta) \\propto \\exp\\left(-\\lambda(\\theta^2 - b^2)^2\\right)$ 定义，其中参数 $\\lambda  0$ 且 $b  0$。设观测数据 $y$ 的似然为 $p(y \\mid \\theta) = \\mathcal{N}(y; \\theta, s^2)$，其中方差 $s^2  0$ 已知。对于本问题，将观测数据固定为 $y = 0$。则未归一化的后验为 $p(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$，相关的能量函数为 $U(\\theta; y) = -\\log p(\\theta \\mid y)$，忽略不依赖于 $\\theta$ 的加性常数。\n\n定义温度为 $T \\geq 1$ 时的温度化后验为 $p_T(\\theta \\mid y) \\propto p(\\theta \\mid y)^{1/T}$，这对应于将能量按因子 $1/T$ 进行缩放，即 $U_T(\\theta; y) = U(\\theta; y)/T$。在马尔可夫链蒙特卡洛（MCMC）中，温度化是一种改善跨模态混合的方法。在本问题中，您必须使用 $-\\log p(\\theta \\mid y)$ 通过能量壁垒来量化多模态性，并计算温度化如何减少双阱后验中模态之间的期望到达时间。\n\n您的任务是：\n- 从后验 $p(\\theta \\mid y)$、能量 $U(\\theta; y)$ 和温度化后验 $p_T(\\theta \\mid y)$ 的定义出发，在双阱结构存在时，识别 $U(\\theta; y)$ 的两个对称最小值点和中心鞍点（局部最大值点）的位置。建立双阱结构存在时关于 $\\lambda$、$b$ 和 $s$ 的条件，并确定最小值点关于 $\\lambda$、$b$ 和 $s$ 的精确位置。\n- 将能量壁垒高度 $\\Delta E$ 定义为差值 $U(\\theta_{\\text{saddle}}; y) - U(\\theta_{\\text{min}}; y)$，其中 $\\theta_{\\text{saddle}}$ 是中心鞍点，$\\theta_{\\text{min}}$ 是两个对称最小值点中的任意一个。为给定的模型参数推导 $\\Delta E$ 的封闭形式表达式。\n- 使用基于原理的一维过阻尼动力学中稀有事件跃迁的近似（例如 Eyring-Kramers 原理），论证在温度 $T$ 下两个模态之间的期望到达时间 $\\tau(T)$ 由一个依赖于壁垒高度和温度的指数因子主导。推导由 $R(T) = \\tau(T)/\\tau(1)$ 定义的缩减因子 $R(T)$，并将其完全用 $\\Delta E$ 和 $T$ 表示。\n- 所有能量必须以与自然对数一致的无量纲单位报告（即与 $-\\log$ 密度相关的能量单位），所有时间也必须以无量纲单位报告。您必须为壁垒高度 $\\Delta E$ 和缩减因子 $R(T)$ 生成数值输出。\n\n实现一个程序，对于下面测试套件中的每个参数元组 $(\\lambda, b, s, T)$，计算：\n- 壁垒高度 $\\Delta E$。\n- 缩减因子 $R(T) = \\tau(T)/\\tau(1)$。\n\n如果给定参数元组不满足双阱条件，则为该元组返回序对 $[\\text{nan}, \\text{nan}]$。最终输出必须是单行，包含一个逗号分隔的浮点数列表的列表，每个内部列表对应一个测试用例，按下面给出的顺序排列，并用方括号括起来。例如，输出应具有形式 $[[\\Delta E_1, R(T)_1],[\\Delta E_2, R(T)_2],\\ldots]$。\n\n使用以下参数值测试套件来测试解的不同方面：\n- 模式良好分离的一般情况（理想路径）：$(\\lambda, b, s, T) = (1.0, 2.0, 1.0, 1.0)$。\n- 模式中度分离且进行非平凡温度化：$(\\lambda, b, s, T) = (1.5, 1.5, 0.8, 2.0)$。\n- 宽似然和浅阱，强温度化：$(\\lambda, b, s, T) = (0.8, 1.2, 1.5, 4.0)$。\n- 接近双阱存在边界：$(\\lambda, b, s, T) = (1.0, 0.6, 1.0, 3.0)$。\n- 深阱和极高温度化：$(\\lambda, b, s, T) = (2.0, 1.8, 0.7, 10.0)$。\n\n为覆盖率设计：\n- 第一个案例检查基线情况，其中 $T = 1$，温度化不改变到达时间。\n- 第二个和第三个案例探讨了增加 $T$ 如何针对不同壁垒高度减少到达时间。\n- 第四个案例检验了在多模态性消失附近的表现，以测试近似的稳定性。\n- 第五个案例探究了极端温度化的情况。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[[\\Delta E_1,R_1],[\\Delta E_2,R_2],[\\Delta E_3,R_3],[\\Delta E_4,R_4],[\\Delta E_5,R_5]]$）。", "solution": "该问题已经过验证，并被确定为计算统计学中一个适定且有科学依据的问题。它是自洽的，其前提和目标清晰、一致且可形式化。因此，我们可以继续进行完整求解。\n\n解答分为四个步骤：首先，我们从后验分布推导能量函数 $U(\\theta)$；其次，我们识别能量函数的临界点并建立双阱势的条件；第三，我们推导能量壁垒 $\\Delta E$ 的封闭形式表达式；第四，我们基于一个已建立的稀有事件动力学近似推导到达时间缩减因子 $R(T)$。\n\n### 步骤 1：能量函数的推导\n\n未归一化的后验分布 $p(\\theta \\mid y)$ 与先验 $p(\\theta)$ 和似然 $p(y \\mid \\theta)$ 的乘积成正比。\n先验密度给出为 $p(\\theta) \\propto \\exp\\left(-\\lambda(\\theta^2 - b^2)^2\\right)$，其中 $\\lambda  0$ 且 $b  0$。\n似然是一个高斯分布，$p(y \\mid \\theta) = \\mathcal{N}(y; \\theta, s^2)$，其已知方差 $s^2  0$。对于给定的数据 $y=0$，似然变为：\n$$\np(y=0 \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi s^2}} \\exp\\left(-\\frac{(0 - \\theta)^2}{2s^2}\\right) \\propto \\exp\\left(-\\frac{\\theta^2}{2s^2}\\right)\n$$\n未归一化的后验则为：\n$$\np(\\theta \\mid y=0) \\propto p(y=0 \\mid \\theta) p(\\theta) \\propto \\exp\\left(-\\frac{\\theta^2}{2s^2}\\right) \\exp\\left(-\\lambda(\\theta^2 - b^2)^2\\right)\n$$\n$$\np(\\theta \\mid y=0) \\propto \\exp\\left( -\\left[ \\frac{\\theta^2}{2s^2} + \\lambda(\\theta^2 - b^2)^2 \\right] \\right)\n$$\n能量函数 $U(\\theta; y)$ 定义为 $U(\\theta; y) = -\\log p(\\theta \\mid y)$，忽略任何不依赖于 $\\theta$ 的加性常数。因此，设 $y=0$ 并省去显式的依赖关系符号，我们得到：\n$$\nU(\\theta) = \\frac{\\theta^2}{2s^2} + \\lambda(\\theta^2 - b^2)^2\n$$\n展开四次项，我们可以将能量函数重写为：\n$$\nU(\\theta) = \\frac{\\theta^2}{2s^2} + \\lambda(\\theta^4 - 2b^2\\theta^2 + b^4) = \\lambda\\theta^4 + \\left(\\frac{1}{2s^2} - 2\\lambda b^2\\right)\\theta^2 + \\lambda b^4\n$$\n\n### 步骤 2：临界点与双阱条件\n\n能量函数 $U(\\theta)$ 的临界点（最小值点、最大值点和鞍点）在其关于 $\\theta$ 的一阶导数为零处找到。\n$$\n\\frac{dU}{d\\theta} = \\frac{d}{d\\theta} \\left[ \\frac{\\theta^2}{2s^2} + \\lambda(\\theta^2 - b^2)^2 \\right] = \\frac{2\\theta}{2s^2} + \\lambda \\cdot 2(\\theta^2 - b^2) \\cdot (2\\theta)\n$$\n$$\n\\frac{dU}{d\\theta} = \\frac{\\theta}{s^2} + 4\\lambda\\theta(\\theta^2 - b^2) = \\theta \\left[ \\frac{1}{s^2} + 4\\lambda(\\theta^2 - b^2) \\right] = 0\n$$\n$$\n\\theta \\left[ 4\\lambda\\theta^2 + \\frac{1}{s^2} - 4\\lambda b^2 \\right] = 0\n$$\n该方程产生三个临界点：\n1.  一个临界点在 $\\theta = 0$。\n2.  另外两个通过求解 $4\\lambda\\theta^2 + \\frac{1}{s^2} - 4\\lambda b^2 = 0$ 得到，这给出 $\\theta^2 = b^2 - \\frac{1}{4\\lambda s^2}$。这些解是实数当且仅当 $b^2  \\frac{1}{4\\lambda s^2}$。\n\n为了对这些点进行分类，我们检查二阶导数 $U''(\\theta)$：\n$$\nU''(\\theta) = \\frac{d}{d\\theta} \\left[ 4\\lambda\\theta^3 + \\left(\\frac{1}{s^2} - 4\\lambda b^2\\right)\\theta \\right] = 12\\lambda\\theta^2 + \\frac{1}{s^2} - 4\\lambda b^2\n$$\n在 $\\theta = 0$ 处，二阶导数为 $U''(0) = \\frac{1}{s^2} - 4\\lambda b^2$。为使 $\\theta=0$ 成为局部最大值点（一个代表阱间壁垒的鞍点），我们必须有 $U''(0)  0$。这给出了条件：\n$$\n\\frac{1}{s^2}  4\\lambda b^2 \\implies \\boldsymbol{4\\lambda b^2 s^2  1}\n$$\n这就是存在双阱势的条件。它与另外两个临界点为实数的条件相同。当此条件成立时，$\\theta_{\\text{saddle}} = 0$。\n\n现在我们对另外两个临界点 $\\theta^2 = b^2 - \\frac{1}{4\\lambda s^2}$ 进行分类。\n$$\nU''\\left(\\theta^2 = b^2 - \\frac{1}{4\\lambda s^2}\\right) = 12\\lambda\\left(b^2 - \\frac{1}{4\\lambda s^2}\\right) + \\frac{1}{s^2} - 4\\lambda b^2 = 12\\lambda b^2 - \\frac{3}{s^2} + \\frac{1}{s^2} - 4\\lambda b^2\n$$\n$$\n= 8\\lambda b^2 - \\frac{2}{s^2} = 2\\left(4\\lambda b^2 - \\frac{1}{s^2}\\right)\n$$\n根据双阱条件 $4\\lambda b^2 s^2  1$，我们有 $4\\lambda b^2  1/s^2$，这意味着 $4\\lambda b^2 - 1/s^2  0$。因此，在这些点上的二阶导数为正，确认它们是局部最小值点。这些最小值点的位置是：\n$$\n\\theta_{\\text{min}} = \\pm \\sqrt{b^2 - \\frac{1}{4\\lambda s^2}}\n$$\n\n### 步骤 3：能量壁垒 $\\Delta E$ 的推导\n\n能量壁垒 $\\Delta E$ 定义为鞍点能量与最小值点能量之差：$\\Delta E = U(\\theta_{\\text{saddle}}) - U(\\theta_{\\text{min}})$。\n鞍点 $\\theta_{\\text{saddle}} = 0$ 的能量是：\n$$\nU(0) = \\frac{0^2}{2s^2} + \\lambda(0^2 - b^2)^2 = \\lambda b^4\n$$\n最小值点的能量在 $\\theta_{\\text{min}}^2 = b^2 - \\frac{1}{4\\lambda s^2}$ 处计算：\n$$\nU(\\theta_{\\text{min}}) = \\frac{\\theta_{\\text{min}}^2}{2s^2} + \\lambda(\\theta_{\\text{min}}^2 - b^2)^2\n$$\n$$\nU(\\theta_{\\text{min}}) = \\frac{1}{2s^2}\\left(b^2 - \\frac{1}{4\\lambda s^2}\\right) + \\lambda\\left(\\left(b^2 - \\frac{1}{4\\lambda s^2}\\right) - b^2\\right)^2\n$$\n$$\nU(\\theta_{\\text{min}}) = \\frac{b^2}{2s^2} - \\frac{1}{8\\lambda s^4} + \\lambda\\left(-\\frac{1}{4\\lambda s^2}\\right)^2 = \\frac{b^2}{2s^2} - \\frac{1}{8\\lambda s^4} + \\frac{1}{16\\lambda s^4} = \\frac{b^2}{2s^2} - \\frac{1}{16\\lambda s^4}\n$$\n现在，我们计算能量壁垒 $\\Delta E$：\n$$\n\\Delta E = U(0) - U(\\theta_{\\text{min}}) = \\lambda b^4 - \\left(\\frac{b^2}{2s^2} - \\frac{1}{16\\lambda s^4}\\right) = \\lambda b^4 - \\frac{b^2}{2s^2} + \\frac{1}{16\\lambda s^4}\n$$\n这个表达式是一个完全平方：\n$$\n\\Delta E = \\lambda \\left(b^4 - \\frac{b^2}{2\\lambda s^2} + \\frac{1}{16\\lambda^2 s^4}\\right) = \\lambda \\left( b^2 - \\frac{1}{4\\lambda s^2} \\right)^2\n$$\n这就是能量壁垒的最终封闭形式表达式。\n\n### 步骤 4：到达时间缩减因子 $R(T)$ 的推导\n\n在 MCMC 和统计物理学的背景下，从温度化后验 $p_T(\\theta) \\propto p(\\theta)^{1/T} \\propto \\exp(-U(\\theta)/T)$ 进行采样，等同于在一个有效温度为 $T$（在玻尔兹曼常数为 1 的单位下）的条件下，模拟一个具有能量势 $U(\\theta)$ 的物理系统。\n根据过阻尼动力学的 Eyring-Kramers 原理，两个能量最小值点之间的平均跃迁时间（期望到达时间 $\\tau$）由一个与能量壁垒 $\\Delta E$ 和温度相关的指数项主导。对于一个具有能量 $U(\\theta)$ 和温度 $T$ 的系统，这表示为：\n$$\n\\tau(T) \\propto \\exp\\left(\\frac{\\Delta E}{T}\\right)\n$$\n指数前因子取决于势在最小值点和鞍点处的曲率，但对于高壁垒，指数项是压倒性主导的。问题引导我们关注这个主导因子。\n缩减因子 $R(T)$ 定义为温度 $T$ 下的到达时间与温度 $T=1$ 下的到达时间之比：\n$$\nR(T) = \\frac{\\tau(T)}{\\tau(1)}\n$$\n假设与指数项相比，指数前因子对 $T$ 的依赖性很弱或可以忽略不计，我们有：\n$$\nR(T) \\approx \\frac{\\exp(\\Delta E / T)}{\\exp(\\Delta E / 1)} = \\exp\\left(\\frac{\\Delta E}{T} - \\Delta E\\right) = \\exp\\left(\\Delta E \\left(\\frac{1}{T} - 1\\right)\\right)\n$$\n如此得到的 $R(T)$ 表达式仅依赖于壁垒高度 $\\Delta E$ 和温度 $T$，符合要求。\n因此，计算步骤如下：对于每个参数集 $(\\lambda, b, s, T)$，首先检查是否 $4\\lambda b^2 s^2  1$。如果不满足，则系统不是双模态的，结果为 $[\\text{nan}, \\text{nan}]$。如果条件成立，则使用推导出的公式计算 $\\Delta E$ 和 $R(T)$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the energy barrier and hitting time reduction factor for a double-well posterior.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda, b, s, T)\n        (1.0, 2.0, 1.0, 1.0),   # General well-separated modes\n        (1.5, 1.5, 0.8, 2.0),   # Moderately separated modes with nontrivial tempering\n        (0.8, 1.2, 1.5, 4.0),   # Broad likelihood and mild wells, strong tempering\n        (1.0, 0.6, 1.0, 3.0),   # Near the boundary of double-well existence\n        (2.0, 1.8, 0.7, 10.0),  # Strong wells and very high tempering\n    ]\n\n    results = []\n    for case in test_cases:\n        lam, b, s, T = case\n\n        # Step 1: Check the double-well condition: 4 * lambda * b^2 * s^2 > 1\n        # If this condition is not met, the potential has a single minimum,\n        # and the concept of an energy barrier between modes is not applicable.\n        condition = 4.0 * lam * b**2 * s**2\n        if condition = 1.0:\n            results.append([np.nan, np.nan])\n            continue\n\n        # Step 2: Compute the energy barrier height Delta E.\n        # The derived formula is: Delta_E = lambda * (b^2 - 1 / (4 * lambda * s^2))^2\n        term_in_paren = b**2 - 1.0 / (4.0 * lam * s**2)\n        delta_E = lam * (term_in_paren**2)\n\n        # Step 3: Compute the hitting time reduction factor R(T).\n        # The derived formula is: R(T) = exp(Delta_E * (1/T - 1))\n        # This approximates the ratio of expected hitting times tau(T)/tau(1).\n        if T == 1.0:\n            R_T = 1.0\n        else:\n            R_T = np.exp(delta_E * (1.0 / T - 1.0))\n        \n        results.append([delta_E, R_T])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{de:.8f},{rt:.8f}]\" for de, rt in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "3289096"}]}