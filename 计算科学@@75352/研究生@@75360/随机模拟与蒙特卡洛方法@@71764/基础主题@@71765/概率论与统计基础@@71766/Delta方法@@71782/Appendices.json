{"hands_on_practices": [{"introduction": "德尔塔方法通常以现成公式的形式出现，但其真正的力量在于其深刻的理论基础。这个练习将带你回归本源，不使用任何预先给出的德尔塔方法公式。通过从中心极限定理（CLT）和泰勒展开出发，你将亲手推导出估计量方差的一阶近似，并揭示函数曲率如何引入统计偏差，从而牢固掌握该方法的核心原理。[@problem_id:3352081]", "problem": "考虑一个蒙特卡洛情景，其中一个正量通过样本平均进行估计。令 $\\{X_{i}\\}_{i=1}^{n}$ 为独立同分布的随机变量，并令 $h(X)$ 为一个可测的、几乎必然为正且方差有限的被积函数。定义正参数 $\\theta \\equiv \\mathbb{E}[h(X)] > 0$ 及其蒙特卡洛估计量 $\\hat{\\theta}_{n} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} h(X_{i})$。记 $\\sigma_{h}^{2} \\equiv \\mathrm{Var}(h(X)) \\in (0,\\infty)$。令 $g(x) = \\ln x$ 为自然对数。\n\n仅从中心极限定理 (CLT) 和二阶泰勒展开出发，且不使用任何预先给定的 delta 方法公式，完成以下任务：\n\n- 当 $n \\to \\infty$ 时，推导 $\\mathrm{Var}(g(\\hat{\\theta}_{n}))$ 的一阶近似的闭式解，该解仅用 $\\theta$、$\\sigma_{h}^{2}$ 和 $n$ 表示。\n- 解释对数的曲率（其二阶导数）如何决定 $g(\\hat{\\theta}_{n})$ 作为 $g(\\theta)$ 的估计量的偏差的符号和主阶项。\n\n你的最终答案必须是 $\\mathrm{Var}(g(\\hat{\\theta}_{n}))$ 的一阶近似的单个闭式解析表达式，用 $\\theta$、$\\sigma_{h}^{2}$ 和 $n$ 表示。无需进行数值计算。不要包含任何单位。", "solution": "该问题是有效的，因为它具有科学依据、是适定的且客观的，并且位于统计推断和蒙特卡洛方法的标准框架内。所有必要信息都已提供，问题没有矛盾或含糊之处。\n\n这个问题要求完成两个主要任务：首先，推导一个变换后的蒙特卡洛估计量的方差的一阶近似；其次，解释这个变换后估计量的偏差。\n\n将随机变量序列 $\\{h(X_{i})\\}_{i=1}^{n}$ 记为 $\\{Y_{i}\\}_{i=1}^{n}$。根据问题陈述，这些是独立同分布 (i.i.d.) 的随机变量，其期望为 $\\mathbb{E}[Y_{i}] = \\theta$，方差为 $\\mathrm{Var}(Y_{i}) = \\sigma_{h}^{2}$。蒙特卡洛估计量是样本均值 $\\hat{\\theta}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i}$。\n\n该估计量的期望为 $\\mathbb{E}[\\hat{\\theta}_{n}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} Y_{i}\\right] = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}[Y_{i}] = \\frac{1}{n}(n\\theta) = \\theta$，这表明 $\\hat{\\theta}_{n}$ 是 $\\theta$ 的一个无偏估计量。\n由于 $Y_i$ 的独立性，该估计量的方差为 $\\mathrm{Var}(\\hat{\\theta}_{n}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} Y_{i}\\right) = \\frac{1}{n^{2}}\\sum_{i=1}^{n} \\mathrm{Var}(Y_{i}) = \\frac{1}{n^{2}}(n\\sigma_{h}^{2}) = \\frac{\\sigma_{h}^{2}}{n}$。\n\n根据中心极限定理 (CLT)，当 $n \\to \\infty$ 时，标准化的样本均值的分布收敛于一个正态分布。CLT 的一个常见表述是：\n$$ \\sqrt{n}(\\hat{\\theta}_{n} - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma_{h}^{2}) $$\n其中 $\\xrightarrow{d}$ 表示依分布收敛。这意味着对于大的 $n$，$\\hat{\\theta}_{n}$ 近似服从正态分布，其均值为 $\\theta$，方差为 $\\frac{\\sigma_{h}^{2}}{n}$。\n\n**第一部分：方差近似的推导**\n\n我们被要求找到 $\\mathrm{Var}(g(\\hat{\\theta}_{n}))$ 的一个近似，其中 $g(x) = \\ln x$。推导必须从基本原理出发，即中心极限定理和泰勒展开。我们对函数 $g(\\hat{\\theta}_{n})$ 在点 $\\theta = \\mathbb{E}[\\hat{\\theta}_{n}]$ 附近进行一阶泰勒级数展开。\n一阶展开式为：\n$$ g(\\hat{\\theta}_{n}) = g(\\theta) + g'(\\theta)(\\hat{\\theta}_{n} - \\theta) + R_{1} $$\n其中 $R_{1}$ 是余项，当 $\\hat{\\theta}_{n}$ 接近 $\\theta$ 时，它是比线性项更高阶的无穷小。对于 $g(x) = \\ln x$，其一阶导数为 $g'(x) = \\frac{1}{x}$。在 $x = \\theta$ 处求值得 $g'(\\theta) = \\frac{1}{\\theta}$。\n因此，一阶近似为：\n$$ g(\\hat{\\theta}_{n}) \\approx g(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta) $$\n我们关心的是这个表达式的方差。使用方差的性质，即对于常数 $a$ 和随机变量 $Z$，有 $\\mathrm{Var}(aZ+b) = a^{2}\\mathrm{Var}(Z)$：\n$$ \\mathrm{Var}(g(\\hat{\\theta}_{n})) \\approx \\mathrm{Var}\\left(g(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta)\\right) $$\n由于 $g(\\theta)=\\ln(\\theta)$ 和 $\\theta$ 是常数，它们对方差没有贡献。该表达式简化为：\n$$ \\mathrm{Var}(g(\\hat{\\theta}_{n})) \\approx \\mathrm{Var}\\left(\\frac{1}{\\theta}\\hat{\\theta}_{n}\\right) = \\left(\\frac{1}{\\theta}\\right)^{2} \\mathrm{Var}(\\hat{\\theta}_{n}) $$\n代入已知的方差 $\\mathrm{Var}(\\hat{\\theta}_{n}) = \\frac{\\sigma_{h}^{2}}{n}$：\n$$ \\mathrm{Var}(g(\\hat{\\theta}_{n})) \\approx \\frac{1}{\\theta^{2}} \\left( \\frac{\\sigma_{h}^{2}}{n} \\right) = \\frac{\\sigma_{h}^{2}}{n\\theta^{2}} $$\n这就是当 $n \\to \\infty$ 时 $g(\\hat{\\theta}_{n})$ 的方差所要求的一阶近似。\n\n**第二部分：偏差与曲率的作用**\n\n为了分析 $g(\\hat{\\theta}_{n})$ 作为 $g(\\theta) = \\ln \\theta$ 的估计量的偏差，我们必须近似其期望 $\\mathbb{E}[g(\\hat{\\theta}_{n})]$。一阶泰勒展开是不够的，因为 $\\mathbb{E}[\\hat{\\theta}_{n} - \\theta] = 0$。我们必须进行二阶泰勒展开：\n$$ g(\\hat{\\theta}_{n}) = g(\\theta) + g'(\\theta)(\\hat{\\theta}_{n} - \\theta) + \\frac{1}{2}g''(\\theta)(\\hat{\\theta}_{n} - \\theta)^{2} + R_{2} $$\n对于 $g(x) = \\ln x$，其二阶导数为 $g''(x) = -\\frac{1}{x^{2}}$。在 $x = \\theta$ 处求值得 $g''(\\theta) = -\\frac{1}{\\theta^{2}}$。将导数代入展开式可得：\n$$ g(\\hat{\\theta}_{n}) \\approx \\ln(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta) - \\frac{1}{2\\theta^{2}}(\\hat{\\theta}_{n} - \\theta)^{2} $$\n对两边取期望，并利用期望的线性性质：\n$$ \\mathbb{E}[g(\\hat{\\theta}_{n})] \\approx \\mathbb{E}\\left[\\ln(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta) - \\frac{1}{2\\theta^{2}}(\\hat{\\theta}_{n} - \\theta)^{2}\\right] $$\n$$ \\mathbb{E}[g(\\hat{\\theta}_{n})] \\approx \\ln(\\theta) + \\frac{1}{\\theta}\\mathbb{E}[\\hat{\\theta}_{n} - \\theta] - \\frac{1}{2\\theta^{2}}\\mathbb{E}[(\\hat{\\theta}_{n} - \\theta)^{2}] $$\n我们计算这些期望项：\n- 第一项是常数：$\\mathbb{E}[\\ln(\\theta)] = \\ln(\\theta)$。\n- 第二项为零，因为 $\\hat{\\theta}_{n}$ 是 $\\theta$ 的无偏估计量：$\\mathbb{E}[\\hat{\\theta}_{n} - \\theta] = \\mathbb{E}[\\hat{\\theta}_{n}] - \\theta = \\theta - \\theta = 0$。\n- 第三项的期望是 $\\hat{\\theta}_{n}$ 的方差的定义：$\\mathbb{E}[(\\hat{\\theta}_{n} - \\theta)^{2}] = \\mathrm{Var}(\\hat{\\theta}_{n}) = \\frac{\\sigma_{h}^{2}}{n}$。\n\n将这些结果代回期望的近似式中：\n$$ \\mathbb{E}[g(\\hat{\\theta}_{n})] \\approx \\ln(\\theta) - \\frac{1}{2\\theta^{2}}\\left(\\frac{\\sigma_{h}^{2}}{n}\\right) $$\n估计量的偏差定义为 $\\mathrm{Bias}(g(\\hat{\\theta}_{n})) = \\mathbb{E}[g(\\hat{\\theta}_{n})] - g(\\theta)$。因此，偏差的主阶项为：\n$$ \\mathrm{Bias}(g(\\hat{\\theta}_{n})) \\approx -\\frac{\\sigma_{h}^{2}}{2n\\theta^{2}} $$\n偏差与函数 $g$ 的二阶导数成正比，二阶导数代表其曲率。对于一个一般函数 $g$，偏差主阶项为 $\\frac{1}{2}g''(\\theta)\\mathrm{Var}(\\hat{\\theta}_{n})$。因此，偏差的符号由曲率 $g''(\\theta)$ 的符号决定。对于自然对数函数，$g''(x) = -1/x^{2}$，在其定义域 $(0, \\infty)$ 内对所有 $x$ 都严格为负。由于 $\\theta > 0$，在 $\\theta$ 处的曲率 $g''(\\theta) = -1/\\theta^{2}$ 是负的。考虑到 $\\sigma_{h}^{2} > 0$ 且 $n \\ge 1$，整个偏差项是负的。\n这表明对数函数的凹性（负曲率）导致估计量 $\\ln(\\hat{\\theta}_{n})$ 系统性地低估真值 $\\ln(\\theta)$。这个结果是琴生不等式 (Jensen's inequality) 的一个定量改进。对于凹函数 $g$，琴生不等式表明 $\\mathbb{E}[g(Z)] \\le g(\\mathbb{E}[Z])$。在这里，令 $Z=\\hat{\\theta}_{n}$，我们得到 $\\mathbb{E}[\\ln(\\hat{\\theta}_{n})] \\le \\ln(\\mathbb{E}[\\hat{\\theta}_{n}]) = \\ln(\\theta)$。泰勒展开在 $1/n$ 的主阶上量化了此不等式。", "answer": "$$\\boxed{\\frac{\\sigma_{h}^{2}}{n\\theta^{2}}}$$", "id": "3352081"}, {"introduction": "在许多现代统计应用中，特别是在贝叶斯推断中，我们处理的估计量通常来自相关的样本序列，例如马尔可夫链蒙特卡洛（MCMC）的输出。这个练习将德尔塔方法的应用从独立同分布（i.i.d.）数据推广到了更复杂的MCMC场景。你需要运用马尔可夫链中心极限定理，并通过对自协方差求和来正确计算渐近方差，这对于在高级随机模拟中应用德尔塔方法至关重要。[@problem_id:3352103]", "problem": "考虑一个贝叶斯模型，其中正标量参数 $\\theta$ 的后验分布为 $\\operatorname{Gamma}(\\alpha,\\beta)$ 分布，其形状参数 $\\alpha>0$，率参数 $\\beta>0$。一个马尔可夫链蒙特卡洛 (MCMC) 过程生成了一个严平稳、遍历的马尔可夫链 $\\{\\theta_t\\}_{t=1}^{\\infty}$，其不变分布等于后验分布，且后验均值为 $\\theta=\\mathbb{E}_{\\pi}[\\theta_t]$。将后验均值的蒙特卡洛估计量定义为 $\\hat{\\theta}_n = \\frac{1}{n}\\sum_{t=1}^{n}\\theta_t$。假设马尔可夫链中心极限定理 (CLT) 对 $\\hat{\\theta}_n$ 成立，并且在平稳性下 $\\{\\theta_t\\}$ 的自相关函数是几何的，即对于所有整数 $k\\geq 0$，\n$$\n\\rho_k = \\operatorname{Corr}(\\theta_t, \\theta_{t+k}) = c^k\n$$\n其中 $c\\in(0,1)$ 是一个固定常数。\n\n您的任务是分析重参数化估计量 $g(\\hat{\\theta}_n)=\\log(\\hat{\\theta}_n)$。请仅使用基于自协方差的马尔可夫链中心极限定理 (CLT) 定义和在后验均值附近的一阶泰勒展开，推导归一化误差 $\\sqrt{n}\\,\\big(g(\\hat{\\theta}_n)-g(\\theta)\\big)$ 的渐近方差常数，并将其表示为仅包含 $\\alpha$ 和 $c$ 的单个闭式解析表达式。请勿引入任何辅助数值近似。请以解析表达式的形式提供最终答案。无需四舍五入，不涉及单位。", "solution": "该问题要求计算对数变换后的后验均值蒙特卡洛估计量的归一化误差的渐近方差常数。我们用随机变量 $\\Theta$ 表示我们感兴趣的参数。其后验分布为 $\\pi(\\Theta) = \\operatorname{Gamma}(\\alpha, \\beta)$，形状参数 $\\alpha > 0$，率参数 $\\beta > 0$。\n\n问题指出，后验均值用 $\\theta$ 表示。对于 $\\operatorname{Gamma}(\\alpha, \\beta)$ 分布，其均值为：\n$$\n\\theta = \\mathbb{E}[\\Theta] = \\frac{\\alpha}{\\beta}\n$$\n该分布的方差为：\n$$\n\\operatorname{Var}(\\Theta) = \\frac{\\alpha}{\\beta^2}\n$$\n给定一个平稳遍历的马尔可夫链 $\\{\\theta_t\\}_{t=1}^{\\infty}$，其不变分布即为该后验分布。后验均值 $\\theta$ 的蒙特卡洛估计量为 $\\hat{\\theta}_n = \\frac{1}{n}\\sum_{t=1}^{n}\\theta_t$。\n\n问题假设马尔可夫链中心极限定理 (CLT) 对 $\\hat{\\theta}_n$ 成立。该定理指出，归一化误差的分布收敛于一个正态分布：\n$$\n\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2_{asy})\n$$\n其中 $\\xrightarrow{d}$ 表示依分布收敛，$\\sigma^2_{asy}$ 是渐近方差常数。任务是找到变换后估计量 $g(\\hat{\\theta}_n)$ 对应的渐近方差常数，其中 $g(x) = \\log(x)$。根据理论统计和微积分中的标准惯例，我们假设 $\\log(x)$ 表示自然对数 $\\ln(x)$。\n\n首先，我们必须确定 $\\sigma^2_{asy}$。对于平稳马尔可夫链，渐近方差由所有自协方差之和给出：\n$$\n\\sigma^2_{asy} = \\sum_{k=-\\infty}^{\\infty} \\operatorname{Cov}(\\theta_t, \\theta_{t+k})\n$$\n设 $\\gamma_k = \\operatorname{Cov}(\\theta_t, \\theta_{t+k})$ 为滞后 $k$ 时的自协方差。由于平稳性，$\\gamma_k = \\gamma_{-k}$。因此，该表达式可以写成：\n$$\n\\sigma^2_{asy} = \\gamma_0 + 2\\sum_{k=1}^{\\infty} \\gamma_k\n$$\n其中 $\\gamma_0 = \\operatorname{Var}(\\theta_t) = \\operatorname{Var}(\\Theta) = \\frac{\\alpha}{\\beta^2}$。自相关函数 $\\rho_k$ 定义为 $\\rho_k = \\frac{\\gamma_k}{\\gamma_0}$，所以 $\\gamma_k = \\rho_k \\gamma_0$。问题给出，对于常数 $c \\in (0,1)$ 和所有整数 $k \\geq 0$，有 $\\rho_k = c^k$。\n\n将这些代入 $\\sigma^2_{asy}$ 的表达式中：\n$$\n\\sigma^2_{asy} = \\gamma_0 + 2\\sum_{k=1}^{\\infty} (c^k \\gamma_0) = \\gamma_0 \\left(1 + 2\\sum_{k=1}^{\\infty} c^k\\right)\n$$\n求和项是一个无穷几何级数：$\\sum_{k=1}^{\\infty} c^k = \\frac{c}{1-c}$，因为 $|c|  1$。\n将此结果代回 $\\sigma^2_{asy}$ 的表达式中：\n$$\n\\sigma^2_{asy} = \\gamma_0 \\left(1 + 2\\frac{c}{1-c}\\right) = \\frac{\\alpha}{\\beta^2} \\left(\\frac{1-c+2c}{1-c}\\right) = \\frac{\\alpha}{\\beta^2} \\left(\\frac{1+c}{1-c}\\right)\n$$\n现在，我们分析变换后的估计量 $g(\\hat{\\theta}_n) = \\ln(\\hat{\\theta}_n)$。问题要求在真实后验均值 $\\theta$ 附近对 $g(\\hat{\\theta}_n)$ 进行一阶泰勒展开。\n$$\ng(\\hat{\\theta}_n) \\approx g(\\theta) + g'(\\theta)(\\hat{\\theta}_n - \\theta)\n$$\n其中 $g'(\\theta)$ 是 $g(x)$ 在 $x=\\theta$ 处的导数。\n整理各项，我们得到：\n$$\ng(\\hat{\\theta}_n) - g(\\theta) \\approx g'(\\theta)(\\hat{\\theta}_n - \\theta)\n$$\n两边乘以 $\\sqrt{n}$：\n$$\n\\sqrt{n}\\big(g(\\hat{\\theta}_n) - g(\\theta)\\big) \\approx g'(\\theta) \\sqrt{n}(\\hat{\\theta}_n - \\theta)\n$$\n这个近似，被称为Delta方法，意味着左侧的渐近分布与右侧的分布相同。由于 $\\sqrt{n}(\\hat{\\theta}_n - \\theta)$ 收敛于一个均值为 $0$、方差为 $\\sigma^2_{asy}$ 的正态分布，因此右侧收敛于一个均值为 $g'(\\theta) \\times 0 = 0$、方差为 $(g'(\\theta))^2 \\sigma^2_{asy}$ 的正态分布。\n\n因此，归一化误差 $\\sqrt{n}(g(\\hat{\\theta}_n) - g(\\theta))$ 的渐近方差常数，我们称之为 $V_g$，是：\n$$\nV_g = (g'(\\theta))^2 \\sigma^2_{asy}\n$$\n函数是 $g(x) = \\ln(x)$，所以其导数为 $g'(x) = \\frac{1}{x}$。我们在后验均值 $\\theta = \\frac{\\alpha}{\\beta}$ 处计算该导数：\n$$\ng'(\\theta) = \\frac{1}{\\theta} = \\frac{1}{\\alpha/\\beta} = \\frac{\\beta}{\\alpha}\n$$\n现在我们可以将 $g'(\\theta)$ 和 $\\sigma^2_{asy}$ 的表达式代入 $V_g$ 的公式中：\n$$\nV_g = \\left(\\frac{\\beta}{\\alpha}\\right)^2 \\left( \\frac{\\alpha}{\\beta^2} \\left(\\frac{1+c}{1-c}\\right) \\right)\n$$\n$$\nV_g = \\frac{\\beta^2}{\\alpha^2} \\cdot \\frac{\\alpha}{\\beta^2} \\cdot \\frac{1+c}{1-c}\n$$\n分子和分母中的 $\\beta^2$ 项相互抵消。类似地，一个 $\\alpha$ 因子也被抵消。\n$$\nV_g = \\frac{1}{\\alpha} \\cdot \\frac{1+c}{1-c}\n$$\n这就是渐近方差常数的最终表达式，按要求仅用 $\\alpha$ 和 $c$ 表示。", "answer": "$$\n\\boxed{\\frac{1}{\\alpha}\\frac{1+c}{1-c}}\n$$", "id": "3352103"}, {"introduction": "理论推导虽然富有洞察力，但在实际研究中，我们经常遇到难以甚至无法求得解析导数的复杂函数。本练习将理论付诸实践，指导你构建一个完全数值化的工作流程来应用德尔塔方法。你将通过结合蒙特卡洛积分来评估函数值，使用有限差分来估计雅可比矩阵，最终完成不确定性的传播。这项编码实践不仅能让你掌握一项强大的计算技术，还能让你学会分析近似过程中产生的数值误差。[@problem_id:3352079]", "problem": "设 $\\theta \\in \\mathbb{R}^2$ 表示一个参数向量，其坐标为 $\\theta = (\\mu,\\sigma)$，其中 $\\mu \\in \\mathbb{R}$ 且 $\\sigma \\in (0,\\infty)$。考虑由下式定义的映射 $g:\\mathbb{R}^2 \\to \\mathbb{R}^2$\n$$\ng(\\theta)\n=\n\\begin{bmatrix}\n\\mathbb{E}_{\\theta}[X^3] \\\\\n\\mathbb{E}_{\\theta}[\\sin(X)]\n\\end{bmatrix},\n\\quad\nX \\sim \\mathcal{N}(\\mu,\\sigma^2),\n$$\n其中正弦函数的角度以弧度为单位。\n\n假设 $\\hat{\\theta}$ 是 $\\theta$ 的一个估计量，对于大样本量，其分布近似为均值为 $\\theta$、协方差矩阵为 $V_{\\theta}$（一个给定的、固定的 $2 \\times 2$ 半正定矩阵）的多元正态分布。\n\n您将实现一个完全数值化的蒙特卡洛过程，以使用有限差分来估计雅可比矩阵 $J_g(\\theta)$（即 $g$ 在 $\\theta$ 处的一阶偏导数 $2 \\times 2$ 矩阵），其中映射 $g$ 本身也通过蒙特卡洛积分进行评估。然后，您将使用这个估计的雅可比矩阵进行 delta 方法的方差计算，并量化雅可比矩阵近似误差对所得 delta 方法协方差估计的影响。\n\n使用的基本原理：\n- 映射 $g(\\theta)$ 由正态分布下的期望定义，并根据大数定律，允许使用样本均值作为蒙特卡洛近似。\n- 雅可比矩阵 $J_g(\\theta)$ 可以用步长为 $h$ 的对称有限差分来近似。\n- delta 方法源于 $g(\\hat{\\theta})$ 在 $\\theta$ 周围的一阶泰勒展开以及 $\\hat{\\theta}$ 的渐近正态性。\n\n您的任务是：\n1. 为任意 $\\theta = (\\mu,\\sigma)$ 实现 $g(\\theta)$ 的蒙特卡洛估计量，具体如下。抽取 $M$ 个独立的标准正态样本 $Z_1,\\dots,Z_M \\sim \\mathcal{N}(0,1)$，并为 $i \\in \\{1,\\dots,M\\}$ 定义 $X_i(\\mu,\\sigma) = \\mu + \\sigma Z_i$。通过样本均值来近似 $g(\\theta)$：\n$$\n\\widehat{g}_1(\\theta) = \\frac{1}{M}\\sum_{i=1}^M X_i(\\mu,\\sigma)^3, \n\\quad\n\\widehat{g}_2(\\theta) = \\frac{1}{M}\\sum_{i=1}^M \\sin\\bigl(X_i(\\mu,\\sigma)\\bigr).\n$$\n2. 使用相同的基础标准正态抽样（公共随机数）来减少方差，为 $J_g(\\theta)$ 的每一列实现对称有限差分近似。对于 $\\mu$ 方向，使用\n$$\n\\widehat{J}_{\\cdot,1}(\\theta;h,M)\n=\n\\frac{\\widehat{g}(\\mu+h,\\sigma) - \\widehat{g}(\\mu-h,\\sigma)}{2h},\n$$\n对于 $\\sigma$ 方向，使用\n$$\n\\widehat{J}_{\\cdot,2}(\\theta;h,M)\n=\n\\frac{\\widehat{g}(\\mu,\\sigma+h) - \\widehat{g}(\\mu,\\sigma-h)}{2h}.\n$$\n使用相同的 $Z_1,\\dots,Z_M$ 来计算所有的四次评估 $\\widehat{g}(\\mu\\pm h,\\sigma)$ 和 $\\widehat{g}(\\mu,\\sigma\\pm h)$。\n3. 给定 $V_{\\theta}$，构建 delta 方法的协方差估计\n$$\n\\widehat{\\Sigma}_g(\\theta;h,M) = \\widehat{J}_g(\\theta;h,M)\\, V_{\\theta}\\, \\widehat{J}_g(\\theta;h,M)^{\\top}.\n$$\n4. 为了进行评估，推导解析雅可比矩阵 $J_g(\\theta)$ 和相应的解析 delta 方法协方差\n$$\n\\Sigma_g(\\theta) = J_g(\\theta)\\, V_{\\theta}\\, J_g(\\theta)^{\\top}.\n$$\n5. 对于下面指定的每个测试案例，计算相对弗罗贝尼乌斯误差\n$$\n\\mathrm{Err}(\\theta,h,M,V_{\\theta})\n=\n\\frac{\\left\\| \\widehat{\\Sigma}_g(\\theta;h,M) - \\Sigma_g(\\theta) \\right\\|_F}{\\left\\| \\Sigma_g(\\theta) \\right\\|_F}.\n$$\n\n角度单位要求：所有三角函数参数均以弧度为单位。\n\n测试套件：\n- 案例 1：$\\mu = 0.5$, $\\sigma = 1.2$,\n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.04  0.01 \\\\\n0.01  0.09\n\\end{bmatrix},\n$$\n$h = 10^{-3}$，$M = 50000$，种子 $= 12345$。\n- 案例 2：$\\mu = 0.5$, $\\sigma = 1.2$,\n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.04  0.01 \\\\\n0.01  0.09\n\\end{bmatrix},\n$$\n$h = 10^{-1}$，$M = 50000$，种子 $= 67890$。\n- 案例 3：$\\mu = 0.5$, $\\sigma = 1.2$,\n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.04  0.01 \\\\\n0.01  0.09\n\\end{bmatrix},\n$$\n$h = 10^{-5}$，$M = 2000$，种子 $= 13579$。\n- 案例 4：$\\mu = 1.5$, $\\sigma = 0.5$,\n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.01  -0.003 \\\\\n-0.003  0.02\n\\end{bmatrix},\n$$\n$h = 5 \\times 10^{-4}$，$M = 200000$，种子 $= 24680$。\n\n要求的最终输出格式：您的程序应生成一行输出，其中包含四个案例的四个相对误差，形式为逗号分隔的列表，并用方括号括起来，例如\n$$\n[\\mathrm{Err}_1,\\mathrm{Err}_2,\\mathrm{Err}_3,\\mathrm{Err}_4].\n$$\n每个条目必须是一个实数。", "solution": "该问题已经过验证，被认为是可靠的。这是一个在计算统计学领域内适定且具有科学依据的问题，没有矛盾、歧义或伪科学主张。所有为获得唯一、可复现解所需的数据和定义都已提供。\n\n主要目标是量化当基础雅可比矩阵通过数值方法估计时，delta 方法的协方差矩阵近似中的误差。这涉及一个双层蒙特卡洛过程：函数 $g(\\theta)$ 本身是一个通过蒙特卡洛方法估计的期望，然后其雅可比矩阵 $J_g(\\theta)$ 使用基于该蒙特卡洛估计量的有限差分进行近似。误差分析将此完全数值化的结果与解析的基准真相进行比较。\n\n求解过程遵循以下步骤：\n1.  推导映射 $g(\\theta)$ 及其雅可比矩阵 $J_g(\\theta)$ 的解析表达式。\n2.  按照规定实现 $g(\\theta)$ 和 $J_g(\\theta)$ 的数值估计量。\n3.  计算解析和估计的 delta 方法协方差矩阵。\n4.  为每个测试案例计算指定的相对弗罗贝尼乌斯误差。\n\n**1. 解析推导**\n\n设 $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$。参数向量为 $\\theta = (\\mu, \\sigma)$。映射为 $g(\\theta) = [\\mathbb{E}_{\\theta}[X^3], \\mathbb{E}_{\\theta}[\\sin(X)]]^{\\top}$。\n\n第一分量，$g_1(\\theta) = \\mathbb{E}[X^3]$:\n正态分布的三阶中心矩为 $0$。原始矩 $m_k = \\mathbb{E}[X^k]$ 与中心矩之间的关系通过中心化变量 $X-\\mu$ 给出。更直接的方法是使用矩生成函数 $M_X(t) = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$ 或展开 $X=\\mu+\\sigma Z$，其中 $Z \\sim \\mathcal{N}(0,1)$。\n使用后者，$\\mathbb{E}[X^3] = \\mathbb{E}[(\\mu + \\sigma Z)^3] = \\mathbb{E}[\\mu^3 + 3\\mu^2\\sigma Z + 3\\mu\\sigma^2 Z^2 + \\sigma^3 Z^3]$。\n根据期望的线性性以及标准正态分布的矩 $\\mathbb{E}[Z]=0$，$\\mathbb{E}[Z^2]=1$ 和 $\\mathbb{E}[Z^3]=0$，我们得到：\n$g_1(\\mu, \\sigma) = \\mu^3 + 3\\mu\\sigma^2 \\mathbb{E}[Z^2] = \\mu^3 + 3\\mu\\sigma^2$。\n\n第二分量，$g_2(\\theta) = \\mathbb{E}[\\sin(X)]$:\n我们通过考虑 $X$ 的特征函数 $\\phi_X(t) = \\mathbb{E}[e^{itX}] = \\exp(i\\mu t - \\frac{1}{2}\\sigma^2 t^2)$ 来计算它。\n使用欧拉公式 $\\sin(X) = \\mathrm{Im}(e^{iX})$ 和期望的线性性：\n$\\mathbb{E}[\\sin(X)] = \\mathbb{E}[\\mathrm{Im}(e^{iX})] = \\mathrm{Im}(\\mathbb{E}[e^{iX}])$。\n$\\mathbb{E}[e^{iX}]$ 是 $\\phi_X(1) = \\exp(i\\mu - \\frac{1}{2}\\sigma^2) = e^{-\\sigma^2/2}e^{i\\mu} = e^{-\\sigma^2/2}(\\cos(\\mu) + i\\sin(\\mu))$。\n虚部是：\n$g_2(\\mu, \\sigma) = \\sin(\\mu) e^{-\\sigma^2/2}$。\n\n解析映射为：\n$g(\\theta) = \\begin{bmatrix} \\mu^3 + 3\\mu\\sigma^2 \\\\ \\sin(\\mu) e^{-\\sigma^2/2} \\end{bmatrix}$。\n\n解析雅可比矩阵 $J_g(\\theta)$ 是 $g$ 关于 $\\mu$ 和 $\\sigma$ 的一阶偏导数矩阵：\n$J_g(\\theta) = \\begin{bmatrix} \\frac{\\partial g_1}{\\partial \\mu}  \\frac{\\partial g_1}{\\partial \\sigma} \\\\ \\frac{\\partial g_2}{\\partial \\mu}  \\frac{\\partial g_2}{\\partial \\sigma} \\end{bmatrix}$。\n偏导数是：\n$\\frac{\\partial g_1}{\\partial \\mu} = 3\\mu^2 + 3\\sigma^2$\n$\\frac{\\partial g_1}{\\partial \\sigma} = 6\\mu\\sigma$\n$\\frac{\\partial g_2}{\\partial \\mu} = \\cos(\\mu) e^{-\\sigma^2/2}$\n$\\frac{\\partial g_2}{\\partial \\sigma} = \\sin(\\mu) \\cdot e^{-\\sigma^2/2} \\cdot (-\\sigma) = -\\sigma \\sin(\\mu) e^{-\\sigma^2/2}$\n\n因此，解析雅可比矩阵为：\n$$\nJ_g(\\theta) = \\begin{bmatrix} 3\\mu^2 + 3\\sigma^2  6\\mu\\sigma \\\\ \\cos(\\mu) e^{-\\sigma^2/2}  -\\sigma \\sin(\\mu) e^{-\\sigma^2/2} \\end{bmatrix}\n$$\n\n**2. 数值估计程序**\n\n实现将遵循指定的数值方法。\n\n一个函数 `g_hat(mu, sigma, Z)`，对于给定的参数 $\\mu, \\sigma$ 和一个预先生成的包含 $M$ 个标准正态样本的数组 $Z = (Z_1, \\dots, Z_M)$，来估计 $g(\\theta)$。它计算 $X_i = \\mu + \\sigma Z_i$，然后返回 $X_i^3$ 和 $\\sin(X_i)$ 的样本均值。\n\n第二个函数 `J_hat(mu, sigma, h, Z)` 估计雅可比矩阵 $J_g(\\theta)$。它使用所提供的对称有限差分公式。关键是，它通过将相同的样本数组 `Z` 传递给每个 `g_hat` 的调用来采用公共随机数（CRN）技术：\n$$\n\\widehat{J}_{\\cdot,1} = \\frac{\\text{g\\_hat}(\\mu+h, \\sigma, Z) - \\text{g\\_hat}(\\mu-h, \\sigma, Z)}{2h}\n$$\n$$\n\\widehat{J}_{\\cdot,2} = \\frac{\\text{g\\_hat}(\\mu, \\sigma+h, Z) - \\text{g\\_hat}(\\mu, \\sigma-h, Z)}{2h}\n$$\n然后将这两个列向量组合成 $2 \\times 2$ 的估计雅可比矩阵 $\\widehat{J}_g(\\theta; h, M)$。\n\n**3. Delta 方法与误差计算**\n\n对于每个测试案例，我们计算两个协方差矩阵：\n解析 delta 方法协方差：$\\Sigma_g(\\theta) = J_g(\\theta)\\, V_{\\theta}\\, J_g(\\theta)^{\\top}$，使用解析推导的 $J_g(\\theta)$。\n估计的 delta 方法协方差：$\\widehat{\\Sigma}_g(\\theta;h,M) = \\widehat{J}_g(\\theta;h,M)\\, V_{\\theta}\\, \\widehat{J}_g(\\theta;h,M)^{\\top}$，使用数值估计的 $\\widehat{J}_g$。\n\n最终误差计算为这两个矩阵之差的相对弗罗贝尼乌斯范数：\n$$\n\\mathrm{Err} = \\frac{\\left\\| \\widehat{\\Sigma}_g(\\theta;h,M) - \\Sigma_g(\\theta) \\right\\|_F}{\\left\\| \\Sigma_g(\\theta) \\right\\|_F}\n$$\n其中 $\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}$。\n\n**4. 测试案例执行**\n\n一个循环将遍历四个测试案例。在每次迭代中：\n1.  提取参数 $(\\mu, \\sigma, V_{\\theta}, h, M, \\text{seed})$。\n2.  为随机数生成器设定种子。\n3.  抽取 $M$ 个标准正态样本。\n4.  如上所述计算 $\\Sigma_g(\\theta)$ 和 $\\widehat{\\Sigma}_g(\\theta;h,M)$。\n5.  计算并存储相对误差 $\\mathrm{Err}$。\n\n最后，将四个误差值的列表按指定格式进行格式化并打印。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef g_hat_mc(mu, sigma, z_samples):\n    \"\"\"\n    Computes the Monte Carlo estimate of the mapping g(theta).\n    \n    Args:\n        mu (float): The mean parameter.\n        sigma (float): The standard deviation parameter.\n        z_samples (np.ndarray): An array of standard normal random samples.\n    \n    Returns:\n        np.ndarray: A 2-element array containing the estimates of E[X^3] and E[sin(X)].\n    \"\"\"\n    x_samples = mu + sigma * z_samples\n    g1_hat = np.mean(x_samples**3)\n    g2_hat = np.mean(np.sin(x_samples))  # np.sin uses radians\n    return np.array([g1_hat, g2_hat])\n\ndef J_g_hat_mc(mu, sigma, h, z_samples):\n    \"\"\"\n    Computes the finite-difference estimate of the Jacobian of g(theta)\n    using common random numbers.\n    \n    Args:\n        mu (float): The mean parameter.\n        sigma (float): The standard deviation parameter.\n        h (float): The finite difference step size.\n        z_samples (np.ndarray): An array of standard normal random samples.\n    \n    Returns:\n        np.ndarray: A 2x2 matrix representing the estimated Jacobian.\n    \"\"\"\n    # First column: partial derivatives with respect to mu\n    g_plus_mu = g_hat_mc(mu + h, sigma, z_samples)\n    g_minus_mu = g_hat_mc(mu - h, sigma, z_samples)\n    J_col1 = (g_plus_mu - g_minus_mu) / (2 * h)\n    \n    # Second column: partial derivatives with respect to sigma\n    g_plus_sigma = g_hat_mc(mu, sigma + h, z_samples)\n    g_minus_sigma = g_hat_mc(mu, sigma - h, z_samples)\n    J_col2 = (g_plus_sigma - g_minus_sigma) / (2 * h)\n    \n    # Assemble the Jacobian matrix from its columns\n    return np.stack([J_col1, J_col2], axis=1)\n\ndef J_g_analytic(mu, sigma):\n    \"\"\"\n    Computes the analytic Jacobian of g(theta).\n    \n    Args:\n        mu (float): The mean parameter.\n        sigma (float): The standard deviation parameter.\n    \n    Returns:\n        np.ndarray: A 2x2 matrix representing the analytic Jacobian.\n    \"\"\"\n    J11 = 3 * mu**2 + 3 * sigma**2\n    J12 = 6 * mu * sigma\n    \n    exp_term = np.exp(-0.5 * sigma**2)\n    J21 = np.cos(mu) * exp_term\n    J22 = -sigma * np.sin(mu) * exp_term\n    \n    return np.array([[J11, J12], [J21, J22]])\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute the errors.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Base case\n        {'mu': 0.5, 'sigma': 1.2, 'V_theta': [[0.04, 0.01], [0.01, 0.09]], 'h': 1e-3, 'M': 50000, 'seed': 12345},\n        # Case 2: Large finite-difference step h\n        {'mu': 0.5, 'sigma': 1.2, 'V_theta': [[0.04, 0.01], [0.01, 0.09]], 'h': 1e-1, 'M': 50000, 'seed': 67890},\n        # Case 3: Small Monte Carlo sample size M\n        {'mu': 0.5, 'sigma': 1.2, 'V_theta': [[0.04, 0.01], [0.01, 0.09]], 'h': 1e-5, 'M': 2000, 'seed': 13579},\n        # Case 4: Different parameter set, large M\n        {'mu': 1.5, 'sigma': 0.5, 'V_theta': [[0.01, -0.003], [-0.003, 0.02]], 'h': 5e-4, 'M': 200000, 'seed': 24680},\n    ]\n\n    results = []\n    for case in test_cases:\n        mu = case['mu']\n        sigma = case['sigma']\n        V_theta = np.array(case['V_theta'])\n        h = case['h']\n        M = case['M']\n        seed = case['seed']\n\n        # --- Analytic \"Ground Truth\" Calculation ---\n        J_analytic = J_g_analytic(mu, sigma)\n        Sigma_g_analytic = J_analytic @ V_theta @ J_analytic.T\n        \n        # --- Numerical Estimation ---\n        # 1. Set up the random number generator and draw samples\n        rng = np.random.default_rng(seed)\n        z_samples = rng.standard_normal(M)\n        \n        # 2. Estimate the Jacobian using Monte Carlo and finite differences\n        J_hat = J_g_hat_mc(mu, sigma, h, z_samples)\n        \n        # 3. Compute the estimated delta method covariance matrix\n        Sigma_g_hat = J_hat @ V_theta @ J_hat.T\n\n        # --- Error Quantification ---\n        # Compute the relative Frobenius error\n        numerator = np.linalg.norm(Sigma_g_hat - Sigma_g_analytic, 'fro')\n        denominator = np.linalg.norm(Sigma_g_analytic, 'fro')\n        \n        # Denominator should not be zero for the given test cases\n        if denominator == 0:\n            # This case is unlikely here but is good practice to handle.\n            # If numerator is also 0, error is 0. Otherwise, error is infinite.\n            relative_error = 0.0 if numerator == 0.0 else np.inf\n        else:\n            relative_error = numerator / denominator\n        \n        results.append(relative_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3352079"}]}