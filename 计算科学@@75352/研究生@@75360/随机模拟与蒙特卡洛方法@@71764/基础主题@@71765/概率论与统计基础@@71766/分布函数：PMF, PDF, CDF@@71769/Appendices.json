{"hands_on_practices": [{"introduction": "现实世界中的许多现象无法被完美地归类为纯粹的离散或连续模型。本练习将探讨混合分布，它结合了离散的概率质量（PMF）和连续的概率密度（PDF）。通过为这样一个混合分布设计并验证一个拒绝采样器，您将巩固对这些基本函数（PMF、PDF 和 CDF）如何相互关联的理解，并学会如何将一个理论分布转化为一个实用的随机数生成器。[@problem_id:3304408]", "problem": "给定一个实数轴上的混合目标分布，它由一个带原子的离散部分和一个带密度的连续部分组成。设该目标分布由一组有限的原子位置 $\\{a_i\\}_{i=1}^m$ 及其概率 $\\{p_i\\}_{i=1}^m$ (使得 $\\sum_{i=1}^m p_i = w_{\\mathrm{d}} \\in [0,1]$) 和一个在 $\\mathbb{R}$ 上的连续概率密度函数 (PDF) $f(x)$（权重为 $w_{\\mathrm{c}} = 1 - w_{\\mathrm{d}}$）描述。其累积分布函数 (CDF) 为\n$$\nF(x) \\;=\\; \\sum_{i=1}^m p_i \\,\\mathbf{1}\\{a_i \\le x\\} \\;+\\; w_{\\mathrm{c}} \\int_{-\\infty}^x f(t)\\,dt.\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 表示指示函数。离散部分的概率质量函数 (PMF) 是映射 $i \\mapsto p_i$，连续部分的 PDF 是 $w_{\\mathrm{c}} f(x)$。\n\n你的任务如下。\n\n- 从第一性原理出发，仅使用概率质量函数 (PMF)、概率密度函数 (PDF) 和累积分布函数 (CDF) 的定义，并将接受-拒绝范式作为基本事实，提出一种两阶段拒绝采样算法，该算法将离散原子与连续密度的处理分开。假设给定：\n  - 常数 $\\{c_i\\}_{i=1}^m$，满足对所有 $i$ 都有 $c_i \\ge p_i$，\n  - 一个在 $\\mathbb{R}$ 上的提议密度 $q(x)$ 和一个有限常数 $M \\ge \\sup_x \\frac{f(x)}{q(x)}$，\n  - 一个连续包络常数 $c_{\\mathrm{c}} \\ge w_{\\mathrm{c}} M$，\n  并定义总包络质量 $C \\equiv \\sum_{i=1}^m c_i + c_{\\mathrm{c}}$。\n\n- 从数学上证明，你的算法所产生的接受样本的分布律其 CDF 恰好是 $F(x)$。\n\n- 将你的算法实现为一个程序，该程序对下面的每个测试用例，生成 $n$ 个接受样本，并数值上验证经验累积分布函数在柯尔莫哥洛夫距离\n$$\nD_n \\;\\equiv\\; \\sup_{x \\in \\mathbb{R}} \\big| \\widehat{F}_n(x) - F(x) \\big|,\n$$\n意义下接近 $F(x)$，其中 $\\widehat{F}_n(x)$ 是 $n$ 个接受样本的经验累积分布函数。通过在一个覆盖以下所列分布的有效支撑集的、足够精细的 $\\mathbb{R}$ 网格上进行评估，来近似这个上确界。\n\n你可以依赖的基础知识：\n- PMF、PDF、CDF 和混合分布的定义。\n- 接受-拒绝原则：如果目标非负函数 $t(z)$ 被提议分布 $g$ 的 $M g(z)$ 所控制，那么通过从 $g$ 中提议 $Z \\sim g$ 并以概率 $t(Z)/(M g(Z))$ 接受，可以得到密度与 $t(z)$ 成正比的接受样本。\n\n程序要求：\n- 对每个测试用例使用固定的随机种子以确保可复现性。\n- 对每个测试用例，在一个固定的网格上计算 $D_n$，并返回一个布尔值，指示是否有 $D_n \\le \\tau$，其中 $\\tau$ 是一个指定的容差。\n- 你的程序必须产生单行输出，其中包含所有测试用例的布尔结果，形式为方括号内以逗号分隔的列表，例如 $[\\mathrm{True},\\mathrm{False},\\mathrm{True}]$。\n\n测试套件：\n- 测试用例 1 (带原子的混合拉普拉斯分布):\n  - 原子：$a_1 = -1$, $a_2 = 2$，质量分别为 $p_1 = 0.25$, $p_2 = 0.15$；因此 $w_{\\mathrm{d}} = 0.40$ 且 $w_{\\mathrm{c}} = 0.60$。\n  - 连续密度 $f$ 是位置为 $0$、尺度为 $1$ 的拉普拉斯分布，即 $f(x) = \\frac{1}{2} e^{-|x|}$。\n  - 提议密度 $q$ 与同样的 Laplace$(0,1)$ 密度相等，因此可以取 $M = 1$。\n  - 包络常数：$c_1 = 1.2 \\, p_1$, $c_2 = 1.2 \\, p_2$, $c_{\\mathrm{c}} = 1.5 \\, w_{\\mathrm{c}}$。\n  - 样本量 $n = 50000$ 个接受的抽样，随机种子 $12345$。\n  - 用于 $D_n$ 的网格：在 $[-8,8]$ 上等距分布的 $2001$ 个点，并增补原子点 $\\{-1,2\\}$。\n  - 容差 $\\tau = 0.02$。\n\n- 测试用例 2 (纯连续正态分布):\n  - 无原子 ($m=0$)，因此 $w_{\\mathrm{d}} = 0$ 且 $w_{\\mathrm{c}} = 1$。\n  - 连续密度 $f$ 是均值为 $0$、标准差为 $1.5$ 的正态分布，即 $f(x) = \\frac{1}{1.5 \\sqrt{2\\pi}} \\exp\\!\\big(-\\frac{x^2}{2 \\cdot 1.5^2}\\big)$。\n  - 提议密度 $q$ 与同样的 Normal$(0,1.5)$ 密度相等，因此取 $M = 1$ 且 $c_{\\mathrm{c}} = 1$。\n  - 样本量 $n = 50000$ 个接受的抽样，随机种子 $24680$。\n  - 用于 $D_n$ 的网格：在 $[-8,8]$ 上等距分布的 $2001$ 个点。\n  - 容差 $\\tau = 0.02$。\n\n- 测试用例 3 (纯离散分布):\n  - 原子：$a_1 = 0$, $a_2 = 3$，质量分别为 $p_1 = 0.7$, $p_2 = 0.3$；因此 $w_{\\mathrm{d}} = 1$ 且 $w_{\\mathrm{c}} = 0$。\n  - 无连续部分；选择任意占位的 $f$ 和 $q$，但设置 $c_{\\mathrm{c}} = 0$ 以确保永远不会选择连续分支。\n  - 包络常数：$c_1 = 1.1 \\, p_1$, $c_2 = 1.1 \\, p_2$。\n  - 样本量 $n = 50000$ 个接受的抽样，随机种子 $98765$。\n  - 用于 $D_n$ 的网格：在 $[-2,5]$ 上等距分布的 $1401$ 个点，并增补原子点 $\\{0,3\\}$。\n  - 容差 $\\tau = 0.02$。\n\n输出规范：\n- 你的程序必须在没有用户输入的情况下运行，并以精确格式 $[b_1,b_2,b_3]$ 打印一行，其中如果对应的测试用例满足 $D_n \\le \\tau$，则 $b_j$ 为 $\\mathrm{True}$，否则为 $\\mathrm{False}$。", "solution": "问题陈述已经过仔细审查，并被确定为有效。它在科学上植根于概率论和计算统计学，问题提法清晰，目标明确，并为理论推导和计算实现提供了完整且一致的参数集。\n\n### 第一部分：两阶段拒绝采样算法的推导\n\n目标分布是离散部分和连续部分的混合。其累积分布函数 (CDF) 由下式给出：\n$$\nF(x) = \\sum_{i=1}^m p_i \\mathbf{1}\\{a_i \\le x\\} + w_{\\mathrm{c}} \\int_{-\\infty}^x f(t)\\,dt\n$$\n其中 $\\{a_i\\}_{i=1}^m$ 是原子位置及其概率 $\\{p_i\\}_{i=1}^m$，$w_{\\mathrm{d}} = \\sum p_i$ 且 $w_{\\mathrm{c}} = 1 - w_{\\mathrm{d}}$。函数 $f(x)$ 是一个概率密度函数 (PDF)。\n\n由于存在离散质量点（原子），这样的分布在标准意义上没有 PDF。为了应用拒绝采样原则，我们可以使用一个包含狄拉克 $\\delta$ 函数 $\\delta(\\cdot)$ 的广义密度函数来描述目标分布：\n$$\nt(x) = \\sum_{i=1}^m p_i \\delta(x - a_i) + w_{\\mathrm{c}} f(x)\n$$\n函数 $t(x)$ 不是一个密度函数，而是一个正函数，其在 $\\mathbb{R}$ 上的积分为 $\\sum p_i + w_c \\int f(t) dt = w_d + w_c = 1$。我们的目标是从 $t(x)$ 所描述的分布律中生成样本。\n\n拒绝采样法需要一个提议分布和一个控制目标的包络函数。问题为离散部分提供了常数 $\\{c_i\\}_{i=1}^m$，为连续部分提供了 $c_{\\mathrm{c}}$。我们用这些常数来构建一个类似混合的包络过程。这就导出了一个两阶段算法。\n\n让我们在 $m+1$ 个索引 $\\{0, 1, \\dots, m\\}$ 上定义一个分类分布。索引 $0$ 对应连续部分，索引 $i \\in \\{1, \\dots, m\\}$ 对应离散原子。我们定义选择这些类别的概率如下：\n$$\n\\pi_0 = \\frac{c_{\\mathrm{c}}}{C}, \\quad \\pi_i = \\frac{c_i}{C} \\quad \\text{for } i \\in \\{1, \\dots, m\\}\n$$\n其中 $C = \\sum_{i=1}^m c_i + c_{\\mathrm{c}}$ 是总包络质量。注意 $\\sum_{i=0}^m \\pi_i = 1$，所以这是一个有效的概率分布。\n\n该算法首先抽取一个类别，然后根据该类别提议一个值，最后执行拒绝测试。\n\n**两阶段拒绝采样算法：**\n\n要生成一个接受样本，重复以下步骤直至样本被接受：\n\n1.  **阶段 1：选择部分。**\n    从集合 $\\{0, 1, \\dots, m\\}$ 中以概率 $P(J=j) = \\pi_j$ 抽取一个类别索引 $J$。\n\n2.  **阶段 2：提议与接受/拒绝。**\n    - 如果 $J = i$ (对于某个 $i \\in \\{1, \\dots, m\\}$) (选择了离散部分)：\n        a. 提议确定性值 $X = a_i$。\n        b. 抽取一个均匀随机变量 $U \\sim U(0,1)$。\n        c. 如果 $U \\le \\frac{p_i}{c_i}$，则接受提议 $X=a_i$。条件 $c_i \\ge p_i$ 确保此概率至多为 $1$。如果接受，样本即为 $a_i$；终止循环。\n\n    - 如果 $J = 0$ (选择了连续部分)：\n        a. 从提议密度 $q(x)$ 中抽取一个提议值 $X$。\n        b. 抽取一个均匀随机变量 $U \\sim U(0,1)$。\n        c. 如果 $U \\le \\frac{w_{\\mathrm{c}} f(X)}{c_{\\mathrm{c}} q(X)}$，则接受提议 $X$。条件 $M \\ge \\sup_x \\frac{f(x)}{q(x)}$ 和 $c_{\\mathrm{c}} \\ge w_{\\mathrm{c}} M$ 确保对所有 $x$ 都有 $w_{\\mathrm{c}} f(x) \\le w_{\\mathrm{c}} M q(x) \\le c_{\\mathrm{c}} q(x)$，因此接受概率至多为 $1$。如果接受，样本即为 $X$；终止循环。\n\n### 第二部分：正确性的数学证明\n\n令 $Y$ 为算法生成的一个接受样本所代表的随机变量。我们必须证明 $Y$ 的 CDF，$F_Y(x) = P(Y \\le x)$，等于目标 CDF $F(x)$。\n\n首先，我们计算在循环的单次迭代中接受一个提议的总概率，记为 $P(\\text{accept})$。\n$$\nP(\\text{accept}) = \\sum_{j=0}^m P(\\text{accept} | J=j) P(J=j)\n$$\n对于离散部分 ($j=i > 0$)：\n$P(\\text{accept} | J=i) = P\\left(U \\le \\frac{p_i}{c_i}\\right) = \\frac{p_i}{c_i}$。\n对于连续部分 ($j=0$)：\n$P(\\text{accept} | J=0) = \\int_{-\\infty}^{\\infty} P(\\text{accept } X | X=t, J=0) q(t) dt = \\int_{-\\infty}^{\\infty} \\frac{w_{\\mathrm{c}} f(t)}{c_{\\mathrm{c}} q(t)} q(t) dt = \\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}} \\int_{-\\infty}^{\\infty} f(t) dt = \\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}}$，因为 $f(x)$ 是一个 PDF。\n\n结合这些，总接受概率是：\n$$\nP(\\text{accept}) = P(\\text{accept}|J=0)P(J=0) + \\sum_{i=1}^m P(\\text{accept}|J=i)P(J=i)\n$$\n$$\nP(\\text{accept}) = \\left(\\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}}\\right) \\left(\\frac{c_{\\mathrm{c}}}{C}\\right) + \\sum_{i=1}^m \\left(\\frac{p_i}{c_i}\\right) \\left(\\frac{c_i}{C}\\right) = \\frac{w_{\\mathrm{c}}}{C} + \\sum_{i=1}^m \\frac{p_i}{C} = \\frac{w_{\\mathrm{c}} + w_{\\mathrm{d}}}{C} = \\frac{1}{C}\n$$\n现在我们推导接受样本 $Y$ 的 CDF。根据条件概率的定义：\n$$\nF_Y(x) = P(Y \\le x) = \\frac{P(\\text{accepted proposal } X \\le x)}{P(\\text{accept})}\n$$\n分子是在一次迭代中，生成一个小于等于 $x$ *并且*被接受的提议的概率。我们通过全概率公式，以所选部分 $J$ 为条件来计算这个概率。\n$$\nP(\\text{accepted proposal } X \\le x) = \\sum_{j=0}^{m} P(\\text{accepted } X \\le x | J=j) P(J=j)\n$$\n- 对于 $j=0$ (连续)：\n提议值为 $X \\sim q(x)$。事件为 $X \\le x$ 且被接受。\n$$\nP(\\text{accepted } X \\le x | J=0) = \\int_{-\\infty}^x P(\\text{accept at } t | J=0) q(t) dt = \\int_{-\\infty}^x \\frac{w_{\\mathrm{c}} f(t)}{c_{\\mathrm{c}} q(t)} q(t) dt = \\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}} \\int_{-\\infty}^x f(t) dt\n$$\n来自此分支的总贡献是 $\\left(\\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}} \\int_{-\\infty}^x f(t) dt\\right) \\cdot P(J=0) = \\left(\\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}} \\int_{-\\infty}^x f(t) dt\\right) \\cdot \\frac{c_{\\mathrm{c}}}{C} = \\frac{w_{\\mathrm{c}}}{C} \\int_{-\\infty}^x f(t) dt$。\n\n- 对于 $j=i > 0$ (离散)：\n提议值为常数 $a_i$。事件为 $a_i \\le x$ 且被接受。\n$$\nP(\\text{accepted } X \\le x | J=i) = P(a_i \\le x \\text{ and } U \\le p_i/c_i) = P(a_i \\le x) \\cdot P(U \\le p_i/c_i) = \\mathbf{1}\\{a_i \\le x\\} \\cdot \\frac{p_i}{c_i}\n$$\n来自此分支的总贡献是 $\\left(\\mathbf{1}\\{a_i \\le x\\} \\frac{p_i}{c_i}\\right) \\cdot P(J=i) = \\left(\\mathbf{1}\\{a_i \\le x\\} \\frac{p_i}{c_i}\\right) \\cdot \\frac{c_i}{C} = \\frac{p_i}{C} \\mathbf{1}\\{a_i \\le x\\}$。\n\n将所有对分子的贡献相加：\n$$\nP(\\text{accepted proposal } X \\le x) = \\frac{w_{\\mathrm{c}}}{C} \\int_{-\\infty}^x f(t) dt + \\sum_{i=1}^m \\frac{p_i}{C} \\mathbf{1}\\{a_i \\le x\\} = \\frac{1}{C} \\left( w_{\\mathrm{c}} \\int_{-\\infty}^x f(t) dt + \\sum_{i=1}^m p_i \\mathbf{1}\\{a_i \\le x\\} \\right)\n$$\n最后，我们计算接受样本 $Y$ 的 CDF：\n$$\nF_Y(x) = \\frac{P(\\text{accepted proposal } X \\le x)}{P(\\text{accept})} = \\frac{\\frac{1}{C} \\left( w_{\\mathrm{c}} \\int_{-\\infty}^x f(t) dt + \\sum_{i=1}^m p_i \\mathbf{1}\\{a_i \\le x\\} \\right)}{1/C}\n$$\n$$\nF_Y(x) = \\sum_{i=1}^m p_i \\mathbf{1}\\{a_i \\le x\\} + w_{\\mathrm{c}} \\int_{-\\infty}^x f(t) dt\n$$\n这恰好是目标 CDF $F(x)$。证明完毕。Q.E.D.\n\n### 第三部分：实现与数值验证\n\n所推导的算法针对三个指定的测试用例进行实现。对于每种情况，生成 $n$ 个样本，并通过在指定的网格上计算柯尔莫哥洛夫距离 $D_n$，将经验 CDF $\\widehat{F}_n(x)$ 与真实 CDF $F(x)$ 进行比较。结果是一个布尔值，指示 $D_n$ 是否在给定的容差 $\\tau$ 之内。\n\n经验 CDF 计算公式为 $\\widehat{F}_n(x) = \\frac{1}{n} \\sum_{k=1}^n \\mathbf{1}\\{Y_k \\le x\\}$，其中 $\\{Y_k\\}_{k=1}^n$ 是生成的样本。通过取测试网格上各点的最大绝对差来近似 $D_n$ 的上确界。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import laplace, norm\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for the two-stage rejection sampler.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1: Mixed Laplace with atoms\n        {\n            \"atoms\": np.array([-1.0, 2.0]),\n            \"probs\": np.array([0.25, 0.15]),\n            \"f_dist\": laplace(loc=0, scale=1),\n            \"q_dist\": laplace(loc=0, scale=1),\n            \"M\": 1.0,\n            \"c_factors\": (1.2, 1.5),  # (discrete_factor, continuous_factor)\n            \"n_samples\": 50000,\n            \"seed\": 12345,\n            \"grid_params\": (-8.0, 8.0, 2001),\n            \"tolerance\": 0.02,\n        },\n        # Test case 2: Pure continuous normal\n        {\n            \"atoms\": np.array([]),\n            \"probs\": np.array([]),\n            \"f_dist\": norm(loc=0, scale=1.5),\n            \"q_dist\": norm(loc=0, scale=1.5),\n            \"M\": 1.0,\n            \"c_factors\": (1.0, 1.0), # Factors are not really used here, c_c=1\n            \"n_samples\": 50000,\n            \"seed\": 24680,\n            \"grid_params\": (-8.0, 8.0, 2001),\n            \"tolerance\": 0.02,\n        },\n        # Test case 3: Purely discrete\n        {\n            \"atoms\": np.array([0.0, 3.0]),\n            \"probs\": np.array([0.7, 0.3]),\n            \"f_dist\": None,  # Placeholder\n            \"q_dist\": None,  # Placeholder\n            \"M\": 1.0, # Placeholder\n            \"c_factors\": (1.1, 0.0), # No continuous part\n            \"n_samples\": 50000,\n            \"seed\": 98765,\n            \"grid_params\": (-2.0, 5.0, 1401),\n            \"tolerance\": 0.02,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(run_test_case(**case))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef run_test_case(atoms, probs, f_dist, q_dist, M, c_factors, n_samples, seed, grid_params, tolerance):\n    \"\"\"\n    Executes a single test case for the sampler and returns the verification result.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # --- 1. Setup sampler parameters ---\n    m = len(atoms)\n    w_d = np.sum(probs)\n    w_c = 1.0 - w_d\n\n    discrete_c_factor, continuous_c_factor = c_factors\n    \n    # Envelope constants\n    c_i = discrete_c_factor * probs\n    \n    if w_c > 0:\n        c_c = continuous_c_factor * w_c * M\n    else:\n        # Handles pure discrete case where w_c is 0\n        c_c = 0.0\n        \n    C = np.sum(c_i) + c_c\n\n    # Category selection probabilities (pi_0 for continuous, pi_1...pi_m for discrete)\n    pi_vec = np.zeros(m + 1)\n    if C > 0:\n        pi_vec[0] = c_c / C\n        pi_vec[1:] = c_i / C\n    else: # This branch is for cases like pure continuous with M=1, c_c=1 -> C=1\n        if m == 0 and w_c == 1.0:\n            C = 1.0\n            c_c = 1.0\n            pi_vec[0] = 1.0\n            \n    # --- 2. Define the sampler function ---\n    def sampler():\n        while True:\n            # Stage 1: Select Component\n            j = rng.choice(m + 1, p=pi_vec)\n            \n            # Stage 2: Propose and Accept/Reject\n            if j == 0:  # Continuous component\n                if c_c == 0: continue # Should not happen if pi_vec[0]>0, but for safety\n                \n                x_proposal = q_dist.rvs(random_state=rng)\n                u = rng.uniform(0, 1)\n                \n                f_val = f_dist.pdf(x_proposal)\n                q_val = q_dist.pdf(x_proposal)\n                \n                # Avoid division by zero if q(x)=0. If f(x) is also 0, prob is 0.\n                if q_val > 0:\n                    acceptance_prob = (w_c * f_val) / (c_c * q_val)\n                    if u = acceptance_prob:\n                        return x_proposal\n            else:  # Discrete component j-1\n                i = j - 1\n                x_proposal = atoms[i]\n                u = rng.uniform(0, 1)\n                \n                acceptance_prob = probs[i] / c_i[i]\n                if u = acceptance_prob:\n                    return x_proposal\n\n    # --- 3. Generate samples ---\n    samples = np.array([sampler() for _ in range(n_samples)])\n\n    # --- 4. Define true CDF ---\n    def F_true(x):\n        F_val = np.zeros_like(x, dtype=float)\n        # Discrete part\n        for i in range(m):\n            F_val += probs[i] * (x >= atoms[i])\n        # Continuous part\n        if w_c > 0:\n            F_val += w_c * f_dist.cdf(x)\n        return F_val\n\n    # --- 5. Compute Kolmogorov distance ---\n    grid_start, grid_end, grid_points = grid_params\n    x_grid = np.linspace(grid_start, grid_end, grid_points)\n    # Augment grid with atom locations\n    if m > 0:\n        x_grid = np.unique(np.concatenate((x_grid, atoms)))\n    \n    # Calculate true CDF on the grid\n    true_cdf_on_grid = F_true(x_grid)\n    \n    # Calculate empirical CDF on the grid\n    sorted_samples = np.sort(samples)\n    empirical_cdf_on_grid = np.searchsorted(sorted_samples, x_grid, side='right') / n_samples\n    \n    # Kolmogorov distance\n    D_n = np.max(np.abs(empirical_cdf_on_grid - true_cdf_on_grid))\n    \n    return D_n = tolerance\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3304408"}, {"introduction": "在许多应用中，尤其是在模拟和数据分析领域，我们需要快速且重复地查询离散分布的累积分布函数（CDF）。本练习将离散CDF的数学定义与高效算法设计的原理联系起来。它要求您利用如树状数组（Fenwick tree）等数据结构，将CDF查询的时间复杂度从线性时间$O(n)$降低到对数时间$O(\\log n)$，从而培养您分析计算瓶颈并应用恰当算法解决方案的能力。[@problem_id:3304360]", "problem": "考虑一个离散随机变量 $X$，其结果为有限的实数值集合 $\\{x_1, x_2, \\dots, x_n\\}$，并具有概率质量函数 (PMF) $p_X(x_i)$，满足对所有 $i$ 都有 $p_X(x_i) \\ge 0$ 且 $\\sum_{i=1}^n p_X(x_i) = 1$。累积分布函数 (CDF) $F_X(x)$ 由基本定律 $F_X(x) = \\mathbb{P}(X \\le x) = \\sum_{i: x_i \\le x} p_X(x_i)$ 定义。您必须设计一个累积和数据结构，该结构能够对任意查询值 $x \\in \\mathbb{R}$ 在 $O(\\log n)$ 时间内评估 $F_X(x)$，并从第一性原理出发分析内存与查询之间的权衡。\n\n仅从 PMF 和 CDF 的核心定义出发，提出并论证一种数据结构和算法策略，该策略：\n- 构建一个基于按实数值排序的结果的累积和结构，支持对 $F_X(x)$ 进行 $O(\\log n)$ 时间的查询。\n- （可选）支持在 $O(\\log n)$ 时间内对 PMF 进行点更新（其中一个结果的概率增加 $\\Delta$，另一个结果的概率减少相同的 $\\Delta$，以保持归一化 $\\sum_i p_X(x_i) = 1$），并解释这对内存和查询性能的影响。\n\n您的程序必须实现您提出的数据结构，并为以下测试套件生成数值结果。分布参数和查询值以显式列表形式给出。所有概率必须作为小数处理（无百分号），且本问题中没有物理单位。\n\n测试套件：\n- 测试用例 $1$（一般情况）：结果 $[1.0,\\,2.5,\\,3.7,\\,5.0,\\,8.0]$，概率 $[0.1,\\,0.2,\\,0.05,\\,0.25,\\,0.4]$，查询 $[0.0,\\,1.0,\\,2.0,\\,2.5,\\,10.0]$。\n- 测试用例 $2$（边界条件和零值）：结果 $[-5.0,\\,-1.0,\\,0.0,\\,7.0]$，概率 $[0.0,\\,0.2,\\,0.3,\\,0.5]$，查询 $[-10.0,\\,-5.0,\\,-1.0,\\,0.0,\\,7.0,\\,100.0]$。\n- 测试用例 $3$（具有结构化分布和保持归一化的更新的边缘情况）：结果 $\\left[1^2,\\,2^2,\\,3^2,\\,\\dots,\\,16^2\\right] = [1,\\,4,\\,9,\\,16,\\,25,\\,36,\\,49,\\,64,\\,81,\\,100,\\,121,\\,144,\\,169,\\,196,\\,225,\\,256]$，概率 $p_i = \\dfrac{i}{\\sum_{j=1}^{16} j}$ (对于 $i \\in \\{1,2,\\dots,16\\}$)，应用一个更新 $\\Delta = 0.01$，使 $p_1$ 增加 $\\Delta$，$p_{16}$ 减少 $\\Delta$（结果概率必须保持非负且总和仍为 $1$），查询 $[0.0,\\,50.0,\\,100.0,\\,256.0]$。\n\n要求：\n- 您的数据结构必须通过首先对排序后的结果进行二分搜索，找到满足 $x_k \\le x$ 的最大索引 $k$，然后使用您的累积和结构返回截至 $k$ 的累积概率质量，从而为每个 CDF 查询 $F_X(x)$ 实现 $O(\\log n)$ 的时间复杂度。\n- 您的分析必须从原理上讨论在选择静态前缀和数组、二进制索引树（Fenwick 树）和线段树时，内存与查询/更新性能之间的权衡。从关于累积和与二分搜索的标准事实出发，论证 $O(\\log n)$ 的查询复杂度和内存界限。\n- 您的程序必须内嵌测试套件，并且无需任何外部输入即可运行。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。该列表必须按上述指定顺序（首先是测试用例1的查询，然后是测试用例2，最后是测试用例3）包含所有测试用例的所有查询的 CDF 值（作为浮点数），即输出格式为：$[r_1, r_2, \\dots, r_m]$，其中每个 $r_i$ 是一个浮点数，$m$ 是所有测试用例中的查询总数。", "solution": "我们从基本定义开始。对于一个离散随机变量 $X$，其结果为 $\\{x_1, x_2, \\dots, x_n\\}$，概率质量函数 (PMF) 为 $p_X(x_i)$，则累积分布函数 (CDF) 定义为\n$$\nF_X(x) = \\mathbb{P}(X \\le x) = \\sum_{i: x_i \\le x} p_X(x_i).\n$$\n如果结果按升序排序，即 $x_1 \\le x_2 \\le \\dots \\le x_n$，那么对于任何查询值 $x$，评估 $F_X(x)$ 需要定位满足 $x_k \\le x$ 的最大索引 $k$，然后计算部分和 $\\sum_{i=1}^k p_X(x_i)$。\n\n一个基准方法是使用二分搜索在 $O(\\log n)$ 时间内评估 $k$，然后通过扫描在 $O(n)$ 时间内计算 $\\sum_{i=1}^k p_X(x_i)$。这导致每次查询的时间复杂度为 $O(n)$，并非最优。为了改进到每次查询 $O(\\log n)$，我们需要一个累积和数据结构，该结构支持在 $O(\\log n)$ 时间（或更快）内计算前缀和，从而使总复杂度由定位 $k$ 的二分搜索和前缀和评估主导。\n\n累积和的核心原理是，部分和可以存储在一个分层结构中，该结构将索引集分解为长度为2的幂的区间。这种分解源于整数的二进制表示。两种标准的数据结构体现了这一思想：\n\n1. 静态前缀和数组：\n   - 预计算前缀和数组 $S$，其中 $S_k = \\sum_{i=1}^k p_X(x_i)$，对于 $k = 1, 2, \\dots, n$。\n   - 查询：通过二分搜索找到 $k$ 并返回 $S_k$。搜索复杂度为 $O(\\log n)$，访问前缀和为 $O(1)$，总查询复杂度为 $O(\\log n)$。\n   - 更新：对 $p_X(x_i)$ 的点更新需要更新所有 $k \\ge i$ 的 $S_k$，在最坏情况下需要 $O(n)$ 时间。\n   - 内存：$O(n)$ 用于存储 $S$。\n\n2. 二进制索引树（Fenwick 树）：\n   - 存储一个长度为 $n$ 的树状数组 $T$，其中每个条目表示大小为2的幂的区间上的部分和。树的排列方式使得每个索引 $i$ 对一组由 $i$ 的最低有效位决定的节点有贡献，并被这些节点覆盖。\n   - 前缀和操作：\n     对于一个基于1的索引 $k$，前缀和通过迭代累加 $T[k]$，然后设置 $k \\leftarrow k - (k  -k)$，并重复此过程直到 $k=0$ 来获得。这里，$(k  -k)$ 提取 $k$ 的最低有效位，这决定了结束于 $k$ 且包含在累积和中的最大2的幂次块。每一步都通过移除 $k$ 的最低有效2的幂次分量来减小 $k$，因此步数受 $k$ 的位数限制，即 $O(\\log n)$。\n     形式上，操作为\n     $$\n     \\text{prefix\\_sum}(k) = \\sum_{\\text{iterates over }k} T[k], \\quad k \\leftarrow k - (k  -k).\n     $$\n   - 点更新操作：\n     要将 $\\Delta$ 加到 $p_X(x_i)$ 上，我们更新 $T[i] \\leftarrow T[i] + \\Delta$，然后设置 $i \\leftarrow i + (i  -i)$ 并重复直到 $i  n$。这是沿树向上遍历，并更新所有覆盖索引 $i$ 的部分和。每一步都通过最低有效2的幂次分量来推进 $i$，因此步数为 $O(\\log n)$。\n     形式上，\n     $$\n     \\text{update}(i, \\Delta):\\quad \\text{while } i \\le n:~ T[i] \\leftarrow T[i] + \\Delta;~ i \\leftarrow i + (i  -i).\n     $$\n   - 查询：通过在结果上进行二分搜索（$O(\\log n)$）找到 $k$，并返回 $\\text{prefix\\_sum}(k)$（$O(\\log n)$），总查询复杂度为 $O(\\log n)$。\n   - 内存：$O(n)$ 用于存储 $T$；与前缀和数组具有相同的渐近内存，但访问模式不同。\n   - 优点：支持 $O(\\log n)$ 的点更新，使其适用于 PMF 随时间变化但通过补偿性更新保持归一化的情况。\n\n3. 线段树：\n   - 在二叉树结构中存储区间上的和。前缀和或一般范围和的查询以及点更新都在 $O(\\log n)$ 时间内运行。\n   - 内存：由于树节点开销，通常为 $O(2n)$到 $O(4n)$，比二进制索引树大。\n   - 优点：支持更一般的范围查询；缺点：内存开销更大，实现更复杂。\n\n权衡分析：\n- 如果分布是静态的，前缀和数组加二分搜索对于查询是最优的：每次查询 $O(\\log n)$，实现复杂度最低，内存为 $O(n)$。但更新成本高昂，每次更新为 $O(n)$。\n- 如果分布需要动态点更新同时保持 $\\sum_i p_X(x_i) = 1$，二进制索引树能以 $O(n)$ 内存实现查询和更新均为 $O(\\log n)$。这在两种操作之间取得了性能平衡。\n- 线段树支持最广泛的查询范围，但内存开销更高（通常是 $n$ 的一个较大常数倍），且更复杂；它适用于查询超出前缀和范围的情况。\n\n此问题的算法设计：\n- 将结果 $\\{x_i\\}$ 与其概率一起按升序排序，使索引对齐。\n- 对于查询 $x$，使用二分搜索找到 $k = \\max\\{i : x_i \\le x\\}$，如果 $x  x_1$ 则 $k = 0$。此二分搜索为 $O(\\log n)$。\n- 使用前缀和数组或二进制索引树来获取 $\\sum_{i=1}^k p_X(x_i)$：\n  - 使用前缀和数组，在 $O(1)$ 时间内返回 $S_k$。\n  - 使用二进制索引树，在 $O(\\log n)$ 时间内返回 $\\text{prefix\\_sum}(k)$。\n- 对于动态更新（如第三个测试用例中），对一个索引 $i$ 应用 $\\Delta$，对另一个索引 $j$ 应用 $-\\Delta$，以保持归一化。在二进制索引树中，使用 $\\text{update}(i, \\Delta)$ 和 $\\text{update}(j, -\\Delta)$ 应用两个更新，每个更新时间为 $O(\\log n)$。由于 $\\Delta$ 的选择保证了概率保持非负，PMF 仍然有效。\n\n从第一性原理出发的复杂度论证：\n- 对 $n$ 个已排序结果进行二分搜索需要将查询值 $x$ 与结果进行比较，并通过将区间减半来缩小搜索区域；步数是 $O(\\log n)$。\n- 二进制索引树的前缀和操作通过在每一步移除其最低有效2的幂次贡献来减小索引，这受索引的位数限制；因此是 $O(\\log n)$ 步。\n- 前缀和数组和二进制索引树的内存都是 $n$ 的线性函数，因为它们每个索引存储一个实数（直到常数因子开销）。\n\n程序输出：\n- 程序使用二进制索引树计算三个测试用例中所有查询的 CDF 值。对于测试用例3，它在回答查询之前应用指定的保持归一化的更新。\n- 最终输出是包含所有 CDF 结果的扁平化列表的单行：$[r_1, r_2, \\dots, r_m]$，其中每个 $r_i$ 是对应于指定顺序中给定查询的 $F_X(x)$ 的浮点数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport bisect\nfrom typing import List, Tuple\n\nclass FenwickTree:\n    \"\"\"\n    Fenwick Tree (Binary Indexed Tree) for prefix sums over a 1-based array.\n    Supports:\n    - build in O(n)\n    - point updates in O(log n)\n    - prefix sums in O(log n)\n    \"\"\"\n    def __init__(self, size: int):\n        self.n = size\n        # 1-based indexing for tree\n        self.tree = [0.0] * (self.n + 1)\n\n    @staticmethod\n    def _lsb(i: int) -> int:\n        return i  -i\n\n    def build(self, arr: List[float]) -> None:\n        # Build Fenwick tree in O(n) time.\n        # Copy arr into tree and propagate contributions.\n        for i in range(1, self.n + 1):\n            self.tree[i] += arr[i - 1]\n            j = i + (i  -i)\n            if j = self.n:\n                self.tree[j] += self.tree[i]\n\n    def update(self, index_1based: int, delta: float) -> None:\n        i = index_1based\n        while i = self.n:\n            self.tree[i] += delta\n            i += (i  -i)\n\n    def prefix_sum(self, index_1based: int) -> float:\n        s = 0.0\n        i = index_1based\n        while i > 0:\n            s += self.tree[i]\n            i -= (i  -i)\n        return s\n\n\nclass CDFEvaluator:\n    \"\"\"\n    Evaluates F_X(x) = sum_{i: x_i = x} p_i in O(log n) time using:\n    - binary search over sorted outcomes to find k = max{i: x_i = x}\n    - Fenwick tree prefix sums to compute sum_{i=1..k} p_i\n    Also supports point updates to probabilities with Fenwick tree propagation.\n    \"\"\"\n    def __init__(self, outcomes: List[float], probabilities: List[float]):\n        assert len(outcomes) == len(probabilities), \"Outcomes and probabilities length mismatch.\"\n        n = len(outcomes)\n        # Sort by outcomes, keep aligned probabilities\n        paired = sorted(zip(outcomes, probabilities), key=lambda t: t[0])\n        self.outcomes = [t[0] for t in paired]\n        self.probabilities = [t[1] for t in paired]\n        self.n = n\n        # Build Fenwick tree\n        self.ft = FenwickTree(n)\n        self.ft.build(self.probabilities)\n\n    def query_cdf(self, x: float) -> float:\n        # Find largest index k such that outcomes[k-1] = x\n        # bisect_right returns insertion point, so k = insertion_index\n        k = bisect.bisect_right(self.outcomes, x)\n        if k == 0:\n            return 0.0\n        return self.ft.prefix_sum(k)\n\n    def point_update(self, index_zero_based: int, delta: float) -> None:\n        \"\"\"\n        Update probability at a given zero-based index by delta; the caller must\n        ensure normalization across paired updates.\n        \"\"\"\n        # Apply update to internal probability array\n        self.probabilities[index_zero_based] += delta\n        # Propagate update to Fenwick tree\n        self.ft.update(index_zero_based + 1, delta)\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Test case 1\n    outcomes1 = [1.0, 2.5, 3.7, 5.0, 8.0]\n    probs1 = [0.1, 0.2, 0.05, 0.25, 0.4]\n    queries1 = [0.0, 1.0, 2.0, 2.5, 10.0]\n\n    # Test case 2\n    outcomes2 = [-5.0, -1.0, 0.0, 7.0]\n    probs2 = [0.0, 0.2, 0.3, 0.5]\n    queries2 = [-10.0, -5.0, -1.0, 0.0, 7.0, 100.0]\n\n    # Test case 3\n    outcomes3 = [i * i for i in range(1, 17)]  # 1^2 to 16^2\n    total_weight = sum(range(1, 17))\n    probs3 = [i / total_weight for i in range(1, 17)]\n    # Apply normalization-preserving update: +0.01 to p1, -0.01 from p16\n    delta = 0.01\n    queries3 = [0.0, 50.0, 100.0, 256.0]\n\n    # Initialize evaluators\n    evaluator1 = CDFEvaluator(outcomes1, probs1)\n    evaluator2 = CDFEvaluator(outcomes2, probs2)\n    evaluator3 = CDFEvaluator(outcomes3, probs3)\n\n    # Apply updates for test case 3\n    # Need to locate the indices corresponding to outcomes 1 and 256 in evaluator3's sorted outcomes\n    # Since outcomes3 is already sorted ascending, index 0 -> 1, index 15 -> 256\n    # Ensure nonnegativity after update\n    # p1 increases by delta, p16 decreases by delta\n    evaluator3.point_update(0, delta)\n    evaluator3.point_update(15, -delta)\n\n    results: List[float] = []\n\n    # Compute results for test case 1\n    for x in queries1:\n        results.append(evaluator1.query_cdf(x))\n\n    # Compute results for test case 2\n    for x in queries2:\n        results.append(evaluator2.query_cdf(x))\n\n    # Compute results for test case 3\n    for x in queries3:\n        results.append(evaluator3.query_cdf(x))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3304360"}, {"introduction": "在蒙特卡洛模拟中，估计罕见事件的概率（例如，极端金融损失或系统故障的可能性）是一项常见但艰巨的任务，朴素的模拟方法往往效率低下。本练习介绍了一种强大的方差缩减技术——重要性采样。您将设计一个估计量，它将计算精力集中在关键区域，从而更有效地估计一个很小的CDF值（即当$t$处于分布的尾部时$F_X(t)$的值）。通过比较您的重要性采样估计量与朴素估计量的方差，您将获得量化高级蒙特卡洛方法效率的实践经验。[@problem_id:3304377]", "problem": "请使用专注于 $t$ 邻域的重要性采样（IS）方法，为重尾随机变量 $X$ 的累积分布函数（CDF）$F_X(t) = \\mathbb{P}(X \\le t)$ 设计一个无偏蒙特卡洛（MC）估计量，并当 $t$ 移向尾部时，将其单样本方差与朴素采样的方差进行比较。\n\n设 $X$ 是一个重尾连续随机变量，其概率密度函数（PDF）为 $f_X(x)$，累积分布函数（CDF）为 $F_X(t) = \\int_{-\\infty}^t f_X(x)\\,\\mathrm{d}x$。考虑为位于左尾部的阈值 $t$ 估计 $F_X(t)$。你将通过从一个将概率质量集中在 $t$ 附近的提议密度 $g_t(x)$ 中采样来构建一个无偏IS估计量，并将其单样本方差与朴素指示函数估计量的单样本方差进行比较。\n\n使用的基本原理和定义：\n- 具有概率密度函数（PDF）$f_X$ 的连续随机变量 $X$ 的累积分布函数（CDF）为 $F_X(t) = \\mathbb{P}(X \\le t) = \\int_{-\\infty}^t f_X(x)\\,\\mathrm{d}x$。\n- 基于从 $f_X$ 抽取的独立同分布样本 $X_1,\\dots,X_n \\sim f_X$ 的 $F_X(t)$ 的朴素MC估计量是 $\\widehat{F}_{\\text{naive}}(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}$，其单样本方差为 $V_{\\text{naive}}(t) = F_X(t)\\left(1 - F_X(t)\\right)$。\n- 重要性采样恒等式：对于任意可积函数 $h(x)$ 和任意支撑集覆盖 $f_X$ 支撑集的提议密度 $g(x)$，有 $\\mathbb{E}_f[h(X)] = \\mathbb{E}_g\\!\\left[h(X)\\,\\frac{f_X(X)}{g(X)}\\right]$。\n\n目标分布和提议分布族的规范：\n- 重尾目标分布：自由度 $\\nu = 3$ 的学生t分布，记为 $X \\sim t_\\nu$ (其中 $\\nu = 3$)。设其PDF为 $f_\\nu(x)$，CDF为 $F_\\nu(t)$。\n- 集中于阈值的提议分布：对于每个阈值 $t$，使用高斯提议分布 $g_t(x) = \\mathcal{N}(t, \\sigma^2)$，其中 $\\sigma = 1$。该提议分布具有已知闭式解的PDF $g_t(x)$ 和对数PDF $\\log g_t(x)$。\n\n无偏IS估计量设计：\n- 对于一个固定的阈值 $t$，定义IS估计量\n$$\n\\widehat{F}_{\\text{IS}}(t) \\;=\\; \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}\\,w_t(X_i),\n\\quad X_i \\overset{\\text{i.i.d.}}{\\sim} g_t,\\quad w_t(x) \\;=\\; \\frac{f_\\nu(x)}{g_t(x)}.\n$$\n- 从第一性原理出发，证明 $\\widehat{F}_{\\text{IS}}(t)$ 是无偏的，并根据 $g_t$ 下的二阶矩求出其单样本方差：\n$$\nV_{\\text{IS}}(t) \\;=\\; \\mathbb{E}_{g_t}\\!\\left[\\left(\\mathbf{1}\\{X \\le t\\}\\,w_t(X)\\right)^2\\right] \\;-\\; \\left(F_\\nu(t)\\right)^2.\n$$\n\n数值比较任务：\n- 对于每个阈值 $t$，计算单样本方差之比\n$$\nR(t) \\;=\\; \\frac{V_{\\text{naive}}(t)}{V_{\\text{IS}}(t)} \\;=\\; \\frac{F_\\nu(t)\\left(1 - F_\\nu(t)\\right)}{\\mathbb{E}_{g_t}\\!\\left[\\left(\\mathbf{1}\\{X \\le t\\}\\,w_t(X)\\right)^2\\right] - \\left(F_\\nu(t)\\right)^2}.\n$$\n- 分子 $F_\\nu(t)\\left(1 - F_\\nu(t)\\right)$ 必须根据自由度 $\\nu = 3$ 的学生t分布的CDF精确计算。分母的第一项 $\\mathbb{E}_{g_t}[\\cdot]$ 必须通过从 $g_t$ 采样并对被积函数求平均的MC方法进行近似。使用一个大的辅助MC预算 $M$ 来稳健地估计这个二阶矩。对于所有阈值，你必须使用 $\\sigma = 1$ 和 $M = 400000$。\n\n测试套件：\n- 自由度：$\\nu = 3$。\n- 提议分布标准差：$\\sigma = 1$。\n- 用于二阶矩估计的蒙特卡洛预算：$M = 400000$。\n- 阈值：$t \\in \\{-0.5,\\,-2.0,\\,-3.0,\\,-4.0,\\,-5.0\\}$。\n\n随机性：\n- 为了可复现性，使用固定的随机种子 $s = 123456$。\n\n要求的最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，即列表\n$$\n\\left[ R(-0.5),\\, R(-2.0),\\, R(-3.0),\\, R(-4.0),\\, R(-5.0) \\right],\n$$\n每个条目四舍五入到六位小数。\n\n注释：\n- 所有计算都是无量纲的；不涉及物理单位。\n- 确保 $f_\\nu$ 相对于 $g_t$ 的绝对连续性得到满足，并仅限于连续情况（不需要概率质量函数（PMF））。\n- 你的实现必须是一个完整的、可运行的程序，该程序执行这些计算并按规定打印所需的单行输出。", "solution": "所述问题具有科学依据、提法明确，并包含了获得唯一、可验证解所需的所有信息。它代表了计算统计学中一个标准且有意义的练习，特别涉及蒙特卡洛估计的方差缩减技术。因此，该问题被认为是**有效的**。\n\n主要目标是为学生t分布的累积分布函数（CDF）构建一个基于重要性采样（IS）的估计量，并评估其相对于朴素蒙特卡洛估计量的效率。效率通过它们各自单样本方差的比率来量化。\n\n设 $X$ 是一个服从自由度 $\\nu=3$ 的学生t分布的随机变量。其概率密度函数（PDF）记为 $f_\\nu(x)$，其CDF记为 $F_\\nu(t)$。需要估计的量是 $F_\\nu(t) = \\mathbb{P}(X \\le t) = \\int_{-\\infty}^t f_\\nu(x) \\, \\mathrm{d}x$。这可以表示为一个期望：\n$$\nF_\\nu(t) = \\mathbb{E}_{f_\\nu}[\\mathbf{1}\\{X \\le t\\}]\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n$F_\\nu(t)$ 的朴素蒙特卡洛估计量基于从目标密度 $f_\\nu(x)$ 中抽取的 $n$ 个独立同分布（i.i.d.）样本 $X_1, \\dots, X_n$：\n$$\n\\widehat{F}_{\\text{naive}}(t) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}\n$$\n每一项 $\\mathbf{1}\\{X_i \\le t\\}$ 是一个成功概率为 $p = \\mathbb{P}(X_i \\le t) = F_\\nu(t)$ 的伯努利随机变量。因此，单样本方差（即 $n=1$ 时的方差）是：\n$$\nV_{\\text{naive}}(t) = \\text{Var}(\\mathbf{1}\\{X \\le t\\}) = p(1-p) = F_\\nu(t)(1-F_\\nu(t))\n$$\n对于 $F_\\nu(t)$ 非常小（即 $t$ 深入左尾）的稀有事件，该方差近似为 $F_\\nu(t)$。\n\n重要性采样旨在通过从一个不同的分布，即提议分布 $g_t(x)$ 中采样来减少此方差，该分布被选择以将样本集中在感兴趣的区域。在这里，提议分布是一个以阈值 $t$ 为中心的高斯分布：$g_t(x) = \\mathcal{N}(t, \\sigma^2)$，其中 $\\sigma=1$。\n\nIS估计量是通过相对于提议密度 $g_t$ 重写期望来构建的：\n$$\nF_\\nu(t) = \\int_{-\\infty}^{\\infty} \\mathbf{1}\\{x \\le t\\} f_\\nu(x) \\, \\mathrm{d}x = \\int_{-\\infty}^{\\infty} \\mathbf{1}\\{x \\le t\\} \\frac{f_\\nu(x)}{g_t(x)} g_t(x) \\, \\mathrm{d}x = \\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\} w_t(X)]\n$$\n其中 $w_t(x) = f_\\nu(x) / g_t(x)$ 是重要性权重。$g_t(x)$ 的支撑集（整个 $\\mathbb{R}$）覆盖了 $f_\\nu(x)$ 的支撑集（整个 $\\mathbb{R}$），因此权重是良定义的。\n\n基于从提议分布 $g_t(x)$ 中抽取的 $n$ 个独立同分布样本 $X_1, \\dots, X_n$ 的IS估计量是：\n$$\n\\widehat{F}_{\\text{IS}}(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}\\,w_t(X_i)\n$$\n这个估计量是无偏的。我们通过取其关于 $g_t$ 的期望来证明这一点：\n$$\n\\mathbb{E}_{g_t}[\\widehat{F}_{\\text{IS}}(t)] = \\mathbb{E}_{g_t}\\left[\\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}w_t(X_i)\\right] = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_{g_t}[\\mathbf{1}\\{X_i \\le t\\}w_t(X_i)]\n$$\n根据IS恒等式的推导，$\\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\}w_t(X)] = F_\\nu(t)$。因此，\n$$\n\\mathbb{E}_{g_t}[\\widehat{F}_{\\text{IS}}(t)] = \\frac{1}{n} \\sum_{i=1}^n F_\\nu(t) = F_\\nu(t)\n$$\n\nIS估计量的单样本方差由单个加权样本项 $Y_i = \\mathbf{1}\\{X_i \\le t\\}w_t(X_i)$ 的方差给出：\n$$\nV_{\\text{IS}}(t) = \\text{Var}_{g_t}(\\mathbf{1}\\{X \\le t\\}w_t(X)) = \\mathbb{E}_{g_t}[(\\mathbf{1}\\{X \\le t\\}w_t(X))^2] - (\\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\}w_t(X)])^2\n$$\n代入 $\\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\}w_t(X)] = F_\\nu(t)$ 并注意到 $(\\mathbf{1}\\{X \\le t\\})^2 = \\mathbf{1}\\{X \\le t\\}$，我们得到：\n$$\nV_{\\text{IS}}(t) = \\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\} (w_t(X))^2] - (F_\\nu(t))^2\n$$\n让我们将二阶矩项表示为 $M_2(t) = \\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\} (w_t(X))^2]$。方差为 $V_{\\text{IS}}(t) = M_2(t) - (F_\\nu(t))^2$。\n\n任务是计算方差缩减比 $R(t) = V_{\\text{naive}}(t) / V_{\\text{IS}}(t)$。\n为了计算这个比率，我们遵循指定的数值步骤：\n1.  对于每个阈值 $t$，分子 $V_{\\text{naive}}(t) = F_\\nu(t)(1-F_\\nu(t))$ 是使用自由度为 $\\nu=3$ 的学生t分布CDF的精确值 $F_\\nu(t)$ 计算的。\n2.  分母中的二阶矩项 $M_2(t)$ 通过一次独立的、高预算的蒙特卡洛模拟来估计。我们抽取大量样本 $Y_j \\sim g_t$，$j=1, \\dots, M$，并计算样本均值：\n    $$\n    \\widehat{M}_2(t) = \\frac{1}{M} \\sum_{j=1}^{M} \\mathbf{1}\\{Y_j \\le t\\}(w_t(Y_j))^2\n    $$\n    问题指定了 $M=400000$ 的预算。为了保持数值稳定性，特别是在尾部值的情况下，权重平方 $(w_t(Y_j))^2$ 使用对数概率计算：\n    $$\n    (w_t(Y_j))^2 = \\left(\\frac{f_\\nu(Y_j)}{g_t(Y_j)}\\right)^2 = \\exp\\left(2 \\left(\\log f_\\nu(Y_j) - \\log g_t(Y_j)\\right)\\right)\n    $$\n3.  然后，IS方差被估计为 $\\widehat{V}_{\\text{IS}}(t) = \\widehat{M}_2(t) - (F_\\nu(t))^2$。\n4.  最后，比率计算为 $R(t) = V_{\\text{naive}}(t) / \\widehat{V}_{\\text{IS}}(t)$。\n\n该算法通过遍历给定的阈值 $t \\in \\{-0.5, -2.0, -3.0, -4.0, -5.0\\}$ 来进行，使用 $\\nu=3$、$\\sigma=1$、$M=400000$ 和一个固定的随机种子以保证可复现性。比率 $R(t)  1$ 表明对于给定的阈值 $t$，重要性采样策略比朴素采样方法更有效（即方差更小）。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as student_t\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the ratio of naive MC variance to importance sampling MC variance\n    for estimating the CDF of a Student's t-distribution.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Parameters\n    nu = 3.0  # Degrees of freedom for Student's t\n    sigma = 1.0  # Std dev for Gaussian proposal\n    M = 400000  # MC budget for second-moment estimation\n    thresholds = [-0.5, -2.0, -3.0, -4.0, -5.0]\n    seed = 123456\n\n    # Initialize a random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    results = []\n    \n    for t_val in thresholds:\n        # Step 1: Compute the exact per-sample variance of the naive estimator.\n        # F_nu(t) = P(X = t) for X ~ student_t(nu).\n        F_nu_t = student_t.cdf(t_val, df=nu)\n\n        # V_naive(t) = F_nu(t) * (1 - F_nu(t))\n        V_naive_t = F_nu_t * (1.0 - F_nu_t)\n\n        # Step 2: Estimate the per-sample variance of the IS estimator via MC.\n        # a) Generate samples from the proposal distribution g_t(x) = N(t, sigma^2).\n        samples_g = rng.normal(loc=t_val, scale=sigma, size=M)\n\n        # b) Estimate M_2(t) = E_g[ (1{X=t} * w_t(X))^2 ]\n        # The MC estimator is (1/M) * sum( 1{Y_j=t} * (w_t(Y_j))^2 ) \n        # where Y_j are samples from g_t.\n        # This simplifies to E_g[ 1{X=t} * w_t(X)^2 ].\n        \n        # We only need to compute weights for samples that contribute to the integral,\n        # i.e., where the indicator function is 1.\n        integration_samples = samples_g[samples_g = t_val]\n\n        if integration_samples.size == 0:\n            M2_hat_t = 0.0\n        else:\n            # Use log-PDFs for numerical stability when computing weights.\n            # log(w_t(x)) = log(f_nu(x)) - log(g_t(x))\n            # w_t(x)^2 = exp(2 * log(w_t(x)))\n            log_f_nu = student_t.logpdf(integration_samples, df=nu)\n            log_g_t = norm.logpdf(integration_samples, loc=t_val, scale=sigma)\n            \n            log_w_sq = 2.0 * (log_f_nu - log_g_t)\n            w_sq = np.exp(log_w_sq)\n\n            # The expectation is over all M samples, but terms for Y_j > t_val are zero.\n            # So, we sum the non-zero terms and divide by the total number of samples M.\n            M2_hat_t = np.sum(w_sq) / M\n\n        # c) Compute the estimated IS variance\n        # V_IS(t) = M_2(t) - (F_nu(t))^2\n        V_is_t = M2_hat_t - (F_nu_t**2)\n\n        # Step 3: Compute the ratio of variances.\n        # We check for V_is_t > 0 to prevent division by zero or nonsensical results,\n        # though this is unlikely with a large M if the variance is non-zero.\n        if V_is_t > 0:\n            ratio = V_naive_t / V_is_t\n        else:\n            # Handle cases where the estimated variance is non-positive.\n            # A large value signifies variance reduction is extremely effective.\n            ratio = np.inf\n\n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3304377"}]}