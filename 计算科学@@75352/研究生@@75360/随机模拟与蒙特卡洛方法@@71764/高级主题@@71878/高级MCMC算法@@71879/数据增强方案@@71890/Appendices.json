{"hands_on_practices": [{"introduction": "数据增强最自然的应用之一是处理具有内在潜变量的模型。本练习使用一个有限状态马尔可夫链 [@problem_id:3301965] 来演示如何将未观测到的状态序列视为“缺失数据”，从而简化贝叶斯推断。通过一个假设的状态路径来增强数据，我们可以利用多项式转移似然与狄利克雷先验之间的共轭性，推导出一个简单的吉布斯采样器。这个练习旨在培养在经典潜变量模型中推导条件后验分布的基本技能。", "problem": "考虑一个具有$K=3$个隐状态的时间齐次有限状态马尔可夫链，其转移矩阵为$P=\\{p_{i,j}\\}_{i,j=1}^{3}$，其中每一行$P_{i,\\cdot}$都位于2-单纯形上。假设对矩阵的行施加独立的狄利克雷先验：对于每个$i \\in \\{1,2,3\\}$，$P_{i,\\cdot} \\sim \\mathrm{Dir}(\\alpha_{i,1}, \\alpha_{i,2}, \\alpha_{i,3})$，其中超参数$\\alpha_{i,j}  0$。为了实现共轭更新，使用一种数据增强方案，将隐状态序列$s_{1:T}$视为增强数据。假设给定一个长度为$T=10$的完整示例隐状态序列为$s_{1:10} = (1,2,3,1,3,3,2,1,2,3)$。\n\n提供的超参数为\n$$\n\\alpha_{1,\\cdot} = (0.7,\\,1.2,\\,0.9), \\quad \\alpha_{2,\\cdot} = (1.1,\\,0.6,\\,0.8), \\quad \\alpha_{3,\\cdot} = (0.5,\\,0.5,\\,0.5).\n$$\n\n从核心定义和经过充分检验的事实出发，包括具有完整状态路径的马尔可夫链转移似然和贝叶斯法则，按以下步骤进行：\n\n1. 在给定增强序列$s_{1:T}$和狄利克雷先验的情况下，推导每一行$P_{i,\\cdot}$的全条件后验分布。您的推导必须从基于转移计数$n_{i,j} = \\sum_{t=1}^{T-1} \\mathbf{1}\\{s_t=i,\\,s_{t+1}=j\\}$的似然因式分解开始，并且必须得出一个后验分布的闭式表达式，该表达式为一个狄利克雷分布，其参数结合了先验超参数和观测到的转移计数。\n\n2. 通过刻画第一行$P_{1,\\cdot}$的全条件分布，对给定$s_{1:T}$的转移矩阵执行一步Gibbs更新。然后，在该全条件下计算特定转移概率$p_{1,3}$（从状态1到状态3）的后验均值。将您的最终数值答案四舍五入到四位有效数字。\n\n最终答案必须是一个实数。无需单位。", "solution": "问题陈述已经过仔细验证，并被确定为有效。它在贝叶斯统计和马尔可夫链理论方面有科学依据，是适定的，提供了所有必要信息，并且表述客观。因此，我们可以进行完整的解答。\n\n该问题要求完成两项主要任务：首先，推导马尔可夫链转移矩阵各行全条件后验分布的一般形式；其次，将此结果应用于特定案例以计算后验均值。\n\n**1. 全条件后验分布的推导**\n\n给定一个具有$K$个状态的时间齐次有限状态马尔可夫链，其转移矩阵为$P = \\{p_{i,j}\\}_{i,j=1}^{K}$，其中$p_{i,j} = P(s_{t+1}=j | s_t=i)$。每一行$P_{i,\\cdot} = (p_{i,1}, \\dots, p_{i,K})$是$(K-1)$-单纯形上的一个概率向量。\n\n贝叶斯框架要求为未知参数$P$指定一个先验分布，并根据数据指定一个似然函数。然后通过贝叶斯法则找到后验分布。\n\n**先验分布：**\n假设转移矩阵$P$的各行是先验独立的。对于每一行$i \\in \\{1,\\dots,K\\}$，先验是一个狄利克雷分布：\n$$\nP_{i,\\cdot} \\sim \\mathrm{Dir}(\\alpha_{i,1}, \\dots, \\alpha_{i,K})\n$$\n单行$P_{i,\\cdot}$的概率密度函数 (PDF) 由下式给出：\n$$\np(P_{i,\\cdot} | \\alpha_{i,\\cdot}) = \\frac{\\Gamma\\left(\\sum_{j=1}^{K} \\alpha_{i,j}\\right)}{\\prod_{j=1}^{K} \\Gamma(\\alpha_{i,j})} \\prod_{j=1}^{K} p_{i,j}^{\\alpha_{i,j}-1}\n$$\n由于独立性，整个矩阵$P$的联合先验是各行先验的乘积：\n$$\np(P | \\alpha) = \\prod_{i=1}^{K} p(P_{i,\\cdot} | \\alpha_{i,\\cdot})\n$$\n\n**似然函数：**\n数据是一个完整的隐状态序列$s_{1:T} = (s_1, s_2, \\dots, s_T)$。在给定转移矩阵$P$的情况下，该序列的似然由每次转移的概率乘积决定：\n$$\n\\mathcal{L}(P; s_{1:T}) = p(s_{1:T} | P) = p(s_1) \\prod_{t=1}^{T-1} P(s_{t+1} | s_t, P) = p(s_1) \\prod_{t=1}^{T-1} p_{s_t, s_{t+1}}\n$$\n在推断转移概率时，通常以第一个状态$s_1$为条件。似然可以通过聚合转移来重新表达。令$n_{i,j}$为序列中从状态$i$到状态$j$的观测转移次数：\n$$\nn_{i,j} = \\sum_{t=1}^{T-1} \\mathbf{1}\\{s_t=i, s_{t+1}=j\\}\n$$\n其中$\\mathbf{1}\\{\\cdot\\}$是指示函数。似然函数则变为：\n$$\n\\mathcal{L}(P; s_{1:T}) \\propto \\prod_{i=1}^{K} \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}}\n$$\n至关重要的是，该似然可以按转移矩阵的行$i$进行因式分解：\n$$\n\\mathcal{L}(P; s_{1:T}) \\propto \\prod_{i=1}^{K} \\left( \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}} \\right)\n$$\n项$\\prod_{j=1}^{K} p_{i,j}^{n_{i,j}}$是状态$i$出转移的多项分布的核。\n\n**后验分布：**\n根据贝叶斯法则，$P$的后验分布与似然和先验的乘积成正比：\n$$\np(P | s_{1:T}, \\alpha) \\propto \\mathcal{L}(P; s_{1:T}) \\cdot p(P | \\alpha)\n$$\n由于先验和似然都可按$P$的行进行因式分解，后验分布也可以分解：\n$$\np(P | s_{1:T}, \\alpha) \\propto \\prod_{i=1}^{K} \\left[ \\left( \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}} \\right) \\left( \\frac{\\Gamma\\left(\\sum_{j=1}^{K} \\alpha_{i,j}\\right)}{\\prod_{j=1}^{K} \\Gamma(\\alpha_{i,j})} \\prod_{j=1}^{K} p_{i,j}^{\\alpha_{i,j}-1} \\right) \\right]\n$$\n这表明，每一行$P_{i,\\cdot}$的全条件后验可以独立推导。对于单行$i$：\n$$\np(P_{i,\\cdot} | s_{1:T}, \\alpha_{i,\\cdot}) \\propto \\left( \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}} \\right) \\cdot \\left( \\prod_{j=1}^{K} p_{i,j}^{\\alpha_{i,j}-1} \\right)\n$$\n合并各项，我们得到：\n$$\np(P_{i,\\cdot} | s_{1:T}, \\alpha_{i,\\cdot}) \\propto \\prod_{j=1}^{K} p_{i,j}^{n_{i,j} + \\alpha_{i,j} - 1}\n$$\n这个表达式是带有更新参数的狄利克雷分布的核。因此，转移矩阵第$i$行的全条件后验分布为：\n$$\nP_{i,\\cdot} | s_{1:T}, \\alpha_{i,\\cdot} \\sim \\mathrm{Dir}(\\alpha_{i,1} + n_{i,1}, \\alpha_{i,2} + n_{i,2}, \\dots, \\alpha_{i,K} + n_{i,K})\n$$\n这证明了狄利克雷先验与转移计数的多项似然的共轭性。\n\n**2. Gibbs更新与后验均值计算**\n\n现在我们将此结果应用于具有$K=3$个状态的具体问题。\n\n**转移计数：**\n首先，我们必须从给定序列$s_{1:10} = (1,2,3,1,3,3,2,1,2,3)$中计算转移计数$n_{i,j}$。共有$T-1=9$次转移：\n$1 \\to 2$, $2 \\to 3$, $3 \\to 1$, $1 \\to 3$, $3 \\to 3$, $3 \\to 2$, $2 \\to 1$, $1 \\to 2$, $2 \\to 3$。\n\n将这些转移聚合到一个计数矩阵$N = \\{n_{i,j}\\}$中：\n- 从状态1的转移：$1 \\to 2$ (两次)，$1 \\to 3$ (一次)。所以，$n_{1,1}=0$, $n_{1,2}=2$, $n_{1,3}=1$。\n- 从状态2的转移：$2 \\to 3$ (两次)，$2 \\to 1$ (一次)。所以，$n_{2,1}=1$, $n_{2,2}=0$, $n_{2,3}=2$。\n- 从状态3的转移：$3 \\to 1$ (一次)，$3 \\to 2$ (一次)，$3 \\to 3$ (一次)。所以，$n_{3,1}=1$, $n_{3,2}=1$, $n_{3,3}=1$。\n\n计数矩阵为：\n$$ N = \\begin{pmatrix} 0  2  1 \\\\ 1  0  2 \\\\ 1  1  1 \\end{pmatrix} $$\n\n**$P_{1,\\cdot}$的全条件分布：**\n问题要求刻画第一行$P_{1,\\cdot} = (p_{1,1}, p_{1,2}, p_{1,3})$的全条件分布。\n该行的先验超参数为$\\alpha_{1,\\cdot} = (0.7, 1.2, 0.9)$。\n该行的转移计数为$n_{1,\\cdot} = (n_{1,1}, n_{1,2}, n_{1,3}) = (0, 2, 1)$。\n\n使用推导出的公式，后验参数$\\alpha'_{1,\\cdot}$为：\n$$\n\\alpha'_{1,j} = \\alpha_{1,j} + n_{1,j}\n$$\n$$\n\\alpha'_{1,1} = 0.7 + 0 = 0.7\n$$\n$$\n\\alpha'_{1,2} = 1.2 + 2 = 3.2\n$$\n$$\n\\alpha'_{1,3} = 0.9 + 1 = 1.9\n$$\n因此，转移矩阵第一行的全条件后验分布为：\n$$\nP_{1,\\cdot} | s_{1:10}, \\alpha_{1,\\cdot} \\sim \\mathrm{Dir}(0.7, 3.2, 1.9)\n$$\n\n**$p_{1,3}$的后验均值：**\n我们需要计算转移概率$p_{1,3}$的后验均值。狄利克雷分布的一个标准性质是，如果一个随机向量$(X_1, \\dots, X_K) \\sim \\mathrm{Dir}(\\beta_1, \\dots, \\beta_K)$，其第$j$个分量的期望值为：\n$$\n\\mathbb{E}[X_j] = \\frac{\\beta_j}{\\sum_{k=1}^{K} \\beta_k}\n$$\n将此应用于我们的$P_{1,\\cdot}$的后验分布， $p_{1,3}$的后验均值为：\n$$\n\\mathbb{E}[p_{1,3} | s_{1:10}, \\alpha_{1,\\cdot}] = \\frac{\\alpha'_{1,3}}{\\alpha'_{1,1} + \\alpha'_{1,2} + \\alpha'_{1,3}}\n$$\n代入计算出的后验参数：\n$$\n\\mathbb{E}[p_{1,3} | s_{1:10}, \\alpha_{1,\\cdot}] = \\frac{1.9}{0.7 + 3.2 + 1.9} = \\frac{1.9}{5.8}\n$$\n现在，我们计算其数值：\n$$\n\\frac{1.9}{5.8} = \\frac{19}{58} \\approx 0.327586206...\n$$\n四舍五入到四位有效数字，我们得到$0.3276$。", "answer": "$$\\boxed{0.3276}$$", "id": "3301965"}, {"introduction": "对于像逻辑回归这样的非共轭模型，其后验分布没有标准形式，这使得吉布斯采样变得复杂。本练习将探讨一种现代解决方案：Pólya-Gamma (PG) 数据增强 [@problem_id:3302005]。通过引入经过精心选择的潜变量，复杂的逻辑斯蒂似然函数可以被转化为高斯混合形式，从而得到一个出人意料地简单而高效的吉布斯采样器。这个编码练习将提供实现前沿MCMC技术的实践经验。", "problem": "考虑一个具有二元响应和高斯先验的贝叶斯逻辑回归模型，该模型通过基于 Pólya–Gamma (PG) 数据增强构建的吉布斯采样器进行一步拟合。设有 $n$ 个观测值，其预测变量为 $x_i \\in \\mathbb{R}^p$，响应为 $y_i \\in \\{0,1\\}$，回归系数为 $\\beta \\in \\mathbb{R}^p$。假设先验为 $\\beta \\sim \\mathcal{N}(0, \\sigma_\\beta^2 I_p)$。逻辑模型的似然函数为 $p(y_i \\mid x_i, \\beta) = \\operatorname{Bernoulli}(\\pi_i)$，其中 $\\pi_i = (1 + \\exp(-x_i^\\top \\beta))^{-1}$。在 Pólya–Gamma (PG) 增强下，单次完整的吉布斯迭代会采样潜变量 $\\omega_i$，然后从其全条件分布中采样 $\\beta$，其中 Pólya–Gamma 分布表示为 $\\operatorname{PG}(b, c)$，且 $b \\in \\mathbb{R}_+$, $c \\in \\mathbb{R}$。\n\n您的任务是为每个指定的测试用例，精确地实现一次完整的吉布斯迭代，包括：\n- 使用指定的截断水平 $K$，通过截断无穷级数表示法，为每个 $i \\in \\{1,\\dots,n\\}$ 采样 $\\omega_i \\sim \\operatorname{PG}(1, x_i^\\top \\beta^{(0)})$。\n- 给定 $\\omega = (\\omega_1,\\dots,\\omega_n)$、先验 $\\beta \\sim \\mathcal{N}(0, \\sigma_\\beta^2 I_p)$ 以及带有 PG 增强的逻辑似然，从 $\\beta^{(1)}$ 的全条件分布中进行采样。\n\n使用以下综合测试套件。对于每个用例，取初始状态 $\\beta^{(0)} = 0_p$（$p$ 维零向量），并使用指定的截断水平 $K$、先验方差 $\\sigma_\\beta^2$ 以及给定的数据 $(X,y)$。下面所有的实数、整数和向量都是精确值，必须按原样使用。\n\n测试用例 1：\n- $n = 5$, $p = 2$。\n- $X^{(1)} = \\begin{bmatrix}\n1.0  -0.5 \\\\\n1.2  0.3 \\\\\n-0.7  1.4 \\\\\n0.0  -1.0 \\\\\n2.0  2.0\n\\end{bmatrix}$,\n$y^{(1)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$。\n- $\\sigma_\\beta^2 = 5.0$。\n- $K = 200$。\n\n测试用例 2：\n- $n = 8$, $p = 3$。\n- $X^{(2)} = \\begin{bmatrix}\n1.0  0.5  -1.0 \\\\\n-0.3  2.0  0.7 \\\\\n0.8  -0.6  1.5 \\\\\n1.2  1.1  0.0 \\\\\n-1.5  0.4  -0.2 \\\\\n0.0  -1.2  0.9 \\\\\n2.0  0.0  1.0 \\\\\n-0.7  -0.8  0.3\n\\end{bmatrix}$,\n$y^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$。\n- $\\sigma_\\beta^2 = 2.5$。\n- $K = 150$。\n\n测试用例 3（用于测试先验数值稳定性的秩亏设计）：\n- $n = 5$, $p = 3$。\n- $X^{(3)} = \\begin{bmatrix}\n1.0  1.0  2.0 \\\\\n0.9  1.1  2.0 \\\\\n1.1  0.9  2.0 \\\\\n-0.5  -0.5  -1.0 \\\\\n2.0  2.0  4.0\n\\end{bmatrix}$,\n$y^{(3)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$。\n- $\\sigma_\\beta^2 = 10.0$。\n- $K = 50$。\n\n随机性与确定性要求：\n- 为伪随机数生成器使用单个全局基础种子 $s_0 = 2025$。对于测试用例索引 $j \\in \\{1,2,3\\}$，使用种子 $s_j = s_0 + j$ 创建一个独立的生成器。\n- 使用双精度浮点运算。\n\nPólya–Gamma 采样器要求：\n- 对 $\\omega \\sim \\operatorname{PG}(1, c)$ 使用截断无穷级数表示：\n$$\n\\omega \\approx \\frac{1}{2 \\pi^2} \\sum_{k=1}^{K} \\frac{g_k}{\\left(k - \\tfrac{1}{2}\\right)^2 + \\frac{c^2}{4 \\pi^2}}, \\quad g_k \\overset{\\text{i.i.d.}}{\\sim} \\operatorname{Gamma}(1,1).\n$$\n- 使用给定的 $K$ 精确计算该级数。\n\n每次迭代的计算复杂度核算模型：\n- 每次加法、减法、乘法、除法或调用随机变量生成器计为一次基本运算。\n- 像 $\\pi$ 这样的常数运算成本为零；读取数组元素的成本为零。\n- 对每个 $i$ 计算 $c_i = x_i^\\top \\beta^{(0)}$ 的成本是 $p$ 次乘法加 $(p-1)$ 次加法。\n- 对每个 $\\omega_i$，令 $c = c_i$，并定义每个 $i$ 的开销为：$1$ 次乘法用于 $c^2$， $1$ 次除法用于 $c^2/(4\\pi^2)$，以及 $1$ 次乘法或除法用于将最终级数和乘以 $1/(2\\pi^2)$。对于每个 $k \\in \\{1,\\dots,K\\}$，每项的成本恰好是 $6$ 次运算：一次减法得到 $k - \\tfrac{1}{2}$，一次乘法对其平方，一次加法加上常数偏移，一次 gamma 随机抽取，一次除法计算 $g_k$ 除以分母，以及一次加法累加级数。\n- 组合高斯线性代数以采样 $\\beta$：通过将 $X$ 的每一行乘以 $\\omega_i$ 来形成 $W X$（成本为 $n p$ 次乘法），然后使用朴素算法计算 $X^\\top (W X)$，成本为 $p^2 n$ 次乘法和 $p^2 (n-1)$ 次加法（总计 $p^2 (2n - 1)$ 次运算）。将岭项 $(1/\\sigma_\\beta^2) I_p$ 加到对角线上：$1$ 次除法计算 $1/\\sigma_\\beta^2$ 加 $p$ 次加法到对角线。\n- 计算 $\\kappa = y - \\tfrac{1}{2}\\mathbf{1}$：恰好 $n$ 次减法。形成 $b = X^\\top \\kappa$：成本为 $p n$ 次乘法和 $p(n-1)$ 次加法（总计 $p (2n - 1)$ 次运算）。\n- 使用数值稳定的方法计算精度矩阵 $A$ 的 Cholesky 分解 $A = L L^\\top$；计为 $\\left\\lfloor p^3/3 \\right\\rfloor$ 次基本运算。\n- 使用 $L$ 和 $L^\\top$ 的前向和后向替换求解 $A \\mu = b$：两次三角求解合并计算为 $2 p^2$ 次运算。\n- 抽取 $z \\sim \\mathcal{N}(0, I_p)$：$p$ 次独立标准正态抽取计为 $p$ 次运算。\n- 使用单次三角求解 $L^\\top x = z$ 形成一个协方差为 $A^{-1}$ 的高斯样本，并设置 $\\beta^{(1)} = \\mu + x$：三角求解计为 $p^2$ 次运算，最终求和计为 $p$ 次加法。\n\n在此模型下，给定 $n$、$p$、$K$ 的一次迭代总运算次数为\n$$\n\\begin{aligned}\n\\mathrm{Ops}(n,p,K) \\;=\\; n\\big( p + (p-1) + 1 + 1 + 1 + 6K \\big) \\\\\n+ n p \\\\\n+ p^2 (2n - 1) \\\\\n+ 1 + p \\\\\n+ n \\\\\n+ p (2n - 1) \\\\\n+ \\left\\lfloor \\frac{p^3}{3} \\right\\rfloor \\\\\n+ 3 p^2 \\\\\n+ p \\\\\n+ p.\n\\end{aligned}\n$$\n此公式必须完全按照所述实现。\n\n程序要求：\n- 对每个测试用例，完全按照上述规定执行一次完整的吉布斯迭代，并使用提供的核算模型计算总运算次数。\n- 您的程序应生成单行输出，其中包含三个用例的结果，形式为一个包含三个列表的逗号分隔列表，每个测试用例一个列表。对于每个测试用例 $j$，输出一个列表，其中包含按顺序排列的采样 $\\beta^{(1)}$ 的条目，后跟该用例的整数总运算次数。例如，最终格式应类似于\n$[\\,[\\beta^{(1)}_{1},\\dots,\\beta^{(1)}_{p},\\mathrm{Ops}]\\, , \\, [\\dots] \\, , \\, [\\dots]\\,]$,\n所有浮点数均以标准十进制形式打印。", "solution": "该问题要求实现贝叶斯逻辑回归的单次吉布斯采样器迭代。这是通过使用 Pólya–Gamma (PG) 数据增强方案来实现的，这是一种在具有类逻辑函数的模型中简化后验推断的强大技术。该过程包括两个主要步骤：采样辅助的 Pólya–Gamma 潜变量，然后从其全条件分布中采样回归系数，由于数据增强，该分布变为多元高斯分布。\n\n对于单个观测值 $y_i \\in \\{0, 1\\}$，其预测变量向量为 $x_i \\in \\mathbb{R}^p$，系数为 $\\beta \\in \\mathbb{R}^p$，逻辑似然函数由下式给出\n$$ p(y_i \\mid x_i, \\beta) = \\frac{(e^{x_i^\\top \\beta})^{y_i}}{1 + e^{x_i^\\top \\beta}} $$\n通过引入一个遵循 Pólya–Gamma 分布（表示为 $\\operatorname{PG}(b, c)$）的潜变量 $\\omega_i$，可以重写此表达式。关键恒等式是：\n$$ \\frac{(e^{\\psi})^a}{(1+e^{\\psi})^b} = 2^{-b} e^{(a-b/2)\\psi} \\int_0^\\infty e^{-\\omega \\psi^2/2} p(\\omega \\mid b, 0) \\, d\\omega $$\n此恒等式的一个变换（通过倾斜 PG 分布）使我们能够将逻辑似然表示为高斯混合：\n$$ p(y_i \\mid x_i, \\beta) \\propto e^{\\kappa_i x_i^\\top \\beta} \\int_0^\\infty e^{-\\frac{\\omega_i}{2}(x_i^\\top \\beta)^2} p(\\omega_i \\mid 1, x_i^\\top \\beta) \\, d\\omega_i $$\n其中 $\\kappa_i = y_i - \\frac{1}{2}$。这种表述是有利的，因为在给定 $\\omega_i$ 的条件下，似然项与关于 $\\beta$ 的高斯密度成比例。\n\n吉布斯采样器从参数的全条件分布中迭代抽样。对于此模型，两个步骤是：\n1.  给定当前系数 $\\beta^{(t)}$，采样潜变量 $\\omega = (\\omega_1, \\dots, \\omega_n)$。\n2.  给定数据和新采样的潜变量 $\\omega$，采样系数 $\\beta^{(t+1)}$。\n\n**第 1 步：采样潜变量 $\\omega_i$**\n每个 $\\omega_i$ 的全条件分布是 $\\omega_i \\mid \\beta \\sim \\operatorname{PG}(1, x_i^\\top \\beta)$。问题指定从初始状态 $\\beta^{(0)} = 0_p$（$p$ 维零向量）开始。因此，PG 分布的参数 $c_i = x_i^\\top \\beta^{(0)}$ 对所有 $i=1, \\dots, n$ 均为 $0$。因此，我们需要采样 $\\omega_i \\sim \\operatorname{PG}(1, 0)$。\n问题要求使用特定的截断无穷级数表示法进行此采样过程：\n$$ \\omega_i \\approx \\frac{1}{2 \\pi^2} \\sum_{k=1}^{K} \\frac{g_k}{\\left(k - \\frac{1}{2}\\right)^2 + \\frac{c_i^2}{4 \\pi^2}} $$\n其中 $g_k \\overset{\\text{i.i.d.}}{\\sim} \\operatorname{Gamma}(1,1)$（这等同于速率为 $1$ 的指数分布），$K$ 是指定的截断水平。由于 $c_i=0$，公式简化为：\n$$ \\omega_i \\approx \\frac{1}{2 \\pi^2} \\sum_{k=1}^{K} \\frac{g_k}{\\left(k - \\frac{1}{2}\\right)^2} $$\n对于每个观测值 $i$，我们生成 $K$ 个 Gamma 变量并计算此和。\n\n**第 2 步：采样系数 $\\beta^{(1)}$**\n给定采样的潜变量 $\\omega_1, \\dots, \\omega_n$，我们从其全条件分布 $p(\\beta \\mid y, X, \\omega)$ 中采样 $\\beta^{(1)}$。后验分布与先验和增强似然的乘积成正比：\n$$ p(\\beta \\mid y, X, \\omega) \\propto p(\\beta) \\prod_{i=1}^n p(y_i \\mid x_i, \\beta, \\omega_i) $$\n先验为 $\\beta \\sim \\mathcal{N}(0, \\sigma_\\beta^2 I_p)$，其密度为 $p(\\beta) \\propto \\exp\\left(-\\frac{1}{2\\sigma_\\beta^2} \\beta^\\top \\beta\\right)$。\n增强似然为 $\\prod_{i=1}^n \\exp\\left( \\kappa_i x_i^\\top \\beta - \\frac{\\omega_i}{2}(x_i^\\top\\beta)^2 \\right)$。\n结合这些项，$\\beta$ 的对数后验是一个二次函数：\n$$ \\log p(\\beta \\mid \\dots) = C - \\frac{1}{2\\sigma_\\beta^2} \\beta^\\top \\beta + \\sum_{i=1}^n \\left( \\kappa_i x_i^\\top \\beta - \\frac{\\omega_i}{2}(x_i^\\top\\beta)^2 \\right) \\\\ = C - \\frac{1}{2} \\left( \\beta^\\top \\left( \\sum_{i=1}^n \\omega_i x_i x_i^\\top + \\frac{1}{\\sigma_\\beta^2}I_p \\right) \\beta - 2 \\left( \\sum_{i=1}^n \\kappa_i x_i \\right)^\\top \\beta \\right) $$\n这是一个多元高斯分布 $\\mathcal{N}(\\mu_\\beta, \\Sigma_\\beta)$ 的核。精度矩阵 $\\Sigma_\\beta^{-1}$ 和均值 $\\mu_\\beta$ 是：\n$$ A = \\Sigma_\\beta^{-1} = X^\\top W X + \\frac{1}{\\sigma_\\beta^2}I_p $$\n$$ \\mu_\\beta = A^{-1} (X^\\top \\kappa) $$\n其中 $W = \\operatorname{diag}(\\omega_1, \\dots, \\omega_n)$ 且 $\\kappa = (y_1 - \\frac{1}{2}, \\dots, y_n - \\frac{1}{2})^\\top$。\n要从 $\\mathcal{N}(\\mu_\\beta, A^{-1})$ 中采样 $\\beta^{(1)}$，我们遵循指定的方法：\n1.  计算精度矩阵 $A$ 和向量 $b = X^\\top \\kappa$。\n2.  通过求解线性系统 $A \\mu_\\beta = b$ 来计算均值 $\\mu_\\beta$。\n3.  对精度矩阵进行 Cholesky 分解，$A = LL^\\top$。\n4.  抽取一个独立标准正态变量的向量，$z \\sim \\mathcal{N}(0, I_p)$。\n5.  通过计算 $x = (L^\\top)^{-1} z$ 从 $\\mathcal{N}(0, A^{-1})$ 生成一个样本。这是通过求解三角系统 $L^\\top x = z$ 来完成的。\n6.  最终样本通过加上均值得到：$\\beta^{(1)} = \\mu_\\beta + x$。\n\n最后，根据一个详细的核算模型，使用明确提供的精确公式计算此迭代的计算成本，该模型根据每次算术运算和随机变量生成来累加成本。此实现将严格遵守所有指定的数值、用于随机数生成的种子以及每个测试用例的程序步骤。", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian logistic regression problem using one Gibbs iteration \n    with a Pólya–Gamma data augmentation scheme for three test cases.\n    \"\"\"\n    \n    # Store test case parameters\n    test_cases = [\n        {\n            \"n\": 5, \"p\": 2,\n            \"X\": np.array([\n                [1.0, -0.5], [1.2, 0.3], [-0.7, 1.4], [0.0, -1.0], [2.0, 2.0]\n            ]),\n            \"y\": np.array([1, 0, 1, 0, 1]),\n            \"sigma_beta_sq\": 5.0,\n            \"K\": 200,\n            \"case_index\": 1\n        },\n        {\n            \"n\": 8, \"p\": 3,\n            \"X\": np.array([\n                [1.0, 0.5, -1.0], [-0.3, 2.0, 0.7], [0.8, -0.6, 1.5], [1.2, 1.1, 0.0],\n                [-1.5, 0.4, -0.2], [0.0, -1.2, 0.9], [2.0, 0.0, 1.0], [-0.7, -0.8, 0.3]\n            ]),\n            \"y\": np.array([1, 0, 1, 1, 0, 0, 1, 0]),\n            \"sigma_beta_sq\": 2.5,\n            \"K\": 150,\n            \"case_index\": 2\n        },\n        {\n            \"n\": 5, \"p\": 3,\n            \"X\": np.array([\n                [1.0, 1.0, 2.0], [0.9, 1.1, 2.0], [1.1, 0.9, 2.0],\n                [-0.5, -0.5, -1.0], [2.0, 2.0, 4.0]\n            ]),\n            \"y\": np.array([1, 1, 0, 0, 1]),\n            \"sigma_beta_sq\": 10.0,\n            \"K\": 50,\n            \"case_index\": 3\n        }\n    ]\n\n    s0 = 2025\n    results = []\n\n    def sample_polya_gamma(rng, c, K):\n        \"\"\"Samples from PG(1, c) using a truncated series.\"\"\"\n        c2_term = c**2 / (4 * np.pi**2)\n        g_k = rng.standard_gamma(1, size=K)\n        k_vals = np.arange(1, K + 1)\n        denominators = (k_vals - 0.5)**2 + c2_term\n        omega = np.sum(g_k / denominators) / (2 * np.pi**2)\n        return omega\n\n    def calculate_ops(n, p, K):\n        \"\"\"Calculates total operations based on the provided accounting model.\"\"\"\n        term1 = n * (p + (p - 1) + 1 + 1 + 1 + 6 * K)\n        term2 = n * p\n        term3 = p**2 * (2 * n - 1)\n        term4 = 1 + p\n        term5 = n\n        term6 = p * (2 * n - 1)\n        term7 = p**3 // 3\n        term8 = 3 * p**2\n        term9 = p\n        term10 = p\n        return int(term1 + term2 + term3 + term4 + term5 + term6 + term7 + term8 + term9 + term10)\n\n    for case in test_cases:\n        n, p = case[\"n\"], case[\"p\"]\n        X, y = case[\"X\"], case[\"y\"]\n        sigma_beta_sq, K = case[\"sigma_beta_sq\"], case[\"K\"]\n        case_index = case[\"case_index\"]\n\n        # Set up the random number generator for the current case\n        seed = s0 + case_index\n        rng = np.random.default_rng(seed)\n\n        # Initial state beta_0 is a zero vector\n        beta_0 = np.zeros(p)\n\n        # --- Step 1: Sample latent variables omega ---\n        # Since beta_0 is zero, c_i = x_i^T beta_0 = 0 for all i\n        c_vals = X @ beta_0\n        \n        omegas = np.array([sample_polya_gamma(rng, c_vals[i], K) for i in range(n)])\n\n        # --- Step 2: Sample beta^(1) from its full conditional ---\n        W = np.diag(omegas)\n        \n        # Precision matrix of the Gaussian conditional for beta\n        A = X.T @ W @ X + (1 / sigma_beta_sq) * np.identity(p)\n        \n        # Mean component\n        kappa = y - 0.5\n        b = X.T @ kappa\n        \n        # Mean of the Gaussian conditional\n        mu = linalg.solve(A, b, assume_a='pos')\n        \n        # Sample from N(mu, A^-1) using Cholesky decomposition\n        L = linalg.cholesky(A, lower=True)\n        z = rng.standard_normal(size=p)\n        \n        # Solve L^T x = z for x\n        x = linalg.solve_triangular(L.T, z, lower=False)\n        \n        beta_1 = mu + x\n        \n        # --- Calculate total operations ---\n        ops = calculate_ops(n, p, K)\n\n        # Append results for this case\n        case_result = list(beta_1) + [ops]\n        results.append(case_result)\n\n    # Format and print the final output exactly as required\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3302005"}, {"introduction": "本练习将引导我们超越简单地应用数据增强，而去批判性地评估哪种增强策略更优。我们将研究中心化与非中心化参数化这两种构建分层模型的方法 [@problem_id:3301996]，它们对MCMC的性能有着显著影响。你将首先通过分析后验参数相关性来从理论上预测哪种方法更好，然后通过编写两种吉布斯采样器并比较它们的混合速度来验证你的预测。本练习旨在强调，特别是在分层模型中，选择合适的增强方案是设计高效算法的关键。", "problem": "考虑一个具有标量潜变量、标量位置参数和已知观测噪声的分层正态模型。该模型具体规定如下。参数 $ \\mu $ 具有先验 $ \\mu \\sim \\mathcal{N}(0, v_0) $。在给定 $ \\mu $ 和一个方差参数 $ \\sigma^2 $（假定已知）的条件下，潜变量 $ z $ 的分布为 $ z \\mid \\mu, \\sigma^2 \\sim \\mathcal{N}(\\mu, \\sigma^2) $。在给定 $ z $ 和一个已知的观测方差 $ \\tau^2 $ 的条件下，独立观测值 $ y_1, \\dots, y_n $ 的分布为 $ y_i \\mid z, \\tau^2 \\sim \\mathcal{N}(z, \\tau^2) $，其中 $ i = 1, \\dots, n $。这种生成式设定是潜变量的中心化参数化。\n\n一种非中心化参数化通过引入一个辅助标准正态变量 $ \\eta $ 并定义 $ z = \\mu + \\sigma \\eta $（其中 $ \\eta \\sim \\mathcal{N}(0, 1) $）来给出。在这种参数化下，观测模型变为 $ y_i \\mid \\mu, \\eta, \\sigma^2, \\tau^2 \\sim \\mathcal{N}(\\mu + \\sigma \\eta, \\tau^2) $，对于 $ i = 1, \\dots, n $ 独立成立。\n\n任务是，使用一个基于高斯后验结构（例如，通过联合二次型所蕴含的后验协方差和相关性，或者通过 Fisher 信息考量）的、有原则的、基于依赖性的准则，来预测在哪种参数化方案下，双区块 Gibbs 采样器能够获得更快的混合速度。这里的采样器分别是在中心化方案下对 $ (\\mu, z) $ 进行采样，以及在非中心化方案下对 $ (\\mu, \\eta) $ 进行采样。预测需在 $ \\sigma^2 $ 相对于由 $ y_1, \\dots, y_n $ 通过 $ \\tau^2 $ 和 $ n $ 携带的数据信息而言较大的情况下进行。您的预测应通过分析区块间的后验耦合如何依赖于 $ \\sigma^2 $、$ \\tau^2 $ 和 $ n $，从第一性原理推导得出。\n\n然后，通过实现两个 Gibbs 采样器来验证您的预测（一个用于中心化参数化，更新 $ z \\mid \\mu, y $ 和 $ \\mu \\mid z $；另一个用于非中心化参数化，更新 $ \\eta \\mid \\mu, y $ 和 $ \\mu \\mid \\eta $）。使用生成的 $ \\mu $ 的马尔可夫链蒙特卡洛 (MCMC) 样本，在经过合适的老化期后，为每个链计算经验性滞后-1自相关，并确定在每个测试案例中，哪种参数化产生更小的自相关（这被解释为更快的混合）。\n\n使用以下基线事实和定义作为您推导的基础：贝叶斯定理；多元正态分布及其条件分布的形式；对于高斯目标分布，Gibbs 更新在条件变量上是线性的这一观察；由二次型蕴含的后验协方差和相关矩阵的定义。不要假设或直接引用任何给出混合速率公式的结果；相反，您应通过将后验依赖性与线性 Gibbs 算子的收缩性质联系起来，来推导出您的预测器。\n\n实现您的程序以处理以下参数值的测试套件。在所有情况下，将所有 $ i $ 的 $ y_i $ 设置为 0，以将结构性依赖效应与随机数据波动分离开来。测试套件如下：\n- 案例 1（相对于数据信息而言，先验方差较大）：$ n = 1 $，$ \\tau^2 = 10 $，$ \\sigma^2 = 1000 $，$ v_0 = 100 $。\n- 案例 2（相对于先验方差而言，数据信息较强）：$ n = 50 $，$ \\tau^2 = 1 $，$ \\sigma^2 = 0.01 $，$ v_0 = 100 $。\n- 案例 3（中等情况）：$ n = 10 $，$ \\tau^2 = 5 $，$ \\sigma^2 = 1 $，$ v_0 = 10 $。\n\n您的程序必须：\n- 通过计算中心化参数化（区块 $ \\mu $ 和 $ z $）和非中心化参数化（区块 $ \\mu $ 和 $ \\eta $）中两个 Gibbs 区块之间的后验相关性，为每个案例推导一个预测。这里使用由高斯二次型所蕴含的后验精度矩阵的逆。绝对后验相关性较小的参数化应被预测为混合更快。\n- 为每个案例运行两个 Gibbs 采样器（中心化和非中心化），使用固定的随机种子，每个链至少有 $ 5000 $ 次迭代的老化期，并且总迭代次数至少为 $ 20000 $ 次。计算老化期后每个链中采样出的 $ \\mu $ 值的经验性滞后-1自相关，并宣布自相关较小的参数化为经验上混合更快的。\n- 对于每个案例，输出一个布尔值，指示您基于相关的预测是否与经验比较结果一致。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，“[result1,result2,result3]”）。每个结果都必须是布尔值“True”或“False”。答案中不涉及物理单位，也不出现角度或百分比。", "solution": "用户希望预测并验证分层正态模型的两种参数化方法——中心化（CP）和非中心化（NCP）——哪一种能为双区块 Gibbs 采样器带来更快的混合速度。预测将基于每种参数化中区块间的后验相关性。验证将通过比较生成的参数 $\\mu$ 样本的经验滞后-1自相关来完成。\n\n### 问题验证\n问题陈述清晰、自包含，并在贝叶斯统计和蒙特卡洛方法中有科学依据。模型是标准的，参数已提供，预测和验证的标准定义明确。该问题是有效的，可以按所述方式解决。\n\n### 理论分析与预测准则\n\n在二元高斯目标分布上，双区块 Gibbs 采样器的效率由两个区块之间的后验相关性决定。设区块为 $(\\theta_1, \\theta_2)$。Gibbs 采样器从条件分布 $p(\\theta_1|\\theta_2)$ 和 $p(\\theta_2|\\theta_1)$ 中迭代抽样。链的收敛速率由后验相关的平方 $\\rho^2 = \\text{corr}(\\theta_1, \\theta_2 | \\text{data})^2$ 控制。较小的相关性绝对值 $|\\rho|$ 意味着较小的 $\\rho^2$，从而导致更快的收敛和样本中更少的自相关（即更快的混合）。\n\n我们的预测将基于计算每种参数化的后验相关性。后验相关性绝对值较小的参数化被预测为混合更快。\n\n联合后验分布与似然和先验的乘积成正比。对于高斯模型，对数后验是参数的二次函数，$\\log p(\\theta | y) \\propto -\\frac{1}{2} \\theta^T Q \\theta + \\text{线性项}$，其中 $\\theta$ 是参数向量，$Q$ 是后验精度矩阵。对于双变量向量 $\\theta = (\\theta_1, \\theta_2)^T$，后验相关性由 $\\rho = -Q_{12} / \\sqrt{Q_{11}Q_{22}}$ 给出。我们将所有 $i$ 的数据 $y_i=0$ 设置，这意味着样本均值 $\\bar{y}=0$。\n\n**1. 中心化参数化 (CP)**\n\n采样区块为 $(\\mu, z)$。对数后验密度为：\n$$ \\log p(\\mu, z | y) \\propto \\log p(y|z, \\tau^2) + \\log p(z|\\mu, \\sigma^2) + \\log p(\\mu | v_0) $$\n当 $y_i=0$ 时，对数似然项为 $\\log p(y|z, \\tau^2) \\propto -n z^2 / (2\\tau^2)$。\n$$ \\log p(\\mu, z | y) \\propto -\\frac{n z^2}{2\\tau^2} - \\frac{(z - \\mu)^2}{2\\sigma^2} - \\frac{\\mu^2}{2v_0} $$\n展开二次项揭示了 $(\\mu, z)$ 的后验精度矩阵：\n$$ \\log p(\\mu, z | y) \\propto -\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma^2} + \\frac{1}{v_0}\\right)\\mu^2 + \\left(\\frac{n}{\\tau^2} + \\frac{1}{\\sigma^2}\\right)z^2 - \\frac{2}{\\sigma^2} \\mu z \\right] $$\n这意味着后验精度矩阵是：\n$$ Q_{\\text{CP}} = \\begin{pmatrix} 1/v_0 + 1/\\sigma^2  -1/\\sigma^2 \\\\ -1/\\sigma^2  n/\\tau^2 + 1/\\sigma^2 \\end{pmatrix} $$\n$\\mu$ 和 $z$ 之间的后验相关性为：\n$$ \\rho_{\\text{CP}} = \\frac{-Q_{\\text{CP},12}}{\\sqrt{Q_{\\text{CP},11} Q_{\\text{CP},22}}} = \\frac{1/\\sigma^2}{\\sqrt{(1/v_0 + 1/\\sigma^2)(n/\\tau^2 + 1/\\sigma^2)}} $$\n\n**2. 非中心化参数化 (NCP)**\n\n采样区块为 $(\\mu, \\eta)$，其中 $z = \\mu + \\sigma\\eta$。对数后验密度为：\n$$ \\log p(\\mu, \\eta | y) \\propto \\log p(y|\\mu, \\eta, \\sigma^2, \\tau^2) + \\log p(\\eta) + \\log p(\\mu | v_0) $$\n当 $y_i=0$ 时，数据项基于 $y_i \\sim \\mathcal{N}(\\mu + \\sigma\\eta, \\tau^2)$，所以 $\\log p(y|\\mu,\\eta,\\dots) \\propto -n(\\mu + \\sigma\\eta)^2/(2\\tau^2)$。\n$$ \\log p(\\mu, \\eta | y) \\propto -\\frac{n(\\mu+\\sigma\\eta)^2}{2\\tau^2} - \\frac{\\eta^2}{2} - \\frac{\\mu^2}{2v_0} $$\n展开二次项：\n$$ \\log p(\\mu, \\eta | y) \\propto -\\frac{1}{2} \\left[ \\left(\\frac{n}{\\tau^2} + \\frac{1}{v_0}\\right)\\mu^2 + \\left(\\frac{n\\sigma^2}{\\tau^2} + 1\\right)\\eta^2 + \\frac{2n\\sigma}{\\tau^2}\\mu\\eta \\right] $$\n$(\\mu, \\eta)$ 的后验精度矩阵为：\n$$ Q_{\\text{NCP}} = \\begin{pmatrix} n/\\tau^2 + 1/v_0  n\\sigma/\\tau^2 \\\\ n\\sigma/\\tau^2  n\\sigma^2/\\tau^2 + 1 \\end{pmatrix} $$\n$\\mu$ 和 $\\eta$ 之间的后验相关性为：\n$$ \\rho_{\\text{NCP}} = \\frac{-Q_{\\text{NCP},12}}{\\sqrt{Q_{\\text{NCP},11} Q_{\\text{NCP},22}}} = \\frac{-n\\sigma/\\tau^2}{\\sqrt{(n/\\tau^2 + 1/v_0)(n\\sigma^2/\\tau^2 + 1)}} $$\n其中 $\\sigma = \\sqrt{\\sigma^2}$。\n\n### 经验验证\n\n我们将为两种参数化实现双区块 Gibbs 采样器。采样器所需的条件分布是从联合后验推导出来的。对于具有精度矩阵 $Q$ 的一般二元正态分布，条件分布 $p(\\theta_1|\\theta_2)$ 是 $\\mathcal{N}(-Q_{12}\\theta_2/Q_{11}, 1/Q_{11})$。\n\n**CP 采样器：**\n1. 采样 $z^{(t+1)} \\sim p(z | \\mu^{(t)}, y) = \\mathcal{N}\\left(\\frac{\\mu^{(t)}/\\sigma^2}{n/\\tau^2 + 1/\\sigma^2}, \\left(n/\\tau^2 + 1/\\sigma^2\\right)^{-1}\\right)$\n2. 采样 $\\mu^{(t+1)} \\sim p(\\mu | z^{(t+1)}) = \\mathcal{N}\\left(\\frac{z^{(t+1)}/\\sigma^2}{1/v_0 + 1/\\sigma^2}, \\left(1/v_0 + 1/\\sigma^2\\right)^{-1}\\right)$\n\n**NCP 采样器：**\n1. 采样 $\\eta^{(t+1)} \\sim p(\\eta | \\mu^{(t)}, y) = \\mathcal{N}\\left(\\frac{-n\\sigma\\mu^{(t)}/\\tau^2}{n\\sigma^2/\\tau^2 + 1}, \\left(n\\sigma^2/\\tau^2 + 1\\right)^{-1}\\right)$\n2. 采样 $\\mu^{(t+1)} \\sim p(\\mu | \\eta^{(t+1)}, y) = \\mathcal{N}\\left(\\frac{-n\\sigma\\eta^{(t+1)}/\\tau^2}{n/\\tau^2 + 1/v_0}, \\left(n/\\tau^2 + 1/v_0\\right)^{-1}\\right)$\n\n对于每个测试案例，我们运行两个采样器，丢弃一个老化期序列，然后计算 $\\mu$ 样本的经验滞后-1自相关。产生较小自相关的参数化被认为是经验上更快的。然后将预测与此经验发现进行比较。\n\n### 测试案例分析\n\n- **案例 1（大 $\\sigma^2=1000$，小数据信息 $n/\\tau^2=0.1$）：** 此处 $|\\rho_{\\text{CP}}|$ 很小（$\\approx 0.03$），而 $|\\rho_{\\text{NCP}}|$ 很大（$\\approx 0.95$）。我们预测 CP 会更快。\n- **案例 2（小 $\\sigma^2=0.01$，大数据信息 $n/\\tau^2=50$）：** 此处 $|\\rho_{\\text{CP}}|$ 很大（$\\approx 0.82$），而 $|\\rho_{\\text{NCP}}|$ 较小（$\\approx 0.58$）。我们预测 NCP 会更快。\n- **案例 3（中等情况）：** 此处 $|\\rho_{\\text{CP}}|$（$\\approx 0.55$）小于 $|\\rho_{\\text{NCP}}|$（$\\approx 0.80$）。我们预测 CP 会更快。\n\n该实现将系统地执行这些计算和比较。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Predicts and verifies Gibbs sampler performance for centered vs. non-centered\n    parametrizations of a hierarchical normal model.\n    \"\"\"\n    np.random.seed(0)\n\n    test_cases = [\n        # Case 1 (large prior variance relative to data information)\n        (1, 10.0, 1000.0, 100.0),\n        # Case 2 (strong data information relative to prior variance)\n        (50, 1.0, 0.01, 100.0),\n        # Case 3 (moderate regime)\n        (10, 5.0, 1.0, 10.0),\n    ]\n\n    n_iter = 20000\n    burn_in = 5000\n\n    results = []\n    \n    for case in test_cases:\n        n, tau2, sigma2, v0 = case\n        \n        # --- Theoretical Prediction using Posterior Correlation ---\n        \n        # Centered Parametrization (CP) posterior correlation for (mu, z)\n        q11_cp = 1.0 / sigma2 + 1.0 / v0\n        q22_cp = float(n) / tau2 + 1.0 / sigma2\n        q12_cp = -1.0 / sigma2\n        rho_cp = -q12_cp / np.sqrt(q11_cp * q22_cp)\n\n        # Non-Centered Parametrization (NCP) posterior correlation for (mu, eta)\n        sigma = np.sqrt(sigma2)\n        q11_ncp = float(n) / tau2 + 1.0 / v0\n        q22_ncp = float(n) * sigma2 / tau2 + 1.0\n        q12_ncp = float(n) * sigma / tau2\n        rho_ncp = -q12_ncp / np.sqrt(q11_ncp * q22_ncp)\n        \n        # Prediction: smaller absolute correlation means faster mixing\n        predicted_faster_is_cp = abs(rho_cp)  abs(rho_ncp)\n\n        # --- Empirical Verification using Gibbs Samplers ---\n        \n        # CP Gibbs Sampler\n        mu_samples_cp = np.zeros(n_iter)\n        mu_cp = 0.0\n        # Pre-compute conditional variances\n        s2_z_cp = 1.0 / (n / tau2 + 1.0 / sigma2)\n        s2_mu_cp = 1.0 / (1.0 / v0 + 1.0 / sigma2)\n        for i in range(n_iter):\n            # Update z | mu, y\n            mean_z = s2_z_cp * (mu_cp / sigma2)\n            z = np.random.normal(mean_z, np.sqrt(s2_z_cp))\n            # Update mu | z\n            mean_mu = s2_mu_cp * (z / sigma2)\n            mu_cp = np.random.normal(mean_mu, np.sqrt(s2_mu_cp))\n            mu_samples_cp[i] = mu_cp\n        \n        # NCP Gibbs Sampler\n        mu_samples_ncp = np.zeros(n_iter)\n        mu_ncp = 0.0\n        # Pre-compute conditional variances\n        s2_eta_ncp = 1.0 / (1.0 + n * sigma2 / tau2)\n        s2_mu_ncp = 1.0 / (1.0 / v0 + n / tau2)\n        for i in range(n_iter):\n            # Update eta | mu, y\n            mean_eta = s2_eta_ncp * (-n * sigma * mu_ncp / tau2)\n            eta = np.random.normal(mean_eta, np.sqrt(s2_eta_ncp))\n            # Update mu | eta, y\n            mean_mu = s2_mu_ncp * (-n * sigma * eta / tau2)\n            mu_ncp = np.random.normal(mean_mu, np.sqrt(s2_mu_ncp))\n            mu_samples_ncp[i] = mu_ncp\n\n        # Post-processing: remove burn-in\n        mu_samples_cp_post_burn = mu_samples_cp[burn_in:]\n        mu_samples_ncp_post_burn = mu_samples_ncp[burn_in:]\n\n        # Compute empirical lag-1 autocorrelation for mu\n        # np.corrcoef calculates the correlation matrix; [0, 1] gives the lag-1 ACF\n        acf_cp = np.corrcoef(mu_samples_cp_post_burn[:-1], mu_samples_cp_post_burn[1:])[0, 1]\n        acf_ncp = np.corrcoef(mu_samples_ncp_post_burn[:-1], mu_samples_ncp_post_burn[1:])[0, 1]\n        \n        # Empirical result: smaller ACF means faster mixing\n        empirically_faster_is_cp = acf_cp  acf_ncp\n        \n        # --- Comparison ---\n        # Check if the theoretical prediction matches the empirical result\n        agreement = (predicted_faster_is_cp == empirically_faster_is_cp)\n        results.append(str(agreement))\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3301996"}]}