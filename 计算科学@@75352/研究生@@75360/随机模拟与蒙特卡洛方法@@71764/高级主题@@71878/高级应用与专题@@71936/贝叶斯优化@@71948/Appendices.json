{"hands_on_practices": [{"introduction": "贝叶斯优化的有效性取决于其采集函数，它能智能地引导对最优值的搜索。当存在现实世界的约束时，该函数必须在寻求更优目标值和满足约束之间取得平衡。本练习将带你从第一性原理出发，推导约束预期增益（Constrained Expected Improvement, CEI），展示如何将改进的概率和满足约束的概率结合成一个统一的、有原则的准则。[@problem_id:3291581]", "problem": "一个研究团队正在使用约束贝叶斯优化 (Constrained Bayesian Optimization) 来为一个黑箱模拟器调整一个标量设计变量 $x \\in \\mathbb{R}$。其目标是在单个不等式约束 $c(x) \\le 0$ 的条件下，最小化一个未知目标函数 $f(x)$。由于该模拟器计算成本高昂，该团队采用高斯过程 (GP) 回归 (Gaussian Process (GP)) 来对潜在的无噪声函数 $f$ 和 $c$ 进行建模。在一个候选点 $x$ 处，潜在值 $f(x)$ 和 $c(x)$ 的当前高斯过程后验是独立的，并服从正态分布：\n- $f(x) \\sim \\mathcal{N}(\\mu_{f}, \\sigma_{f}^{2})$，\n- $c(x) \\sim \\mathcal{N}(\\mu_{c}, \\sigma_{c}^{2})$，\n$f(x)$ 和 $c(x)$ 之间的预测独立性是基于独立的先验和条件独立的数据似然。\n\n将在 $x$ 处为了最小化的约束期望提升 (CEI) 定义为期望\n$$\n\\operatorname{CEI}(x) \\equiv \\mathbb{E}\\big[(f_{\\star}-f(x))_{+} \\,\\mathbf{1}\\{c(x)\\le 0\\}\\big],\n$$\n其中 $(a)_{+} \\equiv \\max\\{a,0\\}$，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数，$f_{\\star}$ 是当前观测到的最佳可行目标值。\n\n仅从期望、指示函数和正态分布性质的定义出发，完成以下任务：\n1. 在所述的高斯过程独立性假设下，推导 $\\operatorname{CEI}(x)$ 的闭式表达式，并用 $\\mu_{f}$、$\\sigma_{f}$、$f_{\\star}$、$\\mu_{c}$ 和 $\\sigma_{c}$ 来表示你的答案。\n2. 在一个参数为 $\\mu_{f} = 0.4$, $\\sigma_{f} = 0.3$, $f_{\\star} = 0.4$, $\\mu_{c} = 0$, $\\sigma_{c} = 1$ 的候选点处，对你的表达式进行数值计算。\n\n将最终数值答案四舍五入到四位有效数字。将最终结果表示为一个无单位的纯数。", "solution": "该问题是有效的，因为它在科学上基于贝叶斯优化理论，是适定的、客观的，并且包含得出唯一解所需的所有信息。\n\n约束期望提升 $\\operatorname{CEI}(x)$ 被定义为两项乘积的期望：目标函数中的潜在提升 $(f_{\\star}-f(x))_{+}$，以及可行性指示函数 $\\mathbf{1}\\{c(x)\\le 0\\}$。\n$$\n\\operatorname{CEI}(x) = \\mathbb{E}\\big[(f_{\\star}-f(x))_{+} \\,\\mathbf{1}\\{c(x)\\le 0\\}\\big]\n$$\n该期望是关于随机变量 $f(x)$ 和 $c(x)$ 的联合概率分布计算的。问题陈述了 $f(x)$ 和 $c(x)$ 的高斯过程后验是独立的。令 $Y_f = f(x)$ 和 $Y_c = c(x)$。它们的分布如下：\n- $Y_f \\sim \\mathcal{N}(\\mu_{f}, \\sigma_{f}^{2})$\n- $Y_c \\sim \\mathcal{N}(\\mu_{c}, \\sigma_{c}^{2})$\n\n由于它们的独立性，联合概率密度函数是它们各自概率密度函数的乘积：$p(y_f, y_c) = p_f(y_f) p_c(y_c)$。\n此外，对于两个独立的随机变量 $A$ 和 $B$，它们乘积的期望等于它们各自期望的乘积，即 $\\mathbb{E}[AB] = \\mathbb{E}[A]\\mathbb{E}[B]$。在我们的情况中，项 $(f_{\\star}-Y_f)_{+}$ 仅是 $Y_f$ 的函数，而 $\\mathbf{1}\\{Y_c\\le 0\\}$ 仅是 $Y_c$ 的函数。因此，这两个派生的随机变量也是独立的。我们可以将期望分开：\n$$\n\\operatorname{CEI}(x) = \\mathbb{E}\\big[(f_{\\star}-Y_f)_{+}\\big] \\cdot \\mathbb{E}\\big[\\mathbf{1}\\{Y_c\\le 0\\}\\big]\n$$\n我们现在将分别计算每个期望。\n\n**第一部分：闭式表达式的推导**\n\n让我们首先计算目标提升的期望，我们将其表示为 $I_f$：\n$$\nI_f = \\mathbb{E}\\big[(f_{\\star}-Y_f)_{+}\\big] = \\int_{-\\infty}^{\\infty} (f_{\\star}-y_f)_{+} \\, p_f(y_f) \\, dy_f\n$$\n其中 $p_f(y_f)$ 是均值为 $\\mu_f$、方差为 $\\sigma_f^2$ 的正态分布的概率密度函数 (PDF)。项 $(f_{\\star}-y_f)_{+} = \\max\\{f_{\\star}-y_f, 0\\}$ 仅在 $y_f  f_{\\star}$ 时非零。因此，积分的上限变为 $f_{\\star}$：\n$$\nI_f = \\int_{-\\infty}^{f_{\\star}} (f_{\\star}-y_f) \\, p_f(y_f) \\, dy_f\n$$\n我们可以将这个积分分成两部分：\n$$\nI_f = f_{\\star} \\int_{-\\infty}^{f_{\\star}} p_f(y_f) \\, dy_f - \\int_{-\\infty}^{f_{\\star}} y_f \\, p_f(y_f) \\, dy_f\n$$\n为了计算这些积分，我们进行变量替换以标准化 $Y_f$。令 $Z_f = \\frac{Y_f - \\mu_f}{\\sigma_f}$。变量 $Z_f$ 服从标准正态分布 $Z_f \\sim \\mathcal{N}(0, 1)$，其 PDF 为 $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$，累积分布函数 (CDF) 为 $\\Phi(z)$。微分元变换为 $p_f(y_f) \\, dy_f = \\phi(z_f) \\, dz_f$。积分上限 $y_f = f_{\\star}$ 变为 $z_f = \\frac{f_{\\star}-\\mu_f}{\\sigma_f}$。我们定义 $\\gamma_f = \\frac{f_{\\star}-\\mu_f}{\\sigma_f}$。\n\n第一个积分是概率 $P(Y_f \\le f_{\\star})$：\n$$\n\\int_{-\\infty}^{f_{\\star}} p_f(y_f) \\, dy_f = P(Y_f \\le f_{\\star}) = P\\left(Z_f \\le \\frac{f_{\\star}-\\mu_f}{\\sigma_f}\\right) = \\Phi(\\gamma_f)\n$$\n第二个积分是：\n$$\n\\int_{-\\infty}^{f_{\\star}} y_f \\, p_f(y_f) \\, dy_f = \\int_{-\\infty}^{\\gamma_f} (\\mu_f + \\sigma_f z_f) \\, \\phi(z_f) \\, dz_f\n$$\n$$\n= \\mu_f \\int_{-\\infty}^{\\gamma_f} \\phi(z_f) \\, dz_f + \\sigma_f \\int_{-\\infty}^{\\gamma_f} z_f \\, \\phi(z_f) \\, dz_f\n$$\n根据定义，积分 $\\int_{-\\infty}^{\\gamma_f} \\phi(z_f) \\, dz_f$ 是 $\\Phi(\\gamma_f)$。对于第二个积分，我们使用性质 $\\frac{d}{dz}(-\\phi(z)) = z\\phi(z)$。\n$$\n\\int_{-\\infty}^{\\gamma_f} z_f \\, \\phi(z_f) \\, dz_f = [-\\phi(z_f)]_{-\\infty}^{\\gamma_f} = -\\phi(\\gamma_f) - \\lim_{z_f \\to -\\infty} (-\\phi(z_f)) = -\\phi(\\gamma_f)\n$$\n所以，第二个积分的计算结果为 $\\mu_f \\Phi(\\gamma_f) - \\sigma_f \\phi(\\gamma_f)$。\n将这些结果合并到 $I_f$ 中：\n$$\nI_f = f_{\\star} \\Phi(\\gamma_f) - (\\mu_f \\Phi(\\gamma_f) - \\sigma_f \\phi(\\gamma_f))\n$$\n$$\nI_f = (f_{\\star} - \\mu_f) \\Phi(\\gamma_f) + \\sigma_f \\phi(\\gamma_f)\n$$\n\n接下来，我们计算可行性指示函数的期望，我们将其表示为 $I_c$：\n$$\nI_c = \\mathbb{E}\\big[\\mathbf{1}\\{Y_c\\le 0\\}\\big] = \\int_{-\\infty}^{\\infty} \\mathbf{1}\\{y_c\\le 0\\} \\, p_c(y_c) \\, dy_c\n$$\n这就是 $Y_c$ 小于或等于 $0$ 的概率：\n$$\nI_c = P(Y_c \\le 0) = \\int_{-\\infty}^{0} p_c(y_c) \\, dy_c\n$$\n用 $Z_c = \\frac{Y_c - \\mu_c}{\\sigma_c} \\sim \\mathcal{N}(0, 1)$ 对变量 $Y_c$ 进行标准化，条件 $Y_c \\le 0$ 变为 $Z_c \\le \\frac{0-\\mu_c}{\\sigma_c} = -\\frac{\\mu_c}{\\sigma_c}$。\n因此，概率为：\n$$\nI_c = P\\left(Z_c \\le -\\frac{\\mu_c}{\\sigma_c}\\right) = \\Phi\\left(-\\frac{\\mu_c}{\\sigma_c}\\right)\n$$\n最后，$\\operatorname{CEI}(x)$ 的完整表达式是 $I_f$ 和 $I_c$ 的乘积：\n$$\n\\operatorname{CEI}(x) = \\left[ (f_{\\star} - \\mu_f) \\Phi\\left(\\frac{f_{\\star} - \\mu_f}{\\sigma_f}\\right) + \\sigma_f \\phi\\left(\\frac{f_{\\star} - \\mu_f}{\\sigma_f}\\right) \\right] \\Phi\\left(-\\frac{\\mu_c}{\\sigma_c}\\right)\n$$\n这就是所要求的闭式表达式。\n\n**第二部分：数值计算**\n\n对于一个候选点 $x$，我们有以下给定的参数值：\n- $\\mu_{f} = 0.4$\n- $\\sigma_{f} = 0.3$\n- $f_{\\star} = 0.4$\n- $\\mu_{c} = 0$\n- $\\sigma_{c} = 1$\n\n首先，我们计算标准正态 CDF 和 PDF 的参数。\n对于目标部分：\n$$\n\\gamma_f = \\frac{f_{\\star} - \\mu_f}{\\sigma_f} = \\frac{0.4 - 0.4}{0.3} = 0\n$$\n对于约束部分：\n$$\n-\\frac{\\mu_c}{\\sigma_c} = -\\frac{0}{1} = 0\n$$\n现在，我们在这些参数点上计算特殊函数的值：\n- $\\Phi(0) = 0.5$\n- $\\phi(0) = \\frac{1}{\\sqrt{2\\pi}}$\n\n将这些值代入 $\\operatorname{CEI}(x)$ 的表达式中：\n$$\n\\operatorname{CEI}(x) = \\left[ (0.4 - 0.4) \\Phi(0) + 0.3 \\cdot \\phi(0) \\right] \\cdot \\Phi(0)\n$$\n$$\n\\operatorname{CEI}(x) = \\left[ 0 \\cdot 0.5 + 0.3 \\cdot \\frac{1}{\\sqrt{2\\pi}} \\right] \\cdot 0.5\n$$\n$$\n\\operatorname{CEI}(x) = \\left( 0.3 \\cdot \\frac{1}{\\sqrt{2\\pi}} \\right) \\cdot 0.5\n$$\n$$\n\\operatorname{CEI}(x) = \\frac{0.15}{\\sqrt{2\\pi}}\n$$\n现在我们计算其数值：\n$$\n\\operatorname{CEI}(x) \\approx \\frac{0.15}{2.50662827} \\approx 0.059841342\n$$\n四舍五入到四位有效数字，我们得到 $0.05984$。", "answer": "$$\\boxed{0.05984}$$", "id": "3291581"}, {"introduction": "虽然像预期增益（Expected Improvement）这样的解析采集函数非常强大，但另一种高效的方法是使用基于采样的策略。本练习将指导你实现汤普森采样（Thompson Sampling, TS），一种流行的基于采样的策略。通过从高斯过程后验中抽取一个函数样本并找到其最大值，你将获得基于高斯过程的优化计算机制的实践经验，并理解TS如何自然地平衡探索与利用。[@problem_id:3291534]", "problem": "为使用带有平方指数协方差的高斯过程先验的连续域贝叶斯优化，实现一个单步汤普森采样策略。您的程序必须为多个测试用例中的每一个计算出所选的下一个评估位置，并按规定格式输出结果。\n\n从以下基本原理开始：\n- 在一个紧区间上，作用于未知函数 $f$ 的高斯过程（GP）先验，其均值为零，协方差核为正定核。其定义性质为：任何有限次函数评估的集合都服从联合多元正态分布。\n- 对于方差参数为 $\\alpha^2$、长度尺度为 $\\ell$ 的平方指数协方差核，其核函数由下式给出：\n$$\nk(x,x') \\;=\\; \\alpha^2 \\exp\\!\\Big(-\\,\\frac{(x-x')^2}{2\\,\\ell^2}\\Big).\n$$\n- 观测值受到独立同分布的高斯噪声（已知方差为 $\\tau^2$）的干扰，因此对于观测到的输入 $\\{x_i\\}_{i=1}^n$ 和输出 $\\{y_i\\}_{i=1}^n$，我们有 $y_i \\,=\\, f(x_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,\\tau^2)$ 且在各 $i$ 之间相互独立。\n\n定义与实现要求：\n- 考虑一个闭区间 $[a,b] \\subset \\mathbb{R}$ 和一个由 $M$ 个等距点组成的离散化网格 $X_\\star = \\{x_\\star^{(j)}\\}_{j=1}^M$，该网格包含端点 $a$ 和 $b$。\n- 给定观测数据 $(X,y)$，其中 $X = (x_1,\\dots,x_n)$ 且 $y=(y_1,\\dots,y_n)$，在网格点 $X_\\star$ 上的高斯过程后验是一个多元正态分布。您必须将高斯条件恒等式应用于 $(f(X), f(X_\\star))$ 的联合先验，并考虑 $y$ 上的加性高斯噪声，以推导出在 $X_\\star$ 上的后验均值向量 $m_\\star$ 和后验协方差矩阵 $\\Sigma_\\star$。\n- 在网格 $X_\\star$ 上的单步汤普森采样策略是，从后验分布中抽取一个样本 $g_\\star \\sim \\mathcal{N}(m_\\star,\\Sigma_\\star)$，并选择网格上的最大化者作为下一个要评估的输入：\n$$\nx_{\\text{TS}} \\;=\\; \\arg\\max_{x \\in X_\\star} g_\\star(x).\n$$\n- 如果在数值容差范围内出现多个最大值，则选择其中索引最小的一个（即最左侧的网格点）。\n\n数值要求：\n- 使用 Cholesky 分解来求解线性方程组和从多元正态分布中采样。\n- 在分解任何协方差矩阵时，向其对角线添加一个小的正“抖动”$\\epsilon$，以确保数值上的正定性。使用 $\\epsilon = 10^{-9}$。\n- 所有算术运算均为实值运算，不带物理单位。\n- 对于每个测试用例，在抽取后验样本之前，使用指定种子初始化伪随机数生成器。种子必须独立地应用于每个测试用例，并且仅用于采样步骤。\n\n离散化：\n- 对于所有测试用例，使用相同的网格大小 $M = 501$ 和区间 $[a,b]=[0,1]$。因此，$x_\\star^{(j)} = a + (j-1)\\,\\frac{b-a}{M-1}$，其中 $j=1,\\dots,M$。\n\n测试套件：\n- 用例 1（仅先验）：\n  - 输入：无观测数据 ($n=0$)。\n  - 超参数：$\\alpha = 1.2$，$\\ell = 0.18$，$\\tau = 0.05$（当 $n=0$ 时未使用）。\n  - 种子：$1234$。\n- 用例 2（单个带噪观测）：\n  - 输入：$X=(0.3)$，$y=(0.4)$，因此 $n=1$。\n  - 超参数：$\\alpha = 0.9$，$\\ell = 0.2$，$\\tau = 0.01$。\n  - 种子：$2025$。\n- 用例 3（中等数据量，中等噪声）：\n  - 输入：$X=(0.15,\\,0.5,\\,0.85)$，$y=(1.0,\\,-0.2,\\,0.7)$，因此 $n=3$。\n  - 超参数：$\\alpha = 1.0$，$\\ell = 0.25$，$\\tau = 0.05$。\n  - 种子：$7$。\n- 用例 4（近似共线设计，极低噪声）：\n  - 输入：$X=(0.499,\\,0.5,\\,0.501,\\,0.9)$，$y=(0.0,\\,0.0,\\,0.0,\\,0.2)$，因此 $n=4$。\n  - 超参数：$\\alpha = 0.8$，$\\ell = 0.05$，$\\tau = 10^{-4}$。\n  - 种子：$42$。\n\n您的程序必须为每个用例在网格 $X_\\star$ 上计算 $x_{\\text{TS}}$，并将这四个值按用例顺序作为单行输出，格式为一个包含四个实数的 Python 风格列表，例如 $[r_1,r_2,r_3,r_4]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的结果，结果为逗号分隔的列表，没有空格，并严格按照用例的顺序排列：$[x_{\\text{TS,1}},x_{\\text{TS,2}},x_{\\text{TS,3}},x_{\\text{TS,4}}]$。", "solution": "我们从高斯过程先验和高斯似然模型开始。设 $f \\sim \\mathcal{GP}(0,k)$，其中 $k(x,x') = \\alpha^2 \\exp\\!\\big(-\\frac{(x-x')^2}{2\\ell^2}\\big)$。观测值满足 $y_i = f(x_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\tau^2)$。对于有限的输入集，其联合分布是多元正态的，我们将利用这一点来推导后验预测分布。\n\n定义观测输入 $X = (x_1,\\dots,x_n)$ 和网格输入 $X_\\star = (x_\\star^{(1)},\\dots,x_\\star^{(M)})$。构建以下协方差矩阵和向量：\n- $n \\times n$ 的格拉姆矩阵 $K_{XX}$，其元素为 $[K_{XX}]_{ij} = k(x_i,x_j)$。\n- $M \\times n$ 的互协方差矩阵 $K_{\\star X}$，其元素为 $[K_{\\star X}]_{j i} = k(x_\\star^{(j)}, x_i)$。\n- $M \\times M$ 的网格格拉姆矩阵 $K_{\\star \\star}$，其元素为 $[K_{\\star \\star}]_{j j'} = k(x_\\star^{(j)}, x_\\star^{(j')})$。\n\n令 $\\sigma^2_n = \\tau^2$ 表示噪声方差。$(f(X), f(X_\\star))$ 的联合先验是高斯分布，其均值为零，协方差为\n$$\n\\begin{bmatrix}\nK_{XX}  K_{X \\star} \\\\\nK_{\\star X}  K_{\\star \\star}\n\\end{bmatrix}.\n$$\n在高斯观测噪声下，似然函数意味着 $y \\mid f(X) \\sim \\mathcal{N}(f(X), \\sigma_n^2 I_n)$。通过对 $f(X)$ 进行边缘化，得到 $y$ 的边缘分布为 $y \\sim \\mathcal{N}(0, K_{XX} + \\sigma_n^2 I_n)$。多元正态分布的高斯条件恒等式随后给出了在网格点上的后验预测分布：\n$$\nf_\\star \\mid X, y, X_\\star \\sim \\mathcal{N}\\big(m_\\star, \\Sigma_\\star \\big),\n$$\n其中\n$$\nm_\\star \\;=\\; K_{\\star X}\\,(K_{XX} + \\sigma_n^2 I_n)^{-1} y,\n$$\n和\n$$\n\\Sigma_\\star \\;=\\; K_{\\star \\star} \\;-\\; K_{\\star X}\\,(K_{XX} + \\sigma_n^2 I_n)^{-1} K_{X \\star}.\n$$\n\n这些公式直接源于对联合高斯向量进行条件化的标准结果。具体来说，如果\n$$\n\\begin{bmatrix} u \\\\ v \\end{bmatrix} \\sim \\mathcal{N}\\!\\left( \\begin{bmatrix} \\mu_u \\\\ \\mu_v \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{uu}  \\Sigma_{uv} \\\\ \\Sigma_{vu}  \\Sigma_{vv} \\end{bmatrix} \\right),\n$$\n那么\n$$\nu \\mid v \\sim \\mathcal{N}\\!\\big(\\mu_u + \\Sigma_{uv}\\Sigma_{vv}^{-1}(v - \\mu_v),\\; \\Sigma_{uu} - \\Sigma_{uv}\\Sigma_{vv}^{-1}\\Sigma_{vu}\\big).\n$$\n将 $u = f_\\star$，$v = y$，$\\mu_u = 0$，$\\mu_v=0$，$\\Sigma_{uu} = K_{\\star \\star}$，$\\Sigma_{uv} = K_{\\star X}$ 以及 $\\Sigma_{vv} = K_{XX} + \\sigma_n^2 I_n$ 进行对应，即可得到所述的后验分布。\n\n汤普森采样需要从后验分布中抽取一个函数样本，在网格 $X_\\star$ 上，这表现为抽取一个样本 $g_\\star \\sim \\mathcal{N}(m_\\star, \\Sigma_\\star)$。为了高效且稳定地进行计算：\n- 对正定矩阵 $K_{XX} + \\sigma_n^2 I_n + \\epsilon I_n$ 使用 Cholesky 分解以避免显式矩阵求逆。定义 $L = \\operatorname{chol}(K_{XX} + \\sigma_n^2 I_n + \\epsilon I_n)$，其中 $\\epsilon = 10^{-9}$。通过求解两个三角系统来计算 $(K_{XX} + \\sigma_n^2 I_n)^{-1} y$：首先解 $L z = y$ 得到 $z$，然后解 $L^\\top \\alpha = z$ 得到 $\\alpha$，这样便得到 $\\alpha = (K_{XX} + \\sigma_n^2 I_n)^{-1} y$。类似地，通过解 $L W = K_{X \\star}$ 得到 $W$ 来计算 $V = L^{-1} K_{X \\star}$，因此 $K_{\\star X} (K_{XX} + \\sigma_n^2 I_n)^{-1} K_{X \\star} = (K_{\\star X} L^{-T})(L^{-1} K_{X \\star}) = V^\\top V$。\n- 因此，将后验均值计算为 $m_\\star = K_{\\star X}\\,\\alpha$，后验协方差计算为 $\\Sigma_\\star = K_{\\star \\star} - V^\\top V$。为了采样时的数值稳定性，如果需要，可在分解 $\\Sigma_\\star$ 之前向其添加 $\\epsilon I_M$。\n- 为进行采样，计算 Cholesky 因子 $L_\\star = \\operatorname{chol}(\\Sigma_\\star + \\epsilon I_M)$，并设 $g_\\star = m_\\star + L_\\star z$，其中 $z \\sim \\mathcal{N}(0, I_M)$。在抽取 $z$ 之前，立即为每个测试用例按规定设置伪随机数生成器的种子。\n\n当 $n=0$ 时，没有观测数据。在这种情况下，$m_\\star = 0$ 且 $\\Sigma_\\star = K_{\\star \\star}$，采样过程简化为从网格上的先验分布中抽取样本。\n\n最后，计算汤普森采样决策为 $x_{\\text{TS}} = \\arg\\max_{x \\in X_\\star} g_\\star(x)$，并通过选择最小索引来解决平局问题。由于 $X_\\star$ 是在 $[0,1]$ 上等距分布的 $M=501$ 个点，我们有 $x_\\star^{(j)} = \\frac{j-1}{500}$，其中 $j=1,\\dots,501$。\n\n我们现在概述每个测试用例的算法：\n- 在 $[0,1]$ 上构建 $X_\\star$，其中 $M=501$。\n- 如果 $n0$，构建 $K_{XX}$、$K_{\\star X}$ 和 $K_{\\star \\star}$。将 $\\sigma_n^2 I_n$ 和抖动 $\\epsilon I_n$ 添加到 $K_{XX}$，并进行 Cholesky 分解。通过三角求解和舒尔补公式计算 $m_\\star$ 和 $\\Sigma_\\star$。如果需要，向 $\\Sigma_\\star$ 添加抖动。\n- 如果 $n=0$，设置 $m_\\star = 0$ 和 $\\Sigma_\\star = K_{\\star \\star}$（带抖动）。\n- 初始化伪随机数生成器，并通过 $\\Sigma_\\star + \\epsilon I_M$ 的 Cholesky 因子抽取 $g_\\star \\sim \\mathcal{N}(m_\\star,\\Sigma_\\star)$。\n- 返回使 $g_\\star$ 最大化的网格点。\n\n此过程基于高斯过程和多元正态条件化的基本属性，确保了在一个离散化为精细网格的连续域上，单步汤普森采样的实现是正确且数值稳定的。\n\n程序将针对四个指定的用例执行这些步骤，并以要求的格式单行打印出四个选定的网格位置。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef se_kernel(x, xprime, alpha, ell):\n    # Squared Exponential (RBF) kernel for 1D arrays x, xprime\n    # Returns Gram matrix of shape (len(x), len(xprime))\n    x = np.atleast_2d(x).T  # (n,1)\n    xprime = np.atleast_2d(xprime).T  # (m,1)\n    d2 = (x - xprime.T) ** 2  # (n,m)\n    return (alpha ** 2) * np.exp(-0.5 * d2 / (ell ** 2))\n\ndef cholesky_psd(A, jitter=1e-9, max_tries=5):\n    # Attempt a Cholesky factorization, increasing jitter if needed\n    jitter_scale = 1.0\n    for _ in range(max_tries):\n        try:\n            L = np.linalg.cholesky(A + (jitter * jitter_scale) * np.eye(A.shape[0]))\n            return L\n        except np.linalg.LinAlgError:\n            jitter_scale *= 10.0\n    # Final attempt may still fail; raise the error\n    L = np.linalg.cholesky(A + (jitter * jitter_scale) * np.eye(A.shape[0]))\n    return L\n\ndef gp_posterior_params(X_obs, y_obs, X_star, alpha, ell, tau, jitter=1e-9):\n    # Compute posterior mean m_star and covariance Sigma_star at X_star\n    n = len(X_obs)\n    M = len(X_star)\n    K_ss = se_kernel(X_star, X_star, alpha, ell)\n    if n == 0:\n        m_star = np.zeros(M)\n        # Add jitter for numerical stability\n        Sigma_star = K_ss.copy()\n        # Ensure symmetry\n        Sigma_star = (Sigma_star + Sigma_star.T) * 0.5\n        return m_star, Sigma_star\n    # Build K_xx and cross-covariances\n    K_xx = se_kernel(X_obs, X_obs, alpha, ell)\n    # Add noise variance and jitter to K_xx\n    K_xx_noisy = K_xx + (tau**2) * np.eye(n) + jitter * np.eye(n)\n    L = cholesky_psd(K_xx_noisy, jitter=jitter)\n    # Solve for alpha_vec = (K_xx + tau^2 I)^{-1} y\n    # First solve L z = y, then L^T alpha_vec = z\n    z = np.linalg.solve(L, y_obs)\n    alpha_vec = np.linalg.solve(L.T, z)\n    K_sx = se_kernel(X_star, X_obs, alpha, ell)  # (M,n)\n    # Posterior mean\n    m_star = K_sx @ alpha_vec  # (M,)\n    # Compute V = L^{-1} K_xs where K_xs = K(X_obs, X_star)\n    K_xs = K_sx.T  # (n,M)\n    V = np.linalg.solve(L, K_xs)  # (n,M)\n    # Posterior covariance: K_ss - V^T V\n    Sigma_star = K_ss - V.T @ V\n    # Symmetrize to remove numerical asymmetry\n    Sigma_star = (Sigma_star + Sigma_star.T) * 0.5\n    return m_star, Sigma_star\n\ndef thompson_sample_argmax(X_star, m_star, Sigma_star, seed, jitter=1e-9):\n    # Draw one sample from N(m_star, Sigma_star) using Cholesky and return argmax location\n    # Add jitter to ensure PSD\n    Ls = cholesky_psd(Sigma_star, jitter=jitter)\n    rng = np.random.RandomState(seed)\n    z = rng.randn(len(X_star))\n    g_star = m_star + Ls @ z\n    # Argmax with tie-break to smallest index\n    idx = int(np.argmax(g_star))\n    return X_star[idx]\n\ndef solve():\n    # Define grid [0,1] with M=501\n    M = 501\n    a, b = 0.0, 1.0\n    X_star = np.linspace(a, b, M)\n\n    # Define the test cases from the problem statement.\n    # Each case: dict with X_obs, y_obs, alpha, ell, tau, seed\n    test_cases = [\n        # Case 1: prior only\n        {\n            \"X_obs\": np.array([]),\n            \"y_obs\": np.array([]),\n            \"alpha\": 1.2,\n            \"ell\": 0.18,\n            \"tau\": 0.05,  # unused\n            \"seed\": 1234,\n        },\n        # Case 2: single observation\n        {\n            \"X_obs\": np.array([0.3]),\n            \"y_obs\": np.array([0.4]),\n            \"alpha\": 0.9,\n            \"ell\": 0.2,\n            \"tau\": 0.01,\n            \"seed\": 2025,\n        },\n        # Case 3: three observations\n        {\n            \"X_obs\": np.array([0.15, 0.5, 0.85]),\n            \"y_obs\": np.array([1.0, -0.2, 0.7]),\n            \"alpha\": 1.0,\n            \"ell\": 0.25,\n            \"tau\": 0.05,\n            \"seed\": 7,\n        },\n        # Case 4: nearly collinear design, very low noise\n        {\n            \"X_obs\": np.array([0.499, 0.5, 0.501, 0.9]),\n            \"y_obs\": np.array([0.0, 0.0, 0.0, 0.2]),\n            \"alpha\": 0.8,\n            \"ell\": 0.05,\n            \"tau\": 1e-4,\n            \"seed\": 42,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X_obs = case[\"X_obs\"]\n        y_obs = case[\"y_obs\"]\n        alpha = case[\"alpha\"]\n        ell = case[\"ell\"]\n        tau = case[\"tau\"]\n        seed = case[\"seed\"]\n\n        m_star, Sigma_star = gp_posterior_params(X_obs, y_obs, X_star, alpha, ell, tau, jitter=1e-9)\n        x_ts = thompson_sample_argmax(X_star, m_star, Sigma_star, seed=seed, jitter=1e-9)\n        # Ensure Python float printing consistency\n        results.append(x_ts)\n\n    # Final print statement in the exact required format (no spaces).\n    print(f\"[{','.join(map(lambda v: repr(float(v)), results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3291534"}, {"introduction": "在许多科学应用中，单次实验的成本和精度并非固定不变；我们通常可以选择运行成本更高、保真度更高的评估。本练习探讨了这样一种场景，其中评估值来自于长度可调的马尔可夫链蒙特卡洛（MCMC）模拟。你将制定一个策略，联合优化“在哪里”采样和“模拟多长时间”这两个决策，以最大化每单位计算成本所获得的信息增益，这展示了贝叶斯实验设计框架的灵活性。[@problem_id:3291533]", "problem": "给定一个有限域、单步贝叶斯实验设计问题。该问题形式化了当输入 $x$ 处的评估是通过运行具有依赖于输入的混合时间的有限长度马尔可夫链获得时，贝叶斯优化与基于马尔可夫链蒙特卡洛（MCMC）的估计器之间的相互作用。目标是从一个离散域中选择单个输入 $x$ 和一个整数链长 $L \\ge 1$，以共同最大化一个成本敏感、基于方差缩减的采集函数。\n\n基本和建模假设：\n\n- 未知目标函数 $f(\\cdot)$ 被建模为具有零均值和协方差（核）$k(\\cdot,\\cdot)$ 的高斯过程（GP）先验。该先验定义在有限候选集 $\\mathcal{X} = \\{x_1, \\ldots, x_N\\}$ 上。\n- 在选定的输入 $x \\in \\mathcal{X}$ 处，获得观测值的唯一方法是使用马尔可夫链蒙特卡洛（MCMC）估计器来估计期望。该链具有积分自相关时间（混合时间）$\\tau(x)  0$，且被积函数在目标分布下的方差为 $\\sigma_g^2(x)  0$。观测值是长度为 $L \\in \\mathbb{N}$ 的链的时间平均值，其噪声方差为\n  $\n  v_n(x,L) \\approx \\dfrac{2 \\, \\tau(x) \\, \\sigma_g^2(x)}{L} \\equiv \\dfrac{a(x)}{L},\n  $\n  这源于马尔可夫链蒙特卡洛中标准有效样本量近似，其中 $a(x) \\equiv 2 \\tau(x) \\sigma_g^2(x)$。\n- 在输入 $x$ 处运行长度为 $L$ 的 MCMC 链的计算成本建模为\n  $\n  \\mathrm{cost}(x,L) = b + t(x) \\, L,\n  $\n  其中 $b \\ge 0$ 是一个固定的开销成本，$t(x)  0$ 是在 $x$ 处的每次迭代时间。\n\n给定在 $x$ 处带有噪声方差 $v_n(x,L)$ 的单个带噪观测值，通过高斯条件作用，GP 在任意 $x' \\in \\mathcal{X}$ 处的后验方差从 $\\mathrm{Var}[f(x')]$ 减小到\n$\n\\mathrm{Var}_{\\text{post}}[f(x')] = \\mathrm{Var}_{\\text{prior}}[f(x')] - \\dfrac{k(x',x)^2}{\\mathrm{Var}_{\\text{prior}}[f(x)] + v_n(x,L)}.\n$\n定义一个加权积分方差缩减泛函\n$\nS(x,L) \\equiv \\sum_{i=1}^N w_i \\left( \\mathrm{Var}_{\\text{prior}}[f(x_i)] - \\mathrm{Var}_{\\text{post}}[f(x_i)] \\right) = \\sum_{i=1}^N w_i \\, \\dfrac{k(x_i,x)^2}{\\mathrm{Var}_{\\text{prior}}[f(x)] + v_n(x,L)},\n$\n其中 $w_i  0$ 是给定的权重。我们希望最大化的采集函数是单位成本的方差缩减，\n$\nA(x,L) \\equiv \\dfrac{S(x,L)}{\\mathrm{cost}(x,L)}.\n$\n您必须选择 $x \\in \\mathcal{X}$ 和 $L \\in \\mathbb{N}$ 来最大化 $A(x,L)$，约束条件为 $L_{\\min} \\le L \\le L_{\\max}$，其中 $L_{\\min} = 1$ 且 $L_{\\max}$ 是一个给定的上界。如果 $A(x,L)$ 的最大值出现平局，选择最小的 $x$ 索引（从零开始），然后在平局的项中选择最小的 $L$。\n\n所有高斯过程先验量都是已知的。核函数是平方指数核（也称为径向基函数），其方差参数为 $\\sigma_f^2$，长度尺度为 $\\ell$，即\n$\nk(x,x') = \\sigma_f^2 \\exp\\!\\left( - \\dfrac{(x-x')^2}{2 \\ell^2} \\right).\n$\n没有先验数据，因此所有方差和协方差都从先验计算。先验均值为零。\n\n测试套件和使用的参数：\n\n- 离散域对所有测试用例共享，为\n  $\n  \\mathcal{X} = \\{ x_0, x_1, x_2, x_3, x_4 \\} = \\{ 0.0, 0.25, 0.5, 0.75, 1.0 \\}.\n  $\n  在最终答案中对 $\\mathcal{X}$ 使用从零开始的索引。\n\n- 每个测试用例指定了核参数 $(\\sigma_f^2, \\ell)$、权重 $\\{w_i\\}_{i=1}^5$、开销 $b$、每次迭代时间 $\\{t(x_i)\\}_{i=1}^5$、噪声尺度参数 $\\{a(x_i)\\}_{i=1}^5$ 和最大链长 $L_{\\max}$。除非另有说明，$L_{\\min} = 1$。\n\n- 对于所有测试用例，加权积分方差缩减 $S(x,L)$ 的计算方式与上文完全相同，采集函数为 $A(x,L) = S(x,L)/\\mathrm{cost}(x,L)$。\n\n提供一个程序，解决以下四个测试用例：\n\n- 测试用例 1：\n  - $\\sigma_f^2 = 1.0$, $\\ell = 0.35$,\n  - $w = [1, 1, 1, 1, 1]$,\n  - $b = 10.0$,\n  - $t = [1.0, 1.0, 1.0, 1.0, 1.0]$,\n  - $a = [3.0, 2.0, 1.0, 2.0, 3.0]$,\n  - $L_{\\max} = 100000$.\n\n- 测试用例 2：\n  - $\\sigma_f^2 = 1.0$, $\\ell = 0.35$,\n  - $w = [1, 1, 1, 1, 1]$,\n  - $b = 100.0$,\n  - $t = [0.5, 0.5, 0.5, 0.5, 0.5]$,\n  - $a = [3.0, 2.0, 1.0, 2.0, 3.0]$,\n  - $L_{\\max} = 100000$.\n\n- 测试用例 3：\n  - $\\sigma_f^2 = 1.0, \\ell = 0.05$,\n  - $w = [1, 1, 1, 1, 1]$,\n  - $b = 10.0$,\n  - $t = [1.0, 1.0, 1.0, 1.0, 1.0]$,\n  - $a = [3.0, 2.0, 1.0, 2.0, 3.0]$,\n  - $L_{\\max} = 100000$.\n\n- 测试用例 4：\n  - $\\sigma_f^2 = 1.5$, $\\ell = 0.2$,\n  - $w = [1, 2, 1, 1, 2]$,\n  - $b = 30.0$,\n  - $t = [1.0, 2.0, 0.5, 3.0, 0.8]$,\n  - $a = [5.0, 1.0, 0.8, 2.5, 4.0]$,\n  - $L_{\\max} = 100000$.\n\n角度单位和物理单位不适用于此问题。所有计算都是无量纲的。您的程序必须为每个测试用例计算一个三元组，包括：\n- 所选的输入索引（从零开始的整数），\n- 所选的链长（正整数），\n- 相应的最大化采集值 $A(x,L)$，四舍五入到六位小数。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含结果，形式为一个逗号分隔的四个三元组列表，每个三元组本身是一个用方括号括起来的逗号分隔列表。例如：\n  $\n  [ [i_1, L_1, A_1], [i_2, L_2, A_2], [i_3, L_3, A_3], [i_4, L_4, A_4] ].\n  $\n- 在输出前将每个采集值四舍五入到六位小数。如果 $A(x,L)$ 出现平局，则应用上述的平局打破规则。\n\n对科学真实性和推导的要求：\n\n- 从高斯过程先验的基本定义以及根据积分自相关时间计算的 MCMC 平均值方差开始，如上所述。\n- 不要假设任何未从上述基础推导出的快捷公式。\n- 确保在提供的参数下，所有计算在数值上都是良定义的。\n\n您的任务是实现一个完整的、可运行的程序，执行所述计算并以指定的确切格式输出结果。该程序不得要求任何用户输入，也不得访问任何外部资源。", "solution": "用户想要解决一个单步贝叶斯实验设计问题。目标是从一个有限集 $\\mathcal{X}$ 中选择一个输入点 $x$ 和一个马尔可夫链蒙特卡洛（MCMC）链长 $L$，以最大化采集函数 $A(x, L)$，该函数定义为信息论增益与计算成本之比。\n\n问题验证如下：\n- **科学基础**：该问题在统计机器学习领域，特别是在使用高斯过程的贝叶斯优化和 MCMC 方法中，有充分的理论基础。GP 后验方差更新的公式和 MCMC 样本方差的近似是标准的。最大化单位成本的信息增益是实验设计中的一个典型问题。\n- **良定性**：所有参数和函数都有明确定义。优化是在一个有限的对 $(x, L)$ 集合上进行的，因为 $x \\in \\mathcal{X}$（有限）且 $L \\in \\{L_{\\min}, \\dots, L_{\\max}\\}$（有限）。$A(x, L)$ 的最大值保证存在。提供的平局打破规则确保了选择唯一的解。\n- **完整性和一致性**：问题陈述是自洽的，为每个测试用例提供了所有必要的定义、常数和参数。没有内部矛盾。\n\n该问题是有效的并且可以解决。\n\n需要最大化的采集函数是：\n$$\nA(x,L) = \\dfrac{S(x,L)}{\\mathrm{cost}(x,L)}\n$$\n我们分别分析分子 $S(x,L)$ 和分母 $\\mathrm{cost}(x,L)$。\n\n**1. 对各组成部分建模**\n\n在输入 $x$ 处运行长度为 $L$ 的 MCMC 链的成本由下式给出：\n$$\n\\mathrm{cost}(x,L) = b + t(x) \\, L\n$$\n其中 $b \\ge 0$ 是固定开销，$t(x)  0$ 是每次迭代的时间。\n\n加权积分方差缩减 $S(x,L)$ 为：\n$$\nS(x,L) = \\sum_{i=1}^N w_i \\, \\dfrac{k(x_i,x)^2}{\\mathrm{Var}_{\\text{prior}}[f(x)] + v_n(x,L)}\n$$\n由于没有先验数据且 GP 先验均值为零，任意点 $x'$ 的先验方差由该点的核函数值给出：$\\mathrm{Var}_{\\text{prior}}[f(x')] = k(x',x')$。指定的核函数是平方指数核，$k(x,x') = \\sigma_f^2 \\exp\\left( - \\frac{(x-x')^2}{2 \\ell^2} \\right)$，这意味着对于任何 $x \\in \\mathcal{X}$，先验方差是均匀的：$\\mathrm{Var}_{\\text{prior}}[f(x)] = k(x,x) = \\sigma_f^2$。\n\n观测噪声方差为 $v_n(x,L) = a(x)/L$。将这些代入 $S(x,L)$ 的表达式中：\n$$\nS(x,L) = \\sum_{i=1}^N w_i \\, \\dfrac{k(x_i,x)^2}{\\sigma_f^2 + \\frac{a(x)}{L}}\n$$\n分母不依赖于求和索引 $i$。我们可以将其从求和中提取出来：\n$$\nS(x,L) = \\frac{1}{\\sigma_f^2 + \\frac{a(x)}{L}} \\left( \\sum_{i=1}^N w_i k(x_i,x)^2 \\right)\n$$\n对于固定的评估点 $x$，总和 $\\sum_{i=1}^N w_i k(x_i,x)^2$ 是一个常数。我们将其定义为常数 $C(x)$：\n$$\nC(x) \\equiv \\sum_{i=1}^N w_i k(x_i,x)^2\n$$\n$C(x)$ 可以为每个 $x \\in \\mathcal{X}$ 预先计算。$S(x,L)$ 的表达式简化为：\n$$\nS(x,L) = \\frac{C(x)}{\\sigma_f^2 + \\frac{a(x)}{L}}\n$$\n现在，组合成完整的采集函数 $A(x,L)$：\n$$\nA(x,L) = \\frac{\\frac{C(x)}{\\sigma_f^2 + \\frac{a(x)}{L}}}{b + t(x)L} = \\frac{C(x)}{\\left(\\sigma_f^2 + \\frac{a(x)}{L}\\right) (b + t(x)L)}\n$$\n展开分母得到：\n$$\nA(x,L) = \\frac{C(x)}{b\\sigma_f^2 + \\sigma_f^2 t(x) L + \\frac{b a(x)}{L} + a(x)t(x)}\n$$\n\n**2. 优化链长 $L$**\n\n对于一个固定的候选点 $x$，$C(x)$ 是一个正常数。为了最大化 $A(x,L)$，我们必须使其分母相对于 $L$ 最小化。设 $D_x(L)$ 为分母：\n$$\nD_x(L) = b\\sigma_f^2 + \\sigma_f^2 t(x) L + \\frac{b a(x)}{L} + a(x)t(x)\n$$\n我们想要找到 $L \\in \\{L_{\\min}, \\dots, L_{\\max}\\}$ 来最小化 $D_x(L)$。常数项 $b\\sigma_f^2$ 和 $a(x)t(x)$ 不影响最小值的位置。分母中依赖于 $L$ 的部分是：\n$$\ng_x(L) = (\\sigma_f^2 t(x)) L + \\frac{b a(x)}{L}\n$$\n该函数的形式为 $K_1 L + K_2/L$，其中 $K_1 = \\sigma_f^2 t(x)  0$ 且 $K_2 = b a(x) \\ge 0$。对于 $L  0$，这是一个凸函数。我们可以通过将 $L$ 视为连续变量并将其导数设为零来找到其最小值：\n$$\n\\frac{dg_x}{dL} = K_1 - \\frac{K_2}{L^2} = 0 \\implies L^2 = \\frac{K_2}{K_1}\n$$\n因此，$L$ 的最优连续值为：\n$$\nL^*_{\\text{real}}(x) = \\sqrt{\\frac{K_2}{K_1}} = \\sqrt{\\frac{b \\, a(x)}{\\sigma_f^2 \\, t(x)}}\n$$\n由于 $L$ 必须是整数，且 $g_x(L)$ 是一个凸函数，整数最小值必须位于包围连续最小值 $L^*_{\\text{real}}(x)$ 的两个整数之一。也就是说，最优整数 $L$ 必须是 $\\lfloor L^*_{\\text{real}}(x) \\rfloor$ 或 $\\lceil L^*_{\\text{real}}(x) \\rceil$ 中的一个，并受约束 $L_{\\min} \\le L \\le L_{\\max}$ 的限制。\n\n**3. 求解策略**\n\n这种分析性的洞见使我们能够避免对高达 $L_{\\max}$ 的所有可能的 $L$ 值进行代价高昂的暴力搜索。总体算法如下：\n\n对于每个测试用例：\n1. 初始化最大采集值 `max_A` 为一个负数，并存储相应的最优对 $(x^*, L^*)$。\n2. 对每个候选点 $x_j \\in \\mathcal{X}$（$j=0, \\dots, N-1$）进行迭代。\n3. 对于每个 $x_j$：\n    a. 预计算常数 $C(x_j) = \\sum_{i=1}^N w_i k(x_i,x_j)^2$。\n    b. 计算最优连续链长 $L^*_{\\text{real}}(x_j) = \\sqrt{\\frac{b \\cdot a(x_j)}{\\sigma_f^2 \\cdot t(x_j)}}$。\n    c. 确定 $L$ 的整数候选集。这些候选值为 $\\lfloor L^*_{\\text{real}}(x_j) \\rfloor$ 和 $\\lceil L^*_{\\text{real}}(x_j) \\rceil$，并将其限制在区间 $[L_{\\min}, L_{\\max}]$ 内。这样，对于每个 $x_j$，最多只需要测试两个不同的整数值。\n    d. 对于每个候选整数 $L$：\n        i. 计算采集值 $A(x_j, L)$。\n        ii. 将此值与当前最大值 `max_A` 进行比较。如果 $A(x_j, L)$ 严格大于 `max_A`，则更新 `max_A` 并将 $(x_j, L)$ 存储为新的最佳对。\n4. 循环的结构是按递增顺序迭代 $x$ 索引，并对每个 $x$ 按递增顺序检查候选 $L$ 值。这与严格不等式 `A  max_A` 相结合，正确地实现了指定的平局打破规则（最小的 $x$ 索引，然后是最小的 $L$）。\n5. 在检查完 $\\mathcal{X}$ 中的所有点后，存储的对 $(x^*, L^*)$ 和相应的 `max_A` 即为该测试用例的解。最终的采集值四舍五入到六位小数。\n\n对所有四个测试用例重复此过程。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a set of Bayesian experimental design problems to find the optimal\n    input point and MCMC chain length that maximize a cost-sensitive acquisition function.\n    \"\"\"\n    # Define the shared discrete domain for all test cases.\n    X_domain = np.array([0.0, 0.25, 0.5, 0.75, 1.0])\n    N = len(X_domain)\n\n    test_cases = [\n        # Test case 1\n        {\n            \"sf2\": 1.0, \"ell\": 0.35, \"w\": np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"b\": 10.0, \"t\": np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"a\": np.array([3.0, 2.0, 1.0, 2.0, 3.0]), \"L_max\": 100000\n        },\n        # Test case 2\n        {\n            \"sf2\": 1.0, \"ell\": 0.35, \"w\": np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"b\": 100.0, \"t\": np.array([0.5, 0.5, 0.5, 0.5, 0.5]),\n            \"a\": np.array([3.0, 2.0, 1.0, 2.0, 3.0]), \"L_max\": 100000\n        },\n        # Test case 3\n        {\n            \"sf2\": 1.0, \"ell\": 0.05, \"w\": np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"b\": 10.0, \"t\": np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"a\": np.array([3.0, 2.0, 1.0, 2.0, 3.0]), \"L_max\": 100000\n        },\n        # Test case 4\n        {\n            \"sf2\": 1.5, \"ell\": 0.2, \"w\": np.array([1.0, 2.0, 1.0, 1.0, 2.0]),\n            \"b\": 30.0, \"t\": np.array([1.0, 2.0, 0.5, 3.0, 0.8]),\n            \"a\": np.array([5.0, 1.0, 0.8, 2.5, 4.0]), \"L_max\": 100000\n        }\n    ]\n\n    def kernel(x1, x2, sf2, ell):\n        \"\"\"Computes the squared exponential kernel value.\"\"\"\n        return sf2 * np.exp(-0.5 * ((x1 - x2) ** 2) / (ell ** 2))\n\n    all_results = []\n    \n    for case in test_cases:\n        sf2, ell, w = case[\"sf2\"], case[\"ell\"], case[\"w\"]\n        b, t, a = case[\"b\"], case[\"t\"], case[\"a\"]\n        L_max, L_min = case[\"L_max\"], 1\n\n        best_x_idx, best_L = -1, -1\n        max_A = -1.0\n\n        # Pre-compute the matrix of kernel values between all points in the domain.\n        K = np.zeros((N, N))\n        for i in range(N):\n            for j in range(N):\n                K[i, j] = kernel(X_domain[i], X_domain[j], sf2, ell)\n\n        # Iterate through each candidate point x_j to evaluate.\n        for j in range(N):\n            t_j, a_j = t[j], a[j]\n            \n            # Calculate C(x_j) = sum_i w_i * k(x_i, x_j)^2, a constant for fixed j.\n            C_j = np.sum(w * (K[:, j] ** 2))\n\n            L_candidates = []\n            if b == 0:\n                # If overhead cost is zero, A is maximized at the minimum chain length.\n                L_candidates = [L_min]\n            # Constraints from the problem statement t(x)  0 and sf2 given as  0\n            # ensure the denominator is positive.\n            else:\n                L_real = np.sqrt((b * a_j) / (sf2 * t_j))\n                l_cand_1 = int(np.floor(L_real))\n                l_cand_2 = int(np.ceil(L_real))\n                \n                # Clamp candidates to the allowed range [L_min, L_max].\n                l1 = max(L_min, min(L_max, l_cand_1))\n                l2 = max(L_min, min(L_max, l_cand_2))\n                \n                L_candidates = sorted(list(set([l1, l2])))\n\n            # Evaluate acquisition function for the candidate L values.\n            for L in L_candidates:\n                if L == 0: continue\n                cost_val = b + t_j * L\n                if cost_val == 0: continue\n\n                # A(x,L) = S(x,L) / cost(x,L)\n                # S(x,L) = C(x) / (sf2 + a(x)/L)\n                S_val = C_j / (sf2 + a_j/L)\n                A_val = S_val / cost_val\n\n                # The loop order (j asc, then L asc) and strict inequality\n                # correctly implement the specified tie-breaking rule.\n                if A_val  max_A:\n                    max_A = A_val\n                    best_x_idx = j\n                    best_L = L\n        \n        all_results.append([best_x_idx, best_L, max_A])\n\n    # Format the final output string as per problem specification.\n    result_str_parts = [f\"[{res[0]},{res[1]},{res[2]:.6f}]\" for res in all_results]\n    final_output = f\"[{','.join(result_str_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3291533"}]}