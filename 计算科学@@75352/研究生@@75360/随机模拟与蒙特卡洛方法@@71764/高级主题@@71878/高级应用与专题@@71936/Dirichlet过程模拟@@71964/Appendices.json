{"hands_on_practices": [{"introduction": "狄利克雷过程 (DP) 的一个核心特性是其预测分布可以通过波利亚瓮 (Pólya urn) 机制来直观理解。本练习将通过一个具体的贝塔-伯努利模型，让你亲手实现这一预测过程。通过将序贯抽样结果与精确的贝塔-二项后验预测分布进行比较，你将深入理解DP的“强者愈强”特性，并验证其预测规则的正确性。[@problem_id:3340246]", "problem": "考虑一个在二元样本空间上的狄利克雷过程 (DP) 先验。设 $G \\sim \\mathrm{DP}(\\alpha, H)$，其中 $H$ 是在 $\\{0,1\\}$ 上的一个概率测度，对于给定的 $a>0$ 和 $b>0$，满足 $H(\\{1\\}) = \\frac{a}{a+b}$ 和 $H(\\{0\\}) = \\frac{b}{a+b}$。在给定 $G$ 的条件下，设 $Y_1, \\dots, Y_n \\stackrel{\\mathrm{iid}}{\\sim} G$ 是观测到的二元结果。要求您使用第一性原理推导并验证该模型下 $m$ 次新试验的后验预测分布，并实现一个程序，通过精确概率计算和蒙特卡洛模拟在数值上检验该理论。\n\n基本原理：\n- 狄利克雷过程 (DP) 的定义属性是，对于空间的任意有限可测划分 $(A_1,\\dots,A_K)$，有 $(G(A_1),\\dots,G(A_K)) \\sim \\mathrm{Dirichlet}(\\alpha H(A_1), \\dots, \\alpha H(A_K))$。\n- Blackwell–MacQueen Pólya 缸 (PU) 模型指出，在狄利克雷过程先验下，对于下一个观测值 $Y_{n+1}$ 的预测分布，对任意可测集 $A$ 给出如下公式\n$$\n\\mathbb{P}(Y_{n+1} \\in A \\mid Y_{1:n}) \\;=\\; \\frac{\\alpha}{\\alpha + n} H(A) \\;+\\; \\frac{1}{\\alpha + n} \\sum_{i=1}^n \\mathbf{1}\\{Y_i \\in A\\}.\n$$\n- 对于二元空间 $\\{0,1\\}$，上述公式简化为\n$$\n\\mathbb{P}(Y_{n+1} = 1 \\mid Y_{1:n}) \\;=\\; \\frac{\\alpha \\frac{a}{a+b} + s}{\\alpha + n}, \\qquad\n\\mathbb{P}(Y_{n+1} = 0 \\mid Y_{1:n}) \\;=\\; \\frac{\\alpha \\frac{b}{a+b} + f}{\\alpha + n},\n$$\n其中 $s = \\sum_{i=1}^n Y_i$ 且 $f = n - s$。\n- $m$ 次新试验中 1 的计数，$K = \\sum_{j=1}^m Y_{n+j}$，服从一个特化为两类的狄利克雷-多项分布，即一个贝塔-二项分布，其形状参数为 $\\alpha_1 = \\alpha \\frac{a}{a+b} + s$ 和 $\\alpha_0 = \\alpha \\frac{b}{a+b} + f$：\n$$\n\\mathbb{P}(K = k \\mid Y_{1:n}) \\;=\\; \\binom{m}{k} \\frac{B(\\alpha_1 + k, \\alpha_0 + m - k)}{B(\\alpha_1, \\alpha_0)}, \\qquad k = 0,1,\\dots,m,\n$$\n其中 $B(\\cdot,\\cdot)$ 是贝塔函数。\n\n您的任务：\n1. 基于上述基本事实，根据 Pólya 缸 (PU) 更新规则，推导任意一个特定有序长度为 $m$ 的序列 $(y_{n+1},\\dots,y_{n+m}) \\in \\{0,1\\}^m$ 的概率的显式表达式。证明此概率仅取决于序列中 1 的数量 $k$，并且等于 $k$ 的贝塔-二项概率质量除以 $\\binom{m}{k}$。\n2. 推导在给定 $Y_{1:n}$ 和参数 $(\\alpha,a,b)$ 的条件下，$K = \\sum_{j=1}^m Y_{n+j}$ 的贝塔-二项预测概率质量函数，并用 $\\alpha, a, b, s, f$ 明确表示其形状参数。\n\n实现要求：\n- 实现一个程序，对下面的每个测试用例执行以下两种验证：\n  - 精确验证：使用 PU 乘积公式计算给定长度为 $m$ 的有序序列的概率，并验证其与闭式解值 $\\binom{m}{k}^{-1} \\binom{m}{k} \\frac{B(\\alpha_1 + k, \\alpha_0 + m - k)}{B(\\alpha_1, \\alpha_0)} = \\frac{B(\\alpha_1 + k, \\alpha_0 + m - k)}{B(\\alpha_1, \\alpha_0)}$ 在 $10^{-12}$ 的绝对容差内匹配。此处 $k$ 是序列中 1 的数量，$\\alpha_1 = \\alpha \\frac{a}{a+b} + s$，以及 $\\alpha_0 = \\alpha \\frac{b}{a+b} + f$，其中 $s = \\sum_{i=1}^n Y_i$ 且 $f = n - s$。对于 $m = 0$，将空乘积视为 $1$，并声明精确验证已满足。\n  - 蒙特卡洛验证：使用固定的随机种子，通过 PU 模型模拟 $R$ 个 $m$ 次新试验的后验预测重复样本，计算 $K$ 的经验分布，并将其与贝塔-二项概率质量函数进行比较。如果 $k \\in \\{0,\\dots,m\\}$ 上的最大绝对偏差最多为 $\\varepsilon = 0.01$，则声明蒙特卡洛检查通过。对于 $m=0$，声明此检查已满足。\n\n测试套件：\n- 用例 1：$\\alpha = 3.7$, $a = 2.3$, $b = 1.1$, $n = 8$, $Y_{1:n} = [1,0,1,1,0,0,1,0]$, $m = 6$, 要测试的有序序列 $[1,0,1,0,1,0]$。\n- 用例 2：$\\alpha = 0.5$, $a = 0.7$, $b = 0.9$, $n = 5$, $Y_{1:n} = [0,0,1,0,1]$, $m = 5$, 要测试的有序序列 $[1,1,1,0,0]$。\n- 用例 3：$\\alpha = 4.2$, $a = 5.0$, $b = 2.0$, $n = 7$, $Y_{1:n} = [1,1,0,1,1,1,1]$, $m = 0$, 要测试的有序序列 $[]$ (空)。\n- 用例 4：$\\alpha = 2.0$, $a = 1.0$, $b = 1.0$, $n = 10$, $Y_{1:n} = [1,1,1,1,1,1,1,1,1,1]$, $m = 4$, 要测试的有序序列 $[1,1,0,1]$。\n\n蒙特卡洛详情：\n- 对于每个 $m \\ge 1$ 的测试用例，使用恰好 $R = 200{,}000$ 个后验预测重复样本，并使用固定的种子以确保结果可复现。\n- 此问题不涉及任何物理单位。\n\n最终输出规格：\n- 对于每个测试用例，输出一个布尔值，当且仅当精确验证和蒙特卡洛验证都通过时，该值为真。\n- 您的程序应生成一行输出，其中包含四个用例的四个布尔值，以逗号分隔并用方括号括起，例如 $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True}]$。", "solution": "问题陈述经验证在科学上是合理的、适定的且内部一致的，符合贝叶斯非参数化的既定原则。我们着手解决该问题，内容包括所需的理论推导，以及随后的用于数值验证的算法设计。\n\n### 理论推导\n\n该问题设置在二元样本空间 $\\{0,1\\}$ 上的狄利克雷过程 (DP) 先验的背景下。我们给定一个基础测度 $H$，其定义为 $H(\\{1\\}) = \\frac{a}{a+b}$ 和 $H(\\{0\\}) = \\frac{b}{a+b}$，其中 $a, b > 0$。DP 的集中参数为 $\\alpha > 0$。我们已经观测到数据 $Y_{1:n} = (Y_1, \\dots, Y_n)$。后续观测的预测分布由 Blackwell-MacQueen Pólya 缸 (PU) 模型决定。设 $s = \\sum_{i=1}^n Y_i$ 为初始数据中 1 的数量（成功次数），$f = n-s$ 为 0 的数量（失败次数）。\n\n下一个观测值 $Y_{n+1}$ 的预测概率由下式给出：\n$$\n\\mathbb{P}(Y_{n+1} = 1 \\mid Y_{1:n}) = \\frac{\\alpha H(\\{1\\}) + s}{\\alpha + n} = \\frac{\\alpha \\frac{a}{a+b} + s}{\\alpha + n}\n$$\n$$\n\\mathbb{P}(Y_{n+1} = 0 \\mid Y_{1:n}) = \\frac{\\alpha H(\\{0\\}) + f}{\\alpha + n} = \\frac{\\alpha \\frac{b}{a+b} + f}{\\alpha + n}\n$$\n让我们定义观测到 $Y_{1:n}$ 后的后验超参数：\n$\\alpha_1 = \\alpha H(\\{1\\}) + s = \\alpha \\frac{a}{a+b} + s$\n$\\alpha_0 = \\alpha H(\\{0\\}) + f = \\alpha \\frac{b}{a+b} + f$\n注意 $\\alpha_1 + \\alpha_0 = \\alpha(H(\\{1\\}) + H(\\{0\\})) + (s+f) = \\alpha(1) + n = \\alpha + n$。\n因此，我们可以将预测概率写为：\n$\\mathbb{P}(Y_{n+1} = 1 \\mid Y_{1:n}) = \\frac{\\alpha_1}{\\alpha_1 + \\alpha_0}$ 和 $\\mathbb{P}(Y_{n+1} = 0 \\mid Y_{1:n}) = \\frac{\\alpha_0}{\\alpha_1 + \\alpha_0}$。\n\n**1. 特定有序序列的概率**\n\n我们希望推导未来特定结果序列 $\\mathbf{y}' = (y_{n+1}, \\dots, y_{n+m}) \\in \\{0,1\\}^m$ 的概率。使用概率的链式法则和 PU 模型：\n$$\n\\mathbb{P}(\\mathbf{Y}_{n+1:n+m} = \\mathbf{y}' \\mid Y_{1:n}) = \\prod_{j=1}^{m} \\mathbb{P}(Y_{n+j} = y_{n+j} \\mid Y_{1:n+j-1})\n$$\n设 $s_{j-1} = s + \\sum_{l=1}^{j-1} y_{n+l}$ 和 $f_{j-1} = f + (j-1) - \\sum_{l=1}^{j-1} y_{n+l}$ 分别为 $j-1$ 次新试验后 1 和 0 的总计数。第 $j$ 次试验 $Y_{n+j}$ 的概率是：\n$$\n\\mathbb{P}(Y_{n+j} = y_{n+j} \\mid Y_{1:n+j-1}) = \\frac{\\alpha H(\\{y_{n+j}\\}) + \\text{count of } y_{n+j} \\text{ in } Y_{1:n+j-1}}{\\alpha + n + j - 1}\n$$\n完整乘积的分母是：\n$$\n\\prod_{j=1}^{m} (\\alpha + n + j - 1) = (\\alpha+n)(\\alpha+n+1)\\dots(\\alpha+n+m-1) = \\frac{\\Gamma(\\alpha+n+m)}{\\Gamma(\\alpha+n)} = \\frac{\\Gamma(\\alpha_1+\\alpha_0+m)}{\\Gamma(\\alpha_1+\\alpha_0)}\n$$\n现在，考虑分子。设序列 $\\mathbf{y}'$ 包含 $k$ 个 1 和 $m-k$ 个 0。\n对应 $k$ 个 1 的分子项将是 $(\\alpha_1), (\\alpha_1+1), \\dots, (\\alpha_1+k-1)$。序列中出现的第 $i$ 个 1 的项是 $(\\alpha_1 + i-1)$，无论其位置如何。例如，如果第一个 1 在位置 $j$，它之前的 1 的数量是 0，所以该项是 $\\alpha H(\\{1\\}) + s = \\alpha_1$。如果第二个 1 在位置 $j'$，它之前的 1 的数量是 1，所以该项是 $\\alpha H(\\{1\\}) + s+1 = \\alpha_1+1$。这意味着与 1 相关联的分子项的乘积对其排序是不变的。\n1 的乘积： $(\\alpha_1)(\\alpha_1+1)\\dots(\\alpha_1+k-1) = \\frac{\\Gamma(\\alpha_1+k)}{\\Gamma(\\alpha_1)}$。\n类似地，与 0 相关联的分子项的乘积是：\n0 的乘积： $(\\alpha_0)(\\alpha_0+1)\\dots(\\alpha_0+m-k-1) = \\frac{\\Gamma(\\alpha_0+m-k)}{\\Gamma(\\alpha_0)}$。\n\n结合这些部分，任何具有 $k$ 个 1 和 $m-k$ 个 0 的特定序列 $\\mathbf{y}'$ 的概率是：\n$$\n\\mathbb{P}(\\mathbf{y}') = \\frac{\\frac{\\Gamma(\\alpha_1+k)}{\\Gamma(\\alpha_1)} \\frac{\\Gamma(\\alpha_0+m-k)}{\\Gamma(\\alpha_0)}}{\\frac{\\Gamma(\\alpha_1+\\alpha_0+m)}{\\Gamma(\\alpha_1+\\alpha_0)}}\n= \\frac{\\Gamma(\\alpha_1+k) \\Gamma(\\alpha_0+m-k)}{\\Gamma(\\alpha_1+\\alpha_0+m)} \\times \\frac{\\Gamma(\\alpha_1+\\alpha_0)}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_0)}\n$$\n使用贝塔函数的定义 $B(x,y) = \\frac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x+y)}$，我们得到最终表达式：\n$$\n\\mathbb{P}(\\mathbf{Y}_{n+1:n+m} = \\mathbf{y}' \\mid Y_{1:n}) = \\frac{B(\\alpha_1+k, \\alpha_0+m-k)}{B(\\alpha_1, \\alpha_0)}\n$$\n这表明概率仅取决于序列中 1 ($k$) 和 0 ($m-k$) 的数量，而与它们的顺序无关。这个性质被称为可交换性。\n\n**2. 贝塔-二项 PMF 的推导**\n设 $K = \\sum_{j=1}^m Y_{n+j}$ 为 $m$ 次新试验中 1 的总数的随机变量。我们想求其概率质量函数 (PMF)，$\\mathbb{P}(K=k \\mid Y_{1:n})$。\n事件 $\\{K=k\\}$ 是所有长度为 $m$ 且恰好包含 $k$ 个 1 的可能序列的不交并集。这种不同序列的数量由二项式系数 $\\binom{m}{k}$ 给出。\n如前述推导所示，这些序列中的每一个都有相同的概率：\n$$\nP(\\text{any single sequence with } k \\text{ ones}) = \\frac{B(\\alpha_1+k, \\alpha_0+m-k)}{B(\\alpha_1, \\alpha_0)}\n$$\n因此，观测到 $k$ 个 1 的总概率是这些不交事件的概率之和：\n$$\n\\mathbb{P}(K=k \\mid Y_{1:n}) = \\sum_{\\text{sequences with k ones}} P(\\text{sequence}) = \\binom{m}{k} \\frac{B(\\alpha_1+k, \\alpha_0+m-k)}{B(\\alpha_1, \\alpha_0)}\n$$\n这是贝塔-二项分布的 PMF，通常表示为 $K \\sim \\text{Beta-Binomial}(m, \\alpha_1, \\alpha_0)$。参数是试验次数 $m$ 和形状参数 $\\alpha_1 = \\alpha \\frac{a}{a+b} + s$ 及 $\\alpha_0 = \\alpha \\frac{b}{a+b} + f$。\n\n### 算法设计与验证\n\n该实现将为每个测试用例验证这些理论结果。\n\n**1. 精确验证**\n此检查确认特定序列的 Pólya 缸概率的序列乘积与从可交换性推导出的优雅闭式表达式相匹配。\n- **PU 乘积计算**：我们通过从 $j=1$ 到 $m$ 迭代来直接实现链式法则。在每一步 $j$ 中，我们根据初始计数 $s, f$ 和到目前为止生成的 $j-1$ 个结果来计算结果 $y_{n+j}$ 的预测概率。将这些概率相乘以获得序列的总概率 $P_{\\text{PU}}$。\n$$ P_{\\text{PU}} = \\prod_{j=1}^m \\frac{\\alpha H(\\{y_{n+j}\\}) + (\\text{initial count of } y_{n+j}) + (\\text{count of } y_{n+j} \\text{ in } y_{n+1:n+j-1})}{\\alpha + n + j-1} $$\n- **闭式解计算**：我们使用推导出的公式计算概率 $P_{\\text{Beta}}$：\n$$ P_{\\text{Beta}} = \\frac{B(\\alpha_1+k, \\alpha_0+m-k)}{B(\\alpha_1, \\alpha_0)} $$\n其中 $k$ 是测试序列中 1 的数量。为了确保数值稳定性，特别是对于小概率，计算在对数尺度上使用 `betaln` 函数（对数贝塔函数）执行，即 $\\log(P_{\\text{Beta}}) = \\text{betaln}(\\alpha_1+k, \\alpha_0+m-k) - \\text{betaln}(\\alpha_1, \\alpha_0)$。然后对结果取指数。\n- **比较**：如果 $|P_{\\text{PU}} - P_{\\text{Beta}}| \\le 10^{-12}$，则检查通过。对于 $m=0$，空乘积为 $1$，贝塔函数比率也为 $1$，因此根据定义，该检查已满足。\n\n**2. 蒙特卡洛验证**\n此检查确认从 Pólya 缸模型进行模拟能够经验性地再现成功次数 $K$ 的理论贝塔-二项预测分布。\n- **模拟**：对于 $R = 200,000$ 个重复样本，我们通过顺序应用 PU 规则来模拟长度为 $m$ 的序列。在每一步 $j=1,\\dots,m$，我们从参数为 $p_j = \\mathbb{P}(Y_{n+j}=1 \\mid Y_{1:n+j-1})$ 的伯努利分布中抽取 $Y_{n+j}$。对于每个模拟序列，我们计算 1 的总数 $K$。使用固定的随机种子以保证可复现性。\n- **经验 PMF**：我们为 $R$ 个模拟的 $K$ 值构建一个直方图。然后经验 PMF 为 $\\hat{P}(K=k) = (K=k \\text{ 的计数}) / R$，其中 $k=0, \\dots, m$。\n- **理论 PMF**：我们使用推导出的 PMF 公式计算 $k=0, \\dots, m$ 的精确贝塔-二项概率 $\\mathbb{P}(K=k)$。为保证稳定性，使用涉及 `gammaln` 和 `betaln` 的对数尺度计算。\n- **比较**：如果经验 PMF 和理论 PMF 之间的最大绝对偏差低于容差 $\\varepsilon = 0.01$，即 $\\max_{k \\in \\{0,\\dots,m\\}} |\\hat{P}(K=k) - \\mathbb{P}(K=k)| \\le 0.01$，则检查通过。对于 $m=0$，此检查根据定义已满足。\n\n当且仅当精确验证和蒙特卡洛验证都成功时，一个测试用例才被视为通过。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import betaln, gammaln\n\ndef solve():\n    \"\"\"\n    Solves the problem by running through the test suite, performing\n    exact and Monte Carlo verifications for each case.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"alpha\": 3.7, \"a\": 2.3, \"b\": 1.1, \"n\": 8, \n            \"Y_1_to_n\": [1, 0, 1, 1, 0, 0, 1, 0], \"m\": 6, \n            \"test_sequence\": [1, 0, 1, 0, 1, 0]\n        },\n        {\n            \"alpha\": 0.5, \"a\": 0.7, \"b\": 0.9, \"n\": 5, \n            \"Y_1_to_n\": [0, 0, 1, 0, 1], \"m\": 5, \n            \"test_sequence\": [1, 1, 1, 0, 0]\n        },\n        {\n            \"alpha\": 4.2, \"a\": 5.0, \"b\": 2.0, \"n\": 7, \n            \"Y_1_to_n\": [1, 1, 0, 1, 1, 1, 1], \"m\": 0, \n            \"test_sequence\": []\n        },\n        {\n            \"alpha\": 2.0, \"a\": 1.0, \"b\": 1.0, \"n\": 10, \n            \"Y_1_to_n\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"m\": 4, \n            \"test_sequence\": [1, 1, 0, 1]\n        },\n    ]\n\n    results = []\n    \n    # Use a fixed seed for the entire process for reproducibility\n    np.random.seed(42)\n\n    for case in test_cases:\n        alpha, a, b, n = case[\"alpha\"], case[\"a\"], case[\"b\"], case[\"n\"]\n        Y_1_to_n, m, test_sequence = case[\"Y_1_to_n\"], case[\"m\"], case[\"test_sequence\"]\n        \n        # Per problem spec, for m=0, both checks pass automatically.\n        if m == 0:\n            results.append(True)\n            continue\n            \n        # Common initial calculations\n        s = sum(Y_1_to_n)\n        f = n - s\n        H1 = a / (a + b)\n        H0 = 1.0 - H1\n        alpha_1 = alpha * H1 + s\n        alpha_0 = alpha * H0 + f\n\n        # --- 1. Exact Verification ---\n        exact_check_passed = False\n        \n        # 1a. PU product rule probability\n        prob_pu = 1.0\n        s_current, f_current = s, f\n        for i in range(m):\n            y_next = test_sequence[i]\n            denominator = alpha + n + i\n            if y_next == 1:\n                numerator = alpha * H1 + s_current\n                prob_pu *= numerator / denominator\n                s_current += 1\n            else: # y_next == 0\n                numerator = alpha * H0 + f_current\n                prob_pu *= numerator / denominator\n                f_current += 1\n        \n        # 1b. Closed-form Beta function ratio probability\n        k = sum(test_sequence)\n        log_prob_beta = betaln(alpha_1 + k, alpha_0 + m - k) - betaln(alpha_1, alpha_0)\n        prob_beta = np.exp(log_prob_beta)\n        \n        # 1c. Comparison\n        if np.isclose(prob_pu, prob_beta, atol=1e-12, rtol=0):\n            exact_check_passed = True\n\n        # --- 2. Monte Carlo Verification ---\n        mc_check_passed = False\n        R = 200000\n        \n        # 2a. Simulation\n        k_counts = np.zeros(m + 1, dtype=np.int64)\n        for _ in range(R):\n            s_sim, f_sim = s, f\n            k_sim = 0\n            for i in range(m):\n                p1_sim = (alpha * H1 + s_sim) / (alpha + n + i)\n                draw = np.random.binomial(1, p1_sim)\n                if draw == 1:\n                    s_sim += 1\n                    k_sim += 1\n                else: # draw == 0\n                    f_sim += 1\n            k_counts[k_sim] += 1\n            \n        # 2b. Empirical PMF\n        empirical_pmf = k_counts / R\n        \n        # 2c. Theoretical PMF (Beta-Binomial)\n        theoretical_pmf = np.zeros(m + 1)\n        log_beta_alpha_initial = betaln(alpha_1, alpha_0)\n        for k_val in range(m + 1):\n            log_comb_mk = gammaln(m + 1) - gammaln(k_val + 1) - gammaln(m - k_val + 1)\n            log_beta_ratio = betaln(alpha_1 + k_val, alpha_0 + m - k_val) - log_beta_alpha_initial\n            theoretical_pmf[k_val] = np.exp(log_comb_mk + log_beta_ratio)\n            \n        # 2d. Comparison\n        max_abs_dev = np.max(np.abs(empirical_pmf - theoretical_pmf))\n        if max_abs_dev = 0.01:\n            mc_check_passed = True\n\n        # Final decision for the case\n        results.append(exact_check_passed and mc_check_passed)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3340246"}, {"introduction": "折棒构造 (stick-breaking construction) 是理解和模拟狄利克雷过程的另一种基本视角。本练习聚焦于折棒过程中的一个关键量——剩余质量 (residual mass)，即在有限次折断后剩余的“木棒”长度。你将推导其精确的概率分布，并通过模拟验证一个用于量化其尾部概率的指数界，从而深入理解该构造的收敛性质及其在有限近似中的应用。[@problem_id:3340212]", "problem": "考虑集中参数为 $\\alpha0$ 的狄利克雷过程的 Griffiths–Engen–McCloskey (GEM) 折棍构造。令 $\\{V_j\\}_{j\\ge 1}$ 为独立同分布的随机变量，其中 $V_j\\sim \\mathrm{Beta}(1,\\alpha)$，并为 $j\\ge 1$ 定义折棍权重 $P_j = V_j \\prod_{i=1}^{j-1} (1 - V_i)$。对于一个固定的正整数 $K$，定义经过 $K$ 次折断后的剩余质量为 $R_K=\\prod_{j=1}^{K}(1 - V_j)$。您的任务是刻画 $R_K$ 的分布，并通过推导和模拟两种方法验证事件 $R_K\\varepsilon$ 的指数尾部界。\n\n仅从基本的分布事实和变量替换法则出发，按以下步骤进行：\n\n1) 推导 $-\\log R_K$ 的分布。您的推导必须仅依赖于：\n- GEM 折棍构造的定义，$V_j\\sim \\mathrm{Beta}(1,\\alpha)$ 独立同分布，以及 $R_K=\\prod_{j=1}^{K}(1 - V_j)$。\n- 标准的变量变换和独立性性质。\n- 独立同分布随机变量之和的矩生成函数的经过充分检验的性质。\n\n2) 使用 Chernoff–Markov 界，为一个给定的 $\\varepsilon\\in(0,1)$ 推导 $\\mathbb{P}(R_K\\varepsilon)$ 的一个可计算的指数界。通过优化 Chernoff 参数，将该界表示为 $\\alpha$、$K$ 和 $\\varepsilon$ 的显式函数。清晰地说明非平凡界适用的参数范围，以及在其他情况下平凡上界是什么。\n\n3) 设计一个模拟算法来估计 $\\mathbb{P}(R_K\\varepsilon)$ 并经验性地刻画 $-\\log R_K$ 的分布。您的模拟必须使用与 GEM 折棍构造一致的独立抽样。您的实现可以等价地模拟一个由您在第 (1) 部分的推导所蕴含的精确分布变换，只要该等价性得到了推导的证明。为保证可复现性，请使用固定的伪随机种子。对于每组参数集，估计：\n- 通过 $N$ 次独立重复的蒙特卡洛方法估计经验概率 $\\widehat{p}=\\mathbb{P}(R_K\\varepsilon)$。\n- $-\\log R_K$ 的经验均值和方差。\n\n4) 计算您在第 (1) 部分的推导所蕴含的精确值 $p_{\\mathrm{exact}}=\\mathbb{P}(R_K\\varepsilon)$，并评估您在第 (2) 部分得到的指数尾部界 $p_{\\mathrm{bound}}$。在不同的参数设置下，数值上验证：\n- 蒙特卡洛估计值 $\\widehat{p}$ 与精确值 $p_{\\mathrm{exact}}$ 足够接近，即满足 $|\\widehat{p}-p_{\\mathrm{exact}}|\\le c\\sqrt{p_{\\mathrm{exact}}(1-p_{\\mathrm{exact}})/N}$，其中 $c$ 为一固定常数。\n- 界是有效的，即 $p_{\\mathrm{exact}}\\le p_{\\mathrm{bound}}$（在可忽略的数值舍入误差范围内）。\n- $-\\log R_K$ 的样本均值和方差与推导出的理论值在规定的容差范围内相匹配。\n\n对于所有部分，没有物理单位。所有角度（如果出现）必须以弧度为单位。所有百分比形式的答案必须表示为 $(0,1)$ 内的小数。\n\n测试套件与答案规范：\n- 使用以下五个测试用例，每个用例指定为一个元组 $(\\alpha,K,\\varepsilon,N)$：\n  - $(1.0,10,0.2,150000)$\n  - $(0.5,20,0.05,150000)$\n  - $(5.0,50,0.5,150000)$\n  - $(2.0,1,0.5,150000)$\n  - $(2.0,100,0.1,150000)$\n- 对于每个测试用例，产生三个布尔结果：\n  - $b_{\\mathrm{prob}}$: $|\\widehat{p}-p_{\\mathrm{exact}}|\\le 4\\sqrt{p_{\\mathrm{exact}}(1-p_{\\mathrm{exact}})/N}$ 是否成立。\n  - $b_{\\mathrm{bound}}$: $p_{\\mathrm{exact}}\\le p_{\\mathrm{bound}}+10^{-12}$ 是否成立。\n  - $b_{\\mathrm{mom}}$: $-\\log R_K$ 的样本均值的相对误差至多为 $0.05$ 且 $-\\log R_K$ 的样本方差的相对误差至多为 $0.10$ 是否同时成立。\n- 您的程序应生成单行输出，其中包含 15 个布尔结果，按测试用例和标准排序，形式为用方括号括起来的逗号分隔列表，例如 $[b_{\\mathrm{prob,1}},b_{\\mathrm{bound,1}},b_{\\mathrm{mom,1}},\\dots,b_{\\mathrm{prob,5}},b_{\\mathrm{bound,5}},b_{\\mathrm{mom,5}}]$。\n\n您的最终答案必须是一个完整的、可运行的程序，该程序实现上述的模拟、精确计算和检查，并仅打印指定的单行输出。", "solution": "用户提供的问题经评估是有效的。它在科学上基于随机过程理论，特别是狄利克雷过程及其折棍构造。该问题是良定的，提供了所有必要的参数和定义，并要求基于标准的、可验证的数学和统计方法给出解决方案。问题中没有矛盾、歧义或伪科学主张。\n\n解决方案按照问题陈述中指定的四个相互关联的部分进行。\n\n### 第 1 部分：$-\\log R_K$ 分布的推导\n\n经过 $K$ 次折断后的剩余质量定义为 $R_K = \\prod_{j=1}^{K}(1 - V_j)$，其中 $\\{V_j\\}_{j=1}^K$ 是服从贝塔 (Beta) 分布的独立同分布 (i.i.d.) 随机变量，即 $V_j \\sim \\mathrm{Beta}(1, \\alpha)$，$\\alpha  0$。\n\n我们关心 $Y_K = -\\log R_K$ 的分布。利用对数的性质，我们可以将 $Y_K$ 写成一个和式：\n$$Y_K = -\\log\\left(\\prod_{j=1}^{K}(1 - V_j)\\right) = \\sum_{j=1}^{K} \\left[-\\log(1 - V_j)\\right]$$\n由于变量 $V_j$ 是独立同分布的，变换后的变量 $X_j = -\\log(1 - V_j)$ 也是独立同分布的。为了求出它们的和 $Y_K$ 的分布，我们首先确定单个项 $X_j$ 的分布。\n\n令 $X = -\\log(1 - V)$，其中 $V \\sim \\mathrm{Beta}(1, \\alpha)$。一个 $\\mathrm{Beta}(a, b)$ 变量的概率密度函数 (PDF) 为 $f_V(v) = \\frac{v^{a-1}(1-v)^{b-1}}{B(a, b)}$，适用于 $v \\in (0,1)$，其中 $B(a, b)$ 是贝塔函数。在我们的例子中，$a=1$，$b=\\alpha$。贝塔函数为 $B(1, \\alpha) = \\frac{\\Gamma(1)\\Gamma(\\alpha)}{\\Gamma(1+\\alpha)} = \\frac{1 \\cdot \\Gamma(\\alpha)}{\\alpha \\Gamma(\\alpha)} = \\frac{1}{\\alpha}$。\n因此，$V$ 的 PDF 为：\n$$f_V(v) = \\frac{v^{1-1}(1-v)^{\\alpha-1}}{1/\\alpha} = \\alpha (1-v)^{\\alpha-1}, \\quad v \\in (0,1)$$\n我们使用变量替换技术来求 $X$ 的 PDF。变换为 $X = g(V) = -\\log(1-V)$。其反变换为 $V = g^{-1}(X) = 1 - e^{-X}$。$V$ 的定义域是 $(0,1)$，这映射到 $X$ 的定义域 $(0, \\infty)$。反变换的雅可比行列式的绝对值为：\n$$\\left|\\frac{dV}{dX}\\right| = \\left|\\frac{d}{dX}(1 - e^{-X})\\right| = |e^{-X}| = e^{-X}$$\n$X$ 的 PDF 由 $f_X(x) = f_V(v(x)) \\left|\\frac{dv}{dx}\\right|$ 给出：\n$$f_X(x) = \\alpha(1 - (1 - e^{-x}))^{\\alpha-1} \\cdot e^{-X} = \\alpha(e^{-x})^{\\alpha-1} e^{-x} = \\alpha e^{-x(\\alpha-1)} e^{-x} = \\alpha e^{-\\alpha x}, \\quad x  0$$\n这是率参数为 $\\lambda = \\alpha$ 的指数分布的 PDF。因此，$X_j = -\\log(1-V_j) \\sim \\mathrm{Exponential}(\\alpha)$。\n\n随机变量 $Y_K = -\\log R_K$ 是 $K$ 个独立同分布的 $\\mathrm{Exponential}(\\alpha)$ 变量之和。概率论中一个著名的结论是，$K$ 个率参数为 $\\lambda$ 的独立同分布指数随机变量之和服从形状参数为 $k=K$、率参数为 $\\lambda$ 的伽马分布。\n因此，$-\\log R_K$ 的分布为：\n$$-\\log R_K \\sim \\mathrm{Gamma}(K, \\alpha)$$\n（使用形状-率参数化）。\n\n一个 $\\mathrm{Gamma}(k, \\lambda)$ 分布的均值和方差分别为 $k/\\lambda$ 和 $k/\\lambda^2$。对于 $Y_K \\sim \\mathrm{Gamma}(K, \\alpha)$，其理论矩为：\n$$\\mathbb{E}[-\\log R_K] = \\frac{K}{\\alpha} \\quad \\text{和} \\quad \\mathrm{Var}(-\\log R_K) = \\frac{K}{\\alpha^2}$$\n\n### 第 2 部分：$\\mathbb{P}(R_K  \\varepsilon)$ 的 Chernoff-Markov 界\n\n我们寻求对于 $\\varepsilon \\in (0,1)$ 的概率 $p = \\mathbb{P}(R_K  \\varepsilon)$ 的一个指数界。这个概率可以根据 $Y_K = -\\log R_K$ 重写为：\n$$p = \\mathbb{P}(R_K  \\varepsilon) = \\mathbb{P}(-\\log R_K  -\\log \\varepsilon)$$\n令 $a = -\\log \\varepsilon$。由于 $\\varepsilon \\in (0,1)$，我们有 $a0$。问题就变成了对 $\\mathbb{P}(Y_K  a)$ 进行界定。\n\n我们对下尾使用 Chernoff-Markov 界定技术。对于任何参数 $t  0$，不等式 $Y_K  a$ 等价于 $t Y_K > t a$，这意味着 $e^{t Y_K} > e^{t a}$。应用马尔可夫不等式：\n$$\\mathbb{P}(Y_K  a) = \\mathbb{P}(e^{tY_K} > e^{ta}) \\le \\frac{\\mathbb{E}[e^{tY_K}]}{e^{ta}} = e^{-ta} M_{Y_K}(t)$$\n其中 $M_{Y_K}(t)$ 是 $Y_K$ 的矩生成函数 (MGF)。\n\n一个 $\\mathrm{Exponential}(\\alpha)$ 随机变量 $X_j$ 的 MGF 是 $M_{X_j}(t) = \\frac{\\alpha}{\\alpha-t}$，适用于 $t\\alpha$。由于 $Y_K$ 是 $K$ 个这样的独立同分布变量之和，其 MGF 为：\n$$M_{Y_K}(t) = \\left(M_{X_j}(t)\\right)^K = \\left(\\frac{\\alpha}{\\alpha-t}\\right)^K, \\quad t  \\alpha$$\n该界对任何 $t0$ 都有效。为了得到最紧的界，我们必须关于 $t \\in (-\\infty, 0)$ 最小化 $g(t) = e^{-ta} \\left(\\frac{\\alpha}{\\alpha-t}\\right)^K$。最小化 $g(t)$ 等价于最小化 $\\log g(t)$：\n$$\\log g(t) = -ta + K\\log\\alpha - K\\log(\\alpha - t)$$\n关于 $t$ 求导并令其为零，得到最优的 $t$：\n$$\\frac{d}{dt}\\log g(t) = -a - K\\left(\\frac{-1}{\\alpha-t}\\right) = -a + \\frac{K}{\\alpha-t} = 0 \\implies t_{\\mathrm{opt}} = \\alpha - \\frac{K}{a}$$\n优化要求 $t_{\\mathrm{opt}}  0$。代入 $a = -\\log\\varepsilon$：\n$$\\alpha - \\frac{K}{-\\log\\varepsilon}  0 \\implies \\alpha  \\frac{K}{-\\log\\varepsilon} \\implies \\varepsilon  e^{-K/\\alpha}$$\n**情况 1：非平凡界 ($\\varepsilon  e^{-K/\\alpha}$)**\n当此条件成立时，$t_{\\mathrm{opt}}$ 位于有效范围 $(-\\infty, 0)$ 内。我们将 $t_{\\mathrm{opt}} = \\alpha - K/a$ 代回界的表达式中：\n$$p_{\\mathrm{bound}} = e^{-(\\alpha-K/a)a} \\left(\\frac{\\alpha}{\\alpha - (\\alpha-K/a)}\\right)^K = e^{-a\\alpha+K} \\left(\\frac{\\alpha}{K/a}\\right)^K = e^{-a\\alpha}e^K \\left(\\frac{a\\alpha}{K}\\right)^K$$\n代入 $a = -\\log\\varepsilon = \\log(1/\\varepsilon)$：\n$$p_{\\mathrm{bound}} = e^{\\alpha\\log\\varepsilon}e^K \\left(\\frac{\\alpha\\log(1/\\varepsilon)}{K}\\right)^K = \\varepsilon^{\\alpha} e^K \\left(\\frac{\\alpha\\log(1/\\varepsilon)}{K}\\right)^K$$\n\n**情况 2：平凡界 ($\\varepsilon \\le e^{-K/\\alpha}$)**\n如果此条件成立，则 $t_{\\mathrm{opt}} \\ge 0$。函数 $g(t)$ 是凸函数，其在半直线 $(-\\infty, 0)$ 上的最小值出现在边界处，即当 $t \\to 0^-$ 时。\n$$\\lim_{t\\to 0^-} g(t) = \\lim_{t\\to 0^-} e^{-ta} \\left(\\frac{\\alpha}{\\alpha-t}\\right)^K = e^0 \\left(\\frac{\\alpha}{\\alpha}\\right)^K = 1$$\n在此范围内，Chernoff 界是平凡上界 $p_{\\mathrm{bound}} = 1$。\n\n### 第 3 和 4 部分：模拟设计与验证\n\n**模拟与计算：** 基于第 1 部分的推导，我们可以设计一个高效的模拟。我们不需要为每次重复实验模拟 $K$ 个贝塔变量，而是直接模拟一个 $\\mathrm{Gamma}(K, \\alpha)$ 变量，它的分布与 $-\\log R_K$ 完全相同。\n\n对于每个测试用例 $(\\alpha, K, \\varepsilon, N)$：\n1.  **模拟：** 从 $\\mathrm{Gamma}(K, \\alpha)$ 分布中生成 $N$ 个独立样本 $y_1, \\dots, y_N$。在 Python 的 `scipy` 和 `numpy` 库中，这对应于形状参数为 $k=K$、尺度参数为 $\\theta=1/\\alpha$ 的伽马分布。\n2.  **经验估计：**\n    -   经验概率 $\\widehat{p} = \\mathbb{P}(R_K  \\varepsilon)$ 通过计算样本中满足 $y_i  -\\log\\varepsilon$ 的比例来估计：$\\widehat{p} = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{I}(y_i  -\\log\\varepsilon)$。\n    -   计算样本 $\\{y_i\\}$ 的经验均值 $\\widehat{\\mu}_Y$ 和方差 $\\widehat{\\sigma}^2_Y$。\n3.  **精确值：**\n    -   精确概率 $p_{\\mathrm{exact}} = \\mathbb{P}(-\\log R_K  -\\log\\varepsilon)$ 是 $\\mathrm{Gamma}(K, \\alpha)$ 分布在 $-\\log\\varepsilon$ 处的累积分布函数 (CDF) 的值。\n    -   理论均值为 $\\mu_Y = K/\\alpha$，理论方差为 $\\sigma^2_Y = K/\\alpha^2$。\n4.  **Chernoff 界：** 使用第 2 部分的公式计算界 $p_{\\mathrm{bound}}$，具体取决于 $\\varepsilon  e^{-K/\\alpha}$ 是否成立。\n\n**验证检查：** 对于每个测试用例，我们计算三个布尔值：\n1.  $b_{\\mathrm{prob}}$：此项检查蒙特卡洛估计值 $\\widehat{p}$ 是否在统计上接近精确概率 $p_{\\mathrm{exact}}$。条件 $|\\widehat{p}-p_{\\mathrm{exact}}|\\le 4\\sqrt{p_{\\mathrm{exact}}(1-p_{\\mathrm{exact}})/N}$ 验证估计误差是否落在 $4$-sigma 置信区间内，这是对模拟准确性的严格测试。\n2.  $b_{\\mathrm{bound}}$：此项验证我们推导的 Chernoff 界的有效性。条件 $p_{\\mathrm{exact}}\\le p_{\\mathrm{bound}}+10^{-12}$ 确认精确概率不超过该界，其中包含一个用于浮点数不精确性的小容差。\n3.  $b_{\\mathrm{mom}}$：此项验证理论矩计算和模拟的分布正确性。它要求样本均值的相对误差至多为 $5\\%$，样本方差的相对误差至多为 $10\\%$。\n\n实现将对每个指定的参数集执行这些步骤，并生成所需的布尔输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats as sps\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving, simulating, and validating properties\n    of the residual mass in a GEM stick-breaking process.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, K, epsilon, N)\n        (1.0, 10, 0.2, 150000),\n        (0.5, 20, 0.05, 150000),\n        (5.0, 50, 0.5, 150000),\n        (2.0, 1, 0.5, 150000),\n        (2.0, 100, 0.1, 150000)\n    ]\n\n    results = []\n    \n    # Use a fixed pseudo-random seed for reproducibility across all test cases.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    for alpha, K, epsilon, N in test_cases:\n        # Based on Part 1, -log(R_K) ~ Gamma(shape=K, rate=alpha).\n        # In numpy/scipy, this corresponds to shape=K and scale=1/alpha.\n        gamma_shape = float(K)\n        gamma_rate = float(alpha)\n        gamma_scale = 1.0 / gamma_rate\n\n        # --- Part 3: Simulation ---\n        # Generate N samples from the Gamma distribution.\n        # This is equivalent to simulating the stick-breaking process for -log(R_K).\n        y_samples = rng.gamma(shape=gamma_shape, scale=gamma_scale, size=N)\n        \n        # The critical value for the comparison\n        x_crit = -np.log(epsilon)\n\n        # Estimate the probability P(R_K > epsilon) = P(-log R_K  -log epsilon)\n        p_hat = np.mean(y_samples  x_crit)\n\n        # Estimate the mean and variance of -log(R_K)\n        mean_emp = np.mean(y_samples)\n        # Use ddof=1 for the sample variance\n        var_emp = np.var(y_samples, ddof=1)\n\n        # --- Part 4: Exact Computation and Validation ---\n        \n        # Calculate exact probability using the Gamma CDF\n        p_exact = sps.gamma.cdf(x_crit, a=gamma_shape, scale=gamma_scale)\n        \n        # Calculate the theoretical moments of -log(R_K) ~ Gamma(K, alpha)\n        mean_th = gamma_shape / gamma_rate  # K / alpha\n        var_th = gamma_shape / (gamma_rate**2) # K / alpha^2\n\n        # --- Part 2: Calculate the Chernoff Bound ---\n        \n        # Check condition for the non-trivial bound\n        if epsilon > np.exp(-K / alpha):\n            log_eps_inv = np.log(1.0 / epsilon)\n            term1 = epsilon**alpha\n            term2 = np.exp(K)\n            term3 = ((alpha * log_eps_inv) / K)**K\n            p_bound = term1 * term2 * term3\n        else:\n            # Trivial bound\n            p_bound = 1.0\n\n        # --- Perform the three required validation checks ---\n\n        # 1. Probability estimate check\n        # For p_exact=0 or p_exact=1, the standard error is 0.\n        if p_exact == 0.0 or p_exact == 1.0:\n            prob_check_threshold = 0.0\n        else:\n            prob_check_threshold = 4.0 * np.sqrt(p_exact * (1.0 - p_exact) / N)\n        \n        b_prob = np.abs(p_hat - p_exact) = prob_check_threshold\n\n        # 2. Bound validity check\n        b_bound = p_exact = p_bound + 1e-12\n\n        # 3. Moments match check\n        mean_rel_error = np.abs(mean_emp - mean_th) / mean_th if mean_th != 0 else np.abs(mean_emp)\n        var_rel_error = np.abs(var_emp - var_th) / var_th if var_th != 0 else np.abs(var_emp)\n        \n        b_mom = (mean_rel_error = 0.05) and (var_rel_error = 0.10)\n        \n        results.extend([b_prob, b_bound, b_mom])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3340212"}, {"introduction": "在贝叶斯非参数模型中，一个核心任务是计算后验分布下的期望值，例如一个随机测度 $G$ 的泛函期望 $\\mathbb{E}[G(f) \\mid X_{1:n}]$。本练习将折棒构造应用于后验狄利克雷过程，带领你从理解先验转向执行实际的后验推断。你将实现一个精巧的无偏蒙特卡洛估计器，通过随机化方法处理折棒表述的无限性，从而避免截断带来的偏差，这是将理论应用于高效计算的关键一步。[@problem_id:3340233]", "problem": "给定一个狄利克雷过程先验、一个有界可测检验函数和数据集。令 $G \\sim \\operatorname{DP}(\\alpha, H)$ 是一个狄利克雷过程 (DP)，其集中参数为 $\\alpha \\in (0,\\infty)$，基测度为 $H$，定义在一个完备可分度量空间上。令 $X_{1:n} = (X_1,\\ldots,X_n)$ 是从 $G$ 中抽取的条件独立同分布的观测值。考虑一个有界可测函数 $f:\\mathbb{R}\\to\\mathbb{R}$，并定义随机线性泛函 $G(f) = \\int f(x)\\,G(\\mathrm{d}x)$。目标是计算后验期望 $\\mathbb{E}[G(f)\\mid X_{1:n}]$。\n\n仅从关于狄利克雷过程及其后验共轭性的基本性质，以及 Sethuraman 折棍构造出发，完成以下任务：\n\n- 在不使用任何专门捷径的情况下，从第一性原理推导 $\\mathbb{E}[G(f)\\mid X_{1:n}]$ 的解析表达式。你的推导必须从“后验分布本身是一个具有更新参数的狄利克雷过程”这一定义，以及狄利克雷过程作为折棍随机概率测度的 Sethuraman 表示法开始。\n- 构造一个 $\\mathbb{E}[G(f)\\mid X_{1:n}]$ 的无偏蒙特卡洛（MC）估计量，该估计量使用后验折棍采样。你的估计量必须是无偏的，没有确定性截断误差；为实现此目标，请使用一个单项随机化估计量，该估计量从折棍分量上的一个提议分布中采样一个索引，并进行适当的重加权，以使其期望值等于该无穷级数和。\n- 经验性地验证你的无偏估计量的样本均值与你的解析表达式在与蒙特卡洛标准误成正比的数据驱动容差范围内一致。\n\n为确保数值明确性和可测试性，对基测度和检验函数使用以下建模选择：\n\n- 基测度 $H$ 是 $\\mathbb{R}$ 上的标准正态分布 $\\mathcal{N}(0,1)$。\n- 检验函数为 $f(x) = \\mathrm{e}^{-x^2}$，适用于所有 $x \\in \\mathbb{R}$。\n\n实现一个程序，对下面的每个测试用例，执行以下所有步骤：\n\n1. 基于狄利克雷过程的后验特性，计算 $\\mathbb{E}[G(f)\\mid X_{1:n}]$ 的解析值。\n2. 使用集中参数为 $\\alpha + n$ 的后验折棍采样构造一个无偏估计量，并验证 $R$ 次独立重复实验的样本均值与解析值之差在 6 倍蒙特卡洛标准误之内。使用 Sethuraman 折棍表示，其中 $V_k \\sim \\operatorname{Beta}(1,\\alpha+n)$ 且独立原子 $\\Theta_k \\sim \\tilde{H}$，而 $\\tilde{H}$ 是后验基测度。\n3. 对每个测试用例使用 $R = 60000$ 次重复实验，并使用下面指定的固定随机种子以保证可复现性。\n\n测试套件：\n\n- 情况 1：$\\alpha = 0.7$，$n=0$，$X_{1:n} = \\varnothing$，种子 $= 12345$。\n- 情况 2：$\\alpha = 1.5$，$n=3$，$X_{1:n} = (0.0, 1.0, -1.0)$，种子 $= 54321$。\n- 情况 3：$\\alpha = 3.0$，$n=1$，$X_{1:n} = (2.0)$，种子 $= 11111$。\n\n后验基测度为 $\\tilde{H} = \\frac{\\alpha H + \\sum_{i=1}^{n}\\delta_{X_i}}{\\alpha + n}$，其中 $\\delta_{x}$ 表示位于 $x$ 的单位点质量。从 $\\tilde{H}$ 采样时，以 $\\alpha/(\\alpha+n)$ 的概率从 $H$ 中抽取，否则从观测到的原子 $X_1,\\ldots,X_n$ 中均匀选择。\n\n你的程序必须输出单行结果，该行包含一个方括号内的逗号分隔列表，每个测试用例对应一个布尔值，表示该用例的经验均值是否在指定容差内与解析值匹配。\n\n最终输出格式：\n\n- 最后一行必须严格遵循 $[b_1,b_2,b_3]$ 的格式，其中每个 $b_j$ 是 $\\mathrm{True}$ 或 $\\mathrm{False}$。\n\n本问题不涉及物理单位。不出现角度。所有数值必须以原始小数值报告，不带任何百分号。程序不得要求用户输入。\n\n为确保无偏性和有限的期望计算成本，请注意以下重要实现细节：\n\n- 使用集中参数为 $c = \\alpha + n$ 的后验折棍表示。\n- 令 $\\pi_k = V_k \\prod_{j=1}^{k-1}(1 - V_j)$ 为折棍权重，其中 $V_k \\sim \\operatorname{Beta}(1,c)$ 是独立的。\n- 使用单项随机化估计量：从支撑集为 $\\{1,2,\\ldots\\}$ 的提议分布 $q_k$ 中抽取一个索引 $K$，仅模拟前 $K$ 次折棍以计算 $\\pi_K$，抽取一个原子 $\\Theta_K \\sim \\tilde{H}$，并返回 $W = \\pi_K f(\\Theta_K) / q_K$。选择 $q_k$ 为平均折棍权重 $q_k = \\mathbb{E}[\\pi_k]$，这样 $\\sum_{k=1}^{\\infty} q_k = 1$，并且为了计算效率，对 $K$ 的采样是几何分布的。\n\n你的程序应生成单行输出，其中包含一个方括号内的逗号分隔列表的结果（例如，$[b_1,b_2,b_3]$）。", "solution": "该问题是有效的，因为它在科学上基于贝叶斯非参数理论，是适定、客观的，并为解析推导和数值实现提供了完整且一致的设置。\n\n解决方案包括三个主要部分：后验期望 $\\mathbb{E}[G(f)\\mid X_{1:n}]$ 的解析推导、该数量的无偏蒙特卡洛估计量的构造，以及比较两者的经验性验证。\n\n### 1. 解析推导\n\n推导依赖于狄利克雷过程（$\\operatorname{DP}$）的两个基本性质。\n\n首先，狄利克雷过程是自共轭的。如果一个随机概率测度 $G$ 从狄利克雷过程先验 $G \\sim \\operatorname{DP}(\\alpha, H)$ 中抽取，并且观测值 $X_{1:n}$ 是从 $G$ 中独立同分布地抽取的，那么给定观测值时 $G$ 的后验分布也是一个狄利克雷过程。后验参数更新如下：\n$$ (G \\mid X_{1:n}) \\sim \\operatorname{DP}(\\alpha', \\tilde{H}) $$\n其中后验集中参数为 $\\alpha' = \\alpha + n$，后验基测度是先验基测度与位于观测值位置的原子质量的混合：\n$$ \\tilde{H} = \\frac{\\alpha H + \\sum_{i=1}^{n}\\delta_{X_i}}{\\alpha + n} $$\n这里，$\\delta_{x}$ 表示位于 $x$ 的狄拉克 δ 测度。\n\n其次，一个随机测度 $G' \\sim \\operatorname{DP}(c, H')$ 的期望是其归一化的基测度，即 $\\mathbb{E}[G'] = H'$。这个性质可以从 Sethuraman 折棍构造中推导出来。从 $\\operatorname{DP}(c, H')$ 中抽取的一个样本 $G'$ 可以表示为加权点质量的无穷和：\n$$ G' = \\sum_{k=1}^{\\infty} \\pi_k \\delta_{\\Theta_k} $$\n其中权重（或“棍子长度”）为 $\\pi_k = V_k \\prod_{j=1}^{k-1}(1-V_j)$，且 $V_k \\sim \\operatorname{Beta}(1, c)$ 是独立同分布的，而原子 $\\Theta_k \\sim H'$ 也是独立同分布的，且与权重无关。我们感兴趣的量是线性泛函 $G(f) = \\int f(x) G(\\mathrm{d}x)$ 的后验期望。对于一个泛指的 $G' \\sim \\operatorname{DP}(c, H')$，其 $G'(f)$ 的期望为：\n$$ \\mathbb{E}[G'(f)] = \\mathbb{E}\\left[\\sum_{k=1}^{\\infty} \\pi_k f(\\Theta_k)\\right] $$\n根据期望的线性性质（并由富比尼定理保证），我们可以交换求和与期望的顺序：\n$$ \\mathbb{E}[G'(f)] = \\sum_{k=1}^{\\infty} \\mathbb{E}[\\pi_k f(\\Theta_k)] $$\n由于棍长 $\\pi_k$ 和原子 $\\Theta_k$ 的独立性，上式变为：\n$$ \\mathbb{E}[G'(f)] = \\sum_{k=1}^{\\infty} \\mathbb{E}[\\pi_k] \\mathbb{E}[f(\\Theta_k)] $$\n由于对所有 $k$ 都有 $\\Theta_k \\sim H'$，我们得到 $\\mathbb{E}[f(\\Theta_k)] = \\int f(x) H'(\\mathrm{d}x)$。折棍权重几乎必然和为一，即 $\\sum_{k=1}^{\\infty} \\pi_k = 1$，所以它们的期望和也为一，即 $\\sum_{k=1}^{\\infty} \\mathbb{E}[\\pi_k] = 1$。这导出了以下结果：\n$$ \\mathbb{E}[G'(f)] = \\left(\\sum_{k=1}^{\\infty} \\mathbb{E}[\\pi_k]\\right) \\int f(x) H'(\\mathrm{d}x) = \\int f(x) H'(\\mathrm{d}x) $$\n将此结果应用于后验分布 $(G \\mid X_{1:n})$，我们确定 $c = \\alpha' = \\alpha+n$ 且 $H' = \\tilde{H}$。因此，所求的后验期望是：\n$$ \\mathbb{E}[G(f) \\mid X_{1:n}] = \\int f(x) \\tilde{H}(\\mathrm{d}x) $$\n代入 $\\tilde{H}$ 的定义：\n$$ \\mathbb{E}[G(f) \\mid X_{1:n}] = \\int f(x) \\left( \\frac{\\alpha}{\\alpha + n} H(\\mathrm{d}x) + \\frac{1}{\\alpha + n} \\sum_{i=1}^{n} \\delta_{X_i}(\\mathrm{d}x) \\right) $$\n根据积分的线性性质，上式简化为：\n$$ \\mathbb{E}[G(f) \\mid X_{1:n}] = \\frac{\\alpha}{\\alpha + n} \\int f(x) H(\\mathrm{d}x) + \\frac{1}{\\alpha + n} \\sum_{i=1}^{n} f(X_i) $$\n对于问题中的具体选择，$H$ 是标准正态分布 $\\mathcal{N}(0,1)$，其密度函数为 $\\phi(x) = (2\\pi)^{-1/2} e^{-x^2/2}$，且 $f(x) = e^{-x^2}$。积分项为：\n$$ \\int_{-\\infty}^{\\infty} e^{-x^2} \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} \\mathrm{d}x = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-\\frac{3}{2}x^2} \\mathrm{d}x $$\n将被积函数看作方差为 $\\sigma^2 = 1/3$ 的未归一化高斯密度，该积分的值为 $\\sqrt{2\\pi \\sigma^2} = \\sqrt{2\\pi/3}$。因此：\n$$ \\int f(x) H(\\mathrm{d}x) = \\frac{1}{\\sqrt{2\\pi}} \\sqrt{\\frac{2\\pi}{3}} = \\frac{1}{\\sqrt{3}} $$\n后验期望的最终解析表达式为：\n$$ \\mu_{\\text{post}} = \\frac{\\alpha}{\\alpha + n} \\frac{1}{\\sqrt{3}} + \\frac{1}{\\alpha + n} \\sum_{i=1}^{n} e^{-X_i^2} $$\n\n### 2. 无偏蒙特卡洛估计量\n\n解析表达式 $\\mu_{\\text{post}}$ 是随机变量 $S = G(f) = \\sum_{k=1}^{\\infty} \\pi_k f(\\Theta_k)$ 在后验分布下的期望。一个模拟 $S$ 的朴素蒙特卡洛方法需要截断无穷级数，从而引入偏差。为了创建一个无偏估计量，我们使用单项随机化估计量。\n\n令目标为 $\\mu_{\\text{post}} = \\mathbb{E}[S]$。我们可以写出 $\\mu_{\\text{post}} = \\sum_{k=1}^\\infty \\mathbb{E}[\\pi_k f(\\Theta_k)]$。单项估计量针对的是这个期望的无穷和。我们在整数上选择一个提议概率质量函数 $q = (q_k)_{k=1}^{\\infty}$。估计量 $W$ 的单次重复实验是通过采样一个索引 $K \\sim q$ 然后构造值 $W = \\frac{\\pi_K f(\\Theta_K)}{q_K}$ 来构建的。$W$ 的期望（对所有随机性——$K$ 的选择、折棍 $V_j$ 和原子 $\\Theta_k$——取期望）是：\n$$ \\mathbb{E}[W] = \\sum_{k=1}^{\\infty} q_k \\mathbb{E}\\left[ \\frac{\\pi_k f(\\Theta_k)}{q_k} \\mid K=k \\right] = \\sum_{k=1}^{\\infty} \\mathbb{E}[\\pi_k f(\\Theta_k)] = \\mathbb{E}\\left[\\sum_{k=1}^{\\infty} \\pi_k f(\\Theta_k)\\right] = \\mu_{\\text{post}} $$\n因此，$W$ 是 $\\mu_{\\text{post}}$ 的一个无偏估计量。对于提议分布 $q_k$ 的一个好选择是，当函数 $f$ 已知时，选择与被求和项成正比的分布，这能最小化估计量的方差。然而，由于 $\\pi_k$ 是随机的，一个实用的选择是 $q_k = \\mathbb{E}[\\pi_k]$。\n\n令 $c = \\alpha+n$。后验折棍变量为 $V_k \\sim \\operatorname{Beta}(1,c)$。$\\pi_k = V_k \\prod_{j=1}^{k-1} (1-V_j)$ 的期望是：\n$$ q_k = \\mathbb{E}[\\pi_k] = \\mathbb{E}[V_k] \\prod_{j=1}^{k-1} \\mathbb{E}[1-V_j] = \\frac{1}{1+c} \\left(\\frac{c}{1+c}\\right)^{k-1} $$\n这是在 $\\{1, 2, \\ldots\\}$ 上，成功概率为 $p = 1/(1+c)$ 的几何分布的概率质量函数。\n\n生成单次重复实验 $W$ 的算法如下：\n1. 设置 $c = \\alpha+n$。\n2. 从几何分布 $\\operatorname{Geometric}(p=1/(1+c))$ 中采样一个索引 $K$。\n3. 生成 $K$ 个折棍变量 $V_1, \\ldots, V_K \\sim \\operatorname{Beta}(1,c)$。\n4. 计算实现的权重 $\\pi_K = V_K \\prod_{j=1}^{K-1} (1-V_j)$。\n5. 从 $\\tilde{H}$ 中采样一个原子 $\\Theta_K$。具体操作是：以 $\\alpha/c$ 的概率从 $H=\\mathcal{N}(0,1)$ 中抽取，或以 $n/c$ 的概率从数据 $\\{X_1, \\ldots, X_n\\}$ 中均匀抽取。\n6. 计算函数值 $f(\\Theta_K) = e^{-\\Theta_K^2}$。\n7. 计算提议概率 $q_K = \\frac{1}{1+c}(\\frac{c}{1+c})^{K-1}$。\n8. 估计量的值为 $W = \\frac{\\pi_K f(\\Theta_K)}{q_K}$。\n\n为了数值稳定性，涉及乘积的计算在对数域中执行。\n\n### 3. 经验性验证\n\n对每个测试用例，我们执行以下操作：\n1. 使用推导出的公式计算解析值 $\\mu_{\\text{post}}$。\n2. 生成 $R = 60000$ 次无偏估计量的独立重复实验 $W_1, \\ldots, W_R$。\n3. 计算样本均值 $\\bar{W} = \\frac{1}{R} \\sum_{r=1}^R W_r$。\n4. 计算样本标准差 $s_W = \\sqrt{\\frac{1}{R-1}\\sum_{r=1}^R (W_r - \\bar{W})^2}$。\n5. 计算均值的蒙特卡洛标准误 (MCSE)：$\\text{MCSE} = s_W / \\sqrt{R}$。\n6. 验证经验均值与解析值之间的绝对差是否在 6 倍 MCSE 的容差范围内：$|\\bar{W} - \\mu_{\\text{post}}| \\leq 6 \\times \\text{MCSE}$。结果是一个布尔值，指示此条件是否成立。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases specified.\n    \"\"\"\n\n    test_cases = [\n        {'alpha': 0.7, 'data': np.array([]), 'seed': 12345},\n        {'alpha': 1.5, 'data': np.array([0.0, 1.0, -1.0]), 'seed': 54321},\n        {'alpha': 3.0, 'data': np.array([2.0]), 'seed': 11111}\n    ]\n\n    R = 60000\n    results = []\n\n    for case in test_cases:\n        alpha = case['alpha']\n        data = case['data']\n        seed = case['seed']\n        n = len(data)\n\n        # 1. Compute analytical value\n        h_f_integral = 1.0 / np.sqrt(3.0)\n        \n        sum_f_x = np.sum(np.exp(-data**2))\n        \n        c = alpha + n\n        \n        if c > 0:\n            analytical_val = (alpha / c) * h_f_integral + (1.0 / c) * sum_f_x\n        else: # n=0, alpha=0, but problem states alpha > 0, so this is just for safety.\n            analytical_val = h_f_integral\n\n        # 2. Run Monte Carlo simulation\n        rng = np.random.default_rng(seed)\n        replicates = np.array([_get_one_replicate(alpha, data, n, c, rng) for _ in range(R)])\n\n        # 3. Verify\n        mc_mean = np.mean(replicates)\n        mc_std = np.std(replicates, ddof=1)\n        mc_se = mc_std / np.sqrt(R)\n\n        tolerance = 6.0 * mc_se\n        is_verified = np.abs(mc_mean - analytical_val) = tolerance\n        results.append(is_verified)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _get_one_replicate(alpha, data, n, c, rng):\n    \"\"\"\n    Generates a single replicate of the unbiased estimator.\n    \"\"\"\n    # 1. Sample K from Geometric(p=1/(1+c))\n    p = 1.0 / (1.0 + c)\n    # numpy.random.geometric gives number of trials until success, which is on {1, 2, ...}\n    K = rng.geometric(p)\n\n    # 2. Sample Vs and compute log(pi_K)\n    # V_k ~ Beta(1, c)\n    Vs = rng.beta(1, c, size=K)\n    \n    # log pi_K = log V_K + sum_{j=1 to K-1} log(1 - V_j)\n    # Using 0-based indexing for Vs array of size K:\n    # log pi_K = log(Vs[K-1]) + sum(log(1-Vs[0...K-2]))\n    if K > 1:\n        log_pi_K = np.log(Vs[K - 1]) + np.sum(np.log(1.0 - Vs[:K - 1]))\n    else:  # K = 1\n        log_pi_K = np.log(Vs[0])\n\n    # 3. Sample Theta_K from the posterior base measure H_tilde\n    prob_H = alpha / c\n    \n    # Sample from H with prob alpha/c, from empirical measure with prob n/c\n    # This also handles n=0 case, as prob_H becomes 1.\n    if rng.random()  prob_H:\n        theta_K = rng.standard_normal()\n    else:\n        theta_K = rng.choice(data)\n\n    # 4. Compute log f(Theta_K)\n    log_f_theta_K = -theta_K**2\n\n    # 5. Compute log q_K, where q_K = E[pi_K]\n    # log q_K = log(1/(1+c)) + (K-1)*log(c/(1+c))\n    # log q_K = -log(1+c) + (K-1)*(log(c) - log(1+c))\n    # log q_K = (K-1)*log(c) - K*log(1+c)\n    if K > 1:\n        log_q_K = (K - 1) * np.log(c) - K * np.log(1.0 + c)\n    else: # K=1\n        log_q_K = -np.log(1.0 + c)\n\n    # 6. Compute log of the estimator W\n    log_W = log_pi_K + log_f_theta_K - log_q_K\n\n    return np.exp(log_W)\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3340233"}]}