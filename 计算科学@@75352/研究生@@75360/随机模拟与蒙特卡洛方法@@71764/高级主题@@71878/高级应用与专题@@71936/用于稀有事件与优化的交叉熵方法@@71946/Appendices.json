{"hands_on_practices": [{"introduction": "我们从一个基础但极具启发性的练习开始。该练习要求我们为一个简单的一维高斯模型推导交叉熵（CE）方法的参数更新法则，并将其应用于稀有事件概率的估计。通过这个练习[@problem_id:3351653]，您将亲手实践如何将最小化Kullback-Leibler（KL）散度的理论目标，转化为一个对精英样本进行最大似然估计的实际问题，从而为理解CE方法奠定坚实的理论和编程基础。", "problem": "考虑一个在标准正态参考模型下的罕见事件概率估计问题。设 $X$ 是一个服从 $X \\sim \\mathcal{N}(0,1)$ 的实值随机变量，罕见事件为超出集 $\\{X \\ge \\gamma\\}$，其中 $\\gamma$ 是一个大阈值。你将通过在一个均值平移的高斯族 $g_\\mu = \\mathcal{N}(\\mu,1)$ 上迭代地开发和实现一个交叉熵方法，来为概率 $p(\\gamma) = \\mathbb{P}(X \\ge \\gamma)$ 构建一个高效的重要性抽样估计器，并从第一性原理推导相应的参数更新法则。\n\n任务：\n\n1) 推导交叉熵更新。从以下基础出发：\n- 从密度为 $h(x)$ 的分布到密度为 $g_\\mu(x)$ 的参数族成员的 Kullback-Leibler 散度 (KLD) 为 $D_{\\mathrm{KL}}(h \\,\\|\\, g_\\mu) = \\int h(x) \\log\\!\\big(\\frac{h(x)}{g_\\mu(x)}\\big) \\, dx$，且在 $\\mu$ 上最小化 $D_{\\mathrm{KL}}(h \\,\\|\\, g_\\mu)$ 等价于在 $\\mu$ 上最大化 $\\int h(x) \\log g_\\mu(x) \\, dx$。\n- 用于估计 $p(\\gamma)$ 的零方差重要性分布的密度正比于 $\\mathbb{1}\\{x \\ge \\gamma\\} f(x)$，其中 $f(x)$ 是标准正态分布 $\\mathcal{N}(0,1)$ 的密度，$\\mathbb{1}\\{\\cdot\\}$ 表示指示函数。\n- 在用于罕见事件估计的交叉熵方法中，人们用一系列易于处理的参数分布 $g_{\\mu_t}$ 来替代难以处理的零方差分布，并在每次迭代 $t$ 中，最大化 $\\int h_t(x) \\log g_\\mu(x)\\,dx$ 的一个基于样本的代理目标，其中 $h_t$ 是一个集中在高性能水平集上的水平分布。在迭代 $t$ 中，一个标准且数值稳定的水平集构建方法是，使用当前在 $g_{\\mu_t}$ 下的样本的经验 $(1-\\rho)$-分位数来定义一个数据驱动的阈值 $c_t$，然后聚焦于精英集 $\\{x \\ge c_t\\}$。\n\n仅使用上述基础，推导迭代法则：对于具有固定单位方差的高斯族 $g_\\mu=\\mathcal{N}(\\mu,1)$，更新后的参数 $\\mu_{t+1}$ 等于从 $g_{\\mu_t}$ 中抽取的精英样本的经验均值。你的推导必须明确展示 Kullback-Leibler 散度目标如何简化为精英集上的最大似然问题，以及为什么将方差固定为1会导致精英样本均值成为最大化者。\n\n2) 指定并实现以下算法，通过使用最终提议分布 $g_{\\mu_T}$ 的重要性抽样来估计 $p(\\gamma)$：\n- 输入：阈值 $\\gamma$，每次迭代的样本量 $n$，精英比例 $\\rho \\in (0,1)$，迭代次数 $T$，以及一个随机种子 $s$。\n- 初始化：$\\mu_0 \\leftarrow 0$ 且 $c_0 \\leftarrow -\\infty$。\n- 对于每次迭代 $t \\in \\{0,1,\\dots,T-1\\}$：\n  - 从 $\\mathcal{N}(\\mu_t,1)$ 中抽取 $n$ 个独立样本 $X_1,\\dots,X_n$。\n  - 令 $q_t$ 为 $\\{X_i\\}_{i=1}^n$ 的经验 $(1-\\rho)$-分位数。\n  - 设置水平 $c_t \\leftarrow \\min\\{\\gamma, q_t\\}$。\n  - 定义精英集 $\\mathcal{E}_t = \\{i \\in \\{1,\\dots,n\\}: X_i \\ge c_t\\}$。将 $\\mu_{t+1}$ 更新为 $\\{X_i: i \\in \\mathcal{E}_t\\}$ 上的经验均值。\n- 经过 $T$ 次迭代后，设 $\\mu^\\star \\leftarrow \\mu_T$。使用一组新的从 $\\mathcal{N}(\\mu^\\star,1)$ 中抽取的 $n$ 个独立样本 $Y_1,\\dots,Y_n$，计算重要性抽样估计器\n  $$\\widehat{p}(\\gamma) \\;=\\; \\frac{1}{n} \\sum_{j=1}^n \\left[\\frac{f(Y_j)}{g_{\\mu^\\star}(Y_j)} \\, \\mathbb{1}\\{Y_j \\ge \\gamma\\}\\right],$$\n  其中 $f$ 是 $\\mathcal{N}(0,1)$ 的密度，$g_{\\mu^\\star}$ 是 $\\mathcal{N}(\\mu^\\star,1)$ 的密度。将似然比表示为 $\\mu^\\star$ 和 $Y_j$ 的闭式函数。\n\n3) 将上述算法实现为一个完整、可运行的程序，该程序：\n- 使用下面提供的测试套件。\n- 产生单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，即格式为 $[r_1,r_2,\\dots]$。\n- 为了数值稳定性，如果在任何迭代中精英集为空，则用当前样本的最大值替换 $\\mu_{t+1}$。\n- 在输出前将每个返回的概率估计四舍五入到 $12$ 位小数，但打印的对象必须是浮点数。\n\n测试套件：\n- 案例 1：$(\\gamma, n, \\rho, T, s) = (2.0, 5000, 0.1, 7, 42)$。\n- 案例 2：$(\\gamma, n, \\rho, T, s) = (4.0, 8000, 0.2, 10, 314159)$。\n- 案例 3：$(\\gamma, n, \\rho, T, s) = (0.0, 4000, 0.1, 4, 7)$。\n- 案例 4：$(\\gamma, n, \\rho, T, s) = (5.5, 12000, 0.2, 12, 2023)$。\n\n你的程序必须硬编码这些案例，使用指定的种子以保证可复现性，并输出包含列表 $[\\widehat{p}_1, \\widehat{p}_2, \\widehat{p}_3, \\widehat{p}_4]$ 的单行，其中每个 $\\widehat{p}_k$ 是案例 $k$ 的四舍五入后的重要性抽样估计值。不允许用户输入。不涉及物理单位或角度单位。任何地方都不得使用百分比；所有输出均为实值浮点数。", "solution": "该问题经验证是自洽的、有科学依据且定义明确的。我们接下来提供解决方案，该方案包括两部分：交叉熵参数更新法则的理论推导和指定算法的实现。\n\n**第 1 部分：交叉熵更新法则的推导**\n\n在每次迭代 $t$ 中，交叉熵方法的目标是为我们的抽样分布 $g_\\mu(x)$ 找到一个参数 $\\mu$，以最小化从一个“理想”目标分布 $h_t(x)$ 到 $g_\\mu(x)$ 的 Kullback-Leibler (KL) 散度。\n\n如问题所述，在 $\\mu$ 上最小化 KL 散度 $D_{\\mathrm{KL}}(h_t \\,\\|\\, g_\\mu)$ 等价于最大化交叉熵项 $\\int h_t(x) \\log g_\\mu(x) \\, dx$。因此，参数更新由下式给出：\n$$ \\mu_{t+1} = \\arg\\max_{\\mu} \\int h_t(x) \\log g_\\mu(x) \\, dx $$\n\n在此问题中，我们从当前分布 $f_{\\mu_t}(x) = \\mathcal{N}(\\mu_t, 1)$ 中抽样。高性能区域由一个超出水平 $c_t$ 定义，理想分布 $h_t(x)$ 的密度正比于原始密度，但条件是处于此高性能区域内。即 $h_t(x) \\propto \\mathbb{1}\\{x \\ge c_t\\} f_{\\mu_t}(x)$。更正式地，其密度为：\n$$ h_t(x) = \\frac{\\mathbb{1}\\{x \\ge c_t\\} f_{\\mu_t}(x)}{\\int \\mathbb{1}\\{y \\ge c_t\\} f_{\\mu_t}(y) \\, dy} $$\n分母是关于 $x$ 的归一化常数，不依赖于待优化的参数 $\\mu$。因此，最大化交叉熵等价于最大化：\n$$ \\mathcal{L}(\\mu) = \\int \\mathbb{1}\\{x \\ge c_t\\} f_{\\mu_t}(x) \\log g_\\mu(x) \\, dx $$\n\n这个积分通常是难以计算的。交叉熵方法使用蒙特卡洛样本来近似它。我们从当前分布 $f_{\\mu_t}(x) = \\mathcal{N}(\\mu_t, 1)$ 中抽取 $n$ 个独立样本 $X_1, \\dots, X_n$。该目标的经验版本或基于样本的版本变为：\n$$ \\widehat{\\mathcal{L}}(\\mu) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{1}\\{X_i \\ge c_t\\} \\log g_\\mu(X_i) $$\n最大化此表达式等价于最大化不含 $\\frac{1}{n}$ 因子的求和项。指示函数 $\\mathbb{1}\\{X_i \\ge c_t\\}$ 仅选择属于精英集 $\\mathcal{E}_t = \\{i \\mid X_i \\ge c_t\\}$ 的样本。因此，下一个参数 $\\mu_{t+1}$ 的优化问题是：\n$$ \\mu_{t+1} = \\arg\\max_{\\mu} \\sum_{i \\in \\mathcal{E}_t} \\log g_\\mu(X_i) $$\n\n这个目标恰好是给定精英样本 $\\{X_i \\mid i \\in \\mathcal{E}_t\\}$ 作为观测数据时，分布族 $g_\\mu$ 的参数 $\\mu$ 的对数似然函数。因此，更新交叉熵参数的问题被简化为在精英集上的最大似然估计 (MLE) 问题。\n\n对于参数族 $g_\\mu(x) = \\mathcal{N}(\\mu, 1)$，其概率密度函数为：\n$$ g_\\mu(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left( -\\frac{(x-\\mu)^2}{2} \\right) $$\n相应的对数密度为：\n$$ \\log g_\\mu(x) = -\\frac{1}{2}\\log(2\\pi) - \\frac{(x-\\mu)^2}{2} $$\n精英样本的对数似然函数为：\n$$ L(\\mu; \\{X_i\\}_{i \\in \\mathcal{E}_t}) = \\sum_{i \\in \\mathcal{E}_t} \\log g_\\mu(X_i) = \\sum_{i \\in \\mathcal{E}_t} \\left( -\\frac{1}{2}\\log(2\\pi) - \\frac{(X_i-\\mu)^2}{2} \\right) $$\n为找到使 $L(\\mu)$ 最大化的 $\\mu$ 值，我们对其关于 $\\mu$ 求导并令其为零。不依赖于 $\\mu$ 的项可以忽略。\n$$ \\frac{\\partial L}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\sum_{i \\in \\mathcal{E}_t} \\left( - \\frac{(X_i-\\mu)^2}{2} \\right) = - \\sum_{i \\in \\mathcal{E}_t} \\frac{1}{2} \\cdot 2(X_i - \\mu) \\cdot (-1) = \\sum_{i \\in \\mathcal{E}_t} (X_i - \\mu) $$\n令导数为零以找到临界点：\n$$ \\sum_{i \\in \\mathcal{E}_t} (X_i - \\mu) = 0 $$\n$$ \\left(\\sum_{i \\in \\mathcal{E}_t} X_i\\right) - |\\mathcal{E}_t|\\mu = 0 $$\n$$ \\mu = \\frac{1}{|\\mathcal{E}_t|} \\sum_{i \\in \\mathcal{E}_t} X_i $$\n二阶导数 $\\frac{\\partial^2 L}{\\partial \\mu^2} = -|\\mathcal{E}_t|$ 是负的（假设精英集非空，即 $|\\mathcal{E}_t| > 0$），这证实了该临界点是一个最大值点。\n\n因此，对于具有固定单位方差的高斯族，其均值参数 $\\mu$ 的交叉熵更新法则是将新参数 $\\mu_{t+1}$ 设置为第 $t$ 次迭代中精英样本的样本均值。推导至此完成。\n\n**第 2 部分：算法说明和似然比**\n\n该算法按问题陈述中的规定实现。最终估计步骤的一个关键组成部分是重要性抽样权重，或称似然比 $\\frac{f(y)}{g_{\\mu^\\star}(y)}$。此处，$f$ 是标准正态分布 $\\mathcal{N}(0,1)$ 的密度，$g_{\\mu^\\star}$ 是重要性抽样分布 $\\mathcal{N}(\\mu^\\star, 1)$ 的密度。\n\n设 $Y_j$ 是从 $g_{\\mu^\\star}$ 中抽取的一个样本。其密度分别为：\n$$ f(Y_j) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{Y_j^2}{2}\\right) $$\n$$ g_{\\mu^\\star}(Y_j) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(Y_j-\\mu^\\star)^2}{2}\\right) $$\n似然比是它们的商：\n$$ \\frac{f(Y_j)}{g_{\\mu^\\star}(Y_j)} = \\frac{\\exp\\left(-\\frac{Y_j^2}{2}\\right)}{\\exp\\left(-\\frac{(Y_j-\\mu^\\star)^2}{2}\\right)} = \\exp\\left(-\\frac{Y_j^2}{2} - \\left(-\\frac{(Y_j-\\mu^\\star)^2}{2}\\right)\\right) $$\n$$ = \\exp\\left(\\frac{1}{2} \\left[ -Y_j^2 + (Y_j - \\mu^\\star)^2 \\right] \\right) $$\n$$ = \\exp\\left(\\frac{1}{2} \\left[ -Y_j^2 + Y_j^2 - 2Y_j\\mu^\\star + (\\mu^\\star)^2 \\right] \\right) $$\n$$ = \\exp\\left(\\frac{1}{2} \\left[ -2Y_j\\mu^\\star + (\\mu^\\star)^2 \\right] \\right) = \\exp\\left(-Y_j\\mu^\\star + \\frac{(\\mu^\\star)^2}{2}\\right) $$\n这就是用于实现最终重要性抽样估计器的闭式表达式：\n$$ \\widehat{p}(\\gamma) = \\frac{1}{n} \\sum_{j=1}^n \\mathbb{1}\\{Y_j \\ge \\gamma\\} \\exp\\left(-Y_j\\mu^\\star + \\frac{(\\mu^\\star)^2}{2}\\right) $$\n\n实现遵循了问题中列出的程序步骤，包括对空精英集的指定处理方式。", "answer": "```python\nimport numpy as np\n\ndef cross_entropy_is(gamma, n, rho, T, s):\n    \"\"\"\n    Estimates P(X >= gamma) for X ~ N(0,1) using the cross-entropy method.\n\n    Args:\n        gamma (float): The exceedance threshold.\n        n (int): The sample size per iteration.\n        rho (float): The elite fraction (0  rho  1).\n        T (int): The number of iterations.\n        s (int): The random seed.\n\n    Returns:\n        float: The importance sampling estimate of the probability.\n    \"\"\"\n    rng = np.random.default_rng(s)\n    \n    # Initialization\n    mu_t = 0.0\n    \n    # Iterative update of the mean parameter\n    for _ in range(T):\n        # Draw samples from the current proposal distribution N(mu_t, 1)\n        samples = rng.normal(loc=mu_t, scale=1.0, size=n)\n        \n        # Determine the level for the elite set\n        q_t = np.quantile(samples, 1 - rho)\n        c_t = min(gamma, q_t)\n        \n        # Identify the elite samples\n        elite_samples = samples[samples >= c_t]\n        \n        # Update mu for the next iteration\n        if elite_samples.size == 0:\n            # Fallback as specified: use the maximum of the current samples\n            mu_t_plus_1 = np.max(samples)\n        else:\n            # Update mu to be the mean of the elite samples\n            mu_t_plus_1 = np.mean(elite_samples)\n            \n        mu_t = mu_t_plus_1\n\n    # Final parameter for importance sampling\n    mu_star = mu_t\n    \n    # Final estimation using importance sampling\n    # Draw a fresh set of n samples from the final proposal N(mu_star, 1)\n    y_samples = rng.normal(loc=mu_star, scale=1.0, size=n)\n    \n    # Calculate the likelihood ratio f(Y)/g_mu*(Y)\n    # f is N(0,1) density, g_mu* is N(mu_star,1) density.\n    # The ratio simplifies to exp(-mu*Y + mu*^2/2)\n    likelihood_ratio = np.exp(-mu_star * y_samples + (mu_star**2) / 2.0)\n    \n    # Indicator function for the rare event {Y >= gamma}\n    indicators = (y_samples >= gamma)\n    \n    # Importance sampling estimator\n    p_hat = np.mean(likelihood_ratio * indicators)\n    \n    return p_hat\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (gamma, n, rho, T, s)\n        (2.0, 5000, 0.1, 7, 42),\n        (4.0, 8000, 0.2, 10, 314159),\n        (0.0, 4000, 0.1, 4, 7),\n        (5.5, 12000, 0.2, 12, 2023),\n    ]\n\n    results = []\n    for gamma, n, rho, T, s in test_cases:\n        p_estimate = cross_entropy_is(gamma, n, rho, T, s)\n        # Round to 12 decimal places as required.\n        # The result of round() is a float.\n        rounded_result = round(p_estimate, 12)\n        results.append(rounded_result)\n\n    # Print the final output in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3351653"}, {"introduction": "在掌握了一维情形后，我们将挑战扩展到更贴近实际应用的多维问题。这个练习探讨了在高维设定中使用对角协方差矩阵的交叉熵方法，这是一种在参数数量和模型灵活性之间进行权衡的常用技术。通过推导其参数更新规则并分析其内在的偏倚-方差权衡[@problem_id:3351734]，您将深入理解在处理复杂问题时进行模型简化的必要性及其对算法性能的影响。", "problem": "考虑用于罕见事件模拟和优化的交叉熵 (CE) 方法，该方法旨在寻找一个参数化的提议分布 $g_{\\theta}$，以最小化与理想零方差分布之间的 Kullback–Leibler (KL) 散度。设提议分布族为多元高斯分布，其参数为 $\\theta = (\\mu, \\Sigma)$，其中 $\\mu \\in \\mathbb{R}^{d}$ 且 $\\Sigma \\in \\mathbb{R}^{d \\times d}$。在第 $t$ 次迭代中，给定 $n$ 个独立样本 $\\{x_{i}\\}_{i=1}^{n}$ 和满足 $\\sum_{i=1}^{n} w_{i} = 1$ 的非负权重 $\\{w_{i}\\}_{i=1}^{n}$（例如，归一化的精英权重），CE 更新通过最大化加权对数似然 $\\sum_{i=1}^{n} w_{i} \\ln g_{\\mu,\\Sigma}(x_{i})$ 来选择 $(\\mu^{(t+1)}, \\Sigma^{(t+1)})$，此步骤之后可以选择性地跟随一个平滑步骤，该步骤使用系数 $\\alpha \\in (0, 1]$ 与先前的参数形成凸组合。假设在所有迭代中，协方差矩阵被限制为对角矩阵，即 $\\Sigma = \\operatorname{diag}(\\sigma_{1}^{2}, \\dots, \\sigma_{d}^{2})$。从 KL 散度和最大似然估计的基本定义出发，推导在此限制下对角协方差项的闭式 CE 更新规则，并包含平滑步骤。\n\n在您的推导中，将 CE 目标视为具有对角协方差的多元高斯分布的加权最大似然问题，并明确展示 $\\Sigma$ 为对角矩阵的限制如何改变优化过程。对于向量 $v \\in \\mathbb{R}^{d}$，将 $\\operatorname{diag}(v)$ 定义为对角线上为 $v$ 的对角矩阵；对于方阵 $A \\in \\mathbb{R}^{d \\times d}$，将 $\\operatorname{diag}(A)$ 定义为仅保留 $A$ 的对角线元素所形成的对角矩阵。您可以假设标准的正则性条件成立，从而允许交换微分和求和的顺序。\n\n从第一性原理出发，讨论在高维设置（$d$ 相对于 $n$ 较大）中将 $\\Sigma$ 限制为对角矩阵所引入的偏差-方差权衡。从参数数量和估计稳定性的角度进行量化讨论，并解释这种权衡如何影响 CE 算法逼近理想分布的能力以及罕见事件估计的效率。\n\n将您的最终答案表示为平滑后的对角协方差更新 $\\Sigma^{(t+1)}$ 的单一闭式解析矩阵表达式，该表达式应使用 $n$, $d$, $w_{i}$, $x_{i}$, $\\mu^{(t+1)}$, $\\alpha$ 和 $\\Sigma^{(t)}$ 来表示。无需四舍五入。", "solution": "交叉熵 (CE) 方法旨在通过最小化与理想零方差重要性采样分布之间的 Kullback-Leibler (KL) 散度，来找到一个最优的参数化提议分布 $g_{\\theta}$。这等同于最大化精英样本的加权对数似然。在这里，我们推导多元高斯提议分布 $g_{\\mu, \\Sigma}(x)$ 的更新规则，其中协方差矩阵 $\\Sigma$ 被限制为对角矩阵。\n\n### 对角协方差更新的推导\n\n目标是关于参数 $(\\mu, \\Sigma)$，在给定样本 $\\{x_i\\}$ 和权重 $\\{w_i\\}$ 的情况下，最大化加权对数似然函数 $L(\\mu, \\Sigma) = \\sum_{i=1}^{n} w_i \\ln g_{\\mu, \\Sigma}(x_i)$。\n\n多元高斯分布 $N(\\mu, \\Sigma)$ 的概率密度函数 (PDF) 为：\n$$\ng_{\\mu, \\Sigma}(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right)\n$$\n在 $\\Sigma$ 为对角矩阵的限制下，即 $\\Sigma = \\operatorname{diag}(\\sigma_1^2, \\dots, \\sigma_d^2)$，其行列式为 $|\\Sigma| = \\prod_{j=1}^d \\sigma_j^2$，其逆矩阵为 $\\Sigma^{-1} = \\operatorname{diag}(\\sigma_1^{-2}, \\dots, \\sigma_d^{-2})$。对数 PDF 变为：\n$$\n\\ln g_{\\mu, \\Sigma}(x) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\sum_{j=1}^d \\ln(\\sigma_j^2) - \\frac{1}{2}\\sum_{j=1}^d \\frac{(x_j - \\mu_j)^2}{\\sigma_j^2}\n$$\n加权对数似然则为：\n$$\nL(\\mu, \\Sigma) = \\sum_{i=1}^n w_i \\left( -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\sum_{j=1}^d \\ln(\\sigma_j^2) - \\frac{1}{2}\\sum_{j=1}^d \\frac{(x_{ij} - \\mu_j)^2}{\\sigma_j^2} \\right)\n$$\n由于权重是归一化的，$\\sum_{i=1}^n w_i = 1$，表达式得以简化。我们可以对每个分量 $j$ 分别进行优化。\n$$\nL(\\mu, \\Sigma) = C - \\frac{1}{2}\\sum_{j=1}^d \\ln(\\sigma_j^2) - \\frac{1}{2}\\sum_{j=1}^d \\frac{1}{\\sigma_j^2} \\sum_{i=1}^n w_i (x_{ij} - \\mu_j)^2\n$$\n为找到最大化 $L$ 的参数，我们求偏导数。均值的标准更新是加权样本均值：\n$$\n\\mu^{(t+1)}_{\\text{raw}} = \\sum_{i=1}^{n} w_i x_i\n$$\n根据问题陈述，我们使用最终平滑后的均值 $\\mu^{(t+1)}$ 来推导 $\\Sigma$ 的更新。我们对每个对角方差项 $\\sigma_j^2$ 最大化 $L$。设 $v_j = \\sigma_j^2$：\n$$\n\\frac{\\partial L}{\\partial v_j} = -\\frac{1}{2v_j} + \\frac{1}{2v_j^2} \\sum_{i=1}^n w_i (x_{ij} - \\mu_j^{(t+1)})^2 = 0\n$$\n将导数设为零并求解 $v_j = \\sigma_j^2$ 可得：\n$$\n(\\sigma_j^2)_{\\text{raw}} = \\sum_{i=1}^{n} w_i (x_{ij} - \\mu_j^{(t+1)})^2\n$$\n这是第 $j$ 个分量的加权样本方差，围绕均值 $\\mu^{(t+1)}$ 计算。以矩阵形式表示，未平滑的对角协方差更新 $\\Sigma^{(t+1)}_{\\text{raw}}$ 是通过取加权样本协方差矩阵的对角线得到的：\n$$\n\\Sigma^{(t+1)}_{\\text{raw}} = \\operatorname{diag}\\left( \\sum_{i=1}^{n} w_i (x_i - \\mu^{(t+1)})(x_i - \\mu^{(t+1)})^T \\right)\n$$\n最后，我们加入带有系数 $\\alpha$ 的平滑步骤：\n$$\n\\Sigma^{(t+1)} = \\alpha \\Sigma^{(t+1)}_{\\text{raw}} + (1-\\alpha) \\Sigma^{(t)}\n$$\n代入 $\\Sigma^{(t+1)}_{\\text{raw}}$ 的表达式，我们得到最终的更新规则。\n\n### 偏差-方差权衡讨论\n\n将协方差矩阵 $\\Sigma$ 限制为对角矩阵是一个关键的建模选择，它直接影响偏差-方差权衡，尤其是在高维设置（$d$ 很大）中。\n\n**参数数量与估计稳定性（方差减小）**：\n一个完整的协方差矩阵 $\\Sigma \\in \\mathbb{R}^{d \\times d}$ 有 $d(d+1)/2$ 个自由参数。要稳定地估计如此多的参数，需要大量的样本（$n \\gg d^2$）。当 $n$ 不够大时，样本协方差矩阵具有高方差，并且可能是病态的或奇异的。\n\n通过将 $\\Sigma$ 限制为对角矩阵，我们将待估计的参数数量从 $O(d^2)$ 减少到仅 $d$ 个（方差 $\\sigma_j^2$）。每个方差 $\\sigma_j^2$ 仅使用数据的第 $j$ 个分量独立估计。这极大地减少了获得稳定估计所需的样本数量，从而降低了参数估计的方差。这在高维情况下是一个显著的优势，因为在高维情况下 $n$ 通常与 $d$ 相当甚至更小。\n\n**模型表达能力（偏差引入）**：\n对角矩阵的限制施加了一个强假设：在提议分布中，变量之间是不相关的。CE 方法试图逼近的理想零方差分布可能具有复杂的相关性结构。通过强制协方差为对角矩阵，我们引入了结构性偏差。提议分布族可能不包含理想分布的良好近似。即使是最好的对角协方差高斯分布，在 KL 散度方面也可能远非最优，导致最终的罕见事件概率估计器的方差高于必要水平。\n\n**对 CE 算法效率的影响**：\n这种权衡在于迭代更新的稳定性与最终提议分布的渐近质量之间。一个完整的协方差矩阵提供较低的偏差，如果能够准确估计，则可能产生更高效的重要性采样器（更低的方差）。然而，在典型的 CE 设置中，精英样本数量 $n$ 较小，完整协方差估计的高方差会使算法不稳定，参数在迭代之间剧烈波动或收敛到较差的局部最优解。\n\n对角假设虽然有偏差，但提供了正则化。它确保每次迭代时参数更新的稳定性和鲁棒性，防止对小的精英集过拟合。这通常会带来更可靠（尽管可能是次优的）CE 算法收敛。对于许多高维问题，这种权衡是有利的，因为估计稳定性对于算法的正常运行至关重要。", "answer": "$$\n\\boxed{\\alpha \\operatorname{diag}\\left( \\sum_{i=1}^{n} w_i (x_i - \\mu^{(t+1)})(x_i - \\mu^{(t+1)})^T \\right) + (1-\\alpha) \\Sigma^{(t)}}\n$$", "id": "3351734"}, {"introduction": "现实世界中的许多稀有事件并非只有单一的发生路径，这要求我们的采样分布具备捕捉多个模式的能力。本练习将引导您探索一种更强大的建模方法：使用高斯混合模型（GMM）作为交叉熵方法的参数化族。您将推导在精英样本集上使用期望最大化（EM）算法来更新GMM参数的完整过程[@problem_id:3351674]，这对于解决具有复杂多模态特征的稀有事件估计和优化问题至关重要。", "problem": "考虑用于稀有事件模拟和连续优化的交叉熵方法 (CEM)，该方法通过迭代更新参数化采样分布，将概率质量集中在高性能区域。设在第 $t$ 次迭代时的采样族为一个具有参数 $\\Theta^{(t)}=\\{\\pi_{k}^{(t)},\\mu_{k}^{(t)},\\Sigma_{k}^{(t)}\\}_{k=1}^{K}$ 和密度\n$$\nq_{\\Theta^{(t)}}(x)=\\sum_{k=1}^{K}\\pi_{k}^{(t)}\\,\\mathcal{N}\\!\\left(x\\mid \\mu_{k}^{(t)},\\Sigma_{k}^{(t)}\\right),\n$$\n的 $K$ 分量高斯混合模型，其中 $\\sum_{k=1}^{K}\\pi_{k}^{(t)}=1$，对所有 $k$ 都有 $\\pi_{k}^{(t)}\\geq 0$，且每个 $\\Sigma_{k}^{(t)}$ 都是对称正定的。从 $q_{\\Theta^{(t)}}$ 中抽取 $n$ 个独立样本 $\\{X_{i}\\}_{i=1}^{n}$，评估其性能得分 $S(X_{i})$，并通过选择得分最高的 $m$ 个索引来构建基数为 $|\\mathcal{E}_{t}|=m$ 的精英集 $\\mathcal{E}_{t}\\subset\\{1,\\dots,n\\}$（等价于，对于一个定义了前 $\\rho$ 分位数的自适应阈值 $\\gamma_{t}$，选择所有满足 $S(X_{i})\\geq \\gamma_{t}$ 的索引 $i$，其中 $\\rho\\in(0,1)$ 且 $m=\\lfloor \\rho n\\rfloor$）。定义均匀精英权重，如果 $i\\in\\mathcal{E}_{t}$ 则 $w_{i}=m^{-1}$，否则 $w_{i}=0$。下一次迭代的参数 $\\Theta^{(t+1)}$ 是通过最大化精英加权的对数似然函数\n$$\n\\mathcal{L}(\\Theta)=\\sum_{i=1}^{n}w_{i}\\,\\ln\\!\\Bigg(\\sum_{k=1}^{K}\\pi_{k}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k},\\Sigma_{k}\\right)\\Bigg)\n$$\n得到的，该最大化过程针对混合参数 $\\{\\pi_{k},\\mu_{k},\\Sigma_{k}\\}_{k=1}^{K}$，并服从约束条件 $\\sum_{k=1}^{K}\\pi_{k}=1$、$\\pi_{k}\\geq 0$ 和 $\\Sigma_{k}$ 的正定性。\n\n从混合模型的标准潜变量表示和期望最大化 (EM) 算法出发，推导对于 $i\\in\\mathcal{E}_{t}$ 的精英加权责任度，以及混合权重、分量均值和分量协方差的相应最大化器（M步更新）。将每个所求的量明确地表示为关于 $\\{X_{i}\\}_{i\\in\\mathcal{E}_{t}}$、$\\{\\pi_{k}^{(t)},\\mu_{k}^{(t)},\\Sigma_{k}^{(t)}\\}_{k=1}^{K}$ 和 $K$ 的解析表达式，除了上面定义的精英加权外，不引入任何其他近似。\n\n你的最终答案必须包含以下四个按顺序排列的表达式：\n$($i$)$ 对于 $i\\in\\mathcal{E}_{t}$ 的责任度，\n$($ii$)$ 更新后的混合权重，\n$($iii$)$ 更新后的分量均值，以及\n$($iv$)$ 更新后的分量协方差，\n并写成一个单行矩阵。无需进行数值计算。", "solution": "用户希望在交叉熵方法 (CEM) 的背景下，找到高斯混合模型 (GMM) 参数的M步更新规则。这些更新是通过使用期望最大化 (EM) 算法最大化加权对数似然函数来推导的。\n\n首先，问题经验证是有效的。这是一个计算统计学和机器学习领域的良定问题，其基础是成熟的数学原理。所有必要的信息都已提供，且定义是一致的。\n\n目标是找到参数 $\\Theta = \\{\\pi_{k}, \\mu_{k}, \\Sigma_{k}\\}_{k=1}^{K}$，以最大化精英加权的对数似然函数：\n$$\n\\mathcal{L}(\\Theta)=\\sum_{i=1}^{n}w_{i}\\,\\ln\\!\\Bigg(\\sum_{k=1}^{K}\\pi_{k}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k},\\Sigma_{k}\\right)\\Bigg)\n$$\n对于精英集 $\\mathcal{E}_{t}$中的索引 $i$（其中 $m = |\\mathcal{E}_t|$），权重定义为 $w_{i}=m^{-1}$，否则 $w_{i}=0$。因此，求和可以限制在精英集内：\n$$\n\\mathcal{L}(\\Theta)=\\frac{1}{m}\\sum_{i\\in\\mathcal{E}_{t}}\\ln\\!\\Bigg(\\sum_{k=1}^{K}\\pi_{k}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k},\\Sigma_{k}\\right)\\Bigg)\n$$\n最大化 $\\mathcal{L}(\\Theta)$ 等价于最大化去掉常数因子 $1/m$ 的求和项。设EM算法的目标函数为精英数据的对数似然：\n$$\n\\mathcal{L}_{\\mathcal{E}}(\\Theta) = \\sum_{i\\in\\mathcal{E}_{t}}\\ln\\!\\Bigg(\\sum_{k=1}^{K}\\pi_{k}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k},\\Sigma_{k}\\right)\\Bigg)\n$$\n我们使用EM算法来最大化此函数。该算法引入潜变量 $Z_{i}$，其中 $Z_i=k$ 表示样本 $X_i$ 是由混合模型的第 $k$ 个分量生成的。\n\n**E步（期望步）**\n\n在E步中，我们计算完全数据对数似然的期望，该期望以观测数据 $\\{X_i\\}_{i\\in\\mathcal{E}_t}$ 和当前参数估计 $\\Theta^{(t)} = \\{\\pi_{k}^{(t)}, \\mu_{k}^{(t)}, \\Sigma_{k}^{(t)}\\}_{k=1}^{K}$ 为条件。这个期望就是Q函数 $Q(\\Theta|\\Theta^{(t)})$。\n\n需要计算的核心量是样本 $X_i$ 属于分量 $k$ 的后验概率，这被称为责任度，我们用 $\\gamma_{ik}$ 表示。这个量代表 $P(Z_i=k \\mid X_i, \\Theta^{(t)})$。对于任何 $i \\in \\mathcal{E}_t$ 的样本 $X_i$，使用贝叶斯定理：\n$$\n\\gamma_{ik} = \\frac{P(X_i \\mid Z_i=k, \\Theta^{(t)}) P(Z_i=k \\mid \\Theta^{(t)})}{P(X_i \\mid \\Theta^{(t)})} = \\frac{\\pi_{k}^{(t)}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k}^{(t)},\\Sigma_{k}^{(t)}\\right)}{\\sum_{j=1}^{K}\\pi_{j}^{(t)}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{j}^{(t)},\\Sigma_{j}^{(t)}\\right)}\n$$\n该表达式是第 (i) 部分“精英加权的责任度”的答案。这里的术语“精英加权”意味着这些责任度是为精英集中的样本计算的，似然函数也是在这些样本上定义的。\n\n在M步中需要最大化的Q函数是：\n$$\nQ(\\Theta|\\Theta^{(t)}) = \\sum_{i\\in\\mathcal{E}_{t}}\\sum_{k=1}^{K} \\gamma_{ik} \\ln\\left( \\pi_k \\mathcal{N}(X_i | \\mu_k, \\Sigma_k) \\right)\n$$\n$$\nQ(\\Theta|\\Theta^{(t)}) = \\sum_{i\\in\\mathcal{E}_{t}}\\sum_{k=1}^{K} \\gamma_{ik} \\ln(\\pi_k) + \\sum_{i\\in\\mathcal{E}_{t}}\\sum_{k=1}^{K} \\gamma_{ik} \\ln(\\mathcal{N}(X_i | \\mu_k, \\Sigma_k))\n$$\n\n**M步（最大化步）**\n\n在M步中，我们找到参数 $\\Theta^{(t+1)}$，它能最大化关于 $\\Theta = \\{\\pi_k, \\mu_k, \\Sigma_k\\}$ 的 $Q(\\Theta|\\Theta^{(t)})$。\n\n**1. 混合权重 ($\\pi_k$) 的更新**\n\n我们在约束 $\\sum_{k=1}^{K} \\pi_k = 1$ 下最大化 $\\sum_{i\\in\\mathcal{E}_{t}}\\sum_{k=1}^{K} \\gamma_{ik} \\ln(\\pi_k)$。使用拉格朗日乘子 $\\lambda$，我们得到：\n$$\n\\Lambda = \\sum_{i\\in\\mathcal{E}_{t}}\\sum_{k=1}^{K} \\gamma_{ik} \\ln(\\pi_k) + \\lambda\\left(\\sum_{k=1}^{K}\\pi_k - 1\\right)\n$$\n对 $\\pi_k$ 求导并令其为零，得出：\n$$\n\\frac{\\partial \\Lambda}{\\partial \\pi_k} = \\frac{1}{\\pi_k}\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} + \\lambda = 0 \\implies \\pi_k = -\\frac{1}{\\lambda} \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}\n$$\n对 $k$ 求和并使用约束 $\\sum_k \\pi_k=1$：\n$$\n1 = -\\frac{1}{\\lambda} \\sum_{k=1}^{K}\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} = -\\frac{1}{\\lambda} \\sum_{i\\in\\mathcal{E}_{t}}\\sum_{k=1}^{K} \\gamma_{ik}\n$$\n由于对于每个 $i$，都有 $\\sum_{k=1}^K \\gamma_{ik} = 1$，所以求和变为 $\\sum_{i\\in\\mathcal{E}_{t}} 1 = m$。因此，$1 = -m/\\lambda$，这意味着 $\\lambda = -m$。\n将 $\\lambda$ 代回，我们得到更新后的混合权重：\n$$\n\\pi_{k}^{(t+1)} = \\frac{1}{m}\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}\n$$\n这是第 (ii) 部分的表达式。\n\n**2. 分量均值 ($\\mu_k$) 的更新**\n\n我们最大化Q函数中依赖于 $\\mu_k$ 的部分：\n$$\n\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} \\ln(\\mathcal{N}(X_i | \\mu_k, \\Sigma_k)) = \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} \\left(-\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\Sigma_k| - \\frac{1}{2}(X_i-\\mu_k)^T\\Sigma_k^{-1}(X_i-\\mu_k)\\right)\n$$\n为了关于 $\\mu_k$ 最大化此式，我们只需要最小化 $\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} (X_i-\\mu_k)^T\\Sigma_k^{-1}(X_i-\\mu_k)$。对 $\\mu_k$ 求导并令其为零：\n$$\n\\frac{\\partial}{\\partial \\mu_k} \\left( \\dots \\right) = \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} (2\\Sigma_k^{-1}(X_i - \\mu_k)) = 0\n$$\n$$\n\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} (X_i - \\mu_k) = 0 \\implies \\left(\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}\\right)\\mu_k = \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} X_i\n$$\n分量 $k$ 的均值的更新公式为：\n$$\n\\mu_{k}^{(t+1)} = \\frac{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} X_i}{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}}\n$$\n这是第 (iii) 部分的表达式。\n\n**3. 分量协方差 ($\\Sigma_k$) 的更新**\n\n我们使用新的均值 $\\mu_{k}^{(t+1)}$，关于 $\\Sigma_k$ 最大化Q函数：\n$$\n\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} \\left(-\\frac{1}{2}\\ln|\\Sigma_k| - \\frac{1}{2}(X_i-\\mu_k^{(t+1)})^T\\Sigma_k^{-1}(X_i-\\mu_k^{(t+1)})\\right)\n$$\n令 $N_k = \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}$。需要最大化的表达式为：\n$$\n-\\frac{N_k}{2}\\ln|\\Sigma_k| - \\frac{1}{2} \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} (X_i-\\mu_k^{(t+1)})^T\\Sigma_k^{-1}(X_i-\\mu_k^{(t+1)})\n$$\n这个标准最大化问题的解是加权样本协方差矩阵：\n$$\n\\Sigma_{k}^{(t+1)} = \\frac{1}{N_k} \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} (X_i - \\mu_{k}^{(t+1)})(X_i - \\mu_{k}^{(t+1)})^T\n$$\n将 $N_k$ 代回：\n$$\n\\Sigma_{k}^{(t+1)} = \\frac{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} (X_i - \\mu_{k}^{(t+1)})(X_i - \\mu_{k}^{(t+1)})^T}{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}}\n$$\n这是第 (iv) 部分的表达式。四个所求的表达式被收集在最终答案中。注意，第四个表达式中的 $\\mu_k^{(t+1)}$ 指的是第三个表达式的结果。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\pi_{k}^{(t)}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k}^{(t)},\\Sigma_{k}^{(t)}\\right)}{\\sum_{j=1}^{K}\\pi_{j}^{(t)}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{j}^{(t)},\\Sigma_{j}^{(t)}\\right)} \\\\\\\\\n\\frac{1}{m}\\sum_{i\\in\\mathcal{E}_{t}} \\frac{\\pi_{k}^{(t)}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k}^{(t)},\\Sigma_{k}^{(t)}\\right)}{\\sum_{j=1}^{K}\\pi_{j}^{(t)}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{j}^{(t)},\\Sigma_{j}^{(t)}\\right)} \\\\\\\\\n\\frac{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} X_i}{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}} \\\\\\\\\n\\frac{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} \\left(X_i - \\mu_{k}^{(t+1)}\\right)\\left(X_i - \\mu_{k}^{(t+1)}\\right)^T}{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}}\n\\end{pmatrix}\n}\n$$\n其中，第三和第四个元素中的 $\\gamma_{ik}$ 表示作为第一个元素给出的责任度表达式，第四个元素中的 $\\mu_{k}^{(t+1)}$ 表示作为第三个元素给出的更新后的均值表达式。", "id": "3351674"}]}