{"hands_on_practices": [{"introduction": "在构建晕表和归并树时，一个核心问题是如何界定一个天体的范围，特别是当它位于另一个更大天体的引力场中时。子晕并没有一个清晰的物理边界；它的大小实际上是由其宿主晕的潮汐力决定的。这个练习将指导你通过第一性原理推导出潮汐半径的解析表达式，这是理解子晕物理以及在数值模拟中定义其范围的基础。[@problem_id:3468967]", "problem": "一个质量为 $m$ 的子晕在一个半径为 $R$ 的圆形轨道上绕着一个宿主晕运动。假设宿主晕可以被视为质量为 $M$ 的点质量。推导子晕的潮汐半径 $r_t$ 的表达式。然后，计算当 $R=1$ Mpc，$M=10^{14} M_\\odot$，子晕质量 $m=10^{12} M_\\odot$ 时，$r_t$ 的值（以 Mpc 为单位）。", "solution": "潮汐半径 $r_t$ 是指在一个子晕中，其自身引力恰好能被宿主晕的潮汐力平衡的距离。对于一个质量为 $m$ 的子晕，在距离一个质量为 $M$ 的点质量宿主 $R$ 处以圆形轨道运行时，其潮汐半径可以通过以下经典公式近似：\n$$\nr_t \\approx R \\left( \\frac{m}{3M} \\right)^{1/3}\n$$\n这个公式源于对子晕周围旋转参考系中有效引力势的分析，并确定了内拉格朗日点（L1）的位置。\n\n现在，我们将给定的数值代入这个公式中进行计算：\n-   $R = 1$ Mpc\n-   $m = 10^{12} M_\\odot$\n-   $M = 10^{14} M_\\odot$\n\n$$\nr_t \\approx (1 \\, \\text{Mpc}) \\times \\left( \\frac{10^{12} \\, M_\\odot}{3 \\times 10^{14} \\, M_\\odot} \\right)^{1/3}\n$$\n$$\nr_t \\approx \\left( \\frac{1}{300} \\right)^{1/3} \\, \\text{Mpc}\n$$\n$$\nr_t \\approx (0.003333...)^{1/3} \\, \\text{Mpc} \\approx 0.14938 \\, \\text{Mpc}\n$$\n\n将结果四舍五入到四位小数，我们得到 $r_t \\approx 0.1494$ Mpc。", "answer": "$$\\boxed{0.1494}$$", "id": "3468967"}, {"introduction": "理解了子晕的物理边界后，我们转向在模拟数据中识别这些结构的实际挑战。一种常用的算法“朋友的朋友”(Friends-of-Friends, FOF) 仅根据空间距离连接粒子，有时会错误地将两个物理上独立但空间上靠近的晕“桥接”在一起。这个编程练习将带你直面这个问题，通过实现一个更先进的相空间分裂算法来修正这种桥接伪影，让你亲身体验如何提升晕表的准确性。[@problem_id:3468933]", "problem": "给定一个简化的数值宇宙学设定，以研究在存在朋友的朋友（Friends-of-Friends, FOF）桥接伪影的情况下，如何构建暗晕星表和并合树。朋友的朋友（FOF）算法定义为通过在位形空间中使用固定的连接长度将粒子连接成群组；任何分离距离小于连接长度 $l$ 的两个粒子都会被连接，并通过传递闭包定义一个单一的群组。在现实场景中，当分离链的距离小于 $l$ 时，即使两个不同暗晕的相空间分布是不同的，FOF也可能错误地将它们桥接在一起。您的任务是实现一个完整的、可运行的程序，该程序使用相空间分裂来检测和校正此类FOF桥接，并评估其对推断的并合率和质量增长历史的影响。\n\n基本基础和定义：\n- FOF算法使用连接长度 $l$ 通过位形空间连接来对粒子进行分组。\n- 在一个快照时间 $t$ 的暗晕星表由多个暗晕组成，每个暗晕都有一组粒子标识符，其质量等于分配给该暗晕的粒子质量之和。\n- 跨越连续快照的并合树通过共享的粒子标识符连接暗晕；当时间 $t$ 的多个前身暗晕对时间 $t+\\Delta t$ 的单个后代暗晕贡献的粒子超过一个最小比例时，就会发生并合事件。\n- 相空间分裂尝试通过在六维相空间 $(x,y,z,v_x,v_y,v_z)$ 中进行聚类，将单个FOF群组分解为多个暗晕。\n\n要实现的检测和校正方法：\n- 对于每个FOF群组，使用中位数绝对偏差对位置和速度进行逐维归一化，以形成无量纲坐标。尝试使用对归一化的 $(x,y,z,v_x,v_y,v_z)$ 数据进行确定性k-均值聚类，将其分裂成 $k=2$ 个相空间星团。定义星团分离度规\n$$\nB \\equiv \\frac{\\|\\boldsymbol{c}_1 - \\boldsymbol{c}_2\\|_2}{\\frac{1}{N}\\sum_{i=1}^{N}\\min_{j\\in\\{1,2\\}}\\|\\boldsymbol{x}_i - \\boldsymbol{c}_j\\|_2},\n$$\n其中 $\\boldsymbol{c}_1,\\boldsymbol{c}_2$ 是星团中心，$\\boldsymbol{x}_i$ 是粒子相空间坐标，而 $N$ 是群组的大小。如果 $B$ 超过阈值 $B_{\\mathrm{th}}$ 且两个星团的大小都至少为 $N$ 的指定最小分数，则将该群组分裂为两个暗晕；否则，保持该群组完整。此校正独立地应用于每个快照的朴素FOF星表。\n\n并合树构建：\n- 在连续的快照之间，对于 $t_{s+1}$ 时的每个后代暗晕，将其在 $t_s$ 时的主要前身定义为具有最大共享粒子标识符数量的暗晕。次要前身是 $t_s$ 时同样与后代暗晕共享粒子且超过最小共享比例 $f_{\\min}$ 的暗晕。如果存在至少一个与主要前身的质量比至少为 $\\mu_{\\min}$ 的次要前身，则计数一次在 $(t_s\\to t_{s+1})$ 的并合。\n- 并合率定义为总并合次数除以以吉年（Gyr）为单位的总时间间隔。\n- 质量增长历史是为最终时间的主要后代暗晕定义的。将其主要分支的前身向后追溯到 $t_0$，以计算质量历史 $M_{\\mathrm{inferred}}(t_s)$，其中 $s=0,1,\\dots,S$。将此与基准真相质量历史 $M_{\\mathrm{true}}(t_s)$（来自用于生成合成数据的已知真实暗晕标签）进行比较，使用平均相对误差\n$$\n\\epsilon = \\frac{1}{S+1}\\sum_{s=0}^{S}\\frac{|M_{\\mathrm{inferred}}(t_s) - M_{\\mathrm{true}}(t_s)|}{M_{\\mathrm{true}}(t_s)}.\n$$\n\n程序要求：\n- 为每个快照生成合成粒子数据。每个粒子都有一个唯一的标识符（在快照之间保持不变）、以兆秒差距每$h$（Mpc$/h$）为单位的位置 $(x,y,z)$，以及以千米每秒（km/s）为单位的速度 $(v_x,v_y,v_z)$。每个粒子都具有相同的质量 $m_p$，以太阳质量每$h$（$M_\\odot/h$）为单位。使用以暗晕中心为中心、具有指定离散度的正态分布来生成位置和速度。\n- 为每个测试用例构建三个星表：一个可能存在桥接的朴素FOF星表，一个使用相空间分裂的校正星表，以及一个使用已知真实标签的基准真相星表。\n- 构建并合树并计算：\n    1. 以 $\\mathrm{Gyr}^{-1}$ 为单位的并合率。\n    2. 最终主要分支的平均相对质量增长误差 $\\epsilon$。\n- 将校正带来的改进报告为两种度量的朴素误差与校正后误差之差：\n    - 并合率改进：$\\Delta r = r_{\\mathrm{naive}} - r_{\\mathrm{corrected}}$，单位为 $\\mathrm{Gyr}^{-1}$。\n    - 质量增长改进：$\\Delta \\epsilon = \\epsilon_{\\mathrm{naive}} - \\epsilon_{\\mathrm{corrected}}$（无量纲）。\n\n单位和输出格式：\n- 位置必须以 Mpc$/h$ 为单位，速度以 km/s 为单位，质量以 $M_\\odot/h$ 为单位，时间以 Gyr 为单位。报告的并合率必须以 $\\mathrm{Gyr}^{-1}$ 为单位，报告的质量增长误差是无量纲的。将所有报告的改进四舍五入到四位小数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，顺序为\n$[\\Delta r_1,\\Delta \\epsilon_1,\\Delta r_2,\\Delta \\epsilon_2,\\Delta r_3,\\Delta \\epsilon_3]$，\n其中下标表示测试用例索引。\n\n测试套件：\n实现以下三个测试用例，每个都由参数完全指定。请精确使用指定的值。\n\n- 测试用例1（理想情况）：\n    - 暗晕数量：$H=2$。\n    - 快照：$t=[0.0,1.0,2.0]$ Gyr。\n    - 连接长度：$l=0.2$ Mpc$/h$。\n    - 粒子质量：$m_p=10^9$ $M_\\odot/h$。\n    - 每个暗晕的粒子数：$N=200$。\n    - 位置离散度：$\\sigma_x=\\sigma_y=\\sigma_z=0.05$ Mpc$/h$。\n    - 速度离散度：$\\sigma_{v_x}=\\sigma_{v_y}=\\sigma_{v_z}=30$ km/s。\n    - 暗晕A沿 $x$ 轴的中心：对所有 $s$，$x_A(t_s)=0.0$ Mpc$/h$；暗晕B：$x_B(t_0)=0.5$ Mpc$/h$，$x_B(t_1)=0.3$ Mpc$/h$，$x_B(t_2)=0.1$ Mpc$/h$。\n    - 速度中心：暗晕A有 $v_{x,A}=+100$ km/s，暗晕B有 $v_{x,B}=-100$ km/s，其他速度分量为零。\n    - 在快照 $t_2$ 应用朴素FOF桥接（FOF星表在 $t_2$ 将两个暗晕合并为一个群组）；其他快照使用真实标签。\n    - 相空间分裂阈值：$B_{\\mathrm{th}}=2.5$，最小星团分数：$0.2$，最小共享分数：$f_{\\min}=0.2$，最小质量比：$\\mu_{\\min}=0.25$。\n    - 随机种子：$123$。\n\n- 测试用例2（接近检测阈值的边界情况）：\n    - 暗晕数量：$H=2$。\n    - 快照：$t=[0.0,1.0,2.0]$ Gyr。\n    - 连接长度：$l=0.2$ Mpc$/h$。\n    - 粒子质量：$m_p=10^9$ $M_\\odot/h$。\n    - 每个暗晕的粒子数：$N=150$。\n    - 位置离散度：$\\sigma_x=\\sigma_y=\\sigma_z=0.06$ Mpc$/h$。\n    - 速度离散度：$\\sigma_{v_x}=\\sigma_{v_y}=\\sigma_{v_z}=50$ km/s。\n    - 暗晕A沿 $x$ 轴的中心：$x_A(t_s)=0.0$ Mpc$/h$；暗晕B：$x_B(t_0)=0.4$ Mpc$/h$，$x_B(t_1)=0.25$ Mpc$/h$，$x_B(t_2)=0.18$ Mpc$/h$。\n    - 速度中心：暗晕A有 $v_{x,A}=+30$ km/s，暗晕B有 $v_{x,B}=-20$ km/s，其他速度分量为零。\n    - 在快照 $t_2$ 应用朴素FOF桥接；其他快照使用真实标签。\n    - 相空间分裂阈值：$B_{\\mathrm{th}}=2.5$，最小星团分数：$0.2$，最小共享分数：$f_{\\min}=0.2$，最小质量比：$\\mu_{\\min}=0.25$。\n    - 随机种子：$456$。\n\n- 测试用例3（无桥接的边缘情况）：\n    - 暗晕数量：$H=1$。\n    - 快照：$t=[0.0,1.0,2.0]$ Gyr。\n    - 连接长度：$l=0.2$ Mpc$/h$。\n    - 粒子质量：$m_p=10^9$ $M_\\odot/h$。\n    - 每个暗晕的粒子数：$N=250$。\n    - 位置离散度：$\\sigma_x=\\sigma_y=\\sigma_z=0.05$ Mpc$/h$。\n    - 速度离散度：$\\sigma_{v_x}=\\sigma_{v_y}=\\sigma_{v_z}=30$ km/s。\n    - 暗晕A沿 $x$ 轴的中心：对所有 $s$，$x_A(t_s)=0.0$ Mpc$/h$；速度中心在 $v_{x,A}=+50$ km/s，其他分量为零。\n    - 从不应用朴素FOF桥接。\n    - 相空间分裂阈值：$B_{\\mathrm{th}}=2.5$，最小星团分数：$0.2$，最小共享分数：$f_{\\min}=0.2$，最小质量比：$\\mu_{\\min}=0.25$。\n    - 随机种子：$789$。\n\n您的程序必须构建三个星表（朴素FOF、校正后和基准真相），为每个星表构建并合树，计算以 $\\mathrm{Gyr}^{-1}$ 为单位的并合率和平均相对质量增长误差 $\\epsilon$，然后按指定格式输出每个测试用例的改进量 $\\Delta r$ 和 $\\Delta \\epsilon$，四舍五入到四位小数，在一行中显示：“[r1,r2,r3,r4,r5,r6]”。", "solution": "该问题要求实现并评估一种相空间分裂算法，以校正朋友的朋友（Friends-of-Friends, FOF）暗晕星表中的桥接伪影。我们将构建一个完整的程序来生成合成粒子数据、应用校正、构建并合树，并量化在并合率和质量增长历史估计方面的改进。解决方案按几个逻辑上不同的步骤进行：数据生成、星表构建（基准真相、朴素和校正后）、并合树分析和度量计算。\n\n**1. 合成数据生成**\n对于每个测试用例和快照，我们生成一个粒子分布。每个粒子由一个唯一的标识符、一个三维位置向量 $\\boldsymbol{x}=(x,y,z)$、一个三维速度向量 $\\boldsymbol{v}=(v_x,v_y,v_z)$ 和一个恒定的质量 $m_p$ 定义。生成一组 $H$ 个暗晕，每个暗晕具有指定数量的粒子 $N_h$。给定暗晕的粒子位置和速度是从以该暗晕的相空间中心 $(\\boldsymbol{x}_c(t), \\boldsymbol{v}_c(t))$ 为中心、具有指定离散度 $(\\sigma_x, \\sigma_y, \\sigma_z)$ 和 $(\\sigma_{vx}, \\sigma_{vy}, \\sigma_{vz})$ 的三维高斯分布中抽取的。每个粒子都被分配一个真实的暗晕ID，用于构建基准真相星表。为每个测试用例提供的随机种子确保了可复现性。\n\n**2. 星表构建**\n对于每个快照，我们构建三种类型的暗晕星表：\n- **基准真相星表**：通过将在数据生成步骤中共享相同真实暗晕ID的所有粒子分组来形成暗晕。这代表了理想的、被完美识别的暗晕集合。\n- **朴素FOF星表**：此星表根据每个测试用例的具体说明构建，以模拟FOF桥接伪影。对于测试用例1和2，快照 $t_0$ 和 $t_1$ 的星表与基准真相相同，而在快照 $t_2$ 时，所有粒子被合并成一个单一的、巨大的暗晕，以表示一次灾难性的桥接事件。对于没有桥接的测试用例3，朴素星表在所有快照中都与基准真相星表相同。\n- **校正后星表**：此星表通过对朴素FOF星表中的每个暗晕应用分裂算法来生成。这是问题的核心。\n\n**2.1. 相空间分裂算法**\n对于朴素星表中的每个群组（暗晕），我们判断它是否是需要被分裂的桥接系统。\n1.  **归一化**：群组中所有粒子的六维相空间坐标 $(\\boldsymbol{x}, \\boldsymbol{v})$ 进行逐维归一化。归一化至关重要，它使得具有不同物理单位的量（例如 Mpc/$h$ vs. km/s）在相空间中进行距离计算时具有可比性。我们使用中位数绝对偏差（MAD），一种稳健的统计度量，进行缩放。对于每个维度 $j \\in \\{1, \\dots, 6\\}$，粒子 $i$ 的归一化坐标 $p'_{i,j}$ 为：\n    $$\n    p'_{i,j} = \\frac{p_{i,j} - \\text{median}(p_j)}{\\text{MAD}(p_j)}\n    $$\n    其中 $p_j$ 是维度 $j$ 中所有粒子坐标的向量，$\\text{MAD}(p_j) = \\text{median}(|p_j - \\text{median}(p_j)|)$。如果 $\\text{MAD}(p_j)$ 为零，则所有粒子的该归一化维度都设置为零。\n2.  **聚类**：我们在六维归一化相空间数据上执行确定性的k-均值聚类，其中 $k=2$。k-均值算法迭代地将粒子划分为两个簇，以最小化簇内平方和。为确保确定性，在从粒子集中随机选择初始两个簇质心之前，使用测试用例的种子来初始化随机数生成器。\n3.  **分裂准则**：在k-均值算法收敛后，我们评估这次分裂是物理上有意义的，还是聚类本身造成的伪影。一个桥接的暗晕应该对应于相空间中两个分离良好的簇。我们计算簇分离度规 $B$：\n    $$\n    B \\equiv \\frac{\\|\\boldsymbol{c}_1 - \\boldsymbol{c}_2\\|_2}{\\frac{1}{N}\\sum_{i=1}^{N}\\min_{j\\in\\{1,2\\}}\\|\\boldsymbol{x}_i - \\boldsymbol{c}_j\\|_2}\n    $$\n    其中 $\\boldsymbol{c}_1, \\boldsymbol{c}_2$ 是两个结果簇的六维质心，$\\boldsymbol{x}_i$ 是六维归一化粒子坐标，$N$ 是群组中的总粒子数。$B$ 值较大表示簇中心之间的距离相对于平均簇内粒子到中心的距离要大。\n4.  **决策**：当且仅当满足两个条件时，原始FOF群组才被分裂成两个新的暗晕：\n    - 分离度规 $B$ 超过给定的阈值 $B_{\\mathrm{th}}$。\n    - 两个簇都足够大，即它们的粒子数不少于原始群组总粒子数 $N$ 的一个最小分数。\n    如果不满足这些条件，该群组在校正后星表中仍保持为单个暗晕。\n\n**3. 并合树构建和度量**\n利用每个快照的星表，我们构建并合树来分析宇宙演化。\n- **树链接**：对于快照 $t_{s+1}$ 时的后代暗晕 $D$，其在 $t_s$ 时的主要前身是与 $D$ 共享粒子数最多的暗晕 $P_{\\text{main}}$。在 $t_s$ 时的任何其他暗晕 $P$ 都是一个潜在的次要前身。\n- **并合率 ($r$)**：如果一个后代暗晕 $D$ 至少有一个次要前身 $P_{\\text{sec}}$ 满足以下两个条件，则为其计数一次并合事件：\n    1.  其粒子的一个显著部分贡献给了后代：$|P_{\\text{sec}} \\cap D| / |P_{\\text{sec}}| \\ge f_{\\min}$。\n    2.  其与主要前身的质量比是显著的：$M(P_{\\text{sec}}) / M(P_{\\text{main}}) \\ge \\mu_{\\min}$。\n    总并合率是所有时间步中此类并合事件的总数，除以模拟的总时间间隔（$t_{\\text{final}} - t_{\\text{initial}}$）。\n- **质量增长历史误差 ($\\epsilon$)**：我们关注单个主要暗晕谱系的演化。我们识别最终快照时的主要后代暗晕（在这些测试用例中，是对应于原始真实暗晕ID 0的暗晕），并将其主要前身分支追溯回过去。这产生了推断的质量历史 $M_{\\mathrm{inferred}}(t_s)$。将其与真实质量历史 $M_{\\mathrm{true}}(t_s)$（从基准真相星表得出）进行比较，使用平均相对误差：\n    $$\n    \\epsilon = \\frac{1}{S+1}\\sum_{s=0}^{S}\\frac{|M_{\\mathrm{inferred}}(t_s) - M_{\\mathrm{true}}(t_s)|}{M_{\\mathrm{true}}(t_s)}\n    $$\n    其中 $S+1$ 是快照的数量。\n\n**4. 最终分析**\n对于每个测试用例，我们为朴素星表和校正后星表计算并合率（$r$）和质量增长误差（$\\epsilon$）。然后通过差值计算改进量：\n- 并合率改进：$\\Delta r = r_{\\mathrm{naive}} - r_{\\mathrm{corrected}}$\n- 质量增长改进：$\\Delta \\epsilon = \\epsilon_{\\mathrm{naive}} - \\epsilon_{\\mathrm{corrected}}$\n这些结果量化了相空间分裂算法在恢复更准确的暗晕组装图景方面的有效性。最终输出汇集了所有测试用例的这些改进量。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n\n    def generate_particle_data(case_params, seed):\n        \"\"\"Generates particle data for all snapshots in a test case.\"\"\"\n        np.random.seed(seed)\n        all_snapshots_data = []\n        \n        particle_id_counter = 0\n        for s, t_s in enumerate(case_params['t']):\n            snapshot_particles = []\n            \n            for h in range(case_params['H']):\n                num_particles = case_params['particles_per_halo'][h]\n                center_pos = np.array(case_params['pos_centers'][h][s])\n                center_vel = np.array(case_params['vel_centers'][h])\n                \n                pos_disp = np.array(case_params['pos_disp'])\n                vel_disp = np.array(case_params['vel_disp'])\n                \n                positions = np.random.normal(loc=center_pos, scale=pos_disp, size=(num_particles, 3))\n                velocities = np.random.normal(loc=center_vel, scale=vel_disp, size=(num_particles, 3))\n                \n                for i in range(num_particles):\n                    particle = {\n                        'id': particle_id_counter + i,\n                        'pos': positions[i],\n                        'vel': velocities[i],\n                        'true_halo_id': h\n                    }\n                    snapshot_particles.append(particle)\n                particle_id_counter += num_particles\n            \n            # Since particle IDs are constant, we only need to generate them once\n            # but for this logic, we keep track of which particles belong to halos.\n            # Here we reset the counter for next snapshot's generation but use the same ID logic.\n            if s == 0:\n                first_snapshot_particles = snapshot_particles\n            else:\n                for i in range(len(snapshot_particles)):\n                    snapshot_particles[i]['id'] = first_snapshot_particles[i]['id']\n            \n            all_snapshots_data.append(snapshot_particles)\n\n        return all_snapshots_data\n\n    def build_catalogs(all_snapshots_data, case_params):\n        \"\"\"Builds ground-truth, naive, and corrected catalogs.\"\"\"\n        gt_catalogs = []\n        naive_catalogs = []\n        \n        # Ground-truth and naive catalogs\n        for s_idx, snapshot_data in enumerate(all_snapshots_data):\n            gt_cat = {}\n            for p in snapshot_data:\n                h_id = p['true_halo_id']\n                if h_id not in gt_cat:\n                    gt_cat[h_id] = set()\n                gt_cat[h_id].add(p['id'])\n            gt_catalogs.append(list(gt_cat.values()))\n            \n            if case_params['apply_bridging'] and s_idx == len(all_snapshots_data) - 1:\n                all_ids = set()\n                for halo in gt_cat.values():\n                    all_ids.update(halo)\n                naive_catalogs.append([all_ids])\n            else:\n                naive_catalogs.append(list(gt_cat.values()))\n        \n        # Corrected catalog\n        corrected_catalogs = []\n        for s_idx, naive_cat in enumerate(naive_catalogs):\n            corrected_cat = []\n            snapshot_data = all_snapshots_data[s_idx]\n            \n            for group_ids in naive_cat:\n                if len(group_ids)  2:\n                    corrected_cat.append(group_ids)\n                    continue\n\n                group_particles = [p for p in snapshot_data if p['id'] in group_ids]\n                \n                # Apply phase-space splitting\n                split_halos = phase_space_split(group_particles, case_params, case_params['seed'])\n                corrected_cat.extend(split_halos)\n            \n            corrected_catalogs.append(corrected_cat)\n\n        return gt_catalogs, naive_catalogs, corrected_catalogs\n\n    def deterministic_kmeans(data, k, seed, max_iters=100):\n        \"\"\"A simple, deterministic k-means implementation.\"\"\"\n        np.random.seed(seed)\n        # Initialize centroids by picking k random points from data\n        initial_indices = np.random.choice(data.shape[0], k, replace=False)\n        centroids = data[initial_indices]\n        \n        for _ in range(max_iters):\n            distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n            assignments = np.argmin(distances, axis=1)\n            \n            new_centroids = np.array([data[assignments == i].mean(axis=0) for i in range(k)])\n            \n            if np.all(centroids == new_centroids):\n                break\n            centroids = new_centroids\n        \n        return assignments, centroids\n\n    def phase_space_split(group_particles, params, seed):\n        \"\"\"Attempts to split a group of particles using phase-space clustering.\"\"\"\n        N = len(group_particles)\n        \n        # Form 6D phase-space array\n        phase_space_data = np.array([[*p['pos'], *p['vel']] for p in group_particles])\n        \n        # Normalize data using MAD\n        medians = np.median(phase_space_data, axis=0)\n        mad = np.median(np.abs(phase_space_data - medians), axis=0)\n        \n        # Handle zero MAD\n        non_zero_mad = mad > 1e-9\n        normalized_data = np.zeros_like(phase_space_data)\n        normalized_data[:, non_zero_mad] = (phase_space_data[:, non_zero_mad] - medians[non_zero_mad]) / mad[non_zero_mad]\n        \n        # Deterministic k-means with k=2\n        assignments, centroids = deterministic_kmeans(normalized_data, k=2, seed=seed)\n        \n        cluster1_indices = np.where(assignments == 0)[0]\n        cluster2_indices = np.where(assignments == 1)[0]\n        \n        # Check minimum cluster size\n        min_size = params['min_cluster_fraction'] * N\n        if len(cluster1_indices)  min_size or len(cluster2_indices)  min_size:\n            particle_ids = {p['id'] for p in group_particles}\n            return [particle_ids]\n\n        # Calculate B metric\n        c1, c2 = centroids\n        inter_cluster_dist = np.linalg.norm(c1 - c2)\n        \n        distances_to_c1 = np.linalg.norm(normalized_data - c1, axis=1)\n        distances_to_c2 = np.linalg.norm(normalized_data - c2, axis=1)\n        min_distances = np.minimum(distances_to_c1, distances_to_c2)\n        mean_intra_cluster_dist = np.mean(min_distances)\n        \n        B = inter_cluster_dist / mean_intra_cluster_dist if mean_intra_cluster_dist > 1e-9 else float('inf')\n\n        if B > params['B_th']:\n            group1_ids = {group_particles[i]['id'] for i in cluster1_indices}\n            group2_ids = {group_particles[i]['id'] for i in cluster2_indices}\n            return [group1_ids, group2_ids]\n        else:\n            particle_ids = {p['id'] for p in group_particles}\n            return [particle_ids]\n\n    def compute_metrics(catalogs, true_catalogs, params):\n        \"\"\"Computes merger rate and mass growth error.\"\"\"\n        m_p = params['m_p']\n        \n        # Merger Rate\n        total_time = params['t'][-1] - params['t'][0]\n        num_mergers = 0\n        for s in range(len(catalogs) - 1):\n            cat_s = catalogs[s]\n            cat_s1 = catalogs[s+1]\n            \n            for descendant_ids in cat_s1:\n                progenitors = []\n                for prog_idx, progenitor_ids in enumerate(cat_s):\n                    shared_ids = progenitor_ids.intersection(descendant_ids)\n                    if len(shared_ids) > 0:\n                        progenitors.append({'ids': progenitor_ids, 'shared': len(shared_ids)})\n                \n                if len(progenitors)  2:\n                    continue\n                \n                main_progenitor = max(progenitors, key=lambda p: p['shared'])\n                mass_main = len(main_progenitor['ids']) * m_p\n                \n                is_merger = False\n                for prog in progenitors:\n                    if prog['ids'] == main_progenitor['ids']:\n                        continue\n                     \n                    link_fraction = prog['shared'] / len(prog['ids']) if len(prog['ids']) > 0 else 0\n                    \n                    if link_fraction >= params['f_min']:\n                        mass_sec = len(prog['ids']) * m_p\n                        if mass_sec / mass_main >= params['mu_min']:\n                            is_merger = True\n                            break\n                if is_merger:\n                    num_mergers += 1\n        \n        merger_rate = num_mergers / total_time if total_time > 0 else 0\n\n        # Mass Growth Error\n        # Identify main branch based on true halo ID 0\n        \n        # True mass history\n        M_true = []\n        for s in range(len(true_catalogs)):\n            found = False\n            for true_halo in true_catalogs[s]:\n                if 0 in true_halo:  # Assuming particle 0 is in halo 0\n                    M_true.append(len(true_halo) * m_p)\n                    found = True\n                    break\n            if not found: # if halo does not exist\n                 M_true.append(0)\n\n        # Inferred mass history\n        M_inferred = []\n        # Find the final descendant containing particle 0\n        final_main_desc_ids = None\n        for halo_ids in catalogs[-1]:\n            if 0 in halo_ids:\n                final_main_desc_ids = halo_ids\n                break\n        \n        if final_main_desc_ids is None: # Main halo disappeared\n            M_inferred = [0] * len(catalogs)\n        else:\n            current_halo_ids = final_main_desc_ids\n            M_inferred.append(len(current_halo_ids) * m_p)\n            \n            for s in range(len(catalogs) - 2, -1, -1):\n                progenitors = []\n                for progenitor_ids in catalogs[s]:\n                    shared_count = len(progenitor_ids.intersection(current_halo_ids))\n                    if shared_count > 0:\n                        progenitors.append({'ids': progenitor_ids, 'shared': shared_count})\n                \n                if not progenitors:\n                    # No progenitor found, branch ends\n                    M_inferred.extend([0] * (s + 1))\n                    break\n                \n                main_progenitor = max(progenitors, key=lambda p: p['shared'])\n                current_halo_ids = main_progenitor['ids']\n                M_inferred.append(len(current_halo_ids) * m_p)\n            \n            M_inferred.reverse()\n\n        # Calculate error\n        total_frac_error = 0\n        for i in range(len(M_true)):\n            if M_true[i] > 0:\n                frac_error = np.abs(M_inferred[i] - M_true[i]) / M_true[i]\n                total_frac_error += frac_error\n        \n        mass_growth_error = total_frac_error / len(M_true)\n\n        return merger_rate, mass_growth_error\n\n    test_cases = [\n        # Test case 1\n        {\n            'H': 2, 't': [0.0, 1.0, 2.0], 'l': 0.2, 'm_p': 1e9,\n            'particles_per_halo': [200, 200],\n            'pos_disp': [0.05, 0.05, 0.05], 'vel_disp': [30.0, 30.0, 30.0],\n            'pos_centers': [[[0.0,0,0], [0.0,0,0], [0.0,0,0]],\n                            [[0.5,0,0], [0.3,0,0], [0.1,0,0]]],\n            'vel_centers': [[100.0,0,0], [-100.0,0,0]],\n            'apply_bridging': True, 'B_th': 2.5, 'min_cluster_fraction': 0.2,\n            'f_min': 0.2, 'mu_min': 0.25, 'seed': 123\n        },\n        # Test case 2\n        {\n            'H': 2, 't': [0.0, 1.0, 2.0], 'l': 0.2, 'm_p': 1e9,\n            'particles_per_halo': [150, 150],\n            'pos_disp': [0.06, 0.06, 0.06], 'vel_disp': [50.0, 50.0, 50.0],\n            'pos_centers': [[[0.0,0,0],[0.0,0,0],[0.0,0,0]],\n                            [[0.4,0,0],[0.25,0,0],[0.18,0,0]]],\n            'vel_centers': [[30.0,0,0], [-20.0,0,0]],\n            'apply_bridging': True, 'B_th': 2.5, 'min_cluster_fraction': 0.2,\n            'f_min': 0.2, 'mu_min': 0.25, 'seed': 456\n        },\n        # Test case 3\n        {\n            'H': 1, 't': [0.0, 1.0, 2.0], 'l': 0.2, 'm_p': 1e9,\n            'particles_per_halo': [250],\n            'pos_disp': [0.05, 0.05, 0.05], 'vel_disp': [30.0, 30.0, 30.0],\n            'pos_centers': [[[0.0,0,0],[0.0,0,0],[0.0,0,0]]],\n            'vel_centers': [[50.0,0,0]],\n            'apply_bridging': False, 'B_th': 2.5, 'min_cluster_fraction': 0.2,\n            'f_min': 0.2, 'mu_min': 0.25, 'seed': 789\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        data = generate_particle_data(params, params['seed'])\n        gt_cats, naive_cats, corr_cats = build_catalogs(data, params)\n        \n        r_naive, e_naive = compute_metrics(naive_cats, gt_cats, params)\n        r_corr, e_corr = compute_metrics(corr_cats, gt_cats, params)\n        \n        delta_r = r_naive - r_corr\n        delta_e = e_naive - e_corr\n        \n        results.append(f\"{delta_r:.4f}\")\n        results.append(f\"{delta_e:.4f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3468933"}, {"introduction": "面对众多可用的晕识别算法，我们如何判断哪种算法在何种场景下表现更优？最后的这个实践将我们的关注点从构建目录转移到评估目录。你将实现一套标准化的基准测试，用于将不同（合成的）晕识别算法的输出与“真实”参考进行比较，并计算诸如错误链接率和峰值质量偏差等关键指标。对于任何处理模拟数据的研究者来说，这都是一项至关重要的技能。[@problem_id:3468920]", "problem": "使用匹配的初始条件和共享的粒子标识符空间，跨多个晕轮识别算法（halo finders）构建和评估标准化的基准指标。给定用于基准参考和几个晕轮识别算法的紧凑、合成的并合树和晕轮星表。您的任务是实现一个程序，为每个测试用例计算每个识别算法的三个指标，然后在该测试用例中对所有识别算法的这些指标进行平均：\n- 在粒子重叠匹配方案下，预测树与基准参考树之间的错连率。\n- 预测森林与基准参考森林的分支长度分布（根到叶路径长度）之间的柯尔莫哥洛夫-斯米尔诺夫距离。\n- 对于匹配的 $z=0$ 叶节点，$M_{\\mathrm{peak}}$（主前身分支上的最大质量）的平均绝对分数偏差。\n\n所有星表共享相同的粒子标识符宇宙。一个晕轮由以下内容完全指定：\n- 一个快照索引 $s \\in \\{0,1,2,\\dots\\}$。\n- 一个唯一的晕轮标签（一个字符串）。\n- 一个成员粒子标识符集合 $\\mathcal{P}$。\n- 一个标量晕轮质量 $M$（以与粒子数成比例的一致任意单位表示；答案为无量纲小数）。\n\n每个并合树都是一个有向无环图，其边从快照 $s$ 的前身指向快照 $s+1$ 的后代。基准参考树是基准。\n\n需要实现的定义：\n1) 固定快照 $s$ 的粒子重叠匹配：对于一个预测的晕轮 $h_p$ 和在同一快照 $s$ 的一组基准参考晕轮 $\\{h_{g}\\}$，定义整数重叠 $w(h_p,h_g) = |\\mathcal{P}(h_p) \\cap \\mathcal{P}(h_g)|$。$h_p$ 的匹配基准参考晕轮是使 $w(h_p,h_g)$ 最大化的那个 $h_g$。若出现平局，则按较大的 $M(h_g)$ 解决，然后按字典序最小的晕轮标签解决。如果最大重叠为 $0$，则声明 $h_p$ 在快照 $s$ 未匹配。\n2) 错连率：对于每个预测的边 $(u \\to v)$，让它们在相同快照的基准参考中的匹配为 $(u' \\to v')$（跳过任一端点未匹配的边）。如果 $(u' \\to v')$ 是基准参考图中的一条边，则该边是正确的。错连率是所考虑的预测边中不正确的边所占的比例。如果所考虑的边数为 $0$，则定义错连率为 $0$。\n3) 分支长度分布：对于一个森林，将分支长度定义为根到叶路径上的边数。枚举所有不同的根到叶路径，并将其长度收集到一个多重集中。对于两个有限的分支长度样本，将柯尔莫哥洛夫-斯米尔诺夫统计量 $D$ 定义为所有整数 $x$ 上 $|F_1(x) - F_2(x)|$ 的上确界，其中 $F_i$ 是经验累积分布函数。如果两个样本都为空，则定义 $D=0$。\n4) 沿逆时序遍历的 $M_{\\mathrm{peak}}$ 主前身选择：给定最终快照 $s_{\\max}$ 的一个叶晕轮，使用父边向后遍历到更早的快照。如果在某一步存在多个父节点，选择与当前子晕轮粒子重叠最大的那个；若出现平局，则按较大的父节点 $M$ 解决，然后按字典序最小的标签解决。将 $M_{\\mathrm{peak}}$ 定义为此主分支路径上的最大 $M$，包括叶节点本身。为比较预测和基准参考的 $M_{\\mathrm{peak}}$ 值，使用快照匹配规则将快照 $s_{\\max}$ 的预测叶节点与基准参考叶节点进行匹配。对于每个匹配对，计算绝对分数偏差 $\\delta = |M^{\\mathrm{pred}}_{\\mathrm{peak}} - M^{\\mathrm{true}}_{\\mathrm{peak}}| / \\max(M^{\\mathrm{true}}_{\\mathrm{peak}}, \\epsilon)$，其中 $\\epsilon = 10^{-12}$ 以避免除以零。测试用例的 $M_{\\mathrm{peak}}$ 偏差指标是所有匹配叶节点的 $\\delta$ 的平均值。如果没有匹配的叶节点，则将该指标定义为 $0$。\n\n对于每个测试用例，将提供的识别算法的三个指标进行平均，生成一个列表 $[r_{\\mathrm{mislink}}, D_{\\mathrm{KS}}, \\overline{\\delta}_{M_{\\mathrm{peak}}}]$。\n\n测试套件输入规范如下。在每种情况下，您都会得到：\n- 基准参考星表：一组在指定快照的晕轮，每个晕轮都有标签、粒子集和质量 $M$；以及一组有向边。\n- 一组识别算法星表，每个也同样被指定。\n\n使用这些确切的测试配置：\n\n测试用例A（两个不相交的谱系，其中一个识别算法中出现错误的中间合并）：\n- 基准参考：\n  - 快照 $0$：晕轮 T1，$\\mathcal{P}=\\{1,2,3\\}$，$M=3$；T2，$\\mathcal{P}=\\{4,5,6\\}$，$M=3$。\n  - 快照 $1$：晕轮 T3，$\\mathcal{P}=\\{1,2,3\\}$，$M=3$；T4，$\\mathcal{P}=\\{4,5,6\\}$，$M=3$。\n  - 快照 $2$：晕轮 T5，$\\mathcal{P}=\\{1,2,3\\}$，$M=3$；T6，$\\mathcal{P}=\\{4,5,6\\}$，$M=3$。\n  - 边：$(\\text{T1}\\to\\text{T3})$, $(\\text{T2}\\to\\text{T4})$, $(\\text{T3}\\to\\text{T5})$, $(\\text{T4}\\to\\text{T6})$。\n- 识别算法星表：\n  - Friends-of-Friends (FOF)：\n    - 快照 $0$：F1 $\\{1,2,3\\}$，$M=3$；F2 $\\{4,5,6\\}$，$M=3$。\n    - 快照 $1$：F3 $\\{1,2,3,4,5,6\\}$，$M=6$。\n    - 快照 $2$：F5 $\\{1,2,3\\}$，$M=3$；F6 $\\{4,5,6\\}$，$M=3$。\n    - 边：$(\\text{F1}\\to\\text{F3})$, $(\\text{F2}\\to\\text{F3})$, $(\\text{F3}\\to\\text{F5})$, $(\\text{F3}\\to\\text{F6})$。\n  - Spherical Overdensity (SO)：\n    - 与基准参考相同（标签 S1..S6 与 T1..T6 并行，具有相应的 $\\mathcal{P}$、$M$ 和边）。\n  - Rockstar：\n    - 快照 $0$：R1 $\\{1,2,3\\}$，$M=3$；R2 $\\{4,5,6\\}$，$M=3$。\n    - 快照 $1$：R3 $\\{1,2,3\\}$，$M=4$；R4 $\\{4,5,6\\}$，$M=4$。\n    - 快照 $2$：R5 $\\{1,2,3\\}$，$M=3$；R6 $\\{4,5,6\\}$，$M=3$。\n    - 边：$(\\text{R1}\\to\\text{R3})$, $(\\text{R2}\\to\\text{R4})$, $(\\text{R3}\\to\\text{R5})$, $(\\text{R4}\\to\\text{R6})$。\n\n测试用例B（一次干净的双体合并）：\n- 基准参考：\n  - 快照 $0$：H1 $\\{1,2\\}$，$M=2$；H2 $\\{3,4\\}$，$M=2$。\n  - 快照 $1$：H3 $\\{1,2,3,4\\}$，$M=4$。\n  - 边：$(\\text{H1}\\to\\text{H3})$, $(\\text{H2}\\to\\text{H3})$。\n- 识别算法星表：\n  - Spherical Overdensity (SO)：与基准参考相同（标签 S1..S3 对应于 H1..H3）。\n  - Amiga Halo Finder (AHF)：与基准参考相同（标签 A1..A3 对应于 H1..H3）。\n\n测试用例C（具有碎裂和缺失链接的单个轨迹）：\n- 基准参考：\n  - 快照 $0$：T1 $\\{1,2,3,4,5\\}$，$M=5$。\n  - 快照 $1$：T2 $\\{1,2,3,4,5\\}$，$M=5$。\n  - 快照 $2$：T3 $\\{1,2,3,4,5\\}$，$M=5$。\n  - 边：$(\\text{T1}\\to\\text{T2})$, $(\\text{T2}\\to\\text{T3})$。\n- 识别算法星表：\n  - Amiga Halo Finder (AHF)，快照不连接：\n    - 快照 $0$：A1 $\\{1,2,3,4,5\\}$，$M=5$。\n    - 快照 $1$：A4 $\\{1,2,3,4,5\\}$，$M=5$。\n    - 快照 $2$：A7 $\\{1,2,3,4,5\\}$，$M=5$。\n    - 边：无。\n  - SUBFIND，具有分裂的中间体和重连接：\n    - 快照 $0$：S1 $\\{1,2,3,4,5\\}$，$M=5$。\n    - 快照 $1$：S4 $\\{1,2\\}$，$M=2$；S5 $\\{3,4,5\\}$，$M=3$。\n    - 快照 $2$：S7 $\\{1,2,3,4,5\\}$，$M=5$。\n    - 边：$(\\text{S1}\\to\\text{S4})$, $(\\text{S1}\\to\\text{S5})$, $(\\text{S4}\\to\\text{S7})$, $(\\text{S5}\\to\\text{S7})$。\n\n您的程序必须：\n- 精确实现上述定义，包括所有平局决胜规则。\n- 对于每个测试用例，为每个识别算法计算每个指标，然后按指标在该案例中的所有识别算法间取平均值，以生成 $[r_{\\mathrm{mislink}}, D_{\\mathrm{KS}}, \\overline{\\delta}_{M_{\\mathrm{peak}}}]$。\n- 生成单行输出，其中包含测试用例A、测试用例B和测试用例C的这三个元素列表的列表，按此顺序排列，作为逗号分隔并用方括号括起来的列表，例如 $[[a,b,c],[d,e,f],[g,h,i]]$，其中包含小数。不要打印任何其他文本。\n\n不涉及角度。所有输出必须是无量纲小数（而不是百分比）。不允许随机化，也不需要外部输入。", "solution": "该问题提出了一个在计算宇宙学领域有效且定义明确的挑战：实现一套标准化的基准测试套件，用于评估晕轮识别和并合树构建算法。所提供的定义精确、客观，并基于已建立的天体物理学实践。问题是自包含的，为三个不同的测试用例提供了所有必要的合成数据。我将继续进行详细的解答。\n\n该实现围绕一个核心类 `MergerTree` 构建，该类封装了给定数据集（无论是基准参考还是来自某个识别算法）的晕轮星表和并合历史图。然后，主程序逻辑遍历指定的测试用例，并为每个测试用例，计算每个识别算法所需的三个指标并求其平均值。\n\n**1. 数据表示**\n\n一个晕轮由其属性表示：快照索引 $s$、唯一标签、粒子标识符集合 $\\mathcal{P}$ 和质量 $M$。并合树是一个有向无环图，由连接连续快照间晕轮的边定义。这些都存储在 `MergerTree` 对象中，该对象提供了通过标签或快照方便地访问晕轮的功能，并通过前向（从前身到后代）和后向（从后代到前身）邻接列表访问图结构。\n\n**2. 粒子重叠匹配**\n\n所有比较的一个关键首步是在每个快照 $s$ 上，建立识别算法星表中的晕轮与基准参考星表中的晕轮之间的对应关系。对于给定的预测晕轮 $h_p$，我们在同一快照的所有基准参考晕轮集合 $\\{h_g\\}_s$ 中找到它的匹配项。匹配标准是最大化粒子重叠，定义为 $w(h_p, h_g) = |\\mathcal{P}(h_p) \\cap \\mathcal{P}(h_g)|$。实现遵循指定的平局决胜规则：\n1.  选择具有更大质量 $M(h_g)$ 的基准参考晕轮 $h_g$。\n2.  如果平局仍然存在，选择字典序最小标签的晕轮。\n\n如果最大重叠为 $0$，则预测晕轮 $h_p$ 在该快照被视为未匹配。此匹配过程系统地应用于识别算法星表中的所有晕轮，从而创建一个从预测晕轮标签到基准参考晕轮标签的全面映射。\n\n**3. 指标 I：错连率 ($r_{\\mathrm{mislink}}$)**\n\n错连率量化了预测并合树中时间连接的准确性。对于识别算法树中的每个边 $(u \\to v)$，我们识别它们在基准参考中的匹配项 $(u' \\to v')$。如果 $u$ 或 $v$ 未匹配，则该边不用于此指标的计算。当且仅当对应的边 $(u' \\to v')$ 存在于基准参考并合树中时，该边才被视为正确。错连率是所考虑的边中不正确的边所占的比例：\n$$ r_{\\mathrm{mislink}} = \\frac{N_{\\rm incorrect}}{N_{\\rm considered}} $$\n如果所考虑的边数 $N_{\\rm considered}$ 为 $0$，则错连率定义为 $0$。\n\n**4. 指标 II：分支长度柯尔莫哥洛夫-斯米尔诺夫距离 ($D_{\\mathrm{KS}}$)**\n\n此指标比较演化历史长度的统计分布。分支长度定义为并合树森林中从根（没有前身的晕轮）到叶（没有后代的晕轮）的路径上的边数。\n首先，对于基准参考和识别算法的森林，我们都枚举所有不同的根到叶路径，并将其长度收集到两个多重集 $L_{\\rm true}$ 和 $L_{\\rm pred}$ 中。\n然后计算这两个样本之间的柯尔莫哥洛夫-斯米尔诺夫（KS）统计量 $D$。它是它们的经验累积分布函数（ECDFs）$F_{\\rm true}$ 和 $F_{\\rm pred}$ 之间的最大绝对差：\n$$ D_{\\mathrm{KS}} = \\sup_{x} |F_{\\rm true}(x) - F_{\\rm pred}(x)| $$\n其中 $F(x) = \\frac{1}{|L|} \\sum_{l \\in L} I(l \\le x)$，$I(\\cdot)$ 是指示函数。通过在并集 $L_{\\rm true} \\cup L_{\\rm pred}$ 中存在的每个唯一值处评估差异来计算上确界。如果两个样本多重集都为空，则定义 $D_{\\mathrm{KS}}$ 为 $0$。\n\n**5. 指标 III：峰值质量的平均绝对分数偏差 ($\\overline{\\delta}_{M_{\\mathrm{peak}}}$)**\n\n该指标评估识别算法正确追踪晕轮主前身分支峰值质量的能力。首先，对于任何给定的晕轮，通过从该晕轮向后追溯时间来确定其主前身分支。在每一步中，如果一个晕轮有多个前身，则选择与后代粒子重叠最大的那个作为主前身。平局通过选择质量 $M$ 较大的前身，然后按字典序最小的标签来解决。峰值质量 $M_{\\mathrm{peak}}$ 是沿着这条主谱系发现的最大质量，包括起始晕轮本身。\n\n为了计算该指标，我们考虑识别算法星表中最终快照 $s_{\\max}$ 的所有叶晕轮。使用粒子重叠匹配规则将这些叶晕轮中的每一个与 $s_{\\max}$ 处的基准参考晕轮进行匹配。对于每个成功匹配的对 $(h_p, h_g)$，我们计算它们各自的峰值质量 $M_{\\mathrm{peak}}^{\\mathrm{pred}}$ 和 $M_{\\mathrm{peak}}^{\\mathrm{true}}$。绝对分数偏差计算如下：\n$$ \\delta = \\frac{|M_{\\mathrm{peak}}^{\\mathrm{pred}} - M_{\\mathrm{peak}}^{\\mathrm{true}}|}{\\max(M_{\\mathrm{peak}}^{\\mathrm{true}}, \\epsilon)} $$\n其中 $\\epsilon=10^{-12}$ 是一个很小的常数，以防止除以零。最终的指标值是所有匹配叶对的 $\\delta$ 的算术平均值。如果在 $s_{\\max}$ 没有可匹配的叶，则该指标定义为 $0$。\n\n**6. 最终平均**\n\n对于每个测试用例，为每个识别算法计算三个指标——$r_{\\mathrm{mislink}}$、$D_{\\mathrm{KS}}$ 和 $\\overline{\\delta}_{M_{\\mathrm{peak}}}$。测试用例的最终结果是一个三元组值，其中每个值是该测试用例中所有识别算法对应指标的平均值。", "answer": "```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the validation and solution process.\n    \"\"\"\n    test_cases_data = [\n        {\n            \"name\": \"Test Case A\",\n            \"gt\": {\n                \"halos\": [\n                    (\"T1\", 0, {1, 2, 3}, 3), (\"T2\", 0, {4, 5, 6}, 3),\n                    (\"T3\", 1, {1, 2, 3}, 3), (\"T4\", 1, {4, 5, 6}, 3),\n                    (\"T5\", 2, {1, 2, 3}, 3), (\"T6\", 2, {4, 5, 6}, 3),\n                ],\n                \"edges\": [(\"T1\", \"T3\"), (\"T2\", \"T4\"), (\"T3\", \"T5\"), (\"T4\", \"T6\")],\n            },\n            \"finders\": {\n                \"FOF\": {\n                    \"halos\": [\n                        (\"F1\", 0, {1, 2, 3}, 3), (\"F2\", 0, {4, 5, 6}, 3),\n                        (\"F3\", 1, {1, 2, 3, 4, 5, 6}, 6),\n                        (\"F5\", 2, {1, 2, 3}, 3), (\"F6\", 2, {4, 5, 6}, 3),\n                    ],\n                    \"edges\": [(\"F1\", \"F3\"), (\"F2\", \"F3\"), (\"F3\", \"F5\"), (\"F3\", \"F6\")],\n                },\n                \"SO\": {\n                    \"halos\": [\n                        (\"S1\", 0, {1, 2, 3}, 3), (\"S2\", 0, {4, 5, 6}, 3),\n                        (\"S3\", 1, {1, 2, 3}, 3), (\"S4\", 1, {4, 5, 6}, 3),\n                        (\"S5\", 2, {1, 2, 3}, 3), (\"S6\", 2, {4, 5, 6}, 3),\n                    ],\n                    \"edges\": [(\"S1\", \"S3\"), (\"S2\", \"S4\"), (\"S3\", \"S5\"), (\"S4\", \"S6\")],\n                },\n                \"Rockstar\": {\n                    \"halos\": [\n                        (\"R1\", 0, {1, 2, 3}, 3), (\"R2\", 0, {4, 5, 6}, 3),\n                        (\"R3\", 1, {1, 2, 3}, 4), (\"R4\", 1, {4, 5, 6}, 4),\n                        (\"R5\", 2, {1, 2, 3}, 3), (\"R6\", 2, {4, 5, 6}, 3),\n                    ],\n                    \"edges\": [(\"R1\", \"R3\"), (\"R2\", \"R4\"), (\"R3\", \"R5\"), (\"R4\", \"R6\")],\n                },\n            },\n        },\n        {\n            \"name\": \"Test Case B\",\n            \"gt\": {\n                \"halos\": [(\"H1\", 0, {1, 2}, 2), (\"H2\", 0, {3, 4}, 2), (\"H3\", 1, {1, 2, 3, 4}, 4)],\n                \"edges\": [(\"H1\", \"H3\"), (\"H2\", \"H3\")],\n            },\n            \"finders\": {\n                \"SO\": {\n                    \"halos\": [(\"S1\", 0, {1, 2}, 2), (\"S2\", 0, {3, 4}, 2), (\"S3\", 1, {1, 2, 3, 4}, 4)],\n                    \"edges\": [(\"S1\", \"S3\"), (\"S2\", \"S3\")],\n                },\n                \"AHF\": {\n                    \"halos\": [(\"A1\", 0, {1, 2}, 2), (\"A2\", 0, {3, 4}, 2), (\"A3\", 1, {1, 2, 3, 4}, 4)],\n                    \"edges\": [(\"A1\", \"A3\"), (\"A2\", \"A3\")],\n                },\n            },\n        },\n        {\n            \"name\": \"Test Case C\",\n            \"gt\": {\n                \"halos\": [\n                    (\"T1\", 0, {1, 2, 3, 4, 5}, 5),\n                    (\"T2\", 1, {1, 2, 3, 4, 5}, 5),\n                    (\"T3\", 2, {1, 2, 3, 4, 5}, 5),\n                ],\n                \"edges\": [(\"T1\", \"T2\"), (\"T2\", \"T3\")],\n            },\n            \"finders\": {\n                \"AHF\": {\n                    \"halos\": [\n                        (\"A1\", 0, {1, 2, 3, 4, 5}, 5),\n                        (\"A4\", 1, {1, 2, 3, 4, 5}, 5),\n                        (\"A7\", 2, {1, 2, 3, 4, 5}, 5),\n                    ],\n                    \"edges\": [],\n                },\n                \"SUBFIND\": {\n                     \"halos\": [\n                        (\"S1\", 0, {1, 2, 3, 4, 5}, 5),\n                        (\"S4\", 1, {1, 2}, 2), (\"S5\", 1, {3, 4, 5}, 3),\n                        (\"S7\", 2, {1, 2, 3, 4, 5}, 5),\n                    ],\n                    \"edges\": [(\"S1\", \"S4\"), (\"S1\", \"S5\"), (\"S4\", \"S7\"), (\"S5\", \"S7\")],\n                },\n            },\n        }\n    ]\n\n    class MergerTree:\n        def __init__(self, halo_spec, edge_spec):\n            self.halos = {label: {'s': s, 'p': p, 'm': m} for label, s, p, m in halo_spec}\n            self.fwd_edges = defaultdict(list)\n            self.back_edges = defaultdict(list)\n            for u, v in edge_spec:\n                self.fwd_edges[u].append(v)\n                self.back_edges[v].append(u)\n            \n            self.halos_by_snap = defaultdict(list)\n            for label, data in self.halos.items():\n                self.halos_by_snap[data['s']].append(label)\n\n            all_labels = set(self.halos.keys())\n            self.roots = sorted([l for l in all_labels if not self.back_edges[l]])\n            self.leaves = sorted([l for l in all_labels if not self.fwd_edges[l]])\n            \n        def get_main_progenitor(self, child_label):\n            parents = self.back_edges.get(child_label)\n            if not parents:\n                return None\n            if len(parents) == 1:\n                return parents[0]\n            \n            child_particles = self.halos[child_label]['p']\n            best_parent = None\n            max_overlap = -1\n            best_mass = -1\n\n            sorted_parents = sorted(parents)\n\n            for parent_label in sorted_parents:\n                parent_data = self.halos[parent_label]\n                overlap = len(child_particles.intersection(parent_data['p']))\n                \n                if overlap > max_overlap:\n                    max_overlap = overlap\n                    best_mass = parent_data['m']\n                    best_parent = parent_label\n                elif overlap == max_overlap:\n                    if parent_data['m'] > best_mass:\n                        best_mass = parent_data['m']\n                        best_parent = parent_label\n            return best_parent\n\n        def get_branch_lengths(self):\n            lengths = []\n            for root in self.roots:\n                q = [(root, 0)]\n                visited_paths = set()\n                while q:\n                    curr, length = q.pop(0)\n                    if curr in self.leaves:\n                        lengths.append(length)\n                    children = self.fwd_edges.get(curr, [])\n                    for child in children:\n                        # This DFS-like traversal on DAG will explore all paths\n                        q.append((child, length + 1))\n            return lengths\n        \n        def get_mpeak(self, leaf_label):\n            branch = []\n            curr = leaf_label\n            while curr:\n                branch.append(curr)\n                curr = self.get_main_progenitor(curr)\n            \n            masses = [self.halos[label]['m'] for label in branch]\n            return max(masses) if masses else 0\n\n    def compute_all_matches(finder_tree, gt_tree):\n        matches = {}\n        for s, finder_labels in finder_tree.halos_by_snap.items():\n            gt_halos_at_s = gt_tree.halos_by_snap.get(s, [])\n            if not gt_halos_at_s:\n                continue\n            \n            gt_halo_data_at_s = [(label, gt_tree.halos[label]) for label in gt_halos_at_s]\n\n            for f_label in finder_labels:\n                f_halo = finder_tree.halos[f_label]\n                best_match = None\n                max_overlap = -1\n                best_mass = -1\n\n                # Sort GT halos for lexicographical tie-breaking\n                sorted_gt_data = sorted(gt_halo_data_at_s, key=lambda x: x[0])\n\n                for gt_label, gt_halo in sorted_gt_data:\n                    overlap = len(f_halo['p'].intersection(gt_halo['p']))\n                    \n                    if overlap > max_overlap:\n                        max_overlap = overlap\n                        best_mass = gt_halo['m']\n                        best_match = gt_label\n                    elif overlap == max_overlap:\n                        if gt_halo['m'] > best_mass:\n                            best_mass = gt_halo['m']\n                            best_match = gt_label\n                \n                if max_overlap > 0:\n                    matches[f_label] = best_match\n        return matches\n\n    def calc_mislink_rate(finder_tree, gt_tree, matches):\n        considered_edges = 0\n        incorrect_edges = 0\n        for u, children in finder_tree.fwd_edges.items():\n            for v in children:\n                if u in matches and v in matches:\n                    considered_edges += 1\n                    u_prime, v_prime = matches[u], matches[v]\n                    if v_prime not in gt_tree.fwd_edges.get(u_prime, []):\n                        incorrect_edges += 1\n        return incorrect_edges / considered_edges if considered_edges > 0 else 0.0\n\n    def calc_ks_dist(finder_tree, gt_tree):\n        lengths1 = np.array(finder_tree.get_branch_lengths())\n        lengths2 = np.array(gt_tree.get_branch_lengths())\n\n        if len(lengths1) == 0 and len(lengths2) == 0:\n            return 0.0\n        if len(lengths1) == 0 or len(lengths2) == 0:\n            return 1.0\n\n        n1, n2 = len(lengths1), len(lengths2)\n        data_all = np.unique(np.concatenate([lengths1, lengths2]))\n        \n        lengths1.sort()\n        lengths2.sort()\n\n        cdf1 = np.searchsorted(lengths1, data_all, side='right') / n1\n        cdf2 = np.searchsorted(lengths2, data_all, side='right') / n2\n        \n        return np.max(np.abs(cdf1 - cdf2))\n\n    def calc_mpeak_dev(finder_tree, gt_tree, matches):\n        max_snap = max(finder_tree.halos_by_snap.keys()) if finder_tree.halos_by_snap else -1\n        if max_snap == -1: return 0.0\n\n        finder_leaves_at_max_snap = [l for l in finder_tree.leaves if finder_tree.halos[l]['s'] == max_snap]\n        \n        deviations = []\n        epsilon = 1e-12\n\n        for f_leaf in finder_leaves_at_max_snap:\n            if f_leaf in matches:\n                gt_leaf_match = matches[f_leaf]\n                \n                mpeak_pred = finder_tree.get_mpeak(f_leaf)\n                mpeak_true = gt_tree.get_mpeak(gt_leaf_match)\n\n                deviation = abs(mpeak_pred - mpeak_true) / max(mpeak_true, epsilon)\n                deviations.append(deviation)\n        \n        return np.mean(deviations) if deviations else 0.0\n\n    final_results = []\n    for case_data in test_cases_data:\n        gt_tree = MergerTree(case_data['gt']['halos'], case_data['gt']['edges'])\n        \n        case_metrics = []\n        for finder_name, finder_data in case_data['finders'].items():\n            finder_tree = MergerTree(finder_data['halos'], finder_data['edges'])\n            matches = compute_all_matches(finder_tree, gt_tree)\n            \n            r_mislink = calc_mislink_rate(finder_tree, gt_tree, matches)\n            d_ks = calc_ks_dist(finder_tree, gt_tree)\n            delta_mpeak = calc_mpeak_dev(finder_tree, gt_tree, matches)\n            \n            case_metrics.append([r_mislink, d_ks, delta_mpeak])\n        \n        avg_metrics = np.mean(case_metrics, axis=0).tolist()\n        final_results.append(avg_metrics)\n\n    list_of_strings = []\n    for sublist in final_results:\n        list_of_strings.append(f\"[{','.join(map(str, sublist))}]\")\n    output_string = f\"[{','.join(list_of_strings)}]\"\n    print(output_string)\n\nsolve()\n```", "id": "3468920"}]}