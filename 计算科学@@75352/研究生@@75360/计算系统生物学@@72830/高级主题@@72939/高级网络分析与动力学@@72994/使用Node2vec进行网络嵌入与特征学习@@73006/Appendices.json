{"hands_on_practices": [{"introduction": "node2vec 算法的第一步是通过有偏随机游走在网络中生成节点序列。这个练习将带你深入该过程的核心，通过一个简单的蛋白质相互作用网络基序，亲手计算在给定超参数 $p$ 和 $q$ 的情况下，从一个节点到其邻居的转移概率。掌握这一计算是理解 node2vec 如何探索网络结构的关键。[@problem_id:3331426]", "problem": "在一项计算系统生物学研究中，考虑一个由三个蛋白质 $a$、$b$ 和 $c$ 组成的蛋白质-蛋白质相互作用（PPI）网络基序。这三个蛋白质连接成一个无向三角形，边 $a$–$b$、$b$–$c$ 和 $c$–$a$ 上的单位边权重均为 $1$。为了使用 node2vec 学习网络嵌入，采用了一种带有返回参数 $p$ 和进出参数 $q$ 的二阶有偏随机游走。\n\n假设一次游走当前位于节点 $b$，而前一个节点是 $a$。在这个无权三角形上使用参数为 $(p,q)=(2,0.5)$ 的 node2vec 算法，计算从 $b$ 到其每个邻居 $a$ 和 $c$ 的下一步转移概率。请将您的答案表示为一个行向量 $\\left(P_{b\\to a}\\;\\;P_{b\\to c}\\right)$，并使用精确的分数形式。请勿近似或四舍五入。", "solution": "该问题是有效的。它在科学上基于计算网络科学中已确立的 `node2vec` 算法，问题陈述清晰，提供了所有必要信息，并且表述客观。我们可以开始求解。\n\n`node2vec` 算法的核心是一种有偏的二阶随机游走。在给定游走刚从前一个节点 $t$ 到达当前节点 $v$ 的情况下，从 $v$ 转移到下一个节点 $x$ 的概率由一个特定的加权方案定义。\n\n设游走刚刚经过边 $(t, v)$ 到达当前节点 $v$。从 $v$ 到其某个邻居 $x$ 的未归一化转移概率由下式给出：\n$$\n\\pi_{vx} = \\alpha_{pq}(t, x) \\cdot w_{vx}\n$$\n其中 $w_{vx}$ 是边 $(v, x)$ 的静态权重，而 $\\alpha_{pq}(t, x)$ 是 `node2vec` 的搜索偏置。该搜索偏置取决于返回参数 $p$ 和进出参数 $q$，定义如下：\n$$\n\\alpha_{pq}(t, x) =\n\\begin{cases}\n    \\frac{1}{p}   \\text{如果 } d_{tx} = 0 \\\\\n    1   \\text{如果 } d_{tx} = 1 \\\\\n    \\frac{1}{q}   \\text{如果 } d_{tx} = 2\n\\end{cases}\n$$\n此处，$d_{tx}$ 表示前一个节点 $t$ 和潜在的下一个节点 $x$ 之间的最短路径距离。\n\n然后，通过将未归一化概率除以节点 $v$ 的所有邻居的未归一化概率之和，来计算归一化转移概率 $P(v \\to x)$：\n$$\nP(v \\to x) = \\frac{\\pi_{vx}}{\\sum_{x' \\in N(v)} \\pi_{vx'}}\n$$\n其中 $N(v)$ 是节点 $v$ 的邻居集合。\n\n在给定的问题中，网络是由节点 $a$、$b$ 和 $c$ 组成的无权三角形。这意味着所有存在边的权重都为 $1$，即 $w_{ab} = w_{bc} = w_{ca} = 1$。游走当前位于节点 $v=b$，前一个节点是 $t=a$。当前节点 $b$ 的邻居是 $N(b) = \\{a, c\\}$。我们必须计算从 $b$ 到 $a$ 以及从 $b$ 到 $c$ 的转移概率。给定的参数为 $p=2$ 和 $q=0.5$。\n\n我们需要为每个邻居 $x \\in N(b)$ 计算未归一化转移概率 $\\pi_{bx}$。\n\n**1. 从 $b$ 转移到 $a$ ($x=a$)：**\n潜在的下一个节点是 $a$。我们必须确定前一个节点 $t=a$ 和下一个节点 $x=a$ 之间的最短路径距离 $d_{tx}$。一个节点到其自身的距离为 $d_{aa} = 0$。\n根据搜索偏置的公式，当 $d_{tx}=0$ 时，我们有 $\\alpha_{pq}(a, a) = \\frac{1}{p}$。\n边权重为 $w_{ba} = 1$。\n未归一化转移概率为：\n$$\n\\pi_{ba} = \\alpha_{pq}(a, a) \\cdot w_{ba} = \\frac{1}{p} \\cdot 1 = \\frac{1}{2}\n$$\n\n**2. 从 $b$ 转移到 $c$ ($x=c$)：**\n潜在的下一个节点是 $c$。我们必须确定前一个节点 $t=a$ 和下一个节点 $x=c$ 之间的最短路径距离 $d_{tx}$。在三角形基序中，节点 $a$ 和 $c$ 由一条边直接相连。因此，最短路径距离为 $d_{ac} = 1$。\n根据搜索偏置的公式，当 $d_{tx}=1$ 时，我们有 $\\alpha_{pq}(a, c) = 1$。\n边权重为 $w_{bc} = 1$。\n未归一化转移概率为：\n$$\n\\pi_{bc} = \\alpha_{pq}(a, c) \\cdot w_{bc} = 1 \\cdot 1 = 1\n$$\n注意，进出参数 $q$ 在本次计算中未使用，因为节点 $b$ 的邻居中没有一个与前一个节点 $a$ 的距离为 $2$。\n\n现在，我们计算归一化常数，即从 $b$ 出发所有可能下一步的未归一化概率之和：\n$$\nZ = \\sum_{x' \\in N(b)} \\pi_{bx'} = \\pi_{ba} + \\pi_{bc} = \\frac{1}{2} + 1 = \\frac{3}{2}\n$$\n\n最后，我们将每个未归一化概率除以归一化常数 $Z$ 来计算归一化转移概率。\n\n从 $b$ 转移到 $a$ 的概率为：\n$$\nP_{b \\to a} = \\frac{\\pi_{ba}}{Z} = \\frac{1/2}{3/2} = \\frac{1}{2} \\cdot \\frac{2}{3} = \\frac{1}{3}\n$$\n\n从 $b$ 转移到 $c$ 的概率为：\n$$\nP_{b \\to c} = \\frac{\\pi_{bc}}{Z} = \\frac{1}{3/2} = 1 \\cdot \\frac{2}{3} = \\frac{2}{3}\n$$\n\n最终的转移概率为 $P_{b\\to a} = \\frac{1}{3}$ 和 $P_{b\\to c} = \\frac{2}{3}$。问题要求将答案表示为行向量 $(P_{b\\to a}\\;\\;P_{b\\to c})$。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}}\n$$", "id": "3331426"}, {"introduction": "生成节点序列后，我们如何利用它们来学习节点的向量表示？这个练习将揭示学习过程的内部运作，要求你为一个正样本和两个负样本对，执行单步随机梯度上升来更新源节点的嵌入向量。通过这个具体的计算，你将清晰地看到嵌入向量是如何根据其在随机游走中出现的上下文进行迭代调整的。[@problem_id:3331408]", "problem": "在一个来自计算系统生物学的蛋白质-蛋白质相互作用网络中，假设我们使用带有负采样的 node2vec skip-gram 模型来训练节点表示。设源（节点）嵌入为一个向量 $\\mathbf{z}_u \\in \\mathbb{R}^2$，上下文（节点）嵌入为向量 $\\mathbf{z}'_x \\in \\mathbb{R}^2$。对于单个随机更新步骤，考虑以下源自访问了节点 $u$ 并观察到上下文节点 $v$ 的二阶随机游走的设置：\n- 正共现对是 $(u,v)$。\n- 两个负样本是 $n_1$ 和 $n_2$。\n\n使用以下核心定义作为出发点：\n- logistic sigmoid 函数为 $\\sigma(t) = \\frac{1}{1+\\exp(-t)}$。\n- 具有两个负样本的单样本对数似然目标是\n$$\nL(u,v,n_1,n_2) \\;=\\; \\ln\\!\\big(\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)\\big) \\;+\\; \\sum_{i=1}^{2} \\ln\\!\\big(\\sigma(-\\,\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})\\big).\n$$\n\n假设当前的嵌入和学习率如下：\n- $\\mathbf{z}_u = (1,0)$，\n- $\\mathbf{z}'_v = (0,1)$，\n- $\\mathbf{z}'_{n_1} = (1,1)$，\n- $\\mathbf{z}'_{n_2} = (-1,0)$，\n- 学习率 $\\eta = 0.05$。\n\n在此步骤中，将 $\\mathbf{z}'_v$、$\\mathbf{z}'_{n_1}$ 和 $\\mathbf{z}'_{n_2}$ 视为常数。从给定的 $\\mathbf{z}_u$ 开始，对 $L(u,v,n_1,n_2)$ 关于 $\\mathbf{z}_u$ 执行一步随机梯度上升。计算此单步更新后的源嵌入向量 $\\mathbf{z}_u^{\\text{new}}$。将最终向量的每个分量四舍五入到六位有效数字。以单个行向量的形式提供你的最终答案。", "solution": "本题的目标是执行一步随机梯度上升来更新源节点嵌入 $\\mathbf{z}_u$。随机梯度上升的更新规则由下式给出：\n$$\n\\mathbf{z}_u^{\\text{new}} = \\mathbf{z}_u + \\eta \\nabla_{\\mathbf{z}_u} L\n$$\n其中 $\\eta$ 是学习率，$\\nabla_{\\mathbf{z}_u} L$ 是对数似然目标函数 $L$ 关于源嵌入 $\\mathbf{z}_u$ 的梯度。\n\n对数似然目标函数如下所示：\n$$\nL(u,v,n_1,n_2) = \\ln(\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)) + \\sum_{i=1}^{2} \\ln(\\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i}))\n$$\n\n首先，我们必须计算梯度 $\\nabla_{\\mathbf{z}_u} L$。我们将分别计算 $L$ 中每一项的梯度。logistic sigmoid 函数 $\\sigma(t) = (1+\\exp(-t))^{-1}$ 的导数是 $\\sigma'(t) = \\sigma(t)(1-\\sigma(t))$。\n\n对于任何标量值函数 $f(x)$，$\\ln(f(x))$ 的导数是 $\\frac{f'(x)}{f(x)}$。因此，$\\ln(\\sigma(t))$ 关于 $t$ 的导数是：\n$$\n\\frac{d}{dt}\\ln(\\sigma(t)) = \\frac{\\sigma'(t)}{\\sigma(t)} = \\frac{\\sigma(t)(1-\\sigma(t))}{\\sigma(t)} = 1-\\sigma(t)\n$$\n\n现在，我们应用链式法则来求 $L$ 中每一项关于向量 $\\mathbf{z}_u$ 的梯度。\n\n1.  **正样本项的梯度**：\n    设第一项为 $L_{pos} = \\ln(\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v))$。使用链式法则，其梯度为：\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{pos} = \\frac{d}{d(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)} \\ln(\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)) \\cdot \\nabla_{\\mathbf{z}_u}(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)\n    $$\n    这可以简化为：\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{pos} = (1 - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)) \\mathbf{z}'_v\n    $$\n\n2.  **负样本项的梯度**：\n    设一个负样本项为 $L_{neg,i} = \\ln(\\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i}))$。使用链式法则，其梯度为：\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{neg,i} = \\frac{d}{d(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})} \\ln(\\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})) \\cdot \\nabla_{\\mathbf{z}_u}(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})\n    $$\n    这可以简化为：\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{neg,i} = (1 - \\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})) (-\\mathbf{z}'_{n_i})\n    $$\n    使用恒等式 $\\sigma(-x) = 1 - \\sigma(x)$，我们可以将项 $(1 - \\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i}))$ 重写为 $1 - (1 - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})) = \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})$。因此，一个负样本项的梯度为：\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{neg,i} = -\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})\\mathbf{z}'_{n_i}\n    $$\n\n结合这些结果，$L$ 的完整梯度是：\n$$\n\\nabla_{\\mathbf{z}_u} L = (1 - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v))\\mathbf{z}'_v - \\sum_{i=1}^{2} \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})\\mathbf{z}'_{n_i}\n$$\n$$\n\\nabla_{\\mathbf{z}_u} L = (1 - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v))\\mathbf{z}'_v - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_1})\\mathbf{z}'_{n_1} - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_2})\\mathbf{z}'_{n_2}\n$$\n\n接下来，我们代入给定的数值：\n$\\mathbf{z}_u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$\\mathbf{z}'_v = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，$\\mathbf{z}'_{n_1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，$\\mathbf{z}'_{n_2} = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$。\n\n我们计算点积：\n- $\\mathbf{z}_u^{\\top}\\mathbf{z}'_v = (1)(0) + (0)(1) = 0$\n- $\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_1} = (1)(1) + (0)(1) = 1$\n- $\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_2} = (1)(-1) + (0)(0) = -1$\n\n现在，我们对这些值求 sigmoid 函数的值：\n- $\\sigma(0) = \\frac{1}{1+\\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2}$\n- $\\sigma(1) = \\frac{1}{1+\\exp(-1)}$\n- $\\sigma(-1) = \\frac{1}{1+\\exp(1)}$\n\n将这些值代入梯度表达式：\n$$\n\\nabla_{\\mathbf{z}_u} L = \\left(1 - \\frac{1}{2}\\right)\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - \\frac{1}{1+\\exp(-1)}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{1+\\exp(1)}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\nabla_{\\mathbf{z}_u} L = \\frac{1}{2}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - \\frac{1}{1+\\exp(-1)}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{1+\\exp(1)}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n$$\n让我们求梯度向量的分量：\nx 分量：\n$0 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{1+\\exp(-1)} - (-1) \\cdot \\frac{1}{1+\\exp(1)} = -\\frac{1}{1+\\exp(-1)} + \\frac{1}{1+\\exp(1)}$\n使用 $\\exp(-1) = 1/e$，这变为 $-\\frac{1}{1+1/e} + \\frac{1}{1+e} = -\\frac{e}{e+1} + \\frac{1}{e+1} = \\frac{1-e}{e+1}$。\ny 分量：\n$1 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{1+\\exp(-1)} - 0 \\cdot \\frac{1}{1+\\exp(1)} = \\frac{1}{2} - \\frac{e}{e+1} = \\frac{e+1-2e}{2(e+1)} = \\frac{1-e}{2(e+1)}$。\n\n所以，梯度向量是：\n$$\n\\nabla_{\\mathbf{z}_u} L = \\begin{pmatrix} \\frac{1-e}{e+1} \\\\ \\frac{1-e}{2(e+1)} \\end{pmatrix}\n$$\n在数值上，$\\frac{1-e}{e+1} \\approx \\frac{1-2.7182818}{1+2.7182818} \\approx -0.46211716$。\n所以，$\\nabla_{\\mathbf{z}_u} L \\approx \\begin{pmatrix} -0.46211716 \\\\ -0.23105858 \\end{pmatrix}$。\n\n现在我们用 $\\eta = 0.05$ 执行更新步骤：\n$$\n\\mathbf{z}_u^{\\text{new}} = \\mathbf{z}_u + \\eta \\nabla_{\\mathbf{z}_u} L = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 0.05 \\begin{pmatrix} \\frac{1-e}{e+1} \\\\ \\frac{1-e}{2(e+1)} \\end{pmatrix}\n$$\n$$\n\\mathbf{z}_u^{\\text{new}} = \\begin{pmatrix} 1 + 0.05\\left(\\frac{1-e}{e+1}\\right) \\\\ 0.05\\left(\\frac{1-e}{2(e+1)}\\right) \\end{pmatrix}\n$$\n计算各分量的数值：\nx 分量： $1 + 0.05 \\times (-0.46211716) = 1 - 0.023105858 \\approx 0.976894142$。\ny 分量： $0.05 \\times (-0.23105858) = -0.011552929$。\n\n将每个分量四舍五入到六位有效数字：\n- 第一个分量 $0.976894142$ 变为 $0.976894$。\n- 第二个分量 $-0.011552929$ 变为 $-0.0115529$。\n\n更新后的源嵌入向量是 $\\mathbf{z}_u^{\\text{new}} = (0.976894, -0.0115529)$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.976894  -0.0115529\n\\end{pmatrix}\n}\n$$", "id": "3331408"}, {"introduction": "在真实的生物网络中，某些节点（如蛋白质“枢纽”）的度非常高，这可能导致它们在随机游走语料中被过度表示。本练习将探讨一种重要的二次采样技术来解决此问题，要求你推导并应用节点丢弃概率公式。这有助于你理解算法如何减轻高频节点的过度影响，以学习到更具信息量的嵌入。[@problem_id:3331429]", "problem": "考虑一个在计算系统生物学中对蛋白质-蛋白质相互作用网络使用 node2vec 的节点嵌入工作流。Node2vec 通过有偏随机游走生成节点序列，这些序列随后用于训练一个 Skip-Gram with Negative Sampling (SGNS) 模型。将所有游走产生的所有节点标记的多重集视为一个语料库，并令节点标记的经验相对频率表示为 $f(w) \\in (0,1)$，其中 $w$ 索引一个节点。为了限制非常频繁的节点（例如，高度数中心节点）的主导作用，在 SGNS 训练之前会应用一个亚采样过程。该过程以概率 $P_{\\text{discard}}(w)$ 随机丢弃一个标记 $w$，该概率仅取决于 $f(w)$ 和一个阈值 $t \\in (0,1)$。\n\n从以下基于表示学习中语料库亚采样的设计原则出发：\n1. 亚采样在不同标记间是独立的，因此期望保留相对频率满足 $f_{\\text{ret}}(w)=f(w)\\,a(w)$，其中 $a(w)$ 是接受概率。\n2. 稀有节点不应被降采样：如果 $f(w) \\le t$，则 $a(w)=1$。\n3. 对于满足 $f(w) \\ge t$ 的频繁节点，保留的相对频率应压缩动态范围，同时保留数量级上的顺序，具体满足 $f_{\\text{ret}}(w)=\\sqrt{t\\,f(w)}$。\n\n(a) 推导出与上述原则一致的 $P_{\\text{discard}}(w)$ 作为 $f(w)$ 和 $t$ 的函数的闭式表达式。\n\n(b) 利用在无向网络上进行无偏随机游走时，平稳访问概率满足 $\\pi(w) \\propto d(w)$（其中 $d(w)$ 是节点 $w$ 的度）这一事实，用一到三句话解释，与不进行亚采样相比，推导出的亚采样规则如何改变来自高度数中心节点对 SGNS 训练的期望贡献的缩放关系。\n\n(c) 在一个具体场景中，假设亚采样阈值为 $t=1.0 \\times 10^{-4}$，一个中心节点 $H$ 在语料库中出现的经验相对频率为 $f(H)=1.6 \\times 10^{-3}$。计算 $P_{\\text{discard}}(H)$ 的数值。给出最终答案，一个实数即可；无需单位。", "solution": "我们分步解决这个问题。\n\n**(a) 推导 $P_{\\text{discard}}(w)$**\n\n首先，我们将问题中给出的三个设计原则形式化。令 $a(w)$ 为接受（保留）一个节点标记 $w$ 的概率，则丢弃概率为 $P_{\\text{discard}}(w) = 1 - a(w)$。根据原则 1，保留后的期望相对频率为 $f_{\\text{ret}}(w) = f(w) \\cdot a(w)$。\n\n- **情况 1：稀有节点 ($f(w) \\le t$)**\n根据原则 2，稀有节点不应被降采样，因此接受概率 $a(w) = 1$。所以，丢弃概率为 $P_{\\text{discard}}(w) = 1 - a(w) = 1 - 1 = 0$。\n\n- **情况 2：频繁节点 ($f(w) > t$)**\n根据原则 3，对于频繁节点，保留后的相对频率被设定为 $f_{\\text{ret}}(w) = \\sqrt{t \\cdot f(w)}$。我们将此与原则 1 结合：\n$$\nf(w) \\cdot a(w) = \\sqrt{t \\cdot f(w)}\n$$\n从中解出接受概率 $a(w)$：\n$$\na(w) = \\frac{\\sqrt{t \\cdot f(w)}}{f(w)} = \\sqrt{\\frac{t \\cdot f(w)}{f(w)^2}} = \\sqrt{\\frac{t}{f(w)}}\n$$\n因此，丢弃概率为：\n$$\nP_{\\text{discard}}(w) = 1 - a(w) = 1 - \\sqrt{\\frac{t}{f(w)}}\n$$\n\n将两种情况合并，我们可以得到 $P_{\\text{discard}}(w)$ 的闭式表达式：\n$$\nP_{\\text{discard}}(w) = \\max\\left(0, 1 - \\sqrt{\\frac{t}{f(w)}}\\right)\n$$\n这个表达式优雅地涵盖了两种情况：当 $f(w) \\le t$ 时，$\\sqrt{t/f(w)} \\ge 1$，所以 $1 - \\sqrt{t/f(w)} \\le 0$，最大值取 $0$。当 $f(w) > t$ 时，表达式即为我们推导出的公式。\n\n**(b) 解释对高度数中心节点贡献的缩放关系变化**\n\n在无偏随机游走中，节点的平稳访问概率（即其在语料库中的期望频率 $f(w)$）与其度 $d(w)$ 成正比，即 $f(w) \\propto d(w)$。在不进行亚采样时，一个节点对 SGNS 训练的贡献与其在语料库中的频率成正比，因此与它的度成线性关系，缩放关系为 $O(d(w))$。应用我们推导出的亚采样规则后，对于度很高的频繁节点（满足 $f(w) > t$），其被保留的频率变为 $f_{\\text{ret}}(w) = \\sqrt{t \\cdot f(w)}$。由于 $f(w) \\propto d(w)$，保留频率现在与 $\\sqrt{d(w)}$ 成正比，即 $f_{\\text{ret}}(w) \\propto \\sqrt{d(w)}$。因此，亚采样将高度数中心节点对训练贡献的缩放关系从线性（$O(d(w))$）降低到了次线性（$O(\\sqrt{d(w)})$），从而减轻了它们不成比例的影响。\n\n**(c) 计算 $P_{\\text{discard}}(H)$**\n\n给定亚采样阈值 $t = 1.0 \\times 10^{-4}$ 和中心节点 $H$ 的经验相对频率 $f(H) = 1.6 \\times 10^{-3}$。\n首先，我们比较 $f(H)$ 和 $t$：\n$1.6 \\times 10^{-3} > 1.0 \\times 10^{-4}$，所以节点 $H$ 是一个频繁节点。\n我们使用为频繁节点推导的公式：\n$$\nP_{\\text{discard}}(H) = 1 - \\sqrt{\\frac{t}{f(H)}}\n$$\n代入数值：\n$$\nP_{\\text{discard}}(H) = 1 - \\sqrt{\\frac{1.0 \\times 10^{-4}}{1.6 \\times 10^{-3}}} = 1 - \\sqrt{\\frac{1}{16}} = 1 - \\frac{1}{4} = 0.75\n$$\n因此，丢弃中心节点 $H$ 的一个标记的概率是 0.75。", "answer": "$$\\boxed{0.75}$$", "id": "3331429"}]}