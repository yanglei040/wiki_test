{"hands_on_practices": [{"introduction": "本练习将循环神经网络（RNN）分解为其最基本的组成部分：单步状态转移。通过手动计算隐藏状态的更新，您将对RNN如何按时间顺序处理信息有一个具体的理解，这是所有更复杂的循环架构的基础构件。[@problem_id:3344978]", "problem": "在计算系统生物学中，循环神经网络（Recurrent Neural Network (RNN)）被用于建模生物时间序列，例如基因表达动态。考虑一个代表两个转录调控模块活动的最小二维隐藏状态。该系统是一个离散时间非线性状态空间模型，其中下一个隐藏状态是通过对当前输入和先前隐藏状态的仿射组合应用逐元素的饱和非线性函数 $\\phi$ 来生成的，而单一标量输出则由当前隐藏状态的线性读出产生。饱和非线性函数是双曲正切函数 $\\phi(u) = \\tanh(u)$，逐元素应用。\n\n假设在时间索引 $t$ 处的参数化如下：\n- 输入到隐藏层的矩阵 $W_{xh} = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}$，\n- 隐藏层到隐藏层的矩阵 $W_{hh} = \\begin{pmatrix}0.5  0 \\\\ 0  0.8\\end{pmatrix}$，\n- 偏置向量 $b = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$，\n- 输入向量 $x_t = \\begin{pmatrix}0.2 \\\\ -0.1\\end{pmatrix}$，\n- 先前的隐藏状态 $h_{t-1} = \\begin{pmatrix}0.0 \\\\ 0.5\\end{pmatrix}$，\n- 隐藏层到输出的读出矩阵 $W_{hy} = \\begin{pmatrix}1  -1\\end{pmatrix}$，\n- 输出偏置 $c = 0$。\n\n从具有逐元素饱和激活和线性读出的离散时间非线性状态空间模型的核心定义出发，为此 RNN 构建合适的隐藏状态更新和输出映射。然后，计算给定参数下的当前隐藏状态 $h_t$ 和标量输出 $y_t$ 的数值。将 $h_t$ 和 $y_t$ 的每个分量四舍五入到六位有效数字。这些值是无量纲的归一化量。将您的最终答案表示为单个行向量 $\\begin{pmatrix}h_{t,1}  h_{t,2}  y_t\\end{pmatrix}$。", "solution": "问题要求在给定一组参数、先前的隐藏状态 $h_{t-1}$ 和当前输入 $x_t$ 的情况下，计算一个简单的循环神经网络 (RNN) 的当前隐藏状态 $h_t$ 和标量输出 $y_t$。该模型由一个离散时间非线性状态空间公式定义。\n\n首先，我们根据问题描述建立 RNN 的控制方程。隐藏状态更新规则是通过对输入 $x_t$ 和先前隐藏状态 $h_{t-1}$ 的仿射变换应用逐元素的激活函数 $\\phi$ 来给出的。一般方程为：\n$$h_t = \\phi(W_{xh}x_t + W_{hh}h_{t-1} + b)$$\n输出 $y_t$ 是当前隐藏状态 $h_t$ 的线性读出：\n$$y_t = W_{hy}h_t + c$$\n\n问题指定激活函数为双曲正切函数 $\\phi(u) = \\tanh(u)$，逐元素应用。给定的参数是：\n- 输入到隐藏层的矩阵: $W_{xh} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$\n- 隐藏层到隐藏层的矩阵: $W_{hh} = \\begin{pmatrix} 0.5  0 \\\\ 0  0.8 \\end{pmatrix}$\n- 偏置向量: $b = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n- 输入向量: $x_t = \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix}$\n- 先前的隐藏状态: $h_{t-1} = \\begin{pmatrix} 0.0 \\\\ 0.5 \\end{pmatrix}$\n- 隐藏层到输出的读出矩阵: $W_{hy} = \\begin{pmatrix} 1  -1 \\end{pmatrix}$\n- 输出偏置: $c = 0$\n\n我们的第一步是计算激活函数的参数，即仿射组合 $a_t = W_{xh}x_t + W_{hh}h_{t-1} + b$。\n$$a_t = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix} + \\begin{pmatrix} 0.5  0 \\\\ 0  0.8 \\end{pmatrix} \\begin{pmatrix} 0.0 \\\\ 0.5 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n我们分别计算矩阵-向量乘积：\n$$W_{xh}x_t = \\begin{pmatrix} (1)(0.2) + (0)(-0.1) \\\\ (0)(0.2) + (1)(-0.1) \\end{pmatrix} = \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix}$$\n$$W_{hh}h_{t-1} = \\begin{pmatrix} (0.5)(0.0) + (0)(0.5) \\\\ (0)(0.0) + (0.8)(0.5) \\end{pmatrix} = \\begin{pmatrix} 0.0 \\\\ 0.4 \\end{pmatrix}$$\n现在，我们将各项相加得到 $a_t$：\n$$a_t = \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix} + \\begin{pmatrix} 0.0 \\\\ 0.4 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0.2 + 0.0 + 0 \\\\ -0.1 + 0.4 + 0 \\end{pmatrix} = \\begin{pmatrix} 0.2 \\\\ 0.3 \\end{pmatrix}$$\n\n接下来，我们通过应用逐元素的双曲正切函数来计算当前隐藏状态 $h_t$，$h_t = \\phi(a_t) = \\tanh(a_t)$。\n$$h_t = \\begin{pmatrix} \\tanh(0.2) \\\\ \\tanh(0.3) \\end{pmatrix}$$\n我们计算 $h_t$ 各分量的数值：\n$$h_{t,1} = \\tanh(0.2) \\approx 0.19737532...$$\n$$h_{t,2} = \\tanh(0.3) \\approx 0.29131261...$$\n根据问题陈述，我们将这些值四舍五入到六位有效数字：\n$$h_{t,1} \\approx 0.197375$$\n$$h_{t,2} \\approx 0.291313$$\n所以，当前隐藏状态为 $h_t \\approx \\begin{pmatrix} 0.197375 \\\\ 0.291313 \\end{pmatrix}$。\n\n最后，我们使用线性读出方程 $y_t = W_{hy}h_t + c$ 计算标量输出 $y_t$。\n$$y_t = \\begin{pmatrix} 1  -1 \\end{pmatrix} h_t + 0$$\n为了获得更好的精度，我们在进行最终四舍五入之前，在此计算中使用 $h_t$ 的未四舍五入的值。\n$$y_t = (1) \\times h_{t,1} + (-1) \\times h_{t,2} = h_{t,1} - h_{t,2}$$\n$$y_t \\approx 0.19737532 - 0.29131261 = -0.09393729...$$\n将此结果四舍五入到六位有效数字：\n$$y_t \\approx -0.0939373$$\n\n问题要求将最终答案表示为单个行向量 $\\begin{pmatrix} h_{t,1}  h_{t,2}  y_t \\end{pmatrix}$。使用四舍五入后的值，该向量为：\n$$\\begin{pmatrix} 0.197375  0.291313  -0.0939373 \\end{pmatrix}$$", "answer": "$$\\boxed{\\begin{pmatrix} 0.197375  0.291313  -0.0939373 \\end{pmatrix}}$$", "id": "3344978"}, {"introduction": "简单的RNN在处理长期依赖性方面存在困难。本练习将探讨门控循环单元（Gated Recurrent Unit, GRU），这是一种使用门控机制来控制信息流的先进架构。您将计算一个GRU的状态更新，更重要的是，解释门控如何决定保留哪些过去的信息以及整合哪些新信息，这是对生物记忆进行建模的一个关键概念。[@problem_id:3344975]", "problem": "一个带有门控循环单元（GRU）架构的二维循环神经网络（RNN）被用来为一个生物时间序列建模，该序列捕捉了在时变刺激下两个相互作用的基因表达模块的联合动态。在时间 $t$，GRU 的隐藏状态更新使用逐元素门控机制，将过去的信息与一个受刺激调制的候选状态相结合。候选隐藏状态是通过将双曲正切非线性函数应用于当前输入贡献和一个经重置门控的循环贡献的仿射和来计算的。具体来说，考虑在时间 $t$ 的以下量：\n- 更新门向量 $z_t = \\begin{bmatrix}0.1\\\\0.9\\end{bmatrix}$，\n- 重置门向量 $r_t = \\begin{bmatrix}0.5\\\\0.2\\end{bmatrix}$，\n- 输入到隐藏层的贡献 $W_{xh}x_t = \\begin{bmatrix}0.3\\\\-0.1\\end{bmatrix}$，\n- 隐藏层到隐藏层的贡献 $W_{hh}h_{t-1} = \\begin{bmatrix}0.4\\\\0.2\\end{bmatrix}$，\n- 上一个隐藏状态 $h_{t-1} = \\begin{bmatrix}0.2\\\\0.8\\end{bmatrix}$。\n\n假设标准的 GRU 逐元素更新，其中候选隐藏状态由 $\\tilde{h}_t = \\tanh\\!\\big(W_{xh}x_t + r_t \\odot (W_{hh}h_{t-1})\\big)$ 给出，新的隐藏状态为 $h_t = (1 - z_t)\\odot h_{t-1} + z_t \\odot \\tilde{h}_t$，其中 $\\odot$ 表示哈达玛（逐元素）积，$\\tanh$ 函数逐元素应用。\n\n任务：\n1. 计算候选隐藏状态 $\\tilde{h}_t$。\n2. 计算更新后的隐藏状态 $h_t$。\n3. 根据隐藏状态更新中的凸组合系数，确定保留了更多过去信息的维度索引 $j \\in \\{1,2\\}$，这被解释为在 $(1 - z_t)$ 中具有较大系数的维度。\n\n将所有数值四舍五入到四位有效数字。以单行矩阵的形式报告最终答案，顺序为 $\\big(\\tilde{h}_{t,1}, \\tilde{h}_{t,2}, h_{t,1}, h_{t,2}, j\\big)$，其中 $j$ 以普通整数形式给出。", "solution": "该问题要求计算候选隐藏状态 $\\tilde{h}_t$、新的隐藏状态 $h_t$，并为一个门控循环单元（GRU）确定保留更多过去信息的维度。该问题定义明确，有科学依据，并包含获得唯一解所需的所有信息。\n\n给定的量为：\n- 更新门向量: $z_t = \\begin{bmatrix}0.1\\\\0.9\\end{bmatrix}$\n- 重置门向量: $r_t = \\begin{bmatrix}0.5\\\\0.2\\end{bmatrix}$\n- 输入到隐藏层的贡献: $W_{xh}x_t = \\begin{bmatrix}0.3\\\\-0.1\\end{bmatrix}$\n- 隐藏层到隐藏层的贡献: $W_{hh}h_{t-1} = \\begin{bmatrix}0.4\\\\0.2\\end{bmatrix}$\n- 上一个隐藏状态: $h_{t-1} = \\begin{bmatrix}0.2\\\\0.8\\end{bmatrix}$\n\n更新方程为：\n- 候选隐藏状态: $\\tilde{h}_t = \\tanh\\!\\big(W_{xh}x_t + r_t \\odot (W_{hh}h_{t-1})\\big)$\n- 新的隐藏状态: $h_t = (1 - z_t)\\odot h_{t-1} + z_t \\odot \\tilde{h}_t$\n\n在此，$\\odot$ 表示逐元素的哈达玛积，$\\tanh$ 函数被逐元素应用。所有最终数值都将四舍五入到四位有效数字。\n\n**任务1：计算候选隐藏状态 $\\tilde{h}_t$。**\n\n首先，我们计算经重置门控的循环贡献，即 $r_t \\odot (W_{hh}h_{t-1})$。\n$$\nr_t \\odot (W_{hh}h_{t-1}) = \\begin{bmatrix}0.5\\\\0.2\\end{bmatrix} \\odot \\begin{bmatrix}0.4\\\\0.2\\end{bmatrix} = \\begin{bmatrix}0.5 \\times 0.4\\\\0.2 \\times 0.2\\end{bmatrix} = \\begin{bmatrix}0.2\\\\0.04\\end{bmatrix}\n$$\n接下来，我们将输入贡献 $W_{xh}x_t$ 加到这个结果上。\n$$\nW_{xh}x_t + r_t \\odot (W_{hh}h_{t-1}) = \\begin{bmatrix}0.3\\\\-0.1\\end{bmatrix} + \\begin{bmatrix}0.2\\\\0.04\\end{bmatrix} = \\begin{bmatrix}0.3 + 0.2\\\\-0.1 + 0.04\\end{bmatrix} = \\begin{bmatrix}0.5\\\\-0.06\\end{bmatrix}\n$$\n最后，我们逐元素应用双曲正切函数 $\\tanh$ 来获得 $\\tilde{h}_t$。\n$$\n\\tilde{h}_t = \\tanh\\left(\\begin{bmatrix}0.5\\\\-0.06\\end{bmatrix}\\right) = \\begin{bmatrix}\\tanh(0.5)\\\\\\tanh(-0.06)\\end{bmatrix}\n$$\n我们计算数值并四舍五入到四位有效数字。\n$$\n\\tanh(0.5) \\approx 0.462117... \\approx 0.4621\n$$\n$$\n\\tanh(-0.06) \\approx -0.059928... \\approx -0.05993\n$$\n因此，候选隐藏状态为：\n$$\n\\tilde{h}_t = \\begin{bmatrix}0.4621\\\\-0.05993\\end{bmatrix}\n$$\n\n**任务2：计算更新后的隐藏状态 $h_t$。**\n\n更新方程为 $h_t = (1 - z_t)\\odot h_{t-1} + z_t \\odot \\tilde{h}_t$。该方程表示一个凸组合，其中更新门 $z_t$ 在前一个状态 $h_{t-1}$ 和候选状态 $\\tilde{h}_t$ 之间进行插值。\n\n首先，我们计算向量 $(1 - z_t)$。\n$$\n1 - z_t = 1 - \\begin{bmatrix}0.1\\\\0.9\\end{bmatrix} = \\begin{bmatrix}1 - 0.1\\\\1 - 0.9\\end{bmatrix} = \\begin{bmatrix}0.9\\\\0.1\\end{bmatrix}\n$$\n现在我们分别计算和的两个项。第一项，代表来自过去的贡献，是：\n$$\n(1 - z_t) \\odot h_{t-1} = \\begin{bmatrix}0.9\\\\0.1\\end{bmatrix} \\odot \\begin{bmatrix}0.2\\\\0.8\\end{bmatrix} = \\begin{bmatrix}0.9 \\times 0.2\\\\0.1 \\times 0.8\\end{bmatrix} = \\begin{bmatrix}0.18\\\\0.08\\end{bmatrix}\n$$\n第二项，代表来自新候选状态的贡献，是：\n$$\nz_t \\odot \\tilde{h}_t = \\begin{bmatrix}0.1\\\\0.9\\end{bmatrix} \\odot \\begin{bmatrix}0.4621\\\\-0.05993\\end{bmatrix} = \\begin{bmatrix}0.1 \\times 0.4621\\\\0.9 \\times (-0.05993)\\end{bmatrix} = \\begin{bmatrix}0.04621\\\\-0.053937\\end{bmatrix}\n$$\n最后，我们将这两项相加得到新的隐藏状态 $h_t$。\n$$\nh_t = \\begin{bmatrix}0.18\\\\0.08\\end{bmatrix} + \\begin{bmatrix}0.04621\\\\-0.053937\\end{bmatrix} = \\begin{bmatrix}0.18 + 0.04621\\\\0.08 - 0.053937\\end{bmatrix} = \\begin{bmatrix}0.22621\\\\0.026063\\end{bmatrix}\n$$\n将每个分量四舍五入到四位有效数字：\n$$\nh_t = \\begin{bmatrix}0.2262\\\\0.02606\\end{bmatrix}\n$$\n\n**任务3：确定保留更多过去信息的维度索引 $j$。**\n\n从前一个隐藏状态 $h_{t-1}$ 保留的信息量由系数向量 $(1 - z_t)$ 控制。问题将保留更多过去信息的维度定义为该向量中具有较大系数的维度。我们已经计算出：\n$$\n1 - z_t = \\begin{bmatrix}0.9\\\\0.1\\end{bmatrix}\n$$\n第一个维度（$j=1$）的系数是 $(1-z_t)_1 = 0.9$。\n第二个维度（$j=2$）的系数是 $(1-z_t)_2 = 0.1$。\n比较这两个值，我们发现 $0.9 > 0.1$。因此，第一个维度保留了更多的过去信息。索引是 $j=1$。\n\n最终答案由 $\\tilde{h}_t$ 的元素、$h_t$ 的元素和索引 $j$ 组成，排列成一个单行矩阵：$(\\tilde{h}_{t,1}, \\tilde{h}_{t,2}, h_{t,1}, h_{t,2}, j)$。\n- $\\tilde{h}_{t,1} = 0.4621$\n- $\\tilde{h}_{t,2} = -0.05993$\n- $h_{t,1} = 0.2262$\n- $h_{t,2} = 0.02606$\n- $j = 1$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.4621  -0.05993  0.2262  0.02606  1 \\end{pmatrix}}\n$$", "id": "3344975"}, {"introduction": "只有当RNN能够被训练来模拟真实数据时，它才是有用的。这最后一个练习弥合了网络输出与单细胞基因组学中典型的含噪声、过度离散的计数数据之间的鸿沟。您将构建一个完整的训练目标，它结合了统计上合适的似然函数（负二项分布）和一个鼓励生物学上合理的平滑动态的正则化项。[@problem_id:3344950]", "problem": "您正在使用循环神经网络 (RNN) 对单细胞核糖核酸测序 (scRNA-seq) 中单个基因的计数轨迹进行建模。在每个由 $t \\in \\{1,2,\\dots,T\\}$ 索引的离散时间点，RNN 输出一个非负均值参数 $ \\mu_t $ 和一个共享的离散参数 $ r > 0 $。观测到的计数为 $ y_t \\in \\{0,1,2,\\dots\\} $。在 scRNA-seq 中，一个被广泛接受的计数生成模型是负二项分布，它由均值和离散度参数化，能够捕捉泊松模型无法解释的过度离散现象。\n\n从负二项分布作为非负整数上的计数分布的基本定义出发，其参数化产生的期望为 $ \\mathbb{E}[Y] = \\mu $，方差为 $ \\operatorname{Var}(Y) = \\mu + \\mu^2 / r $，请推导单个时间点的对数似然 $ \\ell(\\mu_t, r; y_t) $ 的表达式，然后对于给定的序列 $ \\{y_t\\}_{t=1}^{T} $ 计算总对数似然 $ \\sum_{t=1}^{T} \\ell(\\mu_t, r; y_t) $。请使用通过伽马函数实现的、与实值 $ r > 0 $ 一致的表示方法，以确保离散度 $ r $ 不被限制为整数值。\n\n为了鼓励生成符合生物学合理性的轨迹，请基于源于经典正则化理论的离散时间有限差分算子，对 RNN 的均值序列 $ \\{\\mu_t\\}_{t=1}^{T} $ 提出一个平滑度惩罚项。实现二阶差分惩罚\n$$\nS_2(\\mu) \\;=\\; \\sum_{t=3}^{T} \\left( \\mu_t - 2 \\mu_{t-1} + \\mu_{t-2} \\right)^2,\n$$\n该惩罚项对离散曲率的快速变化进行惩罚，并倾向于平滑变化的 $ \\mu_t $。构建带惩罚的目标函数\n$$\nJ(\\mu, r; y) \\;=\\; \\sum_{t=1}^{T} \\ell(\\mu_t, r; y_t) \\;-\\; \\lambda \\, S_2(\\mu),\n$$\n其中 $ \\lambda \\ge 0 $ 是一个用户指定的正则化强度。\n\n您的程序必须为下面的每个测试用例计算一个三元组，该三元组由总对数似然 $ \\sum_{t=1}^{T} \\ell(\\mu_t, r; y_t) $、平滑度惩罚 $ S_2(\\mu) $ 和带惩罚的目标 $ J(\\mu, r; y) $ 组成。所有输出均为实数。不涉及物理单位。不使用角度。\n\n测试套件（每个用例指定 $ y $、$ \\mu $、$ r $ 和 $ \\lambda $）：\n- 用例 $1$（通用一致性）：$ y = [3,4,5,6,7] $，$ \\mu = [3.2,3.8,4.5,5.5,6.6] $，$ r = 10.0 $，$ \\lambda = 0.1 $。\n- 用例 $2$（接近泊松极限和零计数）：$ y = [0,0,1,0,2] $，$ \\mu = [0.2,0.3,0.9,0.1,1.8] $，$ r = 100.0 $，$ \\lambda = 0.5 $。\n- 用例 $3$（高度过度离散和突变）：$ y = [1,10,2,12,3] $，$ \\mu = [0.9,9.5,1.2,10.8,2.4] $，$ r = 0.7 $，$ \\lambda = 1.0 $。\n- 用例 $4$（尖峰和在其他地方的小均值）：$ y = [0,1,0,20,0] $，$ \\mu = [0.05,0.8,0.05,15.0,0.05] $，$ r = 2.0 $，$ \\lambda = 2.0 $。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表。列表中的每个元素对应一个测试用例，并且本身也是一个由三个浮点数组成的列表 $ [\\text{LL}, \\text{S}, \\text{J}] $。例如，输出必须类似于 $ [[\\text{LL}_1,\\text{S}_1,\\text{J}_1],[\\text{LL}_2,\\text{S}_2,\\text{J}_2],\\dots] $，且不含任何附加文本。", "solution": "该问题要求为一个由负二项 (NB) 分布建模的计数时间序列计算一个带惩罚的对数似然目标函数。该目标函数由 $J(\\mu, r; y) = \\sum_{t=1}^{T} \\ell(\\mu_t, r; y_t) - \\lambda S_2(\\mu)$ 给出。为了计算此函数，我们必须首先为单时间点对数似然 $\\ell(\\mu_t, r; y_t)$ 推导出一个显式公式，然后实现总对数似然、平滑度惩罚 $S_2(\\mu)$ 和最终目标 $J$ 的计算。\n\n**1. 负二项对数似然 $\\ell(\\mu_t, r; y_t)$ 的推导**\n\n问题通过均值 $\\mathbb{E}[Y] = \\mu$ 和方差 $\\operatorname{Var}(Y) = \\mu + \\mu^2/r$ 来指定负二项分布。这是生物信息学中一种常见的参数化方法。为了找到相应的概率质量函数 (PMF)，我们将此参数化与一个更常见的、由失败次数 $r$ 和成功概率 $p$ 定义的参数化方法联系起来。在 $r$ 次失败前观测到 $k \\in \\{0, 1, 2, \\dots\\}$ 次成功的 PMF 为：\n$$ P(Y=k|r, p) = \\binom{k+r-1}{k} p^k (1-p)^r $$\n对于此参数化，均值和方差分别为 $\\mathbb{E}[Y] = pr/(1-p)$ 和 $\\operatorname{Var}(Y) = pr/(1-p)^2$。\n\n通过将指定的均值 $\\mu$ 与期望的公式等同，我们可以用 $\\mu$ 和 $r$ 来表示 $p$：\n$$ \\mu = \\frac{pr}{1-p} \\implies \\mu(1-p) = pr \\implies \\mu - \\mu p = pr \\implies \\mu = p(\\mu+r) \\implies p = \\frac{\\mu}{\\mu+r} $$\n由此，失败的概率为 $1-p = 1 - \\frac{\\mu}{\\mu+r} = \\frac{r}{\\mu+r}$。\n\n我们可以验证这种重新参数化与给定的方差是一致的：\n$$ \\operatorname{Var}(Y) = \\frac{pr}{(1-p)^2} = \\frac{(\\mu/(\\mu+r))r}{(r/(\\mu+r))^2} = \\frac{\\mu r}{\\mu+r} \\frac{(\\mu+r)^2}{r^2} = \\frac{\\mu(\\mu+r)}{r} = \\mu + \\frac{\\mu^2}{r} $$\n重新参数化是正确的。\n\n问题要求一个针对实值 $r > 0$ 的公式，这可以通过使用伽马函数 $\\Gamma(z)$ 和恒等式 $\\binom{n}{k} = \\frac{\\Gamma(n+1)}{\\Gamma(k+1)\\Gamma(n-k+1)}$ 来表示二项式系数以实现。对于我们的二项式系数，令 $n=k+r-1$（这里用 $y_t$ 表示 $k$）：\n$$ \\binom{y_t+r-1}{y_t} = \\frac{\\Gamma(y_t+r-1+1)}{\\Gamma(y_t+1)\\Gamma(y_t+r-1-y_t+1)} = \\frac{\\Gamma(y_t+r)}{\\Gamma(y_t+1)\\Gamma(r)} $$\n将此式以及 $p$ 和 $1-p$ 的表达式代入均值为 $\\mu_t$ 的观测值 $y_t$ 的 PMF 表达式中，可得：\n$$ P(Y=y_t|\\mu_t, r) = \\frac{\\Gamma(y_t+r)}{\\Gamma(y_t+1)\\Gamma(r)} \\left(\\frac{\\mu_t}{\\mu_t+r}\\right)^{y_t} \\left(\\frac{r}{\\mu_t+r}\\right)^r $$\n因此，单时间点对数似然 $\\ell(\\mu_t, r; y_t) = \\log P(Y=y_t|\\mu_t, r)$ 为：\n$$ \\ell(\\mu_t, r; y_t) = \\log\\left(\\frac{\\Gamma(y_t+r)}{\\Gamma(y_t+1)\\Gamma(r)}\\right) + y_t\\log\\left(\\frac{\\mu_t}{\\mu_t+r}\\right) + r\\log\\left(\\frac{r}{\\mu_t+r}\\right) $$\n利用对数的性质，我们可以将此表达式展开为适合计算的形式。在计算上，使用对数伽马函数 $\\log\\Gamma(z)$ 更为稳定。\n$$ \\ell(\\mu_t, r; y_t) = \\log\\Gamma(y_t+r) - \\log\\Gamma(y_t+1) - \\log\\Gamma(r) + y_t(\\log\\mu_t - \\log(\\mu_t+r)) + r(\\log r - \\log(\\mu_t+r)) $$\n合并包含 $\\log(\\mu_t+r)$ 的项，得到单个时间点的最终表达式：\n$$ \\ell(\\mu_t, r; y_t) = \\log\\Gamma(y_t+r) - \\log\\Gamma(y_t+1) - \\log\\Gamma(r) + y_t\\log\\mu_t + r\\log r - (y_t+r)\\log(\\mu_t+r) $$\n整个时间序列的总对数似然是所有时间点上的总和：\n$$ \\sum_{t=1}^{T} \\ell(\\mu_t, r; y_t) $$\n\n**2. 平滑度惩罚 $S_2(\\mu)$**\n\n问题定义了一个二阶有限差分惩罚，以鼓励均值参数序列 $\\{\\mu_t\\}_{t=1}^{T}$ 的平滑性。该惩罚由下式给出：\n$$ S_2(\\mu) = \\sum_{t=3}^{T} \\left( \\mu_t - 2 \\mu_{t-1} + \\mu_{t-2} \\right)^2 $$\n该项惩罚均值轨迹 $\\mu(t)$ 的二阶导数的离散模拟的大值，倾向于接近线性的轨迹。求和从 $t=3$ 开始，因此它对长度为 $T \\ge 3$ 的时间序列有定义。对于 $T  3$，惩罚为 $0$。\n\n**3. 带惩罚的目标 $J(\\mu, r; y)$**\n\n最终的目标函数结合了总对数似然（衡量对数据的拟合优度）和平滑度惩罚（强制施加关于轨迹形状的先验信念）。一个非负的正则化参数 $\\lambda \\ge 0$ 控制着两者之间的权衡：\n$$ J(\\mu, r; y) = \\sum_{t=1}^{T} \\ell(\\mu_t, r; y_t) - \\lambda S_2(\\mu) $$\n程序将为每个提供的测试用例计算这三个量：总对数似然 $\\sum_{t=1}^{T} \\ell(\\mu_t, r; y_t)$、平滑度惩罚 $S_2(\\mu)$ 和带惩罚的目标 $J(\\mu, r; y)$。该实现将利用 `scipy` 库中数值稳定的对数伽马函数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the results.\n    \"\"\"\n\n    def calculate_objective(y, mu, r, lam):\n        \"\"\"\n        Calculates the total log-likelihood, smoothness penalty, and penalized objective\n        for a single test case.\n\n        Args:\n            y (list of int): The observed counts time series.\n            mu (list of float): The mean parameters time series from the RNN.\n            r (float): The shared dispersion parameter.\n            lam (float): The regularization strength.\n\n        Returns:\n            list of float: A list containing [total_log_likelihood, smoothness_penalty, penalized_objective].\n        \"\"\"\n        # Convert lists to numpy arrays for vectorized operations\n        y = np.array(y, dtype=float)\n        mu = np.array(mu, dtype=float)\n\n        # --- Total Log-Likelihood (LL) ---\n        # The log-likelihood formula is:\n        # LL = log(Gamma(y+r)) - log(Gamma(y+1)) - log(Gamma(r))\n        #      + y*log(mu) + r*log(r) - (y+r)*log(mu+r)\n        \n        # The term y*log(mu) is 0 when y is 0. Using np.where handles this correctly\n        # and avoids potential `0 * -inf = nan` issues if mu could be 0.\n        # The test cases ensure mu > 0, but this is a robust practice.\n        y_log_mu = np.where(y > 0, y * np.log(mu), 0.0)\n\n        log_lik_terms = (gammaln(y + r) -\n                         gammaln(y + 1) -\n                         gammaln(r) +\n                         y_log_mu +\n                         r * np.log(r) -\n                         (y + r) * np.log(mu + r))\n        \n        total_ll = np.sum(log_lik_terms)\n\n        # --- Smoothness Penalty (S2) ---\n        T = len(mu)\n        if T  3:\n            s2_penalty = 0.0\n        else:\n            # S2 = sum_{t=3 to T} (mu_t - 2*mu_{t-1} + mu_{t-2})^2\n            second_diffs = mu[2:] - 2 * mu[1:-1] + mu[:-2]\n            s2_penalty = np.sum(second_diffs**2)\n\n        # --- Penalized Objective (J) ---\n        penalized_objective = total_ll - lam * s2_penalty\n        \n        return [total_ll, s2_penalty, penalized_objective]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (y, mu, r, lambda)\n        ([3, 4, 5, 6, 7], [3.2, 3.8, 4.5, 5.5, 6.6], 10.0, 0.1),\n        # Case 2\n        ([0, 0, 1, 0, 2], [0.2, 0.3, 0.9, 0.1, 1.8], 100.0, 0.5),\n        # Case 3\n        ([1, 10, 2, 12, 3], [0.9, 9.5, 1.2, 10.8, 2.4], 0.7, 1.0),\n        # Case 4\n        ([0, 1, 0, 20, 0], [0.05, 0.8, 0.05, 15.0, 0.05], 2.0, 2.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        y_vals, mu_vals, r_val, lambda_val = case\n        result_triple = calculate_objective(y_vals, mu_vals, r_val, lambda_val)\n        results.append(result_triple)\n\n    # Format the final output string as specified: [[LL1,S1,J1],[LL2,S2,J2],...]\n    formatted_results = []\n    for res in results:\n        # Convert each float in the result triple to a string, join with commas,\n        # and enclose in brackets to form a string like \"[1.23,4.56,7.89]\".\n        formatted_results.append(f\"[{','.join(map(str, res))}]\")\n\n    # Join the formatted strings for each case and enclose in outer brackets.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3344950"}]}