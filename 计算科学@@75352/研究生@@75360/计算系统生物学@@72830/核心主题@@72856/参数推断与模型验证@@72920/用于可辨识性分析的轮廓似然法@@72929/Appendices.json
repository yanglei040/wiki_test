{"hands_on_practices": [{"introduction": "在系统生物学建模中，我们常常假设测量误差服从高斯分布。这个练习是一个基础的“纸笔”推导，旨在通过解析地处理噪声方差这一滋扰参数，来推导出剖面似然函数。这个过程将揭示最大化似然与最小化残差平方和之间的深刻联系，这是模型拟合的基石。[@problem_id:3340975]", "problem": "考虑一个计算系统生物学中的单输出动态模型，该模型由一个常微分方程（ODE）描述，它通过一个光滑的观测函数将参数向量 $\\theta \\in \\mathbb{R}^{p}$ 映射到一个预测的测量轨迹。设观测数据为 $\\{(t_{i}, y_{i})\\}_{i=1}^{n}$，其中 $t_{i}$ 是已知的采样时间，观测模型为\n$$\ny_{i} = f(t_{i}; \\theta) + \\varepsilon_{i},\n$$\n其中测量误差 $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$ 是独立同分布的，$\\sigma^{2} > 0$ 是一个未知的讨厌方差。假设该ODE和观测映射使得 $f(t_{i}; \\theta)$ 对于所有的 $i$ 和参数域中所有的 $\\theta$ 都是良定义且有限的。\n\n从高斯似然的定义和观测的独立性出发，写出数据的对数似然 $\\ell(\\theta, \\sigma^{2})$。然后，对于一个固定的 $\\theta$，在定义域 $\\sigma^{2} > 0$ 上，解析地对 $\\ell(\\theta, \\sigma^{2})$ 关于 $\\sigma^{2}$ 进行最大化，并将最大化算子代回 $\\ell(\\theta, \\sigma^{2})$ 中，以获得仅作为 $\\theta$ 函数的剖面对数似然 $\\tilde{\\ell}(\\theta)$。用残差平方和\n$$\nS(\\theta) = \\sum_{i=1}^{n}\\left(y_{i} - f(t_{i}; \\theta)\\right)^{2}\n$$\n的闭式形式表达您的最终结果。\n提供 $\\tilde{\\ell}(\\theta)$ 的单个解析表达式作为最终答案。不要省略不依赖于 $\\theta$ 的加性常数。您的最终答案应该是一个没有单位且无需四舍五入的单一表达式。", "solution": "首先对问题陈述进行严格的验证过程。\n\n### 步骤 1：提取已知条件\n-   **模型：** 一个单输出动态模型，其中参数向量 $\\theta \\in \\mathbb{R}^{p}$ 映射到一个预测轨迹 $f(t; \\theta)$。\n-   **数据：** 在已知采样时间 $t_i$ 上的 $n$ 个观测值集合 $\\{(t_{i}, y_{i})\\}_{i=1}^{n}$。\n-   **观测模型：** 数据与模型之间的关系由 $y_{i} = f(t_{i}; \\theta) + \\varepsilon_{i}$ 给出。\n-   **误差结构：** 测量误差 $\\varepsilon_{i}$ 是独立同分布（i.i.d.）的，服从均值为 $0$、未知方差为 $\\sigma^{2} > 0$ 的正态分布，记为 $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$。\n-   **函数性质：** 观测映射 $f(t_{i}; \\theta)$ 对于所有相关输入都是良定义且有限的。\n-   **定义量：** 残差平方和定义为 $S(\\theta) = \\sum_{i=1}^{n}\\left(y_{i} - f(t_{i}; \\theta)\\right)^{2}$。\n-   **目标：** 通过首先写出完整对数似然 $\\ell(\\theta, \\sigma^2)$，然后关于 $\\sigma^2$ 解析地最大化它，来推导剖面对数似然 $\\tilde{\\ell}(\\theta)$。最终表达式应以 $S(\\theta)$ 表示并包括所有常数。\n\n### 步骤 2：使用提取的已知条件进行验证\n-   **科学基础：** 该问题具有科学基础。它描述了带有加性高斯噪声的非线性回归模型中参数估计的标准统计框架。这是所有定量STEM领域（包括计算系统生物学）中一个基础且广泛使用的方法论。\n-   **良定性：** 该问题是良定的。它提供了执行所要求的推导所需的所有必要信息和定义。目标清晰明确，能够导出一个唯一的解析解。\n-   **客观性：** 该问题使用精确、客观的数学语言陈述，没有任何主观或有偏见的措辞。\n\n### 步骤 3：结论与行动\n该问题被认为是 **有效的**，因为它科学上合理、良定、客观，并符合有效科学问题的所有标准。现在将开始求解过程。\n\n---\n\n剖面对数似然函数 $\\tilde{\\ell}(\\theta)$ 的推导过程如下。\n\n首先，我们构建似然函数 $L(\\theta, \\sigma^2)$。假设误差 $\\varepsilon_i$ 来自独立同分布的正态分布 $\\mathcal{N}(0, \\sigma^2)$，这意味着每个观测值 $y_i$ 是一个来自均值为 $f(t_i; \\theta)$、方差为 $\\sigma^2$ 的正态分布的随机变量。单个观测值 $y_i$ 的概率密度函数（PDF）为：\n$$\np(y_i | \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - f(t_i; \\theta))^2}{2\\sigma^2} \\right)\n$$\n由于观测值的独立性，整个数据集（包含 $n$ 个观测值）的似然是各个PDF的乘积：\n$$\nL(\\theta, \\sigma^2) = \\prod_{i=1}^{n} p(y_i | \\theta, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - f(t_i; \\theta))^2}{2\\sigma^2} \\right)\n$$\n这个表达式可以整合为：\n$$\nL(\\theta, \\sigma^2) = \\left( \\frac{1}{2\\pi\\sigma^2} \\right)^{n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - f(t_i; \\theta))^2 \\right)\n$$\n使用残差平方和的既定定义 $S(\\theta) = \\sum_{i=1}^{n} (y_i - f(t_i; \\theta))^2$，似然函数简化为：\n$$\nL(\\theta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{S(\\theta)}{2\\sigma^2} \\right)\n$$\n接下来，我们通过取 $L(\\theta, \\sigma^2)$ 的自然对数来确定对数似然函数 $\\ell(\\theta, \\sigma^2)$：\n$$\n\\ell(\\theta, \\sigma^2) = \\ln(L(\\theta, \\sigma^2)) = \\ln\\left( (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{S(\\theta)}{2\\sigma^2} \\right) \\right)\n$$\n利用对数的性质 $\\ln(a \\cdot b) = \\ln(a) + \\ln(b)$ 和 $\\ln(a^b) = b \\ln(a)$，我们得到：\n$$\n\\ell(\\theta, \\sigma^2) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{S(\\theta)}{2\\sigma^2}\n$$\n这可以展开为：\n$$\n\\ell(\\theta, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{S(\\theta)}{2\\sigma^2}\n$$\n为了找到剖面对数似然 $\\tilde{\\ell}(\\theta)$，我们必须首先对于一个固定的参数向量 $\\theta$，就讨厌参数 $\\sigma^2$ 对 $\\ell(\\theta, \\sigma^2)$ 进行最大化。我们通过对 $\\ell(\\theta, \\sigma^2)$ 关于 $\\sigma^2$ 求偏导数并将其设为零来找到临界点：\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = \\frac{\\partial}{\\partial \\sigma^2} \\left( -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{S(\\theta)}{2\\sigma^2} \\right) = -\\frac{n}{2\\sigma^2} + \\frac{S(\\theta)}{2(\\sigma^2)^2}\n$$\n将导数设为零以求得估计量 $\\hat{\\sigma}^2(\\theta)$：\n$$\n-\\frac{n}{2\\hat{\\sigma}^2} + \\frac{S(\\theta)}{2(\\hat{\\sigma}^2)^2} = 0\n$$\n假设 $S(\\theta) > 0$ 从而 $\\hat{\\sigma}^2 > 0$，我们可以乘以 $2(\\hat{\\sigma}^2)^2$：\n$$\n-n\\hat{\\sigma}^2 + S(\\theta) = 0\n$$\n解出 $\\hat{\\sigma}^2$ 得到以 $\\theta$ 为条件的方差的最大似然估计量：\n$$\n\\hat{\\sigma}^2(\\theta) = \\frac{S(\\theta)}{n}\n$$\n为确认这是一个最大值，我们检查二阶导数：\n$$\n\\frac{\\partial^2 \\ell}{(\\partial \\sigma^2)^2} = \\frac{\\partial}{\\partial \\sigma^2} \\left( -\\frac{n}{2}(\\sigma^2)^{-1} + \\frac{S(\\theta)}{2}(\\sigma^2)^{-2} \\right) = \\frac{n}{2}(\\sigma^2)^{-2} - S(\\theta)(\\sigma^2)^{-3} = \\frac{n}{2(\\sigma^2)^2} - \\frac{S(\\theta)}{(\\sigma^2)^3}\n$$\n在 $\\sigma^2 = \\hat{\\sigma}^2(\\theta) = S(\\theta)/n$ 处求值：\n$$\n\\left. \\frac{\\partial^2 \\ell}{(\\partial \\sigma^2)^2} \\right|_{\\sigma^2=\\hat{\\sigma}^2} = \\frac{n}{2(S(\\theta)/n)^2} - \\frac{S(\\theta)}{(S(\\theta)/n)^3} = \\frac{n^3}{2S(\\theta)^2} - \\frac{n^3 S(\\theta)}{S(\\theta)^3} = \\frac{n^3}{2S(\\theta)^2} - \\frac{n^3}{S(\\theta)^2} = -\\frac{n^3}{2S(\\theta)^2}\n$$\n由于对于非平凡拟合有 $n > 0$ 和 $S(\\theta) > 0$，二阶导数为负。这证实了 $\\hat{\\sigma}^2(\\theta) = S(\\theta)/n$ 是一个局部最大值。由于它是定义域 $\\sigma^2 > 0$ 内唯一的临界点，因此它是全局最大值。\n\n最后，将 $\\hat{\\sigma}^2(\\theta)$ 代回对数似然函数 $\\ell(\\theta, \\sigma^2)$，得到剖面对数似然 $\\tilde{\\ell}(\\theta)$：\n$$\n\\tilde{\\ell}(\\theta) = \\ell(\\theta, \\hat{\\sigma}^2(\\theta)) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{S(\\theta)}{n}\\right) - \\frac{S(\\theta)}{2\\left(\\frac{S(\\theta)}{n}\\right)}\n$$\n最后一项简化为 $\\frac{n}{2}$。因此，我们有：\n$$\n\\tilde{\\ell}(\\theta) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{S(\\theta)}{n}\\right) - \\frac{n}{2}\n$$\n这个表达式可以通过合并对数项和常数来进一步紧凑化：\n$$\n\\tilde{\\ell}(\\theta) = -\\frac{n}{2} \\left[ \\ln(2\\pi) + \\ln(S(\\theta)) - \\ln(n) + 1 \\right]\n$$\n使用 $\\ln(e) = 1$ 并合并方括号内的项：\n$$\n\\tilde{\\ell}(\\theta) = -\\frac{n}{2} \\left[ \\ln\\left(\\frac{2\\pi S(\\theta)}{n}\\right) + \\ln(e) \\right]\n$$\n这导出了剖面对数似然的最终闭式表达式：\n$$\n\\tilde{\\ell}(\\theta) = -\\frac{n}{2} \\ln\\left(\\frac{2\\pi e S(\\theta)}{n}\\right)\n$$\n这是所求的关于 $\\theta$ 的函数，所有常数均已保留。", "answer": "$$\n\\boxed{-\\frac{n}{2} \\ln\\left(\\frac{2\\pi e S(\\theta)}{n}\\right)}\n$$", "id": "3340975"}, {"introduction": "当模型的不同参数组合能够产生相同的输出时，就会出现参数混淆或不可辨识的问题。本练习将通过一个实际的编码任务，展示如何利用费雪信息矩阵 (Fisher Information Matrix, FIM) ——一个与对数似然曲面曲率密切相关的概念——来量化评估并选择最有效的新实验，以打破参数混淆，从而提高参数的可辨识性。[@problem_id:3340909]", "problem": "考虑一个单物种动力学系统，其状态 $x(t)$ 由常微分方程 $dx/dt = r\\,x(t)$ 和初始条件 $x(0) = x_0$ 控制。其解析解为 $x(t) = x_0 \\exp(r t)$。存在一个基准可观测量 $h_{\\mathrm{base}}(x,\\theta) = c\\,x(t)$，其中 $\\theta = (x_0,r,c)$ 表示未知参数。如果仅随时间测量 $h_{\\mathrm{base}}(x,\\theta)$，参数对 $(x_0,c)$ 将会混淆（只有乘积 $c\\,x_0$ 与 $r$ 一起是结构可辨识的）。您的任务是确定哪个额外的可观测量 $h_i(x,\\theta)$，在与 $h_{\\mathrm{base}}(x,\\theta)$ 同时测量时，能最大程度地降低混淆参数对 $(x_0,c)$ 的剖面似然的平坦度。\n\n使用以下候选测量函数，每个函数都将与基准测量一起，在相同的时间点进行评估：\n- $h_1(x,\\theta) = x(t)$,\n- $h_2(x,\\theta) = \\dot{x}(t) = r\\,x(t)$,\n- $h_3(x,\\theta) = \\int_0^t x(s)\\,ds$,\n- $h_4(x,\\theta) = \\big(x(t)\\big)^2$.\n\n假设每个测量输出都有一个独立的的高斯观测模型，每个标量观测的方差相同，均为 $\\sigma^2$，并假设测量是在指定的时间网格 $\\{t_k\\}_{k=1}^N$ 上进行的，没有缺失数据。使用独立高斯噪声下对数似然的标准定义以及剖面似然的概念，其定义为固定一个参数，并对剩余参数优化对数似然。为将剖面平坦度的降低与信息论论据联系起来，请使用费雪信息矩阵（FIM）。FIM定义为在标称参数处对数似然的期望曲率（负海森矩阵），并在高斯模型下根据输出对参数的灵敏度计算得出。使用一个基于限制在 $(x_0,c)$ 块上的FIM的标量来量化关于 $(x_0,c)$ 的信息量，并选择当添加到 $h_{\\mathrm{base}}(x,\\theta)$ 时能为该块产生最大信息增益的候选者 $h_i$。\n\n您的程序必须：\n- 实现上面列出的 $x(t)$ 和所有 $h_i(x,\\theta)$。\n- 对于一个给定的测试用例 $(\\theta, \\{t_k\\}, \\sigma)$，使用输出对 $(x_0,r,c)$ 的有限差分灵敏度，为组合测量集 $\\{h_{\\mathrm{base}}, h_i\\}$ 计算在 $\\theta$ 处的期望费雪信息矩阵。\n- 提取对应于参数对 $(x_0,c)$ 的 $2\\times 2$ 费雪信息块，并计算一个标量信息分数，该分数在该块信息量增加时会增大。您必须使用该 $2\\times 2$ 块行列式的自然对数，并在对角线上添加一个小的数值正则化项以避免奇异性。\n- 对于每个测试用例，返回候选可观测量 $h_i$ 的索引 $i \\in \\{1,2,3,4\\}$，该可观测量与基准测量结合时，能使 $(x_0,c)$ 块的信息分数最大化。\n\n使用以下测试套件：\n- 测试用例A（一般情况）：$\\theta = (x_0,r,c) = (2.0,\\,0.7,\\,0.6)$, $\\{t_k\\} = \\mathrm{linspace}(0,\\,1.5,\\,16)$, $\\sigma = 0.1$.\n- 测试用例B（近常数动态边界）：$\\theta = (x_0,r,c) = (1.0,\\,0.05,\\,0.5)$, $\\{t_k\\} = \\mathrm{linspace}(0,\\,0.3,\\,7)$, $\\sigma = 0.2$.\n- 测试用例C（较高噪声边缘情况）：$\\theta = (x_0,r,c) = (3.0,\\,1.2,\\,0.3)$, $\\{t_k\\} = \\mathrm{linspace}(0,\\,0.6,\\,13)$, $\\sigma = 0.5$.\n- 测试用例D（振幅主导区域）：$\\theta = (x_0,r,c) = (0.5,\\,0.9,\\,1.3)$, $\\{t_k\\} = \\mathrm{linspace}(0,\\,1.0,\\,11)$, $\\sigma = 0.05$.\n\n在所有情况下，将时间视为无量纲，并将最终答案表示为无单位的索引。\n\n输出规范：\n- 您的程序应产生单行输出，其中包含四个测试用例所选候选者的索引，形式为方括号括起来的逗号分隔列表（例如，$\\mathrm{[i_A,i_B,i_C,i_D]}$），其中每个条目是 $\\{1,2,3,4\\}$ 中的一个整数。\n\n您的解决方案必须是一个完整、可运行的程序，能够执行所述计算，而无需任何用户输入或外部文件。", "solution": "The user wants to identify which of four candidate observables, $h_1, h_2, h_3, h_4$, when added to a baseline measurement $h_{\\mathrm{base}}$, provides the most information to distinguish the parameters $x_0$ and $c$ in a simple exponential growth model.\n\n### **1. Problem Validation**\nThe problem statement is validated according to the specified criteria.\n\n- **Step 1: Extract Givens**\n    - **Model**: $dx/dt = r\\,x(t)$ with $x(0) = x_0$. Solution: $x(t) = x_0 \\exp(r t)$.\n    - **Parameters**: $\\theta = (x_0, r, c)$.\n    - **Observables**: \n        - Baseline: $h_{\\mathrm{base}}(t) = c\\,x(t)$.\n        - Candidates: $h_1(t) = x(t)$, $h_2(t) = r\\,x(t)$, $h_3(t) = \\int_0^t x(s)\\,ds = \\frac{x_0}{r}(\\exp(rt)-1)$, $h_4(t) = (x(t))^2$.\n    - **Methodology**:\n        1.  Assume independent Gaussian noise with variance $\\sigma^2$ for each scalar measurement.\n        2.  For each combined measurement $\\{h_{\\mathrm{base}}, h_i\\}$, compute the $3 \\times 3$ Fisher Information Matrix (FIM), $F$, for parameters $(x_0, r, c)$. The total FIM is the sum of the FIMs for each observable: $F_{\\text{total}} = F_{\\text{base}} + F_{i}$.\n        3.  The FIM for a single observable $y(t;\\theta)$ measured at times $\\{t_k\\}$ is given by $F_{jk} = \\frac{1}{\\sigma^2} \\sum_{k=1}^N \\frac{\\partial y(t_k)}{\\partial \\theta_j} \\frac{\\partial y(t_k)}{\\partial \\theta_k}$.\n        4.  Sensitivities $\\frac{\\partial y}{\\partial \\theta_j}$ are to be computed using finite differences.\n        5.  Extract the $2 \\times 2$ submatrix of $F_{\\text{total}}$ corresponding to parameters $(x_0, c)$.\n        6.  Compute the information score as $S_i = \\log(\\det(F_{(x_0,c)} + \\epsilon I))$, where $\\epsilon$ is a small regularization constant and $I$ is the identity matrix.\n        7.  Select the index $i \\in \\{1,2,3,4\\}$ that maximizes $S_i$.\n    - **Test Cases**: Specific values for $\\theta$, $\\{t_k\\}$, and $\\sigma$ are provided.\n\n- **Step 2: Validate Using Extracted Givens**\n    - The problem is **scientifically grounded**, using standard principles of dynamical systems, parameter estimation, and optimal experimental design (FIM).\n    - It is **well-posed**, with a clear objective and a precise, quantitative method to reach a unique answer for each test case.\n    - The problem is **objective** and free from ambiguity.\n    - All necessary information is provided, making the problem **complete and consistent**. Its components (model, observables, analysis method) are standard and do not violate any scientific or mathematical principles.\n\n- **Step 3: Verdict and Action**\n    - The problem is deemed **valid**. A solution will be provided.\n\n### **2. Principle-Based Solution Design**\n\nThe core of the problem is to quantify the information gain about a parameter subset $(x_0, c)$ when a new measurement is added. The Fisher Information Matrix (FIM) provides the necessary theoretical framework.\n\n**Fisher Information Matrix (FIM)**\nFor a model with parameters $\\theta$ and measured outputs corrupted by independent, identically distributed Gaussian noise with variance $\\sigma^2$, the FIM is a matrix whose elements are given by:\n$$\nF_{jk} = \\frac{1}{\\sigma^2} \\sum_{m} \\sum_{l=1}^{N} \\left( \\frac{\\partial y_m(t_l; \\theta)}{\\partial \\theta_j} \\right) \\left( \\frac{\\partial y_m(t_l; \\theta)}{\\partial \\theta_k} \\right)\n$$\nwhere $y_m$ is the $m$-th observable, $t_l$ are the measurement times, and $\\theta_j, \\theta_k$ are the parameters. The FIM is the sum of the FIMs for each individual observable.\n\n**Structural Non-Identifiability of $(x_0, c)$ with $h_{\\mathrm{base}}$**\nThe baseline observable is $h_{\\mathrm{base}}(t) = c \\cdot x_0 e^{rt}$. The parameters $c$ and $x_0$ only appear as a product, $c\\,x_0$. This leads to structural non-identifiability. Mathematically, the sensitivity vectors with respect to $x_0$ and $c$ are linearly dependent:\n$$\n\\frac{\\partial h_{\\mathrm{base}}}{\\partial x_0} = c \\, e^{rt} \\quad \\text{and} \\quad \\frac{\\partial h_{\\mathrm{base}}}{\\partial c} = x_0 \\, e^{rt}\n$$\nThus, $\\frac{\\partial h_{\\mathrm{base}}}{\\partial c} = \\frac{x_0}{c} \\frac{\\partial h_{\\mathrm{base}}}{\\partial x_0}$. This linear dependence makes the corresponding FIM block for $(x_0, c)$ singular, reflecting the inability to distinguish these parameters. Its determinant is zero.\n\n**Resolving Non-Identifiability**\nAdding a second observable $h_i$ can resolve this issue if its sensitivities break the linear dependency. All candidate observables $h_1, \\dots, h_4$ are functions of $x(t)$ and $r$, but not $c$. Therefore, for each of them:\n$$\n\\frac{\\partial h_i}{\\partial c} = 0\n$$\nThe total FIM for the combined system $\\{h_{\\mathrm{base}}, h_i\\}$ is $F_{\\text{total}} = F_{\\text{base}} + F_{i}$. The block corresponding to $(x_0, c)$ will now have contributions from both observables. Because $\\frac{\\partial h_i}{\\partial c} = 0$ while $\\frac{\\partial h_i}{\\partial x_0} \\neq 0$, the new total sensitivity vectors for $x_0$ and $c$ are no longer linearly dependent, and the FIM block becomes non-singular.\n\n**Information Metric**\nThe determinant of a FIM block, $\\det(F_{(x_0,c)})$, is a measure of the information content for that parameter subset. A larger determinant corresponds to a smaller joint confidence region and thus better identifiability. We are asked to find the observable $h_i$ that maximizes this determinant (or its logarithm, which is a monotonic transformation).\n\n**Algorithmic Implementation**\nThe solution will be implemented as a Python program following these steps for each test case:\n1.  Define the parameter set $\\theta = (x_0, r, c)$, the time grid $\\{t_k\\}$, and the noise standard deviation $\\sigma$.\n2.  Define functions for the system state $x(t)$ and all observables $h_{\\mathrm{base}}, h_1, h_2, h_3, h_4$.\n3.  Implement a function to compute the sensitivity matrix $S$ for any given observable, where $S_{lk} = \\frac{\\partial y(t_l)}{\\partial \\theta_k}$. This will be done numerically using a central finite-difference scheme as requested.\n4.  Implement a function to compute the FIM from a sensitivity matrix: $F = \\frac{1}{\\sigma^2} S^T S$.\n5.  For each candidate $h_i$ ($i \\in \\{1,2,3,4\\}$):\n    a. Calculate $F_{\\text{base}}$ and $F_{i}$.\n    b. Compute the total FIM: $F_{\\text{total}} = F_{\\text{base}} + F_{i}$.\n    c. Extract the $2 \\times 2$ submatrix for $(x_0, c)$, which are the parameters at indices $0$ and $2$.\n    d. Regularize the submatrix by adding a small diagonal term $\\epsilon I$ to ensure numerical stability.\n    e. Calculate the score as the natural logarithm of the determinant of the regularized submatrix.\n6.  Determine the index $i$ that yields the maximum score and report it.\nThis process is repeated for all four test cases.", "answer": "```python\nimport numpy as np\n\ndef x_model(t, params):\n    \"\"\"\n    Computes the state x(t) = x0 * exp(r*t).\n    \n    Args:\n        t (np.ndarray): Time points.\n        params (tuple): (x0, r, c).\n        \n    Returns:\n        np.ndarray: State x at times t.\n    \"\"\"\n    x0, r, c = params\n    return x0 * np.exp(r * t)\n\ndef h_base(t, params):\n    \"\"\"Observable h_base(t) = c * x(t).\"\"\"\n    x0, r, c = params\n    return c * x_model(t, params)\n\ndef h1(t, params):\n    \"\"\"Observable h1(t) = x(t).\"\"\"\n    return x_model(t, params)\n    \ndef h2(t, params):\n    \"\"\"Observable h2(t) = r * x(t).\"\"\"\n    x0, r, c = params\n    return r * x_model(t, params)\n\ndef h3(t, params):\n    \"\"\"Observable h3(t) = integral of x(s) from 0 to t.\"\"\"\n    x0, r, c = params\n    # Handle the case r -> 0 to avoid division by zero.\n    if abs(r)  1e-9:\n        return x0 * t\n    return (x_model(t, (x0, r, c)) - x0) / r\n\ndef h4(t, params):\n    \"\"\"Observable h4(t) = x(t)^2.\"\"\"\n    return x_model(t, params)**2\n\ndef calculate_sensitivities(model_func, params, t_grid):\n    \"\"\"\n    Calculates sensitivities of a model output with respect to parameters\n    using central finite differences.\n    \"\"\"\n    n_params = len(params)\n    n_times = len(t_grid)\n    sensitivities = np.zeros((n_times, n_params))\n    \n    eps_rel = 1e-7  # Relative step size for finite differences\n\n    for i in range(n_params):\n        params_plus = list(params)\n        params_minus = list(params)\n        \n        # Robust step size selection\n        step = eps_rel * (abs(params[i]) + 1e-8)\n        \n        params_plus[i] += step\n        params_minus[i] -= step\n        \n        y_plus = model_func(t_grid, tuple(params_plus))\n        y_minus = model_func(t_grid, tuple(params_minus))\n        \n        sensitivities[:, i] = (y_plus - y_minus) / (2 * step)\n        \n    return sensitivities\n\ndef calculate_fim(sensitivities, sigma):\n    \"\"\"\n    Calculates the Fisher Information Matrix from a sensitivity matrix.\n    FIM = (1/sigma^2) * S^T * S\n    \"\"\"\n    return (sensitivities.T @ sensitivities) / (sigma**2)\n\ndef find_best_observable(params, t_grid, sigma):\n    \"\"\"\n    Finds the best observable h_i to add to h_base for identifying (x0, c).\n    \"\"\"\n    candidate_observables = [h1, h2, h3, h4]\n    scores = []\n    \n    # FIM for h_base is calculated once\n    sens_base = calculate_sensitivities(h_base, params, t_grid)\n    fim_base = calculate_fim(sens_base, sigma)\n    \n    for h_i in candidate_observables:\n        # Calculate FIM for the candidate observable\n        sens_i = calculate_sensitivities(h_i, params, t_grid)\n        fim_i = calculate_fim(sens_i, sigma)\n        \n        # Total FIM is the sum\n        fim_total = fim_base + fim_i\n        \n        # Extract the 2x2 block for parameters (x0, c) at indices 0 and 2\n        fim_block = fim_total[np.ix_([0, 2], [0, 2])]\n        \n        # Regularize and compute score (log-determinant)\n        reg = 1e-12\n        fim_block_reg = fim_block + reg * np.eye(2)\n        score = np.log(np.linalg.det(fim_block_reg))\n        scores.append(score)\n        \n    # Return the 1-based index of the best observable\n    best_index = np.argmax(scores) + 1\n    return best_index\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case A\n        {'params': (2.0, 0.7, 0.6), 't_grid': np.linspace(0.0, 1.5, 16), 'sigma': 0.1},\n        # Test case B\n        {'params': (1.0, 0.05, 0.5), 't_grid': np.linspace(0.0, 0.3, 7), 'sigma': 0.2},\n        # Test case C\n        {'params': (3.0, 1.2, 0.3), 't_grid': np.linspace(0.0, 0.6, 13), 'sigma': 0.5},\n        # Test case D\n        {'params': (0.5, 0.9, 1.3), 't_grid': np.linspace(0.0, 1.0, 11), 'sigma': 0.05}\n    ]\n\n    results = []\n    for case in test_cases:\n        best_idx = find_best_observable(case['params'], case['t_grid'], case['sigma'])\n        results.append(best_idx)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3340909"}, {"introduction": "结构不可辨识性是模型内在的特性，但有时可以通过组合来自不同实验条件的数据来解决。这个动手编码练习将通过计算和比较一个信号通路模型的剖面似然，来阐明这一原理。您将看到，在单个实验中无法辨识的参数，在引入了补充实验的数据后，是如何变得可辨识的。[@problem_id:3340990]", "problem": "考虑一个单室信号模型，其中受体或下游效应器的激活分数 $x(t)$ 在一个恒定输入 $u(t)$ 和两个动力学过程（激活，速率为 $k_{\\mathrm{on}}$；失活，速率为 $k_{\\mathrm{off}}$）下演化。状态方程为常微分方程 $\\dot{x}(t) = k_{\\mathrm{on}}\\,u(t)\\,(1 - x(t)) - k_{\\mathrm{off}}\\,x(t)$，初始条件为 $x(0) = 0$。测量模型为 $y(t) = s\\,x(t) + \\epsilon(t)$，其中 $s$ 是一个正的观测尺度，$\\epsilon(t)$ 是均值为零、方差 $\\sigma^2$ 已知的加性高斯噪声。在恒定输入 $u(t) = u_0$ 和足够长的采集时间 $t \\to \\infty$ 下，系统达到稳态 $x_\\infty = \\dfrac{k_{\\mathrm{on}}\\,u_0}{k_{\\mathrm{on}}\\,u_0 + k_{\\mathrm{off}}}$，因此，长时间测量满足 $y \\approx s\\,x_\\infty$。\n\n两个实验在不同的抑制剂下探测同一个模型，这些抑制剂对动力学参数起乘法作用：\n- 实验 $1$ 修改激活过程：$k_{\\mathrm{on}} \\mapsto \\alpha\\,k_{\\mathrm{on}}$，其中 $\\alpha \\in (0,1]$，并保持失活过程不变。\n- 实验 $2$ 修改失活过程：$k_{\\mathrm{off}} \\mapsto \\beta\\,k_{\\mathrm{off}}$，其中 $\\beta \\ge 1$，并保持激活过程不变。\n\n对于每个实验 $i \\in \\{1,2\\}$，定义有效稳态分数 $x_{\\infty,i} = \\dfrac{\\alpha_i\\,k_{\\mathrm{on}}\\,u_0}{\\alpha_i\\,k_{\\mathrm{on}}\\,u_0 + \\beta_i\\,k_{\\mathrm{off}}}$ 和相应的测量 $y_i = s\\,x_{\\infty,i} + \\epsilon_i$。为解决此问题，假设噪声实现 $\\epsilon_i$ 是已知的确定性偏移量，并且方差 $\\sigma^2$ 对于似然函数是已知的。\n\n设 $a = k_{\\mathrm{on}}\\,u_0$ 和 $b = k_{\\mathrm{off}}$。给定在 $\\{(\\alpha_i,\\beta_i)\\}$ 条件下的测量值 $\\{y_i\\}$，高斯负对数似然（相差一个可加常数）与归一化残差平方和 $\\sum_i \\left(\\dfrac{y_i - s\\,\\dfrac{\\alpha_i\\,a}{\\alpha_i\\,a + \\beta_i\\,b}}{\\sigma}\\right)^2$ 成正比。参数值 $s = \\vartheta$ 的剖面似然是通过在固定 $s = \\vartheta$ 的同时，对冗余参数 $(a,b)$ 在 $a  0$ 和 $b  0$ 的约束下最小化该平方和来获得的。\n\n你的任务是实现一个完整的程序，该程序：\n- 为指定的实验设计和已知的真实参数生成合成的稳态数据 $y_i$。\n- 通过对每个固定的 $s$ 值在 $(a,b)$ 上最小化归一化残差平方和，来计算 $s$ 值网格上的联合剖面似然。\n- 使用带有卡方截断值的似然比准则来评估 $s$ 的结构可辨识性。具体来说，对于给定的设计，计算剖面 $\\mathrm{PL}(s)$ 作为最小化的归一化残差平方和。设 $\\mathrm{PL}_{\\min}$ 为其在网格上的最小值。$s$ 的 $95\\%$ 置信集为 $\\{s \\mid \\mathrm{PL}(s) - \\mathrm{PL}_{\\min} \\le \\chi^2_{1,0.95}\\}$，其中 $\\chi^2_{1,0.95}$ 表示自由度为 $1$ 的卡方分布的 $95\\%$ 分位数。当且仅当此置信集在扫描的网格内双侧有界时，声明 $s$ 是可辨识的。否则，声明 $s$ 是不可辨识的。\n\n从以下基本依据开始：\n- 状态方程 $\\dot{x}(t) = k_{\\mathrm{on}}\\,u(t)\\,(1 - x(t)) - k_{\\mathrm{off}}\\,x(t)$ 在 $u(t) = u_0$ 和 $x(0) = 0$ 的条件下，通过设置 $\\dot{x}(t) = 0$ 得到稳态 $x_\\infty = \\dfrac{k_{\\mathrm{on}}\\,u_0}{k_{\\mathrm{on}}\\,u_0 + k_{\\mathrm{off}}}$。\n- 抑制剂对 $k_{\\mathrm{on}}$ 和 $k_{\\mathrm{off}}$ 起乘法且独立的作用，得到 $x_{\\infty,i} = \\dfrac{\\alpha_i\\,k_{\\mathrm{on}}\\,u_0}{\\alpha_i\\,k_{\\mathrm{on}}\\,u_0 + \\beta_i\\,k_{\\mathrm{off}}}$。\n- 高斯负对数似然与残差平方和除以 $\\sigma^2$ 成正比，似然比检验采用 $\\chi^2$ 阈值。\n\n设计一个多实验组合，以展示汇集数据如何打破单条件拟合中存在的不可辨识性。使用以下真实参数和测量噪声尺度：\n- $u_0 = 1.0$， $k_{\\mathrm{on}} = 1.5$， $k_{\\mathrm{off}} = 0.8$， $s = 2.0$， $\\sigma = 0.02$。\n- 对于每次测量 $y_i$，使用下面各案例中指定的确定性噪声偏移量 $\\epsilon_i$。\n\n实现四个测试案例：\n- 案例 1（单条件不可辨识性）：一个实验，条件为 $(\\alpha_1,\\beta_1) = (0.4,1.0)$ 和 $\\epsilon_1 = 0.005$。在剖面似然中仅使用 $y_1$；计算 $s$ 的可辨识性布尔值。\n- 案例 2（联合汇集解决不可辨识性）：两个实验联合汇集，条件为 $(\\alpha_1,\\beta_1) = (0.4,1.0)$、$\\epsilon_1 = 0.005$ 和 $(\\alpha_2,\\beta_2) = (1.0,3.0)$、$\\epsilon_2 = -0.005$。基于汇集数据计算 $s$ 的可辨识性布尔值。\n- 案例 3（相同条件无法解决不可辨识性）：两个实验条件相同，$(\\alpha_1,\\beta_1) = (1.0,1.0)$、$\\epsilon_1 = 0.003$ 和 $(\\alpha_2,\\beta_2) = (1.0,1.0)$、$\\epsilon_2 = -0.003$。基于汇集数据计算 $s$ 的可辨识性布尔值。\n- 案例 4（强失活抑制剂解决不可辨识性）：两个实验联合汇集，条件为 $(\\alpha_1,\\beta_1) = (0.4,1.0)$、$\\epsilon_1 = 0.005$ 和 $(\\alpha_2,\\beta_2) = (1.0,10.0)$、$\\epsilon_2 = -0.003$。基于汇集数据计算 $s$ 的可辨识性布尔值。\n\n对于每个案例，在 $s \\in [0.3,4.0]$ 的网格上使用包含 $41$ 个点的均匀离散化来扫描 $s$。使用卡方截断值 $\\chi^2_{1,0.95} \\approx 3.841458821$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的、按上述案例顺序排列的逗号分隔列表的结果。每个条目必须是一个布尔值，指示 $s$ 的可辨识性，最终输出行的格式应为 \"[b1,b2,b3,b4]\"。由于在此公式中所有量都是无量纲的，因此不需要单位。", "solution": "该问题要求实现一个剖面似然分析，以评估单室信号模型中尺度因子 $s$ 的参数可辨识性。该分析将在四个不同的实验场景下进行，以展示组合来自不同实验的数据如何解决可辨识性问题。\n\n首先，我们建立该问题的数学基础。系统的状态 $x(t)$ 由常微分方程 $\\dot{x}(t) = k_{\\mathrm{on}}\\,u(t)\\,(1 - x(t)) - k_{\\mathrm{off}}\\,x(t)$ 控制，其中 $x(0) = 0$。在恒定输入 $u(t) = u_0$ 下，系统达到稳态，可通过设置 $\\dot{x}(t) = 0$ 求得。这得到 $k_{\\mathrm{on}}\\,u_0\\,(1 - x_\\infty) = k_{\\mathrm{off}}\\,x_\\infty$，解得 $x_\\infty = \\dfrac{k_{\\mathrm{on}}\\,u_0}{k_{\\mathrm{on}}\\,u_0 + k_{\\mathrm{off}}}$。\n\n问题引入了重新参数化 $a = k_{\\mathrm{on}}\\,u_0$ 和 $b = k_{\\mathrm{off}}$，因此稳态为 $x_\\infty = \\dfrac{a}{a+b}$。实验涉及对速率常数进行乘法修改的抑制剂。对于由扰动因子 $(\\alpha_i, \\beta_i)$ 标识的实验 $i$，有效稳态由下式给出：\n$$x_{\\infty,i} = \\frac{\\alpha_i\\,a}{\\alpha_i\\,a + \\beta_i\\,b}$$\n实验 $i$ 的测量模型为 $y_i = s\\,x_{\\infty,i} + \\epsilon_i$，其中 $s$ 是我们感兴趣的观测尺度参数，$\\epsilon_i$ 是一个测量偏移量。\n\n任务的核心是计算 $s$ 的剖面似然。假设高斯噪声的方差 $\\sigma^2$ 已知，参数 $(s, a, b)$ 的负对数似然与归一化残差的平方和成正比。特定值 $s = \\vartheta$ 的剖面似然定义为该目标函数在冗余参数 $(a,b)$ 上的最小值：\n$$ \\mathrm{PL}(\\vartheta) = \\min_{a0, b0} \\sum_i \\left( \\frac{y_i - \\vartheta\\,\\dfrac{\\alpha_i\\,a}{\\alpha_i\\,a + \\beta_i\\,b}}{\\sigma} \\right)^2 $$\n\n为了评估可辨识性，我们使用似然比检验。$s$ 的 $95\\%$ 置信集定义为 $\\{s \\mid \\mathrm{PL}(s) - \\mathrm{PL}_{\\min} \\le \\chi^2_{1,0.95}\\}$，其中 $\\mathrm{PL}_{\\min}$ 是剖面似然在扫描的 $s$ 范围内的最小值，而 $\\chi^2_{1,0.95} \\approx 3.841458821$ 是自由度为 1 的卡方分布的 $95\\%$ 分位数。如果此置信区间在预定义的网格 $[0.3, 4.0]$ 内双侧有界，则参数 $s$ 被声明为可辨识的。这意味着剖面似然在网格的两端必须升至阈值 $\\mathrm{PL}_{\\min} + \\chi^2_{1,0.95}$ 之上。\n\n在实现数值程序之前，进行结构可辨识性分析可以洞察预期结果。\n对于单个实验（案例 1），我们有一个方程 $y_1 = s \\frac{\\alpha_1 a}{\\alpha_1 a + \\beta_1 b} + \\epsilon_1$，和三个未知参数 $(s,a,b)$。该系统是欠定的，参数不是唯一可辨识的。对于任何给定的 $s  y_1-\\epsilon_1$，都可以找到一个满足该方程的比率 $b/a$，从而导致一条由同等最优解组成的路径。因此，我们预期 $s$ 是不可辨识的。\n\n对于两个实验（案例 2、3、4），我们有一个包含两个方程的方程组：\n$$ y_1 - \\epsilon_1 = s \\frac{\\alpha_1 a}{\\alpha_1 a + \\beta_1 b} $$\n$$ y_2 - \\epsilon_2 = s \\frac{\\alpha_2 a}{\\alpha_2 a + \\beta_2 b} $$\n我们可以重新整理每个方程以求解比率 $b/a$：$\\frac{b}{a} = \\frac{\\alpha_i}{\\beta_i} \\left(\\frac{s}{y_i-\\epsilon_i} - 1\\right)$。将两个实验中 $b/a$ 的表达式相等，可以得到一个仅含 $s$ 的方程：\n$$ \\frac{\\alpha_1}{\\beta_1} \\left(\\frac{s}{y_1-\\epsilon_1} - 1\\right) = \\frac{\\alpha_2}{\\beta_2} \\left(\\frac{s}{y_2-\\epsilon_2} - 1\\right) $$\n这是一个关于 $s$ 的线性方程。当且仅当 $s$ 的系数不同时，存在唯一的 $s$ 解，这可以简化为条件 $\\frac{\\alpha_1}{\\beta_1} \\neq \\frac{\\alpha_2}{\\beta_2}$。\n- 在案例 2 中，$\\frac{\\alpha_1}{\\beta_1} = 0.4$ 和 $\\frac{\\alpha_2}{\\beta_2} = 1/3 \\approx 0.333$。比率不同，因此 $s$ 应该是可辨识的。\n- 在案例 3 中，$\\frac{\\alpha_1}{\\beta_1} = 1.0$ 和 $\\frac{\\alpha_2}{\\beta_2} = 1.0$。比率相同，意味着从结构角度看，这些实验是冗余的。关于 $s$ 的方程变得微不足道 ($0=0$)，$s$ 仍然是不可辨识的。\n- 在案例 4 中，$\\frac{\\alpha_1}{\\beta_1} = 0.4$ 和 $\\frac{\\alpha_2}{\\beta_2} = 0.1$。比率不同，因此 $s$ 应该是可辨识的。\n\n实现将按以下步骤进行：\n$1$。定义真实参数、常数和 $s$ 的网格。\n$2$。对于四个测试案例中的每一个：\n    a. 使用提供的真实参数 $(k_{\\mathrm{on}}, k_{\\mathrm{off}}, s)$、实验条件 $(\\alpha_i, \\beta_i)$ 和噪声偏移 $\\epsilon_i$ 生成合成测量数据 $\\{y_i\\}$。\n    b. 初始化一个数组来存储剖面似然值 $\\mathrm{PL}(s)$。\n    c. 遍历 $s$ 网格中的每个值 $\\vartheta$。对于每个 $\\vartheta$，定义一个代表残差平方和的关于参数 $(a,b)$ 的目标函数。\n    d. 使用 `scipy.optimize.minimize` 及其 `L-BFGS-B` 方法和边界条件 $a0, b0$ 来找到该目标函数的最小值。这个最小值就是 $\\mathrm{PL}(\\vartheta)$。\n    e. 扫描完整个网格后，找到剖面的全局最小值 $\\mathrm{PL}_{\\min}$。\n    f. 应用可辨识性准则：检查是否 $\\mathrm{PL}(s_{min\\_grid})  \\mathrm{PL}_{\\min} + \\chi^2_{1,0.95}$ 且 $\\mathrm{PL}(s_{max\\_grid})  \\mathrm{PL}_{\\min} + \\chi^2_{1,0.95}$。\n    g. 存储得到的布尔值。\n$3$。以指定的格式 `[b1,b2,b3,b4]` 打印最终的布尔值列表。\n这个过程将从数值上验证我们结构分析的结论。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Computes parameter identifiability for a systems biology model using profile likelihood.\n    \"\"\"\n    # True parameters for data generation\n    u0_true = 1.0\n    k_on_true = 1.5\n    k_off_true = 0.8\n    s_true = 2.0\n    sigma = 0.02\n\n    # Derived true parameters for optimization\n    a_true = k_on_true * u0_true\n    b_true = k_off_true\n\n    # Constants for the analysis\n    chi2_cutoff = 3.841458821\n    s_grid = np.linspace(0.3, 4.0, 41)\n\n    # Definition of the four test cases\n    test_cases = [\n        {\n            # Case 1: Single experiment, expected to be non-identifiable.\n            \"alphas\": np.array([0.4]),\n            \"betas\": np.array([1.0]),\n            \"epsilons\": np.array([0.005]),\n        },\n        {\n            # Case 2: Two distinct experiments, expected to be identifiable.\n            \"alphas\": np.array([0.4, 1.0]),\n            \"betas\": np.array([1.0, 3.0]),\n            \"epsilons\": np.array([0.005, -0.005]),\n        },\n        {\n            # Case 3: Two identical experiments, expected to be non-identifiable.\n            \"alphas\": np.array([1.0, 1.0]),\n            \"betas\": np.array([1.0, 1.0]),\n            \"epsilons\": np.array([0.003, -0.003]),\n        },\n        {\n            # Case 4: Two distinct experiments, expected to be identifiable.\n            \"alphas\": np.array([0.4, 1.0]),\n            \"betas\": np.array([1.0, 10.0]),\n            \"epsilons\": np.array([0.005, -0.003]),\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        alphas = case[\"alphas\"]\n        betas = case[\"betas\"]\n        epsilons = case[\"epsilons\"]\n\n        # 1. Generate synthetic data using true parameter values\n        x_inf_true = (alphas * a_true) / (alphas * a_true + betas * b_true)\n        y_data = s_true * x_inf_true + epsilons\n\n        # Define the objective function (sum of squared normalized residuals)\n        # to be minimized for each point in the profile likelihood.\n        def objective_function(params, s_val, alphas_exp, betas_exp, y_exp, sigma_val):\n            a, b = params\n            # The denominator can't be zero due to a0, b0 and alpha,beta0\n            y_model = s_val * (alphas_exp * a) / (alphas_exp * a + betas_exp * b)\n            residuals = (y_exp - y_model) / sigma_val\n            return np.sum(residuals**2)\n\n        # 2. Compute the profile likelihood over the grid of s values\n        pl_values = []\n        for s_val in s_grid:\n            # The optimization finds the best-fit nuisance parameters (a, b) for a fixed s.\n            # Initial guess is set to the true values for stability.\n            # Bounds enforce the physical constraint that rates must be positive.\n            res = minimize(\n                objective_function,\n                x0=[a_true, b_true],\n                args=(s_val, alphas, betas, y_data, sigma),\n                method='L-BFGS-B',\n                bounds=[(1e-9, None), (1e-9, None)]\n            )\n            pl_values.append(res.fun)\n\n        pl_values = np.array(pl_values)\n\n        # 3. Assess identifiability based on the likelihood ratio test\n        pl_min = np.min(pl_values)\n        threshold = pl_min + chi2_cutoff\n\n        # s is identifiable if the 95% confidence interval is bounded on both sides\n        # within the scanned grid. This means the profile at the grid boundaries must\n        # be above the threshold.\n        is_identifiable = (pl_values[0]  threshold) and (pl_values[-1]  threshold)\n        results.append(is_identifiable)\n\n    # Format and print the final output as a single line\n    print(f\"[{','.join(map(str, (r.item() for r in results)))}]\")\n\nsolve()\n```", "id": "3340990"}]}