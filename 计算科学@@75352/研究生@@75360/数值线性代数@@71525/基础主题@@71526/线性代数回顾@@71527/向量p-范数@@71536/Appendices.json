{"hands_on_practices": [{"introduction": "本练习探讨不同 $p$-范数之间的基本关系。通过推导范数等价不等式的最优常数，你将定量地理解不同范数单位球的几何形状差异，以及范数的选择如何影响距离和误差的度量。这是分析数值算法收敛性和稳定性的基石。[@problem_id:3600685]", "problem": "设 $n \\in \\mathbb{N}$，并考虑装备了向量 $p$-范数 $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$, $\\|x\\|_{2} = \\left(\\sum_{i=1}^{n} x_{i}^{2}\\right)^{1/2}$ 以及 $\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |x_{i}|$ 的 $\\mathbb{R}^{n}$ 空间。定义 $f(x) = \\|x\\|_{1}$ 和 $g(x) = \\|x\\|_{\\infty}$。对于任意 $x,y \\in \\mathbb{R}^{n}$，导出绝对差 $|\\|x\\|_{1} - \\|y\\|_{1}|$ 和 $|\\|x\\|_{\\infty} - \\|y\\|_{\\infty}|$ 的上界，并用 $\\|x-y\\|_{2}$ 和 $\\|x-y\\|_{1}$ 表示。仅从范数的公理和适用于 $\\mathbb{R}^{n}$ 的成熟不等式（例如三角不等式和 Cauchy–Schwarz 不等式）出发，证明对于每个界，都存在一个最小的常数 $L$（仅取决于 $n$），使得对于所有 $x,y \\in \\mathbb{R}^{n}$，\n$$\n|\\|x\\|_{1} - \\|y\\|_{1}| \\leq L \\,\\|x-y\\|_{p} \\quad \\text{和} \\quad |\\|x\\|_{\\infty} - \\|y\\|_{\\infty}| \\leq L \\,\\|x-y\\|_{p},\n$$\n其中 $p \\in \\{1,2\\}$ 为适用值。明确地求出这些最优 Lipschitz 常数作为 $n$ 的函数，并证明它们是最佳的（即没有更小的常数对所有 $x,y$ 都成立）。将你的最终答案表示为有序行向量 $(L_{1,1}, L_{1,2}, L_{\\infty,1}, L_{\\infty,2})$，其中 $L_{\\alpha,p}$ 表示使得 $|\\|x\\|_{\\alpha} - \\|y\\|_{\\alpha}| \\leq L_{\\alpha,p} \\,\\|x-y\\|_{p}$ 对所有 $x,y \\in \\mathbb{R}^{n}$ 成立的最小常数。提供精确表达式；无需四舍五入。", "solution": "所述问题是有效的。这是一个关于向量范数的 Lipschitz 连续性的数值线性代数中的适定问题，所有术语都经过形式化定义，并与已建立的数学原理一致。未检测到科学、逻辑或事实上的缺陷。\n\n该问题要求找到在 $\\mathbb{R}^{n}$ 上函数 $f(x) = \\|x\\|_{1}$ 和 $g(x) = \\|x\\|_{\\infty}$ 的最小常数 $L$，即 Lipschitz 常数。这些常数是为形式为 $|\\|x\\|_{\\alpha} - \\|y\\|_{\\alpha}| \\leq L_{\\alpha,p} \\|x-y\\|_{p}$ 的不等式而求的，其中 $\\alpha \\in \\{1, \\infty\\}$ 且 $p \\in \\{1, 2\\}$。\n\n向量空间上任何范数 $\\|\\cdot\\|$ 的一个基本性质是反三角不等式，它指出对于任何向量 $x$ 和 $y$：\n$$ |\\|x\\| - \\|y\\|| \\leq \\|x-y\\| $$\n这直接从范数的三角不等式公理推导得出：\n$\\|x\\| = \\|x-y+y\\| \\leq \\|x-y\\| + \\|y\\| \\implies \\|x\\| - \\|y\\| \\leq \\|x-y\\|$。\n$\\|y\\| = \\|y-x+x\\| \\leq \\|y-x\\| + \\|x\\| \\implies \\|y\\| - \\|x\\| \\leq \\|y-x\\| = \\|x-y\\|$。\n结合这两个结果可得 $|\\|x\\| - \\|y\\|| \\leq \\|x-y\\|$。\n\n将此一般原理应用于我们的特定范数，我们有：\n$$ |\\|x\\|_{\\alpha} - \\|y\\|_{\\alpha}| \\leq \\|x-y\\|_{\\alpha} $$\n问题是找到最佳常数 $L_{\\alpha,p}$ 使得 $|\\|x\\|_{\\alpha} - \\|y\\|_{\\alpha}| \\leq L_{\\alpha,p} \\|x-y\\|_{p}$。使用反三角不等式，我们需要找到最小的常数 $L_{\\alpha,p}$ 使得：\n$$ \\|x-y\\|_{\\alpha} \\leq L_{\\alpha,p} \\|x-y\\|_{p} $$\n令 $z = x-y$。由于 $x$ 和 $y$ 是 $\\mathbb{R}^n$ 中的任意向量，所以 $z$ 可以是 $\\mathbb{R}^n$ 中的任意向量。因此，问题等价于找到最小的常数 $L_{\\alpha,p}$，使得对于所有 $z \\in \\mathbb{R}^n$：\n$$ \\|z\\|_{\\alpha} \\leq L_{\\alpha,p} \\|z\\|_{p} $$\n这个常数是从赋范空间 $(\\mathbb{R}^n, \\|\\cdot\\|_p)$ 到 $(\\mathbb{R}^n, \\|\\cdot\\|_\\alpha)$ 的恒等映射的算子范数。它被形式化地定义为：\n$$ L_{\\alpha,p} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{\\alpha}}{\\|z\\|_{p}} $$\n为了证明一个常数 $L$ 确实是上确界（即最小可能常数），我们必须首先证明对于所有 $z$ 都有 $\\|z\\|_{\\alpha} \\leq L \\|z\\|_{p}$，然后通过找到一个特定的非零向量 $z_0$ 使得 $\\|z_0\\|_{\\alpha} = L \\|z_0\\|_{p}$ 来证明这个界是紧的。\n\n我们现在将确定四个常数 $L_{1,1}$、$L_{1,2}$、$L_{\\infty,1}$ 和 $L_{\\infty,2}$。\n\n1.  **$L_{1,1}$ 的确定**\n    我们寻求常数 $L_{1,1} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{1}}{\\|z\\|_{1}}$。\n    对于任意非零向量 $z \\in \\mathbb{R}^n$，比值 $\\frac{\\|z\\|_{1}}{\\|z\\|_{1}}$ 恒为 $1$。\n    因此，$L_{1,1} = 1$。不等式为 $|\\|x\\|_{1} - \\|y\\|_{1}| \\leq \\|x-y\\|_{1}$，这本身就是 $1$-范数的反三角不等式。为了确认它是最优常数，令 $x=(1,0,...,0)$ 且 $y=0$。那么 $|\\|x\\|_1 - \\|y\\|_1| = 1$ 且 $\\|x-y\\|_1 = 1$，所以任何小于 $1$ 的常数都将不成立。因此，$L_{1,1}=1$。\n\n2.  **$L_{1,2}$ 的确定**\n    我们寻求常数 $L_{1,2} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{1}}{\\|z\\|_{2}}$。\n    设 $z = (z_1, z_2, \\dots, z_n) \\in \\mathbb{R}^n$。我们有 $\\|z\\|_1 = \\sum_{i=1}^n |z_i|$ 和 $\\|z\\|_2 = \\left(\\sum_{i=1}^n z_i^2\\right)^{1/2}$。\n    我们对向量 $u = (1, 1, \\dots, 1)$ 和 $v = (|z_1|, |z_2|, \\dots, |z_n|)$ 应用 Cauchy–Schwarz 不等式。\n    该不等式表明 $(u \\cdot v)^2 \\leq \\|u\\|_2^2 \\|v\\|_2^2$。\n    这里，$u \\cdot v = \\sum_{i=1}^n 1 \\cdot |z_i| = \\|z\\|_1$。\n    $\\|u\\|_2^2 = \\sum_{i=1}^n 1^2 = n$。\n    $\\|v\\|_2^2 = \\sum_{i=1}^n |z_i|^2 = \\sum_{i=1}^n z_i^2 = \\|z\\|_2^2$。\n    将这些代入 Cauchy-Schwarz 不等式得到：\n    $$ (\\|z\\|_1)^2 \\leq n \\|z\\|_2^2 $$\n    对两边取平方根，我们得到 $\\|z\\|_1 \\leq \\sqrt{n} \\|z\\|_2$。\n    这意味着对于所有 $z \\neq 0$，有 $\\frac{\\|z\\|_{1}}{\\|z\\|_{2}} \\leq \\sqrt{n}$，所以 $L_{1,2} \\leq \\sqrt{n}$。\n    为了证明这个界是最优的，我们必须找到一个使等式成立的向量。Cauchy-Schwarz 不等式中等号成立的充要条件是向量线性相关，即对于某个标量 $c$，有 $v = c \\cdot u$。这意味着 $|z_1| = |z_2| = \\dots = |z_n|$。\n    我们选择 $z_0 = (1, 1, \\dots, 1)$。\n    对于这个向量，$\\|z_0\\|_1 = \\sum_{i=1}^n |1| = n$。\n    且 $\\|z_0\\|_2 = \\left(\\sum_{i=1}^n 1^2\\right)^{1/2} = \\sqrt{n}$。\n    对于这个向量，比值为 $\\frac{\\|z_0\\|_{1}}{\\|z_0\\|_{2}} = \\frac{n}{\\sqrt{n}} = \\sqrt{n}$。\n    因为我们找到了一个达到界 $\\sqrt{n}$ 的向量，所以上确界必定是 $\\sqrt{n}$。\n    因此，$L_{1,2} = \\sqrt{n}$。\n\n3.  **$L_{\\infty,1}$ 的确定**\n    我们寻求常数 $L_{\\infty,1} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{\\infty}}{\\|z\\|_{1}}$。\n    根据定义，$\\|z\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |z_i|$ 且 $\\|z\\|_1 = \\sum_{i=1}^n |z_i|$。\n    设 $k$ 是一个索引，使得 $|z_k| = \\|z\\|_{\\infty}$。\n    和为 $\\|z\\|_1 = |z_1| + \\dots + |z_k| + \\dots + |z_n|$。\n    由于每个 $|z_i| \\geq 0$，和必然大于或等于任何单个项。特别地，$\\|z\\|_1 \\geq |z_k| = \\|z\\|_{\\infty}$。\n    所以，$\\|z\\|_{\\infty} \\leq \\|z\\|_{1}$。\n    这意味着对于所有 $z \\neq 0$，有 $\\frac{\\|z\\|_{\\infty}}{\\|z\\|_{1}} \\leq 1$，所以 $L_{\\infty,1} \\leq 1$。\n    为了证明这个界是最优的，考虑一个标准基向量，例如 $z_0 = e_1 = (1, 0, \\dots, 0)$。\n    对于这个向量，$\\|z_0\\|_{\\infty} = \\max\\{1, 0, \\dots, 0\\} = 1$。\n    且 $\\|z_0\\|_1 = |1| + |0| + \\dots + |0| = 1$。\n    比值为 $\\frac{\\|z_0\\|_{\\infty}}{\\|z_0\\|_{1}} = \\frac{1}{1} = 1$。\n    因为达到了这个界，所以上确界是 $1$。\n    因此，$L_{\\infty,1} = 1$。\n\n4.  **$L_{\\infty,2}$ 的确定**\n    我们寻求常数 $L_{\\infty,2} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{\\infty}}{\\|z\\|_{2}}$。\n    设 $k$ 是一个索引，使得 $|z_k| = \\|z\\|_{\\infty}$。\n    我们有 $\\|z\\|_{\\infty}^2 = |z_k|^2 = z_k^2$。\n    $2$-范数的平方是 $\\|z\\|_2^2 = \\sum_{i=1}^n z_i^2 = z_1^2 + \\dots + z_k^2 + \\dots + z_n^2$。\n    由于每个 $z_i^2 \\geq 0$，和大于或等于任何单个项：$\\|z\\|_2^2 \\geq z_k^2 = \\|z\\|_{\\infty}^2$。\n    取平方根（范数是非负的），我们得到 $\\|z\\|_2 \\geq \\|z\\|_{\\infty}$。\n    这意味着对于所有 $z \\neq 0$，有 $\\frac{\\|z\\|_{\\infty}}{\\|z\\|_{2}} \\leq 1$，所以 $L_{\\infty,2} \\leq 1$。\n    为了证明这个界是最优的，再次考虑标准基向量 $z_0 = e_1 = (1, 0, \\dots, 0)$。\n    对于这个向量，$\\|z_0\\|_{\\infty} = \\max\\{1, 0, \\dots, 0\\} = 1$。\n    且 $\\|z_0\\|_{2} = (1^2 + 0^2 + \\dots + 0^2)^{1/2} = 1$。\n    比值为 $\\frac{\\|z_0\\|_{\\infty}}{\\|z_0\\|_{2}} = \\frac{1}{1} = 1$。\n    因为达到了这个界，所以上确界是 $1$。\n    因此，$L_{\\infty,2} = 1$。\n\n四个最优 Lipschitz 常数是：\n$L_{1,1} = 1$\n$L_{1,2} = \\sqrt{n}$\n$L_{\\infty,1} = 1$\n$L_{\\infty,2} = 1$\n\n最终答案是有序行向量 $(L_{1,1}, L_{1,2}, L_{\\infty,1}, L_{\\infty,2})$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  \\sqrt{n}  1  1\n\\end{pmatrix}\n}\n$$", "id": "3600685"}, {"introduction": "从基本性质转向微积分，本练习旨在研究包含范数的函数的可微性。你将理解为何平方欧几里得范数 $(\\frac{1}{2}\\|x\\|_2^2)$ 对于基于梯度的优化方法表现极佳，而相比之下，$\\ell_1$-范数的非光滑性则需要引入更普适的次梯度概念。这一区别是现代优化与机器学习领域的核心，尤其在稀疏恢复和正则化等议题中至关重要。[@problem_id:3600719]", "problem": "令 $n \\in \\mathbb{N}$，并考虑由 $f(x) = \\frac{1}{2}\\|x\\|_{2}^{2}$ 定义的函数 $f : \\mathbb{R}^{n} \\to \\mathbb{R}$，其中 $\\|x\\|_{2}$ 表示欧几里得（或 $2$-）范数，且 $\\mathbb{R}^{n}$ 上的内积为标准点积。仅使用可微性、梯度和向量 $p$-范数的基本定义，推导梯度 $\\nabla f(x)$ 的显式形式，并证明梯度映射 $x \\mapsto \\nabla f(x)$ 关于 $2$-范数是 $L$-利普希茨连续的，其中 $L$ 为最小可能常数。另外，令 $g : \\mathbb{R}^{n} \\to \\mathbb{R}$ 由 $g(x) = \\|x\\|_{1}$ 定义，其中 $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$。分析 $g$ 在一个或多个坐标 $x_{i}$ 为零的点 $x$ 处的可微性，并刻画 $g$ 在这些点处的所有次梯度的集合。将 $f$ 的梯度映射的最小利普希茨常数 $L$ 作为你的最终答案报告。无需四舍五入。", "solution": "该问题提出了关于定义在 $\\mathbb{R}^{n}$ 上的两个函数 $f(x)$ 和 $g(x)$ 的两个不同任务。第一个任务是求出 $f(x) = \\frac{1}{2}\\|x\\|_{2}^{2}$ 的梯度，然后确定其梯度映射的最小利普希茨常数。第二个任务是分析 $g(x)=\\|x\\|_{1}$ 的可微性并刻画其次微分。我们将系统地处理每个部分。\n\n首先，我们分析函数 $f(x) = \\frac{1}{2}\\|x\\|_{2}^{2}$。\n问题要求使用其基本定义推导梯度 $\\nabla f(x)$。如果存在一个向量（记作 $\\nabla f(x)$），使得对于任意位移向量 $h \\in \\mathbb{R}^{n}$，以下等式成立，那么函数 $f: \\mathbb{R}^{n} \\to \\mathbb{R}$ 在点 $x \\in \\mathbb{R}^{n}$ 处是可微的：\n$$f(x+h) = f(x) + \\langle \\nabla f(x), h \\rangle + o(\\|h\\|_{2})$$\n其中 $\\langle \\cdot, \\cdot \\rangle$ 是 $\\mathbb{R}^{n}$ 上的标准点积，而 $o(\\|h\\|_{2})$ 是一个满足 $\\lim_{\\|h\\|_{2} \\to 0} \\frac{o(\\|h\\|_{2})}{\\|h\\|_{2}} = 0$ 的项。\n\n我们来展开 $f(x+h)$：\n$$f(x+h) = \\frac{1}{2}\\|x+h\\|_{2}^{2}$$\n根据欧几里得范数由内积的定义，$\\|v\\|_{2}^{2} = \\langle v, v \\rangle$。\n$$f(x+h) = \\frac{1}{2}\\langle x+h, x+h \\rangle$$\n利用内积的双线性性质：\n$$f(x+h) = \\frac{1}{2}(\\langle x, x \\rangle + \\langle x, h \\rangle + \\langle h, x \\rangle + \\langle h, h \\rangle)$$\n由于标准点积是对称的，$\\langle x, h \\rangle = \\langle h, x \\rangle$。\n$$f(x+h) = \\frac{1}{2}(\\langle x, x \\rangle + 2\\langle x, h \\rangle + \\langle h, h \\rangle)$$\n$$f(x+h) = \\frac{1}{2}\\|x\\|_{2}^{2} + \\langle x, h \\rangle + \\frac{1}{2}\\|h\\|_{2}^{2}$$\n注意到 $f(x) = \\frac{1}{2}\\|x\\|_{2}^{2}$，我们有：\n$$f(x+h) = f(x) + \\langle x, h \\rangle + \\frac{1}{2}\\|h\\|_{2}^{2}$$\n将此表达式与可微性的定义 $f(x+h) = f(x) + \\langle \\nabla f(x), h \\rangle + o(\\|h\\|_{2})$ 进行比较，我们可以将关于 $h$ 的线性项确定为 $\\langle x, h \\rangle$。这表明 $\\nabla f(x) = x$。余项是 $\\frac{1}{2}\\|h\\|_{2}^{2}$。为了确认这一识别，我们必须验证该余项确实是 $o(\\|h\\|_{2})$。我们检查极限：\n$$\\lim_{\\|h\\|_{2} \\to 0} \\frac{\\frac{1}{2}\\|h\\|_{2}^{2}}{\\|h\\|_{2}} = \\lim_{\\|h\\|_{2} \\to 0} \\frac{1}{2}\\|h\\|_{2} = 0$$\n极限为 $0$，所以余项是 $o(\\|h\\|_{2})$。因此，$f(x)$ 的梯度是 $\\nabla f(x) = x$。\n\n接下来，我们必须证明梯度映射（我们记作 $G(x) = \\nabla f(x) = x$）关于 $2$-范数是 $L$-利普希茨连续的，其中 $L$ 是最小可能常数。如果对于所有 $x, y \\in \\mathbb{R}^{n}$，以下不等式成立，则映射 $G: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ 相对于范数 $\\|\\cdot\\|$ 是 $L$-利普希茨连续的：\n$$\\|G(x) - G(y)\\| \\le L \\|x - y\\|$$\n在我们的例子中，映射是 $G(x)=x$，范数是 $2$-范数。所以我们需要找到最小的 $L \\ge 0$ 使得：\n$$\\|x - y\\|_{2} \\le L \\|x - y\\|_{2}$$\n对于所有 $x, y \\in \\mathbb{R}^{n}$ 成立。\n如果 $x=y$，不等式变为 $0 \\le 0$，这对任何 $L$ 都成立。\n如果 $x \\neq y$，我们有 $\\|x-y\\|_{2}  0$，两边可以同时除以它得到：\n$$1 \\le L$$\n这表明对于任何常数 $L \\ge 1$，该不等式都成立。最小的这样的常数是 $L=1$。为了证明 $L=1$ 是最小的，我们必须表明对于任何 $L'  1$，该条件都会被违反。如果我们选择 $L'  1$，我们可以选取任意两个不同的点，例如 $x=(1, 0, \\dots, 0)$ 和 $y=(0, \\dots, 0)$。那么 $\\|x-y\\|_{2}=1$ 且 $\\|G(x)-G(y)\\|_{2} = \\|x-y\\|_{2} = 1$。利普希茨条件会要求 $1 \\le L' \\cdot 1$，即 $1 \\le L'$，这与 $L'1$ 相矛盾。因此，最小可能的利普希茨常数是 $L=1$。\n\n现在，我们转向问题的第二部分，关于函数 $g(x) = \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$。\n首先，我们来分析它的可微性。函数 $g(x)$ 是形如 $\\phi_i(x) = |x_i|$ 的函数之和。标量函数 $\\phi(t)=|t|$ 在除 $t=0$ 之外的所有点都可微。\n一个多变量函数在某点可微，如果它的所有偏导数在该点的一个邻域内存在且连续（这是一个充分但非必要条件），或者更基本地说，如果它在该点可以被一个线性函数很好地近似。如果任何一个偏导数在某点不存在，则该函数在该点不可微。\n我们来計算 $g(x)$ 在点 $x$ 关于 $x_k$ 的偏导数。\n$$\\frac{\\partial g}{\\partial x_k}(x) = \\lim_{h \\to 0} \\frac{g(x + h e_k) - g(x)}{h}$$\n其中 $e_k$ 是第 $k$ 个标准基向量。\n$$g(x + h e_k) = \\sum_{i \\neq k} |x_i| + |x_k + h|$$\n所以，极限变为：\n$$\\frac{\\partial g}{\\partial x_k}(x) = \\lim_{h \\to 0} \\frac{\\left(\\sum_{i \\neq k} |x_i| + |x_k + h|\\right) - \\left(\\sum_{i=1}^{n} |x_i|\\right)}{h} = \\lim_{h \\to 0} \\frac{|x_k + h| - |x_k|}{h}$$\n如果 $x_k \\neq 0$，那么对于足够小的 $h$，$x_k+h$ 与 $x_k$ 的符号相同。在这种情况下，$|t|$ 的导数是 $\\text{sgn}(t)$，所以 $\\frac{\\partial g}{\\partial x_k}(x) = \\text{sgn}(x_k)$。因此，如果对于 $i=1, \\dots, n$ 所有的 $x_i \\neq 0$，$g$ 就是可微的，其梯度为 $\\nabla g(x) = (\\text{sgn}(x_1), \\dots, \\text{sgn}(x_n))$。\n现在，考虑一个点 $x$，其中至少有一个坐标（比如 $x_k$）为零。偏导数的表达式变为：\n$$\\frac{\\partial g}{\\partial x_k}(x) = \\lim_{h \\to 0} \\frac{|0 + h| - |0|}{h} = \\lim_{h \\to 0} \\frac{|h|}{h}$$\n这个极限不存在。右极限是 $\\lim_{h \\to 0^+} \\frac{h}{h} = 1$，而左极限是 $\\lim_{h \\to 0^-} \\frac{-h}{h} = -1$。由于偏导数 $\\frac{\\partial g}{\\partial x_k}$ 不存在，函数 $g(x)$ 在任何有一个或多个坐标为零的点 $x$ 處都不可微。\n\n最后，我们刻画 $g(x)$ 的次梯度集合。对于一个凸函数（如 $g(x)$），向量 $v \\in \\mathbb{R}^n$ 是其在 $x$ 处的次梯度，如果对所有 $y \\in \\mathbb{R}^n$ 都满足：\n$$g(y) \\ge g(x) + \\langle v, y-x \\rangle$$\n所有这些次梯度的集合称为次微分，记作 $\\partial g(x)$。\n由于 $g(x) = \\sum_{i=1}^{n} |x_i|$ 是可分离凸函数的和，它的次微分是其分量函数的次微分的笛卡尔积。令 $\\phi_i(x_i) = |x_i|$。那么 $\\partial g(x) = \\partial \\phi_1(x_1) \\times \\dots \\times \\partial \\phi_n(x_n)$。\n我们需要求出标量绝对值函数 $\\phi(t)=|t|$ 的次微分。\n\\begin{itemize}\n    \\item 如果 $t_0 > 0$，$\\phi(t)$ 可微，导数为 $1$。其次微分是一个单元素集合：$\\partial \\phi(t_0) = \\{1\\} = \\{\\text{sgn}(t_0)\\}$。\n    \\item 如果 $t_0  0$，$\\phi(t)$ 可微，导数为 $-1$。其次微分是一个单元素集合：$\\partial \\phi(t_0) = \\{-1\\} = \\{\\text{sgn}(t_0)\\}$。\n    \\item 如果 $t_0=0$，我们寻找 $v$ 使得 $|t| \\ge |0| + v(t-0)$，即对于所有 $t \\in \\mathbb{R}$ 都有 $|t| \\ge vt$。\n      如果 $t > 0$，我们需要 $t \\ge vt$，这意味着 $v \\le 1$。\n      如果 $t  0$，我们需要 $-t \\ge vt$，这意味着 $-1 \\le v$（因为我们除以一个负数 $t$）。\n      综合这些，我们必须有 $v \\in [-1, 1]$。因此，在 $t_0=0$ 处，$\\partial \\phi(0) = [-1, 1]$。\n\\end{itemize}\n问题要求刻画 $g$ 在一个或多个坐标 $x_i$ 为零的点处的次梯度。对于任意 $x \\in \\mathbb{R}^n$，向量 $v = (v_1, \\dots, v_n)$ 是 $g$ 在 $x$ 处的次梯度，当且仅当每个分量 $v_i$ 都属于 $|x_i|$ 在 $x_i$ 处的次微分。\n所以，所有次梯度的集合 $\\partial g(x)$ 是所有满足以下条件的向量 $v \\in \\mathbb{R}^n$ 的集合：\n$$v_i = \\begin{cases} \\text{sgn}(x_i)  \\text{if } x_i \\neq 0 \\\\ c_i \\in [-1, 1]  \\text{if } x_i = 0 \\end{cases}$$\n这就完成了刻画。题目要求的最终答案是 $f(x)$ 的梯度映射的最小利普希茨常数 $L$，我们求得其值为 $1$。", "answer": "$$\\boxed{1}$$", "id": "3600719"}, {"introduction": "最后的这个练习将理论付诸实践，要求你设计、分析并实现将向量投影到 $\\ell_1$ 和 $\\ell_\\infty$ 球上的算法。这类投影是约束优化中的基本构件，本练习为你搭建了从理论理解到高效实用计算的桥梁。通过分析这些算法的计算复杂度，你将锻炼成为数值计算从业者的一项关键技能。[@problem_id:3600683]", "problem": "设 $n\\in\\mathbb{N}$，并设 $y\\in\\mathbb{R}^n$ 和 $\\tau\\in\\mathbb{R}$ 且 $\\tau\\ge 0$。对于 $p\\in\\{1,2,\\infty\\}$，向量 $p$-范数定义为 $\\|y\\|_1=\\sum_{i=1}^n |y_i|$，$\\|y\\|_2=\\left(\\sum_{i=1}^n y_i^2\\right)^{1/2}$，以及 $\\|y\\|_\\infty=\\max_{1\\le i\\le n}|y_i|$。半径为 $\\tau$ 的闭 $\\ell_p$-球是凸集 $B_p(\\tau)=\\{x\\in\\mathbb{R}^n:\\|x\\|_p\\le \\tau\\}$。将 $y$ 到一个非空闭凸集 $C\\subset\\mathbb{R}^n$ 上的欧几里得投影（即在 $\\ell_2$ 中的度量投影）是在 $x\\in C$ 上最小化欧几里得距离 $\\|x-y\\|_2$ 的唯一一点 $x^\\star\\in C$。在本问题中，您将：\n- 从第一性原理（从上述定义和凸最小化的标准最优性条件开始）设计算法，以计算欧几里得投影 $P_{B_1(\\tau)}(y)$ 和 $P_{B_\\infty(\\tau)}(y)$。\n- 证明这些投影映射在欧几里得范数下是非扩张的：对于所有 $u,v\\in\\mathbb{R}^n$，有 $\\|P_{B_1(\\tau)}(u)-P_{B_1(\\tau)}(v)\\|_2\\le \\|u-v\\|_2$ 以及 $\\|P_{B_\\infty(\\tau)}(u)-P_{B_\\infty(\\tau)}(v)\\|_2\\le \\|u-v\\|_2$。\n- 通过识别以数值稳定方式实现时的主要操作计数和渐近复杂度，比较它们的计算权衡。\n\n您的程序必须实现您推导出的算法，并对以下测试套件产生可验证的输出。在比较实数时，使用数值容差 $\\varepsilon=10^{-12}$。\n\n单向量投影测试（可行性检查）：\n1. $y^{(1)}=(3,-1,2,0.5)$，$\\tau^{(1)}=2$。验证 $\\|P_{B_1(\\tau^{(1)})}(y^{(1)})\\|_1\\le \\tau^{(1)}+\\varepsilon$ 和 $\\|P_{B_\\infty(\\tau^{(1)})}(y^{(1)})\\|_\\infty\\le \\tau^{(1)}+\\varepsilon$。\n2. $y^{(2)}=(0,0,0,0)$，$\\tau^{(2)}=0$。验证 $\\|P_{B_1(\\tau^{(2)})}(y^{(2)})\\|_1\\le \\tau^{(2)}+\\varepsilon$ 和 $\\|P_{B_\\infty(\\tau^{(2)})}(y^{(2)})\\|_\\infty\\le \\tau^{(2)}+\\varepsilon$。\n3. $y^{(3)}=(1,1,1,1)$，$\\tau^{(3)}=2$。验证 $\\|P_{B_1(\\tau^{(3)})}(y^{(3)})\\|_1\\le \\tau^{(3)}+\\varepsilon$ 和 $\\|P_{B_\\infty(\\tau^{(3)})}(y^{(3)})\\|_\\infty\\le \\tau^{(3)}+\\varepsilon$。\n4. $y^{(4)}=(10,-10,0.1,-0.1,5)$，$\\tau^{(4)}=7$。验证 $\\|P_{B_1(\\tau^{(4)})}(y^{(4)})\\|_1\\le \\tau^{(4)}+\\varepsilon$ 和 $\\|P_{B_\\infty(\\tau^{(4)})}(y^{(4)})\\|_\\infty\\le \\tau^{(4)}+\\varepsilon$。\n\n配对非扩张性测试（欧几里得范数）：\nA. $y_a^{(A)}=(3,-1,2)$ 和 $y_b^{(A)}=(2.5,0,-1)$，$\\tau^{(A)}=2$。验证 $\\|P_{B_1(\\tau^{(A)})}(y_a^{(A)})-P_{B_1(\\tau^{(A)})}(y_b^{(A)})\\|_2\\le \\|y_a^{(A)}-y_b^{(A)}\\|_2+\\varepsilon$ 和 $\\|P_{B_\\infty(\\tau^{(A)})}(y_a^{(A)})-P_{B_\\infty(\\tau^{(A)})}(y_b^{(A)})\\|_2\\le \\|y_a^{(A)}-y_b^{(A)}\\|_2+\\varepsilon$。\nB. $y_a^{(B)}=(-5,4,-3,2)$ 和 $y_b^{(B)}=(1,-2,3,-4)$，$\\tau^{(B)}=3$。用 $\\tau^{(B)}$ 验证同样的两个不等式。\nC. $y_a^{(C)}=(0.2,-0.3,0.4,-0.5,0.6)$ 和 $y_b^{(C)}=(-0.1,0.1,-0.2,0.2,-0.3)$，$\\tau^{(C)}=0.7$。用 $\\tau^{(C)}$ 验证同样的两个不等式。\n\n额外的范数特定检查：\n- 对于 $\\ell_\\infty$-球投影，还需对上述每对向量验证其在 $\\ell_\\infty$ 范数下的非扩张性：$\\|P_{B_\\infty(\\tau)}(y_a)-P_{B_\\infty(\\tau)}(y_b)\\|_\\infty\\le \\|y_a-y_b\\|_\\infty+\\varepsilon$。\n\n计算权衡量化：\n- 使用主要操作计数的代理，报告比率\n$$\nR(n)=\\frac{n\\log_2 n}{n}=\\log_2 n\n$$\n对于 $n\\in\\{20,1000,100000\\}$，将 $n\\log_2 n$ 解释为 $\\ell_1$-球投影算法的主要成本（由于排序），并将 $n$ 解释为 $\\ell_\\infty$-球投影算法的主要成本（由于分量裁剪）。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。该列表应按顺序包含：\n  - 四个单向量可行性检查产生的八个布尔值（每个测试两个：先 $\\ell_1$ 后 $\\ell_\\infty$）。\n  - 三个欧几里得非扩张性配对检查产生的六个布尔值（每对两个：先 $\\ell_1$ 后 $\\ell_\\infty$）。\n  - $\\ell_\\infty$-范数非扩张性检查产生的三个布尔值（每对一个）。\n  - 三个浮点数，分别为 $R(20)$、$R(1000)$ 和 $R(100000)$，按此顺序。\n因此，最终输出列表有 $20$ 个条目：$17$ 个布尔值后跟 $3$ 个浮点数。", "solution": "该问题被认定为有效。这是一个凸优化中的适定问题，基于标准定义和数值线性代数的既定原则。所有数据和条件都是自洽且一致的。\n\n### 1. 到 $\\ell_\\infty$-球上的欧几里得投影\n\n一个向量 $y \\in \\mathbb{R}^n$ 到闭 $\\ell_\\infty$-球 $B_\\infty(\\tau) = \\{x \\in \\mathbb{R}^n : \\|x\\|_\\infty \\le \\tau\\}$ 上的欧几里得投影是以下优化问题的解 $x^\\star$：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|x-y\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_\\infty \\le \\tau\n$$\n目标函数为 $f(x) = \\frac{1}{2}\\sum_{i=1}^n (x_i - y_i)^2$。约束 $\\|x\\|_\\infty \\le \\tau$ 等价于对所有 $i \\in \\{1, \\dots, n\\}$ 的分量约束集合 $|x_i| \\le \\tau$，可以写作 $-\\tau \\le x_i \\le \\tau$。\n\n由于目标函数和约束都是按分量可分的，这个 $n$ 维问题可以解耦成 $n$ 个独立的标量问题：\n$$\n\\min_{x_i \\in [-\\tau, \\tau]} \\frac{1}{2}(x_i - y_i)^2 \\quad \\text{for } i=1, \\dots, n\n$$\n对于每个分量 $i$，$(x_i - y_i)^2$ 的无约束最小化子是 $x_i = y_i$。\n- 如果 $|y_i| \\le \\tau$，那么 $y_i$ 在可行区间 $[-\\tau, \\tau]$ 内，所以最优解是 $x_i^\\star = y_i$。\n- 如果 $y_i > \\tau$，二次函数 $(x_i - y_i)^2$ 在 $[-\\tau, \\tau]$ 上是严格递减的。最小值在右端点处达到，所以 $x_i^\\star = \\tau$。\n- 如果 $y_i  -\\tau$，二次函数在 $[-\\tau, \\tau]$ 上是严格递增的。最小值在左端点处达到，所以 $x_i^\\star = -\\tau$。\n\n这三种情况可以紧凑地写为 $x_i^\\star = \\text{sign}(y_i)\\min(|y_i|, \\tau)$，或等价地写为 $x_i^\\star = \\text{median}(-\\tau, y_i, \\tau)$。这个操作也称为裁剪。因此，投影由下式给出：\n$$\nP_{B_\\infty(\\tau)}(y)_i = \\max(-\\tau, \\min(y_i, \\tau))\n$$\n\n### 2. 到 $\\ell_1$-球上的欧几里得投影\n\n$y \\in \\mathbb{R}^n$ 到闭 $\\ell_1$-球 $B_1(\\tau) = \\{x \\in \\mathbb{R}^n : \\|x\\|_1 \\le \\tau\\}$ 上的欧几里得投影是以下问题的解 $x^\\star$：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|x-y\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_1 \\le \\tau\n$$\n如果 $y$ 已经在此球内，即 $\\|y\\|_1 \\le \\tau$，那么最小距离为 $0$，在 $x^\\star = y$ 处达到。\n\n如果 $\\|y\\|_1 > \\tau$，投影 $x^\\star$ 必须位于球的边界上，即 $\\|x^\\star\\|_1 = \\tau$。这是一个带约束的凸优化问题。我们构造拉格朗日函数：\n$$\nL(x, \\lambda) = \\frac{1}{2}\\|x-y\\|_2^2 + \\lambda (\\|x\\|_1 - \\tau)\n$$\n其中 $\\lambda \\ge 0$ 是拉格朗日乘子。最优性的 Karush-Kuhn-Tucker (KKT) 条件是：\n1. 原始可行性：$\\|x\\|_1 \\le \\tau$\n2. 对偶可行性：$\\lambda \\ge 0$\n3. 互补松弛性：$\\lambda (\\|x\\|_1 - \\tau) = 0$\n4. 稳定性：$0 \\in \\partial_x L(x, \\lambda) = x - y + \\lambda \\partial \\|x\\|_1$\n\n稳定性条件意味着 $y - x \\in \\lambda \\partial \\|x\\|_1$，其中 $\\partial \\|x\\|_1$ 是 $\\ell_1$-范数的次梯度。对于每个分量 $i$，这意味着 $y_i - x_i = \\lambda s_i$，其中 $s_i \\in \\partial |x_i|$。绝对值函数的次梯度在 $t \\ne 0$ 时为 $\\text{sign}(t)$，在 $t=0$ 时为 $[-1, 1]$。这给出 $x_i = y_i - \\lambda s_i$。通过情况分析可以揭示，解必须具有 $x_i = \\text{sign}(y_i)\\max(0, |y_i| - \\lambda)$ 的形式。这个操作被称为软阈值算子，记为 $S_\\lambda(y)$。\n\n由于我们假设了 $\\|y\\|_1 > \\tau$，投影必须在边界上，所以 $\\|x\\|_1 = \\tau$。这意味着我们必须找到一个 $\\lambda > 0$ 使得 $f(\\lambda) := \\|S_\\lambda(y)\\|_1 = \\sum_{i=1}^n \\max(0, |y_i| - \\lambda) = \\tau$。函数 $f(\\lambda)$ 是连续、分段线性和关于 $\\lambda$ 单调递减的。由于 $f(0) = \\|y\\|_1 > \\tau$ 并且当 $\\lambda \\to \\infty$ 时 $f(\\lambda) \\to 0$，因此存在唯一的解 $\\lambda > 0$。\n\n为了高效地找到 $\\lambda$，我们可以使用以下算法：\n1. 如果 $\\|y\\|_1 \\le \\tau$，返回 $y$。\n2. 对于 $\\tau=0$，投影是零向量。\n3. 令 $u_i = |y_i|$ 并将这些值按降序排序：$u_{(1)} \\ge u_{(2)} \\ge \\dots \\ge u_{(n)}$。\n4. 找到整数 $\\rho = \\max\\left\\{k \\in \\{1, \\dots, n\\} \\mid u_{(k)} > \\frac{1}{k}\\left(\\sum_{j=1}^k u_{(j)} - \\tau\\right)\\right\\}$。这可以通过对 $k=1, \\dots, n$ 进行线性扫描找到。\n5. 设置阈值 $\\lambda = \\frac{1}{\\rho}\\left(\\sum_{j=1}^\\rho u_{(j)} - \\tau\\right)$。\n6. 投影是 $x^\\star = S_\\lambda(y) = \\text{sign}(y) \\odot \\max(0, |y|-\\lambda)$，其中 $\\odot$ 是逐元素乘积，向量运算是逐元素的。\n\n### 3. 欧几里得范数下非扩张性的证明\n\n在希尔伯特空间（例如具有欧几里得内积的 $\\mathbb{R}^n$）中，到一个非空、闭、凸集 $C$ 上的投影算子 $P_C$ 已知是非扩张的。集合 $B_1(\\tau)$ 和 $B_\\infty(\\tau)$ 是非空的（对于 $\\tau \\ge 0$）、闭的和凸的，因此这个通用性质适用。\n\n证明：对于任意 $u,v \\in \\mathbb{R}^n$，令 $x_u = P_C(u)$ 和 $x_v = P_C(v)$。刻画投影 $x_u$ 的变分不等式是 $\\langle u - x_u, z - x_u \\rangle \\le 0$，对所有 $z \\in C$ 成立。\n由于 $x_v \\in C$，我们可以设 $z=x_v$，得到：\n$$\n\\langle u - x_u, x_v - x_u \\rangle \\le 0\n$$\n类似地，对于投影 $x_v$，我们有 $\\langle v - x_v, z' - x_v \\rangle \\le 0$ 对所有 $z' \\in C$ 成立。设 $z' = x_u$ 得到：\n$$\n\\langle v - x_v, x_u - x_v \\rangle \\le 0\n$$\n将这两个不等式相加：\n$$\n\\langle u - x_u, x_v - x_u \\rangle + \\langle v - x_v, x_u - x_v \\rangle \\le 0\n$$\n重新整理各项：\n$$\n\\langle u - x_u, x_v - x_u \\rangle - \\langle v - x_v, x_v - x_u \\rangle \\le 0\n$$\n$$\n\\langle (u - v) - (x_u - x_v), x_v - x_u \\rangle \\le 0\n$$\n$$\n-\\langle u - v, x_u - x_v \\rangle + \\|x_u - x_v\\|_2^2 \\le 0\n$$\n这给出了强非扩张性：$\\|x_u - x_v\\|_2^2 \\le \\langle u - v, x_u - x_v \\rangle$。\n根据柯西-施瓦茨不等式，$\\langle u - v, x_u - x_v \\rangle \\le \\|u-v\\|_2 \\|x_u - x_v\\|_2$。\n将此代入前一个不等式：\n$$\n\\|x_u - x_v\\|_2^2 \\le \\|u-v\\|_2 \\|x_u - x_v\\|_2\n$$\n如果 $\\|x_u - x_v\\|_2 \\ne 0$，我们可以用它来除，从而得到非扩张性：\n$$\n\\|P_C(u) - P_C(v)\\|_2 \\le \\|u - v\\|_2\n$$\n如果 $\\|x_u - x_v\\|_2 = 0$，不等式平凡成立。此证明对 $P_{B_1(\\tau)}$ 和 $P_{B_\\infty(\\tau)}$ 均适用。\n\n### 4. $P_{B_\\infty(\\tau)}$ 在 $\\ell_\\infty$-范数下的非扩张性\n\n到 $B_\\infty(\\tau)$ 上的投影在 $\\ell_\\infty$-范数下也是非扩张的。\n令 $x_u = P_{B_\\infty(\\tau)}(u)$ 和 $x_v = P_{B_\\infty(\\tau)}(v)$。投影由逐分量应用的标量裁剪函数 $f(t) = \\text{median}(-\\tau, t, \\tau)$ 给出。函数 $f$ 是 1-利普希茨的，因为它的导数（在存在的地方）为 $0$ 或 $1$。因此，对于任意标量 $t_a, t_b$，我们有 $|f(t_a) - f(t_b)| \\le |t_a - t_b|$。\n将此应用于每个分量 $i$：\n$$\n|(x_u)_i - (x_v)_i| = |f(u_i) - f(v_i)| \\le |u_i - v_i|\n$$\n对所有分量 $i$ 取最大值：\n$$\n\\|x_u - x_v\\|_\\infty = \\max_i |(x_u)_i - (x_v)_i| \\le \\max_i |u_i - v_i| = \\|u-v\\|_\\infty\n$$\n因此，$\\|P_{B_\\infty(\\tau)}(u) - P_{B_\\infty(\\tau)}(v)\\|_\\infty \\le \\|u-v\\|_\\infty$。\n\n### 5. 计算权衡\n\n- **$P_{B_\\infty(\\tau)}$**：该算法涉及一个逐分量裁剪操作。对于一个维度为 $n$ 的向量，每个分量需要常数次操作（两次比较和一次赋值）。因此，总计算成本与 $n$ 呈线性关系，渐近复杂度为 $O(n)$。\n- **$P_{B_1(\\tau)}$**：该算法的主要计算步骤是：\n    1. 计算 $\\|y\\|_1$：$O(n)$ 次操作。\n    2. 排序 $|y|$：$O(n \\log n)$ 次操作。\n    3. 通过线性扫描找到 $\\rho$：$O(n)$ 次操作。\n    4. 通过软阈值计算最终投影：$O(n)$ 次操作。\n主要步骤是排序，因此总体渐近复杂度为 $O(n \\log n)$。\n\n对于大的 $n$，$\\ell_\\infty$-球投影的 $O(n)$ 复杂度比 $\\ell_1$-球投影的 $O(n \\log n)$ 复杂度效率显著更高。如问题中所定义的，主要操作计数的比率为 $R(n) = \\frac{n \\log_2 n}{n} = \\log_2 n$，它随着 $n$ 的增长而增长。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef proj_l_inf_ball(y: np.ndarray, tau: float) - np.ndarray:\n    \"\"\"\n    Computes the Euclidean projection of a vector y onto the L_infinity-ball B_inf(tau).\n    x_i = median(-tau, y_i, tau)\n    \"\"\"\n    if tau  0:\n        raise ValueError(\"Radius tau must be non-negative.\")\n    return np.clip(y, -tau, tau)\n\ndef proj_l1_ball(y: np.ndarray, tau: float) - np.ndarray:\n    \"\"\"\n    Computes the Euclidean projection of a vector y onto the L1-ball B_1(tau).\n    The algorithm is based on Duchi et al., \"Efficient Projections onto the L1-Ball...\" (2008).\n    \"\"\"\n    if tau  0:\n        raise ValueError(\"Radius tau must be non-negative.\")\n    \n    # If tau is 0, the L1 ball is just the origin.\n    if tau == 0:\n        return np.zeros_like(y)\n    \n    y_abs = np.abs(y)\n    l1_norm = np.sum(y_abs)\n    \n    # If the vector is already in the ball, projection is the identity.\n    if l1_norm = tau:\n        return y\n    \n    # The core of the algorithm for vectors outside the ball.\n    n = len(y)\n    \n    # Sort absolute values in descending order.\n    # We could sort y_abs directly, but we only need the sorted values.\n    u = np.sort(y_abs)[::-1]\n    \n    # Compute cumulative sums of the sorted values.\n    s = np.cumsum(u)\n    \n    # Find rho, the number of non-zero elements in the projected vector.\n    # This corresponds to finding the largest k such that u_k - (1/k)(s_k - tau) > 0.\n    k_array = np.arange(1, n + 1)\n    condition = u > (s - tau) / k_array\n    \n    # rho is the index of the last True value in 'condition'\n    # np.where returns a tuple, we take the first element (the array of indices).\n    # The indices in `where` correspond to the sorted array `u`.\n    rho_candidates = np.where(condition)[0]\n    rho = rho_candidates[-1] + 1\n    \n    # Compute the Lagrange multiplier lambda.\n    lambda_val = (s[rho - 1] - tau) / rho\n    \n    # Apply soft thresholding.\n    return np.sign(y) * np.maximum(y_abs - lambda_val, 0)\n\ndef solve():\n    \"\"\"\n    Main function to run all tests and generate the final output.\n    \"\"\"\n    eps = 1e-12\n    results = []\n\n    # Test cases definition\n    tests_single = [\n        (np.array([3, -1, 2, 0.5]), 2.0),\n        (np.array([0, 0, 0, 0]), 0.0),\n        (np.array([1, 1, 1, 1]), 2.0),\n        (np.array([10, -10, 0.1, -0.1, 5]), 7.0)\n    ]\n\n    tests_pair = [\n        (np.array([3, -1, 2]), np.array([2.5, 0, -1]), 2.0),\n        (np.array([-5, 4, -3, 2]), np.array([1, -2, 3, -4]), 3.0),\n        (np.array([0.2, -0.3, 0.4, -0.5, 0.6]), np.array([-0.1, 0.1, -0.2, 0.2, -0.3]), 0.7)\n    ]\n    \n    n_vals_ratio = [20, 1000, 100000]\n\n    # --- Single-vector projection tests ---\n    for y, tau in tests_single:\n        # L1 projection feasibility\n        p_l1 = proj_l1_ball(y, tau)\n        results.append(np.linalg.norm(p_l1, 1) = tau + eps)\n        \n        # L-inf projection feasibility\n        p_linf = proj_l_inf_ball(y, tau)\n        results.append(np.linalg.norm(p_linf, ord=np.inf) = tau + eps)\n\n    # --- Pairwise nonexpansiveness tests (Euclidean norm) ---\n    for ya, yb, tau in tests_pair:\n        dist_y = np.linalg.norm(ya - yb, 2)\n        \n        # L1 projection nonexpansiveness\n        pa_l1 = proj_l1_ball(ya, tau)\n        pb_l1 = proj_l1_ball(yb, tau)\n        dist_p_l1 = np.linalg.norm(pa_l1 - pb_l1, 2)\n        results.append(dist_p_l1 = dist_y + eps)\n        \n        # L-inf projection nonexpansiveness\n        pa_linf = proj_l_inf_ball(ya, tau)\n        pb_linf = proj_l_inf_ball(yb, tau)\n        dist_p_linf = np.linalg.norm(pa_linf - pb_linf, 2)\n        results.append(dist_p_linf = dist_y + eps)\n        \n    # --- Additional norm-specific check (L-inf nonexpansiveness of L-inf projection) ---\n    for ya, yb, tau in tests_pair:\n        dist_y_inf = np.linalg.norm(ya - yb, ord=np.inf)\n        pa_linf = proj_l_inf_ball(ya, tau)\n        pb_linf = proj_l_inf_ball(yb, tau)\n        dist_p_linf_inf = np.linalg.norm(pa_linf - pb_linf, ord=np.inf)\n        results.append(dist_p_linf_inf = dist_y_inf + eps)\n\n    # --- Computational tradeoff quantification ---\n    for n in n_vals_ratio:\n        # R(n) = (n log2 n) / n = log2 n\n        results.append(np.log2(n))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r).lower() if isinstance(r, bool) else str(r) for r in results)}]\")\n\nsolve()\n```", "id": "3600683"}]}