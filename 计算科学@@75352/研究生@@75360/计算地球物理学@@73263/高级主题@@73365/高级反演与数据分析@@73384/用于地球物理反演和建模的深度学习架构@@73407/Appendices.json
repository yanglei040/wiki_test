{"hands_on_practices": [{"introduction": "地球物理反演中的许多现代深度学习方法都源于对经典方法的重新解释和推广。本练习旨在揭示吉洪诺夫（Tikhonov）正则化（一种基础的反演技术）与一个简单的卷积神经网络（CNN）之间的深刻联系。通过在傅里叶域中分析这个反演问题，您将推导出经典解，并理解它如何等价于一个经过优化的线性滤波器，从而为将反演过程视为一个学习任务奠定基础 [@problem_id:3583423]。", "problem": "考虑一个定义在长度为 $N \\in \\mathbb{N}$ 的周期性网格上的一维循环卷积正演模型，其中数据 $\\mathbf{d} \\in \\mathbb{C}^{N}$ 由模型 $\\mathbf{m} \\in \\mathbb{C}^{N}$ 通过 $\\mathbf{d} = \\mathbf{F}\\mathbf{m} + \\mathbf{n}$ 生成，$\\mathbf{F}$ 是由与核 $\\mathbf{h} \\in \\mathbb{C}^{N}$ 的卷积所生成的循环矩阵，$\\mathbf{n}$ 是加性噪声。令 $\\mathbf{U} \\in \\mathbb{C}^{N \\times N}$ 表示酉离散傅里叶变换 (DFT) 矩阵，使得 $\\mathbf{U}^{*}\\mathbf{F}\\mathbf{U} = \\operatorname{diag}(\\hat{h}(k))$ 为对角矩阵，其对角元为 $\\hat{h}(k)$，即 $\\mathbf{h}$ 的 DFT。考虑带有恒等罚项的 Tikhonov 正则化，即最小化目标函数 $J(\\mathbf{m}) = \\|\\mathbf{F}\\mathbf{m} - \\mathbf{d}\\|_{2}^{2} + \\lambda \\|\\mathbf{m}\\|_{2}^{2}$，其中标量 $\\lambda > 0$。\n\n- 推导正规方程，并利用 DFT 对循环算子的对角化，以 $\\hat{h}(k)$、$\\hat{d}(k)$ 和 $\\lambda$ 表示傅里叶域中的解 $\\hat{m}(k)$。\n\n- 将核的功率谱定义为 $|\\hat{h}(k)|^{2}$。以 $|\\hat{h}(k)|^{2}$ 的极值和 $\\lambda$ 来推导正规矩阵 $\\mathbf{F}^{*}\\mathbf{F} + \\lambda \\mathbf{I}$ 的条件数，并解释当 $\\lambda$ 变化时，较小的 $|\\hat{h}(k)|^{2}$ 值如何影响反演的稳定性。\n\n- 假设真实模型 $\\mathbf{m}_{\\mathrm{true}}$ 和噪声 $\\mathbf{n}$ 是相互独立、零均值、广义平稳且白色的，其功率谱分别为常数 $S_{m}(k) \\equiv \\sigma_{m}^{2} > 0$ 和 $S_{n}(k) \\equiv \\sigma_{n}^{2} > 0$。计算每个频率的期望均方误差 $\\mathbb{E}\\left[|\\hat{m}(k) - \\hat{m}_{\\mathrm{true}}(k)|^{2}\\right]$，将其表示为 $|\\hat{h}(k)|^{2}$ 和 $\\lambda$ 的函数，并确定使该误差在所有频率上的平均值最小的常数正则化参数 $\\lambda^{\\star}$。\n\n- 现在将 Tikhonov 正则化解释为卷积神经网络 (CNN) 的单个线性层，这是一个在数据域上作用的、具有单个线性卷积层的卷积神经网络 (CNN)。约束该线性层的频率响应为 $\\hat{G}(k) = \\overline{\\hat{h}(k)}\\,\\varphi\\!\\left(|\\hat{h}(k)|^{2}\\right)$，其中 $\\varphi:[0,\\infty)\\to\\mathbb{R}$ 是一个仅依赖于 $s = |\\hat{h}(k)|^{2}$ 的标量函数，$\\overline{\\hat{h}(k)}$ 表示复共轭。在对 $\\mathbf{m}_{\\mathrm{true}}$ 和 $\\mathbf{n}$ 相同的统计假设下，确定能使估计量 $\\hat{m}(k) = \\hat{G}(k)\\hat{d}(k)$ 的期望均方误差最小的函数 $\\varphi(s)$。将最终结果以单行矩阵的形式提供为数对 $\\left(\\lambda^{\\star}, \\varphi(s)\\right)$。无需四舍五入，也无需单位。", "solution": "我们从 Tikhonov 目标函数 $J(\\mathbf{m}) = \\|\\mathbf{F}\\mathbf{m} - \\mathbf{d}\\|_{2}^{2} + \\lambda \\|\\mathbf{m}\\|_{2}^{2}$ 开始，对于 $\\lambda > 0$，这是一个严格凸的二次泛函。通过将梯度设为零可得到一阶最优性条件（正规方程），即\n$$(\\mathbf{F}^{*}\\mathbf{F} + \\lambda \\mathbf{I})\\mathbf{m} = \\mathbf{F}^{*}\\mathbf{d}.$$\n利用酉离散傅里叶变换 (DFT) 矩阵 $\\mathbf{U}$ 对循环矩阵进行对角化，我们有 $\\mathbf{F} = \\mathbf{U}^{*}\\operatorname{diag}(\\hat{h}(k))\\mathbf{U}$，因此\n$$\\mathbf{F}^{*}\\mathbf{F} = \\mathbf{U}^{*}\\operatorname{diag}(|\\hat{h}(k)|^{2})\\mathbf{U}.$$\n用 $\\mathbf{U}$ 左乘正规方程，并定义 $\\hat{m} = \\mathbf{U}\\mathbf{m}$ 和 $\\hat{d} = \\mathbf{U}\\mathbf{d}$，得到\n$$\\left[\\operatorname{diag}(|\\hat{h}(k)|^{2}) + \\lambda \\mathbf{I}\\right]\\hat{m} = \\operatorname{diag}(\\overline{\\hat{h}(k)})\\hat{d}.$$\n这在不同频率 $k$ 之间是解耦的，给出了谱域解\n$$\\hat{m}(k) = \\frac{\\overline{\\hat{h}(k)}\\,\\hat{d}(k)}{|\\hat{h}(k)|^{2} + \\lambda}.$$\n\n接下来，我们分析条件数。正规矩阵为 $\\mathbf{F}^{*}\\mathbf{F} + \\lambda \\mathbf{I} = \\mathbf{U}^{*}\\operatorname{diag}(|\\hat{h}(k)|^{2} + \\lambda)\\mathbf{U}$，因此其特征值为 $\\{|\\hat{h}(k)|^{2} + \\lambda\\}_{k}$。因此，2-范数条件数为\n$$\\kappa_{2}(\\mathbf{F}^{*}\\mathbf{F} + \\lambda \\mathbf{I}) = \\frac{\\max_{k}\\left(|\\hat{h}(k)|^{2}\\right) + \\lambda}{\\min_{k}\\left(|\\hat{h}(k)|^{2}\\right) + \\lambda}.$$\n当 $\\lambda$ 很小时，较小的 $|\\hat{h}(k)|^{2}$ 值会增​​大条件数，导致噪声放大和不稳定性。增加 $\\lambda$ 会将最小特征值提升 $\\lambda$，从而减小 $\\kappa_{2}$ 并提高稳定性，但代价是引入偏差。\n\n现在我们在假设的随机模型下计算期望均方误差。设真实模型 $\\mathbf{m}_{\\mathrm{true}}$ 和噪声 $\\mathbf{n}$ 是相互独立、零均值、白色的，其功率谱为常数 $S_{m}(k) \\equiv \\sigma_{m}^{2}$ 和 $S_{n}(k) \\equiv \\sigma_{n}^{2}$。在傅里叶域中，\n$$\\hat{d}(k) = \\hat{h}(k)\\,\\hat{m}_{\\mathrm{true}}(k) + \\hat{n}(k).$$\nTikhonov 估计量为\n$$\\hat{m}(k) = \\frac{\\overline{\\hat{h}(k)}}{|\\hat{h}(k)|^{2} + \\lambda}\\,\\hat{d}(k) = \\frac{|\\hat{h}(k)|^{2}}{|\\hat{h}(k)|^{2} + \\lambda}\\,\\hat{m}_{\\mathrm{true}}(k) + \\frac{\\overline{\\hat{h}(k)}}{|\\hat{h}(k)|^{2} + \\lambda}\\,\\hat{n}(k).$$\n为方便起见，定义 $s = |\\hat{h}(k)|^{2} \\geq 0$。频率 $k$ 处的估计误差为\n$$\\hat{e}(k) = \\hat{m}(k) - \\hat{m}_{\\mathrm{true}}(k) = -\\frac{\\lambda}{s + \\lambda}\\,\\hat{m}_{\\mathrm{true}}(k) + \\frac{\\overline{\\hat{h}(k)}}{s + \\lambda}\\,\\hat{n}(k).$$\n根据独立性和零均值假设，每个频率的期望均方误差为\n$$\\mathbb{E}\\left[|\\hat{e}(k)|^{2}\\right] = \\left(\\frac{\\lambda}{s + \\lambda}\\right)^{2}\\mathbb{E}\\left[|\\hat{m}_{\\mathrm{true}}(k)|^{2}\\right] + \\frac{|\\hat{h}(k)|^{2}}{(s + \\lambda)^{2}}\\mathbb{E}\\left[|\\hat{n}(k)|^{2}\\right] = \\frac{\\lambda^{2}\\sigma_{m}^{2} + s\\,\\sigma_{n}^{2}}{(s + \\lambda)^{2}}.$$\n由于问题在 $k$ 上是可分的，该表达式在 $k$ 上的平均值可以通过最小化每个 $k$ 的项来最小化。对 $\\lambda$ 求导，\n$$\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}\\left[\\frac{\\lambda^{2}\\sigma_{m}^{2} + s\\,\\sigma_{n}^{2}}{(s + \\lambda)^{2}}\\right] = \\frac{2(s + \\lambda)\\left[\\lambda \\sigma_{m}^{2}(s + \\lambda) - (\\lambda^{2}\\sigma_{m}^{2} + s\\,\\sigma_{n}^{2})\\right]}{(s + \\lambda)^{4}} = \\frac{2s\\left(\\lambda \\sigma_{m}^{2} - \\sigma_{n}^{2}\\right)}{(s + \\lambda)^{3}}.$$\n将导数设为零，并注意到 $s \\geq 0$，得到唯一的最小化解\n$$\\lambda^{\\star} = \\frac{\\sigma_{n}^{2}}{\\sigma_{m}^{2}}.$$\n\n最后，将 Tikhonov 解释为在数据域上作用的卷积神经网络 (CNN) 的单个线性层，其频率响应为 $\\hat{G}(k) = \\overline{\\hat{h}(k)}\\,\\varphi(s)$，其中 $s = |\\hat{h}(k)|^{2}$。估计量变为\n$$\\hat{m}(k) = \\hat{G}(k)\\hat{d}(k) = \\overline{\\hat{h}(k)}\\,\\varphi(s)\\left[\\hat{h}(k)\\hat{m}_{\\mathrm{true}}(k) + \\hat{n}(k)\\right] = s\\,\\varphi(s)\\,\\hat{m}_{\\mathrm{true}}(k) + \\overline{\\hat{h}(k)}\\,\\varphi(s)\\,\\hat{n}(k).$$\n误差为\n$$\\hat{e}(k) = \\left[s\\,\\varphi(s) - 1\\right]\\hat{m}_{\\mathrm{true}}(k) + \\overline{\\hat{h}(k)}\\,\\varphi(s)\\,\\hat{n}(k),$$\n其期望功率为\n$$\\mathbb{E}\\left[|\\hat{e}(k)|^{2}\\right] = \\left|s\\,\\varphi(s) - 1\\right|^{2}\\sigma_{m}^{2} + s\\,|\\varphi(s)|^{2}\\sigma_{n}^{2}.$$\n对于每个固定的 $s \\geq 0$，针对实数 $\\varphi(s)$ 最小化此表达式（该表达式对于 $\\varphi$ 是凸的），得到一阶条件\n$$2\\left[s\\,\\varphi(s) - 1\\right]s\\,\\sigma_{m}^{2} + 2s\\,\\varphi(s)\\,\\sigma_{n}^{2} = 0,$$\n化简后得到\n$$\\varphi(s)\\left[s^{2}\\sigma_{m}^{2} + s\\,\\sigma_{n}^{2}\\right] = s\\,\\sigma_{m}^{2}.$$\n因此最优的标量函数是\n$$\\varphi(s) = \\frac{\\sigma_{m}^{2}}{s\\,\\sigma_{m}^{2} + \\sigma_{n}^{2}}.$$\n通过此选择，线性 CNN 实现了维纳滤波器 $\\hat{G}(k) = \\overline{\\hat{h}(k)}\\,\\sigma_{m}^{2}/\\left(s\\,\\sigma_{m}^{2} + \\sigma_{n}^{2}\\right)$，当 $\\lambda = \\sigma_{n}^{2}/\\sigma_{m}^{2}$ 时，这与 Tikhonov 正则化一致。\n\n总之，最优正则化参数和最优 CNN 标量频率映射分别为 $\\lambda^{\\star} = \\sigma_{n}^{2}/\\sigma_{m}^{2}$ 和 $\\varphi(s) = \\sigma_{m}^{2}/(s\\,\\sigma_{m}^{2} + \\sigma_{n}^{2})$。", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{\\sigma_{n}^{2}}{\\sigma_{m}^{2}}  \\frac{\\sigma_{m}^{2}}{s\\,\\sigma_{m}^{2} + \\sigma_{n}^{2}}\\end{pmatrix}}$$", "id": "3583423"}, {"introduction": "在为地球物理成像设计神经网络时，架构的选择直接决定了模型能够解析的特征尺度。本练习聚焦于U-Net（一种在地球物理领域广泛应用的架构）的一个核心概念：感受野（receptive field）。您将通过第一性原理推导感受野的大小如何依赖于网络的深度和卷积核尺寸，并将其与从地震数据中恢复速度模型波长的物理能力直接联系起来 [@problem_id:3583499]。", "problem": "考虑一个 U-Net 风格的卷积神经网络 (CNN) 的一维变体，它沿地震时间轴应用，用于将炮集输入映射到一维垂直地震速度剖面。假设采用以下架构和约定，这些在实践中是标准的，并且当偏移距不是限制因素时，对于垂直剖面分析在科学上是合理的：\n\n- 网络沿时间严格一维，输入采样间隔为 $\\Delta t$（单位：秒）。\n- 编码器有 $L$ 个分辨率降低阶段。在每个编码器阶段 $l \\in \\{1,\\dots,L\\}$，有两个卷积核大小为 $k$（奇数，步幅为 $1$，膨胀为 $1$，并选择零填充以保持长度）的卷积层，然后是一个卷积核大小为 $2$、步幅为 $2$ 的最大池化层。\n- 在瓶颈层（经过 $L$ 次池化后），有两个卷积核大小为 $k$（步幅为 $1$，膨胀为 $1$，相同填充）的卷积层。\n- 解码器有 $L$ 个分辨率增加阶段。每个解码器阶段使用最近邻上采样（无卷积核）将分辨率提高 $2$ 倍，与相应的编码器特征图（跳跃连接）拼接，然后应用两个卷积核大小为 $k$（步幅为 $1$，膨胀为 $1$，相同填充）的卷积层。\n- 最后的 $1 \\times 1$ 卷积（卷积核大小为 $1$）将特征映射到输出。\n- 所有卷积都是线性且时不变的；没有大于 $1$ 的膨胀。\n\n您可以使用以下经过充分检验的事实：\n\n- 对于一个按前向顺序由 $i$ 索引的层序列，每个层的卷积核大小为 $k_i$，步幅为 $s_i$，则一个输出样本的理论感受野长度（以输入样本为单位）由基础值 $1$ 加上每个层由其卷积核贡献的增量跨度给出，该增量跨度乘以其前面所有层的步幅的乘积。\n- 一个卷积核大小为 $2$、步幅为 $2$ 的最大池化层贡献的感受野增量等于当前的有效输入步幅，并在此后将该步幅加倍。\n- 因子为 $2$ 的最近邻上采样本身不扩大感受野，但它将传播到后续层的有效输入步幅减半。\n\n假设背景速度为恒定值 $v_0$（单位：米/秒）且为近法向入射单次散射（单次散射 (Born) 近似），因此双程走时 $t$ 和深度 $z$ 之间的关系为 $t = \\frac{2 z}{v_0}$。在此映射下，深度波长为 $\\lambda$ 的正弦速度扰动对应于双程走时中周期为 $T = \\frac{2 \\lambda}{v_0}$ 的正弦波。为了使网络能够从炮集输入中捕获该分量的至少一个完整周期，沿时间轴的理论感受野必须跨越至少一个周期，即 $R_t \\, \\Delta t \\geq T$，其中 $R_t$ 是沿时间轴的以输入样本为单位的感受野。\n\n任务：\n\n- 根据上述 U-Net 架构，从第一性原理推导单个输出样本相对于输入的理论感受野 $R_t(L,k)$（以输入样本为单位）的显式闭式表达式。\n- 使用时间-深度关系 $t = \\frac{2 z}{v_0}$ 和单周期准则，将最大可恢复深度波长 $\\lambda_{\\max}$（单位：米）表示为 $L$、$k$、$\\Delta t$ 和 $v_0$ 的闭式解析函数。\n\n陈述您需要的任何额外最小假设。最终波长以米为单位表示。最终答案必须是单个闭式表达式。无需四舍五入。", "solution": "问题陈述已经过分析，并被确定为有效。它具有科学依据、是适定的、内部一致，并包含了推导出唯一解所需的所有必要信息。其语言精确，所用的物理和计算模型是该领域的标准简化方法。\n\n关于带有跳跃连接的 U-Net 架构中的感受野计算，需要一个最小假设：输出神经元的理论感受野由来自输入的最长影响路径决定。在指定的 U-Net 架构中，这对应于贯穿编码器、瓶颈层和解码器整个深度的路径。在每个解码器阶段，上采样的特征图与来自跳跃连接的特征图拼接在一起。后续卷积层中神经元的感受野是其相对于两个拼接输入的感受野的并集。由于上采样路径更深，其感受野严格更大，并包含了跳跃连接路径的感受野。因此，总的感受野是通过追踪这条最深路径来确定的，这是标准的解释。\n\n第一个任务是推导理论感受野 $R_t(L,k)$（以输入样本为单位）的表达式。我们使用提供的感受野增长公式：总感受野是 $1$ 加上所有层的增量贡献之和。一个卷积核大小为 $k_i$ 的层 $i$ 的贡献是 $(k_i-1)$ 乘以所有前面层的步幅乘积 $S_{i-1}$。\n$R_t = 1 + \\sum_{i} (k_i - 1) S_{i-1}$。\n\n让我们沿着网络的最长路径追踪感受野的增长和累积步幅。\n\n1. **编码器**：编码器有 $L$ 个阶段。对于每个阶段 $l \\in \\{1, \\dots, L\\}$：\n阶段 $l$ 输入端的累积步幅为 $S_{\\text{in},l} = 2^{l-1}$。\n每个阶段有两个卷积核大小为 $k$、步幅为 $1$ 的卷积层，以及一个卷积核大小为 $2$、步幅为 $2$ 的最大池化层。\n- 第一个卷积层（步幅 $1$）贡献 $(k-1)S_{\\text{in},l} = (k-1)2^{l-1}$。\n- 第二个卷积层（步幅 $1$）贡献 $(k-1)S_{\\text{in},l} = (k-1)2^{l-1}$。\n- 最大池化层（步幅 $2$）贡献 $(2-1)S_{\\text{in},l} = 2^{l-1}$。\n编码器阶段 $l$ 的总贡献为 $\\Delta R_l^{\\text{enc}} = (k-1)2^{l-1} + (k-1)2^{l-1} + 2^{l-1} = (2k-1)2^{l-1}$。\n所有 $L$ 个编码器阶段的总贡献是 $l$ 的总和：\n$$ \\Delta R_{\\text{enc}} = \\sum_{l=1}^{L} (2k-1)2^{l-1} = (2k-1) \\sum_{l=1}^{L} 2^{l-1} $$\n这是一个几何级数 $\\sum_{j=0}^{L-1} 2^j = \\frac{2^L-1}{2-1} = 2^L - 1$。\n$$ \\Delta R_{\\text{enc}} = (2k-1)(2^L - 1) $$\n经过 $L$ 个编码器阶段后，累积步幅为 $2^L$。\n\n2. **瓶颈层**：瓶颈层跟在编码器之后。其输入端的累积步幅为 $S_{\\text{bottle}} = 2^L$。\n它由两个卷积核大小为 $k$、步幅为 $1$ 的卷积层组成。\n- 第一个卷积贡献 $(k-1)S_{\\text{bottle}} = (k-1)2^L$。\n- 第二个卷积贡献 $(k-1)S_{\\text{bottle}} = (k-1)2^L$。\n瓶颈层的总贡献是：\n$$ \\Delta R_{\\text{bottle}} = 2(k-1)2^L $$\n步幅在通过瓶颈层时不变。\n\n3. **解码器**：解码器有 $L$ 个阶段。我们将其从最靠近瓶颈层的阶段到最靠近输出的阶段索引为 $l \\in \\{1, \\dots, L\\}$。\n对于每个解码器阶段 $l$，有一个上采样步骤，后跟两个卷积层。\n进入解码器阶段 $l$ 的累积步幅是 $2^{L-l+1}$。因子为 $2$ 的上采样将此步幅减小为 $S_{\\text{dec},l} = \\frac{1}{2} \\cdot 2^{L-l+1} = 2^{L-l}$。此步幅应用于后续的卷积。\n两个卷积层都具有卷积核大小 $k$ 和步幅 $1$。\n解码器阶段 $l$ 的总贡献是：\n$\\Delta R_l^{\\text{dec}} = (k-1)S_{\\text{dec},l} + (k-1)S_{\\text{dec},l} = 2(k-1)2^{L-l}$。\n所有 $L$ 个解码器阶段的总贡献是：\n$$ \\Delta R_{\\text{dec}} = \\sum_{l=1}^{L} 2(k-1)2^{L-l} = 2(k-1) \\sum_{l=1}^{L} 2^{L-l} $$\n令 $j=L-l$。该和变为 $\\sum_{j=0}^{L-1} 2^j = 2^L - 1$。\n$$ \\Delta R_{\\text{dec}} = 2(k-1)(2^L - 1) $$\n\n4. **最终卷积**：应用一个最后的 $1 \\times 1$ 卷积（卷积核大小为 $1$，步幅为 $1$）。进入该层的步幅为 $1$。它对感受野的贡献是 $(1-1) \\cdot 1 = 0$。\n\n5. **总感受野**：总感受野 $R_t(L,k)$ 是初始值 $1$ 与所有贡献之和：\n$$ R_t(L,k) = 1 + \\Delta R_{\\text{enc}} + \\Delta R_{\\text{bottle}} + \\Delta R_{\\text{dec}} $$\n$$ R_t(L,k) = 1 + (2k-1)(2^L-1) + 2(k-1)2^L + 2(k-1)(2^L-1) $$\n我们可以将带有 $(2^L-1)$ 的项分组：\n$$ R_t(L,k) = 1 + \\left( (2k-1) + 2(k-1) \\right)(2^L-1) + 2(k-1)2^L $$\n$$ R_t(L,k) = 1 + (2k-1+2k-2)(2^L-1) + (2k-2)2^L $$\n$$ R_t(L,k) = 1 + (4k-3)(2^L-1) + (2k-2)2^L $$\n展开各项：\n$$ R_t(L,k) = 1 + (4k-3)2^L - (4k-3) + (2k-2)2^L $$\n$$ R_t(L,k) = 1 - 4k + 3 + (4k-3 + 2k-2)2^L $$\n$$ R_t(L,k) = 4 - 4k + (6k-5)2^L $$\n$$ R_t(L,k) = (6k-5)2^L - 4(k-1) $$\n这就是理论感受野的闭式表达式。\n\n第二个任务是找到最大可恢复深度波长 $\\lambda_{\\max}$。\n捕获时间周期为 $T$ 的特征的条件是，时间上的感受野 $R_t \\cdot \\Delta t$ 必须至少为一个周期：$R_t \\Delta t \\ge T$。\n网络可以解析的最大周期 $T_{\\max}$ 受其感受野的限制：\n$$ T_{\\max} = R_t(L,k) \\Delta t $$\n问题给出了双程走时周期 $T$ 与速度扰动的深度波长 $\\lambda$ 之间的物理关系：$T = \\frac{2\\lambda}{v_0}$。\n因此，最大可恢复波长 $\\lambda_{\\max}$ 对应于 $T_{\\max}$：\n$$ T_{\\max} = \\frac{2\\lambda_{\\max}}{v_0} $$\n将 $T_{\\max}$ 的两个表达式相等：\n$$ \\frac{2\\lambda_{\\max}}{v_0} = R_t(L,k) \\Delta t $$\n解出 $\\lambda_{\\max}$：\n$$ \\lambda_{\\max} = \\frac{v_0 \\Delta t}{2} R_t(L,k) $$\n代入推导出的 $R_t(L,k)$ 表达式：\n$$ \\lambda_{\\max} = \\frac{v_0 \\Delta t}{2} \\left[ (6k-5)2^L - 4(k-1) \\right] $$\n这个表达式也可以写成：\n$$ \\lambda_{\\max} = v_0 \\Delta t \\left[ (6k-5)2^{L-1} - 2(k-1) \\right] $$\n这就是最大可恢复深度波长的最终闭式解析函数。", "answer": "$$\\boxed{\\frac{v_0 \\Delta t}{2} \\left( (6k-5)2^L - 4(k-1) \\right)}$$", "id": "3583499"}, {"introduction": "将传统的迭代优化算法“展开”成一个深度网络，是构建兼具物理解释性和强大性能的现代反演模型的有效策略。本练习将指导您设计并分析一个展开的梯度下降网络，其中每一步的步长和预条件子都是可学习的参数。通过推导其收敛和稳定性条件，您将深入理解如何确保这种“学习优化器”在求解反演问题时保持数值上的稳健性 [@problem_id:3583447]。", "problem": "为全波形反演设计一个数学上精确的、展开的类梯度架构，其中每一层执行一个步骤 $$m^{k+1} = m^{k} - \\alpha_k P_k \\nabla J(m^k),$$ 目标函数为 $$J(m) = \\lVert F(m) - d \\rVert_2^2 + \\lambda R(m),$$ 假设前向算子和正则化器使得 $J$ 的梯度在欧几里得范数下是全局 Lipschitz 连续的，且 Lipschitz 常数为 $L$。矩阵 $P_k$ 代表学习到的线性预条件子，标量 $\\alpha_k$ 是学习到的步长。使用具有 $L$-Lipschitz 梯度的函数的基本平滑不等式、对称半正定矩阵的性质以及算子范数界作为您推导的基础。\n\n您必须执行以下操作。\n\n1) 从一个 $L$-平滑函数的平滑不等式 $$J(y) \\le J(x) + \\nabla J(x)^\\top (y-x) + \\frac{L}{2} \\lVert y-x \\rVert_2^2,$$ 开始，推导更新步骤 $$m^{+} = m - \\alpha P \\nabla J(m),$$ 的充分下降条件，该条件表示为一个显式的不等式，它将步长 $\\alpha$、学习到的矩阵 $P$ 和 Lipschitz 常数 $L$ 耦合在一起。您的推导必须通过处理其对称部分 $$S \\equiv \\frac{P + P^\\top}{2},$$ 来处理 $P$ 不一定对称的一般情况，并且必须以一个可实现的形式的界结束\n$$\\text{若 } \\mu \\equiv \\lambda_{\\min}(S)  0 \\text{ 且 } M \\equiv \\lVert P \\rVert_2, \\text{ 则 } J(m^{+})  J(m) \\text{ 每当 } 0  \\alpha  \\frac{2\\mu}{L M^2} \\text{ 时成立。}$$\n\n2) 为单层映射 $$T(m) \\equiv m - \\alpha P \\nabla J(m)$$ 提供一个 Lipschitz 稳定性界。具体来说，推导 $T$ 在欧几里得范数下的 Lipschitz 常数的一个充分上界，形式为 $$\\lVert T(x) - T(y) \\rVert_2 \\le K \\lVert x - y \\rVert_2,$$ 并用 $\\alpha$、$L$ 和 $\\lVert P \\rVert_2$ 显式表示 $K$。清楚地说明使此界为有限的任何附加充分条件，以及（如果可用）使 $T$ 在适当的范数下成为非扩张的或收缩的任何保守参数范围。\n\n3) 在一个二次代理上实现并评估该理论，该代理在带有 Tikhonov 正则化的线性化全波形反演中是标准的，即 $$J(m) = \\lVert A m - b \\rVert_2^2 + \\lambda \\lVert L m \\rVert_2^2,$$ 其中 $A \\in \\mathbb{R}^{n \\times n}$，$b \\in \\mathbb{R}^n$，$L \\in \\mathbb{R}^{n \\times n}$ 和 $\\lambda  0$ 是固定的。对于这个二次函数，Hessian 矩阵是常数，$$H = \\nabla^2 J(m) = 2\\left(A^\\top A + \\lambda L^\\top L\\right),$$ 所以 $\\nabla J$ 的全局 Lipschitz 常数等于 $$L = \\lVert H \\rVert_2.$$\n\n对于数值评估，使用维度 $n = 3$ 和以下测试套件（这些选择是无量纲的，因此不需要物理单位）：\n\n- 所有案例的共享数据：\n  - $$A = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.0  0.3 \\\\ 0.0  0.3  1.5 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.3 \\end{bmatrix}, \\quad L = \\begin{bmatrix} 1.0  0.0  0.0 \\\\ 0.0  1.0  -1.0 \\\\ 0.0  0.0  1.0 \\end{bmatrix}, \\quad \\lambda = 0.1,$$\n  - 初始模型 $$m^{0} = \\begin{bmatrix} -0.2 \\\\ 0.5 \\\\ 1.0 \\end{bmatrix}.$$\n- 四种学习到的层配置：\n  - 案例 1：$$P = \\operatorname{diag}(0.5, 1.0, 2.0), \\quad \\alpha = \\frac{1}{4} \\cdot \\frac{2 \\mu}{L M^2}.$$\n  - 案例 2：与案例 1 相同的 $P$，$$\\alpha = \\frac{2 \\mu}{L M^2}.$$\n  - 案例 3：与案例 1 相同的 $P$，$$\\alpha = 1.1 \\cdot \\frac{2 \\mu}{L M^2}.$$\n  - 案例 4：$$P = \\begin{bmatrix} 1.0  0.5  0.0 \\\\ 0.0  1.0  0.2 \\\\ 0.0  0.0  0.8 \\end{bmatrix}, \\quad \\alpha = \\frac{1}{2} \\cdot \\frac{2 \\mu}{L M^2},$$ 其中对于非对称的 $P$，$\\mu = \\lambda_{\\min}\\!\\left(\\frac{P + P^\\top}{2}\\right)$ 且 $M = \\lVert P \\rVert_2.$\n\n对于每个案例：\n  - 计算理论阈值 $$\\alpha_\\star = \\frac{2 \\mu}{L M^2}.$$\n  - 报告理论严格下降条件的布尔值，即 $$\\alpha  \\alpha_\\star \\text{ 和 } \\mu  0$$ 是否成立。\n  - 执行一次更新 $$m^{1} = m^{0} - \\alpha P \\nabla J(m^{0})$$ 并报告实际严格下降的布尔值，即 $$J(m^{1})  J(m^{0})$$ 是否成立。\n  - 报告保守的 Lipschitz 界 $$K_{\\mathrm{bound}} = 1 + \\alpha L \\lVert P \\rVert_2.$$\n  - 对于此二次函数 $J$，单层映射是线性的：$$T(m) = \\left(I - \\alpha P H\\right)m + \\alpha P \\cdot \\mathrm{const},$$ 所以其精确的欧几里得 Lipschitz 常数等于 $$\\lVert I - \\alpha P H \\rVert_2.$$ 报告此值。\n\n您的程序必须输出一行，其中包含连接成一个列表的所有案例结果，顺序完全如下\n$$[\\text{theory\\_descent}_1, \\text{actual\\_descent}_1, K_{\\mathrm{bound},1}, K_{\\mathrm{actual},1}, \\; \\ldots \\;, \\text{theory\\_descent}_4, \\text{actual\\_descent}_4, K_{\\mathrm{bound},4}, K_{\\mathrm{actual},4}],$$\n其中布尔值为小写，浮点数为四舍五入到 $6$ 位小数的十进制数。输出必须是单行，且严格为一个用方括号括起来的逗号分隔列表，不含额外文本。", "solution": "该问题是有效的，因为它在科学上基于优化理论，问题设定良好，包含所有必要的数据和条件，并且陈述客观。它为学习优化算法的分析提供了一个标准而严谨的练习。\n\n解决方案按要求分为三个部分。\n\n### 第 1 部分：充分下降条件的推导\n\n我们从一个具有 $L$-Lipschitz 连续梯度 $\\nabla J$ 的函数 $J(m)$ 的基本不等式开始，该不等式对其定义域中的任意两点 $x$ 和 $y$ 均成立：\n$$J(y) \\le J(x) + \\nabla J(x)^\\top (y-x) + \\frac{L}{2} \\lVert y-x \\rVert_2^2.$$\n我们关注的是一种预处理类梯度方法的单步更新，其形式为\n$$m^{k+1} = m^{k} - \\alpha_k P_k \\nabla J(m^k).$$\n为简化单步的符号表示，令 $m = m^k$，$m^{+} = m^{k+1}$，$\\alpha = \\alpha_k$，以及 $P = P_k$。更新步骤为 $m^{+} = m - \\alpha P \\nabla J(m)$。\n\n让我们将 $x = m$ 和 $y = m^{+}$ 代入平滑不等式：\n$$J(m^{+}) \\le J(m) + \\nabla J(m)^\\top (m^{+} - m) + \\frac{L}{2} \\lVert m^{+} - m \\rVert_2^2.$$\n位移向量是 $m^{+} - m = -\\alpha P \\nabla J(m)$。将此表达式代入不等式中得到：\n$$J(m^{+}) \\le J(m) + \\nabla J(m)^\\top (-\\alpha P \\nabla J(m)) + \\frac{L}{2} \\lVert -\\alpha P \\nabla J(m) \\rVert_2^2.$$\n为简洁起见，令 $g \\equiv \\nabla J(m)$。不等式变为：\n$$J(m^{+}) \\le J(m) - \\alpha g^\\top P g + \\frac{L \\alpha^2}{2} \\lVert P g \\rVert_2^2.$$\n为实现严格下降，我们需要 $J(m^{+})  J(m)$。如果右侧后两项之和严格为负，则此条件得到保证：\n$$- \\alpha g^\\top P g + \\frac{L \\alpha^2}{2} \\lVert P g \\rVert_2^2  0.$$\n假设步长 $\\alpha  0$ 且我们不处于驻点（即 $g \\neq 0$），我们可以除以 $\\alpha$：\n$$- g^\\top P g + \\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2  0,$$\n可以重新排列为：\n$$\\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2  g^\\top P g.$$\n项 $g^\\top P g$ 是一个二次型。由于预条件子 $P$ 不保证是对称的，这个二次型与 $P$ 的特征值没有直接关系。然而，对于任何实矩阵 $P$ 和向量 $g$，二次型由 $P$ 的对称部分决定。令 $S \\equiv \\frac{P + P^\\top}{2}$。则：\n$$g^\\top S g = g^\\top \\left(\\frac{P + P^\\top}{2}\\right) g = \\frac{1}{2} (g^\\top P g + g^\\top P^\\top g) = \\frac{1}{2} (g^\\top P g + (P g)^\\top g).$$\n因为 $(P g)^\\top g$ 是一个标量，它等于其转置，$(P g)^\\top g = g^\\top P g$。因此，$g^\\top S g = g^\\top P g$。\n\n下降不等式可以使用 $S$ 重写：\n$$\\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2  g^\\top S g.$$\n为了建立一个只依赖于 $P$ 的性质而不依赖于具体梯度 $g$ 的充分条件，我们使用给定的界。我们给定 $\\mu \\equiv \\lambda_{\\min}(S)  0$，其中 $\\lambda_{\\min}(S)$ 是 $P$ 的对称部分的最小特征值。根据对称矩阵 $S$ 的瑞利商定理，我们有：\n$$g^\\top S g \\ge \\lambda_{\\min}(S) \\lVert g \\rVert_2^2 = \\mu \\lVert g \\rVert_2^2.$$\n我们还给定 $M \\equiv \\lVert P \\rVert_2$，这是 $P$ 的谱范数。根据诱导矩阵范数的定义：\n$$\\lVert P g \\rVert_2 \\le \\lVert P \\rVert_2 \\lVert g \\rVert_2 = M \\lVert g \\rVert_2 \\implies \\lVert P g \\rVert_2^2 \\le M^2 \\lVert g \\rVert_2^2.$$\n为了保证不等式 $\\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2  g^\\top S g$ 对任何 $g \\neq 0$ 都成立，满足一个更保守的不等式就足够了，该不等式通过用一个上界替换左侧，用一个下界替换右侧而得到：\n$$\\frac{L \\alpha}{2} (M^2 \\lVert g \\rVert_2^2)  \\mu \\lVert g \\rVert_2^2.$$\n由于我们假设 $g \\neq 0$，我们可以除以正标量 $\\lVert g \\rVert_2^2$：\n$$\\frac{L \\alpha M^2}{2}  \\mu.$$\n解出 $\\alpha$，我们得到关于步长的充分条件：\n$$\\alpha  \\frac{2\\mu}{L M^2}.$$\n将此与 $\\alpha  0$ 以及二次型的下界为正（即 $\\mu  0$）的要求相结合，我们得到保证严格下降的最终条件：\n$$J(m^{+})  J(m) \\text{ 每当 } \\mu  0 \\text{ 且 } 0  \\alpha  \\frac{2\\mu}{L M^2} \\text{ 时成立。}$$\n\n### 第 2 部分：Lipschitz 稳定性界\n\n我们的目标是为单层映射 $T(m) \\equiv m - \\alpha P \\nabla J(m)$ 在欧几里得范数下找到一个 Lipschitz 常数 $K$。也就是说，我们寻求一个界 $K$，使得对于任意两点 $x$ 和 $y$：\n$$\\lVert T(x) - T(y) \\rVert_2 \\le K \\lVert x - y \\rVert_2.$$\n让我们分析差值 $T(x) - T(y)$：\n$$T(x) - T(y) = (x - \\alpha P \\nabla J(x)) - (y - \\alpha P \\nabla J(y)) = (x - y) - \\alpha P (\\nabla J(x) - \\nabla J(y)).$$\n取 $L_2$-范数并应用三角不等式：\n$$\\lVert T(x) - T(y) \\rVert_2 = \\lVert (x - y) - \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 \\le \\lVert x - y \\rVert_2 + \\lVert \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2.$$\n我们使用矩阵和向量范数的性质来分析第二项：\n$$\\lVert \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 = |\\alpha| \\cdot \\lVert P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 \\le |\\alpha| \\cdot \\lVert P \\rVert_2 \\cdot \\lVert \\nabla J(x) - \\nabla J(y) \\rVert_2.$$\n梯度 $\\nabla J$ 是全局 Lipschitz 连续的，常数为 $L$，意味着 $\\lVert \\nabla J(x) - \\nabla J(y) \\rVert_2 \\le L \\lVert x - y \\rVert_2$。将此代入我们的表达式：\n$$\\lVert \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 \\le |\\alpha| \\cdot \\lVert P \\rVert_2 \\cdot (L \\lVert x - y \\rVert_2).$$\n假设步长 $\\alpha  0$，我们将其代回主不等式：\n$$\\lVert T(x) - T(y) \\rVert_2 \\le \\lVert x - y \\rVert_2 + \\alpha L \\lVert P \\rVert_2 \\lVert x - y \\rVert_2.$$\n提出公因子 $\\lVert x-y \\rVert_2$：\n$$\\lVert T(x) - T(y) \\rVert_2 \\le (1 + \\alpha L \\lVert P \\rVert_2) \\lVert x - y \\rVert_2.$$\n这为 $T$ 的 Lipschitz 常数建立了一个充分上界，由下式给出\n$$K = 1 + \\alpha L \\lVert P \\rVert_2.$$\n只要步长 $\\alpha$、梯度的 Lipschitz 常数 $L$ 以及预条件子的范数 $\\lVert P \\rVert_2$ 都是有限的，这个界就是有限的。\n这个特定的界是根据三角不等式推导出来的，因此是保守的。由于 $\\alpha  0$，$L  0$，且 $\\lVert P \\rVert_2 \\ge 0$，这个界 $K$ 总是大于或等于 $1$。因此，它不能用来证明映射 $T$ 是一个收缩映射（$K  1$）。要证明收缩性，需要更紧密的分析，这通常特定于问题的结构。对于 $J$ 是二次函数的特殊情况，如第 3 部分所示，映射 $T$ 变为仿射映射，其精确的 Lipschitz 常数可以计算出来，并且可能确实小于 $1$。\n\n### 第 3 部分：数值评估设置\n\n对于数值评估，我们给定二次目标函数：\n$$J(m) = \\lVert A m - b \\rVert_2^2 + \\lambda \\lVert L m \\rVert_2^2.$$\n这可以展开为 $J(m) = (A m - b)^\\top(A m - b) + \\lambda (L m)^\\top(L m)$，它简化为一个标准的二次型：\n$$J(m) = m^\\top A^\\top A m - 2b^\\top A m + b^\\top b + \\lambda m^\\top L^\\top L m = m^\\top(A^\\top A + \\lambda L^\\top L) m - 2 (A^\\top b)^\\top m + b^\\top b.$$\n梯度 $\\nabla J(m)$ 通过对 $m$ 求导得到：\n$$\\nabla J(m) = 2(A^\\top A + \\lambda L^\\top L)m - 2A^\\top b.$$\nHessian 矩阵 $\\nabla^2 J(m)$ 是梯度的导数：\n$$H \\equiv \\nabla^2 J(m) = 2(A^\\top A + \\lambda L^\\top L).$$\n由于 Hessian 矩阵 $H$ 是一个常数矩阵，梯度 $\\nabla J(m)$ 是 $m$ 的一个线性映射加上一个常数，这使其成为全局 Lipschitz 的。$\\nabla J(m)$ 的 Lipschitz 常数是其导数矩阵 $H$ 的算子范数。因此，全局 Lipschitz 常数 $L$ 由下式给出：\n$$L = \\lVert H \\rVert_2 = \\lVert 2(A^\\top A + \\lambda L^\\top L) \\rVert_2.$$\n单层映射 $T(m)$ 变成一个仿射变换：\n$$T(m) = m - \\alpha P \\nabla J(m) = m - \\alpha P (H m - 2A^\\top b) = (I - \\alpha P H) m + 2\\alpha P A^\\top b.$$\n仿射映射 $f(x) = M x + c$ 的 Lipschitz 常数是 $\\lVert M \\rVert_2$。因此，我们的映射 $T(m)$ 的精确 Lipschitz 常数是：\n$$K_{\\mathrm{actual}} = \\lVert I - \\alpha P H \\rVert_2.$$\n下面的实现将使用这些公式计算每个指定案例的量。", "answer": "[true,true,1.124233,0.941913,false,true,1.496931,0.767651,false,false,1.546624,0.744416,true,true,1.385732,0.852655]", "id": "3583447"}]}