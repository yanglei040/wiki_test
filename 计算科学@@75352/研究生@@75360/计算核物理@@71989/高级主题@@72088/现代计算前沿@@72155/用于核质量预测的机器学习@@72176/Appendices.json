{"hands_on_practices": [{"introduction": "核质量的实验测量值总是伴随着不确定性，正确处理这些不确定性是建立可靠预测模型的关键。本练习将从最大似然估计的第一性原理出发，推导如何合并多个独立测量结果，并阐明这如何直接转化为加权最小二乘损失函数中的样本权重 [@problem_id:3568202]。掌握这一基本功是确保模型训练正确反映实验数据精度的基石。", "problem": "考虑一个核素，其质量盈余由两个独立的实验进行测量。设其潜在的真实质量盈余为 $m \\in \\mathbb{R}$。每个报告值 $y_{i}$ 由高斯测量方程 $y_{i} = m + \\epsilon_{i}$ 建模，其中 $\\epsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{i}^{2})$ 且 $(\\epsilon_{1}, \\epsilon_{2})$ 相互独立。对于同一原子核，给定以下两份报告：$y_{1} = -72382.6\\,\\mathrm{keV}$，标准不确定度 $\\sigma_{1} = 4.0\\,\\mathrm{keV}$；以及 $y_{2} = -72385.1\\,\\mathrm{keV}$，标准不确定度 $\\sigma_{2} = 2.5\\,\\mathrm{keV}$。从高斯似然出发，且不使用任何快捷公式，推导 $m$ 的最大似然估计量 $\\hat{m}$ 及其标准不确定度 $\\sigma_{\\hat{m}}$。然后，在机器学习（ML）的加权最小二乘训练目标背景下，其中训练损失函数的形式为 $L = \\sum_{i} w_{i} \\left(y_{i} - f(x_{i})\\right)^{2}$，推导能够恢复高斯最大似然原理的样本权重 $w_{i}$ 的选择，并确定如果将两次测量首先聚合为位于 $y = \\hat{m}$ 的单个标签并用作单个训练样本时，应分配的单一等效损失权重 $w_{\\mathrm{agg}}$。对于给定数据，数值计算 $\\hat{m}$、$\\sigma_{\\hat{m}}$ 和 $w_{\\mathrm{agg}}$。将 $\\hat{m}$ 以 $\\mathrm{keV}$ 为单位表示，并四舍五入到五位有效数字；将 $\\sigma_{\\hat{m}}$ 以 $\\mathrm{keV}$ 为单位表示，并四舍五入到四位有效数字；将 $w_{\\mathrm{agg}}$ 以 $\\mathrm{keV}^{-2}$ 为单位表示，并四舍五入到四位有效数字。", "solution": "该问题提法恰当且有科学依据。我们继续进行推导和计算。\n\n问题陈述，每次对真实质量盈余 $m$ 的测量 $y_i$ 都由一个高斯分布建模。在给定真实值 $m$ 和标准不确定度 $\\sigma_i$ 的情况下，观测到值 $y_i$ 的概率密度函数（PDF）为：\n$$p(y_i|m, \\sigma_i) = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp\\left(-\\frac{(y_i - m)^2}{2\\sigma_i^2}\\right)$$\n我们有两个独立的测量值 $(y_1, \\sigma_1)$ 和 $(y_2, \\sigma_2)$。由于独立性，同时观测到 $y_1$ 和 $y_2$ 的联合概率，即似然函数 $L(m)$，是各PDF的乘积：\n$$L(m; y_1, y_2) = p(y_1|m, \\sigma_1) \\cdot p(y_2|m, \\sigma_2) = \\frac{1}{2\\pi\\sigma_1\\sigma_2} \\exp\\left(-\\frac{(y_1 - m)^2}{2\\sigma_1^2} - \\frac{(y_2 - m)^2}{2\\sigma_2^2}\\right)$$\n最大似然估计量（MLE）$\\hat{m}$ 是使 $L(m)$ 最大化的 $m$ 值。最大化对数似然函数 $\\ell(m) = \\ln(L(m))$ 是等价的，且计算上更简单：\n$$\\ell(m) = \\ln\\left(\\frac{1}{2\\pi\\sigma_1\\sigma_2}\\right) - \\frac{(y_1 - m)^2}{2\\sigma_1^2} - \\frac{(y_2 - m)^2}{2\\sigma_2^2}$$\n为了找到最大值，我们对 $\\ell(m)$ 关于 $m$ 求导，并令其为零。常数项在求导后消失。\n$$\\frac{d\\ell(m)}{dm} = -\\frac{2(y_1 - m)(-1)}{2\\sigma_1^2} - \\frac{2(y_2 - m)(-1)}{2\\sigma_2^2} = \\frac{y_1 - m}{\\sigma_1^2} + \\frac{y_2 - m}{\\sigma_2^2}$$\n将导数设为零以求解估计量 $\\hat{m}$：\n$$\\frac{y_1 - \\hat{m}}{\\sigma_1^2} + \\frac{y_2 - \\hat{m}}{\\sigma_2^2} = 0$$\n$$\\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2} = \\hat{m}\\left(\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}\\right)$$\n解出 $\\hat{m}$：\n$$\\hat{m} = \\frac{\\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2}}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}} = \\frac{\\sum_{i=1}^{2} \\frac{y_i}{\\sigma_i^2}}{\\sum_{i=1}^{2} \\frac{1}{\\sigma_i^2}}$$\n这是测量值的反方差加权平均值。\n\n接下来，我们推导估计量的标准不确定度 $\\sigma_{\\hat{m}}$。最大似然估计量的方差 $\\sigma_{\\hat{m}}^2$ 由克拉默-拉奥下界给出，对于无偏估计量，它等于费雪信息 $I(m)$ 的倒数。费雪信息为 $I(m) = -E\\left[\\frac{d^2\\ell(m)}{dm^2}\\right]$。我们计算对数似然的二阶导数：\n$$\\frac{d^2\\ell(m)}{dm^2} = \\frac{d}{dm}\\left(\\frac{y_1 - m}{\\sigma_1^2} + \\frac{y_2 - m}{\\sigma_2^2}\\right) = -\\frac{1}{\\sigma_1^2} - \\frac{1}{\\sigma_2^2} = -\\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}$$\n由于二阶导数是一个常数，不依赖于数据 $y_i$，其期望就是其本身的值。\n$$I(m) = -E\\left[-\\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}\\right] = \\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}$$\n$\\hat{m}$ 的方差是费雪信息的倒数：\n$$\\sigma_{\\hat{m}}^2 = [I(m)]^{-1} = \\left(\\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}\\right)^{-1} = \\frac{1}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}$$\n标准不确定度 $\\sigma_{\\hat{m}}$ 是方差的平方根：\n$$\\sigma_{\\hat{m}} = \\sqrt{\\sigma_{\\hat{m}}^2} = \\left(\\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}\\right)^{-1/2}$$\n\n现在，我们将其与机器学习（ML）中的加权最小二乘训练目标联系起来。损失函数给定为 $L = \\sum_{i} w_{i} \\left(y_{i} - f(x_{i})\\right)^{2}$。在我们的案例中，我们为一个核素估计单个参数 $m$，因此模型预测是恒定的，即 $f(x_i) = m$。损失函数简化为：\n$$L(m) = \\sum_{i=1}^{2} w_i (y_i - m)^2$$\n最小化此损失函数应等价于最大化高斯似然。最大化似然等价于最小化负对数似然。忽略常数项，需要最小化的目标是：\n$$-\\ell(m) \\propto \\sum_{i=1}^{2} \\frac{(y_i - m)^2}{2\\sigma_i^2}$$\n为了使机器学习损失 $L(m)$ 与此目标等价，它们关于 $m$ 的函数形式必须匹配。比较这两个表达式：\n$$\\sum_{i=1}^{2} w_i (y_i - m)^2 \\quad \\text{vs.} \\quad \\sum_{i=1}^{2} \\frac{1}{2\\sigma_i^2} (y_i - m)^2$$\n如果权重 $w_i$ 与 $1/\\sigma_i^2$ 成正比，则此等价性成立。在统计加权中，常规选择是设 $w_i = 1/\\sigma_i^2$。这使得损失函数等于卡方统计量 $\\chi^2$。\n\n最后，我们考虑将两次测量聚合为单个数据点 $(y, \\sigma_y)$，其中 $y = \\hat{m}$ 且 $\\sigma_y = \\sigma_{\\hat{m}}$。此单个样本的训练损失将为 $L_{\\mathrm{agg}} = w_{\\mathrm{agg}}(y - m)^2 = w_{\\mathrm{agg}}(\\hat{m} - m)^2$。遵循刚推导的原理，合适的权重 $w_{\\mathrm{agg}}$ 是此聚合数据点方差的倒数。我们的聚合点 $\\hat{m}$ 的方差是 $\\sigma_{\\hat{m}}^2$。因此：\n$$w_{\\mathrm{agg}} = \\frac{1}{\\sigma_{\\hat{m}}^2}$$\n代入 $\\sigma_{\\hat{m}}^2$ 的表达式：\n$$w_{\\mathrm{agg}} = \\frac{1}{\\left(\\frac{1}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}\\right)} = \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}$$\n由于 $w_i = 1/\\sigma_i^2$，我们看到 $w_{\\mathrm{agg}} = w_1 + w_2$。聚合权重是单个权重的和。\n\n数值计算：\n给定数据：$y_{1} = -72382.6\\,\\mathrm{keV}$，$\\sigma_{1} = 4.0\\,\\mathrm{keV}$；以及 $y_{2} = -72385.1\\,\\mathrm{keV}$，$\\sigma_{2} = 2.5\\,\\mathrm{keV}$。\n首先，计算方差和权重：\n$\\sigma_1^2 = (4.0)^2 = 16.0\\,\\mathrm{keV}^2 \\implies w_1 = 1/16.0 = 0.0625\\,\\mathrm{keV}^{-2}$\n$\\sigma_2^2 = (2.5)^2 = 6.25\\,\\mathrm{keV}^2 \\implies w_2 = 1/6.25 = 0.16\\,\\mathrm{keV}^{-2}$\n\n计算 $\\hat{m}$：\n$$\\hat{m} = \\frac{w_1 y_1 + w_2 y_2}{w_1 + w_2} = \\frac{(0.0625)(-72382.6) + (0.16)(-72385.1)}{0.0625 + 0.16} = \\frac{-4523.9125 - 11581.616}{0.2225} = \\frac{-16105.5285}{0.2225} \\approx -72384.4000\\,\\mathrm{keV}$$\n四舍五入到五位有效数字，$\\hat{m} = -72384\\,\\mathrm{keV}$。\n\n计算 $\\sigma_{\\hat{m}}$：\n$$\\sigma_{\\hat{m}} = (w_1 + w_2)^{-1/2} = (0.2225)^{-1/2} \\approx \\sqrt{4.494382...} \\approx 2.119995...\\,\\mathrm{keV}$$\n四舍五入到四位有效数字，$\\sigma_{\\hat{m}} = 2.120\\,\\mathrm{keV}$。\n\n计算 $w_{\\mathrm{agg}}$：\n$$w_{\\mathrm{agg}} = w_1 + w_2 = 0.0625 + 0.16 = 0.2225\\,\\mathrm{keV}^{-2}$$\n四舍五入到四位有效数字，$w_{\\mathrm{agg}} = 0.2225\\,\\mathrm{keV}^{-2}$。", "answer": "$$\\boxed{\\begin{pmatrix} -72384  2.120  0.2225 \\end{pmatrix}}$$", "id": "3568202"}, {"introduction": "成功的物理机器学习模型往往巧妙地融合了领域知识，这一过程称为特征工程。本练习将探讨如何将核物理的见解（如半经验质量公式和幻数）转化为一个可解释的线性预测器的特征 [@problem_id:3568248]。我们还将引入一种简单的对抗性测试，以量化模型对这些关键特征的敏感度，这是理解和验证模型行为的重要步骤。", "problem": "您的任务是设计和评估一个用于原子核结合能的线性机器学习预测器，该预测器对一个编码了邻近原子核幻数信息的工程特征具有明确的敏感性。您将通过翻转该工程特征来测试其对抗鲁棒性，同时保持原子核的物理身份不变。目标是量化预测的结合能对这种特征错误设定的敏感性。该问题必须实现为一个完整的、可运行的程序，无需任何用户输入即可产生所需的输出。\n\n基本出发点是半经验质量公式，它为原子核结合能提供了一个经过充分检验的基线。一个拥有质子数 $Z$ 和中子数 $N$ 的原子核的总结合能被建模为液滴项、对能贡献以及一个与邻近幻数相关的小的壳层修正项的叠加。该模型并非旨在精确无误，但必须在科学上是合理的并且量纲一致。\n\n1. 基准真相映射。使用以下组合定义以兆电子伏特（megaelectronvolt (MeV)）为单位的基准真相结合能函数 $B^{\\star}(Z,N)$：\n   - 令 $A = Z + N$。\n   - 液滴模型基线项使用常数 $a_v = 15.75$、$a_s = 17.80$、$a_c = 0.711$、$a_a = 23.70$（均以 $\\mathrm{MeV}$ 为单位，并带有适当的量纲缩放）进行组合：\n     $$B_{\\mathrm{LDM}}(Z,N) = a_v A - a_s A^{2/3} - a_c \\frac{Z(Z-1)}{A^{1/3}} - a_a \\frac{(N-Z)^2}{A}.$$\n   - 对能项使用 $a_p = 11.20$ 和标准的宇称规则：\n     - 如果 $Z$ 和 $N$ 均为偶数，$\\delta = + a_p A^{-1/2}$，\n     - 如果 $Z$ 和 $N$ 均为奇数，$\\delta = - a_p A^{-1/2}$，\n     - 否则，$\\delta = 0$。\n   - 幻数邻近性指示器 $s(Z,N)$ 是一个二元工程特征，由固定的质子幻数集 $\\{2,8,20,28,50,82\\}$ 和中子幻数集 $\\{2,8,20,28,50,82,126\\}$ 定义。如果 $Z$ 到质子集的最小距离或 $N$ 到中子集的最小距离小于或等于 $1$，则设 $s(Z,N) = 1$，否则设 $s(Z,N) = 0$。类壳层修正使用两个常数 $\\gamma_0 = 3.00$ 和 $\\gamma_1 = 8.00$（单位为 $\\mathrm{MeV}$），形式如下：\n     $$S_{\\mathrm{shell}}(Z,N) = \\left(\\gamma_0 + \\gamma_1 A^{-1/3}\\right) s(Z,N).$$\n   - 基准真相结合能为：\n     $$B^{\\star}(Z,N) = B_{\\mathrm{LDM}}(Z,N) + \\delta(Z,N) + S_{\\mathrm{shell}}(Z,N).$$\n   所有计算都以 $\\mathrm{MeV}$ 为单位进行解释，并且 $A$ 的幂次带有隐含的单位，以确保整体能量量纲的一致性。\n\n2. 线性预测器的特征工程。对于每个原子核 $(Z,N)$，定义以下特征矢量 $x(Z,N)$：\n   - 偏置项 $x_0 = 1$，\n   - $x_1 = A$，\n   - $x_2 = A^{2/3}$，\n   - $x_3 = Z(Z-1) A^{-1/3}$，\n   - $x_4 = (N-Z)^2 A^{-1}$，\n   - $x_5 = \\sigma(Z,N) A^{-1/2}$，其中如果 $Z$ 和 $N$ 均为偶数，$\\sigma(Z,N) = +1$；如果 $Z$ 和 $N$ 均为奇数，$\\sigma(Z,N) = -1$；否则 $\\sigma(Z,N) = 0$。\n   - $x_6 = s(Z,N)$，\n   - $x_7 = s(Z,N) A^{-1/3}$。\n   预测器是线性的：$\\widehat{B}(Z,N) = \\beta^\\top x(Z,N)$，其中参数矢量 $\\beta$ 将从数据中学习得到。\n\n3. 训练数据构建。通过在满足 $A \\ge 16$ 的矩形网格上枚举原子核来构建一个训练集：\n   - 质子数 $Z$ 在 $\\{8,11,14,\\dots,82\\}$ 中（步长为 $3$），\n   - 中子数 $N$ 在 $\\{8,12,16,\\dots,126\\}$ 中（步长为 $4$），\n   - 仅保留那些满足 $A = Z + N \\le 208$ 和 $A \\ge 16$ 的 $(Z,N)$。\n   对于每个保留的 $(Z,N)$，计算其目标值 $B^{\\star}(Z,N)$ 和特征矢量 $x(Z,N)$。通过最小化训练集上 $B^{\\star}(Z,N)$ 和 $\\widehat{B}(Z,N)$ 之间的方差和来拟合线性模型参数 $\\beta$。\n\n4. 对抗性错误设定实验。对于一个固定的 $(Z,N)$，使用正确的 $s(Z,N)$ 定义 $x^{\\mathrm{true}}(Z,N)$，并定义一个对抗性扰动特征矢量 $x^{\\mathrm{adv}}(Z,N)$，它与前者完全相同，只是工程化的幻数指示器被翻转，$s^{\\mathrm{adv}}(Z,N) = 1 - s(Z,N)$，并且相互作用特征 $x_7$ 更新为 $s^{\\mathrm{adv}}(Z,N) A^{-1/3}$。$x$ 的所有其他分量保持不变。在 $\\beta$ 从训练中固定后，将 $(Z,N)$ 处的敏感性定义为在此翻转下预测值的绝对变化：\n   $$\\Delta(Z,N) = \\left|\\beta^\\top x^{\\mathrm{adv}}(Z,N) - \\beta^\\top x^{\\mathrm{true}}(Z,N)\\right|.$$\n   以兆电子伏特表示 $\\Delta(Z,N)$，并四舍五入到 $3$ 位小数。\n\n5. 测试套件。为以下涵盖一系列情况的原子核（每个均以 $(Z,N)$ 指定）评估 $\\Delta(Z,N)$：\n   - $(26,30)$，\n   - $(50,82)$，\n   - $(82,126)$，\n   - $(8,8)$，\n   - $(27,32)$，\n   - $(29,29)$。\n   这些案例包括近幻数区和非幻数区、轻核和重核，以及偶偶、奇偶和奇奇宇称模式。\n\n6. 最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表顺序与测试套件中的顺序相同，并四舍五入到 $3$ 位小数，例如，“[1.234,5.678,9.012,3.210,4.321,6.543]”。数值必须以兆电子伏特为单位。不应打印任何额外文本。\n\n您的实现必须是自包含的、确定性的，并且不需要用户输入或外部文件。它必须可以在任何装有标准数值库的现代 Python 3 环境中运行。", "solution": "用户提供了一个问题，要求设计和评估一个用于原子核结合能的线性机器学习模型。任务的核心是量化模型对一个代表邻近原子核幻数信息的工程特征的敏感性。该问题被验证为科学上可靠、适定且客观。它基于核物理的既定原理（半经验质量公式）和标准的机器学习技术（普通最小二乘回归）。\n\n解决方案按以下逻辑步骤进行：定义物理模型、构建机器学习特征和训练数据、拟合模型参数、推导指定敏感性度量的解析表达式，最后为一组给定的测试原子核计算该度量。\n\n### 1. 基准真相和预测器模型构建\n\n问题定义了一个“基准真相”结合能函数 $B^{\\star}(Z,N)$，它基于半经验质量公式（SEMF），并增加了一个壳层修正项。对于一个有 $Z$ 个质子和 $N$ 个中子，质量数为 $A=Z+N$ 的原子核，该函数为：\n$$B^{\\star}(Z,N) = B_{\\mathrm{LDM}}(Z,N) + \\delta(Z,N) + S_{\\mathrm{shell}}(Z,N)$$\n各项定义如下：\n-   **液滴模型 (LDM) 部分**：$B_{\\mathrm{LDM}}(Z,N) = a_v A - a_s A^{2/3} - a_c \\frac{Z(Z-1)}{A^{1/3}} - a_a \\frac{(N-Z)^2}{A}$。\n-   **对能项**：$\\delta(Z,N) = a_p \\sigma(Z,N) A^{-1/2}$，其中对于偶偶核 $\\sigma(Z,N)$ 为 $+1$，对于奇奇核为 $-1$，否则为 $0$。\n-   **类壳层修正**：$S_{\\mathrm{shell}}(Z,N) = (\\gamma_0 + \\gamma_1 A^{-1/3}) s(Z,N)$。该项依赖于一个工程化的二元特征 $s(Z,N)$，如果 $Z$ 或 $N$ 与某个幻数的差距在 $\\pm 1$ 以内，则该特征为 $1$，否则为 $0$。质子幻数集为 $\\{2,8,20,28,50,82\\}$，中子幻数集为 $\\{2,8,20,28,50,82,126\\}$。\n\n一个线性预测器 $\\widehat{B}(Z,N)$ 被构建用来近似 $B^{\\star}(Z,N)$。该预测是工程特征的线性组合，$\\widehat{B}(Z,N) = \\beta^\\top x(Z,N)$，其中 $\\beta$ 是待学习的参数向量，$x(Z,N)$ 是一个 8 维特征矢量：\n$$x(Z,N) = \\left[ 1, A, A^{2/3}, \\frac{Z(Z-1)}{A^{1/3}}, \\frac{(N-Z)^2}{A}, \\sigma(Z,N)A^{-1/2}, s(Z,N), s(Z,N)A^{-1/3} \\right]^\\top$$\n选择特征 $x_1$ 到 $x_7$ 以直接对应于 $B^{\\star}(Z,N)$ 公式中的各项，从而有效地将问题线性化。项 $x_0=1$ 代表偏置或截距。\n\n### 2. 通过线性回归进行模型训练\n\n参数 $\\beta = [\\beta_0, \\beta_1, \\dots, \\beta_7]^\\top$ 是通过将模型拟合到训练数据集来确定的。该数据集是通过在指定的网格上采样原子核 $(Z,N)$ 生成的：$Z \\in \\{8, 11, \\dots, 80\\}$ 和 $N \\in \\{8, 12, \\dots, 124\\}$，并满足约束条件 $16 \\le A \\le 208$。对于 $M$ 个训练原子核中的每一个，我们计算特征矢量 $x^{(i)}$ 和基准真相结合能 $y^{(i)} = B^{\\star}(Z_i, N_i)$。这就构成了一个大小为 $M \\times 8$ 的设计矩阵 $X$ 和一个大小为 $M \\times 1$ 的目标矢量 $y$。\n\n最优参数矢量 $\\beta$ 是通过最小化方差和来找到的，这是普通最小二乘法 (OLS) 的原理。\n$$\\min_\\beta \\sum_{i=1}^{M} \\left(y^{(i)} - \\beta^\\top x^{(i)}\\right)^2 \\quad \\iff \\quad \\min_\\beta \\| y - X\\beta \\|_2^2$$\n这个问题的著名解析解由正规方程给出：$\\beta = (X^\\top X)^{-1} X^\\top y$。为了数值稳定性，该系统使用奇异值分解 (SVD) 等方法求解，这些方法已在标准数值库中实现。由于特征矢量是由基准真相模型的基本函数构建的，学习到的参数 $\\beta$ 预计将近似于 SEMF 的物理常数，即 $\\beta_1 \\approx a_v$，$\\beta_2 \\approx -a_s$，$\\beta_3 \\approx -a_c$，$\\beta_4 \\approx -a_a$，$\\beta_5 \\approx a_p$，$\\beta_6 \\approx \\gamma_0$ 和 $\\beta_7 \\approx \\gamma_1$。截距 $\\beta_0$ 预计接近于零。\n\n### 3. 对抗性敏感性分析\n\n核心任务是量化模型对工程特征 $s(Z,N)$ 错误设定的敏感性。对于一个给定的原子核 $(Z,N)$，我们定义两个特征矢量：\n-   $x^{\\mathrm{true}}$ 使用正确的值 $s(Z,N)$。\n-   $x^{\\mathrm{adv}}$ 使用翻转后的值 $s^{\\mathrm{adv}} = 1 - s(Z,N)$。\n\n相应的预测值为 $\\widehat{B}^{\\mathrm{true}} = \\beta^\\top x^{\\mathrm{true}}$ 和 $\\widehat{B}^{\\mathrm{adv}} = \\beta^\\top x^{\\mathrm{adv}}$。敏感性度量 $\\Delta(Z,N)$ 是这些预测值之间的绝对差：\n$$\\Delta(Z,N) = \\left| \\widehat{B}^{\\mathrm{adv}} - \\widehat{B}^{\\mathrm{true}} \\right| = \\left| \\beta^\\top x^{\\mathrm{adv}} - \\beta^\\top x^{\\mathrm{true}} \\right| = \\left| \\beta^\\top (x^{\\mathrm{adv}} - x^{\\mathrm{true}}) \\right|$$\n差分矢量 $\\delta_x = x^{\\mathrm{adv}} - x^{\\mathrm{true}}$ 仅在对应于特征 $x_6$ 和 $x_7$ 的分量上非零。\n-   $x_6$ 的差值：$(1 - s(Z,N)) - s(Z,N) = 1 - 2s(Z,N)$。\n-   $x_7$ 的差值：$(1 - s(Z,N))A^{-1/3} - s(Z,N)A^{-1/3} = (1 - 2s(Z,N))A^{-1/3}$。\n\n因此，敏感性为：\n$$\\Delta(Z,N) = \\left| \\beta_6 (1 - 2s(Z,N)) + \\beta_7 (1 - 2s(Z,N))A^{-1/3} \\right|$$\n提出公因式 $(1 - 2s(Z,N))$：\n$$\\Delta(Z,N) = \\left| (1 - 2s(Z,N)) (\\beta_6 + \\beta_7 A^{-1/3}) \\right|$$\n由于 $s(Z,N)$ 不是 $0$ 就是 $1$，所以项 $(1 - 2s(Z,N))$ 不是 $1$ 就是 $-1$。因此，其绝对值总是 $1$。这导出了敏感性的简化表达式：\n$$\\Delta(Z,N) = \\left| \\beta_6 + \\beta_7 (Z+N)^{-1/3} \\right|$$\n这个结果意义重大：预测对翻转幻数邻近性指示器的敏感性仅取决于质量数 $A = Z+N$ 和学习到的参数 $\\beta_6$ 和 $\\beta_7$，而与原子核实际是否邻近幻数无关。当二元指示器 $s(Z,N)$ 被翻转时，该值代表了与两个壳层修正特征相关的预测结合能的总变化。\n\n### 4. 实现与计算\n该过程被实现为一个 Python 程序。首先，根据指定的网格生成训练数据，并构建特征矩阵 $X$ 和目标矢量 $y$。使用 `numpy.linalg.lstsq` 函数来计算参数矢量 $\\beta$。最后，对于测试套件中的每个原子核，使用推导出的公式和拟合得到的 $\\beta_6$ 和 $\\beta_7$ 的值来计算敏感性 $\\Delta(Z,N)$。结果四舍五入到 $3$ 位小数并按要求格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating the sensitivity of a linear nuclear binding\n    energy model to an engineered feature.\n    \"\"\"\n    \n    # 1. Define constants and ground-truth model\n    # Physical constants (MeV)\n    a_v = 15.75\n    a_s = 17.80\n    a_c = 0.711\n    a_a = 23.70\n    a_p = 11.20\n    gamma_0 = 3.00\n    gamma_1 = 8.00\n    \n    # Magic numbers\n    p_magic = {2, 8, 20, 28, 50, 82}\n    n_magic = {2, 8, 20, 28, 50, 82, 126}\n\n    def is_near_magic(k, magic_set):\n        \"\"\"Checks if a number k is within 1 of any number in the magic set.\"\"\"\n        for m in magic_set:\n            if abs(k - m) = 1:\n                return True\n        return False\n\n    def get_s(Z, N):\n        \"\"\"Calculates the magic proximity indicator s(Z,N).\"\"\"\n        if is_near_magic(Z, p_magic) or is_near_magic(N, n_magic):\n            return 1.0\n        return 0.0\n\n    def get_sigma(Z, N):\n        \"\"\"Calculates the pairing indicator sigma(Z,N).\"\"\"\n        if Z % 2 == 0 and N % 2 == 0:\n            return 1.0  # even-even\n        if Z % 2 != 0 and N % 2 != 0:\n            return -1.0  # odd-odd\n        return 0.0  # even-odd or odd-even\n\n    def get_ground_truth_binding_energy(Z, N):\n        \"\"\"Calculates B_star(Z,N) using the SEMF.\"\"\"\n        A = float(Z + N)\n        if A == 0: return 0.0\n\n        # Liquid Drop Model terms\n        B_ldm = (a_v * A - \n                 a_s * A**(2.0/3.0) - \n                 a_c * Z * (Z - 1.0) * A**(-1.0/3.0) - \n                 a_a * (N - Z)**2 / A)\n        \n        # Pairing term\n        delta = a_p * get_sigma(Z, N) * A**(-1.0/2.0)\n\n        # Shell-like Correction Term\n        s_val = get_s(Z, N)\n        S_shell = (gamma_0 + gamma_1 * A**(-1.0/3.0)) * s_val\n        \n        return B_ldm + delta + S_shell\n\n    def get_feature_vector(Z, N):\n        \"\"\"Constructs the feature vector x(Z,N).\"\"\"\n        A = float(Z + N)\n        if A == 0: return np.zeros(8)\n        \n        x = np.zeros(8, dtype=np.float64)\n        x[0] = 1.0\n        x[1] = A\n        x[2] = A**(2.0/3.0)\n        x[3] = Z * (Z - 1.0) * A**(-1.0/3.0)\n        x[4] = (N - Z)**2 / A if A != 0 else 0.0\n        x[5] = get_sigma(Z, N) * A**(-1.0/2.0)\n        s_val = get_s(Z, N)\n        x[6] = s_val\n        x[7] = s_val * A**(-1.0/3.0)\n        return x\n\n    # 2. Generate training data\n    train_z_range = range(8, 82 + 1, 3)\n    train_n_range = range(8, 126 + 1, 4)\n    \n    feature_matrix_list = []\n    target_vector_list = []\n\n    for Z in train_z_range:\n        for N in train_n_range:\n            A = Z + N\n            if 16 = A = 208:\n                features = get_feature_vector(Z, N)\n                target = get_ground_truth_binding_energy(Z, N)\n                feature_matrix_list.append(features)\n                target_vector_list.append(target)\n                \n    X = np.array(feature_matrix_list, dtype=np.float64)\n    y = np.array(target_vector_list, dtype=np.float64)\n\n    # 3. Train the linear model\n    # Solve X * beta = y for beta using least squares\n    beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n\n    # 4. Adversarial misspecification experiment\n    test_cases = [\n        (26, 30),\n        (50, 82),\n        (82, 126),\n        (8, 8),\n        (27, 32),\n        (29, 29)\n    ]\n\n    results = []\n    beta_6 = beta[6]\n    beta_7 = beta[7]\n\n    for Z, N in test_cases:\n        A = float(Z + N)\n        # Simplified formula for sensitivity: Delta = |beta_6 + beta_7 * A**(-1/3)|\n        delta = np.abs(beta_6 + beta_7 * A**(-1.0/3.0))\n        results.append(delta)\n\n    # 5. Final output formatting\n    # Format each result to exactly 3 decimal places\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3568248"}, {"introduction": "科学机器学习的终极目标之一不仅是预测，更是指导科学发现。本练习将带您进入主动学习（active learning）的前沿领域，利用模型的预测不确定性来建议下一个最值得进行的实验 [@problem_id:3568168]。您将实现一个“采集函数”，用以确定测量哪个原子核的$S_{2n}$值能够最大程度地降低我们对中子滴线位置的不确定性，从而形成理论与实验的闭环。", "problem": "要求您设计并实现一个单步决策理论采集函数（decision-theoretic acquisition function），用于提议一次新的双中子分离能测量，以便在质子数固定的情况下，最大限度地减少双中子滴线位置的不确定性。您的程序必须计算此采集分数，并对多个测试用例的指定候选集进行排序。\n\n对于固定的质子数 $Z$，双中子分离能 $S_{2n}(Z,N)$ 定义为核结合能之差，即 $S_{2n}(Z,N) = B(Z,N) - B(Z,N-2)$。质子数为 $Z$ 的元素的双中子滴线位置是满足 $S_{2n}(Z,N_{\\mathrm{d}})  0$ 且 $S_{2n}(Z,N_{\\mathrm{d}}+2) \\le 0$ 的最大中子数 $N_{\\mathrm{d}}$。在此问题中，您将在一个离散的中子网格上工作，并将 $S_{2n}(Z,N)$ 视为定义在一组整数 $N$ 值上的潜函数（latent function）$g(N)$。假设 $g(N)$ 服从高斯过程先验（等效于离散网格上的联合高斯先验），其均值向量为 $\\boldsymbol{\\mu}$，协方差矩阵为 $\\mathbf{K}$，并伴有方差为 $\\tau^2$ 的加性独立高斯观测噪声。\n\n您必须从第一性原理出发，推导出一个单步采集分数，该分数用于量化在候选点中子数 $N_{\\mathrm{c}}$ 处测量 $g(N)$ 时，滴线位置 $N_{\\mathrm{d}}$ 后验方差的期望减少量，然后对所提供候选集中的每个候选点计算该分数并进行排序。请使用以下基本事实作为您的出发点：\n\n- 如果 $\\begin{bmatrix} g(N_*) \\\\ g(N_{\\mathrm{c}}) \\end{bmatrix}$ 是联合高斯分布，其协方差矩阵项为 $k_{**} = \\mathrm{Var}\\!\\left[g(N_*)\\right]$、$k_{\\mathrm{cc}} = \\mathrm{Var}\\!\\left[g(N_{\\mathrm{c}})\\right]$ 和 $k_{*\\mathrm{c}} = \\mathrm{Cov}\\!\\left[g(N_*), g(N_{\\mathrm{c}})\\right]$，并且获得了一次带噪声的观测 $y_{\\mathrm{c}} = g(N_{\\mathrm{c}}) + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0,\\tau^2)$，那么在 $N_*$ 处的后验方差将从 $k_{**}$ 减少到 $k_{**} - \\frac{k_{*\\mathrm{c}}^2}{k_{\\mathrm{cc}} + \\tau^2}$。\n- 如果标量参数 $\\theta$ 由平滑函数 $h$ 的根通过 $h(\\theta) = 0$ 隐式定义，并且在名义点上 $h$ 存在高斯不确定性，则对于一个合适的线性化点 $\\theta_0$（其中 $h'(\\theta_0) \\neq 0$），delta 方法给出的方差传播近似为 $\\mathrm{Var}(\\theta) \\approx \\dfrac{\\mathrm{Var}\\!\\left[h(\\theta_0)\\right]}{\\left(h'(\\theta_0)\\right)^2}$。\n\n按如下方式将上述理论操作化。对于给定的测试用例：\n\n1. 固定一个中子数的离散网格 $N \\in \\{N_{\\min}, N_{\\min}+1, \\dots, N_{\\max}\\}$。将此网格上的 $g(N)$ 视为联合高斯分布，其均值为 $\\mu(N)$，协方差 $K(N,N')$ 由整数网格上的平方指数核函数（squared-exponential kernel）给出：\n$$\nK(N,N') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(N - N')^2}{2 \\ell^2}\\right),\n$$\n其中 $\\sigma_f^2$ 是信号方差，$\\ell$ 是以中子数单位表示的相关长度。测量噪声为 $\\tau^2$。\n\n2. 将名义线性化索引 $N_*$ 定义为网格上使 $|\\mu(N)|$ 最小化的点（即在当前均值下，最有可能接近 $S_{2n} = 0$ 水平集的点）。通过网格上的有限差分来近似斜率 $g'(N_*)$：\n- 如果 $N_*$ 严格位于 $N_{\\min}$ 和 $N_{\\max}$ 之间，使用中心差分 $g'(N_*) \\approx \\dfrac{\\mu(N_*+1) - \\mu(N_*-1)}{2}$。\n- 如果 $N_* = N_{\\min}$，使用向前差分 $g'(N_*) \\approx \\mu(N_{\\min}+1) - \\mu(N_{\\min})$。\n- 如果 $N_* = N_{\\max}$，使用向后差分 $g'(N_*) \\approx \\mu(N_{\\max}) - \\mu(N_{\\max}-1)$。\n\n3. 对于提供的候选集 $\\mathcal{C}$（网格的一个子集）中的每个候选点 $N_{\\mathrm{c}}$，计算一个与 $\\mathrm{Var}(N_{\\mathrm{d}})$ 的期望减少量成正比的采集分数，假设在 $N_{\\mathrm{c}}$ 进行一次观测并被吸收。使用高斯条件化事实来量化 $\\mathrm{Var}\\!\\left[g(N_*)\\right]$ 的减少量，并使用 delta 方法将此减少量传播到 $\\mathrm{Var}(N_{\\mathrm{d}})$。为确保当 $|g'(N_*)|$ 非常小时的数值稳定性，通过在 $g'(N_*)^2$ 上加上一个小的 $\\varepsilon  0$（其中 $\\varepsilon = 10^{-12}$）来正则化分母。\n\n4. 按采集分数降序对 $\\mathcal{C}$ 中的候选点进行排序，若分数相同则按 $N_{\\mathrm{c}}$ 升序排序。\n\n您必须实现上述算法，并在以下三个测试用例上运行。在每个案例中，所有能量单位均为兆电子伏特（mega-electron-volts），但您的代码应纯粹进行数值计算，无需单位转换。最终的程序输出应为单行，包含一个由三个列表组成的列表，其中每个内部列表是该测试用例的候选集按采集分数降序排序的结果。\n\n测试用例 A (正常穿越路径)：\n- 质子数 $Z = 50$。\n- 中子网格 $N \\in \\{60, 61, \\dots, 90\\}$。\n- 均值函数 $\\mu(N) = 8.0 - 0.3 \\times (N - 60)$。\n- 核函数超参数 $\\sigma_f = 2.0$, $\\ell = 2.5$。\n- 观测噪声方差 $\\tau^2 = 0.16$。\n- 候选集 $\\mathcal{C} = \\{65, 80, 85, 86, 87, 90\\}$。\n\n测试用例 B (网格上均值全为正)：\n- 质子数 $Z = 20$。\n- 中子网格 $N \\in \\{20, 21, \\dots, 40\\}$。\n- 均值函数 $\\mu(N) = 12.0 - 0.1 \\times (N - 20)$。\n- 核函数超参数 $\\sigma_f = 1.5$, $\\ell = 3.0$。\n- 观测噪声方差 $\\tau^2 = 0.09$。\n- 候选集 $\\mathcal{C} = \\{20, 30, 40\\}$。\n\n测试用例 C (网格上均值全为负)：\n- 质子数 $Z = 28$。\n- 中子网格 $N \\in \\{52, 53, \\dots, 62\\}$。\n- 均值函数 $\\mu(N) = -2.0 - 0.1 \\times (N - 52)$。\n- 核函数超参数 $\\sigma_f = 1.0$, $\\ell = 2.0$。\n- 观测噪声方差 $\\tau^2 = 0.04$。\n- 候选集 $\\mathcal{C} = \\{52, 56, 60, 62\\}$。\n\n最终输出格式：\n- 您的程序应产生单行输出，其中包含一个用方括号括起来的、以逗号分隔的列表形式的结果，每个元素对应一个测试用例，其本身也是一个用方括号括起来的、以逗号分隔的列表，包含按采集分数降序排列的候选点中子数。例如，一个有效的输出格式是 $[[n_{1,1},n_{1,2}], [n_{2,1},n_{2,2},n_{2,3}], [n_{3,1}]]$，但在打印的字符串中任何地方都没有空格。具体来说，输出必须看起来像一个形如 `[[\\cdots],[\\cdots],[\\cdots]]` 的、不含空格的字符串。", "solution": "此问题要求设计并实现一个单步决策理论采集函数。目标是从一组候选点中确定一个未来的测量点，该点预期能最大限度地减少双中子滴线位置的不确定性。该问题的基础在于贝叶斯建模，具体来说是使用高斯过程（GP）来表示我们关于双中子分离能 $S_{2n}$ 的知识。\n\n首先，我们将问题形式化。对于固定的质子数 $Z$，双中子分离能被建模为一个在中子数 $N$ 的离散网格上的潜函数 $g(N)$。此函数被假定服从一个高斯过程先验，这意味着对于任何有限点集 $\\{N_1, N_2, \\dots, N_m\\}$，函数值向量 $[g(N_1), g(N_2), \\dots, g(N_m)]^T$ 服从一个多元高斯分布。该分布由一个均值函数 $\\mu(N)$ 和一个协方差函数（或称核函数）$K(N,N')$ 完全确定。问题提供了这些函数的形式：\n- 均值函数：$\\mu(N)$，为每个测试用例指定。\n- 协方差函数：一个平方指数核函数，$K(N,N') = \\sigma_f^2 \\exp\\left(-\\frac{(N - N')^2}{2 \\ell^2}\\right)$，其中 $\\sigma_f^2$ 是信号方差，$\\ell$ 是相关长度。\n\n双中子滴线 $N_{\\mathrm{d}}$ 定义为满足 $S_{2n}(Z,N_{\\mathrm{d}})  0$ 且 $S_{2n}(Z,N_{\\mathrm{d}}+2) \\le 0$ 的最大中子数。对于一个平滑变化的 $S_{2n}$ 函数，这可以近似为寻根条件 $g(N_{\\mathrm{d}}) = S_{2n}(Z,N_{\\mathrm{d}}) \\approx 0$。\n\n我们的目标是量化进行一次新的、带噪声的测量 $y_{\\mathrm{c}} = g(N_{\\mathrm{c}}) + \\epsilon$ 的效用，其中噪声 $\\epsilon$ 是均值为 $0$、方差为 $\\tau^2$ 的高斯噪声。这种效用，或称采集分数，被定义为滴线位置方差 $\\mathrm{Var}(N_{\\mathrm{d}})$ 的期望减少量。\n\n为了将我们函数的方差 $\\mathrm{Var}[g(N)]$ 与滴线位置的方差 $\\mathrm{Var}(N_{\\mathrm{d}})$ 联系起来，我们采用 delta 方法。滴线由 $g(N_{\\mathrm{d}}) = 0$ 隐式定义。将 $g(N)$ 在一个合适的点 $N_*$ 附近进行线性化，我们有 $g(N) \\approx g(N_*) + g'(N_*)(N - N_*)$。设 $g(N_{\\mathrm{d}}) = 0$，可得 $0 \\approx g(N_*) + g'(N_*)(N_{\\mathrm{d}} - N_*)$，整理后可得到滴线位置的估计值：$N_{\\mathrm{d}} \\approx N_* - \\frac{g(N_*)}{g'(N_*)}$。通过这个线性化表达式传播方差，得到 delta 方法的近似：\n$$\n\\mathrm{Var}(N_{\\mathrm{d}}) \\approx \\mathrm{Var}\\left(N_* - \\frac{g(N_*)}{g'(N_*)}\\right) = \\frac{\\mathrm{Var}[g(N_*)]}{(g'(N_*))^2}\n$$\n问题指定线性化点 $N_*$ 应该是先验均值 $|\\mu(N)|$ 最小的网格点。这是我们当前对根位置的最佳猜测。斜率 $g'(N_*)$ 是通过在 $N_*$ 处对均值函数 $\\mu(N)$ 应用所提供的规则进行有限差分来近似的。\n\n在任何测量之前，$g(N_*)$ 的方差是先验方差，由核函数给出：$\\mathrm{Var}_{prior}[g(N_*)] = K(N_*, N_*) = k_{**}$。\n问题提供了在 $N_{\\mathrm{c}}$ 处进行一次带噪声的测量后，$g(N_*)$ 的后验方差的标准公式：\n$$\n\\mathrm{Var}_{post}[g(N_*)] = k_{**} - \\frac{k_{*\\mathrm{c}}^2}{k_{\\mathrm{cc}} + \\tau^2}\n$$\n其中 $k_{**} = K(N_*,N_*)$，$k_{\\mathrm{cc}} = K(N_{\\mathrm{c}},N_{\\mathrm{c}})$，以及 $k_{*\\mathrm{c}} = K(N_*,N_{\\mathrm{c}})$。\n\n因此，$g(N_*)$ 方差的减少量为：\n$$\n\\Delta \\mathrm{Var}[g(N_*)] = \\mathrm{Var}_{prior}[g(N_*)] - \\mathrm{Var}_{post}[g(N_*)] = \\frac{k_{*\\mathrm{c}}^2}{k_{\\mathrm{cc}} + \\tau^2}\n$$\n采集分数 $A(N_{\\mathrm{c}})$ 是滴线位置方差的期望减少量。使用 delta 方法，我们将 $\\mathrm{Var}[g(N_*)]$ 的减少量传播到 $\\mathrm{Var}(N_{\\mathrm{d}})$ 的减少量：\n$$\nA(N_{\\mathrm{c}}) = \\Delta \\mathrm{Var}(N_{\\mathrm{d}}) \\approx \\frac{\\Delta \\mathrm{Var}[g(N_*)]}{(g'(N_*))^2} = \\frac{1}{(g'(N_*))^2} \\left(\\frac{k_{*\\mathrm{c}}^2}{k_{\\mathrm{cc}} + \\tau^2}\\right)\n$$\n为了处理斜率可能非常接近于零的情况，在分母的斜率平方项中加入了一个小的正则化项 $\\varepsilon = 10^{-12}$。采集分数的最终表达式为：\n$$\nA(N_{\\mathrm{c}}) = \\frac{k_{*\\mathrm{c}}^2}{((g'(N_*))^2 + \\varepsilon)(k_{\\mathrm{cc}} + \\tau^2)}\n$$\n对于平方指数核函数，$k_{\\mathrm{cc}} = K(N_{\\mathrm{c}}, N_{\\mathrm{c}}) = \\sigma_f^2$ 且 $k_{*\\mathrm{c}} = K(N_*, N_{\\mathrm{c}}) = \\sigma_f^2 \\exp\\left(-\\frac{(N_* - N_{\\mathrm{c}})^2}{2 \\ell^2}\\right)$。\n\n每个测试用例的总体算法如下：\n1.  构建从 $N_{\\min}$ 到 $N_{\\max}$ 的完整中子网格 $\\mathcal{N}$。\n2.  对所有 $N \\in \\mathcal{N}$ 计算均值函数 $\\mu(N)$。\n3.  确定线性化点 $N_* = \\arg\\min_{N \\in \\mathcal{N}} |\\mu(N)|$。\n4.  通过在 $N_*$ 处对 $\\mu(N)$ 应用指定的有限差分规则（向前、向后或中心），计算斜率 $g'(N_*)$。\n5.  对于候选集 $\\mathcal{C}$ 中的每个候选点中子数 $N_{\\mathrm{c}}$：\n    a. 计算核函数项 $k_{\\mathrm{cc}} = \\sigma_f^2$ 和 $k_{*\\mathrm{c}} = \\sigma_f^2 \\exp\\left(-\\frac{(N_* - N_{\\mathrm{c}})^2}{2 \\ell^2}\\right)$。\n    b. 使用推导出的公式计算采集分数 $A(N_{\\mathrm{c}})$。\n6.  按分数 $A(N_{\\mathrm{c}})$ 的降序对候选点 $N_{\\mathrm{c}} \\in \\mathcal{C}$ 进行排序，若分数相同则选择较小的 $N_{\\mathrm{c}}$。这将为该测试用例生成最终的排序列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case A\n        {\n            \"N_min\": 60, \"N_max\": 90,\n            \"mu_params\": {\"c\": 8.0, \"m\": -0.3, \"N0\": 60},\n            \"sigma_f\": 2.0, \"ell\": 2.5, \"tau_sq\": 0.16,\n            \"candidates\": [65, 80, 85, 86, 87, 90]\n        },\n        # Test Case B\n        {\n            \"N_min\": 20, \"N_max\": 40,\n            \"mu_params\": {\"c\": 12.0, \"m\": -0.1, \"N0\": 20},\n            \"sigma_f\": 1.5, \"ell\": 3.0, \"tau_sq\": 0.09,\n            \"candidates\": [20, 30, 40]\n        },\n        # Test Case C\n        {\n            \"N_min\": 52, \"N_max\": 62,\n            \"mu_params\": {\"c\": -2.0, \"m\": -0.1, \"N0\": 52},\n            \"sigma_f\": 1.0, \"ell\": 2.0, \"tau_sq\": 0.04,\n            \"candidates\": [52, 56, 60, 62]\n        }\n    ]\n    \n    epsilon = 1e-12\n    all_results = []\n\n    for case in test_cases:\n        N_min, N_max = case[\"N_min\"], case[\"N_max\"]\n        mu_params = case[\"mu_params\"]\n        sigma_f = case[\"sigma_f\"]\n        ell = case[\"ell\"]\n        tau_sq = case[\"tau_sq\"]\n        candidates = case[\"candidates\"]\n\n        # 1. Define the grid and mean function\n        N_grid = np.arange(N_min, N_max + 1)\n        mu_func = lambda N: mu_params[\"c\"] + mu_params[\"m\"] * (N - mu_params[\"N0\"])\n        mu_vec = mu_func(N_grid)\n\n        # 2. Find the linearization point N_star\n        idx_star = np.argmin(np.abs(mu_vec))\n        N_star = N_grid[idx_star]\n\n        # 3. Compute the slope g'(N_star)\n        if N_star == N_min:\n            # Forward difference\n            g_prime_N_star = mu_vec[1] - mu_vec[0]\n        elif N_star == N_max:\n            # Backward difference\n            g_prime_N_star = mu_vec[-1] - mu_vec[-2]\n        else:\n            # Centered difference\n            g_prime_N_star = (mu_vec[idx_star + 1] - mu_vec[idx_star - 1]) / 2.0\n            \n        g_prime_N_star_sq_reg = g_prime_N_star**2 + epsilon\n\n        # 4. Compute acquisition score for each candidate\n        scores = []\n        k_cc = sigma_f**2\n        \n        # Denominator is constant across candidates for a given test case\n        denominator = g_prime_N_star_sq_reg * (k_cc + tau_sq)\n\n        for Nc in candidates:\n            # Calculate k_star_c\n            k_star_c = sigma_f**2 * np.exp(-((N_star - Nc)**2) / (2 * ell**2))\n            \n            # Numerator\n            numerator = k_star_c**2\n            \n            # Acquisition score\n            score = numerator / denominator\n            scores.append((score, Nc))\n\n        # 5. Rank candidates\n        # Sort by score descending, then by Nc ascending for ties\n        scores.sort(key=lambda x: (-x[0], x[1]))\n        \n        ranked_candidates = [Nc for score, Nc in scores]\n        all_results.append(ranked_candidates)\n\n    # Final print statement in the exact required format\n    inner_results_str = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_str = f\"[{','.join(inner_results_str)}]\"\n    print(final_str)\n\nsolve()\n\n```", "id": "3568168"}]}