{"hands_on_practices": [{"introduction": "要真正掌握近端梯度法，最好的起点是亲手执行一次迭代。这个练习 [@problem_id:2195110] 将引导你处理一个简单的约束优化问题，即在非负象限中寻找一个最近点。通过这个实践，你将具体地体会到近端梯度法的核心思想：首先对目标函数的光滑部分执行一步标准的梯度下降，然后通过近端映射步骤来处理非光滑的约束部分，从而将抽象的理论转化为具体的操作。", "problem": "考虑一个优化问题，即寻找一个点 $x = (x_1, x_2) \\in \\mathbb{R}^2$ 来最小化函数 $F(x)$，并满足其分量的非负约束，即 $x_1 \\ge 0$ 且 $x_2 \\ge 0$。需要最小化的函数是点 $x$ 到目标点 $a$ 的欧几里得距离的平方，由 $F(x) = \\frac{1}{2}\\|x - a\\|_2^2$ 给出。\n\n通过将光滑部分定义为 $f(x) = \\frac{1}{2}\\|x - a\\|_2^2$，非光滑部分 $g(x)$ 定义为非负象限的指示函数，该问题可以转化为近端算法的标准形式 $\\min_{x} f(x) + g(x)$。如果 $x_1 \\ge 0$ 且 $x_2 \\ge 0$，指示函数 $g(x)$ 的值为零，否则为无穷大。\n\n你的任务是应用近端梯度法来解决这个问题。近端梯度法的迭代更新规则由下式给出：\n$$ x_{k+1} = \\text{prox}_{\\gamma g}(x_k - \\gamma \\nabla f(x_k)) $$\n其中 $\\gamma$ 是步长，$\\text{prox}_{\\gamma g}$ 是与函数 $g$ 相关联的近端算子。\n\n给定目标点 $a = (5, -4)$，初始点 $x_0 = (1, 1)$，以及步长 $\\gamma = 0.2$，请计算下一次迭代的结果 $x_1$。将你的答案表示为行向量 $(x_{1,1}, x_{1,2})$，其中 $x_{1,1}$ 和 $x_{1,2}$ 是向量 $x_1$ 的分量。", "solution": "我们要在非负象限上最小化 $F(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$。在近端梯度分解中，设 $f(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$ 且 $g(x)=\\iota_{\\mathbb{R}_{+}^{2}}(x)$，即为可行集 $\\mathbb{R}_{+}^{2}=\\{x\\in\\mathbb{R}^{2}:x_{1}\\ge 0,\\ x_{2}\\ge 0\\}$ 的指示函数。\n\n$f$ 的梯度由下式给出\n$$\n\\nabla f(x)=x-a.\n$$\n$\\gamma g$ 在点 $z$ 处的近端算子是到 $\\mathbb{R}_{+}^{2}$ 上的欧几里得投影：\n$$\n\\text{prox}_{\\gamma g}(z)=\\operatorname*{arg\\,min}_{y\\in\\mathbb{R}^{2}}\\left\\{\\iota_{\\mathbb{R}_{+}^{2}}(y)+\\frac{1}{2\\gamma}\\|y-z\\|_{2}^{2}\\right\\}=P_{\\mathbb{R}_{+}^{2}}(z),\n$$\n也就是在零处的分量截断：\n$$\nP_{\\mathbb{R}_{+}^{2}}(z)=\\big(\\max\\{z_{1},0\\},\\ \\max\\{z_{2},0\\}\\big).\n$$\n\n当 $a=(5,-4)$，$x_{0}=(1,1)$ 且 $\\gamma=0.2$ 时，计算在 $x_{0}$ 处的梯度：\n$$\n\\nabla f(x_{0})=x_{0}-a=(1,1)-(5,-4)=(-4,5).\n$$\n执行梯度步：\n$$\nx_{0}-\\gamma \\nabla f(x_{0})=(1,1)-0.2\\,(-4,5)=(1+0.8,\\ 1-1)=\\left(\\frac{9}{5},\\ 0\\right).\n$$\n应用近端映射，即到 $\\mathbb{R}_{+}^{2}$ 上的投影：\n$$\nx_{1}=\\text{prox}_{\\gamma g}\\big(x_{0}-\\gamma \\nabla f(x_{0})\\big)=P_{\\mathbb{R}_{+}^{2}}\\left(\\frac{9}{5},0\\right)=\\left(\\frac{9}{5},0\\right),\n$$\n因为两个分量都已是非负的。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{9}{5}  0 \\end{pmatrix}}$$", "id": "2195110"}, {"introduction": "算法的收敛性是其实用价值的基石，而步长的选择是近端梯度法收敛的关键。这个练习 [@problem_id:3415791] 深入探讨了步长对算法稳定性的决定性影响。通过分析一个简化的二次目标函数，你将推导出保证收敛的精确步长条件，并构造一个因步长过大而导致发散的反例，从而深刻理解算法不稳定的根源，并认识到回溯线搜索等自适应策略在设计稳健算法时的重要性。", "problem": "考虑一个与线性逆问题相关的复合目标，该目标为标量决策变量 $x \\in \\mathbb{R}$ 定义为 $F(x) = f(x) + g(x)$，其中 $f(x) = \\frac{1}{2}\\|A x - b\\|^{2}$ 且 $g(x)$ 是一个真、下半连续的凸函数。假设 $A = \\sqrt{L}$ 且 $L  0$，以及 $b = 0$，因此 $f(x) = \\frac{L}{2}x^{2}$ 且梯度 $\\nabla f$ 是 Lipschitz 连续的，其 Lipschitz 常数为 $L$。邻近梯度法 (PGM) 由迭代 $x_{k+1} = \\operatorname{prox}_{\\gamma g}(x_{k} - \\gamma \\nabla f(x_{k}))$ 定义，其中 $\\gamma  0$ 是一个固定步长，$\\operatorname{prox}_{\\gamma g}$ 表示 $\\gamma g$ 的邻近算子。考虑 $g(x) = 0$ 且初始化 $x_{0} \\neq 0$ 的特殊情况。\n\n从第一性原理和邻近算子的定义出发，推导 $x_{k+1}$ 关于 $x_{k}$ 的显式迭代映射，并获得 $x_{k}$ 作为 $k$、$x_{0}$、$\\gamma$ 和 $L$ 的函数的闭式表达式。利用此推导，确定迭代收敛时 $\\gamma$ 所需满足的精确条件。然后，通过选择 $\\gamma = \\frac{5}{2L}$ 构建一个展示带振荡发散的具体反例，并计算在此 $\\gamma$ 选择下所得线性迭代算子的谱半径。将谱半径作为你的最终答案报告。\n\n最后，讨论在 Lipschitz 常数 $L$ 未知或估计不佳时，邻近梯度法中用于防止逆问题和数据同化工作流中出现不稳定性的科学上合理的实用保障措施。你的讨论应从第一性原理出发，解释这些保障措施为何有效，并应与上述假设保持一致。最终报告的量必须是谱半径，表示为单个实数值或单个闭式解析表达式。无需四舍五入。", "solution": "该问题要求在特定条件下分析邻近梯度法 (PGM)。目标函数为 $F(x) = f(x) + g(x)$，其中 $x \\in \\mathbb{R}$ 是一个标量变量。其组成部分为 $f(x) = \\frac{1}{2}\\|Ax - b\\|^2$ 和 $g(x)$，一个真、下半连续的凸函数。参数指定为 $A = \\sqrt{L}$ (其中 $L  0$) 和 $b = 0$。这可将目标函数的光滑部分简化为：\n$$ f(x) = \\frac{1}{2}\\|(\\sqrt{L})x - 0\\|^2 = \\frac{1}{2}(\\sqrt{L}x)^2 = \\frac{L}{2}x^2 $$\n$f(x)$ 的梯度为 $\\nabla f(x) = \\frac{d}{dx}\\left(\\frac{L}{2}x^2\\right) = Lx$。其二阶导数为 $f''(x) = L$，即梯度 $\\nabla f(x)$ 的 Lipschitz 常数。\n\nPGM 迭代定义为 $x_{k+1} = \\operatorname{prox}_{\\gamma g}(x_{k} - \\gamma \\nabla f(x_{k}))$。我们被要求考虑 $g(x) = 0$ 的特殊情况。\n\n首先，我们必须确定当 $g(x)=0$ 时邻近算子 $\\operatorname{prox}_{\\gamma g}$ 的形式。根据定义，对于一个点 $z \\in \\mathbb{R}$：\n$$ \\operatorname{prox}_{\\gamma g}(z) = \\arg\\min_{u \\in \\mathbb{R}} \\left\\{ \\gamma g(u) + \\frac{1}{2}\\|u - z\\|^2 \\right\\} $$\n代入 $g(x)=0$，需要最小化的表达式变为：\n$$ \\arg\\min_{u \\in \\mathbb{R}} \\left\\{ \\gamma \\cdot 0 + \\frac{1}{2}(u - z)^2 \\right\\} = \\arg\\min_{u \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}(u - z)^2 \\right\\} $$\n当自变量为零时，二次项 $\\frac{1}{2}(u - z)^2$ 取得唯一的最小值，这发生在 $u = z$ 处。因此，零函数的邻近算子是恒等算子：\n$$ \\operatorname{prox}_{\\gamma (0)}(z) = z $$\n将此结果代入 PGM 迭代公式，我们发现对于 $g(x) = 0$ 的情况，该方法简化为标准的梯度下降法：\n$$ x_{k+1} = \\operatorname{prox}_{\\gamma (0)}(x_{k} - \\gamma \\nabla f(x_{k})) = x_{k} - \\gamma \\nabla f(x_{k}) $$\n现在，我们代入梯度的具体表达式 $\\nabla f(x_k) = Lx_k$：\n$$ x_{k+1} = x_k - \\gamma (Lx_k) = (1 - \\gamma L)x_k $$\n这就是 $x_{k+1}$ 关于 $x_k$ 的显式迭代映射。它是一个线性递推关系。\n\n为了找到 $x_k$ 的闭式表达式，我们可以从初始条件 $x_0$ 开始展开递推关系：\n$$ x_1 = (1 - \\gamma L)x_0 $$\n$$ x_2 = (1 - \\gamma L)x_1 = (1 - \\gamma L)(1 - \\gamma L)x_0 = (1 - \\gamma L)^2 x_0 $$\n通过归纳法， $x_k$ 的闭式表达式为：\n$$ x_k = (1 - \\gamma L)^k x_0 $$\n序列 $\\{x_k\\}$ 收敛于 $F(x) = \\frac{L}{2}x^2$ 的最优点 $x^*=0$。对于任意非零初始值 $x_0 \\neq 0$，要使序列收敛到 0，该等比数列的公比的绝对值必须严格小于 1。这给出了收敛条件：\n$$ |1 - \\gamma L|  1 $$\n这个绝对值不等式等价于以下两个不等式：\n$$ -1  1 - \\gamma L \\quad \\text{和} \\quad 1 - \\gamma L  1 $$\n从第一个不等式得出：\n$$ -1  1 - \\gamma L \\implies \\gamma L  2 \\implies \\gamma  \\frac{2}{L} $$\n从第二个不等式得出：\n$$ 1 - \\gamma L  1 \\implies -\\gamma L  0 \\implies \\gamma L > 0 $$\n因为给定 $L > 0$，这意味着 $\\gamma > 0$，这在问题陈述中也已给出。因此，收敛的精确条件是 $0  \\gamma  \\frac{2}{L}$。\n\n接下来，我们构造一个表现出带振荡发散的反例。我们被要求使用步长 $\\gamma = \\frac{5}{2L}$。该值不满足收敛条件，因为 $\\frac{5}{2L} > \\frac{2}{L}$。\n让我们为这个 $\\gamma$ 的选择计算迭代因子 $1 - \\gamma L$：\n$$ 1 - \\gamma L = 1 - \\left(\\frac{5}{2L}\\right)L = 1 - \\frac{5}{2} = -\\frac{3}{2} $$\n迭代映射变为：\n$$ x_{k+1} = \\left(-\\frac{3}{2}\\right)x_k $$\n闭式解为 $x_k = \\left(-\\frac{3}{2}\\right)^k x_0$。因为底数为负，所以 $x_k$ 的符号在每次迭代中交替变化，从而引起振荡。因为底数的绝对值 $|-\\frac{3}{2}| = \\frac{3}{2} > 1$，所以迭代值的大小 $|x_k| = (\\frac{3}{2})^k |x_0|$ 呈指数级增长至无穷大。这展示了带振荡的发散。\n\n最后的任务是计算所得线性迭代算子的谱半径。该迭代是一个形如 $x_{k+1} = Gx_k$ 的线性映射，其中算子 $G$ 是与标量 $(1 - \\gamma L)$ 的乘法。对于一维系统，该算子可以由 $1 \\times 1$ 矩阵 $[1 - \\gamma L]$ 表示。$1 \\times 1$ 矩阵 $[c]$ 的特征值就是标量 $c$ 本身。在我们的例子中，唯一的特征值是 $\\lambda = 1 - \\gamma L$。谱半径 $\\rho(G)$ 是所有特征值绝对值的最大值。\n$$ \\rho(G) = |\\lambda| = |1 - \\gamma L| $$\n对于 $\\gamma = \\frac{5}{2L}$ 的特定选择，我们有：\n$$ \\rho(G) = \\left|1 - \\left(\\frac{5}{2L}\\right)L\\right| = \\left|1 - \\frac{5}{2}\\right| = \\left|-\\frac{3}{2}\\right| = \\frac{3}{2} $$\n谱半径为 $\\frac{3}{2}$。\n\n最后，需要讨论实用的保障措施。采用固定步长 $\\gamma$ 的 PGM 的收敛性关键地依赖于条件 $0  \\gamma  2/L$，其中 $L$ 是 $\\nabla f$ 的 Lipschitz 常数。在许多实际的逆问题和数据同化应用中，算子 $A$ 可能是一个大矩阵（例如，表示一个积分算子的离散化），计算其最大奇异值以求得 $L = \\|A\\|^2$ 在计算上可能是不可行的或不可能的。使用过大的步长 $\\gamma$（即 $\\gamma \\ge 2/L$）可能导致反例中展示的那种不稳定性和发散。\n\n一种科学上合理且广泛使用的保障措施是采用自适应步长策略，最常见的是**回溯线搜索**。该方法背后的原理是确保每一步都能使目标函数充分下降，而无需先验地知道 $L$。\n\n该过程如下：在每次迭代 $k$ 中，从一个试验步长开始，比如 $\\gamma_k^{(0)}$。然后，计算下一个候选迭代点 $x_{k+1}^{(j)} = \\operatorname{prox}_{\\gamma_k^{(j)} g}(x_k - \\gamma_k^{(j)} \\nabla f(x_k))$。接着检查一个条件以确定此步是否可接受。对于 PGM，一个标准的回溯条件是基于驱动该迭代的 $f$ 的二次上界。我们检查以下不等式是否成立：\n$$ f(x_{k+1}^{(j)}) \\le f(x_k) + \\langle \\nabla f(x_k), x_{k+1}^{(j)} - x_k \\rangle + \\frac{1}{2\\gamma_k^{(j)}} \\|x_{k+1}^{(j)} - x_k\\|^2 $$\n该条件实质上验证了我们的步长 $\\gamma_k^{(j)}$ 足够小，使得 $1/\\gamma_k^{(j)}$ 是对 Lipschitz 常数的一个充分的局部估计。如果条件满足，则接受该步（$x_{k+1} = x_{k+1}^{(j)}$），我们甚至可以将 $\\gamma_k^{(j)}$ 用作下一次迭代的初始猜测。如果条件不满足，则减小步长（例如，对于某个缩减因子 $\\beta \\in (0,1)$，令 $\\gamma_k^{(j+1)} = \\beta \\gamma_k^{(j)}$），并重复检查。\n\n从第一性原理出发，对于具有 $L$-Lipschitz 梯度的函数，其下降引理保证了 $f(y) \\le f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{L}{2}\\|y-x\\|^2$。这意味着对于任何 $\\gamma_k \\le 1/L$，回溯条件将总是被满足。因此，回溯搜索保证在有限次数的缩减后终止（对于给定的 $k$），并找到一个能确保 $F$ 的代理函数下降的步长，从而得到一个即使在 $L$ 未知的情况下也能够证明收敛的算法。这可以防止数值不稳定性，并使该算法在复杂的逆问题实际应用中具有鲁棒性。", "answer": "$$ \\boxed{\\frac{3}{2}} $$", "id": "3415791"}, {"introduction": "理论的最终目的是解决实际问题。这个高级编码实践 [@problem_id:3415789] 将带你进入一个真实的科学计算场景：一个带偏微分方程（PDE）约束的反问题。你将综合运用之前学到的知识，为这个问题实现一个带回溯线搜索的近端梯度求解器，以从稀疏的观测数据中恢复一个稀疏的源项。这个练习不仅锻炼了你对近端梯度法和 $L_1$ 正则化的理解，还让你掌握了伴随状态法等高效计算梯度的关键技巧，完美地连接了优化理论与复杂反问题的求解实践。", "problem": "要求您为一个带 $\\ell_{1}$ 型先验的线性偏微分方程（PDE）约束的逆问题，实现一个带回溯线搜索的近端梯度法。未知量是一个控制向量 $u \\in \\mathbb{R}^{N}$，它在状态方程中充当分布源。状态 $y \\in \\mathbb{R}^{N}$ 满足一个离散的一维 Dirichlet 边界值问题\n$$\nA y = u,\n$$\n其中 $A \\in \\mathbb{R}^{N \\times N}$ 是单位区间上带有齐次 Dirichlet 边界条件的负拉普拉斯算子的标准二阶有限差分离散，即 $A = \\frac{1}{h^{2}} \\operatorname{tridiag}(-1,2,-1)$，其中 $h = \\frac{1}{N+1}$。观测是状态的线性采样，\n$$\nd = C y^{\\star} + \\varepsilon,\n$$\n其中 $y^{\\star}$ 是通过 $A y^{\\star} = u^{\\star}$ 对应于未知的真实控制 $u^{\\star}$ 的状态，$C \\in \\mathbb{R}^{m \\times N}$ 选择 $m$ 个网格点，$\\varepsilon$ 是加性零均值高斯噪声。数据失配项为\n$$\nf(u) = \\frac{1}{2}\\|C y(u) - d\\|_{2}^{2}, \\quad \\text{其中 } y(u) \\text{ 由 } A y(u) = u \\text{ 定义},\n$$\n正则化项为\n$$\ng(u) = \\lambda \\|u\\|_{1},\n$$\n其中 $\\lambda  0$。目标是通过使用带回溯的近端梯度法来最小化 $F(u) = f(u) + g(u)$，从而恢复一个近似解 $\\widehat{u}$，并在此过程中估计 $\\nabla f$ 的未知 Lipschitz 常数。\n\n您必须从第一性原理出发，仅使用定义和经过验证的事实来实现以下组件。\n\n1. 无需求解稠密矩阵的正向和伴随模型算子：\n   - 正向算子求解 $A y = u$ 得到 $y$，观测算子通过在指定索引处选择 $y$ 的分量来应用 $C$。\n   - 梯度 $\\nabla f(u)$ 必须通过伴随状态法以无矩阵（matrix-free）的方式计算。利用 $A$ 是对称正定的事实，可以推导出梯度由求解以下方程的伴随变量 $p$ 给出\n     $$\n     A p = C^{\\top} (C y - d),\n     $$\n     因此 $\\nabla f(u) = p$。\n\n2. 带回溯的近端梯度步：\n   - 令 $g(u) = \\lambda \\|u\\|_{1}$ 的近端映射为逐分量应用的软阈值算子，即对于步长 $t  0$，\n     $$\n     \\operatorname{prox}_{t g}(z) = \\operatorname{sign}(z)\\,\\max\\{|z| - \\lambda t, 0\\}。\n     $$\n   - 基于 $L$-Lipschitz 连续梯度的基本特性实现回溯线搜索。您必须使用这样一个不等式：对于一个 $L$-Lipschitz 连续的梯度，二次上界模型是 $f$ 的一个上界。由于 $L$ 是未知的，使用回溯法找到满足以下条件的 $t$\n     $$\n     f(u^{+}) \\le f(u) + \\nabla f(u)^{\\top} (u^{+} - u) + \\frac{1}{2 t}\\|u^{+} - u\\|_{2}^{2},\n     $$\n     其中 $u^{+} = \\operatorname{prox}_{t g}\\big(u - t \\nabla f(u)\\big)$。从一个初始步长 $t_{0}  0$ 开始，并以因子 $\\eta \\in (0,1)$ 缩小步长，直到该不等式成立。\n\n3. 停止准则：\n   - 当目标函数的相对减小量满足以下条件时终止\n     $$\n     \\frac{|F(u_{k+1}) - F(u_{k})|}{\\max\\{1, F(u_{k})\\}} \\le \\tau,\n     $$\n     其中容差为 $\\tau  0$，或达到最大迭代次数 $K_{\\max}$ 时终止。\n\n观测算子定义。对于给定的 $N$ 和 $m$，确定性地定义观测索引集为 $i_{j} = \\left\\lfloor \\frac{j (N-1)}{m-1} \\right\\rfloor$，其中 $j = 0, 1, \\dots, m-1$，$C$ 提取这些索引对应的 $y[i_{j}]$。通过将向量 $r \\in \\mathbb{R}^{m}$ 的分量在相同索引处补零插入到 $\\mathbb{R}^{N}$ 的零向量中来定义 $C^{\\top}$。\n\n真实值和数据生成。对于每个测试案例，固定相同的真实控制 $u^{\\star} \\in \\mathbb{R}^{N}$，其在索引 $k_{1} = \\lfloor 0.15 N \\rfloor$、$k_{2} = \\lfloor 0.50 N \\rfloor$、$k_{3} = \\lfloor 0.80 N \\rfloor$ 处有三个非零项，振幅分别为 $a_{1} = 1.0$、$a_{2} = -0.7$、$a_{3} = 0.5$。设置 $u^{\\star}[k_{j}] = a_{j}$，其他位置为 0。通过求解 $A y^{\\star} = u^{\\star}$ 形成 $y^{\\star}$。观测为 $d = C y^{\\star} + \\varepsilon$，其中 $\\varepsilon$ 是方差为 $\\sigma^{2}$ 的独立同分布高斯噪声。使用固定的随机种子，以确保每次运行产生相同的噪声。\n\n近端梯度算法输入。对于每个测试案例，您将获得 $N$、$m$、$\\lambda$、噪声标准差 $\\sigma$、初始步长 $t_{0}$、收缩因子 $\\eta$、最大迭代次数 $K_{\\max}$ 和容差 $\\tau$。\n\n测试套件。您的程序必须在以下四种情况下运行求解器，并对每种情况返回相对重建误差\n$$\nE = \\frac{\\| \\widehat{u} - u^{\\star} \\|_{2}}{\\|u^{\\star}\\|_{2}},\n$$\n结果为浮点数。\n\n- 案例 A（基准：观测良好，正则化适中）：$N = 128$，$m = 64$，$\\lambda = 1\\times 10^{-3}$，$\\sigma = 1\\times 10^{-3}$，$t_{0} = 1.0$，$\\eta = 0.5$，$K_{\\max} = 200$，$\\tau = 1\\times 10^{-6}$。\n- 案例 B（强正则化）：$N = 128$，$m = 64$，$\\lambda = 5\\times 10^{-2}$，$\\sigma = 1\\times 10^{-3}$，$t_{0} = 1.0$，$\\eta = 0.5$，$K_{\\max} = 200$，$\\tau = 1\\times 10^{-6}$。\n- 案例 C（更稀疏的观测）：$N = 128$，$m = 16$，$\\lambda = 5\\times 10^{-3}$，$\\sigma = 1\\times 10^{-3}$，$t_{0} = 1.0$，$\\eta = 0.5$，$K_{\\max} = 200$，$\\tau = 1\\times 10^{-6}$。\n- 案例 D（噪声更大的数据）：$N = 128$，$m = 64$，$\\lambda = 2\\times 10^{-3}$，$\\sigma = 5\\times 10^{-3}$，$t_{0} = 1.0$，$\\eta = 0.5$，$K_{\\max} = 200$，$\\tau = 1\\times 10^{-6}$。\n\n实现约束和输出格式。\n- 您必须按照上述描述，通过伴随状态求解来实现梯度计算，无需构建任何稠密的标准方程。\n- 您必须按照规定实现带回溯线搜索的近端梯度法，从 $u_{0} = 0$ 开始。\n- 为保证可复现性，对所有噪声实现，请将随机种子固定为您选择的某个常数值。\n- 您的程序应生成单行输出，其中包含案例 A、B、C 和 D 的四个相对误差列表，格式为用方括号括起来的逗号分隔列表，例如 $[e_{A},e_{B},e_{C},e_{D}]$。\n- 此问题不涉及物理单位。\n\n您的提交必须是最终答案部分指定的完整、可运行的程序。", "solution": "我们从一个线性偏微分方程（PDE）约束的逆问题的基本设置开始，该问题带有一个加性线性观测算子。离散状态 $y \\in \\mathbb{R}^{N}$ 和控制 $u \\in \\mathbb{R}^{N}$ 通过带有齐次 Dirichlet 边界条件的负拉普拉斯算子的对称正定有限差分离散连接起来。令 $h = \\frac{1}{N+1}$，则\n$$\nA = \\frac{1}{h^{2}}\\begin{bmatrix}\n2  -1  0  \\cdots  0\\\\\n-1  2  -1  \\ddots  \\vdots\\\\\n0  -1  2  \\ddots  0\\\\\n\\vdots  \\ddots  \\ddots  \\ddots  -1\\\\\n0  \\cdots  0  -1  2\n\\end{bmatrix}.\n$$\n该矩阵是严格对角占优且对称正定的，因此是可逆的。状态方程为 $A y = u$，所以 $y = A^{-1} u$。观测是由 $C \\in \\mathbb{R}^{m \\times N}$ 选择的点采样，我们将其实现为问题中指定的索引选择算子。\n\n要最小化的目标函数是 $F(u) = f(u) + g(u)$，其中\n$$\nf(u) = \\frac{1}{2}\\|C y(u) - d\\|_{2}^{2}, \\quad g(u) = \\lambda \\|u\\|_{1},\n$$\n且 $y(u)$ 由 $A y(u) = u$ 定义。\n\n梯度计算原理。由于 $y$ 线性依赖于 $u$，且 $C$ 是线性的，数据失配项 $f$ 是可微的，其梯度可通过伴随状态法给出。令 $r(u) = C y(u) - d \\in \\mathbb{R}^{m}$。$f$ 在方向 $\\delta u$ 上的 Gâteaux 导数为\n$$\n\\mathrm{D} f(u)[\\delta u] = \\langle r(u), C \\, \\mathrm{D} y(u)[\\delta u] \\rangle = \\langle r(u), C A^{-1} \\delta u \\rangle.\n$$\n使用欧几里得内积和伴随恒等式 $\\langle r, C A^{-1} \\delta u \\rangle = \\langle C^{\\top} r, A^{-1} \\delta u \\rangle = \\langle A^{-\\top} C^{\\top} r, \\delta u \\rangle$，导数的 Riesz 表示得出\n$$\n\\nabla f(u) = A^{-\\top} C^{\\top} r(u).\n$$\n由于 $A$ 是对称的，$A^{-\\top} = A^{-1}$，因此梯度是伴随状态 $p$，定义为\n$$\nA p = C^{\\top} r(u), \\quad \\text{即 } \\nabla f(u) = p.\n$$\n这可以通过两次关于 $A$ 的线性求解来计算：一次计算 $y(u)$，第二次计算 $p$。\n\n近端映射原理。函数 $g(u) = \\lambda \\|u\\|_{1}$ 是真、凸且下半连续的。其对于步长 $t  0$ 的近端映射是可分离的，并由软阈值给出，\n$$\n\\operatorname{prox}_{t g}(z) = \\arg\\min_{u} \\left\\{ \\frac{1}{2 t} \\|u - z\\|_{2}^{2} + \\lambda \\|u\\|_{1} \\right\\}\n= \\operatorname{sign}(z) \\, \\max\\{|z| - \\lambda t, 0\\},\n$$\n逐分量应用。这是凸分析中一个经过充分检验并广泛使用的事实。\n\n回溯线搜索原理。设 $f$ 具有 $L$-Lipschitz 连续梯度，意即\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\|_{2} \\le L \\|x - y\\|_{2}, \\quad \\forall x,y.\n$$\n一个基本推论，有时称为下降引理，表明对于所有 $x,y$ 和任何满足 Lipschitz 条件的 $L$，\n$$\nf(y) \\le f(x) + \\nabla f(x)^{\\top} (y - x) + \\frac{L}{2} \\|y - x\\|_{2}^{2}.\n$$\n等价地，对于步长 $t \\le \\frac{1}{L}$，\n$$\nf(y) \\le f(x) + \\nabla f(x)^{\\top} (y - x) + \\frac{1}{2 t} \\|y - x\\|_{2}^{2}.\n$$\n在近端梯度法中，首先计算候选点\n$$\nu^{+}(t) = \\operatorname{prox}_{t g}\\big(u - t \\nabla f(u)\\big),\n$$\n然后使用回溯法减小 $t$，直到上界条件\n$$\nf\\big(u^{+}(t)\\big) \\le f(u) + \\nabla f(u)^{\\top} \\big(u^{+}(t) - u\\big) + \\frac{1}{2 t} \\|u^{+}(t) - u\\|_{2}^{2}\n$$\n成立。这个条件是下降引理的直接应用，并且不需要预先知道 $L$。如果不等式成立，则接受该步长，并设置 $u \\leftarrow u^{+}(t)$。否则，以固定的 $\\eta \\in (0,1)$ 缩小步长 $t \\leftarrow \\eta t$ 并重试。这样可以自适应地估计局部 Lipschitz 常数的一个有效界，并保证在温和条件下目标函数下降。\n\n算法设计。对于每次迭代：\n1. 通过求解 $A y = u$ 计算 $y$，计算残差 $r = C y - d$，以及 $f(u) = \\frac{1}{2}\\|r\\|_{2}^{2}$。\n2. 通过求解 $A p = C^{\\top} r$ 计算 $p$ 并设置 $\\nabla f(u) = p$。\n3. 将 $t$ 初始化为当前猜测值（从 $t_{0}$ 开始）。\n4. 回溯循环：形成 $u^{+} = \\operatorname{prox}_{t g}(u - t p)$，通过求解 $A y^{+} = u^{+}$ 并评估 $\\frac{1}{2}\\|C y^{+} - d\\|_{2}^{2}$ 来计算 $f(u^{+})$，并检查上界不等式。如果失败，缩小 $t \\leftarrow \\eta t$ 并重复。\n5. 接受 $u \\leftarrow u^{+}$，更新目标函数 $F(u) = f(u) + \\lambda \\|u\\|_{1}$，并检查停止准则\n$$\n\\frac{|F_{k+1} - F_{k}|}{\\max\\{1, F_{k}\\}} \\le \\tau,\n$$\n或者如果 $k$ 达到 $K_{\\max}$ 则停止。\n\n无矩阵特性。该算法仅使用关于 $A$ 的求解以及 $C$ 和 $C^{\\top}$ 的应用，从不显式地构建稠密正规方程或求逆，满足了无矩阵梯度的要求。由于 $A$ 是固定的，预先计算 $A$ 的稀疏分解并在所有求解中重复使用是高效的。\n\n测试套件和结果。对于四个指定的案例中的每一个，我们生成 $u^{\\star}$ 作为在指定索引和振幅处的三个尖峰，求解 $y^{\\star}$，用固定的高斯噪声种子生成观测值 $d = C y^{\\star} + \\varepsilon$，并使用给定的参数运行带回溯的近端梯度法。每个案例的输出是相对误差\n$$\nE = \\frac{\\| \\widehat{u} - u^{\\star} \\|_{2}}{\\|u^{\\star}\\|_{2}}.\n$$\n我们期望案例 A 由于有足够的观测和适度的正则化而产生相对较小的误差。由于强烈的收缩偏差，案例 B 可能会产生较大的误差。案例 C 测试了观测较少和正则化适中的欠定情况。案例 D 测试了在较高噪声下的鲁棒性。程序会打印一行，其中包含四个误差值，格式为 $[e_{A},e_{B},e_{C},e_{D}]$。", "answer": "```python\nimport numpy as np\nfrom scipy.sparse import diags, csc_matrix\nfrom scipy.sparse.linalg import splu\n\ndef build_poisson_dirichlet_matrix(N: int) - csc_matrix:\n    \"\"\"\n    Build the 1D negative Laplacian with Dirichlet boundary conditions on (0,1),\n    using second-order centered finite differences on N interior points.\n    A = (1/h^2) * tridiag(-1, 2, -1)\n    \"\"\"\n    h = 1.0 / (N + 1)\n    main = 2.0 * np.ones(N) / (h * h)\n    off = -1.0 * np.ones(N - 1) / (h * h)\n    A = diags([off, main, off], offsets=[-1, 0, 1], format='csc')\n    return A\n\ndef choose_observation_indices(N: int, m: int) - np.ndarray:\n    \"\"\"\n    Deterministically select m observation indices in [0, N-1].\n    Use integer flooring of an equally spaced grid including endpoints to avoid randomness.\n    \"\"\"\n    if m == 1:\n        return np.array([N // 2], dtype=int)\n    # floor of linspace(0, N-1, m) ensures monotone, typically distinct for m=N\n    idx = np.floor(np.linspace(0, N - 1, m)).astype(int)\n    # Ensure uniqueness just in case; if duplicates occur, we adjust slightly\n    # but keep length m by filling missing with nearest.\n    uniq, counts = np.unique(idx, return_counts=True)\n    if uniq.size  m:\n        # Fill missing with nearest unused indices\n        used = set(idx.tolist())\n        needed = m - len(used)\n        # simple fill: scan through 0..N-1 and add missing\n        fill = []\n        for j in range(N):\n            if j not in used:\n                fill.append(j)\n                if len(fill) == needed:\n                    break\n        idx = np.array(list(used) + fill, dtype=int)\n        idx.sort()\n        idx = idx[:m]\n    return idx\n\ndef soft_threshold(z: np.ndarray, alpha: float) - np.ndarray:\n    \"\"\"\n    Soft-thresholding operator: prox_{alpha * ||.||_1}(z)\n    \"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - alpha, 0.0)\n\nclass PDEInverseProblem:\n    def __init__(self, N: int, m: int, lam: float, sigma: float, rng: np.random.Generator):\n        self.N = N\n        self.m = m\n        self.lam = lam\n        self.sigma = sigma\n        self.rng = rng\n        # Build and factorize A\n        A = build_poisson_dirichlet_matrix(N)\n        self.A_lu = splu(A)  # sparse LU factorization for repeated solves\n        # Observations indices\n        self.obs_idx = choose_observation_indices(N, m)\n        # Ground truth u*\n        self.u_true = np.zeros(N, dtype=float)\n        k1 = int(np.floor(0.15 * N))\n        k2 = int(np.floor(0.50 * N))\n        k3 = int(np.floor(0.80 * N))\n        self.u_true[k1] = 1.0\n        self.u_true[k2] = -0.7\n        self.u_true[k3] = 0.5\n        # Generate data d = C y_true + noise\n        self.y_true = self.solve_A(self.u_true)\n        self.d = self.apply_C(self.y_true) + self.rng.normal(0.0, self.sigma, size=self.m)\n\n    def solve_A(self, rhs: np.ndarray) - np.ndarray:\n        \"\"\"\n        Solve A x = rhs using the precomputed LU factorization.\n        \"\"\"\n        return self.A_lu.solve(rhs)\n\n    def apply_C(self, y: np.ndarray) - np.ndarray:\n        \"\"\"\n        Apply observation operator: select entries at obs_idx.\n        \"\"\"\n        return y[self.obs_idx]\n\n    def apply_CT(self, r: np.ndarray) - np.ndarray:\n        \"\"\"\n        Apply the adjoint observation operator: insert residuals at obs_idx.\n        \"\"\"\n        v = np.zeros(self.N, dtype=float)\n        v[self.obs_idx] = r\n        return v\n\n    def f_val(self, u: np.ndarray) - float:\n        \"\"\"\n        Compute f(u) = 0.5 * ||C y(u) - d||^2\n        \"\"\"\n        y = self.solve_A(u)\n        r = self.apply_C(y) - self.d\n        return 0.5 * float(np.dot(r, r))\n\n    def f_and_grad(self, u: np.ndarray):\n        \"\"\"\n        Compute f(u) and grad f(u), returning (f, grad, y, residual)\n        \"\"\"\n        y = self.solve_A(u)\n        r = self.apply_C(y) - self.d\n        f = 0.5 * float(np.dot(r, r))\n        rhs = self.apply_CT(r)\n        p = self.solve_A(rhs)\n        grad = p\n        return f, grad, y, r\n\n    def F_val(self, u: np.ndarray) - float:\n        \"\"\"\n        Compute F(u) = f(u) + lam * ||u||_1\n        \"\"\"\n        return self.f_val(u) + self.lam * float(np.sum(np.abs(u)))\n\ndef proximal_gradient_backtracking(problem: PDEInverseProblem,\n                                   t0: float,\n                                   eta: float,\n                                   Kmax: int,\n                                   tol: float):\n    \"\"\"\n    Proximal gradient with backtracking for F = f + lam*||.||_1.\n    Returns the estimated u and the final objective value.\n    \"\"\"\n    N = problem.N\n    lam = problem.lam\n    # Initialize\n    u = np.zeros(N, dtype=float)\n    f_u, grad_u, _, _ = problem.f_and_grad(u)\n    F_u = f_u + lam * float(np.sum(np.abs(u)))\n    t = t0\n    for k in range(Kmax):\n        # Backtracking loop\n        accepted = False\n        bt_iter = 0\n        while not accepted and bt_iter  50:\n            # Candidate via proximal step\n            z = soft_threshold(u - t * grad_u, lam * t)\n            dz = z - u\n            # Evaluate f(z)\n            y_z = problem.solve_A(z)\n            r_z = problem.apply_C(y_z) - problem.d\n            f_z = 0.5 * float(np.dot(r_z, r_z))\n            # Quadratic upper bound Q_t(z; u)\n            quad = f_u + float(np.dot(grad_u, dz)) + 0.5 * (np.dot(dz, dz) / t)\n            if f_z = quad + 1e-12:\n                accepted = True\n                u_new = z\n                f_u_new = f_z\n            else:\n                t *= eta\n                bt_iter += 1\n        if not accepted:\n            # Accept the last candidate anyway to prevent stalling\n            u_new = z\n            f_u_new = f_z\n        F_u_new = f_u_new + lam * float(np.sum(np.abs(u_new)))\n        # Check convergence\n        rel_dec = abs(F_u_new - F_u) / max(1.0, F_u)\n        # Prepare next iteration\n        u = u_new\n        F_u = F_u_new\n        # Recompute grad at new u for next iteration\n        f_u, grad_u, _, _ = problem.f_and_grad(u)\n        if rel_dec = tol:\n            break\n    return u, F_u\n\ndef run_test_case(N, m, lam, sigma, t0, eta, Kmax, tol, rng):\n    problem = PDEInverseProblem(N=N, m=m, lam=lam, sigma=sigma, rng=rng)\n    u_est, _ = proximal_gradient_backtracking(problem, t0=t0, eta=eta, Kmax=Kmax, tol=tol)\n    # Relative error\n    num = np.linalg.norm(u_est - problem.u_true)\n    den = np.linalg.norm(problem.u_true)\n    err = float(num / den) if den > 0 else float(num)\n    return err\n\ndef solve():\n    # Define the test cases from the problem statement with a fixed RNG for reproducibility.\n    rng = np.random.default_rng(20231011)\n    test_cases = [\n        # Case A\n        {\"N\": 128, \"m\": 64, \"lam\": 1e-3, \"sigma\": 1e-3, \"t0\": 1.0, \"eta\": 0.5, \"Kmax\": 200, \"tol\": 1e-6},\n        # Case B\n        {\"N\": 128, \"m\": 64, \"lam\": 5e-2, \"sigma\": 1e-3, \"t0\": 1.0, \"eta\": 0.5, \"Kmax\": 200, \"tol\": 1e-6},\n        # Case C\n        {\"N\": 128, \"m\": 16, \"lam\": 5e-3, \"sigma\": 1e-3, \"t0\": 1.0, \"eta\": 0.5, \"Kmax\": 200, \"tol\": 1e-6},\n        # Case D\n        {\"N\": 128, \"m\": 64, \"lam\": 2e-3, \"sigma\": 5e-3, \"t0\": 1.0, \"eta\": 0.5, \"Kmax\": 200, \"tol\": 1e-6},\n    ]\n\n    results = []\n    for case in test_cases:\n        # Use a new RNG derived from the master to ensure independent but reproducible noise\n        # by splitting the bit generator.\n        child_seed = rng.integers(0, np.iinfo(np.int32).max)\n        child_rng = np.random.default_rng(int(child_seed))\n        err = run_test_case(\n            N=case[\"N\"], m=case[\"m\"], lam=case[\"lam\"], sigma=case[\"sigma\"],\n            t0=case[\"t0\"], eta=case[\"eta\"], Kmax=case[\"Kmax\"], tol=case[\"tol\"],\n            rng=child_rng\n        )\n        results.append(err)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3415789"}]}