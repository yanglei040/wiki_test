{"hands_on_practices": [{"introduction": "迭代软阈值算法（ISTA）的核心在于其近端步骤，该步骤通过一个称为软阈值算子的非线性映射来促进稀疏性。这个练习旨在通过一个具体的数值计算，让您亲身体验软阈值算子是如何工作的。通过从第一性原理出发，您将理解为什么某些分量被置零，而另一些则被“收缩”，从而对ISTA促进稀疏性的机制建立直观的认识。[@problem_id:3392980]", "problem": "考虑一个典型的稀疏线性反问题，其形式为最小化一个二次数据失配项和一个逐元素的稀疏性促进惩罚项之和：寻找 $x \\in \\mathbb{R}^{5}$ 以最小化 $f(x) + g(x)$，其中 $f(x) = \\tfrac{1}{2}\\|A x - y\\|_{2}^{2}$ 且 $g(x) = \\lambda \\|x\\|_{1}$。在迭代软阈值算法 (ISTA) 中，在迭代点 $x^{k}$ 处对 $f$ 进行步长为 $1/L$ 的前向（梯度）步骤，之后是对 $g$ 的近端步骤。假设在某次迭代中，前向步骤之后，中间向量为 $u \\in \\mathbb{R}^{5}$，并且近端步骤中使用的标量阈值为 $\\tau > 0$。经过缩放的 $\\ell_{1}$-范数在 $u$ 处的近端映射记作 $S_{\\tau}(u)$。\n\n对于具体值 $u = (\\,3,\\,-1,\\,\\tfrac{3}{2},\\,-4,\\,\\tfrac{1}{2}\\,)^{\\top} \\in \\mathbb{R}^{5}$ 和 $\\tau = \\tfrac{3}{2}$，请精确计算 $S_{\\tau}(u)$，并基于凸分析的基本原理和近端算子的性质来解释哪些分量被设置为零，哪些被收缩（以及朝哪个方向收缩）。将最终答案表示为具有精确有理数分量的单个行向量。不要四舍五入。", "solution": "该问题是有效的。它是适定的，科学上基于凸优化和反问题的原理，并为得到唯一、可验证的解提供了所有必要信息。\n\n该问题要求计算经过缩放的 $\\ell_1$-范数的近端算子，通常称为软阈值算子，记作 $S_{\\tau}(u)$。该算子是用于解决稀疏优化问题的迭代软阈值算法 (ISTA) 的核心。\n\n根据凸分析的基本原理，将带有缩放参数 $\\gamma > 0$ 的函数 $h(x)$ 的近端算子应用于向量 $v$，定义为以下目标函数的唯一最小化子：\n$$\n\\text{prox}_{\\gamma h}(v) = \\arg\\min_{z} \\left( \\gamma h(z) + \\frac{1}{2} \\|z - v\\|_{2}^{2} \\right)\n$$\n在本问题的背景下，该函数是 $\\ell_1$-范数，算子记作 $S_{\\tau}(u)$。这意味着我们求解 $x = S_{\\tau}(u)$，其中：\n$$\nx = \\arg\\min_{x \\in \\mathbb{R}^{5}} \\left( \\tau \\|x\\|_{1} + \\frac{1}{2} \\|x - u\\|_{2}^{2} \\right)\n$$\n要最小化的目标函数是 $J(x) = \\tau \\|x\\|_{1} + \\frac{1}{2} \\|x - u\\|_{2}^{2}$。该目标函数的一个关键性质是其在向量 $x$ 的各个分量上是可分的。我们可以将范数重写为各分量的和：\n$$\nJ(x) = \\tau \\sum_{i=1}^{5} |x_i| + \\frac{1}{2} \\sum_{i=1}^{5} (x_i - u_i)^2 = \\sum_{i=1}^{5} \\left( \\tau |x_i| + \\frac{1}{2} (x_i - u_i)^2 \\right)\n$$\n由于整个最小化问题是各项之和，而每一项仅依赖于一个分量 $x_i$，我们可以通过对每个分量 $i \\in \\{1, 2, 3, 4, 5\\}$ 独立地最小化每一项来最小化整个和。因此，对于每个 $i$，我们求解标量最小化问题：\n$$\nx_i = \\arg\\min_{\\xi \\in \\mathbb{R}} \\left( \\tau |\\xi| + \\frac{1}{2} (\\xi - u_i)^2 \\right)\n$$\n令 $J_i(\\xi) = \\tau |\\xi| + \\frac{1}{2} (\\xi - u_i)^2$。由于 $J_i(\\xi)$ 是一个凸函数，其最小值在其次梯度包含零的位置达到。$J_i$ 在 $\\xi$ 处的次微分为：\n$$\n\\partial J_i(\\xi) = \\tau \\partial|\\xi| + (\\xi - u_i)\n$$\n绝对值函数 $\\partial|\\xi|$ 的次微分为：\n$$\n\\partial|\\xi| = \\begin{cases} \\{1\\} & \\text{if } \\xi > 0 \\\\ \\{-1\\} & \\text{if } \\xi < 0 \\\\ [-1, 1] & \\text{if } \\xi = 0 \\end{cases}\n$$\n最优性条件是 $0 \\in \\partial J_i(x_i)$。我们针对最优值 $x_i$ 分情况分析此条件：\n\n情况 1：$x_i > 0$。\n次梯度为 $\\partial J_i(x_i) = \\{\\tau + (x_i - u_i)\\}$。将其设为零得到 $\\tau + x_i - u_i = 0$，这意味着 $x_i = u_i - \\tau$。为使此解与假设 $x_i > 0$ 一致，我们必须有 $u_i - \\tau > 0$，即 $u_i > \\tau$。\n\n情况 2：$x_i < 0$。\n次梯度为 $\\partial J_i(x_i) = \\{-\\tau + (x_i - u_i)\\}$。将其设为零得到 $-\\tau + x_i - u_i = 0$，这意味着 $x_i = u_i + \\tau$。为使此解与假设 $x_i < 0$ 一致，我们必须有 $u_i + \\tau < 0$，即 $u_i < -\\tau$。\n\n情况 3：$x_i = 0$。\n最优性条件变为 $0 \\in \\tau [-1, 1] + (0 - u_i)$，这可以简化为 $u_i \\in \\tau [-1, 1]$。这等价于 $-\\tau \\le u_i \\le \\tau$，或 $|u_i| \\le \\tau$。\n\n综合这三种情况，我们得到作用于标量 $u_i$ 的软阈值算子的显式公式：\n$$\nx_i = S_{\\tau}(u_i) = \\begin{cases} u_i - \\tau & \\text{if } u_i > \\tau \\\\ 0 & \\text{if } |u_i| \\le \\tau \\\\ u_i + \\tau & \\text{if } u_i < -\\tau \\end{cases}\n$$\n这个规则解释了该算子的行为。如果分量 $u_i$ 的绝对值小于或等于阈值 $\\tau$，它就被设置为零，从而促进稀疏性。如果其绝对值超过阈值，它在保持其符号的同时，向零收缩了 $\\tau$ 的量。\n\n现在我们将此规则应用于给定的向量 $u = (\\,3,\\,-1,\\,\\frac{3}{2},\\,-4,\\,\\frac{1}{2}\\,)^{\\top}$ 和阈值 $\\tau = \\frac{3}{2}$。\n\n对于第一个分量 $u_1 = 3$：\n由于 $u_1 = 3 > \\tau = \\frac{3}{2}$，该分量被收缩。\n$x_1 = u_1 - \\tau = 3 - \\frac{3}{2} = \\frac{6}{2} - \\frac{3}{2} = \\frac{3}{2}$。\n\n对于第二个分量 $u_2 = -1$：\n由于 $|u_2| = |-1| = 1 \\le \\tau = \\frac{3}{2}$，该分量被设置为零。\n$x_2 = 0$。\n\n对于第三个分量 $u_3 = \\frac{3}{2}$：\n由于 $|u_3| = |\\frac{3}{2}| = \\frac{3}{2} \\le \\tau = \\frac{3}{2}$，该分量被设置为零（它落在阈值区域的边界上）。\n$x_3 = 0$。\n\n对于第四个分量 $u_4 = -4$：\n由于 $u_4 = -4 < -\\tau = -\\frac{3}{2}$，该分量被收缩。\n$x_4 = u_4 + \\tau = -4 + \\frac{3}{2} = -\\frac{8}{2} + \\frac{3}{2} = -\\frac{5}{2}$。\n\n对于第五个分量 $u_5 = \\frac{1}{2}$：\n由于 $|u_5| = |\\frac{1}{2}| = \\frac{1}{2} \\le \\tau = \\frac{3}{2}$，该分量被设置为零。\n$x_5 = 0$。\n\n综合这些结果，最终的向量是 $S_{\\tau}(u) = (\\frac{3}{2}, 0, 0, -\\frac{5}{2}, 0)^{\\top}$。按要求，这被表示为单个行向量。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{2} & 0 & 0 & -\\frac{5}{2} & 0\n\\end{pmatrix}\n}\n$$", "id": "3392980"}, {"introduction": "理论上有效的算法在实践中的表现，往往取决于其关键参数的明智选择。对于ISTA而言，正则化参数 $\\lambda$ 的设置至关重要。这个练习将引导您探索两个关键方面：首先，通过构建一个反例，您将看到一个过大的 $\\lambda$ 会如何导致算法停滞在无意义的零解上；其次，您将从概率论的角度推导出一个选择 $\\lambda$ 的原则性方法，以避免解被噪声淹没。[@problem_id:3392955]", "problem": "考虑一个线性逆问题，其中未知向量为 $x^{\\star} \\in \\mathbb{R}^{n}$，传感矩阵为 $A \\in \\mathbb{R}^{n \\times n}$，观测数据为 $b \\in \\mathbb{R}^{n}$，由 $b = A x^{\\star} + \\eta$ 生成，其中 $\\eta$ 是一个加性噪声向量。用于解决 $\\ell_{1}$ 正则化最小二乘问题 $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ 的迭代软阈值算法 (ISTA) 由迭代映射 $x^{k+1} = \\mathcal{S}_{\\lambda \\tau}\\!\\left(x^{k} + \\tau A^{\\top}(b - A x^{k})\\right)$ 定义，其中 $\\tau > 0$ 是满足一个标准利普希茨条件的步长，$\\mathcal{S}_{\\theta}$ 表示分量软阈值算子 $\\big(\\mathcal{S}_{\\theta}(z)\\big)_{i} = \\mathrm{sign}(z_{i}) \\max\\{|z_{i}| - \\theta, 0\\}$。\n\n(a) 当正则化参数 $\\lambda$ 相对于数据项过大时，在朴素的阈值选择下，构造一个反例来证明算法会停滞（在零向量处形成平台）。具体来说，取 $n = 3$, $A = I_{3}$, $\\tau = 1$, 以及 $b = (0.12,\\,-0.09,\\,0)^{\\top}$。从上述定义出发，不使用任何捷径，运用第一性原理进行推理，精确地说明为什么在此配置下，当 $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$ 时，ISTA 的迭代结果 $x^{k}$ 对于所有 $k \\geq 1$ 都会恒等于零。\n\n(b) 在一个含噪数据同化的设定中，假设噪声 $\\eta$ 具有独立同分布 (i.i.d.) 的高斯条目，其均值为零，方差为 $\\sigma^{2}$，即 $\\eta \\sim \\mathcal{N}(0,\\,\\sigma^{2} I_{n})$。基于 $n$ 个独立同分布高斯变量最大值的分布，通过概率论证，且不援引任何预先陈述的捷径，推导出一个关于 $\\lambda$ 的、作为 $\\sigma$ 和 $n$ 的函数的解析缩放关系。该缩放关系应能防止更新被噪声主导，同时避免对那些条目值超过典型噪声驱动平台的信号产生平凡零解。使用得到的缩放关系，为 $n = 1024$ 和 $\\sigma = 0.03$ 计算一个推荐的 $\\lambda$ 值。\n\n将 $\\lambda$ 的最终数值四舍五入到四位有效数字。答案以纯数字形式表示，不带单位。", "solution": "该问题提出了与迭代软阈值算法 (ISTA) 相关的两个不同任务。部分 (a) 要求在特定条件下证明迭代停滞，而部分 (b) 则要求在随机设定下为正则化参数 $\\lambda$ 推导一个有原则的缩放关系。\n\n### 部分 (a)：在零向量处的停滞\n\n对于最小化问题 $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$，我们已知 ISTA 的更新规则：\n$$\nx^{k+1} = \\mathcal{S}_{\\lambda \\tau}\\!\\left(x^{k} + \\tau A^{\\top}(b - A x^{k})\\right)\n$$\n问题为此部分指定了参数：$n = 3$，单位矩阵 $A = I_{3}$，步长 $\\tau = 1$，以及数据向量 $b = (0.12,\\,-0.09,\\,0)^{\\top}$。\n\n将 $A=I_{3}$ 和 $\\tau=1$ 代入更新规则，可以得到一个显著的简化。对于任意迭代值 $x^k$：\n$$\nx^{k+1} = \\mathcal{S}_{\\lambda \\cdot 1}\\!\\left(x^{k} + 1 \\cdot I_{3}^{\\top}(b - I_{3} x^{k})\\right) = \\mathcal{S}_{\\lambda}\\!\\left(x^{k} + (b - x^{k})\\right) = \\mathcal{S}_{\\lambda}(b)\n$$\n这个结果表明，对于任何 $k \\geq 0$，下一个迭代值 $x^{k+1}$ 完全由软阈值算子 $\\mathcal{S}_{\\lambda}$ 对数据向量 $b$ 的作用决定。它与当前的迭代值 $x^k$ 无关。因此，从 $k=1$ 开始的所有迭代值都是相同的：\n$$\nx^{1} = \\mathcal{S}_{\\lambda}(b)\n$$\n$$\nx^{2} = \\mathcal{S}_{\\lambda}(b)\n$$\n$$\n\\vdots\n$$\n$$\nx^{k} = \\mathcal{S}_{\\lambda}(b) \\quad \\text{对于所有 } k \\geq 1.\n$$\n问题要求我们证明，在 $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$ 的条件下，这些迭代值会恒等于零，即对所有 $k \\geq 1$ 都有 $x^k = 0$。\n\n对于我们的特定配置，其中 $A=I_3$，该条件变为 $\\lambda \\geq \\|I_3^{\\top} b\\|_{\\infty} = \\|b\\|_{\\infty}$。向量 $b=(0.12,\\,-0.09,\\,0)^{\\top}$ 的 $\\ell_{\\infty}$ 范数是：\n$$\n\\|b\\|_{\\infty} = \\max\\big(|0.12|, |-0.09|, |0|\\big) = 0.12\n$$\n所以条件变为 $\\lambda \\geq 0.12$。\n\n现在我们在此条件下分析迭代值的表达式 $x^k = \\mathcal{S}_{\\lambda}(b)$。软阈值算子 $\\mathcal{S}_{\\theta}$ 按分量定义为：\n$$\n\\big(\\mathcal{S}_{\\theta}(z)\\big)_{i} = \\mathrm{sign}(z_{i}) \\max\\{|z_{i}| - \\theta, 0\\}\n$$\n要使整个向量 $\\mathcal{S}_{\\theta}(z)$ 为零向量，其充分必要条件是对于每个分量 $i$，都有 $\\max\\{|z_{i}| - \\theta, 0\\} = 0$。这又等价于对所有 $i$ 都有 $|z_i| \\leq \\theta$ 的条件。这可以用 $\\ell_{\\infty}$ 范数紧凑地表示为 $\\|z\\|_{\\infty} \\leq \\theta$。\n\n将此应用于我们的问题，使得 $x^k = \\mathcal{S}_{\\lambda}(b)$ 为零向量的条件是 $\\|b\\|_{\\infty} \\leq \\lambda$。对于我们选择的 $A=I_3$，这恰好是问题陈述中给出的条件 $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$。\n\n具体来说，当 $\\lambda \\geq 0.12$ 时：\n对于第一个分量 $b_1 = 0.12$：$|b_1|=0.12$。因为 $\\lambda \\geq 0.12$，所以 $|b_1| - \\lambda \\leq 0$，因此 $\\max\\{|b_1|-\\lambda, 0\\}=0$。\n对于第二个分量 $b_2 = -0.09$：$|b_2|=0.09$。因为 $\\lambda \\geq 0.12$，所以 $|b_2| - \\lambda  0$，因此 $\\max\\{|b_2|-\\lambda, 0\\}=0$。\n对于第三个分量 $b_3 = 0$：$|b_3|=0$。因为 $\\lambda \\geq 0.12$，所以 $|b_3| - \\lambda  0$，因此 $\\max\\{|b_3|-\\lambda, 0\\}=0$。\n\n由于 $\\mathcal{S}_{\\lambda}(b)$ 的所有分量都为零，我们有 $\\mathcal{S}_{\\lambda}(b) = 0$。\n因此，对于任意选择的初始向量 $x^0$，第一个迭代值是 $x^1 = \\mathcal{S}_{\\lambda}(b) = 0$，并且所有后续的迭代值 $x^k$ (对于 $k \\geq 1$) 都保持在零向量。这证明了所述的停滞现象。\n\n### 部分 (b)：正则化参数的概率缩放\n\n在这一部分，我们要推导一个关于 $\\lambda$ 的缩放关系，以防止算法被噪声主导。模型是 $b = Ax^\\star + \\eta$，其中 $\\eta$ 是一个由独立同分布高斯噪声分量组成的向量，$\\eta_i \\sim \\mathcal{N}(0, \\sigma^2)$。\n\n正则化的一个关键原则是选择足够大的 $\\lambda$ 来抑制噪声，但又不能大到错误地消除真实的信号分量。一个常见的策略是选择 $\\lambda$ 略高于在被阈值化的项中预期会看到的噪声水平。\n\n考虑从自然的初始猜测 $x^0 = 0$ 开始的 ISTA 的第一次迭代。软阈值算子的自变量是 $x^0 + \\tau A^{\\top}(b - A x^0) = \\tau A^\\top b$。如果我们考虑一个没有真实信号的场景 ($x^\\star=0$)，那么 $b=\\eta$，这个自变量就变成了 $\\tau A^\\top \\eta$。为了防止算法拟合噪声，我们希望这一项被阈值化为零。这要求 $\\tau A^\\top\\eta$ 的每个分量的幅值都小于或等于阈值 $\\lambda\\tau$。这可以写成：\n$$\n\\|\\tau A^\\top \\eta\\|_{\\infty} \\leq \\lambda\\tau \\implies \\|A^\\top \\eta\\|_{\\infty} \\leq \\lambda\n$$\n这给了我们一个选择 $\\lambda$ 的标准：它应该是 $\\|A^\\top \\eta\\|_{\\infty}$ 的可能值的上界。\n\n为了进行解析推导，我们必须刻画向量 $v = A^\\top \\eta$ 的分布。这个分布依赖于矩阵 $A$。在推导这类通用阈值时所做的一个标准假设是，传感矩阵 $A$ 是标准正交的，即 $A^\\top A = I_n$。在这个假设下，$v$ 的协方差是：\n$$\n\\mathrm{Cov}(v) = E[v v^\\top] = E[A^\\top \\eta \\eta^\\top A] = A^\\top E[\\eta \\eta^\\top] A = A^\\top (\\sigma^2 I_n) A = \\sigma^2 A^\\top A = \\sigma^2 I_n\n$$\n由于 $E[v] = A^\\top E[\\eta] = 0$，向量 $v = A^\\top \\eta$ 的分量 $v_i$ 是均值为0、方差为 $\\sigma^2$ 的独立同分布高斯随机变量，与原始噪声分量 $\\eta_i$ 一样。\n\n我们的任务现在简化为寻找随机变量 $M_n = \\|v\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |v_i|$ 的一个高概率上界。令 $Z_i = v_i/\\sigma$ 为独立同分布的标准正态变量，$Z_i \\sim \\mathcal{N}(0, 1)$。我们寻求对 $\\max_i |Z_i|$ 的一个估计，然后将结果按 $\\sigma$ 进行缩放。\n\n令 $Y = \\max_{1 \\leq i \\leq n} |Z_i|$。$|Z_i|$ 的累积分布函数 (CDF) 为 $F_{|Z|}(y) = P(|Z_i| \\leq y) = 2\\Phi(y)-1$ (对于 $y \\geq 0$)，其中 $\\Phi$ 是标准正态分布的CDF。由于 $|Z_i|$ 是独立同分布的，它们最大值的CDF是：\n$$\nF_Y(y) = P(Y \\leq y) = [F_{|Z|}(y)]^n = (2\\Phi(y)-1)^n\n$$\n我们想找到一个阈值 $y_n$，使得对于大的 $n$，$P(Y  y_n)$ 很小。我们可以使用联合界来近似这个尾部概率：\n$$\nP(Y  y) = P(\\exists i: |Z_i|  y) \\leq \\sum_{i=1}^n P(|Z_i|y) = n P(|Z_1|y)\n$$\n概率 $P(|Z_1|y)$ 是 $2(1-\\Phi(y))$。对于大的 $y$，我们可以使用标准高斯尾部近似：\n$$\n1-\\Phi(y) \\approx \\frac{\\phi(y)}{y} = \\frac{1}{\\sqrt{2\\pi}y}\\exp\\left(-\\frac{y^2}{2}\\right)\n$$\n因此，$P(Yy) \\approx 2n(1-\\Phi(y)) \\approx \\frac{2n}{\\sqrt{2\\pi}y}\\exp\\left(-\\frac{y^2}{2}\\right)$。我们寻找一个 $y$ 随 $n$ 变化的缩放关系，使得当 $n \\to \\infty$ 时这个概率趋于零。让我们测试候选的缩放关系 $y = \\sqrt{2\\ln n}$：\n$$\nP(Y  \\sqrt{2\\ln n}) \\approx \\frac{2n}{\\sqrt{2\\pi}\\sqrt{2\\ln n}}\\exp\\left(-\\frac{( \\sqrt{2\\ln n} )^2}{2}\\right) = \\frac{2n}{\\sqrt{4\\pi\\ln n}}\\exp(-\\ln n) = \\frac{2n}{2\\sqrt{\\pi\\ln n}} \\left(\\frac{1}{n}\\right) = \\frac{1}{\\sqrt{\\pi\\ln n}}\n$$\n当 $n \\to \\infty$ 时，这个概率趋于 $0$。这表明对于大的 $n$， $n$ 个独立同分布的 $|Z_i|$ 变量的最大值很可能不会超过 $\\sqrt{2\\ln n}$。这证明了选择该值作为阈值是合理的。\n\n按 $\\sigma$ 缩放回去，我们得到 $\\lambda$ 的具有解析依据的缩放关系：\n$$\n\\lambda = \\sigma \\sqrt{2 \\ln n}\n$$\n这便是著名的通用阈值。它提供了一个能够自适应问题维度 $n$ 和噪声水平 $\\sigma$ 的参数选择。\n\n现在，我们为 $n = 1024$ 和 $\\sigma = 0.03$ 计算推荐的 $\\lambda$ 值。\n首先，计算 $\\ln(1024)$：\n$$\n\\ln(1024) = \\ln(2^{10}) = 10 \\ln(2)\n$$\n使用 $\\ln(2)$ 的一个标准值 $\\ln(2) \\approx 0.69314718$：\n$$\n\\ln(1024) \\approx 6.9314718\n$$\n现在，将此值代入 $\\lambda$ 的公式：\n$$\n\\lambda = 0.03 \\times \\sqrt{2 \\times 6.9314718} = 0.03 \\times \\sqrt{13.8629436} \\approx 0.03 \\times 3.7232974\n$$\n$$\n\\lambda \\approx 0.1116989\n$$\n将此值四舍五入到四位有效数字，得到：\n$$\n\\lambda \\approx 0.1117\n$$", "answer": "$$\n\\boxed{0.1117}\n$$", "id": "3392955"}, {"introduction": "在许多现代科学与工程应用中，待解决的反问题规模巨大，其观测算子 $A$ 无法在单台计算机的内存中完整存储。为了将ISTA应用于此类大规模问题，我们必须重新设计其核心计算步骤，使其能够并行执行。这个实践练习将指导您如何推导和实现一个分布式版本的ISTA，其中关键的梯度计算和步长估计都在多个处理器上协同完成，这是将优化算法应用于“大数据”场景的一项核心技能。[@problem_id:3392991]", "problem": "考虑一个线性逆问题，其中未知状态 $x \\in \\mathbb{R}^n$ 通过一个线性观测算子 $A \\in \\mathbb{R}^{m \\times n}$ 被观测，得到带噪声的数据 $b \\in \\mathbb{R}^m$。目标是通过最小化以下凸泛函来恢复 $x$ 的一个稀疏估计：\n$$\nJ(x) = \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1,\n$$\n其中 $\\lambda  0$ 是一个正则化参数，$\\lVert \\cdot \\rVert_1$ 表示 $\\ell_1$-范数。迭代软阈值算法 (ISTA) 是一种邻近梯度法，专为形如 $f(x) + g(x)$ 的目标函数设计，其中 $f$ 是光滑的，$g$ 是非光滑的。该算法首先对 $f$ 执行一个步长为 $\\tau$ 的梯度步，然后应用 $g$ 的邻近算子。在此问题中，$f(x) = \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2$ 且 $g(x) = \\lambda \\lVert x \\rVert_1$。$f$ 的梯度由 $\\nabla f(x) = A^{\\top} (A x - b)$ 给出，任何可行的步长 $\\tau$ 必须满足 $0  \\tau  2/L$，其中 $L$ 是 $\\nabla f$ 的利普希茨常数，它等于谱范数的平方 $L = \\lVert A \\rVert_2^2$。\n\n假设 $A$ 是一个并行观测算子，它没有被显式地构建。相反，它按行分块分布在 $p$ 个逻辑处理器上，形式如下：\n$$\nA = \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_p \\end{bmatrix}, \\quad b = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_p \\end{bmatrix},\n$$\n其中 $A_i \\in \\mathbb{R}^{m_i \\times n}$，$b_i \\in \\mathbb{R}^{m_i}$，且 $\\sum_{i=1}^p m_i = m$。每个处理器 $i$ 可以将其局部正演算子应用于任何 $x$，得到 $y_i = A_i x$，也可以将其局部伴随算子应用于一个局部向量 $r_i \\in \\mathbb{R}^{m_i}$，得到 $g_i = A_i^{\\top} r_i$。在计算 ISTA 所需的乘积时，您绝不能显式地构建 $A$；相反，您只能使用这些局部正演和伴随算子应用，并通过对维度匹配的向量求和来跨处理器聚合。\n\n您的任务如下：\n- 从邻近梯度法和块行算子的线性代数性质出发，推导如何在不显式构建 $A$ 的情况下，仅使用局部可用的算子 $A_i$ 和 $A_i^{\\top}$ 来计算梯度项 $\\nabla f(x) = A^{\\top} (A x - b)$。\n- 设计一种方法，仅使用通过分布式局部算子 $A_i$ 和 $A_i^{\\top}$ 实现的与 $A$ 和 $A^{\\top}$ 的乘积来近似利普希茨常数 $L = \\lVert A \\rVert_2^2$。对自伴随映射 $A^{\\top} A$ 应用幂迭代方案。\n- 使用推导出的分布式梯度计算和估计的利普希茨常数来实现 ISTA，以选择一个步长 $\\tau \\in (0, 2/L)$。对 $\\ell_1$-正则化使用软阈值算子。\n- 为了验证，还需实现一个使用显式矩阵 $A$ 来计算梯度的中心化参考版本，并使用相同的 $\\tau$ 和 $\\lambda$ 运行 ISTA。\n\n测试套件规范。您的程序必须实现并运行以下三个测试用例，每个用例都必须生成一个稀疏的合成真实基准 $x_{\\star}$、一个随机高斯矩阵 $A$ 以及带噪声的数据 $b = A x_{\\star} + \\eta$，其中噪声 $\\eta$ 是独立同分布的高斯噪声。在每个测试中，将 $A$ 分成 $p$ 个行块，使得 $m_i$ 尽可能相等。对于真实基准 $x_{\\star}$，在均匀随机的位置上选择恰好 $s$ 个非零项，每一项都独立地从标准正态分布中抽取。分布式和中心化 ISTA 使用相同的步长，选择为 $\\tau = \\alpha / L_{\\text{est}}$，其中 $\\alpha = 0.99$，$L_{\\text{est}}$ 是 $L$ 的分布式幂迭代估计值。用 $x^{0} = 0$ 初始化 ISTA，并精确运行 $K$ 次迭代。\n\n- 测试用例 1 (正常情况): $(n, m, p, s, \\lambda, \\sigma, K, \\text{seed}) = (64, 256, 4, 8, 10^{-2}, 10^{-3}, 200, 42)$。\n- 测试用例 2 (单个处理器的边界情况): $(n, m, p, s, \\lambda, \\sigma, K, \\text{seed}) = (64, 256, 1, 8, 10^{-2}, 10^{-3}, 200, 43)$。\n- 测试用例 3 (单行分块的极端并行情况): $(n, m, p, s, \\lambda, \\sigma, K, \\text{seed}) = (64, 256, 256, 8, 10^{-2}, 10^{-3}, 200, 44)$。\n\n对于每个测试用例，计算并记录以下两个量：\n- 在从标准正态分布中抽取的随机测试点 $x^{\\text{test}}$ 处的相对梯度聚合误差，定义为\n$$\n\\varepsilon_{\\text{grad}} = \\frac{\\lVert g_{\\text{dist}} - g_{\\text{cent}} \\rVert_2}{\\max\\{10^{-16}, \\lVert g_{\\text{cent}} \\rVert_2\\}},\n$$\n其中 $g_{\\text{dist}}$ 是仅使用 $A_i$ 和 $A_i^{\\top}$ 对 $A^{\\top}(A x^{\\text{test}} - b)$ 进行的分布式计算，而 $g_{\\text{cent}}$ 是使用显式矩阵 $A$ 的中心化计算。\n- $K$ 次 ISTA 迭代后的相对解差异，定义为\n$$\n\\varepsilon_{\\text{sol}} = \\frac{\\lVert x^{K}_{\\text{dist}} - x^{K}_{\\text{cent}} \\rVert_2}{\\max\\{10^{-16}, \\lVert x^{K}_{\\text{cent}} \\rVert_2\\}},\n$$\n其中 $x^{K}_{\\text{dist}}$ 和 $x^{K}_{\\text{cent}}$ 分别是分布式和中心化 ISTA 的迭代结果。\n\n您的程序应生成单行输出，其中包含三个测试用例的结果，聚合成一个包含六个浮点数的列表，顺序如下：\n$$\n[\\varepsilon_{\\text{grad}}^{(1)}, \\varepsilon_{\\text{sol}}^{(1)}, \\varepsilon_{\\text{grad}}^{(2)}, \\varepsilon_{\\text{sol}}^{(2)}, \\varepsilon_{\\text{grad}}^{(3)}, \\varepsilon_{\\text{sol}}^{(3)}],\n$$\n以方括号括起来的逗号分隔列表形式打印。不允许有其他输出。所有计算纯属数学计算；不涉及物理单位。", "solution": "该问题要求设计并实现一个分布式迭代软阈值算法 (ISTA) 来解决稀疏恢复问题。核心任务是推导梯度的分布式计算方法，设计一种以分布式方式估计所需利普希茨常数的方法，然后实现该算法并与中心化对应版本进行验证。\n\n目标是通过最小化以下凸泛函来找到线性逆问题 $b \\approx Ax$ 的一个稀疏解 $x \\in \\mathbb{R}^n$：\n$$\nJ(x) = \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1,\n$$\n其中 $f(x) = \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2$ 是光滑的数据保真项，$g(x) = \\lambda \\lVert x \\rVert_1$ 是非光滑的稀疏性促进正则化项。\n\n### 1. 分布式梯度的推导\n\nISTA 算法需要计算光滑项的梯度 $\\nabla f(x)$。该梯度由 $\\nabla f(x) = A^{\\top} (A x - b)$ 给出。问题规定，算子 $A \\in \\mathbb{R}^{m \\times n}$ 和数据向量 $b \\in \\mathbb{R}^m$ 被划分为 $p$ 个行块：\n$$\nA = \\begin{bmatrix} A_1 \\\\ A_2 \\\\ \\vdots \\\\ A_p \\end{bmatrix}, \\quad b = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_p \\end{bmatrix},\n$$\n其中每个块 $A_i \\in \\mathbb{R}^{m_i \\times n}$ 和 $b_i \\in \\mathbb{R}^{m_i}$ 由逻辑处理器 $i$ 管理。目标是仅使用 $A_i$ 及其伴随算子 $A_i^{\\top}$ 的局部应用来计算 $\\nabla f(x)$。\n\n首先，我们用这些块来表示残差向量 $r = A x - b$：\n$$\nr = A x - b = \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_p \\end{bmatrix} x - \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_p \\end{bmatrix} = \\begin{bmatrix} A_1 x - b_1 \\\\ \\vdots \\\\ A_p x - b_p \\end{bmatrix} = \\begin{bmatrix} r_1 \\\\ \\vdots \\\\ r_p \\end{bmatrix}.\n$$\n这表明，每个局部残差分量 $r_i = A_i x - b_i$ 可以在处理器 $i$ 上通过将局部正演算子 $A_i$ 应用于全局状态向量 $x$ 来独立计算。\n\n接下来，我们将全局伴随算子 $A^{\\top}$ 应用于残差 $r$。块列矩阵的转置是块行矩阵：\n$$\nA^{\\top} = \\begin{bmatrix} A_1^{\\top}  A_2^{\\top}  \\cdots  A_p^{\\top} \\end{bmatrix}.\n$$\n梯度是这个块行矩阵和块列残差向量的乘积：\n$$\n\\nabla f(x) = A^{\\top} r = \\begin{bmatrix} A_1^{\\top}  \\cdots  A_p^{\\top} \\end{bmatrix} \\begin{bmatrix} r_1 \\\\ \\vdots \\\\ r_p \\end{bmatrix} = \\sum_{i=1}^{p} A_i^{\\top} r_i.\n$$\n代入 $r_i$ 的表达式，我们得到梯度的分布式计算公式：\n$$\n\\nabla f(x) = \\sum_{i=1}^{p} A_i^{\\top} (A_i x - b_i).\n$$\n梯度的计算流程如下：\n1.  将当前迭代值 $x$ 广播给所有 $p$ 个处理器。\n2.  每个处理器 $i$ 计算其局部残差 $r_i = A_i x - b_i$。\n3.  每个处理器 $i$ 计算其局部梯度贡献 $g_i = A_i^{\\top} r_i$，这是一个在 $\\mathbb{R}^n$ 中的向量。\n4.  通过对局部贡献求和获得全局梯度 $\\nabla f(x)$：$\\nabla f(x) = \\sum_{i=1}^{p} g_i$。此步骤需要在所有处理器间进行聚合（例如，归约求和操作）。\n\n### 2. 利普希茨常数的分布式估计\n\n如果步长 $\\tau$ 满足 $0  \\tau  2/L$，其中 $L$ 是 $\\nabla f$ 的利普希茨常数，则 ISTA 的收敛性得到保证。对于我们选择的 $f(x)$，该常数是 $A$ 的谱范数的平方，即 $L = \\lVert A \\rVert_2^2$。谱范数 $\\lVert A \\rVert_2$ 是 $A$ 的最大奇异值 $\\sigma_{\\max}(A)$。因此，$L = \\sigma_{\\max}(A)^2$。\n\n$A$ 的奇异值是自伴随矩阵 $A^{\\top}A$ 的特征值的平方根。因此，$L$ 是 $A^{\\top}A$ 的最大特征值，即 $L = \\lambda_{\\max}(A^{\\top}A)$。我们可以使用幂迭代法来近似这个主特征值。\n\n幂迭代算法生成一系列向量 $v_k$，该序列收敛于对应主特征值的特征向量。从一个随机单位向量 $v_0 \\in \\mathbb{R}^n$ 开始，迭代过程为：\n$$\nv_{k+1} = \\frac{(A^{\\top}A) v_k}{\\lVert (A^{\\top}A) v_k \\rVert_2}.\n$$\n相应的特征值估计为 $\\lambda_k = \\lVert (A^{\\top}A) v_k \\rVert_2$。此方法的核心是矩阵-向量乘积 $w = (A^{\\top}A) v$。我们可以用分布式的方式计算这个乘积，类似于梯度计算。令 $u = Av$，则 $w = A^{\\top}u$。分布式计算过程如下：\n1.  在每个处理器 $i$ 上计算 $u$ 的局部部分：$u_i = A_i v$。\n2.  在每个处理器 $i$ 上计算对 $w$ 的局部贡献：$w_i = A_i^{\\top} u_i$。\n3.  聚合得到最终结果：$w = \\sum_{i=1}^{p} w_i$。\n\n用于估计 $L$ 的分布式幂迭代算法如下：\n1.  初始化一个随机向量 $v \\in \\mathbb{R}^n$ 并将其归一化，$\\lVert v \\rVert_2 = 1$。\n2.  迭代固定次数：\n    a. 在每个处理器 $i$ 上，计算 $u_i = A_i v$。\n    b. 在每个处理器 $i$ 上，计算 $w_i = A_i^{\\top} u_i$。\n    c. 将结果聚合成 $w = \\sum_{i=1}^{p} w_i$。\n    d. 特征值估计为 $L_{\\text{est}} = \\lVert w \\rVert_2$。\n    e. 为下一次迭代归一化向量：$v = w / L_{\\text{est}}$。\n3.  经过足够多的迭代后，$L_{\\text{est}}$ 提供了对 $L = \\lambda_{\\max}(A^{\\top}A)$ 的良好近似。\n\n### 3. 迭代软阈值算法 (ISTA)\n\nISTA 是一种邻近梯度法。每次迭代包括对光滑部分 $f(x)$ 进行一个标准的梯度下降步，然后应用非光滑部分 $g(x)$ 的邻近算子。更新规则是：\n$$\nx^{k+1} = \\text{prox}_{\\tau g}(x^k - \\tau \\nabla f(x^k)).\n$$\n对于 $g(x) = \\lambda \\lVert x \\rVert_1$，$\\tau g(x) = (\\tau \\lambda) \\lVert x \\rVert_1$ 的邻近算子是软阈值算子，其分量形式定义为：\n$$\n[\\text{prox}_{\\tau g}(z)]_j = S_{\\tau \\lambda}(z_j) = \\text{sign}(z_j) \\max(|z_j| - \\tau \\lambda, 0).\n$$\n因此，完整的 ISTA 更新步骤是：\n$$\nx^{k+1} = S_{\\tau \\lambda}(x^k - \\tau \\nabla f(x^k)).\n$$\n在我们的分布式实现中，我们用 $x^0=0$ 初始化，估计 $L$ 来选择步长 $\\tau = \\alpha / L_{\\text{est}}$（按规定 $\\alpha=0.99$），然后迭代 $K$ 次，在每一步中使用第 1 节的分布式方法计算 $\\nabla f(x^k)$。\n\n### 4. 验证框架\n\n为验证分布式实现的正确性，我们还实现了一个中心化版本的算法。中心化版本可以访问完整的矩阵 $A$，并直接计算梯度 $\\nabla f(x) = A^{\\top}(Ax - b)$。分布式和中心化算法都使用相同的初始向量 $x^0=0$ 和相同的步长 $\\tau$（从分布式幂迭代估计 $L_{\\text{est}}$ 导出），运行固定的迭代次数 $K$。\n\n两种方法的数学等价性意味着，在精确算术下，结果将是相同的。在浮点算术中，可能会出现微小的差异。这些差异通过两个度量来量化：\n1.  相对梯度聚合误差 $\\varepsilon_{\\text{grad}}$，它比较两种方法在随机测试点 $x^{\\text{test}}$ 处计算的梯度：\n$$\n\\varepsilon_{\\text{grad}} = \\frac{\\lVert g_{\\text{dist}} - g_{\\text{cent}} \\rVert_2}{\\max\\{10^{-16}, \\lVert g_{\\text{cent}} \\rVert_2\\}}.\n$$\n2.  相对解差异 $\\varepsilon_{\\text{sol}}$，它比较 $K$ 步后的最终迭代结果：\n$$\n\\varepsilon_{\\text{sol}} = \\frac{\\lVert x^{K}_{\\text{dist}} - x^{K}_{\\text{cent}} \\rVert_2}{\\max\\{10^{-16}, \\lVert x^{K}_{\\text{cent}} \\rVert_2\\}}.\n$$\n这些误差度量预计会接近机器精度，从而证实分布式实现正确地复制了中心化算法。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and validates a distributed Iterative Soft-Thresholding Algorithm (ISTA).\n    \"\"\"\n\n    test_cases = [\n        # (n, m, p, s, lambda, sigma, K, seed)\n        (64, 256, 4, 8, 10**-2, 10**-3, 200, 42),\n        (64, 256, 1, 8, 10**-2, 10**-3, 200, 43),\n        (64, 256, 256, 8, 10**-2, 10**-3, 200, 44),\n    ]\n\n    results = []\n\n    def soft_threshold(z, T):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - T, 0)\n\n    for case in test_cases:\n        n, m, p, s, lam, sigma, K, seed = case\n\n        # 1. Generate synthetic data\n        np.random.seed(seed)\n        A = np.random.randn(m, n)\n        \n        x_star = np.zeros(n)\n        support = np.random.choice(n, s, replace=False)\n        x_star[support] = np.random.randn(s)\n        \n        noise = sigma * np.random.randn(m)\n        b = A @ x_star + noise\n\n        # 2. Create distributed operators (simulation)\n        row_indices = np.array_split(np.arange(m), p)\n        A_blocks = [A[indices, :] for indices in row_indices]\n        b_blocks = [b[indices] for indices in row_indices]\n\n        # 3. Estimate Lipschitz constant using distributed power iteration\n        def estimate_lipschitz_dist(A_bl, n_dim, num_iters=20):\n            v = np.random.randn(n_dim)\n            norm_v = np.linalg.norm(v)\n            if norm_v > 0:\n                v = v / norm_v\n            else: # Extremely unlikely case\n                v = np.zeros(n_dim)\n                v[0] = 1.0\n\n            L_est = 0.0\n            for _ in range(num_iters):\n                # Apply A^T * A to v in a distributed fashion\n                # u_i = A_i * v\n                u_blocks = [A_i @ v for A_i in A_bl]\n                # w = sum(A_i^T * u_i)\n                w = sum(A_bl[i].T @ u_blocks[i] for i in range(len(A_bl)))\n                \n                L_est = np.linalg.norm(w)\n                if L_est  1e-16: # Matrix is effectively null\n                    return 0.0\n                v = w / L_est\n            return L_est\n\n        L_est = estimate_lipschitz_dist(A_blocks, n)\n        tau = 0.99 / L_est if L_est > 0 else 1.0\n\n        # 4. Compute relative gradient aggregation error\n        x_test = np.random.randn(n)\n\n        # Centralized gradient\n        g_cent = A.T @ (A @ x_test - b)\n        \n        # Distributed gradient\n        g_dist = sum(A_blocks[i].T @ (A_blocks[i] @ x_test - b_blocks[i]) for i in range(p))\n\n        g_cent_norm = np.linalg.norm(g_cent)\n        eps_grad = np.linalg.norm(g_dist - g_cent) / max(1e-16, g_cent_norm)\n        results.append(eps_grad)\n\n        # 5. Run ISTA and compute relative solution discrepancy\n        \n        # Distributed ISTA\n        x_k_dist = np.zeros(n)\n        for _ in range(K):\n            grad_f_dist = sum(A_blocks[i].T @ (A_blocks[i] @ x_k_dist - b_blocks[i]) for i in range(p))\n            z_dist = x_k_dist - tau * grad_f_dist\n            x_k_dist = soft_threshold(z_dist, tau * lam)\n\n        # Centralized ISTA\n        x_k_cent = np.zeros(n)\n        for _ in range(K):\n            grad_f_cent = A.T @ (A @ x_k_cent - b)\n            z_cent = x_k_cent - tau * grad_f_cent\n            x_k_cent = soft_threshold(z_cent, tau * lam)\n            \n        x_k_cent_norm = np.linalg.norm(x_k_cent)\n        eps_sol = np.linalg.norm(x_k_dist - x_k_cent) / max(1e-16, x_k_cent_norm)\n        results.append(eps_sol)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3392991"}]}