{"hands_on_practices": [{"introduction": "为了有效管理偏差-方差权衡，我们首先必须能够量化它。本练习为此提供了一个通用的数学框架，利用强大的奇异值分解 (SVD) 工具来处理线性反问题。我们将推导均方误差，并将其分解为其组成部分：偏差的平方和方差，从而明确地揭示这种权衡关系。[@problem_id:3368363]", "problem": "考虑一个在有限维欧几里得空间中的线性逆问题，其数据模型为 $y = A x + \\varepsilon$，其中 $A$ 允许进行奇异值分解 (SVD) $A = U \\Sigma V^{\\top}$，其奇异值为 $\\{\\sigma_i\\}_{i=1}^r$，左奇异向量为 $\\{u_i\\}_{i=1}^r$，右奇异向量为 $\\{v_i\\}_{i=1}^r$。假设加性噪声 $\\varepsilon$ 是零均值、正态分布（高斯分布）、独立同分布 (i.i.d.)，其协方差为 $\\sigma^2 I$，其中 $\\sigma^2 > 0$ 是已知的。考虑一个形式如下的谱滤波器正则化估计量\n$$\n\\hat{x}_{\\alpha} \\;=\\; \\sum_{i=1}^r \\frac{f_i(\\alpha)}{\\sigma_i} \\,\\langle y, u_i \\rangle \\, v_i,\n$$\n其中 $\\alpha > 0$ 是正则化参数，$\\{f_i(\\alpha)\\}_{i=1}^r$ 是将 $[0,\\infty)$ 映射到 $[0,1]$ 的滤波函数。\n\n您的任务是：\n\n1. 仅从数据模型、奇异向量的正交规范性以及期望的线性性出发，推导均方误差 (MSE) 的闭式表达式。MSE 定义为以噪声为条件的期望 $\\mathbb{E}_{\\varepsilon}\\|\\hat{x}_{\\alpha} - x\\|^2$，表达式应以 $\\{f_i(\\alpha)\\}$、$\\{\\sigma_i\\}$、系数 $\\{\\langle x, v_i\\rangle\\}$ 和 $\\sigma^2$ 来表示。\n\n2. 特化到吉洪诺夫 (Tikhonov) 正则化，其中对每个 $i$ 都有 $f_i(\\alpha) = \\sigma_i^2/(\\sigma_i^2 + \\alpha)$。使用您在第1部分中得到的表达式，定性地解释增加 $\\alpha$ 如何改变 MSE 中通常被认为是偏差平方和方差的贡献。\n\n3. 在一维情况 $r=1$ 下，奇异值为 $\\sigma_1 = s > 0$，真实系数的大小为 $a = |\\langle x, v_1\\rangle|$，假设如第2部分所述的吉洪诺夫正则化。计算使 $\\mathbb{E}_{\\varepsilon}\\|\\hat{x}_{\\alpha} - x\\|^2$ 在 $\\alpha > 0$ 上最小化的值 $\\alpha^{\\star}$。将您的最终答案表示为关于 $a$ 和 $\\sigma$ 的闭式解析表达式。无需四舍五入，也无需报告物理单位。", "solution": "该问题陈述是正则化线性逆问题领域的标准表述，在假设最后一部分中的信号分量非零的合理条件下，该问题在数学上是合理的、自洽的且适定的。该问题是有效的。\n\n### 第1部分：均方误差 (MSE) 的推导\n\n均方误差 (MSE) 定义为误差向量平方范数的期望，以噪声实现 $\\varepsilon$ 为条件：\n$$\n\\text{MSE} \\;=\\; \\mathbb{E}_{\\varepsilon}\\left[\\|\\hat{x}_{\\alpha} - x\\|^2\\right]\n$$\n$x$ 的向量空间可以分解为 $A$ 的零空间的正交补空间（由右奇异向量 $\\{v_i\\}_{i=1}^r$ 张成）和 $A$ 的零空间（由 $\\{v_j\\}_{j=r+1}^n$ 张成）。因此，我们可以将真实解 $x$ 写为：\n$$\nx \\;=\\; \\sum_{i=1}^r \\langle x, v_i \\rangle v_i + \\sum_{j=r+1}^n \\langle x, v_j \\rangle v_j\n$$\n估计量 $\\hat{x}_{\\alpha}$ 被构造为 $\\{v_i\\}_{i=1}^r$ 的线性组合，因此完全位于这些向量张成的子空间内。误差向量 $\\hat{x}_{\\alpha} - x$ 可以被分解为两个正交分量：\n$$\n\\hat{x}_{\\alpha} - x \\;=\\; \\left(\\hat{x}_{\\alpha} - \\sum_{i=1}^r \\langle x, v_i \\rangle v_i\\right) - \\left(\\sum_{j=r+1}^n \\langle x, v_j \\rangle v_j\\right)\n$$\n由于这两个分量的正交性，根据勾股定理，平方范数是它们各自平方范数的和：\n$$\n\\|\\hat{x}_{\\alpha} - x\\|^2 \\;=\\; \\left\\|\\hat{x}_{\\alpha} - \\sum_{i=1}^r \\langle x, v_i \\rangle v_i\\right\\|^2 + \\left\\|\\sum_{j=r+1}^n \\langle x, v_j \\rangle v_j\\right\\|^2\n$$\n将估计量 $\\hat{x}_{\\alpha}$ 的定义代入第一项，并利用基向量 $\\{v_i\\}_{i=1}^r$ 的正交规范性：\n$$\n\\left\\|\\sum_{i=1}^r \\frac{f_i(\\alpha)}{\\sigma_i} \\langle y, u_i \\rangle v_i - \\sum_{i=1}^r \\langle x, v_i \\rangle v_i\\right\\|^2 \\;=\\; \\left\\|\\sum_{i=1}^r \\left(\\frac{f_i(\\alpha)}{\\sigma_i} \\langle y, u_i \\rangle - \\langle x, v_i \\rangle\\right) v_i\\right\\|^2 \\;=\\; \\sum_{i=1}^r \\left(\\frac{f_i(\\alpha)}{\\sigma_i} \\langle y, u_i \\rangle - \\langle x, v_i \\rangle\\right)^2\n$$\n第二项是 $x$ 在 $A$ 的零空间中分量的平方范数，这是一个不可约减的误差分量，因为估计量无法访问这个子空间。该项相对于噪声 $\\varepsilon$ 是常数：\n$$\n\\left\\|\\sum_{j=r+1}^n \\langle x, v_j \\rangle v_j\\right\\|^2 \\;=\\; \\sum_{j=r+1}^n \\langle x, v_j \\rangle^2\n$$\n现在我们将数据模型 $y = Ax + \\varepsilon$ 代入项 $\\langle y, u_i \\rangle$ 中：\n$$\n\\langle y, u_i \\rangle \\;=\\; \\langle Ax + \\varepsilon, u_i \\rangle \\;=\\; \\langle Ax, u_i \\rangle + \\langle \\varepsilon, u_i \\rangle\n$$\n利用SVD的性质 $Av_k = \\sigma_k u_k$，因此 $A (\\sum_k \\langle x, v_k \\rangle v_k) = \\sum_k \\langle x, v_k \\rangle \\sigma_k u_k$，我们得到：\n$$\n\\langle Ax, u_i \\rangle \\;=\\; \\left\\langle \\sum_{k=1}^r \\langle x, v_k \\rangle \\sigma_k u_k, u_i \\right\\rangle \\;=\\; \\sum_{k=1}^r \\langle x, v_k \\rangle \\sigma_k \\langle u_k, u_i \\rangle \\;=\\; \\sigma_i \\langle x, v_i \\rangle\n$$\n所以，$\\langle y, u_i \\rangle = \\sigma_i \\langle x, v_i \\rangle + \\langle \\varepsilon, u_i \\rangle$。将其代回：\n$$\n\\frac{f_i(\\alpha)}{\\sigma_i} \\langle y, u_i \\rangle - \\langle x, v_i \\rangle \\;=\\; \\frac{f_i(\\alpha)}{\\sigma_i} (\\sigma_i \\langle x, v_i \\rangle + \\langle \\varepsilon, u_i \\rangle) - \\langle x, v_i \\rangle \\;=\\; (f_i(\\alpha)-1) \\langle x, v_i \\rangle + \\frac{f_i(\\alpha)}{\\sigma_i} \\langle \\varepsilon, u_i \\rangle\n$$\n平方和变为：\n$$\n\\sum_{i=1}^r \\left( (f_i(\\alpha)-1) \\langle x, v_i \\rangle + \\frac{f_i(\\alpha)}{\\sigma_i} \\langle \\varepsilon, u_i \\rangle \\right)^2\n$$\n现在我们取期望 $\\mathbb{E}_{\\varepsilon}$。噪声 $\\varepsilon$ 的均值为零，所以 $\\mathbb{E}_{\\varepsilon}[\\varepsilon] = 0$。这意味着 $\\mathbb{E}_{\\varepsilon}[\\langle \\varepsilon, u_i \\rangle] = \\langle \\mathbb{E}_{\\varepsilon}[\\varepsilon], u_i \\rangle = 0$。因此，平方展开中的交叉项在取期望后消失。我们剩下：\n$$\n\\mathbb{E}_{\\varepsilon}\\left[\\sum_{i=1}^r \\left( (f_i(\\alpha)-1)^2 \\langle x, v_i \\rangle^2 + \\left(\\frac{f_i(\\alpha)}{\\sigma_i}\\right)^2 \\langle \\varepsilon, u_i \\rangle^2 \\right)\\right] = \\sum_{i=1}^r \\left( (f_i(\\alpha)-1)^2 \\langle x, v_i \\rangle^2 + \\left(\\frac{f_i(\\alpha)}{\\sigma_i}\\right)^2 \\mathbb{E}_{\\varepsilon}[\\langle \\varepsilon, u_i \\rangle^2] \\right)\n$$\n项 $\\mathbb{E}_{\\varepsilon}[\\langle \\varepsilon, u_i \\rangle^2]$ 是随机变量 $\\langle \\varepsilon, u_i \\rangle = u_i^\\top\\varepsilon$ 的方差。已知 $\\text{Cov}(\\varepsilon) = \\sigma^2 I$，则方差为：\n$$\n\\mathbb{E}_{\\varepsilon}[\\langle \\varepsilon, u_i \\rangle^2] \\;=\\; \\text{Var}(u_i^\\top\\varepsilon) \\;=\\; u_i^\\top \\text{Cov}(\\varepsilon) u_i \\;=\\; u_i^\\top (\\sigma^2 I) u_i \\;=\\; \\sigma^2 (u_i^\\top u_i) \\;=\\; \\sigma^2\n$$\n因为 $u_i$ 是一个单位向量。\n综合所有部分，MSE为：\n$$\n\\text{MSE} \\;=\\; \\sum_{i=1}^r \\left( (f_i(\\alpha)-1)^2 \\langle x, v_i \\rangle^2 + \\frac{f_i(\\alpha)^2 \\sigma^2}{\\sigma_i^2} \\right) + \\sum_{j=r+1}^n \\langle x, v_j \\rangle^2\n$$\n\n### 第2部分：吉洪诺夫正则化的解释\n\n对于吉洪诺夫正则化，滤波函数为 $f_i(\\alpha) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha}$。MSE包含三个部分：偏差平方、方差和不可约减的零空间误差。正则化参数 $\\alpha$ 只影响前两部分。\n\n偏差平方是求和中的第一项：\n$$\n\\text{Bias}^2(\\alpha) \\;=\\; \\sum_{i=1}^r (f_i(\\alpha)-1)^2 \\langle x, v_i \\rangle^2 \\;=\\; \\sum_{i=1}^r \\left(\\frac{\\sigma_i^2}{\\sigma_i^2+\\alpha}-1\\right)^2 \\langle x, v_i \\rangle^2 \\;=\\; \\sum_{i=1}^r \\left(\\frac{-\\alpha}{\\sigma_i^2+\\alpha}\\right)^2 \\langle x, v_i \\rangle^2 \\;=\\; \\sum_{i=1}^r \\frac{\\alpha^2}{(\\sigma_i^2+\\alpha)^2} \\langle x, v_i \\rangle^2\n$$\n方差是第二项：\n$$\n\\text{Var}(\\alpha) \\;=\\; \\sum_{i=1}^r \\frac{f_i(\\alpha)^2 \\sigma^2}{\\sigma_i^2} \\;=\\; \\sum_{i=1}^r \\left(\\frac{\\sigma_i^2}{\\sigma_i^2+\\alpha}\\right)^2 \\frac{\\sigma^2}{\\sigma_i^2} \\;=\\; \\sum_{i=1}^r \\frac{\\sigma_i^2}{(\\sigma_i^2+\\alpha)^2} \\sigma^2\n$$\n分析增加 $\\alpha > 0$ 的影响：\n- **偏差平方**：对每个分量 $i$，项 $\\frac{\\alpha}{\\sigma_i^2+\\alpha}$ 是 $\\alpha$ 的单调递增函数。当 $\\alpha \\to 0^+$ 时，该项趋近于 $0$。当 $\\alpha \\to \\infty$ 时，该项趋近于 $1$。因此，总偏差平方 $\\text{Bias}^2(\\alpha)$ 是 $\\alpha$ 的单调递增函数。增加 $\\alpha$ 会加强正则化，将估计值向 $0$ 收缩。这会引入一个离真实解更大的系统性偏差（bias）。\n- **方差**：对每个分量 $i$，项 $\\frac{1}{(\\sigma_i^2+\\alpha)^2}$ 是 $\\alpha$ 的单调递减函数。当 $\\alpha \\to 0^+$ 时，该项为 $\\frac{1}{(\\sigma_i^2)^2}$，对于小的 $\\sigma_i$，方差项 $(\\sigma^2/\\sigma_i^2)$ 可能很大。当 $\\alpha \\to \\infty$ 时，该项趋近于 $0$。因此，总方差 $\\text{Var}(\\alpha)$ 是 $\\alpha$ 的单调递减函数。增加 $\\alpha$ 会抑制噪声的放大，特别是对于与小奇异值相关的分量，从而减少估计量的总方差。\n\n总之，增加正则化参数 $\\alpha$ 会增加偏差的平方，同时减少方差。这是正则化中经典的偏差-方差权衡。\n\n### 第3部分：一维情况下的最优 $\\alpha$\n\n在一维情况下，我们有 $r=1$，$\\sigma_1 = s > 0$，以及 $|\\langle x, v_1\\rangle| = a$。令 $c = \\langle x, v_1\\rangle$，所以 $c^2 = a^2$。需要最小化的 MSE，作为 $\\alpha$ 的函数，是偏差项和方差项之和（零空间误差是常数，不影响最小值的位置）：\n$$\nJ(\\alpha) \\;=\\; \\frac{\\alpha^2}{(s^2+\\alpha)^2} c^2 + \\frac{s^2}{(s^2+\\alpha)^2} \\sigma^2 \\;=\\; \\frac{\\alpha^2 c^2 + s^2 \\sigma^2}{(s^2+\\alpha)^2}\n$$\n为了找到使 $J(\\alpha)$ 在 $\\alpha > 0$ 上最小化的值 $\\alpha^{\\star}$，我们计算关于 $\\alpha$ 的导数并将其设为零。我们假设 $a > 0$，这意味着 $c \\neq 0$。如果 $a=0$，偏差项消失，$J(\\alpha)$ 成为 $\\alpha$ 的严格递减函数，意味着在 $(0, \\infty)$ 中不存在最小值。\n使用除法法则求导：\n$$\n\\frac{dJ}{d\\alpha} \\;=\\; \\frac{(2\\alpha c^2)(s^2+\\alpha)^2 - (\\alpha^2 c^2 + s^2 \\sigma^2)(2(s^2+\\alpha))}{(s^2+\\alpha)^4}\n$$\n将分子设为零，并除以非零因子 $2(s^2+\\alpha)$（因为 $s>0, \\alpha>0$）：\n$$\n(2\\alpha c^2)\\frac{1}{2}(s^2+\\alpha) - (\\alpha^2 c^2 + s^2 \\sigma^2) \\;=\\; 0\n$$\n$$\n\\alpha c^2 (s^2+\\alpha) - (\\alpha^2 c^2 + s^2 \\sigma^2) \\;=\\; 0\n$$\n$$\n\\alpha s^2 c^2 + \\alpha^2 c^2 - \\alpha^2 c^2 - s^2 \\sigma^2 \\;=\\; 0\n$$\n$$\n\\alpha s^2 c^2 - s^2 \\sigma^2 \\;=\\; 0\n$$\n因为 $s > 0$，我们可以除以 $s^2$：\n$$\n\\alpha c^2 = \\sigma^2\n$$\n解出 $\\alpha$，我们得到：\n$$\n\\alpha^{\\star} \\;=\\; \\frac{\\sigma^2}{c^2}\n$$\n代入 $c^2=a^2$，最优正则化参数为：\n$$\n\\alpha^{\\star} \\;=\\; \\frac{\\sigma^2}{a^2}\n$$\n二阶导数检验可以确认这是一个最小值。一阶导数的符号在这一点附近从负变正，也确认了它是一个最小值。", "answer": "$$\\boxed{\\frac{\\sigma^2}{a^2}}$$", "id": "3368363"}, {"introduction": "在推导出偏差和方差的表达式之后，下一步自然是利用这一理解来选择最优的正则化参数值。本练习探讨了在截断奇异值分解 (TSVD) 正则化中选择截断水平 $k$ 的两种著名方法：启发式的 Morozov 差异原理和基于统计的 Stein 无偏风险估计 (SURE)。通过比较这些方法，我们将学习如何根据数据本身做出有原则的选择，以平衡偏差-方差的权衡。[@problem_id:3368364]", "problem": "考虑数据同化中的线性逆问题，其观测模型为 $y = A x_{\\star} + \\varepsilon$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个已知的秩为 $r \\leq \\min\\{m,n\\}$ 的正演算子，$x_{\\star} \\in \\mathbb{R}^{n}$ 是未知的真实状态，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$ 是加性高斯噪声，其噪声方差 $\\sigma^{2} > 0$ 已知。设 $A$ 的奇异值分解为 $A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是标准正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 的对角线上有奇异值 $s_{1} \\geq s_{2} \\geq \\cdots \\geq s_{r} > 0$。定义截断水平为 $k \\in \\{0,1,\\dots,r\\}$ 的截断奇异值分解估计量为\n$$\nx^{(k)} \\;=\\; \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} y}{s_{i}} \\, v_{i}, \\quad \\text{及预测数据} \\quad \\hat{y}^{(k)} \\;=\\; A x^{(k)} \\;=\\; \\sum_{i=1}^{k} u_{i} u_{i}^{\\top} y,\n$$\n其中 $u_{i}$ 和 $v_{i}$ 是 $A$ 的左右奇异向量。将张成空间 $\\{u_{1},\\dots,u_{k}\\}$ 上的正交投影算子记为 $S_{k} := \\sum_{i=1}^{k} u_{i} u_{i}^{\\top}$，无噪声数据记为 $y_{\\star} := A x_{\\star} = \\sum_{i=1}^{r} u_{i} u_{i}^{\\top} y_{\\star}$。\n\n从以上定义和高斯噪声下期望平方误差的偏差-方差分解这一基本统计原理出发，推导：\n\n1. Morozov型差异原理停止准则 $k(\\sigma)$，该准则选择最小的 $k$ 使得数据残差 $\\|y - \\hat{y}^{(k)}\\|$ 与已知噪声水平一致。使用一个乘以期望噪声范数的容忍因子 $\\tau \\geq 1$ 来表示您的准则。您的推导必须从 $S_{k}$ 和残差 $r^{(k)} := y - \\hat{y}^{(k)} = (I - S_{k}) y$ 的定义出发，并且必须证明为何该停止准则能使残差与噪声水平对齐。\n\n2. 在数据空间中，使用针对高斯噪声的Stein无偏风险估计 (SURE)，为线性估计量 $\\hat{y}^{(k)} = S_{k} y$ 推导一个易于处理的SURE目标函数的表达式，并求出在 $\\{0,1,\\dots,r\\}$ 上最小化SURE的 $k$。将该最小化子用观测系数 $u_{i}^{\\top} y$ 和 $\\sigma^{2}$ 显式地表示出来。\n\n最后，将差异原理准则 $k(\\sigma)$ 和SURE最小化准则（关于 $k$）这两个解析表达式，写成仅由上述定义的量构成的、关于 $(U,y,\\sigma,\\tau,m,r)$ 的函数。最终答案必须以一对闭式解析表达式（而非不等式）的形式，使用LaTeX的 $\\mathrm{pmatrix}$ 环境在一个单行矩阵中给出，无需任何文字解释。不需要四舍五入。不涉及单位。", "solution": "该问题陈述已经过验证，被认为是有效的。它具有科学依据，是适定的、客观的，为一个逆问题的正则化理论中的标准问题提供了清晰而完整的设定。\n\n我们现在推导所要求的两个参数选择准则。该问题设定在线性逆问题 $y = A x_{\\star} + \\varepsilon$ 的背景下，其中 $y \\in \\mathbb{R}^{m}$ 是观测值，$A \\in \\mathbb{R}^{m \\times n}$ 是正演算子，$x_{\\star} \\in \\mathbb{R}^{n}$ 是待恢复的真实状态，$\\varepsilon \\in \\mathbb{R}^{m}$ 是一个噪声向量，其分量服从均值为 $0$、方差为 $\\sigma^{2}$ 的正态分布，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$。$x_{\\star}$ 的截断奇异值分解 (TSVD) 估计量由 $x^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} y}{s_{i}} v_{i}$ 给出，这导出预测数据 $\\hat{y}^{(k)} = A x^{(k)}$。根据所提供的定义 $\\hat{y}^{(k)} = S_k y$，其中 $S_k = \\sum_{i=1}^{k} u_{i} u_{i}^{\\top}$ 是到前 $k$ 个左奇异向量 $\\{u_1, \\dots, u_k\\}$ 的张成空间上的正交投影算子。\n\n### 1. Morozov型差异原理\n\nMorozov差异原理是一种选择正则化参数（在此例中为截断水平 $k$）的启发式方法，它要求模型的预测残差与数据中的噪声水平保持一致。对于给定的 $k$，残差为 $r^{(k)} = y - \\hat{y}^{(k)} = y - S_k y = (I_m - S_k)y$。\n\n其核心思想是选择一个参数 $k$，使得残差的范数与噪声的期望范数相当。如果残差大得多，则模型欠拟合（偏差高）。如果残差小得多，则模型过拟合（拟合了噪声）。\n\n噪声向量 $\\varepsilon$ 的协方差为 $\\sigma^2 I_m$。噪声的期望平方范数为：\n$$\nE\\left[\\|\\varepsilon\\|^2\\right] = E\\left[\\varepsilon^{\\top}\\varepsilon\\right] = \\text{Tr}\\left(E\\left[\\varepsilon\\varepsilon^{\\top}\\right]\\right) = \\text{Tr}\\left(\\text{Cov}(\\varepsilon)\\right) = \\text{Tr}(\\sigma^2 I_m) = m \\sigma^2\n$$\n因此，噪声的均方根 (RMS) 幅值为 $\\sqrt{m \\sigma^2} = \\sqrt{m}\\sigma$。差异原理准则的构建方式是，选择参数 $k$ 使得残差范数 $\\|r^{(k)}\\|$ 与此噪声水平相平衡，通常通过满足 $\\|r^{(k)}\\| \\leq \\tau \\sqrt{m}\\sigma$ 来实现，其中 $\\tau \\geq 1$ 是一个容忍因子。将其平方，得到关于残差平方范数的条件：\n$$\n\\|r^{(k)}\\|^2 \\leq \\tau^2 m \\sigma^2\n$$\n残差平方范数可以用数据 $y$ 和投影算子 $S_k$ 来表示。由于 $(I_m - S_k)$ 也是一个正交投影算子，我们有 $(I_m-S_k)^2 = (I_m-S_k)$：\n$$\n\\|r^{(k)}\\|^2 = \\|(I_m - S_k)y\\|^2 = y^{\\top}(I_m - S_k)^{\\top}(I_m - S_k)y = y^{\\top}(I_m - S_k)y\n$$\n将 $y$ 在左奇异向量的标准正交基 $\\{u_i\\}_{i=1}^m$ 中展开，我们有 $y = \\sum_{i=1}^m (u_i^{\\top} y) u_i$。应用投影算子 $(I_m - S_k)$：\n$$\n(I_m - S_k)y = \\left(I_m - \\sum_{i=1}^k u_i u_i^{\\top}\\right) \\left(\\sum_{j=1}^m (u_j^{\\top} y) u_j\\right) = \\sum_{j=k+1}^m (u_j^{\\top} y) u_j\n$$\n因此，平方范数是指数大于 $k$ 的系数的平方和：\n$$\n\\|r^{(k)}\\|^2 = \\left\\| \\sum_{i=k+1}^m (u_i^{\\top} y) u_i \\right\\|^2 = \\sum_{i=k+1}^m (u_i^{\\top} y)^2\n$$\n量 $\\sum_{i=k+1}^m (u_i^{\\top} y)^2$ 是关于 $k$ 的单调非增函数。较大的 $k$ 意味着求和项数更少，因此残差更小或相等。差异原理旨在找到与数据一致的最简单模型（最小的 $k$）。这转化为寻找满足条件的最小 $k$。因此，准则是将 $k(\\sigma)$ 选为集合 $\\{0, 1, \\dots, r\\}$ 中满足条件的最小整数：\n$$\nk(\\sigma) = \\min \\left\\{ k \\in \\{0, 1, \\dots, r\\} \\mid \\sum_{i=k+1}^m (u_i^\\top y)^2 \\le \\tau^2 m \\sigma^2 \\right\\}\n$$\n该准则将残差与噪声水平对齐，因为对于一个合适的 $k$，真实信号 $y_{\\star} = A x_{\\star}$ 的主要部分被投影 $S_k y_{\\star}$ 捕获。残差 $r^{(k)} = (I-S_k)y_\\star + (I-S_k)\\varepsilon$ 随后由噪声分量 $(I-S_k)\\varepsilon$ 主导，其期望平方范数为 $E[\\|(I-S_k)\\varepsilon\\|^2] = \\text{Tr}((I-S_k)\\sigma^2 I_m) = (m-k)\\sigma^2$。该原理将*观测到*的残差范数 $\\|r^{(k)}\\|^2$ 与总的期望噪声能量 $m\\sigma^2$ 进行比较，当残差“足够小”时停止。\n\n### 2. Stein无偏风险估计 (SURE)\n\n对于在加性高斯噪声下观测到的真实信号 $y_\\star$ 的一个估计量 $\\hat{y}$，SURE 提供了对均方误差 (MSE) 或风险 $E[\\|\\hat{y} - y_\\star\\|^2]$ 的一个无偏估计。对于形式为 $\\hat{y} = L y$ 的线性估计量，其中 $y = y_\\star + \\varepsilon$ 且 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$，SURE公式为：\n$$\n\\text{SURE}(y; L) = \\|L y - y\\|^2 - m \\sigma^2 + 2 \\sigma^2 \\text{div}_y(Ly)\n$$\n在我们的问题中，数据的估计量为 $\\hat{y}^{(k)} = S_k y$，因此线性算子为 $L = S_k$。我们需要计算该算子的各项。\n\n第一项是残差的平方范数：\n$$\n\\|S_k y - y\\|^2 = \\|-(I_m - S_k)y\\|^2 = \\|r^{(k)}\\|^2 = \\sum_{i=k+1}^m (u_i^\\top y)^2\n$$\n第二项是 $-m\\sigma^2$。\n\n第三项涉及映射 $y \\mapsto S_k y$ 的散度。对于一个线性映射 $Ly$，其散度是矩阵表示的迹：\n$$\n\\text{div}_y(S_k y) = \\text{Tr}(S_k)\n$$\n投影算子 $S_k$ 定义为 $S_k = \\sum_{i=1}^k u_i u_i^{\\top}$。它的迹是：\n$$\n\\text{Tr}(S_k) = \\text{Tr}\\left(\\sum_{i=1}^k u_i u_i^{\\top}\\right) = \\sum_{i=1}^k \\text{Tr}(u_i u_i^{\\top})\n$$\n利用迹的循环性质 $\\text{Tr}(AB) = \\text{Tr}(BA)$：\n$$\n\\text{Tr}(u_i u_i^{\\top}) = \\text{Tr}(u_i^{\\top} u_i) = \\text{Tr}(\\|u_i\\|^2)\n$$\n由于奇异向量 $u_i$ 是标准正交的，$\\|u_i\\|^2 = 1$。标量的迹就是其本身。所以，$\\text{Tr}(u_i u_i^{\\top}) = 1$。\n$$\n\\text{Tr}(S_k) = \\sum_{i=1}^k 1 = k\n$$\n将这些分量代入SURE公式，得到给定 $k$ 的目标函数：\n$$\n\\text{SURE}(k) = \\|(I_m - S_k)y\\|^2 - m \\sigma^2 + 2 \\sigma^2 k\n$$\n使用以数据系数表示的残差范数表达式：\n$$\n\\text{SURE}(k) = \\sum_{i=k+1}^m (u_i^\\top y)^2 - m \\sigma^2 + 2 k \\sigma^2\n$$\n为了找到最优的 $k$，我们必须在允许的范围 $k \\in \\{0, 1, \\dots, r\\}$ 上最小化此函数。项 $-m\\sigma^2$ 相对于 $k$ 是常数，在最小化过程中可以忽略。因此，我们寻求找到：\n$$\nk_{\\text{SURE}} = \\arg\\min_{k \\in \\{0, 1, \\dots, r\\}} \\left( \\sum_{i=k+1}^m (u_i^\\top y)^2 + 2k\\sigma^2 \\right)\n$$\n这个表达式完美地展示了偏差-方差权衡。第一项 $\\sum_{i=k+1}^m (u_i^\\top y)^2$ 是平方残差，它与估计的偏差有关，并随着 $k$ 的增加而减小。第二项 $2k\\sigma^2$ 是对模型复杂度（自由度）的惩罚，它与估计的方差有关，并随着 $k$ 的增加而增加。SURE准则找到了能够最佳平衡这两个相互竞争影响的 $k$ 值。\n\n推导出的这两个准则的表达式现在可以以最终格式呈现。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\min \\left\\{ k \\in \\{0, 1, \\dots, r\\} \\mid \\sum_{i=k+1}^m (u_i^\\top y)^2 \\le \\tau^2 m \\sigma^2 \\right\\} & \\arg\\min_{k \\in \\{0, 1, \\dots, r\\}} \\left( \\sum_{i=k+1}^m (u_i^\\top y)^2 + 2k\\sigma^2 \\right) \\end{pmatrix}}\n$$", "id": "3368364"}, {"introduction": "偏差-方差权衡不仅限于静态反问题，它也是动力系统中数据同化的核心概念。本练习在一个简单的状态空间模型中展示了这一原理，该模型是卡尔曼滤波和 4D-Var 等方法的基础。在这里，动力学模型本身充当了一种正则化形式，我们将量化我们对模型的置信度（由模型误差方差 $Q$ 表示）如何直接控制状态估计中的偏差和方差之间的平衡。[@problem_id:3368359]", "problem": "考虑一个在两步窗口 $t \\in \\{0,1\\}$ 上的标量线性状态空间模型，其动态方程为 $x_{1}=\\rho x_{0}+w_{0}$，观测方程为 $y_{t}=x_{t}+\\varepsilon_{t}$。假设 $w_{0}$ 服从于方差为 $Q$ 的零均值高斯分布，$\\varepsilon_{t}$ 是方差为 $R$ 的独立零均值高斯随机变量，且所有这些变量相互独立。设 $x_{0}$ 的先验为不当平坦先验，这可以理解为方差趋于无穷大的高斯先验的极限。在这些假设下，定义 $(x_{0},x_{1})$ 的最大后验 (MAP) 估计器。然后，将真实状态 $x_{0}$ 和 $x_{1}$ 视为固定的（未知）常数，并仅对测量噪声 $\\varepsilon_{0}$ 和 $\\varepsilon_{1}$ 求平均，推导 $x_{1}$ 的 MAP 估计器的偏差、$x_{1}$ 的 MAP 估计器的方差，并将它们组合起来得到时间 $t=1$ 时的均方误差，定义为 $\\mathbb{E}\\big[(\\hat{x}_{1}-x_{1})^{2}\\big]$，作为 $Q$、$R$、$\\rho$、$x_{0}$ 和 $x_{1}$ 的显式解析函数。请以均方误差的单一闭式解析表达式的精确形式报告你的最终答案。无需单位。如果进行任何数值简化，请不要四舍五入。", "solution": "该模型是线性高斯模型。由于 $x_{0}$ 具有不当平坦先验且噪声为独立高斯噪声，$(x_{0},x_{1})$ 的后验密度与三个高斯似然的乘积成正比：一个是在给定 $x_{0}$ 下的 $y_{0}$ 的似然，一个是在给定 $x_{1}$ 下的 $y_{1}$ 的似然，还有一个是在给定 $x_{0}$ 下的 $x_{1}$ 的似然。因此，负对数后验（在相差一个加性常数的情况下）是加权残差平方和\n$$\nJ(x_{0},x_{1}) \\;=\\; \\frac{1}{R}\\,(y_{0}-x_{0})^{2} \\;+\\; \\frac{1}{R}\\,(y_{1}-x_{1})^{2} \\;+\\; \\frac{1}{Q}\\,(x_{1}-\\rho x_{0})^{2}.\n$$\n最大后验 (MAP) 估计 $(\\hat{x}_{0},\\hat{x}_{1})$ 最小化 $J(x_{0},x_{1})$。这是一个严格凸二次函数，因此其最小值点可以通过将梯度设为零来获得：\n$$\n\\frac{\\partial J}{\\partial x_{0}} \\;=\\; -\\frac{2}{R}(y_{0}-x_{0}) \\;-\\; \\frac{2\\rho}{Q}(x_{1}-\\rho x_{0}) \\;=\\; 0,\n$$\n$$\n\\frac{\\partial J}{\\partial x_{1}} \\;=\\; -\\frac{2}{R}(y_{1}-x_{1}) \\;+\\; \\frac{2}{Q}(x_{1}-\\rho x_{0}) \\;=\\; 0.\n$$\n重新整理这两个方程得到一个关于 $(x_{0},x_{1})$ 的线性系统。定义 $A=\\frac{1}{R}$ 和 $B=\\frac{1}{Q}$ 以简化符号。那么正规方程可以写成\n$$\n\\begin{pmatrix}\nA + \\rho^{2}B & -\\rho B \\\\\n-\\rho B & A + B\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{0} \\\\\nx_{1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nA y_{0} \\\\\nA y_{1}\n\\end{pmatrix}.\n$$\n令 $M$ 为上述 $2\\times 2$ 矩阵，$b$ 为右侧向量。$M$ 的行列式为\n$$\n\\det(M) \\;=\\; (A + \\rho^{2}B)(A + B) - (\\rho B)^{2} \\;=\\; A^{2} + A B (1 + \\rho^{2}) \\;=\\; A\\big(A + B(1+\\rho^{2})\\big).\n$$\n$M$ 的逆矩阵为\n$$\nM^{-1} \\;=\\; \\frac{1}{\\det(M)} \\begin{pmatrix}\nA + B & \\rho B \\\\\n\\rho B & A + \\rho^{2}B\n\\end{pmatrix}.\n$$\n因此，MAP 估计为\n$$\n\\begin{pmatrix}\n\\hat{x}_{0} \\\\\n\\hat{x}_{1}\n\\end{pmatrix}\n\\;=\\;\nM^{-1} b\n\\;=\\;\n\\frac{1}{\\det(M)}\n\\begin{pmatrix}\n(A + B)A y_{0} + \\rho B A y_{1} \\\\\n\\rho B A y_{0} + (A + \\rho^{2}B) A y_{1}\n\\end{pmatrix}.\n$$\n特别地，\n$$\n\\hat{x}_{1} \\;=\\; \\alpha\\, y_{0} + \\beta\\, y_{1},\n\\quad\\text{其中}\\quad\n\\alpha \\;=\\; \\frac{\\rho A B}{\\det(M)}, \\qquad \\beta \\;=\\; \\frac{A\\left(A + \\rho^{2}B\\right)}{\\det(M)}.\n$$\n我们现在通过将真实状态 $x_{0}$ 和 $x_{1}$ 视为固定常数，并对独立的测量噪声 $\\varepsilon_{0}$ 和 $\\varepsilon_{1}$ 求平均，来量化 $\\hat{x}_{1}$ 的偏差-方差权衡。根据定义，\n$$\ny_{0} \\;=\\; x_{0} + \\varepsilon_{0}, \\qquad y_{1} \\;=\\; x_{1} + \\varepsilon_{1}, \\qquad \\mathbb{E}[\\varepsilon_{t}] \\;=\\; 0, \\qquad \\operatorname{Var}(\\varepsilon_{t}) \\;=\\; R, \\qquad \\operatorname{Cov}(\\varepsilon_{0},\\varepsilon_{1}) \\;=\\; 0.\n$$\n偏差是\n$$\n\\operatorname{Bias}(\\hat{x}_{1}) \\;=\\; \\mathbb{E}[\\hat{x}_{1}] - x_{1}\n\\;=\\; \\alpha\\, x_{0} + \\beta\\, x_{1} - x_{1}\n\\;=\\; \\alpha\\, x_{0} + (\\beta - 1)\\, x_{1}.\n$$\n使用 $\\det(M)=A^{2}+A B(1+\\rho^{2})$ 以及 $\\alpha$ 和 $\\beta$ 的公式，\n$$\n\\beta - 1 \\;=\\; \\frac{A(A+\\rho^{2}B)}{\\det(M)} - 1 \\;=\\; \\frac{A(A+\\rho^{2}B) - \\det(M)}{\\det(M)} \\;=\\; \\frac{-A B}{\\det(M)}.\n$$\n因此，\n$$\n\\operatorname{Bias}(\\hat{x}_{1}) \\;=\\; \\frac{\\rho A B}{\\det(M)}\\, x_{0} \\;-\\; \\frac{A B}{\\det(M)}\\, x_{1}\n\\;=\\; \\frac{A B}{\\det(M)}\\,\\big(\\rho x_{0} - x_{1}\\big).\n$$\n将 $A=\\frac{1}{R}$，$B=\\frac{1}{Q}$ 和 $\\det(M)=\\frac{1}{R}\\Big(\\frac{1}{R}+\\frac{1+\\rho^{2}}{Q}\\Big)$ 代回，偏差简化为\n$$\n\\operatorname{Bias}(\\hat{x}_{1}) \\;=\\; \\frac{\\frac{1}{R}\\cdot \\frac{1}{Q}}{\\frac{1}{R}\\left(\\frac{1}{R}+\\frac{1+\\rho^{2}}{Q}\\right)}\\,\\big(\\rho x_{0}-x_{1}\\big)\n\\;=\\; \\frac{\\frac{1}{Q}}{\\frac{1}{R}+\\frac{1+\\rho^{2}}{Q}}\\,\\big(\\rho x_{0}-x_{1}\\big)\n\\;=\\; \\frac{R}{Q + R(1+\\rho^{2})}\\,\\big(\\rho x_{0}-x_{1}\\big).\n$$\n接下来，$\\hat{x}_{1}$ 的方差是\n$$\n\\operatorname{Var}(\\hat{x}_{1}) \\;=\\; \\alpha^{2}\\operatorname{Var}(y_{0}) + \\beta^{2}\\operatorname{Var}(y_{1}) \\;=\\; R\\left(\\alpha^{2} + \\beta^{2}\\right).\n$$\n用 $A$ 和 $B$ 计算 $\\alpha^{2} + \\beta^{2}$：\n$$\n\\alpha^{2} + \\beta^{2} \\;=\\; \\frac{(\\rho A B)^{2} + \\left(A(A + \\rho^{2}B)\\right)^{2}}{\\det(M)^{2}}\n\\;=\\; \\frac{A^{2}\\left(\\rho^{2}B^{2} + (A + \\rho^{2}B)^{2}\\right)}{\\det(M)^{2}}.\n$$\n因为 $\\det(M)=A\\big(A + B(1+\\rho^{2})\\big)$，我们有 $\\det(M)^{2} = A^{2}\\big(A + B(1+\\rho^{2})\\big)^{2}$。因此\n$$\n\\operatorname{Var}(\\hat{x}_{1}) \\;=\\; R\\,\\frac{\\rho^{2}B^{2} + \\left(A + \\rho^{2}B\\right)^{2}}{\\left(A + B(1+\\rho^{2})\\right)^{2}}.\n$$\n代入 $A=\\frac{1}{R}$ 和 $B=\\frac{1}{Q}$ 得到\n$$\n\\operatorname{Var}(\\hat{x}_{1}) \\;=\\; R\\,\\frac{\\frac{\\rho^{2}}{Q^{2}} + \\left(\\frac{1}{R} + \\frac{\\rho^{2}}{Q}\\right)^{2}}{\\left(\\frac{1}{R} + \\frac{1+\\rho^{2}}{Q}\\right)^{2}}.\n$$\n为了代数上的清晰，将分子和分母同乘以 $R^{2}Q^{2}$，以获得一个等价、简化的有理式形式：\n$$\n\\operatorname{Var}(\\hat{x}_{1}) \\;=\\; R\\,\\frac{R^2\\rho^2 + (Q+R\\rho^2)^2}{(Q+R(1+\\rho^2))^2}.\n$$\n最后，均方误差是\n$$\n\\operatorname{MSE}(\\hat{x}_{1}) \\;=\\; \\big(\\operatorname{Bias}(\\hat{x}_{1})\\big)^{2} + \\operatorname{Var}(\\hat{x}_{1}),\n$$\n这得到\n$$\n\\operatorname{MSE}(\\hat{x}_{1}) \\;=\\; \\left(\\frac{R}{Q + R(1+\\rho^{2})}\\right)^{2}\\big(\\rho x_{0} - x_{1}\\big)^{2} \\;+\\; R\\,\\frac{R^2\\rho^2 + (Q+R\\rho^2)^2}{(Q+R(1+\\rho^2))^2}.\n$$\n该表达式明确地量化了作为 $Q$ 和 $R$ 函数的偏差-方差权衡：当 $Q$ 减小（更强的动态正则化）时，如果真实状态偏离精确的动态关系（$\\rho x_{0} \\neq x_{1}$），偏差项会增大，而方差项会减小；相反，当 $Q$ 增大（较弱的正则化）时，偏差项会缩小，方差项会向 $R$ 增加。\n\n（注：解答中 `Var(x_1_hat)` 的分子 `Q^2 + 2ρ^2RQ + R^2ρ^2(1+ρ^2)` 是 `R^2ρ^2 + (Q+Rρ^2)^2 = R^2ρ^2 + Q^2 + 2Q Rρ^2 + R^2ρ^4 = Q^2 + 2Q Rρ^2 + R^2ρ^2(1+ρ^2)` 的展开式，两者等价。）", "answer": "$$\\boxed{\\left(\\frac{R}{Q + R(1+\\rho^{2})}\\right)^{2}\\left(\\rho x_{0} - x_{1}\\right)^{2} \\;+\\; R\\,\\frac{R^2\\rho^2 + (Q+R\\rho^2)^2}{\\left(Q + R(1+\\rho^2)\\right)^2}}$$", "id": "3368359"}]}