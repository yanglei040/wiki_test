{"hands_on_practices": [{"introduction": "任何参数选择方法的基础都始于对其数学形式的深刻理解。本练习旨在通过在谱域（spectral domain）中推导差异泛函，来揭示差异原理的核心机制[@problem_id:3376643]。通过将数据空间中的残差范数与问题的奇异值和谱系数联系起来，你将为理论分析和高效计算奠定坚实的基础。", "problem": "考虑一个有限维线性逆问题，其数据空间维度为 $m$，参数空间维度为 $n$，且已知正演算子矩阵 $A \\in \\mathbb{C}^{m \\times n}$。给定奇异值分解 (SVD)，其定义为将矩阵分解为 $A = U \\Sigma V^{*}$ 的形式，其中 $U \\in \\mathbb{C}^{m \\times m}$ 和 $V \\in \\mathbb{C}^{n \\times n}$ 是酉矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是对角（可能为矩形）矩阵，其对角元为非负数。将非零奇异值记为 $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r} > 0$，其中 $r = \\mathrm{rank}(A)$。令 $\\{u_{i}\\}_{i=1}^{m}$ 和 $\\{v_{i}\\}_{i=1}^{n}$ 分别表示左、右奇异向量（即 $U$ 和 $V$ 的列向量）。令 $y^{\\delta} \\in \\mathbb{C}^{m}$ 表示含噪数据，其已知噪声水平为 $\\delta > 0$。\n\n一大类谱正则化方法对每个正则化参数 $\\alpha > 0$，通过对与 $A$ 的值域相关的奇异分量应用标量滤波因子 $g_{\\alpha}(\\sigma)$，同时保持正交补不变，来定义数据空间中的一个预测 $A x_{\\alpha}$。具体而言，若将数据系数写为 $c_{i} = u_{i}^{*} y^{\\delta}$（$i = 1, \\dots, m$），则正则化预测具有如下形式\n$$\nA x_{\\alpha} = \\sum_{i=1}^{r} g_{\\alpha}(\\sigma_{i}) \\, c_{i} \\, u_{i}.\n$$\n用于参数选择的差异原理旨在寻找 $\\alpha$，使得残差范数满足\n$$\n\\| A x_{\\alpha} - y^{\\delta} \\| = \\tau \\, \\delta,\n$$\n其中 $\\tau \\geq 1$ 是一个固定的安全因子，$\\|\\cdot\\|$ 表示 $\\mathbb{C}^{m}$ 上的欧几里得范数。\n\n从上述定义和奇异向量的标准正交性出发，推导平方差异泛函\n$$\nD(\\alpha) = \\| A x_{\\alpha} - y^{\\delta} \\|^{2}\n$$\n的一个闭式解析表达式，该表达式用奇异值 $\\sigma_{i}$、数据系数 $c_{i} = u_{i}^{*} y^{\\delta}$ 和滤波因子 $g_{\\alpha}(\\sigma_{i})$ 来表示。你的最终答案必须是 $D(\\alpha)$ 的单个解析表达式；最终答案中不要包含方程或不等式。无需四舍五入，也没有物理单位。", "solution": "用户希望推导平方差异泛函 $D(\\alpha) = \\| A x_{\\alpha} - y^{\\delta} \\|^{2}$ 的一个闭式解析表达式。\n\n题目指出，左奇异向量集 $\\{u_i\\}_{i=1}^{m}$ 是酉矩阵 $U \\in \\mathbb{C}^{m \\times m}$ 的列向量，这意味着它们构成了数据空间 $\\mathbb{C}^m$ 的一组标准正交基。$\\mathbb{C}^m$ 中的任何向量，例如含噪数据向量 $y^{\\delta}$，都可以表示为这些基向量的线性组合。$y^{\\delta}$ 在这组基下的展开式由下式给出：\n$$\ny^{\\delta} = \\sum_{i=1}^{m} (u_{i}^{*} y^{\\delta}) u_{i}\n$$\n题目将此展开式的系数定义为 $c_{i} = u_{i}^{*} y^{\\delta}$。代入此定义，我们得到：\n$$\ny^{\\delta} = \\sum_{i=1}^{m} c_{i} u_{i}\n$$\n正则化预测 $A x_{\\alpha}$ 由下列表达式给出：\n$$\nA x_{\\alpha} = \\sum_{i=1}^{r} g_{\\alpha}(\\sigma_{i}) c_{i} u_{i}\n$$\n其中 $r = \\mathrm{rank}(A)$ 是非零奇异值的数量。\n\n差异泛函 $D(\\alpha)$ 是残差向量 $A x_{\\alpha} - y^{\\delta}$ 的欧几里得范数的平方。现在我们可以通过代入 $A x_{\\alpha}$ 和 $y^{\\delta}$ 的表达式来写出残差向量：\n$$\nA x_{\\alpha} - y^{\\delta} = \\left( \\sum_{i=1}^{r} g_{\\alpha}(\\sigma_{i}) c_{i} u_{i} \\right) - \\left( \\sum_{i=1}^{m} c_{i} u_{i} \\right)\n$$\n为了合并各项，我们可以将第二个从 $i=1$ 到 $m$ 的求和式拆分为两部分：一部分从 $i=1$ 到 $r$，另一部分从 $i=r+1$ 到 $m$。这样得到：\n$$\n\\sum_{i=1}^{m} c_{i} u_{i} = \\sum_{i=1}^{r} c_{i} u_{i} + \\sum_{i=r+1}^{m} c_{i} u_{i}\n$$\n将此代回残差向量的表达式中，可得：\n$$\nA x_{\\alpha} - y^{\\delta} = \\left( \\sum_{i=1}^{r} g_{\\alpha}(\\sigma_{i}) c_{i} u_{i} \\right) - \\left( \\sum_{i=1}^{r} c_{i} u_{i} + \\sum_{i=r+1}^{m} c_{i} u_{i} \\right)\n$$\n现在我们可以将对应于相同基向量 $u_{i}$ 的项进行分组：\n$$\nA x_{\\alpha} - y^{\\delta} = \\sum_{i=1}^{r} (g_{\\alpha}(\\sigma_{i}) c_{i} - c_{i}) u_{i} - \\sum_{i=r+1}^{m} c_{i} u_{i}\n$$\n从第一个求和式中提取系数 $c_{i}$ 作为公因子，我们得到残差向量在标准正交基 $\\{u_i\\}$ 下的表示：\n$$\nA x_{\\alpha} - y^{\\delta} = \\sum_{i=1}^{r} \\big(g_{\\alpha}(\\sigma_{i}) - 1\\big) c_{i} u_{i} + \\sum_{i=r+1}^{m} (-c_{i}) u_{i}\n$$\n在标准正交基 $\\{u_i\\}$ 下展开的向量 $v = \\sum_{i=1}^{m} \\beta_{i} u_{i}$，其欧几里得范数的平方由其系数模的平方和给出，即 $\\|v\\|^2 = \\sum_{i=1}^{m} |\\beta_{i}|^2$。这是标准正交性 $u_{i}^{*}u_{j} = \\delta_{ij}$ 的一个推论，被称为帕塞瓦尔恒等式。\n\n将此应用于我们的残差向量，其系数为：当 $1 \\leq i \\leq r$ 时，$\\beta_{i} = (g_{\\alpha}(\\sigma_{i}) - 1) c_{i}$；当 $r+1 \\leq i \\leq m$ 时，$\\beta_{i} = -c_{i}$。\n因此，平方差异泛函 $D(\\alpha)$ 为：\n$$\nD(\\alpha) = \\| A x_{\\alpha} - y^{\\delta} \\|^{2} = \\sum_{i=1}^{r} \\big| \\big(g_{\\alpha}(\\sigma_{i}) - 1\\big) c_{i} \\big|^{2} + \\sum_{i=r+1}^{m} |-c_{i}|^{2}\n$$\n利用复数模的性质 $|ab| = |a||b|$，并注意到 $|-c_i| = |c_i|$，我们可以将表达式化简为：\n$$\nD(\\alpha) = \\sum_{i=1}^{r} |g_{\\alpha}(\\sigma_{i}) - 1|^{2} |c_{i}|^{2} + \\sum_{i=r+1}^{m} |c_{i}|^{2}\n$$\n这就是平方差异泛函的最终闭式解析表达式。第一项表示残差的“数据误差”分量（即在 $A$ 的值域内的部分）的范数平方，而第二项表示残差的“噪声”分量（即与 $A$ 的值域正交的部分）的范数平方。", "answer": "$$\n\\boxed{\\sum_{i=1}^{r} |g_{\\alpha}(\\sigma_{i}) - 1|^{2} |c_{i}|^{2} + \\sum_{i=r+1}^{m} |c_{i}|^{2}}\n$$", "id": "3376643"}, {"introduction": "差异原理是一个强大的工具，但它的有效性取决于对噪声水平的准确了解和对前向模型的精确假设。本练习通过一个简化的思想实验，探讨了当前向模型存在偏差（一种在实际应用中常见的情况）时会发生什么[@problem_id:3376624]。通过这个实践，你将学会批判性地评估该方法的局限性，并理解为何它会因模型错误而导致过度正则化。", "problem": "考虑一个具有二维观测和标量状态的线性逆问题。真实的正演算子是矩阵 $A \\in \\mathbb{R}^{2 \\times 1}$，由 $A = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 给出。观测噪声是加性的且有界，一次观测 $y_{\\delta} \\in \\mathbb{R}^{2}$ 满足 $y_{\\delta} = A x^{\\dagger} + \\eta$，其中 $x^{\\dagger} \\in \\mathbb{R}$ 是真实状态，$\\eta \\in \\mathbb{R}^{2}$ 是噪声，其范数已知为 $\\|\\eta\\|_{2} = \\delta > 0$。假设噪声的实现与第二个坐标对齐，使得 $\\eta = \\begin{pmatrix} 0 \\\\ \\delta \\end{pmatrix}$，因此 $y_{\\delta} = \\begin{pmatrix} x^{\\dagger} \\\\ \\delta \\end{pmatrix}$。一位数据分析师使用了一个有偏的正演算子 $A + \\Delta A = \\tilde{A}$，其中 $\\tilde{A} = \\begin{pmatrix} 1 \\\\ \\varepsilon \\end{pmatrix}$，并带有一个固定的偏差参数 $\\varepsilon \\in [0,1)$。\n\n该分析师应用了参数为 $\\alpha \\geq 0$ 的 Tikhonov 正则化，其定义为泛函的最小化子：\n$$\nJ_{\\alpha}(x) = \\|\\tilde{A} x - y_{\\delta}\\|_{2}^{2} + \\alpha \\, \\|x\\|_{2}^{2}.\n$$\n该分析师使用 Morozov 差异原理（也称为差异原理）来选择 $\\alpha$，该原理规定选择 $\\alpha$ 使得数据失配等于噪声水平的预定倍数：\n$$\n\\|\\tilde{A} \\, x_{\\alpha}(y_{\\delta}) - y_{\\delta}\\|_{2} = \\tau \\, \\delta,\n$$\n其中 $x_{\\alpha}(y_{\\delta})$ 是参数 $\\alpha$ 对应的 Tikhonov 解，$\\tau > 1$ 是一个固定的安全因子。在本问题中，取信噪比为 $x^{\\dagger}/\\delta = 2$，安全因子为 $\\tau = \\sqrt{2}$；即 $x^{\\dagger} = 2 \\delta$ 且 $\\tau^{2} = 2$。\n\n任务：\n- 从 Tikhonov 正则化的定义和最小化子 $x_{\\alpha}(y_{\\delta})$ 的正规方程出发，根据上述差异原理条件推导出关于 $\\alpha$ 的显式方程。\n- 以闭合形式求解此方程，以获得由差异原理选择的 $\\alpha$，作为偏差 $\\varepsilon \\in [0,1)$ 的函数。\n- 以无偏情况 $\\varepsilon = 0$ 为参考，确定偏差如何改变所选的 $\\alpha$，并解释当 $\\varepsilon > 0$ 时差异原理导致过度正则化的机制。\n- 提出一种基于统计的诊断方法，该方法可在数据同化设置中使用新息统计量来检测此类模型偏差，并精确说明在无偏正演模型和正确指定的噪声下，需要监控的标量统计量及其期望值。\n\n请以单个闭合形式表达式给出在所述设置下差异原理参数 $\\alpha$ 关于 $\\varepsilon$ 的解，不得进行数值近似。不要包含单位。不要四舍五入。以其精确的解析形式表达最终结果。", "solution": "所述问题具有科学依据，是自洽且适定的。它在逆问题理论中提出了一个标准的、尽管简化的场景，用以说明模型误差与用于 Tikhonov 正则化中参数选择的 Morozov 差异原理之间的相互作用。所有提供的数据和定义都是一致且充分的，足以推导出唯一解。因此，我们可以着手求解。\n\n问题要求进行若干推导和解释。我们将按顺序逐一处理。\n\n首先，我们推导 Tikhonov 正则化解 $x_{\\alpha}(y_{\\delta})$，为简洁起见，我们将其记为 $x_{\\alpha}$。状态 $x$ 是一个标量，因此 Tikhonov 泛函为\n$$\nJ_{\\alpha}(x) = \\|\\tilde{A} x - y_{\\delta}\\|_{2}^{2} + \\alpha \\|x\\|_{2}^{2} = (\\tilde{A}x - y_{\\delta})^{\\top}(\\tilde{A}x - y_{\\delta}) + \\alpha x^2\n$$\n为了找到最小化子，我们计算关于 $x$ 的导数并将其设为零：\n$$\n\\frac{dJ_{\\alpha}}{dx} = \\frac{d}{dx} \\left( x^2 (\\tilde{A}^{\\top}\\tilde{A}) - 2x(\\tilde{A}^{\\top}y_{\\delta}) + y_{\\delta}^{\\top}y_{\\delta} + \\alpha x^2 \\right) = 2x(\\tilde{A}^{\\top}\\tilde{A}) - 2(\\tilde{A}^{\\top}y_{\\delta}) + 2\\alpha x = 0\n$$\n这得到了标量情况下的正规方程：\n$$\n(\\tilde{A}^{\\top}\\tilde{A} + \\alpha)x_{\\alpha} = \\tilde{A}^{\\top}y_{\\delta}\n$$\n解是\n$$\nx_{\\alpha} = \\frac{\\tilde{A}^{\\top}y_{\\delta}}{\\tilde{A}^{\\top}\\tilde{A} + \\alpha}\n$$\n给定有偏正演算子 $\\tilde{A} = \\begin{pmatrix} 1 \\\\ \\varepsilon \\end{pmatrix}$ 和观测值 $y_{\\delta} = \\begin{pmatrix} x^{\\dagger} \\\\ \\delta \\end{pmatrix}$。由于 $x^{\\dagger} = 2\\delta$，我们有 $y_{\\delta} = \\begin{pmatrix} 2\\delta \\\\ \\delta \\end{pmatrix} = \\delta \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$。我们来计算 $x_{\\alpha}$ 表达式中的各项：\n$$\n\\tilde{A}^{\\top}\\tilde{A} = \\begin{pmatrix} 1  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\varepsilon \\end{pmatrix} = 1 + \\varepsilon^2\n$$\n$$\n\\tilde{A}^{\\top}y_{\\delta} = \\begin{pmatrix} 1  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 2\\delta \\\\ \\delta \\end{pmatrix} = 2\\delta + \\varepsilon\\delta = \\delta(2+\\varepsilon)\n$$\n将这些代入 $x_{\\alpha}$ 的表达式中，得到：\n$$\nx_{\\alpha} = \\frac{\\delta(2+\\varepsilon)}{1+\\varepsilon^2+\\alpha}\n$$\n接下来，我们使用 Morozov 差异原理来找到关于 $\\alpha$ 的方程。该原理指出，选择 $\\alpha$ 使得残差范数满足 $\\|\\tilde{A}x_{\\alpha} - y_{\\delta}\\|_{2} = \\tau\\delta$。给定 $\\tau = \\sqrt{2}$，这变为 $\\|\\tilde{A}x_{\\alpha} - y_{\\delta}\\|_{2}^2 = 2\\delta^2$。\n\n为了计算残差范数，我们可以在 $\\tilde{A}$ 的奇异向量基中分解观测值 $y_{\\delta}$。由于 $\\tilde{A}$ 是一个 $2 \\times 1$ 矩阵，它有一个非零奇异值 $\\sigma_1 = \\|\\tilde{A}\\|_2 = \\sqrt{\\tilde{A}^{\\top}\\tilde{A}} = \\sqrt{1+\\varepsilon^2}$。相应的左奇异向量是 $u_1 = \\frac{1}{\\sigma_1}\\tilde{A} = \\frac{1}{\\sqrt{1+\\varepsilon^2}}\\begin{pmatrix} 1 \\\\ \\varepsilon \\end{pmatrix}$。另一个左奇异向量 $u_2$ 张成 $\\tilde{A}^{\\top}$ 的零空间，并且必须与 $u_1$ 正交。我们可以选择 $u_2 = \\frac{1}{\\sqrt{1+\\varepsilon^2}}\\begin{pmatrix} -\\varepsilon \\\\ 1 \\end{pmatrix}$。\n\n残差向量为 $r_{\\alpha} = \\tilde{A}x_{\\alpha} - y_{\\delta}$。其范数的平方是 $\\|r_{\\alpha}\\|_{2}^2 = \\|(u_1^{\\top}r_{\\alpha})u_1 + (u_2^{\\top}r_{\\alpha})u_2\\|_{2}^2 = (u_1^{\\top}r_{\\alpha})^2 + (u_2^{\\top}r_{\\alpha})^2$。\n$u_1$ 分量是：\n$$\nu_1^{\\top}r_{\\alpha} = u_1^{\\top}(\\tilde{A}x_{\\alpha} - y_{\\delta}) = u_1^{\\top}(\\sigma_1 u_1 x_{\\alpha}) - u_1^{\\top}y_{\\delta} = \\sigma_1 x_{\\alpha} - u_1^{\\top}y_{\\delta}\n$$\n我们有 $u_1^{\\top}y_{\\delta} = \\frac{1}{\\sqrt{1+\\varepsilon^2}}\\begin{pmatrix} 1  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 2\\delta \\\\ \\delta \\end{pmatrix} = \\frac{\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}}$。\n代入 $x_{\\alpha}$：\n$$\n\\sigma_1 x_{\\alpha} - u_1^{\\top}y_{\\delta} = \\sqrt{1+\\varepsilon^2}\\frac{\\delta(2+\\varepsilon)}{1+\\varepsilon^2+\\alpha} - \\frac{\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}} = \\frac{\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}} \\left( \\frac{1+\\varepsilon^2}{1+\\varepsilon^2+\\alpha} - 1 \\right) = \\frac{\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}} \\left( \\frac{-\\alpha}{1+\\varepsilon^2+\\alpha} \\right)\n$$\n$u_2$ 分量是：\n$$\nu_2^{\\top}r_{\\alpha} = u_2^{\\top}(\\tilde A x_{\\alpha} - y_{\\delta}) = (u_2^{\\top}\\tilde A) x_{\\alpha} - u_2^{\\top}y_{\\delta} = 0 - u_2^{\\top}y_{\\delta}\n$$\n$$\n-u_2^{\\top}y_{\\delta} = -\\frac{1}{\\sqrt{1+\\varepsilon^2}}\\begin{pmatrix} -\\varepsilon  1 \\end{pmatrix} \\begin{pmatrix} 2\\delta \\\\ \\delta \\end{pmatrix} = -\\frac{-2\\delta\\varepsilon + \\delta}{\\sqrt{1+\\varepsilon^2}} = \\frac{\\delta(2\\varepsilon-1)}{\\sqrt{1+\\varepsilon^2}}\n$$\n残差的范数平方是：\n$$\n\\|r_{\\alpha}\\|_{2}^2 = \\left( \\frac{-\\alpha\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}(1+\\varepsilon^2+\\alpha)} \\right)^2 + \\left( \\frac{\\delta(2\\varepsilon-1)}{\\sqrt{1+\\varepsilon^2}} \\right)^2 = \\frac{\\delta^2}{1+\\varepsilon^2} \\left[ \\frac{\\alpha^2(2+\\varepsilon)^2}{(1+\\varepsilon^2+\\alpha)^2} + (2\\varepsilon-1)^2 \\right]\n$$\n将其设为等于 $2\\delta^2$ 并消去 $\\delta^2$：\n$$\n\\frac{1}{1+\\varepsilon^2} \\left[ \\frac{\\alpha^2(2+\\varepsilon)^2}{(1+\\varepsilon^2+\\alpha)^2} + (1-2\\varepsilon)^2 \\right] = 2\n$$\n$$\n\\frac{\\alpha^2(2+\\varepsilon)^2}{(1+\\varepsilon^2+\\alpha)^2} = 2(1+\\varepsilon^2) - (1-2\\varepsilon)^2 = 2+2\\varepsilon^2 - (1-4\\varepsilon+4\\varepsilon^2) = 1+4\\varepsilon-2\\varepsilon^2\n$$\n设一个新变量 $X = \\frac{\\alpha}{1+\\varepsilon^2+\\alpha}$。方程变为：\n$$\nX^2 (2+\\varepsilon)^2 = 1+4\\varepsilon-2\\varepsilon^2\n$$\n由于 $\\alpha \\ge 0$ 且 $\\varepsilon \\in [0,1)$，所以 $X \\ge 0$。我们可以解出 $X$：\n$$\nX = \\frac{\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}\n$$\n（我们取正根，并注意到 $2+\\varepsilon > 0$）。现在我们根据 $X$ 求解 $\\alpha$。从 $X = \\frac{\\alpha}{1+\\varepsilon^2+\\alpha}$，我们有 $X(1+\\varepsilon^2+\\alpha) = \\alpha$，这得到 $X(1+\\varepsilon^2) = \\alpha(1-X)$，因此：\n$$\n\\alpha = \\frac{X(1+\\varepsilon^2)}{1-X}\n$$\n代入 $X$ 的表达式：\n$$\n\\alpha = \\frac{\\frac{\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}(1+\\varepsilon^2)}{1 - \\frac{\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}} = \\frac{(1+\\varepsilon^2)\\frac{\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}}{\\frac{2+\\varepsilon-\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}}\n$$\n这可以简化为 $\\alpha$ 的最终闭合形式表达式：\n$$\n\\alpha(\\varepsilon) = \\frac{(1+\\varepsilon^2)\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon-\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}\n$$\n这就是所要求的表达式。现在我们来分析它。对于无偏情况 $\\varepsilon=0$：\n$$\n\\alpha(0) = \\frac{(1)\\sqrt{1}}{2-\\sqrt{1}} = 1\n$$\n对于 $\\varepsilon>0$，$\\alpha$ 的值会增加。例如，$\\alpha'(0) = 3$。一个更大的 $\\alpha$ 意味着更强的正则化（解 $x_{\\alpha}$ 更偏向于 $0$）。在存在模型偏差的情况下，差异原理会导致过度正则化。其机制如下：总的数据失配 $\\| \\tilde{A}x_\\alpha - y_\\delta \\|^2$ 由一个可通过 $\\alpha$ 调节的部分（残差在 $\\tilde{A}$ 值域上的投影）和一个不可调节的部分（残差在 $\\tilde{A}^\\top$ 零空间上的投影）组成。后一部分 $(u_2^\\top y_\\delta)^2 = \\frac{\\delta^2(1-2\\varepsilon)^2}{1+\\varepsilon^2}$ 是由模型误差导致的“不可约”失配。对于 $\\varepsilon \\in (0,1)$，这个不可约失配比 $\\varepsilon=0$ 时要小。然而，差异原理要求总失配固定为 $2\\delta^2$。为了补偿较小的不可约失配，该原理迫使失配的可调节部分变得比无偏情况下更大。由于可调节部分是 $\\alpha$ 的单调递增函数，因此会选择一个更大的 $\\alpha$。该原理将有偏模型无法解释的部分信号误解为噪声，并增加正则化强度来抑制它。\n\n最后，我们为模型偏差提出一种基于统计的诊断方法。在数据同化设置中，人们监控新息向量 $d_k = y_k - \\tilde{A}x_{b,k}$，其中 $y_k$ 是在时间 $k$ 的观测值，$x_{b,k}$ 是背景（预报）状态。新息的理论期望是 $E[d_k] = E[y_k - \\tilde{A}x_{b,k}] = E[Ax_{t,k} + \\eta_k - \\tilde{A}x_{b,k}]$。在无偏模型（$\\tilde{A}=A$）、无偏预报（$E[x_{b,k}] = E[x_{t,k}]$）和零均值噪声（$E[\\eta_k]=0$）的原假设下，期望新息为零：$E[d_k] = A E[x_{t,k}]-A E[x_{b,k}] = 0$。\n如果模型有偏，则 $E[d_k] = (A-\\tilde{A})E[x_{t,k}] = -(\\Delta A)E[x_{t,k}]$。持续的非零均值新息是模型偏差的标志。\n要监控的具体标量统计量将是时间平均新息向量的一个分量。考虑到本问题中偏差的结构 $\\Delta A = \\begin{pmatrix} 0 \\\\ \\varepsilon \\end{pmatrix}$，偏差仅影响第二个分量。\n因此，要监控的标量统计量是 $S = \\text{mean}_{k}\\left((d_k)_2\\right) = \\frac{1}{N}\\sum_{k=1}^{N}(y_k - \\tilde{A}x_{b,k})_2$。\n在无偏正演模型（$\\tilde{A}=A=\\begin{pmatrix}1\\\\0\\end{pmatrix}$）和正确指定的零均值噪声下，该统计量的期望值为：\n$$E[S] = E[(y_k - Ax_{b,k})_2] = E[(Ax_{t,k} + \\eta_k - Ax_{b,k})_2]$$\n由于 $Ax$ 的第二个分量始终为 $0$，这变为 $E[(\\eta_k)_2]$。对于正确指定的零均值噪声，该期望值为 $0$。S 值与 0 的统计显著偏差将表明正演模型的第二个分量可能存在偏差。", "answer": "$$\n\\boxed{\\frac{(1+\\varepsilon^2)\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon-\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}}\n$$", "id": "3376624"}, {"introduction": "从理论到实践的飞跃是科学研究的关键一步。本练习要求你将差异原理应用于一个动态时变系统，这在数据同化等领域中非常典型[@problem_id:3376683]。你将设计并实现一个完整的序贯算法，通过结合理论推导和数值实现，从而掌握在复杂计算场景中应用正则化方法的核心技能。", "problem": "考虑一个在离散时间 $t \\in \\{1,\\dots,T\\}$ 上提出的一系列线性逆问题，其观测模型为 $y_t^\\delta = A_t x_t^\\dagger + \\eta_t$，其中 $A_t \\in \\mathbb{R}^{m_t \\times n}$ 是一个已知的正演算子，$x_t^\\dagger \\in \\mathbb{R}^{n}$ 是未知的真值，$y_t^\\delta \\in \\mathbb{R}^{m_t}$ 是带噪观测值，$\\eta_t \\in \\mathbb{R}^{m_t}$ 是加性观测噪声，其均值为零，协方差为已知的正定矩阵 $R_t \\in \\mathbb{R}^{m_t \\times m_t}$。假设 $R_t$ 是对角矩阵，且对角线元素严格为正。定义加权残差范数 $\\|R_t^{-1/2}(A_t x - y_t^\\delta)\\|_2$，其中 $R_t^{-1/2}$ 是 $R_t^{-1}$ 的对称正定平方根。\n\n在每个时间 $t$，通过求解一个使用前一时刻估计值 $x_{t-1}$ 作为参考的 Tikhonov 正则化加权最小二乘子问题来估计 $x_t$：\n$$\nx_t(\\alpha_t) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2}\\|R_t^{-1/2}(A_t x - y_t^\\delta)\\|_2^2 + \\frac{\\alpha_t}{2} \\|x - x_{t-1}\\|_2^2 \\right\\},\n$$\n初始化为 $x_0 = 0$。正则化参数 $\\alpha_t > 0$ 在每个时间 $t$ 通过序贯差异原理来选择：\n$$\n\\|R_t^{-1/2}(A_t x_t(\\alpha_t) - y_t^\\delta)\\|_2 \\approx \\tau \\sqrt{m_t},\n$$\n其中 $\\tau > 0$ 是一个用户指定的可信因子，$m_t$ 是时间 $t$ 的观测数量。\n\n从加权最小二乘、协方差为 $R_t$ 的高斯噪声和 Tikhonov 正则化的核心定义出发，推导一个有理论依据的序贯参数选择规则，该规则在每个时间步确定 $\\alpha_t$，以满足差异条件。您的推导应证明，在标准条件下，对于固定的 $(A_t, R_t, y_t^\\delta, x_{t-1})$，映射 $\\alpha \\mapsto \\|R_t^{-1/2}(A_t x_t(\\alpha) - y_t^\\delta)\\|_2$ 关于 $\\alpha$ 是单调的，从而允许在每个时间步 $t$ 对 $\\alpha_t$ 使用一维求根过程。然后，指定一个可实现的算法，该算法在每个 $t$ 通过区间限定和二分法（或类似的可靠单调求根方法）生成 $\\alpha_t$，并求解相应的正规方程以得到 $x_t(\\alpha_t)$。\n\n为评估追踪性能，假设未知真值根据一个稳定的线性高斯状态模型演化：\n$$\nx_t^\\dagger = a x_{t-1}^\\dagger + w_t, \\quad w_t \\sim \\mathcal{N}(0, q^2 I_n),\n$$\n其中 $x_0^\\dagger = 0$。观测值由以下方式生成：\n$$\ny_t^\\delta = A_t x_t^\\dagger + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma^2 I_{m_t}),\n$$\n其中 $\\sigma > 0$ 为已知。在每个时间 $t$，$A_t$ 是已知的，并可能随 $t$ 变化。设 $L = I_n$ 为单位矩阵。\n\n实现序贯差异原理，并针对下述每个测试案例，报告在所有时间 $t \\in \\{1,\\dots,T\\}$ 上汇总的以下三个量化指标：\n- 平均残差比 $r_{\\mathrm{mean}} = \\frac{1}{T} \\sum_{t=1}^T \\frac{\\|R_t^{-1/2}(A_t x_t(\\alpha_t) - y_t^\\delta)\\|_2}{\\tau \\sqrt{m_t}}$。\n- 平均追踪误差 $e_{\\mathrm{mean}} = \\frac{1}{T} \\sum_{t=1}^T \\|x_t(\\alpha_t) - x_t^\\dagger\\|_2$。\n- 命中率 $h = \\frac{1}{T} \\sum_{t=1}^T \\mathbf{1}\\left( \\left| \\frac{\\|R_t^{-1/2}(A_t x_t(\\alpha_t) - y_t^\\delta)\\|_2}{\\tau \\sqrt{m_t}} - 1 \\right| \\le \\gamma \\right)$，容差 $\\gamma = 0.05$（以小数表示，而非百分比）。\n\n所有模拟必须使用固定的随机种子以保证确定性。使用以下参数值测试套件，其中 $I_k$ 表示 $k \\times k$ 单位矩阵，所有量均为无量纲：\n- 案例 A（理想情况，超定）：$n = 40$, $m_t \\equiv 30$, $T = 25$, $a = 0.98$, $q = 0.01$, $\\sigma = 0.02$, $\\tau = 1.05$, 种子 $= 1234$。\n- 案例 B（边缘情况，欠定且动力学更快）：$n = 40$, $m_t \\equiv 20$, $T = 25$, $a = 0.98$, $q = 0.05$, $\\sigma = 0.02$, $\\tau = 1.05$, 种子 $= 1235$。\n- 案例 C（差异目标不匹配）：$n = 40$, $m_t \\equiv 30$, $T = 25$, $a = 0.98$, $q = 0.01$, $\\sigma = 0.02$, $\\tau = 0.85$, 种子 $= 1236$。\n\n对于每个案例，将 $A_t$ 构建为一个尺寸为 $m_t \\times n$ 的平滑时变矩阵，方法是取一个具有独立标准正态分布元素的固定基底矩阵，并加上一个振幅为 $0.1$ 乘以一个独立扰动矩阵的正弦扰动，即 $A_t = A^{\\mathrm{base}} + \\epsilon \\sin(2\\pi t/T) A^{\\mathrm{pert}}$，其中 $\\epsilon = 0.1$。对所有 $t$，设置 $R_t = \\sigma^2 I_{m_t}$。\n\n您的程序应：\n- 实现序贯差异原理，在每个时间点使用鲁棒的区间限定和二分法方案，在固定区间 $\\alpha \\in [\\alpha_{\\min}, \\alpha_{\\max}]$（其中 $\\alpha_{\\min} = 10^{-8}$ 和 $\\alpha_{\\max} = 10^{8}$）上选择 $\\alpha_t$，当残差范数中的绝对差异低于 $10^{-6}$ 的容差或达到固定的最大迭代次数时停止。如果目标值不在区间内，则返回使残差最接近目标的端点。\n- 为每个案例计算三个指标，并以浮点数形式返回。\n\n最终输出格式：您的程序应生成单行输出，包含 9 个结果（每个案例三个，按案例 A、案例 B、案例 C 的顺序排列），形式为一个用方括号括起来的逗号分隔列表（例如，“[rA,eA,hA,rB,eB,hB,rC,eC,hC]”）。不应打印任何额外文本。此处不涉及角度、距离或物理单位；所有量均为无量纲。通过使用指定的种子且无外部输入来确保数值可复现性。", "solution": "我们从动态背景下的加权最小二乘和 Tikhonov 正则化的定义开始。对于每个时间 $t$，观测模型为 $y_t^\\delta = A_t x_t^\\dagger + \\eta_t$，其中 $\\eta_t$ 是均值为零、协方差为 $R_t$（假设为正定且已知）的高斯噪声。自然的数据失配项是平方范数 $\\|R_t^{-1/2}(A_t x - y_t^\\delta)\\|_2^2$。正则化施加了先验信息；这里我们使用一个二次惩罚项，该惩罚项强制相对于前一时刻的估计值 $x_{t-1}$ 实现时间平滑性，从而得到子问题\n$$\n\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|R_t^{-1/2}(A_t x - y_t^\\delta)\\|_2^2 + \\frac{\\alpha_t}{2} \\|x - x_{t-1}\\|_2^2.\n$$\n对于任何 $\\alpha_t > 0$，这是一个严格凸的二次泛函，存在唯一的极小值点 $x_t(\\alpha_t)$。通过将梯度设为零，我们得到正规方程：\n$$\n\\left(A_t^\\top R_t^{-1} A_t + \\alpha_t I_n\\right) x_t(\\alpha_t) = A_t^\\top R_t^{-1} y_t^\\delta + \\alpha_t x_{t-1}.\n$$\n因为 $R_t$ 是正定的且 $\\alpha_t > 0$，所以左侧的矩阵是对称正定的，该线性系统是适定的。\n\n差异原理规定选择 $\\alpha_t$，使得预测的数据失配与噪声水平相匹配（相差一个因子 $\\tau$），具体为\n$$\n\\|R_t^{-1/2}(A_t x_t(\\alpha_t) - y_t^\\delta)\\|_2 = \\tau \\sqrt{m_t}.\n$$\n为了证明对 $\\alpha_t$ 使用求根过程的合理性，我们考察标量函数\n$$\n\\phi_t(\\alpha) = \\|R_t^{-1/2}(A_t x_t(\\alpha) - y_t^\\delta)\\|_2 - \\tau \\sqrt{m_t}.\n$$\n我们寻求 $\\alpha_t$ 使得 $\\phi_t(\\alpha_t) = 0$。在标准条件下（例如，$A_t$ 固定，$R_t \\succ 0$，$x_{t-1}$ 固定），函数 $\\alpha \\mapsto x_t(\\alpha)$ 是连续的，并且残差范数 $\\alpha \\mapsto \\|R_t^{-1/2}(A_t x_t(\\alpha) - y_t^\\delta)\\|_2$ 关于 $\\alpha$ 是非递减的。一个简洁的论证如下。定义 $H_t = A_t^\\top R_t^{-1} A_t$ 和 $b_t = A_t^\\top R_t^{-1} y_t^\\delta$。那么 $x_t(\\alpha)$ 求解 $(H_t + \\alpha I) x = b_t + \\alpha x_{t-1}$。设 $P_t(\\alpha) = A_t (H_t + \\alpha I)^{-1} A_t^\\top R_t^{-1}$。可以证明，残差 $r_t(\\alpha) = R_t^{-1/2}(A_t x_t(\\alpha) - y_t^\\delta)$ 等于\n$$\nr_t(\\alpha) = \\left(I_{m_t} - R_t^{1/2} A_t (H_t + \\alpha I)^{-1} A_t^\\top R_t^{-1/2}\\right) R_t^{-1/2}(y_t^\\delta - A_t x_{t-1}).\n$$\n映射 $\\alpha \\mapsto (H_t + \\alpha I)^{-1}$ 在 $(0,\\infty)$ 上关于 Loewner 序是矩阵单调递减的。因此，$I - R_t^{1/2} A_t (H_t + \\alpha I)^{-1} A_t^\\top R_t^{-1/2}$ 关于 Loewner 序是单调递增的，这意味着其导出的残差范数 $\\|r_t(\\alpha)\\|_2$ 关于 $\\alpha$ 是非递减的。连续性源于矩阵求逆在正定锥上的连续性。因此，像二分法这样的标量求根方法可以应用于 $\\phi_t(\\alpha)$ 在目标值被界定的任何区间上。\n\n算法设计。对于每个时间 $t$，我们按以下步骤进行：\n- 输入：$A_t$，$R_t = \\sigma^2 I_{m_t}$，$y_t^\\delta$，前一时刻的估计值 $x_{t-1}$，目标水平 $\\tau \\sqrt{m_t}$，以及界限 $\\alpha_{\\min} = 10^{-8}$，$\\alpha_{\\max} = 10^{8}$。\n- 定义一个求解器，对于给定的 $\\alpha$，求解\n$$\n(H_t + \\alpha I) x = b_t + \\alpha x_{t-1},\n$$\n其中 $H_t = A_t^\\top R_t^{-1} A_t$ 和 $b_t = A_t^\\top R_t^{-1} y_t^\\delta$，使用一个用于对称正定系统的鲁棒线性求解器（或者，如果数值上不强制对称性，则使用通用线性求解器）。\n- 定义残差范数函数 $g_t(\\alpha) = \\|R_t^{-1/2}(A_t x_t(\\alpha) - y_t^\\delta)\\|_2$。在 $\\alpha_{\\min}$ 和 $\\alpha_{\\max}$ 处计算 $\\phi_t(\\alpha) = g_t(\\alpha) - \\tau \\sqrt{m_t}$。如果 $\\phi_t(\\alpha_{\\min}) \\le 0 \\le \\phi_t(\\alpha_{\\max})$，应用二分法找到 $\\alpha_t \\in [\\alpha_{\\min}, \\alpha_{\\max}]$，使得 $|\\phi_t(\\alpha_t)|$ 小于一个很小的容差（例如，绝对值小于 $10^{-6}$）。如果目标值不在区间内，则选择能使 $g_t(\\alpha)$ 最接近 $\\tau \\sqrt{m_t}$ 的那个端点作为 $\\alpha_t$。\n- 设置 $x_t = x_t(\\alpha_t)$ 并继续到 $t+1$。\n\n追踪性能分析。动态先验 $\\|x - x_{t-1}\\|_2^2$ 通过参考值 $x_{t-1}$ 将时间耦合起来，并鼓励时间平滑性。当过程噪声标准差 $q$ 很小（真值变化缓慢）且选择的 $\\tau$ 近似匹配噪声水平时，差异原理会选择一个能够平衡数据拟合与先验约束的 $\\alpha_t$；平均残差比会集中在 1 附近，平均追踪误差保持较小。当动力学更快（$q$ 更大）或问题是欠定的（$m_t  n$）时，差异原理倾向于减小 $\\alpha_t$ 以强调数据拟合，从而允许更快的响应，但代价是方差增加和可能更大的追踪误差。如果相对于真实噪声水平将 $\\tau$ 设置得太低，算法会追求一个过于理想的残差，导致 $\\alpha_t$ 变小和过拟合；平均残差比会低于 1，命中率下降。\n\n测试套件的实现细节。我们通过 $x_t^\\dagger = a x_{t-1}^\\dagger + w_t$ 来模拟 $x_t^\\dagger$，其中 $x_0^\\dagger = 0$，$a = 0.98$，$w_t \\sim \\mathcal{N}(0, q^2 I_n)$。每个 $A_t$ 构建为 $A_t = A^{\\mathrm{base}} + \\epsilon \\sin(2\\pi t/T) A^{\\mathrm{pert}}$，其中 $\\epsilon = 0.1$，$A^{\\mathrm{base}}$ 和 $A^{\\mathrm{pert}}$ 具有独立的标准正态分布元素。观测值为 $y_t^\\delta = A_t x_t^\\dagger + \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0, \\sigma^2 I_{m_t})$。我们为每个案例固定随机种子以确保可复现性。对于每个案例，我们计算：\n- $r_{\\mathrm{mean}} = \\frac{1}{T} \\sum_{t=1}^T \\frac{\\|R_t^{-1/2}(A_t x_t(\\alpha_t) - y_t^\\delta)\\|_2}{\\tau \\sqrt{m_t}}$，\n- $e_{\\mathrm{mean}} = \\frac{1}{T} \\sum_{t=1}^T \\|x_t(\\alpha_t) - x_t^\\dagger\\|_2$，\n- $h = \\frac{1}{T} \\sum_{t=1}^T \\mathbf{1}\\left( \\left| \\frac{\\|R_t^{-1/2}(A_t x_t(\\alpha_t) - y_t^\\delta)\\|_2}{\\tau \\sqrt{m_t}} - 1 \\right| \\le 0.05 \\right)$。\n\n然后我们按案例 A、案例 B、案例 C 的顺序报告这 9 个浮点数值 $(r_{\\mathrm{mean}}, e_{\\mathrm{mean}}, h)$，以单个逗号分隔的列表形式包含在方括号内，占一行，无任何附加文本。该算法使用区间限定和二分法来求解 $\\alpha_t$，其界限为 $\\alpha_{\\min} = 10^{-8}$，$\\alpha_{\\max} = 10^{8}$，绝对残差容差为 $10^{-6}$，并设置了固定的最大迭代次数以确保终止。这种设计遵循了序贯差异原理，并允许在各种不同的测试案例中对追踪性能进行一致的评估。", "answer": "```python\nimport numpy as np\n\ndef simulate_and_evaluate(seed, n, m, T, a, q, sigma, tau):\n    rng = np.random.default_rng(seed)\n    # Generate base and perturbation matrices for A_t\n    A_base = rng.standard_normal((m, n))\n    A_pert = rng.standard_normal((m, n))\n    eps = 0.1\n\n    # True state initialization\n    x_true = np.zeros(n)\n    # Estimator initialization\n    x_est_prev = np.zeros(n)\n\n    # Covariance inverse components\n    R_inv_scalar = 1.0 / (sigma ** 2)\n    target = tau * np.sqrt(m)\n\n    residual_ratios = []\n    tracking_errors = []\n    hits = []\n\n    # Helper: solve for x given alpha\n    def solve_x_for_alpha(A_t, y_t, x_prev, alpha):\n        # Solve (A^T R^{-1} A + alpha I) x = A^T R^{-1} y + alpha x_prev\n        # Use R^{-1} = (1/sigma^2) I\n        H = R_inv_scalar * (A_t.T @ A_t)\n        rhs = R_inv_scalar * (A_t.T @ y_t) + alpha * x_prev\n        # Add alpha*I to H\n        # To improve numerical stability, use np.linalg.solve on the SPD-ish system\n        M = H + alpha * np.eye(n)\n        try:\n            x = np.linalg.solve(M, rhs)\n        except np.linalg.LinAlgError:\n            # Fallback to least squares if numerical issues arise\n            x, *_ = np.linalg.lstsq(M, rhs, rcond=None)\n        return x\n\n    # Helper: compute residual norm for given alpha\n    def residual_norm(A_t, y_t, x_prev, alpha):\n        x = solve_x_for_alpha(A_t, y_t, x_prev, alpha)\n        r = A_t @ x - y_t\n        # Weighted residual norm: ||R^{-1/2} r|| = ||r|| / sigma\n        rn = np.linalg.norm(r) / sigma\n        return rn, x\n\n    # Bisection to find alpha_t\n    def choose_alpha(A_t, y_t, x_prev):\n        alpha_lo = 1e-8\n        alpha_hi = 1e8\n        tol = 1e-6\n        max_it = 60\n\n        rn_lo, x_lo = residual_norm(A_t, y_t, x_prev, alpha_lo)\n        rn_hi, x_hi = residual_norm(A_t, y_t, x_prev, alpha_hi)\n        phi_lo = rn_lo - target\n        phi_hi = rn_hi - target\n\n        # If not bracketed, return endpoint closest to target\n        if phi_lo > 0 and phi_hi > 0:\n            # Both above target; pick smaller alpha to reduce residual\n            return alpha_lo, x_lo, rn_lo\n        if phi_lo  0 and phi_hi  0:\n            # Both below target; pick larger alpha to increase residual\n            return alpha_hi, x_hi, rn_hi\n\n        # Now bracketed: phi_lo = 0 = phi_hi\n        alo, ahi = alpha_lo, alpha_hi\n        x_mid = None\n        rn_mid = None\n        for _ in range(max_it):\n            amid = np.sqrt(alo * ahi)\n            rn_mid, x_mid = residual_norm(A_t, y_t, x_prev, amid)\n            phi_mid = rn_mid - target\n            if abs(phi_mid) = tol:\n                return amid, x_mid, rn_mid\n            if phi_mid > 0:\n                # Residual too large; decrease alpha\n                ahi = amid\n            else:\n                # Residual too small; increase alpha\n                alo = amid\n        # After max iterations, pick the closer endpoint among alo, ahi, amid\n        # Evaluate endpoints to choose best\n        rn_alo, x_alo = residual_norm(A_t, y_t, x_prev, alo)\n        rn_ahi, x_ahi = residual_norm(A_t, y_t, x_prev, ahi)\n        # Choose argument minimizing |rn - target|\n        cands = [(alo, x_alo, rn_alo), (amid, x_mid, rn_mid), (ahi, x_ahi, rn_ahi)]\n        best = min(cands, key=lambda t: abs(t[2] - target))\n        return best\n\n    # Time loop\n    for t in range(1, T + 1):\n        # Time-varying A_t\n        s = np.sin(2.0 * np.pi * t / T)\n        A_t = A_base + eps * s * A_pert\n        # Evolve truth\n        w_t = rng.standard_normal(n) * q\n        x_true = a * x_true + w_t\n        # Observation\n        eta_t = rng.standard_normal(m) * sigma\n        y_t = A_t @ x_true + eta_t\n\n        # Choose alpha via discrepancy principle\n        alpha_t, x_est, rn = choose_alpha(A_t, y_t, x_est_prev)\n        # Update\n        x_est_prev = x_est\n\n        # Metrics\n        residual_ratio = rn / target\n        err = np.linalg.norm(x_est - x_true)\n        hit = 1.0 if abs(residual_ratio - 1.0) = 0.05 else 0.0\n\n        residual_ratios.append(residual_ratio)\n        tracking_errors.append(err)\n        hits.append(hit)\n\n    r_mean = float(np.mean(residual_ratios))\n    e_mean = float(np.mean(tracking_errors))\n    h_frac = float(np.mean(hits))\n    return r_mean, e_mean, h_frac\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (seed, n, m, T, a, q, sigma, tau)\n        (1234, 40, 30, 25, 0.98, 0.01, 0.02, 1.05),  # Case A\n        (1235, 40, 20, 25, 0.98, 0.05, 0.02, 1.05),  # Case B\n        (1236, 40, 30, 25, 0.98, 0.01, 0.02, 0.85),  # Case C\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, n, m, T, a, q, sigma, tau = case\n        r_mean, e_mean, h_frac = simulate_and_evaluate(seed, n, m, T, a, q, sigma, tau)\n        results.extend([r_mean, e_mean, h_frac])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3376683"}]}