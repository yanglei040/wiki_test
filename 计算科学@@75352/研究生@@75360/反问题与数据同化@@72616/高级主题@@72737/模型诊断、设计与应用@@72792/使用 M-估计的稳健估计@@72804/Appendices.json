{"hands_on_practices": [{"introduction": "M-估计的核心思想是通过降低异常值在求解过程中的影响来增强估计的稳健性。迭代重加权最小二乘（Iteratively Reweighted Least Squares, IRLS）是一种经典且直观的算法，它通过为每个数据点分配一个权重来实现这一目标。此练习将指导您推导并计算Huber损失函数的相应权重，让您亲身体验该算法如何识别并抑制异常数据点，从而为理解鲁棒估计奠定坚实的基础。[@problem_id:3418133]", "problem": "考虑一个数据同化中的线性反演问题，该问题旨在通过线性观测算子 $H$ 从观测值 $y$ 中估计状态向量 $x$，使得残差为 $r=y-Hx$。为提高对离群值的稳健性，假设目标是最小化残差的稳健损失之和，具体来说是使用调谐常数 $c>0$ 的Huber损失。Huber损失定义为\n$$\n\\rho_{c}(r)=\\begin{cases}\n\\frac{1}{2}r^{2},  |r|\\leq c,\\\\\nc|r|-\\frac{1}{2}c^{2},  |r|>c.\n\\end{cases}\n$$\n在用于稳健$M$-估计的迭代重加权最小二乘（IRLS）方案中，稳健目标函数 $\\sum_{i}\\rho_{c}(r_{i})$ 关于 $x$ 的梯度是使用得分函数 $\\psi_{c}(r)=\\frac{d}{dr}\\rho_{c}(r)$ 计算的，并且我们寻求一个加权最小二乘系统，其正规方程在给定迭代点附近与稳健目标的正规方程相匹配。从这些基本定义出发，通过令稳健目标和加权最小二乘目标的梯度相等，推导与Huber损失相关的IRLS权重函数 $w(r)$。然后，对于残差 $r=\\begin{pmatrix}-5  -1  0.2  3\\end{pmatrix}$ 和调谐常数 $c=1.5$，计算每个残差的Huber IRLS权重 $w(r_{i})$。最后，简要解释这些权重在Gauss-Newton步骤中对每次观测对更新的贡献所产生的影响。将最终权重表示为一个行向量。无需四舍五入；请提供精确值。", "solution": "该问题要求推导Huber损失的迭代重加权最小二乘（IRLS）权重函数，计算一组特定残差的这些权重，并对结果进行解释。\n\n首先，我们推导与Huber损失 $\\rho_c(r)$ 相关的IRLS权重函数 $w(r)$。$M$-估计旨在最小化目标函数 $J_{robust}(x) = \\sum_{i} \\rho_{c}(r_i)$，其中 $r = y - Hx$ 是残差向量。该目标函数关于状态向量 $x$ 的梯度使用链式法则求得：\n$$ \\nabla_x J_{robust}(x) = \\sum_i \\frac{d\\rho_c(r_i)}{dr_i} \\nabla_x r_i $$\n残差分量 $r_i = y_i - \\sum_k H_{ik} x_k$ 关于 $x$ 的导数是 $\\nabla_x r_i = -h_i^T$，其中 $h_i$ 是矩阵 $H$ 的第 $i$ 行。损失函数的导数 $\\frac{d\\rho_c(r)}{dr}$ 定义为得分函数 $\\psi_c(r)$。让我们根据Huber损失的定义来计算 $\\psi_c(r)$：\n$$ \\rho_{c}(r)=\\begin{cases} \\frac{1}{2}r^{2},  |r|\\leq c\\\\ c|r|-\\frac{1}{2}c^{2},  |r|>c \\end{cases} $$\n对于 $|r| \\leq c$，导数为 $\\psi_c(r) = \\frac{d}{dr} \\left( \\frac{1}{2}r^2 \\right) = r$。\n对于 $|r| > c$，我们考虑两种情况。如果 $r > c$，则 $\\rho_c(r) = cr - \\frac{1}{2}c^2$，因此 $\\psi_c(r) = c$。如果 $r  -c$，则 $\\rho_c(r) = -cr - \\frac{1}{2}c^2$，因此 $\\psi_c(r) = -c$。在这两种情况下，$\\psi_c(r) = c \\cdot \\text{sgn}(r)$。\n综合这些，得分函数为：\n$$ \\psi_c(r) = \\begin{cases} r,  |r| \\leq c \\\\ c \\cdot \\text{sgn}(r),  |r| > c \\end{cases} $$\n因此，稳健目标函数的梯度为 $\\nabla_x J_{robust}(x) = -\\sum_i h_i^T \\psi_c(r_i) = -H^T \\vec{\\psi}_c(r)$，其中 $\\vec{\\psi}_c(r)$ 是元素为 $\\psi_c(r_i)$ 的列向量。达到最小值的必要条件是梯度为零，这给出了$M$-估计器的正规方程：\n$$ H^T \\vec{\\psi}_c(r) = 0 $$\nIRLS算法通过迭代求解一个加权最小二乘问题来解决这个非线性系统。加权最小二乘问题的目标函数是 $J_{WLS}(x) = \\frac{1}{2} \\sum_i w_i r_i^2 = \\frac{1}{2}(y-Hx)^T W (y-Hx)$，其中 $W$ 是由权重 $w_i$ 构成的对角矩阵。该目标的梯度为：\n$$ \\nabla_x J_{WLS}(x) = -H^T W (y-Hx) = -H^T W r $$\n相应的正规方程为：\n$$ H^T W r = 0 $$\nIRLS的核心原理是定义权重 $w_i$，使得WLS问题的正规方程与$M$-估计器的正规方程相匹配。通过比较两组正规方程，我们令乘以 $H^T$ 的项相等：\n$$ W r = \\vec{\\psi}_c(r) $$\n这种等价关系必须对每个分量 $i$ 成立，因此 $w_i r_i = \\psi_c(r_i)$。权重函数 $w(r)$ 因此可以定义为：\n$$ w(r) = \\frac{\\psi_c(r)}{r} $$\n代入 $\\psi_c(r)$ 的表达式：\n对于 $|r| \\leq c$（且 $r \\neq 0$），$w(r) = \\frac{r}{r} = 1$。对于 $r=0$，我们可以定义 $w(0) = \\lim_{r \\to 0} \\frac{r}{r} = 1$。\n对于 $|r| > c$，$w(r) = \\frac{c \\cdot \\text{sgn}(r)}{r} = \\frac{c}{|r|}$。\n因此，Huber损失的IRLS权重函数为：\n$$ w(r) = \\begin{cases} 1,  |r| \\leq c \\\\ \\frac{c}{|r|},  |r| > c \\end{cases} $$\n这可以更紧凑地写为 $w(r) = \\min(1, \\frac{c}{|r|})$。\n\n接下来，我们为给定的残差 $r=\\begin{pmatrix}-5  -1  0.2  3\\end{pmatrix}$ 和调谐常数 $c=1.5$ 计算权重。我们将推导出的权重函数应用于每个残差分量 $r_i$。\n\n对于 $r_1 = -5$：其绝对值为 $|r_1|=5$。由于 $5 > 1.5$，这是一个离群值。\n$w(r_1) = \\frac{c}{|r_1|} = \\frac{1.5}{5} = \\frac{3/2}{5} = \\frac{3}{10}$。\n\n对于 $r_2 = -1$：其绝对值为 $|r_2|=1$。由于 $1 \\leq 1.5$，这是一个内点值。\n$w(r_2) = 1$。\n\n对于 $r_3 = 0.2$：其绝对值为 $|r_3|=0.2$。由于 $0.2 \\leq 1.5$，这是一个内点值。\n$w(r_3) = 1$。\n\n对于 $r_4 = 3$：其绝对值为 $|r_4|=3$。由于 $3 > 1.5$，这是一个离群值。\n$w(r_4) = \\frac{c}{|r_4|} = \\frac{1.5}{3} = \\frac{1}{2}$。\n\n计算出的权重为 $w(r_1)=\\frac{3}{10}$，$w(r_2)=1$，$w(r_3)=1$ 和 $w(r_4)=\\frac{1}{2}$。\n\n最后，对这些权重进行解释：IRLS算法通过求解一系列加权最小二乘问题，来迭代地逼近$M$-估计问题的解。权重 $w_i$ 决定了每次观测 $y_i$ 在每一步对解的更新的影响程度。\n具有小残差（$|r_i| \\le c$）的观测被视为内点值。对于这些观测，Huber权重为 $w_i=1$，这意味着它们以全部强度对最小二乘拟合做出贡献，就像在标准的（非加权）最小二乘问题中一样。在我们的例子中，对应于残差 $r_2=-1$ 和 $r_3=0.2$ 的观测被视为内点值。\n具有大残差（$|r_i| > c$）的观测被标记为离群值。它们的权重被设置为 $w_i = c/|r_i|  1$。这降低了它们对成本函数的贡献，从而减小了它们对 $x$ 解的影响。残差越大，权重越小。对于我们的数据，残差为 $r_4=3$ 的观测被适度降权至 $\\frac{1}{2}$，而具有最大残差 $r_1=-5$ 的观测则被更显著地降权至 $\\frac{3}{10}$。这种机制确保了潜在的离群值不会主导估计过程，使得对 $x$ 的最终估计对数据 $y$ 中的严重误差更为稳健。", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{3}{10}  1  1  \\frac{1}{2} \\end{pmatrix} } $$", "id": "3418133"}, {"introduction": "从经典的IRLS方法过渡到现代优化技术，我们发现许多强大的算法（如近端梯度法和ADMM）都依赖于一个称为近端算子（proximal operator）的基本构建模块。这个练习将提供宝贵的实践经验，您将为包括Huber损失、Welsch损失和Geman-McClure损失在内的几个重要M-估计量推导并实现近端算子。掌握这些算子是为更复杂的逆问题构建高效求解器的关键一步。[@problem_id:3418049]", "problem": "考虑一个反问题，其中数据同化步骤被建模为对一个标量残差的鲁棒近端更新。对于给定的残差 $r \\in \\mathbb{R}$、正步长参数 $\\tau \\in \\mathbb{R}_{+}$ 和一个鲁棒惩罚函数 $\\rho : \\mathbb{R} \\to \\mathbb{R}_{+}$ (一个 M-估计量)，近端映射被定义为唯一的最小化子\n$$\n\\operatorname{prox}_{\\tau \\rho}(r) \\in \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ \\frac{1}{2} (x - r)^2 + \\tau \\, \\rho(x) \\right\\}.\n$$\n该近端映射是用于鲁棒反演和数据同化的近端分裂方法中的一个基本构建模块。从可微函数 $\\rho$ 的一阶最优性条件出发，即目标函数关于 $x$ 的导数为零，为以下三种 M-估计量推导 $\\operatorname{prox}_{\\tau \\rho}(r)$ 的可实现表达式或算法：\n\n- Huber 惩罚，其阈值为 $\\delta \\in \\mathbb{R}_{+}$：\n$$\n\\rho_{\\mathrm{Huber}}(x) = \\begin{cases}\n\\frac{1}{2} x^2,  \\text{if } |x| \\le \\delta, \\\\\n\\delta |x| - \\frac{1}{2} \\delta^2,  \\text{if } |x| > \\delta,\n\\end{cases}\n$$\n其中 $|\\,\\cdot\\,|$ 表示绝对值。\n\n- Welsch (Leclerc) 惩罚，其尺度为 $c \\in \\mathbb{R}_{+}$：\n$$\n\\rho_{\\mathrm{Welsch}}(x) = \\frac{c^2}{2}\\left(1 - \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right).\n$$\n\n- Geman–McClure 惩罚，其尺度为 $c \\in \\mathbb{R}_{+}$：\n$$\n\\rho_{\\mathrm{GM}}(x) = \\frac{1}{2} \\frac{x^2}{1 + \\left(\\frac{x}{c}\\right)^2}.\n$$\n\n您的程序必须通过调用一阶最优性条件，为上述每种 $\\rho$ 实现一个近端算子 $x = \\operatorname{prox}_{\\tau \\rho}(r)$。在存在闭式解的情况下，使用它来获得一个精确的分段定义表达式。在不存在初等闭式解的情况下，基于最优性方程推导一个良态的标量求根方案，并实现一个鲁棒的数值求解器（带有收敛容差和安全措施），该求解器能为所提供的测试套件中的任何输入可靠地返回最小化子。推导和实现必须是通用的且纯数学的；不涉及任何物理单位。\n\n测试套件。使用以下参数值和残差来检验一般行为、边界条件和边缘情况：\n\n- Huber 惩罚，$\\delta = 1.25$，$\\tau = 0.75$，在残差 $r \\in \\{-3.00, -\\delta(1+\\tau), 0.00, \\delta(1+\\tau), 2.70\\}$ 处进行评估，即 $r \\in \\{-3.00, -1.25(1+0.75), 0.00, 1.25(1+0.75), 2.70\\}$。\n\n- Welsch 惩罚， $c = 1.00$，$\\tau = 0.50$，在残差 $r \\in \\{-2.00, -1.00, 0.00, 1.00, 2.00\\}$ 处进行评估。\n\n- Geman–McClure 惩罚， $c = 1.50$，$\\tau = 0.80$，在残差 $r \\in \\{-3.00, -1.50, 0.00, 1.50, 3.00\\}$ 处进行评估。\n\n最终输出格式。您的程序应生成单行输出，其中包含按上述顺序列出的近端值（首先是所有 Huber 结果，按指定的 $r$ 顺序；然后是所有 Welsch 结果；最后是所有 Geman–McClure 结果），格式化为方括号内以逗号分隔的列表，例如 $[x_1,x_2,\\dots,x_{15}]$。每个条目 $x_i$ 必须是一个实数（浮点数）。不应打印任何额外文本。", "solution": "该问题陈述经评估有效。它在科学上基于近端微积分和鲁棒统计的既定数学理论。该问题是良态的，提供了推导和实现所需近端算子的所有必要定义、参数和约束。问题以客观、正式的语言表述，没有歧义或主观性陈述。\n\n任务是找到目标函数 $J(x)$ 的最小化子 $x^\\star$：\n$$\nx^\\star = \\operatorname{prox}_{\\tau \\rho}(r) = \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ J(x) = \\frac{1}{2} (x - r)^2 + \\tau \\, \\rho(x) \\right\\}\n$$\n对于给定的残差 $r$、步长 $\\tau  0$ 和一个鲁棒惩罚函数 $\\rho(x)$。假设 $\\rho(x)$ 在最小化子处是可微的，一阶最优性条件是目标函数 $J'(x)$ 的导数必须为零：\n$$\nJ'(x) = \\frac{d}{dx} \\left( \\frac{1}{2} (x - r)^2 + \\tau \\, \\rho(x) \\right) = (x - r) + \\tau \\, \\rho'(x) = 0\n$$\n这个最优性方程 $x + \\tau \\rho'(x) = r$ 构成了为每个指定的 M-估计量推导近端算子的基础。\n\n### 1. Huber 惩罚\n\nHuber 惩罚函数由阈值 $\\delta  0$ 参数化，其表达式为：\n$$\n\\rho_{\\mathrm{Huber}}(x) = \\begin{cases}\n\\frac{1}{2} x^2,  \\text{if } |x| \\le \\delta, \\\\\n\\delta |x| - \\frac{1}{2} \\delta^2,  \\text{if } |x| > \\delta.\n\\end{cases}\n$$\n该函数是凸的且连续可微。其导数 $\\psi_{\\mathrm{Huber}}(x) = \\rho'_{\\mathrm{Huber}}(x)$ 为：\n$$\n\\psi_{\\mathrm{Huber}}(x) = \\begin{cases}\nx,  \\text{if } |x| \\le \\delta, \\\\\n\\delta \\operatorname{sgn}(x),  \\text{if } |x| > \\delta.\n\\end{cases}\n$$\n根据 $x$ 的值，将最优性条件 $x - r + \\tau \\psi_{\\mathrm{Huber}}(x) = 0$ 分为三种情况进行分析。\n\n情况1：$|x| \\le \\delta$。最优性条件为 $x - r + \\tau x = 0$，得到 $(1+\\tau)x = r$，即 $x = \\frac{r}{1+\\tau}$。如果 $|x| \\le \\delta$，即 $|\\frac{r}{1+\\tau}| \\le \\delta$ 或 $|r| \\le \\delta(1+\\tau)$，则该解是自洽的。\n\n情况2：$x > \\delta$。最优性条件为 $x - r + \\tau \\delta = 0$，得到 $x = r - \\tau\\delta$。如果 $x > \\delta$，即 $r - \\tau\\delta > \\delta$ 或 $r > \\delta(1+\\tau)$，则该解是一致的。\n\n情况3：$x  -\\delta$。最优性条件为 $x - r - \\tau \\delta = 0$，得到 $x = r + \\tau\\delta$。如果 $x  -\\delta$，即 $r + \\tau\\delta  -\\delta$ 或 $r  -\\delta(1+\\tau)$，则该解是一致的。\n\n结合这些互斥的情况，可得到 Huber 惩罚的近端算子的完整闭式解：\n$$\n\\operatorname{prox}_{\\tau \\rho_{\\mathrm{Huber}}}(r) = \\begin{cases}\nr + \\tau\\delta,  \\text{if } r  -\\delta(1+\\tau) \\\\\n\\frac{r}{1+\\tau},  \\text{if } |r| \\le \\delta(1+\\tau) \\\\\nr - \\tau\\delta,  \\text{if } r > \\delta(1+\\tau)\n\\end{cases}\n$$\n该表达式是精确的，将直接实现。\n\n### 2. Welsch (Leclerc) 惩罚\n\nWelsch 惩罚，其尺度参数为 $c  0$，定义为：\n$$\n\\rho_{\\mathrm{Welsch}}(x) = \\frac{c^2}{2}\\left(1 - \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right)\n$$\n该函数是光滑但非凸的。其导数为：\n$$\n\\psi_{\\mathrm{Welsch}}(x) = \\rho'_{\\mathrm{Welsch}}(x) = x \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\n$$\n一阶最优性条件为 $x - r + \\tau \\psi_{\\mathrm{Welsch}}(x) = 0$，这导致以下非线性方程：\n$$\nx \\left(1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right) = r\n$$\n该方程没有关于 $x$ 的初等闭式解。我们必须找到函数 $g(x) = x \\left(1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right) - r$ 的根。注意，当且仅当 $r=0$ 时，$x=0$ 是解。函数 $h(x) = x \\left(1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right)$ 是奇函数，因此 $\\operatorname{prox}(-r) = -\\operatorname{prox}(r)$。我们可以求解 $r>0$ 的情况，然后扩展解。对于给定的测试用例参数（$\\tau=0.5$），函数 $h(x)$ 是严格单调的，保证了对于任何 $r$ 都存在唯一解。\n\n我们采用牛顿-拉夫逊方法来求解 $g(x) = 0$。迭代更新公式为 $x_{k+1} = x_k - g(x_k)/g'(x_k)$。导数 $g'(x)$ 为：\n$$\ng'(x) = \\frac{d}{dx} \\left[ x \\left(1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right) - r \\right] = 1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right) \\left(1 - \\frac{2x^2}{c^2}\\right)\n$$\n一个好的初始猜测是 $x_0 = r$，因为当 $|x|$ 很大时，指数项消失，方程近似为 $x=r$。算法通过迭代进行，直到 $x$ 的变化量低于指定的容差。\n\n### 3. Geman–McClure 惩罚\n\nGeman–McClure 惩罚，其尺度参数为 $c  0$，表达式为：\n$$\n\\rho_{\\mathrm{GM}}(x) = \\frac{1}{2} \\frac{x^2}{1 + \\left(\\frac{x}{c}\\right)^2} = \\frac{c^2}{2} \\frac{x^2}{c^2 + x^2}\n$$\n该函数也是光滑且非凸的。其导数为：\n$$\n\\psi_{\\mathrm{GM}}(x) = \\rho'_{\\mathrm{GM}}(x) = \\frac{c^4 x}{(c^2+x^2)^2}\n$$\n最优性条件 $x - r + \\tau \\psi_{\\mathrm{GM}}(x) = 0$ 给出以下非线性方程：\n$$\nx \\left(1 + \\frac{\\tau c^4}{(c^2+x^2)^2}\\right) = r\n$$\n与 Welsch 惩罚一样，这需要数值解。我们定义一个函数 $g(x) = x \\left(1 + \\frac{\\tau c^4}{(c^2+x^2)^2}\\right) - r$ 并求其根。其性质与 Welsch 情况相似：当 $r=0$ 时，$x=0$ 是解；该映射是奇函数；对于给定的测试参数（$\\tau=0.8$），存在唯一解。\n\n我们再次使用牛顿-拉夫逊方法。导数 $g'(x)$ 为：\n$$\ng'(x) = \\frac{d}{dx} \\left[ x \\left(1 + \\frac{\\tau c^4}{(c^2+x^2)^2}\\right) - r \\right] = 1 + \\frac{\\tau c^4 (c^2 - 3x^2)}{(c^2+x^2)^3}\n$$\n初始猜测再次取为 $x_0=r$。数值实现将使用此迭代方案找到 $g(x)$ 的根。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef prox_huber(r, tau, delta):\n    \"\"\"\n    Computes the proximal operator for the Huber penalty.\n    \n    prox(r) = argmin_x { 0.5 * (x - r)^2 + tau * rho_huber(x) }\n    \n    This has a closed-form solution.\n    \"\"\"\n    threshold = delta * (1.0 + tau)\n    if r  -threshold:\n        return r + tau * delta\n    elif r > threshold:\n        return r - tau * delta\n    else:\n        return r / (1.0 + tau)\n\ndef prox_welsch(r, tau, c, tol=1e-12, max_iter=100):\n    \"\"\"\n    Computes the proximal operator for the Welsch penalty using Newton's method.\n    \n    prox(r) = argmin_x { 0.5 * (x - r)^2 + tau * rho_welsch(x) }\n    \n    The optimality condition is g(x) = x * (1 + tau*exp(-(x/c)^2)) - r = 0.\n    \"\"\"\n    if r == 0.0:\n        return 0.0\n\n    # The function is odd, so we can solve for abs(r) and apply the sign.\n    r_abs = abs(r)\n    sign = np.sign(r)\n\n    # Initial guess x0 = r. Since we solve for r_abs, x0 = r_abs.\n    x = r_abs\n    c_sq = c * c\n\n    for _ in range(max_iter):\n        x_sq_over_c_sq = (x / c)**2\n        exp_term = np.exp(-x_sq_over_c_sq)\n        \n        # g(x) = x * (1 + tau * exp_term) - r_abs\n        g_x = x * (1.0 + tau * exp_term) - r_abs\n        \n        if abs(g_x)  tol:\n            return x * sign\n\n        # g'(x) = 1 + tau * exp_term * (1 - 2 * (x/c)^2)\n        gp_x = 1.0 + tau * exp_term * (1.0 - 2.0 * x_sq_over_c_sq)\n        \n        # Newton-Raphson update step\n        # Safeguard: gp_x is positive for the problem's parameters.\n        if gp_x == 0.0:\n            break\n        x = x - g_x / gp_x\n\n    return x * sign\n\ndef prox_geman_mcclure(r, tau, c, tol=1e-12, max_iter=100):\n    \"\"\"\n    Computes the proximal operator for the Geman-McClure penalty using Newton's method.\n    \n    prox(r) = argmin_x { 0.5 * (x - r)^2 + tau * rho_gm(x) }\n    \n    The optimality condition is g(x) = x * (1 + tau*c^4 / (c^2+x^2)^2) - r = 0.\n    \"\"\"\n    if r == 0.0:\n        return 0.0\n\n    r_abs = abs(r)\n    sign = np.sign(r)\n    \n    # Initial guess\n    x = r_abs\n    c_sq = c * c\n    tau_c4 = tau * c * c * c * c\n\n    for _ in range(max_iter):\n        x_sq = x * x\n        denom = c_sq + x_sq\n        denom_sq = denom * denom\n        \n        # g(x) = x * (1 + tau*c^4 / (c^2+x^2)^2) - r_abs\n        g_x = x * (1.0 + tau_c4 / denom_sq) - r_abs\n        \n        if abs(g_x)  tol:\n            return x * sign\n        \n        # g'(x) = 1 + tau*c^4 * (c^2 - 3*x^2) / (c^2+x^2)^3\n        denom_cub = denom * denom_sq\n        gp_x = 1.0 + tau_c4 * (c_sq - 3.0 * x_sq) / denom_cub\n\n        # Safeguard: gp_x is positive for the problem's parameters.\n        if gp_x == 0.0:\n            break\n            \n        x = x - g_x / gp_x\n\n    return x * sign\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the test suite for each M-estimator.\n    \"\"\"\n    results = []\n    \n    # Huber penalty test cases\n    delta_huber = 1.25\n    tau_huber = 0.75\n    r_huber_vals = [-3.00, -delta_huber * (1.0 + tau_huber), 0.00, delta_huber * (1.0 + tau_huber), 2.70]\n    for r in r_huber_vals:\n        results.append(prox_huber(r, tau_huber, delta_huber))\n        \n    # Welsch penalty test cases\n    c_welsch = 1.00\n    tau_welsch = 0.50\n    r_welsch_vals = [-2.00, -1.00, 0.00, 1.00, 2.00]\n    for r in r_welsch_vals:\n        results.append(prox_welsch(r, tau_welsch, c_welsch))\n        \n    # Geman-McClure penalty test cases\n    c_gm = 1.50\n    tau_gm = 0.80\n    r_gm_vals = [-3.00, -1.50, 0.00, 1.50, 3.00]\n    for r in r_gm_vals:\n        results.append(prox_geman_mcclure(r, tau_gm, c_gm))\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3418049"}, {"introduction": "在掌握了近端算子这一关键构建模块后，我们便可以将它们组装成一个完整、强大的算法。交替方向乘子法（Alternating Direction Method of Multipliers, ADMM）是一种灵活且广泛应用的分裂方法，能够有效解决复杂的优化问题。在此最终练习中，您将推导应用于鲁棒正则化逆问题的ADMM算法的完整迭代步骤，从而学习如何将M-估计量与正则化项在一个精密的框架内结合起来。[@problem_id:3418091]", "problem": "考虑一个数据同化中的线性反演问题，其中状态向量 $x \\in \\mathbb{R}^{n}$ 通过一个已知的线性观测算子 $A \\in \\mathbb{R}^{m \\times n}$ 从带噪观测 $y \\in \\mathbb{R}^{m}$ 中推断得出。为增强对观测误差中离群值的稳健性，数据失配项采用一类最大似然型估计（M-估计）中的稳健损失函数进行建模，该函数记作一个逐元素作用的凸、真、下半连续函数 $\\rho:\\mathbb{R} \\to \\mathbb{R}$。通过一个强度为 $\\lambda  0$ 的线性正则化算子 $L \\in \\mathbb{R}^{p \\times n}$ 施加二次正则化。该稳健提法可写为\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\sum_{i=1}^{m} \\rho\\!\\left((A x - y)_{i}\\right) + \\lambda \\|L x\\|_{2}^{2}.\n$$\n引入一个分裂变量 $z \\in \\mathbb{R}^{m}$ 来表示残差，其约束为 $z = A x - y$。交替方向乘子法（ADMM）将被应用于此约束重构问题。使用尺度对偶 ADMM，其罚参数为 $\\mu  0$，与约束 $A x - y - z = 0$ 相关联的尺度对偶变量为 $u \\in \\mathbb{R}^{m}$。\n\n从等式约束凸优化的增广拉格朗日函数以及真、凸、下半连续函数的邻近算子的核心定义出发，推导显式的 ADMM 迭代过程。你的推导必须：\n- 建立与约束 $A x - y - z = 0$ 对应的尺度增广拉格朗日函数。\n- 通过对增广拉格朗日函数关于 $x$ 求最小化来获得 $x$ 的更新，并将其表示为一个线性系统的闭式解。\n- 通过关于 $z$ 求最小化来获得 $z$ 的更新，并使用 $\\rho$ 的邻近算子来表示它。\n- 写出尺度对偶变量的更新。\n\n用 $A$、$L$、$y$、$\\lambda$、$\\mu$ 以及当前迭代值 $(x^{k}, z^{k}, u^{k})$ 来表示这三个更新。你的最终答案必须是一个单一的闭式解析表达式，将 $x$-更新、$z$-更新和尺度对偶更新的右侧项紧凑地列为一个行向量。无需数值近似，也不涉及任何单位。不要在最终的方框答案中包含任何方程或不等式；仅按要求呈现更新的表达式。", "solution": "我们首先通过引入辅助变量 $z \\in \\mathbb{R}^{m}$ 来表示观测残差，从而对原始的稳健目标函数进行等式约束重构。问题变为\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{m}} \\left[ \\sum_{i=1}^{m} \\rho(z_{i}) + \\lambda \\|L x\\|_{2}^{2} \\right] \\quad \\text{subject to} \\quad A x - y - z = 0.\n$$\n定义函数 $f(x) = \\lambda \\|L x\\|_{2}^{2}$ 和 $g(z) = \\sum_{i=1}^{m} \\rho(z_{i})$。尺度对偶形式的交替方向乘子法（ADMM）引入了与线性约束相关联的尺度对偶变量 $u \\in \\mathbb{R}^{m}$，并使用一个增广拉格朗日罚参数 $\\mu  0$。根据久经检验的凸优化原理，构建尺度增广拉格朗日函数：\n$$\n\\mathcal{L}_{\\mu}^{\\text{scaled}}(x, z, u) = f(x) + g(z) + \\frac{\\mu}{2} \\left\\|A x - y - z + u \\right\\|_{2}^{2} - \\frac{\\mu}{2} \\|u\\|_{2}^{2}.\n$$\nADMM 迭代过程在对 $\\mathcal{L}_{\\mu}^{\\text{scaled}}$ 分别关于 $x$ 和 $z$ 进行最小化之间交替进行，然后进行尺度对偶变量的更新。\n\n对于 $x$-更新，我们在保持 $(z^{k}, u^{k})$ 固定的同时，对 $x$ 进行最小化：\n$$\nx^{k+1} \\in \\arg\\min_{x} \\left[ \\lambda \\|L x\\|_{2}^{2} + \\frac{\\mu}{2} \\left\\|A x - y - z^{k} + u^{k} \\right\\|_{2}^{2} \\right].\n$$\n两项都是关于 $x$ 的凸函数且可微。一阶最优性条件（通过将梯度设为零得到）得出\n$$\n\\nabla_{x} \\left[ \\lambda \\|L x\\|_{2}^{2} \\right] + \\nabla_{x} \\left[ \\frac{\\mu}{2} \\left\\|A x - y - z^{k} + u^{k} \\right\\|_{2}^{2} \\right] = 0,\n$$\n化简为\n$$\n2 \\lambda L^{\\top} L x + \\mu A^{\\top} \\left( A x - y - z^{k} + u^{k} \\right) = 0.\n$$\n合并关于 $x$ 的项，我们得到正规方程组\n$$\n\\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right) x = \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right).\n$$\n假设矩阵 $\\mu A^{\\top} A + 2 \\lambda L^{\\top} L$ 是可逆的，则 $x$-更新的闭式解为\n$$\nx^{k+1} = \\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right)^{-1} \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right).\n$$\n\n对于 $z$-更新，我们在保持 $(x^{k+1}, u^{k})$ 固定的同时，对 $z$ 进行最小化：\n$$\nz^{k+1} \\in \\arg\\min_{z} \\left[ \\sum_{i=1}^{m} \\rho(z_{i}) + \\frac{\\mu}{2} \\left\\|A x^{k+1} - y - z + u^{k} \\right\\|_{2}^{2} \\right].\n$$\n记 $v^{k} = A x^{k+1} - y + u^{k} \\in \\mathbb{R}^{m}$。由于 $g(z) = \\sum_{i=1}^{m} \\rho(z_{i})$ 是可分的，且二次罚项也是可分的，因此该问题在各个坐标上是解耦的。根据真、凸、下半连续函数 $h:\\mathbb{R}^{m} \\to \\mathbb{R}$ 的邻近算子的定义，\n$$\n\\operatorname{prox}_{\\tau h}(v) := \\arg\\min_{z} \\left\\{ h(z) + \\frac{1}{2 \\tau} \\|z - v\\|_{2}^{2} \\right\\},\n$$\n我们可以将 $z$-更新重写为步长为 $\\tau = \\frac{1}{\\mu}$ 的邻近步：\n$$\nz^{k+1} = \\operatorname{prox}_{\\frac{1}{\\mu} g}\\!\\left( v^{k} \\right) = \\operatorname{prox}_{\\frac{1}{\\mu} \\sum_{i=1}^{m} \\rho}\\!\\left( A x^{k+1} - y + u^{k} \\right).\n$$\n因为 $g$ 是可分的，所以该邻近算子是逐元素作用的，并且可以等价地表示为将标量邻近算子 $\\operatorname{prox}_{\\frac{1}{\\mu}\\rho}$ 逐坐标地应用：\n$$\nz^{k+1} = \\left[ \\operatorname{prox}_{\\frac{1}{\\mu}\\rho}\\!\\left( (A x^{k+1} - y + u^{k})_{i} \\right) \\right]_{i=1}^{m}.\n$$\n\n最后，$u$ 的尺度对偶变量更新直接遵循标准尺度 ADMM 对等式约束 $A x - y - z = 0$ 的定义：\n$$\nu^{k+1} = u^{k} + \\left( A x^{k+1} - y - z^{k+1} \\right).\n$$\n\n综合这三个更新，我们得到用当前迭代值 $(x^{k}, z^{k}, u^{k})$、算子 $A$、$L$、数据 $y$ 以及参数 $(\\lambda, \\mu)$ 紧凑表示的 ADMM 迭代过程：\n- $x$-更新：\n$$\nx^{k+1} = \\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right)^{-1} \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right),\n$$\n- $z$-更新：\n$$\nz^{k+1} = \\operatorname{prox}_{\\frac{1}{\\mu}\\rho}\\!\\left( A x^{k+1} - y + u^{k} \\right),\n$$\n- 尺度对偶更新：\n$$\nu^{k+1} = u^{k} + A x^{k+1} - y - z^{k+1}.\n$$\n这些公式完成了在分裂 $z = A x - y$ 下，针对具有二次正则化的稳健 M-估计数据失配问题的 ADMM 推导。", "answer": "$$\\boxed{\\begin{pmatrix}\n\\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right)^{-1} \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right)  \\operatorname{prox}_{\\frac{1}{\\mu}\\rho}\\!\\left( A x^{k+1} - y + u^{k} \\right)  u^{k} + A x^{k+1} - y - z^{k+1}\n\\end{pmatrix}}$$", "id": "3418091"}]}