{"hands_on_practices": [{"introduction": "这个练习是掌握偏微分方程约束优化领域的基石。它介绍了伴随法，这是计算约束优化问题梯度的核心工具。通过一个经典的泊松方程示例[@problem_id:3409520]，您将亲手推导出伴随方程和目标泛函的梯度表达式。这个过程能帮助您深刻理解如何将一个复杂的无限维优化问题，转化为求解一系列偏微分方程的有效方法，为解决更高级的问题打下坚实基础。", "problem": "考虑一个有界、开放、连通的域 $\\Omega \\subset \\mathbb{R}^{d}$，其边界 $\\partial \\Omega$ 为 Lipschitz 边界，其中 $d \\in \\{2,3\\}$。令 $y_{d} \\in L^{2}(\\Omega)$ 表示一个给定的数据场，在反问题和数据同化的背景下解释为目标状态。对于一个控制 $u \\in L^{2}(\\Omega)$，将状态 $y \\in H_{0}^{1}(\\Omega)$ 定义为椭圆偏微分方程 (PDE) 的弱解\n$$\n-\\Delta y = u \\quad \\text{在 } \\Omega \\text{ 内}, \\qquad y = 0 \\quad \\text{在 } \\partial \\Omega \\text{ 上},\n$$\n其中 $\\Delta$ 是拉普拉斯算子，$H_{0}^{1}(\\Omega)$ 是 Sobolev 空间，其中的函数具有平方可积的梯度且在 $\\partial \\Omega$ 上的迹为零。\n\n考虑带有 Tikhonov 正则化参数 $\\alpha > 0$ 的二次追踪型目标泛函，\n$$\nJ(y,u) = \\frac{1}{2}\\|y - y_{d}\\|_{L^{2}(\\Omega)}^{2} + \\frac{\\alpha}{2}\\|u\\|_{L^{2}(\\Omega)}^{2}.\n$$\n定义约化泛函 $j(u) := J(S(u),u)$，其中 $S : L^{2}(\\Omega) \\to H_{0}^{1}(\\Omega)$ 将 $u$ 映射到该 PDE 的弱解 $y$。从变分法的基本原理和控制 PDE 的弱形式出发，推导与状态方程相关的伴随方程，并用它来求得约化泛函 $j(u)$ 关于 $u$ 的梯度。您的推导必须通过方向导数的内积表示和在 $\\Omega$ 上的 Green 恒等式来明确证明引入伴随变量的合理性，并且必须仅使用椭圆 PDE 弱解和 Hilbert 空间 $L^{2}(\\Omega)$ 中 Fréchet 导数的基本定义。\n\n作为最终答案，报告 $j(u)$ 的 $L^{2}(\\Omega)$-梯度的闭式解析表达式。无需进行数值计算。如果您的最终解析表达式涉及任何辅助变量，则必须通过从所述原理推导出的 PDE 和边界条件来隐式地指定它。最终答案必须是单个无单位的表达式。", "solution": "问题陈述经核实具有科学依据、适定且完整。它代表了 PDE 约束优化和反问题领域的一个标准问题。我们可以开始推导。\n\n目标是求出约化泛函 $j(u) = J(S(u), u)$ 关于控制变量 $u \\in L^{2}(\\Omega)$ 的梯度。该梯度是 Hilbert 空间 $L^{2}(\\Omega)$ 中的一个元素，我们记为 $\\nabla j(u)$。根据 Riesz 表示定理，梯度通过 Fréchet 导数 $j'(u)$ 定义如下：\n$$\nj'(u)h = \\langle \\nabla j(u), h \\rangle_{L^{2}(\\Omega)} = \\int_{\\Omega} \\nabla j(u) h \\,d\\mathbf{x}\n$$\n对所有扰动 $h \\in L^{2}(\\Omega)$ 成立。Fréchet 导数作用于方向 $h$ 时，由方向导数给出：\n$$\nj'(u)h = \\lim_{\\epsilon \\to 0} \\frac{j(u+\\epsilon h) - j(u)}{\\epsilon}.\n$$\n\n约化泛函由下式给出\n$$\nj(u) = \\frac{1}{2}\\|S(u) - y_{d}\\|_{L^{2}(\\Omega)}^{2} + \\frac{\\alpha}{2}\\|u\\|_{L^{2}(\\Omega)}^{2}.\n$$\n令 $y = S(u)$ 为与控制 $u$ 对应的状态，它是以下方程的弱解\n$$\n-\\Delta y = u \\quad \\text{在 } \\Omega \\text{ 内}, \\qquad y=0 \\quad \\text{在 } \\partial \\Omega \\text{ 上}.\n$$\n该状态方程的弱形式为：求 $y \\in H_{0}^{1}(\\Omega)$，使得对于所有测试函数 $v \\in H_{0}^{1}(\\Omega)$，\n$$\n\\int_{\\Omega} \\nabla y \\cdot \\nabla v \\,d\\mathbf{x} = \\int_{\\Omega} u v \\,d\\mathbf{x}.\n$$\n这个方程定义了控制到状态的映射 $S: u \\mapsto y$。由于控制 PDE 是线性的，所以映射 $S$ 是一个线性算子。\n\n现在我们计算 $j(u)$ 的方向导数。让我们考虑一个扰动 $u + \\epsilon h$，其中 $\\epsilon > 0$ 是一个小参数，$h \\in L^{2}(\\Omega)$ 是一个方向。对应的状态是 $y_{u+\\epsilon h} = S(u+\\epsilon h)$。由于 $S$ 的线性，我们有 $y_{u+\\epsilon h} = S(u) + \\epsilon S(h)$。我们定义灵敏度状态 $\\delta y := S(h)$。这个 $\\delta y \\in H_{0}^{1}(\\Omega)$ 是源项为 $h$ 的 PDE 的弱解：\n$$\n-\\Delta(\\delta y) = h \\quad \\text{在 } \\Omega \\text{ 内}, \\qquad \\delta y = 0 \\quad \\text{在 } \\partial \\Omega \\text{ 上}.\n$$\n灵敏度方程的弱形式为：求 $\\delta y \\in H_{0}^{1}(\\Omega)$，使得对于所有 $v \\in H_{0}^{1}(\\Omega)$，\n$$\n\\int_{\\Omega} \\nabla (\\delta y) \\cdot \\nabla v \\,d\\mathbf{x} = \\int_{\\Omega} h v \\,d\\mathbf{x}.\n$$\n\n现在我们计算 $j(u+\\epsilon h)$：\n$$\nj(u+\\epsilon h) = \\frac{1}{2}\\|S(u+\\epsilon h) - y_{d}\\|_{L^{2}(\\Omega)}^{2} + \\frac{\\alpha}{2}\\|u+\\epsilon h\\|_{L^{2}(\\Omega)}^{2} \\\\\n= \\frac{1}{2}\\|(y + \\epsilon \\delta y) - y_{d}\\|_{L^{2}(\\Omega)}^{2} + \\frac{\\alpha}{2}\\|u+\\epsilon h\\|_{L^{2}(\\Omega)}^{2}.\n$$\n我们展开平方范数，它由 $L^{2}(\\Omega)$ 内积 $\\langle f, g \\rangle_{L^{2}(\\Omega)} = \\int_{\\Omega} f g \\,d\\mathbf{x}$ 定义：\n$$\n\\|(y - y_{d}) + \\epsilon \\delta y\\|_{L^{2}(\\Omega)}^{2} = \\|y-y_{d}\\|^{2} + 2\\epsilon\\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)} + \\epsilon^{2}\\|\\delta y\\|^{2} \\\\\n\\|u+\\epsilon h\\|_{L^{2}(\\Omega)}^{2} = \\|u\\|^{2} + 2\\epsilon\\langle u, h \\rangle_{L^{2}(\\Omega)} + \\epsilon^{2}\\|h\\|^{2}.\n$$\n将这些展开式代入 $j(u+\\epsilon h)$ 的表达式中，得到：\n$$\nj(u+\\epsilon h) = \\left(\\frac{1}{2}\\|y-y_{d}\\|^{2} + \\frac{\\alpha}{2}\\|u\\|^{2}\\right) + \\epsilon \\left( \\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)} + \\alpha \\langle u, h \\rangle_{L^{2}(\\Omega)} \\right) + O(\\epsilon^{2}).\n$$\n认识到第一项是 $j(u)$，我们求出方向导数：\n$$\nj'(u)h = \\lim_{\\epsilon \\to 0} \\frac{j(u+\\epsilon h) - j(u)}{\\epsilon} = \\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)} + \\alpha \\langle u, h \\rangle_{L^{2}(\\Omega)}.\n$$\n这个表达式目前涉及 $\\delta y$，它依赖于扰动方向 $h$。为了获得梯度，我们必须将此导数表示为某个函数与 $h$ 的内积。这通过引入一个伴随状态来实现。\n\n我们引入伴随状态 $p \\in H_{0}^{1}(\\Omega)$ 作为辅助问题（即伴随方程）的弱解。该方程的目的是用一个含 $h$ 的项来替换含 $\\delta y$ 的项。我们将伴随方程定义为：\n$$\n-\\Delta p = y - y_d \\quad \\text{在 } \\Omega \\text{ 内}, \\qquad p = 0 \\quad \\text{在 } \\partial \\Omega \\text{ 上}.\n$$\n这个 PDE 的源项被精确地选择为内积项 $\\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)}$ 中 $\\delta y$ 的前置因子。伴随方程的弱形式为：求 $p \\in H_{0}^{1}(\\Omega)$，使得对于所有测试函数 $w \\in H_{0}^{1}(\\Omega)$，\n$$\n\\int_{\\Omega} \\nabla p \\cdot \\nabla w \\,d\\mathbf{x} = \\int_{\\Omega} (y-y_{d}) w \\,d\\mathbf{x}.\n$$\n\n现在，我们使用弱形式来变换项 $\\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)}$。\n根据内积的定义，我们有 $\\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)} = \\int_{\\Omega} (y-y_{d}) \\delta y \\,d\\mathbf{x}$。\n在伴随方程的弱形式中，我们可以选择测试函数 $w = \\delta y$，因为 $\\delta y \\in H_{0}^{1}(\\Omega)$。这得到：\n$$\n\\int_{\\Omega} \\nabla p \\cdot \\nabla (\\delta y) \\,d\\mathbf{x} = \\int_{\\Omega} (y-y_{d}) \\delta y \\,d\\mathbf{x} = \\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)}.\n$$\n接下来，我们转向灵敏度方程的弱形式。我们可以自由选择任何测试函数 $v \\in H_{0}^{1}(\\Omega)$。由于伴随状态 $p$ 在 $H_{0}^{1}(\\Omega)$ 中，我们可以选择 $v=p$。这给出：\n$$\n\\int_{\\Omega} \\nabla (\\delta y) \\cdot \\nabla p \\,d\\mathbf{x} = \\int_{\\Omega} h p \\,d\\mathbf{x} = \\langle h, p \\rangle_{L^{2}(\\Omega)}.\n$$\n根据点积的对称性，$\\nabla p \\cdot \\nabla (\\delta y) = \\nabla (\\delta y) \\cdot \\nabla p$。因此，我们建立了以下关键恒等式：\n$$\n\\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)} = \\langle p, h \\rangle_{L^{2}(\\Omega)}.\n$$\n这个使用弱形式的证明，是应用 Green 恒等式的严格对应方法，因为弱形式本身就是通过分部积分（Green 恒等式的一种形式）推导出来的，并通过将函数空间限制在 $H_{0}^{1}(\\Omega)$ 来包含边界条件。\n\n将此恒等式代回到我们的方向导数 $j'(u)h$ 的表达式中：\n$$\nj'(u)h = \\langle p, h \\rangle_{L^{2}(\\Omega)} + \\alpha \\langle u, h \\rangle_{L^{2}(\\Omega)}.\n$$\n利用内积的线性，我们合并这些项：\n$$\nj'(u)h = \\langle p + \\alpha u, h \\rangle_{L^{2}(\\Omega)}.\n$$\n现在，它已是所需的形式 $j'(u)h = \\langle g, h \\rangle_{L^{2}(\\Omega)}$，其中 $g = p + \\alpha u$。根据 Riesz 表示定理，泛函 $j(u)$ 在 Hilbert 空间 $L^{2}(\\Omega)$ 中的梯度就是这个函数 $g$。\n因此，约化泛函的梯度是\n$$\n\\nabla j(u) = p + \\alpha u,\n$$\n其中 $p$ 是定义为 $-\\Delta p = y - y_d$ 在 $H_0^1(\\Omega)$ 中的弱解的伴随状态，而 $y$ 是状态变量，即 $-\\Delta y = u$ 在 $H_0^1(\\Omega)$ 中的弱解。", "answer": "$$\n\\boxed{p + \\alpha u}\n$$", "id": "3409520"}, {"introduction": "在掌握了伴随法的基本原理之后，我们将进入一个重要的实际应用领域：四维变分数据同化（4D-Var）。这个练习[@problem_id:3409490]模拟了气象预报等领域中的典型场景，要求您处理时间离散模型，并推导增量4D-Var方法中的正规方程。通过这个练习，您将把连续理论与大规模实际计算联系起来，并接触到线性化和预处理等在求解实际逆问题时至关重要的技术。", "problem": "考虑一个强约束四维变分(4D-Var)资料同化问题，其状态向量为 $x \\in \\mathbb{R}^{n}$，由一个在同化时间 $t_{0}, t_{1}, \\dots, t_{N}$ 上时间离散的偏微分方程(PDE)模型控制。从时间步 $k$ 到 $k+1$ 的受PDE约束的离散模型映射表示为 $m_{k}: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$，并假设模型是完美的（无模型误差），因此 $x_{k+1} = m_{k}(x_{k})$。在时间 $t_{k}$ 的观测 $y_{k} \\in \\mathbb{R}^{p_{k}}$ 通过一个（通常为非线性的）观测算子 $h_{k}: \\mathbb{R}^{n} \\to \\mathbb{R}^{p_{k}}$ 与模型状态相关联，并带有加性观测误差 $\\varepsilon_{k}$，该误差服从零均值、协方差为对称正定矩阵 $R_{k} \\in \\mathbb{R}^{p_{k} \\times p_{k}}$ 的高斯分布。背景（也称先验）状态 $x_{b} \\in \\mathbb{R}^{n}$ 具有高斯误差，其均值为零，协方差为对称正定矩阵 $B \\in \\mathbb{R}^{n \\times n}$。初始状态 $x_{0}$ 的代价函数为\n$$\nJ(x_{0}) = \\tfrac{1}{2} \\,(x_{0} - x_{b})^{\\top} B^{-1} (x_{0} - x_{b}) \\;+\\; \\tfrac{1}{2} \\sum_{k=0}^{N} \\big(h_{k}(x_{k}) - y_{k}\\big)^{\\top} R_{k}^{-1} \\big(h_{k}(x_{k}) - y_{k}\\big),\n$$\n其中 $x_{k}$ 通过模型积分 $x_{k} = M_{0,k}(x_{0})$ 获得，其中对于 $k \\ge 1$，$M_{0,k} = m_{k-1} \\circ \\cdots \\circ m_{0}$，而 $M_{0,0} = \\mathrm{Id}$。\n\n设 $\\{x_{k}^{t}\\}_{k=0}^{N}$ 是从当前迭代值 $x_{0}^{t}$ 开始通过积分模型得到的参考轨迹。定义切线性模型矩阵 $M_{k} = \\nabla m_{k}(x_{k}^{t}) \\in \\mathbb{R}^{n \\times n}$ 及其乘积 $M_{0,k} = M_{k-1} \\cdots M_{0}$ (对于 $k \\ge 1$)，其中 $M_{0,0} = I$。定义线性化观测矩阵 $H_{k} = \\nabla h_{k}(x_{k}^{t}) \\in \\mathbb{R}^{p_{k} \\times n}$ 以及在轨迹处的新息 $d_{k} = y_{k} - h_{k}(x_{k}^{t}) \\in \\mathbb{R}^{p_{k}}$。考虑 $J$ 在该轨迹周围的高斯-牛顿线性化，得到关于控制增量 $\\delta x_{0} \\in \\mathbb{R}^{n}$ 的增量代价：\n$$\nJ_{\\mathrm{inc}}(\\delta x_{0}) \\;=\\; \\tfrac{1}{2}\\,\\delta x_{0}^{\\top} B^{-1} \\delta x_{0} \\;+\\; \\tfrac{1}{2} \\sum_{k=0}^{N} \\big( H_{k} \\, M_{0,k} \\, \\delta x_{0} - d_{k} \\big)^{\\top} R_{k}^{-1} \\big( H_{k} \\, M_{0,k} \\, \\delta x_{0} - d_{k} \\big).\n$$\n通过对称正定平方根 $B^{1/2}$ 引入背景预处理，令 $\\delta x_{0} = B^{1/2} v$，其中 $v \\in \\mathbb{R}^{n}$ 为预处理后的控制增量。\n\n从上述定义和高斯-牛顿近似（忽略超出了一阶线性化的观测模型非线性的二阶项）出发，推导 $v$ 的正规方程，并将其表示为对称正定线性系统的形式。您的推导必须从给定的代价函数和切线性化开始，并通过一阶最优性条件进行。然后，仅用已知量 $B^{1/2}$、$M_{0,k}$、$H_{k}$、$R_{k}$ 和 $d_{k}$ 写出预处理后的正规方程。\n\n答案规格：\n- 您的最终答案必须是由预处理高斯-牛顿Hessian算子和正规方程的右端项组成的对，且仅以指定符号的解析矩阵/向量表达式表示。在最终的方框表达式中不要包含任何等号或赋值号。\n- 使用 $\\mathrm{pmatrix}$ 环境将该对表示为一个双元素的行矩阵，第一个元素是算子，第二个元素是右端向量。\n- 不需要进行数值计算，也不涉及任何单位。", "solution": "用户提供了一个问题陈述，经检验，该问题在科学上是合理的、适定的、客观的且内部一致的。该问题要求从一个给定的增量代价函数出发，推导4D-Var资料同化问题的预处理正规方程。\n\n分析始于所提供的增量代价函数 $J_{\\mathrm{inc}}(\\delta x_{0})$，它是完整代价函数在参考轨迹周围的二次近似。该函数需要相对于初始状态增量 $\\delta x_{0} \\in \\mathbb{R}^{n}$ 进行最小化。该函数为：\n$$ J_{\\mathrm{inc}}(\\delta x_{0}) = \\frac{1}{2} \\delta x_{0}^{\\top} B^{-1} \\delta x_{0} + \\frac{1}{2} \\sum_{k=0}^{N} \\big( H_{k} M_{0,k} \\delta x_{0} - d_{k} \\big)^{\\top} R_{k}^{-1} \\big( H_{k} M_{0,k} \\delta x_{0} - d_{k} \\big) $$\n这里，$\\delta x_{0}$ 是控制变量。我们引入预处理来改善最小化问题的条件数。预处理通过变量替换 $\\delta x_{0} = B^{1/2} v$ 来定义，其中 $v \\in \\mathbb{R}^{n}$ 是新的预处理控制变量，$B^{1/2}$ 是背景误差协方差矩阵 $B$ 的对称正定平方根。\n\n我们将此变量替换代入代价函数 $J_{\\mathrm{inc}}$，得到预处理后的代价函数，记为 $J_{p}(v) = J_{\\mathrm{inc}}(B^{1/2} v)$。\n\n背景项变为：\n$$ \\frac{1}{2} (B^{1/2} v)^{\\top} B^{-1} (B^{1/2} v) = \\frac{1}{2} v^{\\top} (B^{1/2})^{\\top} B^{-1} B^{1/2} v $$\n由于 $B$ 是对称的，$B^{1/2}$ 也是对称的，因此 $(B^{1/2})^{\\top} = B^{1/2}$。此外，$B^{-1} = (B^{1/2}B^{1/2})^{-1} = (B^{1/2})^{-1}(B^{1/2})^{-1}$。我们用 $B^{-1/2}$ 表示 $(B^{1/2})^{-1}$。该表达式简化为：\n$$ \\frac{1}{2} v^{\\top} B^{1/2} B^{-1/2} B^{-1/2} B^{1/2} v = \\frac{1}{2} v^{\\top} I I v = \\frac{1}{2} v^{\\top} v $$\n\n对于每个时间索引 $k$ 的观测项变为：\n$$ \\frac{1}{2} \\big( H_{k} M_{0,k} (B^{1/2} v) - d_{k} \\big)^{\\top} R_{k}^{-1} \\big( H_{k} M_{0,k} B^{1/2} v - d_{k} \\big) $$\n\n将所有项相加，预处理后的代价函数 $J_{p}(v)$ 为：\n$$ J_{p}(v) = \\frac{1}{2} v^{\\top} v + \\frac{1}{2} \\sum_{k=0}^{N} \\big( H_{k} M_{0,k} B^{1/2} v - d_{k} \\big)^{\\top} R_{k}^{-1} \\big( H_{k} M_{0,k} B^{1/2} v - d_{k} \\big) $$\n为了找到这个二次函数的最小值，我们计算它关于 $v$ 的梯度，并将其设为零向量。这是一阶最优性条件。我们使用以下标准矩阵微积分恒等式：对于向量 $u$、对称矩阵 $A$ 和向量 $c$，有 $\\nabla_{u}(\\frac{1}{2} u^{\\top} A u) = A u$ 和 $\\nabla_{u}((Au - c)^{\\top}W(Au-c)) = 2A^{\\top}W(Au-c)$。\n\n背景项 $\\frac{1}{2} v^{\\top} v$ 关于 $v$ 的梯度就是 $v$。\n\n对于观测部分，我们将乘以 $v$ 的算子表示为 $\\mathcal{A}_{k} = H_{k} M_{0,k} B^{1/2}$。和式中第 $k$ 项的梯度是：\n$$ \\nabla_{v} \\left[ \\frac{1}{2} (\\mathcal{A}_{k} v - d_{k})^{\\top} R_{k}^{-1} (\\mathcal{A}_{k} v - d_{k}) \\right] = \\mathcal{A}_{k}^{\\top} R_{k}^{-1} (\\mathcal{A}_{k} v - d_{k}) $$\n$$ = (\\mathcal{A}_{k}^{\\top} R_{k}^{-1} \\mathcal{A}_{k}) v - \\mathcal{A}_{k}^{\\top} R_{k}^{-1} d_{k} $$\n完整代价函数 $\\nabla_{v} J_{p}(v)$ 的梯度是其所有项梯度的总和：\n$$ \\nabla_{v} J_{p}(v) = v + \\sum_{k=0}^{N} \\left[ (\\mathcal{A}_{k}^{\\top} R_{k}^{-1} \\mathcal{A}_{k}) v - \\mathcal{A}_{k}^{\\top} R_{k}^{-1} d_{k} \\right] $$\n我们可以将乘以 $v$ 的项和常数项分组：\n$$ \\nabla_{v} J_{p}(v) = \\left( I + \\sum_{k=0}^{N} \\mathcal{A}_{k}^{\\top} R_{k}^{-1} \\mathcal{A}_{k} \\right) v - \\sum_{k=0}^{N} \\mathcal{A}_{k}^{\\top} R_{k}^{-1} d_{k} $$\n将梯度设为零，即 $\\nabla_{v} J_{p}(v) = 0$，得到关于 $v$ 的正规方程：\n$$ \\left( I + \\sum_{k=0}^{N} \\mathcal{A}_{k}^{\\top} R_{k}^{-1} \\mathcal{A}_{k} \\right) v = \\sum_{k=0}^{N} \\mathcal{A}_{k}^{\\top} R_{k}^{-1} d_{k} $$\n该方程的形式为 $\\mathcal{H}_{p} v = g_{p}$，其中 $\\mathcal{H}_{p}$ 是预处理Hessian算子，$g_{p}$ 是预处理右端向量。\n\n我们现在代回定义 $\\mathcal{A}_{k} = H_{k} M_{0,k} B^{1/2}$，以给定的矩阵和向量表示这些量。\n\n预处理Hessian算子是：\n$$ \\mathcal{H}_{p} = I + \\sum_{k=0}^{N} (H_{k} M_{0,k} B^{1/2})^{\\top} R_{k}^{-1} (H_{k} M_{0,k} B^{1/2}) $$\n利用性质 $(ABC)^{\\top} = C^{\\top}B^{\\top}A^{\\top}$ 和 $B^{1/2}$ 的对称性，即 $(B^{1/2})^{\\top} = B^{1/2}$：\n$$ \\mathcal{H}_{p} = I + \\sum_{k=0}^{N} B^{1/2} M_{0,k}^{\\top} H_{k}^{\\top} R_{k}^{-1} H_{k} M_{0,k} B^{1/2} $$\n我们可以从求和项的左右两侧提出因子 $B^{1/2}$ 以获得最终形式：\n$$ \\mathcal{H}_{p} = I + B^{1/2} \\left( \\sum_{k=0}^{N} M_{0,k}^{\\top} H_{k}^{\\top} R_{k}^{-1} H_{k} M_{0,k} \\right) B^{1/2} $$\n\n预处理右端向量是：\n$$ g_{p} = \\sum_{k=0}^{N} (H_{k} M_{0,k} B^{1/2})^{\\top} R_{k}^{-1} d_{k} = \\sum_{k=0}^{N} B^{1/2} M_{0,k}^{\\top} H_{k}^{\\top} R_{k}^{-1} d_{k} $$\n提出因子 $B^{1/2}$ 得到：\n$$ g_{p} = B^{1/2} \\left( \\sum_{k=0}^{N} M_{0,k}^{\\top} H_{k}^{\\top} R_{k}^{-1} d_{k} \\right) $$\n这些 $\\mathcal{H}_{p}$ 和 $g_{p}$ 的表达式是预处理正规方程的分量，且仅用问题陈述中指定的量来表示。算子 $\\mathcal{H}_{p}$ 是对称正定的，因为它是单位矩阵（对称正定）与一个对称半正定矩阵之和。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nI + B^{1/2} \\left( \\sum_{k=0}^{N} M_{0,k}^{\\top} H_{k}^{\\top} R_{k}^{-1} H_{k} M_{0,k} \\right) B^{1/2} & B^{1/2} \\sum_{k=0}^{N} M_{0,k}^{\\top} H_{k}^{\\top} R_{k}^{-1} d_{k}\n\\end{pmatrix}\n}\n$$", "id": "3409490"}, {"introduction": "最后的这个练习将带您进入优化的前沿领域——不确定性下的优化。您将处理一个模型参数随机的偏微分方程约束问题，目标是利用风险规避度量“条件风险价值”（CVaR）找到一个鲁棒的控制策略[@problem_id:3409489]。这要求您为一个非光滑的目标函数推导次梯度，从而亲身体验随机优化与鲁棒优化中强大的数学工具。", "problem": "考虑一个有界 Lipschitz 域 $\\Omega \\subset \\mathbb{R}^{d}$，$d \\in \\{2,3\\}$，并设偏微分方程 (PDE) 的状态 $u(\\cdot;\\xi,m) \\in H_{0}^{1}(\\Omega)$ 是线性椭圆边值问题弱形式的解\n$$\n-\\nabla \\cdot \\big(a(x,\\xi)\\nabla u(x;\\xi,m)\\big) \\;=\\; q(x) + m(x) \\quad \\text{in } \\Omega,\\qquad u(\\cdot;\\xi,m)=0 \\quad \\text{on } \\partial\\Omega,\n$$\n其中 $a(\\cdot,\\xi)$ 是一个随机、一致椭圆的系数场，$\\xi$ 是一个随机参数，$q \\in L^{2}(\\Omega)$ 是一个确定性源项，控制 $m \\in L^{2}(\\Omega)$ 是一个分布式强迫项。设 $B:H_{0}^{1}(\\Omega)\\to\\mathbb{R}^{p}$ 是一个有界线性观测算子，$d\\in\\mathbb{R}^{p}$ 是固定数据。定义数据失配泛函\n$$\nJ(m,\\xi)\\;=\\;\\tfrac{1}{2}\\,\\|B\\,u(\\cdot;\\xi,m)-d\\|_{2}^{2}.\n$$\n采用一致性风险理论中水平为 $\\tau\\in(0,1)$ 的条件风险价值 (CVaR) 作为 $J(m,\\xi)$ 的风险度量。假设 $\\{\\,\\xi_{i}\\,\\}_{i=1}^{N}$ 是从 $\\xi$ 的分布中抽取的独立同分布样本，并假设 $a(\\cdot,\\xi)$ 满足标准的一致椭圆性和有界性假设，以确保状态和伴随 PDE 的适定性。\n\n通过执行以下任务，建立一个针对 $\\xi$ 中不确定性的风险规避的随机 PDE 约束鲁棒优化公式：\n\n- 从条件风险价值的基本定义出发，使用样本 $\\{\\,\\xi_{i}\\,\\}_{i=1}^{N}$ 为数据失配 $J(m,\\xi)$ 推导风险规避目标的样本均值近似。\n\n- 对于得到的样本均值目标，推导关于控制 $m \\in L^{2}(\\Omega)$ 的次梯度，该次梯度应完全用 PDE 的解和线性算子表示，并明确指出必须为每个样本求解的伴随 PDE。\n\n你的最终答案必须以解析表达式而非数值的形式给出。推导过程中不允许使用外部参考文献或快捷公式。所有数学实体都必须用 LaTeX 书写。最终加框的答案必须包含样本均值近似和关于 $m$ 的次梯度，并使用 $\\mathrm{pmatrix}$ 环境表示为单行矩阵。", "solution": "问题需要进行验证。\n\n### 步骤 1：提取已知条件\n-   域：$\\Omega \\subset \\mathbb{R}^{d}$，$d \\in \\{2,3\\}$，是一个有界 Lipschitz 域。\n-   状态方程：$u(\\cdot;\\xi,m) \\in H_{0}^{1}(\\Omega)$ 是 $-\\nabla \\cdot \\big(a(x,\\xi)\\nabla u(x;\\xi,m)\\big) = q(x) + m(x)$ 在 $\\Omega$ 中弱形式的解，边界条件为 $u(\\cdot;\\xi,m)=0$ 在 $\\partial\\Omega$上。\n-   系数场：$a(\\cdot,\\xi)$ 是一个随机、一致椭圆的系数场，其中 $\\xi$ 是一个随机参数。假设其满足标准的一致椭圆性和有界性假设。\n-   源项：$q \\in L^{2}(\\Omega)$ 是一个确定性源项，而 $m \\in L^{2}(\\Omega)$ 是一个分布式控制（强迫项）。\n-   观测：$B:H_{0}^{1}(\\Omega)\\to\\mathbb{R}^{p}$ 是一个有界线性观测算子，$d\\in\\mathbb{R}^{p}$ 是一个固定数据向量。\n-   数据失配泛函：$J(m,\\xi) = \\tfrac{1}{2}\\,\\|B\\,u(\\cdot;\\xi,m)-d\\|_{2}^{2}$。\n-   风险度量：置信水平为 $\\tau\\in(0,1)$ 的条件风险价值 (CVaR)。\n-   样本：$\\{\\,\\xi_{i}\\,\\}_{i=1}^{N}$ 是从 $\\xi$ 的分布中抽取的独立同分布 (i.i.d.) 样本。\n\n### 步骤 2：使用提取的已知条件进行验证\n-   **科学依据**：该问题位于不确定性下 PDE 约束优化的成熟领域，也称为鲁棒优化。在该领域，使用条件风险价值 (CVaR) 作为一致性风险度量是标准做法。控制方程是一个线性椭圆 PDE，是数学物理和工程中的一个典型模型。所有概念都在泛函分析和优化理论中被严格定义。\n-   **适定性**：该问题要求推导目标函数及其次梯度。这是一个明确定义的数学任务。$a(\\cdot,\\xi)$ 的一致椭圆性和有界性假设通过 Lax-Milgram 定理确保了状态和伴随 PDE 的适定性，这是后续推导的必要条件。\n-   **客观性**：问题使用精确、无歧义的数学术语陈述。没有主观或基于意见的成分。\n-   **完整性和一致性**：问题为进行推导提供了所有必要的定义和背景。所提供的信息中没有明显的矛盾。\n\n### 步骤 3：结论与行动\n该问题被认为是**有效的**。这是关于由偏微分方程控制系统的风险规避优化理论中的一个标准但高级的练习。现在开始求解过程。\n\n### 推导\n\n目标是为控制 $m$ 建立一个风险规避优化问题，并为其解推导必要的表达式。风险规避是针对系数场 $a(x, \\xi)$ 中的不确定性，该不确定性由随机参数 $\\xi$ 捕捉。数据失配 $J(m, \\xi)$ 是一个随机变量，我们的目标是最小化其风险，该风险由条件风险价值度量。\n\n#### 任务 1：CVaR 目标的样本均值近似\n\n一个实值随机变量 $X$ 在置信水平 $\\tau \\in (0,1)$ 下的条件风险价值 (CVaR) 定义为在 $X$ 超过其风险价值 (VaR) 条件下的期望值，其中 $\\text{VaR}_{\\tau}(X)$ 是 $X$ 分布的 $\\tau$-分位数。Rockafellar 和 Uryasev 的一个基本结果为优化目的提供了一个更方便的表示：\n$$\n\\text{CVaR}_{\\tau}(X) = \\inf_{t \\in \\mathbb{R}} \\left( t + \\frac{1}{1-\\tau} \\mathbb{E}\\left[ (X - t)_+ \\right] \\right),\n$$\n其中 $(z)_+ = \\max(0, z)$。在关于 $X_i$ 分布的某些条件下，下确界在 $t = \\text{VaR}_{\\tau}(X)$ 处达到。\n\n在我们的问题中，随机变量是数据失配泛函 $X = J(m,\\xi)$。风险规避优化问题是找到一个控制 $m$，使得 $J(m,\\xi)$ 的 CVaR 最小化。这可以被构建为一个关于控制 $m \\in L^2(\\Omega)$ 和辅助标量变量 $t \\in \\mathbb{R}$ 的联合最小化问题：\n$$\n\\min_{m \\in L^2(\\Omega), t \\in \\mathbb{R}} \\left( t + \\frac{1}{1-\\tau} \\mathbb{E}\\left[ (J(m,\\xi) - t)_+ \\right] \\right).\n$$\n期望 $\\mathbb{E}[\\cdot]$ 是关于 $\\xi$ 的概率测度计算的。由于我们给定了一个有限的独立同分布样本 $\\{\\xi_i\\}_{i=1}^N$，我们可以使用样本均值来近似该期望。这就得到了风险规避目标函数的样本均值近似 (SAA)，记为 $\\mathcal{R}_N(m,t)$：\n$$\n\\mathcal{R}_N(m,t) = t + \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\left( J(m,\\xi_i) - t \\right)_+.\n$$\n设 $u_i = u(\\cdot;m,\\xi_i)$ 是在给定控制 $m$ 和样本 $\\xi_i$ 下状态 PDE 的解。失配为 $J(m,\\xi_i) = \\frac{1}{2}\\|B u_i - d\\|_2^2$。将此代入 SAA 目标，我们得到第一个任务的最终形式：\n$$\n\\mathcal{R}_N(m,t) = t + \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\max\\left(0, \\frac{1}{2}\\|B u(\\cdot;m,\\xi_i) - d\\|_2^2 - t\\right).\n$$\n\n#### 任务 2：SAA 目标关于 $m$ 的次梯度\n\n为了求解优化问题 $\\min_{m,t} \\mathcal{R}_N(m,t)$，通常使用基于梯度的方法。由于函数 $\\max(0,z)$ 在 $z=0$ 处不可微，我们必须使用次梯度。我们被要求推导 $\\mathcal{R}_N(m,t)$ 关于控制 $m \\in L^2(\\Omega)$ 的一个次梯度。\n\n目标函数为 $\\mathcal{R}_N(m,t) = t + \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\max(0, J(m,\\xi_i) - t)$。\n项 $t$ 相对于 $m$ 是常数。根据次微分的线性性，$\\mathcal{R}_N$ 关于 $m$ 的一个次梯度，记为 $g_m \\in \\partial_m \\mathcal{R}_N(m,t)$，由下式给出：\n$$\ng_m = \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} g_{m,i},\n$$\n其中 $g_{m,i} \\in \\partial_m \\max(0, J(m,\\xi_i) - t)$。\n\n我们应用次微分的链式法则。对于一个复合函数 $f(g(x))$，其中 $f$ 是凸函数，$g$ 是可微函数，我们有 $\\partial (f \\circ g)(x) = \\partial f(g(x)) \\cdot \\nabla g(x)$。\n这里，令 $f(z) = \\max(0,z)$ 且 $g_i(m) = J(m,\\xi_i) - t$。$f(z)$ 的次微分是：\n$$\n\\partial f(z) = \\partial (\\cdot)_+|_z =\n\\begin{cases}\n    \\{0\\}        & \\text{if } z  0 \\\\\n    [0, 1]      \\text{if } z = 0 \\\\\n    \\{1\\}         \\text{if } z > 0\n\\end{cases}\n$$\n$g_i(m)$ 关于 $m$ 的梯度就是 $\\nabla_m J(m,\\xi_i)$。\n因此，一个次梯度 $g_{m,i}$ 由 $g_{m,i} = \\lambda_i \\nabla_m J(m,\\xi_i)$ 给出，其中 $\\lambda_i \\in \\partial (\\cdot)_+|_{J(m,\\xi_i)-t}$。即，\n$$\n\\lambda_i \\in\n\\begin{cases}\n    \\{1\\}         \\text{if } J(m,\\xi_i) > t \\\\\n    \\{0\\}         \\text{if } J(m,\\xi_i)  t \\\\\n    [0, 1]      \\text{if } J(m,\\xi_i) = t\n\\end{cases}\n$$\n因此，SAA 目标关于 $m$ 的一个次梯度是：\n$$\ng_m = \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\lambda_i \\nabla_m J(m,\\xi_i),\n$$\n对于满足上述条件的任意 $\\lambda_i$ 选择。\n\n最后一步是使用伴随方法推导 Fréchet 导数（梯度）$\\nabla_m J(m,\\xi_i)$ 的表达式。对于每个样本 $i \\in \\{1,\\dots,N\\}$，令 $a_i(x) = a(x,\\xi_i)$, $u_i(x) = u(x;m,\\xi_i)$。状态 $u_i \\in H_0^1(\\Omega)$ 满足状态方程的弱形式：\n$$\n\\int_{\\Omega} a_i(x) \\nabla u_i \\cdot \\nabla v \\,dx = \\int_{\\Omega} (q(x)+m(x))v(x) \\,dx, \\quad \\forall v \\in H_0^1(\\Omega).\n$$\n失配为 $J(m,\\xi_i) = \\frac{1}{2}\\|B u_i - d\\|_2^2$。其关于 $m$ 在方向 $\\delta m \\in L^2(\\Omega)$ 上的导数由下式给出：\n$$\n\\langle \\nabla_m J(m,\\xi_i), \\delta m \\rangle_{L^2} = \\langle B u_i - d, B\\dot{u}_i \\rangle_{\\mathbb{R}^p},\n$$\n其中 $\\dot{u}_i = \\frac{du_i}{dm}[\\delta m] \\in H_0^1(\\Omega)$ 是状态灵敏度。将状态方程关于 $m$ 在方向 $\\delta m$ 上求导，得到 $\\dot{u}_i$ 的灵敏度方程：\n$$\n\\int_{\\Omega} a_i(x) \\nabla \\dot{u}_i \\cdot \\nabla v \\,dx = \\int_{\\Omega} \\delta m(x) v(x) \\,dx, \\quad \\forall v \\in H_0^1(\\Omega).\n$$\n为了避免为每个 $\\delta m$ 求解 $\\dot{u}_i$，我们引入一个伴随状态 $p_i \\in H_0^1(\\Omega)$。伴随方程的定义是为了将失配导数与伴随变量关联起来。我们可以将失配导数写为 $\\langle B u_i - d, B\\dot{u}_i \\rangle_{\\mathbb{R}^p} = \\langle B^*(B u_i - d), \\dot{u}_i \\rangle$，其中 $B^*:\\mathbb{R}^p \\to (H_0^1(\\Omega))^*$ 是 $B$ 的伴随算子。我们为 $p_i$ 定义伴随方程，使得对于任意 $w \\in H_0^1(\\Omega)$：\n$$\n\\int_{\\Omega} a_i(x) \\nabla w \\cdot \\nabla p_i \\,dx = \\langle B^*(B u_i - d), w \\rangle_{(H_0^1)^*, H_0^1}.\n$$\n左边的双线性形式是对称的，所以 $\\int a_i \\nabla w \\cdot \\nabla p_i \\,dx = \\int a_i \\nabla p_i \\cdot \\nabla w \\,dx$。这个伴随 PDE 的强形式是：\n$$\n-\\nabla \\cdot(a_i(x) \\nabla p_i(x)) = B^*(B u_i - d) \\quad \\text{in } \\Omega, \\qquad p_i = 0 \\quad \\text{on } \\partial\\Omega.\n$$\n现在，在伴随方程的弱形式中令 $w=\\dot{u}_i$，我们得到：\n$$\n\\langle \\nabla_m J(m,\\xi_i), \\delta m \\rangle_{L^2} = \\langle B^*(B u_i - d), \\dot{u}_i \\rangle = \\int_{\\Omega} a_i(x) \\nabla \\dot{u}_i \\cdot \\nabla p_i \\,dx.\n$$\n并且，在灵敏度方程的弱形式中令 $v=p_i$，我们得到：\n$$\n\\int_{\\Omega} a_i(x) \\nabla \\dot{u}_i \\cdot \\nabla p_i \\,dx = \\int_{\\Omega} \\delta m(x) p_i(x) \\,dx = \\langle p_i, \\delta m \\rangle_{L^2(\\Omega)}.\n$$\n令这两个表达式相等，我们得到 $\\langle \\nabla_m J(m,\\xi_i), \\delta m \\rangle_{L^2} = \\langle p_i, \\delta m \\rangle_{L^2}$。由于这对所有 $\\delta m \\in L^2(\\Omega)$ 都成立，我们可以将梯度确定为：\n$$\n\\nabla_m J(m,\\xi_i) = p_i.\n$$\n将此结果代回到 $g_m$ 的表达式中，我们得到 $\\mathcal{R}_N(m,t)$ 关于 $m$ 的一个次梯度：\n$$\ng_m = \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\lambda_i p_i,\n$$\n其中对于每个 $i=1, \\dots, N$，$p_i$ 是上面定义的伴随 PDE 的解，而它又依赖于状态 PDE 的解 $u_i$。系数 $\\lambda_i$ 从先前定义的 $\\max(0,\\cdot)$ 函数的次微分中选取。\n\n推导至此完成。结果总结在最终答案中。", "answer": "$$\n\\boxed{\\begin{pmatrix} t + \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\max\\left(0, \\frac{1}{2}\\|B u(\\cdot;m,\\xi_i) - d\\|_2^2 - t\\right)  \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\lambda_i p_i \\end{pmatrix}}\n$$", "id": "3409489"}]}