{"hands_on_practices": [{"introduction": "许多学习型迭代格式是对经典算法（如邻近梯度下降法，Proximal Gradient Descent, PGD）的直接修改。这些迭代方法的收敛性由其更新算子的谱特性决定。本练习提供了一个动手实践的机会，要求您推导经典方案和学习型方案的更新算子，并通过分析其特征值来量化比较它们的稳定性，从而在优化理论和深度学习实践之间架起一座桥梁 [@problem_id:3396250]。", "problem": "考虑带加性噪声的线性逆问题，其中观测数据建模为 $y = A x + \\epsilon$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是已知系统矩阵，$x \\in \\mathbb{R}^{n}$ 是未知状态，$\\epsilon \\in \\mathbb{R}^{n}$ 是加性噪声。为了恢复 $x$，通常最小化一个数据失配项和一个正则化项之和，定义一个形式如下的目标函数\n$$\nF(x) = \\frac{1}{2} \\| A x - y \\|_2^2 + \\lambda R(x),\n$$\n其中 $\\lambda > 0$ 是正则化参数，$R(x)$ 是一个惩罚项。在此问题中，惩罚项是二次的，由下式给出\n$$\nR(x) = \\frac{1}{2} \\| L x \\|_2^2,\n$$\n其中 $L$ 是一个已知的线性算子。\n\n近端梯度下降（PGD）方法根据以下规则形成迭代\n$$\nx^{k+1} = \\operatorname{prox}_{\\lambda R}\\!\\left( x^k - \\tau A^\\top (A x^k - y) \\right),\n$$\n其中 $\\tau > 0$ 是步长，$\\operatorname{prox}_{\\lambda R}$ 表示与 $\\lambda R$ 相关联的近端算子。一个学习型展开方案将近端算子替换为一个学习到的算子，该算子经过训练，例如，用以近似近端映射的作用。在此问题中，学习到的近端算子被假定为线性的，并用矩阵 $M \\in \\mathbb{R}^{n \\times n}$ 表示，因此学习型更新规则为\n$$\nx^{k+1} = M \\left( x^k - \\tau A^\\top (A x^k - y) \\right).\n$$\n\n您的任务是：\n- 从上述基本原理和核心定义出发，推导采用二次惩罚项的经典PGD方法和采用线性训练近端算子的学习型展开方案的线性更新算子。将每次迭代表示为仿射映射 $x^{k+1} = G x^k + c$ 的形式，并根据 $G$ 的谱给出不动点存在且稳定的条件。\n- 对于下面提供的每个测试用例，计算每个更新算子的特征值、它们的模长、谱半径，并判断不动点是否线性稳定。当且仅当谱半径严格小于1时，不动点被认为是线性稳定的。\n\n使用以下参数值的测试套件。在所有情况下，$n = 3$，范数 $\\| A \\|_2$ 表示 $A$ 的谱范数。\n\n情况 $\\mathbf{1}$ (良态，非扩张的学习型近端算子):\n- $$A_1 = \\begin{bmatrix} 1  0.2  0 \\\\ 0.2  0.8  0.1 \\\\ 0  0.1  0.5 \\end{bmatrix}, \\quad L_1 = I_3, \\quad \\lambda_1 = 0.5, \\quad M_1 = 0.9 I_3,$$\n- $$\\tau_1 = 0.9 \\cdot \\frac{2}{\\|A_1\\|_2^2}.$$\n\n情况 $\\mathbf{2}$ (接近阈值的步长，扩张的学习型近端算子):\n- $$A_2 = \\begin{bmatrix} 1  0.3  0 \\\\ 0.3  0.7  0.2 \\\\ 0  0.2  0.4 \\end{bmatrix}, \\quad L_2 = I_3, \\quad \\lambda_2 = 0.4, \\quad M_2 = 1.1 I_3,$$\n- $$\\tau_2 = 0.99 \\cdot \\frac{2}{\\|A_2\\|_2^2}.$$\n\n情况 $\\mathbf{3}$ (病态系统，精确的经典近端算子):\n- $$A_3 = \\begin{bmatrix} 1.5  -0.9  0 \\\\ -0.9  0.6  0.3 \\\\ 0  0.3  0.2 \\end{bmatrix}, \\quad L_3 = \\begin{bmatrix} -1  1  0 \\\\ 0  -1  1 \\end{bmatrix}, \\quad \\lambda_3 = 0.8,$$\n- 对于学习型近端算子，设置 $$M_3 = \\left(I_3 + \\lambda_3 L_3^\\top L_3 \\right)^{-1},$$\n- $$\\tau_3 = 0.7 \\cdot \\frac{2}{\\|A_3\\|_2^2}.$$\n\n情况 $\\mathbf{4}$ (无正则化，精确阈值，临界经典稳定性):\n- $$A_4 = \\begin{bmatrix} 0.9  0.2  0.1 \\\\ 0.2  0.5  0.05 \\\\ 0.1  0.05  0.3 \\end{bmatrix}, \\quad L_4 = 0_{3 \\times 3}, \\quad \\lambda_4 = 0.5, \\quad M_4 = 0.95 I_3,$$\n- $$\\tau_4 = 1.0 \\cdot \\frac{2}{\\|A_4\\|_2^2}.$$\n\n对于每种情况，构建经典的PGD更新矩阵\n$$\nG_{\\text{classical}} = S \\left( I_3 - \\tau A^\\top A \\right), \\quad \\text{其中} \\quad S = \\left( I_3 + \\lambda L^\\top L \\right)^{-1},\n$$\n和学习型更新矩阵\n$$\nG_{\\text{learned}} = M \\left( I_3 - \\tau A^\\top A \\right).\n$$\n\n对于每个 $G_{\\text{classical}}$ 和 $G_{\\text{learned}}$，计算：\n- 按降序排列的特征值模长列表。\n- 谱半径，定义为特征值中的最大模长。\n- 根据谱半径条件指示不动点线性稳定性的布尔值。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有情况的结果，形式为方括号括起来的逗号分隔列表。每个情况的结果必须是以下形式的列表\n$$\n[\\text{eig\\_mags\\_classical}, \\text{eig\\_mags\\_learned}, \\rho_{\\text{classical}}, \\rho_{\\text{learned}}, \\text{stable}_{\\text{classical}}, \\text{stable}_{\\text{learned}}],\n$$\n其中 $\\text{eig\\_mags\\_classical}$ 和 $\\text{eig\\_mags\\_learned}$ 是浮点数列表。例如，整个输出应如下所示\n$$\n[[\\ldots],[\\ldots],\\ldots].\n$$\n不应打印任何额外文本。不涉及角度，也不需要物理单位。所有数值应根据情况以浮点数或布尔值形式提供。", "solution": "该问题要求推导并分析两种用于求解线性逆问题的迭代方案的稳定性：经典的近端梯度下降（PGD）方法及其学习型展开变体。\n\n### 更新算子的推导\n\n如果一个形式为 $x^{k+1} = f(x^k)$ 的迭代方案可以表示为 $x^{k+1} = G x^k + c$（其中 $G$ 为常数矩阵，$c$ 为常数向量），则该方案是一个仿射映射。我们将为经典方法和学习型方法推导这种形式。\n\n#### 1. 经典近端梯度下降（PGD）\n\nPGD更新规则由下式给出：\n$$\nx^{k+1} = \\operatorname{prox}_{\\lambda R}\\!\\left( x^k - \\tau A^\\top (A x^k - y) \\right)\n$$\n令 $z^k = x^k - \\tau A^\\top (A x^k - y)$。则更新为 $x^{k+1} = \\operatorname{prox}_{\\lambda R}(z^k)$。\n\n近端算子 $\\operatorname{prox}_{\\gamma g}(v)$ 定义为函数 $u \\mapsto g(u) + \\frac{1}{2\\gamma} \\|u - v\\|_2^2$ 的唯一最小化子。在我们的例子中，需要最小化的函数是 $J(u)$，其中 $\\gamma=1$ 且 $g(u) = \\lambda R(u) = \\frac{\\lambda}{2} \\|Lu\\|_2^2$：\n$$\n\\operatorname{prox}_{\\lambda R}(z^k) = \\arg\\min_{u \\in \\mathbb{R}^n} \\left( \\frac{\\lambda}{2} \\|Lu\\|_2^2 + \\frac{1}{2} \\|u - z^k\\|_2^2 \\right)\n$$\n这是一个二次、严格凸的优化问题，因此存在唯一的最小化子。我们通过将其关于 $u$ 的梯度设为零来求解：\n$$\n\\nabla_u J(u) = \\nabla_u \\left( \\frac{\\lambda}{2} u^\\top L^\\top L u + \\frac{1}{2} (u - z^k)^\\top (u - z^k) \\right) = 0\n$$\n$$\n\\lambda L^\\top L u + (u - z^k) = 0\n$$\n整理以求解 $u$：\n$$\n(\\lambda L^\\top L + I) u = z^k\n$$\n其中 $I$ 是单位矩阵。由于 $L^\\top L$ 是半正定的，对于 $\\lambda > 0$，矩阵 $(I + \\lambda L^\\top L)$ 是正定的，因此是可逆的。解为：\n$$\nu = (I + \\lambda L^\\top L)^{-1} z^k\n$$\n我们定义矩阵 $S = (I + \\lambda L^\\top L)^{-1}$。因此，近端算子是一个线性变换：$\\operatorname{prox}_{\\lambda R}(z^k) = S z^k$。\n\n现在，我们将其代入PGD更新方程：\n$$\nx^{k+1} = S z^k = S \\left( x^k - \\tau A^\\top (A x^k - y) \\right)\n$$\n展开各项以分离 $x^k$ 和 $y$：\n$$\nx^{k+1} = S \\left( x^k - \\tau A^\\top A x^k + \\tau A^\\top y \\right)\n$$\n$$\nx^{k+1} = S (I - \\tau A^\\top A) x^k + S \\tau A^\\top y\n$$\n这是一个仿射映射 $x^{k+1} = G_{\\text{classical}} x^k + c_{\\text{classical}}$，其更新矩阵和常数向量由下式给出：\n$$\nG_{\\text{classical}} = S (I - \\tau A^\\top A) = (I + \\lambda L^\\top L)^{-1} (I - \\tau A^\\top A)\n$$\n$$\nc_{\\text{classical}} = S \\tau A^\\top y = (I + \\lambda L^\\top L)^{-1} \\tau A^\\top y\n$$\n$G_{\\text{classical}}$ 的表达式与问题描述中提供的一致。\n\n#### 2. 学习型展开方案\n\n学习型方案的更新规则直接给出：\n$$\nx^{k+1} = M \\left( x^k - \\tau A^\\top (A x^k - y) \\right)\n$$\n其中 $M$ 是一个学习到的矩阵。通过展开 $M$，我们可以立即识别出仿射形式：\n$$\nx^{k+1} = M \\left( (I - \\tau A^\\top A) x^k + \\tau A^\\top y \\right)\n$$\n$$\nx^{k+1} = M (I - \\tau A^\\top A) x^k + M \\tau A^\\top y\n$$\n这是一个仿射映射 $x^{k+1} = G_{\\text{learned}} x^k + c_{\\text{learned}}$，其中：\n$$\nG_{\\text{learned}} = M (I - \\tau A^\\top A)\n$$\n$$\nc_{\\text{learned}} = M \\tau A^\\top y\n$$\n$G_{\\text{learned}}$ 的表达式也与问题描述中提供的一致。\n\n### 不动点稳定性分析\n\n当且仅当迭代算子 $G$ 是一个压缩映射时，对于任何初始猜测 $x^0$，迭代方法 $x^{k+1} = G x^k + c$ 收敛到一个唯一的不动点 $x^*$。不动点 $x^*$ 满足方程 $x^* = G x^* + c$。\n\n为了分析线性稳定性，我们考虑误差 $e^k = x^k - x^*$ 的演化。从更新方程中减去不动点方程得到：\n$$\nx^{k+1} - x^* = (G x^k + c) - (G x^* + c) = G (x^k - x^*)\n$$\n$$\ne^{k+1} = G e^k\n$$\n这意味着 $e^k = G^k e^0$。对于任何初始误差 $e^0$，误差 $e^k$ 收敛到零的充要条件是当 $k \\to \\infty$ 时，矩阵的幂 $G^k$ 收敛到零矩阵。此条件满足的充要条件是 $G$ 的谱半径（记为 $\\rho(G)$）严格小于1。谱半径定义为 $G$ 的特征值模长的最大值：\n$$\n\\rho(G) = \\max_{i} |\\mu_i|\n$$\n其中 $\\mu_i$ 是 $G$ 的特征值。\n\n因此，当且仅当 $\\rho(G)  1$ 时，迭代方案的不动点是线性稳定的。每个案例的数值计算将基于构建相应的 $G$ 矩阵并评估其谱半径。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing the stability of classical and learned proximal\n    gradient descent for four test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"A\": np.array([[1, 0.2, 0], [0.2, 0.8, 0.1], [0, 0.1, 0.5]]),\n            \"L\": np.eye(3),\n            \"lambda_val\": 0.5,\n            \"M\": 0.9 * np.eye(3),\n            \"tau_prefactor\": 0.9,\n        },\n        {\n            \"A\": np.array([[1, 0.3, 0], [0.3, 0.7, 0.2], [0, 0.2, 0.4]]),\n            \"L\": np.eye(3),\n            \"lambda_val\": 0.4,\n            \"M\": 1.1 * np.eye(3),\n            \"tau_prefactor\": 0.99,\n        },\n        {\n            \"A\": np.array([[1.5, -0.9, 0], [-0.9, 0.6, 0.3], [0, 0.3, 0.2]]),\n            \"L\": np.array([[-1, 1, 0], [0, -1, 1]]),\n            \"lambda_val\": 0.8,\n            \"M\": None,  # M is derived from L and lambda\n            \"tau_prefactor\": 0.7,\n        },\n        {\n            \"A\": np.array([[0.9, 0.2, 0.1], [0.2, 0.5, 0.05], [0.1, 0.05, 0.3]]),\n            \"L\": np.zeros((3, 3)),\n            \"lambda_val\": 0.5,\n            \"M\": 0.95 * np.eye(3),\n            \"tau_prefactor\": 1.0,\n        },\n    ]\n\n    results_for_all_cases = []\n\n    for i, case in enumerate(test_cases):\n        A = case[\"A\"]\n        L = case[\"L\"]\n        lambda_val = case[\"lambda_val\"]\n        tau_prefactor = case[\"tau_prefactor\"]\n        n = A.shape[0]\n        I_n = np.eye(n)\n\n        # Calculate step size tau\n        norm_A_sq = np.linalg.norm(A, 2)**2\n        tau = tau_prefactor * (2 / norm_A_sq)\n\n        # Common term in both G matrices\n        common_term = I_n - tau * A.T @ A\n        \n        # --- Classical PGD ---\n        # Construct S = (I + lambda * L^T * L)^-1\n        L_T_L = L.T @ L\n        S = np.linalg.inv(I_n + lambda_val * L_T_L)\n        \n        # Construct G_classical\n        G_classical = S @ common_term\n        \n        # Analyze G_classical\n        eigvals_c = np.linalg.eigvals(G_classical)\n        mags_c = np.abs(eigvals_c)\n        mags_c_sorted = -np.sort(-mags_c) # Sort descending\n        rho_c = mags_c_sorted[0]\n        stable_c = rho_c  1.0\n\n        # --- Learned Scheme ---\n        # Construct M\n        if i == 2:  # Case 3\n            M = S\n        else:\n            M = case[\"M\"]\n        \n        # Construct G_learned\n        G_learned = M @ common_term\n        \n        # Analyze G_learned\n        eigvals_l = np.linalg.eigvals(G_learned)\n        mags_l = np.abs(eigvals_l)\n        mags_l_sorted = -np.sort(-mags_l) # Sort descending\n        rho_l = mags_l_sorted[0]\n        stable_l = rho_l  1.0\n        \n        # Format the output for the current case\n        mags_c_list_str = f\"[{','.join([f'{m:.8f}' for m in mags_c_sorted])}]\"\n        mags_l_list_str = f\"[{','.join([f'{m:.8f}' for m in mags_l_sorted])}]\"\n        \n        current_case_str = (\n            f\"[{mags_c_list_str},\"\n            f\"{mags_l_list_str},\"\n            f\"{rho_c:.8f},\"\n            f\"{rho_l:.8f},\"\n            f\"{str(stable_c).lower()},\"\n            f\"{str(stable_l).lower()}]\"\n        )\n        results_for_all_cases.append(current_case_str)\n        \n    print(f\"[{','.join(results_for_all_cases)}]\")\n\nsolve()\n```", "id": "3396250"}, {"introduction": "现实世界中的反演问题通常包含物理约束，例如解的非负性或其他边界条件。学习型展开方案可以通过在网络层中设计可微的“软”壁垒函数来整合这些约束，从而在优化过程中引导解朝向可行集。本练习旨在挑战您实现一个能够强制执行不等式约束的学习型邻近映射，并评估其有效性，为您提供关于如何构建具有物理意识的神经网络的见解 [@problem_id:3396268]。", "problem": "考虑在展开优化背景下，一个带线性不等式约束的二次逆问题。任务是设计并分析一个学习的近端映射，该映射在固定深度的展开方案中使用可微障碍函数来施加不等式约束，并评估在迭代过程中可行性的维持情况。\n\n从以下基础出发：\n- 线性最小二乘数据拟合的目标由函数 $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x} - \\mathbf{y}\\|_2^2$ 表示，其中 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 是一个测量算子，$\\mathbf{y} \\in \\mathbb{R}^m$ 是一个给定的数据向量。\n- 约束集由 $\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$ 定义，其中 $\\mathbf{C} \\in \\mathbb{R}^{p \\times n}$ 且 $\\mathbf{d} \\in \\mathbb{R}^p$。\n- 使用可微障碍函数来软性施加不等式约束。对于标量参数 $u$ 的平滑障碍函数由 $b_\\tau(u) = \\tau \\log\\big(1 + \\exp(u/\\tau)\\big)$ 给出，其中温度参数 $\\tau  0$。该函数处处可微，并且当 $\\tau \\to 0$ 时，它近似于正部函数。\n\n你必须设计一个固定深度为 $K$ 的展开迭代方案，该方案首先对数据保真项执行梯度下降步骤，然后应用一个学习的近端映射，该映射结合了障碍函数以抑制对 $\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$ 的违反。具体来说，在每次迭代 $k$ 中：\n- 使用步长 $\\alpha  0$ 对 $f(\\mathbf{x})$ 进行梯度下降更新，以获得中间点 $\\mathbf{z}^{(k)}$。\n- 将由 $\\boldsymbol{\\theta}$ 参数化的学习近端映射 $P_{\\boldsymbol{\\theta}}$ 应用于 $\\mathbf{z}^{(k)}$，以获得旨在施加约束的 $\\mathbf{x}^{(k+1)} = P_{\\boldsymbol{\\theta}}(\\mathbf{z}^{(k)})$。\n\n学习的近端映射 $P_{\\boldsymbol{\\theta}}$ 的构建应与通过链式法则对与线性约束相关的障碍函数进行下降相一致。你必须实现一个仅使用线性算子和逐元素平滑函数的映射，其参数 $\\boldsymbol{\\theta}$ 在展开迭代中被视为固定的学习值。初始迭代点必须是 $\\mathbf{x}^{(0)} = \\mathbf{0}$。\n\n将迭代 $k$ 的可行性定义为 $\\mathbf{C}\\mathbf{x}^{(k)} - \\mathbf{d}$ 的所有分量均小于或等于容差 $\\varepsilon  0$ 的条件。对于给定的测试用例，跨迭代的可行性维护是一个布尔值，当且仅当每个迭代点 $\\mathbf{x}^{(k)}$（其中 $k = 1, 2, \\ldots, K$）根据此容差都是可行时，该值为真。此外，将最终最大违反量定义为非负标量 $\\max\\big\\{0, \\max_i\\big((\\mathbf{C}\\mathbf{x}^{(K)} - \\mathbf{d})_i\\big)\\big\\}$。\n\n你的程序必须实现上述展开方案，并为每个测试用例生成一个结果，该结果包含可行性维护布尔值和最终最大违反量。将所有测试用例的结果聚合到单行输出中，该输出包含一个用方括号括起来的逗号分隔列表，其中每个测试用例的结果是一个形式为 $[\\text{布尔值}, \\text{浮点数}]$ 的双元素列表。\n\n使用以下测试套件，其中所有矩阵和向量都已明确指定。在所有情况下，维度为 $n = m = 3$，约束数量 $p$ 不定，迭代次数为 $K = 25$，可行性容差为 $\\varepsilon = 10^{-8}$。\n\n- 测试用例 1（理想情况）：\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.5  0.1 \\\\ 0.0  0.1  2.0 \\end{bmatrix}$, $\\mathbf{y} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ -0.1 \\end{bmatrix}$。\n  - $\\mathbf{C} = \\mathbf{I}_3$, $\\mathbf{d} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$。\n  - 步长 $\\alpha = 0.15$，学习的近端参数 $\\boldsymbol{\\theta} = (\\sigma, \\beta, \\tau) = (0.35, 1.0, 0.1)$。\n\n- 测试用例 2（更紧的箱式约束）：\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.5  0.1 \\\\ 0.0  0.1  2.0 \\end{bmatrix}$, $\\mathbf{y} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ -0.1 \\end{bmatrix}$。\n  - $\\mathbf{C} = \\mathbf{I}_3$, $\\mathbf{d} = \\begin{bmatrix} 0.2 \\\\ 0.2 \\\\ 0.05 \\end{bmatrix}$。\n  - 步长 $\\alpha = 0.15$，学习的近端参数 $\\boldsymbol{\\theta} = (0.5, 2.0, 0.08)$。\n\n- 测试用例 3（混合约束，包括一个和约束）：\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.5  0.1 \\\\ 0.0  0.1  2.0 \\end{bmatrix}$, $\\mathbf{y} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ -0.1 \\end{bmatrix}$。\n  - $\\mathbf{C} = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 1  1  1 \\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix} 0.4 \\\\ 0.4 \\\\ 0.4 \\\\ 0.6 \\end{bmatrix}$。\n  - 步长 $\\alpha = 0.2$，学习的近端参数 $\\boldsymbol{\\theta} = (0.4, 1.5, 0.07)$。\n\n- 测试用例 4（激进的数据步）：\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.5  0.1 \\\\ 0.0  0.1  2.0 \\end{bmatrix}$, $\\mathbf{y} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ -0.1 \\end{bmatrix}$。\n  - $\\mathbf{C} = \\mathbf{I}_3$, $\\mathbf{d} = \\begin{bmatrix} 0.3 \\\\ 0.3 \\\\ 0.3 \\end{bmatrix}$。\n  - 步长 $\\alpha = 0.65$，学习的近端参数 $\\boldsymbol{\\theta} = (0.45, 2.0, 0.05)$。\n\n- 测试用例 5（病态前向算子与单个聚合约束）：\n  - $\\mathbf{A} = \\begin{bmatrix} 3.0  2.5  0.0 \\\\ 2.5  2.8  1.2 \\\\ 0.0  1.2  0.5 \\end{bmatrix}$, $\\mathbf{y} = \\begin{bmatrix} 2.0 \\\\ -1.0 \\\\ 0.5 \\end{bmatrix}$。\n  - $\\mathbf{C} = \\begin{bmatrix} 1  1  1 \\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix} 0.7 \\end{bmatrix}$。\n  - 步长 $\\alpha = 0.3$，学习的近端参数 $\\boldsymbol{\\theta} = (0.6, 3.0, 0.1)$。\n\n程序必须：\n- 实现包含 $K = 25$ 次迭代和可行性容差 $\\varepsilon = 10^{-8}$ 的展开方案。\n- 为每个测试用例计算序列 $\\{\\mathbf{x}^{(k)}\\}_{k=1}^{K}$，并评估可行性维护布尔值和最终最大违反量。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个内部测试用例的结果本身就是一个列表 $[\\text{布尔值}, \\text{浮点数}]$。例如，一个有效的输出格式是 $[[\\text{True}, 0.0],[\\text{False}, 0.1234],\\ldots]$。\n\n本问题不涉及任何物理单位、角度单位或百分比单位。所有量均为无量纲实数。", "solution": "用户提供了一个问题，要求为约束逆问题设计并实现一个学习的、展开的优化方案。解决方案必须按照特定格式呈现，从问题验证开始。\n\n### 问题验证\n\n**第一步：提取已知信息**\n\n- **目标函数**：$f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x} - \\mathbf{y}\\|_2^2$，其中 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 且 $\\mathbf{y} \\in \\mathbb{R}^m$。\n- **不等式约束**：$\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$，其中 $\\mathbf{C} \\in \\mathbb{R}^{p \\times n}$ 且 $\\mathbf{d} \\in \\mathbb{R}^p$。\n- **平滑障碍函数**：对于标量参数 $u$，障碍函数为 $b_\\tau(u) = \\tau \\log\\big(1 + \\exp(u/\\tau)\\big)$，其中温度参数 $\\tau  0$。\n- **展开迭代方案**：一个固定深度为 $K$ 的方案。\n  - 初始迭代点：$\\mathbf{x}^{(0)} = \\mathbf{0}$。\n  - 对于 $k = 0, \\dots, K-1$：\n    1.  **梯度下降更新**：计算中间点 $\\mathbf{z}^{(k)} = \\mathbf{x}^{(k)} - \\alpha \\nabla f(\\mathbf{x}^{(k)})$，步长为 $\\alpha  0$。\n    2.  **学习的近端映射**：应用映射 $P_{\\boldsymbol{\\theta}}$ 以获得 $\\mathbf{x}^{(k+1)} = P_{\\boldsymbol{\\theta}}(\\mathbf{z}^{(k)})$。\n- **近端映射规范**：$P_{\\boldsymbol{\\theta}}$ 由 $\\boldsymbol{\\theta}$ 参数化，与通过链式法则对与约束相关的障碍函数进行下降相一致，并且仅使用线性算子和逐元素平滑函数。\n- **可行性度量**：\n  - **迭代 $k$ 的可行性**：$\\mathbf{C}\\mathbf{x}^{(k)} - \\mathbf{d}$ 的所有分量均小于或等于容差 $\\varepsilon  0$。\n  - **可行性维护**：一个布尔值，当且仅当每个迭代点 $\\mathbf{x}^{(k)}$（其中 $k = 1, 2, \\ldots, K$）都可行时为真。\n  - **最终最大违反量**：非负标量 $\\max\\big\\{0, \\max_i\\big((\\mathbf{C}\\mathbf{x}^{(K)} - \\mathbf{d})_i\\big)\\big\\}$。\n- **全局参数**：$n = m = 3$，$K = 25$，$\\varepsilon = 10^{-8}$。\n- **测试用例**：提供了五个测试用例，每个都指定了矩阵 $\\mathbf{A}$、$\\mathbf{C}$，向量 $\\mathbf{y}$、$\\mathbf{d}$，以及参数 $\\alpha$ 和 $\\boldsymbol{\\theta} = (\\sigma, \\beta, \\tau)$。\n\n**第二步：使用提取的已知信息进行验证**\n\n1.  **科学依据**：该问题在数值优化、逆问题和机器学习领域有坚实的基础。最小二乘法、线性约束、障碍方法（特别是 softplus 函数）以及基于梯度的展开方案都是标准且严谨的概念。\n2.  **适定性**：该问题是适定的。它提供了所有必要的数学定义、算法步骤和数值数据，以得出唯一的计算结果。对学习的近端映射 $P_{\\boldsymbol{\\theta}}$ 的描述有足够的约束，以暗示一个特定的、可形式化的结构（对障碍惩罚项的梯度下降步骤）。\n3.  **客观性**：问题以精确、客观的数学语言陈述，没有歧义或主观声明。\n4.  **完整性与一致性**：问题陈述是自包含且内部一致的。每个测试用例中矩阵和向量的维度都是兼容的。\n5.  **相关性与可形式化性**：该问题与指定主题“学习的迭代方案和展开优化”直接相关，并且完全可以形式化为一个计算算法。\n\n**第三步：结论与行动**\n\n问题被判定为 **有效**。将提供一个完整的、合理的解决方案。\n\n### 基于原理的解决方案设计\n\n该问题要求构建一个展开的迭代算法来解决一个约束优化问题。总体目标是在约束 $\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$ 下最小化数据保真项 $f(\\mathbf{x})$。该展开方案交替执行最小化数据误差的步骤和施加约束的步骤。\n\n**1. 展开迭代方案**\n\n该算法从一个初始估计 $\\mathbf{x}^{(0)} = \\mathbf{0}$ 开始，并对其进行固定次数 $K$ 的迭代优化。每次迭代 $k$（从 $k=0$ 到 $k=K-1$）包括两个主要步骤：\n\n**步骤 A：数据保真项更新**\n\n此步骤对数据保真目标函数 $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x} - \\mathbf{y}\\|_2^2$ 执行梯度下降。该函数关于 $\\mathbf{x}$ 的梯度为：\n$$\n\\nabla f(\\mathbf{x}) = \\mathbf{A}^T(\\mathbf{A}\\mathbf{x} - \\mathbf{y})\n$$\n从当前迭代点 $\\mathbf{x}^{(k)}$ 开始，此步骤的更新规则是：\n$$\n\\mathbf{z}^{(k)} = \\mathbf{x}^{(k)} - \\alpha \\nabla f(\\mathbf{x}^{(k)}) = \\mathbf{x}^{(k)} - \\alpha \\mathbf{A}^T(\\mathbf{A}\\mathbf{x}^{(k)} - \\mathbf{y})\n$$\n其中 $\\alpha  0$ 是步长。所得向量 $\\mathbf{z}^{(k)}$ 是一个中间估计，它更接近于最小化数据失配，但可能会违反约束。\n\n**步骤 B：通过学习的近端映射施加约束**\n\n这一步 $\\mathbf{x}^{(k+1)} = P_{\\boldsymbol{\\theta}}(\\mathbf{z}^{(k)})$ 旨在将中间估计 $\\mathbf{z}^{(k)}$ 拉回可行域。约束 $\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$ 等价于对所有 $i$ 都有 $(\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i \\le 0$。当 $(\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i  0$ 时，发生违反。\n\n我们使用提供的平滑障碍函数 $b_\\tau(u) = \\tau \\log(1 + \\exp(u/\\tau))$ 来惩罚此类违反。该函数近似于正部函数 $\\max(0, u)$。对于给定的 $\\mathbf{x}$，总障碍惩罚是每个约束的惩罚之和，并由参数 $\\beta$ 缩放：\n$$\nB(\\mathbf{x}) = \\sum_{i=1}^{p} \\beta \\cdot b_\\tau\\big((\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i\\big)\n$$\n近端映射 $P_{\\boldsymbol{\\theta}}$ 的结构是从 $\\mathbf{z}^{(k)}$ 开始对该障碍函数执行一步梯度下降。首先，我们推导 $B(\\mathbf{x})$ 的梯度。障碍函数的导数是 sigmoid 函数：\n$$\nb'_\\tau(u) = \\frac{d}{du} \\left[\\tau \\log(1+e^{u/\\tau})\\right] = \\frac{1}{1+e^{-u/\\tau}} =: \\text{sigm}_\\tau(u)\n$$\n使用链式法则，总障碍惩罚 $B(\\mathbf{x})$ 的梯度为：\n$$\n\\nabla B(\\mathbf{x}) = \\sum_{i=1}^{p} \\beta \\cdot b'_\\tau\\big((\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i\\big) \\cdot \\nabla_x\\big((\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i\\big) = \\beta \\mathbf{C}^T \\mathbf{s}(\\mathbf{x})\n$$\n其中 $\\mathbf{s}(\\mathbf{x})$ 是一个向量，其第 $i$ 个分量是 $s_i(\\mathbf{x}) = \\text{sigm}_\\tau\\big((\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i\\big)$。\n\n学习的近端映射 $P_{\\boldsymbol{\\theta}}$ 从点 $\\mathbf{z}^{(k)}$ 开始，对障碍惩罚执行单步梯度下降，步长为 $\\sigma$：\n$$\n\\mathbf{x}^{(k+1)} = P_{\\boldsymbol{\\theta}}(\\mathbf{z}^{(k)}) = \\mathbf{z}^{(k)} - \\sigma \\nabla_z B(\\mathbf{z}^{(k)})\n$$\n代入梯度的表达式，我们得到此步骤的最终更新规则：\n$$\n\\mathbf{x}^{(k+1)} = \\mathbf{z}^{(k)} - \\sigma \\beta \\mathbf{C}^T \\text{sigm}_\\tau(\\mathbf{C}\\mathbf{z}^{(k)} - \\mathbf{d})\n$$\n参数 $\\boldsymbol{\\theta} = (\\sigma, \\beta, \\tau)$ 被视为固定的“学习”值，用于控制约束施加步骤的行为。$\\sigma$ 控制步长，$\\beta$ 缩放障碍函数的影响，$\\tau$ 控制 soft-plus 障碍函数的锐度。这种构造满足了问题中只使用线性算子（$\\mathbf{C}$、$\\mathbf{C}^T$）和逐元素平滑函数（$\\text{sigm}_\\tau$）的要求。\n\n**2. 算法总结与评估**\n\n完整的算法展开如下：\n1.  初始化 $\\mathbf{x}^{(0)} = \\mathbf{0}$。\n2.  对于 $k = 0, 1, \\dots, K-1$：\n    a. 计算 $\\mathbf{z}^{(k)} = \\mathbf{x}^{(k)} - \\alpha \\mathbf{A}^T(\\mathbf{A}\\mathbf{x}^{(k)} - \\mathbf{y})$。\n    b. 计算 $\\mathbf{x}^{(k+1)} = \\mathbf{z}^{(k)} - \\sigma \\beta \\mathbf{C}^T \\text{sigm}_\\tau(\\mathbf{C}\\mathbf{z}^{(k)} - \\mathbf{d})$。\n    c. 检查可行性：验证 $\\mathbf{C}\\mathbf{x}^{(k+1)} - \\mathbf{d}$ 的所有分量是否都 $\\le \\varepsilon$。\n3.  在 $K$ 次迭代后，根据 $\\mathbf{x}^{(K)}$ 计算最终最大违反量。\n\n当且仅当步骤 2.c 中的检查对所有 $k=0, \\dots, K-1$ 都通过时，可行性维护布尔值为真。最终最大违反量计算为 $\\max\\{0, \\max_i ((\\mathbf{C}\\mathbf{x}^{(K)} - \\mathbf{d})_i)\\}$。这为最终迭代点满足约束的程度提供了一个定量度量。", "answer": "```python\nimport numpy as np\n\ndef run_unrolled_scheme(A, y, C, d, alpha, theta, K, eps):\n    \"\"\"\n    Implements the unrolled optimization scheme for a given test case.\n\n    Args:\n        A (np.ndarray): Measurement operator.\n        y (np.ndarray): Data vector.\n        C (np.ndarray): Constraint matrix.\n        d (np.ndarray): Constraint vector.\n        alpha (float): Step size for data fidelity term.\n        theta (tuple): Parameters (sigma, beta, tau) for the proximal mapping.\n        K (int): Number of iterations.\n        eps (float): Feasibility tolerance.\n\n    Returns:\n        list: A list containing the feasibility maintenance boolean and the \n              final maximum violation float.\n    \"\"\"\n    sigma, beta, tau = theta\n    n = A.shape[1]\n    x = np.zeros(n)\n    feasibility_maintained = True\n\n    for _ in range(K):\n        # Step 1: Gradient descent on the data fidelity term f(x)\n        grad_f = A.T @ (A @ x - y)\n        z = x - alpha * grad_f\n\n        # Step 2: Apply the learned proximal mapping to enforce constraints\n        # Argument of the sigmoid function\n        u = C @ z - d\n        \n        # Elementwise sigmoid function: 1 / (1 + exp(-u/tau))\n        # Add a small constant to tau to prevent division by zero, though not strictly needed for given test cases\n        s = 1.0 / (1.0 + np.exp(-u / (tau + 1e-12)))\n        \n        # Gradient of the barrier function B(z)\n        grad_b = beta * (C.T @ s)\n        \n        # Update x for the next iteration\n        x_next = z - sigma * grad_b\n        \n        # Check feasibility for the new iterate x^(k+1)\n        violations = C @ x_next - d\n        if np.any(violations > eps):\n            feasibility_maintained = False\n        \n        x = x_next\n        \n    # After all K iterations, calculate the final maximum violation\n    final_violations = C @ x - d\n    final_max_violation = np.maximum(0.0, np.max(final_violations))\n    \n    return [feasibility_maintained, final_max_violation]\n\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the unrolled optimization scheme for each,\n    then prints the results in the specified format.\n    \"\"\"\n    \n    # Common parameters\n    K = 25\n    eps = 1e-8\n    \n    # Test Suite\n    A_common = np.array([[1.0, 0.2, 0.0], [0.2, 1.5, 0.1], [0.0, 0.1, 2.0]])\n    y_common = np.array([0.4, 0.6, -0.1])\n    \n    test_cases = [\n        # Test case 1 (happy path)\n        {\n            \"A\": A_common, \"y\": y_common,\n            \"C\": np.eye(3), \"d\": np.array([0.5, 0.5, 0.5]),\n            \"alpha\": 0.15, \"theta\": (0.35, 1.0, 0.1)\n        },\n        # Test case 2 (tighter box constraints)\n        {\n            \"A\": A_common, \"y\": y_common,\n            \"C\": np.eye(3), \"d\": np.array([0.2, 0.2, 0.05]),\n            \"alpha\": 0.15, \"theta\": (0.5, 2.0, 0.08)\n        },\n        # Test case 3 (mixed constraints)\n        {\n            \"A\": A_common, \"y\": y_common,\n            \"C\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]),\n            \"d\": np.array([0.4, 0.4, 0.4, 0.6]),\n            \"alpha\": 0.2, \"theta\": (0.4, 1.5, 0.07)\n        },\n        # Test case 4 (aggressive data step)\n        {\n            \"A\": A_common, \"y\": y_common,\n            \"C\": np.eye(3), \"d\": np.array([0.3, 0.3, 0.3]),\n            \"alpha\": 0.65, \"theta\": (0.45, 2.0, 0.05)\n        },\n        # Test case 5 (ill-conditioned operator)\n        {\n            \"A\": np.array([[3.0, 2.5, 0.0], [2.5, 2.8, 1.2], [0.0, 1.2, 0.5]]),\n            \"y\": np.array([2.0, -1.0, 0.5]),\n            \"C\": np.array([[1, 1, 1]]), \"d\": np.array([0.7]),\n            \"alpha\": 0.3, \"theta\": (0.6, 3.0, 0.1)\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_unrolled_scheme(\n            case[\"A\"], case[\"y\"], case[\"C\"], case[\"d\"],\n            case[\"alpha\"], case[\"theta\"], K, eps\n        )\n        results.append(result)\n\n    # Convert a list of lists like [[True, 0.0], [False, 0.123]]\n    # into the string \"[[True, 0.0],[False, 0.123]]\"\n    result_strings = [f\"[{str(r[0]).lower()}, {r[1]:.8f}]\" for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3396268"}, {"introduction": "随着展开式网络变得更深（层数更多），为反向传播存储中间激活所需的内存可能成为一个严重的瓶颈。可逆架构通过增加计算时间来换取内存使用量的大幅减少，从而为训练更深的模型提供了一种解决方案。本分析练习要求您推导标准训练和可逆训练两种模式下的时间和内存复杂度，从而量化在设计大规模深度学习模型时至关重要的内存与计算之间的根本权衡 [@problem_id:3396297]。", "problem": "考虑训练一个用于线性反问题 $y = A x + \\varepsilon$ 的 $K$ 层展开式迭代收缩阈值算法 (ISTA) 网络，其中前向算子 $A \\in \\mathbb{R}^{n \\times n}$，潜向量 $x \\in \\mathbb{R}^{n}$ 经过 $\\ell_{1}$ 正则化以促进稀疏性。在展开式架构中，每一层都对数据保真项执行一步梯度下降，然后进行逐项收缩映射。假设存在以下科学上标准的建模选择和实现惯例，您必须将它们作为推导的基础：\n\n- 每一层对当前的稀疏迭代量 $x^{k}$ 执行一次与 $A$ 的乘法，对一个密集残差向量执行一次与 $A^{\\top}$ 的乘法，然后进行逐元素收缩。即，每层的前向计算工作是以下各项的总和：\n  1) 在 $A x^{k}$ 中访问 $x^{k}$ 的 $s$ 个非零项，\n  2) 对于一个密集残差 $r^{k}$，在 $A^{\\top} r^{k}$ 中访问 $n$ 个项，\n  3) $n$ 次标量收缩操作，以及\n  4) 对 $n$ 维数组进行的线性时间向量加法，您可以将其吸收到 $n$ 项中。\n- 对于所有层 $k=1,\\dots,K$，稀疏迭代量 $x^{k}$ 都有恰好 $s$ 个非零项（跨层稀疏度恒定），并以压缩稀疏表示法存储，该表示法为每个非零项记录一个浮点值和一个整数索引。在计算内存时，将每个记录的值或索引视为一个单位。\n- 在使用 backpropagation (backprop) 进行训练期间，所有中间层激活值 $\\{x^{1},\\dots,x^{K}\\}$ 都被保留。此外，峰值时需要 $2$ 个长度为 $n$ 的密集工作缓冲区（例如，一个残差和一个梯度携带缓冲区），您必须将其包含在峰值内存计算中。忽略参数内存和与批次相关的开销；重点关注峰值时的激活值和工作缓冲区内存。\n- 在可逆设计中，$K$ 层变换通过标准的加性耦合（additive coupling）变为双射，从而保留信息，使得在不存储中间激活值的情况下也能进行 backpropagation。在该可逆模式下，仅保留终端激活值 $x^{K}$（以相同的稀疏格式）以及峰值时相同的 $2$ 个长度为 $n$ 的密集工作缓冲区。\n- 对于每个训练样本的时间核算，将通过线性算子和逐元素收缩的反向传播过程建模为与前向传播过程中的相同操作序列的额外一次应用等效的工作量。在可逆设计中，由于不存储中间激活值，反向传播过程必须首先重新计算它们，这在执行反向等效过程之前会产生一次额外的前向等效过程。因此，每层使用以下前向等效单位：\n  Backprop：每层 $2$ 个前向等效单位（一个前向，一个反向等效），\n  可逆：每层 $3$ 个前向等效单位（一个前向，一个重计算，一个反向等效）。\n\n在这些假设下，导出以下各项关于 $n$、$s$ 和 $K$ 的符号表达式：\n1) backpropagation 训练期间的峰值激活内存，以及\n2) 可逆训练期间的峰值激活内存。\n然后，使用上述前向等效核算，推导可逆设计的每样本总训练时间与 backpropagation 设计的每样本总训练时间的比率。\n\n将您的最终答案表示为包含三个表达式 $[M_{\\mathrm{bp}}(n,s,K),\\,M_{\\mathrm{rev}}(n,s,K),\\,R_{T}]$ 的单行向量，其中 $M_{\\mathrm{bp}}$ 和 $M_{\\mathrm{rev}}$ 分别表示 backpropagation 和可逆设计下的峰值激活内存，而 $R_{T}$ 是可逆与 backpropagation 训练时间之比。不需要进行数值舍入。不需要单位。", "solution": "已经对问题陈述进行了分析，并确定其是有效的。它在计算科学和深度学习领域，特别是在反问题和展开式优化方面，具有科学依据。该问题是适定的（well-posed），为得出唯一且有意义的解提供了所有必要的假设和定义。其语言客观，设置内部一致且可形式化。因此，我们可以进行推导。\n\n任务是推导与一个展开式 $K$ 层 ISTA 网络相关的三个量：标准 backpropagation 训练的峰值内存 ($M_{\\mathrm{bp}}$)、可逆设计训练的峰值内存 ($M_{\\mathrm{rev}}$)，以及总训练时间的比率 ($R_{T}$)。\n\n**1. Backpropagation 训练的峰值内存 ($M_{\\mathrm{bp}}$)**\n\n根据问题陈述，使用标准 backpropagation 进行训练需要保留所有中间层激活值 $\\{x^{1}, \\dots, x^{K}\\}$。\n- 每个激活向量 $x^{k}$ 都是稀疏的，包含恰好 $s$ 个非零项。\n- 每个稀疏向量的存储格式是一种压缩表示法，其中每个非零项需要存储一个浮点值和一个整数索引。这相当于每个非零项需要 $2$ 个内存单位。\n- 因此，存储单个稀疏激活向量 $x^{k}$ 所需的内存为 $2 \\times s = 2s$ 个单位。\n- 由于所有 $K$ 个中间激活值都被存储，这些激活值的总内存为 $K \\times (2s) = 2Ks$ 个单位。\n- 除了激活值，问题还指明有 $2$ 个长度为 $n$ 的密集工作缓冲区。这些缓冲区的内存为 $2 \\times n = 2n$ 个单位。\n- 峰值内存 $M_{\\mathrm{bp}}$ 是存储的激活值和工作缓冲区的内存之和。\n\n$$M_{\\mathrm{bp}}(n,s,K) = (\\text{激活值内存}) + (\\text{工作缓冲区内存})$$\n$$M_{\\mathrm{bp}}(n,s,K) = 2Ks + 2n$$\n\n**2. 可逆训练的峰值内存 ($M_{\\mathrm{rev}}$)**\n\n在可逆设计中，无需存储所有中间激活值。\n- 问题陈述指出，只保留终端激活值 $x^{K}$。\n- 该向量以与之前相同的稀疏格式存储。因此，存储 $x^{K}$ 的内存成本为 $2s$ 个单位。\n- 问题还指出，峰值时需要相同的 $2$ 个长度为 $n$ 的密集工作缓冲区。这些缓冲区的内存成本仍然是 $2n$ 个单位。\n- 可逆设计的峰值内存 $M_{\\mathrm{rev}}$ 是单个存储的激活值和工作缓冲区的内存之和。\n\n$$M_{\\mathrm{rev}}(n,s,K) = (\\text{终端激活值内存}) + (\\text{工作缓冲区内存})$$\n$$M_{\\mathrm{rev}}(n,s,K) = 2s + 2n$$\n\n**3. 训练时间的比率 ($R_{T}$)**\n\n问题提供了一个训练计算成本的模型，以每层的“前向等效单位”来衡量。设 $T_{\\mathrm{fwd}}$ 代表单层一次前向传播的计算工作量。\n\n- **Backpropagation 训练时间 ($T_{\\mathrm{bp}}$):**\n  - 对于 $K$ 层中的每一层，成本被指定为 $2$ 个前向等效单位。这包括一次前向传播和一次反向传播（假设其成本等效）。\n  - 整个 $K$ 层网络的总训练时间是各层成本的总和。\n  $$T_{\\mathrm{bp}} = \\sum_{k=1}^{K} (2 \\cdot T_{\\mathrm{fwd}}) = K \\cdot (2 T_{\\mathrm{fwd}}) = 2K T_{\\mathrm{fwd}}$$\n\n- **可逆训练时间 ($T_{\\mathrm{rev}}$):**\n  - 对于可逆设计，中间激活值不被存储，必须在反向传播期间重新计算。\n  - 每层的成本被指定为 $3$ 个前向等效单位。这包括原始的前向传播、一次重计算过程（另一次前向传播）和一次反向传播。\n  - $K$ 层网络的总训练时间是：\n  $$T_{\\mathrm{rev}} = \\sum_{k=1}^{K} (3 \\cdot T_{\\mathrm{fwd}}) = K \\cdot (3 T_{\\mathrm{fwd}}) = 3K T_{\\mathrm{fwd}}$$\n\n- **时间比率 ($R_T$):**\n  - 比率 $R_{T}$ 是可逆设计的总训练时间除以 backpropagation 设计的总训练时间。\n  $$R_{T} = \\frac{T_{\\mathrm{rev}}}{T_{\\mathrm{bp}}} = \\frac{3K T_{\\mathrm{fwd}}}{2K T_{\\mathrm{fwd}}}$$\n  - 项 $K$ 和 $T_{\\mathrm{fwd}}$ 相互抵消，得出一个恒定的比率。\n  $$R_{T} = \\frac{3}{2}$$\n\n综合这三个结果，我们得到 backpropagation 的峰值内存为 $M_{\\mathrm{bp}}(n,s,K) = 2n + 2Ks$，可逆设计的峰值内存为 $M_{\\mathrm{rev}}(n,s,K) = 2n + 2s$，训练时间比率为 $R_{T} = \\frac{3}{2}$。", "answer": "$$ \\boxed{[2n + 2Ks, 2n + 2s, \\frac{3}{2}]} $$", "id": "3396297"}]}