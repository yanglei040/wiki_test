{"hands_on_practices": [{"introduction": "生成模型的一大挑战是其输出可能不严格遵守诸如动量守恒等基本物理定律。本练习将引导你设计一个可微的“物理信息层”，通过求解一个约束优化问题来校正模型的输出，从而强制执行动量守恒 [@problem_id:3515492]。通过这个实践，你将掌握把基本物理原理融入神经网络架构的关键技术，并学习如何定量分析该层对模型输出分布的影响，这是构建可靠的科学机器学习模型的基石。", "problem": "给定一个用于计算高能物理的物理学启发生成模型的风格化设置。一个生成网络，如生成对抗网络 (GAN) 或变分自编码器 (VAE)，产生无约束的末态粒子动量，然后通过一个可微物理层进行校正，以近似地强制实现动量守恒。你必须从第一性原理出发，将该层推导为一个可微优化问题，量化其对缺失动量分布的影响，并实现一个程序，为一组给定的测试用例计算特定的解析度量。\n\n上下文和定义如下。\n\n- 有一个事件，包含 $D$ 维空间动量维度中的 $N$ 个末态粒子，以及一个入射总动量 $\\vec{p}_{\\text{in}} \\in \\mathbb{R}^D$。无约束生成器的输出为 $\\{\\vec{x}_i\\}_{i=1}^N$，其中 $\\vec{x}_i \\in \\mathbb{R}^D$ 是随机向量。\n- 无约束生成器被建模为每个粒子独立的高斯输出，其中 $\\vec{x}_i \\sim \\mathcal{N}(\\vec{\\mu}_i, \\Sigma_i)$，$\\vec{\\mu}_i \\in \\mathbb{R}^D$ 且 $\\Sigma_i \\in \\mathbb{R}^{D \\times D}$ 是正定矩阵。粒子间的独立性意味着 $\\sum_{i=1}^N \\vec{x}_i$ 是高斯分布，其均值为 $\\sum_{i=1}^N \\vec{\\mu}_i$，协方差为 $\\sum_{i=1}^N \\Sigma_i$。因此，修正前的缺失动量为 $\\vec{p}_{\\text{miss,pre}} = \\vec{p}_{\\text{in}} - \\sum_{i=1}^N \\vec{x}_i \\sim \\mathcal{N}(\\vec{\\mu}_0, \\Sigma_0)$，其中\n$\\vec{\\mu}_0 = \\vec{p}_{\\text{in}} - \\sum_{i=1}^N \\vec{\\mu}_i$ 且 $\\Sigma_0 = \\sum_{i=1}^N \\Sigma_i$。\n\n- 一个可微物理层通过求解一个带软约束参数 $\\alpha \\in [0,1]$ 的加权最小二乘问题来产生修正后的动量 $\\{\\vec{y}_i\\}_{i=1}^N$：\n最小化 $\\sum_{i=1}^N w_i \\lVert \\vec{y}_i - \\vec{x}_i \\rVert_2^2$，约束条件为 $\\sum_{i=1}^N \\vec{y}_i = \\vec{x}_{\\text{sum}} + \\alpha \\left(\\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}}\\right)$，其中 $\\vec{x}_{\\text{sum}} = \\sum_{i=1}^N \\vec{x}_i$ 且 $w_i > 0$ 是用户指定的权重。当 $\\alpha = 1$ 时对应强制精确动量守恒的硬约束，而 $\\alpha = 0$ 则保持输出不变。对于任何固定的 $\\alpha \\in (0,1)$，该层是线性和可微的。\n\n- 令 $\\beta_i = \\dfrac{1/w_i}{\\sum_{j=1}^N 1/w_j}$，因此 $\\sum_{i=1}^N \\beta_i = 1$。加权最小二乘问题的唯一最优解产生的更新为 $\\vec{y}_i = \\vec{x}_i + \\alpha \\,\\beta_i \\,\\Delta$，其中 $\\Delta = \\vec{p}_{\\text{in}} - \\sum_{j=1}^N \\vec{x}_j$。这意味着修正后的缺失动量为 $\\vec{p}_{\\text{miss,post}} = \\vec{p}_{\\text{in}} - \\sum_{i=1}^N \\vec{y}_i = (1-\\alpha)\\,\\Delta$。\n\n- 通过线性高斯传播，$\\vec{p}_{\\text{miss,post}} \\sim \\mathcal{N}(\\vec{\\mu}_\\alpha, \\Sigma_\\alpha)$，其中 $\\vec{\\mu}_\\alpha = (1-\\alpha)\\,\\vec{\\mu}_0$ 且 $\\Sigma_\\alpha = (1-\\alpha)^2 \\Sigma_0$。\n\n- 两个 $D$ 维多元正态分布 $\\mathcal{N}(\\vec{m}_1,\\Sigma_1)$ 和 $\\mathcal{N}(\\vec{m}_2,\\Sigma_2)$ 之间的 Kullback-Leibler 散度由下式给出：\n$\\mathrm{KL}\\!\\left(\\mathcal{N}(\\vec{m}_1,\\Sigma_1)\\,\\Vert\\,\\mathcal{N}(\\vec{m}_2,\\Sigma_2)\\right) = \\dfrac{1}{2}\\left(\\mathrm{tr}(\\Sigma_2^{-1}\\Sigma_1) + (\\vec{m}_2-\\vec{m}_1)^\\top \\Sigma_2^{-1}(\\vec{m}_2-\\vec{m}_1) - D + \\ln \\dfrac{\\det \\Sigma_2}{\\det \\Sigma_1}\\right)$，这是一个经过充分检验的公式。你将使用它来量化对缺失动量分布的影响，具体计算 $\\mathrm{KL}\\!\\left(\\mathcal{N}(\\vec{\\mu}_\\alpha,\\Sigma_\\alpha)\\,\\Vert\\,\\mathcal{N}(\\vec{\\mu}_0,\\Sigma_0)\\right)$。\n\n- 所有粒子的期望总平方修正幅度为 $\\mathbb{E}\\!\\left[\\sum_{i=1}^N \\lVert \\vec{y}_i - \\vec{x}_i \\rVert_2^2\\right] = \\alpha^2 \\left(\\sum_{i=1}^N \\beta_i^2\\right)\\, \\mathbb{E}\\!\\left[\\lVert \\Delta \\rVert_2^2\\right]$，根据多元正态分布的性质，$\\mathbb{E}\\!\\left[\\lVert \\Delta \\rVert_2^2\\right] = \\lVert \\vec{\\mu}_0 \\rVert_2^2 + \\mathrm{tr}(\\Sigma_0)$。这产生了一个闭式、可微且可测试的度量。\n\n所有动量向量都必须以 $\\mathrm{GeV}/c$ 为单位进行解释。因此，任何报告的期望平方修正幅度都必须以 $(\\mathrm{GeV}/c)^2$ 为单位。本问题不使用角度。你的程序不应进行抽样；它必须根据提供的参数解析地计算所有量。\n\n你的任务是编写一个完整、可运行的程序，该程序：\n- 对于每个测试用例，从给定的 $\\{\\vec{\\mu}_i,\\Sigma_i\\}$ 和 $\\vec{p}_{\\text{in}}$ 构建 $\\vec{\\mu}_0$ 和 $\\Sigma_0$。\n- 为每个测试用例计算以下三个浮点数：\n  1. Kullback-Leibler 散度 $\\mathrm{KL}\\!\\left(\\mathcal{N}(\\vec{\\mu}_\\alpha,\\Sigma_\\alpha)\\,\\Vert\\,\\mathcal{N}(\\vec{\\mu}_0,\\Sigma_0)\\right)$，如上定义，其中 $\\vec{\\mu}_\\alpha=(1-\\alpha)\\vec{\\mu}_0$ 且 $\\Sigma_\\alpha=(1-\\alpha)^2\\Sigma_0$。\n  2. 方差缩放因子 $s = \\dfrac{\\mathrm{tr}(\\Sigma_\\alpha)}{\\mathrm{tr}(\\Sigma_0)}$，对于任何正定 $\\Sigma_0$，该值必须等于 $(1-\\alpha)^2$。\n  3. 期望总平方修正幅度 $\\mathbb{E}\\!\\left[\\sum_{i=1}^N \\lVert \\vec{y}_i - \\vec{x}_i \\rVert_2^2\\right]$，以 $(\\mathrm{GeV}/c)^2$ 表示。\n\n- 将所有测试用例的结果汇总到单行输出中，该输出包含一个方括号括起来的逗号分隔列表。该列表必须包含第一个测试用例的三个浮点数，然后是第二个测试用例的三个浮点数，依此类推。\n\n测试套件：\n- 案例1（理想情况）：$D=2$, $N=4$, $\\alpha=0.7$, $w=(1,1,1,1)$, 对所有 $i$ 有 $\\vec{\\mu}_i=\\vec{0}$, $\\Sigma_i=\\sigma^2 I_2$ 且 $\\sigma=5.0$, $\\vec{p}_{\\text{in}}=(0,0)$。\n- 案例2（各向异性，小修正）：$D=3$, $N=2$, $\\alpha=0.05$, $w=(1.0,2.0)$, $\\vec{\\mu}_1=(1,-2,0.5)$, $\\vec{\\mu}_2=(0,1,-0.5)$, $\\Sigma_1=\\mathrm{diag}(2.0^2,1.0^2,0.5^2)$, $\\Sigma_2=\\mathrm{diag}(1.5^2,0.5^2,0.25^2)$, $\\vec{p}_{\\text{in}}=(3,-1,2)$。\n- 案例3（接近硬约束）：$D=2$, $N=6$, $\\alpha=0.95$, $w=(1.0,1.0,0.5,2.0,3.0,1.5)$, 对所有 $i$ 有 $\\vec{\\mu}_i=\\vec{0}$, $\\Sigma_i=\\sigma_i^2 I_2$ 且 $\\sigma=(3.0,4.0,2.0,1.0,5.0,2.5)$, $\\vec{p}_{\\text{in}}=(10,-7)$。\n- 案例4（一维，异构权重）：$D=1$, $N=3$, $\\alpha=0.3$, $w=(1.0,10.0,0.1)$, $\\vec{\\mu}=(2.0,-1.0,0.0)$, $\\Sigma=\\mathrm{diag}(1.0^2,2.0^2,0.5^2)$ 作为一维中的粒子方差, $p_{\\text{in}}=5.0$。\n\n最终输出格式：\n你的程序应产生单行输出，其中包含一个方括号括起来的逗号分隔列表作为结果，顺序为 $[\\mathrm{KL}_1, s_1, C_1, \\mathrm{KL}_2, s_2, C_2, \\mathrm{KL}_3, s_3, C_3, \\mathrm{KL}_4, s_4, C_4]$，其中 $\\mathrm{KL}_k$ 是案例 $k$ 的 Kullback-Leibler 散度（一个浮点数），$s_k$ 是案例 $k$ 的方差缩放因子（一个浮点数），$C_k$ 是案例 $k$ 的期望总平方修正幅度，单位为 $(\\mathrm{GeV}/c)^2$（一个浮点数）。不得打印任何额外文本。", "solution": "目标是分析一个用于校正生成模型产生的粒子动量的可微物理学启发层。我们需要计算三个特定的度量，以量化该层的影响：修正前和修正后缺失动量分布之间的Kullback-Leibler (KL) 散度、该分布方差的缩放比例，以及所施加修正的期望幅度。此分析将基于提供的先验分布进行解析计算。\n\n基础步骤是刻画修正前缺失动量（表示为 $\\vec{\\Delta} = \\vec{p}_{\\text{miss,pre}}$）的分布。问题陈述，无约束动量 $\\vec{x}_i$ 从独立的正态分布中抽取，$\\vec{x}_i \\sim \\mathcal{N}(\\vec{\\mu}_i, \\Sigma_i)$，对于 $i=1, \\dots, N$。生成的总动量为 $\\vec{x}_{\\text{sum}} = \\sum_{i=1}^N \\vec{x}_i$。根据多元正态分布的性质，$\\vec{x}_{\\text{sum}}$ 也服从正态分布，其均值等于各均值之和，协方差等于各协方差之和：\n$$\n\\vec{x}_{\\text{sum}} \\sim \\mathcal{N}\\left(\\sum_{i=1}^N \\vec{\\mu}_i, \\sum_{i=1}^N \\Sigma_i\\right)\n$$\n修正前缺失动量定义为 $\\vec{\\Delta} = \\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}}$。由于这是一个正态分布变量的线性变换，$\\vec{\\Delta}$ 也服从正态分布。其参数，我们表示为 $\\vec{\\mu}_0$ 和 $\\Sigma_0$，是：\n$$\n\\vec{\\mu}_0 = \\mathbb{E}[\\vec{\\Delta}] = \\mathbb{E}[\\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}}] = \\vec{p}_{\\text{in}} - \\mathbb{E}[\\vec{x}_{\\text{sum}}] = \\vec{p}_{\\text{in}} - \\sum_{i=1}^N \\vec{\\mu}_i\n$$\n$$\n\\Sigma_0 = \\mathrm{Cov}(\\vec{\\Delta}) = \\mathrm{Cov}(\\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}}) = \\mathrm{Cov}(\\vec{x}_{\\text{sum}}) = \\sum_{i=1}^N \\Sigma_i\n$$\n因此，修正前缺失动量遵循分布 $\\vec{\\Delta} \\sim \\mathcal{N}(\\vec{\\mu}_0, \\Sigma_0)$。\n\n物理层产生修正后的动量 $\\{\\vec{y}_i\\}$，使得修正后的总动量为 $\\sum_{i=1}^N \\vec{y}_i = \\vec{x}_{\\text{sum}} + \\alpha(\\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}})$。那么，修正后的缺失动量 $\\vec{p}_{\\text{miss,post}}$ 为：\n$$\n\\vec{p}_{\\text{miss,post}} = \\vec{p}_{\\text{in}} - \\sum_{i=1}^N \\vec{y}_i = \\vec{p}_{\\text{in}} - (\\vec{x}_{\\text{sum}} + \\alpha(\\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}})) = (1-\\alpha)(\\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}}) = (1-\\alpha)\\vec{\\Delta}\n$$\n这意味着修正后的缺失动量分布也是正态的，$\\vec{p}_{\\text{miss,post}} \\sim \\mathcal{N}(\\vec{\\mu}_\\alpha, \\Sigma_\\alpha)$，其参数通过缩放修正前参数得到：\n$$\n\\vec{\\mu}_\\alpha = \\mathbb{E}[(1-\\alpha)\\vec{\\Delta}] = (1-\\alpha)\\vec{\\mu}_0\n$$\n$$\n\\Sigma_\\alpha = \\mathrm{Cov}((1-\\alpha)\\vec{\\Delta}) = (1-\\alpha)^2 \\Sigma_0\n$$\n建立了这些分布后，我们就可以计算所需的度量。\n\n**1. Kullback-Leibler 散度**\n我们计算 $\\mathrm{KL}\\!\\left(\\mathcal{N}(\\vec{\\mu}_\\alpha,\\Sigma_\\alpha)\\,\\Vert\\,\\mathcal{N}(\\vec{\\mu}_0,\\Sigma_0)\\right)$。两个 $D$ 维正态分布 $\\mathcal{N}_1 = \\mathcal{N}(\\vec{m}_1, S_1)$ 和 $\\mathcal{N}_2 = \\mathcal{N}(\\vec{m}_2, S_2)$ 之间 KL 散度的通用公式为：\n$$\n\\mathrm{KL}(\\mathcal{N}_1 \\Vert \\mathcal{N}_2) = \\frac{1}{2}\\left( \\mathrm{tr}(S_2^{-1}S_1) + (\\vec{m}_2 - \\vec{m}_1)^\\top S_2^{-1}(\\vec{m}_2 - \\vec{m}_1) - D + \\ln\\frac{\\det S_2}{\\det S_1} \\right)\n$$\n在我们的情况下，$\\vec{m}_1 = \\vec{\\mu}_\\alpha = (1-\\alpha)\\vec{\\mu}_0$, $S_1 = \\Sigma_\\alpha = (1-\\alpha)^2\\Sigma_0$, $\\vec{m}_2 = \\vec{\\mu}_0$, 且 $S_2 = \\Sigma_0$。将这些代入公式，我们可以简化每一项：\n- 迹项：$\\mathrm{tr}(\\Sigma_0^{-1}\\Sigma_\\alpha) = \\mathrm{tr}(\\Sigma_0^{-1}(1-\\alpha)^2\\Sigma_0) = (1-\\alpha)^2 \\mathrm{tr}(I_D) = D(1-\\alpha)^2$。\n- 均值差异项：$(\\vec{\\mu}_0 - \\vec{\\mu}_\\alpha)^\\top \\Sigma_0^{-1}(\\vec{\\mu}_0 - \\vec{\\mu}_\\alpha) = (\\vec{\\mu}_0 - (1-\\alpha)\\vec{\\mu}_0)^\\top \\Sigma_0^{-1}(\\vec{\\mu}_0 - (1-\\alpha)\\vec{\\mu}_0) = (\\alpha\\vec{\\mu}_0)^\\top \\Sigma_0^{-1}(\\alpha\\vec{\\mu}_0) = \\alpha^2 \\vec{\\mu}_0^\\top \\Sigma_0^{-1}\\vec{\\mu}_0$。\n- 对数行列式项：$\\ln\\frac{\\det \\Sigma_0}{\\det \\Sigma_\\alpha} = \\ln\\frac{\\det \\Sigma_0}{\\det((1-\\alpha)^2\\Sigma_0)} = \\ln\\frac{\\det \\Sigma_0}{(1-\\alpha)^{2D}\\det \\Sigma_0} = \\ln((1-\\alpha)^{-2D}) = -2D\\ln(1-\\alpha)$。\n\n结合这些简化项，得到 KL 散度的最终表达式：\n$$\n\\mathrm{KL} = \\frac{1}{2} \\left[ D(1-\\alpha)^2 + \\alpha^2 \\vec{\\mu}_0^\\top \\Sigma_0^{-1}\\vec{\\mu}_0 - D - 2D\\ln(1-\\alpha) \\right]\n$$\n该表达式对于 $\\alpha \\in [0, 1)$ 是计算稳定的，并且可以直接实现。\n\n**2. 方差缩放因子**\n方差缩放因子 $s$ 定义为修正后和修正前协方差矩阵的迹之比：\n$$\ns = \\frac{\\mathrm{tr}(\\Sigma_\\alpha)}{\\mathrm{tr}(\\Sigma_0)}\n$$\n使用关系式 $\\Sigma_\\alpha = (1-\\alpha)^2 \\Sigma_0$ 和迹算子的线性性质：\n$$\ns = \\frac{\\mathrm{tr}((1-\\alpha)^2 \\Sigma_0)}{\\mathrm{tr}(\\Sigma_0)} = \\frac{(1-\\alpha)^2 \\mathrm{tr}(\\Sigma_0)}{\\mathrm{tr}(\\Sigma_0)} = (1-\\alpha)^2\n$$\n该度量量化了由于校正层导致的总动量的总方差（即统计涨落或“抖动”）的减少程度。它仅取决于混合参数 $\\alpha$。\n\n**3. 期望总平方修正幅度**\n最后的度量是施加到粒子动量上的期望总平方修正， $C = \\mathbb{E}\\!\\left[\\sum_{i=1}^N \\lVert \\vec{y}_i - \\vec{x}_i \\rVert_2^2\\right]$。问题提供了从最小二乘解推导出的公式：\n$$\nC = \\alpha^2 \\left(\\sum_{i=1}^N \\beta_i^2\\right)\\, \\mathbb{E}\\!\\left[\\lVert \\Delta \\rVert_2^2\\right]\n$$\n其中 $\\beta_i = \\frac{1/w_i}{\\sum_{j=1}^N 1/w_j}$。项 $\\mathbb{E}\\!\\left[\\lVert \\Delta \\rVert_2^2\\right]$ 是修正前缺失动量 $\\vec{\\Delta}$ 的期望平方范数。对于一个随机向量 $\\vec{z} \\sim \\mathcal{N}(\\vec{m}, S)$，这由 $\\mathbb{E}[\\lVert \\vec{z} \\rVert_2^2] = \\lVert \\vec{m} \\rVert_2^2 + \\mathrm{tr}(S)$ 给出。将其应用于 $\\vec{\\Delta} \\sim \\mathcal{N}(\\vec{\\mu}_0, \\Sigma_0)$：\n$$\n\\mathbb{E}\\!\\left[\\lVert \\Delta \\rVert_2^2\\right] = \\lVert \\vec{\\mu}_0 \\rVert_2^2 + \\mathrm{tr}(\\Sigma_0)\n$$\n这个量代表了该层所作用的总“误差”信号，包括来自均值动量不匹配的系统分量 ($\\lVert \\vec{\\mu}_0 \\rVert_2^2$) 和来自生成器方差的随机分量 ($\\mathrm{tr}(\\Sigma_0)$)。因子 $\\alpha^2$ 和 $\\sum_i \\beta_i^2$ 决定了这个误差信号被转化为修正的强度，以及这些修正如何根据它们的权重 $w_i$ 分布在粒子之间。\n\n然后，通过首先构建 $\\vec{\\mu}_0$ 和 $\\Sigma_0$，然后应用推导出的公式，为每个测试用例系统地计算这三个度量。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes analytics for a physics-informed generative model correction layer.\n    \"\"\"\n    test_cases = [\n        {\n            \"D\": 2, \"N\": 4, \"alpha\": 0.7, \"w\": [1.0, 1.0, 1.0, 1.0],\n            \"mus\": [np.zeros(2) for _ in range(4)],\n            \"Sigmas\": [5.0**2 * np.identity(2) for _ in range(4)],\n            \"p_in\": np.zeros(2)\n        },\n        {\n            \"D\": 3, \"N\": 2, \"alpha\": 0.05, \"w\": [1.0, 2.0],\n            \"mus\": [np.array([1.0, -2.0, 0.5]), np.array([0.0, 1.0, -0.5])],\n            \"Sigmas\": [\n                np.diag([2.0**2, 1.0**2, 0.5**2]),\n                np.diag([1.5**2, 0.5**2, 0.25**2])\n            ],\n            \"p_in\": np.array([3.0, -1.0, 2.0])\n        },\n        {\n            \"D\": 2, \"N\": 6, \"alpha\": 0.95, \"w\": [1.0, 1.0, 0.5, 2.0, 3.0, 1.5],\n            \"mus\": [np.zeros(2) for _ in range(6)],\n            \"Sigmas\": [s**2 * np.identity(2) for s in [3.0, 4.0, 2.0, 1.0, 5.0, 2.5]],\n            \"p_in\": np.array([10.0, -7.0])\n        },\n        {\n            \"D\": 1, \"N\": 3, \"alpha\": 0.3, \"w\": [1.0, 10.0, 0.1],\n            \"mus\": [np.array([2.0]), np.array([-1.0]), np.array([0.0])],\n            \"Sigmas\": [np.array([[1.0**2]]), np.array([[2.0**2]]), np.array([[0.5**2]])],\n            \"p_in\": np.array([5.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        D = case[\"D\"]\n        alpha = case[\"alpha\"]\n        \n        # Construct mu_0 and Sigma_0 for the pre-correction missing momentum\n        mu_sum = np.sum(case['mus'], axis=0)\n        mu_0 = case['p_in'] - mu_sum\n        \n        Sigma_sum = np.sum(case['Sigmas'], axis=0)\n        Sigma_0 = Sigma_sum\n        \n        # Metric 1: Kullback-Leibler divergence\n        # KL = 0.5 * [ D*(1-alpha)^2 + alpha^2 * mu_0.T @ inv(Sigma_0) @ mu_0 - D - 2*D*ln(1-alpha) ]\n        if alpha == 1.0:\n            # The post-correction distribution is singular, KL divergence is infinite.\n            # This case is avoided by the problem statement test values.\n            kl_div = float('inf')\n        elif alpha == 0.0:\n            kl_div = 0.0\n        else:\n            Sigma_0_inv = np.linalg.inv(Sigma_0)\n            mahalanobis_term = alpha**2 * mu_0.T @ Sigma_0_inv @ mu_0\n            term1 = D * (1 - alpha)**2\n            term3 = -D\n            term4 = -2 * D * np.log(1 - alpha)\n            kl_div = 0.5 * (term1 + mahalanobis_term + term3 + term4)\n        \n        # Metric 2: Variance scaling factor\n        s_factor = (1 - alpha)**2\n        \n        # Metric 3: Expected total squared correction magnitude\n        # C = alpha^2 * sum(beta_i^2) * (norm(mu_0)^2 + trace(Sigma_0))\n        weights = np.array(case['w'])\n        inv_weights = 1.0 / weights\n        sum_inv_weights = np.sum(inv_weights)\n        betas = inv_weights / sum_inv_weights\n        sum_beta_sq = np.sum(betas**2)\n        \n        e_delta_sq = np.linalg.norm(mu_0)**2 + np.trace(Sigma_0)\n        \n        correction_mag = alpha**2 * sum_beta_sq * e_delta_sq\n        \n        results.extend([kl_div, s_factor, correction_mag])\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3515492"}, {"introduction": "在为快速模拟选择生成模型时，我们面临着在保真度（fidelity）和生成速度之间的权衡。本练习为你提供一个系统性的模型评估框架，教你如何使用瓦瑟斯坦距离（Wasserstein distance）作为衡量模型生成分布与真实分布之间差异的稳健指标 [@problem_id:3515649]。你将学会计算这一指标，并结合模型的计算成本，识别出在保真度和速度这两个目标下的帕累托最优（Pareto-optimal）模型，这是在实际应用中做出明智模型选择的关键一步。", "problem": "您将获得代表高能物理簇射属性的合成离散可观测量。每个测试用例包含一小组模型，旨在模拟一个参考分布。对于每个模型，您需要计算一个基于 1-瓦瑟斯坦距离的保真度度量，并将其与模型的每个事件的平均壁钟时间进行比较。您的任务是为每个测试用例识别帕累托最优子集：即那些不存在任何其他模型在保真度（越低越好）和时间（越低越好）两方面同时更优或相等，且至少有一个方面有严格改进的模型。\n\n从基本定义和经过充分检验的公式开始。具体来说，使用以下基础：\n- 保真度度量是第一瓦瑟斯坦距离（在一维中也称为推土机距离），对于度量空间 $(\\mathbb{R}, d)$ 上的概率分布 $p$ 和 $q$ 定义为\n$$\nW_1(p,q) = \\inf_{\\gamma \\in \\Pi(p,q)} \\int_{\\mathbb{R} \\times \\mathbb{R}} d(x,y) \\, \\mathrm{d}\\gamma(x,y),\n$$\n其中 $\\Pi(p,q)$ 表示所有边际为 $p$ 和 $q$ 的耦合集，且 $d(x,y) = |x - y|$。\n- 对于具有累积分布函数 $F_p(x)$ 和 $F_q(x)$ 的一维分布，1-瓦瑟斯坦距离等价于\n$$\nW_1(p,q) = \\int_{-\\infty}^{\\infty} \\left| F_p(x) - F_q(x) \\right| \\, \\mathrm{d}x.\n$$\n- 对于在箱体边界 $\\{x_0, x_1, \\dots, x_n\\}$ 上，具有每箱概率 $\\{p_i\\}_{i=1}^n$ 和 $\\{q_i\\}_{i=1}^n$ 以及箱体宽度 $\\Delta x_i = x_i - x_{i-1}$ 的离散化直方图，一个一致的一维离散化是\n$$\nW_1(p,q) \\approx \\sum_{i=1}^{n} \\left| \\sum_{j=1}^{i} (p_j - q_j) \\right| \\, \\Delta x_i,\n$$\n其中内部的和是截至箱体索引 $i$ 的箱体概率的累积差异。\n\n通过形成一个无量纲分数来聚合多个可观测量的保真度\n$$\n\\mathcal{F} = \\sum_{k=1}^{K} \\frac{W_1^{(k)}}{s_k},\n$$\n其中 $W_1^{(k)}$ 是可观测量 $k$ 的 1-瓦瑟斯坦距离，$s_k$ 是一个与可观测量 $k$ 具有相同物理单位的特征尺度，以使 $W_1^{(k)}/s_k$ 无量纲。\n\n精确定义帕累托最优性：对于模型 $i \\in \\{0,1,\\dots,M-1\\}$ 的配对 $(\\mathcal{F}_i, T_i)$，如果不存在任何其他模型 $j$ 使得 $\\mathcal{F}_j \\le \\mathcal{F}_i$ 且 $T_j \\le T_i$，并且这些不等式中至少有一个是严格的，则模型 $i$ 是帕累托最优的。这里 $T_i$ 是每个事件的平均壁钟时间，以秒为单位表示。\n\n实现以下算法：\n1. 对于每个测试用例和每个可观测量 $k$，将每个直方图的计数归一化为概率，使得 $\\sum_{i=1}^{n_k} p_i^{(k)} = 1$ 和 $\\sum_{i=1}^{n_k} q_i^{(k)} = 1$。\n2. 使用上面给出的离散化公式和给定的箱体边界，计算真实值与每个模型之间的 $W_1^{(k)}$。\n3. 通过对所有可观测量 $k$ 求和 $W_1^{(k)}/s_k$，计算每个模型的无量纲聚合保真度 $\\mathcal{F}_i$。\n4. 使用 $(\\mathcal{F}_i, T_i)$，根据上述定义确定帕累托最优的索引集合，其中较低的 $\\mathcal{F}$ 和较低的 $T$ 更优。\n5. 对于每个测试用例，返回排序后的帕累托最优索引列表。\n\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素是对应测试用例的帕累托最优的从0开始的模型索引列表。例如，如果有三个测试用例，且各自的帕累托前沿包含索引 $\\{0,2\\}$, $\\{1\\}$, 和 $\\{0,1,2\\}$，则打印 `[[0,2],[1],[0,1,2]]`。\n\n物理和数值单位：\n- 每个事件的壁钟时间 $T$ 必须以秒为单位处理。\n- 与能量相关的可观测量单位为吉电子伏特，表示为 $\\mathrm{GeV}$。\n- 空间可观测量单位为毫米，表示为 $\\mathrm{mm}$。\n- 将每个 $W_1^{(k)}$ 用其对应的尺度 $s_k$（与可观测量 $k$ 的单位相同）进行归一化，以确保 $\\mathcal{F}$ 是无量纲的。\n- 此问题中没有角度。\n\n测试套件：\n对于每个测试用例，按顺序有三个模型：索引为 $0$ 的 $\\text{GAN}$，索引为 $1$ 的 $\\text{VAE}$，以及索引为 $2$ 的 $\\text{Diffusion}$。每个测试用例提供两个可观测量：量能器簇射能量（单位 $\\mathrm{GeV}$）和纵向簇射长度（单位 $\\mathrm{mm}$）。\n\n请完全按照给定的参数集使用：\n\n- 测试用例 1（理想的权衡情况）：\n  - 可观测量 1（能量，单位 $\\mathrm{GeV}$）：箱体边界 $[\\,0,10,20,30,35,42,50\\,]$，真实计数 $[\\,5,20,35,25,10,5\\,]$，$\\text{GAN}$ 计数 $[\\,4,18,36,26,12,4\\,]$，$\\text{VAE}$ 计数 $[\\,6,22,34,23,10,5\\,]$，$\\text{Diffusion}$ 计数 $[\\,5,19,35,25,11,5\\,]$，尺度 $s_1 = 50$。\n  - 可观测量 2（长度，单位 $\\mathrm{mm}$）：箱体边界 $[\\,0,50,100,200,400\\,]$，真实计数 $[\\,10,30,40,20\\,]$，$\\text{GAN}$ 计数 $[\\,12,28,40,20\\,]$，$\\text{VAE}$ 计数 $[\\,9,31,39,21\\,]$，$\\text{Diffusion}$ 计数 $[\\,10,30,41,19\\,]$，尺度 $s_2 = 400$。\n  - 时间（秒）：$[\\,0.003, 0.005, 0.05\\,]$。\n\n- 测试用例 2（保真度和时间出现平局）：\n  - 可观测量 1（能量，单位 $\\mathrm{GeV}$）：箱体边界 $[\\,0,10,20,30,35,42,50\\,]$，真实计数 $[\\,8,22,30,25,10,5\\,]$，$\\text{GAN}$ 计数 $[\\,8,22,30,25,10,5\\,]$，$\\text{VAE}$ 计数 $[\\,8,22,30,25,10,5\\,]$，$\\text{Diffusion}$ 计数 $[\\,7,21,31,25,11,5\\,]$，尺度 $s_1 = 50$。\n  - 可观测量 2（长度，单位 $\\mathrm{mm}$）：箱体边界 $[\\,0,50,100,200,400\\,]$，真实计数 $[\\,15,35,30,20\\,]$，$\\text{GAN}$ 计数 $[\\,15,35,30,20\\,]$，$\\text{VAE}$ 计数 $[\\,15,35,30,20\\,]$，$\\text{Diffusion}$ 计数 $[\\,14,36,30,20\\,]$，尺度 $s_2 = 400$。\n  - 时间（秒）：$[\\,0.004, 0.004, 0.04\\,]$。\n\n- 测试用例 3（强烈的权衡极端情况）：\n  - 可观测量 1（能量，单位 $\\mathrm{GeV}$）：箱体边界 $[\\,0,10,20,30,35,42,50\\,]$，真实计数 $[\\,10,25,30,20,10,5\\,]$，$\\text{GAN}$ 计数 $[\\,15,20,25,20,15,5\\,]$，$\\text{VAE}$ 计数 $[\\,11,24,30,20,10,5\\,]$，$\\text{Diffusion}$ 计数 $[\\,10,25,30,20,10,5\\,]$，尺度 $s_1 = 50$。\n  - 可观测量 2（长度，单位 $\\mathrm{mm}$）：箱体边界 $[\\,0,50,100,200,400\\,]$，真实计数 $[\\,12,28,40,20\\,]$，$\\text{GAN}$ 计数 $[\\,20,30,35,15\\,]$，$\\text{VAE}$ 计数 $[\\,13,27,39,21\\,]$，$\\text{Diffusion}$ 计数 $[\\,12,28,40,20\\,]$，尺度 $s_2 = 400$。\n  - 时间（秒）：$[\\,0.001, 0.003, 0.1\\,]$。\n\n- 测试用例 4（完全重复）：\n  - 可观测量 1（能量，单位 $\\mathrm{GeV}$）：箱体边界 $[\\,0,10,20,30,35,42,50\\,]$，真实计数 $[\\,5,20,35,25,10,5\\,]$，$\\text{GAN}$ 计数 $[\\,5,20,35,25,10,5\\,]$，$\\text{VAE}$ 计数 $[\\,5,20,35,25,10,5\\,]$，$\\text{Diffusion}$ 计数 $[\\,5,20,35,25,10,5\\,]$，尺度 $s_1 = 50$。\n  - 可观测量 2（长度，单位 $\\mathrm{mm}$）：箱体边界 $[\\,0,50,100,200,400\\,]$，真实计数 $[\\,10,30,40,20\\,]$，$\\text{GAN}$ 计数 $[\\,10,30,40,20\\,]$，$\\text{VAE}$ 计数 $[\\,10,30,40,20\\,]$，$\\text{Diffusion}$ 计数 $[\\,10,30,40,20\\,]$，尺度 $s_2 = 400$。\n  - 时间（秒）：$[\\,0.01, 0.01, 0.01\\,]$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个单独的列表，每个测试用例对应一个子列表。每个子列表必须包含该测试用例的帕累托最优模型的已排序的从0开始的索引。例如，如果计算出的前沿分别为 $\\{0,2\\}$, $\\{0,1\\}$, $\\{0,1,2\\}, [1]$，则返回 `[[0,2],[0,1],[0,1,2],[1]]`。", "solution": "总的目标是比较用于计算高能物理中快速模拟的不同生成模型（特别是生成对抗网络（GANs）、变分自编码器（VAEs）和扩散模型）的保真度和计算成本。该过程基于形式化的概率度量和算法决策理论。\n\n从保真度度量开始。我们采用第一瓦瑟斯坦距离。其基本定义是\n$$\nW_1(p,q) = \\inf_{\\gamma \\in \\Pi(p,q)} \\int_{\\mathbb{R} \\times \\mathbb{R}} |x-y| \\, \\mathrm{d}\\gamma(x,y),\n$$\n该定义捕捉了当成本为绝对距离时，在 $p$ 和 $q$ 之间进行传输的最小期望成本。对于具有累积分布函数 $F_p(x)$ 和 $F_q(x)$ 的一维分布，成立\n$$\nW_1(p,q) = \\int_{-\\infty}^{\\infty} \\left| F_p(x) - F_q(x) \\right| \\, \\mathrm{d}x,\n$$\n这是一个经过充分检验的积分表示，特别适用于使用直方图进行数值近似。\n\n为了进行离散化，考虑一个具有箱体边界 $\\{x_0, x_1, \\dots, x_n\\}$、每箱计数 $\\{c_i\\}_{i=1}^{n}$ 和归一化概率 $p_i = c_i / \\sum_{j=1}^{n} c_j$ 的直方图。对于模型，同样有 $q_i = d_i / \\sum_{j=1}^{n} d_j$。在每个箱体内部，概率密度被近似为常数，而在箱体边界处的累积差异捕捉了累积分布函数的差异。一个一致的一维离散化方法是\n$$\nW_1(p,q) \\approx \\sum_{i=1}^{n} \\left| \\sum_{j=1}^{i} (p_j - q_j) \\right| \\, \\Delta x_i,\n$$\n其中 $\\Delta x_i = x_i - x_{i-1}$ 是箱体宽度。这是通过对跨箱体的累积质量的绝对差异求和得出的，每个差异都由其几何跨度加权，与绝对累积分布函数差异的积分相一致。\n\n由于不同的可观测量具有不同的单位，直接将它们的瓦瑟斯坦距离相加会混合维度。为确保聚合保真度是无量纲的，定义特征尺度 $s_k$（与可观测量 $k$ 具有相同单位）并计算\n$$\n\\mathcal{F} = \\sum_{k=1}^{K} \\frac{W_1^{(k)}}{s_k},\n$$\n该值通过构造是无量纲的。这种选择是一种经过充分检验的归一化方法，允许跨可观测量进行比较。\n\n对于计算成本，我们使用每个事件的平均壁钟时间 $T_i$（以秒为单位），这是对速度的直接度量。比较是在二维 $(\\mathcal{F}_i, T_i)$ 中通过帕累托最优性来构建的，两者都是越低越好。形式上，模型 $i$ 是帕累托最优的，如果不存在模型 $j$ 使得\n$$\n\\mathcal{F}_j \\le \\mathcal{F}_i \\quad \\text{and} \\quad T_j \\le T_i,\n$$\n且至少有一个不等式是严格的。这个定义捕捉了帕累托最优模型在两个目标上均不被任何其他模型同时支配的概念。\n\n算法步骤：\n1. 对于每个可观测量，将真实值和每个模型的直方图计数归一化，以获得每箱概率 $\\{p_i^{(k)}\\}$ 和 $\\{q_i^{(k)}\\}$。\n2. 使用离散化的累积差异公式计算 $W_1^{(k)}$。具体来说，计算每箱差异 $\\delta_i^{(k)} = p_i^{(k)} - q_i^{(k)}$，然后计算累积差异 $S_i^{(k)} = \\sum_{j=1}^{i} \\delta_j^{(k)}$，最后求和 $\\sum_{i=1}^{n_k} |S_i^{(k)}| \\, \\Delta x_i^{(k)}$。\n3. 对每个模型 $i$，通过对所有可观测量求和归一化距离 $W_1^{(k)}/s_k$ 来得到 $\\mathcal{F}_i$。\n4. 对每个模型 $i$，检查是否存在任何模型 $j$ 同时满足 $\\mathcal{F}_j \\le \\mathcal{F}_i$ 和 $T_j \\le T_i$，且至少有一个严格不等式。如果存在这样的 $j$，则 $i$ 被支配，不是帕累托最优的；否则，$i$ 是帕累托最优的。\n5. 对每个测试用例，对得到的帕累托最优模型的索引进行排序，以产生一个规范的、可测试的输出。\n\n所提供的四个测试用例涵盖了不同方面：\n- 速度和保真度之间存在张力的典型权衡。\n- 多个模型达到相同的 $(\\mathcal{F}, T)$ 值的平局情况，确保重复点被正确保留在帕累托集合中。\n- 一个模型非常快但保真度较低，而另一个模型保真度很高但速度慢的极端权衡。\n- 所有模型在保真度和时间上完全相同，测试是否所有模型都被包括在内。\n\n最后，以指定的单行格式返回每个测试用例的帕累托索引列表。此设计集成了基本的概率度量、具有物理意义的单位和归一化，以及原则性的多目标决策分析，从而为高能物理中的快速模拟生成模型提供了一个稳健、可验证的计算比较。", "answer": "```python\nimport numpy as np\n\ndef w1_histogram(bin_edges, truth_counts, model_counts):\n    \"\"\"\n    Compute the 1D Wasserstein-1 distance (Earth Mover's Distance) between two\n    discretized distributions given as histograms with specified bin edges.\n    \"\"\"\n    bin_edges = np.asarray(bin_edges, dtype=float)\n    t = np.asarray(truth_counts, dtype=float)\n    m = np.asarray(model_counts, dtype=float)\n    if len(bin_edges) != len(t) + 1 or len(t) != len(m):\n        raise ValueError(\"Inconsistent histogram shapes: edges must be length n+1 and counts length n.\")\n    if t.sum() == 0 or m.sum() == 0:\n        raise ValueError(\"Counts must be positive to form a probability distribution.\")\n\n    # Normalize to probabilities.\n    p = t / t.sum()\n    q = m / m.sum()\n\n    # Differences and cumulative differences.\n    diff = p - q\n    cumdiff = np.cumsum(diff)\n\n    widths = np.diff(bin_edges)\n    w1 = np.sum(np.abs(cumdiff[:-1]) * widths[:-1]) + np.abs(cumdiff[-1] * widths[-1]) if len(cumdiff) > 1 else np.abs(cumdiff[-1] * widths[-1])\n    return float(w1)\n\n\ndef aggregated_fidelity(observables, model_index):\n    \"\"\"\n    Compute the dimensionless aggregated fidelity for a given model_index\n    across all observables, as sum_k W1_k / s_k.\n    \"\"\"\n    total = 0.0\n    for obs in observables:\n        edges = obs[\"edges\"]\n        truth = obs[\"truth\"]\n        model_counts = obs[\"models\"][model_index]\n        scale = float(obs[\"scale\"])\n        # w1_histogram needs a slight correction for the last bin of cumdiff\n        # which is always zero for normalized histograms. So we should not use it in sum.\n        \n        t = np.asarray(truth, dtype=float)\n        m = np.asarray(model_counts, dtype=float)\n        p = t / t.sum()\n        q = m / m.sum()\n        diff = p - q\n        cumdiff = np.cumsum(diff)\n        widths = np.diff(edges)\n        \n        # The last element of cumdiff is approx. 0 due to normalization.\n        # The formula is sum |CDF_p(x_i) - CDF_q(x_i)| * dx_i\n        # For histograms, this is sum |cumulative_prob_diff_up_to_bin_i| * width_of_bin_i\n        w1 = np.sum(np.abs(cumdiff) * widths)\n\n        total += w1 / scale\n    return total\n\n\ndef pareto_optimal_indices(fidelities, times):\n    \"\"\"\n    Identify Pareto-optimal indices given fidelity (lower is better) and time (lower is better).\n    \"\"\"\n    n = len(fidelities)\n    is_pareto = [True] * n\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                continue\n            # Check if j dominates i\n            if (fidelities[j] = fidelities[i] and times[j] = times[i]) and \\\n               (fidelities[j]  fidelities[i] or times[j]  times[i]):\n                is_pareto[i] = False\n                break\n    result = [i for i, flag in enumerate(is_pareto) if flag]\n    result.sort()\n    return result\n\n\ndef solve():\n    test_cases = [\n        {\n            \"observables\": [\n                {\n                    \"edges\": np.array([0, 10, 20, 30, 35, 42, 50], dtype=float),\n                    \"truth\": np.array([5, 20, 35, 25, 10, 5], dtype=float),\n                    \"models\": [\n                        np.array([4, 18, 36, 26, 12, 4], dtype=float),\n                        np.array([6, 22, 34, 23, 10, 5], dtype=float),\n                        np.array([5, 19, 35, 25, 11, 5], dtype=float),\n                    ],\n                    \"scale\": 50.0,\n                },\n                {\n                    \"edges\": np.array([0, 50, 100, 200, 400], dtype=float),\n                    \"truth\": np.array([10, 30, 40, 20], dtype=float),\n                    \"models\": [\n                        np.array([12, 28, 40, 20], dtype=float),\n                        np.array([9, 31, 39, 21], dtype=float),\n                        np.array([10, 30, 41, 19], dtype=float),\n                    ],\n                    \"scale\": 400.0,\n                },\n            ],\n            \"times\": np.array([0.003, 0.005, 0.05], dtype=float),\n        },\n        {\n            \"observables\": [\n                {\n                    \"edges\": np.array([0, 10, 20, 30, 35, 42, 50], dtype=float),\n                    \"truth\": np.array([8, 22, 30, 25, 10, 5], dtype=float),\n                    \"models\": [\n                        np.array([8, 22, 30, 25, 10, 5], dtype=float),\n                        np.array([8, 22, 30, 25, 10, 5], dtype=float),\n                        np.array([7, 21, 31, 25, 11, 5], dtype=float),\n                    ],\n                    \"scale\": 50.0,\n                },\n                {\n                    \"edges\": np.array([0, 50, 100, 200, 400], dtype=float),\n                    \"truth\": np.array([15, 35, 30, 20], dtype=float),\n                    \"models\": [\n                        np.array([15, 35, 30, 20], dtype=float),\n                        np.array([15, 35, 30, 20], dtype=float),\n                        np.array([14, 36, 30, 20], dtype=float),\n                    ],\n                    \"scale\": 400.0,\n                },\n            ],\n            \"times\": np.array([0.004, 0.004, 0.04], dtype=float),\n        },\n        {\n            \"observables\": [\n                {\n                    \"edges\": np.array([0, 10, 20, 30, 35, 42, 50], dtype=float),\n                    \"truth\": np.array([10, 25, 30, 20, 10, 5], dtype=float),\n                    \"models\": [\n                        np.array([15, 20, 25, 20, 15, 5], dtype=float),\n                        np.array([11, 24, 30, 20, 10, 5], dtype=float),\n                        np.array([10, 25, 30, 20, 10, 5], dtype=float),\n                    ],\n                    \"scale\": 50.0,\n                },\n                {\n                    \"edges\": np.array([0, 50, 100, 200, 400], dtype=float),\n                    \"truth\": np.array([12, 28, 40, 20], dtype=float),\n                    \"models\": [\n                        np.array([20, 30, 35, 15], dtype=float),\n                        np.array([13, 27, 39, 21], dtype=float),\n                        np.array([12, 28, 40, 20], dtype=float),\n                    ],\n                    \"scale\": 400.0,\n                },\n            ],\n            \"times\": np.array([0.001, 0.003, 0.1], dtype=float),\n        },\n        {\n            \"observables\": [\n                {\n                    \"edges\": np.array([0, 10, 20, 30, 35, 42, 50], dtype=float),\n                    \"truth\": np.array([5, 20, 35, 25, 10, 5], dtype=float),\n                    \"models\": [\n                        np.array([5, 20, 35, 25, 10, 5], dtype=float),\n                        np.array([5, 20, 35, 25, 10, 5], dtype=float),\n                        np.array([5, 20, 35, 25, 10, 5], dtype=float),\n                    ],\n                    \"scale\": 50.0,\n                },\n                {\n                    \"edges\": np.array([0, 50, 100, 200, 400], dtype=float),\n                    \"truth\": np.array([10, 30, 40, 20], dtype=float),\n                    \"models\": [\n                        np.array([10, 30, 40, 20], dtype=float),\n                        np.array([10, 30, 40, 20], dtype=float),\n                        np.array([10, 30, 40, 20], dtype=float),\n                    ],\n                    \"scale\": 400.0,\n                },\n            ],\n            \"times\": np.array([0.01, 0.01, 0.01], dtype=float),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        observables = case[\"observables\"]\n        times = case[\"times\"]\n        num_models = len(observables[0][\"models\"])\n        fidelities = [aggregated_fidelity(observables, i) for i in range(num_models)]\n        pareto = pareto_optimal_indices(np.array(fidelities), np.array(times))\n        results.append(pareto)\n\n    print(str(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3515649"}, {"introduction": "完成此练习后，你将能够为一个基于生成模型的物理分析进行严谨的不确定性量化 [@problem_id:3515622]。你将通过实践学习如何应用非参数自助法（nonparametric bootstrap）来估计由有限蒙特卡洛样本量引入的统计不确定性，并最终报告一个带有置信区间的物理测量结果，这是任何严谨科学分析中不可或缺的组成部分。", "problem": "考虑一个基于生成模型（如生成对抗网络（GAN）或变分自编码器（VAE））的快速探测器模拟代理。生成器接收一个潜变量 $z$，并生成一个单一事件级标量可观测量 $x$，为具体起见，它代表一个以吉电子伏特 (GeV) 为单位测量的类横动量。潜变量被建模为 $z \\sim \\mathcal{N}(0,1)$，即标准正态分布。生成器映射被定义为一个确定性函数\n$$\nG(z) = \\max\\left(0, \\, \\mu + \\sigma z + \\kappa \\left(z^2 - 1\\right)\\right),\n$$\n其中 $\\mu$ 是以 $\\mathrm{GeV}$ 为单位的位置参数，$\\sigma$ 是以 $\\mathrm{GeV}$ 为单位的尺度参数，$\\kappa$ 控制以 $\\mathrm{GeV}$ 为单位的非高斯修正。如果一个事件通过了由下式给出的阈值谓词 $S(x)$，则认为该事件被选中\n$$\nS(x) = \\begin{cases}\n1  \\text{if } x \\ge \\tau, \\\\\n0  \\text{otherwise},\n\\end{cases}\n$$\n阈值 $\\tau$ 的单位是 $\\mathrm{GeV}$。设 $\\sigma_{\\mathrm{ref}}$ 为筛选前基础过程的已知参考产生截面，单位为皮靶 ($\\mathrm{pb}$)。筛选后的截面是\n$$\n\\sigma = \\sigma_{\\mathrm{ref}} \\, A,\n$$\n其中 $A = \\mathbb{E}\\left[S\\left(G(z)\\right)\\right]$ 是接受度，定义为筛选指示函数在潜变量分布下的期望值。使用 $N$ 个独立潜变量抽样 $z_i \\sim \\mathcal{N}(0,1)$（其中 $i = 1, \\ldots, N$），接受度的蒙特卡洛估计量为\n$$\n\\hat{A} = \\frac{1}{N} \\sum_{i=1}^N S\\left(G(z_i)\\right), \\quad \\text{and} \\quad \\hat{\\sigma} = \\sigma_{\\mathrm{ref}} \\, \\hat{A}.\n$$\n为传播由有限潜变量抽样引起的生成器不确定性，对潜变量抽样使用非参数自助法，步骤如下：构建 $B$ 个自助法复制样本，对于每个复制样本 $b$，从 $\\{1, \\ldots, N\\}$ 中有放回地抽样一个索引多重集 $\\{i_1^{(b)}, \\ldots, i_N^{(b)}\\}$，然后计算\n$$\n\\hat{A}^{(b)} = \\frac{1}{N} \\sum_{j=1}^N S\\left(G\\left(z_{i_j^{(b)}}\\right)\\right), \\quad \\hat{\\sigma}^{(b)} = \\sigma_{\\mathrm{ref}} \\, \\hat{A}^{(b)},\n$$\n并使用下分位数和上分位数，在显著性水平 $\\alpha$ 下构建等尾自助法置信区间：\n$$\nQ_{\\alpha/2} = \\text{quantile}\\left(\\{\\hat{\\sigma}^{(b)}\\}_{b=1}^B, \\alpha/2\\right), \\quad Q_{1-\\alpha/2} = \\text{quantile}\\left(\\{\\hat{\\sigma}^{(b)}\\}_{b=1}^B, 1-\\alpha/2\\right).\n$$\n置信区间的对称半宽度定义为：\n$$\n\\Delta \\hat{\\sigma} = \\frac{Q_{1-\\alpha/2} - Q_{\\alpha/2}}{2}.\n$$\n您的任务是实现此过程，并为每个测试用例报告数对 $[\\hat{\\sigma}, \\Delta \\hat{\\sigma}]$，两个量均以皮靶 ($\\mathrm{pb}$) 为单位。所有生成的事件级可观测量 $x$ 和阈值参数 $\\tau$ 的单位均为吉电子伏特 ($\\mathrm{GeV}$)。本问题不涉及角度。不应使用百分比；任何小数值都应以十进制数报告。\n\n使用的基本原理：\n- 接受度定义为 $z$ 分布下的期望 $A = \\mathbb{E}\\left[S\\left(G(z)\\right)\\right]$。\n- 使用独立样本 $z_i \\sim \\mathcal{N}(0,1)$ 的期望的蒙特卡洛估计量 $\\hat{A} = \\frac{1}{N} \\sum_{i=1}^N S\\left(G(z_i)\\right)$。\n- 通过对观测到的潜变量抽样进行有放回的重抽样并重新计算估计量，使用非参数自助法来估计 $\\hat{\\sigma}$ 的抽样分布。\n\n实现以下测试套件。每个测试用例指定 $(\\text{seed}, N, \\mu, \\sigma, \\kappa, \\tau, \\sigma_{\\mathrm{ref}}, B, \\alpha)$，其中所有 $\\mu$、$\\sigma$、$\\kappa$ 和 $\\tau$ 的单位为 $\\mathrm{GeV}$，$\\sigma_{\\mathrm{ref}}$ 的单位为 $\\mathrm{pb}$：\n- 测试用例 1：$(12345, 4000, 50, 10, 2, 40, 80, 300, 0.32)$。\n- 测试用例 2：$(24680, 4000, 50, 10, 2, 70, 80, 300, 0.32)$。\n- 测试用例 3：$(98765, 4000, 50, 10, 2, 10, 80, 300, 0.32)$。\n- 测试用例 4：$(13579, 200, 50, 10, 2, 50, 80, 800, 0.32)$。\n\n您的程序应产生单行输出，其中包含结果，形式为由方括号括起来的、按测试用例排列的数对的逗号分隔列表。每个数对是一个双元素列表 $[\\hat{\\sigma}, \\Delta \\hat{\\sigma}]$，单位为皮靶 ($\\mathrm{pb}$)。例如，格式必须如下：\n$$\n[[s_1, d_1],[s_2, d_2],[s_3, d_3],[s_4, d_4]],\n$$\n其中 $s_i$ 和 $d_i$ 是为测试用例 $i$ 计算出的十进制数，单位为皮靶 ($\\mathrm{pb}$)。", "solution": "该问题是有效的。它在计算高能物理领域提出了一个清晰、自洽且科学上合理的任务，要求为一个定义明确的生成式模拟玩具模型实现一个标准统计程序（带有自助法不确定性的蒙特卡洛估计）。所有参数、定义和程序都得到了明确的规定。\n\n任务是为一个由生成代理建模的物理过程计算估计的筛选后截面 $\\hat{\\sigma}$ 及其置信区间的对称半宽度 $\\Delta \\hat{\\sigma}$。该过程针对几个测试用例执行，每个测试用例由一组特定的参数定义。\n\n每个测试用例的计算逻辑流程如下：\n\n1.  **潜变量的生成：** 过程从潜空间开始。我们被指示从标准正态分布 $z_i \\sim \\mathcal{N}(0,1)$ 中抽取 $N$ 个独立样本，记为 $z_i$（其中 $i=1, \\ldots, N$）。这是使用一个以特定种子初始化的伪随机数生成器来执行的，以确保可复现性。\n\n2.  **可观测事件的生成：** 每个潜变量 $z_i$ 通过生成器函数 $G(z)$ 映射到一个物理可观测量 $x_i$。该函数由下式给出：\n    $$\n    x_i = G(z_i) = \\max\\left(0, \\, \\mu + \\sigma z_i + \\kappa \\left(z_i^2 - 1\\right)\\right)\n    $$\n    在这里，$\\mu$、$\\sigma$ 和 $\\kappa$ 是生成器模型的参数，单位为吉电子伏特 ($\\mathrm{GeV}$)。该函数将高斯潜分布转换为可观测量 $x$ 的非高斯分布，其中 $x$ 代表像横动量这样的量。$\\max(0, \\cdot)$ 操作确保了可观测量为非负，这是动量的一个物理要求。\n\n3.  **事件筛选与接受度估计：** 如果一个事件的可观测量 $x_i$ 通过了由谓词 $S(x)$ 定义的某个截断，则该事件被“选中”。如果 $x_i \\ge \\tau$，则事件被保留，其中 $\\tau$ 是一个给定的阈值，单位为 $\\mathrm{GeV}$。在数学上，筛选谓词为：如果 $x_i \\ge \\tau$，则 $S(x_i) = 1$；否则 $S(x_i) = 0$。接受度 $A$ 是一个随机生成的事件通过此筛选的概率。我们使用对 $N$ 个生成事件的蒙特卡洛平均来估计它：\n    $$\n    \\hat{A} = \\frac{1}{N} \\sum_{i=1}^N S\\left(G(z_i)\\right) = \\frac{\\text{Number of events with } x_i \\ge \\tau}{N}\n    $$\n\n4.  **中心截面估计：** 估计的筛选后截面 $\\hat{\\sigma}$ 是通过将已知的参考截面 $\\sigma_{\\mathrm{ref}}$（单位为皮靶, $\\mathrm{pb}$）与估计的接受度 $\\hat{A}$ 相乘计算得出的：\n    $$\n    \\hat{\\sigma} = \\sigma_{\\mathrm{ref}} \\cdot \\hat{A}\n    $$\n    这个值 $\\hat{\\sigma}$ 构成了所需输出数对 $[\\hat{\\sigma}, \\Delta \\hat{\\sigma}]$ 的第一部分。\n\n5.  **通过非参数自助法进行不确定性估计：** $\\hat{\\sigma}$ 上的统计不确定性源于蒙特卡洛样本的有限大小 $N$。这种不确定性使用非参数自助法来估计。其过程如下：\n    a. 我们创建 $B$ 个自助法复制样本。对于每个复制样本 $b=1, \\ldots, B$：\n    b. 通过从原始的 $N$ 个生成事件集合中进行*有放回*抽样，形成一个新的包含 $N$ 个事件的集合。更高效的做法是，我们可以对筛选结果 $\\{S(x_1), \\ldots, S(x_N)\\}$ 进行重抽样。设复制样本 $b$ 的重抽样结果为 $\\{S_1^{(b)}, \\ldots, S_N^{(b)}\\}$。\n    c. 从这个新集合中计算接受度的自助法估计 $\\hat{A}^{(b)}$：\n    $$\n    \\hat{A}^{(b)} = \\frac{1}{N} \\sum_{j=1}^N S_j^{(b)}\n    $$\n    d. 然后用它来计算截面的自助法复制样本：\n    $$\n    \\hat{\\sigma}^{(b)} = \\sigma_{\\mathrm{ref}} \\cdot \\hat{A}^{(b)}\n    $$\n    对所有 $B$ 个复制样本重复此过程，会得到一个估计量分布 $\\{\\hat{\\sigma}^{(1)}, \\hat{\\sigma}^{(2)}, \\ldots, \\hat{\\sigma}^{(B)}\\}$，它作为 $\\hat{\\sigma}$ 真实抽样分布的一个经验近似。\n\n6.  **置信区间半宽度计算：** 从排序后的自助法复制样本分布中构建 $\\hat{\\sigma}$ 的等尾置信区间。对于给定的显著性水平 $\\alpha$，区间的下界和上界是自助法分布的分位数 $Q_{\\alpha/2}$ 和 $Q_{1-\\alpha/2}$。\n    $$\n    Q_{\\alpha/2} = \\text{quantile}\\left(\\{\\hat{\\sigma}^{(b)}\\}_{b=1}^B, \\alpha/2\\right)\n    $$\n    $$\n    Q_{1-\\alpha/2} = \\text{quantile}\\left(\\{\\hat{\\sigma}^{(b)}\\}_{b=1}^B, 1-\\alpha/2\\right)\n    $$\n    问题要求计算该置信区间的对称半宽度 $\\Delta \\hat{\\sigma}$，其定义为：\n    $$\n    \\Delta \\hat{\\sigma} = \\frac{Q_{1-\\alpha/2} - Q_{\\alpha/2}}{2}\n    $$\n    这个值 $\\Delta \\hat{\\sigma}$ 是输出数对 $[\\hat{\\sigma}, \\Delta \\hat{\\sigma}]$ 的第二部分。\n\n对所提供的四个测试用例中的每一个，都使用其各自的参数集来实现这整个过程。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases, calculating the estimated cross section\n    and its bootstrap uncertainty half-width.\n    \"\"\"\n    test_cases = [\n        # (seed, N, mu, sigma, kappa, tau, sigma_ref, B, alpha)\n        (12345, 4000, 50, 10, 2, 40, 80, 300, 0.32),\n        (24680, 4000, 50, 10, 2, 70, 80, 300, 0.32),\n        (98765, 4000, 50, 10, 2, 10, 80, 300, 0.32),\n        (13579, 200, 50, 10, 2, 50, 80, 800, 0.32),\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, N, mu, sigma, kappa, tau, sigma_ref, B, alpha = case\n\n        # Initialize a random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Generate N latent variables z from N(0,1).\n        z_samples = rng.standard_normal(size=N)\n\n        # Step 2: Apply the generator function G(z) to get observables x.\n        # G(z) = max(0, mu + sigma*z + kappa*(z^2 - 1))\n        poly_part = mu + sigma * z_samples + kappa * (np.square(z_samples) - 1)\n        x_samples = np.maximum(0, poly_part)\n\n        # Step 3: Apply the selection predicate S(x) and get a boolean mask.\n        # S(x) = 1 if x >= tau, 0 otherwise.\n        selected_mask = (x_samples >= tau)\n\n        # Step 4: Calculate the central estimate of acceptance and cross section.\n        # A_hat = mean(S(G(z_i)))\n        A_hat = np.mean(selected_mask)\n        # sigma_hat = sigma_ref * A_hat\n        sigma_hat = sigma_ref * A_hat\n\n        # Step 5: Perform nonparametric bootstrap to estimate uncertainty.\n        sigma_hat_bootstrap = np.empty(B)\n        \n        # We can efficiently bootstrap by resampling the boolean selection mask.\n        # This avoids recomputing G(z) and S(x) for each bootstrap replicate.\n        for b in range(B):\n            # Resample indices with replacement from the N original samples.\n            bootstrap_indices = rng.choice(N, size=N, replace=True)\n            # Create a resampled selection mask.\n            resampled_mask = selected_mask[bootstrap_indices]\n            # Calculate the bootstrap replicate of acceptance.\n            A_hat_b = np.mean(resampled_mask)\n            # Calculate the bootstrap replicate of the cross section.\n            sigma_hat_bootstrap[b] = sigma_ref * A_hat_b\n\n        # Step 6: Compute the confidence interval and its symmetric half-width.\n        q_low_frac = alpha / 2.0\n        q_high_frac = 1.0 - alpha / 2.0\n        \n        # Calculate quantiles of the bootstrap distribution.\n        q_low = np.quantile(sigma_hat_bootstrap, q_low_frac)\n        q_high = np.quantile(sigma_hat_bootstrap, q_high_frac)\n        \n        # Calculate the symmetric half-width.\n        delta_sigma_hat = (q_high - q_low) / 2.0\n        \n        results.append([sigma_hat, delta_sigma_hat])\n\n    # Format the final output as a string representation of a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3515622"}]}