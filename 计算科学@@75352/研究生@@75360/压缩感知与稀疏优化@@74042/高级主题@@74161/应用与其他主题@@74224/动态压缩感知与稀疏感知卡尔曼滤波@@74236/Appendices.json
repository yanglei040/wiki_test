{"hands_on_practices": [{"introduction": "要真正掌握稀疏卡尔曼滤波，我们必须首先理解其核心的单步更新机制。本练习旨在剖析这一过程，通过一个简化的模型，您将推导并计算最大后验（MAP）估计。这个练习将帮助您从第一性原理出发，直观地理解先验信念、新的测量数据以及稀疏性约束（通过$L_1$范数实现）是如何共同作用，从而产生一个更新后的稀疏状态估计的 [@problem_id:3445409]。", "problem": "考虑在卡尔曼滤波器（KF）的一种稀疏感知变体下的动态稀疏状态估计问题的单个时间步。状态转移是单位矩阵，即 $F=I$，当前时间步的先验是高斯分布 $x \\sim \\mathcal{N}(\\mu, P)$，其中 $P=\\tau^2 I$。测量模型为 $y = H x + v$，其中 $v \\sim \\mathcal{N}(0, R)$ 且 $R=\\sigma^2 I$。为了促进估计的稀疏性，我们在负对数后验中增加了一个 $\\ell_1$ 惩罚项 $\\lambda \\|x\\|_1$，这等价于对 $x$ 施加一个拉普拉斯先验。\n\n使用以下具体实例：\n- 维度 $n=5$，测量维度 $m=3$。\n- 测量矩阵 $H$ 的行是标准正交的，并作为坐标 $\\{1,3,5\\}$ 的选择器：\n  $$H=\\begin{pmatrix}1  0  0  0  0\\\\ 0  0  1  0  0\\\\ 0  0  0  0  1\\end{pmatrix}.$$\n- 噪声方差为 $\\sigma^2=0.04$，先验方差为 $\\tau^2=0.25$，稀疏性权重为 $\\lambda=0.15$。\n- 先验均值为 $\\mu=\\begin{pmatrix}0.02 \\\\ -0.08 \\\\ 0 \\\\ 0.03 \\\\ -0.01\\end{pmatrix}$，测量值为 $y=\\begin{pmatrix}0.10\\\\ -0.09\\\\ 0.02\\end{pmatrix}$。\n\n将此时间步的稀疏感知最大后验（MAP）估计 $x^\\star$ 定义为下式的唯一最小化子：\n$$\n\\min_{x\\in\\mathbb{R}^5}\\ \\frac{1}{2\\sigma^2}\\|y-Hx\\|_2^2+\\frac{1}{2\\tau^2}\\|x-\\mu\\|_2^2+\\lambda\\|x\\|_1.\n$$\n从高斯似然和高斯先验模型以及 $\\ell_1$ 惩罚项出发，根据第一性原理推导最优性条件的坐标形式以及最终的软阈值解。然后，根据给定数据计算显式向量 $x^\\star$，并确定在 $\\mu$ 和 $x^\\star$ 之间支撑集发生变化的索引集合；即支撑集的对称差\n$$\n\\Delta=\\{i:\\ (\\mu_i=0\\ \\text{and}\\ x^\\star_i\\neq 0)\\ \\text{or}\\ (\\mu_i\\neq 0\\ \\text{and}\\ x^\\star_i=0)\\}.\n$$\n\n最终答案请提供 $\\Delta$ 的基数，形式为一个整数。不要四舍五入；要求提供精确整数。", "solution": "该问题是良态的，在稀疏信号恢复和贝叶斯估计领域有科学依据。所有必要的数据都已提供，没有内部矛盾。我们可以开始求解。\n\n目标是找到向量 $x^\\star \\in \\mathbb{R}^5$，使其最小化以下函数：\n$$\nJ(x) = \\frac{1}{2\\sigma^2}\\|y-Hx\\|_2^2+\\frac{1}{2\\tau^2}\\|x-\\mu\\|_2^2+\\lambda\\|x\\|_1\n$$\n该函数是两个严格凸的二次项和一个凸的 $\\ell_1$ 范数项的和。因此，该和是严格凸的，这保证了唯一最小化子 $x^\\star$ 的存在。\n\n最小化子 $x^\\star$ 由一阶最优性条件 $0 \\in \\partial J(x^\\star)$ 刻画，其中 $\\partial J(x)$ 是 $J(x)$ 的次梯度。函数 $J(x)$ 可以分解为 $J(x) = f(x) + g(x)$，其中 $f(x) = \\frac{1}{2\\sigma^2}\\|y-Hx\\|_2^2+\\frac{1}{2\\tau^2}\\|x-\\mu\\|_2^2$ 是可微的，而 $g(x) = \\lambda\\|x\\|_1$ 是不可微但凸的。那么最优性条件由下式给出：\n$$\n0 \\in \\nabla f(x^\\star) + \\partial g(x^\\star) \\implies -\\nabla f(x^\\star) \\in \\partial g(x^\\star)\n$$\n$f(x)$ 的梯度是：\n$$\n\\nabla f(x) = \\frac{1}{\\sigma^2} H^T(Hx - y) + \\frac{1}{\\tau^2}(x - \\mu)\n$$\n$g(x) = \\lambda \\sum_{i=1}^5 |x_i|$ 的次梯度是其各分量次梯度的笛卡尔积，即 $\\partial g(x) = \\lambda \\times_{i=1}^5 \\partial |x_i|$。绝对值函数在点 $z$ 的次梯度为：\n$$\n\\partial|z| = \\begin{cases} \\{\\text{sgn}(z)\\}  \\text{if } z \\neq 0 \\\\ [-1, 1]  \\text{if } z = 0 \\end{cases}\n$$\n测量矩阵 $H$ 的特定结构使得问题可以解耦。给定\n$$\nH=\\begin{pmatrix}1  0  0  0  0\\\\ 0  0  1  0  0\\\\ 0  0  0  0  1\\end{pmatrix}\n$$\n项 $\\|y-Hx\\|_2^2$ 展开为 $(y_1-x_1)^2 + (y_2-x_3)^2 + (y_3-x_5)^2$。其他项，$\\|x-\\mu\\|_2^2 = \\sum_{i=1}^5 (x_i-\\mu_i)^2$ 和 $\\|x\\|_1 = \\sum_{i=1}^5 |x_i|$，也按坐标可分。因此，总目标函数 $J(x)$ 可以写成五个独立函数之和，$J(x) = \\sum_{i=1}^5 J_i(x_i)$，它们可以被独立地最小化。\n\n我们可以将坐标分为两组：观测到的索引 $I_{obs} = \\{1, 3, 5\\}$ 和未观测到的索引 $I_{unobs} = \\{2, 4\\}$。\n\n**情况1：未观测到的索引 $i \\in \\{2, 4\\}$**\n对于这些索引，坐标 $x_i$ 的目标函数为：\n$$\nJ_i(x_i) = \\frac{1}{2\\tau^2}(x_i - \\mu_i)^2 + \\lambda|x_i|\n$$\n最优性条件是 $-\\frac{dJ_i}{dx_i}\\big|_{x_i \\neq 0} \\in \\lambda \\partial|x_i^\\star|$。二次项的导数是 $\\frac{1}{\\tau^2}(x_i-\\mu_i)$。这导致条件 $\\frac{1}{\\tau^2}(\\mu_i - x_i^\\star) \\in \\lambda \\cdot \\partial|x_i^\\star|$。\n这是求解 $\\ell_1$ 范数的近端算子的问题，其解是著名的软阈值函数：\n$$\nx_i^\\star = \\text{soft}(\\mu_i, \\lambda\\tau^2) = \\text{sgn}(\\mu_i) \\max(|\\mu_i| - \\lambda\\tau^2, 0)\n$$\n\n**情况2：观测到的索引 $i \\in \\{1, 3, 5\\}$**\n我们将测量索引 $j$ 与状态索引 $i$ 关联如下：$j=1 \\leftrightarrow i=1$，$j=2 \\leftrightarrow i=3$，$j=3 \\leftrightarrow i=5$。坐标 $x_i$ 的目标函数为：\n$$\nJ_i(x_i) = \\frac{1}{2\\sigma^2}(y_j - x_i)^2 + \\frac{1}{2\\tau^2}(x_i - \\mu_i)^2 + \\lambda|x_i|\n$$\n二次部分是 $\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)x_i^2 - 2\\left(\\frac{y_j}{\\sigma^2} + \\frac{\\mu_i}{\\tau^2}\\right)x_i \\right] + \\text{const}$。通过配方法，这部分与 $(x_i - z_{i,0})^2$ 成正比，其中\n$$\nz_{i,0} = \\frac{y_j/\\sigma^2 + \\mu_i/\\tau^2}{1/\\sigma^2 + 1/\\tau^2} = \\frac{\\tau^2 y_j + \\sigma^2 \\mu_i}{\\sigma^2 + \\tau^2}\n$$\n并且平方项的系数是 $\\frac{1}{2\\gamma_{obs}^2}$，其中 $\\frac{1}{\\gamma_{obs}^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}$，所以 $\\gamma_{obs}^2 = \\frac{\\sigma^2\\tau^2}{\\sigma^2+\\tau^2}$。\n对 $x_i$ 的最小化问题等价于最小化 $\\frac{1}{2\\gamma_{obs}^2}(x_i-z_{i,0})^2 + \\lambda|x_i|$。解同样由软阈值给出：\n$$\nx_i^\\star = \\text{soft}(z_{i,0}, \\lambda\\gamma_{obs}^2) = \\text{sgn}(z_{i,0}) \\max(|z_{i,0}| - \\lambda\\gamma_{obs}^2, 0)\n$$\n\n**数值计算**\n给定的参数值为：\n$\\sigma^2 = 0.04$，$\\tau^2 = 0.25$，$\\lambda = 0.15$。\n数据为：\n$\\mu=\\begin{pmatrix}0.02 \\\\ -0.08 \\\\ 0 \\\\ 0.03 \\\\ -0.01\\end{pmatrix}$ 和 $y=\\begin{pmatrix}0.10\\\\ -0.09\\\\ 0.02\\end{pmatrix}$。\n\n首先，我们计算两种情况下的阈值：\n未观测索引的阈值：$T_{unobs} = \\lambda\\tau^2 = 0.15 \\times 0.25 = 0.0375$。\n观测索引的阈值：$T_{obs} = \\lambda\\gamma_{obs}^2 = \\lambda \\frac{\\sigma^2\\tau^2}{\\sigma^2+\\tau^2} = 0.15 \\times \\frac{0.04 \\times 0.25}{0.04+0.25} = 0.15 \\times \\frac{0.01}{0.29} = \\frac{0.0015}{0.29}$。\n\n现在我们计算 $x^\\star$ 的每个分量：\n-   **$i=2$（未观测）：** $x_2^\\star = \\text{soft}(\\mu_2, T_{unobs}) = \\text{soft}(-0.08, 0.0375)$。因为 $|\\mu_2| = 0.08 > 0.0375$，所以 $x_2^\\star = - (0.08 - 0.0375) = -0.0425$。\n-   **$i=4$（未观测）：** $x_4^\\star = \\text{soft}(\\mu_4, T_{unobs}) = \\text{soft}(0.03, 0.0375)$。因为 $|\\mu_4| = 0.03  0.0375$，所以 $x_4^\\star = 0$。\n\n-   **$i=1$（观测，$j=1$）：**\n    $z_{1,0} = \\frac{\\tau^2 y_1 + \\sigma^2 \\mu_1}{\\sigma^2+\\tau^2} = \\frac{0.25(0.10) + 0.04(0.02)}{0.29} = \\frac{0.025 + 0.0008}{0.29} = \\frac{0.0258}{0.29}$。\n    $|z_{1,0}| = \\frac{0.0258}{0.29} > T_{obs} = \\frac{0.0015}{0.29}$。\n    $x_1^\\star = z_{1,0} - T_{obs} = \\frac{0.0258}{0.29} - \\frac{0.0015}{0.29} = \\frac{0.0243}{0.29} = \\frac{243}{2900}$。\n\n-   **$i=3$（观测，$j=2$）：**\n    $z_{3,0} = \\frac{\\tau^2 y_2 + \\sigma^2 \\mu_3}{\\sigma^2+\\tau^2} = \\frac{0.25(-0.09) + 0.04(0)}{0.29} = \\frac{-0.0225}{0.29}$。\n    $|z_{3,0}| = \\frac{0.0225}{0.29} > T_{obs} = \\frac{0.0015}{0.29}$。\n    $x_3^\\star = z_{3,0} + T_{obs} = \\frac{-0.0225}{0.29} + \\frac{0.0015}{0.29} = \\frac{-0.021}{0.29} = -\\frac{21}{290}$。\n\n-   **$i=5$（观测，$j=3$）：**\n    $z_{5,0} = \\frac{\\tau^2 y_3 + \\sigma^2 \\mu_5}{\\sigma^2+\\tau^2} = \\frac{0.25(0.02) + 0.04(-0.01)}{0.29} = \\frac{0.005 - 0.0004}{0.29} = \\frac{0.0046}{0.29}$。\n    $|z_{5,0}| = \\frac{0.0046}{0.29} > T_{obs} = \\frac{0.0015}{0.29}$。\n    $x_5^\\star = z_{5,0} - T_{obs} = \\frac{0.0046}{0.29} - \\frac{0.0015}{0.29} = \\frac{0.0031}{0.29} = \\frac{31}{2900}$。\n\n得到的 MAP 估计为 $x^\\star = \\begin{pmatrix} 243/2900 \\\\ -0.0425 \\\\ -21/290 \\\\ 0 \\\\ 31/2900 \\end{pmatrix}$。除了 $x_4^\\star$ 外，所有分量都非零。\n\n**支撑集变化分析**\n我们需要找到 $\\mu$ 和 $x^\\star$ 的支撑集的对称差：\n$\\Delta=\\{i:\\ (\\mu_i=0\\ \\text{and}\\ x^\\star_i\\neq 0)\\ \\text{or}\\ (\\mu_i\\neq 0\\ \\text{and}\\ x^\\star_i=0)\\}$。\n\n让我们逐一检查每个索引：\n-   $i=1$：$\\mu_1 = 0.02 \\neq 0$ 且 $x_1^\\star = \\frac{243}{2900} \\neq 0$。无变化。\n-   $i=2$：$\\mu_2 = -0.08 \\neq 0$ 且 $x_2^\\star = -0.0425 \\neq 0$。无变化。\n-   $i=3$：$\\mu_3 = 0$ 且 $x_3^\\star = -\\frac{21}{290} \\neq 0$。一个零元素变成了非零元素。$3 \\in \\Delta$。\n-   $i=4$：$\\mu_4 = 0.03 \\neq 0$ 且 $x_4^\\star = 0$。一个非零元素变成了零。$4 \\in \\Delta$。\n-   $i=5$：$\\mu_5 = -0.01 \\neq 0$ 且 $x_5^\\star = \\frac{31}{2900} \\neq 0$。无变化。\n\n支撑集发生变化的索引集合为 $\\Delta = \\{3, 4\\}$。\n该集合的基数为 $|\\Delta| = 2$。", "answer": "$$\\boxed{2}$$", "id": "3445409"}, {"introduction": "理论理解是基础，但将算法付诸实践是掌握动态稀疏估计的关键一步。本练习将指导您实现稀疏感知卡尔曼滤波更新步骤中的核心优化算法——迭代软阈值算法（ISTA）。通过编写代码来执行单次近端梯度更新，您将学会如何处理光滑项与非光滑项的复合目标函数，并理解诸如计算Lipschitz常数和选择步长等实际问题的重要性 [@problem_id:3445416]。", "problem": "考虑动态压缩感知中带有稀疏性促进先验的线性高斯状态空间模型中的单个时间步。设潜状态为向量 $x \\in \\mathbb{R}^n$，预测均值为 $\\hat{x}_{t|t-1} \\in \\mathbb{R}^n$，预测协方差为对称正定矩阵 $P_{t|t-1} \\in \\mathbb{R}^{n \\times n}$。设测量矩阵为 $H_t \\in \\mathbb{R}^{m \\times n}$，测量向量为 $y_t \\in \\mathbb{R}^m$，测量噪声协方差为对称正定矩阵 $R_t \\in \\mathbb{R}^{m \\times m}$。定义目标函数\n$$\nf(x) = g(x) + \\lambda \\lVert x \\rVert_1,\n$$\n其中\n$$\ng(x) = \\frac{1}{2} (y_t - H_t x)^\\top R_t^{-1} (y_t - H_t x) + \\frac{1}{2} (x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1} (x - \\hat{x}_{t|t-1}),\n$$\n且 $\\lambda \\ge 0$ 是一个稀疏权重。光滑部分 $g(x)$ 是二次可微的，其梯度是 Lipschitz 连续的。$g(x)$ 的 Hessian 矩阵是一个对称正定矩阵\n$$\nQ = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}.\n$$\n$\\nabla g$ 的 Lipschitz 常数等于 $Q$ 的谱范数，即 $Q$ 的最大特征值。使用该 Lipschitz 常数的倒数作为单次近端梯度步的步长。你必须从初始化 $x^{(0)} = \\hat{x}_{t|t-1}$ 开始，实现迭代收缩阈值算法 (ISTA) 或快速迭代收缩阈值算法 (FISTA) 的一次迭代。对于 FISTA，使用标准初始化 $z^{(0)} = x^{(0)}$ 和 $t^{(0)} = 1$，并执行恰好一次加速近端梯度更新。\n\n你的程序必须：\n- 计算精确的 Lipschitz 常数 $L$（即 $Q$ 的最大特征值），并将步长设置为 $s = 1/L$。\n- 从给定的初始化开始，对指定方法（ISTA 或 FISTA）执行恰好一次迭代，并计算新的迭代点 $x^{(1)}$。\n- 评估目标函数下降量 $\\Delta = f(x^{(0)}) - f(x^{(1)})$。\n\n以下测试套件指定了五个独立的案例。对于每个案例，计算目标函数下降量 $\\Delta$（作为浮点数）。\n\n测试案例 1 (一般情况, ISTA):\n- $n = 4$, $m = 3$\n- $H_t = \\begin{bmatrix}1  0  0  0 \\\\ 0  1  0  1 \\\\ 0  0  1  1\\end{bmatrix}$\n- $R_t = \\operatorname{diag}(0.5, 1.0, 2.0)$\n- $P_{t|t-1} = \\operatorname{diag}(1.0, 1.5, 0.8, 2.0)$\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix}0.5 \\\\ -0.2 \\\\ 0.1 \\\\ 0.0\\end{bmatrix}$\n- $y_t = \\begin{bmatrix}0.9 \\\\ -0.5 \\\\ 0.2\\end{bmatrix}$\n- $\\lambda = 0.1$\n- 方法: ISTA\n\n测试案例 2 (无稀疏性的边界情况, ISTA):\n- $n = 4$, $m = 3$\n- $H_t = \\begin{bmatrix}2.0  -1.0  0.0  0.0 \\\\ 0.0  1.0  -1.0  0.0 \\\\ 1.0  0.0  1.0  -1.0\\end{bmatrix}$\n- $R_t = \\operatorname{diag}(1.0, 1.0, 1.0)$\n- $P_{t|t-1} = \\operatorname{diag}(1.0, 1.0, 1.0, 1.0)$\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$\n- $y_t = \\begin{bmatrix}1.0 \\\\ -1.0 \\\\ 0.5\\end{bmatrix}$\n- $\\lambda = 0.0$\n- 方法: ISTA\n\n测试案例 3 (无测量影响，强先验, FISTA):\n- $n = 4$, $m = 3$\n- $H_t = \\begin{bmatrix}0.0  0.0  0.0  0.0 \\\\ 0.0  0.0  0.0  0.0 \\\\ 0.0  0.0  0.0  0.0\\end{bmatrix}$\n- $R_t = \\operatorname{diag}(1.0, 1.0, 1.0)$\n- $P_{t|t-1} = \\operatorname{diag}(0.2, 0.2, 0.2, 0.2)$\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix}3.0 \\\\ -2.0 \\\\ 0.0 \\\\ 0.5\\end{bmatrix}$\n- $y_t = \\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$\n- $\\lambda = 0.5$\n- 方法: FISTA\n\n测试案例 4 (强测量，弱先验, FISTA):\n- $n = 4$, $m = 3$\n- $H_t = \\begin{bmatrix}0.5  1.0  -0.5  0.2 \\\\ 1.5  -0.2  0.0  0.1 \\\\ -0.3  0.0  0.8  1.2\\end{bmatrix}$\n- $R_t = \\operatorname{diag}(0.1, 0.2, 0.3)$\n- $P_{t|t-1} = \\operatorname{diag}(5.0, 3.0, 4.0, 6.0)$\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$\n- $y_t = \\begin{bmatrix}1.0 \\\\ -0.5 \\\\ 0.75\\end{bmatrix}$\n- $\\lambda = 0.05$\n- 方法: FISTA\n\n测试案例 5 (极端稀疏权重, ISTA):\n- $n = 4$, $m = 3$\n- $H_t = \\begin{bmatrix}1.0  0.0  0.0  0.0 \\\\ 0.0  1.0  0.0  0.0 \\\\ 0.0  0.0  1.0  0.0\\end{bmatrix}$\n- $R_t = \\operatorname{diag}(1.0, 1.0, 1.0)$\n- $P_{t|t-1} = \\operatorname{diag}(1.0, 1.0, 1.0, 1.0)$\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix}0.2 \\\\ -0.1 \\\\ 0.05 \\\\ 0.0\\end{bmatrix}$\n- $y_t = \\begin{bmatrix}0.5 \\\\ -0.3 \\\\ 0.2\\end{bmatrix}$\n- $\\lambda = 10.0$\n- 方法: ISTA\n\n最终输出格式：\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，“[result1,result2,result3]”）。每个结果必须是相应测试案例的目标函数下降量 $\\Delta$，表示为浮点数。", "solution": "该问题要求计算在动态压缩感知背景下，将近端梯度法应用于一个最大后验 (MAP) 估计问题，经过一次迭代后目标函数的下降量。该问题定义明确，科学上合理，并为获得唯一解提供了所有必要的数据。\n\n需要最小化的目标函数由下式给出\n$$\nf(x) = g(x) + \\lambda \\lVert x \\rVert_1,\n$$\n其中 $g(x)$ 是光滑、可微的部分，而 $\\lambda \\lVert x \\rVert_1$ 是非光滑正则化项。潜状态为 $x \\in \\mathbb{R}^n$。\n\n光滑部分 $g(x)$ 由两个二次项组成，分别对应于测量的负对数似然和状态预测的负对数先验：\n$$\ng(x) = \\frac{1}{2} (y_t - H_t x)^\\top R_t^{-1} (y_t - H_t x) + \\frac{1}{2} (x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1} (x - \\hat{x}_{t|t-1}).\n$$\n此处，$y_t \\in \\mathbb{R}^m$ 是测量值，$H_t \\in \\mathbb{R}^{m \\times n}$ 是测量矩阵，$R_t \\in \\mathbb{R}^{m \\times m}$ 是测量噪声协方差，$\\hat{x}_{t|t-1} \\in \\mathbb{R}^n$ 是预测状态均值，而 $P_{t|t-1} \\in \\mathbb{R}^{n \\times n}$ 是预测状态协方差。参数 $\\lambda \\ge 0$ 控制促进稀疏性的 $L_1$ 范数正则化的强度。\n\n近端梯度法旨在解决此类复合优化问题。近端梯度法的一次迭代形式为：对光滑部分进行一步梯度下降，然后应用非光滑部分的近端算子。通用更新规则为：\n$$\nx^{(k+1)} = \\text{prox}_{s h}(x^{(k)} - s \\nabla g(x^{(k)})),\n$$\n其中 $k$ 是迭代索引，$s > 0$ 是步长，而 $\\text{prox}_{s h}$ 是函数 $s \\cdot h(x)$ 的近端算子。对于 $h(x) = \\lambda \\lVert x \\rVert_1$，其近端算子是逐元素的软阈值函数 $\\mathcal{S}_{\\alpha}(z)$：\n$$\n[\\mathcal{S}_{\\alpha}(z)]_i = \\text{sign}(z_i) \\max(|z_i| - \\alpha, 0).\n$$\n因此，针对我们具体问题的近端梯度更新为：\n$$\nx^{(k+1)} = \\mathcal{S}_{s\\lambda}(x^{(k)} - s \\nabla g(x^{(k)})).\n$$\n\n$g(x)$ 的梯度为：\n$$\n\\nabla g(x) = H_t^\\top R_t^{-1} (H_t x - y_t) + P_{t|t-1}^{-1} (x - \\hat{x}_{t|t-1}).\n$$\n问题指定使用步长 $s=1/L$，其中 $L$ 是 $\\nabla g(x)$ 的 Lipschitz 常数。该常数是 $g(x)$ 的 Hessian 矩阵的最大特征值（谱范数），Hessian 矩阵由下式给出：\n$$\nQ = \\nabla^2 g(x) = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}.\n$$\n问题要求从初始化 $x^{(0)} = \\hat{x}_{t|t-1}$ 开始执行单次迭代。\n\n提到的两种方法是 ISTA（迭代收缩阈值算法）和 FISTA（快速 ISTA）。\n1.  **ISTA**：更新规则正是上面描述的近端梯度更新：\n    $$x^{(1)}_{\\text{ISTA}} = \\mathcal{S}_{s\\lambda}(x^{(0)} - s \\nabla g(x^{(0)})).$$\n2.  **FISTA**：FISTA 引入了一个加速步骤。使用指定的标准初始化 $z^{(0)} = x^{(0)}$ 和 $t^{(0)}=1$，状态的第一次更新 $x^{(1)}$ 是通过从中间序列点 $z^{(0)}$ 进行近端梯度步来计算的：\n    $$x^{(1)}_{\\text{FISTA}} = \\mathcal{S}_{s\\lambda}(z^{(0)} - s \\nabla g(z^{(0)})).$$\n    由于 $z^{(0)} = x^{(0)}$，在第一次迭代中，ISTA 和 FISTA 对 $x^{(1)}$ 的计算是相同的。这两种方法仅从第二次迭代起才出现差异。因此，对于所有测试案例，找到 $x^{(1)}$ 的过程是相同的。\n\n计算每个测试案例的目标函数下降量 $\\Delta = f(x^{(0)}) - f(x^{(1)})$ 的整体步骤如下：\n\n1.  **初始化**：设置初始状态 $x^{(0)} = \\hat{x}_{t|t-1}$。\n2.  **Hessian 矩阵和 Lipschitz 常数**：构建矩阵 $R_t^{-1}$ 和 $P_{t|t-1}^{-1}$。计算 Hessian 矩阵 $Q = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$。计算 Lipschitz 常数 $L = \\lambda_{\\max}(Q)$，即对称矩阵 $Q$ 的最大特征值。\n3.  **步长**：计算步长 $s=1/L$。\n4.  **初始目标值**：计算 $f(x^{(0)})$。注意，由于 $x^{(0)} = \\hat{x}_{t|t-1}$，$g(x^{(0)})$ 中的第二项为零。\n    $$\n    f(x^{(0)}) = \\frac{1}{2} (y_t - H_t x^{(0)})^\\top R_t^{-1} (y_t - H_t x^{(0)}) + \\lambda \\lVert x^{(0)} \\rVert_1.\n    $$\n5.  **梯度计算**：初始点 $\\nabla g(x^{(0)})$ 的梯度也得到简化，因为 $(x^{(0)} - \\hat{x}_{t|t-1})$ 项为零：\n    $$\n    \\nabla g(x^{(0)}) = H_t^\\top R_t^{-1} (H_t x^{(0)} - y_t) = -H_t^\\top R_t^{-1} (y_t - H_t x^{(0)}).\n    $$\n6.  **近端梯度步**：计算更新后的状态 $x^{(1)}$：\n    $$\n    x^{(1)} = \\mathcal{S}_{s\\lambda}(x^{(0)} - s \\nabla g(x^{(0)})).\n    $$\n7.  **最终目标值**：计算新状态下的目标函数值 $f(x^{(1)})$：\n    $$\n    f(x^{(1)}) = \\frac{1}{2} (y_t - H_t x^{(1)})^\\top R_t^{-1} (y_t - H_t x^{(1)}) + \\frac{1}{2} (x^{(1)} - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1} (x^{(1)} - \\hat{x}_{t|t-1}) + \\lambda \\lVert x^{(1)} \\rVert_1.\n    $$\n8.  **计算下降量**：最终结果是目标函数下降量 $\\Delta = f(x^{(0)}) - f(x^{(1)})$。对于正确实现的、步长满足 $s \\le 1/L$ 的近端梯度步，该下降量保证为非负。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the five test cases for the dynamic compressed sensing problem.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (general case, ISTA)\n        {\n            \"H_t\": np.array([[1, 0, 0, 0], [0, 1, 0, 1], [0, 0, 1, 1]]),\n            \"R_t_diag\": np.array([0.5, 1.0, 2.0]),\n            \"P_t_minus_1_diag\": np.array([1.0, 1.5, 0.8, 2.0]),\n            \"x_hat\": np.array([0.5, -0.2, 0.1, 0.0]),\n            \"y_t\": np.array([0.9, -0.5, 0.2]),\n            \"lambda_\": 0.1,\n            \"method\": \"ISTA\"\n        },\n        # Test case 2 (boundary case with no sparsity, ISTA)\n        {\n            \"H_t\": np.array([[2.0, -1.0, 0.0, 0.0], [0.0, 1.0, -1.0, 0.0], [1.0, 0.0, 1.0, -1.0]]),\n            \"R_t_diag\": np.array([1.0, 1.0, 1.0]),\n            \"P_t_minus_1_diag\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"x_hat\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"y_t\": np.array([1.0, -1.0, 0.5]),\n            \"lambda_\": 0.0,\n            \"method\": \"ISTA\"\n        },\n        # Test case 3 (no measurement influence, strong prior, FISTA)\n        {\n            \"H_t\": np.array([[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]]),\n            \"R_t_diag\": np.array([1.0, 1.0, 1.0]),\n            \"P_t_minus_1_diag\": np.array([0.2, 0.2, 0.2, 0.2]),\n            \"x_hat\": np.array([3.0, -2.0, 0.0, 0.5]),\n            \"y_t\": np.array([0.0, 0.0, 0.0]),\n            \"lambda_\": 0.5,\n            \"method\": \"FISTA\"\n        },\n        # Test case 4 (strong measurement, weak prior, FISTA)\n        {\n            \"H_t\": np.array([[0.5, 1.0, -0.5, 0.2], [1.5, -0.2, 0.0, 0.1], [-0.3, 0.0, 0.8, 1.2]]),\n            \"R_t_diag\": np.array([0.1, 0.2, 0.3]),\n            \"P_t_minus_1_diag\": np.array([5.0, 3.0, 4.0, 6.0]),\n            \"x_hat\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"y_t\": np.array([1.0, -0.5, 0.75]),\n            \"lambda_\": 0.05,\n            \"method\": \"FISTA\"\n        },\n        # Test case 5 (extreme sparsity weight, ISTA)\n        {\n            \"H_t\": np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]]),\n            \"R_t_diag\": np.array([1.0, 1.0, 1.0]),\n            \"P_t_minus_1_diag\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"x_hat\": np.array([0.2, -0.1, 0.05, 0.0]),\n            \"y_t\": np.array([0.5, -0.3, 0.2]),\n            \"lambda_\": 10.0,\n            \"method\": \"ISTA\"\n        },\n    ]\n\n    results = []\n\n    def soft_threshold(z, alpha):\n        \"\"\"Element-wise soft-thresholding function.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - alpha, 0)\n\n    def objective_function(x, y_t, H_t, R_t_inv, x_hat, P_t_minus_1_inv, lambda_):\n        \"\"\"Computes the objective function f(x).\"\"\"\n        meas_err = y_t - H_t @ x\n        pred_err = x - x_hat\n        g_x = 0.5 * meas_err.T @ R_t_inv @ meas_err + 0.5 * pred_err.T @ P_t_minus_1_inv @ pred_err\n        l1_norm = np.linalg.norm(x, 1)\n        return g_x + lambda_ * l1_norm\n\n    for case in test_cases:\n        H_t = case[\"H_t\"]\n        y_t = case[\"y_t\"]\n        x_hat = case[\"x_hat\"]\n        lambda_ = case[\"lambda_\"]\n\n        R_t_inv = np.diag(1.0 / case[\"R_t_diag\"])\n        P_t_minus_1_inv = np.diag(1.0 / case[\"P_t_minus_1_diag\"])\n\n        # Compute the Hessian Q of the smooth part g(x)\n        Q = H_t.T @ R_t_inv @ H_t + P_t_minus_1_inv\n\n        # Lipschitz constant L is the largest eigenvalue of Q\n        # Use eigvalsh for symmetric matrices\n        eigenvalues = np.linalg.eigvalsh(Q)\n        L = np.max(eigenvalues)\n\n        # Step size s = 1/L\n        s = 1.0 / L\n\n        # Initialization\n        x_0 = x_hat\n\n        # Calculate initial objective value f(x_0)\n        f_x0 = objective_function(x_0, y_t, H_t, R_t_inv, x_hat, P_t_minus_1_inv, lambda_)\n\n        # Calculate gradient of g(x) at x_0\n        # grad_g(x_0) = H^T R^{-1} (H x_0 - y) + P^{-1} (x_0 - x_hat)\n        # Since x_0 = x_hat, the second term is zero.\n        grad_g_x0 = H_t.T @ R_t_inv @ (H_t @ x_0 - y_t)\n        \n        # Argument for the proximal operator\n        z = x_0 - s * grad_g_x0\n        \n        # Perform one proximal gradient step to get x_1\n        x_1 = soft_threshold(z, s * lambda_)\n\n        # Calculate new objective value f(x_1)\n        f_x1 = objective_function(x_1, y_t, H_t, R_t_inv, x_hat, P_t_minus_1_inv, lambda_)\n        \n        # Compute the objective decrease\n        delta = f_x0 - f_x1\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3445416"}, {"introduction": "现实世界中的许多系统本质上是非线性的，这给状态估计带来了新的挑战。本练习将引导您把前述的稀疏优化技术与扩展卡尔曼滤波（EKF）相结合，以处理非线性的测量模型。您将首先通过计算雅可比矩阵来线性化测量函数，然后在一个经过线性化的模型上执行一步近端梯度更新，从而亲身体验如何构建一个稀疏感知的EKF滤波器 [@problem_id:3445444]。", "problem": "给定一个在时间索引 $t$ 处的非线性测量函数，定义为 $h_t(x) = \\begin{bmatrix} \\sin(a^\\top x) \\\\ b^\\top x \\end{bmatrix}$，其中 $a \\in \\mathbb{R}^n$ 和 $b \\in \\mathbb{R}^n$ 是已知的， $x \\in \\mathbb{R}^n$ 是状态。考虑在动态压缩感知设置中，一个带有稀疏性感知正则化的扩展卡尔曼滤波器 (EKF) 的单时间步。从先验均值 $\\hat{x}_{t|t-1} \\in \\mathbb{R}^n$ 和先验协方差 $P_{t|t-1} \\in \\mathbb{R}^{n \\times n}$ 出发，给定协方差为 $R_t \\in \\mathbb{R}^{2 \\times 2}$ 的测量值 $y_t \\in \\mathbb{R}^2$，执行一次正则化的 EKF 更新，该更新使用权重为 $\\lambda \\ge 0$ 的 $\\ell_1$ 惩罚项来促进稀疏性。\n\n目标是计算 $h_t$ 在 $\\hat{x}_{t|t-1}$ 处的雅可比矩阵 $H_t$，并对在 $\\hat{x}_{t|t-1}$ 处线性化测量函数得到的最大后验 (MAP) 估计器执行一次近端梯度步。该近端梯度步对应于稀疏性感知 EKF 的一次数值更新。\n\n请将您的推导和算法建立在以下公认原则的基础上：\n\n- 扩展卡尔曼滤波器 (EKF) 使用一阶泰勒近似，围绕先验估计 $\\hat{x}_{t|t-1}$ 对非线性测量函数 $h_t(x)$ 进行线性化。\n- 在高斯先验 $x \\sim \\mathcal{N}(\\hat{x}_{t|t-1}, P_{t|t-1})$ 和协方差为 $R_t$ 的高斯测量噪声下，线性化后的负对数后验是二次项之和。\n- $\\ell_1$ 范数惩罚项 $\\lambda \\lVert x \\rVert_1$ 对应于拉普拉斯先验，并导出一个凸复合目标函数，该函数可以通过使用软阈值算子的近端梯度步来最小化。\n\n您的任务是：\n\n- 从第一性原理出发，推导 $h_t(x)$ 在 $x = \\hat{x}_{t|t-1}$ 处的雅可比矩阵 $H_t$。\n- 围绕 $x = \\hat{x}_{t|t-1}$ 线性化测量方程，并构成线性化残差 $r_t = y_t - h_t(\\hat{x}_{t|t-1})$。\n- 定义线性化负对数后验的光滑部分 $g(x)$：\n  $$g(x) = \\frac{1}{2}\\left(r_t - H_t (x - \\hat{x}_{t|t-1})\\right)^\\top R_t^{-1}\\left(r_t - H_t (x - \\hat{x}_{t|t-1})\\right) + \\frac{1}{2}(x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1}(x - \\hat{x}_{t|t-1}).$$\n- 从 $x^{(0)} = \\hat{x}_{t|t-1}$ 开始，计算一次近端梯度更新，步长为 $\\alpha = 1/L$，其中 $L$ 是对称正定矩阵 $H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$ 的最大特征值。更新公式为\n  $$x^{(1)} = \\operatorname{prox}_{\\alpha \\lambda \\lVert \\cdot \\rVert_1}\\left(x^{(0)} - \\alpha \\nabla g(x^{(0)})\\right),$$\n  其中 $\\operatorname{prox}_{\\tau \\lVert \\cdot \\rVert_1}(z)$ 是逐元素软阈值算子，由 $\\left[\\operatorname{prox}_{\\tau \\lVert \\cdot \\rVert_1}(z)\\right]_i = \\operatorname{sign}(z_i)\\max\\{|z_i| - \\tau, 0\\}$ 给出。\n- 为每个提供的测试用例输出 $x^{(1)}$。\n\n实现以下明确的测试套件。在所有情况下，状态维度为 $n = 3$，测量维度为 $2$。所有矩阵都是对称的，所有协方差矩阵都是严格正定的。\n\n测试用例 #1 (一般情况):\n- $a = \\begin{bmatrix} 0.7 \\\\ -1.2 \\\\ 0.5 \\end{bmatrix}$, $b = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ -0.5 \\end{bmatrix}$。\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix} 0.2 \\\\ -0.3 \\\\ 0.1 \\end{bmatrix}$。\n- $P_{t|t-1} = \\begin{bmatrix} 0.5  0.1  0.0 \\\\ 0.1  0.3  0.05 \\\\ 0.0  0.05  0.8 \\end{bmatrix}$。\n- $y_t = \\begin{bmatrix} 0.4 \\\\ -0.15 \\end{bmatrix}$。\n- $R_t = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.1 \\end{bmatrix}$。\n- $\\lambda = 0.15$。\n\n测试用例 #2 (接近零的雅可比分量且无稀疏性):\n- $a = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ -0.25 \\end{bmatrix}$, $b = \\begin{bmatrix} 0.2 \\\\ -0.3 \\\\ 0.6 \\end{bmatrix}$。\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix} 1.4 \\\\ 0.2 \\\\ 0.0 \\end{bmatrix}$。\n- $P_{t|t-1} = \\begin{bmatrix} 0.2  0.02  0.0 \\\\ 0.02  0.4  0.01 \\\\ 0.0  0.01  0.6 \\end{bmatrix}$。\n- $y_t = \\begin{bmatrix} 0.8 \\\\ -0.05 \\end{bmatrix}$。\n- $R_t = \\begin{bmatrix} 0.02  0.0 \\\\ 0.0  0.2 \\end{bmatrix}$。\n- $\\lambda = 0.0$。\n\n测试用例 #3 (无信息先验，测量主导):\n- $a = \\begin{bmatrix} 0.3 \\\\ -0.8 \\\\ 0.25 \\end{bmatrix}$, $b = \\begin{bmatrix} -0.6 \\\\ 0.4 \\\\ 0.1 \\end{bmatrix}$。\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$。\n- $P_{t|t-1} = \\begin{bmatrix} 10.0  0.0  0.0 \\\\ 0.0  10.0  0.0 \\\\ 0.0  0.0  10.0 \\end{bmatrix}$。\n- $y_t = \\begin{bmatrix} 0.05 \\\\ 0.0 \\end{bmatrix}$。\n- $R_t = \\begin{bmatrix} 0.1  0.0 \\\\ 0.0  0.1 \\end{bmatrix}$。\n- $\\lambda = 0.05$。\n\n测试用例 #4 (强稀疏性压力):\n- $a = \\begin{bmatrix} -0.5 \\\\ 1.5 \\\\ -1.0 \\end{bmatrix}$, $b = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ 0.0 \\end{bmatrix}$。\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.03 \\end{bmatrix}$。\n- $P_{t|t-1} = \\begin{bmatrix} 0.1  0.02  0.0 \\\\ 0.02  0.1  0.01 \\\\ 0.0  0.01  0.1 \\end{bmatrix}$。\n- $y_t = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$。\n- $R_t = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.05 \\end{bmatrix}$。\n- $\\lambda = 1.0$。\n\n计算要求：\n\n- 严格根据 $h_t(x)$ 的导数定义来计算在 $x = \\hat{x}_{t|t-1}$ 处的雅可比矩阵 $H_t$。\n- 使用线性化残差 $r_t = y_t - h_t(\\hat{x}_{t|t-1})$，其中 $h_t(\\hat{x}_{t|t-1})$ 按分量计算为 $h_t(\\hat{x}_{t|t-1}) = \\begin{bmatrix} \\sin(a^\\top \\hat{x}_{t|t-1}) \\\\ b^\\top \\hat{x}_{t|t-1} \\end{bmatrix}$。\n- 构建矩阵 $M = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$，并设置步长 $\\alpha = 1 / L$，其中 $L$ 是 $M$ 的最大特征值。\n- 计算梯度 $\\nabla g(\\hat{x}_{t|t-1})$，并使用软阈值算子执行一次近端梯度步。\n\n最终输出格式：\n\n- 对于每个测试用例，将更新后的状态 $x_{t|t}^{(1)}$ 以浮点数列表的形式输出，顺序与 $x$ 的分量顺序一致。\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目对应于上面列出的一个测试用例，并且每个条目本身是一个表示该用例的 $x_{t|t}^{(1)}$ 的列表（例如，$\\left[ [\\dots], [\\dots], [\\dots], [\\dots] \\right]$）。", "solution": "该问题是有效的，因为它在科学上基于状态估计和优化的既定原则，其设定是完整且一致的，并以客观、正式的语言表述。因此，我们可以进行推导和求解。\n\n问题的核心是为稀疏性感知扩展卡尔曼滤波器 (EKF) 执行单步更新。此更新被构建为求解最大后验 (MAP) 估计问题的近端梯度算法的单步。此 MAP 问题的目标函数结合了线性化的测量模型、状态的高斯先验以及用于促进稀疏性的 $\\ell_1$ 正则化项。\n\n时间 $t$ 的状态由 $x_t \\in \\mathbb{R}^n$ 表示。系统通过非线性测量函数 $h_t(x_t)$ 进行观测，测量值 $y_t \\in \\mathbb{R}^2$ 受到零均值高斯噪声的干扰，其协方差为 $R_t \\in \\mathbb{R}^{2 \\times 2}$。测量模型为：\n$$y_t = h_t(x_t) + v_t, \\quad v_t \\sim \\mathcal{N}(0, R_t)$$\n其中 $h_t(x) = \\begin{bmatrix} \\sin(a^\\top x) \\\\ b^\\top x \\end{bmatrix}$，对于已知的向量 $a, b \\in \\mathbb{R}^n$。\n\n关于状态的先验信念由一个高斯分布给出，其均值为 $\\hat{x}_{t|t-1} \\in \\mathbb{R}^n$，协方差为 $P_{t|t-1} \\in \\mathbb{R}^{n \\times n}$，即 $x_t \\sim \\mathcal{N}(\\hat{x}_{t|t-1}, P_{t|t-1})$。\n\nEKF 的第一步是将非线性测量函数 $h_t(x)$ 在先验状态估计 $\\hat{x}_{t|t-1}$ 周围进行线性化。这通过一阶泰勒展开实现：\n$$h_t(x) \\approx h_t(\\hat{x}_{t|t-1}) + H_t (x - \\hat{x}_{t|t-1})$$\n其中 $H_t$ 是 $h_t(x)$ 在 $x = \\hat{x}_{t|t-1}$ 处求值的雅可比矩阵。让我们来推导这个雅可比矩阵。函数 $h_t(x)$ 有两个分量，$h_{t,1}(x) = \\sin(a^\\top x)$ 和 $h_{t,2}(x) = b^\\top x$。雅可比矩阵是一个 $2 \\times n$ 的矩阵，其行是这些分量的梯度。\n\n第一个分量的梯度，使用链式法则，是：\n$$\\nabla_x h_{t,1}(x) = \\nabla_x (\\sin(a^\\top x)) = \\cos(a^\\top x) \\cdot \\nabla_x(a^\\top x) = \\cos(a^\\top x) \\cdot a$$\n第二个分量的梯度是：\n$$\\nabla_x h_{t,2}(x) = \\nabla_x (b^\\top x) = b$$\n因此，雅可比矩阵是：\n$$H_t(x) = \\begin{bmatrix} (\\nabla_x h_{t,1}(x))^\\top \\\\ (\\nabla_x h_{t,2}(x))^\\top \\end{bmatrix} = \\begin{bmatrix} \\cos(a^\\top x) a^\\top \\\\ b^\\top \\end{bmatrix}$$\n对于 EKF 更新，我们在先验估计 $x = \\hat{x}_{t|t-1}$ 处对其求值：\n$$H_t = \\begin{bmatrix} \\cos(a^\\top \\hat{x}_{t|t-1}) a^\\top \\\\ b^\\top \\end{bmatrix}$$\n\nMAP 估计旨在寻找使后验概率最大化的状态 $x$，这等价于最小化负对数后验。使用高斯先验、高斯测量噪声，并增加一个 $\\ell_1$ 惩罚项以鼓励稀疏性，需要最小化的目标函数是：\n$$F(x) = \\frac{1}{2} (y_t - h_t^{\\text{lin}}(x))^\\top R_t^{-1} (y_t - h_t^{\\text{lin}}(x)) + \\frac{1}{2} (x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1} (x - \\hat{x}_{t|t-1}) + \\lambda \\lVert x \\rVert_1$$\n其中 $h_t^{\\text{lin}}(x)$ 是线性化的测量模型。让我们定义线性化残差（或新息）为 $r_t = y_t - h_t(\\hat{x}_{t|t-1})$。那么 $y_t - h_t^{\\text{lin}}(x) = y_t - (h_t(\\hat{x}_{t|t-1}) + H_t(x - \\hat{x}_{t|t-1})) = r_t - H_t(x - \\hat{x}_{t|t-1})$。目标函数可以写成 $F(x) = g(x) + \\lambda \\lVert x \\rVert_1$，其中 $g(x)$ 是光滑部分：\n$$g(x) = \\frac{1}{2}(r_t - H_t (x - \\hat{x}_{t|t-1}))^\\top R_t^{-1}(r_t - H_t (x - \\hat{x}_{t|t-1})) + \\frac{1}{2}(x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1}(x - \\hat{x}_{t|t-1})$$\n该目标是一个复合函数，由一个光滑、可微的部分 $g(x)$ 和一个非光滑、凸的部分 $\\lambda \\lVert x \\rVert_1$ 组成。这种结构非常适合使用近端梯度法。该方法的迭代更新规则是：\n$$x^{(k+1)} = \\operatorname{prox}_{\\alpha \\lambda \\lVert \\cdot \\rVert_1}(x^{(k)} - \\alpha \\nabla g(x^{(k)}))$$\n其中 $\\alpha > 0$ 是步长，$\\operatorname{prox}$ 是近端算子。\n\n问题要求从 $x^{(0)} = \\hat{x}_{t|t-1}$ 开始进行单步更新。我们首先需要 $g(x)$ 的梯度：\n$$\\nabla g(x) = \\nabla_x \\left( \\frac{1}{2}(r_t - H_t (x - \\hat{x}_{t|t-1}))^\\top R_t^{-1}(r_t - H_t (x - \\hat{x}_{t|t-1})) \\right) + \\nabla_x \\left( \\frac{1}{2}(x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1}(x - \\hat{x}_{t|t-1}) \\right)$$\n$$\\nabla g(x) = -H_t^\\top R_t^{-1}(r_t - H_t(x - \\hat{x}_{t|t-1})) + P_{t|t-1}^{-1}(x - \\hat{x}_{t|t-1})$$\n在起始点 $x^{(0)} = \\hat{x}_{t|t-1}$ 处计算梯度：\n$$\\nabla g(\\hat{x}_{t|t-1}) = -H_t^\\top R_t^{-1}(r_t - H_t(\\hat{x}_{t|t-1} - \\hat{x}_{t|t-1})) + P_{t|t-1}^{-1}(\\hat{x}_{t|t-1} - \\hat{x}_{t|t-1}) = -H_t^\\top R_t^{-1} r_t$$\n近端算子的参数则为：\n$$z = x^{(0)} - \\alpha \\nabla g(x^{(0)}) = \\hat{x}_{t|t-1} - \\alpha(-H_t^\\top R_t^{-1} r_t) = \\hat{x}_{t|t-1} + \\alpha H_t^\\top R_t^{-1} r_t$$\n步长 $\\alpha$ 被选择为梯度 $\\nabla g(x)$ 的利普希茨常数 $L$ 的倒数。利普希茨常数由 $g(x)$ 的海森矩阵的最大特征值给出：\n$$\\nabla^2 g(x) = \\nabla_x(\\nabla g(x)) = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$$\n令 $M = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$。由于 $P_{t|t-1}$ 和 $R_t$ 是对称正定的， $M$ 也是对称正定的。利普希茨常数是 $L = \\lambda_{\\max}(M)$，步长是 $\\alpha = 1/L$。\n\n最后一部分是 $\\ell_1$ 范数的近端算子，即逐元素的软阈值算子。对于一个阈值 $\\tau \\ge 0$，它定义为：\n$$[\\operatorname{prox}_{\\tau \\lVert \\cdot \\rVert_1}(z)]_i = \\operatorname{sign}(z_i)\\max\\{|z_i| - \\tau, 0\\}$$\n在我们的情况下，阈值是 $\\tau = \\alpha \\lambda$。所以单步更新是：\n$$x^{(1)} = \\operatorname{prox}_{\\alpha \\lambda \\lVert \\cdot \\rVert_1}(z)$$\n\n总结一下，对于每个测试用例，我们执行以下计算：\n1.  计算标量积 $s_a = a^\\top \\hat{x}_{t|t-1}$ 和 $s_b = b^\\top \\hat{x}_{t|t-1}$。\n2.  计算雅可比矩阵 $H_t = \\begin{bmatrix} \\cos(s_a) a^\\top \\\\ b^\\top \\end{bmatrix}$。\n3.  计算预测测量值 $h_t(\\hat{x}_{t|t-1}) = [\\sin(s_a), s_b]^\\top$。\n4.  计算线性化残差 $r_t = y_t - h_t(\\hat{x}_{t|t-1})$。\n5.  计算矩阵的逆 $R_t^{-1}$ 和 $P_{t|t-1}^{-1}$。\n6.  构建矩阵 $M = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$。\n7.  找到 $M$ 的最大特征值 $L$，即 $L = \\lambda_{\\max}(M)$。\n8.  设置步长 $\\alpha = 1/L$。\n9.  计算中间向量 $z = \\hat{x}_{t|t-1} + \\alpha H_t^\\top R_t^{-1} r_t$。\n10. 设置阈值 $\\tau = \\alpha \\lambda$。\n11. 通过对 $z$ 的每个分量应用阈值为 $\\tau$ 的软阈值算子来计算更新后的状态 $x^{(1)}$。\n这个过程为稀疏性感知 EKF 的一步提供了所需的状态估计。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a single proximal-gradient update for a sparsity-aware Extended Kalman Filter\n    for four given test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"a\": np.array([0.7, -1.2, 0.5]),\n            \"b\": np.array([1.0, 0.0, -0.5]),\n            \"x_prior\": np.array([0.2, -0.3, 0.1]),\n            \"P_prior\": np.array([[0.5, 0.1, 0.0], [0.1, 0.3, 0.05], [0.0, 0.05, 0.8]]),\n            \"y\": np.array([0.4, -0.15]),\n            \"R\": np.array([[0.05, 0.0], [0.0, 0.1]]),\n            \"lambda_\": 0.15\n        },\n        {\n            \"a\": np.array([1.0, 0.5, -0.25]),\n            \"b\": np.array([0.2, -0.3, 0.6]),\n            \"x_prior\": np.array([1.4, 0.2, 0.0]),\n            \"P_prior\": np.array([[0.2, 0.02, 0.0], [0.02, 0.4, 0.01], [0.0, 0.01, 0.6]]),\n            \"y\": np.array([0.8, -0.05]),\n            \"R\": np.array([[0.02, 0.0], [0.0, 0.2]]),\n            \"lambda_\": 0.0\n        },\n        {\n            \"a\": np.array([0.3, -0.8, 0.25]),\n            \"b\": np.array([-0.6, 0.4, 0.1]),\n            \"x_prior\": np.array([0.0, 0.0, 0.0]),\n            \"P_prior\": np.array([[10.0, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 10.0]]),\n            \"y\": np.array([0.05, 0.0]),\n            \"R\": np.array([[0.1, 0.0], [0.0, 0.1]]),\n            \"lambda_\": 0.05\n        },\n        {\n            \"a\": np.array([-0.5, 1.5, -1.0]),\n            \"b\": np.array([0.0, 1.0, 0.0]),\n            \"x_prior\": np.array([0.05, -0.02, 0.03]),\n            \"P_prior\": np.array([[0.1, 0.02, 0.0], [0.02, 0.1, 0.01], [0.0, 0.01, 0.1]]),\n            \"y\": np.array([0.0, 0.0]),\n            \"R\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n            \"lambda_\": 1.0\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        a = case[\"a\"]\n        b = case[\"b\"]\n        x_prior = case[\"x_prior\"]\n        P_prior = case[\"P_prior\"]\n        y = case[\"y\"]\n        R = case[\"R\"]\n        lambda_ = case[\"lambda_\"]\n\n        # 1. Compute scalar products\n        a_T_x_prior = a.T @ x_prior\n        b_T_x_prior = b.T @ x_prior\n\n        # 2. Compute Jacobian H_t\n        H_t = np.vstack([np.cos(a_T_x_prior) * a.T, b.T])\n\n        # 3. Compute predicted measurement and linearized residual r_t\n        h_x_prior = np.array([np.sin(a_T_x_prior), b_T_x_prior])\n        r_t = y - h_x_prior\n\n        # 4. Compute matrix inverses\n        R_inv = np.linalg.inv(R)\n        P_prior_inv = np.linalg.inv(P_prior)\n\n        # 5. Form the Hessian matrix M of the smooth part of the objective\n        M = H_t.T @ R_inv @ H_t + P_prior_inv\n\n        # 6. Find the largest eigenvalue L of M\n        # M is symmetric positive definite, use eigvalsh for numerical stability and efficiency.\n        eigenvalues = np.linalg.eigvalsh(M)\n        L = np.max(eigenvalues)\n\n        # 7. Set step size alpha\n        alpha = 1.0 / L\n\n        # 8. Compute the argument for the proximal operator\n        # z = x^(0) - alpha * grad(g(x^(0)))\n        grad_g_x0 = -H_t.T @ R_inv @ r_t\n        z = x_prior - alpha * grad_g_x0\n\n        # 9. Set threshold tau for soft-thresholding\n        tau = alpha * lambda_\n\n        # 10. Compute updated state x^(1) via soft-thresholding\n        x_updated = np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n        \n        results.append(x_updated.tolist())\n\n    # Format the final output string to match the required format: [[...],[...],...]\n    final_output_str = str(results).replace(\" \", \"\")\n    print(final_output_str)\n\nsolve()\n```", "id": "3445444"}]}