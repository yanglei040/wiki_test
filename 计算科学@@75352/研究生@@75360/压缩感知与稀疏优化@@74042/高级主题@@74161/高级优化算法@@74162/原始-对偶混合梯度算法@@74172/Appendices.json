{"hands_on_practices": [{"introduction": "原始-对偶混合梯度（PDHG）算法的核心在于原始变量和对偶变量的近端步骤交替进行。第一个练习将聚焦于一个关键的计算元操作：计算共轭函数 $g^*$ 的近端算子。我们将运用基础的Moreau恒等式，来展示这个在对偶空间中的操作如何可以方便地转换到原始空间中执行，这一技巧在使用诸如 $\\ell_1$ 范数等常见正则化项时，对于高效实现算法至关重要。[@problem_id:3467285]", "problem": "考虑压缩感知和稀疏优化中的复合凸优化模型，其形式为 $\\min_{x \\in \\mathbb{R}^{n}} f(x) + g(Kx)$，其中 $f$ 和 $g$ 是正常、闭、凸函数，而 $K$ 是一个线性算子。原始-对偶混合梯度(PDHG)算法（也称为Chambolle-Pock (CP)算法）迭代进行原始-对偶更新，这需要在对偶变量上计算凸共轭函数 $g^{*}$ 的近端算子。请从近端算子和凸共轭的定义出发，仅使用凸分析中已确立的事实，为 $g(z) = \\lambda \\|z\\|_{1}$ 的情况推导出 $\\operatorname{prox}_{\\sigma g^{*}}(y)$ 的闭式表达式，该表达式应通过在缩放后的参数上求值的原始函数 $g$ 的近端算子来表示。然后，对于具体参数 $\\lambda = 1$，$\\sigma = 2$ 和对偶向量 $y = (3,\\,-0.6,\\,1.0,\\,-2.2) \\in \\mathbb{R}^{4}$，计算 $\\operatorname{prox}_{\\sigma g^{*}}(y)$ 的值。最后，在PDHG对偶变量更新的背景下，解释当 $g(z)=\\lambda\\|z\\|_{1}$ 时，为什么通过原始近端算子来表示对偶近端算子在计算上是有利的。请将您最终计算出的近端值以单行向量的形式给出。无需四舍五入，不涉及单位。", "solution": "我们从凸分析中的基本定义开始。对于一个正常、闭、凸函数 $h \\colon \\mathbb{R}^{m} \\to \\mathbb{R} \\cup \\{+\\infty\\}$ 和一个参数 $\\tau  0$，其邻近算子定义为：\n$$\n\\operatorname{prox}_{\\tau h}(v) \\triangleq \\arg\\min_{u \\in \\mathbb{R}^{m}} \\left\\{ h(u) + \\frac{1}{2\\tau} \\|u - v\\|_{2}^{2} \\right\\}。\n$$\n凸共轭 $h^{*}$ 定义为：\n$$\nh^{*}(y) \\triangleq \\sup_{z \\in \\mathbb{R}^{m}} \\big\\{ \\langle y, z \\rangle - h(z) \\big\\}。\n$$\n一个函数与其凸共轭的邻近算子之间的一个基本关系，即著名的 Moreau 分解，可以从这些定义和 Fenchel-Young 不等式中推导出来。具体来说，可以证明对于任意 $y \\in \\mathbb{R}^{m}$ 和 $\\tau  0$，有：\n$$\n\\operatorname{prox}_{\\tau h^{*}}(y) + \\tau \\operatorname{prox}_{h/\\tau}\\!\\left(\\frac{y}{\\tau}\\right) = y,\n$$\n这等价于：\n$$\n\\operatorname{prox}_{\\tau h^{*}}(y) = y - \\tau \\operatorname{prox}_{h/\\tau}\\!\\left(\\frac{y}{\\tau}\\right)。\n$$\n我们将使用这个恒等式，令 $h = g$，其中 $g(z) = \\lambda \\|z\\|_{1}$。为了计算 $\\operatorname{prox}_{g/\\sigma}$，我们观察到 $g/\\sigma (z) = \\frac{\\lambda}{\\sigma} \\|z\\|_{1}$ 在坐标上是可分的，因此对于任意 $u \\in \\mathbb{R}^{m}$，最小化问题\n$$\n\\operatorname{prox}_{g/\\sigma}(u) = \\arg\\min_{z \\in \\mathbb{R}^{m}} \\left\\{ \\frac{\\lambda}{\\sigma} \\|z\\|_{1} + \\frac{1}{2}\\|z - u\\|_{2}^{2} \\right\\}\n$$\n可以分解为 $m$ 个标量问题。对于标量变量 $z \\in \\mathbb{R}$ 和标量 $u \\in \\mathbb{R}$，最优性条件可以使用绝对值的次微分来写出：$0 \\in \\frac{\\lambda}{\\sigma} \\partial |z| + (z - u)$。求解此式可得软阈值（也称为收缩）算子：\n$$\nS_{\\theta}(u) \\triangleq \\operatorname{sign}(u)\\,\\max\\{|u| - \\theta,\\,0\\},\n$$\n其中阈值 $\\theta = \\frac{\\lambda}{\\sigma}$，逐分量应用。因此，\n$$\n\\operatorname{prox}_{g/\\sigma}(u) = S_{\\lambda/\\sigma}(u) \\quad \\text{(逐分量)}。\n$$\n将其与为 $h=g$ 推导出的 Moreau 分解相结合，得到：\n$$\n\\operatorname{prox}_{\\sigma g^{*}}(y) = y - \\sigma\\,\\operatorname{prox}_{g/\\sigma}\\!\\left(\\frac{y}{\\sigma}\\right) = y - \\sigma\\,S_{\\lambda/\\sigma}\\!\\left(\\frac{y}{\\sigma}\\right),\n$$\n逐分量应用。作为交叉检验，注意到对于 $g(z)=\\lambda\\|z\\|_{1}$，其凸共轭是 $g^{*}(y) = \\iota_{\\{\\,\\|y\\|_{\\infty} \\le \\lambda\\,\\}}(y)$，即半径为 $\\lambda$ 的闭 $\\ell_{\\infty}$ 球的指示函数。因此 $\\operatorname{prox}_{\\sigma g^{*}}(y)$ 是 $y$ 在该 $\\ell_{\\infty}$ 球上的欧几里得投影，对于可分坐标，这只是简单的逐坐标裁剪到 $[-\\lambda, \\lambda]$。这与上述表达式一致，因为通过 Moreau 分解，软阈值和裁剪是互补的。\n\n我们现在计算所要求的近端值，参数为 $\\lambda = 1$，$\\sigma = 2$ 和 $y = (3,\\,-0.6,\\,1.0,\\,-2.2)$。阈值为 $\\theta = \\lambda/\\sigma = 1/2 = 0.5$，我们需要逐分量计算 $S_{0.5}(y/\\sigma) = S_{0.5}(y/2)$：\n- 第一个坐标：$\\frac{3}{2} = 1.5$；$S_{0.5}(1.5) = \\operatorname{sign}(1.5)\\,\\max\\{1.5 - 0.5, 0\\} = 1.0$。\n- 第二个坐标：$\\frac{-0.6}{2} = -0.3$；$|{-0.3}| = 0.3  0.5$，所以 $S_{0.5}(-0.3) = 0$。\n- 第三个坐标：$\\frac{1.0}{2} = 0.5$；$|0.5| - 0.5 = 0$，所以 $S_{0.5}(0.5) = 0$。\n- 第四个坐标：$\\frac{-2.2}{2} = -1.1$；$S_{0.5}(-1.1) = \\operatorname{sign}(-1.1)\\,(1.1 - 0.5) = -0.6$。\n\n现在应用 $\\operatorname{prox}_{\\sigma g^{*}}(y) = y - \\sigma\\,S_{0.5}(y/2)$：\n- 第一个坐标：$3 - 2 \\cdot 1.0 = 1$。\n- 第二个坐标：$-0.6 - 2 \\cdot 0 = -0.6$。\n- 第三个坐标：$1.0 - 2 \\cdot 0 = 1.0$。\n- 第四个坐标：$-2.2 - 2 \\cdot (-0.6) = -2.2 + 1.2 = -1.0$。\n\n因此，\n$$\n\\operatorname{prox}_{\\sigma g^{*}}(y) = (1,\\,-0.6,\\,1.0,\\,-1.0).\n$$\n\n在PDHG (Chambolle-Pock) 算法中的计算优势：对偶更新通常需要计算 $\\operatorname{prox}_{\\sigma g^{*}}$，对于一般的 $g$，这可能与投影到由 $g^{*}$ 隐含的复杂凸集上一样困难。对于 $g(z) = \\lambda\\|z\\|_{1}$，$g^{*}$ 是 $\\ell_{\\infty}$ 球的指示函数，所以 $\\operatorname{prox}_{\\sigma g^{*}}$ 是一个涉及逐坐标裁剪的投影。使用 Moreau 分解将对偶近端算子的计算简化为在缩放后的参数上计算原始近端算子 $\\operatorname{prox}_{g/\\sigma}$，对于1-范数，这就是软阈值。软阈值是一种逐元素的闭式运算，计算成本为 $O(m)$，且坐标之间没有耦合，从而提供了一种简单、快速且数值稳定的更新方法。在实践中，这也允许在不同的算法和参数之间复用 $g$ 的原始近端算子代码，避免了在对偶空间中对 $g^{*}$ 及其几何约束的显式操作。", "answer": "$$\\boxed{\\begin{pmatrix}1  -0.6  1  -1\\end{pmatrix}}$$", "id": "3467285"}, {"introduction": "在掌握了一个关键部件之后，我们现在退后一步，从整体上理解PDHG算法的行为。本练习将探讨一个简化的场景，其中目标函数的两个部分都是光滑的，从而揭示PDHG与我们所熟悉的梯度下降法之间的深刻联系。通过分析算法在特定渐近极限下的迭代过程，你将发现PDHG如何能被解释为一种预处理梯度方法，这为理解其收敛行为提供了宝贵的直觉。[@problem_id:3467351]", "problem": "考虑形如 $\\min_{x \\in \\mathbb{R}} f(x) + g(Kx)$ 的复合最小化问题，其中 $K$ 是一个线性算子，$f$ 和 $g$ 是具有 Lipschitz 连续梯度的凸函数。在 $K = I$ 且 $f$ 和 $g$ 均为二次连续可微的特殊情况下，分析使用精确邻近映射实现的原始-对偶混合梯度（Primal-Dual Hybrid Gradient, PDHG）算法，该算法也称为 Chambolle–Pock 算法。\n\n使用标量二次模型 $f(x) = \\frac{a}{2} x^{2}$ 和 $g(z) = \\frac{b}{2} z^{2}$，其中 $a  0$ 且 $b  0$，并设 $K = I$。假设在 PDHG 迭代中没有外推（即设 $\\theta = 0$）。对于一般的凸函数 $f$ 和 $g$，PDHG 更新由原始变量和对偶变量上的基本邻近梯度结构定义：对于步长 $\\tau  0$ 和 $\\sigma  0$，\n$$\ny^{k+1} = \\operatorname{prox}_{\\sigma g^{*}}\\!\\left(y^{k} + \\sigma K x^{k}\\right), \\qquad x^{k+1} = \\operatorname{prox}_{\\tau f}\\!\\left(x^{k} - \\tau K^{\\top} y^{k+1}\\right),\n$$\n其中 $g^{*}$ 表示 $g$ 的凸共轭，且 $\\operatorname{prox}_{\\gamma h}(u) = \\arg\\min_{v} \\left\\{ h(v) + \\frac{1}{2\\gamma} \\|v - u\\|^{2} \\right\\}$。\n\n从这些定义以及凸共轭和邻近算子的基本性质出发，推导在此标量二次设定下的精确 PDHG 迭代，并明确展示原始更新 $x^{k+1}$ 如何线性地依赖于 $x^{k}$ 和 $y^{k}$。然后，考虑 $\\sigma \\to \\infty$ 的渐近状态，并证明 PDHG 方案表现得像作用于和目标函数 $h(x) = f(x) + g(x)$ 的“对称”梯度法，即 $x$ 的更新变为一个单步线性收缩，等同于使用某个有效步长对 $h$ 进行的经典梯度下降。通过将此状态下 PDHG 的收缩因子与经典梯度下降在 $h$ 上的收缩因子进行匹配，确定有效梯度下降步长 $\\eta$ 关于 $a$ 和 $\\tau$ 的闭式解析表达式。\n\n你的最终答案必须是 $\\eta$ 的这个单一解析表达式。不需要四舍五入，也没有单位。", "solution": "该问题要求对一个特定的标量二次最小化问题分析原始-对偶混合梯度（PDHG）算法。目标是推导显式迭代，检验当对偶步长参数 $\\sigma$ 趋于无穷大时的渐近行为，并将此极限与经典梯度下降法相关联，以确定有效步长 $\\eta$。\n\n该最小化问题为 $\\min_{x \\in \\mathbb{R}} f(x) + g(Kx)$，具体的函数和算子如下：\n- $f(x) = \\frac{a}{2} x^{2}$，其中 $a  0$。\n- $g(z) = \\frac{b}{2} z^{2}$，其中 $b  0$。\n- $K = I$，单位算子，因此 $K^{\\top} = I$。\n\nPDHG 算法的更新由下式给出：\n$$y^{k+1} = \\operatorname{prox}_{\\sigma g^{*}}\\!\\left(y^{k} + \\sigma K x^{k}\\right)$$\n$$x^{k+1} = \\operatorname{prox}_{\\tau f}\\!\\left(x^{k} - \\tau K^{\\top} y^{k+1}\\right)$$\n\n我们的推导分为几个步骤：\n1.  计算 $g$ 的凸共轭 $g^{*}$。\n2.  计算 $f$ 和 $g^{*}$ 的邻近算子。\n3.  推导给定模型的显式 PDHG 迭代。\n4.  在极限 $\\sigma \\to \\infty$ 下分析迭代。\n5.  将极限迭代与作用于和目标函数 $h(x) = f(x) + g(x)$ 的标准梯度下降更新进行比较。\n\n首先，我们求 $g(z) = \\frac{b}{2} z^{2}$ 的凸共轭 $g^{*}(y)$。根据定义：\n$$g^{*}(y) = \\sup_{z \\in \\mathbb{R}} \\left( yz - g(z) \\right) = \\sup_{z \\in \\mathbb{R}} \\left( yz - \\frac{b}{2}z^{2} \\right)$$\n上确界内的表达式是关于 $z$ 的二次函数。由于 $b  0$，它是一个开口向下的抛物线，其最大值可以通过将其关于 $z$ 的导数设为零来找到：\n$$\\frac{d}{dz}\\left( yz - \\frac{b}{2}z^{2} \\right) = y - bz = 0 \\implies z = \\frac{y}{b}$$\n将这个 $z$ 的值代回表达式，得到上确界的值：\n$$g^{*}(y) = y\\left(\\frac{y}{b}\\right) - \\frac{b}{2}\\left(\\frac{y}{b}\\right)^{2} = \\frac{y^{2}}{b} - \\frac{y^{2}}{2b} = \\frac{1}{2b}y^{2}$$\n\n接下来，我们推导所需的邻近算子。函数 $h$ 的邻近算子定义为 $\\operatorname{prox}_{\\gamma h}(u) = \\arg\\min_{v} \\left\\{ h(v) + \\frac{1}{2\\gamma} \\|v - u\\|^{2} \\right\\}$。对于标量变量，这成为 $\\frac{1}{2\\gamma}(v-u)^{2}$。\n\n对于 $f(x) = \\frac{a}{2}x^{2}$：\n$$\\operatorname{prox}_{\\tau f}(u) = \\arg\\min_{v \\in \\mathbb{R}} \\left\\{ \\frac{a}{2}v^{2} + \\frac{1}{2\\tau}(v-u)^{2} \\right\\}$$\n一阶最优性条件通过对 $v$ 求导并将结果设为零来找到：\n$$av + \\frac{1}{\\tau}(v-u) = 0 \\implies v(a\\tau + 1) = u \\implies v = \\frac{u}{1+a\\tau}$$\n所以，$\\operatorname{prox}_{\\tau f}(u) = \\frac{u}{1+a\\tau}$。\n\n对于 $g^{*}(y) = \\frac{1}{2b}y^{2}$，其函数形式与 $f(x)$ 相同，只是 $a$ 被替换为 $\\frac{1}{b}$，步长参数是 $\\sigma$ 而不是 $\\tau$。通过类比，我们立即得到：\n$$\\operatorname{prox}_{\\sigma g^{*}}(u) = \\frac{u}{1 + (\\frac{1}{b})\\sigma} = \\frac{u}{1 + \\frac{\\sigma}{b}}$$\n\n现在，我们将这些显式形式代入 PDHG 迭代方程，并使用 $K=I$：\n$$y^{k+1} = \\operatorname{prox}_{\\sigma g^{*}}\\!\\left(y^{k} + \\sigma x^{k}\\right) = \\frac{y^{k} + \\sigma x^{k}}{1 + \\frac{\\sigma}{b}}$$\n$$x^{k+1} = \\operatorname{prox}_{\\tau f}\\!\\left(x^{k} - \\tau y^{k+1}\\right) = \\frac{x^{k} - \\tau y^{k+1}}{1+a\\tau}$$\n为了用 $x^k$ 表示 $x^{k+1}$，我们将 $y^{k+1}$ 的表达式代入 $x^{k+1}$ 的方程中：\n$$x^{k+1} = \\frac{1}{1+a\\tau} \\left( x^{k} - \\tau \\left( \\frac{y^{k} + \\sigma x^{k}}{1 + \\frac{\\sigma}{b}} \\right) \\right)$$\n问题要求我们在渐近极限中忽略对 $y^k$ 的依赖。我们关心的是 $x^{k+1}$ 如何依赖于 $x^k$。让我们首先在极限 $\\sigma \\to \\infty$ 下考察整个系统。$y^{k+1}$ 的更新变为：\n$$\\lim_{\\sigma \\to \\infty} y^{k+1} = \\lim_{\\sigma \\to \\infty} \\frac{y^{k} + \\sigma x^{k}}{1 + \\frac{\\sigma}{b}} = \\lim_{\\sigma \\to \\infty} \\frac{\\frac{y^{k}}{\\sigma} + x^{k}}{\\frac{1}{\\sigma} + \\frac{1}{b}} = \\frac{0 + x^{k}}{0 + \\frac{1}{b}} = b x^{k}$$\n在此极限下，对偶变量 $y^{k+1}$ 变得与原始变量 $x^k$ 耦合。将此极限关系代入 $x$ 的更新方程中得到：\n$$x^{k+1} = \\frac{x^{k} - \\tau (b x^{k})}{1+a\\tau} = \\frac{x^k(1-\\tau b)}{1+a\\tau} = \\left(\\frac{1-\\tau b}{1+a\\tau}\\right) x^{k}$$\n这表明在极限 $\\sigma \\to \\infty$ 下，PDHG 的 $x$ 更新简化为对 $x$ 的单步线性收缩。收缩因子为 $\\rho_{\\text{PDHG}} = \\frac{1-\\tau b}{1+a\\tau}$。\n\n我们现在将其与作用于和目标函数 $h(x) = f(x) + g(x)$ 的经典梯度下降法进行比较：\n$$h(x) = \\frac{a}{2}x^{2} + \\frac{b}{2}x^{2} = \\frac{a+b}{2}x^{2}$$\n$h(x)$ 的梯度是 $\\nabla h(x) = (a+b)x$。\n使用有效步长 $\\eta$ 的经典梯度下降更新是：\n$$x^{k+1} = x^{k} - \\eta \\nabla h(x^{k}) = x^{k} - \\eta(a+b)x^{k} = \\left(1 - \\eta(a+b)\\right)x^{k}$$\n梯度下降的收缩因子是 $\\rho_{\\text{GD}} = 1 - \\eta(a+b)$。\n\n为了找到有效步长 $\\eta$，我们令两个收缩因子相等：\n$$\\rho_{\\text{PDHG}} = \\rho_{\\text{GD}} \\implies \\frac{1-\\tau b}{1+a\\tau} = 1 - \\eta(a+b)$$\n解出 $\\eta$：\n$$\\eta(a+b) = 1 - \\frac{1-\\tau b}{1+a\\tau} = \\frac{(1+a\\tau) - (1-\\tau b)}{1+a\\tau} = \\frac{1+a\\tau - 1+\\tau b}{1+a\\tau} = \\frac{a\\tau + b\\tau}{1+a\\tau} = \\frac{\\tau(a+b)}{1+a\\tau}$$\n因为 $a  0$ 且 $b  0$，我们有 $a+b  0$，所以我们可以除以 $(a+b)$：\n$$\\eta = \\frac{\\tau(a+b)}{(a+b)(1+a\\tau)} = \\frac{\\tau}{1+a\\tau}$$\n这就是有效梯度下降步长 $\\eta$ 的闭式解析表达式。", "answer": "$$\\boxed{\\frac{\\tau}{1+\\tau a}}$$", "id": "3467351"}, {"introduction": "理论上的算法在有限精度的计算机上实现时必须具有鲁棒性。最后一个练习将解决一个实际挑战：如何处理近端算子计算中可能出现的、可能导致数值不稳定的极小非零值。你将分析一种常见的保护策略——将小数值阈值化为零——并推导该策略引入的计算误差的上界，从而在抽象的算法设计与实际实现之间架起一座桥梁。[@problem_id:3467272]", "problem": "考虑在原始-对偶分裂框架下的压缩感知目标，使用原始-对偶混合梯度（PDHG，也称为 Chambolle-Pock 算法）：最小化复合凸目标 $f(Kx)+g(x)$，其中 $K \\in \\mathbb{R}^{m \\times n}$，$f:\\mathbb{R}^{m} \\to \\mathbb{R}$ 是凸且可微的，而 $g:\\mathbb{R}^{n} \\to \\mathbb{R}$ 是真、凸且下半连续的。具体到最小二乘数据保真项 $f(z) = \\frac{1}{2}\\|z - b\\|_{2}^{2}$（其中 $b \\in \\mathbb{R}^{m}$）和稀疏性促进正则化项 $g(x) = \\lambda \\|x\\|_{1}$（其中 $\\lambda  0$）。设 PDHG 的步长为 $\\tau  0$ 和 $\\sigma  0$，满足稳定性条件 $\\tau \\sigma \\|K\\|^{2}  1$，其中 $\\|K\\|$ 表示由欧几里得范数诱导的算子范数。\n\n在 PDHG 的原始更新中，对给定输入 $v \\in \\mathbb{R}^{n}$，计算由参数 $\\tau$ 缩放的 $g$ 的邻近算子，其精确解为 $x^{\\star} = \\operatorname{prox}_{\\tau g}(v)$，该解是强凸子问题\n$$\nQ(u;v) \\equiv g(u) + \\frac{1}{2\\tau}\\|u - v\\|_{2}^{2}.\n$$\n的最小化子。\n在有限精度算术中，$x^{\\star}$ 中量级非常小的分量容易受到舍入误差和非规格化（denormalization）的影响，这可能会降低数值稳定性。为缓解此问题，考虑一种在邻近计算的输出上施加一个阈值下限 $\\delta  0$ 的保障措施：通过应用逐元素规则定义有保障的近似解 $\\widetilde{x}$\n$$\n\\widetilde{x}_{i} =\n\\begin{cases}\nx^{\\star}_{i},  \\text{若 } |x^{\\star}_{i}| \\ge \\delta, \\\\\n0,  \\text{若 } |x^{\\star}_{i}|  \\delta,\n\\end{cases}\n\\quad \\text{对所有 } i \\in \\{1,\\dots,n\\}.\n$$\n这种保障措施确保了非常小的坐标精确为零，从而防止了微小量级在迭代过程中的传播。\n\n从邻近算子的定义和 $g$ 的可分性出发，推导用 $\\widetilde{x}$ 替换 $x^{\\star}$ 所导致的子问题目标 $Q(\\cdot;v)$ 增加量的最坏情况上界。在每次迭代中最多有 $r \\in \\{0,1,\\dots,n\\}$ 个坐标受阈值下限影响（即，因为 $|x^{\\star}_{i}|  \\delta$ 而从一个非零的 $x^{\\star}_{i}$ 被设置为 $0$）的假设下，得出经过 $T \\in \\mathbb{N}$ 次 PDHG 迭代后累积的诱导次优性的界。你的最终答案必须是关于 $\\delta$、$\\tau$、$r$ 和 $T$ 的单个闭式解析表达式。", "solution": "该问题要求推导在原始-对偶混合梯度（PDHG）算法的原始更新步骤中，应用硬阈值保障措施所产生的累积次优性的最坏情况上界。目标函数的形式为 $f(Kx)+g(x)$，具体选择为 $f(z) = \\frac{1}{2}\\|z - b\\|_{2}^{2}$ 和 $g(x) = \\lambda \\|x\\|_{1}$（其中 $\\lambda  0$）。\n\n原始更新涉及计算 $\\tau g$ 的邻近算子，即子问题的精确解 $x^{\\star}$：\n$$\n\\min_{u \\in \\mathbb{R}^{n}} Q(u;v) \\equiv g(u) + \\frac{1}{2\\tau}\\|u - v\\|_{2}^{2}\n$$\n对于一个给定的向量 $v \\in \\mathbb{R}^{n}$。有保障的解 $\\widetilde{x}$ 是通过将 $x^{\\star}$ 中任何量级小于阈值 $\\delta  0$ 的分量设置为零来获得的。我们想要找到子问题目标增加量 $Q(\\widetilde{x}; v) - Q(x^{\\star}; v)$ 的一个上界，然后将此界在 $T$ 次迭代中累积起来。\n\n首先，我们分析单次迭代的次优性。目标函数 $Q(u;v)$ 关于 $u$ 的分量是可分的。给定 $g(x) = \\lambda \\|x\\|_{1} = \\lambda \\sum_{i=1}^{n} |x_i|$，我们可以将 $Q(u;v)$ 写成其分量的和：\n$$\nQ(u;v) = \\sum_{i=1}^{n} \\left( \\lambda |u_i| + \\frac{1}{2\\tau}(u_i - v_i)^{2} \\right) = \\sum_{i=1}^{n} Q_i(u_i; v_i)\n$$\n精确最小化子 $x^{\\star}$ 是通过独立地最小化每个 $Q_i(u_i; v_i)$ 来找到的。$Q_i(u_i; v_i)$ 的最小化子由软阈值算子给出：\n$$\nx^{\\star}_{i} = \\operatorname{prox}_{\\tau \\lambda |\\cdot|}(v_i) = \\operatorname{sign}(v_i) \\max(|v_i| - \\tau\\lambda, 0)\n$$\n有保障的解 $\\widetilde{x}$ 逐元素定义为：\n$$\n\\widetilde{x}_{i} =\n\\begin{cases}\nx^{\\star}_{i},  \\text{若 } |x^{\\star}_{i}| \\ge \\delta \\\\\n0,  \\text{若 } |x^{\\star}_{i}|  \\delta\n\\end{cases}\n$$\n子问题目标的增加量是每个分量增加量的总和：\n$$\n\\Delta Q = Q(\\widetilde{x}; v) - Q(x^{\\star}; v) = \\sum_{i=1}^{n} \\left( Q_i(\\widetilde{x}_i; v_i) - Q_i(x^{\\star}_i; v_i) \\right)\n$$\n这个和中的项仅在索引 $i$ 满足 $\\widetilde{x}_i \\neq x^{\\star}_i$ 时才非零。根据保障规则，这在 $|x^{\\star}_{i}|  \\delta$ 时发生。问题指明我们关心的是非零值被设置为零的情况，因此我们考虑受影响的索引集 $I_{\\delta} = \\{i \\in \\{1,\\dots,n\\} \\mid 0  |x^{\\star}_i|  \\delta\\}$。对于 $i \\in I_{\\delta}$，我们有 $\\widetilde{x}_i = 0$。对于 $i \\notin I_{\\delta}$，该项为零。因此，总增加量是：\n$$\n\\Delta Q = \\sum_{i \\in I_{\\delta}} \\left( Q_i(0; v_i) - Q_i(x^{\\star}_i; v_i) \\right)\n$$\n我们来分析这个和中对于一个受影响的索引 $i \\in I_{\\delta}$ 的单项：\n$$\n\\Delta Q_i = Q_i(0; v_i) - Q_i(x^{\\star}_i; v_i) = \\left( \\lambda |0| + \\frac{1}{2\\tau}(0 - v_i)^{2} \\right) - \\left( \\lambda |x^{\\star}_i| + \\frac{1}{2\\tau}(x^{\\star}_i - v_i)^{2} \\right)\n$$\n$$\n\\Delta Q_i = \\frac{1}{2\\tau}v_i^2 - \\lambda |x^{\\star}_i| - \\frac{1}{2\\tau}( (x^{\\star}_i)^2 - 2x^{\\star}_i v_i + v_i^2 ) = \\frac{1}{2\\tau}(- (x^{\\star}_i)^2 + 2x^{\\star}_i v_i) - \\lambda |x^{\\star}_i|\n$$\n为了简化这个表达式，我们用 $x^{\\star}_i$ 来表示 $v_i$。由于 $i \\in I_{\\delta}$，我们有 $x^{\\star}_i \\neq 0$。软阈值规则 $x^{\\star}_{i} = \\operatorname{sign}(v_i) (|v_i| - \\tau\\lambda)$ 意味着 $\\operatorname{sign}(v_i) = \\operatorname{sign}(x^{\\star}_i)$ 且 $|v_i| = |x^{\\star}_i| + \\tau\\lambda$。因此，我们可以写出 $v_i = x^{\\star}_i + \\tau\\lambda \\operatorname{sign}(x^{\\star}_i)$。\n将 $v_i$ 的这个表达式代入 $\\Delta Q_i$ 的方程中：\n$$\n\\Delta Q_i = \\frac{1}{2\\tau}(- (x^{\\star}_i)^2 + 2x^{\\star}_i (x^{\\star}_i + \\tau\\lambda \\operatorname{sign}(x^{\\star}_i))) - \\lambda |x^{\\star}_i|\n$$\n$$\n\\Delta Q_i = \\frac{1}{2\\tau}(- (x^{\\star}_i)^2 + 2(x^{\\star}_i)^2 + 2\\tau\\lambda x^{\\star}_i \\operatorname{sign}(x^{\\star}_i)) - \\lambda |x^{\\star}_i|\n$$\n使用恒等式 $x^{\\star}_i \\operatorname{sign}(x^{\\star}_i) = |x^{\\star}_i|$：\n$$\n\\Delta Q_i = \\frac{1}{2\\tau}((x^{\\star}_i)^2 + 2\\tau\\lambda |x^{\\star}_i|) - \\lambda |x^{\\star}_i| = \\frac{1}{2\\tau}(x^{\\star}_i)^2 + \\lambda |x^{\\star}_i| - \\lambda |x^{\\star}_i|\n$$\n$$\n\\Delta Q_i = \\frac{1}{2\\tau}(x^{\\star}_i)^2\n$$\n这就是被阈值化为零的单个分量 $i$ 导致的目标函数的精确增加量。\n\n现在，我们为这个增加量寻找一个最坏情况下的界。对于任何受影响的索引 $i \\in I_{\\delta}$，我们有 $|x^{\\star}_i|  \\delta$，这意味着 $(x^{\\star}_i)^2  \\delta^2$。因此，对于每个 $i \\in I_{\\delta}$，\n$$\n\\Delta Q_i  \\frac{\\delta^2}{2\\tau}\n$$\n单次迭代中目标函数的总增加量是所有受影响索引上的总和：\n$$\n\\Delta Q = \\sum_{i \\in I_{\\delta}} \\Delta Q_i = \\sum_{i \\in I_{\\delta}} \\frac{1}{2\\tau}(x^{\\star}_i)^2\n$$\n问题陈述，在每次迭代中，最多有 $r$ 个坐标受阈值下限的影响，所以集合 $I_{\\delta}$ 的大小，记为 $|I_{\\delta}|$，满足 $|I_{\\delta}| \\le r$。单次迭代中 $\\Delta Q$ 的最坏情况上界是通过假设受影响的分量数量最多，且每个分量的量级都取恰好低于 $\\delta$ 的最大可能值得到的：\n$$\n\\Delta Q = \\sum_{i \\in I_{\\delta}} \\frac{1}{2\\tau}(x^{\\star}_i)^2 \\le \\sum_{i \\in I_{\\delta}} \\frac{\\delta^2}{2\\tau} = |I_{\\delta}| \\frac{\\delta^2}{2\\tau} \\le r \\frac{\\delta^2}{2\\tau}\n$$\n所以，在单次 PDHG 迭代中诱导的最坏情况次优性由 $r \\frac{\\delta^2}{2\\tau}$ 界定。\n\n最后，问题要求在 $T$ 次迭代后累积的诱导次优性的一个界。这是来自 $T$ 个独立的原始更新子问题的次优性增加量之和。假设最坏情况在 $T$ 次迭代中的每一次都发生，累积界 $E_{\\text{cumulative}}$ 是单次迭代界的总和：\n$$\nE_{\\text{cumulative}} \\le \\sum_{k=1}^{T} \\left( r \\frac{\\delta^2}{2\\tau} \\right) = T \\cdot r \\frac{\\delta^2}{2\\tau}\n$$\n因此，累积界的最终表达式是 $\\frac{Tr\\delta^2}{2\\tau}$。", "answer": "$$\n\\boxed{\\frac{Tr\\delta^{2}}{2\\tau}}\n$$", "id": "3467272"}]}