{"hands_on_practices": [{"introduction": "总变差（Total Variation, TV）是信号和图像处理中用于促进分段平滑的关键工具。本练习通过一个简单的$2 \\times 2$图像示例，对比了两种主要的二维TV正则化形式：各向异性TV和各向同性TV。通过亲手计算，您将直观地理解这两种正则化项如何以不同方式惩罚不同方向上的变化，这是理解融合LASSO惩罚效应的基础。[@problem_id:3447189]", "problem": "考虑一个在 $2 \\times 2$ 网格上表示的二维离散图像，其具有齐次 Neumann 边界条件（边界处的法向梯度为零）。设灰度强度数组 $u$ 为\n$$\nu \\;=\\;\n\\begin{pmatrix}\n0  2 \\\\\n3  1\n\\end{pmatrix}.\n$$\n使用前向差分离散梯度：对于每个像素 $(i,j)$，如果 $j  2$，水平分量为 $D_{x}u_{i,j} = u_{i,j+1} - u_{i,j}$，且 $D_{x}u_{i,2} = 0$；如果 $i  2$，垂直分量为 $D_{y}u_{i,j} = u_{i+1,j} - u_{i,j}$，且 $D_{y}u_{2,j} = 0$。各向异性全变分 $\\mathrm{TV}_{\\mathrm{aniso}}$ 使用每个像素处离散梯度分量的一范数，而各向同性全变分 $\\mathrm{TV}_{\\mathrm{iso}}$ 使用每个像素处离散梯度的二范数（欧几里得范数）。从离散梯度和范数的这些核心定义出发，计算给定图像的 $\\mathrm{TV}_{\\mathrm{aniso}}$ 和 $\\mathrm{TV}_{\\mathrm{iso}}$。然后，基于范数不等式的基本原理，确定哪种方向上的差异（仅水平、仅垂直，或在同一像素点同时存在水平和垂直差异）被 $\\mathrm{TV}_{\\mathrm{aniso}}$ 与 $\\mathrm{TV}_{\\mathrm{iso}}$ 更强地惩罚，并解释为什么这与压缩感知中融合最小绝对收缩和选择算子 (fused LASSO) 惩罚项的结构相一致。以精确形式给出 $\\mathrm{TV}_{\\mathrm{aniso}}$ 和 $\\mathrm{TV}_{\\mathrm{iso}}$ 的值，不要四舍五入。", "solution": "该问题提法明确且具有科学依据，为获得唯一解提供了所有必要的定义和数据。我们开始求解。\n\n该图像由一个 $2 \\times 2$ 的强度数组 $u$ 表示，其中索引 $(i,j)$ 分别代表从1开始的行和列。给定的图像为：\n$$\nu = \\begin{pmatrix} u_{1,1}  u_{1,2} \\\\ u_{2,1}  u_{2,2} \\end{pmatrix} = \\begin{pmatrix} 0  2 \\\\ 3  1 \\end{pmatrix}\n$$\n每个像素 $(i,j)$ 处的离散梯度是一个向量 $(\\nabla u)_{i,j} = (D_x u_{i,j}, D_y u_{i,j})$。这些分量由带有齐次 Neumann 边界条件的前向差分格式定义。\n对于一个 $2 \\times 2$ 网格（其中最大索引为 $2$），定义如下：\n$D_x u_{i,j} = u_{i,j+1} - u_{i,j}$ (对于 $j  2$)，且 $D_x u_{i,2} = 0$。\n$D_y u_{i,j} = u_{i+1,j} - u_{i,j}$ (对于 $i  2$)，且 $D_y u_{2,j} = 0$。\n\n我们计算四个像素点各自的离散梯度向量：\n1.  对于像素 $(1,1)$：\n    $D_x u_{1,1} = u_{1,2} - u_{1,1} = 2 - 0 = 2$。\n    $D_y u_{1,1} = u_{2,1} - u_{1,1} = 3 - 0 = 3$。\n    因此，$(\\nabla u)_{1,1} = (2, 3)$。\n\n2.  对于像素 $(1,2)$：\n    $D_x u_{1,2} = 0$（边界条件，因为 $j=2$）。\n    $D_y u_{1,2} = u_{2,2} - u_{1,2} = 1 - 2 = -1$。\n    因此，$(\\nabla u)_{1,2} = (0, -1)$。\n\n3.  对于像素 $(2,1)$：\n    $D_x u_{2,1} = u_{2,2} - u_{2,1} = 1 - 3 = -2$。\n    $D_y u_{2,1} = 0$（边界条件，因为 $i=2$）。\n    因此，$(\\nabla u)_{2,1} = (-2, 0)$。\n\n4.  对于像素 $(2,2)$：\n    $D_x u_{2,2} = 0$（边界条件，因为 $j=2$）。\n    $D_y u_{2,2} = 0$（边界条件，因为 $i=2$）。\n    因此，$(\\nabla u)_{2,2} = (0, 0)$。\n\n各向异性全变分 $\\mathrm{TV}_{\\mathrm{aniso}}$ 是每个像素点梯度向量的 $\\ell_1$ 范数之和：\n$$\n\\mathrm{TV}_{\\mathrm{aniso}}(u) = \\sum_{i=1}^2 \\sum_{j=1}^2 \\|(\\nabla u)_{i,j}\\|_1 = \\sum_{i=1}^2 \\sum_{j=1}^2 \\left( |D_x u_{i,j}| + |D_y u_{i,j}| \\right)\n$$\n代入计算出的值：\n$\\mathrm{TV}_{\\mathrm{aniso}}(u) = \\|(2, 3)\\|_1 + \\|(0, -1)\\|_1 + \\|(-2, 0)\\|_1 + \\|(0, 0)\\|_1$\n$\\mathrm{TV}_{\\mathrm{aniso}}(u) = (|2| + |3|) + (|0| + |-1|) + (|-2| + |0|) + (|0| + |0|)$\n$\\mathrm{TV}_{\\mathrm{aniso}}(u) = (2 + 3) + (0 + 1) + (2 + 0) + 0 = 5 + 1 + 2 = 8$。\n\n各向同性全变分 $\\mathrm{TV}_{\\mathrm{iso}}$ 是每个像素点梯度向量的 $\\ell_2$ 范数（欧几里得范数）之和：\n$$\n\\mathrm{TV}_{\\mathrm{iso}}(u) = \\sum_{i=1}^2 \\sum_{j=1}^2 \\|(\\nabla u)_{i,j}\\|_2 = \\sum_{i=1}^2 \\sum_{j=1}^2 \\sqrt{(D_x u_{i,j})^2 + (D_y u_{i,j})^2}\n$$\n代入计算出的值：\n$\\mathrm{TV}_{\\mathrm{iso}}(u) = \\|(2, 3)\\|_2 + \\|(0, -1)\\|_2 + \\|(-2, 0)\\|_2 + \\|(0, 0)\\|_2$\n$\\mathrm{TV}_{\\mathrm{iso}}(u) = \\sqrt{2^2 + 3^2} + \\sqrt{0^2 + (-1)^2} + \\sqrt{(-2)^2 + 0^2} + \\sqrt{0^2 + 0^2}$\n$\\mathrm{TV}_{\\mathrm{iso}}(u) = \\sqrt{4 + 9} + \\sqrt{1} + \\sqrt{4} + 0 = \\sqrt{13} + 1 + 2 = 3 + \\sqrt{13}$。\n\n接下来，我们分析每种 TV 变体对哪个方向的差异惩罚更强。这个比较根植于 $\\mathbb{R}^2$ 中 $\\ell_1$ 和 $\\ell_2$ 范数的性质。设一个像素点的梯度向量为 $\\mathbf{g} = (g_x, g_y)$。各向异性惩罚是 $\\|\\mathbf{g}\\|_1 = |g_x| + |g_y|$，各向同性惩罚是 $\\|\\mathbf{g}\\|_2 = \\sqrt{g_x^2 + g_y^2}$。\n这两种范数之间的基本关系是 $\\|\\mathbf{g}\\|_2 \\le \\|\\mathbf{g}\\|_1$。当且仅当 $\\mathbf{g}$ 的一个分量为零时，等号成立。\n\n情况 1：梯度与某个坐标轴对齐（仅有水平或仅有垂直差异）。例如，$\\mathbf{g} = (g_x, 0)$ 且 $g_x \\neq 0$。\n各向异性惩罚为 $|g_x| + |0| = |g_x|$。\n各向同性惩罚为 $\\sqrt{g_x^2 + 0^2} = |g_x|$。\n在这种情况下，两种惩罚是相同的。\n\n情况 2：梯度同时具有水平和垂直分量（例如，“对角线”方向的差异），因此 $g_x \\neq 0$ 且 $g_y \\neq 0$。\n各向异性惩罚为 $|g_x| + |g_y|$。\n各向同性惩罚为 $\\sqrt{g_x^2 + g_y^2}$。\n在这里，严格不等式 $\\|\\mathbf{g}\\|_2  \\|\\mathbf{g}\\|_1$ 成立。例如，如果对于某个常数 $c \\neq 0$ 有 $\\mathbf{g} = (c, c)$，那么 $\\|\\mathbf{g}\\|_1 = 2|c|$，而 $\\|\\mathbf{g}\\|_2 = \\sqrt{c^2+c^2} = \\sqrt{2}|c|$。显然，$2|c| > \\sqrt{2}|c|$。\n\n这表明，对于一个给定欧几里得大小 $\\|\\mathbf{g}\\|_2$ 的梯度，当梯度在各分量间均匀分布时（例如，沿着对角线方向），$\\ell_1$ 范数（各向异性惩罚）达到最大值；而当它与某个坐标轴对齐时，$\\ell_1$ 范数达到最小值。因此，相对于对轴对齐差异的惩罚，$\\mathrm{TV}_{\\mathrm{aniso}}$ 更严厉地惩罚同时存在的水平和垂直差异。这一特性鼓励梯度稀疏且与坐标轴对齐的解，这可能在图像重建问题中导致“块状”或“阶梯状”的人工痕迹。相比之下，$\\mathrm{TV}_{\\mathrm{iso}}$ 是旋转不变的，只惩罚梯度的大小，而不惩罚其方向。\n\n与融合 LASSO (fused LASSO) 的联系是直接的。对于一维信号 $\\mathbf{x} = (x_1, \\ldots, x_n)$，融合 LASSO 惩罚项包含一个形式为 $\\lambda \\sum_{i=2}^n |x_i - x_{i-1}|$ 的项。该项恰好是信号 $\\mathbf{x}$ 的一维各向异性全变分。其目的是通过惩罚相邻系数之间的差异来促进分段常数解。\n\n将此惩罚项直接推广到二维图像 $u$ 上，就是分别惩罚两个维度上的差异：\n$$\nP(u) = \\lambda_v \\sum_{i,j} |u_{i+1,j} - u_{i,j}| + \\lambda_h \\sum_{i,j} |u_{i,j+1} - u_{i,j}|\n$$\n如果 $\\lambda_v = \\lambda_h = \\lambda$，则该式变为 $\\lambda \\sum_{i,j} (|u_{i+1,j} - u_{i,j}| + |u_{i,j+1} - u_{i,j}|)$，这正是 $\\lambda \\cdot \\mathrm{TV}_{\\mathrm{aniso}}(u)$。这种惩罚的结构是各个差分分量绝对值之和，这是 LASSO 的 $\\ell_1$ 范数惩罚的特征。正是这种可分离的结构导致了上面分析的各向异性行为：它比惩罚轴对齐的梯度更强地惩罚对角线方向的梯度，这一行为继承自 $\\ell_1$ 范数的性质。因此，融合 LASSO 在结构上是各向异性全变分正则化的一种应用。", "answer": "$$\n\\boxed{\\begin{pmatrix} 8  3 + \\sqrt{13} \\end{pmatrix}}\n$$", "id": "3447189"}, {"introduction": "从直观理解过渡到严格的理论是掌握一个概念的关键。本练习将深入探讨融合LASSO模型的核心机制，要求您运用凸优化的基本原理，特别是次微分。通过推导最优性条件，您将精确地揭示$\\ell_1$范数惩罚项和TV惩罚项是如何相互作用，从而产生一个既稀疏（部分系数恰好为零）又分段常数（部分相邻系数完全相等）的解。[@problem_id:3447208]", "problem": "考虑融合最小绝对收缩和选择算子 (Fused LASSO) 信号逼近器，也称为带附加坐标稀疏惩罚项的一维全变分 (TV) 降噪。设给定 $y \\in \\mathbb{R}^{n}$，定义优化问题\n$$\n\\min_{\\beta \\in \\mathbb{R}^{n}} \\; \\frac{1}{2}\\|\\beta - y\\|_{2}^{2} + \\lambda_{1}\\|\\beta\\|_{1} + \\lambda_{2}\\|D\\beta\\|_{1},\n$$\n其中 $\\lambda_{1} \\ge 0$，$\\lambda_{2} \\ge 0$，$D \\in \\mathbb{R}^{(n-1)\\times n}$ 是一阶前向差分算子，其行满足 $(D\\beta)_{i} = \\beta_{i+1} - \\beta_{i}$，其中 $i \\in \\{1,\\dots,n-1\\}$。在全文中，使用正常闭凸函数 $g:\\mathbb{R}^{p}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ 在点 $x \\in \\mathbb{R}^{p}$ 处的次微分定义，\n$$\n\\partial g(x) := \\left\\{\\, v \\in \\mathbb{R}^{p} \\; : \\; g(z) \\ge g(x) + \\langle v, z - x\\rangle \\;\\; \\text{for all } z \\in \\mathbb{R}^{p} \\,\\right\\},\n$$\n以及无约束凸最小化的充要最优性条件，即 $0 \\in \\partial f(x^{\\star})$，其中 $f$ 是凸函数，$x^{\\star}$ 是一个最小化点。\n\n1) 从上述次微分定义出发，以坐标形式写出 $\\ell_{1}$ 范数 $\\|\\beta\\|_{1}$ 在任意点 $\\beta \\in \\mathbb{R}^{n}$ 处的次微分。\n\n2) 使用凸目标函数的最优性条件和次微分求和的性质，推导在任何最小化点 $\\beta^{\\star}$ 处必须成立的坐标包含关系：\n$$\n0 \\in \\beta_{i}^{\\star} - y_{i} + \\lambda_{1} u_{i}^{\\star} + \\lambda_{2}(D^{\\top}v^{\\star})_{i} \\quad \\text{for each } i \\in \\{1,\\dots,n\\},\n$$\n对于某个 $u^{\\star} \\in \\partial \\|\\beta^{\\star}\\|_{1}$ 和某个 $v^{\\star} \\in \\partial \\|D\\beta^{\\star}\\|_{1}$。用此证明，对于任意固定的 $v \\in \\mathbb{R}^{n-1}$，坐标 $\\beta_{i}^{\\star}$ 可写为某个一维强凸问题的解，该解具有依赖于 $y_{i}$、$\\lambda_{1}$ 和 $(D^{\\top}v)_{i}$ 的唯一闭式解，并由此得出 $\\beta_{i}^{\\star} = 0$ 精确成立的充要条件。\n\n3) 特化到 $n=4$ 的情况，并假设在最优解处，TV 次梯度分量满足 $v_{1}^{\\star} = \\frac{1}{2}$，$v_{2}^{\\star} = -\\frac{1}{3}$，以及作为 $D^{\\top}$ 边界条件的 $v_{0}^{\\star} = 0$，$v_{4}^{\\star} = 0$。请提供一个关于 $y_{2}$、$\\lambda_{1}$ 和 $\\lambda_{2}$ 的单一闭式解析表达式，仅使用诸如 $\\operatorname{sign}$、$\\max$ 和绝对值等初等函数。你的最终答案必须是一个依赖于 $y_{2}$、$\\lambda_{1}$ 和 $\\lambda_{2}$ 的单一解析表达式，不含不等式和分段情况。无需四舍五入。", "solution": "我们从凸分析的基本原理开始。正常闭凸函数 $g:\\mathbb{R}^{p}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ 在点 $x$ 处的次微分定义为\n$$\n\\partial g(x) := \\{\\, v \\in \\mathbb{R}^{p} : g(z) \\ge g(x) + \\langle v, z-x\\rangle \\text{ for all } z \\in \\mathbb{R}^{p} \\,\\}.\n$$\n对于正常闭凸函数之和 $f = f_{1}+f_{2}$，一个标准且经过充分检验的次微分求和法则表明\n$$\n\\partial f(x) \\subseteq \\partial f_{1}(x) + \\partial f_{2}(x),\n$$\n在温和的正则性条件下，例如其中一个加项在 $x$ 处连续时，等号成立。对于无约束凸最小化问题，点 $x^{\\star}$ 是 $f$ 的最小化点当且仅当 $0 \\in \\partial f(x^{\\star})$。\n\n步骤 1：$\\ell_{1}$ 范数的次微分。考虑 $g(\\beta) = \\|\\beta\\|_{1} = \\sum_{i=1}^{n} |\\beta_{i}|$。其次微分在各坐标上是可分的：\n$$\n\\partial \\|\\beta\\|_{1} = \\prod_{i=1}^{n} \\partial |\\beta_{i}|,\n$$\n其中，对于标量 $t \\in \\mathbb{R}$，绝对值的次微分为\n$$\n\\partial |t| = \n\\begin{cases}\n\\{ \\operatorname{sign}(t) \\},  \\text{if } t \\neq 0,\\\\\n[-1,1],  \\text{if } t = 0.\n\\end{cases}\n$$\n因此，用坐标形式表示，\n$$\n\\partial \\|\\beta\\|_{1} = \\left\\{\\, u \\in \\mathbb{R}^{n} : u_{i} =\n\\begin{cases}\n\\operatorname{sign}(\\beta_{i}),  \\text{if } \\beta_{i} \\neq 0,\\\\\n\\xi \\text{ with } \\xi \\in [-1,1],  \\text{if } \\beta_{i} = 0,\n\\end{cases}\n\\text{ for all } i \\in \\{1,\\dots,n\\}\\,\\right\\}.\n$$\n\n步骤 2：融合最小绝对收缩和选择算子 (LASSO) 目标函数的最优性。定义目标函数\n$$\nF(\\beta) := \\frac{1}{2}\\|\\beta - y\\|_{2}^{2} + \\lambda_{1}\\|\\beta\\|_{1} + \\lambda_{2}\\|D\\beta\\|_{1}.\n$$\n第一项是光滑的，其梯度为 $\\nabla \\left(\\frac{1}{2}\\|\\beta - y\\|_{2}^{2}\\right) = \\beta - y$。第二和第三项是凸的，但通常非光滑。根据次微分求和法则，\n$$\n\\partial F(\\beta) = (\\beta - y) + \\lambda_{1}\\,\\partial \\|\\beta\\|_{1} + \\lambda_{2}\\, D^{\\top} \\partial \\|D\\beta\\|_{1},\n$$\n这里我们使用了线性映射次微分的链式法则：$\\partial \\|D\\beta\\|_{1} = D^{\\top} \\partial \\|z\\|_{1}\\big|_{z = D\\beta}$。因此，在最小化点 $\\beta^{\\star}$ 处，存在 $u^{\\star} \\in \\partial \\|\\beta^{\\star}\\|_{1}$ 和 $v^{\\star} \\in \\partial \\|D\\beta^{\\star}\\|_{1}$，使得充要最优性条件成立：\n$$\n0 \\in \\beta^{\\star} - y + \\lambda_{1} u^{\\star} + \\lambda_{2} D^{\\top} v^{\\star}.\n$$\n等价地，在坐标上，对每个 $i \\in \\{1,\\dots,n\\}$，\n$$\n0 \\in \\beta_{i}^{\\star} - y_{i} + \\lambda_{1} u_{i}^{\\star} + \\lambda_{2} (D^{\\top} v^{\\star})_{i}.\n$$\n因为 $D$ 是一阶前向差分，$(D\\beta)_{i} = \\beta_{i+1} - \\beta_{i}$ 对 $i \\in \\{1,\\dots,n-1\\}$ 成立。直接计算可得\n$$\nD^{\\top} v = \\begin{bmatrix}\n- v_{1} \\\\\nv_{1} - v_{2} \\\\\n\\vdots \\\\\nv_{n-2} - v_{n-1} \\\\\nv_{n-1}\n\\end{bmatrix},\n$$\n使得对于内部索引 $i \\in \\{2,\\dots,n-1\\}$，有 $(D^{\\top} v)_{i} = v_{i-1} - v_{i}$；在边界处，有 $(D^{\\top} v)_{1} = -v_{1}$，$(D^{\\top} v)_{n} = v_{n-1}$。\n\n固定任意 $v \\in \\mathbb{R}^{n-1}$ 并定义平移后的数据\n$$\nz_{i} := y_{i} - \\lambda_{2} (D^{\\top} v)_{i}, \\quad \\text{for } i \\in \\{1,\\dots,n\\}.\n$$\n那么坐标形式的最优性包含关系变为\n$$\n0 \\in \\beta_{i}^{\\star} - z_{i} + \\lambda_{1} u_{i}^{\\star}, \\quad \\text{with } u_{i}^{\\star} \\in \\partial |\\beta_{i}^{\\star}|.\n$$\n这正是在单变量 $b$ 上的标量问题的的一阶最优性条件，\n$$\n\\min_{b \\in \\mathbb{R}} \\; \\frac{1}{2}(b - z_{i})^{2} + \\lambda_{1} |b|.\n$$\n该问题有一个由软阈值运算给出的唯一闭式解，\n$$\nb^{\\star} = \\operatorname{sign}(z_{i}) \\max\\left(|z_{i}| - \\lambda_{1}, 0\\right).\n$$\n因此，对于任何固定的 $v$，\n$$\n\\beta_{i}^{\\star} = \\operatorname{sign}\\!\\left(y_{i} - \\lambda_{2}(D^{\\top} v)_{i}\\right) \\max\\!\\left(\\left|y_{i} - \\lambda_{2}(D^{\\top} v)_{i}\\right| - \\lambda_{1}, 0\\right).\n$$\n从这个表示中，$\\beta_{i}^{\\star} = 0$ 当且仅当阈值条件\n$$\n\\left|y_{i} - \\lambda_{2}(D^{\\top} v^{\\star})_{i}\\right| \\le \\lambda_{1}\n$$\n成立，其中 $v^{\\star} \\in \\partial \\|D\\beta^{\\star}\\|_{1}$。这精确地展示了即使在存在 TV 耦合的情况下，精确零值是如何出现的：相邻差分的次梯度 $(D^{\\top} v^{\\star})_{i}$ 在进行 $\\ell_{1}$ 收缩之前平移了有效数据 $y_{i}$，如果平移后的量值最多为 $\\lambda_{1}$，则该坐标为零。\n\n步骤 3：特化到 $n=4$ 和所求表达式。对于 $n=4$，假设最优 TV 次梯度满足 $v_{1}^{\\star} = \\frac{1}{2}$，$v_{2}^{\\star} = -\\frac{1}{3}$，以及边界条件 $v_{0}^{\\star} = 0$，$v_{4}^{\\star} = 0$。对于 $i=2$（一个内部索引），我们有\n$$\n(D^{\\top} v^{\\star})_{2} = v_{1}^{\\star} - v_{2}^{\\star} = \\frac{1}{2} - \\left(-\\frac{1}{3}\\right) = \\frac{5}{6}.\n$$\n因此，\n$$\n\\beta_{2}^{\\star} = \\operatorname{sign}\\!\\left(y_{2} - \\lambda_{2}\\cdot \\frac{5}{6}\\right) \\max\\!\\left(\\left|y_{2} - \\lambda_{2}\\cdot \\frac{5}{6}\\right| - \\lambda_{1}, 0\\right).\n$$\n这是一个关于变量 $y_{2}$、$\\lambda_{1}$ 和 $\\lambda_{2}$ 的单一闭式解析表达式，它通过外层的 $\\max$ 隐式地编码了精确零值条件。", "answer": "$$\\boxed{\\operatorname{sign}\\!\\left(y_{2} - \\frac{5}{6}\\lambda_{2}\\right)\\,\\max\\!\\left(\\left|y_{2} - \\frac{5}{6}\\lambda_{2}\\right| - \\lambda_{1},\\, 0\\right)}$$", "id": "3447208"}, {"introduction": "理论理解固然重要，但在实践中求解融合LASSO问题需要强大的算法。本练习将介绍交替方向乘子法（Alternating Direction Method of Multipliers, ADMM），这是一种解决此类问题的通用且高效的算法。通过手动完成一次完整的迭代，您将具体地了解到这个复杂的优化问题如何被分解为一系列更简单的子问题，从而获得对算法工作原理的坚实理解。[@problem_id:3447147]", "problem": "考虑融合最小绝对收缩和选择算子 (fused LASSO) 问题，该问题通过逐元素稀疏性和一维全变分稀疏性来增强最小二乘数据保真度。fused LASSO 的目标函数为\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\lambda_{2} \\|D \\beta\\|_{1},\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$y \\in \\mathbb{R}^{n}$ 是观测数据，$D \\in \\mathbb{R}^{(p-1) \\times p}$ 是编码了一维全变分的一阶前向差分算子。交替方向乘子法 (ADMM) 是一种用于结构化凸优化的分解方法。引入辅助变量 $z \\in \\mathbb{R}^{p}$ 和 $s \\in \\mathbb{R}^{p-1}$ 来分离非光滑项，线性约束为 $z = \\beta$ 和 $s = D \\beta$。从尺度增广拉格朗日原理出发，为原始变量 $\\beta$、$z$、$s$ 和尺度对偶变量 $u$、$v$ 构建 ADMM 更新，然后对以下指定实例进行一次完整的 ADMM 数值迭代。\n\n使用 $n=p=4$，设置\n$$\nX = I_{4}, \\quad y = \\begin{pmatrix} 3 \\\\ -3 \\\\ 9 \\\\ -5 \\end{pmatrix}, \\quad \\lambda_{1} = 1, \\quad \\lambda_{2} = 1, \\quad \\rho = 1,\n$$\n并令前向差分矩阵为\n$$\nD = \\begin{pmatrix}\n-1  1  0  0 \\\\\n0  -1  1  0 \\\\\n0  0  -1  1\n\\end{pmatrix}.\n$$\n将辅助变量和尺度对偶变量初始化为零，\n$$\nz^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad s^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad v^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n从增广拉格朗日和一阶最优性原理出发，推导 ADMM 更新方程，并对给定实例数值计算第一次迭代的值 $\\beta^{(1)}$、$z^{(1)}$、$s^{(1)}$、$u^{(1)}$、$v^{(1)}$。然后构建堆叠的原始约束违反向量\n$$\nr^{(1)} = \\begin{pmatrix} \\beta^{(1)} - z^{(1)} \\\\ D \\beta^{(1)} - s^{(1)} \\end{pmatrix} \\in \\mathbb{R}^{7},\n$$\n并计算其欧几里得范数的平方 $\\|r^{(1)}\\|_{2}^{2}$。提供 $\\|r^{(1)}\\|_{2}^{2}$ 的精确值作为最终答案。不需要四舍五入。", "solution": "该问题要求推导交替方向乘子法 (ADMM) 并将其应用于融合 LASSO 目标函数。我们将首先验证问题陈述，然后推导通用的 ADMM 更新，最后用给定的数值数据应用它们进行一次迭代，以计算所需的量。\n\n### 问题验证\n**第 1 步：提取已知条件**\n- 目标：$\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\lambda_{2} \\|D \\beta\\|_{1}$\n- 维度：$n=p=4$\n- 数据：$y = \\begin{pmatrix} 3 \\\\ -3 \\\\ 9 \\\\ -5 \\end{pmatrix}$, $X = I_{4}$\n- 正则化参数：$\\lambda_{1} = 1$, $\\lambda_{2} = 1$\n- ADMM 惩罚参数：$\\rho = 1$\n- 差分算子：$D = \\begin{pmatrix} -1  1  0  0 \\\\ 0  -1  1  0 \\\\ 0  0  -1  1 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 4}$\n- 初始条件：$z^{(0)} = \\mathbf{0} \\in \\mathbb{R}^4$, $s^{(0)} = \\mathbf{0} \\in \\mathbb{R}^3$, $u^{(0)} = \\mathbf{0} \\in \\mathbb{R}^4$, $v^{(0)} = \\mathbf{0} \\in \\mathbb{R}^3$\n- 用于 ADMM 分离的约束：$z = \\beta$, $s = D \\beta$\n- 目标量：$\\|r^{(1)}\\|_{2}^{2}$，其中 $r^{(1)} = \\begin{pmatrix} \\beta^{(1)} - z^{(1)} \\\\ D \\beta^{(1)} - s^{(1)} \\end{pmatrix}$\n\n**第 2 步：使用提取的已知条件进行验证**\n该问题是科学上合理的、适定的、客观的和完整的。它描述了一个标准的融合 LASSO 正则化回归问题，这是稀疏优化和压缩感知中的一个基本课题。所选择的求解方法 ADMM，是针对此问题结构的一种标准且合适算法。所提供的数据在数学上是一致的，并且足以执行所要求的计算。该问题是有效的。\n\n### ADMM 公式化与推导\n\n原问题等价于以下约束优化问题：\n$$\n\\min_{\\beta, z, s} \\ \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|z\\|_{1} + \\lambda_{2} \\|s\\|_{1} \\quad \\text{subject to} \\quad \\beta - z = 0, \\ D\\beta - s = 0.\n$$\n该问题的尺度增广拉格朗日函数 $\\mathcal{L}_{\\rho}$ 为：\n$$\n\\mathcal{L}_{\\rho}(\\beta, z, s, u, v) = \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda_{1}\\|z\\|_{1} + \\lambda_{2}\\|s\\|_{1} + \\frac{\\rho}{2}\\|\\beta - z + u\\|_{2}^{2} + \\frac{\\rho}{2}\\|D\\beta - s + v\\|_{2}^{2} - \\frac{\\rho}{2}\\|u\\|_{2}^{2} - \\frac{\\rho}{2}\\|v\\|_{2}^{2}\n$$\n其中 $u$ 和 $v$ 是尺度对偶变量。ADMM 算法交替地对原始变量 $\\beta, z, s$ 最小化 $\\mathcal{L}_{\\rho}$，然后更新对偶变量 $u, v$。\n\n第 $(k+1)$ 次迭代包括以下步骤：\n1.  **$\\beta$-最小化：**\n    $\\beta^{(k+1)} = \\arg\\min_{\\beta} \\left( \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\frac{\\rho}{2}\\|\\beta - z^{(k)} + u^{(k)}\\|_{2}^{2} + \\frac{\\rho}{2}\\|D\\beta - s^{(k)} + v^{(k)}\\|_{2}^{2} \\right)$\n    这是一个关于 $\\beta$ 的二次目标函数。一阶最优性条件（将关于 $\\beta$ 的梯度设为零）产生一个线性系统：\n    $$\n    X^T(X\\beta - y) + \\rho(\\beta - z^{(k)} + u^{(k)}) + D^T\\rho(D\\beta - s^{(k)} + v^{(k)}) = 0\n    $$\n    $$\n    (X^TX + \\rho I + \\rho D^TD)\\beta = X^Ty + \\rho(z^{(k)} - u^{(k)}) + \\rho D^T(s^{(k)} - v^{(k)})\n    $$\n    更新公式为 $\\beta^{(k+1)} = (X^TX + \\rho(I + D^TD))^{-1} (X^Ty + \\rho(z^{(k)} - u^{(k)}) + \\rho D^T(s^{(k)} - v^{(k)}))$。\n\n2.  **$z$-最小化：**\n    $z^{(k+1)} = \\arg\\min_{z} \\left( \\lambda_{1}\\|z\\|_{1} + \\frac{\\rho}{2}\\|\\beta^{(k+1)} - z + u^{(k)}\\|_{2}^{2} \\right) = \\arg\\min_{z} \\left( \\lambda_{1}\\|z\\|_{1} + \\frac{\\rho}{2}\\|z - (\\beta^{(k+1)} + u^{(k)})\\|_{2}^{2} \\right)$\n    这是 $\\ell_1$ 范数的邻近算子，即软阈值算子 $S_{\\kappa}(\\cdot)$：\n    $z^{(k+1)} = S_{\\lambda_1/\\rho}(\\beta^{(k+1)} + u^{(k)})$，其中 $(S_{\\kappa}(a))_i = \\text{sign}(a_i) \\max(|a_i| - \\kappa, 0)$。\n\n3.  **$s$-最小化：**\n    $s^{(k+1)} = \\arg\\min_{s} \\left( \\lambda_{2}\\|s\\|_{1} + \\frac{\\rho}{2}\\|D\\beta^{(k+1)} - s + v^{(k)}\\|_{2}^{2} \\right) = \\arg\\min_{s} \\left( \\lambda_{2}\\|s\\|_{1} + \\frac{\\rho}{2}\\|s - (D\\beta^{(k+1)} + v^{(k)})\\|_{2}^{2} \\right)$\n    类似地，这也通过软阈值求解：\n    $s^{(k+1)} = S_{\\lambda_2/\\rho}(D\\beta^{(k+1)} + v^{(k)})$。\n\n4.  **对偶变量更新：**\n    $u^{(k+1)} = u^{(k)} + \\beta^{(k+1)} - z^{(k+1)}$\n    $v^{(k+1)} = v^{(k)} + D\\beta^{(k+1)} - s^{(k+1)}$\n\n### 第一次 ADMM 迭代 (k=0)\n\n我们使用给定的初始条件 $z^{(0)}=\\mathbf{0}, s^{(0)}=\\mathbf{0}, u^{(0)}=\\mathbf{0}, v^{(0)}=\\mathbf{0}$ 和参数 $\\lambda_1=\\lambda_2=\\rho=1$, $X=I_4$ 执行一次迭代。\n\n**1. 计算 $\\beta^{(1)}$**\n更新方程简化为：\n$$\n\\beta^{(1)} = (I_4^T I_4 + 1(I_4 + D^TD))^{-1} (I_4^T y + 1(\\mathbf{0} - \\mathbf{0}) + 1D^T(\\mathbf{0} - \\mathbf{0})) = (2I_4 + D^TD)^{-1} y\n$$\n首先，我们计算矩阵 $D^TD$：\n$$\nD^T = \\begin{pmatrix} -1  0  0 \\\\ 1  -1  0 \\\\ 0  1  -1 \\\\ 0  0  1 \\end{pmatrix}, \\quad D^TD = \\begin{pmatrix} 1  -1  0  0 \\\\ -1  2  -1  0 \\\\ 0  -1  2  -1 \\\\ 0  0  -1  1 \\end{pmatrix}\n$$\n需要求逆的矩阵是 $A = 2I_4 + D^TD$：\n$$\nA = 2\\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  1  0 \\\\ 0  0  0  1 \\end{pmatrix} + \\begin{pmatrix} 1  -1  0  0 \\\\ -1  2  -1  0 \\\\ 0  -1  2  -1 \\\\ 0  0  -1  1 \\end{pmatrix} = \\begin{pmatrix} 3  -1  0  0 \\\\ -1  4  -1  0 \\\\ 0  -1  4  -1 \\\\ 0  0  -1  3 \\end{pmatrix}\n$$\n我们求解线性系统 $A\\beta^{(1)} = y$：\n$$\n\\begin{pmatrix} 3  -1  0  0 \\\\ -1  4  -1  0 \\\\ 0  -1  4  -1 \\\\ 0  0  -1  3 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -3 \\\\ 9 \\\\ -5 \\end{pmatrix}\n$$\n求解该系统得到：\n$\\beta^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix}$。\n\n**2. 计算 $z^{(1)}$**\n$z^{(1)} = S_{\\lambda_1/\\rho}(\\beta^{(1)} + u^{(0)}) = S_{1/1}(\\beta^{(1)} + \\mathbf{0}) = S_1(\\beta^{(1)})$。\n当 $\\beta^{(1)} = (1, 0, 2, -1)^T$ 且阈值为 $\\kappa=1$ 时：\n- $z_1^{(1)} = S_1(1) = \\text{sign}(1)\\max(|1|-1, 0) = 0$\n- $z_2^{(1)} = S_1(0) = \\text{sign}(0)\\max(|0|-1, 0) = 0$\n- $z_3^{(1)} = S_1(2) = \\text{sign}(2)\\max(|2|-1, 0) = 1$\n- $z_4^{(1)} = S_1(-1) = \\text{sign}(-1)\\max(|-1|-1, 0) = 0$\n因此，$z^{(1)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$。\n\n**3. 计算 $s^{(1)}$**\n$s^{(1)} = S_{\\lambda_2/\\rho}(D\\beta^{(1)} + v^{(0)}) = S_{1/1}(D\\beta^{(1)} + \\mathbf{0}) = S_1(D\\beta^{(1)})$。\n首先，计算参数 $D\\beta^{(1)}$：\n$$\nD\\beta^{(1)} = \\begin{pmatrix} -1  1  0  0 \\\\ 0  -1  1  0 \\\\ 0  0  -1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1(1) + 1(0) \\\\ -1(0) + 1(2) \\\\ -1(2) + 1(-1) \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 2 \\\\ -3 \\end{pmatrix}\n$$\n现在，应用阈值为 $\\kappa=1$ 的软阈值：\n- $s_1^{(1)} = S_1(-1) = 0$\n- $s_2^{(1)} = S_1(2) = 1$\n- $s_3^{(1)} = S_1(-3) = -2$\n因此，$s^{(1)} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -2 \\end{pmatrix}$。\n\n题目也隐含地要求在完整迭代描述中计算 $u^{(1)}$ 和 $v^{(1)}$。\n**4. 计算 $u^{(1)}$ 和 $v^{(1)}$**\n$u^{(1)} = u^{(0)} + \\beta^{(1)} - z^{(1)} = \\mathbf{0} + \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$。\n$v^{(1)} = v^{(0)} + D\\beta^{(1)} - s^{(1)} = \\mathbf{0} + \\begin{pmatrix} -1 \\\\ 2 \\\\ -3 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\end{pmatrix}$。\n\n### 计算最终量\n\n问题要求计算堆叠的原始约束违反向量的欧几里得范数的平方，即 $\\|r^{(1)}\\|_{2}^{2}$。向量 $r^{(1)}$ 定义为：\n$$\nr^{(1)} = \\begin{pmatrix} \\beta^{(1)} - z^{(1)} \\\\ D\\beta^{(1)} - s^{(1)} \\end{pmatrix}\n$$\n$r^{(1)}$ 的两个分量正好是对偶变量的更新量（因为初始对偶变量为零）。\n$$\n\\beta^{(1)} - z^{(1)} = u^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\nD\\beta^{(1)} - s^{(1)} = v^{(1)} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n因此，堆叠向量为：\n$$\nr^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n其欧几里得范数的平方是其各分量平方的和：\n$$\n\\|r^{(1)}\\|_{2}^{2} = 1^2 + 0^2 + 1^2 + (-1)^2 + (-1)^2 + 1^2 + (-1)^2 = 1 + 0 + 1 + 1 + 1 + 1 + 1 = 6.\n$$", "answer": "$$\n\\boxed{6}\n$$", "id": "3447147"}]}