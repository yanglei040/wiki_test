{"hands_on_practices": [{"introduction": "许多强大的稀疏先验可以被理解为高斯尺度混合（Gaussian Scale Mixtures, GSM）。本练习将引导你从高斯-逆伽马混合模型中推导出学生t先验（Student-t prior）。通过分析得到的惩罚函数[@problem_id:3451059]，你将揭示为什么在某些问题中它比标准的 $\\ell_1$ 范数是更好的选择，特别是它能够避免对大的、真实的系数进行过度惩罚的特性。", "problem": "考虑一个实系数 $x \\in \\mathbb{R}$，其具有一个通过高斯尺度混合 (GSM) 构建的分层稀疏先验。具体来说，给定一个隐方差 $v > 0$，令 $x \\,|\\, v \\sim \\mathcal{N}(0, v)$，并令混合密度为一个逆伽马分布 $v \\sim \\mathrm{InvGamma}(\\alpha, \\beta)$，其形状参数为 $\\alpha = \\nu/2$，尺度参数为 $\\beta = \\nu \\tau^{2}/2$，其中 $\\nu > 0$ 是自由度参数，$\\tau > 0$ 是一个尺度参数。定义对于 $r = |x|$ 的负对数先验惩罚项 $\\phi(r)$ (相差一个加性常数) 为 $\\phi(r) := -\\ln p(x)$，其中加性常数的选择使得 $\\phi(0) = 0$。\n\n任务：\n- 仅从高斯密度、逆伽马密度和GSM构造的定义出发，推导惩罚项 $\\phi(r)$ 作为 $r$、$\\nu$ 和 $\\tau$ 的函数的显式闭式表达式。\n- 使用关于 $r$ 的一阶和二阶导数，确定惩罚项在 $r \\ge 0$ 的哪些区间上是凸的或凹的，并说明 $\\phi(r)$ 是否在 $r \\ge 0$ 上严格递增。\n- 将 $\\phi(r)$ 在 $r \\to \\infty$ 时的渐近增长与 $\\ell_{1}$ 惩罚项 $\\phi_{\\ell_{1}}(r) = r$ 的渐近增长进行比较。\n- 考虑在最大后验 (MAP) 估计器下的标量高斯去噪：对于给定的观测值 $y \\in \\mathbb{R}$ 和噪声方差 $\\sigma^{2} > 0$，定义\n$$\n\\hat{x}(y) \\in \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ \\frac{1}{2 \\sigma^{2}} (y - x)^{2} + \\lambda \\, \\phi(|x|) \\right\\},\n$$\n其中 $\\lambda > 0$ 是一个正则化权重。确定相关的近端算子是否存在一个精确阈值 $T > 0$，使得对于所有 $|y| \\le T$ 都有 $\\hat{x}(y) = 0$。如果存在这样的阈值，请给出 $T$ 的闭式解；如果不存在，请说明 $T = 0$ 并证明这一事实。将其与 $\\ell_{1}$ 情况 $\\phi_{\\ell_{1}}(r) = r$ 进行对比，并确定该情况下的相应阈值。\n\n你最终报告的答案必须是你推导出的 $\\phi(r)$ 的显式表达式（其中加性常数的选择使得 $\\phi(0) = 0$）。不需要数值近似。", "solution": "我们从高斯尺度混合 (GSM) 构造开始。根据定义，$x$ 在给定 $v$ 下的条件先验是高斯的，\n$$\np(x \\,|\\, v) \\;=\\; \\frac{1}{\\sqrt{2 \\pi v}} \\exp\\!\\left(-\\frac{x^{2}}{2 v}\\right),\n$$\n混合密度是逆伽马分布，其形状参数为 $\\alpha = \\nu/2$，尺度参数为 $\\beta = \\nu \\tau^{2}/2$：\n$$\np(v) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\, v^{-\\alpha - 1} \\exp\\!\\left(-\\frac{\\beta}{v}\\right), \\quad v > 0.\n$$\n$x$ 的边缘先验通过对 $v$ 积分得到：\n$$\np(x) \\;=\\; \\int_{0}^{\\infty} p(x \\,|\\, v) \\, p(v) \\, \\mathrm{d}v \n\\;=\\; \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2 \\pi v}} \\exp\\!\\left(-\\frac{x^{2}}{2 v}\\right) \\cdot \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\, v^{-\\alpha - 1} \\exp\\!\\left(-\\frac{\\beta}{v}\\right) \\, \\mathrm{d}v.\n$$\n合并 $v$ 的幂次和指数项得到\n$$\np(x) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)\\sqrt{2\\pi}} \\int_{0}^{\\infty} v^{-(\\alpha + \\tfrac{3}{2})} \\exp\\!\\left(-\\frac{\\tfrac{x^{2}}{2} + \\beta}{v}\\right) \\, \\mathrm{d}v.\n$$\n定义 $C := \\tfrac{x^{2}}{2} + \\beta$。对于 $p > 0$，使用标准积分恒等式\n$$\n\\int_{0}^{\\infty} v^{-p-1} \\exp\\!\\left(-\\frac{C}{v}\\right) \\, \\mathrm{d}v \\;=\\; C^{-p} \\, \\Gamma(p),\n$$\n并令 $p = \\alpha + \\tfrac{1}{2}$，我们得到\n$$\np(x) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)\\sqrt{2\\pi}} \\, \\Gamma\\!\\left(\\alpha + \\tfrac{1}{2}\\right) \\, C^{-(\\alpha + \\tfrac{1}{2})}.\n$$\n代入 $\\alpha = \\nu/2$ 和 $\\beta = \\nu \\tau^{2}/2$，我们有 $C = \\tfrac{1}{2}(x^{2} + \\nu \\tau^{2})$ 且\n$$\np(x) \\;=\\; \\frac{\\left(\\tfrac{\\nu \\tau^{2}}{2}\\right)^{\\nu/2}}{\\Gamma(\\nu/2)\\sqrt{2\\pi}} \\, \\Gamma\\!\\left(\\tfrac{\\nu + 1}{2}\\right) \\left(\\tfrac{x^{2} + \\nu \\tau^{2}}{2}\\right)^{-\\tfrac{\\nu + 1}{2}}.\n$$\n重新整理常数项，得到具有 $\\nu$ 自由度和尺度 $\\tau$ 的标准学生t分布密度，\n$$\np(x) \\;=\\; \\frac{\\Gamma\\!\\left(\\tfrac{\\nu + 1}{2}\\right)}{\\Gamma\\!\\left(\\tfrac{\\nu}{2}\\right) \\sqrt{\\pi \\nu} \\, \\tau} \\left(1 + \\frac{x^{2}}{\\nu \\tau^{2}}\\right)^{-\\tfrac{\\nu + 1}{2}}.\n$$\n定义对于 $r = |x|$ 的惩罚项 $\\phi(r)$ 为 $\\phi(r) := -\\ln p(x)$ (相差一个加性常数，该常数的选择使得 $\\phi(0) = 0$）。使用上面的表达式，\n$$\n-\\ln p(x) \\;=\\; \\text{const} + \\frac{\\nu + 1}{2} \\, \\ln\\!\\left(1 + \\frac{x^{2}}{\\nu \\tau^{2}}\\right).\n$$\n施加 $\\phi(0) = 0$ 的条件来确定加性常数，从而得到显式惩罚项\n$$\n\\phi(r) \\;=\\; \\frac{\\nu + 1}{2} \\, \\ln\\!\\left(1 + \\frac{r^{2}}{\\nu \\tau^{2}}\\right).\n$$\n\n我们现在分析其在 $r \\ge 0$ 上的曲率和单调性。对 $r$ 求导：\n$$\n\\phi'(r) \\;=\\; \\frac{\\nu + 1}{2} \\cdot \\frac{2 r}{\\nu \\tau^{2} + r^{2}} \\;=\\; \\frac{(\\nu + 1) \\, r}{\\nu \\tau^{2} + r^{2}}.\n$$\n对于 $r \\ge 0$，我们有 $\\phi'(r) \\ge 0$，且 $\\phi'(r) = 0$ 当且仅当 $r = 0$，因此 $\\phi$ 在 $r > 0$ 上是严格递增的。二阶导数是\n$$\n\\phi''(r) \\;=\\; (\\nu + 1) \\cdot \\frac{(\\nu \\tau^{2} + r^{2}) - 2 r^{2}}{(\\nu \\tau^{2} + r^{2})^{2}}\n\\;=\\; (\\nu + 1) \\cdot \\frac{\\nu \\tau^{2} - r^{2}}{(\\nu \\tau^{2} + r^{2})^{2}}.\n$$\n因此，当 $0 \\le r  \\sqrt{\\nu}\\,\\tau$ 时 $\\phi''(r) > 0$，在 $r = \\sqrt{\\nu}\\,\\tau$ 时 $\\phi''(r) = 0$，而当 $r > \\sqrt{\\nu}\\,\\tau$ 时 $\\phi''(r)  0$。所以，该惩罚项在原点附近是凸的，在 $r = \\sqrt{\\nu}\\,\\tau$ 处有一个拐点，并且对于足够大的 $r$ 是凹的。\n\n对于渐近增长，当 $r \\to \\infty$ 时，\n$$\n\\phi(r) \\;=\\; \\frac{\\nu + 1}{2} \\, \\ln\\!\\left(\\frac{r^{2}}{\\nu \\tau^{2}}\\left(1 + o(1)\\right)\\right)\n\\;=\\; (\\nu + 1) \\, \\ln r \\;-\\; \\frac{\\nu + 1}{2} \\, \\ln(\\nu \\tau^{2}) \\;+\\; o(1),\n$$\n其呈对数增长。相比之下，$\\ell_{1}$ 惩罚项 $\\phi_{\\ell_{1}}(r) = r$ 呈线性增长。因此，学生t惩罚项对大系数的惩罚远小于 $\\ell_{1}$，这反映了其重尾特性。\n\n接下来我们分析在最大后验 (MAP) 估计器下进行标量高斯去噪的阈值问题，其目标函数为\n$$\nJ(x) \\;=\\; \\frac{1}{2 \\sigma^{2}} (y - x)^{2} + \\lambda \\, \\phi(|x|).\n$$\n我们检查是否存在 $T > 0$ 使得对于所有 $|y| \\le T$ 都有 $\\hat{x}(y) = 0$。因为 $\\phi$ 在 $r = 0$ 处是可微的，且\n$$\n\\phi'(0^{+}) \\;=\\; \\lim_{r \\downarrow 0} \\frac{(\\nu + 1) \\, r}{\\nu \\tau^{2} + r^{2}} \\;=\\; 0,\n$$\n所以 $J$ 在 $x=0$ 处的单侧导数为\n$$\n\\left.\\frac{\\mathrm{d}}{\\mathrm{d}x} J(x)\\right|_{x \\downarrow 0} \\;=\\; -\\frac{y}{\\sigma^{2}} + \\lambda \\, \\phi'(0^{+}) \\;=\\; -\\frac{y}{\\sigma^{2}}, \n\\quad\n\\left.\\frac{\\mathrm{d}}{\\mathrm{d}x} J(x)\\right|_{x \\uparrow 0} \\;=\\; -\\frac{y}{\\sigma^{2}} - \\lambda \\, \\phi'(0^{+}) \\;=\\; -\\frac{y}{\\sigma^{2}}.\n$$\n为了使 $x=0$ 成为一个极小值点，我们需要 0 属于 $J$ 在 0 点的次微分。由于 $J$ 在 0 点是可微的，这里的次微分简化为单点集 $\\{-y/\\sigma^{2}\\}$，而该集合包含 0 当且仅当 $y=0$。因此，不存在非零的阈值区域：唯一使得极小值点恰好为零的情况是 $y=0$。所以，对于学生t惩罚项，阈值为 $T=0$。这与 $\\ell_{1}$ 惩罚项 $\\phi_{\\ell_{1}}(r) = r$ 的情况形成对比，其在 0 点的次梯度是区间 $[-1, 1]$，从而得出众所周知的软阈值条件 $|y|/\\sigma^{2} \\le \\lambda$ 使得 $\\hat{x}(y) = 0$，即阈值 $T = \\lambda \\, \\sigma^{2}$。\n\n总结如下：\n- 由学生t GSM先验导出的显式惩罚项是 $\\phi(r) = \\tfrac{\\nu + 1}{2} \\ln\\!\\left(1 + \\tfrac{r^{2}}{\\nu \\tau^{2}}\\right)$，且 $\\phi(0) = 0$。\n- $\\phi$ 在 $r \\ge 0$ 上严格递增，在 $0 \\le r  \\sqrt{\\nu}\\,\\tau$ 上是凸的，在 $r = \\sqrt{\\nu}\\,\\tau$ 有一个拐点，在 $r > \\sqrt{\\nu}\\,\\tau$ 上是凹的。\n- 当 $r \\to \\infty$ 时，$\\phi(r)$ 呈对数增长，慢于 $\\ell_{1}$ 的线性增长。\n- 使用此惩罚项的标量MAP去噪器没有非零阈值 ($T = 0$)，这与 $\\ell_{1}$ 的情况 ($T = \\lambda \\sigma^{2}$) 不同。", "answer": "$$\\boxed{\\frac{\\nu + 1}{2} \\,\\ln\\!\\left(1 + \\frac{r^{2}}{\\nu \\tau^{2}}\\right)}$$", "id": "3451059"}, {"introduction": "现实世界中的稀疏性往往以结构化的形式出现，即系数以组为单位稀疏，而非单个稀疏。这需要我们调整先验和算法来利用这种结构。本练习着重于推导“块软阈值”（block-soft-thresholding）算子——解决组套索（group lasso）问题的关键算法构件，并将其应用于一个具体的数值例子中[@problem_id:3451068]。", "problem": "考虑一个在压缩感知中具有分组结构的线性逆问题。测量模型为 $y = A x + \\varepsilon$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^{n}$，且噪声 $\\varepsilon$ 是独立的，服从高斯分布 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$。假设系数向量 $x$ 被划分为不相交的组 $\\{G_{g}\\}_{g=1}^{G}$，其中 $x_{G_{g}} \\in \\mathbb{R}^{|G_{g}|}$ 表示对应于 $G_{g}$ 中索引的子向量。假设通过高斯尺度混合 (GSM) 构建了一个层级稀疏性先验：对于每个组 $g$，条件先验为 $p(x_{G_{g}} \\mid \\tau_{g}) = \\mathcal{N}(0, \\tau_{g} I_{|G_{g}|})$，并且选择尺度 $\\tau_{g}$ 上的混合分布，使得 $x_{G_{g}}$ 上的边际先验是旋转不变且重尾的。使用这种 GSM 构造和在上述高斯似然下的最大后验 (MAP) 估计，可以得到形式为 $\\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_{2}$ 的惩罚项，其中 $\\lambda  0$。\n\n从高斯似然、GSM 层级先验以及凸惩罚的近端算子定义出发，推导出作为组套索惩罚 $\\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_{2}$ 的近端映射出现的块软阈值算子。然后，当设计矩阵和数据指定为\n$$\nA = I_{3 \\times 3}, \\quad y = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0.5 \\end{pmatrix},\n$$\n时，从 $x^{(0)} = 0$ 开始，使用步长 $t = 1$，对该惩罚最小二乘目标应用一个近端梯度步，其中有两个组 $G_{1} = \\{1,2\\}$ 和 $G_{2} = \\{3\\}$，正则化参数 $\\lambda = 2$。令 $z = x^{(0)} + t A^{\\top}(y - A x^{(0)})$ 表示近端算子的梯度步输入。使用你推导出的块软阈值算子，在给定的分组下计算 $x^{(1)} = \\operatorname{prox}_{t \\lambda \\sum_{g} \\|\\cdot\\|_{2}}(z)$。\n\n最后，提供欧几里得范数 $\\|x^{(1)}\\|_{2}$ 作为你的唯一最终答案。不需要四舍五入。", "solution": "该问题要求推导块软阈值算子，并将其应用于组套索惩罚最小二乘问题的一个近端梯度算法步骤中。最后，我们必须计算结果向量的欧几里得范数。\n\n其底层的优化问题源于最大后验 (MAP) 估计。给定高斯似然 $p(y|x) \\propto \\exp(-\\frac{1}{2\\sigma^2}\\|y - Ax\\|_2^2)$ 和一个关于 $x$ 的层级先验，该先验导出了形式为 $\\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_{2}$ 的惩罚项，MAP 估计通过最小化负对数后验来找到，这等价于求解组套索问题：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\left( \\frac{1}{2} \\|y - Ax\\|_2^2 + \\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_2 \\right)\n$$\n这是一个形式为 $J(x) = f(x) + h(x)$ 的目标函数，其中 $f(x) = \\frac{1}{2}\\|y - Ax\\|_2^2$ 是一个光滑凸函数，而 $h(x) = \\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_2$ 是一个非光滑凸函数。这类问题可以使用近端梯度方法高效求解。近端梯度算法的单步迭代如下：\n$$\nx^{(k+1)} = \\operatorname{prox}_{t h}(x^{(k)} - t \\nabla f(x^{(k)}))\n$$\n其中 $t  0$ 是步长。$f(x)$ 的梯度是 $\\nabla f(x) = A^\\top(Ax - y)$。因此，更新步骤可以写成：\n$$\nx^{(k+1)} = \\operatorname{prox}_{t \\lambda \\sum_{g} \\|\\cdot\\|_{G_g, 2}}(x^{(k)} + t A^\\top(y - Ax^{(k)}))\n$$\n其中我们使用符号 $\\|\\cdot\\|_{G_g, 2}$ 来表示对子向量 $x_{G_g}$ 应用 $\\ell_2$-范数。令 $z = x^{(k)} + t A^\\top(y - Ax^{(k)})$。更新的核心是计算近端算子 $\\operatorname{prox}_{t h}(z)$。\n\n首先，我们推导这个近端算子的形式，即块软阈值算子。\n函数 $\\phi(x)$ 的近端算子定义为：\n$$\n\\operatorname{prox}_{\\phi}(z) = \\arg \\min_{x} \\left( \\phi(x) + \\frac{1}{2} \\|x - z\\|_2^2 \\right)\n$$\n在我们的问题中，函数是 $\\phi(x) = \\alpha \\sum_{g=1}^{G} \\|x_{G_g}\\|_2$，为简化符号，我们设 $\\alpha = t \\lambda$。近端算子的优化问题变为：\n$$\n\\arg \\min_{x} \\left( \\alpha \\sum_{g=1}^{G} \\|x_{G_g}\\|_2 + \\frac{1}{2} \\|x - z\\|_2^2 \\right)\n$$\n由于组 $\\{G_g\\}$ 构成了索引的一个划分，平方欧几里得范数项在这些组上是可分的：$\\|x - z\\|_2^2 = \\sum_{g=1}^{G} \\|x_{G_g} - z_{G_g}\\|_2^2$。这种可分性使我们能够将最小化问题分解为 $G$ 个独立的子问题，每个组一个：\n$$\n\\min_{x} \\sum_{g=1}^{G} \\left( \\alpha \\|x_{G_g}\\|_2 + \\frac{1}{2} \\|x_{G_g} - z_{G_g}\\|_2^2 \\right)\n$$\n完整向量 $x$ 的解是通过连接各个子问题的解 $x_{G_g}^*$ 得到的：\n$$\nx_{G_g}^* = \\arg \\min_{u \\in \\mathbb{R}^{|G_g|}} \\left( \\alpha \\|u\\|_2 + \\frac{1}{2} \\|u - v\\|_2^2 \\right)\n$$\n其中我们令 $u = x_{G_g}$ 和 $v = z_{G_g}$。\n为了解决这个子问题，我们使用次微分计算。一阶最优性条件是在最小值 $u^*$ 处，0 必须在目标函数的次微分中：\n$$\n0 \\in \\partial \\left( \\alpha \\|u^*\\|_2 + \\frac{1}{2} \\|u^* - v\\|_2^2 \\right) = \\alpha \\partial(\\|u^*\\|_2) + (u^* - v)\n$$\n$\\ell_2$-范数的次微分由下式给出：\n$$\n\\partial(\\|u\\|_2) = \\begin{cases} \\{ u / \\|u\\|_2 \\}  \\text{if } u \\neq 0 \\\\ \\{ w \\in \\mathbb{R}^{|G_g|} : \\|w\\|_2 \\le 1 \\}  \\text{if } u = 0 \\end{cases}\n$$\n我们分析解 $u^*$ 的两种情况：\n\n情况 1：$u^* = 0$。\n最优性条件变为 $v \\in \\alpha \\partial(\\|0\\|_2) = \\{w: \\|w\\|_2 \\le \\alpha \\}$。这意味着如果 $\\|v\\|_2 \\le \\alpha$，那么 $u^*=0$ 是一个有效的解。\n\n情况 2：$u^* \\neq 0$。\n最优性条件变为 $0 = \\alpha \\frac{u^*}{\\|u^*\\|_2} + u^* - v$。\n整理得到 $v = u^* + \\alpha \\frac{u^*}{\\|u^*\\|_2} = u^* \\left(1 + \\frac{\\alpha}{\\|u^*\\|_2}\\right)$。\n这个方程意味着 $v$ 必须是 $u^*$ 的一个正向缩放，所以 $u^*$ 和 $v$ 是共线的，并且指向同一个方向。我们可以写成 $u^* = c v$，其中某个标量 $c  0$。对 $v = u^*(1 + \\alpha/\\|u^*\\|_2)$ 两边取范数，我们得到 $\\|v\\|_2 = \\|u^*\\|_2(1 + \\alpha/\\|u^*\\|_2) = \\|u^*\\|_2 + \\alpha$。因此，$\\|u^*\\|_2 = \\|v\\|_2 - \\alpha$。因为 $\\|u^*\\|_2  0$，所以这种情况只有在 $\\|v\\|_2  \\alpha$ 时才可能。\n将 $\\|u^*\\|_2$ 代回到 $v$ 的表达式中：\n$v = u^* \\left(1 + \\frac{\\alpha}{\\|v\\|_2 - \\alpha}\\right) = u^* \\left(\\frac{\\|v\\|_2 - \\alpha + \\alpha}{\\|v\\|_2 - \\alpha}\\right) = u^* \\frac{\\|v\\|_2}{\\|v\\|_2 - \\alpha}$。\n解出 $u^*$ 得到 $u^* = v \\frac{\\|v\\|_2 - \\alpha}{\\|v\\|_2} = \\left(1 - \\frac{\\alpha}{\\|v\\|_2}\\right) v$。\n\n结合这两种情况，子问题的解是：\n$$\nx_{G_g}^* = \\begin{cases} \\left(1 - \\frac{\\alpha}{\\|z_{G_g}\\|_2}\\right) z_{G_g}  \\text{if } \\|z_{G_g}\\|_2  \\alpha \\\\ 0  \\text{if } \\|z_{G_g}\\|_2 \\le \\alpha \\end{cases}\n$$\n这可以紧凑地写成 $x_{G_g}^* = \\left(1 - \\frac{\\alpha}{\\|z_{G_g}\\|_2}\\right)_+ z_{G_g}$，其中 $(c)_+ = \\max(0, c)$。这就是块软阈值算子。\n\n现在我们将此应用于具体问题。我们需要从 $x^{(0)} = 0$ 开始计算 $x^{(1)}$。\n更新步骤是 $x^{(1)} = \\operatorname{prox}_{t \\lambda \\sum_{g} \\|\\cdot\\|_{2}}(z)$，其中 $z = x^{(0)} + t A^{\\top}(y - A x^{(0)})$。给定的参数是：\n$$\nA = I_{3 \\times 3}, \\quad y = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0.5 \\end{pmatrix}, \\quad x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad t = 1, \\quad \\lambda = 2\n$$\n首先，我们计算近端算子的参数 $z$：\n$$\nz = x^{(0)} + t A^{\\top}(y - A x^{(0)}) = 0 + 1 \\cdot I_{3 \\times 3}^\\top(y - I_{3 \\times 3} \\cdot 0) = y = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0.5 \\end{pmatrix}\n$$\n块软阈值的参数是 $\\alpha = t \\lambda = 1 \\cdot 2 = 2$。\n分组为 $G_1 = \\{1,2\\}$ 和 $G_2 = \\{3\\}$。我们相应地划分 $z$：\n$$\nz_{G_1} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}, \\quad z_{G_2} = \\begin{pmatrix} 0.5 \\end{pmatrix}\n$$\n现在我们将推导出的算子应用于每个组子向量。\n\n对于组 $G_1$：\n首先，计算 $\\ell_2$-范数：\n$$\n\\|z_{G_1}\\|_2 = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n$$\n我们将这个范数与 $\\alpha$ 进行比较。由于 $\\|z_{G_1}\\|_2 = 5  \\alpha = 2$，我们处于“收缩”情况：\n$$\nx_{G_1}^{(1)} = \\left(1 - \\frac{\\alpha}{\\|z_{G_1}\\|_2}\\right) z_{G_1} = \\left(1 - \\frac{2}{5}\\right) \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\frac{3}{5} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 9/5 \\\\ 12/5 \\end{pmatrix} = \\begin{pmatrix} 1.8 \\\\ 2.4 \\end{pmatrix}\n$$\n\n对于组 $G_2$：\n首先，计算 $\\ell_2$-范数：\n$$\n\\|z_{G_2}\\|_2 = |0.5| = 0.5\n$$\n我们将这个范数与 $\\alpha$ 进行比较。由于 $\\|z_{G_2}\\|_2 = 0.5 \\le \\alpha = 2$，我们处于“阈值”情况：\n$$\nx_{G_2}^{(1)} = 0\n$$\n\n合并两个组的结果，我们得到更新后的向量 $x^{(1)}$：\n$$\nx^{(1)} = \\begin{pmatrix} x_{G_1}^{(1)} \\\\ x_{G_2}^{(1)} \\end{pmatrix} = \\begin{pmatrix} 1.8 \\\\ 2.4 \\\\ 0 \\end{pmatrix}\n$$\n\n最后，问题要求计算这个结果向量的欧几里得范数 $\\|x^{(1)}\\|_2$。\n$$\n\\|x^{(1)}\\|_2 = \\sqrt{(1.8)^2 + (2.4)^2 + 0^2} = \\sqrt{\\left(\\frac{9}{5}\\right)^2 + \\left(\\frac{12}{5}\\right)^2 + 0}\n$$\n$$\n\\|x^{(1)}\\|_2 = \\sqrt{\\frac{81}{25} + \\frac{144}{25}} = \\sqrt{\\frac{81 + 144}{25}} = \\sqrt{\\frac{225}{25}} = \\sqrt{9} = 3\n$$\n另外，由于 $x_{G_2}^{(1)}=0$，范数就是第一个块分量的范数：\n$$\n\\|x^{(1)}\\|_2 = \\|x_{G_1}^{(1)}\\|_2 = \\left\\| \\frac{3}{5} z_{G_1} \\right\\|_2 = \\frac{3}{5} \\|z_{G_1}\\|_2 = \\frac{3}{5} \\cdot 5 = 3\n$$\n$x^{(1)}$ 的欧几里得范数是 $3$。", "answer": "$$\\boxed{3}$$", "id": "3451068"}, {"introduction": "高斯尺度混合（GSM）框架不仅可用于施加稀疏性，还能构建对测量值中的异常值具有鲁棒性的模型。这个最终的练习将这些思想结合起来，要求你实现一个完整的迭代重加权最小二乘（Iteratively Reweighted Least Squares, IRLS）算法。本练习将展示理论上的GSM模型如何直接转化为一个实用且强大的算法，用于解决现实世界中的逆问题[@problem_id:3451047]。", "problem": "考虑压缩感知（CS）中的鲁棒稀疏恢复任务，其目标是根据测量值 $y \\in \\mathbb{R}^m$ 和传感矩阵 $A \\in \\mathbb{R}^{m \\times n}$，通过求解一个由鲁棒数据保真项和稀疏性诱导正则化组成的目标函数，来估计系数向量 $x \\in \\mathbb{R}^n$。目标是最小化以下函数\n$$\n\\sum_{j=1}^m \\rho\\!\\left( (y - A x)_j \\right) + \\sum_{i=1}^n \\phi\\!\\left( |x_i| \\right),\n$$\n其中残差惩罚 $\\rho$ 和系数惩罚 $\\phi$ 均源于高斯尺度混合（GSMs）。要求是实现一个迭代重加权最小二乘（IRLS）算法，该算法利用GSM结构来产生显式的权重更新。\n\n此推导的上下文相关基本依据必须如下：\n- 高斯尺度混合（GSM）表示，其中以一个潜在精度为条件的随机变量是高斯分布的，而该潜在精度具有Gamma先验。具体而言，对于残差和系数，使用如下分层定义的模型\n$$\nr_j \\mid \\tau_j \\sim \\mathcal{N}(0, \\tau_j^{-1}), \\quad \\tau_j \\sim \\mathrm{Gamma}(a_r, b_r),\n$$\n$$\nx_i \\mid \\lambda_i \\sim \\mathcal{N}(0, \\lambda_i^{-1}), \\quad \\lambda_i \\sim \\mathrm{Gamma}(a_x, b_x),\n$$\n其中 $r = y - A x$，Gamma分布使用形状-速率参数化，并且 $(a_r, b_r)$ 和 $(a_x, b_x)$ 是固定的正常数超参数。\n- 从GSM中产生重加权最小二乘的期望最大化或半二次解释。\n\n您必须从上述第一性原理出发，推导与残差和系数相关的IRLS权重更新，并展示IRLS如何导致求解一系列形式如下的加权最小二乘问题\n$$\n\\min_x \\ \\frac{1}{2} \\sum_{j=1}^m w_j \\, \\big( (y - A x)_j \\big)^2 + \\frac{1}{2} \\sum_{i=1}^n v_i \\, x_i^2,\n$$\n其中 $w_j$ 和 $v_i$ 是从GSM结构推导出的函数。您必须提供从GSM推导出的这些权重的显式公式，并实现一个在权重更新和求解相应正规方程之间交替的算法\n$$\n\\left( A^\\top W A + V \\right) x = A^\\top W y,\n$$\n其中 $W = \\mathrm{diag}(w_1, \\ldots, w_m)$ 且 $V = \\mathrm{diag}(v_1, \\ldots, v_n)$。\n\n您实现的程序必须：\n- 对残差使用超参数 $a_r = 2.0, b_r = 1.0$，对系数使用超参数 $a_x = 1.0, b_x = 10^{-2}$。\n- 将 $x$ 初始化为零向量，并进行迭代，直到 $x$ 的相对变化小于 $10^{-6}$ 或达到最大200次迭代。\n- 为评估目标值，您必须使用GSM所蕴含的边缘惩罚，具体到与 $x$ 无关的加性常数，即\n$$\n\\rho(r) = (a_r + \\tfrac{1}{2}) \\, \\log\\!\\left( b_r + \\tfrac{1}{2} r^2 \\right),\n\\quad\n\\phi(|x|) = (a_x + \\tfrac{1}{2}) \\, \\log\\!\\left( b_x + \\tfrac{1}{2} x^2 \\right).\n$$\n\n您的程序必须运行以下定义的四个测试用例，每个用例都使用指定的随机种子以确保可复现性：\n- 测试用例1（具有中等噪声和稀疏真实值的一般恢复）：\n  - 维度：$m = 60, n = 40$。\n  - 随机种子：$0$。\n  - 真实稀疏度：$x$ 中有 $K = 5$ 个非零项。\n  - 传感矩阵 $A$：项从标准正态分布中独立采样，并且列被归一化为单位 $\\ell_2$ 范数。\n  - 真实系数 $x^\\star$：均匀随机选择 $K$ 个索引，其值从 $\\mathcal{N}(0, 1)$ 中采样，其余设置为 $0$。\n  - 噪声标准差：$\\sigma = 0.05$。\n  - 离群值：一部分（$0.1$）的残差项受到标准差为 $1.0$ 的额外噪声的扰动。\n  - 测量向量：$y = A x^\\star + \\epsilon + o$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$，$o$ 是如上所述的离群值向量。\n  - 报告恢复的 $x$ 与 $x^\\star$ 之间的均方误差，结果为浮点数。\n\n- 测试用例2（零测量值和欠定系统的边界情况）：\n  - 维度：$m = 50, n = 80$。\n  - 随机种子：$1$。\n  - 真实系数 $x^\\star$：全为零。\n  - 传感矩阵 $A$：如上生成并对列进行归一化。\n  - 测量向量：$y = 0$。\n  - 报告恢复的 $x$ 的 $\\ell_2$ 范数，结果为浮点数。\n\n- 测试用例3（具有重复列的近奇异设计）：\n  - 维度：$m = 30, n = 30$。\n  - 随机种子：$2$。\n  - 传感矩阵 $A$：如上生成；归一化后将列 $1$ 设置为与列 $0$ 相等以引入共线性。\n  - 真实系数 $x^\\star$：在随机位置有 $5$ 个非零项，其值从 $\\mathcal{N}(0, 1)$ 中采样。\n  - 噪声标准差：$\\sigma = 0.02$。\n  - 测量向量：$y = A x^\\star + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$。\n  - 报告收敛解处的目标函数值 $\\sum_j \\rho((y-Ax)_j) + \\sum_i \\phi(|x_i|)$，结果为浮点数。\n\n- 测试用例4（测量值中存在大量离群值）：\n  - 维度：$m = 80, n = 50$。\n  - 随机种子：$3$。\n  - 真实稀疏度：$K = 8$。\n  - 传感矩阵 $A$：如上生成并进行归一化。\n  - 真实系数 $x^\\star$：$K$ 个非零项从 $\\mathcal{N}(0, 1)$ 中采样。\n  - 噪声标准差：$\\sigma = 0.01$。\n  - 离群值：一部分（$0.3$）的残差项受到标准差为 $5.0$ 的额外噪声的扰动。\n  - 测量向量：$y = A x^\\star + \\epsilon + o$，如上定义。\n  - 报告收敛解处的残差惩罚之和 $\\sum_j \\rho((y-Ax)_j)$，结果为浮点数。\n\n您的程序应生成单行输出，其中包含四个测试用例的结果，格式为方括号括起来的逗号分隔列表，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_k$ 是对应于测试用例 $k$ 的浮点数结果。", "solution": "目标是最小化以下函数：\n$$\n\\mathcal{J}(x) = \\sum_{j=1}^m \\rho\\!\\left( (y - A x)_j \\right) + \\sum_{i=1}^n \\phi\\!\\left( |x_i| \\right)\n$$\n其中惩罚项 $\\rho$ 和 $\\phi$ 是从高斯尺度混合（GSM）模型推导出来的。该问题可以被置于一个概率框架中，以寻找系数向量 $x$ 的最大后验（MAP）估计。GSM框架提供了一个分层贝叶斯模型，这有助于通过期望最大化（EM）解释来推导IRLS算法。\n\n分层模型规定如下：\n1. 残差 $r_j = (y - Ax)_j$ 被建模为条件高斯分布，具有潜在精度变量 $\\tau_j$：\n$$\nr_j \\mid \\tau_j \\sim \\mathcal{N}(0, \\tau_j^{-1})\n$$\n精度 $\\tau_j$ 被赋予一个Gamma先验分布：\n$$\n\\tau_j \\sim \\mathrm{Gamma}(a_r, b_r)\n$$\n其中 $a_r > 0$ 和 $b_r > 0$ 分别是固定的形状和速率超参数。\n\n2. 系数 $x_i$ 被建模为条件高斯分布，具有潜在精度变量 $\\lambda_i$：\n$$\nx_i \\mid \\lambda_i \\sim \\mathcal{N}(0, \\lambda_i^{-1})\n$$\n精度 $\\lambda_i$ 也被赋予一个Gamma先验：\n$$\n\\lambda_i \\sim \\mathrm{Gamma}(a_x, b_x)\n$$\n具有形状 $a_x > 0$ 和速率 $b_x > 0$。\n\nIRLS算法是通过将潜在精度 $\\{\\tau_j\\}_{j=1}^m$ 和 $\\{\\lambda_i\\}_{i=1}^n$ 视作缺失数据，并应用EM算法来寻找 $x$ 的MAP估计而推导出来的。EM算法在期望（E）步和最大化（M）步之间交替进行。\n\n**E步：计算潜在变量的期望**\n在E步中，于第 $k$ 次迭代时，我们计算给定当前参数估计 $x^{(k)}$ 和观测数据 $y$ 的潜在变量的后验分布。由于条件独立性，我们可以分别考虑每个潜在变量。\n\n对于残差精度 $\\tau_j$，其后验由贝叶斯法则给出：\n$$\np(\\tau_j \\mid r_j^{(k)}) \\propto p(r_j^{(k)} \\mid \\tau_j) p(\\tau_j)\n$$\n其中 $r_j^{(k)} = (y - Ax^{(k)})_j$。似然项来自高斯模型，$p(r_j^{(k)} \\mid \\tau_j) \\propto \\tau_j^{1/2} \\exp(-\\frac{1}{2} \\tau_j (r_j^{(k)})^2)$。先验是Gamma分布，$p(\\tau_j) \\propto \\tau_j^{a_r-1} \\exp(-b_r \\tau_j)$。将它们结合起来得到后验核：\n$$\np(\\tau_j \\mid r_j^{(k)}) \\propto \\tau_j^{a_r + 1/2 - 1} \\exp\\left(-\\left(b_r + \\frac{1}{2}(r_j^{(k)})^2\\right)\\tau_j\\right)\n$$\n这是Gamma分布的核，具体为 $\\mathrm{Gamma}\\left(a_r + \\frac{1}{2}, b_r + \\frac{1}{2}(r_j^{(k)})^2\\right)$。\n在此后验分布下 $\\tau_j$ 的期望是：\n$$\nw_j^{(k+1)} \\triangleq E[\\tau_j \\mid r_j^{(k)}] = \\frac{a_r + 1/2}{b_r + \\frac{1}{2}(r_j^{(k)})^2} = \\frac{2 a_r + 1}{2 b_r + (r_j^{(k)})^2}\n$$\n\n类似地，对于系数精度 $\\lambda_i$，其给定 $x_i^{(k)}$ 的后验是：\n$$\np(\\lambda_i \\mid x_i^{(k)}) \\propto p(x_i^{(k)} \\mid \\lambda_i) p(\\lambda_i) \\propto \\lambda_i^{a_x + 1/2 - 1} \\exp\\left(-\\left(b_x + \\frac{1}{2}(x_i^{(k)})^2\\right)\\lambda_i\\right)\n$$\n这对应于一个 $\\mathrm{Gamma}\\left(a_x + \\frac{1}{2}, b_x + \\frac{1}{2}(x_i^{(k)})^2\\right)$ 分布。\n$\\lambda_i$ 的期望是：\n$$\nv_i^{(k+1)} \\triangleq E[\\lambda_i \\mid x_i^{(k)}] = \\frac{a_x + 1/2}{b_x + \\frac{1}{2}(x_i^{(k)})^2} = \\frac{2 a_x + 1}{2 b_x + (x_i^{(k)})^2}\n$$\n这些期望，$w_j^{(k+1)}$ 和 $v_i^{(k+1)}$，将作为重加权最小二乘问题中的权重。\n\n**M步：更新系数向量**\n在M步中，我们通过最小化以先前估计 $x^{(k)}$ 为条件的完整数据负对数后验的期望来更新 $x$ 的估计。这等价于最小化辅助函数 $Q(x | x^{(k)})$：\n$$\nx^{(k+1)} = \\arg \\min_x Q(x \\mid x^{(k)}) = \\arg \\min_x E_{\\tau, \\lambda \\mid y, x^{(k)}}[-\\log p(y, x, \\{\\tau_j\\}, \\{\\lambda_i\\})]\n$$\n负对数联合概率，在不考虑常数项的情况下，与下式成正比：\n$$\n-\\log p(y, x, \\{\\tau_j\\}, \\{\\lambda_i\\}) \\propto \\frac{1}{2}\\sum_{j=1}^m \\tau_j (y_j - (Ax)_j)^2 + \\frac{1}{2}\\sum_{i=1}^n \\lambda_i x_i^2 - \\log p(\\{\\tau_j\\}) - \\log p(\\{\\lambda_i\\})\n$$\n对潜在变量的后验取期望，并只保留与 $x$ 相关的项，我们得到M步的目标：\n$$\n\\arg \\min_x \\left( \\frac{1}{2}\\sum_{j=1}^m E[\\tau_j | r_j^{(k)}] (y_j - (Ax)_j)^2 + \\frac{1}{2}\\sum_{i=1}^n E[\\lambda_i | x_i^{(k)}] x_i^2 \\right)\n$$\n代入在E步中计算的期望 $w_j^{(k+1)}$ 和 $v_i^{(k+1)}$，我们得到加权最小二乘问题：\n$$\nx^{(k+1)} = \\arg \\min_x \\left( \\frac{1}{2}\\sum_{j=1}^m w_j^{(k+1)} (y_j - (Ax)_j)^2 + \\frac{1}{2}\\sum_{i=1}^n v_i^{(k+1)} x_i^2 \\right)\n$$\n这是一个关于 $x$ 的二次函数。为了找到最小值，我们将其关于 $x$ 的梯度设为零：\n$$\n\\nabla_x \\left( \\frac{1}{2} (y-Ax)^\\top W^{(k+1)} (y-Ax) + \\frac{1}{2} x^\\top V^{(k+1)} x \\right) = 0\n$$\n其中 $W^{(k+1)} = \\mathrm{diag}(w_j^{(k+1)})$ 和 $V^{(k+1)} = \\mathrm{diag}(v_i^{(k+1)})$。\n$$\n-A^\\top W^{(k+1)} (y - Ax) + V^{(k+1)} x = 0\n$$\n重新整理这些项，得到正规方程组：\n$$\n(A^\\top W^{(k+1)} A + V^{(k+1)}) x = A^\\top W^{(k+1)} y\n$$\n求解这个关于 $x$ 的线性系统，得到更新后的估计 $x^{(k+1)}$。\n\n**IRLS算法总结**\n完整的IRLS算法如下：\n1. 初始化 $x^{(0)}$（例如，$x^{(0)} = 0$），设置超参数 $a_r, b_r, a_x, b_x$ 和收敛容差 $\\epsilon$。\n2. 对于 $k = 0, 1, 2, \\dots$ 直到收敛：\n    a. **权重更新（E步）**：根据当前估计 $x^{(k)}$ 计算残差并更新权重。\n    $$\n    r^{(k)} = y - Ax^{(k)}\n    $$\n    $$\n    w_j^{(k+1)} = \\frac{2 a_r + 1}{2 b_r + (r_j^{(k)})^2} \\quad \\text{for } j=1, \\dots, m\n    $$\n    $$\n    v_i^{(k+1)} = \\frac{2 a_x + 1}{2 b_x + (x_i^{(k)})^2} \\quad \\text{for } i=1, \\dots, n\n    $$\n    b. **系数更新（M步）**：构建对角矩阵 $W^{(k+1)}$ 和 $V^{(k+1)}$，并为 $x^{(k+1)}$ 求解线性系统：\n    $$\n    x^{(k+1)} = (A^\\top W^{(k+1)} A + V^{(k+1)})^{-1} A^\\top W^{(k+1)} y\n    $$\n    c. **检查收敛性**：如果 $\\frac{\\|x^{(k+1)} - x^{(k)}\\|_2}{\\|x^{(k)}\\|_2 + \\epsilon_{norm}}  \\epsilon_{tol}$ 或达到最大迭代次数，则停止。\n\n所提供的边缘惩罚 $\\rho(r) = (a_r + \\tfrac{1}{2}) \\log(b_r + \\tfrac{1}{2} r^2)$ 和 $\\phi(|x|) = (a_x + \\tfrac{1}{2}) \\log(b_x + \\tfrac{1}{2} x^2)$ 与此分层模型一致，因为它们表示通过对潜在精度变量进行积分得到的负对数边缘似然，且不考虑与 $x$ 无关的常数。实现将遵循此推导的算法。", "answer": "[0.002824647907530686,2.022204558299863e-10,-57.0674512497676,14.73976868958223]", "id": "3451047"}]}