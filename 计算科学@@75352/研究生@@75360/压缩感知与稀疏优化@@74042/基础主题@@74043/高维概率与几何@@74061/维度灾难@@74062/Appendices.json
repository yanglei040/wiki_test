{"hands_on_practices": [{"introduction": "要通过 $\\ell_1$ 范数最小化精确恢复稀疏信号，传感矩阵 $A$ 必须满足所谓的零空间性质 (Null Space Property, NSP)。在深入探讨复杂的统计学理论之前，一个简单的线性代数维数论证就已经为我们揭示了一个严格的限制。这个练习将引导你通过一个显式的构造，展示为何当测量次数过少 (即 $m  2k$) 时，无论采用何种算法，恢复都从根本上是不可能的，从而以一种具体而直观的方式揭示了维度灾难的一种表现形式 ([@problem_id:3486783])。", "problem": "设 $d$、$m$ 和 $k$ 为正整数，满足 $d \\geq 2k$。回顾一下，如果一个矩阵 $A \\in \\mathbb{R}^{m \\times d}$ 满足 $k$ 阶零空间性质 (NSP)，那么对于其零空间中的任意非零向量 $h \\in \\ker(A)$ 以及任意大小为 $|S| \\leq k$ 的索引集 $S \\subset \\{1,2,\\dots,d\\}$，都有 $\\|h_{S}\\|_{1}  \\|h_{S^{c}}\\|_{1}$。NSP 是通过$\\ell_1$-最小化精确恢复所有 $k$-稀疏信号的充要条件。\n\n仅使用基础线性代数和范数不等式，完成以下任务：\n\n1. 对于 $m  2k$ 的情况，构造一个显式的矩阵 $A \\in \\mathbb{R}^{m \\times d}$ 和一个非零向量 $h \\in \\ker(A)$，以及一个大小为 $|S|=k$ 的集合 $S \\subset \\{1,\\dots,d\\}$，使得 $\\|h_{S}\\|_{1} \\geq \\|h_{S^{c}}\\|_{1}$，从而证明 $k$ 阶 NSP 不成立。你的构造应仅依赖于以下事实：$\\mathbb{R}^{m}$ 中任意多于 $m$ 个向量的集合都是线性相关的，以及以下观察：对于一个支撑集最多包含 $2k$ 个索引的向量，其绝对值最大的 $k$ 个分量之和至少是其 $\\ell_{1}$ 范数的一半。从零空间的维度及其与坐标子空间的交集的角度提供几何直观，说明当 $m$ 相对于 $k$ 和 $d$ 过小时，维度灾难的一种表现形式。\n\n2. 根据你的构造和推理，确定最小整数 $m_{\\min}(k)$，使得当 $m  m_{\\min}(k)$ 时，对于任何满足 $d \\geq 2k$ 的矩阵 $A \\in \\mathbb{R}^{m \\times d}$，$k$ 阶 NSP 必定不成立，而当 $m \\geq m_{\\min}(k)$ 时，这种仅由维度和线性相关性计数所导致的不可避免的障碍不再存在。以关于 $k$ 的封闭形式表达式给出你的最终答案。不要包含任何单位。如果你选择为特定的 $k$ 提供一个数值，请不要四舍五入；然而，最终答案要求是关于 $k$ 的通用封闭形式表达式。", "solution": "`...`", "answer": "$$\n\\boxed{2k}\n$$", "id": "3486783"}, {"introduction": "上一个练习建立了一个确定性的下界 $m \\ge 2k$。然而，对于典型的随机矩阵，其性能表现出统计性为特征，真实所需测量数由一个更精细的量——在给定稀疏信号处的 $\\ell_1$ 范数下降锥的“统计维度”——所决定。这个练习将带你深入该理论的核心，通过渐近分析推导出这个统计维度的著名标度律，即它如何以 $2k\\ln(n/k)$ 的形式依赖于信号维度 $n$ 和稀疏度 $k$，从而从数学上量化了维度灾难在高维极限下的影响 ([@problem_id:3486664])。", "problem": "考虑一个固定的 $k$-稀疏向量 $x_{0} \\in \\mathbb{R}^{n}$，其支撑集为 $S \\subset \\{1,\\dots,n\\}$ 且 $|S|=k$，并考虑线性测量 $y = A x_{0}$，其中感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的元素是独立同分布 (i.i.d.) 的标准正态随机变量。令 $\\|\\cdot\\|_{1}$ 表示 $\\mathbb{R}^{n}$ 上的 $\\ell_{1}$ 范数。将 $\\|\\cdot\\|_{1}$ 在 $x_{0}$ 处的下降锥定义为\n$$\n\\mathcal{D} := \\left\\{ d \\in \\mathbb{R}^{n} : \\exists\\, t > 0 \\text{ such that } \\|x_{0} + t d\\|_{1} \\le \\|x_{0}\\|_{1} \\right\\}.\n$$\n将闭凸锥 $\\mathcal{C} \\subset \\mathbb{R}^{n}$ 的统计维度定义为\n$$\n\\delta(\\mathcal{C}) := \\mathbb{E}\\left[\\|\\Pi_{\\mathcal{C}}(g)\\|_{2}^{2}\\right],\n$$\n其中 $\\Pi_{\\mathcal{C}}$ 是到 $\\mathcal{C}$ 上的欧几里得投影，$g \\sim \\mathcal{N}(0, I_{n})$ 是 $\\mathbb{R}^{n}$ 中的一个标准正态向量。锥积分几何中一个经过充分检验的事实是，对于通过 $\\ell_{1}$ 最小化进行的凸恢复，存在一个尖锐的成功/失败阈值，出现在 $m \\approx \\delta(\\mathcal{D})$ 附近。\n\n从基本原理和核心定义出发，执行以下步骤：\n\n1. 用次微分 $\\partial \\|\\cdot\\|_{1}(x_{0})$ 表示 $\\|\\cdot\\|_{1}$ 在 $x_{0}$ 处的法锥 $\\mathcal{N}$，并利用锥与其极锥之间的关系，将 $\\delta(\\mathcal{D})$ 改写为标准正态向量到 $\\partial \\|\\cdot\\|_{1}(x_{0})$ 的一个缩放版本的期望平方距离。\n\n2. 仅使用标准正态分布的基本性质，推导出 $\\delta(\\mathcal{D})$ 的一个单参数变分表示，形式为\n$$\n\\delta(\\mathcal{D}) = \\inf_{\\tau \\ge 0} \\Psi_{n,k}(\\tau),\n$$\n其中 $\\Psi_{n,k}(\\tau)$ 是一个包含标准正态概率密度函数和尾分布的显式表达式。\n\n3. 分析渐近区域 $n \\to \\infty$，$k \\to \\infty$，且 $k/n \\to 0$。使用高斯尾部的拉普拉斯型渐近，确定 $\\delta(\\mathcal{D})$ 作为 $n$ 和 $k$ 的显式函数的主阶行为。\n\n你的最终答案必须是 $\\delta(\\mathcal{D})$ 在指定区域下的主阶渐近项，以一个关于 $n$ 和 $k$ 的单一闭式解析表达式给出。不要包含低阶项。不需要四舍五入。所有对数都使用自然对数 $\\ln(\\cdot)$ 表示。", "solution": "该问题要求解与 $k$-稀疏向量 $x_0 \\in \\mathbb{R}^n$ 处的 $\\ell_1$-范数相关的下降锥 $\\mathcal{D}$ 的统计维度 $\\delta(\\mathcal{D})$ 的主阶渐近行为。\n\n### 第1部分：将统计维度与法锥关联起来\n\n令 $f(x) = \\|x\\|_1$。$f$ 在 $x_0$ 处的下降锥由方向导数为非正的方向 $d$ 的集合给出：\n$$\n\\mathcal{D} = \\{d \\in \\mathbb{R}^n : f'(x_0; d) \\le 0\\}\n$$\n凸函数的方向导数是其 次微分的支撑函数：$f'(x_0; d) = \\max_{z \\in \\partial f(x_0)} \\langle z, d \\rangle$。因此，\n$$\n\\mathcal{D} = \\{d \\in \\mathbb{R}^n : \\langle z, d \\rangle \\le 0 \\text{ for all } z \\in \\partial f(x_0)\\}\n$$\n令 $K = \\partial \\|x_0\\|_1$ 为 $\\ell_1$-范数在 $x_0$ 处的次微分。在 $x_0$ 处对次水平集 $\\{x : \\|x\\|_1 \\le \\|x_0\\|_1\\}$ 的法锥是该次微分的锥包，即 $\\mathcal{N} = \\text{cone}(K)$。\n根据上面的定义，集合 $\\mathcal{D}$ 是集合 $K$ 的极锥。由于 $K$ 本身不一定是锥，我们考虑它的锥包 $\\mathcal{N}$。$\\mathcal{N}$ 的极锥是：\n$$\n\\mathcal{N}^* = \\{ d \\in \\mathbb{R}^n : \\langle v, d \\rangle \\le 0 \\text{ for all } v \\in \\mathcal{N} \\}\n$$\n由于任何 $v \\in \\mathcal{N}$ 都可以写成 $v = \\lambda z$ 的形式，其中 $\\lambda \\ge 0$ 且 $z \\in K$，所以对所有 $v \\in \\mathcal{N}$ 都有 $\\langle v, d \\rangle \\le 0$ 的条件等价于对所有 $z \\in K$ 都有 $\\langle z, d \\rangle \\le 0$。因此，$\\mathcal{D} = \\mathcal{N}^*$。\n\n一个关键的恒等式将闭凸锥 $\\mathcal{C}$ 的统计维度与其极锥 $\\mathcal{C}^*$ 联系起来：\n$$\n\\delta(\\mathcal{C}) = \\mathbb{E}\\left[ \\text{dist}(g, \\mathcal{C}^*)^2 \\right]\n$$\n其中 $g \\sim \\mathcal{N}(0, I_n)$ 且 $\\text{dist}(g, A) = \\inf_{a \\in A} \\|g - a\\|_2$ 是点 $g$ 到集合 $A$ 的欧几里得距离。这个恒等式源于 Moreau 分解定理，$g = \\Pi_{\\mathcal{C}}(g) + \\Pi_{\\mathcal{C}^*}(g)$，它意味着 $\\|\\Pi_{\\mathcal{C}}(g)\\|_2 = \\|g - \\Pi_{\\mathcal{C}^*}(g)\\|_2 = \\text{dist}(g, \\mathcal{C}^*)$。\n\n将此恒等式应用于我们的下降锥 $\\mathcal{D}$，我们利用 $\\mathcal{D}^* = (\\mathcal{N}^*)^* = \\mathcal{N}$ 这一事实，因为 $\\mathcal{N}$ 是一个闭凸锥。\n$$\n\\delta(\\mathcal{D}) = \\mathbb{E}\\left[ \\text{dist}(g, \\mathcal{D}^*)^2 \\right] = \\mathbb{E}\\left[ \\text{dist}(g, \\mathcal{N})^2 \\right]\n$$\n法锥是 $\\mathcal{N} = \\text{cone}(K) = \\{\\lambda z : \\lambda \\ge 0, z \\in K\\}$。$g$ 到 $\\mathcal{N}$ 的平方距离是\n$$\n\\text{dist}(g, \\mathcal{N})^2 = \\min_{v \\in \\mathcal{N}} \\|g - v\\|_2^2 = \\min_{\\lambda \\ge 0, z \\in K} \\|g - \\lambda z\\|_2^2 = \\min_{\\lambda \\ge 0} \\min_{z \\in K} \\|g - \\lambda z\\|_2^2 = \\min_{\\lambda \\ge 0} \\text{dist}(g, \\lambda K)^2\n$$\n这里，$\\lambda K$ 是 $\\partial \\|x_0\\|_1 = K$ 的一个“缩放版本”。因此，我们已经将 $\\delta(\\mathcal{D})$ 表示为一个涉及到一个标准正态向量到次微分的缩放版本的距离的期望。\n\n### 第2部分：变分表示\n\n来自高斯过程理论的一个强有力的结果（Gordon 不等式或相关原理）允许在这种情况下交换期望和最小化。\n$$\n\\delta(\\mathcal{D}) = \\mathbb{E}\\left[ \\min_{\\tau \\ge 0} \\text{dist}(g, \\tau K)^2 \\right] = \\min_{\\tau \\ge 0} \\mathbb{E}\\left[ \\text{dist}(g, \\tau K)^2 \\right]\n$$\n我们定义 $\\Psi_{n,k}(\\tau) := \\mathbb{E}\\left[\\text{dist}(g, \\tau K)^2\\right]$ 并为其找到一个显式表达式。\n次微分 $K = \\partial \\|x_0\\|_1$ 由 $K = \\{ z \\in \\mathbb{R}^n : z_S = s, \\|z_{S^c}\\|_\\infty \\le 1 \\}$ 给出，其中 $S$ 是 $x_0$ 的支撑集，大小为 $|S|=k$，$S^c$ 是其补集，$s = \\text{sign}((x_0)_S)$。\n平方距离是\n$$\n\\text{dist}(g, \\tau K)^2 = \\min_{z \\in K} \\|g - \\tau z\\|_2^2 = \\|g_S - \\tau s\\|_2^2 + \\min_{\\|z_{S^c}\\|_\\infty \\le 1} \\|g_{S^c} - \\tau z_{S^c}\\|_2^2\n$$\n对 $z_{S^c}$ 的最小化可以按坐标分离。对于每个 $i \\in S^c$，我们在 $|z_i| \\le 1$ 的约束下最小化 $(g_i - \\tau z_i)^2$。这等价于在 $w_i \\in [-\\tau, \\tau]$ 的约束下最小化 $(g_i - w_i)^2$。最小值通过将 $g_i$ 投影到区间 $[-\\tau, \\tau]$ 上来达到，其平方距离为 $\\text{dist}(g_i, [-\\tau, \\tau])^2 = (\\max(0, |g_i|-\\tau))^2 = (|g_i|-\\tau)_+^2$。\n因此，$\\text{dist}(g, \\tau K)^2 = \\|g_S - \\tau s\\|_2^2 + \\sum_{i \\in S^c} (|g_i|-\\tau)_+^2$。\n\n我们现在对 $g \\sim \\mathcal{N}(0, I_n)$ 取期望。\n第一项：$\\mathbb{E}[\\|g_S - \\tau s\\|_2^2] = \\sum_{i \\in S} \\mathbb{E}[(g_i - \\tau s_i)^2]$。由于 $s_i^2 = 1$ 且 $g_i \\sim \\mathcal{N}(0,1)$ 是独立的，其中 $\\mathbb{E}[g_i]=0, \\mathbb{E}[g_i^2]=1$，我们有 $\\mathbb{E}[(g_i - \\tau s_i)^2] = \\mathbb{E}[g_i^2] - 2\\tau s_i \\mathbb{E}[g_i] + \\tau^2 s_i^2 = 1 - 0 + \\tau^2 = 1+\\tau^2$。对 $S$ 中的 $k$ 个指标求和得到 $k(1+\\tau^2)$。\n\n第二项：$\\sum_{i \\in S^c} \\mathbb{E}[(|g_i|-\\tau)_+^2] = (n-k)\\mathbb{E}[(|Z|-\\tau)_+^2]$，其中 $Z \\sim \\mathcal{N}(0,1)$。\n令 $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$ 为标准正态PDF，$Q(\\tau) = \\int_\\tau^\\infty \\phi(z)dz$ 为尾概率。\n$$\n\\mathbb{E}[(|Z|-\\tau)_+^2] = \\int_{-\\infty}^\\infty (\\max(0, |z|-\\tau))^2 \\phi(z)dz = 2 \\int_\\tau^\\infty (z-\\tau)^2 \\phi(z)dz\n$$\n展开平方并积分（使用恒等式 $\\int_\\tau^\\infty z\\phi(z)dz = \\phi(\\tau)$ 和 $\\int_\\tau^\\infty z^2\\phi(z)dz = \\tau\\phi(\\tau)+Q(\\tau)$）得到：\n$$\n\\mathbb{E}[(|Z|-\\tau)_+^2] = 2 \\left[ (\\tau\\phi(\\tau)+Q(\\tau)) - 2\\tau(\\phi(\\tau)) + \\tau^2 Q(\\tau) \\right] = 2(1+\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau)\n$$\n合并各项，我们得到变分形式：\n$$\n\\Psi_{n,k}(\\tau) = k(1+\\tau^2) + (n-k) \\left[ 2(1+\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau) \\right]\n$$\n其中 $\\delta(\\mathcal{D}) = \\inf_{\\tau \\ge 0} \\Psi_{n,k}(\\tau)$。\n\n### 第3部分：渐近分析\n\n为了找到下确界，我们将 $\\Psi_{n,k}(\\tau)$ 对 $\\tau$ 求导并令其为零。令 $\\tau^*$ 为最小值点。\n$$\n\\frac{d\\Psi_{n,k}}{d\\tau} = 2k\\tau + (n-k) \\left[ 4\\tau Q(\\tau) - 4\\phi(\\tau) \\right] = 0\n$$\n这给出了 $\\tau^* > 0$ 的一阶条件 (FOC)：\n$$\nk\\tau^* = 2(n-k) \\left[ \\phi(\\tau^*) - \\tau^*Q(\\tau^*) \\right]\n$$\n我们将此条件代回到 $\\Psi_{n,k}(\\tau^*)$ 的表达式中以进行简化。\n$$\n\\delta(\\mathcal{D}) = \\Psi_{n,k}(\\tau^*) = k(1+\\tau^{*2}) + (n-k)\\left[ 2(1+\\tau^{*2})Q(\\tau^*) - 2\\tau^*\\phi(\\tau^*) \\right]\n$$\n从一阶条件，我们可以用 $k$ 和 $\\tau^*$ 表示 $(n-k)$：$(n-k) = \\frac{k\\tau^*}{2(\\phi(\\tau^*)-\\tau^*Q(\\tau^*))}$。代入此式：\n\\begin{align*}\n\\Psi_{n,k}(\\tau^*) = k(1+\\tau^{*2}) + \\frac{k\\tau^*}{2(\\phi-\\tau^*Q)} \\left[ 2(1+\\tau^{*2})Q - 2\\tau^*\\phi \\right] \\\\\n= k(1+\\tau^{*2}) - \\frac{k\\tau^*}{(\\phi-\\tau^*Q)} \\left[ \\tau^*\\phi - (1+\\tau^{*2})Q \\right] \\\\\n= k \\left[ \\frac{(1+\\tau^{*2})(\\phi-\\tau^*Q) - \\tau^{*2}\\phi + \\tau^*(1+\\tau^{*2})Q}{\\phi-\\tau^*Q} \\right] \\\\\n= k \\left[ \\frac{\\phi + \\tau^{*2}\\phi - \\tau^*Q - \\tau^{*3}Q - \\tau^{*2}\\phi + \\tau^*Q + \\tau^{*3}Q}{\\phi-\\tau^*Q} \\right] \\\\\n= k \\frac{\\phi(\\tau^*)}{\\phi(\\tau^*) - \\tau^*Q(\\tau^*)}\n\\end{align*}\n在 $n\\to\\infty, k\\to\\infty, k/n\\to 0$ 的区域中，比率 $(n-k)/k$ 很大，从一阶条件可以推断出 $\\tau^*$ 必定很大。对于大的 $\\tau$，我们使用 Mills 比率的渐近展开：$Q(\\tau) \\sim \\phi(\\tau)(\\frac{1}{\\tau} - \\frac{1}{\\tau^3} + \\dots)$。\n这得到 $\\phi(\\tau) - \\tau Q(\\tau) \\sim \\phi(\\tau) - \\tau \\phi(\\tau)(\\frac{1}{\\tau} - \\frac{1}{\\tau^3}) = \\frac{\\phi(\\tau)}{\\tau^2}$。\n将此主阶行为代入 $\\delta(\\mathcal{D})$ 的简化表达式中：\n$$\n\\delta(\\mathcal{D}) \\approx k \\frac{\\phi(\\tau^*)}{\\phi(\\tau^*)/\\tau^{*2}} = k\\tau^{*2}\n$$\n现在我们来求 $\\tau^{*2}$ 的主阶行为。我们在 FOC 中使用渐近形式：\n$$\nk\\tau^* \\approx 2(n-k) \\frac{\\phi(\\tau^*)}{\\tau^{*2}} = 2(n-k) \\frac{1}{\\tau^{*2}\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\tau^{*2}}{2}\\right)\n$$\n对两边取自然对数：\n$$\n\\ln(k) + 3\\ln(\\tau^*) \\approx \\ln(2(n-k)) - \\frac{\\tau^{*2}}{2} - \\frac{1}{2}\\ln(2\\pi)\n$$\n重新整理得到 $\\tau^{*2}$：\n$$\n\\frac{\\tau^{*2}}{2} + 3\\ln(\\tau^*) \\approx \\ln\\left(\\frac{n-k}{k}\\right) + \\text{const}\n$$\n在极限 $n/k \\to \\infty$ 下，$\\ln(n/k)$ 很大。对于大的 $\\tau^*$，$\\tau^{*2}/2$ 项在左侧占主导地位。主阶平衡是：\n$$\n\\frac{\\tau^{*2}}{2} \\approx \\ln\\left(\\frac{n}{k}\\right) \\implies \\tau^{*2} \\approx 2\\ln\\left(\\frac{n}{k}\\right)\n$$\n将此与 $\\delta(\\mathcal{D}) \\approx k\\tau^{*2}$ 结合，我们得到主阶渐近行为：\n$$\n\\delta(\\mathcal{D}) \\approx k \\left( 2\\ln\\left(\\frac{n}{k}\\right) \\right) = 2k\\ln\\left(\\frac{n}{k}\\right)\n$$\n该表达式代表了下降锥统计维度的渐近展开中的主项。", "answer": "$$\n\\boxed{2k\\ln\\left(\\frac{n}{k}\\right)}\n$$", "id": "3486664"}, {"introduction": "在理解了维度灾难的基本限制及标度律之后，我们转向一个更具建设性的问题：如何主动地减弱其影响？答案在于利用信号的先验结构，即所谓的基于模型的压缩感知。这个计算练习将量化这种“结构之福”，通过数值比较非结构化稀疏模型和树状结构稀疏模型所需的测量次数，清晰地展示利用信号结构如何能显著降低采样复杂度，从而将一个深刻的理论优势变得具体可触 ([@problem_id:3486799])。", "problem": "要求您形式化阐述，基于模型的树状结构稀疏性如何通过与非结构化稀疏性相比，减少均匀恢复所需的测量次数，从而在压缩感知中缓解维度灾难。在一个环境维度为 $n$ 的空间中，处理 $k$-稀疏信号，并考虑随机亚高斯测量矩阵。您必须使用的基本理论是子空间联合视角以及针对低维集合的亚高斯嵌入的标准测度集中现象：要嵌入一个失真度至多为 $ \\delta \\in (0,1) $ 的单一 $k$ 维子空间，需要的测量次数约为 $m = \\mathcal{O}( \\delta^{-2} k )$；要均匀嵌入 $L$ 个此类子空间的联合，一个标准的网格与联合界论证会将所需测量次数增加一个约为 $ \\log L $ 的项。该原理被广泛接受，并应作为出发点。\n\n基于此，您必须为两种模型推导、论证并实现显式的测量复杂度估计器：\n\n- 非结构化 $k$-稀疏性：该模型是 $ \\mathbb{R}^n $ 中由单位矩阵的任意 $k$ 个列向量张成的所有坐标子空间的联合。其模型基数为 $ L_{\\mathrm{un}} = \\binom{n}{k} $。\n\n- 基于有根 $d$ 元树的树状结构 $k$-稀疏性：一个支撑集是在一个具有 $n$ 个节点的大型完全有根 $d$ 元树中嵌入的任意大小为 $k$ 的连通有根子树。此类支撑集数量的一个上界可通过计算形状和嵌入方式的数量来获得。一个经过充分检验的组合学事实是，具有 $k$ 个节点的有根有序 $d$ 元树的形状数量等于\n$$\nT_d(k) = \\frac{1}{((d-1)k+1)} \\binom{d k}{k}.\n$$\n一个保守且标准的放置界限是，嵌入的支撑集数量不超过 $ n \\cdot T_d(k) $，当然也不能超过 $ \\binom{n}{k} $。因此，结构化模型的基数可以由下式界定\n$$\nL_{\\mathrm{str}} \\leq \\min\\!\\left\\{ n \\cdot T_d(k), \\binom{n}{k} \\right\\}.\n$$\n\n使用子空间联合嵌入原理以及上述基数，推导出形式如下的估计器\n$$\nm \\approx \\left\\lceil c_0 \\, \\delta^{-2} \\, \\big( k + \\log L \\big) \\right\\rceil,\n$$\n其中 $c_0 > 0$ 是一个固定的绝对常数，$\\log$ 是自然对数，$L$ 是模型基数。对于非结构化模型，此公式应用 $L = L_{\\mathrm{un}}$；对于树状结构模型，应用 $L = L_{\\mathrm{str}}$。您必须完全按照所写实现此估计器，使用自然对数，并在需要时通过伽马函数的对数以数值稳定的方式计算 $ \\log \\binom{a}{b} $ 和 $ \\log T_d(k) $。您可以假设 $n$ 相对于 $k$ 足够大，以使嵌入计数界限有意义。\n\n您的程序必须为下面测试套件中的每组参数计算整数对 $[m_{\\mathrm{un}}, m_{\\mathrm{str}}]$，其中 $m_{\\mathrm{un}}$ 是非结构化估计值，$m_{\\mathrm{str}}$ 是结构化估计值。所有对数必须是自然对数，最终答案是通过应用向上取整函数得到的整数。不涉及物理单位。不使用角度。不使用百分比。\n\n测试套件（每个元组为 $ (n,k,d,\\delta,c_0) $）：\n- 情况 $1$（理想情况，大环境维度）：$(n,k,d,\\delta,c_0) = (16384, 64, 2, 0.25, 2.0)$。\n- 情况 $2$（更高分支因子）：$(n,k,d,\\delta,c_0) = (4096, 32, 4, 0.25, 2.0)$。\n- 情况 $3$（边界情况 $k=1$）：$(n,k,d,\\delta,c_0) = (2048, 1, 2, 0.20, 2.0)$。\n- 情况 $4$（中等大小和分支）：$(n,k,d,\\delta,c_0) = (8192, 16, 3, 0.30, 2.0)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个情况的结果，形式为逗号分隔的列表的列表，每个内部列表为 $[ m_{\\mathrm{un}}, m_{\\mathrm{str}} ]$。例如，输出应具有以下形式\n\"[ [m_un_case1,m_str_case1],[m_un_case2,m_str_case2],[m_un_case3,m_str_case3],[m_un_case4,m_str_case4] ]\"\n不移除空格或添加额外文本。使用您的实现所计算出的精确整数值。", "solution": "首先对问题陈述进行严格的验证过程。\n\n### 步骤 1：提取给定信息\n提供的显式数据、变量和条件如下：\n- **环境维度**：$n$\n- **稀疏度**：$k$\n- **测量矩阵类型**：随机亚高斯\n- **理论框架**：压缩感知的子空间联合视角。\n- **基本原理**：均匀嵌入 $L$ 个 $k$ 维子空间的联合需要 $m = \\mathcal{O}(\\delta^{-2}(k + \\log L))$ 次测量，失真度为 $\\delta \\in (0,1)$。\n- **估计器公式**：$m \\approx \\left\\lceil c_0 \\, \\delta^{-2} \\, \\big( k + \\log L \\big) \\right\\rceil$，其中 $\\log$ 是自然对数，$c_0 > 0$ 是一个常数。\n- **非结构化稀疏性模型**：所有维度为 $k$ 的坐标子空间的联合。\n- **非结构化模型的基数**：$L_{\\mathrm{un}} = \\binom{n}{k}$。\n- **树状结构稀疏性模型**：一个支撑集是在一个具有 $n$ 个节点的完全有根 $d$ 元树中的一个大小为 $k$ 的连通有根子树。\n- **树形状的数量**：$T_d(k) = \\frac{1}{((d-1)k+1)} \\binom{d k}{k}$。\n- **结构化模型的基数界**：$L_{\\mathrm{str}} \\leq \\min\\!\\left\\{ n \\cdot T_d(k), \\binom{n}{k} \\right\\}$。该界限将用于估计器中的基数 $L$。\n- **数值计算**：$\\log \\binom{a}{b}$ 和 $\\log T_d(k)$ 将使用伽马函数的对数进行计算。\n- **任务**：为每个测试用例计算整数对 $[ m_{\\mathrm{un}}, m_{\\mathrm{str}} ]$。\n- **测试套件**：\n    - 情况 1：$(n,k,d,\\delta,c_0) = (16384, 64, 2, 0.25, 2.0)$\n    - 情况 2：$(n,k,d,\\delta,c_0) = (4096, 32, 4, 0.25, 2.0)$\n    - 情况 3：$(n,k,d,\\delta,c_0) = (2048, 1, 2, 0.20, 2.0)$\n    - 情况 4：$(n,k,d,\\delta,c_0) = (8192, 16, 3, 0.30, 2.0)$\n\n### 步骤 2：使用提取的给定信息进行验证\n根据既定标准评估问题的有效性。\n- **科学依据**：该问题牢固地植根于压缩感知和稀疏信号恢复理论。子空间联合模型、亚高斯随机矩阵的使用，以及由此产生的形式为 $m \\propto k + \\log L$ 的测量界限，都是该领域的标准且成熟的结果。模型基数的组合公式是正确的。\n- **适定性**：问题被完全指定。它为估计器 $m$ 提供了显式的数学公式，为模型基数 $L_{\\mathrm{un}}$ 和 $L_{\\mathrm{str}}$ 提供了精确的定义，为每个测试用例提供了所有必要的参数，并为数值计算提供了清晰的说明。每个情况都存在一个唯一的、确定性的解。\n- **客观性**：该问题以精确的数学和算法术语陈述，没有任何主观语言或意见。\n\n该问题不存在任何无效性缺陷。它并非科学上不成立、不可形式化、不完整、不切实际、不适定、微不足道或无法验证。问题的核心是应用已建立的理论估计器来量化基于模型的压缩感知中的一个基本概念：通过利用信号结构实现的采样复杂度降低。所有测试用例都满足 $n$ 相对于 $k$ 足够大的假设，确保了模型比较的有意义性。\n\n### 步骤 3：结论与行动\n问题有效。将制定一个完整的解决方案。\n\n### 解决方案推导\n目标是形式化并计算在两种不同稀疏性模型（非结构化和树状结构）下均匀信号恢复所需的测量次数。分析基于所提供的测量次数估计器 $m$：\n$$\nm \\approx \\left\\lceil c_0 \\, \\delta^{-2} \\, \\big( k + \\log L \\big) \\right\\rceil\n$$\n在这里，$k$ 代表信号子空间的内在维度，而 $\\log L$ 则说明了可能子空间族群的复杂性或大小。更复杂的模型（更大的 $L$）需要更多的测量来区分其构成的子空间。\n\n**1. 非结构化稀疏性模型**\n在标准的非结构化稀疏性模型中，从 $n$ 个可能的索引中任选 $k$ 个索引的子集都可以构成信号的支撑集。这对应于一个信号模型，该模型是所有 $k$ 维坐标子空间的联合。这类子空间的数量，即模型基数，由二项式系数给出：\n$$\nL_{\\mathrm{un}} = \\binom{n}{k}\n$$\n因此，所需测量次数 $m_{\\mathrm{un}}$ 通过将 $L = L_{\\mathrm{un}}$ 代入通用公式来估计：\n$$\nm_{\\mathrm{un}} = \\left\\lceil c_0 \\, \\delta^{-2} \\, \\left( k + \\log \\binom{n}{k} \\right) \\right\\rceil\n$$\n为了在 $n$ 和 $k$ 很大时保持数值稳定性，项 $\\log \\binom{n}{k}$ 不通过直接计算二项式系数来获得。而是利用对数的性质和伽马函数 $\\Gamma(z)$（其中 $\\Gamma(z+1)=z!$）来计算：\n$$\n\\log \\binom{n}{k} = \\log\\left(\\frac{n!}{k!(n-k)!}\\right) = \\log(\\Gamma(n+1)) - \\log(\\Gamma(k+1)) - \\log(\\Gamma(n-k+1))\n$$\n这在数值上使用对数伽马函数（通常表示为 `gammaln`）来实现。\n\n**2. 树状结构稀疏性模型**\n在此模型中，信号的支撑集索引被约束，必须在施加于 $n$ 个索引之上的一个更大的、预定义的 $d$ 元树结构内形成一个大小为 $k$ 的连通有根子树。与非结构化模型相比，这种结构性约束显著减少了允许的支撑集数量。\n问题为模型基数 $L_{\\mathrm{str}}$ 提供了一个上界，我们被指示在计算中使用该上界：\n$$\nL_{\\mathrm{str}} = \\min\\!\\left\\{ n \\cdot T_d(k), \\binom{n}{k} \\right\\}\n$$\n项 $T_d(k)$ 是具有 $k$ 个节点的有根有序 $d$ 元树的可能形状数量：\n$$\nT_d(k) = \\frac{1}{(d-1)k+1} \\binom{dk}{k}\n$$\n$n \\cdot T_d(k)$ 中的因子 $n$ 是将这些树形状“放置”或嵌入到更大的 $n$ 节点环境树中的方式数量的一个保守界限。与 $\\binom{n}{k}$ 取最小值确保了基数不会超过可能的 $k$-子集的总数。\n\n所需测量次数 $m_{\\mathrm{str}}$ 通过代入 $L=L_{\\mathrm{str}}$ 来估计：\n$$\nm_{\\mathrm{str}} = \\left\\lceil c_0 \\, \\delta^{-2} \\, \\left( k + \\log L_{\\mathrm{str}} \\right) \\right\\rceil\n$$\n对数项 $\\log L_{\\mathrm{str}}$ 计算如下：\n$$\n\\log L_{\\mathrm{str}} = \\min\\left( \\log n + \\log T_d(k), \\log \\binom{n}{k} \\right)\n$$\n其中 $\\log T_d(k)$ 以稳定方式计算如下：\n$$\n\\log T_d(k) = \\log\\binom{dk}{k} - \\log\\big((d-1)k+1\\big)\n$$\n而 $\\log\\binom{dk}{k}$ 同样使用对数伽马函数计算。\n\n$m_{\\mathrm{un}}$ 和 $m_{\\mathrm{str}}$ 的比较展示了“结构的优越性”。由于树状结构模型施加了强约束，对于非平凡的 $k$，$L_{\\mathrm{str}}$ 通常远小于 $L_{\\mathrm{un}}$，尤其是在 $n$ 很大时。这导致 $\\log L$ 项变得小得多，从而显著减少了所需的测量次数 $m$，缓解了部分由非结构化项 $\\binom{n}{k}$ 中可能性的组合爆炸所引起的维度灾难。实现将首先为 $\\log \\binom{n}{k}$ 和 $\\log T_d(k)$ 定义稳定函数，然后用它们计算 $L_{\\mathrm{un}}$ 和 $L_{\\mathrm{str}}$，最后为每个测试用例计算测量估计值 $m_{\\mathrm{un}}$ 和 $m_{\\mathrm{str}}$。", "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Computes and prints the estimated number of measurements for unstructured\n    and tree-structured sparsity models based on the problem specification.\n    \"\"\"\n\n    test_cases = [\n        # (n, k, d, delta, c0)\n        (16384, 64, 2, 0.25, 2.0),\n        (4096, 32, 4, 0.25, 2.0),\n        (2048, 1, 2, 0.20, 2.0),\n        (8192, 16, 3, 0.30, 2.0),\n    ]\n\n    def log_binom(n, k):\n        \"\"\"\n        Computes log(n choose k) using the log-gamma function for numerical stability.\n        Handles edge cases k=0 or k=n. If k  n or k  0, returns -inf.\n        \"\"\"\n        if k  0 or k > n:\n            return -np.inf\n        if k == 0 or k == n:\n            return 0\n        # For symmetry and to potentially use smaller numbers if k  n/2\n        if k > n / 2:\n            k = n - k\n        return gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)\n\n    def log_T_d(k, d):\n        \"\"\"\n        Computes log of the number of rooted ordered d-ary tree shapes on k nodes.\n        T_d(k) = (1 / ((d-1)k + 1)) * (dk choose k)\n        \"\"\"\n        if k == 0:\n            return 0  # There is one tree of size 0 (the empty tree)\n        if k == 1:\n            return 0  # One tree of size 1 (a single node), log(1) = 0\n            \n        log_binom_term = log_binom(d * k, k)\n        denominator_term = np.log((d - 1) * k + 1)\n        return log_binom_term - denominator_term\n\n    results = []\n    for case in test_cases:\n        n, k, d, delta, c0 = case\n\n        # Pre-compute the common factor\n        pre_factor = c0 / (delta**2)\n\n        # --- Unstructured Model Calculation ---\n        # L_un = binom(n, k)\n        log_L_un = log_binom(n, k)\n        \n        # m_un = ceil(c0 * delta^-2 * (k + log(L_un)))\n        m_un = int(np.ceil(pre_factor * (k + log_L_un)))\n\n        # --- Tree-Structured Model Calculation ---\n        # L_str = min(n * T_d(k), binom(n, k))\n        # log(L_str) = min(log(n) + log(T_d(k)), log(binom(n,k)))\n        if k == 0:\n             # A k=0 sparse signal is the zero vector, requiring 0 measurements.\n             # In the context of embedding a 0-dim subspace, k=0 also works.\n             log_L_str = -np.inf # Effectively makes m_str zero.\n        else:\n             log_Td = log_T_d(k, d)\n             log_L_str_candidate = np.log(n) + log_Td\n             log_L_str = min(log_L_str_candidate, log_L_un)\n\n        # m_str = ceil(c0 * delta^-2 * (k + log(L_str)))\n        if k == 0:\n            m_str = 0\n        else:\n            m_str = int(np.ceil(pre_factor * (k + log_L_str)))\n            \n        results.append([m_un, m_str])\n\n    # Format the output exactly as specified\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3486799"}]}