{"hands_on_practices": [{"introduction": "本练习旨在巩固您对吉洪诺夫正则化背后基本线性代数的理解。通过在对角化基中推导正规方程并求解估计量，您将精确地看到正则化器如何在谱分量的层面上与数据保真项相互作用。这种动手计算阐明了正则化参数 $\\lambda$ 作为谱滤波器的作用。[@problem_id:3490519]", "problem": "考虑一个压缩感知中的线性逆问题，其中一个未知信号向量 $x \\in \\mathbb{R}^{p}$ 通过一个测量矩阵 $X \\in \\mathbb{R}^{m \\times p}$ 从测量值 $y \\in \\mathbb{R}^{m}$ 中被估计出来。估计值 $x_{\\lambda}$ 通过最小化一个 Tikhonov 正则化最小二乘目标函数得到\n$$\nJ(x) = \\frac{1}{2} \\|X x - y\\|_{2}^{2} + \\frac{\\lambda}{2} \\|L x\\|_{2}^{2},\n$$\n其中 $L \\in \\mathbb{R}^{p \\times p}$ 是一个固定的线性正则化算子，$\\lambda  0$ 是正则化参数。假设 $X^{\\top} X$ 和 $L^{\\top} L$ 是对称半正定的，可以交换，并且可以被一个正交矩阵 $Q \\in \\mathbb{R}^{p \\times p}$ 同时对角化。设其对角化形式为\n$$\nQ^{\\top}(X^{\\top} X)Q = \\operatorname{diag}(a_{1}, a_{2}, \\dots, a_{p}), \\quad Q^{\\top}(L^{\\top} L)Q = \\operatorname{diag}(b_{1}, b_{2}, \\dots, b_{p}),\n$$\n并定义 $w := Q^{\\top} X^{\\top} y \\in \\mathbb{R}^{p}$。\n\n从基本原理出发，推导最小化子 $x_{\\lambda}$ 的正规方程，并利用同时对角化得到 $x_{\\lambda}$ 在 $X^{\\top} X$ 和 $L^{\\top} L$ 的特征基下的闭式表示。然后，对于 $p=4$ 的具体情况，给定特征值和变换后的数据\n$$\n(a_{1}, a_{2}, a_{3}, a_{4}) = (9, 4, 1, 0), \\quad (b_{1}, b_{2}, b_{3}, b_{4}) = (1, 4, 9, 16), \\quad (w_{1}, w_{2}, w_{3}, w_{4}) = (3, 2, 1, 0),\n$$\n以及正则化水平 $\\lambda = 2$，计算欧几里得范数平方 $\\|x_{\\lambda}\\|_{2}^{2}$ 的精确值，并表示为单个闭式表达式。无需四舍五入。", "solution": "目标是找到 Tikhonov 正则化最小二乘代价函数的最小化子 $x_{\\lambda}$：\n$$\nJ(x) = \\frac{1}{2} \\|X x - y\\|_{2}^{2} + \\frac{\\lambda}{2} \\|L x\\|_{2}^{2}\n$$\n为了找到最小化子，我们必须计算 $J(x)$ 关于 $x$ 的梯度并将其设为零。\n\n首先，我们展开目标函数中的各项。欧几里得范数的平方定义为 $\\|v\\|_2^2 = v^{\\top}v$。\n第一项是最小二乘数据保真项：\n$$\n\\frac{1}{2} \\|X x - y\\|_{2}^{2} = \\frac{1}{2} (X x - y)^{\\top}(X x - y) = \\frac{1}{2} (x^{\\top}X^{\\top} - y^{\\top})(X x - y) = \\frac{1}{2} (x^{\\top}X^{\\top}X x - x^{\\top}X^{\\top}y - y^{\\top}X x + y^{\\top}y)\n$$\n由于 $y^{\\top}X x$ 是一个标量，它等于其转置 $(y^{\\top}X x)^{\\top} = x^{\\top}X^{\\top}y$。因此，我们可以将此项写为：\n$$\n\\frac{1}{2} (x^{\\top}X^{\\top}X x - 2 x^{\\top}X^{\\top}y + y^{\\top}y)\n$$\n第二项是正则化项：\n$$\n\\frac{\\lambda}{2} \\|L x\\|_{2}^{2} = \\frac{\\lambda}{2} (L x)^{\\top}(L x) = \\frac{\\lambda}{2} x^{\\top}L^{\\top}L x\n$$\n将它们结合起来，完整的目标函数是：\n$$\nJ(x) = \\frac{1}{2} x^{\\top}X^{\\top}X x - x^{\\top}X^{\\top}y + \\frac{1}{2} y^{\\top}y + \\frac{\\lambda}{2} x^{\\top}L^{\\top}L x\n$$\n现在，我们计算梯度 $\\nabla_x J(x)$。对于对称矩阵 $A$，使用标准矩阵微积分恒等式 $\\nabla_x (x^{\\top}Ax) = 2Ax$ 和 $\\nabla_x (b^{\\top}x) = b$，我们有：\n$$\n\\nabla_x J(x) = \\frac{1}{2} (2 X^{\\top}X x) - X^{\\top}y + 0 + \\frac{\\lambda}{2} (2 L^{\\top}L x)\n$$\n$$\n\\nabla_x J(x) = X^{\\top}X x - X^{\\top}y + \\lambda L^{\\top}L x\n$$\n将梯度设为零，$\\nabla_x J(x) = 0$，得到最小化子 $x_{\\lambda}$ 的正规方程：\n$$\nX^{\\top}X x_{\\lambda} + \\lambda L^{\\top}L x_{\\lambda} = X^{\\top}y\n$$\n$$\n(X^{\\top}X + \\lambda L^{\\top}L) x_{\\lambda} = X^{\\top}y\n$$\n这是正规方程的一般形式。\n\n接下来，我们使用给定的同时对角化来求解 $x_{\\lambda}$。给定一个正交矩阵 $Q$，使得 $Q^{\\top}Q = QQ^{\\top} = I$ 并且：\n$$\nQ^{\\top}(X^{\\top} X)Q = A = \\operatorname{diag}(a_{1}, a_{2}, \\dots, a_{p})\n$$\n$$\nQ^{\\top}(L^{\\top} L)Q = B = \\operatorname{diag}(b_{1}, b_{2}, \\dots, b_{p})\n$$\n由此，我们可以将 $X^{\\top}X$ 和 $L^{\\top}L$ 表示为 $X^{\\top}X = QAQ^{\\top}$ 和 $L^{\\top}L = QBQ^{\\top}$。将这些代入正规方程：\n$$\n(QAQ^{\\top} + \\lambda QBQ^{\\top}) x_{\\lambda} = X^{\\top}y\n$$\n$$\nQ(A + \\lambda B)Q^{\\top} x_{\\lambda} = X^{\\top}y\n$$\n两边同时左乘 $Q^{\\top}$：\n$$\nQ^{\\top}Q(A + \\lambda B)Q^{\\top} x_{\\lambda} = Q^{\\top}X^{\\top}y\n$$\n$$\n(A + \\lambda B)Q^{\\top} x_{\\lambda} = Q^{\\top}X^{\\top}y\n$$\n让我们将在特征基中的解向量定义为 $\\tilde{x}_{\\lambda} = Q^{\\top}x_{\\lambda}$，并使用给定的定义 $w = Q^{\\top}X^{\\top}y$。方程变为：\n$$\n(A + \\lambda B)\\tilde{x}_{\\lambda} = w\n$$\n由于 $A$ 和 $B$ 是对角矩阵，它们的和也是一个对角矩阵：\n$$\nA + \\lambda B = \\operatorname{diag}(a_1 + \\lambda b_1, a_2 + \\lambda b_2, \\dots, a_p + \\lambda b_p)\n$$\n该方程组解耦为 $p$ 个独立的标量方程：\n$$\n(a_i + \\lambda b_i) (\\tilde{x}_{\\lambda})_i = w_i \\quad \\text{对于 } i = 1, \\dots, p\n$$\n因此，解在特征基中的分量是：\n$$\n(\\tilde{x}_{\\lambda})_i = \\frac{w_i}{a_i + \\lambda b_i}\n$$\n问题要求解的欧几里得范数平方 $\\|x_{\\lambda}\\|_{2}^{2}$。由于 $Q$ 是一个正交矩阵，它保持欧几里得范数不变。我们可以如下证明：\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = x_{\\lambda}^{\\top}x_{\\lambda} = (Q\\tilde{x}_{\\lambda})^{\\top}(Q\\tilde{x}_{\\lambda}) = \\tilde{x}_{\\lambda}^{\\top}Q^{\\top}Q\\tilde{x}_{\\lambda} = \\tilde{x}_{\\lambda}^{\\top}I\\tilde{x}_{\\lambda} = \\tilde{x}_{\\lambda}^{\\top}\\tilde{x}_{\\lambda} = \\|\\tilde{x}_{\\lambda}\\|_{2}^{2}\n$$\n因此，我们可以通过对 $\\tilde{x}_{\\lambda}$ 的分量的平方求和来计算范数：\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\sum_{i=1}^{p} ((\\tilde{x}_{\\lambda})_i)^2 = \\sum_{i=1}^{p} \\left( \\frac{w_i}{a_i + \\lambda b_i} \\right)^2\n$$\n现在，我们代入问题中给出的具体值：$p=4$, $\\lambda=2$, $(a_1, a_2, a_3, a_4) = (9, 4, 1, 0)$, $(b_1, b_2, b_3, b_4) = (1, 4, 9, 16)$ 以及 $(w_1, w_2, w_3, w_4) = (3, 2, 1, 0)$。\n\n我们计算 $\\tilde{x}_{\\lambda}$ 的分量：\n当 $i=1$ 时：\n$$\n(\\tilde{x}_{\\lambda})_1 = \\frac{w_1}{a_1 + \\lambda b_1} = \\frac{3}{9 + 2(1)} = \\frac{3}{11}\n$$\n当 $i=2$ 时：\n$$\n(\\tilde{x}_{\\lambda})_2 = \\frac{w_2}{a_2 + \\lambda b_2} = \\frac{2}{4 + 2(4)} = \\frac{2}{12} = \\frac{1}{6}\n$$\n当 $i=3$ 时：\n$$\n(\\tilde{x}_{\\lambda})_3 = \\frac{w_3}{a_3 + \\lambda b_3} = \\frac{1}{1 + 2(9)} = \\frac{1}{19}\n$$\n当 $i=4$ 时：\n$$\n(\\tilde{x}_{\\lambda})_4 = \\frac{w_4}{a_4 + \\lambda b_4} = \\frac{0}{0 + 2(16)} = \\frac{0}{32} = 0\n$$\n现在，我们计算范数的平方 $\\|x_{\\lambda}\\|_{2}^{2} = \\|\\tilde{x}_{\\lambda}\\|_{2}^{2}$：\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\left(\\frac{3}{11}\\right)^2 + \\left(\\frac{1}{6}\\right)^2 + \\left(\\frac{1}{19}\\right)^2 + 0^2\n$$\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\frac{9}{121} + \\frac{1}{36} + \\frac{1}{361}\n$$\n为了对这些分数求和，我们找到一个公分母。分母分别是 $121=11^2$，$36=6^2$ 和 $361=19^2$。由于 $11$，$6$ 和 $19$ 两两互质，最小公分母是它们的乘积：$121 \\times 36 \\times 361 = 4356 \\times 361 = 1572516$。\n我们将每个分数转换为这个公分母：\n$$\n\\frac{9}{121} = \\frac{9 \\times 36 \\times 361}{121 \\times 36 \\times 361} = \\frac{9 \\times 12996}{1572516} = \\frac{116964}{1572516}\n$$\n$$\n\\frac{1}{36} = \\frac{1 \\times 121 \\times 361}{36 \\times 121 \\times 361} = \\frac{43681}{1572516}\n$$\n$$\n\\frac{1}{361} = \\frac{1 \\times 121 \\times 36}{361 \\times 121 \\times 36} = \\frac{4356}{1572516}\n$$\n现在我们对分子求和：\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\frac{116964 + 43681 + 4356}{1572516} = \\frac{165001}{1572516}\n$$\n分母的质因数是 $2$, $3$, $11$, 和 $19$。经检查，分子 $165001$ 不能被这些质数中的任何一个整除，因此该分数是最简形式。", "answer": "$$\\boxed{\\frac{165001}{1572516}}$$", "id": "3490519"}, {"introduction": "从纯理论走向实际应用，本练习对于理解如何使用和评估正则化模型至关重要。您将推导并实现关键的诊断工具，包括帽子矩阵、有效自由度以及交叉验证的高效计算公式。这项综合性练习将抽象的代数解与模型选择和性能评估的具体任务联系起来。[@problem_id:3490562]", "problem": "给定一个线性逆问题，需要通过 Tikhonov 正则化，从测量值 $ y \\in \\mathbb{R}^n $ 和设计矩阵 $ X \\in \\mathbb{R}^{n \\times p} $ 来估计一个系数向量 $ \\beta \\in \\mathbb{R}^p $。考虑罚最小二乘估计量，其定义为以下泛函的最小化子：\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\| y - X \\beta \\|_2^2 + \\lambda \\, \\| L \\beta \\|_2^2,\n$$\n其中 $ \\lambda \\ge 0 $ 是一个正则化参数，$ L \\in \\mathbb{R}^{m \\times p} $ 是一个给定的线性算子。从线性代数和优化的基础出发，包括无约束最小二乘的正规方程组、正交投影和线性平滑器的性质，以及 Sherman–Morrison–Woodbury 恒等式。不要假设帽子矩阵、有效自由度或交叉验证捷径的任何预先推导好的公式。\n\n任务：\n1. 通过将梯度设为零并求解得到的正规方程组，推导估计量 $ \\hat{\\beta}_\\lambda $。然后，将拟合响应 $ \\hat{y}_\\lambda = X \\hat{\\beta}_\\lambda $ 表示为 $ \\hat{y}_\\lambda = S_\\lambda \\, y $ 的形式，并用 $ X $、$ L $ 和 $ \\lambda $ 明确地表示出线性平滑器（帽子矩阵）$ S_\\lambda \\in \\mathbb{R}^{n \\times n} $。\n2. 使用线性平滑器的有效自由度定义 $ \\mathrm{df}_\\lambda = \\mathrm{trace}\\!\\left( \\frac{\\partial \\hat{y}_\\lambda}{\\partial y} \\right) $，从 $ S_\\lambda $ 推导出一个可计算的 $ \\mathrm{df}_\\lambda $ 表达式。从第一性原理出发证明所有步骤。\n3. 对于留一交叉验证 (LOOCV)，对每个索引 $ i \\in \\{1,\\dots,n\\} $，定义留一拟合值 $ \\hat{y}^{(-i)}_{\\lambda, i} $，该值是通过在移除了第 $ i $ 个观测值的数据集上重新拟合模型得到的。推导留一残差 $e_i^{\\mathrm{LOO}} = y_i - \\hat{y}^{(-i)}_{\\lambda, i}$ 的公式，用全样本残差和 $ S_\\lambda $ 的对角线元素表示。利用此公式得到 LOOCV 均方误差 $\\mathrm{LOOCV}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n (e_i^{\\mathrm{LOO}})^2$ 的闭式表达式，而无需显式地重新拟合 $ n $ 次。\n4. 通过将 LOOCV 中的逐点杠杆值替换为从 $ \\mathrm{df}_\\lambda $ 导出的平均值，来定义广义交叉验证 (GCV) 均方误差 $\\mathrm{GCV}_\\lambda$。推导 $\\mathrm{GCV}_\\lambda$ 的一个可计算表达式。\n\n然后实现一个程序，对以下测试套件，为每个案例计算三元组 $[ \\mathrm{df}_\\lambda, \\mathrm{LOOCV}_\\lambda, \\mathrm{GCV}_\\lambda ]$，并打印一行包含这些三元组列表的输出。所有浮点数输出必须精确到 $ 8 $ 位小数，使用标准“四舍五入到最近的偶数”规则。输出格式必须是单行，包含一个用方括号括起来的逗号分隔的结果列表，其中每个结果本身也是一个列表，例如 $ [[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots] $。\n\n测试套件：\n- 案例 A（正常情况，岭回归）：$ n = 5 $，$ p = 3 $，其中\n$$\nX^{(A)} = \\begin{bmatrix}\n1  -1  2 \\\\\n0  2  -1 \\\\\n3  0  1 \\\\\n-2  1  0 \\\\\n1  1  1\n\\end{bmatrix}, \\quad\ny^{(A)} = \\begin{bmatrix}\n1 \\\\ -1 \\\\ 3 \\\\ 0 \\\\ 2\n\\end{bmatrix}, \\quad\nL^{(A)} = I_3, \\quad\n\\lambda^{(A)} = 0.5.\n$$\n- 案例 B（边界情况，普通最小二乘）：与 $ X^{(A)} $、$ y^{(A)} $ 和 $ L^{(A)} $ 相同，但\n$$\n\\lambda^{(B)} = 0.\n$$\n- 案例 C（边缘情况，极端正则化）：与 $ X^{(A)} $、$ y^{(A)} $ 和 $ L^{(A)} $ 相同，但\n$$\n\\lambda^{(C)} = 10^6.\n$$\n- 案例 D（使用一阶差分算子的一般 Tikhonov）：与 $ X^{(A)} $ 和 $ y^{(A)} $ 相同，但使用一阶差分算子\n$$\nL^{(D)} = \\begin{bmatrix}\n-1  1  0 \\\\\n0  -1  1\n\\end{bmatrix}, \\quad\n\\lambda^{(D)} = 1.\n$$\n- 案例 E（欠定系统，岭稳定化）：$ n = 3 $，$ p = 5 $，其中\n$$\nX^{(E)} = \\begin{bmatrix}\n1  0  -1  2  0 \\\\\n0  1  1  -1  2 \\\\\n2  -1  0  1  1\n\\end{bmatrix}, \\quad\ny^{(E)} = \\begin{bmatrix}\n1 \\\\ 0 \\\\ -1\n\\end{bmatrix}, \\quad\nL^{(E)} = I_5, \\quad\n\\lambda^{(E)} = 0.3.\n$$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表，每个条目是对应测试案例的列表 $ [ \\mathrm{df}_\\lambda, \\mathrm{LOOCV}_\\lambda, \\mathrm{GCV}_\\lambda ] $，顺序为 A、B、C、D、E。每个实数必须精确到 $ 8 $ 位小数。本问题不涉及单位，也不使用角度。", "solution": "任务是推导和计算与 Tikhonov 正则化相关的关键量。目标是找到最小化罚最小二乘泛函的估计量 $ \\beta \\in \\mathbb{R}^p $：\n$$\nJ(\\beta) = \\| y - X \\beta \\|_2^2 + \\lambda \\| L \\beta \\|_2^2\n$$\n其中 $ y \\in \\mathbb{R}^n $ 是响应向量，$ X \\in \\mathbb{R}^{n \\times p} $ 是设计矩阵，$ L \\in \\mathbb{R}^{m \\times p} $ 是惩罚算子，$ \\lambda \\ge 0 $ 是正则化参数。\n\n**1. 估计量和平滑矩阵的推导**\n目标函数可以用矩阵-向量乘积写成：\n$$\nJ(\\beta) = (y - X \\beta)^T (y - X \\beta) + \\lambda (L \\beta)^T (L \\beta)\n$$\n展开各项，我们得到：\n$$\nJ(\\beta) = (y^T - \\beta^T X^T)(y - X \\beta) + \\lambda \\beta^T L^T L \\beta\n$$\n$$\nJ(\\beta) = y^T y - y^T X \\beta - \\beta^T X^T y + \\beta^T X^T X \\beta + \\lambda \\beta^T L^T L \\beta\n$$\n由于 $ \\beta^T X^T y $ 是一个标量，它等于其转置 $ y^T X \\beta $。因此，我们可以合并交叉项：\n$$\nJ(\\beta) = y^T y - 2 \\beta^T X^T y + \\beta^T (X^T X + \\lambda L^T L) \\beta\n$$\n为了找到最小化子 $ \\hat{\\beta}_\\lambda $，我们计算 $ J(\\beta) $ 关于 $ \\beta $ 的梯度，并将其设为零向量。使用标准矩阵微积分法则（$ \\frac{\\partial b^T A c}{\\partial c} = A^T b $ 和 $ \\frac{\\partial c^T A c}{\\partial c} = (A+A^T)c $），并注意到 $ X^T X + \\lambda L^T L $ 是对称的，我们得到：\n$$\n\\nabla_\\beta J(\\beta) = -2 X^T y + 2 (X^T X + \\lambda L^T L) \\beta\n$$\n将梯度设为零，得到罚问题的正规方程组：\n$$\n(X^T X + \\lambda L^T L) \\hat{\\beta}_\\lambda = X^T y\n$$\n如果 $ \\lambda  0 $ 且 $ X $ 和 $ L $ 的零空间仅在原点相交（即 $ \\mathrm{ker}(X) \\cap \\mathrm{ker}(L) = \\{0\\} $），或者如果 $ \\lambda = 0 $ 且 $ X^T X $ 可逆（即 $ X $ 具有满列秩），则矩阵 $ M = X^T X + \\lambda L^T L $ 是可逆的。假设 $ M $ 可逆，我们可以求解估计量 $ \\hat{\\beta}_\\lambda $：\n$$\n\\hat{\\beta}_\\lambda = (X^T X + \\lambda L^T L)^{-1} X^T y\n$$\n拟合响应向量 $ \\hat{y}_\\lambda $ 是通过将设计矩阵应用于估计出的系数得到的：\n$$\n\\hat{y}_\\lambda = X \\hat{\\beta}_\\lambda = X (X^T X + \\lambda L^T L)^{-1} X^T y\n$$\n根据定义，拟合响应是观测响应的线性变换，$ \\hat{y}_\\lambda = S_\\lambda y $。通过比较这两个表达式，我们可以确定线性平滑器，或称帽子矩阵，$ S_\\lambda $：\n$$\nS_\\lambda = X (X^T X + \\lambda L^T L)^{-1} X^T\n$$\n$ S_\\lambda $ 是一个 $ n \\times n $ 矩阵，它将观测数据 $ y $ 映射到拟合值 $ \\hat{y}_\\lambda $。\n\n**2. 有效自由度的推导**\n有效自由度 $ \\mathrm{df}_\\lambda $ 定义为拟合值关于观测值的雅可比矩阵的迹：\n$$\n\\mathrm{df}_\\lambda = \\mathrm{trace}\\!\\left( \\frac{\\partial \\hat{y}_\\lambda}{\\partial y} \\right)\n$$\n我们有线性关系 $ \\hat{y}_\\lambda = S_\\lambda y $。让我们按分量写出。$ \\hat{y}_\\lambda $ 的第 $ i $ 个元素由下式给出：\n$$\n\\hat{y}_{\\lambda, i} = \\sum_{j=1}^n (S_\\lambda)_{ij} y_j\n$$\n第 $ i $ 个拟合值关于第 $ j $ 个观测值的偏导数是：\n$$\n\\frac{\\partial \\hat{y}_{\\lambda, i}}{\\partial y_j} = (S_\\lambda)_{ij}\n$$\n这恰好是矩阵 $ S_\\lambda $ 的第 $ i $ 行和第 $ j $ 列的元素。因此，雅可比矩阵就是平滑矩阵本身：\n$$\n\\frac{\\partial \\hat{y}_\\lambda}{\\partial y} = S_\\lambda\n$$\n将此代入有效自由度的定义，我们得到可计算的表达式：\n$$\n\\mathrm{df}_\\lambda = \\mathrm{trace}(S_\\lambda)\n$$\n\n**3. 留一交叉验证 (LOOCV) 公式的推导**\n对于每个观测值 $ i \\in \\{1,\\dots,n\\} $，我们定义一个在除第 $ i $ 对 $(x_i^T, y_i)$ 之外的所有数据上训练的留一 (LOO) 模型。令 $ X^{(-i)} $ 为移除第 $ i $ 行 $ x_i^T $ 后的矩阵 $ X $，令 $ y^{(-i)} $ 为移除第 $ i $ 个元素 $ y_i $ 后的向量 $ y $。LOO 估计量 $ \\hat{\\beta}_\\lambda^{(-i)} $ 是：\n$$\n\\hat{\\beta}_\\lambda^{(-i)} = \\left( (X^{(-i)})^T X^{(-i)} + \\lambda L^T L \\right)^{-1} (X^{(-i)})^T y^{(-i)}\n$$\n涉及简化数据集的项可以表示为对全数据集矩阵的秩-1 更新：\n$$\n(X^{(-i)})^T X^{(-i)} = \\sum_{j \\neq i} x_j x_j^T = X^T X - x_i x_i^T\n$$\n$$\n(X^{(-i)})^T y^{(-i)} = \\sum_{j \\neq i} x_j y_j = X^T y - x_i y_i\n$$\n令 $ A = X^T X + \\lambda L^T L $。将这些代入 LOO 估计量的表达式中得到：\n$$\n\\hat{\\beta}_\\lambda^{(-i)} = (A - x_i x_i^T)^{-1} (X^T y - x_i y_i)\n$$\n我们使用 Sherman-Morrison 公式来计算秩-1 更新的逆：$(B - uv^T)^{-1} = B^{-1} + \\frac{B^{-1}uv^TB^{-1}}{1 - v^TB^{-1}u}$。令 $ B=A $ 且 $ u=v=x_i $，我们有：\n$$\n(A - x_i x_i^T)^{-1} = A^{-1} + \\frac{A^{-1} x_i x_i^T A^{-1}}{1 - x_i^T A^{-1} x_i}\n$$\n第 $ i $ 个观测值的 LOO 预测值为 $ \\hat{y}_{\\lambda, i}^{(-i)} = x_i^T \\hat{\\beta}_\\lambda^{(-i)} $。代入上式：\n$$\n\\hat{y}_{\\lambda, i}^{(-i)} = x_i^T \\left( A^{-1} + \\frac{A^{-1} x_i x_i^T A^{-1}}{1 - x_i^T A^{-1} x_i} \\right) (X^T y - x_i y_i)\n$$\n让我们识别两个关键量。首先，在 $ i $ 处的全数据拟合值：$ \\hat{y}_{\\lambda, i} = x_i^T \\hat{\\beta}_\\lambda = x_i^T A^{-1} X^T y $。其次，平滑矩阵 $ S_\\lambda = X A^{-1} X^T $ 的第 $ i $ 个对角元素：$ S_{\\lambda, ii} = x_i^T A^{-1} x_i $。\n展开 $ \\hat{y}_{\\lambda, i}^{(-i)} $ 的表达式：\n$$\n\\hat{y}_{\\lambda, i}^{(-i)} = \\left( x_i^T A^{-1} + \\frac{(x_i^T A^{-1} x_i) x_i^T A^{-1}}{1 - S_{\\lambda, ii}} \\right) (X^T y - x_i y_i)\n$$\n$$\n= \\left( 1 + \\frac{S_{\\lambda, ii}}{1 - S_{\\lambda, ii}} \\right) x_i^T A^{-1} (X^T y - x_i y_i) = \\frac{1}{1 - S_{\\lambda, ii}} (x_i^T A^{-1} X^T y - y_i x_i^T A^{-1} x_i)\n$$\n$$\n\\hat{y}_{\\lambda, i}^{(-i)} = \\frac{\\hat{y}_{\\lambda, i} - y_i S_{\\lambda, ii}}{1 - S_{\\lambda, ii}}\n$$\n现在我们计算 LOO 残差 $ e_i^{\\mathrm{LOO}} = y_i - \\hat{y}_{\\lambda, i}^{(-i)} $：\n$$\ne_i^{\\mathrm{LOO}} = y_i - \\frac{\\hat{y}_{\\lambda, i} - y_i S_{\\lambda, ii}}{1 - S_{\\lambda, ii}} = \\frac{y_i(1 - S_{\\lambda, ii}) - (\\hat{y}_{\\lambda, i} - y_i S_{\\lambda, ii})}{1 - S_{\\lambda, ii}}\n$$\n$$\ne_i^{\\mathrm{LOO}} = \\frac{y_i - y_i S_{\\lambda, ii} - \\hat{y}_{\\lambda, i} + y_i S_{\\lambda, ii}}{1 - S_{\\lambda, ii}} = \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - S_{\\lambda, ii}}\n$$\n这个关键结果表明，LOO 残差可以从普通的全样本残差 $ e_i = y_i - \\hat{y}_{\\lambda, i} $ 和帽子矩阵 $ S_\\lambda $ 相应的对角元素计算得出，而无需重新拟合模型。\n那么 LOOCV 均方误差为：\n$$\n\\mathrm{LOOCV}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n (e_i^{\\mathrm{LOO}})^2 = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - S_{\\lambda, ii}} \\right)^2\n$$\n\n**4. 广义交叉验证 (GCV) 公式的推导**\n广义交叉验证通过将分母中每个单独的杠杆得分 $ S_{\\lambda, ii} $ 替换为其平均值来近似 LOOCV。平均杠杆值为：\n$$\n\\bar{S}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n S_{\\lambda, ii} = \\frac{1}{n} \\mathrm{trace}(S_\\lambda)\n$$\n从第 2 部分我们知道 $ \\mathrm{trace}(S_\\lambda) = \\mathrm{df}_\\lambda $。因此，平均杠杆值就是 $ \\frac{\\mathrm{df}_\\lambda}{n} $。将这个平均值代入 LOOCV 公式中的每个 $ S_{\\lambda, ii} $，得到 GCV 统计量：\n$$\n\\mathrm{GCV}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - \\mathrm{trace}(S_\\lambda)/n} \\right)^2 = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - \\mathrm{df}_\\lambda/n} \\right)^2\n$$\n分母相对于求和索引 $ i $ 是常数，因此可以提取出来：\n$$\n\\mathrm{GCV}_\\lambda = \\frac{1}{(1 - \\mathrm{df}_\\lambda/n)^2} \\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_{\\lambda, i})^2 \\right)\n$$\n注意到括号中的项是训练均方误差，$ \\mathrm{MSE}_\\lambda = \\frac{1}{n} \\|y - \\hat{y}_\\lambda\\|_2^2 $，我们得到了 GCV 的标准可计算表达式：\n$$\n\\mathrm{GCV}_\\lambda = \\frac{\\mathrm{MSE}_\\lambda}{\\left(1 - \\mathrm{df}_\\lambda/n\\right)^2} = \\frac{\\frac{1}{n} \\|y - \\hat{y}_\\lambda\\|_2^2}{\\left(1 - \\frac{\\mathrm{df}_\\lambda}{n}\\right)^2}\n$$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes effective degrees of freedom, LOOCV score, and GCV score\n    for Tikhonov regularization for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    X_A = np.array([\n        [1, -1, 2],\n        [0, 2, -1],\n        [3, 0, 1],\n        [-2, 1, 0],\n        [1, 1, 1]\n    ])\n    y_A = np.array([[1], [-1], [3], [0], [2]])\n    L_A = np.identity(3)\n    lambda_A = 0.5\n    \n    lambda_B = 0.0\n    \n    lambda_C = 1e6\n    \n    L_D = np.array([\n        [-1, 1, 0],\n        [0, -1, 1]\n    ])\n    lambda_D = 1.0\n    \n    X_E = np.array([\n        [1, 0, -1, 2, 0],\n        [0, 1, 1, -1, 2],\n        [2, -1, 0, 1, 1]\n    ])\n    y_E = np.array([[1], [0], [-1]])\n    L_E = np.identity(5)\n    lambda_E = 0.3\n\n    test_cases = [\n        # Case A\n        {\"X\": X_A, \"y\": y_A, \"L\": L_A, \"lam\": lambda_A},\n        # Case B\n        {\"X\": X_A, \"y\": y_A, \"L\": L_A, \"lam\": lambda_B},\n        # Case C\n        {\"X\": X_A, \"y\": y_A, \"L\": L_A, \"lam\": lambda_C},\n        # Case D\n        {\"X\": X_A, \"y\": y_A, \"L\": L_D, \"lam\": lambda_D},\n        # Case E\n        {\"X\": X_E, \"y\": y_E, \"L\": L_E, \"lam\": lambda_E},\n    ]\n\n    results = []\n    for case in test_cases:\n        X, y, L, lam = case[\"X\"], case[\"y\"], case[\"L\"], case[\"lam\"]\n        n, p = X.shape\n\n        # 1. Compute the smoother matrix S_lambda\n        XTX = X.T @ X\n        # L may not be square, so L.T @ L is required\n        LTL = L.T @ L\n        A = XTX + lam * LTL\n        \n        A_inv = np.linalg.inv(A)\n        \n        S_lambda = X @ A_inv @ X.T\n        \n        # 2. Compute effective degrees of freedom df_lambda\n        df_lambda = np.trace(S_lambda)\n       \n        # 3. Compute LOOCV_lambda\n        y_hat = S_lambda @ y\n        residuals = y - y_hat\n        S_ii = np.diag(S_lambda)\n        \n        # Reshape S_ii to (n,1) for element-wise division with residuals (n,1)\n        loo_residuals = residuals / (1 - S_ii.reshape(-1, 1))\n        \n        loocv_lambda = np.mean(loo_residuals**2)\n        \n        # 4. Compute GCV_lambda\n        mse = np.mean(residuals**2)\n        \n        denominator = 1 - df_lambda / n\n        if np.isclose(denominator, 0):\n             # Handle potential division by zero if df_lambda is very close to n\n             gcv_lambda = np.inf\n        else:\n             gcv_lambda = mse / (denominator**2)\n\n        # Round results to 8 decimal places (ties to even) and append\n        # np.round uses \"round half to even\", which is the specified method.\n        results.append([\n            np.round(df_lambda, 8),\n            np.round(loocv_lambda, 8),\n            np.round(gcv_lambda, 8)\n        ])\n\n    # Format the output string to match the required format\n    output_parts = []\n    for res in results:\n        # Format each rounded number to a string with 8 decimal places\n        formatted_res = [f\"{num:.8f}\" for num in res]\n        output_parts.append(f\"[{','.join(formatted_res)}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3490562"}, {"introduction": "$\\ell_2$ 正则化与 $\\ell_1$ 正则化之间有何根本区别？本练习深入探讨了区分岭回归解与Lasso解的深层数学特性。通过分析它们各自解路径的平滑性和可微性，您将领会为何岭回归产生平滑的系数轨迹而Lasso能执行变量选择，并理解这对计算算法的深远影响。[@problem_id:3490569]", "problem": "考虑一个线性逆问题，其设计矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，数据向量为 $b \\in \\mathbb{R}^m$。对于参数 $\\lambda  0$，将 Tikhonov (岭) 估计量 $\\beta_{\\mathrm{ridge}}(\\lambda,b)$ 定义为目标函数 $J_{\\mathrm{ridge}}(\\beta;\\lambda,b) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\tfrac{\\lambda}{2}\\|\\beta\\|_2^2$ 的任意最小化子，将最小绝对值收缩和选择算子 (lasso) 估计量 $\\beta_{\\mathrm{lasso}}(\\lambda,b)$ 定义为 $J_{\\mathrm{lasso}}(\\beta;\\lambda,b) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\lambda\\|\\beta\\|_1$ 的任意最小化子。始终假设 $\\lambda  0$。当需要 lasso 解的唯一性时，假设 $A$ 满足一个通有性条件（例如，列处于一般位置），从而对于几乎所有的 $b$，在每个固定的 $\\lambda  0$ 下都存在唯一的 lasso 解。\n\n根据凸分析和可微性的基本原理，判断关于这些估计量对 $b$ 和 $\\lambda$ 的依赖关系的陈述中，哪些是必然为真的。选择所有适用的选项。\n\nA. 对于每个固定的 $A$ 和每个 $\\lambda  0$，映射 $(b,\\lambda) \\mapsto \\beta_{\\mathrm{ridge}}(\\lambda,b)$ 在任何 $(b,\\lambda)$ 的邻域内都是实解析的，因此平滑地依赖于 $b$ 和 $\\lambda$。\n\nB. 如果 $A$ 是满列秩的，那么对于每个 $\\lambda  0$，映射 $(b,\\lambda) \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ 在 $(b,\\lambda)$ 上处处连续可微。\n\nC. 在所述的唯一性/通有性条件下，对于任何固定的 $b$，lasso 正则化路径 $\\lambda \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ 在任何紧区间上都是关于 $\\lambda$ 的分段仿射函数，有有限个断点，在这些断点处激活集会发生变化；在这些断点处，路径通常是不可微的。\n\nD. 诸如最小角回归 (LARS) 和同伦方法等正则化路径算法依赖于岭回归的分段线性特性，以有限多个线性段计算整个岭回归路径。\n\nE. 对于岭回归，其一阶最优性系统可以写成一个关于 $(\\beta,b,\\lambda)$ 的平滑方程，并且对于每个 $\\lambda  0$，其关于 $\\beta$ 的雅可比矩阵是可逆的，因此根据隐函数定理，解局部平滑地依赖于 $(b,\\lambda)$；特别地，可以在 $\\lambda$ 上进行延拓，并且在每个 $\\lambda  0$ 处都有一个良定义的切线方向。\n\n选择正确的选项。", "solution": "我们将基于岭回归和 lasso 估计量各自的一阶最优性条件来分析它们的性质。\n\n**岭回归分析**\n岭回归的目标函数是 $J_{\\mathrm{ridge}}(\\beta) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\tfrac{\\lambda}{2}\\|\\beta\\|_2^2$。由于 $\\lambda  0$，该函数在 $\\beta$ 上是严格凸且连续可微的 ($C^\\infty$)。存在唯一的最小化子，可通过将关于 $\\beta$ 的梯度设为零来找到：\n$$ \\nabla_\\beta J_{\\mathrm{ridge}} = A^T(A\\beta - b) + \\lambda\\beta = 0 $$\n重新整理可得线性系统：\n$$ (A^T A + \\lambda I) \\beta = A^T b $$\n矩阵 $A^T A$ 是半正定的。对于 $\\lambda  0$，矩阵 $A^T A + \\lambda I$ 是正定的，因此是可逆的。这使我们能够显式地写出唯一解：\n$$ \\beta_{\\mathrm{ridge}}(\\lambda,b) = (A^T A + \\lambda I)^{-1} A^T b $$\n\n**Lasso 回归分析**\nLasso 的目标函数是 $J_{\\mathrm{lasso}}(\\beta) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\lambda\\|\\beta\\|_1$。由于 $\\ell_1$ 范数项的存在，该函数是凸的但并非处处可微。其最优性条件由次梯度微积分给出：\n$$ 0 \\in \\partial_\\beta J_{\\mathrm{lasso}}(\\beta) $$\n这等价于：\n$$ 0 \\in A^T(A\\beta - b) + \\lambda \\partial\\|\\beta\\|_1 $$\n其中 $\\partial\\|\\beta\\|_1$ 是 $\\ell_1$ 范数的次微分。这组条件被称为 lasso 的 Karush-Kuhn-Tucker (KKT) 条件，它表明必须存在一个次梯度向量 $s \\in \\partial\\|\\beta\\|_1$ 使得：\n$$ A^T(b - A\\beta) = \\lambda s $$\n次梯度向量 $s$ 的分量满足：如果 $\\beta_i \\neq 0$，则 $s_i = \\mathrm{sgn}(\\beta_i)$；如果 $\\beta_i = 0$，则 $s_i \\in [-1, 1]$。\n\n有了这些基础知识，我们现在来评估每个选项。\n\n---\n**选项 A. 对于每个固定的 $A$ 和每个 $\\lambda  0$，映射 $(b,\\lambda) \\mapsto \\beta_{\\mathrm{ridge}}(\\lambda,b)$ 在任何 $(b,\\lambda)$ 的邻域内都是实解析的，因此平滑地依赖于 $b$ 和 $\\lambda$。**\n\n显式解是 $\\beta_{\\mathrm{ridge}}(\\lambda,b) = (A^T A + \\lambda I)^{-1} A^T b$。我们来考察其对 $b$ 和 $\\lambda$ 的依赖性。\n1.  项 $A^T b$ 是关于 $b$ 的线性函数。线性映射是实解析的。\n2.  项 $(A^T A + \\lambda I)^{-1}$ 依赖于 $\\lambda$。一个矩阵 $M$ 的逆矩阵的元素可以通过克莱姆法则表示为 $M$ 的伴随矩阵与 $M$ 的行列式的比值。\n    -   此处，$M(\\lambda) = A^T A + \\lambda I$。\n    -   行列式 $\\det(A^T A + \\lambda I)$ 是一个关于 $\\lambda$ 的多项式。对于 $\\lambda  0$，该行列式非零，因为矩阵是正定的。\n    -   伴随矩阵 $\\text{adj}(A^T A + \\lambda I)$ 的元素也是关于 $\\lambda$ 的多项式。\n    -   因此，逆矩阵 $(A^T A + \\lambda I)^{-1}$ 的每个元素都是 $\\lambda$ 的有理函数，其分母对于所有 $\\lambda  0$ 都非零。\n一个在一个变量 ($\\lambda$) 上是有理函数，在另一个变量 ($b$) 上是线性函数的函数，在其定义域上对两个变量都是实解析的。由于实解析函数是无限次可微的 ($C^\\infty$)，所以它是平滑的。因此，该陈述是正确的。\n\n**结论：正确。**\n\n---\n**选项 B. 如果 $A$ 是满列秩的，那么对于每个 $\\lambda  0$，映射 $(b,\\lambda) \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ 在 $(b,\\lambda)$ 上处处连续可微。**\n\nlasso 解由其 KKT 条件刻画。已知解路径 $\\beta_{\\mathrm{lasso}}(b, \\lambda)$ 在激活变量集（即系数非零的变量集合）发生变化的点是不可微的。对于一个固定的 $b$，当 $\\lambda$ 变化时，系数可能变为零或非零。类似地，对于一个固定的 $\\lambda$，当 $b$ 变化时，激活集也可能改变。\n让我们考虑一个满足 KKT 条件的点 $(\\beta, b, \\lambda)$。如果我们处于一个区域，其中激活集 $\\mathcal{A}=\\{i \\mid \\beta_i \\neq 0\\}$ 和激活系数的符号是恒定的，那么限制在 $\\mathcal{A}$ 上的解的行为类似于在列 $A_{\\mathcal{A}}$ 上带有偏移的无惩罚最小二乘问题：$\\beta_{\\mathcal{A}} = (A_{\\mathcal{A}}^T A_{\\mathcal{A}})^{-1} (A_{\\mathcal{A}}^T b - \\lambda s_{\\mathcal{A}})$。这个局部表达式在 $(b, \\lambda)$ 上是平滑的。然而，在这样一个区域的边界处，当某个系数 $\\beta_i$ 变为零，或者对于某个非激活系数 $j$ 的次梯度条件 $|(A^T(b - A\\beta))_j|$ 达到其界限 $\\lambda$ 时，激活集就会改变。在这些“断点”处，解的函数形式会突然改变。这导致解路径出现一个“扭结”，在该处导数是不连续的。因此，映射 $(b,\\lambda) \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ 是连续的，但并非处处可微。$A$ 的满列秩并不会改变这个源于 $\\ell_1$ 范数不可微性的基本性质。\n\n**结论：不正确。**\n\n---\n**选项 C. 在所述的唯一性/通有性条件下，对于任何固定的 $b$，lasso 正则化路径 $\\lambda \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ 在任何紧区间上都是关于 $\\lambda$ 的分段仿射函数，有有限个断点，在这些断点处激活集会发生变化；在这些断点处，路径通常是不可微的。**\n\n这个陈述精确地描述了 lasso 解路径的一个基本性质。正如在对选项 B 的分析中所确立的，在断点（激活集发生变化的 $\\lambda$ 值）之间，激活系数由 $\\beta_{\\mathcal{A}}(\\lambda) = (A_{\\mathcal{A}}^T A_{\\mathcal{A}})^{-1} (A_{\\mathcal{A}}^T b - \\lambda s_{\\mathcal{A}})$ 给出。此处，$s_{\\mathcal{A}}$ 是激活系数的符号向量，在断点之间是恒定的。\n这个表达式表明，对于 $i \\in \\mathcal{A}$ 的每个分量 $\\beta_i(\\lambda)$ 都是 $\\lambda$ 的仿射函数（形式为 $c_1 - c_2\\lambda$）。对于 $j \\notin \\mathcal{A}$ 的非激活系数 $\\beta_j(\\lambda)$ 恒为 0。因此，解向量 $\\beta(\\lambda)$ 的每个分量都是 $\\lambda$ 的分段仿射函数。路径本身，即向量函数 $\\lambda \\mapsto \\beta(\\lambda)$，因此是分段线性的。可以证明，在 $\\lambda  0$ 的任何紧区间上，只有有限个这样的断点。在这些点上，导数 $d\\beta/d\\lambda$ 通常不存在，因为激活集和 KKT 条件的结构发生了变化。\n\n**结论：正确。**\n\n---\n**选项 D. 诸如最小角回归 (LARS) 和同伦方法等正则化路径算法依赖于岭回归的分段线性特性，以有限多个线性段计算整个岭回归路径。**\n\n这个陈述包含两个基本错误。\n1.  LARS 及相关的同伦算法是专门为计算 *lasso* 问题（及其变体）的解路径而设计的，而不是为岭回归。它们利用了选项 C 中描述的 lasso 路径的分段线性性质。\n2.  岭回归路径 $\\beta_{\\mathrm{ridge}}(\\lambda) = (A^T A + \\lambda I)^{-1} A^T b$ 不是分段线性的。正如在对选项 A 的分析中所确立的，对于 $\\lambda  0$，它是一条平滑的、实解析的曲线。它不是由有限数量的线性段组成的。\n该陈述的前提在两个方面都是不正确的。\n\n**结论：不正确。**\n\n---\n**选项 E. 对于岭回归，其一阶最优性系统可以写成一个关于 $(\\beta,b,\\lambda)$ 的平滑方程，并且对于每个 $\\lambda  0$，其关于 $\\beta$ 的雅可比矩阵是可逆的，因此根据隐函数定理，解局部平滑地依赖于 $(b,\\lambda)$；特别地，可以在 $\\lambda$ 上进行延拓，并且在每个 $\\lambda  0$ 处都有一个良定义的切线方向。**\n\n让我们使用隐函数定理 (IFT) 将此形式化。一阶最优性条件是 $F(\\beta, b, \\lambda) = (A^T A + \\lambda I)\\beta - A^T b = 0$。\n1.  函数 $F: \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}_{0} \\to \\mathbb{R}^n$ 是其参数 $(\\beta, b, \\lambda)$ 的一个平滑（实际上是实解析）函数。\n2.  IFT 要求 $F$ 关于 $\\beta$ 的雅可比矩阵是可逆的。我们来计算这个雅可比矩阵：\n    $$ D_\\beta F(\\beta, b, \\lambda) = \\frac{\\partial}{\\partial \\beta^T} \\left[ (A^T A + \\lambda I)\\beta - A^T b \\right] = A^T A + \\lambda I $$\n3.  对于任何 $\\lambda  0$，这个雅可比矩阵 $A^T A + \\lambda I$ 是正定的，因此是可逆的。\nIFT 的条件在任何解点都得到满足。该定理继而保证了存在一个唯一的、局部定义的平滑函数 $\\beta(b, \\lambda)$ 来求解该方程。其平滑度与 $F$ 的阶数相同，即 $C^\\infty$。这证实了岭回归解平滑地依赖于其参数。\n平滑路径 $\\lambda \\mapsto \\beta(\\lambda)$ 的存在意味着其导数，即切线方向 $d\\beta/d\\lambda$，对于所有 $\\lambda  0$ 都是良定义的。将恒等式 $(A^T A + \\lambda I)\\beta(\\lambda) = A^T b$ 关于 $\\lambda$ 求导可得：\n$$ I \\cdot \\beta(\\lambda) + (A^T A + \\lambda I) \\frac{d\\beta(\\lambda)}{d\\lambda} = 0 \\implies \\frac{d\\beta(\\lambda)}{d\\lambda} = - (A^T A + \\lambda I)^{-1} \\beta(\\lambda) $$\n这个切向量的表达式是良定义的，因为对于所有 $\\lambda  0$ 逆矩阵都存在。该陈述是 IFT 的一个正确应用。\n\n**结论：正确。**", "answer": "$$\\boxed{ACE}$$", "id": "3490569"}]}