{"hands_on_practices": [{"introduction": "在尝试求解一个优化问题之前，我们首先需要理解其解的根本性质。本练习将引导你探索 LASSO 问题的最优性条件，即 Karush-Kuhn-Tucker (KKT) 条件 [@problem_id:3470505]。通过推导这些条件，我们不仅能验证一个候选解是否真正最优，还能深入了解稀疏解的结构，并确定在何种情况下最简单的零向量解即为问题的最优解。", "problem": "考虑最小绝对值收敛和选择算子 (Lasso) 问题，这是一个复合凸优化问题\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，且 $\\lambda  0$。从复合目标的凸分析基本原理出发，通过援引此无约束复合问题的 Karush-Kuhn-Tucker (KKT) 条件，推导其最优性条件。明确地刻画 $\\ell_{1}$ 范数的次微分，并解释最优解非零分量的符号是如何由次梯度决定的。然后，将你的刻画应用于以下具体实例\n$$\nA=\\begin{pmatrix}\n1  0  2 \\\\\n0  1  -1 \\\\\n1  1  0 \\\\\n2  -1  1\n\\end{pmatrix}, \\qquad\nb=\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1 \\\\\n4\n\\end{pmatrix},\n$$\n并确定使得 $x^{\\star}=\\mathbf{0}$ 满足 KKT 条件并因此成为最优解的最小 $\\lambda$ 值（表示为一个实数）。将最终答案以单个实数形式给出。无需四舍五入。", "solution": "该问题要求推导 Lasso 问题的最优性条件，然后针对一个具体实例，找出使得零向量成为最优解的最小正则化参数 $\\lambda$。\n\nLasso 目标函数是一个形如 $F(x) = f(x) + g(x)$ 的复合函数，其中 $x \\in \\mathbb{R}^n$。其两个组成部分是：\n1.  一个光滑、凸、可微的数据保真项：$f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$。\n2.  一个非光滑、凸、不可微的正则化项：$g(x) = \\lambda \\|x\\|_{1}$，其中 $\\lambda  0$。\n\n对于一个凸函数 $F(x)$，一个点 $x^{\\star}$ 是全局最小值点的充分必要条件是，零向量包含在 $F$ 于 $x^{\\star}$ 处的次微分中。这是一阶最优性条件，也是 Karush-Kuhn-Tucker (KKT) 条件在非光滑无约束凸问题上的推广。该条件为：\n$$\n\\mathbf{0} \\in \\partial F(x^{\\star})\n$$\n由于 $f(x)$ 是凸且连续可微的，而 $g(x)$ 是凸的，它们和的次微分等于它们次微分的和：\n$$\n\\partial F(x) = \\partial f(x) + \\partial g(x)\n$$\n一个可微函数的次微分是仅包含其梯度的集合。$f(x)$ 的梯度为：\n$$\n\\nabla f(x) = A^T(A x - b)\n$$\n因此，$\\partial f(x) = \\{\\nabla f(x)\\}$。$g(x) = \\lambda \\|x\\|_{1}$ 的次微分是 $\\partial g(x) = \\lambda \\partial \\|x\\|_{1}$。\n\n综合这些，$x^{\\star}$ 的最优性条件变为：\n$$\n\\mathbf{0} \\in A^T(A x^{\\star} - b) + \\lambda \\partial \\|x^{\\star}\\|_{1}\n$$\n这可以重写为：\n$$\nA^T(b - A x^{\\star}) \\in \\lambda \\partial \\|x^{\\star}\\|_{1}\n$$\n\n接下来，我们刻画 $\\ell_1$-范数 $\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|$ 的次微分。由于该函数是可分的，其次微分是其各个分量 $|x_i|$ 的次微分的笛卡尔积。绝对值函数 $h(z) = |z|$ 在点 $z \\in \\mathbb{R}$ 处的次微分是：\n$$\n\\partial |z| = \\begin{cases} \\{\\text{sgn}(z)\\}  \\text{if } z \\neq 0 \\\\ [-1, 1]  \\text{if } z = 0 \\end{cases}\n$$\n因此，次微分 $\\partial \\|x\\|_{1}$ 是所有向量 $s \\in \\mathbb{R}^n$（称为次梯度）的集合，其分量 $s_i$ 满足：\n$$\ns_i = \\begin{cases} \\text{sgn}(x_i)  \\text{if } x_i \\neq 0 \\\\ v_i \\in [-1, 1]  \\text{if } x_i = 0 \\end{cases}\n$$\n\n最优性条件 $A^T(b - A x^{\\star}) \\in \\lambda \\partial \\|x^{\\star}\\|_{1}$ 意味着必须存在一个次梯度 $s^{\\star} \\in \\partial \\|x^{\\star}\\|_{1}$，使得 $A^T(b - A x^{\\star}) = \\lambda s^{\\star}$。对 $i=1, \\ldots, n$ 按分量进行分析：\n\\begin{enumerate}\n    \\item 如果 $x^{\\star}_i \\neq 0$，那么 $s^{\\star}_i = \\text{sgn}(x^{\\star}_i)$。条件变为 $(A^T(b - A x^{\\star}))_i = \\lambda \\cdot \\text{sgn}(x^{\\star}_i)$。这意味着 $|(A^T(b - A x^{\\star}))_i| = \\lambda$，并且非零分量 $x^{\\star}_i$ 的符号由向量 $A^T(b - A x^{\\star})$ 相应分量的符号决定。具体来说，$\\text{sgn}(x^{\\star}_i) = \\frac{1}{\\lambda} (A^T(b-Ax^{\\star}))_i$。\n    \\item 如果 $x^{\\star}_i = 0$，那么 $s^{\\star}_i \\in [-1, 1]$。条件变为 $(A^T(b - A x^{\\star}))_i = \\lambda s^{\\star}_i$，这意味着 $|(A^T(b - A x^{\\star}))_i| \\leq \\lambda$。\n\\end{enumerate}\n\n现在，我们应用这个框架来确定使得 $x^{\\star} = \\mathbf{0}$ 成为最优解的最小 $\\lambda  0$ 值。我们将 $x^{\\star} = \\mathbf{0}$ 代入最优性条件。对于每个分量 $i$，我们都属于情况 2，因为 $x^{\\star}_i = 0$。条件变为：\n$$\n| (A^T(b - A\\mathbf{0}))_i | \\leq \\lambda \\quad \\forall i=1, \\ldots, n\n$$\n这可以简化为：\n$$\n| (A^T b)_i | \\leq \\lambda \\quad \\forall i=1, \\ldots, n\n$$\n这组不等式必须对所有分量 $i$ 成立。这等价于要求 $\\lambda$ 大于或等于向量 $A^T b$ 所有分量中的最大绝对值。这个最大值是 $A^T b$ 的 $\\ell_{\\infty}$-范数。\n$$\n\\lambda \\geq \\max_{i} |(A^T b)_i| = \\|A^T b\\|_{\\infty}\n$$\n因此，使得 $x^{\\star} = \\mathbf{0}$ 是最优解的最小 $\\lambda$ 值为 $\\lambda = \\|A^T b\\|_{\\infty}$。\n\n我们给定的具体实例是：\n$$\nA=\\begin{pmatrix}\n1  0  2 \\\\\n0  1  -1 \\\\\n1  1  0 \\\\\n2  -1  1\n\\end{pmatrix}, \\qquad\nb=\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1 \\\\\n4\n\\end{pmatrix}\n$$\n首先，我们计算 $A$ 的转置：\n$$\nA^T = \\begin{pmatrix}\n1  0  1  2 \\\\\n0  1  1  -1 \\\\\n2  -1  0  1\n\\end{pmatrix}\n$$\n接下来，我们计算乘积 $A^T b$：\n$$\nA^T b = \\begin{pmatrix}\n1  0  1  2 \\\\\n0  1  1  -1 \\\\\n2  -1  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1 \\\\\n4\n\\end{pmatrix}\n= \\begin{pmatrix}\n1(3) + 0(-2) + 1(1) + 2(4) \\\\\n0(3) + 1(-2) + 1(1) + (-1)(4) \\\\\n2(3) + (-1)(-2) + 0(1) + 1(4)\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 + 0 + 1 + 8 \\\\\n0 - 2 + 1 - 4 \\\\\n6 + 2 + 0 + 4\n\\end{pmatrix}\n= \\begin{pmatrix}\n12 \\\\\n-5 \\\\\n12\n\\end{pmatrix}\n$$\n最后，我们计算该向量的 $\\ell_{\\infty}$-范数以找到最小的 $\\lambda$：\n$$\n\\lambda = \\|A^T b\\|_{\\infty} = \\max(|12|, |-5|, |12|) = \\max(12, 5, 12) = 12\n$$\n因此，使得 $x^{\\star} = \\mathbf{0}$ 成为最优解的最小 $\\lambda$ 值为 $12$。", "answer": "$$\\boxed{12}$$", "id": "3470505"}, {"introduction": "近端梯度方法通过迭代逐步逼近最优解，而近端梯度映射 $G_{\\alpha}(x)$ 不仅是算法迭代的核心，更是一个关键的诊断工具 [@problem_id:3470531]。这个练习要求你对一个具体的点直接计算 $G_{\\alpha}(x)$，从而具体理解算法如何融合梯度信息与近端步骤。此过程还将揭示 $G_{\\alpha}(x)$ 的范数如何量化当前点与满足最优性条件之间的“残差”或差距。", "problem": "考虑最小绝对收缩和选择算子（Lasso）的复合优化问题，该问题在 $x \\in \\mathbb{R}^{n}$ 上最小化 $F(x) = f(x) + g(x)$，其中 $f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$ 且 $g(x) = \\lambda \\|x\\|_{1}$。设一个正常、闭、凸函数 $g$ 的近端算子定义为\n$$\n\\operatorname{prox}_{\\tau g}(z) \\triangleq \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ g(u) + \\frac{1}{2 \\tau}\\|u - z\\|_{2}^{2} \\right\\},\n$$\n并设步长为 $\\alpha  0$ 的近端梯度映射定义为\n$$\nG_{\\alpha}(x) \\triangleq \\frac{1}{\\alpha}\\left(x - \\operatorname{prox}_{\\alpha g}\\big(x - \\alpha \\nabla f(x)\\big)\\right).\n$$\n给定 Lasso 实例，其参数为\n$$\nA = \\begin{pmatrix}\n1  2  -1 \\\\\n-2  1  1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\ -3\n\\end{pmatrix}, \\quad\n\\lambda = 1, \\quad\n\\alpha = \\frac{1}{2}, \\quad\nx = \\begin{pmatrix}\n2 \\\\ -1 \\\\ 0\n\\end{pmatrix}.\n$$\n(a) 仅使用上述定义和凸分析的基本性质，推导 Lasso 问题中 $G_{\\alpha}(x)$ 的显式表达式，该表达式应通过对 $x - \\alpha \\nabla f(x)$ 进行坐标级软阈值操作来表示，并用 $\\alpha$ 和 $\\lambda$ 确定阈值参数。\n\n(b) 对于给定的数据，精确计算 $\\|G_{\\alpha}(x)\\|_{\\infty}$ 的值。\n\n(c) 简要解释 $G_{\\alpha}(x)$ 各分量的大小如何量化点 $x$ 处对次梯度最优性条件 $0 \\in A^{\\top}(A x - b) + \\lambda \\,\\partial \\|x\\|_{1}$ 的违反程度。你最终报告的答案必须仅为 (b) 部分要求的值。无需四舍五入；报告精确值，不带单位。", "solution": "问题要求对特定 Lasso 实例的近端梯度映射进行三部分分析。我们首先验证问题陈述，发现其是良定的、有科学依据且自洽的。我们继续进行求解。\n\n(a) 推导 Lasso 问题的 $G_{\\alpha}(x)$ 的显式表达式。\n\n近端梯度映射定义为 $G_{\\alpha}(x) \\triangleq \\frac{1}{\\alpha}\\left(x - \\operatorname{prox}_{\\alpha g}\\big(x - \\alpha \\nabla f(x)\\big)\\right)$。为了推导其在 Lasso 问题中的显式形式，我们必须首先确定 $\\nabla f(x)$ 和 $\\operatorname{prox}_{\\alpha g}(z)$ 的表达式。\n\n首先，我们求光滑部分 $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ 的梯度。\n$f(x) = \\frac{1}{2}(A x - b)^{\\top}(A x - b) = \\frac{1}{2}(x^{\\top}A^{\\top} - b^{\\top})(A x - b) = \\frac{1}{2}(x^{\\top}A^{\\top}A x - 2 x^{\\top}A^{\\top}b + b^{\\top}b)$。\n关于 $x$ 的梯度是：\n$$\n\\nabla f(x) = \\frac{1}{2}(2 A^{\\top}A x - 2 A^{\\top}b) = A^{\\top}(A x - b).\n$$\n接下来，我们推导函数 $g(x) = \\lambda \\|x\\|_1$ 的近端算子，参数为 $\\tau = \\alpha$。根据定义：\n$$\n\\operatorname{prox}_{\\alpha g}(z) = \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ g(u) + \\frac{1}{2\\alpha}\\|u - z\\|_{2}^{2} \\right\\} = \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\lambda \\|u\\|_{1} + \\frac{1}{2\\alpha}\\|u - z\\|_{2}^{2} \\right\\}.\n$$\n目标函数关于 $u$ 的各分量是可分的。我们可以写成 $\\|u\\|_1 = \\sum_{i=1}^n |u_i|$ 和 $\\|u-z\\|_2^2 = \\sum_{i=1}^n (u_i-z_i)^2$。因此，我们可以对每个分量 $u_i$ 独立地进行最小化：\n$$\n\\min_{u_i \\in \\mathbb{R}} \\left\\{ \\lambda |u_i| + \\frac{1}{2\\alpha}(u_i - z_i)^2 \\right\\}.\n$$\n一阶最优性条件可以通过次梯度微积分找到。目标函数关于 $u_i$ 的次梯度是 $\\lambda \\partial|u_i| + \\frac{1}{\\alpha}(u_i - z_i)$。令其包含 $0$ 可得 $z_i - u_i \\in \\alpha \\lambda \\partial|u_i|$。绝对值函数的次微分在 $u_i \\ne 0$ 时是 $\\partial|u_i| = \\operatorname{sign}(u_i)$，在 $u_i = 0$ 时是 $\\partial|u_i| = [-1, 1]$。\n- 如果 $u_i  0$，条件变为 $z_i - u_i = \\alpha\\lambda$，所以 $u_i = z_i - \\alpha\\lambda$。这仅在 $z_i - \\alpha\\lambda  0$ 时成立，即 $z_i  \\alpha\\lambda$。\n- 如果 $u_i  0$，条件变为 $z_i - u_i = -\\alpha\\lambda$，所以 $u_i = z_i + \\alpha\\lambda$。这仅在 $z_i + \\alpha\\lambda  0$ 时成立，即 $z_i  -\\alpha\\lambda$。\n- 如果 $u_i = 0$，条件是 $z_i \\in \\alpha\\lambda [-1, 1]$，即 $|z_i| \\le \\alpha\\lambda$。\n\n综合这些情况，解 $u_i$ 由下式给出：\n$$\nu_i = \\begin{cases} z_i - \\alpha \\lambda  \\text{if } z_i  \\alpha \\lambda \\\\ 0  \\text{if } |z_i| \\le \\alpha \\lambda \\\\ z_i + \\alpha \\lambda  \\text{if } z_i  -\\alpha \\lambda \\end{cases}.\n$$\n这就是坐标级软阈值算子，通常记作 $S_{\\kappa}(z)$，其中阈值参数为 $\\kappa = \\alpha\\lambda$。一个紧凑的表达式是 $S_{\\kappa}(z_i) = \\operatorname{sign}(z_i) \\max(0, |z_i| - \\kappa)$。所以，$\\operatorname{prox}_{\\alpha g}(z) = S_{\\alpha\\lambda}(z)$。\n\n将这些表达式代入 $G_{\\alpha}(x)$ 的定义中，我们得到：\n$$\nG_{\\alpha}(x) = \\frac{1}{\\alpha}\\left(x - S_{\\alpha\\lambda}\\big(x - \\alpha A^{\\top}(Ax - b)\\big)\\right).\n$$\n这就是 Lasso 的近端梯度映射关于坐标级软阈值的显式表达式。阈值参数是 $\\alpha\\lambda$。\n\n(b) 对给定数据计算 $\\|G_{\\alpha}(x)\\|_{\\infty}$。\n\n我们给定的数据是：\n$A = \\begin{pmatrix} 1  2  -1 \\\\ -2  1  1 \\end{pmatrix}$， $b = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$， $\\lambda = 1$， $\\alpha = \\frac{1}{2}$，以及 $x = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$。\n\n第1步：计算梯度 $\\nabla f(x) = A^{\\top}(Ax - b)$。\n首先，计算残差 $r = Ax - b$：\n$$\nAx = \\begin{pmatrix} 1  2  -1 \\\\ -2  1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(2) + (2)(-1) + (-1)(0) \\\\ (-2)(2) + (1)(-1) + (1)(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -5 \\end{pmatrix}.\n$$\n$$\nr = Ax - b = \\begin{pmatrix} 0 \\\\ -5 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -2 \\end{pmatrix}.\n$$\n现在，计算梯度：\n$$\n\\nabla f(x) = A^{\\top}r = \\begin{pmatrix} 1  -2 \\\\ 2  1 \\\\ -1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} (1)(-1) + (-2)(-2) \\\\ (2)(-1) + (1)(-2) \\\\ (-1)(-1) + (1)(-2) \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -4 \\\\ -1 \\end{pmatrix}.\n$$\n\n第2步：计算近端算子的自变量 $z = x - \\alpha \\nabla f(x)$。\n当 $\\alpha = \\frac{1}{2}$ 时：\n$$\nz = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 3 \\\\ -4 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 - \\frac{3}{2} \\\\ -1 + 2 \\\\ 0 + \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\\\ \\frac{1}{2} \\end{pmatrix}.\n$$\n\n第3步：应用软阈值算子 $\\operatorname{prox}_{\\alpha g}(z) = S_{\\alpha\\lambda}(z)$。\n阈值参数是 $\\kappa = \\alpha\\lambda = \\frac{1}{2} \\cdot 1 = \\frac{1}{2}$。\n我们逐分量地应用 $S_{1/2}(z)$：\n$$\n(S_{1/2}(z))_1 = \\operatorname{sign}(z_1) \\max(0, |z_1| - \\kappa) = \\operatorname{sign}(\\frac{1}{2}) \\max(0, |\\frac{1}{2}| - \\frac{1}{2}) = 1 \\cdot 0 = 0.\n$$\n$$\n(S_{1/2}(z))_2 = \\operatorname{sign}(z_2) \\max(0, |z_2| - \\kappa) = \\operatorname{sign}(1) \\max(0, |1| - \\frac{1}{2}) = 1 \\cdot \\frac{1}{2} = \\frac{1}{2}.\n$$\n$$\n(S_{1/2}(z))_3 = \\operatorname{sign}(z_3) \\max(0, |z_3| - \\kappa) = \\operatorname{sign}(\\frac{1}{2}) \\max(0, |\\frac{1}{2}| - \\frac{1}{2}) = 1 \\cdot 0 = 0.\n$$\n所以，$\\operatorname{prox}_{\\alpha g}(z) = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix}$。\n\n第4步：计算 $G_{\\alpha}(x) = \\frac{1}{\\alpha}\\left(x - \\operatorname{prox}_{\\alpha g}(z)\\right)$。\n$$\nG_{\\alpha}(x) = \\frac{1}{1/2} \\left( \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix} \\right) = 2 \\begin{pmatrix} 2 \\\\ -1 - \\frac{1}{2} \\\\ 0 \\end{pmatrix} = 2 \\begin{pmatrix} 2 \\\\ -\\frac{3}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -3 \\\\ 0 \\end{pmatrix}.\n$$\n\n第5步：计算无穷范数 $\\|G_{\\alpha}(x)\\|_{\\infty}$。\n$$\n\\|G_{\\alpha}(x)\\|_{\\infty} = \\max(|4|, |-3|, |0|) = \\max(4, 3, 0) = 4.\n$$\n\n(c) 解释 $G_{\\alpha}(x)$ 如何量化对次梯度最优性条件的违反程度。\n\n对于凸问题 $\\min_x F(x) = f(x) + g(x)$，其一阶充要最优性条件是：$x^*$ 是一个极小值点当且仅当 $0 \\in \\partial F(x^*)$，其中 $\\partial F(x^*)$ 是 $F$ 在 $x^*$ 处的次微分。对于 Lasso 问题，此条件是 $0 \\in \\nabla f(x^*) + \\partial g(x^*)$，可写作：\n$$\n0 \\in A^{\\top}(A x^* - b) + \\lambda \\,\\partial \\|x^*\\|_{1}.\n$$\n近端梯度法生成一个序列 $x^{k+1} = \\operatorname{prox}_{\\alpha g}(x^k - \\alpha \\nabla f(x^k))$。一个点 $x^*$ 是极小值点当且仅当它是这次迭代的一个不动点，即 $x^* = \\operatorname{prox}_{\\alpha g}(x^* - \\alpha \\nabla f(x^*))$。这个不动点方程等价于最优性条件。为了证明这一点，回顾 (a) 部分，$u = \\operatorname{prox}_{\\tau g}(z)$ 等价于 $z - u \\in \\tau \\partial g(u)$。将此应用于不动点方程，令 $u=x^*$，$z=x^*-\\alpha \\nabla f(x^*)$，以及 $\\tau=\\alpha$，我们得到：\n$$\n(x^* - \\alpha \\nabla f(x^*)) - x^* \\in \\alpha \\, \\partial g(x^*),\n$$\n化简为 $-\\alpha \\nabla f(x^*) \\in \\alpha \\, \\partial g(x^*)$。因为 $\\alpha  0$，这等价于 $-\\nabla f(x^*) \\in \\partial g(x^*)$，或 $0 \\in \\nabla f(x^*) + \\partial g(x^*)$。\n\n近端梯度映射是 $G_{\\alpha}(x) = \\frac{1}{\\alpha}\\left(x - \\operatorname{prox}_{\\alpha g}(x - \\alpha \\nabla f(x))\\right)$。一个点 $x$ 是近端梯度迭代的不动点当且仅当 $G_{\\alpha}(x) = 0$。因此，条件 $G_{\\alpha}(x) = 0$ 等价于次梯度最优性条件在 $x$ 处得到满足。\n\n因此，$G_{\\alpha}(x)$ 可作为一个残差，或衡量点 $x$ 距离满足最优性条件的程度。$G_{\\alpha}(x)$ 的大小（在任何范数下，如无穷范数）量化了对该条件的违反程度。一个非零分量 $(G_{\\alpha}(x))_i$ 表明 $x$ 的第 $i$ 个坐标不满足不动点条件，因此最优性条件在该坐标上被违反。在迭代算法中，$\\|G_{\\alpha}(x)\\|$ 常被用作终止准则；当该值低于一个小的容差时，算法停止，表明当前迭代点已足够接近最优点。", "answer": "$$\n\\boxed{4}\n$$", "id": "3470531"}, {"introduction": "近端梯度下降的收敛性与收敛速度在很大程度上取决于步长的选择。一个合适的步长必须适应目标函数光滑部分的几何特性，这通常由其梯度的 Lipschitz 常数来刻画 [@problem_id:3470519]。本练习将指导你推导并计算坐标级的 Lipschitz 常数，揭示数据矩阵 $A$ 如何直接影响损失函数的“曲率”，进而为优化算法（如近端坐标下降法）选择安全且有效的自适应步长提供理论依据。", "problem": "考虑稀疏估计中的复合目标，其中光滑的数据保真项是二次的，非光滑的惩罚项促进稀疏性。令 $g(x)$ 表示光滑项，$h(x)$ 表示非光滑项，其中\n$$\ng(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}, \\quad h(x) = \\lambda \\|x\\|_{1},\n$$\n对于矩阵 $A \\in \\mathbb{R}^{m \\times n}$，向量 $b \\in \\mathbb{R}^{m}$，以及正则化参数 $\\lambda  0$。用于复合目标 $g(x) + h(x)$ 的近端梯度下降法利用了 $g(x)$ 的梯度信息和 $h(x)$ 的近端算子。\n\n仅使用以下基础元素：\n- 光滑函数的梯度定义及其坐标表示，\n- 标量函数和梯度映射的 Lipschitz 连续性定义，\n- 缩放绝对值的近端算子定义，\n推导 $g(x)$ 梯度的坐标级 Lipschitz 常数 $L_{i}$，其定义为满足以下条件的最小常数\n$$\n\\left|\\frac{\\partial g}{\\partial x_{i}}(x + t e_{i}) - \\frac{\\partial g}{\\partial x_{i}}(x)\\right| \\leq L_{i} |t| \\quad \\text{对于所有 } x \\in \\mathbb{R}^{n}, \\; t \\in \\mathbb{R},\n$$\n其中 $e_{i}$ 是 $\\mathbb{R}^{n}$ 中的第 $i$ 个标准基向量。解释这些常数如何确定用于最小绝对值收敛和选择算子（LASSO）的近端梯度下降法中的自适应坐标级步长，其中 $h(x) = \\lambda \\|x\\|_{1}$。\n\n然后，对于特定矩阵\n$$\nA = \\begin{pmatrix}\n1  0  2 \\\\\n0  2  -1 \\\\\n1  2  1 \\\\\n1  -1  0\n\\end{pmatrix} \\in \\mathbb{R}^{4 \\times 3},\n$$\n计算坐标级 Lipschitz 常数 $L_{1}$、$L_{2}$ 和 $L_{3}$ 的数值。将你的最终答案以一个包含 $L_{1}$、$L_{2}$ 和 $L_{3}$ 的单行矩阵形式给出。无需四舍五入。", "solution": "**坐标级 Lipschitz 常数 $L_i$ 的推导**\n\n目标函数的光滑部分由 $g(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ 给出。我们可以将此表达式展开为二次型：\n$$\ng(x) = \\frac{1}{2} (Ax - b)^{T}(Ax - b) = \\frac{1}{2} (x^{T}A^{T} - b^{T})(Ax - b) = \\frac{1}{2} (x^{T}A^{T}Ax - 2b^{T}Ax + b^{T}b).\n$$\n$g(x)$ 相对于向量 $x$ 的梯度为：\n$$\n\\nabla g(x) = \\frac{1}{2} (2A^{T}Ax - 2A^{T}b) = A^{T}(Ax - b).\n$$\n$g(x)$ 的第 $i$ 个偏导数，记为 $\\frac{\\partial g}{\\partial x_i}(x)$，是梯度向量 $\\nabla g(x)$ 的第 $i$ 个分量。令 $A_i$ 表示矩阵 $A$ 的第 $i$ 列。那么 $A^T$ 的第 $i$ 行就是 $A_i^T$。因此，我们可以将第 $i$ 个偏导数写为：\n$$\n\\frac{\\partial g}{\\partial x_i}(x) = [\\nabla g(x)]_i = A_i^{T}(Ax - b).\n$$\n为了找到坐标级 Lipschitz 常数 $L_i$，我们必须分析当只有 $x$ 的第 $i$ 个坐标被扰动时，该偏导数的变化。我们在点 $x + t e_i$ 处计算 $\\frac{\\partial g}{\\partial x_i}$，其中 $e_i$ 是第 $i$ 个标准基向量，$t \\in \\mathbb{R}$：\n$$\n\\frac{\\partial g}{\\partial x_i}(x + t e_i) = A_i^{T}(A(x + t e_i) - b).\n$$\n利用矩阵向量乘积的线性性质，我们得到：\n$$\n\\frac{\\partial g}{\\partial x_i}(x + t e_i) = A_i^{T}(Ax + t A e_i - b).\n$$\n根据定义，乘积 $A e_i$ 提取了 $A$ 的第 $i$ 列，即 $A_i$。所以，\n$$\n\\frac{\\partial g}{\\partial x_i}(x + t e_i) = A_i^{T}(Ax - b + t A_i) = A_i^{T}(Ax - b) + t A_i^{T}A_i.\n$$\n现在，我们计算 Lipschitz 常数定义中所需的差值：\n$$\n\\frac{\\partial g}{\\partial x_i}(x + t e_i) - \\frac{\\partial g}{\\partial x_i}(x) = \\left( A_i^{T}(Ax - b) + t A_i^{T}A_i \\right) - A_i^{T}(Ax - b) = t A_i^{T}A_i.\n$$\n项 $A_i^{T}A_i$ 是列向量 $A_i$ 与自身的点积，它等于 $A_i$ 的欧几里得范数的平方，即 $A_i^{T}A_i = \\|A_i\\|_2^2$。因此，差值为 $t \\|A_i\\|_2^2$。\n坐标级 Lipschitz 条件是：\n$$\n\\left| \\frac{\\partial g}{\\partial x_i}(x + t e_i) - \\frac{\\partial g}{\\partial x_i}(x) \\right| \\leq L_i |t|.\n$$\n代入我们求得的差值：\n$$\n|t \\|A_i\\|_2^2| \\leq L_i |t|.\n$$\n由于 $\\|A_i\\|_2^2 \\geq 0$，这可以简化为：\n$$\n|t| \\|A_i\\|_2^2 \\leq L_i |t|.\n$$\n对于任何 $t \\neq 0$，我们可以除以 $|t|$ 得到 $\\|A_i\\|_2^2 \\leq L_i$。问题要求的是满足此不等式的最小常数 $L_i$。这个最小值恰好是下界本身。因此，$g(x)$ 的第 $i$ 个偏导数的坐标级 Lipschitz 常数为：\n$$\nL_i = \\|A_i\\|_2^2.\n$$\n\n**$L_i$ 在 LASSO 自适应步长中的作用**\n\n在用于 LASSO 目标 $F(x) = g(x) + h(x) = \\frac{1}{2}\\|Ax-b\\|_2^2 + \\lambda\\|x\\|_1$ 的近端坐标下降法中，我们一次迭代更新一个坐标，同时保持其他坐标固定。要在第 $k$ 次迭代中更新第 $i$ 个坐标，我们需要解决一维子问题：\n$$\nx_i^{(k+1)} = \\arg\\min_{u \\in \\mathbb{R}} F(x_1^{(k)}, \\dots, x_{i-1}^{(k)}, u, x_{i+1}^{(k)}, \\dots, x_n^{(k)}).\n$$\n这个子问题等价于最小化一个关于变化量 $d = u - x_i^{(k)}$ 的目标函数：\n$$\n\\min_{d} g(x^{(k)} + de_i) + \\lambda|x_i^{(k)} + d| + \\lambda\\sum_{j \\neq i}|x_j^{(k)}|.\n$$\n偏导数 $\\frac{\\partial g}{\\partial x_i}$ 的 $L_i$-Lipschitz 连续性允许我们沿第 $i$ 个坐标方向构建 $g(x)$ 的一个二次上界（这是下降引理的一个推论）：\n$$\ng(x^{(k)} + de_i) \\leq g(x^{(k)}) + d \\frac{\\partial g}{\\partial x_i}(x^{(k)}) + \\frac{L_i}{2}d^2.\n$$\n将此上界代入子问题，我们最小化一个主要化函数：\n$$\n\\min_d \\left( g(x^{(k)}) + d \\frac{\\partial g}{\\partial x_i}(x^{(k)}) + \\frac{L_i}{2}d^2 + \\lambda|x_i^{(k)} + d| \\right).\n$$\n去掉常数项并令 $u = x_i^{(k)} + d$，我们求解：\n$$\n\\min_u \\left( (u - x_i^{(k)}) \\frac{\\partial g}{\\partial x_i}(x^{(k)}) + \\frac{L_i}{2}(u - x_i^{(k)})^2 + \\lambda|u| \\right).\n$$\n整理各项以匹配近端算子问题的形式：\n$$\n\\min_u \\left( \\frac{L_i}{2} \\left( u^2 - 2u x_i^{(k)} + (x_i^{(k)})^2 \\right) + u \\frac{\\partial g}{\\partial x_i}(x^{(k)}) + \\lambda|u| + \\dots \\right).\n$$\n这等价于最小化：\n$$\n\\min_u \\left( \\frac{L_i}{2} \\left(u - \\left(x_i^{(k)} - \\frac{1}{L_i}\\frac{\\partial g}{\\partial x_i}(x^{(k)})\\right)\\right)^2 + \\lambda|u| \\right).\n$$\n这恰好是 $h_i(u) = \\lambda|u|$ 的缩放近端算子的定义：\n$$\nx_i^{(k+1)} = \\text{prox}_{\\frac{\\lambda}{L_i}|\\cdot|}\\left(x_i^{(k)} - \\frac{1}{L_i}\\frac{\\partial g}{\\partial x_i}(x^{(k)}) \\right).\n$$\n缩放 L1 范数的近端算子是软阈值算子，$S_{\\alpha}(z) = \\text{sgn}(z)\\max(|z|-\\alpha, 0)$。因此，更新规则为：\n$$\nx_i^{(k+1)} = S_{\\lambda/L_i}\\left(x_i^{(k)} - \\frac{1}{L_i}\\frac{\\partial g}{\\partial x_i}(x^{(k)})\\right).\n$$\n这是对第 $i$ 个坐标使用步长 $\\gamma_i = 1/L_i$ 的近端梯度更新。坐标级 Lipschitz 常数 $L_i$ 决定了每个坐标的自适应步长的倒数。较大的 $L_i$ 表示目标函数沿第 $i$ 个坐标轴的曲率更大，需要一个更小、更谨慎的步长 $\\gamma_i$ 来确保收敛。\n\n**对给定矩阵 A 的数值计算**\n\n给定的矩阵 $A \\in \\mathbb{R}^{4 \\times 3}$ 是：\n$$\nA = \\begin{pmatrix} 1  0  2 \\\\ 0  2  -1 \\\\ 1  2  1 \\\\ 1  -1  0 \\end{pmatrix}.\n$$\n$A$ 的列向量是：\n$$\nA_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad A_2 = \\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad A_3 = \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\n我们计算坐标级 Lipschitz 常数 $L_i = \\|A_i\\|_2^2$ 对于 $i=1, 2, 3$。\n\n对于 $i=1$：\n$$\nL_1 = \\|A_1\\|_2^2 = 1^2 + 0^2 + 1^2 + 1^2 = 1 + 0 + 1 + 1 = 3.\n$$\n\n对于 $i=2$：\n$$\nL_2 = \\|A_2\\|_2^2 = 0^2 + 2^2 + 2^2 + (-1)^2 = 0 + 4 + 4 + 1 = 9.\n$$\n\n对于 $i=3$：\n$$\nL_3 = \\|A_3\\|_2^2 = 2^2 + (-1)^2 + 1^2 + 0^2 = 4 + 1 + 1 + 0 = 6.\n$$\n坐标级 Lipschitz 常数的数值为 $L_1=3$，$L_2=9$ 和 $L_3=6$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3  9  6\n\\end{pmatrix}\n}\n$$", "id": "3470519"}]}