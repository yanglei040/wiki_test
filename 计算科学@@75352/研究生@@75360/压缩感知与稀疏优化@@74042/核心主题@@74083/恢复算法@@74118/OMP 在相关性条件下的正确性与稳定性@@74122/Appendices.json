{"hands_on_practices": [{"introduction": "要深入理解字典的几何特性如何影响稀疏恢复的性能，我们必须首先掌握其核心度量指标：互相关性 $\\mu(A)$ 和累积相关性 $\\mu_1(s)$。本练习将通过一个高度对称的“等角”字典，引导你进行直接的推导计算。通过解决这个理想化的案例，你将为这些抽象的度量指标如何由字典原子的几何排列所决定，建立起具体而深刻的直觉。[@problem_id:3441533]", "problem": "考虑一个字典矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其列为 $\\{a_{1}, \\dots, a_{n}\\}$，其中每一列都是单位范数，即对所有 $j \\in \\{1, \\dots, n\\}$ 都有 $\\|a_{j}\\|_{2} = 1$。假设该字典是等角的，即对于所有不同的索引 $i \\neq j$，其内积的绝对值满足 $|\\langle a_{i}, a_{j} \\rangle| = c$，其中 $c \\in [0,1)$ 是一个固定的常数。在压缩感知的背景下，$A$ 的互相关性 $\\mu(A)$ 定义为不同列之间的最大绝对内积，而累积相关性（也称为巴贝尔函数）$\\mu_{1}(s)$ 定义为，对于所有包含 $s$ 个索引的集合以及所有不在该集合中的列索引，该列与由该集合索引的 $s$ 个列之间的绝对内积之和的最大值。\n\n从互相关性和累积相关性的基本定义出发，对上述等角字典进行完整推导，以 $c$ 和 $s$ 显式计算 $\\mu(A)$ 和 $\\mu_{1}(s)$，并验证等式 $\\mu_{1}(s) = s c$。以解析表达式的形式提供最终答案。无需四舍五入，也不涉及任何单位。您的最终答案必须列出数对 $\\mu(A)$ 和 $\\mu_{1}(s)$。", "solution": "首先，将根据要求的协议，通过仔细审查其前提和定义来验证问题陈述。\n\n**步骤 1：提取已知条件**\n问题提供了以下数据和定义：\n- 一个字典矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其列为 $\\{a_{1}, \\dots, a_{n}\\}$。\n- 各列为单位范数：对所有 $j \\in \\{1, \\dots, n\\}$ 都有 $\\|a_{j}\\|_{2} = 1$。\n- 字典是等角的：对所有不同的索引 $i \\neq j$，其内积的绝对值为 $|\\langle a_{i}, a_{j} \\rangle| = c$，其中 $c$ 是一个满足 $c \\in [0,1)$ 的固定常数。\n- 互相关性的定义：$\\mu(A) = \\max_{i \\neq j} |\\langle a_{i}, a_{j} \\rangle|$。\n- 累积相关性（巴贝尔函数）的定义：$\\mu_{1}(s) = \\max_{|S|=s} \\max_{j \\notin S} \\sum_{i \\in S} |\\langle a_{j}, a_{i} \\rangle|$。\n\n**步骤 2：使用提取的已知条件进行验证**\n根据所需标准对问题进行评估：\n- **科学依据：** 该问题牢固地植根于压缩感知和稀疏表示的数学理论。字典、互相关性、累积相关性和等角性等概念在该领域是标准且定义明确的。没有违背科学或数学原理的地方。\n- **适定性：** 该问题是适定的。它提供了所有必要的定义和条件，以唯一确定量 $\\mu(A)$ 和 $\\mu_{1}(s)$。任务是根据给定的定义和字典的特定结构进行直接计算。\n- **客观性：** 语言是形式化、精确且客观的，使用了标准的数学术语。没有主观或模糊的陈述。\n\n该问题被发现不存在所有指明的缺陷（科学上不合理、不可形式化、设置不完整、不切实际、不适定、伪深刻、外部不可验证）。等角字典存在的假设是该领域理论分析中的一个标准手段。\n\n**步骤 3：结论与行动**\n该问题被视为**有效**。现在将提供一个完整的、有理有据的解答。\n\n**互相关性 $\\mu(A)$ 的推导**\n\n字典矩阵 $A$ 的互相关性定义为矩阵中任意两个不同列之间的最大绝对内积。其形式化定义是：\n$$\n\\mu(A) = \\max_{1 \\le i  j \\le n} |\\langle a_i, a_j \\rangle|\n$$\n问题陈述该字典是等角的。这是一个特定的结构特性，其定义条件是：对于任意一对不同的列索引 $i \\neq j$，它们内积的绝对值是一个常数 $c$。即：\n$$\n|\\langle a_i, a_j \\rangle| = c \\quad \\text{for all } i \\neq j\n$$\n将此条件直接代入互相关性的定义，我们观察到，取最大值的集合 $\\{|\\langle a_i, a_j \\rangle| : i \\neq j\\}$ 只包含一个元素，即常数 $c$。因此，这个集合的最大值显然是 $c$。\n$$\n\\mu(A) = \\max_{i \\neq j} \\{c\\} = c\n$$\n因此，对于一个等角字典，其互相关性恰好是两两之间的绝对内积的常数值。\n\n**累积相关性 $\\mu_{1}(s)$ 的推导**\n\n累积相关性，或巴贝尔函数，$\\mu_{1}(s)$，衡量单个列与另外 $s$ 个列组成的集合之间的最大累积相关性。其形式化定义是：\n$$\n\\mu_{1}(s) = \\max_{S \\subset \\{1, \\dots, n\\}, |S|=s} \\left( \\max_{j \\notin S} \\sum_{i \\in S} |\\langle a_j, a_i \\rangle| \\right)\n$$\n为了计算给定等角字典的这个值，我们首先分析最内层的和 $\\sum_{i \\in S} |\\langle a_j, a_i \\rangle|$，其中 $S$ 是一个基数为 $|S|=s$ 的任意索引集合，而 $j$ 是一个满足 $j \\notin S$ 的任意索引。\n\n根据字典的等角特性，对于任何索引 $i \\in S$ 和任何索引 $j \\notin S$，索引 $i$ 和 $j$ 必然是不同的。问题陈述保证了对于任何不同的索引，都有 $|\\langle a_j, a_i \\rangle| = c$。\n因此，求和中的每一项都等于常数 $c$：\n$$\n|\\langle a_j, a_i \\rangle| = c \\quad \\text{for all } i \\in S \\text{ and } j \\notin S\n$$\n现在，这个和可以重写为：\n$$\n\\sum_{i \\in S} |\\langle a_j, a_i \\rangle| = \\sum_{i \\in S} c\n$$\n由于集合 $S$ 恰好包含 $s$ 个元素（即 $|S|=s$），该和由 $s$ 个相同的项组成，每一项都等于 $c$。因此，这个和的值是：\n$$\n\\sum_{i \\in S} c = s \\cdot c\n$$\n关键的是，这个结果 $sc$ 与索引集 $S$ 的具体选择（只要其基数为 $s$）以及索引 $j$ 的具体选择（只要 $j$ 不在 $S$ 中）无关。\n\n现在，我们必须应用 $\\mu_1(s)$ 定义中的最大化算子：\n$$\n\\mu_{1}(s) = \\max_{|S|=s} \\left( \\max_{j \\notin S} (sc) \\right)\n$$\n由于待最大化的表达式 $sc$ 对于任何有效的 $S$ 和 $j$ 的选择都是一个常数值，因此最大化操作是平凡的。一个常数值的最大值就是它本身。\n$$\n\\max_{j \\notin S} (sc) = sc\n$$\n以及随后，\n$$\n\\max_{|S|=s} (sc) = sc\n$$\n这就完成了推导，并验证了问题陈述中提出的等式。对于一个参数为 $c$ 的等角字典，累积相关性函数是关于 $s$ 的一个线性函数：\n$$\n\\mu_{1}(s) = sc\n$$\n\n最终答案要求给出 $\\mu(A)$ 和 $\\mu_{1}(s)$ 的显式表达式。根据以上推导，它们是 $\\mu(A) = c$ 和 $\\mu_1(s) = sc$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nc  sc\n\\end{pmatrix}\n}\n$$", "id": "3441533"}, {"introduction": "OMP 的选择规则——最大化与残差的内积——对字典原子的范数（norm）十分敏感，而理论分析通常假设原子是单位范数的。本实践将探讨这一关键但常被忽视的现实问题，通过具体的例子展示非归一化列如何误导算法。你将通过实现和比较不同的归一化策略，更深刻地理解为什么列归一化是确保 OMP 稳定性和正确性的一个标准且必要的预处理步骤。[@problem_id:3441539]", "problem": "设 $A \\in \\mathbb{R}^{m \\times n}$ 是一个字典，其列 $a_j \\in \\mathbb{R}^m$ 不一定被归一化。考虑将正交匹配追踪（Orthogonal Matching Pursuit, OMP）算法应用于测量值 $y = A x^\\star + e$，其中 $x^\\star \\in \\mathbb{R}^n$ 是 $k$-稀疏的，$e \\in \\mathbb{R}^m$ 是噪声，且 $k \\in \\mathbb{N}$ 是已知的。OMP 在第 $t$ 次迭代的选择步骤中，会选择使内积绝对值 $|\\langle a_j, r^{(t)} \\rangle|$ 最大化的索引 $j$，其中 $r^{(t)}$ 是当前的残差。这个选择规则对列范数 $\\|a_j\\|_2$ 很敏感，因为 $|\\langle a_j, r^{(t)} \\rangle|$ 会随着 $\\|a_j\\|_2$ 缩放。定义以下相干性概念：\n- 标准互相关性（在列归一化后计算）为\n$$\n\\mu(A) = \\max_{i \\neq j} \\left| \\frac{a_i^\\top a_j}{\\|a_i\\|_2 \\, \\|a_j\\|_2} \\right|.\n$$\n- 未归一化相干性为\n$$\n\\tilde{\\mu}(A) = \\max_{i \\neq j} \\left| a_i^\\top a_j \\right|.\n$$\n为了减轻列范数不均匀的影响，考虑一个由 $\\alpha \\in [0,1]$ 参数化的对角缩放的幂归一化族：构建缩放后的字典 $B_\\alpha$，其列为 $b_j = a_j / \\|a_j\\|_2^{\\alpha}$。注意，$\\alpha = 0$ 会使 $A$ 保持不变，$\\alpha = 1$ 会将所有列归一化为单位范数，而中间的 $\\alpha$ 值会部分减弱列范数的差异。\n\n您的任务是设计并实现一个程序，对于一组固定的问题实例测试套件，通过以下方式量化列归一化如何改变 OMP 的正确性和稳定性：\n1. 计算 $\\mu(A)$ 和 $\\tilde{\\mu}(A)$。\n2. 在 $B_\\alpha$ 上针对 $\\alpha \\in \\{0, 1\\}$ 的选择运行 OMP，以获得两个正确性指标（精确支撑集恢复）。\n3. 在 $\\alpha \\in \\{0, 0.5, 1\\}$ 中搜索能够恢复精确支撑集恢复的最小指数 $\\alpha^\\star$。\n\n使用以下精确定义和假设：\n- OMP 迭代 $k$ 步，其中 $k$ 是真实的稀疏度；在每一步中，它从未被选择的索引中选择一个新的列，该列使得 $|\\langle b_j, r^{(t)} \\rangle|$ 最大化，并通过选择最小的索引来打破平局。每次选择后，它会在选定的支撑集上重新计算最小二乘拟合，并更新残差。\n- 精确支撑集恢复意味着 OMP 在 $k$ 次迭代后选择的索引集合等于 $x^\\star$ 的支撑集。\n- 对于每个测试用例，在 $A$ 的列归一化版本上计算 $\\mu(A)$（即，在将每列缩放至单位范数后），并在原始未缩放的 $A$ 上计算 $\\tilde{\\mu}(A)$。\n- 噪声 $e$ 是任意的，但对每个测试用例是固定的；本问题中没有物理单位。\n\n测试套件：\n- 测试用例 T1（具有强烈范数不平衡的理想路径，会误导未归一化的 OMP）：\n  - $m = 3$, $n = 5$。\n  - 列：\n    - $a_1 = (1, 0, 0)^\\top$，\n    - $a_2 = (0, 1, 0)^\\top$，\n    - $a_3 = (0, 0, 1)^\\top$，\n    - $a_4 = (1, 1, 0)^\\top$，\n    - $a_5 = 10 \\cdot (0.25, 0.25, \\sqrt{0.875})^\\top$。\n  - 信号：$x^\\star = (1, 1, 0, 0, 0)^\\top$，稀疏度 $k = 2$。\n  - 噪声：$e = (0, 0, 0)^\\top$。\n\n- 测试用例 T2（原始缩放下的边界情况；小噪声会错误地打破平局，除非进行缩放）：\n  - $m = 3$, $n = 3$。\n  - 列：\n    - $a_1 = (1, 0, 0)^\\top$，\n    - $a_2 = 5 \\cdot (0.2, 0, \\sqrt{0.96})^\\top$，\n    - $a_3 = (0, 1, 0)^\\top$。\n  - 信号：$x^\\star = (1, 0, 0)^\\top$，稀疏度 $k = 1$。\n  - 噪声：$e = (0, 0, 0.02)^\\top$。\n\n- 测试用例 T3（中等相干性和轻微的范数变化；原始和归一化的 OMP 都是稳定的）：\n  - $m = 4$, $n = 6$。\n  - 列：\n    - $a_1 = (1, 0, 0, 0)^\\top$，\n    - $a_2 = (0, 1, 0, 0)^\\top$，\n    - $a_3 = (0, 0, 1, 0)^\\top$，\n    - $a_4 = (0, 0, 0, 1)^\\top$，\n    - $a_5 = 0.5 \\cdot (1, 1, 0, 0)^\\top$，\n    - $a_6 = 0.5 \\cdot (0, 1, 1, 0)^\\top$。\n  - 信号：$x^\\star = (1.2, 0, 0.8, 0, 0, 0)^\\top$，稀疏度 $k = 2$。\n  - 噪声：$e = (0.01, -0.02, 0.015, 0)^\\top$。\n\n程序要求：\n- 对于每个测试用例，程序必须输出一个列表 $[\\mu(A), \\tilde{\\mu}(A), s_0, s_1, \\alpha^\\star]$，其中 $s_0$ 是一个布尔值，表示使用 $\\alpha = 0$ 的 OMP 是否实现精确支撑集恢复，$s_1$ 是一个布尔值，表示使用 $\\alpha = 1$ 的 OMP 是否实现精确支撑集恢复，$\\alpha^\\star$ 是 $\\{0, 0.5, 1\\}$ 中使 OMP 实现精确支撑集恢复的最小值；如果无一成功，则为 $\\alpha^\\star$ 输出 $-1$。值 $\\mu(A)$ 和 $\\tilde{\\mu}(A)$ 必须是浮点数。\n- 您的程序应生成单行输出，其中包含 T1、T2 和 T3 的结果，格式为由方括号括起来的、以逗号分隔的各测试用例列表，例如：$[[\\dots],[\\dots],[\\dots]]$。", "solution": "该问题要求分析正交匹配追踪（OMP）算法在给定传感矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的不同列归一化策略下的性能。性能评估基于从测量值 $y = A x^\\star + e$ 中精确恢复已知 $k$-稀疏信号 $x^\\star$ 的支撑集，其中 $e$ 是噪声。\n\n任务的核心是实现 OMP，并在一个缩放字典族 $B_\\alpha$ 上进行测试，其中 $B_\\alpha$ 的列由 $b_j = a_j / \\|a_j\\|_2^{\\alpha}$ 给出，$\\alpha \\in \\{0, 0.5, 1\\}$。我们还将为原始字典 $A$ 计算两个相干性度量：标准互相关性 $\\mu(A)$ 和未归一化相干性 $\\tilde{\\mu}(A)$。\n\n解决方案按以下步骤进行：\n$1$. 对每个测试用例，我们首先按定义计算相干性 $\\mu(A)$ 和 $\\tilde{\\mu}(A)$。\n$2$. 然后我们实现 OMP 算法。\n$3$. 对每个指定的 $\\alpha \\in \\{0, 0.5, 1\\}$ 值，我们运行 OMP 进行 $k$ 次迭代。\n$4$. 对每次运行，我们将 OMP 选择的支撑集与 $x^\\star$ 的真实支撑集进行比较，以确定恢复是否精确。\n$5$. 最后，我们将结果整理成指定的输出格式。\n\n**1. 相干性计算**\n- 标准互相关性 $\\mu(A)$ 定义为 $\\mu(A) = \\max_{i \\neq j} \\left| \\frac{a_i^\\top a_j}{\\|a_i\\|_2 \\, \\|a_j\\|_2} \\right|$。这等同于计算 $A$ 的列归一化版本的格拉姆矩阵的最大非对角线绝对值项。设 $A'$ 是列为 $a'_j = a_j / \\|a_j\\|_2$ 的矩阵。则 $\\mu(A)$ 是 $G' = (A')^\\top A'$ 的非对角线元素的最大绝对值。\n- 未归一化相干性 $\\tilde{\\mu}(A)$ 定义为 $\\tilde{\\mu}(A) = \\max_{i \\neq j} | a_i^\\top a_j |$。这是格拉姆矩阵 $G = A^\\top A$ 的最大非对角线绝对值项。\n\n**2. 正交匹配追踪（OMP）实现**\nOMP 算法被实现用于恢复 $k$-稀疏信号 $x^\\star$ 的支撑集。\n- **输入**：字典 $A \\in \\mathbb{R}^{m \\times n}$，测量向量 $y \\in \\mathbb{R}^m$，稀疏度 $k$，以及归一化指数 $\\alpha$。\n- **初始化**：\n    - 残差初始化为测量向量：$r^{(0)} = y$。\n    - 选定索引的集合为空：$S_0 = \\emptyset$。\n    - 形成一个缩放后的字典 $B_\\alpha$，其列为 $b_j = a_j / \\|a_j\\|_2^{\\alpha}$。对于范数为 0 的列，我们可以认为其范数为 1 以避免除以零。\n- **迭代**：对于 $t = 1, \\dots, k$：\n    - **选择**：找到一个尚未在支撑集 $S_{t-1}$ 中的索引 $j_t$，该索引能最大化与当前残差的相关性：\n    $$\n    j_t = \\arg\\max_{j \\notin S_{t-1}} |\\langle b_j, r^{(t-1)} \\rangle|\n    $$\n    如果出现平局，则选择最小的索引 $j$。\n    - **更新支撑集**：新的支撑集为 $S_t = S_{t-1} \\cup \\{j_t\\}$。\n    - **更新残差**：通过在当前支撑集上使用原始字典 $A$ 解决最小二乘问题来找到新的信号估计：\n    $$\n    x_{S_t} = \\arg\\min_{z \\in \\mathbb{R}^{|S_t|}} \\|y - A_{S_t} z\\|_2^2 = (A_{S_t}^\\top A_{S_t})^{-1} A_{S_t}^\\top y\n    $$\n    其中 $A_{S_t}$ 是包含由 $S_t$ 索引的列的 $A$ 的子矩阵。使用伪逆以实现稳健的实现。新的残差是投影误差：\n    $$\n    r^{(t)} = y - A_{S_t} x_{S_t}\n    $$\n- **输出**：最终的支撑集 $S_k$。\n\n**3. 测试用例分析**\n\n**测试用例 T1：**\n- $A \\in \\mathbb{R}^{3 \\times 5}$，$k=2$，$x^\\star = (1, 1, 0, 0, 0)^\\top$，$e=(0,0,0)^\\top$。\n- 真实支撑集 $S^\\star=\\{0, 1\\}$。\n- $y = 1 \\cdot a_1 + 1 \\cdot a_2 = (1, 1, 0)^\\top$。\n- 列范数为 $\\|a_1\\|_2=1, \\|a_2\\|_2=1, \\|a_3\\|_2=1, \\|a_4\\|_2=\\sqrt{2}, \\|a_5\\|_2=10$。\n- **相干性**：\n    - $\\mu(A) = \\max_{i \\neq j} |(a_i/\\|a_i\\|_2)^\\top (a_j/\\|a_j\\|_2)| = |(a_3/\\|a_3\\|_2)^\\top(a_5/\\|a_5\\|_2)| = \\sqrt{0.875} \\approx 0.9354$。\n    - $\\tilde{\\mu}(A) = \\max_{i \\neq j} |a_i^\\top a_j| = |a_3^\\top a_5| = 10\\sqrt{0.875} \\approx 9.3541$。\n- **OMP 运行**：\n    - $\\alpha=0$：选择规则使用 $|\\langle a_j, y \\rangle|$。相关性为 $\\{1, 1, 0, 2, 5\\}$。在第一步中，选择 $a_5$（索引 4）。这是不正确的。$s_0 = \\text{False}$。\n    - $\\alpha=1$：选择规则使用 $|\\langle a_j/\\|a_j\\|_2, y \\rangle|$。相关性为 $\\{1, 1, 0, 2/\\sqrt{2}, 5/10\\} = \\{1, 1, 0, \\sqrt{2}, 0.5\\}$。在第一步中，选择 $a_4$（索引 3）。这是不正确的。$s_1 = \\text{False}$。\n    - $\\alpha=0.5$：选择规则使用 $|\\langle a_j/\\|a_j\\|_2^{0.5}, y \\rangle|$。相关性为 $\\{1, 1, 0, 2/2^{0.25}, 5/\\sqrt{10}\\} \\approx \\{1, 1, 0, 1.682, 1.581\\}$。在第一步中，选择 $a_4$（索引 3）。这是不正确的。\n- **结果**：对于 $\\alpha \\in \\{0, 0.5, 1\\}$，OMP 都未能恢复正确的支撑集。因此，$\\alpha^\\star = -1$。\n- **输出**：$[\\sqrt{0.875}, 10\\sqrt{0.875}, \\text{False}, \\text{False}, -1.0]$\n\n**测试用例 T2：**\n- $A \\in \\mathbb{R}^{3 \\times 3}$，$k=1$，$x^\\star = (1, 0, 0)^\\top$，$e=(0, 0, 0.02)^\\top$。\n- 真实支撑集 $S^\\star=\\{0\\}$。\n- $y = 1 \\cdot a_1 + e = (1, 0, 0.02)^\\top$。\n- 列范数为 $\\|a_1\\|_2=1, \\|a_2\\|_2=5, \\|a_3\\|_2=1$。\n- **相干性**：\n    - $\\mu(A) = |(a_1/\\|a_1\\|_2)^\\top (a_2/\\|a_2\\|_2)| = 1/5 = 0.2$。\n    - $\\tilde{\\mu}(A) = |a_1^\\top a_2| = 1$。\n- **OMP 运行**（$k=1$ 步）：\n    - $\\alpha=0$：我们根据 $|\\langle a_j, y \\rangle|$ 进行选择。\n        - $|\\langle a_1, y \\rangle| = 1$。\n        - $|\\langle a_2, y \\rangle| = |1 \\cdot 1 + (5\\sqrt{0.96}) \\cdot 0.02| \\approx 1.098$。\n        - $|\\langle a_3, y \\rangle| = 0$。\n        由于其大范数放大了噪声分量，选择了 $a_2$（索引 1）。不正确。$s_0 = \\text{False}$。\n    - $\\alpha=1$：我们根据 $|\\langle a_j/\\|a_j\\|_2, y \\rangle|$ 进行选择。\n        - $|\\langle a'_1, y \\rangle| = 1/1=1$。\n        - $|\\langle a'_2, y \\rangle| \\approx 1.098/5 \\approx 0.22$。\n        - $|\\langle a'_3, y \\rangle| = 0$。\n        选择 $a_1$（索引 0）。正确。$s_1 = \\text{True}$。\n    - $\\alpha=0.5$：我们根据 $|\\langle a_j/\\|a_j\\|_2^{0.5}, y \\rangle|$ 进行选择。\n        - 对 $a_1$ 的相关性：$1/\\sqrt{1}=1$。\n        - 对 $a_2$ 的相关性：$1.098/\\sqrt{5} \\approx 0.49$。\n        选择 $a_1$（索引 0）。正确。\n- **结果**：$\\alpha=0$ 的 OMP 失败，而 $\\alpha=0.5$ 和 $\\alpha=1$ 成功。最小的成功指数是 $\\alpha^\\star=0.5$。\n- **输出**：$[0.2, 1.0, \\text{False}, \\text{True}, 0.5]$\n\n**测试用例 T3：**\n- $A \\in \\mathbb{R}^{4 \\times 6}$，$k=2$，$x^\\star=(1.2, 0, 0.8, 0, 0, 0)^\\top$，$e=(0.01, -0.02, 0.015, 0)^\\top$。\n- 真实支撑集 $S^\\star=\\{0, 2\\}$。\n- $y=1.2 a_1 + 0.8 a_3 + e = (1.21, -0.02, 0.815, 0)^\\top$。\n- 列范数为 $\\|a_1..a_4\\|_2=1$, $\\|a_5\\|_2=\\|a_6\\|_2=1/\\sqrt{2}$。\n- **相干性**：\n    - $\\mu(A) = |a_1^\\top (a_5/\\|a_5\\|_2)| = 0.5 / (1/\\sqrt{2}) = 1/\\sqrt{2} \\approx 0.7071$。\n    - $\\tilde{\\mu}(A) = |a_1^\\top a_5| = 0.5$。\n- **OMP 运行**：\n    - $\\alpha=0$：\n        - 迭代 1：$r^{(0)} = y$。相关性 $|\\langle a_j, y \\rangle|$ 为 $\\{1.21, 0.02, 0.815, 0, 0.595, 0.3975\\}$。选择索引 $0$。$S_1=\\{0\\}$。\n        - 迭代 2：$r^{(1)} = y - (a_1^\\top y / \\|a_1\\|_2^2) a_1 = (0, -0.02, 0.815, 0)^\\top$。对于 $j \\notin S_1$，相关性 $|\\langle a_j, r^{(1)} \\rangle|$ 为 $\\{0.02, 0.815, 0, 0.01, 0.3975\\}$。选择索引 $2$。\n        - 最终支撑集 $\\{0, 2\\}$。正确。$s_0 = \\text{True}$。\n    - $\\alpha=1$：\n        - 迭代 1：相关性 $|\\langle a_j/\\|a_j\\|_2, y \\rangle|$ 为 $\\{1.21, 0.02, 0.815, 0, 0.595\\sqrt{2}, 0.3975\\sqrt{2}\\} \\approx \\{1.21, 0.02, 0.815, 0, 0.841, 0.562\\}$。选择索引 $0$。\n        - 迭代 2：残差 $r^{(1)}$ 相同。相关性 $|\\langle a_j/\\|a_j\\|_2, r^{(1)} \\rangle|$ 为 $\\{0.02, 0.815, 0, 0.01\\sqrt{2}, 0.3975\\sqrt{2}\\} \\approx \\{0.02, 0.815, 0, 0.014, 0.562\\}$。选择索引 $2$。\n        - 最终支撑集 $\\{0, 2\\}$。正确。$s_1 = \\text{True}$。\n- **结果**：$\\alpha=0$ 和 $\\alpha=1$ 的 OMP 都成功。最小的成功指数是 $\\alpha^\\star=0.0$。\n- **输出**：$[1/\\sqrt{2}, 0.5, \\text{True}, \\text{True}, 0.0]$", "answer": "```python\nimport numpy as np\n\ndef run_omp(A, y, k, alpha):\n    \"\"\"\n    Implements the Orthogonal Matching Pursuit algorithm.\n\n    Args:\n        A (np.ndarray): The dictionary/sensing matrix (m x n).\n        y (np.ndarray): The measurement vector (m,).\n        k (int): The sparsity level.\n        alpha (float): The power-normalization exponent.\n\n    Returns:\n        set: The set of selected indices (the support).\n    \"\"\"\n    m, n = A.shape\n\n    # 1. Scale the dictionary to form B_alpha\n    norms = np.linalg.norm(A, axis=0)\n    # Avoid division by zero for zero-norm columns (not in test cases).\n    norms[norms == 0] = 1.0\n    B = A / (norms**alpha)\n\n    support = []\n    residual = y.copy()\n    available_indices = list(range(n))\n\n    for _ in range(k):\n        # 2. Selection step: find index that maximizes correlation\n        if not available_indices:\n            break\n        \n        correlations = np.abs(B[:, available_indices].T @ residual)\n        max_corr = np.max(correlations)\n\n        # Tie-breaking: find all indices with max correlation\n        # and choose the one with the smallest original index.\n        max_indices_in_subset = np.where(np.isclose(correlations, max_corr))[0]\n        \n        # Map back to original indices\n        original_indices_with_max_corr = [available_indices[i] for i in max_indices_in_subset]\n        selected_index = min(original_indices_with_max_corr)\n        \n        # 3. Update support and available indices\n        support.append(selected_index)\n        available_indices.remove(selected_index)\n        \n        # 4. Update residual via least-squares on original A\n        A_S = A[:, sorted(support)]\n        # Solve least squares: x = (A_S^T A_S)^-1 A_S^T y\n        # Using pinv is robust.\n        x_S = np.linalg.pinv(A_S) @ y\n        residual = y - A_S @ x_S\n        \n    return set(support)\n\ndef analyze_case(A, x_star, e):\n    \"\"\"\n    Analyzes a single test case for OMP performance.\n\n    Args:\n        A (np.ndarray): The dictionary matrix.\n        x_star (np.ndarray): The ground truth sparse signal.\n        e (np.ndarray): The noise vector.\n\n    Returns:\n        list: A list containing [mu(A), tilde_mu(A), s0, s1, alpha_star].\n    \"\"\"\n    # Problem setup from givens\n    m, n = A.shape\n    k = np.count_nonzero(x_star)\n    true_support = set(np.flatnonzero(x_star))\n    y = A @ x_star + e\n\n    # 1. Compute coherences\n    # mu(A) on column-normalized A\n    col_norms = np.linalg.norm(A, axis=0)\n    col_norms[col_norms == 0] = 1.0 # Avoid division by zero\n    A_norm = A / col_norms\n    G_norm = A_norm.T @ A_norm\n    np.fill_diagonal(G_norm, 0)\n    mu_A = np.max(np.abs(G_norm))\n    \n    # tilde_mu(A) on unscaled A\n    G_un = A.T @ A\n    np.fill_diagonal(G_un, 0)\n    tilde_mu_A = np.max(np.abs(G_un))\n    \n    # 2  3. Run OMP for alpha={0, 1} and search for alpha_star\n    s0, s1 = False, False\n    alpha_star = -1.0\n    \n    alphas_to_test = [0.0, 0.5, 1.0]\n    \n    for alpha in alphas_to_test:\n        omp_support = run_omp(A, y, k, alpha)\n        is_correct = (omp_support == true_support)\n        \n        if alpha == 0.0:\n            s0 = is_correct\n        if alpha == 1.0:\n            s1 = is_correct\n\n        if is_correct and alpha_star == -1.0:\n            alpha_star = alpha\n            \n    return [mu_A, tilde_mu_A, s0, s1, alpha_star]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run analysis, and print results.\n    \"\"\"\n    # Test case T1\n    A1 = np.array([\n        [1.0, 0.0, 0.0, 1.0, 2.5],\n        [0.0, 1.0, 0.0, 1.0, 2.5],\n        [0.0, 0.0, 1.0, 0.0, 10 * np.sqrt(0.875)]\n    ])\n    x_star1 = np.array([1.0, 1.0, 0.0, 0.0, 0.0])\n    e1 = np.array([0.0, 0.0, 0.0])\n    \n    # Test case T2\n    A2 = np.array([\n        [1.0, 5.0 * 0.2, 0.0],\n        [0.0, 0.0,         1.0],\n        [0.0, 5.0 * np.sqrt(0.96), 0.0]\n    ])\n    x_star2 = np.array([1.0, 0.0, 0.0])\n    e2 = np.array([0.0, 0.0, 0.02])\n\n    # Test case T3\n    A3 = np.array([\n        [1.0, 0.0, 0.0, 0.0, 0.5, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.5, 0.5],\n        [0.0, 0.0, 1.0, 0.0, 0.0, 0.5],\n        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n    ])\n    x_star3 = np.array([1.2, 0.0, 0.8, 0.0, 0.0, 0.0])\n    e3 = np.array([0.01, -0.02, 0.015, 0.0])\n\n    test_cases = [\n        (A1, x_star1, e1),\n        (A2, x_star2, e2),\n        (A3, x_star3, e3)\n    ]\n    \n    results = []\n    for A, x_star, e in test_cases:\n        res = analyze_case(A, x_star, e)\n        results.append(res)\n    \n    # Format the final output string as specified\n    # Example: [[0.935,9.354,False,False,-1.0],[0.2,1.0,False,True,0.5],[0.707,0.5,True,True,0.0]]\n    # Python's str(True) is 'True', which is fine. The problem doesn't specify lowercase.\n    outer_list = []\n    for res_list in results:\n        # Convert each item in the inner list to a string\n        inner_list_str = [str(item) for item in res_list]\n        outer_list.append(f\"[{','.join(inner_list_str)}]\")\n    \n    final_output = f\"[{','.join(outer_list)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3441539"}, {"introduction": "当字典中的原子高度相关时，标准的贪婪选择策略可能导致 OMP 失败。本项高级实践将引导你从分析 OMP 的局限性，转向设计一个更为鲁棒的算法。你将实现一个“重加权 OMP”，它在选择原子时会主动惩罚那些与已选原子高度相关的候选者，从而在复杂的恢复场景中做出更明智的决策。这个练习体现了算法设计的一个核心思想：将问题的先验结构知识（在此即为相关性）融入算法本身，以提升其性能。[@problem_id:3441569]", "problem": "考虑一个具有列归一化原子 $A \\in \\mathbb{R}^{m \\times n}$（列为 $\\{a_j\\}_{j=1}^n$）的传感矩阵，一个支撑集为 $S \\subset \\{1,\\dots,n\\}$ 且大小为 $|S| = k$ 的 $k$-稀疏信号 $x^\\star \\in \\mathbb{R}^n$，以及测量值 $y = A x^\\star + e$，其中 $e \\in \\mathbb{R}^m$ 是加性噪声。目标是分析并实现正交匹配追踪 (OMP) 的一种变体，该变体使用重加权相关性来惩罚与已选原子相干的候选原子，并经验性地研究在互相关性条件下正确性的改进情况。\n\n使用以下基础定义作为基础：\n- 互相关性 $\\mu(A) := \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$，其中 $\\langle \\cdot, \\cdot \\rangle$ 表示标准欧几里得内积。\n- 在标准正交匹配追踪 (OMP) 中，在第 $t$ 次迭代时，已选索引集为 $T_t$，残差为 $r_t = y - A_{T_t} x_{T_t}$，相关性向量为 $c_t = A^\\top r_t$，下一个选定的索引 $j_t \\notin T_t$ 是通过在 $j \\notin T_t$ 上最大化 $|\\langle a_j, r_t \\rangle|$ 来选择的。\n\n在重加权选择规则中，对于每个候选索引 $j \\notin T_t$，定义累积相干性分数\n$$\ns_j(t) := \\sum_{i \\in T_t} \\big|\\langle a_j, a_i \\rangle\\big|,\n$$\n以及一个正的重加权参数 $\\gamma  0$。提出一个权重函数\n$$\nw_j(t) := \\frac{1}{1 + \\gamma\\, s_j(t)}.\n$$\n修改 OMP 的选择步骤为选择\n$$\nj_t \\in \\arg\\max_{j \\notin T_t} \\Big( w_j(t)\\, \\big| \\langle a_j, r_t \\rangle \\big| \\Big).\n$$\n迭代过程包括在更新后的集合 $T_{t+1} := T_t \\cup \\{j_t\\}$ 上进行最小二乘拟合，更新残差 $r_{t+1} := y - A_{T_{t+1}} x_{T_{t+1}}$，并在 $k$ 次迭代后终止。\n\n您的任务：\n- 实现标准的 OMP 和上文定义的重加权 OMP 变体。\n- 对于每个测试用例，构造矩阵 $A$、稀疏支撑集 $S$ 以及支撑集上的系数以形成 $x^\\star$，生成 $y = A x^\\star + e$，其中噪声向量 $e$ 满足 $\\|e\\|_2 = \\eta \\|y_0\\|_2$（其中 $y_0 := A x^\\star$）且噪声水平为指定的 $\\eta \\ge 0$，两种算法都精确运行 $k$ 次迭代，并返回支撑集 $T_{\\text{std}}$ 和 $T_{\\text{rw}}$。\n- 将支撑集恢复误差定义为整数\n$$\n\\text{err}(T,S) := k - |T \\cap S|,\n$$\n即在 $k$ 次选择中不正确的选择次数。将改进量报告为整数\n$$\n\\Delta := \\text{err}(T_{\\text{std}},S) - \\text{err}(T_{\\text{rw}},S).\n$$\n一个正的 $\\Delta$ 值表示重加权 OMP 的支撑集恢复性能比标准 OMP 好，错误数减少了 $\\Delta$；零值表示性能相当；负值表示性能更差。\n\n测试套件规范和构造细节：\n- $A$ 的所有列必须缩放至单位 $\\ell_2$-范数。为保证可复现性，请使用固定的伪随机种子。\n- 对于聚类相干字典，通过将列 $a_j$ 替换为以下形式来创建相干对 $(i,j)$\n$$\na_j \\leftarrow \\frac{\\alpha\\, a_i + \\sqrt{1-\\alpha^2}\\, u}{\\|\\alpha\\, a_i + \\sqrt{1-\\alpha^2}\\, u\\|_2},\n$$\n其中 $u$ 是一个随机高斯向量，它与 $a_i$ 正交化以满足 $\\langle a_i, u \\rangle \\approx 0$，并且 $\\alpha \\in (0,1)$ 控制了引入的成对相干性 $|\\langle a_i, a_j \\rangle| \\approx \\alpha$。\n- 将 $x^\\star$ 在 $S$ 上的非零项设置为正值，这些值在 $k$ 个支撑集索引上从最大到最小线性递减，以避免平凡的相等情况。\n\n使用以下四个测试用例：\n- 测试用例 1（低相干性，无噪声）：\n    - 维度：$m = 60$，$n = 120$，$k = 8$，种子 $7$。\n    - 字典构造：独立同分布的零均值单位方差高斯条目，然后进行列归一化。\n    - 支撑集：$S = \\{5,17,23,42,55,63,88,107\\}$。\n    - S 上的系数：在八个索引上从 $1.0$ 线性递减到 $0.3$。\n    - 噪声水平：$\\eta = 0$。\n    - 重加权参数：$\\gamma = 0.5$。\n- 测试用例 2（中等聚类相干性，小噪声）：\n    - 维度：$m = 60$，$n = 120$，$k = 8$，种子 $11$。\n    - 字典构造：首先同上构造高斯字典，然后在索引 $(10,11)$、$(30,31)$、$(70,71)$、$(95,96)$ 处引入相干对，$\\alpha = 0.98$，并对这些列对进行重新归一化。\n    - 支撑集：$S = \\{10,30,70,95,20,50,90,110\\}$。\n    - S 上的系数：在八个索引上从 $1.0$ 线性递减到 $0.3$。\n    - 噪声水平：$\\eta = 0.02$，相对于 $\\|y_0\\|_2$。\n    - 重加权参数：$\\gamma = 1.0$。\n- 测试用例 3（高聚类相干性，中等噪声）：\n    - 维度：$m = 60$，$n = 120$，$k = 10$，种子 $19$。\n    - 字典构造：首先构造高斯字典，然后在索引 $(5,6)$、$(25,26)$、$(45,46)$、$(65,66)$、$(85,86)$ 处引入相干对，$\\alpha = 0.99$，并对这些列对进行重新归一化。\n    - 支撑集：$S = \\{5,25,45,65,85,12,34,56,78,100\\}$。\n    - S 上的系数：在十个索引上从 $1.0$ 线性递减到 $0.2$。\n    - 噪声水平：$\\eta = 0.05$，相对于 $\\|y_0\\|_2$。\n    - 重加权参数：$\\gamma = 1.5$。\n- 测试用例 4（边界情况，不重加权）：\n    - 使用与测试用例 2 完全相同的构造，但设置 $\\gamma = 0$ 以验证与标准 OMP 的性能相同。\n\n您的程序应生成单行输出，其中包含四个测试用例的改进量 $\\Delta$，形式为用方括号括起来的逗号分隔列表，例如 $[d_1,d_2,d_3,d_4]$，其中每个 $d_i$ 是按上述定义计算的整数。不应打印任何其他文本。本问题中没有物理单位或角度单位；所有报告的量均为无量纲量。", "solution": "用户提供了一个来自稀疏信号恢复和压缩感知领域的有效且适定的问题陈述。该问题具有科学依据，经过形式化指定，并且可通过计算进行验证。\n\n目标是实现并评估正交匹配追踪（OMP）的一种重加权变体（RW-OMP），该变体旨在当传感矩阵（或字典）$A$ 包含高度相干的列时，提高支撑集恢复的性能。我们将在一个包含不同字典相干性和噪声水平的测试套件上，将这种重加权 OMP（RW-OMP）与标准 OMP 进行比较。\n\n所要解决的根本挑战是，当字典原子不是相互非相干时，标准 OMP 的性能会显著下降。由 $\\mu(A) = \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$ 量化的高相干性，可能导致 OMP 错误地选择一个不在真实支撑集 $S$ 中、但与 $S$ 中的某个原子高度相关的原子。\n\n算法流程如下：\n\n在第 $t$ 次迭代（从 $t=1$ 到 $k$）：\n令 $T_{t-1}$ 为先前迭代中选定的索引集合，令 $r_{t-1}$ 为相应的残差。初始时，$T_0 = \\emptyset$ 且 $r_0 = y$。\n\n**1. 标准正交匹配追踪 (OMP)**\n标准 OMP 算法贪婪地选择与当前残差最相关的原子。\n- **相关性计算**：计算所有原子与残差的内积：$c_j = \\langle a_j, r_{t-1} \\rangle$，适用于所有 $j \\in \\{1, \\dots, n\\} \\setminus T_{t-1}$。\n- **索引选择**：识别使绝对相关性最大化的索引 $j_t$：\n$$\nj_t = \\arg\\max_{j \\notin T_{t-1}} |\\langle a_j, r_{t-1} \\rangle|\n$$\n- **支撑集更新**：扩充支撑集：$T_t = T_{t-1} \\cup \\{j_t\\}$。\n- **信号与残差更新**：通过求解一个限制在当前支撑集 $A_{T_t}$ 中的原子上的最小二乘问题，来形成新的信号估计 $x_t$：\n$$\nx_t = \\arg\\min_{z: \\text{supp}(z) \\subseteq T_t} \\|y - A z\\|_2^2\n$$\n解由 $x_{T_t} = A_{T_t}^\\dagger y$ 给出，其中 $A_{T_t}^\\dagger = (A_{T_t}^\\top A_{T_t})^{-1} A_{T_t}^\\top$ 是摩尔-彭若斯伪逆。新的残差是信号中无法被已选原子解释的剩余部分：\n$$\nr_t = y - A_{T_t} x_{T_t}\n$$\n\n**2. 重加权正交匹配追踪 (RW-OMP)**\n该重加权变体修改了选择步骤，以惩罚那些与已在支撑集 $T_{t-1}$ 中的原子高度相干的候选原子。\n- **相关性计算**：与标准 OMP 相同，计算 $c_j = \\langle a_j, r_{t-1} \\rangle$。\n- **重加权**：对于每个候选原子 $a_j$（其中 $j \\notin T_{t-1}$），计算其与已选原子的累积相干性：\n$$\ns_j(t-1) = \\sum_{i \\in T_{t-1}} |\\langle a_j, a_i \\rangle|\n$$\n然后使用给定的参数 $\\gamma > 0$ 计算一个权重因子：\n$$\nw_j(t-1) = \\frac{1}{1 + \\gamma s_j(t-1)}\n$$\n对于累积相干性低的原子，这个权重 $w_j(t-1)$ 接近 1；对于累积相干性高的原子，该权重则向 0 递减。\n- **索引选择**：下一个索引 $j_t$ 通过最大化加权相关性来选择：\n$$\nj_t = \\arg\\max_{j \\notin T_{t-1}} \\Big( w_j(t-1) \\cdot |\\langle a_j, r_{t-1} \\rangle| \\Big)\n$$\n- **支撑集与残差更新**：这些步骤与标准 OMP 完全相同。\n\n**仿真设置与评估**\n我们将按规定以编程方式构造四个测试用例。对于每个用例，我们将生成矩阵 $A$、稀疏信号 $x^\\star$ 以及测量向量 $y = A x^\\star + e$。噪声向量 $e$ 被缩放，使其范数 $\\|e\\|_2$ 是无噪声信号范数 $\\|y_0\\|_2 = \\|A x^\\star\\|_2$ 的一个特定比例 $\\eta$。\n字典由独立同分布（i.i.d.）的高斯条目生成，然后进行列归一化。对于具有结构化相干性的情况，特定列将被替换为一个现有列与一个正交随机向量的线性组合，以达到目标相干值 $\\alpha$。\n\n每种算法的性能通过支撑集恢复误差来衡量，该误差定义为算法选择的索引中不在真实支撑集 $S$ 内的索引数量：\n$$\n\\text{err}(T,S) = k - |T \\cap S|\n$$\n最终报告的度量是改进量 $\\Delta$，即标准方法和重加权方法的误差之差：\n$$\n\\Delta = \\text{err}(T_{\\text{std}}, S) - \\text{err}(T_{\\text{rw}}, S)\n$$\n正的 $\\Delta$ 值表示 RW-OMP 比标准 OMP 正确识别了更多的真实支撑集索引。$\\Delta=0$ 表示性能相同，这在测试用例 4 中是预期的，因为 $\\gamma=0$ 使得两种算法等价。", "answer": "```python\nimport numpy as np\n\ndef run_omp(A, y, k, gamma=0.0):\n    \"\"\"\n    Performs standard or reweighted Orthogonal Matching Pursuit.\n\n    Args:\n        A (np.ndarray): The sensing matrix (m x n) with unit-norm columns.\n        y (np.ndarray): The measurement vector (m x 1).\n        k (int): The sparsity level.\n        gamma (float): The reweighting parameter. gamma=0 corresponds to standard OMP.\n\n    Returns:\n        list: The estimated support set of size k.\n    \"\"\"\n    m, n = A.shape\n    T = []\n    r = y.copy()\n    available_indices = list(range(n))\n    \n    A_T_cache = np.zeros((m, k))\n\n    for t in range(k):\n        correlations = A.T @ r\n        \n        # Restrict correlations to available indices\n        current_correlations = correlations[available_indices]\n\n        if gamma > 0 and t > 0:\n            # Get submatrix of already selected atoms\n            A_selected = A_T_cache[:, :t]\n            \n            # Get submatrix of available atoms\n            A_available = A[:, available_indices]\n\n            # Calculate cumulative coherence scores for available indices\n            coherence_scores = np.sum(np.abs(A_available.T @ A_selected), axis=1)\n            \n            # Calculate weights\n            weights = 1.0 / (1.0 + gamma * coherence_scores)\n            \n            # Apply weights\n            objective = weights * np.abs(current_correlations)\n        else:\n            objective = np.abs(current_correlations)\n\n        # Find the index with the maximum objective value among available indices\n        best_idx_in_available = np.argmax(objective)\n        j_t = available_indices.pop(best_idx_in_available)\n        \n        T.append(j_t)\n        A_T_cache[:, t] = A[:, j_t]\n\n        # Update residual via least-squares\n        A_T = A_T_cache[:, :t+1]\n        x_T = np.linalg.pinv(A_T) @ y\n        r = y - A_T @ x_T\n        \n    return sorted(T)\n\ndef solve():\n    \"\"\"\n    Runs the full simulation for all test cases and prints the result.\n    \"\"\"\n    test_cases = [\n        # Case 1: low coherence, noise-free\n        {\n            \"m\": 60, \"n\": 120, \"k\": 8, \"seed\": 7,\n            \"coherent_pairs\": [], \"alpha\": 0,\n            \"S\": {5, 17, 23, 42, 55, 63, 88, 107},\n            \"coeffs\": (1.0, 0.3), \"eta\": 0.0, \"gamma\": 0.5\n        },\n        # Case 2: moderate clustered coherence, small noise\n        {\n            \"m\": 60, \"n\": 120, \"k\": 8, \"seed\": 11,\n            \"coherent_pairs\": [(10, 11), (30, 31), (70, 71), (95, 96)], \"alpha\": 0.98,\n            \"S\": {10, 30, 70, 95, 20, 50, 90, 110},\n            \"coeffs\": (1.0, 0.3), \"eta\": 0.02, \"gamma\": 1.0\n        },\n        # Case 3: high clustered coherence, moderate noise\n        {\n            \"m\": 60, \"n\": 120, \"k\": 10, \"seed\": 19,\n            \"coherent_pairs\": [(5, 6), (25, 26), (45, 46), (65, 66), (85, 86)], \"alpha\": 0.99,\n            \"S\": {5, 25, 45, 65, 85, 12, 34, 56, 78, 100},\n            \"coeffs\": (1.0, 0.2), \"eta\": 0.05, \"gamma\": 1.5\n        },\n        # Case 4: boundary, no reweighting (same as case 2 but gamma=0)\n        {\n            \"m\": 60, \"n\": 120, \"k\": 8, \"seed\": 11,\n            \"coherent_pairs\": [(10, 11), (30, 31), (70, 71), (95, 96)], \"alpha\": 0.98,\n            \"S\": {10, 30, 70, 95, 20, 50, 90, 110},\n            \"coeffs\": (1.0, 0.3), \"eta\": 0.02, \"gamma\": 0.0\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        m, n, k, seed = case[\"m\"], case[\"n\"], case[\"k\"], case[\"seed\"]\n        np.random.seed(seed)\n\n        # 1. Construct the dictionary A\n        A = np.random.randn(m, n)\n        # Column normalize\n        A /= np.linalg.norm(A, axis=0)\n\n        # Induce coherent pairs\n        for i, j in case[\"coherent_pairs\"]:\n            # Convert to 0-based index\n            i_idx, j_idx = i - 1, j - 1\n            alpha = case[\"alpha\"]\n            \n            a_i = A[:, i_idx]\n            u_rand = np.random.randn(m)\n            # Orthogonalize u w.r.t. a_i using Gram-Schmidt\n            u_ortho = u_rand - (u_rand @ a_i) * a_i\n            u_norm = u_ortho / np.linalg.norm(u_ortho)\n            \n            # Construct new coherent column\n            a_j_new = alpha * a_i + np.sqrt(1 - alpha**2) * u_norm\n            # Replace and re-normalize (safeguard)\n            A[:, j_idx] = a_j_new / np.linalg.norm(a_j_new)\n\n        # 2. Construct the sparse signal x_star\n        # Convert 1-based support to 0-based\n        S_true = {s_idx - 1 for s_idx in case[\"S\"]}\n        x_star = np.zeros(n)\n        coeffs = np.linspace(case[\"coeffs\"][0], case[\"coeffs\"][1], k)\n        # Assign coefficients to sorted indices for reproducibility\n        sorted_S_true = sorted(list(S_true))\n        x_star[sorted_S_true] = coeffs\n        \n        # 3. Generate measurements y\n        y0 = A @ x_star\n        eta = case[\"eta\"]\n        if eta > 0:\n            noise = np.random.randn(m)\n            noise_norm = np.linalg.norm(noise)\n            y0_norm = np.linalg.norm(y0)\n            scaled_noise = noise * (eta * y0_norm / noise_norm)\n            y = y0 + scaled_noise\n        else:\n            y = y0\n\n        # 4. Run algorithms\n        # Standard OMP (gamma=0)\n        T_std = run_omp(A, y, k, gamma=0.0)\n        \n        # Reweighted OMP\n        T_rw = run_omp(A, y, k, gamma=case[\"gamma\"])\n        \n        # 5. Calculate errors and improvement\n        err_std = k - len(set(T_std).intersection(S_true))\n        err_rw = k - len(set(T_rw).intersection(S_true))\n        \n        delta = err_std - err_rw\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3441569"}]}