{"hands_on_practices": [{"introduction": "坐标下降法是求解大规模稀疏优化问题（如Lasso）的基石算法之一。本练习将引导您通过第一性原理分析，揭示坐标下降法的核心计算优势：通过缓存残差，每次坐标更新的成本仅与该坐标对应的数据列的稀疏度成正比。通过与ISTA/FISTA等全梯度方法进行直接的计算量对比 ([@problem_id:3436973])，您将深入理解为何坐标下降法在处理列稀疏矩阵时表现得如此高效。", "problem": "考虑最小绝对收缩和选择算子 (Lasso) 问题，其定义为最小化复合凸目标函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\tfrac{1}{2}\\lVert A x - y \\rVert_2^2$ 且 $g(x) = \\lambda \\lVert x \\rVert_1$，并且 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^m$，$x \\in \\mathbb{R}^n$，$\\lambda \\ge 0$。假设 $A$ 以列稀疏格式存储，令 $a_j \\in \\mathbb{R}^m$ 表示第 $j$ 列，其稀疏度为 $s_j = \\mathrm{nnz}(a_j)$，其中 $\\mathrm{nnz}(\\cdot)$ 表示非零项的数量。假设残差 $r = A x - y$ 在每次坐标更新后被显式缓存和更新。\n\n仅使用以下基本事实，推导每次更新和每次迭代的计算成本：\n\n- $f(x)$ 的梯度是 $\\nabla f(x) = A^\\top (A x - y)$。\n- 坐标 $j$ 的坐标级李普希茨常数是 $L_j = \\lVert a_j \\rVert_2^2$。\n- 标量函数 $\\lambda |\\cdot|$ 的近端算子是软阈值映射 $S_\\tau(t) = \\mathrm{sign}(t)\\max\\{|t| - \\tau, 0\\}$。\n- 对于一个具有 $s$ 个非零项的稀疏向量和一个稠密向量之间的稀疏点积，其算术成本为 $O(s)$，每个非零项计为一个乘加对。\n- 对于一个具有 $N$ 个非零项的稀疏矩阵向量积，其算术成本为 $O(N)$，每个非零项计为一个乘加对。\n\n第 1 部分。证明对 $x$ 的坐标 $j$ 进行精确的坐标级近端更新为\n$$\nx_j^+ \\leftarrow S_{\\lambda/L_j}\\!\\left(x_j - \\frac{a_j^\\top r}{L_j}\\right),\n$$\n并且，当残差 $r$ 被缓存时，更新单个坐标 $j$ 并刷新残差\n$$\nr^+ \\leftarrow r + a_j (x_j^+ - x_j)\n$$\n的总算术工作量为 $O(\\mathrm{nnz}(a_j))$。您的推导必须从 $\\nabla f(x)$、$L_j$ 和近端算子的定义出发，并且必须根据 $s_j = \\mathrm{nnz}(a_j)$ 明确证明算术工作量计数的每一步。\n\n第 2 部分。对于迭代软阈值算法 (ISTA) 和快速迭代软阈值算法 (FISTA)，每次迭代都使用两个稀疏矩阵向量积来计算梯度：一个用于 $A x$，一个用于 $A^\\top r$。证明单次 ISTA/FISTA 迭代的算术成本为 $O(\\mathrm{nnz}(A))$，直至一个不依赖于 $m$ 或 $n$ 的固定常数因子。解释为什么在列稀疏的情况下，每次迭代的成本主要由与 $\\mathrm{nnz}(A)$ 成比例的运算决定。\n\n第 3 部分。假设使用以下工作量模型来比较在固定精度下的计算效率：\n\n- 经过短暂的瞬态过程后，坐标下降 (CD) 反复只更新一个大小为 $k = |S|$ 的激活集 $S \\subset \\{1,\\dots,n\\}$，因为由于软阈值的作用，$S$ 之外的坐标保持为零。在此阶段，CD 对每个活动坐标执行 $T$ 次更新，总共进行 $T k$ 次坐标更新。对 $S$ 进行一次完整扫描的总算术成本与 $\\sum_{j \\in S} s_j$ 成正比。\n- ISTA/FISTA 需要 $E$ 次完整迭代才能达到相同的目标值精度，每次迭代的算术工作量与 $\\mathrm{nnz}(A)$ 成正比。\n\n在 $E=T$ 的简化假设下（即两种方法达到所需精度需要相同数量的外层迭代次数），CD 相对于 ISTA/FISTA 的预测运行时优势因子为\n$$\n\\rho \\equiv \\frac{\\text{每次 ISTA/FISTA 迭代的工作量}}{\\text{每次 CD 对 }S \\text{ 的扫描工作量}} = \\frac{\\mathrm{nnz}(A)}{\\sum_{j \\in S} s_j}.\n$$\n为下面的每个测试用例计算 $\\rho$。假设近端标量运算（软阈值）和常数时间标量更新的成本与稀疏算术相比可以忽略不计，并且每个非零项每次乘加对贡献一个单位的工作量。使用以下定义：\n\n- $\\mathrm{nnz}(A) = \\sum_{j=1}^n s_j$，\n- $s_j = \\mathrm{nnz}(a_j)$，\n- $S$ 是激活集。\n\n测试套件。对于下面的每个用例，计算并报告如上定义的标量 $\\rho$。\n\n- 用例 A（理想情况；高度列稀疏且激活集小）：令 $m = 100000$，$n = 50000$，$s_j = 5$ 对所有 $j$ 成立，且 $S$ 是大小为 $k = 500$ 的任意子集。\n- 用例 B（异构稀疏性；激活集集中在最稀疏的列上）：令 $m = 20000$，$n = 20000$，并定义 $s_j = 2 + \\left\\lfloor 8 \\cdot \\frac{j-1}{n-1} \\right\\rfloor$，使得 $s_j \\in \\{2,3,\\dots,10\\}$ 随 $j$ 近似线性增加。令 $S$ 为具有最小 $s_j$ 值的 $k = 100$ 个索引。\n- 用例 C（边界情况；稠密列且无稀疏优势）：令 $m = 5000$，$n = 5000$，$s_j = m$ 对所有 $j$ 成立，且 $S = \\{1,\\dots,n\\}$，因此 $k = n$。\n- 用例 D（倾斜混合；绝大多数非常稀疏，少数中等稠密；激活集小且在非常稀疏的列中）：令 $m = 100000$，$n = 100000$，并定义前 $90000$ 列的 $s_j = 3$，后 $10000$ 列的 $s_j = 1000$。令 $S$ 是前 $90000$ 列中大小为 $k = 1000$ 的任意子集。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序为用例 A、B、C、D。每个条目必须是一个浮点数。例如，一个有四个结果的输出必须看起来像 $[r_A,r_B,r_C,r_D]$，其中每个 $r_\\cdot$ 是用标准十进制表示法书写的浮点数，不带单位或百分号。[@problem_id:483]", "solution": "用户提供了一个分为三部分的问题，内容涉及坐标下降（CD）与迭代软阈值算法（ISTA）在解决 Lasso 优化问题上的计算成本分析。该问题是有效的，因为它在科学上基于凸优化理论，内部一致，问题设定良好，并为完整解答提供了所有必要信息。\n\n### 第 1 部分：坐标下降更新与单次更新成本\n\nLasso 目标函数为 $F(x) = f(x) + g(x)$，其中 $f(x) = \\frac{1}{2}\\lVert A x - y \\rVert_2^2$ 是平滑的最小二乘项，而 $g(x) = \\lambda \\lVert x \\rVert_1 = \\sum_{i=1}^n \\lambda |x_i|$ 是非平滑的正则化项。\n\n坐标下降算法一次更新一个坐标 $x_j$，同时保持所有其他坐标 $x_{i \\neq j}$ 不变。这涉及求解一个一维优化问题。对此，一个标准方法是针对单个坐标 $x_j$ 的近端梯度更新。这种更新的一般形式是：\n$$\nx_j^+ \\leftarrow \\mathrm{prox}_{\\gamma g_j}(x_j - \\gamma \\nabla_j f(x))\n$$\n其中 $g_j(t) = \\lambda |t|$ 是对应于坐标 $j$ 的正则化项分量，$\\nabla_j f(x) = \\frac{\\partial f}{\\partial x_j}$ 是 $f(x)$ 的第 $j$ 个偏导数，$\\gamma$ 是步长。对于坐标级更新，最优步长是梯度坐标级李普希茨常数的倒数，即 $\\gamma_j = 1/L_j$。问题中说明了 $L_j = \\lVert a_j \\rVert_2^2$。\n\n梯度 $\\nabla f(x) = A^\\top(Ax-y)$ 的第 $j$ 个分量由 $A^\\top$ 的第 $j$ 行（即 $A$ 的第 $j$ 列 $a_j$）与残差向量 $r = Ax-y$ 的点积给出。因此，\n$$\n\\nabla_j f(x) = a_j^\\top (Ax - y) = a_j^\\top r\n$$\n函数 $\\alpha |\\cdot|$ 的近端算子是软阈值算子 $S_\\alpha(z) = \\mathrm{sign}(z) \\max\\{|z|-\\alpha, 0\\}$。在我们的情况下，该函数是 $\\gamma_j g_j(t) = \\frac{1}{L_j} \\lambda |t|$，所以软阈值算子的参数是 $\\alpha = \\lambda/L_j$。\n\n将 $\\gamma = 1/L_j$、$\\nabla_j f(x) = a_j^\\top r$ 以及近端算子的形式代入更新规则，我们得到：\n$$\nx_j^+ \\leftarrow \\mathrm{prox}_{\\lambda/L_j |\\cdot|}\\left(x_j - \\frac{1}{L_j} a_j^\\top r\\right) = S_{\\lambda/L_j}\\left(x_j - \\frac{a_j^\\top r}{L_j}\\right)\n$$\n这便完成了更新公式的推导。\n\n接下来，我们分析执行此更新和刷新缓存的残差 $r$ 的计算成本。该过程包括两个步骤：\n1.  **计算 $x_j$ 的更新**：主要的计算工作是点积 $a_j^\\top r$。鉴于 $a_j$ 是一个具有 $s_j = \\mathrm{nnz}(a_j)$ 个非零项的稀疏向量，而 $r$ 是一个大小为 $m$ 的稠密向量，此操作的算术运算成本为 $O(s_j)$（具体来说是 $s_j$ 次乘法和 $s_j-1$ 次加法）。所有其他运算（标量减法、除法和软阈值映射）都花费常数时间，即 $O(1)$。因此，计算 $x_j^+$ 的成本主要由点积决定，成本为 $O(s_j)$。\n2.  **更新残差**：新的残差 $r^+$ 根据 $x_j$ 的变化量（记为 $\\Delta x_j = x_j^+ - x_j$）计算。\n    $$\n    r^+ = A x^+ - y = A (x + \\Delta x_j e_j) - y = (A x - y) + A(\\Delta x_j e_j) = r + \\Delta x_j a_j\n    $$\n    其中 $e_j$ 是第 $j$ 个标准基向量。更新 $r^+ \\leftarrow r + (\\Delta x_j) a_j$ 是一个缩放向量加法（SAXPY 操作）。由于 $a_j$ 只有 $s_j$ 个非零项，所以 $r$ 中只有 $s_j$ 个元素会被修改。这需要 $s_j$ 次乘法和 $s_j$ 次加法，总成本为 $O(s_j)$。\n\n一次完整的坐标更新（更新 $x_j$ 然后更新 $r$）的总算术成本是这两个步骤成本的总和，即 $O(s_j) + O(s_j) = O(s_j)$。\n\n### 第 2 部分：ISTA/FISTA 的单次迭代成本\n\nISTA（以及 FISTA）的一次迭代需要计算完整梯度 $\\nabla f(x) = A^\\top(Ax-y)$。如问题所述，这是通过两个稀疏矩阵向量积实现的。\n1.  计算残差 $r=Ax-y$。乘积 $Ax$，其中 $A$ 是一个具有 $\\mathrm{nnz}(A)$ 个非零项的稀疏矩阵，根据给定的成本模型，其运算成本为 $O(\\mathrm{nnz}(A))$。随后的向量减法成本为 $O(m)$。\n2.  计算梯度 $\\nabla f(x) = A^\\top r$。矩阵 $A^\\top$ 同样具有 $\\mathrm{nnz}(A)$ 个非零项。此乘积的运算成本同样为 $O(\\mathrm{nnz}(A))$。\n\nISTA/FISTA 迭代的其余部分，如向量加法和逐元素的近端映射，其成本为 $O(n)$ 或 $O(m)$。在典型的高维稀疏设置中，非零项数 $\\mathrm{nnz}(A)$ 是主导项，即 $\\mathrm{nnz}(A) \\gg n$ 且 $\\mathrm{nnz}(A) \\gg m$。因此，ISTA 或 FISTA 的每次迭代总成本由两个矩阵向量积主导，从而产生 $O(\\mathrm{nnz}(A))$ 的计算复杂度。\n\n### 第 3 部分：优势因子 $\\rho$ 的计算\n\n优势因子 $\\rho$ 比较了 ISTA/FISTA 一次迭代的工作量与 CD 对激活集 $S$ 进行一次扫描的工作量。根据问题定义和上述推导的成本，我们有：\n$$\n\\rho = \\frac{\\text{Work(ISTA 迭代)}}{\\text{Work(CD 扫描 } S)} = \\frac{O(\\mathrm{nnz}(A))}{O\\left(\\sum_{j \\in S} s_j\\right)}\n$$\n假设大O表示法中隐藏的常数相当，可以消去，公式简化为：\n$$\n\\rho = \\frac{\\mathrm{nnz}(A)}{\\sum_{j \\in S} s_j}\n$$\n我们现在为每个给定的测试用例计算 $\\rho$。\n\n**用例 A**：\n-   $n = 50000$，$s_j=5$ 对所有 $j$ 成立，$k = |S| = 500$。\n-   $\\mathrm{nnz}(A) = n \\times s_j = 50000 \\times 5 = 250000$。\n-   $\\sum_{j \\in S} s_j = k \\times s_j = 500 \\times 5 = 2500$。\n-   $\\rho_A = \\frac{250000}{2500} = 100.0$。\n\n**用例 B**：\n-   $n = 20000$，$s_j = 2 + \\lfloor 8 \\cdot \\frac{j-1}{n-1} \\rfloor$，$k = |S| = 100$。\n-   激活集 $S$ 包含具有最小 $s_j$ 的 100 个索引。最小值为 $s_1 = 2 + \\lfloor 0 \\rfloor = 2$。因此，对于所有 $j \\in S$，$s_j = 2$。\n-   $\\sum_{j \\in S} s_j = k \\times 2 = 100 \\times 2 = 200$。\n-   $\\mathrm{nnz}(A) = \\sum_{j=1}^n s_j = \\sum_{j=1}^{20000} \\left(2 + \\lfloor 8 \\frac{j-1}{19999} \\rfloor\\right) = 2n + \\sum_{k=0}^{19999} \\lfloor \\frac{8k}{19999} \\rfloor$。\n    求和项的计算结果为 $70001$。\n    $\\mathrm{nnz}(A) = 2 \\times 20000 + 70001 = 40000 + 70001 = 110001$。\n-   $\\rho_B = \\frac{110001}{200} = 550.005$。\n\n**用例 C**：\n-   $n = 5000$，$m=5000$，$s_j = m = 5000$ 对所有 $j$ 成立，$S = \\{1, \\dots, n\\}$。\n-   $\\mathrm{nnz}(A) = n \\times m = 5000 \\times 5000 = 25000000$。\n-   由于 $S$ 包含所有列，$\\sum_{j \\in S} s_j = \\sum_{j=1}^n s_j = \\mathrm{nnz}(A)$。\n-   $\\rho_C = \\frac{\\mathrm{nnz}(A)}{\\mathrm{nnz}(A)} = 1.0$。\n\n**用例 D**：\n-   $n = 100000$，$k = |S| = 1000$。$S$ 是前 $90000$ 列的一个子集。\n-   对于 $j \\in \\{1, \\dots, 90000\\}$，$s_j=3$；对于 $j \\in \\{90001, \\dots, 100000\\}$，$s_j=1000$。\n-   $\\mathrm{nnz}(A) = (90000 \\times 3) + (10000 \\times 1000) = 270000 + 10000000 = 10270000$。\n-   对于任何 $j \\in S$，$s_j = 3$。\n-   $\\sum_{j \\in S} s_j = k \\times 3 = 1000 \\times 3 = 3000$。\n-   $\\rho_D = \\frac{10270000}{3000} = \\frac{10270}{3} \\approx 3423.333333$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the predicted runtime advantage factor of Coordinate Descent (CD)\n    over ISTA/FISTA for the Lasso problem under a simplified workload model.\n    \"\"\"\n\n    test_cases = [\n        # Case A: Happy path; highly column-sparse with small active set\n        {\n            \"name\": \"A\",\n            \"params\": {\"m\": 100000, \"n\": 50000, \"k\": 500},\n            \"s_func\": lambda j, n, m: 5,\n            \"s_active_func\": lambda s_all: 5\n        },\n        # Case B: Heterogeneous sparsity; active set on sparsest columns\n        {\n            \"name\": \"B\",\n            \"params\": {\"m\": 20000, \"n\": 20000, \"k\": 100},\n            \"s_func\": lambda j, n, m: 2 + int(8 * (j - 1) / (n - 1)),\n            \"s_active_func\": lambda s_all: 2\n        },\n        # Case C: Boundary; dense columns and no sparsity advantage\n        {\n            \"name\": \"C\",\n            \"params\": {\"m\": 5000, \"n\": 5000, \"k\": 5000},\n            \"s_func\": lambda j, n, m: m,\n            \"s_active_func\": lambda s_all: 5000\n        },\n        # Case D: Skewed mixture; small active set among very sparse columns\n        {\n            \"name\": \"D\",\n            \"params\": {\"m\": 100000, \"n\": 100000, \"k\": 1000},\n            \"s_func\": lambda j, n, m: 3 if j = 90000 else 1000,\n            \"s_active_func\": lambda s_all: 3\n        },\n    ]\n\n    results = []\n    \n    # The advantage factor rho is defined as nnz(A) / sum_{j in S} s_j\n    # where nnz(A) = sum_{j=1 to n} s_j\n\n    # --- Case A ---\n    case_a = test_cases[0]\n    p_a = case_a[\"params\"]\n    n_a, k_a = p_a[\"n\"], p_a[\"k\"]\n    s_j_a = case_a[\"s_func\"](1, n_a, p_a[\"m\"]) # s_j is constant\n    nnz_A_a = n_a * s_j_a\n    sum_s_j_S_a = k_a * case_a[\"s_active_func\"](None)\n    rho_a = nnz_A_a / sum_s_j_S_a\n    results.append(rho_a)\n\n    # --- Case B ---\n    case_b = test_cases[1]\n    p_b = case_b[\"params\"]\n    n_b, k_b = p_b[\"n\"], p_b[\"k\"]\n    s_values_b = [case_b[\"s_func\"](j, n_b, p_b[\"m\"]) for j in range(1, n_b + 1)]\n    nnz_A_b = sum(s_values_b)\n    sum_s_j_S_b = k_b * case_b[\"s_active_func\"](s_values_b)\n    rho_b = nnz_A_b / sum_s_j_S_b\n    results.append(rho_b)\n\n    # --- Case C ---\n    case_c = test_cases[2]\n    p_c = case_c[\"params\"]\n    n_c, m_c, k_c = p_c[\"n\"], p_c[\"m\"], p_c[\"k\"]\n    s_j_c = case_c[\"s_func\"](1, n_c, m_c)\n    nnz_A_c = n_c * s_j_c\n    # S = {1,...,n}, so sum over S is the same as sum over all columns\n    sum_s_j_S_c = nnz_A_c \n    rho_c = nnz_A_c / sum_s_j_S_c\n    results.append(rho_c)\n    \n    # --- Case D ---\n    case_d = test_cases[3]\n    p_d = case_d[\"params\"]\n    n_d, k_d = p_d[\"n\"], p_d[\"k\"]\n    n1, s1 = 90000, 3\n    n2, s2 = 10000, 1000\n    nnz_A_d = n1 * s1 + n2 * s2\n    sum_s_j_S_d = k_d * case_d[\"s_active_func\"](None)\n    rho_d = nnz_A_d / sum_s_j_S_d\n    results.append(rho_d)\n\n    # Format the final output as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3436973"}, {"introduction": "任何算法的计算优势都不是绝对的，它依赖于具体问题的结构。在认识到坐标下降法于稀疏问题中的威力后，本练习将带您探索其性能边界 ([@problem_id:3437000])。我们将构建一个假设场景：当数据矩阵虽然稠密，但拥有能够实现快速矩阵向量乘法（如基于FFT的变换）的特殊结构时，分析坐标下降法是否依然是最佳选择，从而培养在不同问题特性下选择最合适优化策略的能力。", "problem": "考虑最小绝对收缩和选择算子 (LASSO) 问题\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 可以通过结构化变换进行快速乘法，且其列是稠密的。您正在比较两种一阶方法：迭代收縮阈值算法 (ISTA) 及其加速变体快速迭代收縮阈值算法 (FISTA)，与坐标下降法 (CD) 进行对比。假设使用以下计算模型：\n\n- 一次与 $A$ 的乘法成本为 $c_{A} \\, m \\log_{2}(m)$ 次标量运算，一次与 $A^{\\top}$ 的乘法成本为 $c_{A^{\\top}} \\, m \\log_{2}(m)$ 次标量运算，这反映了具有以2为底的对数复杂度的快速变换。相对于变换成本，可以忽略向量加法和软阈值映射的成本。\n- 一次 ISTA (或 FISTA) 迭代恰好执行一次与 $A$ 的乘法和一次与 $A^{\\top}$ 的乘法。\n- 对坐标 $j$ 的单次 CD 更新需要计算 $a_{j}^{\\top} r$，其中 $r := A x - y$，然后更新 $r \\leftarrow r + \\Delta x_{j} a_{j}$。如果 $A$ 只能通过其快速变换访问（没有显式的列存储）并且列是稠密的，那么每次 CD 更新的成本为 $c_{t} \\, m$ 次标量运算，与 $j$ 无关，因为物化并应用一个稠密列 $a_{j}$ 并不比一次全长向量运算更便宜。如果显式的列可用，成本将与 $a_{j}$ 中的非零元素数量 $\\text{nnz}(a_{j})$ 成正比，但在这种设定下，对所有 $j$ 都有 $\\text{nnz}(a_{j}) = m$。\n\n定义一个 CD 轮次为 $s$ 次连续的坐标更新（可能使用不同的索引）。设 $m = 16384$，$c_{A} = 3$，$c_{A^{\\top}} = 3$，$c_{t} = 2$。请从第一性原理出发，推导出一个符号表达式，用于表示临界轮次大小 $s_{\\star}$，在该大小时，一个 CD 轮次的总成本等于在上述假设下一次 ISTA 迭代的成本，然后使用给定的参数数值计算 $s_{\\star}$。您的最终答案必须是 $s_{\\star}$ 的单个值。此外，在您的推导中，请用 $\\text{nnz}(a_{j})$ 和变换成本来陈述一般条件，该条件刻画了在稠密列、快速变换的情境下，坐标下降法相对于 ISTA/FISTA 何时失去其计算优势。最终值 $s_{\\star}$ 无需进行四舍五入。", "solution": "该问题在数值优化和稀疏信号处理领域具有科学依据。LASSO 目标函数以及提到的算法（ISTA、FISTA、CD）都是标准的。问题的核心是基于一个指定的模型对计算成本进行比较。\n\n该模型呈现了一个场景，其中矩阵 $A$ 具有快速变换，这意味着它具有全局结构（如 Fourier 或 Hadamard 矩阵），但 CD 的单次坐标更新成本却被设定为 $O(m)$。通常，使用列 $a_j = A e_j$ 需要通过变换来生成它，成本为 $O(m \\log m)$。该问题通过假定一个 $O(m)$ 的成本来规避这一点，并声明这是“因为物化并应用一个稠密列 $a_j$ 并不比一次全长向量运算更便宜”。这构成了该问题计算模型的一条公理。虽然这可能是对真实世界实现的简化，但它不构成逻辑矛盾或违反数学原理。它定义了分析的具体背景。\n\n该问题是适定的，提供了推导量 $s_{\\star}$ 所需的所有信息。语言客观而精确。问题不简单，需要正确应用给定的成本模型。它可以完全形式化，并且与其所述主題相关。该问题被判定为**有效**。将提供完整的解决方案。\n\n**求解推导**\n\n主要任务是找到临界轮次大小 $s_{\\star}$，在该大小时，一次坐标下降 (CD) 轮次的计算成本等于一次 ISTA/FISTA 迭代的成本。我们从问题计算模型中阐述的第一性原理出发。\n\n首先，我们确定 ISTA 或 FISTA 单次迭代的成本。问题陈述，这两种算法的单次迭代都恰好执行一次与 $A$ 的乘法和一次与 $A^{\\top}$ 的乘法。其他操作（如向量加法和收缩）的成本被认为可以忽略不计。\n\n与 $A$ 相乘的成本给出为 $C_{A} = c_{A} m \\log_{2}(m)$。\n与 $A^{\\top}$ 相乘的成本给出为 $C_{A^{\\top}} = c_{A^{\\top}} m \\log_{2}(m)$。\n\n因此，一次 ISTA 或 FISTA 迭代的总成本，我们记为 $C_{\\text{ISTA}}$，是这两个成本之和：\n$$\nC_{\\text{ISTA}} = C_{A} + C_{A^{\\top}} = c_{A} m \\log_{2}(m) + c_{A^{\\top}} m \\log_{2}(m) = (c_{A} + c_{A^{\\top}}) m \\log_{2}(m)\n$$\n\n接下来，我们确定一个 CD 轮次的成本。一个 CD 轮次定义为 $s$ 次连续的坐标更新。问题指明，单次坐标更新的成本为 $C_{\\text{CD\\_update}} = c_{t} m$，且与坐标索引 $j$ 无关。\n\n一个大小为 $s$ 的 CD 轮次的总成本，记为 $C_{\\text{CD\\_epoch}}(s)$，是单次更新的成本乘以更新次数 $s$：\n$$\nC_{\\text{CD\\_epoch}}(s) = s \\cdot C_{\\text{CD\\_update}} = s \\, c_{t} m\n$$\n\n临界轮次大小 $s_{\\star}$ 定义为使这两个成本相等的 $s$ 的值：\n$$\nC_{\\text{CD\\_epoch}}(s_{\\star}) = C_{\\text{ISTA}}\n$$\n代入成本表达式，我们得到控制方程：\n$$\ns_{\\star} \\, c_{t} m = (c_{A} + c_{A^{\\top}}) m \\log_{2}(m)\n$$\n为了找到 $s_{\\star}$ 的符号表达式，我们对其求解。$m$ 项可以方便地从两侧消掉，得到：\n$$\ns_{\\star} = \\frac{c_{A} + c_{A^{\\top}}}{c_{t}} \\log_{2}(m)\n$$\n这就是临界轮次大小的符号表达式。它表明，对于一组固定的成本系数，$s_{\\star}$ 随问题维度 $m$ 成对数增长。\n\n现在，我们使用提供的参数进行数值计算：$m = 16384$，$c_{A} = 3$，$c_{A^{\\top}} = 3$，$c_{t} = 2$。首先，我们计算 $m$ 的对数：\n$$\nm = 16384 = 2^{14}\n$$\n因此，$\\log_{2}(m) = \\log_{2}(2^{14}) = 14$。\n\n将所有数值代入 $s_{\\star}$ 的表达式中：\n$$\ns_{\\star} = \\frac{3 + 3}{2} \\times 14 = \\frac{6}{2} \\times 14 = 3 \\times 14 = 42\n$$\n因此，对于给定的参数，在 CD 中执行 $42$ 次坐标更新的计算成本与执行一次完整的 ISTA 或 FISTA 迭代相同。\n\n最后，我们探讨在稠密列、快速变换的情境下，坐标下降法失去其计算优势的一般条件。CD 的传统优势源于其低廉的单次更新成本，尤其是当矩阵 $A$ 的列 $a_j$ 是稀疏的时候。更新坐标 $x_j$ 的成本与对应列中的非零元素数量 $\\text{nnz}(a_j)$ 成正比。当 $\\text{nnz}(a_j) \\ll m$ 时，这个成本很低。\n\n相比之下，像 ISTA/FISTA 这样的方法每一步都需要进行完整的梯度计算 $A^{\\top}(Ax-y)$。当 $A$ 具有快速变换时，其成本为 $O(m \\log m)$。CD 的一次完整遍历或“轮次”涉及更新所有 $n$ 个坐标。一次完整 CD 遍历的成本约为 $O(\\sum_{j=1}^{n} \\text{nnz}(a_j))$。\n\n当坐标下降法的单次更新成本相对于基于变换的方法的成本不再很小时，它就失去了计算优势。这种情况发生在 $A$ 的列是稠密的时候，即 $\\text{nnz}(a_j) \\approx m$。在这种情境下，单次 CD 更新的成本为 $O(m)$，而遍历所有 $n$ 个坐标的一次完整过程的成本变为 $O(nm)$。\n\n因此，在稠密列、快速变换的情境下，CD 相对于 ISTA/FISTA 失去其计算优势的条件是，当一次完整 CD 遍历的成本 $O(nm)$ 不再显著小于一次 ISTA/FISTA 迭代的成本 $O(m \\log m)$ 时。比较这些成本，如果 $nm \\gtrsim m \\log m$，即简化为 $n \\gtrsim \\log m$，则 CD 处于劣势。由于在大多数实际的高维问题中，特征数量 $n$ 远大于 $\\log m$，因此在稠密列和 $A$ 存在快速变换的条件下，坐标下降法在单次遍历的基础上是计算不利的。其竞争力将完全取决于它能否用少量更新（$s \\ll n$）比单次 ISTA/FISTA 迭代在求解过程中取得多得多的进展。", "answer": "$$\\boxed{42}$$", "id": "3437000"}, {"introduction": "坐标下降法的优雅之处在于其“分而治之”的策略，即将复杂的高维问题分解为一系列易于处理的一维子问题。这一思想的威力并不仅限于凸优化，本练习将引导您将其应用于更具挑战性的非凸$L_p$ ($0 \\lt p \\lt 1$)正则化问题中 ([@problem_id:3437037])。通过推导并实现非凸标量近端算子的求解器，您将亲身体验坐标下降框架的通用性，并从经验上验证精确坐标最小化在加速复杂模型收敛方面的计算优势。", "problem": "考虑压缩感知中的稀疏回归问题，其目标是最小化目标函数 $$F(x) = \\tfrac{1}{2}\\lVert A x - b\\rVert_2^2 + \\lambda \\sum_{j=1}^{n} \\lvert x_j \\rvert^p,$$ 其中设计矩阵 $A \\in \\mathbb{R}^{m \\times n}$，响应向量 $b \\in \\mathbb{R}^m$，正则化参数 $\\lambda  0$，惩罚指数 $p \\in (0,1)$，决策变量 $x \\in \\mathbb{R}^n$。当 $p \\in (0,1)$ 时，惩罚项 $\\sum_j \\lambda \\lvert x_j \\rvert^p$ 是非凸的，并且已知具有促进稀疏性的作用。您的任务是推导并实现利用精确坐标级最小化来提高效率的计算过程。\n\n从坐标下降和一维一阶最优性的基本原理出发，完成以下任务：\n\n- 推导。对于一个固定的坐标 $j$，将不含第 $j$ 个坐标的残差定义为 $r_{-j}(x) = A x - b - a_j x_j$，其中 $a_j$ 是 $A$ 的第 $j$ 列。证明，对于替代 $x_j$ 的标量变量 $t$，最小化 $F(x)$ 的问题，在不考虑与 $t$ 无关的加性常数的情况下，可以简化为最小化一维函数 $$g_j(t) = \\tfrac{1}{2} \\lVert r_{-j}(x) + a_j t \\rVert_2^2 + \\lambda \\lvert t \\rvert^p = \\tfrac{1}{2} L_j (t - z_j)^2 + \\lambda \\lvert t \\rvert^p,$$ 其中 $L_j = \\lVert a_j \\rVert_2^2$ 且 $z_j = - \\tfrac{a_j^\\top r_{-j}(x)}{L_j}$。由此得出结论，精确的坐标级更新是标量近端映射 $$\\operatorname{prox}_{\\alpha \\lvert \\cdot \\rvert^p}(z) \\in \\arg\\min_{t \\in \\mathbb{R}} \\tfrac{1}{2} (t - z)^2 + \\alpha \\lvert t \\rvert^p,$$ 其中 $\\alpha = \\lambda / L_j$ 且 $z=z_j$。推导非零最小化子的必要最优性条件，即 $$t - z + \\alpha p \\operatorname{sign}(t) \\lvert t \\rvert^{p-1} = 0,$$ 并论证为何一个将 $g_j(0)$ 与标量方程在 $t \\ge 0$ 时的较大正根处的值进行比较的决策规则，足以选出全局最小化子。解释为何可以利用在 $t \\ge 0$ 的单调分支上的稳健二分法程序来计算非零最小化子，利用函数 $\\phi(t) = t - \\lvert z \\rvert + \\alpha p t^{p-1}$ 在 $$t_{\\min} = \\big(\\alpha p (1-p)\\big)^{\\tfrac{1}{2-p}}$$ 处有唯一最小化子这一事实。提供一种在较大根存在时定位它的区间策略。\n\n- 算法。实现两种共享相同标量近端子程序的算法：\n  - 精确循环坐标下降法，它按顺序访问坐标 $j = 1, \\dots, n$，维护残差 $r = A x - b$，并在每一步计算 $z_j = x_j - \\tfrac{a_j^\\top r}{L_j}$，应用参数为 $\\alpha = \\lambda / L_j$ 的标量近端算子，并在 $t \\ne x_j$ 时更新 $r \\leftarrow r + a_j (t - x_j)$。\n  - 具有固定全局步长 $1/L$ 的近端梯度法，其中 $L$ 是 $A$ 的算子 2-范数的平方，即 $L = \\lVert A \\rVert_2^2$，使用更新 $y = x - \\tfrac{1}{L} A^\\top (A x - b)$，然后是参数为 $\\alpha = \\lambda/L$ 的逐元素标量近端算子。\n\n- 运算计数模型。为了比较计算优势，仅将涉及数据矩阵 $A$ 条目的乘法和加法（模拟矩阵-向量乘法累加对）计数为单位运算。在此模型下：\n  - 乘积 $A x$ 或 $A^\\top r$ 各消耗 $m n$ 个单位运算。\n  - 在坐标下降中，计算 $a_j^\\top r$ 消耗 $m$ 个单位运算，更新 $r \\leftarrow r + a_j \\Delta$ 消耗 $m$ 个单位运算；如果 $\\Delta = 0$，则跳过残差更新，消耗为 $0$。\n  - 不涉及 $A$ 的标量运算（包括近端求根、不涉及 $A$ 的向量更新、范数计算和阈值比较）在此模型中成本为零。\n使用这些规则为每种方法累积运算计数，直到收敛。\n\n- 收敛与停止。使用迭代点的相对变化作为停止准则。具体来说，当 $$\\frac{\\lVert x^{k+1} - x^k \\rVert_2}{\\lVert x^k \\rVert_2 + 10^{-12}} \\le \\varepsilon,$$ 满足时终止，容差为 $\\varepsilon = 10^{-6}$。为坐标下降设置 $50$ 次完整遍历的硬上限，为近端梯度设置 $500$ 次迭代的硬上限。两种方法均以 $x^0 = 0$ 初始化。\n\n- 测试套件。为了可复现性，按以下过程构建合成数据：抽取 $A$，其条目为独立的标准正态分布，将每列归一化为单位欧几里得范数，构建一个稀疏的真实值 $x^\\star$，其在均匀随机选择的不同索引上有恰好 $s$ 个非零值，这些值独立地从标准正态分布中抽取，并设置 $b = A x^\\star + \\eta$，其中 $\\eta$ 的条目独立地服从均值为 $0$、标准差为 $\\sigma$ 的正态分布。对于以下每种情况，请指明 $(m,n,s,p,\\lambda,\\sigma,\\text{seed})$：\n  - 情况 1：$(50,200,10,0.5,0.1,0.01,0)$。\n  - 情况 2：$(80,80,5,0.9,0.05,0.01,1)$。\n  - 情况 3：$(30,300,5,0.2,0.2,0.02,2)$。\n\n- 必需的输出。对于每种情况，运行两种方法直到收敛，并计算比率 $$R = \\frac{\\text{近端梯度法运算计数}}{\\text{坐标下降法运算计数}}。$$ 您的程序应生成单行输出，其中包含按情况 $1,2,3$ 顺序排列的结果，形式为逗号分隔的列表并用方括号括起，例如 $$[R_1,R_2,R_3].$$ 条目必须是浮点数。不涉及物理单位，也不使用角度。\n\n实现必须是自包含的、确定性的，并且无需用户输入即可运行。标量近端子程序必须在有闭式解时产生闭式解，否则使用由您的推导证明其合理性的稳健二分法程序。目标是在给定的运算计数模型下，通过提供的测试套件，凭经验证明精确坐标级最小化的计算优势。", "solution": "用户要求我完成以下任务：\n1. 推导并分析坐标下降法应用于 $L_p$ 正则化（$p \\in (0,1)$）回归问题的一维子问题。\n2. 实现并比较两种算法：循环坐标下降（CD）和近端梯度（PG）。\n3. 比较必须基于一个关注矩阵相关运算的特定运算计数模型。\n4. 在三个指定的测试案例上运行比较，并报告运算计数的比率。\n\n首先，我将验证问题。该问题提法明确，在优化理论方面有科学依据，并为获得唯一、可验证的解提供了所有必要的细节。参数、算法和评估指标都已明确定义。没有发现不一致或含糊之处。该问题是有效的。\n\n我将按以下步骤进行求解：首先详细阐述数学推导，然后描述基于这些推导的算法及其实现，最后在提供的测试案例上执行比较。\n\n### 坐标级最小化规则的推导\n\n目标函数为 $F(x) = \\tfrac{1}{2}\\lVert A x - b\\rVert_2^2 + \\lambda \\sum_{j=1}^{n} \\lvert x_j \\rvert^p$。为了对第 $j$ 个坐标执行坐标下降，我们固定所有其他坐标 $x_k$（$k \\ne j$），并关于 $x_j$ 最小化 $F(x)$。令 $t$ 为第 $j$ 个坐标的标量变量。我们可以写出 $Ax = \\sum_{k \\ne j} a_k x_k + a_j t = (A x - a_j x_j) + a_j t$。\n设不包含第 $j$ 个坐标贡献的残差为 $r_{-j}(x) = A x - b - a_j x_j$。目标函数作为 $t$ 的函数变为：\n$$ g_j(t) = \\tfrac{1}{2}\\lVert (A x - a_j x_j - b) + a_j t \\rVert_2^2 + \\lambda \\lvert t \\rvert^p + C_{-j} $$\n其中 $C_{-j} = \\lambda \\sum_{k \\ne j} \\lvert x_k \\rvert^p$ 是一个关于 $t$ 的常数。\n$$ g_j(t) = \\tfrac{1}{2}\\lVert r_{-j}(x) + a_j t \\rVert_2^2 + \\lambda \\lvert t \\rvert^p + C_{-j} $$\n展开平方范数：\n$$ \\lVert r_{-j}(x) + a_j t \\rVert_2^2 = \\lVert r_{-j}(x) \\rVert_2^2 + 2t \\, a_j^\\top r_{-j}(x) + t^2 \\lVert a_j \\rVert_2^2 $$\n因此，最小化 $g_j(t)$ 等价于最小化：\n$$ \\tilde{g}_j(t) = \\tfrac{1}{2} \\lVert a_j \\rVert_2^2 t^2 + a_j^\\top r_{-j}(x) t + \\lambda \\lvert t \\rvert^p $$\n令 $L_j = \\lVert a_j \\rVert_2^2$。对关于 $t$ 的二次项进行配方：\n$$ \\tilde{g}_j(t) = \\tfrac{1}{2} L_j \\left( t^2 + 2 \\frac{a_j^\\top r_{-j}(x)}{L_j} t \\right) + \\lambda \\lvert t \\rvert^p = \\tfrac{1}{2} L_j \\left( t + \\frac{a_j^\\top r_{-j}(x)}{L_j} \\right)^2 - \\frac{(a_j^\\top r_{-j}(x))^2}{2 L_j} + \\lambda \\lvert t \\rvert^p $$\n定义 $z_j = - \\frac{a_j^\\top r_{-j}(x)}{L_j}$，则关于 $t$ 要最小化的表达式简化为：\n$$ \\tfrac{1}{2} L_j (t - z_j)^2 + \\lambda \\lvert t \\rvert^p $$\n这里省略了不含 $t$ 的常数项。这确认了问题陈述中给出的形式。坐标更新可以使用完整残差 $r = Ax-b$ 来表示。由于 $r_{-j}(x) = r - a_j x_j$，我们有：\n$$ z_j = - \\frac{a_j^\\top (r - a_j x_j)}{L_j} = - \\frac{a_j^\\top r - L_j x_j}{L_j} = x_j - \\frac{a_j^\\top r}{L_j} $$\n这提供了一种计算上简便的 $z_j$ 计算方法。\n\n因此，坐标 $j$ 的最小化问题是：\n$$ t_j^* = \\arg\\min_{t \\in \\mathbb{R}} \\left\\{ \\tfrac{1}{2} L_j (t - z_j)^2 + \\lambda \\lvert t \\rvert^p \\right\\} $$\n两边除以 $L_j$，这等价于：\n$$ t_j^* = \\arg\\min_{t \\in \\mathbb{R}} \\left\\{ \\tfrac{1}{2} (t - z_j)^2 + \\frac{\\lambda}{L_j} \\lvert t \\rvert^p \\right\\} $$\n这就是标量近端算子的定义，$t_j^* = \\operatorname{prox}_{\\alpha \\lvert \\cdot \\rvert^p}(z_j)$，其中 $\\alpha = \\lambda / L_j$。\n\n为了找到最小化子，我们分析一维目标函数 $h(t) = \\tfrac{1}{2}(t-z)^2 + \\alpha |t|^p$。由于 $|t|^p$ 是对称的，全局最小化子 $t^*$ 的符号必须与 $z$ 的符号相同。因此，我们可以将 $z$ 替换为 $|z|$，并分析 $t \\ge 0$ 的情况。对于 $t \\ge 0$，目标函数为 $h(t) = \\tfrac{1}{2}(t-|z|)^2 + \\alpha t^p$。\n其在 $t  0$ 时的导数为 $h'(t) = t - |z| + \\alpha p t^{p-1}$。非零最小化子必须满足最优性条件 $h'(t)=0$，即 $t - z + \\alpha p \\operatorname{sign}(t) \\lvert t \\rvert^{p-1} = 0$。\n\n令 $\\phi(t) = t - |z| + \\alpha p t^{p-1}$ 对于 $t0$。我们寻找 $\\phi(t)$ 的根。\n$\\phi(t)$ 的导数是 $\\phi'(t) = 1 + \\alpha p (p-1) t^{p-2} = 1 - \\alpha p (1-p) t^{p-2}$。由于 $p \\in (0,1)$，我们有 $1-p  0$。\n$\\phi'(t)$ 在 $t_{\\min} = (\\alpha p(1-p))^{1/(2-p)}$ 处有一个唯一的零点，这对应于 $\\phi(t)$ 的最小值。\n当 $t \\to 0^+$ 时，$\\phi(t) \\to \\infty$（因为 $p-1  0$）。当 $t \\to \\infty$ 时，$\\phi(t) \\to \\infty$。\n- 如果最小值 $\\phi(t_{\\min}) \\ge 0$，则对于所有 $t  0$，$\\phi(t) \\ge 0$，这意味着 $h(t)$ 在 $t \\ge 0$ 上单调递增。最小值在 $t=0$ 处取得。\n- 如果 $\\phi(t_{\\min})  0$，$\\phi(t)$ 会两次穿过横轴。设根为 $t_1  t_{\\min}  t_2$。这些是 $h(t)$ 的驻点。对二阶导数 $h''(t) = \\phi'(t)$ 的分析表明，$h(t)$ 在 $t_1$ 处有一个局部最大值，在 $t_2$ 处有一个局部最小值。\n因此，$h(t)$ 在 $t \\ge 0$ 上的全局最小化子只能是 $t=0$ 或较大的根 $t_2$。这证明了该决策规则的合理性：我们找到较大的正根 $t_2$（如果存在），并将目标函数值 $h(t_2)$ 与 $h(0) = \\frac{1}{2}z^2$ 进行比较，以找到 $t \\ge 0$ 时的最小化子。全局最小化子就是这个结果乘以 $\\operatorname{sign}(z)$。\n\n为了在根存在时（即当 $\\phi(t_{\\min})  0$ 时）找到根 $t_2$，我们可以使用二分法。我们需要一个区间 $[t_a, t_b]$，使得 $\\phi(t_a)$ 和 $\\phi(t_b)$ 的符号相反。我们知道 $t_2  t_{\\min}$ 且 $\\phi(t_{\\min})  0$。我们需要一个上界 $t_b$ 使得 $\\phi(t_b)  0$。考虑 $t=|z|$。对于 $t  |z|$，$t-|z|  0$ 且 $\\alpha p t^{p-1}  0$，因此 $\\phi(t)  0$。这意味着任何根都必须小于或等于 $|z|$。在 $t=|z|$ 时，$\\phi(|z|) = |z| - |z| + \\alpha p |z|^{p-1} = \\alpha p |z|^{p-1}  0$。因此，较大根 $t_2$ 的一个有效区间是 $[t_{\\min}, |z|]$。在此区间内对 $\\phi(t)$ 使用二分法程序可以稳健地找到 $t_2$。\n\n### 算法与计算模型\n\n**精确循环坐标下降（CD）**：\n该算法以循环方式遍历坐标 $j=1, \\dots, n$。对于每个坐标，它使用上面推导的近端算子计算一维子问题的精确最小化子。该实现维护一个残差向量 $r=Ax-b$。在每一步 $j$ 中，它计算 $z_j = x_j - a_j^\\top r / L_j$。由于列是标准化的，$L_j=1$。通过标量近端求解器找到新的坐标值 $t$。如果坐标值发生变化（$t \\ne x_j$），则高效地更新残差 $r \\leftarrow r + a_j(t-x_j)$。\n\n**近端梯度法（PG）**：\n这是一种迭代式一阶方法。每次迭代包括两个步骤：对目标函数的光滑部分进行梯度下降步，然后对非光滑惩罚项进行近端步。更新公式为 $x^{k+1} = \\operatorname{prox}_{\\alpha g}(x^k - \\gamma \\nabla f(x^k))$，其中 $f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2$ 且 $g(x) = \\lambda \\sum_j |x_j|^p$。梯度为 $\\nabla f(x) = A^\\top(Ax-b)$。为保证收敛，步长 $\\gamma$ 设为 $1/L$，其中 $L$ 是 $\\nabla f$ 的一个李普希茨常数。该常数为 $L = \\lVert A^\\top A \\rVert_2 = \\lVert A \\rVert_2^2$，即 $A$ 的最大奇异值的平方。近端算子的参数变为 $\\alpha = \\lambda/L$。由于惩罚项是可分的，近端算子逐元素应用。\n\n**运算计数模型**：\n我们通过对涉及数据矩阵 $A$ 的运算进行计数来比较计算成本。\n- 矩阵-向量乘积 $Ax$ 或 $A^\\top r$ 消耗 $mn$ 次运算。\n- 在 CD 中，对于每个坐标 $j$，计算梯度分量 $a_j^\\top r$ 消耗 $m$ 次运算。如果坐标被更新，残差更新 $r \\leftarrow r + a_j \\Delta x_j$ 再消耗 $m$ 次运算。因此，一次非零更新消耗 $2m$ 次运算，而一次零更新（坐标值不变）消耗 $m$ 次运算。对 $n$ 个坐标的一次完整遍历可能消耗 $mn$ 到 $2mn$ 次运算。\n- 在 PG 中，每次迭代需要计算完整梯度 $A^\\top(Ax-b)$，这涉及一次 $Ax$ 乘积和一次 $A^\\top r$ 乘积，总共消耗 $2mn$ 次运算。\n- 在此模型下，所有其他运算，包括标量子问题的求根，成本都为零。\n\n### 测试过程与结果\n\n两种算法都以 $x^0 = 0$ 初始化，并运行直至迭代点的相对变化 $\\lVert x^{k+1} - x^k \\rVert_2 / (\\lVert x^k \\rVert_2 + 10^{-12})$ 小于或等于容差 $\\varepsilon = 10^{-6}$。为 CD 设置了 $50$ 次完整遍历的最大限制，为 PG 设置了 $500$ 次迭代的最大限制。在三个指定的合成数据集上进行比较，并为每种情况计算总运算次数的比率 $R = (\\text{PG 计数}) / (\\text{CD 计数})$。预期 CD 会更高效，特别是对于稀疏解，因为它可以在坐标保持为零时跳过昂贵的残差更新，从而导致 $R  1$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svdvals\n\ndef solve_prox_lp(z, alpha, p, bisection_iters=100):\n    \"\"\"\n    Computes the proximal operator for the Lp penalty.\n    argmin_t { 1/2 * (t - z)^2 + alpha * |t|^p } for p in (0,1).\n    \"\"\"\n    if p = 0 or p >= 1:\n        raise ValueError(\"p must be in (0, 1)\")\n\n    abs_z = abs(z)\n    if abs_z == 0.0:\n        return 0.0\n\n    # The derivative of the objective (for t>0) is phi(t) = t - |z| + alpha*p*t^(p-1)\n    # The minimum of phi(t) occurs at t_min.\n    try:\n        t_min = (alpha * p * (1 - p))**(1.0 / (2 - p))\n    except (ValueError, OverflowError): # Handles large exponents or negative bases\n        return 0.0\n\n    def phi(t, abs_z_val, alpha_val, p_val):\n        # handle t=0 case gracefully, although we call it on t>0\n        if t = 1e-15: return np.inf\n        return t - abs_z_val + alpha_val * p_val * t**(p_val - 1)\n\n    phi_at_t_min = phi(t_min, abs_z, alpha, p)\n\n    if phi_at_t_min >= 0:\n        # phi(t) >= 0 for all t>0, so objective is increasing. Minimum is at t=0.\n        return 0.0\n\n    # If phi(t_min)  0, there are two positive roots. We need the larger one, t2.\n    # The bracket for t2 is [t_min, abs_z]\n    low = t_min\n    high = abs_z\n    \n    for _ in range(bisection_iters):\n        mid = (low + high) / 2.0\n        if mid == low or mid == high:\n            break\n        if phi(mid, abs_z, alpha, p) > 0:\n            high = mid\n        else:\n            low = mid\n    \n    t2 = (low + high) / 2.0\n\n    # Compare objective value at 0 and t2 to find the global minimum for t>=0\n    val_at_0 = 0.5 * abs_z**2\n    val_at_t2 = 0.5 * (t2 - abs_z)**2 + alpha * t2**p\n    \n    if val_at_0 = val_at_t2:\n        return 0.0\n    else:\n        return np.sign(z) * t2\n\ndef solve_prox_lp_vec(z_vec, alpha, p):\n    \"\"\"Vectorized version of the Lp proximal operator.\"\"\"\n    return np.array([solve_prox_lp(z_i, alpha, p) for z_i in z_vec])\n\ndef generate_data(m, n, s, sigma, seed):\n    \"\"\"Generates synthetic data for the sparse regression problem.\"\"\"\n    rng = np.random.default_rng(seed)\n    A = rng.standard_normal((m, n))\n    A /= np.linalg.norm(A, axis=0, keepdims=True)\n    \n    x_star = np.zeros(n)\n    indices = rng.choice(n, s, replace=False)\n    x_star[indices] = rng.standard_normal(s)\n    \n    eta = rng.standard_normal(m) * sigma\n    b = A @ x_star + eta\n    return A, b\n\ndef cd(A, b, p, lambda_, max_passes, tol):\n    \"\"\"Cyclic Coordinate Descent.\"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    r = -b.copy()  # Initial residual r = A@x - b\n    op_count = 0.0\n\n    # Per problem, columns of A are normalized, so L_j = 1 for all j.\n    L_j_val = 1.0 \n    alpha = lambda_ / L_j_val\n\n    for k in range(max_passes):\n        x_old = x.copy()\n        \n        for j in range(n):\n            aj = A[:, j]\n            \n            op_count += m  # For dot product a_j.T @ r\n            z_j = x[j] - np.dot(aj, r) / L_j_val\n\n            t = solve_prox_lp(z_j, alpha, p)\n            \n            delta_xj = t - x[j]\n            \n            if abs(delta_xj) > 1e-15:\n                x[j] = t\n                r += aj * delta_xj\n                op_count += m  # For residual update\n        \n        rel_change = np.linalg.norm(x - x_old) / (np.linalg.norm(x_old) + 1e-12)\n        if rel_change = tol:\n            break\n            \n    return x, op_count\n\ndef pg(A, b, p, lambda_, max_iters, tol):\n    \"\"\"Proximal Gradient Method.\"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    op_count = 0.0\n\n    try:\n        L = svdvals(A)[0]**2\n    except np.linalg.LinAlgError:\n        # Fallback for matrices where SVD might fail, though unlikely with this data\n        U, s, Vh = np.linalg.svd(A, full_matrices=False)\n        L = s[0]**2\n        \n    alpha = lambda_ / L\n    \n    for k in range(max_iters):\n        x_old = x.copy()\n        \n        # Gradient step\n        r = A @ x - b\n        op_count += m * n  # For A @ x\n        grad = A.T @ r\n        op_count += m * n  # For A.T @ r\n        \n        y = x - (1/L) * grad\n        \n        # Proximal step\n        x = solve_prox_lp_vec(y, alpha, p)\n        \n        rel_change = np.linalg.norm(x - x_old) / (np.linalg.norm(x_old) + 1e-12)\n        if rel_change = tol:\n            break\n            \n    return x, op_count\n\ndef solve():\n    \"\"\"Main function to run test cases and print results.\"\"\"\n    test_cases = [\n        # (m, n, s, p, lambda, sigma, seed)\n        (50, 200, 10, 0.5, 0.1, 0.01, 0),\n        (80, 80, 5, 0.9, 0.05, 0.01, 1),\n        (30, 300, 5, 0.2, 0.2, 0.02, 2),\n    ]\n\n    results = []\n    tol = 1e-6\n\n    for i, params in enumerate(test_cases):\n        m, n, s, p, lambda_, sigma, seed = params\n        \n        A, b = generate_data(m, n, s, sigma, seed)\n        \n        _, count_cd = cd(A, b, p, lambda_, max_passes=50, tol=tol)\n        _, count_pg = pg(A, b, p, lambda_, max_iters=500, tol=tol)\n        \n        if count_cd > 0:\n            ratio = count_pg / count_cd\n        else:\n            ratio = float('inf')\n        \n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n\n```", "id": "3437037"}]}