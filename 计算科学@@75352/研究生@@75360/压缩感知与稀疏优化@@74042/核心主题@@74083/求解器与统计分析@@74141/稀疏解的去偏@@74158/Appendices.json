{"hands_on_practices": [{"introduction": "理解去偏的第一步是亲眼见证Lasso估计器所固有的偏差。本练习通过一个具体的数值示例，清晰地展示了$\\ell_1$正则化导致的系数收缩效应。通过从头开始实现迭代软阈值算法（ISTA）来求解Lasso问题，然后对识别出的支撑集应用简单的最小二乘重拟合，您将能够直接比较有偏的Lasso估计和去偏后的估计，并量化其间的改进。这个实践是掌握后续更高级去偏技术的基础。", "problem": "考虑压缩感知和稀疏优化中的典型稀疏恢复设定。令传感矩阵表示为 $A \\in \\mathbb{R}^{m \\times n}$，真实稀疏信号表示为 $x^{\\star} \\in \\mathbb{R}^{n}$。观测到的测量值由 $y = A x^{\\star} + \\eta$ 给出，其中 $\\eta \\in \\mathbb{R}^{m}$ 代表加性噪声。最小绝对收缩和选择算子 (Lasso) 估计 $\\widehat{x}_{\\text{Lasso}}$ 定义为凸泛函\n$$\n\\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\n的最小化子，其中 $\\lambda \\ge 0$ 是一个正则化参数，$\\|\\cdot\\|_{1}$ 表示 $\\ell_{1}$ 范数。众所周知，$\\ell_{1}$ 惩罚项会引起收缩，从而在估计器中相对于 $x^{\\star}$ 产生坐标级的偏差。\n\n要求您从基本原理出发，实现迭代软阈值算法 (ISTA) 来计算 $\\widehat{x}_{\\text{Lasso}}$。使用 Lasso 目标函数分解为一个光滑项 $g(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ 和一个非光滑项 $h(x) = \\lambda \\|x\\|_{1}$。$g$ 的梯度是 $\\nabla g(x) = A^{\\top}(A x - y)$，$h$ 的近端算子 (proximal operator) 是坐标级的软阈值算子。选择一个满足 $0  t \\le \\frac{1}{L}$ 的恒定步长 $t$，其中 $L$ 是 $\\nabla g$ 的 Lipschitz 常数，等于 $A$ 的谱范数的平方，即 $L = \\|A\\|_{2}^{2}$。从 $x^{(0)} = 0$ 开始，生成迭代\n$$\nx^{(k+1)} = \\operatorname{soft}(x^{(k)} - t \\nabla g(x^{(k)}), \\lambda t),\n$$\n其中对于任何 $z \\in \\mathbb{R}^{n}$ 和 $\\tau \\ge 0$，该算子按坐标定义为\n$$\n\\operatorname{soft}(z_{i}, \\tau) = \\operatorname{sign}(z_{i}) \\cdot \\max(|z_{i}| - \\tau, 0).\n$$\n\n计算出 $\\widehat{x}_{\\text{Lasso}}$ 后，使用一个固定的阈值 $\\tau_{\\text{sup}}  0$ 来定义估计的支撑集 $\\widehat{S} = \\{ i \\in \\{1,\\dots,n\\} : |\\widehat{x}_{\\text{Lasso}, i}|  \\tau_{\\text{sup}}\\}$。通过求解以下问题对 $\\widehat{S}$ 进行受限的最小二乘重拟合\n$$\n\\min_{z \\in \\mathbb{R}^{|\\widehat{S}|}} \\|A_{\\widehat{S}} z - y\\|_{2}^{2},\n$$\n其中 $A_{\\widehat{S}}$ 表示由 $\\widehat{S}$ 索引的列组成的 $A$ 的子矩阵。使用 Moore–Penrose 伪逆来获得 $z^{\\text{LS}} = A_{\\widehat{S}}^{\\dagger} y$；然后通过设置 $(\\widehat{x}_{\\text{refit}})_{\\widehat{S}} = z^{\\text{LS}}$ 并且对于 $i \\notin \\widehat{S}$ 设置 $(\\widehat{x}_{\\text{refit}})_{i} = 0$ 来定义重拟合估计 $\\widehat{x}_{\\text{refit}} \\in \\mathbb{R}^{n}$。\n\n对于 $\\widehat{x}_{\\text{Lasso}}$ 和 $\\widehat{x}_{\\text{refit}}$，计算坐标级偏差向量 $b = \\widehat{x} - x^{\\star}$，并使用平均绝对偏差\n$$\n\\operatorname{MAB}(b) = \\frac{1}{n} \\sum_{i=1}^{n} |b_{i}|\n$$\n和最大绝对偏差\n$$\n\\operatorname{MaxAB}(b) = \\max_{1 \\le i \\le n} |b_{i}|\n$$\n来总结偏差。\n\n为以下测试套件实现一个程序来执行上述操作。每个测试用例指定 $A$、$x^{\\star}$、$\\eta$ 和 $\\lambda$，并要求使用 $\\tau_{\\text{sup}} = 10^{-6}$。\n\n测试用例 1 (良态，中等正则化):\n- 维度：$m = 8$, $n = 6$。\n- 矩阵 $A$:\n$$\n\\begin{bmatrix}\n0.36  -0.07  0.22  0.10  -0.31  0.41 \\\\\n-0.12  0.25  0.30  -0.41  0.05  -0.08 \\\\\n0.45  0.18  -0.08  0.03  0.26  -0.19 \\\\\n0.05  -0.31  0.41  0.17  -0.02  0.12 \\\\\n-0.27  0.11  -0.36  -0.28  0.44  0.06 \\\\\n0.14  0.39  0.07  -0.02  -0.23  0.28 \\\\\n0.31  -0.22  0.18  -0.35  0.09  -0.27 \\\\\n-0.19  0.33  -0.12  0.29  0.37  -0.15\n\\end{bmatrix}\n$$\n- 真实值 $x^{\\star} = [0.0,\\, 1.5,\\, 0.0,\\, -2.0,\\, 0.0,\\, 0.5]$。\n- 噪声 $\\eta = [0.01,\\,-0.02,\\,0.015,\\,0.0,\\,-0.005,\\,0.008,\\,0.012,\\,-0.009]$。\n- 观测值 $y = A x^{\\star} + \\eta$。\n- 正则化 $\\lambda = 0.1$。\n\n测试用例 2 (极大正则化；空支撑集边缘情况):\n- 使用与测试用例 1 中相同的 $A$。\n- 真实值 $x^{\\star} = [0.0,\\, 0.0,\\, 2.0,\\, 0.0,\\, 0.0,\\, 0.0]$。\n- 噪声 $\\eta = [0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0]$。\n- 观测值 $y = A x^{\\star} + \\eta$。\n- 正则化 $\\lambda = 100.0$。\n\n测试用例 3 (相关列；中等正则化):\n- 维度：$m = 8$, $n = 6$。\n- 矩阵 $A$:\n$$\n\\begin{bmatrix}\n0.40  0.50  0.49  -0.10  0.02  0.33 \\\\\n-0.20  -0.25  -0.24  0.12  -0.18  -0.31 \\\\\n0.35  0.42  0.41  -0.22  0.27  0.05 \\\\\n-0.05  -0.06  -0.06  0.30  0.12  -0.28 \\\\\n0.10  0.12  0.12  -0.26  -0.33  0.18 \\\\\n0.28  0.35  0.34  0.04  0.15  -0.11 \\\\\n-0.17  -0.21  -0.21  0.09  -0.07  0.24 \\\\\n0.22  0.27  0.26  -0.19  0.31  -0.09\n\\end{bmatrix}\n$$\n- 真实值 $x^{\\star} = [0.0,\\, 1.2,\\, 1.0,\\, 0.0,\\, 0.0,\\, 0.0]$。\n- 噪声 $\\eta = [0.003,\\,-0.004,\\,0.002,\\,0.006,\\,-0.005,\\,-0.001,\\,0.004,\\,-0.003]$。\n- 观测值 $y = A x^{\\star} + \\eta$。\n- 正则化 $\\lambda = 0.15$。\n\n对于每个测试用例，计算：\n1. 通过 ISTA 计算 Lasso 估计 $\\widehat{x}_{\\text{Lasso}}$，步长为 $t = 1/\\|A\\|_{2}^{2}$，从零初始化，迭代直至收敛（定义为 $\\|x^{(k+1)} - x^{(k)}\\|_{2} \\le 10^{-8}$）或达到最大迭代次数 $20000$。\n2. 使用 $\\tau_{\\text{sup}} = 10^{-6}$ 估计支撑集 $\\widehat{S}$。\n3. 使用 Moore–Penrose 伪逆，在 $\\widehat{S}$ 上通过最小二乘法计算重拟合估计 $\\widehat{x}_{\\text{refit}}$。\n4. $\\widehat{x}_{\\text{Lasso}}$ 和 $\\widehat{x}_{\\text{refit}}$ 的平均绝对偏差和最大绝对偏差。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，按顺序 $[\\operatorname{MAB}(\\widehat{x}_{\\text{Lasso}} - x^{\\star}), \\operatorname{MAB}(\\widehat{x}_{\\text{refit}} - x^{\\star}), \\operatorname{MaxAB}(\\widehat{x}_{\\text{Lasso}} - x^{\\star}), \\operatorname{MaxAB}(\\widehat{x}_{\\text{refit}} - x^{\\star})]$ 输出一个包含四个浮点数的列表。将每个用例的三个列表聚合到一个顶层列表中，因此最终输出格式为\n$$\n\\big[ [r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}], [r_{2,1}, r_{2,2}, r_{2,3}, r_{2,4}], [r_{3,1}, r_{3,2}, r_{3,3}, r_{3,4}] \\big].\n$$\n不涉及物理单位，所有答案必须是实数。", "solution": "该问题要求在压缩感知的背景下，实现并比较两种稀疏信号估计算法：Lasso 及其使用最小二乘重拟合的去偏变体。我们被给予了完整的算法和分析框架，来对三个不同的测试用例执行此任务。解决方案涉及数值优化、线性代数和统计评估。\n\n核心问题是找到一个稀疏解 $x \\in \\mathbb{R}^{n}$，以求解线性方程组 $y = Ax + \\eta$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个传感矩阵，$y \\in \\mathbb{R}^{m}$ 是带噪测量值，而 $\\eta \\in \\mathbb{R}^{m}$ 是加性噪声。假设真实信号 $x^{\\star} \\in \\mathbb{R}^{n}$ 是稀疏的。\n\n首先，我们计算 Lasso 估计 $\\widehat{x}_{\\text{Lasso}}$，它被定义为以下凸优化问题的解：\n$$\n\\widehat{x}_{\\text{Lasso}} = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}.\n$$\n这里，$\\lambda \\ge 0$ 是一个正则化参数，它控制数据保真项 $\\|A x - y\\|_{2}^{2}$ 和稀疏诱导惩罚项 $\\|x\\|_{1}$ 之间的权衡。目标函数是一个光滑、凸、可微函数 $g(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ 和一个非光滑、凸函数 $h(x) = \\lambda \\|x\\|_{1}$ 的和。\n\n问题指定使用迭代软阈值算法 (ISTA)，一种近端梯度方法，来求解 $\\widehat{x}_{\\text{Lasso}}$。从初始猜测 $x^{(0)} = 0$ 开始，ISTA 通过迭代更新规则生成一个估计序列：\n$$\nx^{(k+1)} = \\operatorname{prox}_{t h}(x^{(k)} - t \\nabla g(x^{(k)})).\n$$\n此更新包括两个步骤：对光滑部分 $g(x)$ 进行标准的梯度下降步骤，以及应用非光滑部分 $h(x)$ 的近端算子。$g(x)$ 的梯度是 $\\nabla g(x) = A^{\\top}(A x - y)$。$h(x) = \\lambda \\|x\\|_{1}$ 的近端算子是软阈值函数，按坐标应用：\n$$\n(\\operatorname{prox}_{t h}(z))_i = \\operatorname{soft}(z_i, \\lambda t) = \\operatorname{sign}(z_i) \\cdot \\max(|z_i| - \\lambda t, 0).\n$$\n步长 $t$ 必须满足 $0  t \\le 1/L$ 以保证收敛，其中 $L$ 是梯度 $\\nabla g(x)$ 的 Lipschitz 常数。对于此问题，$L$ 是矩阵 $A$ 的谱范数的平方，即 $L = \\|A\\|_{2}^{2} = \\sigma_{\\max}^2(A)$，其中 $\\sigma_{\\max}(A)$ 是 $A$ 的最大奇异值。我们将使用步长的上界，即 $t = 1/L = 1/\\|A\\|_{2}^{2}$。迭代将持续进行，直到估计值的变化足够小，即 $\\|x^{(k+1)} - x^{(k)}\\|_{2} \\le 10^{-8}$，或者达到最大迭代次数 $20000$。\n\nLasso 中的 $\\ell_1$ 惩罚项已知会导致收缩，这会在估计的非零系数中引入偏差。为了减轻这种偏差，通过一个两阶段过程计算第二个估计器 $\\widehat{x}_{\\text{refit}}$。首先，我们识别 Lasso 解的支撑集（非零系数的索引集）：\n$$\n\\widehat{S} = \\{ i \\in \\{1,\\dots,n\\} : |\\widehat{x}_{\\text{Lasso}, i}|  \\tau_{\\text{sup}} \\},\n$$\n其中 $\\tau_{\\text{sup}} = 10^{-6}$ 是一个用于考虑数值精度的小阈值。\n\n其次，我们对这个估计的支撑集执行一个无惩罚的普通最小二乘 (OLS) 拟合。这涉及求解：\n$$\n\\min_{z \\in \\mathbb{R}^{|\\widehat{S}|}} \\|A_{\\widehat{S}} z - y\\|_{2}^{2},\n$$\n其中 $A_{\\widehat{S}}$ 是由索引在 $\\widehat{S}$ 中的列组成的 $A$ 的子矩阵。这个 OLS 问题的解由 $z^{\\text{LS}} = A_{\\widehat{S}}^{\\dagger} y$ 给出，其中 $A_{\\widehat{S}}^{\\dagger}$ 是 $A_{\\widehat{S}}$ 的 Moore-Penrose 伪逆。即使 $A_{\\widehat{S}}$ 是秩亏的或病态的，使用伪逆也能确保一个唯一、稳定的解。然后，通过将其在支撑集 $\\widehat{S}$ 上的系数设置为 $z^{\\text{LS}}$ 并将所有其他系数设置为零来构造重拟合估计 $\\widehat{x}_{\\text{refit}} \\in \\mathbb{R}^{n}$：\n$$\n(\\widehat{x}_{\\text{refit}})_i = \\begin{cases} (z^{\\text{LS}})_j  \\text{if } i \\text{ is the } j\\text{-th index in } \\widehat{S} \\\\ 0  \\text{if } i \\notin \\widehat{S} \\end{cases}.\n$$\n如果估计的支撑集 $\\widehat{S}$ 为空，则重拟合估计为零向量，即 $\\widehat{x}_{\\text{refit}} = 0$。\n\n最后，为了评估两种估计器的性能，我们为每个估计 $\\widehat{x} \\in \\{\\widehat{x}_{\\text{Lasso}}, \\widehat{x}_{\\text{refit}}\\}$ 计算坐标级偏差向量 $b = \\widehat{x} - x^{\\star}$。偏差通过两个指标进行总结：平均绝对偏差 (MAB) 和最大绝对偏差 (MaxAB)，定义如下：\n$$\n\\operatorname{MAB}(b) = \\frac{1}{n} \\sum_{i=1}^{n} |b_{i}|, \\quad \\operatorname{MaxAB}(b) = \\max_{1 \\le i \\le n} |b_{i}|.\n$$\n\n我们将把这整个过程应用于提供的三个测试用例中的每一个。对于每种情况，我们首先根据给定的 $A$、$x^{\\star}$ 和 $\\eta$ 构造测量向量 $y = A x^{\\star} + \\eta$。然后，我们实现 ISTA 算法来找到 $\\widehat{x}_{\\text{Lasso}}$，接着进行支撑集识别和最小二乘重拟合来找到 $\\widehat{x}_{\\text{refit}}$。随后，我们计算四个指定的偏差指标，并以要求的格式报告它们。这个系统性的过程允许在问题设置的不同条件下，直接比较 Lasso 和去偏重拟合估计器的偏差属性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the debiasing sparse solutions problem for three test cases.\n    For each case, it computes the Lasso estimate via ISTA, performs\n    least-squares refitting, and calculates bias metrics for both estimates.\n    \"\"\"\n\n    # --- Helper Functions ---\n\n    def soft_threshold(z, tau):\n        \"\"\"\n        Soft-thresholding operator.\n        \"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    def ista_solver(A, y, lambda_val, max_iter=20000, tol=1e-8):\n        \"\"\"\n        Iterative Soft-Thresholding Algorithm (ISTA) for solving Lasso.\n        \"\"\"\n        m, n = A.shape\n        # L = np.linalg.norm(A, ord=2)**2 is slow. Using SVD is faster.\n        # As L is the largest eigenvalue of A.T @ A, we compute it directly.\n        L = np.max(np.linalg.eigvalsh(A.T @ A))\n        t = 1.0 / L\n        \n        x_k = np.zeros(n)\n        for _ in range(max_iter):\n            grad = A.T @ (A @ x_k - y)\n            z = x_k - t * grad\n            x_k_plus_1 = soft_threshold(z, lambda_val * t)\n            \n            if np.linalg.norm(x_k_plus_1 - x_k) = tol:\n                break\n            \n            x_k = x_k_plus_1\n        \n        return x_k\n\n    def refit_least_squares(A, y, x_lasso, tau_sup=1e-6):\n        \"\"\"\n        Performs least-squares refitting on the support of the Lasso estimate.\n        \"\"\"\n        n = A.shape[1]\n        support = np.where(np.abs(x_lasso) > tau_sup)[0]\n        \n        x_refit = np.zeros(n)\n        \n        if support.size > 0:\n            A_S = A[:, support]\n            try:\n                z_ls = np.linalg.pinv(A_S) @ y\n                x_refit[support] = z_ls\n            except np.linalg.LinAlgError:\n                # This case is unlikely with pseudoinverse but good practice.\n                pass\n                \n        return x_refit\n\n    def calculate_bias_metrics(x_est, x_star):\n        \"\"\"\n        Calculates MAB and MaxAB for a given estimate.\n        \"\"\"\n        bias = x_est - x_star\n        mab = np.mean(np.abs(bias))\n        max_ab = np.max(np.abs(bias))\n        return mab, max_ab\n\n    # --- Test Case Definitions ---\n\n    A1 = np.array([\n        [0.36, -0.07, 0.22, 0.10, -0.31, 0.41],\n        [-0.12, 0.25, 0.30, -0.41, 0.05, -0.08],\n        [0.45, 0.18, -0.08, 0.03, 0.26, -0.19],\n        [0.05, -0.31, 0.41, 0.17, -0.02, 0.12],\n        [-0.27, 0.11, -0.36, -0.28, 0.44, 0.06],\n        [0.14, 0.39, 0.07, -0.02, -0.23, 0.28],\n        [0.31, -0.22, 0.18, -0.35, 0.09, -0.27],\n        [-0.19, 0.33, -0.12, 0.29, 0.37, -0.15]\n    ])\n\n    A3 = np.array([\n        [0.40, 0.50, 0.49, -0.10, 0.02, 0.33],\n        [-0.20, -0.25, -0.24, 0.12, -0.18, -0.31],\n        [0.35, 0.42, 0.41, -0.22, 0.27, 0.05],\n        [-0.05, -0.06, -0.06, 0.30, 0.12, -0.28],\n        [0.10, 0.12, 0.12, -0.26, -0.33, 0.18],\n        [0.28, 0.35, 0.34, 0.04, 0.15, -0.11],\n        [-0.17, -0.21, -0.21, 0.09, -0.07, 0.24],\n        [0.22, 0.27, 0.26, -0.19, 0.31, -0.09]\n    ])\n\n    test_cases = [\n        {\n            \"A\": A1,\n            \"x_star\": np.array([0.0, 1.5, 0.0, -2.0, 0.0, 0.5]),\n            \"eta\": np.array([0.01, -0.02, 0.015, 0.0, -0.005, 0.008, 0.012, -0.009]),\n            \"lambda_val\": 0.1\n        },\n        {\n            \"A\": A1,\n            \"x_star\": np.array([0.0, 0.0, 2.0, 0.0, 0.0, 0.0]),\n            \"eta\": np.zeros(8),\n            \"lambda_val\": 100.0\n        },\n        {\n            \"A\": A3,\n            \"x_star\": np.array([0.0, 1.2, 1.0, 0.0, 0.0, 0.0]),\n            \"eta\": np.array([0.003, -0.004, 0.002, 0.006, -0.005, -0.001, 0.004, -0.003]),\n            \"lambda_val\": 0.15\n        }\n    ]\n\n    # --- Main Processing Loop ---\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        x_star = case[\"x_star\"]\n        eta = case[\"eta\"]\n        lambda_val = case[\"lambda_val\"]\n        \n        # 1. Compute measurements\n        y = A @ x_star + eta\n        \n        # 2. Compute Lasso estimate\n        x_lasso = ista_solver(A, y, lambda_val)\n        \n        # 3. Compute refitted estimate\n        x_refit = refit_least_squares(A, y, x_lasso)\n        \n        # 4. Calculate bias metrics\n        mab_lasso, maxab_lasso = calculate_bias_metrics(x_lasso, x_star)\n        mab_refit, maxab_refit = calculate_bias_metrics(x_refit, x_star)\n        \n        case_results = [mab_lasso, mab_refit, maxab_lasso, maxab_refit]\n        results.append(case_results)\n\n    # --- Final Output ---\n    # Convert list of lists to string representation as specified.\n    # The default str(list) includes spaces after commas, which is acceptable.\n    # The join then combines these string representations with a comma.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3442554"}, {"introduction": "除了简单的后处理修正，我们还可以通过更精巧的迭代方法来主动减少偏差。本练习介绍了一种强大的技术——重加权$\\ell_1$最小化，它通过自适应地调整惩罚权重来对抗标准Lasso的均匀收缩效应。您将实现此重加权方案，观察支撑集和系数值如何随迭代演化，并最终将结果与最终的最小二乘重拟合进行比较，以评估组合效应。", "problem": "给定一个在线性反问题框架下的稀疏恢复与去偏任务。令 $A \\in \\mathbb{R}^{m \\times n}$，$x^\\star \\in \\mathbb{R}^n$ 是一个 $k$-稀疏向量（最多有 $k$ 个非零项），$y \\in \\mathbb{R}^m$ 是由 $y = A x^\\star + \\eta$ 建模的带噪观测值，其中 $\\eta$ 是加性噪声。诱导稀疏性的点估计通过加权 $\\ell_1$ 正则化最小二乘目标函数来构建，其基本的非加权形式是压缩感知中用于稀疏优化的标准工具。任务是实现两轮迭代的重加权 $\\ell_1$ 最小化，然后对最终的支撑集应用普通最小二乘重构，以进一步减小收缩偏差。\n\n基本基础和给定设置：\n- 数据模型为 $y = A x^\\star + \\eta$，且测量矩阵 $A$ 已知。\n- 初始稀疏估计器通过求解一个凸优化问题获得，该问题惩罚系数的 $\\ell_1$ 范数，这是稀疏恢复中对计数（$\\ell_0$）伪范数广泛接受的替代。\n- 重加权通过迭代地为小系数分配较大的权重、为大系数分配较小的权重来减小偏差，而在选定的支撑集上进行最小二乘重构则减小了由 $\\ell_1$ 惩罚项引起的收缩偏差。\n\n矩阵和向量规格：\n- 使用 $m = 64$，$n = 128$ 和 $k = 10$。\n- 生成矩阵 $A$，其元素独立采样自均值为零、经 $1/\\sqrt{m}$ 缩放的正态分布，即 $A_{ij} \\sim \\mathcal{N}(0, 1/m)$。\n- 生成一个支撑集大小为 $k$ 的基准稀疏向量 $x^\\star$，方法是无放回地均匀随机选择 $k$ 个索引，并将这些位置的元素设置为从 $\\mathcal{N}(0,1)$ 中独立抽取的值；所有其他元素设置为零。\n- 生成噪声 $\\eta$，其元素独立抽取自 $\\mathcal{N}(0, \\sigma^2)$，并设置 $y = A x^\\star + \\eta$。\n- 对所有随机数生成过程使用固定的随机种子 $12345$ 以确保可复现性。\n- 在所有测试用例中均使用固定的噪声标准差 $\\sigma = 0.02$，以保持 $y$ 不变。\n\n要实现的算法任务：\n1. 在一个重加权方案中求解两次加权 $\\ell_1$ 正则化最小二乘问题，从非加权情况开始：\n   - 第 0 轮迭代：当所有权重等于 $1$ 时，求解 $x^{(0)}$。\n   - 第 1 轮迭代：使用一个严格为正的参数 $\\epsilon$ 从 $x^{(0)}$ 计算新权重，并求解 $x^{(1)}$。\n   - 第 2 轮迭代：从 $x^{(1)}$ 计算新权重，并求解 $x^{(2)}$。\n   加权优化问题具有以下形式\n   $$\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\sum_{i=1}^n w_i |x_i|,$$\n   其中 $w_i$ 是在每次重加权迭代中确定的正权重。您必须使用基于一阶最优性条件和近端微积分（迭代软阈值算法）推导出的有原则的算法来获得每个加权问题的解，并确保步长是根据数据保真项梯度的 Lipschitz 常数选择的。初始权重必须对所有 $i$ 均为 $w_i = 1$。\n2. 将估计值 $x$ 的支撑集定义为幅度超过阈值 $\\tau$ 的元素索引集：$S(x) = \\{ i \\in \\{1,\\dots,n\\} : |x_i|  \\tau \\}$。\n3. 通过求解以下问题，在最终支撑集 $S(x^{(2)})$ 上执行最小二乘重构\n   $$\\min_{z \\in \\mathbb{R}^{|S(x^{(2)})|}} \\|A_{S(x^{(2)})} z - y\\|_2^2,$$\n   然后将解嵌入回 $\\mathbb{R}^n$ 中，方法是将系数放置在 $S(x^{(2)})$ 上，并在其他位置置零。\n\n报告要求：\n- 对于每个测试用例，报告：\n  1. 初始非加权求解后的支撑集大小， $|S(x^{(0)})|$（一个整数）。\n  2. 第二次重加权求解后的支撑集大小， $|S(x^{(2)})|$（一个整数）。\n  3. 从第 0 轮到第 1 轮迭代的支撑集变化幅度，定义为对称差的大小 $|S(x^{(0)}) \\,\\triangle\\, S(x^{(1)})|$（一个整数）。\n  4. 从第 1 轮到第 2 轮迭代的支撑集变化幅度，定义为 $|S(x^{(1)}) \\,\\triangle\\, S(x^{(2)})|$（一个整数）。\n  5. $x^{(2)}$ 在真实支撑集 $S(x^\\star)$ 上的平均系数幅度偏差，定义为\n     $$b_{\\text{pre}} = \\frac{1}{|S(x^\\star)|} \\sum_{i \\in S(x^\\star)} \\left( |x^{(2)}_i| - |x^\\star_i| \\right) \\in \\mathbb{R}$$\n     （一个浮点数；负值表示相对于基准真相的收缩）。\n  6. 最小二乘重构后在真实支撑集上的平均系数幅度偏差，定义为\n     $$b_{\\text{post}} = \\frac{1}{|S(x^\\star)|} \\sum_{i \\in S(x^\\star)} \\left( |x^{\\text{LS}}_i| - |x^\\star_i| \\right) \\in \\mathbb{R},$$\n     其中 $x^{\\text{LS}}$ 是限制在 $S(x^{(2)})$ 上的最小二乘重构结果（一个浮点数）。\n\n测试套件：\n在以下四个测试用例上运行您的程序，所有用例均使用上述方法构建的相同 $A$ 和 $y$（$\\sigma = 0.02$，种子为 $12345$）。每个测试用例指定了正则化参数 $\\lambda$、重加权参数 $\\epsilon$ 和支撑集阈值 $\\tau$：\n- 测试用例 1：$\\lambda = 0.05$，$\\epsilon = 10^{-3}$，$\\tau = 10^{-4}$。\n- 测试用例 2：$\\lambda = 0.10$，$\\epsilon = 10^{-3}$，$\\tau = 10^{-4}$。\n- 测试用例 3：$\\lambda = 0.05$，$\\epsilon = 10^{-6}$，$\\tau = 10^{-4}$。\n- 测试用例 4：$\\lambda = 0.05$，$\\epsilon = 10^{-3}$，$\\tau = 10^{-2}$。\n\n输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例对应一个子列表，按给定顺序排列，子列表中的元素严格按照 $[|S(x^{(0)})|, |S(x^{(2)})|, |S(x^{(0)}) \\,\\triangle\\, S(x^{(1)})|, |S(x^{(1)}) \\,\\triangle\\, S(x^{(2)})|, b_{\\text{pre}}, b_{\\text{post}}]$ 的顺序排列。例如，总输出应如下所示：\n$$\\texttt{[[s0\\_1,s2\\_1,c01\\_1,c12\\_1,bpre\\_1,bpost\\_1],[s0\\_2,s2\\_2,c01\\_2,c12\\_2,bpre\\_2,bpost\\_2],[s0\\_3,s2\\_3,c01\\_3,c12\\_3,bpre\\_3,bpost\\_3],[s0\\_4,s2\\_4,c01\\_4,c12\\_4,bpre\\_4,bpost\\_4]]}$$\n此问题不涉及物理单位，因此您必须报告纯数字。不涉及角度。如果出现百分比，必须以小数形式表示，但本任务不要求输出百分比。", "solution": "该问题要求实现并评估一个用于稀疏信号恢复的重加权 $\\ell_1$ 最小化算法，并随后执行一个最小二乘去偏步骤。该任务被构建为一个线性反问题，这是信号处理、统计学和机器学习中的一个标准框架。\n\n底层数据生成模型由以下线性方程给出：\n$$ y = A x^\\star + \\eta $$\n其中 $x^\\star \\in \\mathbb{R}^n$ 是我们旨在恢复的未知 $k$-稀疏信号，$A \\in \\mathbb{R}^{m \\times n}$ 是测量或传感矩阵，$\\eta \\in \\mathbb{R}^m$ 代表加性噪声，$y \\in \\mathbb{R}^m$ 是观测到的测量向量。维度指定为 $m=64$，$n=128$，稀疏度为 $k=10$。矩阵 $A$ 的元素独立地从正态分布 $\\mathcal{N}(0, 1/m)$ 中抽取，真实信号 $x^\\star$ 的非零元素从 $\\mathcal{N}(0,1)$ 中抽取，噪声分量从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取，其中 $\\sigma = 0.02$。\n\n为了从 $y$ 和 $A$ 中恢复 $x^\\star$ 的估计值，我们求解以下加权 $\\ell_1$ 正则化最小二乘优化问题：\n$$ \\min_{x \\in \\mathbb{R}^n} J(x) \\quad \\text{其中} \\quad J(x) = \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\sum_{i=1}^n w_i |x_i| $$\n这里，$\\lambda  0$ 是一个在数据保真度与稀疏性之间取得平衡的正则化参数，$w_i  0$ 是可以调整以改善解的权重。该目标函数是两部分凸函数的和：一个光滑、可微的数据保真项 $f(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ 和一个非光滑、凸的正则化项 $g(x) = \\lambda \\sum_{i=1}^n w_i |x_i|$。\n\n这种结构使得该问题适合使用近端梯度方法。所采用的具体算法是迭代软阈值算法（ISTA），其遵循以下更新规则：\n$$ x_{t+1} = \\text{prox}_{\\alpha g} \\left( x_t - \\alpha \\nabla f(x_t) \\right) $$\n其中 $t$ 是迭代索引，$\\alpha$ 是步长。数据保真项的梯度是 $\\nabla f(x) = A^T(Ax - y)$。加权 $\\ell_1$ 范数的近端算子是逐元素的软阈值函数：\n$$ \\left( \\text{prox}_{\\alpha g}(z) \\right)_i = \\text{S}_{\\alpha \\lambda w_i}(z_i) = \\text{sign}(z_i) \\max(|z_i| - \\alpha \\lambda w_i, 0) $$\n为使 ISTA 收敛，步长 $\\alpha$ 的选择必须满足 $0  \\alpha \\le 1/L$，其中 $L$ 是梯度 $\\nabla f(x)$ 的 Lipschitz 常数。该常数等于 $A^T A$ 的最大特征值，也即 $A$ 的最大奇异值的平方，$L = \\sigma_{\\max}(A)^2$。我们将使用步长 $\\alpha = 1/L$。\n\n因此，完整的 ISTA 更新过程为：\n$$ x_{t+1} = \\text{S}_{\\frac{\\lambda}{L} w} \\left( x_t - \\frac{1}{L} A^T(Ax_t - y) \\right) $$\n其中软阈值操作是逐元素应用的，其阈值向量由权重 $w$ 构建。\n\n问题指定了一个重加权方案以减小 $\\ell_1$ 正则化固有的偏差。该方案包括三个主要求解步骤：\n1.  **第 0 轮迭代**：使用非加权问题求解 $x^{(0)}$，这对应于将所有权重设置为 $w_i^{(0)} = 1$。\n2.  **第 1 轮迭代**：根据第一步的结果更新权重。$x^{(0)}$ 中较大的系数幅度表明它更可能是真实支撑集的一部分，因此应该受到较少的惩罚。权重更新为 $w_i^{(1)} = 1 / (|x_i^{(0)}| + \\epsilon)$，其中 $\\epsilon  0$ 是一个确保稳定性的微小参数。使用这些新权重执行 ISTA 求解以获得 $x^{(1)}$。\n3.  **第 2 轮迭代**：重复此过程。根据 $x^{(1)}$ 计算新权重 $w_i^{(2)} = 1 / (|x_i^{(1)}| + \\epsilon)$，并通过求解相应的加权问题找到最终的正则化估计 $x^{(2)}$。\n\n从 ISTA 得到的解向量 $x^{(j)}$ 并非完全稀疏。必须通过阈值化来确定支撑集。估计值 $x$ 的支撑集定义为 $S(x) = \\{ i : |x_i|  \\tau \\}$，其中 $\\tau$ 是一个很小的阈值。\n\n最后，为了进一步减轻 $\\ell_1$ 惩罚项引起的收缩偏差，执行一个最小二乘重构步骤。这包括在最终支撑集 $S_{\\text{final}} = S(x^{(2)})$ 上求解一个无正则化的最小二乘问题。令 $A_{S_{\\text{final}}}$ 为 $A$ 中仅包含由 $S_{\\text{final}}$ 索引的列构成的子矩阵。去偏后的系数 $z^{\\text{LS}}$ 通过求解以下问题得到：\n$$ \\min_{z \\in \\mathbb{R}^{|S_{\\text{final}}|}} \\|A_{S_{\\text{final}}} z - y\\|_2^2 $$\n其闭式解为 $z^{\\text{LS}} = (A_{S_{\\text{final}}}^T A_{S_{\\text{final}}})^\\dagger A_{S_{\\text{final}}}^T y$，其中 $\\dagger$ 表示 Moore-Penrose 伪逆。最终的去偏估计 $x^{\\text{LS}}$ 通过将 $z^{\\text{LS}}$ 的系数放置到支撑集索引 $S_{\\text{final}}$ 上并将所有其他项设置为零来构建。\n\n整个过程对四个测试用例执行，改变参数 $\\lambda$、$\\epsilon$ 和 $\\tau$。对于每个用例，我们报告六个指标：$x^{(0)}$ 和 $x^{(2)}$ 的支撑集大小、迭代间的支撑集变化，以及在最小二乘重构前后真实支撑集上的平均系数幅度偏差。所有随机过程均使用种子 $12345$ 以保证可复现性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a sparse recovery and debiasing problem using reweighted l1 minimization.\n    \"\"\"\n\n    # --- Problem Setup ---\n    m, n, k = 64, 128, 10\n    sigma = 0.02\n    seed = 12345\n\n    # --- Data Generation ---\n    rng = np.random.default_rng(seed)\n\n    # Generate matrix A\n    A = rng.normal(0, 1 / np.sqrt(m), size=(m, n))\n\n    # Generate sparse vector x_star\n    support_star_indices = rng.choice(n, k, replace=False)\n    x_star = np.zeros(n)\n    x_star[support_star_indices] = rng.normal(0, 1, size=k)\n    support_star_set = set(support_star_indices)\n\n    # Generate noise and observation y\n    eta = rng.normal(0, sigma, size=m)\n    y = A @ x_star + eta\n\n    # --- Algorithm Parameters ---\n    # Lipschitz constant of the gradient of the least-squares term\n    L = np.linalg.svd(A, compute_uv=False)[0] ** 2\n    ista_step_size = 1.0 / L\n    ista_iterations = 5000\n\n    # Test cases\n    test_cases = [\n        # (lambda, epsilon, tau)\n        (0.05, 1e-3, 1e-4),\n        (0.10, 1e-3, 1e-4),\n        (0.05, 1e-6, 1e-4),\n        (0.05, 1e-3, 1e-2),\n    ]\n\n    all_results = []\n\n    # --- Helper Functions ---\n    def soft_threshold(z, T):\n        \"\"\"Element-wise soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - T, 0)\n\n    def ista_solve(A, y, lambda_val, weights):\n        \"\"\"\n        Solves the weighted l1-regularized least-squares problem using ISTA.\n        \"\"\"\n        x = np.zeros(A.shape[1])\n        At = A.T\n        thresholds = lambda_val * weights * ista_step_size\n        \n        for _ in range(ista_iterations):\n            gradient = At @ (A @ x - y)\n            z = x - ista_step_size * gradient\n            x = soft_threshold(z, thresholds)\n        return x\n\n    def get_support(x, tau):\n        \"\"\"Identifies the support of a vector based on a magnitude threshold.\"\"\"\n        return set(np.where(np.abs(x) > tau)[0])\n\n    def ls_refit(A, y, support_indices):\n        \"\"\"Performs least-squares refitting on the identified support.\"\"\"\n        x_ls = np.zeros(A.shape[1])\n        if not support_indices:\n            return x_ls\n        \n        support_list = sorted(list(support_indices))\n        A_S = A[:, support_list]\n        \n        # Solve the least-squares problem: min ||A_S z - y||_2^2\n        z, _, _, _ = np.linalg.lstsq(A_S, y, rcond=None)\n        \n        x_ls[support_list] = z\n        return x_ls\n\n    for lambda_val, epsilon, tau in test_cases:\n        case_results = []\n\n        # -- Iteration 0 (Unweighted l1) --\n        w0 = np.ones(n)\n        x0 = ista_solve(A, y, lambda_val, w0)\n        S0 = get_support(x0, tau)\n        case_results.append(len(S0))\n\n        # -- Iteration 1 (Reweighted) --\n        w1 = 1.0 / (np.abs(x0) + epsilon)\n        x1 = ista_solve(A, y, lambda_val, w1)\n        S1 = get_support(x1, tau)\n\n        # -- Iteration 2 (Reweighted) --\n        w2 = 1.0 / (np.abs(x1) + epsilon)\n        x2 = ista_solve(A, y, lambda_val, w2)\n        S2 = get_support(x2, tau)\n        case_results.append(len(S2))\n\n        # -- Support Change Metrics --\n        change01 = len(S0.symmetric_difference(S1))\n        case_results.append(change01)\n        change12 = len(S1.symmetric_difference(S2))\n        case_results.append(change12)\n\n        # -- Least-Squares Refit on final support S2 --\n        x_ls = ls_refit(A, y, S2)\n\n        # -- Bias Metrics on True Support --\n        x_star_on_support = x_star[support_star_indices]\n        x2_on_support = x2[support_star_indices]\n        x_ls_on_support = x_ls[support_star_indices]\n\n        # Bias before refit\n        b_pre = np.mean(np.abs(x2_on_support) - np.abs(x_star_on_support))\n        case_results.append(b_pre)\n        \n        # Bias after refit\n        b_post = np.mean(np.abs(x_ls_on_support) - np.abs(x_star_on_support))\n        case_results.append(b_post)\n\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The output format is a list of lists.\n    # `repr` is used to get the string representation of floats without losing precision.\n    formatted_results = [\n        f\"[{res[0]},{res[1]},{res[2]},{res[3]},{repr(res[4])},{repr(res[5])}]\"\n        for res in all_results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3442510"}, {"introduction": "去偏的思想具有广泛的适用性，并不局限于标准的稀疏模型。许多现实世界的问题表现出结构化稀疏性，即系数以组（group）的形式整体稀疏，而非个体稀疏。本练习将去偏原理推广到组Lasso（Group Lasso）的情境中。您将为组Lasso目标函数实现近端梯度算法，并对所选定的组别应用最小二乘重拟合，从而证明去偏方法在标准Lasso模型之外的通用性和有效性。", "problem": "考虑一个具有块结构设计矩阵和预定义分组的线性测量模型。设 $A \\in \\mathbb{R}^{n \\times p}$，$y \\in \\mathbb{R}^{n}$，以及列索引 $\\{1,2,\\ldots,p\\}$ 的一个划分为不相交的组 $\\mathcal{G} = \\{g_{1}, g_{2}, \\ldots, g_{m}\\}$，其中每个 $g_{j} \\subset \\{1,2,\\ldots,p\\}$ 且当 $j \\neq k$ 时 $g_{j} \\cap g_{k} = \\emptyset$，并且 $\\bigcup_{j=1}^{m} g_{j} = \\{1,2,\\ldots,p\\}$。未知系数向量为 $x \\in \\mathbb{R}^{p}$，其在组 $g$ 上的限制为 $x_{g} \\in \\mathbb{R}^{|g|}$。假设数据 $y$ 是通过线性模型 $y = A x_{\\star} + \\varepsilon$ 从一个真实向量 $x_{\\star} \\in \\mathbb{R}^{p}$ 生成的，其中 $\\varepsilon$ 是加性噪声。\n\n定义组最小绝对收缩和选择算子（group LASSO）估计量为凸优化问题的任意解\n$$\n\\widehat{x}^{\\mathrm{GL}} \\in \\arg\\min_{x \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g} \\|x_{g}\\|_{2} \\right\\},\n$$\n其中 $\\lambda  0$ 是一个正则化参数，$w_{g} = \\sqrt{|g|}$ 是组 $g$ 的权重。如果 $\\|\\widehat{x}^{\\mathrm{GL}}_{g}\\|_{2}  0$，则认为组 $g$ 被选中。令 $S \\subset \\{1,2,\\ldots,p\\}$ 表示属于所有被选中组的索引的并集。\n\n定义后选择最小二乘（LS）重拟合为 $\\widehat{x}^{\\mathrm{LS}} \\in \\mathbb{R}^{p}$，其中在 $S$ 之外的坐标严格为零，在 $S$ 之内的坐标通过求解限制在 $S$ 上的残差最小化问题得到：\n$$\n\\widehat{x}^{\\mathrm{LS}}_{S} \\in \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\|A_{S} z - y\\|_{2}, \\quad \\widehat{x}^{\\mathrm{LS}}_{S^{c}} = 0,\n$$\n其中 $A_{S} \\in \\mathbb{R}^{n \\times |S|}$ 表示由 $S$ 索引的列组成的 $A$ 的子矩阵。LS 重拟合用于对组 LASSO 估计中由收缩引起的偏差进行去偏。\n\n您的任务是实现一个程序，对于下面定义的每个测试用例，该程序能够生成 $\\widehat{x}^{\\mathrm{GL}}$，识别被选中的组，构造 $S$，计算受限的 LS 重拟合 $\\widehat{x}^{\\mathrm{LS}}$，并通过标量\n$$\nB = \\|\\widehat{x}^{\\mathrm{GL}} - x_{\\star}\\|_{2} - \\|\\widehat{x}^{\\mathrm{LS}} - x_{\\star}\\|_{2}.\n$$\n来量化偏差减少量。\n每个测试用例的输出是实数 $B$。\n\n您必须按照规定为每个测试用例构造合成数据 $A$、$x_{\\star}$ 和 $y$。所有随机元素必须通过为每个测试用例固定随机种子来保证可复现性。设计矩阵 $A$ 的列必须被归一化为单位 $\\ell_{2}$-范数。真实向量 $x_{\\star}$ 必须支撑在一组已知的活动组上，并且在每个活动组内，系数应从标准正态分布中抽取，然后进行缩放，使得该组的 $\\ell_{2}$-范数等于 $1$。\n\n程序必须能处理任意大小的组，并且必须数值稳定。如果选定的集合 $S$ 为空，则定义 $\\widehat{x}^{\\mathrm{LS}} = 0$，并相应地计算偏差减少量。\n\n测试套件规范：\n\n- 测试用例 1：$n = 60$，$p = 50$，组 $\\mathcal{G}$ 是 10 个大小为 5 的连续块，活动组的索引为 $[1,4,7]$（组编号从零开始），噪声标准差 $\\sigma = 0.1$，正则化参数 $\\lambda = 0.25$，随机种子为 1。\n- 测试用例 2：$n = 60$，$p = 50$，组 $\\mathcal{G}$ 是 10 个大小为 5 的连续块，活动组的索引为 $[2,3]$，噪声标准差 $\\sigma = 0.5$，正则化参数 $\\lambda = 0.6$，随机种子为 2。\n- 测试用例 3：$n = 60$，$p = 50$，组 $\\mathcal{G}$ 是 10 个大小为 5 的连续块，活动组的索引为 $[0,5,9]$，噪声标准差 $\\sigma = 0.1$，正则化参数 $\\lambda = 5.0$，随机种子为 3。\n- 测试用例 4：$n = 60$，$p = 50$，组 $\\mathcal{G}$ 是大小为 $[3,7,4,6,10,5,5,5,5]$ 的连续块，活动组的索引为 $[1,4,7]$，噪声标准差 $\\sigma = 0.0$，正则化参数 $\\lambda = 0.15$，随机种子为 4。\n\n所有角度（如有）必须以弧度为单位，尽管本问题不涉及角度。不涉及物理单位。最终输出必须是实数。\n\n最终输出格式：\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，即 $[B_{1},B_{2},B_{3},B_{4}]$，其中 $B_{i}$ 是如上定义的测试用例 $i$ 的偏差减少量。输出必须是浮点数。不得打印任何额外文本。", "solution": "该问题要求在具有分组协变量的线性模型背景下，实现并比较两种估计量：组最小绝对收缩和选择算子（Group LASSO）和后选择最小二乘（LS）重拟合。目标是量化通过 LS 重拟合实现的偏差减少。\n\n**1. 模型和数据生成**\n\n该问题基于线性模型 $y = A x_{\\star} + \\varepsilon$，其中 $y \\in \\mathbb{R}^{n}$ 是观测向量，$A \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$x_{\\star} \\in \\mathbb{R}^{p}$ 是未知的稀疏真实向量，$\\varepsilon \\in \\mathbb{R}^{n}$ 是加性噪声。系数向量的索引 $\\{1, \\ldots, p\\}$ 被划分为 $m$ 个不相交的组 $\\mathcal{G} = \\{g_1, \\ldots, g_m\\}$。\n\n对于每个测试用例，我们根据规范生成合成数据：\n- 为保证可复现性，随机数生成器需要设定种子。\n- 矩阵 $A$ 从标准正态分布中抽取，随后其列被归一化为单位 $\\ell_2$-范数，即对所有 $j \\in \\{1, \\ldots, p\\}$，都有 $\\|A_{:,j}\\|_{2} = 1$。\n- 真实向量 $x_{\\star}$ 被构造成组稀疏的。对于每个指定的活动组 $g$，系数向量 $(x_{\\star})_g$ 从标准正态分布中抽取，然后进行缩放以使其 $\\ell_2$-范数为 1，即 $\\|(x_{\\star})_g\\|_{2} = 1$。非活动组中的系数为零。\n- 噪声向量 $\\varepsilon$ 从各向同性的高斯分布中抽取，其标准差为 $\\sigma$，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n- 然后观测向量计算为 $y = A x_{\\star} + \\varepsilon$。\n\n**2. 组 LASSO 估计量**\n\n组 LASSO 估计量 $\\widehat{x}^{\\mathrm{GL}}$ 通过求解以下凸优化问题得到：\n$$\n\\widehat{x}^{\\mathrm{GL}} \\in \\arg\\min_{x \\in \\mathbb{R}^{p}} F(x) = \\arg\\min_{x \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} \\sqrt{|g|} \\|x_{g}\\|_{2} \\right\\}\n$$\n目标函数 $F(x)$ 是一个光滑、凸的数据保真项 $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ 和一个非光滑、凸的正则化项 $h(x) = \\lambda \\sum_{g \\in \\mathcal{G}} \\sqrt{|g|} \\|x_{g}\\|_{2}$ 的和。\n\n解决这种复合优化问题的一个标准且有效的算法是近端梯度下降（PGD），也称为迭代收缩阈值算法（ISTA）。PGD 的更新规则是：\n$$\nx^{(k+1)} = \\mathrm{prox}_{\\gamma h}\\left(x^{(k)} - \\gamma \\nabla f(x^{(k)})\\right)\n$$\n其中 $k$ 是迭代索引，$\\gamma$ 是步长，$\\nabla f(x)$ 是光滑项的梯度，$\\mathrm{prox}_{\\gamma h}(\\cdot)$ 是正则化项的近端算子。\n\n- 梯度为 $\\nabla f(x) = A^T(Ax - y)$。\n- 步长 $\\gamma$ 的选择必须满足 $\\gamma \\le 1/L$，其中 $L$ 是 $\\nabla f(x)$ 的利普希茨常数。$L$ 是 $A^T A$ 的最大特征值，即 $L = \\lambda_{\\max}(A^T A) = \\sigma_{\\max}(A)^2$。固定的步长 $\\gamma = 1/L$ 保证收敛。\n- $h(x)$ 的近端算子在各组之间是可分的，对应于逐组的软阈值操作。对于一个向量 $z \\in \\mathbb{R}^p$ 和每个组 $g \\in \\mathcal{G}$：\n$$\n[\\mathrm{prox}_{\\gamma h}(z)]_g = \\mathrm{prox}_{\\gamma \\lambda \\sqrt{|g|} \\|\\cdot\\|_2}(z_g) = \\left(1 - \\frac{\\gamma \\lambda \\sqrt{|g|}}{\\|z_g\\|_2}\\right)_{+} z_g\n$$\n其中 $(c)_{+} = \\max(0, c)$。此操作会收缩向量 $z_g$ 的范数，并在其范数低于阈值 $\\gamma \\lambda \\sqrt{|g|}$ 时将其设为零。\n\nPGD 算法以 $x^{(0)} = 0$ 初始化，并迭代进行，直到估计值的相对变化低于指定的容差。\n\n**3. 使用后选择最小二乘法进行去偏**\n\n由于正则化项的收缩效应，组 LASSO 估计量会偏向于零。为了减轻这种偏差，一种常用技术是执行后选择普通最小二乘（LS）重拟合。\n\n首先，从组 LASSO 解中识别出被选中的组集：如果 $\\|\\widehat{x}^{\\mathrm{GL}}_g\\|_2  0$，则组 $g$ 被选中。集合 $S$ 由属于这些被选中组的所有索引的并集构成。\n\n然后，通过求解一个限制在由 $S$ 索引的 $A$ 的列所定义的子空间上的标准最小二乘问题来计算 LS 估计量 $\\widehat{x}^{\\mathrm{LS}}$：\n$$\n\\widehat{x}^{\\mathrm{LS}}_{S} = \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\|A_{S} z - y\\|_{2}\n$$\n$\\widehat{x}^{\\mathrm{LS}}$ 中对应于不在 $S$ 中的索引的系数被设置为零，即 $\\widehat{x}^{\\mathrm{LS}}_{S^c} = 0$。这个 LS 问题可以使用例如 `numpy.linalg.lstsq` 函数进行稳健求解。如果集合 $S$ 为空（即组 LASSO 未选择任何组），$\\widehat{x}^{\\mathrm{LS}}$ 被定义为零向量。\n\n**4. 量化偏差减少**\n\n去偏过程的有效性通过比较每个估计量与真实向量 $x_{\\star}$ 的 $\\ell_2$-距离来衡量。偏差减少量 $B$ 定义为这些估计误差的差值：\n$$\nB = \\|\\widehat{x}^{\\mathrm{GL}} - x_{\\star}\\|_{2} - \\|\\widehat{x}^{\\mathrm{LS}} - x_{\\star}\\|_{2}\n$$\n一个正的 $B$ 值表明 LS 重拟合估计量比组 LASSO 估计量更接近真实向量 $x_{\\star}$，从而成功地减少了总估计误差。最终的实现为每个指定的测试用例计算这个值 $B$。", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        # n, p, group_spec, active_groups, sigma, lambda_reg, seed\n        (60, 50, (10, 5), [1, 4, 7], 0.1, 0.25, 1),\n        (60, 50, (10, 5), [2, 3], 0.5, 0.6, 2),\n        (60, 50, (10, 5), [0, 5, 9], 0.1, 5.0, 3),\n        (60, 50, [3, 7, 4, 6, 10, 5, 5, 5, 5], [1, 4, 7], 0.0, 0.15, 4),\n    ]\n\n    results = []\n    for params in test_cases:\n        b_val = calculate_bias_reduction(params)\n        results.append(b_val)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef calculate_bias_reduction(params):\n    \"\"\"\n    Runs a single test case: generates data, computes estimators, and returns bias reduction.\n    \"\"\"\n    n, p, group_spec, active_groups, sigma, lambda_reg, seed = params\n\n    # Create group structure from specification\n    if isinstance(group_spec, tuple):\n        num_groups, size_per_group = group_spec\n        group_sizes = [size_per_group] * num_groups\n    else:\n        group_sizes = group_spec\n    \n    current_idx = 0\n    groups = []\n    for size in group_sizes:\n        groups.append(np.arange(current_idx, current_idx + size))\n        current_idx += size\n\n    # Generate synthetic data\n    A, y, x_star = generate_data(n, p, groups, active_groups, sigma, seed)\n\n    # 1. Solve for Group LASSO estimate\n    x_gl = group_lasso_proximal_gradient(A, y, groups, lambda_reg)\n\n    # 2. Identify selected groups and form the index set S\n    selected_indices = []\n    for g in groups:\n        if np.linalg.norm(x_gl[g]) > 1e-9:  # Use a small tolerance for non-zero check\n            selected_indices.extend(g)\n    \n    S = sorted(list(set(selected_indices)))\n\n    # 3. Compute post-selection Least Squares (LS) refit\n    x_ls = np.zeros(p)\n    if len(S) > 0:\n        A_S = A[:, S]\n        # Use lstsq for numerical stability\n        z, _, _, _ = np.linalg.lstsq(A_S, y, rcond=None)\n        x_ls[S] = z\n\n    # 4. Quantify the bias reduction\n    error_gl = np.linalg.norm(x_gl - x_star)\n    error_ls = np.linalg.norm(x_ls - x_star)\n    B = error_gl - error_ls\n    \n    return B\n\ndef generate_data(n, p, groups, active_group_indices, sigma, seed):\n    \"\"\"\n    Generates synthetic data (A, y, x_star) for the Group LASSO problem.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate design matrix A with unit-norm columns\n    A = rng.standard_normal((n, p))\n    A /= np.linalg.norm(A, axis=0, keepdims=True)\n    \n    # Generate ground-truth vector x_star\n    x_star = np.zeros(p)\n    for group_idx in active_group_indices:\n        group_indices = groups[group_idx]\n        group_size = len(group_indices)\n        \n        # Draw coefficients from N(0,1)\n        coeffs = rng.standard_normal(group_size)\n        \n        # Scale group to have unit l2-norm\n        norm_coeffs = np.linalg.norm(coeffs)\n        if norm_coeffs > 0:\n            coeffs /= norm_coeffs\n            \n        x_star[group_indices] = coeffs\n        \n    # Generate noise and the measurement vector y\n    noise = rng.standard_normal(n) * sigma\n    y = A @ x_star + noise\n    \n    return A, y, x_star\n\ndef group_lasso_proximal_gradient(A, y, groups, lambda_reg, max_iter=2000, tol=1e-7):\n    \"\"\"\n    Solves the Group LASSO problem using Proximal Gradient Descent.\n    \"\"\"\n    n, p = A.shape\n    \n    # Precompute terms for efficiency\n    AtA = A.T @ A\n    Aty = A.T @ y\n    \n    # Determine step size from the Lipschitz constant of the gradient\n    L = scipy.linalg.svdvals(AtA)[0]\n    gamma = 1.0 / L\n    \n    x = np.zeros(p)\n    group_weights = np.array([np.sqrt(len(g)) for g in groups])\n    \n    for _ in range(max_iter):\n        x_old = x.copy()\n        \n        # Gradient step\n        grad = AtA @ x - Aty\n        z = x - gamma * grad\n        \n        # Proximal step (group-wise soft-thresholding)\n        for i, g in enumerate(groups):\n            z_g = z[g]\n            norm_z_g = np.linalg.norm(z_g)\n            \n            threshold = gamma * lambda_reg * group_weights[i]\n            \n            if norm_z_g > threshold:\n                shrinkage = 1.0 - threshold / norm_z_g\n                x[g] = shrinkage * z_g\n            else:\n                x[g] = 0.0\n        \n        # Convergence check\n        diff_norm = np.linalg.norm(x - x_old)\n        x_old_norm = np.linalg.norm(x_old)\n        if x_old_norm > 0 and diff_norm / x_old_norm  tol:\n            break\n            \n    return x\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3442491"}]}