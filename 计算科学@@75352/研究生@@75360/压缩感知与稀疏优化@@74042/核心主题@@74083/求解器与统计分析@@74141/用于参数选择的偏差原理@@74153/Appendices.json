{"hands_on_practices": [{"introduction": "要应用差异原则，我们首先需要理解残差范数 $\\|A x_{\\lambda} - y\\|_{2}$ 是如何随着正则化参数 $\\lambda$ 变化的。本练习将通过一个基础的计算，引导你从 LASSO 问题的最优性一阶条件出发，在一个简化的正交设定下推导出残差与 $\\lambda$ 之间的精确关系。这个过程不仅能让你掌握差异原则的具体计算方法，还能加深对 LASSO 解的结构及其与对偶变量之间联系的理解 [@problem_id:3487555]。", "problem": "考虑$\\ell_{1}$-正则化最小二乘问题（最小绝对收缩和选择算子 (LASSO)）\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，且 $\\lambda > 0$。差异原则（discrepancy principle）要求选择$\\lambda$，使得数据失配达到已知的噪声水平，即\n$$\n\\|A x_{\\lambda} - y\\|_{2} = \\delta,\n$$\n对于给定的$\\delta > 0$。假设$A$具有标准正交列，即$A^{\\top}A = I_{n}$。\n\n从凸复合最小化的基本一阶最优性条件（Karush–Kuhn–Tucker (KKT) 次梯度平稳性）和$\\ell_{1}$范数的次微分定义出发，推导残差$r_{\\lambda} := y - A x_{\\lambda}$如何通过向量$z \\in \\partial \\|x_{\\lambda}\\|_{1}$编码对偶可行性，以及这如何编码对偶凭证的稀疏模式。将您的推导特殊化到$A = I_{n}$的情况，以获得$r_{\\lambda}$、$y$和$\\lambda$之间显式的、逐坐标的关系。\n\n现在取$n = 5$，$A = I_{5}$，以及\n$$\ny = \\begin{pmatrix}4 \\\\ -2 \\\\ 1 \\\\ 0.5 \\\\ -0.25\\end{pmatrix}.\n$$\n假设噪声水平已知，并且差异原则以如下方式强制执行\n$$\n\\delta = \\frac{\\sqrt{93}}{4}.\n$$\n仅使用上述原理和您得到的特殊化的残差关系，确定由差异原则选择的精确$\\lambda > 0$值。给出您的最终答案为精确值；不要四舍五入或近似。", "solution": "该问题是有效的，因为它在科学上基于凸优化和反问题的理论，是适定的，有足够的信息得到唯一解，并使用客观、正式的语言陈述。我们开始求解。\n\nLASSO问题的目标函数是\n$$\nJ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}.\n$$\n这是一个凸函数，是一个凸二次项和凸$\\ell_1$范数之和。一个点$x_{\\lambda} \\in \\mathbb{R}^{n}$是极小值点，当且仅当零向量位于$J$在$x_{\\lambda}$处的次微分中。一阶最优性条件是：\n$$\n0 \\in \\partial J(x_{\\lambda}).\n$$\n使用次微分的求和法则，我们有：\n$$\n\\partial J(x_{\\lambda}) = \\nabla\\left(\\frac{1}{2}\\|A x_{\\lambda} - y\\|_{2}^{2}\\right) + \\partial\\left(\\lambda \\|x_{\\lambda}\\|_{1}\\right).\n$$\n最小二乘项的梯度是$A^{\\top}(A x_{\\lambda} - y)$。$\\ell_1$项的次微分是$\\lambda \\partial \\|x_{\\lambda}\\|_{1}$。因此，最优性条件变为：\n$$\n0 \\in A^{\\top}(A x_{\\lambda} - y) + \\lambda \\partial \\|x_{\\lambda}\\|_{1}.\n$$\n设残差为$r_{\\lambda} := y - A x_{\\lambda}$。该条件可以改写为：\n$$\nA^{\\top}r_{\\lambda} \\in \\lambda \\partial \\|x_{\\lambda}\\|_{1}.\n$$\n这意味着存在一个向量$z \\in \\partial \\|x_{\\lambda}\\|_{1}$，使得$A^{\\top}r_{\\lambda} = \\lambda z$。向量$z$是$\\ell_1$范数在$x_{\\lambda}$处的一个次梯度。任何这样的$z$的分量必须满足：\n$$\nz_i = \\begin{cases} \\text{sign}((x_{\\lambda})_i) & \\text{if } (x_{\\lambda})_i \\neq 0 \\\\ \\in [-1, 1] & \\text{if } (x_{\\lambda})_i = 0 \\end{cases}\n$$\n对$i=1, \\dots, n$。关系$A^{\\top}r_{\\lambda} = \\lambda z$展示了残差$r_{\\lambda}$如何编码对偶可行性。向量$\\nu = \\frac{1}{\\lambda}A^{\\top}r_{\\lambda}$可作为对偶凭证。对$\\nu=z$的最优性条件是$\\|\\nu\\|_{\\infty} \\le 1$，并且对于$x_{\\lambda}$的支撑集中的任何索引$i$（其中$(x_{\\lambda})_i \\neq 0$），我们必须有$\\nu_i = \\text{sign}((x_{\\lambda})_i)$。这意味着原始解$x_{\\lambda}$的活跃分量对应于对偶凭证$\\nu$中位于可行域$[-1, 1]$边界上的分量。这就是$x_{\\lambda}$的稀疏模式被编码的方式。\n\n现在，我们将此推导特殊化到$A = I_n$的情况。条件$A^{\\top}A=I_n$被满足。最优性条件$A^{\\top}r_{\\lambda} \\in \\lambda \\partial \\|x_{\\lambda}\\|_{1}$简化为$r_{\\lambda} \\in \\lambda \\partial \\|x_{\\lambda}\\|_{1}$，因为$A=I_n$。\n残差是$r_{\\lambda} = y - A x_{\\lambda} = y - x_{\\lambda}$。\n所以，我们有$y - x_{\\lambda} \\in \\lambda \\partial \\|x_{\\lambda}\\|_{1}$，这意味着存在$z \\in \\partial \\|x_{\\lambda}\\|_{1}$使得$y - x_{\\lambda} = \\lambda z$，或者\n$$\nx_{\\lambda} = y - \\lambda z.\n$$\n我们逐分量分析，$(x_{\\lambda})_i = y_i - \\lambda z_i$。\n1.  如果$(x_{\\lambda})_i \\neq 0$，那么$z_i = \\text{sign}((x_{\\lambda})_i)$。方程为$(x_{\\lambda})_i = y_i - \\lambda \\text{sign}((x_{\\lambda})_i)$。整理可得$y_i = (x_{\\lambda})_i + \\lambda \\text{sign}((x_{\\lambda})_i)$。由于$\\lambda > 0$，右侧的符号是$\\text{sign}((x_{\\lambda})_i)$。因此，$\\text{sign}((x_{\\lambda})_i) = \\text{sign}(y_i)$。为使此成立，我们必须有$|y_i| > \\lambda$。在这种情况下，$(x_{\\lambda})_i = y_i - \\lambda \\text{sign}(y_i)$。\n2.  如果$(x_{\\lambda})_i = 0$，那么$z_i \\in [-1, 1]$。方程变为$0 = y_i - \\lambda z_i$，这意味着$z_i = y_i / \\lambda$。条件$|z_i| \\le 1$转化为$|y_i/\\lambda| \\le 1$，即$|y_i| \\le \\lambda$。\n\n因此，解$x_{\\lambda}$由软阈值算子给出：$(x_{\\lambda})_i = \\text{sign}(y_i)\\max(|y_i|-\\lambda, 0)$。\n\n我们现在推导残差$r_{\\lambda} = y - x_{\\lambda}$的逐坐标关系。\n1.  如果$|y_i| > \\lambda$，则$(x_{\\lambda})_i = y_i - \\lambda \\text{sign}(y_i)$。残差分量为$(r_{\\lambda})_i = y_i - (y_i - \\lambda \\text{sign}(y_i)) = \\lambda \\text{sign}(y_i)$。\n2.  如果$|y_i| \\le \\lambda$，则$(x_{\\lambda})_i = 0$。残差分量为$(r_{\\lambda})_i = y_i - 0 = y_i$。\n\n这就建立了$r_{\\lambda}$、$y$和$\\lambda$之间所要求的逐坐标关系：\n$$\n(r_{\\lambda})_i = \\begin{cases} \\lambda \\text{sign}(y_i) & \\text{if } |y_i| > \\lambda \\\\ y_i & \\text{if } |y_i| \\le \\lambda \\end{cases}\n$$\n这可以紧凑地写为$(r_{\\lambda})_i = \\text{sign}(y_i) \\min(|y_i|, \\lambda)$。\n\n接下来，我们使用这个关系来找到所要求的特定$\\lambda$。给定$n=5$，$A=I_5$，\n$y = \\begin{pmatrix}4 \\\\ -2 \\\\ 1 \\\\ 0.5 \\\\ -0.25\\end{pmatrix}$，以及$\\delta = \\frac{\\sqrt{93}}{4}$。\n差异原则指出$\\|A x_{\\lambda} - y\\|_{2} = \\delta$，即$\\|r_{\\lambda}\\|_{2} = \\delta$。两边平方，我们需要解$\\|r_{\\lambda}\\|_{2}^{2} = \\delta^2 = \\frac{93}{16}$。\n残差的范数平方为：\n$$\n\\|r_{\\lambda}\\|_{2}^2 = \\sum_{i=1}^5 (r_{\\lambda})_i^2 = \\sum_{i=1}^5 (\\min(|y_i|, \\lambda))^2.\n$$\n$y$各分量的绝对值为$|y_1|=4$，$|y_2|=2$，$|y_3|=1$，$|y_4|=0.5=\\frac{1}{2}$，以及$|y_5|=0.25=\\frac{1}{4}$。\n让我们定义函数$f(\\lambda) = \\sum_{i=1}^5 (\\min(|y_i|, \\lambda))^2$。我们需要解$f(\\lambda) = \\frac{93}{16}$以求出$\\lambda > 0$。函数$f(\\lambda)$是连续的分段二次函数，其转折点在$\\lambda \\in \\{\\frac{1}{4}, \\frac{1}{2}, 1, 2, 4\\}$。\n\n- 对于$0  \\lambda \\le \\frac{1}{4}$：对所有$i$，$\\min(|y_i|, \\lambda) = \\lambda$。\n$f(\\lambda) = 5\\lambda^2$。\n$5\\lambda^2 = \\frac{93}{16} \\implies \\lambda^2 = \\frac{93}{80} > 1 \\implies \\lambda > 1$。在此区间内无解。\n\n- 对于$\\frac{1}{4}  \\lambda \\le \\frac{1}{2}$：$\\min(|y_5|, \\lambda) = \\frac{1}{4}$，且对于$i=1,2,3,4$，$\\min(|y_i|, \\lambda) = \\lambda$。\n$f(\\lambda) = 4\\lambda^2 + (\\frac{1}{4})^2 = 4\\lambda^2 + \\frac{1}{16}$。\n$4\\lambda^2 + \\frac{1}{16} = \\frac{93}{16} \\implies 4\\lambda^2 = \\frac{92}{16} = \\frac{23}{4} \\implies \\lambda^2 = \\frac{23}{16}$。\n$\\lambda = \\frac{\\sqrt{23}}{4} \\approx \\frac{4.8}{4} = 1.2$。这不在$(\\frac{1}{4}, \\frac{1}{2}]$区间内。\n\n- 对于$\\frac{1}{2}  \\lambda \\le 1$：$\\min(|y_5|, \\lambda) = \\frac{1}{4}$，$\\min(|y_4|, \\lambda) = \\frac{1}{2}$，且对于$i=1,2,3$，$\\min(|y_i|, \\lambda) = \\lambda$。\n$f(\\lambda) = 3\\lambda^2 + (\\frac{1}{2})^2 + (\\frac{1}{4})^2 = 3\\lambda^2 + \\frac{1}{4} + \\frac{1}{16} = 3\\lambda^2 + \\frac{5}{16}$。\n$3\\lambda^2 + \\frac{5}{16} = \\frac{93}{16} \\implies 3\\lambda^2 = \\frac{88}{16} = \\frac{11}{2} \\implies \\lambda^2 = \\frac{11}{6}$。\n$\\lambda = \\sqrt{\\frac{11}{6}} \\approx 1.35$。这不在$(\\frac{1}{2}, 1]$区间内。\n\n- 对于$1  \\lambda \\le 2$：$\\min(|y_5|, \\lambda) = \\frac{1}{4}$，$\\min(|y_4|, \\lambda) = \\frac{1}{2}$，$\\min(|y_3|, \\lambda) = 1$，且对于$i=1,2$，$\\min(|y_i|, \\lambda) = \\lambda$。\n$f(\\lambda) = 2\\lambda^2 + 1^2 + (\\frac{1}{2})^2 + (\\frac{1}{4})^2 = 2\\lambda^2 + 1 + \\frac{1}{4} + \\frac{1}{16} = 2\\lambda^2 + \\frac{16+4+1}{16} = 2\\lambda^2 + \\frac{21}{16}$。\n我们令其等于$\\frac{93}{16}$：\n$$\n2\\lambda^2 + \\frac{21}{16} = \\frac{93}{16}\n$$\n$$\n2\\lambda^2 = \\frac{93-21}{16} = \\frac{72}{16} = \\frac{9}{2}\n$$\n$$\n\\lambda^2 = \\frac{9}{4}\n$$\n由于$\\lambda > 0$，我们取正根：$\\lambda = \\sqrt{\\frac{9}{4}} = \\frac{3}{2} = 1.5$。\n值$\\lambda = 1.5$在区间$(1, 2]$内。因此，这是正确的解。\n\n为了验证，我们来计算$f(1.5)$:\n$f(1.5) = (\\min(4, 1.5))^2 + (\\min(2, 1.5))^2 + (\\min(1, 1.5))^2 + (\\min(0.5, 1.5))^2 + (\\min(0.25, 1.5))^2$\n$f(1.5) = (1.5)^2 + (1.5)^2 + 1^2 + (0.5)^2 + (0.25)^2$\n$f(1.5) = (\\frac{3}{2})^2 + (\\frac{3}{2})^2 + 1^2 + (\\frac{1}{2})^2 + (\\frac{1}{4})^2 = \\frac{9}{4} + \\frac{9}{4} + 1 + \\frac{1}{4} + \\frac{1}{16}$\n$f(1.5) = \\frac{18}{4} + \\frac{1}{4} + 1 + \\frac{1}{16} = \\frac{19}{4} + 1 + \\frac{1}{16} = \\frac{76}{16} + \\frac{16}{16} + \\frac{1}{16} = \\frac{93}{16}$。\n计算正确。\n由差异原则选择的$\\lambda$的精确值是$\\frac{3}{2}$。", "answer": "$$\n\\boxed{\\frac{3}{2}}\n$$", "id": "3487555"}, {"introduction": "上一个练习可能会让人觉得，满足差异原则的正则化参数 $\\lambda$ 总是唯一存在的。本练习将挑战这一直观想法，通过构建一个特殊例子，展示残差函数 $r(\\lambda)$ 并非总是严格单调递增的 [@problem_id:3487594]。这种情况会导致存在一个区间的 $\\lambda$ 值都能满足条件，从而揭示了差异原则应用中可能出现的模糊性，并引入了选择最小有效参数这一实用的“决胜法则”(tie-breaking rule)。", "problem": "考虑最小绝对收缩和选择算子 (LASSO) 的参数选择问题，对于一个感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$、一个测量向量 $y \\in \\mathbb{R}^{m}$ 和一个正则化参数 $\\lambda \\geq 0$，该问题由以下凸目标函数定义：\n$$\nx_{\\lambda} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}.\n$$\n定义残差函数 $r(\\lambda) = \\|A x_{\\lambda} - y\\|_{2}$。差异原则旨在寻找一个参数 $\\lambda$，使得 $r(\\lambda) = \\delta$，其中 $\\delta > 0$ 是一个给定的噪声水平估计。\n\n构建一个具体的反例，其中映射 $\\lambda \\mapsto r(\\lambda)$ 有一个平坦段，从而有多个 $\\lambda$ 值满足偏差方程 $r(\\lambda) = \\delta$。具体地，取 $m=3$，$n=2$，以及\n$$\nA = \\begin{pmatrix} 1  1 \\\\ 2  2 \\\\ 2  2 \\end{pmatrix}, \\qquad y = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}.\n$$\n对于此实例：\n\n1. 从凸优化（通过次梯度的最优性）和线性代数的基本原理出发，推导 $r(\\lambda)$ 作为 $\\lambda$ 函数的显式公式。\n\n2. 证明 $r(\\lambda)$ 有一个平坦段，并确定其起始点（即 $r(\\lambda)$ 开始保持不变的最小 $\\lambda$ 值）。\n\n3. 设噪声水平为测量值的欧几里得范数，即 $\\delta = \\|y\\|_{2}$。确定所有满足差异原则 $r(\\lambda) = \\delta$ 的 $\\lambda \\geq 0$。\n\n4. 当存在多个可行的 $\\lambda$ 值时，采用决胜规则，选择满足偏差方程的最小 $\\lambda$ 值，以避免过度正则化，同时满足噪声约束。根据此规则，计算选定的参数值。请将最终答案以单个实数形式给出，无需四舍五入。", "solution": "用户提供了一个有效的问题陈述。该问题在科学上基于凸优化和正则化理论，是良定的，并提供了所有必要的信息。\n\n该问题要求对一个特定问题实例中与最小绝对收缩和选择算子 (LASSO) 相关的残差函数进行分析。LASSO 解，记为 $x_{\\lambda}$，被定义为目标函数的最小化子：\n$$\nF(x) = \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\n其中 $x \\in \\mathbb{R}^{n}$，$A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，并且 $\\lambda \\ge 0$ 是正则化参数。该问题的给定数据为：\n$m=3$，$n=2$，以及\n$$\nA = \\begin{pmatrix} 1  1 \\\\ 2  2 \\\\ 2  2 \\end{pmatrix}, \\qquad y = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}\n$$\n\n**1. 残差函数 $r(\\lambda)$ 的推导**\n\n首先，我们分析矩阵-向量积 $Ax$ 的结构。令 $x = (x_1, x_2)^T$。\n$$\nAx = \\begin{pmatrix} 1  1 \\\\ 2  2 \\\\ 2  2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} x_1 + x_2 \\\\ 2x_1 + 2x_2 \\\\ 2x_1 + 2x_2 \\end{pmatrix} = (x_1 + x_2) \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}\n$$\n我们定义向量 $u = (1, 2, 2)^T$ 和标量变量 $s = x_1 + x_2$。那么 $Ax = su$。线性算子 $A$ 的值域是 $\\mathbb{R}^3$ 中由向量 $u$ 张成的一维子空间。\n\n通过代入 $Ax = su$，LASSO 目标函数可以被重写为：\n$$\nF(x) = \\frac{1}{2} \\|su - y\\|_{2}^{2} + \\lambda ( |x_1| + |x_2| )\n$$\n对 $x \\in \\mathbb{R}^2$ 的最小化可以分解为一个嵌套的最小化问题。我们首先对所有产生给定 $s = x_1+x_2$ 的 $x$ 进行最小化，然后对 $s$ 的所有可能值进行最小化：\n$$\n\\min_{x \\in \\mathbb{R}^2} F(x) = \\min_{s \\in \\mathbb{R}} \\left( \\frac{1}{2} \\|su - y\\|_{2}^{2} + \\lambda \\min_{x_1+x_2 = s} \\{|x_1| + |x_2|\\} \\right)\n$$\n内部最小化项 $\\min_{x_1+x_2 = s} \\{|x_1| + |x_2|\\}$ 可以被简化。根据三角不等式，有 $|s| = |x_1 + x_2| \\le |x_1| + |x_2|$。例如，通过选择 $x_1 = s$ 和 $x_2=0$，可以达到这个下界。因此，$\\min_{x_1+x_2 = s} \\{|x_1| + |x_2|\\} = |s|$。\n\n原来的二维问题化简为关于变量 $s$ 的一维问题：\n$$\ns_{\\lambda} = \\arg\\min_{s \\in \\mathbb{R}} \\left\\{ G(s) = \\frac{1}{2} \\|su - y\\|_{2}^{2} + \\lambda |s| \\right\\}\n$$\n为了解决这个问题，我们首先计算必要的向量积。\n$u$ 的平方范数是 $\\|u\\|_2^2 = u^T u = 1^2 + 2^2 + 2^2 = 1+4+4=9$。\n$u$ 和 $y$ 的内积是 $u^T y = (1)(1) + (2)(2) + (2)(2) = 1+4+4=9$。\n项 $\\|su-y\\|_2^2$ 可以展开为：\n$$\n\\|su-y\\|_2^2 = (su-y)^T(su-y) = s^2(u^T u) - 2s(u^T y) + y^T y = 9s^2 - 18s + \\|y\\|_2^2\n$$\n关于 $s$ 的目标函数，忽略不影响最小化子的常数项 $\\frac{1}{2}\\|y\\|_2^2$，为：\n$$\nH(s) = \\frac{9}{2}s^2 - 9s + \\lambda |s|\n$$\n我们使用次梯度最优性条件来找到最小化子 $s_{\\lambda}$，该条件表明 $0$ 必须在 $H(s)$ 于 $s=s_{\\lambda}$ 处的次微分中。次微分是：\n$$\n\\partial H(s) = 9s - 9 + \\lambda \\partial|s|\n$$\n其中，如果 $s \\neq 0$，则 $\\partial|s| = \\{\\mathrm{sgn}(s)\\}$；如果 $s = 0$，则 $\\partial|s| = [-1, 1]$。\n\n情况1：$s_{\\lambda} > 0$。条件 $0 \\in \\partial H(s_{\\lambda})$ 变为 $9s_{\\lambda} - 9 + \\lambda(1) = 0$，这得出 $9s_{\\lambda} = 9 - \\lambda$，所以 $s_{\\lambda} = 1 - \\frac{\\lambda}{9}$。这与 $s_{\\lambda} > 0$ 一致，当且仅当 $1 - \\frac{\\lambda}{9} > 0$，即 $\\lambda  9$。\n\n情况2：$s_{\\lambda}  0$。条件变为 $9s_{\\lambda} - 9 + \\lambda(-1) = 0$，这得出 $9s_{\\lambda} = 9 + \\lambda$，所以 $s_{\\lambda} = 1 + \\frac{\\lambda}{9}$。对于 $\\lambda \\ge 0$，这得出 $s_{\\lambda} > 0$，与假设 $s_{\\lambda}  0$ 矛盾。所以这种情况不可能。\n\n情况3：$s_{\\lambda} = 0$。条件是 $0 \\in 9(0) - 9 + \\lambda[-1, 1]$，化简为 $9 \\in [-\\lambda, \\lambda]$。这成立当且仅当 $\\lambda \\ge 9$。\n\n综合这些情况，最优值 $s_{\\lambda}$ 为：\n$$\ns_{\\lambda} = \\begin{cases} 1 - \\frac{\\lambda}{9}  \\text{if } 0 \\le \\lambda  9 \\\\ 0  \\text{if } \\lambda \\ge 9 \\end{cases}\n$$\n残差函数为 $r(\\lambda) = \\|A x_{\\lambda} - y\\|_{2}$。由于 $A x_{\\lambda} = s_{\\lambda}u$，我们有 $r(\\lambda) = \\|s_{\\lambda}u - y\\|_{2}$。\n我们有 $r(\\lambda)^2 = \\|s_{\\lambda}u - y\\|_{2}^2 = 9s_{\\lambda}^2 - 18s_{\\lambda} + \\|y\\|_2^2$。由于 $\\|y\\|_2^2=9$，这等于 $r(\\lambda)^2 = 9s_{\\lambda}^2 - 18s_{\\lambda} + 9 = 9(s_{\\lambda}-1)^2$。\n取平方根，我们得到 $r(\\lambda) = 3|s_{\\lambda}-1|$。\n\n现在，我们代入 $s_{\\lambda}$ 的表达式：\n- 对于 $0 \\le \\lambda  9$：$s_{\\lambda} - 1 = (1 - \\frac{\\lambda}{9}) - 1 = -\\frac{\\lambda}{9}$。\n  $r(\\lambda) = 3\\left|-\\frac{\\lambda}{9}\\right| = 3\\frac{\\lambda}{9} = \\frac{\\lambda}{3}$。\n- 对于 $\\lambda \\ge 9$：$s_{\\lambda} - 1 = 0 - 1 = -1$。\n  $r(\\lambda) = 3|-1| = 3$。\n\n因此，$r(\\lambda)$ 的显式公式为：\n$$\nr(\\lambda) = \\begin{cases} \\frac{\\lambda}{3}  \\text{if } 0 \\le \\lambda  9 \\\\ 3  \\text{if } \\lambda \\ge 9 \\end{cases}\n$$\n\n**2. 平坦段和起始点**\n\n从推导出的公式可知，对于所有 $\\lambda \\ge 9$，$r(\\lambda)$ 函数是恒定的。函数在此段上的值为 $r(\\lambda)=3$。这就是平坦段。起始点是这种恒定行为开始的最小 $\\lambda$ 值，即 $\\lambda=9$。\n\n**3. 求解偏差方程**\n\n差异原则要求选择 $\\lambda$ 使得 $r(\\lambda) = \\delta$，其中噪声水平给定为 $\\delta = \\|y\\|_2$。首先，我们计算 $\\delta$：\n$$\n\\delta = \\|y\\|_{2} = \\sqrt{1^2 + 2^2 + 2^2} = \\sqrt{1+4+4} = \\sqrt{9} = 3.\n$$\n我们需要求解方程 $r(\\lambda) = 3$。\n\n再次，我们考虑函数 $r(\\lambda)$ 的两部分：\n- 对于 $0 \\le \\lambda  9$：方程为 $\\frac{\\lambda}{3} = 3$，这意味着 $\\lambda=9$。然而，这个值不在区间 $[0, 9)$ 内，因此在此范围内无解。\n- 对于 $\\lambda \\ge 9$：方程为 $3 = 3$。这对该区间内所有的 $\\lambda$ 都成立。\n\n因此，所有满足差异原则 $r(\\lambda)=3$ 的 $\\lambda \\ge 0$ 值的集合是区间 $[9, \\infty)$。\n\n**4. 参数 $\\lambda$ 的选择**\n\n问题为存在多个 $\\lambda$ 值满足偏差方程的情况指定了一个决胜规则：选择满足条件的最小 $\\lambda$。解集是 $[9, \\infty)$。该集合中的最小值是 $9$。\n\n在此规则下，选定的参数值为 $\\lambda=9$。", "answer": "$$\\boxed{9}$$", "id": "3487594"}, {"introduction": "在理论分析和纸笔计算之外，差异原则在实际的算法应用中扮演着关键角色。许多现实世界的问题并没有 $x_\\lambda$ 的闭式解，必须通过迭代算法求解。本练习将带你进入算法实践，探索如何将差异原则与经典的迭代硬阈值（IHT）算法相结合 [@problem_id:3487541]。你将通过编程实现并比较两种不同的策略：一是将其用作固定稀疏度下的迭代停止准则，二是利用它在不同稀疏度模型之间进行选择，从而深入体会该原则在算法设计中的灵活性和实用性。", "problem": "给定一个带有加性高斯噪声的线性逆问题，其真实信号是稀疏的。设 $A \\in \\mathbb{R}^{m \\times n}$ 是一个已知的传感矩阵，$x_\\star \\in \\mathbb{R}^{n}$ 是未知的目标向量，其非零项至多有 $s$ 个，而 $y \\in \\mathbb{R}^{m}$ 是由模型 $y = A x_\\star + e$ 生成的测量向量，其中 $e \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ 且 $\\sigma > 0$ 是已知的。任务是实现并比较两种基于差异原则的参数选择策略，用于迭代硬阈值（IHT）算法。\n\n设计基本原理：\n- 数据保真度的目标是最小二乘函数 $f(x) = \\tfrac{1}{2} \\|A x - y\\|_2^2$，其梯度为 $\\nabla f(x) = A^\\top (A x - y)$。\n- 梯度 $\\nabla f$ 是利普希茨连续的，其利普希茨常数为 $L = \\|A\\|_2^2$，其中 $\\|A\\|_2$ 是谱范数（最大的奇异值）。\n- 迭代硬阈值（IHT）算法是一种投影梯度下降方法，它将梯度步投影到非凸集 $S_k = \\{x \\in \\mathbb{R}^n : \\|x\\|_0 \\le k\\}$ 上，其更新规则为 $x^{(t+1)} = \\mathcal{H}_k\\!\\left(x^{(t)} - \\mu \\nabla f\\!\\left(x^{(t)}\\right)\\right)$，步长为 $\\mu \\in (0, 2/L)$，其中 $\\mathcal{H}_k(\\cdot)$ 保留 $k$ 个（按绝对值）最大的元素，并将其余元素置零。\n- 差异原则断言，如果 $\\sigma$ 已知，当残差范数与噪声水平在乘性安全因子 $\\tau > 1$ 的范围内匹配时，即当 $\\|A x - y\\|_2 \\le \\tau \\sqrt{m}\\, \\sigma$ 时，就实现了合适的拟合。\n\n您必须实现并比较以下两种策略：\n\n1) 固定稀疏度的 IHT 与偏差停止策略：\n- 给定一个固定的稀疏度水平 $k$，从 $x^{(0)} = 0$ 开始，以步长 $\\mu = 1/L$ 运行 IHT。\n- 一旦满足差异条件 $\\|A x^{(t)} - y\\|_2 \\le \\tau \\sqrt{m}\\, \\sigma$，或达到最大迭代次数 $T_{\\max}$，立即停止。\n- 返回最终的残差范数 $\\|A \\hat{x}^{(k)} - y\\|_2$，差异条件是否达成（布尔值），所用的迭代次数，以及相对重构误差 $\\|\\hat{x}^{(k)} - x_\\star\\|_2 / \\|x_\\star\\|_2$。\n\n2) 通过收敛时的偏差进行可变稀疏度选择：\n- 对于从 $k_{\\min}$ 到 $k_{\\max}$ 的每个 $k$，以步长 $\\mu = 1/L$ 运行 IHT 直至收敛（不要使用差异原则来停止）。收敛的定义是迭代稳定，例如，在最大迭代次数 $T_{\\max}$ 内，相对变化低于一个小阈值或支撑集稳定。\n- 对每个 $k$ 收敛后，检验差异条件 $\\|A x^{(\\infty)} - y\\|_2 \\le \\tau \\sqrt{m}\\, \\sigma$。选择满足收敛时差异条件的最小 $k$。如果 $[k_{\\min}, k_{\\max}]$ 中没有一个 $k$ 满足差异条件，则报告失败。\n- 返回选择的 $k$（如果失败则为 $-1$），一个表示是否达成差异条件的布尔值，最终的残差范数，以及相对重构误差。\n\n实现要求：\n- 使用硬阈值算子 $\\mathcal{H}_k$，该算子保留绝对值最大的 $k$ 个元素，并将其余元素置零。如果 $k \\ge n$，则 $\\mathcal{H}_k$ 为恒等算子。\n- 使用步长 $\\mu = 1/L$，其中 $L = \\|A\\|_2^2$，通过谱范数 $\\|A\\|_2$ 计算。\n- 在可变 $k$ 策略中，使用形如 $\\|x^{(t+1)} - x^{(t)}\\|_2 / \\max\\{1, \\|x^{(t)}\\|_2\\} \\le \\varepsilon$ 的相对变化准则来判断收敛，其中 $\\varepsilon$ 是一个小的容差，或使用支撑集稳定条件，并设置最大迭代次数上限以确保终止。\n\n测试套件：\n您必须按规定使用独立的随机种子为每种情况生成合成数据。对于每个测试用例，构造一个矩阵 $A$，其元素独立地服从 $A_{ij} \\sim \\mathcal{N}(0, 1/m)$ 分布，生成一个 $s$-稀疏的真实信号 $x_\\star$（通过均匀随机选择 $s$ 个支撑集索引并从 $\\mathcal{N}(0, 1)$ 中抽取非零幅值），然后生成 $y = A x_\\star + e$，其中 $e \\sim \\mathcal{N}(0, \\sigma^2 I_m)$。所有计算都是无量纲的，不涉及物理单位。\n\n- 情况 1（理想情况）：\n  - $m = 64$, $n = 128$, $s = 8$, $k_{\\text{fixed}} = 8$, $\\sigma = 0.02$, $\\tau = 1.1$, seed $= 1$, $k_{\\min} = 1$, $k_{\\max} = 16$, $T_{\\max} = 1000$, $\\varepsilon = 10^{-6}$。\n\n- 情况 2（固定稀疏度不足）：\n  - $m = 64$, $n = 128$, $s = 8$, $k_{\\text{fixed}} = 4$, $\\sigma = 0.02$, $\\tau = 1.1$, seed $= 2$, $k_{\\min} = 1$, $k_{\\max} = 16$, $T_{\\max} = 1000$, $\\varepsilon = 10^{-6}$。\n\n- 情况 3（偏差条件过于严格）：\n  - $m = 64$, $n = 128$, $s = 10$, $k_{\\text{fixed}} = 10$, $\\sigma = 0.05$, $\\tau = 0.9$, seed $= 3$, $k_{\\min} = 1$, $k_{\\max} = 32$, $T_{\\max} = 1200$, $\\varepsilon = 10^{-6}$。\n\n对于每种情况，计算并报告以下输出：\n- 对于固定 $k$、偏差停止的 IHT：\n  - 最终残差范数 $\\|A \\hat{x}^{(k)} - y\\|_2$（一个实数）。\n  - 是否在 $T_{\\max}$ 前达到差异条件（一个布尔值）。\n  - 使用的迭代次数（一个整数）。\n  - 相对重构误差 $\\|\\hat{x}^{(k)} - x_\\star\\|_2 / \\|x_\\star\\|_2$（一个实数）。\n- 对于可变 $k$ 选择策略：\n  - 选择的 $k$（一个整数，如果 $[k_{\\min}, k_{\\max}]$ 中没有 $k$ 在收敛时达到差异条件，则为 $-1$）。\n  - 是否达到差异条件（一个布尔值）。\n  - 最终残差范数（一个实数）。\n  - 相对重构误差（一个实数）。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个测试用例的结果按以下顺序组成一个子列表：\n  - $\\left[\\|A \\hat{x}^{(k)} - y\\|_2,\\ \\text{disc\\_fixed},\\ \\text{iters\\_fixed},\\ \\text{relErr\\_fixed},\\ k_{\\text{sel}},\\ \\text{disc\\_var},\\ \\|A \\hat{x}^{(\\text{var})} - y\\|_2,\\ \\text{relErr\\_var}\\right]$。\n- 对于三个测试用例，最终打印的行必须是一个包含三个列表的列表，例如 $[[\\cdots],[\\cdots],[\\cdots]]$。", "solution": "该问题要求在一个稀疏信号恢复问题中，实现并比较两种在迭代硬阈值（IHT）框架内用于参数选择的不同策略。问题的核心在于应用 Morozov 差异原则。我们将首先严格地构建问题，描述 IHT 算法，然后详细说明两种参数选择方案。\n\n**问题构建**\n\n我们给定一个由以下模型描述的线性逆问题：\n$$ y = A x_\\star + e $$\n其中：\n- $y \\in \\mathbb{R}^{m}$ 是测量观测向量。\n- $A \\in \\mathbb{R}^{m \\times n}$ 是已知的传感矩阵。\n- $x_\\star \\in \\mathbb{R}^{n}$ 是未知的真实信号，假设其为 $s$-稀疏，意味着它最多有 $s$ 个非零项。这可以正式表述为 $\\|x_\\star\\|_0 \\le s$，其中 $\\|\\cdot\\|_0$ 表示 $\\ell_0$-“范数”（非零元素的数量）。\n- $e \\in \\mathbb{R}^{m}$ 是一个加性噪声向量，建模为白高斯噪声，即 $e \\sim \\mathcal{N}(0, \\sigma^2 I_m)$，其中 $I_m$ 是 $m \\times m$ 的单位矩阵，噪声标准差 $\\sigma > 0$ 是已知的。\n\n目标是根据测量值 $y$ 和矩阵 $A$ 找到 $x_\\star$ 的一个估计 $\\hat{x}$。这通常被构建为一个优化问题，旨在平衡数据保真度与稀疏性先验。一种常见的方法是求解：\n$$ \\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|A x - y\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_0 \\le k $$\n其中 $k$ 是对真实稀疏度 $s$ 的估计。项 $f(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ 是最小二乘数据保真度代价函数。\n\n**迭代硬阈值（IHT）**\n\nIHT 是一种为解决稀疏约束优化问题而设计的投影梯度下降算法。每次迭代包括两个步骤：对目标函数 $f(x)$ 进行一次标准的梯度下降，然后投影到 $k$-稀疏向量的集合上。\n\n$f(x)$ 的梯度由下式给出：\n$$ \\nabla f(x) = A^\\top (A x - y) $$\n该梯度是利普希茨连续的，其常数 $L$ 可由 $L = \\|A^\\top A\\|_2 = \\|A\\|_2^2$ 界定，其中 $\\|A\\|_2$ 是 $A$ 的谱范数。\n\nIHT 的更新规则为：\n$$ x^{(t+1)} = \\mathcal{H}_k\\left(x^{(t)} - \\mu \\nabla f\\left(x^{(t)}\\right)\\right) $$\n其中：\n- $x^{(t)}$ 是在第 $t$ 次迭代时的估计值。\n- $\\mu$ 是步长。为保证梯度步骤的收敛，我们需要 $\\mu \\in (0, 2/L)$。本问题指定使用 $\\mu = 1/L$。\n- $\\mathcal{H}_k(\\cdot)$ 是硬阈值算子。对于向量 $v \\in \\mathbb{R}^n$，$\\mathcal{H}_k(v)$ 返回一个新向量，其中只保留了 $v$ 中绝对值最大的 $k$ 个分量，所有其他分量均被置零。如果 $k \\geq n$，$\\mathcal{H}_k$ 为恒等算子。\n\n**差异原则**\n\n当噪声水平 $\\sigma$ 已知时，差异原则为选择正则化参数提供了一种规范的方法。它假定一个好的重构 $\\hat{x}$ 不应完美地拟合含噪数据 $y$，因为这会涉及到拟合噪声。相反，残差范数 $\\|A\\hat{x} - y\\|_2$ 应与噪声的期望范数在同一数量级。噪声能量的期望值为 $\\mathbb{E}[\\|e\\|_2^2] = \\sum_{i=1}^m \\mathbb{E}[e_i^2] = m\\sigma^2$，这意味着 $\\|e\\|_2 \\approx \\sqrt{m}\\sigma$。因此，差异原则为残差范数设定了一个目标：\n$$ \\|A\\hat{x} - y\\|_2 \\le \\eta $$\n其中目标残差定义为 $\\eta = \\tau \\sqrt{m}\\, \\sigma$。参数 $\\tau > 1$ 是一个安全因子，用于考虑噪声范数的统计波动。选择 $\\tau  1$ 将要求数据拟合水平比典型噪声基底更严格，这可能无法实现或导致过拟合。\n\n我们现在将详细介绍使用此原则与 IHT 相结合的两种策略。\n\n**策略1：固定稀疏度的 IHT 与差异停止策略**\n\n在此策略中，模型复杂度（稀疏度 $k$）是预先固定的。差异原则用于确定迭代算法的停止时间。\n算法流程如下：\n1. 固定稀疏度水平 $k$。\n2. 初始化解的估计值，$x^{(0)} = 0$。\n3. 预先计算步长 $\\mu = 1/\\|A\\|_2^2$ 和差异目标 $\\eta = \\tau \\sqrt{m}\\sigma$。\n4. 对 $t=0, 1, 2, \\dots, T_{\\max}-1$ 进行迭代：\n   a. 计算 IHT 更新：$x^{(t+1)} = \\mathcal{H}_k(x^{(t)} - \\mu A^\\top(Ax^{(t)} - y))$。\n   b. 计算残差范数：$r^{(t+1)} = \\|Ax^{(t+1)} - y\\|_2$。\n   c. 如果 $r^{(t+1)} \\le \\eta$，则原则被满足。算法终止，返回 $\\hat{x} = x^{(t+1)}$。\n5. 如果循环完成而未满足条件，算法在 $T_{\\max}$ 次迭代后终止，返回最终的迭代结果 $\\hat{x} = x^{(T_{\\max})}$。\n\n**策略2：通过收敛时的差异进行可变稀疏度选择**\n\n在此策略中，差异原则用于选择模型复杂度 $k$ 本身。其思想是找到能够解释数据且与噪声水平一致的最简单模型（最小的 $k$）。\n算法流程如下：\n1. 为稀疏度水平定义一个搜索范围 $[k_{\\min}, k_{\\max}]$。\n2. 预先计算 $\\mu = 1/\\|A\\|_2^2$ 和 $\\eta = \\tau \\sqrt{m}\\sigma$。\n3. 对于从 $k_{\\min}$ 到 $k_{\\max}$ 的每个 $k$：\n   a. 对当前的稀疏度 $k$ 运行 IHT 算法。此内循环*不*使用差异原则来停止。相反，它一直运行直到迭代收敛或达到最大迭代次数 $T_{\\max}$。收敛性由迭代的相对变化来衡量：$\\|x^{(t+1)} - x^{(t)}\\|_2 / \\max\\{1, \\|x^{(t)}\\|_2\\} \\le \\varepsilon$。\n   b. 令稀疏度为 $k$ 时的收敛解为 $\\hat{x}_k$。\n   c. 检查此解是否满足差异原则：$\\|A\\hat{x}_k - y\\|_2 \\le \\eta$。\n   d. 如果条件满足，则这个 $k$ 是一个有效的候选值。由于我们是从最小到最大的 $k$ 进行迭代，第一个满足条件的 $k$ 即为所选值。算法终止，返回 $k$ 和相应的解 $\\hat{x}_k$。\n4. 如果遍历 $[k_{\\min}, k_{\\max}]$ 中的所有 $k$ 后仍未找到任何满足条件的 $k$，则该过程失败。在这种情况下，我们通过返回 $k=-1$ 来报告失败。\n\n所提供的测试用例将允许在不同条件下对这两种方法进行定量比较：一个适定场景、一个固定k策略中模型复杂度被低估的场景，以及一个差异目标过于严格的场景。", "answer": "```python\nimport numpy as np\n\ndef hard_threshold(v, k):\n    \"\"\"\n    Hard-thresholding operator H_k.\n    Keeps the k largest (by magnitude) entries of v and sets others to zero.\n    \"\"\"\n    if k == 0:\n        return np.zeros_like(v)\n    if k >= v.shape[0]:\n        return v\n    \n    # Get indices of the k largest magnitude entries\n    indices = np.argsort(np.abs(v))[::-1][:k]\n    \n    v_k = np.zeros_like(v)\n    v_k[indices] = v[indices]\n    return v_k\n\ndef generate_data(m, n, s, sigma, seed):\n    \"\"\"\n    Generates synthetic data for the sparse recovery problem.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate matrix A\n    A = rng.normal(0, 1.0/np.sqrt(m), (m, n))\n    \n    # Generate sparse signal x_star\n    x_star = np.zeros(n)\n    support = rng.choice(n, s, replace=False)\n    x_star[support] = rng.normal(0, 1, s)\n    \n    # Generate noise e\n    noise = rng.normal(0, sigma, m)\n    \n    # Generate measurements y\n    y = A @ x_star + noise\n    \n    return A, y, x_star\n\ndef iht_fixed_k_discrepancy(A, y, x_star, k, sigma, tau, T_max):\n    \"\"\"\n    Implements Strategy 1: Fixed-sparsity IHT with discrepancy stopping.\n    \"\"\"\n    m, n = A.shape\n    \n    L = np.linalg.norm(A, ord=2)**2\n    mu = 1.0 / L\n    \n    discrepancy_target = tau * np.sqrt(m) * sigma\n    \n    x = np.zeros(n)\n    discrepancy_reached = False\n    \n    iters = 0\n    for t in range(T_max):\n        iters = t + 1\n        grad = A.T @ (A @ x - y)\n        x_update = x - mu * grad\n        x = hard_threshold(x_update, k)\n        \n        residual_norm = np.linalg.norm(A @ x - y)\n        if residual_norm = discrepancy_target:\n            discrepancy_reached = True\n            break\n            \n    final_residual_norm = np.linalg.norm(A @ x - y)\n    relative_error = np.linalg.norm(x - x_star) / np.linalg.norm(x_star)\n    \n    return final_residual_norm, discrepancy_reached, iters, relative_error\n\ndef iht_variable_k_selection(A, y, x_star, k_min, k_max, sigma, tau, T_max, epsilon):\n    \"\"\"\n    Implements Strategy 2: Variable-sparsity selection by discrepancy at convergence.\n    \"\"\"\n    m, n = A.shape\n\n    L = np.linalg.norm(A, ord=2)**2\n    mu = 1.0 / L\n\n    discrepancy_target = tau * np.sqrt(m) * sigma\n\n    selected_k = -1\n    discrepancy_achieved = False\n    final_x = None\n\n    for k in range(k_min, k_max + 1):\n        x = np.zeros(n)\n        for t in range(T_max):\n            x_old = x.copy()\n            grad = A.T @ (A @ x - y)\n            x_update = x - mu * grad\n            x = hard_threshold(x_update, k)\n            \n            norm_x_old = np.linalg.norm(x_old)\n            rel_change = np.linalg.norm(x - x_old) / max(1.0, norm_x_old)\n            \n            if rel_change = epsilon:\n                break\n        \n        # After convergence for sparsity k, check discrepancy\n        residual_norm_k = np.linalg.norm(A @ x - y)\n\n        if residual_norm_k = discrepancy_target:\n            selected_k = k\n            discrepancy_achieved = True\n            final_x = x\n            break \n        \n        # Store the last iterate in case of failure\n        if k == k_max:\n             final_x = x\n             \n    if final_x is None: # This case should not happen if k_max >= k_min\n        final_x = np.zeros(n)\n\n    final_residual_norm = np.linalg.norm(A @ final_x - y)\n    relative_error = np.linalg.norm(final_x - x_star) / np.linalg.norm(x_star)\n\n    return selected_k, discrepancy_achieved, final_residual_norm, relative_error\n\ndef solve():\n    \"\"\"\n    Main solver function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: \"happy path\"\n        {'m': 64, 'n': 128, 's': 8, 'k_fixed': 8, 'sigma': 0.02, 'tau': 1.1, \n         'seed': 1, 'k_min': 1, 'k_max': 16, 'T_max': 1000, 'eps': 1e-6},\n        # Case 2: \"insufficient fixed sparsity\"\n        {'m': 64, 'n': 128, 's': 8, 'k_fixed': 4, 'sigma': 0.02, 'tau': 1.1,\n         'seed': 2, 'k_min': 1, 'k_max': 16, 'T_max': 1000, 'eps': 1e-6},\n        # Case 3: \"overly strict discrepancy\"\n        {'m': 64, 'n': 128, 's': 10, 'k_fixed': 10, 'sigma': 0.05, 'tau': 0.9,\n         'seed': 3, 'k_min': 1, 'k_max': 32, 'T_max': 1200, 'eps': 1e-6}\n    ]\n\n    all_case_results_str = []\n\n    for case in test_cases:\n        A, y, x_star = generate_data(case['m'], case['n'], case['s'], case['sigma'], case['seed'])\n        \n        # Strategy 1\n        res_norm_fixed, disc_fixed, iters_fixed, rel_err_fixed = iht_fixed_k_discrepancy(\n            A, y, x_star, case['k_fixed'], case['sigma'], case['tau'], case['T_max']\n        )\n        \n        # Strategy 2\n        k_sel, disc_var, res_norm_var, rel_err_var = iht_variable_k_selection(\n            A, y, x_star, case['k_min'], case['k_max'], case['sigma'], case['tau'], case['T_max'], case['eps']\n        )\n        \n        case_str = (\n            f\"[{res_norm_fixed:.8f},\"\n            f\"{str(disc_fixed).lower()},\"\n            f\"{iters_fixed},\"\n            f\"{rel_err_fixed:.8f},\"\n            f\"{k_sel},\"\n            f\"{str(disc_var).lower()},\"\n            f\"{res_norm_var:.8f},\"\n            f\"{rel_err_var:.8f}]\"\n        )\n        all_case_results_str.append(case_str)\n        \n    final_output = f\"[{','.join(all_case_results_str)}]\"\n    print(final_output)\n\n# This is a placeholder for where the execution would happen in a real environment.\n# Since this must be a self-contained script, we can call solve() to demonstrate functionality.\n# In the final output, only the Python code block should be present as per the problem structure.\n# solve()\n```", "id": "3487541"}]}