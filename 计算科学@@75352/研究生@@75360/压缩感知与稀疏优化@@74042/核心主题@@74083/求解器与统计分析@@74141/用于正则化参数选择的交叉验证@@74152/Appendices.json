{"hands_on_practices": [{"introduction": "在稀疏优化领域，我们经常遇到两种等价的问题形式：约束形式的基追踪降噪（Basis Pursuit Denoising, BPDN）和惩罚形式的LASSO。通过从凸分析的基本原理出发，推导约束边界 $\\epsilon$ 与正则化参数 $\\lambda$ 之间的解析映射关系，您将能够深刻理解数据保真度与解的稀疏度之间的权衡。这项练习有助于您掌握在不同问题设定下灵活转换和解释参数的实用技能，从而深化对稀疏正则化核心思想的认识 [@problem_id:3441813]。", "problem": "考虑压缩感知和稀疏优化中的基追踪降噪公式，该公式通过求解凸约束问题 $\\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1}$ subject to $\\|y - A x\\|_{2} \\leq \\epsilon$ 来寻求稀疏向量，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个感知矩阵，$y \\in \\mathbb{R}^{m}$ 是一个带噪测量向量，$\\epsilon \\geq 0$ 是一个噪声容限。一种常见的替代方法是二次惩罚拉格朗日形式（也称为$\\ell_{1}$正则化最小二乘问题），它引入了一个正则化参数 $\\lambda > 0$ 并最小化目标函数 $\\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}$。在实践中，$\\lambda$ 通常通过 $K$ 折交叉验证来选择，这是一种数据驱动的过程，它将测量值进行划分，并通过最小化样本外预测误差来选择 $\\lambda$。\n\n从基本的凸分析和最优性原理（范数的凸性、欧几里得投影和 Karush–Kuhn–Tucker (KKT) 条件）出发，推导一个显式映射 $\\epsilon(\\lambda)$，该映射在以下科学上合理的结构性假设下，通过令约束问题和惩罚问题的解相等，将约束容限 $\\epsilon$ 与正则化参数 $\\lambda$ 联系起来：$A$ 的列是标准正交的，即 $A^{\\top} A = I_{n}$。将 $\\epsilon(\\lambda)$ 纯粹用 $A$ 和 $y$ 表示，不涉及任何数值算法。\n\n然后，简要解释在何种条件下，当 $\\lambda$ 变化时，约束形式和惩罚形式会产生等价解（例如，可行性、凸性、强对偶性以及权衡曲线上的单调性），以及对 $\\lambda$ 的 $K$ 折交叉验证如何通过您推导的映射导出对 $\\epsilon$ 的相应选择。您的最终答案必须是 $\\epsilon(\\lambda)$ 的单个闭式解析表达式，不需要单位。如果您引入任何近似，请确保对其进行严格证明；最终表达式不需要四舍五入。", "solution": "该问题陈述被评估为有效，因为它在科学上基于凸优化和稀疏信号处理的原理，是适定的，并以客观、正式的语言表述。矩阵 $A$ 的列是标准正交的这一简化假设是推导解析性见解的标准技术，并不会使问题无效。因此，我们可以进行完整求解。\n\n目标是找到一个映射 $\\epsilon(\\lambda)$，以确保在 $A$ 的列是标准正交的（即 $A^{\\top} A = I_{n}$，其中 $I_n$ 是 $n \\times n$ 单位矩阵）这一结构性假设下，约束基追踪降噪 (BPDN) 问题，\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|y - A x\\|_{2} \\leq \\epsilon $$\n的解与惩罚拉格朗日 (LASSO) 问题，\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\left\\{ F(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\} $$\n的解相同。\n\n首先，我们求解 LASSO 问题。目标函数 $F(x)$ 是凸的，因为它是两个凸函数之和。一个向量 $x^*_{\\lambda}$ 是最小化子，当且仅当零向量位于 $F$ 在 $x^*_{\\lambda}$ 处的次微分中，即条件 $0 \\in \\partial F(x^*_{\\lambda})$。\n次微分由 $\\partial F(x) = \\nabla \\left( \\frac{1}{2}\\|y - Ax\\|_{2}^{2} \\right) + \\lambda \\partial \\|x\\|_{1}$ 给出。\n可微二次项的梯度为：\n$$ \\nabla \\left( \\frac{1}{2}\\|y - A x\\|_{2}^{2} \\right) = A^{\\top}(Ax - y) $$\n$\\ell_1$范数 $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$ 的次微分是向量 $g \\in \\mathbb{R}^n$ 的集合，其中如果 $x_i \\neq 0$，则 $g_i = \\text{sign}(x_i)$；如果 $x_i = 0$，则 $g_i \\in [-1, 1]$。\n\n因此，LASSO 解 $x^*_{\\lambda}$ 的最优性条件是：\n$$ 0 \\in A^{\\top}(Ax^*_{\\lambda} - y) + \\lambda \\partial \\|x^*_{\\lambda}\\|_{1} $$\n$$ A^{\\top}(y - Ax^*_{\\lambda}) \\in \\lambda \\partial \\|x^*_{\\lambda}\\|_{1} $$\n使用给定的假设 $A^{\\top} A = I_n$，该条件简化为：\n$$ A^{\\top}y - x^*_{\\lambda} \\in \\lambda \\partial \\|x^*_{\\lambda}\\|_{1} $$\n这意味着存在一个次梯度向量 $g \\in \\partial \\|x^*_{\\lambda}\\|_{1}$，使得 $A^{\\top}y - x^*_{\\lambda} = \\lambda g$。我们可以对每个 $i \\in \\{1, \\dots, n\\}$ 逐分量分析：\n$$(x^*_{\\lambda})_i = (A^{\\top}y)_i - \\lambda g_i$$\n1.  如果 $(x^*_{\\lambda})_i > 0$，则 $g_i = 1$，这意味着 $(x^*_{\\lambda})_i = (A^{\\top}y)_i - \\lambda$。为保持一致性，必须有 $(A^{\\top}y)_i - \\lambda > 0$，即 $(A^{\\top}y)_i > \\lambda$。\n2.  如果 $(x^*_{\\lambda})_i  0$，则 $g_i = -1$，这意味着 $(x^*_{\\lambda})_i = (A^{\\top}y)_i + \\lambda$。一致性要求 $(A^{\\top}y)_i + \\lambda  0$，即 $(A^{\\top}y)_i  -\\lambda$。\n3.  如果 $(x^*_{\\lambda})_i = 0$，则 $g_i \\in [-1, 1]$。这意味着 $0 = (A^{\\top}y)_i - \\lambda g_i$，所以 $g_i = (A^{\\top}y)_i / \\lambda$。一致性要求 $|(A^{\\top}y)_i / \\lambda| \\leq 1$，即 $|(A^{\\top}y)_i| \\leq \\lambda$。\n\n综合这些情况，得到 $x^*_{\\lambda}$ 每个分量的解：\n$$ (x^*_{\\lambda})_i = \\begin{cases} (A^{\\top}y)_i - \\lambda  \\text{if } (A^{\\top}y)_i  \\lambda \\\\ 0  \\text{if } |(A^{\\top}y)_i| \\leq \\lambda \\\\ (A^{\\top}y)_i + \\lambda  \\text{if } (A^{\\top}y)_i  -\\lambda \\end{cases} $$\n这就是软阈值算子，记为 $S_{\\lambda}(\\cdot)$。解为 $x^*_{\\lambda} = S_{\\lambda}(A^{\\top}y)$，其中该算子是逐分量作用的。\n\n为了使 BPDN 和 LASSO 问题的解等价，LASSO 解 $x^*_{\\lambda}$ 必须满足 BPDN 约束且等号成立，即 $\\|y - A x^*_{\\lambda}\\|_{2} = \\epsilon$。这就定义了映射 $\\epsilon(\\lambda)$。我们现在计算这个残差范数。\n令 $P = AA^{\\top}$ 为到 $A$ 的列空间 $\\mathcal{R}(A)$ 上的正交投影矩阵。我们可以将向量 $y$ 分解为两个正交分量：$y = Py + (I - P)y$，其中 $Py \\in \\mathcal{R}(A)$ 且 $(I-P)y \\in \\mathcal{R}(A)^{\\perp}$。项 $Ax^*_{\\lambda}$ 也在 $\\mathcal{R}(A)$ 中。\n残差向量为 $r = y - Ax^*_{\\lambda} = (Py - Ax^*_{\\lambda}) + (I-P)y$。由于这两个分量是正交的，根据勾股定理，其范数的平方由下式给出：\n$$ \\|y - A x^*_{\\lambda}\\|_{2}^{2} = \\|Py - Ax^*_{\\lambda}\\|_{2}^{2} + \\|(I-P)y\\|_{2}^{2} $$\n代入 $P=AA^{\\top}$，第一项变为：\n$$ \\|AA^{\\top}y - A x^*_{\\lambda}\\|_{2}^{2} = \\|A(A^{\\top}y - x^*_{\\lambda})\\|_{2}^{2} $$\n利用性质 $A^{\\top} A = I_n$，这简化为：\n$$ (A^{\\top}y - x^*_{\\lambda})^{\\top} A^{\\top} A (A^{\\top}y - x^*_{\\lambda}) = \\|A^{\\top}y - x^*_{\\lambda}\\|_{2}^{2} $$\n所以我们有：\n$$ \\epsilon(\\lambda)^2 = \\|y - A x^*_{\\lambda}\\|_{2}^{2} = \\|A^{\\top}y - x^*_{\\lambda}\\|_{2}^{2} + \\|(I - AA^{\\top})y\\|_{2}^{2} $$\n令 $z = A^{\\top}y$。则 $x^*_{\\lambda} = S_{\\lambda}(z)$。我们需要计算 $\\|z - S_{\\lambda}(z)\\|_{2}^{2}$。我们来考察其第 $i$ 个分量：\n- 如果 $|z_i| \\leq \\lambda$，则 $(S_{\\lambda}(z))_i = 0$，所以该分量为 $z_i$。其平方值为 $z_i^2 = \\min(|z_i|, \\lambda)^2$。\n- 如果 $z_i  \\lambda$，则 $(S_{\\lambda}(z))_i = z_i - \\lambda$，所以该分量为 $z_i - (z_i - \\lambda) = \\lambda$。其平方值为 $\\lambda^2 = \\min(|z_i|, \\lambda)^2$。\n- 如果 $z_i  -\\lambda$，则 $(S_{\\lambda}(z))_i = z_i + \\lambda$，所以该分量为 $z_i - (z_i + \\lambda) = -\\lambda$。其平方值为 $(-\\lambda)^2 = \\lambda^2 = \\min(|z_i|, \\lambda)^2$。\n在所有情况下，$(z - S_{\\lambda}(z))$ 的第 $i$ 个分量的平方都是 $\\min(|z_i|, \\lambda)^2$。\n因此，范数的平方是这些分量的总和：\n$$ \\|A^{\\top}y - x^*_{\\lambda}\\|_{2}^{2} = \\sum_{i=1}^{n} \\min\\left(|(A^{\\top}y)_i|, \\lambda\\right)^{2} $$\n将此代回 $\\epsilon(\\lambda)^2$ 的表达式中，我们得到最终的映射：\n$$ \\epsilon(\\lambda)^2 = \\|(I - AA^{\\top})y\\|_{2}^{2} + \\sum_{i=1}^{n} \\min\\left(|(A^{\\top}y)_i|, \\lambda\\right)^{2} $$\n取平方根得到 $\\epsilon(\\lambda)$。\n\n约束 (BPDN) 和惩罚 (LASSO) 形式之间的等价性是凸优化中的一个一般原理。对于任何 $\\lambda  0$，LASSO 问题的解 $x^*_{\\lambda}$ 也解一个对应的 BPDN 问题，其中 $\\epsilon = \\|y - Ax^*_{\\lambda}\\|_2$。反之，对于某个范围（从 $0$ 到 $\\|y\\|_2$）内的任何 $\\epsilon$，BPDN 问题的解 $x^*_{\\epsilon}$ 也解一个对应的 LASSO 问题，其中 $\\lambda \\ge 0$。这种对应关系源于约束优化的 Karush-Kuhn-Tucker (KKT) 条件。解的存在性和唯一性由平方 $\\ell_2$ 范数的严格凸性和 $\\ell_1$ 范数的凸性保证。强对偶性成立（例如，在 Slater 条件下），确保 KKT 条件是取得最优性的充分条件，并且最优原始值和对偶值相等，这巩固了拉格朗日乘子 $\\lambda$ 和约束界 $\\epsilon$ 之间的联系。映射 $\\epsilon(\\lambda)$ 是连续且单调递增的：更强的惩罚 $\\lambda$ 会导致更稀疏的解，这通常对数据的拟合程度较差，从而导致更大的残差范数 $\\epsilon$。\n\n$K$ 折交叉验证 (CV) 是一种用于选择正则化参数 $\\lambda$ 的数据驱动方法。它的工作原理是将 $m$ 个测量值（$A$ 的行和 $y$ 的相应条目）的集合划分为 $K$ 个不相交的子集。对于每一折，在 $K-1$ 个子集上训练一个 LASSO 模型，并在留出的子集上评估其预测误差。对一系列 $\\lambda$ 值重复此过程，并选择使所有折的平均预测误差最小化的那个值，记为 $\\lambda^*$。我们推导出的映射提供了从这种数据驱动的 $\\lambda$ 选择到 BPDN 公式的直接桥梁。一旦 CV 得出最优的 $\\lambda^*$，就可以使用推导出的 $\\epsilon(\\lambda^*)$ 表达式计算相应的噪声容限 $\\epsilon^*$。这使得人们可以用一个经过严格选择、并有望提供良好样本外预测性能的参数 $\\epsilon^* = \\epsilon(\\lambda^*)$ 来解决（通常更直观的）BPDN 问题。", "answer": "$$\\boxed{\\sqrt{\\|(I - A A^{\\top}) y\\|_{2}^{2} + \\sum_{i=1}^{n} \\min(|(A^{\\top} y)_i|, \\lambda)^{2}}}$$", "id": "3441813"}, {"introduction": "K折交叉验证是机器学习中选择超参数的黄金标准，但在压缩感知领域，其应用需格外谨慎，尤其是在样本数量接近理论极限时。压缩感知理论为稳定信号恢复提供了所需的最小样本量 $m_{\\min}$ 的精确界限。本练习将引导您分析交叉验证的数据划分机制如何与该理论下界相互作用，揭示为何看似标准的验证流程可能导致训练子问题失效 [@problem_id:3441878]。掌握这一关键点对于在样本稀缺的高维场景下设计可靠的验证策略至关重要。", "problem": "考虑压缩感知中的线性逆模型，其中观测值 $y \\in \\mathbb{R}^m$ 通过 $y = A x_{\\star} + w$ 得到。这里，$A \\in \\mathbb{R}^{m \\times n}$ 的条目是均值为 $0$、方差为 $1/m$ 的独立同分布高斯随机变量，$x_{\\star} \\in \\mathbb{R}^n$ 是一个未知的 $s$-稀疏向量，$w \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ 是加性噪声。一个标准的估计器是最小绝对值收缩和选择算子 (LASSO)，对于给定的正则化水平 $\\lambda  0$，它在一个测量值的训练子集 $(A_{\\text{tr}}, y_{\\text{tr}})$ 上求解 $\\min_{x \\in \\mathbb{R}^n} \\tfrac{1}{2} \\| A_{\\text{tr}} x - y_{\\text{tr}} \\|_2^2 + \\lambda \\| x \\|_1$，并在一个不相交的验证子集 $(A_{\\text{val}}, y_{\\text{val}})$ 上进行评分；正则化水平 $\\lambda$ 通过交叉验证 (CV) 来选择。众所周知并被广泛使用的是，对于高斯设计，稳定恢复一个 $s$-稀疏向量所需的最小样本复杂度 $m_{\\min}$ (在绝对常数和对数因子内) 约为 $m_{\\min} \\asymp C \\, s \\log(n/s)$，其中 $C  0$ 是某个绝对常数。此外，通过选择 $A$ 的行构成的子矩阵保持相同的分布形式。在全文中，假设 $\\log$ 表示自然对数。\n\n假设 $n$、$s$ 和 $m$ 的取值使得 $m$ 仅略大于 $m_{\\min}$。为具体起见，取 $n = 10^5$，$s = 100$，$C = 2$，并假设 $m = 1500$。你考虑使用 $K$-折交叉验证，在每一折中，它将 $A$ 的一个行子集（以及 $y$ 的相应条目）作为验证集保留，并使用剩余的行进行训练。\n\n下列哪项陈述是正确的？\n\nA. 当 $m$ 仅略高于最小样本复杂度时，使用小 $K$ 值（例如 $K=5$）的等大小 $K$-折交叉验证，可能会使每个训练子问题相对于信息论阈值而言是欠采样的，从而使恢复不稳定或不可能；一个直接的缓解方法是选择足够大的 $K$，以确保每次训练划分都使用至少达到稳定恢复所需的最小测量数。\n\nB. 因为验证样本不用于训练，所以交叉验证不会改变训练问题的样本复杂度；因此，如果完整问题高于阈值，那么 $K$-折交叉验证中的所有训练子问题也都会高于阈值，无论 $K$ 为何值。\n\nC. 一种为保持最小样本复杂度而进行的合适分层是：保留一个随机的列（特征）子集用于验证，同时保留所有 $m$ 个测量值用于训练；这可以使训练子问题保持在阈值以上，而不改变感知行的分布。\n\nD. 一种为保持最小样本复杂度而进行的分层划分是：构建的折，其验证集大小最多为 $m - m_{\\min}$（这样每次训练划分都至少使用 $m_{\\min}$ 个测量值），并将验证行分布到各个折中，以使每行最多被用于验证一次；当 $m$ 接近 $m_{\\min}$ 时，这通常需要使用很多折。\n\nE. 在信息论阈值附近，二折交叉验证是最佳的，因为它最大化了验证集的大小，从而减少了验证误差的方差；由于有限等距性质 (RIP)，训练问题保持稳定，因此在这种情况下应首选较小的 $K$。\n\n选择所有适用项。", "solution": "这个问题的核心在于，要通过像 LASSO 这样的方法稳定地恢复一个 $s$-稀疏向量，测量数量必须达到或超过一个最小阈值 $m_{\\min}$。问题陈述了这个阈值为 $m_{\\min} \\asymp C s \\log(n/s)$。给定参数 $n = 10^5$，$s = 100$，$C = 2$，最小样本数近似为：\n$$m_{\\min} = 2 \\cdot 100 \\cdot \\log\\left(\\frac{10^5}{100}\\right) = 200 \\cdot \\log(1000) \\approx 200 \\cdot 6.9078 = 1381.55$$\n可用的总测量数为 $m=1500$。由于 $m  m_{\\min}$，完整问题是可解的。\n\n在 $K$-折交叉验证中， $m$ 个测量值（行）被划分为 $K$ 个不相交的折，每个折的大小为 $m/K$。在每次运行中，一个折用于验证，另外 $K-1$ 个折用于训练。\n每个训练子问题的测量数量为：\n$$m_{\\text{tr}} = m \\cdot \\frac{K-1}{K} = m \\left(1 - \\frac{1}{K}\\right)$$\n为了使 LASSO 估计器在每次训练运行中都有效，训练测量数 $m_{\\text{tr}}$ 本身必须足以实现稳定恢复。也就是说，我们必须有 $m_{\\text{tr}} \\ge m_{\\min}$。代入数值：\n$$1500 \\left(1 - \\frac{1}{K}\\right) \\ge 1381.55$$\n$$1 - \\frac{1}{K} \\ge \\frac{1381.55}{1500} \\approx 0.921$$\n$$\\frac{1}{K} \\le 1 - 0.921 = 0.079$$\n$$K \\ge \\frac{1}{0.079} \\approx 12.66$$\n因此，为确保每个训练子问题都是适定的，我们必须使用 $K \\ge 13$。这意味着需要大量的折，这个过程接近于留一法交叉验证。\n\n现在，我们基于此分析评估每个选项。\n\n**A. 当 $m$ 仅略高于最小样本复杂度时，使用小 $K$ 值（例如 $K = 5$）的等大小 $K$-折交叉验证，可能会使每个训练子问题相对于信息论阈值而言是欠采样的，从而使恢复不稳定或不可能；一个直接的缓解方法是选择足够大的 $K$，以确保每次训练划分都使用至少达到稳定恢复所需的最小测量数。**\n我们用 $K=5$ 的例子来测试。训练样本数将是 $m_{\\text{tr}} = 1500 \\left(1 - \\frac{1}{5}\\right) = 1500 \\cdot \\frac{4}{5} = 1200$。由于 $1200  m_{\\min} \\approx 1382$，训练子问题确实是欠采样的。这意味着对于任何 $\\lambda$ 值，在这些训练集上的 LASSO 解都将是不稳定的，并且不能可靠地恢复真实的稀疏信号。提出的缓解措施是选择一个足够大的 $K$ 使得 $m_{\\text{tr}} \\ge m_{\\min}$，我们的推导表明 $K \\ge 13$。这个陈述与我们的分析完全一致。\n**结论：正确。**\n\n**B. 因为验证样本不用于训练，所以交叉验证不会改变训练问题的样本复杂度；因此，如果完整问题高于阈值，那么 $K$-折交叉验证中的所有训练子问题也都会高于阈值，无论 $K$ 为何值。**\n这个陈述包含一个根本性的误解。“样本复杂度” $m_{\\min}$ 是问题规模（$n, s$）的内在属性，不会改变。然而，交叉验证明确地*减少*了可用于训练的样本数量（$m_{\\text{tr}} = m(1-1/K)  m$）。问题的核心恰恰在于，虽然完整问题高于阈值（$m  m_{\\min}$），但训练子问题可能不高于阈值（$m_{\\text{tr}}$ 可能小于 $m_{\\min}$）。如 $K=5$ 的例子所示，这个陈述是错误的。\n**结论：不正确。**\n\n**C. 一种为保持最小样本复杂度而进行的合适分层是：保留一个随机的列（特征）子集用于验证，同时保留所有 $m$ 个测量值用于训练；这可以使训练子问题保持在阈值以上，而不改变感知行的分布。**\n这提出了一个不同的 CV 策略：划分 $A$ 的列（特征），而不是行（测量）。在稀疏恢复的背景下，目标是识别向量 $x_{\\star} \\in \\mathbb{R}^n$ 的非零条目。保留列意味着训练过程将无法估计与这些列对应的系数。这不是一个调整正则化参数 $\\lambda$ 的有效方法，$\\lambda$ 的目的是在整个特征空间上控制稀疏性和预测误差。执行此问题 CV 的标准和正确方法是模拟对新测量的泛化能力，这需要划分行。\n**结论：不正确。**\n\n**D. 一种为保持最小样本复杂度而进行的分层划分是：构建的折，其验证集大小最多为 $m - m_{\\min}$（这样每次训练划分都至少使用 $m_{\\min}$ 个测量值），并将验证行分布到各个折中，以使每行最多被用于验证一次；当 $m$ 接近 $m_{\\min}$ 时，这通常需要使用很多折。**\n这个陈述形式化了一个有效的 CV 划分条件。训练集的大小为 $m_{\\text{tr}} = m - m_{\\text{val}}$，其中 $m_{\\text{val}}$ 是验证集的大小。稳定恢复的条件是 $m_{\\text{tr}} \\ge m_{\\min}$，这等价于 $m - m_{\\text{val}} \\ge m_{\\min}$，或 $m_{\\text{val}} \\le m - m_{\\min}$。陈述的第一部分是正确的。第二部分指出，如果 $m$ 接近 $m_{\\min}$（即 $m - m_{\\min}$很小），则需要很多折。如果我们使用标准的 $K$-折 CV 方案，其中所有 $m$ 行在 $K$ 个折中都恰好被用作验证一次，那么验证实例的总数是 $m$。如果每个验证折的大小为 $m_{\\text{val}}$，那么 $K \\cdot m_{\\text{val}} \\approx m$。将其与约束条件 $m_{\\text{val}} \\le m - m_{\\min}$ 结合，我们得到 $K \\ge m / m_{\\text{val}} \\ge m / (m - m_{\\min})$。当 $m$ 接近 $m_{\\min}$ 时，分母 $m - m_{\\min}$ 是一个小数字，使得所需的 $K$ 很大。我们的计算得出 $m/(m-m_{\\min}) \\approx 1500 / (1500-1381.55) \\approx 12.66$，证实了需要很多折。\n**结论：正确。**\n\n**E. 在信息论阈值附近，二折交叉验证是最佳的，因为它最大化了验证集的大小，从而减少了验证误差的方差；由于有限等距性质 (RIP)，训练问题保持稳定，因此在这种情况下应首选较小的 $K$。**\n我们来分析 $K=2$ 的情况。训练集大小变为 $m_{\\text{tr}} = m/2 = 1500/2 = 750$。这远低于所需的最小值 $m_{\\min} \\approx 1382$。因此，训练子问题被严重欠采样，将不会稳定。有限等距性质 (RIP) 正是在测量数量相对于给定稀疏水平不足时会失效的性质。声称最大化验证集大小是最佳的，在这种情况下也是错误的；它会在训练方面产生严重有偏（实际上是无用的）模型。\n**结论：不正确。**", "answer": "$$\\boxed{AD}$$", "id": "3441878"}, {"introduction": "在确定交叉验证误差曲线后，如何高效地找到其最小值点是下一个挑战。传统的网格搜索不仅计算成本高昂，还可能错过网格点之间的最优解。通过对交叉验证误差曲线的局部形状进行建模，我们可以设计出更智能的搜索策略。本练习将介绍一种基于曲率的自适应搜索算法，它利用误差曲线的凸性假设来迭代地加密搜索网格，从而更精确、更快速地定位最优的正则化参数 $\\lambda$ [@problem_id:3441826]。掌握这种超越蛮力搜索的高级优化技巧，对于处理大规模模型的参数选择问题至关重要。", "problem": "考虑压缩感知线性模型 $y = A x_{0} + w$，其中 $A \\in \\mathbb{R}^{n \\times p}$ 的列在 $\\ell_{2}$-范数下归一化，$x_{0} \\in \\mathbb{R}^{p}$ 是 $s$-稀疏的且 $s \\ll p$，$w \\in \\mathbb{R}^{n}$ 是零均值亚高斯噪声，其方差代理为 $\\sigma^{2}$。对于每个正则化水平 $\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}] \\subset (0, \\infty)$，定义最小绝对收缩和选择算子 (LASSO) 估计量 $ \\hat{x}_{\\lambda} \\in \\arg\\min_{x \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2} \\| y - A x \\|_{2}^{2} + \\lambda \\| x \\|_{1} \\right\\}$。令预测风险为 $R(\\lambda) = \\mathbb{E}\\left[ \\| y' - A' \\hat{x}_{\\lambda}^{(\\mathrm{train})} \\|_{2}^{2} \\right]$，其中 $(A', y')$ 是从与 $(A, y)$ 相同分布中抽取的独立测试对，而 $\\hat{x}_{\\lambda}^{(\\mathrm{train})}$ 是在独立的训练样本上训练得到的。令风险的 $K$-折交叉验证 (CV) 估计为 $E_{n}(\\lambda)$，它是在大小为 $n$ 的训练样本上，对 $K$ 个折的留出残差范数取平均值。\n\n假设以下经过充分检验的事实为基本依据：\n- 对于固定的 $[\\lambda_{\\min}, \\lambda_{\\max}]$，在亚高斯设计和保证 $\\hat{x}_{\\lambda}$ 相对于 $\\lambda$ 稳定性的正则性条件下，风险 $R(\\lambda)$ 在 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 上是严格凸且二次连续可微的，并有唯一的最小化子 $\\lambda^{\\star} \\in (\\lambda_{\\min}, \\lambda_{\\max})$ 满足 $R'(\\lambda^{\\star}) = 0$ 和 $R''(\\lambda^{\\star})  0$。\n- $K$-折 CV 风险估计 $E_{n}(\\lambda)$ 是 $R(\\lambda)$ 在 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 上的一个一致收敛估计量，即当 $n \\to \\infty$ 且 $K$ 固定时，$\\sup_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} | E_{n}(\\lambda) - R(\\lambda) | \\xrightarrow{p} 0$。\n\n你需要选择一个选项，该选项指定了一种基于 CV 误差曲线曲率的 $\\lambda$ 值网格的自适应细化方案，并且该方案同时满足：\n- 使用 $E_{n}(\\lambda)$ 的曲率信息来插入候选点，\n- 并且在所述事实下，对于连续的 $\\lambda$ 值，随着迭代的进行和 $n \\to \\infty$，建立了所选 $\\lambda$ 向 $\\lambda^{\\star}$ 的（依概率）收敛性。\n\n哪个选项正确地提出了这样一种方案并给出了有效的收敛性论证？\n\nA. 使用一个粗糙网格 $\\Lambda^{(0)} = \\{ \\lambda_{0} = \\lambda_{\\min}  \\lambda_{1}  \\cdots  \\lambda_{m} = \\lambda_{\\max} \\}$ 进行初始化，其最大网格宽度至多为一个固定的 $h_{0}  0$。在迭代 $t \\in \\{ 0, 1, \\ldots \\}$ 时，在 $\\Lambda^{(t)}$ 上评估 $E_{n}(\\lambda)$，并令 $i^{\\star}$ 为离散最小化子的一个索引，即对于所有 $\\lambda \\in \\Lambda^{(t)}$ 都有 $E_{n}(\\lambda_{i^{\\star}}) \\leq E_{n}(\\lambda)$。如果 $i^{\\star} \\in \\{ 1, \\ldots, m^{(t)} - 1 \\}$，则形成区间 $[\\lambda_{i^{\\star}-1}, \\lambda_{i^{\\star}+1}]$，并通过三元组 $\\left( \\lambda_{i^{\\star}-1}, E_{n}(\\lambda_{i^{\\star}-1}) \\right)$、$\\left( \\lambda_{i^{\\star}}, E_{n}(\\lambda_{i^{\\star}}) \\right)$ 和 $\\left( \\lambda_{i^{\\star}+1}, E_{n}(\\lambda_{i^{\\star}+1}) \\right)$ 拟合唯一的二次插值 $q_{t}$。令 $\\tilde{\\lambda}_{t} = \\arg\\min_{\\lambda \\in \\mathbb{R}} q_{t}(\\lambda)$，并将 $\\lambda^{\\mathrm{new}}_{t} = \\min\\{\\max\\{\\tilde{\\lambda}_{t}, \\lambda_{i^{\\star}-1}\\}, \\lambda_{i^{\\star}+1}\\}$ 插入网格中：$\\Lambda^{(t+1)} = \\Lambda^{(t)} \\cup \\{ \\lambda^{\\mathrm{new}}_{t} \\}$。如果 $i^{\\star} \\in \\{ 0, m^{(t)} \\}$，则仅通过对相邻区间进行二分来细化边界。当区间宽度 $\\lambda_{i^{\\star}+1} - \\lambda_{i^{\\star}-1} \\leq \\delta_{t}$ 时停止，其中 $\\delta_{t} \\downarrow 0$。选择 $\\hat{\\lambda}_{t}$ 作为 $\\Lambda^{(t)}$ 上的离散最小化子。在严格凸性和一致收敛性下，该区间最终以高概率包含 $\\lambda^{\\star}$，并且截断的二次最小化子在 $\\lambda^{\\star}$ 附近产生区间的几何收缩；因此当 $t \\to \\infty$ 和 $n \\to \\infty$ 时，$\\hat{\\lambda}_{t} \\xrightarrow{p} \\lambda^{\\star}$。\n\nB. 使用一个粗糙网格 $\\Lambda^{(0)}$ 进行初始化，并在每次迭代中，通过对每个连续的三元组使用二阶差商来计算离散曲率。仅在曲率幅度最小的区间中插入新点，从而加密较平坦的区域以避免过拟合。持续此过程直到最大网格宽度低于某个容差。然后，所选的 $\\hat{\\lambda}_{t}$ 收敛到 $\\lambda^{\\star}$，而无需任何凸性或一致性假设，因为细化平坦区域可以控制估计方差。\n\nC. 使用一个粗糙网格进行初始化，并在每次迭代中，通过对一个三次多项式进行全局最小二乘拟合来近似整个区间上的导数 $E_{n}'(\\lambda)$。对于某个步长 $\\alpha_{t} \\in (0, 1]$ 执行割线更新 $\\tilde{\\lambda}_{t+1} = \\tilde{\\lambda}_{t} - \\alpha_{t} \\frac{E_{n}'(\\tilde{\\lambda}_{t})}{\\Delta E_{n}'}$，其中 $\\Delta E_{n}'$ 是端点之间的导数差。将 $\\tilde{\\lambda}_{t+1}$ 插入网格并重复。收敛性成立，因为多项式导数可以减少噪声，并且割线更新在一般非凸性下也能收敛。\n\nD. 使用均匀二分细化：重复地将网格中的每个区间减半，以获得一个网格宽度趋于 0 的嵌套网格序列。在每个阶段，选择当前网格上使 $E_{n}(\\lambda)$ 最小的 $\\lambda$。这保证了 $\\hat{\\lambda}_{t}$ 收敛到连续统上的精确最小化子，而不管 $E_{n}(\\lambda)$ 中的随机波动如何，因为密集采样消除了对凸性或一致性的任何需求。\n\nE. 从一个粗糙网格开始，标记所有经验曲率（通过三点有限差分计算）超过固定阈值 $\\tau  0$ 的区间，并将中点并行插入所有标记的区间。重复此过程，直到没有区间的曲率超过该阈值。选择达到最小 $E_{n}(\\lambda)$ 的最小的 $\\lambda$。收敛性得到保证，因为高曲率表示接近最小化子，并且阈值化可以在没有任何区间限定的情况下过滤噪声。\n\n选择正确的选项。", "solution": "我们从所述的基本依据出发。预测风险 $R(\\lambda)$ 在 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 上是严格凸且二次连续可微的，并有唯一的最小化子 $\\lambda^{\\star}$ 满足 $R'(\\lambda^{\\star}) = 0$ 和 $R''(\\lambda^{\\star})  0$。$K$-折交叉验证 (CV) 估计量 $E_{n}(\\lambda)$ 是一致收敛的，即当 $n \\to \\infty$ 时，$\\sup_{\\lambda} |E_{n}(\\lambda) - R(\\lambda)| \\xrightarrow{p} 0$。\n\n具有收敛性的曲率自适应细化的原理推导：\n- 严格凸性和光滑性意味着 $R(\\lambda)$ 的单峰性和唯一的最小化子 $\\lambda^{\\star}$。对于任何 $\\varepsilon  0$，根据连续性和严格凸性，存在 $c_{\\varepsilon}  0$ 使得对于所有 $\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}] \\setminus (\\lambda^{\\star} - \\varepsilon, \\lambda^{\\star} + \\varepsilon)$ 都有 $R(\\lambda) \\geq R(\\lambda^{\\star}) + c_{\\varepsilon}$。一致收敛性得出，对于足够大的 $n$，以任意接近 1 的概率，$E_{n}(\\lambda)$ 继承了一个近似单峰的结构：$E_{n}(\\lambda)$ 在 $c_{\\varepsilon}/2$ 的范围内一致地接近 $R(\\lambda)$。因此，$E_{n}$ 在任何网格宽度小于 $\\varepsilon$ 的网格上的离散最小化子以高概率位于 $(\\lambda^{\\star} - \\varepsilon, \\lambda^{\\star} + \\varepsilon)$ 内。\n- 在当前离散最小化子周围的一个区间内进行基于曲率的局部二次插值，提供了一个近似的牛顿步。具体来说，对于靠近 $\\lambda^{\\star}$ 的三个点 $(\\lambda_{i^{\\star}-1}, E_{n}(\\lambda_{i^{\\star}-1}))$、$(\\lambda_{i^{\\star}}, E_{n}(\\lambda_{i^{\\star}}))$、$(\\lambda_{i^{\\star}+1}, E_{n}(\\lambda_{i^{\\star}+1}))$，唯一的二次插值 $q_{t}$ 的最小化子位于\n$$\n\\tilde{\\lambda}_{t} \\in \\left( \\lambda_{i^{\\star}-1}, \\lambda_{i^{\\star}+1} \\right)\n$$\n前提是数据足够接近一个严格凸的图形。将 $\\tilde{\\lambda}_{t}$ 截断到该区间内可以防止因有限样本噪声引起的偏移。在 $R''(\\lambda^{\\star})  0$ 和一致收敛性下，随着区间局部化到 $\\lambda^{\\star}$ 附近，二次模型产生一个收缩性质：新的候选点渐近地将区间宽度按一个常数因子缩小，从而实现几何收缩。使用一个 $\\delta_{t} \\downarrow 0$ 的停止规则，区间宽度趋于 0，并且在迭代细化的网格上的离散最小化子 $\\hat{\\lambda}_{t}$ 在 $t \\to \\infty$ 和 $n \\to \\infty$ 时依概率收敛到 $\\lambda^{\\star}$。\n- 结合这些事实：区间限定以高概率维持一个包含 $\\lambda^{\\star}$ 的邻域；局部二次曲率驱动的插入加速了最小化子周围的网格细化；一致收敛性将最小化操作从 $E_{n}$ 转移到 $R$，从而得到 $\\hat{\\lambda}_{t} \\xrightarrow{p} \\lambda^{\\star}$。\n\n逐项分析选项：\n\nA. 此选项在 $E_{n}$ 的当前离散最小化子周围构建一个区间，并利用局部二次插值的最小化子进行曲率驱动的插入。将 $\\tilde{\\lambda}_{t}$ 截断到该区间内提供了对噪声的鲁棒性。停止规则 $\\delta_{t} \\downarrow 0$ 确保了最小化子附近的网格变得任意精细。其收敛性论证依赖于 $R(\\lambda)$ 的严格凸性和 $E_{n}(\\lambda)$ 的一致收敛性：(i) 区间最终以高概率包含 $\\lambda^{\\star}$，(ii) 在严格凸邻域中的二次插值产生一个近似牛顿步，从而使区间收缩，以及 (iii) 在围绕 $\\lambda^{\\star}$ 且宽度收缩至 0 的网格上的离散最小化子依概率收敛到 $\\lambda^{\\star}$。这是一个有效的曲率自适应方案，并有正确的收敛理据。结论 — 正确。\n\nB. 此选项细化曲率幅度最小（即较平坦）的区间，其信念是控制方差就足够了，无需凸性或一致性假设。这忽略了一个事实，即严格凸函数的最小化子位于正曲率区域；细化平坦区域会浪费远离最小化子的解析度。此外，声称无需一致收敛性或凸性就能收敛是没有根据的：没有这些，由于噪声，$E_{n}$ 可能会有伪极小值，而加密平坦区域并不能保证局部化到 $\\lambda^{\\star}$ 周围。结论 — 不正确。\n\nC. 此选项执行全局三次拟合和类似割线的更新，但没有区间限定。在存在有限样本噪声的情况下，对 $E_{n}$ 的全局多项式拟合可能会扭曲 $\\lambda^{\\star}$ 附近的局部曲率。没有区间限定或单调的导数符号变化，割线更新可能会振荡或发散，特别是当 $E_{n}$ 仅是近似凸且受估计误差影响时。在一般非凸性下收敛的主张没有得到基本事实或随机设置下标准求根理论的支持。结论 — 不正确。\n\nD. 均匀二分细化在各处都增加了网格密度，并在每个阶段选择离散最小化子。虽然密集采样减少了近似误差，但要从一个有噪声的目标函数的最小化子收敛到连续统的最小化子，需要一致收敛性和凸性，以确保伪极小值不会误导选择。该选项明确抛弃了对凸性和一致性的需求，因此过度声称：在有限样本中，密集网格可能会锁定在远离 $\\lambda^{\\star}$ 的噪声引起的极小值上。结论 — 不正确。\n\nE. 对高经验曲率进行阈值化并在所有标记的区间插入中点是一种并行的细化策略，没有区间限定。高经验曲率可能由噪声引起，而插入中点并不能足够精确地靶向最小化子；它可能会细化多个相距甚远的区域，却未能围绕真正的最小化子收缩一个区间。缺乏区间限定机制或与一致收敛性和严格凸性相关的收敛证明，使得该主张无效。结论 — 不正确。\n\n因此，正确的选择是选项 A 中的方案。", "answer": "$$\\boxed{A}$$", "id": "3441826"}]}