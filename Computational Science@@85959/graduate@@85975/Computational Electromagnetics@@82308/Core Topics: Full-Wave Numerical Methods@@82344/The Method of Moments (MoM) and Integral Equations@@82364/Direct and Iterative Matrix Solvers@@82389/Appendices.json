{"hands_on_practices": [{"introduction": "Understanding how iterative solvers perform is key to building efficient numerical methods. This practice delves into the local Fourier analysis of the Successive Over-Relaxation (SOR) method, a classic iterative solver and a workhorse smoother in multigrid algorithms. By deriving the optimal relaxation parameter for a model problem, you will gain hands-on experience with the analytical tools used to predict and optimize the convergence of iterative methods [@problem_id:3299116].", "problem": "In computational electromagnetics, consider the static curl-curl equation in a homogeneous, source-free, one-dimensional periodic medium with uniform magnetic permeability and electric permittivity. On a uniform Yee staggered grid with mesh spacing $h$, a single tangential electric field component reduces to a scalar equation in which the discrete curl-curl operator equals the standard second-difference operator\n$$\n(A u)_{j} \\;=\\; \\frac{1}{h^{2}}\\left(2 u_{j} - u_{j-1} - u_{j+1}\\right),\n$$\nwith periodic extension to the infinite grid. We wish to solve $A u = b$ using Successive Over-Relaxation (SOR) with relaxation parameter $\\omega>0$ in lexicographic ordering, based on the splitting $A = D - L - U$, where $D$ is the diagonal, $L$ the strictly lower, and $U$ the strictly upper part of $A$.\n\nUsing local Fourier analysis on the infinite grid, proceed as follows:\n\n1) Derive the Fourier symbol of the SOR error-propagation operator for a Fourier mode $e^{\\mathrm{i} j \\theta}$, where $\\theta \\in [0,\\pi]$ is the dimensionless angular frequency (angles in radians). Denote the corresponding amplification factor by $g_{\\omega}(\\theta)$.\n\n2) Define the high-frequency set as $\\mathcal{H} = \\{\\theta \\in [\\pi/2, \\pi]\\}$. Show that, for $0<\\omega<2$, the worst-case high-frequency amplification is attained at $\\theta = \\pi/2$, and write the high-frequency smoothing factor as\n$$\n\\sigma(\\omega) \\;=\\; \\max_{\\theta \\in \\mathcal{H}} \\, |g_{\\omega}(\\theta)| \\;=\\; |g_{\\omega}(\\tfrac{\\pi}{2})|.\n$$\n\n3) Determine the optimal relaxation parameter $\\omega_{\\star}$ that minimizes $\\sigma(\\omega)$ over $\\omega>0$, and give $\\sigma(\\omega_{\\star})$ in closed form.\n\n4) Consider a two-grid method with one pre-smoothing and one post-smoothing SOR step, and an idealized coarse-grid correction that exactly eliminates all low-frequency error components $\\theta \\in [0,\\pi/2)$ while leaving the high-frequency set $\\mathcal{H}$ unchanged. Under this idealization, predict the two-grid error reduction factor as\n$$\n\\rho_{\\mathrm{TG}}(\\omega) \\;=\\; \\left(\\sigma(\\omega)\\right)^{2},\n$$\nand evaluate it at $\\omega_{\\star}$.\n\nProvide your final answer as a pair consisting of the optimal relaxation parameter $\\omega_{\\star}$ and the corresponding two-grid error reduction factor $\\rho_{\\mathrm{TG}}(\\omega_{\\star})$, in exact analytical form. Do not include units in your final answer. If you choose to approximate numerically, round to four significant figures.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It describes a standard problem in the local Fourier analysis (or von Neumann analysis) of iterative multigrid methods. All terms are standard and the setup is consistent.\n\nThe solution proceeds by following the four steps outlined in the problem.\n\n1) Derivation of the Fourier symbol of the SOR error-propagation operator.\n\nThe discrete operator is given by\n$$ (A u)_{j} = \\frac{1}{h^{2}}\\left(2 u_{j} - u_{j-1} - u_{j+1}\\right) $$\nLet's consider the scaled operator $A' = h^2 A$, so that $(A' u)_{j} = 2 u_{j} - u_{j-1} - u_{j+1}$. The system to solve is $A'u = h^2 b$.\nThe operator $A'$ can be split as $A' = D' - L' - U'$, where $D'$ is the diagonal part, $L'$ is the strictly lower triangular part, and $U'$ is the strictly upper triangular part. In this one-dimensional context with lexicographic ordering (from smaller $j$ to larger $j$):\n- $(D'u)_j = 2u_j$\n- $(L'u)_j = u_{j-1}$\n- $(U'u)_j = u_{j+1}$\n\nThe Successive Over-Relaxation (SOR) iteration for solving $A'u=b'$ is defined by the splitting $A' = D' - L' - U'$ and the relaxation parameter $\\omega > 0$. The error $e^{(k)} = u^{(k)}-u$ at iteration $k$ propagates according to the equation:\n$$ (D' - \\omega L') e^{(k+1)} = ((1-\\omega)D' + \\omega U') e^{(k)} $$\nWriting this equation for the component $j$ gives:\n$$ 2 e_j^{(k+1)} - \\omega e_{j-1}^{(k+1)} = 2(1-\\omega) e_j^{(k)} + \\omega e_{j+1}^{(k)} $$\nWe analyze the behavior of a single Fourier mode for the error, $e_j^{(k)} = (g_{\\omega}(\\theta))^k e^{\\mathrm{i} j \\theta}$, where $g_{\\omega}(\\theta)$ is the amplification factor for the dimensionless angular frequency $\\theta$. Substituting this into the error propagation equation, we get:\n$$ g_{\\omega}(\\theta) \\left( 2 e^{\\mathrm{i} j \\theta} - \\omega e^{\\mathrm{i} (j-1) \\theta} \\right) = 2(1-\\omega) e^{\\mathrm{i} j \\theta} + \\omega e^{\\mathrm{i} (j+1) \\theta} $$\nDividing by $e^{\\mathrm{i} j \\theta}$ (which is non-zero):\n$$ g_{\\omega}(\\theta) \\left( 2 - \\omega e^{-\\mathrm{i} \\theta} \\right) = 2(1-\\omega) + \\omega e^{\\mathrm{i} \\theta} $$\nSolving for the amplification factor $g_{\\omega}(\\theta)$ yields its Fourier symbol:\n$$ g_{\\omega}(\\theta) = \\frac{2(1-\\omega) + \\omega e^{\\mathrm{i}\\theta}}{2 - \\omega e^{-\\mathrm{i}\\theta}} $$\n\n2) High-frequency smoothing factor $\\sigma(\\omega)$.\n\nThe high-frequency set is defined as $\\mathcal{H} = \\{\\theta \\in [\\pi/2, \\pi]\\}$. The smoothing factor is the worst-case (maximum) amplification factor over this set: $\\sigma(\\omega) = \\max_{\\theta \\in \\mathcal{H}} |g_{\\omega}(\\theta)|$.\nLet's compute the squared magnitude of $g_{\\omega}(\\theta)$:\n$$ |g_{\\omega}(\\theta)|^2 = g_{\\omega}(\\theta) \\overline{g_{\\omega}(\\theta)} = \\frac{2(1-\\omega) + \\omega e^{\\mathrm{i}\\theta}}{2 - \\omega e^{-\\mathrm{i}\\theta}} \\cdot \\frac{2(1-\\omega) + \\omega e^{-\\mathrm{i}\\theta}}{2 - \\omega e^{\\mathrm{i}\\theta}} $$\nThe numerator is:\n$$ N = \\left(2(1-\\omega) + \\omega \\cos\\theta \\right)^2 + (\\omega \\sin\\theta)^2 = 4(1-\\omega)^2 + 4\\omega(1-\\omega)\\cos\\theta + \\omega^2\\cos^2\\theta + \\omega^2\\sin^2\\theta = 4(1-2\\omega+\\omega^2) + 4\\omega(1-\\omega)\\cos\\theta + \\omega^2 = 5\\omega^2 - 8\\omega + 4 + (4\\omega - 4\\omega^2)\\cos\\theta $$\nThe denominator is:\n$$ D = (2 - \\omega\\cos\\theta)^2 + (\\omega \\sin\\theta)^2 = 4 - 4\\omega\\cos\\theta + \\omega^2\\cos^2\\theta + \\omega^2\\sin^2\\theta = \\omega^2 + 4 - 4\\omega\\cos\\theta $$\nLet $f(c) = |g_{\\omega}(\\theta)|^2$ where $c = \\cos\\theta$. For $\\theta \\in [\\pi/2, \\pi]$, $c$ is in the interval $[-1, 0]$.\n$$ f(c) = \\frac{5\\omega^2-8\\omega+4 + (4\\omega-4\\omega^2)c}{\\omega^2+4 - 4\\omega c} $$\nTo find the maximum, we compute the derivative with respect to $c$:\n$$ f'(c) = \\frac{(4\\omega-4\\omega^2)(\\omega^2+4-4\\omega c) - (5\\omega^2-8\\omega+4 + (4\\omega-4\\omega^2)c)(-4\\omega)}{(\\omega^2+4 - 4\\omega c)^2} $$\nThe sign of $f'(c)$ is determined by its numerator. After expansion and simplification, the terms containing $c$ cancel out, and the numerator becomes:\n$$ \\text{Num}' = 4\\omega(8 - 12\\omega + 6\\omega^2 - \\omega^3) = 4\\omega(2-\\omega)^3 $$\nFor $0 < \\omega < 2$, we have $4\\omega > 0$ and $(2-\\omega)^3 > 0$. Thus, $f'(c) > 0$. This means $f(c)$ is a monotonically increasing function of $c$.\nThe maximum value of $f(c)$ for $c \\in [-1, 0]$ is attained at $c=0$, which corresponds to $\\theta = \\pi/2$.\nTherefore, the high-frequency smoothing factor is $\\sigma(\\omega) = |g_{\\omega}(\\pi/2)|$.\nWe evaluate $g_{\\omega}(\\theta)$ at $\\theta = \\pi/2$:\n$$ g_{\\omega}(\\pi/2) = \\frac{2(1-\\omega) + \\omega e^{\\mathrm{i}\\pi/2}}{2 - \\omega e^{-\\mathrm{i}\\pi/2}} = \\frac{2-2\\omega + \\mathrm{i}\\omega}{2 + \\mathrm{i}\\omega} $$\nThe squared magnitude is:\n$$ \\sigma(\\omega)^2 = |g_{\\omega}(\\pi/2)|^2 = \\frac{(2-2\\omega)^2 + \\omega^2}{2^2 + \\omega^2} = \\frac{4 - 8\\omega + 4\\omega^2 + \\omega^2}{4+\\omega^2} = \\frac{5\\omega^2 - 8\\omega + 4}{\\omega^2 + 4} $$\n\n3) Optimal relaxation parameter $\\omega_{\\star}$.\n\nTo find the optimal relaxation parameter $\\omega_{\\star}$ that minimizes $\\sigma(\\omega)$, we can minimize $\\sigma(\\omega)^2$. Let $S(\\omega) = \\sigma(\\omega)^2$. We compute the derivative of $S(\\omega)$ and set it to zero:\n$$ S'(\\omega) = \\frac{(10\\omega - 8)(\\omega^2+4) - (5\\omega^2-8\\omega+4)(2\\omega)}{(\\omega^2 + 4)^2} = 0 $$\nSetting the numerator to zero gives:\n$$ 10\\omega^3 - 8\\omega^2 + 40\\omega - 32 - (10\\omega^3 - 16\\omega^2 + 8\\omega) = 0 $$\n$$ 8\\omega^2 + 32\\omega - 32 = 0 \\implies \\omega^2 + 4\\omega - 4 = 0 $$\nSolving the quadratic equation for $\\omega$:\n$$ \\omega = \\frac{-4 \\pm \\sqrt{16-4(1)(-4)}}{2} = \\frac{-4 \\pm \\sqrt{32}}{2} = \\frac{-4 \\pm 4\\sqrt{2}}{2} = -2 \\pm 2\\sqrt{2} $$\nSince $\\omega > 0$, we must choose the positive root:\n$$ \\omega_{\\star} = 2\\sqrt{2} - 2 = 2(\\sqrt{2}-1) $$\nNext, we find the minimal smoothing factor $\\sigma(\\omega_{\\star})$. We use the relation $\\omega_{\\star}^2 = 4 - 4\\omega_{\\star}$ from the quadratic equation to simplify the expression for $S(\\omega_{\\star})$:\n$$ S(\\omega_{\\star}) = \\frac{5\\omega_{\\star}^2 - 8\\omega_{\\star} + 4}{\\omega_{\\star}^2 + 4} = \\frac{5(4-4\\omega_{\\star}) - 8\\omega_{\\star} + 4}{(4-4\\omega_{\\star}) + 4} = \\frac{20 - 20\\omega_{\\star} - 8\\omega_{\\star} + 4}{8 - 4\\omega_{\\star}} = \\frac{24-28\\omega_{\\star}}{8-4\\omega_{\\star}} = \\frac{6-7\\omega_{\\star}}{2-\\omega_{\\star}} $$\nSubstituting $\\omega_{\\star} = 2\\sqrt{2}-2$:\n$$ S(\\omega_{\\star}) = \\frac{6 - 7(2\\sqrt{2}-2)}{2 - (2\\sqrt{2}-2)} = \\frac{6 - 14\\sqrt{2} + 14}{4 - 2\\sqrt{2}} = \\frac{20 - 14\\sqrt{2}}{4 - 2\\sqrt{2}} = \\frac{10 - 7\\sqrt{2}}{2 - \\sqrt{2}} $$\nRationalizing the denominator:\n$$ S(\\omega_{\\star}) = \\frac{10 - 7\\sqrt{2}}{2 - \\sqrt{2}} \\cdot \\frac{2 + \\sqrt{2}}{2 + \\sqrt{2}} = \\frac{20 + 10\\sqrt{2} - 14\\sqrt{2} - 14}{4-2} = \\frac{6 - 4\\sqrt{2}}{2} = 3 - 2\\sqrt{2} $$\nSo, $\\sigma(\\omega_{\\star})^2 = 3-2\\sqrt{2}$. We can recognize this as $(\\sqrt{2}-1)^2$.\nThus, the optimal smoothing factor is $\\sigma(\\omega_{\\star}) = \\sqrt{3-2\\sqrt{2}} = \\sqrt{(\\sqrt{2}-1)^2} = \\sqrt{2}-1$.\n\n4) Two-grid error reduction factor $\\rho_{\\mathrm{TG}}(\\omega_{\\star})$.\n\nThe described two-grid cycle consists of one pre-smoothing step, an idealized coarse-grid correction, and one post-smoothing step. The pre-smoother acts on the error, multiplying each Fourier component by $g_{\\omega}(\\theta)$. The idealized coarse-grid correction perfectly annihilates all low-frequency error components (for $\\theta \\in [0, \\pi/2)$) while leaving high-frequency components (for $\\theta \\in \\mathcal{H} = [\\pi/2, \\pi]$) untouched. The post-smoother then acts on the remaining high-frequency error, again multiplying each component by $g_{\\omega}(\\theta)$.\nTherefore, the amplification factor for a low-frequency mode is $0$, and for a high-frequency mode it is $(g_{\\omega}(\\theta))^2$. The overall two-grid error reduction factor is the maximum amplification factor over all modes:\n$$ \\rho_{\\mathrm{TG}}(\\omega) = \\max_{\\theta \\in [0, \\pi]} |\\text{amplification}(\\theta)| = \\max \\left( \\max_{\\theta \\in [0, \\pi/2)} |0|, \\max_{\\theta \\in [\\pi/2, \\pi]} |(g_{\\omega}(\\theta))^2| \\right) $$\n$$ \\rho_{\\mathrm{TG}}(\\omega) = \\left( \\max_{\\theta \\in [\\pi/2, \\pi]} |g_{\\omega}(\\theta)| \\right)^2 = (\\sigma(\\omega))^2 $$\nThis confirms the formula provided in the problem statement.\nWe evaluate this factor at the optimal parameter $\\omega_{\\star}$:\n$$ \\rho_{\\mathrm{TG}}(\\omega_{\\star}) = (\\sigma(\\omega_{\\star}))^2 $$\nFrom the previous step, we found $\\sigma(\\omega_{\\star})^2 = 3-2\\sqrt{2}$.\nSo, the two-grid error reduction factor at the optimal relaxation parameter is\n$$ \\rho_{\\mathrm{TG}}(\\omega_{\\star}) = 3-2\\sqrt{2} $$\nThe final answer is the pair $(\\omega_{\\star}, \\rho_{\\mathrm{TG}}(\\omega_{\\star}))$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2(\\sqrt{2}-1) & 3-2\\sqrt{2}\n\\end{pmatrix}\n}\n$$", "id": "3299116"}, {"introduction": "The dense linear systems arising from boundary integral equations present a significant computational challenge. This exercise guides you through the implementation of mixed-precision iterative refinement, a powerful technique that combines the speed of single-precision factorization with the accuracy of double-precision computation. By using a fast, approximate $LU$ decomposition as a preconditioner within an iterative loop, you will learn how to significantly accelerate solvers for problems such as electromagnetic scattering [@problem_id:3299108].", "problem": "Consider the classical frequency-domain scattering of a transverse magnetic field by a smooth, perfectly conducting circular cylinder of radius $a$ in two spatial dimensions. Under a time-harmonic dependence $e^{\\mathrm{i} \\omega t}$, Maxwell's equations reduce to the scalar Helmholtz equation for the out-of-plane electric field, whose boundary integral representation on the cylinder boundary leads to a dense linear system. Using a periodic trapezoidal Nyström discretization with $N$ uniformly spaced nodes on the circle and weight $w = 2 \\pi a / N$, a simple single-layer discretization with a stabilization term produces the complex linear system\n$$\nA x = b,\n$$\nwhere $A \\in \\mathbb{C}^{N \\times N}$ has entries\n$$\nA_{ij} = \n\\begin{cases}\nw \\, G\\!\\left(\\left\\| \\mathbf{r}_i - \\mathbf{r}_j \\right\\|\\right), & i \\neq j, \\\\\nw \\, G\\!\\left(s_{\\mathrm{avg}}\\right) + \\beta, & i = j,\n\\end{cases}\n$$\nwith $\\mathbf{r}_j = \\left(a \\cos \\theta_j, a \\sin \\theta_j\\right)$, $\\theta_j = 2 \\pi j / N$, $s_{\\mathrm{avg}} = 2 \\pi a / N$, and the two-dimensional free-space Green's function\n$$\nG(\\rho) = \\frac{\\mathrm{i}}{4} H_0^{(1)}\\!\\left(k_c \\rho\\right), \\quad k_c = k \\left(1 + \\mathrm{i} \\alpha\\right),\n$$\nwhere $H_0^{(1)}$ is the Hankel function of the first kind and order zero, $k$ is the real wavenumber in $\\mathrm{m}^{-1}$, and $\\alpha \\ge 0$ is a small absorption parameter (dimensionless) used to avoid non-physical singular behavior in the discrete diagonal. The right-hand side represents the incident plane wave evaluated on the boundary,\n$$\nb_i = \\exp\\!\\left(\\mathrm{i} \\, k_c \\, d_x \\, x_i\\right),\n$$\nwhere $x_i$ is the $x$-coordinate of $\\mathbf{r}_i$ and $d_x = 1$ is the $x$-component of a unit propagation direction. The stabilization parameter $\\beta > 0$ is a small real number (dimensionless) added to the diagonal to emulate combined-field effects and suppress near-nullspace components.\n\nYou are to implement mixed-precision iterative refinement to solve $A x = b$ in double precision using a single-precision complex LU factorization of $A$ as a right-preconditioner. Specifically, let $\\epsilon_{32}$ denote the machine precision of single precision and let $\\epsilon_{64}$ denote the machine precision of double precision. The algorithm must:\n- Construct $A$ and $b$ in double precision ($64$-bit complex).\n- Compute a single-precision ($32$-bit complex) LU factorization of $A$ with partial pivoting, which serves as the preconditioner.\n- Initialize $x^{(0)}$ to the zero vector in double precision.\n- For iterations $m = 0, 1, 2, \\dots$, compute the double-precision residual\n$$\nr^{(m)} = b - A x^{(m)}.\n$$\n- Convert $r^{(m)}$ to single precision, solve for the single-precision correction $\\Delta x^{(m)}$ using the single-precision LU factors,\n$$\nA \\, \\Delta x^{(m)} \\approx r^{(m)},\n$$\nand convert $\\Delta x^{(m)}$ back to double precision.\n- Update $x^{(m+1)} = x^{(m)} + \\Delta x^{(m)}$ in double precision.\n- Terminate when the double-precision relative residual\n$$\n\\eta^{(m)} = \\frac{\\left\\| r^{(m)} \\right\\|_2}{\\left\\| b \\right\\|_2}\n$$\nis below a target tolerance $\\tau = 10^{-11}$, or a maximum of $M = 50$ iterations is reached. The target tolerance is $\\tau = 10^{-11}$, and the maximum iterations is $M = 50$.\n\nStarting from the foundational principles of the frequency-domain Maxwell equations, the scalar Helmholtz equation, and the definition of the Green's function $G(\\rho)$, derive the discrete system structure above and the mixed-precision iterative refinement procedure. Then implement a complete, runnable program that constructs the matrix $A$ and vector $b$ based on the specified parameters, performs the iterative refinement, and returns the final achieved relative residual norm $\\eta^{(m_\\star)}$ for each test case, where $m_\\star$ is the final iteration index for that case.\n\nAll physical parameters must use the specified units: the cylinder radius $a$ is in $\\mathrm{m}$ and the wavenumber $k$ is in $\\mathrm{m}^{-1}$. The final relative residual norms are dimensionless. Angles are in radians. Your algorithm must use the vector $2$-norm.\n\nDesign a test suite with the following parameter sets $(N, k, \\alpha, \\beta, a)$:\n- Case $1$ (happy path): $(64, 20.0, 0.010, 0.10, 1.0)$.\n- Case $2$ (low-frequency edge): $(16, 0.50, 0.020, 0.20, 1.0)$.\n- Case $3$ (higher frequency, larger $N$): $(128, 40.0, 0.005, 0.05, 1.0)$.\n- Case $4$ (mild stabilization stress): $(64, 10.0, 0.005, 0.02, 1.0)$.\n\nFor each case, compute the final relative residual norm $\\eta^{(m_\\star)}$ achieved by the mixed-precision iterative refinement. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each float formatted in scientific notation with exactly $8$ significant digits, in the order of the cases above, for example, $\\left[\\text{res}_1,\\text{res}_2,\\text{res}_3,\\text{res}_4\\right]$.", "solution": "The fundamental base starts with Maxwell's equations for a time-harmonic field with angular frequency $\\omega$, which under the transverse magnetic polarization in two spatial dimensions reduce to the scalar Helmholtz equation\n$$\n\\left(\\nabla^2 + k^2\\right) u(\\mathbf{r}) = 0,\n$$\nin the exterior of the scatterer, where $k = \\omega / c$ is the free-space wavenumber in $\\mathrm{m}^{-1}$ and $u$ is the out-of-plane electric field component. For a perfectly conducting scatterer, $u$ satisfies appropriate boundary conditions on the scatterer boundary $\\Gamma$, and the scattered field can be represented via a boundary integral formulation using the free-space Green's function $G(\\rho)$ of the two-dimensional Helmholtz operator. The single-layer potential for the scattered field is\n$$\nu^{\\mathrm{scat}}(\\mathbf{r}) = \\int_{\\Gamma} G\\!\\left(\\left\\| \\mathbf{r} - \\mathbf{r}' \\right\\|\\right) \\sigma(\\mathbf{r}') \\, \\mathrm{d}s(\\mathbf{r}'),\n$$\nwith $\\sigma$ the unknown surface density. The free-space Green's function is\n$$\nG(\\rho) = \\frac{\\mathrm{i}}{4} H_0^{(1)}(k \\rho),\n$$\nwhere $H_0^{(1)}$ is the Hankel function of order zero and the first kind. To avoid non-physical singularities and emulate material losses, a small absorption can be introduced by replacing $k$ with the complex wavenumber $k_c = k \\left(1 + \\mathrm{i} \\alpha\\right)$, with $\\alpha \\ge 0$ dimensionless. On the boundary, collocation of the integral representation with a stabilized diagonal term (mimicking combined-field effects) yields the discrete dense system\n$$\nA x = b,\n$$\nwhere the unknown vector $x$ approximates samples of $\\sigma$ at boundary nodes and $b$ encodes the incident field boundary condition. For a circular boundary of radius $a$, parameterized by $\\theta \\in [0, 2\\pi)$ as $\\mathbf{r}(\\theta) = \\left(a \\cos \\theta, a \\sin \\theta\\right)$, a periodic trapezoidal Nyström discretization with $N$ uniform nodes $\\theta_j = 2 \\pi j / N$, weight $w = 2 \\pi a / N$, and stabilization $\\beta > 0$ leads to\n$$\nA_{ij} = \n\\begin{cases}\nw \\, \\dfrac{\\mathrm{i}}{4} H_0^{(1)}\\!\\left(k_c \\left\\| \\mathbf{r}_i - \\mathbf{r}_j \\right\\|\\right), & i \\neq j, \\\\\nw \\, \\dfrac{\\mathrm{i}}{4} H_0^{(1)}\\!\\left(k_c s_{\\mathrm{avg}}\\right) + \\beta, & i = j,\n\\end{cases}\n$$\nwith $s_{\\mathrm{avg}} = 2 \\pi a / N$. This diagonal modeling replaces the weakly singular self-interaction with a local average over one panel length and adds $\\beta$ to improve conditioning. The right-hand side for a plane wave incidence along the $x$-axis is $b_i = \\exp\\!\\left(\\mathrm{i} \\, k_c \\, x_i\\right)$, where $x_i = a \\cos \\theta_i$.\n\nWe now derive the mixed-precision iterative refinement algorithm. The goal is to obtain a solution in double precision to tight tolerance $\\tau$ while only computing a factorization in single precision. Let $A \\in \\mathbb{C}^{N \\times N}$ and $b \\in \\mathbb{C}^N$ be formed in double precision. Compute a single-precision LU factorization with partial pivoting:\n$$\nP A = L U,\n$$\nwhere the permutation $P$, lower-triangular $L$, and upper-triangular $U$ are stored implicitly by standard routines, all in single precision arithmetic. Initialize the double-precision iterate $x^{(0)} = 0$. The refinement iterations are then\n$$\nr^{(m)} = b - A x^{(m)}, \\quad \\Delta x^{(m)} \\approx A^{-1} r^{(m)},\n$$\nwhere the approximation of $A^{-1} r^{(m)}$ is realized by solving the triangular systems $U y = L^{-1} P r^{(m)}$ using the single-precision LU factors. Concretely, convert $r^{(m)}$ from double to single precision, apply forward and backward substitution in single precision, and convert the resulting $\\Delta x^{(m)}$ back to double precision. Update\n$$\nx^{(m+1)} = x^{(m)} + \\Delta x^{(m)}.\n$$\nAfter each update, compute the double-precision relative residual norm\n$$\n\\eta^{(m)} = \\frac{\\left\\| r^{(m)} \\right\\|_2}{\\left\\| b \\right\\|_2}.\n$$\nStop when $\\eta^{(m)} \\le \\tau$ or when $m$ reaches the maximum $M$.\n\nThe principle underlying iterative refinement is a backward error correction: the residual $r^{(m)}$ encodes the accumulated error in $x^{(m)}$ when measured in double precision. Solving for $\\Delta x^{(m)}$ with a lower-precision preconditioner reduces the forward error provided that the condition number $\\kappa_2(A)$ and the single-precision machine epsilon $\\epsilon_{32}$ satisfy approximately $\\kappa_2(A) \\, \\epsilon_{32} < 1$; this ensures that the single-precision triangular solves produce sufficiently accurate corrections so that the double-precision residual decreases geometrically until the double-precision rounding limits are encountered. The absorption parameter $\\alpha$ and stabilization $\\beta$ reduce $\\kappa_2(A)$, improving the likelihood that iterative refinement converges to a tight tolerance.\n\nAlgorithmic design details:\n- Construct $\\mathbf{r}_j = \\left(a \\cos \\theta_j, a \\sin \\theta_j\\right)$ with $\\theta_j = 2 \\pi j / N$. The pairwise distances $\\|\\mathbf{r}_i - \\mathbf{r}_j\\|$ on a circle are $\\sqrt{a^2 + a^2 - 2 a^2 \\cos(\\theta_i - \\theta_j)} = a \\sqrt{2 - 2 \\cos(\\theta_i - \\theta_j)} = 2 a \\left| \\sin\\!\\left(\\frac{\\theta_i - \\theta_j}{2}\\right) \\right|$, which we compute numerically for robustness.\n- Form $A$ in double precision using $G(\\rho) = \\dfrac{\\mathrm{i}}{4} H_0^{(1)}\\!\\left(k_c \\rho\\right)$ with $k_c = k \\left(1 + \\mathrm{i} \\alpha\\right)$, and set the diagonal with $w \\, G(s_{\\mathrm{avg}}) + \\beta$.\n- Form $b$ with $b_i = \\exp\\!\\left(\\mathrm{i} \\, k_c \\, x_i\\right)$, $x_i = a \\cos \\theta_i$.\n- Compute single-precision LU factors $(L, U, P)$ via a standard routine and reuse them in all refinement steps.\n- Iterate until $\\eta^{(m)} \\le \\tau$ or $m = M$ and record the final $\\eta^{(m_\\star)}$.\n\nThe test suite explores different regimes:\n- Case $1$ uses moderate $N$ and $k$ with small absorption and stabilization, representing a well-conditioned scenario.\n- Case $2$ probes low-frequency with small $N$, potentially testing near-singular behavior mitigated by $\\alpha$ and $\\beta$.\n- Case $3$ uses higher frequency and larger $N$, stressing both the conditioning and the computational cost of dense assembly.\n- Case $4$ reduces stabilization, challenging the refinement's robustness.\n\nThe program will assemble these cases, perform iterative refinement, and output the final relative residual norms as a single line with comma-separated scientific-notation floats with exactly $8$ significant digits, enclosed in square brackets, in the order of the cases. The results are dimensionless, and all angles are in radians.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import hankel1\nfrom scipy.linalg import lu_factor, lu_solve\n\ndef assemble_system(N, k, alpha, beta, a):\n    \"\"\"\n    Assemble the dense boundary integral system matrix A and RHS b\n    for a circular cylinder of radius a in 2D using a stabilized\n    single-layer discretization with periodic trapezoidal rule.\n\n    Parameters:\n        N (int): number of boundary nodes\n        k (float): real wavenumber (1/m)\n        alpha (float): absorption parameter (dimensionless)\n        beta (float): diagonal stabilization parameter (dimensionless)\n        a (float): cylinder radius (m)\n\n    Returns:\n        A64 (np.ndarray): complex128 system matrix (N x N)\n        b64 (np.ndarray): complex128 right-hand side (N,)\n    \"\"\"\n    # Discretization points on the circle\n    j = np.arange(N, dtype=np.float64)\n    theta = 2.0 * np.pi * j / float(N)\n    # Boundary points r_j = (a cos theta_j, a sin theta_j)\n    x = a * np.cos(theta)\n    y = a * np.sin(theta)\n\n    # Complex wavenumber with small absorption\n    kc = complex(k, k * alpha)  # k * (1 + i*alpha) == k + i*k*alpha\n\n    # Pairwise distances on a circle: robust computation\n    # Using broadcasting to compute all pairwise differences\n    theta_i = theta.reshape(-1, 1)\n    theta_j = theta.reshape(1, -1)\n    dtheta = theta_i - theta_j\n    # rho_ij = 2 a |sin((theta_i - theta_j)/2)|\n    rho = 2.0 * a * np.abs(np.sin(0.5 * dtheta))\n\n    # Weight for trapezoidal rule\n    w = 2.0 * np.pi * a / float(N)\n    # Average panel length for diagonal replacement\n    s_avg = w\n\n    # Assemble A in complex128\n    A64 = np.empty((N, N), dtype=np.complex128)\n    # Off-diagonal: w * G(rho_ij)\n    # Green's function G(rho) = i/4 * H0^(1)(kc * rho)\n    # SciPy's hankel1 function handles complex arguments correctly.\n    z = kc * rho\n    # Compute Hankel of order 0, first kind\n    H0 = hankel1(0, z)\n    G = 1j * 0.25 * H0\n    A64[:] = w * G\n\n    # Diagonal: replace with w * G(s_avg) + beta\n    H0_diag = hankel1(0, kc * s_avg)\n    G_diag = 1j * 0.25 * H0_diag\n    np.fill_diagonal(A64, w * G_diag + beta)\n\n    # Right-hand side: incident plane wave along +x: b_i = exp(i * kc * x_i)\n    b64 = np.exp(1j * kc * x)\n\n    return A64, b64\n\ndef mixed_precision_iterative_refinement(A64, b64, max_iters=50, tol=1e-11):\n    \"\"\"\n    Perform mixed-precision iterative refinement using single-precision\n    LU factorization as a preconditioner to solve A x = b in double precision.\n\n    Parameters:\n        A64 (np.ndarray): complex128 system matrix\n        b64 (np.ndarray): complex128 right-hand side\n        max_iters (int): maximum number of refinement iterations\n        tol (float): target relative residual tolerance\n\n    Returns:\n        rel_res (float): final achieved relative residual norm\n    \"\"\"\n    # Single-precision copy and LU factorization\n    A32 = A64.astype(np.complex64)\n    try:\n        lu32, piv = lu_factor(A32)\n    except Exception:\n        # In case factorization fails, return a large residual to indicate failure.\n        return float(\"inf\")\n\n    # Initialize x in double precision\n    x64 = np.zeros_like(b64, dtype=np.complex128)\n\n    # Precompute norm of b\n    norm_b = np.linalg.norm(b64)\n    if norm_b == 0.0:\n        return 0.0\n\n    # Iterate\n    rel_res = np.inf\n    for _ in range(max_iters):\n        # Residual in double precision\n        r64 = b64 - A64 @ x64\n        norm_r = np.linalg.norm(r64)\n        rel_res = norm_r / norm_b\n\n        if rel_res = tol:\n            break\n\n        # Solve for correction in single precision using LU\n        r32 = r64.astype(np.complex64)\n        dx32 = lu_solve((lu32, piv), r32)\n        dx64 = dx32.astype(np.complex128)\n\n        # Update\n        x64 += dx64\n\n    # Recompute final double-precision relative residual to be consistent\n    r64 = b64 - A64 @ x64\n    rel_res = np.linalg.norm(r64) / norm_b\n    return float(rel_res.real)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (N, k, alpha, beta, a)\n    test_cases = [\n        (64, 20.0, 0.010, 0.10, 1.0),  # Case 1\n        (16, 0.50, 0.020, 0.20, 1.0),  # Case 2\n        (128, 40.0, 0.005, 0.05, 1.0), # Case 3\n        (64, 10.0, 0.005, 0.02, 1.0),  # Case 4\n    ]\n\n    results = []\n    for (N, k, alpha, beta, a) in test_cases:\n        A64, b64 = assemble_system(N, k, alpha, beta, a)\n        rel_res = mixed_precision_iterative_refinement(A64, b64, max_iters=50, tol=1e-11)\n        results.append(rel_res)\n\n    # Format floats in scientific notation with exactly 8 significant digits.\n    formatted = [f\"{val:.8e}\" for val in results]\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3299108"}, {"introduction": "While direct solvers for sparse systems are often used as black-box tools, their remarkable efficiency stems from sophisticated graph-theoretic algorithms. This practice invites you to look under the hood by analyzing the Cholesky factorization of a stiffness matrix from a small tetrahedral mesh. You will construct the elimination tree for a given node ordering and use it to predict parallel performance, providing a concrete understanding of how fill-reducing permutations and task scheduling are fundamental to modern sparse solvers [@problem_id:3299155].", "problem": "A static electric potential field in a simply connected, homogeneous three-dimensional dielectric is modeled by the scalar Laplace equation and discretized by the Finite Element Method (FEM) with linear tetrahedral elements on a small mesh. The assembled stiffness matrix is Symmetric Positive Definite (SPD). The tetrahedral mesh consists of two tetrahedra sharing a triangular face:\n- Tetrahedron $\\mathrm{T}_1$ has vertex set $\\{1,2,3,4\\}$.\n- Tetrahedron $\\mathrm{T}_2$ has vertex set $\\{1,2,3,5\\}$.\n\nAssume standard nodal basis functions and assembly so that the symmetric sparsity pattern of the global stiffness matrix connects any pair of nodes that appear together in at least one tetrahedron.\n\nConsider the fill-reducing permutation $P$ that orders the unknowns as $\\{4,5,1,2,3\\}$ (i.e., new column/row index $1$ corresponds to old node $4$, new index $2$ to old node $5$, new index $3$ to old node $1$, new index $4$ to old node $2$, and new index $5$ to old node $3$). A direct solver based on Cholesky factorization is employed.\n\nTasks:\n- Using only the foundational definitions of sparse Cholesky factorization and its elimination graph, construct the elimination tree of the permuted stiffness matrix under the ordering $P$.\n- Now consider a multifrontal direct solver that treats each column as a separate frontal task (no supernodal amalgamation). The frontal matrix size at column $j$ is defined to be $f_j = 1 + d_j$, where $d_j$ is the number of higher-index neighbors of column $j$ in the filled graph at the instant of eliminating column $j$. Assume the dense Cholesky factorization cost model for a frontal matrix of order $f$, namely a computational work of $\\frac{1}{3} f^3$ floating-point operations, and assume unit time per floating-point operation. Suppose there are effectively unlimited processors and negligible scheduling/communication overhead, so that the minimum parallel completion time is given by the sum of the node weights $\\frac{1}{3} f_j^3$ along the critical path of the elimination tree.\n\nWhat is the exact value of the minimum parallel completion time (in floating-point operation units) implied by this model for this mesh and ordering? Provide your answer as an exact rational number with no units. Do not round.", "solution": "The problem asks for the minimum parallel completion time for the Cholesky factorization of a specific sparse matrix, given a particular node ordering and a simplified parallel performance model. The solution requires a step-by-step analysis of the sparse factorization process.\n\n**Step 1: Construct the Graph of the Stiffness Matrix**\n\nThe problem states that the sparsity pattern of the symmetric stiffness matrix $A$ connects any pair of nodes that appear together in at least one tetrahedron. The total set of nodes is $V = \\{1, 2, 3, 4, 5\\}$.\n- Tetrahedron $\\mathrm{T}_1 = \\{1, 2, 3, 4\\}$ forms a clique $K_4$ on its vertices. The edges are $(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)$.\n- Tetrahedron $\\mathrm{T}_2 = \\{1, 2, 3, 5\\}$ forms a clique $K_4$ on its vertices. The edges are $(1,2), (1,3), (1,5), (2,3), (2,5), (3,5)$.\n\nThe graph of the matrix, $G(A)$, has vertices $V$ and edges formed by the union of the edges from both cliques. The sets of neighbors (adjacency lists) for each node $i$, denoted $\\mathrm{Adj}_G(i)$, are:\n- $\\mathrm{Adj}_G(1) = \\{2, 3, 4, 5\\}$\n- $\\mathrm{Adj}_G(2) = \\{1, 3, 4, 5\\}$\n- $\\mathrm{Adj}_G(3) = \\{1, 2, 4, 5\\}$\n- $\\mathrm{Adj}_G(4) = \\{1, 2, 3\\}$\n- $\\mathrm{Adj}_G(5) = \\{1, 2, 3\\}$\n\n**Step 2: Determine the Filled Graph for the Given Ordering**\n\nThe permutation $P$ specifies the elimination ordering of the nodes: $\\{4, 5, 1, 2, 3\\}$. For clarity, we will denote the new indices by primes: $1' \\to 4$, $2' \\to 5$, $3' \\to 1$, $4' \\to 2$, $5' \\to 3$. We need to find the filled graph, $G^+$, that results from symbolically factoring the permuted matrix $A_P = PAP^T$.\n\nThe filled graph $G^+$ contains the original edges of $G(A_P)$ plus \"fill\" edges. A fill edge $(i, j)$ is added during the elimination of a node $k$ if $(k, i)$ and $(k, j)$ are edges in the graph just before $k$'s elimination, but $(i, j)$ is not. This is equivalent to stating that all higher-numbered neighbors of an eliminated node form a clique.\n\nLet's simulate the elimination process following the new ordering:\n1.  **Eliminate node $1'$ (original node $4$):** The neighbors of node $4$ are $\\{1, 2, 3\\}$, which correspond to new indices $\\{3', 4', 5'\\}$. In the original graph $G$, these three nodes form a clique (they are all in both $\\mathrm{T}_1$ and $\\mathrm{T}_2$). Thus, no new edges are added.\n2.  **Eliminate node $2'$ (original node $5$):** The neighbors of node $5$ are $\\{1, 2, 3\\}$, which correspond to new indices $\\{3', 4', 5'\\}$. As before, these nodes already form a clique. No fill-in occurs.\n3.  **Eliminate node $3'$ (original node $1$):** At this stage, the remaining graph consists of the clique on nodes $\\{1, 2, 3\\}$ (new indices $\\{3', 4', 5'\\}$). The higher-indexed neighbors of $3'$ are $\\{4', 5'\\}$. These are already connected. No fill-in occurs.\n4.  **Eliminate node $4'$ (original node $2$):** The only remaining higher-indexed neighbor is $5'$, so no new edge can be formed.\n\nSince no edges were added at any step, the filled graph $G^+$ is identical to the original graph $G(A)$ (with nodes relabeled according to the permutation). $G^+ = G(A)$.\n\n**Step 3: Construct the Elimination Tree**\n\nThe elimination tree $T$ is defined on the nodes in the new ordering $\\{1', 2', 3', 4', 5'\\}$. The parent of a node $j$ is the lowest-indexed neighbor of $j$ in the filled graph $G^+$ that has an index greater than $j$.\n$$ \\mathrm{parent}(j) = \\min \\{ k \\mid k  j \\text{ and } (j, k) \\text{ is an edge in } G^+ \\} $$\n\nUsing the new indices and the adjacencies in $G^+$:\n-   **Node $1'$ (old 4):** Neighbors are $\\{3', 4', 5'\\}$. All have indices greater than $1'$.\n    $\\mathrm{parent}(1') = \\min\\{3', 4', 5'\\} = 3'$.\n-   **Node $2'$ (old 5):** Neighbors are $\\{3', 4', 5'\\}$. All have indices greater than $2'$.\n    $\\mathrm{parent}(2') = \\min\\{3', 4', 5'\\} = 3'$.\n-   **Node $3'$ (old 1):** Higher-indexed neighbors are $\\{4', 5'\\}$.\n    $\\mathrm{parent}(3') = \\min\\{4', 5'\\} = 4'$.\n-   **Node $4'$ (old 2):** The only higher-indexed neighbor is $\\{5'\\}$.\n    $\\mathrm{parent}(4') = 5'$.\n-   **Node $5'$ (old 3):** This is the highest-indexed node, so it is the root of the tree and has no parent.\n\nThe structure of the elimination tree is: nodes $1'$ and $2'$ are children of $3'$; $3'$ is a child of $4'$; and $4'$ is a child of $5'$.\n\n**Step 4: Calculate Computational Work for Each Node**\n\nThe computational work (cost) for eliminating column $j$ is given by $W_j = \\frac{1}{3} f_j^3$, where $f_j = 1 + d_j$. The quantity $d_j$ is the number of neighbors of $j$ in $G^+$ with an index greater than $j$.\n\n-   **Node $j=1'$:** Higher-indexed neighbors in $G^+$ are $\\{3', 4', 5'\\}$.\n    $d_{1'} = 3$, so $f_{1'} = 1+3=4$.\n    $W_{1'} = \\frac{1}{3} (4^3) = \\frac{64}{3}$.\n-   **Node $j=2'$:** Higher-indexed neighbors in $G^+$ are $\\{3', 4', 5'\\}$.\n    $d_{2'} = 3$, so $f_{2'} = 1+3=4$.\n    $W_{2'} = \\frac{1}{3} (4^3) = \\frac{64}{3}$.\n-   **Node $j=3'$:** Higher-indexed neighbors in $G^+$ are $\\{4', 5'\\}$.\n    $d_{3'} = 2$, so $f_{3'} = 1+2=3$.\n    $W_{3'} = \\frac{1}{3} (3^3) = \\frac{27}{3} = 9$.\n-   **Node $j=4'$:** Higher-indexed neighbor in $G^+$ is $\\{5'\\}$.\n    $d_{4'} = 1$, so $f_{4'} = 1+1=2$.\n    $W_{4'} = \\frac{1}{3} (2^3) = \\frac{8}{3}$.\n-   **Node $j=5'$:** There are no higher-indexed neighbors.\n    $d_{5'} = 0$, so $f_{5'} = 1+0=1$.\n    $W_{5'} = \\frac{1}{3} (1^3) = \\frac{1}{3}$.\n\n**Step 5: Determine the Critical Path and Minimum Parallel Time**\n\nWith unlimited processors, the work at a node can only begin after the work at all of its children in the elimination tree is complete. The minimum parallel completion time is the sum of the work costs $W_j$ along the critical path, which is the longest path from any leaf to the root of the tree, weighted by the costs $W_j$.\n\nThe elimination tree has two leaves, $1'$ and $2'$. This gives two paths to the root $5'$:\n1.  Path A: $1' \\to 3' \\to 4' \\to 5'$\n2.  Path B: $2' \\to 3' \\to 4' \\to 5'$\n\nLet's calculate the total weighted length for each path:\n-   Length(Path A) = $W_{1'} + W_{3'} + W_{4'} + W_{5'}$\n    $= \\frac{64}{3} + 9 + \\frac{8}{3} + \\frac{1}{3}$\n    $= \\frac{64}{3} + \\frac{27}{3} + \\frac{8}{3} + \\frac{1}{3}$\n    $= \\frac{64 + 27 + 8 + 1}{3} = \\frac{100}{3}$.\n-   Length(Path B) = $W_{2'} + W_{3'} + W_{4'} + W_{5'}$\n    $= \\frac{64}{3} + 9 + \\frac{8}{3} + \\frac{1}{3}$\n    $= \\frac{100}{3}$.\n\nBoth paths have the same length. The critical path length, and thus the minimum parallel completion time, is $\\frac{100}{3}$.", "answer": "$$\\boxed{\\frac{100}{3}}$$", "id": "3299155"}]}