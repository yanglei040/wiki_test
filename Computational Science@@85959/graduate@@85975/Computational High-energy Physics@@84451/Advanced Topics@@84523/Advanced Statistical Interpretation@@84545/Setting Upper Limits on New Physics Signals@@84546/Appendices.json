{"hands_on_practices": [{"introduction": "We begin our practice by exploring the simplest possible scenario in a search for new physics: a \"background-free\" experiment that observes zero events. This foundational exercise allows us to derive an upper limit from first principles, building a solid intuition for the statistical logic without the complication of nuisance parameters or complex data. By tackling this problem from both a classical frequentist and a Bayesian perspective [@problem_id:3533332], we can directly compare their formalisms and see how they converge in this idealized yet insightful case.", "problem": "A counting experiment in a high-energy physics search is modeled as a Poisson process. The expected number of events is the sum of an unknown signal yield and a known background yield. Let the unknown signal yield be denoted by $\\mu \\geq 0$, and the known background yield by $b \\geq 0$. The number of observed events $n$ is then distributed according to the Poisson probability mass function $P(n \\mid \\mu, b) = \\exp\\!\\left(-(\\mu + b)\\right)\\,(\\mu + b)^{n}/n!$. Consider the case where a single signal region yields $n = 0$ observed events and the expected background is $b = 0$. Using this model and starting from the foundational definitions of a Poisson process and confidence construction, perform the following:\n\n1. Derive the classical frequentist one-sided upper limit on $\\mu$ at confidence level $1-\\alpha$ using the Neyman construction for a one-sided interval whose lower endpoint is fixed at $0$. Provide the final closed-form expression for the upper limit as a function of $\\alpha$.\n\n2. Independently, derive the Bayesian $1-\\alpha$ upper credible bound on $\\mu$ under a non-informative prior that is flat on $\\mu \\geq 0$, using Bayesâ€™ theorem and the Poisson likelihood from the same model. Provide the final closed-form expression for the upper bound as a function of $\\alpha$.\n\n3. Compare the two results obtained above by determining whether the classical frequentist upper limit and the Bayesian upper credible bound coincide in this special case, and if so, provide the common closed-form expression.\n\nYour final answer must be a single closed-form analytic expression in terms of $\\alpha$ only. The quantity $\\mu$ is dimensionless (event count), so no unit specification is required. No numerical rounding is required for the final result.", "solution": "The problem is first validated against the required criteria.\n\n### Step 1: Extract Givens\n-   **Model**: Counting experiment modeled as a Poisson process.\n-   **Signal Yield**: $\\mu \\geq 0$ (unknown).\n-   **Background Yield**: $b \\geq 0$ (known).\n-   **Total Expected Events**: $\\mu + b$.\n-   **Observed Events**: $n$.\n-   **Probability Mass Function (PMF)**: $P(n \\mid \\mu, b) = \\exp(-(\\mu + b))\\,(\\mu + b)^{n}/n!$.\n-   **Specific Conditions**: Observed events $n = 0$, expected background $b = 0$.\n-   **Confidence Level (CL)**: $1-\\alpha$.\n-   **Task 1 (Frequentist)**: Derive the one-sided upper limit on $\\mu$ at CL $1-\\alpha$ using the Neyman construction for an interval $[0, \\mu_{up}]$.\n-   **Task 2 (Bayesian)**: Derive the $1-\\alpha$ upper credible bound on $\\mu$ using a non-informative flat prior on $\\mu \\geq 0$.\n-   **Task 3 (Comparison)**: Compare the results and provide the common expression if they are identical.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated for validity.\n-   **Scientifically Grounded**: The problem uses the Poisson distribution, which is the standard and correct statistical model for counting experiments in particle physics and other fields. The concepts of frequentist upper limits (Neyman construction) and Bayesian credible bounds are fundamental and well-established methods in statistical inference. The scenario, while simplified ($b=0$, $n=0$), represents a valid and important limiting case (a \"background-free\" experiment with no observed events).\n-   **Well-Posed**: The problem is well-defined, providing all necessary information: the statistical model, the specific data, the desired confidence/credible level, and the statistical frameworks to be used. The questions are precise and lead to unique, derivable solutions.\n-   **Objective**: The language is clear, formal, and free of any subjective or opinion-based content.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically sound, formally stated, complete, and poses a non-trivial but solvable question in statistical data analysis.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. The solution will now be derived.\n\n***\n\n### 1. Frequentist Upper Limit Derivation\n\nThe classical frequentist one-sided upper limit, $\\mu_{up}$, at a confidence level of $1-\\alpha$ is constructed based on the Neyman principle. For a one-sided interval $[0, \\mu_{up}]$, the limit is defined as the value of the parameter $\\mu$ for which the probability of observing an outcome as restrictive or more restrictive than the one observed, $n_{obs}$, is equal to $\\alpha$. In a counting experiment, a smaller number of observed events is more restrictive for an upper limit on the signal.\n\nGiven the observed number of events $n_{obs}$, the upper limit $\\mu_{up}$ is the value of $\\mu$ that solves the equation:\n$$P(n \\leq n_{obs} \\mid \\mu_{up}) = \\alpha$$\n\nThe problem specifies the Poisson model. With the given condition $b=0$, the total expected number of events is simply $\\mu$. The probability mass function is:\n$$P(n \\mid \\mu) = \\frac{\\exp(-\\mu) \\mu^n}{n!}$$\n\nThe problem states that the number of observed events is $n_{obs} = 0$. Substituting this into the defining equation for the upper limit, we get:\n$$P(n \\leq 0 \\mid \\mu_{up}) = \\alpha$$\nFor a Poisson distribution, the only way $n \\leq 0$ can occur is if $n=0$. Therefore, the equation simplifies to:\n$$P(n = 0 \\mid \\mu_{up}) = \\alpha$$\n\nUsing the Poisson PMF with $n=0$:\n$$P(n=0 \\mid \\mu_{up}) = \\frac{\\exp(-\\mu_{up}) \\mu_{up}^0}{0!} = \\exp(-\\mu_{up})$$\nHere we use the facts that $\\mu_{up}^0 = 1$ and $0! = 1$.\n\nSetting this equal to $\\alpha$:\n$$\\exp(-\\mu_{up}) = \\alpha$$\n\nTo solve for $\\mu_{up}$, we take the natural logarithm of both sides:\n$$-\\mu_{up} = \\ln(\\alpha)$$\n$$\\mu_{up} = -\\ln(\\alpha)$$\nThis is the classical frequentist one-sided upper limit on $\\mu$.\n\n### 2. Bayesian Upper Credible Bound Derivation\n\nThe Bayesian approach starts with a prior probability distribution for the unknown parameter $\\mu$, denoted by $\\pi(\\mu)$. This is updated to a posterior probability distribution, $P(\\mu \\mid n_{obs})$, using the observed data $n_{obs}$ via Bayes' theorem. A $1-\\alpha$ credible interval is an interval in the parameter space that contains $1-\\alpha$ of the total posterior probability. For a one-sided upper bound $\\mu_{up}$, the interval is $[0, \\mu_{up}]$, and it must satisfy:\n$$\\int_0^{\\mu_{up}} P(\\mu \\mid n_{obs}) \\, d\\mu = 1-\\alpha$$\n\nBayes' theorem states:\n$$P(\\mu \\mid n) = \\frac{P(n \\mid \\mu) \\pi(\\mu)}{\\int_0^{\\infty} P(n \\mid \\mu') \\pi(\\mu') \\, d\\mu'}$$\n\nThe problem provides the following:\n-   **Likelihood**, $P(n \\mid \\mu)$: The Poisson distribution with $b=0$, so $P(n \\mid \\mu) = \\frac{\\exp(-\\mu) \\mu^n}{n!}$.\n-   **Prior**, $\\pi(\\mu)$: A non-informative prior that is flat for $\\mu \\geq 0$. We can write this as $\\pi(\\mu) = C$ for $\\mu \\geq 0$, where $C$ is a constant. As this constant will cancel out, we can set $C=1$.\n-   **Data**, $n_{obs} = 0$.\n\nFirst, we construct the numerator of Bayes' theorem (the posterior unnormalized):\n$$P(n=0 \\mid \\mu) \\pi(\\mu) = \\left( \\frac{\\exp(-\\mu) \\mu^0}{0!} \\right) \\cdot 1 = \\exp(-\\mu)$$\n\nNext, we compute the denominator, which is the normalization constant (also called the marginal likelihood or evidence):\n$$\\int_0^{\\infty} P(n=0 \\mid \\mu') \\pi(\\mu') \\, d\\mu' = \\int_0^{\\infty} \\exp(-\\mu') \\, d\\mu'$$\nThis integral evaluates to:\n$$\\int_0^{\\infty} \\exp(-\\mu') \\, d\\mu' = [-\\exp(-\\mu')]_0^{\\infty} = (-\\lim_{x \\to \\infty} \\exp(-x)) - (-\\exp(0)) = 0 - (-1) = 1$$\n\nTherefore, the normalized posterior distribution for $\\mu$ given $n=0$ is:\n$$P(\\mu \\mid n=0) = \\frac{\\exp(-\\mu)}{1} = \\exp(-\\mu) \\quad \\text{for } \\mu \\geq 0$$\nThis posterior distribution is an exponential distribution with a rate parameter of $1$.\n\nNow, we find the upper bound $\\mu_{up}$ by integrating the posterior from $0$ to $\\mu_{up}$ and setting the result to $1-\\alpha$:\n$$\\int_0^{\\mu_{up}} \\exp(-\\mu) \\, d\\mu = 1-\\alpha$$\nPerforming the integration:\n$$[-\\exp(-\\mu)]_0^{\\mu_{up}} = 1-\\alpha$$\n$$(-\\exp(-\\mu_{up})) - (-\\exp(0)) = 1-\\alpha$$\n$$-\\exp(-\\mu_{up}) + 1 = 1-\\alpha$$\n\nSolving for $\\exp(-\\mu_{up})$:\n$$-\\exp(-\\mu_{up}) = -\\alpha$$\n$$\\exp(-\\mu_{up}) = \\alpha$$\n\nFinally, solving for $\\mu_{up}$:\n$$-\\mu_{up} = \\ln(\\alpha)$$\n$$\\mu_{up} = -\\ln(\\alpha)$$\nThis is the Bayesian $1-\\alpha$ upper credible bound on $\\mu$.\n\n### 3. Comparison of Results\n\nThe classical frequentist one-sided upper limit at $1-\\alpha$ confidence level, derived for $n=0$ and $b=0$, is:\n$$\\mu_{up}^{\\text{freq}} = -\\ln(\\alpha)$$\n\nThe Bayesian one-sided upper credible bound at $1-\\alpha$ credibility, derived for $n=0$, $b=0$ and a flat prior on $\\mu \\geq 0$, is:\n$$\\mu_{up}^{\\text{Bayes}} = -\\ln(\\alpha)$$\n\nThe two results are identical. This is a special but notable case where the frequentist and Bayesian results coincide. Therefore, the common closed-form expression for the limit/bound is $-\\ln(\\alpha)$.", "answer": "$$\\boxed{-\\ln(\\alpha)}$$", "id": "3533332"}, {"introduction": "Real-world analyses are rarely as simple as our first example; they often involve numerous sources of uncertainty and complex likelihood functions. To manage this complexity, physicists employ powerful asymptotic approximations. This practice introduces the widely-used CLs method and the concept of the \"Asimov\" dataset to derive the expected sensitivity of an experiment before any data is actually collected [@problem_id:3533347]. By working through this derivation, you will gain insight into the statistical machinery that powers limit setting in modern high-energy physics.", "problem": "A single-parameter signal-strength hypothesis $ \\mu \\ge 0 $ is tested with the profile likelihood ratio test statistic $ q_{\\mu} \\equiv -2 \\ln \\lambda(\\mu) $, where $ \\lambda(\\mu) $ is the profile likelihood ratio built from a likelihood $ L(\\mu,\\boldsymbol{\\theta}) $ for data and nuisance parameters $ \\boldsymbol{\\theta} $. In the large-sample regime, assume the Wald expansion holds for the one-sided test:\n- $ q_{\\mu} \\approx \\frac{(\\mu - \\hat{\\mu})^{2}}{\\sigma^{2}} $ if $ \\hat{\\mu} \\le \\mu $,\n- $ q_{\\mu} = 0 $ if $ \\hat{\\mu}  \\mu $,\nwhere $ \\hat{\\mu} $ is the maximum-likelihood estimator of $ \\mu $, and $ \\sigma $ is the asymptotic standard deviation of $ \\hat{\\mu} $ evaluated at the true parameter value.\n\nAssume further that $ \\hat{\\mu} \\sim \\mathcal{N}(\\mu', \\sigma^{2}) $ under a data-generating (true) signal strength $ \\mu' $, and that the distribution of $ q_{\\mu} $ is well approximated by a noncentral chi-square distribution with one degree of freedom and noncentrality parameter $ \\Lambda = \\frac{(\\mu - \\mu')^{2}}{\\sigma^{2}} $, up to the one-sided truncation encoded above.\n\nDefine the modified confidence level (CLs) as\n$$\nCL_{s}(\\mu) \\equiv \\frac{p_{\\mu}}{p_{b}},\n$$\nwhere $ p_{\\mu} = \\Pr(q_{\\mu} \\ge q_{\\mu,\\text{obs}} \\mid \\mu \\text{ is true}) $ and $ p_{b} = \\Pr(q_{\\mu} \\ge q_{\\mu,\\text{obs}} \\mid \\mu'=0) $ are tail probabilities of the same test statistic $ q_{\\mu} $ under the signal and background ($ \\mu'=0 $) hypotheses, respectively.\n\nFor the median expected upper limit construction, set the observed test statistic equal to its Asimov value under the background-only hypothesis (the Asimov dataset is defined as the idealized dataset equal to the model expectation, which yields $ \\hat{\\mu}_{A} = \\mu' $ and $ q_{\\mu,A} = -2 \\ln \\lambda_{A}(\\mu) $ deterministically). Denote by $ \\sigma $ the Asimov standard deviation of $ \\hat{\\mu} $ evaluated at $ \\mu'=0 $.\n\nUsing only the assumptions above, derive and solve the equation $ CL_{s}(\\mu) = \\alpha $ for the median expected upper limit $ \\mu_{\\text{up}} $ in closed form, expressing your final answer in terms of $ \\sigma $, $ \\alpha $, and the standard normal cumulative distribution function $ \\Phi(\\cdot) $ and its functional inverse $ \\Phi^{-1}(\\cdot) $. No numerical evaluation is required. Provide your final answer as a single analytic expression. Since $ \\mu $ is dimensionless, no physical units are needed.", "solution": "The problem statement is first subjected to a validation process.\n\n### Step 1: Extract Givens\n- A single-parameter signal-strength hypothesis $ \\mu \\ge 0 $.\n- The profile likelihood ratio test statistic is $ q_{\\mu} \\equiv -2 \\ln \\lambda(\\mu) $, from a likelihood $ L(\\mu,\\boldsymbol{\\theta}) $.\n- In the large-sample regime, the Wald expansion for the one-sided test holds:\n  - $ q_{\\mu} \\approx \\frac{(\\mu - \\hat{\\mu})^{2}}{\\sigma^{2}} $ if $ \\hat{\\mu} \\le \\mu $.\n  - $ q_{\\mu} = 0 $ if $ \\hat{\\mu}  \\mu $.\n- $ \\hat{\\mu} $ is the maximum-likelihood estimator of $ \\mu $.\n- $ \\hat{\\mu} \\sim \\mathcal{N}(\\mu', \\sigma^{2}) $ is the distribution of the estimator under a true signal strength $ \\mu' $.\n- The modified confidence level (CLs) is defined as $CL_{s}(\\mu) \\equiv \\frac{p_{\\mu}}{p_{b}}$.\n- $ p_{\\mu} = \\Pr(q_{\\mu} \\ge q_{\\mu,\\text{obs}} \\mid \\mu \\text{ is true}) $, which implies the true signal strength is $ \\mu' = \\mu $.\n- $ p_{b} = \\Pr(q_{\\mu} \\ge q_{\\mu,\\text{obs}} \\mid \\mu'=0) $.\n- For the median expected upper limit, the observed test statistic $ q_{\\mu,\\text{obs}} $ is set to its Asimov value under the background-only hypothesis ($ \\mu'=0 $).\n- The Asimov dataset is defined such that the estimator takes its expectation value, $ \\hat{\\mu}_{A} = \\mu' $.\n- $ \\sigma $ is the Asimov standard deviation of $ \\hat{\\mu} $ evaluated at $ \\mu'=0 $.\n- The task is to solve the equation $ CL_{s}(\\mu) = \\alpha $ for the median expected upper limit $ \\mu_{\\text{up}} $ in closed form.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem uses standard, well-established concepts and methods from high-energy physics statistics for setting upper limits on new physics signals. The profile likelihood ratio, the test statistic $ q_{\\mu} $, its asymptotic behavior, the CLs method, and the Asimov dataset are all foundational tools in this field. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The problem provides a complete set of definitions and assumptions. It specifies the goal clearly: to solve a particular equation for a specific variable. There are no ambiguities that would prevent a unique solution within the specified framework.\n- **Objective:** The language is formal, precise, and devoid of any subjective or speculative content.\n- **No Flaws:** The problem does not violate any of the specified invalidity criteria. It is not factually unsound, is directly relevant to the topic, is internally consistent and complete, is based on a standard theoretical approximation (Asimov), is well-structured, and is a non-trivial derivation.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of the Upper Limit\n\nThe objective is to find the value of $ \\mu $, denoted $ \\mu_{\\text{up}} $, that satisfies the equation $ CL_{s}(\\mu) = \\alpha $. This requires calculating the quantities $ p_{\\mu} $ and $ p_{b} $.\n\nFirst, we must determine the value of the \"observed\" test statistic, $ q_{\\mu, \\text{obs}} $. The problem specifies that for the median expected limit, the observation is taken to be the Asimov dataset generated under the background-only hypothesis ($ \\mu' = 0 $). By definition of the Asimov dataset, the estimator takes its expected value, so the \"observed\" best-fit signal strength is $ \\hat{\\mu}_{\\text{obs}} = \\mu' = 0 $.\n\nThe test statistic $ q_{\\mu} $ is being evaluated for a hypothetical signal strength $ \\mu  0 $. We use the Asimov observation $ \\hat{\\mu}_{\\text{obs}} = 0 $ in the formula for $ q_{\\mu} $. Since we are setting an upper limit, we are interested in $\\mu  0$, which ensures $ \\hat{\\mu}_{\\text{obs}} = 0 \\le \\mu $. This falls into the first case of the definition of $ q_{\\mu} $, yielding:\n$$\nq_{\\mu, \\text{obs}} = \\frac{(\\mu - \\hat{\\mu}_{\\text{obs}})^{2}}{\\sigma^{2}} = \\frac{(\\mu - 0)^{2}}{\\sigma^{2}} = \\frac{\\mu^{2}}{\\sigma^{2}}\n$$\nThis value of $ q_{\\mu, \\text{obs}} $ applies for the entire calculation of the limit on $ \\mu $.\n\nNext, we calculate the $ p $-value under the background-only hypothesis, $ p_{b} = \\Pr(q_{\\mu} \\ge q_{\\mu,\\text{obs}} \\mid \\mu'=0) $.\nUnder this hypothesis, the distribution of the estimator is $ \\hat{\\mu} \\sim \\mathcal{N}(0, \\sigma^{2}) $.\n$$\np_{b} = \\Pr\\left(q_{\\mu} \\ge \\frac{\\mu^{2}}{\\sigma^{2}} \\mid \\mu' = 0\\right)\n$$\nFor the inequality to hold with a non-zero right-hand side, $ q_{\\mu} $ must be non-zero. This requires that $ \\hat{\\mu} \\le \\mu $ and $ q_{\\mu} = \\frac{(\\mu - \\hat{\\mu})^{2}}{\\sigma^{2}} $. The condition we need to evaluate is:\n$$\n\\frac{(\\mu - \\hat{\\mu})^{2}}{\\sigma^{2}} \\ge \\frac{\\mu^{2}}{\\sigma^{2}} \\quad \\text{and} \\quad \\hat{\\mu} \\le \\mu\n$$\nThe first inequality simplifies to $ |\\mu - \\hat{\\mu}| \\ge |\\mu| $. Since we are seeking an upper limit, $ \\mu  0 $, this is $ |\\mu - \\hat{\\mu}| \\ge \\mu $. This gives two possibilities:\n$ 1.  \\mu - \\hat{\\mu} \\ge \\mu \\implies -\\hat{\\mu} \\ge 0 \\implies \\hat{\\mu} \\le 0 $.\n$ 2.  \\mu - \\hat{\\mu} \\le -\\mu \\implies 2\\mu \\le \\hat{\\mu} $.\nSo, the first inequality corresponds to the event $ (\\hat{\\mu} \\le 0) \\cup (\\hat{\\mu} \\ge 2\\mu) $. We must take the intersection of this with the condition $ \\hat{\\mu} \\le \\mu $.\n- The intersection of $ (\\hat{\\mu} \\le 0) $ and $ (\\hat{\\mu} \\le \\mu) $ is $ (\\hat{\\mu} \\le 0) $ because $ \\mu  0 $.\n- The intersection of $ (\\hat{\\mu} \\ge 2\\mu) $ and $ (\\hat{\\mu} \\le \\mu) $ is the empty set for $ \\mu  0 $.\nThus, the combined condition simplifies to just $ \\hat{\\mu} \\le 0 $.\nThe probability $ p_{b} $ is therefore:\n$$\np_{b} = \\Pr(\\hat{\\mu} \\le 0 \\mid \\mu'=0)\n$$\nSince $ \\hat{\\mu} \\sim \\mathcal{N}(0, \\sigma^{2}) $, we can standardize the variable. Let $ Z = \\hat{\\mu}/\\sigma \\sim \\mathcal{N}(0, 1) $.\n$$\np_{b} = \\Pr\\left(\\frac{\\hat{\\mu}}{\\sigma} \\le \\frac{0}{\\sigma}\\right) = \\Pr(Z \\le 0) = \\Phi(0) = \\frac{1}{2}\n$$\nwhere $ \\Phi(\\cdot) $ is the cumulative distribution function (CDF) of the standard normal distribution.\n\nNow, we calculate the $ p $-value under the signal-plus-background hypothesis, $ p_{\\mu} = \\Pr(q_{\\mu} \\ge q_{\\mu,\\text{obs}} \\mid \\mu'=\\mu) $.\nUnder this hypothesis, the distribution of the estimator is $ \\hat{\\mu} \\sim \\mathcal{N}(\\mu, \\sigma^{2}) $. The condition $ q_{\\mu} \\ge q_{\\mu, \\text{obs}} $ is identical to the one analyzed before, which simplifies to the event $ \\hat{\\mu} \\le 0 $.\nThe probability $ p_{\\mu} $ is thus:\n$$\np_{\\mu} = \\Pr(\\hat{\\mu} \\le 0 \\mid \\mu'=\\mu)\n$$\nStandardizing the variable $ \\hat{\\mu} $, which is now drawn from $ \\mathcal{N}(\\mu, \\sigma^{2}) $:\n$$\np_{\\mu} = \\Pr\\left(\\frac{\\hat{\\mu} - \\mu}{\\sigma} \\le \\frac{0 - \\mu}{\\sigma}\\right) = \\Pr\\left(Z \\le -\\frac{\\mu}{\\sigma}\\right) = \\Phi\\left(-\\frac{\\mu}{\\sigma}\\right)\n$$\nUsing the symmetry property of the normal CDF, $ \\Phi(-x) = 1 - \\Phi(x) $, this can also be written as $ p_{\\mu} = 1 - \\Phi(\\mu/\\sigma) $.\n\nFinally, we substitute $ p_{b} $ and $ p_{\\mu} $ into the $ CL_{s} $ equation to find the upper limit $ \\mu_{\\text{up}} $:\n$$\nCL_{s}(\\mu_{\\text{up}}) = \\frac{p_{\\mu}}{p_{b}} = \\alpha\n$$\n$$\n\\frac{\\Phi\\left(-\\frac{\\mu_{\\text{up}}}{\\sigma}\\right)}{\\frac{1}{2}} = \\alpha\n$$\n$$\n2 \\Phi\\left(-\\frac{\\mu_{\\text{up}}}{\\sigma}\\right) = \\alpha\n$$\n$$\n\\Phi\\left(-\\frac{\\mu_{\\text{up}}}{\\sigma}\\right) = \\frac{\\alpha}{2}\n$$\nTo solve for $ \\mu_{\\text{up}} $, we apply the inverse normal CDF, $ \\Phi^{-1}(\\cdot) $, to both sides of the equation:\n$$\n-\\frac{\\mu_{\\text{up}}}{\\sigma} = \\Phi^{-1}\\left(\\frac{\\alpha}{2}\\right)\n$$\n$$\n\\mu_{\\text{up}} = -\\sigma \\Phi^{-1}\\left(\\frac{\\alpha}{2}\\right)\n$$\nThis is a valid closed-form solution. A common convention is to express the result using the upper tail of the normal distribution. Using the identity $ \\Phi^{-1}(x) = -\\Phi^{-1}(1-x) $:\n$$\n\\mu_{\\text{up}} = -\\sigma \\left[ - \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right) \\right] = \\sigma \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right)\n$$\nBoth expressions are equivalent. We will provide the latter as the final answer.", "answer": "$$\n\\boxed{\\sigma \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right)}\n$$", "id": "3533347"}, {"introduction": "Our final practice is a true-to-life computational challenge: combining measurements from multiple, independent channels to enhance statistical power. This task requires us to move beyond single-channel statistics and construct a combined likelihood that properly accounts for correlated systematic uncertainties using a covariance matrix. By implementing a multi-channel combination and studying how the final limit depends on the assumed correlations [@problem_id:3533325], you will develop the practical skills needed to synthesize data and evaluate the robustness of a result in a realistic analysis setting.", "problem": "You are given a multi-channel combination problem for setting an upper limit on a non-negative signal strength parameter $\\,\\mu\\,$ in the context of computational high-energy physics. The combination involves $\\,3\\,$ statistically independent counting channels whose measurements are approximated as jointly Gaussian. Each channel $\\,i\\,$ provides a scalar observable $\\,y_i\\,$ modeled as $y_i = \\mu s_i + b_i + \\epsilon_i$, where $\\,s_i\\,$ is the expected signal yield per unit signal strength, $\\,b_i\\,$ is the nominal background expectation, and the vector of fluctuations $\\,\\boldsymbol{\\epsilon}\\,$ follows a multivariate normal distribution $\\,\\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\,$. The covariance $\\,\\boldsymbol{\\Sigma}\\,$ is the sum of a statistical component and a systematic component with block structure. Use the following foundational elements as the starting point for your derivation:\n\n- The likelihood for a multivariate normal model with mean $\\,\\boldsymbol{\\mu}_{y}(\\mu) = \\mu \\boldsymbol{s} + \\boldsymbol{b}\\,$ and covariance $\\,\\boldsymbol{\\Sigma}\\,$, and the corresponding maximum likelihood methodology.\n- Generalized least squares inference for linear models with known covariance.\n- Large-sample asymptotic behavior of the profile likelihood ratio (Wald approximation) for a single parameter of interest $\\,\\mu\\,$ with the physical constraint $\\,\\mu \\ge 0\\,$.\n- Confidence Level (CL) inversion for one-sided upper limits at confidence $\\,1 - \\alpha\\,$, with $\\,\\alpha = 0.05\\,$.\n\nDefine the $\\,3\\,$-channel inputs explicitly as follows:\n\n- Signal expectation vector $\\,\\boldsymbol{s} = (s_1, s_2, s_3) = (3.0, 2.0, 1.5)\\,$.\n- Background expectation vector $\\,\\boldsymbol{b} = (b_1, b_2, b_3) = (25.0, 18.0, 12.0)\\,$.\n- Statistical covariance (Poisson approximation) $\\,\\boldsymbol{\\Sigma}_{\\text{stat}} = \\mathrm{diag}(b_1, b_2, b_3)\\,$.\n- Systematic covariance $\\,\\boldsymbol{\\Sigma}_{\\text{sys}}\\,$ defined by a block structure:\n  - Channels $\\,1\\,$ and $\\,2\\,$ share a partially correlated nuisance with fractional scale $\\,f_{12} = 0.08\\,$ and correlation coefficient $\\,\\rho_{12} = 0.7\\,$. This contributes $\\,f_{12}^2 b_i^2\\,$ to the diagonal for $\\,i \\in \\{1,2\\}\\,$ and $\\,\\rho_{12} f_{12}^2 b_1 b_2\\,$ to the off-diagonal $\\,\\{1,2\\}\\,$ block.\n  - Channel $\\,3\\,$ has an independent nuisance with fractional scale $\\,f_3 = 0.10\\,$ contributing $\\,f_3^2 b_3^2\\,$ only to the $\\,\\{3,3\\}\\,$ entry.\n- The full covariance is $\\,\\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}_{\\text{stat}} + \\boldsymbol{\\Sigma}_{\\text{sys}}\\,$.\n\nCorrelation misspecification is modeled as off-diagonal inflation of the systematic covariance block between channels $\\,1\\,$ and $\\,2\\,$ only. For a scalar inflation factor $\\,\\kappa \\ge 0\\,$, the misspecified covariance is\n$$\n\\boldsymbol{\\Sigma}(\\kappa) = \\boldsymbol{\\Sigma}_{\\text{stat}} + \\boldsymbol{\\Sigma}_{\\text{sys}}(\\kappa),\n\\quad\n\\boldsymbol{\\Sigma}_{\\text{sys}}(\\kappa)_{12} = \\boldsymbol{\\Sigma}_{\\text{sys}}(\\kappa)_{21} = \\kappa \\,\\rho_{12} f_{12}^2 b_1 b_2,\n$$\nwith all diagonal entries unchanged relative to $\\,\\boldsymbol{\\Sigma}_{\\text{sys}}\\,$ and all other off-diagonal entries equal to $\\,0\\,$.\n\nAssume the observed data vector equals the nominal background expectation (Asimov dataset), that is $\\,\\boldsymbol{y} = \\boldsymbol{b}\\,$. Starting from the above foundational elements, derive from first principles:\n\n- The estimator of $\\,\\mu\\,$ obtained by maximizing the Gaussian likelihood (do not assume any pre-supplied formula).\n- The sampling variance of the estimator under the model with known $\\,\\boldsymbol{\\Sigma}(\\kappa)\\,$.\n- The one-sided $\\,95\\,$ Confidence Level (CL) upper limit on $\\,\\mu\\,$ obtained by asymptotic inversion of the corresponding one-sided Wald test with the physical constraint $\\,\\mu \\ge 0\\,$.\n\nYou must implement these derivations in a complete program that:\n\n- Constructs $\\,\\boldsymbol{\\Sigma}(\\kappa)\\,$ for each provided test value of $\\,\\kappa\\,$.\n- Computes the combined upper limit on $\\,\\mu\\,$ at $\\,95\\,$ CL using the derived expressions.\n- Uses $\\,\\alpha = 0.05\\,$ for the confidence level.\n- Treats all quantities as dimensionless; no physical units are required.\n- Outputs the results as decimal floats rounded to $\\,6\\,$ digits after the decimal point.\n\nTest Suite:\n\n- Case $\\,1\\,$ (happy path): $\\,\\kappa = 1.0\\,$.\n- Case $\\,2\\,$ (deflation): $\\,\\kappa = 0.5\\,$.\n- Case $\\,3\\,$ (no correlation): $\\,\\kappa = 0.0\\,$.\n- Case $\\,4\\,$ (inflation near a strong-correlation edge but remaining positive definite): $\\,\\kappa = 1.3\\,$.\n\nFinal Output Format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $\\,\\texttt{[result1,result2,result3,result4]}\\,$), in the order of the test cases listed above.", "solution": "The problem requires the derivation and calculation of a one-sided $95\\%$ confidence level (CL) upper limit on a signal strength parameter $\\mu$ from a combination of three measurement channels. The derivation must start from first principles, namely the multivariate Gaussian likelihood, generalized least squares, and the asymptotic properties of the likelihood estimator (the Wald approximation).\n\nLet the vector of measurements be $\\boldsymbol{y} = (y_1, y_2, y_3)^T$. The statistical model is given by\n$$\n\\boldsymbol{y} = \\mu \\boldsymbol{s} + \\boldsymbol{b} + \\boldsymbol{\\epsilon},\n$$\nwhere $\\boldsymbol{s}$ is the signal expectation vector, $\\boldsymbol{b}$ is the background expectation vector, and $\\boldsymbol{\\epsilon}$ is a vector of random fluctuations. The fluctuations are modeled as a multivariate normal distribution with a mean of zero and a covariance matrix $\\boldsymbol{\\Sigma}$, i.e., $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})$. Consequently, the distribution of the observable $\\boldsymbol{y}$ is $\\boldsymbol{y} \\sim \\mathcal{N}(\\mu \\boldsymbol{s} + \\boldsymbol{b}, \\boldsymbol{\\Sigma})$.\n\n### Part 1: Derivation of the Maximum Likelihood Estimator of $\\mu$\n\nThe likelihood function $L(\\mu)$ is the probability density function of the multivariate normal distribution evaluated at the observed data $\\boldsymbol{y}$, viewed as a function of the parameter $\\mu$:\n$$\nL(\\mu | \\boldsymbol{y}) = \\frac{1}{(2\\pi)^{3/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\boldsymbol{y} - (\\mu \\boldsymbol{s} + \\boldsymbol{b}))^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{y} - (\\mu \\boldsymbol{s} + \\boldsymbol{b}))\\right).\n$$\nTo find the maximum likelihood estimator (MLE) $\\hat{\\mu}$, we maximize $L(\\mu)$. It is equivalent and more convenient to maximize the log-likelihood, $\\ln L(\\mu)$:\n$$\n\\ln L(\\mu) = C - \\frac{1}{2} (\\boldsymbol{y} - \\boldsymbol{b} - \\mu \\boldsymbol{s})^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{y} - \\boldsymbol{b} - \\mu \\boldsymbol{s}),\n$$\nwhere $C = -\\frac{3}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}|$ is a constant with respect to $\\mu$. Maximizing $\\ln L(\\mu)$ is equivalent to minimizing the quadratic term, which is the generalized chi-squared function $\\chi^2(\\mu)$:\n$$\n\\chi^2(\\mu) = (\\boldsymbol{d} - \\mu\\boldsymbol{s})^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{d} - \\mu\\boldsymbol{s}),\n$$\nwhere we define $\\boldsymbol{d} = \\boldsymbol{y} - \\boldsymbol{b}$. Expanding this quadratic form in $\\mu$:\n$$\n\\chi^2(\\mu) = \\boldsymbol{d}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{d} - 2\\mu \\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{d} + \\mu^2 \\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s}.\n$$\nTo find the value of $\\mu$ that minimizes $\\chi^2(\\mu)$, we compute the derivative with respect to $\\mu$ and set it to zero:\n$$\n\\frac{d\\chi^2}{d\\mu} = -2\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{d} + 2\\mu \\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s} = 0.\n$$\nSolving for $\\mu$ gives the unconstrained MLE, $\\hat{\\mu}$:\n$$\n\\hat{\\mu} = \\frac{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{d}}{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s}} = \\frac{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{y} - \\boldsymbol{b})}{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s}}.\n$$\nThis is the well-known generalized least squares (GLS) estimator for the linear model $\\boldsymbol{d} = \\mu \\boldsymbol{s} + \\boldsymbol{\\epsilon}$.\n\n### Part 2: Derivation of the Sampling Variance of $\\hat{\\mu}$\n\nThe estimator $\\hat{\\mu}$ is a linear function of the random variable $\\boldsymbol{y}$. Its sampling variance, $\\mathrm{Var}(\\hat{\\mu}) = \\sigma_{\\hat{\\mu}}^2$, can be calculated using the properties of linear transformations of random vectors. We note that $\\boldsymbol{s}$, $\\boldsymbol{b}$, and $\\boldsymbol{\\Sigma}$ are constants.\n$$\n\\hat{\\mu} = \\frac{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1}}{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s}} \\boldsymbol{y} - \\frac{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{b}}{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s}}.\n$$\nThe variance of $\\hat{\\mu}$ depends only on the term involving $\\boldsymbol{y}$:\n$$\n\\sigma_{\\hat{\\mu}}^2 = \\mathrm{Var}\\left(\\frac{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1}}{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s}} \\boldsymbol{y}\\right).\n$$\nUsing the rule $\\mathrm{Var}(\\boldsymbol{A}\\boldsymbol{y}) = \\boldsymbol{A} \\mathrm{Cov}(\\boldsymbol{y}) \\boldsymbol{A}^T$ with $\\boldsymbol{A} = \\frac{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1}}{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s}}$ and $\\mathrm{Cov}(\\boldsymbol{y}) = \\boldsymbol{\\Sigma}$:\n$$\n\\sigma_{\\hat{\\mu}}^2 = \\left(\\frac{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1}}{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s}}\\right) \\boldsymbol{\\Sigma} \\left(\\frac{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1}}{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s}}\\right)^T = \\frac{1}{(\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s})^2} (\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1}) \\boldsymbol{\\Sigma} (\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{s}).\n$$\nSince $(\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1})^T = \\boldsymbol{\\Sigma}^{-T}\\boldsymbol{s} = \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{s}$ because $\\boldsymbol{\\Sigma}$ is symmetric. Simplifying the expression:\n$$\n\\sigma_{\\hat{\\mu}}^2 = \\frac{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s}}{(\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s})^2} = \\frac{1}{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s}}.\n$$\nThus, the standard deviation of the estimator is $\\sigma_{\\hat{\\mu}} = (\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s})^{-1/2}$.\n\n### Part 3: Derivation of the One-Sided Upper Limit\n\nWe use the Wald approximation, which states that for large samples, the MLE $\\hat{\\mu}$ is approximately normally distributed around the true value $\\mu_{\\text{true}}$ with variance $\\sigma_{\\hat{\\mu}}^2$: $\\hat{\\mu} \\sim \\mathcal{N}(\\mu_{\\text{true}}, \\sigma_{\\hat{\\mu}}^2)$.\n\nTo find the one-sided upper limit $\\mu_{\\text{up}}$ at a confidence level of $1-\\alpha$, we employ the standard method of inverting a hypothesis test. We find the value of $\\mu_{\\text{up}}$ such that if it were the true signal strength, the probability of observing a result at least as compatible with the background-only hypothesis as the one observed ($\\hat{\\mu}_{\\text{obs}}$) is equal to $\\alpha$. Compatibility with background-only means smaller (or more negative) values of the estimated signal strength.\nThe p-value for a hypothesized signal strength $\\mu$ is therefore $p_{\\mu} = P(\\hat{\\mu} \\le \\hat{\\mu}_{\\text{obs}} | \\mu_{\\text{true}}=\\mu)$.\n$$\np_{\\mu} = P\\left(\\frac{\\hat{\\mu} - \\mu}{\\sigma_{\\hat{\\mu}}} \\le \\frac{\\hat{\\mu}_{\\text{obs}} - \\mu}{\\sigma_{\\hat{\\mu}}}\\right) = \\Phi\\left(\\frac{\\hat{\\mu}_{\\text{obs}} - \\mu}{\\sigma_{\\hat{\\mu}}}\\right),\n$$\nwhere $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0, 1)$. The upper limit $\\mu_{\\text{up}}$ is the value of $\\mu$ for which $p_{\\mu} = \\alpha$:\n$$\n\\Phi\\left(\\frac{\\hat{\\mu}_{\\text{obs}} - \\mu_{\\text{up}}}{\\sigma_{\\hat{\\mu}}}\\right) = \\alpha.\n$$\nLet $Z_{\\alpha} = \\Phi^{-1}(\\alpha)$ be the $\\alpha$-quantile of the standard normal distribution. Then:\n$$\n\\frac{\\hat{\\mu}_{\\text{obs}} - \\mu_{\\text{up}}}{\\sigma_{\\hat{\\mu}}} = Z_{\\alpha}.\n$$\nSolving for $\\mu_{\\text{up}}$:\n$$\n\\mu_{\\text{up}} = \\hat{\\mu}_{\\text{obs}} - Z_{\\alpha} \\sigma_{\\hat{\\mu}}.\n$$\nUsing the identity $Z_{\\alpha} = -Z_{1-\\alpha}$, the expression becomes:\n$$\n\\mu_{\\text{up}} = \\hat{\\mu}_{\\text{obs}} + Z_{1-\\alpha} \\sigma_{\\hat{\\mu}}.\n$$\nThe problem specifies the use of an Asimov dataset, where the observed data equals the nominal background expectation, $\\boldsymbol{y} = \\boldsymbol{b}$. For this dataset, $\\boldsymbol{d} = \\boldsymbol{y} - \\boldsymbol{b} = \\boldsymbol{0}$. The observed estimator is:\n$$\n\\hat{\\mu}_{\\text{obs}} = \\frac{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{0}}{\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s}} = 0.\n$$\nThis value satisfies the physical constraint $\\mu \\ge 0$. The formula for the upper limit simplifies to:\n$$\n\\mu_{\\text{up}} = Z_{1-\\alpha} \\sigma_{\\hat{\\mu}} = Z_{1-\\alpha} (\\boldsymbol{s}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{s})^{-1/2}.\n$$\nFor a $95\\%$ CL, $\\alpha = 0.05$, and $Z_{0.95} \\approx 1.64485$.\n\n### Part 4: Application to the Specific Problem\n\nThe final step is to construct the covariance matrix $\\boldsymbol{\\Sigma}(\\kappa)$ for each test case and apply the derived formula.\nThe total covariance is $\\boldsymbol{\\Sigma}(\\kappa) = \\boldsymbol{\\Sigma}_{\\text{stat}} + \\boldsymbol{\\Sigma}_{\\text{sys}}(\\kappa)$.\n- Signal vector: $\\boldsymbol{s} = (3.0, 2.0, 1.5)^T$.\n- Background vector: $\\boldsymbol{b} = (25.0, 18.0, 12.0)^T$.\n- Statistical covariance: $\\boldsymbol{\\Sigma}_{\\text{stat}} = \\mathrm{diag}(b_1, b_2, b_3) = \\mathrm{diag}(25, 18, 12)$.\n- Systematic covariance $\\boldsymbol{\\Sigma}_{\\text{sys}}(\\kappa)$:\n  - $(\\boldsymbol{\\Sigma}_{\\text{sys}}(\\kappa))_{11} = (f_{12} b_1)^2 = (0.08 \\times 25.0)^2 = 4.0$.\n  - $(\\boldsymbol{\\Sigma}_{\\text{sys}}(\\kappa))_{22} = (f_{12} b_2)^2 = (0.08 \\times 18.0)^2 = 2.0736$.\n  - $(\\boldsymbol{\\Sigma}_{\\text{sys}}(\\kappa))_{33} = (f_3 b_3)^2 = (0.10 \\times 12.0)^2 = 1.44$.\n  - The off-diagonal term affected by $\\kappa$ is:\n    $(\\boldsymbol{\\Sigma}_{\\text{sys}}(\\kappa))_{12} = (\\boldsymbol{\\Sigma}_{\\text{sys}}(\\kappa))_{21} = \\kappa \\rho_{12} f_{12}^2 b_1 b_2 = \\kappa \\times 0.7 \\times 0.08^2 \\times 25.0 \\times 18.0 = 2.016 \\kappa$.\n  - All other off-diagonal entries are $0$.\n\nThe full covariance matrix for a given $\\kappa$ is:\n$$\n\\boldsymbol{\\Sigma}(\\kappa) = \\begin{pmatrix} 25  0  0 \\\\ 0  18  0 \\\\ 0  0  12 \\end{pmatrix} + \\begin{pmatrix} 4.0  2.016\\kappa  0 \\\\ 2.016\\kappa  2.0736  0 \\\\ 0  0  1.44 \\end{pmatrix} = \\begin{pmatrix} 29.0  2.016\\kappa  0 \\\\ 2.016\\kappa  20.0736  0 \\\\ 0  0  13.44 \\end{pmatrix}.\n$$\nFor each $\\kappa$ in the test suite $\\{1.0, 0.5, 0.0, 1.3\\}$, we will compute this matrix, invert it, calculate $\\boldsymbol{s}^T \\boldsymbol{\\Sigma}(\\kappa)^{-1} \\boldsymbol{s}$, and finally compute $\\mu_{\\text{up}}(\\kappa)$.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Calculates the 95% CL upper limit on the signal strength parameter mu\n    for a 3-channel combination problem with varying correlation misspecification.\n    \"\"\"\n    \n    # Define problem inputs\n    s_vec = np.array([3.0, 2.0, 1.5])\n    b_vec = np.array([25.0, 18.0, 12.0])\n    \n    f12 = 0.08\n    rho12 = 0.7\n    f3 = 0.10\n    alpha = 0.05\n    \n    # Test cases for the correlation inflation factor kappa\n    test_cases = [1.0, 0.5, 0.0, 1.3]\n    \n    results = []\n    \n    # Calculate the quantile of the standard normal distribution\n    # for a one-sided 95% confidence interval\n    z_val = norm.ppf(1 - alpha)\n    \n    # Calculate the statistical covariance matrix (diagonal)\n    # This component is independent of kappa\n    sigma_stat = np.diag(b_vec)\n    \n    for kappa in test_cases:\n        # Step 1: Construct the covariance matrix Sigma(kappa)\n        \n        # Initialize the systematic covariance matrix\n        sigma_sys_k = np.zeros((3, 3))\n        \n        # Calculate diagonal elements of the systematic covariance matrix\n        sigma_sys_k[0, 0] = (f12 * b_vec[0])**2\n        sigma_sys_k[1, 1] = (f12 * b_vec[1])**2\n        sigma_sys_k[2, 2] = (f3 * b_vec[2])**2\n        \n        # Calculate the off-diagonal element for channels 1 and 2\n        off_diag = kappa * rho12 * (f12**2) * b_vec[0] * b_vec[1]\n        sigma_sys_k[0, 1] = off_diag\n        sigma_sys_k[1, 0] = off_diag\n        \n        # The full covariance is the sum of statistical and systematic components\n        sigma_k = sigma_stat + sigma_sys_k\n        \n        # Step 2: Calculate the standard deviation of the mu estimator\n        \n        # Invert the covariance matrix\n        sigma_inv_k = np.linalg.inv(sigma_k)\n        \n        # Calculate the Fisher information for mu (s^T * Sigma^-1 * s)\n        fisher_info_mu = s_vec.T @ sigma_inv_k @ s_vec\n        \n        # The standard deviation of the mu estimator is sqrt(1 / Fisher_info)\n        sigma_mu = np.sqrt(1 / fisher_info_mu)\n        \n        # Step 3: Calculate the 95% CL upper limit\n        \n        # For the Asimov dataset (y=b), the best-fit mu_hat is 0.\n        # The upper limit mu_up = mu_hat + Z * sigma_mu = 0 + Z * sigma_mu\n        mu_up = z_val * sigma_mu\n        \n        results.append(mu_up)\n        \n    # Format the final output as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "3533325"}]}