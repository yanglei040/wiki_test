{"hands_on_practices": [{"introduction": "This practice explores the fundamental trade-off between the statistical purity of an unbinned likelihood and the computational convenience of a binned one. We will use the concept of Fisher information, which sets the ultimate limit on the precision of a parameter estimate, to quantitatively measure the information lost when data is binned. This exercise [@problem_id:3540423] provides a first-principles understanding of how binning choices directly impact statistical power, a crucial consideration in the design of any physics analysis.", "problem": "You are studying the information-theoretic efficiency of unbinned versus binned maximum likelihood fits for an exponential lifetime model within a finite observation window. Consider independent and identically distributed samples drawn from the exponential probability density function defined on the nonnegative line, $f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$ and parameter $\\lambda > 0$, but suppose the data acquisition is limited to a finite window $[0, T]$. Condition on observing exactly $N$ events within this window. For the unbinned case, the likelihood is the product of the truncated density on $[0, T]$. For the binned case, partition the window $[0, T]$ into $m$ equal-width bins and use the multinomial likelihood for the bin counts conditioned on $N$. Using the foundational definitions below, derive analytical expressions and evaluate them numerically.\n\nFundamental definitions:\n- The log-likelihood for independent observations $x_1, \\dots, x_N$ with density $f(x \\mid \\lambda)$ is $\\ell(\\lambda) = \\sum_{i=1}^N \\log f(x_i \\mid \\lambda)$.\n- The Fisher information for parameter $\\lambda$ from $N$ independent samples is $I(\\lambda) = - \\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right]$, where the expectation is with respect to the true data-generating distribution.\n- For a multinomial model with probabilities $p_k(\\lambda)$ that depend on $\\lambda$ and total count $N$, the Fisher information for $\\lambda$ is $I(\\lambda) = N \\sum_{k=1}^m \\frac{1}{p_k(\\lambda)} \\left(\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}\\right)^2$, provided $p_k(\\lambda) > 0$ for all $k$.\n\nTasks:\n1. Unbinned information under truncation. Given the observation window $[0, T]$, treat the data as drawn from the truncated exponential on $[0, T]$, that is, the conditional density $f_T(x \\mid \\lambda) = \\frac{\\lambda e^{-\\lambda x}}{1 - e^{-\\lambda T}}$ for $0 \\le x \\le T$. Starting from the Fisher information definition, derive the per-event Fisher information for $\\lambda$ under this truncation, and hence obtain the total Fisher information for $N$ observations in $[0, T]$.\n2. Binned information with equal-width bins. Partition $[0, T]$ into $m$ equal-width bins with edges $0, \\Delta, 2\\Delta, \\dots, T$, where $\\Delta = T/m$. Let $p_k(\\lambda)$ denote the bin probability under $f_T(x \\mid \\lambda)$ for bin $k \\in \\{1, \\dots, m\\}$. Starting from the multinomial Fisher information definition above, derive an expression for the total Fisher information for $\\lambda$ in terms of $p_k(\\lambda)$ and $\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}$.\n3. Optimization of $m$ subject to a minimal expected occupancy constraint. For numerical stability of binned fits in practice, enforce a constraint that the expected count in every bin must be at least a positive threshold $c_{\\min}$. With $N$ total events, this means $N \\cdot p_k(\\lambda) \\ge c_{\\min}$ for all $k \\in \\{1, \\dots, m\\}$. Among all integers $m \\in \\{1, 2, \\dots, N\\}$ satisfying this constraint, choose $m^\\star$ that maximizes the binned Fisher information (equivalently, minimizes information loss relative to the unbinned case). If multiple $m$ achieve the same maximum to within floating-point tolerance, select the largest such $m$.\n4. Implement a program that, for each test case specified below, computes:\n   - The total unbinned Fisher information $I_{\\text{unb}}(\\lambda, T, N)$.\n   - The total binned Fisher information $I_{\\text{bin}}(\\lambda, T, N, m^\\star)$ using the optimized $m^\\star$ under the occupancy constraint.\n   - The ratio $r = I_{\\text{bin}} / I_{\\text{unb}}$.\n   - The optimized bin count $m^\\star$.\n   Report floating-point results rounded to six decimal places.\n\nAssumptions and clarifications:\n- All quantities are dimensionless; no physical units are used.\n- Angles are not involved.\n- All probabilities must sum to one, and all $p_k(\\lambda)$ must be strictly positive for the Fisher information to be finite.\n- Use the conditional-on-$N$ framework consistently for both unbinned and binned cases.\n\nTest suite:\nCompute the outputs for the following parameter sets $(\\lambda, T, N, c_{\\min})$:\n- Case A: $(0.5, 6.0, 1000, 5)$\n- Case B: $(0.2, 20.0, 500, 10)$\n- Case C: $(1.0, 3.0, 30, 5)$\n\nFinal output format:\nYour program should produce a single line containing the results as a comma-separated list of lists, one list per test case, in the order [Case A, Case B, Case C]. Each inner list must be of the form $[m^\\star, I_{\\text{unb}}, I_{\\text{bin}}, r]$, where $m^\\star$ is an integer and the other entries are floats rounded to six decimal places. For example: [[mA, IunbA, IbinA, rA],[mB, IunbB, IbinB, rB],[mC, IunbC, IbinC, rC]].", "solution": "The problem is subjected to validation against the specified criteria.\n\n### Step 1: Extract Givens\n- **Probability Density Function (PDF)**: Exponential distribution on the non-negative line, $f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$ and $\\lambda > 0$.\n- **Observation Window**: Data acquisition is limited to the interval $[0, T]$.\n- **Truncated Conditional Density**: For $N$ events observed in $[0, T]$, the density is $f_T(x \\mid \\lambda) = \\frac{\\lambda e^{-\\lambda x}}{1 - e^{-\\lambda T}}$ for $0 \\le x \\le T$.\n- **Log-Likelihood Definition**: For i.i.d. observations $x_1, \\dots, x_N$, $\\ell(\\lambda) = \\sum_{i=1}^N \\log f(x_i \\mid \\lambda)$.\n- **Unbinned Fisher Information Definition**: $I(\\lambda) = - \\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right]$.\n- **Binned Model**: The window $[0, T]$ is partitioned into $m$ equal-width bins. Bin width is $\\Delta = T/m$.\n- **Multinomial Binned Fisher Information Definition**: For total count $N$ and bin probabilities $p_k(\\lambda)$, $I(\\lambda) = N \\sum_{k=1}^m \\frac{1}{p_k(\\lambda)} \\left(\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}\\right)^2$, for $p_k(\\lambda) > 0$.\n- **Optimization Constraint**: For a chosen number of bins $m$, the expected count in every bin must be at least $c_{\\min}$, i.e., $N \\cdot p_k(\\lambda) \\ge c_{\\min}$ for all $k \\in \\{1, \\dots, m\\}$.\n- **Optimization Objective**: Find $m^\\star \\in \\{1, 2, \\dots, N\\}$ that satisfies the constraint and maximizes the binned Fisher information. If there is a tie, select the largest such $m$.\n- **Test Cases**:\n    - Case A: $(\\lambda, T, N, c_{\\min}) = (0.5, 6.0, 1000, 5)$\n    - Case B: $(\\lambda, T, N, c_{\\min}) = (0.2, 20.0, 500, 10)$\n    - Case C: $(\\lambda, T, N, c_{\\min}) = (1.0, 3.0, 30, 5)$\n- **Output Requirements**: For each test case, compute $[m^\\star, I_{\\text{unb}}, I_{\\text{bin}}, r]$, with $I_{\\text{unb}}$, $I_{\\text{bin}}$, and $r = I_{\\text{bin}} / I_{\\text{unb}}$ rounded to six decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity:\n- **Scientifically Grounded**: The problem is based on fundamental principles of statistical inference, namely maximum likelihood estimation and Fisher information theory, applied to the exponential distribution. This is a standard and well-understood topic in computational physics and statistics. The setup is scientifically sound.\n- **Well-Posed**: The problem specifies all necessary definitions, constraints, and objectives. The derivation tasks are clearly stated. The optimization task for $m^\\star$ has a clear objective function (maximizing $I_{\\text{bin}}$), a well-defined constraint set, and a tie-breaking rule, ensuring a unique solution exists.\n- **Objective**: The problem is stated using precise mathematical language, free from subjective or ambiguous terminology.\n- **Other Flaws**: The problem does not violate any of the invalidity criteria. It is self-contained, logically consistent, and computationally feasible. The mathematical framework is standard. The provided test cases are reasonable and do not introduce contradictions.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution is provided below.\n\nThe solution proceeds by deriving the analytical expressions for the required quantities and then implementing them numerically.\n\n**1. Unbinned Fisher Information, $I_{\\text{unb}}$**\n\nThe log-likelihood for a single observation $x$ from the truncated density $f_T(x \\mid \\lambda) = \\frac{\\lambda e^{-\\lambda x}}{1 - e^{-\\lambda T}}$ is:\n$$ \\ell_1(\\lambda) = \\log(f_T(x \\mid \\lambda)) = \\log(\\lambda) - \\lambda x - \\log(1 - e^{-\\lambda T}) $$\nThe first derivative with respect to $\\lambda$ (the score function) is:\n$$ \\frac{\\partial \\ell_1(\\lambda)}{\\partial \\lambda} = \\frac{1}{\\lambda} - x - \\frac{1}{1 - e^{-\\lambda T}} \\cdot (-e^{-\\lambda T}) \\cdot (-T) = \\frac{1}{\\lambda} - x - \\frac{T e^{-\\lambda T}}{1 - e^{-\\lambda T}} $$\nThe second derivative is:\n$$ \\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2} = -\\frac{1}{\\lambda^2} - \\frac{\\partial}{\\partial \\lambda}\\left(\\frac{T e^{-\\lambda T}}{1 - e^{-\\lambda T}}\\right) $$\nUsing the quotient rule, the derivative of the second term is $\\frac{-T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2}$. Thus:\n$$ \\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2} = -\\frac{1}{\\lambda^2} + \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2} $$\nThis expression is independent of the random variable $x$. Therefore, the expectation $\\mathbb{E}\\left[\\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2}\\right]$ is the expression itself. The Fisher information for a single event is:\n$$ I_1(\\lambda) = -\\mathbb{E}\\left[\\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2}\\right] = - \\left(-\\frac{1}{\\lambda^2} + \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2}\\right) = \\frac{1}{\\lambda^2} - \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2} $$\nFor $N$ independent and identically distributed observations, the total Fisher information is additive:\n$$ I_{\\text{unb}}(\\lambda, T, N) = N \\cdot I_1(\\lambda) = N \\left(\\frac{1}{\\lambda^2} - \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2}\\right) $$\nThis expression is non-negative, as can be shown by the inequality $\\sinh(v) \\ge v$ for $v \\ge 0$.\n\n**2. Binned Fisher Information, $I_{\\text{bin}}$**\n\nFor $m$ bins of equal width $\\Delta = T/m$, the probability of an event falling into bin $k \\in \\{1, \\dots, m\\}$, with edges $[(k-1)\\Delta, k\\Delta]$, is given by the integral of the truncated density:\n$$ p_k(\\lambda) = \\int_{(k-1)\\Delta}^{k\\Delta} f_T(x \\mid \\lambda) dx = \\frac{1}{1 - e^{-\\lambda T}} \\int_{(k-1)\\Delta}^{k\\Delta} \\lambda e^{-\\lambda x} dx $$\n$$ p_k(\\lambda) = \\frac{[-e^{-\\lambda x}]_{(k-1)\\Delta}^{k\\Delta}}{1 - e^{-\\lambda T}} = \\frac{e^{-\\lambda(k-1)\\Delta} - e^{-\\lambda k\\Delta}}{1 - e^{-\\lambda T}} $$\nFor numerical stability, this can be rewritten using the `expm1` function, where $\\text{expm1}(z) = e^z - 1$:\n$$ p_k(\\lambda) = \\frac{e^{-\\lambda k\\Delta}(e^{\\lambda\\Delta} - 1)}{e^{-\\lambda T}(e^{\\lambda T} - 1)} = e^{\\lambda(T-k\\Delta)} \\frac{\\text{expm1}(\\lambda\\Delta)}{\\text{expm1}(\\lambda T)} $$\nThe Fisher information for the binned model is $I_{\\text{bin}}(\\lambda) = N \\sum_{k=1}^m \\frac{1}{p_k(\\lambda)} \\left(\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}\\right)^2$. This can be rewritten as $N \\sum_{k=1}^m p_k(\\lambda) \\left(\\frac{\\partial \\log p_k(\\lambda)}{\\partial \\lambda}\\right)^2$. Taking the logarithm of the stable expression for $p_k(\\lambda)$:\n$$ \\log p_k(\\lambda) = \\lambda(T-k\\Delta) + \\log(\\text{expm1}(\\lambda\\Delta)) - \\log(\\text{expm1}(\\lambda T)) $$\nDifferentiating with respect to $\\lambda$:\n$$ \\frac{\\partial \\log p_k(\\lambda)}{\\partial \\lambda} = (T - k\\Delta) + \\frac{\\Delta e^{\\lambda\\Delta}}{\\text{expm1}(\\lambda\\Delta)} - \\frac{T e^{\\lambda T}}{\\text{expm1}(\\lambda T)} $$\nThis provides a numerically stable way to compute the terms in the sum for $I_{\\text{bin}}$.\n\n**3. Optimization of Bin Count, $m^\\star$**\n\nThe constraint is $N \\cdot p_k(\\lambda) \\ge c_{\\min}$ for all $k = 1, \\dots, m$. The bin probability $p_k(\\lambda)$ is a monotonically decreasing function of $k$. Thus, the constraint is most restrictive for the last bin, $k=m$. The condition simplifies to:\n$$ N \\cdot p_m(\\lambda) \\ge c_{\\min} $$\nUsing the stable expression for $p_k(\\lambda)$ with $k=m$ and $\\Delta=T/m$:\n$$ p_m(\\lambda) = \\frac{\\text{expm1}(\\lambda T/m)}{\\text{expm1}(\\lambda T)} $$\nThe Fisher information $I_{\\text{bin}}(m)$ is expected to be a monotonically increasing function of $m$ (as finer binning preserves more information), approaching $I_{\\text{unb}}$ as $m \\to \\infty$. Therefore, to maximize $I_{\\text{bin}}(m)$ subject to the constraint, we should choose the largest possible $m$ that satisfies it. The function $p_m(\\lambda)$ is a monotonically decreasing function of $m$. This means we can search for the largest integer $m \\in \\{1, \\dots, N\\}$ that satisfies $N \\cdot \\frac{\\text{expm1}(\\lambda T/m)}{\\text{expm1}(\\lambda T)} \\ge c_{\\min}$. A linear scan from $m=1$ upwards is efficient enough for the given problem constraints. The first value of $m$ to violate the constraint establishes the upper bound, and $m^\\star$ is the value immediately preceding it.\n\nThe implementation will follow these derived formulas.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all specified test cases.\n    It orchestrates the calculation for each case and formats the final output.\n    \"\"\"\n    test_cases = [\n        (0.5, 6.0, 1000, 5),    # Case A\n        (0.2, 20.0, 500, 10),   # Case B\n        (1.0, 3.0, 30, 5),      # Case C\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = compute_results_for_case(*case)\n        all_results.append(result)\n\n    # Format the final output string as specified\n    formatted_results = []\n    for res in all_results:\n        # res is [m_star, i_unb, i_bin, ratio]\n        s = f'[{res[0]},{res[1]:.6f},{res[2]:.6f},{res[3]:.6f}]'\n        formatted_results.append(s)\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef compute_results_for_case(lam, T, N, c_min):\n    \"\"\"\n    Computes all required quantities for a single test case.\n\n    Args:\n        lam (float): The lambda parameter of the exponential distribution.\n        T (float): The upper bound of the observation window.\n        N (int): The total number of events.\n        c_min (int): The minimum required expected occupancy per bin.\n\n    Returns:\n        list: A list containing [m_star, I_unb, I_bin, ratio].\n    \"\"\"\n    # 1. Compute the unbinned Fisher information\n    i_unb = calculate_i_unb(lam, T, N)\n\n    # 2. Find the optimal number of bins m_star\n    m_star = find_m_star(lam, T, N, c_min)\n\n    # 3. Compute the binned Fisher information for m_star\n    i_bin = 0.0\n    if m_star > 0:\n        i_bin = calculate_i_bin(lam, T, N, m_star)\n\n    # 4. Compute the ratio of binned to unbinned information\n    ratio = i_bin / i_unb if i_unb > 0 else 0.0\n\n    # 5. Return the results\n    return [m_star, i_unb, i_bin, ratio]\n\n\ndef calculate_i_unb(lam, T, N):\n    \"\"\"\n    Calculates the total unbinned Fisher information for N events.\n    Formula: N * (1/lambda^2 - (T^2 * exp(-lambda*T)) / (1 - exp(-lambda*T))^2)\n    \"\"\"\n    lam_T = lam * T\n    # The parameters ensure lam > 0, T > 0, so lam_T > 0.\n    # No risk of division by zero in the denominator for lam_T > 0.\n    exp_m_lam_T = np.exp(-lam_T)\n    one_minus_exp = 1.0 - exp_m_lam_T\n    \n    term2 = (T**2 * exp_m_lam_T) / (one_minus_exp**2)\n    i_one_event = (1.0 / lam**2) - term2\n    \n    return N * i_one_event\n\n\ndef find_m_star(lam, T, N, c_min):\n    \"\"\"\n    Finds the optimal number of bins m_star by finding the largest m\n    that satisfies the minimum occupancy constraint.\n    \"\"\"\n    m_star_found = 0\n    lam_T = lam * T\n\n    # Constraint: N * p_m >= c_min ==> expm1(lam*T/m) >= (c_min/N_tot) * expm1(lam*T)\n    # Target for comparison to avoid recomputing the RHS in the loop\n    if np.isclose(lam_T, 0): # Should not happen with given parameters\n        return 1 if N >= c_min else 0\n        \n    threshold = (c_min / N) * np.expm1(lam_T)\n\n    # Linearly scan for m from 1 to N.\n    # p_m is monotonically decreasing in m, so we can stop when the constraint fails.\n    for m in range(1, N + 1):\n        lam_T_over_m = lam_T / m\n        expm1_val = np.expm1(lam_T_over_m)\n        if expm1_val >= threshold:\n            m_star_found = m\n        else:\n            break\n            \n    return m_star_found\n\n\ndef calculate_i_bin(lam, T, N, m):\n    \"\"\"\n    Calculates the total binned Fisher information for a given number of bins m.\n    \"\"\"\n    delta = T / m\n    lam_T = lam * T\n    lam_delta = lam * delta\n\n    def h(x):\n        \"\"\"Helper function for x*exp(x)/(exp(x)-1), stable for x near 0.\"\"\"\n        if np.isclose(x, 0):\n            return 1.0\n        return (x * np.exp(x)) / np.expm1(x)\n\n    h_lam_T = h(lam_T)\n    h_lam_delta = h(lam_delta)\n    \n    if np.isclose(lam_T, 0):\n        return 0.0\n        \n    expm1_lam_T = np.expm1(lam_T)\n    expm1_lam_delta = np.expm1(lam_delta)\n    \n    common_pk_factor = expm1_lam_delta / expm1_lam_T\n    common_dpk_term = (h_lam_delta - h_lam_T) / lam\n\n    info_sum = 0.0\n    for k in range(1, m + 1):\n        T_minus_k_delta = T - k * delta\n        \n        # Calculate p_k = exp(lam*(T-k*delta)) * common_pk_factor\n        pk = np.exp(lam * T_minus_k_delta) * common_pk_factor\n        \n        # Calculate d(log p_k)/d(lam) = (T-k*delta) + common_dpk_term\n        dlogpk_dlam = T_minus_k_delta + common_dpk_term\n        \n        # Term in sum for info is p_k * (d(log p_k)/d(lam))^2\n        if pk > 0:\n            info_sum += pk * (dlogpk_dlam**2)\n    \n    return N * info_sum\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3540423"}, {"introduction": "While binned likelihoods are powerful, their theoretical guarantees can break down in practice, especially with sparse data. This hands-on simulation [@problem_id:3540404] explores such a scenario, focusing on a steeply falling spectrum where many bins may contain few or zero events. By comparing the empirical variance of a fitted parameter to its theoretical Cramér-Rao lower bound, you will quantify the \"variance inflation\" and gain crucial intuition about when to be skeptical of theoretically-derived uncertainties.", "problem": "You are tasked with investigating identifiability of a slope parameter in a sparse binned likelihood, starting from first principles and implementing a complete simulation and fitting study. Consider a steeply falling spectrum modeled by a normalized power-law probability density function (PDF) of the form $f(x \\mid \\alpha) \\propto x^{-\\alpha}$ on a bounded interval $x \\in [x_{\\min}, x_{\\max}]$, where $\\alpha$ is an unknown slope parameter. Divide the interval $[x_{\\min}, x_{\\max}]$ into $B$ disjoint bins with edges $\\{x_0, x_1, \\dots, x_B\\}$, where $x_0 = x_{\\min}$ and $x_B = x_{\\max}$ and $x_i < x_{i+1}$ for all $i$. Suppose the total expected number of events is known and equal to $\\mu$, with $\\mu > 0$.\n\nFundamental base:\n- Each bin $b \\in \\{1, \\dots, B\\}$ receives an observed count $n_b \\in \\{0,1,2,\\dots\\}$ modeled as a draw from an independent Poisson distribution with mean $\\lambda_b(\\alpha)$. The Poisson probability mass function (PMF) is $P(N=n) = e^{-\\lambda} \\lambda^n / n!$, and independent Poisson variables factorize.\n- The expected bin means are given by $\\lambda_b(\\alpha) = \\mu \\, p_b(\\alpha)$, where $p_b(\\alpha)$ is the model-predicted bin probability, i.e., $p_b(\\alpha) = \\int_{x_{b-1}}^{x_b} f(x \\mid \\alpha) \\, dx$, normalized so that $\\sum_{b=1}^B p_b(\\alpha) = 1$.\n- For the power-law model on $[x_{\\min}, x_{\\max}]$, the bin probabilities $p_b(\\alpha)$ are given by exact integrals of $x^{-\\alpha}$ over each bin, divided by the integral over the full range. For $\\alpha \\neq 1$, $\\int x^{-\\alpha}\\, dx = \\frac{x^{1-\\alpha}}{1-\\alpha}$. For $\\alpha = 1$, $\\int x^{-1}\\, dx = \\ln x$. Use these to compute normalized $p_b(\\alpha)$ from first principles.\n\nTasks:\n1. Derive the binned likelihood starting from independent Poisson assumptions and $\\lambda_b(\\alpha) = \\mu p_b(\\alpha)$. Show that when $\\mu$ is known, maximizing the likelihood in $\\alpha$ is equivalent to maximizing the multinomial form $\\sum_{b=1}^B n_b \\ln p_b(\\alpha)$ up to an additive constant independent of $\\alpha$. Your implementation must maximize this objective over $\\alpha$ on a specified bounded grid to obtain the Maximum Likelihood Estimation (MLE) estimate $\\hat{\\alpha}$.\n2. Using the definition of Fisher information, derive the expression for the total Fisher information for $\\alpha$ when $\\mu$ is known. Starting from the definition $I(\\alpha) = \\mathbb{E}\\left[-\\frac{\\partial^2}{\\partial \\alpha^2} \\ln L(\\alpha)\\right]$ with independent Poisson likelihoods, show that the Fisher information can be written in the standard \"per-event\" form $I(\\alpha)=\\mu\\, I_1(\\alpha)$, where $I_1(\\alpha)$ is the sum over bins of $p_b(\\alpha)\\left(\\frac{\\partial}{\\partial \\alpha}\\ln p_b(\\alpha)\\right)^2$. Your program must compute $I_1(\\alpha)$ numerically using a centered finite difference for $\\frac{\\partial}{\\partial \\alpha} \\ln p_b(\\alpha)$ with a small step $h$, and then compute the Cramér–Rao lower bound (CRLB) variance as $\\operatorname{Var}_{\\text{CRLB}}(\\hat{\\alpha}) = \\frac{1}{\\mu I_1(\\alpha)}$ evaluated at the true $\\alpha$.\n3. Implement a Monte Carlo simulation of $R$ pseudo-experiments by drawing $n_b \\sim \\text{Poisson}(\\mu p_b(\\alpha_{\\text{true}}))$ for each bin, independently across bins, for a specified true slope $\\alpha_{\\text{true}}$. For each pseudo-experiment, compute $\\hat{\\alpha}$ by maximizing $\\sum_{b=1}^B n_b \\ln p_b(\\alpha)$ on a fixed grid $\\{\\alpha_j\\}_{j=1}^{N_\\alpha}$, where the grid is linearly spaced on $[\\alpha_{\\min}, \\alpha_{\\max}]$. Use a strictly positive floor for probabilities (e.g., clipping $p_b(\\alpha)$ below a very small threshold before taking logarithms) to avoid numerical issues without altering the normalization condition, and keep the same $\\mu$ across all cases. If a fit lands on the boundary of the grid, keep it as is; do not discard or refit.\n4. Quantify the \"variance inflation\" as the empirical variance of $\\hat{\\alpha}$ over pseudo-experiments divided by the CRLB variance computed at the true $\\alpha$. That is, compute the ratio $\\frac{\\operatorname{Var}_{\\text{emp}}(\\hat{\\alpha})}{\\operatorname{Var}_{\\text{CRLB}}(\\hat{\\alpha})}$ for each test case.\n\nYour program must follow these constraints:\n- All computations must be self-contained and reproducible. Use a fixed random seed where requested.\n- All angles are not used; no angle unit is needed. No physical units are required; treat all quantities as dimensionless.\n- The final outputs for each test case must be a floating-point number representing the variance inflation ratio defined above.\n\nTest suite:\nImplement and run your code on the following four cases. In all cases, use a linearly spaced grid in $\\alpha$ with $N_\\alpha = 1001$ points covering $[\\alpha_{\\min}, \\alpha_{\\max}] = [0.5, 6.0]$, a centered finite difference step $h = 10^{-5}$ for derivatives, and $R = 600$ pseudo-experiments per case. Use a base pseudorandom seed $s_0 = 1729$ and for reproducibility set the seed for case index $k \\in \\{0,1,2,3\\}$ as $s_k = s_0 + k$. For each case, define the true parameters $(\\alpha_{\\text{true}}, \\mu, B, x_{\\min}, x_{\\max})$ as:\n- Case $1$ (happy path, well-populated): $(\\alpha_{\\text{true}}, \\mu, B, x_{\\min}, x_{\\max}) = (3.0, 1000.0, 20, 1.0, 10.0)$.\n- Case $2$ (sparse binning): $(\\alpha_{\\text{true}}, \\mu, B, x_{\\min}, x_{\\max}) = (3.0, 50.0, 80, 1.0, 20.0)$.\n- Case $3$ (extremely sparse and steeper): $(\\alpha_{\\text{true}}, \\mu, B, x_{\\min}, x_{\\max}) = (4.0, 15.0, 80, 1.0, 50.0)$.\n- Case $4$ (steep with boundary effects likely): $(\\alpha_{\\text{true}}, \\mu, B, x_{\\min}, x_{\\max}) = (5.0, 15.0, 40, 1.0, 50.0)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the four variance-inflation results for the four cases in the order listed, as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$. Each element must be a floating-point number.", "solution": "The objective is to perform a detailed analysis of the binned maximum likelihood estimation of a power-law index $\\alpha$, focusing on the estimator's variance and its relationship to the Cramér-Rao lower bound (CRLB). This involves deriving the theoretical expressions for the likelihood and Fisher information from first principles, followed by a Monte Carlo simulation study to empirically measure the estimator's performance under different statistical conditions, particularly in sparse data regimes.\n\n### 1. Theoretical Framework\n\n#### 1.1. The Power-Law Model and Bin Probabilities\n\nThe model is a power-law probability density function (PDF) defined on a bounded interval $[x_{\\min}, x_{\\max}]$:\n$$\nf(x \\mid \\alpha) = C(\\alpha) x^{-\\alpha}\n$$\nwhere $C(\\alpha)$ is the normalization constant. This constant is determined by the condition $\\int_{x_{\\min}}^{x_{\\max}} f(x \\mid \\alpha) \\, dx = 1$, which gives:\n$$\nC(\\alpha) = \\left( \\int_{x_{\\min}}^{x_{\\max}} x^{-\\alpha} \\, dx \\right)^{-1}\n$$\nThe integral depends on the value of $\\alpha$. For $\\alpha \\neq 1$, the indefinite integral is $\\int x^{-\\alpha} \\, dx = \\frac{x^{1-\\alpha}}{1-\\alpha}$. For $\\alpha=1$, it is $\\int x^{-1} \\, dx = \\ln x$. The definite integral over the range is thus:\n$$\n\\int_{x_{\\min}}^{x_{\\max}} x^{-\\alpha} \\, dx = \\begin{cases} \\frac{x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha}}{1-\\alpha} & \\text{if } \\alpha \\neq 1 \\\\ \\ln(x_{\\max}) - \\ln(x_{\\min}) & \\text{if } \\alpha = 1 \\end{cases}\n$$\nThe interval $[x_{\\min}, x_{\\max}]$ is partitioned into $B$ bins with edges $\\{x_0, x_1, \\dots, x_B\\}$. While not specified, we assume a linear binning scheme, where the bin edges are given by $x_i = x_{\\min} + i \\cdot \\frac{x_{\\max}-x_{\\min}}{B}$ for $i=0, \\dots, B$.\n\nThe probability $p_b(\\alpha)$ for an event to fall into a given bin $b$, which spans the interval $[x_{b-1}, x_b]$, is given by the integral of the PDF over that bin:\n$$\np_b(\\alpha) = \\int_{x_{b-1}}^{x_b} f(x \\mid \\alpha) \\, dx = C(\\alpha) \\int_{x_{b-1}}^{x_b} x^{-\\alpha} \\, dx\n$$\nSubstituting the expressions for the integral and the normalization constant, we obtain the normalized bin probabilities:\n$$\np_b(\\alpha) = \\begin{cases} \\frac{x_b^{1-\\alpha} - x_{b-1}^{1-\\alpha}}{x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha}} & \\text{if } \\alpha \\neq 1 \\\\ \\frac{\\ln(x_b) - \\ln(x_{b-1})}{\\ln(x_{\\max}) - \\ln(x_{\\min})} & \\text{if } \\alpha = 1 \\end{cases}\n$$\nBy construction, these probabilities sum to unity: $\\sum_{b=1}^B p_b(\\alpha) = 1$.\n\n#### 1.2. The Binned Likelihood Function (Task 1)\n\nThe observed count $n_b$ in each bin $b$ is modeled as an independent Poisson random variable with mean $\\lambda_b(\\alpha) = \\mu \\, p_b(\\alpha)$, where $\\mu$ is the known total expected number of events. The joint likelihood function $L(\\alpha)$ for a set of observed counts $\\{n_1, \\dots, n_B\\}$ is the product of the individual Poisson probabilities:\n$$\nL(\\alpha) = \\prod_{b=1}^B P(n_b \\mid \\lambda_b(\\alpha)) = \\prod_{b=1}^B \\frac{e^{-\\lambda_b(\\alpha)} \\lambda_b(\\alpha)^{n_b}}{n_b!}\n$$\nTo find the maximum likelihood estimate (MLE) of $\\alpha$, we maximize the log-likelihood, $\\ln L(\\alpha)$:\n$$\n\\ln L(\\alpha) = \\sum_{b=1}^B \\left( n_b \\ln[\\lambda_b(\\alpha)] - \\lambda_b(\\alpha) - \\ln(n_b!) \\right)\n$$\nSubstituting $\\lambda_b(\\alpha) = \\mu p_b(\\alpha)$:\n$$\n\\ln L(\\alpha) = \\sum_{b=1}^B \\left( n_b \\ln[\\mu p_b(\\alpha)] - \\mu p_b(\\alpha) - \\ln(n_b!) \\right)\n$$\nWe can separate the terms that depend on $\\alpha$ from those that do not:\n$$\n\\ln L(\\alpha) = \\sum_{b=1}^B n_b \\ln p_b(\\alpha) + \\sum_{b=1}^B n_b \\ln \\mu - \\sum_{b=1}^B \\mu p_b(\\alpha) - \\sum_{b=1}^B \\ln(n_b!)\n$$\nUsing the identities $\\sum_{b=1}^B p_b(\\alpha) = 1$ and defining the total observed count $N = \\sum_{b=1}^B n_b$:\n$$\n\\ln L(\\alpha) = \\left( \\sum_{b=1}^B n_b \\ln p_b(\\alpha) \\right) + N \\ln \\mu - \\mu - \\sum_{b=1}^B \\ln(n_b!)\n$$\nThe last three terms ($N \\ln \\mu$, $-\\mu$, and $-\\sum \\ln(n_b!)$) are constant with respect to the parameter $\\alpha$. Therefore, maximizing $\\ln L(\\alpha)$ is entirely equivalent to maximizing the term:\n$$\nS(\\alpha) = \\sum_{b=1}^B n_b \\ln p_b(\\alpha)\n$$\nThis is the log-likelihood of a multinomial distribution for counts $\\{n_b\\}$ with probabilities $\\{p_b(\\alpha)\\}$. This derivation fulfills Task 1.\n\n#### 1.3. Fisher Information and the Cramér-Rao Lower Bound (Task 2)\n\nThe Fisher information $I(\\alpha)$ quantifies the amount of information that the observable data carry about the unknown parameter $\\alpha$. It is defined as:\n$$\nI(\\alpha) = \\mathbb{E}\\left[ \\left( \\frac{\\partial}{\\partial \\alpha} \\ln L(\\alpha) \\right)^2 \\right] = -\\mathbb{E}\\left[ \\frac{\\partial^2}{\\partial \\alpha^2} \\ln L(\\alpha) \\right]\n$$\nWe use the second form. The second derivative of the log-likelihood with respect to $\\alpha$ is:\n$$\n\\frac{\\partial^2 \\ln L}{\\partial \\alpha^2} = \\frac{\\partial^2}{\\partial \\alpha^2} \\sum_{b=1}^B \\left( n_b \\ln p_b(\\alpha) - \\mu p_b(\\alpha) \\right) = \\sum_{b=1}^B \\left[ n_b \\left( \\frac{1}{p_b} \\frac{\\partial^2 p_b}{\\partial \\alpha^2} - \\frac{1}{p_b^2} \\left(\\frac{\\partial p_b}{\\partial \\alpha}\\right)^2 \\right) - \\mu \\frac{\\partial^2 p_b}{\\partial \\alpha^2} \\right]\n$$\nTaking the expectation value, we use the fact that $\\mathbb{E}[n_b] = \\lambda_b(\\alpha) = \\mu p_b(\\alpha)$:\n$$\n\\mathbb{E}\\left[\\frac{\\partial^2 \\ln L}{\\partial \\alpha^2}\\right] = \\sum_{b=1}^B \\left[ \\mu p_b \\left( \\frac{1}{p_b} \\frac{\\partial^2 p_b}{\\partial \\alpha^2} - \\frac{1}{p_b^2} \\left(\\frac{\\partial p_b}{\\partial \\alpha}\\right)^2 \\right) - \\mu \\frac{\\partial^2 p_b}{\\partial \\alpha^2} \\right]\n$$\nSimplifying the expression:\n$$\n\\mathbb{E}\\left[\\frac{\\partial^2 \\ln L}{\\partial \\alpha^2}\\right] = \\sum_{b=1}^B \\left[ \\mu \\frac{\\partial^2 p_b}{\\partial \\alpha^2} - \\frac{\\mu}{p_b} \\left(\\frac{\\partial p_b}{\\partial \\alpha}\\right)^2 - \\mu \\frac{\\partial^2 p_b}{\\partial \\alpha^2} \\right] = - \\mu \\sum_{b=1}^B \\frac{1}{p_b(\\alpha)} \\left(\\frac{\\partial p_b(\\alpha)}{\\partial \\alpha}\\right)^2\n$$\nThe Fisher information is the negative of this quantity:\n$$\nI(\\alpha) = \\mu \\sum_{b=1}^B \\frac{1}{p_b(\\alpha)} \\left(\\frac{\\partial p_b(\\alpha)}{\\partial \\alpha}\\right)^2\n$$\nUsing the identity $\\frac{\\partial \\ln(f)}{\\partial x} = \\frac{1}{f} \\frac{\\partial f}{\\partial x}$, we can write this in the standard \"per-event\" form:\n$$\nI(\\alpha) = \\mu \\sum_{b=1}^B p_b(\\alpha) \\left( \\frac{1}{p_b(\\alpha)} \\frac{\\partial p_b(\\alpha)}{\\partial \\alpha} \\right)^2 = \\mu \\sum_{b=1}^B p_b(\\alpha) \\left( \\frac{\\partial \\ln p_b(\\alpha)}{\\partial \\alpha} \\right)^2\n$$\nThis is of the form $I(\\alpha) = \\mu I_1(\\alpha)$, where $I_1(\\alpha) = \\sum_b p_b(\\alpha) (\\frac{\\partial \\ln p_b(\\alpha)}{\\partial \\alpha})^2$ is the Fisher information per event. This derivation completes Task 2.\n\nThe Cramér-Rao lower bound (CRLB) states that for any unbiased estimator $\\hat{\\alpha}$, its variance is bounded by the reciprocal of the Fisher information: $\\operatorname{Var}(\\hat{\\alpha}) \\ge I(\\alpha)^{-1}$. For large sample sizes, the variance of the MLE, $\\operatorname{Var}(\\hat{\\alpha})$, asymptotically approaches this bound. The theoretical minimum variance is thus:\n$$\n\\operatorname{Var}_{\\text{CRLB}}(\\hat{\\alpha}) = \\frac{1}{I(\\alpha)} = \\frac{1}{\\mu I_1(\\alpha)}\n$$\n\n### 2. Computational Methodology\n\nThe study proceeds by implementing the theoretical framework in a simulation.\n\n#### 2.1. Monte Carlo Simulation (Task 3)\n\nFor each test case, we perform $R$ pseudo-experiments. A single experiment consists of:\n1.  Defining the physical setup: $\\alpha_{\\text{true}}, \\mu, B, x_{\\min}, x_{\\max}$.\n2.  Calculating the true bin probabilities $p_b(\\alpha_{\\text{true}})$ and the true expected bin counts $\\lambda_b = \\mu p_b(\\alpha_{\\text{true}})$.\n3.  Generating a set of pseudo-observed counts $\\{n_b\\}_{b=1}^B$ by drawing from independent Poisson distributions: $n_b \\sim \\text{Poisson}(\\lambda_b)$.\n\n#### 2.2. Estimation and Performance Metric (Tasks 3 & 4)\n\nFor each pseudo-experiment dataset $\\{n_b\\}$, the MLE $\\hat{\\alpha}$ is found by maximizing the objective function $S(\\alpha) = \\sum_{b} n_b \\ln p_b(\\alpha)$ over a pre-defined grid of $\\alpha$ values. This is achieved by computing $S(\\alpha_j)$ for each point $\\alpha_j$ on the grid and identifying the $\\alpha_j$ that yields the maximum value. To avoid numerical errors with $\\ln(0)$, a small positive floor is applied to all $p_b$ values before taking the logarithm.\n\nAfter running all $R$ experiments, we obtain a distribution of $R$ estimates, $\\{\\hat{\\alpha}_1, \\dots, \\hat{\\alpha}_R\\}$. The empirical variance, $\\operatorname{Var}_{\\text{emp}}(\\hat{\\alpha})$, is computed from this sample.\n\nThe CRLB variance, $\\operatorname{Var}_{\\text{CRLB}}(\\hat{\\alpha})$, is calculated at the true parameter value $\\alpha_{\\text{true}}$. The derivative $\\frac{\\partial \\ln p_b(\\alpha)}{\\partial \\alpha}$ required for the Fisher information $I_1(\\alpha)$ is computed numerically using a second-order centered finite difference approximation:\n$$\n\\frac{\\partial \\ln p_b(\\alpha)}{\\partial \\alpha} \\approx \\frac{\\ln p_b(\\alpha+h) - \\ln p_b(\\alpha-h)}{2h}\n$$\nfor a small step size $h$.\n\nFinally, the \"variance inflation\" factor is computed as the ratio of the empirical variance to the theoretical lower bound:\n$$\n\\text{Variance Inflation} = \\frac{\\operatorname{Var}_{\\text{emp}}(\\hat{\\alpha})}{\\operatorname{Var}_{\\text{CRLB}}(\\hat{\\alpha})}\n$$\nThis ratio quantifies how close the estimator's actual performance is to the ideal asymptotic limit predicted by the CRLB. A ratio near $1$ indicates an efficient estimator, while a ratio greater than $1$ signals that the CRLB is an overly optimistic estimate of the true uncertainty, a common occurrence in sparse-data regimes.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a simulation study of binned maximum likelihood estimation for a power-law spectrum.\n    It calculates the variance inflation factor, which is the ratio of the empirical variance\n    of the estimated slope parameter to its theoretical Cramér-Rao lower bound.\n    \"\"\"\n\n    test_cases = [\n        # (alpha_true, mu, B, x_min, x_max)\n        (3.0, 1000.0, 20, 1.0, 10.0),\n        (3.0, 50.0, 80, 1.0, 20.0),\n        (4.0, 15.0, 80, 1.0, 50.0),\n        (5.0, 15.0, 40, 1.0, 50.0)\n    ]\n\n    # Global parameters from the problem statement\n    alpha_min, alpha_max = 0.5, 6.0\n    N_alpha = 1001\n    h = 1e-5\n    R = 600\n    s0 = 1729\n    prob_floor = 1e-30\n\n    alpha_grid = np.linspace(alpha_min, alpha_max, N_alpha)\n    results = []\n\n    def calculate_probabilities(alphas, x_edges):\n        \"\"\"\n        Calculates bin probabilities for a power law f(x) ~ x^-alpha for a vector of alphas.\n        \"\"\"\n        alphas = np.atleast_1d(alphas)\n        num_alphas = len(alphas)\n        num_bins = len(x_edges) - 1\n        \n        x_b = x_edges[1:]   # Shape (B,)\n        x_bm1 = x_edges[:-1] # Shape (B,)\n        \n        probs = np.zeros((num_alphas, num_bins), dtype=np.float64)\n        \n        # --- Case alpha != 1 ---\n        mask_ne1 = ~np.isclose(alphas, 1.0)\n        if np.any(mask_ne1):\n            alpha_ne1 = alphas[mask_ne1]\n            power = 1.0 - alpha_ne1\n            \n            # Use broadcasting for bin integrals\n            unnormalized_integrals = (np.power.outer(x_b, power) - np.power.outer(x_bm1, power)) / power\n            \n            # Total integral for normalization\n            total_integral = (np.power(x_edges[-1], power) - np.power(x_edges[0], power)) / power\n            \n            probs[mask_ne1, :] = (unnormalized_integrals / total_integral).T\n\n        # --- Case alpha == 1 ---\n        mask_eq1 = np.isclose(alphas, 1.0)\n        if np.any(mask_eq1):\n            unnormalized_integrals = np.log(x_b) - np.log(x_bm1)\n            total_integral = np.log(x_edges[-1]) - np.log(x_edges[0])\n            probs[mask_eq1, :] = unnormalized_integrals / total_integral\n            \n        return probs.squeeze()\n\n    for k, case in enumerate(test_cases):\n        alpha_true, mu, B, x_min, x_max = case\n        seed = s0 + k\n        rng = np.random.default_rng(seed)\n\n        x_edges = np.linspace(x_min, x_max, B + 1)\n        \n        # --- 1. Calculate CRLB Variance ---\n        # Get probabilities around alpha_true for finite difference\n        p_alpha_plus_h = calculate_probabilities(alpha_true + h, x_edges)\n        p_alpha_minus_h = calculate_probabilities(alpha_true - h, x_edges)\n        p_true = calculate_probabilities(alpha_true, x_edges)\n\n        # Apply floor to avoid log(0)\n        p_alpha_plus_h_clipped = np.maximum(p_alpha_plus_h, prob_floor)\n        p_alpha_minus_h_clipped = np.maximum(p_alpha_minus_h, prob_floor)\n        p_true_clipped = np.maximum(p_true, prob_floor)\n\n        # Numerical derivative of log probabilities\n        d_logp_d_alpha = (np.log(p_alpha_plus_h_clipped) - np.log(p_alpha_minus_h_clipped)) / (2 * h)\n\n        # Per-event Fisher Information I_1(alpha)\n        I1_true = np.sum(p_true_clipped * (d_logp_d_alpha**2))\n\n        # CRLB variance\n        var_crlb = 1.0 / (mu * I1_true)\n\n        # --- 2. Monte Carlo Simulation ---\n        # Pre-calculate probabilities for all alpha grid points\n        prob_matrix = calculate_probabilities(alpha_grid, x_edges)\n        prob_matrix_clipped = np.maximum(prob_matrix, prob_floor)\n        log_prob_matrix = np.log(prob_matrix_clipped)\n\n        # True expected counts\n        lambda_true = mu * p_true\n        \n        alpha_estimates = []\n        for _ in range(R):\n            # Generate pseudo-data\n            n_b = rng.poisson(lambda_true)\n            \n            # Calculate objective function over the alpha grid\n            # This is equivalent to sum(n_b * log(p_b(alpha))) for each alpha\n            log_likelihoods = log_prob_matrix @ n_b\n            \n            # Find MLE for alpha\n            best_alpha_idx = np.argmax(log_likelihoods)\n            hat_alpha = alpha_grid[best_alpha_idx]\n            alpha_estimates.append(hat_alpha)\n\n        # --- 3. Compute Variance Inflation ---\n        # Empirical variance of the estimator (using ddof=1 for sample variance)\n        var_emp = np.var(alpha_estimates, ddof=1)\n        \n        variance_inflation = var_emp / var_crlb\n        results.append(str(variance_inflation))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3540404"}, {"introduction": "The choice between an unbinned and a binned analysis goes beyond just parameter precision; it can fundamentally alter the outcome of model selection. This practice [@problem_id:3540403] investigates how binning affects the ability of information criteria like the Akaike Information Criterion ($AIC$) and Bayesian Information Criterion ($BIC$) to determine the correct underlying complexity of a model. You will see firsthand how data discretization can obscure important features and lead model selection criteria to favor simpler, but incorrect, models.", "problem": "You are given a one-dimensional mixture of Gaussian probability density functions (PDFs) as a generative model for independent event-level observations, and its binned counterpart as a Poisson process for bin counts. Consider two rival model families indexed by the number of Gaussian components $K \\in \\{1,2\\}$, with unknown parameters to be fitted by maximum likelihood. For unbinned data, the independent and identically distributed event-level observations $\\{x_i\\}_{i=1}^N$ are assumed to be drawn from a normalized mixture density $p(x \\mid \\theta, K) = \\sum_{j=1}^{K} w_j \\, \\phi(x; \\mu_j, \\sigma_j)$, where $\\phi(x;\\mu,\\sigma)$ is a Gaussian PDF with mean $\\mu$ and standard deviation $\\sigma$, the mixture weights satisfy $\\sum_{j=1}^{K} w_j = 1$ and $w_j \\ge 0$, and $\\theta$ denotes the full parameter vector. For binned data, the observation is a vector of independent Poisson counts $\\{n_b\\}_{b=1}^{B}$ for $B$ bins with edges $\\{a_b,b_b\\}$, and the expected count in bin $b$ under parameters $\\theta$ is $E_b(\\theta, K) = N \\, P_b(\\theta, K)$, where $P_b(\\theta, K) = \\int_{a_b}^{b_b} p(x \\mid \\theta, K)\\, dx$ and $N = \\sum_{b=1}^B n_b$ is the fixed total number of events.\n\nStarting from the fundamental definitions below, implement and compare the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) in the unbinned and binned settings, and investigate how discretization (binning) alters the effectiveness of these penalties in selecting the number of Gaussian components. Your implementation must be a complete, runnable program that performs maximum likelihood fits and computes AIC/BIC to select between $K=1$ and $K=2$.\n\nFundamental laws and definitions to be used:\n- For unbinned independent data $\\{x_i\\}_{i=1}^N$, the log-likelihood is $\\ell_{\\mathrm{unb}}(\\theta, K) = \\sum_{i=1}^{N} \\log p(x_i \\mid \\theta, K)$ for a normalized PDF $p(x \\mid \\theta, K)$.\n- For binned independent Poisson counts $\\{n_b\\}_{b=1}^B$ with expected counts $\\{E_b(\\theta, K)\\}_{b=1}^B$, the extended log-likelihood is $\\ell_{\\mathrm{bin}}(\\theta, K) = \\sum_{b=1}^B \\left[n_b \\log E_b(\\theta, K) - E_b(\\theta, K) - \\log(n_b!)\\right]$. Terms independent of $\\theta$ may be dropped for maximization and model comparison if they cancel between models.\n- The Akaike Information Criterion (AIC) is $\\mathrm{AIC} = 2k - 2 \\, \\ell(\\hat{\\theta}, K)$, with $k$ the number of free parameters and $\\hat{\\theta}$ the maximum likelihood estimator. The Bayesian Information Criterion (BIC) is $\\mathrm{BIC} = k \\log M - 2 \\, \\ell(\\hat{\\theta}, K)$, where $M$ is the sample size: for unbinned fits use $M=N$, and for binned fits use $M=B$ (the number of independent Poisson counts). In both families, use $k=2$ for $K=1$ (one mean and one standard deviation) and $k=5$ for $K=2$ (two means, two standard deviations, and one independent mixture weight).\n- The Gaussian Cumulative Distribution Function (CDF) appears when integrating a Gaussian PDF over a bin, via $P_b(\\theta, K) = \\sum_{j=1}^K w_j \\left[\\Phi\\!\\left(\\frac{b_b - \\mu_j}{\\sigma_j}\\right) - \\Phi\\!\\left(\\frac{a_b - \\mu_j}{\\sigma_j}\\right)\\right]$, where $\\Phi$ is the Gaussian CDF.\n\nYour program must:\n- Simulate unbinned data for each test case from a specified true mixture using a fixed random seed, then construct binned counts by histogramming into a specified number of equal-width bins on a range chosen by the true mixture’s overall mean and variance.\n- Perform maximum likelihood estimation separately for the unbinned likelihood and for the binned likelihood, for each candidate model $K \\in \\{1,2\\}$. For the unbinned case, you may use the Expectation-Maximization (EM) algorithm for $K=2$ and the closed-form maximum likelihood estimator for $K=1$. For the binned case, perform numerical optimization of the binned log-likelihood (you may drop constant terms that cancel in model selection), with suitable unconstrained reparameterizations to enforce positivity and mixture-weight constraints.\n- Compute $\\mathrm{AIC}$ and $\\mathrm{BIC}$ in both the unbinned and binned settings for $K=1$ and $K=2$, and select the $K$ that minimizes each criterion.\n- Output, for each test case, a list with four integers $[K_{\\mathrm{unb,AIC}}, K_{\\mathrm{unb,BIC}}, K_{\\mathrm{bin,AIC}}, K_{\\mathrm{bin,BIC}}]$.\n\nTest suite to ensure coverage:\n- Case $1$ (single component, coarse bins): seed $= 12345$, $N=1000$, true components $[(1.0, 0.0, 1.0)]$, $B=12$.\n- Case $2$ (single component, fine bins): seed $= 12345$, $N=1000$, true components $[(1.0, 0.0, 1.0)]$, $B=60$.\n- Case $3$ (well-separated two components, coarse bins): seed $= 12346$, $N=1000$, true components $[(0.5, -2.0, 0.7), (0.5, 2.0, 0.7)]$, $B=12$.\n- Case $4$ (well-separated two components, fine bins): seed $= 12346$, $N=1000$, true components $[(0.5, -2.0, 0.7), (0.5, 2.0, 0.7)]$, $B=60$.\n- Case $5$ (overlapping two components, coarse bins): seed $= 12347$, $N=500$, true components $[(0.7, 0.0, 1.0), (0.3, 0.8, 1.0)]$, $B=12$.\n- Case $6$ (overlapping two components, fine bins): seed $= 12347$, $N=500$, true components $[(0.7, 0.0, 1.0), (0.3, 0.8, 1.0)]$, $B=60$.\n- Case $7$ (small sample, two components, coarse bins): seed $= 12348$, $N=80$, true components $[(0.5, -1.0, 1.0), (0.5, 1.0, 1.0)]$, $B=12$.\n- Case $8$ (small sample, two components, fine bins): seed $= 12348$, $N=80$, true components $[(0.5, -1.0, 1.0), (0.5, 1.0, 1.0)]$, $B=60$.\n\nRange and binning instruction:\n- For each case, define the binning range as $[\\mu_{\\mathrm{mix}} - 5 \\sigma_{\\mathrm{mix}}, \\mu_{\\mathrm{mix}} + 5 \\sigma_{\\mathrm{mix}}]$, where $\\mu_{\\mathrm{mix}} = \\sum_j w_j \\mu_j$ and $\\sigma_{\\mathrm{mix}}^2 = \\sum_j w_j (\\sigma_j^2 + \\mu_j^2) - \\mu_{\\mathrm{mix}}^2$. Use $B$ equal-width bins over this range.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists with no spaces, in the order of cases $1$ through $8$, for example: \"[[k11,k12,k13,k14],[k21,k22,k23,k24],...,[k81,k82,k83,k84]]\", where each $k$ is an integer in $\\{1,2\\}$ indicating the selected number of components under the specified criterion and data type.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and contains all necessary information for a unique, verifiable solution. It requires the implementation of standard statistical methods in the context of high-energy physics data analysis. The solution proceeds by first generating data, then performing maximum likelihood fits under two model hypotheses ($K=1$ and $K=2$ Gaussian components) for both unbinned and binned representations of the data, and finally applying information criteria to select the preferred model in each case.\n\n### 1. Data Simulation and Preparation\nFor each test case, a dataset of $N$ independent events $\\{x_i\\}_{i=1}^N$ is simulated. Each event is drawn from the true generative probability density function (PDF), which is a mixture of Gaussian distributions:\n$$p(x \\mid \\theta_{\\mathrm{true}}, K_{\\mathrm{true}}) = \\sum_{j=1}^{K_{\\mathrm{true}}} w_j \\, \\phi(x; \\mu_j, \\sigma_j)$$\nwhere $\\phi(x; \\mu, \\sigma)$ is the Gaussian PDF, and $\\{w_j, \\mu_j, \\sigma_j\\}$ are the true mixture parameters. This is achieved by first sampling a component index $j$ with probability $w_j$, and then drawing a value from the corresponding Gaussian distribution $\\mathcal{N}(\\mu_j, \\sigma_j^2)$.\n\nFor the binned analysis, the data is discretized into a histogram of $B$ bins. The range of the histogram is determined by the overall mean $\\mu_{\\mathrm{mix}}$ and standard deviation $\\sigma_{\\mathrm{mix}}$ of the true mixture distribution, spanning $[\\mu_{\\mathrm{mix}} - 5 \\sigma_{\\mathrm{mix}}, \\mu_{\\mathrm{mix}} + 5 \\sigma_{\\mathrm{mix}}]$. The mixture mean is $\\mu_{\\mathrm{mix}} = \\sum_j w_j \\mu_j$. The mixture variance $\\sigma_{\\mathrm{mix}}^2$ is found using the law of total variance:\n$$\\sigma_{\\mathrm{mix}}^2 = E[\\sigma^2(X|C)] + \\mathrm{Var}(E[X|C]) = \\sum_{j=1}^{K_{\\mathrm{true}}} w_j \\sigma_j^2 + \\left( \\sum_{j=1}^{K_{\\mathrm{true}}} w_j \\mu_j^2 \\right) - \\mu_{\\mathrm{mix}}^2$$\nThe resulting histogram consists of counts $\\{n_b\\}_{b=1}^B$ in each bin, where $\\sum_b n_b = N$.\n\n### 2. Unbinned Maximum Likelihood Estimation\nThe goal is to find the parameter vector $\\hat{\\theta}$ that maximizes the log-likelihood function $\\ell_{\\mathrm{unb}}(\\theta, K) = \\sum_{i=1}^{N} \\log p(x_i \\mid \\theta, K)$.\n\nFor the $K=1$ model, $p(x \\mid \\theta, 1) = \\phi(x; \\mu, \\sigma)$. The maximum likelihood estimators (MLEs) for the parameters have closed-form solutions: the sample mean $\\hat{\\mu} = \\frac{1}{N}\\sum_i x_i$ and the sample variance $\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_i (x_i - \\hat{\\mu})^2$. The maximized log-likelihood is then $\\ell_{\\mathrm{unb}}(\\hat{\\theta}, 1) = \\sum_{i=1}^{N} \\log \\phi(x_i; \\hat{\\mu}, \\hat{\\sigma})$.\n\nFor the $K=2$ model, $p(x \\mid \\theta, 2) = w_1 \\phi(x; \\mu_1, \\sigma_1) + (1-w_1) \\phi(x; \\mu_2, \\sigma_2)$. The likelihood for this mixture model does not have a closed-form maximum. The Expectation-Maximization (EM) algorithm is an iterative procedure well-suited for this problem.\n1.  **Initialization**: Parameters $(\\mu_1, \\sigma_1, \\mu_2, \\sigma_2, w_1)$ are initialized. A deterministic approach is used where the sorted data is split in two, and the moments of each half provide the initial means and standard deviations.\n2.  **E-Step**: The posterior probability, or \"responsibility,\" of component $j$ for generating data point $x_i$ is computed:\n    $$\\gamma_{ij} = \\frac{w_j \\phi(x_i; \\mu_j, \\sigma_j)}{\\sum_{l=1}^{2} w_l \\phi(x_i; \\mu_l, \\sigma_l)}$$\n3.  **M-Step**: The parameters are updated to maximize the expected log-likelihood, given the current responsibilities:\n    $$N_j = \\sum_{i=1}^{N} \\gamma_{ij}, \\quad w_j^{\\mathrm{new}} = \\frac{N_j}{N}, \\quad \\mu_j^{\\mathrm{new}} = \\frac{1}{N_j} \\sum_{i=1}^{N} \\gamma_{ij} x_i, \\quad (\\sigma_j^{\\mathrm{new}})^2 = \\frac{1}{N_j} \\sum_{i=1}^{N} \\gamma_{ij} (x_i - \\mu_j^{\\mathrm{new}})^2$$\nThe E-step and M-step are repeated until the change in the total log-likelihood $\\ell_{\\mathrm{unb}}(\\theta, 2) = \\sum_i \\log p(x_i \\mid \\theta, 2)$ falls below a tolerance.\n\n### 3. Binned Maximum Likelihood Estimation\nFor binned data, the counts $\\{n_b\\}$ are treated as independent Poisson random variables with expectations $\\{E_b(\\theta, K)\\}$. The model selection is based on the binned extended log-likelihood, from which terms independent of parameters $\\theta$ (i.e., $\\sum_b \\log(n_b!)$) are dropped, as they cancel when comparing models:\n$$\\ell^*_{\\mathrm{bin}}(\\theta, K) = \\sum_{b=1}^{B} \\left[ n_b \\log E_b(\\theta, K) - E_b(\\theta, K) \\right]$$\nThe expected count in bin $b$, with edges $[a_b, b_b]$, is given by $E_b(\\theta, K) = N \\cdot P_b(\\theta, K)$, where $P_b(\\theta, K)$ is the probability of an event falling in that bin. This probability is calculated by integrating the PDF over the bin, which involves the Gaussian cumulative distribution function (CDF), $\\Phi(z)$:\n$$P_b(\\theta, K) = \\int_{a_b}^{b_b} p(x \\mid \\theta, K) \\, dx = \\sum_{j=1}^{K} w_j \\left[ \\Phi\\left(\\frac{b_b - \\mu_j}{\\sigma_j}\\right) - \\Phi\\left(\\frac{a_b - \\mu_j}{\\sigma_j}\\right) \\right]$$\nMaximizing $\\ell^*_{\\mathrm{bin}}$ is performed via numerical optimization. To handle parameter constraints, the optimization is done over an unconstrained space. Standard deviations $\\sigma_j > 0$ are reparameterized as $\\sigma_j = \\exp(\\log \\sigma_j)$. The mixture weight $w_1 \\in [0, 1]$ for the $K=2$ case is reparameterized using the logit transformation, $w_1 = (1 + \\exp(-\\alpha_1))^{-1}$. The parameters found from the unbinned fits serve as initial values for the numerical minimizer to promote stable and efficient convergence.\n\n### 4. Model Selection\nThe Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are used to compare the performance of the $K=1$ and $K=2$ models. The model with the lower criterion value is preferred.\n$$\\mathrm{AIC} = 2k - 2 \\ell_{\\mathrm{max}}$$\n$$\\mathrm{BIC} = k \\log M - 2 \\ell_{\\mathrm{max}}$$\nHere, $\\ell_{\\mathrm{max}}$ is the maximized log-likelihood (either $\\ell_{\\mathrm{unb}}$ or $\\ell^*_{\\mathrm{bin}}$), and $k$ is the number of free parameters in the model. As specified, $k=2$ for the $K=1$ model $(\\mu, \\sigma)$ and $k=5$ for the $K=2$ model $(\\mu_1, \\sigma_1, \\mu_2, \\sigma_2, w_1)$. The sample size $M$ is defined as the number of independent observations: for the unbinned fit, $M=N$ (the number of events), and for the binned fit, $M=B$ (the number of bins). This analysis is performed for both unbinned and binned data, yielding four model selections for each test case.", "answer": "```python\nimport numpy as np\nfrom scipy import stats, optimize, special\n\ndef solve():\n    \"\"\"\n    Implements the full analysis pipeline for comparing AIC/BIC in unbinned and binned\n    Gaussian mixture model fits, as specified in the problem statement.\n    \"\"\"\n\n    test_cases = [\n        {'seed': 12345, 'N': 1000, 'components': [(1.0, 0.0, 1.0)], 'B': 12},\n        {'seed': 12345, 'N': 1000, 'components': [(1.0, 0.0, 1.0)], 'B': 60},\n        {'seed': 12346, 'N': 1000, 'components': [(0.5, -2.0, 0.7), (0.5, 2.0, 0.7)], 'B': 12},\n        {'seed': 12346, 'N': 1000, 'components': [(0.5, -2.0, 0.7), (0.5, 2.0, 0.7)], 'B': 60},\n        {'seed': 12347, 'N': 500, 'components': [(0.7, 0.0, 1.0), (0.3, 0.8, 1.0)], 'B': 12},\n        {'seed': 12347, 'N': 500, 'components': [(0.7, 0.0, 1.0), (0.3, 0.8, 1.0)], 'B': 60},\n        {'seed': 12348, 'N': 80, 'components': [(0.5, -1.0, 1.0), (0.5, 1.0, 1.0)], 'B': 12},\n        {'seed': 12348, 'N': 80, 'components': [(0.5, -1.0, 1.0), (0.5, 1.0, 1.0)], 'B': 60},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # --- 1. Data Generation and Preparation ---\n        rng = np.random.default_rng(case['seed'])\n        N = case['N']\n        components = case['components']\n        n_comp = len(components)\n        \n        if n_comp == 1:\n            w, mu, sigma = components[0]\n            unbinned_data = rng.normal(mu, sigma, N)\n        else:\n            weights, means, sigmas = zip(*components)\n            component_indices = rng.choice(n_comp, size=N, p=weights)\n            unbinned_data = np.concatenate([\n                rng.normal(means[i], sigmas[i], np.sum(component_indices == i))\n                for i in range(n_comp)\n            ])\n\n        B = case['B']\n        k_params = {1: 2, 2: 5}\n\n        # --- 2. Unbinned Fits ---\n        # K=1\n        mu1_unb, sigma1_unb = np.mean(unbinned_data), np.std(unbinned_data)\n        logL_unb_k1 = np.sum(stats.norm.logpdf(unbinned_data, loc=mu1_unb, scale=sigma1_unb))\n        \n        # K=2 (EM algorithm)\n        x_sorted = np.sort(unbinned_data)\n        split = N // 2\n        mu1_init, mu2_init = np.mean(x_sorted[:split]), np.mean(x_sorted[split:])\n        sigma1_init, sigma2_init = np.std(x_sorted[:split]), np.std(x_sorted[split:])\n        w1_init = 0.5\n        \n        mu1_em, mu2_em = mu1_init, mu2_init\n        sigma1_em, sigma2_em = max(sigma1_init, 1e-6), max(sigma2_init, 1e-6)\n        w1_em = w1_init\n        \n        for _ in range(100):\n            pdf1 = stats.norm.pdf(unbinned_data, mu1_em, sigma1_em)\n            pdf2 = stats.norm.pdf(unbinned_data, mu2_em, sigma2_em)\n            \n            resp_num1 = w1_em * pdf1\n            denominator = resp_num1 + (1 - w1_em) * pdf2\n            denominator[denominator  1e-9] = 1e-9\n            resp1 = resp_num1 / denominator\n            \n            N1 = np.sum(resp1)\n            if N1  1e-6 or N - N1  1e-6: break\n\n            w1_em = N1 / N\n            mu1_em = np.sum(resp1 * unbinned_data) / N1\n            mu2_em = np.sum((1 - resp1) * unbinned_data) / (N - N1)\n            sigma1_em = np.sqrt(np.sum(resp1 * (unbinned_data - mu1_em)**2) / N1 + 1e-9)\n            sigma2_em = np.sqrt(np.sum((1 - resp1) * (unbinned_data - mu2_em)**2) / (N - N1) + 1e-9)\n\n        logL_unb_k2 = np.sum(np.log(w1_em * stats.norm.pdf(unbinned_data, mu1_em, sigma1_em) + \n                                     (1 - w1_em) * stats.norm.pdf(unbinned_data, mu2_em, sigma2_em) + 1e-12))\n        \n        # Unbinned model selection\n        aic_unb_k1, bic_unb_k1 = 2 * k_params[1] - 2 * logL_unb_k1, k_params[1] * np.log(N) - 2 * logL_unb_k1\n        aic_unb_k2, bic_unb_k2 = 2 * k_params[2] - 2 * logL_unb_k2, k_params[2] * np.log(N) - 2 * logL_unb_k2\n        K_unb_aic = 1 if aic_unb_k1  aic_unb_k2 else 2\n        K_unb_bic = 1 if bic_unb_k1  bic_unb_k2 else 2\n\n        # --- 3. Binned Fits ---\n        true_w, true_mu, true_s = zip(*components)\n        mu_mix = np.sum(np.array(true_w) * np.array(true_mu))\n        var_mix = np.sum(np.array(true_w) * (np.array(true_s)**2 + np.array(true_mu)**2)) - mu_mix**2\n        bin_range = (mu_mix - 5 * np.sqrt(var_mix), mu_mix + 5 * np.sqrt(var_mix))\n        binned_counts, bin_edges = np.histogram(unbinned_data, bins=B, range=bin_range)\n        \n        # Binned Negative Log-Likelihood\n        def binned_nll(params, K):\n            if K == 1:\n                mu1, log_sigma1 = params\n                prob = stats.norm.cdf(bin_edges[1:], mu1, np.exp(log_sigma1)) - \\\n                       stats.norm.cdf(bin_edges[:-1], mu1, np.exp(log_sigma1))\n            else: # K=2\n                mu1, log_sigma1, mu2, log_sigma2, alpha1 = params\n                w1 = special.expit(alpha1)\n                p1 = stats.norm.cdf(bin_edges[1:], mu1, np.exp(log_sigma1)) - \\\n                     stats.norm.cdf(bin_edges[:-1], mu1, np.exp(log_sigma1))\n                p2 = stats.norm.cdf(bin_edges[1:], mu2, np.exp(log_sigma2)) - \\\n                     stats.norm.cdf(bin_edges[:-1], mu2, np.exp(log_sigma2))\n                prob = w1 * p1 + (1 - w1) * p2\n            \n            E_b = N * prob\n            nll = -np.sum(binned_counts * np.log(E_b + 1e-12) - E_b)\n            return nll if np.isfinite(nll) else np.inf\n\n        # K=1 binned fit\n        init_bin_k1 = [mu1_unb, np.log(sigma1_unb)]\n        res_bin_k1 = optimize.minimize(binned_nll, init_bin_k1, args=(1,), method='Nelder-Mead')\n        logL_bin_k1 = -res_bin_k1.fun\n\n        # K=2 binned fit\n        w1_init_clip = np.clip(w1_em, 1e-6, 1 - 1e-6)\n        init_bin_k2 = [mu1_em, np.log(sigma1_em), mu2_em, np.log(sigma2_em), special.logit(w1_init_clip)]\n        res_bin_k2 = optimize.minimize(binned_nll, init_bin_k2, args=(2,), method='Nelder-Mead')\n        logL_bin_k2 = -res_bin_k2.fun\n\n        # Binned model selection\n        aic_bin_k1, bic_bin_k1 = 2 * k_params[1] - 2 * logL_bin_k1, k_params[1] * np.log(B) - 2 * logL_bin_k1\n        aic_bin_k2, bic_bin_k2 = 2 * k_params[2] - 2 * logL_bin_k2, k_params[2] * np.log(B) - 2 * logL_bin_k2\n        K_bin_aic = 1 if aic_bin_k1  aic_bin_k2 else 2\n        K_bin_bic = 1 if bic_bin_k1  bic_bin_k2 else 2\n\n        results.append([K_unb_aic, K_unb_bic, K_bin_aic, K_bin_bic])\n\n    # --- 4. Final Output ---\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3540403"}]}