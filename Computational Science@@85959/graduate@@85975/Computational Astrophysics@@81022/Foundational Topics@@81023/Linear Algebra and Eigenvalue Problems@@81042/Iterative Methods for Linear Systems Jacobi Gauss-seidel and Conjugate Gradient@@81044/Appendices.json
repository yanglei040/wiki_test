{"hands_on_practices": [{"introduction": "To effectively use or design iterative solvers, we must first understand *why* and *how* they converge. This exercise provides a foundational tool, Fourier analysis, to dissect the performance of the Jacobi method when applied to the discrete Poisson equation. By examining how the iteration matrix acts on different Fourier modes of the error, you will derive its spectral properties and reveal the fundamental reasons for its slow convergence on problems like gravitational potential solves [@problem_id:3515732].", "problem": "In computational astrophysics, the self-gravitational potential of a thin, periodic disk is often computed by solving the Poisson equation $-\\nabla^{2}\\phi=\\rho$ on a square domain with periodic boundary conditions. Consider a uniform square domain of side length $L$ discretized with an $n \\times n$ grid of spacing $h=L/n$. The standard five-point finite difference discretization of the operator $-\\nabla^{2}$ on this grid leads to a linear system $A u = b$ with the matrix $A$ defined by the stencil\n$$\nA u_{i,j}=\\frac{1}{h^{2}}\\left(4 u_{i,j}-u_{i+1,j}-u_{i-1,j}-u_{i,j+1}-u_{i,j-1}\\right),\n$$\nfor all interior indices $(i,j)$, extended periodically at the boundaries. The Jacobi iteration for solving $A u = b$ is defined by\n$$\nu^{(m+1)} = u^{(m)} + D^{-1}\\left(b - A u^{(m)}\\right),\n$$\nwhere $D$ is the diagonal of $A$, and its iteration matrix is $G_{J}=I-D^{-1}A$. Because the domain and discretization are periodic, the discrete Fourier modes $u_{i,j}=\\exp\\big(i(k x_{i} + l y_{j})\\big)$, with $x_{i}=i h$, $y_{j}=j h$, and wave numbers $k=\\frac{2\\pi m}{L}$, $l=\\frac{2\\pi p}{L}$ for integers $m,p\\in\\{0,1,\\dots,n-1\\}$, form an eigenbasis for $A$ and $G_{J}$.\n\nStarting from the five-point stencil definition and the periodicity of the grid, derive the eigenvalues of $G_{J}$ acting on the Fourier modes $u_{i,j}=\\exp\\big(i(k x_{i} + l y_{j})\\big)$ in terms of $k$, $l$, and $h$. Then, by analyzing the magnitude of these eigenvalues over all discrete Fourier modes, identify the worst-amplified mode and compute the spectral radius (the maximum magnitude of the eigenvalues of $G_{J}$ over all $(k,l)$). Express the final spectral radius as a pure number with no units. No rounding is required.", "solution": "The user wants to analyze the Jacobi iteration for a finite difference discretization of the Poisson equation on a periodic square domain. This involves finding the eigenvalues of the Jacobi iteration matrix and its spectral radius.\n\nThe problem statement has been validated and is found to be scientifically grounded, well-posed, objective, and internally consistent. It represents a standard problem in numerical analysis for partial differential equations.\n\nFirst, we identify the components of the Jacobi iteration matrix, $G_J = I - D^{-1}A$.\nThe linear operator $A$ is defined by its action on a grid function $u_{i,j}$:\n$$\nA u_{i,j}=\\frac{1}{h^{2}}\\left(4 u_{i,j}-u_{i+1,j}-u_{i-1,j}-u_{i,j+1}-u_{i,j-1}\\right)\n$$\nThe matrix $D$ is the diagonal of $A$. From the stencil, the coefficient of $u_{i,j}$ determines the diagonal entries of $A$. Thus, for any grid point $(i,j)$, the corresponding diagonal element is $\\frac{4}{h^2}$. The matrix $D$ is therefore a scalar multiple of the identity matrix $I$:\n$$\nD = \\frac{4}{h^2} I\n$$\nIts inverse is:\n$$\nD^{-1} = \\frac{h^2}{4} I\n$$\nThe Jacobi iteration matrix is then:\n$$\nG_J = I - D^{-1}A = I - \\frac{h^2}{4} A\n$$\nIf $\\lambda_A$ is an eigenvalue of $A$ corresponding to an eigenvector $v$, then the corresponding eigenvalue of $G_J$, denoted $\\lambda_J$, is found by applying $G_J$ to $v$:\n$$\nG_J v = \\left(I - \\frac{h^2}{4} A\\right)v = I v - \\frac{h^2}{4} (A v) = v - \\frac{h^2}{4} (\\lambda_A v) = (1 - \\frac{h^2}{4}\\lambda_A) v\n$$\nThus, the eigenvalues are related by $\\lambda_J = 1 - \\frac{h^2}{4}\\lambda_A$.\n\nThe problem states that the discrete Fourier modes are the eigenvectors of $A$. We now find the eigenvalues $\\lambda_A$ by applying the operator $A$ to a Fourier mode $u_{i,j} = \\exp(i(k x_i + l y_j))$, where $x_i = i h$ and $y_j = j h$.\n$$\nu_{i+1, j} = \\exp(i(k(i+1)h + ljh)) = \\exp(ikh) \\exp(i(kih + ljh)) = \\exp(ikh) u_{i,j}\n$$\n$$\nu_{i-1, j} = \\exp(i(k(i-1)h + ljh)) = \\exp(-ikh) u_{i,j}\n$$\n$$\nu_{i, j+1} = \\exp(i(kih + l(j+1)h)) = \\exp(ilh) u_{i,j}\n$$\n$$\nu_{i, j-1} = \\exp(i(kih + l(j-1)h)) = \\exp(-ilh) u_{i,j}\n$$\nSubstituting these into the definition of $A u_{i,j}$:\n$$\nA u_{i,j} = \\frac{1}{h^2} \\left[ 4 u_{i,j} - \\exp(ikh) u_{i,j} - \\exp(-ikh) u_{i,j} - \\exp(ilh) u_{i,j} - \\exp(-ilh) u_{i,j} \\right]\n$$\nFactoring out $u_{i,j}$:\n$$\nA u_{i,j} = \\frac{1}{h^2} \\left[ 4 - (\\exp(ikh) + \\exp(-ikh)) - (\\exp(ilh) + \\exp(-ilh)) \\right] u_{i,j}\n$$\nUsing the identity $2\\cos\\theta = e^{i\\theta} + e^{-i\\theta}$, we get the eigenvalues of $A$ for each mode $(k,l)$:\n$$\n\\lambda_A(k,l) = \\frac{1}{h^2} [4 - 2\\cos(kh) - 2\\cos(lh)]\n$$\nThis expression can be further simplified using the half-angle identity $1-\\cos\\theta = 2\\sin^2(\\theta/2)$:\n$$\n\\lambda_A(k,l) = \\frac{2}{h^2} [ (1-\\cos(kh)) + (1-\\cos(lh)) ] = \\frac{4}{h^2} [\\sin^2(kh/2) + \\sin^2(lh/2)]\n$$\nNow we can find the eigenvalues of the Jacobi iteration matrix $G_J$:\n$$\n\\lambda_J(k,l) = 1 - \\frac{h^2}{4}\\lambda_A(k,l) = 1 - \\frac{h^2}{4} \\left( \\frac{1}{h^2} [4 - 2\\cos(kh) - 2\\cos(lh)] \\right)\n$$\n$$\n\\lambda_J(k,l) = 1 - (1 - \\frac{1}{2}\\cos(kh) - \\frac{1}{2}\\cos(lh)) = \\frac{1}{2}(\\cos(kh) + \\cos(lh))\n$$\nThis is the expression for the eigenvalues of $G_J$ in terms of $k$, $l$, and $h$.\n\nNext, we compute the spectral radius, $\\rho(G_J)$, which is the maximum magnitude of these eigenvalues over all allowed modes. The discrete wave numbers are given by $k = \\frac{2\\pi m}{L}$ and $l = \\frac{2\\pi p}{L}$ for integers $m,p \\in \\{0, 1, \\dots, n-1\\}$. Since $h=L/n$, the arguments of the cosine functions become $kh = \\frac{2\\pi m}{n}$ and $lh = \\frac{2\\pi p}{n}$. The eigenvalues for each mode $(m,p)$ are:\n$$\n\\lambda_{J,m,p} = \\frac{1}{2}\\left(\\cos\\left(\\frac{2\\pi m}{n}\\right) + \\cos\\left(\\frac{2\\pi p}{n}\\right)\\right)\n$$\nThe spectral radius is $\\rho(G_J) = \\max_{m,p} |\\lambda_{J,m,p}|$. Since $\\cos\\theta \\in [-1, 1]$, the value of $\\lambda_{J,m,p}$ is bounded in $[-1, 1]$. We seek the modes that achieve these extreme values.\n\nThe maximum value of $\\lambda_{J,m,p}$ occurs when both cosine terms are maximal. This happens for $m=0$ and $p=0$:\n$$\n\\lambda_{J,0,0} = \\frac{1}{2}(\\cos(0) + \\cos(0)) = \\frac{1}{2}(1 + 1) = 1\n$$\nThe magnitude is $|\\lambda_{J,0,0}| = 1$. This corresponds to the $k=0, l=0$ mode (the constant mode), which is an eigenvector of the discrete Laplacian $A$ with eigenvalue $0$. An eigenvalue of $1$ for the iteration matrix means the error component in this mode does not decay, indicating non-convergence.\n\nThe minimum value of $\\lambda_{J,m,p}$ occurs when both cosine terms are minimal. This happens when the arguments are as close to $\\pi$ as possible.\nIf $n$ is an even integer, we can choose $m=n/2$ and $p=n/2$. These are valid integer indices. The arguments become $\\frac{2\\pi(n/2)}{n} = \\pi$.\n$$\n\\lambda_{J,n/2,n/2} = \\frac{1}{2}(\\cos(\\pi) + \\cos(\\pi)) = \\frac{1}{2}(-1 - 1) = -1\n$$\nThe magnitude is $|\\lambda_{J,n/2,n/2}| = 1$. This corresponds to the highest-frequency (Nyquist) mode, often called the \"checkerboard\" mode, $u_{i,j} \\propto (-1)^{i+j}$. An eigenvalue of $-1$ means the error component flips its sign at each iteration without decaying in magnitude, also indicating non-convergence.\n\nThe \"worst-amplified\" modes are those for which the eigenvalue magnitude $|\\lambda_J|$ is maximized. We have found modes for which $|\\lambda_J|=1$. Since the range of $\\lambda_J$ is $[-1,1]$, the maximum possible magnitude is $1$. This maximum is achieved for the constant mode $(m,p)=(0,0)$ for any grid size $n$, and for high-frequency modes like $(m,p)=(n/2, n/2)$ if $n$ is even.\n\nTherefore, the spectral radius of the Jacobi iteration matrix is the maximum of these magnitudes.\n$$\n\\rho(G_J) = \\max_{m,p} |\\lambda_{J,m,p}| = 1\n$$", "answer": "$$ \\boxed{1} $$", "id": "3515732"}, {"introduction": "The Conjugate Gradient method is a cornerstone for solving symmetric positive definite systems, but knowing when to stop the iterations is a critical, practical question. Since the true error is not available during the solve, we need a reliable, computable estimate. This practice explores the fundamental relationship between the error $e_k$, the residual $r_k$, and the $A$-norm, demonstrating how a preconditioner can be used to construct a practical upper bound on the error, which is essential for designing robust stopping criteria [@problem_id:3515779].", "problem": "In a self-consistent field solve for the gravitational potential in a thin, axisymmetric disk, the midplane potential $\\phi$ satisfies a Poisson equation that, upon second-order finite-difference discretization on a uniform mesh with spacing $h$, yields a Symmetric Positive Definite (SPD) linear system $A x = b$. Consider the one-dimensional reduction with $h = 1$, Dirichlet boundaries, and the $3 \\times 3$ SPD stiffness matrix\n$$\nA = \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}.\n$$\nSuppose the Conjugate Gradient (CG) method is applied to $A x = b$, and at iteration $k$ the residual is\n$$\nr_k = \\begin{pmatrix} 0.10 \\\\ -0.05 \\\\ 0.08 \\end{pmatrix}.\n$$\nLet the error be $e_k = x - x_k$, and define the energy norm induced by $A$ as $\\|u\\|_A = \\sqrt{u^{T} A u}$. You have access to a Symmetric Positive Definite (SPD) preconditioner $P$ that approximates $A^{-1}$,\n$$\nP = \\begin{pmatrix}\n0.6  0.2  0 \\\\\n0.2  0.6  0.2 \\\\\n0  0.2  0.6\n\\end{pmatrix},\n$$\nand offline calibration of the solver against high-resolution multigrid corrections yields spectral-equivalence constants $\\alpha$ and $\\beta$ such that, for all $v \\in \\mathbb{R}^{3}$,\n$$\n\\alpha\\, v^{T} P v \\le v^{T} A^{-1} v \\le \\beta\\, v^{T} P v,\n$$\nwith $\\alpha = 0.5$ and $\\beta = 1.5$.\n\nStarting from the definitions of residual, error, and the $A$-inner product, derive a computable upper bound for the energy-norm error $\\|e_k\\|_A$ in terms of the residual $r_k$ and the preconditioner $P$. Then evaluate this bound numerically for the given $r_k$, $A$, $P$, $\\alpha$, and $\\beta$. Report the bound as a single real-valued number. Round your answer to four significant figures. No units are required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n-   Linear system: $A x = b$, where $A$ is a Symmetric Positive Definite (SPD) matrix.\n-   Stiffness matrix:\n    $$\n    A = \\begin{pmatrix}\n    2  -1  0 \\\\\n    -1  2  -1 \\\\\n    0  -1  2\n    \\end{pmatrix}\n    $$\n-   Residual at iteration $k$ of the Conjugate Gradient (CG) method:\n    $$\n    r_k = \\begin{pmatrix} 0.10 \\\\ -0.05 \\\\ 0.08 \\end{pmatrix}\n    $$\n-   Error at iteration $k$: $e_k = x - x_k$.\n-   Energy norm definition: $\\|u\\|_A = \\sqrt{u^{T} A u}$.\n-   SPD Preconditioner:\n    $$\n    P = \\begin{pmatrix}\n    0.6  0.2  0 \\\\\n    0.2  0.6  0.2 \\\\\n    0  0.2  0.6\n    \\end{pmatrix}\n    $$\n-   Spectral-equivalence constants and inequality: For all $v \\in \\mathbb{R}^{3}$,\n    $$\n    \\alpha\\, v^{T} P v \\le v^{T} A^{-1} v \\le \\beta\\, v^{T} P v,\n    $$\n    with $\\alpha = 0.5$ and $\\beta = 1.5$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded. The context is a standard application of numerical linear algebra to solve a discretized partial differential equation in astrophysics. The given matrix $A$ is the well-known $1$D discrete Laplacian operator, which is SPD. The given matrix $P$ is symmetric, and its positive definiteness can be verified by checking that its leading principal minors are positive: $0.6  0$, $0.6^2 - 0.2^2 = 0.32  0$, and $\\det(P) = 0.168  0$. Thus, $P$ is also SPD, as stated. The concept of a preconditioner and spectral equivalence is central to the analysis of iterative methods. The problem is well-posed, providing all necessary information to derive and compute the required bound. The language is objective and precise. The problem is deemed valid.\n\n**Step 3: Derivation and Solution**\nThe objective is to derive a computable upper bound for the energy-norm of the error, $\\|e_k\\|_A$, using the provided quantities.\n\nFirst, we establish the relationship between the error $e_k$ and the residual $r_k$. The residual is defined as $r_k = b - A x_k$. Since $x$ is the exact solution to $A x = b$, we can write:\n$$\nr_k = A x - A x_k = A(x - x_k)\n$$\nBy definition, the error is $e_k = x - x_k$. Substituting this into the expression for the residual gives:\n$$\nr_k = A e_k\n$$\nSince the matrix $A$ is given as Symmetric Positive Definite (SPD), it is invertible. We can therefore express the error in terms of the residual:\n$$\ne_k = A^{-1} r_k\n$$\nNext, we use the definition of the squared energy norm, $\\|e_k\\|_A^2 = e_k^T A e_k$. Substituting the expression for $e_k$ in terms of $r_k$:\n$$\n\\|e_k\\|_A^2 = (A^{-1} r_k)^T A (A^{-1} r_k)\n$$\nUsing the property $(XY)^T = Y^T X^T$, we get:\n$$\n\\|e_k\\|_A^2 = r_k^T (A^{-1})^T A A^{-1} r_k\n$$\nSince $A$ is symmetric, its inverse $A^{-1}$ is also symmetric, so $(A^{-1})^T = A^{-1}$. The expression simplifies to:\n$$\n\\|e_k\\|_A^2 = r_k^T A^{-1} A A^{-1} r_k = r_k^T (A^{-1} A) A^{-1} r_k = r_k^T I A^{-1} r_k\n$$\nThis gives the exact expression for the squared energy-norm error:\n$$\n\\|e_k\\|_A^2 = r_k^T A^{-1} r_k\n$$\nThis expression is not directly computable, as it involves $A^{-1}$, the very matrix inverse we are trying to avoid computing. However, we are given a preconditioner $P$ and a spectral equivalence relationship. The problem provides the inequality:\n$$\n\\alpha\\, v^{T} P v \\le v^{T} A^{-1} v \\le \\beta\\, v^{T} P v\n$$\nThis inequality holds for any vector $v \\in \\mathbb{R}^{3}$. We can let $v = r_k$ to obtain a bound on the term $r_k^T A^{-1} r_k$:\n$$\n\\alpha\\, r_k^{T} P r_k \\le r_k^{T} A^{-1} r_k \\le \\beta\\, r_k^{T} P r_k\n$$\nWe are interested in an upper bound for $\\|e_k\\|_A$. Combining the last two results:\n$$\n\\|e_k\\|_A^2 = r_k^T A^{-1} r_k \\le \\beta\\, r_k^T P r_k\n$$\nTaking the square root of both sides yields the computable upper bound for the energy-norm error:\n$$\n\\|e_k\\|_A \\le \\sqrt{\\beta\\, r_k^T P r_k}\n$$\nNow, we evaluate this bound numerically using the given values.\nGiven:\n$\\beta = 1.5$\n$r_k = \\begin{pmatrix} 0.10 \\\\ -0.05 \\\\ 0.08 \\end{pmatrix}$\n$P = \\begin{pmatrix} 0.6  0.2  0 \\\\ 0.2  0.6  0.2 \\\\ 0  0.2  0.6 \\end{pmatrix}$\n\nFirst, we compute the product $P r_k$:\n$$\nP r_k = \\begin{pmatrix} 0.6  0.2  0 \\\\ 0.2  0.6  0.2 \\\\ 0  0.2  0.6 \\end{pmatrix} \\begin{pmatrix} 0.10 \\\\ -0.05 \\\\ 0.08 \\end{pmatrix} = \\begin{pmatrix} (0.6)(0.10) + (0.2)(-0.05) + (0)(0.08) \\\\ (0.2)(0.10) + (0.6)(-0.05) + (0.2)(0.08) \\\\ (0)(0.10) + (0.2)(-0.05) + (0.6)(0.08) \\end{pmatrix}\n$$\n$$\nP r_k = \\begin{pmatrix} 0.06 - 0.01 \\\\ 0.02 - 0.03 + 0.016 \\\\ -0.01 + 0.048 \\end{pmatrix} = \\begin{pmatrix} 0.05 \\\\ 0.006 \\\\ 0.038 \\end{pmatrix}\n$$\nNext, we compute the quadratic form $r_k^T P r_k$, which is the dot product of $r_k$ and $P r_k$:\n$$\nr_k^T P r_k = \\begin{pmatrix} 0.10  -0.05  0.08 \\end{pmatrix} \\begin{pmatrix} 0.05 \\\\ 0.006 \\\\ 0.038 \\end{pmatrix}\n$$\n$$\nr_k^T P r_k = (0.10)(0.05) + (-0.05)(0.006) + (0.08)(0.038)\n$$\n$$\nr_k^T P r_k = 0.005 - 0.0003 + 0.00304 = 0.00774\n$$\nFinally, we substitute this value into the upper bound expression:\n$$\n\\|e_k\\|_A \\le \\sqrt{\\beta\\, r_k^T P r_k} = \\sqrt{(1.5)(0.00774)} = \\sqrt{0.01161}\n$$\nCalculating the final numerical value:\n$$\n\\sqrt{0.01161} \\approx 0.10774971\n$$\nRounding to four significant figures, the upper bound is $0.1077$.", "answer": "$$\n\\boxed{0.1077}\n$$", "id": "3515779"}, {"introduction": "The theoretical elegance of an algorithm meets the practical constraints of computer hardware when tackling real-world, three-dimensional problems. This exercise grounds the discussion in computational reality by focusing on the structure of the large sparse matrices that arise from discretizing partial differential equations in 3D. You will analyze the sparsity pattern and determine the memory footprint for storing such a matrix, highlighting the critical importance of efficient data structures like the Compressed Sparse Row (CSR) format in making large-scale simulations feasible [@problem_id:3515775].", "problem": "An astrophysical simulation computes a Newtonian gravitational potential $\\Phi$ on a cubic, uniform grid of $N^{3}$ points by solving the Poisson equation $\\nabla^{2} \\Phi = 4 \\pi G \\rho$ with Dirichlet boundary conditions, using second-order central finite differences. This leads to a linear system $A x = b$ whose coefficient matrix $A$ corresponds to the standard $7$-point discrete Laplacian on the three-dimensional grid. The unknowns are ordered in a natural lexicographic (row-major) mapping from $(i,j,k)$ to a single index\n$$\np(i,j,k) = i + N (j - 1) + N^{2} (k - 1),\n$$\nwith $i,j,k \\in \\{1,2,\\dots,N\\}$.\n\nStarting from the discrete operator induced by $\\nabla^{2}$ and the mapping $p(i,j,k)$, derive from first principles the sparsity pattern of $A$ by identifying all index differences $\\Delta = p - q$ for which $a_{pq} \\neq 0$. Using this, determine the semi-bandwidth $\\beta$ of $A$ under the given ordering, defined so that $a_{pq} = 0$ whenever $|p - q| > \\beta$. Next, count the exact total number of nonzero entries $\\mathrm{nnz}(A)$ as a function of $N$.\n\nFinally, assume storage in Compressed Sparse Row (CSR) format, which consists of one value array of length $\\mathrm{nnz}(A)$, one column-index array of length $\\mathrm{nnz}(A)$, and one row-pointer array of length $N^{3} + 1$. Use $8$-byte IEEE 754 floating-point numbers for values and $8$-byte $64$-bit integers for both column indices and row pointers. Derive the total memory footprint $M(N)$ in bytes of storing $A$ in CSR.\n\nProvide your final answer as the ordered pair $(\\beta(N), M(N))$. Express the final memory in bytes. Do not round; give an exact closed-form expression in terms of $N$.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, and objective. All necessary information is provided and is internally consistent.\n\nThe problem asks for the derivation of the semi-bandwidth $\\beta(N)$ and the total memory footprint $M(N)$ for storing a matrix $A$ in Compressed Sparse Row (CSR) format. The matrix $A$ arises from the finite-difference discretization of the Poisson equation $\\nabla^{2} \\Phi = 4 \\pi G \\rho$ on a uniform cubic grid of size $N \\times N \\times N$.\n\nFirst, we analyze the structure of the matrix $A$. The use of a second-order central finite difference scheme for the three-dimensional Laplacian operator $\\nabla^2$ on a uniform grid results in a $7$-point stencil. For any interior grid point $(i,j,k)$, the discretized equation is:\n$$\n\\frac{\\Phi_{i+1,j,k} - 2\\Phi_{i,j,k} + \\Phi_{i-1,j,k}}{h^2} + \\frac{\\Phi_{i,j+1,k} - 2\\Phi_{i,j,k} + \\Phi_{i,j-1,k}}{h^2} + \\frac{\\Phi_{i,j,k+1} - 2\\Phi_{i,j,k} + \\Phi_{i,j-1,k}}{h^2} = (4 \\pi G \\rho)_{i,j,k}\n$$\nwhere $h$ is the uniform grid spacing. Rearranging the terms, we get:\n$$\n\\Phi_{i-1,j,k} + \\Phi_{i+1,j,k} + \\Phi_{i,j-1,k} + \\Phi_{i,j+1,k} + \\Phi_{i,j,k-1} + \\Phi_{i,j,k+1} - 6\\Phi_{i,j,k} = h^2 (4 \\pi G \\rho)_{i,j,k}\n$$\nThis equation defines row $p$ of the linear system $A x = b$, where $p$ is the linear index corresponding to the grid point $(i,j,k)$, and the vector $x$ contains the unknown potential values $\\Phi$. A non-zero entry $a_{pq}$ exists if the potential at grid point $q$ appears in the equation for grid point $p$. This occurs when $q$ corresponds to the point $(i,j,k)$ itself (the diagonal term) or one of its six nearest neighbors: $(i\\pm1,j,k)$, $(i,j\\pm1,k)$, and $(i,j,k\\pm1)$.\n\nThe given lexicographic mapping from grid coordinates $(i,j,k)$ to a linear index $p$ is:\n$$\np(i,j,k) = i + N(j-1) + N^2(k-1)\n$$\nwhere $i, j, k \\in \\{1, 2, \\dots, N\\}$.\n\nTo find the sparsity pattern, we compute the index differences $\\Delta = p - q$ for all non-zero entries $a_{pq}$. Let $p = p(i,j,k)$. The column index $q$ corresponds to a neighboring point.\n\\begin{itemize}\n    \\item For the neighbor $(i-1,j,k)$, $q = p(i-1,j,k) = (i-1) + N(j-1) + N^2(k-1)$. The difference is $\\Delta = p - q = i - (i-1) = 1$.\n    \\item For the neighbor $(i+1,j,k)$, $q = p(i+1,j,k) = (i+1) + N(j-1) + N^2(k-1)$. The difference is $\\Delta = p - q = i - (i+1) = -1$.\n    \\item For the neighbor $(i,j-1,k)$, $q = p(i,j-1,k) = i + N(j-2) + N^2(k-1)$. The difference is $\\Delta = p - q = N(j-1) - N(j-2) = N$.\n    \\item For the neighbor $(i,j+1,k)$, $q = p(i,j+1,k) = i + N(j) + N^2(k-1)$. The difference is $\\Delta = p - q = N(j-1) - N(j) = -N$.\n    \\item For the neighbor $(i,j,k-1)$, $q = p(i,j,k-1) = i + N(j-1) + N^2(k-2)$. The difference is $\\Delta = p - q = N^2(k-1) - N^2(k-2) = N^2$.\n    \\item For the neighbor $(i,j,k+1)$, $q = p(i,j,k+1) = i + N(j-1) + N^2(k)$. The difference is $\\Delta = p - q = N^2(k-1) - N^2(k) = -N^2$.\n\\end{itemize}\nFor the diagonal entry, $p=q$, so $\\Delta = 0$. Thus, the set of all possible index differences for which $a_{pq}$ can be non-zero is $\\{0, \\pm 1, \\pm N, \\pm N^2\\}$.\n\nNext, we determine the semi-bandwidth $\\beta$, defined such that $a_{pq}=0$ if $|p-q| > \\beta$. This is equivalent to finding the maximum absolute index difference.\n$$\n\\beta = \\max_{a_{pq} \\neq 0} |p-q| = \\max\\{|0|, |\\pm 1|, |\\pm N|, |\\pm N^2|\\}\n$$\nAssuming $N \\ge 1$, we have $N^2 \\ge N \\ge 1$. Therefore, the semi-bandwidth is:\n$$\n\\beta(N) = N^2\n$$\n\nNow, we count the exact total number of nonzero entries, $\\mathrm{nnz}(A)$. The matrix $A$ is of size $N^3 \\times N^3$.\nThe total number of non-zero entries is the sum of diagonal and off-diagonal entries.\nThere is one diagonal entry for each of the $N^3$ grid points, so there are $N^3$ diagonal entries.\nThe off-diagonal entries correspond to connections between adjacent grid points. Due to the symmetry of the Laplacian operator ($a_{pq} = a_{qp}$), each connection (or link) between two points contributes two non-zero entries to the matrix. We count the number of internal connections in the grid.\n\\begin{itemize}\n    \\item Connections along the $i$-axis: in each of the $N^2$ rows of points (one for each $(j,k)$ pair), there are $N-1$ connections. Total $i$-connections: $N^2(N-1)$.\n    \\item Connections along the $j$-axis: in each of the $N^2$ columns of points (one for each $(i,k)$ pair), there are $N-1$ connections. Total $j$-connections: $N^2(N-1)$.\n    \\item Connections along the $k$-axis: in each of the $N^2$ \"files\" of points (one for each $(i,j)$ pair), there are $N-1$ connections. Total $k$-connections: $N^2(N-1)$.\n\\end{itemize}\nThe total number of unique connections is $3 \\times N^2(N-1) = 3N^3 - 3N^2$.\nThe total number of off-diagonal entries is twice the number of connections: $2(3N^3 - 3N^2) = 6N^3 - 6N^2$.\nThe total number of non-zero entries is the sum of diagonal and off-diagonal entries:\n$$\n\\mathrm{nnz}(A) = N^3 + (6N^3 - 6N^2) = 7N^3 - 6N^2\n$$\n\nFinally, we derive the total memory footprint $M(N)$ for storing $A$ in CSR format. The problem specifies the components and data types:\n\\begin{itemize}\n    \\item Value array: length $\\mathrm{nnz}(A)$, stores $8$-byte floating-point numbers.\n    \\item Column-index array: length $\\mathrm{nnz}(A)$, stores $8$-byte integers.\n    \\item Row-pointer array: length $N^3+1$, stores $8$-byte integers.\n\\end{itemize}\nThe total memory $M(N)$ in bytes is the sum of the memory required for each component:\n$$\nM(N) = (\\text{length of value array}) \\times 8 + (\\text{length of column-index array}) \\times 8 + (\\text{length of row-pointer array}) \\times 8\n$$\n$$\nM(N) = \\mathrm{nnz}(A) \\times 8 + \\mathrm{nnz}(A) \\times 8 + (N^3+1) \\times 8\n$$\n$$\nM(N) = 16 \\times \\mathrm{nnz}(A) + 8(N^3+1)\n$$\nSubstituting the expression for $\\mathrm{nnz}(A)$:\n$$\nM(N) = 16(7N^3 - 6N^2) + 8(N^3+1)\n$$\n$$\nM(N) = 112N^3 - 96N^2 + 8N^3 + 8\n$$\n$$\nM(N) = 120N^3 - 96N^2 + 8\n$$\nThis expression gives the total memory footprint in bytes as a function of $N$.\n\nThe final answer is the ordered pair $(\\beta(N), M(N))$.\n$$\n\\beta(N) = N^2\n$$\n$$\nM(N) = 120N^3 - 96N^2 + 8\n$$\nIn the required matrix form, this is $\\begin{pmatrix} N^2  120N^3 - 96N^2 + 8 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nN^2  120N^3 - 96N^2 + 8\n\\end{pmatrix}\n}\n$$", "id": "3515775"}]}