{"hands_on_practices": [{"introduction": "Before applying a numerical algorithm, it's often wise to analyze the structure of the problem itself. This practice explores a powerful technique: changing variables to improve the stability of the root-finding process. You will derive the Newton-Raphson update for a dust-grain equilibrium problem in logarithmic temperature coordinates and analyze why this transformation is so effective, especially for physical quantities that span many orders of magnitude [@problem_id:3532590].", "problem": "A dust grain embedded in an optically thin astrophysical radiation field reaches radiative equilibrium when its net heating equals its net cooling. Under standard assumptions, the heating rate per unit mass, denoted by $H$, depends on the ambient radiation field but is independent of the dust temperature $T$, while the cooling rate per unit mass can be approximated by a power law in temperature due to the Planck-mean opacity scaling, $\\kappa_{\\mathrm{P}}(T) \\propto T^{\\beta}$. Combining this with the Stefan–Boltzmann law for thermal emission leads to an effective cooling rate $C T^{\\alpha}$, where $C0$ is a constant proportional to $4 \\pi \\sigma_{\\mathrm{SB}} \\kappa_{0}$ (with $\\sigma_{\\mathrm{SB}}$ the Stefan–Boltzmann constant and $\\kappa_{0}$ a normalization of the opacity), and $\\alpha=4+\\beta0$. The radiative equilibrium condition therefore reduces to the nonlinear root-finding problem for the scalar residual\n$$\nf(T) \\equiv H - C T^{\\alpha} = 0,\n$$\nwith the physical constraint $T0$. \n\nFor computational robustness in root-finding, a common strategy is to transform multiplicative or power-law residuals using logarithmic variables. Consider the change of variables $y \\equiv \\ln T$, and define the transformed residual $g(y) \\equiv f(\\exp(y))$. Starting only from the equilibrium definition and the above physical modeling assumptions, perform the following:\n\n1. Derive $g(y)$ explicitly in terms of $H$, $C$, $\\alpha$, and $y$. \n2. Using the Newton–Raphson method, derive the explicit update formula in the logarithmic variable,\n$$\ny_{n+1} = y_n - \\frac{g(y_n)}{g'(y_n)}.\n$$\nExpress the result entirely in terms of $y_n$, $H$, $C$, and $\\alpha$.\n3. Briefly reason, using the functional form you obtained, why working in $y=\\ln T$ improves stability relative to updating directly in $T$ for large $\\alpha$ or when $H/C$ spans many orders of magnitude, and comment on how this relates to bracketing strategies such as the bisection method for monotone residuals.\n\nExpress your final answer as a single analytic expression for $y_{n+1}$ in terms of $y_n$, $H$, $C$, and $\\alpha$. No numerical evaluation is required.", "solution": "The problem statement is scientifically grounded, well-posed, and internally consistent. It describes a simplified but standard model for the thermal equilibrium of dust grains in astrophysics, a topic relevant to computational astrophysics. The mathematical task is to analyze a root-finding problem using a change of variables, which is a common and valid numerical technique. All parameters and functions are clearly defined. Therefore, the problem is valid, and a solution can be derived.\n\nThe radiative equilibrium of a dust grain is described by the condition that heating equals cooling. The residual function is given as\n$$\nf(T) = H - C T^{\\alpha} = 0\n$$\nwhere $H$ is the constant heating rate, $C T^{\\alpha}$ is the temperature-dependent cooling rate, and $T  0$ is the dust temperature. The constants $H$, $C$, and $\\alpha$ are all positive.\n\nThe problem requires an analysis based on the change of variables $y \\equiv \\ln T$, which implies $T = \\exp(y)$. The transformed residual function is defined as $g(y) \\equiv f(\\exp(y))$.\n\n1. To derive the explicit form of $g(y)$, we substitute $T = \\exp(y)$ into the expression for $f(T)$:\n$$\ng(y) = f(\\exp(y)) = H - C (\\exp(y))^{\\alpha}\n$$\nUsing the property of exponents, $(\\exp(a))^b = \\exp(ab)$, we obtain:\n$$\ng(y) = H - C \\exp(\\alpha y)\n$$\nThis is the explicit form of the transformed residual in terms of $H$, $C$, $\\alpha$, and $y$.\n\n2. The Newton-Raphson method provides an iterative scheme to find a root of a function. For the variable $y$, the update formula is given by:\n$$\ny_{n+1} = y_n - \\frac{g(y_n)}{g'(y_n)}\n$$\nTo apply this formula, we first need to compute the derivative of $g(y)$ with respect to $y$.\n$$\ng'(y) = \\frac{d}{dy} \\left( H - C \\exp(\\alpha y) \\right)\n$$\nSince $H$ and $C$ are constants, the derivative is:\n$$\ng'(y) = 0 - C \\frac{d}{dy} (\\exp(\\alpha y)) = - C (\\alpha \\exp(\\alpha y)) = -\\alpha C \\exp(\\alpha y)\n$$\nNow, we substitute the expressions for $g(y_n)$ and $g'(y_n)$ into the update formula:\n$$\ny_{n+1} = y_n - \\frac{H - C \\exp(\\alpha y_n)}{-\\alpha C \\exp(\\alpha y_n)}\n$$\nWe can simplify this expression by splitting the fraction:\n$$\ny_{n+1} = y_n + \\frac{H - C \\exp(\\alpha y_n)}{\\alpha C \\exp(\\alpha y_n)} = y_n + \\frac{H}{\\alpha C \\exp(\\alpha y_n)} - \\frac{C \\exp(\\alpha y_n)}{\\alpha C \\exp(\\alpha y_n)}\n$$\nThis simplifies to:\n$$\ny_{n+1} = y_n - \\frac{1}{\\alpha} + \\frac{H}{\\alpha C \\exp(\\alpha y_n)}\n$$\nUsing the property $\\exp(-x) = 1/\\exp(x)$, the final expression for the update formula is:\n$$\ny_{n+1} = y_n - \\frac{1}{\\alpha} + \\frac{H}{\\alpha C} \\exp(-\\alpha y_n)\n$$\nThis formula expresses $y_{n+1}$ entirely in terms of $y_n$ and the constants $H$, $C$, and $\\alpha$.\n\n3. Working with the logarithmic variable $y = \\ln T$ offers significant numerical stability advantages over working directly with $T$.\n\nFirst, consider the nature of the functions. The original residual, $f(T) = H - C T^{\\alpha}$, is a power law. For large $\\alpha$, $f(T)$ changes extremely rapidly with $T$, meaning its second derivative, $f''(T) = -C\\alpha(\\alpha-1)T^{\\alpha-2}$, can be very large. The Newton-Raphson method approximates the function locally with a tangent line. For functions with high curvature, this linear approximation is poor, potentially leading to large, unstable update steps or overshooting the root, especially if the initial guess is not very close to the true root. In contrast, the transformed residual, $g(y) = H - C \\exp(\\alpha y)$, is linear in the quantity $\\exp(\\alpha y)$. Finding the root of $g(y)$ is equivalent to solving $H = C\\exp(\\alpha y)$, or $\\ln(H/C) = \\alpha y$. The problem is transformed into finding the intersection of a horizontal line $z=H$ and an exponential curve $z=C\\exp(\\alpha y)$. On a logarithmic plot, this becomes a linear problem. This \"straighter\" functional form makes the linear approximation of the Newton-Raphson method more accurate over a wider range, leading to more robust and faster convergence.\n\nSecond, the range of possible values for the physical parameters can be vast. The ratio $H/C$ can span many orders of magnitude in astrophysical contexts, causing the root $T_{\\text{eq}} = (H/C)^{1/\\alpha}$ to also vary over a very wide range. Working in logarithmic temperature, $y = \\ln T$, compresses this enormous dynamic range. For example, if $T$ varies from $10\\,\\mathrm{K}$ to $10^5\\,\\mathrm{K}$, $y$ only varies from $\\ln(10) \\approx 2.3$ to $\\ln(10^5) \\approx 11.5$. This compression makes it easier to specify initial guesses and prevents numerical overflow or underflow that can occur when calculating terms like $T^{\\alpha}$ for extreme values of $T$ and large $\\alpha$.\n\nFinally, this relates to bracketing methods like bisection. The original function $f(T)$ is monotonic for $T0$, since $f'(T) = -C\\alpha T^{\\alpha-1}  0$. Similarly, $g(y)$ is monotonic since $g'(y) = -\\alpha C \\exp(\\alpha y)  0$. Monotonicity is a sufficient condition for the bisection method to be guaranteed to converge once a valid bracket (an interval $[a,b]$ where the function has opposite signs at the endpoints) is found. Finding such a bracket for $T$ across potentially many orders of magnitude can be inefficient. By transforming to $y = \\ln T$, the search space is compressed, making it computationally cheaper and more robust to establish an initial bracket $[y_a, y_b]$ for the root. Therefore, the logarithmic transformation benefits not only gradient-based methods like Newton-Raphson but also bracketing methods like bisection.", "answer": "$$\n\\boxed{y_n - \\frac{1}{\\alpha} + \\frac{H}{\\alpha C} \\exp(-\\alpha y_n)}\n$$", "id": "3532590"}, {"introduction": "The quadratic convergence of Newton's method is famously fast, but it relies on the function being sufficiently smooth. What happens when this ideal condition is not met? This practice explores a realistic scenario from plasma physics involving a \"kink,\" where the function is continuous but its derivative jumps. By tracing the iterates by hand, you will see how the method navigates this challenge and understand why robust, safeguarded algorithms are essential in practice [@problem_id:3532631].", "problem": "In modeling radiative equilibrium in an optically thin plasma, the net energy balance can be written as a scalar root-finding problem for the temperature $T$: solve $f(T) = 0$, where $f(T) \\equiv n^{2}\\Lambda(T) - \\Gamma$. Here $n$ is the number density, $\\Lambda(T)$ is the cooling function, and $\\Gamma$ is the volumetric heating rate. A common microphysics closure switches the functional form of $\\Lambda(T)$ across a critical temperature $T_{\\mathrm{c}}$ due to the onset of strong line cooling. Consider the following simplified, piecewise model with parameters chosen so that $f(T)$ is continuous but the derivative has a jump at $T_{\\mathrm{c}}$:\n$$\n\\Lambda(T) = \\begin{cases}\nA\\,T^{\\alpha_{1}},  T  T_{\\mathrm{c}},\\\\[4pt]\nB\\,T^{\\alpha_{2}},  T \\ge T_{\\mathrm{c}},\n\\end{cases}\n\\quad\nB \\equiv A\\,T_{\\mathrm{c}}^{\\alpha_{1}-\\alpha_{2}},\n$$\nwith $A = 1$, $\\alpha_{1} = 1$, $\\alpha_{2} = 3$, $T_{\\mathrm{c}} = 1$, $n = 1$, and $\\Gamma = 0.9$ in consistent units. Then\n$$\nf(T) = \\begin{cases}\nT - 0.9,  T  1,\\\\[4pt]\nT^{3} - 0.9,  T \\ge 1,\n\\end{cases}\n$$\nwhich is continuous at $T = 1$ while $f'(T)$ has a jump there.\n\nSuppose one applies the Newton-Raphson method starting from $T_{0} = 1.2$ and uses exact arithmetic. The bisection method is also available and may be used as a safeguard by maintaining a sign-change bracket $[a,b]$ and rejecting any Newton step that leaves the bracket in favor of a bisection step.\n\nWhich option correctly characterizes the Newton-Raphson iteration across the kink at $T = 1$ for this $f(T)$, and correctly states a valid global convergence safeguard for this class of problems?\n\nA. The Newton-Raphson iterates diverge because the derivative discontinuity at $T = 1$ violates the smoothness needed for convergence; bisection cannot help because $f'(T)$ is discontinuous.\n\nB. The Newton-Raphson iterates are $T_{1} \\approx 1.0083$ (still $ 1$), $T_{2} \\approx 0.9673$ (now $ 1$), and $T_{3} = 0.9$ exactly, so the method converges in $3$ steps from $T_{0} = 1.2$; a safeguarded Newton scheme that reverts to bisection whenever a Newton step would leave a maintained sign-change bracket (for example $[0,2]$) guarantees global convergence for continuous $f$.\n\nC. The Newton-Raphson iterates remain entirely above $T = 1$ and converge quadratically to a root at approximately $T \\approx 1.0$; bracketing is unnecessary because $f$ is monotone.\n\nD. The Newton-Raphson method oscillates indefinitely across $T = 1$ and fails to converge; bisection would also fail because $f(T)$ is not strictly monotone on $(0,\\infty)$.", "solution": "The user wants a critical validation of the problem statement, followed by a full solution and evaluation of the options.\n\n### Step 1: Extract Givens\n- The root-finding problem is to solve $f(T) = 0$.\n- The function is defined as $f(T) \\equiv n^{2}\\Lambda(T) - \\Gamma$.\n- The cooling function $\\Lambda(T)$ is a piecewise function:\n$$\n\\Lambda(T) = \\begin{cases}\nA\\,T^{\\alpha_{1}},  T  T_{\\mathrm{c}},\\\\[4pt]\nB\\,T^{\\alpha_{2}},  T \\ge T_{\\mathrm{c}},\n\\end{cases}\n$$\n- The parameter $B$ is defined to ensure continuity of $f(T)$: $B \\equiv A\\,T_{\\mathrm{c}}^{\\alpha_{1}-\\alpha_{2}}$.\n- The given parameter values are: $A = 1$, $\\alpha_{1} = 1$, $\\alpha_{2} = 3$, $T_{\\mathrm{c}} = 1$, $n = 1$, and $\\Gamma = 0.9$.\n- The resulting specific function is:\n$$\nf(T) = \\begin{cases}\nT - 0.9,  T  1,\\\\[4pt]\nT^{3} - 0.9,  T \\ge 1.\n\\end{cases}\n$$\n- It is stated that $f(T)$ is continuous at $T = 1$ but $f'(T)$ has a jump discontinuity.\n- The Newton-Raphson method is to be applied with the starting guess $T_{0} = 1.2$ using exact arithmetic.\n- A safeguarded method is described: use bisection if a Newton step leaves a sign-change bracket $[a, b]$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding**: The problem presents a simplified but physically motivated model of thermal equilibrium in astrophysical plasma, where cooling balances heating. The use of a piecewise power-law for the cooling function $\\Lambda(T)$ is a standard technique in computational astrophysics simulations to approximate complex microphysics. The \"kink\" at $T_{\\mathrm{c}}$ represents the activation of a new, more efficient cooling channel (e.g., atomic line cooling), which is a realistic physical concept. The problem is scientifically grounded.\n\n2.  **Well-Posedness**: We must verify the provided form of $f(T)$ and its properties.\n    - Using the given parameters: $A=1$, $T_{\\mathrm{c}}=1$, $\\alpha_{1}=1$, $\\alpha_{2}=3$, we find $B = 1 \\cdot (1)^{1-3} = 1$.\n    - For $T  T_{\\mathrm{c}}=1$, $\\Lambda(T) = A\\,T^{\\alpha_1} = 1 \\cdot T^1 = T$.\n    - For $T \\ge T_{\\mathrm{c}}=1$, $\\Lambda(T) = B\\,T^{\\alpha_2} = 1 \\cdot T^3 = T^3$.\n    - With $n=1$ and $\\Gamma=0.9$, the function $f(T) = n^2\\Lambda(T) - \\Gamma$ becomes:\n      $$\n      f(T) = \\begin{cases}\n      T - 0.9,  T  1 \\\\\n      T^3 - 0.9,  T \\ge 1\n      \\end{cases}\n      $$\n      This matches the function provided in the problem statement.\n    - **Continuity**: At $T=1$, the limit from the left is $\\lim_{T \\to 1^-} f(T) = 1 - 0.9 = 0.1$. The value at $T=1$ is $f(1) = 1^3 - 0.9 = 0.1$. Since the limit from the left equals the function value (which is also the limit from the right), $f(T)$ is continuous at $T=1$.\n    - **Derivative Discontinuity**:\n      $$\n      f'(T) = \\begin{cases}\n      1,  T  1 \\\\\n      3T^2,  T  1\n      \\end{cases}\n      $$\n      The left-hand derivative at $T=1$ is $f'(1^-) = 1$. The right-hand derivative is $f'(1^+) = 3(1)^2 = 3$. Since $f'(1^-) \\neq f'(1^+)$, the derivative has a jump discontinuity at $T=1$, as stated.\n    - **Existence of a Root**:\n      - For $T  1$, setting $f(T) = T - 0.9 = 0$ gives $T = 0.9$. Since $0.9  1$, this is a valid root.\n      - For $T \\ge 1$, setting $f(T) = T^3 - 0.9 = 0$ gives $T = \\sqrt[3]{0.9} \\approx 0.965$. This value is not in the domain $T \\ge 1$, so there is no root in this region.\n      - The function has a single, unique root at $T = 0.9$. The problem is well-posed.\n\n3.  **Objectivity, Completeness, etc.**: The problem is stated in precise mathematical terms, provides all necessary data, and does not contain any contradictions or ambiguities. It is a non-trivial but solvable problem testing core concepts of numerical analysis.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will proceed with the solution.\n\n### Derivation and Option Analysis\n\nThe Newton-Raphson iteration is given by the formula $T_{k+1} = T_k - \\frac{f(T_k)}{f'(T_k)}$. We start with $T_0 = 1.2$.\n\n**Iteration 1:**\n- The initial guess is $T_0 = 1.2$. Since $T_0  1$, we use the expressions $f(T) = T^3 - 0.9$ and $f'(T) = 3T^2$.\n- $f(T_0) = f(1.2) = (1.2)^3 - 0.9 = 1.728 - 0.9 = 0.828$.\n- $f'(T_0) = f'(1.2) = 3(1.2)^2 = 3(1.44) = 4.32$.\n- The next iterate is:\n$$\nT_1 = T_0 - \\frac{f(T_0)}{f'(T_0)} = 1.2 - \\frac{0.828}{4.32} = 1.2 - \\frac{23}{120} = \\frac{144}{120} - \\frac{23}{120} = \\frac{121}{120}\n$$\n- As a decimal, $T_1 = 1.008\\overline{3}$. Thus, $T_1 \\approx 1.0083$. The iterate is still greater than $1$.\n\n**Iteration 2:**\n- The current iterate is $T_1 = \\frac{121}{120}  1$. We again use $f(T) = T^3 - 0.9$ and $f'(T) = 3T^2$.\n- $f(T_1) = f(\\frac{121}{120}) = (\\frac{121}{120})^3 - 0.9 = \\frac{1771561}{1728000} - \\frac{9}{10} = \\frac{1771561 - 1555200}{1728000} = \\frac{216361}{1728000}$.\n- $f'(T_1) = f'(\\frac{121}{120}) = 3(\\frac{121}{120})^2 = 3 \\cdot \\frac{14641}{14400} = \\frac{43923}{14400}$.\n- The next iterate is:\n$$\nT_2 = T_1 - \\frac{f(T_1)}{f'(T_1)} = \\frac{121}{120} - \\frac{216361/1728000}{43923/14400} = \\frac{121}{120} - \\frac{216361}{1728000} \\cdot \\frac{14400}{43923}\n$$\n- Simplifying the fraction: $\\frac{1728000}{14400} = 120$. So, $T_2 = \\frac{121}{120} - \\frac{216361}{120 \\cdot 43923}$.\n- To confirm the approximate value from the options, we use decimals:\n  $T_1 \\approx 1.008333$.\n  $f(T_1) \\approx (1.008333)^3 - 0.9 \\approx 1.02521 - 0.9 = 0.12521$.\n  $f'(T_1) \\approx 3(1.008333)^2 \\approx 3(1.016736) = 3.05021$.\n  $T_2 \\approx 1.008333 - \\frac{0.12521}{3.05021} \\approx 1.008333 - 0.04105 \\approx 0.96728$.\n- So, $T_2 \\approx 0.9673$. This iterate has crossed the discontinuity, as $T_2  1$.\n\n**Iteration 3:**\n- The current iterate is $T_2$, which is less than $1$. We must now switch to the expressions for $T  1$: $f(T) = T - 0.9$ and $f'(T) = 1$.\n- $f(T_2) = T_2 - 0.9$.\n- $f'(T_2) = 1$.\n- The next iterate is:\n$$\nT_3 = T_2 - \\frac{f(T_2)}{f'(T_2)} = T_2 - \\frac{T_2 - 0.9}{1} = T_2 - (T_2 - 0.9) = 0.9\n$$\n- $T_3$ is calculated to be exactly $0.9$. This is the root of the function, since $f(0.9) = 0.9 - 0.9 = 0$. The method has converged in exactly $3$ steps.\n\n**Analysis of the Safeguard:**\n- The proposed safeguard involves reverting to bisection if a Newton step goes outside a maintained sign-change bracket $[a, b]$.\n- The bisection method's convergence is guaranteed by the Intermediate Value Theorem for any function that is continuous on $[a, b]$ and for which $f(a)f(b)  0$.\n- The function $f(T)$ is continuous everywhere, so this condition is met. The discontinuity of the derivative $f'(T)$ is irrelevant for the convergence guarantee of the bisection method itself.\n- Therefore, a hybrid scheme that uses bisection as a fallback is a valid and robust method to guarantee global convergence for this class of problems (continuous functions), provided a suitable initial bracket can be found.\n- For the example bracket $[0, 2]$:\n  - $f(0) = 0 - 0.9 = -0.9$.\n  - $f(2) = 2^3 - 0.9 = 8 - 0.9 = 7.1$.\n  - Since $f(0)  0$ and $f(2)  0$, $[0, 2]$ is a valid sign-change bracket.\n\nNow we evaluate the given options.\n\n**Option A:** \"The Newton-Raphson iterates diverge because the derivative discontinuity at $T = 1$ violates the smoothness needed for convergence; bisection cannot help because $f'(T)$ is discontinuous.\"\n- This is incorrect. The Newton-Raphson iterates converge, as shown by direct calculation. Furthermore, the bisection method's convergence depends on the continuity of $f(T)$, not $f'(T)$.\n\n**Option B:** \"The Newton-Raphson iterates are $T_{1} \\approx 1.0083$ (still $ 1$), $T_{2} \\approx 0.9673$ (now $ 1$), and $T_{3} = 0.9$ exactly, so the method converges in $3$ steps from $T_{0} = 1.2$; a safeguarded Newton scheme that reverts to bisection whenever a Newton step would leave a maintained sign-change bracket (for example $[0,2]$) guarantees global convergence for continuous $f$.\"\n- This is correct. The calculated iterates match the values given. The method converges in $3$ steps as claimed. The description of the safeguarded scheme and its convergence guarantee for continuous functions is also correct.\n\n**Option C:** \"The Newton-Raphson iterates remain entirely above $T = 1$ and converge quadratically to a root at approximately $T \\approx 1.0$; bracketing is unnecessary because $f$ is monotone.\"\n- This is incorrect. The iterate $T_2$ is less than $1$. The root is exactly at $T=0.9$, not approximately at $T \\approx 1.0$. The convergence is not quadratic across the kink at $T=1$. Although the function is monotone, this does not universally guarantee convergence of Newton's method from any starting point, making a safeguard like bracketing a valuable tool for robustness.\n\n**Option D:** \"The Newton-Raphson method oscillates indefinitely across $T = 1$ and fails to converge; bisection would also fail because $f(T)$ is not strictly monotone on $(0,\\infty)$.\"\n- This is incorrect. The method converges in $3$ steps; it does not oscillate indefinitely. Bisection does not require monotonicity to converge, only continuity and a sign-change bracket. Moreover, this specific function $f(T)$ *is* strictly monotone on $(0, \\infty)$ since its derivative is positive where defined ($f'(T)=1$ for $T1$ and $f'(T)=3T^20$ for $T1$).\n\nBased on the analysis, only option B is correct.", "answer": "$$\\boxed{B}$$", "id": "3532631"}, {"introduction": "This final practice takes you from pencil-and-paper analysis to full computational implementation, a core skill in modern astrophysics. Your task is to solve the celebrated Kepler's equation using the Newton-Raphson method, but with a twist: you will write code to systematically measure and quantify its performance. By exploring how the number of iterations changes with orbital eccentricity, you will connect the abstract theory of convergence to concrete, measurable outcomes in a foundational problem of celestial mechanics [@problem_id:3532652].", "problem": "Consider the classical orbital relation known as Kepler’s equation for an elliptical orbit with eccentricity $e \\in [0,1)$, which relates the mean anomaly $M \\in [0,2\\pi]$ (in radians) to the eccentric anomaly $E \\in [0,2\\pi]$ via\n$$\nM = E - e \\sin E.\n$$\nDefine the scalar function $f(E; e, M) = E - e \\sin E - M$. For each fixed pair $(e,M)$ with $e \\in [0,1)$ and $M \\in [0,2\\pi]$, there is a unique root $E^\\star$ of $f(E; e, M) = 0$. The Newton–Raphson method is the iteration\n$$\nE_{n+1} = E_n - \\frac{f(E_n; e, M)}{f'(E_n; e, M)}, \\quad f'(E; e, M) = 1 - e \\cos E,\n$$\napplied to $f(E; e, M) = 0$. Use the classical initial guess\n$$\nE_0 = M + \\operatorname{sign}(\\sin M)\\,\\min\\left\\{\\frac{\\pi}{2},\\,e\\right\\}.\n$$\nFor the purposes of termination, define the machine precision threshold as the IEEE (Institute of Electrical and Electronics Engineers) $754$ double-precision machine epsilon $\\varepsilon_{\\mathrm{mach}}$, and say that the Newton–Raphson iteration has “reached machine precision” for a given $(e,M)$ when the absolute residual satisfies\n$$\n|f(E_n; e, M)| \\le \\varepsilon_{\\mathrm{mach}}.\n$$\nYour task is to quantify the expected number of Newton–Raphson iterations required to reach machine precision, where the expectation is taken over a uniform distribution of mean anomalies $M$ on $[0,2\\pi]$.\n\nImplement a complete, runnable program that:\n- Computes, for each specified $e$, the expected number of Newton–Raphson iterations needed to reach $|f(E_n; e, M)| \\le \\varepsilon_{\\mathrm{mach}}$, starting from $E_0$ as above, averaged over a discrete uniform grid of $M$ values.\n- Uses a deterministic grid of $M$ values: $M_k = \\frac{2\\pi k}{N-1}$ for $k=0,1,\\dots,N-1$, with $N=2001$.\n- Counts the number of Newton–Raphson iterations as the number of updates $E_{n+1}$ computed until the stopping condition $|f(E_n; e, M)| \\le \\varepsilon_{\\mathrm{mach}}$ is first satisfied; if the initial guess $E_0$ already satisfies the condition, record $0$ iterations for that $(e,M)$.\n- Uses a maximum iteration cap of $100$ iterations per $(e,M)$ pair to avoid infinite loops; if the cap is reached without meeting the residual criterion, record $100$ iterations for that $(e,M)$.\n\nTest Suite:\n- Evaluate the expected iteration count for the following eccentricities (dimensionless): $e \\in \\{0,\\,0.1,\\,0.5,\\,0.9,\\,0.99,\\,0.999999\\}$.\n- Angles must be treated in radians.\n- The expected iteration count for each $e$ must be reported as a floating-point number rounded to $6$ decimal places.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite for $e$. For example, if the expected iteration counts are $x_1, x_2, x_3, x_4, x_5, x_6$, the output must be\n$$\n[x_1,x_2,x_3,x_4,x_5,x_6].\n$$\nEach $x_i$ must be rounded to $6$ decimal places.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in celestial mechanics, mathematically well-posed, and all variables, conditions, and objectives are specified with sufficient precision. No contradictions, ambiguities, or unsound premises are present. We may therefore proceed with a solution.\n\nThe problem requires us to solve the transcendental equation known as Kepler's equation, $M = E - e \\sin E$, for the eccentric anomaly $E$, given the mean anomaly $M$ and the orbital eccentricity $e$. This is a canonical problem in computational astrophysics and celestial mechanics. We are tasked with quantifying the performance of the Newton-Raphson method for this problem by calculating the expected number of iterations required for convergence, averaged over a uniform distribution of mean anomalies $M \\in [0, 2\\pi]$.\n\nFirst, we formulate the problem as a root-finding task. We define the function $f(E; e, M)$ such that its root corresponds to the solution of Kepler's equation:\n$$\nf(E; e, M) = E - e \\sin E - M\n$$\nFinding $E$ such that $f(E; e, M) = 0$ will give us the desired eccentric anomaly.\n\nThe Newton-Raphson method is an iterative root-finding algorithm that generates a sequence of approximations $\\{E_n\\}$ that converge to the root $E^\\star$. The iterative formula is given by:\n$$\nE_{n+1} = E_n - \\frac{f(E_n; e, M)}{f'(E_n; e, M)}\n$$\nTo apply this formula, we must compute the first derivative of $f(E; e, M)$ with respect to $E$:\n$$\nf'(E; e, M) = \\frac{d}{dE}(E - e \\sin E - M) = 1 - e \\cos E\n$$\nFor the specified domain $e \\in [0, 1)$, the derivative $f'(E; e, M)$ is always positive, as $|\\cos E| \\le 1$, which ensures that $1 - e \\cos E  0$. This guarantees that the denominator in the Newton-Raphson update is never zero and that the function $f(E)$ is strictly monotonic, confirming that a unique root exists.\n\nThe computational procedure is implemented as follows:\n1.  The calculation is performed for each eccentricity $e$ in the specified test suite: $\\{0, 0.1, 0.5, 0.9, 0.99, 0.999999\\}$.\n2.  To approximate the expectation over a uniform distribution of $M$, we use a deterministic grid of $N=2001$ points for the mean anomaly: $M_k = \\frac{2\\pi k}{N-1}$ for $k \\in \\{0, 1, \\dots, N-1\\}$.\n3.  For each pair of $(e, M_k)$, we execute the Newton-Raphson algorithm to find the number of iterations required for convergence.\n    a. The iteration starts with the initial guess $E_0$ provided by the classical formula:\n    $$\n    E_0 = M_k + \\operatorname{sign}(\\sin M_k) \\min\\left\\{\\frac{\\pi}{2}, e\\right\\}\n    $$\n    b. An iteration counter, $n$, is initialized to $0$.\n    c. A loop proceeds for a maximum of $100$ iterations. In each step, indexed by $n$, we evaluate the absolute residual $|f(E_n; e, M_k)|$.\n    d. The loop terminates if the stopping criterion $|f(E_n; e, M_k)| \\le \\varepsilon_{\\mathrm{mach}}$ is satisfied, where $\\varepsilon_{\\mathrm{mach}}$ is the IEEE $754$ double-precision machine epsilon. The number of iterations recorded for this case is $n$, which represents the number of updates performed. If the condition is met for $E_0$, the count is $n=0$.\n    e. If the criterion is not met, the next iterate $E_{n+1}$ is calculated, and the counter is incremented.\n    f. If the loop reaches the maximum of $100$ updates without meeting the stopping criterion, the iteration count for that $(e, M_k)$ pair is recorded as $100$.\n4.  After running the algorithm for all $N$ values of $M_k$ for a fixed $e$, the total number of iterations is summed and divided by $N$ to find the average iteration count.\n5.  The final average for each $e$ is rounded to $6$ decimal places and formatted as specified.\n\nThe case $e=0$ is a trivial base case. The equation becomes $M=E$. The initial guess is $E_0 = M + \\operatorname{sign}(\\sin M) \\min\\{\\pi/2, 0\\} = M$. The residual is $|f(E_0)| = |M - 0 \\cdot \\sin(M) - M| = 0$, so the iteration count is $0$ for all $M$. The expected iteration count is therefore exactly $0$. For increasing $e$ approaching $1$, the problem becomes more challenging, and a higher number of iterations is expected, particularly for $M$ near $0$ or $2\\pi$. The provided program implements this logic precisely.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the expected number of Newton-Raphson iterations to solve Kepler's\n    equation, averaged over a grid of mean anomalies for several eccentricities.\n    \"\"\"\n    # Define the test cases and parameters from the problem statement.\n    test_cases = [0.0, 0.1, 0.5, 0.9, 0.99, 0.999999]\n    N = 2001\n    MAX_ITER = 100\n    EPS_MACH = np.finfo(float).eps\n\n    # Create the deterministic grid of mean anomaly M values.\n    # The grid spans [0, 2*pi] inclusive.\n    M_grid = np.linspace(0.0, 2.0 * np.pi, N)\n\n    results = []\n    \n    # Loop over each eccentricity value in the test suite.\n    for e in test_cases:\n        total_iterations = 0\n        \n        # Loop over each mean anomaly M in the grid.\n        for M in M_grid:\n            # Calculate the classical initial guess for E.\n            # E_0 = M + sign(sin(M)) * min(pi/2, e)\n            E_n = M + np.sign(np.sin(M)) * min(np.pi / 2.0, e)\n            \n            iter_count = 0\n            \n            # Newton-Raphson iteration loop.\n            # The loop runs a maximum of MAX_ITER times.\n            while iter_count  MAX_ITER:\n                # Calculate the function f(E_n) = E_n - e*sin(E_n) - M.\n                # The absolute value of f(E_n) is the residual.\n                f_E = E_n - e * np.sin(E_n) - M\n                \n                # Check the stopping condition against machine epsilon.\n                if np.abs(f_E) = EPS_MACH:\n                    break  # Convergence achieved.\n                \n                # Calculate the derivative f'(E_n) = 1 - e*cos(E_n).\n                fp_E = 1.0 - e * np.cos(E_n)\n                \n                # For e in [0, 1), fp_E is always positive, so no division by zero.\n                # A safeguard is not strictly necessary but good practice in general.\n                if fp_E == 0.0:\n                    iter_count = MAX_ITER\n                    break\n\n                # Perform the Newton-Raphson update to get E_{n+1}.\n                E_n = E_n - f_E / fp_E\n                iter_count += 1\n            \n            # Add the final iteration count for this (e, M) pair to the total.\n            # If the loop finished due to reaching MAX_ITER, iter_count will be 100.\n            total_iterations += iter_count\n\n        # Calculate the average number of iterations for the current eccentricity.\n        expected_iterations = total_iterations / N\n        # Format the result to 6 decimal places as required.\n        results.append(f\"{expected_iterations:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3532652"}]}