{"hands_on_practices": [{"introduction": "This practice provides a foundational exercise in the mathematics of stochastic gene expression. To quantify and distinguish sources of noise, we first need a robust mathematical framework. This exercise guides you through a derivation using the law of total variance to decompose the total variation in messenger RNA (mRNA) counts into components arising from distinct biological phenomena, providing a quantitative signature of transcriptional bursting. [@problem_id:3348983]", "problem": "Consider a single-gene messenger ribonucleic acid (mRNA) count $X$ measured at steady state across a population of isogenic cells. Assume that $X$ follows a negative binomial distribution $\\mathrm{NB}(r,p)$ arising from transcriptional bursting, with parameters $r>0$ and $0&lt;p&lt;1$, such that the mean is $m = r(1-p)/p$. A widely used mechanistic representation of this distribution in computational systems biology is the Poisson–Gamma mixture: conditional on a random effective rate $\\Lambda$, the count is $X \\mid \\Lambda \\sim \\mathrm{Poisson}(\\Lambda)$, and the rate is $\\Lambda \\sim \\mathrm{Gamma}(r,\\theta)$ with shape $r$ and scale $\\theta$, where $\\theta$ is chosen to match the mean of $X$. Use only the law of total expectation and the law of total variance, together with the properties of the Poisson and Gamma distributions, to derive an explicit closed-form expression for the squared coefficient of variation $\\mathrm{CV}^{2}$ of $X$ in terms of $m$ and $r$ only, where $\\mathrm{CV}^{2} \\equiv \\mathrm{Var}(X)/(\\mathbb{E}[X])^{2}$. Identify, within your derivation, the term corresponding to the excess variance beyond a purely Poisson birth–death process and briefly justify how it is consistent with bursty production in this mechanistic picture. Express your final answer as a single algebraic expression in $m$ and $r$ only. No units are required.", "solution": "The problem statement is deemed valid as it is scientifically grounded in the established theory of stochastic gene expression, is well-posed with all necessary information provided, and is expressed in objective, formal language. It presents a standard, verifiable derivation within the field of computational systems biology.\n\nThe primary objective is to derive the squared coefficient of variation, $\\mathrm{CV}^{2} = \\mathrm{Var}(X)/(\\mathbb{E}[X])^{2}$, for the mRNA count $X$ in terms of its mean $m$ and the Gamma distribution's shape parameter $r$. The model for $X$ is a Poisson-Gamma mixture, representing transcriptional bursting. The derivation will rely exclusively on the law of total expectation and the law of total variance, along with the fundamental properties of the Poisson and Gamma distributions.\n\nFirst, we recall the necessary properties of the distributions and the laws of probability.\nFor a random variable $Y$ following a Poisson distribution, $Y \\sim \\mathrm{Poisson}(\\lambda)$, its expectation and variance are:\n$\\mathbb{E}[Y] = \\lambda$\n$\\mathrm{Var}(Y) = \\lambda$\n\nFor a random variable $\\Lambda$ following a Gamma distribution, $\\Lambda \\sim \\mathrm{Gamma}(r, \\theta)$, with shape parameter $r$ and scale parameter $\\theta$, its expectation and variance are:\n$\\mathbb{E}[\\Lambda] = r\\theta$\n$\\mathrm{Var}(\\Lambda) = r\\theta^{2}$\n\nThe laws of total expectation and total variance for a random variable $X$ conditioned on another random variable $\\Lambda$ are:\nLaw of Total Expectation: $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X \\mid \\Lambda]]$\nLaw of Total Variance: $\\mathrm{Var}(X) = \\mathbb{E}[\\mathrm{Var}(X \\mid \\Lambda)] + \\mathrm{Var}(\\mathbb{E}[X \\mid \\Lambda])$\n\nThe problem states that $X \\mid \\Lambda \\sim \\mathrm{Poisson}(\\Lambda)$ and $\\Lambda \\sim \\mathrm{Gamma}(r, \\theta)$.\n\nStep 1: Calculate the mean of $X$, $\\mathbb{E}[X]$.\nUsing the law of total expectation:\n$$\n\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X \\mid \\Lambda]]\n$$\nSince $X \\mid \\Lambda \\sim \\mathrm{Poisson}(\\Lambda)$, the inner expectation is $\\mathbb{E}[X \\mid \\Lambda] = \\Lambda$. Substituting this into the equation gives:\n$$\n\\mathbb{E}[X] = \\mathbb{E}[\\Lambda]\n$$\nSince $\\Lambda \\sim \\mathrm{Gamma}(r, \\theta)$, its expectation is $\\mathbb{E}[\\Lambda] = r\\theta$. Therefore, the mean of $X$ is:\n$$\n\\mathbb{E}[X] = r\\theta\n$$\nThe problem defines the mean of $X$ as $m$. Thus, we have the relationship $m = r\\theta$. This allows us to express the scale parameter $\\theta$ in terms of the given parameters $m$ and $r$:\n$$\n\\theta = \\frac{m}{r}\n$$\n\nStep 2: Calculate the variance of $X$, $\\mathrm{Var}(X)$.\nUsing the law of total variance:\n$$\n\\mathrm{Var}(X) = \\mathbb{E}[\\mathrm{Var}(X \\mid \\Lambda)] + \\mathrm{Var}(\\mathbb{E}[X \\mid \\Lambda])\n$$\nWe evaluate each term separately.\n\nFor the first term, $\\mathbb{E}[\\mathrm{Var}(X \\mid \\Lambda)]$:\nSince $X \\mid \\Lambda \\sim \\mathrm{Poisson}(\\Lambda)$, the conditional variance is $\\mathrm{Var}(X \\mid \\Lambda) = \\Lambda$. Taking the expectation over $\\Lambda$ gives:\n$$\n\\mathbb{E}[\\mathrm{Var}(X \\mid \\Lambda)] = \\mathbb{E}[\\Lambda]\n$$\nWe already established that $\\mathbb{E}[\\Lambda] = m$. So, the first term is:\n$$\n\\mathbb{E}[\\mathrm{Var}(X \\mid \\Lambda)] = m\n$$\n\nFor the second term, $\\mathrm{Var}(\\mathbb{E}[X \\mid \\Lambda)]$:\nThe conditional expectation is $\\mathbb{E}[X \\mid \\Lambda] = \\Lambda$. Therefore, we need to compute the variance of $\\Lambda$:\n$$\n\\mathrm{Var}(\\mathbb{E}[X \\mid \\Lambda]) = \\mathrm{Var}(\\Lambda)\n$$\nSince $\\Lambda \\sim \\mathrm{Gamma}(r, \\theta)$, its variance is $\\mathrm{Var}(\\Lambda) = r\\theta^{2}$. We now substitute $\\theta = m/r$ to express this term using $m$ and $r$:\n$$\n\\mathrm{Var}(\\Lambda) = r \\left(\\frac{m}{r}\\right)^2 = r \\frac{m^2}{r^2} = \\frac{m^2}{r}\n$$\n\nCombining the two terms, the total variance of $X$ is:\n$$\n\\mathrm{Var(X)} = m + \\frac{m^2}{r}\n$$\n\nStep 3: Calculate the squared coefficient of variation, $\\mathrm{CV}^2$.\nThe definition is $\\mathrm{CV}^{2} = \\mathrm{Var}(X) / (\\mathbb{E}[X])^{2}$. We have $\\mathbb{E}[X]=m$ and $\\mathrm{Var}(X) = m + m^2/r$.\n$$\n\\mathrm{CV}^{2} = \\frac{m + \\frac{m^2}{r}}{m^2}\n$$\nWe can separate this into two fractions:\n$$\n\\mathrm{CV}^{2} = \\frac{m}{m^2} + \\frac{\\frac{m^2}{r}}{m^2} = \\frac{1}{m} + \\frac{1}{r}\n$$\nThis is the final expression for the squared coefficient of variation in terms of $m$ and $r$.\n\nIdentification and Justification of Excess Variance:\nThe variance of $X$ is given by $\\mathrm{Var}(X) = m + m^{2}/r$. If the mRNA production were a simple, non-bursty birth-death process, the steady-state number of molecules would follow a Poisson distribution. For a Poisson process with mean $m$, the variance would be exactly $m$. This corresponds to the first term in our derived variance. The term $m = \\mathbb{E}[\\Lambda] = \\mathbb{E}[\\mathrm{Var}(X|\\Lambda)]$ represents the \"intrinsic noise,\" which arises from the inherent stochasticity of individual mRNA production and degradation events, averaged over all possible effective transcription rates.\n\nThe second term, $m^{2}/r = \\mathrm{Var}(\\Lambda) = \\mathrm{Var}(\\mathbb{E}[X|\\Lambda])$, represents the \"excess variance\" beyond what is expected from a simple Poisson process. This term is a direct consequence of the variability in the transcription rate $\\Lambda$ itself. In the mechanistic picture of transcriptional bursting, the gene promoter switches stochastically between an active state where mRNA is produced in bursts, and an inactive state. The random variable $\\Lambda$ models this effective rate that fluctuates from cell to cell (and over time). The variance of $\\Lambda$ thus quantifies the noise contribution from these upstream promoter dynamics. This is often termed \"extrinsic noise\" relative to the synthesis/degradation process itself. The parameter $r$ is inversely related to the noisiness of this process; a small $r$ implies infrequent, large bursts, which increases the variability of $\\Lambda$ and thus the overall cell-to-cell variability in mRNA counts, consistent with the $1/r$ dependence. This term is therefore a signature of the bursty nature of gene expression.", "answer": "$$\n\\boxed{\\frac{1}{m} + \\frac{1}{r}}\n$$", "id": "3348983"}, {"introduction": "With the theoretical decomposition of variance in hand, a critical practical question arises: what kind of experiment allows us to measure these different components? This problem explores the crucial concept of parameter identifiability, challenging you to determine which experimental designs are sufficient to untangle the contributions of intrinsic fluctuations, cell-wide extrinsic factors, and technical measurement error. Understanding these principles is essential for designing informative single-cell experiments. [@problem_id:3348928]", "problem": "In a single-cell gene expression assay, let the measured quantity per cell be the scalar random variable $X$, modeled as $X=\\tilde{X}+\\epsilon$, where $\\tilde{X}$ denotes the true cellular expression level and $\\epsilon$ denotes measurement noise. Assume $\\epsilon$ is independent of $\\tilde{X}$ and has zero mean. Biological variability in $\\tilde{X}$ arises from two sources: intrinsic noise due to the stochasticity of the molecular reactions within a cell, and extrinsic noise due to cell-to-cell variation in slowly varying global states. A standard way to formalize this is to posit a latent extrinsic state $Z$ for each cell, and to define intrinsic and extrinsic contributions through conditional moments with respect to $Z$, so that the intrinsic contribution depends on the conditional spread of $\\tilde{X}$ given $Z$, and the extrinsic contribution depends on the variability across cells in the conditional mean of $\\tilde{X}$ given $Z$. The total biological variance $\\mathrm{Var}(\\tilde{X})$ decomposes into an intrinsic component $\\mathrm{Var}_{\\mathrm{int}}(\\tilde{X})$ and an extrinsic component $\\mathrm{Var}_{\\mathrm{ext}}(\\tilde{X})$ in accordance with these definitions. The measurement noise variance is $\\mathrm{Var}(\\epsilon)$.\n\nYou are given single-cell data under experimental designs in which either $1$ reporter or $2$ reporters are measured per cell, and possibly repeated measurements over time, with or without external calibration of the measurement noise. Under the modeling assumptions above (including independence of $\\epsilon$ from $\\tilde{X}$), which of the following options correctly specifies a design and accompanying assumptions that are sufficient to identify, from the observed data, each of $\\mathrm{Var}_{\\mathrm{int}}(\\tilde{X})$, $\\mathrm{Var}_{\\mathrm{ext}}(\\tilde{X})$, and $\\mathrm{Var}(\\epsilon)$? Select all that apply.\n\nA. Single snapshot, single reporter per cell, no external calibration. Fit a flexible parametric family to the across-cell distribution of $X$; with the independence of $\\epsilon$ from $\\tilde{X}$, use the empirical mean and variance of $X$ to uniquely recover $\\mathrm{Var}_{\\mathrm{int}}(\\tilde{X})$, $\\mathrm{Var}_{\\mathrm{ext}}(\\tilde{X})$, and $\\mathrm{Var}(\\epsilon)$.\n\nB. Dual-reporter snapshot: in each cell, measure $2$ independent and identically distributed reporters driven by the same promoter, yielding $X_1=\\tilde{X}_1+\\epsilon_1$ and $X_2=\\tilde{X}_2+\\epsilon_2$, with $\\epsilon_1,\\epsilon_2$ independent of $\\tilde{X}_1,\\tilde{X}_2$ and of each other, and with $\\mathrm{Var}(\\epsilon_1)=\\mathrm{Var}(\\epsilon_2)$ known from an external calibration. Assume that, conditional on the latent extrinsic state $Z$, the intrinsic fluctuations of the two reporters are independent and identically distributed, and that $Z$ is constant across the two reporters in the same cell. Under these conditions, the three variances are identifiable from the joint distribution of $(X_1,X_2)$.\n\nC. Dual-reporter snapshot as in option B, but without any external calibration of measurement noise. The three variances remain identifiable from the joint distribution of $(X_1,X_2)$ by combining the across-cell covariance and the across-cell variances of $X_1$ and $X_2$.\n\nD. Time-lapse single-reporter per cell: in each cell, measure the same reporter at $2$ time points, $X^{(1)}=\\tilde{X}^{(1)}+\\epsilon^{(1)}$ and $X^{(2)}=\\tilde{X}^{(2)}+\\epsilon^{(2)}$, with $\\epsilon^{(1)},\\epsilon^{(2)}$ independent of $\\tilde{X}^{(1)},\\tilde{X}^{(2)}$ and of each other, and $\\mathrm{Var}(\\epsilon^{(1)})=\\mathrm{Var}(\\epsilon^{(2)})$ known from external calibration. Assume the time lag is long compared to the intrinsic correlation time so that, conditional on the same latent extrinsic state $Z$ (which does not change over the lag), the intrinsic components at the two time points are independent and identically distributed. Under these conditions, the three variances are identifiable from the joint distribution of $(X^{(1)},X^{(2)})$.\n\nE. Single snapshot, single reporter per cell, but with $2$ independent technical replicate measurements of the same fixed sample per cell, so that for each cell one observes $X^{(a)}=\\tilde{X}+\\epsilon^{(a)}$ and $X^{(b)}=\\tilde{X}+\\epsilon^{(b)}$, with $\\epsilon^{(a)},\\epsilon^{(b)}$ independent and identically distributed and independent of $\\tilde{X}$. From these data, estimate $\\mathrm{Var}(\\epsilon)$ by within-cell replicate differences, then subtract it from the across-cell variance of $X$ to obtain $\\mathrm{Var}_{\\mathrm{ext}}(\\tilde{X})$, and finally recover $\\mathrm{Var}_{\\mathrm{int}}(\\tilde{X})$ by subtracting $\\mathrm{Var}_{\\mathrm{ext}}(\\tilde{X})$ from $\\mathrm{Var}(\\tilde{X})$.", "solution": "The problem statement asks to identify experimental designs and associated assumptions that permit the unique identification of three variance components of gene expression: intrinsic variance ($\\mathrm{Var}_{\\mathrm{int}}(\\tilde{X})$), extrinsic variance ($\\mathrm{Var}_{\\mathrm{ext}}(\\tilde{X})$), and measurement noise variance ($\\mathrm{Var}(\\epsilon)$).\n\nFirst, the core model and definitions are validated.\n**Givens:**\n1.  Measured expression: $X = \\tilde{X} + \\epsilon$.\n2.  $\\tilde{X}$ is the true expression level; $\\epsilon$ is measurement noise.\n3.  Assumptions: $\\epsilon$ is independent of $\\tilde{X}$, and $\\mathrm{E}[\\epsilon] = 0$.\n4.  A latent extrinsic state $Z$ is posited.\n5.  Intrinsic variance is defined based on the conditional spread of $\\tilde{X}$ given $Z$. Extrinsic variance is defined based on the variability of the conditional mean of $\\tilde{X}$ given $Z$.\n6.  The total biological variance decomposes as $\\mathrm{Var}(\\tilde{X}) = \\mathrm{Var}_{\\mathrm{int}}(\\tilde{X}) + \\mathrm{Var}_{\\mathrm{ext}}(\\tilde{X})$.\n7.  The measurement noise variance is $\\mathrm{Var}(\\epsilon)$.\n\n**Validation:**\n- The given model is a standard representation in computational systems biology for analyzing single-cell expression data.\n- The decomposition of noise into intrinsic and extrinsic components, formalized using a latent variable $Z$ and the law of total variance, is the canonical framework established by Elowitz et al. (2002). Specifically, the law of total variance states $\\mathrm{Var}(\\tilde{X}) = \\mathrm{E}[\\mathrm{Var}(\\tilde{X}|Z)] + \\mathrm{Var}(\\mathrm{E}[\\tilde{X}|Z])$. The problem correctly maps these terms to the definitions of intrinsic and extrinsic variance, respectively:\n    $$ \\mathrm{Var}_{\\mathrm{int}}(\\tilde{X}) = \\mathrm{E}[\\mathrm{Var}(\\tilde{X}|Z)] $$\n    $$ \\mathrm{Var}_{\\mathrm{ext}}(\\tilde{X}) = \\mathrm{Var}(\\mathrm{E}[\\tilde{X}|Z]) $$\n- The independence of measurement noise $\\epsilon$ from the true biological state $\\tilde{X}$ is a standard and necessary assumption for such analyses.\n- The total observed variance is given by $\\mathrm{Var}(X) = \\mathrm{Var}(\\tilde{X} + \\epsilon) = \\mathrm{Var}(\\tilde{X}) + \\mathrm{Var}(\\epsilon)$ due to their independence. Substituting the biological variance decomposition, we get:\n    $$ \\mathrm{Var}(X) = \\mathrm{Var}_{\\mathrm{int}}(\\tilde{X}) + \\mathrm{Var}_{\\mathrm{ext}}(\\tilde{X}) + \\mathrm{Var}(\\epsilon) $$\n- The problem is scientifically grounded, well-posed, and objective. It asks a clear question about parameter identifiability under different experimental scenarios, which is a non-trivial and fundamental challenge in the field. The problem statement is valid.\n\nLet us denote the three unknown quantities as $V_{int} = \\mathrm{Var}_{\\mathrm{int}}(\\tilde{X})$, $V_{ext} = \\mathrm{Var}_{\\mathrm{ext}}(\\tilde{X})$, and $V_{meas} = \\mathrm{Var}(\\epsilon)$. The goal is to find an experimental design that provides enough independent equations from observable data moments (variances, covariances) to uniquely solve for these three unknowns.\n\n**A. Single snapshot, single reporter per cell, no external calibration.**\n- This design yields a single random variable $X$ per cell. From a population of cells, we can estimate the moments of $X$, most importantly its variance, $\\mathrm{Var}(X)$.\n- We have a single equation derived from second moments: $\\mathrm{Var}(X) = V_{int} + V_{ext} + V_{meas}$.\n- There are three unknowns ($V_{int}$, $V_{ext}$, $V_{meas}$) but only one equation. The system is underdetermined.\n- The option suggests that fitting a parametric family can resolve this. This is generally false. For instance, if $\\tilde{X}$ and $\\epsilon$ were both Gaussian, $X$ would also be Gaussian. The distribution of $X$ is described by its mean and variance. The variance only provides the sum of the three components, not the components themselves. Without strong, and likely unjustifiable, assumptions about the specific functional forms of the distributions, a unique decomposition is impossible.\n- **Verdict: Incorrect.**\n\n**B. Dual-reporter snapshot with external calibration.**\n- The design provides two measurements per cell, $(X_1, X_2)$.\n- The model is $X_1 = \\tilde{X}_1 + \\epsilon_1$ and $X_2 = \\tilde{X}_2 + \\epsilon_2$.\n- Key assumptions:\n    - $\\epsilon_1, \\epsilon_2$ are independent of each other and of the true signals, with $\\mathrm{Var}(\\epsilon_1) = \\mathrm{Var}(\\epsilon_2) = V_{meas}$, where $V_{meas}$ is known.\n    - Conditional on the latent state $Z$, $\\tilde{X}_1$ and $\\tilde{X}_2$ are independent and identically distributed (i.i.d.).\n- Let's compute the observable moments. Since the reporters are i.i.d., $\\mathrm{Var}(X_1) = \\mathrm{Var}(X_2)$.\n    $$ \\mathrm{Var}(X_1) = \\mathrm{Var}(\\tilde{X}_1) + \\mathrm{Var}(\\epsilon_1) = (V_{int} + V_{ext}) + V_{meas} $$\n- This provides one equation. Now we compute the covariance, which is the critical new piece of information:\n    $$ \\mathrm{Cov}(X_1, X_2) = \\mathrm{Cov}(\\tilde{X}_1 + \\epsilon_1, \\tilde{X}_2 + \\epsilon_2) = \\mathrm{Cov}(\\tilde{X}_1, \\tilde{X}_2) $$\n    The other covariance terms are zero due to the independence of measurement errors.\n- We use the law of total covariance:\n    $$ \\mathrm{Cov}(\\tilde{X}_1, \\tilde{X}_2) = \\mathrm{E}[\\mathrm{Cov}(\\tilde{X}_1, \\tilde{X}_2 | Z)] + \\mathrm{Cov}(\\mathrm{E}[\\tilde{X}_1 | Z], \\mathrm{E}[\\tilde{X}_2 | Z]) $$\n    - By the assumption of conditional independence of $\\tilde{X}_1$ and $\\tilde{X}_2$ given $Z$, the term $\\mathrm{Cov}(\\tilde{X}_1, \\tilde{X}_2 | Z) = 0$, so $\\mathrm{E}[\\mathrm{Cov}(\\tilde{X}_1, \\tilde{X}_2 | Z)] = 0$. This reflects that intrinsic noise is independent for the two reporters.\n    - Since $\\tilde{X}_1$ and $\\tilde{X}_2$ are i.i.d., they have the same conditional mean, let's call it $\\mu(Z) = \\mathrm{E}[\\tilde{X}_1 | Z] = \\mathrm{E}[\\tilde{X}_2 | Z]$. The second term becomes $\\mathrm{Cov}(\\mu(Z), \\mu(Z)) = \\mathrm{Var}(\\mu(Z)) = \\mathrm{Var}(\\mathrm{E}[\\tilde{X}|Z])$. This is, by definition, the extrinsic variance, $V_{ext}$.\n- Thus, we have the crucial result: $\\mathrm{Cov}(X_1, X_2) = V_{ext}$.\n- We now have a system of two equations from two measurable quantities ($\\mathrm{Var}(X_1)$ and $\\mathrm{Cov}(X_1, X_2)$) and one known quantity ($V_{meas}$):\n    1.  $\\mathrm{Var}(X_1) = V_{int} + V_{ext} + V_{meas}$\n    2.  $\\mathrm{Cov}(X_1, X_2) = V_{ext}$\n- From empirical data, we estimate $\\widehat{\\mathrm{Var}}(X_1)$ and $\\widehat{\\mathrm{Cov}}(X_1, X_2)$. We can solve for the unknowns:\n    - $\\hat{V}_{ext} = \\widehat{\\mathrm{Cov}}(X_1, X_2)$\n    - $\\hat{V}_{int} = \\widehat{\\mathrm{Var}}(X_1) - \\widehat{\\mathrm{Cov}}(X_1, X_2) - V_{meas}$ (since $V_{meas}$ is known)\n- All three variance components are identifiable.\n- **Verdict: Correct.**\n\n**C. Dual-reporter snapshot without external calibration.**\n- This is the same setup as B, but $V_{meas}$ is now an unknown.\n- We have the same two equations from measurable moments:\n    1.  $\\mathrm{Var}(X_1) = V_{int} + V_{ext} + V_{meas}$\n    2.  $\\mathrm{Cov}(X_1, X_2) = V_{ext}$\n- We have two equations but three unknowns ($V_{int}, V_{ext}, V_{meas}$). We can identify $V_{ext}$ from the covariance. Substituting this into the first equation yields $\\mathrm{Var}(X_1) - \\mathrm{Cov}(X_1, X_2) = V_{int} + V_{meas}$. We can determine the sum of intrinsic and measurement variance, but we cannot separate them. The system is underdetermined.\n- **Verdict: Incorrect.**\n\n**D. Time-lapse single-reporter with external calibration.**\n- This design measures the same reporter at two sufficiently separated time points, $(X^{(1)}, X^{(2)})$.\n- The assumptions are structured to be mathematically analogous to the dual-reporter case:\n    - $V_{meas} = \\mathrm{Var}(\\epsilon^{(1)}) = \\mathrm{Var}(\\epsilon^{(2)})$ is known.\n    - The extrinsic state $Z$ is constant for the cell between time points.\n    - The time lag is long enough that intrinsic fluctuations are independent, i.e., $\\tilde{X}^{(1)}$ and $\\tilde{X}^{(2)}$ are independent conditional on $Z$. They are also assumed to be i.i.d. due to stationarity.\n- The derivation is identical to that in option B, with time points replacing reporter indices.\n- $\\mathrm{Var}(X^{(1)}) = V_{int} + V_{ext} + V_{meas}$.\n- $\\mathrm{Cov}(X^{(1)}, X^{(2)}) = V_{ext}$.\n- Since $V_{meas}$ is known, this system is solvable for $V_{int}$ and $V_{ext}$, just as in option B. All three variance components are identifiable.\n- **Verdict: Correct.**\n\n**E. Single snapshot with technical replicates.**\n- The design provides two technical replicate measurements of the *same* fixed sample per cell, $(X^{(a)}, X^{(b)})$.\n- The model is $X^{(a)} = \\tilde{X} + \\epsilon^{(a)}$ and $X^{(b)} = \\tilde{X} + \\epsilon^{(b)}$. The crucial difference from the dual-reporter case is that the true biological quantity $\\tilde{X}$ is identical for both measurements, not a new random draw. $\\tilde{X}$ is a random variable across cells, but fixed for the two replicates of a single cell.\n- The procedure described is to first estimate $V_{meas}$ from within-cell differences. The difference is $D = X^{(a)} - X^{(b)} = \\epsilon^{(a)} - \\epsilon^{(b)}$. Its variance is $\\mathrm{Var}(D) = \\mathrm{Var}(\\epsilon^{(a)}) + \\mathrm{Var}(\\epsilon^{(b)}) = 2V_{meas}$ (assuming independence and identical distribution of errors). Thus, $V_{meas}$ can indeed be estimated from the sample variance of these differences.\n- The next step in the option's description is to subtract this estimated $V_{meas}$ from the \"across-cell variance of $X$ to obtain $\\mathrm{Var}_{\\mathrm{ext}}(\\tilde{X})$\". Let's analyze the across-cell variance of $X^{(a)}$:\n    $$ \\mathrm{Var}(X^{(a)}) = \\mathrm{Var}(\\tilde{X} + \\epsilon^{(a)}) = \\mathrm{Var}(\\tilde{X}) + \\mathrm{Var}(\\epsilon^{(a)}) = (V_{int} + V_{ext}) + V_{meas} $$\n    Subtracting $V_{meas}$ yields $\\mathrm{Var}(X^{(a)}) - V_{meas} = V_{int} + V_{ext}$. This gives the total biological variance, $\\mathrm{Var}(\\tilde{X})$, not the extrinsic variance $V_{ext}$ as claimed. The statement in the option is factually incorrect.\n- Let's check if we can separate $V_{int}$ and $V_{ext}$ using this design, regardless of the flawed description. We need another equation. Let's use the covariance:\n    $$ \\mathrm{Cov}(X^{(a)}, X^{(b)}) = \\mathrm{Cov}(\\tilde{X} + \\epsilon^{(a)}, \\tilde{X} + \\epsilon^{(b)}) = \\mathrm{Cov}(\\tilde{X}, \\tilde{X}) = \\mathrm{Var}(\\tilde{X}) $$\n    This gives $\\mathrm{Cov}(X^{(a)}, X^{(b)}) = V_{int} + V_{ext}$. This is the same information we obtained from $\\mathrm{Var}(X^{(a)}) - V_{meas}$. There is no new independent equation to separate $V_{int}$ from $V_{ext}$. This design allows for the separation of measurement noise from the total biological noise, but it cannot decompose biological noise into its intrinsic and extrinsic components.\n- **Verdict: Incorrect.**", "answer": "$$\\boxed{BD}$$", "id": "3348928"}, {"introduction": "This final practice integrates theory and experimental design into a hands-on computational task. You will implement a moment-matching estimator that uses summary statistics from a simulated dual-reporter experiment—specifically the means, variances, and covariance—to infer the parameters of a model capturing both intrinsic and extrinsic noise. This exercise bridges the gap between abstract models and concrete data analysis, a core skill in computational systems biology. [@problem_id:3348950]", "problem": "Consider a minimal two-gene single-cell expression model in which intrinsic noise arises from conditional Poisson counting and extrinsic noise arises from cell-to-cell variation in a shared multiplicative factor. For gene $i \\in \\{1,2\\}$, let the mRNA count be a random variable $X_i$. Conditioned on an extrinsic factor $Z$, assume $X_i \\mid Z \\sim \\mathrm{Poisson}(c_i Z)$, where $c_i > 0$ is a gene-specific basal transcription scale. Assume $Z$ is strictly positive and shared by both genes, and that $Z \\sim \\mathrm{LogNormal}(\\mu, \\sigma^2)$, meaning $\\ln Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$. Also assume conditional independence of $X_1$ and $X_2$ given $Z$, and that the extrinsic factor $Z$ is independent of all intrinsic fluctuations.\n\nYou are given single-cell summary statistics estimated over a large population: sample means $\\bar{x}_1, \\bar{x}_2$, sample variances $s_1^2, s_2^2$, and the sample covariance $s_{12}$. Assume the Fano factors $F_i = s_i^2 / \\bar{x}_i$ satisfy $F_i > 1$ and that $s_{12} > 0$. Using only the law of total expectation, the law of total variance, the law of total covariance, and the conditional variance of a Poisson random variable, derive a moment-matching estimator for the parameters of the extrinsic distribution and the gene-specific scales. Because the product $(c_i, Z)$ is only identifiable up to a multiplicative constant, enforce the identifiability constraint $\\mathbb{E}[Z] = 1$ when reporting the parameters of the log-normal distribution. Under this constraint, report the inferred $(\\sigma^2, \\mu)$ for $Z$ and $(c_1, c_2)$.\n\nYour program must:\n- Implement the derivation starting only from the definitions above and the laws of total expectation, variance, and covariance, and compute the moment-matching estimates for $(\\sigma^2, \\mu, c_1, c_2)$ under the constraint $\\mathbb{E}[Z] = 1$.\n- Apply the estimator to each of the following test cases, where each case is given as a $5$-tuple $(\\bar{x}_1, s_1^2, \\bar{x}_2, s_2^2, s_{12})$:\n  - Case A: $(5.0, 21.218031767503205, 8.0, 49.518161324808206, 25.948850828005126)$.\n  - Case B: $(2.0, 2.040200668336672, 3.0, 3.090451503757512, 0.060301002505008)$.\n  - Case C: $(1.5, 6.720263076157231, 0.8, 2.2848748305513904, 2.7841403072838565)$.\n  - Case D: $(30.0, 229.26248234415295, 60.0, 857.0499293766118, 398.52496468830597)$.\n- For each case, output the list $[\\sigma^2, \\mu, c_1, c_2]$, with each real number rounded to exactly $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the four-tuples for the cases in the order A, B, C, D, enclosed in square brackets. Concretely, the output must look like a single top-level list of lists, for example, $[[\\dots],[\\dots],[\\dots],[\\dots]]$, with each float rounded to exactly $6$ decimal places.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It represents a standard problem in computational systems biology for decomposing noise sources from dual-reporter single-cell data. We proceed to derive the moment-matching estimators.\n\nThe model is defined by the following:\n1.  mRNA count for gene $i \\in \\{1,2\\}$: a random variable $X_i$.\n2.  Conditional distribution (intrinsic noise): $X_i \\mid Z \\sim \\mathrm{Poisson}(c_i Z)$, where $c_i > 0$.\n3.  Extrinsic factor distribution: $Z \\sim \\mathrm{LogNormal}(\\mu, \\sigma^2)$, with $Z > 0$.\n4.  Conditional independence: $X_1$ and $X_2$ are independent given $Z$.\n5.  Identifiability constraint: $\\mathbb{E}[Z] = 1$.\n\nWe will use properties of the Poisson and LogNormal distributions, along with the laws of total expectation, variance, and covariance. For a random variable $W \\sim \\mathrm{Poisson}(\\lambda)$, its mean and variance are $\\mathbb{E}[W] = \\lambda$ and $\\mathrm{Var}(W) = \\lambda$.\n\nStep $1$: Derive the expression for the mean of $X_i$.\nUsing the law of total expectation, $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X \\mid Y]]$:\n$$ \\mathbb{E}[X_i] = \\mathbb{E}[\\mathbb{E}[X_i \\mid Z]] $$\nGiven $X_i \\mid Z \\sim \\mathrm{Poisson}(c_i Z)$, the conditional mean is $\\mathbb{E}[X_i \\mid Z] = c_i Z$.\n$$ \\mathbb{E}[X_i] = \\mathbb{E}[c_i Z] = c_i \\mathbb{E}[Z] $$\nApplying the identifiability constraint $\\mathbb{E}[Z] = 1$, we get a simple expression for the mean:\n$$ \\mathbb{E}[X_i] = c_i $$\n\nStep $2$: Derive the expression for the variance of $X_i$.\nUsing the law of total variance, $\\mathrm{Var}(X) = \\mathbb{E}[\\mathrm{Var}(X \\mid Y)] + \\mathrm{Var}(\\mathbb{E}[X \\mid Y])$:\n$$ \\mathrm{Var}(X_i) = \\mathbb{E}[\\mathrm{Var}(X_i \\mid Z)] + \\mathrm{Var}(\\mathbb{E}[X_i \\mid Z]) $$\nThe first term, the mean of the conditional variance, corresponds to the intrinsic noise. For a Poisson-distributed variable, $\\mathrm{Var}(X_i \\mid Z) = \\mathbb{E}[X_i \\mid Z] = c_i Z$.\n$$ \\mathbb{E}[\\mathrm{Var}(X_i \\mid Z)] = \\mathbb{E}[c_i Z] = c_i \\mathbb{E}[Z] = c_i $$\nThe second term, the variance of the conditional mean, corresponds to the extrinsic noise. We have $\\mathbb{E}[X_i \\mid Z] = c_i Z$.\n$$ \\mathrm{Var}(\\mathbb{E}[X_i \\mid Z]) = \\mathrm{Var}(c_i Z) = c_i^2 \\mathrm{Var}(Z) $$\nCombining these, the total variance is the sum of intrinsic and extrinsic components:\n$$ \\mathrm{Var}(X_i) = c_i + c_i^2 \\mathrm{Var}(Z) = \\mathbb{E}[X_i] + (\\mathbb{E}[X_i])^2 \\mathrm{Var}(Z) $$\n\nStep $3$: Derive the expression for the covariance of $X_1$ and $X_2$.\nUsing the law of total covariance, $\\mathrm{Cov}(X, Y) = \\mathbb{E}[\\mathrm{Cov}(X, Y \\mid Z)] + \\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z])$:\n$$ \\mathrm{Cov}(X_1, X_2) = \\mathbb{E}[\\mathrm{Cov}(X_1, X_2 \\mid Z)] + \\mathrm{Cov}(\\mathbb{E}[X_1 \\mid Z], \\mathbb{E}[X_2 \\mid Z]) $$\nThe problem states that $X_1$ and $X_2$ are conditionally independent given $Z$, so $\\mathrm{Cov}(X_1, X_2 \\mid Z) = 0$. The first term is therefore $\\mathbb{E}[0] = 0$. The covariance arises solely from the shared extrinsic factor $Z$:\n$$ \\mathrm{Cov}(X_1, X_2) = \\mathrm{Cov}(c_1 Z, c_2 Z) = c_1 c_2 \\mathrm{Var}(Z) $$\n\nStep $4$: Formulate the moment-matching estimators.\nWe equate the derived theoretical moments to the given sample moments $(\\bar{x}_1, \\bar{x}_2, s_1^2, s_2^2, s_{12})$.\nFrom Step $1$, we estimate the gene-specific scales $c_i$:\n$$ \\hat{c}_1 = \\bar{x}_1 $$\n$$ \\hat{c}_2 = \\bar{x}_2 $$\nFrom Step $3$, we can estimate $\\mathrm{Var}(Z)$ using the sample covariance $s_{12}$ and our estimates for $c_1$ and $c_2$:\n$$ s_{12} = \\hat{c}_1 \\hat{c}_2 \\widehat{\\mathrm{Var}(Z)} = \\bar{x}_1 \\bar{x}_2 \\widehat{\\mathrm{Var}(Z)} $$\n$$ \\widehat{\\mathrm{Var}(Z)} = \\frac{s_{12}}{\\bar{x}_1 \\bar{x}_2} $$\nThis estimator isolates the contribution of the shared factor $Z$ to the covariance, providing a direct measure of the extrinsic noise variance.\n\nStep $5$: Determine the parameters of the LogNormal distribution.\nFor $Z \\sim \\mathrm{LogNormal}(\\mu, \\sigma^2)$, the mean and variance are $\\mathbb{E}[Z] = e^{\\mu + \\sigma^2/2}$ and $\\mathrm{Var}(Z) = (e^{\\sigma^2} - 1)e^{2\\mu + \\sigma^2}$.\nUsing the constraint $\\mathbb{E}[Z] = 1$:\n$$ e^{\\mu + \\sigma^2/2} = 1 \\implies \\mu + \\frac{\\sigma^2}{2} = 0 \\implies \\mu = -\\frac{\\sigma^2}{2} $$\nSubstituting this result for $\\mu$ into the variance formula simplifies it significantly:\n$$ \\mathrm{Var}(Z) = (e^{\\sigma^2} - 1)e^{2(-\\sigma^2/2) + \\sigma^2} = (e^{\\sigma^2} - 1)e^{-\\sigma^2 + \\sigma^2} = (e^{\\sigma^2} - 1)e^0 = e^{\\sigma^2} - 1 $$\nNow, we set this equal to our estimator for $\\mathrm{Var}(Z)$:\n$$ e^{\\hat{\\sigma}^2} - 1 = \\widehat{\\mathrm{Var}(Z)} = \\frac{s_{12}}{\\bar{x}_1 \\bar{x}_2} $$\nSolving for $\\hat{\\sigma}^2$:\n$$ e^{\\hat{\\sigma}^2} = 1 + \\frac{s_{12}}{\\bar{x}_1 \\bar{x}_2} $$\n$$ \\hat{\\sigma}^2 = \\ln\\left(1 + \\frac{s_{12}}{\\bar{x}_1 \\bar{x}_2}\\right) $$\nAnd subsequently for $\\hat{\\mu}$:\n$$ \\hat{\\mu} = -0.5 \\times \\hat{\\sigma}^2 $$\n\nSummary of estimators:\n1.  $\\hat{c}_1 = \\bar{x}_1$\n2.  $\\hat{c}_2 = \\bar{x}_2$\n3.  $\\hat{\\sigma}^2 = \\ln(1 + s_{12} / (\\bar{x}_1 \\bar{x}_2))$\n4.  $\\hat{\\mu} = -0.5 \\times \\hat{\\sigma}^2$\n\nThese formulas are implemented in the following program to compute the parameters for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies moment-matching estimators for a two-gene expression model\n    with intrinsic and extrinsic noise.\n    \"\"\"\n    # Define the test cases from the problem statement as a list of tuples.\n    # Each tuple is (x1_bar, s1_sq, x2_bar, s2_sq, s12).\n    test_cases = [\n        # Case A\n        (5.0, 21.218031767503205, 8.0, 49.518161324808206, 25.948850828005126),\n        # Case B\n        (2.0, 2.040200668336672, 3.0, 3.090451503757512, 0.060301002505008),\n        # Case C\n        (1.5, 6.720263076157231, 0.8, 2.2848748305513904, 2.7841403072838565),\n        # Case D\n        (30.0, 229.26248234415295, 60.0, 857.0499293766118, 398.52496468830597),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        x1_bar, _, x2_bar, _, s12 = case\n\n        # Step 1: Estimate c1 and c2 using the law of total expectation\n        # Under the constraint E[Z]=1, we have E[X_i] = c_i.\n        # We use the sample mean as the estimator for the expectation.\n        c1_hat = x1_bar\n        c2_hat = x2_bar\n\n        # Step 2: Estimate Var(Z) using the law of total covariance\n        # Cov(X1, X2) = c1 * c2 * Var(Z) => Var(Z) = Cov(X1, X2) / (c1 * c2)\n        # Using sample moments as estimators: Var(Z)_hat = s12 / (x1_bar * x2_bar)\n        # The problem constraints (c_i > 0, s12 > 0) ensure the denominator is non-zero\n        # and the argument to log will be > 1.\n        var_z_hat = s12 / (x1_bar * x2_bar)\n\n        # Step 3: Estimate sigma^2 for the LogNormal distribution of Z\n        # For a LogNormal Z ~ LogNormal(mu, sigma^2) with E[Z]=1, we have Var(Z) = exp(sigma^2) - 1.\n        # So, exp(sigma^2) - 1 = Var(Z)_hat\n        # => sigma^2 = log(1 + Var(Z)_hat)\n        sigma2_hat = np.log(1.0 + var_z_hat)\n\n        # Step 4: Estimate mu for the LogNormal distribution of Z\n        # The constraint E[Z] = exp(mu + sigma^2 / 2) = 1 implies mu + sigma^2 / 2 = 0.\n        # => mu = -sigma^2 / 2\n        mu_hat = -sigma2_hat / 2.0\n\n        # Collect the estimated parameters for the current case.\n        case_results = [sigma2_hat, mu_hat, c1_hat, c2_hat]\n        all_results.append(case_results)\n\n    # Format the final output string as a list of lists, with each number\n    # rounded to exactly 6 decimal places, as specified in the problem.\n    final_output_parts = []\n    for result_list in all_results:\n        # Format each number to 6 decimal places.\n        formatted_numbers = [f\"{num:.6f}\" for num in result_list]\n        # Create the string for the inner list, e.g., \"[num1,num2,num3,num4]\".\n        inner_list_str = f\"[{','.join(formatted_numbers)}]\"\n        final_output_parts.append(inner_list_str)\n    \n    # Join the parts for each case with commas and wrap in outer brackets.\n    final_output_str = f\"[{','.join(final_output_parts)}]\"\n\n    print(final_output_str)\n\nsolve()\n```", "id": "3348950"}]}