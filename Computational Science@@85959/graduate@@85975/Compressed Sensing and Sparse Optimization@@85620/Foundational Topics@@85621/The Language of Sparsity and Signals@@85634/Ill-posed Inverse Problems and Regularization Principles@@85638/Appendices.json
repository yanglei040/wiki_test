{"hands_on_practices": [{"introduction": "The Lasso estimator is a cornerstone of modern sparse recovery, valued for its ability to select relevant features and regularize ill-posed problems. To truly understand how Lasso promotes sparsity, we must look beyond the high-level formulation and examine its first-order optimality conditions. This practice guides you through deriving and interpreting these Karush-Kuhn-Tucker (KKT) conditions, revealing the precise mathematical mechanism that forces many coefficients of the solution to become exactly zero [@problem_id:3452160].", "problem": "Consider an ill-posed linear inverse problem in which $y \\in \\mathbb{R}^m$ is modeled as $y \\approx A x$ with $A \\in \\mathbb{R}^{m \\times n}$ and $mn$. To stabilize the inversion and promote sparsity, consider the least absolute shrinkage and selection operator (Lasso) estimator\n$$\nx^* \\in \\arg\\min_{x \\in \\mathbb{R}^n} \\;\\; \\frac{1}{2}\\,\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwith $\\lambda0$. Using only the convexity of the objective, the subgradient definition for the $\\ell_1$ norm, and the first-order necessary and sufficient optimality condition for convex problems, identify which of the following statements correctly express the Karush-Kuhn-Tucker (KKT) optimality conditions and interpret them in terms of the support and sign pattern of $x^*$. Let $S \\triangleq \\{ i \\in \\{1,\\dots,n\\} : x_i^* \\neq 0 \\}$ and $S^c$ denote its complement.\n\nA. There exists $z \\in \\partial \\|x^*\\|_1$ such that $A^\\top (A x^* - y) + \\lambda z = 0$, equivalently $A^\\top (y - A x^*) = \\lambda z$. With $z_i = \\operatorname{sign}(x_i^*)$ for $i \\in S$ and $z_i \\in [-1,1]$ for $i \\in S^c$, it follows that for $i \\in S$,\n$$\nA_i^\\top (y - A x^*) = \\lambda \\operatorname{sign}(x_i^*),\n$$\nand for $i \\in S^c$,\n$$\n|A_i^\\top (y - A x^*)| \\le \\lambda.\n$$\n\nB. For any optimal $x^*$ and any $i \\in S^c$, the KKT conditions imply\n$$\n|A_i^\\top (y - A x^*)| \\ge \\lambda.\n$$\n\nC. The subgradient optimality enforces\n$$\nA_S^\\top (y - A x^*) = \\lambda \\operatorname{sign}(x_S^*)\n\\quad \\text{and} \\quad\nA_{S^c}^\\top (y - A x^*) = 0.\n$$\n\nD. If $A_S$ has full column rank and $x_{S^c}^* = 0$, then stationarity on $S$ is equivalent to\n$$\n(A_S^\\top A_S)\\, x_S^* = A_S^\\top y - \\lambda \\operatorname{sign}(x_S^*).\n$$\nMoreover, if in addition the strict dual feasibility condition $\\|A_{S^c}^\\top (y - A x^*)\\|_\\infty  \\lambda$ holds, then $x^*$ is the unique minimizer with support $S$ and sign pattern $\\operatorname{sign}(x_S^*)$.\n\nE. Any subgradient $z \\in \\partial \\|x^*\\|_1$ must satisfy $\\|z\\|_2 \\le 1$, and for all $i \\in S^c$ one must have $z_i = 0$.\n\nSelect all options that are correct.", "solution": "The user wants me to validate the problem statement concerning the optimality conditions of the Lasso estimator and then identify the correct statements from the given options.\n\n### Step 1: Extract Givens\n- **Problem Setup**: An ill-posed linear inverse problem modeled as $y \\approx A x$.\n- **Data and Dimensions**: $y \\in \\mathbb{R}^m$, $x \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$, with $m  n$.\n- **Estimator**: The Lasso estimator is defined as the solution to the minimization problem:\n$$\nx^* \\in \\arg\\min_{x \\in \\mathbb{R}^n} \\;\\; \\frac{1}{2}\\,\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1\n$$\n- **Parameter**: The regularization parameter $\\lambda  0$.\n- **Definitions**:\n    - The support of the solution is $S \\triangleq \\{ i \\in \\{1,\\dots,n\\} : x_i^* \\neq 0 \\}$.\n    - $S^c$ is the complement of the support, i.e., $S^c \\triangleq \\{ i \\in \\{1,\\dots,n\\} : x_i^* = 0 \\}$.\n- **Constraints on Method**: Use only convexity, the subgradient definition for the $\\ell_1$ norm, and the first-order necessary and sufficient optimality condition.\n- **Goal**: Identify the correct statements about the Karush-Kuhn-Tucker (KKT) conditions and their interpretation.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a cornerstone of modern statistics, machine learning, and signal processing, specifically in compressed sensing and sparse optimization. The Lasso formulation is a standard and well-studied topic. The problem is scientifically sound.\n- **Well-Posed**: The problem defines a convex optimization problem. The objective function is the sum of a differentiable convex function and a non-differentiable convex function. Since the function is convex and coercive (it grows to infinity as $\\|x\\| \\to \\infty$), a minimizer $x^*$ is guaranteed to exist. The question asks for properties of any such minimizer, which is a well-defined task.\n- **Objective**: The problem is stated in precise mathematical language, free from subjectivity or ambiguity.\n- **Completeness**: The problem provides all necessary information to derive the first-order optimality conditions.\n- **Consistency and Feasibility**: The setup is standard, with no contradictory or physically impossible conditions. The condition $mn$ correctly frames it as an underdetermined problem where regularization is essential.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will proceed with deriving the solution and evaluating the options.\n\n### Derivation of Optimality Conditions\n\nThe objective function to minimize is $f(x) = \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1$.\nThis function is convex, being the sum of a convex quadratic function and the convex $\\ell_1$-norm (scaled by $\\lambda  0$). For a convex function, a point $x^*$ is a global minimum if and only if the zero vector is contained in the subdifferential of the function at that point.\n$$ 0 \\in \\partial f(x^*) $$\nThe objective function is a sum of a differentiable function $g(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ and a non-differentiable function $h(x) = \\lambda \\|x\\|_1$. By the sum rule for subdifferentials (which applies here since $g(x)$ is differentiable), we have:\n$$ \\partial f(x^*) = \\nabla g(x^*) + \\partial h(x^*) $$\nThe gradient of the least-squares term $g(x)$ is:\n$$ \\nabla g(x) = A^\\top(Ax - y) $$\nThe subdifferential of the $\\ell_1$-norm term $h(x)$ is:\n$$ \\partial h(x) = \\lambda \\, \\partial \\|x\\|_1 $$\nThe subdifferential of the $\\ell_1$-norm, $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$, at a point $x^*$ is the set of all vectors $z \\in \\mathbb{R}^n$ with components $z_i$ satisfying:\n$$\nz_i =\n\\begin{cases}\n\\operatorname{sign}(x_i^*)  \\text{if } x_i^* \\neq 0 \\\\\nv_i \\in [-1, 1]  \\text{if } x_i^* = 0\n\\end{cases}\n$$\nUsing the notation for the support $S$ and its complement $S^c$, a vector $z$ is in $\\partial \\|x^*\\|_1$ if and only if:\n- For $i \\in S$, $z_i = \\operatorname{sign}(x_i^*)$.\n- For $i \\in S^c$, $z_i \\in [-1, 1]$.\n\nThe first-order optimality condition $0 \\in \\nabla g(x^*) + \\partial h(x^*)$ means that there must exist a subgradient vector $z \\in \\partial \\|x^*\\|_1$ such that:\n$$ A^\\top(Ax^* - y) + \\lambda z = 0 $$\nThis can be rearranged to:\n$$ A^\\top(y - Ax^*) = \\lambda z $$\nLet $A_i$ denote the $i$-th column of the matrix $A$. Writing the above vector equation component-wise gives:\n$$ A_i^\\top(y - Ax^*) = \\lambda z_i \\quad \\text{for each } i \\in \\{1, \\dots, n\\} $$\nWe now analyze this condition based on whether the index $i$ is in the support $S$:\n\n1.  **For $i \\in S$ (active set):** Since $x_i^* \\neq 0$, the subgradient component is uniquely determined as $z_i = \\operatorname{sign}(x_i^*)$. The condition becomes:\n    $$ A_i^\\top(y - Ax^*) = \\lambda \\operatorname{sign}(x_i^*) $$\n\n2.  **For $i \\in S^c$ (inactive set):** Since $x_i^* = 0$, the subgradient component $z_i$ can be any value in the interval $[-1, 1]$. The condition is $A_i^\\top(y - Ax^*) = \\lambda z_i$. Since $z_i \\in [-1, 1]$ and $\\lambda0$, this is equivalent to:\n    $$ \\left| \\frac{A_i^\\top(y - Ax^*)}{\\lambda} \\right| \\le 1 \\implies |A_i^\\top(y - Ax^*)| \\le \\lambda $$\n\nThese are the KKT conditions for the Lasso problem.\n\n### Option-by-Option Analysis\n\n**A. There exists $z \\in \\partial \\|x^*\\|_1$ such that $A^\\top (A x^* - y) + \\lambda z = 0$, equivalently $A^\\top (y - A x^*) = \\lambda z$. With $z_i = \\operatorname{sign}(x_i^*)$ for $i \\in S$ and $z_i \\in [-1,1]$ for $i \\in S^c$, it follows that for $i \\in S$,\n$$\nA_i^\\top (y - A x^*) = \\lambda \\operatorname{sign}(x_i^*),\n$$\nand for $i \\in S^c$,\n$$\n|A_i^\\top (y - A x^*)| \\le \\lambda.\n$$**\nThis option presents the derivation of the KKT conditions exactly as performed above. It correctly states the subgradient optimality condition, correctly defines the subgradient of the $\\ell_1$-norm, and correctly interprets the conditions for indices in the support $S$ and its complement $S^c$.\n**Verdict: Correct.**\n\n**B. For any optimal $x^*$ and any $i \\in S^c$, the KKT conditions imply\n$$\n|A_i^\\top (y - A x^*)| \\ge \\lambda.\n$$**\nAs derived above, for $i \\in S^c$, the condition is $|A_i^\\top (y - A x^*)| \\le \\lambda$. The statement in this option presents the reverse inequality (except at the boundary where equality holds). It is possible for the inequality to be strict, i.e., $|A_i^\\top (y - A x^*)|  \\lambda$, which would violate the condition stated in this option. Therefore, this statement is false.\n**Verdict: Incorrect.**\n\n**C. The subgradient optimality enforces\n$$\nA_S^\\top (y - A x^*) = \\lambda \\operatorname{sign}(x_S^*)\n\\quad \\text{and} \\quad\nA_{S^c}^\\top (y - A x^*) = 0.\n$$**\nThe first part, $A_S^\\top (y - A x^*) = \\lambda \\operatorname{sign}(x_S^*)$, is a correct matrix notation for the condition $A_i^\\top(y - Ax^*) = \\lambda \\operatorname{sign}(x_i^*)$ for all $i \\in S$. The second part, $A_{S^c}^\\top (y - A x^*) = 0$, implies that $A_i^\\top(y - Ax^*) = 0$ for all $i \\in S^c$. This corresponds to the case where the subgradient components $z_i$ are all zero for $i \\in S^c$. However, the general condition is $|A_i^\\top(y - Ax^*)| \\le \\lambda$, which allows for non-zero values. The condition $A_{S^c}^\\top (y - A x^*) = 0$ is overly restrictive and not generally true.\n**Verdict: Incorrect.**\n\n**D. If $A_S$ has full column rank and $x_{S^c}^* = 0$, then stationarity on $S$ is equivalent to\n$$\n(A_S^\\top A_S)\\, x_S^* = A_S^\\top y - \\lambda \\operatorname{sign}(x_S^*).\n$$\nMoreover, if in addition the strict dual feasibility condition $\\|A_{S^c}^\\top (y - A x^*)\\|_\\infty  \\lambda$ holds, then $x^*$ is the unique minimizer with support $S$ and sign pattern $\\operatorname{sign}(x_S^*)$.**\n\nLet's analyze the first part. The stationarity condition on the support $S$ is $A_S^\\top(y - Ax^*) = \\lambda \\operatorname{sign}(x_S^*)$. We can decompose $Ax^*$ as $Ax^* = A_S x_S^* + A_{S^c} x_{S^c}^*$. Given that $x_{S^c}^* = 0$, this simplifies to $Ax^* = A_S x_S^*$. Substituting this into the stationarity condition yields $A_S^\\top(y - A_S x_S^*) = \\lambda \\operatorname{sign}(x_S^*)$. Expanding this gives $A_S^\\top y - A_S^\\top A_S x_S^* = \\lambda \\operatorname{sign}(x_S^*)$, which rearranges to $(A_S^\\top A_S)\\, x_S^* = A_S^\\top y - \\lambda \\operatorname{sign}(x_S^*)$. This is a correct derivation.\n\nNow for the second part. It makes a claim about uniqueness for a fixed support and sign pattern. Let's assume there is another minimizer $\\tilde{x}$ with the same support $S$ and sign pattern $\\operatorname{sign}(x_S^*)$. Then $\\tilde{x}_{S^c}=0$ and $\\operatorname{sign}(\\tilde{x}_S) = \\operatorname{sign}(x_S^*)$. It must also satisfy the equation derived above: $(A_S^\\top A_S)\\, \\tilde{x}_S = A_S^\\top y - \\lambda \\operatorname{sign}(\\tilde{x}_S) = A_S^\\top y - \\lambda \\operatorname{sign}(x_S^*)$. Subtracting the equations for $x_S^*$ and $\\tilde{x}_S$ gives $(A_S^\\top A_S)(x_S^* - \\tilde{x}_S) = 0$. Since we assumed $A_S$ has full column rank, the Gram matrix $A_S^\\top A_S$ is invertible. Therefore, $x_S^* - \\tilde{x}_S = 0$, which implies $x_S^* = \\tilde{x}_S$. As both solutions are zero outside the support $S$, we have $x^* = \\tilde{x}$. This proves uniqueness for the given support and sign pattern using only the full rank of $A_S$. The additional condition, $\\|A_{S^c}^\\top (y - A x^*)\\|_\\infty  \\lambda$, known as strict dual feasibility, is a standard condition (along with the full rank of $A_S$) for proving that $x^*$ is the *unique global minimizer* of the Lasso problem. Since this stronger set of conditions implies global uniqueness, it certainly implies the weaker conclusion stated in the option (uniqueness within a class). The statement is logically sound.\n**Verdict: Correct.**\n\n**E. Any subgradient $z \\in \\partial \\|x^*\\|_1$ must satisfy $\\|z\\|_2 \\le 1$, and for all $i \\in S^c$ one must have $z_i = 0$.**\nThe first part claims $\\|z\\|_2 \\le 1$. A subgradient $z \\in \\partial \\|x^*\\|_1$ has components that satisfy $|z_i| \\le 1$ for all $i=1,\\dots,n$. The squared $\\ell_2$-norm is $\\|z\\|_2^2 = \\sum_{i=1}^n z_i^2$. Since $z_i^2 \\le 1$, we have $\\|z\\|_2^2 \\le \\sum_{i=1}^n 1 = n$. Thus, $\\|z\\|_2 \\le \\sqrt{n}$. For $n1$, it is not necessary that $\\|z\\|_2 \\le 1$. For example, if $n=2$ and $x^*=(1,1)^\\top$, then $z=(1,1)^\\top$ is the unique subgradient, and $\\|z\\|_2=\\sqrt{2}1$. The correct norm bound is $\\|z\\|_\\infty \\le 1$. The first part of the statement is false.\nThe second part claims that for $i \\in S^c$, we must have $z_i=0$. As shown in the derivation, the condition for $i \\in S^c$ is that $z_i = \\lambda^{-1} A_i^\\top(y - Ax^*)$ and $|z_i| \\le 1$. There is no reason for $z_i$ to be zero in general. This part of the statement is also false.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{AD}$$", "id": "3452160"}, {"introduction": "A theoretical model is only as good as its practical implementation, and for regularization methods, choosing the parameter $\\lambda$ is a critical and often challenging step. This exercise introduces Stein's Unbiased Risk Estimate (SURE), a powerful, data-driven method for optimal parameter tuning in Gaussian denoising problems. By deriving and applying the SURE formula for the soft-thresholding operator—the core component of many $\\ell_1$ algorithms—you will gain hands-on experience in moving from a statistical principle to a concrete, optimal choice of $\\lambda$ [@problem_id:3452179].", "problem": "Consider the canonical ill-posed inverse problem of denoising in an orthonormal sparsifying transform domain, where the forward operator is the identity and the observation model is additive white Gaussian noise. Let the observation vector be $y \\in \\mathbb{R}^{n}$ with $y = x^{\\star} + \\varepsilon$, where the noise satisfies $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$. We estimate $x^{\\star}$ by applying the soft-thresholding operator with threshold $\\lambda \\ge 0$ componentwise: for $t \\in \\mathbb{R}$,\n$$\n\\eta_{\\lambda}(t) = \\operatorname{sign}(t) \\,\\max\\{|t| - \\lambda, 0\\}.\n$$\nThis is equivalent to the proximal map of the $\\ell_{1}$ regularizer and corresponds to Tikhonov-like regularization tailored to sparsity.\n\nStarting from Stein’s lemma and the definition of Stein’s Unbiased Risk Estimate (SURE), derive an explicit expression for the estimated mean squared error $\\operatorname{SURE}(\\lambda)$ of the soft-thresholding estimator $\\eta_{\\lambda}(y)$ as a function of $\\lambda$, by explicitly computing the divergence term of $\\eta_{\\lambda}(\\cdot)$. Then use this expression to select $\\lambda$ by minimizing $\\operatorname{SURE}(\\lambda)$ for the following data:\n$$\nn = 6,\\quad \\sigma = 0.5,\\quad y = \\begin{pmatrix} 3.0 \\\\ -2.6 \\\\ 1.1 \\\\ -0.9 \\\\ 0.7 \\\\ 0.2 \\end{pmatrix}.\n$$\nProvide the value of the SURE-optimal threshold $\\lambda^{\\star}$ that minimizes the estimated mean squared error. Round your answer to four significant figures. Express the final answer as a real number with no units.", "solution": "The problem requires the derivation of an explicit expression for Stein's Unbiased Risk Estimate (SURE) for the soft-thresholding estimator and its application to find an optimal threshold $\\lambda^{\\star}$ for a given dataset.\n\nFirst, we validate the problem statement.\n\n### Step 1: Extract Givens\n- **Observation Model**: $y = x^{\\star} + \\varepsilon \\in \\mathbb{R}^{n}$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$.\n- **Estimator**: The soft-thresholding operator $\\eta_{\\lambda}(y)$, applied componentwise. For a scalar $t \\in \\mathbb{R}$ and threshold $\\lambda \\ge 0$, the operator is defined as $\\eta_{\\lambda}(t) = \\operatorname{sign}(t) \\,\\max\\{|t| - \\lambda, 0\\}$.\n- **Objective**: Use SURE to select an optimal threshold $\\lambda^{\\star}$.\n- **Task 1**: Derive an explicit expression for $\\operatorname{SURE}(\\lambda)$.\n- **Task 2**: Minimize $\\operatorname{SURE}(\\lambda)$ to find $\\lambda^{\\star}$ for the given data.\n- **Data**: $n = 6$, $\\sigma = 0.5$, and the observation vector $y = \\begin{pmatrix} 3.0 \\\\ -2.6 \\\\ 1.1 \\\\ -0.9 \\\\ 0.7 \\\\ 0.2 \\end{pmatrix}$.\n- **Output Requirement**: Round the final answer $\\lambda^{\\star}$ to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is a standard problem in statistical signal processing and sparse recovery, based on the well-established Stein's lemma and the use of SURE for model selection. The provided data and parameters are complete and consistent. No flaws are identified.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\n### Derivation of the SURE Formula for Soft-Thresholding\n\nStein's Unbiased Risk Estimate provides an estimate of the mean squared error (MSE), $\\mathbb{E}[\\|\\hat{x}(y) - x^{\\star}\\|_2^2]$, for an estimator $\\hat{x}(y)$ of a vector $x^{\\star}$ observed in additive Gaussian noise. For an observation model $y = x^{\\star} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$, and a weakly differentiable estimator $\\hat{x}(y)$, the SURE is given by:\n$$\n\\operatorname{SURE}(y; \\hat{x}) = \\|y - \\hat{x}(y)\\|_2^2 - n\\sigma^2 + 2\\sigma^2 \\nabla \\cdot \\hat{x}(y)\n$$\nwhere $\\nabla \\cdot \\hat{x}(y)$ is the divergence of the function $\\hat{x}(\\cdot)$ evaluated at $y$.\n\nIn our case, the estimator is the soft-thresholding operator $\\hat{x}(y) = \\eta_{\\lambda}(y)$, which is applied componentwise: $\\hat{x}_i(y) = \\eta_{\\lambda}(y_i)$. The SURE formula is a function of the threshold $\\lambda$.\nWe need to compute the two terms involving $\\eta_{\\lambda}(y)$: the squared norm of the residual, $\\|y - \\eta_{\\lambda}(y)\\|_2^2$, and the divergence, $\\nabla \\cdot \\eta_{\\lambda}(y)$.\n\n1.  **Divergence Term**:\n    The divergence is the sum of the partial derivatives of the components of the estimator:\n    $$\n    \\nabla \\cdot \\eta_{\\lambda}(y) = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial y_i} \\eta_{\\lambda}(y_i)\n    $$\n    The soft-thresholding function $\\eta_{\\lambda}(t)$ can be written piece-wise:\n    $$\n    \\eta_{\\lambda}(t) = \\begin{cases} t - \\lambda  \\text{if } t  \\lambda \\\\ 0  \\text{if } -\\lambda \\le t \\le \\lambda \\\\ t + \\lambda  \\text{if } t  -\\lambda \\end{cases}\n    $$\n    This function is weakly differentiable. Its weak derivative with respect to $t$ is an indicator function:\n    $$\n    \\frac{d}{dt}\\eta_{\\lambda}(t) = \\mathbb{I}(|t|  \\lambda)\n    $$\n    where $\\mathbb{I}(\\cdot)$ is the indicator function, which equals $1$ if the condition is true and $0$ otherwise.\n    The divergence is therefore the count of components of $y$ whose magnitudes exceed the threshold $\\lambda$:\n    $$\n    \\nabla \\cdot \\eta_{\\lambda}(y) = \\sum_{i=1}^{n} \\mathbb{I}(|y_i|  \\lambda)\n    $$\n\n2.  **Residual Norm Term**:\n    The residual for the $i$-th component is $y_i - \\eta_{\\lambda}(y_i)$.\n    $$\n    y_i - \\eta_{\\lambda}(y_i) = \\begin{cases} y_i - (y_i - \\lambda) = \\lambda  \\text{if } y_i  \\lambda \\\\ y_i - 0 = y_i  \\text{if } |y_i| \\le \\lambda \\\\ y_i - (y_i + \\lambda) = -\\lambda  \\text{if } y_i  -\\lambda \\end{cases}\n    $$\n    This can be compactly written as $y_i - \\eta_{\\lambda}(y_i) = \\operatorname{sign}(y_i)\\lambda$ if $|y_i|  \\lambda$, and $y_i$ if $|y_i| \\le \\lambda$.\n    Squaring the residual for the $i$-th component gives:\n    $$\n    (y_i - \\eta_{\\lambda}(y_i))^2 = \\begin{cases} \\lambda^2  \\text{if } |y_i|  \\lambda \\\\ y_i^2  \\text{if } |y_i| \\le \\lambda \\end{cases}\n    $$\n    This is equivalent to $\\min(y_i^2, \\lambda^2)$.\n    The squared $\\ell_2$-norm of the residual vector is the sum over all components:\n    $$\n    \\|y - \\eta_{\\lambda}(y)\\|_2^2 = \\sum_{i=1}^{n} \\min(y_i^2, \\lambda^2)\n    $$\n\n3.  **Complete SURE Expression**:\n    Substituting the divergence and residual terms back into the general SURE formula, we obtain the explicit expression for the estimated MSE as a function of $\\lambda$:\n    $$\n    \\operatorname{SURE}(\\lambda) = \\sum_{i=1}^{n} \\min(y_i^2, \\lambda^2) - n\\sigma^2 + 2\\sigma^2 \\sum_{i=1}^{n} \\mathbb{I}(|y_i|  \\lambda)\n    $$\n\n### Minimization of SURE for the Given Data\n\nWe are given $n=6$, $\\sigma=0.5$ (so $\\sigma^2=0.25$), and $y = (3.0, -2.6, 1.1, -0.9, 0.7, 0.2)^T$.\nThe function $\\operatorname{SURE}(\\lambda)$ is continuous in $\\lambda$. Its derivative with respect to $\\lambda$ is piecewise defined. Let the sorted absolute values of the components of $y$ be $z_1 \\le z_2 \\le \\dots \\le z_n$. For any interval $\\lambda \\in (z_k, z_{k+1})$, the term $\\sum_{i=1}^{n} \\mathbb{I}(|y_i|  \\lambda)$ is constant, and $\\sum_{i=1}^{n} \\min(y_i^2, \\lambda^2) = C + (\\text{const}) \\cdot \\lambda^2$. The function is increasing on these intervals. Therefore, the minimum of $\\operatorname{SURE}(\\lambda)$ for $\\lambda \\ge 0$ must occur at one of the values in the set $\\{0\\} \\cup \\{|y_i|\\}_{i=1}^n$.\n\nThe absolute values of the components of $y$ are $|y_i| \\in \\{3.0, 2.6, 1.1, 0.9, 0.7, 0.2\\}$.\nThe candidate values for $\\lambda^{\\star}$ are $\\{0, 0.2, 0.7, 0.9, 1.1, 2.6, 3.0\\}$.\nThe constant term in our SURE expression is $-n\\sigma^2 = -6(0.25) = -1.5$. The coefficient for the divergence count is $2\\sigma^2 = 2(0.25) = 0.5$.\nThe squared values of the components are $y_i^2 \\in \\{9.0, 6.76, 1.21, 0.81, 0.49, 0.04\\}$.\n\nWe now evaluate $\\operatorname{SURE}(\\lambda)$ for each candidate value:\n-   **For $\\lambda = 0$**:\n    $\\sum \\min(y_i^2, 0^2) = 0$.\n    $\\#\\{i: |y_i|  0\\} = 6$.\n    $\\operatorname{SURE}(0) = 0 - 1.5 + 0.5 \\times 6 = 1.5$.\n-   **For $\\lambda = 0.2$**:\n    $\\sum \\min(y_i^2, 0.2^2) = \\min(0.04, 0.04) + 5 \\times (0.04) = 0.24$.\n    $\\#\\{i: |y_i|  0.2\\} = 5$.\n    $\\operatorname{SURE}(0.2) = 0.24 - 1.5 + 0.5 \\times 5 = 0.24 + 1.0 = 1.24$.\n-   **For $\\lambda = 0.7$**:\n    $\\sum \\min(y_i^2, 0.7^2) = (0.04 + 0.49) + 4 \\times (0.49) = 0.53 + 1.96 = 2.49$.\n    $\\#\\{i: |y_i|  0.7\\} = 4$.\n    $\\operatorname{SURE}(0.7) = 2.49 - 1.5 + 0.5 \\times 4 = 2.49 + 0.5 = 2.99$.\n-   **For $\\lambda = 0.9$**:\n    $\\sum \\min(y_i^2, 0.9^2) = (0.04 + 0.49 + 0.81) + 3 \\times (0.81) = 1.34 + 2.43 = 3.77$.\n    $\\#\\{i: |y_i|  0.9\\} = 3$.\n    $\\operatorname{SURE}(0.9) = 3.77 - 1.5 + 0.5 \\times 3 = 3.77$.\n-   **For $\\lambda = 1.1$**:\n    $\\sum \\min(y_i^2, 1.1^2) = (1.34 + 1.21) + 2 \\times (1.21) = 2.55 + 2.42 = 4.97$.\n    $\\#\\{i: |y_i|  1.1\\} = 2$.\n    $\\operatorname{SURE}(1.1) = 4.97 - 1.5 + 0.5 \\times 2 = 4.47$.\n-   **For $\\lambda = 2.6$**:\n    $\\sum \\min(y_i^2, 2.6^2) = (2.55 + 6.76) + 1 \\times (6.76) = 9.31 + 6.76 = 16.07$.\n    $\\#\\{i: |y_i|  2.6\\} = 1$.\n    $\\operatorname{SURE}(2.6) = 16.07 - 1.5 + 0.5 \\times 1 = 15.07$.\n-   **For $\\lambda = 3.0$**:\n    $\\sum \\min(y_i^2, 3.0^2) = (9.31 + 9.0) + 0 \\times (9.0) = 18.31$.\n    $\\#\\{i: |y_i|  3.0\\} = 0$.\n    $\\operatorname{SURE}(3.0) = 18.31 - 1.5 + 0.5 \\times 0 = 16.81$.\n\nComparing the computed SURE values:\n$1.5, 1.24, 2.99, 3.77, 4.47, 15.07, 16.81$.\nThe minimum value is $1.24$, which occurs at $\\lambda^{\\star} = 0.2$.\n\nThe SURE-optimal threshold is $\\lambda^{\\star} = 0.2$. The problem requires rounding to four significant figures.", "answer": "$$\n\\boxed{0.2000}\n$$", "id": "3452179"}, {"introduction": "While the $\\ell_1$-norm is a powerful and widely used convex regularizer, it is known to introduce a systematic bias in the estimates of large, non-zero coefficients. This practice explores advanced nonconvex penalties, such as the Smoothly Clipped Absolute Deviation (SCAD) and Minimax Concave Penalty (MCP), which are designed to mitigate this issue while still promoting sparsity. By deriving their corresponding thresholding functions, you will uncover how they differ from the standard soft-thresholding of the $\\ell_1$-norm and appreciate the fundamental trade-offs between estimation accuracy and the complexities of nonconvex optimization [@problem_id:3452161].", "problem": "Consider the one-dimensional penalized least-squares subproblem that arises in iterative methods for solving ill-posed inverse problems in compressed sensing:\n$$\n\\min_{x \\in \\mathbb{R}} \\; \\frac{1}{2}(x - y)^{2} + \\lambda \\, \\phi(|x|),\n$$\nwhere $y \\in \\mathbb{R}$ is a scalar sufficient statistic (for example, the current back-projection of data in a proximal-gradient iteration when $A^{\\top}A \\approx I$), $\\lambda  0$ is a regularization parameter, and $\\phi$ is a sparsity-promoting penalty applied elementwise. Use as your fundamental base the first-order optimality condition for a minimizer $x^{\\star}$ of the above objective, namely the subgradient equation for a separable one-dimensional penalty:\n$$\n0 \\in x^{\\star} - y + \\lambda \\, \\partial\\big(\\phi(|x^{\\star}|)\\big),\n$$\nand the facts that the penalties below are even functions of $x$, differentiable for $x \\neq 0$, and have piecewise-linear derivatives in $|x|$.\n\nDerive the explicit thresholding functions (closed-form proximal mappings) $T_{\\ell_{1}}(y)$, $T_{\\mathrm{SCAD}}(y)$, and $T_{\\mathrm{MCP}}(y)$ corresponding to the following penalties:\n1. The $\\ell_{1}$ penalty $\\phi_{\\ell_{1}}(|x|) = |x|$.\n2. The Smoothly Clipped Absolute Deviation (SCAD) penalty $\\phi_{\\mathrm{SCAD}}(|x|)$ with parameter $a  2$, defined by its derivative with respect to $|x|$:\n$$\n\\phi_{\\mathrm{SCAD}}'(|x|) = \n\\begin{cases}\n1,  0 \\leq |x| \\leq \\lambda, \\\\\n\\dfrac{a\\lambda - |x|}{(a - 1)\\lambda},  \\lambda  |x| \\leq a\\lambda, \\\\\n0,  |x|  a\\lambda,\n\\end{cases}\n$$\nand extended to $x  0$ by odd symmetry in $x$.\n3. The Minimax Concave Penalty (MCP) $\\phi_{\\mathrm{MCP}}(|x|)$ with parameter $\\gamma  1$, defined by its derivative with respect to $|x|$:\n$$\n\\phi_{\\mathrm{MCP}}'(|x|) =\n\\begin{cases}\n1 - \\dfrac{|x|}{\\gamma \\lambda},  0 \\leq |x| \\leq \\gamma \\lambda, \\\\\n0,  |x|  \\gamma \\lambda,\n\\end{cases}\n$$\nand extended to $x  0$ by odd symmetry in $x$.\n\nYour derivations must start from the stated optimality principle and proceed by case analysis on $|x|$ induced by the piecewise-linear derivatives. Clearly state the resulting thresholding functions in terms of $y$, $\\lambda$, $a$, and $\\gamma$, emphasizing their odd symmetry.\n\nThen, within the context of ill-posed inverse problems and regularization principles in compressed sensing, discuss—based on your derived thresholding functions—why nonconvex penalties such as SCAD and MCP may reduce bias relative to $\\ell_{1}$ for large signal coefficients, and articulate potential pitfalls including algorithmic nonconvexity, sensitivity to parameter choices, and implications for solution stability and uniqueness.\n\nFinally, evaluate all three thresholding functions at the parameter values $\\lambda = 1$, $a = 3.7$, $\\gamma = 3$, and the observation $y_{0} = 2.2$. Express the three values $T_{\\ell_{1}}(y_{0})$, $T_{\\mathrm{MCP}}(y_{0})$, and $T_{\\mathrm{SCAD}}(y_{0})$ as exact rational numbers when possible. If any value does not simplify to a rational number in closed form with the given parameters, leave it in exact symbolic form. No rounding is required.", "solution": "The problem asks for the derivation of three thresholding functions, a discussion of their properties, and a numerical evaluation. The problem is approached by solving the one-dimensional optimization problem:\n$$\n\\min_{x \\in \\mathbb{R}} \\; f(x) = \\frac{1}{2}(x - y)^{2} + P(x),\n$$\nwhere $P(x)$ is the respective penalty term ($\\ell_1$, SCAD, or MCP). The solution $x^{\\star}$ is the value that minimizes $f(x)$.\n\nA minor ambiguity exists in the problem statement. The objective is given as $\\frac{1}{2}(x - y)^{2} + \\lambda \\, \\phi(|x|)$, but the definitions for the derivatives of the SCAD and MCP penalties, $\\phi'_{\\mathrm{SCAD}}(|x|)$ and $\\phi'_{\\mathrm{MCP}}(|x|)$, already include the parameter $\\lambda$. To resolve this, we interpret the entire penalty term as $P(x)$, and the given expressions as the derivative of $P(x)$ with respect to its argument $|x|$. Let's denote this derivative as $P'_{|x|}(|x|) = \\frac{d P(u)}{du}|_{u=|x|}$.\n\nThe first-order necessary condition for a minimum $x^{\\star}$ is given by the subgradient inclusion:\n$$\n0 \\in \\partial f(x^{\\star}) = x^{\\star} - y + \\partial P(x^{\\star}).\n$$\nThis can be rewritten as $y - x^{\\star} \\in \\partial P(x^{\\star})$. The penalty functions $P(x)$ are even, so $P(x) = \\tilde{P}(|x|)$ for some function $\\tilde{P}$ on $\\mathbb{R}_{\\ge 0}$. The subgradient $\\partial P(x)$ for $x \\neq 0$ is a singleton set containing the derivative, $\\partial P(x) = \\{ \\tilde{P}'(|x|) \\mathrm{sgn}(x) \\}$. At $x=0$, the subgradient is the interval $\\partial P(0) = [-\\tilde{P}'(0^+), \\tilde{P}'(0^+)]$.\n\nAn important property of the solution $x^\\star = T(y)$ is that it is an odd function of $y$, i.e., $T(-y) = -T(y)$. This is because replacing $y$ with $-y$ and $x$ with $-x$ in the objective function yields $\\frac{1}{2}(-x - (-y))^2 + P(-x) = \\frac{1}{2}(y-x)^2 + P(x)$, which is the original objective. Thus, if $x^\\star$ minimizes the objective for $y$, then $-x^\\star$ must minimize it for $-y$. This means we can derive the solution for $y  0$ (which implies $x^{\\star} \\geq 0$) and then extend it to all $y \\in \\mathbb{R}$ by odd symmetry.\n\nFor $y  0$, we have $x^{\\star} \\geq 0$. The optimality condition simplifies:\n- If $x^{\\star}  0$, then $y - x^{\\star} = P'_{|x|}(x^{\\star})$.\n- If $x^{\\star} = 0$, then $y \\in [ -P'_{|x|}(0^+), P'_{|x|}(0^+) ]$. Since $y  0$, this becomes $0  y \\leq P'_{|x|}(0^+)$.\n\nWe now apply this framework to each penalty.\n\n### 1. The $\\ell_{1}$ Penalty: $T_{\\ell_{1}}(y)$\nThe penalty is $P(x) = \\lambda |x|$. The derivative with respect to $|x|$ is $P'_{|x|}(|x|) = \\lambda$ for $|x|0$. The right-derivative at $0$ is $P'_{|x|}(0^+) = \\lambda$.\nLet $y  0$.\n- If $x^{\\star}=0$, the condition is $0  y \\leq \\lambda$.\n- If $x^{\\star}  0$, the condition is $y - x^{\\star} = \\lambda$, which implies $x^{\\star} = y - \\lambda$. This is valid only if $x^{\\star}  0$, meaning $y - \\lambda  0$, or $y  \\lambda$.\nCombining these cases for $y0$: $x^{\\star} = \\max(0, y-\\lambda)$.\nExtending by odd symmetry for any $y \\in \\mathbb{R}$:\n$$\nT_{\\ell_{1}}(y) = \\mathrm{sgn}(y) \\max(0, |y|-\\lambda).\n$$\nThis is the well-known soft-thresholding function.\n\n### 2. The SCAD Penalty: $T_{\\mathrm{SCAD}}(y)$\nThe derivative of the penalty with respect to $|x|$ is given for $a  2$:\n$$\nP'_{|x|}(|x|) = \n\\begin{cases}\n\\lambda,  0 \\leq |x| \\leq \\lambda, \\\\\n\\dfrac{a\\lambda - |x|}{a - 1},  \\lambda  |x| \\leq a\\lambda, \\\\\n0,  |x|  a\\lambda.\n\\end{cases}\n$$\nThe right-derivative at $0$ is $P'_{|x|}(0^+) = \\lambda$.\nLet $y  0$, so $x^{\\star} \\geq 0$.\n- If $x^{\\star}=0$, the condition is $0  y \\leq \\lambda$.\n- If $x^{\\star}  0$, we have $y - x^{\\star} = P'_{|x|}(x^{\\star})$. We analyze this by cases on $x^{\\star}$:\n    - Case $0  x^{\\star} \\leq \\lambda$: $y - x^{\\star} = \\lambda \\implies x^{\\star} = y - \\lambda$. This solution is valid for $0  y - \\lambda \\leq \\lambda$, which corresponds to $\\lambda  y \\leq 2\\lambda$.\n    - Case $\\lambda  x^{\\star} \\leq a\\lambda$: $y - x^{\\star} = \\frac{a\\lambda - x^{\\star}}{a-1}$. Solving for $x^{\\star}$:\n      $(a-1)y - (a-1)x^{\\star} = a\\lambda - x^{\\star} \\implies (a-1)y - a\\lambda = (a-2)x^{\\star} \\implies x^{\\star} = \\frac{(a-1)y - a\\lambda}{a-2}$.\n      This solution is valid if $\\lambda  x^{\\star} \\leq a\\lambda$.\n      $\\lambda  \\frac{(a-1)y - a\\lambda}{a-2} \\implies (a-2)\\lambda  (a-1)y - a\\lambda \\implies (2a-2)\\lambda  (a-1)y \\implies y  2\\lambda$.\n      $\\frac{(a-1)y - a\\lambda}{a-2} \\leq a\\lambda \\implies (a-1)y - a\\lambda \\leq a\\lambda(a-2) \\implies (a-1)y \\leq a^2\\lambda-a\\lambda \\implies y \\leq a\\lambda$.\n      So, this case holds for $2\\lambda  y \\leq a\\lambda$.\n    - Case $x^{\\star}  a\\lambda$: $y - x^{\\star} = 0 \\implies x^{\\star} = y$. This is valid for $y  a\\lambda$.\n\nCombining all cases for $y0$ and extending by odd symmetry:\n$$\nT_{\\mathrm{SCAD}}(y) = \n\\begin{cases}\n\\mathrm{sgn}(y)\\max(0, |y|-\\lambda),  |y| \\leq 2\\lambda, \\\\\n\\dfrac{(a-1)y - a\\lambda\\,\\mathrm{sgn}(y)}{a-2},  2\\lambda  |y| \\leq a\\lambda, \\\\\ny,  |y|  a\\lambda.\n\\end{cases}\n$$\nThe first part, for $|y| \\leq 2\\lambda$, is identical to the soft-thresholding function $T_{\\ell_{1}}(y)$ evaluated with parameter $\\lambda$.\n\n### 3. The MCP Penalty: $T_{\\mathrm{MCP}}(y)$\nThe derivative of the penalty with respect to $|x|$ is given for $\\gamma  1$:\n$$\nP'_{|x|}(|x|) =\n\\begin{cases}\n\\lambda \\left(1 - \\dfrac{|x|}{\\gamma \\lambda}\\right),  0 \\leq |x| \\leq \\gamma \\lambda, \\\\\n0,  |x|  \\gamma \\lambda.\n\\end{cases}\n$$\nThe right-derivative at $0$ is $P'_{|x|}(0^+) = \\lambda$.\nLet $y  0$, so $x^{\\star} \\geq 0$.\n- If $x^{\\star}=0$, the condition is $0  y \\leq \\lambda$.\n- If $x^{\\star}  0$, we have $y - x^{\\star} = P'_{|x|}(x^{\\star})$.\n    - Case $0  x^{\\star} \\leq \\gamma\\lambda$: $y - x^{\\star} = \\lambda - \\frac{x^{\\star}}{\\gamma}$. Solving for $x^{\\star}$:\n      $y - \\lambda = x^{\\star}(1 - 1/\\gamma) \\implies x^{\\star} = \\frac{y-\\lambda}{1-1/\\gamma} = \\frac{\\gamma(y-\\lambda)}{\\gamma-1}$.\n      This solution is valid if $0  x^{\\star} \\leq \\gamma\\lambda$.\n      $x^{\\star}  0$ requires $y  \\lambda$.\n      $x^{\\star} \\leq \\gamma\\lambda$ requires $\\frac{\\gamma(y-\\lambda)}{\\gamma-1} \\leq \\gamma\\lambda \\implies y-\\lambda \\leq \\lambda(\\gamma-1) \\implies y \\leq \\gamma\\lambda$.\n      So, this case holds for $\\lambda  y \\leq \\gamma\\lambda$.\n    - Case $x^{\\star}  \\gamma\\lambda$: $y - x^{\\star} = 0 \\implies x^{\\star} = y$. This is valid for $y  \\gamma\\lambda$.\n\nCombining cases for $y0$ and extending by odd symmetry:\n$$\nT_{\\mathrm{MCP}}(y) = \n\\begin{cases}\n0,  |y| \\leq \\lambda, \\\\\n\\dfrac{\\gamma(y-\\lambda\\,\\mathrm{sgn}(y))}{\\gamma-1},  \\lambda  |y| \\leq \\gamma\\lambda, \\\\\ny,  |y|  \\gamma\\lambda.\n\\end{cases}\n$$\n\n### Discussion\nThe derived thresholding functions reveal key properties of the penalties in the context of ill-posed inverse problems.\n- **Bias Reduction with Nonconvex Penalties:**\nThe $\\ell_1$ thresholding function, $T_{\\ell_1}(y)=\\mathrm{sgn}(y)\\max(0, |y|-\\lambda)$, always shrinks large coefficients towards zero by a constant amount $\\lambda$. If $y$ is an observation of a true large coefficient $x_{true}$, the estimate $x^\\star$ will be approximately $|x_{true}|-\\lambda$ (in magnitude), introducing a systematic bias.\nIn contrast, both SCAD and MCP have thresholding functions that become the identity function, $T(y)=y$, for large values of $|y|$ (specifically, $|y|  a\\lambda$ for SCAD and $|y|  \\gamma\\lambda$ for MCP). This means that they do not penalize large coefficients, leading to asymptotically unbiased estimates for large signals. This property, often termed \"unbiasedness\", is a significant advantage of nonconvex penalties over $\\ell_1$.\n\n- **Potential Pitfalls of Nonconvex Penalties:**\n    - **Algorithmic Nonconvexity:** The primary drawback is that the overall objective function, e.g., $\\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 + \\sum_i P(x_i)$, becomes nonconvex. Iterative algorithms like proximal gradient descent, which solve a sequence of one-dimensional subproblems of the form analyzed here, are only guaranteed to converge to a stationary point, which may be a local, not global, minimum. The final solution can be sensitive to the algorithm's initialization.\n    - **Sensitivity to Parameter Choices:** The performance of SCAD and MCP is highly dependent on the choice of their shape parameters ($a$ and $\\gamma$, respectively) in addition to the regularization parameter $\\lambda$. These extra parameters control the degree of concavity and the threshold for unbiasedness. Finding optimal values for these parameters is a difficult model selection problem, often more complex and computationally intensive than for the convex $\\ell_1$ penalty.\n    - **Solution Stability and Uniqueness:** For convex problems like LASSO, the solution is often unique and stable under perturbations of the data. For nonconvex penalties, the existence of multiple local minima can lead to solution instability. Small changes in the data can cause an algorithm to converge to a different local minimum, resulting in a significantly different solution. While these estimators have strong asymptotic guarantees (e.g., oracle properties), their finite-sample stability can be less robust than their convex counterparts.\n\n### Numerical Evaluation\nWe evaluate the three thresholding functions for the given parameters $\\lambda = 1$, $a = 3.7$, $\\gamma = 3$, and the observation $y_0 = 2.2$.\n\n- **$T_{\\ell_{1}}(y_0)$:**\nWith $\\lambda=1$ and $y_0=2.2$, we have $|y_0|  \\lambda$.\n$T_{\\ell_{1}}(2.2) = \\mathrm{sgn}(2.2)(|2.2| - 1) = 1(2.2 - 1) = 1.2 = \\frac{12}{10} = \\frac{6}{5}$.\n\n- **$T_{\\mathrm{MCP}}(y_0)$:**\nWith $\\lambda=1$, $\\gamma=3$, and $y_0=2.2$. The intervals are defined by $\\lambda=1$ and $\\gamma\\lambda = 3$. Since $1  |2.2| \\leq 3$, we use the second case for $T_{\\mathrm{MCP}}(y)$.\n$T_{\\mathrm{MCP}}(2.2) = \\frac{3(2.2 - 1 \\cdot \\mathrm{sgn}(2.2))}{3-1} = \\frac{3(1.2)}{2} = \\frac{3.6}{2} = 1.8 = \\frac{18}{10} = \\frac{9}{5}$.\n\n- **$T_{\\mathrm{SCAD}}(y_0)$:**\nWith $\\lambda=1$, $a=3.7$, and $y_0=2.2$. The intervals are defined by $2\\lambda=2$ and $a\\lambda=3.7$. Since $2  |2.2| \\leq 3.7$, we use the second case for $T_{\\mathrm{SCAD}}(y)$.\n$T_{\\mathrm{SCAD}}(2.2) = \\frac{(3.7 - 1) \\cdot 2.2 - 3.7 \\cdot 1 \\cdot \\mathrm{sgn}(2.2)}{3.7 - 2} = \\frac{2.7 \\cdot 2.2 - 3.7}{1.7} = \\frac{5.94 - 3.7}{1.7} = \\frac{2.24}{1.7}$.\nAs an exact rational number:\n$\\frac{2.24}{1.7} = \\frac{224/100}{17/10} = \\frac{224}{100} \\cdot \\frac{10}{17} = \\frac{224}{170} = \\frac{112}{85}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{6}{5}  \\frac{9}{5}  \\frac{112}{85} \\end{pmatrix}}\n$$", "id": "3452161"}]}