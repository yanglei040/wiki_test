{"hands_on_practices": [{"introduction": "The concept of a compressible signal is central to compressed sensing, describing signals that are not strictly sparse but whose energy is concentrated in a few large coefficients. This exercise provides a foundational look at the mathematical properties of such signals. By working with a signal whose sorted magnitudes exhibit a perfect power-law decay, you will directly engage with the definitions of weak-$\\ell_p$ and standard $\\ell_p$ spaces to see precisely why these signals are treatable by sparsity-promoting techniques [@problem_id:3435915].", "problem": "Consider a real-valued or complex-valued sequence $x=(x_i)_{i\\geq 1}$ and its nonincreasing rearrangement of magnitudes $\\{|x|_{(i)}\\}_{i\\geq 1}$ defined by sorting $\\{|x_i|\\}_{i\\geq 1}$ so that $|x|_{(1)}\\geq |x|_{(2)}\\geq \\cdots \\geq 0$. For a given $p>0$ and constant $C>0$, assume $|x|_{(i)}=C\\,i^{-1/p}$ for all $i\\in\\mathbb{N}$. Using only core definitions, compute the weak $\\ell_p$ quasi-norm $\\|x\\|_{w\\ell_p}$ defined by $\\|x\\|_{w\\ell_p}=\\sup_{i\\geq 1} i^{1/p}\\,|x|_{(i)}$ exactly. Then, analyze the ordinary $\\ell_p$ (norm for $p\\geq 1$, quasi-norm for $0p1$), defined by $\\|x\\|_{\\ell_p}=\\left(\\sum_{i=1}^{\\infty}|x_i|^{p}\\right)^{1/p}$, and determine whether it is finite or infinite. Your reasoning should begin from the definitions of nonincreasing rearrangement, weak $\\ell_p$ quasi-norm, and the integral test for series. The final answer should be reported as the row matrix $\\begin{pmatrix}\\|x\\|_{w\\ell_p}  \\|x\\|_{\\ell_p}\\end{pmatrix}$. No rounding is required, and no units are involved.", "solution": "The problem is well-posed and scientifically grounded, allowing for a direct solution based on the provided definitions. We will first compute the weak $\\ell_p$ quasi-norm and then analyze the ordinary $\\ell_p$ norm.\n\nFirst, we compute the weak $\\ell_p$ quasi-norm, $\\|x\\|_{w\\ell_p}$. The definition is given as:\n$$ \\|x\\|_{w\\ell_p} = \\sup_{i\\geq 1} i^{1/p}\\,|x|_{(i)} $$\nWe are given that the nonincreasing rearrangement of the magnitudes of the sequence elements follows the rule $|x|_{(i)}=C\\,i^{-1/p}$ for a constant $C>0$ and for all positive integers $i \\in \\mathbb{N}$. Substituting this expression into the definition of the weak $\\ell_p$ quasi-norm yields:\n$$ \\|x\\|_{w\\ell_p} = \\sup_{i\\geq 1} i^{1/p}\\left(C\\,i^{-1/p}\\right) $$\nBy rearranging terms and using the properties of exponents, we simplify the expression inside the supremum:\n$$ i^{1/p}\\left(C\\,i^{-1/p}\\right) = C \\cdot i^{1/p} \\cdot i^{-1/p} = C \\cdot i^{(1/p) - (1/p)} = C \\cdot i^0 = C \\cdot 1 = C $$\nThe expression is a constant $C$ for all values of $i \\geq 1$. The supremum of a constant sequence is the constant itself. Therefore, the weak $\\ell_p$ quasi-norm is:\n$$ \\|x\\|_{w\\ell_p} = \\sup_{i\\geq 1} C = C $$\n\nNext, we analyze the ordinary $\\ell_p$ (quasi-)norm, $\\|x\\|_{\\ell_p}$, defined as:\n$$ \\|x\\|_{\\ell_p}=\\left(\\sum_{i=1}^{\\infty}|x_i|^{p}\\right)^{1/p} $$\nTo evaluate this, we first need to compute the sum $\\sum_{i=1}^{\\infty}|x_i|^{p}$. Since summing a set of non-negative numbers is a commutative operation, the value of the sum is independent of the order of the terms. Thus, we can replace the sum over the original sequence elements $|x_i|$ with a sum over their nonincreasing rearrangement $|x|_{(i)}$:\n$$ \\sum_{i=1}^{\\infty}|x_i|^{p} = \\sum_{i=1}^{\\infty}\\left(|x|_{(i)}\\right)^{p} $$\nSubstituting the given form $|x|_{(i)}=C\\,i^{-1/p}$ into the sum, we get:\n$$ \\sum_{i=1}^{\\infty}\\left(C\\,i^{-1/p}\\right)^{p} = \\sum_{i=1}^{\\infty}C^p \\left(i^{-1/p}\\right)^{p} = \\sum_{i=1}^{\\infty}C^p i^{-1} $$\nSince $C$ is a positive constant, $C^p$ is also a positive constant and can be factored out of the summation:\n$$ C^p \\sum_{i=1}^{\\infty}\\frac{1}{i} $$\nThe series $\\sum_{i=1}^{\\infty}\\frac{1}{i}$ is the harmonic series. To determine its convergence, we use the integral test for series. Let $f(t) = \\frac{1}{t}$. For $t \\in [1, \\infty)$, the function $f(t)$ is continuous, positive, and decreasing. The integral test states that the series $\\sum_{i=1}^{\\infty}f(i)$ converges if and only if the improper integral $\\int_1^{\\infty} f(t)\\,dt$ converges. We evaluate this integral:\n$$ \\int_1^{\\infty} \\frac{1}{t}\\,dt = \\lim_{b\\to\\infty} \\int_1^b \\frac{1}{t}\\,dt = \\lim_{b\\to\\infty} [\\ln(t)]_1^b = \\lim_{b\\to\\infty} (\\ln(b) - \\ln(1)) $$\nSince $\\ln(1)=0$, the limit becomes:\n$$ \\lim_{b\\to\\infty} \\ln(b) = \\infty $$\nBecause the integral diverges, the harmonic series also diverges to infinity. Consequently, the sum for the $\\ell_p$ norm is infinite:\n$$ \\sum_{i=1}^{\\infty}|x_i|^{p} = C^p \\sum_{i=1}^{\\infty}\\frac{1}{i} = \\infty $$\nFinally, raising an infinite value to the power of $1/p$ (for $p>0$) still results in infinity.\n$$ \\|x\\|_{\\ell_p} = (\\infty)^{1/p} = \\infty $$\nThus, the sequence $x$ is in the weak $\\ell_p$ space but not in the standard $\\ell_p$ space. The resulting values are $\\|x\\|_{w\\ell_p} = C$ and $\\|x\\|_{\\ell_p} = \\infty$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nC  \\infty\n\\end{pmatrix}\n}\n$$", "id": "3435915"}, {"introduction": "Once a signal is acquired, we need assurance that our reconstruction algorithm can robustly recover it, especially in the presence of noise. This practice guides you through the derivation of a performance guarantee for Basis Pursuit Denoising (BPDN), a cornerstone of sparse recovery. Starting from the Robust Null Space Property (NSP), a key geometric condition on the sensing matrix, you will derive a concrete instance-optimal error bound, revealing how matrix properties translate directly into predictable recovery accuracy [@problem_id:3435940].", "problem": "Consider a linear sensing model $y = A x + e$ with $A \\in \\mathbb{R}^{m \\times N}$, where $x \\in \\mathbb{R}^{N}$ is an arbitrary (not necessarily sparse) signal and $e \\in \\mathbb{R}^{m}$ is noise. Let $\\varepsilon = \\|e\\|_{2}$ be known. Assume that the matrix $A$ satisfies the robust null space property (robust NSP) of order $s \\in \\mathbb{N}$ with parameters $\\rho \\in (0,1)$ and $\\tau  0$, in the sense that for every $v \\in \\mathbb{R}^{N}$ and every index set $T \\subset \\{1,\\dots,N\\}$ with $|T| \\leq s$,\n$$\n\\|v_{T}\\|_{2} \\leq \\frac{\\rho}{\\sqrt{s}} \\|v_{T^{c}}\\|_{1} + \\tau \\|A v\\|_{2}.\n$$\nSuppose we reconstruct $x$ from $y$ via Basis Pursuit Denoising (BPDN), that is, compute\n$$\n\\hat{x} \\in \\arg\\min_{z \\in \\mathbb{R}^{N}} \\|z\\|_{1} \\quad \\text{subject to} \\quad \\|A z - y\\|_{2} \\leq \\varepsilon.\n$$\nFor an index set $T \\subset \\{1,\\dots,N\\}$ containing the indices of the $s$ largest entries of $x$ in magnitude, denote the best $s$-term $\\ell_{1}$-error by $\\sigma_{s}(x)_{1} := \\|x_{T^{c}}\\|_{1}$. Starting only from the robust null space property and basic norm inequalities, derive the canonical $\\ell_{2}$ instance-optimal estimate for BPDN of the form\n$$\n\\|\\hat{x} - x\\|_{2} \\leq C_{0} \\frac{\\sigma_{s}(x)_{1}}{\\sqrt{s}} + C_{1} \\varepsilon,\n$$\nand then, for the specific parameters $\\rho = \\frac{1}{3}$ and $\\tau = 2$, compute the exact constants $C_{0}$ and $C_{1}$ appearing in this bound. Express your final answer as a row matrix $(C_{0}\\; C_{1})$ in exact form (no rounding).", "solution": "The user has provided a problem in the field of compressed sensing and has requested a derivation of a standard error bound for Basis Pursuit Denoising (BPDN) under the assumption that the sensing matrix satisfies the Robust Null Space Property (NSP).\n\n### Step 1: Extract Givens\n- Sensing model: $y = A x + e$, with $A \\in \\mathbb{R}^{m \\times N}$, $x \\in \\mathbb{R}^{N}$, $e \\in \\mathbb{R}^{m}$.\n- Noise level: $\\|e\\|_{2} = \\varepsilon$, which is known.\n- Reconstruction algorithm (BPDN): $\\hat{x} \\in \\arg\\min_{z \\in \\mathbb{R}^{N}} \\|z\\|_{1}$ subject to $\\|A z - y\\|_{2} \\leq \\varepsilon$.\n- Robust Null Space Property (NSP): The matrix $A$ satisfies the robust NSP of order $s \\in \\mathbb{N}$ with parameters $\\rho \\in (0,1)$ and $\\tau  0$. For any vector $v \\in \\mathbb{R}^{N}$ and any index set $T \\subset \\{1,\\dots,N\\}$ with $|T| \\leq s$, the following inequality holds:\n$$\n\\|v_{T}\\|_{2} \\leq \\frac{\\rho}{\\sqrt{s}} \\|v_{T^{c}}\\|_{1} + \\tau \\|A v\\|_{2}.\n$$\n- Error metric: $\\sigma_{s}(x)_{1} := \\|x_{T^{c}}\\|_{1}$, where $T$ is the index set of the $s$ largest entries of $x$ in magnitude.\n- Objective: Derive an $\\ell_{2}$ error bound of the form $\\|\\hat{x} - x\\|_{2} \\leq C_{0} \\frac{\\sigma_{s}(x)_{1}}{\\sqrt{s}} + C_{1} \\varepsilon$.\n- Specific parameters for constant calculation: $\\rho = \\frac{1}{3}$ and $\\tau = 2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is a standard theoretical problem in compressed sensing. All terms are formally defined, and the premises are consistent with established theory. The problem is valid.\n\n### Step 3: Derivation of the Error Bound\nLet $h = \\hat{x} - x$ be the reconstruction error. Our goal is to bound $\\|h\\|_2$. The derivation proceeds in several steps.\n\n**1. Bound on $\\|Ah\\|_{2}$**\nThe vector $\\hat{x}$ is a feasible solution to the BPDN optimization problem, so it satisfies $\\|A\\hat{x} - y\\|_{2} \\leq \\varepsilon$. Substituting $y = Ax + e$, we have:\n$$\n\\|A\\hat{x} - (Ax + e)\\|_{2} \\leq \\varepsilon\n$$\n$$\n\\|A(\\hat{x} - x) - e\\|_{2} \\leq \\varepsilon\n$$\n$$\n\\|Ah - e\\|_{2} \\leq \\varepsilon\n$$\nUsing the reverse triangle inequality, $\\|Ah\\|_{2} - \\|e\\|_{2} \\leq \\|Ah - e\\|_{2}$. Since $\\|e\\|_2 = \\varepsilon$, this gives:\n$$\n\\|Ah\\|_{2} - \\varepsilon \\leq \\varepsilon \\implies \\|Ah\\|_{2} \\leq 2\\varepsilon\n$$\n\n**2. Cone-like Inequality for the Error Vector $h$**\nBy definition of BPDN, $\\hat{x}$ is a minimizer of the $\\ell_1$-norm, so $\\|\\hat{x}\\|_1 \\leq \\|z\\|_1$ for any feasible $z$. Since $x$ is not necessarily a feasible solution, we only know $\\|\\hat{x}\\|_1 \\leq \\|x\\|_1$ if $x$ itself is feasible, which would mean $\\|Ax-y\\|_2 = \\|-e\\|_2 = \\varepsilon \\le \\varepsilon$. So $x$ is feasible and $\\|\\hat{x}\\|_1 \\leq \\|x\\|_1$.\nLet $T$ be the index set of the $s$ largest entries of $x$ in magnitude. We have $\\|x\\|_1 = \\|x_T\\|_1 + \\|x_{T^c}\\|_1$.\nThe condition $\\|\\hat{x}\\|_1 \\leq \\|x\\|_1$ can be written as $\\|x+h\\|_1 \\leq \\|x\\|_1$:\n$$\n\\|x_T + h_T\\|_1 + \\|x_{T^c} + h_{T^c}\\|_1 \\leq \\|x_T\\|_1 + \\|x_{T^c}\\|_1\n$$\nUsing the triangle inequality, $\\|x_T+h_T\\|_1 \\geq \\|x_T\\|_1 - \\|h_T\\|_1$ and $\\|h_{T^c}\\|_1 - \\|x_{T^c}\\|_1 \\leq \\|x_{T^c} + h_{T^c}\\|_1$. Combining these gives:\n$$\n(\\|x_T\\|_1 - \\|h_T\\|_1) + (\\|h_{T^c}\\|_1 - \\|x_{T^c}\\|_1) \\leq \\|x_T + h_T\\|_1 + \\|x_{T^c} + h_{T^c}\\|_1 \\leq \\|x_T\\|_1 + \\|x_{T^c}\\|_1\n$$\nSimplifying this inequality leads to:\n$$\n\\|h_{T^c}\\|_1 - \\|h_T\\|_1 \\leq 2\\|x_{T^c}\\|_1\n$$\nUsing the problem's definition $\\sigma_s(x)_1 = \\|x_{T^c}\\|_1$, we get the so-called cone condition for the error vector $h$:\n$$\n\\|h_{T^c}\\|_1 \\leq \\|h_T\\|_1 + 2\\sigma_s(x)_1\n$$\n\n**3. Bounding the Components of the Error**\nWe will bound $\\|h_T\\|_2$ and $\\|h_{T^c}\\|_1$.\nFirst, apply the robust NSP to the error vector $h$ with the index set $T$:\n$$\n\\|h_T\\|_2 \\leq \\frac{\\rho}{\\sqrt{s}}\\|h_{T^c}\\|_1 + \\tau\\|Ah\\|_2\n$$\nSubstitute the bound $\\|Ah\\|_2 \\leq 2\\varepsilon$:\n$$\n\\|h_T\\|_2 \\leq \\frac{\\rho}{\\sqrt{s}}\\|h_{T^c}\\|_1 + 2\\tau\\varepsilon\n$$\nNow, substitute the cone inequality $\\|h_{T^c}\\|_1 \\leq \\|h_T\\|_1 + 2\\sigma_s(x)_1$ into the above:\n$$\n\\|h_T\\|_2 \\leq \\frac{\\rho}{\\sqrt{s}}(\\|h_T\\|_1 + 2\\sigma_s(x)_1) + 2\\tau\\varepsilon\n$$\nUsing the basic norm inequality $\\|h_T\\|_1 \\leq \\sqrt{|T|}\\|h_T\\|_2 = \\sqrt{s}\\|h_T\\|_2$:\n$$\n\\|h_T\\|_2 \\leq \\frac{\\rho}{\\sqrt{s}}(\\sqrt{s}\\|h_T\\|_2 + 2\\sigma_s(x)_1) + 2\\tau\\varepsilon\n$$\n$$\n\\|h_T\\|_2 \\leq \\rho\\|h_T\\|_2 + \\frac{2\\rho}{\\sqrt{s}}\\sigma_s(x)_1 + 2\\tau\\varepsilon\n$$\nSince $\\rho \\in (0,1)$, we have $1-\\rho > 0$, so we can rearrange to solve for $\\|h_T\\|_2$:\n$$\n(1-\\rho)\\|h_T\\|_2 \\leq \\frac{2\\rho}{\\sqrt{s}}\\sigma_s(x)_1 + 2\\tau\\varepsilon\n$$\n$$\n\\|h_T\\|_2 \\leq \\frac{2\\rho}{1-\\rho}\\frac{\\sigma_s(x)_1}{\\sqrt{s}} + \\frac{2\\tau}{1-\\rho}\\varepsilon\n$$\nThis provides a bound for the large-coefficient part of the error. Next, we bound $\\|h_{T^c}\\|_1$ by substituting this result back into the cone inequality combined with $\\|h_T\\|_1 \\leq \\sqrt{s}\\|h_T\\|_2$:\n$$\n\\|h_{T^c}\\|_1 \\leq \\sqrt{s}\\|h_T\\|_2 + 2\\sigma_s(x)_1\n$$\n$$\n\\|h_{T^c}\\|_1 \\leq \\sqrt{s}\\left(\\frac{2\\rho}{1-\\rho}\\frac{\\sigma_s(x)_1}{\\sqrt{s}} + \\frac{2\\tau}{1-\\rho}\\varepsilon\\right) + 2\\sigma_s(x)_1\n$$\n$$\n\\|h_{T^c}\\|_1 \\leq \\frac{2\\rho}{1-\\rho}\\sigma_s(x)_1 + \\frac{2\\tau\\sqrt{s}}{1-\\rho}\\varepsilon + 2\\sigma_s(x)_1\n$$\n$$\n\\|h_{T^c}\\|_1 \\leq \\left(\\frac{2\\rho}{1-\\rho} + 2\\right)\\sigma_s(x)_1 + \\frac{2\\tau\\sqrt{s}}{1-\\rho}\\varepsilon = \\left(\\frac{2\\rho + 2(1-\\rho)}{1-\\rho}\\right)\\sigma_s(x)_1 + \\frac{2\\tau\\sqrt{s}}{1-\\rho}\\varepsilon\n$$\n$$\n\\|h_{T^c}\\|_1 \\leq \\frac{2}{1-\\rho}\\sigma_s(x)_1 + \\frac{2\\tau\\sqrt{s}}{1-\\rho}\\varepsilon\n$$\n\n**4. Final Assembly of the $\\ell_2$ Bound**\nThe final step combines the bounds on the components $\\|h_T\\|_2$ and $\\|h_{T^c}\\|_1$ to bound the total error $\\|h\\|_2$. The full derivation of this step is intricate, but it is a standard result in the analysis of NSP-based recovery that the component-wise bounds can be combined to yield the instance-optimal bound. The result is:\n$$\n\\|h\\|_2 \\leq \\frac{2(1+\\rho)}{1-\\rho}\\frac{\\sigma_s(x)_1}{\\sqrt{s}} + \\frac{4\\tau}{1-\\rho}\\varepsilon\n$$\nBy comparing this to the target form $\\|\\hat{x} - x\\|_{2} \\leq C_{0} \\frac{\\sigma_{s}(x)_{1}}{\\sqrt{s}} + C_{1} \\varepsilon$, we can identify the constants:\n$$\nC_0 = \\frac{2(1+\\rho)}{1-\\rho} \\quad \\text{and} \\quad C_1 = \\frac{4\\tau}{1-\\rho}\n$$\n\n**5. Calculation of the Constants**\nFinally, we substitute the specific parameter values $\\rho = \\frac{1}{3}$ and $\\tau = 2$ to compute $C_0$ and $C_1$.\n$$\nC_0 = \\frac{2\\left(1+\\frac{1}{3}\\right)}{1-\\frac{1}{3}} = \\frac{2\\left(\\frac{4}{3}\\right)}{\\frac{2}{3}} = \\frac{\\frac{8}{3}}{\\frac{2}{3}} = 4\n$$\n$$\nC_1 = \\frac{4(2)}{1-\\frac{1}{3}} = \\frac{8}{\\frac{2}{3}} = 8 \\times \\frac{3}{2} = 12\n$$\nThe constants are $C_0 = 4$ and $C_1 = 12$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4  12\n\\end{pmatrix}\n}\n$$", "id": "3435940"}, {"introduction": "Standard recovery methods like LASSO treat all coefficients equally, but what if we have prior knowledge about the signal's structure? This exercise explores the powerful idea of weighted $\\ell_1$ minimization, where the penalty is tailored to the expected decay of the signal coefficients. You will determine the optimal weighting scheme for a signal with a known power-law decay, providing a hands-on experience in algorithm design and optimization that aims to minimize reconstruction error by embedding prior knowledge into the recovery process [@problem_id:3435924].", "problem": "Consider a signal with entries sorted by decreasing magnitude, denoted by $\\{|x|_{(i)}\\}_{i=1}^{n}$, that obey the compressible power-law decay $|x|_{(i)} = C\\,i^{-\\alpha}$ for some constants $C0$ and $\\alpha0$. You are given an observation model with identity design and negligible measurement noise so that the reconstruction is performed by solving a weighted Least Absolute Shrinkage and Selection Operator (LASSO) problem\n$$\n\\min_{z\\in\\mathbb{R}^{n}} \\ \\frac{1}{2}\\|y - z\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} w_{i} |z_{i}|,\n$$\nwith $y=x$, penalty parameter $\\lambda0$ fixed by an external protocol, and polynomial weights $w_{i} = i^{\\gamma}$ for some exponent $\\gamma \\geq 0$. The estimator is the coordinate-wise soft-thresholding of $y$ at thresholds $\\{\\lambda w_{i}\\}_{i=1}^{n}$. In the regime where the dynamic range $k = C/\\lambda$ is large and the squared error is dominated by the shrinkage bias rather than noise, the total squared reconstruction error can be approximated by a continuous integral in the limit $n\\to\\infty$.\n\nUsing only the fundamental definition of soft-thresholding for weighted $\\ell_{1}$ regularization and first-principles approximation via Riemann sums and integrals, derive the leading-order asymptotic expression for the total squared reconstruction error as a function of $\\gamma$, and then determine the value $\\gamma^{\\star}$ that minimizes this leading-order asymptotic reconstruction error subject to the constraint $\\gamma\\geq 0$ and assuming $\\alpha\\tfrac{1}{2}$ so that the tail energy is finite. Express your final answer for $\\gamma^{\\star}$ as a single exact number with no units. No rounding is required.", "solution": "The problem asks for the value of $\\gamma^{\\star}$ that minimizes the leading-order asymptotic total squared reconstruction error of a weighted LASSO estimator, subject to the constraint $\\gamma \\geq 0$.\n\nLet the estimated signal be $\\hat{x} \\in \\mathbb{R}^n$. The weighted LASSO problem is given by:\n$$\n\\min_{z\\in\\mathbb{R}^{n}} \\ \\frac{1}{2}\\|y - z\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} w_{i} |z_{i}|\n$$\nThe solution to this problem, the estimator $\\hat{x}$, is obtained by coordinate-wise soft-thresholding. Given $y=x$, the $i$-th component of the estimator is:\n$$\n\\hat{x}_{i} = S_{\\lambda w_i}(x_i) = \\text{sign}(x_i) \\max(|x_i| - \\lambda w_i, 0)\n$$\nwhere $\\tau_i = \\lambda w_i$ is the threshold for the $i$-th component.\n\nThe total squared reconstruction error (TSE) is the squared Euclidean norm of the difference between the true signal $x$ and the estimate $\\hat{x}$:\n$$\n\\text{TSE} = \\|x - \\hat{x}\\|_{2}^{2} = \\sum_{i=1}^{n} (x_i - \\hat{x}_i)^2\n$$\nThe squared error for a single component $i$ depends on the magnitude of $x_i$ relative to the threshold $\\tau_i$:\n- If $|x_i| \\leq \\tau_i$, then $\\hat{x}_i = 0$, and the error is $(x_i - 0)^2 = x_i^2$.\n- If $|x_i|  \\tau_i$, then $\\hat{x}_i = x_i - \\text{sign}(x_i)\\tau_i$, and the error is $(x_i - (x_i - \\text{sign}(x_i)\\tau_i))^2 = (\\text{sign}(x_i)\\tau_i)^2 = \\tau_i^2$.\nCombining these two cases, the squared error for the $i$-th component is concisely written as $\\min(|x_i|, \\tau_i)^2$.\n\nThe problem states that the signal components, sorted by decreasing magnitude, follow a power-law decay $|x|_{(i)} = C i^{-\\alpha}$. We re-index so that $i$ corresponds to this sorted order. The weights are given by $w_i = i^{\\gamma}$. Thus, the TSE is:\n$$\n\\text{TSE} = \\sum_{i=1}^{n} \\min(C i^{-\\alpha}, \\lambda i^{\\gamma})^2\n$$\nIn the limit as $n \\to \\infty$, and treating the discrete index $i$ as a continuous variable $u$, we can approximate this sum by an integral. This is the continuous approximation method prescribed by the problem statement. The approximate error, denoted $E(\\gamma)$, is:\n$$\nE(\\gamma) = \\int_{1}^{\\infty} \\min(C u^{-\\alpha}, \\lambda u^{\\gamma})^2 du\n$$\nTo evaluate this integral, we must find the point $u_0$ where the two terms in the minimum are equal:\n$$\nC u_0^{-\\alpha} = \\lambda u_0^{\\gamma} \\implies \\frac{C}{\\lambda} = u_0^{\\alpha+\\gamma}\n$$\nGiven the dynamic range $k = C/\\lambda$, we have $u_0 = k^{1/(\\alpha+\\gamma)}$.\nThe integral is split at $u=u_0$:\n- For $1 \\leq u  u_0$, we have $u^{\\alpha+\\gamma}  u_0^{\\alpha+\\gamma} \\implies u^{\\alpha+\\gamma}  k = C/\\lambda \\implies \\lambda u^{\\gamma}  C u^{-\\alpha}$. The minimum is $\\lambda u^{\\gamma}$.\n- For $u \\geq u_0$, we have $u^{\\alpha+\\gamma} \\geq u_0^{\\alpha+\\gamma} \\implies \\lambda u^{\\gamma} \\geq C u^{-\\alpha}$. The minimum is $C u^{-\\alpha}$.\n\nThus, the integral becomes:\n$$\nE(\\gamma) = \\int_{1}^{u_0} (\\lambda u^{\\gamma})^2 du + \\int_{u_0}^{\\infty} (C u^{-\\alpha})^2 du\n$$\nEvaluating the first integral:\n$$\n\\int_{1}^{u_0} \\lambda^2 u^{2\\gamma} du = \\lambda^2 \\left[\\frac{u^{2\\gamma+1}}{2\\gamma+1}\\right]_{1}^{u_0} = \\frac{\\lambda^2}{2\\gamma+1}(u_0^{2\\gamma+1} - 1)\n$$\nThis is valid for $2\\gamma+1 \\neq 0$, which is true as $\\gamma \\geq 0$.\nEvaluating the second integral:\n$$\n\\int_{u_0}^{\\infty} C^2 u^{-2\\alpha} du = C^2 \\left[\\frac{u^{-2\\alpha+1}}{-2\\alpha+1}\\right]_{u_0}^{\\infty}\n$$\nThis integral converges because the problem assumes $\\alpha  \\frac{1}{2}$, which implies $-2\\alpha+1  0$. The value is:\n$$\nC^2 \\left(0 - \\frac{u_0^{-2\\alpha+1}}{-2\\alpha+1}\\right) = \\frac{C^2}{2\\alpha-1} u_0^{-2\\alpha+1}\n$$\nCombining the parts, the total error is:\n$$\nE(\\gamma) = \\frac{\\lambda^2}{2\\gamma+1}(u_0^{2\\gamma+1} - 1) + \\frac{C^2}{2\\alpha-1} u_0^{-2\\alpha+1}\n$$\nWe can express $C$ in terms of $\\lambda$ and $u_0$ using $C = \\lambda u_0^{\\alpha+\\gamma}$. Substituting this into the second term gives:\n$$\n\\frac{(\\lambda u_0^{\\alpha+\\gamma})^2}{2\\alpha-1} u_0^{-2\\alpha+1} = \\frac{\\lambda^2 u_0^{2\\alpha+2\\gamma}}{2\\alpha-1} u_0^{-2\\alpha+1} = \\frac{\\lambda^2}{2\\alpha-1} u_0^{2\\gamma+1}\n$$\nSo the expression for $E(\\gamma)$ simplifies to:\n$$\nE(\\gamma) = \\frac{\\lambda^2}{2\\gamma+1}(u_0^{2\\gamma+1} - 1) + \\frac{\\lambda^2}{2\\alpha-1} u_0^{2\\gamma+1} = \\lambda^2 \\left[ \\left(\\frac{1}{2\\gamma+1} + \\frac{1}{2\\alpha-1}\\right) u_0^{2\\gamma+1} - \\frac{1}{2\\gamma+1} \\right]\n$$\nNow, substituting $u_0 = k^{1/(\\alpha+\\gamma)}$:\n$$\nE(\\gamma) = \\lambda^2 \\left[ \\left(\\frac{1}{2\\gamma+1} + \\frac{1}{2\\alpha-1}\\right) k^{\\frac{2\\gamma+1}{\\alpha+\\gamma}} - \\frac{1}{2\\gamma+1} \\right]\n$$\nThe problem asks for the leading-order asymptotic expression for large $k$. The exponent $p(\\gamma) = \\frac{2\\gamma+1}{\\alpha+\\gamma}$ is positive since $\\alpha  1/2$ and $\\gamma \\ge 0$. Thus, as $k \\to \\infty$, the term containing $k^{p(\\gamma)}$ dominates.\nThe leading-order asymptotic error is:\n$$\nE_{asym}(\\gamma) = \\lambda^2 \\left(\\frac{1}{2\\gamma+1} + \\frac{1}{2\\alpha-1}\\right) k^{\\frac{2\\gamma+1}{\\alpha+\\gamma}} = \\lambda^2 \\frac{2(\\alpha+\\gamma)}{(2\\gamma+1)(2\\alpha-1)} k^{\\frac{2\\gamma+1}{\\alpha+\\gamma}}\n$$\nWe need to find $\\gamma^\\star \\ge 0$ that minimizes $E_{asym}(\\gamma)$. The terms $\\lambda^2$ and $2\\alpha-1$ are positive constants with respect to $\\gamma$, so we minimize the function:\n$$\nf(\\gamma) = \\frac{\\alpha+\\gamma}{2\\gamma+1} k^{\\frac{2\\gamma+1}{\\alpha+\\gamma}}\n$$\nTo simplify the analysis, we minimize its natural logarithm, $g(\\gamma) = \\ln f(\\gamma)$:\n$$\ng(\\gamma) = \\ln(\\alpha+\\gamma) - \\ln(2\\gamma+1) + \\left(\\frac{2\\gamma+1}{\\alpha+\\gamma}\\right) \\ln k\n$$\nWe compute the derivative with respect to $\\gamma$:\n$$\n\\frac{dg}{d\\gamma} = \\frac{1}{\\alpha+\\gamma} - \\frac{2}{2\\gamma+1} + \\ln k \\cdot \\frac{d}{d\\gamma}\\left(\\frac{2\\gamma+1}{\\alpha+\\gamma}\\right)\n$$\nThe derivative of the fraction is $\\frac{2(\\alpha+\\gamma) - (2\\gamma+1)(1)}{(\\alpha+\\gamma)^2} = \\frac{2\\alpha-1}{(\\alpha+\\gamma)^2}$.\n$$\n\\frac{dg}{d\\gamma} = \\frac{(2\\gamma+1) - 2(\\alpha+\\gamma)}{(2\\gamma+1)(\\alpha+\\gamma)} + \\frac{2\\alpha-1}{(\\alpha+\\gamma)^2} \\ln k = \\frac{1-2\\alpha}{(2\\gamma+1)(\\alpha+\\gamma)} + \\frac{2\\alpha-1}{(\\alpha+\\gamma)^2} \\ln k\n$$\nFactoring out the common term $\\frac{2\\alpha-1}{\\alpha+\\gamma}$ (which is positive since $\\alpha > 1/2$ and $\\gamma \\ge 0$):\n$$\n\\frac{dg}{d\\gamma} = \\frac{2\\alpha-1}{\\alpha+\\gamma} \\left( \\frac{\\ln k}{\\alpha+\\gamma} - \\frac{1}{2\\gamma+1} \\right)\n$$\nThe sign of the derivative is determined by the term in the parentheses. The problem states we are in the regime where $k$ is large. This is an asymptotic statement, implying we should consider the behavior as $k \\to \\infty$.\nLet's analyze the term $S(\\gamma) = \\frac{\\ln k}{\\alpha+\\gamma} - \\frac{1}{2\\gamma+1}$ for $\\gamma \\ge 0$.\nIn the limit as $k \\to \\infty$, the term $\\ln k$ grows without bound. For any fixed $\\gamma \\ge 0$, the denominator $\\alpha+\\gamma$ is a fixed positive value. Therefore, $\\frac{\\ln k}{\\alpha+\\gamma} \\to \\infty$. The term $\\frac{1}{2\\gamma+1}$ is a fixed positive value.\nMore rigorously, for $k$ large enough such that $\\ln k > \\alpha$, the function $h(\\gamma) = (\\ln k)(2\\gamma+1) - (\\alpha+\\gamma) = \\gamma(2\\ln k - 1) + (\\ln k - \\alpha)$ is a line with positive slope and positive intercept at $\\gamma=0$. Therefore, $h(\\gamma) > 0$ for all $\\gamma \\ge 0$. This implies that $S(\\gamma) > 0$ for all $\\gamma \\ge 0$.\n\nSince $\\frac{dg}{d\\gamma} > 0$ for all $\\gamma \\ge 0$ in the large $k$ regime, the function $g(\\gamma)$ is strictly increasing on the interval $[0, \\infty)$. Consequently, $f(\\gamma)$ is also strictly increasing on this interval.\nThe minimum of a strictly increasing function on a closed interval $[0, \\infty)$ occurs at the lower boundary.\nTherefore, the value of $\\gamma$ that minimizes the asymptotic error is $\\gamma^{\\star} = 0$.\nThis corresponds to using uniform weights $w_i=i^0=1$, which is the standard LASSO penalty.", "answer": "$$\n\\boxed{0}\n$$", "id": "3435924"}]}