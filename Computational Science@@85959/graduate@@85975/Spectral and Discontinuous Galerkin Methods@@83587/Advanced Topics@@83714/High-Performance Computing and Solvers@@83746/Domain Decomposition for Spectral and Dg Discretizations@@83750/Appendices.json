{"hands_on_practices": [{"introduction": "This first practice tackles a fundamental concept in non-conforming domain decomposition: the mortar method. When adjacent subdomains use different mesh resolutions or polynomial degrees, a projection is needed to enforce continuity of the solution. This exercise guides you through calculating the error incurred by such a projection, connecting it directly to the polynomial approximation properties of the solution on the interface. [@problem_id:3381380] Understanding this error is crucial for analyzing the accuracy of non-conforming spectral and Discontinuous Galerkin methods.", "problem": "Consider a nonconforming interface (mortar) in a Discontinuous Galerkin (DG) domain decomposition setting for one-dimensional spectral elements. Let the physical interface (face) be the image of the reference interval $[-1,1]$ under an affine mapping with constant Jacobian $J$, so that $J = h/2$ where $h>0$ is the physical face length. The mortar space is the polynomial space $\\mathbb{P}_{q}$ with $q<p$, and the face trace $u(\\xi)$ is a polynomial of degree $p$ expanded in the orthonormal Legendre basis $\\{L_{k}(\\xi)\\}_{k=0}^{p}$ on $[-1,1]$, where $\\int_{-1}^{1} L_{k}(\\xi)L_{m}(\\xi)\\,\\mathrm{d}\\xi = \\delta_{km}$. The $L^{2}$ mortar projection $\\Pi_{q}:L^{2}([-1,1])\\to \\mathbb{P}_{q}$ is defined via the weighted $L^{2}$ inner product on the physical face,\n$$\n(u,v)_{\\Gamma} := \\int_{-1}^{1} u(\\xi)\\,v(\\xi)\\,J\\,\\mathrm{d}\\xi,\n$$\nand its error is measured by the norm $\\|u-\\Pi_{q}u\\|_{L^{2}(\\Gamma)} := \\left((u-\\Pi_{q}u,u-\\Pi_{q}u)_{\\Gamma}\\right)^{1/2}$.\n\nStarting from the definitions of orthonormality and the $L^{2}$ projection as the unique minimizer of the $L^{2}$ norm of the error, derive the exact expression for the mortar projection error $\\|u-\\Pi_{q}u\\|_{L^{2}(\\Gamma)}$ in terms of the tail coefficients of the orthonormal Legendre expansion of $u$ and the best approximation error\n$$\nE_{q}(u) := \\inf_{v\\in \\mathbb{P}_{q}} \\|u-v\\|_{L^{2}([-1,1])}.\n$$\nThen, for the specific case $p=5$, $q=2$, and\n$$\nu(\\xi) = \\sum_{k=0}^{5} a_{k}\\,L_{k}(\\xi), \\quad a_{3} = 2,\\quad a_{4} = -1,\\quad a_{5} = 3,\n$$\ncompute the exact value of $\\|u-\\Pi_{q}u\\|_{L^{2}(\\Gamma)}$ and express your final answer as a single closed-form analytic expression in terms of $h$. No rounding is required, and no physical units should be used in the final answer.", "solution": "The problem asks for two main results:\n1. A derivation of the exact expression for the mortar projection error $\\|u-\\Pi_{q}u\\|_{L^{2}(\\Gamma)}$ in terms of the tail coefficients of the orthonormal Legendre expansion of $u$ and the best approximation error $E_{q}(u)$.\n2. The computation of this error for a specific case.\n\n**Part 1: General Derivation of the Projection Error**\n\nLet $u(\\xi)$ be a polynomial of degree $p$, $u \\in \\mathbb{P}_{p}$, defined on the reference interval $[-1, 1]$. It has an expansion in the orthonormal Legendre basis $\\{L_{k}(\\xi)\\}_{k=0}^{p}$ given by:\n$$u(\\xi) = \\sum_{k=0}^{p} a_{k}L_{k}(\\xi)$$\nwhere the coefficients are $a_k = \\int_{-1}^{1} u(\\xi) L_k(\\xi) \\,d\\xi$. The orthonormality condition is $\\int_{-1}^{1} L_{k}(\\xi)L_{m}(\\xi)\\,\\mathrm{d}\\xi = \\delta_{km}$.\n\nThe mortar projection $\\Pi_{q}u$ is the $L^{2}$ projection of $u$ onto the polynomial space $\\mathbb{P}_{q}$ for $q<p$. The projection is defined with respect to the weighted inner product on the physical face $\\Gamma$, $(f,g)_{\\Gamma} := \\int_{-1}^{1} f(\\xi)g(\\xi)J\\,\\mathrm{d}\\xi$, where $J = h/2$ is the constant Jacobian.\n\nBy definition, the $L^{2}$ projection $\\Pi_{q}u$ is the unique element in $\\mathbb{P}_{q}$ that minimizes the error in the corresponding norm. This is equivalent to the orthogonality condition that the error $u - \\Pi_{q}u$ is orthogonal to the subspace $\\mathbb{P}_{q}$:\n$$(u - \\Pi_{q}u, v)_{\\Gamma} = 0 \\quad \\forall v \\in \\mathbb{P}_{q}$$\nSince $\\{L_{m}(\\xi)\\}_{m=0}^{q}$ is a basis for $\\mathbb{P}_{q}$, it is sufficient to enforce this for each basis function:\n$$(u - \\Pi_{q}u, L_{m})_{\\Gamma} = 0 \\quad \\text{for } m = 0, 1, \\dots, q$$\nThis implies $(u, L_{m})_{\\Gamma} = (\\Pi_{q}u, L_{m})_{\\Gamma}$.\n\nLet the projection be expressed in the basis of $\\mathbb{P}_q$ as $\\Pi_{q}u(\\xi) = \\sum_{j=0}^{q} c_{j}L_{j}(\\xi)$. Substituting the expansions for $u$ and $\\Pi_q u$ into the orthogonality condition, we get:\n$$\\left(\\sum_{k=0}^{p} a_{k}L_{k}, L_{m}\\right)_{\\Gamma} = \\left(\\sum_{j=0}^{q} c_{j}L_{j}, L_{m}\\right)_{\\Gamma} \\quad \\text{for } m = 0, \\dots, q$$\nUsing the definition of the inner product $(f,g)_{\\Gamma} = J \\int_{-1}^{1} fg \\,d\\xi$:\n$$J \\int_{-1}^{1} \\left(\\sum_{k=0}^{p} a_{k}L_{k}(\\xi)\\right) L_{m}(\\xi) \\,d\\xi = J \\int_{-1}^{1} \\left(\\sum_{j=0}^{q} c_{j}L_{j}(\\xi)\\right) L_{m}(\\xi) \\,d\\xi$$\nSince $J$ is a non-zero constant, it can be canceled. By linearity of the integral:\n$$\\sum_{k=0}^{p} a_{k} \\int_{-1}^{1} L_{k}(\\xi)L_{m}(\\xi) \\,d\\xi = \\sum_{j=0}^{q} c_{j} \\int_{-1}^{1} L_{j}(\\xi)L_{m}(\\xi) \\,d\\xi$$\nUsing the orthonormality relation $\\int_{-1}^{1} L_{k}(\\xi)L_{m}(\\xi)\\,\\mathrm{d}\\xi = \\delta_{km}$:\n$$\\sum_{k=0}^{p} a_{k} \\delta_{km} = \\sum_{j=0}^{q} c_{j} \\delta_{jm}$$\nFor any given $m \\in \\{0, \\dots, q\\}$, the sums collapse to a single term on each side:\n$$a_{m} = c_{m}$$\nThis holds for all $m = 0, \\dots, q$. Therefore, the mortar projection is simply the truncated Legendre series of $u$:\n$$\\Pi_{q}u(\\xi) = \\sum_{k=0}^{q} a_{k}L_{k}(\\xi)$$\nThe projection error is the difference between $u$ and its projection:\n$$u(\\xi) - \\Pi_{q}u(\\xi) = \\sum_{k=0}^{p} a_{k}L_{k}(\\xi) - \\sum_{k=0}^{q} a_{k}L_{k}(\\xi) = \\sum_{k=q+1}^{p} a_{k}L_{k}(\\xi)$$\nThe squared norm of the projection error is:\n$$\\|u-\\Pi_{q}u\\|_{L^{2}(\\Gamma)}^{2} = (u-\\Pi_{q}u, u-\\Pi_{q}u)_{\\Gamma} = \\left(\\sum_{k=q+1}^{p} a_{k}L_{k}, \\sum_{j=q+1}^{p} a_{j}L_{j}\\right)_{\\Gamma}$$\n$$= J \\int_{-1}^{1} \\left(\\sum_{k=q+1}^{p} a_{k}L_{k}(\\xi)\\right) \\left(\\sum_{j=q+1}^{p} a_{j}L_{j}(\\xi)\\right) d\\xi$$\n$$= J \\sum_{k=q+1}^{p} \\sum_{j=q+1}^{p} a_{k}a_{j} \\int_{-1}^{1} L_{k}(\\xi)L_{j}(\\xi) d\\xi$$\n$$= J \\sum_{k=q+1}^{p} \\sum_{j=q+1}^{p} a_{k}a_{j} \\delta_{kj} = J \\sum_{k=q+1}^{p} a_{k}^{2}$$\nTaking the square root gives the error in terms of the tail coefficients $a_k$ for $k > q$:\n$$\\|u-\\Pi_{q}u\\|_{L^{2}(\\Gamma)} = \\sqrt{J \\sum_{k=q+1}^{p} a_{k}^{2}}$$\nNow, we relate this to the best approximation error $E_{q}(u)$, defined as $E_{q}(u) := \\inf_{v\\in \\mathbb{P}_{q}} \\|u-v\\|_{L^{2}([-1,1])}$, where the norm is the standard (unweighted) $L^{2}$ norm on the reference element, $\\|f\\|_{L^{2}([-1,1])} = (\\int_{-1}^{1} (f(\\xi))^2 d\\xi)^{1/2}$.\nThe best approximation in $\\mathbb{P}_{q}$ is given by the orthogonal projection of $u$ onto $\\mathbb{P}_{q}$ with respect to the standard $L^{2}([-1,1])$ inner product. Since the basis $\\{L_k\\}$ is orthonormal for this inner product as well, the best approximation is again the truncated series $\\sum_{k=0}^{q} a_k L_k(\\xi)$.\nThe best approximation error is the norm of the remainder:\n$$E_{q}(u)^{2} = \\left\\| u - \\sum_{k=0}^{q} a_k L_k \\right\\|_{L^{2}([-1,1])}^{2} = \\left\\| \\sum_{k=q+1}^{p} a_k L_k \\right\\|_{L^{2}([-1,1])}^{2}$$\n$$= \\int_{-1}^{1} \\left(\\sum_{k=q+1}^{p} a_{k}L_{k}(\\xi)\\right)^{2} d\\xi = \\sum_{k=q+1}^{p} a_{k}^{2}$$\nThus, the best approximation error is given by the root mean square of the tail coefficients:\n$$E_{q}(u) = \\sqrt{\\sum_{k=q+1}^{p} a_{k}^{2}}$$\nCombining these results, the mortar-projection error can be expressed in terms of both the tail coefficients and the best approximation error as:\n$$\\|u-\\Pi_{q}u\\|_{L^{2}(\\Gamma)} = \\sqrt{J} \\left(\\sum_{k=q+1}^{p} a_{k}^{2}\\right)^{1/2} = \\sqrt{J} E_{q}(u)$$\n\n**Part 2: Specific Calculation**\n\nWe are given the specific case with:\n-   Polynomial degree $p=5$\n-   Mortar space degree $q=2$\n-   Jacobian $J = h/2$\n-   Legendre coefficients $a_{3} = 2$, $a_{4} = -1$, and $a_{5} = 3$.\n\nThe coefficients $a_0, a_1, a_2$ are not needed, as the error only depends on the tail coefficients for $k > q$.\nUsing the formula derived in Part 1, the projection error is:\n$$\\|u-\\Pi_{2}u\\|_{L^{2}(\\Gamma)} = \\sqrt{J \\sum_{k=2+1}^{5} a_{k}^{2}} = \\sqrt{\\frac{h}{2} \\sum_{k=3}^{5} a_{k}^{2}}$$\nWe compute the sum of the squares of the relevant coefficients:\n$$\\sum_{k=3}^{5} a_{k}^{2} = a_{3}^{2} + a_{4}^{2} + a_{5}^{2}$$\n$$\\sum_{k=3}^{5} a_{k}^{2} = (2)^{2} + (-1)^{2} + (3)^{2} = 4 + 1 + 9 = 14$$\nSubstituting this value back into the expression for the error norm:\n$$\\|u-\\Pi_{2}u\\|_{L^{2}(\\Gamma)} = \\sqrt{\\frac{h}{2} \\cdot 14} = \\sqrt{7h}$$\nThis is the exact value of the mortar projection error in terms of $h$.", "answer": "$$\\boxed{\\sqrt{7h}}$$", "id": "3381380"}, {"introduction": "Moving from theory to practice, we often encounter nonlinearities, such as in fluid dynamics problems with convective terms. This exercise explores a critical implementation detail: the choice of numerical quadrature for interface integrals involving nonlinear fluxes. You will quantify the aliasing error that arises from using an under-integrating quadrature rule, a common source of instability and inaccuracy in high-order simulations. [@problem_id:3381360] This hands-on calculation demonstrates why careful consideration of quadrature is essential for robustly handling nonlinear physics.", "problem": "Consider a one-dimensional interface arising from domain decomposition in a spectral element discretization of a scalar conservation law, where flux continuity is enforced by a mortar projection in a Discontinuous Galerkin (DG) method. Let the mortar space be the span of Legendre polynomials up to degree $m=2$ on the canonical interval $[-1,1]$, with basis functions $P_{0}(s)$, $P_{1}(s)$, and $P_{2}(s)$, where $P_{2}(s) = \\frac{1}{2}\\left(3 s^{2} - 1\\right)$. Consider the nonlinear flux $F(u) = u^{3}$ and let the interface trace be the polynomial $u(s) = \\alpha s + \\beta$ with real parameters $\\alpha$ and $\\beta$. The mortar projection coefficient $c_{2}$ is defined by the $L^{2}$ inner product\n$$\nc_{2} = \\int_{-1}^{1} F\\!\\left(u(s)\\right) P_{2}(s) \\, ds.\n$$\nIn practice, this integral is evaluated by quadrature rules. The $3$-point Gauss–Legendre quadrature on $[-1,1]$ is exact for polynomials of degree up to $5$, while the $3$-point Gauss–Lobatto quadrature on $[-1,1]$ with nodes $s \\in \\{-1, 0, 1\\}$ and weights $w \\in \\left\\{\\frac{1}{3}, \\frac{4}{3}, \\frac{1}{3}\\right\\}$ is exact for polynomials of degree up to $3$. Define the aliasing error incurred by using Gauss–Lobatto instead of Gauss–Legendre in the evaluation of $c_{2}$ as\n$$\nE = Q_{\\mathrm{GLob}}\\!\\left(g\\right) - Q_{\\mathrm{Gauss}}\\!\\left(g\\right),\n$$\nwhere $g(s) = F\\!\\left(u(s)\\right) P_{2}(s)$, $Q_{\\mathrm{GLob}}$ denotes the $3$-point Gauss–Lobatto quadrature, and $Q_{\\mathrm{Gauss}}$ denotes the $3$-point Gauss–Legendre quadrature. Compute a closed-form analytic expression for $E$ in terms of $\\alpha$ and $\\beta$. Provide your final answer as a single expression. No rounding is required and no units are associated with the answer.", "solution": "The problem requires the computation of the aliasing error, $E$, resulting from the use of a $3$-point Gauss-Lobatto quadrature rule instead of a $3$-point Gauss-Legendre quadrature rule for a specific mortar projection integral in a Discontinuous Galerkin method.\n\nFirst, we define the integrand $g(s)$.\nThe flux evaluated on the interface trace is:\n$$\nF(u(s)) = (\\alpha s + \\beta)^{3}\n$$\nThe integrand is the product of this flux and the Legendre polynomial $P_{2}(s)$:\n$$\ng(s) = F(u(s)) P_{2}(s) = (\\alpha s + \\beta)^{3} \\left( \\frac{1}{2}(3s^{2} - 1) \\right)\n$$\nThe polynomial $F(u(s))$ has degree $3$, and $P_{2}(s)$ has degree $2$. Therefore, the integrand $g(s)$ is a polynomial of degree $3 + 2 = 5$.\n\nThe aliasing error is defined as $E = Q_{\\mathrm{GLob}}(g) - Q_{\\mathrm{Gauss}}(g)$.\nThe problem states that the $3$-point Gauss-Legendre quadrature, $Q_{\\mathrm{Gauss}}$, is exact for polynomials of degree up to $5$. Since $\\deg(g) = 5$, the quadrature yields the exact value of the integral:\n$$\nQ_{\\mathrm{Gauss}}(g) = \\int_{-1}^{1} g(s) \\, ds\n$$\nThe error expression thus simplifies to:\n$$\nE = Q_{\\mathrm{GLob}}(g) - \\int_{-1}^{1} g(s) \\, ds\n$$\nThis is precisely the quadrature error of the $3$-point Gauss-Lobatto rule when applied to the function $g(s)$.\n\nTo compute $E$, we must calculate both terms on the right-hand side.\n\n**1. Calculation of the exact integral, $\\int_{-1}^{1} g(s) \\, ds$**\n\nFirst, we expand the polynomial $g(s)$:\n$$\nF(u(s)) = (\\alpha s + \\beta)^{3} = \\alpha^{3}s^{3} + 3\\alpha^{2}\\beta s^{2} + 3\\alpha\\beta^{2}s + \\beta^{3}\n$$\nThen, we multiply by $P_{2}(s) = \\frac{3}{2}s^{2} - \\frac{1}{2}$:\n$$\ng(s) = \\left( \\alpha^{3}s^{3} + 3\\alpha^{2}\\beta s^{2} + 3\\alpha\\beta^{2}s + \\beta^{3} \\right) \\left( \\frac{3}{2}s^{2} - \\frac{1}{2} \\right)\n$$\n$$\ng(s) = \\frac{3}{2}\\alpha^{3}s^{5} + \\frac{9}{2}\\alpha^{2}\\beta s^{4} + \\left( \\frac{9}{2}\\alpha\\beta^{2} - \\frac{1}{2}\\alpha^{3} \\right)s^{3} + \\left( \\frac{3}{2}\\beta^{3} - \\frac{3}{2}\\alpha^{2}\\beta \\right)s^{2} - \\frac{3}{2}\\alpha\\beta^{2}s - \\frac{1}{2}\\beta^{3}\n$$\nWhen integrating a polynomial over the symmetric interval $[-1, 1]$, the integrals of all terms with odd powers of $s$ are zero. We only need to consider the even-powered terms:\n$$\n\\int_{-1}^{1} g(s) \\, ds = \\int_{-1}^{1} \\left( \\frac{9}{2}\\alpha^{2}\\beta s^{4} + \\left( \\frac{3}{2}\\beta^{3} - \\frac{3}{2}\\alpha^{2}\\beta \\right)s^{2} - \\frac{1}{2}\\beta^{3} \\right) \\, ds\n$$\nUsing the formula $\\int_{-1}^{1} s^{n} \\, ds = \\frac{2}{n+1}$ for even $n$:\n$$\n\\int_{-1}^{1} s^{4} \\, ds = \\frac{2}{5}, \\quad \\int_{-1}^{1} s^{2} \\, ds = \\frac{2}{3}, \\quad \\int_{-1}^{1} 1 \\, ds = 2\n$$\nSubstituting these into the integral for $g(s)$:\n$$\n\\int_{-1}^{1} g(s) \\, ds = \\frac{9}{2}\\alpha^{2}\\beta \\left(\\frac{2}{5}\\right) + \\left( \\frac{3}{2}\\beta^{3} - \\frac{3}{2}\\alpha^{2}\\beta \\right)\\left(\\frac{2}{3}\\right) - \\frac{1}{2}\\beta^{3}(2)\n$$\n$$\n\\int_{-1}^{1} g(s) \\, ds = \\frac{9}{5}\\alpha^{2}\\beta + (\\beta^{3} - \\alpha^{2}\\beta) - \\beta^{3}\n$$\n$$\n\\int_{-1}^{1} g(s) \\, ds = \\left(\\frac{9}{5} - 1\\right)\\alpha^{2}\\beta = \\frac{4}{5}\\alpha^{2}\\beta\n$$\nSo, $Q_{\\mathrm{Gauss}}(g) = \\frac{4}{5}\\alpha^{2}\\beta$.\n\n**2. Calculation of the Gauss-Lobatto quadrature, $Q_{\\mathrm{GLob}}(g)$**\n\nThe $3$-point Gauss-Lobatto quadrature rule on $[-1, 1]$ is given by:\n$$\nQ_{\\mathrm{GLob}}(g) = w_{1}g(s_{1}) + w_{2}g(s_{2}) + w_{3}g(s_{3})\n$$\nwith nodes $s_{1}=-1$, $s_{2}=0$, $s_{3}=1$ and weights $w_{1}=\\frac{1}{3}$, $w_{2}=\\frac{4}{3}$, $w_{3}=\\frac{1}{3}$.\nWe need to evaluate $g(s)$ at these nodes:\n$$\ng(s) = (\\alpha s + \\beta)^{3} \\left( \\frac{1}{2}(3s^{2} - 1) \\right)\n$$\nAt $s = -1$:\n$$\ng(-1) = (\\beta - \\alpha)^{3} \\left( \\frac{1}{2}(3(-1)^{2} - 1) \\right) = (\\beta - \\alpha)^{3} \\left( \\frac{1}{2}(2) \\right) = (\\beta - \\alpha)^{3}\n$$\nAt $s = 0$:\n$$\ng(0) = (\\beta)^{3} \\left( \\frac{1}{2}(3(0)^{2} - 1) \\right) = \\beta^{3} \\left( -\\frac{1}{2} \\right) = -\\frac{1}{2}\\beta^{3}\n$$\nAt $s = 1$:\n$$\ng(1) = (\\alpha + \\beta)^{3} \\left( \\frac{1}{2}(3(1)^{2} - 1) \\right) = (\\alpha + \\beta)^{3} \\left( \\frac{1}{2}(2) \\right) = (\\alpha + \\beta)^{3}\n$$\nNow, we apply the quadrature formula:\n$$\nQ_{\\mathrm{GLob}}(g) = \\frac{1}{3}g(-1) + \\frac{4}{3}g(0) + \\frac{1}{3}g(1)\n$$\n$$\nQ_{\\mathrm{GLob}}(g) = \\frac{1}{3}(\\beta - \\alpha)^{3} + \\frac{4}{3}\\left(-\\frac{1}{2}\\beta^{3}\\right) + \\frac{1}{3}(\\alpha + \\beta)^{3}\n$$\n$$\nQ_{\\mathrm{GLob}}(g) = \\frac{1}{3} \\left( (\\beta - \\alpha)^{3} + (\\beta + \\alpha)^{3} \\right) - \\frac{2}{3}\\beta^{3}\n$$\nWe use the binomial expansion identity $(x-y)^{3} + (x+y)^{3} = 2x^{3} + 6xy^{2}$. With $x=\\beta$ and $y=\\alpha$:\n$$\n(\\beta - \\alpha)^{3} + (\\beta + \\alpha)^{3} = 2\\beta^{3} + 6\\beta\\alpha^{2}\n$$\nSubstituting this back into the expression for $Q_{\\mathrm{GLob}}(g)$:\n$$\nQ_{\\mathrm{GLob}}(g) = \\frac{1}{3}(2\\beta^{3} + 6\\beta\\alpha^{2}) - \\frac{2}{3}\\beta^{3} = \\frac{2}{3}\\beta^{3} + 2\\beta\\alpha^{2} - \\frac{2}{3}\\beta^{3}\n$$\n$$\nQ_{\\mathrm{GLob}}(g) = 2\\alpha^{2}\\beta\n$$\n\n**3. Final Calculation of the Aliasing Error, $E$**\n\nFinally, we compute the difference:\n$$\nE = Q_{\\mathrm{GLob}}(g) - Q_{\\mathrm{Gauss}}(g) = 2\\alpha^{2}\\beta - \\frac{4}{5}\\alpha^{2}\\beta\n$$\n$$\nE = \\left(2 - \\frac{4}{5}\\right)\\alpha^{2}\\beta = \\left(\\frac{10}{5} - \\frac{4}{5}\\right)\\alpha^{2}\\beta = \\frac{6}{5}\\alpha^{2}\\beta\n$$\nThe aliasing error is $\\frac{6}{5}\\alpha^{2}\\beta$.", "answer": "$$\n\\boxed{\\frac{6}{5}\\alpha^{2}\\beta}\n$$", "id": "3381360"}, {"introduction": "The primary motivation for domain decomposition is to enable parallel computation and achieve high performance. This practice problem delves into the computational heart of many domain decomposition methods: the Schur complement operator. You will develop a performance model by estimating the arithmetic intensity of a matrix-free application of this operator, a key metric for predicting performance on modern computer architectures. [@problem_id:3381398] This analysis connects the abstract algorithm to concrete hardware considerations, a vital skill for developing efficient large-scale scientific codes.", "problem": "Consider the scalar Poisson problem discretized by the Symmetric Interior Penalty Discontinuous Galerkin (DG) method on a three-dimensional domain partitioned into conforming hexahedra. On each element, a tensor-product nodal basis on Gauss–Lobatto–Legendre (GLL) points of polynomial degree $p$ is used, with $n = p+1$ nodes per coordinate direction. Let the set of element-local degrees of freedom be split into interior (all three indices strictly between $0$ and $p$) and boundary (indices with at least one of the three equal to $0$ or $p$). We consider an element-level static condensation that eliminates the interior unknowns, producing a Schur complement operator $S$ acting on the element boundary vector. A domain decomposition preconditioner then applies the matrix-free Schur complement action $y_f = S u_f$ per element as\n$$\ny_f \\;=\\; A_{ff} \\, u_f \\;-\\; A_{fi} \\, A_{ii}^{-1} \\, (A_{if} \\, u_f),\n$$\nwhere $A_{ff}$, $A_{fi}$, $A_{if}$, and $A_{ii}$ are the element-local blocks with the obvious meaning, and $u_f$ is the element-local boundary vector.\n\nAssume the following modeling and implementation details for a matrix-free apply that exploits sum-factorization and a tensor-product Fast Diagonalization Method (FDM) for the interior inverse.\n\n- Basis and counts:\n  - Nodes per dimension $n = p+1$ and interior nodes per dimension $n_i = n-2 = p-1$.\n  - Boundary degrees of freedom per element: $N_f = n^3 - n_i^3$ (all element-local boundary nodes).\n- Geometry and coefficients:\n  - The mapping is affine on each element and coefficients are constant, so metric factors are constant per element and their memory traffic is ignored.\n- One-dimensional tensor contractions (sum-factorization) cost model:\n  - Applying an $n \\times n$ dense matrix along a single tensor-product dimension to a $n \\times n \\times n$ tensor costs $2n^4$ floating-point operations (FLOPs).\n  - Applying an $n \\times n$ dense matrix along a single tensor-product dimension to an $n \\times n$ face tensor costs $2n^3$ FLOPs.\n  - Lifting a face field to the element interior along the normal via an $n_i \\times n$ operator on each of the $n^2$ lines normal to that face costs $2n^3 n_i$ FLOPs per face. There are $6$ faces, and this cost applies to both $A_{if}$ and $A_{fi}$.\n- Interior inverse $A_{ii}^{-1}$ by Fast Diagonalization Method:\n  - Three forward transforms along each of the $n_i$-sized dimensions and three backward transforms, each transform costing $2n_i^4$ FLOPs, for a total of $12n_i^4$ FLOPs.\n  - The diagonal solve in the transformed basis costs $n_i^3$ FLOPs.\n- Face block application $A_{ff}$:\n  - Modeled as a two-dimensional operator on each face realized by two one-dimensional tensor contractions, costing $4n^3$ FLOPs per face, hence $24n^3$ FLOPs over all $6$ faces.\n- Vector algebra:\n  - The final accumulation $y_f = A_{ff} u_f - A_{fi} v_i$ costs $N_f$ FLOPs.\n- Memory traffic model:\n  - Only the element-local boundary input $u_f$ is read from main memory and the output $y_f$ is written to main memory; all other arrays are assumed to be held in cache. Using double precision, the total bytes transferred per element are $16N_f$.\n\nUsing only the definitions above and the stated cost model, derive a closed-form expression for the arithmetic intensity $\\mathrm{AI}(p)$, defined as the ratio of total FLOPs per element to total bytes transferred per element, as a function of the polynomial degree $p$. Express your final answer as a single simplified analytic expression in terms of $p$. Do not approximate or round. State the arithmetic intensity in units of FLOPs per byte (do not include units in the boxed answer).", "solution": "We begin by counting degrees of freedom and setting abbreviations. Let $n = p+1$ be the number of nodal points per dimension and $n_i = n - 2 = p - 1$ the interior nodal points per dimension. The number of boundary degrees of freedom per element is\n$$\nN_f = n^3 - n_i^3 = (p+1)^3 - (p-1)^3.\n$$\nExpanding and simplifying,\n$$\nN_f = \\big(p^3 + 3 p^2 + 3 p + 1\\big) - \\big(p^3 - 3 p^2 + 3 p - 1\\big) = 6 p^2 + 2.\n$$\n\nWe next total the floating-point operations (FLOPs) for the sequence\n$$\ny_f = A_{ff} u_f - A_{fi} \\, A_{ii}^{-1} (A_{if} u_f).\n$$\n\n- Cost of $A_{if} u_f$ (face-to-interior lifting): by assumption, per face the lifting costs $2n^3 n_i$ FLOPs. There are $6$ faces, so\n$$\n\\mathrm{FLOPs}(A_{if} u_f) = 6 \\cdot 2n^3 n_i = 12n^3 n_i.\n$$\n\n- Cost of the interior inverse $A_{ii}^{-1}$ via Fast Diagonalization Method: three forward and three backward transforms at $2n_i^4$ FLOPs each, plus a diagonal solve at $n_i^3$ FLOPs,\n$$\n\\mathrm{FLOPs}(A_{ii}^{-1}) = 6 \\cdot 2n_i^4 + n_i^3 = 12n_i^4 + n_i^3.\n$$\n\n- Cost of $A_{fi} v_i$ (interior-to-face projection): symmetric to $A_{if}$,\n$$\n\\mathrm{FLOPs}(A_{fi} v_i) = 12n^3 n_i.\n$$\n\n- Cost of $A_{ff} u_f$ (face block application): two one-dimensional tensor contractions per face at $2n^3$ FLOPs each, across $6$ faces,\n$$\n\\mathrm{FLOPs}(A_{ff} u_f) = 6 \\cdot (2n^3 + 2n^3) = 24n^3.\n$$\n\n- Cost of the final accumulation $y_f = A_{ff} u_f - A_{fi} v_i$:\n$$\n\\mathrm{FLOPs}(\\mathrm{accumulate}) = N_f.\n$$\n\nSumming these contributions, the total FLOPs per element are\n$$\nF(n) = \\underbrace{12n^3 n_i}_{A_{if}} + \\underbrace{(12n_i^4 + n_i^3)}_{A_{ii}^{-1}} + \\underbrace{12n^3 n_i}_{A_{fi}} + \\underbrace{24n^3}_{A_{ff}} + \\underbrace{N_f}_{\\mathrm{accumulate}}.\n$$\nCollecting like terms,\n$$\nF(n) = 24n^3 n_i + 12n_i^4 + n_i^3 + 24n^3 + N_f.\n$$\nWe now rewrite in terms of $n$ using $n_i = n - 2$ and $N_f = n^3 - (n-2)^3$:\n$$\nF(n) = 24n^3 (n-2) + 12 (n-2)^4 + (n-2)^3 + 24n^3 + \\big(n^3 - (n-2)^3\\big).\n$$\nSimplify step by step. First, combine the $n^3$ terms:\n$$\n24n^3 (n-2) + 24n^3 = 24n^4 - 48n^3 + 24n^3 = 24n^4 - 24n^3.\n$$\nThus\n$$\nF(n) = 24n^4 - 24n^3 + 12 (n-2)^4 + (n-2)^3 + n^3 - (n-2)^3.\n$$\nThe $(n-2)^3$ terms cancel partially, leaving\n$$\nF(n) = 24n^4 - 24n^3 + 12 (n-2)^4 + n^3.\n$$\nEquivalently,\n$$\nF(n) = 24n^4 - 23n^3 + 12 (n-2)^4.\n$$\nFor later convenience, we also expand this fully as a polynomial in $n$ (or directly in $p$). Expanding $(n-2)^4 = n^4 - 8 n^3 + 24 n^2 - 32 n + 16$, we find\n$$\nF(n) = 24n^4 - 23n^3 + 12\\big(n^4 - 8 n^3 + 24 n^2 - 32 n + 16\\big),\n$$\nso\n$$\nF(n) = (24+12) n^4 + (-23-96) n^3 + 288 n^2 - 384 n + 192 = 36n^4 - 119n^3 + 288n^2 - 384n + 192.\n$$\n\nThe total bytes transferred per element under the stated model are\n$$\nB = 16 N_f = 16 \\big(n^3 - (n-2)^3\\big).\n$$\nSince $n = p+1$ and $n-2 = p-1$,\n$$\nN_f = (p+1)^3 - (p-1)^3 = 6 p^2 + 2,\n$$\nhence\n$$\nB(p) = 16 (6 p^2 + 2) = 96 p^2 + 32.\n$$\n\nIt remains to express the FLOPs as a polynomial in $p$. Using $n = p+1$, expand\n$$\nF(p) = 36 (p+1)^4 - 119 (p+1)^3 + 288 (p+1)^2 - 384 (p+1) + 192.\n$$\nCompute term by term with $(p+1)^4 = p^4 + 4 p^3 + 6 p^2 + 4 p + 1$, $(p+1)^3 = p^3 + 3 p^2 + 3 p + 1$, and $(p+1)^2 = p^2 + 2 p + 1$:\n\n$$\n\\begin{aligned}\n36 (p+1)^4 &= 36 p^4 + 144 p^3 + 216 p^2 + 144 p + 36,\\\\\n-119 (p+1)^3 &= -119 p^3 - 357 p^2 - 357 p - 119,\\\\\n288 (p+1)^2 &= 288 p^2 + 576 p + 288,\\\\\n-384 (p+1) &= -384 p - 384,\\\\\n+192 &= 192.\n\\end{aligned}\n$$\n\nSumming coefficients yields\n\n$$\nF(p) = 36 p^4 + 25 p^3 + 147 p^2 - 21 p + 13.\n$$\n\n\nTherefore, the arithmetic intensity, defined as FLOPs per byte,\n$$\n\\mathrm{AI}(p) = \\frac{F(p)}{B(p)} = \\frac{36 p^4 + 25 p^3 + 147 p^2 - 21 p + 13}{96 p^2 + 32}.\n$$\nThis is already a simplified rational function in terms of $p$, and no further factorization cancels across numerator and denominator for general integer $p \\geq 1$. The units are FLOPs per byte as requested.", "answer": "$$\\boxed{\\frac{36 p^{4} + 25 p^{3} + 147 p^{2} - 21 p + 13}{96 p^{2} + 32}}$$", "id": "3381398"}]}