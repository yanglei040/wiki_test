{"hands_on_practices": [{"introduction": "The inverse transform method is a cornerstone technique for generating random variates. This exercise provides hands-on practice in translating the mathematical principle of the inverse Cumulative Distribution Function (CDF) into a functional algorithm for the exponential distribution [@problem_id:2403697]. You will not only implement the sampler but also perform a Kolmogorov-Smirnov test to statistically validate that your generated data conforms to the target distribution, reinforcing a crucial link between generation and verification.", "problem": "You are asked to formalize, implement, and validate sampling from the exponential distribution using first principles. Consider the exponential distribution with rate parameter $\\lambda \\in (0,\\infty)$ and cumulative distribution function $F(x) = \\mathbb{P}(X \\le x)$.\n\nTask:\n1. Using the inverse transform principle, generate $n$ independent samples from the exponential distribution with rate $\\lambda$ from independent draws of a variable $U$ that is uniformly distributed on the unit interval. You must rely on the property that if $U$ is uniformly distributed on the unit interval, then $F^{-1}(U)$ has cumulative distribution function $F$.\n2. For each parameter triple $(\\lambda,n,\\alpha)$, with $\\lambda \\in (0,\\infty)$, $n \\in \\mathbb{N}$, and $\\alpha \\in (0,1)$ as a significance level, conduct a one-sample distributional goodness-of-fit test for the exponential distribution with rate $\\lambda$ based on the $n$ samples you generated. Report the $p$-value and a boolean decision that is $\\text{True}$ if you do not reject the null hypothesis that the data follow an exponential distribution with rate $\\lambda$ at significance level $\\alpha$, and $\\text{False}$ otherwise. The $p$-value must be reported as a real number.\n3. For reproducibility, initialize your pseudo-random number generator once at the beginning with the fixed integer seed $s=123456789$.\n\nTest suite:\n- Case $1$: $(\\lambda,n,\\alpha) = (0.7,1000,0.05)$\n- Case $2$: $(\\lambda,n,\\alpha) = (2.3,200,0.01)$\n- Case $3$: $(\\lambda,n,\\alpha) = (0.1,500,0.10)$\n- Case $4$: $(\\lambda,n,\\alpha) = (10.0,100,0.05)$\n- Case $5$: $(\\lambda,n,\\alpha) = (0.5,5,0.20)$\n\nYour program must:\n- Implement the sampler described in item $1$ to generate the $n$ samples for each case.\n- Implement the test described in item $2$ and compute the $p$-value for each case.\n- For each case $i \\in \\{1,2,3,4,5\\}$, output two values in order: the $p$-value rounded to six decimal places and the boolean decision as specified above.\n\nFinal output format:\n- Your program should produce a single line of output containing all results as a comma-separated list enclosed in square brackets. The list must have length $10$ and be ordered as $[p_1,d_1,p_2,d_2,p_3,d_3,p_4,d_4,p_5,d_5]$, where $p_i$ is the $p$-value (rounded to six decimal places) for case $i$ and $d_i$ is the corresponding boolean decision.", "solution": "The problem requires the generation of random samples from an exponential distribution using the inverse transform method, followed by a statistical validation of these samples using a goodness-of-fit test. The solution is developed in two parts: first, the derivation of the sampling algorithm, and second, the specification of the statistical test.\n\n**1. Inverse Transform Sampling for the Exponential Distribution**\n\nThe exponential distribution is characterized by a rate parameter $\\lambda  0$. Its probability density function (PDF) for a random variable $X$ is given by:\n$$\nf(x; \\lambda) = \\begin{cases} \\lambda e^{-\\lambda x}  \\text{if } x \\ge 0 \\\\ 0  \\text{if } x  0 \\end{cases}\n$$\nThe cumulative distribution function (CDF), $F(x)$, represents the probability that the random variable $X$ takes a value less than or equal to $x$. It is obtained by integrating the PDF from $0$ to $x$:\n$$\nF(x) = \\mathbb{P}(X \\le x) = \\int_0^x \\lambda e^{-\\lambda t} dt = \\left[ -e^{-\\lambda t} \\right]_0^x = 1 - e^{-\\lambda x} \\quad \\text{for } x \\ge 0\n$$\nThe inverse transform sampling method is founded on the principle that if $U$ is a random variable drawn from a standard uniform distribution on the interval $(0, 1)$, then the random variable $X = F^{-1}(U)$ will have the distribution characterized by the CDF $F$.\n\nTo apply this method, we must first find the inverse of the CDF, denoted $F^{-1}(u)$. We set $u = F(x)$ and solve for $x$:\n$$\nu = 1 - e^{-\\lambda x}\n$$\nRearranging the terms to isolate $x$:\n$$\ne^{-\\lambda x} = 1 - u\n$$\nTaking the natural logarithm of both sides:\n$$\n-\\lambda x = \\ln(1 - u)\n$$\nFinally, solving for $x$ yields the inverse CDF:\n$$\nx = F^{-1}(u) = -\\frac{1}{\\lambda} \\ln(1 - u)\n$$\nTherefore, to generate a sample $x$ from an exponential distribution with rate $\\lambda$, one generates a random number $u$ from $\\text{Uniform}(0,1)$ and applies the transformation $x = -\\frac{1}{\\lambda} \\ln(1 - u)$.\n\nA useful simplification arises from the property that if $U \\sim \\text{Uniform}(0,1)$, then the random variable $V = 1 - U$ is also distributed as $\\text{Uniform}(0,1)$. This allows us to replace $1 - u$ with $u$ in the formula, leading to the computationally equivalent and more direct expression:\n$$\nx = -\\frac{1}{\\lambda} \\ln(u)\n$$\nThis formula will be implemented to generate the $n$ samples for each test case. The procedure commences by initializing a pseudo-random number generator with the specified seed $s=123456789$ to ensure reproducibility. Then, for each test case $(\\lambda, n, \\alpha)$, we draw $n$ independent values $u_1, u_2, \\dots, u_n$ from $\\text{Uniform}(0,1)$ and compute the corresponding exponential samples $x_i = -\\frac{1}{\\lambda} \\ln(u_i)$.\n\n**2. Goodness-of-Fit Testing**\n\nAfter generating the sample data $\\{x_1, \\dots, x_n\\}$, a one-sample distributional goodness-of-fit test must be performed. The null hypothesis, $H_0$, is that the data are drawn from an exponential distribution with the specified rate parameter $\\lambda$. The alternative hypothesis, $H_1$, is that the data are not from this distribution.\n\nA standard and appropriate test for this scenario is the one-sample Kolmogorov-Smirnov (K-S) test. This test compares the empirical cumulative distribution function (ECDF) of the sample data, $F_n(x)$, with the theoretical CDF of the hypothesized distribution, $F(x) = 1 - e^{-\\lambda x}$. The ECDF is defined as:\n$$\nF_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(x_i \\le x)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The K-S test statistic, $D_n$, is the maximum absolute difference between the ECDF and the theoretical CDF over all possible values of $x$:\n$$\nD_n = \\sup_x | F_n(x) - F(x) |\n$$\nThe distribution of the $D_n$ statistic under the null hypothesis is known and is independent of the specific continuous distribution being tested. This property allows for the calculation of a $p$-value, defined as the probability of observing a test statistic as extreme as, or more extreme than, the one computed from the data, assuming $H_0$ is true.\n\nThe decision rule is based on the significance level $\\alpha$:\n- If the $p$-value is less than $\\alpha$, we reject the null hypothesis $H_0$.\n- If the $p$-value is greater than or equal to $\\alpha$, we do not reject the null hypothesis $H_0$.\n\nThe problem requires a boolean decision that is $\\text{True}$ if we do not reject $H_0$. Thus, the decision for a given test case will be the result of the comparison $p\\text{-value} \\ge \\alpha$. For implementation, we will use the `scipy.stats.kstest` function. The exponential distribution implementation in `scipy` is parameterized by a location `loc` and a `scale` parameter. To match our target PDF $\\lambda e^{-\\lambda x}$, we must set `loc`$=0$ and `scale`$=1/\\lambda$.\n\nThe complete algorithm for each test case $(\\lambda, n, \\alpha)$ is:\n1. Generate $n$ uniform random numbers $u_1, \\dots, u_n$ using a generator seeded with $s=123456789$.\n2. Transform these into exponential samples $x_i = -\\frac{1}{\\lambda} \\ln(u_i)$.\n3. Perform the K-S test on the samples $\\{x_i\\}$ against the 'expon' distribution with parameters `loc=0` and `scale=1/\\lambda`.\n4. Obtain the $p$-value from the test result.\n5. Compute the boolean decision as $p\\text{-value} \\ge \\alpha$.\n6. Report the $p$-value rounded to six decimal places and the boolean decision.\n\nThis procedure will be executed for all five specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the problem by generating exponential samples via inverse transform\n    and validating them with a Kolmogorov-Smirnov goodness-of-fit test.\n\n    The process adheres to the problem's requirements for validation,\n    reproducibility with a fixed seed, and specific output formatting.\n    \"\"\"\n    \n    # Global seed for reproducibility, as per problem statement.\n    # The modern `default_rng` is used for generating pseudo-random numbers.\n    seed = 123456789\n    rng = np.random.default_rng(seed)\n\n    # Test suite: each tuple is (lambda, n, alpha)\n    test_cases = [\n        (0.7, 1000, 0.05),\n        (2.3, 200, 0.01),\n        (0.1, 500, 0.10),\n        (10.0, 100, 0.05),\n        (0.5, 5, 0.20),\n    ]\n\n    results = []\n    for lambda_val, n, alpha in test_cases:\n        # Task 1: Generate n samples from the exponential distribution with rate lambda.\n        # This is implemented using the inverse transform method.\n        # If U is a random variable from Uniform(0, 1), then X = -ln(U)/lambda\n        # is a random variable from Exponential(lambda).\n        \n        # Generate n uniform random numbers in the interval [0.0, 1.0).\n        uniform_samples = rng.uniform(size=n)\n        \n        # Apply the inverse CDF transformation.\n        exponential_samples = -np.log(uniform_samples) / lambda_val\n\n        # Task 2: Conduct a one-sample Kolmogorov-Smirnov test.\n        # The null hypothesis (H0) is that the generated data follows an\n        # exponential distribution with the given rate lambda_val.\n        # The `scipy.stats.expon` distribution is parameterized by `loc` and `scale`.\n        # For a rate lambda, the scale parameter is 1/lambda and loc is 0.\n        scale_param = 1.0 / lambda_val\n        ks_statistic, p_value = stats.kstest(exponential_samples, 'expon', args=(0, scale_param))\n        \n        # The decision is True if we do not reject H0 at significance level alpha.\n        # This occurs when the p-value is greater than or equal to alpha.\n        decision = p_value = alpha\n        \n        # Format results as specified in the problem statement.\n        p_value_rounded = round(p_value, 6)\n        \n        results.append(p_value_rounded)\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) function correctly converts boolean values to \"True\"/\"False\".\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the main function to solve the problem and print the output.\nsolve()\n```", "id": "2403697"}, {"introduction": "Once samples are generated, a primary task in simulation is often to perform statistical inference on the underlying model. This problem shifts the focus from generation to estimation by having you derive the Maximum Likelihood Estimator (MLE) for the exponential rate parameter $\\lambda$ [@problem_id:3307724]. By further analyzing the estimator's variance and comparing it to the Cramér–Rao Lower Bound, this practice deepens your understanding of estimator efficiency and the theoretical limits of statistical precision.", "problem": "A stochastic simulation produces independent and identically distributed (i.i.d.) samples from an exponential distribution with unknown rate parameter $\\lambda0$ using inverse transform sampling applied to uniform pseudorandom numbers on $[0,1]$. Let $X_{1},\\dots,X_{n}$ denote the resulting i.i.d. sample from the exponential distribution with probability density function $f(x;\\lambda)=\\lambda \\exp(-\\lambda x)$ for $x\\geq 0$ and $f(x;\\lambda)=0$ otherwise. Assume $n\\geq 3$.\n\nStarting only from the model specification and the definition of the likelihood function, perform the following:\n\n- Derive the maximum likelihood estimator (MLE) $\\hat{\\lambda}$ of $\\lambda$.\n- Derive the exact finite-sample variance $\\operatorname{Var}(\\hat{\\lambda})$.\n- Compute the Fisher information for $n$ observations and the corresponding Cramér–Rao lower bound (CRLB) for unbiased estimators of $\\lambda$.\n- Assess efficiency by determining whether the MLE attains the CRLB for finite $n$, and compute the asymptotic efficiency defined as $\\lim_{n\\to\\infty}\\frac{\\text{CRLB}}{\\operatorname{Var}(\\hat{\\lambda})}$.\n\nProvide your final result as a single row matrix in the order $\\bigl(\\hat{\\lambda},\\,\\operatorname{Var}(\\hat{\\lambda}),\\,\\text{CRLB},\\,\\lim_{n\\to\\infty}\\frac{\\text{CRLB}}{\\operatorname{Var}(\\hat{\\lambda})}\\bigr)$. Do not include any units. The answer must be given in exact symbolic form with no numerical rounding.", "solution": "The problem is to derive the maximum likelihood estimator (MLE) for the rate parameter $\\lambda$ of an exponential distribution, its variance, the Cramér–Rao lower bound (CRLB), and its asymptotic efficiency. We are given a sample $X_1, \\dots, X_n$ of independent and identically distributed (i.i.d.) random variables from an exponential distribution with probability density function (PDF) $f(x;\\lambda) = \\lambda \\exp(-\\lambda x)$ for $x \\geq 0$. We are given that the sample size is $n \\geq 3$.\n\n**Part 1: Derivation of the Maximum Likelihood Estimator (MLE) $\\hat{\\lambda}$**\n\nThe likelihood function $L(\\lambda)$ for the i.i.d. sample $X_1, \\dots, X_n$ is the product of the individual probability density functions:\n$$L(\\lambda; X_1, \\dots, X_n) = \\prod_{i=1}^{n} f(X_i; \\lambda) = \\prod_{i=1}^{n} \\lambda \\exp(-\\lambda X_i)$$\nAssuming all $X_i \\geq 0$, this simplifies to:\n$$L(\\lambda) = \\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^{n} X_i\\right)$$\nTo find the MLE, we maximize $L(\\lambda)$ with respect to $\\lambda$. It is more convenient to maximize the log-likelihood function, $\\ell(\\lambda) = \\ln L(\\lambda)$, as the logarithm is a monotonically increasing function.\n$$\\ell(\\lambda) = \\ln\\left(\\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^{n} X_i\\right)\\right) = n\\ln\\lambda - \\lambda \\sum_{i=1}^{n} X_i$$\nTo find the maximum, we compute the derivative of $\\ell(\\lambda)$ with respect to $\\lambda$ and set it to zero. This is the score function.\n$$\\frac{d\\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^{n} X_i$$\nSetting the derivative to zero and solving for $\\lambda$ gives the estimator $\\hat{\\lambda}$:\n$$\\frac{n}{\\hat{\\lambda}} - \\sum_{i=1}^{n} X_i = 0 \\implies \\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^{n} X_i}$$\nThis can also be written in terms of the sample mean $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ as $\\hat{\\lambda} = \\frac{1}{\\bar{X}}$. To confirm this is a maximum, we check the second derivative:\n$$\\frac{d^2\\ell}{d\\lambda^2} = -\\frac{n}{\\lambda^2}$$\nSince $\\lambda  0$ and $n \\geq 3$, the second derivative is always negative, which confirms that $\\hat{\\lambda}$ is indeed the maximum likelihood estimator.\nThe first result is $\\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^{n} X_i}$.\n\n**Part 2: Derivation of the Variance $\\operatorname{Var}(\\hat{\\lambda})$**\n\nThe estimator is $\\hat{\\lambda} = \\frac{n}{S_n}$, where $S_n = \\sum_{i=1}^{n} X_i$. Since each $X_i$ is an i.i.d. exponential random variable with rate $\\lambda$, i.e., $X_i \\sim \\text{Exponential}(\\lambda)$, an equivalent description is $X_i \\sim \\text{Gamma}(1, \\lambda)$. The sum of $n$ i.i.d. $\\text{Gamma}(1, \\lambda)$ random variables is a $\\text{Gamma}(n, \\lambda)$ random variable. Thus, $S_n \\sim \\text{Gamma}(n, \\lambda)$.\nThe PDF of $S_n$ is $f_{S_n}(s) = \\frac{\\lambda^n s^{n-1} \\exp(-\\lambda s)}{\\Gamma(n)}$ for $s \\geq 0$.\nTo find $\\operatorname{Var}(\\hat{\\lambda}) = \\operatorname{Var}\\left(\\frac{n}{S_n}\\right)$, we first need the moments of $S_n^{-1}$. The $k$-th moment of an inverse Gamma variable $S_n^{-1}$ is given by $E[S_n^k]$. For $S_n \\sim \\text{Gamma}(\\alpha, \\beta)$ with PDF $f(s) = \\frac{\\beta^\\alpha s^{\\alpha-1} e^{-\\beta s}}{\\Gamma(\\alpha)}$, we have $E[S_n^k] = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)\\beta^k}$, provided $\\alpha+k0$.\nIn our case, $\\alpha=n$ and the rate is $\\beta=\\lambda$. So, for any $k$ such that $n+k0$:\n$$E[S_n^k] = \\frac{\\Gamma(n+k)}{\\Gamma(n)\\lambda^k}$$\nTo calculate the variance, we need $E[\\hat{\\lambda}]$ and $E[\\hat{\\lambda}^2]$.\nFirst, $E[\\hat{\\lambda}] = E\\left[\\frac{n}{S_n}\\right] = n E[S_n^{-1}]$. Using $k=-1$:\n$$E[S_n^{-1}] = \\frac{\\Gamma(n-1)}{\\Gamma(n)\\lambda^{-1}} = \\frac{(n-2)!}{(n-1)!}\\lambda = \\frac{\\lambda}{n-1}$$\nThis is valid for $n-10$, or $n1$. Since we are given $n \\geq 3$, this condition is met.\n$$E[\\hat{\\lambda}] = \\frac{n\\lambda}{n-1}$$\nNext, $E[\\hat{\\lambda}^2] = E\\left[\\left(\\frac{n}{S_n}\\right)^2\\right] = n^2 E[S_n^{-2}]$. Using $k=-2$:\n$$E[S_n^{-2}] = \\frac{\\Gamma(n-2)}{\\Gamma(n)\\lambda^{-2}} = \\frac{(n-3)!}{(n-1)!}\\lambda^2 = \\frac{\\lambda^2}{(n-1)(n-2)}$$\nThis is valid for $n-20$, or $n2$. Since we are given $n \\geq 3$, this condition is also met.\n$$E[\\hat{\\lambda}^2] = \\frac{n^2\\lambda^2}{(n-1)(n-2)}$$\nThe variance is $\\operatorname{Var}(\\hat{\\lambda}) = E[\\hat{\\lambda}^2] - (E[\\hat{\\lambda}])^2$.\n$$\\operatorname{Var}(\\hat{\\lambda}) = \\frac{n^2\\lambda^2}{(n-1)(n-2)} - \\left(\\frac{n\\lambda}{n-1}\\right)^2 = \\frac{n^2\\lambda^2}{(n-1)(n-2)} - \\frac{n^2\\lambda^2}{(n-1)^2}$$\n$$= n^2\\lambda^2 \\left( \\frac{n-1 - (n-2)}{(n-1)^2(n-2)} \\right) = n^2\\lambda^2 \\left( \\frac{1}{(n-1)^2(n-2)} \\right)$$\nThus, the second result is $\\operatorname{Var}(\\hat{\\lambda}) = \\frac{n^2\\lambda^2}{(n-1)^2(n-2)}$.\n\n**Part 3: Fisher Information and Cramér–Rao Lower Bound (CRLB)**\n\nThe Fisher information for a single observation, $I(\\lambda)$, is given by $I(\\lambda) = -E\\left[\\frac{d^2}{d\\lambda^2} \\ln f(X;\\lambda)\\right]$.\nThe log-PDF for one observation is $\\ln f(x;\\lambda) = \\ln\\lambda - \\lambda x$. The derivatives are:\n$$\\frac{d}{d\\lambda} \\ln f(x;\\lambda) = \\frac{1}{\\lambda} - x$$\n$$\\frac{d^2}{d\\lambda^2} \\ln f(x;\\lambda) = -\\frac{1}{\\lambda^2}$$\nSince the second derivative is a constant with respect to $x$, its expectation is the constant itself:\n$$I(\\lambda) = -E\\left[-\\frac{1}{\\lambda^2}\\right] = \\frac{1}{\\lambda^2}$$\nFor $n$ i.i.d. observations, the total Fisher information is additive: $I_n(\\lambda) = n I(\\lambda) = \\frac{n}{\\lambda^2}$.\nThe Cramér–Rao Lower Bound (CRLB) provides a lower bound on the variance of any unbiased estimator of $\\lambda$. The bound is given by the reciprocal of the Fisher information:\n$$\\text{CRLB} = \\frac{1}{I_n(\\lambda)} = \\frac{1}{n/\\lambda^2} = \\frac{\\lambda^2}{n}$$\nThe third result is $\\text{CRLB} = \\frac{\\lambda^2}{n}$.\n\n**Part 4: Efficiency Assessment**\n\nFirst, we assess if the MLE attains the CRLB for finite $n$. The CRLB applies to unbiased estimators. We found that $E[\\hat{\\lambda}] = \\frac{n\\lambda}{n-1}$, which is not equal to $\\lambda$ for any finite $n$. Since $\\hat{\\lambda}$ is a biased estimator, it cannot attain the CRLB for unbiased estimators.\n\nNext, we compute the asymptotic efficiency, which is defined as the limit of the ratio of the CRLB to the variance of the estimator as $n \\to \\infty$.\n$$\\text{Asymptotic Efficiency} = \\lim_{n\\to\\infty} \\frac{\\text{CRLB}}{\\operatorname{Var}(\\hat{\\lambda})}$$\nSubstituting the expressions for CRLB and $\\operatorname{Var}(\\hat{\\lambda})$:\n$$\\lim_{n\\to\\infty} \\frac{\\frac{\\lambda^2}{n}}{\\frac{n^2\\lambda^2}{(n-1)^2(n-2)}} = \\lim_{n\\to\\infty} \\frac{\\lambda^2}{n} \\cdot \\frac{(n-1)^2(n-2)}{n^2\\lambda^2} = \\lim_{n\\to\\infty} \\frac{(n-1)^2(n-2)}{n^3}$$\nTo evaluate the limit, we look at the ratio of the leading terms of the polynomials in $n$:\n$$\\lim_{n\\to\\infty} \\frac{(n^2 - 2n + 1)(n-2)}{n^3} = \\lim_{n\\to\\infty} \\frac{n^3 - 4n^2 + 5n - 2}{n^3}$$\n$$= \\lim_{n\\to\\infty} \\left(1 - \\frac{4}{n} + \\frac{5}{n^2} - \\frac{2}{n^3}\\right) = 1$$\nThe asymptotic efficiency is $1$, meaning the MLE $\\hat{\\lambda}$ is asymptotically efficient.\nThe fourth result is $1$.\n\n**Final Result Collation**\nThe four results, in the specified order, are:\n$1$. $\\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^n X_i}$\n$2$. $\\operatorname{Var}(\\hat{\\lambda}) = \\frac{n^2 \\lambda^2}{(n-1)^2(n-2)}$\n$3$. $\\text{CRLB} = \\frac{\\lambda^2}{n}$\n$4$. $\\lim_{n\\to\\infty}\\frac{\\text{CRLB}}{\\operatorname{Var}(\\hat{\\lambda})} = 1$\nThese are to be presented as a single row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{n}{\\sum_{i=1}^{n} X_i}  \\frac{n^2 \\lambda^2}{(n-1)^2(n-2)}  \\frac{\\lambda^2}{n}  1\n\\end{pmatrix}\n}\n$$", "id": "3307724"}, {"introduction": "Practical simulations often involve constraints or modifications, such as truncating the sample space to avoid rare, extreme events. This exercise explores the analytical consequences of such a decision by examining an exponential distribution truncated at a maximum value $b$ [@problem_id:3307796]. By deriving the exact conditional mean and variance, you will quantify the bias introduced by this common practice, providing crucial insight into how simulation design choices can systematically alter statistical outcomes.", "problem": "A renewal model of event arrivals is simulated by generating independent waiting times with an exponential distribution. Specifically, let $X$ denote a single waiting time with density $f_{X}(x) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$ and rate parameter $\\lambda  0$. In a Monte Carlo (MC) code path designed to avoid rare extremely long simulations, any draw exceeding a fixed cap $b  0$ is discarded from analysis, and only draws satisfying $X \\le b$ are retained for post hoc estimation of moments.\n\nStarting from the core definitions of the exponential distribution and the definition of conditional expectation and conditional variance, derive closed-form expressions for $E[X \\mid X \\le b]$ and $\\operatorname{Var}(X \\mid X \\le b)$ in terms of $\\lambda$ and $b$. Then, interpret how truncation at $b$ affects the expectation relative to the untruncated mean $E[X]$, discussing the direction and magnitude of the induced bias as a function of $b$.\n\nExpress your final results as analytic expressions in terms of $\\lambda$ and $b$. No numerical approximation or rounding is required.", "solution": "We begin from the fundamental definition of the exponential distribution with rate $\\lambda  0$, whose probability density function is $f_{X}(x) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$, and cumulative distribution function $F_{X}(x) = 1 - \\exp(-\\lambda x)$ for $x \\ge 0$. The conditioning event is $X \\le b$ for some fixed $b  0$. The conditional density on $[0,b]$ is given by the definition of conditional probability density,\n$$\nf_{X \\mid X \\le b}(x) \\;=\\; \\frac{f_{X}(x)}{P(X \\le b)} \\;=\\; \\frac{\\lambda \\exp(-\\lambda x)}{1 - \\exp(-\\lambda b)}, \\quad 0 \\le x \\le b,\n$$\nand $f_{X \\mid X \\le b}(x) = 0$ otherwise. The normalizing constant is $P(X \\le b) = F_{X}(b) = 1 - \\exp(-\\lambda b)$.\n\nThe conditional expectation can be computed as\n$$\nE[X \\mid X \\le b] \\;=\\; \\int_{0}^{b} x \\, f_{X \\mid X \\le b}(x) \\, dx \\;=\\; \\frac{1}{1 - \\exp(-\\lambda b)} \\int_{0}^{b} x \\lambda \\exp(-\\lambda x) \\, dx.\n$$\nTo evaluate the integral, use integration by parts with $u = x$ and $dv = \\lambda \\exp(-\\lambda x) \\, dx$, so $du = dx$ and $v = -\\exp(-\\lambda x)$. Then\n$$\n\\int_{0}^{b} x \\lambda \\exp(-\\lambda x) \\, dx\n= \\left[ -x \\exp(-\\lambda x) \\right]_{0}^{b} + \\int_{0}^{b} \\exp(-\\lambda x) \\, dx\n= - b \\exp(-\\lambda b) + \\frac{1 - \\exp(-\\lambda b)}{\\lambda}.\n$$\nTherefore,\n$$\nE[X \\mid X \\le b]\n= \\frac{- b \\exp(-\\lambda b) + \\frac{1 - \\exp(-\\lambda b)}{\\lambda}}{1 - \\exp(-\\lambda b)}\n= \\frac{1}{\\lambda} - \\frac{b \\exp(-\\lambda b)}{1 - \\exp(-\\lambda b)}\n= \\frac{1}{\\lambda} - \\frac{b}{\\exp(\\lambda b) - 1}.\n$$\n\nNext, compute the conditional second moment:\n$$\nE[X^{2} \\mid X \\le b]\n= \\int_{0}^{b} x^{2} f_{X \\mid X \\le b}(x) \\, dx\n= \\frac{1}{1 - \\exp(-\\lambda b)} \\int_{0}^{b} x^{2} \\lambda \\exp(-\\lambda x) \\, dx.\n$$\nEvaluate the integral by integration by parts: with $u = x^{2}$ and $dv = \\lambda \\exp(-\\lambda x) \\, dx$, we have $du = 2x \\, dx$ and $v = -\\exp(-\\lambda x)$, yielding\n$$\n\\int_{0}^{b} x^{2} \\lambda \\exp(-\\lambda x) \\, dx\n= \\left[ - x^{2} \\exp(-\\lambda x) \\right]_{0}^{b} + \\int_{0}^{b} 2x \\exp(-\\lambda x) \\, dx.\n$$\nA second integration by parts for $\\int_{0}^{b} x \\exp(-\\lambda x) \\, dx$ with $u = x$, $dv = \\exp(-\\lambda x) \\, dx$ gives $v = -\\frac{1}{\\lambda} \\exp(-\\lambda x)$, and hence\n$$\n\\int_{0}^{b} x \\exp(-\\lambda x) \\, dx\n= \\left[ - \\frac{x}{\\lambda} \\exp(-\\lambda x) \\right]_{0}^{b}\n+ \\int_{0}^{b} \\frac{1}{\\lambda} \\exp(-\\lambda x) \\, dx\n= - \\frac{b}{\\lambda} \\exp(-\\lambda b) + \\frac{1 - \\exp(-\\lambda b)}{\\lambda^{2}}.\n$$\nSubstituting back,\n$$\n\\int_{0}^{b} x^{2} \\lambda \\exp(-\\lambda x) \\, dx\n= - b^{2} \\exp(-\\lambda b) + 2 \\left( - \\frac{b}{\\lambda} \\exp(-\\lambda b) + \\frac{1 - \\exp(-\\lambda b)}{\\lambda^{2}} \\right).\n$$\nThat is,\n$$\n\\int_{0}^{b} x^{2} \\lambda \\exp(-\\lambda x) \\, dx\n= \\frac{2 (1 - \\exp(-\\lambda b))}{\\lambda^{2}} - \\exp(-\\lambda b) \\left( b^{2} + \\frac{2 b}{\\lambda} \\right).\n$$\nTherefore,\n$$\nE[X^{2} \\mid X \\le b]\n= \\frac{ \\frac{2 (1 - \\exp(-\\lambda b))}{\\lambda^{2}} - \\exp(-\\lambda b) \\left( b^{2} + \\frac{2 b}{\\lambda} \\right) }{1 - \\exp(-\\lambda b)}\n= \\frac{2}{\\lambda^{2}} - \\frac{ b^{2} + \\frac{2 b}{\\lambda} }{ \\exp(\\lambda b) - 1 }.\n$$\n\nThe conditional variance is\n$$\n\\operatorname{Var}(X \\mid X \\le b)\n= E[X^{2} \\mid X \\le b] - \\left( E[X \\mid X \\le b] \\right)^{2}\n= \\left( \\frac{2}{\\lambda^{2}} - \\frac{ b^{2} + \\frac{2 b}{\\lambda} }{ \\exp(\\lambda b) - 1 } \\right)\n- \\left( \\frac{1}{\\lambda} - \\frac{b}{\\exp(\\lambda b) - 1} \\right)^{2}.\n$$\nWith $t = \\lambda b$ and $A = \\exp(t) - 1$, we may simplify algebraically:\n$$\nE[X \\mid X \\le b] = \\frac{1}{\\lambda} - \\frac{b}{A}, \\quad\nE[X^{2} \\mid X \\le b] = \\frac{2}{\\lambda^{2}} - \\frac{ b^{2} + \\frac{2 b}{\\lambda} }{ A }.\n$$\nThen\n$$\n\\operatorname{Var}(X \\mid X \\le b)\n= \\frac{2}{\\lambda^{2}} - \\frac{ b^{2} + \\frac{2 b}{\\lambda} }{ A }\n- \\left( \\frac{1}{\\lambda^{2}} - \\frac{2 b}{\\lambda A} + \\frac{b^{2}}{A^{2}} \\right)\n= \\frac{1}{\\lambda^{2}} - \\frac{b^{2}}{A} - \\frac{b^{2}}{A^{2}}\n= \\frac{1}{\\lambda^{2}} - \\frac{b^{2} \\exp(\\lambda b)}{ \\left( \\exp(\\lambda b) - 1 \\right)^{2} }.\n$$\n\nInterpretation of truncation on bias: the untruncated exponential mean is $E[X] = \\frac{1}{\\lambda}$. The truncated mean derived above is\n$$\nE[X \\mid X \\le b] = \\frac{1}{\\lambda} - \\frac{b}{\\exp(\\lambda b) - 1}.\n$$\nHence the bias induced by truncation at $b$ relative to the original mean is\n$$\n\\text{Bias}(b) = E[X \\mid X \\le b] - E[X] = - \\frac{b}{\\exp(\\lambda b) - 1},\n$$\nwhich is strictly negative for all $b  0$ and monotonically increasing to $0$ as $b \\to \\infty$. For small $b$, a Taylor expansion shows $E[X \\mid X \\le b] = \\frac{b}{2} + O(b^{2})$, and $\\operatorname{Var}(X \\mid X \\le b) = \\frac{b^{2}}{12} + O(b^{3})$, reflecting the near-uniform behavior on $[0,b]$. As $b \\to \\infty$, both $E[X \\mid X \\le b] \\to \\frac{1}{\\lambda}$ and $\\operatorname{Var}(X \\mid X \\le b] \\to \\frac{1}{\\lambda^{2}}$, recovering the untruncated exponential moments. Thus truncation reduces both the mean and the variance relative to the untruncated distribution, introducing a downward bias in the estimated mean whose magnitude decays with $b$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\lambda}-\\frac{b}{\\exp(\\lambda b)-1}  \\frac{1}{\\lambda^{2}}-\\frac{b^{2}\\exp(\\lambda b)}{\\left(\\exp(\\lambda b)-1\\right)^{2}}\\end{pmatrix}}$$", "id": "3307796"}]}