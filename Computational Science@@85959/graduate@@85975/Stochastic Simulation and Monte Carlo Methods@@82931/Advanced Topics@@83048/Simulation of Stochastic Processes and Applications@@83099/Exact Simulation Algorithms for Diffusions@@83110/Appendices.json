{"hands_on_practices": [{"introduction": "This first practice establishes the motivation for studying exact simulation algorithms. By comparing the computational work of a standard Euler-Maruyama scheme against an idealized exact algorithm, you will analytically determine the regime where the higher upfront complexity of exact simulation pays off in superior efficiency for achieving high-accuracy estimates [@problem_id:3306858]. This exercise sharpens your understanding of how numerical error (bias) and statistical error (variance) combine to determine the overall cost of a Monte Carlo computation.", "problem": "Consider the one-dimensional Itô diffusion defined on the time interval $[0,T]$ by the stochastic differential equation $dX_{t}=\\mu(X_{t})\\,dt+\\sigma(X_{t})\\,dW_{t}$ with deterministic initial condition $X_{0}=x_{0}$, where $W_{t}$ is standard Brownian motion. Let $\\varphi:\\mathbb{R}\\to\\mathbb{R}$ be a bounded, twice continuously differentiable function with bounded derivatives. The computational task is to estimate $I=\\mathbb{E}[\\varphi(X_{T})]$ using two approaches:\n- Approach A: the Euler–Maruyama time discretization with uniform step size $h$ combined with standard Monte Carlo (MC).\n- Approach B: an exact simulation algorithm for diffusions (for example, an acceptance–rejection exact algorithm) combined with standard MC.\n\nAssume the following properties hold uniformly over sufficiently small $h$:\n- The weak (distributional) error of Euler–Maruyama is first order: there exists a constant $b0$ such that $\\big|\\mathbb{E}[\\varphi(X_{T}^{(h)})]-\\mathbb{E}[\\varphi(X_{T})]\\big|\\leq b\\,h$, where $X_{T}^{(h)}$ denotes the Euler–Maruyama approximation at time $T$.\n- The variance of the observable under Euler–Maruyama is bounded uniformly: there exists a constant $v0$ such that $\\operatorname{Var}(\\varphi(X_{T}^{(h)}))\\leq v$ for all sufficiently small $h$.\n- The computational cost per Euler–Maruyama path scales linearly with the number of steps: there exists a constant $c_{E}0$ such that the expected cost per path is $c_{E}\\,(T/h)$.\n- The exact simulation algorithm produces unbiased samples from the law of $X_{T}$ with finite expected cost per sample: there exist constants $c_{X}0$ and $v_{X}0$ such that the expected cost per exact path is $c_{X}$ and $\\operatorname{Var}(\\varphi(X_{T}))\\leq v_{X}$.\n\nFor each approach, let $\\widehat{I}$ be the standard Monte Carlo estimator based on $N$ independent paths. The mean-square error (MSE) decomposes as $\\mathbb{E}\\big[(\\widehat{I}-I)^{2}\\big]=\\text{bias}^{2}+\\operatorname{Var}(\\widehat{I})$, with $\\operatorname{Var}(\\widehat{I})$ scaling as $1/N$.\n\nDefine the total expected computational work as the product of the number of samples and the expected cost per sample, and suppose you choose $h$ and $N$ to meet a target mean-square error $\\epsilon^{2}$ at minimal work for Approach A, and $N$ to meet the same target at minimal work for Approach B.\n\nDerive the asymptotically optimal work for both approaches as functions of $\\epsilon$, and then determine a closed-form expression for the crossover tolerance $\\epsilon_{\\star}$, defined as the positive value of $\\epsilon$ for which the minimal expected work of Approach A equals that of Approach B. Express your final answer as a single analytical expression in terms of $b$, $v$, $v_{X}$, $c_{E}$, $c_{X}$, and $T$.", "solution": "The user has requested a comparison of the computational work required by two different numerical methods for estimating the expectation $I = \\mathbb{E}[\\varphi(X_{T})]$, where $X_t$ is a one-dimensional Itô diffusion. We are asked to find the crossover tolerance $\\epsilon_{\\star}$, where the minimal work for both methods is equal for a target mean-square error $\\epsilon^2 = \\epsilon_{\\star}^2$.\n\nFirst, we analyze each approach separately to determine the minimal computational work required to achieve a mean-square error (MSE) of $\\epsilon^2$. The MSE is defined as $\\mathbb{E}[(\\widehat{I}-I)^2] = (\\text{bias})^2 + \\operatorname{Var}(\\widehat{I})$.\n\n**Approach A: Euler-Maruyama with Monte Carlo**\n\nFor this approach, the estimator $\\widehat{I}_A$ is based on $N_A$ paths of an Euler-Maruyama discretization with step size $h$.\n\nThe squared bias is bounded by $(b h)^2$, where $b  0$ is a constant related to the weak error of the scheme. We take this as an equality for the purpose of optimization: $(\\text{bias}_A)^2 = b^2 h^2$.\n\nThe variance of the Monte Carlo estimator is $\\operatorname{Var}(\\widehat{I}_A) = \\frac{\\operatorname{Var}(\\varphi(X_T^{(h)}))}{N_A}$. The problem provides a uniform bound $\\operatorname{Var}(\\varphi(X_T^{(h)})) \\le v$ for some constant $v  0$. We use the upper bound for our analysis, so $\\operatorname{Var}(\\widehat{I}_A) = \\frac{v}{N_A}$.\n\nThe total MSE is the sum of these two error components:\n$$\n\\text{MSE}_A = b^2 h^2 + \\frac{v}{N_A}\n$$\nWe impose the constraint that the MSE must equal the target tolerance $\\epsilon^2$:\n$$\nb^2 h^2 + \\frac{v}{N_A} = \\epsilon^2\n$$\nThe total expected computational work, $W_A$, is the product of the number of paths and the cost per path:\n$$\nW_A = N_A \\times \\left( c_{E} \\frac{T}{h} \\right)\n$$\nwhere $c_{E}  0$ is the cost constant for the Euler-Maruyama method.\n\nOur goal is to minimize $W_A$ subject to the MSE constraint. We can express $N_A$ from the constraint as $N_A = \\frac{v}{\\epsilon^2 - b^2 h^2}$ (this is valid for $h  \\epsilon/b$). Substituting this into the work expression gives:\n$$\nW_A(h) = \\frac{v c_{E} T}{h(\\epsilon^2 - b^2 h^2)}\n$$\nTo find the minimum, we can find the maximum of the denominator $f(h) = h\\epsilon^2 - b^2 h^3$. We compute the derivative with respect to $h$ and set it to $0$:\n$$\nf'(h) = \\epsilon^2 - 3 b^2 h^2 = 0\n$$\nSolving for $h^2$ yields the optimal choice $h^{*2} = \\frac{\\epsilon^2}{3b^2}$, which gives the optimal step size $h^* = \\frac{\\epsilon}{\\sqrt{3} b}$.\n\nThis result implies that for optimal performance, the error should be distributed such that the squared bias term is one-third of the total MSE:\n$$\nb^2 (h^*)^2 = b^2 \\left( \\frac{\\epsilon^2}{3b^2} \\right) = \\frac{\\epsilon^2}{3}\n$$\nThe remaining two-thirds of the MSE comes from the variance term:\n$$\n\\frac{v}{N_A^*} = \\epsilon^2 - \\frac{\\epsilon^2}{3} = \\frac{2\\epsilon^2}{3}\n$$\nSolving for the optimal number of paths $N_A^*$ gives:\n$$\nN_A^* = \\frac{3v}{2\\epsilon^2}\n$$\nNow we can compute the minimal work $W_A^*$ by substituting the optimal values $h^*$ and $N_A^*$ into the work equation:\n$$\nW_A^*(\\epsilon) = N_A^* \\left( \\frac{c_{E} T}{h^*} \\right) = \\left( \\frac{3v}{2\\epsilon^2} \\right) \\left( \\frac{c_{E} T}{\\frac{\\epsilon}{\\sqrt{3} b}} \\right) = \\frac{3\\sqrt{3}}{2} \\frac{b v c_{E} T}{\\epsilon^3}\n$$\nThe minimal work for Approach A scales as $\\epsilon^{-3}$.\n\n**Approach B: Exact Algorithm with Monte Carlo**\n\nFor this approach, the estimator $\\widehat{I}_B$ is based on $N_B$ paths from an exact simulation algorithm.\n\nThe algorithm produces unbiased samples, so the bias is $0$.\n$$\n\\text{bias}_B = 0\n$$\nThe MSE is therefore purely due to the variance of the Monte Carlo estimator:\n$$\n\\text{MSE}_B = \\operatorname{Var}(\\widehat{I}_B) = \\frac{\\operatorname{Var}(\\varphi(X_T))}{N_B}\n$$\nUsing the provided bound $\\operatorname{Var}(\\varphi(X_T)) \\le v_{X}$, we have $\\text{MSE}_B = \\frac{v_{X}}{N_B}$.\nWe set this equal to the target tolerance $\\epsilon^2$:\n$$\n\\frac{v_{X}}{N_B} = \\epsilon^2\n$$\nThis directly determines the required number of paths:\n$$\nN_B = \\frac{v_{X}}{\\epsilon^2}\n$$\nThe computational work $W_B$ is the product of the number of paths and the constant expected cost per path, $c_{X}$:\n$$\nW_B = N_B \\times c_{X}\n$$\nSubstituting the expression for $N_B$, we find the minimal work required for Approach B:\n$$\nW_B^*(\\epsilon) = \\left( \\frac{v_{X}}{\\epsilon^2} \\right) c_{X} = \\frac{c_{X} v_{X}}{\\epsilon^2}\n$$\nThe minimal work for Approach B scales as $\\epsilon^{-2}$.\n\n**Crossover Tolerance $\\epsilon_{\\star}$**\n\nThe crossover tolerance $\\epsilon_{\\star}$ is the positive value of $\\epsilon$ for which the minimal work of both approaches is equal: $W_A^*(\\epsilon_{\\star}) = W_B^*(\\epsilon_{\\star})$.\n$$\n\\frac{3\\sqrt{3}}{2} \\frac{b v c_{E} T}{\\epsilon_{\\star}^3} = \\frac{c_{X} v_{X}}{\\epsilon_{\\star}^2}\n$$\nSince $\\epsilon_{\\star}  0$, we can multiply both sides by $\\epsilon_{\\star}^3$ to get:\n$$\n\\frac{3\\sqrt{3}}{2} b v c_{E} T = c_{X} v_{X} \\epsilon_{\\star}\n$$\nFinally, we solve for $\\epsilon_{\\star}$:\n$$\n\\epsilon_{\\star} = \\frac{3\\sqrt{3}}{2} \\frac{b v c_{E} T}{c_{X} v_{X}}\n$$\nThis expression gives the tolerance level at which the computational cost of the optimized Euler-Maruyama method equals that of the exact algorithm. For a target error $\\epsilon  \\epsilon_{\\star}$, the exact algorithm (with work scaling as $\\epsilon^{-2}$) becomes more efficient than the Euler-Maruyama method (with work scaling as $\\epsilon^{-3}$). Conversely, for $\\epsilon  \\epsilon_{\\star}$, the Euler-Maruyama method is more efficient.", "answer": "$$\n\\boxed{\\frac{3\\sqrt{3}}{2}\\frac{b v c_{E} T}{c_{X} v_{X}}}\n$$", "id": "3306858"}, {"introduction": "Exact simulation often relies on a change-of-measure from a simple proposal process, like a Brownian bridge, to the target diffusion path. This exercise [@problem_id:3306946] delves into the heart of this method, asking you to derive the key \"potential\" function that governs the acceptance probability and analyze its global bounds for specific diffusions. By connecting the properties of the diffusion's drift to these bounds, you will gain practical insight into what makes a diffusion \"easy\" or \"hard\" to simulate exactly.", "problem": "Consider the one-dimensional Itô diffusion defined by the stochastic differential equation $dX_t = b(X_t)\\,dt + dW_t$ on the real line, where $W_t$ is a standard Wiener process and the drift $b(x)$ is the negative gradient of a convex potential $U(x)$, that is $b(x) = -U'(x)$. Assume that $U$ is twice continuously differentiable, $U''(x) \\ge 0$ for all $x$, and that both $U'$ and $U''$ are globally bounded, meaning there exist finite constants $G \\ge 0$ and $L \\ge 0$ such that $\\sup_{x \\in \\mathbb{R}} |U'(x)| \\le G$ and $\\sup_{x \\in \\mathbb{R}} U''(x) \\le L$. Let $Y_t$ denote a Brownian bridge on the time interval $[0,T]$ from $x_0$ to $x_T$ used as a reference path in a change-of-measure exact algorithm of the Beskos–Roberts type for simulating diffusion bridges. In this framework, the Radon–Nikodym derivative between the diffusion path law and the Brownian bridge path law over $[0,T]$ involves the Feynman–Kac functional with integrand \n$$\nV(y) = \\frac{1}{2}\\left(b(y)^2 + b'(y)\\right) = \\frac{1}{2}\\left((U'(y))^2 - U''(y)\\right).\n$$\nThe standard exact algorithm based on rejection sampling can be formulated via an inhomogeneous Poisson thinning construction, where conditional on a realization of $(Y_t)_{t \\in [0,T]}$ and given a global constant $M \\ge \\sup_{y \\in \\mathbb{R}} V(y)$, acceptance is equivalent to the event that a Poisson process on $[0,T]$ with instantaneous rate $\\lambda(t) = M - V(Y_t)$ has zero points, whose conditional probability equals $\\exp\\left(-\\int_0^T (M - V(Y_t))\\,dt\\right)$. A single-shot acceptance test can be realized using an exponential clock $E \\sim \\mathrm{Exp}(1)$ by accepting if and only if $E \\ge \\int_0^T (M - V(Y_t))\\,dt$, which is equivalent in distribution to the zero-event criterion. In general, the path integral $\\int_0^T V(Y_t)\\,dt$ is not directly computable; however, under the stated global boundedness assumptions and using the convexity of $U$, one can bound $V$ globally: \n$$\nm \\le V(y) \\le M \\quad \\text{for all } y \\in \\mathbb{R},\n$$\nwhere $m = \\inf_{y \\in \\mathbb{R}} V(y)$ and $M = \\sup_{y \\in \\mathbb{R}} V(y)$. This yields a deterministic bracket on the integrated hazard:\n$$\n0 \\le \\int_0^T (M - V(Y_t))\\,dt \\le T\\,(M - m).\n$$\nTherefore, a single-shot exponential-clock test that never errs but may be conservative is: generate $E \\sim \\mathrm{Exp}(1)$ and accept immediately if $E \\ge T\\,(M - m)$; otherwise, defer (in a complete exact algorithm, one would refine local bounds or evaluate the path selectively; here, we focus only on this global single-shot). The probability that this single-shot test accepts without any further computation is at least \n$$\np_{\\mathrm{lb}}(T) = \\mathbb{P}\\big(E \\ge T\\,(M - m)\\big) = \\exp\\big(-T\\,(M - m)\\big).\n$$\nYour tasks are:\n- Starting from the Itô change-of-measure identity and the definition of $V(y)$ above, derive from first principles the global bounds $m$ and $M$ in terms of the convex potential $U$ and its derivatives, using only the assumptions provided. Explicitly specialize your derivation to the following two convex potentials that satisfy the boundedness conditions:\n    1. $U_{\\mathrm{ch}}(x) = \\log\\cosh(\\alpha x)$ with parameter $\\alpha  0$.\n    2. $U_{\\mathrm{sq}}(x) = \\sqrt{1 + (\\alpha x)^2}$ with parameter $\\alpha  0$.\n  For each potential, derive exact closed-form values for $m$ and $M$ by analyzing $U'$ and $U''$, and hence obtain the exact value of $M - m$ as a function of $\\alpha$.\n- Using the single-shot exponential test described above, define the efficiency proxy of this first-stage test as $p_{\\mathrm{lb}}(T) = \\exp(-T\\,(M - m))$. For a given efficiency threshold $\\tau \\in (0,1)$, say that the single-shot test is efficient if $p_{\\mathrm{lb}}(T) \\ge \\tau$.\n- Implement a program that, for each test case $(\\text{potential\\_type}, \\alpha, T, \\tau)$ in the test suite below, computes:\n    - The exact value of $M - m$ implied by your derivation.\n    - The efficiency proxy $p_{\\mathrm{lb}}(T) = \\exp(-T\\,(M - m))$.\n    - An efficiency indicator, defined as $1$ if $p_{\\mathrm{lb}}(T) \\ge \\tau$ and $0$ otherwise.\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a three-element list $[M\\_minus\\_m, p\\_lb, efficient]$ for one test case, in that order. All floating-point outputs should be in standard decimal form. There is no physical unit involved in this problem.\n\nTest suite:\n- Case $1$: $(\\text{potential\\_type} = \\text{\"ch\"}, \\alpha = 0.8, T = 1.0, \\tau = 0.6)$.\n- Case $2$: $(\\text{potential\\_type} = \\text{\"ch\"}, \\alpha = 1.5, T = 1.0, \\tau = 0.2)$.\n- Case $3$: $(\\text{potential\\_type} = \\text{\"sq\"}, \\alpha = 1.0, T = 0.5, \\tau = 0.6)$.\n- Case $4$: $(\\text{potential\\_type} = \\text{\"ch\"}, \\alpha = 0.5, T = 0.0, \\tau = 0.9)$.\n- Case $5$: $(\\text{potential\\_type} = \\text{\"sq\"}, \\alpha = 0.3, T = 3.0, \\tau = 0.7)$.\n\nFinal output format:\nYour program should print a single line exactly of the form\n$[\\,[M\\_minus\\_m^{(1)},p\\_lb^{(1)},\\text{efficient}^{(1)}],\\ldots,[M\\_minus\\_m^{(5)},p\\_lb^{(5)},\\text{efficient}^{(5)}]\\,]$\nwith no spaces after commas.", "solution": "The problem is first validated to ensure it is scientifically grounded, self-contained, and well-posed.\n\n### Step 1: Extract Givens\n- **Stochastic Differential Equation (SDE):** $dX_t = b(X_t)\\,dt + dW_t$.\n- **Drift:** $b(x) = -U'(x)$, where $U(x)$ is a convex potential.\n- **Potential Properties:**\n    - $U$ is twice continuously differentiable ($C^2$).\n    - $U''(x) \\ge 0$ for all $x \\in \\mathbb{R}$.\n    - $\\sup_{x \\in \\mathbb{R}} |U'(x)| \\le G  \\infty$.\n    - $\\sup_{x \\in \\mathbb{R}} U''(x) \\le L  \\infty$.\n- **Feynman–Kac Integrand:** $V(y) = \\frac{1}{2}\\left(b(y)^2 + b'(y)\\right) = \\frac{1}{2}\\left((U'(y))^2 - U''(y)\\right)$.\n- **Global Bounds:** $m = \\inf_{y \\in \\mathbb{R}} V(y)$ and $M = \\sup_{y \\in \\mathbb{R}} V(y)$.\n- **Efficiency Proxy:** $p_{\\mathrm{lb}}(T) = \\exp(-T(M - m))$.\n- **Efficiency Condition:** Test is efficient if $p_{\\mathrm{lb}}(T) \\ge \\tau$ for a threshold $\\tau \\in (0,1)$.\n- **Specific Potentials:**\n    1. $U_{\\mathrm{ch}}(x) = \\log\\cosh(\\alpha x)$ for $\\alpha  0$.\n    2. $U_{\\mathrm{sq}}(x) = \\sqrt{1 + (\\alpha x)^2}$ for $\\alpha  0$.\n- **Tasks:**\n    1. Derive closed-form expressions for $M-m$ for each potential.\n    2. Implement a program to compute $M-m$, $p_{\\mathrm{lb}}(T)$, and an efficiency indicator for given test cases.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding:** The problem is firmly rooted in the theory of stochastic calculus and computational statistics, specifically regarding exact simulation algorithms for diffusion processes (the Beskos–Roberts framework). The use of a change of measure from the diffusion law to a Brownian bridge law, the resulting Radon–Nikodym derivative involving a Feynman–Kac term, and rejection sampling via Poisson thinning are all standard and mathematically rigorous concepts in this field. The chosen potentials are common, well-behaved functions. The problem is scientifically sound.\n- **Well-Posedness:** The definitions are precise, and the objectives are clear. The task is to derive analytical expressions and then perform numerical calculations based on them. The functions involved are smooth and their global extrema can be found using standard calculus, ensuring a unique solution exists.\n- **Objectivity:** The problem statement is formal, quantitative, and free of any subjective or ambiguous language.\n- **Conclusion:** The problem does not violate any of the invalidity criteria. It is a well-defined mathematical problem in computational statistics.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of Global Bounds\n\nThe core of the problem is to find the infimum $m$ and supremum $M$ of the function $V(y) = \\frac{1}{2}\\left((U'(y))^2 - U''(y)\\right)$ for the two specified potentials. The quantity of interest is $M-m$.\n\n#### Case 1: The Potential $U_{\\mathrm{ch}}(x) = \\log\\cosh(\\alpha x)$\n\nFirst, we compute the first and second derivatives of $U_{\\mathrm{ch}}(x)$ for $\\alpha  0$.\nThe first derivative is:\n$$\nU'_{\\mathrm{ch}}(x) = \\frac{d}{dx} \\log\\cosh(\\alpha x) = \\frac{1}{\\cosh(\\alpha x)} \\cdot \\sinh(\\alpha x) \\cdot \\alpha = \\alpha \\tanh(\\alpha x)\n$$\nThe second derivative is:\n$$\nU''_{\\mathrm{ch}}(x) = \\frac{d}{dx} (\\alpha \\tanh(\\alpha x)) = \\alpha \\cdot (\\text{sech}^2(\\alpha x) \\cdot \\alpha) = \\alpha^2 \\text{sech}^2(\\alpha x)\n$$\nNow, we construct the function $V(y)$ using these derivatives:\n$$\nV(y) = \\frac{1}{2}\\left((U'_{\\mathrm{ch}}(y))^2 - U''_{\\mathrm{ch}}(y)\\right) = \\frac{1}{2}\\left((\\alpha \\tanh(\\alpha y))^2 - \\alpha^2 \\text{sech}^2(\\alpha y)\\right)\n$$\nUsing the hyperbolic identity $\\text{sech}^2(z) = 1 - \\tanh^2(z)$, we can simplify $V(y)$:\n$$\nV(y) = \\frac{1}{2}\\left(\\alpha^2 \\tanh^2(\\alpha y) - \\alpha^2(1 - \\tanh^2(\\alpha y))\\right) = \\frac{1}{2}\\left(\\alpha^2 \\tanh^2(\\alpha y) - \\alpha^2 + \\alpha^2 \\tanh^2(\\alpha y)\\right)\n$$\n$$\nV(y) = \\frac{1}{2}\\left(2\\alpha^2 \\tanh^2(\\alpha y) - \\alpha^2\\right) = \\alpha^2 \\tanh^2(\\alpha y) - \\frac{\\alpha^2}{2}\n$$\nTo find the global bounds $m$ and $M$, we analyze the range of $V(y)$. The function $\\tanh(z)$ has a range of $(-1, 1)$. Therefore, $\\tanh^2(z)$ has a range of $[0, 1)$.\nThe infimum of $\\tanh^2(\\alpha y)$ over $y \\in \\mathbb{R}$ is $0$, achieved at $y=0$.\nThe supremum of $\\tanh^2(\\alpha y)$ over $y \\in \\mathbb{R}$ is $1$, approached as $|y| \\to \\infty$.\n\nThe infimum of $V(y)$ is therefore:\n$$\nm = \\inf_{y \\in \\mathbb{R}} V(y) = \\alpha^2 \\cdot 0 - \\frac{\\alpha^2}{2} = -\\frac{\\alpha^2}{2}\n$$\nThe supremum of $V(y)$ is:\n$$\nM = \\sup_{y \\in \\mathbb{R}} V(y) = \\alpha^2 \\cdot 1 - \\frac{\\alpha^2}{2} = \\frac{\\alpha^2}{2}\n$$\nThe difference $M-m$ is:\n$$\nM - m = \\frac{\\alpha^2}{2} - \\left(-\\frac{\\alpha^2}{2}\\right) = \\alpha^2\n$$\n\n#### Case 2: The Potential $U_{\\mathrm{sq}}(x) = \\sqrt{1 + (\\alpha x)^2}$\n\nNext, we analyze the potential $U_{\\mathrm{sq}}(x) = (1 + \\alpha^2 x^2)^{1/2}$ for $\\alpha  0$.\nThe first derivative is:\n$$\nU'_{\\mathrm{sq}}(x) = \\frac{d}{dx} (1 + \\alpha^2 x^2)^{1/2} = \\frac{1}{2}(1 + \\alpha^2 x^2)^{-1/2} \\cdot (2\\alpha^2 x) = \\frac{\\alpha^2 x}{\\sqrt{1 + \\alpha^2 x^2}}\n$$\nThe second derivative is found using the quotient rule:\n$$\nU''_{\\mathrm{sq}}(x) = \\frac{\\alpha^2 \\sqrt{1 + \\alpha^2 x^2} - \\alpha^2 x \\left(\\frac{\\alpha^2 x}{\\sqrt{1 + \\alpha^2 x^2}}\\right)}{1 + \\alpha^2 x^2}\n$$\nMultiplying the numerator and denominator by $\\sqrt{1 + \\alpha^2 x^2}$ simplifies the expression:\n$$\nU''_{\\mathrm{sq}}(x) = \\frac{\\alpha^2 (1 + \\alpha^2 x^2) - (\\alpha^2 x)^2}{(1 + \\alpha^2 x^2)^{3/2}} = \\frac{\\alpha^2 + \\alpha^4 x^2 - \\alpha^4 x^2}{(1 + \\alpha^2 x^2)^{3/2}} = \\frac{\\alpha^2}{(1 + \\alpha^2 x^2)^{3/2}}\n$$\nNow, we construct the function $V(y)$:\n$$\n(U'_{\\mathrm{sq}}(y))^2 = \\frac{\\alpha^4 y^2}{1 + \\alpha^2 y^2}\n$$\n$$\nV(y) = \\frac{1}{2}\\left((U'_{\\mathrm{sq}}(y))^2 - U''_{\\mathrm{sq}}(y)\\right) = \\frac{1}{2}\\left(\\frac{\\alpha^4 y^2}{1 + \\alpha^2 y^2} - \\frac{\\alpha^2}{(1 + \\alpha^2 y^2)^{3/2}}\\right)\n$$\nLet's factor out $\\frac{\\alpha^2}{2}$ and substitute $z = \\alpha y$:\n$$\nV(y) = \\frac{\\alpha^2}{2} \\left(\\frac{z^2}{1 + z^2} - \\frac{1}{(1 + z^2)^{3/2}}\\right)\n$$\nTo find the extrema of the term in the parenthesis, let $f(z) = \\frac{z^2}{1 + z^2} - \\frac{1}{(1 + z^2)^{3/2}}$. We can analyze this function by noting it is even, i.e., $f(z)=f(-z)$, and its behavior depends on $|z|$. Let $u = 1+z^2$, where $u \\in [1, \\infty)$. Then $z^2 = u-1$. We can rewrite $f(z)$ as a function of $u$:\n$$\ng(u) = \\frac{u-1}{u} - u^{-3/2} = 1 - u^{-1} - u^{-3/2}\n$$\nTo find the extrema of $g(u)$ for $u \\ge 1$, we compute its derivative:\n$$\ng'(u) = (-1)(-1)u^{-2} - (-\\frac{3}{2})u^{-5/2} = u^{-2} + \\frac{3}{2}u^{-5/2}\n$$\nFor $u \\ge 1$, both $u^{-2}$ and $u^{-5/2}$ are positive, so $g'(u)  0$. This means $g(u)$ is a strictly increasing function of $u$. Since $u=1+z^2$ is an increasing function of $|z|$, $f(z)=g(1+z^2)$ is a strictly increasing function of $|z|$.\nThe minimum of $f(z)$ occurs at the minimum of $|z|$, which is $z=0$ (corresponding to $u=1$).\n$$\n\\inf_{z \\in \\mathbb{R}} f(z) = f(0) = g(1) = 1 - 1^{-1} - 1^{-3/2} = 1 - 1 - 1 = -1\n$$\nThe supremum of $f(z)$ is the limit as $|z| \\to \\infty$ (corresponding to $u \\to \\infty$).\n$$\n\\sup_{z \\in \\mathbb{R}} f(z) = \\lim_{u \\to \\infty} g(u) = \\lim_{u \\to \\infty} \\left(1 - \\frac{1}{u} - \\frac{1}{u^{3/2}}\\right) = 1\n$$\nNow we can find the bounds for $V(y)$:\n$$\nm = \\inf_{y \\in \\mathbb{R}} V(y) = \\frac{\\alpha^2}{2} \\cdot (\\inf f(\\alpha y)) = \\frac{\\alpha^2}{2} \\cdot (-1) = -\\frac{\\alpha^2}{2}\n$$\n$$\nM = \\sup_{y \\in \\mathbb{R}} V(y) = \\frac{\\alpha^2}{2} \\cdot (\\sup f(\\alpha y)) = \\frac{\\alpha^2}{2} \\cdot (1) = \\frac{\\alpha^2}{2}\n$$\nThe difference $M-m$ is:\n$$\nM - m = \\frac{\\alpha^2}{2} - \\left(-\\frac{\\alpha^2}{2}\\right) = \\alpha^2\n$$\n\n### Conclusion of Derivation\nFor both potentials $U_{\\mathrm{ch}}(x)$ and $U_{\\mathrm{sq}}(x)$, the difference between the global supremum and infimum of the Feynman-Kac integrand $V(y)$ is given by the simple expression:\n$$\nM - m = \\alpha^2\n$$\nThe efficiency proxy is thus $p_{\\mathrm{lb}}(T) = \\exp(-T(M-m)) = \\exp(-T\\alpha^2)$. The test is deemed efficient if $\\exp(-T\\alpha^2) \\ge \\tau$. This result will be used in the implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the efficiency metrics for a single-shot exponential test\n    in an exact algorithm for simulating diffusion bridges.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (potential_type, alpha, T, tau)\n    test_cases = [\n        (\"ch\", 0.8, 1.0, 0.6),\n        (\"ch\", 1.5, 1.0, 0.2),\n        (\"sq\", 1.0, 0.5, 0.6),\n        (\"ch\", 0.5, 0.0, 0.9),\n        (\"sq\", 0.3, 3.0, 0.7),\n    ]\n\n    results = []\n    for case in test_cases:\n        potential_type, alpha, T, tau = case\n\n        # As derived in the solution, for both potential types (\"ch\" and \"sq\"),\n        # the difference M - m is given by alpha^2.\n        # M = sup V(y) and m = inf V(y) where V(y) = 0.5 * ((U'(y))^2 - U''(y)).\n        # For U_ch(x) = log(cosh(alpha*x)), M = alpha^2/2, m = -alpha^2/2.\n        # For U_sq(x) = sqrt(1 + (alpha*x)^2), M = alpha^2/2, m = -alpha^2/2.\n        # In both cases, M - m = alpha^2.\n        M_minus_m = alpha**2\n\n        # The efficiency proxy is p_lb(T) = exp(-T * (M - m)).\n        p_lb = np.exp(-T * M_minus_m)\n\n        # The efficiency indicator is 1 if p_lb = tau, and 0 otherwise.\n        efficient = 1 if p_lb = tau else 0\n        \n        # Store the results for this case.\n        results.append([M_minus_m, p_lb, efficient])\n\n    # Format the final output string as specified:\n    # [[M_minus_m_1,p_lb_1,efficient_1],...,[M_minus_m_5,p_lb_5,efficient_5]]\n    # No spaces are allowed after commas.\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]) + \"]\"\n\n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```", "id": "3306946"}, {"introduction": "Building on the theoretical foundations, this practice challenges you to implement a complete exact simulation algorithm for diffusion bridges from first principles. You will apply the Lamperti transform to standardize volatility, derive the appropriate potential for a pathwise rejection sampling scheme, and construct the simulation using Poisson thinning [@problem_id:3306885]. This capstone exercise will solidify your understanding of the entire workflow, from theoretical derivation to practical, code-based implementation.", "problem": "You are asked to design and implement an exact simulation scheme for a one-dimensional diffusion using a Lamperti transform to unit volatility, a Brownian bridge proposal, and pathwise Poisson thinning based on a computable potential. The goal is to derive from first principles the transformation, the potential, and the thinning acceptance criterion, and then to investigate the minimal envelope constants for the potential on compact intervals. Your final answer must be a complete, runnable program.\n\nFundamental base to be used:\n- The definition of a one-dimensional stochastic differential equation (SDE) with Itô diffusion: $dX_t = b(X_t)\\,dt + \\sigma(X_t)\\,dW_t$, where $b$ is the drift, $\\sigma$ is the volatility, and $W_t$ is a standard Brownian motion.\n- Itô's formula for transformations of Itô diffusions.\n- The Girsanov theorem for changes of measure between drifted Brownian motion and standard Brownian motion and the Feynman–Kac representation tied to path integrals.\n- Properties of the Brownian bridge and Poisson point processes.\n\nExact simulation target and derivation scope:\n- Start from the SDE $dX_t = b(X_t)\\,dt + \\sigma(X_t)\\,dW_t$ and define the Lamperti transform $Y_t = \\eta(X_t)$ where $\\eta'(x) = 1/\\sigma(x)$ for all $x$ and $\\eta$ is strictly increasing. Show that $Y_t$ has unit volatility $dY_t = \\alpha(Y_t)\\,dt + dW_t$ and derive $\\alpha(y)$ from $b(x)$ and $\\sigma(x)$ using Itô's formula.\n- Using the Girsanov theorem and the Feynman–Kac framework for unit-volatility diffusions, derive a potential function $V(y)$ that appears in the acceptance probability for exact path sampling when proposing a Brownian bridge for $Y_t$ between given endpoints.\n- Show how to construct a pathwise Poisson thinning acceptance criterion on $[0,T]\\times[0,M]$ against the graph of an inhomogeneous rate function $\\phi(y) \\ge 0$, where $\\phi(y)$ is obtained by shifting $V(y)$ by a constant $l$ so that $\\phi(y) = V(y) - l \\ge 0$ on the compact interval of interest. Justify that the acceptance event \"no Poisson point lies under the graph of $\\phi(Y_t)$\" yields an exact acceptance probability $\\exp\\big(-\\int_0^T \\phi(Y_s)\\,ds\\big)$.\n- Investigate minimal envelope constants: for a given compact interval $I = [a,b] \\subset \\mathbb{R}$ in the $y$-space, define $l = \\inf_{y\\in I} V(y)$ and $M = \\sup_{y\\in I} \\phi(y) = \\sup_{y\\in I} (V(y) - l)$. Explain why $M$ is the minimal constant such that $\\phi(y) \\le M$ for all $y \\in I$, and discuss the role of this minimal envelope constant in the efficiency of the thinning scheme.\n\nImplementation requirements:\n- Your program must implement the exact simulation acceptance test for a Brownian bridge in $y$-space using a pathwise Poisson thinning construction:\n  - Simulate a Poisson random variable $N$ with mean $M T$, where $M$ is the envelope constant on the compact interval $I$ and $T  0$ is the time horizon.\n  - Conditional on $N$, simulate $N$ proposal times $\\{t_i\\}_{i=1}^N$ independently and uniformly on $[0,T]$ and $N$ marks $\\{v_i\\}_{i=1}^N$ independently and uniformly on $[0,M]$.\n  - Sample jointly the vector $\\{Y_{t_i}\\}_{i=1}^N$ from a Brownian bridge with unit volatility from $y_0$ to $y_T$ over $[0,T]$. The Brownian bridge at times $\\{t_i\\}_{i=1}^N$ is a multivariate normal with mean vector $m_i = y_0 + (t_i/T)(y_T - y_0)$ and covariance $\\mathrm{Cov}(Y_{t_i},Y_{t_j}) = \\min(t_i,t_j) - (t_i t_j)/T$.\n  - Accept the proposed path if and only if $v_i  \\phi(Y_{t_i})$ for all $i \\in \\{1,\\dots,N\\}$, with $\\phi(y) = V(y) - l$ and $l = \\inf_{y\\in I} V(y)$.\n- Your program must compute $V(y)$ for each $y$ by first computing $\\alpha(y)$ and its derivative with respect to $y$. You may use numerical differentiation for $\\alpha'(y)$ if needed, but ensure numerical stability on the interval $I$.\n\nTest suite:\nImplement and run the following three test cases. For each test, let $I$ be a compact interval in the $y$-space as specified, and use the Brownian bridge with endpoints $y_0 = \\eta(x_0)$ and $y_T = \\eta(x_T)$, where $(x_0,x_T)$ are given.\n\n- Test $1$ (happy path, constant potential): Geometric Brownian motion with $b(x) = \\mu x$, $\\sigma(x) = \\sigma x$, parameters $\\mu = 0.2$, $\\sigma = 0.5$, time horizon $T = 0.7$, endpoints $x_0 = 1.0$, $x_T = 1.3$. For this case $\\eta(x) = \\frac{1}{\\sigma}\\log(x)$ and $I = [\\min(\\eta(x_0),\\eta(x_T)) - 0.5,\\max(\\eta(x_0),\\eta(x_T)) + 0.5]$.\n- Test $2$ (Ornstein–Uhlenbeck with constant volatility, nontrivial convex $\\phi$): Ornstein–Uhlenbeck with $b(x) = \\kappa(\\theta - x)$, $\\sigma(x) = \\sigma_0$, parameters $\\kappa = 1.0$, $\\theta = 0.0$, $\\sigma_0 = 1.0$, time horizon $T = 1.0$, endpoints $x_0 = 0.2$, $x_T = -0.1$. Here $\\eta(x) = x/\\sigma_0$, and $I = [-1.0,1.0]$ in $y$-space.\n- Test $3$ (state-dependent volatility, nonconstant but bounded $\\phi$): Driftless diffusion with $b(x) = 0$ and $\\sigma(x) = \\sqrt{1+x^2}$, time horizon $T = 0.5$, endpoints $x_0 = 0.5$, $x_T = -0.2$. Here $\\eta(x) = \\mathrm{arcsinh}(x)$ and $I = [-1.0,1.0]$ in $y$-space.\n\nNumerical requirements:\n- Use a fixed random seed so that results are reproducible.\n- For each test case, numerically compute $l = \\inf_{y\\in I} V(y)$ and $M = \\sup_{y\\in I} (V(y) - l)$ by evaluating $V(y)$ on a fine grid over $I$ and taking the empirical infimum and supremum.\n- Use exact Brownian bridge sampling for the vector $\\{Y_{t_i}\\}$ by drawing from the correct multivariate normal distribution at the sampled times.\n\nAnswer specification and output format:\n- For each test case, return a list containing the minimal envelope constant $M$ as a float, the baseline shift $l$ as a float, and the acceptance decision as a boolean.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the three per-test lists in order. For example, the final output must look like $[[M_1,l_1,\\mathrm{accept}_1],[M_2,l_2,\\mathrm{accept}_2],[M_3,l_3,\\mathrm{accept}_3]]$.\n\nNo physical units or angles are involved in this problem. If you need to report fractions or decimals, use decimal representation.", "solution": "The problem is valid as it is scientifically grounded in the theory of stochastic calculus and Monte Carlo methods, is well-posed with all necessary information provided, and is stated objectively. We proceed with the derivation and solution.\n\nThe objective is to construct an exact simulation algorithm for a one-dimensional Itô diffusion given by the stochastic differential equation (SDE):\n$$\ndX_t = b(X_t)\\,dt + \\sigma(X_t)\\,dW_t\n$$\nwhere $X_t$ is the state of the process at time $t$, $b(x)$ is the drift function, $\\sigma(x)$ is the volatility function, and $W_t$ is a standard one-dimensional Brownian motion. The algorithm will be based on a Lamperti transform, a change of measure argument, and Poisson thinning for path acceptance.\n\nFirst, we perform a Lamperti transform to simplify the SDE. Let $Y_t = \\eta(X_t)$ be a transformation of the process $X_t$, where the function $\\eta(x)$ is chosen to make the volatility of the new process $Y_t$ constant and equal to unity. We require $\\eta(x)$ to be twice continuously differentiable and strictly increasing. By applying Itô's formula to $Y_t = \\eta(X_t)$, we have:\n$$\ndY_t = \\eta'(X_t)dX_t + \\frac{1}{2}\\eta''(X_t)(dX_t)^2\n$$\nUsing the rules of Itô calculus, $(dX_t)^2 = (\\sigma(X_t)dW_t)^2 = \\sigma(X_t)^2 dt$. Substituting the SDE for $dX_t$:\n$$\ndY_t = \\eta'(X_t) \\left( b(X_t)dt + \\sigma(X_t)dW_t \\right) + \\frac{1}{2}\\eta''(X_t)\\sigma(X_t)^2 dt\n$$\n$$\ndY_t = \\left( b(X_t)\\eta'(X_t) + \\frac{1}{2}\\sigma(X_t)^2\\eta''(X_t) \\right)dt + \\eta'(X_t)\\sigma(X_t)dW_t\n$$\nTo achieve unit volatility, we must set the coefficient of the $dW_t$ term to one. This gives the condition for $\\eta'(x)$:\n$$\n\\eta'(x)\\sigma(x) = 1 \\implies \\eta'(x) = \\frac{1}{\\sigma(x)}\n$$\nSince $\\sigma(x)$ is typically assumed to be positive, $\\eta'(x)  0$, and $\\eta(x)$ is strictly increasing. We can define $\\eta(x) = \\int^x \\frac{1}{\\sigma(u)} du$. The inverse function $x = \\eta^{-1}(y)$ exists. With this choice, the SDE for $Y_t$ becomes:\n$$\ndY_t = \\alpha(Y_t)dt + dW_t\n$$\nThe new drift function $\\alpha(y)$ is given by the drift term in the transformed SDE, evaluated at $x = \\eta^{-1}(y)$:\n$$\n\\alpha(y) = \\left[ b(x)\\eta'(x) + \\frac{1}{2}\\sigma(x)^2\\eta''(x) \\right]_{x=\\eta^{-1}(y)}\n$$\nUsing $\\eta'(x) = 1/\\sigma(x)$ and $\\eta''(x) = -\\sigma'(x)/\\sigma(x)^2$, we can simplify $\\alpha(y)$:\n$$\n\\alpha(y) = \\left[ \\frac{b(x)}{\\sigma(x)} - \\frac{1}{2}\\sigma'(x) \\right]_{x=\\eta^{-1}(y)}\n$$\n\nNext, we derive the acceptance criterion. The process $Y_t$ follows a drifted Brownian motion. We wish to simulate paths of $Y_t$ on a time interval $[0,T]$ conditional on its endpoints, $Y_0 = y_0$ and $Y_T = y_T$. The core idea of exact simulation is to propose a candidate path from a simpler process, a Brownian bridge, and then accept or reject it in a way that corrects for the difference in probability measures. A Brownian bridge is a standard Brownian motion conditioned on its endpoints. Let $\\mathbb{Q}$ be the law of a Brownian bridge from $y_0$ to $y_T$, and let $\\mathbb{P}$ be the law of the target process $Y_t$ following $dY_t = \\alpha(Y_t)dt+dW_t$, also conditioned on the same endpoints.\n\nAccording to Girsanov's theorem, the Radon-Nikodym derivative that relates the measure $\\mathbb{P}$ to the measure of a standard (undrifted) Brownian motion is:\n$$\n\\frac{d\\mathbb{P}}{d\\mathbb{Q}_{\\text{BM}}} = \\exp\\left( \\int_0^T \\alpha(Y_s)dY_s - \\frac{1}{2}\\int_0^T \\alpha(Y_s)^2 ds \\right)\n$$\nThe measure of a Brownian bridge $\\mathbb{Q}$ is related to the standard Brownian motion measure $\\mathbb{Q}_{\\text{BM}}$ by a conditioning factor, which is constant for fixed endpoints. Thus, the density of the target law with respect to the proposal law is proportional to this exponential factor. The term $\\int_0^T \\alpha(Y_s)dY_s$ is a stochastic integral, which is inconvenient for pathwise rejection sampling. We can express it as a regular integral by defining a potential $A(y)$ such that $A'(y) = \\alpha(y)$. Using Itô's formula on $A(Y_t)$:\n$$\ndA(Y_t) = A'(Y_t)dY_t + \\frac{1}{2}A''(Y_t)dt = \\alpha(Y_t)dY_t + \\frac{1}{2}\\alpha'(Y_t)dt\n$$\nIntegrating from $0$ to $T$ gives $\\int_0^T \\alpha(Y_s)dY_s = A(Y_T) - A(Y_0) - \\frac{1}{2}\\int_0^T \\alpha'(Y_s)ds$. Substituting this into the Radon-Nikodym derivative:\n$$\n\\frac{d\\mathbb{P}}{d\\mathbb{Q}} \\propto \\exp\\left( A(Y_T) - A(Y_0) - \\frac{1}{2}\\int_0^T \\alpha'(Y_s)ds - \\frac{1}{2}\\int_0^T \\alpha(Y_s)^2 ds \\right)\n$$\nSince the endpoints $Y_0$ and $Y_T$ are fixed, $A(Y_T) - A(Y_0)$ is a constant and can be absorbed into the normalization of the probability. The acceptance probability for a proposed Brownian bridge path $\\{Y_s\\}_{s\\in[0,T]}$ is therefore proportional to:\n$$\nP_{\\text{accept}} \\propto \\exp\\left( -\\int_0^T \\left( \\frac{1}{2}\\alpha(Y_s)^2 + \\frac{1}{2}\\alpha'(Y_s) \\right) ds \\right)\n$$\nThis defines the potential function $V(y)$:\n$$\nV(y) = \\frac{1}{2}\\left( \\alpha(y)^2 + \\alpha'(y) \\right)\n$$\n\nTo implement a rejection sampling scheme with this acceptance probability, we use Poisson thinning. The potential $V(y)$ can be negative, but rates for a Poisson process must be non-negative. We define a shifted, non-negative rate function $\\phi(y)$ over a compact interval of interest $I = [a,b]$:\n$$\n\\phi(y) = V(y) - l, \\quad \\text{where } l = \\inf_{y \\in I} V(y)\n$$\nBy construction, $\\phi(y) \\ge 0$ for all $y \\in I$. The acceptance probability can be rewritten as:\n$$\nP_{\\text{accept}} \\propto \\exp\\left( -\\int_0^T (\\phi(Y_s) + l) ds \\right) = e^{-lT} \\exp\\left( -\\int_0^T \\phi(Y_s) ds \\right)\n$$\nThe term $e^{-lT}$ is a constant. We can simulate the event with probability $\\exp(-\\int_0^T \\phi(Y_s) ds)$ by the following method. Consider a two-dimensional homogeneous Poisson point process on the rectangle $[0,T] \\times [0,M]$ with unit intensity, where $M$ is an upper bound for $\\phi(y)$ on $I$, i.e., $M = \\sup_{y \\in I} \\phi(y)$. The probability that no point $(t_i, v_i)$ from this process falls into the region under the curve of $\\phi(Y_s)$ (i.e., $v_i \\le \\phi(Y_{t_i})$) is precisely $\\exp(-\\int_0^T \\phi(Y_s) ds)$.\n\nThis leads to the pathwise Poisson thinning algorithm:\n1.  For a given compact interval $I$ and time horizon $T$, compute $l = \\inf_{y\\in I} V(y)$ and $M = \\sup_{y\\in I} (V(y) - l)$. This $M$ is the minimal envelope constant. Its minimality is crucial for the efficiency of the algorithm, as the number of proposal points to check is proportional to $M$.\n2.  Generate the number of proposal points $N$ from a Poisson distribution with mean $\\lambda = M \\cdot T$.\n3.  If $N=0$, accept the path. Otherwise, generate $N$ independent proposal times $t_1, \\dots, t_N$ uniformly on $[0,T]$ and $N$ independent marks $v_1, \\dots, v_N$ uniformly on $[0,M]$.\n4.  Propose a path by sampling the values of a Brownian bridge $\\{Y_s\\}_{s\\in[0,T]}$ with endpoints $y_0, y_T$ at the required times $\\{t_i\\}$. The vector $(Y_{t_1}, \\dots, Y_{t_N})$ is drawn from a multivariate normal distribution with mean vector $\\mu_i = y_0 + (t_i/T)(y_T-y_0)$ and covariance matrix $\\Sigma_{ij} = \\min(t_i, t_j) - t_i t_j / T$.\n5.  Accept the proposed path if and only if for all $i \\in \\{1,\\dots,N\\}$, the point $(t_i, v_i)$ is above the rate function graph, i.e., $v_i  \\phi(Y_{t_i})$. Otherwise, reject the path.", "answer": "```python\nimport numpy as np\nfrom scipy.special import erf\n\n# Per the problem spec, we can use numpy and scipy.\n# scipy.special is part of scipy.\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for exact diffusion simulation.\n    \"\"\"\n    np.random.seed(42)\n\n    # --- Test Case Definitions ---\n\n    # Test Case 1: Geometric Brownian Motion\n    mu1, sigma1 = 0.2, 0.5\n    eta1 = lambda x: (1 / sigma1) * np.log(x)\n    alpha1_val = mu1 / sigma1 - sigma1 / 2\n    # V(y) is constant because alpha(y) is constant and alpha'(y) = 0\n    V1 = lambda y: 0.5 * (alpha1_val**2)\n    T1, x0_1, xT_1 = 0.7, 1.0, 1.3\n    y0_1, yT_1 = eta1(x0_1), eta1(xT_1)\n    I1 = [min(y0_1, yT_1) - 0.5, max(y0_1, yT_1) + 0.5]\n    case1 = (V1, T1, y0_1, yT_1, I1)\n\n    # Test Case 2: Ornstein-Uhlenbeck\n    kappa2, theta2, sigma0_2 = 1.0, 0.0, 1.0\n    # eta(x) = x/sigma0, so just x\n    # alpha(y) = kappa*theta/sigma0 - kappa*y. With given params, alpha(y) = -y\n    # alpha'(y) = -1\n    V2 = lambda y: 0.5 * ((-y)**2 - 1)\n    T2, x0_2, xT_2 = 1.0, 0.2, -0.1\n    y0_2, yT_2 = x0_2, xT_2  # Since eta(x) = x\n    I2 = [-1.0, 1.0]\n    case2 = (V2, T2, y0_2, yT_2, I2)\n\n    # Test Case 3: Driftless diffusion\n    # b(x) = 0, sigma(x) = sqrt(1+x^2)\n    # eta(x) = arcsinh(x)\n    # alpha(y) = -0.5 * tanh(y)\n    # alpha'(y) = -0.5 * sech^2(y)\n    # V(y) = 0.5 * (alpha^2 + alpha') = 0.5 * (0.25*tanh^2(y) - 0.5*sech^2(y))\n    # Using sech^2 = 1 - tanh^2, V(y) = 1/8 * (3*tanh^2(y) - 2)\n    V3 = lambda y: (1 / 8) * (3 * np.tanh(y)**2 - 2)\n    T3, x0_3, xT_3 = 0.5, 0.5, -0.2\n    y0_3, yT_3 = np.arcsinh(x0_3), np.arcsinh(xT_3)\n    I3 = [-1.0, 1.0]\n    case3 = (V3, T3, y0_3, yT_3, I3)\n\n    test_cases = [case1, case2, case3]\n    results = []\n\n    for case in test_cases:\n        result = run_simulation(*case)\n        results.append(result)\n\n    # --- Final Output Formatting ---\n    def format_result(res):\n        # Manually format the list string to avoid spaces and format the boolean\n        return f\"[{res[0]},{res[1]},{str(res[2]).lower()}]\"\n\n    formatted_results = [format_result(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_simulation(V, T, y0, yT, I):\n    \"\"\"\n    Executes one instance of the exact simulation algorithm.\n    \n    Args:\n        V (callable): The potential function V(y).\n        T (float): The time horizon.\n        y0 (float): The starting point in y-space.\n        yT (float): The ending point in y-space.\n        I (list or tuple): The compact interval [a, b].\n\n    Returns:\n        list: A list containing [M, l, accept].\n    \"\"\"\n    # Step 1: Numerically compute l and M\n    y_grid = np.linspace(I[0], I[1], 10001)\n    V_values = V(y_grid)\n    \n    l = np.min(V_values)\n    phi_values = V_values - l\n    M = np.max(phi_values)\n\n    # Step 2: Simulate Poisson points\n    if M * T = 0:\n        N = 0\n    else:\n        N = np.random.poisson(M * T)\n\n    if N == 0:\n        return [M, l, True]\n\n    t_proposals = np.random.uniform(0, T, N)\n    v_proposals = np.random.uniform(0, M, N)\n\n    # Step 3: Sample Brownian bridge\n    # Sort times for easier covariance matrix construction\n    sorted_indices = np.argsort(t_proposals)\n    t = t_proposals[sorted_indices]\n    v = v_proposals[sorted_indices]\n\n    # Mean vector\n    mu_bridge = y0 + (t / T) * (yT - y0)\n\n    # Covariance matrix\n    t_i_matrix = np.tile(t, (N, 1))\n    t_j_matrix = t_i_matrix.T\n    cov_bridge = np.minimum(t_i_matrix, t_j_matrix) - (t_i_matrix * t_j_matrix) / T\n    # Add small jitter for numerical stability, though often not needed for bridge\n    cov_bridge += np.eye(N) * 1e-12\n\n    Y_samples = np.random.multivariate_normal(mu_bridge, cov_bridge)\n\n    # Step 4: Acceptance test\n    phi_samples = V(Y_samples) - l\n    \n    accept = np.all(v  phi_samples)\n\n    return [M, l, bool(accept)]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3306885"}]}