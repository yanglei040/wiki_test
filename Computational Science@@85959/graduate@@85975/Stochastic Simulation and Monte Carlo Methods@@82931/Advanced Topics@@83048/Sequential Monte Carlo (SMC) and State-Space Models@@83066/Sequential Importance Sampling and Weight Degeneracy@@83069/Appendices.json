{"hands_on_practices": [{"introduction": "In sequential importance sampling, raw importance weights are normalized to sum to one, creating a discrete probability distribution. This normalization step, while seemingly simple, is not statistically benign; it introduces a crucial dependence structure among the weights. This practice [@problem_id:3336434] challenges you to explore these dependencies from first principles, investigating how the constraint $\\sum_{i=1}^N \\tilde w_i = 1$ affects the covariance of the weights. By working through these statistical properties, you will uncover the motivation and derivation for the most widely used diagnostic for weight degeneracy: the Effective Sample Size ($N_{\\text{eff}}$), a vital tool for monitoring the health of any particle filter.", "problem": "Consider importance sampling for estimating a target expectation. Let $X_1,\\dots,X_N$ be independent and identically distributed (IID) draws from a proposal density $q(x)$, and let the target density be $p(x)$ with $p(x)>0$ on the support of $q(x)$. Define the unnormalized importance weights $w_i = p(X_i)/q(X_i)$ and the normalized weights $\\tilde w_i = w_i/\\sum_{j=1}^N w_j$, so that $\\sum_{i=1}^N \\tilde w_i = 1$. The self-normalized importance sampling estimator of a measurable function $h$ is $\\hat{\\mu} = \\sum_{i=1}^N \\tilde w_i h(X_i)$. In sequential Monte Carlo and particle filtering, resampling decisions and diagnostics often rely on summarizing the dispersion of the normalized weights via an effective sample size (ESS).\n\nStarting from the fundamental definitions of covariance, variance, and conditional expectation, and using only the normalization constraint $\\sum_{i=1}^N \\tilde w_i = 1$ and the IID assumption on $X_i$, select all statements below that are correct about the dependence structure induced by normalization and its implications for variance calculations and effective sample size.\n\nA. Because $\\sum_{j=1}^N \\tilde w_j = 1$ almost surely, for each $i$ one has $\\sum_{j=1}^N \\operatorname{Cov}(\\tilde w_i, \\tilde w_j) = 0$.\n\nB. If the joint distribution of $(\\tilde w_1,\\dots,\\tilde w_N)$ is exchangeable, then for $i \\neq j$ one has $\\operatorname{Cov}(\\tilde w_i, \\tilde w_j) = -\\operatorname{Var}(\\tilde w_i)/(N-1)$.\n\nC. For any function $h$, $\\operatorname{Var}\\!\\left(\\sum_{i=1}^N \\tilde w_i h(X_i)\\right) = \\sum_{i=1}^N \\operatorname{Var}(\\tilde w_i h(X_i))$, because the $\\tilde w_i$ are independent by construction after normalization.\n\nD. If $h(X_i) = c$ for all $i$ (with $c$ a constant), then $\\operatorname{Var}\\!\\left(\\sum_{i=1}^N \\tilde w_i h(X_i)\\right) = c^2 \\sum_{i=1}^N \\operatorname{Var}(\\tilde w_i)$.\n\nE. Under the approximation that the random weights are independent of $h(X_i)$ and the $X_i$ are IID, a practical definition of effective sample size is $N_{\\mathrm{eff}} = 1/\\sum_{i=1}^N \\tilde w_i^2$, obtained by equating the variance of $\\sum_{i=1}^N \\tilde w_i h(X_i)$ to that of an equally weighted average of $N_{\\mathrm{eff}}$ IID samples.\n\nF. The covariance matrix $\\operatorname{Cov}(\\tilde w)$ of $(\\tilde w_1,\\dots,\\tilde w_N)$ is singular with rank at most $N-1$.\n\nG. The normalization constraint implies $\\operatorname{Corr}(\\tilde w_i,\\tilde w_j) = -1$ for all $i \\neq j$.", "solution": "The problem statement is valid. It presents a standard configuration for self-normalized importance sampling and asks for mathematically verifiable properties of the normalized weights, their covariances, and related statistical quantities. The setup is scientifically grounded in the theory of Monte Carlo methods, is well-posed, objective, and contains all necessary information for a rigorous analysis.\n\nThe fundamental insight required for this problem is that the normalization of weights, expressed by the constraint $\\sum_{i=1}^N \\tilde w_i = 1$, introduces a linear dependency among the otherwise exchangeable normalized weights. This dependency has direct consequences on their covariance structure.\n\nLet $\\tilde{\\mathbf{w}}$ be the column vector $(\\tilde w_1, \\dots, \\tilde w_N)^T$. The normalization constraint is $\\sum_{i=1}^N \\tilde w_i = 1$.\n\n**Option A: Because $\\sum_{j=1}^N \\tilde w_j = 1$ almost surely, for each $i$ one has $\\sum_{j=1}^N \\operatorname{Cov}(\\tilde w_i, \\tilde w_j) = 0$.**\n\nThis statement is a direct consequence of the properties of covariance. For any random variable $Y$ and constant $c$, $\\operatorname{Cov}(Y, c) = 0$.\nLet's apply this to our case. For any specific weight $\\tilde w_i$, we can compute its covariance with the sum of all weights, which is a constant ($1$).\n$$ \\operatorname{Cov}\\left(\\tilde w_i, \\sum_{j=1}^N \\tilde w_j\\right) = \\operatorname{Cov}(\\tilde w_i, 1) = 0 $$\nUsing the bilinearity property of the covariance operator, we can expand the left-hand side:\n$$ \\operatorname{Cov}\\left(\\tilde w_i, \\sum_{j=1}^N \\tilde w_j\\right) = \\sum_{j=1}^N \\operatorname{Cov}(\\tilde w_i, \\tilde w_j) $$\nEquating the two expressions gives the desired result:\n$$ \\sum_{j=1}^N \\operatorname{Cov}(\\tilde w_i, \\tilde w_j) = 0 $$\nThis holds for each $i \\in \\{1, \\dots, N\\}$. The reasoning is sound and follows directly from fundamental definitions.\n**Verdict: Correct.**\n\n**Option B: If the joint distribution of $(\\tilde w_1,\\dots,\\tilde w_N)$ is exchangeable, then for $i \\neq j$ one has $\\operatorname{Cov}(\\tilde w_i, \\tilde w_j) = -\\operatorname{Var}(\\tilde w_i)/(N-1)$.**\n\nFirst, we establish that the joint distribution of $(\\tilde w_1, \\dots, \\tilde w_N)$ is indeed exchangeable. The samples $X_1, \\dots, X_N$ are IID from $q(x)$. Therefore, the unnormalized weights $w_i = p(X_i)/q(X_i)$ are also IID. The joint probability distribution of $(w_1, \\dots, w_N)$ is symmetric under any permutation of its components. Since the normalized weights $\\tilde w_i = w_i / \\sum_{k=1}^N w_k$ are a symmetric function of the unnormalized weights (i.e., permuting the $w_k$ results in the same permutation of the $\\tilde w_k$), the joint distribution of $(\\tilde w_1, \\dots, \\tilde w_N)$ is also exchangeable.\n\nExchangeability implies that:\n1. $\\operatorname{Var}(\\tilde w_i) = \\operatorname{Var}(\\tilde w_j)$ for all $i, j$. Let's denote this common variance as $\\sigma^2_{\\tilde w}$.\n2. $\\operatorname{Cov}(\\tilde w_i, \\tilde w_j)$ is the same for all distinct pairs $i \\neq j$. Let's denote this common covariance as $\\rho_{\\tilde w}$.\n\nFrom the result in Option A, for any $i$:\n$$ \\sum_{j=1}^N \\operatorname{Cov}(\\tilde w_i, \\tilde w_j) = 0 $$\nWe can split the sum into the term where $j=i$ and the terms where $j \\neq i$:\n$$ \\operatorname{Cov}(\\tilde w_i, \\tilde w_i) + \\sum_{j \\neq i} \\operatorname{Cov}(\\tilde w_i, \\tilde w_j) = 0 $$\n$$ \\operatorname{Var}(\\tilde w_i) + \\sum_{j \\neq i} \\operatorname{Cov}(\\tilde w_i, \\tilde w_j) = 0 $$\nThere are $N-1$ terms in the summation. Using the consequences of exchangeability:\n$$ \\sigma^2_{\\tilde w} + (N-1)\\rho_{\\tilde w} = 0 $$\nSolving for the covariance $\\rho_{\\tilde w}$:\n$$ \\rho_{\\tilde w} = -\\frac{\\sigma^2_{\\tilde w}}{N-1} $$\nSubstituting back the original expressions:\n$$ \\operatorname{Cov}(\\tilde w_i, \\tilde w_j) = -\\frac{\\operatorname{Var}(\\tilde w_i)}{N-1} \\quad \\text{for } i \\neq j $$\nThe statement is therefore a correct derivation.\n**Verdict: Correct.**\n\n**Option C: For any function $h$, $\\operatorname{Var}\\!\\left(\\sum_{i=1}^N \\tilde w_i h(X_i)\\right) = \\sum_{i=1}^N \\operatorname{Var}(\\tilde w_i h(X_i))$, because the $\\tilde w_i$ are independent by construction after normalization.**\n\nThe reasoning provided in the statement, \"because the $\\tilde w_i$ are independent\", is false. As demonstrated in the analysis for Option B, $\\operatorname{Cov}(\\tilde w_i, \\tilde w_j) = -\\operatorname{Var}(\\tilde w_i)/(N-1)$ for $i \\neq j$. Since $\\operatorname{Var}(\\tilde w_i)$ is generally non-zero, the covariance is non-zero, proving that the normalized weights are not independent.\n\nThe main equation is also incorrect. The variance of a sum of random variables $Z_i = \\tilde w_i h(X_i)$ is given by:\n$$ \\operatorname{Var}\\left(\\sum_{i=1}^N Z_i\\right) = \\sum_{i=1}^N \\operatorname{Var}(Z_i) + \\sum_{i \\neq j} \\operatorname{Cov}(Z_i, Z_j) $$\nThe statement is equivalent to asserting that $\\sum_{i \\neq j} \\operatorname{Cov}(\\tilde w_i h(X_i), \\tilde w_j h(X_j)) = 0$. This is not true in general due to the dependence among the $\\tilde w_i$.\nA simple counterexample is to set $h(X_i) = 1$ for all $i$. The estimator is $\\sum_{i=1}^N \\tilde w_i (1) = 1$. The variance of this constant is $\\operatorname{Var}(1) = 0$. However, the right-hand side of the statement's equation becomes $\\sum_{i=1}^N \\operatorname{Var}(\\tilde w_i)$. Since the weights are random, $\\operatorname{Var}(\\tilde w_i)>0$ in any non-trivial case, so their sum is strictly positive. This shows $0 \\neq \\sum_i \\operatorname{Var}(\\tilde w_i)$, disproving the statement.\n**Verdict: Incorrect.**\n\n**Option D: If $h(X_i) = c$ for all $i$ (with $c$ a constant), then $\\operatorname{Var}\\!\\left(\\sum_{i=1}^N \\tilde w_i h(X_i)\\right) = c^2 \\sum_{i=1}^N \\operatorname{Var}(\\tilde w_i)$.**\n\nLet $h(X_i) = c$. The quantity of interest is $\\sum_{i=1}^N \\tilde w_i h(X_i) = \\sum_{i=1}^N \\tilde w_i c = c \\sum_{i=1}^N \\tilde w_i$.\nUsing the normalization constraint $\\sum_{i=1}^N \\tilde w_i = 1$, this simplifies to $c \\cdot 1 = c$.\nThe variance of this quantity is the variance of a constant:\n$$ \\operatorname{Var}\\!\\left(\\sum_{i=1}^N \\tilde w_i h(X_i)\\right) = \\operatorname{Var}(c) = 0 $$\nThe right-hand side of the proposed equality is $c^2 \\sum_{i=1}^N \\operatorname{Var}(\\tilde w_i)$. As established in the analysis of Option C, $\\operatorname{Var}(\\tilde w_i) > 0$ in general. Thus, for any $c \\neq 0$, the sum $c^2 \\sum_{i=1}^N \\operatorname{Var}(\\tilde w_i) > 0$.\nThe statement claims that $0 = c^2 \\sum_{i=1}^N \\operatorname{Var}(\\tilde w_i)$, which is false for any non-trivial importance sampling problem where the weights vary and $c \\neq 0$.\n**Verdict: Incorrect.**\n\n**Option E: Under the approximation that the random weights are independent of $h(X_i)$ and the $X_i$ are IID, a practical definition of effective sample size is $N_{\\mathrm{eff}} = 1/\\sum_{i=1}^N \\tilde w_i^2$, obtained by equating the variance of $\\sum_{i=1}^N \\tilde w_i h(X_i)$ to that of an equally weighted average of $N_{\\mathrm{eff}}$ IID samples.**\n\nThis statement accurately describes the heuristic derivation of a widely used formula for the effective sample size ($N_{\\mathrm{eff}}$).\nThe variance of an ideal Monte Carlo estimator using $N_{\\mathrm{eff}}$ IID samples, $Z_1, \\dots, Z_{N_{\\mathrm{eff}}}$, drawn from the target distribution $p(x)$ is $\\operatorname{Var}(\\frac{1}{N_{\\mathrm{eff}}}\\sum_{j=1}^{N_{\\mathrm{eff}}} h(Z_j)) = \\frac{\\operatorname{Var}_p(h)}{N_{\\mathrm{eff}}}$.\nThe variance of the self-normalized importance sampling estimator, $\\hat{\\mu} = \\sum_{i=1}^N \\tilde w_i h(X_i)$, is complicated. The standard heuristic simplifies this by making several approximations, which are collectively (and imprecisely) described as treating the weights as independent of the function values $h(X_i)$. More formally, the variance is approximated as if the weights were fixed coefficients applied to IID random variables drawn from the target $p$, with variance $\\sigma_p^2 = \\operatorname{Var}_p(h)$.\nUnder this line of reasoning, the variance becomes:\n$$ \\operatorname{Var}(\\hat{\\mu}) \\approx \\operatorname{Var}\\left(\\sum_{i=1}^N \\tilde w_i Y_i\\right) $$\nwhere $Y_i$ are hypothetical IID samples from $p$ with variance $\\sigma_p^2$. Treating the weights $\\tilde w_i$ as given constants in this step, this becomes:\n$$ \\operatorname{Var}(\\hat{\\mu}) \\approx \\sum_{i=1}^N \\tilde w_i^2 \\operatorname{Var}(Y_i) = \\left(\\sum_{i=1}^N \\tilde w_i^2\\right) \\sigma_p^2 $$\nEquating the variance of the idealized estimator with this approximation:\n$$ \\frac{\\sigma_p^2}{N_{\\mathrm{eff}}} = \\left(\\sum_{i=1}^N \\tilde w_i^2\\right) \\sigma_p^2 $$\nSolving for $N_{\\mathrm{eff}}$ yields:\n$$ N_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^N \\tilde w_i^2} $$\nThe statement correctly identifies this formula and correctly describes the logic used to derive it as a practical measure. The approximations involved are implicitly acknowledged. The statement is a correct description of a standard method in the field.\n**Verdict: Correct.**\n\n**Option F: The covariance matrix $\\operatorname{Cov}(\\tilde w)$ of $(\\tilde w_1,\\dots,\\tilde w_N)$ is singular with rank at most $N-1$.**\n\nLet $\\tilde{\\mathbf{w}} = (\\tilde w_1, \\dots, \\tilde w_N)^T$ be the random vector of normalized weights, and let $\\Sigma = \\operatorname{Cov}(\\tilde{\\mathbf{w}})$ be its covariance matrix.\nThe normalization constraint is $\\sum_{i=1}^N \\tilde w_i = 1$. Let $\\mathbf{1}$ be the $N \\times 1$ vector of ones. The constraint can be written as $\\mathbf{1}^T \\tilde{\\mathbf{w}} = 1$.\nLet $\\boldsymbol{\\mu}_{\\tilde w} = \\mathbb{E}[\\tilde{\\mathbf{w}}]$. By taking the expectation of the constraint, we have $\\mathbf{1}^T \\boldsymbol{\\mu}_{\\tilde w} = 1$.\nThe vector of centered weights is $\\mathbf{z} = \\tilde{\\mathbf{w}} - \\boldsymbol{\\mu}_{\\tilde w}$. We see that $\\mathbf{1}^T \\mathbf{z} = \\mathbf{1}^T \\tilde{\\mathbf{w}} - \\mathbf{1}^T \\boldsymbol{\\mu}_{\\tilde w} = 1 - 1 = 0$.\nThis means that the centered random vector $\\mathbf{z}$ is always confined to the hyperplane orthogonal to the vector $\\mathbf{1}$, a subspace of dimension $N-1$.\nNow consider the product of the covariance matrix $\\Sigma$ with the vector $\\mathbf{1}$:\n$$ \\Sigma \\mathbf{1} = \\operatorname{Cov}(\\tilde{\\mathbf{w}}) \\mathbf{1} = \\mathbb{E}[(\\tilde{\\mathbf{w}} - \\boldsymbol{\\mu}_{\\tilde w})(\\tilde{\\mathbf{w}} - \\boldsymbol{\\mu}_{\\tilde w})^T] \\mathbf{1} = \\mathbb{E}[\\mathbf{z} (\\mathbf{z}^T \\mathbf{1})] $$\nSince $\\mathbf{z}^T \\mathbf{1} = (\\mathbf{1}^T \\mathbf{z})^T = 0^T = 0$, we have:\n$$ \\Sigma \\mathbf{1} = \\mathbb{E}[\\mathbf{z} \\cdot 0] = \\mathbf{0} $$\nThis shows that $\\mathbf{1}$ is a non-zero eigenvector of $\\Sigma$ with eigenvalue $0$. A matrix that has $0$ as an eigenvalue is singular (its determinant is $0$). By the rank-nullity theorem, the rank of $\\Sigma$ is $N - \\dim(\\operatorname{nullspace}(\\Sigma))$. Since the null space contains at least one non-zero vector ($\\mathbf{1}$), its dimension is at least $1$. Therefore, the rank of $\\Sigma$ is at most $N-1$.\n**Verdict: Correct.**\n\n**Option G: The normalization constraint implies $\\operatorname{Corr}(\\tilde w_i,\\tilde w_j) = -1$ for all $i \\neq j$.**\n\nThe correlation between two random variables is their covariance divided by the product of their standard deviations. For $i \\neq j$:\n$$ \\operatorname{Corr}(\\tilde w_i, \\tilde w_j) = \\frac{\\operatorname{Cov}(\\tilde w_i, \\tilde w_j)}{\\sqrt{\\operatorname{Var}(\\tilde w_i) \\operatorname{Var}(\\tilde w_j)}} $$\nAs shown in the analysis of Option B, due to exchangeability, $\\operatorname{Var}(\\tilde w_i) = \\operatorname{Var}(\\tilde w_j)$ and $\\operatorname{Cov}(\\tilde w_i, \\tilde w_j) = -\\operatorname{Var}(\\tilde w_i)/(N-1)$.\nSubstituting these into the correlation formula:\n$$ \\operatorname{Corr}(\\tilde w_i, \\tilde w_j) = \\frac{-\\operatorname{Var}(\\tilde w_i) / (N-1)}{\\sqrt{\\operatorname{Var}(\\tilde w_i) \\cdot \\operatorname{Var}(\\tilde w_i)}} = \\frac{-\\operatorname{Var}(\\tilde w_i) / (N-1)}{\\operatorname{Var}(\\tilde w_i)} = -\\frac{1}{N-1} $$\nA correlation of $-1$ implies perfect negative linear dependence. The statement claims this value is $-1$. This is only true if $N-1=1$, which means $N=2$. For $N=2$, we have $\\tilde w_1 + \\tilde w_2 = 1$, so $\\tilde w_2 = 1 - \\tilde w_1$, which is a perfect linear relationship, leading to $\\operatorname{Corr}(\\tilde w_1, \\tilde w_2) = -1$.\nHowever, the problem is stated for general $N$. For any $N > 2$, the correlation is $-1/(N-1)$, which lies between $-1$ and $0$. The statement is not true in general.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ABEF}$$", "id": "3336434"}, {"introduction": "When the effective sample size drops, indicating severe weight degeneracy, the standard remedy is to perform a resampling step. However, this cure is not without its cost, as it inevitably leads to a loss of particle diversity, a phenomenon known as sample impoverishment. This exercise [@problem_id:3417338] provides a foundational analysis of this trade-off using the canonical multinomial resampling scheme. By deriving the expected number of unique particles that survive the resampling process, you will gain a quantitative understanding of the \"price\" of resampling and how this loss of diversity becomes catastrophic under the extreme weight degeneracy caused by the curse of dimensionality.", "problem": "Consider a Sequential Monte Carlo (SMC) resampling step within a Bayesian data assimilation setting for an inverse problem. You have a weighted ensemble of $N$ particles $\\{x^{i}\\}_{i=1}^{N}$ with normalized importance weights $\\{\\tilde w^{i}\\}_{i=1}^{N}$, where $\\tilde w^{i} \\in (0,1)$ and $\\sum_{i=1}^{N} \\tilde w^{i} = 1$. A standard multinomial resampling scheme draws $N$ independent offspring indices $(A_{1},\\dots,A_{N})$, where each $A_{n} \\in \\{1,\\dots,N\\}$ is conditionally independent given the weights and satisfies $\\mathbb{P}(A_{n} = i \\mid \\tilde w^{1:N}) = \\tilde w^{i}$ for each $i \\in \\{1,\\dots,N\\}$ and each $n \\in \\{1,\\dots,N\\}$.\n\nUsing only the independence of the $N$ categorical draws and the definition of multinomial resampling, derive from first principles the following quantities:\n\nFirst, fix an index $k \\in \\{1,\\dots,N\\}$ and let $C_{k} := \\sum_{n=1}^{N} \\mathbf{1}\\{A_{n} = k\\}$ denote the number of times particle $k$ is selected. Compute the probability that particle $k$ is not selected at all, that is, compute $\\mathbb{P}(C_{k} = 0 \\mid \\tilde w^{1:N})$, expressed in closed form as a function of $N$ and $\\tilde w^{k}$.\n\nSecond, define the number of unique parents selected after resampling as $U := \\sum_{i=1}^{N} \\mathbf{1}\\{C_{i} \\ge 1\\}$. Compute the expected value $\\mathbb{E}[U \\mid \\tilde w^{1:N}]$ in closed form as a function of $N$ and $\\{\\tilde w^{i}\\}_{i=1}^{N}$.\n\nYour derivations must begin from the foundational properties of independent categorical sampling under multinomial resampling, without invoking any pre-derived resampling identities. Conclude by briefly explaining, at the level of qualitative asymptotics, how the expression for $\\mathbb{E}[U \\mid \\tilde w^{1:N}]$ reflects weight degeneracy characteristic of the curse of dimensionality in high-dimensional inverse problems.\n\nExpress your final answers as a single row matrix with two entries: the first entry is $\\mathbb{P}(C_{k} = 0 \\mid \\tilde w^{1:N})$ and the second entry is $\\mathbb{E}[U \\mid \\tilde w^{1:N}]$. No numerical rounding is required, and no physical units apply.", "solution": "The problem statement is a well-posed and scientifically grounded exercise in elementary probability theory applied to the standard model of multinomial resampling in Sequential Monte Carlo methods. All terms are formally defined, the premises are consistent, and the tasks are directly derivable from the provided information. The problem is therefore deemed valid.\n\nWe proceed with the derivations as requested, grounding each step in the first principles of probability and the aforestated problem definitions. Conditional dependence on the weights $\\{\\tilde w^{i}\\}_{i=1}^{N}$ is implicit in all probability and expectation calculations that follow. For notational clarity, we will omit the explicit conditioning notation $\\mid \\tilde w^{1:N}$ until the final statements.\n\nFirst, we compute the probability that a specific particle $k \\in \\{1,\\dots,N\\}$ is not selected at all during the resampling step. Let $C_{k}$ be the number of times particle $k$ is selected. The event $\\{C_{k} = 0\\}$ signifies that in all $N$ independent draws, the index $k$ was never chosen.\n\nLet $A_{n}$ denote the index chosen in the $n$-th draw, for $n \\in \\{1,\\dots,N\\}$. The event $\\{C_{k} = 0\\}$ is equivalent to the intersection of the events $\\{A_{n} \\neq k\\}$ for all $n$.\n$$\n\\mathbb{P}(C_{k} = 0) = \\mathbb{P}(A_{1} \\neq k \\text{ and } A_{2} \\neq k \\text{ and } \\dots \\text{ and } A_{N} \\neq k) = \\mathbb{P}\\left(\\bigcap_{n=1}^{N} \\{A_{n} \\neq k\\}\\right)\n$$\nThe problem states that the draws $(A_{1},\\dots,A_{N})$ are independent. Therefore, the probability of the intersection of these events is the product of their individual probabilities.\n$$\n\\mathbb{P}(C_{k} = 0) = \\prod_{n=1}^{N} \\mathbb{P}(A_{n} \\neq k)\n$$\nFor any single draw $n$, the probability of selecting index $i$ is given as $\\mathbb{P}(A_{n} = i) = \\tilde w^{i}$. The event $\\{A_n \\neq k\\}$ is the complement of the event $\\{A_n = k\\}$. The probability of this complementary event is:\n$$\n\\mathbb{P}(A_{n} \\neq k) = 1 - \\mathbb{P}(A_{n} = k) = 1 - \\tilde w^{k}\n$$\nSince this probability is identical for each of the $N$ independent draws, the product becomes:\n$$\n\\mathbb{P}(C_{k} = 0) = \\prod_{n=1}^{N} (1 - \\tilde w^{k}) = (1 - \\tilde w^{k})^{N}\n$$\nRestoring the explicit conditioning, the first required quantity is $\\mathbb{P}(C_{k} = 0 \\mid \\tilde w^{1:N}) = (1 - \\tilde w^{k})^{N}$.\n\nSecond, we compute the expected number of unique parent particles selected after resampling, denoted by $U$. The definition provided is $U := \\sum_{i=1}^{N} \\mathbf{1}\\{C_{i} \\ge 1\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The event $\\{C_{i} \\ge 1\\}$ signifies that particle $i$ is selected at least once.\n\nWe are asked to find the expectation of $U$. By the linearity of expectation, we can move the expectation operator inside the summation:\n$$\n\\mathbb{E}[U] = \\mathbb{E}\\left[\\sum_{i=1}^{N} \\mathbf{1}\\{C_{i} \\ge 1\\}\\right] = \\sum_{i=1}^{N} \\mathbb{E}[\\mathbf{1}\\{C_{i} \\ge 1\\}]\n$$\nThe expectation of an indicator function is the probability of the event it indicates. Thus, for each particle $i$:\n$$\n\\mathbb{E}[\\mathbf{1}\\{C_{i} \\ge 1\\}] = \\mathbb{P}(C_{i} \\ge 1)\n$$\nThe event that particle $i$ is selected at least once, $\\{C_i \\ge 1\\}$, is the complement of the event that it is never selected, $\\{C_i = 0\\}$. Therefore, its probability is:\n$$\n\\mathbb{P}(C_{i} \\ge 1) = 1 - \\mathbb{P}(C_{i} = 0)\n$$\nUsing the result from our first derivation, we can substitute the expression for $\\mathbb{P}(C_{i} = 0)$ by replacing the index $k$ with $i$:\n$$\n\\mathbb{P}(C_{i} = 0) = (1 - \\tilde w^{i})^{N}\n$$\nThis gives $\\mathbb{P}(C_{i} \\ge 1) = 1 - (1 - \\tilde w^{i})^{N}$. Substituting this back into the sum for $\\mathbb{E}[U]$:\n$$\n\\mathbb{E}[U] = \\sum_{i=1}^{N} \\left(1 - (1 - \\tilde w^{i})^{N}\\right)\n$$\nRestoring the explicit conditioning, the second required quantity is $\\mathbb{E}[U \\mid \\tilde w^{1:N}] = \\sum_{i=1}^{N} (1 - (1 - \\tilde w^{i})^{N})$.\n\nFinally, we explain how the expression for $\\mathbb{E}[U \\mid \\tilde w^{1:N}]$ reflects weight degeneracy characteristic of the curse of dimensionality. In high-dimensional state spaces, Bayesian updating often leads to a phenomenon called weight degeneracy or sample impoverishment. The likelihood function becomes sharply peaked, causing the posterior distribution to be concentrated in a very small volume of the state space. In the context of SMC, this means that after the importance sampling step, a single particle (or a very small number of them) that happens to fall in this high-likelihood region will acquire a weight $\\tilde w^{j}$ that is close to $1$, while the weights of all other particles, $\\tilde w^{i}$ for $i\\neq j$, will be infinitesimally small, i.e., $\\tilde w^{i} \\approx 0$.\n\nLet us analyze the behavior of $\\mathbb{E}[U]$ in this degenerate scenario. Consider the extreme case where one weight $\\tilde w^{j} \\to 1$ and all other weights $\\tilde w^{i} \\to 0$ for $i \\neq j$, subject to the constraint $\\sum_{i=1}^{N} \\tilde w^{i} = 1$.\nThe contribution to $\\mathbb{E}[U]$ from the particle $j$ with the dominant weight is:\n$$\n1 - (1 - \\tilde w^{j})^{N} \\to 1 - (1 - 1)^{N} = 1\n$$\nThis indicates that the high-weight particle is almost certain to be selected as an ancestor.\nThe contribution from any other particle $i \\neq j$ with a negligible weight $\\tilde w^{i} \\approx 0$ can be approximated. For a small value of $x$, $(1-x)^N \\approx 1-Nx$ by the binomial approximation. Thus:\n$$\n1 - (1 - \\tilde w^{i})^{N} \\approx 1 - (1 - N\\tilde w^{i}) = N\\tilde w^{i}\n$$\nSumming these contributions for all $i \\neq j$:\n$$\n\\sum_{i \\neq j} (1 - (1 - \\tilde w^{i})^{N}) \\approx \\sum_{i \\neq j} N\\tilde w^{i} = N \\sum_{i \\neq j} \\tilde w^{i}\n$$\nSince $\\sum_{i=1}^{N} \\tilde w^{i} = 1$, we have $\\sum_{i \\neq j} \\tilde w^{i} = 1 - \\tilde w^{j}$. So, this sum is $N(1-\\tilde w^{j})$.\nThe total expected number of unique particles is then:\n$$\n\\mathbb{E}[U] \\approx \\left(1 - (1 - \\tilde w^{j})^{N}\\right) + N(1-\\tilde w^{j})\n$$\nAs degeneracy becomes extreme, $\\tilde w^{j} \\to 1$, which implies $(1-\\tilde w^{j}) \\to 0$. In this limit:\n$$\n\\lim_{\\tilde w^{j} \\to 1} \\mathbb{E}[U] = 1 + N \\cdot 0 = 1\n$$\nAn expected number of unique particles approaching $1$ means that, after resampling, the new population of $N$ particles is overwhelmingly likely to consist of $N$ identical copies of a single ancestor. This catastrophic loss of diversity is the functional definition of particle filter collapse, a direct consequence of weight degeneracy, which is itself a manifestation of the curse of dimensionality. The derived expression for $\\mathbb{E}[U]$ thus provides a quantitative measure of this degradation.", "answer": "$$\n\\boxed{\\begin{pmatrix} (1 - \\tilde w^{k})^{N} & \\sum_{i=1}^{N} \\left(1 - (1 - \\tilde w^{i})^{N}\\right) \\end{pmatrix}}\n$$", "id": "3417338"}, {"introduction": "While multinomial resampling is fundamental for theoretical analysis, practical implementations often favor more structured and less random algorithms. Systematic resampling is a popular and efficient alternative that is known to reduce the Monte Carlo error introduced by the resampling step itself. This exercise [@problem_id:3417312] offers a concrete, step-by-step walkthrough of the systematic resampling algorithm. By calculating the resampled indices for a given set of weights and analyzing their structure, you will develop a practical intuition for how this method preserves local particle arrangements and why it is often preferred over fully independent resampling schemes.", "problem": "Consider a Sequential Monte Carlo (SMC) particle method applied to a Bayesian inverse problem in data assimilation with $N$ particles and normalized importance weights $\\{w_i\\}_{i=1}^N$ that sum to $1$. The particles are indexed in ascending order of a scalar state coordinate so that neighboring indices correspond to neighboring states. The SMC algorithm employs the systematic resampling scheme, defined as follows: draw a single $U$ from the uniform distribution on $[0, 1/N)$ and form the threshold sequence $t_k = U + (k-1)/N$ for $k = 1, \\dots, N$. Let $c_j = \\sum_{i=1}^j w_i$ denote the cumulative sum of weights. The resampled ancestor index at position $k$ is defined by $a_k = \\min\\{j \\in \\{1,\\dots,N\\} : c_j \\ge t_k\\}$.\n\nYou are given $N = 10$, the normalized weights\n$$\n(w_1, \\dots, w_{10}) = (0.24, 0.01, 0.01, 0.18, 0.18, 0.20, 0.05, 0.03, 0.05, 0.05),\n$$\nand a fixed $U = 0.07$. Compute the full vector of selection indices $(a_1, \\dots, a_{10})$ according to the systematic resampling definition above.\n\nTo quantify how coherent the neighbor selections are in index space (reflecting the well-known correlation structure of systematic resampling), define the neighbor-coherence index\n$$\n\\kappa = \\frac{1}{N-1} \\sum_{k=1}^{N-1} \\mathbf{1}\\left(|a_{k+1} - a_k| \\le 1\\right),\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function. Provide the final answer as the value of $\\kappa$. Express the final answer as a reduced fraction. In your derivation, connect this notion of neighbor coherence to particle diversity in the presence of weight degeneracy and discuss its implication for the curse of dimensionality in high-dimensional inverse problems. The final answer must be a single number without units and must be expressed as a reduced fraction.", "solution": "The problem is first validated against the required criteria. The problem statement is self-contained, scientifically grounded in the theory of Sequential Monte Carlo methods, and algorithmically well-posed. All necessary data are provided, including the number of particles $N=10$, the normalized weights $\\{w_i\\}$, and the specific value of the random draw $U=0.07$. The sum of the provided weights is indeed $0.24 + 0.01 + 0.01 + 0.18 + 0.18 + 0.20 + 0.05 + 0.03 + 0.05 + 0.05 = 1.00$, confirming their normalization. The value $U=0.07$ is consistent with the required distribution $\\mathcal{U}[0, 1/N) = \\mathcal{U}[0, 0.1)$. The definitions of systematic resampling and the neighbor-coherence index $\\kappa$ are precise and allow for a unique solution. The problem is therefore deemed valid and a full solution is warranted.\n\nThe solution proceeds in three steps: first, we compute the ancestor index vector $(a_1, \\dots, a_{10})$; second, we use this vector to compute the neighbor-coherence index $\\kappa$; finally, we discuss the broader implications of this result.\n\nStep 1: Computation of the ancestor index vector $(a_1, \\dots, a_{10})$.\n\nAccording to the definition of systematic resampling, we must first compute the cumulative sum of weights, $c_j = \\sum_{i=1}^j w_i$, and the sequence of thresholds, $t_k = U + (k-1)/N$.\n\nThe given weights are:\n$$ (w_1, \\dots, w_{10}) = (0.24, 0.01, 0.01, 0.18, 0.18, 0.20, 0.05, 0.03, 0.05, 0.05) $$\nThe cumulative sums $c_j$ are:\n\\begin{align*}\nc_1 &= 0.24 \\\\\nc_2 &= 0.24 + 0.01 = 0.25 \\\\\nc_3 &= 0.25 + 0.01 = 0.26 \\\\\nc_4 &= 0.26 + 0.18 = 0.44 \\\\\nc_5 &= 0.44 + 0.18 = 0.62 \\\\\nc_6 &= 0.62 + 0.20 = 0.82 \\\\\nc_7 &= 0.82 + 0.05 = 0.87 \\\\\nc_8 &= 0.87 + 0.03 = 0.90 \\\\\nc_9 &= 0.90 + 0.05 = 0.95 \\\\\nc_{10} &= 0.95 + 0.05 = 1.00\n\\end{align*}\n\nThe number of particles is $N=10$ and the random draw is $U=0.07$. The threshold sequence $t_k = 0.07 + (k-1)/10$ for $k=1, \\dots, 10$ is:\n\\begin{align*}\nt_1 &= 0.07 + 0.0 = 0.07 \\\\\nt_2 &= 0.07 + 0.1 = 0.17 \\\\\nt_3 &= 0.07 + 0.2 = 0.27 \\\\\nt_4 &= 0.07 + 0.3 = 0.37 \\\\\nt_5 &= 0.07 + 0.4 = 0.47 \\\\\nt_6 &= 0.07 + 0.5 = 0.57 \\\\\nt_7 &= 0.07 + 0.6 = 0.67 \\\\\nt_8 &= 0.07 + 0.7 = 0.77 \\\\\nt_9 &= 0.07 + 0.8 = 0.87 \\\\\nt_{10} &= 0.07 + 0.9 = 0.97\n\\end{align*}\n\nThe ancestor index $a_k$ is the smallest index $j$ such that $c_j \\ge t_k$. We find each $a_k$ by comparing $t_k$ with the sequence of cumulative weights $c_j$:\n\\begin{itemize}\n    \\item For $t_1 = 0.07$: The first cumulative weight $c_1=0.24$ is greater than $0.07$. Thus, $a_1=1$.\n    \\item For $t_2 = 0.17$: The first cumulative weight $c_1=0.24$ is greater than $0.17$. Thus, $a_2=1$.\n    \\item For $t_3 = 0.27$: $c_3=0.26  0.27$, but $c_4=0.44 \\ge 0.27$. Thus, $a_3=4$.\n    \\item For $t_4 = 0.37$: $c_3=0.26  0.37$, but $c_4=0.44 \\ge 0.37$. Thus, $a_4=4$.\n    \\item For $t_5 = 0.47$: $c_4=0.44  0.47$, but $c_5=0.62 \\ge 0.47$. Thus, $a_5=5$.\n    \\item For $t_6 = 0.57$: $c_4=0.44  0.57$, but $c_5=0.62 \\ge 0.57$. Thus, $a_6=5$.\n    \\item For $t_7 = 0.67$: $c_5=0.62  0.67$, but $c_6=0.82 \\ge 0.67$. Thus, $a_7=6$.\n    \\item For $t_8 = 0.77$: $c_5=0.62  0.77$, but $c_6=0.82 \\ge 0.77$. Thus, $a_8=6$.\n    \\item For $t_9 = 0.87$: $c_6=0.82  0.87$, but $c_7=0.87 \\ge 0.87$. Thus, $a_9=7$.\n    \\item For $t_{10} = 0.97$: $c_9=0.95  0.97$, but $c_{10}=1.00 \\ge 0.97$. Thus, $a_{10}=10$.\n\\end{itemize}\nThe full vector of selection indices is $(a_1, \\dots, a_{10}) = (1, 1, 4, 4, 5, 5, 6, 6, 7, 10)$.\n\nStep 2: Computation of the neighbor-coherence index $\\kappa$.\n\nThe index is defined as $\\kappa = \\frac{1}{N-1} \\sum_{k=1}^{N-1} \\mathbf{1}\\left(|a_{k+1} - a_k| \\le 1\\right)$. With $N=10$, we have $N-1=9$. We evaluate the indicator function $\\mathbf{1}(\\cdot)$ for each adjacent pair in the vector $a$:\n\\begin{itemize}\n    \\item $k=1$: $|a_2 - a_1| = |1 - 1| = 0 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=2$: $|a_3 - a_2| = |4 - 1| = 3 > 1 \\implies \\mathbf{1}(\\cdot) = 0$.\n    \\item $k=3$: $|a_4 - a_3| = |4 - 4| = 0 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=4$: $|a_5 - a_4| = |5 - 4| = 1 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=5$: $|a_6 - a_5| = |5 - 5| = 0 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=6$: $|a_7 - a_6| = |6 - 5| = 1 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=7$: $|a_8 - a_7| = |6 - 6| = 0 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=8$: $|a_9 - a_8| = |7 - 6| = 1 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=9$: $|a_{10} - a_9| = |10 - 7| = 3 > 1 \\implies \\mathbf{1}(\\cdot) = 0$.\n\\end{itemize}\nThe sum of the indicator function values is $1 + 0 + 1 + 1 + 1 + 1 + 1 + 1 + 0 = 7$.\nTherefore, the neighbor-coherence index is:\n$$ \\kappa = \\frac{7}{9} $$\n\nStep 3: Discussion.\n\nThe calculated value of $\\kappa = 7/9$ is high, indicating that a majority of adjacent indices in the resampled sequence correspond to ancestors that were either identical or immediate neighbors in the original ordered particle set. This high coherence is a hallmark of systematic resampling. Unlike multinomial resampling where each new particle is drawn independently, systematic resampling fixes the entire selection pattern with a single random draw $U$. The thresholds $t_k$ are equispaced, so they tend to select contiguous blocks of ancestors, preserving local structure.\n\nThis connects directly to the problem of **weight degeneracy**. In SMC, a common issue is the concentration of importance weights on a few particles, leaving the rest with negligible weights. This is **weight degeneracy**. Resampling is the standard remedy, aiming to discard low-weight particles and multiply high-weight particles to focus computational effort on promising regions of the state space. The weights provided, with $w_1=0.24$, $w_4=0.18$, $w_5=0.18$, and $w_6=0.20$, show a moderate degree of degeneracy.\n\nResampling, while necessary, introduces its own problem: a loss of **particle diversity**, as the number of unique ancestors in the resampled set is less than $N$. This impoverishment of the sample is a key limitation of particle methods. The high coherence measured by $\\kappa$ is a manifestation of how systematic resampling manages this trade-off. By preserving neighborhood structures, it avoids the complete randomization of particle locations that could occur with independent resampling, which may be beneficial if the high-weight particles form a \"cluster\" in state space.\n\nThis entire dynamic is severely exacerbated by the **curse of dimensionality**, a critical issue in high-dimensional inverse problems. As the dimension of the state space grows, the volume of the space grows exponentially. The posterior distribution targeted by the SMC sampler typically concentrates in a vanishingly small fraction of this volume. Consequently, a fixed number of random particles $N$ are increasingly unlikely to fall in this high-probability region. This leads to extreme weight degeneracy, where often only one particle has a non-zero weight after the update step. In such a catastrophic collapse, any resampling scheme, including systematic, will select only that single particle $N$ times. The resulting resampled set would have $(a_1, \\dots, a_N) = (j, j, \\dots, j)$ for some index $j$. In this case, $\\kappa$ would be $1$, but this would signify a total loss of diversity, not a healthy preservation of structure. The coherence property of systematic resampling is therefore not a panacea for the curse of dimensionality; it simply reflects the structure of the weights it is given. The fundamental problem lies in the inability of importance sampling to effectively explore high-dimensional spaces, a challenge that requires more advanced techniques beyond simple resampling.", "answer": "$$ \\boxed{\\frac{7}{9}} $$", "id": "3417312"}]}