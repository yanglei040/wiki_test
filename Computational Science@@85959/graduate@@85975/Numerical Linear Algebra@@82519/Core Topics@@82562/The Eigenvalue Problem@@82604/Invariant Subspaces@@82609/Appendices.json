{"hands_on_practices": [{"introduction": "To build a robust understanding of invariant subspaces, it is essential to start with the fundamental definitions. This practice [@problem_id:3551519] challenges you to analytically derive all invariant and reducing subspaces for a given matrix. By working through a small, non-diagonalizable example, you will gain a concrete appreciation for how the structure of a matrix, particularly its generalized eigenspaces, dictates its invariant subspaces.", "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ act on $\\mathbb{R}^{3}$ equipped with the standard Euclidean inner product, where\n$$\nA=\\begin{bmatrix}2&1&0\\\\0&2&0\\\\0&0&3\\end{bmatrix}.\n$$\nA subspace $W \\subseteq \\mathbb{R}^{3}$ is called $A$-invariant if $A W \\subseteq W$. A subspace $W$ is called a reducing subspace for $A$ if both $W$ and its orthogonal complement $W^{\\perp}$ are $A$-invariant; equivalently, $W$ is invariant for both $A$ and the adjoint $A^{\\ast}$ with respect to the given inner product.\n\nStarting only from these definitions and the structure of $A$, derive all $A$-invariant subspaces of $\\mathbb{R}^{3}$ and determine which of these are reducing subspaces. Then compute the ordered pair $(N_{\\mathrm{inv}}, N_{\\mathrm{red}})$, where $N_{\\mathrm{inv}}$ is the total number of $A$-invariant subspaces and $N_{\\mathrm{red}}$ is the total number of reducing subspaces, counting also the trivial subspaces $\\{0\\}$ and $\\mathbb{R}^{3}$. Provide your final answer as this ordered pair. No rounding is required and no units are to be used.", "solution": "The problem asks for the determination and enumeration of all $A$-invariant and reducing subspaces for a given matrix $A \\in \\mathbb{R}^{3 \\times 3}$. Let the standard basis for $\\mathbb{R}^3$ be $\\{e_1, e_2, e_3\\}$, where $e_1 = (1, 0, 0)^T$, $e_2 = (0, 1, 0)^T$, and $e_3 = (0, 0, 1)^T$.\n\nFirst, we analyze the structure of the matrix $A$:\n$$\nA=\\begin{bmatrix}2&1&0\\\\0&2&0\\\\0&0&3\\end{bmatrix}\n$$\nThe characteristic polynomial is $\\det(A - \\lambda I) = (2-\\lambda)^2(3-\\lambda)$. The eigenvalues are $\\lambda_1 = 2$ with algebraic multiplicity $m_1 = 2$, and $\\lambda_2 = 3$ with algebraic multiplicity $m_2 = 1$.\n\nAn invariant subspace for $A$ is a subspace $W \\subseteq \\mathbb{R}^3$ such that $Aw \\in W$ for all $w \\in W$. The theory of linear operators states that any invariant subspace $W$ of $A$ can be decomposed as a direct sum of its intersections with the generalized eigenspaces of $A$. Let $G_{\\lambda}(A) = \\ker((A - \\lambda I)^m)$ be the generalized eigenspace for an eigenvalue $\\lambda$ with algebraic multiplicity $m$. Then $W = (W \\cap G_{2}(A)) \\oplus (W \\cap G_{3}(A))$.\n\nLet's determine the generalized eigenspaces for $A$.\nFor $\\lambda_1 = 2$:\n$A - 2I = \\begin{bmatrix}0&1&0\\\\0&0&0\\\\0&0&1\\end{bmatrix}$. The eigenspace is $E_2 = \\ker(A-2I) = \\text{span}\\{e_1\\}$.\nThe generalized eigenspace is $G_2(A) = \\ker((A-2I)^2)$.\n$$\n(A-2I)^2 = \\begin{bmatrix}0&1&0\\\\0&0&0\\\\0&0&1\\end{bmatrix}\\begin{bmatrix}0&1&0\\\\0&0&0\\\\0&0&1\\end{bmatrix} = \\begin{bmatrix}0&0&0\\\\0&0&0\\\\0&0&1\\end{bmatrix}\n$$\nThe null space of $(A-2I)^2$ is the set of vectors $(x, y, z)^T$ where $z=0$. Thus, $G_2(A) = \\text{span}\\{e_1, e_2\\}$.\n\nFor $\\lambda_2 = 3$:\nThe algebraic multiplicity is $1$, so the generalized eigenspace is the same as the eigenspace.\n$A - 3I = \\begin{bmatrix}-1&1&0\\\\0&-1&0\\\\0&0&0\\end{bmatrix}$.\nThe null space is given by $-x+y=0$ and $-y=0$, which implies $x=y=0$. Thus, $G_3(A) = E_3 = \\ker(A-3I) = \\text{span}\\{e_3\\}$.\n\nNow, we find the invariant subspaces of the restrictions of $A$ to its generalized eigenspaces.\n1. Invariant subspaces of $A$ contained in $G_3(A) = \\text{span}\\{e_3\\}$: Since this is a $1$-dimensional eigenspace, the only invariant subspaces are $\\{0\\}$ and $G_3(A)$ itself. There are $2$ such subspaces.\n\n2. Invariant subspaces of $A$ contained in $G_2(A) = \\text{span}\\{e_1, e_2\\}$: The restriction of $A$ to $G_2(A)$ has the matrix $A_2 = \\begin{bmatrix}2&1\\\\0&2\\end{bmatrix}$ in the basis $\\{e_1, e_2\\}$. The invariant subspaces of this operator are:\n   - The $0$-dimensional subspace: $\\{0\\}$.\n   - Any $1$-dimensional invariant subspace must be an eigenspace. The only eigenvalue is $2$, and the corresponding eigenspace is $\\ker(A_2-2I) = \\ker(\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}) = \\text{span}\\{e_1\\}$.\n   - The $2$-dimensional subspace: $G_2(A)$ itself.\n   Thus, there are $3$ invariant subspaces contained in $G_2(A)$.\n\nAn arbitrary $A$-invariant subspace $W$ is a direct sum of an invariant subspace from $G_2(A)$ and one from $G_3(A)$. The total number of $A$-invariant subspaces is the product of the number of such subspaces in each generalized eigenspace.\n$N_{\\mathrm{inv}} = 3 \\times 2 = 6$.\nThe $A$-invariant subspaces are:\n\\begin{enumerate}\n    \\item $\\{0\\} \\oplus \\{0\\} = \\{0\\}$\n    \\item $\\{0\\} \\oplus \\text{span}\\{e_3\\} = \\text{span}\\{e_3\\}$\n    \\item $\\text{span}\\{e_1\\} \\oplus \\{0\\} = \\text{span}\\{e_1\\}$\n    \\item $\\text{span}\\{e_1\\} \\oplus \\text{span}\\{e_3\\} = \\text{span}\\{e_1, e_3\\}$\n    \\item $\\text{span}\\{e_1, e_2\\} \\oplus \\{0\\} = \\text{span}\\{e_1, e_2\\}$\n    \\item $\\text{span}\\{e_1, e_2\\} \\oplus \\text{span}\\{e_3\\} = \\mathbb{R}^3$\n\\end{enumerate}\n\nNext, we identify the reducing subspaces. A subspace $W$ is reducing for $A$ if it is invariant under both $A$ and its adjoint $A^*$. Since we are using the standard Euclidean inner product, $A^* = A^T$.\n$$\nA^T = \\begin{bmatrix}2&0&0\\\\1&2&0\\\\0&0&3\\end{bmatrix}\n$$\nA subspace is reducing if and only if it is in the intersection of the set of $A$-invariant subspaces and the set of $A^T$-invariant subspaces. We proceed to find the invariant subspaces for $A^T$. The eigenvalues of $A^T$ are the same as for $A$: $\\lambda_1 = 2$ and $\\lambda_2 = 3$.\n\nThe generalized eigenspaces for $A^T$ are:\nFor $\\lambda_1=2$: $A^T - 2I = \\begin{bmatrix}0&0&0\\\\1&0&0\\\\0&0&1\\end{bmatrix}$. The eigenspace is $E'_2 = \\ker(A^T-2I) = \\text{span}\\{e_2\\}$.\nThe generalized eigenspace $G_2(A^T) = \\ker((A^T-2I)^2)$.\n$$\n(A^T-2I)^2 = \\begin{bmatrix}0&0&0\\\\0&0&0\\\\0&0&1\\end{bmatrix}\n$$\nThe null space is where $z=0$, so $G_2(A^T) = \\text{span}\\{e_1, e_2\\}$.\n\nFor $\\lambda_2=3$: $G_3(A^T) = \\ker(A^T-3I) = \\text{span}\\{e_3\\}$.\n\nThe invariant subspaces of $A^T$ are constructed similarly.\n1. Invariant subspaces within $G_3(A^T) = \\text{span}\\{e_3\\}$ are $\\{0\\}$ and $\\text{span}\\{e_3\\}$. ($2$ subspaces)\n2. Invariant subspaces within $G_2(A^T) = \\text{span}\\{e_1, e_2\\}$: The restriction of $A^T$ to this space has matrix $A^T_2 = \\begin{bmatrix}2&0\\\\1&2\\end{bmatrix}$.\n   - $\\{0\\}$\n   - The $1$-dimensional invariant subspace (eigenspace) is $\\ker(A^T_2-2I) = \\ker(\\begin{bmatrix}0&0\\\\1&0\\end{bmatrix}) = \\text{span}\\{e_2\\}$.\n   - $G_2(A^T)$ itself.\n   There are $3$ such subspaces.\n\nThe $A^T$-invariant subspaces are:\n\\begin{enumerate}\n    \\item $\\{0\\}$\n    \\item $\\text{span}\\{e_3\\}$\n    \\item $\\text{span}\\{e_2\\}$\n    \\item $\\text{span}\\{e_2, e_3\\}$\n    \\item $\\text{span}\\{e_1, e_2\\}$\n    \\item $\\mathbb{R}^3$\n\\end{enumerate}\n\nThe reducing subspaces for $A$ are those common to both lists of invariant subspaces.\nComparing the two lists:\n- $\\{0\\}$ is common.\n- $\\text{span}\\{e_3\\}$ is common.\n- $\\text{span}\\{e_1\\}$ is only $A$-invariant.\n- $\\text{span}\\{e_1, e_3\\}$ is only $A$-invariant.\n- $\\text{span}\\{e_1, e_2\\}$ is common.\n- $\\mathbb{R}^3$ is common.\n- $\\text{span}\\{e_2\\}$ is only $A^T$-invariant.\n- $\\text{span}\\{e_2, e_3\\}$ is only $A^T$-invariant.\n\nThe reducing subspaces are $\\{0\\}$, $\\text{span}\\{e_3\\}$, $\\text{span}\\{e_1, e_2\\}$, and $\\mathbb{R}^3$.\nThere are $N_{\\mathrm{red}} = 4$ reducing subspaces.\n\nAn equivalent definition of a reducing subspace $W$ is that both $W$ and its orthogonal complement $W^\\perp$ are $A$-invariant.\n- For $W = \\{0\\}$, $W^\\perp = \\mathbb{R}^3$. Both are $A$-invariant.\n- For $W = \\mathbb{R}^3$, $W^\\perp = \\{0\\}$. Both are $A$-invariant.\n- For $W = \\text{span}\\{e_3\\}$, $W^\\perp = \\text{span}\\{e_1, e_2\\}$. Both are in the list of $A$-invariant subspaces.\n- For $W = \\text{span}\\{e_1, e_2\\}$, $W^\\perp = \\text{span}\\{e_3\\}$. Both are in the list of $A$-invariant subspaces.\nThis confirms the four reducing subspaces.\n\nThe total number of $A$-invariant subspaces is $N_{\\mathrm{inv}} = 6$. The total number of reducing subspaces is $N_{\\mathrm{red}} = 4$.\nThe required ordered pair is $(N_{\\mathrm{inv}}, N_{\\mathrm{red}})$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n6 & 4\n\\end{pmatrix}\n}\n$$", "id": "3551519"}, {"introduction": "While analytical enumeration is feasible for small matrices, practical problems require algorithmic approaches. This practice [@problem_id:3551503] introduces the Arnoldi iteration as a powerful method for discovering invariant subspaces by constructing Krylov subspaces. You will implement this fundamental algorithm and use the termination condition, $h_{m+1,m} \\approx 0$, to numerically detect the smallest invariant Krylov subspace for a variety of matrices.", "problem": "You are given square real matrices $A \\in \\mathbb{R}^{n \\times n}$ and nonzero vectors $b \\in \\mathbb{R}^n$. Let the Krylov subspace of order $m$ be defined by\n$$\n\\mathcal{K}_m(A,b) \\equiv \\operatorname{span}\\{b, Ab, A^2 b, \\dots, A^{m-1} b\\}.\n$$\nA subspace $\\mathcal{S} \\subseteq \\mathbb{R}^n$ is called invariant under $A$ if $A \\mathcal{S} \\subseteq \\mathcal{S}$. Your task is to compute, for each given pair $(A,b)$, the smallest positive integer $m$ for which $\\mathcal{K}_m(A,b)$ is invariant under $A$, and to verify this invariance numerically via the residual of the Arnoldi relation.\n\nBase your reasoning only on the following fundamental definitions and well-tested facts:\n- The Krylov subspace $\\mathcal{K}_m(A,b)$ is built by repeatedly applying $A$ to $b$, and its dimension is at most $m$ and at most $n$.\n- The Arnoldi process constructs an orthonormal basis $V_m = [v_1,\\dots,v_m]$ for $\\mathcal{K}_m(A,b)$ (with $v_1 = b/\\lVert b\\rVert_2$) and an upper Hessenberg matrix $H_m \\in \\mathbb{R}^{m \\times m}$ satisfying the Arnoldi relation\n$$\nA V_m = V_m H_m + h_{m+1,m} v_{m+1} e_m^\\top,\n$$\nwhere $e_m \\in \\mathbb{R}^m$ is the $m$-th standard basis vector and $h_{m+1,m} \\ge 0$. In exact arithmetic, $h_{m+1,m} = 0$ if and only if $\\mathcal{K}_m(A,b)$ is invariant under $A$.\n- Numerical computations in finite precision can only establish the equality $h_{m+1,m} = 0$ up to a tolerance.\n\nDesign an algorithm that, given $(A,b)$ and a tolerance $\\tau > 0$, computes the smallest $m$ such that the Arnoldi process satisfies $h_{m+1,m} \\le \\tau$. Interpret this as numerical detection of invariance. Then, verify the invariance by computing the Frobenius norm of the Arnoldi residual\n$$\nR_m \\equiv A V_m - V_m H_m,\n$$\nwhich should be numerically small when $h_{m+1,m}$ is numerically zero. Use Modified Gram–Schmidt with a second reorthogonalization pass to improve numerical stability. Use the Euclidean norm for vector normalization and the Frobenius norm for matrices.\n\nImplement your algorithm as a complete program that solves the following test suite. For each case, use a tolerance $\\tau = 10^{-12}$:\n\n- Case 1 (Diagonalizable with multiple active eigenvalues): Let\n$$\nA_1 = \\operatorname{diag}(1,2,4,7,11), \\quad b_1 = [1, \\, 1, \\, 0, \\, 1, \\, 0]^\\top.\n$$\n- Case 2 (Single Jordan block, non-diagonalizable, full chain): Let $A_2 \\in \\mathbb{R}^{4 \\times 4}$ be the Jordan block with eigenvalue $2$, that is, $(A_2)_{ii} = 2$ and $(A_2)_{i,i+1} = 1$ for $i=1,2,3$, and zeros elsewhere. Let\n$$\nb_2 = [1, \\, 0, \\, 0, \\, 0]^\\top.\n$$\n- Case 3 (Eigenvector input, immediate invariance): Let\n$$\nA_3 = \\begin{bmatrix}\n3 & 1 & 0 & 0 \\\\\n0 & 5 & 2 & 0 \\\\\n0 & 0 & 7 & 0 \\\\\n0 & 0 & 0 & 9\n\\end{bmatrix}, \\quad b_3 = [0, \\, 0, \\, 1, \\, 0]^\\top.\n$$\n- Case 4 (Block diagonal with a nilpotent block): Let\n$$\nA_4 = \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 5 & 0 \\\\\n0 & 0 & 0 & 6\n\\end{bmatrix}, \\quad b_4 = [1, \\, 0, \\, 0, \\, 0]^\\top.\n$$\n\nFor each case, your program must:\n- Run Arnoldi until the first index $m$ such that $h_{m+1,m} \\le \\tau$ is encountered (this is the smallest $m$ declaring numerical invariance).\n- Compute the Frobenius norm $\\lVert R_m \\rVert_F$ to verify the Arnoldi relation and thus the invariance.\n- Return the triple consisting of the integer $m$, the scalar $h_{m+1,m}$, and the scalar $\\lVert R_m \\rVert_F$. Round the two scalar outputs to $12$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a three-element list $[m, h, r]$ corresponding to one test case in the order given above. For example, an output with four cases should look like\n$$\n[[m_1,h_1,r_1],[m_2,h_2,r_2],[m_3,h_3,r_3],[m_4,h_4,r_4]].\n$$\nNo additional text should be printed.", "solution": "The problem requires us to determine the smallest positive integer $m$ for which the Krylov subspace $\\mathcal{K}_m(A,b) = \\operatorname{span}\\{b, Ab, \\dots, A^{m-1} b\\}$ is an invariant subspace under a given real matrix $A \\in \\mathbb{R}^{n \\times n}$ for a given non-zero vector $b \\in \\mathbb{R}^n$. A subspace $\\mathcal{S}$ is invariant under $A$ if for any vector $s \\in \\mathcal{S}$, the vector $As$ is also in $\\mathcal{S}$, which is denoted as $A\\mathcal{S} \\subseteq \\mathcal{S}$.\n\nThe Arnoldi iteration is a direct method for constructing an orthonormal basis for $\\mathcal{K}_m(A,b)$ and provides a criterion for detecting invariance. The process generates a sequence of orthonormal vectors $\\{v_1, v_2, \\dots, v_m\\}$ that form a basis for $\\mathcal{K}_m(A,b)$, where $v_1 = b / \\lVert b \\rVert_2$. These vectors are stored as columns of a matrix $V_m = [v_1, v_2, \\dots, v_m] \\in \\mathbb{R}^{n \\times m}$. The algorithm also produces an upper Hessenberg matrix $H_m \\in \\mathbb{R}^{m \\times m}$. These matrices are related by the Arnoldi relation:\n$$\nA V_m = V_m H_m + h_{m+1,m} v_{m+1} e_m^\\top\n$$\nwhere $v_{m+1}$ is a unit vector orthogonal to the columns of $V_m$, $e_m \\in \\mathbb{R}^m$ is the $m$-th standard basis vector, and $h_{m+1,m} = \\lVert (I - V_m V_m^\\top) A v_m \\rVert_2 \\ge 0$.\n\nThe subspace $\\mathcal{K}_m(A,b)$ is invariant under $A$ if and only if $A v_j \\in \\mathcal{K}_m(A,b)$ for all $j \\in \\{1, \\dots, m\\}$. Due to the structure of the Arnoldi process, this condition simplifies to checking if $A v_m \\in \\mathcal{K}_m(A,b)$. The vector $(I - V_m V_m^\\top) A v_m$ represents the component of $A v_m$ that is orthogonal to $\\mathcal{K}_m(A,b)$. Therefore, $A v_m \\in \\mathcal{K}_m(A,b)$ if and only if this orthogonal component is the zero vector. The norm of this vector is precisely $h_{m+1,m}$. Consequently, in exact arithmetic, the Krylov subspace $\\mathcal{K}_m(A,b)$ is invariant under $A$ if and only if $h_{m+1,m}=0$. This marks a \"breakdown\" of the Arnoldi process, as a new basis vector $v_{m+1}$ cannot be generated. The smallest such $m$ corresponds to the degree of the minimal polynomial of the vector $b$ with respect to the matrix $A$.\n\nIn finite-precision arithmetic, we check for numerical invariance by testing if $h_{m+1,m}$ is smaller than a given tolerance $\\tau > 0$. The task is to find the smallest positive integer $m$ satisfying $h_{m+1,m} \\le \\tau$.\n\nThe algorithm to be implemented is the Arnoldi iteration with Modified Gram-Schmidt (MGS) for orthogonalization. To mitigate the loss of orthogonality that can occur in MGS, a second reorthogonalization step is included.\n\nThe algorithm proceeds as follows:\n1.  Initialize: Given $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^n$, and tolerance $\\tau > 0$. Let $n$ be the dimension of the space. Pre-allocate matrices $V \\in \\mathbb{R}^{n \\times (n+1)}$ and $H \\in \\mathbb{R}^{(n+1) \\times n}$.\n2.  Start: Compute the first basis vector $v_1 = b / \\lVert b \\rVert_2$. Store it as the first column of $V$.\n3.  Iterate for $m = 1, 2, \\dots, n$:\n    a. Let $v_m$ be the $m$-th basis vector (the $m$-th column of $V$). Compute the new vector $w = A v_m$.\n    b. Orthogonalize $w$ against the existing basis $\\{v_1, \\dots, v_m\\}$. This is the first pass of MGS. For $j = 1, \\dots, m$:\n       i.  Compute the projection coefficient: $h_{j,m} = v_j^\\top w$. Store this in the matrix $H$.\n       ii. Subtract the projection from $w$: $w \\leftarrow w - h_{j,m} v_j$.\n    c. Reorthogonalize for numerical stability. This is the second pass. For $j = 1, \\dots, m$:\n       i.  Compute the correction term: $\\delta h = v_j^\\top w$.\n       ii. Update the Hessenberg entry: $h_{j,m} \\leftarrow h_{j,m} + \\delta h$.\n       iii. Update the vector: $w \\leftarrow w - \\delta h v_j$.\n    d. Compute the norm of the resulting vector $w$, which is $h_{m+1,m} = \\lVert w \\rVert_2$. Store this in $H$.\n    e. Check for termination: If $h_{m+1,m} \\le \\tau$, the process terminates. The smallest dimension for which the subspace is numerically invariant is $m$. Break the loop.\n    f. If the process has not terminated, normalize to get the next basis vector: $v_{m+1} = w / h_{m+1,m}$. Store it as the $(m+1)$-th column of $V$.\n\nUpon termination at step $m$, we have the orthonormal basis $V_m = [v_1, \\dots, v_m]$ and the upper Hessenberg matrix $H_m \\in \\mathbb{R}^{m \\times m}$. The residual of the Arnoldi relation is the matrix $R_m = A V_m - V_m H_m$. From the relation, $R_m = h_{m+1,m} v_{m+1} e_m^\\top$. To verify the numerical invariance and correctness of the implementation, we compute the Frobenius norm $\\lVert R_m \\rVert_F$. Theoretically, we have $\\lVert R_m \\rVert_F = \\lVert h_{m+1,m} v_{m+1} e_m^\\top \\rVert_F = h_{m+1,m} \\lVert v_{m+1} e_m^\\top \\rVert_F = h_{m+1,m}$. Computing $\\lVert A V_m - V_m H_m \\rVert_F$ directly serves as a robust numerical check.\n\nThe final output for each test case will be the triplet $(m, h_{m+1,m}, \\lVert R_m \\rVert_F)$, where $m$ is the dimension of the invariant subspace, $h_{m+1,m}$ is the value that triggered termination, and $\\lVert R_m \\rVert_F$ is the computed Frobenius norm of the residual.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef arnoldi_invariance(A, b, tol=1e-12):\n    \"\"\"\n    Computes the smallest m for which the Krylov subspace K_m(A,b) is invariant.\n\n    Args:\n        A (np.ndarray): The square matrix of size n x n.\n        b (np.ndarray): The starting vector of size n.\n        tol (float): The tolerance for h_{m+1,m} to be considered zero.\n\n    Returns:\n        tuple: A triplet (m, h, r) where:\n            m (int): The smallest dimension of the invariant Krylov subspace.\n            h (float): The value of h_{m+1,m} at termination.\n            r (float): The Frobenius norm of the residual matrix R_m = A V_m - V_m H_m.\n    \"\"\"\n    n = A.shape[0]\n    V = np.zeros((n, n + 1), dtype=float)\n    H = np.zeros((n + 1, n), dtype=float)\n\n    # Step 1: Initialize\n    V[:, 0] = b / np.linalg.norm(b)\n\n    for m in range(n):\n        # Step 2: Generate new vector\n        v_m = V[:, m]\n        w = A @ v_m\n\n        # Step 3: Modified Gram-Schmidt with reorthogonalization\n        # First pass (MGS)\n        for j in range(m + 1):\n            h_jm = np.dot(V[:, j], w)\n            w -= h_jm * V[:, j]\n            H[j, m] = h_jm\n\n        # Second reorthogonalization pass for stability\n        for j in range(m + 1):\n            h_corr = np.dot(V[:, j], w)\n            w -= h_corr * V[:, j]\n            H[j, m] += h_corr\n        \n        # Step 4: Compute h_{m+1,m} and check for invariance\n        h_next = np.linalg.norm(w)\n        H[m+1, m] = h_next\n\n        # Termination condition\n        if h_next = tol:\n            m_final = m + 1\n            Vm = V[:, :m_final]\n            Hm = H[:m_final, :m_final]\n            \n            # Verification: Compute residual norm\n            Rm = A @ Vm - Vm @ Hm\n            res_norm = np.linalg.norm(Rm, 'fro')\n            \n            return m_final, h_next, res_norm\n\n        # Step 5: Normalize to get next basis vector\n        V[:, m+1] = w / h_next\n\n    # If loop completes, the space is the full R^n\n    m_final = n\n    Vm = V[:, :n]\n    Hm = H[:n, :n]\n    Rm = A @ Vm - Vm @ Hm\n    res_norm = np.linalg.norm(Rm, 'fro')\n    h_final = H[n, n-1]\n\n    return m_final, h_final, res_norm\n\n\ndef solve():\n    \"\"\"\n    Defines and solves the test suite of problems.\n    \"\"\"\n    tol = 1e-12\n\n    # Case 1\n    A1 = np.diag([1, 2, 4, 7, 11])\n    b1 = np.array([1, 1, 0, 1, 0], dtype=float)\n\n    # Case 2\n    A2 = np.diag([2.0]*4) + np.diag([1.0]*3, k=1)\n    b2 = np.array([1, 0, 0, 0], dtype=float)\n\n    # Case 3\n    A3 = np.array([\n        [3, 1, 0, 0],\n        [0, 5, 2, 0],\n        [0, 0, 7, 0],\n        [0, 0, 0, 9]\n    ], dtype=float)\n    b3 = np.array([0, 0, 1, 0], dtype=float)\n    \n    # Case 4\n    A4 = np.array([\n        [0, 1, 0, 0],\n        [0, 0, 0, 0],\n        [0, 0, 5, 0],\n        [0, 0, 0, 6]\n    ], dtype=float)\n    b4 = np.array([1, 0, 0, 0], dtype=float)\n    \n    test_cases = [\n        (A1, b1),\n        (A2, b2),\n        (A3, b3),\n        (A4, b4),\n    ]\n\n    results = []\n    for A, b in test_cases:\n        m, h, r = arnoldi_invariance(A, b, tol)\n        # Format h and r to 12 decimal places as strings, m is an integer\n        h_str = f\"{h:.12f}\"\n        r_str = f\"{r:.12f}\"\n        results.append(f\"[{m},{h_str},{r_str}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3551503"}, {"introduction": "Numerical algorithms operating in finite precision rarely produce exact invariant subspaces; instead, they yield approximations. This practice [@problem_id:3551536] delves into the essential task of quantifying the quality of such an approximation. By computing the principal angles between an exact subspace and a computed one, you will explore the fundamental relationship between the subspace residual norm and the accuracy of the approximation, a cornerstone of perturbation theory for eigenvalue problems.", "problem": "Let $A \\in \\mathbb{R}^{4 \\times 4}$ be a real symmetric matrix given by\n$$\nA = \\operatorname{diag}(8,\\,7,\\,0,\\,-1).\n$$\nConsider the exact invariant subspace $\\mathcal{U}$ of $A$ associated with the eigenvalues $8$ and $7$, with an orthonormal basis\n$$\nQ = \\begin{bmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0 \\\\\n0  0\n\\end{bmatrix},\n$$\nand a numerically computed two-dimensional subspace $\\widehat{\\mathcal{U}}$ with orthonormal basis\n$$\n\\widehat{Q} = \\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}}\n\\end{bmatrix},\n$$\nwhose columns are $u_{1} = \\frac{1}{\\sqrt{2}}(e_{1} + e_{3})$ and $u_{2} = \\frac{1}{\\sqrt{2}}(e_{2} + e_{4})$, where $\\{e_{i}\\}_{i=1}^{4}$ is the standard basis of $\\mathbb{R}^{4}$.\n\nUsing the foundational definitions of principal angles between subspaces and the invariant subspace concept for symmetric matrices, perform the following:\n\n- Compute the principal angles between $\\mathcal{U}$ and $\\widehat{\\mathcal{U}}$ from their orthonormal bases.\n- Form the Rayleigh–Ritz projected matrix $\\widehat{T} = \\widehat{Q}^{\\top} A \\widehat{Q}$ and the subspace residual $R = A\\widehat{Q} - \\widehat{Q}\\widehat{T}$, and compute $\\|R\\|_{2}$.\n- Let $\\delta$ denote the minimal spectral separation between the eigenvalues of $\\widehat{T}$ and those of $A$ restricted to the orthogonal complement of $\\mathcal{U}$ (that is, the eigenvalues corresponding to the invariant subspace spanned by $\\operatorname{span}\\{e_{3}, e_{4}\\}$). Relate the largest principal angle to $\\|R\\|_{2}$ and $\\delta$ using well-tested bounds for symmetric problems.\n\nExpress the largest principal angle in radians, and round your answer to four significant figures.", "solution": "The problem asks for three main tasks: computing principal angles, computing a Rayleigh-Ritz projection and its residual norm, and relating these quantities via a known theoretical bound.\n\nFirst, we compute the principal angles between the exact invariant subspace $\\mathcal{U}$ and the computed subspace $\\widehat{\\mathcal{U}}$. The principal angles $\\theta_k$ between two subspaces with orthonormal bases $Q$ and $\\widehat{Q}$ are defined by $\\cos(\\theta_k) = \\sigma_k(Q^{\\top}\\widehat{Q})$, where $\\sigma_k$ are the singular values of the matrix $Q^{\\top}\\widehat{Q}$ sorted in descending order.\n\nThe given orthonormal bases are:\n$$\nQ = \\begin{bmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0 \\\\\n0  0\n\\end{bmatrix} \\quad \\text{and} \\quad \\widehat{Q} = \\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n$$\nThe transpose of $Q$ is $Q^{\\top} = \\begin{bmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\end{bmatrix}$. We compute the product $M = Q^{\\top}\\widehat{Q}$:\n$$\nM = Q^{\\top}\\widehat{Q} = \\begin{bmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\end{bmatrix} \\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}}\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n$$\nThe singular values of a diagonal matrix are the absolute values of its diagonal entries. The matrix $M$ is diagonal, so its singular values are $\\sigma_1 = \\frac{1}{\\sqrt{2}}$ and $\\sigma_2 = \\frac{1}{\\sqrt{2}}$.\nThe principal angles are therefore:\n$$\n\\theta_1 = \\arccos\\left(\\frac{1}{\\sqrt{2}}\\right) = \\frac{\\pi}{4} \\text{ radians}\n$$\n$$\n\\theta_2 = \\arccos\\left(\\frac{1}{\\sqrt{2}}\\right) = \\frac{\\pi}{4} \\text{ radians}\n$$\nThe largest principal angle is $\\theta_{\\max} = \\frac{\\pi}{4}$.\n\nNext, we address the second task. We form the Rayleigh–Ritz projected matrix $\\widehat{T} = \\widehat{Q}^{\\top} A \\widehat{Q}$. The matrix $A$ is given as $A = \\operatorname{diag}(8, 7, 0, -1)$.\nFirst, we compute the product $A\\widehat{Q}$:\n$$\nA\\widehat{Q} = \\begin{bmatrix}\n8  0  0  0 \\\\\n0  7  0  0 \\\\\n0  0  0  0 \\\\\n0  0  0  -1\n\\end{bmatrix} \\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}}\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{8}{\\sqrt{2}}  0 \\\\\n0  \\frac{7}{\\sqrt{2}} \\\\\n0  0 \\\\\n0  -\\frac{1}{\\sqrt{2}}\n\\end{bmatrix}\n$$\nNow, we compute $\\widehat{T}$ by pre-multiplying by $\\widehat{Q}^{\\top}$:\n$$\n\\widehat{T} = \\widehat{Q}^{\\top}(A\\widehat{Q}) = \\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}}\n\\end{bmatrix} \\begin{bmatrix}\n\\frac{8}{\\sqrt{2}}  0 \\\\\n0  \\frac{7}{\\sqrt{2}} \\\\\n0  0 \\\\\n0  -\\frac{1}{\\sqrt{2}}\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{8}{2}  0 \\\\\n0  \\frac{7}{2} - \\frac{1}{2}\n\\end{bmatrix} = \\begin{bmatrix}\n4  0 \\\\\n0  3\n\\end{bmatrix}\n$$\nSo, $\\widehat{T} = \\operatorname{diag}(4, 3)$. The eigenvalues of $\\widehat{T}$, known as the Ritz values, are $\\widehat{\\lambda}_1 = 4$ and $\\widehat{\\lambda}_2 = 3$.\n\nNext, we compute the subspace residual matrix $R = A\\widehat{Q} - \\widehat{Q}\\widehat{T}$ and its $2$-norm, $\\|R\\|_{2}$.\n$$\n\\widehat{Q}\\widehat{T} = \\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}}\n\\end{bmatrix} \\begin{bmatrix}\n4  0 \\\\\n0  3\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{4}{\\sqrt{2}}  0 \\\\\n0  \\frac{3}{\\sqrt{2}} \\\\\n\\frac{4}{\\sqrt{2}}  0 \\\\\n0  \\frac{3}{\\sqrt{2}}\n\\end{bmatrix}\n$$\n$$\nR = A\\widehat{Q} - \\widehat{Q}\\widehat{T} = \\begin{bmatrix}\n\\frac{8}{\\sqrt{2}}  0 \\\\\n0  \\frac{7}{\\sqrt{2}} \\\\\n0  0 \\\\\n0  -\\frac{1}{\\sqrt{2}}\n\\end{bmatrix} - \\begin{bmatrix}\n\\frac{4}{\\sqrt{2}}  0 \\\\\n0  \\frac{3}{\\sqrt{2}} \\\\\n\\frac{4}{\\sqrt{2}}  0 \\\\\n0  \\frac{3}{\\sqrt{2}}\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{4}{\\sqrt{2}}  0 \\\\\n0  \\frac{4}{\\sqrt{2}} \\\\\n-\\frac{4}{\\sqrt{2}}  0 \\\\\n0  -\\frac{4}{\\sqrt{2}}\n\\end{bmatrix} = 2\\sqrt{2} \\begin{bmatrix}\n1  0 \\\\\n0  1 \\\\\n-1  0 \\\\\n0  -1\n\\end{bmatrix}\n$$\nThe $2$-norm of a matrix is its largest singular value. The singular values of $R$ are the square roots of the eigenvalues of $R^{\\top}R$.\n$$\nR^{\\top}R = (2\\sqrt{2})^2 \\begin{bmatrix}\n1  0  -1  0 \\\\\n0  1  0  -1\n\\end{bmatrix} \\begin{bmatrix}\n1  0 \\\\\n0  1 \\\\\n-1  0 \\\\\n0  -1\n\\end{bmatrix} = 8 \\begin{bmatrix}\n2  0 \\\\\n0  2\n\\end{bmatrix} = \\begin{bmatrix}\n16  0 \\\\\n0  16\n\\end{bmatrix}\n$$\nThe eigenvalues of $R^{\\top}R$ are $16$ and $16$. The singular values of $R$ are $\\sqrt{16}=4$ and $\\sqrt{16}=4$. The largest singular value is $4$. Therefore, $\\|R\\|_{2} = 4$.\n\nFinally, for the third task, we relate the largest principal angle to $\\|R\\|_{2}$ and the spectral separation $\\delta$. The Davis-Kahan $\\sin\\Theta$ theorem provides bounds for this relationship. For a symmetric matrix $A$, one such bound is:\n$$\n\\|\\sin\\Theta(\\mathcal{U}, \\widehat{\\mathcal{U}})\\|_{2} \\le \\frac{\\|R\\|_{2}}{\\delta}\n$$\nwhere $\\|\\sin\\Theta(\\mathcal{U}, \\widehat{\\mathcal{U}})\\|_{2} = \\sin(\\theta_{\\max})$. The quantity $\\delta$ is defined as the minimal spectral separation between the eigenvalues of $\\widehat{T}$ (the Ritz values) and the eigenvalues of $A$ restricted to $\\mathcal{U}^{\\perp}$, the orthogonal complement of the exact invariant subspace $\\mathcal{U}$.\n\nThe eigenvalues of $A$ are $\\lambda(A) = \\{8, 7, 0, -1\\}$. The subspace $\\mathcal{U}$ is associated with the eigenvalues $\\{8, 7\\}$. Thus, its orthogonal complement $\\mathcal{U}^{\\perp}$ is associated with the eigenvalues $\\{0, -1\\}$. Let this set be $\\Lambda_{A, \\perp} = \\{0, -1\\}$. The eigenvalues of $\\widehat{T}$ are the Ritz values, $\\Lambda_{\\widehat{T}} = \\{4, 3\\}$.\nThe spectral separation $\\delta$ is given by:\n$$\n\\delta = \\min_{\\widehat{\\lambda} \\in \\Lambda_{\\widehat{T}}, \\lambda \\in \\Lambda_{A, \\perp}} |\\widehat{\\lambda} - \\lambda|\n$$\nWe compute the absolute differences:\n$|4 - 0| = 4$\n$|4 - (-1)| = 5$\n$|3 - 0| = 3$\n$|3 - (-1)| = 4$\nThe minimum of these values is $\\delta = 3$.\n\nWe can now verify the bound:\n$$\n\\sin(\\theta_{\\max}) \\le \\frac{\\|R\\|_{2}}{\\delta}\n$$\nSubstituting the computed values: $\\theta_{\\max} = \\frac{\\pi}{4}$, $\\|R\\|_{2} = 4$, and $\\delta = 3$.\n$$\n\\sin\\left(\\frac{\\pi}{4}\\right) \\le \\frac{4}{3}\n$$\n$$\n\\frac{1}{\\sqrt{2}} \\le \\frac{4}{3}\n$$\nNumerically, $\\frac{1}{\\sqrt{2}} \\approx 0.7071$ and $\\frac{4}{3} \\approx 1.3333$. The inequality $0.7071 \\le 1.3333$ holds, confirming the theoretical bound for this specific case.\n\nThe problem asks for the value of the largest principal angle in radians, rounded to four significant figures.\n$$\n\\theta_{\\max} = \\frac{\\pi}{4} \\approx 0.78539816...\n$$\nRounding to four significant figures, we get $0.7854$.", "answer": "$$\n\\boxed{0.7854}\n$$", "id": "3551536"}]}