{"hands_on_practices": [{"introduction": "Before analyzing any Markov chain Monte Carlo (MCMC) output, we must ensure the chain has \"forgotten\" its starting point and converged to its stationary distribution. This initial phase is known as the burn-in period. This first exercise provides a rigorous foundation for this concept, moving beyond qualitative visual inspection. By deriving a burn-in criterion from first principles for a linear Gaussian model, you will gain a quantitative understanding of convergence speed and how it is influenced by the chain's starting position, a critical insight for designing efficient sampling strategies [@problem_id:3370153].", "problem": "Consider the linear Gaussian inverse problem in which the unknown state vector $x \\in \\mathbb{R}^{n}$ is related to data $y \\in \\mathbb{R}^{m}$ by the forward model $y = G x + \\varepsilon$, where $G \\in \\mathbb{R}^{m \\times n}$ is known and the observational noise $\\varepsilon$ is Gaussian with zero mean and positive-definite covariance $\\Gamma \\in \\mathbb{R}^{m \\times m}$. Assume a Gaussian prior $x \\sim \\mathcal{N}(m_{0}, C_{0})$ with mean $m_{0} \\in \\mathbb{R}^{n}$ and symmetric positive-definite covariance $C_{0} \\in \\mathbb{R}^{n \\times n}$. It is a well-tested and standard fact that the posterior distribution is Gaussian with mean $m_{\\mathrm{post}}$ and covariance $C_{\\mathrm{post}}$ given by the unique solution of the linear system based on the normal equations:\n$$\nC_{\\mathrm{post}}^{-1} = C_{0}^{-1} + G^{\\top}\\Gamma^{-1}G, \n\\quad \nm_{\\mathrm{post}} = C_{\\mathrm{post}}\\left(C_{0}^{-1} m_{0} + G^{\\top}\\Gamma^{-1}y\\right).\n$$\n\nTo sample from the posterior, consider the autoregressive Markov chain Monte Carlo (MCMC) kernel that leaves $\\mathcal{N}(m_{\\mathrm{post}}, C_{\\mathrm{post}})$ invariant:\n$$\nx_{k+1} = m_{\\mathrm{post}} + \\rho \\left(x_{k} - m_{\\mathrm{post}}\\right) + \\eta_{k}, \n\\quad \\eta_{k} \\sim \\mathcal{N}(0, (1 - \\rho^{2}) C_{\\mathrm{post}}),\n$$\nwith a fixed contraction parameter $\\rho \\in (0,1)$ and independent innovations $\\eta_{k}$. This update yields a linear Gaussian Markov chain that is ergodic with stationary distribution equal to the posterior.\n\nDefine the burn-in as the minimal nonnegative integer $k$ such that the distance between the marginal distribution of $x_{k}$ (given a specified deterministic starting state $x_{0}$) and the posterior $\\mathcal{N}(m_{\\mathrm{post}}, C_{\\mathrm{post}})$ is within a prescribed tolerance $\\tau$ when measured in total variation distance. You must base your derivation on fundamental definitions together with well-tested facts, including:\n- The definition of total variation distance.\n- Pinsker’s inequality relating total variation distance to Kullback–Leibler divergence.\n- The closed-form Kullback–Leibler divergence for multivariate Gaussian distributions.\n\nYour task is to:\n- Derive, from first principles and the aforementioned well-tested facts, an explicit, computable criterion to decide the minimal burn-in iterations required to guarantee that the total variation distance is at most $\\tau$, separately for two deterministic initializations $x_{0} = m_{0}$ (prior mean) and $x_{0} = m_{\\mathrm{post}}$ (posterior mean).\n- Quantify the effect of the initialization by reporting the two burn-in counts.\n- Additionally, determine a thinning factor $s \\in \\mathbb{N}$ such that the lag-$1$ autocorrelation of the thinned chain $\\{x_{k s}\\}$ is at most a given threshold $\\alpha \\in (0,1)$, using the fact that the lag-$\\ell$ autocorrelation of this autoregressive chain decays geometrically like $\\rho^{\\ell}$ along every posterior mode.\n\nImplement a complete program that performs the following for each test case:\n1. Constructs $C_{\\mathrm{post}}^{-1}$ and $m_{\\mathrm{post}}$ from the provided inputs.\n2. Computes the minimal burn-in iteration count $k_{\\mathrm{prior}}$ for $x_{0} = m_{0}$.\n3. Computes the minimal burn-in iteration count $k_{\\mathrm{post}}$ for $x_{0} = m_{\\mathrm{post}}$.\n4. Computes the minimal thinning factor $s_{\\min}$ such that $\\rho^{s_{\\min}} \\le \\alpha$.\n\nYour program should use only deterministic computations; no sampling is permitted. For numerical linear algebra, use direct linear solves rather than explicit matrix inversion when appropriate.\n\nTest Suite:\nProvide results for the following three scientifically consistent test cases.\n\n- Test Case A (one-dimensional):\n  - $n = 1$, $m = 1$.\n  - $G = [\\,2\\,]$.\n  - $m_{0} = [\\,0\\,]$.\n  - $C_{0} = [\\,[\\,1\\,]\\,]$.\n  - $\\Gamma = [\\,[\\,0.25\\,]\\,]$.\n  - $y = [\\,1.0\\,]$.\n  - $\\rho = 0.9$.\n  - Tolerance $\\tau = 0.05$.\n  - Thinning threshold $\\alpha = 0.2$.\n\n- Test Case B (two-dimensional):\n  - $n = 2$, $m = 2$.\n  - $G = \\begin{bmatrix} 1  -1 \\\\ 0  2 \\end{bmatrix}$.\n  - $m_{0} = [\\,0, 0\\,]^{\\top}$.\n  - $C_{0} = \\mathrm{diag}([\\,4, 1\\,])$.\n  - $\\Gamma = \\mathrm{diag}([\\,0.5, 0.5\\,])$.\n  - $y = [\\,1.0, -1.0\\,]^{\\top}$.\n  - $\\rho = 0.95$.\n  - Tolerance $\\tau = 0.02$.\n  - Thinning threshold $\\alpha = 0.1$.\n\n- Test Case C (five-dimensional):\n  - $n = 5$, $m = 5$.\n  - $G = \\begin{bmatrix}\n    1  0  0  0  0 \\\\\n    0.5  1  0  0  0 \\\\\n    0  0.5  1  0  0 \\\\\n    0  0  0.5  1  0 \\\\\n    0  0  0  0.5  1\n  \\end{bmatrix}$.\n  - $m_{0} = [\\,0, 0, 0, 0, 0\\,]^{\\top}$.\n  - $C_{0} = \\mathrm{diag}([\\,9, 4, 1, 4, 9\\,])$.\n  - $\\Gamma = \\mathrm{diag}([\\,0.5, 2.0, 1.5, 1.0, 0.8\\,])$.\n  - $y = [\\,1.0, -1.0, 0.5, 2.0, -0.5\\,]^{\\top}$.\n  - $\\rho = 0.85$.\n  - Tolerance $\\tau = 0.005$.\n  - Thinning threshold $\\alpha = 0.05$.\n\nFinal Output Format:\nYour program should produce a single line of output containing a list with three entries, one per test case, where each entry is itself a list of three integers $[\\,k_{\\mathrm{prior}}, k_{\\mathrm{post}}, s_{\\min}\\,]$. The final printed line must be exactly of the form\n$$\n[\\,[k_{\\mathrm{prior}}^{A},k_{\\mathrm{post}}^{A},s_{\\min}^{A}],\\,[k_{\\mathrm{prior}}^{B},k_{\\mathrm{post}}^{B},s_{\\min}^{B}],\\,[k_{\\mathrm{prior}}^{C},k_{\\mathrm{post}}^{C},s_{\\min}^{C}]\\,],\n$$\nwith no extra whitespace. No physical units or angle units are involved. All answers must be integers.", "solution": "The problem requires the derivation and implementation of criteria for determining the burn-in period and thinning factor for a specific Markov chain Monte Carlo (MCMC) sampler applied to a linear Gaussian inverse problem. The solution proceeds in three stages: first, a rigorous derivation of the burn-in criterion; second, a derivation of the thinning factor; and third, the implementation of these criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Forward Model**: $y = G x + \\varepsilon$, where $x \\in \\mathbb{R}^{n}$, $y \\in \\mathbb{R}^{m}$, $G \\in \\mathbb{R}^{m \\times n}$.\n-   **Observational Noise**: $\\varepsilon \\sim \\mathcal{N}(0, \\Gamma)$, with $\\Gamma \\in \\mathbb{R}^{m \\times m}$ being a positive-definite covariance matrix.\n-   **Prior Distribution**: $x \\sim \\mathcal{N}(m_{0}, C_{0})$, with mean $m_{0} \\in \\mathbb{R}^{n}$ and symmetric positive-definite covariance $C_{0} \\in \\mathbb{R}^{n \\times n}$.\n-   **Posterior Distribution**: $\\mathcal{N}(m_{\\mathrm{post}}, C_{\\mathrm{post}})$, with precision matrix $C_{\\mathrm{post}}^{-1} = C_{0}^{-1} + G^{\\top}\\Gamma^{-1}G$ and mean $m_{\\mathrm{post}} = C_{\\mathrm{post}}(C_{0}^{-1} m_{0} + G^{\\top}\\Gamma^{-1}y)$.\n-   **MCMC Kernel**: $x_{k+1} = m_{\\mathrm{post}} + \\rho \\left(x_{k} - m_{\\mathrm{post}}\\right) + \\eta_{k}$, with $\\eta_{k} \\sim \\mathcal{N}(0, (1 - \\rho^{2}) C_{\\mathrm{post}})$ and $\\rho \\in (0,1)$.\n-   **Initial State**: Deterministic starting state $x_{0}$.\n-   **Burn-in Definition**: Minimal nonnegative integer $k$ such that the total variation distance between the distribution of $x_k$ and the posterior is at most $\\tau$.\n-   **Tools**: Total variation distance, Pinsker's inequality, closed-form Kullback–Leibler (KL) divergence for multivariate Gaussians.\n-   **Initializations**: $x_{0} = m_{0}$ and $x_{0} = m_{\\mathrm{post}}$.\n-   **Thinning Definition**: Minimal integer $s \\ge 1$ such that the lag-$1$ autocorrelation of the thinned chain $\\{x_{ks}\\}$ is at most $\\alpha$. The lag-$\\ell$ autocorrelation is given to be $\\rho^{\\ell}$.\n-   **Test Data**: Three distinct sets of parameters $(n, m, G, m_0, C_0, \\Gamma, y, \\rho, \\tau, \\alpha)$ are provided.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is rooted in the standard theory of Bayesian inverse problems for linear Gaussian models and basic MCMC theory. The posterior equations are standard. The MCMC sampler is a well-known autoregressive process designed to converge to a target Gaussian distribution. The use of information-theoretic measures like total variation distance and KL divergence is standard for analyzing convergence of probability distributions. All parameters and conditions are specified, and no ambiguities or contradictions are present. The matrices $C_0$ and $\\Gamma$ are specified as positive-definite, ensuring their inverses exist. The posterior precision $C_{\\mathrm{post}}^{-1}$ is the sum of a positive-definite matrix ($C_0^{-1}$) and a positive semi-definite matrix ($G^T \\Gamma^{-1} G$), making $C_{\\mathrm{post}}^{-1}$ positive-definite. Thus, a unique posterior distribution exists. The problem is a formal exercise in applied probability and numerical linear algebra, not a non-formalizable analogy or a triviality.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. Proceeding with the solution.\n\n### Derivation of the Burn-in Criterion\n\nThe burn-in period is defined as the minimum number of iterations $k$ required for the marginal distribution of the chain, $P_k$, to be \"close\" to the stationary posterior distribution, $P_{\\mathrm{post}} = \\mathcal{N}(m_{\\mathrm{post}}, C_{\\mathrm{post}})$. The measure of closeness is the total variation distance, $d_{TV}(P_k, P_{\\mathrm{post}})$. The problem mandates using Pinsker's inequality to obtain an upper bound, which relates the total variation distance to the Kullback-Leibler (KL) divergence:\n$$d_{TV}(P_k, P_{\\mathrm{post}}) \\le \\sqrt{\\frac{1}{2} D_{KL}(P_k || P_{\\mathrm{post}})}$$\nThe burn-in condition $d_{TV}(P_k, P_{\\mathrm{post}}) \\le \\tau$ is thus guaranteed if we enforce the stronger condition:\n$$\\sqrt{\\frac{1}{2} D_{KL}(P_k || P_{\\mathrm{post}})} \\le \\tau \\quad \\iff \\quad D_{KL}(P_k || P_{\\mathrm{post}}) \\le 2\\tau^2$$\nTo use this criterion, we must first find the distribution $P_k$ of the state vector $x_k$ for any given iteration $k$. The chain starts from a deterministic state $x_0$. The update rule is linear and the innovation $\\eta_k$ is Gaussian, so $x_k$ will be Gaussian-distributed for all $k \\ge 1$. Let $x_k \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$.\n\nThe mean $\\mu_k$ evolves according to:\n$$\\mu_{k+1} = E[x_{k+1}] = E[m_{\\mathrm{post}} + \\rho(x_k - m_{\\mathrm{post}}) + \\eta_k] = m_{\\mathrm{post}} + \\rho(\\mu_k - m_{\\mathrm{post}})$$\nThis is a simple geometric progression for the deviation from the posterior mean, $\\delta_k = \\mu_k - m_{\\mathrm{post}}$. We have $\\delta_{k+1} = \\rho \\delta_k$, which implies $\\delta_k = \\rho^k \\delta_0$. Since $x_0$ is deterministic, $\\mu_0 = x_0$, and thus $\\delta_0 = x_0 - m_{\\mathrm{post}}$. The mean at iteration $k$ is:\n$$\\mu_k = m_{\\mathrm{post}} + \\rho^k(x_0 - m_{\\mathrm{post}})$$\nThe covariance $\\Sigma_k$ evolves according to:\n$$\\Sigma_{k+1} = \\mathrm{Cov}(x_{k+1}) = \\mathrm{Cov}(\\rho(x_k - m_{\\mathrm{post}}) + \\eta_k) = \\rho^2 \\mathrm{Cov}(x_k) + \\mathrm{Cov}(\\eta_k)$$\nwhere we used the independence of $x_k$ and $\\eta_k$. Substituting the definitions:\n$$\\Sigma_{k+1} = \\rho^2 \\Sigma_k + (1-\\rho^2)C_{\\mathrm{post}}$$\nStarting with a deterministic $x_0$, the initial covariance is $\\Sigma_0 = \\mathbf{0}$. The recurrence unfolds as:\n$\\Sigma_1 = (1-\\rho^2)C_{\\mathrm{post}}$\n$\\Sigma_2 = \\rho^2(1-\\rho^2)C_{\\mathrm{post}} + (1-\\rho^2)C_{\\mathrm{post}} = (1-\\rho^4)C_{\\mathrm{post}}$\nBy induction, the covariance at iteration $k$ is:\n$$\\Sigma_k = (1 - \\rho^{2k}) C_{\\mathrm{post}}$$\nFor $k=0$, the distribution $P_0$ is a point mass at $x_0$. The total variation distance between a point mass and a continuous distribution (the posterior) is $1$. Since the given tolerances are $\\tau \\ll 1$, the condition $d_{TV} \\le \\tau$ can never be met at $k=0$. Therefore, we seek the minimal integer $k \\ge 1$. For $k \\ge 1$, $\\Sigma_k$ is positive definite, and $P_k = \\mathcal{N}(\\mu_k, \\Sigma_k)$ is a valid non-degenerate Gaussian distribution.\n\nThe KL divergence from a Gaussian $P_1 = \\mathcal{N}(\\mu_1, \\Sigma_1)$ to $P_2 = \\mathcal{N}(\\mu_2, \\Sigma_2)$ is:\n$$D_{KL}(P_1 || P_2) = \\frac{1}{2} \\left( \\mathrm{Tr}(\\Sigma_2^{-1} \\Sigma_1) + (\\mu_2-\\mu_1)^{\\top} \\Sigma_2^{-1} (\\mu_2-\\mu_1) - n + \\ln\\left(\\frac{\\det \\Sigma_2}{\\det \\Sigma_1}\\right) \\right)$$\nIn our case, $P_1 = P_k$ and $P_2 = P_{\\mathrm{post}}$. We substitute $\\mu_1=\\mu_k, \\Sigma_1=\\Sigma_k, \\mu_2=m_{\\mathrm{post}}, \\Sigma_2=C_{\\mathrm{post}}$:\n-   Trace term: $\\mathrm{Tr}(C_{\\mathrm{post}}^{-1} \\Sigma_k) = \\mathrm{Tr}(C_{\\mathrm{post}}^{-1} (1-\\rho^{2k})C_{\\mathrm{post}}) = \\mathrm{Tr}((1-\\rho^{2k})I_n) = n(1-\\rho^{2k})$.\n-   Quadratic term: $(\\mu_2-\\mu_1)^{\\top} \\Sigma_2^{-1} (\\mu_2-\\mu_1) = (-\\rho^k(x_0 - m_{\\mathrm{post}}))^{\\top} C_{\\mathrm{post}}^{-1} (-\\rho^k(x_0 - m_{\\mathrm{post}})) = \\rho^{2k}(x_0 - m_{\\mathrm{post}})^{\\top} C_{\\mathrm{post}}^{-1} (x_0 - m_{\\mathrm{post}})$.\n-   Log-determinant term: $\\ln(\\frac{\\det C_{\\mathrm{post}}}{\\det \\Sigma_k}) = \\ln(\\frac{\\det C_{\\mathrm{post}}}{\\det((1-\\rho^{2k})C_{\\mathrm{post}})}) = \\ln(\\frac{1}{(1-\\rho^{2k})^n}) = -n \\ln(1-\\rho^{2k})$.\n\nCombining these terms, the KL divergence is:\n$$D_{KL}(P_k || P_{\\mathrm{post}}) = \\frac{1}{2} \\left[ n(1-\\rho^{2k}) + \\rho^{2k} D^2 - n - n\\ln(1-\\rho^{2k}) \\right]$$\nwhere $D^2 = (x_0 - m_{\\mathrm{post}})^{\\top} C_{\\mathrm{post}}^{-1} (x_0 - m_{\\mathrm{post}})$ is the squared Mahalanobis distance between $x_0$ and $m_{\\mathrm{post}}$ with respect to $C_{\\mathrm{post}}$.\nSimplifying this expression yields:\n$$D_{KL}(P_k || P_{\\mathrm{post}}) = \\frac{1}{2} \\left[ \\rho^{2k} (D^2 - n) - n\\ln(1-\\rho^{2k}) \\right]$$\nThe burn-in criterion is to find the smallest integer $k \\ge 1$ satisfying:\n$$\\frac{1}{2} \\left[ \\rho^{2k} (D^2 - n) - n\\ln(1-\\rho^{2k}) \\right] \\le 2\\tau^2$$\nThis inequality is solved by numerically iterating on $k=1, 2, 3, \\dots$ until the condition is met.\n\n**Case 1: Initialization at posterior mean ($x_0 = m_{\\mathrm{post}}$)**\nHere, $x_0 - m_{\\mathrm{post}} = \\mathbf{0}$, so $D^2 = 0$. The criterion for $k_{\\mathrm{post}}$ simplifies to finding the smallest integer $k \\ge 1$ such that:\n$$\\frac{1}{2} \\left[ -n\\rho^{2k} - n\\ln(1-\\rho^{2k}) \\right] \\le 2\\tau^2 \\iff -\\rho^{2k} - \\ln(1-\\rho^{2k}) \\le \\frac{4\\tau^2}{n}$$\n\n**Case 2: Initialization at prior mean ($x_0 = m_0$)**\nHere, we first compute $D_{\\mathrm{prior}}^2 = (m_0 - m_{\\mathrm{post}})^{\\top} C_{\\mathrm{post}}^{-1} (m_0 - m_{\\mathrm{post}})$. The criterion for $k_{\\mathrm{prior}}$ is to find the smallest integer $k \\ge 1$ such that:\n$$\\rho^{2k} (D_{\\mathrm{prior}}^2 - n) - n\\ln(1-\\rho^{2k}) \\le 4\\tau^2$$\n\n### Derivation of the Thinning Factor\n\nThe thinning factor $s$ is chosen to reduce autocorrelation in the chain. The problem states that the lag-$\\ell$ autocorrelation of the original chain $\\{x_k\\}$ is $\\rho^{\\ell}$. A thinned chain is formed by taking every $s$-th sample: $\\{x_{ks}\\}$. The lag-$1$ autocorrelation of this thinned chain is equivalent to the lag-$s$ autocorrelation of the original chain. We require this autocorrelation to be no more than a given threshold $\\alpha$.\nThe condition is therefore:\n$$\\rho^s \\le \\alpha$$\nSince $\\rho \\in (0,1)$ and $\\alpha \\in (0,1)$, we can take the natural logarithm of both sides. As $\\ln(\\rho)$ is negative, we must reverse the inequality sign upon division:\n$$s \\ln(\\rho) \\le \\ln(\\alpha) \\implies s \\ge \\frac{\\ln(\\alpha)}{\\ln(\\rho)}$$\nSince $s$ must be a positive integer ($s \\in \\mathbb{N}$), the minimal thinning factor $s_{\\min}$ is the smallest integer satisfying this condition:\n$$s_{\\min} = \\left\\lceil \\frac{\\ln(\\alpha)}{\\ln(\\rho)} \\right\\rceil$$\n\n### Computational Algorithm\n\nFor each test case, the algorithm proceeds as follows:\n1.  Read the parameters $n, m, G, m_0, C_0, \\Gamma, y, \\rho, \\tau, \\alpha$.\n2.  Compute the inverses of the prior and noise covariance matrices, $C_0^{-1}$ and $\\Gamma^{-1}$.\n3.  Compute the posterior precision matrix $C_{\\mathrm{post}}^{-1} = C_{0}^{-1} + G^{\\top}\\Gamma^{-1}G$.\n4.  Compute the vector term $b = C_{0}^{-1} m_{0} + G^{\\top}\\Gamma^{-1}y$.\n5.  Solve the linear system $C_{\\mathrm{post}}^{-1}m_{\\mathrm{post}} = b$ to find the posterior mean $m_{\\mathrm{post}}$.\n6.  To find $k_{\\mathrm{post}}$, iterate $k=1, 2, \\dots$ until $-\\rho^{2k} - \\ln(1-\\rho^{2k}) \\le 4\\tau^2/n$. The first $k$ to satisfy this is $k_{\\mathrm{post}}$.\n7.  To find $k_{\\mathrm{prior}}$, first compute the squared Mahalanobis distance $D_{\\mathrm{prior}}^2 = (m_0 - m_{\\mathrm{post}})^{\\top} C_{\\mathrm{post}}^{-1} (m_0 - m_{\\mathrm{post}})$. Then, iterate $k=1, 2, \\dots$ until $\\rho^{2k} (D_{\\mathrm{prior}}^2 - n) - n\\ln(1-\\rho^{2k}) \\le 4\\tau^2$. The first $k$ to satisfy this is $k_{\\mathrm{prior}}$.\n8.  Compute the minimum thinning factor $s_{\\min} = \\lceil \\ln(\\alpha)/\\ln(\\rho) \\rceil$.\n9.  Collect the integer results $[k_{\\mathrm{prior}}, k_{\\mathrm{post}}, s_{\\min}]$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the MCMC burn-in and thinning problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 1, \"m\": 1,\n            \"G\": np.array([[2.0]]),\n            \"m0\": np.array([0.0]),\n            \"C0\": np.array([[1.0]]),\n            \"Gamma\": np.array([[0.25]]),\n            \"y\": np.array([1.0]),\n            \"rho\": 0.9, \"tau\": 0.05, \"alpha\": 0.2,\n        },\n        {\n            \"n\": 2, \"m\": 2,\n            \"G\": np.array([[1.0, -1.0], [0.0, 2.0]]),\n            \"m0\": np.array([0.0, 0.0]),\n            \"C0\": np.diag([4.0, 1.0]),\n            \"Gamma\": np.diag([0.5, 0.5]),\n            \"y\": np.array([1.0, -1.0]),\n            \"rho\": 0.95, \"tau\": 0.02, \"alpha\": 0.1,\n        },\n        {\n            \"n\": 5, \"m\": 5,\n            \"G\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0],\n                [0.5, 1.0, 0.0, 0.0, 0.0],\n                [0.0, 0.5, 1.0, 0.0, 0.0],\n                [0.0, 0.0, 0.5, 1.0, 0.0],\n                [0.0, 0.0, 0.0, 0.5, 1.0]\n            ]),\n            \"m0\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"C0\": np.diag([9.0, 4.0, 1.0, 4.0, 9.0]),\n            \"Gamma\": np.diag([0.5, 2.0, 1.5, 1.0, 0.8]),\n            \"y\": np.array([1.0, -1.0, 0.5, 2.0, -0.5]),\n            \"rho\": 0.85, \"tau\": 0.005, \"alpha\": 0.05,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(solve_case(case))\n\n    # Format the final output string\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef get_burn_in(n, rho, D2, tau, max_iter=100000):\n    \"\"\"\n    Computes the minimal burn-in iteration count.\n    \"\"\"\n    threshold = 4 * tau**2\n    \n    # Iterate to find the smallest k >= 1 satisfying the condition\n    for k in range(1, max_iter):\n        rho_2k = rho**(2 * k)\n        \n        # This is the LHS of the inequality derived in the solution text\n        # D_KL * 2 = rho^{2k} (D^2 - n) - n ln(1-rho^{2k})\n        kl_term = rho_2k * (D2 - n) - n * np.log(1 - rho_2k)\n        \n        if kl_term = threshold:\n            return k\n    return max_iter # Should not be reached with reasonable max_iter\n\ndef solve_case(params):\n    \"\"\"\n    Processes a single test case.\n    \"\"\"\n    n = params[\"n\"]\n    G, m0, C0, Gamma, y = params[\"G\"], params[\"m0\"], params[\"C0\"], params[\"Gamma\"], params[\"y\"]\n    rho, tau, alpha = params[\"rho\"], params[\"tau\"], params[\"alpha\"]\n\n    # Ensure vectors are column vectors for matrix operations\n    m0 = m0.reshape(-1, 1)\n    y = y.reshape(-1, 1)\n\n    # 1. Compute posterior parameters\n    C0_inv = np.linalg.inv(C0)\n    Gamma_inv = np.linalg.inv(Gamma)\n\n    C_post_inv = C0_inv + G.T @ Gamma_inv @ G\n    b = C0_inv @ m0 + G.T @ Gamma_inv @ y\n    m_post = np.linalg.solve(C_post_inv, b)\n\n    # 2. Compute burn-in for x0 = m_post\n    # D^2 = (m_post - m_post)^T C_post_inv (m_post - m_post) = 0\n    D2_post = 0.0\n    k_post = get_burn_in(n, rho, D2_post, tau)\n\n    # 3. Compute burn-in for x0 = m0\n    delta_m = m0 - m_post\n    D2_prior = (delta_m.T @ C_post_inv @ delta_m).item() # .item() to get scalar\n    k_prior = get_burn_in(n, rho, D2_prior, tau)\n    \n    # 4. Compute thinning factor\n    s_min = int(np.ceil(np.log(alpha) / np.log(rho)))\n\n    return [k_prior, k_post, s_min]\n\nsolve()\n\n```", "id": "3370153"}, {"introduction": "Building upon the theoretical foundations of burn-in, this practice transitions to a more realistic scenario where you will implement a Random-Walk Metropolis MCMC sampler. Here, we introduce a powerful and practical burn-in diagnostic based on the concept of a \"typical set,\" which monitors whether the chain is exploring regions of the parameter space consistent with the data likelihood. This exercise will guide you through implementing this sophisticated, sliding-window diagnostic to determine the burn-in length, after which you will estimate the integrated autocorrelation time to inform a thinning strategy [@problem_id:3370155].", "problem": "Consider a linear inverse problem with Gaussian observational noise in which the data misfit is measured by the negative log-likelihood $-\\tfrac{1}{2}\\lVert \\Gamma^{-1/2}(G(\\theta)-y)\\rVert^2$. Let $G(\\theta)=H\\theta$ where $H\\in\\mathbb{R}^{m\\times d}$ is known, and let $\\Gamma\\in\\mathbb{R}^{m\\times m}$ be a symmetric positive definite observation covariance matrix. Assume a flat prior so that the posterior density is proportional to the likelihood. You will implement a Random-Walk Metropolis Markov chain Monte Carlo (MCMC) algorithm and design a burn-in diagnostic based on deviations in the Kullback-Leibler divergence (KL) to a typical set defined by the likelihood.\n\nFundamental base for the derivation and implementation must start from the following facts: Gaussian noise implies a quadratic negative log-likelihood, the distribution of the normalized squared residual for noise-only data is chi-squared with $m$ degrees of freedom, the typical set associated with a given mass level is defined by central quantiles of that chi-squared law, the Kullback-Leibler divergence between discrete distributions is defined by its basic variational form, and the Random-Walk Metropolis acceptance rule follows from the ratio of target densities for a symmetric proposal.\n\nUse the following fixed model components, common to all test cases. Set the observation dimension to $m=5$ and the parameter dimension to $d=3$. Define\n$$\nH=\\begin{bmatrix}\n1.0  0.5  0.0\\\\\n0.0  1.0  0.5\\\\\n0.2  0.0  1.0\\\\\n1.0  -0.5  0.25\\\\\n0.0  0.0  1.0\n\\end{bmatrix},\n\\quad\n\\Gamma=\\mathrm{diag}\\big(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2,\\sigma_4^2,\\sigma_5^2\\big),\n\\quad\n(\\sigma_1,\\sigma_2,\\sigma_3,\\sigma_4,\\sigma_5)=(0.1,0.1,0.1,0.2,0.15).\n$$\nLet $y\\in\\mathbb{R}^5$ be fixed as\n$$\ny=\\begin{bmatrix}0.475\\\\ 0.025\\\\ 0.85\\\\ 0.9125\\\\ 0.675\\end{bmatrix}.\n$$\nFor any $\\theta\\in\\mathbb{R}^3$, define the squared Mahalanobis misfit\n$$\nS(\\theta)=\\left\\|\\Gamma^{-1/2}\\left(H\\theta - y\\right)\\right\\|^2=\\sum_{i=1}^m \\frac{\\left((H\\theta-y)_i\\right)^2}{\\sigma_i^2}.\n$$\nDefine the central $\\alpha$-typical set for this misfit by the interval $[q_{\\mathrm{low}},q_{\\mathrm{high}}]$ where $q_{\\mathrm{low}}$ and $q_{\\mathrm{high}}$ are the lower and upper central quantiles of the chi-squared distribution with $m$ degrees of freedom that capture a central probability mass of $\\alpha$, that is,\n$$\nq_{\\mathrm{low}}=F_{\\chi^2_m}^{-1}\\left(\\frac{1-\\alpha}{2}\\right),\\qquad q_{\\mathrm{high}}=F_{\\chi^2_m}^{-1}\\left(1-\\frac{1-\\alpha}{2}\\right).\n$$\nFor a finite MCMC subsequence of consecutive indices $\\{t,\\dots,t+W-1\\}$ of length $W$, define the empirical fraction $f_t$ of misfits $S(\\theta_j)$ falling into the typical set $[q_{\\mathrm{low}},q_{\\mathrm{high}}]$. Consider the binary empirical distribution $q_t$ on $\\{\\text{typical},\\text{atypical}\\}$ with masses $f_t$ and $1-f_t$, and compare it to the reference binary distribution $p_\\alpha$ on the same support with masses $\\alpha$ and $1-\\alpha$. Using the fundamental definition of the Kullback-Leibler divergence for discrete distributions, compute the divergence $D_{\\mathrm{KL}}(q_t\\|p_\\alpha)$ for each window. Define the burn-in index $b$ to be the smallest starting index $t$ such that for all windows that start at indices $t'\\ge t$, the divergence satisfies $D_{\\mathrm{KL}}(q_{t'}\\|p_\\alpha)\\le \\varepsilon$. If no such index exists, set $b=N$ by convention, where $N$ is the total number of MCMC samples.\n\nAfter discarding the initial $b$ samples, estimate the thinning recommendation by computing the Integrated Autocorrelation Time (IACT) of the scalar misfit sequence $\\{S(\\theta_j)\\}_{j=b}^{N-1}$ using the initial positive sequence estimator applied to the normalized autocorrelation function. Set the recommended thinning interval to $\\lceil \\mathrm{IACT}\\rceil$. If $b=N$, define the thinning interval to be the integer $0$.\n\nYou must implement a Random-Walk Metropolis MCMC with a Gaussian proposal of the form $\\theta'=\\theta+\\eta$ with $\\eta\\sim\\mathcal{N}(0,\\sigma_{\\mathrm{prop}}^2 I_d)$, where $I_d$ is the $d\\times d$ identity matrix. Use the fundamental Metropolis acceptance rule based on the ratio of posterior densities and the symmetry of the proposal. For each test case, generate a chain of length $N$, compute the sliding-window divergences $D_{\\mathrm{KL}}(q_t\\|p_\\alpha)$, determine the burn-in index $b$, and then compute the thinning recommendation $\\lceil \\mathrm{IACT}\\rceil$ as specified.\n\nTest suite. Use the following three parameter sets; each case explicitly specifies the chain length $N$, the proposal standard deviation $\\sigma_{\\mathrm{prop}}$, the initial parameter $\\theta_0$, the typical set mass $\\alpha$, the window length $W$, and the divergence threshold $\\varepsilon$. All other model components are as defined above. Angles do not appear, and no physical units are involved.\n\nCase $1$ (well-tuned chain):\n- $N=4000$, $\\sigma_{\\mathrm{prop}}=0.35$, $\\theta_0=\\begin{bmatrix}0.0\\\\ 0.0\\\\ 0.0\\end{bmatrix}$, $\\alpha=0.9$, $W=400$, $\\varepsilon=0.02$.\n\nCase $2$ (long transient):\n- $N=4000$, $\\sigma_{\\mathrm{prop}}=0.35$, $\\theta_0=\\begin{bmatrix}5.0\\\\ -5.0\\\\ 5.0\\end{bmatrix}$, $\\alpha=0.9$, $W=400$, $\\varepsilon=0.02$.\n\nCase $3$ (very poor mixing):\n- $N=4000$, $\\sigma_{\\mathrm{prop}}=2.5$, $\\theta_0=\\begin{bmatrix}5.0\\\\ -5.0\\\\ 5.0\\end{bmatrix}$, $\\alpha=0.9$, $W=400$, $\\varepsilon=0.02$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list of two integers $[b,\\lceil \\mathrm{IACT}\\rceil]$ as specified above. Aggregate the three per-case results in order into a single list, for example, in the form $[[b_1,\\lceil \\mathrm{IACT}\\rceil_1],[b_2,\\lceil \\mathrm{IACT}\\rceil_2],[b_3,\\lceil \\mathrm{IACT}\\rceil_3]]$.", "solution": "The problem requires the implementation and application of a post-processing workflow for a Markov chain Monte Carlo (MCMC) simulation. This workflow involves determining a suitable burn-in period and a thinning interval for samples drawn from the posterior distribution of a linear inverse problem. The solution will be presented in a step-by-step manner, beginning with the formulation of the Bayesian inverse problem, followed by the description of the MCMC algorithm, and concluding with the detailed procedures for burn-in and thinning diagnostics.\n\n**1. The Bayesian Inverse Problem**\n\nWe are given a linear forward model $G(\\theta) = H\\theta$, where $\\theta \\in \\mathbb{R}^d$ are the parameters of interest and $H \\in \\mathbb{R}^{m \\times d}$ is the forward operator. The observations $y \\in \\mathbb{R}^m$ are assumed to be corrupted by additive Gaussian noise with zero mean and a known covariance matrix $\\Gamma \\in \\mathbb{R}^{m \\times m}$. The data-generating process is thus $y = H\\theta_{\\text{true}} + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\Gamma)$.\n\nThe likelihood of observing the data $y$ given the parameters $\\theta$ is given by the probability density function of the Gaussian noise:\n$$\nL(y|\\theta) \\propto \\exp\\left(-\\frac{1}{2} (H\\theta - y)^T \\Gamma^{-1} (H\\theta - y)\\right)\n$$\nThis can be expressed using the squared Mahalanobis distance, $S(\\theta)$:\n$$\nS(\\theta) = \\left\\|\\Gamma^{-1/2}\\left(H\\theta - y\\right)\\right\\|^2 = (H\\theta - y)^T \\Gamma^{-1} (H\\theta - y)\n$$\nThus, the likelihood is $L(y|\\theta) \\propto \\exp\\left(-\\frac{1}{2}S(\\theta)\\right)$. The problem specifies a flat prior, $\\pi(\\theta) \\propto c$ for some constant $c$. According to Bayes' theorem, the posterior distribution $\\pi(\\theta|y)$ is proportional to the product of the likelihood and the prior:\n$$\n\\pi(\\theta|y) \\propto L(y|\\theta)\\pi(\\theta) \\propto \\exp\\left(-\\frac{1}{2}S(\\theta)\\right)\n$$\nThis posterior density is the target distribution that we aim to sample from using MCMC.\n\nThe specific model components are fixed: the dimensions are $m=5$ and $d=3$. The matrix $H$, the data vector $y$, and the diagonal covariance matrix $\\Gamma=\\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_5^2)$ are provided.\n\n**2. Random-Walk Metropolis (RWM) Algorithm**\n\nTo draw samples from the posterior distribution $\\pi(\\theta|y)$, we employ the Random-Walk Metropolis algorithm, a specific type of Metropolis-Hastings MCMC. The algorithm proceeds as follows:\n\n1.  **Initialization**: Start with an initial parameter vector $\\theta_0$ at step $j=0$.\n2.  **Iteration**: For each step $j = 0, 1, \\dots, N-2$:\n    a. **Propose**: Generate a candidate sample $\\theta'$ from a symmetric proposal distribution. For the RWM algorithm, this is a Gaussian centered at the current state: $\\theta' = \\theta_j + \\eta$, where $\\eta \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{prop}}^2 I_d)$. The symmetry of the proposal, $q(\\theta'|\\theta_j) = q(\\theta_j|\\theta')$, simplifies the acceptance criterion.\n    b. **Accept**: Calculate the acceptance probability, $A$, which is the ratio of posterior densities:\n    $$\n    A(\\theta'|\\theta_j) = \\min\\left(1, \\frac{\\pi(\\theta'|y)}{\\pi(\\theta_j|y)}\\right) = \\min\\left(1, \\frac{\\exp(-\\frac{1}{2}S(\\theta'))}{\\exp(-\\frac{1}{2}S(\\theta_j))}\\right) = \\min\\left(1, \\exp\\left[-\\frac{1}{2}(S(\\theta') - S(\\theta_j))\\right]\\right)\n    $$\n    c. **Update**: Draw a random number $u$ from a uniform distribution $U(0,1)$. If $u  A(\\theta'|\\theta_j)$, the proposal is accepted, and we set $\\theta_{j+1} = \\theta'$. Otherwise, the proposal is rejected, and the chain remains at the current state, $\\theta_{j+1} = \\theta_j$.\n\nThis procedure generates a sequence of samples $\\{\\theta_0, \\theta_1, \\dots, \\theta_{N-1}\\}$ that, after a sufficient burn-in period, forms a Markov chain whose stationary distribution is the target posterior $\\pi(\\theta|y)$.\n\n**3. Burn-in Diagnostic via Kullback-Leibler Divergence**\n\nThe initial samples of an MCMC chain (the burn-in period) may not be representative of the stationary distribution and must be discarded. The problem specifies a diagnostic to determine the burn-in length $b$.\n\nThe diagnostic is based on the statistical behavior of the misfit $S(\\theta)$. If the model were perfect and we knew the true parameters $\\theta_{true}$, the normalized residual term $S(\\theta_{true}) = \\| \\Gamma^{-1/2}(y-H\\theta_{true}) \\|^2$ would be distributed as a chi-squared random variable with $m$ degrees of freedom, $\\chi^2_m$. The diagnostic checks whether the misfits $S(\\theta_j)$ from the MCMC chain behave according to this reference distribution.\n\n1.  **Typical Set**: A central high-probability region for the $\\chi^2_m$ distribution is defined. For a chosen probability mass $\\alpha$, the central $\\alpha$-typical set is the interval $[q_{\\mathrm{low}}, q_{\\mathrm{high}}]$, where the boundaries are quantiles of the $\\chi^2_m$ distribution:\n    $$\n    q_{\\mathrm{low}} = F_{\\chi^2_m}^{-1}\\left(\\frac{1-\\alpha}{2}\\right), \\quad q_{\\mathrm{high}} = F_{\\chi^2_m}^{-1}\\left(1 - \\frac{1-\\alpha}{2}\\right)\n    $$\n    where $F_{\\chi^2_m}^{-1}$ is the inverse cumulative distribution function (quantile function). By construction, the probability of a $\\chi^2_m$ variate falling into this interval is exactly $\\alpha$.\n\n2.  **KL Divergence**: The diagnostic is computed over sliding windows of length $W$ along the chain. For a window starting at index $t$, we compute the empirical fraction, $f_t$, of samples $\\theta_j$ (for $j \\in \\{t, \\dots, t+W-1\\}$) for which the misfit $S(\\theta_j)$ lies within the typical set $[q_{\\mathrm{low}}, q_{\\mathrm{high}}]$.\n    This defines a binary empirical distribution $q_t$ on the outcomes $\\{\\text{typical}, \\text{atypical}\\}$ with probabilities $(f_t, 1-f_t)$. This is compared to the theoretical reference distribution $p_\\alpha = (\\alpha, 1-\\alpha)$. The discrepancy is measured by the Kullback-Leibler (KL) divergence:\n    $$\n    D_{\\mathrm{KL}}(q_t \\| p_\\alpha) = f_t \\log\\left(\\frac{f_t}{\\alpha}\\right) + (1-f_t) \\log\\left(\\frac{1-f_t}{1-\\alpha}\\right)\n    $$\n    where the convention $0 \\log 0 = 0$ is used.\n\n3.  **Burn-in Index ($b$)**: The burn-in index $b$ is defined as the smallest window starting index $t$ such that for all subsequent windows (i.e., for all $t' \\ge t$), the divergence remains below a given threshold $\\varepsilon$: $D_{\\mathrm{KL}}(q_{t'} \\| p_\\alpha) \\le \\varepsilon$. To find $b$, one can identify the index of the *last* window that violates this condition, say $t_{\\text{last\\_violating}}$. Then, the burn-in period is $b = t_{\\text{last\\_violating}} + 1$. If no window violates the condition, $b=0$. If no such $t$ exists (i.e., the condition is violated arbitrarily far into the chain), the problem specifies the convention $b=N$.\n\n**4. Thinning via Integrated Autocorrelation Time (IACT)**\n\nAfter discarding the burn-in samples, the remaining chain may still exhibit strong correlation between consecutive samples. Thinning is the process of keeping only every $k$-th sample to reduce this correlation, where $k$ is the thinning interval. A common way to determine $k$ is by estimating the Integrated Autocorrelation Time (IACT).\n\n1.  **Stationary Sequence**: We use the sequence of scalar misfits from the stationary part of the chain, $\\{S(\\theta_j)\\}_{j=b}^{N-1}$.\n2.  **Autocorrelation Function (ACF)**: First, the normalized ACF, $\\rho_k$, of this sequence is computed for various lags $k$.\n3.  **IACT Estimation**: The IACT, $\\tau$, is given by $\\tau = 1 + 2\\sum_{k=1}^{\\infty} \\rho_k$. We estimate this using the initial positive sequence method, which truncates the sum at the first lag $M$ where the ACF becomes non-positive:\n    $$\n    \\mathrm{IACT} \\approx 1 + 2 \\sum_{k=1}^{M} \\rho_k, \\quad \\text{where } M = \\max\\{k' \\mid \\rho_k > 0 \\text{ for all } 1 \\le k \\le k'\\}\n    $$\n4.  **Thinning Interval**: The recommended thinning interval is the smallest integer greater than or equal to the IACT, i.e., $\\lceil \\mathrm{IACT} \\rceil$. If the burn-in comprises the entire chain ($b=N$), the stationary sequence is empty, and the thinning interval is defined to be $0$.\n\nThe implementation will systematically execute these steps for each test case provided.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\nfrom scipy.special import rel_entr\nimport math\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the MCMC simulation and analysis for all test cases.\n    \"\"\"\n    # Set a random seed for reproducibility of the stochastic MCMC process.\n    np.random.seed(0)\n\n    # --- Fixed Model Components ---\n    # Observation dimension m, parameter dimension d\n    m_dim, d_dim = 5, 3\n\n    # Forward model matrix H\n    H = np.array([\n        [1.0, 0.5, 0.0],\n        [0.0, 1.0, 0.5],\n        [0.2, 0.0, 1.0],\n        [1.0, -0.5, 0.25],\n        [0.0, 0.0, 1.0]\n    ])\n\n    # Observation vector y\n    y_obs = np.array([0.475, 0.025, 0.85, 0.9125, 0.675])\n\n    # Observation noise standard deviations and inverse variance for Mahalanobis distance\n    sigma_obs = np.array([0.1, 0.1, 0.1, 0.2, 0.15])\n    gamma_inv_diag = 1.0 / sigma_obs**2\n\n    # --- Helper Functions ---\n    def calculate_misfit(theta):\n        \"\"\"Calculates the squared Mahalanobis misfit S(theta).\"\"\"\n        residual = H @ theta - y_obs\n        return np.sum((residual**2) * gamma_inv_diag)\n\n    def run_mcmc(N, theta0, sigma_prop):\n        \"\"\"Runs the Random-Walk Metropolis MCMC sampler.\"\"\"\n        chain = np.zeros((N, d_dim))\n        misfits = np.zeros(N)\n\n        chain[0] = theta0\n        current_misfit = calculate_misfit(theta0)\n        misfits[0] = current_misfit\n\n        for j in range(N - 1):\n            proposal_theta = chain[j] + np.random.normal(0, sigma_prop, size=d_dim)\n            proposal_misfit = calculate_misfit(proposal_theta)\n            \n            # Acceptance probability on the log-scale to avoid potential overflow/underflow\n            log_acceptance_prob = -0.5 * (proposal_misfit - current_misfit)\n            \n            if np.log(np.random.uniform(0, 1))  log_acceptance_prob:\n                chain[j + 1] = proposal_theta\n                current_misfit = proposal_misfit\n            else:\n                chain[j + 1] = chain[j]\n            \n            misfits[j + 1] = current_misfit\n        \n        return misfits\n\n    def calculate_burn_in(misfits, N, W, alpha, epsilon):\n        \"\"\"Determines the burn-in index b based on the KL divergence diagnostic.\"\"\"\n        q_low = chi2.ppf((1.0 - alpha) / 2.0, df=m_dim)\n        q_high = chi2.ppf(1.0 - (1.0 - alpha) / 2.0, df=m_dim)\n        \n        num_windows = N - W + 1\n        if num_windows = 0:\n            return N\n\n        divergences = np.zeros(num_windows)\n        p_alpha = np.array([alpha, 1.0 - alpha])\n\n        for t in range(num_windows):\n            window = misfits[t : t + W]\n            f_t = np.mean((window >= q_low)  (window = q_high))\n            p_t = np.array([f_t, 1.0 - f_t])\n            # rel_entr(p, q) computes p*log(p/q), sum is the KL divergence\n            divergences[t] = np.sum(rel_entr(p_t, p_alpha))\n\n        # We need the smallest t such that for all t' >= t, D_KL = epsilon.\n        # This is equivalent to finding the last t where D_KL > epsilon and adding 1.\n        # For a robust implementation, we check from the end of the chain.\n        is_good_from_here = np.zeros(num_windows, dtype=bool)\n        is_good_from_here[-1] = (divergences[-1] = epsilon)\n        for t in range(num_windows - 2, -1, -1):\n            is_good_from_here[t] = (divergences[t] = epsilon) and is_good_from_here[t + 1]\n        \n        good_starts = np.where(is_good_from_here)[0]\n        if len(good_starts) == 0:\n            return N  # Convention if no such index exists\n        else:\n            return good_starts[0]\n\n    def calculate_iact(misfits, b, N):\n        \"\"\"Estimates the Integrated Autocorrelation Time (IACT).\"\"\"\n        if b >= N:\n            return 0 # Per problem spec\n\n        stat_misfits = misfits[b:]\n        if len(stat_misfits)  2:\n            return 1 # Cannot compute ACF, assume independence\n\n        x = stat_misfits - np.mean(stat_misfits)\n        autocov = np.correlate(x, x, mode='full')[len(x) - 1:]\n        \n        if autocov[0] == 0: # Variance is zero, all samples are identical\n            return 1\n\n        acf = autocov / autocov[0]\n        \n        # Initial positive sequence estimator for IACT\n        iact = 1.0\n        for k in range(1, len(acf)):\n            if acf[k] > 0:\n                iact += 2.0 * acf[k]\n            else:\n                break\n        \n        return math.ceil(iact)\n    \n    # --- Test Cases ---\n    test_cases = [\n        # Case 1: well-tuned chain\n        {'N': 4000, 'sigma_prop': 0.35, 'theta0': np.array([0.0, 0.0, 0.0]),\n         'alpha': 0.9, 'W': 400, 'epsilon': 0.02},\n        # Case 2: long transient\n        {'N': 4000, 'sigma_prop': 0.35, 'theta0': np.array([5.0, -5.0, 5.0]),\n         'alpha': 0.9, 'W': 400, 'epsilon': 0.02},\n        # Case 3: very poor mixing\n        {'N': 4000, 'sigma_prop': 2.5, 'theta0': np.array([5.0, -5.0, 5.0]),\n         'alpha': 0.9, 'W': 400, 'epsilon': 0.02},\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case['N']\n        theta0 = case['theta0']\n        sigma_prop = case['sigma_prop']\n        W = case['W']\n        alpha = case['alpha']\n        epsilon = case['epsilon']\n        \n        # 1. Generate the MCMC chain and get the misfit sequence\n        misfits = run_mcmc(N, theta0, sigma_prop)\n        \n        # 2. Compute the burn-in index\n        burn_in_index = calculate_burn_in(misfits, N, W, alpha, epsilon)\n        \n        # 3. Compute the thinning interval recommendation\n        thinning_interval = calculate_iact(misfits, burn_in_index, N)\n        \n        results.append([burn_in_index, thinning_interval])\n\n    # Format the final output as specified\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n\n```", "id": "3370155"}, {"introduction": "After establishing methods for burn-in and thinning, a crucial question remains: what is the statistical consequence of thinning? While thinning is widely used to reduce autocorrelation, it also discards a significant number of samples. This final exercise directly investigates this trade-off by comparing the variance and confidence interval widths of posterior mean estimates obtained from a full chain versus a thinned chain. Through this simulation-based comparison [@problem_id:3370133], you will develop a nuanced perspective on when thinning is—and is not—a beneficial practice for improving the statistical efficiency of your MCMC estimators.", "problem": "You are given a scalar linear Gaussian inverse problem, a synthetic Markov Chain Monte Carlo (MCMC) sampler that preserves the posterior, and a requirement to estimate uncertainty in the posterior mean using batch means under two strategies: burn-in only and burn-in followed by thinning. Your task is to write a program that simulates the chain, computes Batch Means (BM) variance estimates of the posterior mean estimator, constructs two-sided confidence intervals based on the Student-$t$ distribution, and compares the widths with and without thinning.\n\nStart from the following foundational principles and definitions.\n\n1. Linear Gaussian inverse problem. Assume a one-dimensional unknown $x$ with prior $x \\sim \\mathcal{N}(0,\\tau^2)$, a linear forward operator $h \\in \\mathbb{R}$, and a single observation $y \\in \\mathbb{R}$ with additive noise $\\varepsilon \\sim \\mathcal{N}(0,r^2)$, so that $y = h x + \\varepsilon$. The posterior $p(x \\mid y)$ is Gaussian with mean $m$ and variance $s^2$ given by Bayes’ rule for Gaussian conjugacy:\n$$\ns^2 = \\left(\\tau^{-2} + h^2 r^{-2}\\right)^{-1}, \\quad m = s^2 \\cdot h r^{-2} y.\n$$\n\n2. Synthetic MCMC with specified autocorrelation. Consider the Autoregressive process of order one (AR(1)) defined by\n$$\nX_{t+1} = m + \\rho \\left(X_t - m\\right) + \\sqrt{\\left(1-\\rho^2\\right) s^2}\\, Z_t,\n$$\nwhere $\\{Z_t\\}$ are independent and identically distributed standard normal random variables, and $\\rho \\in (-1,1)$ is the lag-$1$ autocorrelation. This chain is Gaussian and has stationary distribution $\\mathcal{N}(m,s^2)$.\n\n3. Burn-in and thinning. Given a simulated path $\\{X_t\\}_{t=1}^T$:\n- Burn-in discards the first $B$ states to reduce initialization bias, yielding the retained series $\\{X_{B+1},\\dots,X_T\\}$ of length $n = T - B$.\n- Thinning by factor $k \\in \\mathbb{N}$ keeps every $k$-th element of the retained series, i.e., $\\{X_{B+1}, X_{B+1+k}, X_{B+1+2k}, \\dots\\}$ of length $n_{\\mathrm{thin}} = \\left\\lfloor \\frac{n}{k} \\right\\rfloor$.\n\n4. Batch means variance estimation for Markov chain central limit theorem. Let $g(x) = x$ denote the identity function so that the goal is to estimate $\\mu = \\mathbb{E}_{\\pi}[g(X)] = m$. Assume the Markov chain Central Limit Theorem (CLT): with $Y_t = g(X_t)$,\n$$\n\\sqrt{n}\\left(\\bar{Y}_n - \\mu\\right) \\xrightarrow{d} \\mathcal{N}(0,\\sigma^2),\n$$\nwhere $\\bar{Y}_n = \\frac{1}{n} \\sum_{t=1}^n Y_t$ and $\\sigma^2$ is the asymptotic variance. The batch means estimator partitions $Y_1,\\dots,Y_n$ into $b$ non-overlapping batches, each of size $a$ so that $n = a b$. Let the batch means be $\\bar{Y}^{(i)} = \\frac{1}{a} \\sum_{t=(i-1)a+1}^{ia} Y_t$ for $i \\in \\{1,\\dots,b\\}$, and let $\\bar{Y}$ be the overall mean. The estimator of the asymptotic variance is\n$$\n\\hat{\\sigma}^2_{\\mathrm{BM}} = a \\cdot \\frac{1}{b-1} \\sum_{i=1}^b \\left(\\bar{Y}^{(i)} - \\bar{Y}\\right)^2,\n$$\nand therefore the estimator of $\\mathrm{Var}(\\bar{Y}_n)$ is\n$$\n\\widehat{\\mathrm{Var}}(\\bar{Y}_n) = \\frac{\\hat{\\sigma}^2_{\\mathrm{BM}}}{n}.\n$$\nA two-sided confidence interval for $\\mu$ at nominal level $1-\\alpha$ is then\n$$\n\\bar{Y}_n \\pm t_{1-\\alpha/2,\\,b-1} \\sqrt{\\widehat{\\mathrm{Var}}(\\bar{Y}_n)},\n$$\nwhere $t_{1-\\alpha/2,\\,b-1}$ is the quantile of the Student-$t$ distribution with $b-1$ degrees of freedom. The confidence interval width is therefore\n$$\nW = 2 \\, t_{1-\\alpha/2,\\,b-1} \\sqrt{\\widehat{\\mathrm{Var}}(\\bar{Y}_n)}.\n$$\n\nYour program must implement the following steps for each test case:\n\na. Compute the posterior mean $m$ and variance $s^2$ using the given $(h,\\tau^2,r^2,y)$.\n\nb. Simulate the AR(1) chain of length $T$ with parameter $\\rho$, initialized at $X_1 = m + \\delta$ with a specified offset $\\delta$ (the same $\\delta$ is used for all cases), and independent standard normal innovations.\n\nc. Apply burn-in by discarding the first $B$ samples. For the non-thinned path, compute the batch means variance estimate $\\widehat{\\mathrm{Var}}(\\bar{Y}_n)$ using exactly $b$ batches and the corresponding batch size $a = n/b$ (assume parameters ensure that integer division is exact after truncating any extra samples if necessary). Construct the two-sided confidence interval at level $1-\\alpha$ and record its width $W_{\\mathrm{nt}}$.\n\nd. Apply thinning with factor $k$ to the post-burn-in path, compute the batch means variance estimate $\\widehat{\\mathrm{Var}}(\\bar{Y}_{n_{\\mathrm{thin}}})$ using the same $b$ batches (with batch size $a_{\\mathrm{thin}} = n_{\\mathrm{thin}}/b$), construct the confidence interval, and record its width $W_{\\mathrm{th}}$.\n\ne. For each test case, output the tuple containing the estimated variance of the mean without thinning, the estimated variance of the mean with thinning, the confidence interval width without thinning $W_{\\mathrm{nt}}$, the confidence interval width with thinning $W_{\\mathrm{th}}$, and a boolean indicating whether $W_{\\mathrm{th}} > W_{\\mathrm{nt}}$.\n\nNumerical and formatting requirements:\n\n- Use $\\alpha = 0.05$ for nominal level $1-\\alpha = 0.95$.\n- Use the same number of batches $b$ for both the non-thinned and thinned analyses within each test case.\n- Use the initial offset $\\delta = 10.0$.\n- Round all floating-point outputs to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of the form $[\\widehat{\\mathrm{Var}}(\\bar{Y}_n), \\widehat{\\mathrm{Var}}(\\bar{Y}_{n_{\\mathrm{thin}}}), W_{\\mathrm{nt}}, W_{\\mathrm{th}}, \\text{boolean}]$. Do not include any spaces in the output.\n\nTest suite:\n\nProvide results for the following three test cases; each case is defined by $(h,\\tau^2,r^2,y,\\rho,T,B,b,k,\\text{seed})$.\n\n- Case $1$ (moderate autocorrelation, moderate thinning): $(h=\\;1.0,\\;\\tau^2=\\;1.0,\\;r^2=\\;1.0,\\;y=\\;1.5,\\;\\rho=\\;0.5,\\;T=\\;50500,\\;B=\\;500,\\;b=\\;50,\\;k=\\;5,\\;\\text{seed}=\\;12345)$.\n\n- Case $2$ (independent and identically distributed, stronger thinning): $(h=\\;2.0,\\;\\tau^2=\\;1.0,\\;r^2=\\;4.0,\\;y=\\;-1.0,\\;\\rho=\\;0.0,\\;T=\\;50500,\\;B=\\;500,\\;b=\\;50,\\;k=\\;10,\\;\\text{seed}=\\;23456)$.\n\n- Case $3$ (strong autocorrelation, aggressive thinning): $(h=\\;1.0,\\;\\tau^2=\\;4.0,\\;r^2=\\;1.0,\\;y=\\;0.0,\\;\\rho=\\;0.99,\\;T=\\;200500,\\;B=\\;500,\\;b=\\;50,\\;k=\\;100,\\;\\text{seed}=\\;34567)$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing a list with three elements, one per test case, in the exact format\n$[[v_{1,\\mathrm{nt}},v_{1,\\mathrm{th}},w_{1,\\mathrm{nt}},w_{1,\\mathrm{th}},\\mathrm{bool}_1],[v_{2,\\mathrm{nt}},v_{2,\\mathrm{th}},w_{2,\\mathrm{nt}},w_{2,\\mathrm{th}},\\mathrm{bool}_2],[v_{3,\\mathrm{nt}},v_{3,\\mathrm{th}},w_{3,\\mathrm{nt}},w_{3,\\mathrm{th}},\\mathrm{bool}_3]]$,\nwhere $v_{i,\\mathrm{nt}} = \\widehat{\\mathrm{Var}}(\\bar{Y}_n)$ for case $i$, $v_{i,\\mathrm{th}} = \\widehat{\\mathrm{Var}}(\\bar{Y}_{n_{\\mathrm{thin}}})$ for case $i$, $w_{i,\\mathrm{nt}} = W_{\\mathrm{nt}}$ for case $i$, $w_{i,\\mathrm{th}} = W_{\\mathrm{th}}$ for case $i$, and $\\mathrm{bool}_i$ is a boolean for $W_{\\mathrm{th}} > W_{\\mathrm{nt}}$ in case $i$. All floating-point numbers must be rounded to $6$ decimal places. No spaces are allowed in the printed line.", "solution": "The problem is assessed to be valid. The premises are scientifically sound, the definitions are mathematically and algorithmically precise, all necessary data are provided, and the task is well-posed.\n\n### **Problem Validation**\n\n#### **Step 1: Extract Givens**\n\n**1. Linear Gaussian Inverse Problem:**\n- Unknown: $x$, with prior $x \\sim \\mathcal{N}(0,\\tau^2)$.\n- Forward operator: $h \\in \\mathbb{R}$.\n- Observation: $y \\in \\mathbb{R}$, with model $y = h x + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,r^2)$.\n- Posterior $p(x \\mid y)$ is $\\mathcal{N}(m,s^2)$ where:\n  $$s^2 = \\left(\\tau^{-2} + h^2 r^{-2}\\right)^{-1}$$\n  $$m = s^2 \\cdot h r^{-2} y$$\n\n**2. Synthetic MCMC Sampler:**\n- AR(1) process: $X_{t+1} = m + \\rho \\left(X_t - m\\right) + \\sqrt{\\left(1-\\rho^2\\right) s^2}\\, Z_t$.\n- $\\{Z_t\\}$ are i.i.d. $\\mathcal{N}(0,1)$.\n- Autocorrelation: $\\rho \\in (-1,1)$.\n- Stationary distribution: $\\mathcal{N}(m,s^2)$.\n\n**3. Burn-in and Thinning:**\n- Total samples: $T$.\n- Burn-in length: $B$. Retained series $\\{X_{B+1},\\dots,X_T\\}$ has length $n = T-B$.\n- Thinning factor: $k \\in \\mathbb{N}$. Thinned series has length $n_{\\mathrm{thin}} = \\left\\lfloor \\frac{n}{k} \\right\\rfloor$.\n\n**4. Batch Means Variance Estimation:**\n- Estimand: $\\mu = \\mathbb{E}_{\\pi}[g(X)]$ where $g(x)=x$, so $\\mu=m$.\n- Let $\\{Y_t\\}$ be the MCMC chain (post-burn-in).\n- Partition $\\{Y_1,\\dots,Y_n\\}$ into $b$ batches of size $a$ ($n=ab$).\n- Batch means: $\\bar{Y}^{(i)} = \\frac{1}{a} \\sum_{t=(i-1)a+1}^{ia} Y_t$.\n- Overall mean: $\\bar{Y}$.\n- Asymptotic variance estimator: $\\hat{\\sigma}^2_{\\mathrm{BM}} = a \\cdot \\frac{1}{b-1} \\sum_{i=1}^b \\left(\\bar{Y}^{(i)} - \\bar{Y}\\right)^2$.\n- Estimator of $\\mathrm{Var}(\\bar{Y}_n)$: $\\widehat{\\mathrm{Var}}(\\bar{Y}_n) = \\frac{\\hat{\\sigma}^2_{\\mathrm{BM}}}{n}$.\n- $(1-\\alpha)$ CI for $\\mu$: $\\bar{Y}_n \\pm t_{1-\\alpha/2,\\,b-1} \\sqrt{\\widehat{\\mathrm{Var}}(\\bar{Y}_n)}$.\n- CI width: $W = 2 \\, t_{1-\\alpha/2,\\,b-1} \\sqrt{\\widehat{\\mathrm{Var}}(\\bar{Y}_n)}$.\n\n**5. Numerical and Task Parameters:**\n- Confidence level: $1-\\alpha = 0.95$ ($\\alpha = 0.05$).\n- Number of batches: $b$ (same for non-thinned and thinned).\n- Initial offset: $\\delta = 10.0$ from the true mean $m$.\n- Rounding: float outputs to $6$ decimal places.\n\n**6. Test Cases:**\n- Case 1: $(h=1.0,\\;\\tau^2=1.0,\\;r^2=1.0,\\;y=1.5,\\;\\rho=0.5,\\;T=50500,\\;B=500,\\;b=50,\\;k=5,\\;\\text{seed}=12345)$.\n- Case 2: $(h=2.0,\\;\\tau^2=1.0,\\;r^2=4.0,\\;y=-1.0,\\;\\rho=0.0,\\;T=50500,\\;B=500,\\;b=50,\\;k=10,\\;\\text{seed}=23456)$.\n- Case 3: $(h=1.0,\\;\\tau^2=4.0,\\;r^2=1.0,\\;y=0.0,\\;\\rho=0.99,\\;T=200500,\\;B=500,\\;b=50,\\;k=100,\\;\\text{seed}=34567)$.\n\n#### **Step 2: Validate Using Extracted Givens**\n\nThe problem is a well-defined computational exercise in statistical simulation and analysis.\n- **Scientifically Grounded**: The problem is built upon fundamental principles of Bayesian inference (Gaussian conjugacy), time series analysis (AR(1) processes), and MCMC diagnostics (batch means for variance estimation). All formulas provided are standard and correct.\n- **Well-Posed**: For each test case, a complete set of parameters is provided. The instructions for simulation and calculation are unambiguous. The use of a fixed random seed ensures that the simulation is reproducible, leading to a unique solution. The parameters for sample sizes, batches, and thinning are chosen such that batch sizes are exact integers, precluding ambiguity in implementation.\n- **Objective**: The problem is specified using precise mathematical notation and algorithmic steps, devoid of any subjective language.\n\nThe problem does not exhibit any flaws related to scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a formalizable and relevant task within the field of inverse problems and data assimilation.\n\n#### **Step 3: Verdict and Action**\n\nThe problem is **valid**. A complete solution will be provided.\n\n### **Principle-Based Solution**\n\nThe objective is to compare uncertainty estimates for the posterior mean of a parameter in a linear Gaussian inverse problem, derived from a synthetic MCMC chain. The comparison is between two strategies: one using burn-in only, and another using burn-in followed by thinning. The uncertainty is quantified via the width of a confidence interval, calculated using the batch means method. The following steps detail the procedure for each test case.\n\n**a. Posterior Characterization**\nFirst, we characterize the posterior distribution $p(x \\mid y)$. The problem states that for a Gaussian prior $x \\sim \\mathcal{N}(0,\\tau^2)$ and a Gaussian likelihood arising from the model $y = h x + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,r^2)$, the posterior distribution is also Gaussian, $p(x \\mid y) \\sim \\mathcal{N}(m, s^2)$. We apply the provided formulas for the posterior mean $m$ and variance $s^2$, which are a direct result of Bayes' rule for conjugate Gaussian distributions.\n$$s^2 = \\left(\\tau^{-2} + h^2 r^{-2}\\right)^{-1}$$\n$$m = s^2 \\cdot h r^{-2} y$$\n\n**b. MCMC Simulation**\nNext, we simulate a Markov chain that has the posterior distribution $\\mathcal{N}(m,s^2)$ as its stationary distribution. The problem specifies using a Gaussian AR(1) process for this purpose. The evolution of the chain is given by:\n$$X_{t+1} = m + \\rho \\left(X_t - m\\right) + \\sqrt{\\left(1-\\rho^2\\right) s^2}\\, Z_t$$\nwhere $\\{Z_t\\}$ is a sequence of i.i.d. standard normal random variables. This process is designed to have a stationary mean of $m$, a stationary variance of $s^2$, and a lag-$1$ autocorrelation of $\\rho$. We initialize the chain at $X_1 = m + \\delta$ to simulate starting away from the stationary distribution, a common scenario in practice. The simulation is run for a total of $T$ steps. A specific random seed is used to ensure reproducibility.\n\n**c. Analysis without Thinning (Burn-in Only)**\nThe initial portion of an MCMC chain may be biased by the starting value. To mitigate this, we discard the first $B$ samples (the \"burn-in\" period). The remaining series, of length $n = T-B$, is used for analysis.\nThe goal is to estimate the variance of the sample mean $\\bar{Y}_n = \\frac{1}{n} \\sum_{t=B+1}^T X_t$. Because the samples $X_t$ are correlated, the simple variance formula $\\mathrm{Var}(X)/n$ is incorrect. The batch means method addresses this by grouping the $n$ samples into $b$ large batches of size $a=n/b$. The mean of each batch is calculated. If the batch size $a$ is large enough, the batch means are approximately uncorrelated.\nWe then apply the batch means formula for the asymptotic variance, $\\hat{\\sigma}^2_{\\mathrm{BM}} = a \\cdot \\mathrm{Var}(\\{\\text{batch means}\\})$, where the variance of the batch means is calculated with $b-1$ degrees of freedom. The variance of the overall sample mean is then estimated as $\\widehat{\\mathrm{Var}}(\\bar{Y}_n) = \\hat{\\sigma}^2_{\\mathrm{BM}}/n$.\nFinally, a $(1-\\alpha)$ confidence interval for the true mean $m$ is constructed. Under the MCMC Central Limit Theorem, the distribution of $(\\bar{Y}_n - m)/\\sqrt{\\widehat{\\mathrm{Var}}(\\bar{Y}_n)}$ is approximately a Student-$t$ distribution with $b-1$ degrees of freedom. The width of this confidence interval is $W_{\\mathrm{nt}} = 2 \\cdot t_{1-\\alpha/2, b-1} \\cdot \\sqrt{\\widehat{\\mathrm{Var}}(\\bar{Y}_n)}$.\n\n**d. Analysis with Thinning**\nThinning is a technique where only every $k$-th sample of the post-burn-in chain is kept. This reduces the size of the dataset to $n_{\\mathrm{thin}} = \\lfloor n/k \\rfloor$ samples but also reduces the autocorrelation between them. The procedure for the thinned chain is analogous to the non-thinned case. We partition the $n_{\\mathrm{thin}}$ samples into $b$ batches of size $a_{\\mathrm{thin}} = n_{\\mathrm{thin}}/b$. The batch means variance estimate $\\widehat{\\mathrm{Var}}(\\bar{Y}_{n_{\\mathrm{thin}}})$ and the corresponding confidence interval width $W_{\\mathrm{th}}$ are calculated using the same formulas as before, but applied to the thinned data. The number of batches $b$, and thus the degrees of freedom for the t-distribution, remains the same.\n\n**e. Comparison and Output**\nFor each test case, we compute and store five values: the estimated variance of the mean for the non-thinned chain, $\\widehat{\\mathrm{Var}}(\\bar{Y}_n)$; the same for the thinned chain, $\\widehat{\\mathrm{Var}}(\\bar{Y}_{n_{\\mathrm{thin}}})$; their respective confidence interval widths, $W_{\\mathrm{nt}}$ and $W_{\\mathrm{th}}$; and a boolean flag indicating whether thinning resulted in a wider confidence interval ($W_{\\mathrm{th}} > W_{\\mathrm{nt}}$). Thinning reduces the number of samples, which tends to increase the variance of the mean estimator. However, by reducing autocorrelation, it can potentially improve the performance of the batch means method, especially if the original batch size was not large enough to ensure near-independence of batch means. The final comparison reveals the net effect of these competing factors for the given parameters. The final numerical results are rounded to six decimal places and formatted as specified.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t as t_dist\n\ndef solve():\n    \"\"\"\n    Main function to run the MCMC analysis for all test cases and format the output.\n    \"\"\"\n    test_cases = [\n        # (h, tau_sq, r_sq, y, rho, T, B, b, k, seed)\n        (1.0, 1.0, 1.0, 1.5, 0.5, 50500, 500, 50, 5, 12345),\n        (2.0, 1.0, 4.0, -1.0, 0.0, 50500, 500, 50, 10, 23456),\n        (1.0, 4.0, 1.0, 0.0, 0.99, 200500, 500, 50, 100, 34567),\n    ]\n\n    # Global parameters\n    delta = 10.0\n    alpha = 0.05\n    \n    results = []\n    for case in test_cases:\n        result_tuple = process_case(case, delta, alpha)\n        results.append(result_tuple)\n\n    # Format the final output string as per requirements\n    case_strings = []\n    for v_nt, v_th, w_nt, w_th, is_wider in results:\n        # Format floats to 6 decimal places and boolean to lowercase string\n        s = f\"[{v_nt:.6f},{v_th:.6f},{w_nt:.6f},{w_th:.6f},{str(is_wider).lower()}]\"\n        case_strings.append(s)\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\ndef process_case(case, delta, alpha):\n    \"\"\"\n    Processes a single test case for MCMC simulation and analysis.\n    \"\"\"\n    h, tau_sq, r_sq, y, rho, T, B, b, k, seed = case\n    \n    # Step a: Compute posterior mean m and variance s^2\n    s_sq = 1.0 / (1.0/tau_sq + h**2 / r_sq)\n    m = s_sq * h * y / r_sq\n\n    # Step b: Simulate the AR(1) chain\n    rng = np.random.default_rng(seed)\n    X = np.zeros(T)\n    X[0] = m + delta  # Initialize with offset\n    \n    noise_std = np.sqrt((1 - rho**2) * s_sq)\n    \n    for t in range(T - 1):\n        Z_t = rng.standard_normal()\n        X[t+1] = m + rho * (X[t] - m) + noise_std * Z_t\n        \n    # Apply burn-in\n    Y_retained = X[B:]\n    n = T - B\n\n    # --- Analysis for non-thinned chain ---\n    if n % b != 0:\n        # As per problem statement, parameters are chosen to make this exact.\n        # This block is for robustness, but won't be entered for the given cases.\n        n_used_nt = (n // b) * b\n        Y_nt = Y_retained[:n_used_nt]\n    else:\n        Y_nt = Y_retained\n    \n    n_nt = len(Y_nt)\n    a_nt = n_nt // b\n    \n    batch_means_nt = np.mean(Y_nt.reshape(b, a_nt), axis=1)\n    \n    # Batch means variance estimation\n    sigma_sq_bm_hat_nt = a_nt * np.var(batch_means_nt, ddof=1)\n    var_mean_hat_nt = sigma_sq_bm_hat_nt / n_nt\n    \n    # Confidence interval calculation\n    t_quantile = t_dist.ppf(1 - alpha / 2, df=b-1)\n    width_nt = 2 * t_quantile * np.sqrt(var_mean_hat_nt)\n\n    # --- Analysis for thinned chain ---\n    Y_th = Y_retained[::k]\n    n_th = len(Y_th)\n    \n    if n_th % b != 0:\n        # As per problem statement, parameters are chosen to make this exact.\n        n_used_th = (n_th // b) * b\n        Y_th = Y_th[:n_used_th]\n\n    n_th = len(Y_th) # update length after potential truncation\n    a_th = n_th // b\n    \n    batch_means_th = np.mean(Y_th.reshape(b, a_th), axis=1)\n    \n    # Batch means variance estimation\n    sigma_sq_bm_hat_th = a_th * np.var(batch_means_th, ddof=1)\n    var_mean_hat_th = sigma_sq_bm_hat_th / n_th\n    \n    # Confidence interval calculation (t_quantile is the same)\n    width_th = 2 * t_quantile * np.sqrt(var_mean_hat_th)\n    \n    # Step e: Consolidate and return results\n    is_wider = width_th > width_nt\n    \n    return (var_mean_hat_nt, var_mean_hat_th, width_nt, width_th, is_wider)\n\nsolve()\n```", "id": "3370133"}]}