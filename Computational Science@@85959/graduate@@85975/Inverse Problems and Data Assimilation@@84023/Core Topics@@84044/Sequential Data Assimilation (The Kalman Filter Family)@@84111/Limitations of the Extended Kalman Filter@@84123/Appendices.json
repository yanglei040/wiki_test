{"hands_on_practices": [{"introduction": "The Extended Kalman Filter's core strategy is to approximate a nonlinear system with a linear one at each step, but this simplification can introduce systematic errors. This first practice explores one of the most fundamental limitations: the introduction of bias in the state estimate. By setting up a scenario where the measurement perfectly matches the prediction from the prior mean, the EKF sees no innovation and fails to update its estimate, whereas the true Bayesian posterior mean shifts to account for the model's curvature. This exercise [@problem_id:3397756] allows you to directly quantify this bias and gain an intuitive understanding of how model nonlinearity can skew the posterior belief, even in the absence of measurement noise.", "problem": "Consider a single-step data assimilation problem with a scalar latent state $x \\in \\mathbb{R}$. The prior distribution is Gaussian $x \\sim \\mathcal{N}(\\mu, P)$ with mean $\\mu$ and variance $P$. The observation model is $y = h(x) + v$ where $h:\\mathbb{R} \\to \\mathbb{R}$ is a twice-differentiable nonlinear function and $v \\sim \\mathcal{N}(0, R)$ is zero-mean Gaussian measurement noise with variance $R$. The Extended Kalman Filter (EKF) proceeds by linearizing the observation function $h(x)$ at the current estimate and applying the linear Kalman filter update. In contrast, the exact Bayesian posterior mean is given by integrating against the exact likelihood and prior via Bayes' rule.\n\nStarting from the foundational definitions of a Gaussian prior, a Gaussian noise model, Bayes' rule for the posterior, and first-order Taylor linearization, your task is to:\n\n- Implement the EKF measurement update for a single assimilation step using a linearization of $h(x)$ at $x = \\mu$.\n- Compute the exact Bayesian posterior mean $m^\\star(y)$ defined by\n$$\nm^\\star(y) \\equiv \\frac{\\int_{-\\infty}^{\\infty} x \\, \\exp\\!\\left(-\\frac{(x-\\mu)^2}{2P}\\right) \\, \\exp\\!\\left(-\\frac{(y - h(x))^2}{2R}\\right) \\, dx}{\\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{(x-\\mu)^2}{2P}\\right) \\, \\exp\\!\\left(-\\frac{(y - h(x))^2}{2R}\\right) \\, dx},\n$$\nwhich is the Gaussian-prior-weighted expectation of the state under the exact nonlinear likelihood, omitting normalization constants that cancel between numerator and denominator.\n- Quantify the bias in the EKF posterior mean estimate as\n$$\nb \\equiv m^\\star(y) - m^{\\text{EKF}}(y),\n$$\nwhere $m^{\\text{EKF}}(y)$ is the EKF posterior mean after the measurement update. The bias $b$ is a direct measure of the limitation of the EKF due to linearization and model curvature.\n\nFor all tests below, use the convention that angles are measured in radians. To isolate the effect of nonlinearity on bias, set the realized measurement equal to the observation function evaluated at the prior mean, i.e., $y = h(\\mu)$, in each test case. Under this choice, the EKF innovation will be zero by construction, but the exact Bayesian posterior mean may shift due to the curvature of $h(x)$, revealing EKF bias.\n\nImplement a program that computes the bias $b$ for each of the following test cases:\n\n- Test Case 1 (linear observation, \"happy path\"): $h(x) = a x + b$ with $a = 1.3$, $b = -0.2$, prior mean $\\mu = 0.5$, variance $P = 0.09$, noise variance $R = 0.04$, and measurement $y = h(\\mu)$.\n- Test Case 2 (quadratic observation): $h(x) = x^2$, prior mean $\\mu = 0.5$, variance $P = 0.2$, noise variance $R = 0.01$, and measurement $y = h(\\mu)$.\n- Test Case 3 (sinusoidal observation): $h(x) = \\sin(x)$, prior mean $\\mu = 0.3$, variance $P = 0.15$, noise variance $R = 0.02$, and measurement $y = h(\\mu)$. Angles are in radians.\n- Test Case 4 (saturating observation): $h(x) = \\tanh(x)$, prior mean $\\mu = 0.8$, variance $P = 0.3$, noise variance $R = 0.02$, and measurement $y = h(\\mu)$.\n- Test Case 5 (strong nonlinearity and large prior variance): $h(x) = x^2$, prior mean $\\mu = 1.0$, variance $P = 1.0$, noise variance $R = 0.1$, and measurement $y = h(\\mu)$.\n\nFor each test case, compute the exact Bayesian posterior mean $m^\\star(y)$ via numerical integration over $x \\in (-\\infty, \\infty)$ as specified above, compute the EKF posterior mean $m^{\\text{EKF}}(y)$ using a linearization of $h(x)$ at $x=\\mu$ and a standard linear Kalman update, and then report the bias $b$ as a floating-point number. Your program should produce a single line of output containing the biases for the five test cases as a comma-separated list enclosed in square brackets. Express each bias as a decimal number and round to six decimal places in the final output. The final output format must be exactly\n$$\n[\\text{bias}_1,\\text{bias}_2,\\text{bias}_3,\\text{bias}_4,\\text{bias}_5].\n$$", "solution": "The problem is assessed to be valid. It is a well-posed and scientifically grounded exercise in nonlinear state estimation, asking for a quantitative comparison between the Extended Kalman Filter (EKF) and the exact Bayesian posterior. All necessary functions, parameters, and definitions are provided, and there are no contradictions or factual errors. The tautological statement \"EKF, which stands for Extended Kalman Filter (EKF)\" is a minor linguistic flaw but has no bearing on the problem's mathematical or scientific validity.\n\nThe task is to compute the bias of the EKF posterior mean, defined as $b \\equiv m^\\star(y) - m^{\\text{EKF}}(y)$, for a scalar system under a specific measurement condition. Here, $m^{\\text{EKF}}(y)$ is the posterior mean from the EKF update, and $m^\\star(y)$ is the exact Bayesian posterior mean.\n\n### 1. Derivation of the EKF Posterior Mean $m^{\\text{EKF}}(y)$\n\nThe EKF approximates a nonlinear system with a linear one by performing a first-order Taylor series expansion of the nonlinear functions around the current state estimate. For the measurement update, the observation function $h(x)$ is linearized around the prior mean, $x=\\mu$.\n\nThe state and its uncertainty before the measurement are given by the prior distribution, $x \\sim \\mathcal{N}(\\mu, P)$. Thus, the prior estimate is $\\mu$ with variance $P$.\n\nThe first-order Taylor expansion of $h(x)$ around $x=\\mu$ is:\n$$\nh(x) \\approx h(\\mu) + H(x - \\mu)\n$$\nwhere $H$ is the Jacobian of $h(x)$ evaluated at $x=\\mu$. Since $x$ is a scalar, $H$ is the derivative:\n$$\nH = \\left. \\frac{dh}{dx} \\right|_{x=\\mu}\n$$\nThe observation model $y = h(x) + v$ is then approximated as:\n$$\ny \\approx h(\\mu) + H(x - \\mu) + v\n$$\nThis is a linear observation model for the state $x$ with respect to the \"linearized\" measurement $y' = y - h(\\mu) + H\\mu$. More directly, we can apply the standard linear Kalman update equations.\n\nThe key components of the Kalman filter measurement update are:\n1.  **Innovation** (or measurement residual): $\\nu = y - \\hat{y}$, where $\\hat{y}$ is the predicted measurement. For the EKF, the predicted measurement is the observation function evaluated at the prior mean, so $\\hat{y} = h(\\mu)$.\n    $$\n    \\nu = y - h(\\mu)\n    $$\n2.  **Innovation Covariance**: $S = H P H^T + R$. For our scalar case, this is:\n    $$\n    S = H^2 P + R\n    $$\n3.  **Kalman Gain**: $K = P H^T S^{-1}$. In the scalar case:\n    $$\n    K = \\frac{PH}{H^2 P + R}\n    $$\n4.  **Updated State Mean**: The posterior mean $m^{\\text{EKF}}(y)$ is given by the prior mean plus the innovation corrected by the Kalman gain.\n    $$\n    m^{\\text{EKF}}(y) = \\mu + K \\nu = \\mu + K (y - h(\\mu))\n    $$\n\nThe problem specifies a crucial condition: the measurement $y$ is set to the value of the observation function at the prior mean, i.e., $y = h(\\mu)$. Substituting this into the innovation equation gives:\n$$\n\\nu = h(\\mu) - h(\\mu) = 0\n$$\nConsequently, the EKF state update becomes:\n$$\nm^{\\text{EKF}}(y) = \\mu + K \\cdot 0 = \\mu\n$$\nThis result is fundamental to the problem's design. With the measurement set to $h(\\mu)$, the EKF posterior mean is identical to the prior mean, as the linearized model sees no new information. Any deviation of the true posterior mean from $\\mu$ is therefore a direct measure of the error introduced by the linearization.\n\n### 2. The Exact Bayesian Posterior Mean $m^\\star(y)$\n\nBayes' rule states that the posterior probability density function $p(x|y)$ is proportional to the product of the likelihood $p(y|x)$ and the prior $p(x)$:\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\nGiven the problem definitions:\n-   The prior is Gaussian: $p(x) = \\mathcal{N}(x; \\mu, P) \\propto \\exp\\left(-\\frac{(x - \\mu)^2}{2P}\\right)$.\n-   The likelihood, from the noise model $v \\sim \\mathcal{N}(0, R)$, is also Gaussian: $p(y|x) = \\mathcal{N}(y; h(x), R) \\propto \\exp\\left(-\\frac{(y - h(x))^2}{2R}\\right)$.\n\nCombining these gives the unnormalized posterior:\n$$\np(x|y) \\propto \\exp\\left(-\\frac{(x-\\mu)^2}{2P} - \\frac{(y-h(x))^2}{2R}\\right)\n$$\nThe exact posterior mean $m^\\star(y)$ is the expected value of $x$ with respect to this posterior distribution:\n$$\nm^\\star(y) = \\mathbb{E}[x|y] = \\frac{\\int_{-\\infty}^{\\infty} x p(x|y) dx}{\\int_{-\\infty}^{\\infty} p(x|y) dx}\n$$\nSubstituting the expression for the unnormalized posterior yields the formula given in the problem statement:\n$$\nm^\\star(y) = \\frac{\\int_{-\\infty}^{\\infty} x \\, \\exp\\!\\left(-\\frac{(x-\\mu)^2}{2P}\\right) \\, \\exp\\!\\left(-\\frac{(y - h(x))^2}{2R}\\right) \\, dx}{\\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{(x-\\mu)^2}{2P}\\right) \\, \\exp\\!\\left(-\\frac{(y - h(x))^2}{2R}\\right) \\, dx}\n$$\nWhen $h(x)$ is nonlinear, these integrals generally do not have a closed-form solution and must be computed numerically.\n\n### 3. Bias Calculation\n\nThe bias $b$ is the difference between the exact and the EKF-approximated posterior means:\n$$\nb = m^\\star(y) - m^{\\text{EKF}}(y)\n$$\nUsing our derived results, where $m^{\\text{EKF}}(y) = \\mu$:\n$$\nb = m^\\star(y) - \\mu\n$$\nThis bias directly quantifies the error due to the EKF's linearization, isolated from the effects of a random measurement innovation. The bias will be non-zero if the posterior distribution $p(x|y)$ becomes skewed due to the nonlinearity (curvature) of $h(x)$.\n\nFor the linear case, $h(x) = ax+b$, the posterior exponent is a quadratic function of $x$. This means the posterior $p(x|y)$ is exactly Gaussian. The mean of a Gaussian distribution is its mode (the point of maximum probability). In this case, the mean `m*(y)` will be identical to the Kalman filter mean, and thus the bias will be $0$. For nonlinear $h(x)$, the posterior is non-Gaussian, and a bias is expected.\n\n### 4. Computational Procedure for Each Test Case\n\nFor each test case with parameters $(\\mu, P, R)$ and function $h(x)$:\n1.  Set the measurement $y = h(\\mu)$.\n2.  The EKF posterior mean is $m^{\\text{EKF}}(y) = \\mu$.\n3.  Define the integrand for the numerator of $m^\\star(y)$:\n    $$\n    \\text{integrand}_{\\text{num}}(x) = x \\exp\\left(-\\frac{(x-\\mu)^2}{2P} - \\frac{(h(\\mu)-h(x))^2}{2R}\\right)\n    $$\n4.  Define the integrand for the denominator of $m^\\star(y)$:\n    $$\n    \\text{integrand}_{\\text{den}}(x) = \\exp\\left(-\\frac{(x-\\mu)^2}{2P} - \\frac{(h(\\mu)-h(x))^2}{2R}\\right)\n    $$\n5.  Numerically compute the integrals over $(-\\infty, \\infty)$:\n    $$\n    N = \\int_{-\\infty}^{\\infty} \\text{integrand}_{\\text{num}}(x) \\, dx\n    $$\n    $$\n    D = \\int_{-\\infty}^{\\infty} \\text{integrand}_{\\text{den}}(x) \\, dx\n    $$\n6.  Calculate the exact posterior mean: $m^\\star(y) = N/D$.\n7.  Calculate the bias: $b = m^\\star(y) - \\mu$.\nThis procedure will be implemented for all five test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# language: Python\n# version: 3.12\n# libraries:\n#   name: numpy, version: 1.23.5\n#   name: scipy, version: 1.11.4\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Computes the bias of the Extended Kalman Filter (EKF) posterior mean for\n    several test cases with nonlinear observation models.\n    \"\"\"\n\n    # Define the observation functions h(x) for each test case.\n    def h_case1(x):\n        a, b = 1.3, -0.2\n        return a * x + b\n\n    def h_case2(x):\n        return x**2\n\n    def h_case3(x):\n        return np.sin(x)\n\n    def h_case4(x):\n        return np.tanh(x)\n\n    # Test cases defined as a list of dictionaries for clarity.\n    # Each dictionary contains the observation function h, prior mean mu,\n    # prior variance P, and measurement noise variance R.\n    test_cases = [\n        # Test Case 1: Linear observation model\n        {'h': h_case1, 'mu': 0.5, 'P': 0.09, 'R': 0.04},\n        # Test Case 2: Quadratic observation model\n        {'h': h_case2, 'mu': 0.5, 'P': 0.2, 'R': 0.01},\n        # Test Case 3: Sinusoidal observation model\n        {'h': h_case3, 'mu': 0.3, 'P': 0.15, 'R': 0.02},\n        # Test Case 4: Saturating observation model\n        {'h': h_case4, 'mu': 0.8, 'P': 0.3, 'R': 0.02},\n        # Test Case 5: Strong nonlinearity and large prior variance\n        {'h': h_case2, 'mu': 1.0, 'P': 1.0, 'R': 0.1},\n    ]\n\n    results = []\n    for case in test_cases:\n        h = case['h']\n        mu = case['mu']\n        P = case['P']\n        R = case['R']\n\n        # Per the problem statement, the measurement y is set to h(mu).\n        y = h(mu)\n\n        # The EKF posterior mean m_ekf simplifies to mu under this condition,\n        # as the innovation is zero.\n        m_ekf = mu\n\n        # Define the unnormalized posterior PDF, which is proportional to\n        # exp(-((x-mu)^2)/(2P) - ((y-h(x))^2)/(2R)).\n        # We define a function for the exponent to avoid re-calculation and\n        # potential underflow with the full exponential.\n        def posterior_log_pdf_unnormalized(x):\n            prior_term = (x - mu)**2 / (2 * P)\n            likelihood_term = (y - h(x))**2 / (2 * R)\n            return -prior_term - likelihood_term\n\n        # Integrand for the numerator of the exact posterior mean E[x|y]\n        def numerator_integrand(x):\n            return x * np.exp(posterior_log_pdf_unnormalized(x))\n\n        # Integrand for the denominator (normalization constant)\n        def denominator_integrand(x):\n            return np.exp(posterior_log_pdf_unnormalized(x))\n\n        # Perform numerical integration over (-inf, inf)\n        # The quad function returns the integral result and an error estimate.\n        numerator_val, _ = quad(numerator_integrand, -np.inf, np.inf)\n        denominator_val, _ = quad(denominator_integrand, -np.inf, np.inf)\n        \n        # Calculate the exact Bayesian posterior mean m_star\n        if denominator_val == 0:\n            # This case should not be reached with the given parameters,\n            # but is included for robustness.\n            m_star = mu\n        else:\n            m_star = numerator_val / denominator_val\n            \n        # The bias is the difference between the exact mean and the EKF mean.\n        bias = m_star - m_ekf\n        results.append(bias)\n\n    # Format the final output as a comma-separated list of biases,\n    # rounded to six decimal places, enclosed in square brackets.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3397756"}, {"introduction": "The linearization at the heart of the EKF relies on the assumption that the system's dynamics and observation models are smooth and differentiable. This exercise directly challenges this core requirement by introducing non-differentiable functions—the absolute value function $f(x) = |x|$ and the rectified linear function $h(x) = \\max(0,x)$—into the state-space model. Since a valid Jacobian does not exist at points of non-differentiability, the EKF's propagation rules break down, forcing reliance on ad-hoc conventions that can lead to significant errors. By deriving the exact propagated moments for these non-smooth transformations and comparing them to the EKF's approximations, you will witness a hard limit of the filter's applicability [@problem_id:3397733] and appreciate the critical importance of model assumptions.", "problem": "Consider scalar state-space mappings that are non-differentiable or piecewise, and assess the limitations of the Extended Kalman Filter (EKF) for such systems through principled moment computations. Use the following base: the EKF prediction for a nonlinear mapping linearizes a function at the current mean using its Jacobian, and propagates the mean and covariance as if the system were linear. This presumes differentiability at the linearization point and local validity of a first-order Taylor expansion. The exact moment propagation, by contrast, is governed by the definitions of expectation and variance under the true transformation. Work entirely in unitless quantities.\n\nYou will analyze two distinct scalar mappings, each in isolation, to avoid confounding effects:\n\n1. A process (time-update) mapping with a non-differentiable state transition:\n   - State transition: $x_{k+1} = f(x_k) + w_k$, where $f(x) = |x|$, and $w_k \\sim \\mathcal{N}(0,q)$.\n   - Prior: $x_k \\sim \\mathcal{N}(\\mu,\\sigma^2)$ with $\\sigma \\ge 0$.\n   - EKF prediction convention: linearize $f$ at $x=\\mu$ with Jacobian $F = \\frac{d f}{dx}\\big|_{x=\\mu}$ taken as $F = 1$ if $\\mu  0$, $F = -1$ if $\\mu  0$, and $F = 0$ if $\\mu = 0$.\n   - EKF-predicted moments: $\\mu^{\\text{EKF}}_{\\text{pred}} = f(\\mu)$ and $\\sigma^{2,\\text{EKF}}_{\\text{pred}} = F^2 \\sigma^2 + q$.\n   - Exact-predicted moments: $\\mu^{\\text{exact}}_{\\text{pred}} = \\mathbb{E}[|X|]$ and $\\sigma^{2,\\text{exact}}_{\\text{pred}} = \\operatorname{Var}(|X|) + q$, where $X \\sim \\mathcal{N}(\\mu,\\sigma^2)$.\n   - You must derive $\\mathbb{E}[|X|]$ and $\\operatorname{Var}(|X|)$ from first principles and implement them correctly, including the degenerate case $\\sigma = 0$.\n\n2. A measurement (observation) mapping with a piecewise linear, non-differentiable measurement function:\n   - Measurement: $y = h(x) + v$, where $h(x) = \\max(0,x)$ and $v \\sim \\mathcal{N}(0,r)$.\n   - Prior: $x \\sim \\mathcal{N}(\\mu,\\sigma^2)$ with $\\sigma \\ge 0$.\n   - EKF measurement prediction convention: linearize $h$ at $x=\\mu$ with Jacobian $H = \\frac{d h}{dx}\\big|_{x=\\mu}$ taken as $H = 1$ if $\\mu  0$ and $H = 0$ if $\\mu \\le 0$.\n   - EKF-predicted measurement moments: $\\hat{y}^{\\text{EKF}} = h(\\mu)$ and $S^{\\text{EKF}} = H^2 \\sigma^2 + r$.\n   - Exact-predicted measurement moments: $\\hat{y}^{\\text{exact}} = \\mathbb{E}[\\max(0,X)]$ and $S^{\\text{exact}} = \\operatorname{Var}(\\max(0,X)) + r$, where $X \\sim \\mathcal{N}(\\mu,\\sigma^2)$.\n   - You must derive $\\mathbb{E}[\\max(0,X)]$ and $\\operatorname{Var}(\\max(0,X))$ from first principles via truncated Gaussian integrals and implement them correctly, including the degenerate case $\\sigma = 0$.\n\nFor each test case below, compute the discrepancy between the EKF-predicted moments and the exact moments. Specifically:\n- For the process mapping, output the pair $[\\mu^{\\text{EKF}}_{\\text{pred}} - \\mu^{\\text{exact}}_{\\text{pred}},\\ \\sigma^{2,\\text{EKF}}_{\\text{pred}} - \\sigma^{2,\\text{exact}}_{\\text{pred}}]$.\n- For the measurement mapping, output the pair $[\\hat{y}^{\\text{EKF}} - \\hat{y}^{\\text{exact}},\\ S^{\\text{EKF}} - S^{\\text{exact}}]$.\n\nImplement a program that evaluates the following test suite:\n\n- Process (prediction) test cases $(\\mu,\\sigma,q)$:\n  1. $(0, 1, 0.1)$\n  2. $(2, 0.5, 0.01)$\n  3. $(-2, 1.5, 0)$\n  4. $(0, 0.01, 0)$\n\n- Measurement (prediction) test cases $(\\mu,\\sigma,r)$:\n  5. $(0, 1, 0.1)$\n  6. $(2, 0.5, 0.01)$\n  7. $(-2, 0.75, 0)$\n  8. $(0, 0.01, 0)$\n\nAll computations are unitless. Angles are not involved.\n\nYour program should produce a single line of output containing all results as a comma-separated list enclosed in square brackets, in the following order and format:\n- First list the two floats for test case $1$, then the two floats for test case $2$, and so on, through test case $8$.\n- That is, output a list of length $16$: $[\\Delta_1^{(m)},\\Delta_1^{(v)}, \\ldots, \\Delta_8^{(m)},\\Delta_8^{(v)}]$, where each $\\Delta^{(m)}$ is the mean discrepancy and each $\\Delta^{(v)}$ is the variance discrepancy for the corresponding case.\n\nThe derivations must start from the fundamental definitions of expectation and variance, the Gaussian probability density function, and properties of truncated Gaussian integrals. Do not assume differentiability where it does not hold; instead, apply the stated EKF linearization conventions at non-differentiable points. No other shortcuts are allowed.", "solution": "The problem requires an analysis of the limitations of the Extended Kalman Filter (EKF) when applied to systems with non-differentiable nonlinearities. We will compute the exact first and second moments of the state and measurement predictions and compare them to the approximations generated by the EKF. The analysis is divided into two parts, one for a non-differentiable process model and one for a non-differentiable measurement model.\n\nLet a scalar random variable $X$ follow a Gaussian distribution, $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with probability density function (PDF) $p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$. We denote the standard normal PDF as $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$ and the standard normal cumulative distribution function (CDF) as $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) dt$.\n\n### 1. Analysis of the Process Model with $f(x) = |x|$\n\nThe process model is given by $x_{k+1} = |x_k| + w_k$, where the prior state $x_k \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and the process noise $w_k \\sim \\mathcal{N}(0, q)$. We are asked to compare the EKF-predicted moments of $x_{k+1}$ with the exact moments.\n\nFirst, consider the degenerate case where $\\sigma = 0$. The state $x_k$ is deterministically equal to $\\mu$. The transformed state is $|x_k| = |\\mu|$. The exact moments of the transformed state before adding noise are $\\mathbb{E}[|\\mu|] = |\\mu|$ and $\\operatorname{Var}(|\\mu|) = 0$.\n\nFor the general case where $\\sigma  0$, we must compute the moments of the random variable $Y = |X|$, where $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n\n**Exact Second Moment:**\nThe second moment of $Y$ is $\\mathbb{E}[Y^2] = \\mathbb{E}[|X|^2]$. Since $|X|^2 = X^2$, this is equivalent to the second moment of $X$. For any random variable, $\\mathbb{E}[X^2] = \\operatorname{Var}(X) + (\\mathbb{E}[X])^2$. Given $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, we have:\n$$ \\mathbb{E}[|X|^2] = \\sigma^2 + \\mu^2 $$\n\n**Exact First Moment:**\nThe first moment, or expectation, of $Y = |X|$ is given by the definition of expectation:\n$$ \\mathbb{E}[|X|] = \\int_{-\\infty}^{\\infty} |x| p(x) dx = \\int_{0}^{\\infty} x p(x) dx + \\int_{-\\infty}^{0} (-x) p(x) dx $$\nLet's evaluate $\\int_a^b x p(x) dx$. Using the substitution $z = (x-\\mu)/\\sigma$, we have $x = \\sigma z + \\mu$ and $p(x)dx = \\phi(z)dz$.\n$$ \\int_a^b x p(x) dx = \\int_{(a-\\mu)/\\sigma}^{(b-\\mu)/\\sigma} (\\sigma z + \\mu) \\phi(z) dz = \\sigma [-\\phi(z)]_{(a-\\mu)/\\sigma}^{(b-\\mu)/\\sigma} + \\mu [\\Phi(z)]_{(a-\\mu)/\\sigma}^{(b-\\mu)/\\sigma} $$\nApplying this to our integrals:\n$$ \\int_{0}^{\\infty} x p(x) dx = \\sigma\\phi(-\\mu/\\sigma) + \\mu(1-\\Phi(-\\mu/\\sigma)) = \\sigma\\phi(\\mu/\\sigma) + \\mu\\Phi(\\mu/\\sigma) $$\n$$ \\int_{-\\infty}^{0} x p(x) dx = -\\sigma\\phi(-\\mu/\\sigma) + \\mu\\Phi(-\\mu/\\sigma) = -\\sigma\\phi(\\mu/\\sigma) + \\mu(1-\\Phi(\\mu/\\sigma)) $$\nTherefore,\n$$ \\mathbb{E}[|X|] = (\\sigma\\phi(\\mu/\\sigma) + \\mu\\Phi(\\mu/\\sigma)) - (-\\sigma\\phi(\\mu/\\sigma) + \\mu(1-\\Phi(\\mu/\\sigma))) $$\n$$ \\mathbb{E}[|X|] = 2\\sigma\\phi(\\mu/\\sigma) + \\mu(2\\Phi(\\mu/\\sigma) - 1) $$\nThis expression is for $\\sigma  0$. If $\\mu = 0$, it simplifies to $\\mathbb{E}[|X|] = 2\\sigma\\phi(0) = \\sigma\\sqrt{2/\\pi}$.\n\n**Summary of Exact and EKF Moments (Process):**\nThe exact moments of $|X|$ are:\n$$ \\mu_{|X|} = \\mathbb{E}[|X|] = \\begin{cases} 2\\sigma\\phi(\\mu/\\sigma) + \\mu(2\\Phi(\\mu/\\sigma) - 1)  \\text{if } \\sigma  0 \\\\ |\\mu|  \\text{if } \\sigma = 0 \\end{cases} $$\n$$ \\sigma^2_{|X|} = \\operatorname{Var}(|X|) = \\mathbb{E}[|X|^2] - (\\mathbb{E}[|X|])^2 = (\\mu^2 + \\sigma^2) - (\\mu_{|X|})^2 $$\nThe total predicted moments, including process noise $w_k$, are:\n$$ \\mu^{\\text{exact}}_{\\text{pred}} = \\mu_{|X|} $$\n$$ \\sigma^{2,\\text{exact}}_{\\text{pred}} = \\sigma^2_{|X|} + q $$\nThe EKF uses the linearization $F = \\frac{d|x|}{dx}|_{x=\\mu}$, with the convention $F = 1$ if $\\mu0$, $F=-1$ if $\\mu0$, and $F=0$ if $\\mu=0$. The EKF-predicted moments are:\n$$ \\mu^{\\text{EKF}}_{\\text{pred}} = | \\mu | $$\n$$ \\sigma^{2,\\text{EKF}}_{\\text{pred}} = F^2\\sigma^2 + q $$\nThe discrepancy is calculated as $[\\mu^{\\text{EKF}}_{\\text{pred}} - \\mu^{\\text{exact}}_{\\text{pred}},\\ \\sigma^{2,\\text{EKF}}_{\\text{pred}} - \\sigma^{2,\\text{exact}}_{\\text{pred}}]$.\n\n### 2. Analysis of the Measurement Model with $h(x) = \\max(0, x)$\n\nThe measurement model is $y = \\max(0, x) + v$, where the prior state $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and measurement noise $v \\sim \\mathcal{N}(0, r)$. We compare the EKF-predicted measurement moments with the exact ones.\n\nLet $Y = \\max(0, X)$. For $\\sigma = 0$, $Y$ is deterministically $\\max(0, \\mu)$, so $\\mathbb{E}[Y] = \\max(0, \\mu)$ and $\\operatorname{Var}(Y)=0$.\n\nFor $\\sigma  0$:\n\n**Exact First Moment:**\nThe expectation of $Y$ is:\n$$ \\mathbb{E}[\\max(0, X)] = \\int_{-\\infty}^{\\infty} \\max(0, x) p(x) dx = \\int_{0}^{\\infty} x p(x) dx $$\nThis is the same integral as the first one calculated for the process model.\n$$ \\mathbb{E}[\\max(0, X)] = \\sigma\\phi(\\mu/\\sigma) + \\mu\\Phi(\\mu/\\sigma) $$\n\n**Exact Second Moment:**\nThe second moment of $Y$ is:\n$$ \\mathbb{E}[(\\max(0, X))^2] = \\int_{-\\infty}^{\\infty} (\\max(0, x))^2 p(x) dx = \\int_{0}^{\\infty} x^2 p(x) dx $$\nTo evaluate this, we use the standard normal variable $Z = (X-\\mu)/\\sigma \\sim \\mathcal{N}(0,1)$, so $X=\\sigma Z+\\mu$. The integral becomes:\n$$ \\mathbb{E}[Y^2] = \\mathbb{E}[(\\sigma Z+\\mu)^2 \\mathbb{I}(Z  -\\mu/\\sigma)] = \\sigma^2 \\mathbb{E}[Z^2 \\mathbb{I}(Z\\alpha)] + 2\\mu\\sigma \\mathbb{E}[Z\\mathbb{I}(Z\\alpha)] + \\mu^2 \\mathbb{E}[\\mathbb{I}(Z\\alpha)] $$\nwhere $\\alpha = -\\mu/\\sigma$. We have $\\mathbb{E}[\\mathbb{I}(Z\\alpha)] = 1-\\Phi(\\alpha)=\\Phi(\\mu/\\sigma)$, $\\mathbb{E}[Z\\mathbb{I}(Z\\alpha)] = \\phi(\\alpha)=\\phi(\\mu/\\sigma)$, and $\\mathbb{E}[Z^2\\mathbb{I}(Z\\alpha)] = [ -z\\phi(z)+\\Phi(z) ]_\\alpha^\\infty = \\alpha\\phi(\\alpha)+\\Phi(\\infty)-\\Phi(\\alpha) = \\alpha\\phi(\\alpha)+1-\\Phi(\\alpha)$.\nSubstituting these gives:\n$$ \\mathbb{E}[(\\max(0, X))^2] = (\\mu^2 + \\sigma^2)\\Phi(\\mu/\\sigma) + \\mu\\sigma\\phi(\\mu/\\sigma) $$\n\n**Summary of Exact and EKF Moments (Measurement):**\nThe exact moments of $\\max(0,X)$ for $\\sigma0$ are:\n$$ \\hat{y}^{\\text{exact}} = \\mathbb{E}[\\max(0,X)] = \\mu\\Phi(\\mu/\\sigma) + \\sigma\\phi(\\mu/\\sigma) $$\n$$ \\operatorname{Var}(\\max(0,X)) = \\mathbb{E}[(\\max(0,X))^2] - (\\hat{y}^{\\text{exact}})^2 $$\nFor $\\sigma=0$, they are $\\max(0,\\mu)$ and $0$ respectively.\nThe total predicted measurement mean and innovation covariance are:\n$$ \\hat{y}^{\\text{exact}} = \\mathbb{E}[\\max(0, X)] $$\n$$ S^{\\text{exact}} = \\operatorname{Var}(\\max(0, X)) + r $$\nThe EKF uses the linearization $H = \\frac{d\\max(0, x)}{dx}|_{x=\\mu}$, with the convention $H = 1$ if $\\mu0$ and $H=0$ if $\\mu \\le 0$. The EKF-predicted measurement moments are:\n$$ \\hat{y}^{\\text{EKF}} = \\max(0, \\mu) $$\n$$ S^{\\text{EKF}} = H^2\\sigma^2 + r $$\nThe discrepancy is calculated as $[\\hat{y}^{\\text{EKF}} - \\hat{y}^{\\text{exact}}, S^{\\text{EKF}} - S^{\\text{exact}}]$.\n\nThese derivations provide the necessary formulas to evaluate the discrepancy between the EKF approximation and the exact moment propagation for the specified non-differentiable systems. The implementation will proceed based on these results.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_process_discrepancy(mu, sigma, q):\n    \"\"\"\n    Computes the discrepancy between EKF and exact moments for the process model.\n    Process model: x_{k+1} = |x_k| + w_k, w_k ~ N(0, q)\n    Prior: x_k ~ N(mu, sigma^2)\n    \"\"\"\n    # EKF Prediction\n    mu_ekf = np.abs(mu)\n    if mu  0:\n        F = 1.0\n    elif mu  0:\n        F = -1.0\n    else:  # mu == 0\n        F = 0.0\n    var_ekf = F**2 * sigma**2 + q\n\n    # Exact Moment Prediction\n    if sigma == 0:\n        mu_exact_transform = np.abs(mu)\n        var_exact_transform = 0.0\n    else:\n        # Standardized variable xi = mu / sigma\n        xi = mu / sigma\n        phi_xi = norm.pdf(xi)\n        Phi_xi = norm.cdf(xi)\n        \n        # Mean of |X| where X ~ N(mu, sigma^2)\n        # Formula: E[|X|] = 2*sigma*phi(mu/sigma) + mu*(2*Phi(mu/sigma) - 1)\n        mu_exact_transform = 2 * sigma * phi_xi + mu * (2 * Phi_xi - 1)\n        \n        # Variance of |X|\n        # Var[|X|] = E[|X|^2] - (E[|X|])^2 = (mu^2 + sigma^2) - (E[|X|])^2\n        e_abs_x_sq = mu**2 + sigma**2\n        var_exact_transform = e_abs_x_sq - mu_exact_transform**2\n\n    mu_exact = mu_exact_transform\n    var_exact = var_exact_transform + q\n\n    # Discrepancy\n    delta_mu = mu_ekf - mu_exact\n    delta_var = var_ekf - var_exact\n    \n    return [delta_mu, delta_var]\n\n\ndef calculate_measurement_discrepancy(mu, sigma, r):\n    \"\"\"\n    Computes the discrepancy between EKF and exact moments for the measurement model.\n    Measurement model: y = max(0, x) + v, v ~ N(0, r)\n    Prior: x ~ N(mu, sigma^2)\n    \"\"\"\n    # EKF Measurement Prediction\n    y_hat_ekf = np.maximum(0, mu)\n    if mu  0:\n        H = 1.0\n    else:  # mu = 0\n        H = 0.0\n    S_ekf = H**2 * sigma**2 + r\n\n    # Exact Measurement Moment Prediction\n    if sigma == 0:\n        y_hat_exact_transform = np.maximum(0, mu)\n        var_exact_transform = 0.0\n    else:\n        # Standardized variable xi = mu / sigma\n        xi = mu / sigma\n        phi_xi = norm.pdf(xi)\n        Phi_xi = norm.cdf(xi)\n        \n        # Mean of max(0, X) where X ~ N(mu, sigma^2)\n        # Formula: E[max(0,X)] = mu*Phi(mu/sigma) + sigma*phi(mu/sigma)\n        y_hat_exact_transform = mu * Phi_xi + sigma * phi_xi\n        \n        # Variance of max(0, X)\n        # Var[Y] = E[Y^2] - (E[Y])^2\n        # E[Y^2] = (mu^2 + sigma^2)*Phi(mu/sigma) + mu*sigma*phi(mu/sigma)\n        e_max_0_x_sq = (mu**2 + sigma**2) * Phi_xi + mu * sigma * phi_xi\n        var_exact_transform = e_max_0_x_sq - y_hat_exact_transform**2\n\n    y_hat_exact = y_hat_exact_transform\n    S_exact = var_exact_transform + r\n    \n    # Discrepancy\n    delta_y_hat = y_hat_ekf - y_hat_exact\n    delta_S = S_ekf - S_exact\n\n    return [delta_y_hat, delta_S]\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    process_test_cases = [\n        (0.0, 1.0, 0.1),\n        (2.0, 0.5, 0.01),\n        (-2.0, 1.5, 0.0),\n        (0.0, 0.01, 0.0),\n    ]\n\n    measurement_test_cases = [\n        (0.0, 1.0, 0.1),\n        (2.0, 0.5, 0.01),\n        (-2.0, 0.75, 0.0),\n        (0.0, 0.01, 0.0),\n    ]\n\n    results = []\n    \n    for case in process_test_cases:\n        mu, sigma, q = case\n        discrepancy = calculate_process_discrepancy(mu, sigma, q)\n        results.extend(discrepancy)\n\n    for case in measurement_test_cases:\n        mu, sigma, r = case\n        discrepancy = calculate_measurement_discrepancy(mu, sigma, r)\n        results.extend(discrepancy)\n\n    # Format output as a comma-separated list of floats in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3397733"}]}