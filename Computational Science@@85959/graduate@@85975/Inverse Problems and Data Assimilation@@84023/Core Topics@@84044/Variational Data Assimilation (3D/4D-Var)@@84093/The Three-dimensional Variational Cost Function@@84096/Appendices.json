{"hands_on_practices": [{"introduction": "This first practice establishes the foundation of three-dimensional variational (3D-Var) data assimilation by framing it as a weighted least-squares problem. You will derive the analytical solution for the optimal state, known as the analysis, which optimally balances information from a prior estimate (the background) and new measurements (the observations). By implementing this solution, you will gain direct, quantitative insight into how the background and observation error covariances, $B$ and $R$, dictate the influence of each information source on the final result [@problem_id:3152341].", "problem": "You are asked to cast Three-Dimensional Variational (3DVar) data assimilation as a weighted least-squares problem and implement a program that computes the unique minimizer for a set of test cases. The objective is to rigorously connect the formulation to weighted least squares and to quantify how the relative sizes of the background error covariance and the observation error covariance influence the result.\n\nStart from the following fundamental basis:\n- The weighted squared norm of a vector $v$ with respect to a symmetric positive definite matrix $W$ is defined as $\\lVert v \\rVert_{W}^{2} = v^{\\top} W v$.\n- For a linear model mapping vector $x$ to observations $Hx$, and data $y$, the goal of weighted least squares is to minimize a quadratic functional that sums a data-misfit term and a regularization term, each with a symmetric positive definite weight.\n\nIn 3DVar, the analysis state $x$ is defined as the minimizer of the cost\n$$\nJ(x) = \\lVert H x - y \\rVert_{R^{-1}}^{2} + \\lVert x - x_b \\rVert_{B^{-1}}^{2},\n$$\nwhere $x_b$ is the background (prior) state, $B$ is the background error covariance, and $R$ is the observation error covariance. Both $B$ and $R$ are symmetric positive definite. The matrix $H$ is a known linear observation operator.\n\nTasks:\n1. Derive, from first principles using the definitions above and basic multivariate calculus of quadratic forms, the necessary and sufficient optimality condition for the minimizer $x$ of $J(x)$ by setting the gradient to zero. Show that the resulting linear system is symmetric positive definite and therefore has a unique solution.\n2. Based on your derivation, design an algorithm that computes the minimizer using only linear system solves with symmetric positive definite matrices and matrix multiplications. Your algorithm must not form any matrix inverses explicitly. Your algorithm must be valid for any dimensions where $B$ and $R$ are symmetric positive definite and $H$ is any real matrix with compatible dimensions.\n3. Implement a complete, runnable program that evaluates your algorithm on the following test suite. For each test, compute the analysis vector $x$ that minimizes $J(x)$:\n   - Test A (happy path, balanced weights): state dimension $n = 2$, observation dimension $m = 2$,\n     $$\n     H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}.\n     $$\n   - Test B (background-dominated: small $B$, large $R$): $n = 2$, $m = 2$,\n     $$\n     H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 100 \\\\ -100 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 0.01  0 \\\\ 0  0.01 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 10  0 \\\\ 0  10 \\end{bmatrix}.\n     $$\n   - Test C (observation-dominated: large $B$, small $R$): $n = 2$, $m = 2$,\n     $$\n     H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 100 \\\\ -100 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 100  0 \\\\ 0  100 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 0.01  0 \\\\ 0  0.01 \\end{bmatrix}.\n     $$\n   - Test D (partial observation, unobserved component filled by the background): $n = 3$, $m = 2$,\n     $$\n     H = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 1 \\\\ 1 \\\\ 5 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 4  0  0 \\\\ 0  1  0 \\\\ 0  0  0.25 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}.\n     $$\n   - Test E (correlated errors): $n = 2$, $m = 2$,\n     $$\n     H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 2  0.6 \\\\ 0.6  1 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 0.5  0.2 \\\\ 0.2  0.5 \\end{bmatrix}.\n     $$\n4. Output specification:\n   - Your program must produce a single line of output containing the results aggregated over all tests as a comma-separated list enclosed in square brackets.\n   - Each element of the list must be the analysis vector $x$ for that test as a list in the same order as specified above (Tests A through E).\n   - Format: no spaces anywhere in the line. Print each component with exactly $6$ digits after the decimal point.\n   - Example of the required pattern (illustrative only): $[[x_{1,1},x_{1,2}],\\,[x_{2,1},x_{2,2}],\\dots]$. If a state has dimension $3$, it appears as $[x_{i,1},x_{i,2},x_{i,3}]$ within the outer list.\n   - The final output must therefore be a single string representing a list of lists of floats with fixed $6$-decimal formatting, e.g., $[[0.123456,-7.000000],[1.500000,2.250000]]$ for two hypothetical $2$-component cases.\n\nYour solution must be general and not hard-coded to these particular values except for the test suite definition. The output values for each test must be lists of floats, rounded and formatted as specified.", "solution": "### Step 1: Derivation of the Optimality Condition\n\nThe goal is to find the analysis state vector $x$ that minimizes the three-dimensional variational (3DVar) cost function $J(x)$. The cost function is given as:\n$$\nJ(x) = \\lVert H x - y \\rVert_{R^{-1}}^{2} + \\lVert x - x_b \\rVert_{B^{-1}}^{2}\n$$\nwhere $x$ is the state vector of dimension $n$, $x_b$ is the background state vector (dimension $n$), $y$ is the observation vector of dimension $m$, $H$ is the linear observation operator (an $m \\times n$ matrix), $B$ is the background error covariance matrix ($n \\times n$), and $R$ is the observation error covariance matrix ($m \\times m$). Both $B$ and $R$ are symmetric positive definite (SPD) matrices, which implies their inverses, $B^{-1}$ and $R^{-1}$, are also SPD.\n\nUsing the definition of the weighted squared norm, $\\lVert v \\rVert_{W}^{2} = v^{\\top} W v$, we can expand the cost function:\n$$\nJ(x) = (H x - y)^{\\top} R^{-1} (H x - y) + (x - x_b)^{\\top} B^{-1} (x - x_b)\n$$\nTo find the minimizer of $J(x)$, which is a quadratic and convex function of $x$, we compute its gradient with respect to $x$ and set it to zero.\nFirst, we expand the two terms:\n$$\n(H x - y)^{\\top} R^{-1} (H x - y) = x^{\\top}H^{\\top}R^{-1}Hx - x^{\\top}H^{\\top}R^{-1}y - y^{\\top}R^{-1}Hx + y^{\\top}R^{-1}y\n$$\n$$\n(x - x_b)^{\\top} B^{-1} (x - x_b) = x^{\\top}B^{-1}x - x^{\\top}B^{-1}x_b - x_b^{\\top}B^{-1}x + x_b^{\\top}B^{-1}x_b\n$$\nSince $x^{\\top}H^{\\top}R^{-1}y$ is a scalar, it is equal to its transpose, $(x^{\\top}H^{\\top}R^{-1}y)^{\\top} = y^{\\top}(R^{-1})^{\\top}H x$. As $R^{-1}$ is symmetric, this becomes $y^{\\top}R^{-1}H x$. A similar identity holds for the term involving $B^{-1}$. Thus, we can write $J(x)$ as:\n$$\nJ(x) = x^{\\top}H^{\\top}R^{-1}Hx - 2 y^{\\top}R^{-1}Hx + y^{\\top}R^{-1}y + x^{\\top}B^{-1}x - 2 x_b^{\\top}B^{-1}x + x_b^{\\top}B^{-1}x_b\n$$\nNow, we compute the gradient $\\nabla_x J(x)$ using the standard matrix calculus identities $\\nabla_x(c^{\\top}x) = c$ and $\\nabla_x(x^{\\top}Ax) = (A + A^{\\top})x$.\n$$\n\\nabla_x J(x) = \\nabla_x(x^{\\top}H^{\\top}R^{-1}Hx) - \\nabla_x(2 y^{\\top}R^{-1}Hx) + \\nabla_x(x^{\\top}B^{-1}x) - \\nabla_x(2 x_b^{\\top}B^{-1}x)\n$$\nThe matrices $H^{\\top}R^{-1}H$ and $B^{-1}$ are symmetric. Therefore, $\\nabla_x(x^{\\top}(H^{\\top}R^{-1}H)x) = 2(H^{\\top}R^{-1}H)x$ and $\\nabla_x(x^{\\top}B^{-1}x) = 2B^{-1}x$.\nThe linear terms give $\\nabla_x(2 y^{\\top}R^{-1}Hx) = 2(y^{\\top}R^{-1}H)^{\\top} = 2H^{\\top}(R^{-1})^{\\top}y = 2H^{\\top}R^{-1}y$ and $\\nabla_x(2 x_b^{\\top}B^{-1}x) = 2B^{-1}x_b$.\nCombining these, the gradient is:\n$$\n\\nabla_x J(x) = 2 H^{\\top}R^{-1}H x - 2 H^{\\top}R^{-1}y + 2 B^{-1}x - 2 B^{-1}x_b\n$$\nSetting the gradient to zero provides the necessary condition for a minimum:\n$$\nH^{\\top}R^{-1}H x + B^{-1}x = H^{\\top}R^{-1}y + B^{-1}x_b\n$$\nFactoring out $x$ on the left-hand side, we get the linear system for the analysis state $x$:\n$$\n(H^{\\top}R^{-1}H + B^{-1})x = H^{\\top}R^{-1}y + B^{-1}x_b\n$$\nThis is the necessary optimality condition. To show it is also sufficient for a unique minimum, we must demonstrate that the Hessian matrix of $J(x)$ is positive definite. The Hessian is $\\nabla_x^2 J(x) = 2(H^{\\top}R^{-1}H + B^{-1})$.\nThe matrix $H^{\\top}R^{-1}H + B^{-1}$ is symmetric because $B^{-1}$ is symmetric and $(H^{\\top}R^{-1}H)^{\\top} = H^{\\top}(R^{-1})^{\\top}(H^{\\top})^{\\top} = H^{\\top}R^{-1}H$.\nTo prove positive definiteness, consider any non-zero vector $v \\in \\mathbb{R}^n$:\n$$\nv^{\\top}(H^{\\top}R^{-1}H + B^{-1})v = v^{\\top}H^{\\top}R^{-1}Hv + v^{\\top}B^{-1}v = (Hv)^{\\top}R^{-1}(Hv) + v^{\\top}B^{-1}v\n$$\nSince $R$ is SPD, $R^{-1}$ is SPD, so $(Hv)^{\\top}R^{-1}(Hv) \\ge 0$. Since $B$ is SPD, $B^{-1}$ is SPD, so $v^{\\top}B^{-1}v  0$ for $v \\neq 0$.\nThus, $v^{\\top}(H^{\\top}R^{-1}H + B^{-1})v  0$ for all $v \\neq 0$. The Hessian is positive definite, which means $J(x)$ is a strictly convex function and has a unique global minimum. The linear system derived above therefore has a unique solution for $x$.\n\n### Step 2: Algorithm Design\n\nThe derived linear system involves matrix inverses $B^{-1}$ and $R^{-1}$. A numerically stable and efficient algorithm should avoid explicit computation of matrix inverses. An alternative, equivalent formulation, often called the observation space solution, avoids this. This formulation is particularly advantageous when the observation space dimension $m$ is much smaller than the state space dimension $n$. We can derive it using the Sherman-Morrison-Woodbury identity, but a more direct algebraic verification is as follows. The solution can be expressed in terms of an increment to the background state:\n$$\nx = x_b + \\delta x\n$$\nwhere a well-known result from data assimilation theory gives the increment $\\delta x$ as:\n$$\n\\delta x = B H^{\\top} (H B H^{\\top} + R)^{-1} (y - H x_b)\n$$\nThis formula computes the analysis state $x$ without ever forming $B^{-1}$ or $R^{-1}$. The only matrix inverse required is of $(H B H^{\\top} + R)$. The matrix $H B H^{\\top} + R$ is an $m \\times m$ matrix that is guaranteed to be symmetric positive definite because $R$ is SPD and $HBH^{\\top}$ is symmetric positive semi-definite. Instead of explicitly inverting this matrix, we solve a linear system.\n\nThe algorithm is as follows:\n1.  Compute the innovation (or observation-minus-forecast residual) vector: $d = y - H x_b$.\n2.  Compute the matrix product $M_1 = H @ B @ H^{\\top}$.\n3.  Form the matrix for the linear system solver: $M_2 = M_1 + R$.\n4.  Solve the $m \\times m$ linear system $M_2 v = d$ for the vector $v$. Here, $v = (H B H^{\\top} + R)^{-1} d$.\n5.  Compute the state increment $\\delta x = B @ H^{\\top} @ v$.\n6.  Compute the final analysis state: $x = x_b + \\delta x$.\n\nThis algorithm relies only on matrix multiplications and solving one linear system with a symmetric positive definite matrix, fully complying with the problem constraints.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve_3dvar(H: np.ndarray, xb: np.ndarray, y: np.ndarray, B: np.ndarray, R: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the 3D-Var analysis state x that minimizes the cost function J(x).\n\n    The analysis state x is given by the formula:\n    x = xb + B @ H.T @ inv(H @ B @ H.T + R) @ (y - H @ xb)\n    \n    This implementation avoids explicit matrix inversion by solving a linear system.\n\n    Args:\n        H: Observation operator matrix.\n        xb: Background state vector.\n        y: Observation vector.\n        B: Background error covariance matrix.\n        R: Observation error covariance matrix.\n\n    Returns:\n        The analysis state vector x.\n    \"\"\"\n    # 1. Compute the innovation vector\n    d = y - H @ xb\n\n    # 2. Compute the matrix for the linear system in observation space\n    # This matrix is H*B*H' + R\n    HBHt = H @ B @ H.T\n    M = HBHt + R\n    \n    # 3. Solve the linear system M*v = d for v\n    # This is equivalent to v = inv(H*B*H' + R) * d\n    # Since M is symmetric positive definite, we can use a specialized solver.\n    # However, linalg.solve is general and robust.\n    v = linalg.solve(M, d, assume_a='pos')\n\n    # 4. Compute the state increment\n    # This is B*H'*v\n    delta_x = B @ H.T @ v\n\n    # 5. Compute the final analysis state\n    x = xb + delta_x\n\n    return x\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the 3D-Var solver.\n    \"\"\"\n    # Test cases defined in the problem statement\n    test_cases = [\n        # Test A: Happy path, balanced weights\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"xb\": np.array([0.0, 0.0]),\n            \"y\": np.array([1.0, -2.0]),\n            \"B\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"R\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n        },\n        # Test B: Background-dominated\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"xb\": np.array([2.0, 2.0]),\n            \"y\": np.array([100.0, -100.0]),\n            \"B\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n            \"R\": np.array([[10.0, 0.0], [0.0, 10.0]]),\n        },\n        # Test C: Observation-dominated\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"xb\": np.array([2.0, 2.0]),\n            \"y\": np.array([100.0, -100.0]),\n            \"B\": np.array([[100.0, 0.0], [0.0, 100.0]]),\n            \"R\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n        },\n        # Test D: Partial observation\n        {\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n            \"xb\": np.array([1.0, 1.0, 5.0]),\n            \"y\": np.array([10.0, -10.0]),\n            \"B\": np.array([[4.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.25]]),\n            \"R\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n        },\n        # Test E: Correlated errors\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"xb\": np.array([0.0, 0.0]),\n            \"y\": np.array([1.0, 2.0]),\n            \"B\": np.array([[2.0, 0.6], [0.6, 1.0]]),\n            \"R\": np.array([[0.5, 0.2], [0.2, 0.5]]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        x_analysis = solve_3dvar(case[\"H\"], case[\"xb\"], case[\"y\"], case[\"B\"], case[\"R\"])\n        results.append(x_analysis)\n\n    # Format the output string as per the requirements\n    # A single line, comma-separated list of lists, no spaces, 6 decimal places.\n    formatted_results = []\n    for res_vec in results:\n        formatted_vec = [f\"{val:.6f}\" for val in res_vec]\n        formatted_results.append(f\"[{','.join(formatted_vec)}]\")\n    \n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3152341"}, {"introduction": "While direct solutions are useful for small systems, real-world applications like weather forecasting involve state vectors with millions of components, making the explicit construction and inversion of the Hessian matrix computationally infeasible. This exercise introduces the standard technique for overcoming this challenge: matrix-free iterative solvers. You will implement the Conjugate Gradient algorithm to solve the 3D-Var linear system by defining a function for the Hessian-vector product, thereby learning the essential numerical method that makes large-scale data assimilation practical [@problem_id:3426291].", "problem": "Consider the Three-Dimensional Variational (3D-Var) data assimilation cost function defined on state vectors $x \\in \\mathbb{R}^n$ by\n$$\nJ(x) \\equiv \\tfrac{1}{2}\\,(x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2}\\,\\bigl(Hx - y\\bigr)^\\top R^{-1} \\bigl(Hx - y\\bigr),\n$$\nwhere $x_b \\in \\mathbb{R}^n$ is a background state, $y \\in \\mathbb{R}^m$ is an observation vector, $B \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite background error covariance matrix, $R \\in \\mathbb{R}^{m \\times m}$ is a symmetric positive definite observation error covariance matrix, and $H \\in \\mathbb{R}^{m \\times n}$ is a linear observation operator. Assume all matrices are well-conditioned enough to ensure numerical stability under double precision arithmetic.\n\nYour task is to design a matrix-free algorithm that computes the analysis increment $\\delta x \\in \\mathbb{R}^n$ that minimizes $J(x_b + \\delta x)$ by solving the first-order optimality condition linearized around $x_b$. Use the following principles as your fundamental base:\n\n- The gradient of a differentiable scalar functional is defined by its first variation applied to arbitrary directions.\n- The Hessian–vector product is defined as the directional derivative of the gradient applied to an arbitrary vector.\n- The linear algebra identities for adjoints in Euclidean inner products and basic rules of differentials for linear mappings.\n\nYou must:\n\n1. Starting from the above definition of $J(x)$ and the principles listed, derive a matrix-free expression for the Hessian–vector product of $J$ at $x_b$, i.e., a procedure that maps any vector $v \\in \\mathbb{R}^n$ to $H_J[v] \\in \\mathbb{R}^n$, without explicitly forming any dense Hessian matrix. Your derivation should rely only on applying $B^{-1}$, $R^{-1}$, $H$, and $H^\\top$ to vectors.\n\n2. Implement a Conjugate Gradient (CG) method that, using only your Hessian–vector product operator and a right-hand side obtained from the linearized first-order optimality condition at $x_b$, computes the increment $\\delta x$ that minimizes $J(x_b + \\delta x)$. Use a zero initial guess, an absolute or relative residual tolerance at most $10^{-10}$, and a maximum of at least $100$ iterations. Do not form the Hessian explicitly inside CG.\n\n3. For validation, for each test case explicitly form the dense linear system associated with the linearized first-order condition and solve it directly with a dense linear solver to obtain a reference increment $\\delta x_{\\mathrm{ref}}$. Compare your matrix-free CG increment $\\delta x_{\\mathrm{cg}}$ to $\\delta x_{\\mathrm{ref}}$ using the relative error\n$$\n\\varepsilon \\equiv \\frac{\\lVert \\delta x_{\\mathrm{cg}} - \\delta x_{\\mathrm{ref}} \\rVert_2}{\\max\\bigl(\\lVert \\delta x_{\\mathrm{ref}} \\rVert_2,\\, 1\\bigr)}.\n$$\n\n4. Implement all linear solves with $B$ and $R$ via linear system solves (i.e., apply $B^{-1}$ and $R^{-1}$ by solving linear systems), not by explicit matrix inversion inside the CG iterations. You may form explicit inverses only in the separate dense reference path.\n\nYou are given the following test suite. Each test case provides matrices $B$, $R$, $H$, and vectors $x_b$, $y$, with all numerical entries specified. All numbers are given in real-valued units; no physical units are required.\n\n- Test case 1 (happy path, moderate anisotropy):\n  - $n = 5$, $m = 3$.\n  - $B = \\mathrm{diag}(1.0, 2.0, 3.0, 4.0, 5.0)$.\n  - $R = \\mathrm{diag}(0.5, 0.7, 1.1)$.\n  - $H = \\begin{bmatrix}\n    1.0  0.0  0.0  0.0  0.0 \\\\\n    0.0  1.0  1.0  0.0  0.0 \\\\\n    0.0  0.0  0.0  1.0  -1.0\n  \\end{bmatrix}$.\n  - $x_b = \\begin{bmatrix} 0.5 \\\\ -1.0 \\\\ 0.3 \\\\ 0.0 \\\\ 0.2 \\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -0.4 \\\\ 0.1 \\end{bmatrix}$.\n\n- Test case 2 (ill-conditioning and rank-deficient observation geometry stabilized by background):\n  - $n = 4$, $m = 2$.\n  - $B = \\mathrm{diag}(10^{-3}, 1.0, 10^{2}, 10^{-1})$.\n  - $R = \\mathrm{diag}(10^{-2}, 10^{1})$.\n  - $H = \\begin{bmatrix}\n    1.0  1.0  0.0  0.0 \\\\\n    0.0  0.0  1.0  -1.0\n  \\end{bmatrix}$.\n  - $x_b = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 2.0 \\\\ -2.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$.\n\n- Test case 3 (edge case: zero observation operator):\n  - $n = 3$, $m = 2$.\n  - $B = \\mathrm{diag}(2.0, 2.0, 2.0)$.\n  - $R = \\mathrm{diag}(1.0, 1.0)$.\n  - $H = \\begin{bmatrix}\n    0.0  0.0  0.0 \\\\\n    0.0  0.0  0.0\n  \\end{bmatrix}$.\n  - $x_b = \\begin{bmatrix} 3.0 \\\\ -2.0 \\\\ 1.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -4.0 \\end{bmatrix}$.\n\n- Test case 4 (full observation with non-diagonal background covariance):\n  - $n = 3$, $m = 3$.\n  - $B = \\begin{bmatrix}\n    2.0  0.5  0.0 \\\\\n    0.5  1.5  0.2 \\\\\n    0.0  0.2  1.0\n  \\end{bmatrix}$.\n  - $R = \\mathrm{diag}(0.05, 0.2, 0.1)$.\n  - $H = I_3$ (the $3 \\times 3$ identity matrix).\n  - $x_b = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 0.5 \\end{bmatrix}$.\n\nImplementation details and requirements:\n\n- For each test case, define the right-hand side of the linearized first-order optimality equation at $x_b$ and solve it by your matrix-free Conjugate Gradient method using only Hessian–vector products. Use a zero initial guess for $\\delta x$.\n- Independently, form the corresponding dense linear system by explicitly constructing the linearized normal operator and solve for the reference increment $\\delta x_{\\mathrm{ref}}$ using a dense solver.\n- For each test case, compute the scalar relative error $\\varepsilon$ as defined above.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list of the four relative errors, enclosed in square brackets, for example, \"[e1,e2,e3,e4]\". Each $e_k$ must be expressed as a decimal number in standard or scientific notation.\n\nNo angles or physical units are involved; all outputs are dimensionless real numbers. The program must be self-contained and must not read any input. The only allowed libraries are the Python standard library, NumPy, and SciPy, but you must not call any high-level linear solvers beyond those needed for dense validation and basic linear solves for applying $B^{-1}$ and $R^{-1}$ to vectors within the Hessian–vector product.", "solution": "The objective is to find the analysis increment $\\delta x \\in \\mathbb{R}^n$ that minimizes the three-dimensional variational (3D-Var) cost function $J(x_b + \\delta x)$, where $x = x_b + \\delta x$. The cost function is given by:\n$$\nJ(x) = \\frac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b) + \\frac{1}{2}(Hx - y)^\\top R^{-1} (Hx - y)\n$$\nSubstituting $x = x_b + \\delta x$ yields the cost function in terms of the increment $\\delta x$:\n$$\nJ(x_b + \\delta x) = \\frac{1}{2}(\\delta x)^\\top B^{-1} (\\delta x) + \\frac{1}{2}(H(x_b + \\delta x) - y)^\\top R^{-1} (H(x_b + \\delta x) - y)\n$$\nLet the innovation vector be $d = y - Hx_b$. The expression $H(x_b + \\delta x) - y$ simplifies to $H\\delta x - d$. The cost function as a functional of $\\delta x$, which we denote $\\mathcal{J}(\\delta x)$, is:\n$$\n\\mathcal{J}(\\delta x) = \\frac{1}{2}(\\delta x)^\\top B^{-1} (\\delta x) + \\frac{1}{2}(H\\delta x - d)^\\top R^{-1} (H\\delta x - d)\n$$\nThis is a quadratic functional of $\\delta x$. To find the minimum, we compute its gradient with respect to $\\delta x$ and set it to zero. The gradient is found by computing the first variation $\\delta \\mathcal{J}$ for an arbitrary perturbation $u \\in \\mathbb{R}^n$:\n$$\n\\delta \\mathcal{J}(\\delta x; u) = \\lim_{\\epsilon \\to 0} \\frac{\\mathcal{J}(\\delta x + \\epsilon u) - \\mathcal{J}(\\delta x)}{\\epsilon}\n$$\nExpanding $\\mathcal{J}(\\delta x + \\epsilon u)$ and collecting terms of order $\\epsilon$:\n$$\n\\mathcal{J}(\\delta x + \\epsilon u) = \\frac{1}{2}(\\delta x + \\epsilon u)^\\top B^{-1} (\\delta x + \\epsilon u) + \\frac{1}{2}(H(\\delta x + \\epsilon u) - d)^\\top R^{-1} (H(\\delta x + \\epsilon u) - d)\n$$\n$$\n= \\mathcal{J}(\\delta x) + \\epsilon u^\\top B^{-1} \\delta x + \\epsilon (Hu)^\\top R^{-1} (H\\delta x - d) + O(\\epsilon^2)\n$$\nUsing the adjoint property $(Hu)^\\top = u^\\top H^\\top$, the first variation is:\n$$\n\\delta \\mathcal{J}(\\delta x; u) = u^\\top \\left( B^{-1}\\delta x + H^\\top R^{-1}(H\\delta x - d) \\right)\n$$\nBy definition, the gradient $\\nabla \\mathcal{J}(\\delta x)$ is the vector satisfying $\\delta \\mathcal{J}(\\delta x; u) = u^\\top \\nabla \\mathcal{J}(\\delta x)$ for all $u$. Thus:\n$$\n\\nabla \\mathcal{J}(\\delta x) = B^{-1}\\delta x + H^\\top R^{-1}(H\\delta x - d)\n$$\nThe first-order optimality condition is $\\nabla \\mathcal{J}(\\delta x) = 0$, which gives:\n$$\nB^{-1}\\delta x + H^\\top R^{-1}H\\delta x - H^\\top R^{-1}d = 0\n$$\nRearranging gives the linear system for the optimal increment $\\delta x$:\n$$\n\\left( B^{-1} + H^\\top R^{-1}H \\right) \\delta x = H^\\top R^{-1}d\n$$\nSubstituting $d = y - Hx_b$, we get the final form:\n$$\n\\left( B^{-1} + H^\\top R^{-1}H \\right) \\delta x = H^\\top R^{-1}(y - Hx_b)\n$$\nThis is a linear system of the form $\\mathcal{H}\\delta x = b$, where $\\mathcal{H} = B^{-1} + H^\\top R^{-1}H$ is the Hessian of the cost function $\\mathcal{J}$ and $b = H^\\top R^{-1}(y - Hx_b)$ is the right-hand side. Since $B$ and $R$ are symmetric positive definite (SPD), so are $B^{-1}$ and $R^{-1}$. The Hessian $\\mathcal{H}$ is the sum of an SPD matrix ($B^{-1}$) and a symmetric positive semi-definite matrix ($H^\\top R^{-1}H$), making $\\mathcal{H}$ itself SPD and guaranteeing a unique solution.\n\nThe problem requires a matrix-free approach, using the Conjugate Gradient (CG) method. This method requires a function that computes the Hessian-vector product $\\mathcal{H}v$ for any vector $v \\in \\mathbb{R}^n$, without explicitly forming the matrix $\\mathcal{H}$.\n\nThe Hessian-vector product $\\mathcal{H}v$ is derived from the definition of the Hessian as the derivative of the gradient. The action of the Hessian on a vector $v$ is the directional derivative of the gradient in the direction $v$.\n$$\n\\mathcal{H}[v] = \\frac{d}{d\\epsilon}\\nabla \\mathcal{J}(\\delta x + \\epsilon v)\\Big|_{\\epsilon=0}\n$$\nSince $\\nabla\\mathcal{J}$ is linear in $\\delta x$, we have:\n$$\n\\nabla \\mathcal{J}(\\delta x + \\epsilon v) = B^{-1}(\\delta x + \\epsilon v) + H^\\top R^{-1}(H(\\delta x + \\epsilon v) - d)\n$$\n$$\n= \\nabla\\mathcal{J}(\\delta x) + \\epsilon (B^{-1}v + H^\\top R^{-1}Hv)\n$$\nThe derivative with respect to $\\epsilon$ at $\\epsilon=0$ is the Hessian-vector product:\n$$\n\\mathcal{H}[v] = B^{-1}v + H^\\top R^{-1}Hv\n$$\nThis operation can be implemented matrix-free as a sequence of steps:\n1. Compute $v_1 = Hv$.\n2. Compute $v_2 = R^{-1}v_1$ by solving the linear system $Rv_2 = v_1$.\n3. Compute $v_3 = H^\\top v_2$.\n4. Compute $v_4 = B^{-1}v$ by solving the linear system $Bv_4 = v$.\n5. The result is $\\mathcal{H}v = v_3 + v_4$.\n\nThe right-hand side $b = H^\\top R^{-1}(y - Hx_b)$ is also computed without explicit matrix inversions:\n1. Compute the innovation $d = y - Hx_b$.\n2. Compute $d_R = R^{-1}d$ by solving $Rd_R=d$.\n3. The result is $b = H^\\top d_R$.\n\nThe Conjugate Gradient algorithm solves the system $\\mathcal{H}\\delta x = b$ iteratively. Starting with an initial guess $\\delta x_0 = 0$, the algorithm proceeds as follows:\n1.  Initialize:\n    $\\delta x \\leftarrow 0$\n    $r \\leftarrow b - \\mathcal{H}(\\delta x) = b$\n    $p \\leftarrow r$\n    $rs_{\\text{old}} \\leftarrow r^\\top r$\n2.  Iterate for $k=0, 1, 2, \\dots$ until convergence:\n    a. Compute $\\mathcal{H}p$.\n    b. $\\alpha \\leftarrow rs_{\\text{old}} / (p^\\top \\mathcal{H}p)$.\n    c. $\\delta x \\leftarrow \\delta x + \\alpha p$.\n    d. $r \\leftarrow r - \\alpha \\mathcal{H}p$.\n    e. $rs_{\\text{new}} \\leftarrow r^\\top r$.\n    f. If $\\sqrt{rs_{\\text{new}}}$ is below a specified tolerance, stop.\n    g. $p \\leftarrow r + (rs_{\\text{new}} / rs_{\\text{old}}) p$.\n    h. $rs_{\\text{old}} \\leftarrow rs_{\\text{new}}$.\n\nFor validation, the dense linear system is constructed explicitly. The matrices $B^{-1}$ and $R^{-1}$ are computed, followed by the dense Hessian $\\mathcal{H}_{\\text{dense}} = B^{-1} + H^\\top R^{-1} H$. The right-hand side $b_{\\text{dense}}$ is computed similarly. The reference solution $\\delta x_{\\text{ref}}$ is then found by solving $\\mathcal{H}_{\\text{dense}}\\delta x_{\\text{ref}} = b_{\\text{dense}}$ using a direct solver. The relative error $\\varepsilon = \\lVert \\delta x_{\\mathrm{cg}} - \\delta x_{\\mathrm{ref}} \\rVert_2 / \\max(\\lVert \\delta x_{\\mathrm{ref}} \\rVert_2, 1)$ quantifies the accuracy of the matrix-free CG solution.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 3D-Var data assimilation problem for a suite of test cases.\n\n    For each case, it computes the analysis increment using a custom matrix-free\n    Conjugate Gradient (CG) solver. It then validates this result against a\n    reference solution obtained from a direct dense linear system solve.\n    Finally, it computes and prints the relative error between the two solutions.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: happy path, moderate anisotropy\n        {\n            \"n\": 5, \"m\": 3,\n            \"B\": np.diag([1.0, 2.0, 3.0, 4.0, 5.0]),\n            \"R\": np.diag([0.5, 0.7, 1.1]),\n            \"H\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 1.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 1.0, -1.0]\n            ]),\n            \"x_b\": np.array([0.5, -1.0, 0.3, 0.0, 0.2]),\n            \"y\": np.array([1.0, -0.4, 0.1])\n        },\n        # Case 2: ill-conditioning\n        {\n            \"n\": 4, \"m\": 2,\n            \"B\": np.diag([1e-3, 1.0, 1e2, 1e-1]),\n            \"R\": np.diag([1e-2, 1e1]),\n            \"H\": np.array([\n                [1.0, 1.0, 0.0, 0.0],\n                [0.0, 0.0, 1.0, -1.0]\n            ]),\n            \"x_b\": np.array([1.0, -1.0, 2.0, -2.0]),\n            \"y\": np.array([0.0, 1.0])\n        },\n        # Case 3: zero observation operator\n        {\n            \"n\": 3, \"m\": 2,\n            \"B\": np.diag([2.0, 2.0, 2.0]),\n            \"R\": np.diag([1.0, 1.0]),\n            \"H\": np.zeros((2, 3)),\n            \"x_b\": np.array([3.0, -2.0, 1.0]),\n            \"y\": np.array([1.0, -4.0])\n        },\n        # Case 4: full observation with non-diagonal background covariance\n        {\n            \"n\": 3, \"m\": 3,\n            \"B\": np.array([\n                [2.0, 0.5, 0.0],\n                [0.5, 1.5, 0.2],\n                [0.0, 0.2, 1.0]\n            ]),\n            \"R\": np.diag([0.05, 0.2, 0.1]),\n            \"H\": np.identity(3),\n            \"x_b\": np.array([0.0, 0.0, 0.0]),\n            \"y\": np.array([1.0, -2.0, 0.5])\n        }\n    ]\n\n    def cg_solver(A_matvec, b, x0, tol=1e-10, max_iter=100):\n        \"\"\"\n        Solves the linear system Ax=b using the Conjugate Gradient method.\n\n        Args:\n            A_matvec: A function that computes the matrix-vector product A*v.\n            b: The right-hand-side vector.\n            x0: The initial guess for the solution.\n            tol: The convergence tolerance.\n            max_iter: The maximum number of iterations.\n\n        Returns:\n            The solution vector x.\n        \"\"\"\n        x = x0.copy()\n        r = b - A_matvec(x)\n        p = r.copy()\n        rs_old = np.dot(r, r)\n\n        norm_b = np.linalg.norm(b)\n        stop_criterion = tol * norm_b if norm_b > 0 else tol\n\n        if np.sqrt(rs_old)  stop_criterion:\n            return x\n\n        for i in range(max_iter):\n            Ap = A_matvec(p)\n            alpha = rs_old / np.dot(p, Ap)\n            x += alpha * p\n            r -= alpha * Ap\n            rs_new = np.dot(r, r)\n\n            if np.sqrt(rs_new)  stop_criterion:\n                break\n            \n            p = r + (rs_new / rs_old) * p\n            rs_old = rs_new\n            \n        return x\n\n    results = []\n    for case in test_cases:\n        B, R, H, x_b, y = case[\"B\"], case[\"R\"], case[\"H\"], case[\"x_b\"], case[\"y\"]\n        n = case[\"n\"]\n\n        # --- Matrix-Free CG Path ---\n\n        def hessian_vector_product(v):\n            \"\"\"Matrix-free Hessian-vector product: (B^-1 + H' R^-1 H) v\"\"\"\n            term1 = np.linalg.solve(B, v)\n            Hv = H @ v\n            R_inv_Hv = np.linalg.solve(R, Hv)\n            term2 = H.T @ R_inv_Hv\n            return term1 + term2\n\n        # Compute right-hand side: H' R^-1 (y - H x_b)\n        innovation = y - H @ x_b\n        R_inv_innovation = np.linalg.solve(R, innovation)\n        rhs = H.T @ R_inv_innovation\n\n        # Solve for the increment using CG\n        delta_x_cg = cg_solver(hessian_vector_product, rhs, np.zeros(n), max_iter=100)\n\n        # --- Dense Reference Path ---\n        \n        # Explicitly form the Hessian and RHS\n        B_inv = np.linalg.inv(B)\n        R_inv = np.linalg.inv(R)\n        \n        Hess_dense = B_inv + H.T @ R_inv @ H\n        rhs_dense = H.T @ R_inv @ (y - H @ x_b)\n        \n        # Solve for reference increment\n        delta_x_ref = np.linalg.solve(Hess_dense, rhs_dense)\n\n        # --- Compare and calculate error ---\n        \n        diff_norm = np.linalg.norm(delta_x_cg - delta_x_ref)\n        ref_norm = np.linalg.norm(delta_x_ref)\n        \n        error = diff_norm / max(ref_norm, 1.0)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3426291"}, {"introduction": "Many real-world observation processes are nonlinear, a reality that the standard quadratic cost function does not capture. This practice extends the 3D-Var framework to handle a nonlinear observation operator, using sensor saturation as a practical example. You will explore how the non-quadratic cost function necessitates an iterative minimization scheme, such as the Gauss-Newton method, and analyze how different linearization strategies affect the solver's ability to find the true minimum, revealing the complexities and challenges of nonlinear data assimilation [@problem_id:3426295].", "problem": "Consider a three-dimensional variational (3D-Var) data assimilation problem with a nonlinear observation operator that includes saturation effects, motivated by radiance clipping in remote sensing. Starting from the Maximum A Posteriori (MAP) estimator under Gaussian error assumptions, derive the variational cost in the presence of a nonlinear observation operator, and then construct and analyze minimization strategies under different relinearization schedules. Your final deliverable must be a complete, runnable program, as specified at the end of this problem.\n\nYou are provided the following fundamental base:\n- The prior (background) state is modeled as a Gaussian random variable with mean $\\boldsymbol{x}_b \\in \\mathbb{R}^3$ and covariance matrix $\\boldsymbol{B} \\in \\mathbb{R}^{3 \\times 3}$, where $\\boldsymbol{B}$ is symmetric positive definite.\n- The observation errors are independent and Gaussian with covariance matrix $\\boldsymbol{R} \\in \\mathbb{R}^{3 \\times 3}$, where $\\boldsymbol{R}$ is symmetric positive definite.\n- The observation operator includes saturation of radiances, meaning the forward operator maps the state $\\boldsymbol{x} \\in \\mathbb{R}^3$ to observed quantities via a linear map followed by element-wise clipping:\n$$\n\\boldsymbol{h}(\\boldsymbol{x}) = \\mathrm{clip}\\!\\left(\\boldsymbol{A}\\boldsymbol{x};\\, \\boldsymbol{\\ell}, \\boldsymbol{u}\\right),\n$$\nwhere $\\boldsymbol{A} \\in \\mathbb{R}^{3 \\times 3}$, and $\\mathrm{clip}$ is defined componentwise for $\\boldsymbol{z} = \\boldsymbol{A}\\boldsymbol{x}$ by\n$$\n\\mathrm{clip}(z_i; \\ell_i, u_i) = \\min\\left(\\max(z_i,\\ell_i),u_i\\right).\n$$\n- Under these assumptions, the negative log-posterior is proportional to a cost function that combines a quadratic background term and an observation term that depends on $\\boldsymbol{h}(\\boldsymbol{x})$.\n\nTask 1 (Derivation):\n- Derive the explicit form of the variational cost function $J(\\boldsymbol{x})$ starting from the Gaussian error assumptions and Bayes' rule. Do not assume linearity of $\\boldsymbol{h}(\\boldsymbol{x})$; keep the saturation in the observation operator.\n- Derive the gradient $\\nabla J(\\boldsymbol{x})$ and the Gauss–Newton approximate Hessian for the case when the Jacobian of $\\boldsymbol{h}(\\boldsymbol{x})$ exists almost everywhere. Describe the handling of the saturation: for each observation component $i$, the Jacobian contribution is zero when the corresponding component of $\\boldsymbol{A}\\boldsymbol{x}$ is at or beyond a saturation bound, and one when it is strictly within bounds.\n\nTask 2 (Algorithm design):\n- Consider three relinearization schedules in a Gauss–Newton-type minimization:\n  1. Schedule $S_0$ (Frozen Linearization): Linearize $\\boldsymbol{h}(\\boldsymbol{x})$ once at the initial guess $\\boldsymbol{x}^{(0)}$ and solve the resulting quadratic problem exactly. That is, use the linearization\n  $$\n  \\boldsymbol{h}(\\boldsymbol{x}) \\approx \\boldsymbol{h}(\\boldsymbol{x}^{(0)}) + \\boldsymbol{J}_h(\\boldsymbol{x}^{(0)})(\\boldsymbol{x}-\\boldsymbol{x}^{(0)}),\n  $$\n  where $\\boldsymbol{J}_h$ is the Jacobian of $\\boldsymbol{h}$ computed using the interior-mask rule described above. Solve the corresponding quadratic minimizer in closed form via a linear system.\n  2. Schedule $S_1$ (Full Relinearization): At every iteration, compute $\\boldsymbol{h}(\\boldsymbol{x}^{(k)})$ and $\\boldsymbol{J}_h(\\boldsymbol{x}^{(k)})$, form the Gauss–Newton system, and take a step with a backtracking line search that guarantees descent of $J(\\boldsymbol{x})$.\n  3. Schedule $S_2$ (Adaptive Relinearization): Maintain the current linearization point and Jacobian; only relinearize when the active set (the saturation mask) changes compared to the previous accepted iterate. Between relinearizations, use the quadratic surrogate defined by the current linearization to compute steps, and enforce descent on the true cost with a backtracking line search.\n- For schedules $S_1$ and $S_2$, implement a backtracking line search with Armijo-type condition. Use a step size parameter $\\alpha$ initialized to $1$, halved on each backtracking, and an Armijo parameter $\\gamma = 10^{-4}$.\n\nTask 3 (Basin-of-attraction analysis with observation saturation):\n- For each test case below, evaluate the basin of attraction of the minimization by scanning a grid of initial guesses. For each initial guess, run each of the three schedules to convergence (subject to stopping criteria) and record whether the run converges to the best (lowest) cost attainable by the full relinearization across the grid for that test case.\n- Define “success” for a schedule and an initial guess as achieving a final cost $J$ within a tolerance $\\varepsilon_J = 10^{-6}$ of the best cost among all full relinearization ($S_1$) runs starting from the grid for that test case.\n- Stopping criteria for iterative schedules ($S_1$ and $S_2$): maximum iterations $N_{\\max} = 50$, and step-norm threshold $\\|\\boldsymbol{p}^{(k)}\\|_2 \\le 10^{-8}$. For $S_0$, directly solve the single linear system for the frozen-quadratic minimizer.\n- Initial guess grid: use the Cartesian product of $\\{-1.5, -0.75, 0.0, 0.75, 1.5\\}$ in each component, yielding $5^3 = 125$ initial guesses.\n\nTest Suite:\nFor each test case, use the state dimension $3$ with the following fixed matrices and vectors:\n- Background mean:\n$$\n\\boldsymbol{x}_b = \\begin{bmatrix} -1.0 \\\\ 0.0 \\\\ 1.0 \\end{bmatrix}.\n$$\n- Background covariance:\n$$\n\\boldsymbol{B} = \\mathrm{diag}\\!\\left(4.0,\\,1.0,\\,9.0\\right).\n$$\n- Observation covariance:\n$$\n\\boldsymbol{R} = \\mathrm{diag}\\!\\left(0.09,\\,0.04,\\,0.16\\right).\n$$\n- Linear part of observation operator:\n$$\n\\boldsymbol{A} = \\begin{bmatrix}\n1.0  0.5  0.0 \\\\\n0.0  1.0  0.3 \\\\\n0.2  0.0  1.0\n\\end{bmatrix}.\n$$\n- True state used to synthesize observations (noise-free):\n$$\n\\boldsymbol{x}_{\\text{true}} = \\begin{bmatrix} 2.0 \\\\ -1.5 \\\\ 0.5 \\end{bmatrix}.\n$$\n- Observations are synthesized as $\\boldsymbol{y} = \\mathrm{clip}\\!\\left(\\boldsymbol{A}\\boldsymbol{x}_{\\text{true}};\\boldsymbol{\\ell},\\boldsymbol{u}\\right)$ for each test case below, with bounds specified per case:\n\nTest Case 1 (Moderate saturation):\n$$\n\\boldsymbol{\\ell} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\\quad\n\\boldsymbol{u} = \\begin{bmatrix} 1.0 \\\\ 1.2 \\\\ 0.8 \\end{bmatrix}.\n$$\n\nTest Case 2 (Heavy saturation):\n$$\n\\boldsymbol{\\ell} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\\quad\n\\boldsymbol{u} = \\begin{bmatrix} 0.6 \\\\ 0.4 \\\\ 0.5 \\end{bmatrix}.\n$$\n\nTest Case 3 (No saturation; effectively linear):\n$$\n\\boldsymbol{\\ell} = \\begin{bmatrix} -10.0 \\\\ -10.0 \\\\ -10.0 \\end{bmatrix},\\quad\n\\boldsymbol{u} = \\begin{bmatrix} 10.0 \\\\ 10.0 \\\\ 10.0 \\end{bmatrix}.\n$$\n\nImplementation specifics:\n- Use the interior-mask rule for the Jacobian: for each component $i$, if $(\\boldsymbol{A}\\boldsymbol{x})_i$ satisfies $\\ell_i lt; (\\boldsymbol{A}\\boldsymbol{x})_i lt; u_i$, the local derivative is $1$; otherwise it is $0$. The Jacobian of $\\boldsymbol{h}$ at $\\boldsymbol{x}$ is then $\\boldsymbol{J}_h(\\boldsymbol{x}) = \\mathrm{diag}(\\boldsymbol{m}(\\boldsymbol{x}))\\,\\boldsymbol{A}$, where $m_i(\\boldsymbol{x})$ is the interior-mask indicator in $\\{0,1\\}$.\n- The Gauss–Newton system at iteration $k$ is given by the linear system\n$$\n\\left(\\boldsymbol{B}^{-1} + \\boldsymbol{J}_h(\\boldsymbol{x}^{(k)})^\\top \\boldsymbol{R}^{-1}\\boldsymbol{J}_h(\\boldsymbol{x}^{(k)})\\right)\\boldsymbol{p}^{(k)} = -\\nabla J(\\boldsymbol{x}^{(k)}),\n$$\nand the step is $\\boldsymbol{x}^{(k+1)} = \\boldsymbol{x}^{(k)} + \\alpha^{(k)}\\boldsymbol{p}^{(k)}$, where $\\alpha^{(k)}$ is chosen by backtracking to ensure descent.\n- For $S_0$, the closed-form minimizer of the frozen-quadratic surrogate $Q(\\boldsymbol{x})$ defined by the single linearization at $\\boldsymbol{x}^{(0)}$ is obtained by solving\n$$\n\\left(\\boldsymbol{B}^{-1} + \\boldsymbol{J}_0^\\top \\boldsymbol{R}^{-1}\\boldsymbol{J}_0\\right)\\boldsymbol{x}\n= \\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{J}_0^\\top \\boldsymbol{R}^{-1}\\left(\\boldsymbol{y} - \\boldsymbol{c}_0\\right),\n$$\nwhere $\\boldsymbol{J}_0 = \\boldsymbol{J}_h(\\boldsymbol{x}^{(0)})$ and $\\boldsymbol{c}_0 = \\boldsymbol{h}(\\boldsymbol{x}^{(0)}) - \\boldsymbol{J}_0\\boldsymbol{x}^{(0)}$.\n\nRequired Output:\n- For each test case, compute the fraction of successful initial guesses for each schedule $S_0$, $S_1$, and $S_2$, in that order. Express each fraction as a float in $[0,1]$.\n- Your program should produce a single line of output containing the results as a comma-separated list of three lists, one per test case, each inner list containing three floats in the order $[S_0, S_1, S_2]$. For example:\n$$\n[\\,[f_{1,0},f_{1,1},f_{1,2}],\\,[f_{2,0},f_{2,1},f_{2,2}],\\,[f_{3,0},f_{3,1},f_{3,2}]\\,].\n$$\nNo other text should be printed.", "solution": "### Task 1: Derivation of the Variational Cost Function and its Derivatives\n\nThe problem is approached from a Bayesian perspective, where the goal is to find the Maximum A Posteriori (MAP) estimate of the state vector $\\boldsymbol{x} \\in \\mathbb{R}^3$. The posterior probability density function $p(\\boldsymbol{x}|\\boldsymbol{y})$ is given by Bayes' theorem:\n$$\np(\\boldsymbol{x}|\\boldsymbol{y}) \\propto p(\\boldsymbol{y}|\\boldsymbol{x})p(\\boldsymbol{x})\n$$\nwhere $p(\\boldsymbol{y}|\\boldsymbol{x})$ is the likelihood and $p(\\boldsymbol{x})$ is the prior probability distribution.\n\nThe prior information is that the state $\\boldsymbol{x}$ is a Gaussian random variable with mean $\\boldsymbol{x}_b$ (background state) and covariance matrix $\\boldsymbol{B}$. The probability density function is:\n$$\np(\\boldsymbol{x}) \\propto \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_b)^\\top \\boldsymbol{B}^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b)\\right)\n$$\n\nThe relationship between the true state $\\boldsymbol{x}$ and the observations $\\boldsymbol{y}$ is given by $\\boldsymbol{y} = \\boldsymbol{h}(\\boldsymbol{x}) + \\boldsymbol{\\epsilon}$, where $\\boldsymbol{\\epsilon}$ represents the observation errors. The errors are assumed to be zero-mean Gaussian with covariance matrix $\\boldsymbol{R}$. The likelihood of observing $\\boldsymbol{y}$ given a state $\\boldsymbol{x}$ is therefore:\n$$\np(\\boldsymbol{y}|\\boldsymbol{x}) \\propto \\exp\\left(-\\frac{1}{2}(\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))^\\top \\boldsymbol{R}^{-1} (\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))\\right)\n$$\nwhere the nonlinear observation operator is $\\boldsymbol{h}(\\boldsymbol{x}) = \\mathrm{clip}\\!\\left(\\boldsymbol{A}\\boldsymbol{x};\\, \\boldsymbol{\\ell}, \\boldsymbol{u}\\right)$.\n\nCombining the prior and the likelihood, the posterior distribution is:\n$$\np(\\boldsymbol{x}|\\boldsymbol{y}) \\propto \\exp\\left(-\\frac{1}{2} \\left[ (\\boldsymbol{x} - \\boldsymbol{x}_b)^\\top \\boldsymbol{B}^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) + (\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))^\\top \\boldsymbol{R}^{-1} (\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x})) \\right] \\right)\n$$\nMaximizing the posterior probability is equivalent to minimizing its negative logarithm. We define the variational cost function $J(\\boldsymbol{x})$ as half of the term in the exponent:\n$$\nJ(\\boldsymbol{x}) = \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_b)^\\top \\boldsymbol{B}^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) + \\frac{1}{2}(\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))^\\top \\boldsymbol{R}^{-1} (\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))\n$$\nThis is the cost function to be minimized. The first term, $J_b(\\boldsymbol{x})$, penalizes deviations from the background state, while the second term, $J_o(\\boldsymbol{x})$, penalizes mismatches between the model-predicted observations and the actual observations.\n\n**Gradient of the Cost Function:**\nThe gradient $\\nabla J(\\boldsymbol{x})$ is found by differentiating $J(\\boldsymbol{x})$ with respect to $\\boldsymbol{x}$:\n$$\n\\nabla J(\\boldsymbol{x}) = \\nabla J_b(\\boldsymbol{x}) + \\nabla J_o(\\boldsymbol{x})\n$$\nThe gradient of the background term is:\n$$\n\\nabla J_b(\\boldsymbol{x}) = \\boldsymbol{B}^{-1}(\\boldsymbol{x} - \\boldsymbol{x}_b)\n$$\nThe gradient of the observation term requires the chain rule. Let $\\boldsymbol{d}(\\boldsymbol{x}) = \\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x})$. The Jacobian of $\\boldsymbol{d}(\\boldsymbol{x})$ is $-\\boldsymbol{J}_h(\\boldsymbol{x})$, where $\\boldsymbol{J}_h(\\boldsymbol{x})$ is the Jacobian of $\\boldsymbol{h}(\\boldsymbol{x})$.\n$$\n\\nabla J_o(\\boldsymbol{x}) = (\\nabla \\boldsymbol{d}(\\boldsymbol{x}))^\\top \\boldsymbol{R}^{-1} \\boldsymbol{d}(\\boldsymbol{x}) = (-\\boldsymbol{J}_h(\\boldsymbol{x}))^\\top \\boldsymbol{R}^{-1} (\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x})) = -\\boldsymbol{J}_h(\\boldsymbol{x})^\\top \\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))\n$$\nThe Jacobian of $\\boldsymbol{h}(\\boldsymbol{x})$ is defined using the interior-mask rule. Let $z_i = (\\boldsymbol{A}\\boldsymbol{x})_i$. The derivative of $\\mathrm{clip}(z_i; \\ell_i, u_i)$ with respect to $z_i$ is $1$ if $\\ell_i  z_i  u_i$ and $0$ otherwise. Let $\\boldsymbol{M}(\\boldsymbol{x})$ be a diagonal matrix whose $i$-th diagonal element is this indicator. Then, by the chain rule, $\\boldsymbol{J}_h(\\boldsymbol{x}) = \\boldsymbol{M}(\\boldsymbol{x})\\boldsymbol{A}$.\nCombining terms, the full gradient is:\n$$\n\\nabla J(\\boldsymbol{x}) = \\boldsymbol{B}^{-1}(\\boldsymbol{x} - \\boldsymbol{x}_b) - \\boldsymbol{J}_h(\\boldsymbol{x})^\\top \\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))\n$$\n\n**Gauss-Newton Approximate Hessian:**\nThe exact Hessian is $\\nabla^2 J(\\boldsymbol{x}) = \\nabla^2 J_b(\\boldsymbol{x}) + \\nabla^2 J_o(\\boldsymbol{x})$. The background term's Hessian is simply $\\boldsymbol{B}^{-1}$. The observation term's Hessian is:\n$$\n\\nabla^2 J_o(\\boldsymbol{x}) = \\boldsymbol{J}_h(\\boldsymbol{x})^\\top \\boldsymbol{R}^{-1} \\boldsymbol{J}_h(\\boldsymbol{x}) - \\nabla\\left[\\boldsymbol{J}_h(\\boldsymbol{x})^\\top\\right]\\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))\n$$\nThe Gauss-Newton approximation neglects the second term, which involves second derivatives of $\\boldsymbol{h}$ and is scaled by the observation residual $\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x})$. This approximation is accurate when the model is nearly linear or the residuals are small at the solution. The resulting approximate Hessian, $\\boldsymbol{H}_{\\text{GN}}$, is:\n$$\n\\boldsymbol{H}_{\\text{GN}}(\\boldsymbol{x}) = \\boldsymbol{B}^{-1} + \\boldsymbol{J}_h(\\boldsymbol{x})^\\top \\boldsymbol{R}^{-1} \\boldsymbol{J}_h(\\boldsymbol{x})\n$$\nThis Hessian is guaranteed to be symmetric and positive definite (since $\\boldsymbol{B}^{-1}$ and $\\boldsymbol{R}^{-1}$ are, and $\\boldsymbol{J}_h^\\top \\boldsymbol{R}^{-1} \\boldsymbol{J}_h$ is positive semi-definite), which ensures that the search directions computed are descent directions.\n\n### Task 2: Algorithm Design and Analysis\n\nThe three minimization schedules represent different trade-offs between computational cost per iteration and convergence robustness.\n\n**Schedule $S_0$ (Frozen Linearization):** This is the simplest approach. It linearizes the observation operator $\\boldsymbol{h}(\\boldsymbol{x})$ once at the initial guess $\\boldsymbol{x}^{(0)}$. The resulting cost function is purely quadratic, and its minimizer can be found by solving a single $3 \\times 3$ linear system. This method is computationally cheap but its accuracy is highly sensitive to the quality of the initial guess. If $\\boldsymbol{x}^{(0)}$ is far from the true solution, or in a region where the active saturation set is different from that at the solution, the linearization can be poor, leading the solver to a suboptimal point.\n\n**Schedule $S_1$ (Full Relinearization):** This is the standard Gauss-Newton method. At each iteration $k$, it relinearizes $\\boldsymbol{h}(\\boldsymbol{x})$ at the current iterate $\\boldsymbol{x}^{(k)}$, forms the new Gauss-Newton system, and computes a search direction. A backtracking line search is used to ensure that each step robustly decreases the true nonlinear cost function $J(\\boldsymbol{x})$. This method is the most robust of the three, as it constantly updates its local quadratic model of the cost function. However, it is also the most computationally expensive per iteration, as it requires re-evaluating the Jacobian and forming the Hessian matrix at every step.\n\n**Schedule $S_2$ (Adaptive Relinearization):** This schedule is a hybrid, aiming to balance the trade-offs of $S_0$ and $S_1$. It recognizes that the Jacobian $\\boldsymbol{J}_h(\\boldsymbol{x})$ is piecewise constant, changing only when an element of $\\boldsymbol{A}\\boldsymbol{x}$ crosses a saturation boundary. Therefore, the computationally expensive step of forming the Hessian matrix is only performed when the \"active set\" (the set of saturated components) of the current iterate changes relative to the point of the last linearization. Between these updates, the algorithm uses a \"stale\" Hessian but continues to compute the gradient at the current iterate. This can save significant computation if the iterates remain within a region of constant saturation, while still allowing the model to adapt when necessary. Like $S_1$, it uses a backtracking line search to guarantee descent on the true cost function.\n\nThe basin of attraction analysis across the three test cases will illuminate these trade-offs. In the linear case (Test Case 3), all saturation is inactive, the problem is quadratic, and all three methods are expected to converge to the unique global minimum from any starting point. For the nonlinear cases (1 and 2), the cost function landscape is more complex, and we expect $S_1$ to exhibit the largest basin of attraction, with $S_0$ having the smallest. The performance of $S_2$ is expected to be close to $S_1$ but with potentially fewer Hessian updates.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a 3D-Var data assimilation problem with a nonlinear observation operator,\n    evaluating three different minimization schedules.\n    \"\"\"\n\n    # --- System Constants ---\n    X_B = np.array([-1.0, 0.0, 1.0])\n    B = np.diag([4.0, 1.0, 9.0])\n    B_INV = np.linalg.inv(B)\n    R = np.diag([0.09, 0.04, 0.16])\n    R_INV = np.linalg.inv(R)\n    A = np.array([[1.0, 0.5, 0.0], [0.0, 1.0, 0.3], [0.2, 0.0, 1.0]])\n    X_TRUE = np.array([2.0, -1.5, 0.5])\n\n    # --- Algorithm Parameters ---\n    GRID_VALS = [-1.5, -0.75, 0.0, 0.75, 1.5]\n    MAX_ITER = 50\n    TOL_STEP = 1e-8\n    TOL_COST = 1e-6\n    ARMIJO_GAMMA = 1e-4\n\n    # --- Test Case Definitions ---\n    test_cases_params = [\n        {'l': np.array([0.0, 0.0, 0.0]), 'u': np.array([1.0, 1.2, 0.8])}, # Case 1\n        {'l': np.array([0.0, 0.0, 0.0]), 'u': np.array([0.6, 0.4, 0.5])}, # Case 2\n        {'l': np.array([-10.0, -10.0, -10.0]), 'u': np.array([10.0, 10.0, 10.0])} # Case 3\n    ]\n\n    # --- Core Mathematical Functions ---\n    def h_op(x, A_mat, l_vec, u_vec):\n        z = A_mat @ x\n        return np.clip(z, l_vec, u_vec)\n\n    def get_mask(x, A_mat, l_vec, u_vec):\n        z = A_mat @ x\n        return ((z > l_vec)  (z  u_vec))\n\n    def j_h_op(x, A_mat, l_vec, u_vec):\n        mask = get_mask(x, A_mat, l_vec, u_vec).astype(float)\n        M = np.diag(mask)\n        return M @ A_mat\n\n    def cost_function(x, y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec):\n        h_x = h_op(x, A_mat, l_vec, u_vec)\n        term_b = 0.5 * (x - x_b).T @ B_inv @ (x - x_b)\n        term_o = 0.5 * (y - h_x).T @ R_inv @ (y - h_x)\n        return term_b + term_o\n\n    def grad_cost_function(x, y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec):\n        h_x = h_op(x, A_mat, l_vec, u_vec)\n        Jh_x = j_h_op(x, A_mat, l_vec, u_vec)\n        grad_b = B_inv @ (x - x_b)\n        grad_o = -Jh_x.T @ R_inv @ (y - h_x)\n        return grad_b + grad_o\n\n    # --- Solver Implementations for Schedules S0, S1, S2 ---\n    def solve_s0(x0, y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec):\n        J0 = j_h_op(x0, A_mat, l_vec, u_vec)\n        h0 = h_op(x0, A_mat, l_vec, u_vec)\n        c0 = h0 - J0 @ x0\n        H_sys = B_inv + J0.T @ R_inv @ J0\n        rhs = B_inv @ x_b + J0.T @ R_inv @ (y - c0)\n        try:\n            x_final = np.linalg.solve(H_sys, rhs)\n        except np.linalg.LinAlgError:\n            x_final = x0 # Return initial guess if solver fails\n        return x_final\n\n    def solve_s1(x0, y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec):\n        xk = np.copy(x0)\n        args_cost = (y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec)\n\n        for _ in range(MAX_ITER):\n            grad_k = grad_cost_function(xk, *args_cost)\n            Jhk = j_h_op(xk, A_mat, l_vec, u_vec)\n            H_gn = B_inv + Jhk.T @ R_inv @ Jhk\n            \n            try:\n                pk = np.linalg.solve(H_gn, -grad_k)\n            except np.linalg.LinAlgError:\n                break\n\n            if np.linalg.norm(pk) = TOL_STEP:\n                break\n\n            alpha = 1.0\n            Jk = cost_function(xk, *args_cost)\n            slope = grad_k.T @ pk\n\n            for _ in range(10): # Max 10 backtracking steps\n                x_new = xk + alpha * pk\n                J_new = cost_function(x_new, *args_cost)\n                if J_new = Jk + ARMIJO_GAMMA * alpha * slope:\n                    break\n                alpha /= 2.0\n            else: # Line search failed\n                break\n            \n            xk = x_new\n\n        return xk, cost_function(xk, *args_cost)\n\n    def solve_s2(x0, y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec):\n        xk = np.copy(x0)\n        args_cost = (y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec)\n        \n        mask_lin = get_mask(xk, A_mat, l_vec, u_vec)\n        j_lin = j_h_op(xk, A_mat, l_vec, u_vec)\n        H_gn_lin = B_inv + j_lin.T @ R_inv @ j_lin\n        \n        for _ in range(MAX_ITER):\n            grad_k = grad_cost_function(xk, *args_cost)\n            \n            try:\n                pk = np.linalg.solve(H_gn_lin, -grad_k)\n            except np.linalg.LinAlgError:\n                break\n            \n            if np.linalg.norm(pk) = TOL_STEP:\n                break\n\n            alpha = 1.0\n            Jk = cost_function(xk, *args_cost)\n            slope = grad_k.T @ pk\n            \n            for _ in range(10):\n                x_new = xk + alpha * pk\n                J_new = cost_function(x_new, *args_cost)\n                if J_new = Jk + ARMIJO_GAMMA * alpha * slope:\n                    break\n                alpha /= 2.0\n            else:\n                break\n            \n            xk = x_new\n            mask_new = get_mask(xk, A_mat, l_vec, u_vec)\n\n            if not np.array_equal(mask_new, mask_lin):\n                mask_lin = mask_new\n                j_lin = j_h_op(xk, A_mat, l_vec, u_vec)\n                H_gn_lin = B_inv + j_lin.T @ R_inv @ j_lin\n\n        return xk, cost_function(xk, *args_cost)\n\n    # --- Main Execution Logic ---\n    x0_grid = []\n    for i in GRID_VALS:\n        for j in GRID_VALS:\n            for k in GRID_VALS:\n                x0_grid.append(np.array([i, j, k]))\n    num_guesses = len(x0_grid)\n\n    overall_results = []\n\n    for params in test_cases_params:\n        l, u = params['l'], params['u']\n        y = h_op(X_TRUE, A, l, u)\n        \n        args = (y, X_B, B_INV, R_INV, A, l, u)\n\n        # Baseline: run S1 on all guesses to find the best attainable cost\n        s1_final_costs = [solve_s1(x0, *args)[1] for x0 in x0_grid]\n        j_best = min(s1_final_costs) if s1_final_costs else float('inf')\n\n        success_counts = [0, 0, 0]\n\n        # Evaluate S0\n        for x0 in x0_grid:\n            x_final_s0 = solve_s0(x0, *args)\n            if cost_function(x_final_s0, *args) = j_best + TOL_COST:\n                success_counts[0] += 1\n        \n        # Evaluate S1\n        for cost in s1_final_costs:\n            if cost = j_best + TOL_COST:\n                success_counts[1] += 1\n\n        # Evaluate S2\n        for x0 in x0_grid:\n            _, j_final_s2 = solve_s2(x0, *args)\n            if j_final_s2 = j_best + TOL_COST:\n                success_counts[2] += 1\n\n        fractions = [count / num_guesses for count in success_counts]\n        overall_results.append(fractions)\n\n    # --- Format and Print Final Output ---\n    list_strs = [f\"[{','.join(map(str, res_list))}]\" for res_list in overall_results]\n    print(f\"[{','.join(list_strs)}]\")\n\nsolve()\n```", "id": "3426295"}]}