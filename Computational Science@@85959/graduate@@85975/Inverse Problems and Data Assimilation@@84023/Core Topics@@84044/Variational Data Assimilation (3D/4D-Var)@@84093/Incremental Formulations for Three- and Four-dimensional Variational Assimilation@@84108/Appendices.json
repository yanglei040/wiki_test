{"hands_on_practices": [{"introduction": "The incremental approach to 4D-Var reframes a complex non-linear optimization problem into a sequence of more manageable quadratic minimizations. The efficiency of this \"inner loop\" is paramount for the performance of the entire assimilation system. This exercise [@problem_id:3390407] delves into the computational core of 4D-Var by asking you to implement and compare two workhorse iterative solvers: the Preconditioned Conjugate Gradient (PCG) method and the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm. You will explore a key practical question: how to best utilize information from one outer loop to accelerate the next, and specifically, when the Hessian information carried by L-BFGS memory becomes a liability rather than an asset.", "problem": "Consider the incremental quadratic subproblem that arises in the inner loop of four-dimensional variational assimilation (4D-Var), which is solved repeatedly within outer linearization loops. The inner problem is to minimize the strictly convex quadratic functional\n$$\nJ(\\delta x) = \\tfrac{1}{2}\\,\\delta x^{\\top} \\mathbf{H}\\,\\delta x - \\mathbf{g}^{\\top}\\,\\delta x,\n$$\nwhere $\\mathbf{H}\\in\\mathbb{R}^{n\\times n}$ is symmetric positive definite and $\\mathbf{g}\\in\\mathbb{R}^{n}$ is a given vector. In the incremental formulation, $\\mathbf{H}$ can be interpreted as a Gauss–Newton approximation of the Hessian assembled from background and observation terms, for example\n$$\n\\mathbf{H} = \\mathbf{B}^{-1} + \\sum_{t} \\mathbf{M}_{t}^{\\top} \\mathbf{R}_{t}^{-1} \\mathbf{M}_{t},\n$$\nwith $\\mathbf{B}\\in\\mathbb{R}^{n\\times n}$ symmetric positive definite background covariance, $\\mathbf{R}_{t}\\in\\mathbb{R}^{m_t\\times m_t}$ symmetric positive definite observation error covariance at time $t$, and $\\mathbf{M}_{t}\\in\\mathbb{R}^{m_t\\times n}$ the linearized model–observation operator at time $t$. The outer loop updates change the linearization, hence $\\mathbf{H}$ can vary between outer iterations.\n\nThe minimizer $\\delta x^{\\ast}$ is characterized by the first-order optimality condition\n$$\n\\nabla J(\\delta x) = \\mathbf{H}\\,\\delta x - \\mathbf{g} = \\mathbf{0},\n$$\nso that $\\delta x^{\\ast}$ solves the linear system\n$$\n\\mathbf{H}\\,\\delta x = \\mathbf{g}.\n$$\nTwo classes of algorithms are commonly employed:\n- Preconditioned Conjugate Gradient (PCG), which exploits the Krylov subspace structure for symmetric positive definite systems, with a symmetric positive definite preconditioner $\\mathbf{M}\\approx \\mathbf{H}$.\n- Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS), which maintains a small number of past quasi-Newton curvature pairs to approximate $\\mathbf{H}^{-1}$ and performs line searches along quasi-Newton directions.\n\nIn incremental variational assimilation, practitioners sometimes carry the L-BFGS memory across outer loops to accelerate convergence. However, if the curvature of $\\mathbf{H}$ changes across outer loops, the stored curvature pairs may become inconsistent with the new $\\mathbf{H}$, degrading effectiveness.\n\nStarting from the definitions above and the properties of symmetric positive definite matrices, implement a program that:\n1. Solves the inner problem using PCG with a diagonal preconditioner equal to the diagonal of $\\mathbf{H}$, terminating when the relative residual $\\|\\mathbf{g} - \\mathbf{H}\\,\\delta x\\|_{2}/\\|\\mathbf{g}\\|_{2}$ is less than $10^{-8}$ or a maximum of $500$ iterations is reached.\n2. Solves the inner problem using L-BFGS with memory size $m$ and exact step length along the quasi-Newton search direction. The exact step length for a quadratic objective along a search direction $\\mathbf{p}$ is\n$$\n\\alpha = -\\frac{\\nabla J(\\delta x)^{\\top}\\mathbf{p}}{\\mathbf{p}^{\\top}\\mathbf{H}\\,\\mathbf{p}}.\n$$\nTerminate when the same relative residual criterion is met or after $500$ iterations. Use the standard two-loop recursion with initial inverse Hessian scaling set to\n$$\n\\gamma = \\frac{\\mathbf{s}_{k-1}^{\\top}\\mathbf{y}_{k-1}}{\\mathbf{y}_{k-1}^{\\top}\\mathbf{y}_{k-1}}\n$$\nwhen memory is available, and $\\gamma=1$ otherwise. Enforce the curvature condition $\\mathbf{s}^{\\top}\\mathbf{y} > 10^{-12}$ when accepting pairs.\n3. Emulates two outer loops. In the first outer loop, solve the inner problem to convergence from $\\delta x_{0}=\\mathbf{0}$. In the second outer loop, change $\\mathbf{H}$ according to the test case definition, start from the previous solution as the initial guess, and solve again. Perform three second-loop runs per test case:\n   - PCG baseline.\n   - L-BFGS carrying over the memory from the first loop without reset.\n   - L-BFGS with a reset policy that discards memory at the start of the second loop if\n     $$\n     \\Delta = \\frac{\\|\\mathbf{H}_{\\text{new}} - \\mathbf{H}_{\\text{old}}\\|_{F}}{\\|\\mathbf{H}_{\\text{old}}\\|_{F}} > \\tau,\n     $$\n     where $\\|\\cdot\\|_{F}$ is the Frobenius norm and $\\tau$ is a threshold parameter. Otherwise, keep the memory. Use $\\tau=0.2$.\n\nYour implementation must be fully deterministic and self-contained. Angles, when present, must be in radians.\n\nTest Suite. Use the following test cases with dimension $n=30$, memory size $m=10$, tolerance $10^{-8}$, and maximum iterations $500$. For all cases, use $\\mathbf{g}=\\mathbf{1}\\in\\mathbb{R}^{30}$.\n- Case 1 (happy path, no curvature change): $\\mathbf{H}_{1}=\\mathrm{diag}(1,2,\\dots,30)$, $\\mathbf{H}_{2}=\\mathbf{H}_{1}$.\n- Case 2 (eigenvector rotation with anisotropy): Let $\\mathbf{D}=\\mathrm{diag}(\\lambda_{i})$ with $\\lambda_{i}$ logarithmically spaced between $10^{-2}$ and $10^{2}$ over $30$ points. Define an orthogonal matrix $\\mathbf{Q}$ as the product of plane rotations by angle $\\pi/3$ in the coordinate planes $(1,2)$, $(3,4)$, $(5,6)$, $(7,8)$, $(9,10)$, $(11,12)$, $(13,14)$, $(15,16)$, $(17,18)$, $(19,20)$, $(21,22)$, $(23,24)$, $(25,26)$, $(27,28)$, $(29,30)$ (indices are one-based in this description). Set $\\mathbf{H}_{1}=\\mathbf{D}$ and $\\mathbf{H}_{2}=\\mathbf{Q}^{\\top}\\mathbf{D}\\,\\mathbf{Q}$.\n- Case 3 (eigenvalue rescaling without rotation): $\\mathbf{H}_{1}=\\mathrm{diag}(\\lambda_{i})$ with $\\lambda_{i}$ logarithmically spaced between $10^{0}$ and $10^{3}$ over $30$ points, and $\\mathbf{H}_{2}=\\mathrm{diag}(\\mu_{i})$ with $\\mu_{i}$ logarithmically spaced between $10^{-4}$ and $10^{4}$ over $30$ points.\n\nFor each test case, report the number of iterations taken in the second outer loop to meet the termination criterion for:\n- PCG baseline,\n- L-BFGS with carried memory,\n- L-BFGS with the reset policy.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by test case and method, that is\n$$\n[\\text{pcg}_{1},\\text{lbfgs\\_carry}_{1},\\text{lbfgs\\_reset}_{1},\\text{pcg}_{2},\\text{lbfgs\\_carry}_{2},\\text{lbfgs\\_reset}_{2},\\text{pcg}_{3},\\text{lbfgs\\_carry}_{3},\\text{lbfgs\\_reset}_{3}],\n$$\nwhere each entry is an integer number of iterations.", "solution": "This problem requires implementing and comparing two iterative solvers, Preconditioned Conjugate Gradient (PCG) and Limited-memory BFGS (L-BFGS), for the quadratic inner-loop problem of incremental 4D-Var. The core task is to solve the system $\\mathbf{H}\\delta x = \\mathbf{g}$.\n\n1.  **Preconditioned Conjugate Gradient (PCG)**: This method is tailored for symmetric positive-definite systems like the one in the inner loop. The implementation follows the standard PCG algorithm. A diagonal preconditioner, $\\mathbf{M} = \\mathrm{diag}(\\mathbf{H})$, is used, which is simple to compute and invert and often effective at improving the conditioning of the system. The algorithm iteratively builds a basis of conjugate directions to find the solution.\n\n2.  **Limited-memory BFGS (L-BFGS)**: This is a quasi-Newton method that avoids forming, storing, or inverting the full Hessian. Instead, it stores a small number of past state and gradient changes (`s` and `y` vectors) to implicitly approximate the inverse Hessian. The search direction is computed using the standard \"two-loop recursion\". For this quadratic problem, an exact line search can be performed, and the optimal step length $\\alpha$ is computed analytically.\n\n3.  **Two-Loop Simulation**: The simulation emulates two outer loops of 4D-Var to test how L-BFGS adapts to a changing Hessian.\n    - **Loop 1**: The initial problem with Hessian $\\mathbf{H}_1$ is solved to convergence by both PCG and L-BFGS, starting from $\\delta x = \\mathbf{0}$. L-BFGS builds up its memory of curvature pairs.\n    - **Loop 2**: The Hessian is changed to $\\mathbf{H}_2$. The problem is solved again, starting from the solution of Loop 1. Three strategies are compared:\n        - **PCG**: Starts fresh with the new problem. Its performance depends only on the properties of $\\mathbf{H}_2$ and the initial guess.\n        - **L-BFGS Carry-over**: Reuses the memory from Loop 1. This can be efficient if $\\mathbf{H}_2 \\approx \\mathbf{H}_1$ but detrimental otherwise, as the old curvature information is inconsistent with the new problem.\n        - **L-BFGS with Reset**: An adaptive strategy is implemented. It computes the relative change in the Hessian using the Frobenius norm. If this change exceeds a threshold $\\tau=0.2$, the L-BFGS memory is discarded (reset) to prevent pollution from outdated curvature information.\n\nThe Python code in the answer implements these algorithms and the testing framework to generate the required iteration counts for each test case and strategy.", "answer": "```python\nimport numpy as np\nfrom collections import deque\n\ndef pcg(H: np.ndarray, g: np.ndarray, x0: np.ndarray, tol: float, max_iter: int) -> tuple[np.ndarray, int]:\n    \"\"\"\n    Solves the linear system Hx=g for a symmetric positive definite matrix H\n    using the Preconditioned Conjugate Gradient (PCG) method with a diagonal preconditioner.\n\n    Args:\n        H: The system matrix (n x n).\n        g: The right-hand side vector (n).\n        x0: The initial guess for the solution (n).\n        tol: The relative residual tolerance for convergence.\n        max_iter: The maximum number of iterations.\n\n    Returns:\n        A tuple containing the solution vector x and the number of iterations performed.\n    \"\"\"\n    x = np.copy(x0)\n    r = g - H @ x\n    \n    g_norm = np.linalg.norm(g)\n    if g_norm == 0:\n        return x, 0\n\n    if np.linalg.norm(r) / g_norm < tol:\n        return x, 0\n\n    diag_H = np.diag(H)\n    z = r / diag_H\n    p = z.copy()\n    rs_old = np.dot(r, z)\n\n    if rs_old == 0:\n        return x, 0\n\n    for i in range(max_iter):\n        Hp = H @ p\n        if np.dot(p, Hp) == 0: break\n        alpha = rs_old / np.dot(p, Hp)\n        \n        x += alpha * p\n        r -= alpha * Hp\n\n        if np.linalg.norm(r) / g_norm < tol:\n            return x, i + 1\n\n        z = r / diag_H\n        rs_new = np.dot(r, z)\n\n        if rs_old == 0: break\n        beta = rs_new / rs_old\n        p = z + beta * p\n        rs_old = rs_new\n        \n    return x, max_iter\n\ndef lbfgs(H: np.ndarray, g: np.ndarray, x0: np.ndarray, m: int, tol: float, max_iter: int, \n          initial_memory: tuple[deque, deque] | None = None) -> tuple[np.ndarray, int, tuple[deque, deque]]:\n    \"\"\"\n    Minimizes the quadratic functional J(x) = 0.5*x'Hx - g'x using the L-BFGS algorithm.\n\n    Args:\n        H: The Hessian matrix (n x n).\n        g: The linear term vector (n).\n        x0: The initial guess for the minimizer (n).\n        m: The memory size for L-BFGS.\n        tol: The relative gradient norm tolerance for convergence.\n        max_iter: The maximum number of iterations.\n        initial_memory: An optional tuple of deques (s_hist, y_hist) to start with.\n\n    Returns:\n        A tuple containing the solution vector x, the number of iterations,\n        and the final L-BFGS memory (s_hist, y_hist).\n    \"\"\"\n    x = np.copy(x0)\n    \n    if initial_memory is None:\n        s_hist = deque(maxlen=m)\n        y_hist = deque(maxlen=m)\n    else:\n        s_hist, y_hist = initial_memory\n\n    g_norm = np.linalg.norm(g)\n    if g_norm == 0:\n        return x, 0, (s_hist, y_hist)\n        \n    grad = H @ x - g\n    if np.linalg.norm(grad) / g_norm < tol:\n        return x, 0, (s_hist, y_hist)\n\n    for i in range(max_iter):\n        q = grad.copy()\n        \n        alphas = []\n        rhos = []\n        for s_i, y_i in zip(reversed(s_hist), reversed(y_hist)):\n            rho_i = 1.0 / np.dot(s_i, y_i)\n            rhos.append(rho_i)\n            alpha_i = rho_i * np.dot(s_i, q)\n            alphas.append(alpha_i)\n            q -= alpha_i * y_i\n\n        if len(y_hist) > 0:\n            s_k_minus_1 = s_hist[-1]\n            y_k_minus_1 = y_hist[-1]\n            ykyk = np.dot(y_k_minus_1, y_k_minus_1)\n            gamma = np.dot(s_k_minus_1, y_k_minus_1) / ykyk if ykyk != 0 else 1.0\n        else:\n            gamma = 1.0\n        \n        r = gamma * q\n        \n        for s_i, y_i, alpha_i, rho_i in zip(s_hist, y_hist, reversed(alphas), reversed(rhos)):\n            beta = rho_i * np.dot(y_i, r)\n            r += s_i * (alpha_i - beta)\n            \n        p = -r\n\n        grad_dot_p = np.dot(grad, p)\n        p_H_p = np.dot(p, H @ p)\n        \n        if p_H_p <= 0: break\n        \n        alpha_step = -grad_dot_p / p_H_p\n\n        s_new = alpha_step * p\n        x_new = x + s_new\n        grad_new = H @ x_new - g\n        y_new = grad_new - grad\n\n        if np.dot(s_new, y_new) > 1e-12:\n            s_hist.append(s_new)\n            y_hist.append(y_new)\n\n        x = x_new\n        grad = grad_new\n\n        if np.linalg.norm(grad) / g_norm < tol:\n            return x, i + 1, (s_hist, y_hist)\n            \n    return x, max_iter, (s_hist, y_hist)\n\n\ndef run_case(H1, H2, n, m_mem, tol, max_iter, tau):\n    \"\"\"\n    Runs the two-loop simulation for a given test case.\n    \"\"\"\n    g = np.ones(n)\n    x0 = np.zeros(n)\n\n    # Outer Loop 1\n    x_pcg1, _ = pcg(H1, g, x0, tol, max_iter)\n    x_lbfgs1, _, memory1 = lbfgs(H1, g, x0, m_mem, tol, max_iter)\n\n    # Outer Loop 2\n    _, iters_pcg2 = pcg(H2, g, x_pcg1, tol, max_iter)\n    \n    memory1_carry = (memory1[0].copy(), memory1[1].copy())\n    _, iters_lbfgs_carry, _ = lbfgs(H2, g, x_lbfgs1, m_mem, tol, max_iter, initial_memory=memory1_carry)\n\n    h1_norm_f = np.linalg.norm(H1, 'fro')\n    delta = np.linalg.norm(H2 - H1, 'fro') / h1_norm_f if h1_norm_f > 0 else np.linalg.norm(H2, 'fro')\n\n    mem_to_use = None\n    if delta <= tau:\n        mem_to_use = (memory1[0].copy(), memory1[1].copy())\n    \n    _, iters_lbfgs_reset, _ = lbfgs(H2, g, x_lbfgs1, m_mem, tol, max_iter, initial_memory=mem_to_use)\n    \n    return [iters_pcg2, iters_lbfgs_carry, iters_lbfgs_reset]\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite, then prints the results.\n    \"\"\"\n    n = 30\n    m_mem = 10\n    tol = 1e-8\n    max_iter = 500\n    tau = 0.2\n    \n    all_results = []\n    \n    # Case 1: Happy path, no curvature change\n    H1_c1 = np.diag(np.arange(1.0, n + 1.0))\n    H2_c1 = H1_c1\n    all_results.extend(run_case(H1_c1, H2_c1, n, m_mem, tol, max_iter, tau))\n\n    # Case 2: Eigenvector rotation with anisotropy\n    D = np.diag(np.logspace(-2, 2, n))\n    Q = np.eye(n)\n    theta = np.pi / 3\n    c, s = np.cos(theta), np.sin(theta)\n    for i in range(0, n, 2):\n        j = i + 1\n        Q[i, i] = c\n        Q[j, j] = c\n        Q[i, j] = -s\n        Q[j, i] = s\n    H1_c2 = D\n    H2_c2 = Q.T @ D @ Q\n    all_results.extend(run_case(H1_c2, H2_c2, n, m_mem, tol, max_iter, tau))\n\n    # Case 3: Eigenvalue rescaling without rotation\n    H1_c3 = np.diag(np.logspace(0, 3, n))\n    H2_c3 = np.diag(np.logspace(-4, 4, n))\n    all_results.extend(run_case(H1_c3, H2_c3, n, m_mem, tol, max_iter, tau))\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "3390407"}, {"introduction": "The standard variational cost function, with its quadratic penalty terms, is optimal under the assumption of Gaussian error distributions. However, real-world observations are often contaminated by outliers that violate this assumption, potentially corrupting the analysis. This practice [@problem_id:3390445] introduces the concept of robust data assimilation by replacing the quadratic penalty with functions that are less punitive towards large errors. To solve this new, non-quadratic minimization problem, you will implement the Iteratively Reweighted Least Squares (IRLS) algorithm, a powerful method that recasts the problem into a sequence of solvable weighted linear systems, fitting elegantly into the incremental 4D-Var framework.", "problem": "Consider the incremental variational assimilation of a state vector in the presence of robust observation terms and non-Gaussian errors. Let the background state be denoted by $x_b \\in \\mathbb{R}^n$, and let the analysis be sought as $x_a = x_b + \\delta x$, where $\\delta x \\in \\mathbb{R}^n$ is the increment. The background error is assumed Gaussian with covariance $B \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite. Observations can be at a single time (three-dimensional variational assimilation (3D-Var)) or over multiple times with a linear model evolution (four-dimensional variational assimilation (4D-Var)). The observation operator is linearized around the background, yielding a matrix $H \\in \\mathbb{R}^{m \\times n}$ for 3D-Var, and time-indexed pairs $(H_k, M_{0 \\rightarrow k})$ for 4D-Var, where $M_{0 \\rightarrow k} \\in \\mathbb{R}^{n \\times n}$ is the tangent linear model from the initial time to time $k$.\n\nDefine the innovation vector $d \\in \\mathbb{R}^m$ as the difference between observations and the background-projected state, and let the observation error scales be collected into $\\sigma \\in \\mathbb{R}^m$ with strictly positive entries. The robust incremental cost to be minimized has the form\n$$\nJ(\\delta x) = \\frac{1}{2}\\, \\delta x^\\top B^{-1} \\delta x + \\sum_{i=1}^m \\rho\\!\\left(\\frac{(G\\,\\delta x - d)_i}{\\sigma_i}\\right),\n$$\nwhere $G \\in \\mathbb{R}^{m \\times n}$ denotes the linearized mapping from $\\delta x$ to observation space (for 3D-Var, $G = H$; for 4D-Var, $G$ stacks $H_k M_{0 \\rightarrow k}$ for all observation times), $(\\cdot)_i$ denotes the $i$-th component, and $\\rho$ is a robust penalty capturing non-Gaussian error characteristics. Assume that $\\rho$ is convex or pseudo-convex and differentiable almost everywhere, with a well-defined first derivative except possibly at isolated points where a limiting derivative can be used.\n\nStarting from fundamental principles of Maximum A Posteriori estimation and linearization, derive a principled iterative algorithm that minimizes $J(\\delta x)$ without relying on closed-form formulas for non-Gaussian observation models. Your algorithm must be consistent with the incremental formulation, respect the linearized dynamics for 4D-Var, and be able to handle at least the following robust penalties: quadratic (Gaussian), Huber, pseudo-Huber, and Student-$t$, each with specified robustness parameters. The algorithm must terminate when the increment change is sufficiently small or when a maximum number of iterations is reached, and should return the computed increment $\\delta x$.\n\nImplement the algorithm in a program and apply it to the following test suite. In all cases, take the background state $x_b$ to be the zero vector so that the innovation equals the observation vector. All numerical values must be used exactly as given. All linear algebra objects are expressed in standard matrix and vector notation.\n\nTest case $1$ (3D-Var, robust against a single outlier):\n- State dimension $n = 2$.\n- Background covariance $B = \\mathrm{diag}([1.0, 1.0])$.\n- Observation operator $H = I_{2 \\times 2}$.\n- Innovations $d = \\begin{bmatrix} 1.0 \\\\ 10.0 \\end{bmatrix}$.\n- Observation scales $\\sigma = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$.\n- Robust penalty: Huber with threshold parameter $\\delta = 1.0$.\n\nTest case $2$ (3D-Var, near-Gaussian regime with small residuals):\n- State dimension $n = 2$.\n- Background covariance $B = \\mathrm{diag}([1.0, 1.0])$.\n- Observation operator $H = I_{2 \\times 2}$.\n- Innovations $d = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$.\n- Observation scales $\\sigma = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$.\n- Robust penalty: Gaussian quadratic.\n\nTest case $3$ (4D-Var, scalar state with heavy-tailed observation errors and a single large outlier):\n- State dimension $n = 1$.\n- Background covariance $B = [1.0]$.\n- Linear model $x_{k+1} = a\\, x_k$ with $a = 0.9$, for times $k = 0, 1, 2, 3$.\n- Observations at times $k = 1, 2, 3, 4$ with $y = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 20.0 \\\\ 1.0 \\end{bmatrix}$; since $x_b = 0$, the innovation equals $d = y$.\n- Observation operator $H_k = [1]$ for all times.\n- Observation scales $\\sigma = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$.\n- Robust penalty: Student-$t$ with degrees-of-freedom parameter $\\nu = 3.0$.\n\nTest case $4$ (4D-Var, scalar state with a stronger outlier and an $\\ell_1$-like robust penalty via pseudo-Huber):\n- State dimension $n = 1$.\n- Background covariance $B = [1.0]$.\n- Linear model $x_{k+1} = a\\, x_k$ with $a = 0.95$, for times $k = 0, 1, 2, 3$.\n- Observations at times $k = 1, 2, 3, 4$ with $y = \\begin{bmatrix} 2.0 \\\\ -2.0 \\\\ 50.0 \\\\ 2.0 \\end{bmatrix}$; since $x_b = 0$, the innovation equals $d = y$.\n- Observation operator $H_k = [1]$ for all times.\n- Observation scales $\\sigma = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$.\n- Robust penalty: pseudo-Huber with parameter $\\delta = 0.1$.\n\nYour program should compute the increment $\\delta x$ for each test case and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of floating-point numbers representing the components of $\\delta x$ for the corresponding test case. For example, the output should have the form $\\left[[\\cdot,\\cdot],[\\cdot,\\cdot],[\\cdot],[\\cdot]\\right]$. All numerical answers must be reported as floating-point numbers. No physical units or angle units are involved in this problem.", "solution": "The problem requires solving a variational assimilation problem where the observation cost term is non-quadratic, designed to be robust to outliers. This makes the overall cost function $J(\\delta x)$ non-quadratic, so it cannot be minimized by solving a single linear system. The Iteratively Reweighted Least Squares (IRLS) algorithm is a standard method for such problems, which is derived and implemented here.\n\n### Derivation of the IRLS Algorithm\n\nThe cost function is:\n$$\nJ(\\delta x) = \\frac{1}{2}\\, \\delta x^\\top B^{-1} \\delta x + \\sum_{i=1}^m \\rho\\!\\left(\\frac{(G\\,\\delta x - d)_i}{\\sigma_i}\\right)\n$$\nwhere $\\rho$ is a robust penalty function. To find the minimum, we set the gradient $\\nabla J(\\delta x)$ to zero. Applying the chain rule to the observation term, and letting $u_i = (G\\,\\delta x - d)_i/\\sigma_i$ be the normalized residual, we get:\n$$\n\\nabla J(\\delta x) = B^{-1} \\delta x + \\sum_{i=1}^m \\frac{G_i^\\top}{\\sigma_i} \\rho'(u_i) = 0\n$$\nwhere $G_i^\\top$ is the $i$-th column of $G^\\top$. This equation is nonlinear in $\\delta x$ because the derivative of the penalty, $\\rho'(u_i)$, depends on the residuals and thus on $\\delta x$.\n\nThe IRLS method linearizes this equation by introducing weights. We rewrite $\\rho'(u_i)$ as $u_i \\cdot (\\rho'(u_i)/u_i)$ and define the weight function $w(u_i) = \\rho'(u_i)/u_i$. The gradient equation becomes:\n$$\nB^{-1} \\delta x + \\sum_{i=1}^m \\frac{G_i^\\top}{\\sigma_i} w(u_i) u_i = 0 \\implies B^{-1} \\delta x + \\sum_{i=1}^m \\frac{G_i^\\top}{\\sigma_i^2} w(u_i) (G_i \\delta x - d_i) = 0\n$$\nIn matrix form, with $W$ as a diagonal matrix of weights $w(u_i)$ and $R_{\\text{obs}}^{-1}$ as the diagonal matrix of $1/\\sigma_i^2$, we have:\n$$\nB^{-1} \\delta x + G^\\top W R_{\\text{obs}}^{-1} (G \\delta x - d) = 0\n$$\nLet $R_k^{-1} = W_k R_{\\text{obs}}^{-1}$ be the effective inverse observation error covariance at iteration $k$. The equation is now a linear system for $\\delta x$, but the weights $W$ depend on $\\delta x$. This suggests an iterative scheme:\n1.  Start with an initial guess $\\delta x^{(0)}$ (e.g., $\\mathbf{0}$).\n2.  At iteration $k$, compute the residuals $u_i^{(k)}$ and weights $w(u_i^{(k)})$ based on the current estimate $\\delta x^{(k)}$.\n3.  Form the effective covariance $R_k^{-1}$ and solve the following linear system for the next iterate $\\delta x^{(k+1)}$:\n    $$\n    (B^{-1} + G^\\top R_k^{-1} G) \\delta x^{(k+1)} = G^\\top R_k^{-1} d\n    $$\nThis process is repeated until $\\delta x$ converges. This is equivalent to successively minimizing a weighted least-squares problem, hence the name.\n\n### Weight Functions for Robust Penalties\nThe weight function $w(u) = \\rho'(u)/u$ is defined for each penalty:\n- **Gaussian**: $\\rho(u) = \\frac{1}{2}u^2 \\implies w(u) = 1$.\n- **Huber**: $\\rho'(u)/u = \\begin{cases} 1 & |u| \\le \\delta \\\\ \\delta/|u| & |u| > \\delta \\end{cases} \\implies w(u) = \\min(1, \\delta/|u|)$.\n- **Pseudo-Huber**: $\\rho'(u) = u/\\sqrt{1+(u/\\delta)^2} \\implies w(u) = 1/\\sqrt{1+(u/\\delta)^2}$.\n- **Student-t**: $\\rho'(u) = (\\nu+1)u/(\\nu+u^2) \\implies w(u) = (\\nu+1)/(\\nu+u^2)$.\n\nObservations with large normalized residuals $|u_i|$ are assigned smaller weights, reducing their influence on the solution, which is the mechanism of robustness. The implementation in the answer follows this derived algorithm for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef get_weights(u, penalty_type, params, epsilon=1e-9):\n    \"\"\"\n    Calculates the weights for the IRLS algorithm for different robust penalties.\n    w(u) = rho'(u) / u.\n    \n    Args:\n        u (np.ndarray): The normalized residuals.\n        penalty_type (str): The name of the penalty function.\n        params (dict): Parameters for the penalty function.\n        epsilon (float): Small number to handle division by zero.\n\n    Returns:\n        np.ndarray: The array of weights.\n    \"\"\"\n    if penalty_type == 'gaussian':\n        return np.ones_like(u)\n    \n    elif penalty_type == 'huber':\n        delta = params['delta']\n        abs_u = np.abs(u)\n        weights = np.ones_like(u)\n        mask = abs_u > delta\n        weights[mask] = delta / abs_u[mask]\n        return weights\n        \n    elif penalty_type == 'pseudo_huber':\n        delta = params['delta']\n        return 1.0 / np.sqrt(1.0 + (u / delta)**2)\n        \n    elif penalty_type == 'student_t':\n        nu = params['nu']\n        return (nu + 1) / (nu + u**2)\n        \n    else:\n        raise ValueError(f\"Unknown penalty type: {penalty_type}\")\n\ndef solve_irls(B, G, d, sigma, penalty_type, penalty_params, max_iter=100, tol=1e-8):\n    \"\"\"\n    Solves the variational assimilation problem using Iteratively Reweighted Least Squares.\n\n    Args:\n        B (np.ndarray): Background error covariance matrix.\n        G (np.ndarray): Linearized observation operator.\n        d (np.ndarray): Innovation vector.\n        sigma (np.ndarray): Observation error scales.\n        penalty_type (str): Type of robust penalty ('gaussian', 'huber', etc.).\n        penalty_params (dict): Parameters for the penalty function.\n        max_iter (int): Maximum number of iterations.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        np.ndarray: The computed state increment delta_x.\n    \"\"\"\n    n = B.shape[0]\n    delta_x = np.zeros(n)\n    \n    B_inv = np.linalg.inv(B)\n    \n    for k in range(max_iter):\n        prev_delta_x = delta_x.copy()\n        \n        # 1. Calculate normalized residuals\n        residuals = G @ delta_x - d\n        normalized_residuals = residuals / sigma\n        \n        # 2. Calculate weights\n        weights = get_weights(normalized_residuals, penalty_type, penalty_params)\n        \n        # 3. Form effective observation error inverse covariance\n        R_inv_eff = np.diag(weights / (sigma**2))\n        \n        # 4. Construct and solve the linear system\n        # (B_inv + G.T @ R_inv_eff @ G) @ delta_x = G.T @ R_inv_eff @ d\n        A = B_inv + G.T @ R_inv_eff @ G\n        b = G.T @ R_inv_eff @ d\n        \n        delta_x = np.linalg.solve(A, b)\n        \n        # 5. Check for convergence\n        if np.linalg.norm(delta_x - prev_delta_x) < tol:\n            break\n            \n    return delta_x\n\ndef solve():\n    \"\"\"\n    Main function to define and solve the test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: 3D-Var, Huber\n        {\n            'type': '3d_var',\n            'n': 2,\n            'B': np.diag([1.0, 1.0]),\n            'H': np.eye(2),\n            'd': np.array([1.0, 10.0]),\n            'sigma': np.array([1.0, 1.0]),\n            'penalty_type': 'huber',\n            'penalty_params': {'delta': 1.0}\n        },\n        # Case 2: 3D-Var, Gaussian\n        {\n            'type': '3d_var',\n            'n': 2,\n            'B': np.diag([1.0, 1.0]),\n            'H': np.eye(2),\n            'd': np.array([0.1, -0.1]),\n            'sigma': np.array([1.0, 1.0]),\n            'penalty_type': 'gaussian',\n            'penalty_params': {}\n        },\n        # Case 3: 4D-Var, Student-t\n        {\n            'type': '4d_var',\n            'n': 1,\n            'B': np.array([[1.0]]),\n            'a': 0.9,\n            'obs_times': np.array([1, 2, 3, 4]),\n            'H_k': np.array([[1.0]]),\n            'd': np.array([1.0, 1.0, 20.0, 1.0]),\n            'sigma': np.array([1.0, 1.0, 1.0, 1.0]),\n            'penalty_type': 'student_t',\n            'penalty_params': {'nu': 3.0}\n        },\n        # Case 4: 4D-Var, pseudo-Huber\n        {\n            'type': '4d_var',\n            'n': 1,\n            'B': np.array([[1.0]]),\n            'a': 0.95,\n            'obs_times': np.array([1, 2, 3, 4]),\n            'H_k': np.array([[1.0]]),\n            'd': np.array([2.0, -2.0, 50.0, 2.0]),\n            'sigma': np.array([1.0, 1.0, 1.0, 1.0]),\n            'penalty_type': 'pseudo_huber',\n            'penalty_params': {'delta': 0.1}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == '3d_var':\n            G = case['H']\n        elif case['type'] == '4d_var':\n            a = case['a']\n            obs_times = case['obs_times']\n            # M_0_k for scalar case is a^k\n            G = np.array([a**k for k in obs_times]).reshape(-1, 1) @ case['H_k']\n        \n        delta_x = solve_irls(\n            case['B'], G, case['d'], case['sigma'],\n            case['penalty_type'], case['penalty_params']\n        )\n        results.append(list(delta_x))\n\n    def format_list(l):\n        return '[' + ','.join(f\"{x:.15g}\" for x in l) + ']'\n    \n    all_results_str = ','.join([format_list(r) for r in results])\n    final_output = f\"[{all_results_str}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3390445"}, {"introduction": "In operational data assimilation, the relationship between the model state and the observations can be complex, often involving transformations in both space and time. This exercise [@problem_id:3390401] tackles a common complexity: delayed observations, where a measurement at time $t$ corresponds to the model state at an earlier time $t-\\tau$. You will derive the correct adjoint of this delay operator from first principles, demonstrating how to properly route the gradient information backward in time using a buffer. This practice underscores that the adjoint is not merely a matrix transpose but a systematic application of the chain rule, and it reinforces the critical importance of gradient checking to ensure the correctness of your scientific code.", "problem": "Consider a linear, discrete-time dynamical system used in four-dimensional variational assimilation (4D-Var). The state vector is denoted by $x_t \\in \\mathbb{R}^n$, and the model evolution is given by the linear operator\n$$\nx_{t+1} = A x_t,\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is a known, time-invariant matrix. Observations are available at times $t = 0,1,\\dots,T-1$, but they are delayed: each observation $y_t \\in \\mathbb{R}^m$ depends on the state $x_{t-\\tau}$ with a fixed delay $\\tau \\in \\mathbb{Z}_{\\ge 0}$, through a linear observation operator $H \\in \\mathbb{R}^{m \\times n}$,\n$$\ny_t = H x_{t-\\tau} + \\varepsilon_t,\n$$\nwhere $\\varepsilon_t$ is observational noise. In the incremental 4D-Var formulation for a linear model, the control variable is the initial state $x_0$, and the cost function to be minimized is\n$$\nJ(x_0) = \\frac{1}{2} (x_0 - x_b)^\\top B^{-1} (x_0 - x_b) + \\frac{1}{2} \\sum_{t=0}^{T-1} \\left( y_t - H \\,\\tilde{x}_{t-\\tau} \\right)^\\top R^{-1} \\left( y_t - H \\,\\tilde{x}_{t-\\tau} \\right),\n$$\nwhere $x_b \\in \\mathbb{R}^n$ is a given background (prior) state, $B \\in \\mathbb{R}^{n \\times n}$ is the symmetric positive-definite background error covariance, $R \\in \\mathbb{R}^{m \\times m}$ is the symmetric positive-definite observation error covariance, and $\\tilde{x}_{t-\\tau}$ denotes the state used for the model equivalent of the observation at time $t$. Specifically,\n- if $t-\\tau \\ge 0$, then $\\tilde{x}_{t-\\tau} = x_{t-\\tau}$, where $x_{t-\\tau}$ is obtained by propagating $x_0$ forward using the model $x_{t+1} = A x_t$,\n- if $t-\\tau < 0$, then $\\tilde{x}_{t-\\tau} = x^{\\mathrm{pre}}_{t-\\tau}$ is a known pre-window state that does not depend on $x_0$.\n\nThe aim is to incorporate the delay $\\tau$ into the adjoint-based gradient computation correctly, by using a delay-aware observation accumulation (a buffer) during the adjoint sweep, so that the gradient $\\nabla J(x_0)$ is computed accurately. The derivation must start from first principles: the linear model evolution, the definition of the cost function, and the chain rule for differentiation in time-dependent systems. You must not use shortcut formulas; you must derive the adjoint recursion and the buffer accumulation rule that places observation contributions at the correct model time indices.\n\nYour program must implement the following:\n1. A forward model that propagates $x_0$ to $x_t$ for $t = 0,1,\\dots,T-1$ using $x_{t+1} = A x_t$.\n2. A cost function $J(x_0)$ as defined above, using $\\tilde{x}_{t-\\tau}$ with the pre-window substitution for indices $t-\\tau < 0$.\n3. An adjoint computation of the gradient $\\nabla J(x_0)$ with respect to $x_0$, using a delay buffer so that each observation mismatch at time $t$ contributes to the adjoint forcing at the correct state time index $s = t - \\tau$ whenever $s \\ge 0$, and contributes nothing to the gradient when $s < 0$.\n\nYou must validate your gradient implementation by comparing the adjoint-produced directional derivative $\\nabla J(x_0)^\\top d$ against a centered finite-difference approximation $(J(x_0 + h d) - J(x_0 - h d)) / (2h)$ for a specified direction $d \\in \\mathbb{R}^n$ and a small step $h \\in \\mathbb{R}_{>0}$. Report the relative error between these two quantities defined as\n$$\n\\mathrm{err} = \\frac{\\left| \\nabla J(x_0)^\\top d - \\frac{J(x_0 + h d) - J(x_0 - h d)}{2h} \\right|}{\\max\\left(10^{-12}, \\left| \\nabla J(x_0)^\\top d \\right| + \\left| \\frac{J(x_0 + h d) - J(x_0 - h d)}{2h} \\right| \\right)}.\n$$\n\nUse the following fixed numerical setup for all test cases:\n- State dimension $n = 3$ and observation dimension $m = 2$.\n- Model matrix\n$$\nA = \\begin{bmatrix}\n0.9 & 0.1 & 0.0 \\\\\n-0.1 & 0.95 & 0.05 \\\\\n0.0 & -0.05 & 0.9\n\\end{bmatrix}.\n$$\n- Observation operator\n$$\nH = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{bmatrix}.\n$$\n- Background error covariance\n$$\nB = \\mathrm{diag}(0.5^2,\\, 0.7^2,\\, 0.3^2),\n$$\nand observation error covariance\n$$\nR = \\mathrm{diag}(0.2^2,\\, 0.3^2).\n$$\n- Background state\n$$\nx_b = \\begin{bmatrix} 0.5 \\\\ -0.2 \\\\ 0.1 \\end{bmatrix}.\n$$\n- Initial guess for the control (the state to be optimized)\n$$\nx_0 = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.05 \\end{bmatrix}.\n$$\n- Direction for the directional derivative\n$$\nd = \\begin{bmatrix} 0.3 \\\\ -0.4 \\\\ 0.5 \\end{bmatrix},\n$$\nand finite-difference step $h = 10^{-6}$.\n\nConstruct synthetic observations without noise using a \"true\" pre-window initialization given by\n$$\nx^{\\mathrm{true}}_{-\\tau} = \\begin{bmatrix} 0.8 \\\\ -0.6 \\\\ 0.3 \\end{bmatrix},\n$$\npropagated forward by the same model $x_{s+1} = A x_s$ to obtain $x^{\\mathrm{true}}_s$ for all $s = -\\tau, -\\tau + 1, \\dots, T - 1$, and then set $y_t = H x^{\\mathrm{true}}_{t-\\tau}$ for each $t$. For indices $t - \\tau < 0$, the known pre-window states used in the cost function must be exactly $x^{\\mathrm{true}}_{t-\\tau}$.\n\nTest Suite:\n- Case 1 (happy path): window length $T = 6$, delay $\\tau = 0$.\n- Case 2 (delayed observations within window): window length $T = 6$, delay $\\tau = 2$.\n- Case 3 (all observations before window): window length $T = 6$, delay $\\tau = 7$.\n\nFor each case, compute the relative error $\\mathrm{err}$ of the adjoint gradient check as defined above. Your program should produce a single line of output containing the three relative errors as a comma-separated list enclosed in square brackets, for example, $[e_1,e_2,e_3]$, where each $e_i$ is a floating-point number.", "solution": "The problem requires the derivation and implementation of a gradient calculation for a four-dimensional variational (4D-Var) cost function where observations are subject to a time delay. The gradient must be computed using the adjoint method, correctly accounting for the delay. The correctness of the implementation is verified by comparing its output against a finite-difference approximation.\n\n### 1. Mathematical Derivation of the Adjoint Model\n\nThe core of the problem lies in computing the gradient $\\nabla J(x_0)$ of the cost function $J(x_0)$ with respect to the initial state $x_0$. The cost function is:\n$$\nJ(x_0) = J_b(x_0) + J_o(x_0)\n$$\nwhere the background term $J_b(x_0)$ and the observation term $J_o(x_0)$ are:\n$$\nJ_b(x_0) = \\frac{1}{2} (x_0 - x_b)^\\top B^{-1} (x_0 - x_b)\n$$\n$$\nJ_o(x_0) = \\frac{1}{2} \\sum_{t=0}^{T-1} \\left( y_t - H \\,\\tilde{x}_{t-\\tau} \\right)^\\top R^{-1} \\left( y_t - H \\,\\tilde{x}_{t-\\tau} \\right)\n$$\nThe state $\\tilde{x}_{t-\\tau}$ depends on the control variable $x_0$ only if its time index $s = t-\\tau$ is non-negative ($s \\ge 0$). For $s \\ge 0$, we have $\\tilde{x}_s = x_s = A^s x_0$. For $s < 0$, $\\tilde{x}_s = x^{\\mathrm{pre}}_s$ is a known pre-window state and does not depend on $x_0$.\n\nThe gradient is defined via the first variation, $\\delta J$, such that $\\delta J = (\\nabla J(x_0))^\\top \\delta x_0$ for a small perturbation $\\delta x_0$.\n\n**Gradient of the Background Term:**\nThe gradient of the background term is straightforward:\n$$\n\\nabla J_b(x_0) = B^{-1}(x_0 - x_b)\n$$\n\n**Gradient of the Observation Term (Adjoint Method):**\nThe adjoint method efficiently computes the gradient of a functional that depends on a long state trajectory. Let's define the adjoint state vector $\\lambda_t \\in \\mathbb{R}^n$. For a linear system $x_{t+1} = A x_t$, the adjoint recursion is $\\lambda_{t-1} = A^\\top \\lambda_t + f_{t-1}$, where $f_{t-1}$ is the partial gradient of the cost function with respect to the state $x_{t-1}$.\n\nThe gradient of the cost function term for observation $y_t$ with respect to the state $\\tilde{x}_{t-\\tau}$ is:\n$$\n\\nabla_{\\tilde{x}_{t-\\tau}} \\left[ \\frac{1}{2} \\left( y_t - H \\,\\tilde{x}_{t-\\tau} \\right)^\\top R^{-1} \\left( y_t - H \\,\\tilde{x}_{t-\\tau} \\right) \\right] = -H^\\top R^{-1} \\left( y_t - H \\,\\tilde{x}_{t-\\tau} \\right)\n$$\nThis gradient is the forcing applied to the adjoint model at the time index of the state, i.e., at time $s = t-\\tau$. This is only relevant if $s \\ge 0$, as states with $s < 0$ are not part of the control problem's trajectory. A state $x_s$ (where $s \\ge 0$) is influenced by the observation $y_{s+\\tau}$. The key is to iterate through all observations, and for each observation $y_t$, accumulate its corresponding forcing at the correct state time $s = t-\\tau$.\n\nThe complete adjoint-based algorithm for computing $\\nabla J(x_0)$ is as follows:\n\n1.  **Forward Propagation**: Given the initial state guess $x_0$, compute the model trajectory for $t=0, 1, \\dots, T-1$: $x_t = A^t x_0$. This trajectory $\\{x_0, x_1, \\dots, x_{T-1}\\}$ must be stored.\n\n2.  **Adjoint Forcing Accumulation**: Create a forcing buffer vector $f_s \\in \\mathbb{R}^n$ for each state time $s=0, 1, \\dots, T-1$, initialized to zero. Then, iterate through observation times $t=0, 1, \\dots, T-1$:\n    - Calculate the state time index $s = t - \\tau$.\n    - If $s \\ge 0$:\n        - The observation $y_t$ depends on the model state $x_s$.\n        - Compute the innovation: $d_t = y_t - H x_s$.\n        - Add the forcing contribution to the buffer at index $s$: $f_s \\leftarrow f_s - H^\\top R^{-1} d_t$.\n    - If $s < 0$, the observation $y_t$ depends on a pre-window state and thus does not contribute to the gradient.\n\n3.  **Backward Adjoint Integration**: Integrate the adjoint model backward in time from $t=T-1$ to $t=0$.\n    - Initialize the terminal adjoint state: $\\lambda_T = 0$.\n    - For $t = T-1, T-2, \\dots, 0$: $\\lambda_t = A^\\top \\lambda_{t+1} + f_t$.\n\n4.  **Assemble the Gradient**: The gradient of the observation term with respect to the initial state is $\\nabla J_o(x_0) = \\lambda_0$. The total gradient is the sum of the background and observation contributions:\n    $$\n    \\nabla J(x_0) = B^{-1}(x_0 - x_b) + \\lambda_0\n    $$\n\n### 2. Gradient Verification\n\nTo validate the implementation, we use a centered finite-difference approximation of the directional derivative, $\\frac{J(x_0 + h d) - J(x_0 - h d)}{2h}$. A very small relative error between the adjoint computation and the finite-difference approximation confirms the correctness of the adjoint-derived gradient. The provided Python code implements this logic, computes the gradient check for each test case, and reports the relative errors.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 4D-Var problem with delayed observations for three test cases\n    and validates the adjoint-computed gradient against a finite-difference approximation.\n    \"\"\"\n    \n    # --- Fixed numerical setup ---\n    n = 3  # State dimension\n    m = 2  # Observation dimension\n\n    A = np.array([\n        [0.9, 0.1, 0.0],\n        [-0.1, 0.95, 0.05],\n        [0.0, -0.05, 0.9]\n    ])\n\n    H = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0]\n    ])\n\n    # Covariance matrices and their inverses (using diagonal representation)\n    B_diag = np.array([0.5**2, 0.7**2, 0.3**2])\n    B_inv_diag = 1.0 / B_diag\n    R_diag = np.array([0.2**2, 0.3**2])\n    R_inv_diag = 1.0 / R_diag\n\n    # Background and initial guess\n    xb = np.array([0.5, -0.2, 0.1])\n    x0 = np.array([0.2, -0.1, 0.05])\n\n    # Direction for gradient check\n    d = np.array([0.3, -0.4, 0.5])\n    h = 1e-6\n\n    # True pre-window initialization for synthetic data\n    xtrue_init_pre_window = np.array([0.8, -0.6, 0.3])\n\n    test_cases = [\n        (6, 0),  # Case 1: T=6, tau=0\n        (6, 2),  # Case 2: T=6, tau=2\n        (6, 7),  # Case 3: T=6, tau=7\n    ]\n    \n    results = []\n\n    for T, tau in test_cases:\n\n        # --- Generate synthetic data for the current case ---\n        # The true trajectory must span from -tau to T-1\n        min_time_idx = -tau\n        max_time_idx = T - 1\n        \n        xtrue_traj = {}\n        xtrue_traj[min_time_idx] = xtrue_init_pre_window\n        \n        current_x = xtrue_init_pre_window\n        for s in range(min_time_idx, max_time_idx):\n            current_x = A @ current_x\n            xtrue_traj[s + 1] = current_x\n            \n        # Generate observations y_t = H * x_true_{t-tau}\n        y_obs = np.zeros((T, m))\n        for t in range(T):\n            state_idx = t - tau\n            y_obs[t] = H @ xtrue_traj[state_idx]\n\n        # --- Define cost function and gradient for the current case ---\n        \n        def run_forward_model(x0_in, T_in):\n            \"\"\"Propagates x0_in forward in time for T_in steps.\"\"\"\n            traj = np.zeros((T_in, n))\n            if T_in > 0:\n                traj[0] = x0_in\n                for t in range(T_in - 1):\n                    traj[t + 1] = A @ traj[t]\n            return traj\n\n        def cost_function(x0_in):\n            \"\"\"Computes the 4D-Var cost function J(x0_in).\"\"\"\n            # Background term\n            jb = 0.5 * np.sum(B_inv_diag * (x0_in - xb)**2)\n            \n            # Observation term\n            jo = 0.0\n            \n            # Run forward model for the trajectory starting from x0_in\n            model_traj = run_forward_model(x0_in, T)\n\n            for t in range(T):\n                s = t - tau\n                if s >= 0:\n                    model_state = model_traj[s]\n                else:\n                    # Use known pre-window state\n                    model_state = xtrue_traj[s]\n\n                innovation = y_obs[t] - H @ model_state\n                jo += 0.5 * np.sum(R_inv_diag * innovation**2)\n            \n            return jb + jo\n\n        def compute_gradient(x0_in):\n            \"\"\"Computes the gradient of J at x0_in using the adjoint method.\"\"\"\n            # Gradient of the background term\n            grad_b = B_inv_diag * (x0_in - xb)\n            \n            # Run forward model and store trajectory\n            model_traj = run_forward_model(x0_in, T)\n\n            # Compute and accumulate adjoint forcings\n            forcings = np.zeros((T, n))\n            for t in range(T):\n                s = t - tau\n                if s >= 0:\n                    innovation = y_obs[t] - H @ model_traj[s]\n                    # Note: H.T @ (diag(w) * v) is equivalent to H.T @ (w * v) in numpy\n                    forcing_term = -H.T @ (R_inv_diag * innovation)\n                    forcings[s] += forcing_term\n            \n            # Run adjoint model backward\n            lam = np.zeros(n)  # This is lambda_{t+1}\n            for t in range(T - 1, -1, -1):\n                # lam at time t = A.T @ lam_{t+1} + f_t\n                lam = A.T @ lam + forcings[t]\n            \n            # lam is now lambda_0\n            grad_o = lam\n            \n            return grad_b + grad_o\n\n        # --- Perform gradient check ---\n        grad_adj = compute_gradient(x0)\n        dir_deriv_adj = grad_adj.T @ d\n\n        J_plus = cost_function(x0 + h * d)\n        J_minus = cost_function(x0 - h * d)\n        dir_deriv_fd = (J_plus - J_minus) / (2 * h)\n\n        numerator = np.abs(dir_deriv_adj - dir_deriv_fd)\n        denominator = max(1e-12, np.abs(dir_deriv_adj) + np.abs(dir_deriv_fd))\n        \n        relative_error = numerator / denominator\n        results.append(relative_error)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3390401"}]}