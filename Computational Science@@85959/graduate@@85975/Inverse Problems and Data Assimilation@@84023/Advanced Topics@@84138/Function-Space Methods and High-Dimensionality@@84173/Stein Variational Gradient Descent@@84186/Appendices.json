{"hands_on_practices": [{"introduction": "This first exercise demystifies the Stein Variational Gradient Descent (SVGD) update rule by applying it to a simple, concrete scenario. By manually calculating the new positions of two particles targeting a standard normal distribution, you will gain a tangible feel for how the score function (attraction) and the kernel gradient (repulsion) combine to create the particle velocity. This foundational practice is essential for building intuition about the algorithm's behavior at the most granular level [@problem_id:3348310].", "problem": "Consider Stein Variational Gradient Descent (SVGD), which performs functional gradient descent on the Kullback–Leibler divergence by transporting a particle distribution via a velocity field in a Reproducing Kernel Hilbert Space (RKHS). In one dimension, the Stein operator for a differentiable target density $p(x)$ acting on a scalar test function $f$ is $T_{p} f(x) = f^{\\prime}(x) + f(x)\\,\\nabla_{x} \\log p(x)$. Let the target be the standard normal density $p(x) = \\mathcal{N}(0,1)$, and let the RKHS be induced by the Radial Basis Function (RBF) kernel with bandwidth parameter $h > 0$ given by $k_{h}(x,x^{\\prime}) = \\exp\\!\\big(-\\frac{(x-x^{\\prime})^{2}}{2 h^{2}}\\big)$. You are given two initial particles at locations $x_{1}^{(0)} = -1$ and $x_{2}^{(0)} = 2$. Using the SVGD framework starting from the Stein operator definition and the RKHS representer property (without assuming any pre-stated closed-form update), derive the empirical SVGD particle update for a single iteration with step size $\\epsilon > 0$, and then compute the updated particle locations $x_{1}^{(1)}$ and $x_{2}^{(1)}$ explicitly as analytic functions of $h$ and $\\epsilon$. Express your final result in exact closed form (no rounding). The final answer must be a single row vector containing $x_{1}^{(1)}$ and $x_{2}^{(1)}$ as functions of $h$ and $\\epsilon$.", "solution": "The user has asked for a derivation of the Stein Variational Gradient Descent (SVGD) update rule and its application to a specific two-particle system.\n\n### Step 1: Problem Validation\nThe problem statement is validated against the specified criteria.\n\n- **Givens**:\n    -   Method: Stein Variational Gradient Descent (SVGD).\n    -   Stein operator ($1D$): $T_{p} f(x) = f^{\\prime}(x) + f(x)\\,\\nabla_{x} \\log p(x)$.\n    -   Target density: $p(x) = \\mathcal{N}(0,1)$, the standard normal distribution.\n    -   Reproducing Kernel Hilbert Space (RKHS): Induced by the Radial Basis Function (RBF) kernel $k_{h}(x,x^{\\prime}) = \\exp(-\\frac{(x-x^{\\prime})^{2}}{2 h^{2}})$ with bandwidth $h > 0$.\n    -   Initial particle locations: $x_{1}^{(0)} = -1$ and $x_{2}^{(0)} = 2$.\n    -   Number of particles: $N=2$.\n    -   Iteration step size: $\\epsilon > 0$.\n-   **Validation**:\n    -   **Scientifically Grounded:** The problem uses standard, well-established concepts from Bayesian inference, machine learning, and functional analysis (SVGD, KL divergence, RKHS, Stein's method). It is scientifically sound.\n    -   **Well-Posed:** The problem provides all necessary information (target density, kernel, initial conditions, step size) to compute a unique, single-step update for the particle locations.\n    -   **Objective:** The problem is phrased in precise, technical language with no subjective or ambiguous elements.\n    -   The problem is self-contained, consistent, and feasible.\n\n-   **Verdict**: The problem is valid.\n\n### Step 2: Derivation of the SVGD Update Rule\n\nThe objective of SVGD is to transport an initial particle distribution $q_0$ to a target distribution $p$ by applying a velocity field $\\phi(x)$ to the particles. The particles are updated as $x' = x + \\epsilon \\phi(x)$. The optimal velocity field $\\phi(x)$ is the one that maximally decreases the Kullback–Leibler (KL) divergence, $KL(q || p)$, at each step. This corresponds to performing functional gradient descent on the KL divergence in an RKHS $\\mathcal{H}$.\n\nThe directional derivative of the KL divergence with respect to a perturbation in the direction of $\\phi$ is given by:\n$$ \\nabla_{\\phi} KL(q || p) = - \\mathbb{E}_{x \\sim q}[T_p \\phi(x)] $$\nwhere $T_p$ is the Stein operator. To achieve the steepest descent, we must choose $\\phi$ to maximize $\\mathbb{E}_{x \\sim q}[T_p \\phi(x)]$ subject to the constraint that $\\phi$ lies within a unit ball in the RKHS, i.e., $\\|\\phi\\|_{\\mathcal{H}} \\le 1$.\n\nThe expression $F[\\phi] = \\mathbb{E}_{x \\sim q}[T_p \\phi(x)]$ is a linear functional of $\\phi$. By the Riesz representation theorem, for any such linear functional on an RKHS, there exists a unique element $\\psi_q \\in \\mathcal{H}$ such that $F[\\phi] = \\langle \\phi, \\psi_q \\rangle_{\\mathcal{H}}$ for all $\\phi \\in \\mathcal{H}$. The function $\\phi$ that maximizes this inner product under the norm constraint is $\\phi = \\psi_q / \\|\\psi_q\\|_{\\mathcal{H}}$. The direction of steepest ascent is therefore given by $\\psi_q$.\n\nWe can find $\\psi_q$ using the reproducing property of the kernel $k(x, x')$. For any function $f \\in \\mathcal{H}$ and point $y$, we have $f(y) = \\langle f(\\cdot), k(y, \\cdot) \\rangle_{\\mathcal{H}}$. Applying this to $\\psi_q$:\n$$ \\psi_q(y) = \\langle \\psi_q(\\cdot), k(y, \\cdot) \\rangle_{\\mathcal{H}} $$\nSince the inner product is symmetric, $\\langle f, g \\rangle_{\\mathcal{H}} = \\langle g, f \\rangle_{\\mathcal{H}}$, and from the definition of $\\psi_q$, we have $\\langle k(y, \\cdot), \\psi_q(\\cdot) \\rangle_{\\mathcal{H}} = F[k(y, \\cdot)]$. Therefore:\n$$ \\psi_q(y) = F[k(y, \\cdot)] = \\mathbb{E}_{x \\sim q}[T_{p,x} k(y, x)] $$\nHere, $T_{p,x}$ denotes that the Stein operator acts on $k(y,x)$ as a function of its second argument, $x$.\nFor a symmetric kernel, $k(y,x) = k(x,y)$, the optimal velocity field at a point $y$ is:\n$$ \\phi^*(y) = \\psi_q(y) = \\mathbb{E}_{x \\sim q}[T_{p,x} k(x,y)] = \\mathbb{E}_{x \\sim q}[\\nabla_x k(x,y) + k(x,y) \\nabla_x \\log p(x)] $$\nIn practice, the expectation over $q$ is approximated by an empirical average over the current set of $N$ particles $\\{x_i\\}_{i=1}^N$:\n$$ \\phi(y) \\approx \\frac{1}{N} \\sum_{i=1}^N [\\nabla_{x_i} k(x_i, y) + k(x_i, y) \\nabla_{x_i} \\log p(x_i)] $$\nThe SVGD update for a particle $x_j$ is then $x_j^{(t+1)} = x_j^{(t)} + \\epsilon \\phi(x_j^{(t)})$, where the velocity field is evaluated at the particle's current location $x_j^{(t)}$:\n$$ \\phi(x_j^{(t)}) = \\frac{1}{N} \\sum_{i=1}^N [\\nabla_{x_i} k(x_i^{(t)}, x_j^{(t)}) + k(x_i^{(t)}, x_j^{(t)}) \\nabla_{x_i} \\log p(x_i^{(t)})] $$\n\n### Step 3: Application to the Given Problem\n\nWe apply the derived update rule to the specifics of the problem.\n1.  **Target Distribution**: $p(x) = \\mathcal{N}(0,1) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{x^2}{2})$.\n    The log-density is $\\log p(x) = -\\frac{x^2}{2} - \\frac{1}{2}\\log(2\\pi)$.\n    The score function (gradient of the log-density) is $\\nabla_x \\log p(x) = -x$.\n\n2.  **Kernel**: The RBF kernel is $k(x, x') = \\exp(-\\frac{(x-x')^2}{2h^2})$.\n    Its gradient with respect to the first argument is:\n    $$ \\nabla_x k(x, x') = \\frac{\\partial}{\\partial x} \\exp\\left(-\\frac{(x-x')^2}{2h^2}\\right) = \\exp\\left(-\\frac{(x-x')^2}{2h^2}\\right) \\cdot \\left(-\\frac{2(x-x')}{2h^2}\\right) = -k(x,x') \\frac{x-x'}{h^2} $$\n\n3.  **Particle Update Equation**: Let the particle locations at iteration $t=0$ be $\\{x_i^{(0)}\\}_{i=1}^N$. The velocity field for particle $x_j^{(0)}$ is:\n    $$ \\phi(x_j^{(0)}) = \\frac{1}{N} \\sum_{i=1}^N \\left[ -k(x_i^{(0)}, x_j^{(0)}) \\frac{x_i^{(0)} - x_j^{(0)}}{h^2} + k(x_i^{(0)}, x_j^{(0)}) (-x_i^{(0)}) \\right] $$\n    $$ \\phi(x_j^{(0)}) = \\frac{1}{N} \\sum_{i=1}^N k(x_i^{(0)}, x_j^{(0)}) \\left( -\\frac{x_i^{(0)} - x_j^{(0)}}{h^2} - x_i^{(0)} \\right) $$\n\n4.  **Initial Conditions**: We have $N=2$ particles at $x_1^{(0)} = -1$ and $x_2^{(0)} = 2$.\n\n5.  **Compute update for $x_1^{(1)}$**: Let $j=1$.\n    $$ \\phi(x_1^{(0)}) = \\frac{1}{2} \\left[ k(x_1^{(0)}, x_1^{(0)}) \\left( -\\frac{x_1^{(0)} - x_1^{(0)}}{h^2} - x_1^{(0)} \\right) + k(x_2^{(0)}, x_1^{(0)}) \\left( -\\frac{x_2^{(0)} - x_1^{(0)}}{h^2} - x_2^{(0)} \\right) \\right] $$\n    We compute the terms:\n    -   $x_1^{(0)} = -1$, $x_2^{(0)} = 2$.\n    -   $k(x_1^{(0)}, x_1^{(0)}) = \\exp(0) = 1$.\n    -   $k(x_2^{(0)}, x_1^{(0)}) = \\exp\\left(-\\frac{(2 - (-1))^2}{2h^2}\\right) = \\exp\\left(-\\frac{9}{2h^2}\\right)$.\n    -   Term for $i=1$: $1 \\cdot (0 - (-1)) = 1$.\n    -   Term for $i=2$: $\\exp\\left(-\\frac{9}{2h^2}\\right) \\left( -\\frac{2 - (-1)}{h^2} - 2 \\right) = \\exp\\left(-\\frac{9}{2h^2}\\right) \\left( -\\frac{3}{h^2} - 2 \\right)$.\n    The velocity is:\n    $$ \\phi(x_1^{(0)}) = \\frac{1}{2} \\left[ 1 - \\left( \\frac{3}{h^2} + 2 \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) \\right] $$\n    The updated particle location is:\n    $$ x_1^{(1)} = x_1^{(0)} + \\epsilon \\phi(x_1^{(0)}) = -1 + \\frac{\\epsilon}{2} \\left[ 1 - \\left( 2 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) \\right] $$\n\n6.  **Compute update for $x_2^{(1)}$**: Let $j=2$.\n    $$ \\phi(x_2^{(0)}) = \\frac{1}{2} \\left[ k(x_1^{(0)}, x_2^{(0)}) \\left( -\\frac{x_1^{(0)} - x_2^{(0)}}{h^2} - x_1^{(0)} \\right) + k(x_2^{(0)}, x_2^{(0)}) \\left( -\\frac{x_2^{(0)} - x_2^{(0)}}{h^2} - x_2^{(0)} \\right) \\right] $$\n    We compute the terms:\n    -   $k(x_1^{(0)}, x_2^{(0)}) = \\exp\\left(-\\frac{(-1 - 2)^2}{2h^2}\\right) = \\exp\\left(-\\frac{9}{2h^2}\\right)$.\n    -   $k(x_2^{(0)}, x_2^{(0)}) = 1$.\n    -   Term for $i=1$: $\\exp\\left(-\\frac{9}{2h^2}\\right) \\left( -\\frac{-1 - 2}{h^2} - (-1) \\right) = \\exp\\left(-\\frac{9}{2h^2}\\right) \\left( \\frac{3}{h^2} + 1 \\right)$.\n    -   Term for $i=2$: $1 \\cdot (0 - 2) = -2$.\n    The velocity is:\n    $$ \\phi(x_2^{(0)}) = \\frac{1}{2} \\left[ \\left( \\frac{3}{h^2} + 1 \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) - 2 \\right] $$\n    The updated particle location is:\n    $$ x_2^{(1)} = x_2^{(0)} + \\epsilon \\phi(x_2^{(0)}) = 2 + \\frac{\\epsilon}{2} \\left[ \\left( 1 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) - 2 \\right] $$\n    This simplifies to:\n    $$ x_2^{(1)} = 2 - \\epsilon + \\frac{\\epsilon}{2} \\left( 1 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) $$\nThe final expressions for the updated particle locations are:\n$$ x_1^{(1)} = -1 + \\frac{\\epsilon}{2} \\left[ 1 - \\left( 2 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) \\right] $$\n$$ x_2^{(1)} = 2 - \\epsilon + \\frac{\\epsilon}{2} \\left( 1 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) $$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-1 + \\frac{\\epsilon}{2} \\left( 1 - \\left(2 + \\frac{3}{h^2}\\right) \\exp\\left(-\\frac{9}{2h^2}\\right) \\right) & 2 - \\epsilon + \\frac{\\epsilon}{2} \\left( 1 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right)\n\\end{pmatrix}\n}\n$$", "id": "3348310"}, {"introduction": "A key feature of SVGD is its ability to maintain particle diversity and avoid collapsing to a single point, a common failure mode in other optimization methods. This practice delves into the heart of this mechanism by analyzing the balance between the attractive force pulling particles toward high-density regions and the repulsive force from the kernel pushing them apart [@problem_id:3348300]. By deriving the critical separation distance where these forces equilibrate for a two-particle system, you will quantitatively understand how SVGD prevents mode collapse.", "problem": "Consider Stein Variational Gradient Descent (SVGD), defined by the functional gradient flow that transports a set of particles to approximate a target density $p(x)$. For a finite set of $n$ particles $\\{x_{i}\\}_{i=1}^{n}$ in $\\mathbb{R}^{d}$, the empirical SVGD velocity at particle $x_{i}$ is\n$$\n\\phi(x_{i}) \\;=\\; \\frac{1}{n} \\sum_{j=1}^{n} \\Big( k(x_{j},x_{i}) \\,\\nabla \\log p(x_{j}) \\;+\\; \\nabla_{x_{j}} k(x_{j},x_{i}) \\Big),\n$$\nwhere $k(x,y)$ is a positive definite kernel and $\\nabla$ denotes gradient with respect to the argument. In SVGD, the first term is the attraction toward high-density regions and the second term is a repulsion that spreads particles. Let the kernel be the Gaussian radial basis function (RBF) kernel $k(x,y) = \\exp\\!\\big(-\\|x-y\\|^{2}/(2 h^{2})\\big)$ with bandwidth $h>0$.\n\nTask A (scaling of repulsion): In one dimension ($d=1$), for $n=2$ particles at locations $x_{1}$ and $x_{2}$ with inter-particle distance $d = |x_{1}-x_{2}|$, derive the magnitude of the pairwise repulsion contribution from $x_{2}$ to the velocity of $x_{1}$ as a function of $d$ and $h$. You must express it in closed form using only $d$ and $h$.\n\nTask B (quantitative avoidance of mode collapse): Consider the one-dimensional Gaussian target $p(x) = \\mathcal{N}(0,\\sigma^{2})$ with variance $\\sigma^{2} > 0$. Place $n=2$ particles symmetrically at $x_{1}=a$ and $x_{2}=-a$ with $a>0$. Using only the SVGD definition above and the RBF kernel given, compute the exact condition under which the radial component of the SVGD velocity at $x_{1}$ points outward (i.e., increases $|a|$). Then, solve for the unique critical separation $a_{c}(h,\\sigma)$ for which the radial SVGD velocity at $x_{1}$ is exactly zero. Your final answer must be a single closed-form analytic expression for $a_{c}(h,\\sigma)$ with no unevaluated limits or implicit definitions.\n\nAnswer format requirement: Provide only the analytic expression for $a_{c}(h,\\sigma)$ as your final answer. Do not include units. No rounding is required.", "solution": "The user has provided a valid problem statement concerning the properties of Stein Variational Gradient Descent (SVGD). The problem is scientifically grounded, well-posed, and contains all necessary information for a rigorous mathematical derivation. I will proceed with a complete solution.\n\nThe problem is divided into two tasks. I will solve them sequentially to arrive at the final required expression for the critical separation $a_{c}(h, \\sigma)$.\n\nFirst, let's establish the necessary mathematical components.\nThe SVGD velocity at particle $x_i$ is given by:\n$$ \\phi(x_{i}) = \\frac{1}{n} \\sum_{j=1}^{n} \\left( k(x_{j},x_{i}) \\nabla \\log p(x_{j}) + \\nabla_{x_{j}} k(x_{j},x_{i}) \\right) $$\nThe kernel is the one-dimensional ($d=1$) Gaussian RBF kernel:\n$$ k(x,y) = \\exp\\left(-\\frac{(x-y)^2}{2h^2}\\right) $$\nThe gradient of the kernel with respect to its first argument, $x_j$, is:\n$$ \\nabla_{x_{j}} k(x_{j},x_{i}) = \\frac{\\partial}{\\partial x_j} \\exp\\left(-\\frac{(x_j-x_i)^2}{2h^2}\\right) = \\exp\\left(-\\frac{(x_j-x_i)^2}{2h^2}\\right) \\cdot \\left(-\\frac{2(x_j-x_i)}{2h^2}\\right) = -\\frac{x_j-x_i}{h^2} k(x_j,x_i) $$\n\n**Task A: Scaling of Repulsion**\n\nThis task requires the magnitude of the pairwise repulsion contribution from particle $x_2$ to the velocity of particle $x_1$. The number of particles is $n=2$. The repulsion contribution is the part of the velocity update involving the kernel gradient. For the velocity $\\phi(x_1)$, the term corresponding to $j=2$ contains the repulsion from $x_2$. This contribution is given by $\\frac{1}{n} \\nabla_{x_2} k(x_2, x_1)$.\n\nUsing $n=2$ and the derivative calculated above:\n$$ \\text{Repulsion from } x_2 \\text{ on } x_1 = \\frac{1}{2} \\nabla_{x_2} k(x_2, x_1) = \\frac{1}{2} \\left( -\\frac{x_2-x_1}{h^2} k(x_2, x_1) \\right) = \\frac{x_1-x_2}{2h^2} k(x_1, x_2) $$\nThe problem asks for the magnitude of this term. Let $d = |x_1 - x_2|$. The kernel $k(x_1, x_2)$ is always positive.\n$$ k(x_1, x_2) = \\exp\\left(-\\frac{(x_1-x_2)^2}{2h^2}\\right) = \\exp\\left(-\\frac{d^2}{2h^2}\\right) $$\nThe magnitude is therefore:\n$$ \\left| \\frac{x_1-x_2}{2h^2} k(x_1, x_2) \\right| = \\frac{|x_1-x_2|}{2h^2} k(x_1, x_2) = \\frac{d}{2h^2} \\exp\\left(-\\frac{d^2}{2h^2}\\right) $$\nThis expression represents the magnitude of the repulsion force exerted by one particle on another as a function of their separation $d$ and the kernel bandwidth $h$.\n\n**Task B: Quantitative Avoidance of Mode Collapse**\n\nThis task requires us to find the critical separation $a_c(h, \\sigma)$ for a specific setup.\nThe given setup is:\n- Dimension: $d=1$.\n- Number of particles: $n=2$.\n- Particle locations: $x_1 = a$ and $x_2 = -a$, with $a > 0$.\n- Target density: $p(x) = \\mathcal{N}(0, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$.\n\nWe first compute the gradient of the log-target density:\n$$ \\log p(x) = -\\log(\\sqrt{2\\pi}\\sigma) - \\frac{x^2}{2\\sigma^2} $$\n$$ \\nabla \\log p(x) = \\frac{d}{dx} \\log p(x) = -\\frac{x}{\\sigma^2} $$\nAt the particle locations, we have:\n$$ \\nabla \\log p(x_1) = \\nabla \\log p(a) = -\\frac{a}{\\sigma^2} $$\n$$ \\nabla \\log p(x_2) = \\nabla \\log p(-a) = -\\frac{-a}{\\sigma^2} = \\frac{a}{\\sigma^2} $$\n\nNext, we evaluate the SVGD velocity at $x_1 = a$. The velocity expression is:\n$$ \\phi(x_1) = \\frac{1}{2} \\left[ \\left( k(x_1, x_1) \\nabla\\log p(x_1) + \\nabla_{x_1} k(x_1, x_1) \\right) + \\left( k(x_2, x_1) \\nabla\\log p(x_2) + \\nabla_{x_2} k(x_2, x_1) \\right) \\right] $$\nWe compute each term in the sum:\n\nTerm 1 (interaction with self, $j=1$):\n- $k(x_1, x_1) = k(a,a) = \\exp(0) = 1$.\n- $\\nabla\\log p(x_1) = -a/\\sigma^2$.\n- $\\nabla_{x_1} k(x_1, x_1) = -\\frac{x_1-x_1}{h^2} k(x_1,x_1) = 0$.\nSo, the first term in brackets is $(1 \\cdot (-a/\\sigma^2) + 0) = -a/\\sigma^2$.\n\nTerm 2 (interaction with $x_2$, $j=2$):\n- $x_2 - x_1 = -a - a = -2a$.\n- $k(x_2, x_1) = k(-a, a) = \\exp\\left(-\\frac{(-2a)^2}{2h^2}\\right) = \\exp\\left(-\\frac{2a^2}{h^2}\\right)$.\n- $\\nabla\\log p(x_2) = a/\\sigma^2$.\n- $\\nabla_{x_2} k(x_2, x_1) = -\\frac{x_2-x_1}{h^2} k(x_2, x_1) = -\\frac{-2a}{h^2} \\exp\\left(-\\frac{2a^2}{h^2}\\right) = \\frac{2a}{h^2} \\exp\\left(-\\frac{2a^2}{h^2}\\right)$.\nSo, the second term in brackets is:\n$$ \\exp\\left(-\\frac{2a^2}{h^2}\\right) \\cdot \\left(\\frac{a}{\\sigma^2}\\right) + \\frac{2a}{h^2} \\exp\\left(-\\frac{2a^2}{h^2}\\right) = \\exp\\left(-\\frac{2a^2}{h^2}\\right) \\left( \\frac{a}{\\sigma^2} + \\frac{2a}{h^2} \\right) $$\n\nNow, we assemble the full expression for $\\phi(x_1)$:\n$$ \\phi(x_1) = \\frac{1}{2} \\left[ -\\frac{a}{\\sigma^2} + \\exp\\left(-\\frac{2a^2}{h^2}\\right) \\left( \\frac{a}{\\sigma^2} + \\frac{2a}{h^2} \\right) \\right] $$\nFactoring out $a/2$:\n$$ \\phi(x_1) = \\frac{a}{2} \\left[ -\\frac{1}{\\sigma^2} + \\exp\\left(-\\frac{2a^2}{h^2}\\right) \\left( \\frac{1}{\\sigma^2} + \\frac{2}{h^2} \\right) \\right] $$\nThe critical separation $a_c$ is defined as the value of $a$ for which the velocity is zero, i.e., $\\phi(x_1) = 0$. Since we are given $a>0$, we need the term in the square brackets to be zero. Let $a = a_c$:\n$$ -\\frac{1}{\\sigma^2} + \\exp\\left(-\\frac{2a_c^2}{h^2}\\right) \\left( \\frac{1}{\\sigma^2} + \\frac{2}{h^2} \\right) = 0 $$\n$$ \\exp\\left(-\\frac{2a_c^2}{h^2}\\right) \\left( \\frac{1}{\\sigma^2} + \\frac{2}{h^2} \\right) = \\frac{1}{\\sigma^2} $$\n$$ \\exp\\left(-\\frac{2a_c^2}{h^2}\\right) = \\frac{1/\\sigma^2}{1/\\sigma^2 + 2/h^2} $$\nTo simplify the right-hand side, we find a common denominator:\n$$ \\frac{1/\\sigma^2}{(h^2 + 2\\sigma^2)/(\\sigma^2 h^2)} = \\frac{1}{\\sigma^2} \\cdot \\frac{\\sigma^2 h^2}{h^2 + 2\\sigma^2} = \\frac{h^2}{h^2 + 2\\sigma^2} $$\nThe equation for $a_c$ is:\n$$ \\exp\\left(-\\frac{2a_c^2}{h^2}\\right) = \\frac{h^2}{h^2 + 2\\sigma^2} $$\nTo solve for $a_c$, we take the natural logarithm of both sides:\n$$ -\\frac{2a_c^2}{h^2} = \\log\\left(\\frac{h^2}{h^2 + 2\\sigma^2}\\right) $$\nUsing the property $\\log(x/y) = -\\log(y/x)$:\n$$ \\frac{2a_c^2}{h^2} = \\log\\left(\\frac{h^2 + 2\\sigma^2}{h^2}\\right) = \\log\\left(1 + \\frac{2\\sigma^2}{h^2}\\right) $$\nNow, we solve for $a_c^2$:\n$$ a_c^2 = \\frac{h^2}{2} \\log\\left(1 + \\frac{2\\sigma^2}{h^2}\\right) $$\nSince $a > 0$, we take the positive square root:\n$$ a_c(h, \\sigma) = \\sqrt{\\frac{h^2}{2} \\log\\left(1 + \\frac{2\\sigma^2}{h^2}\\right)} $$\nThis can be written as:\n$$ a_c(h, \\sigma) = h \\sqrt{\\frac{1}{2} \\log\\left(1 + \\frac{2\\sigma^2}{h^2}\\right)} $$\nThis is the unique critical separation for which the SVGD velocity at $x_1=a$ is zero. For $a < a_c$, the term $\\exp(-2a^2/h^2)$ is larger, making the velocity positive (outward repulsion dominates). For $a > a_c$, the velocity is negative (inward attraction to the mode at $x=0$ dominates).", "answer": "$$ \\boxed{h \\sqrt{\\frac{1}{2} \\log\\left(1 + \\frac{2\\sigma^2}{h^2}\\right)}} $$", "id": "3348300"}, {"introduction": "Moving from analysis to synthesis, this final practice challenges you to build a complete one-dimensional SVGD sampler from first principles. This exercise goes beyond the core update rule to address crucial practical considerations, such as kernel bandwidth selection using the median heuristic, defining a step size schedule, and implementing robust convergence criteria including the Kernelized Stein Discrepancy (KSD) [@problem_id:3348306]. Successfully implementing and testing this sampler will solidify your understanding and prepare you to apply SVGD to real-world problems.", "problem": "Construct and analyze a one-dimensional Stein Variational Gradient Descent (SVGD) procedure for a log-concave target distribution within the framework of stochastic simulation and Monte Carlo methods. Use the following scientifically grounded bases: the definition of Kullback–Leibler divergence (KL), Stein’s identity with the Langevin–Stein operator, and reproducing kernel Hilbert space (RKHS)-based variational transport. Develop a complete, runnable program that performs SVGD on a concrete family of log-concave targets, implements an explicit kernel choice, a principled step size schedule, and clear convergence criteria, and then applies these to a specified test suite of cases.\n\nYour task consists of the following parts:\n\n1. Start from the definition of Kullback–Leibler divergence and Stein’s identity, and derive the SVGD transport update for a one-dimensional target density using a radial basis function (RBF) kernel. Use the following foundations without providing shortcut formulas:\n   - The Kullback–Leibler divergence between a proposal $q$ and target $p$ is $\\mathrm{KL}(q \\,\\|\\, p) = \\int q(x)\\log\\frac{q(x)}{p(x)}\\,dx$.\n   - The score function is $\\nabla_x \\log p(x)$.\n   - Stein’s identity for the Langevin–Stein operator states that for smooth $f$ with suitable boundary behavior, $\\mathbb{E}_{x\\sim p}\\left[\\nabla_x \\log p(x)\\,f(x) + \\nabla_x f(x)\\right] = 0$.\n   - For a smooth, positive definite kernel $k(x,y)$ with associated Reproducing Kernel Hilbert Space (RKHS), the steepest descent direction in RKHS for decreasing $\\mathrm{KL}(q \\,\\|\\, p)$ leads to a transport map update that can be approximated empirically from particles.\n\n2. Consider the one-dimensional log-concave target family given by the Gaussian distribution $p(x) = \\mathcal{N}(\\mu,\\sigma^2)$, which is log-concave for any real $\\mu$ and positive $\\sigma$. The score function is $\\nabla_x \\log p(x) = -\\frac{x-\\mu}{\\sigma^2}$.\n\n3. Use the radial basis function kernel $k(x,y) = \\exp\\!\\left(-\\frac{(x-y)^2}{2h^2}\\right)$ with bandwidth $h>0$. Compute $h$ via the median heuristic from the current particle positions, scaled by a user-specified positive factor $\\alpha$. When the median is not well-defined (e.g., a single particle) or numerically zero, default to $h=\\alpha\\cdot 1.0$.\n\n4. Implement SVGD with the empirical transport field evaluated at each particle position $x_i$ as an average over all particles $\\{x_j\\}$:\n   - Use the RKHS steepest descent direction specialized to one dimension with the aforementioned kernel, where the kernel gradient term is taken with respect to the first kernel argument.\n   - Employ a step size schedule $\\varepsilon_t = \\varepsilon_0/(1+d\\cdot t)$, where $\\varepsilon_0>0$ and $d\\ge 0$ are specified constants and $t$ is the iteration index.\n\n5. Define the convergence criteria precisely and check them during the run:\n   - Maximum absolute per-iteration particle displacement $\\Delta_{\\max}$ must be below a threshold $\\tau_{\\mathrm{move}}$.\n   - The absolute error in the empirical mean $|\\bar{x}-\\mu|$ must be below a threshold $\\tau_{\\mathrm{mean}}$.\n   - The absolute error in the empirical variance $|\\widehat{\\mathrm{Var}}(x)-\\sigma^2|$ must be below a threshold $\\tau_{\\mathrm{var}}$.\n   - The Kernelized Stein Discrepancy (KSD) with the same kernel must be below a threshold $\\tau_{\\mathrm{ksd}}$. In one dimension with the RBF kernel, use\n     $$\\mathrm{KSD}^2(q,p) = \\mathbb{E}_{x,y\\sim q}\\left[u_p(x,y)\\right],$$\n     where\n     $$u_p(x,y) = s(x)\\,k(x,y)\\,s(y) + s(x)\\,\\nabla_x k(x,y) + s(y)\\,\\nabla_y k(x,y) + \\nabla_x\\nabla_y k(x,y),$$\n     $$s(x) = \\nabla_x \\log p(x),\\quad k(x,y) = \\exp\\!\\left(-\\frac{(x-y)^2}{2h^2}\\right),$$\n     $$\\nabla_x k(x,y) = \\frac{y-x}{h^2}k(x,y),\\quad \\nabla_y k(x,y) = \\frac{x-y}{h^2}k(x,y),$$\n     $$\\nabla_x\\nabla_y k(x,y) = \\left(\\frac{1}{h^2} - \\frac{(x-y)^2}{h^4}\\right)k(x,y).$$\n     Approximate the expectation by the empirical average over the particle set. To reduce computational overhead, you may evaluate the KSD every $m$ iterations (with $m$ specified), and treat the last evaluated KSD as the running value for the convergence check.\n\n6. Initialize particles for each test case using a specified scheme and ensure determinism by fixing a random seed. For uniform initializations on an interval $[a,b]$, draw independent samples uniformly in $[a,b]$. For normal initializations $\\mathcal{N}(m,s^2)$, draw independent samples from that normal distribution.\n\n7. Test Suite. Apply your SVGD implementation to the following cases. For each case, run until convergence or until a maximum number of iterations is reached. For each case, output a boolean indicating whether convergence occurred within the iteration limit. Use the parameters:\n   - Case $1$ (happy path):\n     - Target: $\\mu=1.0$, $\\sigma=1.2$.\n     - Particles: $n=50$.\n     - Initialization: uniform on $[-4.0,4.0]$.\n     - Kernel scaling: $\\alpha=1.0$.\n     - Step size: $\\varepsilon_0=0.3$, $d=0.01$.\n     - Convergence thresholds: $\\tau_{\\mathrm{move}}=1.0\\times 10^{-4}$, $\\tau_{\\mathrm{mean}}=0.05$, $\\tau_{\\mathrm{var}}=0.10$, $\\tau_{\\mathrm{ksd}}=0.02$.\n     - KSD check period: $m=10$.\n     - Maximum iterations: $800$.\n   - Case $2$ (few particles boundary):\n     - Target: $\\mu=0.0$, $\\sigma=1.0$.\n     - Particles: $n=3$.\n     - Initialization: normal $\\mathcal{N}(3.0,0.5^2)$.\n     - Kernel scaling: $\\alpha=1.0$.\n     - Step size: $\\varepsilon_0=0.4$, $d=0.02$.\n     - Convergence thresholds: $\\tau_{\\mathrm{move}}=1.0\\times 10^{-4}$, $\\tau_{\\mathrm{mean}}=0.05$, $\\tau_{\\mathrm{var}}=0.10$, $\\tau_{\\mathrm{ksd}}=0.05$.\n     - KSD check period: $m=10$.\n     - Maximum iterations: $900$.\n   - Case $3$ (small bandwidth stress):\n     - Target: $\\mu=0.5$, $\\sigma=0.7$.\n     - Particles: $n=40$.\n     - Initialization: uniform on $[-10.0,10.0]$.\n     - Kernel scaling: $\\alpha=0.2$.\n     - Step size: $\\varepsilon_0=0.15$, $d=0.015$.\n     - Convergence thresholds: $\\tau_{\\mathrm{move}}=1.0\\times 10^{-4}$, $\\tau_{\\mathrm{mean}}=0.05$, $\\tau_{\\mathrm{var}}=0.10$, $\\tau_{\\mathrm{ksd}}=0.03$.\n     - KSD check period: $m=10$.\n     - Maximum iterations: $1000$.\n   - Case $4$ (broad target, larger kernel scaling):\n     - Target: $\\mu=-2.0$, $\\sigma=3.0$.\n     - Particles: $n=60$.\n     - Initialization: normal $\\mathcal{N}(0.0,5.0^2)$.\n     - Kernel scaling: $\\alpha=1.5$.\n     - Step size: $\\varepsilon_0=0.25$, $d=0.008$.\n     - Convergence thresholds: $\\tau_{\\mathrm{move}}=1.0\\times 10^{-4}$, $\\tau_{\\mathrm{mean}}=0.05$, $\\tau_{\\mathrm{var}}=0.20$, $\\tau_{\\mathrm{ksd}}=0.04$.\n     - KSD check period: $m=10$.\n     - Maximum iterations: $1200$.\n\n8. Program Output Specification. Your program should produce a single line of output containing the convergence results for the above four test cases as a comma-separated list of booleans enclosed in square brackets (e.g., \"[True,False,True,True]\"). No other text should be printed.\n\nNo physical units or angles are involved.", "solution": "The objective is to construct a one-dimensional Stein Variational Gradient Descent (SVGD) method grounded in variational inference and Stein’s identity, then implement and test it on a series of log-concave targets. The procedure rests on the following well-tested foundations: the Kullback–Leibler divergence $\\mathrm{KL}(q \\,\\|\\, p)$, Stein’s identity, and the use of reproducing kernel Hilbert spaces (RKHS) to define a function class within which a steepest descent direction can be derived.\n\n1. Fundamental base and SVGD principle. The Kullback–Leibler divergence between a proposal $q$ and a target $p$ is $\\mathrm{KL}(q \\,\\|\\, p) = \\int q(x)\\log\\frac{q(x)}{p(x)}\\,dx$. The steepest descent for $\\mathrm{KL}(q \\,\\|\\, p)$ with respect to a transport of particles $x\\mapsto x+\\varepsilon \\phi(x)$ is obtained by taking the first variation of the KL divergence under infinitesimal pushforward by $\\phi$. Using Stein’s identity for the Langevin–Stein operator, $\\mathbb{E}_{x\\sim p}\\left[\\nabla_x \\log p(x)\\,f(x) + \\nabla_x f(x)\\right] = 0$, one restricts $\\phi$ to an RKHS induced by a positive definite kernel $k(\\cdot,\\cdot)$ and obtains, by the representer theorem, a steepest descent direction in that RKHS. The resulting SVGD update field $\\phi^\\star(\\cdot)$ is (in expectation under $q$):\n   $$\\phi^\\star(y) = \\mathbb{E}_{x\\sim q}\\left[k(x,y)\\,\\nabla_x\\log p(x) + \\nabla_x k(x,y)\\right].$$\n   In practice, with particles $\\{x_j\\}_{j=1}^n$ from $q$, one uses the empirical approximation\n   $$\\phi^\\star(y) \\approx \\frac{1}{n}\\sum_{j=1}^n \\left[k(x_j,y)\\,\\nabla_x\\log p(x_j) + \\nabla_x k(x_j,y)\\right].$$\n   The particle update for each $x_i$ at iteration $t$ with step size $\\varepsilon_t$ is\n   $$x_i \\leftarrow x_i + \\varepsilon_t\\,\\phi^\\star(x_i).$$\n\n2. Specialization to one-dimensional Gaussian targets. Let the target be $p(x) = \\mathcal{N}(\\mu,\\sigma^2)$, which is log-concave. The score function is\n   $$s(x) = \\nabla_x \\log p(x) = -\\frac{x-\\mu}{\\sigma^2}.$$\n\n3. Kernel choice and derivatives. Use the radial basis function (RBF) kernel\n   $$k(x,y) = \\exp\\!\\left(-\\frac{(x-y)^2}{2h^2}\\right),$$\n   with bandwidth $h>0$. In one dimension, the required derivatives are\n   $$\\nabla_x k(x,y) = \\frac{y-x}{h^2}\\,k(x,y),\\qquad \\nabla_y k(x,y) = \\frac{x-y}{h^2}\\,k(x,y),$$\n   $$\\nabla_x\\nabla_y k(x,y) = \\left(\\frac{1}{h^2} - \\frac{(x-y)^2}{h^4}\\right)\\,k(x,y).$$\n   The bandwidth $h$ is computed by the median heuristic from current particle positions: take the median of the set of squared pairwise distances (excluding the diagonal), then set $h = \\alpha\\,\\sqrt{\\mathrm{median}}$. If the median is undefined or numerically zero, default to $h=\\alpha\\cdot 1.0$ to maintain stability.\n\n4. Transport field assembly and step size schedule. For particles $\\{x_j\\}_{j=1}^n$, compute\n   $$\\phi^\\star(x_i) \\approx \\frac{1}{n}\\sum_{j=1}^n \\left[k(x_j,x_i)\\,s(x_j) + \\nabla_x k(x_j,x_i)\\right],$$\n   which in vectorized form uses the pairwise kernel $k(x_j,x_i)$ matrix and the derivative with respect to the first kernel argument. The step size schedule is chosen as a decreasing function of iteration:\n   $$\\varepsilon_t = \\frac{\\varepsilon_0}{1 + d\\,t},$$\n   with $\\varepsilon_0>0$ and $d\\ge 0$ specified. This choice is a common diminishing step size ensuring stability while allowing for initial rapid movement.\n\n5. Convergence criteria. The run terminates successfully if all of the following are simultaneously achieved:\n   - Maximum change per iteration $\\Delta_{\\max} = \\max_i |x_i^{(t+1)} - x_i^{(t)}|$ is below $\\tau_{\\mathrm{move}}$.\n   - Mean error $|\\bar{x}-\\mu|$ is below $\\tau_{\\mathrm{mean}}$, where $\\bar{x}$ is the empirical mean.\n   - Variance error $|\\widehat{\\mathrm{Var}}(x)-\\sigma^2|$ is below $\\tau_{\\mathrm{var}}$, using the population variance with divisor $n$.\n   - Kernelized Stein Discrepancy (KSD) with the same kernel is below $\\tau_{\\mathrm{ksd}}$. The KSD squared is computed via the V-statistic:\n     $$\\mathrm{KSD}^2(q,p) \\approx \\frac{1}{n^2}\\sum_{j=1}^n\\sum_{i=1}^n u_p(x_j,x_i),$$\n     with\n     $$u_p(x,y) = s(x)\\,k(x,y)\\,s(y) + s(x)\\,\\nabla_x k(x,y) + s(y)\\,\\nabla_y k(x,y) + \\nabla_x\\nabla_y k(x,y).$$\n   To manage computational cost, compute the KSD every $m$ iterations (with $m$ specified per case) and use the latest value for convergence checking. If the maximum number of iterations is reached without meeting all criteria, the run is deemed non-convergent.\n\n6. Initialization and determinism. For uniform initializations on $[a,b]$, sample $n$ independent points from $\\mathrm{Uniform}(a,b)$. For normal initializations $\\mathcal{N}(m,s^2)$, sample $n$ independent points from that normal. Fix the random seed once to ensure determinism across runs.\n\n7. Test suite and parameters. The four test cases probe different facets: a typical case with moderate particle count, a boundary case with very few particles, a sensitivity case with small bandwidth scaling, and a broad-target case. The parameters (targets, particle counts, initializations, kernel scaling $\\alpha$, step sizes $\\varepsilon_0$ and $d$, thresholds $\\tau_{\\mathrm{move}}$, $\\tau_{\\mathrm{mean}}$, $\\tau_{\\mathrm{var}}$, $\\tau_{\\mathrm{ksd}}$, KSD check period $m$, and maximum iterations) are specified explicitly in the problem statement. These values are scientifically plausible and numerically stable for a one-dimensional SVGD.\n\n8. Output specification. For each case, produce a boolean indicating whether convergence occurred under the stated criteria within the iteration limit. Aggregate the results into a single line formatted as a comma-separated list enclosed in square brackets, e.g., \"[True,False,True,True]\". No additional output is permitted.\n\nAlgorithmic integration:\n- Compute the score $s(x)$ at current particles using $s(x) = -\\frac{x-\\mu}{\\sigma^2}$.\n- Compute the bandwidth $h$ via the median heuristic with scaling factor $\\alpha$.\n- Assemble the kernel matrix $K$ and the derivative matrix for the repulsion term $\\nabla_x k(x_j,x_i) = \\frac{x_i-x_j}{h^2}K_{j,i}$.\n- Compute the transport field $\\phi^\\star(x_i)$ by averaging across all particles.\n- Update the particles using the diminishing step size schedule.\n- Monitor $\\Delta_{\\max}$, mean and variance errors, and KSD (periodically) for convergence.\n- Repeat until convergence or reaching the maximum number of iterations.\n\nThis design directly implements SVGD from first principles: the transport field arises from the functional gradient of KL divergence projected into an RKHS via Stein’s identity, the kernel and its derivatives are explicit, and the convergence criteria involve both moment matching and the kernelized Stein discrepancy, which quantitatively measures closeness to the target in a way tailored to the chosen kernel. The four test cases collectively examine typical performance, particle scarcity effects, kernel bandwidth sensitivity, and behavior on broad targets.", "answer": "```python\nimport numpy as np\n\ndef median_bandwidth(x, alpha):\n    n = x.shape[0]\n    if n < 2:\n        return alpha * 1.0\n    diffs = x[:, None] - x[None, :]\n    d2 = (diffs ** 2)\n    # Exclude diagonal\n    mask = ~np.eye(n, dtype=bool)\n    vals = d2[mask]\n    # Use positive entries for robustness; fallback if empty or zero median\n    pos_vals = vals[vals > 0]\n    if pos_vals.size == 0:\n        return alpha * 1.0\n    med = np.median(pos_vals)\n    if med <= 0 or not np.isfinite(med):\n        return alpha * 1.0\n    h = alpha * np.sqrt(med)\n    # Ensure h is not too small\n    if h < 1e-6:\n        h = alpha * 1.0\n    return h\n\ndef svgd_1d_gaussian(mu, sigma, n_particles, init_type, init_params,\n                     alpha, eps0, decay, max_iter,\n                     tol_move, tol_mean, tol_var, tol_ksd,\n                     ksd_every=10, rng=np.random.default_rng(0)):\n    # Initialize particles\n    if init_type == \"uniform\":\n        a, b = init_params\n        x = rng.uniform(low=a, high=b, size=n_particles)\n    elif init_type == \"normal\":\n        m, s = init_params\n        x = rng.normal(loc=m, scale=s, size=n_particles)\n    else:\n        raise ValueError(\"Unknown init_type\")\n\n    def score(x_arr):\n        return -(x_arr - mu) / (sigma ** 2)\n\n    def compute_ksd(x_arr):\n        n = x_arr.shape[0]\n        if n == 0:\n            return np.inf\n        h = median_bandwidth(x_arr, alpha)\n        Xj = x_arr[:, None]\n        Xi = x_arr[None, :]\n        D = Xj - Xi  # D[j,i] = x_j - x_i\n        K = np.exp(- (D ** 2) / (2.0 * (h ** 2)))\n        s = score(x_arr)\n        # Terms:\n        # T1 = s_j * K * s_i\n        T1 = (s[:, None] * s[None, :]) * K\n        # dK/dx (first arg) = (y - x)/h^2 * K = (Xi - Xj)/h^2 * K = -D/h^2 * K\n        dK_dx = (-D / (h ** 2)) * K\n        # dK/dy (second arg) = (x - y)/h^2 * K = D/h^2 * K\n        dK_dy = (D / (h ** 2)) * K\n        T2 = (s[:, None]) * dK_dx\n        T3 = (s[None, :]) * dK_dy\n        # Mixed derivative: (1/h^2 - (x-y)^2/h^4) * K = (1/h^2 - D^2/h^4) * K\n        mixed = ((1.0 / (h ** 2)) - (D ** 2) / (h ** 4)) * K\n        U = T1 + T2 + T3 + mixed\n        ksd2 = np.sum(U) / (n ** 2)\n        # Numerical guard\n        if ksd2 < 0:\n            ksd2 = max(ksd2, 0.0)\n        return float(np.sqrt(ksd2))\n\n    converged = False\n    last_ksd = np.inf\n    for t in range(max_iter):\n        h = median_bandwidth(x, alpha)\n        # Pairwise structures\n        Xj = x[:, None]\n        Xi = x[None, :]\n        D = Xj - Xi  # shape (n,n), D[j,i] = x_j - x_i\n        K = np.exp(- (D ** 2) / (2.0 * (h ** 2)))\n        s = score(x)\n        # ∇_x k(x_j, x_i) wrt first argument: (x_i - x_j)/h^2 * K = -D/h^2 * K\n        grad_term = (-D / (h ** 2)) * K\n        # Transport field at each x_i: average over j\n        phi = (K.T @ s + np.sum(grad_term, axis=0)) / n_particles\n        eps_t = eps0 / (1.0 + decay * t)\n        delta = eps_t * phi\n        x_new = x + delta\n        delta_max = float(np.max(np.abs(delta)))\n        # Moment errors\n        mean_err = float(abs(np.mean(x_new) - mu))\n        var_err = float(abs(np.var(x_new) - (sigma ** 2)))\n        # Periodic KSD\n        if (t % ksd_every) == 0:\n            last_ksd = compute_ksd(x_new)\n        # Check convergence\n        if (delta_max < tol_move) and (mean_err < tol_mean) and (var_err < tol_var) and (last_ksd < tol_ksd):\n            converged = True\n            break\n        x = x_new\n\n    return converged\n\ndef solve():\n    rng = np.random.default_rng(0)\n    test_cases = [\n        {\n            \"mu\": 1.0, \"sigma\": 1.2, \"n_particles\": 50,\n            \"init_type\": \"uniform\", \"init_params\": (-4.0, 4.0),\n            \"alpha\": 1.0, \"eps0\": 0.3, \"decay\": 0.01,\n            \"max_iter\": 800, \"tol_move\": 1.0e-4,\n            \"tol_mean\": 0.05, \"tol_var\": 0.10, \"tol_ksd\": 0.02,\n            \"ksd_every\": 10\n        },\n        {\n            \"mu\": 0.0, \"sigma\": 1.0, \"n_particles\": 3,\n            \"init_type\": \"normal\", \"init_params\": (3.0, 0.5),\n            \"alpha\": 1.0, \"eps0\": 0.4, \"decay\": 0.02,\n            \"max_iter\": 900, \"tol_move\": 1.0e-4,\n            \"tol_mean\": 0.05, \"tol_var\": 0.10, \"tol_ksd\": 0.05,\n            \"ksd_every\": 10\n        },\n        {\n            \"mu\": 0.5, \"sigma\": 0.7, \"n_particles\": 40,\n            \"init_type\": \"uniform\", \"init_params\": (-10.0, 10.0),\n            \"alpha\": 0.2, \"eps0\": 0.15, \"decay\": 0.015,\n            \"max_iter\": 1000, \"tol_move\": 1.0e-4,\n            \"tol_mean\": 0.05, \"tol_var\": 0.10, \"tol_ksd\": 0.03,\n            \"ksd_every\": 10\n        },\n        {\n            \"mu\": -2.0, \"sigma\": 3.0, \"n_particles\": 60,\n            \"init_type\": \"normal\", \"init_params\": (0.0, 5.0),\n            \"alpha\": 1.5, \"eps0\": 0.25, \"decay\": 0.008,\n            \"max_iter\": 1200, \"tol_move\": 1.0e-4,\n            \"tol_mean\": 0.05, \"tol_var\": 0.20, \"tol_ksd\": 0.04,\n            \"ksd_every\": 10\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        res = svgd_1d_gaussian(\n            mu=case[\"mu\"], sigma=case[\"sigma\"], n_particles=case[\"n_particles\"],\n            init_type=case[\"init_type\"], init_params=case[\"init_params\"],\n            alpha=case[\"alpha\"], eps0=case[\"eps0\"], decay=case[\"decay\"],\n            max_iter=case[\"max_iter\"], tol_move=case[\"tol_move\"],\n            tol_mean=case[\"tol_mean\"], tol_var=case[\"tol_var\"], tol_ksd=case[\"tol_ksd\"],\n            ksd_every=case[\"ksd_every\"], rng=rng\n        )\n        results.append(res)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3348306"}]}