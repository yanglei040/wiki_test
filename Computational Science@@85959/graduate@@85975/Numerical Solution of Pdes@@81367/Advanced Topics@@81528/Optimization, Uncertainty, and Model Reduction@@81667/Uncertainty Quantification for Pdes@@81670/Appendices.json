{"hands_on_practices": [{"introduction": "A cornerstone of uncertainty quantification is understanding how sensitive a model's output is to its uncertain inputs. The adjoint method provides an exceptionally elegant and computationally efficient technique for calculating these sensitivities, or gradients, for a given quantity of interest (QoI). This practice [@problem_id:3459206] guides you through the foundational derivation of the continuous adjoint problem, a powerful tool that sidesteps the high cost of traditional finite-difference sensitivity analysis.", "problem": "Consider a bounded Lipschitz domain $D \\subset \\mathbb{R}^{d}$ with $d \\in \\{2,3\\}$ and outward unit normal $\\boldsymbol{n}$ on its boundary $\\partial D$. Let $a : D \\to \\mathbb{R}$ be a strictly positive coefficient satisfying $a \\in L^{\\infty}(D)$ and $a(x) \\ge a_{\\min}  0$ almost everywhere in $D$. For a given source $f \\in L^{2}(D)$, the state $u \\in H_{0}^{1}(D)$ is defined as the unique weak solution of the linear elliptic Partial Differential Equation (PDE)\n$$\n-\\nabla \\cdot \\big(a(x)\\,\\nabla u(x)\\big) = f(x) \\quad \\text{in } D, \n\\qquad \nu = 0 \\quad \\text{on } \\partial D,\n$$\nin the sense that the weak form\n$$\n\\int_{D} a(x)\\,\\nabla u(x)\\cdot \\nabla v(x)\\,dx = \\int_{D} f(x)\\,v(x)\\,dx\n$$\nholds for all $v \\in H_{0}^{1}(D)$. Define the quantity of interest (QoI) $Q : H_{0}^{1}(D) \\to \\mathbb{R}$ by\n$$\nQ(u) := \\int_{D} \\psi(x)\\,u(x)\\,dx,\n$$\nwith a fixed weight $\\psi \\in L^{2}(D)$. In the context of Uncertainty Quantification (UQ), treat $a$ as the uncertain input field and consider perturbations $\\delta a \\in L^{\\infty}(D)$ such that $a+\\delta a$ remains strictly positive.\n\nStarting from the weak formulation, use the continuous adjoint method to:\n1) Derive the adjoint problem for the adjoint field $p \\in H_{0}^{1}(D)$ associated with the QoI $Q(u)$, including appropriate boundary conditions justified by the functional setting.\n2) Derive the Fréchet derivative of $Q$ with respect to $a$ in the direction $\\delta a$, denoted $\\delta Q[a;\\delta a]$, and express it entirely in terms of inner products of the primal gradient $\\nabla u$ and the adjoint gradient $\\nabla p$. Then, identify the pointwise Fréchet derivative density $\\delta Q/\\delta a(x)$ so that\n$$\n\\delta Q[a;\\delta a] = \\int_{D} \\frac{\\delta Q}{\\delta a}(x)\\,\\delta a(x)\\,dx.\n$$\n\nYour final answer must be a single closed-form analytic expression for the pointwise Fréchet derivative density $\\delta Q/\\delta a(x)$. No numerical evaluation is required. All integrals must be expressed in base International System of Units (SI) and all angles, if any appear, in radians. No rounding is needed.", "solution": "The problem is first validated to ensure it is well-posed and scientifically sound.\n\n### Step 1: Extract Givens\n-   **Domain**: A bounded Lipschitz domain $D \\subset \\mathbb{R}^{d}$ with $d \\in \\{2,3\\}$.\n-   **Coefficient**: A function $a : D \\to \\mathbb{R}$ such that $a \\in L^{\\infty}(D)$ and there exists a constant $a_{\\min}  0$ with $a(x) \\ge a_{\\min}$ almost everywhere in $D$.\n-   **Source**: A function $f \\in L^{2}(D)$.\n-   **State Variable**: The state $u \\in H_{0}^{1}(D)$ is the unique weak solution to a linear elliptic PDE.\n-   **PDE (Strong Form)**: $-\\nabla \\cdot \\big(a(x)\\,\\nabla u(x)\\big) = f(x)$ in $D$, with boundary condition $u = 0$ on $\\partial D$.\n-   **PDE (Weak Form)**: For all test functions $v \\in H_{0}^{1}(D)$, the following holds:\n    $$ \\int_{D} a(x)\\,\\nabla u(x)\\cdot \\nabla v(x)\\,dx = \\int_{D} f(x)\\,v(x)\\,dx $$\n-   **Quantity of Interest (QoI)**: A functional $Q : H_{0}^{1}(D) \\to \\mathbb{R}$ defined by:\n    $$ Q(u) := \\int_{D} \\psi(x)\\,u(x)\\,dx $$\n    where $\\psi \\in L^{2}(D)$ is a fixed weight function.\n-   **Uncertainty Model**: The coefficient $a$ is treated as an uncertain input field. We consider perturbations $\\delta a \\in L^{\\infty}(D)$ such that $a+\\delta a$ remains strictly positive.\n-   **Objective**:\n    1.  Derive the adjoint problem for the adjoint field $p \\in H_{0}^{1}(D)$.\n    2.  Derive the Fréchet derivative of $Q$ with respect to $a$ in the direction $\\delta a$, denoted $\\delta Q[a;\\delta a]$, and identify the pointwise Fréchet derivative density $\\frac{\\delta Q}{\\delta a}(x)$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is a standard application of the continuous adjoint method for sensitivity analysis of a solution to a linear elliptic PDE. This is a fundamental and well-established topic in applied mathematics, numerical analysis, and engineering. The PDE is the variable-coefficient Poisson equation, a canonical model for diffusion processes. The use of Sobolev spaces ($H_{0}^{1}(D)$) and Lebesgue spaces ($L^{2}(D)$, $L^{\\infty}(D)$) is standard for the modern theory of PDEs.\n-   **Well-Posed**: The existence and uniqueness of the a weak solution $u \\in H_{0}^{1}(D)$ for the primal problem is guaranteed by the Lax-Milgram theorem. The bilinear form $B(u,v) = \\int_{D} a \\nabla u \\cdot \\nabla v \\,dx$ is continuous and coercive on $H_{0}^{1}(D) \\times H_{0}^{1}(D)$, and the linear functional $L(v) = \\int_{D} f v \\,dx$ is continuous on $H_{0}^{1}(D)$. The problem of finding the Fréchet derivative is also well-posed.\n-   **Objective**: The problem is stated using precise mathematical language, leaving no room for ambiguity.\n\nThe problem contains no scientific fallacies, inconsistencies, or ill-posed elements. It is a valid and standard problem in the field of PDE-constrained optimization and uncertainty quantification.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution is provided below.\n\n### Solution Derivation\n\nThe solution process involves three mains steps: linearizing the governing PDE to obtain the sensitivity equation, defining the adjoint problem, and combining these to derive the Fréchet derivative of the QoI.\n\nLet the state $u$ be implicitly dependent on the coefficient $a$, denoted as $u(a)$. A perturbation $\\delta a$ of the coefficient $a$ induces a perturbation in the state, which to first order is given by the Gâteaux derivative of $u$ with respect to $a$ in the direction $\\delta a$, denoted $\\delta u$. The perturbed state is $u(a+\\delta a) \\approx u(a) + \\delta u$.\n\nThe state $u = u(a)$ satisfies the weak form:\n$$ \\int_{D} a(x)\\,\\nabla u(x)\\cdot \\nabla v(x)\\,dx = \\int_{D} f(x)\\,v(x)\\,dx \\quad \\forall v \\in H_{0}^{1}(D) \\quad (1) $$\nThe perturbed state $u+\\delta u$ corresponding to the perturbed coefficient $a+\\delta a$ satisfies:\n$$ \\int_{D} (a(x)+\\delta a(x))\\,\\nabla (u(x)+\\delta u(x))\\cdot \\nabla v(x)\\,dx = \\int_{D} f(x)\\,v(x)\\,dx \\quad \\forall v \\in H_{0}^{1}(D) \\quad (2) $$\nExpanding $(2)$:\n$$ \\int_{D} a\\nabla u \\cdot \\nabla v \\,dx + \\int_{D} a\\nabla \\delta u \\cdot \\nabla v \\,dx + \\int_{D} \\delta a\\nabla u \\cdot \\nabla v \\,dx + \\int_{D} \\delta a\\nabla \\delta u \\cdot \\nabla v \\,dx = \\int_{D} f v \\,dx $$\nSubtracting equation $(1)$ from this expansion and neglecting the second-order term $\\int_{D} \\delta a\\nabla \\delta u \\cdot \\nabla v \\,dx$ (which is $O(\\|\\delta a\\|\\|\\delta u\\|)$), we obtain the linearized sensitivity equation for $\\delta u \\in H_{0}^{1}(D)$:\n$$ \\int_{D} a(x)\\,\\nabla \\delta u(x)\\cdot \\nabla v(x)\\,dx = - \\int_{D} \\delta a(x)\\,\\nabla u(x)\\cdot \\nabla v(x)\\,dx \\quad \\forall v \\in H_{0}^{1}(D) \\quad (3) $$\n\nNext, we consider the first-order variation of the QoI, $\\delta Q$.\n$$ Q(u+\\delta u) = \\int_{D} \\psi(x)\\,(u(x)+\\delta u(x))\\,dx = \\int_{D} \\psi u\\,dx + \\int_{D} \\psi \\delta u\\,dx = Q(u) + \\delta Q $$\nThe Fréchet derivative of $Q$ with respect to $u$ in the direction $\\delta u$ is thus:\n$$ \\delta Q = \\int_{D} \\psi(x)\\,\\delta u(x)\\,dx \\quad (4) $$\nOur goal is to express $\\delta Q$ in terms of $\\delta a$ without needing to explicitly solve equation $(3)$ for $\\delta u$. This is achieved by introducing an adjoint problem.\n\n**1) The Adjoint Problem**\n\nWe define the adjoint state $p \\in H_{0}^{1}(D)$ as the solution to the following weak problem:\nFind $p \\in H_{0}^{1}(D)$ such that for all test functions $w \\in H_{0}^{1}(D)$:\n$$ \\int_{D} a(x)\\,\\nabla w(x)\\cdot \\nabla p(x)\\,dx = \\int_{D} \\psi(x)\\,w(x)\\,dx $$\nDue to the symmetry of the bilinear form, i.e., $\\int_D a \\nabla w \\cdot \\nabla p \\,dx = \\int_D a \\nabla p \\cdot \\nabla w \\,dx$, this is equivalent to the standard weak form:\n$$ \\int_{D} a(x)\\,\\nabla p(x)\\cdot \\nabla w(x)\\,dx = \\int_{D} \\psi(x)\\,w(x)\\,dx \\quad \\forall w \\in H_{0}^{1}(D) \\quad (5) $$\nThis weak form corresponds to the strong-form adjoint PDE:\n$$ -\\nabla \\cdot \\big(a(x)\\,\\nabla p(x)\\big) = \\psi(x) \\quad \\text{in } D $$\nThe requirement that the solution $p$ and test functions $w$ belong to $H_{0}^{1}(D)$ implies the homogeneous Dirichlet boundary condition:\n$$ p = 0 \\quad \\text{on } \\partial D $$\nThis completes the derivation of the adjoint problem.\n\n**2) The Fréchet Derivative Density**\n\nNow we connect the primal, sensitivity, and adjoint equations.\nIn the adjoint weak form $(5)$, we choose the test function to be the state sensitivity, $w = \\delta u \\in H_{0}^{1}(D)$. This gives:\n$$ \\int_{D} a(x)\\,\\nabla p(x)\\cdot \\nabla \\delta u(x)\\,dx = \\int_{D} \\psi(x)\\,\\delta u(x)\\,dx $$\nThe right-hand side is exactly the expression for $\\delta Q$ from equation $(4)$. Using the symmetry of the dot product, we have:\n$$ \\delta Q = \\int_{D} a(x)\\,\\nabla \\delta u(x)\\cdot \\nabla p(x)\\,dx $$\nNow, consider the sensitivity equation $(3)$. We can choose the test function to be the adjoint state, $v = p \\in H_{0}^{1}(D)$, which yields:\n$$ \\int_{D} a(x)\\,\\nabla \\delta u(x)\\cdot \\nabla p(x)\\,dx = - \\int_{D} \\delta a(x)\\,\\nabla u(x)\\cdot \\nabla p(x)\\,dx $$\nBy equating the two expressions for $\\int_{D} a\\,\\nabla \\delta u \\cdot \\nabla p\\,dx$, we arrive at the desired expression for the variation of the QoI:\n$$ \\delta Q[a;\\delta a] = - \\int_{D} \\delta a(x)\\,\\nabla u(x)\\cdot \\nabla p(x)\\,dx $$\nThis is the Fréchet derivative of $Q$ with respect to $a$ in the direction $\\delta a$, expressed in terms of the primal solution $u$ and the adjoint solution $p$.\n\nThe problem defines the pointwise Fréchet derivative density, $\\frac{\\delta Q}{\\delta a}(x)$, through the relation:\n$$ \\delta Q[a;\\delta a] = \\int_{D} \\frac{\\delta Q}{\\delta a}(x)\\,\\delta a(x)\\,dx $$\nBy direct comparison with our derived result, we have:\n$$ \\int_{D} \\frac{\\delta Q}{\\delta a}(x)\\,\\delta a(x)\\,dx = \\int_{D} \\big(-\\nabla u(x)\\cdot \\nabla p(x)\\big)\\,\\delta a(x)\\,dx $$\nSince this identity must hold for any admissible perturbation $\\delta a \\in L^{\\infty}(D)$, the integrands must be equal almost everywhere. Therefore, the pointwise Fréchet derivative density is:\n$$ \\frac{\\delta Q}{\\delta a}(x) = -\\nabla u(x)\\cdot \\nabla p(x) $$\nwhere $u$ is the solution to the primal problem and $p$ is the solution to the adjoint problem.", "answer": "$$\n\\boxed{-\\nabla u(x)\\cdot \\nabla p(x)}\n$$", "id": "3459206"}, {"introduction": "Building on the concept of sensitivity, we can now tackle a practical forward uncertainty propagation problem. Here, we model an uncertain diffusion coefficient using a truncated Karhunen–Loève expansion—a standard method for representing a random field with a finite number of random variables. This exercise [@problem_id:3459219] challenges you to apply first-order perturbation theory, combined with the adjoint method, to derive analytical expressions for the mean and variance of an output QoI, connecting abstract sensitivity analysis to concrete statistical estimation.", "problem": "Consider the one-dimensional steady diffusion Partial Differential Equation (PDE)\n$$\n-(a(x,\\omega)\\,u'(x,\\omega))' \\;=\\; f(x), \\quad x\\in(0,1),\n$$\nwith homogeneous Dirichlet boundary conditions $u(0,\\omega)=u(1,\\omega)=0$. Let $f(x)\\equiv 1$ and consider the linear quantity of interest $Q(u)=\\int_0^1 u(x,\\omega)\\,\\psi(x)\\,dx$ with $\\psi(x)\\equiv 1$. The diffusion coefficient is lognormal,\n$$\na(x,\\omega)\\;=\\;\\exp\\!\\left(\\mu \\;+\\; Y(x,\\omega)\\right),\n$$\nwhere $Y(x,\\omega)$ is a zero-mean Gaussian random field with covariance\n$$\nC(x,y)\\;=\\;\\sigma^{2}\\,\\exp\\!\\left(-\\frac{|x-y|}{\\ell}\\right),\n$$\nfor fixed correlation length $\\ell0$ and amplitude $\\sigma0$. Let the Karhunen–Loève (KL) expansion of $Y$ be\n$$\nY(x,\\omega)\\;=\\;\\sum_{k=1}^{\\infty}\\sqrt{\\lambda_{k}}\\,\\xi_{k}(\\omega)\\,\\phi_{k}(x),\n$$\nwhere $\\{\\xi_{k}\\}_{k\\ge 1}$ are independent standard normal random variables, and $\\{(\\lambda_{k},\\phi_{k})\\}_{k\\ge 1}$ are the eigenpairs of the covariance operator associated with $C$, with $\\{\\phi_{k}\\}$ orthonormal in $L^{2}(0,1)$. Define the $m$-term truncated field\n$$\nY_{m}(x,\\omega)\\;=\\;\\sum_{k=1}^{m}\\sqrt{\\lambda_{k}}\\,\\xi_{k}(\\omega)\\,\\phi_{k}(x),\n$$\nand the truncated lognormal coefficient $a_{m}(x,\\omega)=\\exp\\!\\left(\\mu+Y_{m}(x,\\omega)\\right)$. Assume a small-fluctuation regime in which the dimensionless amplitude $\\sigma$ is sufficiently small that first-order perturbations in $Y$ dominate, and neglect all terms of order higher than linear in $\\xi_{k}$.\n\nStarting from first principles of linear elliptic operators, the weak formulation, and the adjoint representation of linear functionals, derive, to leading order in $\\sigma$, closed-form analytic expressions for the expectation $\\mathbb{E}[Q(u_{m})]$ and the variance $\\operatorname{Var}[Q(u_{m})]$, where $u_{m}$ solves\n$$\n-\\big(a_{m}(x,\\omega)\\,u_{m}'(x,\\omega)\\big)'\\;=\\;1,\\quad u_{m}(0,\\omega)=u_{m}(1,\\omega)=0.\n$$\nYour expressions must be written explicitly in terms of $\\mu$, $\\{\\lambda_{k},\\phi_{k}\\}_{k=1}^{m}$, and integrals over $(0,1)$, without introducing uncomputed numerical constants or unspecified auxiliary functions. Round no quantities. Express the final answer as a single analytical expression containing both $\\mathbb{E}[Q(u_{m})]$ and $\\operatorname{Var}[Q(u_{m})]$ in a single row matrix using the $\\mathrm{pmatrix}$ environment as specified.", "solution": "We begin by recalling the structure of the problem. The coefficient is lognormal, $a(x,\\omega)=\\exp(\\mu+Y(x,\\omega))$, with mean baseline $a_{0}:=\\exp(\\mu)$. In a small-fluctuation regime, we expand $a$ to first order in $Y$:\n$$\na(x,\\omega)\\;=\\;a_{0}\\,\\exp\\big(Y(x,\\omega)\\big)\\;\\approx\\;a_{0}\\,\\big(1+Y(x,\\omega)\\big),\n$$\nneglecting terms that are quadratic and higher in $Y$. In this regime, replacing $Y$ by its $m$-term truncated Karhunen–Loève (KL) expansion $Y_{m}$ yields\n$$\na_{m}(x,\\omega)\\;\\approx\\;a_{0}\\,\\big(1+Y_{m}(x,\\omega)\\big),\\qquad\nY_{m}(x,\\omega)=\\sum_{k=1}^{m}\\sqrt{\\lambda_{k}}\\,\\xi_{k}(\\omega)\\,\\phi_{k}(x).\n$$\n\nWe analyze the solution $u_{m}$ by a first-order perturbation about the deterministic mean-coefficient solution. Denote by $\\bar{u}$ the solution of the deterministic baseline problem\n$$\n-\\big(a_{0}\\,\\bar{u}'(x)\\big)'\\;=\\;1,\\qquad \\bar{u}(0)=\\bar{u}(1)=0.\n$$\nBecause $a_{0}$ is constant, this simplifies to\n$$\n-a_{0}\\,\\bar{u}''(x)\\;=\\;1,\\qquad \\bar{u}(0)=\\bar{u}(1)=0.\n$$\nIntegrating twice, we obtain\n$$\n\\bar{u}''(x)\\;=\\;-\\frac{1}{a_{0}},\\quad\n\\bar{u}'(x)\\;=\\;-\\frac{x}{a_{0}}+C_{1},\\quad\n\\bar{u}(x)\\;=\\;-\\frac{x^{2}}{2a_{0}}+C_{1}x+C_{0},\n$$\nand imposing $\\bar{u}(0)=0$ gives $C_{0}=0$, while $\\bar{u}(1)=0$ yields $C_{1}=\\frac{1}{2a_{0}}$. Thus,\n$$\n\\bar{u}(x)\\;=\\;\\frac{x-x^{2}}{2a_{0}},\\qquad\n\\bar{u}'(x)\\;=\\;\\frac{1-2x}{2a_{0}}.\n$$\n\nWe now write the first-order expansion of $u_{m}$ with respect to $Y_{m}$:\n$$\nu_{m}(x,\\omega)\\;\\approx\\;\\bar{u}(x)\\;+\\;\\sum_{k=1}^{m}\\sqrt{\\lambda_{k}}\\,\\xi_{k}(\\omega)\\,u_{k}(x),\n$$\nwhere each $u_{k}$ is the first-order correction associated with the $k$-th KL mode. Substituting $a_{m}\\approx a_{0}(1+Y_{m})$ and $u_{m}\\approx \\bar{u}+\\sum\\sqrt{\\lambda_{k}}\\xi_{k}u_{k}$ into\n$$\n-\\big(a_{m}\\,u_{m}'\\big)'\\;=\\;1,\n$$\nand retaining only terms up to first order in $\\xi_{k}$ yields the hierarchy\n$$\n-a_{0}\\,\\bar{u}''\\;=\\;1,\\qquad -a_{0}\\,u_{k}''\\;-\\;a_{0}\\,\\big(\\phi_{k}\\,\\bar{u}'\\big)'\\;=\\;0,\\quad k=1,\\dots,m,\n$$\nwhere each $u_{k}$ satisfies homogeneous Dirichlet boundary conditions $u_{k}(0)=u_{k}(1)=0$. The second relation can be written as\n$$\n-a_{0}\\,u_{k}''(x)\\;=\\;a_{0}\\,\\big(\\phi_{k}(x)\\,\\bar{u}'(x)\\big)',\\qquad u_{k}(0)=u_{k}(1)=0.\n$$\n\nWe are interested in the quantity of interest\n$$\nQ(u_{m})\\;=\\;\\int_{0}^{1} u_{m}(x,\\omega)\\,\\psi(x)\\,dx,\\qquad \\psi(x)\\equiv 1.\n$$\nUsing the first-order expansion,\n$$\nQ(u_{m})\\;\\approx\\;\\int_{0}^{1}\\bar{u}(x)\\,dx\\;+\\;\\sum_{k=1}^{m}\\sqrt{\\lambda_{k}}\\,\\xi_{k}(\\omega)\\,\\int_{0}^{1}u_{k}(x)\\,dx.\n$$\nBecause $\\mathbb{E}[\\xi_{k}]=0$, the leading-order expectation is\n$$\n\\mathbb{E}\\big[Q(u_{m})\\big]\\;\\approx\\;\\int_{0}^{1}\\bar{u}(x)\\,dx.\n$$\nEvaluating this integral,\n$$\n\\int_{0}^{1}\\bar{u}(x)\\,dx\\;=\\;\\int_{0}^{1}\\frac{x-x^{2}}{2a_{0}}\\,dx\\;=\\;\\frac{1}{2a_{0}}\\left(\\frac{1}{2}-\\frac{1}{3}\\right)\\;=\\;\\frac{1}{12\\,a_{0}}\\;=\\;\\frac{1}{12\\,\\exp(\\mu)}.\n$$\nTherefore,\n$$\n\\mathbb{E}\\big[Q(u_{m})\\big]\\;\\approx\\;\\frac{1}{12\\,\\exp(\\mu)}.\n$$\n\nNext, we derive the leading-order variance. Since $Q(u_{m})$ is affine in the independent standard normal variables $\\xi_{k}$ to first order, the variance is the sum of the squares of the linear coefficients:\n$$\n\\operatorname{Var}\\big[Q(u_{m})\\big]\\;\\approx\\;\\sum_{k=1}^{m}\\lambda_{k}\\,\\left(\\int_{0}^{1}u_{k}(x)\\,dx\\right)^{2}.\n$$\nTo evaluate $\\int_{0}^{1}u_{k}(x)\\,dx$ without explicitly solving $u_{k}$, we use the adjoint representation. Define the self-adjoint operator $L: H_{0}^{1}(0,1)\\cap H^{2}(0,1)\\to L^{2}(0,1)$ by\n$$\nL v\\;=\\;-a_{0}\\,v'',\n$$\nwith homogeneous Dirichlet boundary conditions. The first-order correction satisfies\n$$\nL u_{k}\\;=\\;a_{0}\\,\\big(\\phi_{k}\\,\\bar{u}'\\big)'.\n$$\nFor the linear functional $Q(v)=\\int_{0}^{1} v(x)\\,\\psi(x)\\,dx$ and self-adjoint $L$, there exists an adjoint state $w$ solving\n$$\nL w\\;=\\;\\psi,\\qquad w(0)=w(1)=0,\n$$\nsuch that\n$$\nQ(u_{k})\\;=\\;\\int_{0}^{1}u_{k}(x)\\,\\psi(x)\\,dx\\;=\\;\\int_{0}^{1}\\big(L u_{k}\\big)(x)\\,w(x)\\,dx.\n$$\nSubstituting $L u_{k}=a_{0}\\,(\\phi_{k}\\,\\bar{u}')'$ and integrating by parts once (noting $w(0)=w(1)=0$),\n$$\n\\int_{0}^{1}\\big(L u_{k}\\big)\\,w\\,dx\n\\;=\\;\\int_{0}^{1}a_{0}\\,\\big(\\phi_{k}\\,\\bar{u}'\\big)'\\,w\\,dx\n\\;=\\;-\\,\\int_{0}^{1}a_{0}\\,\\phi_{k}(x)\\,\\bar{u}'(x)\\,w'(x)\\,dx.\n$$\nWith $\\psi(x)\\equiv 1$, the adjoint $w$ satisfies the same deterministic problem as $\\bar{u}$:\n$$\nL w\\;=\\;1\\;\\;\\Longrightarrow\\;\\;w(x)=\\bar{u}(x),\\qquad w'(x)=\\bar{u}'(x).\n$$\nHence,\n$$\n\\int_{0}^{1}u_{k}(x)\\,dx\\;=\\;Q(u_{k})\\;=\\;-\\,a_{0}\\,\\int_{0}^{1}\\phi_{k}(x)\\,\\big(\\bar{u}'(x)\\big)^{2}\\,dx.\n$$\nUsing $\\bar{u}'(x)=\\frac{1-2x}{2a_{0}}$, we obtain\n$$\n\\big(\\bar{u}'(x)\\big)^{2}\\;=\\;\\frac{(1-2x)^{2}}{4\\,a_{0}^{2}},\n$$\nand therefore\n$$\n\\int_{0}^{1}u_{k}(x)\\,dx\\;=\\;-\\,a_{0}\\,\\int_{0}^{1}\\phi_{k}(x)\\,\\frac{(1-2x)^{2}}{4\\,a_{0}^{2}}\\,dx\n\\;=\\;-\\,\\frac{1}{4\\,a_{0}}\\,\\int_{0}^{1}\\phi_{k}(x)\\,(1-2x)^{2}\\,dx.\n$$\nIt follows that the leading-order variance is\n$$\n\\operatorname{Var}\\big[Q(u_{m})\\big]\\;\\approx\\;\\sum_{k=1}^{m}\\lambda_{k}\\left(\\frac{1}{4\\,a_{0}}\\int_{0}^{1}\\phi_{k}(x)\\,(1-2x)^{2}\\,dx\\right)^{2}\n\\;=\\;\\frac{1}{16\\,a_{0}^{2}}\\sum_{k=1}^{m}\\lambda_{k}\\left(\\int_{0}^{1}\\phi_{k}(x)\\,(1-2x)^{2}\\,dx\\right)^{2}.\n$$\n\nCollecting the results and substituting $a_{0}=\\exp(\\mu)$, the leading-order expectation and variance of the truncated $m$-mode model are\n$$\n\\mathbb{E}\\big[Q(u_{m})\\big]\\;\\approx\\;\\frac{1}{12\\,\\exp(\\mu)},\\qquad\n\\operatorname{Var}\\big[Q(u_{m})\\big]\\;\\approx\\;\\frac{1}{16\\,\\exp(2\\mu)}\\sum_{k=1}^{m}\\lambda_{k}\\left(\\int_{0}^{1}\\phi_{k}(x)\\,(1-2x)^{2}\\,dx\\right)^{2}.\n$$\nThese are closed-form analytic expressions in terms of $\\mu$, $\\{\\lambda_{k},\\phi_{k}\\}_{k=1}^{m}$, and integrals over $(0,1)$, as required. They quantify the effect of truncating the Karhunen–Loève expansion to $m$ modes: the expectation is unchanged to first order, while the variance is obtained by summing the contributions of the retained modes only.", "answer": "$$\\boxed{\\begin{pmatrix}\n\\dfrac{1}{12\\,\\exp(\\mu)}  \\dfrac{1}{16\\,\\exp(2\\mu)}\\displaystyle\\sum_{k=1}^{m}\\lambda_{k}\\left(\\int_{0}^{1}\\phi_{k}(x)\\,(1-2x)^{2}\\,dx\\right)^{2}\n\\end{pmatrix}}$$", "id": "3459219"}, {"introduction": "While analytical and perturbation methods offer deep insight, many complex systems necessitate sampling-based approaches like the Monte Carlo method. However, the computational cost of naive Monte Carlo can be prohibitive for expensive PDE models. This practice [@problem_id:3459226] delves into the theory of the Multilevel Monte Carlo (MLMC) method, a powerful variance reduction technique that achieves significant speedups by strategically combining computations across a hierarchy of model fidelities. By deriving the computational complexity of MLMC from first principles, you will gain a firm understanding of its remarkable efficiency and the conditions under which it outperforms standard Monte Carlo.", "problem": "Consider an elliptic partial differential equation (PDE) with random input coefficients on a bounded domain, and a scalar quantity of interest defined by a functional $Q(u)$ of the PDE solution $u$. The goal is to approximate the expectation $E[Q(u)]$ to a prescribed root mean square error (RMSE) tolerance $\\varepsilon \\in (0,1)$. Assume that a hierarchy of spatial discretizations indexed by the level $\\ell \\in \\{0,1,\\dots,L\\}$ is used, with mesh size $h_{\\ell} \\sim 2^{-\\ell}$, and define the level-dependent approximation $Q_{\\ell}$ of $Q$ produced by a numerical solver on level $\\ell$. Suppose the following standard rates hold for an elliptic PDE with a consistent solver and regular $Q$:\n- The weak (bias) rate: $\\left|E[Q_{\\ell} - Q]\\right| \\sim C_{b} \\, 2^{-\\alpha \\ell}$ for some $\\alpha  0$ and constant $C_{b}  0$.\n- The strong rate for multilevel corrections: $\\operatorname{Var}(Q_{\\ell} - Q_{\\ell-1}) \\sim C_{v} \\, 2^{-\\beta \\ell}$ for some $\\beta  0$ and constant $C_{v}  0$.\n- The single-sample computational work on level $\\ell$ obeys $\\mathcal{C}_{\\ell} \\sim C_{c} \\, 2^{\\gamma \\ell}$ for some $\\gamma  0$ and constant $C_{c}  0$.\n\nThe Multilevel Monte Carlo (MLMC) estimator uses the telescoping identity $E[Q_{L}] = E[Q_{0}] + \\sum_{\\ell=1}^{L} E[Q_{\\ell} - Q_{\\ell-1}]$ and independently samples $N_{\\ell}$ realizations of the level differences $Q_{\\ell} - Q_{\\ell-1}$. The single-level Monte Carlo estimator uses $N$ independent realizations of $Q_{L}$ only. Let the total mean-square error be split into the sum of the squared bias and the sampling variance, and assume the bias tolerance is enforced by choosing $L$ such that $|E[Q - Q_{L}]| \\le \\varepsilon/\\sqrt{2}$, while the sampling variance is controlled to satisfy the remaining $\\varepsilon/\\sqrt{2}$ budget.\n\nTask A (Derivation from first principles): Starting from the definitions of the mean-square error and the cost models given above, derive the asymptotic work-complexity with respect to $\\varepsilon$ for both MLMC and single-level Monte Carlo.\n1. Show that the optimal MLMC allocation of samples $N_{\\ell}$ minimizing total work subject to a sampling-variance constraint yields a total work scaling\n$$\nW_{\\mathrm{MLMC}}(\\varepsilon) \\sim \\varepsilon^{-2} \\left(\\sum_{\\ell=0}^{L} \\sqrt{\\operatorname{Var}(Q_{\\ell}-Q_{\\ell-1}) \\, \\mathcal{C}_{\\ell}}\\right)^{2},\n$$\nand deduce the following regimes under the given rates:\n- If $\\beta  \\gamma$, then $W_{\\mathrm{MLMC}}(\\varepsilon) = O(\\varepsilon^{-2})$.\n- If $\\beta = \\gamma$, then $W_{\\mathrm{MLMC}}(\\varepsilon) = O\\!\\left(\\varepsilon^{-2} (\\log \\varepsilon^{-1})^{2}\\right)$.\n- If $\\beta  \\gamma$, then $W_{\\mathrm{MLMC}}(\\varepsilon) = O\\!\\left(\\varepsilon^{-2 - (\\gamma - \\beta)/\\alpha}\\right)$.\n2. Show that the single-level Monte Carlo work is\n$$\nW_{\\mathrm{SL}}(\\varepsilon) \\sim N \\, \\mathcal{C}_{L},\n$$\nwith $N \\sim \\varepsilon^{-2}$ and $L \\sim (\\log_{2}(\\varepsilon^{-1}))/\\alpha$, yielding the scaling\n$$\nW_{\\mathrm{SL}}(\\varepsilon) = O\\!\\left(\\varepsilon^{-2 - \\gamma/\\alpha}\\right).\n$$\nExplain carefully each step using only the mean-square error decomposition, the MLMC telescoping representation, and the cost model.\n\nTask B (Validation specific to elliptic PDEs): Under the assumption $\\alpha  \\tfrac{1}{2}\\beta$ that is common for elliptic PDEs with sufficiently regular $Q$ (for example, when the weak rate is at least half the strong rate), validate that when $\\beta  \\gamma$ the MLMC work achieves the optimal scaling $O(\\varepsilon^{-2})$ while the single-level work is strictly worse, i.e., $O(\\varepsilon^{-2 - \\gamma/\\alpha})$ with exponent strictly greater than $2$. Provide a clear condition in terms of $(\\alpha,\\beta,\\gamma)$ which guarantees this comparison.\n\nProgramming task: Write a complete, runnable program that, for each parameter tuple $(\\alpha,\\beta,\\gamma)$ in the test suite below, computes:\n- The MLMC work exponent $e_{\\mathrm{MLMC}}$ defined by $W_{\\mathrm{MLMC}}(\\varepsilon) = O(\\varepsilon^{-e_{\\mathrm{MLMC}}})$ (use $e_{\\mathrm{MLMC}} = 2$ when $\\beta \\ge \\gamma$, and $e_{\\mathrm{MLMC}} = 2 + (\\gamma - \\beta)/\\alpha$ when $\\beta  \\gamma$).\n- A boolean $b_{\\log}$ indicating whether the MLMC complexity has the extra $(\\log \\varepsilon^{-1})^{2}$ factor (this occurs only when $\\beta = \\gamma$).\n- The single-level work exponent $e_{\\mathrm{SL}} = 2 + \\gamma/\\alpha$ defined by $W_{\\mathrm{SL}}(\\varepsilon) = O(\\varepsilon^{-e_{\\mathrm{SL}}})$.\n- A boolean $b_{\\mathrm{MLMC\\_better}}$ that is true if $e_{\\mathrm{MLMC}}  e_{\\mathrm{SL}}$.\n- A boolean $b_{\\mathrm{valid}}$ that is true if both $\\alpha  \\tfrac{1}{2}\\beta$ and $\\beta  \\gamma$, and $e_{\\mathrm{MLMC}} = 2$.\n\nYour program should use the following test suite of $(\\alpha,\\beta,\\gamma)$ values:\n- Test case $1$: $(\\alpha,\\beta,\\gamma) = (1.2, 2.0, 1.0)$.\n- Test case $2$ (boundary): $(\\alpha,\\beta,\\gamma) = (1.0, 2.0, 2.0)$.\n- Test case $3$: $(\\alpha,\\beta,\\gamma) = (0.8, 1.2, 1.0)$.\n- Test case $4$: $(\\alpha,\\beta,\\gamma) = (0.5, 0.8, 1.0)$.\n- Test case $5$: $(\\alpha,\\beta,\\gamma) = (0.4, 0.7, 0.5)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is itself a bracketed, comma-separated list in the order $[e_{\\mathrm{MLMC}}, b_{\\log}, e_{\\mathrm{SL}}, b_{\\mathrm{MLMC\\_better}}, b_{\\mathrm{valid}}]$. For example, the complete output should look like\n$[[x_{1},y_{1},z_{1},u_{1},v_{1}],[x_{2},y_{2},z_{2},u_{2},v_{2}],\\dots]$,\nwhere each $x_{i}$, $y_{i}$, $z_{i}$, $u_{i}$, $v_{i}$ is a float or boolean as specified above. Angles are not involved and no physical units are required; all quantities are pure scalings. Ensure all numerical comparisons to detect equality (for deciding the log-factor) use a tolerance of $10^{-12}$.", "solution": "The problem is well-posed and scientifically grounded in the theory of Multilevel Monte Carlo (MLMC) methods for uncertainty quantification. It provides a complete and consistent set of definitions and assumptions to derive and analyze the computational complexity of MLMC and single-level Monte Carlo (SLMC) estimators. We can therefore proceed with the solution.\n\nThe solution is presented in two parts as requested: Task A involves the derivation of work-complexity for MLMC and SLMC, and Task B validates the superiority of MLMC under specific conditions.\n\n### Task A: Derivation of Work-Complexity\n\n**1. Multilevel Monte Carlo (MLMC) Complexity**\n\nThe MLMC method estimates the expectation $E[Q_L]$ of a fine-level approximation $Q_L$ to a quantity of interest $Q$ using a telescoping sum of expectations of differences between levels:\n$$\nE[Q_L] = E[Q_0] + \\sum_{\\ell=1}^L E[Q_\\ell - Q_{\\ell-1}]\n$$\nThe MLMC estimator $\\hat{Q}_L^{\\mathrm{MLMC}}$ approximates this sum by drawing an independent set of $N_\\ell$ samples for each level-difference term $Y_\\ell = Q_\\ell - Q_{\\ell-1}$ (with $Q_{-1} \\equiv 0$, so $Y_0 = Q_0$).\n$$\n\\hat{Q}_L^{\\mathrm{MLMC}} = \\sum_{\\ell=0}^L \\hat{Y}_\\ell = \\sum_{\\ell=0}^L \\frac{1}{N_\\ell} \\sum_{i=1}^{N_\\ell} Y_\\ell^{(i)}\n$$\nThe total Mean Square Error (MSE) is bounded by $\\varepsilon^2$ and is decomposed into a squared bias term and a variance term: $\\text{MSE} = (\\text{Bias})^2 + \\text{Variance} \\le \\varepsilon^2$.\n\nThe bias is due to the spatial discretization on the finest level $L$: $\\text{Bias} = E[Q_L - Q]$. The variance arises from the Monte Carlo sampling: $\\text{Variance} = \\operatorname{Var}(\\hat{Q}_L^{\\mathrm{MLMC}})$.\n\nThe problem specifies an error budget:\n- Squared Bias: $|E[Q_L - Q]|^2 \\le (\\varepsilon/\\sqrt{2})^2 = \\varepsilon^2/2$.\n- Sampling Variance: $\\operatorname{Var}(\\hat{Q}_L^{\\mathrm{MLMC}}) \\le \\varepsilon^2/2$.\n\nFirst, we determine the required finest level $L$. The bias constraint and the weak convergence rate $|E[Q_\\ell - Q]| \\sim C_b 2^{-\\alpha \\ell}$ imply:\n$$\nC_b 2^{-\\alpha L} \\lesssim \\frac{\\varepsilon}{\\sqrt{2}} \\implies 2^{\\alpha L} \\gtrsim \\sqrt{2} C_b \\varepsilon^{-1} \\implies L \\gtrsim \\frac{1}{\\alpha} \\log_2(\\varepsilon^{-1})\n$$\nAsymptotically, $L$ scales as $L \\sim \\log(\\varepsilon^{-1})$.\n\nNext, we analyze the sampling variance. Since the samples on each level are independent, the total variance is the sum of the variances from each level:\n$$\n\\operatorname{Var}(\\hat{Q}_L^{\\mathrm{MLMC}}) = \\sum_{\\ell=0}^L \\operatorname{Var}(\\hat{Y}_\\ell) = \\sum_{\\ell=0}^L \\frac{\\operatorname{Var}(Y_\\ell)}{N_\\ell} = \\sum_{\\ell=0}^L \\frac{V_\\ell}{N_\\ell}\n$$\nwhere $V_\\ell = \\operatorname{Var}(Q_\\ell - Q_{\\ell-1})$. The variance constraint is $\\sum_{\\ell=0}^L \\frac{V_\\ell}{N_\\ell} \\le \\frac{\\varepsilon^2}{2}$.\n\nThe total computational work (cost) is the sum of costs over all levels:\n$$\nW_{\\mathrm{MLMC}} = \\sum_{\\ell=0}^L N_\\ell \\mathcal{C}_\\ell\n$$\nwhere $\\mathcal{C}_\\ell$ is the cost of a single sample on level $\\ell$.\n\nWe must find the number of samples $N_\\ell$ that minimizes the work $W_{\\mathrm{MLMC}}$ subject to the variance constraint. This is a constrained optimization problem. We use the method of Lagrange multipliers. The objective is to minimize $\\sum N_\\ell \\mathcal{C}_\\ell$ such that $\\sum V_\\ell/N_\\ell = \\varepsilon^2/2$. The Lagrangian is:\n$$\n\\mathcal{L}(N_0, \\dots, N_L, \\lambda) = \\sum_{\\ell=0}^L N_\\ell \\mathcal{C}_\\ell + \\lambda \\left( \\sum_{\\ell=0}^L \\frac{V_\\ell}{N_\\ell} - \\frac{\\varepsilon^2}{2} \\right)\n$$\nSetting the partial derivative with respect to $N_k$ to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial N_k} = \\mathcal{C}_k - \\lambda \\frac{V_k}{N_k^2} = 0 \\implies N_k = \\sqrt{\\lambda \\frac{V_k}{\\mathcal{C}_k}}\n$$\nWe substitute this optimal $N_k$ back into the variance constraint to find the Lagrange multiplier $\\lambda$:\n$$\n\\sum_{\\ell=0}^L \\frac{V_\\ell}{\\sqrt{\\lambda V_\\ell/\\mathcal{C}_\\ell}} = \\sum_{\\ell=0}^L \\sqrt{\\frac{V_\\ell \\mathcal{C}_\\ell}{\\lambda}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{\\ell=0}^L \\sqrt{V_\\ell \\mathcal{C}_\\ell} = \\frac{\\varepsilon^2}{2}\n$$\nThis gives $\\sqrt{\\lambda} = \\frac{2}{\\varepsilon^2} \\sum_{\\ell=0}^L \\sqrt{V_\\ell \\mathcal{C}_\\ell}$.\n\nNow, we substitute this $\\sqrt{\\lambda}$ into the expression for the total work:\n$$\nW_{\\mathrm{MLMC}} = \\sum_{\\ell=0}^L N_\\ell \\mathcal{C}_\\ell = \\sum_{\\ell=0}^L \\left(\\sqrt{\\lambda \\frac{V_\\ell}{\\mathcal{C}_\\ell}}\\right) \\mathcal{C}_\\ell = \\sqrt{\\lambda} \\sum_{\\ell=0}^L \\sqrt{V_\\ell \\mathcal{C}_\\ell}\n$$\n$$\nW_{\\mathrm{MLMC}} = \\left( \\frac{2}{\\varepsilon^2} \\sum_{\\ell=0}^L \\sqrt{V_\\ell \\mathcal{C}_\\ell} \\right) \\left( \\sum_{\\ell=0}^L \\sqrt{V_\\ell \\mathcal{C}_\\ell} \\right) = \\frac{2}{\\varepsilon^2} \\left( \\sum_{\\ell=0}^L \\sqrt{V_\\ell \\mathcal{C}_\\ell} \\right)^2\n$$\nIgnoring the constant factor of $2$ for asymptotic analysis, the work scales as requested:\n$$\nW_{\\mathrm{MLMC}}(\\varepsilon) \\sim \\varepsilon^{-2} \\left(\\sum_{\\ell=0}^{L} \\sqrt{\\operatorname{Var}(Q_{\\ell}-Q_{\\ell-1}) \\, \\mathcal{C}_{\\ell}}\\right)^{2}\n$$\nNow, we use the given rates: $V_\\ell \\sim C_v 2^{-\\beta \\ell}$ (for $\\ell \\ge 1$) and $\\mathcal{C}_\\ell \\sim C_c 2^{\\gamma \\ell}$. The term inside the sum behaves as $\\sqrt{V_\\ell \\mathcal{C}_\\ell} \\sim \\sqrt{C_v C_c} 2^{(\\gamma-\\beta)\\ell/2}$. The $\\ell=0$ term $\\sqrt{V_0 \\mathcal{C}_0}$ is a constant. The sum's asymptotic behavior as $L \\to \\infty$ is determined by the geometric series $\\sum_{\\ell=1}^L (2^{(\\gamma-\\beta)/2})^\\ell$.\nWe analyze the three regimes for this geometric sum, with $L \\sim \\log(\\varepsilon^{-1})$:\n- **Case 1: $\\beta  \\gamma$**: The exponent $(\\gamma-\\beta)/2$ is negative. The geometric series converges as $L \\to \\infty$. The sum $\\sum_{\\ell=0}^L \\sqrt{V_\\ell \\mathcal{C}_\\ell}$ is bounded by a constant independent of $L$ (and hence $\\varepsilon$). Therefore, the total work is:\n$$\nW_{\\mathrm{MLMC}}(\\varepsilon) \\sim \\varepsilon^{-2} \\cdot (\\text{const.})^2 = O(\\varepsilon^{-2})\n$$\n- **Case 2: $\\beta = \\gamma$**: The exponent $(\\gamma-\\beta)/2$ is zero. The term in the sum is constant. The sum is $\\sum_{\\ell=0}^L (\\text{const.}) \\sim L+1$. Since $L \\sim \\log(\\varepsilon^{-1})$, the sum grows logarithmically. The total work is:\n$$\nW_{\\mathrm{MLMC}}(\\varepsilon) \\sim \\varepsilon^{-2} (L+1)^2 \\sim \\varepsilon^{-2} (\\log(\\varepsilon^{-1}))^2 = O(\\varepsilon^{-2}(\\log \\varepsilon^{-1})^2)\n$$\n- **Case 3: $\\beta  \\gamma$**: The exponent $(\\gamma-\\beta)/2$ is positive. The geometric sum is dominated by its last term, $2^{(\\gamma-\\beta)L/2}$. We have $L \\sim \\frac{1}{\\alpha}\\log_2(\\varepsilon^{-1})$, so $2^L \\sim (\\varepsilon^{-1})^{1/\\alpha}$. This gives:\n$$\n\\sum_{\\ell=0}^L \\sqrt{V_\\ell \\mathcal{C}_\\ell} \\sim 2^{(\\gamma-\\beta)L/2} = (2^L)^{(\\gamma-\\beta)/2} \\sim ((\\varepsilon^{-1})^{1/\\alpha})^{(\\gamma-\\beta)/2} = \\varepsilon^{-(\\gamma-\\beta)/(2\\alpha)}\n$$\nThe total work is:\n$$\nW_{\\mathrm{MLMC}}(\\varepsilon) \\sim \\varepsilon^{-2} \\left( \\varepsilon^{-(\\gamma-\\beta)/(2\\alpha)} \\right)^2 = \\varepsilon^{-2 - (\\gamma-\\beta)/\\alpha} = O(\\varepsilon^{-2 - (\\gamma-\\beta)/\\alpha})\n$$\nThese derivations match the results specified in Task A.1.\n\n**2. Single-Level Monte Carlo (SLMC) Complexity**\n\nThe SLMC estimator uses $N$ samples on a single, sufficiently fine level $L$:\n$$\n\\hat{Q}_L^{\\mathrm{SL}} = \\frac{1}{N} \\sum_{i=1}^N Q_L^{(i)}\n$$\nThe error budget is again split between bias and variance.\nThe bias constraint $|E[Q_L - Q]| \\le \\varepsilon/\\sqrt{2}$ is identical to the MLMC case, requiring the same choice of finest level $L$:\n$$\nL \\sim \\frac{1}{\\alpha} \\log_2(\\varepsilon^{-1})\n$$\nThe variance of the SLMC estimator is $\\operatorname{Var}(\\hat{Q}_L^{\\mathrm{SL}}) = \\frac{\\operatorname{Var}(Q_L)}{N}$. This must be bounded by $\\varepsilon^2/2$. As $L \\to \\infty$, the solution $u_L$ converges to $u$, so $Q_L$ converges to $Q$. Thus, $\\operatorname{Var}(Q_L)$ converges to a constant $\\operatorname{Var}(Q)$, which we denote $\\sigma_Q^2$. The variance constraint becomes:\n$$\n\\frac{\\sigma_Q^2}{N} \\lesssim \\frac{\\varepsilon^2}{2} \\implies N \\gtrsim \\frac{2\\sigma_Q^2}{\\varepsilon^2}\n$$\nAsymptotically, the required number of samples is $N \\sim \\varepsilon^{-2}$.\n\nThe total work for SLMC is $W_{\\mathrm{SL}} = N \\cdot \\mathcal{C}_L$. Substituting the scalings for $N$ and $\\mathcal{C}_L$:\n$$\nN \\sim \\varepsilon^{-2}\n$$\n$$\n\\mathcal{C}_L \\sim C_c 2^{\\gamma L} \\sim C_c (2^L)^\\gamma \\sim C_c ((\\varepsilon^{-1})^{1/\\alpha})^\\gamma = C_c \\varepsilon^{-\\gamma/\\alpha}\n$$\nCombining these gives the total work for SLMC:\n$$\nW_{\\mathrm{SL}}(\\varepsilon) \\sim \\varepsilon^{-2} \\cdot \\varepsilon^{-\\gamma/\\alpha} = \\varepsilon^{-2 - \\gamma/\\alpha} = O(\\varepsilon^{-2-\\gamma/\\alpha})\n$$\nThis derivation matches the result specified in Task A.2.\n\n### Task B: Validation of MLMC Superiority\n\nWe are asked to validate that MLMC is superior to SLMC under the conditions $\\alpha  \\beta/2$ and $\\beta  \\gamma$.\n\nFrom Task A, if $\\beta  \\gamma$, the MLMC complexity is:\n$$\nW_{\\mathrm{MLMC}}(\\varepsilon) = O(\\varepsilon^{-2})\n$$\nThis is the optimal complexity for a Monte Carlo-based method, corresponding to an exponent $e_{\\mathrm{MLMC}}=2$.\n\nThe SLMC complexity is always:\n$$\nW_{\\mathrm{SL}}(\\varepsilon) = O(\\varepsilon^{-2 - \\gamma/\\alpha})\n$$\ncorresponding to an exponent $e_{\\mathrm{SL}} = 2 + \\gamma/\\alpha$.\n\nFor MLMC to be strictly superior in terms of the complexity exponent, we require $e_{\\mathrm{MLMC}}  e_{\\mathrm{SL}}$.\nComparing the exponents:\n$$\n2  2 + \\frac{\\gamma}{\\alpha}\n$$\nThis inequality simplifies to $\\gamma/\\alpha  0$. The problem statement defines $\\gamma  0$ and $\\alpha  0$, so this condition is always satisfied.\nTherefore, whenever $\\beta  \\gamma$, MLMC achieves the optimal $O(\\varepsilon^{-2})$ complexity, while SLMC is strictly worse with a complexity of $O(\\varepsilon^{-2-\\gamma/\\alpha})$. The condition that guarantees this comparison is simply $\\beta  \\gamma$.\n\nThe additional condition $\\alpha  \\beta/2$ is not mathematically required for this specific comparison but is mentioned as a common scenario for elliptic PDEs with sufficiently regular quantities of interest. It ensures that the weak convergence rate $\\alpha$ is sufficiently high relative to the strong convergence rate $\\beta$. For instance, in finite element methods, one might find $\\alpha \\approx p$ and $\\beta \\approx 2p$ for element degree $p$, in which case $\\alpha = \\beta/2$. Having $\\alpha  \\beta/2$ ensures that the bias is not the dominant constraint limiting overall performance, which is a favorable situation for MLMC.\n\nIn summary, the condition that guarantees the optimal $O(\\varepsilon^{-2})$ scaling for MLMC is $\\beta  \\gamma$. Under the problem's standing assumptions that $\\alpha0$ and $\\gamma0$, this scaling is always strictly better than the SLMC scaling of $O(\\varepsilon^{-2 - \\gamma/\\alpha})$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes MLMC and SLMC complexity metrics for a suite of test parameters.\n    \"\"\"\n    test_cases = [\n        # (alpha, beta, gamma)\n        (1.2, 2.0, 1.0),\n        (1.0, 2.0, 2.0),\n        (0.8, 1.2, 1.0),\n        (0.5, 0.8, 1.0),\n        (0.4, 0.7, 0.5),\n    ]\n\n    results = []\n    # Tolerance for floating-point equality checks\n    TOL = 1e-12\n\n    for case in test_cases:\n        alpha, beta, gamma = case\n\n        # 1. Compute MLMC work exponent (e_mlmc)\n        if beta  gamma:\n            e_mlmc = 2.0 + (gamma - beta) / alpha\n        else:  # beta >= gamma\n            e_mlmc = 2.0\n\n        # 2. Compute boolean for log factor (b_log)\n        # The log factor appears if beta is equal to gamma.\n        b_log = abs(beta - gamma)  TOL\n\n        # 3. Compute single-level work exponent (e_sl)\n        e_sl = 2.0 + gamma / alpha\n\n        # 4. Compute boolean for MLMC superiority (b_mlmc_better)\n        # This is true if the MLMC exponent is strictly smaller than the SLMC exponent.\n        b_mlmc_better = e_mlmc  e_sl\n\n        # 5. Compute boolean for validation condition (b_valid)\n        # This is true if alpha > beta/2 and beta > gamma and e_mlmc == 2.\n        # The condition e_mlmc == 2 is redundant given beta > gamma and the\n        # problem's rule for computing e_mlmc, but we include it for rigor.\n        is_alpha_gt_beta_half = alpha > beta / 2.0\n        is_beta_gt_gamma = beta > gamma\n        is_e_mlmc_optimal = abs(e_mlmc - 2.0)  TOL\n\n        b_valid = is_alpha_gt_beta_half and is_beta_gt_gamma and is_e_mlmc_optimal\n\n        # Store the results for this test case\n        case_result = [e_mlmc, b_log, e_sl, b_mlmc_better, b_valid]\n        results.append(case_result)\n\n    # Format the final output string\n    # E.g., [[2.0, False, 2.833, True, True], [2.0, True, 4.0, True, False]]\n    result_strings = []\n    for res in results:\n        # Format list elements for the final string representation\n        formatted_res = []\n        for item in res:\n            if isinstance(item, bool):\n                formatted_res.append(str(item))\n            elif isinstance(item, float):\n                # Using a general representation that avoids excessive decimals\n                formatted_res.append(f\"{item:.16g}\")\n            else:\n                formatted_res.append(str(item))\n        result_strings.append(f\"[{', '.join(formatted_res)}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3459226"}]}