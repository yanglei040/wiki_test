{"hands_on_practices": [{"introduction": "Understanding the adjoint system begins with deriving it in the continuous setting. This foundational exercise guides you through the process using the Lagrangian framework and integration by parts. By carefully analyzing the boundary terms that arise from Green's identity, you will derive the correct adjoint boundary conditions corresponding to different types of state constraints (Dirichlet, Neumann, and Robin), a critical skill for applying the 'optimize-then-discretize' paradigm correctly [@problem_id:3429633].", "problem": "Consider a bounded domain $\\Omega \\subset \\mathbb{R}^{d}$ with sufficiently smooth boundary $\\Gamma$ and outward unit normal $\\boldsymbol{n}$. The boundary is partitioned into three disjoint measurable parts $\\Gamma_{D}$, $\\Gamma_{N}$, and $\\Gamma_{R}$ such that $\\overline{\\Gamma_{D}} \\cup \\overline{\\Gamma_{N}} \\cup \\overline{\\Gamma_{R}} = \\Gamma$. Let $\\kappa \\in C^{1}(\\overline{\\Omega})$ satisfy $\\kappa(\\boldsymbol{x}) \\ge \\kappa_{0} > 0$ for all $\\boldsymbol{x} \\in \\overline{\\Omega}$. Given a desired state $y_{d} \\in L^{2}(\\Omega)$ and a control $u \\in L^{2}(\\Omega)$, the state $y$ is governed by the partial differential equation\n$$\n- \\nabla \\cdot \\big( \\kappa \\nabla y \\big) \\;=\\; u \\quad \\text{in } \\Omega,\n$$\nwith mixed boundary conditions\n$$\ny \\;=\\; 0 \\quad \\text{on } \\Gamma_{D}, \\qquad \\kappa \\nabla y \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{N}, \\qquad \\alpha\\, y \\;+\\; \\beta\\, \\kappa \\nabla y \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{R},\n$$\nwhere $\\alpha, \\beta \\in C(\\Gamma_{R})$ and $(\\alpha(\\boldsymbol{s}), \\beta(\\boldsymbol{s})) \\neq (0,0)$ at each $\\boldsymbol{s} \\in \\Gamma_{R}$. Consider the quadratic tracking cost functional\n$$\nJ(y,u) \\;=\\; \\frac{1}{2} \\int_{\\Omega} \\big( y - y_{d} \\big)^{2} \\, \\mathrm{d}\\boldsymbol{x}.\n$$\nUsing only the Lagrangian approach with an adjoint variable $p$ and the fundamental Green’s identity (integration by parts), derive from first principles the adjoint boundary conditions that ensure stationarity of the Lagrangian with respect to variations of $y$ subject to the given boundary constraints. In particular:\n- Justify, via explicit boundary term analysis, what the adjoint boundary condition must be on $\\Gamma_{D}$ and on $\\Gamma_{N}$, and explain why they are of different type.\n- Treat the Robin segment $\\Gamma_{R}$ by enforcing the linearized boundary constraint for variations, and deduce the Robin-type adjoint boundary condition in the form\n$$\n\\alpha^{\\ast} \\, p \\;+\\; \\beta^{\\ast} \\, \\kappa \\nabla p \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{R}.\n$$\nYour answer must be the ordered pair of adjoint Robin coefficients $(\\alpha^{\\ast}, \\beta^{\\ast})$, expressed as a single row matrix. No numerical approximation is required. Provide your final answer as a closed-form analytic expression. Do not include any units in your final answer.", "solution": "We start from the Lagrangian for the constrained optimization problem. Introduce the adjoint (Lagrange multiplier) $p \\in H^{1}(\\Omega)$ and define\n$$\n\\mathcal{L}(y,u,p) \\;=\\; \\frac{1}{2} \\int_{\\Omega} \\big( y - y_{d} \\big)^{2} \\, \\mathrm{d}\\boldsymbol{x} \\;+\\; \\int_{\\Omega} p \\Big( - \\nabla \\cdot \\big( \\kappa \\nabla y \\big) - u \\Big) \\, \\mathrm{d}\\boldsymbol{x}.\n$$\nTo derive the adjoint equation and boundary conditions, we consider the first variation of $\\mathcal{L}$ with respect to $y$ in a direction $v$. The admissible variations $v$ must satisfy the linearized boundary conditions induced by the state boundary constraints, namely\n$$\nv \\;=\\; 0 \\quad \\text{on } \\Gamma_{D}, \n\\qquad \\kappa \\nabla v \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{N}, \n\\qquad \\alpha\\, v \\;+\\; \\beta\\, \\kappa \\nabla v \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{R}.\n$$\nThe Gâteaux derivative of $\\mathcal{L}$ with respect to $y$ in direction $v$ is\n$$\n\\delta_{y}\\mathcal{L}(y,u,p)[v] \\;=\\; \\int_{\\Omega} (y - y_{d})\\, v \\, \\mathrm{d}\\boldsymbol{x} \\;+\\; \\int_{\\Omega} p \\big( - \\nabla \\cdot (\\kappa \\nabla v) \\big) \\, \\mathrm{d}\\boldsymbol{x}.\n$$\nApply Green’s identity to the second integral. For sufficiently smooth $p$ and $v$,\n$$\n\\int_{\\Omega} p \\big( - \\nabla \\cdot (\\kappa \\nabla v) \\big) \\, \\mathrm{d}\\boldsymbol{x}\n\\;=\\;\n- \\int_{\\Omega} v \\, \\nabla \\cdot (\\kappa \\nabla p) \\, \\mathrm{d}\\boldsymbol{x}\n\\;+\\; \\int_{\\Gamma} \\kappa \\Big( v \\, \\nabla p \\cdot \\boldsymbol{n} \\;-\\; (\\nabla v \\cdot \\boldsymbol{n}) \\, p \\Big) \\, \\mathrm{d}s.\n$$\nHence\n$$\n\\delta_{y}\\mathcal{L}(y,u,p)[v]\n\\;=\\;\n\\int_{\\Omega} \\Big( (y - y_{d}) - \\nabla \\cdot (\\kappa \\nabla p) \\Big) v \\, \\mathrm{d}\\boldsymbol{x}\n\\;+\\; \\int_{\\Gamma} \\kappa \\Big( v \\, \\nabla p \\cdot \\boldsymbol{n} \\;-\\; (\\nabla v \\cdot \\boldsymbol{n}) \\, p \\Big) \\, \\mathrm{d}s.\n$$\nStationarity with respect to all admissible $v$ requires the interior term to vanish for all $v$, yielding the adjoint partial differential equation\n$$\n- \\nabla \\cdot (\\kappa \\nabla p) \\;=\\; y - y_{d} \\quad \\text{in } \\Omega.\n$$\nIt remains to enforce that the boundary integral vanishes for all admissible $v$ satisfying the linearized boundary conditions on each boundary segment. We analyze each segment separately.\n\nOn $\\Gamma_{D}$: The admissible variations satisfy $v = 0$ on $\\Gamma_{D}$, while $\\nabla v \\cdot \\boldsymbol{n}$ is not constrained by this condition. The boundary integral restricted to $\\Gamma_{D}$ reduces to\n$$\n\\int_{\\Gamma_{D}} \\kappa \\Big( 0 \\cdot \\nabla p \\cdot \\boldsymbol{n} \\;-\\; (\\nabla v \\cdot \\boldsymbol{n}) \\, p \\Big) \\, \\mathrm{d}s\n\\;=\\; - \\int_{\\Gamma_{D}} \\kappa \\, (\\nabla v \\cdot \\boldsymbol{n}) \\, p \\, \\mathrm{d}s.\n$$\nBecause $(\\nabla v \\cdot \\boldsymbol{n})$ can be chosen arbitrarily (in the trace sense) under $v=0$, the only way this integral vanishes for all such $v$ is to impose\n$$\np \\;=\\; 0 \\quad \\text{on } \\Gamma_{D}.\n$$\nThus, a Dirichlet state boundary condition induces a Dirichlet adjoint boundary condition on the same segment.\n\nOn $\\Gamma_{N}$: The admissible variations satisfy $\\kappa \\nabla v \\cdot \\boldsymbol{n} = 0$ on $\\Gamma_{N}$, while $v$ is otherwise unconstrained on $\\Gamma_{N}$. The boundary integral restricted to $\\Gamma_{N}$ becomes\n$$\n\\int_{\\Gamma_{N}} \\kappa \\Big( v \\, \\nabla p \\cdot \\boldsymbol{n} \\;-\\; 0 \\cdot p \\Big) \\, \\mathrm{d}s\n\\;=\\; \\int_{\\Gamma_{N}} \\kappa \\, v \\, \\nabla p \\cdot \\boldsymbol{n} \\, \\mathrm{d}s.\n$$\nBecause $v$ can be chosen arbitrarily on $\\Gamma_{N}$, the only way this integral vanishes for all such $v$ is to impose\n$$\n\\kappa \\nabla p \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{N}.\n$$\nThus, a Neumann state boundary condition induces a Neumann adjoint boundary condition on the same segment. This explains the difference between the Dirichlet and Neumann cases: on $\\Gamma_{D}$ the trace of $v$ is fixed to zero, forcing $p$ to vanish; on $\\Gamma_{N}$ the normal derivative of $v$ is fixed to zero, forcing the normal flux of $p$ to vanish.\n\nOn $\\Gamma_{R}$: The admissible variations satisfy the linearized Robin constraint\n$$\n\\alpha \\, v \\;+\\; \\beta \\, \\kappa \\nabla v \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{R}.\n$$\nThe boundary integral restricted to $\\Gamma_{R}$ reads\n$$\n\\int_{\\Gamma_{R}} \\kappa \\Big( v \\, \\nabla p \\cdot \\boldsymbol{n} \\;-\\; (\\nabla v \\cdot \\boldsymbol{n}) \\, p \\Big) \\, \\mathrm{d}s.\n$$\nTo enforce vanishing for all such $v$, it suffices to eliminate $v$ in favor of $\\nabla v \\cdot \\boldsymbol{n}$ (or vice versa). When $\\alpha(\\boldsymbol{s}) \\neq 0$ at a boundary point, we can write $v \\,=\\, - (\\beta \\kappa / \\alpha) \\, \\nabla v \\cdot \\boldsymbol{n}$. Substituting pointwise gives the integrand\n$$\n\\kappa \\left( - \\frac{\\beta \\kappa}{\\alpha} \\, (\\nabla v \\cdot \\boldsymbol{n}) \\, \\nabla p \\cdot \\boldsymbol{n} \\;-\\; (\\nabla v \\cdot \\boldsymbol{n}) \\, p \\right)\n\\;=\\;\n- \\kappa \\, (\\nabla v \\cdot \\boldsymbol{n}) \\left( \\frac{\\beta \\kappa}{\\alpha} \\, \\nabla p \\cdot \\boldsymbol{n} \\;+\\; p \\right).\n$$\nBecause $(\\nabla v \\cdot \\boldsymbol{n})$ can be chosen arbitrarily under the linearized constraint, the factor in parentheses must vanish, yielding\n$$\n\\alpha \\, p \\;+\\; \\beta \\, \\kappa \\nabla p \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{R} \\cap \\{ \\alpha \\neq 0 \\}.\n$$\nA symmetric argument applies at points where $\\beta(\\boldsymbol{s}) \\neq 0$, solving instead for $\\nabla v \\cdot \\boldsymbol{n}$ in terms of $v$ and leading to the same condition. Since $(\\alpha,\\beta)$ are not simultaneously zero, we conclude that the adjoint Robin boundary condition everywhere on $\\Gamma_{R}$ is\n$$\n\\alpha^{\\ast} \\, p \\;+\\; \\beta^{\\ast} \\, \\kappa \\nabla p \\cdot \\boldsymbol{n} \\;=\\; 0\n\\quad \\text{with} \\quad\n\\alpha^{\\ast} \\;=\\; \\alpha, \\;\\; \\beta^{\\ast} \\;=\\; \\beta.\n$$\nIn summary, integration by parts and enforcement of the linearized boundary constraints on variations $v$ imply\n- On $\\Gamma_{D}$: $p = 0$ (Dirichlet).\n- On $\\Gamma_{N}$: $\\kappa \\nabla p \\cdot \\boldsymbol{n} = 0$ (Neumann).\n- On $\\Gamma_{R}$: $\\alpha p + \\beta \\kappa \\nabla p \\cdot \\boldsymbol{n} = 0$ (Robin with unchanged coefficients).\nTherefore, the ordered pair of adjoint Robin coefficients is $(\\alpha^{\\ast}, \\beta^{\\ast}) = (\\alpha, \\beta)$.", "answer": "$$\\boxed{\\begin{pmatrix}\\alpha & \\beta\\end{pmatrix}}$$", "id": "3429633"}, {"introduction": "After understanding the continuous theory, the next step is to translate these concepts into a computable, discrete form. This practice delves into the 'discretize-then-optimize' approach, where the PDE is first discretized using the finite element method. You will derive the gradient of the discrete objective functional by constructing and using the discrete adjoint equation, revealing the direct relationship between the continuous gradient expression and its matrix-vector representation involving stiffness and mass matrices [@problem_id:3429645].", "problem": "Consider the following linear-quadratic optimal control problem constrained by a linear elliptic partial differential equation on a bounded, polygonal domain $\\Omega \\subset \\mathbb{R}^{2}$ with homogeneous Dirichlet boundary conditions. Let $V := H_{0}^{1}(\\Omega)$ and define the bilinear form $a(\\cdot,\\cdot)$ and the $L^{2}(\\Omega)$ inner product $(\\cdot,\\cdot)$ by\n$$\na(y,v) := \\int_{\\Omega} \\nabla y \\cdot \\nabla v \\, dx + \\int_{\\Omega} c(x) \\, y \\, v \\, dx, \\quad (u,v) := \\int_{\\Omega} u \\, v \\, dx,\n$$\nwhere $c \\in L^{\\infty}(\\Omega)$ satisfies $c(x) \\ge 0$ almost everywhere. For a given desired state $y_{d} \\in L^{2}(\\Omega)$ and regularization parameter $\\alpha > 0$, the objective functional is\n$$\nJ(y,u) := \\frac{1}{2} \\, \\| y - y_{d} \\|_{L^{2}(\\Omega)}^{2} + \\frac{\\alpha}{2} \\, \\| u \\|_{L^{2}(\\Omega)}^{2},\n$$\nsubject to the weak state equation\n$$\na(y,v) = (u,v) \\quad \\text{for all } v \\in V.\n$$\nLet $V_{h} \\subset V$ be a conforming finite element space with basis $\\{ \\varphi_{i} \\}_{i=1}^{n}$, and approximate the state, adjoint, and control in $V_{h}$ as\n$$\ny_{h} = \\sum_{i=1}^{n} y_{i} \\, \\varphi_{i}, \\quad p_{h} = \\sum_{i=1}^{n} p_{i} \\, \\varphi_{i}, \\quad u_{h} = \\sum_{i=1}^{n} u_{i} \\, \\varphi_{i}.\n$$\nDefine the stiffness matrix $\\boldsymbol{K} \\in \\mathbb{R}^{n \\times n}$ and mass matrix $\\boldsymbol{M} \\in \\mathbb{R}^{n \\times n}$ by\n$$\n\\boldsymbol{K}_{ij} := a(\\varphi_{j}, \\varphi_{i}), \\quad \\boldsymbol{M}_{ij} := (\\varphi_{j}, \\varphi_{i}),\n$$\nand let $\\boldsymbol{y}_{d} \\in \\mathbb{R}^{n}$ denote the coefficient vector of the $L^{2}(\\Omega)$ projection of $y_{d}$ onto $V_{h}$, i.e., $(y_{d,h}, v_{h}) = (y_{d}, v_{h})$ for all $v_{h} \\in V_{h}$. The discrete state and adjoint equations are\n$$\n\\boldsymbol{K} \\, \\boldsymbol{y} = \\boldsymbol{M} \\, \\boldsymbol{u}, \\quad \\boldsymbol{K} \\, \\boldsymbol{p} = \\boldsymbol{M} \\, (\\boldsymbol{y} - \\boldsymbol{y}_{d}),\n$$\nand the discrete reduced objective is\n$$\nj_{h}(\\boldsymbol{u}) := \\frac{1}{2} \\, (\\boldsymbol{y} - \\boldsymbol{y}_{d})^{\\top} \\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d}) + \\frac{\\alpha}{2} \\, \\boldsymbol{u}^{\\top} \\boldsymbol{M} \\boldsymbol{u},\n$$\nwhere $\\boldsymbol{y} = \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{u}$. Starting only from these definitions and the variational form, derive the discrete reduced gradient with respect to the Euclidean inner product on $\\mathbb{R}^{n}$ via the discrete adjoint equation, and then eliminate the adjoint and state variables to express this gradient entirely in terms of $\\boldsymbol{K}$, $\\boldsymbol{M}$, $\\alpha$, $\\boldsymbol{u}$, and $\\boldsymbol{y}_{d}$. Finally, comment on how this discrete reduced gradient relates to the discretized continuous gradient obtained from the $L^{2}(\\Omega)$ expression $\\alpha \\, u + p$.\n\nYour final answer must be the single closed-form analytic expression for the discrete reduced gradient vector in terms of $\\boldsymbol{K}$, $\\boldsymbol{M}$, $\\alpha$, $\\boldsymbol{u}$, and $\\boldsymbol{y}_{d}$, enclosed in a box. No rounding is required, and no physical units apply.", "solution": "The objective is to derive the gradient of the discrete reduced objective functional\n$$\nj_{h}(\\boldsymbol{u}) := \\frac{1}{2} \\, (\\boldsymbol{y} - \\boldsymbol{y}_{d})^{\\top} \\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d}) + \\frac{\\alpha}{2} \\, \\boldsymbol{u}^{\\top} \\boldsymbol{M} \\boldsymbol{u}\n$$\nwith respect to the Euclidean inner product on $\\mathbb{R}^{n}$. The state vector $\\boldsymbol{y}$ depends on the control vector $\\boldsymbol{u}$ through the discrete state equation $\\boldsymbol{K} \\boldsymbol{y} = \\boldsymbol{M} \\boldsymbol{u}$. The problem stipulates the use of the discrete adjoint method.\n\nLet us compute the Gâteaux derivative of $j_{h}(\\boldsymbol{u})$ in an arbitrary direction $\\delta \\boldsymbol{u} \\in \\mathbb{R}^{n}$. Let $\\boldsymbol{y}(\\boldsymbol{u}) = \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{u}$. A perturbation $\\delta \\boldsymbol{u}$ in the control induces a perturbation $\\delta \\boldsymbol{y}$ in the state, given by linearizing the state equation:\n$$\n\\boldsymbol{K} \\, \\delta\\boldsymbol{y} = \\boldsymbol{M} \\, \\delta\\boldsymbol{u}.\n$$\nThe first variation of $j_{h}$ is\n$$\n\\delta j_{h} = Dj_{h}(\\boldsymbol{u})[\\delta \\boldsymbol{u}] = (\\boldsymbol{y} - \\boldsymbol{y}_{d})^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{y} + \\alpha \\, \\boldsymbol{u}^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{u}.\n$$\nThe adjoint method introduces an adjoint state $\\boldsymbol{p}$ to eliminate $\\delta \\boldsymbol{y}$. The discrete adjoint equation is given as:\n$$\n\\boldsymbol{K} \\boldsymbol{p} = \\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d}).\n$$\nThe bilinear form $a(\\cdot,\\cdot)$ is symmetric, which implies the stiffness matrix $\\boldsymbol{K}$ is symmetric, i.e., $\\boldsymbol{K}^{\\top} = \\boldsymbol{K}$. The mass matrix $\\boldsymbol{M}$ is also symmetric by definition, $\\boldsymbol{M}^{\\top} = \\boldsymbol{M}$. Using the symmetry of $\\boldsymbol{K}$, the adjoint equation can be written as $\\boldsymbol{K}^{\\top} \\boldsymbol{p} = \\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d})$.\n\nNow we manipulate the first term in the expression for $\\delta j_{h}$:\n$$\n(\\boldsymbol{y} - \\boldsymbol{y}_{d})^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{y} = (\\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d}))^{\\top} \\delta\\boldsymbol{y}.\n$$\nSubstituting the expression for $\\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d})$ from the adjoint equation:\n$$\n(\\boldsymbol{y} - \\boldsymbol{y}_{d})^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{y} = (\\boldsymbol{K}^{\\top} \\boldsymbol{p})^{\\top} \\delta\\boldsymbol{y} = \\boldsymbol{p}^{\\top} \\boldsymbol{K} \\, \\delta\\boldsymbol{y}.\n$$\nNext, we substitute the expression for $\\boldsymbol{K} \\, \\delta\\boldsymbol{y}$ from the perturbed state equation:\n$$\n\\boldsymbol{p}^{\\top} \\boldsymbol{K} \\, \\delta\\boldsymbol{y} = \\boldsymbol{p}^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{u}.\n$$\nSubstituting this result back into the expression for $\\delta j_{h}$:\n$$\n\\delta j_{h} = \\boldsymbol{p}^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{u} + \\alpha \\, \\boldsymbol{u}^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{u}.\n$$\nUsing the symmetry of $\\boldsymbol{M}$, we can rewrite this as:\n$$\n\\delta j_{h} = (\\boldsymbol{M} \\boldsymbol{p})^{\\top} \\delta\\boldsymbol{u} + (\\alpha \\boldsymbol{M} \\boldsymbol{u})^{\\top} \\delta\\boldsymbol{u} = (\\alpha \\boldsymbol{M} \\boldsymbol{u} + \\boldsymbol{M} \\boldsymbol{p})^{\\top} \\delta\\boldsymbol{u}.\n$$\nThe gradient of $j_{h}$ with respect to the Euclidean inner product, denoted $\\nabla j_{h}(\\boldsymbol{u})$, is defined by the relation $\\delta j_{h} = (\\nabla j_{h}(\\boldsymbol{u}))^{\\top} \\delta \\boldsymbol{u}$ for all $\\delta \\boldsymbol{u}$. Therefore, we identify the gradient as:\n$$\n\\nabla j_{h}(\\boldsymbol{u}) = \\alpha \\boldsymbol{M} \\boldsymbol{u} + \\boldsymbol{M} \\boldsymbol{p} = \\boldsymbol{M} (\\alpha \\boldsymbol{u} + \\boldsymbol{p}).\n$$\nThe expression $\\boldsymbol{M}(\\alpha \\boldsymbol{u} + \\boldsymbol{p})$ is the Riesz representation of the $L^2$ gradient in Euclidean space. The vector $\\alpha \\boldsymbol{u} + \\boldsymbol{p}$ contains the coefficients of the function $\\alpha u_{h} + p_{h}$, and multiplying by the mass matrix $\\boldsymbol{M}$ is equivalent to taking the $L^2$ inner product with the basis functions.\n\nFinally, we eliminate the state $\\boldsymbol{y}$ and adjoint $\\boldsymbol{p}$ variables:\n1. State: $\\boldsymbol{y} = \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{u}$\n2. Adjoint: $\\boldsymbol{p} = \\boldsymbol{K}^{-1} \\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d})$\n\nSubstitute (1) into (2):\n$$\n\\boldsymbol{p} = \\boldsymbol{K}^{-1} \\boldsymbol{M} (\\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{u} - \\boldsymbol{y}_{d}).\n$$\nNow substitute this expression for $\\boldsymbol{p}$ into the gradient formula $\\nabla j_{h}(\\boldsymbol{u}) = \\boldsymbol{M} (\\alpha \\boldsymbol{u} + \\boldsymbol{p})$:\n$$\n\\nabla j_{h}(\\boldsymbol{u}) = \\boldsymbol{M} \\left( \\alpha \\boldsymbol{u} + \\boldsymbol{K}^{-1} \\boldsymbol{M} (\\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{u} - \\boldsymbol{y}_{d}) \\right).\n$$\nExpanding this expression gives the final form of the gradient:\n$$\n\\nabla j_{h}(\\boldsymbol{u}) = \\alpha \\boldsymbol{M} \\boldsymbol{u} + \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{u} - \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{y}_{d}.\n$$\nThis can be grouped as:\n$$\n\\nabla j_{h}(\\boldsymbol{u}) = (\\alpha \\boldsymbol{M} + \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M}) \\boldsymbol{u} - \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{y}_{d}.\n$$\nThis is the required closed-form expression for the discrete reduced gradient.", "answer": "$$\n\\boxed{(\\alpha \\boldsymbol{M} + \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M}) \\boldsymbol{u} - \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{y}_{d}}\n$$", "id": "3429645"}, {"introduction": "The power of the adjoint method extends to modern, complex applications, including physics-informed machine learning. In this advanced practice, you will use the adjoint method to compute the gradient of a loss function defined by the residual of a time-dependent PDE, a technique used to train generative models to produce physically plausible solutions. This exercise involves deriving and implementing the discrete adjoint for a time-dependent operator and validating the resulting gradient against finite differences, a crucial and standard verification step in computational science [@problem_id:3429635].", "problem": "Consider the Partial Differential Equation (PDE) residual operator associated with the heat equation on a unit space-time domain, defined by $r(y) = \\partial_t y - \\Delta y$, where $y$ is a scalar field on $[0,1] \\times [0,1] \\times [0,1]$ with spatial variables and time. Let the spatial domain be discretized by $N_x$ points in the $x$-direction and $N_y$ points in the $y$-direction, and let time be discretized by $N_t$ points, all uniformly spaced. Denote the grid spacings by $\\Delta x = 1/(N_x - 1)$, $\\Delta y = 1/(N_y - 1)$, and $\\Delta t = 1/(N_t - 1)$. Let the interior spatial indices be $i \\in \\{1, \\dots, N_x - 2\\}$ and $j \\in \\{1, \\dots, N_y - 2\\}$, with time indices $n \\in \\{0, \\dots, N_t - 2\\}$.\n\nDefine the discrete residual at interior points by the standard explicit forward difference in time and the five-point stencil for the Laplacian (with homogeneous Dirichlet boundary values equal to $0$):\n$$\nr_{n,i,j}(y) = \\frac{y_{n+1,i,j} - y_{n,i,j}}{\\Delta t} - \\frac{y_{n,i+1,j} + y_{n,i-1,j} + y_{n,i,j+1} + y_{n,i,j-1} - 4 y_{n,i,j}}{\\Delta x^2},\n$$\nwhere $y_{n,i,j}$ denotes the value of $y$ at time index $n$ and spatial indices $(i,j)$, and the Laplacian is evaluated at time index $n$. Assume $\\Delta x = \\Delta y$.\n\nLet the generator produce samples $y_\\theta$ parameterized linearly by a vector of parameters $\\theta \\in \\mathbb{R}^K$ via a fixed set of basis functions $\\{\\phi_k\\}_{k=1}^K$, with\n$$\ny_\\theta(n,i,j) = \\sum_{k=1}^K \\theta_k \\, \\phi_k(n,i,j).\n$$\nThe basis functions are specified as separable products of sines that are zero on the spatial boundaries,\n$$\n\\phi_k(n,i,j) = \\sin\\!\\left(p_x^{(k)} \\pi x_i\\right) \\, \\sin\\!\\left(p_y^{(k)} \\pi y_j\\right) \\, \\sin\\!\\left(\\beta^{(k)} \\pi t_n\\right),\n$$\nwhere $x_i = i \\Delta x$, $y_j = j \\Delta y$, and $t_n = n \\Delta t$. The integers $p_x^{(k)}$, $p_y^{(k)}$, and $\\beta^{(k)}$ specify spatial and temporal frequencies. The homogeneous Dirichlet boundary condition is enforced implicitly by the spatial sine factors.\n\nDefine the residual penalty functional\n$$\nR(\\theta) = \\sum_{n=0}^{N_t-2} \\sum_{i=1}^{N_x-2} \\sum_{j=1}^{N_y-2} \\left( r_{n,i,j}\\!\\left(y_\\theta\\right) \\right)^2.\n$$\n\nTask:\n1. Derive the discrete adjoint $A^\\ast$ of the residual operator $A$ such that $r = A y$ under the standard Euclidean inner product on the discrete grid, and use it to express the gradient $\\nabla_\\theta R(\\theta)$ as an explicit function of $\\theta$ and the basis functions $\\phi_k$.\n2. Implement a program to compute $R(\\theta)$ and $\\nabla_\\theta R(\\theta)$ using the adjoint of the residual operator for given test cases.\n3. For each test case, validate the adjoint-based gradient against a numerical central finite difference approximation of the gradient and report the relative error defined by\n$$\n\\mathrm{err} = \\frac{\\left\\|\\nabla_\\theta R(\\theta)\\big|_{\\text{adjoint}} - \\nabla_\\theta R(\\theta)\\big|_{\\text{finite diff}}\\right\\|_2}{\\max\\!\\left(10^{-12}, \\left\\|\\nabla_\\theta R(\\theta)\\big|_{\\text{adjoint}}\\right\\|_2\\right)}.\n$$\nUse the perturbation size $\\varepsilon = 10^{-6}$ in the central finite difference.\n\nTest Suite:\nProvide results for the following three test cases. In each case, the spatial and temporal frequencies $(p_x^{(k)}, p_y^{(k)}, \\beta^{(k)})$ and the parameter vector $\\theta$ are specified.\n\n- Test Case $1$ (edge case, zero parameters):\n  - $N_x = 10$, $N_y = 10$, $N_t = 5$.\n  - Basis frequencies: $\\left[(1,1,1), (2,1,1)\\right]$.\n  - Parameters: $\\theta = [0, 0]$.\n\n- Test Case $2$ (general case):\n  - $N_x = 16$, $N_y = 16$, $N_t = 12$.\n  - Basis frequencies: $\\left[(1,2,1), (2,2,1), (3,1,2)\\right]$.\n  - Parameters: $\\theta = [0.4, -0.3, 0.2]$.\n\n- Test Case $3$ (boundary in time, minimal time levels):\n  - $N_x = 12$, $N_y = 12$, $N_t = 2$.\n  - Basis frequencies: $\\left[(1,1,1), (2,3,1), (3,3,1), (4,1,2)\\right]$.\n  - Parameters: $\\theta = [0.1, -0.2, 0.05, 0.3]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the relative errors for the three test cases as a comma-separated list enclosed in square brackets (e.g., $\\left[ \\mathrm{err}_1, \\mathrm{err}_2, \\mathrm{err}_3 \\right]$). The entries must be decimal numbers.", "solution": "### 1. Theoretical Derivation\n\nThe primary task is to compute the gradient of the residual penalty functional, $R(\\theta)$. The adjoint method provides an efficient way to calculate this gradient.\n\n**1.1. Discrete Formulation**\n\nLet the state vector $y$ be a collection of values $\\{y_{n,i,j}\\}$ on a discrete grid. The discrete residual operator $A$ is a linear operator defined by:\n$$\n(Ay)_{n,i,j} = \\frac{y_{n+1,i,j} - y_{n,i,j}}{\\Delta t} - \\frac{y_{n,i+1,j} + y_{n,i-1,j} + y_{n,i,j+1} + y_{n,i,j-1} - 4 y_{n,i,j}}{\\Delta x^2}\n$$\nWe can write $A$ as $A = \\partial_t^+ - \\Delta_d$, where $\\partial_t^+$ is the forward time difference operator and $\\Delta_d$ is the discrete Laplacian.\n\n**1.2. Gradient via Adjoint Method**\n\nThe functional is $R(\\theta) = \\langle r(y_\\theta), r(y_\\theta) \\rangle$. The state $y_\\theta$ is a linear function of $\\theta$: $y_\\theta = \\sum_{k=1}^K \\theta_k \\phi_k$.\nThe gradient with respect to $\\theta_m$ is:\n$$\n\\frac{\\partial R}{\\partial \\theta_m} = 2 \\left\\langle r(y_\\theta), \\frac{\\partial r(y_\\theta)}{\\partial \\theta_m} \\right\\rangle = 2 \\langle r(y_\\theta), A \\phi_m \\rangle\n$$\nIntroducing the adjoint operator $A^*$, defined by $\\langle A y, q \\rangle = \\langle y, A^* q \\rangle$, we can rewrite the gradient:\n$$\n\\frac{\\partial R}{\\partial \\theta_m} = 2 \\langle \\phi_m, A^* r(y_\\theta) \\rangle\n$$\nThis is the core of the adjoint method. We define the adjoint state vector $p = A^* r(y_\\theta)$. The gradient is then computed as $(\\nabla_\\theta R(\\theta))_m = 2 \\langle \\phi_m, p \\rangle$.\n\n**1.3. Derivation of the Adjoint Operator $A^***$\n\nWe derive the adjoint of $A = \\partial_t^+ - \\Delta_d$ by summation by parts. The discrete Laplacian $\\Delta_d$ with homogeneous Dirichlet boundary conditions is self-adjoint, so $\\Delta_d^* = \\Delta_d$.\nFor the time derivative part, summation by parts in time yields the adjoint $(\\partial_t^+)^*$ operator. The full adjoint operator $A^* = (\\partial_t^+)^* - \\Delta_d$ acts on a residual vector $q$ to produce the adjoint state $p = A^*q$. Its components are:\n\\begin{align*}\np_{N_t-1} &= \\frac{q_{N_t-2}}{\\Delta t} \\\\\np_n &= \\frac{q_{n-1} - q_n}{\\Delta t} - (\\Delta_d q)_n \\quad \\text{for } n = N_t-2, \\dots, 1 \\\\\np_0 &= -\\frac{q_0}{\\Delta t} - (\\Delta_d q)_0\n\\end{align*}\nThis is a system that is solved for $p$ *backwards* in time.\n\n### 2. Implementation\n\nThe following Python code implements the adjoint-based gradient calculation and validates it against a finite difference approximation.\n\n```python\nimport numpy as np\n\ndef apply_laplacian_to_y(y_slice, dx2):\n    \"\"\"Computes the Laplacian of a 2D slice y_n, returning it on the interior.\"\"\"\n    lap = (y_slice[2:, 1:-1] + y_slice[:-2, 1:-1] + \n           y_slice[1:-1, 2:] + y_slice[1:-1, :-2] - \n           4 * y_slice[1:-1, 1:-1]) / dx2\n    return lap\n\ndef apply_laplacian_to_q(q_slice, dx2):\n    \"\"\"Computes the Laplacian of a 2D slice q_n on the interior grid, assuming zero padding.\"\"\"\n    q_padded = np.pad(q_slice, pad_width=1, mode='constant', constant_values=0)\n    lap = (q_padded[2:, 1:-1] + q_padded[:-2, 1:-1] + \n           q_padded[1:-1, 2:] + q_padded[1:-1, :-2] - \n           4 * q_padded[1:-1, 1:-1]) / dx2\n    return lap\n\ndef compute_residual(y, dx, dt):\n    \"\"\"Computes the discrete residual r = A*y.\"\"\"\n    Nt, Nx, Ny = y.shape\n    dx2 = dx**2\n    r = np.zeros((Nt - 1, Nx - 2, Ny - 2))\n    \n    for n in range(Nt - 1):\n        y_forward = y[n + 1, 1:Nx-1, 1:Ny-1]\n        y_current = y[n, 1:Nx-1, 1:Ny-1]\n        dt_term = (y_forward - y_current) / dt\n        lap_y_n = apply_laplacian_to_y(y[n], dx2)\n        r[n, :, :] = dt_term - lap_y_n\n    return r\n\ndef compute_objective(r):\n    \"\"\"Computes the objective functional R = ||r||^2.\"\"\"\n    return np.sum(r**2)\n\ndef apply_adjoint(r, dx, dt):\n    \"\"\"Computes the adjoint state p = A*r by solving the backward system.\"\"\"\n    Nt_res, Nx_int, Ny_int = r.shape\n    Nt = Nt_res + 1\n    dx2 = dx**2\n    \n    p = np.zeros((Nt, Nx_int, Ny_int))\n\n    lap_r = np.zeros_like(r)\n    for n in range(Nt_res):\n        lap_r[n, :, :] = apply_laplacian_to_q(r[n], dx2)\n\n    if Nt > 1:\n        # Final time step for adjoint state p\n        p[Nt-1] = r[Nt-2] / dt\n        \n        # Backward time stepping for n = Nt-2 down to 1\n        for n in range(Nt - 2, 0, -1):\n            p[n] = (r[n-1] - r[n]) / dt - lap_r[n]\n        \n        # Initial time step for adjoint state p\n        p[0] = -r[0] / dt - lap_r[0]\n        \n    return p\n\ndef compute_y_theta(theta, phi):\n    \"\"\"Computes y_theta = sum(theta_k * phi_k).\"\"\"\n    return np.einsum('k,knij->nij', theta, phi)\n\ndef process_case(Nx, Ny, Nt, freqs, theta, eps):\n    \"\"\"Runs a single test case to compute the gradient validation error.\"\"\"\n    dx = 1.0 / (Nx - 1)\n    dt = 1.0 / (Nt - 1) if Nt > 1 else 1.0\n\n    x = np.linspace(0, 1, Nx)\n    y_coords = np.linspace(0, 1, Ny)\n    t = np.linspace(0, 1, Nt)\n    \n    K = len(freqs)\n    phi = np.zeros((K, Nt, Nx, Ny))\n    for k in range(K):\n        px, py, beta = freqs[k]\n        sin_x = np.sin(px * np.pi * x)\n        sin_y = np.sin(py * np.pi * y_coords)\n        sin_t = np.sin(beta * np.pi * t)\n        phi[k] = np.outer(sin_t, np.outer(sin_x, sin_y)).reshape(Nt, Nx, Ny)\n\n    # --- Adjoint Gradient Calculation ---\n    y_theta = compute_y_theta(theta, phi)\n    r = compute_residual(y_theta, dx, dt)\n    p = apply_adjoint(r, dx, dt)\n    \n    grad_adj = np.zeros(K)\n    phi_interior = phi[:, :, 1:Nx-1, 1:Ny-1]\n    for k in range(K):\n        grad_adj[k] = 2 * np.sum(phi_interior[k] * p)\n        \n    # --- Finite Difference Gradient Calculation ---\n    grad_fd = np.zeros(K)\n    theta_arr = np.array(theta, dtype=float)\n    for k in range(K):\n        theta_plus =  theta_arr.copy()\n        theta_plus[k] += eps\n        y_plus = compute_y_theta(theta_plus, phi)\n        r_plus = compute_residual(y_plus, dx, dt)\n        R_plus = compute_objective(r_plus)\n        \n        theta_minus = theta_arr.copy()\n        theta_minus[k] -= eps\n        y_minus = compute_y_theta(theta_minus, phi)\n        r_minus = compute_residual(y_minus, dx, dt)\n        R_minus = compute_objective(r_minus)\n        \n        grad_fd[k] = (R_plus - R_minus) / (2 * eps)\n        \n    # --- Error Calculation ---\n    norm_adj = np.linalg.norm(grad_adj)\n    norm_diff = np.linalg.norm(grad_adj - grad_fd)\n    \n    error = norm_diff / max(1e-12, norm_adj)\n    return error\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        {'Nx': 10, 'Ny': 10, 'Nt': 5, 'freqs': [(1, 1, 1), (2, 1, 1)], 'theta': [0.0, 0.0]},\n        {'Nx': 16, 'Ny': 16, 'Nt': 12, 'freqs': [(1, 2, 1), (2, 2, 1), (3, 1, 2)], 'theta': [0.4, -0.3, 0.2]},\n        {'Nx': 12, 'Ny': 12, 'Nt': 2, 'freqs': [(1, 1, 1), (2, 3, 1), (3, 3, 1), (4, 1, 2)], 'theta': [0.1, -0.2, 0.05, 0.3]}\n    ]\n    eps = 1e-6\n    results = []\n    \n    for case in test_cases:\n        error = process_case(case['Nx'], case['Ny'], case['Nt'],\n                             case['freqs'], case['theta'], eps)\n        results.append(error)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n# To generate the answer, one would run the solve() function.\n# solve()\n```", "answer": "[0.0,2.753303975583711e-09,2.320499086701889e-09]", "id": "3429635"}]}