{"hands_on_practices": [{"introduction": "This first practice grounds our study in the fundamental mechanics of Krylov subspace methods for symmetric systems. The MINRES algorithm is built upon the Lanczos process, which generates an optimal, low-dimensional projection of the full system. By manually performing the first few steps of the Lanczos iteration for a small, representative matrix [@problem_id:3421772], you will gain direct insight into how the orthonormal basis and the corresponding tridiagonal matrix are constructed, demystifying the \"black box\" of the iterative solver.", "problem": "Consider the linear system arising from the centered finite-difference discretization of the one-dimensional Poisson equation $-u''(x)=f(x)$ on the unit interval with homogeneous Dirichlet boundary conditions, where the mesh parameter scaling is absorbed into the right-hand side for simplicity. The resulting symmetric positive definite matrix is\n$$\nA=\\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix},\n$$\nwhich is a standard model problem in Krylov subspace methods used for the numerical solution of partial differential equations. In the Minimum Residual method (MINRES), the Lanczos process generates an orthonormal basis for the Krylov subspace and a symmetric tridiagonal projection of $A$. Transpose-Free Quasi-Minimal Residual (TFQMR) is a related method for nonsymmetric problems, but here the focus is on the symmetric case relevant to MINRES.\n\nStarting from the initial guess $x_0=\\mathbf{0}$ and the right-hand side vector $b=\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$, define the initial residual $r_0=b-Ax_0$ and the first Lanczos basis vector $v_1=r_0/\\|r_0\\|$. Using the fundamental definitions of Krylov subspaces and the three-term recurrence that characterizes the Lanczos process on symmetric matrices, carry out exactly two Lanczos iterations to construct the orthonormal basis vectors $v_1$ and $v_2$, and assemble the $2\\times 2$ tridiagonal matrix $T_2$ whose entries are the projection coefficients generated by these iterations.\n\nAs a final scalar diagnostic related to the spectral compression that underpins MINRES, compute the determinant of the tridiagonal matrix $T_2$. Provide this determinant as a single exact real number. No rounding is required, and no units are involved. The answer must be given as a single real-valued number.", "solution": "The problem requires the computation of the determinant of a $2 \\times 2$ tridiagonal matrix $T_2$, which is generated by performing two iterations of the Lanczos process on a given symmetric matrix $A$. The process starts with a specific right-hand side vector $b$ and initial guess $x_0$.\n\nThe given matrix is:\n$$\nA=\\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}\n$$\nThe right-hand side vector is $b=\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$, and the initial guess is $x_0=\\mathbf{0}=\\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$.\n\nThe Lanczos process is an iterative algorithm that generates an orthonormal basis $\\{v_j\\}$ for the Krylov subspace $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$ and a symmetric tridiagonal matrix $T_k$ whose entries are the coefficients of a three-term recurrence relation. The standard algorithm proceeds as follows:\n\n1.  Initialize: Calculate the initial residual $r_0 = b - Ax_0$. Set $\\beta_1 = \\|r_0\\|_2$ and $v_1 = r_0 / \\beta_1$. Set $v_0 = \\mathbf{0}$.\n2.  Iterate for $j=1, 2, \\dots$:\n    a. Compute $w_j = A v_j$.\n    b. Compute the diagonal coefficient $\\alpha_j = v_j^T w_j$.\n    c. Compute the unnormalized next vector $\\tilde{v}_{j+1} = w_j - \\alpha_j v_j - \\beta_j v_{j-1}$.\n    d. Compute the off-diagonal coefficient $\\beta_{j+1} = \\|\\tilde{v}_{j+1}\\|_2$.\n    e. If $\\beta_{j+1} \\neq 0$, normalize to get the next basis vector $v_{j+1} = \\tilde{v}_{j+1} / \\beta_{j+1}$.\n\nWe will carry out this process for two iterations to find the coefficients $\\alpha_1$, $\\beta_2$, and $\\alpha_2$, which form the matrix $T_2$.\n\n**Initialization Step:**\n\nFirst, we compute the initial residual $r_0$:\n$$\nr_0 = b - Ax_0 = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} - \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}\n$$\nNext, we compute the Euclidean norm of $r_0$ to find $\\beta_1$ and then normalize $r_0$ to get $v_1$:\n$$\n\\beta_1 = \\|r_0\\|_2 = \\sqrt{1^2 + 0^2 + 0^2} = 1\n$$\n$$\nv_1 = \\frac{r_0}{\\beta_1} = \\frac{1}{1} \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}\n$$\nThe problem statement defines $v_1=r_0/\\|r_0\\|$, which is consistent with our initialization.\n\n**First Lanczos Iteration ($j=1$):**\n\nWe compute $w_1 = Av_1$:\n$$\nw_1 = Av_1 = \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}2 \\\\ -1 \\\\ 0\\end{pmatrix}\n$$\nThe first diagonal coefficient $\\alpha_1$ is the projection of $w_1$ onto $v_1$:\n$$\n\\alpha_1 = v_1^T w_1 = \\begin{pmatrix}1 & 0 & 0\\end{pmatrix} \\begin{pmatrix}2 \\\\ -1 \\\\ 0\\end{pmatrix} = (1)(2) + (0)(-1) + (0)(0) = 2\n$$\nNow we compute the unnormalized vector for the next step. For $j=1$, the recurrence is $\\tilde{v}_2 = w_1 - \\alpha_1 v_1 - \\beta_1 v_0$. Since $v_0 = \\mathbf{0}$, this simplifies:\n$$\n\\tilde{v}_2 = w_1 - \\alpha_1 v_1 = \\begin{pmatrix}2 \\\\ -1 \\\\ 0\\end{pmatrix} - 2 \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix}\n$$\nThe first off-diagonal coefficient $\\beta_2$ is the norm of this vector:\n$$\n\\beta_2 = \\|\\tilde{v}_2\\|_2 = \\sqrt{0^2 + (-1)^2 + 0^2} = 1\n$$\nSince $\\beta_2 \\neq 0$, we find the second orthonormal basis vector $v_2$:\n$$\nv_2 = \\frac{\\tilde{v}_2}{\\beta_2} = \\frac{1}{1} \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix}\n$$\n\n**Second Lanczos Iteration ($j=2$):**\n\nWe compute $w_2 = Av_2$:\n$$\nw_2 = Av_2 = \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -2 \\\\ 1\\end{pmatrix}\n$$\nThe second diagonal coefficient $\\alpha_2$ is the projection of $w_2$ onto $v_2$:\n$$\n\\alpha_2 = v_2^T w_2 = \\begin{pmatrix}0 & -1 & 0\\end{pmatrix} \\begin{pmatrix}1 \\\\ -2 \\\\ 1\\end{pmatrix} = (0)(1) + (-1)(-2) + (0)(1) = 2\n$$\nThe problem only requires the construction of $T_2$, so we do not need to compute $\\beta_3$ or $v_3$.\n\n**Constructing and Evaluating $T_2$:**\n\nThe $k \\times k$ tridiagonal matrix $T_k$ generated by the Lanczos process is given by:\n$$\nT_k = \\begin{pmatrix}\n\\alpha_1 & \\beta_2 & & \\\\\n\\beta_2 & \\alpha_2 & \\ddots & \\\\\n & \\ddots & \\ddots & \\beta_k \\\\\n & & \\beta_k & \\alpha_k\n\\end{pmatrix}\n$$\nFor $k=2$, using the coefficients we calculated ($\\alpha_1=2$, $\\beta_2=1$, $\\alpha_2=2$), the matrix $T_2$ is:\n$$\nT_2 = \\begin{pmatrix}\n\\alpha_1 & \\beta_2 \\\\\n\\beta_2 & \\alpha_2\n\\end{pmatrix} = \\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n$$\nThe final step is to compute the determinant of $T_2$:\n$$\n\\det(T_2) = (\\alpha_1)(\\alpha_2) - (\\beta_2)^2 = (2)(2) - (1)^2 = 4 - 1 = 3\n$$\nThe determinant of the tridiagonal matrix $T_2$ is $3$.", "answer": "$$\\boxed{3}$$", "id": "3421772"}, {"introduction": "Moving from theory to practice, a crucial skill is selecting the appropriate numerical tool for the job. MINRES is highly efficient for symmetric systems, while TFQMR offers a robust alternative for general nonsymmetric problems. This exercise [@problem_id:3421838] challenges you to devise a practical, matrix-free strategy to diagnose the properties of an unknown operator and make an informed choice between these two solvers, a scenario commonly faced in computational science.", "problem": "You are implementing a matrix-free Krylov solver for a linear system $A u = b$ arising from a finite element discretization of a linear partial differential equation. The operator $A$ is available only as a function that returns $A x$ for a given vector $x$. A left preconditioner $M^{-1}$ is available via a function that returns $M^{-1} r$ for any vector $r$, but the explicit matrices $A$ and $M$ are not assembled. You must decide between the Minimum Residual method (MINRES) and the Transpose-Free Quasi-Minimal Residual method (TFQMR) based on whether the operator is symmetric and on its definiteness, and you must ensure that any preconditioning assumptions required by the chosen solver hold.\n\nStarting from the definitions of self-adjointness and definiteness in an inner-product space, propose matrix-free numerical tests to assess:\n\n- Whether $A$ is symmetric (self-adjoint) with respect to the Euclidean inner product $\\langle x, y \\rangle = x^{\\top} y$, and\n- Whether $A$ is positive definite, indefinite, or positive semidefinite,\n\nand explain how to adapt the solver selection between MINRES and TFQMR based on the test results, taking into account the role of preconditioning in the required inner product. Consider that the partial differential equation may yield a null space (for example, pure Neumann boundary conditions in Poisson problems), and that the preconditioner may not be exactly symmetric in practice. Assume double precision arithmetic with machine unit roundoff $\\epsilon_{\\mathrm{mach}}$ on the order of $10^{-16}$.\n\nWhich option below gives a scientifically sound set of matrix-free tests and a correct solver-selection policy under these constraints?\n\nA. Use random probe tests for symmetry: draw $k$ random nonzero pairs $(x_i, y_i)$ and compute the normalized antisymmetry measures\n$\\eta_i = \\dfrac{\\left|\\langle A x_i, y_i \\rangle - \\langle x_i, A y_i \\rangle\\right|}{\\|x_i\\| \\, \\|A y_i\\| + \\|y_i\\| \\, \\|A x_i\\|}$, accept symmetry if $\\max_i \\eta_i \\le c \\, \\epsilon_{\\mathrm{mach}}$ for a modest constant $c$. For definiteness, run $m$ steps of matrix-free Lanczos on $A$ with a random start to estimate extremal Ritz values $\\lambda_{\\min}$ and $\\lambda_{\\max}$ of $A$ in the Euclidean inner product; classify as symmetric positive definite if $\\lambda_{\\min} > 0$, indefinite if $\\lambda_{\\min} < 0 < \\lambda_{\\max}$, and positive semidefinite if $\\lambda_{\\min} \\approx 0 \\le \\lambda_{\\max}$; if a known null space exists (e.g., constants), project it out before testing. Verify the preconditioner is symmetric positive definite by checking $\\langle M z, z \\rangle > 0$ for random $z \\ne 0$ and small antisymmetry in $\\langle M u, v \\rangle - \\langle u, M v \\rangle$. Choose MINRES if $A$ is symmetric (definiteness can be indefinite) and $M$ is symmetric positive definite, using the $M$-inner product $\\langle x, y \\rangle_M = \\langle M x, y \\rangle$ for preconditioned MINRES; otherwise, choose TFQMR.\n\nB. To test symmetry, approximate $\\|A x - A^{\\top} x\\|$ for random $x$ and accept symmetry if the norm is below $10^{-8}$; to test definiteness, check that $\\|A x\\|$ increases monotonically with $\\|x\\|$. Choose TFQMR only if $A$ is symmetric positive definite; otherwise, choose MINRES because it minimizes the residual for any $A$.\n\nC. Use a single-vector energy test: draw random nonzero $x$ and compute $\\rho(x) = \\langle A x, x \\rangle / \\langle x, x \\rangle$. If $\\rho(x) > 0$, conclude $A$ is symmetric positive definite; if $\\rho(x) < 0$, conclude $A$ is nonsymmetric. Choose Conjugate Gradient for symmetric indefinite systems because it is faster than MINRES; otherwise choose TFQMR.\n\nD. Use a single directional-derivative test: pick one random nonzero pair $(x, y)$, compute $\\left|\\langle A x, y \\rangle - \\langle x, A y \\rangle\\right|$, and accept symmetry if it is below $10^{-3}$. If a negative Rayleigh quotient $\\langle A x, x \\rangle / \\langle x, x \\rangle$ is ever observed, conclude $A$ is nonsymmetric. Always use MINRES on the left-preconditioned system $M^{-1} A u = M^{-1} b$ whenever $M$ is positive definite, because left preconditioning symmetrizes any $A$ in the Euclidean inner product; otherwise use TFQMR.", "solution": "The problem asks for a robust, matrix-free strategy to select between MINRES and TFQMR. The correct strategy must correctly test for the properties of the operator $A$ and the preconditioner $M$ that are required by the algorithms.\n\n**Option A is correct.** It proposes a comprehensive and sound procedure:\n*   **Symmetry Test:** It uses multiple random probe vectors with a properly normalized measure to test for symmetry ($\\langle Ax, y \\rangle \\approx \\langle x, Ay \\rangle$). This is a robust method to check for self-adjointness in finite-precision arithmetic.\n*   **Definiteness Test:** It uses a few steps of the matrix-free Lanczos algorithm to estimate the extremal eigenvalues (Ritz values). This is the standard and most reliable way to determine if a symmetric matrix is positive definite, indefinite, or semidefinite without forming the matrix.\n*   **Preconditioner Check:** It correctly identifies that for preconditioned MINRES, the preconditioner $M$ must be symmetric positive definite (SPD) and that this property can be tested using the provided matrix-free $M^{-1}$ operator.\n*   **Solver Selection:** It applies the correct logic: use MINRES if $A$ is symmetric and $M$ is SPD (since the preconditioned operator $M^{-1}A$ is symmetric in the $M$-inner product); otherwise, default to the general-purpose TFQMR.\n\n**Other options are incorrect:**\n*   **Option B** is fundamentally flawed because testing symmetry via $\\|A x - A^{\\top} x\\|$ requires access to $A^{\\top}$, which is not available in a matrix-free context where only the action of $A$ is provided. Its solver selection logic is also reversed.\n*   **Option C** relies on a non-robust single-vector test and makes incorrect logical deductions (e.g., a negative Rayleigh quotient does not imply a matrix is nonsymmetric). It also incorrectly suggests using Conjugate Gradient for indefinite systems, for which it is not designed.\n*   **Option D** uses a non-robust single-pair test and, most critically, is based on the false premise that left preconditioning ($M^{-1}A$) symmetrizes a general operator $A$ in the Euclidean inner product. Applying MINRES to a nonsymmetric operator would lead to failure.", "answer": "$$\\boxed{A}$$", "id": "3421838"}, {"introduction": "Even with the correct solver chosen, preconditioning can introduce subtle yet profound challenges. When using left preconditioning with TFQMR, the algorithm monitors the norm of the *preconditioned* residual, which can be a poor and misleading proxy for the true residual norm. This exercise [@problem_id:3421783] uses a concrete example to demonstrate this potential pitfall and guides you through deriving a rigorous strategy to adapt the solver's tolerance, ensuring your solution achieves the desired accuracy.", "problem": "Consider the one-dimensional, steady convection–diffusion boundary value problem on the interval $\\left[0,1\\right]$,\n$$\n-\\varepsilon\\,u''(x) + \\beta\\,u'(x) = f(x),\\quad u(0)=0,\\quad u(1)=0,\n$$\nwith positive parameters $\\varepsilon$ and $\\beta$. Discretize using second-order central differences for $u''(x)$ and first-order upwind for $u'(x)$ on a uniform grid with two interior points, so that the resulting linear system is $A\\,x=b$ with\n$$\nA=\\begin{pmatrix}\n2\\varepsilon/h^{2}+\\beta/h & -\\varepsilon/h^{2} \\\\\n-\\left(\\varepsilon/h^{2}+\\beta/h\\right) & 2\\varepsilon/h^{2}+\\beta/h\n\\end{pmatrix},\n$$\nwhere $h$ is the uniform grid spacing. Let $\\varepsilon=10^{-4}$, $\\beta=2$, and $h=1/3$, and choose the right-hand side $b=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$. Consider left preconditioning with the diagonal matrix\n$$\nM=\\begin{pmatrix}\n1 & 0 \\\\\n0 & 10^{5}\n\\end{pmatrix},\n$$\nso the left-preconditioned system is $M^{-1}A\\,x=M^{-1}b$. Let $r_{k}=b-A\\,x_{k}$ denote the true residual at an iterate $x_{k}$ and $\\hat{r}_{k}=M^{-1}r_{k}$ the left-preconditioned residual that is monitored by Transpose-Free Quasi-Minimal Residual (TFQMR).\n\n(a) Using only the core definitions of residuals and preconditioning, evaluate the initial residuals at $x_{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ and compute the ratio\n$$\nR=\\frac{\\|r_{0}\\|_{2}}{\\|\\hat{r}_{0}\\|_{2}},\n$$\nto demonstrate a case where the recursively monitored residual in left-preconditioned TFQMR significantly underestimates the true residual.\n\n(b) Starting from norm equivalence and the submultiplicativity of the operator norm, derive a tolerance adaptation strategy that guarantees $\\|r_{k}\\|_{2}\\le t_{\\mathrm{true}}$ whenever TFQMR’s recursive monitor satisfies $\\|\\hat{r}_{k}\\|_{2}\\le \\tau_{\\mathrm{TFQMR}}$. Express the adaptation in the form $\\tau_{\\mathrm{TFQMR}}=\\alpha\\,t_{\\mathrm{true}}$ and give $\\alpha$ in terms of $M$.\n\n(c) For the specific $M$ above, evaluate the scaling factor $\\alpha$ numerically, and provide its value rounded to four significant figures. No units are required for the final numerical value.", "solution": "The problem is divided into three parts: (a) a numerical demonstration, (b) a theoretical derivation, and (c) a final numerical evaluation. We will address each part in sequence.\n\n(a) Evaluation of the initial residuals and their norm ratio.\nWe are given the parameters $\\varepsilon = 10^{-4}$, $\\beta = 2$, and the grid spacing $h = 1/3$. We first evaluate the entries of the matrix $A$.\nThe required terms are:\n$$\n\\frac{\\varepsilon}{h^2} = \\frac{10^{-4}}{(1/3)^2} = \\frac{10^{-4}}{1/9} = 9 \\times 10^{-4} = 0.0009\n$$\n$$\n\\frac{\\beta}{h} = \\frac{2}{1/3} = 6\n$$\nUsing these values, we can construct the matrix $A$:\n$$\nA = \\begin{pmatrix}\n2\\varepsilon/h^{2}+\\beta/h & -\\varepsilon/h^{2} \\\\\n-\\left(\\varepsilon/h^{2}+\\beta/h\\right) & 2\\varepsilon/h^{2}+\\beta/h\n\\end{pmatrix}\n= \\begin{pmatrix}\n2(0.0009) + 6 & -0.0009 \\\\\n-(0.0009 + 6) & 2(0.0009) + 6\n\\end{pmatrix}\n= \\begin{pmatrix}\n6.0018 & -0.0009 \\\\\n-6.0009 & 6.0018\n\\end{pmatrix}\n$$\nThe initial guess for the solution is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. The initial true residual $r_0$ is defined as $r_0 = b - A x_0$. With $x_0$ being the zero vector, this simplifies to $r_0 = b$.\nGiven $b = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, we have $r_0 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe Euclidean norm (or $L_2$-norm) of the initial true residual is:\n$$\n\\|r_0\\|_2 = \\sqrt{0^2 + 1^2} = 1\n$$\nNext, we compute the initial left-preconditioned residual, $\\hat{r}_0 = M^{-1}r_0$. The preconditioning matrix is given as $M = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^5 \\end{pmatrix}$. Its inverse is:\n$$\nM^{-1} = \\begin{pmatrix} 1^{-1} & 0 \\\\ 0 & (10^5)^{-1} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{-5} \\end{pmatrix}\n$$\nNow we compute $\\hat{r}_0$:\n$$\n\\hat{r}_0 = M^{-1}r_0 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{-5} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (0)(1) \\\\ (0)(0) + (10^{-5})(1) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 10^{-5} \\end{pmatrix}\n$$\nThe Euclidean norm of the initial preconditioned residual is:\n$$\n\\|\\hat{r}_0\\|_2 = \\sqrt{0^2 + (10^{-5})^2} = 10^{-5}\n$$\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{\\|r_0\\|_2}{\\|\\hat{r}_0\\|_2} = \\frac{1}{10^{-5}} = 10^5\n$$\nThis result demonstrates that for this specific problem setup, the norm of the monitored residual $\\|\\hat{r}_0\\|_2$ is $5$ orders of magnitude smaller than the norm of the true residual $\\|r_0\\|_2$.\n\n(b) Derivation of the tolerance adaptation strategy.\nThe relationship between the true residual $r_k$ and the left-preconditioned residual $\\hat{r}_k$ is given by $\\hat{r}_k = M^{-1}r_k$.\nMultiplying from the left by $M$, we obtain an expression for the true residual in terms of the preconditioned one:\n$$\nr_k = M\\hat{r}_k\n$$\nWe are interested in the relationship between the norms of these vectors. Taking the vector $2$-norm of both sides, we get:\n$$\n\\|r_k\\|_2 = \\|M\\hat{r}_k\\|_2\n$$\nUsing the submultiplicative property of induced matrix norms (also known as consistency property), which states that for any compatible matrix $A$ and vector $v$, $\\|Av\\| \\le \\|A\\|\\|v\\|$, we can establish an inequality:\n$$\n\\|M\\hat{r}_k\\|_2 \\le \\|M\\|_2 \\|\\hat{r}_k\\|_2\n$$\nwhere $\\|M\\|_2$ is the matrix $2$-norm (operator norm) induced by the vector $2$-norm. This leads to the inequality:\n$$\n\\|r_k\\|_2 \\le \\|M\\|_2 \\|\\hat{r}_k\\|_2\n$$\nThe TFQMR algorithm monitors the preconditioned residual norm $\\|\\hat{r}_k\\|_2$ and typically stops when it falls below a user-defined tolerance, which we denote as $\\tau_{\\mathrm{TFQMR}}$. That is, the stopping criterion is $\\|\\hat{r}_k\\|_2 \\le \\tau_{\\mathrm{TFQMR}}$.\nSubstituting this into our inequality gives an upper bound on the true residual norm at termination:\n$$\n\\|r_k\\|_2 \\le \\|M\\|_2 \\tau_{\\mathrm{TFQMR}}\n$$\nOur goal is to ensure that the true residual norm is bounded by a desired tolerance $t_{\\mathrm{true}}$, i.e., $\\|r_k\\|_2 \\le t_{\\mathrm{true}}$. To guarantee this, we must choose $\\tau_{\\mathrm{TFQMR}}$ such that the upper bound on $\\|r_k\\|_2$ is at most $t_{\\mathrm{true}}$:\n$$\n\\|M\\|_2 \\tau_{\\mathrm{TFQMR}} \\le t_{\\mathrm{true}}\n$$\nTo obtain the least restrictive (largest) possible tolerance $\\tau_{\\mathrm{TFQMR}}$ that satisfies this condition, we set the terms to be equal:\n$$\n\\|M\\|_2 \\tau_{\\mathrm{TFQMR}} = t_{\\mathrm{true}}\n$$\nSolving for $\\tau_{\\mathrm{TFQMR}}$, we find the adaptive tolerance strategy:\n$$\n\\tau_{\\mathrm{TFQMR}} = \\frac{1}{\\|M\\|_2} t_{\\mathrm{true}}\n$$\nComparing this to the requested form $\\tau_{\\mathrm{TFQMR}} = \\alpha\\,t_{\\mathrm{true}}$, we identify the scaling factor $\\alpha$ as:\n$$\n\\alpha = \\frac{1}{\\|M\\|_2}\n$$\n\n(c) Numerical evaluation of the scaling factor $\\alpha$.\nFrom the result of part (b), we have $\\alpha = 1/\\|M\\|_2$. We need to calculate the $2$-norm of the given preconditioner $M$:\n$$\nM = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^5 \\end{pmatrix}\n$$\nThe matrix $2$-norm $\\|M\\|_2$ is defined as the maximum singular value of $M$. For a diagonal matrix $D = \\mathrm{diag}(d_1, d_2, \\dots, d_n)$, the singular values are simply the absolute values of the diagonal entries, $|d_i|$. The $2$-norm is the maximum of these values.\nIn this case, the diagonal entries of $M$ are $1$ and $10^5$. Their absolute values are $1$ and $10^5$.\nTherefore, the $2$-norm of $M$ is:\n$$\n\\|M\\|_2 = \\max(|1|, |10^5|) = 10^5\n$$\nNow we can compute the scaling factor $\\alpha$:\n$$\n\\alpha = \\frac{1}{\\|M\\|_2} = \\frac{1}{10^5} = 10^{-5}\n$$\nThe problem asks for this value to be rounded to four significant figures. The exact value is $0.00001$. To express this with four significant figures, we write it in scientific notation as $1.000 \\times 10^{-5}$.", "answer": "$$\n\\boxed{1.000 \\times 10^{-5}}\n$$", "id": "3421783"}]}