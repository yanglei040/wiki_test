{"hands_on_practices": [{"introduction": "Before implementing any analysis method, it is crucial to understand its statistical mechanical foundations. A potential of mean force is defined along a low-dimensional collective variable, $s$, but our simulations sample the high-dimensional space of all atomic coordinates, $q$. This exercise, [@problem_id:3461102], challenges you to consider the consequences of this projection. Properly transforming a probability density from $q$-space to $s$-space requires a metric correction, known as the Jacobian factor, which accounts for the volume of configuration space corresponding to an interval in the collective variable. This practice will solidify your understanding of why this correction is necessary and how it combines with the standard reweighting factor to produce a physically meaningful free energy profile.", "problem": "A system with microscopic coordinates $q \\in \\mathbb{R}^{D}$ evolves under a potential energy $U(q)$ at temperature $T$. The unbiased equilibrium probability density of $q$ is $P(q) \\propto \\exp(-\\beta U(q))$ with $\\beta = 1/(k_{B} T)$, where $k_{B}$ is the Boltzmann constant. A single scalar collective variable is defined by a generally nonlinear map $s = \\phi(q)$, assumed differentiable and possibly many-to-one. The Jacobian factor associated with this map (for the $1$-dimensional $s$) is $J(q) = \\left| \\partial \\phi / \\partial q \\right|$, understood as the magnitude of the gradient along the preimage manifold of $s$ in $q$-space.\n\nTo enhance sampling, $M$ biased simulations (windows) are performed. In window $i \\in \\{1,\\dots,M\\}$, the system is simulated under a bias $w_{i}(s)$ added to the physical potential, so the simulated distribution is $P_{i}(q) \\propto \\exp\\left(-\\beta \\left[U(q) + w_{i}(\\phi(q))\\right]\\right)$. From each window, a set of microstates $\\{q_{i,n}\\}$ with corresponding values $s_{i,n} = \\phi(q_{i,n})$ is collected. The goal is to estimate the potential of mean force $F(s)$ defined by $P_{s}(s) \\propto \\exp(-\\beta F(s))$, where $P_{s}(s)$ is the unbiased probability density of $s$.\n\nStarting from fundamental definitions of equilibrium statistical mechanics and probability transformation under change of variables, decide how the Jacobian $J(q)$ enters the construction of histograms in $s$-space from the biased samples, and how the correction for both bias and mapping is used to obtain $F(s)$. Which option correctly prescribes the per-sample contribution to an $s$-bin and the subsequent free-energy estimate, up to an overall normalization constant?\n\nA. Each sample contributes a weight $\\exp\\left(\\beta w_{i}(s_{i,n})\\right)$ to its $s$-bin; the Jacobian $J(q)$ is not needed because binning in $s$ already changes variables. Then compute $F(s)$ via $F(s) = -k_{B} T \\ln \\hat{P}_{s}(s) + C$.\n\nB. Each sample contributes a weight $\\exp\\left(\\beta w_{i}(s_{i,n})\\right)/J(q_{i,n})$ to its $s$-bin, reflecting both bias reweighting and the change-of-variables measure. Then compute $F(s)$ via $F(s) = -k_{B} T \\ln \\hat{P}_{s}(s) + C$.\n\nC. Each sample contributes a weight $J(q_{i,n}) \\exp\\left(-\\beta w_{i}(s_{i,n})\\right)$ to its $s$-bin because a larger Jacobian increases the measure in $s$; then compute $F(s)$ via $F(s) = -k_{B} T \\ln \\hat{P}_{s}(s) + C$.\n\nD. The bias can be ignored when constructing histograms, while the mapping requires multiplying each contribution by $J(q_{i,n})^{2}$ since determinants enter quadratically; then compute $F(s)$ via $F(s) = -k_{B} T \\ln \\hat{P}_{s}(s) + C$.", "solution": "The user requires a critical validation of a problem in statistical mechanics concerning the estimation of the potential of mean force from biased simulations, followed by a detailed solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- System microscopic coordinates: $q \\in \\mathbb{R}^{D}$.\n- Potential energy function: $U(q)$.\n- System temperature: $T$.\n- Unbiased equilibrium probability density of microstates: $P(q) \\propto \\exp(-\\beta U(q))$, with $\\beta = 1/(k_{B} T)$.\n- Single scalar collective variable (CV): $s = \\phi(q)$, where $\\phi$ is a differentiable, many-to-one map.\n- Jacobian factor: $J(q) = \\left| \\partial \\phi / \\partial q \\right|$, defined as the magnitude of the gradient of $\\phi(q)$.\n- Number of biased simulations (windows): $M$.\n- Bias potential in window $i$: $w_{i}(s)$.\n- Simulated probability density in window $i$: $P_{i}(q) \\propto \\exp\\left(-\\beta \\left[U(q) + w_{i}(\\phi(q))\\right]\\right)$.\n- Data collected: sets of microstates $\\{q_{i,n}\\}$ and corresponding CV values $s_{i,n} = \\phi(q_{i,n})$ from each window $i$.\n- Target quantity: The potential of mean force (PMF), $F(s)$.\n- Definition of PMF: $P_{s}(s) \\propto \\exp(-\\beta F(s))$, where $P_{s}(s)$ is the unbiased probability density of the collective variable $s$.\n- Question: Determine the correct per-sample contribution to a histogram in $s$-space and the subsequent formula for $F(s)$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem is firmly established in the field of computational statistical mechanics, specifically relating to enhanced sampling methods like Umbrella Sampling and the analysis thereof using methods like the Weighted Histogram Analysis Method (WHAM). All concepts—Boltzmann distribution, collective variables, bias potentials, potential of mean force, and the change of probability measure—are standard and correctly defined.\n- **Well-Posed:** The problem provides all necessary definitions to derive the relationship between the sampled data and the desired quantity, $F(s)$. It asks for a specific mathematical prescription, which can be derived from first principles.\n- **Objective:** The problem is stated in precise, technical language without ambiguity or subjectivity. The definition of the Jacobian $J(q)$ as the magnitude of the gradient of $\\phi(q)$ is a standard formulation in the context of the co-area formula for transforming probability densities. Specifically, for a function $\\phi: \\mathbb{R}^D \\to \\mathbb{R}$, the gradient $\\nabla\\phi(q)$ is a vector in $\\mathbb{R}^D$, and its magnitude $|\\nabla\\phi(q)|$ is the quantity referred to. This is consistent with rigorous derivations of the PMF.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is scientifically sound, well-posed, objective, and presents a standard, non-trivial problem in computational physics/chemistry. I will proceed with the derivation and solution.\n\n### Derivation and Solution\n\nThe goal is to construct an estimator for the unbiased probability density of the collective variable, $P_s(s)$, from samples obtained in biased simulations. The potential of mean force, $F(s)$, is then directly related to $P_s(s)$.\n\n**1. Unbiased Probability Density of the Collective Variable, $P_s(s)$**\n\nThe probability of observing the system with a collective variable value in an infinitesimal range $[s, s+ds]$ is obtained by integrating the full microstate probability density $P(q)$ over all microstates $q$ for which $\\phi(q)$ falls in this range. A formal way to express the probability density $P_s(s)$ is by using the Dirac delta function:\n$$ P_s(s) = \\int_{\\mathbb{R}^D} P(q) \\, \\delta(s - \\phi(q)) \\, dq $$\nwhere $P(q) = Z^{-1} \\exp(-\\beta U(q))$ is the normalized, unbiased probability density of the microstates, with $Z = \\int \\exp(-\\beta U(q)) \\, dq$.\n\nThis integral can be evaluated using the co-area formula, which relates an integral over a volume to an integral over level surfaces. For any integrable function $g(q)$, the following holds:\n$$ \\int_{\\mathbb{R}^D} g(q) \\, \\delta(s - \\phi(q)) \\, dq = \\int_{\\phi(q)=s} \\frac{g(q')}{|\\nabla \\phi(q')|} \\, dS_{q'} $$\nwhere the integral on the right is over the $(D-1)$-dimensional surface defined by $\\phi(q')=s$, $dS_{q'}$ is the surface area element, and $|\\nabla \\phi(q')|$ is the magnitude of the gradient of $\\phi$ at point $q'$, which the problem defines as the Jacobian $J(q')$.\n\nApplying this to our expression for $P_s(s)$ with $g(q) = P(q)$:\n$$ P_s(s) = \\int_{\\phi(q)=s} \\frac{P(q')}{J(q')} \\, dS_{q'} $$\nSubstituting $P(q') = Z^{-1} \\exp(-\\beta U(q'))$:\n$$ P_s(s) = \\frac{1}{Z} \\int_{\\phi(q)=s} \\frac{\\exp(-\\beta U(q'))}{J(q')} \\, dS_{q'} $$\nThis shows that the unbiased probability density $P_s(s)$ is proportional to the Boltzmann factor $\\exp(-\\beta U(q))$ divided by the Jacobian factor $J(q)$, integrated over the constant-$s$ surface.\n\n**2. Reweighting from Biased Simulations**\n\nIn simulation $i$, samples $\\{q_{i,n}\\}$ are drawn from the biased distribution:\n$$ P_i(q) = Z_i^{-1} \\exp\\left(-\\beta \\left[U(q) + w_i(\\phi(q))\\right]\\right) $$\nwhere $Z_i$ is the partition function of the biased system.\nWe can express the unbiased Boltzmann factor in terms of the biased probability density:\n$$ \\exp(-\\beta U(q)) = Z_i \\, P_i(q) \\, \\exp\\left(+\\beta w_i(\\phi(q))\\right) $$\nSince we are interested in estimating a quantity proportional to $P_s(s)$, we can ignore the normalization constants $Z$ and $Z_i$. Therefore, a sample $q$ drawn from the distribution $P_i(q)$ can be seen as representing a contribution to the unbiased ensemble proportional to $\\exp(+\\beta w_i(\\phi(q)))$.\n\n**3. Combining Reweighting and the Jacobian Correction**\n\nTo build an estimator for $P_s(s)$, we must account for both the reweighting and the Jacobian correction. For each sample $q_{i,n}$ from simulation $i$, with its corresponding value $s_{i,n}=\\phi(q_{i,n})$, its contribution to the unbiased histogram at $s_{i,n}$ must be proportional to the term in the integrand of the expression for $P_s(s)$, evaluated at $q_{i,n}$, and reweighted to the unbiased ensemble.\n\nThe per-sample contribution to the density at $s$ is proportional to $\\frac{\\exp(-\\beta U(q))}{J(q)}$. Using the reweighting identity, this is proportional to $\\frac{P_i(q) \\exp(+\\beta w_i(\\phi(q)))}{J(q)}$. Since samples are already drawn according to $P_i(q)$, the weight that must be applied to each sample to construct the histogram is the remaining factor.\n\nThus, the weight for a sample $q_{i,n}$ is:\n$$ \\text{weight} \\propto \\frac{\\exp(+\\beta w_i(s_{i,n}))}{J(q_{i,n})} $$\nA histogram, $\\hat{P}_s(s)$, is constructed by summing these weights for all samples from all $M$ simulations that fall into the corresponding bin for $s$.\n\n**4. Calculating the Potential of Mean Force, $F(s)$**\n\nThe PMF is defined by $P_s(s) \\propto \\exp(-\\beta F(s))$. Solving for $F(s)$:\n$$ \\ln(P_s(s)) = -\\beta F(s) + \\text{constant} $$\n$$ F(s) = -\\frac{1}{\\beta} \\ln(P_s(s)) + C' $$\n$$ F(s) = -k_B T \\ln(P_s(s)) + C' $$\nUsing our histogram estimator $\\hat{P}_s(s)$, which is proportional to the true density $P_s(s)$, we get:\n$$ F(s) = -k_B T \\ln(\\hat{P}_s(s)) + C $$\nwhere $C$ is an arbitrary additive constant.\n\n### Option-by-Option Analysis\n\n**A. Each sample contributes a weight $\\exp\\left(\\beta w_{i}(s_{i,n})\\right)$ to its $s$-bin; the Jacobian $J(q)$ is not needed because binning in $s$ already changes variables. Then compute $F(s)$ via $F(s) = -k_{B} T \\ln \\hat{P}_{s}(s) + C$.**\nThis option correctly identifies the bias reweighting factor but incorrectly omits the Jacobian correction $1/J(q)$. The argument that binning accounts for the change of variables is mathematically unsound. The transformation of a probability density from a $D$-dimensional space to a $1$-dimensional space via a nonlinear map requires a metric correction term, which is the Jacobian factor.\n**Verdict: Incorrect.**\n\n**B. Each sample contributes a weight $\\exp\\left(\\beta w_{i}(s_{i,n})\\right)/J(q_{i,n})$ to its $s$-bin, reflecting both bias reweighting and the change-of-variables measure. Then compute $F(s)$ via $F(s) = -k_{B} T \\ln \\hat{P}_{s}(s) + C$.**\nThis option correctly combines the bias reweighting factor $\\exp(\\beta w_i(s_{i,n}))$ with the Jacobian correction term $1/J(q_{i,n})$. The subsequent calculation of $F(s)$ is also the standard definition based on the estimated probability density. This matches our derivation from first principles.\n**Verdict: Correct.**\n\n**C. Each sample contributes a weight $J(q_{i,n}) \\exp\\left(-\\beta w_{i}(s_{i,n})\\right)$ to its $s$-bin because a larger Jacobian increases the measure in $s$; then compute $F(s)$ via $F(s) = -k_{B} T \\ln \\hat{P}_{s}(s) + C$.**\nThis option is incorrect on two counts. First, the bias reweighting factor is $\\exp(+\\beta w_i)$, not $\\exp(-\\beta w_i)$. Second, the Jacobian factor enters as $1/J(q)$, not $J(q)$. A larger gradient magnitude $J(q)$ means that the level surfaces of $\\phi(q)$ are closer together, implying a smaller, not larger, volume of configuration space corresponds to a given interval $ds$.\n**Verdict: Incorrect.**\n\n**D. The bias can be ignored when constructing histograms, while the mapping requires multiplying each contribution by $J(q_{i,n})^{2}$ since determinants enter quadratically; then compute $F(s)$ via $F(s) = -k_{B} T \\ln \\hat{P}_{s}(s) + C$.**\nThis option is fundamentally flawed. The bias potential profoundly alters the sampled distribution and cannot be ignored. The claim that the Jacobian enters as $J(q)^2$ is mathematically incorrect and lacks any theoretical justification.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "3461102"}, {"introduction": "Having established the correct theoretical formula, the next step is to translate it into robust computer code. The core equations of WHAM involve sums of exponential terms of the form $\\sum_k n_k \\exp(\\beta f_k - \\beta W_k(s_i))$, which are highly susceptible to numerical overflow and underflow, especially at low temperatures (large $\\beta$) or with strong biasing potentials. This practical exercise, [@problem_id:3461097], guides you through the derivation of the \"log-sum-exp\" technique. Mastering this method is essential for writing stable scientific software, as it allows you to accurately compute the logarithm of such a sum without ever evaluating the large, problematic exponential terms directly.", "problem": "Consider a set of $K$ biased Molecular Dynamics (MD) simulations performed at constant temperature $T$ and inverse thermal energy $\\beta = 1/(k_B T)$, where $k_B$ is the Boltzmann constant. Each simulation $k$ imposes a bias potential $W_k(s)$ on a scalar collective variable $s$. Let $s$ be discretized into bins indexed by $i$, with bin centers $s_i$. The Weighted Histogram Analysis Method (WHAM) combines these simulations to estimate the unbiased probability distribution over $s$. In this context, the WHAM reweighting for bin $i$ involves a denominator that aggregates contributions across simulations, defined by\n$$\nD_i = \\sum_{k=1}^{K} n_k \\exp\\!\\left[\\beta f_k - \\beta W_k(s_i)\\right],\n$$\nwhere $n_k \\geq 1$ is the number of uncorrelated samples contributed by simulation $k$ and $f_k$ are reduced free energy offsets determined by self-consistency within WHAM. For large $\\beta$ or stiff biases $W_k(s)$, direct evaluation of $D_i$ can overflow due to exponential terms with large positive arguments. Starting from the canonical ensemble and the basic properties of exponentials and logarithms (monotonicity of $\\exp(\\cdot)$ and the identity $\\ln(\\exp(x)) = x$), derive a numerically stable expression for $\\ln D_i$ that avoids overflow and underflow by recentering the exponents, without altering $D_i$'s value. Your derivation must introduce only quantities already present in the definition of $D_i$ and use elementary operations available in standard numerical libraries. Provide the final closed-form analytical expression for $\\ln D_i$ expressed solely in terms of $\\beta$, $f_k$, $W_k(s_i)$, and $n_k$. No rounding is required and no physical units should be included in the final answer.", "solution": "The problem asks for a numerically stable expression for the natural logarithm of the Weighted Histogram Analysis Method (WHAM) denominator, $\\ln(D_i)$. The given expression for $D_i$ is:\n$$\nD_i = \\sum_{k=1}^{K} n_k \\exp\\!\\left[\\beta f_k - \\beta W_k(s_i)\\right]\n$$\nHere, $\\beta$ is the inverse thermal energy, $n_k$ is the number of samples from simulation $k$, $f_k$ is the reduced free energy offset for simulation $k$, and $W_k(s_i)$ is the bias potential of simulation $k$ evaluated at the center of bin $i$.\n\nThe challenge arises when the argument of the exponential function, $\\beta f_k - \\beta W_k(s_i)$, is a large positive number for any simulation $k$. In such cases, the term $\\exp[\\beta f_k - \\beta W_k(s_i)]$ can exceed the floating-point representation capabilities of a computer, a condition known as numerical overflow. A direct computation of $D_i$ followed by taking the logarithm would fail.\n\nTo derive a stable expression for $\\ln(D_i)$, we can use a common algebraic technique often referred to as the \"log-sum-exp\" trick. This involves identifying the largest exponent in the sum, factoring it out, and then applying the properties of logarithms. This procedure does not alter the mathematical value of $D_i$ but recasts the calculation into a numerically stable form.\n\nLet's define the exponent for the $k$-th term as $A_k(s_i) = \\beta f_k - \\beta W_k(s_i)$. The expression for $D_i$ becomes:\n$$\nD_i = \\sum_{k=1}^{K} n_k \\exp(A_k(s_i))\n$$\nNow, let's define a constant $C_i$ for each bin $i$ as the maximum value of these exponents across all $K$ simulations:\n$$\nC_i = \\max_{k' \\in \\{1, \\dots, K\\}} \\left\\{ A_{k'}(s_i) \\right\\} = \\max_{k' \\in \\{1, \\dots, K\\}} \\left\\{ \\beta f_{k'} - \\beta W_{k'}(s_i) \\right\\}\n$$\nWe use $k'$ as a dummy index for the maximization to distinguish it from the summation index $k$.\n\nWe can now rewrite the expression for $D_i$ by adding and subtracting $C_i$ inside each exponent. This is a null operation that preserves the value of each term.\n$$\nD_i = \\sum_{k=1}^{K} n_k \\exp\\left[ A_k(s_i) - C_i + C_i \\right]\n$$\nUsing the property of exponentials, $\\exp(a+b) = \\exp(a)\\exp(b)$, we can split the exponential:\n$$\nD_i = \\sum_{k=1}^{K} n_k \\exp\\left[ A_k(s_i) - C_i \\right] \\exp(C_i)\n$$\nSince $\\exp(C_i)$ is a constant with respect to the summation index $k$, it can be factored out of the sum:\n$$\nD_i = \\exp(C_i) \\left( \\sum_{k=1}^{K} n_k \\exp\\left[ A_k(s_i) - C_i \\right] \\right)\n$$\nThis expression is mathematically identical to the original definition of $D_i$. Now, we can take the natural logarithm of both sides to find $\\ln(D_i)$:\n$$\n\\ln(D_i) = \\ln\\left( \\exp(C_i) \\left( \\sum_{k=1}^{K} n_k \\exp\\left[ A_k(s_i) - C_i \\right] \\right) \\right)\n$$\nUsing the property of logarithms, $\\ln(xy) = \\ln(x) + \\ln(y)$, we separate the two factors:\n$$\n\\ln(D_i) = \\ln\\left( \\exp(C_i) \\right) + \\ln\\left( \\sum_{k=1}^{K} n_k \\exp\\left[ A_k(s_i) - C_i \\right] \\right)\n$$\nBy the fundamental identity $\\ln(\\exp(x)) = x$, the first term simplifies to $C_i$:\n$$\n\\ln(D_i) = C_i + \\ln\\left( \\sum_{k=1}^{K} n_k \\exp\\left[ A_k(s_i) - C_i \\right] \\right)\n$$\nThis form is numerically stable. The argument of the exponential function inside the sum is now $A_k(s_i) - C_i = A_k(s_i) - \\max_{k'}\\{A_{k'}(s_i)\\}$. By the definition of the maximum, this difference is always less than or equal to zero for all $k$. Therefore, the value of $\\exp[A_k(s_i) - C_i]$ will be between $0$ and $1$, preventing overflow. The term for which the exponent is maximized will have an adjusted exponent of $0$, resulting in $\\exp(0)=1$, which guarantees that the argument of the final logarithm is at least $1$ (since $n_k \\ge 1$), thus preventing underflow in the logarithm.\n\nTo provide the final expression solely in terms of the initial quantities, we substitute the full expressions for $A_k(s_i)$ and $C_i$. Since $\\beta > 0$, it can be factored out of the maximization:\n$$\nC_i = \\beta \\max_{k' \\in \\{1, \\dots, K\\}} \\left\\{ f_{k'} - W_{k'}(s_i) \\right\\}\n$$\nSubstituting this and $A_k(s_i)$ into our derived expression yields the final answer:\n$$\n\\ln(D_i) = \\beta \\max_{k' \\in \\{1, \\dots, K\\}} \\left\\{ f_{k'} - W_{k'}(s_i) \\right\\} + \\ln\\left( \\sum_{k=1}^{K} n_k \\exp\\left[ \\beta(f_k - W_k(s_i)) - \\beta \\max_{k'' \\in \\{1, \\dots, K\\}} \\left\\{ f_{k''} - W_{k''}(s_i) \\right\\} \\right] \\right)\n$$\nThis expression provides a numerically robust method for calculating $\\ln(D_i)$, avoiding overflow and underflow issues inherent in the direct evaluation of the original formula.", "answer": "$$\n\\boxed{\\beta \\max_{k' \\in \\{1, \\dots, K\\}} \\left( f_{k'} - W_{k'}(s_i) \\right) + \\ln\\left( \\sum_{k=1}^{K} n_k \\exp\\left( \\beta \\left( f_k - W_k(s_i) \\right) - \\beta \\max_{k'' \\in \\{1, \\dots, K\\}} \\left( f_{k''} - W_{k''}(s_i) \\right) \\right) \\right)}\n$$", "id": "3461097"}, {"introduction": "Real-world molecular systems often feature collective variables that are inherently periodic, such as the dihedral angles that govern molecular conformations. Applying standard analysis techniques without accounting for this topology can introduce severe artifacts, like artificial energy barriers at the periodic boundary. This final practice, [@problem_id:3461108], focuses on the correct procedures for handling periodic systems in the context of umbrella sampling and WHAM. It will test your ability to apply the correct distance metrics (the minimum-image convention) for bias potentials and to ensure that all stages of the analysis, from histogramming to data smoothing, properly respect the circular nature of the coordinate.", "problem": "A one-dimensional collective variable $s$ measuring a torsional angle is strictly periodic with period $L$, so that physically distinct states correspond to $s \\in [0,L)$ and the distance between two values is defined on the circle. In umbrella sampling, window $i$ applies a harmonic bias $w_i(s)$ centered at $s_{0,i}$, and configurations are sampled according to the canonical ensemble at inverse temperature $\\beta$ with total potential energy $U(s) + w_i(s)$. The canonically biased probability density in window $i$ is therefore proportional to $\\exp\\!\\left[-\\beta\\left(U(s)+w_i(s)\\right)\\right]$, and the unbiased target density $p(s)$ is proportional to $\\exp\\!\\left[-\\beta U(s)\\right]$. Weighted histogram analysis seeks to reconstruct $p(s)$ from many windows by reweighting each sampled configuration’s contribution to bins and then estimating the free energy $F(s)$ via $F(s) = -k_{\\mathrm{B}} T \\ln p(s)$ up to an additive constant, where $k_{\\mathrm{B}}$ is the Boltzmann constant and $T$ is the absolute temperature.\n\nConsider umbrella windows with centers $s_{0,i}$ placed near the periodic boundary, for example $s_{0,i} \\approx L - \\Delta$ for small $\\Delta > 0$. During sampling, many configurations have $s$ values that cross the boundary, e.g., values slightly larger than $L$ or slightly negative when expressed continuously before wrapping. You are tasked with deciding, from first principles, how contributions from such samples should be counted into histogram bins that straddle the boundary and how to prevent artificial discontinuities in the reconstructed $F(s)$.\n\nWhich of the following procedures are correct in ensuring scientifically consistent counting across the boundary and preventing artificial discontinuities in $F(s)$?\n\nA. Build the histogram on $[0,L)$ without any wrapping, assign any sample with $s>L$ to a bin beyond $L$, and discard these contributions during normalization to avoid double counting at the boundary.\n\nB. Map each sampled $s$ to its representative $s' \\in [0,L)$ via $s' = s - L \\left\\lfloor s/L \\right\\rfloor$, assign histogram bins using $s'$, and compute the harmonic bias with the minimum-image distance $d(s',s_{0,i}) = \\min_{n \\in \\mathbb{Z}} \\left| s' - s_{0,i} + nL \\right|$ so that the reweighting factor depends smoothly across the boundary.\n\nC. To enforce continuity, duplicate any sample whose $s$ lies within a small threshold $\\delta$ of either boundary into both the first and last bins, and average its weight across the two to reduce boundary artifacts.\n\nD. When estimating $F(s)$ from the reweighted histogram, impose the periodicity constraint $F(0) = F(L)$ explicitly and, if smoothing is applied to $p(s)$ or $F(s)$, use kernels defined on the circle with geodesic distance on the manifold (i.e., periodic kernels) rather than Euclidean kernels on an interval.\n\nE. Shift any window center $s_{0,i}$ near $L$ by $\\pm L$ during analysis so that $|s - s_{0,i}|$ appears small without using a minimum-image convention, and leave all sampled $s$ values unwrapped so that the histogram has a clear boundary at $s=0$ and $s=L$.\n\nSelect all that apply.", "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   **System**: A one-dimensional collective variable $s$ that is strictly periodic with period $L$.\n-   **State Space**: Physically distinct states correspond to $s \\in [0,L)$. The distance metric is defined on a circle.\n-   **Simulation Method**: Umbrella sampling is used.\n-   **Bias Potential**: Window $i$ applies a harmonic bias $w_i(s)$ centered at $s_{0,i}$.\n-   **Ensemble**: Sampling is performed in the canonical ensemble at inverse temperature $\\beta = (k_{\\mathrm{B}} T)^{-1}$.\n-   **Biased Probability Density**: In window $i$, the probability density is proportional to $\\exp[-\\beta(U(s)+w_i(s))]$.\n-   **Unbiased Probability Density**: The target quantity is $p(s) \\propto \\exp[-\\beta U(s)]$.\n-   **Free Energy**: The free energy is defined as $F(s) = -k_{\\mathrm{B}} T \\ln p(s)$ (up to an additive constant).\n-   **Analysis Method**: Weighted histogram analysis (WHAM) is used to reconstruct $p(s)$.\n-   **Challenge**: For windows with centers $s_{0,i}$ near the periodic boundary (e.g., $s_{0,i} \\approx L$), samples cross the boundary (e.g., $s > L$ or $s < 0$). The task is to identify correct procedures for handling these samples to ensure consistent counting and prevent artificial discontinuities in $F(s)$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is firmly rooted in the principles of statistical mechanics and computational physics. Umbrella sampling, the weighted histogram analysis method (WHAM), the concept of a periodic collective variable (like a dihedral angle), and the definition of free energy are all standard and accurately described. The challenge of handling periodic boundary conditions is a real and important issue in molecular simulations.\n-   **Well-Posed**: The problem is well-defined. It asks for the correct procedures to handle a specific, common technical challenge in data analysis for molecular simulations. The goal is clear, and the context is specified with sufficient detail.\n-   **Objective**: The problem statement is expressed in precise, objective, and standard scientific terminology. There are no subjective or ambiguous statements.\n\nThe problem does not exhibit any of the invalidity flaws. It is a standard, valid problem in the field of computational molecular science.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation of Principles for Periodic Systems\n\nThe core of the problem lies in correctly treating a system whose coordinate $s$ is periodic. The space of the collective variable is not the real line $\\mathbb{R}$, but a circle $S^1$ of circumference $L$. All physical quantities and mathematical operations must respect this topology.\n\n1.  **Topological Equivalence**: Any two points $s_1$ and $s_2$ are physically identical if $s_1 = s_2 + nL$ for some integer $n \\in \\mathbb{Z}$. Consequently, any single-valued function of the physical state, such as the unbiased potential energy $U(s)$ and the free energy $F(s)$, must be periodic with period $L$: $U(s) = U(s+L)$ and $F(s) = F(s+L)$.\n\n2.  **Distance Metric**: The distance between two points $s_a$ and $s_b$ on a circle is the shortest arc length between them. This is known as the minimum-image distance, given by $d(s_a, s_b) = \\min_{n \\in \\mathbb{Z}} |s_a - s_b + nL|$. A harmonic bias potential, $w_i(s) = \\frac{1}{2}k_i (\\text{distance from } s_{0,i})^2$, must use this metric to be a smooth and physically meaningful function on the circle: $w_i(s) = \\frac{1}{2}k_i [d(s, s_{0,i})]^2$. Using the simple Euclidean distance $|s - s_{0,i}|$ would create an unphysical energy cusp at the periodic boundary for windows centered near it.\n\n3.  **Histogram Binning**: A simulation trajectory yields a sequence of $s$ values that are typically \"unwrapped\" and may fall outside the fundamental domain $[0, L)$. To construct a histogram of the probability density $p(s)$ on the circle, each sampled value of $s$ must be mapped to its unique representative $s'$ in the chosen fundamental domain, e.g., $s' \\in [0, L)$. The correct mapping is $s' = s \\pmod L$, which can be implemented as $s' = s - L \\lfloor s/L \\rfloor$. The histogram is populated using these wrapped coordinates $s'$.\n\n4.  **Continuity and Post-processing**: The resulting free energy profile $F(s)$ must be periodic, meaning $F(0) = F(L)$. While a perfect analysis of infinite data would guarantee this, finite data and numerical methods may introduce a small mismatch. It is a valid step to enforce this periodicity. Furthermore, if any smoothing is applied to the data (e.g., to $p(s)$ or $F(s)$), the smoothing kernel must also respect the circular topology. This requires a periodic kernel (e.g., a wrapped Gaussian), which correctly handles points near the boundary (e.g., at $s=\\epsilon$) by acknowledging their proximity to points at the other end of the interval (e.g., at $s=L-\\epsilon$).\n\nBased on these first principles, we can now evaluate each option.\n\n### Option-by-Option Analysis\n\n**A. Build the histogram on $[0,L)$ without any wrapping, assign any sample with $s>L$ to a bin beyond $L$, and discard these contributions during normalization to avoid double counting at the boundary.**\n\nThis procedure is fundamentally flawed. A sample at $s = L + \\epsilon$ is physically identical to a sample at $s=\\epsilon$. Discarding the sample at $s=L+\\epsilon$ means throwing away valid statistical information. This artificially depletes the sampling of bins near $s=0$, leading to an incorrect probability density and a corresponding artificial barrier in the free energy profile. This approach treats the periodic variable as if it were confined to a box, which contradicts the problem definition.\n**Verdict: Incorrect.**\n\n**B. Map each sampled $s$ to its representative $s' \\in [0,L)$ via $s' = s - L \\left\\lfloor s/L \\right\\rfloor$, assign histogram bins using $s'$, and compute the harmonic bias with the minimum-image distance $d(s',s_{0,i}) = \\min_{n \\in \\mathbb{Z}} \\left| s' - s_{0,i} + nL \\right|$ so that the reweighting factor depends smoothly across the boundary.**\n\nThis procedure correctly addresses the core issues.\n1.  The mapping $s' = s - L \\lfloor s/L \\rfloor$ correctly folds all sampled coordinates into the fundamental domain $[0,L)$, consistent with Principle 3. This ensures every sample is counted in its physically correct location.\n2.  Using the minimum-image distance $d(s', s_{0,i})$ to define the harmonic bias potential is crucial, as explained in Principle 2. This ensures the bias energy $w_i(s)$ is a continuous and smooth function on the circle, which is necessary for the reweighting factor $\\exp[\\beta w_i(s)]$ to be well-behaved across the periodic boundary. This prevents artificial energy jumps and ensures the physics of the biased simulation is correct.\nThis option describes the standard, correct implementation for umbrella sampling with periodic variables.\n**Verdict: Correct.**\n\n**C. To enforce continuity, duplicate any sample whose $s$ lies within a small threshold $\\delta$ of either boundary into both the first and last bins, and average its weight across the two to reduce boundary artifacts.**\n\nThis is an ad-hoc, heuristic procedure that is not founded on rigorous statistical principles. It violates the basic rule of histogramming, where each sample contributes to exactly one bin. Duplicating and averaging sample contributions is a form of data manipulation that distorts the underlying probability distribution. It attempts to cosmetically smooth a boundary artifact that arises from an incorrect treatment, rather than solving the problem at its root by respecting the system's topology. The correct way to achieve continuity is by using a periodic representation from the start.\n**Verdict: Incorrect.**\n\n**D. When estimating $F(s)$ from the reweighted histogram, impose the periodicity constraint $F(0) = F(L)$ explicitly and, if smoothing is applied to $p(s)$ or $F(s)$, use kernels defined on the circle with geodesic distance on the manifold (i.e., periodic kernels) rather than Euclidean kernels on an interval.**\n\nThis option describes correct and often necessary post-processing steps, consistent with Principle 4.\n1.  The true free energy $F(s)$ must be periodic. Imposing the constraint $F(0) = F(L)$ ensures the final result is physically meaningful, correcting for minor discrepancies that can arise from finite sampling.\n2.  If smoothing is performed, using a periodic kernel is essential. A standard Euclidean kernel would treat the points at $s=0$ and $s=L$ as far apart, creating smoothing artifacts at the boundaries. A periodic kernel correctly recognizes that the domain is circular and that $s=0$ is adjacent to $s=L$, thus preserving the continuity and periodicity of the profile.\nBoth practices are sound and consistent with a rigorous treatment of periodic data.\n**Verdict: Correct.**\n\n**E. Shift any window center $s_{0,i}$ near $L$ by $\\pm L$ during analysis so that $|s - s_{0,i}|$ appears small without using a minimum-image convention, and leave all sampled $s$ values unwrapped so that the histogram has a clear boundary at $s=0$ and $s=L$.**\n\nThis is an incorrect and cumbersome workaround that fails to be a general solution. Leaving $s$ values unwrapped constructs a histogram on the real line, which is the wrong topology. Shifting the center $s_{0,i}$ is a \"trick\" to make a simple Euclidean distance calculation $|s-s_{0,i}|$ give the correct minimum-image distance, but it only works for samples $s$ that are close to the shifted center. It does not work for all possible $s$ values and is not equivalent to a proper minimum-image calculation. For example, if $s_{0,i}=0.9L$ is shifted to $-0.1L$, a sample at $s=0.4L$ would have an incorrectly calculated large distance to the center. The robust method is to always compute the minimum-image distance. This option explicitly avoids the correct convention and also fails to bin the data correctly.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{BD}$$", "id": "3461108"}]}