{"hands_on_practices": [{"introduction": "A proposed reaction coordinate is only as good as its ability to predict the outcome of a reaction. This practice provides a direct, hands-on implementation of the most fundamental validation test: checking if the level sets of a candidate coordinate are true isocommittor surfaces [@problem_id:3402833]. By simulating trajectories and analyzing the resulting committor values, you will learn to computationally verify whether a coordinate truly captures the system's progress from reactants to products.", "problem": "You are tasked with building a principled cross-check for validating a proposed one-dimensional reaction coordinate in Molecular Dynamics (MD), using committor analysis based on the fundamental stochastic dynamics of a molecular system. Consider a two-dimensional configuration space with coordinates $x$ and $y$. The dynamics are modeled in dimensionless units by the overdamped Langevin equation (also known as the Smoluchowski dynamics), which for a potential energy $U(x,y)$ reads\n$$\ndx_t = -\\mu \\,\\partial_x U(x_t,y_t)\\, dt + \\sqrt{2 D \\, dt}\\, \\eta_x(t), \\quad dy_t = -\\mu \\,\\partial_y U(x_t,y_t)\\, dt + \\sqrt{2 D \\, dt}\\, \\eta_y(t),\n$$\nwhere $\\mu$ is the mobility, $D$ is the diffusion coefficient, and $\\eta_x(t)$, $\\eta_y(t)$ are independent standard normal random variables at each time step. The committor function $q(x_0,y_0)$ is defined as the probability that a trajectory starting at $(x_0,y_0)$ reaches basin $B$ before basin $A$. Basins are defined here by threshold conditions on $x$, specifically basin $A$ is the set of configurations with $x \\le x_A$, and basin $B$ is the set with $x \\ge x_B$, with $x_A  0  x_B$. A proposed reaction coordinate is a scalar function $\\xi(x,y)$; “isosurfaces” are the sets of points satisfying $\\xi(x,y)=c$ for a constant $c$.\n\nYour program must implement the following validation procedure:\n1. Use the overdamped Langevin dynamics as the fundamental evolution law (no inertial term). Integrate the stochastic differential equation using the Euler–Maruyama method with the specified parameters in dimensionless units.\n2. For each proposed reaction coordinate $\\xi(x,y)$ and each specified isosurface level $c$, construct a set of initial conditions on the isosurface $\\xi(x,y)=c$ by varying $y$ uniformly in a symmetric interval and determining the corresponding $x$ that satisfies the isosurface condition.\n3. For each initial condition $(x_0,y_0)$ on the isosurface, approximate the committor $q(x_0,y_0)$ by “shooting”: launch multiple independent overdamped Langevin trajectories from $(x_0,y_0)$, evolve each until it hits either basin $A$ or basin $B$ or a maximum number of steps, and record whether basin $B$ was reached before basin $A$. The estimated committor $q(x_0,y_0)$ is the fraction of trajectories that reach basin $B$ first.\n4. At fixed isosurface level $c$, aggregate the set $\\{q(x_0,y_0)\\}$ over the different initial conditions and test whether the empirical distribution of $q$ is unimodal and narrow. Conclude that a reaction coordinate is insufficient if, for any tested $c$, the distribution of $q$ at fixed $c$ fails unimodality or is not narrow.\n5. Unimodality must be assessed by estimating a smooth density of the empirical $q$ values and counting the number of distinct modes. Narrowness must be assessed by comparing the empirical standard deviation of the $q$ values against a threshold.\n\nThe potential energy is specified as\n$$\nU(x,y) = (x^2 - 1)^2 + k \\, y^2,\n$$\nwith gradient\n$$\n\\partial_x U(x,y) = 4x(x^2 - 1), \\quad \\partial_y U(x,y) = 2k y.\n$$\nThis yields a symmetric double well in $x$ with minima near $x=\\pm 1$ and a weakly confining quadratic term in $y$. Work entirely in dimensionless units. The basins are defined by $x_A=-0.9$ and $x_B=0.9$. The mobility is $\\mu=1$, the diffusion coefficient is $D=0.4$, and the potential parameter is $k=0.1$. Use a time step $dt=0.005$ and a maximum number of steps $N_{\\text{max}}=1500$ per trajectory. For each isosurface and each initial condition, use $N_{\\text{shoot}}=30$ shooting trajectories. For constructing isosurface initial conditions, sample $N_{\\text{iso}}=21$ values of $y$ uniformly in the interval $[-Y_{\\max},Y_{\\max}]$ with $Y_{\\max}=1$. Treat hitting of a basin as absorbing: once a trajectory satisfies $x\\le x_A$ or $x\\ge x_B$, it is terminated and considered to have reached $A$ or $B$ respectively. If a trajectory does not reach either basin within $N_{\\text{max}}$ steps, count it as neither; for committor estimation, only the recorded first hits are counted toward the fraction. If all trajectories for a given initial condition fail to reach either basin, the committor estimate for that initial condition should be set to $0.5$ by symmetry.\n\nYou must validate two candidate reaction coordinates:\n1. $\\xi_1(x,y) = x$.\n2. $\\xi_2(x,y) = x + \\alpha y$ with $\\alpha = 1.5$.\n\nThe isosurface levels to be tested are $c \\in \\{-0.5, 0.0, 0.5\\}$. For each candidate $\\xi$ and each $c$, compute the set of committor estimates $\\{q(x_0,y_0)\\}$ over the constructed isosurface initial conditions, then:\n- Estimate a smooth density over $q \\in [0,1]$ and determine whether there is at most one mode (unimodality).\n- Compute the empirical standard deviation $\\sigma_q$ of the $\\{q\\}$ values and determine whether it is at most the narrowness threshold $\\sigma_{\\text{th}}=0.12$.\n\nA reaction coordinate $\\xi$ is considered “sufficient” if, for all tested $c$, the distribution of $q$ at fixed $c$ is unimodal and narrow. Otherwise, conclude “insufficient.”\n\nYour program must produce a single line of output containing the sufficiency verdicts for $\\xi_1$ and $\\xi_2$ respectively, as a comma-separated Python list of booleans enclosed in square brackets, for example $[{\\rm True},{\\rm False}]$. No other output is permitted.\n\nTest Suite Parameters to Implement Exactly:\n- Potential parameter: $k=0.1$.\n- Basins: $x_A=-0.9$, $x_B=0.9$.\n- Dynamics: $\\mu=1$, $D=0.4$, $dt=0.005$, $N_{\\text{max}}=1500$.\n- Shooting: $N_{\\text{shoot}}=30$ trajectories per initial condition.\n- Isosurface sampling: $N_{\\text{iso}}=21$ points with $y$ uniformly spaced in $[-1,1]$, $Y_{\\max}=1$.\n- Reaction coordinates: $\\xi_1(x,y)=x$, $\\xi_2(x,y)=x+\\alpha y$ with $\\alpha=1.5$.\n- Isosurface levels: $c=-0.5, 0.0, 0.5$.\n- Unimodality detection: use a smooth density estimate over $q \\in [0,1]$ and count modes.\n- Narrowness threshold: $\\sigma_{\\text{th}}=0.12$.\n- Units: report and compute in dimensionless units.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with booleans in Python syntax, in the order $[\\text{sufficient}(\\xi_1),\\text{sufficient}(\\xi_2)]$.", "solution": "This problem requires implementing a computational workflow to validate reaction coordinates using committor analysis. The solution involves several key components: a function to simulate the overdamped Langevin dynamics using the Euler-Maruyama method; a \"shooting\" function that estimates the committor value for a given starting point by running multiple trajectories; a procedure to generate initial conditions on the specified isosurfaces; and finally, a main function that orchestrates the validation by calculating committor distributions for each reaction coordinate and isosurface, and then applying the specified unimodality and narrowness tests. The core of the validation lies in checking whether the committor distribution on a proposed transition surface (an isosurface of the candidate RC) is sharply peaked, which indicates that the RC value is a good predictor of the reaction outcome. The code implements this logic for the two candidate RCs and outputs the sufficiency verdict for each.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\ndef solve():\n    \"\"\"\n    Main function to validate reaction coordinates using committor analysis.\n    \"\"\"\n    # Define problem parameters\n    K = 0.1\n    X_A = -0.9\n    X_B = 0.9\n    MU = 1.0\n    D = 0.4\n    DT = 0.005\n    N_MAX = 1500\n    N_SHOOT = 30\n    N_ISO = 21\n    Y_MAX = 1.0\n    ALPHA = 1.5\n    SIGMA_TH = 0.12\n    ISO_LEVELS = [-0.5, 0.0, 0.5]\n    \n    # Pre-calculate noise standard deviation for efficiency\n    NOISE_STD = np.sqrt(2 * D * DT)\n\n    def grad_U(x, y):\n        \"\"\"Computes the gradient of the potential energy U(x,y).\"\"\"\n        grad_x = 4.0 * x * (x**2 - 1.0)\n        grad_y = 2.0 * K * y\n        return grad_x, grad_y\n\n    def run_trajectory(x0, y0):\n        \"\"\"\n        Simulates a single overdamped Langevin trajectory.\n        Returns 'A' or 'B' if a basin is reached, otherwise 'None'.\n        \"\"\"\n        x, y = x0, y0\n        # Generate all random numbers at once for vectorization\n        noise_x = np.random.randn(N_MAX) * NOISE_STD\n        noise_y = np.random.randn(N_MAX) * NOISE_STD\n        \n        for i in range(N_MAX):\n            grad_x, grad_y = grad_U(x, y)\n            x += -MU * grad_x * DT + noise_x[i]\n            y += -MU * grad_y * DT + noise_y[i]\n            \n            if x = X_A:\n                return 'A'\n            if x = X_B:\n                return 'B'\n        return 'None'\n\n    def calculate_committor(x0, y0):\n        \"\"\"\n        Estimates the committor value q(x0, y0) by shooting multiple trajectories.\n        \"\"\"\n        a_hits = 0\n        b_hits = 0\n        for _ in range(N_SHOOT):\n            result = run_trajectory(x0, y0)\n            if result == 'A':\n                a_hits += 1\n            elif result == 'B':\n                b_hits += 1\n        \n        total_hits = a_hits + b_hits\n        if total_hits == 0:\n            return 0.5\n        return b_hits / total_hits\n\n    def count_modes(data, grid_size=100):\n        \"\"\"\n        Estimates the number of modes in a dataset using KDE.\n        At most one mode is considered 'unimodal'.\n        \"\"\"\n        if len(data)  2 or np.allclose(data, data[0]):\n            return 1\n\n        try:\n            # Use default bandwidth selection (Scott's rule)\n            kde = gaussian_kde(data)\n            grid = np.linspace(0, 1, grid_size)\n            density = kde.evaluate(grid)\n        except (np.linalg.LinAlgError, ValueError):\n            return 1  # Fallback for singular matrix cases\n\n        # Find local maxima to count modes\n        modes = 0\n        # Check for internal modes (peaks)\n        for i in range(1, len(density) - 1):\n            if density[i]  density[i-1] and density[i]  density[i+1]:\n                modes += 1\n        \n        # Check for modes at the boundaries\n        if density[0]  density[1]:\n            modes += 1\n        if density[-1]  density[-2]:\n            modes += 1\n            \n        # A montonic distribution will have 1 mode at a boundary.\n        # A flat distribution will have 0 modes. We consider 0 modes as passing the\n        # \"at most one mode\" test.\n        if modes == 0 and not np.allclose(density, density[0]):\n             # This means monotonic or a single very broad peak, treat as unimodal\n             return 1\n        \n        return modes\n\n    def test_rc (rc_x_generator, levels_to_test):\n        \"\"\"\n        Tests a reaction coordinate for sufficiency across specified isosurface levels.\n        \"\"\"\n        for c in levels_to_test:\n            y_values = np.linspace(-Y_MAX, Y_MAX, N_ISO)\n            initial_conditions = [(rc_x_generator(y, c), y) for y in y_values]\n            \n            q_values = [calculate_committor(x0, y0) for x0, y0 in initial_conditions]\n            \n            q_std = np.std(q_values)\n            is_narrow = (q_std = SIGMA_TH)\n            \n            num_modes = count_modes(q_values)\n            is_unimodal = (num_modes = 1)\n            \n            if not (is_unimodal and is_narrow):\n                return False  # Fails early if any level is insufficient\n        return True\n\n    # Define generators for x-coordinates on isosurfaces\n    rc1_gen = lambda y, c: c\n    rc2_gen = lambda y, c: c - ALPHA * y\n    \n    # Perform validation for both reaction coordinates\n    results = [\n        test_rc(rc1_gen, ISO_LEVELS),\n        test_rc(rc2_gen, ISO_LEVELS)\n    ]\n    \n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3402833"}, {"introduction": "When a simple reaction coordinate fails the isocommittor test, it often signifies the presence of other \"hidden\" slow variables that are crucial to the reaction but absent from the coordinate's definition. This exercise introduces a powerful statistical framework to diagnose such deficiencies [@problem_id:3402799]. You will learn to use mixture models and information criteria to detect if the committor data, at a fixed reaction coordinate value, is better explained by multiple underlying states, thereby revealing the inadequacy of the candidate coordinate.", "problem": "Consider a system undergoing rare-event transitions between two metastable basins, denoted as set $A$ and set $B$, with a possible intermediate metastable sub-ensemble labeled by a discrete hidden state $I \\in \\{0,1\\}$. The committor function $q(\\mathbf{x})$ is defined as the probability that a trajectory initiated at configuration $\\mathbf{x}$ reaches set $B$ before set $A$. A proposed one-dimensional reaction coordinate (RC) is a measurable function $\\xi: \\mathbf{x} \\mapsto \\mathbb{R}$. An ideal RC is such that $q(\\mathbf{x})$ depends only on $\\xi(\\mathbf{x})$, i.e., $q(\\mathbf{x}) = \\tilde{q}(\\xi(\\mathbf{x}))$, without dependence on any hidden variable.\n\nYou are to formulate a principled statistical test for whether $q(\\mathbf{x})$ conditioned on $\\xi(\\mathbf{x})$ is independent of a hidden state $I$, and, if dependence is detected, to perform an RC augmentation with a discrete latent variable and validate that the augmentation removes the hidden dependence. Your program must be self-contained and perform the following tasks on synthetic datasets that mimic Molecular Dynamics (MD) committor sampling.\n\nFundamental base and assumptions:\n- The committor is defined by $q(\\mathbf{x}) = \\mathbb{P}_{\\mathbf{x}}(\\tau_B  \\tau_A)$, where $\\tau_A$ and $\\tau_B$ are first-hitting times to sets $A$ and $B$ respectively, under dynamics that are Markovian in the full state $\\mathbf{x}$.\n- Given a reaction coordinate $\\xi(\\mathbf{x}) \\in \\mathbb{R}$, a necessary condition for $\\xi$ to be a good RC is that, for a fixed value of $\\xi$, the distribution of observed outcomes of short trajectories (reaching set $B$ before $A$ denoted by $y=1$, or reaching set $A$ before $B$ denoted by $y=0$) is Bernoulli with a parameter that depends only on $\\xi$ and not on any hidden state $I$.\n- If a hidden state $I$ is relevant at fixed $\\xi$, then the conditional distribution of $y \\in \\{0,1\\}$ given $\\xi$ may be described by a mixture of Bernoulli distributions with mixture weights reflecting $\\mathbb{P}(I=1 \\mid \\xi)$.\n\nSynthetic data generation model:\n- Let $\\xi \\in [-1,1]$ be sampled independently and identically distributed (i.i.d.) uniformly.\n- Let $I \\in \\{0,1\\}$ be a hidden state with $\\mathbb{P}(I=1) = 1/2$.\n- Define a base logit $L_{\\text{base}}(\\xi) = \\alpha \\,\\xi + b$, with $\\alpha \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$.\n- Define a localized modulation $g(\\xi) = \\exp\\left(-\\xi^2 / (2 s^2)\\right)$ with width parameter $s  0$.\n- For a hidden-state contrast $d \\ge 0$, define the hidden-state-dependent logit\n$$\nL_I(\\xi) = L_{\\text{base}}(\\xi) + (2 I - 1)\\, d\\, g(\\xi),\n$$\nand the corresponding Bernoulli parameter\n$$\nq_I(\\xi) = \\frac{1}{1 + \\exp\\left(- L_I(\\xi)\\right)}.\n$$\n- For each sample, generate an outcome $y \\sim \\text{Bernoulli}(q_I(\\xi))$ independently.\n\nDetection, augmentation, and validation protocol:\n1. Partition the interval $[-1,1]$ into $B$ bins of equal width, with edges chosen so that each bin $b$ contains $n_b$ samples, where $n_b$ is the number of observations with $\\xi$ in the bin. Only bins with $n_b \\ge n_{\\min}$ are considered.\n2. For each such bin $b$, test model $H_0$ (single-component Bernoulli) against model $H_1$ (two-component Bernoulli mixture with a discrete latent state) using maximum likelihood and the Bayesian Information Criterion (BIC). Specifically:\n   - Under $H_0$, the data $\\{y_i\\}_{i=1}^{n_b}$ are i.i.d. $\\text{Bernoulli}(p)$ with parameter $p \\in (0,1)$.\n   - Under $H_1$, the data are i.i.d. from the mixture $\\pi \\,\\text{Bernoulli}(p_1) + (1-\\pi)\\,\\text{Bernoulli}(p_0)$ with parameters $\\pi \\in (0,1)$, $p_0 \\in (0,1)$, and $p_1 \\in (0,1)$.\n   - Let $\\ell_0$ be the maximized log-likelihood under $H_0$ and $\\ell_1$ be the maximized log-likelihood under $H_1$. Define\n     $$\n     \\mathrm{BIC}_0 = -2 \\ell_0 + k_0 \\log n_b, \\quad \\mathrm{BIC}_1 = -2 \\ell_1 + k_1 \\log n_b,\n     $$\n     with $k_0 = 1$ and $k_1 = 3$. Let $\\Delta \\mathrm{BIC}_b = \\mathrm{BIC}_0 - \\mathrm{BIC}_1$. Positive $\\Delta \\mathrm{BIC}_b$ indicates that the mixture is favored.\n3. Aggregate pre-augmentation evidence by taking the maximum improvement $\\Delta \\mathrm{BIC}_{\\max}^{\\text{pre}} = \\max_b \\Delta \\mathrm{BIC}_b$ across bins considered.\n4. If $\\Delta \\mathrm{BIC}_{\\max}^{\\text{pre}}$ exceeds a threshold $T_{\\text{pre}}$, declare that dependence on a hidden state is detected. In that case, augment the RC by introducing an inferred discrete latent label $\\hat{I}$ as follows:\n   - For each bin $b$, fit the mixture model under $H_1$ by maximum likelihood using the Expectation-Maximization (EM) algorithm and compute posterior responsibilities for each sample in the bin. Assign a hard label $\\hat{I} \\in \\{0,1\\}$ by maximum responsibility.\n   - Consider the augmented RC $(\\xi, \\hat{I})$ that explicitly includes the latent state prediction.\n5. Validate the augmentation by repeating the BIC comparison within each bin $b$ separately for each group $\\hat{I}=0$ and $\\hat{I}=1$. Aggregate post-augmentation evidence by $\\Delta \\mathrm{BIC}_{\\max}^{\\text{post}} = \\max_{b,\\hat{I}} \\Delta \\mathrm{BIC}_{b,\\hat{I}}$.\n6. If $\\Delta \\mathrm{BIC}_{\\max}^{\\text{pre}} \\ge T_{\\text{pre}}$ and $\\Delta \\mathrm{BIC}_{\\max}^{\\text{post}}  T_{\\text{post}}$, then declare that augmentation successfully removed hidden-state dependence.\n\nImplementation constraints:\n- Use maximum likelihood for both $H_0$ and $H_1$. For $H_1$, use the Expectation-Maximization (EM) algorithm with multiple random restarts to mitigate local maxima. Maintain numerical stability by constraining Bernoulli parameters to $[\\varepsilon,1-\\varepsilon]$ with $\\varepsilon$ a small positive number.\n- Use thresholds $T_{\\text{pre}} = 6.0$, $T_{\\text{post}} = 2.0$, number of bins $B$ as specified per test case, and a minimum bin count $n_{\\min} = 50$.\n\nYour program must:\n- Generate three synthetic datasets using the model above, with the following parameter tuples $(N, \\alpha, b, d, s, B, \\text{seed})$:\n  1. $(N=4000, \\alpha=4.0, b=0.0, d=2.0, s=0.25, B=6, \\text{seed}=1)$,\n  2. $(N=4000, \\alpha=4.0, b=0.0, d=0.0, s=0.25, B=6, \\text{seed}=2)$,\n  3. $(N=600, \\alpha=4.0, b=0.0, d=0.5, s=0.25, B=6, \\text{seed}=3)$.\n- For each dataset, perform the detection, augmentation, and validation protocol described above with $T_{\\text{pre}} = 6.0$, $T_{\\text{post}} = 2.0$, and $n_{\\min} = 50$.\n- Produce as the final result for each dataset a single integer:\n  - Output $\\;1\\;$ if hidden-state dependence is detected pre-augmentation and successfully removed post-augmentation according to the thresholds,\n  - Output $\\;0\\;$ otherwise.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[$r_1,r_2,r_3$]\"), where $r_i$ is the integer result for dataset $i$. No physical units are involved in this problem, and all probabilities are real numbers in $[0,1]$ without units. Angles are not used. Percentages must not be used; use decimal numbers instead.", "solution": "The solution to this problem is a statistical protocol designed to detect and account for hidden variables that a simple reaction coordinate might miss. The core idea is to model the committor outcomes (ending in state A or B) at a fixed reaction coordinate value. If the coordinate is sufficient, the outcomes should follow a simple Bernoulli distribution. If a hidden variable is present, the outcomes are better described by a mixture of Bernoulli distributions.\n\nThe protocol proceeds in three steps:\n1.  **Detection:** We use the Bayesian Information Criterion (BIC) to compare the single-Bernoulli model (null hypothesis) against a two-component mixture model (alternative hypothesis) for data binned by the reaction coordinate value. A significant preference for the mixture model indicates the presence of a hidden variable.\n2.  **Augmentation:** If a hidden variable is detected, the reaction coordinate is augmented. For each data point, we use the fitted mixture model to infer which of the two hidden states it most likely belongs to, creating a new discrete label. The augmented RC is now a pair consisting of the original coordinate and this new label.\n3.  **Validation:** The statistical test is repeated on the data, now grouped by the augmented RC. If the mixture model is no longer preferred within these new subgroups, the augmentation is considered successful, having created a more complete description of the reaction.\n\nThe Python code implements this entire workflow, including the Expectation-Maximization (EM) algorithm for fitting the mixture model, and applies the full detection-augmentation-validation logic to each of the specified test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit, logsumexp\n\n# Define problem constants\nEPS = 1e-9\nN_RESTARTS = 10\nMAX_EM_ITER = 100\nEM_TOL = 1e-6\nT_PRE = 6.0\nT_POST = 2.0\nN_MIN = 50\n\ndef generate_data(N, alpha, b, d, s, rng):\n    \"\"\"Generates synthetic data based on the problem specification.\"\"\"\n    xi = rng.uniform(-1, 1, N)\n    I = rng.integers(0, 2, size=N)\n    \n    l_base = alpha * xi + b\n    g = np.exp(-xi**2 / (2 * s**2))\n    l_I = l_base + (2 * I - 1) * d * g\n    \n    q_I = expit(l_I)\n    \n    y = rng.binomial(1, q_I)\n    return xi, y\n\ndef fit_mixture_em(y_data, rng):\n    \"\"\"\n    Fits a 2-component Bernoulli mixture model using Expectation-Maximization.\n    \"\"\"\n    n = len(y_data)\n    best_log_L = -np.inf\n    best_params = (0.5, 0.25, 0.75)\n    best_responsibilities = np.full(n, 0.5)\n\n    for _ in range(N_RESTARTS):\n        # Initialize by random partition\n        perm = rng.permutation(n)\n        mid = n // 2\n        y_group0, y_group1 = y_data[perm[:mid]], y_data[perm[mid:]]\n        \n        pi = 0.5\n        p0 = np.mean(y_group0) if mid  0 else 0.5\n        p1 = np.mean(y_group1) if (n - mid)  0 else 0.5\n        \n        if p0  p1: p0, p1 = p1, p0\n        \n        pi = np.clip(pi, EPS, 1 - EPS)\n        p0 = np.clip(p0, EPS, 1 - EPS)\n        p1 = np.clip(p1, EPS, 1 - EPS)\n        if np.abs(p0 - p1)  EPS:\n            p0 = np.clip(p0 - 0.05, EPS, 1 - EPS)\n            p1 = np.clip(p1 + 0.05, EPS, 1 - EPS)\n\n        log_L = -np.inf\n        for _ in range(MAX_EM_ITER):\n            # E-step\n            log_p_y_0 = y_data * np.log(p0) + (1 - y_data) * np.log(1 - p0)\n            log_p_y_1 = y_data * np.log(p1) + (1 - y_data) * np.log(1 - p1)\n            \n            log_joints = np.array([np.log(1 - pi) + log_p_y_0, np.log(pi) + log_p_y_1])\n            log_marg_prob_y = logsumexp(log_joints, axis=0)\n            \n            gamma_1 = np.exp(log_joints[1] - log_marg_prob_y)\n            \n            # Log-Likelihood\n            new_log_L = np.sum(log_marg_prob_y)\n            if np.abs(new_log_L - log_L)  EM_TOL:\n                log_L = new_log_L\n                break\n            log_L = new_log_L\n\n            # M-step\n            gamma_sum_1 = np.sum(gamma_1)\n            gamma_sum_0 = n - gamma_sum_1\n            \n            pi = gamma_sum_1 / n\n            p0 = np.sum((1 - gamma_1) * y_data) / gamma_sum_0 if gamma_sum_0  EPS else EPS\n            p1 = np.sum(gamma_1 * y_data) / gamma_sum_1 if gamma_sum_1  EPS else 1 - EPS\n            \n            if p0  p1:\n                p0, p1 = p1, p0\n                pi = 1 - pi\n            \n            pi = np.clip(pi, EPS, 1 - EPS)\n            p0 = np.clip(p0, EPS, 1 - EPS)\n            p1 = np.clip(p1, EPS, 1 - EPS)\n\n        if log_L  best_log_L:\n            best_log_L = log_L\n            best_params = (pi, p0, p1)\n            \n            # Recalculate final responsibilities with best params\n            final_p0, final_p1 = best_params[1], best_params[2]\n            log_p_y_0_final = y_data * np.log(final_p0) + (1 - y_data) * np.log(1 - final_p0)\n            log_p_y_1_final = y_data * np.log(final_p1) + (1 - y_data) * np.log(1 - final_p1)\n            log_joints_final = np.array([np.log(1 - best_params[0]) + log_p_y_0_final, np.log(best_params[0]) + log_p_y_1_final])\n            log_marg_prob_y_final = logsumexp(log_joints_final, axis=0)\n            best_responsibilities = np.exp(log_joints_final[1] - log_marg_prob_y_final)\n\n    return best_log_L, best_params, best_responsibilities\n\ndef analyze_bin(y_data, rng):\n    \"\"\"Calculates Delta BIC and inferred labels for a bin of data.\"\"\"\n    n_b = len(y_data)\n    y_sum = np.sum(y_data)\n\n    # H0 model: single Bernoulli\n    if n_b == 0: return -np.inf, np.array([])\n    p_h0 = np.clip(y_sum / n_b, EPS, 1 - EPS)\n    log_L0 = y_sum * np.log(p_h0) + (n_b - y_sum) * np.log(1 - p_h0)\n    bic0 = -2 * log_L0 + 1 * np.log(n_b)\n    \n    # H1 model: 2-component mixture\n    if y_sum == 0 or y_sum == n_b:\n        log_L1 = log_L0\n        labels = np.zeros_like(y_data) if y_sum == 0 else np.ones_like(y_data)\n    else:\n        log_L1, _, responsibilities = fit_mixture_em(y_data, rng)\n        labels = (responsibilities = 0.5).astype(int)\n\n    bic1 = -2 * log_L1 + 3 * np.log(n_b)\n    \n    delta_bic = bic0 - bic1\n    return delta_bic, labels\n\ndef run_full_analysis(N, alpha, b, d, s, B, seed):\n    \"\"\"Performs the full detection, augmentation, and validation protocol.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    xi, y = generate_data(N, alpha, b, d, s, rng)\n\n    bin_width = 2.0 / B\n    bin_indices = np.floor((xi - (-1)) / bin_width).astype(int)\n    bin_indices = np.clip(bin_indices, 0, B - 1)\n\n    # Pre-augmentation analysis\n    delta_bic_max_pre = -np.inf\n    all_labels = np.full(N, -1, dtype=int)\n    valid_bins = []\n\n    for b_idx in range(B):\n        mask = (bin_indices == b_idx)\n        y_bin = y[mask]\n        \n        if len(y_bin)  N_MIN: continue\n        \n        valid_bins.append(b_idx)\n        delta_bic, labels = analyze_bin(y_bin, rng)\n        delta_bic_max_pre = max(delta_bic_max_pre, delta_bic)\n        all_labels[mask] = labels\n\n    # Detection check\n    if delta_bic_max_pre  T_PRE:\n        return 0\n\n    # Post-augmentation validation\n    delta_bic_max_post = -np.inf\n    for b_idx in valid_bins:\n        for i_hat in [0, 1]:\n            mask = (bin_indices == b_idx)  (all_labels == i_hat)\n            y_subgroup = y[mask]\n            \n            if len(y_subgroup)  N_MIN: continue\n            \n            delta_bic, _ = analyze_bin(y_subgroup, rng)\n            delta_bic_max_post = max(delta_bic_max_post, delta_bic)\n    \n    # If no subgroup qualified, max_post is -inf, which is  T_POST. Correct.\n\n    # Final decision\n    return 1 if delta_bic_max_post  T_POST else 0\n\ndef solve():\n    \"\"\"Main function to run the analysis for all test cases.\"\"\"\n    test_cases = [\n        (4000, 4.0, 0.0, 2.0, 0.25, 6, 1),\n        (4000, 4.0, 0.0, 0.0, 0.25, 6, 2),\n        (600, 4.0, 0.0, 0.5, 0.25, 6, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_full_analysis(*case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3402799"}, {"introduction": "The insights gained from committor analysis are only reliable if the underlying numerical simulations are accurate. This practice addresses the critical issue of numerical error, which can confound the validation of a reaction coordinate [@problem_id:3402822]. You will implement methods to estimate and test for integrator bias using Richardson extrapolation and distinguish this numerical artifact from the genuine physical signature of an invalid reaction coordinate, ensuring your conclusions are robust and physically meaningful.", "problem": "Consider a system evolving under the overdamped Langevin stochastic differential equation, where a configuration $\\mathbf{x}$ undergoes dynamics according to $d\\mathbf{x}_t = -\\beta D \\nabla U(\\mathbf{x}_t)\\, dt + \\sqrt{2D}\\, d\\mathbf{W}_t$, with temperature parameter $\\beta$, diffusion coefficient $D$, potential energy $U(\\mathbf{x})$, and Wiener process $\\mathbf{W}_t$. The committor function $q(\\mathbf{x})$ is defined as the probability that a trajectory initiated at $\\mathbf{x}$ will reach a target set $B$ before a reactant set $A$, and satisfies the backward Kolmogorov equation with absorbing boundary conditions on $A$ and $B$. In molecular simulation practice, $q(\\mathbf{x})$ is estimated empirically as the fraction of shooting trajectories that hit $B$ before $A$ when integrated with a finite time step $\\Delta t$ and a chosen integrator. Denote such a finite-time-step estimate by $q_{\\Delta t}(\\mathbf{x})$. For an integrator with weak order $p$ (where $p$ is a positive integer), the weak discretization bias in the committor estimate at fixed $\\mathbf{x}$ often admits an expansion of the form $q_{\\Delta t}(\\mathbf{x}) = q_{0}(\\mathbf{x}) + c(\\mathbf{x}) \\, (\\Delta t)^{p} + \\mathcal{O}((\\Delta t)^{p+1})$, where $q_{0}(\\mathbf{x})$ is the $\\Delta t \\to 0$ limit and $c(\\mathbf{x})$ is an integrator- and state-dependent coefficient. In addition, each empirical estimate of $q_{\\Delta t}(\\mathbf{x})$ computed from $n$ independent shooting trajectories is a binomial proportion with random sampling error of order $\\sqrt{q_{\\Delta t}(\\mathbf{x})(1-q_{\\Delta t}(\\mathbf{x}))/n}$.\n\nYour task is to implement a principled procedure that, given empirical counts from finite-time-step committor experiments at two step sizes $\\Delta t$ and $\\Delta t/2$ for a single reaction-coordinate value $s$, will:\n- Use Richardson extrapolation based on the known weak order $p$ of the integrator to estimate $q_{0}(\\mathbf{x})$.\n- Perform a statistical test to decide whether the observed difference between $q_{\\Delta t}(\\mathbf{x})$ and $q_{\\Delta t/2}(\\mathbf{x})$ is statistically significant relative to binomial sampling noise, thereby indicating detectable integrator bias at the tested scales.\n- Perform an \"isocommittor consistency\" overdispersion test across multiple microstates $\\{\\mathbf{x}_i\\}$ that share the same reaction coordinate value $s$ but differ in orthogonal degrees of freedom. Using only the smaller step size $\\Delta t/2$ to minimize bias, determine whether the variability of the empirical committor estimates across the $\\{\\mathbf{x}_i\\}$ exceeds what is expected from binomial sampling alone (which would indicate reaction coordinate invalidity at $s$).\n\nFrom first principles, base your approach on the following core definitions and well-tested facts:\n- The committor $q(\\mathbf{x})$ is the splitting probability for a Markov process with absorbing sets $A$ and $B$.\n- For a weak order $p$ integrator, weak errors in expectations of sufficiently smooth observables scale as $\\mathcal{O}((\\Delta t)^p)$.\n- An empirical committor fraction $\\hat{q} = k/n$ from $n$ independent trials with $k$ \"successes\" (hits to $B$ before $A$) is a binomial proportion.\n- For two independent binomial proportions $\\hat{q}_1$ and $\\hat{q}_2$ with counts $(k_1,n_1)$ and $(k_2,n_2)$, a pooled-proportion $z$-test can assess whether $\\hat{q}_1 - \\hat{q}_2$ deviates from zero beyond sampling error at a specified significance level.\n- A Pearson chi-square statistic comparing observed counts $\\{k_i\\}$ to a common-probability null model across microstates with trials $\\{n_i\\}$ can be used to test for overdispersion (excess variability) beyond binomial sampling.\n\nImplement the following, in pure mathematical and logical terms:\n- Given weak order $p$, and empirical proportions $\\hat{q}_{\\Delta t}$ and $\\hat{q}_{\\Delta t/2}$ at step sizes $\\Delta t$ and $\\Delta t/2$, respectively, construct a bias-reduced estimator for $q_0$ using Richardson extrapolation that cancels the leading $\\mathcal{O}((\\Delta t)^p)$ term. Do not assume any form for higher-order coefficients beyond their order of magnitude.\n- Use a two-proportion pooled $z$-test at a two-sided significance level $\\alpha = 0.05$ to determine if the difference between $\\hat{q}_{\\Delta t}$ and $\\hat{q}_{\\Delta t/2}$ indicates detectable integrator bias at the tested scales. Answer the decision as a Boolean, where $True$ means \"detectable integrator bias\" and $False$ means \"no detectable integrator bias.\"\n- For the isocommittor consistency test, using only the data at $\\Delta t/2$ across multiple microstates at the same $s$, test the null hypothesis that all microstates share a common committor $q$ and any variability arises solely from binomial noise. Use the Pearson chi-square statistic with degrees of freedom equal to the number of microstates minus one, and a two-sided significance level $\\alpha = 0.05$. Return a Boolean, where $True$ means \"reaction coordinate invalidity detected\" and $False$ means \"no evidence of reaction coordinate invalidity at the tested precision.\"\n- Additionally, return a Boolean that reports whether the Richardson-extrapolated estimate of $q_0$ falls outside the physically admissible interval $[0,1]$, which would indicate model mismatch or unresolved higher-order effects at the tested scales.\n\nTest Suite:\nYour program must apply the above procedure to the following three cases. Each case provides the weak order $p$, aggregate counts at $\\Delta t$ and $\\Delta t/2$, and a set of microstate-level counts at $\\Delta t/2$ for an isocommittor consistency test. All counts are independent binomial outcomes.\n\n- Case $1$ (mid-barrier, first-order integrator):\n  - Weak order: $p = 1$.\n  - Aggregate at $\\Delta t$: $n_1 = 20000$, $k_1 = 10400$.\n  - Aggregate at $\\Delta t/2$: $n_2 = 20000$, $k_2 = 10200$.\n  - Microstates at $\\Delta t/2$ (same $s$, four distinct microstates): $(n_i, k_i) \\in \\{(4000, 1400), (4000, 2000), (4000, 2600), (4000, 2000)\\}$.\n\n- Case $2$ (near-basin, second-order integrator):\n  - Weak order: $p = 2$.\n  - Aggregate at $\\Delta t$: $n_1 = 10000$, $k_1 = 520$.\n  - Aggregate at $\\Delta t/2$: $n_2 = 10000$, $k_2 = 505$.\n  - Microstates at $\\Delta t/2$ (same $s$, four distinct microstates): $(n_i, k_i) \\in \\{(4000, 192), (4000, 204), (4000, 208), (4000, 196)\\}$.\n\n- Case $3$ (near-product, first-order integrator, boundary effect):\n  - Weak order: $p = 1$.\n  - Aggregate at $\\Delta t$: $n_1 = 8000$, $k_1 = 7760$.\n  - Aggregate at $\\Delta t/2$: $n_2 = 8000$, $k_2 = 7920$.\n  - Microstates at $\\Delta t/2$ (same $s$, four distinct microstates): $(n_i, k_i) \\in \\{(4000, 3960), (4000, 3975), (4000, 3965), (4000, 3972)\\}$.\n\nFinal Output Specification:\n- For each case, produce a list with four entries: $[q^{\\mathrm{Rich}}_{0}, \\mathrm{bias\\_detected}, \\mathrm{rc\\_invalid}, \\mathrm{out\\_of\\_bounds}]$, where:\n  - $q^{\\mathrm{Rich}}_{0}$ is the Richardson-extrapolated estimate of the zero-time-step committor, rounded to six decimal places.\n  - $\\mathrm{bias\\_detected}$ is a Boolean from the two-proportion $z$-test at significance $\\alpha = 0.05$.\n  - $\\mathrm{rc\\_invalid}$ is a Boolean from the isocommittor chi-square test at significance $\\alpha = 0.05$.\n  - $\\mathrm{out\\_of\\_bounds}$ is a Boolean that is $True$ if $q^{\\mathrm{Rich}}_{0} \\notin [0,1]$ and $False$ otherwise.\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets, with each case formatted as its own bracketed list and $q^{\\mathrm{Rich}}_{0}$ printed with exactly six digits after the decimal point. For example: $[[0.123456,True,False,False],[\\dots],[\\dots]]$.", "solution": "This problem is addressed by applying a series of distinct statistical and numerical methods. The solution is structured into four main components:\n1.  **Richardson Extrapolation:** To get a more accurate estimate of the true committor value ($q_0$) in the limit of an infinitely small time step, we use Richardson extrapolation. This technique combines results from two different time steps ($\\Delta t$ and $\\Delta t/2$) to cancel out the leading-order numerical error term, assuming the error follows a known power law based on the integrator's weak order $p$.\n2.  **Integrator Bias Test:** We use a two-proportion z-test to determine if the difference in the empirical committor estimates from the two time steps is statistically significant. A significant difference implies that the numerical integration error (bias) is detectable with the given amount of data.\n3.  **Isocommittor Consistency Test:** To check the validity of the reaction coordinate itself, we use a Pearson's chi-square test for homogeneity. This test is applied to the data from multiple microstates that all share the same reaction coordinate value. The null hypothesis is that all these microstates have the same underlying committor probability, and any observed variation is due to random sampling noise. If the test rejects this hypothesis, it means there is 'overdispersion'—more variation than expected—indicating that the reaction coordinate is incomplete.\n4.  **Physical Bounds Check:** The Richardson-extrapolated estimate is checked to ensure it falls within the physical range of a probability, $[0, 1]$. A value outside this range can signal that the simple error model is insufficient.\n\nThe provided code implements these four procedures for each test case.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, chi2\n\ndef solve():\n    \"\"\"\n    Applies a validation procedure for committor analysis to three test cases.\n\n    The procedure includes:\n    1. Richardson extrapolation to estimate the zero-timestep committor.\n    2. A two-proportion z-test to detect integrator bias.\n    3. A chi-square test for isocommittor consistency to validate the reaction coordinate.\n    4. A check for the physical validity of the extrapolated committor value.\n    \"\"\"\n    \n    # Critical value for the statistical tests at a two-sided significance level alpha = 0.05\n    alpha = 0.05\n    z_critical = norm.ppf(1 - alpha / 2)\n\n    test_cases = [\n        {\n            \"p\": 1,\n            \"agg_dt\": {\"n\": 20000, \"k\": 10400},\n            \"agg_dt_half\": {\"n\": 20000, \"k\": 10200},\n            \"microstates_dt_half\": [\n                {\"n\": 4000, \"k\": 1400},\n                {\"n\": 4000, \"k\": 2000},\n                {\"n\": 4000, \"k\": 2600},\n                {\"n\": 4000, \"k\": 2000},\n            ]\n        },\n        {\n            \"p\": 2,\n            \"agg_dt\": {\"n\": 10000, \"k\": 520},\n            \"agg_dt_half\": {\"n\": 10000, \"k\": 505},\n            \"microstates_dt_half\": [\n                {\"n\": 4000, \"k\": 192},\n                {\"n\": 4000, \"k\": 204},\n                {\"n\": 4000, \"k\": 208},\n                {\"n\": 4000, \"k\": 196},\n            ]\n        },\n        {\n            \"p\": 1,\n            \"agg_dt\": {\"n\": 8000, \"k\": 7760},\n            \"agg_dt_half\": {\"n\": 8000, \"k\": 7920},\n            \"microstates_dt_half\": [\n                {\"n\": 4000, \"k\": 3960},\n                {\"n\": 4000, \"k\": 3975},\n                {\"n\": 4000, \"k\": 3965},\n                {\"n\": 4000, \"k\": 3972},\n            ]\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        p = case[\"p\"]\n        n1, k1 = case[\"agg_dt\"][\"n\"], case[\"agg_dt\"][\"k\"]\n        n2, k2 = case[\"agg_dt_half\"][\"n\"], case[\"agg_dt_half\"][\"k\"]\n        microstates = case[\"microstates_dt_half\"]\n\n        # --- 1. Richardson Extrapolation ---\n        q1_hat = k1 / n1\n        q2_hat = k2 / n2\n        q_rich_0 = (2**p * q2_hat - q1_hat) / (2**p - 1)\n\n        # --- 4. Physical Bounds Check ---\n        out_of_bounds = not (0 = q_rich_0 = 1)\n\n        # --- 2. Integrator Bias Test (Two-Proportion Z-Test) ---\n        q_pool_bias = (k1 + k2) / (n1 + n2)\n        if q_pool_bias == 0 or q_pool_bias == 1:\n            z_stat_bias = 0.0\n        else:\n            se_bias = np.sqrt(q_pool_bias * (1 - q_pool_bias) * (1/n1 + 1/n2))\n            z_stat_bias = (q1_hat - q2_hat) / se_bias\n        \n        bias_detected = np.abs(z_stat_bias)  z_critical\n\n        # --- 3. Isocommittor Consistency Test (Chi-Square Test) ---\n        num_microstates = len(microstates)\n        df_rc = num_microstates - 1\n        chi2_critical = chi2.ppf(1 - alpha, df=df_rc)\n\n        k_micro = np.array([m[\"k\"] for m in microstates])\n        n_micro = np.array([m[\"n\"] for m in microstates])\n\n        k_total_micro = np.sum(k_micro)\n        n_total_micro = np.sum(n_micro)\n        \n        q_pool_rc = k_total_micro / n_total_micro\n\n        if q_pool_rc == 0 or q_pool_rc == 1:\n            chi2_stat_rc = 0.0\n        else:\n            expected_k = n_micro * q_pool_rc\n            # This is the Pearson's chi-square test statistic for homogeneity of proportions\n            chi2_stat_rc = np.sum((k_micro - expected_k)**2 / (n_micro * q_pool_rc * (1-q_pool_rc)))\n        \n        rc_invalid = chi2_stat_rc  chi2_critical\n\n        current_result = [q_rich_0, bias_detected, rc_invalid, out_of_bounds]\n        results.append(current_result)\n    \n    # Format the final output string to match the problem specification\n    formatted_results = []\n    for res in results:\n        q_str = f\"{res[0]:.6f}\"\n        # Convert Python booleans to required string representation\n        bias_str = str(res[1])\n        rc_str = str(res[2])\n        oob_str = str(res[3])\n        formatted_results.append(f\"[{q_str},{bias_str},{rc_str},{oob_str}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3402822"}]}