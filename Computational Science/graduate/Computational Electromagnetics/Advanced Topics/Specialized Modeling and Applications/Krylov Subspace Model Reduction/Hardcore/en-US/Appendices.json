{
    "hands_on_practices": [
        {
            "introduction": "The foundation of reliable model order reduction is the ability to estimate the error of an approximation without knowing the exact solution. This practice delves into the theoretical heart of this capability by asking you to derive a rigorous *a posteriori* error bound. By linking the unobservable transfer function error, $\\|H(s) - \\hat{H}(s)\\|_{2}$, to the computable Petrov-Galerkin residual, $r_u(s)$, you will establish a cornerstone principle of trustworthy model reduction . Mastering this derivation provides a fundamental understanding of how error estimators work, a critical skill for validating reduced-order models.",
            "id": "3322125",
            "problem": "Consider a semi-discrete formulation of Maxwell’s equations obtained, for example, by a curl-conforming finite element method or finite integration technique, which yields a linear time-invariant (LTI) descriptor system of the form $E \\dot{x}(t) = A x(t) + B u(t)$ with output $y(t) = C x(t)$, where $E \\in \\mathbb{C}^{n \\times n}$ is typically Hermitian positive semidefinite, $A \\in \\mathbb{C}^{n \\times n}$ arises from material and topological operators, and $B \\in \\mathbb{C}^{n \\times m}$, $C \\in \\mathbb{C}^{p \\times n}$ are input and output matrices consistent with the chosen ports or probes. The frequency-domain transfer function is $H(s) = C (s E - A)^{-1} B$, with $s \\in \\mathbb{C}$ in the open right half-plane ensuring well-posedness. A Petrov–Galerkin reduced-order model is constructed by projection with trial basis $V \\in \\mathbb{C}^{n \\times r}$ and test basis $W \\in \\mathbb{C}^{n \\times r}$ such that $W^{*} E V$ is nonsingular, leading to the reduced transfer function $\\hat{H}(s) = C V \\left(s W^{*} E V - W^{*} A V\\right)^{-1} W^{*} B$.\n\nFix a frequency $s \\in \\mathbb{C}$ such that $(s E - A)$ is invertible. For a given input $u \\in \\mathbb{C}^{m}$, define $x(s) \\in \\mathbb{C}^{n}$ by $(s E - A) x(s) = B u$ and the reduced state $\\hat{x}(s) = V \\hat{z}(s)$ where $\\hat{z}(s) \\in \\mathbb{C}^{r}$ satisfies the Petrov–Galerkin condition $W^{*} \\left((s E - A) V \\hat{z}(s) - B u\\right) = 0$. Define the Petrov–Galerkin residual at frequency $s$ associated with $u$ by $r_{u}(s) = B u - (s E - A) \\hat{x}(s)$. Assume the Euclidean vector norm and the induced spectral operator $2$-norm throughout.\n\nStarting only from the definitions above and standard norm inequalities, derive an a posteriori error estimator at the fixed frequency $s$ for the induced $2$-norm of the transfer function error $\\|H(s) - \\hat{H}(s)\\|_{2}$ that depends explicitly on the Petrov–Galerkin residual and the operator norm of $(s E - A)^{-1}$. Your derivation must begin from the error in the state and output implied by the residual and arrive at a bound of the form “norm of a resolvent-like factor times a computable residual magnitude.” Then, discuss how to obtain computable bounds on the resolvent factor by local solves at the frequency $s$ without forming $(s E - A)^{-1}$ explicitly, and how to bound the worst-case residual over inputs using only projected quantities. Conclude by stating your final estimator as a single closed-form expression in terms of the Petrov–Galerkin residual and the resolvent factor. No numerical evaluation is required; provide the final estimator expression symbolically as your answer.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of model order reduction for linear time-invariant systems, well-posed, objective, and self-contained. We proceed with the derivation.\n\nOur goal is to derive an a posteriori error estimator for the induced $2$-norm of the transfer function error, $\\|H(s) - \\hat{H}(s)\\|_{2}$, at a fixed frequency $s \\in \\mathbb{C}$. The derivation will commence from the error in the state variable.\n\nLet $x(s) \\in \\mathbb{C}^{n}$ be the state of the full system and $\\hat{x}(s) = V \\hat{z}(s) \\in \\mathbb{C}^{n}$ be the approximate state from the reduced-order model for a given input $u \\in \\mathbb{C}^{m}$ at frequency $s$. The state error is defined as $e_x(s) = x(s) - \\hat{x}(s)$.\n\nFrom the problem statement, the full system state satisfies the frequency-domain equation:\n$$ (s E - A) x(s) = B u $$\nThe Petrov-Galerkin residual $r_{u}(s)$ is defined by:\n$$ r_{u}(s) = B u - (s E - A) \\hat{x}(s) $$\nThis can be rearranged to express the action of the system matrix on the approximate state:\n$$ (s E - A) \\hat{x}(s) = B u - r_{u}(s) $$\nSubtracting this equation from the full system equation, we obtain an expression for the state error:\n$$ (s E - A) x(s) - (s E - A) \\hat{x}(s) = B u - (B u - r_{u}(s)) $$\n$$ (s E - A) (x(s) - \\hat{x}(s)) = r_{u}(s) $$\n$$ (s E - A) e_x(s) = r_{u}(s) $$\nSince the problem states that $(s E - A)$ is invertible for the chosen $s$, we can formally write the state error in terms of the residual:\n$$ e_x(s) = (s E - A)^{-1} r_{u}(s) $$\nThis equation provides a direct link between the state estimation error and the computable residual.\n\nNext, we relate the state error to the error in the output. The output of the full system is $y(s) = C x(s)$, and the output corresponding to the approximate state is $\\hat{y}(s) = C \\hat{x}(s)$. The output error is $e_y(s) = y(s) - \\hat{y}(s)$.\n$$ e_y(s) = C x(s) - C \\hat{x}(s) = C(x(s) - \\hat{x}(s)) = C e_x(s) $$\nSubstituting the expression for $e_x(s)$, we get:\n$$ e_y(s) = C (s E - A)^{-1} r_{u}(s) $$\nThe output error can also be expressed in terms of the transfer function error. By definition, $y(s) = H(s) u$ and $\\hat{y}(s) = \\hat{H}(s) u$. Therefore:\n$$ e_y(s) = (H(s) - \\hat{H}(s)) u $$\nCombining these two expressions for the output error gives the exact error representation:\n$$ (H(s) - \\hat{H}(s)) u = C (s E - A)^{-1} r_{u}(s) $$\nTo find a bound on the norm of the transfer function error, $\\|H(s) - \\hat{H}(s)\\|_{2}$, we take the Euclidean vector norm ($2$-norm) of both sides.\n$$ \\|(H(s) - \\hat{H}(s)) u\\|_{2} = \\|C (s E - A)^{-1} r_{u}(s)\\|_{2} $$\nUsing the property of induced operator norms, $\\|M v\\|_{2} \\le \\|M\\|_{2} \\|v\\|_{2}$, we can establish an inequality:\n$$ \\|(H(s) - \\hat{H}(s)) u\\|_{2} \\le \\|C (s E - A)^{-1}\\|_{2} \\|r_{u}(s)\\|_{2} $$\nA further application of the submultiplicative property of norms, $\\|MN\\|_{2} \\le \\|M\\|_{2} \\|N\\|_{2}$, yields:\n$$ \\|(H(s) - \\hat{H}(s)) u\\|_{2} \\le \\|C\\|_{2} \\|(s E - A)^{-1}\\|_{2} \\|r_{u}(s)\\|_{2} $$\nThe induced $2$-norm of the transfer function error matrix is defined as the maximum amplification of the output error norm over all unit-norm inputs:\n$$ \\|H(s) - \\hat{H}(s)\\|_{2} = \\sup_{\\|u\\|_{2}=1} \\|(H(s) - \\hat{H}(s)) u\\|_{2} $$\nApplying this definition to our inequality, we obtain the error bound:\n$$ \\|H(s) - \\hat{H}(s)\\|_{2} \\le \\sup_{\\|u\\|_{2}=1} \\left( \\|C\\|_{2} \\|(s E - A)^{-1}\\|_{2} \\|r_{u}(s)\\|_{2} \\right) $$\nSince $\\|C\\|_{2}$ and $\\|(s E - A)^{-1}\\|_{2}$ are independent of the input $u$, we can write:\n$$ \\|H(s) - \\hat{H}(s)\\|_{2} \\le \\|C\\|_{2} \\|(s E - A)^{-1}\\|_{2} \\sup_{\\|u\\|_{2}=1} \\|r_{u}(s)\\|_{2} $$\nThis inequality provides an a posteriori error bound of the desired form: a \"resolvent-like factor,\" which is $\\|C\\|_{2} \\|(s E - A)^{-1}\\|_{2}$, multiplied by a \"computable residual magnitude,\" $\\sup_{\\|u\\|_{2}=1} \\|r_{u}(s)\\|_{2}$.\n\nThe problem then asks for a discussion on computing the factors in this bound.\n$1$. The resolvent factor, $\\|(s E - A)^{-1}\\|_{2}$, represents the largest amplification factor of the operator $(s E - A)^{-1}$. For large-scale systems, forming the inverse explicitly is computationally prohibitive. Instead, its norm can be estimated. The norm is equal to the reciprocal of the smallest singular value of $(s E - A)$, i.e., $\\|(s E - A)^{-1}\\|_{2} = 1/\\sigma_{\\min}(s E - A)$. This singular value can be estimated using iterative methods, such as the inverse power method or Lanczos/Arnoldi algorithms, applied to $(s E - A)^{*}(s E - A)$ or $(s E - A)(s E - A)^{*}$. Each iteration of these methods requires solving a linear system of the form $(s E - A)x=b$ or $(s E - A)^{*}x=b$, which are the \"local solves at the frequency $s$\" mentioned in the problem. These solves are performed with iterative methods like GMRES or BiCGSTAB, which only require matrix-vector products with the large, sparse matrices $E$ and $A$.\n\n$2$. The worst-case residual magnitude, $\\sup_{\\|u\\|_{2}=1} \\|r_{u}(s)\\|_{2}$, is the induced $2$-norm of the residual operator. To see this, we express the residual $r_u(s)$ as a linear operator acting on $u$. From the definitions, $\\hat{x}(s) = V \\hat{z}(s)$ and $\\hat{z}(s) = (s W^{*} E V - W^{*} A V)^{-1} W^{*} B u$. Substituting this into the definition of the residual:\n$$ r_{u}(s) = B u - (s E - A) V \\left(s W^{*} E V - W^{*} A V\\right)^{-1} W^{*} B u $$\n$$ r_{u}(s) = \\left(I - (s E - A) V \\left(s W^{*} E V - W^{*} A V\\right)^{-1} W^{*} B\\right) u $$\nThe worst-case residual is the norm of the $n \\times m$ matrix operator:\n$$ \\sup_{\\|u\\|_{2}=1} \\|r_{u}(s)\\|_{2} = \\left\\| I - (s E - A) V \\left(s W^{*} E V - W^{*} A V\\right)^{-1} W^{*} B \\right\\|_{2} $$\nComputing this norm requires forming this matrix. This involves operating with the large matrices $A$ and $E$ (specifically, forming $(sE-A)V = s(EV)-(AV)$) but also utilizes the \"projected quantities\" $W^*AV$, $W^*EV$, and $W^*B$ to form and invert the small $r \\times r$ matrix $(s W^{*} E V - W^{*} A V)$. Once the $n \\times m$ residual operator matrix is formed, its $2$-norm can be computed (e.g., via SVD), which is computationally expensive but provides a rigorous bound. For certain choices of projection bases $V$ and $W$ (e.g., from Krylov methods), this expression can be simplified significantly.\n\nFinally, the problem asks for the estimator as a single closed-form expression. The derived upper bound serves as the a posteriori error estimator. Denoting the worst-case residual norm over all unit-norm inputs as $\\mathcal{R}(s) = \\sup_{\\|u\\|_{2}=1} \\|r_u(s)\\|_2$, the estimator $\\Delta(s)$ is:\n$$ \\Delta(s) = \\|C\\|_{2} \\|(s E - A)^{-1}\\|_{2} \\mathcal{R}(s) $$\nThis expression clearly separates the estimator into a factor related to the full system's properties (resolvent) and a factor related to the quality of the approximation (residual).",
            "answer": "$$ \\boxed{\\|C\\|_{2} \\|(s E - A)^{-1}\\|_{2} \\sup_{\\|u\\|_{2}=1} \\|r_u(s)\\|_{2}} $$"
        },
        {
            "introduction": "While single-frequency error bounds are fundamental, practical electromagnetic systems must perform reliably across a wide operational bandwidth. This exercise transitions from the theoretical point-wise estimate to a practical, computable measure of wideband performance. You will design and implement an error indicator that aggregates local residuals at multiple frequency samples, using quadrature weights to approximate an integral error norm, such as the $\\mathcal{H}_2$ norm, over a frequency band . This hands-on coding task develops the skills to quantify and compare the wideband accuracy of different reduced models, a crucial step in engineering design.",
            "id": "3322065",
            "problem": "Consider a frequency-domain formulation of Maxwell's equations discretized by the finite element method, producing a generalized linear time-invariant system with matrices $E \\in \\mathbb{R}^{n \\times n}$ and $A \\in \\mathbb{R}^{n \\times n}$, an input matrix $B \\in \\mathbb{R}^{n \\times m}$, and an output matrix $C \\in \\mathbb{R}^{p \\times n}$. For a complex frequency shift $s \\in \\mathbb{C}$, the full-order state $X(s) \\in \\mathbb{C}^{n \\times m}$ satisfies the linear system $$(sE - A) X(s) = B,$$ and the associated transfer function is $$H(s) = C X(s) = C (sE - A)^{-1} B.$$ In projection-based model reduction using a Krylov subspace, a column-orthonormal projection matrix $V \\in \\mathbb{R}^{n \\times r}$ defines reduced operators $$E_r = V^\\top E V, \\quad A_r = V^\\top A V, \\quad B_r = V^\\top B, \\quad C_r = C V.$$ The reduced-order approximation of the state is then $$\\widetilde{X}(s) = V (s E_r - A_r)^{-1} B_r,$$ and the corresponding reduced transfer function is $$H_r(s) = C_r (s E_r - A_r)^{-1} B_r.$$\n\nWideband accuracy over a frequency interval can be assessed by aggregating information at multiple frequency shifts $\\{ s_i \\}_{i=1}^q$ with nonnegative quadrature weights $\\{ w_i \\}_{i=1}^q$ chosen to approximate an integral over the band, in the spirit of the Hardy space $\\mathcal{H}_2$ norm on the imaginary axis. Your task is to design an error indicator that combines the residuals produced by the reduced approximation at these shifts, with the aggregation controlled by the quadrature weights, so that it acts as a physically meaningful and computationally tractable surrogate for an $\\mathcal{H}_2$-like norm over the band. The residual at a shift should be defined in terms of the original full-order operators and the reduced-order approximate state.\n\nBased on first principles, derive such an indicator and implement a program to compute it for the following test suite. Each test case provides $E$, $A$, $B$, $V$, a list of imaginary-axis shifts $s_i = \\jmath \\omega_i$ with $\\omega_i \\in \\mathbb{R}_{\\ge 0}$, and quadrature weights $w_i \\in \\mathbb{R}_{\\ge 0}$. The residual norm to be combined across shifts must be computed in a way that is consistent for multiple right-hand sides (multiple columns of $B$), and the aggregation across shifts must incorporate the given weights.\n\nYou must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. Express each result as a floating-point number. No physical units are required in the output.\n\nUse the following test suite:\n\n- Test case 1 (happy path, multiple inputs):\n  - $n=4$, $m=2$, $r=2$.\n  - $$E = \\mathrm{diag}(2.0,\\,1.5,\\,1.0,\\,0.8).$$\n  - $$A = \\begin{bmatrix}\n  6.0  -1.0  0.0  0.0 \\\\\n  -1.0  5.0  -1.0  0.0 \\\\\n  0.0  -1.0  4.5  -0.8 \\\\\n  0.0  0.0  -0.8  3.5\n  \\end{bmatrix}.$$\n  - $$B = \\begin{bmatrix}\n  1.0  0.5 \\\\\n  0.2  -0.3 \\\\\n  0.0  0.1 \\\\\n  -0.1  0.2\n  \\end{bmatrix}.$$\n  - $$V = \\begin{bmatrix}\n  1.0  0.0 \\\\\n  0.0  1.0 \\\\\n  0.0  0.0 \\\\\n  0.0  0.0\n  \\end{bmatrix}.$$\n  - Shifts on the imaginary axis: $\\omega_1 = 1.0\\times 10^8$, $\\omega_2 = 2.0\\times 10^8$, $\\omega_3 = 3.0\\times 10^8$.\n  - Simpson's rule weights over the band $[1.0\\times 10^8,\\,3.0\\times 10^8]$: $w_1 = \\dfrac{2.0\\times 10^8}{6}$, $w_2 = \\dfrac{4\\cdot 2.0\\times 10^8}{6}$, $w_3 = \\dfrac{2.0\\times 10^8}{6}$.\n\n- Test case 2 (identity mass, zero weight edge):\n  - $n=4$, $m=1$, $r=1$.\n  - $$E = I_4.$$\n  - $$A = \\begin{bmatrix}\n  4.0  -1.0  0.0  0.0 \\\\\n  -1.0  4.0  -1.0  0.0 \\\\\n  0.0  -1.0  3.5  -1.0 \\\\\n  0.0  0.0  -1.0  3.0\n  \\end{bmatrix}.$$\n  - $$B = \\begin{bmatrix}\n  1.0 \\\\\n  0.0 \\\\\n  0.5 \\\\\n  -0.2\n  \\end{bmatrix}.$$\n  - $$V = \\frac{1}{2} \\begin{bmatrix}\n  1.0 \\\\\n  1.0 \\\\\n  1.0 \\\\\n  1.0\n  \\end{bmatrix}.$$\n  - Shifts: $\\omega_1 = 5.0\\times 10^7$, $\\omega_2 = 1.5\\times 10^8$.\n  - Weights: $w_1 = 1.0$, $w_2 = 0.0$.\n\n- Test case 3 (near-resonant band, single input):\n  - $n=3$, $m=1$, $r=1$.\n  - $$E = \\mathrm{diag}(1.0,\\,0.8,\\,0.6).$$\n  - $$A = \\begin{bmatrix}\n  2.0  -0.9  0.0 \\\\\n  -0.9  2.2  -0.8 \\\\\n  0.0  -0.8  1.8\n  \\end{bmatrix}.$$\n  - $$B = \\begin{bmatrix}\n  0.3 \\\\\n  0.1 \\\\\n  0.2\n  \\end{bmatrix}.$$\n  - $$V = \\begin{bmatrix}\n  1.0 \\\\\n  0.0 \\\\\n  0.0\n  \\end{bmatrix}.$$\n  - Shifts: $\\omega_1 = 1.0\\times 10^7$, $\\omega_2 = 7.0\\times 10^7$, $\\omega_3 = 1.3\\times 10^8$, $\\omega_4 = 1.9\\times 10^8$.\n  - Trapezoidal weights for uniform spacing $h = 6.0\\times 10^7$: $w_1 = \\dfrac{h}{2}$, $w_2 = h$, $w_3 = h$, $w_4 = \\dfrac{h}{2}$.\n\n- Test case 4 (consistency check, full-order subspace should give zero residual):\n  - Use the same $E$, $A$, $B$, shifts, and weights as in Test case 3.\n  - Choose $V = I_3$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3,result4]$). Each $resultk$ must be a floating-point number computed by aggregating the residuals at the given shifts using the provided weights, as defined by your derived indicator.",
            "solution": "The problem requires the derivation and implementation of a wideband error indicator for a projection-based model reduction of a linear time-invariant system arising from discretized Maxwell's equations. The indicator must aggregate residuals at multiple frequency points, weighted by quadrature coefficients, to serve as a surrogate for an $\\mathcal{H}_2$-like norm over a frequency band.\n\n### Step 1: Problem Validation\n\nThe first step is a critical validation of the problem statement.\n\n#### Step 1.1: Extracted Givens\n- **Full-Order System**:\n  - Equation: $(sE - A) X(s) = B$\n  - Matrices: $E \\in \\mathbb{R}^{n \\times n}$, $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times m}$, $C \\in \\mathbb{R}^{p \\times n}$\n  - State: $X(s) \\in \\mathbb{C}^{n \\times m}$\n  - Transfer Function: $H(s) = C (sE - A)^{-1} B$\n- **Reduced-Order System**:\n  - Projection Matrix: $V \\in \\mathbb{R}^{n \\times r}$, column-orthonormal ($V^\\top V = I_r$)\n  - Reduced Operators: $E_r = V^\\top E V$, $A_r = V^\\top A V$, $B_r = V^\\top B$, $C_r = C V$\n  - Approximate State: $\\widetilde{X}(s) = V (s E_r - A_r)^{-1} B_r$\n  - Approximate Transfer Function: $H_r(s) = C_r (s E_r - A_r)^{-1} B_r$\n- **Error Indicator Requirements**:\n  - Sample points: A set of complex frequency shifts $\\{ s_i \\}_{i=1}^q$ where $s_i = \\jmath \\omega_i$ and $\\omega_i \\in \\mathbb{R}_{\\ge 0}$.\n  - Quadrature weights: A set of nonnegative weights $\\{ w_i \\}_{i=1}^q$.\n  - The indicator must combine residuals defined in terms of the full-order operators ($E, A$) and the reduced-order approximate state ($\\widetilde{X}(s)$).\n  - The aggregation must use the weights $w_i$.\n  - The norm used for the residual must be consistent for multiple right-hand sides (i.e., multiple columns of $B$).\n- **Test Suite**: Four test cases are provided, each specifying the numerical values for $E$, $A$, $B$, $V$, shifts $\\{\\omega_i\\}$, and weights $\\{w_i\\}$. The output format for the computed indicator values is specified. Note that the matrix $C$ is introduced in the preamble but is not provided in the test cases and is not required for a state-based residual calculation.\n\n#### Step 1.2: Validation Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the established theory of projection-based model order reduction for linear time-invariant systems, a standard technique in computational electromagnetics and control theory. All equations and definitions are standard.\n- **Well-Posed**: The problem is well-posed. It asks for the derivation of a specific quantity (an error indicator) based on clear constraints and provides all necessary data for its computation in the test suite. The absence of the matrix $C$ is not a flaw, as the task is explicitly to define the residual in terms of the state approximation $\\widetilde{X}(s)$, not the output approximation $H_r(s)$.\n- **Objective**: The problem is stated using precise and objective mathematical language.\n- **Flaw Analysis**:\n  1.  **Scientific Unsoundness**: None. The framework is standard.\n  2.  **Non-Formalizable/Irrelevant**: None. The problem is highly specific and formalizable.\n  3.  **Incomplete/Contradictory Setup**: None. All required data for calculating the state-based residual are provided.\n  4.  **Unrealistic/Infeasible**: None. The provided matrices are small, well-conditioned, and suitable for a computational test.\n  5.  **Ill-Posed/Poorly Structured**: None. The derivation leads to a unique and meaningful numerical result.\n  6.  **Pseudo-Profound/Trivial**: None. The task requires a correct derivation and implementation, which is non-trivial. Test case 4, where the projection space is the full space, provides a non-trivial consistency check.\n  7.  **Outside Scientific Verifiability**: None. The result is numerically verifiable.\n\n#### Step 1.3: Verdict and Action\nThe problem is **valid**. A solution will be derived and implemented.\n\n### Step 2: Derivation of the Error Indicator\n\nThe goal is to construct a scalar error indicator $\\mathcal{E}$ that quantifies the accuracy of the reduced-order model over a frequency band. The derivation proceeds from first principles as requested.\n\n1.  **The State Residual**: The fundamental measure of error is the residual, which quantifies how well the approximate solution satisfies the original governing equation. The full-order system is defined by $(sE - A) X(s) = B$. We substitute the approximate state from the reduced model, $\\widetilde{X}(s)$, into this equation. The resulting imbalance is the state residual matrix $R(s) \\in \\mathbb{C}^{n \\times m}$:\n    $$ R(s) = (sE - A) \\widetilde{X}(s) - B $$\n    A smaller residual implies a better approximation of the system's internal dynamics.\n\n2.  **Norm of the Residual**: The residual $R(s)$ is a matrix. To quantify its magnitude at a single frequency $s_i$, we require a matrix norm. For problems with multiple inputs (i.e., $m  1$), a norm that treats all columns of the residual matrix equitably is needed. The Frobenius norm is the natural choice, as it is the matrix extension of the Euclidean vector norm and is defined as the square root of the sum of the squared magnitudes of all its elements. The squared Frobenius norm is given by:\n    $$ \\| R(s_i) \\|_F^2 = \\sum_{j=1}^n \\sum_{k=1}^m |(R(s_i))_{jk}|^2 = \\mathrm{Tr}(R(s_i)^H R(s_i)) $$\n    where $H$ denotes the conjugate transpose. This norm is computationally convenient and properly aggregates the error across all states and all inputs.\n\n3.  **Aggregation Across Frequencies**: The problem asks for a wideband error indicator that approximates an $\\mathcal{H}_2$-like norm, which is defined by an integral over the frequency axis. We are given quadrature points $s_i = \\jmath \\omega_i$ and corresponding weights $w_i$. This setup directly corresponds to a numerical quadrature scheme for approximating an integral. The integral of the squared Frobenius norm of the residual over a frequency band can be approximated by a weighted sum:\n    $$ \\int_{\\text{band}} \\|R(\\jmath \\omega)\\|_F^2 \\, d\\omega \\approx \\sum_{i=1}^q w_i \\|R(s_i)\\|_F^2 $$\n    This weighted sum, let us call it $\\mathcal{E}^2$, represents the total squared error over the band, as approximated by the given quadrature rule.\n\n4.  **Final Error Indicator**: The final error indicator, $\\mathcal{E}$, should have the same units as the residual norm itself, not its square. Therefore, we define the indicator as the square root of the aggregated sum:\n    $$ \\mathcal{E} = \\sqrt{\\sum_{i=1}^q w_i \\|R(s_i)\\|_F^2} $$\n    This is our physically meaningful and computationally tractable surrogate for the integrated residual norm.\n\n### Step 3: Computational Algorithm\n\nBased on the derived indicator, the algorithm for its computation is as follows:\n\n1.  Given the system matrices $E, A, B$, the projection matrix $V$, the frequency sample points $\\{\\omega_i\\}_{i=1}^q$, and the quadrature weights $\\{w_i\\}_{i=1}^q$.\n2.  Pre-compute the reduced-order matrices, which are independent of frequency:\n    $$ E_r = V^\\top E V \\quad \\in \\mathbb{R}^{r \\times r} $$\n    $$ A_r = V^\\top A V \\quad \\in \\mathbb{R}^{r \\times r} $$\n    $$ B_r = V^\\top B \\quad \\in \\mathbb{R}^{r \\times m} $$\n3.  Initialize a variable for the total weighted squared error, $\\mathcal{E}^2_{\\text{total}} = 0$.\n4.  For each sample index $i = 1, \\dots, q$:\n    a.  Set the complex frequency $s_i = \\jmath \\omega_i$.\n    b.  Form the reduced system matrix for this frequency: $M_{r,i} = s_i E_r - A_r$.\n    c.  Solve the $r \\times r$ complex linear system for the reduced state coefficient matrix $Y(s_i) \\in \\mathbb{C}^{r \\times m}$:\n        $$ M_{r,i} Y(s_i) = B_r \\implies Y(s_i) = M_{r,i}^{-1} B_r $$\n    d.  Project the solution back to the full $n$-dimensional space to obtain the approximate state $\\widetilde{X}(s_i) \\in \\mathbb{C}^{n \\times m}$:\n        $$ \\widetilde{X}(s_i) = V Y(s_i) $$\n    e.  Compute the full-order residual matrix $R(s_i) \\in \\mathbb{C}^{n \\times m}$:\n        $$ R(s_i) = (s_i E - A) \\widetilde{X}(s_i) - B $$\n    f.  Calculate the squared Frobenius norm of the residual, $\\epsilon_i^2 = \\|R(s_i)\\|_F^2$.\n    g.  Add the weighted contribution to the total: $\\mathcal{E}^2_{\\text{total}} = \\mathcal{E}^2_{\\text{total}} + w_i \\epsilon_i^2$.\n5.  The final error indicator is $\\mathcal{E} = \\sqrt{\\mathcal{E}^2_{\\text{total}}}$.\n\nThis algorithm will be implemented to solve the provided test cases. For Test Case 4, where $V = I_n$, we have $r=n$. The reduced system is identical to the full system, leading to $\\widetilde{X}(s) = X(s)$. Consequently, the residual $R(s)$ must be zero for all $s$, and the final indicator $\\mathcal{E}$ must be zero. This serves as a critical check for the correctness of the implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and computes a wideband error indicator for Krylov subspace model reduction.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (happy path, multiple inputs)\n        {\n            \"E\": np.diag([2.0, 1.5, 1.0, 0.8]),\n            \"A\": np.array([\n                [6.0, -1.0, 0.0, 0.0],\n                [-1.0, 5.0, -1.0, 0.0],\n                [0.0, -1.0, 4.5, -0.8],\n                [0.0, 0.0, -0.8, 3.5]\n            ]),\n            \"B\": np.array([\n                [1.0, 0.5],\n                [0.2, -0.3],\n                [0.0, 0.1],\n                [-0.1, 0.2]\n            ]),\n            \"V\": np.array([\n                [1.0, 0.0],\n                [0.0, 1.0],\n                [0.0, 0.0],\n                [0.0, 0.0]\n            ]),\n            \"omegas\": np.array([1.0e8, 2.0e8, 3.0e8]),\n            \"weights\": np.array([2.0e8 / 6, 4 * 2.0e8 / 6, 2.0e8 / 6])\n        },\n        # Test case 2 (identity mass, zero weight edge)\n        {\n            \"E\": np.identity(4),\n            \"A\": np.array([\n                [4.0, -1.0, 0.0, 0.0],\n                [-1.0, 4.0, -1.0, 0.0],\n                [0.0, -1.0, 3.5, -1.0],\n                [0.0, 0.0, -1.0, 3.0]\n            ]),\n            \"B\": np.array([\n                [1.0],\n                [0.0],\n                [0.5],\n                [-0.2]\n            ]),\n            \"V\": 0.5 * np.array([\n                [1.0],\n                [1.0],\n                [1.0],\n                [1.0]\n            ]),\n            \"omegas\": np.array([5.0e7, 1.5e8]),\n            \"weights\": np.array([1.0, 0.0])\n        },\n        # Test case 3 (near-resonant band, single input)\n        {\n            \"E\": np.diag([1.0, 0.8, 0.6]),\n            \"A\": np.array([\n                [2.0, -0.9, 0.0],\n                [-0.9, 2.2, -0.8],\n                [0.0, -0.8, 1.8]\n            ]),\n            \"B\": np.array([\n                [0.3],\n                [0.1],\n                [0.2]\n            ]),\n            \"V\": np.array([\n                [1.0],\n                [0.0],\n                [0.0]\n            ]),\n            \"omegas\": np.array([1.0e7, 7.0e7, 1.3e8, 1.9e8]),\n            \"weights\": np.array([6e7/2, 6e7, 6e7, 6e7/2])\n        },\n        # Test case 4 (consistency check, full-order subspace should give zero residual)\n        {\n            \"E\": np.diag([1.0, 0.8, 0.6]),\n            \"A\": np.array([\n                [2.0, -0.9, 0.0],\n                [-0.9, 2.2, -0.8],\n                [0.0, -0.8, 1.8]\n            ]),\n            \"B\": np.array([\n                [0.3],\n                [0.1],\n                [0.2]\n            ]),\n            \"V\": np.identity(3),\n            \"omegas\": np.array([1.0e7, 7.0e7, 1.3e8, 1.9e8]),\n            \"weights\": np.array([6e7/2, 6e7, 6e7, 6e7/2])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        E, A, B, V = case[\"E\"], case[\"A\"], case[\"B\"], case[\"V\"]\n        omegas, weights = case[\"omegas\"], case[\"weights\"]\n        \n        # 1. Compute reduced system matrices\n        E_r = V.T @ E @ V\n        A_r = V.T @ A @ V\n        B_r = V.T @ B\n\n        total_weighted_sq_err = 0.0\n\n        for omega, w in zip(omegas, weights):\n            if w == 0.0:\n                continue\n\n            # 2. Set complex frequency\n            s = 1j * omega\n\n            # 3. Solve the reduced system\n            # M_r * Y = B_r\n            M_r = s * E_r - A_r\n            try:\n                Y = np.linalg.solve(M_r, B_r)\n            except np.linalg.LinAlgError:\n                # Handle potential singularity, though not expected in these test cases\n                # If singular, the residual norm would be infinite.\n                Y = np.linalg.pinv(M_r) @ B_r\n            \n            # 4. Reconstruct approximate full-order state\n            X_tilde = V @ Y\n\n            # 5. Compute the residual\n            R = (s * E - A) @ X_tilde - B\n\n            # 6. Compute squared Frobenius norm of the residual\n            sq_frob_norm = np.linalg.norm(R, 'fro')**2\n\n            # 7. Add weighted contribution to total error\n            total_weighted_sq_err += w * sq_frob_norm\n\n        # 8. Compute the final indicator\n        indicator = np.sqrt(total_weighted_sq_err)\n        results.append(indicator)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The previous practices focused on *analyzing* model error; this capstone exercise closes the loop by using an error indicator to *actively guide* the construction of the reduced model itself. You will implement an adaptive enrichment strategy for a rational Krylov subspace, where the algorithm iteratively identifies the frequency of maximum error and enriches the basis to specifically target and reduce that error. By completing this practice for a realistic waveguide model , you will build a complete, automated workflow that emulates state-of-the-art techniques for generating efficient and accurate compact models.",
            "id": "3322115",
            "problem": "Implement a complete program that constructs, reduces, and evaluates a second-order linear time-invariant descriptor model for a one-dimensional surrogate of a rectangular waveguide and computes a wideband error indicator that approximates the Hardy space $H_\\infty$ norm of the model reduction error. The model reduction must use an adaptive Krylov subspace enrichment strategy driven by the peak of the sampled frequency-domain error indicator.\n\nStart from the following fundamental base and modeling assumptions that are standard in computational electromagnetics. Maxwell’s equations in a perfectly electrically conducting rectangular waveguide, in a lossless, homogeneous medium with unit permeability and permittivity (so that the wave speed is $c = 1$ in normalized units), admit separation of variables into a cutoff constant associated with the cross section and a propagation equation along the axial coordinate. In a single-mode surrogate along the axial coordinate $z \\in (0,L)$ with cutoff wavenumber $k_c = \\pi/a$ (corresponding to a dominant mode in a rectangular waveguide with wider side length $a$), the transverse field component can be modeled by the scalar second-order system in the frequency domain\n$$\n(K - \\omega^2 M)\\, q(\\omega) = b, \\quad y(\\omega) = c^\\top q(\\omega),\n$$\nwhere $M$ is a symmetric positive definite “mass” operator, $K$ is a symmetric positive definite “stiffness” operator that includes both the axial second derivative and the cutoff term $k_c^2$, $b$ is an input excitation localized near $z=0$, and $c$ extracts the response near $z=L$. The frequency variable is angular frequency $\\omega$ in radians per second. The corresponding single-input/single-output transfer function is\n$$\nG(\\mathrm{j}\\omega) = c^\\top (K - \\omega^2 M)^{-1} b.\n$$\n\nDiscretize the interval $(0,L)$ by $N_z$ interior grid points with spacing $h = L/(N_z+1)$ and impose Dirichlet boundary conditions at $z=0$ and $z=L$ (perfect electric conductor terminations). Use the standard second-order centered finite-difference approximation of the one-dimensional Laplacian to form the stiffness matrix\n$$\nK = T + k_c^2 I, \\quad T = \\frac{1}{h^2}\\,\\mathrm{tridiag}(-1,2,-1)\\in\\mathbb{R}^{N_z\\times N_z}, \\quad k_c = \\frac{\\pi}{a},\n$$\nand set the mass matrix to\n$$\nM = I \\in \\mathbb{R}^{N_z\\times N_z}.\n$$\nUse a unit point input at the leftmost interior node and extract the response at the rightmost interior node, that is\n$$\nb = e_1 \\in \\mathbb{R}^{N_z}, \\quad c = e_{N_z} \\in \\mathbb{R}^{N_z}.\n$$\n\nDefine the wideband error indicator that approximates $\\lVert G - G_r\\rVert_{H_\\infty}$ by uniform sampling of the frequency axis,\n$$\n\\mathcal{E} = \\max_{\\omega\\in\\Omega_{\\mathrm{s}}} \\left| G(\\mathrm{j}\\omega) - G_r(\\mathrm{j}\\omega)\\right|,\n$$\nwhere $\\Omega_{\\mathrm{s}} = \\{\\omega_1,\\dots,\\omega_{N_\\Omega}\\}$ is a user-specified set of angular frequencies in radians per second, $G(\\mathrm{j}\\omega)$ is the full transfer function, and $G_r(\\mathrm{j}\\omega)$ is the reduced transfer function.\n\nConstruct the reduced model by a projection onto a rational Krylov subspace for the second-order form, built from frequency-shifted solutions at adaptively selected frequencies. Specifically, for each selected angular frequency $\\omega_k$, form the vector\n$$\nv_k = (K - \\omega_k^2 M)^{-1} b,\n$$\northonormalize it against the current basis $\\{v_1,\\dots,v_{k-1}\\}$ in the standard Euclidean inner product, and augment the basis matrix $V = [v_1,\\dots,v_r] \\in \\mathbb{C}^{N_z\\times r}$. Then define the reduced operators and reduced input and output as\n$$\nK_r = V^H K V,\\quad M_r = V^H M V,\\quad b_r = V^H b,\\quad c_r^\\top = c^\\top V,\n$$\nand the reduced transfer function as\n$$\nG_r(\\mathrm{j}\\omega) = c_r^\\top (K_r - \\omega^2 M_r)^{-1} b_r.\n$$\nUse the following adaptive enrichment strategy:\n- Initialize the basis with one vector computed at the midpoint frequency in the sampled band.\n- At each iteration, evaluate the sampled indicator $\\left| G(\\mathrm{j}\\omega) - G_r(\\mathrm{j}\\omega)\\right|$ over the entire sampled set $\\Omega_{\\mathrm{s}}$, find the peak frequency $\\omega_\\star$ that maximizes the indicator, and enrich the basis with the new vector $(K - \\omega_\\star^2 M)^{-1} b$ (orthonormalized).\n- Stop when the current maximum sampled indicator is below a given tolerance or when the maximum allowed basis size is reached.\n\nTo avoid exact singularities at resonance, add an infinitesimal positive loss in the sampling and enrichment solves by replacing $K - \\omega^2 M$ with $K - (1 - \\mathrm{j}\\eta)\\omega^2 M$, where $\\eta$ is a fixed small positive scalar.\n\nAngle units must be in radians per second. All other quantities are non-dimensional in the normalized unit system. The final answers required for each test are pure numbers (floats and integers) with no unit labels in the output.\n\nYour program must implement the above and run the following test suite. For each test case, construct the model, run the adaptive enrichment, and report a triple consisting of:\n- the final maximum sampled indicator $\\mathcal{E}$ as a float,\n- the angular frequency $\\omega_\\star$ at which the last iteration’s indicator attains its maximum as a float,\n- the final reduced basis size $r$ as an integer.\n\nEach test case is specified by the tuple $(N_z, L, a, \\omega_{\\min}, \\omega_{\\max}, N_\\Omega, \\text{tol}, r_{\\max})$:\n1. $(60, 1.0, 1.0, 2.0, 3.5, 40, 1\\times 10^{-5}, 6)$\n2. $(60, 1.0, 1.0, 4.0, 6.0, 60, 1\\times 10^{-4}, 12)$\n3. $(60, 1.0, 1.0, 6.5, 8.5, 60, 1\\times 10^{-4}, 14)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list in the order $[\\mathcal{E}, \\omega_\\star, r]$ for the corresponding test case. For example, a syntactically valid output for three tests would look like\n\"[ [0.0123,4.567,5],[0.001,5.432,7],[0.1,7.0,8] ]\"\nbut with the actual computed values from your implementation. The angular frequencies must be in radians per second. No additional text should be printed.",
            "solution": "The posed problem is valid as it is scientifically grounded, well-posed, objective, and complete, barring a minor standard numerical parameter which can be assumed. It requires the implementation of an adaptive model order reduction scheme for a second-order system, a well-established procedure in computational science and engineering.\n\nThe objective is to construct a reduced-order model (ROM) for a one-dimensional finite-difference discretization of a surrogate waveguide model. This reduction is to be performed using an adaptive rational Krylov subspace method, where the subspace is enriched iteratively by selecting new expansion frequencies that correspond to the peaks in the frequency-domain error between the full-order model (FOM) and the ROM.\n\n### 1. Full-Order Model (FOM) Formulation\n\nThe problem begins with a second-order linear time-invariant system in the frequency domain, representing the transverse field component in a single-mode waveguide:\n$$\n(K - \\omega^2 M)\\, q(\\omega) = b, \\quad y(\\omega) = c^\\top q(\\omega)\n$$\nThe system's frequency response is characterized by the transfer function $G(\\mathrm{j}\\omega) = c^\\top (K - \\omega^2 M)^{-1} b$.\n\nThe spatial domain is the interval $z \\in (0, L)$, which is discretized using $N_z$ interior grid points. The grid spacing is $h = L / (N_z + 1)$. The model employs Dirichlet boundary conditions at $z=0$ and $z=L$.\n\nThe system matrices are defined as follows:\n-   The **mass matrix** $M$ is the identity matrix, $M = I \\in \\mathbb{R}^{N_z \\times N_z}$.\n-   The **stiffness matrix** $K \\in \\mathbb{R}^{N_z \\times N_z}$ is given by $K = T + k_c^2 I$, where $k_c = \\pi/a$ is the cutoff wavenumber. The matrix $T$ is the standard second-order centered finite-difference approximation of the negative second derivative operator, scaled by $1/h^2$:\n    $$\n    T = \\frac{1}{h^2}\n    \\begin{pmatrix}\n    2  -1   \\\\\n    -1  2  -1  \\\\\n     \\ddots  \\ddots  \\ddots \\\\\n      -1  2  -1 \\\\\n       -1  2\n    \\end{pmatrix}\n    $$\n    Both $K$ and $M$ are symmetric and positive definite.\n-   The **input vector** $b$ represents a point source excitation at the first interior node, $b = e_1 = [1, 0, \\dots, 0]^\\top \\in \\mathbb{R}^{N_z}$.\n-   The **output vector** $c$ represents a probe at the last interior node, $c = e_{N_z} = [0, \\dots, 0, 1]^\\top \\in \\mathbb{R}^{N_z}$.\n\n### 2. Model Reduction by Projection\n\nModel order reduction aims to find a low-dimensional approximation of the FOM. We use a projection-based method where the state vector $q(\\omega) \\in \\mathbb{C}^{N_z}$ is approximated within a low-dimensional subspace. This subspace is spanned by the columns of an orthonormal matrix $V \\in \\mathbb{C}^{N_z \\times r}$, where $r \\ll N_z$ is the reduced order. The approximation is $q(\\omega) \\approx V z(\\omega)$, where $z(\\omega) \\in \\mathbb{C}^r$ is the reduced state vector.\n\nSubstituting this ansatz into the FOM equation and applying a Galerkin projection (enforcing the residual to be orthogonal to the subspace spanned by $V$) yields the ROM:\n$$\nV^H (K - \\omega^2 M) V z(\\omega) = V^H b\n$$\n$$\n(V^H K V - \\omega^2 V^H M V) z(\\omega) = V^H b\n$$\nThis defines the reduced system matrices:\n-   Reduced stiffness matrix: $K_r = V^H K V \\in \\mathbb{C}^{r \\times r}$\n-   Reduced mass matrix: $M_r = V^H M V \\in \\mathbb{C}^{r \\times r}$\n-   Reduced input vector: $b_r = V^H b \\in \\mathbb{C}^r$\n\nThe reduced output is $y_r(\\omega) = c^\\top (V z(\\omega)) = (c^\\top V) z(\\omega) = c_r^\\top z(\\omega)$, with the reduced output vector defined as $c_r = V^H c \\in \\mathbb{C}^r$. Note the use of the conjugate transpose to form $c_r$, which is required for consistency when $V$ is complex.\n\nThe reduced transfer function is then:\n$$\nG_r(\\mathrm{j}\\omega) = c_r^\\top (K_r - \\omega^2 M_r)^{-1} b_r\n$$\n\n### 3. Adaptive Rational Krylov Method\n\nThe quality of the ROM depends critically on the choice of the projection basis $V$. The problem specifies an adaptive rational Krylov method. The basis vectors are generated from solutions of the FOM at specific, adaptively chosen angular frequencies (expansion points) $\\omega_k$:\n$$\nv'_k = (K - \\omega_k^2 M)^{-1} b\n$$\nThese vectors are then orthonormalized to form the basis $V = [v_1, v_2, \\dots, v_r]$. We use the numerically stable Modified Gram-Schmidt (MGS) algorithm for orthonormalization. For a new vector $v'$, MGS proceeds by sequentially removing its projection onto each existing basis vector.\n\nThe adaptive procedure is as follows:\n1.  **Initialization**: The frequency band of interest is $[\\omega_{\\min}, \\omega_{\\max}]$, sampled at $N_\\Omega$ points to form the set $\\Omega_s$. The initial basis consists of a single vector ($r=1$) generated at the midpoint frequency, $\\omega_1 = (\\omega_{\\min} + \\omega_{\\max}) / 2$.\n2.  **Iteration**: The algorithm proceeds iteratively to enrich the basis. In each iteration, with a basis of size $r$:\n    a. The ROM $\\{K_r, M_r, b_r, c_r\\}$ is constructed.\n    b. The error indicator $\\mathcal{E}$ is computed by finding the maximum absolute difference between the full and reduced transfer functions over the sample frequencies:\n       $$\n       \\mathcal{E} = \\max_{\\omega \\in \\Omega_s} |G(\\mathrm{j}\\omega) - G_r(\\mathrm{j}\\omega)|\n       $$\n    c. The frequency $\\omega_\\star \\in \\Omega_s$ where this maximum error occurs is identified.\n    d. **Stopping Criteria Check**: If $\\mathcal{E}$ is below a specified tolerance `tol` or if the basis size $r$ has reached the maximum allowed size `r_max`, the algorithm terminates.\n    e. **Enrichment**: If the stopping criteria are not met, the basis is augmented with a new vector. This new vector is generated using $\\omega_\\star$ as the next expansion point. The raw vector $(K - \\omega_\\star^2 M)^{-1} b$ is computed and orthonormalized against the existing basis vectors using MGS to produce $v_{r+1}$. The basis is updated, $V \\leftarrow [V, v_{r+1}]$, and its size becomes $r+1$.\n\n### 4. Numerical Implementation\n\nTo avoid singularities when an expansion frequency $\\omega_k$ coincides with a natural frequency of the undamped system (a resonance), a small artificial damping term is introduced. The system matrix is modified to $K - (1 - \\mathrm{j}\\eta)\\omega^2 M$, where $\\eta$ is a small positive scalar. For this problem, a standard value of $\\eta = 10^{-8}$ is chosen. This modification makes the system matrices complex, and consequently the basis $V$ and the reduced matrices will be complex.\n\nThe solution algorithm is:\n1.  For each test case, construct the FOM matrices $K, M, b, c$. $K$ is constructed as a sparse matrix.\n2.  Pre-compute the FOM transfer function $G(\\mathrm{j}\\omega)$ for all $\\omega \\in \\Omega_s$. This is done by solving the complex linear system $(K - (1 - \\mathrm{j}\\eta)\\omega^2 M)q(\\omega)=b$ for each frequency.\n3.  Initialize the basis $V$ with one vector generated at the band's midpoint frequency. Set $r=1$.\n4.  Enter the adaptive loop:\n    - Construct the current $r \\times r$ ROM.\n    - Evaluate the ROM transfer function $G_r(\\mathrm{j}\\omega)$ over $\\Omega_s$. This involves solving a small $r \\times r$ linear system for each frequency.\n    - Compute the error indicator $\\mathcal{E}$ and its peak frequency $\\omega_\\star$.\n    - If $\\mathcal{E}  \\text{tol}$ or $r \\ge r_{\\max}$, exit the loop and report $(\\mathcal{E}, \\omega_\\star, r)$.\n    - Otherwise, enrich the basis to size $r+1$ using $\\omega_\\star$ as the expansion point and the MGS procedure. Increment $r$ and continue the loop.\n\nThis process yields the final error, the peak frequency of that error, and the final size of the reduced model for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.sparse import linalg as sla\n\ndef build_fom(Nz, L, a):\n    \"\"\"Builds the full-order model matrices and vectors.\"\"\"\n    h = L / (Nz + 1)\n    kc = np.pi / a\n\n    # Stiffness matrix T for the 1D Laplacian\n    diagonals = [-np.ones(Nz-1), 2*np.ones(Nz), -np.ones(Nz-1)]\n    T = sparse.diags(diagonals, [-1, 0, 1], shape=(Nz, Nz), format='csc') / (h**2)\n    \n    # K and M matrices\n    I_Nz = sparse.identity(Nz, format='csc')\n    K = T + (kc**2) * I_Nz\n    M = I_Nz.copy()\n\n    # Input and output vectors\n    b = np.zeros(Nz)\n    b[0] = 1.0\n    c = np.zeros(Nz)\n    c[Nz-1] = 1.0\n\n    return K, M, b, c\n\ndef evaluate_transfer_function(sys_mat, b, c, omega_samples, eta, is_fom=True):\n    \"\"\"Evaluates the transfer function G(jw) for a set of frequencies.\"\"\"\n    K, M = sys_mat\n    Nz = K.shape[0]\n    G_values = np.zeros(len(omega_samples), dtype=np.complex128)\n\n    if is_fom:\n        # For FOM, K and M are sparse\n        for i, w in enumerate(omega_samples):\n            if w == 0:\n                # Handle DC case, though not expected in test cases\n                A = K\n            else:\n                A = K - (1 - 1j * eta) * (w**2) * M\n            # Use sparse solver for the large system\n            q = sla.spsolve(A, b)\n            G_values[i] = c.T @ q\n    else:\n        # For ROM, Kr and Mr are small dense matrices\n        Kr, Mr = sys_mat\n        br, cr_T = b, c\n        for i, w in enumerate(omega_samples):\n            if w == 0:\n                Ar = Kr\n            else:\n                Ar = Kr - (w**2) * Mr\n            # Use dense solver for the small system\n            zr = np.linalg.solve(Ar, br)\n            G_values[i] = cr_T @ zr\n            \n    return G_values\n\ndef run_adaptive_reduction(Nz, L, a, w_min, w_max, Nw, tol, r_max):\n    \"\"\"Performs the adaptive Krylov subspace model reduction.\"\"\"\n    eta = 1e-8\n    \n    # 1. Setup FOM and frequency samples\n    K, M, b, c = build_fom(Nz, L, a)\n    omega_samples = np.linspace(w_min, w_max, Nw)\n\n    # 2. Pre-compute FOM response\n    G_full = evaluate_transfer_function((K, M), b, c, omega_samples, eta, is_fom=True)\n\n    # 3. Initialization (r=1)\n    w_expand = (w_min + w_max) / 2.0\n    A_expand = K - (1 - 1j * eta) * (w_expand**2) * M\n    v_raw = sla.spsolve(A_expand, b)\n    v_new = v_raw / np.linalg.norm(v_raw)\n    V = v_new.reshape(-1, 1)\n    r = 1\n\n    # 4. Adaptive loop\n    while True:\n        # Project FOM to create ROM\n        br = V.T.conj() @ b\n        cr_T = c.T @ V # c is real, so c.T is fine\n        Kr = V.T.conj() @ K @ V\n        Mr = V.T.conj() @ M @ V\n\n        # Evaluate ROM response\n        G_reduced = evaluate_transfer_function((Kr, Mr), br, cr_T, omega_samples, eta, is_fom=False)\n\n        # Compute error\n        error_indicator = np.abs(G_full - G_reduced)\n        max_error = np.max(error_indicator)\n        w_star_idx = np.argmax(error_indicator)\n        w_star = omega_samples[w_star_idx]\n\n        # Check stopping criteria\n        if max_error  tol or r >= r_max:\n            return max_error, w_star, r\n\n        # Enrich basis at peak error frequency\n        w_expand = w_star\n        A_expand = K - (1 - 1j * eta) * (w_expand**2) * M\n        v_raw = sla.spsolve(A_expand, b)\n        \n        # Modified Gram-Schmidt orthonormalization\n        w = v_raw.copy()\n        for i in range(r):\n            v_i = V[:, i]\n            proj_coeff = np.vdot(v_i, w)\n            w -= proj_coeff * v_i\n        \n        norm_w = np.linalg.norm(w)\n        if norm_w  1e-10: \n            # Vector is linearly dependent, stop enrichment\n            return max_error, w_star, r\n            \n        v_new = w / norm_w\n        V = np.hstack((V, v_new.reshape(-1, 1)))\n        r += 1\n\n\ndef solve():\n    test_cases = [\n        # (Nz, L, a, w_min, w_max, Nw, tol, r_max)\n        (60, 1.0, 1.0, 2.0, 3.5, 40, 1e-5, 6),\n        (60, 1.0, 1.0, 4.0, 6.0, 60, 1e-4, 12),\n        (60, 1.0, 1.0, 6.5, 8.5, 60, 1e-4, 14),\n    ]\n\n    results = []\n    for case in test_cases:\n        E, w_star, r = run_adaptive_reduction(*case)\n        results.append([E, w_star, r])\n\n    # Format output string manually to match the required format\n    # \"[ [E1,w1,r1],[E2,w2,r2],... ]\"\n    results_str_list = []\n    for res in results:\n        # res[0] is E (float), res[1] is w_star (float), res[2] is r (int)\n        results_str_list.append(f\"[{res[0]},{res[1]},{res[2]}]\")\n    final_output = f\"[{','.join(results_str_list)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}