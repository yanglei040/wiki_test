{
    "hands_on_practices": [
        {
            "introduction": "Many challenges in computational electromagnetics, such as material characterization and tomographic imaging, can be framed as inverse problems. We seek to infer underlying physical parameters (like permittivity, $\\epsilon$) by minimizing the mismatch between simulated and measured data. While gradient-based optimization is a powerful tool for this, computing the gradient of the misfit with respect to a high-dimensional parameter field can be computationally intractable. This exercise introduces the adjoint-state method, an elegant and highly efficient technique for calculating such gradients at a cost independent of the number of parameters, making large-scale, physics-constrained machine learning feasible .",
            "id": "3327841",
            "problem": "Consider an inverse-scattering setup where the spatially varying electric permittivity $ \\epsilon(\\mathbf{r}) $ in a bounded domain $ \\Omega \\subset \\mathbb{R}^{3} $ is to be inferred using a machine-learning model driven by data misfit minimization. The computational physics forward model is the scalar Helmholtz equation for a time-harmonic field $ u(\\mathbf{r}) $ at angular frequency $ \\omega $, with real, positive magnetic permeability $ \\mu $ and real, positive permittivity $ \\epsilon(\\mathbf{r}) $. For each experiment indexed by $ m = 1, \\dots, N_{s} $, the forward field $ u_{m}(\\mathbf{r}) $ satisfies\n$$\n\\left(\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})\\right) u_{m}(\\mathbf{r}) = s_{m}(\\mathbf{r}) \\quad \\text{in } \\Omega,\n$$\nwhere $ s_{m}(\\mathbf{r}) $ is a known source distribution. Assume homogeneous absorbing boundary conditions on $ \\partial \\Omega $ that enforce a well-posed radiation condition and make the boundary terms vanish in the adjoint-state derivation. Measurements are acquired at receiver locations $ \\{\\mathbf{r}_{j}\\}_{j=1}^{N_{r}} $ via a linear sampling operator $ \\mathcal{C} $ defined by\n$$\n\\mathcal{C} u_{m} = \\left(u_{m}(\\mathbf{r}_{1}), \\dots, u_{m}(\\mathbf{r}_{N_{r}})\\right)^{\\top} \\in \\mathbb{C}^{N_{r}}.\n$$\nGiven complex-valued observed data vectors $ \\mathbf{d}_{m} \\in \\mathbb{C}^{N_{r}} $, the data misfit functional is\n$$\nJ(\\epsilon) = \\frac{1}{2} \\sum_{m=1}^{N_{s}} \\left\\| \\mathcal{C} u_{m} - \\mathbf{d}_{m} \\right\\|_{2}^{2},\n$$\nwhere $ \\|\\cdot\\|_{2} $ denotes the Euclidean norm induced by the Hermitian inner product on $ \\mathbb{C}^{N_{r}} $. The training objective in the machine-learning framework is to minimize $ J(\\epsilon) $ with respect to the field parameter $ \\epsilon(\\mathbf{r}) $.\n\nUsing the adjoint-state method, derive the functional gradient $ \\nabla_{\\epsilon} J(\\mathbf{r}) $ with respect to $ \\epsilon(\\mathbf{r}) $ under the assumption that $ \\mu $ and $ \\omega $ are fixed and real. In your derivation, start from the scalar Helmholtz equation and the definition of $ J(\\epsilon) $, and use only integration-by-parts and adjoint-operator identities consistent with the stated boundary conditions. Explicitly write the forward and adjoint field equations, including their right-hand sides. Express the final gradient as a single closed-form analytic expression in terms of the forward fields $ u_{m} $ and the adjoint fields $ p_{m} $.\n\nYour final answer must be a single closed-form analytic expression. Do not include units in the final answer. If you introduce any angles, they must be in radians. No rounding is required for the final analytical expression.",
            "solution": "The problem requires the derivation of the functional gradient of a data misfit functional $J(\\epsilon)$ with respect to the spatially varying permittivity $ \\epsilon(\\mathbf{r}) $. The adjoint-state method is the specified technique for this derivation. The functional gradient, denoted $ \\nabla_{\\epsilon} J(\\mathbf{r}) $, is defined by the first variation of the functional $J$, such that for a small perturbation $ \\delta\\epsilon(\\mathbf{r}) $, the change in $J$ is given by:\n$$ \\delta J = J(\\epsilon + \\delta\\epsilon) - J(\\epsilon) = \\int_{\\Omega} \\nabla_{\\epsilon} J(\\mathbf{r}) \\, \\delta\\epsilon(\\mathbf{r}) \\, d\\mathbf{r} + O(\\|\\delta\\epsilon \\|^2) $$\nOur goal is to find an expression for $ \\nabla_{\\epsilon} J(\\mathbf{r}) $ by relating a perturbation $ \\delta\\epsilon $ to the resulting change $ \\delta J $.\n\nThe misfit functional $J(\\epsilon)$ is given by:\n$$ J(\\epsilon) = \\frac{1}{2} \\sum_{m=1}^{N_{s}} \\left\\| \\mathcal{C} u_{m} - \\mathbf{d}_{m} \\right\\|_{2}^{2} $$\nwhere $u_m$ implicitly depends on $\\epsilon$ through the scalar Helmholtz equation. The norm is the Euclidean norm on $ \\mathbb{C}^{N_{r}} $, induced by the Hermitian inner product, $ \\|\\mathbf{v}\\|_{2}^{2} = \\mathbf{v}^{H}\\mathbf{v} $. Thus, we can write $J(\\epsilon)$ as:\n$$ J(\\epsilon) = \\frac{1}{2} \\sum_{m=1}^{N_{s}} (\\mathcal{C} u_{m} - \\mathbf{d}_{m})^{H} (\\mathcal{C} u_{m} - \\mathbf{d}_{m}) $$\n\nFirst, we compute the first variation of $J$ with respect to a variation in the fields $ u_m $. A small perturbation $ \\epsilon \\rightarrow \\epsilon + \\delta\\epsilon $ induces a perturbation in the field $ u_m \\rightarrow u_m + \\delta u_m $. The corresponding first-order variation in $J$ is:\n$$ \\delta J = \\frac{1}{2} \\sum_{m=1}^{N_s} \\left[ (\\mathcal{C}\\delta u_m)^H (\\mathcal{C}u_m - \\mathbf{d}_m) + (\\mathcal{C}u_m - \\mathbf{d}_m)^H (\\mathcal{C}\\delta u_m) \\right] $$\nThe two terms in the bracket are complex conjugates of each other. Therefore, their sum is $2 \\text{Re} [(\\mathcal{C}u_m - \\mathbf{d}_m)^H (\\mathcal{C}\\delta u_m)] $.\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\left[ (\\mathcal{C}u_m - \\mathbf{d}_m)^H (\\mathcal{C}\\delta u_m) \\right] $$\nThis expression can be written using the definition of the measurement operator $ \\mathcal{C} $. Let $ (\\mathcal{C}u_m)_j = u_m(\\mathbf{r}_j) $. Let $ (\\mathbf{d}_m)_j = d_{mj} $.\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\left[ \\sum_{j=1}^{N_r} \\overline{(u_m(\\mathbf{r}_j) - d_{mj})} \\, \\delta u_m(\\mathbf{r}_j) \\right] $$\n\nNext, we relate the field perturbation $ \\delta u_m $ to the parameter perturbation $ \\delta\\epsilon $. The forward field $ u_m $ satisfies the state equation:\n$$ L_\\epsilon u_m(\\mathbf{r}) \\equiv \\left(\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})\\right) u_{m}(\\mathbf{r}) = s_{m}(\\mathbf{r}) $$\nThe perturbed field $ u_m + \\delta u_m $ satisfies the same equation with the perturbed parameter $ \\epsilon + \\delta\\epsilon $:\n$$ \\left(\\nabla^{2} + \\omega^{2} \\mu (\\epsilon(\\mathbf{r}) + \\delta\\epsilon(\\mathbf{r}))\\right) (u_{m}(\\mathbf{r}) + \\delta u_m(\\mathbf{r})) = s_{m}(\\mathbf{r}) $$\nExpanding this equation and keeping only terms of first order in $ \\delta\\epsilon $ and $ \\delta u_m $:\n$$ (\\nabla^{2} + \\omega^{2} \\mu \\epsilon) u_m + (\\nabla^{2} + \\omega^{2} \\mu \\epsilon) \\delta u_m + (\\omega^{2} \\mu \\delta\\epsilon) u_m + O(\\delta\\epsilon \\delta u_m) = s_m $$\nSubtracting the original state equation, $ (\\nabla^{2} + \\omega^{2} \\mu \\epsilon) u_m = s_m $, we obtain the linearized forward equation, also known as the sensitivity equation:\n$$ L_\\epsilon(\\delta u_m) = (\\nabla^{2} + \\omega^{2} \\mu \\epsilon) \\delta u_m = -\\omega^2 \\mu \\delta\\epsilon \\, u_m $$\n\nNow we introduce the adjoint-state method to eliminate $ \\delta u_m $ from the expression for $ \\delta J $. We define an adjoint field $ p_m(\\mathbf{r}) $ for each experiment $m$. We use the standard $ L^2(\\Omega) $ inner product for complex-valued functions, $ \\langle f, g \\rangle = \\int_{\\Omega} f(\\mathbf{r})\\overline{g(\\mathbf{r})} \\, d\\mathbf{r} $. The adjoint of an operator $ A $ is denoted $ A^\\dagger $ and satisfies $ \\langle A f, g \\rangle = \\langle f, A^\\dagger g \\rangle $. The operator $ L_\\epsilon = \\nabla^2 + \\omega^2\\mu\\epsilon $ is self-adjoint, $ L_\\epsilon^\\dagger = L_\\epsilon $, because $ \\omega, \\mu, \\epsilon $ are real and the problem statement guarantees that boundary terms from integration by parts vanish.\n\nWe can express $ \\delta J $ using an integral over $ \\Omega $ by introducing Dirac delta functions:\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\left[ \\int_{\\Omega} \\delta u_m(\\mathbf{r}) \\left( \\sum_{j=1}^{N_r} \\overline{(u_m(\\mathbf{r}_j) - d_{mj})} \\delta(\\mathbf{r} - \\mathbf{r}_j) \\right) d\\mathbf{r} \\right] $$\nThis can be written in terms of the inner product as:\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\langle \\delta u_m, q_m \\rangle $$\nwhere $ q_m(\\mathbf{r}) $ is the adjoint source for the $m$-th experiment:\n$$ q_m(\\mathbf{r}) = \\sum_{j=1}^{N_r} \\overline{(u_m(\\mathbf{r}_j) - d_{mj})} \\delta(\\mathbf{r} - \\mathbf{r}_j) $$\nFrom the sensitivity equation, we can write $ \\delta u_m = -L_\\epsilon^{-1}(\\omega^2 \\mu \\delta\\epsilon \\, u_m) $. Substituting this into the expression for $ \\delta J $:\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\langle -L_\\epsilon^{-1}(\\omega^2 \\mu \\delta\\epsilon \\, u_m), q_m \\rangle $$\nUsing the property of the adjoint operator, $ \\langle A f, g \\rangle = \\langle f, A^\\dagger g \\rangle $, we move $ L_\\epsilon^{-1} $ to the other side of the inner product:\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\langle \\omega^2 \\mu \\delta\\epsilon \\, u_m, -(L_\\epsilon^{-1})^\\dagger q_m \\rangle $$\nWe now define the adjoint field $ p_m $ as the solution to $ p_m = (L_\\epsilon^{-1})^\\dagger q_m $. This is equivalent to solving the adjoint equation $ L_\\epsilon^\\dagger p_m = q_m $. Since $ L_\\epsilon^\\dagger = L_\\epsilon $, the adjoint problem is:\n$$ L_\\epsilon p_m(\\mathbf{r}) = (\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})) p_{m}(\\mathbf{r}) = q_m(\\mathbf{r}) $$\nSubstituting $ p_m $ back into the expression for $ \\delta J $:\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\langle \\omega^2 \\mu \\delta\\epsilon \\, u_m, -p_m \\rangle $$\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\left[ \\int_{\\Omega} (\\omega^2 \\mu \\delta\\epsilon \\, u_m) \\overline{(-p_m)} \\, d\\mathbf{r} \\right] $$\nSince $ \\omega, \\mu $ are real constants and $ \\delta\\epsilon $ is a real perturbation, we can pull them out of the real part operator:\n$$ \\delta J = \\sum_{m=1}^{N_s} \\left( -\\omega^2 \\mu \\int_{\\Omega} \\delta\\epsilon(\\mathbf{r}) \\, \\text{Re} \\left[ u_m(\\mathbf{r}) \\overline{p_m(\\mathbf{r})} \\right] \\, d\\mathbf{r} \\right) $$\nInterchanging the summation and integration:\n$$ \\delta J = \\int_{\\Omega} \\left( - \\omega^2 \\mu \\sum_{m=1}^{N_s} \\text{Re} \\left[ u_m(\\mathbf{r}) \\overline{p_m(\\mathbf{r})} \\right] \\right) \\delta\\epsilon(\\mathbf{r}) \\, d\\mathbf{r} $$\nBy comparing this with the definition $ \\delta J = \\int_{\\Omega} \\nabla_{\\epsilon} J(\\mathbf{r}) \\, \\delta\\epsilon(\\mathbf{r}) \\, d\\mathbf{r} $, we can identify the functional gradient.\n\nThe required elements are:\nThe forward field equations are, for $ m=1, \\dots, N_s $:\n$$ \\left(\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})\\right) u_{m}(\\mathbf{r}) = s_{m}(\\mathbf{r}) \\quad \\text{in } \\Omega $$\nThe adjoint field equations are, for $ m=1, \\dots, N_s $:\n$$ \\left(\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})\\right) p_{m}(\\mathbf{r}) = \\sum_{j=1}^{N_r} \\overline{(u_{m}(\\mathbf{r}_{j}) - d_{mj})} \\delta(\\mathbf{r} - \\mathbf{r}_{j}) \\quad \\text{in } \\Omega $$\nThe final expression for the functional gradient is:\n$$ \\nabla_{\\epsilon} J(\\mathbf{r}) = - \\omega^2 \\mu \\sum_{m=1}^{N_s} \\text{Re} \\left[ u_m(\\mathbf{r}) \\overline{p_m(\\mathbf{r})} \\right] $$\nThe real part can also be written as $ \\text{Re}[u_m \\overline{p_m}] = \\text{Re}[u_m] \\text{Re}[p_m] + \\text{Im}[u_m] \\text{Im}[p_m] $. The expression is given in its most compact form.",
            "answer": "$$ \\boxed{- \\omega^{2} \\mu \\sum_{m=1}^{N_{s}} \\text{Re} \\left[ u_{m}(\\mathbf{r}) \\overline{p_{m}(\\mathbf{r})} \\right]} $$"
        },
        {
            "introduction": "A powerful strategy in scientific machine learning is to imbue model architectures with inductive biases that reflect the underlying physics. For problems defined on meshes, Graph Neural Networks (GNNs) offer a natural framework. This practice reveals a profound connection between GNNs and the Finite Element Method (FEM), demonstrating that the message-passing operations in a GNN can be designed to exactly mirror the mathematical structure of the FEM assembly process for differential operators like the curl-curl operator . By building the physics directly into the network layers, we create more efficient, robust, and generalizable models.",
            "id": "3327879",
            "problem": "You are to design and implement a single-layer graph neural network that exactly mirrors the finite element method assembly of the curl–curl bilinear form for a two-dimensional discretization using lowest-order edge degrees of freedom. Begin from a valid fundamental base in computational electromagnetics and derive the message passing rule. Then encode Perfect Electric Conductor (PEC) boundary conditions in the graph so that boundary edges are clamped to zero during the update. Finally, implement a complete program that computes the layer output for a given small mesh and several parameter sets and prints the results in the specified format.\n\nStart from the time-harmonic Maxwell curl–curl operator written in weak form: given a vector test function $v$ and an electric field $E$, the bilinear form is\n$$\na(E,v) = \\int_{\\Omega} \\mu^{-1} \\left( \\nabla \\times E \\right) \\cdot \\left( \\nabla \\times v \\right) \\, d\\Omega,\n$$\nwhere $\\mu$ is the magnetic permeability and $\\Omega$ is the computational domain. In a lowest-order edge-based discretization (compatible with the Sobolev space $H(\\mathrm{curl})$), associate one degree of freedom to each oriented edge as the line integral of the tangential component of $E$ along that edge. On a mesh composed of cells with piecewise constant $\\mu$ and area $A$, define the discrete curl per cell $k$ as\n$$\nc_k(x) = \\frac{1}{A_k} \\sum_{e \\in \\partial k} s_{k,e} \\, x_e,\n$$\nwhere $x_e$ is the edge degree of freedom (a dimensionless quantity for this problem), $A_k$ is the area of cell $k$, and $s_{k,e} \\in \\{+1,-1\\}$ is the sign induced by comparing the global orientation of edge $e$ with the chosen counterclockwise orientation of the boundary of cell $k$. With cell-wise constant $\\mu_k$, the assembled global stiffness (curl–curl) matrix $K$ has the structure\n$$\nK = C^{\\top} \\, \\mathrm{diag}\\!\\left( \\frac{\\mu_1^{-1}}{A_1}, \\ldots, \\frac{\\mu_K^{-1}}{A_K} \\right) \\, C,\n$$\nwhere $C \\in \\mathbb{R}^{K \\times E}$ is the signed cell–edge incidence matrix with entries $C_{k,e} = s_{k,e}$, $K$ is the number of cells, and $E$ is the number of edges. In other words, the bilinear form reduces to\n$$\na(x,y) = x^{\\top} K \\, y.\n$$\n\nConstruct a graph $\\mathcal{G}$ where each node corresponds to one oriented mesh edge, and connect two nodes if they co-bound at least one cell. The single-layer graph neural network must implement the message passing\n$$\ny = \\left( \\mathbf{1} - m \\right) \\odot \\left( K x \\right),\n$$\nwhere $x \\in \\mathbb{R}^E$ is the input edge feature vector (dimensionless), $y \\in \\mathbb{R}^E$ is the output, $m \\in \\{0,1\\}^E$ is a binary boundary mask that encodes Perfect Electric Conductor boundary conditions ($m_e = 1$ for boundary edges and $m_e = 0$ for interior edges), and $\\odot$ denotes elementwise multiplication. This enforces the Dirichlet condition by clamping boundary nodes to zero.\n\nUse the following fixed mesh, geometry, and orientation:\n- Domain is subdivided into $2$ axis-aligned unit square cells: a left cell $k=0$ with area $A_0 = 1$ and a right cell $k=1$ with area $A_1 = 1$.\n- The unique oriented edges are indexed and oriented as follows, all lengths equal to $1$ (dimensionless units):\n  - $e_0$: bottom-left edge from $(0,0)$ to $(1,0)$ (global orientation along $+x$).\n  - $e_1$: bottom-right edge from $(1,0)$ to $(2,0)$ (global orientation along $+x$).\n  - $e_2$: middle vertical edge from $(1,0)$ to $(1,1)$ (global orientation along $+y$).\n  - $e_4$: top-left edge from $(0,1)$ to $(1,1)$ (global orientation along $+x$).\n  - $e_5$: top-right edge from $(1,1)$ to $(2,1)$ (global orientation along $+x$).\n  - $e_6$: left boundary vertical edge from $(0,0)$ to $(0,1)$ (global orientation along $+y$).\n  - $e_7$: right boundary vertical edge from $(2,0)$ to $(2,1)$ (global orientation along $+y$).\n- The counterclockwise per-cell boundary traversal induces the signed incidence matrix $C \\in \\mathbb{R}^{2 \\times 7}$ with nonzero entries\n  - For the left cell $k=0$: $C_{0,0} = +1$, $C_{0,2} = +1$, $C_{0,4} = -1$, $C_{0,6} = -1$.\n  - For the right cell $k=1$: $C_{1,1} = +1$, $C_{1,7} = +1$, $C_{1,5} = -1$, $C_{1,2} = -1$.\nAll other entries of $C$ are $0$.\n\nDefine the boundary mask $m \\in \\{0,1\\}^7$ with $m_e = 1$ for boundary edges and $m_e = 0$ for interior edges. In this mesh, $e_2$ is the unique interior edge, so choose\n$$\nm = [1,\\, 1,\\, 0,\\, 1,\\, 1,\\, 1,\\, 1],\n$$\nordered by edge index $[e_0, e_1, e_2, e_4, e_5, e_6, e_7]$.\n\nYour program must:\n- Assemble $K$ from $C$ and $(\\mu_0, \\mu_1)$ using the formula $K = C^{\\top} \\, \\mathrm{diag}\\!\\left( \\mu_0^{-1}/A_0,\\, \\mu_1^{-1}/A_1 \\right) \\, C$.\n- Compute the single-layer output $y = \\left( \\mathbf{1} - m \\right) \\odot \\left( K x \\right)$.\n- Use the following test suite, which covers a diverse set of cases:\n  $1.$ Happy path: $\\mu_0 = 1$, $\\mu_1 = 1$, $x = [1,\\, 2,\\, -1,\\, 0.5,\\, -0.5,\\, 0,\\, 0]$.\n  $2.$ Material contrast: $\\mu_0 = 0.5$, $\\mu_1 = 2$, $x = [-0.25,\\, 0.75,\\, 1,\\, -1.25,\\, 0,\\, 0.25,\\, -0.25]$.\n  $3.$ Boundary-driven excitation: $\\mu_0 = 1$, $\\mu_1 = 1$, $x = [0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 3,\\, -3]$.\n\nAll quantities are dimensionless in this problem. For each test case, return the full output vector $y \\in \\mathbb{R}^7$ as a list of floats, rounded to six decimal places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to one test case and is itself a comma-separated list of its $7$ rounded outputs enclosed in square brackets. For example, the output format must be\n$$\n[ [y_{1,0}, y_{1,1}, \\ldots, y_{1,6}], [y_{2,0}, \\ldots, y_{2,6}], [y_{3,0}, \\ldots, y_{3,6}] ].\n$$",
            "solution": "The problem is scientifically and mathematically sound, providing a self-contained and well-posed computational task. It asks for the implementation of a single-layer graph neural network (GNN) that exactly reproduces the action of the finite element method (FEM) curl-curl operator on a vector of edge-based degrees of freedom. The problem is grounded in the principles of computational electromagnetics. All necessary data, including mesh geometry, material properties, and input vectors, are provided. The problem is therefore deemed valid and a solution will be presented.\n\nThe core of the problem is to compute the output vector $y \\in \\mathbb{R}^7$ of a GNN layer, defined by the operation\n$$\ny = \\left( \\mathbf{1} - m \\right) \\odot \\left( K x \\right),\n$$\nwhere $x \\in \\mathbb{R}^7$ is the input vector of edge features, $K \\in \\mathbb{R}^{7 \\times 7}$ is a matrix representing the GNN's linear transformation, $m \\in \\{0, 1\\}^7$ is a binary mask, $\\mathbf{1}$ is a vector of ones, and $\\odot$ denotes the elementwise (Hadamard) product.\n\nThe matrix $K$ is the global stiffness matrix derived from the FEM discretization of the time-harmonic Maxwell curl-curl weak form,\n$$\na(E,v) = \\int_{\\Omega} \\mu^{-1} \\left( \\nabla \\times E \\right) \\cdot \\left( \\nabla \\times v \\right) \\, d\\Omega.\n$$\nIn the lowest-order edge element framework (e.g., Nédélec elements of the first kind on triangles or quadrilaterals), degrees of freedom $x_e$ represent the tangential component of the electric field integrated along each mesh edge $e$. The discrete counterpart of this bilinear form is $x^{\\top} K y$. The problem provides the assembled structure of $K$ as\n$$\nK = C^{\\top} D C,\n$$\nwhere $C \\in \\mathbb{R}^{2 \\times 7}$ is the signed cell-edge incidence matrix, and $D \\in \\mathbb{R}^{2 \\times 2}$ is a diagonal matrix containing material and geometric information.\n\nThe operation $Kx = C^{\\top} D C x$ can be interpreted as a message-passing scheme on the graph of the mesh, which establishes the connection to GNNs. The computation proceeds in three steps:\n1.  **Aggregation (Edge to Cell):** The product $z = C x$ computes a vector of dimension $2$ (the number of cells). Each component $z_k = (Cx)_k = \\sum_{e} C_{k,e} x_e$ is the discrete circulation of the field $x$ around the boundary of cell $k$. Given the definition of the discrete curl, $c_k(x) = \\frac{1}{A_k} z_k$, this step is equivalent to computing the curl of the field within each cell, scaled by the cell area.\n2.  **Transformation (Cell-wise):** The product $w = Dz$ scales each cell's aggregated value. Here, $D = \\mathrm{diag}(\\mu_0^{-1}/A_0, \\mu_1^{-1}/A_1)$. This step applies the physical properties (inverse permeability $\\mu^{-1}$) and geometric scaling (inverse area $A^{-1}$) to the cell-level quantities.\n3.  **Propagation (Cell to Edge):** The product $C^{\\top} w$ maps the transformed cell-level data back to the edges. The matrix $C^{\\top}$ acts as a \"scatter\" or discrete gradient-like operator. For each edge $e$, the resulting value is the sum of contributions from the two cells it co-bounds, correctly accounting for orientation.\n\nFinally, the operation $y = (\\mathbf{1} - m) \\odot (Kx)$ applies the Perfect Electric Conductor (PEC) boundary condition. On a PEC boundary, the tangential component of the electric field must be zero. The mask $m$ is $1$ for boundary edges and $0$ for interior edges. The vector $(\\mathbf{1} - m)$ is therefore a selector for interior edges. The elementwise product clamps the output values on all boundary edges to $0$, enforcing the homogeneous Dirichlet condition $y_e = 0$ for $e \\in \\partial \\Omega$.\n\nTo implement the solution, we first construct the matrices from the problem givens. The edges are ordered as $[e_0, e_1, e_2, e_4, e_5, e_6, e_7]$, corresponding to indices $0$ through $6$.\n\nThe signed incidence matrix $C \\in \\mathbb{R}^{2 \\times 7}$ is constructed based on the specified orientations.\nFor cell $k=0$ (row $0$): $C_{0,0}=+1$, $C_{0,2}=+1$, $C_{0,4}=-1$ (for edge $e_4$ at index $3$), $C_{0,6}=-1$ (for edge $e_6$ at index $5$).\nFor cell $k=1$ (row $1$): $C_{1,1}=+1$, $C_{1,7}=+1$ (for edge $e_7$ at index $6$), $C_{1,5}=-1$ (for edge $e_5$ at index $4$), $C_{1,2}=-1$.\nThis yields the matrix:\n$$\nC = \\begin{pmatrix}\n1 & 0 & 1 & -1 & 0 & -1 & 0 \\\\\n0 & 1 & -1 & 0 & -1 & 0 & 1\n\\end{pmatrix}\n$$\nThe boundary mask is given as $m = [1, 1, 0, 1, 1, 1, 1]$, which correctly identifies edge $e_2$ (at index $2$) as the sole interior edge. The selector for the interior is therefore:\n$$\n\\mathbf{1} - m = [0, 0, 1, 0, 0, 0, 0]\n$$\nThe cell areas are $A_0 = 1$ and $A_1 = 1$. The diagonal matrix $D$ for a given test case with permeabilities $(\\mu_0, \\mu_1)$ is:\n$$\nD = \\begin{pmatrix} \\mu_0^{-1} & 0 \\\\ 0 & \\mu_1^{-1} \\end{pmatrix}\n$$\nFor each test case, we construct $D$ from the given $(\\mu_0, \\mu_1)$, assemble $K = C^{\\top} D C$, and compute $y = (\\mathbf{1}-m) \\odot (Kx)$ using the provided input vector $x$. The results for each case are rounded to six decimal places.\n\nFor Test Case 1: $(\\mu_0, \\mu_1) = (1, 1)$, $x = [1, 2, -1, 0.5, -0.5, 0, 0]$.\n$D = \\mathrm{diag}(1, 1) = I$.\n$K = C^{\\top} C$.\n$Kx = [-0.5, 3.5, -4.0, 0.5, -3.5, 0.5, 3.5]^{\\top}$.\n$y = [0, 0, -4.0, 0, 0, 0, 0]^{\\top}$.\n\nFor Test Case 2: $(\\mu_0, \\mu_1) = (0.5, 2)$, $x = [-0.25, 0.75, 1, -1.25, 0, 0.25, -0.25]$.\n$D = \\mathrm{diag}(1/0.5, 1/2) = \\mathrm{diag}(2, 0.5)$.\n$K = C^{\\top} D C$.\n$Kx = [3.5, -0.25, 3.75, -3.5, 0.25, -3.5, -0.25]^{\\top}$.\n$y = [0, 0, 3.75, 0, 0, 0, 0]^{\\top}$.\n\nFor Test Case 3: $(\\mu_0, \\mu_1) = (1, 1)$, $x = [0, 0, 0, 0, 0, 3, -3]$.\n$D = \\mathrm{diag}(1, 1) = I$.\n$K = C^{\\top} C$.\n$Kx = [-3.0, -3.0, 0.0, 3.0, 3.0, 3.0, -3.0]^{\\top}$.\n$y = [0, 0, 0.0, 0, 0, 0, 0]^{\\top}$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the output of a single-layer graph-neural-network-like operator\n    that mirrors the FEM assembly of a curl-curl bilinear form.\n    \"\"\"\n    # Define the mesh and problem geometry.\n    # The cell-edge incidence matrix C is 2x7.\n    # Rows correspond to cells k=0 (left) and k=1 (right).\n    # Columns correspond to edges ordered as [e0, e1, e2, e4, e5, e6, e7].\n    C = np.array([\n        [1, 0, 1, -1, 0, -1, 0],  # Cell k=0\n        [0, 1, -1, 0, -1, 0, 1]   # Cell k=1\n    ], dtype=np.float64)\n\n    # Cell areas.\n    A0, A1 = 1.0, 1.0\n\n    # Boundary mask m: 1 for boundary edges, 0 for interior edges.\n    # The unique interior edge is e2, at index 2.\n    m = np.array([1, 1, 0, 1, 1, 1, 1], dtype=np.float64)\n\n    # Selector for interior edges, (1 - m).\n    interior_selector = 1.0 - m\n\n    # Define the test cases.\n    test_cases = [\n        {\n            \"mu0\": 1.0, \"mu1\": 1.0,\n            \"x\": np.array([1.0, 2.0, -1.0, 0.5, -0.5, 0.0, 0.0], dtype=np.float64)\n        },\n        {\n            \"mu0\": 0.5, \"mu1\": 2.0,\n            \"x\": np.array([-0.25, 0.75, 1.0, -1.25, 0.0, 0.25, -0.25], dtype=np.float64)\n        },\n        {\n            \"mu0\": 1.0, \"mu1\": 1.0,\n            \"x\": np.array([0.0, 0.0, 0.0, 0.0, 0.0, 3.0, -3.0], dtype=np.float64)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mu0, mu1 = case[\"mu0\"], case[\"mu1\"]\n        x = case[\"x\"]\n\n        # 1. Construct the diagonal matrix D of material/geometric properties.\n        D = np.diag([1.0 / (mu0 * A0), 1.0 / (mu1 * A1)])\n\n        # 2. Assemble the stiffness matrix K = C^T * D * C.\n        K = C.T @ D @ C\n        \n        # 3. Apply the linear operator: Kx.\n        Kx = K @ x\n\n        # 4. Apply the boundary condition by elementwise-multiplying with the\n        #    interior edge selector.\n        y = interior_selector * Kx\n\n        # 5. Round the results to six decimal places and format as a list.\n        y_rounded = [round(val, 6) for val in y]\n        results.append(y_rounded)\n\n    # Final print statement in the exact required format.\n    # The format is a string representation of a list of lists.\n    # str() on a list automatically adds spaces, e.g., '[1.0, 2.0]'.\n    # ','.join() then combines these strings.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond serving as surrogate models or inverse problem solvers, machine learning can function as an intelligent diagnostic tool within complex simulation workflows. A persistent challenge in computational electromagnetics is the appearance of spurious, non-physical solutions in numerical eigensolvers. This exercise guides you through building a complete, end-to-end machine learning pipeline to automatically detect these spurious modes based on features derived from fundamental physical principles, such as the divergence-free condition and mode orthogonality, and even propose automated corrective actions . This demonstrates how ML can be used to enhance the reliability and automation of traditional numerical simulations.",
            "id": "3327863",
            "problem": "You are tasked with designing and implementing a complete, runnable program that trains a binary classifier to detect spurious eigenmodes in discrete curl–curl eigenproblems arising in computational electromagnetics, using features derived from divergence constraints and mode orthogonality, and then to propose automated mesh repair suggestions. The classifier must be trained on a synthetic dataset generated by the program itself, based on physically principled constructions of discrete magnetic field modes. The entire program must be self-contained and produce a single-line output aggregating the results for a provided test suite.\n\nBegin from the laws of electromagnetism and mathematically derive the features used for classification. Use the following fundamental base:\n- Maxwell's equations for macroscopic media, including the divergence-free constraint for magnetic flux density, $\\,\\nabla \\cdot \\mathbf{B} = 0\\,$, and the source-free curl–curl eigenproblem for the electric field $\\,\\mathbf{E}\\,$,\n$$\n\\nabla \\times \\left( \\mu^{-1} \\nabla \\times \\mathbf{E} \\right) = \\lambda \\, \\varepsilon \\, \\mathbf{E},\n$$\nwith appropriate boundary conditions in a simply connected domain, where $\\,\\mu\\,$ is the magnetic permeability, $\\,\\varepsilon\\,$ is the electric permittivity, and $\\,\\lambda\\,$ is the eigenvalue.\n- Orthogonality of distinct eigenmodes under the energy inner product associated with the self-adjoint curl–curl operator in a homogeneous medium, implying for distinct eigenfunctions $\\,\\mathbf{E}_i\\,$ and $\\,\\mathbf{E}_j\\,$\n$$\n\\int_{\\Omega} \\varepsilon \\, \\mathbf{E}_i \\cdot \\mathbf{E}_j \\, d\\Omega = 0.\n$$\n- In discrete settings, spurious modes often arise from violations of exact sequence properties leading to non-solenoidal fields or mode duplication; hence principled features should quantify divergence and (non-)orthogonality.\n\nYour program must:\n1. Construct a synthetic dataset of discrete two-dimensional magnetic field modes $\\,\\mathbf{B}(x,y) = \\left(B_x(x,y), B_y(x,y)\\right)\\,$ on uniform rectangular meshes. Generate two types of modes:\n   - Physically plausible divergence-free modes constructed from analytic patterns guaranteeing $\\,\\nabla \\cdot \\mathbf{B} \\approx 0\\,$ on the grid.\n   - Spurious modes constructed as gradients of scalar potentials $\\,\\phi(x,y)\\,$ such that $\\,\\mathbf{B} = \\nabla \\phi\\,$, for which $\\,\\nabla \\cdot \\mathbf{B} = \\nabla^2 \\phi \\neq 0\\,$ in general.\n2. For any discrete mode pair $\\,(\\mathbf{B}^{(1)}, \\mathbf{B}^{(2)})\\,$ on a given uniform mesh with spacings $\\,h_x\\,$ and $\\,h_y\\,$, compute the following features for each mode:\n   - The relative divergence magnitude\n     $$\n     R_{\\mathrm{div}} = \\frac{\\left\\| \\nabla \\cdot \\mathbf{B} \\right\\|_{L^2}}{\\left\\| \\mathbf{B} \\right\\|_{L^2} + \\epsilon},\n     $$\n     where $\\,\\epsilon > 0\\,$ is a small stabilization constant, and the $\\,L^2\\,$ norm is discretized with area element $\\,h_x h_y\\,$. Use discrete central differences for interior points and consistent one-sided differences at boundaries for approximating $\\,\\partial B_x / \\partial x\\,$ and $\\,\\partial B_y / \\partial y\\,$.\n   - A non-orthogonality index between the two modes\n     $$\n     R_{\\mathrm{nonorth}}^{(i)} = \\frac{\\left| \\langle \\mathbf{B}^{(i)}, \\mathbf{B}^{(j)} \\rangle \\right|}{\\left\\| \\mathbf{B}^{(i)} \\right\\|_{L^2} \\, \\left\\| \\mathbf{B}^{(j)} \\right\\|_{L^2} + \\epsilon}, \\quad i \\neq j,\n     $$\n     using the discrete inner product $\\,\\langle \\mathbf{U}, \\mathbf{V} \\rangle = \\sum \\left(U_x V_x + U_y V_y\\right) h_x h_y\\,$.\n   - A mesh anisotropy quality metric\n     $$\n     Q_{\\mathrm{aspect}} = \\max\\left( \\frac{h_x}{h_y}, \\frac{h_y}{h_x} \\right).\n     $$\n3. Train a binary classifier using logistic regression with $\\,L^2\\,$ regularization to predict whether a mode is spurious ($\\,1\\,$) or physical ($\\,0\\,$) from the feature vector\n   $$\n   \\mathbf{x} = \\left[ R_{\\mathrm{div}}, \\, R_{\\mathrm{nonorth}}, \\, Q_{\\mathrm{aspect}} \\right],\n   $$\n   using the synthetic dataset. Standardize features prior to training using training-set mean and standard deviation. The classifier must be implemented directly in the program (no external machine learning libraries) using batch gradient descent on the cross-entropy loss with an $\\,L^2\\,$ penalty. Fix the random seed for reproducibility.\n4. For each test case, compute features for both modes, apply the trained classifier to obtain predicted labels (boolean values), and then propose a single automated mesh repair suggestion code for the case, following the rule-based mapping below:\n   - If $\\,Q_{\\mathrm{aspect}} > 3.0\\,$, suggestion code $\\,1\\,$ (recommend local isotropic refinement to reduce anisotropy).\n   - Else if any mode is predicted spurious and has $\\,R_{\\mathrm{div}} > 0.15\\,$, suggestion code $\\,2\\,$ (recommend adopting divergence-conforming discretization or adding a divergence penalty term).\n   - Else if $\\,\\max\\left(R_{\\mathrm{nonorth}}^{(1)}, R_{\\mathrm{nonorth}}^{(2)}\\right) > 0.95\\,$, suggestion code $\\,3\\,$ (recommend orthogonalization or improved eigensolver settings to separate nearly duplicate modes).\n   - Else suggestion code $\\,0\\,$ (no repair).\n5. Implement the entire pipeline deterministically and produce a single line of output containing a list of results for the test suite, where each test case result is a list of the form\n   $$\n   \\left[ \\text{label}^{(1)}, \\, \\text{label}^{(2)}, \\, \\text{suggestion\\_code} \\right],\n   $$\n   with the two labels as boolean values and the suggestion code as an integer.\n\nAll quantities in the program are dimensionless. Angles, where present in analytic functions, are in radians. The final output line must be exactly the string representation of a Python list of the per-test-case lists, for example\n$$\n\\left[ [\\text{True}, \\, \\text{False}, \\, 2], \\, [\\text{False}, \\, \\text{False}, \\, 0] \\right].\n$$\n\nTest Suite:\nProvide four scientifically meaningful cases that exercise different aspects of the classifier and the suggestion logic. Use $\\,n_x = 64\\,$, $\\,n_y = 64\\,$ grid points in each direction for all tests. Let the domain sizes be $\\,L_x\\,$ and $\\,L_y\\,$, so spacings are $\\,h_x = L_x / (n_x - 1)\\,$ and $\\,h_y = L_y / (n_y - 1)\\,$.\n- Case $\\,1\\,$ (well-resolved physical modes, happy path): $\\,L_x = 1.0\\,$, $\\,L_y = 1.0\\,$. Mode $\\,1\\,$ is divergence-free with wavenumber $\\,k = 3\\,$. Mode $\\,2\\,$ is divergence-free with wavenumber $\\,k = 5\\,$.\n- Case $\\,2\\,$ (one spurious gradient mode): $\\,L_x = 1.0\\,$, $\\,L_y = 1.0\\,$. Mode $\\,1\\,$ is a gradient field from scalar potential $\\,\\phi(x,y) = \\sin(10 x)\\sin(10 y)\\,$. Mode $\\,2\\,$ is divergence-free with wavenumber $\\,k = 4\\,$.\n- Case $\\,3\\,$ (anisotropic mesh quality issue): $\\,L_x = 1.0\\,$, $\\,L_y = 5.0\\,$. Mode $\\,1\\,$ is divergence-free with wavenumber $\\,k = 4\\,$. Mode $\\,2\\,$ is divergence-free with wavenumber $\\,k = 6\\,$.\n- Case $\\,4\\,$ (nearly duplicate modes, high non-orthogonality): $\\,L_x = 1.0\\,$, $\\,L_y = 1.0\\,$. Mode $\\,1\\,$ is divergence-free with wavenumber $\\,k = 8\\,$. Mode $\\,2\\,$ equals mode $\\,1\\,$ plus small additive noise of magnitude $\\,10^{-3}\\,$ to each component.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result itself being a list $\\,\\left[\\text{label}^{(1)}, \\text{label}^{(2)}, \\text{suggestion\\_code}\\right]\\,$, for example\n$$\n\\left[ [\\text{False}, \\, \\text{False}, \\, 0], \\, [\\text{True}, \\, \\text{False}, \\, 2], \\, [\\text{False}, \\, \\text{False}, \\, 1], \\, [\\text{False}, \\, \\text{False}, \\, 3] \\right].\n$$\n\nImplementation constraints:\n- Use only the Python language runtime and the libraries specified in the execution environment. No external machine learning libraries are permitted.\n- Fix the random number generator seed in the program to ensure deterministic behavior.\n- The training dataset must contain at least $\\,200\\,$ modes, drawn from the analytic constructions above, with balanced physical and spurious examples.",
            "solution": "The user wants a meticulous and exacting STEM professor to validate and solve a problem.\n\nThe problem asks me to implement a complete program that performs the following tasks:\n1.  **Generates a synthetic dataset** of 2D electromagnetic modes, divided into \"physical\" (divergence-free) and \"spurious\" (gradient-field) types.\n2.  **Computes physically-motivated features** for each mode: relative divergence ($R_{\\mathrm{div}}$), non-orthogonality ($R_{\\mathrm{nonorth}}$) with a paired mode, and mesh aspect ratio ($Q_{\\mathrm{aspect}}$).\n3.  **Trains a logistic regression classifier** from scratch to distinguish between physical and spurious modes based on these features.\n4.  **Applies the trained classifier** to a predefined test suite.\n5.  **Proposes automated mesh repair suggestions** based on a set of rules applied to the features and classification results.\n\nThe entire process must be deterministic and self-contained, using only specified Python libraries (`numpy`, `scipy`).\n\n### Validation of the Problem Statement\n\nI have meticulously reviewed the problem statement against the required criteria.\n\n*   **Scientific Grounding**: The problem is excellent. It is deeply rooted in a real, significant challenge in computational electromagnetics: the appearance of non-physical, \"spurious\" solutions in numerical eigensolvers for the Maxwell curl-curl equations. The chosen features ($R_{\\mathrm{div}}$, $R_{\\mathrm{nonorth}}$) directly correspond to the fundamental mathematical properties that distinguish true eigenmodes from spurious ones in a Hilbert space setting (i.e., adherence to divergence constraints and orthogonality). The method of constructing synthetic data (using stream functions for divergence-free fields and scalar potentials for spurious gradient fields) is also standard and physically sound. The overall concept—using machine learning as a diagnostic tool for numerical simulations—is a contemporary and valid research direction.\n\n*   **Well-Posedness & Completeness**: The problem is exceptionally well-posed. It provides:\n    *   Explicit mathematical formulas for all features.\n    *   The exact algorithm for the classifier (logistic regression with L2 regularization via batch gradient descent).\n    *   Specific instructions for data generation and numerical methods (e.g., central/one-sided differences for derivatives).\n    *   A complete test suite with all necessary parameters.\n    *   Unambiguous rules for the final \"mesh repair\" suggestion logic.\n    *   A precise output format.\n    *   Constraints on the execution environment.\n\n    The single minor ambiguity, the exact form of the \"divergence-free mode with wavenumber $k$\", is easily resolved by adopting the standard stream-function approach, which is heavily implied by the physics described.\n\n*   **Objectivity**: The problem is entirely objective, specified using mathematical equations, computational algorithms, and quantitative rules. There are no subjective elements.\n\n**Verdict**: The problem is **valid**. It is a well-designed, scientifically sound, and challenging task that is perfectly suited for a rigorous, principle-based solution.\n\n### Principle-Based Solution Design\n\nThe solution will be developed by directly translating the underlying physical and mathematical principles into a computational algorithm.\n\n#### 1. Theoretical Foundation: The Curl-Curl Eigenproblem and Spurious Modes\n\nThe starting point is the source-free curl-curl eigenproblem for the electric field $\\mathbf{E}$ in a domain $\\Omega$, stated as:\n$$\n\\nabla \\times \\left( \\mu^{-1} \\nabla \\times \\mathbf{E} \\right) = \\lambda \\varepsilon \\mathbf{E}\n$$\nwhere $\\mu$ is the magnetic permeability, $\\varepsilon$ is the electric permittivity, and $\\lambda = \\omega^2$ are the eigenvalues corresponding to the squared resonant frequencies $\\omega$. The operator $\\mathcal{L} = \\nabla \\times (\\mu^{-1} \\nabla \\times \\cdot)$ is self-adjoint with respect to the energy inner product $\\langle \\mathbf{u}, \\mathbf{v} \\rangle_{\\varepsilon} = \\int_{\\Omega} \\varepsilon \\, \\mathbf{u} \\cdot \\mathbf{v} \\, d\\Omega$. A key property of self-adjoint operators is that their eigenfunctions corresponding to distinct eigenvalues are orthogonal:\n$$\n\\text{For } \\lambda_i \\neq \\lambda_j, \\quad \\langle \\mathbf{E}_i, \\mathbf{E}_j \\rangle_{\\varepsilon} = \\int_{\\Omega} \\varepsilon \\, \\mathbf{E}_i \\cdot \\mathbf{E}_j \\, d\\Omega = 0\n$$\nFurthermore, in a source-free region, Maxwell's equations impose the divergence-free condition $\\nabla \\cdot \\mathbf{D} = 0$, which for a homogeneous medium ($\\varepsilon$ is constant) simplifies to $\\nabla \\cdot \\mathbf{E} = 0$.\n\nWhen this continuous problem is discretized (e.g., using the Finite Element Method), the discrete operators may fail to perfectly replicate the properties of their continuous counterparts. This deficiency can lead to the emergence of *spurious modes* which pollute the computed spectrum. These spurious solutions typically fall into two categories:\n1.  **Non-solenoidal modes**: Solutions that incorrectly have a non-zero divergence ($\\nabla \\cdot \\mathbf{E} \\neq 0$).\n2.  **Gradient-field modes**: A subset of the first category, these are non-physical, zero-frequency solutions of the form $\\mathbf{E} = \\nabla \\phi$, where $\\phi$ is a scalar potential. Such fields are incorrectly admitted into the solution space because they lie in the null space of the curl operator ($\\nabla \\times (\\nabla \\phi) = 0$), which the discrete curl operator may fail to capture correctly.\n\nThis project models the analogous problem for the magnetic field $\\mathbf{B}$, which is governed by similar physics ($\\nabla \\cdot \\mathbf{B} = 0$).\n\n#### 2. Feature Engineering from First Principles\n\nThe classification task relies on features designed to detect the signature of these spurious modes.\n\n*   **Relative Divergence ($R_{\\mathrm{div}}$)**: This feature directly quantifies the violation of the fundamental physical law $\\nabla \\cdot \\mathbf{B} = 0$. A true physical mode should have a divergence that is zero (analytically) or very small (numerically, due to discretization error). A spurious gradient-field mode $\\mathbf{B} = \\nabla\\phi$ will have a divergence $\\nabla \\cdot \\mathbf{B} = \\nabla^2\\phi$, which is generally non-zero. The feature is defined as:\n    $$\n    R_{\\mathrm{div}} = \\frac{\\left\\| \\nabla \\cdot \\mathbf{B} \\right\\|_{L^2}}{\\left\\| \\mathbf{B} \\right\\|_{L^2} + \\epsilon}\n    $$\n    This normalization makes the feature independent of the mode's amplitude.\n\n*   **Non-Orthogonality Index ($R_{\\mathrm{nonorth}}$)**: This feature checks for violation of the orthogonality property of distinct eigenmodes. Numerical issues, such as nearly degenerate eigenvalues, can lead to computed modes that are not properly orthogonal. This feature, defined as the absolute value of the cosine of the angle between two mode vectors, detects this:\n    $$\n    R_{\\mathrm{nonorth}}^{(i)} = \\frac{\\left| \\langle \\mathbf{B}^{(i)}, \\mathbf{B}^{(j)} \\rangle \\right|}{\\left\\| \\mathbf{B}^{(i)} \\right\\|_{L^2} \\, \\left\\| \\mathbf{B}^{(j)} \\right\\|_{L^2} + \\epsilon}\n    $$\n    A value close to $0$ indicates orthogonality, while a value close to $1$ indicates linear dependence (e.g., duplicate modes). The discrete inner product $\\langle \\mathbf{U}, \\mathbf{V} \\rangle = \\sum (U_x V_x + U_y V_y) h_x h_y$ is a discrete approximation of the unweighted integral $\\int \\mathbf{U} \\cdot \\mathbf{V} \\, d\\Omega$.\n\n*   **Mesh Anisotropy ($Q_{\\mathrm{aspect}}$)**: Poor mesh quality, particularly high aspect ratios in cells, is known to degrade the accuracy of numerical solutions and can exacerbate the problem of spurious modes. This feature is a standard mesh quality metric:\n    $$\n    Q_{\\mathrm{aspect}} = \\max\\left( \\frac{h_x}{h_y}, \\frac{h_y}{h_x} \\right)\n    $$\n\n#### 3. Synthetic Data Generation\n\nTo train a classifier, we need a labeled dataset. We construct one based on the theoretical properties of physical and spurious modes.\n\n*   **Physical (Divergence-Free) Modes**: These are constructed using a 2D stream function $\\psi(x,y)$. The magnetic field is defined as the curl of a vector potential $\\mathbf{A} = (0, 0, \\psi(x,y))$, which yields $\\mathbf{B} = \\nabla \\times \\mathbf{A} = (\\partial_y \\psi, -\\partial_x \\psi)$. By construction, $\\nabla \\cdot \\mathbf{B} = \\partial_x(\\partial_y \\psi) + \\partial_y(-\\partial_x \\psi) = 0$. We use sinusoidal functions for $\\psi$ to mimic oscillatory eigenmodes.\n\n*   **Spurious (Gradient-Field) Modes**: These are constructed as the gradient of a scalar potential $\\phi(x,y)$, $\\mathbf{B} = \\nabla\\phi = (\\partial_x \\phi, \\partial_y \\phi)$. These fields are curl-free but generally not divergence-free. This mimics the primary source of spurious solutions in many numerical schemes.\n\n#### 4. Logistic Regression Classifier\n\nA binary logistic regression model is implemented to classify modes as physical ($y=0$) or spurious ($y=1$).\n\n*   **Model**: The probability of a mode being spurious is modeled as $P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)$, where $\\mathbf{x} = [R_{\\mathrm{div}}, R_{\\mathrm{nonorth}}, Q_{\\mathrm{aspect}}]$ is the feature vector, $\\mathbf{w}$ are the weights, $b$ is the bias, and $\\sigma(z) = 1/(1+e^{-z})$ is the sigmoid function.\n*   **Training**: The model parameters $(\\mathbf{w}, b)$ are optimized by minimizing the regularized binary cross-entropy loss function using batch gradient descent. The loss function is:\n    $$\n    J(\\mathbf{w}, b) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log(p^{(i)}) + (1-y^{(i)}) \\log(1-p^{(i)}) \\right] + \\frac{\\alpha_{reg}}{2m} \\|\\mathbf{w}\\|_2^2\n    $$\n    where $m$ is the number of training samples, $p^{(i)}$ is the predicted probability for sample $i$, and $\\alpha_{reg}$ is the L2 regularization parameter. Prior to training, features are standardized to have zero mean and unit variance to improve the convergence of gradient descent.\n\n#### 5. Automated Repair Logic\n\nThe final step connects the analysis back to a practical action. A simple, rule-based expert system uses the classifier's output and the feature values to suggest a corrective action for the numerical simulation setup, providing a rudimentary feedback loop for improving simulation quality. The rules directly address the issues identified by the features.\n\nThis completes the principled design of the solution. The implementation in the `final_answer` block will follow this structure.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the entire pipeline: dataset generation,\n    feature computation, classifier training, and testing.\n    \"\"\"\n    # Fix the random seed for reproducibility\n    np.random.seed(42)\n\n    # --- Global Constants ---\n    STABILIZATION_EPS = 1e-9\n    TRAINING_SAMPLES = 200  # Will be created from 100 pairs\n    GRID_SIZE_TRAIN = 32\n    \n    # Logistic Regression Hyperparameters\n    LEARNING_RATE = 0.1\n    ITERATIONS = 2000\n    REG_PARAM = 0.01\n\n    # --- Helper Functions for Grid and Mode Generation ---\n    def make_grid(nx, ny, Lx, Ly):\n        \"\"\"Creates a 2D mesh grid.\"\"\"\n        x = np.linspace(0, Lx, nx)\n        y = np.linspace(0, Ly, ny)\n        X, Y = np.meshgrid(x, y)\n        hx = Lx / (nx - 1)\n        hy = Ly / (ny - 1)\n        return X, Y, hx, hy\n\n    def generate_physical_mode(k_val, X, Y):\n        \"\"\"Generates a divergence-free mode from a stream function.\"\"\"\n        # psi = sin(kx*x) * sin(ky*y)\n        # Bx = d(psi)/dy, By = -d(psi)/dx\n        kx = k_val\n        ky = k_val\n        psi = np.sin(kx * X) * np.sin(ky * Y)\n        Bx = ky * np.sin(kx * X) * np.cos(ky * Y)\n        By = -kx * np.cos(kx * X) * np.sin(ky * Y)\n        return Bx, By\n\n    def generate_spurious_mode(phi_func, X, Y):\n        \"\"\"Generates a spurious gradient-field mode from a scalar potential.\"\"\"\n        # B = grad(phi)\n        # Bx = d(phi)/dx, By = d(phi)/dy\n        phi = phi_func(X, Y)\n        # np.gradient returns derivatives along axis 0 (y), then axis 1 (x)\n        gy, gx = np.gradient(phi, Y[0,1]-Y[0,0], X[1,0]-X[0,0])\n        return gx, gy\n\n    # --- Feature Computation Functions ---\n    def compute_divergence(Bx, By, hx, hy):\n        \"\"\"Computes the divergence of a 2D vector field using np.gradient.\"\"\"\n        _, dBx_dx = np.gradient(Bx, hy, hx)\n        dBy_dy, _ = np.gradient(By, hy, hx)\n        return dBx_dx + dBy_dy\n\n    def compute_l2_norm(B_comp, hx, hy):\n        \"\"\"Computes the discrete L2 norm of a vector field.\"\"\"\n        Bx, By = B_comp\n        integrand = Bx**2 + By**2\n        return np.sqrt(np.sum(integrand) * hx * hy)\n\n    def compute_inner_product(B1_comp, B2_comp, hx, hy):\n        \"\"\"Computes the discrete inner product of two vector fields.\"\"\"\n        B1x, B1y = B1_comp\n        B2x, B2y = B2_comp\n        integrand = B1x * B2x + B1y * B2y\n        return np.sum(integrand) * hx * hy\n\n    def compute_features(B1_comp, B2_comp, hx, hy):\n        \"\"\"Computes the full feature set for a pair of modes.\"\"\"\n        # Feature 3: Mesh Anisotropy\n        Q_aspect = max(hx / hy, hy / hx)\n\n        # Compute norms for both modes\n        norm_B1 = compute_l2_norm(B1_comp, hx, hy)\n        norm_B2 = compute_l2_norm(B2_comp, hx, hy)\n\n        # Feature 1: Relative Divergence for each mode\n        div_B1 = compute_divergence(B1_comp[0], B1_comp[1], hx, hy)\n        norm_div_B1 = np.sqrt(np.sum(div_B1**2) * hx * hy)\n        R_div1 = norm_div_B1 / (norm_B1 + STABILIZATION_EPS)\n\n        div_B2 = compute_divergence(B2_comp[0], B2_comp[1], hx, hy)\n        norm_div_B2 = np.sqrt(np.sum(div_B2**2) * hx * hy)\n        R_div2 = norm_div_B2 / (norm_B2 + STABILIZATION_EPS)\n\n        # Feature 2: Non-Orthogonality\n        inner_prod = compute_inner_product(B1_comp, B2_comp, hx, hy)\n        R_nonorth = abs(inner_prod) / ((norm_B1 * norm_B2) + STABILIZATION_EPS)\n        \n        features1 = np.array([R_div1, R_nonorth, Q_aspect])\n        features2 = np.array([R_div2, R_nonorth, Q_aspect])\n\n        return features1, features2\n\n    # --- Logistic Regression Classifier (from scratch) ---\n    class LogisticRegressor:\n        def __init__(self, lr, n_iter, lambda_):\n            self.lr = lr\n            self.n_iter = n_iter\n            self.lambda_ = lambda_\n            self.weights = None\n            self.bias = None\n\n        def _sigmoid(self, z):\n            # Clip to avoid overflow in exp\n            z = np.clip(z, -500, 500)\n            return 1 / (1 + np.exp(-z))\n\n        def fit(self, X, y):\n            n_samples, n_features = X.shape\n            self.weights = np.zeros(n_features)\n            self.bias = 0\n\n            # Batch Gradient Descent\n            for _ in range(self.n_iter):\n                linear_model = np.dot(X, self.weights) + self.bias\n                y_predicted = self._sigmoid(linear_model)\n\n                # Compute gradients\n                dw = (1 / n_samples) * (np.dot(X.T, (y_predicted - y)) + self.lambda_ * self.weights)\n                db = (1 / n_samples) * np.sum(y_predicted - y)\n\n                # Update weights and bias\n                self.weights -= self.lr * dw\n                self.bias -= self.lr * db\n\n        def predict_proba(self, X):\n            linear_model = np.dot(X, self.weights) + self.bias\n            return self._sigmoid(linear_model)\n\n        def predict(self, X, threshold=0.5):\n            probas = self.predict_proba(X)\n            return probas >= threshold\n\n    # --- 1. Generate Training Data ---\n    num_pairs = TRAINING_SAMPLES // 2\n    X_train_list = []\n    y_train_list = []\n\n    for i in range(num_pairs):\n        # Randomize mesh\n        nx, ny = GRID_SIZE_TRAIN, GRID_SIZE_TRAIN\n        Lx = np.random.uniform(1.0, 5.0)\n        Ly = np.random.uniform(1.0, 5.0)\n        X, Y, hx, hy = make_grid(nx, ny, Lx, Ly)\n\n        # Generate a pair of modes, each randomly physical or spurious\n        modes = []\n        labels = []\n        for _ in range(2):\n            if np.random.rand() > 0.5: # Physical\n                k = np.random.uniform(2.0, 10.0)\n                modes.append(generate_physical_mode(k, X, Y))\n                labels.append(0) # 0 for physical\n            else: # Spurious\n                k = np.random.uniform(2.0, 10.0)\n                modes.append(generate_spurious_mode(lambda x,y: np.sin(k*x)*np.cos(k*y), X, Y))\n                labels.append(1) # 1 for spurious\n\n        # Compute features for the pair\n        features1, features2 = compute_features(modes[0], modes[1], hx, hy)\n        \n        X_train_list.append(features1)\n        y_train_list.append(labels[0])\n        X_train_list.append(features2)\n        y_train_list.append(labels[1])\n        \n    X_train = np.array(X_train_list)\n    y_train = np.array(y_train_list)\n\n    # --- 2. Standardize Features and Train Classifier ---\n    mean = X_train.mean(axis=0)\n    std = X_train.std(axis=0)\n    std[std == 0] = 1 # Avoid division by zero if a feature is constant\n    X_train_std = (X_train - mean) / std\n\n    classifier = LogisticRegressor(lr=LEARNING_RATE, n_iter=ITERATIONS, lambda_=REG_PARAM)\n    classifier.fit(X_train_std, y_train)\n\n    # --- 3. Process Test Suite ---\n    test_cases_params = [\n        {'Lx': 1.0, 'Ly': 1.0, 'mode1_type': 'physical', 'mode1_k': 3, 'mode2_type': 'physical', 'mode2_k': 5},\n        {'Lx': 1.0, 'Ly': 1.0, 'mode1_type': 'spurious', 'mode1_phi': lambda x,y: np.sin(10*x)*np.sin(10*y), 'mode2_type': 'physical', 'mode2_k': 4},\n        {'Lx': 1.0, 'Ly': 5.0, 'mode1_type': 'physical', 'mode1_k': 4, 'mode2_type': 'physical', 'mode2_k': 6},\n        {'Lx': 1.0, 'Ly': 1.0, 'mode1_type': 'physical', 'mode1_k': 8, 'mode2_type': 'duplicate', 'mode2_k': 8}\n    ]\n    nx_test, ny_test = 64, 64\n    results = []\n\n    for case in test_cases_params:\n        X, Y, hx, hy = make_grid(nx_test, ny_test, case['Lx'], case['Ly'])\n        \n        # Generate modes for the test case\n        B1, B2 = None, None\n        \n        if case['mode1_type'] == 'physical':\n            B1 = generate_physical_mode(case['mode1_k'], X, Y)\n        elif case['mode1_type'] == 'spurious':\n            B1 = generate_spurious_mode(case['mode1_phi'], X, Y)\n\n        if case['mode2_type'] == 'physical':\n            B2 = generate_physical_mode(case['mode2_k'], X, Y)\n        elif case['mode2_type'] == 'duplicate':\n            B_base = generate_physical_mode(case['mode2_k'], X, Y)\n            noise_mag = 1e-3\n            noise_x = noise_mag * (2 * np.random.rand(nx_test, ny_test) - 1)\n            noise_y = noise_mag * (2 * np.random.rand(nx_test, ny_test) - 1)\n            B2 = (B_base[0] + noise_x, B_base[1] + noise_y)\n        \n        # Compute features\n        features1, features2 = compute_features(B1, B2, hx, hy)\n        \n        # Standardize features using training set stats\n        test_features = np.array([features1, features2])\n        test_features_std = (test_features - mean) / std\n        \n        # Get predictions\n        predictions = classifier.predict(test_features_std)\n        label1, label2 = bool(predictions[0]), bool(predictions[1])\n        \n        # Apply repair suggestion logic\n        Q_aspect = features1[2] # same for both\n        R_div1, R_div2 = features1[0], features2[0]\n        R_nonorth = features1[1]\n        \n        suggestion_code = 0 # Default: no repair\n        if Q_aspect > 3.0:\n            suggestion_code = 1\n        elif (label1 and R_div1 > 0.15) or (label2 and R_div2 > 0.15):\n            suggestion_code = 2\n        elif R_nonorth > 0.95:\n            suggestion_code = 3\n\n        results.append([label1, label2, suggestion_code])\n\n    # --- 4. Print Final Output ---\n    print(results)\n\n\nsolve()\n```"
        }
    ]
}