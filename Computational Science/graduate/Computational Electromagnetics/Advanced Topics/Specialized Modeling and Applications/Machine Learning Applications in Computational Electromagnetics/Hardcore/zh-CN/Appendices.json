{
    "hands_on_practices": [
        {
            "introduction": "在将机器学习应用于电磁逆散射等问题时，一个核心任务是根据观测数据调整材料参数（如介电常数 $ \\epsilon(\\mathbf{r}) $）。基于梯度的方法是解决这类大规模优化问题的关键，但这需要高效地计算损失函数相对于成千上万个模型参数的梯度。本练习将指导您使用伴随状态法（adjoint-state method）推导这一梯度，这是物理信息机器学习和可微物理模拟领域的一项基本技能 ()。",
            "id": "3327841",
            "problem": "考虑一个逆散射设置，其中有界域 $ \\Omega \\subset \\mathbb{R}^{3} $ 内空间变化的介电常数 $ \\epsilon(\\mathbf{r}) $ 是待通过一个由数据失配最小化驱动的机器学习模型来推断的。计算物理正演模型是关于角频率为 $ \\omega $ 的时谐场 $ u(\\mathbf{r}) $ 的标量亥姆霍兹方程，其中磁导率 $ \\mu $ 和介电常数 $ \\epsilon(\\mathbf{r}) $ 均为实数和正数。对于由 $ m = 1, \\dots, N_{s} $ 索引的每个实验，正演场 $ u_{m}(\\mathbf{r}) $ 满足\n$$\n\\left(\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})\\right) u_{m}(\\mathbf{r}) = s_{m}(\\mathbf{r}) \\quad \\text{in } \\Omega,\n$$\n其中 $ s_{m}(\\mathbf{r}) $ 是一个已知的源分布。假设在 $ \\partial \\Omega $ 上采用均匀吸收边界条件，该条件强制一个适定的辐射条件，并使边界项在伴随态推导中消失。测量通过一个线性采样算子 $ \\mathcal{C} $ 在接收点位置 $ \\{\\mathbf{r}_{j}\\}_{j=1}^{N_{r}} $ 处采集，该算子定义为\n$$\n\\mathcal{C} u_{m} = \\left(u_{m}(\\mathbf{r}_{1}), \\dots, u_{m}(\\mathbf{r}_{N_{r}})\\right)^{\\top} \\in \\mathbb{C}^{N_{r}}.\n$$\n给定复值观测数据向量 $ \\mathbf{d}_{m} \\in \\mathbb{C}^{N_{r}} $，数据失配泛函为\n$$\nJ(\\epsilon) = \\frac{1}{2} \\sum_{m=1}^{N_{s}} \\left\\| \\mathcal{C} u_{m} - \\mathbf{d}_{m} \\right\\|_{2}^{2},\n$$\n其中 $ \\|\\cdot\\|_{2} $ 表示由 $ \\mathbb{C}^{N_{r}} $ 上的厄米内积导出的欧几里得范数。在机器学习框架中，训练目标是关于场参数 $ \\epsilon(\\mathbf{r}) $ 最小化 $ J(\\epsilon) $。\n\n使用伴随态方法，在 $ \\mu $ 和 $ \\omega $ 是固定实数的假设下，推导关于 $ \\epsilon(\\mathbf{r}) $ 的泛函梯度 $ \\nabla_{\\epsilon} J(\\mathbf{r}) $。在推导过程中，从标量亥姆霍兹方程和 $ J(\\epsilon) $ 的定义出发，并且只使用与所述边界条件一致的分部积分和伴随算子恒等式。明确写出正演和伴随场方程，包括它们的右端项。将最终梯度表示为关于正演场 $ u_{m} $ 和伴随场 $ p_{m} $ 的单一闭式解析表达式。\n\n你的最终答案必须是单一的闭式解析表达式。最终答案中不要包含单位。如果引入任何角度，必须以弧度为单位。最终的解析表达式不需要四舍五入。",
            "solution": "该问题要求推导数据失配泛函 $J(\\epsilon)$ 关于空间变化的介电常数 $ \\epsilon(\\mathbf{r}) $ 的泛函梯度。伴随态方法是为此推导指定的技术。泛函梯度，记作 $ \\nabla_{\\epsilon} J(\\mathbf{r}) $，由泛函 $J$ 的一阶变分定义，即对于一个小的微扰 $ \\delta\\epsilon(\\mathbf{r}) $，$J$ 的变化由下式给出：\n$$ \\delta J = J(\\epsilon + \\delta\\epsilon) - J(\\epsilon) = \\int_{\\Omega} \\nabla_{\\epsilon} J(\\mathbf{r}) \\, \\delta\\epsilon(\\mathbf{r}) \\, d\\mathbf{r} + O(\\|\\delta\\epsilon \\|^2) $$\n我们的目标是通过将微扰 $ \\delta\\epsilon $ 与其引起的变化 $ \\delta J $ 联系起来，找到 $ \\nabla_{\\epsilon} J(\\mathbf{r}) $ 的表达式。\n\n失配泛函 $J(\\epsilon)$ 由下式给出：\n$$ J(\\epsilon) = \\frac{1}{2} \\sum_{m=1}^{N_{s}} \\left\\| \\mathcal{C} u_{m} - \\mathbf{d}_{m} \\right\\|_{2}^{2} $$\n其中 $u_m$ 通过标量亥姆霍兹方程隐式地依赖于 $\\epsilon$。该范数是 $ \\mathbb{C}^{N_{r}} $ 上的欧几里得范数，由厄米内积 $ \\|\\mathbf{v}\\|_{2}^{2} = \\mathbf{v}^{H}\\mathbf{v} $ 导出。因此，我们可以将 $J(\\epsilon)$ 写为：\n$$ J(\\epsilon) = \\frac{1}{2} \\sum_{m=1}^{N_{s}} (\\mathcal{C} u_{m} - \\mathbf{d}_{m})^{H} (\\mathcal{C} u_{m} - \\mathbf{d}_{m}) $$\n\n首先，我们计算 $J$ 相对于场 $ u_m $ 的变分的一阶变分。一个小的微扰 $ \\epsilon \\rightarrow \\epsilon + \\delta\\epsilon $ 会引起场的一个微扰 $ u_m \\rightarrow u_m + \\delta u_m $。$J$ 中相应的一阶变分为：\n$$ \\delta J = \\frac{1}{2} \\sum_{m=1}^{N_s} \\left[ (\\mathcal{C}\\delta u_m)^H (\\mathcal{C}u_m - \\mathbf{d}_m) + (\\mathcal{C}u_m - \\mathbf{d}_m)^H (\\mathcal{C}\\delta u_m) \\right] $$\n方括号中的两项互为复共轭。因此，它们的和为 $2 \\text{Re} [(\\mathcal{C}u_m - \\mathbf{d}_m)^H (\\mathcal{C}\\delta u_m)] $。\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\left[ (\\mathcal{C}u_m - \\mathbf{d}_m)^H (\\mathcal{C}\\delta u_m) \\right] $$\n该表达式可以用测量算子 $ \\mathcal{C} $ 的定义来写。令 $ (\\mathcal{C}u_m)_j = u_m(\\mathbf{r}_j) $。令 $ (\\mathbf{d}_m)_j = d_{mj} $。\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\left[ \\sum_{j=1}^{N_r} \\overline{(u_m(\\mathbf{r}_j) - d_{mj})} \\, \\delta u_m(\\mathbf{r}_j) \\right] $$\n\n接下来，我们将场微扰 $ \\delta u_m $ 与参数微扰 $ \\delta\\epsilon $ 联系起来。正演场 $ u_m $ 满足状态方程：\n$$ L_\\epsilon u_m(\\mathbf{r}) \\equiv \\left(\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})\\right) u_{m}(\\mathbf{r}) = s_{m}(\\mathbf{r}) $$\n受扰场 $ u_m + \\delta u_m $ 在受扰参数 $ \\epsilon + \\delta\\epsilon $ 下满足相同的方程：\n$$ \\left(\\nabla^{2} + \\omega^{2} \\mu (\\epsilon(\\mathbf{r}) + \\delta\\epsilon(\\mathbf{r}))\\right) (u_{m}(\\mathbf{r}) + \\delta u_m(\\mathbf{r})) = s_{m}(\\mathbf{r}) $$\n展开此方程并仅保留 $ \\delta\\epsilon $ 和 $ \\delta u_m $ 的一阶项：\n$$ (\\nabla^{2} + \\omega^{2} \\mu \\epsilon) u_m + (\\nabla^{2} + \\omega^{2} \\mu \\epsilon) \\delta u_m + (\\omega^{2} \\mu \\delta\\epsilon) u_m + O(\\delta\\epsilon \\delta u_m) = s_m $$\n减去原始状态方程 $ (\\nabla^{2} + \\omega^{2} \\mu \\epsilon) u_m = s_m $，我们得到线性化的正演方程，也称为灵敏度方程：\n$$ L_\\epsilon(\\delta u_m) = (\\nabla^{2} + \\omega^{2} \\mu \\epsilon) \\delta u_m = -\\omega^2 \\mu \\delta\\epsilon \\, u_m $$\n\n现在我们引入伴随态方法来从 $ \\delta J $ 的表达式中消除 $ \\delta u_m $。我们为每个实验 $m$ 定义一个伴随场 $ p_m(\\mathbf{r}) $。我们对复值函数使用标准的 $ L^2(\\Omega) $ 内积，$ \\langle f, g \\rangle = \\int_{\\Omega} f(\\mathbf{r})\\overline{g(\\mathbf{r})} \\, d\\mathbf{r} $。算子 $ A $ 的伴随算子记作 $ A^\\dagger $，并满足 $ \\langle A f, g \\rangle = \\langle f, A^\\dagger g \\rangle $。算子 $ L_\\epsilon = \\nabla^2 + \\omega^2\\mu\\epsilon $ 是自伴的，$ L_\\epsilon^\\dagger = L_\\epsilon $，因为 $ \\omega, \\mu, \\epsilon $ 是实数，并且问题陈述保证了分部积分产生的边界项为零。\n\n我们可以通过引入狄拉克δ函数，使用在 $ \\Omega $ 上的积分来表示 $ \\delta J $：\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\left[ \\int_{\\Omega} \\delta u_m(\\mathbf{r}) \\left( \\sum_{j=1}^{N_r} \\overline{(u_m(\\mathbf{r}_j) - d_{mj})} \\delta(\\mathbf{r} - \\mathbf{r}_j) \\right) d\\mathbf{r} \\right] $$\n这可以用内积的形式写成：\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\langle \\delta u_m, q_m \\rangle $$\n其中 $ q_m(\\mathbf{r}) $ 是第 $m$ 个实验的伴随源：\n$$ q_m(\\mathbf{r}) = \\sum_{j=1}^{N_r} \\overline{(u_m(\\mathbf{r}_j) - d_{mj})} \\delta(\\mathbf{r} - \\mathbf{r}_j) $$\n根据灵敏度方程，我们可以写出 $ \\delta u_m = -L_\\epsilon^{-1}(\\omega^2 \\mu \\delta\\epsilon \\, u_m) $。将此代入 $ \\delta J $ 的表达式中：\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\langle -L_\\epsilon^{-1}(\\omega^2 \\mu \\delta\\epsilon \\, u_m), q_m \\rangle $$\n利用伴随算子的性质 $ \\langle A f, g \\rangle = \\langle f, A^\\dagger g \\rangle $，我们将 $ L_\\epsilon^{-1} $ 移到内积的另一边：\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\langle \\omega^2 \\mu \\delta\\epsilon \\, u_m, -(L_\\epsilon^{-1})^\\dagger q_m \\rangle $$\n我们现在将伴随场 $ p_m $ 定义为 $ p_m = (L_\\epsilon^{-1})^\\dagger q_m $ 的解。这等价于求解伴随方程 $ L_\\epsilon^\\dagger p_m = q_m $。由于 $ L_\\epsilon^\\dagger = L_\\epsilon $，伴随问题是：\n$$ L_\\epsilon p_m(\\mathbf{r}) = (\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})) p_{m}(\\mathbf{r}) = q_m(\\mathbf{r}) $$\n将 $ p_m $ 代回 $ \\delta J $ 的表达式中：\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\langle \\omega^2 \\mu \\delta\\epsilon \\, u_m, -p_m \\rangle $$\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\left[ \\int_{\\Omega} (\\omega^2 \\mu \\delta\\epsilon \\, u_m) \\overline{(-p_m)} \\, d\\mathbf{r} \\right] $$\n由于 $ \\omega, \\mu $ 是实常数，$ \\delta\\epsilon $ 是实微扰，我们可以将它们从实部算子中提出：\n$$ \\delta J = \\sum_{m=1}^{N_s} \\left( -\\omega^2 \\mu \\int_{\\Omega} \\delta\\epsilon(\\mathbf{r}) \\, \\text{Re} \\left[ u_m(\\mathbf{r}) \\overline{p_m(\\mathbf{r})} \\right] \\, d\\mathbf{r} \\right) $$\n交换求和与积分的顺序：\n$$ \\delta J = \\int_{\\Omega} \\left( - \\omega^2 \\mu \\sum_{m=1}^{N_s} \\text{Re} \\left[ u_m(\\mathbf{r}) \\overline{p_m(\\mathbf{r})} \\right] \\right) \\delta\\epsilon(\\mathbf{r}) \\, d\\mathbf{r} $$\n通过将其与定义 $ \\delta J = \\int_{\\Omega} \\nabla_{\\epsilon} J(\\mathbf{r}) \\, \\delta\\epsilon(\\mathbf{r}) \\, d\\mathbf{r} $ 进行比较，我们可以确定泛函梯度。\n\n所需的要素如下：\n正演场方程为，对于 $ m=1, \\dots, N_s $：\n$$ \\left(\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})\\right) u_{m}(\\mathbf{r}) = s_{m}(\\mathbf{r}) \\quad \\text{in } \\Omega $$\n伴随场方程为，对于 $ m=1, \\dots, N_s $：\n$$ \\left(\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})\\right) p_{m}(\\mathbf{r}) = \\sum_{j=1}^{N_r} \\overline{(u_{m}(\\mathbf{r}_{j}) - d_{mj})} \\delta(\\mathbf{r} - \\mathbf{r}_{j}) \\quad \\text{in } \\Omega $$\n泛函梯度的最终表达式为：\n$$ \\nabla_{\\epsilon} J(\\mathbf{r}) = - \\omega^2 \\mu \\sum_{m=1}^{N_s} \\text{Re} \\left[ u_m(\\mathbf{r}) \\overline{p_m(\\mathbf{r})} \\right] $$\n实部也可以写成 $ \\text{Re}[u_m \\overline{p_m}] = \\text{Re}[u_m] \\text{Re}[p_m] + \\text{Im}[u_m] \\text{Im}[p_m] $。该表达式以其最紧凑的形式给出。",
            "answer": "$$ \\boxed{- \\omega^{2} \\mu \\sum_{m=1}^{N_{s}} \\text{Re} \\left[ u_{m}(\\mathbf{r}) \\overline{p_{m}(\\mathbf{r})} \\right]} $$"
        },
        {
            "introduction": "现代机器学习的一大前沿是构建能够内蕴物理定律和几何结构的“物理归纳”模型。本练习将揭示图神经网络（GNN）的“消息传递”机制与有限元法（FEM）中刚度矩阵的组装过程之间深刻的内在联系。通过亲手实现一个模拟电磁场旋度-旋度算子（curl–curl operator）的GNN层，您将学会如何设计出在结构上就与底层物理离散化方案保持一致的神经网络，从而极大地提升模型的性能和泛化能力 ()。",
            "id": "3327879",
            "problem": "您需要设计并实现一个单层图神经网络，该网络需精确模拟使用最低阶边自由度的二维离散化中，旋度-旋度双线性形式的有限元方法组装过程。请从计算电磁学的一个有效基本原理出发，推导出消息传递规则。然后，在图中编码完美电导体（PEC）边界条件，使得边界边在更新过程中被钳位到零。最后，实现一个完整的程序，该程序能针对给定的小型网格和几组参数集计算该层的输出，并按指定格式打印结果。\n\n从以弱形式写出的时谐麦克斯韦旋度-旋度算子开始：给定一个向量测试函数 $v$ 和一个电场 $E$，其双线性形式为\n$$\na(E,v) = \\int_{\\Omega} \\mu^{-1} \\left( \\nabla \\times E \\right) \\cdot \\left( \\nabla \\times v \\right) \\, d\\Omega,\n$$\n其中 $\\mu$ 是磁导率，$\\Omega$ 是计算域。在最低阶基于边的离散化（与索伯列夫空间 $H(\\mathrm{curl})$ 兼容）中，为每个定向边关联一个自由度，该自由度定义为 $E$ 的切向分量沿该边的线积分。在一个由具有分片常数 $\\mu$ 和面积 $A$ 的单元组成的网格上，定义每个单元 $k$ 的离散旋度为\n$$\nc_k(x) = \\frac{1}{A_k} \\sum_{e \\in \\partial k} s_{k,e} \\, x_e,\n$$\n其中 $x_e$ 是边自由度（在本问题中为无量纲量），$A_k$ 是单元 $k$ 的面积，而 $s_{k,e} \\in \\{+1,-1\\}$ 是通过比较边 $e$ 的全局方向与所选的单元 $k$ 边界的逆时针方向而产生的符号。对于单元常数 $\\mu_k$，组装的全局刚度（旋度-旋度）矩阵 $K$ 具有以下结构\n$$\nK = C^{\\top} \\, \\mathrm{diag}\\!\\left( \\frac{\\mu_1^{-1}}{A_1}, \\ldots, \\frac{\\mu_K^{-1}}{A_K} \\right) \\, C,\n$$\n其中 $C \\in \\mathbb{R}^{K \\times E}$ 是带符号的单元-边关联矩阵，其元素为 $C_{k,e} = s_{k,e}$，$K$ 是单元数量，$E$ 是边的数量。换句话说，该双线性形式简化为\n$$\na(x,y) = x^{\\top} K \\, y.\n$$\n\n构建一个图 $\\mathcal{G}$，其中每个节点对应一个定向的网格边，如果两个节点共同界定至少一个单元，则将它们连接起来。该单层图神经网络必须实现如下的消息传递\n$$\ny = \\left( \\mathbf{1} - m \\right) \\odot \\left( K x \\right),\n$$\n其中 $x \\in \\mathbb{R}^E$ 是输入边特征向量（无量纲），$y \\in \\mathbb{R}^E$ 是输出，$m \\in \\{0,1\\}^E$ 是一个编码完美电导体边界条件的二进制边界掩码（对于边界边，$m_e = 1$；对于内部边，$m_e = 0$），$\\odot$ 表示逐元素乘法。这通过将边界节点钳位到零来强制施加狄利克雷条件。\n\n使用以下固定的网格、几何形状和方向：\n- 域被细分为 $2$ 个轴对齐的单位正方形单元：一个左侧单元 $k=0$，面积 $A_0 = 1$；一个右侧单元 $k=1$，面积 $A_1 = 1$。\n- 唯一的定向边按如下方式索引和定向，所有长度均为 $1$（无量纲单位）：\n  - $e_0$：左下方的边，从 $(0,0)$ 到 $(1,0)$（全局方向沿 $+x$ 轴）。\n  - $e_1$：右下方的边，从 $(1,0)$ 到 $(2,0)$（全局方向沿 $+x$ 轴）。\n  - $e_2$：中间的垂直边，从 $(1,0)$ 到 $(1,1)$（全局方向沿 $+y$ 轴）。\n  - $e_4$：左上方的边，从 $(0,1)$ 到 $(1,1)$（全局方向沿 $+x$ 轴）。\n  - $e_5$：右上方的边，从 $(1,1)$ 到 $(2,1)$（全局方向沿 $+x$ 轴）。\n  - $e_6$：左边界的垂直边，从 $(0,0)$ 到 $(0,1)$（全局方向沿 $+y$ 轴）。\n  - $e_7$：右边界的垂直边，从 $(2,0)$ 到 $(2,1)$（全局方向沿 $+y$ 轴）。\n- 每个单元边界的逆时针遍历导出了带符号的关联矩阵 $C \\in \\mathbb{R}^{2 \\times 7}$，其非零元素如下\n  - 对于左侧单元 $k=0$：$C_{0,0} = +1$, $C_{0,2} = +1$, $C_{0,4} = -1$, $C_{0,6} = -1$。\n  - 对于右侧单元 $k=1$：$C_{1,1} = +1$, $C_{1,7} = +1$, $C_{1,5} = -1$, $C_{1,2} = -1$。\n$C$ 的所有其他元素均为 $0$。\n\n定义边界掩码 $m \\in \\{0,1\\}^7$，其中对于边界边，$m_e = 1$；对于内部边，$m_e = 0$。在此网格中，$e_2$ 是唯一的内部边，因此选择\n$$\nm = [1,\\, 1,\\, 0,\\, 1,\\, 1,\\, 1,\\, 1],\n$$\n按边索引 $[e_0, e_1, e_2, e_4, e_5, e_6, e_7]$ 排序。\n\n您的程序必须：\n- 使用公式 $K = C^{\\top} \\, \\mathrm{diag}\\!\\left( \\mu_0^{-1}/A_0,\\, \\mu_1^{-1}/A_1 \\right) \\, C$ 从 $C$ 和 $(\\mu_0, \\mu_1)$ 组装 $K$。\n- 计算单层输出 $y = \\left( \\mathbf{1} - m \\right) \\odot \\left( K x \\right)$。\n- 使用以下测试套件，它涵盖了多种不同情况：\n  $1.$ 正常路径：$\\mu_0 = 1$, $\\mu_1 = 1$, $x = [1,\\, 2,\\, -1,\\, 0.5,\\, -0.5,\\, 0,\\, 0]$。\n  $2.$ 材料对比：$\\mu_0 = 0.5$, $\\mu_1 = 2$, $x = [-0.25,\\, 0.75,\\, 1,\\, -1.25,\\, 0,\\, 0.25,\\, -0.25]$。\n  $3.$ 边界驱动激励：$\\mu_0 = 1$, $\\mu_1 = 1$, $x = [0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 3,\\, -3]$。\n\n本问题中所有量均为无量纲。对于每个测试用例，返回完整的输出向量 $y \\in \\mathbb{R}^7$，形式为浮点数列表，四舍五入到六位小数。\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个条目对应一个测试用例，其本身也是一个由方括号括起来的、包含其 $7$ 个四舍五入后输出值的逗号分隔列表。例如，输出格式必须为\n$$\n[ [y_{1,0}, y_{1,1}, \\ldots, y_{1,6}], [y_{2,0}, \\ldots, y_{2,6}], [y_{3,0}, \\ldots, y_{3,6}] ].\n$$",
            "solution": "该问题在科学上和数学上都是合理的，提供了一个自洽且适定的计算任务。它要求实现一个单层图神经网络（GNN），该网络能精确再现有限元方法（FEM）的旋度-旋度算子在基于边的自由度向量上的作用。该问题基于计算电磁学的原理。所有必要的数据，包括网格几何、材料属性和输入向量，均已提供。因此，该问题被认为是有效的，并将在此提出一个解决方案。\n\n问题的核心是计算 GNN 层的输出向量 $y \\in \\mathbb{R}^7$，该操作定义为\n$$\ny = \\left( \\mathbf{1} - m \\right) \\odot \\left( K x \\right),\n$$\n其中 $x \\in \\mathbb{R}^7$ 是输入边特征向量，$K \\in \\mathbb{R}^{7 \\times 7}$ 是表示 GNN 线性变换的矩阵，$m \\in \\{0, 1\\}^7$ 是一个二进制掩码，$\\mathbf{1}$ 是全一向量，$\\odot$ 表示逐元素（哈达玛）积。\n\n矩阵 $K$ 是从时谐麦克斯韦旋度-旋度弱形式的 FEM 离散化中导出的全局刚度矩阵，\n$$\na(E,v) = \\int_{\\Omega} \\mu^{-1} \\left( \\nabla \\times E \\right) \\cdot \\left( \\nabla \\times v \\right) \\, d\\Omega.\n$$\n在最低阶边元框架中（例如，三角形或四边形上的第一类 Nédélec 元），自由度 $x_e$ 表示电场的切向分量沿每个网格边 $e$ 的积分。该双线性形式的离散对应项是 $x^{\\top} K y$。问题提供了 $K$ 的组装结构为\n$$\nK = C^{\\top} D C,\n$$\n其中 $C \\in \\mathbb{R}^{2 \\times 7}$ 是带符号的单元-边关联矩阵，$D \\in \\mathbb{R}^{2 \\times 2}$ 是一个包含材料和几何信息的对角矩阵。\n\n操作 $Kx = C^{\\top} D C x$ 可以解释为网格图上的一种消息传递方案，这建立了与 GNN 的联系。计算过程分为三个步骤：\n1.  **聚合（从边到单元）：**乘积 $z = C x$ 计算出一个维度为 $2$（单元数量）的向量。每个分量 $z_k = (Cx)_k = \\sum_{e} C_{k,e} x_e$ 是场 $x$ 围绕单元 $k$ 边界的离散环量。根据离散旋度的定义 $c_k(x) = \\frac{1}{A_k} z_k$，此步骤相当于计算每个单元内场的旋度，并按单元面积进行缩放。\n2.  **变换（逐单元）：**乘积 $w = Dz$ 对每个单元的聚合值进行缩放。这里，$D = \\mathrm{diag}(\\mu_0^{-1}/A_0, \\mu_1^{-1}/A_1)$。此步骤将物理属性（磁导率的倒数 $\\mu^{-1}$）和几何缩放（面积的倒数 $A^{-1}$）应用于单元级量。\n3.  **传播（从单元到边）：**乘积 $C^{\\top} w$ 将变换后的单元级数据映射回边。矩阵 $C^{\\top}$ 充当“散布”或离散类梯度算子。对于每个边 $e$，其结果值是它所共同界定的两个单元贡献的总和，并正确考虑了方向。\n\n最后，操作 $y = (\\mathbf{1} - m) \\odot (Kx)$ 应用了完美电导体（PEC）边界条件。在 PEC 边界上，电场的切向分量必须为零。掩码 $m$ 对于边界边为 $1$，对于内部边为 $0$。因此，向量 $(\\mathbf{1} - m)$ 是一个内部边的选择器。逐元素乘积将所有边界边上的输出值钳位到 $0$，从而强制施加齐次狄利克雷条件 $y_e = 0$（对于 $e \\in \\partial \\Omega$）。\n\n为实现该解决方案，我们首先根据问题给定的信息构建矩阵。边按 $[e_0, e_1, e_2, e_4, e_5, e_6, e_7]$ 的顺序排列，对应索引 $0$ 到 $6$。\n\n带符号关联矩阵 $C \\in \\mathbb{R}^{2 \\times 7}$ 是根据指定的方向构建的。\n对于单元 $k=0$（第 $0$ 行）：$C_{0,0}=+1$，$C_{0,2}=+1$，$C_{0,4}=-1$（对于索引为 $3$ 的边 $e_4$），$C_{0,6}=-1$（对于索引为 $5$ 的边 $e_6$）。\n对于单元 $k=1$（第 $1$ 行）：$C_{1,1}=+1$，$C_{1,7}=+1$（对于索引为 $6$ 的边 $e_7$），$C_{1,5}=-1$（对于索引为 $4$ 的边 $e_5$），$C_{1,2}=-1$。\n这得到了矩阵：\n$$\nC = \\begin{pmatrix}\n1  0  1  -1  0  -1  0 \\\\\n0  1  -1  0  -1  0  1\n\\end{pmatrix}\n$$\n边界掩码给定为 $m = [1, 1, 0, 1, 1, 1, 1]$，它正确地将边 $e_2$（在索引 $2$ 处）识别为唯一的内部边。因此，内部选择器为：\n$$\n\\mathbf{1} - m = [0, 0, 1, 0, 0, 0, 0]\n$$\n单元面积为 $A_0 = 1$ 和 $A_1 = 1$。对于给定的具有磁导率 $(\\mu_0, \\mu_1)$ 的测试用例，对角矩阵 $D$ 为：\n$$\nD = \\begin{pmatrix} \\mu_0^{-1}  0 \\\\ 0  \\mu_1^{-1} \\end{pmatrix}\n$$\n对于每个测试用例，我们从给定的 $(\\mu_0, \\mu_1)$ 构建 $D$，组装 $K = C^{\\top} D C$，并使用提供的输入向量 $x$ 计算 $y = (\\mathbf{1}-m) \\odot (Kx)$。每个用例的结果都四舍五入到六位小数。\n\n对于测试用例 1：$(\\mu_0, \\mu_1) = (1, 1)$, $x = [1, 2, -1, 0.5, -0.5, 0, 0]$。\n$D = \\mathrm{diag}(1, 1) = I$。\n$K = C^{\\top} C$。\n$Kx = [-0.5, 3.5, -4.0, 0.5, -3.5, 0.5, 3.5]^{\\top}$。\n$y = [0, 0, -4.0, 0, 0, 0, 0]^{\\top}$。\n\n对于测试用例 2：$(\\mu_0, \\mu_1) = (0.5, 2)$, $x = [-0.25, 0.75, 1, -1.25, 0, 0.25, -0.25]$。\n$D = \\mathrm{diag}(1/0.5, 1/2) = \\mathrm{diag}(2, 0.5)$。\n$K = C^{\\top} D C$。\n$Kx = [3.5, -0.25, 3.75, -3.5, 0.25, -3.5, -0.25]^{\\top}$。\n$y = [0, 0, 3.75, 0, 0, 0, 0]^{\\top}$。\n\n对于测试用例 3：$(\\mu_0, \\mu_1) = (1, 1)$, $x = [0, 0, 0, 0, 0, 3, -3]$。\n$D = \\mathrm{diag}(1, 1) = I$。\n$K = C^{\\top} C$。\n$Kx = [-3.0, -3.0, 0.0, 3.0, 3.0, 3.0, -3.0]^{\\top}$。\n$y = [0, 0, 0.0, 0, 0, 0, 0]^{\\top}$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the output of a single-layer graph-neural-network-like operator\n    that mirrors the FEM assembly of a curl-curl bilinear form.\n    \"\"\"\n    # Define the mesh and problem geometry.\n    # The cell-edge incidence matrix C is 2x7.\n    # Rows correspond to cells k=0 (left) and k=1 (right).\n    # Columns correspond to edges ordered as [e0, e1, e2, e4, e5, e6, e7].\n    C = np.array([\n        [1, 0, 1, -1, 0, -1, 0],  # Cell k=0\n        [0, 1, -1, 0, -1, 0, 1]   # Cell k=1\n    ], dtype=np.float64)\n\n    # Cell areas.\n    A0, A1 = 1.0, 1.0\n\n    # Boundary mask m: 1 for boundary edges, 0 for interior edges.\n    # The unique interior edge is e2, at index 2.\n    m = np.array([1, 1, 0, 1, 1, 1, 1], dtype=np.float64)\n\n    # Selector for interior edges, (1 - m).\n    interior_selector = 1.0 - m\n\n    # Define the test cases.\n    test_cases = [\n        {\n            \"mu0\": 1.0, \"mu1\": 1.0,\n            \"x\": np.array([1.0, 2.0, -1.0, 0.5, -0.5, 0.0, 0.0], dtype=np.float64)\n        },\n        {\n            \"mu0\": 0.5, \"mu1\": 2.0,\n            \"x\": np.array([-0.25, 0.75, 1.0, -1.25, 0.0, 0.25, -0.25], dtype=np.float64)\n        },\n        {\n            \"mu0\": 1.0, \"mu1\": 1.0,\n            \"x\": np.array([0.0, 0.0, 0.0, 0.0, 0.0, 3.0, -3.0], dtype=np.float64)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mu0, mu1 = case[\"mu0\"], case[\"mu1\"]\n        x = case[\"x\"]\n\n        # 1. Construct the diagonal matrix D of material/geometric properties.\n        D = np.diag([1.0 / (mu0 * A0), 1.0 / (mu1 * A1)])\n\n        # 2. Assemble the stiffness matrix K = C^T * D * C.\n        K = C.T @ D @ C\n        \n        # 3. Apply the linear operator: Kx.\n        Kx = K @ x\n\n        # 4. Apply the boundary condition by elementwise-multiplying with the\n        #    interior edge selector.\n        y = interior_selector * Kx\n\n        # 5. Round the results to six decimal places and format as a list.\n        y_rounded = [round(val, 6) for val in y]\n        results.append(y_rounded)\n\n    # Final print statement in the exact required format.\n    # The format is a string representation of a list of lists.\n    # str() on a list automatically adds spaces, e.g., '[1.0, 2.0]'.\n    # ','.join() then combines these strings.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "机器学习不仅能用于求解正问题和逆问题，还能作为强大的诊断工具来提升传统数值模拟的可靠性和自动化水平。在电磁本征值问题的有限元分析中，伪模（spurious modes）的出现是一个长期存在的难题，它们是无物理意义的计算假象。本练习将引导您完成一个完整的项目：构建并训练一个分类器，利用从物理约束（如无散度条件）中提取的特征，来自动识别并报告这些伪模，从而将机器学习集成到复杂的科学计算工作流中 ()。",
            "id": "3327863",
            "problem": "您的任务是设计并实现一个完整的、可运行的程序，该程序训练一个二元分类器，用于检测计算电磁学中离散旋度-旋度本征问题产生的伪本征模。分类器使用从散度约束和模式正交性派生的特征，并在此基础上提出自动化的网格修复建议。该分类器必须在程序自身生成的合成数据集上进行训练，该数据集基于离散磁场模式的物理原理构建。整个程序必须是自包含的，并为提供的测试套件生成一个单行输出，汇总所有结果。\n\n从电磁学定律开始，并从数学上推导出用于分类的特征。使用以下基本原理：\n- 宏观介质的麦克斯韦方程组，包括磁通量密度的无散度约束 $\\,\\nabla \\cdot \\mathbf{B} = 0\\,$，以及电场 $\\,\\mathbf{E}\\,$ 的无源旋度-旋度本征问题\n$$\n\\nabla \\times \\left( \\mu^{-1} \\nabla \\times \\mathbf{E} \\right) = \\lambda \\, \\varepsilon \\, \\mathbf{E},\n$$\n在一个单连通域中，具有适当的边界条件，其中 $\\,\\mu\\,$ 是磁导率，$\\,\\varepsilon\\,$ 是电容率，$\\,\\lambda\\,$ 是本征值。\n- 在与均匀介质中自伴旋度-旋度算子相关的能量内积下，不同本征模的正交性，这意味着对于不同的本征函数 $\\,\\mathbf{E}_i\\,$ 和 $\\,\\mathbf{E}_j\\,$\n$$\n\\int_{\\Omega} \\varepsilon \\, \\mathbf{E}_i \\cdot \\mathbf{E}_j \\, d\\Omega = 0.\n$$\n- 在离散设置中，伪模式通常源于对正合序列性质的违反，导致非螺线场或模式重复；因此，有原则的特征应该量化散度和（非）正交性。\n\n您的程序必须：\n1. 在均匀矩形网格上，构建一个离散二维磁场模式 $\\,\\mathbf{B}(x,y) = \\left(B_x(x,y), B_y(x,y)\\right)\\,$ 的合成数据集。生成两种类型的模式：\n   - 物理上合理的无散度模式，由保证网格上 $\\,\\nabla \\cdot \\mathbf{B} \\approx 0\\,$ 的解析模式构建。\n   - 伪模式，构造为标量势 $\\,\\phi(x,y)\\,$ 的梯度，使得 $\\,\\mathbf{B} = \\nabla \\phi\\,$，通常情况下 $\\,\\nabla \\cdot \\mathbf{B} = \\nabla^2 \\phi \\neq 0\\,$。\n2. 对于给定均匀网格上间距为 $\\,h_x\\,$ 和 $\\,h_y\\,$ 的任意离散模式对 $\\,(\\mathbf{B}^{(1)}, \\mathbf{B}^{(2)})\\,$，为每个模式计算以下特征：\n   - 相对散度大小\n     $$\n     R_{\\mathrm{div}} = \\frac{\\left\\| \\nabla \\cdot \\mathbf{B} \\right\\|_{L^2}}{\\left\\| \\mathbf{B} \\right\\|_{L^2} + \\epsilon},\n     $$\n     其中 $\\,\\epsilon  0\\,$ 是一个小的稳定化常数，$L^2$范数使用面积元 $\\,h_x h_y\\,$ 进行离散化。使用离散中心差分计算内部点，并在边界使用一致的单边差分来近似 $\\,\\partial B_x / \\partial x\\,$ 和 $\\,\\partial B_y / \\partial y\\,$。\n   - 两种模式之间的非正交性指数\n     $$\n     R_{\\mathrm{nonorth}}^{(i)} = \\frac{\\left| \\langle \\mathbf{B}^{(i)}, \\mathbf{B}^{(j)} \\rangle \\right|}{\\left\\| \\mathbf{B}^{(i)} \\right\\|_{L^2} \\, \\left\\| \\mathbf{B}^{(j)} \\right\\|_{L^2} + \\epsilon}, \\quad i \\neq j,\n     $$\n     使用离散内积 $\\,\\langle \\mathbf{U}, \\mathbf{V} \\rangle = \\sum \\left(U_x V_x + U_y V_y\\right) h_x h_y\\,$。\n   - 网格各向异性质量度量\n     $$\n     Q_{\\mathrm{aspect}} = \\max\\left( \\frac{h_x}{h_y}, \\frac{h_y}{h_x} \\right).\n     $$\n3. 使用带 $L^2$ 正则化的逻辑回归训练一个二元分类器，根据特征向量\n   $$\n   \\mathbf{x} = \\left[ R_{\\mathrm{div}}, \\, R_{\\mathrm{nonorth}}, \\, Q_{\\mathrm{aspect}} \\right],\n   $$\n   使用合成数据集预测一个模式是伪模式（$\\,1\\,$）还是物理模式（$\\,0\\,$）。在训练前，使用训练集的均值和标准差对特征进行标准化。分类器必须在程序中直接实现（不使用外部机器学习库），使用批量梯度下降法优化带 $L^2$ 惩罚项的交叉熵损失。为保证可复现性，请固定随机种子。\n4. 对于每个测试案例，计算两种模式的特征，应用训练好的分类器获得预测标签（布尔值），然后根据以下基于规则的映射，为该案例提出一个单一的自动化网格修复建议代码：\n   - 如果 $\\,Q_{\\mathrm{aspect}}  3.0\\,$，建议代码为 $\\,1\\,$（建议进行局部各向同性细化以减少各向异性）。\n   - 否则，如果任一模式被预测为伪模式且其 $\\,R_{\\mathrm{div}}  0.15\\,$，建议代码为 $\\,2\\,$（建议采用保散离散化或添加散度惩罚项）。\n   - 否则，如果 $\\,\\max\\left(R_{\\mathrm{nonorth}}^{(1)}, R_{\\mathrm{nonorth}}^{(2)}\\right)  0.95\\,$，建议代码为 $\\,3\\,$（建议进行正交化或改进本征求解器设置以分离几乎重复的模式）。\n   - 否则，建议代码为 $\\,0\\,$（无需修复）。\n5. 确定性地实现整个流程，并生成一个单行输出，其中包含测试套件的结果列表，每个测试案例的结果是一个形如\n   $$\n   \\left[ \\text{label}^{(1)}, \\, \\text{label}^{(2)}, \\, \\text{suggestion\\_code} \\right]\n   $$\n   的列表，其中两个标签是布尔值，建议代码是整数。\n\n程序中的所有量均为无量纲。解析函数中出现的角度（如果存在）均以弧度为单位。最终输出行必须是Python列表的精确字符串表示，例如\n$$\n\\left[ [\\text{True}, \\, \\text{False}, \\, 2], \\, [\\text{False}, \\, \\text{False}, \\, 0] \\right].\n$$\n\n测试套件：\n提供四个具有科学意义的案例，用于检验分类器的不同方面和建议逻辑。所有测试中，每个方向的网格点数均为 $\\,n_x = 64\\,$，$\\,n_y = 64\\,$。设域大小为 $\\,L_x\\,$ 和 $\\,L_y\\,$，因此间距为 $\\,h_x = L_x / (n_x - 1)\\,$ 和 $\\,h_y = L_y / (n_y - 1)\\,$。\n- 案例 $\\,1\\,$ (分辨良好的物理模式，理想路径)：$\\,L_x = 1.0\\,$，$\\,L_y = 1.0\\,$。模式 $\\,1\\,$ 是无散度的，波数 $\\,k = 3\\,$。模式 $\\,2\\,$ 是无散度的，波数 $\\,k = 5\\,$。\n- 案例 $\\,2\\,$ (一个伪梯度模式)：$\\,L_x = 1.0\\,$，$\\,L_y = 1.0\\,$。模式 $\\,1\\,$ 是来自标量势 $\\,\\phi(x,y) = \\sin(10 x)\\sin(10 y)\\,$ 的梯度场。模式 $\\,2\\,$ 是无散度的，波数 $\\,k = 4\\,$。\n- 案例 $\\,3\\,$ (各向异性网格质量问题)：$\\,L_x = 1.0\\,$，$\\,L_y = 5.0\\,$。模式 $\\,1\\,$ 是无散度的，波数 $\\,k = 4\\,$。模式 $\\,2\\,$ 是无散度的，波数 $\\,k = 6\\,$。\n- 案例 $\\,4\\,$ (几乎重复的模式，高度非正交)：$\\,L_x = 1.0\\,$，$\\,L_y = 1.0\\,$。模式 $\\,1\\,$ 是无散度的，波数 $\\,k = 8\\,$。模式 $\\,2\\,$ 等于模式 $\\,1\\,$ 加上大小为 $\\,10^{-3}\\,$ 的小加性噪声到每个分量。\n\n您的程序应生成一个单行输出，其中包含结果，形式为一个用方括号括起来的逗号分隔列表，每个测试案例结果本身就是一个列表 $\\,\\left[\\text{label}^{(1)}, \\text{label}^{(2)}, \\text{suggestion\\_code}\\right]\\,$，例如\n$$\n\\left[ [\\text{False}, \\, \\text{False}, \\, 0], \\, [\\text{True}, \\, \\text{False}, \\, 2], \\, [\\text{False}, \\, \\text{False}, \\, 1], \\, [\\text{False}, \\, \\text{False}, \\, 3] \\right].\n$$\n\n实现约束：\n- 仅使用Python语言运行时和执行环境中指定的库。不允许使用外部机器学习库。\n- 在程序中固定随机数生成器的种子以确保确定性行为。\n- 训练数据集必须包含至少 $\\,200\\,$ 个模式，这些模式从上述解析结构中抽取，并包含均衡的物理和伪示例。",
            "solution": "用户希望一位严谨苛刻的STEM教授来验证并解决一个问题。\n\n该问题要求我实现一个完整的程序，执行以下任务：\n1.  **生成一个合成数据集**，包含二维电磁模式，分为“物理”（无散度）和“伪”（梯度场）两类。\n2.  **为每个模式计算基于物理原理的特征**：相对散度（$R_{\\mathrm{div}}$）、与配对模式的非正交性（$R_{\\mathrm{nonorth}}$）以及网格纵横比（$Q_{\\mathrm{aspect}}$）。\n3.  **从零开始训练一个逻辑回归分类器**，根据这些特征区分物理模式和伪模式。\n4.  **将训练好的分类器应用于一个预定义的测试套件**。\n5.  **根据应用于特征和分类结果的一套规则，提出自动化的网格修复建议**。\n\n整个过程必须是确定性和自包含的，仅使用指定的Python库（`numpy`，`scipy`）。\n\n### 问题陈述验证\n\n我已根据所需标准仔细审查了问题陈述。\n\n*   **科学依据**：这个问题非常出色。它深深植根于计算电磁学中一个真实而重大的挑战：在麦克斯韦旋度-旋度方程的数值本征求解器中出现非物理的“伪”解。所选的特征（$R_{\\mathrm{div}}$、$R_{\\mathrm{nonorth}}$）直接对应于在希尔伯特空间设置中区分真实本征模与伪本征模的基本数学性质（即，遵守散度约束和正交性）。构建合成数据的方法（使用流函数生成无散度场，使用标量势生成伪梯度场）也是标准且物理上合理的。将机器学习用作数值模拟的诊断工具这一整体概念是一个 contemporary and valid research direction.\n\n*   **适定性与完整性**：该问题陈述的适定性极佳。它提供了：\n    *   所有特征的明确数学公式。\n    *   分类器的精确算法（通过批量梯度下降的带L2正则化的逻辑回归）。\n    *   关于数据生成和数值方法的具体说明（例如，使用中心/单边差分计算导数）。\n    *   一个包含所有必要参数的完整测试套件。\n    *   用于最终“网格修复”建议逻辑的明确规则。\n    *   一个精确的输出格式。\n    *   对执行环境的约束。\n\n    唯一的小模糊之处是“波数为k的无散度模式”的确切形式，但这可以通过采用流函数方法轻松解决，物理描述中已强烈暗示了这一点。\n\n*   **客观性**：该问题完全是客观的，使用数学方程、计算算法和量化规则进行规定。没有任何主观元素。\n\n**结论**：该问题是**有效的**。这是一个设计良好、科学合理且富有挑战性的任务，非常适合采用严谨、基于原则的解决方案。\n\n### 基于原则的解决方案设计\n\n解决方案将通过将底层的物理和数学原理直接转化为计算算法来开发。\n\n#### 1. 理论基础：旋度-旋度本征问题与伪模式\n\n起点是域 $\\Omega$ 中电场 $\\mathbf{E}$ 的无源旋度-旋度本征问题，表述为：\n$$\n\\nabla \\times \\left( \\mu^{-1} \\nabla \\times \\mathbf{E} \\right) = \\lambda \\varepsilon \\mathbf{E}\n$$\n其中 $\\mu$ 是磁导率，$\\varepsilon$ 是电容率，$\\lambda = \\omega^2$ 是对应于共振频率平方 $\\omega^2$ 的本征值。算子 $\\mathcal{L} = \\nabla \\times (\\mu^{-1} \\nabla \\times \\cdot)$ 在能量内积 $\\langle \\mathbf{u}, \\mathbf{v} \\rangle_{\\varepsilon} = \\int_{\\Omega} \\varepsilon \\, \\mathbf{u} \\cdot \\mathbf{v} \\, d\\Omega$ 下是自伴的。自伴算子的一个关键性质是其对应于不同本征值的本征函数是正交的：\n$$\n\\text{对于 } \\lambda_i \\neq \\lambda_j, \\quad \\langle \\mathbf{E}_i, \\mathbf{E}_j \\rangle_{\\varepsilon} = \\int_{\\Omega} \\varepsilon \\, \\mathbf{E}_i \\cdot \\mathbf{E}_j \\, d\\Omega = 0\n$$\n此外，在无源区域，麦克斯韦方程组施加了无散度条件 $\\nabla \\cdot \\mathbf{D} = 0$，对于均匀介质（$\\varepsilon$ 是常数），这简化为 $\\nabla \\cdot \\mathbf{E} = 0$。\n\n当这个连续问题被离散化（例如，使用有限元法）时，离散算子可能无法完美复制其连续对应物的性质。这种缺陷可能导致污染计算谱的*伪模式*的出现。这些伪解通常分为两类：\n1.  **非螺线模式**：错误地具有非零散度（$\\nabla \\cdot \\mathbf{E} \\neq 0$）的解。\n2.  **梯度场模式**：第一类的一个子集，这些是形如 $\\mathbf{E} = \\nabla \\phi$ 的非物理、零频率解，其中 $\\phi$ 是一个标量势。这类场被错误地纳入解空间，因为它们位于旋度算子的零空间中（$\\nabla \\times (\\nabla \\phi) = 0$），而离散旋度算子可能无法正确捕捉到这一点。\n\n本项目模拟了磁场 $\\mathbf{B}$ 的类似问题，它受相似的物理学支配（$\\nabla \\cdot \\mathbf{B} = 0$）。\n\n#### 2. 从第一性原理进行特征工程\n\n分类任务依赖于旨在检测这些伪模式特征的特征。\n\n*   **相对散度（$R_{\\mathrm{div}}$）**：此特征直接量化了对基本物理定律 $\\nabla \\cdot \\mathbf{B} = 0$ 的违反程度。一个真实的物理模式应具有为零（解析上）或非常小（数值上，由于离散化误差）的散度。一个伪梯度场模式 $\\mathbf{B} = \\nabla\\phi$ 将具有散度 $\\nabla \\cdot \\mathbf{B} = \\nabla^2\\phi$，这通常是非零的。该特征定义为：\n    $$\n    R_{\\mathrm{div}} = \\frac{\\left\\| \\nabla \\cdot \\mathbf{B} \\right\\|_{L^2}}{\\left\\| \\mathbf{B} \\right\\|_{L^2} + \\epsilon}\n    $$\n    这种归一化使该特征与模式的幅度无关。\n\n*   **非正交性指数（$R_{\\mathrm{nonorth}}$）**：此特征检查是否违反了不同本征模的正交性属性。数值问题，如几乎简并的本征值，可能导致计算出的模式不完全正交。该特征，定义为两个模式向量之间夹角的余弦绝对值，可以检测到这一点：\n    $$\n    R_{\\mathrm{nonorth}}^{(i)} = \\frac{\\left| \\langle \\mathbf{B}^{(i)}, \\mathbf{B}^{(j)} \\rangle \\right|}{\\left\\| \\mathbf{B}^{(i)} \\right\\|_{L^2} \\, \\left\\| \\mathbf{B}^{(j)} \\right\\|_{L^2} + \\epsilon}\n    $$\n    接近 $0$ 的值表示正交，而接近 $1$ 的值表示线性相关（例如，重复模式）。离散内积 $\\langle \\mathbf{U}, \\mathbf{V} \\rangle = \\sum (U_x V_x + U_y V_y) h_x h_y$ 是未加权积分 $\\int \\mathbf{U} \\cdot \\mathbf{V} \\, d\\Omega$ 的离散近似。\n\n*   **网格各向异性（$Q_{\\mathrm{aspect}}$）**：差的网格质量，特别是单元的高纵横比，已知会降低数值解的准确性，并可能加剧伪模式问题。此特征是一个标准的网格质量度量：\n    $$\n    Q_{\\mathrmaspect}} = \\max\\left( \\frac{h_x}{h_y}, \\frac{h_y}{h_x} \\right)\n    $$\n\n#### 3. 合成数据生成\n\n为了训练分类器，我们需要一个带标签的数据集。我们基于物理模式和伪模式的理论特性构建一个。\n\n*   **物理（无散度）模式**：这些是使用二维流函数 $\\psi(x,y)$ 构建的。磁场定义为矢量势 $\\mathbf{A} = (0, 0, \\psi(x,y))$ 的旋度，产生 $\\mathbf{B} = \\nabla \\times \\mathbf{A} = (\\partial_y \\psi, -\\partial_x \\psi)$。根据构造，$\\nabla \\cdot \\mathbf{B} = \\partial_x(\\partial_y \\psi) + \\partial_y(-\\partial_x \\psi) = 0$。我们使用正弦函数作为 $\\psi$ 来模拟振荡的本征模。\n\n*   **伪（梯度场）模式**：这些是作为标量势 $\\phi(x,y)$ 的梯度构建的，$\\mathbf{B} = \\nabla\\phi = (\\partial_x \\phi, \\partial_y \\phi)$。这些场是无旋的，但通常不是无散度的。这模拟了许多数值方案中伪解的主要来源。\n\n#### 4. 逻辑回归分类器\n\n实现一个二元逻辑回归模型来将模式分类为物理（$y=0$）或伪（$y=1$）。\n\n*   **模型**：模式为伪模式的概率建模为 $P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)$，其中 $\\mathbf{x} = [R_{\\mathrm{div}}, R_{\\mathrm{nonorth}}, Q_{\\mathrm{aspect}}]$ 是特征向量，$\\mathbf{w}$ 是权重， $b$ 是偏置，$\\sigma(z) = 1/(1+e^{-z})$ 是 sigmoid 函数。\n*   **训练**：模型参数 $(\\mathbf{w}, b)$ 通过使用批量梯度下降最小化正则化的二元交叉熵损失函数来优化。损失函数为：\n    $$\n    J(\\mathbf{w}, b) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log(p^{(i)}) + (1-y^{(i)}) \\log(1-p^{(i)}) \\right] + \\frac{\\alpha_{reg}}{2m} \\|\\mathbf{w}\\|_2^2\n    $$\n    其中 $m$ 是训练样本数，$p^{(i)}$ 是样本 $i$ 的预测概率，$\\alpha_{reg}$ 是L2正则化参数。在训练之前，特征被标准化为零均值和单位方差，以改善梯度下降的收敛性。\n\n#### 5. 自动化修复逻辑\n\n最后一步将分析与实际行动联系起来。一个简单的、基于规则的专家系统使用分类器的输出和特征值来为数值模拟设置建议一个纠正措施，为提高模拟质量提供了一个初步的反馈回路。这些规则直接针对由特征识别出的问题。\n\n这完成了解决方案的原则性设计。`final_answer` 块中的实现将遵循此结构。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the entire pipeline: dataset generation,\n    feature computation, classifier training, and testing.\n    \"\"\"\n    # Fix the random seed for reproducibility\n    np.random.seed(42)\n\n    # --- Global Constants ---\n    STABILIZATION_EPS = 1e-9\n    TRAINING_SAMPLES = 200  # Will be created from 100 pairs\n    GRID_SIZE_TRAIN = 32\n    \n    # Logistic Regression Hyperparameters\n    LEARNING_RATE = 0.1\n    ITERATIONS = 2000\n    REG_PARAM = 0.01\n\n    # --- Helper Functions for Grid and Mode Generation ---\n    def make_grid(nx, ny, Lx, Ly):\n        \"\"\"Creates a 2D mesh grid.\"\"\"\n        x = np.linspace(0, Lx, nx)\n        y = np.linspace(0, Ly, ny)\n        X, Y = np.meshgrid(x, y)\n        hx = Lx / (nx - 1)\n        hy = Ly / (ny - 1)\n        return X, Y, hx, hy\n\n    def generate_physical_mode(k_val, X, Y):\n        \"\"\"Generates a divergence-free mode from a stream function.\"\"\"\n        # psi = sin(kx*x) * sin(ky*y)\n        # Bx = d(psi)/dy, By = -d(psi)/dx\n        kx = k_val\n        ky = k_val\n        psi = np.sin(kx * X) * np.sin(ky * Y)\n        Bx = ky * np.sin(kx * X) * np.cos(ky * Y)\n        By = -kx * np.cos(kx * X) * np.sin(ky * Y)\n        return Bx, By\n\n    def generate_spurious_mode(phi_func, X, Y):\n        \"\"\"Generates a spurious gradient-field mode from a scalar potential.\"\"\"\n        # B = grad(phi)\n        # Bx = d(phi)/dx, By = d(phi)/dy\n        phi = phi_func(X, Y)\n        # np.gradient returns derivatives along axis 0 (y), then axis 1 (x)\n        gy, gx = np.gradient(phi, Y[0,1]-Y[0,0], X[1,0]-X[0,0])\n        return gx, gy\n\n    # --- Feature Computation Functions ---\n    def compute_divergence(Bx, By, hx, hy):\n        \"\"\"Computes the divergence of a 2D vector field using np.gradient.\"\"\"\n        _, dBx_dx = np.gradient(Bx, hy, hx)\n        dBy_dy, _ = np.gradient(By, hy, hx)\n        return dBx_dx + dBy_dy\n\n    def compute_l2_norm(B_comp, hx, hy):\n        \"\"\"Computes the discrete L2 norm of a vector field.\"\"\"\n        Bx, By = B_comp\n        integrand = Bx**2 + By**2\n        return np.sqrt(np.sum(integrand) * hx * hy)\n\n    def compute_inner_product(B1_comp, B2_comp, hx, hy):\n        \"\"\"Computes the discrete inner product of two vector fields.\"\"\"\n        B1x, B1y = B1_comp\n        B2x, B2y = B2_comp\n        integrand = B1x * B2x + B1y * B2y\n        return np.sum(integrand) * hx * hy\n\n    def compute_features(B1_comp, B2_comp, hx, hy):\n        \"\"\"Computes the full feature set for a pair of modes.\"\"\"\n        # Feature 3: Mesh Anisotropy\n        Q_aspect = max(hx / hy, hy / hx)\n\n        # Compute norms for both modes\n        norm_B1 = compute_l2_norm(B1_comp, hx, hy)\n        norm_B2 = compute_l2_norm(B2_comp, hx, hy)\n\n        # Feature 1: Relative Divergence for each mode\n        div_B1 = compute_divergence(B1_comp[0], B1_comp[1], hx, hy)\n        norm_div_B1 = np.sqrt(np.sum(div_B1**2) * hx * hy)\n        R_div1 = norm_div_B1 / (norm_B1 + STABILIZATION_EPS)\n\n        div_B2 = compute_divergence(B2_comp[0], B2_comp[1], hx, hy)\n        norm_div_B2 = np.sqrt(np.sum(div_B2**2) * hx * hy)\n        R_div2 = norm_div_B2 / (norm_B2 + STABILIZATION_EPS)\n\n        # Feature 2: Non-Orthogonality\n        inner_prod = compute_inner_product(B1_comp, B2_comp, hx, hy)\n        R_nonorth = abs(inner_prod) / ((norm_B1 * norm_B2) + STABILIZATION_EPS)\n        \n        features1 = np.array([R_div1, R_nonorth, Q_aspect])\n        features2 = np.array([R_div2, R_nonorth, Q_aspect])\n\n        return features1, features2\n\n    # --- Logistic Regression Classifier (from scratch) ---\n    class LogisticRegressor:\n        def __init__(self, lr, n_iter, lambda_):\n            self.lr = lr\n            self.n_iter = n_iter\n            self.lambda_ = lambda_\n            self.weights = None\n            self.bias = None\n\n        def _sigmoid(self, z):\n            # Clip to avoid overflow in exp\n            z = np.clip(z, -500, 500)\n            return 1 / (1 + np.exp(-z))\n\n        def fit(self, X, y):\n            n_samples, n_features = X.shape\n            self.weights = np.zeros(n_features)\n            self.bias = 0\n\n            # Batch Gradient Descent\n            for _ in range(self.n_iter):\n                linear_model = np.dot(X, self.weights) + self.bias\n                y_predicted = self._sigmoid(linear_model)\n\n                # Compute gradients\n                dw = (1 / n_samples) * (np.dot(X.T, (y_predicted - y)) + self.lambda_ * self.weights)\n                db = (1 / n_samples) * np.sum(y_predicted - y)\n\n                # Update weights and bias\n                self.weights -= self.lr * dw\n                self.bias -= self.lr * db\n\n        def predict_proba(self, X):\n            linear_model = np.dot(X, self.weights) + self.bias\n            return self._sigmoid(linear_model)\n\n        def predict(self, X, threshold=0.5):\n            probas = self.predict_proba(X)\n            return probas >= threshold\n\n    # --- 1. Generate Training Data ---\n    num_pairs = TRAINING_SAMPLES // 2\n    X_train_list = []\n    y_train_list = []\n\n    for i in range(num_pairs):\n        # Randomize mesh\n        nx, ny = GRID_SIZE_TRAIN, GRID_SIZE_TRAIN\n        Lx = np.random.uniform(1.0, 5.0)\n        Ly = np.random.uniform(1.0, 5.0)\n        X, Y, hx, hy = make_grid(nx, ny, Lx, Ly)\n\n        # Generate a pair of modes, each randomly physical or spurious\n        modes = []\n        labels = []\n        for _ in range(2):\n            if np.random.rand() > 0.5: # Physical\n                k = np.random.uniform(2.0, 10.0)\n                modes.append(generate_physical_mode(k, X, Y))\n                labels.append(0) # 0 for physical\n            else: # Spurious\n                k = np.random.uniform(2.0, 10.0)\n                modes.append(generate_spurious_mode(lambda x,y: np.sin(k*x)*np.cos(k*y), X, Y))\n                labels.append(1) # 1 for spurious\n\n        # Compute features for the pair\n        features1, features2 = compute_features(modes[0], modes[1], hx, hy)\n        \n        X_train_list.append(features1)\n        y_train_list.append(labels[0])\n        X_train_list.append(features2)\n        y_train_list.append(labels[1])\n        \n    X_train = np.array(X_train_list)\n    y_train = np.array(y_train_list)\n\n    # --- 2. Standardize Features and Train Classifier ---\n    mean = X_train.mean(axis=0)\n    std = X_train.std(axis=0)\n    std[std == 0] = 1 # Avoid division by zero if a feature is constant\n    X_train_std = (X_train - mean) / std\n\n    classifier = LogisticRegressor(lr=LEARNING_RATE, n_iter=ITERATIONS, lambda_=REG_PARAM)\n    classifier.fit(X_train_std, y_train)\n\n    # --- 3. Process Test Suite ---\n    test_cases_params = [\n        {'Lx': 1.0, 'Ly': 1.0, 'mode1_type': 'physical', 'mode1_k': 3, 'mode2_type': 'physical', 'mode2_k': 5},\n        {'Lx': 1.0, 'Ly': 1.0, 'mode1_type': 'spurious', 'mode1_phi': lambda x,y: np.sin(10*x)*np.sin(10*y), 'mode2_type': 'physical', 'mode2_k': 4},\n        {'Lx': 1.0, 'Ly': 5.0, 'mode1_type': 'physical', 'mode1_k': 4, 'mode2_type': 'physical', 'mode2_k': 6},\n        {'Lx': 1.0, 'Ly': 1.0, 'mode1_type': 'physical', 'mode1_k': 8, 'mode2_type': 'duplicate', 'mode2_k': 8}\n    ]\n    nx_test, ny_test = 64, 64\n    results = []\n\n    for case in test_cases_params:\n        X, Y, hx, hy = make_grid(nx_test, ny_test, case['Lx'], case['Ly'])\n        \n        # Generate modes for the test case\n        B1, B2 = None, None\n        \n        if case['mode1_type'] == 'physical':\n            B1 = generate_physical_mode(case['mode1_k'], X, Y)\n        elif case['mode1_type'] == 'spurious':\n            B1 = generate_spurious_mode(case['mode1_phi'], X, Y)\n\n        if case['mode2_type'] == 'physical':\n            B2 = generate_physical_mode(case['mode2_k'], X, Y)\n        elif case['mode2_type'] == 'duplicate':\n            B_base = generate_physical_mode(case['mode2_k'], X, Y)\n            noise_mag = 1e-3\n            noise_x = noise_mag * (2 * np.random.rand(nx_test, ny_test) - 1)\n            noise_y = noise_mag * (2 * np.random.rand(nx_test, ny_test) - 1)\n            B2 = (B_base[0] + noise_x, B_base[1] + noise_y)\n        \n        # Compute features\n        features1, features2 = compute_features(B1, B2, hx, hy)\n        \n        # Standardize features using training set stats\n        test_features = np.array([features1, features2])\n        test_features_std = (test_features - mean) / std\n        \n        # Get predictions\n        predictions = classifier.predict(test_features_std)\n        label1, label2 = bool(predictions[0]), bool(predictions[1])\n        \n        # Apply repair suggestion logic\n        Q_aspect = features1[2] # same for both\n        R_div1, R_div2 = features1[0], features2[0]\n        R_nonorth = features1[1]\n        \n        suggestion_code = 0 # Default: no repair\n        if Q_aspect > 3.0:\n            suggestion_code = 1\n        elif (label1 and R_div1 > 0.15) or (label2 and R_div2 > 0.15):\n            suggestion_code = 2\n        elif R_nonorth > 0.95:\n            suggestion_code = 3\n\n        results.append([label1, label2, suggestion_code])\n\n    # --- 4. Print Final Output ---\n    print(results)\n\n\nsolve()\n```"
        }
    ]
}