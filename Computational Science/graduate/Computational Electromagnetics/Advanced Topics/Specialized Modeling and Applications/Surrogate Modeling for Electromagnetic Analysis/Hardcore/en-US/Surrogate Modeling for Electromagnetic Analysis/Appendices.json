{
    "hands_on_practices": [
        {
            "introduction": "Real-world measurements are inevitably corrupted by noise, which can cause the data to violate fundamental physical laws. This practice addresses the crucial task of refining measured scattering-parameter data to ensure it complies with the principles of reciprocity and passivity. You will implement a convex optimization algorithm to find the minimal correction that enforces these physical constraints, a powerful technique for creating physically-consistent models from experimental or high-fidelity simulation data .",
            "id": "3352830",
            "problem": "Consider a linear and time-invariant two-port network characterized in the scattering representation. Let the incident and reflected power-wave vectors be denoted by $\\mathbf{a} \\in \\mathbb{C}^{2}$ and $\\mathbf{b} \\in \\mathbb{C}^{2}$, respectively, and let the scattering matrix $S(\\omega) \\in \\mathbb{C}^{2 \\times 2}$ satisfy $\\mathbf{b} = S(\\omega)\\,\\mathbf{a}$ at angular frequency $\\omega$. Assume equal real port reference impedances. The following foundational principles are to be used as the base of your reasoning: (i) conservation of energy and the Poynting theorem for passive media imply that the total outgoing power cannot exceed the incoming power, which in the power-wave normalization implies $\\|\\mathbf{b}\\|_{2} \\le \\|\\mathbf{a}\\|_{2}$ for every excitation $\\mathbf{a}$; (ii) the Lorentz reciprocity theorem implies that for reciprocal media under the stated conditions, the scattering matrix satisfies $S(\\omega) = S(\\omega)^{\\mathsf{T}}$. From these, it follows that for a passive network the largest singular value of $S(\\omega)$ does not exceed $1$, and for a reciprocal network $S(\\omega)$ is complex symmetric. Suppose that, due to small measurement noise, sampled values of $S(\\omega)$ at three discrete frequencies are perturbed away from exact reciprocity and, in some cases, away from passivity. Your tasks are to: (1) test reciprocity and passivity numerically at each dataset level, and (2) construct a corrected surrogate $\\tilde{S}(\\omega)$ at each sampled frequency that minimally perturbs the measured data in the Frobenius norm while satisfying both constraints $\\tilde{S}(\\omega) = \\tilde{S}(\\omega)^{\\mathsf{T}}$ and $\\|\\tilde{S}(\\omega)\\|_{2} \\le 1$. Concretely, for each dataset, solve the convex feasibility-correction problem at each sampled frequency:\n$$\n\\min_{\\Delta \\in \\mathbb{C}^{2 \\times 2}} \\ \\|\\Delta\\|_{\\mathrm{F}} \\quad \\text{subject to} \\quad S_{\\mathrm{meas}}(\\omega) + \\Delta = \\tilde{S}(\\omega), \\ \\ \\tilde{S}(\\omega) = \\tilde{S}(\\omega)^{\\mathsf{T}}, \\ \\ \\|\\tilde{S}(\\omega)\\|_{2} \\le 1,\n$$\nand take the corrected surrogate as $\\tilde{S}(\\omega)$. For each dataset, report four booleans indicating whether all three measured samples satisfy reciprocity and passivity, and whether all three corrected surrogates satisfy reciprocity and passivity, together with the total perturbation magnitude defined as the sum (over the three frequencies) of the Frobenius norms $\\|S_{\\mathrm{meas}}(\\omega) - \\tilde{S}(\\omega)\\|_{\\mathrm{F}}$. There are no physical units for scattering parameters; treat all quantities as dimensionless complex numbers. Angles, when conceptually referenced, are in radians. All numerical answers are required as plain booleans and real numbers.\n\nImplement an algorithm that is universally applicable in any modern programming language to:\n- compute singular values and the spectral norm,\n- check reciprocity by testing $S(\\omega) = S(\\omega)^{\\mathsf{T}}$ within a numerical tolerance,\n- compute a minimal Frobenius-norm correction that enforces both reciprocity and passivity at each frequency sample.\n\nDesign your program to process the following test suite (three datasets, each with three frequency samples). Each $2 \\times 2$ complex matrix is given in rectangular form as $a + bj$ with $j = \\sqrt{-1}$:\n\nDataset A (three frequencies):\n- $\\omega_{1}$: $\\begin{bmatrix} 0.199001 + 0.0199666\\,j & 0.099875 - 0.0049979\\,j \\\\ 0.097955 + 0.0029395\\,j & 0.299265 + 0.0209828\\,j \\end{bmatrix}$\n- $\\omega_{2}$: $\\begin{bmatrix} 0.179964 - 0.0035998\\,j & 0.119904 + 0.0047987\\,j \\\\ 0.120924 + 0.0042359\\,j & 0.279986 + 0.0027999\\,j \\end{bmatrix}$\n- $\\omega_{3}$: $\\begin{bmatrix} 0.22 + 0\\,j & 0.089982 - 0.0017999\\,j \\\\ 0.0909908 - 0.0013649\\,j & 0.309613 + 0.0154935\\,j \\end{bmatrix}$\n\nDataset B (three frequencies):\n- $\\omega_{1}$: $\\begin{bmatrix} 0.532 + 0.005\\,j & 0.456 - 0.004\\,j \\\\ 0.470 + 0.003\\,j & 0.532 - 0.006\\,j \\end{bmatrix}$\n- $\\omega_{2}$: $\\begin{bmatrix} 0.540 + 0.002\\,j & 0.450 + 0.006\\,j \\\\ 0.455 - 0.005\\,j & 0.525 - 0.001\\,j \\end{bmatrix}$\n- $\\omega_{3}$: $\\begin{bmatrix} 0.520 - 0.001\\,j & 0.462 + 0\\,j \\\\ 0.458 + 0.010\\,j & 0.538 + 0.003\\,j \\end{bmatrix}$\n\nDataset C (three frequencies):\n- $\\omega_{1}$: $\\begin{bmatrix} 0.25 + 0.01\\,j & 0.12 - 0.02\\,j \\\\ 0.17 + 0.01\\,j & 0.18 - 0.005\\,j \\end{bmatrix}$\n- $\\omega_{2}$: $\\begin{bmatrix} 0.22 - 0.015\\,j & 0.11 + 0\\,j \\\\ 0.16 - 0.02\\,j & 0.20 + 0.008\\,j \\end{bmatrix}$\n- $\\omega_{3}$: $\\begin{bmatrix} 0.28 + 0\\,j & 0.10 + 0.015\\,j \\\\ 0.14 - 0.005\\,j & 0.19 + 0.012\\,j \\end{bmatrix}$\n\nNumerical specifications:\n- Use reciprocity tolerance $\\varepsilon_{\\mathrm{rec}} = 10^{-9}$ applied as $\\max_{i,j} |S_{ij} - S_{ji}| \\le \\varepsilon_{\\mathrm{rec}}$.\n- Use passivity tolerance $\\varepsilon_{\\mathrm{pas}} = 10^{-9}$ applied as $\\sigma_{\\max}(S) \\le 1 + \\varepsilon_{\\mathrm{pas}}$, where $\\sigma_{\\max}$ denotes the largest singular value.\n- For the correction step, minimize the Frobenius norm per frequency sample subject to both constraints. A principled method is to compute the orthogonal projection onto the intersection of the convex set of complex symmetric matrices and the convex spectral-norm unit ball. Your implementation must ensure the returned $\\tilde{S}(\\omega)$ satisfies both constraints within the given tolerances.\n- Report the total perturbation per dataset as the sum of the three Frobenius norms $\\sum_{k=1}^{3} \\|S_{\\mathrm{meas}}(\\omega_k) - \\tilde{S}(\\omega_k)\\|_{\\mathrm{F}}$, rounded to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of three records, one per dataset, each record being a list of five elements in the order: $[\\text{reciprocal\\_before}, \\text{passive\\_before}, \\text{reciprocal\\_after}, \\text{passive\\_after}, \\text{total\\_perturbation}]$. The booleans must be literal $True$ or $False$, and the total perturbation a real number rounded to six decimal places. The entire output must be a single line with no extra text, for example: $[[\\ldots],[\\ldots],[\\ldots]]$.",
            "solution": "The problem is well-posed and scientifically grounded in the principles of electromagnetic network theory. It will be solved by adhering to the provided constraints and definitions.\n\nThe core task is to find a corrected scattering matrix $\\tilde{S}(\\omega)$ that is \"closest\" to a measured matrix $S_{\\mathrm{meas}}(\\omega)$ in the Frobenius norm sense, while satisfying the physical constraints of reciprocity and passivity. This is a classic convex optimization problem known as projection onto a convex set.\n\nThe constraints define two convex sets in the space of $2 \\times 2$ complex matrices, $\\mathbb{C}^{2 \\times 2}$:\n1.  The set of reciprocal matrices, which for a two-port network under the given conditions are complex symmetric matrices. This is a linear subspace and hence a convex set, defined as $C_{\\mathrm{rec}} = \\{ X \\in \\mathbb{C}^{2 \\times 2} \\mid X = X^{\\mathsf{T}} \\}$.\n2.  The set of passive matrices, defined by the condition that the spectral norm (largest singular value) does not exceed $1$. This is the closed unit ball with respect to the spectral norm, a convex set defined as $C_{\\mathrm{pas}} = \\{ X \\in \\mathbb{C}^{2 \\times 2} \\mid \\|X\\|_{2} \\le 1 \\}$.\n\nThe problem is to find the orthogonal projection of the measured matrix $S_{\\mathrm{meas}}$ onto the intersection of these two sets, $C_{\\mathrm{rec}} \\cap C_{\\mathrm{pas}}$. The optimization problem is formally stated as:\n$$\n\\tilde{S} = \\underset{X}{\\operatorname{argmin}} \\|S_{\\mathrm{meas}} - X\\|_{\\mathrm{F}} \\quad \\text{subject to} \\quad X \\in C_{\\mathrm{rec}} \\cap C_{\\mathrm{pas}}.\n$$\nHere, $\\| \\cdot \\|_{\\mathrm{F}}$ denotes the Frobenius norm.\n\nWhile there is no closed-form expression for the projection onto an intersection of arbitrary convex sets, this specific problem can be solved by an iterative procedure known as Dykstra's algorithm. This algorithm finds the projection of a point onto the intersection of two convex sets by alternating projections onto the individual sets while maintaining correction terms.\n\nLet $P_{C_{\\mathrm{rec}}}(A)$ be the projection of a matrix $A$ onto the set of symmetric matrices, and $P_{C_{\\mathrm{pas}}}(A)$ be the projection onto the set of passive matrices.\n- The projection onto the subspace of complex symmetric matrices is given by:\n  $$\n  P_{C_{\\mathrm{rec}}}(A) = \\frac{A + A^{\\mathsf{T}}}{2}\n  $$\n- The projection onto the spectral norm unit ball is found via the Singular Value Decomposition (SVD). If $A = U \\Sigma V^{\\mathsf{H}}$ is the SVD of $A$, where $\\Sigma = \\operatorname{diag}(\\sigma_1, \\sigma_2, \\dots)$, then the projection is:\n  $$\n  P_{C_{\\mathrm{pas}}}(A) = U \\hat{\\Sigma} V^{\\mathsf{H}}, \\quad \\text{where} \\quad \\hat{\\Sigma} = \\operatorname{diag}(\\min(\\sigma_1, 1), \\min(\\sigma_2, 1), \\dots)\n  $$\n  This operation effectively \"caps\" the singular values at $1$.\n\nDykstra's algorithm, applied to this problem, is as follows:\nInitialize $X_0 = S_{\\mathrm{meas}}$, and correction matrices $Q_0 = \\mathbf{0}$, $R_0 = \\mathbf{0}$.\nFor $k = 1, 2, \\dots$ until convergence:\n1.  Project onto $C_{\\mathrm{rec}}$: $Y_k = P_{C_{\\mathrm{rec}}}(X_{k-1} + Q_{k-1})$\n2.  Update the first correction term: $Q_k = (X_{k-1} + Q_{k-1}) - Y_k$\n3.  Project onto $C_{\\mathrm{pas}}$: $X_k = P_{C_{\\mathrm{pas}}}(Y_k + R_{k-1})$\n4.  Update the second correction term: $R_k = (Y_k + R_{k-1}) - X_k$\n\nThe sequence $X_k$ converges to the desired corrected matrix $\\tilde{S}$.\n\nFor each dataset, the following procedure is executed:\n1.  The properties of the three measured matrices $S_{\\mathrm{meas}}(\\omega_k)$ are tested.\n    -   **Reciprocity:** A matrix $S$ is considered reciprocal if $\\max_{i,j} |S_{ij} - S_{ji}| \\le \\varepsilon_{\\mathrm{rec}} = 10^{-9}$. For the $2 \\times 2$ case, this simplifies to $|S_{12} - S_{21}| \\le 10^{-9}$.\n    -   **Passivity:** A matrix $S$ is considered passive if its largest singular value $\\sigma_{\\max}(S) \\le 1 + \\varepsilon_{\\mathrm{pas}} = 1 + 10^{-9}$.\n2.  For each $S_{\\mathrm{meas}}(\\omega_k)$, the corrected matrix $\\tilde{S}(\\omega_k)$ is computed using Dykstra's algorithm described above.\n3.  The resulting corrected matrices $\\tilde{S}(\\omega_k)$ are verified to satisfy both reciprocity and passivity constraints within the specified tolerances. By construction, the algorithm guarantees this.\n4.  The perturbation magnitude for each frequency is calculated as $\\|S_{\\mathrm{meas}}(\\omega_k) - \\tilde{S}(\\omega_k)\\|_{\\mathrm{F}}$. The total perturbation for the dataset is the sum of these three norms.\n5.  The final results for each dataset—four boolean flags and the total perturbation—are compiled.\n\nAn important observation for the provided datasets is that for every measured matrix $S_{\\mathrm{meas}}$, the projection onto the symmetric subspace, $P_{C_{\\mathrm{rec}}}(S_{\\mathrm{meas}})$, already satisfies the passivity constraint, i.e., $P_{C_{\\mathrm{rec}}}(S_{\\mathrm{meas}}) \\in C_{\\mathrm{pas}}$. In this special case, Dykstra's algorithm converges in a single iteration to $\\tilde{S} = P_{C_{\\mathrm{rec}}}(S_{\\mathrm{meas}})$. While our implementation uses the general iterative algorithm for correctness and robustness, this feature of the data simplifies the correction process significantly. The perturbation is simply the norm of the skew-symmetric part of the measured matrix, $\\|S_{\\mathrm{meas}} - \\tilde{S}\\|_{\\mathrm{F}} = \\|\\frac{1}{2}(S_{\\mathrm{meas}} - S_{\\mathrm{meas}}^{\\mathsf{T}})\\|_{\\mathrm{F}}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Validates and corrects scattering matrix data for reciprocity and passivity.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = {\n        \"A\": [\n            np.array([[0.199001 + 0.0199666j, 0.099875 - 0.0049979j],\n                      [0.097955 + 0.0029395j, 0.299265 + 0.0209828j]]),\n            np.array([[0.179964 - 0.0035998j, 0.119904 + 0.0047987j],\n                      [0.120924 + 0.0042359j, 0.279986 + 0.0027999j]]),\n            np.array([[0.22 + 0.j, 0.089982 - 0.0017999j],\n                      [0.0909908 - 0.0013649j, 0.309613 + 0.0154935j]])\n        ],\n        \"B\": [\n            np.array([[0.532 + 0.005j, 0.456 - 0.004j],\n                      [0.470 + 0.003j, 0.532 - 0.006j]]),\n            np.array([[0.540 + 0.002j, 0.450 + 0.006j],\n                      [0.455 - 0.005j, 0.525 - 0.001j]]),\n            np.array([[0.520 - 0.001j, 0.462 + 0.j],\n                      [0.458 + 0.010j, 0.538 + 0.003j]])\n        ],\n        \"C\": [\n            np.array([[0.25 + 0.01j, 0.12 - 0.02j],\n                      [0.17 + 0.01j, 0.18 - 0.005j]]),\n            np.array([[0.22 - 0.015j, 0.11 + 0.j],\n                      [0.16 - 0.02j, 0.20 + 0.008j]]),\n            np.array([[0.28 + 0.j, 0.10 + 0.015j],\n                      [0.14 - 0.005j, 0.19 + 0.012j]])\n        ]\n    }\n\n    # Numerical specifications\n    reciprocity_tol = 1e-9\n    passivity_tol = 1e-9\n\n    def project_symmetric(A):\n        \"\"\"Projects a matrix onto the set of complex symmetric matrices.\"\"\"\n        return (A + A.T) / 2\n\n    def project_passive(A):\n        \"\"\"Projects a matrix onto the spectral norm unit ball.\"\"\"\n        U, s, Vh = np.linalg.svd(A, full_matrices=False)\n        s_capped = np.minimum(s, 1.0)\n        return U @ np.diag(s_capped) @ Vh\n\n    def dykstra_projection(S, max_iter=100):\n        \"\"\"\n        Computes the projection of a matrix S onto the intersection of\n        symmetric matrices and the passive (spectral norm <= 1) matrices\n        using Dykstra's algorithm.\n        \"\"\"\n        X = S.copy()\n        Q = np.zeros_like(S, dtype=np.complex128)\n        R = np.zeros_like(S, dtype=np.complex128)\n\n        for _ in range(max_iter):\n            # Project onto C1 (symmetric matrices)\n            Y_temp = X + Q\n            Y = project_symmetric(Y_temp)\n            Q = Y_temp - Y\n            \n            # Project onto C2 (passive matrices)\n            X_temp = Y + R\n            X = project_passive(X_temp)\n            R = X_temp - X\n        \n        return X\n\n    overall_results = []\n    \n    for dataset_key in sorted(test_cases.keys()):\n        dataset = test_cases[dataset_key]\n        \n        reciprocal_before = True\n        passive_before = True\n        reciprocal_after = True\n        passive_after = True\n        total_perturbation = 0.0\n\n        for S_meas in dataset:\n            # Check reciprocity of measured matrix\n            if np.abs(S_meas[0, 1] - S_meas[1, 0]) > reciprocity_tol:\n                reciprocal_before = False\n            \n            # Check passivity of measured matrix\n            s_vals_before = np.linalg.svd(S_meas, compute_uv=False)\n            if np.max(s_vals_before) > 1.0 + passivity_tol:\n                passive_before = False\n                \n            # Compute the corrected surrogate matrix\n            S_tilde = dykstra_projection(S_meas)\n\n            # Check reciprocity of corrected matrix\n            if np.abs(S_tilde[0, 1] - S_tilde[1, 0]) > reciprocity_tol:\n                reciprocal_after = False\n\n            # Check passivity of corrected matrix\n            s_vals_after = np.linalg.svd(S_tilde, compute_uv=False)\n            if np.max(s_vals_after) > 1.0 + passivity_tol:\n                passive_after = False\n            \n            # Calculate and accumulate perturbation\n            perturbation = np.linalg.norm(S_meas - S_tilde, 'fro')\n            total_perturbation += perturbation\n\n        overall_results.append([\n            reciprocal_before,\n            passive_before,\n            reciprocal_after,\n            passive_after,\n            total_perturbation\n        ])\n\n    # Final print statement in the exact required format.\n    results_str_parts = []\n    for res in overall_results:\n        # Format: [bool,bool,bool,bool,float] with float rounded to 6 decimal places.\n        part = (f\"[{str(res[0])},{str(res[1])},{str(res[2])},{str(res[3])},\"\n                f\"{round(res[4], 6):.6f}]\")\n        results_str_parts.append(part)\n    final_output = f\"[{','.join(results_str_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "This comprehensive exercise guides you through the construction of a full parametric surrogate model for the resonant modes of an electromagnetic cavity. You will discretize the governing Maxwell's equations, generate training data, and implement mode tracking to handle parametric evolution and potential mode crossings. The core of the practice involves using Proper Orthogonal Decomposition (POD) to create a reduced-order basis for the modal fields and enforcing physical constraints like orthogonality, encapsulating a complete workflow in physics-informed model order reduction .",
            "id": "3352845",
            "problem": "You are given a parametric one-dimensional electromagnetic cavity model suitable for constructing a surrogate of eigenfrequencies and modal fields with an enforced weighted orthogonality. The cavity is a segment of length $L$ with perfectly conducting end walls at $x = 0$ and $x = L$. Consider source-free, time-harmonic fields in the frequency domain with angular frequency $\\omega$, and let the permittivity be spatially varying but piecewise constant. The permeability is constant. The governing fundamental equation for a transverse electrical scalar field $E(x)$ is derived from Maxwell's equations in source-free media: for constant permeability $\\mu$ and spatially varying permittivity $\\epsilon(x)$, the field satisfies\n$$\n-\\frac{\\mathrm{d}^2}{\\mathrm{d}x^2}E(x) = \\omega^2 \\epsilon(x) E(x),\n$$\nwith boundary conditions $E(0) = 0$ and $E(L) = 0$. This is a Sturm–Liouville eigenvalue problem with a positive weight $\\epsilon(x)$.\n\nDiscretize the interval $[0,L]$ with $N$ interior points $\\{x_i\\}_{i=1}^N$, uniform spacing $h = \\frac{L}{N+1}$, and Dirichlet boundary conditions. Using second-order centered finite differences, form the stiffness matrix $K \\in \\mathbb{R}^{N \\times N}$ and the mass matrix $M(\\mathbf{p}) \\in \\mathbb{R}^{N \\times N}$ as follows:\n- $K$ is tridiagonal with entries $K_{ii} = \\frac{2}{h^2}$, $K_{i,i-1} = K_{i,i+1} = -\\frac{1}{h^2}$, for $i$ in the appropriate ranges.\n- $M(\\mathbf{p})$ is diagonal with entries $M_{ii}(\\mathbf{p}) = \\epsilon(x_i;\\mathbf{p})\\, h$, where $\\epsilon(x;\\mathbf{p})$ is the permittivity profile parameterized by $\\mathbf{p}$.\n\nThe generalized symmetric eigenproblem is\n$$\nK \\mathbf{u} = \\lambda(\\mathbf{p}) M(\\mathbf{p}) \\mathbf{u}, \\quad \\text{with } \\lambda(\\mathbf{p}) = \\omega^2(\\mathbf{p}),\n$$\nand the eigenmodes can be normalized using the weighted inner product\n$$\n\\langle \\mathbf{u}_m, \\mathbf{u}_n \\rangle_\\epsilon = \\mathbf{u}_m^\\top M(\\mathbf{p}) \\mathbf{u}_n = \\delta_{mn}.\n$$\n\nDefine a single-parameter permittivity model with vector parameter $\\mathbf{p} = [p]$ as follows. Let $L$ be dimensionless and equal to $L=1$, and consider two permittivity constants $\\epsilon_1$ and $\\epsilon_2$. The permittivity is piecewise constant:\n$$\n\\epsilon(x;p) = \\begin{cases}\n\\epsilon_1, & 0 \\le x \\le p L, \\\\\n\\epsilon_2, & p L < x \\le L,\n\\end{cases}\n$$\nwith $p \\in (0,1)$. Let $\\mu = 1$. All quantities are dimensionless.\n\nYour tasks:\n1. Starting from the fundamental governing equation and boundary conditions, explain why the discretization leads to a generalized symmetric eigenproblem with a positive definite mass matrix and why the resulting eigenvectors can be made orthonormal with respect to the weighted inner product $\\langle \\cdot, \\cdot \\rangle_\\epsilon$.\n2. Implement a parametric sampling and mode-tracking methodology across a training set of parameter values $\\{p_j\\}$ to ensure consistent eigenmode indices. Specifically, at each successive parameter, reorder the modes to maximize the absolute weighted overlaps with the previous step using the inner product $\\langle \\cdot, \\cdot \\rangle_\\epsilon$. Formulate this as an assignment problem and justify the approach.\n3. Construct a surrogate model for the lowest $k$ eigenfrequencies $\\omega_n(p)$ by fitting a low-degree polynomial, and for the corresponding modal fields $\\mathbf{E}_n(p)$ by performing Proper Orthogonal Decomposition (POD) for each mode across the training set, then fitting polynomial surrogates for the POD coefficients. Enforce the orthogonality constraint $\\langle \\mathbf{E}_m,\\mathbf{E}_n \\rangle_\\epsilon = \\delta_{mn}$ on the predicted fields at a new parameter $p^\\star$ via a weighted Gram–Schmidt process.\n4. Evaluate the surrogate performance and mode tracking at specific test parameters by computing:\n   - The mean relative error of predicted eigenfrequencies compared to the true ones at $p^\\star$, defined by\n     $$\n     e_{\\mathrm{freq}}(p^\\star) = \\frac{1}{k} \\sum_{n=1}^k \\frac{|\\omega_n^{\\mathrm{pred}}(p^\\star) - \\omega_n^{\\mathrm{true}}(p^\\star)|}{\\omega_n^{\\mathrm{true}}(p^\\star)}.\n     $$\n   - The maximum absolute deviation from orthonormality of the predicted fields, defined by\n     $$\n     e_{\\mathrm{orth}}(p^\\star) = \\max_{m,n} \\left| \\mathbf{E}_m^{\\mathrm{pred}}(p^\\star)^\\top M(p^\\star) \\mathbf{E}_n^{\\mathrm{pred}}(p^\\star) - \\delta_{mn} \\right|.\n     $$\n   - A boolean mode-tracking correctness indicator at $p^\\star$, which is true if and only if each predicted mode $\\mathbf{E}_n^{\\mathrm{pred}}(p^\\star)$ has the largest absolute weighted overlap with the true mode $\\mathbf{E}_n^{\\mathrm{true}}(p^\\star)$ of the same index, that is,\n     $$\n     t(p^\\star) = \\bigwedge_{n=1}^k \\left( \\underset{m \\in \\{1,\\dots,k\\}}{\\arg\\max} \\left| \\mathbf{E}_n^{\\mathrm{true}}(p^\\star)^\\top M(p^\\star) \\mathbf{E}_m^{\\mathrm{pred}}(p^\\star) \\right| = n \\right).\n     $$\n\nNumerical specifications:\n- Use $N = 120$, $L = 1$, $k = 3$, $\\epsilon_1 = 1$, $\\epsilon_2 = 4$.\n- Use a training set of $p$ values uniformly spaced in $[0.2, 0.8]$ inclusive, with at least $9$ points. The surrogate polynomial degree should be $3$ for both frequencies and POD coefficients, and the POD basis rank per mode should capture at least $99.9\\%$ of the energy while being capped at $5$.\n- All quantities and computations are dimensionless, and angles are not used.\n\nTest suite:\n- Evaluate the metrics at the following test parameters: $p^\\star \\in \\{0.25, 0.50, 0.90\\}$, which cover a general case, a symmetric split, and an extrapolation beyond the training interval.\n- For each test parameter, compute and return the three metrics $e_{\\mathrm{freq}}(p^\\star)$, $e_{\\mathrm{orth}}(p^\\star)$, and $t(p^\\star)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should be ordered by test parameter and flattened so that it contains, in sequence, the three values for each test parameter. Concretely, the output must be\n$$\n[e_{\\mathrm{freq}}(0.25), e_{\\mathrm{orth}}(0.25), t(0.25), e_{\\mathrm{freq}}(0.50), e_{\\mathrm{orth}}(0.50), t(0.50), e_{\\mathrm{freq}}(0.90), e_{\\mathrm{orth}}(0.90), t(0.90)].\n$$\nEach $e_{\\mathrm{freq}}$ and $e_{\\mathrm{orth}}$ must be printed as floating-point numbers, and each $t$ must be printed as a boolean.",
            "solution": "The problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, objective, and complete. It describes a standard problem in computational electromagnetics and surrogate modeling with clear, verifiable requirements.\n\n### 1. Discretization and Eigenproblem Formulation\n\nThe analysis begins with the one-dimensional Helmholtz equation for the transverse electric field $E(x)$ in a source-free medium with spatially varying permittivity $\\epsilon(x)$ and constant permeability $\\mu$.\n$$\n-\\frac{\\mathrm{d}^2}{\\mathrm{d}x^2}E(x) = \\omega^2 \\mu \\epsilon(x) E(x)\n$$\nGiven the boundary conditions $E(0) = 0$ and $E(L) = 0$, this constitutes a Sturm–Liouville eigenvalue problem. We are given $\\mu=1$ and define the eigenvalue as $\\lambda = \\omega^2$.\n$$\n-\\frac{\\mathrm{d}^2}{\\mathrm{d}x^2}E(x) = \\lambda \\epsilon(x) E(x)\n$$\nTo solve this numerically, the domain $[0,L]$ is discretized into $N+2$ points, $\\{x_i\\}_{i=0}^{N+1}$, where $x_i = i h$ and $h = L/(N+1)$. The points $x_1, \\dots, x_N$ are interior points, while $x_0=0$ and $x_{N+1}=L$ are boundary points. The field $E(x)$ is represented by a vector $\\mathbf{u} \\in \\mathbb{R}^N$, where $u_i = E(x_i)$ for $i=1, \\dots, N$. The boundary conditions imply $u_0 = E(x_0) = 0$ and $u_{N+1} = E(x_{N+1}) = 0$.\n\nThe second derivative operator is approximated using a second-order centered finite difference scheme at each interior point $x_i$:\n$$\n\\left.\\frac{\\mathrm{d}^2 E}{\\mathrm{d}x^2}\\right|_{x_i} \\approx \\frac{E(x_{i+1}) - 2E(x_i) + E(x_{i-1})}{h^2} = \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2}\n$$\nSubstituting this into the governing equation for each interior node $i=1, \\dots, N$:\n$$\n-\\frac{1}{h^2}(u_{i-1} - 2u_i + u_{i+1}) = \\lambda \\epsilon(x_i) u_i\n$$\nThe full set of $N$ equations can be assembled into a single matrix equation. The left-hand side forms the matrix-vector product $K\\mathbf{u}$, where $K$ is the stiffness matrix. For $i=1$, the equation involves $u_0$, which is zero, so the term $-u_0/h^2$ vanishes. For $i=N$, it involves $u_{N+1}$, which is also zero. This results in the specified symmetric tridiagonal matrix $K \\in \\mathbb{R}^{N \\times N}$:\n$$\nK = \\frac{1}{h^2}\n\\begin{pmatrix}\n2 & -1 & & & \\\\\n-1 & 2 & -1 & & \\\\\n& \\ddots & \\ddots & \\ddots & \\\\\n& & -1 & 2 & -1 \\\\\n& & & -1 & 2\n\\end{pmatrix}\n$$\nThe right-hand side can be written as $\\lambda M \\mathbf{u}$. The discretization of $\\epsilon(x) u_i$ over the volume element corresponding to point $x_i$ (which is of length $h$) gives the term $\\lambda (\\epsilon(x_i) h) u_i$. This leads to a diagonal mass matrix $M \\in \\mathbb{R}^{N \\times N}$ with entries $M_{ii} = \\epsilon(x_i)h$.\nThis yields the generalized symmetric eigenproblem:\n$$\nK \\mathbf{u} = \\lambda M(\\mathbf{p}) \\mathbf{u}\n$$\nThe matrix $K$ is symmetric by construction. The mass matrix $M(\\mathbf{p})$ is diagonal, and therefore also symmetric.\n\nA matrix is positive definite if all its eigenvalues are strictly positive.\nFor the stiffness matrix $K$, its eigenvalues are known analytically to be $\\mu_j = \\frac{2}{h^2}(1 - \\cos(\\frac{j\\pi}{N+1}))$ for $j=1,\\dots,N$. Since $j \\le N$, $\\frac{j\\pi}{N+1} \\in (0, \\pi)$, so $\\cos(\\frac{j\\pi}{N+1}) < 1$, which implies $\\mu_j > 0$ for all $j$. Thus, $K$ is positive definite.\nFor the mass matrix $M(\\mathbf{p})$, its diagonal entries are $M_{ii}(\\mathbf{p}) = \\epsilon(x_i; \\mathbf{p})h$. Since physical permittivity $\\epsilon(x)$ is always positive, and the spacing $h$ is positive, all diagonal entries of $M$ are positive. A diagonal matrix with all positive diagonal entries is positive definite.\n\nSince $K$ and $M$ are symmetric and $M$ is positive definite, the problem is a generalized symmetric definite eigenproblem. A key property of such problems is that eigenvectors $\\mathbf{u}_m, \\mathbf{u}_n$ corresponding to distinct eigenvalues $\\lambda_m \\neq \\lambda_n$ are orthogonal with respect to both matrices, i.e., $\\mathbf{u}_m^\\top K \\mathbf{u}_n = 0$ and $\\mathbf{u}_m^\\top M \\mathbf{u}_n = 0$. For any eigenvector $\\mathbf{u}_n$, the quadratic form $\\mathbf{u}_n^\\top M \\mathbf{u}_n$ is positive because $M$ is positive definite. This allows for normalization. We can scale each eigenvector such that its $M$-norm is unity:\n$$\n\\mathbf{u}_n \\leftarrow \\frac{\\mathbf{u}_n}{\\sqrt{\\mathbf{u}_n^\\top M \\mathbf{u}_n}}\n$$\nAfter this normalization, the set of eigenvectors $\\{\\mathbf{u}_n\\}$ satisfies the weighted orthonormality condition $\\langle \\mathbf{u}_m, \\mathbf{u}_n \\rangle_\\epsilon = \\mathbf{u}_m^\\top M \\mathbf{u}_n = \\delta_{mn}$.\n\n### 2. Parametric Sampling and Mode Tracking\n\nWhen solving the eigenproblem $K \\mathbf{u} = \\lambda M(p) \\mathbf{u}$ at different parameter values $p_j$, standard eigensolvers typically return the eigenvalues sorted in ascending order. However, the physical identity of a mode can change its position in the sorted list as $p$ varies. This phenomenon is known as mode crossing. To build a meaningful surrogate, we must track each physical mode consistently across the parameter space.\n\nOur strategy relies on the principle of modal continuity: for a small change in the parameter $p_j \\to p_{j+1}$, the eigenvector (mode shape) $\\mathbf{u}_n(p)$ should also change minimally. We can quantify the similarity between mode shapes using a vector projection, specifically the weighted inner product.\n\nLet $\\{\\mathbf{u}_n(p_j)\\}_{n=1}^k$ be the set of correctly ordered and normalized eigenvectors at parameter $p_j$. At the next parameter value $p_{j+1}$, the eigensolver returns a raw, sorted set of eigenvectors $\\{\\tilde{\\mathbf{u}}_m(p_{j+1})\\}_{m=1}^k$. To find the correct ordering for this new set, we compute an overlap matrix $C \\in \\mathbb{R}^{k \\times k}$ where each entry $C_{nm}$ measures the similarity between the $n$-th tracked mode from the previous step and the $m$-th raw mode from the current step. The inner product must use the mass matrix $M(p_{j+1})$ corresponding to the current parameter value:\n$$\nC_{nm} = \\left| \\langle \\mathbf{u}_n(p_j), \\tilde{\\mathbf{u}}_m(p_{j+1}) \\rangle_{M(p_{j+1})} \\right| = \\left| \\mathbf{u}_n(p_j)^\\top M(p_{j+1}) \\tilde{\\mathbf{u}}_m(p_{j+1}) \\right|\n$$\nThe absolute value is taken because the sign of an eigenvector is arbitrary. We seek to find a permutation $\\pi$ of $\\{1, \\dots, k\\}$ that reorders the new modes, $\\mathbf{u}_n(p_{j+1}) = \\tilde{\\mathbf{u}}_{\\pi(n)}(p_{j+1})$, such that the total similarity is maximized: $\\max_{\\pi} \\sum_{n=1}^k C_{n, \\pi(n)}$. This is a classic linear assignment problem. It can be solved efficiently by algorithms such as the Hungarian algorithm, which is implemented in `scipy.optimize.linear_sum_assignment`. This function finds the assignment that minimizes total cost, so we provide it with the negative of our similarity matrix, $-C$.\n\n### 3. Surrogate Model Construction\n\nThe goal is to create computationally cheap approximations, or surrogates, for the eigenfrequencies $\\omega_n(p)$ and eigenvectors $\\mathbf{E}_n(p)$.\n\n**Frequency Surrogate:** For each mode $n \\in \\{1,\\dots,k\\}$, we have a set of training data $(p_j, \\omega_n(p_j))$ obtained from the mode-tracked sampling process. We fit a polynomial of degree $3$ to this data. This yields a mapping $p \\mapsto \\omega_n^{\\mathrm{pred}}(p)$.\n\n**Field Surrogate (POD):** For each mode $n \\in \\{1,\\dots,k\\}$, we have a collection of mode-tracked eigenvectors $\\{\\mathbf{E}_n(p_j)\\}_{j=1}^{N_s}$, where $N_s$ is the number of training points. These vectors are called snapshots. Proper Orthogonal Decomposition (POD) is used to find a low-dimensional basis that optimally captures the variance in this set of snapshots.\n\n1.  **Snapshot Matrix:** The snapshots for mode $n$ are arranged as columns of a snapshot matrix $S_n = [\\mathbf{E}_n(p_1), \\dots, \\mathbf{E}_n(p_{N_s})] \\in \\mathbb{R}^{N \\times N_s}$.\n2.  **POD Basis:** The SVD of $S_n$ is computed: $S_n = U_n \\Sigma_n V_n^\\top$. The columns of $U_n$, called the POD modes or basis vectors $\\{\\boldsymbol{\\phi}_{ni}\\}$, form an orthonormal basis for the space spanned by the snapshots.\n3.  **Basis Truncation:** The singular values in $\\Sigma_n$ indicate the importance of each basis vector. The \"energy\" captured by the $i$-th basis vector is proportional to $\\sigma_{ni}^2$. We select the first $r_n$ basis vectors such that the cumulative energy $\\sum_{i=1}^{r_n} \\sigma_{ni}^2 / \\sum_{i=1}^{N_s} \\sigma_{ni}^2$ is at least $0.999$, with the additional constraint that $r_n \\le 5$.\n4.  **Coefficient Surrogates:** Any snapshot can be approximated by its projection onto the truncated POD basis: $\\mathbf{E}_n(p_j) \\approx \\sum_{i=1}^{r_n} c_{ni}(p_j) \\boldsymbol{\\phi}_{ni}$. The coefficients are $c_{ni}(p_j) = \\boldsymbol{\\phi}_{ni}^\\top \\mathbf{E}_n(p_j)$. For each mode $n$ and each basis index $i \\in \\{1, \\dots, r_n\\}$, we fit a degree-$3$ polynomial to the training data $(p_j, c_{ni}(p_j))$. This gives a surrogate function $p \\mapsto c_{ni}^{\\mathrm{pred}}(p)$.\n5.  **Field Reconstruction:** At a new parameter $p^\\star$, the predicted field is reconstructed using the POD basis and the predicted coefficients: $\\mathbf{E}_n^{\\mathrm{pred}}(p^\\star) = \\sum_{i=1}^{r_n} c_{ni}^{\\mathrm{pred}}(p^\\star) \\boldsymbol{\\phi}_{ni}$.\n\n**Orthogonality Enforcement:** The set of predicted fields $\\{\\mathbf{E}_n^{\\mathrm{pred}}(p^\\star)\\}_{n=1}^k$ is not guaranteed to be orthonormal with respect to the mass matrix $M(p^\\star)$. To enforce this physical constraint, we apply a weighted Gram-Schmidt process. Given the predicted vectors $\\{\\mathbf{v}_n = \\mathbf{E}_n^{\\mathrm{pred}}(p^\\star)\\}_{n=1}^k$, we generate an orthonormal set $\\{\\mathbf{q}_n\\}_{n=1}^k$ as follows:\n$$\n\\mathbf{w}_n = \\mathbf{v}_n - \\sum_{j=1}^{n-1} (\\mathbf{q}_j^\\top M(p^\\star) \\mathbf{v}_n) \\mathbf{q}_j \\quad ; \\quad \\mathbf{q}_n = \\frac{\\mathbf{w}_n}{\\sqrt{\\mathbf{w}_n^\\top M(p^\\star) \\mathbf{w}_n}}\n$$\nThe final predicted fields are this orthonormal set $\\{\\mathbf{q}_n\\}$.\n\n### 4. Surrogate Model Evaluation\n\nThe performance of the surrogate model is assessed at specified test parameters $p^\\star$ using three metrics comparing the surrogate's predictions to the \"true\" values obtained by a full numerical solution of the eigenproblem at $p^\\star$.\n\n-   **Frequency Error ($e_{\\mathrm{freq}}$):** The mean relative error across the $k$ predicted eigenfrequencies.\n-   **Orthogonality Error ($e_{\\mathrm{orth}}$):** The maximum absolute deviation of the predicted fields' Gram matrix from the identity matrix, after the Gram-Schmidt enforcement. This measures how well the final predicted fields satisfy the orthonormality constraint.\n-   **Mode-Tracking Correctness ($t$):** A boolean value indicating if each predicted mode corresponds to the correct true mode. This is verified by computing the overlap matrix between the true modes $\\{\\mathbf{E}_n^{\\mathrm{true}}\\}$ and the predicted modes $\\{\\mathbf{E}_m^{\\mathrm{pred}}\\}$ at $p^\\star$ and checking if the maximum overlap for each true mode occurs with the predicted mode of the same index.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main function to construct, train, and evaluate the surrogate model.\n    \"\"\"\n    # Numerical specifications\n    N = 120\n    L = 1.0\n    k = 3\n    eps1 = 1.0\n    eps2 = 4.0\n    poly_deg = 3\n    pod_energy_threshold = 0.999\n    pod_rank_max = 5\n\n    # Training and test parameters\n    train_points = 9\n    p_train = np.linspace(0.2, 0.8, train_points)\n    p_test = [0.25, 0.50, 0.90]\n\n    # Helper function to get system matrices\n    def get_system_matrices(p, N, L, eps1, eps2):\n        h = L / (N + 1)\n        x = np.linspace(h, L - h, N)\n        \n        # Stiffness matrix K\n        main_diag_K = np.full(N, 2.0 / h**2)\n        off_diag_K = np.full(N - 1, -1.0 / h**2)\n        K = np.diag(main_diag_K) + np.diag(off_diag_K, k=1) + np.diag(off_diag_K, k=-1)\n        \n        # Mass matrix M\n        eps_values = np.where(x <= p * L, eps1, eps2)\n        M = np.diag(eps_values * h)\n        \n        return K, M\n\n    # Helper function to solve the eigensystem\n    def solve_eigensystem(p, N, L, eps1, eps2, k):\n        K, M = get_system_matrices(p, N, L, eps1, eps2)\n        # Solve the generalized eigenvalue problem K u = lambda M u\n        \n        # scipy.linalg.eigh returns eigenvalues in ascending order\n        eigenvalues, eigenvectors = eigh(K, M, subset_by_index=[0, k - 1])\n        \n        # Normalize eigenvectors such that u.T @ M @ u = 1\n        for i in range(k):\n            norm = np.sqrt(eigenvectors[:, i].T @ M @ eigenvectors[:, i])\n            eigenvectors[:, i] /= norm\n            \n        # omega = sqrt(lambda)\n        omegas = np.sqrt(eigenvalues)\n        return omegas, eigenvectors, M\n\n    # --- Training Phase ---\n    \n    # Store training data (frequencies and eigenvectors)\n    train_data_omegas = np.zeros((train_points, k))\n    train_data_evecs = np.zeros((train_points, N, k))\n\n    # Solve for the first parameter point\n    omegas_prev, evecs_prev, _ = solve_eigensystem(p_train[0], N, L, eps1, eps2, k)\n    train_data_omegas[0, :] = omegas_prev\n    train_data_evecs[0, :, :] = evecs_prev\n\n    # Solve for subsequent training points with mode tracking\n    for j in range(1, train_points):\n        p_j = p_train[j]\n        omegas_raw, evecs_raw, M_j = solve_eigensystem(p_j, N, L, eps1, eps2, k)\n        \n        # Mode tracking using linear assignment problem\n        # Cost matrix: negative absolute overlap\n        overlap_matrix = np.abs(evecs_prev.T @ M_j @ evecs_raw)\n        cost_matrix = -overlap_matrix\n        \n        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n        \n        # Reorder the raw solutions\n        omegas_tracked = omegas_raw[col_ind]\n        evecs_tracked = evecs_raw[:, col_ind]\n        \n        train_data_omegas[j, :] = omegas_tracked\n        train_data_evecs[j, :, :] = evecs_tracked\n        \n        # Update previous for next iteration\n        evecs_prev = evecs_tracked\n\n    # --- Surrogate Construction ---\n    \n    # 1. Frequency surrogates\n    omega_surrogates = [np.poly1d(np.polyfit(p_train, train_data_omegas[:, i], poly_deg)) for i in range(k)]\n\n    # 2. Field surrogates (POD)\n    pod_bases = []\n    pod_coeff_surrogates = []\n\n    for n in range(k):\n        # Snapshot matrix for mode n\n        S_n = train_data_evecs[:, :, n].T\n        \n        # SVD for POD basis\n        U, s, _ = np.linalg.svd(S_n, full_matrices=False)\n        \n        # Truncate basis based on energy\n        energies = s**2\n        cumulative_energy = np.cumsum(energies) / np.sum(energies)\n        r_n = np.searchsorted(cumulative_energy, pod_energy_threshold, side='right') + 1\n        r_n = min(r_n, pod_rank_max, U.shape[1])\n        \n        pod_basis = U[:, :r_n]\n        pod_bases.append(pod_basis)\n        \n        # Project snapshots to get coefficients\n        coeffs = (pod_basis.T @ S_n).T\n        \n        # Fit surrogates for coefficients\n        coeff_surrogates_n = [np.poly1d(np.polyfit(p_train, coeffs[:, i], poly_deg)) for i in range(r_n)]\n        pod_coeff_surrogates.append(coeff_surrogates_n)\n\n    # --- Evaluation Phase ---\n    results = []\n    for p_star in p_test:\n        # 1. Get \"true\" solution at p_star\n        omegas_true, evecs_true, M_star = solve_eigensystem(p_star, N, L, eps1, eps2, k)\n\n        # 2. Get \"predicted\" solution from surrogate\n        # Predict frequencies\n        omegas_pred = np.array([omega_surrogates[n](p_star) for n in range(k)])\n        \n        # Predict fields\n        evecs_pred_raw = np.zeros((N, k))\n        for n in range(k):\n            # Predict POD coefficients\n            coeffs_pred = np.array([coeff_surrogate(p_star) for coeff_surrogate in pod_coeff_surrogates[n]])\n            # Reconstruct field\n            evecs_pred_raw[:, n] = pod_bases[n] @ coeffs_pred\n\n        # Enforce orthogonality via weighted Gram-Schmidt\n        evecs_pred = np.zeros_like(evecs_pred_raw)\n        for n in range(k):\n            v = evecs_pred_raw[:, n]\n            w = v - np.sum([((evecs_pred[:, j].T @ M_star @ v) * evecs_pred[:, j]) for j in range(n)], axis=0)\n            norm = np.sqrt(w.T @ M_star @ w)\n            evecs_pred[:, n] = w / norm\n\n        # 3. Compute metrics\n        # Frequency error\n        e_freq = np.mean(np.abs(omegas_pred - omegas_true) / omegas_true)\n        \n        # Orthogonality error\n        gram_matrix = evecs_pred.T @ M_star @ evecs_pred\n        e_orth = np.max(np.abs(gram_matrix - np.eye(k)))\n        \n        # Mode tracking correctness\n        # Note: the predicted vectors need to be checked against the *true* vectors\n        # The Gram-Schmidt process preserves the order of the input vectors\n        tracking_overlap = np.abs(evecs_true.T @ M_star @ evecs_pred)\n        # For each true mode (row), find the index of the predicted mode (col) with max overlap\n        max_overlap_indices = np.argmax(tracking_overlap, axis=1)\n        t_correct = bool(np.all(max_overlap_indices == np.arange(k)))\n        \n        results.extend([e_freq, e_orth, t_correct])\n        \n    # Format and print the final output\n    output_str = \"[\"\n    for i, item in enumerate(results):\n        if isinstance(item, bool):\n            output_str += str(item)\n        else:\n            output_str += f\"{item}\"\n        if i < len(results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Complex electromagnetic systems are often composed of multiple, interacting sub-components. This practice demonstrates the power of modular surrogate modeling by first learning a compact surrogate for a single scatterer's T-matrix, which characterizes its response to arbitrary incident waves. You will then use this component surrogate within a larger multiple-scattering framework to efficiently predict the combined response of a two-scatterer system, validating how accurately the composition rules perform with the learned model .",
            "id": "3352891",
            "problem": "Consider the scalar two-dimensional (2D) time-harmonic Helmholtz equation for a field $u(\\mathbf{r})$ with wavenumber $k$ (in $\\mathrm{m}^{-1}$). Let there be a pair of identical perfectly electrically conducting (PEC) circular cylinders of radius $a$ (in $\\mathrm{m}$), centered on the $x$-axis at positions $x_{1}=-d/2$ and $x_{2}=+d/2$, with $d>2a$ (in $\\mathrm{m}$). The background medium is homogeneous and lossless. A plane wave $u^{\\mathrm{inc}}(\\mathbf{r})=\\exp(i k x)$ is incident from $x=-\\infty$. The scattered field $u^{\\mathrm{sca}}$ obeys the Sommerfeld radiation condition.\n\nFundamental base and definitions:\n- The scalar Helmholtz equation is $\\nabla^{2} u + k^{2} u = 0$ in the exterior of the cylinders.\n- On each PEC cylinder boundary, the Dirichlet boundary condition holds: $u=0$ at $r=a$.\n- In a cylindrical multipole expansion about a cylinder’s center, the outgoing field basis functions are the Hankel functions of the first kind $H_{m}^{(1)}(k r) e^{i m \\theta}$ and the regular (non-singular) basis functions are the Bessel functions of the first kind $J_{m}(k r) e^{i m \\theta}$, where $m \\in \\mathbb{Z}$ and $(r,\\theta)$ are local polar coordinates about the cylinder’s center.\n\nSingle-scatterer $T$-matrix:\n- For a single PEC cylinder, the boundary condition $u=0$ at $r=a$ implies a diagonal single-particle $T$-matrix in the cylindrical harmonic basis. The diagonal entries $T_{m}(k)$ map the incoming regular coefficients $a_{m}$ to outgoing coefficients $b_{m}$ via $b_{m}=T_{m}(k)\\,a_{m}$. Using separation of variables and enforcing the boundary condition, one obtains $T_{m}(k)$ in terms of $J_{m}$ and $H_{m}^{(1)}$ evaluated at $k a$.\n\nComposition for multiple scattering:\n- Let $a_{p,m}$ and $b_{p,m}$ denote the incoming regular and outgoing cylindrical harmonic coefficients of order $m$ about cylinder $p \\in \\{1,2\\}$.\n- The multiple scattering coupling is described by translation operators derived from Graf’s addition theorem. For two cylinders separated by a distance $d$ along the $x$-axis (so that the inter-center angle is zero), the off-diagonal translation from outgoing mode $n$ at cylinder $q$ to regular mode $m$ at cylinder $p$ is given by the coefficient $H_{n-m}^{(1)}(k d)$.\n- The coupled system for two cylinders can be written in block-matrix form by assembling the self $T$-matrices on the block diagonal and the off-diagonal translation blocks. If $\\mathbf{b}$ denotes the stacked vector of all $b_{p,m}$ and $\\mathbf{a}^{\\mathrm{ext}}$ denotes the stacked vector of the incident regular coefficients (from the plane wave) at each cylinder, then the composition rule is expressed as\n$$\n\\left(\\mathbf{I} - \\mathbf{T}\\,\\mathbf{S}\\right)\\,\\mathbf{b} = \\mathbf{T}\\,\\mathbf{a}^{\\mathrm{ext}},\n$$\nwhere $\\mathbf{T}$ is block-diagonal with the single-particle $T$-matrices and $\\mathbf{S}$ contains the off-diagonal Hankel translation blocks and zero diagonal blocks.\n\nSurrogate modeling objective:\n- You must “learn” a single-particle $T$-matrix surrogate $\\widehat{T}_{m}(k)$ as a function of $k$ for each cylindrical harmonic order $m$ in a truncated range $m \\in \\{-M,\\ldots,M\\}$, by fitting a polynomial-in-$k$ model to training samples of the true $T_{m}(k)$ generated from the first-principles boundary condition for a PEC cylinder.\n- Use a complex-valued ridge-regularized least-squares fit for each $m$ to determine polynomial coefficients of degree $P$. The training set consists of $N_{\\mathrm{train}}$ evenly spaced sample points $k \\in [k_{\\min},k_{\\max}]$.\n\nIncident plane wave expansion:\n- For a plane wave $u^{\\mathrm{inc}}(\\mathbf{r})=\\exp(i k x)$, its local regular expansion about a cylinder centered at $x_{p}$ has coefficients $a_{p,m}^{\\mathrm{ext}} = i^{m}\\,\\exp(i k x_{p})$ for each $m \\in \\{-M,\\ldots,M\\}$.\n\nNumerical task:\n- Implement the full multiple scattering block system using the learned surrogate $\\widehat{\\mathbf{T}}(k)$ to predict $\\mathbf{b}_{\\mathrm{sur}}(k,d)$ for the two-cylinder configuration and compare it to the “ground-truth” $\\mathbf{b}_{\\mathrm{true}}(k,d)$ computed by using the exact single-particle $T$-matrix obtained from the boundary condition.\n- Use the relative error metric\n$$\n\\varepsilon(k,d) = \\frac{\\|\\mathbf{b}_{\\mathrm{sur}}(k,d) - \\mathbf{b}_{\\mathrm{true}}(k,d)\\|_{2}}{\\|\\mathbf{b}_{\\mathrm{true}}(k,d)\\|_{2}},\n$$\nwhere $\\|\\cdot\\|_{2}$ is the Euclidean norm.\n\nSpecifications, units, and parameters:\n- Use cylinder radius $a = 0.15\\,\\mathrm{m}$.\n- Use truncation order $M$ that is sufficiently large for accuracy over the training range; specifically choose $M = \\lceil k_{\\max}\\,a \\rceil + 8$.\n- Use polynomial degree $P = 5$ and ridge regularization strength $\\lambda = 10^{-8}$ (dimensionless).\n- Training interval and size: $k_{\\min} = 2.0\\,\\mathrm{m}^{-1}$, $k_{\\max} = 8.0\\,\\mathrm{m}^{-1}$, and $N_{\\mathrm{train}} = 50$.\n- Test suite: five $(k,d)$ pairs with $k$ in $\\mathrm{m}^{-1}$ and $d$ in $\\mathrm{m}$:\n    - Case $1$: $(k,d)=(6.0,\\,0.32)$,\n    - Case $2$: $(k,d)=(6.0,\\,0.80)$,\n    - Case $3$: $(k,d)=(6.0,\\,2.00)$,\n    - Case $4$: $(k,d)=(3.0,\\,0.50)$,\n    - Case $5$: $(k,d)=(8.0,\\,0.50)$.\n  These satisfy $d>2a$.\n- Angle units are radians. Physical units: $k$ in $\\mathrm{m}^{-1}$, $a$ and $d$ in $\\mathrm{m}$.\n\nProgram requirements:\n- Your program must\n    $1$. Generate the ground-truth single-particle $T_{m}(k)$ from the PEC boundary condition for training samples in $[k_{\\min},k_{\\max}]$ for each $m \\in \\{-M,\\ldots,M\\}$.\n    $2$. Fit a complex-valued polynomial surrogate $\\widehat{T}_{m}(k)$ of degree $P$ with ridge regularization $\\lambda$ for each $m$ using the training data.\n    $3$. For each test-case pair $(k,d)$, assemble the two-particle block system using the off-diagonal translation coefficients $H_{n-m}^{(1)}(k d)$, solve for both the surrogate-predicted $\\mathbf{b}_{\\mathrm{sur}}(k,d)$ and the ground-truth $\\mathbf{b}_{\\mathrm{true}}(k,d)$, and compute $\\varepsilon(k,d)$.\n    $4$. Round each $\\varepsilon(k,d)$ to six decimal places (expressed as a decimal, not a percentage).\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, e.g., $[\\varepsilon_{1},\\varepsilon_{2},\\varepsilon_{3},\\varepsilon_{4},\\varepsilon_{5}]$.\n\nAssumptions and reminders:\n- Use only the cylindrical harmonic orders in $\\{-M,\\ldots,M\\}$.\n- Ensure all computations are dimensionally consistent. All distances should be in $\\mathrm{m}$ and wavenumbers in $\\mathrm{m}^{-1}$.\n- The plane-wave propagation direction is along $+x$.",
            "solution": "The user has provided a problem in computational electromagnetics that requires the validation of a surrogate model for scattering analysis. The problem is scientifically valid, well-posed, and contains all necessary information for a unique solution. The methodology will proceed in three main stages: first, the derivation and implementation of the ground-truth physical model based on first principles; second, the construction of the data-driven surrogate model using polynomial regression; and third, the quantitative comparison of the surrogate model's predictions against the ground truth for a set of specified test cases.\n\nFirst principles model: The two-dimensional (2D) time-harmonic electromagnetic scattering from a set of objects is governed by the scalar Helmholtz equation, $\\nabla^2 u + k^2 u = 0$, where $u$ represents a field component (e.g., $E_z$ or $H_z$) and $k$ is the wavenumber of the background medium. For a perfectly electrically conducting (PEC) cylinder, the total field $u$ must vanish on its surface, leading to a Dirichlet boundary condition, $u=0$.\n\nThe field is decomposed into modes using a cylindrical harmonic basis. The total field outside a single cylinder is the sum of an incident field, $u^{\\mathrm{inc}}$, and a scattered field, $u^{\\mathrm{sca}}$. These fields are expanded as:\n$$u^{\\mathrm{inc}}(\\mathbf{r}) = \\sum_{m=-\\infty}^{\\infty} a_m J_m(kr) e^{im\\theta}$$\n$$u^{\\mathrm{sca}}(\\mathbf{r}) = \\sum_{m=-\\infty}^{\\infty} b_m H_m^{(1)}(kr) e^{im\\theta}$$\nwhere $(r,\\theta)$ are local polar coordinates, $a_m$ and $b_m$ are modal coefficients, $J_m$ is the Bessel function of the first kind (representing regular, non-singular waves), and $H_m^{(1)}$ is the Hankel function of the first kind (representing outgoing waves that satisfy the Sommerfeld radiation condition).\n\nThe boundary condition $u=u^{\\mathrm{inc}}+u^{\\mathrm{sca}}=0$ at the cylinder radius $r=a$ must hold for all angles $\\theta$. By orthogonality, this implies for each mode $m$:\n$$a_m J_m(ka) + b_m H_m^{(1)}(ka) = 0$$\nThis relation defines the single-scatterer $T$-matrix, $T_m(k)$, which maps incoming coefficients $a_m$ to outgoing coefficients $b_m$ via $b_m = T_m(k) a_m$. Solving for $T_m(k)$ gives the exact formula:\n$$T_m(k) = -\\frac{J_m(ka)}{H_m^{(1)}(ka)}$$\nFor a practical numerical implementation, the infinite sum over modes $m$ is truncated to a finite range $m \\in \\{-M, \\ldots, M\\}$. The truncation order $M$ is chosen to be $M = \\lceil k_{\\max} a \\rceil + 8$, which is $M = \\lceil 8.0 \\times 0.15 \\rceil + 8 = 10$.\n\nFor multiple scattering between two cylinders, labeled $p=1$ and $p=2$, the outgoing field from one cylinder acts as an incoming field for the other. This coupling is captured by the T-matrix formalism. The total coefficients of the field exciting cylinder $p$, denoted $\\mathbf{a}_p$, are the sum of the external incident field, $\\mathbf{a}_p^{\\mathrm{ext}}$, and the field scattered from cylinder $q \\ne p$. This leads to a coupled system of equations:\n$$(\\mathbf{I} - \\mathbf{T}\\mathbf{S})\\mathbf{b} = \\mathbf{T}\\mathbf{a}^{\\mathrm{ext}}$$\nHere, $\\mathbf{b}$ is the stacked vector of all unknown outgoing coefficients $b_{p,m}$, $\\mathbf{a}^{\\mathrm{ext}}$ is the stacked vector of known incident field coefficients, $\\mathbf{T}$ is a block-diagonal matrix containing the single-cylinder T-matrices for each cylinder, and $\\mathbf{S}$ is a block-off-diagonal matrix containing translation operators.\nFor two identical cylinders, the block structure is:\n$$\\begin{pmatrix} \\mathbf{I} & -\\mathbf{T}^{(1)} \\mathbf{S}^{(12)} \\\\ -\\mathbf{T}^{(2)} \\mathbf{S}^{(21)} & \\mathbf{I} \\end{pmatrix} \\begin{pmatrix} \\mathbf{b}_1 \\\\ \\mathbf{b}_2 \\end{pmatrix} = \\begin{pmatrix} \\mathbf{T}^{(1)} \\mathbf{a}_1^{\\mathrm{ext}} \\\\ \\mathbf{T}^{(2)} \\mathbf{a}_2^{\\mathrm{ext}} \\end{pmatrix}$$\nwhere $\\mathbf{T}^{(1)}=\\mathbf{T}^{(2)}$ are diagonal matrices with entries $T_m(k)$. The incident field is a plane wave $u^{\\mathrm{inc}}(x) = \\exp(ikx)$, whose coefficients at cylinder $p$ (centered at $x_p$) are $a_{p,m}^{\\mathrm{ext}} = i^m \\exp(ikx_p)$. For this problem, $x_1 = -d/2$ and $x_2=d/2$. The translation matrices $\\mathbf{S}^{(12)}$ and $\\mathbf{S}^{(21)}$ describe the transformation of outgoing modes from one cylinder to regular modes at the other. According to the problem statement, the entry of the translation matrix corresponding to the mapping from an outgoing mode $n$ at one cylinder to a regular mode $m$ at the other is $H_{n-m}^{(1)}(kd)$. This defines the elements of $\\mathbf{S}^{(12)}$ and $\\mathbf{S}^{(21)}$, which are taken to be identical Toeplitz matrices.\n\nSurrogate Model: The objective is to replace the computationally expensive first-principles T-matrix, $T_m(k)$, with a simple polynomial surrogate, $\\widehat{T}_m(k)$. For each mode $m$, a polynomial of degree $P=5$ is sought:\n$$\\widehat{T}_m(k) = \\sum_{p=0}^{P} c_{m,p} k^p$$\nThe complex coefficients $c_{m,p}$ are determined by fitting this model to training data. The data is generated by sampling the exact $T_m(k)$ at $N_{\\mathrm{train}}=50$ evenly spaced wavenumbers $k_j$ in the interval $[k_{\\min}, k_{\\max}] = [2.0, 8.0]\\, \\mathrm{m}^{-1}$. The fitting is performed using complex-valued ridge-regularized least-squares, which minimizes the loss function:\n$$L(\\mathbf{c}_m) = \\sum_{j=1}^{N_{\\mathrm{train}}} \\left| \\sum_{p=0}^{P} c_{m,p} k_j^p - T_m(k_j) \\right|^2 + \\lambda \\sum_{p=0}^{P} |c_{m,p}|^2$$\nwith regularization strength $\\lambda=10^{-8}$. This minimization problem has a closed-form solution via the normal equations: $(\\mathbf{V}^H\\mathbf{V} + \\lambda\\mathbf{I})\\mathbf{c}_m = \\mathbf{V}^H\\mathbf{y}_m$, where $\\mathbf{V}$ is the Vandermonde matrix of the sample wavenumbers $k_j$, $\\mathbf{y}_m$ is the vector of true $T_m(k_j)$ values, and $\\mathbf{c}_m$ is the vector of polynomial coefficients.\n\nEvaluation: For each test pair $(k,d)$, the multiple scattering system is assembled and solved twice. First, using the exact T-matrix, $T_m(k)$, to obtain the ground-truth scattered coefficients $\\mathbf{b}_{\\mathrm{true}}(k,d)$. Second, using the learned surrogate, $\\widehat{T}_m(k)$, to obtain the predicted coefficients $\\mathbf{b}_{\\mathrm{sur}}(k,d)$. The accuracy of the surrogate model is then quantified by the relative error:\n$$\\varepsilon(k,d) = \\frac{\\|\\mathbf{b}_{\\mathrm{sur}}(k,d) - \\mathbf{b}_{\\mathrm{true}}(k,d)\\|_{2}}{\\|\\mathbf{b}_{\\mathrm{true}}(k,d)\\|_{2}}$$\nwhere $\\|\\cdot\\|_2$ is the Euclidean norm of the full vector of scattered coefficients for both cylinders. The following program implements this entire procedure.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import jv, hankel1\n\ndef solve():\n    \"\"\"\n    Solves the two-cylinder scattering problem using both a ground-truth T-matrix\n    and a trained polynomial surrogate, then computes the relative error.\n    \"\"\"\n    # -- 1. Define constants and parameters --\n    a = 0.15  # Cylinder radius in m\n    P = 5  # Polynomial degree for surrogate model\n    reg_lambda = 1e-8  # Ridge regularization strength\n    k_min, k_max = 2.0, 8.0  # Training wavenumber range in m^-1\n    N_train = 50  # Number of training samples\n\n    # Calculate harmonic truncation order M\n    M = int(np.ceil(k_max * a)) + 8\n    m_vals = np.arange(-M, M + 1)\n    num_modes = len(m_vals)\n\n    # Test cases: (k [m^-1], d [m])\n    test_cases = [\n        (6.0, 0.32),\n        (6.0, 0.80),\n        (6.0, 2.00),\n        (3.0, 0.50),\n        (8.0, 0.50),\n    ]\n\n    # -- 2. Train the T-matrix surrogate model --\n    surrogate_coeffs = {}\n    k_samples = np.linspace(k_min, k_max, N_train)\n    V_train = np.vander(k_samples, P + 1, increasing=True)\n    \n    # Precompute part of the normal equations matrix\n    V_H_V = V_train.T.conj() @ V_train\n    A_reg = V_H_V + reg_lambda * np.identity(P + 1)\n\n    for m in m_vals:\n        # Generate training data y_m = T_m(k)\n        ka_samples = k_samples * a\n        y_m = -jv(m, ka_samples) / hankel1(m, ka_samples)\n\n        # Solve for polynomial coefficients c_m using normal equations\n        # (V^H V + lambda I) c = V^H y\n        b_vec = V_train.T.conj() @ y_m\n        c_m = np.linalg.solve(A_reg, b_vec)\n        \n        # Store coefficients (note: np.polyval expects highest power first)\n        surrogate_coeffs[m] = c_m[::-1]\n\n    # -- 3. Evaluate surrogate for each test case --\n    results = []\n    for k_test, d_test in test_cases:\n        # -- Assemble system components --\n        \n        # Incident field coefficients for cylinders at x = -d/2 and x = +d/2\n        a_ext1 = (1j**m_vals) * np.exp(1j * k_test * (-d_test / 2.0))\n        a_ext2 = (1j**m_vals) * np.exp(1j * k_test * (d_test / 2.0))\n        a_ext_full = np.concatenate([a_ext1, a_ext2])\n\n        # Diagonal T-matrix entries (true and surrogate)\n        t_true_diag = -jv(m_vals, k_test * a) / hankel1(m_vals, k_test * a)\n        t_sur_diag = np.array([np.polyval(surrogate_coeffs[m], k_test) for m in m_vals])\n        \n        T_true_mat = np.diag(t_true_diag)\n        T_sur_mat = np.diag(t_sur_diag)\n\n        # Translation matrix S\n        S_trans = np.zeros((num_modes, num_modes), dtype=np.complex128)\n        for i, m in enumerate(m_vals):\n            for j, n in enumerate(m_vals):\n                # Per problem spec, S_mn = H_{n-m}(kd)\n                S_trans[i, j] = hankel1(n - m, k_test * d_test)\n\n        # -- Assemble and solve the TRUE multiple scattering system --\n        A_block_true = -T_true_mat @ S_trans\n        A_true = np.block([\n            [np.eye(num_modes), A_block_true],\n            [A_block_true,      np.eye(num_modes)]\n        ])\n        c_true = np.concatenate([T_true_mat @ a_ext1, T_true_mat @ a_ext2])\n        b_true = np.linalg.solve(A_true, c_true)\n        \n        # -- Assemble and solve the SURROGATE multiple scattering system --\n        A_block_sur = -T_sur_mat @ S_trans\n        A_sur = np.block([\n            [np.eye(num_modes), A_block_sur],\n            [A_block_sur,      np.eye(num_modes)]\n        ])\n        c_sur = np.concatenate([T_sur_mat @ a_ext1, T_sur_mat @ a_ext2])\n        b_sur = np.linalg.solve(A_sur, c_sur)\n\n        # -- Compute and store the relative error --\n        error_norm = np.linalg.norm(b_sur - b_true)\n        true_norm = np.linalg.norm(b_true)\n        relative_error = error_norm / true_norm if true_norm > 0 else 0.0\n        results.append(round(relative_error, 6))\n\n    # -- 4. Final output --\n    # Format the results as a comma-separated list in brackets.\n    output_str = f\"[{','.join(f'{r:.6f}' for r in results)}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}