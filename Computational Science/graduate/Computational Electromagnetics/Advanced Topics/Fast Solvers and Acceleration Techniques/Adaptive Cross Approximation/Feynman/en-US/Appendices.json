{
    "hands_on_practices": [
        {
            "introduction": "Before employing any fast algorithm, it is crucial to understand the source of its efficiency. The Adaptive Cross Approximation gains its speed not just from compressing individual matrix blocks, but from how this compression scales across an entire hierarchically structured matrix. This practice guides you through a first-principles derivation of the computational cost of ACA, revealing the near-linear complexity, $\\mathcal{O}(r N \\log N)$, that makes it a cornerstone of modern fast solvers for large-scale problems. ",
            "id": "3287917",
            "problem": "In boundary integral formulations of time-harmonic computational electromagnetics, a Galerkin discretization over $N$ boundary unknowns yields a dense system matrix whose entries are given by pairwise evaluations of a weakly singular kernel. A hierarchical matrix (H-matrix) representation partitions the matrix into blocks using a balanced cluster tree on the index set, and treats well-separated (admissible) blocks via low-rank compression and near-field blocks directly. Consider an H-matrix built on a balanced binary cluster tree of depth $\\mathcal{O}(\\log N)$, where the number of admissible blocks is $\\mathcal{O}(N \\log N)$ and each kernel entry evaluation costs $\\mathcal{O}(1)$.\n\nFor an admissible block $\\mathbf{B} \\in \\mathbb{C}^{m \\times n}$, Adaptive Cross Approximation (ACA) seeks a rank-$r$ factorization by iteratively adding rank-$1$ outer products until a prescribed tolerance is met. In each ACA iteration, a pivot column index $j_{k}$ and pivot row index $i_{k}$ are selected by scanning magnitudes of a column vector and a row vector of the current residual (or their on-the-fly evaluations), and the corresponding pivot column and pivot row are evaluated by kernel calls. Assume the following modeling assumptions that are standard in fast assembly:\n- Each full pivot column evaluation costs $\\mathcal{O}(m)$ kernel calls and scanning it to select $i_{k}$ costs an additional $\\mathcal{O}(m)$ operations.\n- Each full pivot row evaluation costs $\\mathcal{O}(n)$ kernel calls and scanning it to select $j_{k+1}$ costs an additional $\\mathcal{O}(n)$ operations.\n- Forming and appending the length-$m$ and length-$n$ factor vectors for the rank-$1$ update, including the necessary rescalings, costs $\\mathcal{O}(m+n)$ arithmetic operations per iteration.\n- The algorithm terminates when a numerical rank $r$ (independent of $N$ for fixed frequency and admissibility parameters) is reached.\n\nUsing only these assumptions, and without invoking any prepackaged complexity formulas, do the following:\n- Derive the per-block operation count as a function of $m$, $n$, and $r$ that includes pivot search and column/row evaluations, in big-$\\mathcal{O}$ notation.\n- Then, aggregate this cost over all admissible blocks of the H-matrix to obtain the total asymptotic cost in big-$\\mathcal{O}$ notation as a function of $N$ and $r$, using only the fact that the number of admissible blocks is $\\mathcal{O}(N \\log N)$ in a balanced tree and each index participates in $\\mathcal{O}(\\log N)$ admissible interactions across levels.\n\nReport only the final asymptotic total cost across all admissible blocks, in big-$\\mathcal{O}$ notation, as your answer. No numerical approximation is required.",
            "solution": "The problem is valid. It is scientifically grounded in the well-established field of fast multipole and hierarchical matrix methods for computational science. The problem is well-posed, providing a clear set of assumptions and a well-defined objective. It is stated objectively and contains sufficient, consistent information for a rigorous derivation of the requested asymptotic complexity.\n\nThe derivation of the total asymptotic cost proceeds in two main stages: first, determining the cost of applying Adaptive Cross Approximation (ACA) to a single admissible block, and second, aggregating this cost over all admissible blocks within the H-matrix structure.\n\nFirst, we determine the computational cost of building a rank-$r$ ACA for a single admissible block $\\mathbf{B} \\in \\mathbb{C}^{m \\times n}$. The ACA algorithm is iterative, constructing a rank-$1$ update at each step for a total of $r$ steps, as the target rank $r$ is assumed to be reached. We analyze the cost of a single iteration, say iteration $k$, based on the provided assumptions.\n\n$1$. A pivot column must be selected and evaluated. This requires selecting an index $j_k$. The corresponding column of the residual matrix is then computed. The problem states that the cost of evaluating the full pivot column (via kernel calls) is $\\mathcal{O}(m)$.\n$2$. This column is then scanned to find the pivot row index $i_k$ corresponding to the entry with the largest magnitude. This scan costs an additional $\\mathcal{O}(m)$ operations.\n$3$. The pivot row corresponding to index $i_k$ is then evaluated. This costs $\\mathcal{O}(n)$ operations.\n$4$. This row is scanned to find the next pivot column index $j_{k+1}$. This scan costs an additional $\\mathcal{O}(n)$ operations.\n$5$. Finally, the rank-$1$ update vectors (a column vector of length $m$ and a row vector of length $n$) are formed and scaled. This is stated to cost $\\mathcal{O}(m+n)$ arithmetic operations.\n\nThe total cost for a single iteration of ACA is the sum of the costs of these steps:\n$$ \\text{Cost per iteration} = \\mathcal{O}(m) + \\mathcal{O}(m) + \\mathcal{O}(n) + \\mathcal{O}(n) + \\mathcal{O}(m+n) = \\mathcal{O}(m+n) $$\nSince the algorithm performs $r$ such iterations to reach the target rank, and the rank $r$ is independent of $m$ and $n$, the total cost to approximate a single $m \\times n$ block is:\n$$ \\text{Cost}_{\\text{ACA}}(\\mathbf{B}) = r \\cdot \\mathcal{O}(m+n) = \\mathcal{O}(r(m+n)) $$\nNext, we aggregate this per-block cost over all admissible blocks in the H-matrix. Let $\\mathcal{A}$ be the set of all admissible blocks. Each block corresponds to an interaction between a row index cluster $\\tau$ and a column index cluster $\\sigma$, such that the block size is $|\\tau| \\times |\\sigma|$. The total cost, $C_{\\text{total}}$, is the sum of costs over all blocks in $\\mathcal{A}$.\n$$ C_{\\text{total}} = \\sum_{(\\tau, \\sigma) \\in \\mathcal{A}} \\text{Cost}_{\\text{ACA}}(\\mathbf{B}_{\\tau, \\sigma}) = \\sum_{(\\tau, \\sigma) \\in \\mathcal{A}} \\mathcal{O}(r(|\\tau| + |\\sigma|)) $$\nSince $r$ is a constant factor with respect to the summation, we can write:\n$$ C_{\\text{total}} = \\mathcal{O}(r) \\sum_{(\\tau, \\sigma) \\in \\mathcal{A}} (|\\tau| + |\\sigma|) = \\mathcal{O}(r) \\left( \\sum_{(\\tau, \\sigma) \\in \\mathcal{A}} |\\tau| + \\sum_{(\\tau, \\sigma) \\in \\mathcal{A}} |\\sigma| \\right) $$\nTo evaluate the sums, we use the crucial information provided: \"each index participates in $\\mathcal{O}(\\log N)$ admissible interactions across levels.\" Let's analyze the first sum, $S_{\\text{rows}} = \\sum_{(\\tau, \\sigma) \\in \\mathcal{A}} |\\tau|$. We can rewrite this sum by first summing over the individual row indices $p \\in \\{1, \\dots, N\\}$:\n$$ S_{\\text{rows}} = \\sum_{(\\tau, \\sigma) \\in \\mathcal{A}} \\sum_{p \\in \\tau} 1 $$\nBy swapping the order of summation, we get:\n$$ S_{\\text{rows}} = \\sum_{p=1}^{N} \\sum_{\\substack{(\\tau, \\sigma) \\in \\mathcal{A} \\\\ p \\in \\tau}} 1 $$\nThe inner sum counts the number of admissible blocks $(\\tau, \\sigma)$ for which the index $p$ is a member of the row cluster $\\tau$. According to the problem statement, this quantity is $\\mathcal{O}(\\log N)$ for each index $p$. Substituting this into the expression for $S_{\\text{rows}}$:\n$$ S_{\\text{rows}} = \\sum_{p=1}^{N} \\mathcal{O}(\\log N) = N \\cdot \\mathcal{O}(\\log N) = \\mathcal{O}(N \\log N) $$\nBy an identical argument based on symmetry, the sum over the column dimensions, $S_{\\text{cols}} = \\sum_{(\\tau, \\sigma) \\in \\mathcal{A}} |\\sigma|$, is also $\\mathcal{O}(N \\log N)$.\n$$ S_{\\text{cols}} = \\sum_{q=1}^{N} \\sum_{\\substack{(\\tau, \\sigma) \\in \\mathcal{A} \\\\ q \\in \\sigma}} 1 = \\sum_{q=1}^{N} \\mathcal{O}(\\log N) = \\mathcal{O}(N \\log N) $$\nFinally, we substitute these results back into the expression for the total cost:\n$$ C_{\\text{total}} = \\mathcal{O}(r) (S_{\\text{rows}} + S_{\\text{cols}}) = \\mathcal{O}(r) (\\mathcal{O}(N \\log N) + \\mathcal{O}(N \\log N)) $$\nThis simplifies to the final asymptotic total cost for the ACA-based assembly of all admissible blocks.\n$$ C_{\\text{total}} = \\mathcal{O}(r N \\log N) $$\nThis complexity represents the total number of arithmetic operations required to construct the compressed representations of the far-field portion of the H-matrix.",
            "answer": "$$\n\\boxed{\\mathcal{O}(r N \\log N)}\n$$"
        },
        {
            "introduction": "Translating a theoretical algorithm into robust code often reveals subtle but critical challenges, such as preserving physical laws within the numerical approximation. In computational electromagnetics, physical reciprocity dictates that the impedance matrix $Z$ should be symmetric ($Z = Z^\\top$), a property that standard ACA does not inherently maintain. This hands-on coding exercise challenges you to implement ACA and confront this issue directly, programming a symmetrized update scheme to ensure your numerical model respects the underlying physics. ",
            "id": "3287916",
            "problem": "You are to study symmetry preservation under Adaptive Cross Approximation (ACA) pivoting for complex-valued, indefinite Electric Field Integral Equation (EFIE) impedance blocks. Consider a homogeneous, isotropic, reciprocal medium so that the Electric Field Integral Equation (EFIE) impedance matrix $Z$ constructed by the Method of Moments (MoM) under a Galerkin scheme with identical testing and basis functions satisfies reciprocity symmetry $Z = Z^\\top$ (transpose, not conjugate transpose). For a set of $N$ collocation points on a circular wire of radius $R$, define positions $\\{\\mathbf{r}_i\\}_{i=1}^N$ on the circle in the plane, with $\\mathbf{r}_i = \\left(R \\cos\\theta_i, R \\sin\\theta_i, 0\\right)$ and $\\theta_i = 2\\pi(i-1)/N$. Let the wavenumber be $k$ in radians per meter, and let the patch size parameter be $a$ in meters. The free-space scalar Green’s function is defined as $G(\\mathbf{r},\\mathbf{r}') = \\dfrac{e^{\\mathrm{i} k \\|\\mathbf{r} - \\mathbf{r}'\\|}}{\\|\\mathbf{r} - \\mathbf{r}'\\|}$, and the EFIE block entries are approximated as\n$$\nZ_{ij} = a^2 \\, G\\!\\left(\\mathbf{r}_i,\\mathbf{r}_j\\right) = a^2 \\, \\frac{e^{\\mathrm{i} k d_{ij}}}{d_{ij}},\n$$\nwhere $d_{ij} = \\max\\!\\left(\\|\\mathbf{r}_i - \\mathbf{r}_j\\|, a\\right)$ regularizes the diagonal by using $d_{ii} = a$. Distances $\\|\\cdot\\|$ and $a$ are measured in meters, and $k$ is measured in radians per meter. This construction yields a complex-valued, indefinite matrix $Z$ that obeys $Z = Z^\\top$ by reciprocity symmetry $G(\\mathbf{r},\\mathbf{r}') = G(\\mathbf{r}',\\mathbf{r})$.\n\nDefine the Adaptive Cross Approximation (ACA) approximation $Z_{\\text{ACA}}$ as a sum of $K$ rank-one updates with residual-driven pivoting. In an uncorrected, one-sided ACA iteration starting from the zero approximation, at iteration $t$:\n1. Compute the residual $R^{(t)} = Z - Z_{\\text{ACA}}^{(t)}$.\n2. Choose a pivot index $(i_t,j_t)$ that maximizes $\\left|R^{(t)}_{i_t,j_t}\\right|$.\n3. Form the column $c_t = R^{(t)}(:,j_t)$, the row $r_t = R^{(t)}(i_t,:)$, and the pivot $p_t = R^{(t)}_{i_t,j_t}$.\n4. Update $Z_{\\text{ACA}}^{(t+1)} = Z_{\\text{ACA}}^{(t)} + \\frac{1}{p_t} \\, c_t \\, r_t^\\top$.\n\nIt is known that the uncorrected update can destroy reciprocity symmetry even if $Z$ is symmetric, because $\\frac{1}{p_t} \\, c_t \\, r_t^\\top$ is generally not symmetric. A simple pivot correction to restore reciprocity symmetry is to symmetrize each rank-one update as\n$$\n\\Delta Z_t^{\\text{sym}} = \\frac{1}{2}\\left(\\frac{1}{p_t} c_t r_t^\\top + \\left(\\frac{1}{p_t} c_t r_t^\\top\\right)^\\top\\right),\n$$\nwhich enforces $\\Delta Z_t^{\\text{sym}} = \\left(\\Delta Z_t^{\\text{sym}}\\right)^\\top$ and thus maintains $Z_{\\text{ACA}}^{(t)} = \\left(Z_{\\text{ACA}}^{(t)}\\right)^\\top$ for all $t$.\n\nYour task is to implement both the uncorrected and the pivot-corrected ACA schemes, compute the symmetry defect of each approximation, and verify whether the pivot-corrected scheme achieves a prescribed tolerance. Use the following definitions:\n- The relative symmetry defect for an approximation $A$ is\n$$\n\\delta(A) = \\frac{\\|A - A^\\top\\|_F}{\\|Z\\|_F},\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm.\n- ACA terminates either when the iteration count reaches $K$ or when the current pivot magnitude drops below a small numerical threshold $\\tau$.\n\nConstruct $Z$ for each test case, compute $Z_{\\text{ACA}}$ using uncorrected ACA and pivot-corrected ACA with identical $(K,\\tau)$, and report whether the pivot-corrected $Z_{\\text{ACA}}$ satisfies $\\delta\\!\\left(Z_{\\text{ACA}}\\right) \\le \\epsilon$ for the supplied tolerance $\\epsilon$.\n\nUnits and numerical conventions:\n- Distances $\\|\\mathbf{r}_i - \\mathbf{r}_j\\|$ and patch size $a$ must be in meters.\n- The wavenumber $k$ must be in radians per meter.\n- Angles used to place points on the circle are in radians.\n- The tolerance $\\epsilon$ is dimensionless and must be treated as a decimal value.\n\nTest suite:\n- Case $1$: $N=32$, $R=0.5$ meters, $a=0.02$ meters, $k=10.0$ radians per meter, $K=8$, $\\tau=10^{-14}$, $\\epsilon=10^{-10}$.\n- Case $2$: $N=48$, $R=0.5$ meters, $a=0.015$ meters, $k=20.0$ radians per meter, $K=10$, $\\tau=10^{-14}$, $\\epsilon=10^{-12}$.\n- Case $3$: $N=24$, $R=0.5$ meters, $a=0.05$ meters, $k=3.0$ radians per meter, $K=6$, $\\tau=10^{-14}$, $\\epsilon=10^{-9}$.\n- Case $4$: $N=12$, $R=0.5$ meters, $a=0.10$ meters, $k=1.0$ radians per meter, $K=5$, $\\tau=10^{-14}$, $\\epsilon=10^{-12}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). For each test case, output a boolean that is true if the pivot-corrected $Z_{\\text{ACA}}$ satisfies $\\delta\\!\\left(Z_{\\text{ACA}}\\right) \\le \\epsilon$, and false otherwise. The final output must have the form\n$$\n[\\text{bool}_1,\\text{bool}_2,\\text{bool}_3,\\text{bool}_4].\n$$",
            "solution": "The problem requires an analysis of symmetry preservation in the Adaptive Cross Approximation (ACA) algorithm when applied to complex symmetric matrices arising from the Method of Moments (MoM) discretization of the Electric Field Integral Equation (EFIE). We must implement both a standard, uncorrected ACA and a pivot-corrected, symmetrized version, and then verify if the latter maintains symmetry within a specified tolerance for a given set of test cases.\n\nFirst, we establish the mathematical foundation for the impedance matrix $Z$. The problem is set in a homogeneous, isotropic, and reciprocal medium. When the EFIE is discretized using a Galerkin scheme, where the testing functions are identical to the basis functions, the resulting impedance matrix $Z$ inherits the reciprocity of the underlying physics. This manifests as the matrix being symmetric, $Z = Z^\\top$, not Hermitian. The matrix entries $Z_{ij}$ are given by a simplified model using the scalar Green's function for point sources:\n$$\nZ_{ij} = a^2 \\, G\\!\\left(\\mathbf{r}_i,\\mathbf{r}_j\\right) = a^2 \\, \\frac{e^{\\mathrm{i} k d_{ij}}}{d_{ij}}\n$$\nwhere $k$ is the wavenumber, $a$ is a patch size parameter, and $\\mathbf{r}_i, \\mathbf{r}_j$ are collocation points. The distance $d_{ij}$ is regularized to handle the diagonal entries ($i=j$) where the physical distance $\\|\\mathbf{r}_i - \\mathbf{r}_i\\|$ is $0$. The regularization is defined as $d_{ij} = \\max\\!\\left(\\|\\mathbf{r}_i - \\mathbf{r}_j\\|, a\\right)$, which ensures that $d_{ii}=a$, preventing a division-by-zero singularity. Since the Euclidean distance is symmetric, $\\|\\mathbf{r}_i - \\mathbf{r}_j\\| = \\|\\mathbf{r}_j - \\mathbf{r}_i\\|$, it follows that $d_{ij} = d_{ji}$ and thus $Z_{ij} = Z_{ji}$, confirming that the constructed matrix $Z$ is indeed symmetric.\n\nThe core of the problem lies in the ACA algorithm, a fast method for generating a low-rank approximation of a matrix. The approximation, $Z_{\\text{ACA}}$, is built iteratively as a sum of rank-$1$ matrices: $Z_{\\text{ACA}} = \\sum_{t=1}^K \\Delta Z_t$. In a standard, uncorrected ACA scheme, the process at iteration $t$ is as follows:\n$1$. The residual matrix is $R^{(t)} = Z - Z_{\\text{ACA}}^{(t)}$, where $Z_{\\text{ACA}}^{(t)}$ is the approximation after $t-1$ steps. Initially, $Z_{\\text{ACA}}^{(1)}$ is a zero matrix.\n$2$. A pivot element $(i_t, j_t)$ is selected by finding the entry with the largest magnitude in the current residual: $(i_t, j_t) = \\operatorname{argmax}_{(i,j)} |R^{(t)}_{i,j}|$.\n$3$. The corresponding row $r_t = R^{(t)}(i_t, :)$, column $c_t = R^{(t)}(:, j_t)$, and pivot value $p_t = R^{(t)}_{i_t, j_t}$ are extracted from the residual.\n$4$. A rank-$1$ update is formed and added to the approximation: $\\Delta Z_t = \\frac{1}{p_t} c_t r_t^\\top$.\n\nThe issue arises in step $4$. Even if the original matrix $Z$ is symmetric, the rank-$1$ update $\\Delta Z_t = \\frac{1}{p_t} c_t r_t^\\top$ is generally not symmetric. This is because the pivot selection does not guarantee $i_t=j_t$, and even if it did, the column $c_t$ and row $r_t$ are extracted from a residual matrix that may have already lost its symmetry in previous steps. Consequently, the final approximation $Z_{\\text{ACA}}$ will not be symmetric.\n\nTo remedy this, a pivot-corrected scheme is proposed. The pivot selection process (steps $1-3$) remains identical, ensuring the same residual-driven adaptivity. However, the update step is modified to explicitly enforce symmetry. The new rank-$1$ update $\\Delta Z_t^{\\text{sym}}$ is a symmetrized version of the original:\n$$\n\\Delta Z_t^{\\text{sym}} = \\frac{1}{2}\\left(\\frac{1}{p_t} c_t r_t^\\top + \\left(\\frac{1}{p_t} c_t r_t^\\top\\right)^\\top\\right) = \\frac{1}{2p_t}\\left(c_t r_t^\\top + r_t c_t^\\top\\right)\n$$\nSince $(\\Delta Z_t^{\\text{sym}})^\\top = \\Delta Z_t^{\\text{sym}}$ by construction, adding this update at each step guarantees that if the approximation $Z_{\\text{ACA}}^{(t)}$ is symmetric, then $Z_{\\text{ACA}}^{(t+1)} = Z_{\\text{ACA}}^{(t)} + \\Delta Z_t^{\\text{sym}}$ will also be symmetric. Starting with a symmetric zero matrix, the final approximation $Z_{\\text{ACA}}$ is guaranteed to be symmetric.\n\nThe task is to implement this corrected scheme and quantify its success. The metric for success is the relative symmetry defect, defined as:\n$$\n\\delta(A) = \\frac{\\|A - A^\\top\\|_F}{\\|Z\\|_F}\n$$\nwhere $\\|\\cdot\\|_F$ is the Frobenius norm. For the pivot-corrected scheme, we expect this defect to be very close to machine precision, as the primary source of asymmetry would be floating-point rounding errors. The algorithm terminates after $K$ iterations or if the pivot magnitude $|p_t|$ falls below a numerical threshold $\\tau$.\n\nThe solution proceeds as follows:\n$1$. For each test case, we first generate the $N$ collocation points $\\mathbf{r}_i$ on a circle of radius $R$.\n$2$. We then construct the full $N \\times N$ complex symmetric matrix $Z$ according to the given formula, computing the regularized distances $d_{ij}$ and then the matrix elements $Z_{ij}$.\n$3$. We implement the ACA algorithm with the symmetrized update rule. The algorithm iteratively determines pivots from the residual, constructs the symmetrized rank-$1$ update, adds it to the approximation, and subtracts it from the residual. This continues until $K$ iterations are completed or the pivot magnitude drops below $\\tau$.\n$4$. After the ACA process terminates, we compute the relative symmetry defect $\\delta(Z_{\\text{ACA}})$ for the resulting approximation.\n$5$. Finally, we check if $\\delta(Z_{\\text{ACA}}) \\le \\epsilon$ for the given tolerance $\\epsilon$. The boolean result of this check is recorded for each test case.\nThe aggregation of these boolean results constitutes the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of verifying symmetry preservation in pivot-corrected ACA.\n    \"\"\"\n    # Test cases defined in the problem statement.\n    # Format: (N, R, a, k, K, tau, epsilon)\n    test_cases = [\n        (32, 0.5, 0.02, 10.0, 8, 1e-14, 1e-10),\n        (48, 0.5, 0.015, 20.0, 10, 1e-14, 1e-12),\n        (24, 0.5, 0.05, 3.0, 6, 1e-14, 1e-9),\n        (12, 0.5, 0.10, 1.0, 5, 1e-14, 1e-12),\n    ]\n\n    def run_aca_case(N, R, a, k, K, tau, epsilon):\n        \"\"\"\n        Runs a single test case for the ACA symmetry check.\n        \n        Args:\n            N (int): Number of collocation points.\n            R (float): Radius of the circular wire in meters.\n            a (float): Patch size parameter for regularization in meters.\n            k (float): Wavenumber in radians per meter.\n            K (int): Maximum number of ACA iterations.\n            tau (float): Pivot magnitude termination threshold.\n            epsilon (float): Tolerance for the relative symmetry defect.\n\n        Returns:\n            bool: True if the symmetry defect condition is met, False otherwise.\n        \"\"\"\n        # Step 1: Generate collocation points on a circle in the x-y plane.\n        thetas = 2 * np.pi * np.arange(N) / N\n        points = np.zeros((N, 3))\n        points[:, 0] = R * np.cos(thetas)\n        points[:, 1] = R * np.sin(thetas)\n\n        # Step 2: Construct the full impedance matrix Z.\n        # Vectorized computation of pairwise Euclidean distances.\n        diffs = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n        dist_euc = np.linalg.norm(diffs, axis=2)\n        \n        # Apply regularization: d_ij = max(||r_i - r_j||, a)\n        dist_reg = np.maximum(dist_euc, a)\n        \n        # Calculate the matrix Z. Use np.complex128 for precision.\n        # To avoid division by zero if a=0 (not in tests, but good practice),\n        # handle the diagonal separately. However, dist_reg is never zero here.\n        Z = (a**2) * np.exp(1j * k * dist_reg) / dist_reg\n        Z = Z.astype(np.complex128)\n\n        # Step 3: Run the pivot-corrected ACA algorithm.\n        Z_aca = np.zeros_like(Z, dtype=np.complex128)\n        residual = Z.copy()\n        \n        for _ in range(K):\n            # Find the pivot (i_t, j_t) by maximizing |R_ij|.\n            flat_idx = np.argmax(np.abs(residual))\n            i_t, j_t = np.unravel_index(flat_idx, residual.shape)\n            \n            # Get the pivot value.\n            p_t = residual[i_t, j_t]\n            \n            # Check for termination condition based on pivot magnitude.\n            if np.abs(p_t)  tau:\n                break\n            \n            # Extract the pivot row and column from the residual.\n            c_t = residual[:, j_t]\n            r_t = residual[i_t, :]\n            \n            # Form the rank-1 update.\n            update_term = np.outer(c_t, r_t) / p_t\n            \n            # Symmetrize the update to enforce reciprocity.\n            sym_update = 0.5 * (update_term + update_term.T)\n            \n            # Update the ACA approximation and the residual.\n            Z_aca += sym_update\n            residual -= sym_update\n            \n        # Step 4: Calculate the relative symmetry defect.\n        norm_Z_F = np.linalg.norm(Z, 'fro')\n        \n        # Guard against norm_Z_F being zero for an all-zero matrix.\n        if norm_Z_F == 0:\n            # If Z is zero, the only valid approximation is also zero.\n            # a non-zero approximation would have infinite relative defect.\n            return np.linalg.norm(Z_aca - Z_aca.T, 'fro') == 0\n\n        sym_error_norm_F = np.linalg.norm(Z_aca - Z_aca.T, 'fro')\n        defect = sym_error_norm_F / norm_Z_F\n        \n        # Step 5: Verify if the defect is within the prescribed tolerance.\n        return defect = epsilon\n\n    results = []\n    for case in test_cases:\n        result = run_aca_case(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format \"[bool_1,bool_2,bool_3,bool_4]\".\n    # Python's str(True) is 'True'; convert to lowercase 'true'.\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The power of Adaptive Cross Approximation extends beyond mere matrix compression; the information it generates can serve as a potent diagnostic tool to guide other parts of a simulation. The numerical rank needed by ACA to approximate an interaction block is a direct measure of the complexity of the physical interaction it represents. In this advanced practice, you will leverage this concept to build an a posteriori error indicator, using the ACA rank to drive an adaptive meshing algorithm and intelligently refine a model where it is needed most. ",
            "id": "3287860",
            "problem": "You are to implement a complete, runnable program that constructs an a posteriori adaptive meshing indicator for boundary element interactions driven by Adaptive Cross Approximation (ACA) ranks, and uses it to decide which boundary geometry patches to refine. The physical context is a two-dimensional perfectly electrically conducting square cavity modeled via a boundary integral formulation of the scalar Helmholtz equation, and the numerical kernel is the free-space two-dimensional Green’s function. The goal is to demonstrate, using a simple and universal computational proxy, how near-resonant wavenumbers induce higher numerical ranks in off-diagonal interaction blocks and therefore trigger more mesh refinement according to a rank-based indicator.\n\nStart from the following foundational base:\n- The time-harmonic scalar Helmholtz equation in two dimensions is given by $\\nabla^2 u + k^2 u = 0$, where $u$ is the field and $k$ is the wavenumber.\n- The free-space two-dimensional Green’s function for the Helmholtz operator is $G_k(\\mathbf{r}, \\mathbf{r}') = \\dfrac{i}{4} H_0^{(1)}(k \\lVert \\mathbf{r} - \\mathbf{r}' \\rVert)$, where $H_0^{(1)}$ is the Hankel function of the first kind and order zero, and $\\lVert \\cdot \\rVert$ denotes the Euclidean norm.\n- For a uniform boundary discretization, a collocation boundary element matrix uses entries $G_{ij} = G_k(\\mathbf{r}_i, \\mathbf{r}_j) w_j$ for $i \\neq j$, where $\\mathbf{r}_j$ are panel midpoints and $w_j$ is the panel length. The self-term $i = j$ corresponds to a weak singularity that is not needed when only off-diagonal inter-patch interactions are considered as in this problem.\n- Adaptive Cross Approximation (ACA) builds a low-rank approximation of a matrix block by adaptively selecting pivotal rows and columns to form rank-$r$ updates until a relative residual norm tolerance is reached, thereby providing a data-driven estimator of the numerical rank required to approximate a block to a prescribed accuracy.\n\nYou must implement the following steps:\n1. Geometry and discretization. Consider a square cavity of side length $a = 1$ with boundary discretized uniformly into $M = 160$ straight panels. Use $M = 160$ panels. Denote the panel length by $h = \\dfrac{4}{M}$ and the panel midpoints $\\{\\mathbf{r}_j\\}_{j=1}^M$ in counterclockwise order starting at the middle of the bottom edge. Construct the dense complex-valued matrix $G \\in \\mathbb{C}^{M \\times M}$ with entries\n   - $G_{ij} = \\dfrac{i}{4} H_0^{(1)}\\!\\left(k \\lVert \\mathbf{r}_i - \\mathbf{r}_j \\rVert\\right) h$ for $i \\neq j$,\n   - $G_{ii} = 0$ (the diagonal is unused in this task).\n2. Patch partition. Partition the index set $\\{1,2,\\dots,M\\}$ into $P = 8$ contiguous patches of equal size, each of $M/P = 20$ consecutive indices, preserving the boundary ordering.\n3. ACA-based rank indicator. For a given wavenumber $k$, for each patch $p \\in \\{0,1,\\dots,P-1\\}$:\n   - Define the neighbor set $\\mathcal{N}(p) = \\{(p-1) \\bmod P, (p+1) \\bmod P\\}$.\n   - For each neighbor $q \\in \\mathcal{N}(p)$, form the off-diagonal interaction block $B_{pq} = G[I_p, I_q]$, where $I_p$ and $I_q$ are the index sets of patches $p$ and $q$.\n   - Estimate the ACA numerical rank $r_{pq}$ of $B_{pq}$ as the smallest integer $r$ such that the Frobenius-norm relative residual of the ACA approximation is less than a tolerance $\\varepsilon = 10^{-3}$, with a hard cap $r \\leq \\min(\\lvert I_p \\rvert, \\lvert I_q \\rvert)$.\n   - Define the patch indicator as $I_p = \\max_{q \\in \\mathcal{N}(p)} r_{pq}$.\n4. Refinement decision rule. Given a rank threshold $R_{\\mathrm{thr}} = 6$, mark patch $p$ for refinement if and only if $I_p  R_{\\mathrm{thr}}$. One refinement step splits each marked patch’s panels into two equal panels, but for this assignment, you only need to report the number of patches that would be refined at the first iteration for each test case.\n5. Validation via near-resonant behavior. Consider that a two-dimensional rectangular cavity with Dirichlet boundary conditions has eigen-wavenumbers approximately $k_{mn} \\approx \\pi \\sqrt{m^2 + n^2}/a$ for positive integers $m,n$. Use $a=1$ and target the near-resonant case $k \\approx \\pi \\sqrt{2}$, which is the $(m,n)=(1,1)$ mode for a square. Although the boundary integral with the free-space kernel is a simplification, the block rank required to resolve oscillatory interactions typically increases with $k$, enabling a rank-based adaptive refinement indicator.\n\nYou must implement the full program, including the ACA routine, and evaluate the refinement counts for the following test suite of wavenumbers:\n- Test case $1$ (low-frequency baseline): $k = 0.2$.\n- Test case $2$ (off-resonant moderate): $k = 1.0$.\n- Test case $3$ (near-resonant): $k = \\pi \\sqrt{2}$.\n\nFor each test case, run the steps above and compute the integer number of patches that satisfy $I_p  R_{\\mathrm{thr}}$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example, $[n_1,n_2,n_3]$, where each $n_j$ is the count of patches marked for refinement for test case $j$. No angles or dimensional physical units are required in the output because the results are unitless integers. Ensure that all numerical constants and parameters appearing in this problem, including $M = 160$, $P = 8$, $\\varepsilon = 10^{-3}$, and $R_{\\mathrm{thr}} = 6$, are used exactly as specified.",
            "solution": "The problem requires the implementation of a computational routine to identify regions of a discretized boundary that require mesh refinement. The context is the numerical solution of the two-dimensional scalar Helmholtz equation, $\\nabla^2 u + k^2 u = 0$, in a square cavity using a Boundary Element Method (BEM). The refinement decision is based on an a posteriori error indicator derived from the numerical ranks of interaction matrices between boundary patches, estimated using the Adaptive Cross Approximation (ACA) algorithm. The core hypothesis to be demonstrated is that near-resonant wavenumbers lead to higher numerical ranks, thus triggering more refinement.\n\nThe solution is constructed through the following logical steps:\n\n1.  **Boundary Discretization and Matrix Formulation**:\n    The boundary of the square cavity, with side length $a=1$, is discretized into $M=160$ uniform straight-line segments, or panels. The total perimeter is $4a=4$, so each panel has length $h = 4/M = 0.025$. The geometry is defined by the set of panel midpoints, $\\{\\mathbf{r}_j\\}_{j=1}^M$. As specified, the ordering begins such that the boundary between the last and first panels lies at the midpoint of the bottom edge, $(0.5, 0)$, proceeding counterclockwise.\n\n    The BEM formulation for the Helmholtz equation relates boundary values via an integral equation involving the free-space Green's function. For a collocation scheme, this results in a dense linear system $G \\mathbf{x} = \\mathbf{y}$. The matrix $G \\in \\mathbb{C}^{M \\times M}$ represents the interactions between all panel pairs. Its entries are given by:\n    $$ G_{ij} = G_k(\\mathbf{r}_i, \\mathbf{r}_j) w_j = \\frac{i}{4} H_0^{(1)}(k \\|\\mathbf{r}_i - \\mathbf{r}_j\\|) h $$\n    for $i \\neq j$. Here, $k$ is the wavenumber, $H_0^{(1)}$ is the Hankel function of the first kind and order zero, representing outgoing cylindrical waves, and $w_j = h$ is the integration weight, taken as the panel length. The diagonal entries $G_{ii}$ involve a singular kernel and are not required for this problem, so we set them to $G_{ii}=0$.\n\n2.  **Patch-Based Partitioning**:\n    To analyze interactions locally, the set of all panel indices $\\{0, 1, \\dots, M-1\\}$ is partitioned into $P=8$ contiguous, non-overlapping patches. Each patch $p \\in \\{0, \\dots, P-1\\}$ contains $M/P = 20$ consecutive panel indices. This partitioning divides the dense matrix $G$ into a $P \\times P$ block matrix, where each block $B_{pq} = G[I_p, I_q]$ contains the interactions between panels in patch $p$ and panels in patch $q$.\n\n3.  **Adaptive Cross Approximation (ACA) for Rank Estimation**:\n    The numerical rank of an interaction block $B_{pq}$ provides a quantitative measure of its complexity. Smooth, slowly varying interactions can be accurately represented by a low-rank approximation, whereas highly oscillatory interactions require a higher rank. ACA is an algebraic, matrix-only algorithm to construct such a low-rank approximation, $B_{pq} \\approx U V^H$, where $U$ and $V$ are matrices with a small number of columns, $r$.\n\n    The algorithm proceeds iteratively. Starting with a residual matrix $R_0 = B_{pq}$, at each step $k$, it identifies the largest-magnitude entry (the pivot) in the current residual $R_k$. A rank-$1$ matrix, formed from the corresponding row and column of $R_k$, is used to update the approximation, and the residual is updated accordingly: $R_{k+1} = R_k - \\mathbf{u}_k \\mathbf{v}_k^H$. The process continues until the Frobenius norm of the residual, $\\|R_r\\|_F$, falls below a specified tolerance relative to the norm of the original block, i.e., $\\|B_{pq} - U_r V_r^H\\|_F \\leq \\varepsilon \\|B_{pq}\\|_F$. The number of steps, $r$, required to meet this criterion is the estimated numerical rank of the block for the tolerance $\\varepsilon=10^{-3}$.\n\n4.  **Rank-Based Refinement Indicator**:\n    The physical principle motivating the indicator is that fields inside a near-resonant cavity are highly complex and oscillatory. This complexity is mirrored in the BEM matrix, where interaction blocks corresponding to these complex fields exhibit high numerical rank. Therefore, the rank can serve as an effective proxy for the local solution error.\n\n    For each patch $p$, we consider its interactions with its immediate neighbors along the boundary, defined by the set $\\mathcal{N}(p) = \\{(p-1) \\bmod P, (p+1) \\bmod P\\}$. The ACA-estimated ranks of the corresponding interaction blocks, $r_{pq}$ for $q \\in \\mathcal{N}(p)$, quantify the complexity of these interactions. A high rank indicates that the underlying field behavior between these patches is difficult to resolve with the current mesh. To be conservative, the refinement indicator for patch $p$ is defined as the maximum of these ranks:\n    $$ I_p = \\max_{q \\in \\mathcal{N}(p)} r_{pq} $$\n\n5.  **Refinement Decision and Validation**:\n    A simple decision rule is applied: a patch $p$ is marked for refinement if its indicator $I_p$ exceeds a predefined threshold, $R_{\\mathrm{thr}}=6$. This means that if the interaction with either neighbor is sufficiently complex, the patch's discretization is deemed inadequate.\n\n    The validation is performed by testing three wavenumbers:\n    -   $k = 0.2$: A low frequency, far from any resonance. Interactions are expected to be smooth, leading to low ranks and no refinement.\n    -   $k = 1.0$: A moderate, off-resonant frequency. Ranks may increase slightly but are expected to remain below the threshold.\n    -   $k = \\pi\\sqrt{2} \\approx 4.44$: This is chosen to be near the fundamental $(1,1)$ resonant mode of a square Dirichlet cavity ($k_{11} = \\pi \\sqrt{1^2+1^2}/a$). Near resonance, fields become highly oscillatory, and we anticipate that the numerical ranks will significantly increase, causing the indicators $I_p$ to exceed the threshold $R_{\\mathrm{thr}}$ and trigger refinement for multiple patches.\n\nThe implementation will construct the geometry and BEM matrix, then for each test case wavenumber, it will iterate through the patches, compute the ACA ranks of neighbor interactions, evaluate the indicator, and count the number of patches that satisfy the refinement criterion.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import hankel1\n\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Problem constants\n    a = 1.0  # Side length of the square cavity\n    M = 160  # Total number of panels\n    P = 8    # Number of patches\n    epsilon = 1e-3  # ACA relative tolerance\n    R_thr = 6      # Rank threshold for refinement\n\n    # Test cases\n    test_cases_k = [\n        0.2,                   # Low-frequency baseline\n        1.0,                   # Off-resonant moderate\n        np.pi * np.sqrt(2.0),  # Near-resonant\n    ]\n\n    # Generate geometry once, as it's independent of k\n    panel_midpoints, panel_length = generate_geometry(M, a)\n\n    results = []\n    for k in test_cases_k:\n        # 1. Construct the BEM matrix for the given wavenumber k\n        G = build_bem_matrix(k, panel_midpoints, panel_length)\n        \n        # 2. Calculate the number of patches to refine\n        refinement_count = calculate_refinement_count(G, M, P, epsilon, R_thr)\n        results.append(refinement_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef generate_geometry(M, a):\n    \"\"\"\n    Generates the panel midpoints for a square cavity.\n    The ordering starts from the middle of the bottom edge and proceeds counter-clockwise.\n    \"\"\"\n    if M % 4 != 0:\n        raise ValueError(\"M must be a multiple of 4.\")\n    \n    M_side = M // 4\n    h = a / M_side\n    \n    points = np.zeros((M, 2))\n    \n    # The split point is at (0.5*a, 0).\n    # Panels are ordered starting from the right of this split point.\n    \n    # Indices 0 to M_side/2 - 1: Bottom-right edge\n    n_br = M_side // 2\n    for j in range(n_br):\n        points[j, 0] = a * 0.5 + (j + 0.5) * h\n        points[j, 1] = 0.0\n\n    # Indices n_br to n_br + M_side - 1: Right edge\n    for j in range(M_side):\n        idx = j + n_br\n        points[idx, 0] = a\n        points[idx, 1] = (j + 0.5) * h\n\n    # Indices n_br + M_side to n_br + 2*M_side - 1: Top edge\n    for j in range(M_side):\n        idx = j + n_br + M_side\n        points[idx, 0] = a - (j + 0.5) * h\n        points[idx, 1] = a\n\n    # Indices n_br + 2*M_side to n_br + 3*M_side - 1: Left edge\n    for j in range(M_side):\n        idx = j + n_br + 2 * M_side\n        points[idx, 0] = 0.0\n        points[idx, 1] = a - (j + 0.5) * h\n\n    # Indices n_br + 3*M_side to M-1: Bottom-left edge\n    for j in range(n_br):\n        idx = j + n_br + 3 * M_side\n        points[idx, 0] = (j + 0.5) * h\n        points[idx, 1] = 0.0\n        \n    return points, 4.0 * a / M\n\n\ndef build_bem_matrix(k, panel_midpoints, h):\n    \"\"\"\n    Constructs the dense BEM matrix G.\n    \"\"\"\n    M = panel_midpoints.shape[0]\n    \n    # Use broadcasting for efficient distance calculation\n    # diffs.shape = (M, M, 2)\n    diffs = panel_midpoints[:, np.newaxis, :] - panel_midpoints[np.newaxis, :, :]\n    # distances.shape = (M, M)\n    distances = np.linalg.norm(diffs, axis=2)\n    \n    # Avoid division by zero for diagonal elements, though they will be overwritten\n    np.fill_diagonal(distances, 1.0)\n    \n    # Calculate Green's function values\n    G = (1j / 4.0) * hankel1(0, k * distances) * h\n    \n    # Set diagonal elements to 0 as specified\n    np.fill_diagonal(G, 0)\n    \n    return G\n\n\ndef aca(matrix_block, tol):\n    \"\"\"\n    Estimates the numerical rank of a matrix block using Adaptive Cross Approximation.\n    \"\"\"\n    m, n = matrix_block.shape\n    max_rank = min(m, n)\n    \n    residual = matrix_block.copy()\n    norm_fro_orig_sq = np.sum(np.abs(matrix_block)**2)\n\n    if norm_fro_orig_sq == 0:\n        return 0\n    \n    rank = 0\n    for r in range(max_rank):\n        # Find the pivot element in the current residual matrix\n        flat_idx = np.argmax(np.abs(residual))\n        i_piv, j_piv = np.unravel_index(flat_idx, residual.shape)\n        \n        pivot_val = residual[i_piv, j_piv]\n        \n        # If residual matrix is numerically zero, stop\n        if np.abs(pivot_val)**2  1e-30 * norm_fro_orig_sq:\n            break\n            \n        u_vec = residual[:, j_piv]\n        v_vec_T = residual[i_piv, :] / pivot_val\n        \n        # Update the residual with a rank-1 matrix\n        residual -= np.outer(u_vec, v_vec_T)\n        \n        rank += 1\n        \n        # Check stopping criterion based on Frobenius norm of the residual\n        norm_fro_res_sq = np.sum(np.abs(residual)**2)\n        if norm_fro_res_sq  tol**2 * norm_fro_orig_sq:\n            return rank\n            \n    return rank\n\n\ndef calculate_refinement_count(G, M, P, epsilon, R_thr):\n    \"\"\"\n    Calculates the number of patches to be refined based on ACA ranks.\n    \"\"\"\n    panels_per_patch = M // P\n    refinement_count = 0\n    \n    for p in range(P):\n        # Define neighbor patches\n        q_prev = (p - 1 + P) % P\n        q_next = (p + 1) % P\n        \n        # Get index sets for the current patch and its neighbors\n        I_p_slice = slice(p * panels_per_patch, (p + 1) * panels_per_patch)\n        I_q_prev_slice = slice(q_prev * panels_per_patch, (q_prev + 1) * panels_per_patch)\n        I_q_next_slice = slice(q_next * panels_per_patch, (q_next + 1) * panels_per_patch)\n        \n        # Extract interaction blocks\n        B_pq_prev = G[I_p_slice, I_q_prev_slice]\n        B_pq_next = G[I_p_slice, I_q_next_slice]\n        \n        # Estimate numerical ranks using ACA\n        r_pq_prev = aca(B_pq_prev, epsilon)\n        r_pq_next = aca(B_pq_next, epsilon)\n        \n        # Define patch indicator\n        Indicator_p = max(r_pq_prev, r_pq_next)\n        \n        # Apply refinement rule\n        if Indicator_p > R_thr:\n            refinement_count += 1\n            \n    return refinement_count\n\n# Entry point of the script\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}