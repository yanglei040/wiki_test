{
    "hands_on_practices": [
        {
            "introduction": "This exercise is fundamental to understanding the power and limitations of Monte Carlo methods. Before deploying complex simulations, it is essential to grasp how the accuracy of an estimate relates to computational effort. This practice guides you through a first-principles derivation to establish the canonical $\\mathcal{O}(N^{-1/2})$ convergence rate, revealing why achieving high precision with Monte Carlo can be computationally expensive and motivating the need for variance reduction techniques. ",
            "id": "3332260",
            "problem": "In computational electromagnetics, consider the frequency-domain representation of the electric field at a fixed observation point $\\mathbf{r}_{0}$ due to a current distribution $\\mathbf{J}(\\mathbf{r})$ supported on a bounded domain $\\Omega \\subset \\mathbb{R}^{3}$. The field can be written as a volume integral\n$$\nI \\equiv \\int_{\\Omega} \\mathbf{G}(\\mathbf{r}_{0},\\mathbf{r}) \\cdot \\mathbf{J}(\\mathbf{r}) \\,\\mathrm{d}\\mathbf{r},\n$$\nwhere $\\mathbf{G}(\\mathbf{r}_{0},\\mathbf{r})$ is the electromagnetic dyadic Green’s function and $\\cdot$ denotes the Euclidean inner product. To evaluate $I$ numerically, you adopt Monte Carlo importance sampling with a probability density $p(\\mathbf{r})$ that is strictly positive on $\\Omega$ and absolutely continuous with respect to Lebesgue measure. Define the random variable\n$$\nW \\equiv \\frac{f(X)}{p(X)}, \\quad \\text{where} \\quad f(\\mathbf{r}) \\equiv \\mathbf{G}(\\mathbf{r}_{0},\\mathbf{r}) \\cdot \\mathbf{J}(\\mathbf{r}), \\quad X \\sim p,\n$$\nand use $N$ independent and identically distributed samples $\\{X_{i}\\}_{i=1}^{N}$ to form the estimator\n$$\nI_{N} \\equiv \\frac{1}{N}\\sum_{i=1}^{N}\\frac{f(X_{i})}{p(X_{i})}.\n$$\nAssume $\\mathbb{E}[W^{2}]<\\infty$ so that the variance $\\sigma^{2} \\equiv \\mathrm{Var}(W)$ is finite. Starting from first principles, namely the definition of unbiasedness for Monte Carlo estimators and the variance of the average of independent and identically distributed random variables, derive a closed-form expression for the mean square error $\\mathbb{E}\\!\\left[(I_{N}-I)^{2}\\right]$ of $I_{N}$ in terms of $\\sigma^{2}$ and $N$. Then, given a target tolerance $\\varepsilon>0$ for the mean square error, determine the minimal $N$ that guarantees $\\mathbb{E}\\!\\left[(I_{N}-I)^{2}\\right]\\leq \\varepsilon^{2}$, expressed in closed form in terms of $\\sigma^{2}$ and $\\varepsilon$. Finally, provide the continuous approximation to this minimal $N$ and show that it reduces to the standard Monte Carlo complexity scaling. The final answer must be a single analytical expression for $N$ with no units.",
            "solution": "The problem requires the derivation of the mean square error (MSE) for a Monte Carlo estimator, the determination of the minimal sample size $N$ to meet a given error tolerance, and the analysis of the continuous approximation to this sample size. The derivation shall proceed from first principles.\n\nFirst, we analyze the properties of the estimator $I_{N}$. The problem defines the integral of interest as $I \\equiv \\int_{\\Omega} f(\\mathbf{r}) \\,\\mathrm{d}\\mathbf{r}$, where $f(\\mathbf{r}) \\equiv \\mathbf{G}(\\mathbf{r}_{0},\\mathbf{r}) \\cdot \\mathbf{J}(\\mathbf{r})$. The Monte Carlo method utilizes importance sampling with a probability density function (PDF) $p(\\mathbf{r})$ to estimate this integral. A random variable $X$ is drawn from this distribution, $X \\sim p$. The estimator is constructed from $N$ independent and identically distributed (i.i.d.) samples $\\{X_{i}\\}_{i=1}^{N}$.\n\nThe core of the method lies in the random variable $W \\equiv \\frac{f(X)}{p(X)}$. We first compute its expected value, $\\mathbb{E}[W]$. By the definition of expectation for a continuous random variable, this is:\n$$\n\\mathbb{E}[W] = \\int_{\\Omega} \\frac{f(\\mathbf{r})}{p(\\mathbf{r})} p(\\mathbf{r}) \\,\\mathrm{d}\\mathbf{r} = \\int_{\\Omega} f(\\mathbf{r}) \\,\\mathrm{d}\\mathbf{r} = I.\n$$\nThis demonstrates that the expected value of a single weighted sample is the integral $I$ we wish to compute.\n\nNext, we establish that the estimator $I_{N} \\equiv \\frac{1}{N}\\sum_{i=1}^{N} W_{i}$, where $W_{i} \\equiv \\frac{f(X_{i})}{p(X_{i})}$, is an unbiased estimator of $I$. Using the linearity of the expectation operator:\n$$\n\\mathbb{E}[I_{N}] = \\mathbb{E}\\left[\\frac{1}{N}\\sum_{i=1}^{N} W_{i}\\right] = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}[W_{i}].\n$$\nSince the samples $\\{X_{i}\\}$ are identically distributed, the random variables $\\{W_{i}\\}$ are also identically distributed. Therefore, $\\mathbb{E}[W_{i}] = \\mathbb{E}[W] = I$ for all $i \\in \\{1, 2, \\dots, N\\}$. Substituting this into the expression for $\\mathbb{E}[I_{N}]$ gives:\n$$\n\\mathbb{E}[I_{N}] = \\frac{1}{N}\\sum_{i=1}^{N} I = \\frac{1}{N}(NI) = I.\n$$\nThus, the estimator $I_{N}$ is unbiased.\n\nNow, we derive the mean square error (MSE) of the estimator, defined as $\\mathbb{E}[(I_{N}-I)^{2}]$. Since $I = \\mathbb{E}[I_{N}]$, the MSE is equivalent to the variance of the estimator $I_{N}$:\n$$\n\\mathbb{E}[(I_{N}-I)^{2}] = \\mathbb{E}[(I_{N}-\\mathbb{E}[I_{N}])^{2}] \\equiv \\mathrm{Var}(I_{N}).\n$$\nWe proceed to calculate $\\mathrm{Var}(I_{N})$:\n$$\n\\mathrm{Var}(I_{N}) = \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N} W_{i}\\right).\n$$\nUsing the variance property $\\mathrm{Var}(aY) = a^{2}\\mathrm{Var}(Y)$ with $a = 1/N$, we obtain:\n$$\n\\mathrm{Var}(I_{N}) = \\frac{1}{N^{2}}\\mathrm{Var}\\left(\\sum_{i=1}^{N} W_{i}\\right).\n$$\nThe problem states that the samples $\\{X_{i}\\}$ are independent. This implies that the random variables $\\{W_{i}\\}$ are also independent. For a sum of independent random variables, the variance of the sum is the sum of the variances:\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{N} W_{i}\\right) = \\sum_{i=1}^{N}\\mathrm{Var}(W_{i}).\n$$\nSince the $\\{W_{i}\\}$ are also identically distributed, their variances are equal. The problem defines this common variance as $\\sigma^{2} \\equiv \\mathrm{Var}(W)$. Note that the existence of this finite variance is guaranteed by the assumption $\\mathbb{E}[W^{2}]<\\infty$, since $\\mathrm{Var}(W) = \\mathbb{E}[W^{2}] - (\\mathbb{E}[W])^{2}$. Thus, $\\mathrm{Var}(W_{i}) = \\sigma^{2}$ for all $i$.\n$$\n\\sum_{i=1}^{N}\\mathrm{Var}(W_{i}) = \\sum_{i=1}^{N}\\sigma^{2} = N\\sigma^{2}.\n$$\nSubstituting this result back into the expression for $\\mathrm{Var}(I_{N})$ yields the closed-form expression for the MSE:\n$$\n\\mathbb{E}[(I_{N}-I)^{2}] = \\mathrm{Var}(I_{N}) = \\frac{1}{N^{2}}(N\\sigma^{2}) = \\frac{\\sigma^{2}}{N}.\n$$\n\nThe second task is to find the minimal integer $N$ that guarantees the MSE is no greater than a specified tolerance $\\varepsilon^{2}$, where $\\varepsilon > 0$. The condition is:\n$$\n\\mathbb{E}[(I_{N}-I)^{2}] \\leq \\varepsilon^{2}.\n$$\nSubstituting our derived expression for the MSE:\n$$\n\\frac{\\sigma^{2}}{N} \\leq \\varepsilon^{2}.\n$$\nAssuming the non-trivial case where $\\sigma^{2} > 0$, we can rearrange the inequality to solve for $N$:\n$$\nN \\geq \\frac{\\sigma^{2}}{\\varepsilon^{2}}.\n$$\nSince $N$ must be an integer representing the number of samples, the minimal value of $N$ that satisfies this condition is the smallest integer that is greater than or equal to $\\frac{\\sigma^{2}}{\\varepsilon^{2}}$. This corresponds to the ceiling function:\n$$\nN_{\\min} = \\left\\lceil \\frac{\\sigma^{2}}{\\varepsilon^{2}} \\right\\rceil.\n$$\n\nThe final task is to provide the continuous approximation to this minimal $N$ and analyze its scaling. For a large number of samples, the difference between a number and its ceiling is negligible. We can therefore approximate the minimal $N$ by dropping the ceiling function:\n$$\nN \\approx \\frac{\\sigma^{2}}{\\varepsilon^{2}}.\n$$\nThis expression reveals the standard complexity scaling of Monte Carlo methods. The root mean square error (RMSE), which is the standard deviation of the estimator, is $\\sqrt{\\mathbb{E}[(I_{N}-I)^{2}]} = \\sqrt{\\frac{\\sigma^{2}}{N}} = \\frac{\\sigma}{\\sqrt{N}}$. If we set this RMSE equal to our tolerance $\\varepsilon$, we have $\\varepsilon = \\frac{\\sigma}{\\sqrt{N}}$, which rearranges to $N = \\frac{\\sigma^{2}}{\\varepsilon^{2}}$. This shows that the number of samples $N$ required to achieve a desired error tolerance $\\varepsilon$ is proportional to $\\varepsilon^{-2}$, or $N = \\mathcal{O}(\\varepsilon^{-2})$. Equivalently, the error converges as $\\varepsilon = \\mathcal{O}(N^{-1/2})$. This is the canonical convergence rate for standard Monte Carlo integration, and our derivation confirms that the estimator follows this characteristic scaling law.",
            "answer": "$$\n\\boxed{\\frac{\\sigma^{2}}{\\varepsilon^{2}}}\n$$"
        },
        {
            "introduction": "Integral equation methods in electromagnetics frequently involve kernels that become singular, for example, when an observation point approaches a source point. This practice offers a hands-on coding exercise to address this challenge using stratified sampling, a variance reduction technique that is particularly effective for integrands with sharp features. By analyzing both a correctly implemented scheme and a common but flawed alternative, you will gain a deep appreciation for how careful sampler design is crucial for obtaining unbiased and accurate results. ",
            "id": "3332310",
            "problem": "You are given a computational electromagnetics context in which boundary integral formulations involve singular kernels that behave as $1/|\\mathbf{r} - \\mathbf{r}'|$ near $\\mathbf{r}' = \\mathbf{r}$. Consider a local planar surface patch around an observation point $\\mathbf{r}$ that lies near a boundary edge so that the integration region is a wedge sector, and model the singular kernel locally by the scalar function $f(\\rho,\\theta) = 1/\\rho$ in polar coordinates $(\\rho,\\theta)$ on the surface. Let the integration domain be the wedge-annulus\n$$\n\\mathcal{D}(\\alpha,\\varepsilon,R) = \\left\\{ (\\rho,\\theta)\\,:\\, \\varepsilon \\le \\rho \\le R,\\; -\\frac{\\alpha}{2} \\le \\theta \\le \\frac{\\alpha}{2} \\right\\},\n$$\nwhere $\\varepsilon$ is a small positive exclusion radius (to regularize the singularity), $R$ is the outer radius of the local patch, and $\\alpha$ is the wedge aperture angle. Angles are measured in radians, and all lengths are measured in meters. The integral of interest is\n$$\nI(\\alpha,\\varepsilon,R) = \\iint_{\\mathcal{D}(\\alpha,\\varepsilon,R)} \\frac{1}{\\rho}\\,\\mathrm{d}A,\n$$\nwith $\\mathrm{d}A = \\rho\\,\\mathrm{d}\\rho\\,\\mathrm{d}\\theta$.\n\nYour task is to construct and analyze a stratified sampling scheme suitable for a Monte Carlo (MC) estimator of $I(\\alpha,\\varepsilon,R)$ near the singularity, in comparison to a uniform-area MC estimator. The analysis must quantify the effect on bias at the singularity. Work from first principles and use only core definitions and well-tested facts; do not assume any pre-derived shortcut formulas.\n\n1. Starting from the definition of the integral $I(\\alpha,\\varepsilon,R)$ and the area element $\\mathrm{d}A = \\rho\\,\\mathrm{d}\\rho\\,\\mathrm{d}\\theta$, derive an exact closed-form expression for $I(\\alpha,\\varepsilon,R)$ in terms of $\\alpha$, $\\varepsilon$, and $R$. Express the final result in meters.\n\n2. Define the total area of the wedge-annulus\n$$\nA_{\\text{tot}}(\\alpha,\\varepsilon,R) = \\frac{\\alpha}{2}\\left(R^2 - \\varepsilon^2\\right).\n$$\nDescribe a correct stratified sampling scheme that partitions the radial interval $[\\varepsilon,R]$ into $K$ strata whose areas are equal, and explain how to weight stratum-wise estimates to produce an unbiased estimator of $I(\\alpha,\\varepsilon,R)$. Then, define an incorrect stratified scheme that partitions the radial interval into $K$ equal-width bins but assigns equal weights to each bin mean when estimating $I(\\alpha,\\varepsilon,R)$, and explain why it is biased near the singularity.\n\n3. For the incorrect scheme, consider the estimator that multiplies the unweighted average of per-bin means by $A_{\\text{tot}}(\\alpha,\\varepsilon,R)$. Compute the expected value of this estimator and derive its bias relative to the exact integral. Your derivation should be explicit in terms of the $K$ bin boundaries and must reveal how the bias depends on $\\varepsilon$ and $K$. Express the bias in meters.\n\n4. Implement a program, using deterministic calculations rather than random sampling, that outputs the bias of three estimators for each test case:\n   - The uniform-area MC estimator with proper area weighting (expected bias is to be computed analytically).\n   - The correct stratified estimator with $K$ equal-area radial strata and proper area weighting.\n   - The incorrect stratified estimator with $K$ equal-width radial strata and equal weights per bin mean, scaled by $A_{\\text{tot}}(\\alpha,\\varepsilon,R)$.\n\nUse the following test suite of parameter values, which exercises interior points, boundary-edge points, and near-corner behavior:\n- Test case $1$: $\\alpha = 2\\pi$ (full disk), $\\varepsilon = 10^{-3}\\,\\mathrm{m}$, $R = 1.0\\,\\mathrm{m}$, $K = 8$.\n- Test case $2$: $\\alpha = \\pi$ (half-disk), $\\varepsilon = 10^{-6}\\,\\mathrm{m}$, $R = 1.0\\,\\mathrm{m}$, $K = 16$.\n- Test case $3$: $\\alpha = \\pi/4$ (narrow wedge), $\\varepsilon = 10^{-8}\\,\\mathrm{m}$, $R = 0.2\\,\\mathrm{m}$, $K = 12$.\n- Test case $4$: $\\alpha = 2\\pi$ (full disk), $\\varepsilon = 0.5\\,\\mathrm{m}$, $R = 1.0\\,\\mathrm{m}$, $K = 4$.\n\nEach test case should produce a list of three floating-point numbers in meters: $\\left[\\text{bias}_{\\text{uniform}}, \\text{bias}_{\\text{strat-correct}}, \\text{bias}_{\\text{strat-wrong}}\\right]$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself the list for one test case. For example,\n$$\n\\left[ [b_{1,1},\\,b_{1,2},\\,b_{1,3}],\\; [b_{2,1},\\,b_{2,2},\\,b_{2,3}],\\; [b_{3,1},\\,b_{3,2},\\,b_{3,3}],\\; [b_{4,1},\\,b_{4,2},\\,b_{4,3}] \\right].\n$$\nAll angles must be in radians, and all outputs must be expressed in meters as decimal numbers.",
            "solution": "The problem requires the analysis and comparison of different Monte Carlo (MC) integration schemes for a singular integral arising in computational electromagnetics. The analysis proceeds in four parts: first, a direct calculation of the integral; second, a conceptual description of two stratified sampling schemes; third, a quantitative derivation of the bias for one of these schemes; and fourth, a deterministic numerical computation of the bias for three estimators across a suite of test cases.\n\n### 1. Exact Calculation of the Integral\n\nThe integral of interest is given by\n$$\nI(\\alpha,\\varepsilon,R) = \\iint_{\\mathcal{D}(\\alpha,\\varepsilon,R)} \\frac{1}{\\rho}\\,\\mathrm{d}A\n$$\nThe domain of integration is the wedge-annulus $\\mathcal{D}(\\alpha,\\varepsilon,R) = \\{ (\\rho,\\theta) \\mid \\varepsilon \\le \\rho \\le R, \\; -\\alpha/2 \\le \\theta \\le \\alpha/2 \\}$, where $\\rho$ is the radial coordinate, $\\theta$ is the angular coordinate, $\\varepsilon$ is the inner radius, $R$ is the outer radius, and $\\alpha$ is the total aperture angle. The differential area element in polar coordinates is $\\mathrm{d}A = \\rho\\,\\mathrm{d}\\rho\\,\\mathrm{d}\\theta$.\n\nSubstituting the area element into the integral expression, we have:\n$$\nI(\\alpha,\\varepsilon,R) = \\int_{-\\alpha/2}^{\\alpha/2} \\int_{\\varepsilon}^{R} \\frac{1}{\\rho} (\\rho\\,\\mathrm{d}\\rho\\,\\mathrm{d}\\theta)\n$$\nThe term $\\rho$ from the area element cancels the singular term $1/\\rho$ in the integrand, a process known as singularity cancellation that is a key feature of this type of integral in polar coordinates. The integral simplifies to:\n$$\nI(\\alpha,\\varepsilon,R) = \\int_{-\\alpha/2}^{\\alpha/2} \\int_{\\varepsilon}^{R} 1\\,\\mathrm{d}\\rho\\,\\mathrm{d}\\theta\n$$\nThe integrand is now a constant, and the integral is separable with respect to the variables $\\rho$ and $\\theta$. We can evaluate the integrals independently:\n$$\n\\int_{-\\alpha/2}^{\\alpha/2} \\mathrm{d}\\theta = \\left[\\theta\\right]_{-\\alpha/2}^{\\alpha/2} = \\frac{\\alpha}{2} - \\left(-\\frac{\\alpha}{2}\\right) = \\alpha\n$$\n$$\n\\int_{\\varepsilon}^{R} \\mathrm{d}\\rho = \\left[\\rho\\right]_{\\varepsilon}^{R} = R - \\varepsilon\n$$\nMultiplying these results yields the exact, closed-form expression for the integral:\n$$\nI(\\alpha,\\varepsilon,R) = \\alpha(R - \\varepsilon)\n$$\nSince $\\alpha$ is in radians (dimensionless) and both $R$ and $\\varepsilon$ are in meters ($m$), the unit of $I$ is meters, as required.\n\n### 2. Analysis of Stratified Sampling Schemes\n\nThe fundamental principle of Monte Carlo integration is to estimate an integral $I = \\int_{\\mathcal{D}} f(x) \\, \\mathrm{d}A$ by the random variable $\\hat{I} = A_{\\text{tot}} \\cdot \\bar{f}$, where $\\bar{f}$ is the average of $f(x_i)$ evaluated at $N$ points $x_i$ sampled uniformly from the domain $\\mathcal{D}$ of total area $A_{\\text{tot}}$. The estimator is unbiased, as its expected value is $E[\\hat{I}] = I$.\n\nStratified sampling partitions the domain $\\mathcal{D}$ into $K$ disjoint strata $\\mathcal{D}_k$ of area $A_k$. An estimate for the integral over each stratum, $I_k = \\int_{\\mathcal{D}_k} f(x)\\,\\mathrm{d}A$, is computed. The total estimate is the sum of stratum estimates, $\\hat{I}_{\\text{strat}} = \\sum_{k=1}^K \\hat{I}_k$. A properly weighted estimator remains unbiased. The deterministic calculation of the expectation of such an estimator involves using the true mean of the function within each stratum, $\\bar{f}_k = I_k/A_k$.\n\n**Correct Stratified Scheme (Equal-Area Strata)**\nThis scheme partitions the domain into $K$ strata of equal area. Since the area of a stratum between radii $\\rho_{k-1}$ and $\\rho_k$ is $A_k = \\frac{\\alpha}{2}(\\rho_k^2 - \\rho_{k-1}^2)$, making all $A_k$ equal to $A_{\\text{tot}}/K$ implies $\\rho_k^2 - \\rho_{k-1}^2$ must be constant. This leads to the radial boundaries:\n$$\n\\rho_k = \\sqrt{\\varepsilon^2 + \\frac{k}{K}(R^2 - \\varepsilon^2)} \\quad \\text{for } k=0, 1, \\dots, K\n$$\nThe correct stratified estimator weights the contribution from each stratum by its area. The expected value of this estimator (or its deterministic equivalent) is:\n$$\nE[\\hat{I}_{\\text{strat-correct}}] = \\sum_{k=1}^K I_k = \\sum_{k=1}^K \\iint_{\\mathcal{D}_k} f(\\rho,\\theta)\\,\\mathrm{d}A = I(\\alpha,\\varepsilon,R)\n$$\nSince its expectation equals the true value of the integral, this estimator is unbiased. Its bias is exactly $0$.\n\n**Incorrect Stratified Scheme (Equal-Width Radial Bins, Equal Weights)**\nThis scheme partitions the radial interval $[\\varepsilon, R]$ into $K$ bins of equal width, $\\Delta\\rho = (R-\\varepsilon)/K$, with boundaries $\\rho_k = \\varepsilon + k \\cdot \\Delta\\rho$. The resulting stratum areas $A_k = \\frac{\\alpha}{2}(\\rho_k^2 - \\rho_{k-1}^2)$ are not equal; they increase with $k$.\n\nThe specified incorrect estimator takes the form:\n$$\n\\hat{I}_{\\text{strat-wrong}} = A_{\\text{tot}} \\left( \\frac{1}{K} \\sum_{k=1}^K \\bar{f}_k \\right)\n$$\nwhere $\\bar{f}_k = I_k/A_k$ is the true mean of the function $f$ in the $k$-th stratum. This estimator is biased because it computes an unweighted arithmetic mean of the per-stratum function averages, $\\bar{f}_k$. The correct total integral is the sum of the stratum integrals, $I = \\sum I_k = \\sum A_k \\bar{f}_k$. The corresponding true average of $f$ over the whole domain is $\\bar{f} = I/A_{\\text{tot}} = \\sum (A_k/A_{\\text{tot}}) \\bar{f}_k$, which is a weighted average. By using equal weights ($1/K$) instead of the correct area-proportional weights ($A_k/A_{\\text{tot}}$), the estimator misrepresents the function's overall average, leading to bias. This bias is particularly severe near the singularity (small $\\rho$), where $f = 1/\\rho$ is large but the stratum area is small. The incorrect scheme gives the large function average in this small region the same weight as the small function average in the largest region, causing a systematic overestimation.\n\n### 3. Bias Derivation for the Incorrect Scheme\n\nThe bias of an estimator $\\hat{I}$ is defined as $B = E[\\hat{I}] - I$. For the incorrect stratified estimator, we first compute its expected value, $E[\\hat{I}_{\\text{strat-wrong}}]$.\n\nThe mean value of $f(\\rho,\\theta) = 1/\\rho$ in the $k$-th stratum $\\mathcal{D}_k$ (defined by radii $\\rho_{k-1}$ and $\\rho_k$) is:\n$$\n\\bar{f}_k = \\frac{\\int_{\\mathcal{D}_k} f(\\rho,\\theta)\\,\\mathrm{d}A}{\\int_{\\mathcal{D}_k} \\mathrm{d}A} = \\frac{I_k}{A_k} = \\frac{\\alpha(\\rho_k - \\rho_{k-1})}{\\frac{\\alpha}{2}(\\rho_k^2 - \\rho_{k-1}^2)} = \\frac{2(\\rho_k - \\rho_{k-1})}{(\\rho_k - \\rho_{k-1})(\\rho_k + \\rho_{k-1})} = \\frac{2}{\\rho_k + \\rho_{k-1}}\n$$\nThe boundaries for the equal-width bins are $\\rho_k = \\varepsilon + k \\frac{R-\\varepsilon}{K}$ for $k=0, \\dots, K$.\n\nThe expected value of the incorrect estimator is:\n$$\nE[\\hat{I}_{\\text{strat-wrong}}] = A_{\\text{tot}} \\left( \\frac{1}{K} \\sum_{k=1}^K \\bar{f}_k \\right) = \\frac{A_{\\text{tot}}}{K} \\sum_{k=1}^K \\frac{2}{\\rho_k + \\rho_{k-1}}\n$$\nSubstituting $A_{\\text{tot}} = \\frac{\\alpha}{2}(R^2 - \\varepsilon^2)$, we get:\n$$\nE[\\hat{I}_{\\text{strat-wrong}}] = \\frac{\\alpha(R^2 - \\varepsilon^2)}{2K} \\sum_{k=1}^K \\frac{2}{\\varepsilon + k\\frac{R-\\varepsilon}{K} + \\varepsilon + (k-1)\\frac{R-\\varepsilon}{K}} = \\frac{\\alpha(R^2 - \\varepsilon^2)}{K} \\sum_{k=1}^K \\frac{1}{2\\varepsilon + (2k-1)\\frac{R-\\varepsilon}{K}}\n$$\nThe bias is the difference between this expected value and the true integral value $I = \\alpha(R - \\varepsilon)$:\n$$\nB_{\\text{strat-wrong}} = E[\\hat{I}_{\\text{strat-wrong}}] - I = \\left( \\frac{\\alpha(R^2 - \\varepsilon^2)}{K} \\sum_{k=1}^K \\frac{1}{2\\varepsilon + (2k-1)\\frac{R-\\varepsilon}{K}} \\right) - \\alpha(R - \\varepsilon)\n$$\nThis expression reveals the bias is a function of all parameters $\\alpha, \\varepsilon, R, K$. The summation term, heavily influenced by the first few terms where $k$ is small, quantifies how the incorrect averaging over-weights the contribution from the singular region near $\\rho = \\varepsilon$.\n\n### 4. Deterministic Bias Calculation\n\nThe program must calculate the bias for three estimators.\n1.  **Uniform-area MC estimator**: As established, a standard MC estimator with samples drawn uniformly from the area is unbiased. Thus, $E[\\hat{I}_{\\text{uniform}}] = I$, and its bias is $B_{\\text{uniform}} = 0$.\n2.  **Correct stratified estimator**: As established, a properly weighted stratified sampling scheme is unbiased, regardless of the stratification strategy (equal-area or otherwise). Thus, $E[\\hat{I}_{\\text{strat-correct}}] = I$, and its bias is $B_{\\text{strat-correct}} = 0$.\n3.  **Incorrect stratified estimator**: The bias $B_{\\text{strat-wrong}}$ is calculated using the formula derived in the previous section.\n\nThe implementation will therefore compute $[0.0, 0.0, B_{\\text{strat-wrong}}]$ for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deterministically calculating the bias of three\n    Monte Carlo estimators for a singular integral in computational electromagnetics.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, epsilon, R, K)\n        (2 * np.pi, 1e-3, 1.0, 8),\n        (np.pi, 1e-6, 1.0, 16),\n        (np.pi / 4, 1e-8, 0.2, 12),\n        (2 * np.pi, 0.5, 1.0, 4),\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, epsilon, R, K = case\n\n        # 1. Uniform-area MC estimator bias\n        # The standard uniform-area MC estimator is unbiased by construction.\n        # Its expected value is the true integral value.\n        bias_uniform = 0.0\n\n        # 2. Correct stratified estimator bias\n        # A correctly weighted stratified sampling scheme is also unbiased by construction.\n        # Its expected value is the sum of the exact integrals over the strata.\n        bias_strat_correct = 0.0\n\n        # 3. Incorrect stratified estimator bias\n        \n        # Calculate the exact value of the integral\n        I_exact = alpha * (R - epsilon)\n\n        # Calculate the total area of the integration domain\n        A_tot = (alpha / 2.0) * (R**2 - epsilon**2)\n\n        # Calculate the expected value of the incorrect estimator\n        # This estimator uses equal-width radial bins and unweighted averaging of\n        # per-stratum function means.\n        \n        delta_rho = (R - epsilon) / K\n        sum_f_bar = 0.0\n        for k in range(1, K + 1):\n            rho_k_minus_1 = epsilon + (k - 1) * delta_rho\n            rho_k = epsilon + k * delta_rho\n            \n            # The mean value of f=1/rho in the k-th stratum is 2 / (rho_k + rho_{k-1})\n            f_bar_k = 2.0 / (rho_k + rho_k_minus_1)\n            sum_f_bar += f_bar_k\n            \n        # Expected value of the estimator\n        E_wrong = (A_tot / K) * sum_f_bar\n        \n        # Bias of the incorrect estimator\n        bias_strat_wrong = E_wrong - I_exact\n\n        results.append([bias_uniform, bias_strat_correct, bias_strat_wrong])\n\n    # Format the final output string according to the problem specification.\n    # The output should be a single line representing a list of lists.\n    outer_parts = []\n    for sublist in results:\n        # Use a high-precision format for the floating point numbers\n        inner_parts = [f\"{x:.15e}\" for x in sublist]\n        outer_parts.append(f\"[{','.join(inner_parts)}]\")\n    final_string = f\"[{','.join(outer_parts)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond simple forward analysis, a key task in engineering is design optimization. When designs are subject to manufacturing or material uncertainties, this involves optimizing an expectation, which requires computing gradients of statistical averages. This advanced practice explores two cornerstone techniques for this task—the pathwise and score-function estimators—in a practical antenna optimization context. By implementing and comparing both, you will discover their distinct performance characteristics and understand the trade-offs involved in choosing a gradient estimation strategy for stochastic problems. ",
            "id": "3332261",
            "problem": "Consider a simplified computational electromagnetics setting in which a single-resonance two-port antenna behaves as a band-pass system with power transmission characterized by a Lorentzian response. The objective functional is defined as the band-integrated transmitted power \n$$\nJ(\\theta) \\equiv \\int_{\\omega_{\\min}}^{\\omega_{\\max}} |S_{21}(\\omega; \\theta, \\epsilon_{\\mathrm{eff}})|^2 \\, d\\omega,\n$$\nwhere $\\theta \\in \\mathbb{R}$ is a scalar shape parameter and $\\epsilon_{\\mathrm{eff}}$ is an effective permittivity random variable modeling material uncertainty. All quantities in this problem are dimensionless to avoid unit conversion issues. The band is specified by fixed finite limits $\\omega_{\\min}$ and $\\omega_{\\max}$, and angular frequency $\\omega$ is dimensionless. The magnitude-squared transmission is modeled by a single-pole Lorentzian\n$$\n|S_{21}(\\omega; \\theta, \\epsilon_{\\mathrm{eff}})|^2 = \\frac{\\Gamma^2}{(\\omega - \\omega_0(\\theta,\\epsilon_{\\mathrm{eff}}))^2 + \\Gamma^2},\n$$\nwith fixed bandwidth parameter $\\Gamma > 0$ and resonance frequency\n$$\n\\omega_0(\\theta,\\epsilon_{\\mathrm{eff}}) = \\frac{\\pi}{L(\\theta)\\sqrt{\\epsilon_{\\mathrm{eff}}}},\n$$\nwhere the resonant length depends on the shape parameter as\n$$\nL(\\theta) = L_0 \\left(1 + \\beta \\theta\\right),\n$$\nwith fixed $L_0 > 0$ and $\\beta$ such that $1 + \\beta \\theta > 0$ for all tested $\\theta$. The effective permittivity is modeled by a field-filling mixture\n$$\n\\epsilon_{\\mathrm{eff}}(\\theta,\\xi) = \\bigl(1 - F(\\theta)\\bigr)\\,\\epsilon_{\\mathrm{air}} + F(\\theta)\\,\\epsilon_{\\mathrm{mat}},\n$$\nwhere the filling factor $F(\\theta) = F_0 + \\kappa \\theta$ satisfies $0 < F(\\theta) < 1$ in all tested cases, $\\epsilon_{\\mathrm{air}}$ is fixed, and $\\epsilon_{\\mathrm{mat}}$ is a scalar random permittivity with distribution\n$$\n\\epsilon_{\\mathrm{mat}} \\sim \\mathcal{N}(\\mu, \\sigma^2).\n$$\nThe randomness can be represented by a standard normal variable $\\xi \\sim \\mathcal{N}(0,1)$ via the reparameterization $\\epsilon_{\\mathrm{mat}} = \\mu + \\sigma \\xi$. Therefore, the induced distribution of $\\epsilon_{\\mathrm{eff}}$ is Gaussian with mean and standard deviation that depend on $\\theta$ through $F(\\theta)$.\n\nYour task is to derive from first principles two unbiased Monte Carlo gradient estimators of $\\nabla_{\\theta} \\mathbb{E}[J(\\theta)]$:\n- A pathwise (reparameterization) gradient estimator that differentiates $J(\\theta,\\epsilon_{\\mathrm{eff}}(\\theta,\\xi))$ with respect to $\\theta$ inside the expectation over $\\xi$.\n- A score-function (likelihood ratio) gradient estimator that treats $\\epsilon_{\\mathrm{eff}}$ as a draw from a $\\theta$-dependent density and uses the identity for the derivative of an expectation with respect to a distribution parameter, taking into account any explicit dependence of $J$ on $\\theta$.\n\nYou must start from the definitions of scattering parameters and the resonance model, the definition of expectation over a parameterized probability density, and the rules of differentiation under the integral. Do not assume or quote any gradient estimator formula as given; derive them from these bases. Then, implement both gradient estimators and empirically compare their single-sample estimator variances by Monte Carlo in the following discrete approximation setting:\n- Use a uniform grid of $N_{\\omega}$ points over $[\\omega_{\\min}, \\omega_{\\max}]$ with spacing $\\Delta \\omega$ to approximate the integral $J(\\theta)$ by a Riemann sum.\n- For the pathwise estimator, use the reparameterization by $\\xi$ and compute the derivative of $J$ with respect to $\\theta$ by differentiating the Lorentzian integrand with respect to $\\omega_0$ and then using the chain rule for $\\omega_0(\\theta,\\epsilon_{\\mathrm{eff}}(\\theta,\\xi))$.\n- For the score-function estimator, treat $\\epsilon_{\\mathrm{eff}}$ as a Gaussian with mean and variance that depend on $\\theta$ and use the identity for a parameterized expectation to write $\\nabla_{\\theta} \\mathbb{E}[J(\\theta)]$ as the sum of an explicit derivative term and a likelihood ratio term involving the derivative of the log-density.\n\nFor numerical evaluation, use the following fixed constants for all test cases:\n- $\\omega_{\\min} = 0$, $\\omega_{\\max} = 10$, $N_{\\omega} = 501$, and $\\Gamma = 0.15$.\n- $L_0 = 1$, $\\beta = 0.2$, $F_0 = 0.4$, $\\kappa = 0.3$, and $\\epsilon_{\\mathrm{air}} = 1$.\n- $\\mu = 3$, $\\sigma = 0.3$.\n\nImplement a program that, for each test case below, draws independent and identically distributed samples from the specified randomness, computes the pathwise and score-function gradient estimators for each sample, and then reports:\n- The sample mean of the pathwise gradient estimator,\n- The sample mean of the score-function gradient estimator,\n- The sample variance of the pathwise gradient estimator,\n- The sample variance of the score-function gradient estimator,\n- The ratio of the score-function variance to the pathwise variance.\n\nUse the following test suite, which probes a nominal case and two boundary-leaning cases for the filling factor while maintaining $L(\\theta) > 0$ and $0 < F(\\theta) < 1$:\n1. $(\\theta, N, \\text{seed}) = (0.0, 8000, 12345)$,\n2. $(\\theta, N, \\text{seed}) = (-1.1, 8000, 54321)$,\n3. $(\\theta, N, \\text{seed}) = (1.5, 8000, 2024)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of three sublists, one per test case, each sublist containing the five floats described above, rounded to six decimal places, with no spaces. For example, the output format must look exactly like\n\"[[m1p,m1s,v1p,v1s,r1],[m2p,m2s,v2p,v2s,r2],[m3p,m3s,v3p,v3s,r3]]\".",
            "solution": "The problem posed is to derive and implement two distinct Monte Carlo estimators for the gradient of an expected value, $\\nabla_{\\theta} \\mathbb{E}[J(\\theta)]$, in the context of a simplified model of an antenna. The problem is scientifically grounded, mathematically well-posed, and all necessary parameters and functional forms are provided. The underlying physical model, a Lorentzian response, is a standard approximation for resonant systems. The modeling of material uncertainty via a Gaussian random variable is a conventional approach in uncertainty quantification. While the Gaussian distribution has support on the entire real line, including non-physical negative values for permittivity, the parameters chosen place the mean sufficiently far from zero (many standard deviations away) that the probability of sampling a non-physical value is computationally negligible. Therefore, the problem is deemed valid and a solution can be constructed.\n\nFirst, we formalize the dependencies. The objective functional is $J(\\theta, \\xi)$, where the dependence on the standard normal random variable $\\xi \\sim \\mathcal{N}(0,1)$ arises through the chain of relations:\n$$ \\epsilon_{\\mathrm{mat}}(\\xi) = \\mu + \\sigma \\xi $$\n$$ \\epsilon_{\\mathrm{eff}}(\\theta, \\xi) = \\bigl(1 - F(\\theta)\\bigr)\\,\\epsilon_{\\mathrm{air}} + F(\\theta)\\,\\epsilon_{\\mathrm{mat}}(\\xi) $$\n$$ \\omega_0(\\theta, \\xi) = \\frac{\\pi}{L(\\theta)\\sqrt{\\epsilon_{\\mathrm{eff}}(\\theta, \\xi)}} $$\n$$ |S_{21}(\\omega; \\theta, \\xi)|^2 = \\frac{\\Gamma^2}{(\\omega - \\omega_0(\\theta, \\xi))^2 + \\Gamma^2} $$\n$$ J(\\theta, \\xi) = \\int_{\\omega_{\\min}}^{\\omega_{\\max}} |S_{21}(\\omega; \\theta, \\xi)|^2 \\, d\\omega $$\nThe shape parameter $\\theta$ also influences the geometry via $L(\\theta) = L_0 (1 + \\beta \\theta)$ and the material composition via $F(\\theta) = F_0 + \\kappa \\theta$. Our goal is to compute $\\nabla_{\\theta}\\mathbb{E}_{\\xi}[J(\\theta, \\xi)]$.\n\n### Pathwise Gradient Estimator Derivation\nThe pathwise, or reparameterization, method applies when the source of randomness can be separated from the parameters of differentiation. Here, $\\xi \\sim \\mathcal{N}(0,1)$ is independent of $\\theta$. We can write the gradient of the expectation as the expectation of the gradient, provided the integrand is a continuously differentiable function of $\\theta$, which is true in this case.\n$$ \\nabla_{\\theta} \\mathbb{E}_{\\xi}[J(\\theta, \\xi)] = \\mathbb{E}_{\\xi}[\\nabla_{\\theta} J(\\theta, \\xi)] $$\nAn unbiased single-sample estimator is therefore $G_P(\\theta, \\xi) = \\nabla_{\\theta} J(\\theta, \\xi)$. We find this derivative by applying the Leibniz integral rule and the chain rule:\n$$ \\nabla_{\\theta} J(\\theta, \\xi) = \\int_{\\omega_{\\min}}^{\\omega_{\\max}} \\nabla_{\\theta} |S_{21}(\\omega; \\theta, \\xi)|^2 \\, d\\omega $$\nThe derivative of the integrand is:\n$$ \\nabla_{\\theta} |S_{21}|^2 = \\frac{\\partial |S_{21}|^2}{\\partial \\omega_0} \\frac{d\\omega_0}{d\\theta} $$\nThe first part of the product is:\n$$ \\frac{\\partial |S_{21}|^2}{\\partial \\omega_0} = \\frac{\\partial}{\\partial \\omega_0} \\left[ \\frac{\\Gamma^2}{(\\omega - \\omega_0)^2 + \\Gamma^2} \\right] = \\frac{2\\Gamma^2(\\omega - \\omega_0)}{[(\\omega - \\omega_0)^2 + \\Gamma^2]^2} $$\nThe total derivative of the resonant frequency $\\omega_0$ with respect to $\\theta$ includes all dependencies:\n$$ \\frac{d\\omega_0}{d\\theta} = \\frac{\\partial \\omega_0}{\\partial L} \\frac{dL}{d\\theta} + \\frac{\\partial \\omega_0}{\\partial \\epsilon_{\\mathrm{eff}}} \\frac{d\\epsilon_{\\mathrm{eff}}}{d\\theta} $$\nThe individual components are:\n$$ \\frac{dL}{d\\theta} = L_0\\beta $$\n$$ \\frac{dF}{d\\theta} = \\kappa \\implies \\frac{d\\epsilon_{\\mathrm{eff}}}{d\\theta} = (\\epsilon_{\\mathrm{mat}} - \\epsilon_{\\mathrm{air}})\\frac{dF}{d\\theta} = \\kappa(\\epsilon_{\\mathrm{mat}} - \\epsilon_{\\mathrm{air}}) $$\n$$ \\frac{\\partial \\omega_0}{\\partial L} = -\\frac{\\pi}{L^2\\sqrt{\\epsilon_{\\mathrm{eff}}}} = -\\frac{\\omega_0}{L} $$\n$$ \\frac{\\partial \\omega_0}{\\partial \\epsilon_{\\mathrm{eff}}} = \\frac{\\pi}{L}\\left(-\\frac{1}{2}\\epsilon_{\\mathrm{eff}}^{-3/2}\\right) = -\\frac{\\omega_0}{2\\epsilon_{\\mathrm{eff}}} $$\nCombining these gives the total derivative of $\\omega_0$:\n$$ \\frac{d\\omega_0}{d\\theta} = -\\frac{\\omega_0(\\theta,\\xi)}{L(\\theta)}(L_0\\beta) - \\frac{\\omega_0(\\theta,\\xi)}{2\\epsilon_{\\mathrm{eff}}(\\theta,\\xi)}\\kappa(\\epsilon_{\\mathrm{mat}}(\\xi) - \\epsilon_{\\mathrm{air}}) = -\\omega_0 \\left( \\frac{L_0 \\beta}{L(\\theta)} + \\frac{\\kappa(\\epsilon_{\\mathrm{mat}} - \\epsilon_{\\mathrm{air}})}{2\\epsilon_{\\mathrm{eff}}} \\right) $$\nThe pathwise gradient estimator is then the numerical integral of the product $\\frac{\\partial |S_{21}|^2}{\\partial \\omega_0} \\frac{d\\omega_0}{d\\theta}$.\n\n### Score-Function Gradient Estimator Derivation\nThe score-function, or likelihood ratio, method handles cases where the probability distribution of the random variable depends on the parameter of interest. Here, $\\epsilon_{\\mathrm{eff}} \\sim p(e; \\theta)$ has a $\\theta$-dependent distribution. The functional $J(\\theta, \\epsilon_{\\mathrm{eff}})$ also has an explicit dependence on $\\theta$ through $L(\\theta)$.\nWe start with $\\nabla_{\\theta} \\mathbb{E}[J] = \\nabla_{\\theta} \\int J(\\theta, e) p(e; \\theta) de$.\nUsing the product rule and the log-derivative identity $\\nabla_{\\theta}p = p \\nabla_{\\theta}\\log p$, we get:\n$$ \\nabla_{\\theta}\\mathbb{E}[J] = \\int \\left[ (\\nabla_{\\theta}J(\\theta, e))p(e;\\theta) + J(\\theta, e)(\\nabla_{\\theta}p(e;\\theta)) \\right] de = \\mathbb{E}\\left[ (\\nabla_{\\theta}J(\\theta, e))|_{e=\\text{const}} + J(\\theta, e)\\nabla_{\\theta}\\log p(e;\\theta) \\right] $$\nAn unbiased single-sample estimator is $G_S(\\theta, e) = (\\nabla_{\\theta}J(\\theta, e))|_{e=\\text{const}} + J(\\theta, e)\\nabla_{\\theta}\\log p(e;\\theta)$.\n\nThe first term, $(\\nabla_{\\theta}J)|_{\\text{explicit}}$, treats $\\epsilon_{\\mathrm{eff}}$ as fixed and differentiates only with respect to explicit appearances of $\\theta$ (i.e., via $L(\\theta)$):\n$$ (\\nabla_{\\theta}J)|_{\\text{explicit}} = \\int_{\\omega_{\\min}}^{\\omega_{\\max}} \\frac{\\partial |S_{21}|^2}{\\partial \\omega_0} \\left( \\frac{\\partial \\omega_0}{\\partial L} \\frac{dL}{d\\theta} \\right) d\\omega = \\int_{\\omega_{\\min}}^{\\omega_{\\max}} \\frac{\\partial |S_{21}|^2}{\\partial \\omega_0} \\left( -\\frac{\\omega_0}{L(\\theta)}L_0\\beta \\right) d\\omega $$\nThe second term contains the score, $\\nabla_{\\theta}\\log p(e;\\theta)$. The distribution of $\\epsilon_{\\mathrm{eff}}$ is Gaussian, $p(e;\\theta) = \\mathcal{N}(e; \\mu_{\\mathrm{eff}}(\\theta), \\sigma^2_{\\mathrm{eff}}(\\theta))$, where:\n$$ \\mu_{\\mathrm{eff}}(\\theta) = (1 - F(\\theta))\\epsilon_{\\mathrm{air}} + F(\\theta)\\mu $$\n$$ \\sigma^2_{\\mathrm{eff}}(\\theta) = (F(\\theta)\\sigma)^2 $$\nThe derivative of the log-density of a Gaussian is a standard result:\n$$ \\nabla_{\\theta}\\log p(e;\\theta) = \\frac{(e - \\mu_{\\mathrm{eff}})}{\\sigma_{\\mathrm{eff}}^2}\\frac{d\\mu_{\\mathrm{eff}}}{d\\theta} + \\frac{(e - \\mu_{\\mathrm{eff}})^2 - \\sigma_{\\mathrm{eff}}^2}{2(\\sigma_{\\mathrm{eff}}^2)^2}\\frac{d\\sigma_{\\mathrm{eff}}^2}{d\\theta} $$\nThe required derivatives of the moments are:\n$$ \\frac{d\\mu_{\\mathrm{eff}}}{d\\theta} = (\\mu - \\epsilon_{\\mathrm{air}})\\frac{dF}{d\\theta} = \\kappa(\\mu - \\epsilon_{\\mathrm{air}}) $$\n$$ \\frac{d\\sigma_{\\mathrm{eff}}^2}{d\\theta} = \\sigma^2 \\frac{d(F(\\theta)^2)}{d\\theta} = 2\\sigma^2 F(\\theta)\\frac{dF}{d\\theta} = 2\\kappa\\sigma^2 F(\\theta) $$\nThe score-function estimator is the sum of the explicit derivative term and the product of the functional $J$ and the score term, all evaluated for a given sample of $\\epsilon_{\\mathrm{eff}}$. For numerical evaluation, the integrals for $J$ and its derivatives are approximated by Riemann sums over a uniform frequency grid.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements pathwise and score-function gradient estimators for a\n    Monte Carlo simulation in computational electromagnetics, and compares their\n    variances.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 8000, 12345),\n        (-1.1, 8000, 54321),\n        (1.5, 8000, 2024),\n    ]\n    \n    # Fixed constants for all test cases\n    w_min = 0.0\n    w_max = 10.0\n    Nw = 501\n    Gamma = 0.15\n    L0 = 1.0\n    beta = 0.2\n    F0 = 0.4\n    kappa = 0.3\n    eps_air = 1.0\n    mu = 3.0\n    sigma = 0.3\n    \n    omega_grid = np.linspace(w_min, w_max, Nw)\n    delta_omega = (w_max - w_min) / (Nw - 1)\n\n    results = []\n    for theta, N_samples, seed in test_cases:\n        rng = np.random.default_rng(seed)\n        \n        pathwise_grads = np.zeros(N_samples)\n        score_grads = np.zeros(N_samples)\n\n        # Pre-compute theta-dependent parameters\n        F_theta = F0 + kappa * theta\n        L_theta = L0 * (1.0 + beta * theta)\n        \n        grad_L_theta = L0 * beta\n        \n        mu_eff = (1.0 - F_theta) * eps_air + F_theta * mu\n        sigma2_eff = (F_theta * sigma)**2\n        \n        grad_mu_eff = kappa * (mu - eps_air)\n        grad_sigma2_eff = 2.0 * kappa * sigma**2 * F_theta\n\n        for i in range(N_samples):\n            # 1. Sample randomness\n            xi = rng.normal()\n            eps_mat = mu + sigma * xi\n            eps_eff = (1.0 - F_theta) * eps_air + F_theta * eps_mat\n\n            # Model is not physically defined for eps_eff = 0.\n            # Probability is negligible, but we skip the sample for robustness.\n            if eps_eff = 0:\n                # To maintain N_samples, we could resample, but for simplicity\n                # and given the rarity, we can just let it result in nan and filter later\n                # or just live with a slightly smaller N_samples in extreme cases.\n                # For this implementation, we proceed, knowing it won't happen.\n                pass\n            \n            # 2. Compute shared quantities\n            w0 = np.pi / (L_theta * np.sqrt(eps_eff))\n            \n            w_diff = omega_grid - w0\n            denom = w_diff**2 + Gamma**2\n            \n            S21_sq_integrand = Gamma**2 / denom\n            \n            # Common derivative term d|S21|^2/dw0\n            d_S21_sq_d_w0 = (2.0 * Gamma**2 * w_diff) / (denom**2)\n            \n            # --- Pathwise Estimator Calculation ---\n            grad_eps_eff = kappa * (eps_mat - eps_air)\n            d_w0_d_L = -w0 / L_theta\n            d_w0_d_eps_eff = -w0 / (2.0 * eps_eff)\n            \n            # Total derivative of w0\n            grad_w0_pathwise = d_w0_d_L * grad_L_theta + d_w0_d_eps_eff * grad_eps_eff\n            \n            grad_J_integrand_pathwise = d_S21_sq_d_w0 * grad_w0_pathwise\n            grad_J_pathwise = np.sum(grad_J_integrand_pathwise) * delta_omega\n            pathwise_grads[i] = grad_J_pathwise\n            \n            # --- Score-Function Estimator Calculation ---\n            # Part 1: Explicit derivative of J w.r.t. theta\n            grad_w0_explicit = d_w0_d_L * grad_L_theta\n            grad_J_explicit_integrand = d_S21_sq_d_w0 * grad_w0_explicit\n            grad_J_explicit = np.sum(grad_J_explicit_integrand) * delta_omega\n            \n            # Part 2: Score term\n            J = np.sum(S21_sq_integrand) * delta_omega\n            \n            score_term1 = ((eps_eff - mu_eff) / sigma2_eff) * grad_mu_eff\n            score_term2 = (((eps_eff - mu_eff)**2 - sigma2_eff) / (2.0 * sigma2_eff**2)) * grad_sigma2_eff\n            score = score_term1 + score_term2\n            \n            grad_J_score = grad_J_explicit + J * score\n            score_grads[i] = grad_J_score\n\n        # Calculate statistics\n        mean_p = np.mean(pathwise_grads)\n        mean_s = np.mean(score_grads)\n        var_p = np.var(pathwise_grads, ddof=1)\n        var_s = np.var(score_grads, ddof=1)\n        \n        # Handle case where pathwise variance is zero to avoid division by zero\n        if var_p == 0:\n            ratio = np.inf if var_s > 0 else 1.0\n        else:\n            ratio = var_s / var_p\n\n        results.append([\n            round(mean_p, 6),\n            round(mean_s, 6),\n            round(var_p, 6),\n            round(var_s, 6),\n            round(ratio, 6)\n        ])\n\n    # Final print statement in the exact required format.\n    # Convert list of lists to string representation without spaces.\n    formatted_results = \",\".join([f\"[{','.join(map(str, sublist))}]\" for sublist in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n\n```"
        }
    ]
}