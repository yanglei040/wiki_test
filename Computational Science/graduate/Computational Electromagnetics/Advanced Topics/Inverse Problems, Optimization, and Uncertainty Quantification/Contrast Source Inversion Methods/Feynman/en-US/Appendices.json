{
    "hands_on_practices": [
        {
            "introduction": "Gradient-based optimization lies at the heart of many modern inversion techniques, including the Contrast Source Inversion (CSI) method. While the adjoint-state method provides an elegant and computationally efficient way to calculate the gradient of the objective function, its implementation can be complex and prone to error. This practice provides a crucial debugging and verification step: the Taylor test, which confirms that the analytically derived adjoint gradient matches a numerical finite-difference approximation, ensuring your implementation is correct before proceeding to full-scale inversion .",
            "id": "3295894",
            "problem": "Consider a one-dimensional frequency-domain scattering problem in a homogeneous background, discretized as a Contrast Source Inversion (CSI) setting. Let the total field $E(x)$ satisfy the Lippmann–Schwinger integral equation on a bounded interval $\\Omega = [0,L]$,\n$$\nE(x) \\;=\\; E_{\\mathrm{inc}}(x) \\;+\\; \\int_{\\Omega} G(x,x')\\, k_0^2\\, \\chi(x')\\, E(x')\\, dx',\n$$\nwhere $E_{\\mathrm{inc}}(x)$ is a given incident field, $\\chi(x)$ is the unknown (real-valued) contrast, $k_0$ is the background wavenumber, and $G(x,x')$ is the one-dimensional free-space Green’s function for the scalar Helmholtz equation,\n$$\nG(x,x') \\;=\\; \\frac{i}{2 k_0}\\, e^{i k_0 |x - x'|}.\n$$\nWe work in dimensionless units throughout.\n\nDiscretize $\\Omega$ with $N$ equispaced nodes $x_n$ with spacing $\\Delta x = L/(N-1)$. Denote $E \\in \\mathbb{C}^N$ as the vector of total fields $E_n \\approx E(x_n)$, and $\\chi \\in \\mathbb{R}^N$ as the vector of contrast samples $\\chi_n \\approx \\chi(x_n)$. Define the dense matrix $G \\in \\mathbb{C}^{N \\times N}$ with entries\n$$\nG_{mn} \\;=\\; \\Delta x \\,\\frac{i}{2 k_0}\\, e^{i k_0 |x_m - x_n|},\n$$\nand the diagonal matrix $V(\\chi) = k_0^2 \\,\\mathrm{diag}(\\chi) \\in \\mathbb{R}^{N \\times N}$. The discrete total field $E(\\chi)$ solves the linear system\n$$\nA(\\chi)\\, E(\\chi) \\;=\\; E_{\\mathrm{inc}}, \n\\qquad A(\\chi) \\;=\\; I \\;-\\; G\\,V(\\chi),\n$$\nwith $I$ the identity matrix and $E_{\\mathrm{inc}} \\in \\mathbb{C}^N$ the incident field sampled at $\\{x_n\\}$.\n\nLet $P \\in \\{0,1\\}^{M \\times N}$ be a selection operator that extracts $M$ interior samples of the total field. Given synthetic data $d \\in \\mathbb{C}^M$ produced by a chosen ground-truth contrast $\\chi_{\\mathrm{true}}$, define the data-misfit objective\n$$\nJ(\\chi) \\;=\\; \\frac{1}{2}\\, \\| P\\,E(\\chi) \\;-\\; d \\|_2^2.\n$$\nBy the adjoint-state method, introduce $\\lambda \\in \\mathbb{C}^N$ solving the adjoint linear system\n$$\nA(\\chi)^{H}\\, \\lambda \\;=\\; P^{H}\\,\\big(P\\,E(\\chi) \\;-\\; d\\big),\n$$\nwhere ${}^{H}$ denotes the conjugate transpose. The gradient of $J$ with respect to $\\chi$ is the real vector $g(\\chi) \\in \\mathbb{R}^N$ with components\n$$\ng_n(\\chi) \\;=\\; k_0^2\\, \\mathrm{Re}\\!\\left\\{ \\overline{\\left(G^{H}\\lambda\\right)_n}\\; E_n(\\chi) \\right\\}, \n\\qquad n = 1,\\dots,N,\n$$\nwhere the overline denotes complex conjugation. This gradient provides the first-order directional derivative of $J$ with respect to real-valued perturbations $\\delta\\chi$ via\n$$\n\\delta J \\;=\\; \\sum_{n=1}^{N} g_n(\\chi)\\, \\delta\\chi_n \\;+\\; \\mathcal{O}(\\|\\delta\\chi\\|^2).\n$$\n\nYour task is to write a complete program that:\n- Constructs the forward model as above for a specific test configuration.\n- Computes the adjoint-based gradient $g(\\chi)$.\n- Verifies numerically that this adjoint-based gradient matches finite-difference approximations of $J$ to first order in $\\delta\\chi$.\n\nUse the following test configuration and test suite, all in dimensionless units:\n- Domain length: $L = 1$.\n- Number of grid points: $N = 80$.\n- Wavenumber: $k_0 = 20\\pi$.\n- Incident field: $E_{\\mathrm{inc}}(x) = e^{i k_0 x}$ sampled at $\\{x_n\\}$.\n- Ground-truth contrast: a Gaussian bump centered at $x=L/2$,\n  $$\n  \\chi_{\\mathrm{true}}(x) \\;=\\; 0.5 \\,\\exp\\!\\left( -\\frac{(x - L/2)^2}{2\\sigma^2} \\right),\n  \\quad \\text{with } \\sigma = 0.15\\,L.\n  $$\n- Data: $d = P\\,E(\\chi_{\\mathrm{true}})$ with the same forward model and selection operator $P$.\n- Selection operator $P$: choose $M = 16$ equispaced interior grid indices (exclude the first and last four nodes at each boundary to avoid edge effects), and let $P$ extract those samples of $E$.\n- Test background contrasts:\n  - For Tests $1$ and $3$, use \n    $$\n    \\chi_0(x) \\;=\\; 0.1\\, \\sin\\!\\left( \\frac{2\\pi x}{L} \\right).\n    $$\n  - For Test $2$, use $\\chi_0(x) \\equiv 0$.\n- Direction vectors:\n  - For Test $1$, use a single-pixel perturbation, i.e., $\\delta\\chi = \\mathbf{e}_p$ with $p = \\lfloor N/3 \\rfloor$.\n  - For Test 2 and Test 3, use a fixed pseudorandom real vector $\\delta\\chi$ with unit $\\ell_2$-norm (seeded for reproducibility), supported on all $N$ entries.\n- Finite-difference step sizes: $\\varepsilon_1 = 1 \\times 10^{-3}$ and $\\varepsilon_2 = 5 \\times 10^{-4}$.\n\nDefine the following three tests and return booleans indicating whether the expected first-order consistency holds:\n- Test $1$ (forward-difference first-order scaling): With $\\chi_0$ as above and $\\delta\\chi = \\mathbf{e}_p$, define\n  $$\n  \\mathrm{FD}(\\varepsilon) \\;=\\; \\frac{J(\\chi_0 + \\varepsilon\\,\\delta\\chi) \\;-\\; J(\\chi_0)}{\\varepsilon}, \n  \\qquad \\mathrm{AD} \\;=\\; g(\\chi_0)^{T} \\delta\\chi,\n  $$\n  and the mismatch $R(\\varepsilon) = \\big| \\mathrm{FD}(\\varepsilon) - \\mathrm{AD} \\big|$. Verify that $R(\\varepsilon_2)/R(\\varepsilon_1)$ lies in $[0.3,\\,0.7]$.\n- Test $2$ (central-difference consistency at zero contrast): With $\\chi_0 \\equiv 0$ and a unit-norm random $\\delta\\chi$, define\n  $$\n  \\mathrm{CD}(\\varepsilon) \\;=\\; \\frac{J(\\chi_0 + \\varepsilon\\,\\delta\\chi) \\;-\\; J(\\chi_0 - \\varepsilon\\,\\delta\\chi)}{2\\varepsilon},\n  \\qquad \\mathrm{AD} \\;=\\; g(\\chi_0)^{T} \\delta\\chi,\n  $$\n  and the mismatch $C(\\varepsilon) = \\big| \\mathrm{CD}(\\varepsilon) - \\mathrm{AD} \\big|$. Verify $C(10^{-4}) \\le 10^{-6}$.\n- Test $3$ (central-difference second-order scaling): With $\\chi_0$ as in Test $1$ and a unit-norm random $\\delta\\chi$, verify that the ratio $C(\\varepsilon_2)/C(\\varepsilon_1)$ lies in $[0.2,\\,0.3]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each result must be a boolean. No other output is permitted.",
            "solution": "The user requests a numerical verification of the adjoint-state gradient for a one-dimensional contrast source inversion problem. The problem is well-posed, scientifically grounded, and provides all necessary information for a complete solution. The verification procedure, often called a Taylor test or gradient check, is a standard and essential step in the development of optimization-based inversion algorithms. It ensures that the analytically derived gradient correctly represents the local first-order behavior of the objective function.\n\nThe core of the problem lies in the Lippmann-Schwinger integral equation, which describes the total electric field $E(x)$ as the sum of an incident field $E_{\\mathrm{inc}}(x)$ and a scattered field generated by the contrast object $\\chi(x)$:\n$$\nE(x) \\;=\\; E_{\\mathrm{inc}}(x) \\;+\\; k_0^2 \\int_{\\Omega} G(x,x')\\, \\chi(x')\\, E(x')\\, dx'\n$$\nUpon discretization on a grid of $N$ points $\\{x_n\\}$, this equation transforms into a linear system for the vector of field samples $E \\in \\mathbb{C}^N$:\n$$\nA(\\chi)\\, E(\\chi) \\;=\\; E_{\\mathrm{inc}}\n$$\nwhere $A(\\chi) = I - G V(\\chi)$. Here, $I \\in \\mathbb{R}^{N \\times N}$ is the identity matrix, $G \\in \\mathbb{C}^{N \\times N}$ is the discretized Green's function operator, and $V(\\chi) \\in \\mathbb{R}^{N \\times N}$ is a diagonal matrix containing the contrast values $\\chi_n \\approx \\chi(x_n)$, scaled by $k_0^2$. Solving this system for $E(\\chi)$ constitutes the forward model.\n\nThe goal of the inversion is to find a contrast $\\chi$ that minimizes the data-misfit objective function $J(\\chi)$. This function quantifies the squared $\\ell_2$-norm difference between the measured data $d \\in \\mathbb{C}^M$ and the corresponding samples of the computed field $E(\\chi)$:\n$$\nJ(\\chi) \\;=\\; \\frac{1}{2}\\, \\| P\\,E(\\chi) \\;-\\; d \\|_2^2\n$$\nThe matrix $P \\in \\{0,1\\}^{M \\times N}$ is a selection operator that extracts the field values at the measurement locations.\n\nTo use gradient-based optimization methods, we need the gradient of $J$ with respect to $\\chi$. A direct computation would be prohibitively expensive, as it would require solving the forward problem once for each component of $\\chi$. The adjoint-state method provides an efficient alternative. By introducing an adjoint field $\\lambda \\in \\mathbb{C}^N$, which is the solution to the adjoint linear system\n$$\nA(\\chi)^{H}\\, \\lambda \\;=\\; P^{H}\\,\\big(P\\,E(\\chi) \\;-\\; d\\big),\n$$\nthe gradient vector $g(\\chi) \\in \\mathbb{R}^N$ can be computed with just one additional linear system solve. The components of the gradient are given by:\n$$\ng_n(\\chi) \\;=\\; k_0^2\\, \\mathrm{Re}\\!\\left\\{ \\overline{\\left(G^{H}\\lambda\\right)_n}\\; E_n(\\chi) \\right\\}\n$$\nThis gradient gives the directional derivative of $J$ in any direction $\\delta\\chi$ as $g(\\chi)^T \\delta\\chi$.\n\nTo verify the correctness of this adjoint-based gradient $g(\\chi)$, we compare its prediction of the change in $J$ with finite-difference approximations. The Taylor series expansion of the objective function around a point $\\chi_0$ in a direction $\\delta\\chi$ is:\n$$\nJ(\\chi_0 + \\varepsilon\\,\\delta\\chi) = J(\\chi_0) + \\varepsilon\\, g(\\chi_0)^T \\delta\\chi + \\mathcal{O}(\\varepsilon^2)\n$$\nFrom this, we can derive the forward-difference approximation for the directional derivative:\n$$\n\\frac{J(\\chi_0 + \\varepsilon\\,\\delta\\chi) - J(\\chi_0)}{\\varepsilon} = g(\\chi_0)^T \\delta\\chi + \\mathcal{O}(\\varepsilon)\n$$\nThe error in this approximation is first-order in $\\varepsilon$. Test $1$ verifies this by checking that halving the step size $\\varepsilon$ approximately halves the error, i.e., the ratio of errors is close to $0.5$.\n\nA more accurate approximation is the central difference:\n$$\n\\frac{J(\\chi_0 + \\varepsilon\\,\\delta\\chi) - J(\\chi_0 - \\varepsilon\\,\\delta\\chi)}{2\\varepsilon} = g(\\chi_0)^T \\delta\\chi + \\mathcal{O}(\\varepsilon^2)\n$$\nThe error for the central-difference approximation is second-order in $\\varepsilon$. Test $2$ verifies the accuracy of the gradient calculation by checking that the central-difference approximation is very close to the adjoint-based directional derivative for a small $\\varepsilon$. Test $3$ verifies the second-order convergence by checking that halving $\\varepsilon$ reduces the error by a factor of approximately $(\\frac{1}{2})^2 = 0.25$.\n\nThe implementation will proceed as follows:\n1.  Set up the computational grid and define physical constants ($L$, $N$, $k_0$).\n2.  Construct the discrete Green's matrix $G$ and the incident field vector $E_{\\mathrm{inc}}$.\n3.  Define the true contrast profile $\\chi_{\\mathrm{true}}$ and generate the synthetic data $d$ by solving the forward problem for $\\chi_{\\mathrm{true}}$ and selecting the required samples.\n4.  Implement functions to:\n    a. Solve the forward problem $A(\\chi)E=E_{\\mathrm{inc}}$ to obtain $E(\\chi)$.\n    b. Calculate the objective function $J(\\chi)$.\n    c. Solve the adjoint problem $A(\\chi)^H \\lambda = \\text{rhs}$ and subsequently calculate the gradient $g(\\chi)$.\n5.  Execute the three specified tests by comparing finite-difference approximations with the adjoint-based directional derivative $g(\\chi_0)^T \\delta\\chi$ for the given test contrasts $\\chi_0$ and perturbation directions $\\delta\\chi$.\n6.  Return the boolean outcomes of the three tests.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the contrast source inversion gradient verification problem.\n    \"\"\"\n    # 1. Define problem parameters and discretize the domain\n    L = 1.0\n    N = 80\n    k0 = 20.0 * np.pi\n    M = 16\n    \n    # Grid\n    x = np.linspace(0, L, N)\n    dx = L / (N - 1)\n    \n    # Incident field\n    E_inc = np.exp(1j * k0 * x)\n    \n    # Green's function matrix\n    x_m, x_n = np.meshgrid(x, x, indexing='ij')\n    G_mat = dx * (1j / (2 * k0)) * np.exp(1j * k0 * np.abs(x_m - x_n))\n\n    # Selection operator (indices)\n    # Exclude first and last 4 nodes. Select from indices 4 to N-5=75.\n    P_indices = np.round(np.linspace(4, N - 5, M)).astype(int)\n\n    # 2. Generate synthetic data from ground-truth contrast\n    sigma = 0.15 * L\n    chi_true = 0.5 * np.exp(-(x - L/2)**2 / (2 * sigma**2))\n\n    # Helper functions that capture the simulation context (G_mat, k0, E_inc, etc.)\n    def get_total_field(chi: np.ndarray) -> np.ndarray:\n        \"\"\"Solves the forward problem for the total field E(chi).\"\"\"\n        I = np.eye(N, dtype=np.complex128)\n        # V(chi) is diagonal, so G*V(chi) is G_mn * k0^2 * chi_n\n        # This is equivalent to scaling columns of G\n        A_chi = I - G_mat * (k0**2 * chi[None, :])\n        E = np.linalg.solve(A_chi, E_inc)\n        return E\n\n    E_true = get_total_field(chi_true)\n    d = E_true[P_indices]\n\n    # 3. Implement core computational functions\n    def get_objective(chi: np.ndarray) -> float:\n        \"\"\"Computes the data misfit objective J(chi).\"\"\"\n        E_chi = get_total_field(chi)\n        residual = E_chi[P_indices] - d\n        return 0.5 * np.linalg.norm(residual)**2\n\n    def get_gradient(chi: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the adjoint-based gradient g(chi).\"\"\"\n        E_chi = get_total_field(chi)\n        \n        # Adjoint RHS: P^H * (P*E(chi) - d)\n        adj_rhs = np.zeros(N, dtype=np.complex128)\n        residual = E_chi[P_indices] - d\n        adj_rhs[P_indices] = residual\n        \n        # Adjoint matrix: A^H = I - V(chi) * G^H\n        # V is real diagonal, so V^H = V\n        # Product V*G^H corresponds to scaling rows of G^H\n        A_chi_H = np.eye(N, dtype=np.complex128) - (k0**2 * chi[:, None]) * G_mat.conj().T\n        \n        lambda_vec = np.linalg.solve(A_chi_H, adj_rhs)\n        \n        # Gradient formula\n        GH_lambda = G_mat.conj().T @ lambda_vec\n        g = k0**2 * np.real(np.conj(GH_lambda) * E_chi)\n        return g\n\n    results = []\n    \n    # 4. Define test configurations\n    chi0_test1_3 = 0.1 * np.sin(2 * np.pi * x / L)\n    chi0_test2 = np.zeros(N)\n\n    p = N // 3\n    delta_chi_test1 = np.zeros(N)\n    delta_chi_test1[p] = 1.0\n\n    np.random.seed(0)\n    delta_chi_test2_3 = np.random.randn(N)\n    delta_chi_test2_3 /= np.linalg.norm(delta_chi_test2_3)\n\n    eps1 = 1e-3\n    eps2 = 5e-4\n\n    # 5. Execute tests\n    # --- Test 1: Forward-difference first-order scaling ---\n    g0_t1 = get_gradient(chi0_test1_3)\n    AD_t1 = g0_t1 @ delta_chi_test1\n    J0_t1 = get_objective(chi0_test1_3)\n    \n    J_p_eps1 = get_objective(chi0_test1_3 + eps1 * delta_chi_test1)\n    FD1 = (J_p_eps1 - J0_t1) / eps1\n    R1 = np.abs(FD1 - AD_t1)\n\n    J_p_eps2 = get_objective(chi0_test1_3 + eps2 * delta_chi_test1)\n    FD2 = (J_p_eps2 - J0_t1) / eps2\n    R2 = np.abs(FD2 - AD_t1)\n\n    ratio1 = R2 / R1 if R1 != 0 else 0\n    results.append(0.3 <= ratio1 <= 0.7)\n\n    # --- Test 2: Central-difference consistency at zero contrast ---\n    g0_t2 = get_gradient(chi0_test2)\n    AD_t2 = g0_t2 @ delta_chi_test2_3\n    eps_t2 = 1e-4\n    \n    J_p = get_objective(chi0_test2 + eps_t2 * delta_chi_test2_3)\n    J_m = get_objective(chi0_test2 - eps_t2 * delta_chi_test2_3)\n    CD_t2 = (J_p - J_m) / (2 * eps_t2)\n    C_eps = np.abs(CD_t2 - AD_t2)\n    \n    results.append(C_eps <= 1e-6)\n    \n    # --- Test 3: Central-difference second-order scaling ---\n    g0_t3 = get_gradient(chi0_test1_3) # Same chi0 as Test 1\n    AD_t3 = g0_t3 @ delta_chi_test2_3 # Same random delta_chi as Test 2\n    \n    Jp1 = get_objective(chi0_test1_3 + eps1 * delta_chi_test2_3)\n    Jm1 = get_objective(chi0_test1_3 - eps1 * delta_chi_test2_3)\n    CD1 = (Jp1 - Jm1) / (2 * eps1)\n    C1 = np.abs(CD1 - AD_t3)\n    \n    Jp2 = get_objective(chi0_test1_3 + eps2 * delta_chi_test2_3)\n    Jm2 = get_objective(chi0_test1_3 - eps2 * delta_chi_test2_3)\n    CD2 = (Jp2 - Jm2) / (2 * eps2)\n    C2 = np.abs(CD2 - AD_t3)\n\n    ratio3 = C2 / C1 if C1 != 0 else 0\n    results.append(0.2 <= ratio3 <= 0.3)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A successful numerical inversion must yield results that are not only mathematically optimal but also physically plausible. In electromagnetic imaging, a fundamental physical constraint for passive materials is that they cannot be a source of net energy, which implies that the imaginary part of the material's susceptibility, $\\text{Im}\\{\\chi\\}$, must be non-negative. This hands-on exercise guides you through deriving this condition from first principles and implementing a diagnostic tool to check for violations of passivity in a noisy, reconstructed image, a vital step in validating and regularizing inverse solutions .",
            "id": "3295880",
            "problem": "Consider a time-harmonic electromagnetic scattering setting under the time dependence convention $e^{-i \\omega t}$ in which a computational reconstruction produces an estimate of a scalar, isotropic electric susceptibility field $\\chi(\\mathbf{r})$ on a discrete grid. In a common formulation of contrast source inversion, the constitutive relation is $ \\mathbf{D}(\\mathbf{r}) = \\varepsilon_b \\left( 1 + \\chi(\\mathbf{r}) \\right) \\mathbf{E}(\\mathbf{r}) $, where $\\varepsilon_b$ is the real, positive background permittivity and $\\mathbf{E}(\\mathbf{r})$ is the complex phasor of the electric field. A material is passive if and only if it does not generate net electromagnetic energy. Under the above time convention, passivity implies a local material inequality $ \\operatorname{Im}\\{\\chi(\\mathbf{r})\\} \\ge 0 $ when the background is lossless, and a global dissipated power nonnegativity: the time-averaged total dissipated power in any finite region must be nonnegative.\n\nStarting from fundamental electromagnetic principles, including Maxwell’s equations and the Poynting theorem, derive a diagnostic that, given discrete samples $\\{\\chi_n\\}_{n=1}^N$ and $\\{E_n\\}_{n=1}^N$ on a grid of $N$ cells with volume $\\{\\Delta V_n\\}_{n=1}^N$, checks:\n- The local passivity condition $ \\operatorname{Im}\\{\\chi_n\\} \\ge 0 $ for all $n$.\n- The global dissipated power nonnegativity condition $ P_{\\mathrm{diss}} \\ge 0 $, where $ P_{\\mathrm{diss}} $ is the discrete approximation of the time-averaged dissipated power over the grid.\n\nYour derivation must begin from a valid base for computational electromagnetics: Maxwell’s equations and the Poynting theorem. Do not assume any shortcut formulas; derive the discrete diagnostic from those principles. Clearly state any assumptions that justify the diagnostic.\n\nThen, implement a program that applies this diagnostic to a set of synthetic “noisy reconstructions” $\\{\\widehat{\\chi}_n\\}$ obtained by perturbing a known ground-truth $\\{\\chi^{\\mathrm{true}}_n\\}$ with independent, identically distributed complex Gaussian noise $\\eta_n \\sim \\mathcal{N}(0,\\sigma^2) + i\\,\\mathcal{N}(0,\\sigma^2)$, i.e., $ \\widehat{\\chi}_n = \\chi^{\\mathrm{true}}_n + \\eta_n $. The diagnostic must flag violations of local passivity and of global dissipated power nonnegativity.\n\nAll physical quantities must be expressed in the International System of Units (SI). Frequencies must be given in hertz (for $f$) or radians per second (for $\\omega$). Phases must be specified in radians. Power must be reported in watts, rounded to $12$ decimal places.\n\nUse the following constants and assumptions:\n- Background relative permittivity $ \\varepsilon_{r,b} = 2.25 $, vacuum permittivity $ \\varepsilon_0 = 8.854187817 \\times 10^{-12} \\,\\mathrm{F/m} $, and background permittivity $ \\varepsilon_b = \\varepsilon_0 \\varepsilon_{r,b} $.\n- Operating frequency $ f = 3.0 \\times 10^{9} \\,\\mathrm{Hz} $, angular frequency $ \\omega = 2\\pi f $.\n- Uniform cell volume $ \\Delta V_n = 1.0 \\times 10^{-6} \\,\\mathrm{m}^3 $ for all $n$.\n\nYour program must implement the diagnostic and evaluate the following test suite. For each test case, construct $E_n$ as a scalar complex field using magnitudes and phases: $ E_n = |E_n| e^{i \\phi_n} $.\n\nTest suite:\n- Case A (nominal, low noise, typical field variation):\n  - Grid size $ N = 5 $.\n  - Ground-truth susceptibility real parts $ \\operatorname{Re}\\{\\chi^{\\mathrm{true}}_n\\} $: $ [0.10,\\,0.05,\\,0.00,\\,0.02,\\,0.08] $.\n  - Ground-truth susceptibility imaginary parts $ \\operatorname{Im}\\{\\chi^{\\mathrm{true}}_n\\} $: $ [0.020,\\,0.010,\\,0.030,\\,0.005,\\,0.015] $.\n  - Field magnitudes $ |E_n| $: $ [1.0,\\,2.0,\\,1.5,\\,0.0,\\,0.5] $.\n  - Field phases $ \\phi_n $ in radians: $ [0.0,\\,\\pi/4,\\,-\\pi/2,\\,0.0,\\,\\pi] $.\n  - Noise standard deviation $ \\sigma = 0.002 $.\n  - Pseudorandom seed $ s = 12345 $.\n- Case B (higher noise, varied field, likely violations):\n  - Grid size $ N = 5 $.\n  - Ground-truth susceptibility real parts $ \\operatorname{Re}\\{\\chi^{\\mathrm{true}}_n\\} $: $ [0.10,\\,0.05,\\,0.00,\\,0.02,\\,0.08] $.\n  - Ground-truth susceptibility imaginary parts $ \\operatorname{Im}\\{\\chi^{\\mathrm{true}}_n\\} $: $ [0.020,\\,0.010,\\,0.030,\\,0.005,\\,0.015] $.\n  - Field magnitudes $ |E_n| $: $ [0.3,\\,0.7,\\,1.2,\\,2.0,\\,1.0] $.\n  - Field phases $ \\phi_n $ in radians: $ [\\pi/3,\\,-\\pi/6,\\,\\pi/2,\\,\\pi/8,\\,-\\pi/5] $.\n  - Noise standard deviation $ \\sigma = 0.030 $.\n  - Pseudorandom seed $ s = 2025 $.\n- Case C (edge case with zero fields, moderate noise):\n  - Grid size $ N = 4 $.\n  - Ground-truth susceptibility real parts $ \\operatorname{Re}\\{\\chi^{\\mathrm{true}}_n\\} $: $ [0.0,\\,0.10,\\,-0.05,\\,0.20] $.\n  - Ground-truth susceptibility imaginary parts $ \\operatorname{Im}\\{\\chi^{\\mathrm{true}}_n\\} $: $ [0.0,\\,0.0,\\,0.0,\\,0.0] $.\n  - Field magnitudes $ |E_n| $: $ [0.0,\\,0.0,\\,0.0,\\,0.0] $.\n  - Field phases $ \\phi_n $ in radians: $ [0.0,\\,0.0,\\,0.0,\\,0.0] $.\n  - Noise standard deviation $ \\sigma = 0.010 $.\n  - Pseudorandom seed $ s = 7 $.\n\nFor each case, the program must:\n- Generate $ \\widehat{\\chi}_n $ from $ \\chi^{\\mathrm{true}}_n $ and the specified noise.\n- Compute booleans indicating whether all $ \\operatorname{Im}\\{\\widehat{\\chi}_n\\} \\ge 0 $ and whether the total dissipated power $ P_{\\mathrm{diss}} $ is nonnegative.\n- Count the number of indices $n$ for which $ \\operatorname{Im}\\{\\widehat{\\chi}_n\\} < 0 $.\n- Compute the total dissipated power $ P_{\\mathrm{diss}} $ in watts, rounded to $12$ decimal places.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing a Python-style list with one entry per test case. Each entry must itself be a list of the form $[\\text{all\\_imag\\_nonneg}, \\text{total\\_power\\_nonneg}, \\text{num\\_negative\\_pixels}, \\text{P\\_diss\\_W}]$, where the first two entries are booleans, the third is an integer, and the fourth is a float rounded to $12$ decimal places. For example, the printed output should look like $[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]]$ on a single line with no extra text.",
            "solution": "The problem requires the derivation and implementation of a diagnostic to verify the physical passivity of a reconstructed electric susceptibility field $\\chi(\\mathbf{r})$ on a discrete grid. The derivation must originate from Maxwell's equations and the Poynting theorem.\n\n### Derivation of the Passivity Diagnostic\n\nWe begin with the macroscopic Maxwell's equations for time-harmonic fields, assuming a time-dependence of $e^{-i\\omega t}$, in a region with no free magnetic charges and no external source currents:\n$$ \\nabla \\times \\mathbf{E} = i\\omega \\mathbf{B} \\quad (1) $$\n$$ \\nabla \\times \\mathbf{H} = -i\\omega \\mathbf{D} \\quad (2) $$\nHere, $\\mathbf{E}$ and $\\mathbf{H}$ are the complex phasors for the electric and magnetic fields, respectively, while $\\mathbf{D}$ and $\\mathbf{B}$ are the corresponding flux densities. The angular frequency is $\\omega$.\n\nThe problem specifies a non-magnetic medium, so the magnetic constitutive relation is $\\mathbf{B} = \\mu_0 \\mathbf{H}$, where $\\mu_0$ is the permeability of free space. The electric constitutive relation is given as $\\mathbf{D}(\\mathbf{r}) = \\varepsilon_b (1 + \\chi(\\mathbf{r})) \\mathbf{E}(\\mathbf{r})$, where $\\varepsilon_b$ is the real, positive permittivity of a lossless background medium and $\\chi(\\mathbf{r})$ is the scalar electric susceptibility contrast.\n\nWe can express the electric displacement field $\\mathbf{D}$ as the sum of the background displacement and the induced electric polarization $\\mathbf{P}$:\n$$ \\mathbf{D} = \\varepsilon_b \\mathbf{E} + \\varepsilon_b \\chi \\mathbf{E} = \\varepsilon_b \\mathbf{E} + \\mathbf{P} $$\nThus, the polarization vector is $\\mathbf{P} = \\varepsilon_b \\chi \\mathbf{E}$. The polarization gives rise to an equivalent volume current density, $\\mathbf{J}_{eq}$, which is the time derivative of the polarization in the time domain. In the frequency domain, this corresponds to:\n$$ \\mathbf{J}_{eq} = -i\\omega \\mathbf{P} = -i\\omega \\varepsilon_b \\chi \\mathbf{E} \\quad (3) $$\nThis current represents the response of the material contrast to the total electric field $\\mathbf{E}$.\n\nThe time-averaged power dissipated by the electromagnetic field within a volume is given by the work done by the field on the currents. The density of this power, $p_{diss}$, is given by $\\frac{1}{2}\\operatorname{Re}\\{\\mathbf{E} \\cdot \\mathbf{J}^*\\}$, where $\\mathbf{J}$ is the relevant current density. In our case, we are interested in the power dissipated specifically by the material contrast, which is represented by the equivalent current $\\mathbf{J}_{eq}$.\nTherefore, the time-averaged power density dissipated by the susceptibility contrast is:\n$$ p_{diss}(\\mathbf{r}) = \\frac{1}{2}\\operatorname{Re}\\{\\mathbf{E}(\\mathbf{r}) \\cdot \\mathbf{J}_{eq}(\\mathbf{r})^*\\} \\quad (4) $$\nWe take the complex conjugate of $\\mathbf{J}_{eq}$ from equation $(3)$. Since $\\omega$ and $\\varepsilon_b$ are real, we have:\n$$ \\mathbf{J}_{eq}^* = (-i\\omega \\varepsilon_b \\chi \\mathbf{E})^* = i\\omega \\varepsilon_b \\chi^* \\mathbf{E}^* $$\nSubstituting this into the expression for $p_{diss}$:\n$$ p_{diss} = \\frac{1}{2}\\operatorname{Re}\\{\\mathbf{E} \\cdot (i\\omega \\varepsilon_b \\chi^* \\mathbf{E}^*)\\} $$\nSince $\\chi$ is a scalar quantity, we can rearrange the terms. The dot product $\\mathbf{E} \\cdot \\mathbf{E}^*$ is equal to $|\\mathbf{E}|^2$, the squared magnitude of the electric field vector.\n$$ p_{diss} = \\frac{1}{2}\\operatorname{Re}\\{i\\omega \\varepsilon_b \\chi^* |\\mathbf{E}|^2\\} $$\nLet's express the complex susceptibility $\\chi$ in terms of its real and imaginary parts, $\\chi = \\operatorname{Re}\\{\\chi\\} + i\\operatorname{Im}\\{\\chi\\}$. Its conjugate is $\\chi^* = \\operatorname{Re}\\{\\chi\\} - i\\operatorname{Im}\\{\\chi\\}$.\n$$ p_{diss} = \\frac{1}{2}\\operatorname{Re}\\{i\\omega \\varepsilon_b (\\operatorname{Re}\\{\\chi\\} - i\\operatorname{Im}\\{\\chi\\}) |\\mathbf{E}|^2\\} $$\n$$ p_{diss} = \\frac{1}{2}\\operatorname{Re}\\{i\\omega \\varepsilon_b \\operatorname{Re}\\{\\chi\\} |\\mathbf{E}|^2 + \\omega \\varepsilon_b \\operatorname{Im}\\{\\chi\\} |\\mathbf{E}|^2\\} $$\nThe first term inside the curly braces is purely imaginary, while the second term is purely real. Taking the real part of the expression yields:\n$$ p_{diss}(\\mathbf{r}) = \\frac{1}{2}\\omega \\varepsilon_b \\operatorname{Im}\\{\\chi(\\mathbf{r})\\} |\\mathbf{E}(\\mathbf{r})|^2 \\quad (5) $$\nThis equation provides the fundamental relationship between the local material properties and the dissipated power density.\n\nA material is defined as passive if it does not generate net electromagnetic energy. This imposes two conditions:\n1.  **Local Passivity**: At any point $\\mathbf{r}$, the time-averaged dissipated power density must be non-negative, $p_{diss}(\\mathbf{r}) \\ge 0$. As the quantities $\\omega$, $\\varepsilon_b$, and $|\\mathbf{E}(\\mathbf{r})|^2$ are all non-negative, equation $(5)$ implies that local passivity is equivalent to the condition:\n    $$ \\operatorname{Im}\\{\\chi(\\mathbf{r})\\} \\ge 0 $$\n    This forms the basis for the first part of our diagnostic.\n\n2.  **Global Passivity**: The total time-averaged power dissipated within any finite volume $V$ must be non-negative. This total power, $P_{diss}$, is the integral of the power density over the volume:\n    $$ P_{diss} = \\int_V p_{diss}(\\mathbf{r}) \\, dV = \\int_V \\frac{1}{2}\\omega \\varepsilon_b \\operatorname{Im}\\{\\chi(\\mathbf{r})\\} |\\mathbf{E}(\\mathbf{r})|^2 \\, dV \\ge 0 $$\n\n### Discretization for the Computational Diagnostic\n\nThe problem provides the reconstructed susceptibility $\\{\\widehat{\\chi}_n\\}_{n=1}^N$ and the electric field $\\{E_n\\}_{n=1}^N$ on a discrete grid of $N$ cells, each with volume $\\Delta V_n$. To formulate the diagnostic, we assume that the susceptibility and electric field are piecewise constant within each cell $n$, taking the values $\\widehat{\\chi}_n$ and $E_n$. The problem also specifies that $E_n$ is a scalar complex field, which is a common simplification in 2D models where only a single vector component is non-zero (e.g., $E_z$). We thus make the assumption that $|\\mathbf{E}(\\mathbf{r})|^2$ can be replaced by $|E_n|^2$ within cell $n$.\n\nThe discrete diagnostic follows directly:\n-   **Local Passivity Check**: For each cell $n \\in \\{1, \\dots, N\\}$, we check if the local passivity criterion is met:\n    $$ \\operatorname{Im}\\{\\widehat{\\chi}_n\\} \\ge 0 $$\n    The diagnostic will count the number of cells $n$ where this condition is violated (i.e., $\\operatorname{Im}\\{\\widehat{\\chi}_n\\} < 0$).\n\n-   **Global Dissipated Power Check**: The integral for the total dissipated power $P_{diss}$ is approximated by a sum over all cells:\n    $$ P_{diss} = \\sum_{n=1}^{N} \\left( \\frac{1}{2}\\omega \\varepsilon_b \\operatorname{Im}\\{\\widehat{\\chi}_n\\} |E_n|^2 \\right) \\Delta V_n $$\n    The diagnostic will compute this sum and check if the global passivity criterion $P_{diss} \\ge 0$ is satisfied.\n\nThese two checks constitute the complete diagnostic to be implemented. The implementation will use the provided noisy susceptibility values $\\widehat{\\chi}_n$ in place of the analytical $\\chi(\\mathbf{r})$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies a passivity diagnostic to noisy susceptibility reconstructions.\n    \"\"\"\n\n    # --- Constants ---\n    EPSILON_0 = 8.854187817e-12  # Vacuum permittivity in F/m\n    EPSILON_R_B = 2.25            # Background relative permittivity\n    FREQ = 3.0e9                  # Operating frequency in Hz\n    DELTA_V = 1.0e-6              # Uniform cell volume in m^3\n\n    # --- Derived Constants ---\n    EPSILON_B = EPSILON_0 * EPSILON_R_B  # Background permittivity\n    OMEGA = 2 * np.pi * FREQ           # Angular frequency in rad/s\n\n    # Prefactor for dissipated power calculation\n    power_prefactor = 0.5 * OMEGA * EPSILON_B * DELTA_V\n\n    # --- Test Suite ---\n    test_cases = [\n        {\n            \"N\": 5,\n            \"re_chi_true\": np.array([0.10, 0.05, 0.00, 0.02, 0.08]),\n            \"im_chi_true\": np.array([0.020, 0.010, 0.030, 0.005, 0.015]),\n            \"E_mag\": np.array([1.0, 2.0, 1.5, 0.0, 0.5]),\n            \"E_phase\": np.array([0.0, np.pi/4, -np.pi/2, 0.0, np.pi]),\n            \"sigma\": 0.002,\n            \"seed\": 12345,\n        },\n        {\n            \"N\": 5,\n            \"re_chi_true\": np.array([0.10, 0.05, 0.00, 0.02, 0.08]),\n            \"im_chi_true\": np.array([0.020, 0.010, 0.030, 0.005, 0.015]),\n            \"E_mag\": np.array([0.3, 0.7, 1.2, 2.0, 1.0]),\n            \"E_phase\": np.array([np.pi/3, -np.pi/6, np.pi/2, np.pi/8, -np.pi/5]),\n            \"sigma\": 0.030,\n            \"seed\": 2025,\n        },\n        {\n            \"N\": 4,\n            \"re_chi_true\": np.array([0.0, 0.10, -0.05, 0.20]),\n            \"im_chi_true\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"E_mag\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"E_phase\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"sigma\": 0.010,\n            \"seed\": 7,\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        N = case[\"N\"]\n        sigma = case[\"sigma\"]\n        seed = case[\"seed\"]\n\n        # --- Setup for the case ---\n        # Initialize random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # 1. Construct ground-truth and electric field arrays\n        chi_true = case[\"re_chi_true\"] + 1j * case[\"im_chi_true\"]\n        E_field = case[\"E_mag\"] * np.exp(1j * case[\"E_phase\"])\n\n        # 2. Generate noisy susceptibility\n        noise_real = rng.normal(loc=0.0, scale=sigma, size=N)\n        noise_imag = rng.normal(loc=0.0, scale=sigma, size=N)\n        eta = noise_real + 1j * noise_imag\n        chi_hat = chi_true + eta\n        \n        # --- Apply Passivity Diagnostic ---\n        \n        # 3. Check local passivity: Im{chi_hat_n} >= 0 for all n\n        im_chi_hat = chi_hat.imag\n        negative_mask = im_chi_hat < 0\n        num_negative_pixels = int(np.sum(negative_mask))\n        all_imag_nonneg = (num_negative_pixels == 0)\n\n        # 4. Compute total dissipated power: P_diss >= 0\n        E_mag_sq = np.abs(E_field)**2\n        power_terms = im_chi_hat * E_mag_sq\n        P_diss_W = power_prefactor * np.sum(power_terms)\n        total_power_nonneg = (P_diss_W >= 0)\n        \n        # Round power to 12 decimal places\n        P_diss_W_rounded = round(P_diss_W, 12)\n\n        all_results.append([\n            all_imag_nonneg,\n            total_power_nonneg,\n            num_negative_pixels,\n            P_diss_W_rounded\n        ])\n    \n    # --- Format and Print Output ---\n    # Manually format the output string to match the required specifications\n    # (e.g., lowercase booleans, no spaces, specific float format)\n    case_strs = []\n    for r in all_results:\n        # Format: [bool,bool,int,float]\n        case_str = (\n            f\"[{str(r[0]).lower()},\"\n            f\"{str(r[1]).lower()},\"\n            f\"{r[2]},\"\n            f\"{r[3]:.12f}]\"\n        )\n        case_strs.append(case_str)\n\n    # Final format: [[...],[...],[...]]\n    final_output = f\"[{','.join(case_strs)}]\"\n    print(final_output)\n\n\nsolve()\n```"
        },
        {
            "introduction": "The transition from a continuous physical model to a discrete numerical system introduces approximations that can profoundly affect the stability and accuracy of the solution. The conditioning of the system matrix is a key indicator of the numerical health of the problem. This practice delves into the numerical properties of the CSI formulation by analyzing how the condition number of the core system matrix changes with grid resolution, providing insight into the widely used \"points per wavelength\" rule-of-thumb and helping you build more robust and reliable computational models .",
            "id": "3295893",
            "problem": "Consider the frequency-domain scattering problem underlying Contrast Source Inversion (CSI) methods in computational electromagnetics. In CSI, the contrast source is defined as $w = \\chi E$, where $E$ is the total electric field and $\\chi$ is the contrast function. The $w$-update in CSI arises from minimizing a data misfit term coupled with a state constraint derived from the Lippmann–Schwinger integral equation for the scalar Helmholtz model, starting from Maxwell's equations in the frequency domain. For a background wavenumber $k_0$ and free-space Helmholtz Green's function $G$, the state operator involves $(I - k_0^2 G)$ acting on $w$.\n\nTo isolate and analyze the effect of the spatial discretization step on the conditioning of the $w$-update linear system, consider a one-dimensional, periodic, scalar Helmholtz setting over a domain of length $L$ measured in meters. Assume $L = N_w \\lambda$, where $\\lambda$ is the free-space wavelength and $N_w$ is the number of wavelengths contained in the domain. Let the discretization step be $\\Delta$, so that the number of grid points is $N = L / \\Delta$. Let the discretized free-space Helmholtz Green's function for one dimension be the circulant matrix $G \\in \\mathbb{C}^{N \\times N}$ generated by the first column $g_j = \\frac{i}{2 k_0} e^{i k_0 r_j}$, where $r_j$ is the periodic distance measured along the grid between the reference point and the $j$-th grid point, $k_0 = \\frac{2 \\pi}{\\lambda}$, and $i$ is the imaginary unit. Define the $w$-update system matrix as\n$$\nA = S^\\ast S + \\alpha (I - k_0^2 G)^\\ast (I - k_0^2 G),\n$$\nwhere $S$ is the data operator. To isolate the intrinsic conditioning introduced by the discretization of the state operator, take $S$ as the identity operator, so the system reduces to\n$$\nA = I + \\alpha (I - k_0^2 G)^\\ast (I - k_0^2 G).\n$$\n\nStarting from the scalar Helmholtz model and the definition of the Green's function, construct $G$ by using the periodic distances on a uniform grid with spacing $\\Delta$ measured in meters. Use the operator $A$ as defined above with a fixed, positive weight $\\alpha$ (dimensionless), and compute the spectral condition number\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)},\n$$\nwhere $\\sigma_{\\max}$ and $\\sigma_{\\min}$ denote the largest and smallest singular values of $A$, respectively. The condition number is dimensionless.\n\nYour task is to implement a program that, for a fixed wavelength $\\lambda$ measured in meters, a fixed number of wavelengths $N_w$, and a fixed $\\alpha$, evaluates how $\\kappa_2(A)$ varies as a function of the discretization step $\\Delta = \\lambda / \\text{Nppw}$, where $\\text{Nppw}$ denotes the number of grid points per wavelength. Based on computed results, determine a rule-of-thumb for grid resolution relative to wavelength (for example, $\\ge 10$ points per wavelength) that yields a well-conditioned $w$-update system.\n\nThe fundamental base you must use includes:\n- The frequency-domain Maxwell's equations imply the scalar Helmholtz equation under standard assumptions, leading to the Lippmann–Schwinger integral formulation.\n- The free-space one-dimensional Helmholtz Green's function is $g(x) = \\frac{i}{2 k_0} e^{i k_0 |x|}$.\n- Uniform-grid discretization of convolution with $g$ over a periodic domain yields a circulant matrix $G$.\n\nCarry out the analysis in purely mathematical and algorithmic terms without invoking any empirical shortcut formulas beyond the core definitions above. Compute the singular values of $A$ in a numerically accurate manner. All physical quantities must be expressed in the specified units.\n\nUse the following parameters for the test suite:\n- Wavelength $\\lambda = 1.0$ meter.\n- Number of wavelengths in the domain $N_w = 2$ (dimensionless).\n- Weight $\\alpha = 1.0$ (dimensionless).\n- Points-per-wavelength values (dimensionless): $\\text{Nppw} \\in \\{2, 4, 6, 8, 10, 12, 16, 25\\}$.\n\nFor each $\\text{Nppw}$ in the above set, construct $A$ as specified and compute $\\kappa_2(A)$ (dimensionless). Your program must output a single line containing the results as a comma-separated list enclosed in square brackets, with each condition number rounded to six decimal places, in the order corresponding to the $\\text{Nppw}$ values listed above. For example, the output format must be exactly like\n$$\n[\\kappa_2(A_{\\text{Nppw}=2}),\\kappa_2(A_{\\text{Nppw}=4}),\\dots,\\kappa_2(A_{\\text{Nppw}=25})],\n$$\nwith each entry a float rounded to six decimal places.",
            "solution": "The problem is well-posed and scientifically grounded in the field of computational electromagnetics, specifically concerning the numerical analysis of inverse scattering methods. The task is to analyze the conditioning of a system matrix that arises in Contrast Source Inversion (CSI) methods. We will proceed with a full solution.\n\nThe core of the problem is to compute the spectral condition number, $\\kappa_2(A)$, of the matrix $A \\in \\mathbb{C}^{N \\times N}$ defined as:\n$$\nA = I + \\alpha (I - k_0^2 G)^\\ast (I - k_0^2 G)\n$$\nwhere $I$ is the $N \\times N$ identity matrix, $\\alpha$ is a positive dimensionless weight, $k_0$ is the background wavenumber, $G$ is the discretized Green's function matrix, and $(\\cdot)^\\ast$ denotes the conjugate transpose. The condition number is defined as the ratio of the largest to the smallest singular value, $\\kappa_2(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A)$.\n\nFirst, let us analyze the properties of the matrix $A$. Let $B = I - k_0^2 G$. The matrix $A$ can be written as $A = I + \\alpha B^\\ast B$. The matrix $B^\\ast B$ is Hermitian and positive semi-definite. Its eigenvalues are real and non-negative. The eigenvalues of $A$ are therefore of the form $1 + \\alpha \\lambda_i(B^\\ast B)$, where $\\lambda_i(B^\\ast B)$ are the eigenvalues of $B^\\ast B$. Since $\\alpha > 0$ and $\\lambda_i(B^\\ast B) \\ge 0$, the eigenvalues of $A$ are all real and greater than or equal to $1$. This means $A$ is a Hermitian positive definite matrix. For a Hermitian matrix, the singular values are the absolute values of the eigenvalues. Since the eigenvalues of $A$ are positive, its singular values are identical to its eigenvalues. Therefore, the condition number is the ratio of the maximum to the minimum eigenvalue:\n$$\n\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}\n$$\nThis simplifies the problem to finding the eigenvalues of $A$. While we could construct the matrix $A$ explicitly and compute its eigenvalues numerically, a more efficient and elegant solution exists by exploiting the structure of the Green's function matrix $G$.\n\nThe problem states that the domain is one-dimensional and periodic, and the discretization grid is uniform. A key principle of numerical analysis is that the discretization of a convolution operator on a uniform periodic grid results in a circulant matrix. The matrix $G$ represents such an operator. A circulant matrix is fully determined by its first column, say $c = (c_0, c_1, \\dots, c_{N-1})^T$, and has the form:\n$$\nC = \\begin{pmatrix}\nc_0 & c_{N-1} & \\dots & c_1 \\\\\nc_1 & c_0 & \\dots & c_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nc_{N-1} & c_{N-2} & \\dots & c_0\n\\end{pmatrix}\n$$\nCirculant matrices have a fundamental property: they are diagonalized by the Discrete Fourier Transform (DFT) matrix. The eigenvalues of a circulant matrix $C$ are given by the DFT of its first column $c$. That is, if $\\hat{c} = \\text{DFT}(c)$, then the eigenvalues of $C$ are the components of the vector $\\hat{c}$.\n\nWe can apply this property to find the eigenvalues of $A$ without assembling the full matrices.\n1.  The Green's function matrix $G$ is circulant. Its eigenvalues, denoted $\\lambda_k(G)$, are the DFT of its first column, $g$. Let $\\hat{g} = \\text{DFT}(g)$. Then $\\lambda_k(G) = \\hat{g}_k$ for $k = 0, \\dots, N-1$.\n\n2.  The matrix $B = I - k_0^2 G$ is a linear combination of two circulant matrices ($I$ is circulant with first column $(1, 0, \\dots, 0)^T$). The result is also a circulant matrix. Its eigenvalues are thus $\\lambda_k(B) = 1 - k_0^2 \\lambda_k(G) = 1 - k_0^2 \\hat{g}_k$.\n\n3.  The conjugate transpose of a circulant matrix, $B^\\ast$, is also circulant. The product of two circulant matrices, $B^\\ast B$, is a commutative product and yields another circulant matrix. The eigenvalues of a product of matrices that are simultaneously diagonalizable (as is the case here) are the products of their respective eigenvalues. The eigenvalues of $B^\\ast$ are the complex conjugates of the eigenvalues of $B$, i.e., $\\overline{\\lambda_k(B)}$. Therefore, the eigenvalues of $B^\\ast B$ are $\\lambda_k(B^\\ast B) = \\lambda_k(B^\\ast) \\lambda_k(B) = \\overline{\\lambda_k(B)} \\lambda_k(B) = |\\lambda_k(B)|^2$.\n\n4.  Finally, the matrix $A = I + \\alpha B^\\ast B$ is also circulant. Its eigenvalues are $\\lambda_k(A) = 1 + \\alpha \\lambda_k(B^\\ast B) = 1 + \\alpha |\\lambda_k(B)|^2$.\n\nCombining these results, we obtain a direct formula for the eigenvalues of $A$:\n$$\n\\lambda_k(A) = 1 + \\alpha |1 - k_0^2 \\hat{g}_k|^2, \\quad k = 0, \\dots, N-1\n$$\nwhere $\\hat{g}_k$ are the components of the DFT of the first column of $G$.\n\nThe computational algorithm is as follows for each specified value of $\\text{Nppw}$:\n1.  Define the physical and numerical parameters based on the problem givens:\n    - Wavelength: $\\lambda = 1.0$ meter.\n    - Number of wavelengths in domain: $N_w = 2$.\n    - Weight: $\\alpha = 1.0$.\n    - Wavenumber: $k_0 = 2\\pi / \\lambda = 2\\pi$ rad/m.\n    - Discretization step: $\\Delta = \\lambda / \\text{Nppw}$.\n    - Domain length: $L = N_w \\lambda = 2.0$ meters.\n    - Number of grid points: $N = L / \\Delta = N_w \\times \\text{Nppw}$.\n\n2.  Construct the first column, $g$, of the Green's function matrix $G$. The $j$-th element is given by $g_j = \\frac{i}{2 k_0} e^{i k_0 r_j}$, where $r_j$ is the periodic distance on the grid from the origin to point $j$. For a grid of $N$ points with spacing $\\Delta$ over a length $L=N\\Delta$, this distance is $r_j = \\min(j\\Delta, L - j\\Delta)$ for $j = 0, 1, \\dots, N-1$.\n\n3.  Compute the DFT of the vector $g$ to find the eigenvalues of $G$: $\\hat{g} = \\text{DFT}(g)$.\n\n4.  Use the derived formula to calculate the eigenvalues of $A$: $\\lambda_k(A) = 1.0 + 1.0 \\times |1.0 - (2\\pi)^2 \\hat{g}_k|^2$.\n\n5.  Find the maximum and minimum values in the set of eigenvalues $\\{\\lambda_k(A)\\}$.\n\n6.  Compute the condition number $\\kappa_2(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$.\n\nThis procedure is numerically efficient and accurate, as it avoids the construction of large $N \\times N$ matrices and leverages the Fast Fourier Transform (FFT) algorithm.\n\nBased on the computed results, a rule-of-thumb can be inferred. A low number of points per wavelength (e.g., $\\text{Nppw}=2$), which violates the Nyquist sampling theorem for wave phenomena, is expected to lead to a very high condition number, indicating an ill-conditioned system. As $\\text{Nppw}$ increases, the discretization becomes finer and represents the continuous operator more accurately, which should generally improve the conditioning of the matrix $A$. A common guideline in computational electromagnetics is to use at least $10$ points per wavelength for acceptable accuracy. The analysis of $\\kappa_2(A)$ will show whether this guideline also leads to a well-conditioned system matrix for this CSI formulation. We expect $\\kappa_2(A)$ to decrease and stabilize as $\\text{Nppw}$ grows.",
            "answer": "```python\nimport numpy as np\nimport scipy.fft\n\ndef solve():\n    \"\"\"\n    Computes the spectral condition number of the CSI w-update matrix\n    for a 1D periodic scalar Helmholtz problem.\n    \"\"\"\n\n    # --- Test Suite Parameters ---\n    lambda_ = 1.0  # Wavelength in meters\n    n_w = 2.0      # Number of wavelengths in domain\n    alpha = 1.0    # Regularization weight\n    nppw_values = [2, 4, 6, 8, 10, 12, 16, 25]\n\n    results = []\n\n    for nppw in nppw_values:\n        # --- 1. Calculate physical and numerical parameters ---\n        # Background wavenumber (rad/m)\n        k0 = 2.0 * np.pi / lambda_\n        \n        # Discretization step (m)\n        delta = lambda_ / nppw\n        \n        # Domain length (m)\n        L = n_w * lambda_\n        \n        # Number of grid points (N = n_w * nppw, an integer)\n        N = int(n_w * nppw)\n\n        # --- 2. Construct the first column of the Green's function matrix G ---\n        # The first column g corresponds to the Green's function evaluated\n        # at distances from the source at the origin (j=0) to all other points j.\n        \n        # Create an array of grid indices\n        j_indices = np.arange(N)\n        \n        # Calculate the periodic distance r_j = min(j*delta, L - j*delta) for each grid point\n        r_distances = np.minimum(j_indices * delta, L - j_indices * delta)\n        \n        # The problem defines the discrete Green's function generating vector as:\n        # g_j = (i / 2*k0) * exp(i*k0*r_j).\n        # This formula is used directly.\n        g_col = (1j / (2.0 * k0)) * np.exp(1j * k0 * r_distances)\n\n        # --- 3. Compute eigenvalues of G using FFT ---\n        # The eigenvalues of a circulant matrix are the DFT of its first column.\n        eig_G = scipy.fft.fft(g_col)\n\n        # --- 4. Compute eigenvalues of A using the derived formula ---\n        # The eigenvalues of A = I + alpha * (I - k0^2*G)' * (I - k0^2*G) can be\n        # calculated directly from the eigenvalues of G.\n        # Let B = I - k0^2*G. eig(B) = 1 - k0^2 * eig(G).\n        # Since A is HPD and circulant, its eigenvalues are:\n        # eig(A) = 1 + alpha * |eig(B)|^2\n        eig_A = 1.0 + alpha * np.abs(1.0 - k0**2 * eig_G)**2\n\n        # --- 5. Compute the spectral condition number ---\n        # For a Hermitian Positive Definite matrix, the condition number is the\n        # ratio of the largest to the smallest eigenvalue.\n        kappa = np.max(eig_A) / np.min(eig_A)\n        \n        results.append(kappa)\n\n    # Format the results for output\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Print the final result in the exact specified format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}