{
    "hands_on_practices": [
        {
            "introduction": "在将进化算法应用于工程设计时，一个核心挑战是如何确保生成的解符合基本的物理定律。本练习探讨了电磁学中的洛伦兹互易定理，该定理要求无源线性各向同性器件的散射矩阵（$S$ 矩阵）必须是对称的。通过这个实践 ，您将学习如何将这一物理约束转化为算法中的“可行性规则”和“修复算子”，这是确保优化结果在物理上有效且有意义的关键技能。",
            "id": "3306120",
            "problem": "考虑一个由麦克斯韦方程组描述的线性、时不变、各向同性的无源电磁介质。对于一个由角频率为 $\\omega$（单位为弧度/秒）的时谐源激励的多端口结构，定义入射波幅值向量 $\\mathbf{a}(\\omega)$ 和散射（出射）波幅值向量 $\\mathbf{b}(\\omega)$。散射矩阵 $\\mathbf{S}(\\omega; x)$ 是通过 $\\mathbf{b}(\\omega) = \\mathbf{S}(\\omega; x)\\,\\mathbf{a}(\\omega)$ 关联这些量的线性算子，其中 $x$ 表示结构的设计参数（例如，几何特征或材料属性分布）。\n\n从洛伦兹互易定理推导出的一个基本结果，应用于具有对称介电常数和磁导率张量的线性时不变各向同性介质的麦克斯韦方程组，表明对于互易系统，散射矩阵必须是对称的，即对所有 $\\omega$ 都有 $\\mathbf{S}(\\omega; x) = \\mathbf{S}^{\\top}(\\omega; x)$。\n\n您的任务是制定一个适用于演化优化算法的可行性规则，以强制实现互易性。该可行性规则必须在一组有限的采样角频率上评估候选设计 $x$，并且只有当模拟出的散射矩阵在指定的数值容差内满足互易性约束时，才宣布该设计是可行的。此外，您必须实现一个修复算子，该算子对给定的（可能不可行的）$\\mathbf{S}(\\omega; x)$ 进行最小程度的修改，以生成一个对称矩阵，从而在不过度改变底层设计的情况下恢复可行性。\n\n您的解决方案必须满足以下要求：\n\n- 从麦克斯韦方程组和洛伦兹互易定理出发，逻辑上证明为什么互易性要求 $\\mathbf{S}(\\omega; x) = \\mathbf{S}^{\\top}(\\omega; x)$。\n- 使用弗罗贝尼乌斯范数定义一个互易性违背度量，并基于第一性原理构建一个可行性规则和一个修复算子。可行性规则必须使用一个容差参数 $\\tau$。\n- 修复算子必须是对称投影 $\\mathbf{R}(\\mathbf{S}) = \\tfrac{1}{2}\\left(\\mathbf{S} + \\mathbf{S}^{\\top}\\right)$，并且您必须从数学优化的角度证明，在所有对称矩阵中，它对 $\\mathbf{S}$ 在弗罗贝尼乌斯范数意义下的扰动最小。\n- 实现一个程序，对下面的每个测试用例，计算以下四个量：\n  1. 修复前的最大互易性违背度，定义为 $v_{\\text{before}} = \\max_{\\omega \\in \\Omega} \\left\\|\\mathbf{S}(\\omega; x) - \\mathbf{S}^{\\top}(\\omega; x)\\right\\|_{F}$。\n  2. 修复后的最大互易性违背度，定义为 $v_{\\text{after}} = \\max_{\\omega \\in \\Omega} \\left\\|\\mathbf{R}(\\mathbf{S}(\\omega; x)) - \\mathbf{R}(\\mathbf{S}(\\omega; x))^{\\top}\\right\\|_{F}$。\n  3. 修复后的可行性，定义为布尔值 $b_{\\text{feasible}} = \\left[v_{\\text{after}} \\le \\tau\\right]$。\n  4. 修复引入的最大扰动，定义为 $d_{\\text{max}} = \\max_{\\omega \\in \\Omega} \\left\\|\\mathbf{R}(\\mathbf{S}(\\omega; x)) - \\mathbf{S}(\\omega; x)\\right\\|_{F}$。\n- 使用数值容差 $\\tau = 10^{-8}$。\n- 角频率以弧度/秒表示；输出为无量纲实数和布尔值。不得使用百分比单位。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个测试用例的结果以 $[v_{\\text{before}}, v_{\\text{after}}, b_{\\text{feasible}}, d_{\\text{max}}]$ 的列表形式表示。\n\n测试套件：\n对于每个测试用例，$\\Omega$ 包含两个角频率，$\\mathbf{S}(\\omega; x)$ 是一个如下指定的 $3\\times 3$ 复数矩阵。\n\n- 测试用例 A（在采样频率上已满足互易性）：\n  频率：$\\Omega = \\{2\\pi \\cdot 10^{9},\\, 2\\pi \\cdot 2 \\cdot 10^{9}\\}$。\n  矩阵：\n  $$\n  \\mathbf{S}(2\\pi \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.10  0.02  0.03\\\\\n  0.02  0.20  0.01\\\\\n  0.03  0.01  0.15\n  \\end{bmatrix}\n  $$\n  $$\n  \\mathbf{S}(2\\pi \\cdot 2 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.08 + 0.02\\mathrm{i}  0.01 - 0.005\\mathrm{i}  0.00 + 0.01\\mathrm{i}\\\\\n  0.01 - 0.005\\mathrm{i}  0.12 + 0.00\\mathrm{i}  0.02 + 0.003\\mathrm{i}\\\\\n  0.00 + 0.01\\mathrm{i}  0.02 + 0.003\\mathrm{i}  0.09 - 0.01\\mathrm{i}\n  \\end{bmatrix}\n  $$\n\n- 测试用例 B（因微小噪声而轻微不对称）：\n  频率：$\\Omega = \\{2\\pi \\cdot 1.2 \\cdot 10^{9},\\, 2\\pi \\cdot 1.8 \\cdot 10^{9}\\}$。\n  矩阵：\n  $$\n  \\mathbf{S}(2\\pi \\cdot 1.2 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.20  0.05 + 0.001  0.00 - 0.002\\\\\n  0.05 - 0.001  0.15  0.01 + 0.0005\\\\\n  0.00 + 0.002  0.01 - 0.0005  0.10\n  \\end{bmatrix}\n  $$\n  $$\n  \\mathbf{S}(2\\pi \\cdot 1.8 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.18 + 0.01\\mathrm{i}  0.02 + 0.00\\mathrm{i} + 0.001 + 0.0005\\mathrm{i}  0.03 - 0.01\\mathrm{i} - 0.001 - 0.0003\\mathrm{i}\\\\\n  0.02 + 0.00\\mathrm{i} - 0.001 - 0.0005\\mathrm{i}  0.16 - 0.005\\mathrm{i}  0.04 + 0.002\\mathrm{i} + 0.0004 - 0.0002\\mathrm{i}\\\\\n  0.03 - 0.01\\mathrm{i} + 0.001 + 0.0003\\mathrm{i}  0.04 + 0.002\\mathrm{i} - 0.0004 + 0.0002\\mathrm{i}  0.11 + 0.00\\mathrm{i}\n  \\end{bmatrix}\n  $$\n\n- 测试用例 C（强不对称）：\n  频率：$\\Omega = \\{2\\pi \\cdot 0.8 \\cdot 10^{9},\\, 2\\pi \\cdot 1.6 \\cdot 10^{9}\\}$。\n  矩阵：\n  $$\n  \\mathbf{S}(2\\pi \\cdot 0.8 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.30 + 0.00\\mathrm{i}  0.10 + 0.05\\mathrm{i}  -0.20 + 0.10\\mathrm{i}\\\\\n  0.05 - 0.10\\mathrm{i}  -0.10 + 0.00\\mathrm{i}  0.25 + 0.00\\mathrm{i}\\\\\n  0.15 - 0.05\\mathrm{i}  0.40 + 0.20\\mathrm{i}  0.05 + 0.30\\mathrm{i}\n  \\end{bmatrix}\n  $$\n  $$\n  \\mathbf{S}(2\\pi \\cdot 1.6 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.25 - 0.05\\mathrm{i}  -0.35 + 0.10\\mathrm{i}  0.20 + 0.00\\mathrm{i}\\\\\n  0.20 - 0.15\\mathrm{i}  0.30 + 0.05\\mathrm{i}  -0.10 + 0.25\\mathrm{i}\\\\\n  0.05 + 0.20\\mathrm{i}  0.12 - 0.30\\mathrm{i}  -0.20 + 0.10\\mathrm{i}\n  \\end{bmatrix}\n  $$\n\n- 测试用例 D（边界情况：零散射）：\n  频率：$\\Omega = \\{2\\pi \\cdot 10^{9},\\, 2\\pi \\cdot 1.5 \\cdot 10^{9}\\}$。\n  矩阵：\n  $$\n  \\mathbf{S}(2\\pi \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0  0  0\\\\\n  0  0  0\\\\\n  0  0  0\n  \\end{bmatrix}\n  \\quad,\\quad\n  \\mathbf{S}(2\\pi \\cdot 1.5 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0  0  0\\\\\n  0  0  0\\\\\n  0  0  0\n  \\end{bmatrix}\n  $$\n\n- 测试用例 E（频率相关且具有中等不对称性）：\n  频率：$\\Omega = \\{2\\pi \\cdot 1.1 \\cdot 10^{9},\\, 2\\pi \\cdot 1.7 \\cdot 10^{9}\\}$。\n  矩阵：\n  $$\n  \\mathbf{S}(2\\pi \\cdot 1.1 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.12  0.03 + 0.005  0.025 - 0.004\\\\\n  0.03 - 0.005  0.18  0.02 + 0.003\\\\\n  0.025 + 0.004  0.02 - 0.003  0.14\n  \\end{bmatrix}\n  $$\n  $$\n  \\mathbf{S}(2\\pi \\cdot 1.7 \\cdot 10^{9}; x) =\n  \\begin{bmatrix}\n  0.10 + 0.02\\mathrm{i}  0.04 - 0.01\\mathrm{i} - 0.01 + 0.005\\mathrm{i}  0.00 + 0.03\\mathrm{i} + 0.006 - 0.004\\mathrm{i}\\\\\n  0.04 - 0.01\\mathrm{i} + 0.01 - 0.005\\mathrm{i}  0.20 + 0.00\\mathrm{i}  0.01 + 0.02\\mathrm{i} - 0.005 + 0.002\\mathrm{i}\\\\\n  0.00 + 0.03\\mathrm{i} - 0.006 + 0.004\\mathrm{i}  0.01 + 0.02\\mathrm{i} + 0.005 - 0.002\\mathrm{i}  0.13 - 0.01\\mathrm{i}\n  \\end{bmatrix}\n  $$\n\n实现细节和输出规范：\n- 使用 $\\tau = 10^{-8}$。\n- 为每个测试用例精确计算如上定义的 $v_{\\text{before}}$、$v_{\\text{after}}$、$b_{\\text{feasible}}$ 和 $d_{\\text{max}}$。\n- 您的程序应生成单行输出，其中包含五个测试用例结果的逗号分隔列表，每个结果表示为列表 $[v_{\\text{before}}, v_{\\text{after}}, b_{\\text{feasible}}, d_{\\text{max}}]$，用方括号括起，例如：\n  $$\n  [\\,[v_{\\text{before}}^{(A)}, v_{\\text{after}}^{(A)}, b_{\\text{feasible}}^{(A)}, d_{\\text{max}}^{(A)}],\\,\\ldots,\\, [v_{\\text{before}}^{(E)}, v_{\\text{after}}^{(E)}, b_{\\text{feasible}}^{(E)}, d_{\\text{max}}^{(E)}]\\,]\n  $$\n无需用户输入；所有数据必须嵌入程序中。",
            "solution": "该问题要求为应用于计算电磁学的演化优化算法，制定并实现一个互易性约束。这涉及到推导该约束的理论基础，定义其违背度的量化度量，并构建一个在强制执行该约束的同时对原始系统扰动最小的修复算子。\n\n### 1. 从洛伦兹互易性推导S矩阵对称性的理论依据\n\n对于一个互易的多端口网络，其散射矩阵的对称性 $\\mathbf{S} = \\mathbf{S}^{\\top}$ 是洛伦兹互易定理的直接推论。推导始于角频率为 $\\omega$ 的时谐电磁场的麦克斯韦方程组，场由相量 $(\\vec{E}, \\vec{H})$ 表示。\n\n在一个由线性、时不变、各向同性介质占据的无源体积 $V$ 中，麦克斯韦的旋度方程为：\n$$ \\nabla \\times \\vec{E} = -j\\omega\\mu\\vec{H} $$\n$$ \\nabla \\times \\vec{H} = j\\omega\\epsilon\\vec{E} $$\n材料属性由介电常数 $\\epsilon$ 和磁导率 $\\mu$ 描述。对于各向同性介质，它们是标量。更一般地，对于互易介质，它们是对称张量，即 $\\epsilon = \\epsilon^{\\top}$ 和 $\\mu = \\mu^{\\top}$。\n\n考虑两组独立的源，在相同频率 $\\omega$ 下产生两组不同的场 $(\\vec{E}_1, \\vec{H}_1)$ 和 $(\\vec{E}_2, \\vec{H}_2)$。根据矢量恒等式，我们有：\n$$ \\nabla \\cdot (\\vec{E}_1 \\times \\vec{H}_2) = \\vec{H}_2 \\cdot (\\nabla \\times \\vec{E}_1) - \\vec{E}_1 \\cdot (\\nabla \\times \\vec{H}_2) = -j\\omega\\mu (\\vec{H}_1 \\cdot \\vec{H}_2) - j\\omega\\epsilon (\\vec{E}_1 \\cdot \\vec{E}_2) $$\n$$ \\nabla \\cdot (\\vec{E}_2 \\times \\vec{H}_1) = \\vec{H}_1 \\cdot (\\nabla \\times \\vec{E}_2) - \\vec{E}_2 \\cdot (\\nabla \\times \\vec{H}_1) = -j\\omega\\mu (\\vec{H}_2 \\cdot \\vec{H}_1) - j\\omega\\epsilon (\\vec{E}_2 \\cdot \\vec{E}_1) $$\n将这两个方程相减得到 $\\nabla \\cdot (\\vec{E}_1 \\times \\vec{H}_2 - \\vec{E}_2 \\times \\vec{H}_1) = 0$。对体积 $V$ 应用散度定理，得到洛伦兹互易定理的积分形式：\n$$ \\oint_{\\partial V} (\\vec{E}_1 \\times \\vec{H}_2 - \\vec{E}_2 \\times \\vec{H}_1) \\cdot d\\vec{A} = 0 $$\n其中 $\\partial V$ 是 $V$ 的边界面。\n\n对于一个多端口设备，该表面 $\\partial V$ 由端口孔径以及包围的金属或辐射边界组成。假设是理想导体或场在无穷远处消失的表面，则该积分仅在端口上非零。每个端口 $k$ 处的场可以分解为导行模式的叠加。对于单模端口，场是振幅为 $a_k$ 的入射波和振幅为 $b_k$ 的散射波的组合。端口 $k$ 处的总切向场与这些振幅以及端口的归一化模式场 $(\\vec{e}_k, \\vec{h}_k)$ 相关。面积分简化为对所有 $N$ 个端口的求和：\n$$ \\sum_{k=1}^{N} \\int_{A_k} (\\vec{E}_{1,t} \\times \\vec{H}_{2,t} - \\vec{E}_{2,t} \\times \\vec{H}_{1,t}) \\cdot d\\vec{A}_k = 0 $$\n通过对模式场进行适当的归一化，这个代数和变成了两种状态下入射波和散射波振幅之间的关系：\n$$ \\sum_{k=1}^{N} (a_{1k} b_{2k} - a_{2k} b_{1k}) = 0 $$\n用向量形式表示，即为 $\\mathbf{a}_1^{\\top} \\mathbf{b}_2 - \\mathbf{a}_2^{\\top} \\mathbf{b}_1 = 0$。使用散射矩阵的定义 $\\mathbf{b} = \\mathbf{S}\\mathbf{a}$，我们可以代入 $\\mathbf{b}_1 = \\mathbf{S}\\mathbf{a}_1$ 和 $\\mathbf{b}_2 = \\mathbf{S}\\mathbf{a}_2$：\n$$ \\mathbf{a}_1^{\\top} (\\mathbf{S}\\mathbf{a}_2) - \\mathbf{a}_2^{\\top} (\\mathbf{S}\\mathbf{a}_1) = 0 $$\n由于 $(\\mathbf{X}\\mathbf{Y})^\\top = \\mathbf{Y}^\\top\\mathbf{X}^\\top$ 且标量是其自身的转置，我们可以写出 $\\mathbf{a}_2^{\\top}\\mathbf{S}\\mathbf{a}_1 = (\\mathbf{a}_1^\\top \\mathbf{S}^\\top \\mathbf{a}_2)$。方程变为：\n$$ \\mathbf{a}_1^{\\top} \\mathbf{S} \\mathbf{a}_2 - \\mathbf{a}_1^{\\top} \\mathbf{S}^{\\top} \\mathbf{a}_2 = \\mathbf{a}_1^{\\top} (\\mathbf{S} - \\mathbf{S}^{\\top}) \\mathbf{a}_2 = 0 $$\n该关系必须对激励向量 $\\mathbf{a}_1$ 和 $\\mathbf{a}_2$ 的任何任意选择都成立。这只有在中间的矩阵是零矩阵时才可能，即 $\\mathbf{S} - \\mathbf{S}^{\\top} = \\mathbf{0}$，这意味着 $\\mathbf{S} = \\mathbf{S}^{\\top}$。这证明了互易性要求散射矩阵是对称的。\n\n### 2. 可行性规则与互易性违背度量\n\n在计算电磁学中，仿真中的数值误差可能导致散射矩阵 $\\mathbf{S}$ 不完全对称，即使建模的物理系统是互易的。对于优化算法，我们需要一个规则来判断候选设计在互易性约束方面是否“可行”。\n\n一个量化违背度的自然方法是测量矩阵的斜对称分量 $\\mathbf{S} - \\mathbf{S}^{\\top}$ 的“大小”。我们为此目的使用弗罗贝尼乌斯范数。复数矩阵 $\\mathbf{A} \\in \\mathbb{C}^{m \\times n}$ 的弗罗贝尼乌斯范数定义为：\n$$ \\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n} |A_{ij}|^2} $$\n给定散射矩阵 $\\mathbf{S}(\\omega; x)$ 的互易性违背度量定义为：\n$$ v(\\mathbf{S}) = \\|\\mathbf{S}(\\omega; x) - \\mathbf{S}^{\\top}(\\omega; x)\\|_F $$\n对于在一组频率 $\\Omega$ 上评估的优化问题，可行性规则是使用一个数值容差 $\\tau > 0$ 来构建的。如果所有采样频率上的最大违背度在此容差范围内，则宣布设计 $x$ 是可行的：\n$$ \\text{如果 } \\max_{\\omega \\in \\Omega} v(\\mathbf{S}(\\omega; x)) \\le \\tau \\text{，则设计 } x \\text{ 是可行的} $$\n\n### 3. 修复算子及其最优性\n\n当发现一个候选设计是不可行的（即其 $\\mathbf{S}$ 矩阵不够对称）时，可以使用一个修复算子将其投影回可行集。所提出的算子是对称投影：\n$$ \\mathbf{R}(\\mathbf{S}) = \\frac{1}{2}(\\mathbf{S} + \\mathbf{S}^{\\top}) $$\n该算子生成一个对称矩阵，因为 $(\\mathbf{R}(\\mathbf{S}))^{\\top} = \\frac{1}{2}(\\mathbf{S}^{\\top} + (\\mathbf{S}^{\\top})^{\\top}) = \\frac{1}{2}(\\mathbf{S}^{\\top} + \\mathbf{S}) = \\mathbf{R}(\\mathbf{S})$。\n\n我们必须证明该算子对 $\\mathbf{S}$ 的扰动最小，这在数学上意味着 $\\mathbf{R}(\\mathbf{S})$ 是以下优化问题的解：\n$$ \\min_{\\mathbf{X}} \\|\\mathbf{S} - \\mathbf{X}\\|_F \\quad \\text{约束条件} \\quad \\mathbf{X} = \\mathbf{X}^{\\top} $$\n我们寻求在弗罗贝尼乌斯范数意义上最接近 $\\mathbf{S}$ 的对称矩阵 $\\mathbf{X}$。我们来最小化范数的平方，$f(\\mathbf{X}) = \\|\\mathbf{S} - \\mathbf{X}\\|_F^2$：\n$$ f(\\mathbf{X}) = \\sum_{i,j} |S_{ij} - X_{ij}|^2 $$\n我们可以通过对 $\\mathbf{X}$ 的独立元素求导并将其设为零来最小化此函数。由于 $\\mathbf{X}$ 是对称的（$X_{ij} = X_{ji}$），独立变量是所有 $i$ 的 $X_{ii}$ 和 $i  j$ 的 $X_{ij}$。\n\n目标函数可以写成：\n$$ f(\\mathbf{X}) = \\sum_{i} |S_{ii} - X_{ii}|^2 + \\sum_{i  j} (|S_{ij} - X_{ij}|^2 + |S_{ji} - X_{ji}|^2) $$\n由于 $X_{ij} = X_{ji}$，这简化为：\n$$ f(\\mathbf{X}) = \\sum_{i} |S_{ii} - X_{ii}|^2 + \\sum_{i  j} (|S_{ij} - X_{ij}|^2 + |S_{ji} - X_{ij}|^2) $$\n分别最小化每一项：\n对于对角元素（$i=j$）：当 $X_{ii} = S_{ii}$ 时，项 $|S_{ii} - X_{ii}|^2$ 最小化。\n对于非对角元素（$i \\neq j$）：考虑 $i  j$ 时对应 $(i, j)$ 和 $(j, i)$ 的项对。令 $g(X_{ij}) = |S_{ij} - X_{ij}|^2 + |S_{ji} - X_{ij}|^2$。令 $X_{ij} = u + iv$，$S_{ij} = a+ib$，$S_{ji} = c+id$。\n$$ g(u, v) = (a-u)^2 + (b-v)^2 + (c-u)^2 + (d-v)^2 $$\n求偏导数并设为零：\n$$ \\frac{\\partial g}{\\partial u} = -2(a-u) - 2(c-u) = 0 \\implies 4u = 2(a+c) \\implies u = \\frac{a+c}{2} $$\n$$ \\frac{\\partial g}{\\partial v} = -2(b-v) - 2(d-v) = 0 \\implies 4v = 2(b+d) \\implies v = \\frac{b+d}{2} $$\n因此，最优的 $X_{ij}$ 是 $u+iv = \\frac{a+c}{2} + i\\frac{b+d}{2} = \\frac{1}{2}((a+ib) + (c+id)) = \\frac{1}{2}(S_{ij} + S_{ji})$。\n结合对角和非对角元素的结果，最优对称矩阵 $\\mathbf{X}$ 的元素为 $X_{ij} = \\frac{1}{2}(S_{ij} + S_{ji})$。\n这恰好是矩阵 $\\mathbf{R}(\\mathbf{S}) = \\frac{1}{2}(\\mathbf{S} + \\mathbf{S}^{\\top})$。\n因此，$\\mathbf{R}(\\mathbf{S})$ 是在弗罗贝尼乌斯范数意义下唯一最接近 $\\mathbf{S}$ 的对称矩阵，这证明了其作为最小扰动修复算子的合理性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the reciprocity validation and repair problem for a series of test cases.\n    \"\"\"\n    \n    # Define the numerical tolerance for the feasibility check.\n    tau = 1e-8\n\n    # Define the test cases from the problem statement.\n    # Each test case is a list of S-matrices given as numpy arrays.\n    test_cases = [\n        # Test Case A (already reciprocal)\n        [\n            np.array([\n                [0.10, 0.02, 0.03],\n                [0.02, 0.20, 0.01],\n                [0.03, 0.01, 0.15]\n            ], dtype=complex),\n            np.array([\n                [0.08 + 0.02j, 0.01 - 0.005j, 0.00 + 0.01j],\n                [0.01 - 0.005j, 0.12 + 0.00j,  0.02 + 0.003j],\n                [0.00 + 0.01j,  0.02 + 0.003j, 0.09 - 0.01j]\n            ], dtype=complex)\n        ],\n        \n        # Test Case B (slightly asymmetric)\n        [\n            np.array([\n                [0.20, 0.05 + 0.001, 0.00 - 0.002],\n                [0.05 - 0.001, 0.15, 0.01 + 0.0005],\n                [0.00 + 0.002, 0.01 - 0.0005, 0.10]\n            ], dtype=complex),\n            np.array([\n                [0.18 + 0.01j, 0.02 + 0.00j + 0.001 + 0.0005j, 0.03 - 0.01j - 0.001 - 0.0003j],\n                [0.02 + 0.00j - 0.001 - 0.0005j, 0.16 - 0.005j, 0.04 + 0.002j + 0.0004 - 0.0002j],\n                [0.03 - 0.01j + 0.001 + 0.0003j, 0.04 + 0.002j - 0.0004 + 0.0002j, 0.11 + 0.00j]\n            ], dtype=complex)\n        ],\n        \n        # Test Case C (strongly asymmetric)\n        [\n            np.array([\n                [0.30 + 0.00j, 0.10 + 0.05j, -0.20 + 0.10j],\n                [0.05 - 0.10j, -0.10 + 0.00j, 0.25 + 0.00j],\n                [0.15 - 0.05j, 0.40 + 0.20j, 0.05 + 0.30j]\n            ], dtype=complex),\n            np.array([\n                [0.25 - 0.05j, -0.35 + 0.10j, 0.20 + 0.00j],\n                [0.20 - 0.15j, 0.30 + 0.05j, -0.10 + 0.25j],\n                [0.05 + 0.20j, 0.12 - 0.30j, -0.20 + 0.10j]\n            ], dtype=complex)\n        ],\n\n        # Test Case D (boundary case: zero scattering)\n        [\n            np.array([\n                [0, 0, 0],\n                [0, 0, 0],\n                [0, 0, 0]\n            ], dtype=complex),\n            np.array([\n                [0, 0, 0],\n                [0, 0, 0],\n                [0, 0, 0]\n            ], dtype=complex)\n        ],\n\n        # Test Case E (frequency-dependent with moderate asymmetry)\n        [\n            np.array([\n                [0.12, 0.03 + 0.005, 0.025 - 0.004],\n                [0.03 - 0.005, 0.18, 0.02 + 0.003],\n                [0.025 + 0.004, 0.02 - 0.003, 0.14]\n            ], dtype=complex),\n            np.array([\n                [0.10 + 0.02j, 0.04 - 0.01j - 0.01 + 0.005j, 0.00 + 0.03j + 0.006 - 0.004j],\n                [0.04 - 0.01j + 0.01 - 0.005j, 0.20 + 0.00j, 0.01 + 0.02j - 0.005 + 0.002j],\n                [0.00 + 0.03j - 0.006 + 0.004j, 0.01 + 0.02j + 0.005 - 0.002j, 0.13 - 0.01j]\n            ], dtype=complex)\n        ]\n    ]\n\n    all_results = []\n    \n    for case_matrices in test_cases:\n        violations_before = []\n        violations_after = []\n        perturbations = []\n\n        for S in case_matrices:\n            # Calculate violation before repair\n            diff_before = S - S.T\n            v_before_single = np.linalg.norm(diff_before, 'fro')\n            violations_before.append(v_before_single)\n\n            # Apply the repair operator\n            S_repaired = 0.5 * (S + S.T)\n\n            # Calculate violation after repair\n            # This should be zero or very close to it due to floating-point arithmetic\n            diff_after = S_repaired - S_repaired.T\n            v_after_single = np.linalg.norm(diff_after, 'fro')\n            violations_after.append(v_after_single)\n\n            # Calculate the perturbation introduced by the repair\n            perturbation_matrix = S_repaired - S\n            d_single = np.linalg.norm(perturbation_matrix, 'fro')\n            perturbations.append(d_single)\n\n        # Compute the maximums over the frequencies for the current test case\n        v_before = max(violations_before)\n        v_after = max(violations_after)\n        d_max = max(perturbations)\n        \n        # Determine feasibility after repair\n        b_feasible = v_after = tau\n        \n        all_results.append([v_before, v_after, b_feasible, d_max])\n\n    # Format the final output string to be a list of lists with no spaces\n    # Example: [[v1,v2,True,v3],[v4,v5,False,v6]]\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "计算电磁学中进化优化的主要瓶颈通常是单个设计方案所需的高昂仿真成本。然而，遗传算子（如交叉）可能会重复生成先前已评估过的设计，从而造成计算资源的浪费。本练习  将指导您通过概率建模，从第一性原理出发量化实现缓存机制所带来的预期时间节省，从而让您亲身体验如何通过实用的软件工程技术来显著加速优化过程。",
            "id": "3306062",
            "problem": "在计算电磁学中，使用遗传算法（GA）对超表面散射体进行优化，其中每个候选设计（基因型）都由时域有限差分（FDTD）求解器进行评估。对于固定的仿真设置，电磁响应是确定性的，因此对由基因型及其仿真设置组成的相同配对重复进行评估，会产生相同的适应度。为了避免重复的 FDTD 求解，提出了一种缓存机制：将由基因型和仿真设置组成的元组的加密哈希构建为一个键。设哈希键定义为 $H(g,s)$，其中 $g$ 是基因型，$s$ 是仿真设置的向量（例如，频率、网格分辨率、边界条件）。假设哈希碰撞概率可忽略不计，查找时间为 $\\mathcal{O}(1)$，并且在所考虑的代中，仿真设置 $s$ 保持不变。缓存初始包含来自上一代的 $N$ 个唯一的基因型，每个都以其键存储。\n\n在一代中，GA 产生 $N$ 个子代，其中比例为 $f_{c}$ 的子代通过交叉生成，其余比例为 $1 - f_{c}$ 的子代通过变异生成。由于表示冗余（例如，对称性和中性编码），交叉操作经常会重现在相同设置下已在前面代中出现过的基因型。通过以下概率对此进行建模：在交叉子代中，一个子代的基因型与缓存中已存在的某个基因型在相同设置下相等的概率为 $p_{c}$；在变异子代中，相应的概率为 $p_{m}$。\n\n对一个唯一键的每次 FDTD 评估会产生评估时间 $t_{e}$，而缓存机制会增加开销：计算哈希的成本为 $t_{h}$，缓存查找的成本为 $t_{\\ell}$，检索缓存适应度的成本为 $t_{r}$。对于唯一键，将新结果存入缓存的成本为 $t_{s}$。不使用缓存的基准情况是每代执行 $N$ 次 FDTD 评估。\n\n仅使用上述建模假设，从第一性原理推导缓存机制与不使用缓存的基准情况相比，每代期望节省的净时间。然后，使用以下参数值对此期望净节省时间进行数值计算：\n- 每代子代数量 $N = 250$。\n- 交叉比例 $f_{c} = 0.85$。\n- 交叉子代中的重复概率 $p_{c} = 0.42$。\n- 变异子代中的重复概率 $p_{m} = 0.07$。\n- FDTD 评估时间 $t_{e} = 18.5$ 秒。\n- 哈希计算时间 $t_{h} = 8.0 \\times 10^{-4}$ 秒。\n- 查找时间 $t_{\\ell} = 5.0 \\times 10^{-4}$ 秒。\n- 检索时间 $t_{r} = 3.0 \\times 10^{-4}$ 秒。\n- 存储时间 $t_{s} = 6.0 \\times 10^{-4}$ 秒。\n\n将每代最终期望节省的净时间以秒为单位表示，并将您的答案四舍五入到 $4$ 位有效数字。",
            "solution": "用户希望通过在遗传算法中实现缓存机制，来求出每代期望节省的净时间。该解将从第一性原理推导，然后进行数值计算。\n\n### 步骤 1：问题验证\n\n**1.1. 提取给定条件**\n-   每代子代数量：$N = 250$\n-   交叉比例：$f_{c} = 0.85$\n-   剩余比例（变异）：$1 - f_{c}$\n-   交叉子代中的重复概率：$p_{c} = 0.42$\n-   变异子代中的重复概率：$p_{m} = 0.07$\n-   FDTD 评估时间：$t_{e} = 18.5$ 秒\n-   哈希计算时间：$t_{h} = 8.0 \\times 10^{-4}$ 秒\n-   缓存查找时间：$t_{\\ell} = 5.0 \\times 10^{-4}$ 秒\n-   缓存适应度检索时间：$t_{r} = 3.0 \\times 10^{-4}$ 秒\n-   新结果存储时间：$t_{s} = 6.0 \\times 10^{-4}$ 秒\n-   基准：每代进行 $N$ 次 FDTD 评估，每次耗时 $t_{e}$。\n-   假设：哈希碰撞概率可忽略不计，查找时间为 $\\mathcal{O}(1)$，仿真设置 $s$ 恒定。\n\n**1.2. 使用提取的给定条件进行验证**\n-   **科学依据**：该问题设置在计算工程（计算电磁学）和计算机科学（演化算法、性能分析）的背景下。使用缓存来避免重复计算是一种标准且有效的技术。该模型是一种概率性能分析，这是一种成熟的数学方法。所有方面在科学上都是合理的。\n-   **适定性**：问题提供了所有必要的参数和一个明确的目标：推导公式并计算数值。该设置允许一个唯一的、稳定的和有意义的解。\n-   **客观性**：语言精确、量化，并且没有主观性。术语在模型上下文中被清晰定义。\n-   **完整性和一致性**：问题是自洽的。推导所需的所有变量都已定义，并具有一致的单位（秒）。没有矛盾之处。\n-   **真实性**：所提供的时间值是现实的。FDTD 仿真在计算上是昂贵的（秒到分钟或小时），而哈希和内存操作非常快（微秒到毫秒）。概率也在合理范围内。\n\n**1.3. 结论与行动**\n该问题是有效的。它在科学上是合理的、适定的、客观的、完整的和一致的。我将继续提供一个完整的解答。\n\n### 步骤 2：解的推导\n\n目标是求出每代期望节省的净时间 $\\Delta T$。这等于不使用缓存的总时间 $T_{uncached}$ 与使用缓存的期望总时间 $E[T_{cached}]$ 之间的差值。\n$$\n\\Delta T = T_{uncached} - E[T_{cached}]\n$$\n不使用缓存的基准过程执行 $N$ 次评估，因此总时间为：\n$$\nT_{uncached} = N t_{e}\n$$\n为了求出使用缓存的期望时间，我们可以确定单个子代的期望时间成本，然后乘以 $N$，因为对所有子代来说这个过程是相同的。设 $E[\\Delta T_{indiv}]$ 为单个子代节省的期望时间。那么总的期望节省时间为 $\\Delta T = N \\cdot E[\\Delta T_{indiv}]$。\n\n单个子代节省的期望时间是各种可能情况下节省的时间与其概率的加权和。一个子代有两种情况：其基因型已在缓存中（“命中”），或不在缓存中（“未命中”）。\n\n首先，让我们确定缓存命中的总概率 $P_{hit}$。子代由交叉（概率为 $f_c$）或变异（概率为 $1-f_c$）生成。\n命中概率由全概率定律给出：\n$$\nP_{hit} = P(\\text{命中}|\\text{交叉})P(\\text{交叉}) + P(\\text{命中}|\\text{变异})P(\\text{变异})\n$$\n根据问题的定义，$P(\\text{命中}|\\text{交叉}) = p_{c}$ 且 $P(\\text{命中}|\\text{变异}) = p_{m}$。\n因此，对于任何给定的子代，缓存命中的总概率为：\n$$\nP_{hit} = p_{c} f_{c} + p_{m} (1 - f_{c})\n$$\n缓存未命中的概率为 $P_{miss} = 1 - P_{hit}$。\n\n现在，我们分析每种情况下节省的时间：\n1.  **缓存命中**：此情况以概率 $P_{hit}$ 发生。\n    -   不使用缓存，时间成本为 $t_{e}$。\n    -   使用缓存，过程是：计算哈希（$t_{h}$）、查找键（$t_{\\ell}$）并检索存储的结果（$t_{r}$）。总成本为 $t_{h} + t_{\\ell} + t_{r}$。\n    -   这种情况下节省的时间为 $\\Delta T_{hit} = t_{e} - (t_{h} + t_{\\ell} + t_{r})$。\n\n2.  **缓存未命中**：此情况以概率 $P_{miss} = 1 - P_{hit}$ 发生。\n    -   不使用缓存，时间成本为 $t_{e}$。\n    -   使用缓存，过程是：计算哈希（$t_{h}$）、查找键（$t_{\\ell}$）、执行 FDTD 评估（$t_{e}$）并存储新结果（$t_{s}$）。总成本为 $t_{h} + t_{\\ell} + t_{e} + t_{s}$。\n    -   这种情况下节省的时间为 $\\Delta T_{miss} = t_{e} - (t_{h} + t_{\\ell} + t_{e} + t_{s}) = -(t_{h} + t_{\\ell} + t_{s})$。这代表了由于缓存开销造成的净成本（负节省）。\n\n每个子代节省的期望时间 $E[\\Delta T_{indiv}]$ 是这两种情况下节省时间的加权平均值：\n$$\nE[\\Delta T_{indiv}] = P_{hit} \\cdot \\Delta T_{hit} + (1 - P_{hit}) \\cdot \\Delta T_{miss}\n$$\n$$\nE[\\Delta T_{indiv}] = P_{hit}(t_{e} - t_{h} - t_{\\ell} - t_{r}) + (1 - P_{hit})(-t_{h} - t_{\\ell} - t_{s})\n$$\n展开表达式：\n$$\nE[\\Delta T_{indiv}] = P_{hit} t_{e} - P_{hit}(t_{h} + t_{\\ell} + t_{r}) - (t_{h} + t_{\\ell} + t_{s}) + P_{hit}(t_{h} + t_{\\ell} + t_{s})\n$$\n$$\nE[\\Delta T_{indiv}] = P_{hit} t_{e} - P_{hit} t_{h} - P_{hit} t_{\\ell} - P_{hit} t_{r} - t_{h} - t_{\\ell} - t_{s} + P_{hit} t_{h} + P_{hit} t_{\\ell} + P_{hit} t_{s}\n$$\n消去并重新整理各项：\n$$\nE[\\Delta T_{indiv}] = P_{hit}(t_{e} + t_{s} - t_{r}) - (t_{h} + t_{\\ell} + t_{s})\n$$\n每代期望节省的总净时间是该值的 $N$ 倍：\n$$\n\\Delta T = N \\left[ P_{hit}(t_{e} + t_{s} - t_{r}) - (t_{h} + t_{\\ell} + t_{s}) \\right]\n$$\n代入 $P_{hit}$ 的表达式：\n$$\n\\Delta T = N \\left[ (p_{c} f_{c} + p_{m} (1 - f_{c}))(t_{e} + t_{s} - t_{r}) - (t_{h} + t_{\\ell} + t_{s}) \\right]\n$$\n\n### 步骤 3：数值计算\n\n现在我们将给定的数值代入推导出的公式中。\n-   $N = 250$\n-   $f_{c} = 0.85$\n-   $p_{c} = 0.42$\n-   $p_{m} = 0.07$\n-   $t_{e} = 18.5$ 秒\n-   $t_{h} = 8.0 \\times 10^{-4}$ 秒 $= 0.0008$ 秒\n-   $t_{\\ell} = 5.0 \\times 10^{-4}$ 秒 $= 0.0005$ 秒\n-   $t_{r} = 3.0 \\times 10^{-4}$ 秒 $= 0.0003$ 秒\n-   $t_{s} = 6.0 \\times 10^{-4}$ 秒 $= 0.0006$ 秒\n\n首先，计算总命中概率 $P_{hit}$：\n$$\nP_{hit} = (0.85)(0.42) + (1 - 0.85)(0.07) = 0.357 + (0.15)(0.07) = 0.357 + 0.0105 = 0.3675\n$$\n接下来，计算 $\\Delta T$ 公式括号内的两个主要项。\n与节省相关的项：\n$$\nt_{e} + t_{s} - t_{r} = 18.5 + 0.0006 - 0.0003 = 18.5003 \\text{ 秒}\n$$\n与开销相关的项：\n$$\nt_{h} + t_{\\ell} + t_{s} = 0.0008 + 0.0005 + 0.0006 = 0.0019 \\text{ 秒}\n$$\n现在，将这些代入 $\\Delta T$ 的表达式中：\n$$\n\\Delta T = 250 \\left[ (0.3675)(18.5003) - 0.0019 \\right]\n$$\n$$\n\\Delta T = 250 \\left[ 6.79886025 - 0.0019 \\right]\n$$\n$$\n\\Delta T = 250 \\left[ 6.79696025 \\right]\n$$\n$$\n\\Delta T = 1699.2400625 \\text{ 秒}\n$$\n问题要求将答案四舍五入到 $4$ 位有效数字。数字是 $1699.2400625$。前四位有效数字是 $1$、$6$、$9$ 和 $9$。第五位数字是 $2$，小于 $5$，所以我们向下取整（截断）。\n$$\n\\Delta T \\approx 1699 \\text{ 秒}\n$$",
            "answer": "$$\n\\boxed{1699}\n$$"
        },
        {
            "introduction": "在复杂的电磁设计问题中，我们常常可以同时使用快速但精度较低的低保真度模型（如矩量法）和慢速但精度更高的高保真度模型（如有限元法）。多保真度优化是一种先进策略，它利用廉价模型进行广泛探索，并借助昂贵模型对有希望的候选解进行精确评估。这项高级实践  将引导您从零开始构建一个完整的多保真度进化算法，您将实现不同保真度的物理模型、用于学习模型差异的“传递映射”以及核心优化逻辑，从而展示如何在有限的计算预算下获得更优的设计。",
            "id": "3306133",
            "problem": "您的任务是设计一种多保真度演化优化算法，用于计算电磁学中反射阵列在不同角度和频率下的相位调谐，该算法通过学习到的传递映射，结合了低保真度的矩量法 (MoM) 模型和高保真度的有限元法 (FEM) 模型。设计变量是分配给一维反射阵列的相位向量。目标是在符合实际电磁建模假设的条件下，最大化在一组频率上指定目标角度的平均归一化主瓣功率。您的程序必须实现低保真度和高保真度模型，学习传递映射，并在相同的昂贵评估预算下，将多保真度演化算法的性能与纯高保真度基准算法进行比较。\n\n基本基础、定义和建模假设：\n1. 从无源均匀空间中的时谐麦克斯韦方程组开始，其中角频率为 $\\omega = 2 \\pi f$，介电常数为 $\\epsilon$，磁导率为 $\\mu$，光速为 $c = 1/\\sqrt{\\epsilon \\mu}$。波数为 $k(f) = \\omega \\sqrt{\\epsilon \\mu} = 2 \\pi f / c$。反射阵列单元被建模为以可控相移进行再辐射，当单元尺寸远小于波长时，在标准近似下，主平面内的远场方向图可由阵列因子描述。\n2. 一个由 $M$ 个单元组成的一维反射阵列，其位置 $x_i$ 以 $s$ 米的间距均匀分布，并以 $\\sum_i x_i = 0$ 为中心。在角度 $\\theta$ 和频率 $f$ 下的波束形成目标由各单元贡献 $\\exp(j \\phi_i(\\theta,f))$ 的相长叠加驱动，其中 $j$ 是虚数单位，$\\phi_i(\\theta,f)$ 表示第 $i$ 个单元贡献的总相位。\n3. 基于矩量法 (MoM) 的低保真度模型：忽略互耦和单元色散。对于相位向量 $\\mathbf{p} = (p_0,\\dots,p_{M-1})$（其中 $p_i \\in [0,2\\pi)$）和频率集 $\\mathcal{F}$，在角度 $\\theta$ 和频率 $f$ 下的低保真度阵列因子模平方为\n$$\nP_L(\\mathbf{p}; \\theta, f) = \\frac{1}{M^2} \\left| \\sum_{i=0}^{M-1} \\exp\\left( j \\left( k(f) x_i \\sin \\theta + p_i \\right) \\right) \\right|^2.\n$$\n定义在频率上平均的低保真度性能为\n$$\n\\bar{P}_L(\\mathbf{p}; \\theta, \\mathcal{F}) = \\frac{1}{|\\mathcal{F}|} \\sum_{f \\in \\mathcal{F}} P_L(\\mathbf{p}; \\theta, f).\n$$\n4. 基于有限元法 (FEM) 的高保真度模型：引入针对互耦和频率相关的单元幅度色散的简单、确定性的参数化校正。设 $f_0$ 为参考（中心）频率。对每个单元 $i$，定义一个幅度因子\n$$\nt_i(f) = 1 + \\gamma \\left( \\frac{f}{f_0} - 1 \\right) \\cos\\left( \\frac{2 \\pi i}{M} \\right),\n$$\n和一个相位校正\n$$\n\\Delta_i(\\mathbf{p}; \\theta, f) = \\alpha \\sum_{j \\in \\mathcal{N}(i)} \\sin(p_i - p_j) + \\beta \\left( \\frac{f}{f_0} - 1 \\right) \\sin \\theta \\cdot \\frac{i}{M-1},\n$$\n其中 $\\mathcal{N}(i)$ 表示最近邻（当索引 $i-1$ 和 $i+1$ 存在时）。高保真度模平方定义为\n$$\nP_H(\\mathbf{p}; \\theta, f) = \\frac{1}{M^2} \\left| \\sum_{i=0}^{M-1} t_i(f) \\exp\\left( j \\left( k(f) x_i \\sin \\theta + p_i + \\Delta_i(\\mathbf{p}; \\theta, f) \\right) \\right) \\right|^2,\n$$\n在频率上平均的高保真度性能为\n$$\n\\bar{P}_H(\\mathbf{p}; \\theta, \\mathcal{F}) = \\frac{1}{|\\mathcal{F}|} \\sum_{f \\in \\mathcal{F}} P_H(\\mathbf{p}; \\theta, f).\n$$\n这些选择代表了一种科学上合理且与互耦和色散的定性效应一致的参数化校正，同时在计算上保持易于处理。\n\n学习传递映射：\n对于一个设计 $\\mathbf{p}$，定义残差（模型差异）为\n$$\nr(\\mathbf{p}) = \\bar{P}_H(\\mathbf{p}; \\theta, \\mathcal{F}) - \\bar{P}_L(\\mathbf{p}; \\theta, \\mathcal{F}).\n$$\n构造一个特征向量 $\\boldsymbol{\\phi}(\\mathbf{p})$，通过线性回归学习一个传递映射：\n- 偏置项：$1$。\n- 低保真度性能：$\\bar{P}_L(\\mathbf{p}; \\theta, \\mathcal{F})$。\n- 平均最近邻余弦：$\\frac{1}{M-1} \\sum_{i=0}^{M-2} \\cos(p_i - p_{i+1})$。\n- 相位方差：$\\frac{1}{M} \\sum_{i=0}^{M-1} (p_i - \\bar{p})^2$，其中 $\\bar{p} = \\frac{1}{M} \\sum_i p_i$。\n- 一次谐波相干性：$\\frac{1}{M} \\left| \\sum_{i=0}^{M-1} \\exp(j p_i) \\right|$。\n使用最小二乘法拟合权重 $\\mathbf{w}$，使得\n$$\n\\Delta(\\mathbf{p}) \\approx \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{p}) \\approx r(\\mathbf{p}),\n$$\n并定义一个代理模型为\n$$\n\\bar{P}_S(\\mathbf{p}; \\theta, \\mathcal{F}) = \\bar{P}_L(\\mathbf{p}; \\theta, \\mathcal{F}) + \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{p}).\n$$\n\n优化目标：\n在 $\\mathbf{p}$ 上最大化 $\\bar{P}(\\mathbf{p})$，其中 $\\bar{P}$ 表示高保真度性能 $\\bar{P}_H$（用于评估）或代理模型 $\\bar{P}_S$（用于廉价的指导）。相位必须满足 $p_i \\in [0,2\\pi)$，并在变异后进行模 $2\\pi$ 回卷。\n\n需要实现的算法：\n1. 基准算法（仅高保真度）：一个简单的昂贵局部演化搜索，它从一个基于物理的初始猜测开始，并使用高斯扰动迭代地变异最佳设计，根据 $\\bar{P}_H$ 接受改进，并消耗昂贵的评估预算。\n2. 多保真度算法（MoM + FEM + 学习传递）：使用对 $\\bar{P}_L$ 的廉价评估和学习到的传递 $\\Delta(\\cdot)$ 来指导探索。周期性地用 $\\bar{P}_H$ 评估由代理模型排序的顶尖候选解，根据累积的数据通过最小二乘法更新传递映射，并消耗与基准算法相同的昂贵评估预算。\n\n基本初始化：\n定义中心频率 $f_0$ 和基于物理的初始相位向量为\n$$\np_i^{(0)} = \\mathrm{wrap}\\left( - k(f_0) x_i \\sin \\theta \\right)\n$$\n以近似地在 $(\\theta, f_0)$ 处对齐贡献，其中 $\\mathrm{wrap}(\\cdot)$ 将相位映射到 $[0,2\\pi)$。\n\n单位：\n- 角度 $\\theta$ 必须以弧度（rad）指定。\n- 频率 $f$ 必须以赫兹（Hz）指定。\n- 间距 $s$ 必须以米（m）指定。\n- 光速 $c$ 必须以米/秒（m/s）指定。\n- 目标 $\\bar{P}$ 是无量纲的，在给定的归一化下位于 $[0,1]$ 区间。\n\n测试套件：\n为以下三个测试用例实现算法。在每个用例中，将改进量报告为一个浮点数：\n$$\n\\Delta_{\\mathrm{imp}} = \\left( \\max_{\\text{budget}} \\bar{P}_H \\text{ under multifidelity} \\right) - \\left( \\max_{\\text{budget}} \\bar{P}_H \\text{ under baseline} \\right).\n$$\n预算指的是高保真度（FEM）评估的最大次数；廉价的低保真度（MoM）评估允许有更多的次数。\n\n- 用例1（理想情况）：\n  - $M = 16$， $s = 0.015$ 米， $\\theta = 0.523599$ 弧度。\n  - $\\mathcal{F} = \\{9.5 \\times 10^9, 1.0 \\times 10^{10}, 1.05 \\times 10^{10}\\}$ Hz, $f_0 = 1.0 \\times 10^{10}$ Hz。\n  - 高保真度参数：$\\alpha = 0.08$， $\\beta = 0.15$， $\\gamma = 0.10$。\n  - 预算：最多 $60$ 次高保真度评估。\n\n- 用例2（边界情况，小阵列和近轴向）：\n  - $M = 4$， $s = 0.030$ 米， $\\theta = 0.0$ 弧度。\n  - $\\mathcal{F} = \\{9.8 \\times 10^9, 1.0 \\times 10^{10}, 1.02 \\times 10^{10}\\}$ Hz, $f_0 = 1.0 \\times 10^{10}$ Hz。\n  - 高保真度参数：$\\alpha = 0.02$， $\\beta = 0.05$， $\\gamma = 0.05$。\n  - 预算：最多 $20$ 次高保真度评估。\n\n- 用例3（挑战性情况，更强的耦合和更宽的带宽）：\n  - $M = 32$， $s = 0.015$ 米， $\\theta = 0.785398$ 弧度。\n  - $\\mathcal{F} = \\{9.0 \\times 10^9, 1.0 \\times 10^{10}, 1.10 \\times 10^{10}\\}$ Hz, $f_0 = 1.0 \\times 10^{10}$ Hz。\n  - 高保真度参数：$\\alpha = 0.15$， $\\beta = 0.25$， $\\gamma = 0.15$。\n  - 预算：最多 $80$ 次高保真度评估。\n\n算法约束：\n- 为确保可复现性，每个测试用例使用一个固定的随机种子。\n- 对于每个用例，基准和多保真度算法消耗的高保真度评估次数不得超过其各自的预算。\n- 传递映射必须通过对一个初始成对评估集进行最小二乘法学习，并在收集到新的成对数据时进行更新。\n- 每次变异后，相位必须进行模 $2\\pi$ 回卷。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按顺序包含三个测试用例的结果，每个改进量格式化为十进制浮点数（而非百分比），例如，“[0.012345,-0.000123,0.054321]”。不应打印任何其他文本。",
            "solution": "The user has provided a problem that requires the design, implementation, and comparison of two evolutionary optimization algorithms for a computational electromagnetics application: reflectarray phase tuning. The problem is to be validated for scientific soundness, completeness, and clarity before a solution is attempted.\n\n### **Step 1: Problem Validation**\n\n#### **Extraction of Givens**\n\n- **Physical Model**: The problem is based on time-harmonic Maxwell's equations. Key entities include angular frequency $\\omega = 2 \\pi f$, speed of light $c = 1/\\sqrt{\\epsilon \\mu}$, and wavenumber $k(f) = 2 \\pi f / c$. The system is a 1D reflectarray of $M$ elements at positions $x_i$ with uniform spacing $s$ and $\\sum_i x_i = 0$.\n- **Low-Fidelity Model ($\\bar{P}_L$)**: This model neglects mutual coupling and element dispersion. Its performance is given by the frequency-averaged array factor:\n$$\n\\bar{P}_L(\\mathbf{p}; \\theta, \\mathcal{F}) = \\frac{1}{|\\mathcal{F}|} \\sum_{f \\in \\mathcal{F}} \\frac{1}{M^2} \\left| \\sum_{i=0}^{M-1} \\exp\\left( j \\left( k(f) x_i \\sin \\theta + p_i \\right) \\right) \\right|^2\n$$\n- **High-Fidelity Model ($\\bar{P}_H$)**: This model includes parametric corrections for mutual coupling and dispersion. The frequency-averaged performance is:\n$$\n\\bar{P}_H(\\mathbf{p}; \\theta, \\mathcal{F}) = \\frac{1}{|\\mathcal{F}|} \\sum_{f \\in \\mathcal{F}} \\frac{1}{M^2} \\left| \\sum_{i=0}^{M-1} t_i(f) \\exp\\left( j \\left( k(f) x_i \\sin \\theta + p_i + \\Delta_i(\\mathbf{p}; \\theta, f) \\right) \\right) \\right|^2\n$$\nwhere $t_i(f)$ is an amplitude factor and $\\Delta_i$ is a phase correction term defined as:\n$$\nt_i(f) = 1 + \\gamma \\left( \\frac{f}{f_0} - 1 \\right) \\cos\\left( \\frac{2 \\pi i}{M} \\right)\n$$\n$$\n\\Delta_i(\\mathbf{p}; \\theta, f) = \\alpha \\sum_{j \\in \\mathcal{N}(i)} \\sin(p_i - p_j) + \\beta \\left( \\frac{f}{f_0} - 1 \\right) \\sin \\theta \\cdot \\frac{i}{M-1}\n$$\n- **Learned Transfer Mapping**: A surrogate model $\\bar{P}_S$ is defined to approximate $\\bar{P}_H$ by correcting $\\bar{P}_L$ with a learned residual $\\Delta(\\mathbf{p})$.\n$$\n\\bar{P}_S(\\mathbf{p}; \\theta, \\mathcal{F}) = \\bar{P}_L(\\mathbf{p}; \\theta, \\mathcal{F}) + \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{p})\n$$\nThe weights $\\mathbf{w}$ are found via least squares regression on the residual $r(\\mathbf{p}) = \\bar{P}_H(\\mathbf{p}) - \\bar{P}_L(\\mathbf{p})$. The feature vector $\\boldsymbol{\\phi}(\\mathbf{p})$ consists of a bias term, $\\bar{P}_L$, mean nearest-neighbor phase cosine, phase variance, and first-harmonic coherence of the phase vector $\\mathbf{p}$.\n- **Optimization**: The objective is to maximize the high-fidelity performance. The design variables are the phases $p_i \\in [0, 2\\pi)$.\n- **Algorithms**: A baseline evolutionary search using only the high-fidelity model is to be compared against a multifidelity algorithm that leverages the low-fidelity and surrogate models to guide the search. Both operate under a fixed budget of high-fidelity evaluations.\n- **Initialization**: A physically motivated initial phase vector is defined as $p_i^{(0)} = \\mathrm{wrap}\\left( -k(f_0) x_i \\sin \\theta \\right)$.\n- **Test Suite**: Three specific test cases are provided with all necessary parameters ($M, s, \\theta, \\mathcal{F}, f_0, \\alpha, \\beta, \\gamma$) and evaluation budgets.\n- **Output**: The required output is the performance improvement $\\Delta_{\\mathrm{imp}}$ of the multifidelity algorithm over the baseline for each test case.\n\n#### **Validation Verdict**\n\n- **Scientifically Grounded**: The problem is valid. The formulation is well-grounded in the principles of antenna theory and computational electromagnetics. The low-fidelity model represents the ideal array factor, a standard analytical tool. The high-fidelity model introduces scientifically plausible parametric corrections that qualitatively model real-world effects like mutual coupling and dispersion in a computationally tractable manner.\n- **Well-Posed and Objective**: The problem is well-posed. The objective functions, constraints, and performance metrics are defined with mathematical precision. The task, which is to implement and compare two specified algorithmic strategies under a fixed computational budget and random seed, is a well-defined computational experiment.\n- **Completeness and Consistency**: The problem is self-contained and provides all necessary definitions, formulas, and parameters for each test case. The only missing constant is the speed of light, $c$, which is a universal physical constant, and its standard value can be assumed. The problem statement is internally consistent.\n\nThe problem satisfies all criteria for validity. It is a rigorous and challenging task in a specialized area of computational science and engineering.\n\n### **Step 2: Solution Design**\n\nThe solution requires implementing the physical models, the learning framework, and the two optimization algorithms as specified.\n\n#### **Modeling and Framework**\n\n1.  **Physical Models**: Functions will be implemented for both $\\bar{P}_L(\\mathbf{p})$ and $\\bar{P}_H(\\mathbf{p})$. These functions will take a phase vector $\\mathbf{p}$ and the case-specific physical parameters as input. The calculations will involve complex number arithmetic, as is standard in phasor-based analysis. The position vector $x_i$ for the centered array is calculated as $x_i = s \\cdot (i - (M-1)/2)$ for $i=0, \\dots, M-1$.\n2.  **Surrogate Framework**: A set of functions will manage the surrogate model.\n    - `calculate_features`: Computes the 5-dimensional feature vector $\\boldsymbol{\\phi}(\\mathbf{p})$ from a given phase vector $\\mathbf{p}$ and its corresponding $\\bar{P}_L$ value.\n    - `update_transfer_mapping`: Takes a dataset of high-fidelity evaluations `(p, p_l, p_h)` and uses `numpy.linalg.lstsq` to compute the optimal weight vector $\\mathbf{w}$ for the linear surrogate model.\n    - `surrogate_model`: Predicts the high-fidelity performance for a new phase vector $\\mathbf{p}$ using the current weights $\\mathbf{w}$ via $\\bar{P}_S(\\mathbf{p}) = \\bar{P}_L(\\mathbf{p}) + \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{p})$.\n\n#### **Optimization Algorithms**\n\nA fixed random seed will be used for each test case to ensure reproducibility of the stochastic algorithms.\n\n1.  **Baseline Algorithm**: This is a simple evolutionary search.\n    - It starts with the physically motivated initial phase vector $p^{(0)}$.\n    - It maintains the best solution found so far, $\\mathbf{p}_{\\text{best}}$.\n    - In each step, it generates a new candidate by applying a Gaussian mutation to $\\mathbf{p}_{\\text{best}}$: $\\mathbf{p}_{\\text{mut}} = \\mathrm{wrap}(\\mathbf{p}_{\\text{best}} + \\mathcal{N}(0, \\sigma^2))$.\n    - The new candidate is evaluated using the expensive high-fidelity model $\\bar{P}_H$.\n    - If the new candidate is an improvement, it replaces $\\mathbf{p}_{\\text{best}}$.\n    - This process repeats until the budget of high-fidelity evaluations is exhausted. The final result is the maximum $\\bar{P}_H$ value observed during the entire search.\n\n2.  **Multifidelity Algorithm**: This algorithm intelligently allocates the expensive evaluation budget.\n    - **Initialization**: It begins by performing a small number of high-fidelity evaluations ($N_{\\text{initial}}$) on random or structured initial points to build an initial surrogate model.\n    - **Iterative Refinement**: The main loop consists of cycles that repeat until the budget is spent:\n        a. **Cheap Search**: An inner loop generates a pool of candidate solutions (`N_cheap_per_cycle`) by mutating the current best-known design. These candidates are evaluated using the fast surrogate model $\\bar{P}_S$.\n        b. **Infill Selection**: The most promising candidate from the pool (the one with the highest predicted $\\bar{P}_S$) is selected for expensive evaluation.\n        c. **Expensive Evaluation**: The selected candidate is evaluated using the true high-fidelity model $\\bar{P}_H$.\n        d. **Model Update**: The new data point $(\\mathbf{p}, \\bar{P}_L, \\bar{P}_H)$ is added to the training set, and the surrogate model weights $\\mathbf{w}$ are re-computed.\n    - The final result is the maximum $\\bar{P}_H$ value observed across all high-fidelity evaluations.\n\nFinally, for each test case, the performance improvement is calculated as $\\Delta_{\\mathrm{imp}} = (\\max \\bar{P}_{H, \\text{multi}}) - (\\max \\bar{P}_{H, \\text{base}})$ and reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ... # No scipy needed for this implementation\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization problems and print the final result.\n    \"\"\"\n    C_LIGHT = 299792458.0  # Speed of light in m/s\n\n    # -------------------------------------------------------------------------\n    # Helper and Model Functions\n    # -------------------------------------------------------------------------\n\n    def get_element_positions(M, s):\n        \"\"\"Computes positions for a centered 1D array.\"\"\"\n        return s * (np.arange(M) - (M - 1) / 2.0)\n\n    def wrap_phases(phases):\n        \"\"\"Wraps phases to the interval [0, 2*pi).\"\"\"\n        return np.mod(phases, 2 * np.pi)\n\n    def low_fidelity_model(p, M, x, theta, freqs, c):\n        \"\"\"Computes the frequency-averaged low-fidelity performance.\"\"\"\n        p_l_vals = []\n        for f in freqs:\n            k = 2 * np.pi * f / c\n            total_phases = k * x * np.sin(theta) + p\n            complex_sum = np.sum(np.exp(1j * total_phases))\n            p_l_at_f = (1.0 / M**2) * np.abs(complex_sum)**2\n            p_l_vals.append(p_l_at_f)\n        return np.mean(p_l_vals)\n\n    def high_fidelity_model(p, M, x, theta, freqs, f0, alpha, beta, gamma, c):\n        \"\"\"Computes the frequency-averaged high-fidelity performance.\"\"\"\n        p_h_vals = []\n        for f in freqs:\n            k = 2 * np.pi * f / c\n            f_ratio_term = (f / f0) - 1.0\n            complex_sum = 0.0\n            for i in range(M):\n                t_i = 1.0 + gamma * f_ratio_term * np.cos(2 * np.pi * i / M)\n                \n                coupling_term = 0.0\n                if M > 1:\n                    if i > 0:\n                        coupling_term += np.sin(p[i] - p[i-1])\n                    if i  M - 1:\n                        coupling_term += np.sin(p[i] - p[i+1])\n                coupling_term *= alpha\n                \n                dispersion_term = beta * f_ratio_term * np.sin(theta) * (i / (M - 1.0)) if M > 1 else 0.0\n                delta_i = coupling_term + dispersion_term\n                \n                total_phase = k * x[i] * np.sin(theta) + p[i] + delta_i\n                complex_sum += t_i * np.exp(1j * total_phase)\n            \n            p_h_at_f = (1.0 / M**2) * np.abs(complex_sum)**2\n            p_h_vals.append(p_h_at_f)\n        return np.mean(p_h_vals)\n\n    def calculate_features(p, p_l_val, M):\n        \"\"\"Computes the feature vector for the surrogate model.\"\"\"\n        features = np.zeros(5)\n        features[0] = 1.0  # Bias\n        features[1] = p_l_val  # Low-fidelity performance\n        if M > 1:\n            features[2] = np.mean(np.cos(p[:-1] - p[1:]))  # Mean nearest-neighbor cosine\n        else: # Case not in tests, but handle defensively.\n            features[2] = 1.0\n        features[3] = np.var(p)  # Phase variance\n        features[4] = np.abs(np.mean(np.exp(1j * p)))  # First-harmonic coherence\n        return features\n\n    def update_transfer_mapping(hf_data, M):\n        \"\"\"Learns the transfer mapping weights via least squares.\"\"\"\n        if not hf_data:\n            return np.zeros(5)\n        \n        num_points = len(hf_data)\n        Phi = np.zeros((num_points, 5))\n        r = np.zeros(num_points)\n        \n        for i, (p_vec, p_l, p_h) in enumerate(hf_data):\n            Phi[i, :] = calculate_features(p_vec, p_l, M)\n            r[i] = p_h - p_l\n        \n        weights, _, _, _ = np.linalg.lstsq(Phi, r, rcond=None)\n        return weights\n\n    def surrogate_model(p, p_l_val, w, M):\n        \"\"\"Predicts high-fidelity performance using the surrogate.\"\"\"\n        features = calculate_features(p, p_l_val, M)\n        correction = np.dot(w, features)\n        return p_l_val + correction\n\n    # -------------------------------------------------------------------------\n    # Optimization Algorithms\n    # -------------------------------------------------------------------------\n\n    def run_baseline_ea(case_params, seed, c):\n        \"\"\"Runs the high-fidelity-only baseline evolutionary algorithm.\"\"\"\n        M, s, theta, freqs, f0, alpha, beta, gamma, budget = (\n            case_params['M'], case_params['s'], case_params['theta'], case_params['freqs'],\n            case_params['f0'], case_params['alpha'], case_params['beta'], case_params['gamma'],\n            case_params['budget'])\n        \n        rng = np.random.default_rng(seed)\n        x = get_element_positions(M, s)\n        mutation_std = 0.2\n\n        k0 = 2 * np.pi * f0 / c\n        p_initial = wrap_phases(-k0 * x * np.sin(theta))\n        \n        p_h_initial = high_fidelity_model(p_initial, M, x, theta, freqs, f0, alpha, beta, gamma, c)\n        eval_count = 1\n        \n        best_p = p_initial\n        best_p_h = p_h_initial\n        all_p_h_values = [p_h_initial]\n\n        while eval_count  budget:\n            p_mutant = wrap_phases(best_p + rng.normal(loc=0.0, scale=mutation_std, size=M))\n            \n            p_h_mutant = high_fidelity_model(p_mutant, M, x, theta, freqs, f0, alpha, beta, gamma, c)\n            eval_count += 1\n            all_p_h_values.append(p_h_mutant)\n            \n            if p_h_mutant > best_p_h:\n                best_p_h = p_h_mutant\n                best_p = p_mutant\n                \n        return np.max(all_p_h_values)\n\n    def run_multifidelity_ea(case_params, seed, c):\n        \"\"\"Runs the multifidelity evolutionary algorithm.\"\"\"\n        M, s, theta, freqs, f0, alpha, beta, gamma, budget = (\n            case_params['M'], case_params['s'], case_params['theta'], case_params['freqs'],\n            case_params['f0'], case_params['alpha'], case_params['beta'], case_params['gamma'],\n            case_params['budget'])\n\n        rng = np.random.default_rng(seed)\n        x = get_element_positions(M, s)\n        mutation_std = 0.2\n        N_initial = 5\n        N_cheap_per_cycle = 50\n\n        hf_data = []  # List of (p, p_l, p_h) tuples\n        eval_count = 0\n        \n        k0 = 2 * np.pi * f0 / c\n        p_initial = wrap_phases(-k0 * x * np.sin(theta))\n        initial_designs = [p_initial] + [wrap_phases(rng.uniform(0, 2 * np.pi, M)) for _ in range(N_initial - 1)]\n\n        for p in initial_designs:\n            if eval_count >= budget: break\n            p_l = low_fidelity_model(p, M, x, theta, freqs, c)\n            p_h = high_fidelity_model(p, M, x, theta, freqs, f0, alpha, beta, gamma, c)\n            eval_count += 1\n            hf_data.append((p, p_l, p_h))\n\n        weights = update_transfer_mapping(hf_data, M)\n\n        while eval_count  budget:\n            best_hf_point = max(hf_data, key=lambda item: item[2])\n            p_base_for_mutation = best_hf_point[0]\n            \n            cheap_candidates = []\n            for _ in range(N_cheap_per_cycle):\n                p_mutant = wrap_phases(p_base_for_mutation + rng.normal(0.0, scale=mutation_std, size=M))\n                p_l_mutant = low_fidelity_model(p_mutant, M, x, theta, freqs, c)\n                p_s_mutant = surrogate_model(p_mutant, p_l_mutant, weights, M)\n                cheap_candidates.append((p_mutant, p_s_mutant))\n                \n            best_cheap_p, _ = max(cheap_candidates, key=lambda item: item[1])\n            \n            p_l_new = low_fidelity_model(best_cheap_p, M, x, theta, freqs, c)\n            p_h_new = high_fidelity_model(best_cheap_p, M, x, theta, freqs, f0, alpha, beta, gamma, c)\n            eval_count += 1\n\n            hf_data.append((best_cheap_p, p_l_new, p_h_new))\n            weights = update_transfer_mapping(hf_data, M)\n            \n        return np.max([item[2] for item in hf_data])\n\n    # -------------------------------------------------------------------------\n    # Main Execution\n    # -------------------------------------------------------------------------\n\n    test_cases = [\n        # Case 1\n        {\n            \"M\": 16, \"s\": 0.015, \"theta\": 0.523599,\n            \"freqs\": np.array([9.5e9, 1.0e10, 1.05e10]), \"f0\": 1.0e10,\n            \"alpha\": 0.08, \"beta\": 0.15, \"gamma\": 0.10, \"budget\": 60\n        },\n        # Case 2\n        {\n            \"M\": 4, \"s\": 0.030, \"theta\": 0.0,\n            \"freqs\": np.array([9.8e9, 1.0e10, 1.02e10]), \"f0\": 1.0e10,\n            \"alpha\": 0.02, \"beta\": 0.05, \"gamma\": 0.05, \"budget\": 20\n        },\n        # Case 3\n        {\n            \"M\": 32, \"s\": 0.015, \"theta\": 0.785398,\n            \"freqs\": np.array([9.0e9, 1.0e10, 1.10e10]), \"f0\": 1.0e10,\n            \"alpha\": 0.15, \"beta\": 0.25, \"gamma\": 0.15, \"budget\": 80\n        }\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        seed = i\n        \n        max_ph_baseline = run_baseline_ea(case, seed, C_LIGHT)\n        max_ph_multi = run_multifidelity_ea(case, seed, C_LIGHT)\n        \n        improvement = max_ph_multi - max_ph_baseline\n        results.append(improvement)\n\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}