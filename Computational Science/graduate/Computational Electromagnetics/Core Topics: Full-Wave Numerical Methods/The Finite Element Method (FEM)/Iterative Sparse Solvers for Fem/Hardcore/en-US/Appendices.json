{
    "hands_on_practices": [
        {
            "introduction": "Before an iterative solver can be applied, the linear system must be constructed from the underlying physics and mesh. This fundamental exercise guides you through the process of manual matrix assembly for the Maxwell curl-curl operator on a small, well-defined 2D mesh . By explicitly building the discrete curl matrix $C$ and mass matrices, you will gain a concrete understanding of how mesh topology, element orientation, and degree-of-freedom ordering directly influence the final system's sparsity pattern and bandwidth.",
            "id": "3321760",
            "problem": "Consider the frequency-domain Maxwell curl-curl formulation in two spatial dimensions for a transverse electric field approximation in a conforming edge (Whitney $1$-form) finite element space on a small mesh. Let the computational domain be meshed by three triangles with counter-clockwise orientation and the following vertex coordinates:\n$$\n\\mathbf{v}_{0}=(0,0),\\quad \\mathbf{v}_{1}=(1,0),\\quad \\mathbf{v}_{2}=(0,1),\\quad \\mathbf{v}_{3}=(1,1),\\quad \\mathbf{v}_{4}=(2,0).\n$$\nThe elements (faces) are:\n$$\nK_{1}=\\triangle(\\mathbf{v}_{0},\\mathbf{v}_{1},\\mathbf{v}_{2}),\\quad K_{2}=\\triangle(\\mathbf{v}_{1},\\mathbf{v}_{3},\\mathbf{v}_{2}),\\quad K_{3}=\\triangle(\\mathbf{v}_{1},\\mathbf{v}_{4},\\mathbf{v}_{3}).\n$$\nUse the lowest-order Nédélec (Whitney $1$-form) basis functions associated with globally oriented edges. Define the following globally oriented edge set and ordering of the degrees of freedom (DOF):\n$$\ne_{1}=(\\mathbf{v}_{0}\\rightarrow\\mathbf{v}_{1}),\\quad\ne_{2}=(\\mathbf{v}_{0}\\rightarrow\\mathbf{v}_{2}),\\quad\ne_{3}=(\\mathbf{v}_{1}\\rightarrow\\mathbf{v}_{2}),\\quad\ne_{4}=(\\mathbf{v}_{1}\\rightarrow\\mathbf{v}_{3}),\\quad\ne_{5}=(\\mathbf{v}_{2}\\rightarrow\\mathbf{v}_{3}),\\quad\ne_{6}=(\\mathbf{v}_{1}\\rightarrow\\mathbf{v}_{4}),\\quad\ne_{7}=(\\mathbf{v}_{3}\\rightarrow\\mathbf{v}_{4}).\n$$\nLet the magnetic permeability be piecewise constant, $\\mu(\\mathbf{x})=\\mu_{k}>0$ on $K_{k}$ for $k\\in\\{1,2,3\\}$, and the electric permittivity be piecewise constant, $\\epsilon(\\mathbf{x})=\\epsilon_{k}>0$ on $K_{k}$ for $k\\in\\{1,2,3\\}$. Denote the angular frequency by $\\omega>0$. In the weak formulation, the curl-curl operator is represented by the matrix\n$$\nA = C^{\\top} M_{\\mu^{-1}} C,\n$$\nwhere $C$ is the face-edge incidence (discrete curl) matrix induced by the chosen global edge orientation and counter-clockwise face orientation, and $M_{\\mu^{-1}}$ is the $L^{2}$ Hodge star for $2$-forms (faces) with weight $\\mu^{-1}$. The $L^{2}$ Hodge star on edges (Whitney $1$-forms) with weight $\\epsilon$ is the mass matrix $M_{\\epsilon}$.\n\nStarting from Maxwell’s equations and the weak $H(\\mathrm{curl})$ formulation, and from the barycentric-coordinate definitions of Whitney $1$-forms, do all of the following for the above mesh and DOF ordering:\n\n1. Explicitly construct the face-edge incidence matrix $C\\in\\mathbb{R}^{3\\times 7}$ using the given global edge orientations and counter-clockwise face orientations.\n2. Using the constancy of $\\mu$ on each triangle and the piecewise-constant curl of Whitney $1$-forms on a triangle, construct $M_{\\mu^{-1}}\\in\\mathbb{R}^{3\\times 3}$.\n3. Using barycentric coordinates on each triangle and the definition of Whitney $1$-forms $w_{ij}=\\lambda_{i}\\nabla\\lambda_{j}-\\lambda_{j}\\nabla\\lambda_{i}$, compute the local $3\\times 3$ edge mass matrices on each element $K_{1}$, $K_{2}$, and $K_{3}$ with weight $\\epsilon_{k}$, and assemble them into the global edge mass matrix $M_{\\epsilon}\\in\\mathbb{R}^{7\\times 7}$.\n4. Determine the exact sparsity pattern (zero/nonzero structure) of the matrix $A+\\omega^{2} M_{\\epsilon}$ and, under the specified DOF ordering $e_{1},\\dots,e_{7}$, compute its half-bandwidth $b$, defined as\n$$\nb=\\max\\{|i-j| : (A+\\omega^{2}M_{\\epsilon})_{ij}\\neq 0\\}.\n$$\n\nReport the single integer $b$ as your final answer. No rounding is required. No units are to be included in the final reported value.",
            "solution": "The user has provided a well-defined problem in computational electromagnetics. The problem requires the calculation of the half-bandwidth of a finite element system matrix for a specific 2D mesh. I will proceed by following the four tasks outlined in the problem statement.\n\nThe vertices of the mesh are given by:\n$$\n\\mathbf{v}_{0}=(0,0),\\quad \\mathbf{v}_{1}=(1,0),\\quad \\mathbf{v}_{2}=(0,1),\\quad \\mathbf{v}_{3}=(1,1),\\quad \\mathbf{v}_{4}=(2,0).\n$$\nThe elements (triangles) are:\n$$\nK_{1}=\\triangle(\\mathbf{v}_{0},\\mathbf{v}_{1},\\mathbf{v}_{2}),\\quad K_{2}=\\triangle(\\mathbf{v}_{1},\\mathbf{v}_{3},\\mathbf{v}_{2}),\\quad K_{3}=\\triangle(\\mathbf{v}_{1},\\mathbf{v}_{4},\\mathbf{v}_{3}).\n$$\nThe globally ordered and oriented edges (Degrees of Freedom) are:\n$$\ne_{1}=(\\mathbf{v}_{0}\\rightarrow\\mathbf{v}_{1}),\\quad\ne_{2}=(\\mathbf{v}_{0}\\rightarrow\\mathbf{v}_{2}),\\quad\ne_{3}=(\\mathbf{v}_{1}\\rightarrow\\mathbf{v}_{2}),\\quad\ne_{4}=(\\mathbf{v}_{1}\\rightarrow\\mathbf{v}_{3}),\\quad\ne_{5}=(\\mathbf{v}_{2}\\rightarrow\\mathbf{v}_{3}),\\quad\ne_{6}=(\\mathbf{v}_{1}\\rightarrow\\mathbf{v}_{4}),\\quad\ne_{7}=(\\mathbf{v}_{3}\\rightarrow\\mathbf{v}_{4}).\n$$\n\nThe final goal is to determine the half-bandwidth $b$ of the matrix $S = A+\\omega^{2} M_{\\epsilon}$, defined as $b=\\max\\{|i-j| : S_{ij}\\neq 0\\}$. The sparsity pattern of $S$ is the union of the sparsity patterns of $A$ and $M_{\\epsilon}$, assuming no coincidental cancellations for arbitrary positive material parameters $\\mu_k, \\epsilon_k$ and frequency $\\omega$. An entry is considered structurally non-zero if it is non-zero for a generic choice of these parameters.\n\n**1. Construction of the Face-Edge Incidence Matrix $C$**\n\nThe face-edge incidence matrix $C \\in \\mathbb{R}^{3\\times 7}$ encodes the topological relationship between faces (elements) and edges. The entry $C_{ki}$ is $+1$ if edge $e_i$ is part of the boundary of face $K_k$ with matching orientation (counter-clockwise for the face), $-1$ if the orientation is opposite, and $0$ if the edge is not on the boundary of the face.\n\n- **Face $K_1 = \\triangle(\\mathbf{v}_0, \\mathbf{v}_1, \\mathbf{v}_2)$:** The counter-clockwise boundary is $\\mathbf{v}_0 \\to \\mathbf{v}_1 \\to \\mathbf{v}_2 \\to \\mathbf{v}_0$.\n  - Edge $e_1 = (\\mathbf{v}_0 \\to \\mathbf{v}_1)$: Matches orientation. $C_{11} = +1$.\n  - Edge $e_3 = (\\mathbf{v}_1 \\to \\mathbf{v}_2)$: Matches orientation. $C_{13} = +1$.\n  - Edge $e_2 = (\\mathbf{v}_0 \\to \\mathbf{v}_2)$: Opposes boundary segment $(\\mathbf{v}_2 \\to \\mathbf{v}_0)$. $C_{12} = -1$.\n  - Row 1 of $C$ is $\\begin{pmatrix} 1 & -1 & 1 & 0 & 0 & 0 & 0 \\end{pmatrix}$.\n\n- **Face $K_2 = \\triangle(\\mathbf{v}_1, \\mathbf{v}_3, \\mathbf{v}_2)$:** The counter-clockwise boundary is $\\mathbf{v}_1 \\to \\mathbf{v}_3 \\to \\mathbf{v}_2 \\to \\mathbf{v}_1$.\n  - Edge $e_4 = (\\mathbf{v}_1 \\to \\mathbf{v}_3)$: Matches orientation. $C_{24} = +1$.\n  - Edge $e_5 = (\\mathbf{v}_2 \\to \\mathbf{v}_3)$: Opposes boundary segment $(\\mathbf{v}_3 \\to \\mathbf{v}_2)$. $C_{25} = -1$.\n  - Edge $e_3 = (\\mathbf{v}_1 \\to \\mathbf{v}_2)$: Opposes boundary segment $(\\mathbf{v}_2 \\to \\mathbf{v}_1)$. $C_{23} = -1$.\n  - Row 2 of $C$ is $\\begin{pmatrix} 0 & 0 & -1 & 1 & -1 & 0 & 0 \\end{pmatrix}$.\n\n- **Face $K_3 = \\triangle(\\mathbf{v}_1, \\mathbf{v}_4, \\mathbf{v}_3)$:** The counter-clockwise boundary is $\\mathbf{v}_1 \\to \\mathbf{v}_4 \\to \\mathbf{v}_3 \\to \\mathbf{v}_1$.\n  - Edge $e_6 = (\\mathbf{v}_1 \\to \\mathbf{v}_4)$: Matches orientation. $C_{36} = +1$.\n  - Edge $e_7 = (\\mathbf{v}_3 \\to \\mathbf{v}_4)$: Opposes boundary segment $(\\mathbf{v}_4 \\to \\mathbf{v}_3)$. $C_{37} = -1$.\n  - Edge $e_4 = (\\mathbf{v}_1 \\to \\mathbf{v}_3)$: Opposes boundary segment $(\\mathbf{v}_3 \\to \\mathbf{v}_1)$. $C_{34} = -1$.\n  - Row 3 of $C$ is $\\begin{pmatrix} 0 & 0 & 0 & -1 & 0 & 1 & -1 \\end{pmatrix}$.\n\nCombining these rows, the matrix $C$ is:\n$$\nC = \\begin{pmatrix}\n1 & -1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & -1 & 1 & -1 & 0 & 0 \\\\\n0 & 0 & 0 & -1 & 0 & 1 & -1\n\\end{pmatrix}\n$$\n\n**2. Construction of the Matrix $M_{\\mu^{-1}}$**\n\nThe matrix $M_{\\mu^{-1}}$ is the Hodge star matrix mapping from 2-forms (faces) to dual 0-forms. For a basis of 2-forms that are characteristic functions on each element (face) and with piecewise constant material property $\\mu_k$ on element $K_k$, this matrix is diagonal. Its entries are given by $(M_{\\mu^{-1}})_{kk} = \\frac{|K_k|}{\\mu_k}$, where $|K_k|$ is the area of element $K_k$. Let's compute the areas of the triangles.\n- $|K_1| = \\frac{1}{2} | \\det(\\mathbf{v}_1 - \\mathbf{v}_0, \\mathbf{v}_2 - \\mathbf{v}_0) | = \\frac{1}{2} \\left| \\det\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right| = \\frac{1}{2}$.\n- $|K_2| = \\frac{1}{2} | \\det(\\mathbf{v}_3 - \\mathbf{v}_1, \\mathbf{v}_2 - \\mathbf{v}_1) | = \\frac{1}{2} \\left| \\det\\begin{pmatrix} 0 & -1 \\\\ 1 & 1 \\end{pmatrix} \\right| = \\frac{1}{2}$.\n- $|K_3| = \\frac{1}{2} | \\det(\\mathbf{v}_4 - \\mathbf{v}_1, \\mathbf{v}_3 - \\mathbf{v}_1) | = \\frac{1}{2} \\left| \\det\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right| = \\frac{1}{2}$.\n\nAll elements have an area of $1/2$. Thus, $M_{\\mu^{-1}}$ is a $3 \\times 3$ diagonal matrix:\n$$\nM_{\\mu^{-1}} = \\begin{pmatrix}\n\\frac{1}{2\\mu_1} & 0 & 0 \\\\\n0 & \\frac{1}{2\\mu_2} & 0 \\\\\n0 & 0 & \\frac{1}{2\\mu_3}\n\\end{pmatrix}\n$$\nSince $\\mu_k > 0$, all diagonal entries are non-zero.\n\n**3. & 4. Sparsity Pattern of $A+\\omega^{2} M_{\\epsilon}$ and Half-Bandwidth**\n\nWe first determine the sparsity pattern of $A = C^{\\top} M_{\\mu^{-1}} C$. The entry $A_{ij}$ is given by:\n$$\nA_{ij} = (C^{\\top} M_{\\mu^{-1}} C)_{ij} = \\sum_{k=1}^3 \\sum_{l=1}^3 (C^{\\top})_{ik} (M_{\\mu^{-1}})_{kl} C_{lj}\n$$\nSince $M_{\\mu^{-1}}$ is diagonal, this simplifies to:\n$$\nA_{ij} = \\sum_{k=1}^3 C_{ki} (M_{\\mu^{-1}})_{kk} C_{kj}\n$$\nAn entry $A_{ij}$ is non-zero if and only if there exists at least one element $K_k$ such that both $C_{ki} \\neq 0$ and $C_{kj} \\neq 0$. This means that edges $e_i$ and $e_j$ must be part of the boundary of a common element $K_k$. This is the \"co-facial\" relationship.\n\nNext, we determine the sparsity pattern of the mass matrix $M_{\\epsilon}$. Its entries are given by:\n$$\n(M_\\epsilon)_{ij} = \\int_\\Omega \\epsilon(\\mathbf{x}) \\mathbf{w}_i \\cdot \\mathbf{w}_j \\, d\\Omega = \\sum_{k=1}^3 \\int_{K_k} \\epsilon_k \\mathbf{w}_i \\cdot \\mathbf{w}_j \\, d\\Omega\n$$\nwhere $\\mathbf{w}_i$ is the Whitney 1-form basis function associated with edge $e_i$. The support of $\\mathbf{w}_i$ is the union of elements adjacent to $e_i$. The integral $\\int_{K_k} \\mathbf{w}_i \\cdot \\mathbf{w}_j \\, d\\Omega$ is non-zero only if both $\\mathbf{w}_i$ and $\\mathbf{w}_j$ are non-zero on element $K_k$. This is true if and only if both edges $e_i$ and $e_j$ are edges of the element $K_k$. Therefore, for a generic mesh and positive material properties, $(M_\\epsilon)_{ij}$ is non-zero if and only if edges $e_i$ and $e_j$ are co-facial.\n\nBoth $A$ and $M_\\epsilon$ have the same sparsity pattern, defined by the co-facial edge-edge relationship. The system matrix $S = A+\\omega^{2} M_{\\epsilon}$ will also have this same sparsity pattern, as there are no structural zeros that would arise from cancellation for arbitrary positive $\\mu_k, \\epsilon_k, \\omega$.\n\nAn entry $S_{ij}$ is non-zero if and only if edges $e_i$ and $e_j$ belong to the same triangle. We identify the sets of edge indices for each triangle:\n- $K_1$: The edges are $e_1, e_2, e_3$. The index set is $\\{1, 2, 3\\}$.\n- $K_2$: The edges are $e_3, e_4, e_5$. The index set is $\\{3, 4, 5\\}$.\n- $K_3$: The edges are $e_4, e_6, e_7$. The index set is $\\{4, 6, 7\\}$.\n\nThe non-zero entries $(i,j)$ in the matrix $S$ are those where $i$ and $j$ both belong to one of these sets. We now find the half-bandwidth $b=\\max\\{|i-j| : S_{ij}\\neq 0\\}$ by checking the maximum index difference within each set.\n- For $\\{1, 2, 3\\}$: The maximum difference is $|3-1| = 2$.\n- For $\\{3, 4, 5\\}$: The maximum difference is $|5-3| = 2$.\n- For $\\{4, 6, 7\\}$: The maximum difference is $|7-4| = 3$.\n\nThe overall maximum difference is the maximum of these values.\n$$\nb = \\max(2, 2, 3) = 3.\n$$\nThis maximum difference occurs for the pair of indices $(4, 7)$. The edges $e_4$ and $e_7$ are both part of triangle $K_3$, so the entry $S_{47}$ (and $S_{74}$) is non-zero. The difference $|4-7|=3$ contributes to the half-bandwidth. No other pair of co-facial edges has a larger index difference.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "With a system matrix assembled, the focus shifts to solving the equation $Ax=b$ efficiently. This practice delves into the inner workings of the Conjugate Gradient (CG) method, the premier iterative solver for symmetric positive-definite systems that arise in many FEM applications . You will first derive the celebrated short-term recurrences that make CG so efficient, and then analyze its convergence rate, demonstrating how preconditioning can dramatically accelerate the solution by favorably reshaping the operator's spectrum.",
            "id": "3321767",
            "problem": "Consider the frequency-domain Maxwell curl-curl equation for the electric field in a simply connected, perfectly conducting cavity with positive, spatially varying material parameters: the magnetic permeability and electric permittivity. Let the weak form be discretized by the Finite Element Method (FEM) with first-order Nédélec edge elements on a conforming tetrahedral mesh, yielding the sparse linear system\n$$\nA x = b,\n$$\nwhere the symmetric positive definite (SPD) system matrix is the mass-augmented curl-curl operator\n$$\nA = C^{\\top} \\Lambda C + \\eta M.\n$$\nHere $C$ is the discrete curl operator, $\\Lambda$ is the diagonal matrix of inverse permeability weights, $M$ is the $L^{2}$ mass matrix associated with permittivity, and $\\eta > 0$ is a strictly positive augmentation parameter. Assume homogeneous Dirichlet boundary conditions in the tangential sense to ensure coercivity. Let $A$ be solved by the Conjugate Gradient (CG) method.\n\nStarting from the Galerkin optimality principle for CG—namely, that iteration $k$ minimizes the $A$-norm of the error over affine spaces spanned by Krylov subspaces—derive the short-term recurrence for CG applied to $A$, including:\n- The three-term update for the search directions.\n- The step-length and recurrence coefficients expressed in terms of inner products that are realizable with sparse matrix-vector operations.\n\nThen consider left preconditioning with an SPD preconditioner $P$: solve\n$$\nP^{-1} A x = P^{-1} b\n$$\nby the Preconditioned Conjugate Gradient (PCG) method. Using the polynomial approximation perspective for CG and the extremal property of Chebyshev polynomials, demonstrate how the spectrum of the preconditioned operator $P^{-1} A$ governs the asymptotic bound on the $A$-norm of the error after $k$ iterations, and give the bound in closed form in terms of the condition number of $P^{-1} A$.\n\nFinally, for a preconditioner $P$ constructed so that the spectrum of $P^{-1} A$ lies entirely within the interval $[\\theta_{\\min}, \\theta_{\\max}] = [0.5, 2]$, compute the numerical value of the theoretical upper bound factor on the reduction of the $A$-norm of the error after $k = 20$ PCG iterations. Round your answer to four significant figures. Express the final value as a dimensionless number.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, self-contained, and consistent. It represents a standard problem in the field of numerical linear algebra for computational science.\n\nThe problem is addressed in three parts as requested: derivation of the Conjugate Gradient (CG) algorithm, analysis of the Preconditioned Conjugate Gradient (PCG) convergence, and a final numerical calculation.\n\n### Part 1: Derivation of the Conjugate Gradient (CG) Algorithm\n\nThe Conjugate Gradient method generates a sequence of approximations $x_k$ to the solution of $Ax=b$. It is based on the Galerkin optimality principle over nested Krylov subspaces. For an initial guess $x_0$ with initial residual $r_0 = b - Ax_0$, the $k$-th iterate $x_k$ is chosen from the affine space $x_0 + \\mathcal{K}_k(A, r_0)$ such that the error $e_k = x - x_k$ is minimized in the $A$-norm, defined as $\\|e_k\\|_A = \\sqrt{e_k^\\top A e_k}$. The Krylov subspace is $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$.\n\nThis minimization is equivalent to the Galerkin condition that the residual $r_k = b - Ax_k = -Ae_k$ is orthogonal to the subspace $\\mathcal{K}_k(A, r_0)$:\n$$\nr_k^\\top v = 0 \\quad \\forall v \\in \\mathcal{K}_k(A, r_0)\n$$\n\nThe CG algorithm constructs a sequence of search directions $\\{p_0, p_1, \\dots, p_{k-1}\\}$ that form an $A$-orthogonal basis for $\\mathcal{K}_k(A, r_0)$, meaning $p_i^\\top A p_j = 0$ for $i \\neq j$. The iterates are updated as:\n$$\nx_{k+1} = x_k + \\alpha_k p_k\n$$\nwhere $\\alpha_k$ is the step length. The corresponding residual update is:\n$$\nr_{k+1} = b - A x_{k+1} = b - A(x_k + \\alpha_k p_k) = (b - A x_k) - \\alpha_k A p_k = r_k - \\alpha_k A p_k\n$$\n\nTo derive the expressions for the coefficients, we use the properties of the algorithm.\nThe new search direction $p_k$ is constructed to be $A$-orthogonal to previous directions, while being in the subspace $\\mathcal{K}_{k+1}(A, r_0)$. The standard \"short-term recurrence\" for the search direction is a two-term update of the form:\n$$\np_k = r_k + \\beta_{k-1} p_{k-1} \\quad (p_0 = r_0)\n$$\n\n**1. Derivation of Step Length $\\alpha_k$**\nThe step length $\\alpha_k$ is chosen to minimize $\\|e_{k+1}\\|_A$. This is achieved when $e_{k+1}$ is $A$-orthogonal to the search direction $p_k$.\n$$\ne_{k+1} = x - x_{k+1} = x - (x_k + \\alpha_k p_k) = e_k - \\alpha_k p_k\n$$\nThe $A$-orthogonality condition is $\\langle e_{k+1}, p_k \\rangle_A = 0$:\n$$\n0 = e_{k+1}^\\top A p_k = (e_k - \\alpha_k p_k)^\\top A p_k = e_k^\\top A p_k - \\alpha_k p_k^\\top A p_k\n$$\nSolving for $\\alpha_k$:\n$$\n\\alpha_k = \\frac{e_k^\\top A p_k}{p_k^\\top A p_k} = \\frac{(A e_k)^\\top p_k}{p_k^\\top A p_k} = \\frac{(-r_k)^\\top p_k}{p_k^\\top A p_k}\n$$\nFrom the construction $p_k = r_k + \\beta_{k-1}p_{k-1}$, we have $p_k^\\top r_k = (r_k + \\beta_{k-1}p_{k-1})^\\top r_k = r_k^\\top r_k + \\beta_{k-1}p_{k-1}^\\top r_k$.\nThe Galerkin condition at iteration $k$ states $r_k \\perp \\mathcal{K}_k(A, r_0)$. Since $p_{k-1} \\in \\mathcal{K}_k(A, r_0)$, we have $p_{k-1}^\\top r_k = 0$.\nThus, $p_k^\\top r_k = r_k^\\top r_k$. Substituting this into the numerator:\n$$\n\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k}\n$$\nThis expression is realizable as it only involves inner products and one sparse matrix-vector product ($Ap_k$).\n\n**2. Derivation of Recurrence Coefficient $\\beta_k$**\nThe coefficient $\\beta_k$ in the search direction update $p_{k+1} = r_{k+1} + \\beta_k p_k$ is chosen to enforce $A$-orthogonality of $p_{k+1}$ and $p_k$:\n$$\n0 = p_{k+1}^\\top A p_k = (r_{k+1} + \\beta_k p_k)^\\top A p_k = r_{k+1}^\\top A p_k + \\beta_k p_k^\\top A p_k\n$$\nSolving for $\\beta_k$:\n$$\n\\beta_k = -\\frac{r_{k+1}^\\top A p_k}{p_k^\\top A p_k}\n$$\nFrom the residual update, $A p_k = \\frac{1}{\\alpha_k}(r_k - r_{k+1})$. Substituting this into the numerator:\n$$\nr_{k+1}^\\top A p_k = r_{k+1}^\\top \\frac{1}{\\alpha_k}(r_k - r_{k+1}) = \\frac{1}{\\alpha_k} (r_{k+1}^\\top r_k - r_{k+1}^\\top r_{k+1})\n$$\nAn essential property of CG is the orthogonality of the residuals: $r_i^\\top r_j = 0$ for $i \\neq j$. This follows from the Galerkin condition: for $j  k$, $r_j \\in \\mathcal{K}_{j+1}(A, r_0) \\subset \\mathcal{K}_k(A, r_0)$, so $r_k^\\top r_j = 0$.\nTherefore, $r_{k+1}^\\top r_k = 0$, and the numerator simplifies to $-\\frac{1}{\\alpha_k} r_{k+1}^\\top r_{k+1}$.\nSubstituting this back into the expression for $\\beta_k$:\n$$\n\\beta_k = - \\frac{-\\frac{1}{\\alpha_k} r_{k+1}^\\top r_{k+1}}{p_k^\\top A p_k} = \\frac{r_{k+1}^\\top r_{k+1}}{\\alpha_k (p_k^\\top A p_k)}\n$$\nUsing the expression for $\\alpha_k$, we have $\\alpha_k(p_k^\\top A p_k) = r_k^\\top r_k$. Thus:\n$$\n\\beta_k = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}\n$$\n\nThe problem statement asks for \"the three-term update for the search directions\". The standard CG algorithm employs the two-term recurrence $p_{k+1} = r_{k+1} + \\beta_k p_k$. This name can be interpreted as the update involving three distinct vector terms ($p_{k+1}$, $r_{k+1}$, $p_k$). However, in numerical analysis, \"three-term recurrence\" typically implies a relation of the form $v_{k+1} = c_k^{(1)} v_k + c_k^{(2)} v_{k-1}$. The residuals $\\{r_k\\}$ do satisfy such a recurrence, which stems from the three-term recurrence of the underlying Lanczos process. The search directions $\\{p_k\\}$ can also be shown to satisfy a more complex three-term recurrence, but the two-term update is what enables the efficiency of the standard algorithm.\n\n### Part 2: PCG Convergence Analysis\n\nFor the preconditioned system $P^{-1}Ax = P^{-1}b$, where $P$ is an SPD preconditioner, the PCG method is applied. This is equivalent to applying the standard CG method to a transformed system where the operator is SPD. Let $P = L_P L_P^\\top$ be the Cholesky factorization of $P$. The system is equivalent to:\n$$\n(L_P^{-1} A L_P^{-\\top}) (L_P^\\top x) = L_P^{-1} b\n$$\nLet $\\hat{A} = L_P^{-1} A L_P^{-\\top}$, $\\hat{x} = L_P^\\top x$, and $\\hat{b} = L_P^{-1} b$. We solve $\\hat{A}\\hat{x} = \\hat{b}$ using CG. The matrix $\\hat{A}$ is SPD. The eigenvalues of $\\hat{A}$ are the same as the eigenvalues of the preconditioned matrix $P^{-1}A$.\n\nThe convergence of CG for the hatted system is measured in the $\\hat{A}$-norm of its error, $\\hat{e}_k = \\hat{x} - \\hat{x}_k$. This norm directly relates to the $A$-norm of the original error $e_k = x - x_k$:\n$$\n\\|\\hat{e}_k\\|_{\\hat{A}}^2 = \\hat{e}_k^\\top \\hat{A} \\hat{e}_k = (L_P^\\top e_k)^\\top (L_P^{-1} A L_P^{-\\top}) (L_P^\\top e_k) = e_k^\\top L_P L_P^{-1} A L_P^{-\\top} L_P^\\top e_k = e_k^\\top A e_k = \\|e_k\\|_A^2\n$$\nSo, minimizing $\\|\\hat{e}_k\\|_{\\hat{A}}$ is equivalent to minimizing $\\|e_k\\|_A$.\n\nFrom the polynomial approximation perspective, the CG error after $k$ iterations is given by $\\hat{e}_k = \\mathcal{P}_k(\\hat{A})\\hat{e}_0$, where $\\mathcal{P}_k$ is a polynomial of degree at most $k$ with $\\mathcal{P}_k(0)=1$. CG finds the specific polynomial that minimizes the $\\hat{A}$-norm of the error. This gives the bound:\n$$\n\\frac{\\|\\hat{e}_k\\|_{\\hat{A}}}{\\|\\hat{e}_0\\|_{\\hat{A}}} = \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\min_{\\mathcal{P}_k \\in \\Pi_k^1} \\max_{\\lambda \\in \\sigma(\\hat{A})} |\\mathcal{P}_k(\\lambda)|\n$$\nwhere $\\Pi_k^1$ is the set of polynomials of degree at most $k$ satisfying $\\mathcal{P}_k(0)=1$.\n\nLet the spectrum of $\\hat{A}$ (and thus $P^{-1}A$) be contained in the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$. The bound becomes:\n$$\n\\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\min_{\\mathcal{P}_k \\in \\Pi_k^1} \\max_{z \\in [\\lambda_{\\min}, \\lambda_{\\max}]} |\\mathcal{P}_k(z)|\n$$\nThis minimax problem is solved by a scaled and shifted Chebyshev polynomial of the first kind, $T_k(y)$. The optimal polynomial is given by:\n$$\n\\mathcal{P}_k(z) = \\frac{T_k\\left(\\frac{\\lambda_{\\max}+\\lambda_{\\min}-2z}{\\lambda_{\\max}-\\lambda_{\\min}}\\right)}{T_k\\left(\\frac{\\lambda_{\\max}+\\lambda_{\\min}}{\\lambda_{\\max}-\\lambda_{\\min}}\\right)}\n$$\nThis polynomial satisfies $\\mathcal{P}_k(0)=1$. The maximum value of $|T_k(y)|$ for $y \\in [-1, 1]$ is $1$. The transformation maps $z \\in [\\lambda_{\\min}, \\lambda_{\\max}]$ to $y \\in [-1, 1]$. Therefore, the maximum of $|\\mathcal{P}_k(z)|$ on the spectral interval is given by the denominator. Let $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ be the condition number of the preconditioned operator $P^{-1}A$.\n$$\n\\frac{\\lambda_{\\max}+\\lambda_{\\min}}{\\lambda_{\\max}-\\lambda_{\\min}} = \\frac{\\kappa+1}{\\kappa-1}\n$$\nThe convergence bound is then:\n$$\n\\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\frac{1}{T_k\\left(\\frac{\\kappa+1}{\\kappa-1}\\right)}\n$$\nFor arguments $|y|>1$, the Chebyshev polynomial is given by $T_k(y) = \\cosh(k \\cdot \\text{arccosh}(y))$.\nThis leads to the well-known asymptotic bound often expressed as $2\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^k$.\n\n### Part 3: Numerical Calculation\n\nGiven: The spectrum of the preconditioned operator $P^{-1}A$ lies entirely within $[\\theta_{\\min}, \\theta_{\\max}] = [0.5, 2]$. The number of PCG iterations is $k=20$.\n\nThe task is to compute the theoretical upper bound factor on the reduction of the $A$-norm of the error, which is the value of the bound derived above.\n\n1.  **Condition Number**:\n    The condition number of the preconditioned matrix is $\\kappa = \\frac{\\theta_{\\max}}{\\theta_{\\min}} = \\frac{2}{0.5} = 4$.\n\n2.  **Argument of Chebyshev Polynomial**:\n    The argument is $\\frac{\\kappa+1}{\\kappa-1} = \\frac{4+1}{4-1} = \\frac{5}{3}$.\n\n3.  **Evaluate the Bound**:\n    The bound factor is $\\frac{1}{T_{20}(5/3)}$. We use the formula $T_k(y) = \\cosh(k \\cdot \\text{arccosh}(y))$.\n    First, find $\\text{arccosh}(5/3)$:\n    Let $u = \\text{arccosh}(5/3)$. This means $\\cosh(u) = 5/3$.\n    Using the identity $\\text{arccosh}(y) = \\ln(y + \\sqrt{y^2-1})$ for $y \\ge 1$:\n    $$\n    u = \\ln\\left(\\frac{5}{3} + \\sqrt{\\left(\\frac{5}{3}\\right)^2 - 1}\\right) = \\ln\\left(\\frac{5}{3} + \\sqrt{\\frac{25}{9} - \\frac{9}{9}}\\right) = \\ln\\left(\\frac{5}{3} + \\sqrt{\\frac{16}{9}}\\right) = \\ln\\left(\\frac{5}{3} + \\frac{4}{3}\\right) = \\ln(3)\n    $$\n    Now, evaluate $T_{20}(5/3)$:\n    $$\n    T_{20}(5/3) = \\cosh(20 \\cdot \\text{arccosh}(5/3)) = \\cosh(20 \\ln 3)\n    $$\n    Using the definition $\\cosh(z) = \\frac{\\exp(z) + \\exp(-z)}{2}$:\n    $$\n    \\cosh(20 \\ln 3) = \\frac{\\exp(20 \\ln 3) + \\exp(-20 \\ln 3)}{2} = \\frac{\\exp(\\ln 3^{20}) + \\exp(\\ln 3^{-20})}{2} = \\frac{3^{20} + 3^{-20}}{2}\n    $$\n    The bound factor is the reciprocal of this value:\n    $$\n    \\text{Bound Factor} = \\frac{1}{\\frac{3^{20} + 3^{-20}}{2}} = \\frac{2}{3^{20} + 3^{-20}}\n    $$\n\n4.  **Numerical Value**:\n    We compute the value of the expression:\n    $3^{20} = (3^{10})^2 = (59049)^2 = 3,486,784,401$\n    $3^{-20}$ is exceedingly small and its contribution to the sum is negligible for this level of precision.\n    $$\n    \\text{Bound Factor} = \\frac{2}{3486784401 + \\frac{1}{3486784401}} \\approx \\frac{2}{3486784401} \\approx 5.73596... \\times 10^{-10}\n    $$\n    Rounding to four significant figures, we get $5.736 \\times 10^{-10}$.",
            "answer": "$$\n\\boxed{5.736 \\times 10^{-10}}\n$$"
        },
        {
            "introduction": "Real-world physical models often present numerical challenges that go beyond simple positive-definite systems. A classic example in electromagnetics is the gauge freedom inherent in potential formulations, which leads to singular system matrices. This practice addresses this issue head-on by examining the coupled $A-\\phi$ formulation . You will derive the FEM system, identify its gauge-related nullspace, and construct the explicit matrix for an orthogonal projector used to enforce a gauge condition, a crucial technique for ensuring a robust and unique solution.",
            "id": "3321795",
            "problem": "Consider time-harmonic electromagnetic fields in a bounded, simply connected, homogeneous, isotropic domain with constant permittivity $\\varepsilon$ and permeability $\\mu$, with time dependence $\\exp(j \\omega t)$, angular frequency $\\omega$, electric field $\\mathbf{E}$, magnetic flux density $\\mathbf{B}$, electric displacement $\\mathbf{D}$, magnetic field $\\mathbf{H}$, impressed current density $\\mathbf{J}$, and charge density $\\rho$. Use the potentials $\\mathbf{A}$ (vector potential) and $\\phi$ (scalar potential) defined by $\\mathbf{E} = - j \\omega \\mathbf{A} - \\nabla \\phi$ and $\\mathbf{B} = \\nabla \\times \\mathbf{A}$, together with Maxwell's equations $\\nabla \\times \\mathbf{H} = \\mathbf{J} + j \\omega \\mathbf{D}$, $\\mathbf{B} = \\mu \\mathbf{H}$, $\\mathbf{D} = \\varepsilon \\mathbf{E}$, and $\\nabla \\cdot \\mathbf{D} = \\rho$, and impose the Coulomb gauge $\\nabla \\cdot \\mathbf{A} = 0$. Assume perfectly electrically conducting boundaries, so that $\\mathbf{n} \\times \\mathbf{A} = \\mathbf{0}$ on the boundary, and $\\phi$ is set to zero on the boundary.\n\na) Starting from these laws and definitions only, derive the weak form of the coupled $\\mathbf{A}$-$\\phi$ formulation under the Coulomb gauge and write the corresponding finite-dimensional linear system obtained by the Finite Element Method (FEM), using first order Nédélec edge elements for $\\mathbf{A}$ and first order Lagrange nodal elements for $\\phi$. Define the discrete operators and matrices as follows: the curl-curl matrix $\\mathbf{K}$, the $\\varepsilon$-weighted edge mass matrix $\\mathbf{M}_{\\varepsilon}$, the discrete gradient matrix $\\mathbf{G}$ mapping nodal degrees of freedom to edge degrees of freedom, and the scalar stiffness matrix $\\mathbf{S}_{\\varepsilon}$ arising from the $\\varepsilon$-weighted scalar Laplacian. Write the block algebraic system in terms of these matrices and the coefficient vectors for the edge basis expansion of $\\mathbf{A}$ and the nodal basis expansion of $\\phi$.\n\nb) Identify the gauge nullspace of this block system by characterizing all pairs of discrete variables that, under a gauge transformation of the form $\\mathbf{A} \\mapsto \\mathbf{A} + \\nabla \\chi$, $\\phi \\mapsto \\phi - j \\omega \\chi$ for a scalar field $\\chi$, leave the fields $\\mathbf{E}$ and $\\mathbf{B}$ invariant, and show how this manifests at the discrete level using only the defined operators.\n\nc) In Krylov subspace iterative solvers such as Generalized Minimal Residual (GMRES), a standard treatment of the gauge-induced singularity is to remove the gradient component of the residual by applying the $\\varepsilon$-weighted orthogonal projector onto the divergence-free edge subspace (the $M_{\\varepsilon}$-orthogonal complement of the discrete gradient space). Using only the matrices and operators defined above, give the explicit matrix expression for this projector. Your final answer must be a single closed-form analytic expression. No rounding is required.",
            "solution": "### Part (a): Derivation of the Weak Form and FEM System\n\nWe begin with the provided time-harmonic Maxwell's equations and potential definitions. The primary equations to solve for $\\mathbf{A}$ and $\\phi$ are Ampere's law and Gauss's law.\n\n1.  **Ampere's Law**: $\\nabla \\times \\mathbf{H} = \\mathbf{J} + j \\omega \\mathbf{D}$.\n    Substituting the constitutive relations $\\mathbf{H} = \\frac{1}{\\mu}\\mathbf{B}$ and $\\mathbf{D} = \\varepsilon \\mathbf{E}$, we get:\n    $$ \\nabla \\times \\left(\\frac{1}{\\mu} \\mathbf{B}\\right) = \\mathbf{J} + j \\omega \\varepsilon \\mathbf{E} $$\n    Now, substitute the potential definitions $\\mathbf{B} = \\nabla \\times \\mathbf{A}$ and $\\mathbf{E} = -j \\omega \\mathbf{A} - \\nabla \\phi$:\n    $$ \\frac{1}{\\mu} \\nabla \\times (\\nabla \\times \\mathbf{A}) = \\mathbf{J} + j \\omega \\varepsilon (-j \\omega \\mathbf{A} - \\nabla \\phi) $$\n    Rearranging gives the vector wave equation for $\\mathbf{A}$:\n    $$ \\frac{1}{\\mu} \\nabla \\times (\\nabla \\times \\mathbf{A}) - \\omega^2 \\varepsilon \\mathbf{A} + j \\omega \\varepsilon \\nabla \\phi = \\mathbf{J} \\quad (*)$$\n\n2.  **Gauss's Law**: $\\nabla \\cdot \\mathbf{D} = \\rho$.\n    Substituting $\\mathbf{D} = \\varepsilon \\mathbf{E}$ and then $\\mathbf{E} = -j \\omega \\mathbf{A} - \\nabla \\phi$:\n    $$ \\nabla \\cdot (\\varepsilon (-j \\omega \\mathbf{A} - \\nabla \\phi)) = \\rho $$\n    $$ -j \\omega \\nabla \\cdot (\\varepsilon \\mathbf{A}) - \\nabla \\cdot (\\varepsilon \\nabla \\phi) = \\rho \\quad (**) $$\n\nThis coupled system of partial differential equations for $\\mathbf{A}$ and $\\phi$ is the starting point for the weak formulation.\n\nTo find the weak form, we test equation $(*)$ with a vector test function $\\mathbf{w}_A$ from the Nédélec space and equation $(**)$ with a scalar test function $w_\\phi$ from the Lagrange space, both satisfying homogeneous boundary conditions. Using integration by parts and applying the boundary conditions ($\\mathbf{n} \\times \\mathbf{w}_A = \\mathbf{0}$ and $w_\\phi = 0$ on $\\partial\\Omega$), the boundary integrals vanish.\n\nThe weak form of the first equation becomes:\n$$ \\int_{\\Omega} \\frac{1}{\\mu} (\\nabla \\times \\mathbf{A}) \\cdot (\\nabla \\times \\mathbf{w}_A) \\, dV - \\int_{\\Omega} \\omega^2 \\varepsilon \\mathbf{A} \\cdot \\mathbf{w}_A \\, dV + \\int_{\\Omega} j \\omega \\varepsilon (\\nabla \\phi) \\cdot \\mathbf{w}_A \\, dV = \\int_{\\Omega} \\mathbf{J} \\cdot \\mathbf{w}_A \\, dV $$\nThe weak form of the second equation becomes:\n$$ \\int_{\\Omega} \\varepsilon (\\nabla \\phi) \\cdot (\\nabla w_\\phi) \\, dV + \\int_{\\Omega} j \\omega \\varepsilon \\mathbf{A} \\cdot (\\nabla w_\\phi) \\, dV = \\int_{\\Omega} \\rho w_\\phi \\, dV $$\n\nNow we discretize these weak forms. We expand the unknown fields in terms of basis functions: $\\mathbf{A} \\approx \\sum_i a_i \\mathbf{w}_i$ and $\\phi \\approx \\sum_j p_j w_j$, where $\\mathbf{w}_i$ are Nédélec edge functions and $w_j$ are Lagrange nodal functions. Let $\\mathbf{a}$ and $\\mathbf{p}$ be the column vectors of the unknown coefficients $\\{a_i\\}$ and $\\{p_j\\}$.\n\nSubstituting these expansions and using the provided matrix definitions, we obtain the discrete system:\n1.  From the first weak form (with constant $\\mu, \\varepsilon$): $(\\frac{1}{\\mu}\\mathbf{K} - \\omega^2 \\mathbf{M}_{\\varepsilon})\\mathbf{a} + j \\omega \\varepsilon \\mathbf{G} \\mathbf{p} = \\mathbf{j}$.\n2.  From the second weak form: $\\mathbf{S}_{\\varepsilon}\\mathbf{p} + j \\omega \\varepsilon \\mathbf{G}^T \\mathbf{a} = \\mathbf{f}$.\n\nWriting this as a single block algebraic system:\n$$ \\begin{pmatrix} \\frac{1}{\\mu}\\mathbf{K} - \\omega^2 \\mathbf{M}_{\\varepsilon}  j \\omega \\varepsilon \\mathbf{G} \\\\ j \\omega \\varepsilon \\mathbf{G}^T  \\mathbf{S}_{\\varepsilon} \\end{pmatrix} \\begin{pmatrix} \\mathbf{a} \\\\ \\mathbf{p} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{j} \\\\ \\mathbf{f} \\end{pmatrix} $$\n\n### Part (b): Gauge Nullspace Identification\n\nA gauge transformation is defined by $\\mathbf{A} \\mapsto \\mathbf{A}' = \\mathbf{A} + \\nabla \\chi$ and $\\phi \\mapsto \\phi' = \\phi - j \\omega \\chi$. This transformation leaves the physical fields $\\mathbf{E}$ and $\\mathbf{B}$ invariant.\n\nAt the discrete level, a scalar field $\\chi$ vanishing on the boundary is represented by a vector of nodal coefficients $\\mathbf{c}$ in the Lagrange basis. The transformation of the coefficient vectors is $\\mathbf{a} \\mapsto \\mathbf{a} + \\mathbf{G}\\mathbf{c}$ and $\\mathbf{p} \\mapsto \\mathbf{p} - j\\omega\\mathbf{c}$.\n\nThe gauge invariance of the continuous problem implies that the discrete system matrix should map the vector corresponding to the gauge mode, $(\\mathbf{G}\\mathbf{c}, -j\\omega\\mathbf{c})^T$, to zero. Let's test this:\n$$ \\begin{pmatrix} \\frac{1}{\\mu}\\mathbf{K} - \\omega^2 \\mathbf{M}_{\\varepsilon}  j \\omega \\varepsilon \\mathbf{G} \\\\ j \\omega \\varepsilon \\mathbf{G}^T  \\mathbf{S}_{\\varepsilon} \\end{pmatrix} \\begin{pmatrix} \\mathbf{G}\\mathbf{c} \\\\ -j \\omega \\mathbf{c} \\end{pmatrix} = \\begin{pmatrix} (\\frac{1}{\\mu}\\mathbf{K} - \\omega^2 \\mathbf{M}_{\\varepsilon})\\mathbf{G}\\mathbf{c} + (j \\omega \\varepsilon \\mathbf{G})(-j \\omega \\mathbf{c}) \\\\ (j \\omega \\varepsilon \\mathbf{G}^T)(\\mathbf{G}\\mathbf{c}) + \\mathbf{S}_{\\varepsilon}(-j \\omega \\mathbf{c}) \\end{pmatrix} $$\nLet's analyze the two resulting block components.\n\n1.  **Top component**:\n    $$ \\frac{1}{\\mu}\\mathbf{K}\\mathbf{G}\\mathbf{c} - \\omega^2 \\mathbf{M}_{\\varepsilon}\\mathbf{G}\\mathbf{c} + \\omega^2 \\varepsilon \\mathbf{G}\\mathbf{c} $$\n    For a conforming discretization (like Nédélec and Lagrange elements), the vector calculus identity $\\nabla \\times (\\nabla \\chi) = \\mathbf{0}$ is preserved exactly. This means $\\mathbf{K}\\mathbf{G} = \\mathbf{0}$. The expression simplifies to:\n    $$ \\omega^2 (\\varepsilon \\mathbf{G} - \\mathbf{M}_{\\varepsilon} \\mathbf{G})\\mathbf{c} $$\n\n2.  **Bottom component**:\n    $$ j \\omega \\varepsilon \\mathbf{G}^T\\mathbf{G}\\mathbf{c} - j \\omega \\mathbf{S}_{\\varepsilon}\\mathbf{c} = -j \\omega (\\mathbf{S}_{\\varepsilon} - \\varepsilon \\mathbf{G}^T\\mathbf{G})\\mathbf{c} $$\n\nNeither of these expressions is identically zero for a general mesh. This is because the discrete operators do not satisfy identities such as $\\mathbf{M}_{\\varepsilon} = \\varepsilon \\mathbf{I}$ (mass matrix is not a scaled identity) or $\\mathbf{S}_{\\varepsilon} = \\varepsilon \\mathbf{G}^T\\mathbf{G}$ (the scalar stiffness matrix and the product of gradient matrices are generally different). The non-zero result is a discretization artifact. The continuous operator has a nullspace, but the discrete operator does not perfectly map the discrete gauge mode to zero. This residual error vanishes as the mesh size $h \\to 0$. Therefore, the vector $(\\mathbf{G}\\mathbf{c}, -j\\omega\\mathbf{c})^T$ is an **approximate null vector** of the discrete system. This manifests as a severe ill-conditioning or singularity in the block matrix, which must be handled by the solver.\n\n### Part (c): Projector Expression\n\nWe want to find the matrix for the $\\mathbf{M}_{\\varepsilon}$-orthogonal projector onto the divergence-free edge subspace. This subspace is the $\\mathbf{M}_{\\varepsilon}$-orthogonal complement of the discrete gradient space, $\\mathcal{G} = \\text{range}(\\mathbf{G})$.\n\nLet $\\mathbf{P}$ be the projector onto the gradient space $\\mathcal{G}$. The projector we seek is then $\\mathbf{I} - \\mathbf{P}$. The projector $\\mathbf{P}$ maps any vector $\\mathbf{r}$ (in the edge space) to its component in $\\mathcal{G}$. This component, $\\mathbf{r}_{\\mathcal{G}}$, can be written as $\\mathbf{Gz}$ for some vector $\\mathbf{z}$ in the nodal space.\n\nThe defining property of the $\\mathbf{M}_{\\varepsilon}$-orthogonal projection is that the error vector, $\\mathbf{r} - \\mathbf{r}_{\\mathcal{G}}$, must be $\\mathbf{M}_{\\varepsilon}$-orthogonal to the subspace $\\mathcal{G}$. That is, for any vector $\\mathbf{v} \\in \\mathcal{G}$:\n$$ \\langle \\mathbf{r} - \\mathbf{r}_{\\mathcal{G}}, \\mathbf{v} \\rangle_{\\mathbf{M}_{\\varepsilon}} = 0 $$\nwhere the inner product is defined as $\\langle \\mathbf{x}, \\mathbf{y} \\rangle_{\\mathbf{M}_{\\varepsilon}} = \\mathbf{x}^T \\mathbf{M}_{\\varepsilon} \\mathbf{y}$.\nSubstituting $\\mathbf{r}_{\\mathcal{G}} = \\mathbf{Gz}$ and acknowledging that any $\\mathbf{v} \\in \\mathcal{G}$ can be written as $\\mathbf{G}\\mathbf{y}$ for some nodal vector $\\mathbf{y}$:\n$$ (\\mathbf{r} - \\mathbf{Gz})^T \\mathbf{M}_{\\varepsilon} (\\mathbf{G}\\mathbf{y}) = 0 \\quad \\text{for all } \\mathbf{y} $$\nThis can be rewritten as:\n$$ \\mathbf{y}^T (\\mathbf{G}^T \\mathbf{M}_{\\varepsilon} (\\mathbf{r} - \\mathbf{Gz})) = 0 \\quad \\text{for all } \\mathbf{y} $$\nFor this to hold for any vector $\\mathbf{y}$, the term in the parenthesis must be the zero vector:\n$$ \\mathbf{G}^T \\mathbf{M}_{\\varepsilon} (\\mathbf{r} - \\mathbf{Gz}) = \\mathbf{0} $$\n$$ \\mathbf{G}^T \\mathbf{M}_{\\varepsilon} \\mathbf{r} - \\mathbf{G}^T \\mathbf{M}_{\\varepsilon} \\mathbf{Gz} = \\mathbf{0} $$\nThis gives a small linear system for the unknown nodal coefficients $\\mathbf{z}$:\n$$ (\\mathbf{G}^T \\mathbf{M}_{\\varepsilon} \\mathbf{G}) \\mathbf{z} = \\mathbf{G}^T \\mathbf{M}_{\\varepsilon} \\mathbf{r} $$\nAssuming the matrix $\\mathbf{G}^T \\mathbf{M}_{\\varepsilon} \\mathbf{G}$ is invertible (which it is, provided that $\\mathbf{G}$ has trivial nullspace, i.e., the potential is fixed at least one node), we can solve for $\\mathbf{z}$:\n$$ \\mathbf{z} = (\\mathbf{G}^T \\mathbf{M}_{\\varepsilon} \\mathbf{G})^{-1} \\mathbf{G}^T \\mathbf{M}_{\\varepsilon} \\mathbf{r} $$\nThe projected vector (the component in the gradient space) is $\\mathbf{r}_{\\mathcal{G}} = \\mathbf{Gz}$:\n$$ \\mathbf{r}_{\\mathcal{G}} = \\mathbf{G} (\\mathbf{G}^T \\mathbf{M}_{\\varepsilon} \\mathbf{G})^{-1} \\mathbf{G}^T \\mathbf{M}_{\\varepsilon} \\mathbf{r} $$\nThis gives the projector matrix $\\mathbf{P}$ onto the gradient space:\n$$ \\mathbf{P} = \\mathbf{G} (\\mathbf{G}^T \\mathbf{M}_{\\varepsilon} \\mathbf{G})^{-1} \\mathbf{G}^T \\mathbf{M}_{\\varepsilon} $$\nThe problem asks for the projector onto the orthogonal complement (the \"divergence-free\" subspace). This projector, $\\mathbf{P}_{\\perp}$, is given by $\\mathbf{I} - \\mathbf{P}$.\n$$\n\\mathbf{P}_{\\perp} = \\mathbf{I} - \\mathbf{G} (\\mathbf{G}^T \\mathbf{M}_{\\varepsilon} \\mathbf{G})^{-1} \\mathbf{G}^T \\mathbf{M}_{\\varepsilon}\n$$\nThis is the final expression for the projector.",
            "answer": "$$ \\boxed{ \\mathbf{I} - \\mathbf{G} (\\mathbf{G}^T \\mathbf{M}_{\\varepsilon} \\mathbf{G})^{-1} \\mathbf{G}^T \\mathbf{M}_{\\varepsilon} } $$"
        }
    ]
}