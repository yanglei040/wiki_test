{
    "hands_on_practices": [
        {
            "introduction": "Before attempting to optimize iterative solvers, it is crucial to first understand their computational demands. This exercise guides you through a detailed analysis of the restarted Generalized Minimal Residual (GMRES) method, a workhorse for non-Hermitian systems common in Method of Moments (MoM). By quantifying the memory footprint, floating-point operations, and communication overhead for a large-scale hypothetical problem, you will gain practical insight into the real-world performance bottlenecks of Krylov methods and appreciate the trade-offs between numerical robustness, such as full reorthogonalization, and computational cost .",
            "id": "3321331",
            "problem": "Consider a Method of Moments (MoM) discretization of a frequency-domain electromagnetic boundary integral equation that yields a linear system $A x = b$ of dimension $N = 10^{6}$ with complex-valued unknowns. You plan to solve this system using the restarted Generalized Minimal Residual method (GMRES) with restart parameter $m = 80$, implemented via the Arnoldi process and classical Gram–Schmidt orthogonalization, with full reorthogonalization enabled to guard against loss of orthogonality.\n\nAssume the following modeling and accounting conventions, which reflect typical high-performance implementations for complex double-precision arithmetic:\n- Each complex number is stored in double precision and occupies $16$ bytes of memory.\n- A complex inner product of two length-$N$ vectors is counted as approximately $8N$ real floating-point operations (flops). This models $6N$ flops for the elementwise complex multiplications and approximately $2N$ flops for the complex additions in the accumulation (neglecting $O(1)$ terms).\n- A scaled complex axpy of the form $y \\leftarrow y - \\alpha x$ on length-$N$ vectors with complex scalar $\\alpha$ is counted as $8N$ flops ($6N$ for the complex multiplications and $2N$ for the complex additions).\n- The restarted GMRES($m$) stores the $(m+1)$-vector Krylov basis generated by Arnoldi, the $(m+1)\\times m$ upper Hessenberg matrix, and exactly three additional length-$N$ complex work vectors used for residuals, preconditioned residuals, and temporary products.\n- For parallel execution across $P$ processes, each complex inner product induces one global reduction; full reorthogonalization doubles the number of inner products relative to a single-pass classical Gram–Schmidt.\n\nUsing only these assumptions and the algorithmic structure of GMRES($m$) with Arnoldi:\n1. Derive and compute the total memory footprint (in gibibytes, GiB; where $1\\,\\mathrm{GiB} = 2^{30}$ bytes) needed to store the Krylov basis, the upper Hessenberg matrix, and the three additional work vectors for $N = 10^{6}$ and $m = 80$.\n2. Derive and compute the additional floating-point operation count per restart cycle that full reorthogonalization imposes beyond a single-pass classical Gram–Schmidt, for $N = 10^{6}$ and $m = 80$.\n3. Determine the additional number of global dot-product reductions per restart cycle caused by full reorthogonalization for $m = 80$.\n\nDiscuss how the computed quantities influence cache behavior (e.g., streaming through vectors that exceed last-level cache) and interprocess communication (e.g., synchronization due to global reductions), grounding your discussion in the derived counts.\n\nAs your final numerical output, report a single row vector $\\begin{pmatrix} M_{\\mathrm{GiB}}  F_{\\mathrm{add}}  R_{\\mathrm{add}} \\end{pmatrix}$, where:\n- $M_{\\mathrm{GiB}}$ is the total memory footprint in GiB, rounded to three significant figures,\n- $F_{\\mathrm{add}}$ is the additional floating-point operations per restart cycle due to full reorthogonalization, rounded to three significant figures,\n- $R_{\\mathrm{add}}$ is the additional number of global dot-product reductions per restart cycle, reported as an exact integer.",
            "solution": "The problem requires the calculation of the memory footprint, additional floating-point operations (flops), and additional communication events for the restarted Generalized Minimal Residual method (GMRES($m$)) with full reorthogonalization. The calculations are based on the provided parameters and cost models.\n\nFirst, we establish the parameters given in the problem statement:\n- The dimension of the linear system is $N = 10^{6}$.\n- The GMRES restart parameter is $m = 80$.\n- Each complex number occupies $16$ bytes.\n- $1\\,\\mathrm{GiB} = 2^{30}$ bytes.\n- A complex inner product of length-$N$ vectors costs $8N$ flops.\n- A complex axpy operation of length-$N$ vectors costs $8N$ flops.\n\n### 1. Total Memory Footprint ($M$)\n\nThe total memory footprint is the sum of the storage required for the Krylov basis, the Hessenberg matrix, and the specified work vectors.\n\n- **Krylov basis**: The Arnoldi process generates an orthonormal basis $\\{q_1, q_2, \\ldots, q_{m+1}\\}$ for the Krylov subspace $\\mathcal{K}_{m+1}(A, r_0)$. This requires storing $m+1$ vectors, each of length $N$. The number of complex values is $(m+1)N$.\n- **Hessenberg matrix**: The Arnoldi process produces an $(m+1) \\times m$ upper Hessenberg matrix, $H_m$. The number of complex values is $(m+1)m$.\n- **Work vectors**: The problem states that $3$ additional length-$N$ complex work vectors are used. The number of complex values is $3N$.\n\nThe total number of complex values to be stored, $C_{\\mathrm{total}}$, is the sum of these components:\n$$\nC_{\\mathrm{total}} = (m+1)N + 3N + (m+1)m = (m+4)N + (m+1)m\n$$\nThe total memory footprint in bytes, $M_{\\mathrm{bytes}}$, is $C_{\\mathrm{total}}$ multiplied by the size of a complex number ($16$ bytes):\n$$\nM_{\\mathrm{bytes}} = 16 \\times \\left( (m+4)N + (m+1)m \\right)\n$$\nTo convert this to gibibytes (GiB), we divide by $2^{30}$.\n$$\nM_{\\mathrm{GiB}} = \\frac{16 \\times \\left( (m+4)N + (m+1)m \\right)}{2^{30}}\n$$\nSubstituting the given values $N = 10^{6}$ and $m = 80$:\n$$\nM_{\\mathrm{GiB}} = \\frac{16 \\times \\left( (80+4) \\times 10^{6} + (80+1) \\times 80 \\right)}{2^{30}}\n$$\n$$\nM_{\\mathrm{GiB}} = \\frac{16 \\times \\left( 84 \\times 10^{6} + 81 \\times 80 \\right)}{2^{30}} = \\frac{16 \\times \\left( 84,000,000 + 6480 \\right)}{1,073,741,824}\n$$\n$$\nM_{\\mathrm{GiB}} = \\frac{16 \\times 84,006,480}{1,073,741,824} = \\frac{1,344,103,680}{1,073,741,824} \\approx 1.25181 \\, \\mathrm{GiB}\n$$\nRounding to three significant figures, the total memory footprint is $M_{\\mathrm{GiB}} = 1.25 \\, \\mathrm{GiB}$. The storage for the Hessenberg matrix ($16 \\times 6480 \\approx 104$ kB) is negligible compared to the storage for the vectors.\n\n### 2. Additional Floating-Point Operations per Restart Cycle ($F_{\\mathrm{add}}$)\n\nThe orthogonalization in the Arnoldi process is performed using classical Gram–Schmidt (CGS). At step $j$ of the Arnoldi iteration (for $j=1, \\ldots, m$), the new vector $v = A q_j$ is orthogonalized against the existing basis vectors $\\{q_1, \\ldots, q_j\\}$. A single pass of CGS involves $j$ inner products ($h_{i,j} = q_i^H v$) and $j$ axpy operations ($v \\leftarrow v - h_{i,j}q_i$).\n\nFull reorthogonalization implies performing the Gram-Schmidt process a second time to correct for loss of orthogonality. This second pass also involves $j$ inner products and $j$ axpy operations. Therefore, the *additional* work at step $j$ from reorthogonalization consists of $j$ inner products and $j$ axpy operations.\n\nOver one full restart cycle ($m$ steps of Arnoldi), the total number of additional inner products is the sum over $j$ from $1$ to $m$:\n$$\nN_{\\mathrm{ip, add}} = \\sum_{j=1}^{m} j = \\frac{m(m+1)}{2}\n$$\nSimilarly, the total number of additional axpy operations is:\n$$\nN_{\\mathrm{axpy, add}} = \\sum_{j=1}^{m} j = \\frac{m(m+1)}{2}\n$$\nThe cost of one inner product is given as $8N$ flops, and the cost of one axpy is also $8N$ flops. The total additional flop count, $F_{\\mathrm{add}}$, is:\n$$\nF_{\\mathrm{add}} = N_{\\mathrm{ip, add}} \\times (8N) + N_{\\mathrm{axpy, add}} \\times (8N)\n$$\n$$\nF_{\\mathrm{add}} = \\frac{m(m+1)}{2} \\times 8N + \\frac{m(m+1)}{2} \\times 8N = m(m+1) \\times 8N\n$$\nSubstituting $N = 10^6$ and $m = 80$:\n$$\nF_{\\mathrm{add}} = 80 \\times (80+1) \\times 8 \\times 10^{6} = 80 \\times 81 \\times 8 \\times 10^{6}\n$$\n$$\nF_{\\mathrm{add}} = 6480 \\times 8 \\times 10^{6} = 51840 \\times 10^{6} = 5.184 \\times 10^{10}\n$$\nRounding to three significant figures, the additional flop count is $F_{\\mathrm{add}} = 5.18 \\times 10^{10}$ flops.\n\n### 3. Additional Global Reductions per Restart Cycle ($R_{\\mathrm{add}}$)\n\nThe problem states that each complex inner product induces one global reduction. Full reorthogonalization adds a second set of inner products, thus adding a corresponding number of global reductions. The number of additional global reductions, $R_{\\mathrm{add}}$, is equal to the number of additional inner products calculated previously.\n$$\nR_{\\mathrm{add}} = N_{\\mathrm{ip, add}} = \\frac{m(m+1)}{2}\n$$\nSubstituting $m = 80$:\n$$\nR_{\\mathrm{add}} = \\frac{80 \\times (80+1)}{2} = 40 \\times 81 = 3240\n$$\nThis is an exact integer value.\n\n### Discussion of Influence on Performance\n\n-   **Cache Behavior**: The total memory footprint of $M_{\\mathrm{GiB}} \\approx 1.25 \\, \\mathrm{GiB}$ is far larger than typical last-level caches (LLCs) on modern CPUs, which are on the order of tens of megabytes. A single length-$N$ vector requires $10^6 \\times 16 \\, \\text{bytes} = 16 \\, \\text{MB}$ of storage. Operations central to GMRES, such as inner products and axpy updates, involve streaming through one or more of these large vectors. For instance, in the orthogonalization step, the algorithm repeatedly accesses the basis vectors $\\{ q_1, \\ldots, q_j \\}$. As the basis size $j$ grows, the set of vectors to be accessed, $j \\times 16 \\, \\text{MB}$, quickly exceeds any on-chip cache. Consequently, these vector operations are memory-bandwidth bound; the processor spends a significant fraction of its time waiting for data to be fetched from main DRAM rather than performing computations. The high flop count, including the additional $F_{\\mathrm{add}} \\approx 5.18 \\times 10^{10}$ flops, often cannot be sustained because the arithmetic units are starved of data.\n\n-   **Interprocess Communication**: In a parallel implementation across $P$ processes, global reductions are synchronization points that require all processes to communicate. The additional $R_{\\mathrm{add}} = 3240$ global reductions per restart cycle, caused by full reorthogonalization, represent a significant communication overhead. Each of these $3240$ operations (e.g., an `MPI_Allreduce` call) introduces latency, as processes must wait for the slowest participant and for the data to travel across the network. The total wall-clock time becomes highly sensitive to network latency and bandwidth. For massively parallel systems where $P$ is large, the cost of these reductions can dominate the overall runtime, eclipsing the cost of floating-point arithmetic. This illustrates a fundamental trade-off in scalable numerical algorithms: the need for numerical robustness (achieved here via reorthogonalization) often comes at the cost of increased communication and synchronization, which can limit parallel scalability.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1.25  5.18 \\times 10^{10}  3240 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A key challenge in solving Electric Field Integral Equation (EFIE) systems is the \"dense-discretization breakdown,\" where the condition number of the MoM matrix degrades as the mesh is refined, drastically slowing solver convergence. This practice demonstrates the power of preconditioning to overcome this issue. Through a hands-on coding exercise, you will model the spectral behavior of the EFIE operator and use the classic Chebyshev convergence bound to predict and compare the GMRES iteration count for a system with and without a quasi-Helmholtz preconditioner, thereby quantifying the profound impact of making the system's condition number independent of the mesh size .",
            "id": "3321343",
            "problem": "Consider a fixed-frequency electromagnetic scattering problem discretized by the Method of Moments (MoM) using Rao–Wilton–Glisson basis functions, leading to a linear system $A(h)\\,x=b$ with mesh parameter $h$ equal to the average edge length normalized by the wavelength (dimensionless). Assume the following empirically validated facts form the fundamental base for the derivation.\n\n- At fixed frequency (no low-frequency scaling), the Electric Field Integral Equation (EFIE) matrix $A(h)$ exhibits dense-discretization breakdown: its smallest singular value scales like $O(h^{p_{\\alpha}})$ and its largest singular value scales like $O(h^{-p_{\\beta}})$, so that the condition number $\\kappa(h)$ scales as $O\\!\\left(h^{-(p_{\\alpha}+p_{\\beta})}\\right)$. A common case for EFIE is $p_{\\alpha}=1$ and $p_{\\beta}=0$.\n- The Generalized Minimal Residual (GMRES) method minimizes the residual over polynomials constrained by $p(0)=1$. When the field of values or spectrum of the coefficient matrix is contained in a real interval $[\\alpha,\\beta]$ on the positive real axis, the extremal properties of Chebyshev polynomials yield an optimal polynomial approximation bound on the residual norm.\n- A quasi-Helmholtz preconditioner $M$ (e.g., loop-star or loop-tree decomposition with appropriate scaling) transforms the system to $M^{-1}A(h)$ and suppresses dense-discretization breakdown, clustering the spectrum in an $h$-independent interval $[\\underline{\\alpha},\\overline{\\beta}]$ on the positive real axis. Consequently, the condition number becomes essentially independent of $h$.\n\nYour task is to predict how the GMRES iteration count changes when the mesh is refined by a factor of $2$ (that is, $h \\mapsto h/2$), both without preconditioning and with a quasi-Helmholtz preconditioner. Work in purely mathematical terms, with $h$ dimensionless. You must implement the following modeling assumptions in your program.\n\n- Model the unpreconditioned spectrum of $A(h)$ as contained in $[\\alpha(h),\\beta(h)]$ with $\\alpha(h)=c_{\\alpha}\\,h^{p_{\\alpha}}$ and $\\beta(h)=c_{\\beta}\\,h^{-p_{\\beta}}$, where $c_{\\alpha}0$, $c_{\\beta}0$, $p_{\\alpha}\\ge 0$, and $p_{\\beta}\\ge 0$.\n- Model the unpreconditioned condition number as $\\kappa(h)=\\dfrac{\\beta(h)}{\\alpha(h)}=\\dfrac{c_{\\beta}}{c_{\\alpha}}\\,h^{-(p_{\\alpha}+p_{\\beta})}$.\n- Model the preconditioned spectrum of $M^{-1}A(h)$ as $[\\underline{\\alpha},\\overline{\\beta}]$ with $\\underline{\\alpha}0$ and $\\overline{\\beta}0$ independent of $h$, yielding a condition number $\\kappa_{p}=\\dfrac{\\overline{\\beta}}{\\underline{\\alpha}}$ independent of $h$.\n- Use the classical Chebyshev optimal-polynomial residual bound on a real positive interval to upper bound the GMRES residual decay. For a given tolerance $\\varepsilon\\in(0,1)$ and condition number $\\kappa\\ge 1$, define the predicted iteration count $m(\\kappa,\\varepsilon)$ to be the smallest nonnegative integer satisfying the Chebyshev-based bound for the minimax polynomial on $[\\alpha,\\beta]$. Implement this using the standard closed-form that follows from the extremal property of Chebyshev polynomials on an interval mapped to $[-1,1]$.\n\nYou must compute, for each test case below, the change in the predicted GMRES iterations when refining $h$ by a factor of $2$, that is, $\\Delta m_{\\mathrm{unprec}}=m(\\kappa(h/2),\\varepsilon)-m(\\kappa(h),\\varepsilon)$ for the unpreconditioned case, and $\\Delta m_{\\mathrm{prec}}=m(\\kappa_{p},\\varepsilon)-m(\\kappa_{p},\\varepsilon)$ for the preconditioned case (which should be $0$ if $\\kappa_{p}$ is strictly $h$-independent).\n\nImplement a program that evaluates the following test suite. In each test case, all parameters are dimensionless. The mesh refinement is always by a factor of $2$.\n\n- Test case $1$ (happy path, moderate refinement impact):\n  - $h_{0}=0.1$, $c_{\\alpha}=0.8$, $p_{\\alpha}=1$, $c_{\\beta}=2.4$, $p_{\\beta}=0$, $\\underline{\\alpha}=0.5$, $\\overline{\\beta}=2.5$, $\\varepsilon=10^{-6}$.\n- Test case $2$ (coarser initial mesh, smaller impact):\n  - $h_{0}=0.4$, $c_{\\alpha}=0.8$, $p_{\\alpha}=1$, $c_{\\beta}=2.4$, $p_{\\beta}=0$, $\\underline{\\alpha}=0.5$, $\\overline{\\beta}=2.5$, $\\varepsilon=10^{-6}$.\n- Test case $3$ (tight tolerance edge case):\n  - $h_{0}=0.05$, $c_{\\alpha}=0.8$, $p_{\\alpha}=1$, $c_{\\beta}=2.4$, $p_{\\beta}=0$, $\\underline{\\alpha}=0.5$, $\\overline{\\beta}=2.5$, $\\varepsilon=10^{-12}$.\n- Test case $4$ (variant constants, loose tolerance):\n  - $h_{0}=0.08$, $c_{\\alpha}=0.6$, $p_{\\alpha}=1$, $c_{\\beta}=1.8$, $p_{\\beta}=0$, $\\underline{\\alpha}=0.7$, $\\overline{\\beta}=2.1$, $\\varepsilon=10^{-3}$.\n\nYour program must produce the results aggregated into a single line in the following exact format: a Python-like list of lists, where each inner list corresponds to a test case and has exactly two integers $[\\Delta m_{\\mathrm{unprec}}, \\Delta m_{\\mathrm{prec}}]$ in this order. For example, the printed line should look like $[[a,b],[c,d],\\dots]$ with no extra spaces or text.\n\nNo physical units are required since $h$ is normalized by wavelength and all quantities are dimensionless. All angles, if any, must be in radians, but no angles are used here. All final numerical answers must be integers.",
            "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **Linear System**: A fixed-frequency electromagnetic scattering problem discretized by the Method of Moments (MoM) leads to a linear system $A(h)\\,x=b$.\n- **Mesh Parameter**: $h$ is the average edge length normalized by the wavelength (dimensionless).\n- **Unpreconditioned Matrix Scaling**: The smallest singular value of $A(h)$ scales as $O(h^{p_{\\alpha}})$, and the largest singular value scales as $O(h^{-p_{\\beta}})$. The condition number $\\kappa(h)$ scales as $O(h^{-(p_{\\alpha}+p_{\\beta})})$.\n- **EFIE Example**: For the Electric Field Integral Equation (EFIE), a common case is $p_{\\alpha}=1$ and $p_{\\beta}=0$.\n- **GMRES Method**: The Generalized Minimal Residual (GMRES) method is used. Its convergence for a matrix with a spectrum in a real positive interval $[\\alpha,\\beta]$ is bounded using Chebyshev polynomials.\n- **Preconditioner**: A quasi-Helmholtz preconditioner $M$ transforms the system to $M^{-1}A(h)$. The spectrum of the preconditioned matrix is clustered in an $h$-independent interval $[\\underline{\\alpha},\\overline{\\beta}]$ on the positive real axis.\n- **Unpreconditioned Spectrum Model**: The spectrum of $A(h)$ is modeled as being contained in $[\\alpha(h),\\beta(h)]$ where $\\alpha(h)=c_{\\alpha}\\,h^{p_{\\alpha}}$ and $\\beta(h)=c_{\\beta}\\,h^{-p_{\\beta}}$, with $c_{\\alpha}0$, $c_{\\beta}0$, $p_{\\alpha}\\ge 0$, and $p_{\\beta}\\ge 0$.\n- **Unpreconditioned Condition Number**: $\\kappa(h)=\\dfrac{\\beta(h)}{\\alpha(h)}=\\dfrac{c_{\\beta}}{c_{\\alpha}}\\,h^{-(p_{\\alpha}+p_{\\beta})}$.\n- **Preconditioned Spectrum Model**: The spectrum of $M^{-1}A(h)$ is modeled as being contained in $[\\underline{\\alpha},\\overline{\\beta}]$, with $\\underline{\\alpha}0$ and $\\overline{\\beta}0$ independent of $h$.\n- **Preconditioned Condition Number**: $\\kappa_{p}=\\dfrac{\\overline{\\beta}}{\\underline{\\alpha}}$, independent of $h$.\n- **Iteration Count Definition**: The predicted iteration count $m(\\kappa,\\varepsilon)$ for a tolerance $\\varepsilon\\in(0,1)$ and condition number $\\kappa\\ge 1$ is the smallest nonnegative integer satisfying the Chebyshev-based bound.\n- **Task**: Compute the change in predicted iterations, $\\Delta m_{\\mathrm{unprec}}=m(\\kappa(h/2),\\varepsilon)-m(\\kappa(h),\\varepsilon)$ and $\\Delta m_{\\mathrm{prec}}=m(\\kappa_{p},\\varepsilon)-m(\\kappa_{p},\\varepsilon)$, when refining the mesh by a factor of $2$ ($h \\mapsto h/2$).\n- **Test Cases**:\n    - Case 1: $h_{0}=0.1$, $c_{\\alpha}=0.8$, $p_{\\alpha}=1$, $c_{\\beta}=2.4$, $p_{\\beta}=0$, $\\underline{\\alpha}=0.5$, $\\overline{\\beta}=2.5$, $\\varepsilon=10^{-6}$.\n    - Case 2: $h_{0}=0.4$, $c_{\\alpha}=0.8$, $p_{\\alpha}=1$, $c_{\\beta}=2.4$, $p_{\\beta}=0$, $\\underline{\\alpha}=0.5$, $\\overline{\\beta}=2.5$, $\\varepsilon=10^{-6}$.\n    - Case 3: $h_{0}=0.05$, $c_{\\alpha}=0.8$, $p_{\\alpha}=1$, $c_{\\beta}=2.4$, $p_{\\beta}=0$, $\\underline{\\alpha}=0.5$, $\\overline{\\beta}=2.5$, $\\varepsilon=10^{-12}$.\n    - Case 4: $h_{0}=0.08$, $c_{\\alpha}=0.6$, $p_{\\alpha}=1$, $c_{\\beta}=1.8$, $p_{\\beta}=0$, $\\underline{\\alpha}=0.7$, $\\overline{\\beta}=2.1$, $\\varepsilon=10^{-3}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n- **Scientifically Grounded**: The problem statement is firmly rooted in the principles of computational electromagnetics and numerical linear algebra. The description of the Method of Moments (MoM), the dense-discretization breakdown of the Electric Field Integral Equation (EFIE), the behavior of quasi-Helmholtz preconditioners, the GMRES algorithm, and its convergence analysis via Chebyshev polynomials are all standard and well-established concepts in the field. The modeling assumptions are simplified but physically and mathematically consistent representations of these phenomena.\n- **Well-Posed**: The problem is precisely defined. All necessary parameters and models are provided explicitly. The task is to calculate a specific quantity, the change in iteration count, based on a defined formula. The inputs for each test case are complete, and the required output format is unambiguous. A unique solution exists for each test case.\n- **Objective**: The problem is stated in formal, objective language. There are no subjective assessments, opinions, or non-scientific claims.\n\n### Step 3: Verdict and Action\nThe problem is scientifically sound, well-posed, objective, and complete. It is therefore deemed **valid**. A solution will be developed.\n\n## Solution\n\nThe objective is to determine the change in the predicted number of GMRES iterations required to solve the MoM linear system $A(h)x=b$ when the mesh parameter $h$ is refined by a factor of $2$. We analyze both the original (unpreconditioned) system and a preconditioned system.\n\n### GMRES Iteration Count Model\nThe convergence of GMRES for a matrix whose spectrum is contained in a positive real interval $[\\alpha, \\beta]$ can be bounded using Chebyshev polynomials. A standard upper bound for the relative residual norm after $m$ iterations is given by:\n$$\n\\frac{\\|r_m\\|}{\\|r_0\\|} \\le 2 \\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^m\n$$\nwhere $\\kappa = \\beta/\\alpha$ is the condition number of the matrix.\n\nWe are tasked to find the predicted iteration count, $m(\\kappa, \\varepsilon)$, defined as the smallest nonnegative integer $m$ that guarantees the relative residual is below a given tolerance $\\varepsilon \\in (0, 1)$. Thus, we set the bound to be less than or equal to $\\varepsilon$:\n$$\n2 \\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^m \\le \\varepsilon\n$$\nSolving this inequality for $m$ proceeds as follows. First, isolate the term raised to the power of $m$:\n$$\n\\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^m \\le \\frac{\\varepsilon}{2}\n$$\nTaking the natural logarithm of both sides:\n$$\nm \\ln\\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right) \\le \\ln\\left( \\frac{\\varepsilon}{2} \\right)\n$$\nSince $\\kappa \\ge 1$, the term $\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}$ is between $0$ and $1$, making its logarithm negative. Therefore, dividing by it reverses the inequality sign:\n$$\nm \\ge \\frac{\\ln(\\varepsilon/2)}{\\ln\\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)}\n$$\nUsing the property $\\ln(1/x) = -\\ln(x)$, this can be written in a more convenient form:\n$$\nm \\ge \\frac{\\ln(2/\\varepsilon)}{\\ln\\left( \\frac{\\sqrt{\\kappa} + 1}{\\sqrt{\\kappa} - 1} \\right)}\n$$\nThe predicted iteration count $m(\\kappa, \\varepsilon)$ is the smallest integer satisfying this condition. This is obtained by taking the ceiling of the right-hand side. The cases in this problem all have $\\kappa  1$.\n$$\nm(\\kappa, \\varepsilon) = \\left\\lceil \\frac{\\ln(2/\\varepsilon)}{\\ln\\left( \\frac{\\sqrt{\\kappa} + 1}{\\sqrt{\\kappa} - 1} \\right)} \\right\\rceil\n$$\n\n### Unpreconditioned System Analysis\nFor the unpreconditioned system, the condition number $\\kappa(h)$ depends on the mesh parameter $h$:\n$$\n\\kappa(h) = \\frac{c_{\\beta}}{c_{\\alpha}} h^{-(p_{\\alpha} + p_{\\beta})}\n$$\nLet the initial mesh parameter be $h_0$. The condition number is $\\kappa_{\\text{before}} = \\kappa(h_0)$. The initial iteration count is:\n$$\nm_{\\text{before}} = m(\\kappa_{\\text{before}}, \\varepsilon)\n$$\nUpon refinement, the new mesh parameter is $h_{\\text{after}} = h_0 / 2$. The new condition number is:\n$$\n\\kappa_{\\text{after}} = \\kappa(h_0/2) = \\frac{c_{\\beta}}{c_{\\alpha}} \\left(\\frac{h_0}{2}\\right)^{-(p_{\\alpha} + p_{\\beta})} = \\kappa(h_0) \\cdot 2^{p_{\\alpha} + p_{\\beta}} = \\kappa_{\\text{before}} \\cdot 2^{p_{\\alpha} + p_{\\beta}}\n$$\nThe iteration count after refinement is:\n$$\nm_{\\text{after}} = m(\\kappa_{\\text{after}}, \\varepsilon)\n$$\nThe change in the number of iterations is the integer difference:\n$$\n\\Delta m_{\\mathrm{unprec}} = m_{\\text{after}} - m_{\\text{before}}\n$$\n\n### Preconditioned System Analysis\nThe quasi-Helmholtz preconditioner $M$ is designed to remedy the dense-discretization breakdown. The spectrum of the preconditioned matrix $M^{-1}A(h)$ is contained in an interval $[\\underline{\\alpha}, \\overline{\\beta}]$ that is independent of the mesh parameter $h$.\nConsequently, the condition number of the preconditioned system is also independent of $h$:\n$$\n\\kappa_p = \\frac{\\overline{\\beta}}{\\underline{\\alpha}}\n$$\nSince $\\kappa_p$ does not change upon mesh refinement, the predicted number of iterations $m(\\kappa_p, \\varepsilon)$ also remains constant. The change in the iteration count is therefore zero:\n$$\n\\Delta m_{\\mathrm{prec}} = m(\\kappa_p, \\varepsilon) - m(\\kappa_p, \\varepsilon) = 0\n$$\n\n### Calculation for Test Cases\nFor each test case, we will apply these formulas.\n1.  Calculate $\\kappa_{\\text{before}} = \\kappa(h_0)$ and $\\kappa_{\\text{after}} = \\kappa(h_0/2)$ for the unpreconditioned system.\n2.  Use the derived formula for $m(\\kappa, \\varepsilon)$ to find $m_{\\text{before}}$ and $m_{\\text{after}}$.\n3.  Compute $\\Delta m_{\\mathrm{unprec}} = m_{\\text{after}} - m_{\\text{before}}$.\n4.  Set $\\Delta m_{\\mathrm{prec}} = 0$.\nThe final result for each case is the integer pair $[\\Delta m_{\\mathrm{unprec}}, \\Delta m_{\\mathrm{prec}}]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the change in GMRES iteration count for unpreconditioned and\n    preconditioned systems upon mesh refinement, based on a Chebyshev\n    convergence bound model.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each tuple contains: (h0, ca, pa, cb, pb, a_bar, b_bar, eps)\n    test_cases = [\n        (0.1, 0.8, 1, 2.4, 0, 0.5, 2.5, 1e-6),\n        (0.4, 0.8, 1, 2.4, 0, 0.5, 2.5, 1e-6),\n        (0.05, 0.8, 1, 2.4, 0, 0.5, 2.5, 1e-12),\n        (0.08, 0.6, 1, 1.8, 0, 0.7, 2.1, 1e-3),\n    ]\n\n    def calculate_iterations(kappa, epsilon):\n        \"\"\"\n        Calculates the predicted GMRES iteration count based on the\n        Chebyshev optimal polynomial bound for a real positive spectrum.\n\n        Args:\n            kappa (float): The condition number of the matrix.\n            epsilon (float): The desired relative residual tolerance.\n\n        Returns:\n            int: The smallest integer number of iterations predicted by the bound.\n        \"\"\"\n        # The model is valid for kappa  1. All test cases satisfy this.\n        if kappa = 1:\n            # A condition number of 1 implies convergence in 1 step.\n            # While this case isn't hit by the problem data, it's a good practice.\n            return 1\n        \n        # Upper bound on iterations m:\n        # m = ln(2/epsilon) / ln((sqrt(kappa) + 1) / (sqrt(kappa) - 1))\n        sqrt_kappa = np.sqrt(kappa)\n        numerator = np.log(2.0 / epsilon)\n        denominator = np.log((sqrt_kappa + 1.0) / (sqrt_kappa - 1.0))\n        \n        # The predicted iteration count is the smallest integer satisfying the bound.\n        m = np.ceil(numerator / denominator)\n        \n        return int(m)\n\n    results = []\n    for case in test_cases:\n        h0, ca, pa, cb, pb, a_bar, b_bar, eps = case\n        \n        # --- Unpreconditioned Case ---\n        \n        # Mesh parameters before and after refinement by a factor of 2.\n        h_before = h0\n        h_after = h0 / 2.0\n        \n        # Common factors for condition number calculation.\n        kappa_ratio = cb / ca\n        power_sum = pa + pb\n        \n        # Condition numbers before and after refinement.\n        kappa_before = kappa_ratio * (h_before ** -power_sum)\n        kappa_after = kappa_ratio * (h_after ** -power_sum)\n        \n        # Predicted iteration counts.\n        m_unprec_before = calculate_iterations(kappa_before, eps)\n        m_unprec_after = calculate_iterations(kappa_after, eps)\n        \n        delta_m_unprec = m_unprec_after - m_unprec_before\n        \n        # --- Preconditioned Case ---\n        \n        # The preconditioned condition number is independent of h.\n        # Therefore, the iteration count does not change upon mesh refinement.\n        delta_m_prec = 0\n        \n        results.append([delta_m_unprec, delta_m_prec])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The most powerful preconditioners are often those informed by the underlying physics of the problem. This advanced practice explores the design of a physics-informed multigrid preconditioner, one of the most sophisticated techniques for solving large-scale systems. Using the canonical example of a PEC sphere, where the EFIE operator is diagonalizable in the vector spherical harmonics basis, you will implement a surrogate spectral model to analyze the convergence of a two-grid method. This exercise provides a concrete look at how aligning coarse-grid correction spaces with the low-order physical modes of the current distribution can lead to highly efficient and scalable solvers .",
            "id": "3321388",
            "problem": "Consider the Electric Field Integral Equation (EFIE) for a perfectly electrically conducting (PEC) sphere of radius $a$, illuminated by a time-harmonic field with angular frequency $\\omega$ and free-space wavenumber $k$. In the Method of Moments (MoM), we seek the surface current density $\\,\\mathbf{J}(\\mathbf{r})\\,$ such that the boundary condition for the total tangential electric field is satisfied. On a sphere, the EFIE operator acting on tangential currents is diagonalizable in the surface vector spherical harmonics basis. Let the degree be $l \\in \\{1,2,\\dots,L_{\\max}\\}$ and the order be $m \\in \\{-l,-l+1,\\dots,l\\}$. For multigrid, we construct a coarse space spanned by all harmonics with degrees $l \\le L_c$.\n\nPhysics-informed coarse levels: The coarse-level basis is chosen to be the low-order surface spherical harmonics, motivated by the fact that for spherical scatterers, the radiated field decomposes naturally in modes indexed by $l,m$, and low-order modes govern the macroscopic current distribution and scattering characteristics. This choice aligns the coarse space with the dominant physics in EFIE on a sphere.\n\nMultigrid-in-Krylov approach: We consider a two-grid preconditioning framework integrated into a Krylov subspace method, such as the Generalized Minimal Residual (GMRES). The two-grid error-propagation operator has the form\n$$\n\\mathbf{E}_{\\mathrm{TG}} \\;=\\; \\left(\\mathbf{I} - \\mathbf{P}\\,\\mathbf{A}_c^{-1}\\,\\mathbf{R}\\,\\mathbf{A}\\right)\\,\\mathbf{S},\n$$\nwhere $\\mathbf{A}$ is the fine-level operator, $\\mathbf{R}$ is the restriction to the coarse space, $\\mathbf{P}$ is the prolongation from coarse to fine, $\\mathbf{A}_c$ is the coarse operator, and $\\mathbf{S}$ is the smoothing operator. Assuming exact coarse-grid solve and exact Galerkin projection, the coarse correction annihilates all error components in the coarse subspace. In the spherical harmonics basis, where $\\mathbf{A}$ is diagonal, the two-grid error factor for each mode $(l,m)$ not represented on the coarse grid reduces to the smoothing factor. The two-grid convergence factor is defined as the spectral radius\n$$\n\\rho_{\\mathrm{TG}} \\;=\\; \\max_{l  L_c,\\, -l \\le m \\le l} \\left| s_l \\right|,\n$$\nwhere $s_l$ is the scalar smoothing factor for degree $l$. Since the operator is diagonal in $(l,m)$, $s_l$ is independent of $m$.\n\nSurrogate EFIE spectral model: To make the problem computationally self-contained and to capture the physics-informed dependence on $l$, we adopt a surrogate spectral law that reflects the qualitative behavior of EFIE on a sphere. In the vector spherical harmonics basis, for each degree $l$, we model the fine-level operator eigenvalue as\n$$\n\\lambda_l(k a) \\;=\\; i\\,\\frac{l(l+1) + (k a)^2}{2l+1} \\;+\\; \\varepsilon\\,\\frac{l(l+1)}{2l+1},\n$$\nwhere $k a$ is the dimensionless size parameter, and $\\varepsilon$ is a small positive real parameter representing dissipative or discretization leakage. This surrogate produces increasing imaginary parts with $l$ (oscillatory behavior associated with radiating modes) and a small real part that regularizes the operator. We normalize units by wave impedance so that $\\lambda_l$ is dimensionless and the convergence factor $\\rho_{\\mathrm{TG}}$ is dimensionless. No physical units are required in the output; all quantities are dimensionless.\n\nTwo smoothing options are considered:\n\n1. Diffusive smoother (physics-informed filter): Motivated by implicit time stepping of the surface diffusion equation for tangential currents, the smoother attenuates high-degree harmonics according to the surface Laplace–Beltrami spectrum. The scalar smoothing factor for degree $l$ is\n$$\ns_l^{\\mathrm{diff}}(\\mu,p) \\;=\\; \\frac{1}{1 + \\mu \\,\\big(l(l+1)\\big)^p},\n$$\nwhere $\\mu0$ controls the strength and $p \\ge 1$ controls the order of diffusion. This filter is independent of the EFIE operator and acts as a physics-informed smoother: larger $l$ experience stronger attenuation.\n\n2. Operator-weighted Richardson smoother: A single Richardson step with parameter $\\omega0$ and scaling $\\gamma0$ applied to the surrogate EFIE operator yields\n$$\ns_l^{\\mathrm{Rich}}(\\omega,\\gamma,k a) \\;=\\; \\left|\\,1 - \\omega\\,\\frac{\\lambda_l(k a)}{\\gamma}\\,\\right|,\n$$\nwhere $\\gamma$ is a scaling parameter chosen to normalize the spectrum, for example $\\gamma = \\max_{1 \\le l \\le L_{\\max}} |\\lambda_l(k a)|$.\n\nTwo-grid convergence factor quantification: With exact coarse-grid correction, all modes with $l \\le L_c$ are removed, and the convergence factor is determined by the worst remaining mode:\n$$\n\\rho_{\\mathrm{TG}} =\n\\begin{cases}\n\\displaystyle \\max_{l=L_c+1,\\dots,L_{\\max}} \\left| s_l^{\\mathrm{diff}}(\\mu,p) \\right|,  \\text{for diffusive smoother} \\\\[1em]\n\\displaystyle \\max_{l=L_c+1,\\dots,L_{\\max}} \\left| s_l^{\\mathrm{Rich}}(\\omega,\\gamma,k a) \\right|,  \\text{for Richardson smoother}\n\\end{cases}\n$$\n\nYour task: Implement a program that, for the parameter sets defined below, constructs the coarse subspace degree threshold $L_c$, evaluates the appropriate smoothing factors for all degrees $l$ in $[1,L_{\\max}]$, applies the exact coarse correction (error for $l \\le L_c$ is zero), and computes the two-grid convergence factor $\\rho_{\\mathrm{TG}}$ as the maximum magnitude of the remaining smoothing factors. The output must be a single line containing the four results as a comma-separated list enclosed in square brackets. Express each convergence factor as a float rounded to six decimal places. Angles do not appear in the input; all quantities are dimensionless.\n\nTest suite:\n\n- Case 1 (diffusive smoother, happy path): $k a = 2.0$, $L_{\\max} = 20$, $L_c = 3$, $\\mu = 0.5$, $p = 1$, $\\varepsilon = 0.02$.\n- Case 2 (operator-weighted Richardson, scaling by spectral radius): $k a = 2.0$, $L_{\\max} = 20$, $L_c = 3$, $\\omega = 0.4$, $\\gamma = \\max_{1 \\le l \\le L_{\\max}} |\\lambda_l(k a)|$, $\\varepsilon = 0.02$.\n- Case 3 (diffusive smoother, high-frequency edge): $k a = 10.0$, $L_{\\max} = 30$, $L_c = 5$, $\\mu = 1.0$, $p = 2$, $\\varepsilon = 0.02$.\n- Case 4 (boundary coarse space covers fine; convergence in one coarse correction): $k a = 6.0$, $L_{\\max} = 12$, $L_c = 12$, $\\mu = 0.8$, $p = 1$, $\\varepsilon = 0.02$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), with each result rounded to six decimal places.",
            "solution": "The problem is subjected to validation against the established criteria.\n\n### Step 1: Extract Givens\n- **Physical System**: Electric Field Integral Equation (EFIE) for a perfectly electrically conducting (PEC) sphere of radius $a$.\n- **Excitation**: Time-harmonic field with angular frequency $\\omega$ and wavenumber $k$.\n- **Basis Functions**: Surface vector spherical harmonics, indexed by degree $l \\in \\{1, 2, \\dots, L_{\\max}\\}$ and order $m$.\n- **Coarse Space**: Spanned by harmonics with degree $l \\le L_c$.\n- **Multigrid Framework**: A two-grid preconditioning approach within a Krylov method (e.g., GMRES).\n- **Two-Grid Error-Propagation Operator**: $\\mathbf{E}_{\\mathrm{TG}} = (\\mathbf{I} - \\mathbf{P}\\,\\mathbf{A}_c^{-1}\\,\\mathbf{R}\\,\\mathbf{A})\\,\\mathbf{S}$.\n- **Assumptions**: Exact coarse-grid solve and exact Galerkin projection.\n- **Surrogate Eigenvalue Model**:\n  $$\n  \\lambda_l(k a) = i\\,\\frac{l(l+1) + (k a)^2}{2l+1} + \\varepsilon\\,\\frac{l(l+1)}{2l+1}\n  $$\n  where $i = \\sqrt{-1}$, $ka$ is the dimensionless size parameter, and $\\varepsilon$ is a small positive real parameter.\n- **Diffusive Smoother Factor**:\n  $$\n  s_l^{\\mathrm{diff}}(\\mu,p) = \\frac{1}{1 + \\mu \\,\\big(l(l+1)\\big)^p}\n  $$\n  with parameters $\\mu  0$ and $p \\ge 1$.\n- **Operator-Weighted Richardson Smoother Factor**:\n  $$\n  s_l^{\\mathrm{Rich}}(\\omega,\\gamma,k a) = \\left|\\,1 - \\omega\\,\\frac{\\lambda_l(k a)}{\\gamma}\\,\\right|\n  $$\n  with parameters $\\omega  0$ and $\\gamma$.\n- **Two-Grid Convergence Factor Definition**:\n  $$\n  \\rho_{\\mathrm{TG}} = \\max_{l=L_c+1,\\dots,L_{\\max}} |s_l|\n  $$\n- **Test Cases**:\n  1. **Type**: Diffusive. Parameters: $k a = 2.0$, $L_{\\max} = 20$, $L_c = 3$, $\\mu = 0.5$, $p = 1$, $\\varepsilon = 0.02$.\n  2. **Type**: Richardson. Parameters: $k a = 2.0$, $L_{\\max} = 20$, $L_c = 3$, $\\omega = 0.4$, $\\gamma = \\max_{1 \\le l \\le L_{\\max}} |\\lambda_l(k a)|$, $\\varepsilon = 0.02$.\n  3. **Type**: Diffusive. Parameters: $k a = 10.0$, $L_{\\max} = 30$, $L_c = 5$, $\\mu = 1.0$, $p = 2$, $\\varepsilon = 0.02$.\n  4. **Type**: Diffusive. Parameters: $k a = 6.0$, $L_{\\max} = 12$, $L_c = 12$, $\\mu = 0.8$, $p = 1$, $\\varepsilon = 0.02$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and scientifically sound.\n- **Scientifically Grounded**: The problem is rooted in computational electromagnetics, employing standard concepts like the Method of Moments, vector spherical harmonics for spherical geometries, and multigrid preconditioning. The use of a surrogate spectral model is a valid technique to create a self-contained, analyzable problem that retains the essential physical characteristics of the underlying operator.\n- **Well-Posed**: All necessary formulas, constants, and parameters for each test case are explicitly provided. The objective—to compute the two-grid convergence factor $\\rho_{\\mathrm{TG}}$—is unambiguous and leads to a unique numerical solution for each case.\n- **Objective**: The problem is stated in precise, quantitative, and unbiased technical language.\n- **Completeness and Consistency**: The problem is self-contained. The special case where $L_c = L_{\\max}$ (Case 4) is a well-defined limit where the coarse space encompasses the entire fine space, leading to a predictable outcome rather than a contradiction.\n- **No other flaws are detected.** The problem is non-trivial, verifiable, and directly pertinent to the specified scientific domain.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\n### Solution Derivation\n\nThe objective is to compute the two-grid convergence factor, $\\rho_{\\mathrm{TG}}$, for four distinct scenarios. This factor represents the slowest-converging error component after one iteration of a two-grid cycle, which consists of a smoothing step and a coarse-grid correction step. The problem states that the coarse-grid correction is exact, meaning any error components corresponding to basis functions with degree $l \\le L_c$ are perfectly eliminated. Consequently, the convergence is governed entirely by the effect of the smoother on the remaining \"high-frequency\" modes, which are those with degree $l$ in the range $\\{L_c+1, \\dots, L_{\\max}\\}$. The convergence factor is the maximum magnitude of the smoothing factors over this range of modes.\n\n$$\n\\rho_{\\mathrm{TG}} = \\max_{l=L_c+1, \\dots, L_{\\max}} |s_l|\n$$\n\nIf the range of indices is empty (i.e., if $L_c \\ge L_{\\max}$), it signifies that the coarse space contains all modes of the fine space. The coarse-grid correction is therefore exact for all components, and the error is annihilated in a single step. In this situation, the convergence factor $\\rho_{\\mathrm{TG}}$ is exactly $0$.\n\nThe specific calculation depends on the type of smoother employed.\n\n**1. Diffusive Smoother**\n\nThe smoothing factor is given by a real-valued function:\n$$\ns_l^{\\mathrm{diff}}(\\mu,p) = \\frac{1}{1 + \\mu \\,\\big(l(l+1)\\big)^p}\n$$\nGiven that $\\mu  0$, $p \\ge 1$, and $l \\ge 1$, the term $\\mu \\, (l(l+1))^p$ is always positive. Therefore, $s_l^{\\mathrm{diff}}$ is always positive, and $|s_l^{\\mathrm{diff}}| = s_l^{\\mathrm{diff}}$. The function $s_l^{\\mathrm{diff}}$ is a monotonically decreasing function of $l$. Thus, the maximum value of $s_l^{\\mathrm{diff}}$ for $l \\in \\{L_c+1, \\dots, L_{\\max}\\}$ will occur at the smallest value of $l$ in this set, which is $l = L_c+1$.\nThe convergence factor simplifies to:\n$$\n\\rho_{\\mathrm{TG}} = s_{L_c+1}^{\\mathrm{diff}} = \\frac{1}{1 + \\mu \\,\\big((L_c+1)(L_c+2)\\big)^p}\n$$\n\n**2. Operator-Weighted Richardson Smoother**\n\nThe smoothing factor is complex-valued and given by:\n$$\ns_l^{\\mathrm{Rich}}(\\omega,\\gamma,k a) = \\left|\\,1 - \\omega\\,\\frac{\\lambda_l(k a)}{\\gamma}\\,\\right|\n$$\nwhere $\\lambda_l(k a)$ is the surrogate eigenvalue for mode $l$:\n$$\n\\lambda_l(k a) = \\varepsilon\\,\\frac{l(l+1)}{2l+1} + i\\,\\frac{l(l+1) + (k a)^2}{2l+1}\n$$\nThe calculation proceeds in two stages:\na. First, determine the scaling parameter $\\gamma$ by finding the maximum magnitude of the eigenvalues over the entire fine space:\n$$\n\\gamma = \\max_{l=1,\\dots,L_{\\max}} |\\lambda_l(k a)|\n$$\nb. Second, compute the smoothing factor $s_l^{\\mathrm{Rich}}$ for each mode in the high-frequency range $l \\in \\{L_c+1, \\dots, L_{\\max}\\}$ and find the maximum among them. Unlike the diffusive case, there is no simple analytical maximum, so a direct search is required.\n$$\n\\rho_{\\mathrm{TG}} = \\max_{l=L_c+1,\\dots,L_{\\max}} \\left|\\,1 - \\omega\\,\\frac{\\lambda_l(k a)}{\\gamma}\\,\\right|\n$$\n\n### Application to Test Cases\n\n**Case 1: Diffusive Smoother**\nParameters: $L_{\\max} = 20$, $L_c = 3$, $\\mu = 0.5$, $p = 1$. The convergence factor is determined by the mode $l=L_c+1=4$.\n$$\n\\rho_{\\mathrm{TG}} = \\frac{1}{1 + (0.5) \\cdot \\big((4)(4+1)\\big)^{1}} = \\frac{1}{1 + 0.5 \\cdot 20} = \\frac{1}{1+10} = \\frac{1}{11} \\approx 0.090909\n$$\n\n**Case 2: Richardson Smoother**\nParameters: $k a = 2.0$, $L_{\\max} = 20$, $L_c = 3$, $\\omega = 0.4$, $\\varepsilon = 0.02$.\na. Calculation of $\\gamma$: We must compute $\\lambda_l(2.0)$ for $l \\in \\{1,\\dots,20\\}$ and find the maximum magnitude. This is a numerical task. A loop from $l=1$ to $20$ reveals that the maximum magnitude occurs at $l=20$, where $|\\lambda_{20}(2.0)| \\approx 10.34354$. Thus, $\\gamma \\approx 10.34354$.\nb. Calculation of $\\rho_{\\mathrm{TG}}$: We then compute $|1 - 0.4 \\cdot \\lambda_l(2.0) / \\gamma|$ for $l \\in \\{4, \\dots, 20\\}$. A numerical search over this range yields a maximum value. The calculation must be performed numerically.\n\n**Case 3: Diffusive Smoother**\nParameters: $L_{\\max} = 30$, $L_c = 5$, $\\mu = 1.0$, $p = 2$. The convergence factor is determined by the mode $l=L_c+1=6$.\n$$\n\\rho_{\\mathrm{TG}} = \\frac{1}{1 + (1.0) \\cdot \\big((6)(6+1)\\big)^{2}} = \\frac{1}{1 + 42^2} = \\frac{1}{1+1764} = \\frac{1}{1765} \\approx 0.000567\n$$\n\n**Case 4: Special Case**\nParameters: $L_{\\max} = 12$, $L_c = 12$.\nHere, $L_c = L_{\\max}$. The set of modes to be smoothed, $\\{l | L_c  l \\le L_{\\max}\\}$, is empty. This means the coarse-grid correction perfectly resolves the error in the entire space in one step.\n$$\n\\rho_{\\mathrm{TG}} = 0.0\n$$\n\nThe provided Python code will implement these calculations numerically to obtain the required precision.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the two-grid convergence factor rho_TG for four test cases\n    based on a surrogate spectral model for the EFIE on a sphere.\n    \"\"\"\n\n    def get_lambda_l(l, ka, eps):\n        \"\"\"\n        Computes the surrogate eigenvalue lambda_l for a given degree l, size\n        parameter ka, and leakage parameter eps.\n        \"\"\"\n        l_term = float(l * (l + 1))\n        denom = float(2 * l + 1)\n        real_part = eps * l_term / denom\n        imag_part = (l_term + ka**2) / denom\n        return complex(real_part, imag_part)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Diffusive smoother, happy path\n        {'type': 'diffusive', 'ka': 2.0, 'Lmax': 20, 'Lc': 3, 'mu': 0.5, 'p': 1, 'eps': 0.02},\n        # Case 2: Operator-weighted Richardson, scaling by spectral radius\n        {'type': 'richardson', 'ka': 2.0, 'Lmax': 20, 'Lc': 3, 'omega': 0.4, 'eps': 0.02},\n        # Case 3: Diffusive smoother, high-frequency edge\n        {'type': 'diffusive', 'ka': 10.0, 'Lmax': 30, 'Lc': 5, 'mu': 1.0, 'p': 2, 'eps': 0.02},\n        # Case 4: Coarse space covers fine space\n        {'type': 'diffusive', 'ka': 6.0, 'Lmax': 12, 'Lc': 12, 'mu': 0.8, 'p': 1, 'eps': 0.02}\n    ]\n\n    results = []\n    for case in test_cases:\n        Lmax = case['Lmax']\n        Lc = case['Lc']\n\n        # If the coarse space covers the entire fine space, convergence is perfect (factor 0).\n        if Lc = Lmax:\n            results.append(0.0)\n            continue\n\n        if case['type'] == 'diffusive':\n            mu = case['mu']\n            p = case['p']\n            \n            # For the diffusive smoother, the function s_l is monotonically decreasing with l.\n            # The maximum over l  Lc occurs at the first value, l = Lc + 1.\n            l_eval = Lc + 1\n            term = (l_eval * (l_eval + 1))**p\n            rho_tg = 1.0 / (1.0 + mu * term)\n            results.append(rho_tg)\n\n        elif case['type'] == 'richardson':\n            ka = case['ka']\n            omega = case['omega']\n            eps = case['eps']\n            \n            # Stage 1: Calculate gamma, the maximum eigenvalue magnitude over the full fine space.\n            l_range_fine = np.arange(1, Lmax + 1)\n            lambdas_fine = [get_lambda_l(l, ka, eps) for l in l_range_fine]\n            gamma = np.max(np.abs(lambdas_fine))\n            \n            # Stage 2: Calculate rho_TG by finding the max smoothing factor over the high-frequency modes.\n            l_range_high = np.arange(Lc + 1, Lmax + 1)\n            lambdas_high = np.array([get_lambda_l(l, ka, eps) for l in l_range_high])\n            \n            s_l_values = np.abs(1.0 - omega * lambdas_high / gamma)\n            rho_tg = np.max(s_l_values)\n            results.append(rho_tg)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{res:.6f}' for res in results)}]\")\n\nsolve()\n```"
        }
    ]
}