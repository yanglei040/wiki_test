## Introduction
The fidelity of astrophysical $N$-body simulations hinges on their ability to respect fundamental physical laws, particularly the conservation of energy and momentum. These principles are not just benchmarks for accuracy but the very foundation upon which physically meaningful long-term integrations are built. However, transitioning from the continuous laws of physics to discrete numerical algorithms on finite-precision computers introduces numerous pathways for error. Without careful design, simulations can exhibit unphysical drifts in energy and momentum, leading to incorrect results such as artificial [orbital decay](@entry_id:160264) or spurious system acceleration. This article addresses the critical gap between exact physical theory and its practical numerical implementation.

This article provides a comprehensive guide to understanding and controlling these conservation laws. In "Principles and Mechanisms," we will explore the theoretical origins of conservation from Hamiltonian dynamics and dissect the numerical sources of violation, from integrator choice to force approximation. "Applications and Interdisciplinary Connections" will demonstrate how these principles are actively used to verify code, construct [subgrid models](@entry_id:755601), and extend simulations to complex multiphysics and relativistic regimes. Finally, "Hands-On Practices" will offer practical exercises to solidify these concepts by diagnosing integrator performance and ensuring robust simulation setup. This structured approach will equip the reader with the necessary theoretical knowledge and practical techniques to build and analyze high-fidelity $N$-body simulations.

## Principles and Mechanisms

The fidelity of an $N$-body simulation is fundamentally judged by its ability to respect the physical laws governing the system it models. Among the most crucial of these are the conservation laws for energy, linear momentum, and angular momentum. While these principles are exact in the continuous, analytical description of gravitational dynamics, their preservation in a numerical context is not automatic. The process of [discretization](@entry_id:145012), approximation, and implementation on finite-precision hardware introduces numerous mechanisms that can violate these sacred laws of physics. This chapter delves into the principles of conservation, explores the mechanisms by which they are threatened in simulations, and presents the techniques developed to mitigate these violations.

### Conservation Laws in Continuous Hamiltonian Systems

The dynamics of an isolated, self-gravitating $N$-body system can be elegantly described within the Hamiltonian framework. The state of the system is defined by the [generalized coordinates](@entry_id:156576) (positions $\mathbf{r}_i$) and their conjugate momenta ($\mathbf{p}_i$) for each of the $N$ particles. The system's evolution is governed by the Hamiltonian, $H$, which is the sum of the total kinetic energy, $T$, and the total potential energy, $V$:

$H(\{\mathbf{r}_i, \mathbf{p}_i\}) = T(\{\mathbf{p}_i\}) + V(\{\mathbf{r}_i\}) = \sum_{i=1}^N \frac{|\mathbf{p}_i|^2}{2m_i} - \sum_{1 \le i  j \le N} \frac{G m_i m_j}{|\mathbf{r}_i - \mathbf{r}_j|}$

Here, $m_i$ is the mass of particle $i$ and $G$ is the gravitational constant. The fundamental conservation laws of the system are a direct consequence of the symmetries of this Hamiltonian, a profound connection established by Noether's theorem. Each continuous symmetry of the Hamiltonian corresponds to a conserved quantity, or **Noether charge**.

*   **Conservation of Energy:** The Hamiltonian has no explicit dependence on time, $t$. This invariance under time translation ($t \to t + \delta t$) implies that the Hamiltonian itself is a conserved quantity. The total energy, $E=H$, of an [isolated system](@entry_id:142067) is constant.

*   **Conservation of Linear Momentum:** The potential energy term depends only on the relative separations of particles, $|\mathbf{r}_i - \mathbf{r}_j|$. Consequently, the Hamiltonian is invariant under a uniform [spatial translation](@entry_id:195093) of all particles ($\mathbf{r}_i \to \mathbf{r}_i + \boldsymbol{\epsilon}$). This [homogeneity of space](@entry_id:172987) gives rise to the conservation of the [total linear momentum](@entry_id:173071), $\mathbf{P} = \sum_{i=1}^N \mathbf{p}_i$. This is the Hamiltonian expression of Newton's third law; for every internal force $\mathbf{F}_{ij}$, there is an equal and opposite force $\mathbf{F}_{ji}$, ensuring the sum of all internal forces is zero.

*   **Conservation of Angular Momentum:** The potential energy also depends only on the scalar distance between particles, which is invariant under a uniform rotation of all position and momentum vectors. This [isotropy of space](@entry_id:171241) ensures the conservation of the [total angular momentum](@entry_id:155748), $\mathbf{L} = \sum_{i=1}^N \mathbf{r}_i \times \mathbf{p}_i$. This arises because gravitational forces are **[central forces](@entry_id:267832)**, acting along the line connecting two particles, which means they produce no internal torques.

These conservation laws, derived from the foundational symmetries of Galilean mechanics, are exact properties of the continuous physical system . Any valid numerical simulation must, to the greatest extent possible, honor these constraints.

### The Challenge of Temporal Discretization: Symplectic Integration

The first and most immediate challenge to conservation arises from the [discretization](@entry_id:145012) of time. Numerical integrators advance the system in discrete steps of size $h$, replacing the continuous flow of Hamilton's equations with a finite-difference map. The properties of this map dictate the long-term fidelity of the simulation.

Consider the classic Kepler problem, which is the foundation of all gravitational dynamics. A simple, first-order integrator like the **explicit forward Euler method** is disastrous for long-term orbital simulations. It is neither time-reversible nor does it preserve the geometric structure of Hamiltonian dynamics. As a result, the numerical energy of a [bound orbit](@entry_id:169599) integrated with this method will exhibit **secular drift**—a systematic, unbounded increase or decrease over time—causing the simulated object to spiral away or decay artificially .

The key to excellent long-term conservation behavior lies in using integrators that preserve the geometric structure of phase space. Such integrators are known as **[symplectic integrators](@entry_id:146553)**. A map is **symplectic** if it preserves the differential two-form $\sum_i d\mathbf{p}_i \wedge d\mathbf{r}_i$, which is equivalent to preserving the phase-space volume elements over time. While this may seem abstract, its practical consequence is profound. For a separable Hamiltonian of the form $H=T(\mathbf{p}) + V(\mathbf{r})$, a widely used family of explicit, second-order symplectic integrators is the **leapfrog** or **velocity Verlet** scheme. Other methods, like the **implicit [midpoint rule](@entry_id:177487)**, are also symplectic.

When applied to a Hamiltonian system, a symplectic integrator does not conserve the true energy $H$ exactly. Instead, it exactly conserves a nearby, perturbed Hamiltonian known as the **shadow Hamiltonian**, $\tilde{H}$. For a second-order, time-reversible symplectic scheme, the shadow Hamiltonian can be expressed as a series in even powers of the timestep $h$:

$\tilde{H} = H + h^2 H_2 + h^4 H_4 + \mathcal{O}(h^6)$

Because the numerical trajectory lies on a [level surface](@entry_id:271902) of this conserved quantity $\tilde{H}$, the true energy $H = \tilde{H} - h^2 H_2 - \dots$ cannot drift secularly. It must instead exhibit bounded, periodic oscillations around the constant value of $\tilde{H}$ . The amplitude of these energy oscillations scales with $h^2$. This remarkable property explains why leapfrog and other symplectic methods are the integrators of choice for long-term simulations in astrophysics.

The leading-order correction term, $H_2$, which governs the dominant deviation of the shadow Hamiltonian from the true one, can be derived using the Baker-Campbell-Hausdorff formula. For the common kick-drift-kick (KDK) form of the [leapfrog integrator](@entry_id:143802), this term is given by :

$H_2 = \frac{1}{12} \sum_{i,k} \frac{1}{m_i m_k} \mathbf{p}_i \cdot \left( \frac{\partial \mathbf{F}_k}{\partial \mathbf{r}_i} \mathbf{p}_k \right) - \frac{1}{24} \sum_{i} \frac{|\mathbf{F}_i|^2}{m_i}$

where $\mathbf{F}_i$ is the force on particle $i$ and $\frac{\partial \mathbf{F}_k}{\partial \mathbf{r}_i}$ is the [tidal tensor](@entry_id:755970). This expression reveals that the deviation of the conserved quantity from the true energy depends on the forces and their spatial gradients, highlighting why energy errors are largest in regions of strong interactions.

### Practical Modifications and Their Consequences

#### Force Regularization and Softening

The Newtonian force law has a singularity at zero separation, leading to infinite forces and computationally intractable timesteps during very close encounters. A standard technique to regularize the dynamics is **[gravitational softening](@entry_id:146273)**, where the [pairwise potential](@entry_id:753090) is modified. A common choice is the **Plummer potential**:

$V_{ij} = -\frac{G m_i m_j}{\sqrt{|\mathbf{r}_i - \mathbf{r}_j|^2 + \epsilon^2}}$

where $\epsilon  0$ is the **[softening length](@entry_id:755011)**. This modification creates a new Hamiltonian system. This softened system still possesses time-translation, space-translation, and [rotational invariance](@entry_id:137644), and therefore its exact dynamics still conserve total energy, linear momentum, and angular momentum . However, the conserved energy is now the energy of the *softened* system, not the original Newtonian energy.

The primary benefit of softening is that the force magnitude is now bounded, reaching a maximum value at a separation of $r = \epsilon/\sqrt{2}$ . This tames the dynamics of close encounters. Furthermore, in the small-separation limit ($r \ll \epsilon$), the potential approximates a harmonic oscillator. This introduces a maximum natural frequency into the system, $\omega_{\max} \approx \sqrt{G(m_1+m_2)/\epsilon^3}$, which provides a clear criterion for timestep selection: the timestep $h$ must be small enough to resolve this fastest timescale (i.e., $h \ll 2\pi/\omega_{\max}$) to control energy errors .

#### Adaptive Timestepping

In systems with a large [dynamic range](@entry_id:270472), such as star clusters or galaxies, a single fixed timestep for all particles is inefficient. A major pitfall, however, is that naive, asynchronous, per-particle adaptive timesteps—where each particle's step size depends on its local conditions—generally break the time-reversibility and symplecticity of the integration scheme. This destroys the existence of a shadow Hamiltonian and reintroduces secular [energy drift](@entry_id:748982) .

To preserve conservation properties, more sophisticated adaptive schemes are required. A successful approach is **hierarchical block-stepping**, where particles are binned into levels with timesteps given by $h_k = h_0 / 2^k$. A key principle for maintaining exact [momentum conservation](@entry_id:149964) in such schemes is that all force interactions must be implemented as symmetric, pairwise momentum exchanges applied simultaneously to both particles. The block-stepping framework can be designed to enforce this by ensuring that the interaction between particles $i$ and $j$ is always handled with a synchronized update using a timestep determined by the slower of the two particles. By adhering strictly to Newton's third law at the discrete level for every interaction, the total momentum can be conserved to machine precision .

### Conservation in Approximate Gravity Solvers

For large $N$, direct-[summation methods](@entry_id:203631) with their $\mathcal{O}(N^2)$ complexity become too slow. Approximate methods, such as Particle-Mesh (PM) and [tree codes](@entry_id:756159), reduce this cost but introduce new sources of error that challenge conservation laws.

#### Particle-Mesh (PM) Methods

PM methods solve Poisson's equation on a grid. This process involves a **[mass assignment](@entry_id:751704) scheme** (e.g., Nearest-Grid-Point (NGP), Cloud-in-Cell (CIC), Triangular-Shaped-Cloud (TSC)) to deposit particle mass onto the grid, a fast Poisson solver (typically using FFTs), and a force interpolation step to get forces back at particle positions.

*   **Momentum Conservation:** In a periodic domain, if the same scheme is used for mass deposition and force interpolation, the [total linear momentum](@entry_id:173071) is exactly conserved. This is because the underlying discrete operators on the grid respect a discrete form of [translational invariance](@entry_id:195885), and the symmetric application of deposition and interpolation kernels ensures that self-forces are zero and pairwise interactions effectively cancel .

*   **Energy and Angular Momentum:** Conservation of these quantities is imperfect. The grid breaks continuous [rotational symmetry](@entry_id:137077), introducing **force anisotropy**, especially at high wavenumbers (short wavelengths). This leads to errors in angular momentum. Furthermore, the [mass assignment](@entry_id:751704) process acts as a smoothing filter, and the finite grid resolution leads to **aliasing**, where unresolved high-frequency power is incorrectly mapped onto lower frequencies. Smoother assignment schemes like TSC suppress high-wavenumber content more effectively, reducing both [aliasing](@entry_id:146322) and anisotropy artifacts compared to the cruder NGP scheme . These errors mean the PM [force field](@entry_id:147325) is not perfectly Hamiltonian, so even a symplectic integrator will not exhibit perfect long-term energy behavior.

#### Tree Codes

Tree codes like the Barnes-Hut algorithm approximate the force from a distant group of particles by treating them as a single cell represented by its low-order [multipole moments](@entry_id:191120) (e.g., monopole). The approximation is accepted if the cell's size $l$ and its distance $R$ satisfy an **opening-angle criterion**, $l/R \le \theta$. This approximation introduces two distinct violations of conservation laws.

1.  **Energy Error from Force Truncation:** The force itself is an approximation. Truncating the [multipole expansion](@entry_id:144850) of the potential at a finite order (e.g., quadrupole) means the calculated force is not derived from the true Newtonian potential. The leading error term in the potential for a cell with vanishing dipole and quadrupole moments (like a symmetric cube) comes from the hexadecapole moment and scales as $(l/R)^4$. The resulting energy error for a particle interacting with this cell therefore scales with the fourth power of the opening angle, $\epsilon_E \propto \theta^4$ . This error is inherent to the force approximation.

2.  **Momentum Violation from Non-Mutual Interactions:** A more subtle problem arises if the [tree traversal](@entry_id:261426) is non-mutual. In a typical implementation, the force on a particle $p$ from a cell $\mathcal{C}$ is computed using the cell's [multipole moments](@entry_id:191120). However, the reaction force on the particles *within* cell $\mathcal{C}$ from particle $p$ is often computed on an individual basis. The force on $p$ is approximate, while the forces on the contents of $\mathcal{C}$ are exact. This asymmetry violates Newton's third law at the level of the interaction pair $(p, \mathcal{C})$. The result is a net spurious force on the system, leading to a violation of linear [momentum conservation](@entry_id:149964). The magnitude of this spurious force, originating from the lowest-order non-zero multipole (typically the quadrupole), leads to a force error that scales as $(l/R)^3$. Thus, the momentum error rate scales with the cube of the opening angle, $\dot{\mathbf{P}}_{\text{err}} \propto \theta^3$ .

### The Bedrock of Error: Finite-Precision Arithmetic

Finally, even the most carefully designed algorithm must confront the limitations of [floating-point arithmetic](@entry_id:146236). Standard IEEE 754 arithmetic is not associative; for floating-point numbers $a, b, c$, it is not guaranteed that $(a+b)+c = a+(b+c)$.

This has direct consequences for momentum conservation. In a direct-summation code, the total force on the system is $\sum_i \mathbf{F}_i = \sum_i \sum_{j \neq i} \mathbf{f}_{ij}$. Even if we use a symmetric scheme where for each pair $(i,j)$ we compute $\mathbf{f}_{ij}$ and add it to $\mathbf{F}_i$ while adding $-\mathbf{f}_{ij}$ to $\mathbf{F}_j$, the final sum $\sum_i \mathbf{F}_i$ may not be zero. The order in which the pairwise forces are added to the running totals for each particle affects the final result due to rounding errors. This results in a small but non-zero net force, causing a random walk in the system's total momentum . This error can be dramatically reduced by using **[compensated summation](@entry_id:635552)** (e.g., Kahan's algorithm), which tracks the "lost" low-order bits from each addition and incorporates them into the next, bringing the momentum error down close to machine precision .

This issue is exacerbated in [parallel computing](@entry_id:139241) environments (multi-core CPUs, GPUs). The scheduling of threads is non-deterministic, meaning the order of force accumulation can vary from one run to the next. This leads to bit-wise different results for forces, and therefore for the final energy and momentum, even when starting from identical [initial conditions](@entry_id:152863). This lack of reproducibility is a major challenge. The solution is to enforce **determinism** by combining a fixed, canonical order for all operations with high-precision accumulation methods like [compensated summation](@entry_id:635552). This ensures that every run produces bit-wise identical results, restoring reproducibility at a small computational cost .

In summary, maintaining the [conservation of energy and momentum](@entry_id:193044) in $N$-body simulations is a multi-layered challenge. It requires a deep understanding of the connection between physical [symmetries and conservation laws](@entry_id:168267), the geometric properties of numerical integrators, the nature of approximations in fast algorithms, and the fundamental limitations of computer arithmetic.