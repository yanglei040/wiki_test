{
    "hands_on_practices": [
        {
            "introduction": "Before we can devise strategies to improve load balance, we must first learn how to quantify its impact. This exercise provides a foundational, hands-on calculation of key performance metrics for a parallel hydrodynamics code. By applying a standard performance model that accounts for computation, communication latency, and bandwidth, you will calculate the speedup and efficiency, thereby gaining a concrete understanding of how imbalance limits parallel performance.",
            "id": "3516510",
            "problem": "A uniform Cartesian mesh hydrodynamics solver advances one explicit Godunov timestep with domain decomposition across $P=8$ processors. The per-processor local update counts (number of cell-updates performed this timestep) are\n$$\nw_1=1200,\\quad w_2=1150,\\quad w_3=1230,\\quad w_4=1070,\\quad w_5=1180,\\quad w_6=940,\\quad w_7=1110,\\quad w_8=1260.\n$$\nThe time per cell-update is $\\tau=1.0\\times 10^{-6}\\ \\mathrm{s}$. During the timestep, each processor performs two nearest-neighbor ghost exchanges. For processor $i$, each ghost exchange message contains $m_i$ thousand scalars, with\n$$\nm_1=80,\\quad m_2=75,\\quad m_3=78,\\quad m_4=70,\\quad m_5=76,\\quad m_6=60,\\quad m_7=74,\\quad m_8=82.\n$$\nAssume a point-to-point message time model $t_{\\mathrm{msg}}=\\alpha+\\beta n$ where $n$ is the number of scalars in the message, $\\alpha=1.2\\times 10^{-4}\\ \\mathrm{s}$, and $\\beta=2.5\\times 10^{-8}\\ \\mathrm{s}$ per scalar. The timestep ends with a binary-tree global reduction to compute the Courant–Friedrichs–Lewy (CFL) number, incurring a barrier time of $3$ stages with per-stage cost $\\gamma=1.6\\times 10^{-4}\\ \\mathrm{s}$. Assume no overlap between computation and communication, no hidden latencies, and negligible input/output and host overheads.\n\nUsing standard definitions from parallel performance analysis, do the following:\n- Compute the mean workload $\\bar{w}$, the standard deviation $\\sigma$, the coefficient of variation $CV=\\sigma/\\bar{w}$, and the imbalance factor $I=w_{\\max}/\\bar{w}$.\n- Predict the serial time $T_1$ and the parallel time $T_P$ for the timestep under the assumptions above.\n- Compute the efficiency $E$ and the speedup $S$.\n\nRound the speedup $S$ to four significant figures. Express your final answer as the speedup, with no units.",
            "solution": "The problem statement is valid as it is scientifically grounded in the principles of parallel computing performance analysis, is well-posed with sufficient and consistent data, and is expressed in objective, formal language. We may therefore proceed with the solution.\n\nThe problem asks for several performance metrics for a parallel hydrodynamics solver executed on $P=8$ processors. We will compute these metrics systematically.\n\nFirst, we analyze the workload distribution. The per-processor local update counts (workloads) are given as:\n$w_1=1200$, $w_2=1150$, $w_3=1230$, $w_4=1070$, $w_5=1180$, $w_6=940$, $w_7=1110$, $w_8=1260$.\n\nThe total workload, $W$, is the sum of the individual processor workloads:\n$$\nW = \\sum_{i=1}^{P} w_i = 1200 + 1150 + 1230 + 1070 + 1180 + 940 + 1110 + 1260 = 9140 \\text{ cell-updates}\n$$\nThe mean workload, $\\bar{w}$, is:\n$$\n\\bar{w} = \\frac{W}{P} = \\frac{9140}{8} = 1142.5 \\text{ cell-updates}\n$$\nThe population standard deviation of the workload, $\\sigma$, is calculated as:\n$$\n\\sigma = \\sqrt{\\frac{1}{P} \\sum_{i=1}^{P} (w_i - \\bar{w})^2} = \\sqrt{\\frac{1}{8} \\left[ (57.5)^2 + (7.5)^2 + (87.5)^2 + (-72.5)^2 + (37.5)^2 + (-202.5)^2 + (-32.5)^2 + (117.5)^2 \\right]}\n$$\n$$\n\\sigma = \\sqrt{\\frac{73550}{8}} = \\sqrt{9193.75} \\approx 95.884\n$$\nThe coefficient of variation, $CV$, which measures the relative load imbalance, is:\n$$\nCV = \\frac{\\sigma}{\\bar{w}} = \\frac{95.884}{1142.5} \\approx 0.08392\n$$\nThe maximum workload is $w_{\\max} = \\max_i(w_i) = 1260$. The imbalance factor, $I$, is:\n$$\nI = \\frac{w_{\\max}}{\\bar{w}} = \\frac{1260}{1142.5} \\approx 1.1028\n$$\nNext, we compute the serial and parallel execution times. The time per cell-update is $\\tau = 1.0 \\times 10^{-6}\\ \\mathrm{s}$.\n\nThe serial time, $T_1$, is the time required to complete the total workload on a single processor, with no communication overhead:\n$$\nT_1 = W \\times \\tau = 9140 \\times (1.0 \\times 10^{-6}\\ \\mathrm{s}) = 9.14 \\times 10^{-3}\\ \\mathrm{s}\n$$\nThe parallel time, $T_P$, is determined by the slowest processor in a bulk synchronous model (no overlap between computation and communication). It is the sum of the maximum computation time, $T_{\\text{comp}}$, and the total communication time, $T_{\\text{comm}}$.\n$$\nT_P = T_{\\text{comp}} + T_{\\text{comm}}\n$$\nThe computation time is limited by the processor with the maximum workload, $w_{\\max}$:\n$$\nT_{\\text{comp}} = w_{\\max} \\times \\tau = 1260 \\times (1.0 \\times 10^{-6}\\ \\mathrm{s}) = 1.26 \\times 10^{-3}\\ \\mathrm{s}\n$$\nThe communication time consists of two components: ghost cell exchange time, $T_{\\text{ghost}}$, and global reduction time, $T_{\\text{redux}}$.\n$$\nT_{\\text{comm}} = T_{\\text{ghost}} + T_{\\text{redux}}\n$$\nThe time for a point-to-point message of $n$ scalars is $t_{\\mathrm{msg}} = \\alpha + \\beta n$, with latency $\\alpha=1.2\\times 10^{-4}\\ \\mathrm{s}$ and inverse bandwidth $\\beta=2.5\\times 10^{-8}\\ \\mathrm{s}/\\text{scalar}$. Each processor performs two ghost exchanges. The time for this phase is determined by the processor with the largest message size. The message sizes in thousands of scalars are given by $m_i$. The maximum size is $m_{\\max} = 82$, corresponding to $n_{\\max} = 82000$ scalars. The cost of the ghost exchange phase is:\n$$\nT_{\\text{ghost}} = 2 \\times (\\alpha + \\beta n_{\\max}) = 2 \\times \\left(1.2 \\times 10^{-4}\\ \\mathrm{s} + (2.5 \\times 10^{-8}\\ \\mathrm{s}/\\text{scalar}) \\times 82000\\ \\text{scalars}\\right)\n$$\n$$\nT_{\\text{ghost}} = 2 \\times (1.2 \\times 10^{-4}\\ \\mathrm{s} + 2.05 \\times 10^{-3}\\ \\mathrm{s}) = 2 \\times (2.17 \\times 10^{-3}\\ \\mathrm{s}) = 4.34 \\times 10^{-3}\\ \\mathrm{s}\n$$\nThe global reduction incurs a barrier time of $3$ stages, with each stage costing $\\gamma = 1.6 \\times 10^{-4}\\ \\mathrm{s}$.\n$$\nT_{\\text{redux}} = 3 \\times \\gamma = 3 \\times (1.6 \\times 10^{-4}\\ \\mathrm{s}) = 4.8 \\times 10^{-4}\\ \\mathrm{s} = 0.48 \\times 10^{-3}\\ \\mathrm{s}\n$$\nThe total communication time is:\n$$\nT_{\\text{comm}} = 4.34 \\times 10^{-3}\\ \\mathrm{s} + 0.48 \\times 10^{-3}\\ \\mathrm{s} = 4.82 \\times 10^{-3}\\ \\mathrm{s}\n$$\nThe total parallel time for the timestep is:\n$$\nT_P = T_{\\text{comp}} + T_{\\text{comm}} = 1.26 \\times 10^{-3}\\ \\mathrm{s} + 4.82 \\times 10^{-3}\\ \\mathrm{s} = 6.08 \\times 10^{-3}\\ \\mathrm{s}\n$$\nFinally, we compute the speedup, $S$, and efficiency, $E$.\nSpeedup is the ratio of serial to parallel time:\n$$\nS = \\frac{T_1}{T_P} = \\frac{9.14 \\times 10^{-3}\\ \\mathrm{s}}{6.08 \\times 10^{-3}\\ \\mathrm{s}} = \\frac{9.14}{6.08} \\approx 1.503289...\n$$\nEfficiency is the speedup per processor:\n$$\nE = \\frac{S}{P} = \\frac{1.503289...}{8} \\approx 0.1879\n$$\nThe problem requests the speedup $S$ rounded to four significant figures.\n$$\nS \\approx 1.503\n$$",
            "answer": "$$\n\\boxed{1.503}\n$$"
        },
        {
            "introduction": "Building on the ability to measure performance, this practice elevates the analysis to a theoretical level by integrating the concept of load imbalance directly into the foundational laws of parallel computing. You will derive modified versions of Amdahl's and Gustafson's laws that incorporate both a load imbalance factor and the complex, level-dependent serial fractions typical of Adaptive Mesh Refinement (AMR) codes. This develops the crucial skill of creating analytical models to predict performance and determine the maximum tolerable imbalance for achieving a desired speedup.",
            "id": "3516502",
            "problem": "Consider an Adaptive Mesh Refinement (AMR) astrophysical magnetohydrodynamics (MHD) solver that advances a hierarchy of refinement levels labeled by an integer index $\\ell \\in \\{0,1,\\dots,L-1\\}$. For a single-process baseline run on a fixed problem size (strong scaling baseline), the total wall-clock time contribution from level $\\ell$ is $W_{\\ell} > 0$. Due to level-synchronous operations such as flux correction, timestep control, and restriction/prolongation, a level-dependent fraction $s(\\ell) \\in [0,1]$ of $W_{\\ell}$ is intrinsically serial, while the remaining fraction $1 - s(\\ell)$ is parallelizable over computational blocks. Suppose the code is executed on $p \\geq 1$ identical processes. Define a load imbalance factor $\\eta \\geq 0$ by the following operational statement: relative to the ideally balanced distribution of parallelizable work, the parallel phase runtime is stretched by a multiplicative factor $1+\\eta$ because some processes are assigned more work than others. Assume that communication overheads are included in $\\eta$ and that the serial phase is not overlapped with the parallel phase.\n\nStarting from the definition of speedup $S(p)$ as the ratio of the single-process runtime to the $p$-process runtime for the configuration under consideration, derive analytic expressions for the expected speedup under fixed-size (strong scaling, often associated with Amdahl’s perspective) and scaled-size (weak scaling, often associated with Gustafson’s perspective) assumptions, explicitly incorporating the level-dependent serial fractions $s(\\ell)$ and the load imbalance factor $\\eta$.\n\nThen, given a target speedup $S^{\\star} > 0$, derive closed-form expressions for the maximum allowable imbalance $\\eta$ that still achieves exactly $S(p) = S^{\\star}$ under each scaling assumption, expressed in terms of $p$, $\\{W_{\\ell}\\}$, and $\\{s(\\ell)\\}$.\n\nYour final answers must be closed-form analytic expressions. No numerical evaluation is required, and no units are to be reported. If any approximation were needed, it would be specified; since none is, present exact expressions.",
            "solution": "The problem requires the derivation of speedup expressions under strong and weak scaling for a parallel AMR code, and subsequently, the derivation of the maximum allowable load imbalance factor $\\eta$ for a given target speedup. The model incorporates level-dependent serial fractions and a load imbalance factor.\n\nFirst, we formalize the components of runtime based on the problem statement. The code consists of $L$ refinement levels, indexed by $\\ell \\in \\{0, 1, \\dots, L-1\\}$. For a reference single-process execution, the wall-clock time for level $\\ell$ is $W_{\\ell}$. The total single-process runtime, $T(1)$, is the sum of the work over all levels.\n$$T(1) = \\sum_{\\ell=0}^{L-1} W_{\\ell}$$\nFor each level $\\ell$, a fraction $s(\\ell)$ of the work $W_{\\ell}$ is serial, and the remaining fraction $1-s(\\ell)$ is parallelizable. The total serial work for the reference problem, $W_{\\text{serial}}$, and the total parallelizable work, $W_{\\text{parallel}}$, are:\n$$W_{\\text{serial}} = \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell}$$\n$$W_{\\text{parallel}} = \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}$$\nNote that $T(1) = W_{\\text{serial}} + W_{\\text{parallel}}$.\n\nWhen the code runs on $p$ processes, the serial part of the work is not sped up. Its runtime remains $W_{\\text{serial}}$. The parallelizable work $W_{\\text{parallel}}$ is distributed among the $p$ processes. Ideally, this would reduce its runtime by a factor of $p$. However, the load imbalance factor $\\eta$ stretches the parallel phase runtime by a factor of $1+\\eta$. The runtime for the parallel phase on $p$ processors, $T_{\\text{parallel}}(p)$, is thus:\n$$T_{\\text{parallel}}(p) = \\frac{W_{\\text{parallel}}}{p} (1+\\eta)$$\nAssuming the serial and parallel phases are not overlapped, the total runtime on $p$ processes, $T(p)$, is the sum of the runtimes of the two phases:\n$$T(p) = W_{\\text{serial}} + T_{\\text{parallel}}(p) = \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + \\frac{1+\\eta}{p} \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}$$\nThis general expression for $T(p)$ serves as the basis for deriving the speedup under both strong and weak scaling assumptions.\n\n**1. Strong Scaling (Amdahl's Perspective)**\n\nUnder the strong scaling assumption, the total problem size is fixed, meaning the work components $W_{\\ell}$ are independent of the number of processes $p$. The single-process runtime is $T_{\\text{strong}}(1) = T(1) = \\sum W_{\\ell}$. The $p$-process runtime $T_{\\text{strong}}(p)$ is given by the general expression for $T(p)$ derived above.\nThe speedup $S_{\\text{strong}}(p)$ is the ratio of the single-process runtime to the $p$-process runtime:\n$$S_{\\text{strong}}(p) = \\frac{T_{\\text{strong}}(1)}{T_{\\text{strong}}(p)} = \\frac{\\sum_{\\ell=0}^{L-1} W_{\\ell}}{\\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + \\frac{1+\\eta}{p} \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}}$$\nThis is the first required expression.\n\n**2. Weak Scaling (Gustafson's Perspective)**\n\nUnder the weak scaling assumption, the problem size per process is held constant. This implies that the parallelizable portion of the work scales linearly with the number of processes $p$, while the serial portion remains fixed. Let $\\{W_{\\ell}\\}$ represent the work components for a single-process problem. For a scaled problem run on $p$ processes, the total serial work remains $W_{\\text{serial}}$, while the total parallelizable work becomes $p \\times W_{\\text{parallel}}$.\nThe time to execute this scaled problem on $p$ processes, $T_{\\text{weak}}(p)$, is:\n$$T_{\\text{weak}}(p) = W_{\\text{serial}} + \\frac{p \\times W_{\\text{parallel}}}{p}(1+\\eta) = \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + (1+\\eta)\\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}$$\nThe definition of weak scaling speedup is the ratio of the time to run the scaled problem on a single serial processor to the time to run it on $p$ parallel processors. The time to run the scaled problem on one processor, $T_{\\text{scaled}}(1)$, would be the sum of its serial and parallel work:\n$$T_{\\text{scaled}}(1) = W_{\\text{serial}} + p \\times W_{\\text{parallel}} = \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + p \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}$$\nThus, the weak scaling speedup $S_{\\text{weak}}(p)$ is:\n$$S_{\\text{weak}}(p) = \\frac{T_{\\text{scaled}}(1)}{T_{\\text{weak}}(p)} = \\frac{\\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + p \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}}{\\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + (1+\\eta)\\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}}$$\nThis is the second required expression.\n\n**3. Maximum Allowable Imbalance for Strong Scaling**\n\nTo find the maximum allowable imbalance $\\eta$ that achieves a target speedup $S^{\\star}$, we set $S_{\\text{strong}}(p) = S^{\\star}$ and solve for $\\eta$.\n$$S^{\\star} = \\frac{\\sum_{\\ell=0}^{L-1} W_{\\ell}}{\\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + \\frac{1+\\eta}{p} \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}}$$\nRearranging the equation to isolate $\\eta$:\n$$S^{\\star} \\left( \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + \\frac{1+\\eta}{p} \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell} \\right) = \\sum_{\\ell=0}^{L-1} W_{\\ell}$$\n$$\\frac{1+\\eta}{p} \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell} = \\frac{1}{S^{\\star}}\\sum_{\\ell=0}^{L-1} W_{\\ell} - \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell}$$\n$$1+\\eta = \\frac{p}{\\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}} \\left( \\frac{\\sum_{\\ell=0}^{L-1} W_{\\ell}}{S^{\\star}} - \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} \\right)$$\n$$\\eta = \\frac{p}{\\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}} \\left( \\frac{\\sum_{\\ell=0}^{L-1} W_{\\ell}}{S^{\\star}} - \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} \\right) - 1$$\nThis closed-form expression gives the required maximum $\\eta$ for the strong scaling case.\n\n**4. Maximum Allowable Imbalance for Weak Scaling**\n\nSimilarly, for the weak scaling case, we set $S_{\\text{weak}}(p) = S^{\\star}$ and solve for $\\eta$.\n$$S^{\\star} = \\frac{\\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + p \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}}{\\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + (1+\\eta)\\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}}$$\nRearranging to solve for $\\eta$:\n$$S^{\\star} \\left( \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + (1+\\eta)\\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell} \\right) = \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + p \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}$$\n$$(1+\\eta)\\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell} = \\frac{1}{S^{\\star}} \\left( \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + p \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell} \\right) - \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell}$$\n$$1+\\eta = \\frac{1}{\\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}} \\left[ \\frac{1}{S^{\\star}} \\left( \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + p \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell} \\right) - \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} \\right]$$\n$$\\eta = \\frac{1}{\\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}} \\left[ \\frac{1}{S^{\\star}} \\left( \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + p \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell} \\right) - \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} \\right] - 1$$\nThis is the final required expression for the maximum allowed $\\eta$ under weak scaling.\nThe four derived expressions constitute the complete solution to the problem.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sum_{\\ell=0}^{L-1} W_{\\ell}}{\\sum_{\\ell=0}^{L-1} s(\\ell) W_{\\ell} + \\frac{1+\\eta}{p} \\sum_{\\ell=0}^{L-1} (1-s(\\ell)) W_{\\ell}} & \\frac{\\sum_{\\ell=0}^{L-1} s(\\ell) W_{\\ell} + p \\sum_{\\ell=0}^{L-1} (1-s(\\ell)) W_{\\ell}}{\\sum_{\\ell=0}^{L-1} s(\\ell) W_{\\ell} + (1+\\eta) \\sum_{\\ell=0}^{L-1} (1-s(\\ell)) W_{\\ell}} & \\frac{p}{\\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}} \\left( \\frac{\\sum_{\\ell=0}^{L-1} W_{\\ell}}{S^{\\star}} - \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} \\right) - 1 & \\frac{1}{\\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell}} \\left[ \\frac{1}{S^{\\star}} \\left( \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} + p \\sum_{\\ell=0}^{L-1} (1-s(\\ell))W_{\\ell} \\right) - \\sum_{\\ell=0}^{L-1} s(\\ell)W_{\\ell} \\right] - 1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Many astrophysical simulations are dynamic, with workloads that shift as structures like galaxies and clusters form. This creates a classic optimization problem: rebalancing the load costs time, but not rebalancing leads to growing inefficiency. This final practice explores this trade-off by asking you to model a system where imbalance grows linearly and derive the optimal frequency at which to repartition the simulation domain. This exercise teaches you to formulate and solve a performance model to guide algorithmic choices, a vital skill for developing efficient, dynamic parallel codes.",
            "id": "3516560",
            "problem": "Consider a cosmological $N$-body simulation advanced in time on a distributed-memory cluster using the Message Passing Interface (MPI). The simulation employs a dynamic domain decomposition based on a space-filling curve to balance computational load across $P$ processes. Gravitational clustering drives inhomogeneity in particle distribution, which, in the absence of immediate repartitioning, produces a growing load imbalance. Let the fractional slowdown due to load imbalance be represented by a dimensionless function $\\eta(t)$, defined such that the instantaneous effective runtime rate is multiplied by $(1+\\eta(t))$, with $\\eta(t)\\ge 0$ and $\\eta(0)=0$. Empirically, over windows shorter than the dynamical time, assume $\\eta(t)$ grows approximately linearly between repartition events at a rate $\\dot{\\eta}>0$, and that a repartition event resets $\\eta(t)$ to approximately $0$.\n\nEach repartition event triggers particle and metadata migration with wall-clock cost modeled as $C_{m}=\\alpha\\, m+\\beta\\, b$, where $m$ is the number of messages and $b$ is the total migrated bytes during that event, $\\alpha$ is the effective per-message latency time, and $\\beta$ is the effective per-byte transmission time. Over a given time window of length $H>0$, suppose we choose to repartition periodically with frequency $f>0$ (period $\\tau=1/f$), and that $m$ and $b$ are the mean values per repartition over the window and can be treated as independent of $f$ under the chosen partitioner’s incremental strategy.\n\nUsing first principles of performance modeling, derive an analytic expression for the optimal repartition frequency $f^{\\star}$ that minimizes the total runtime $T_{\\text{total}}$ over the window of length $H$, where $T_{\\text{total}}$ is the sum of the baseline runtime without imbalance or repartition, the cumulative imbalance-induced overhead integrated over time, and the cumulative migration cost due to repartition events. Assume repartition events are instantaneous except for the additive migration cost $C_{m}$ and that the imbalance grows linearly between events. Express your final answer as a closed-form analytic expression in terms of $\\alpha$, $\\beta$, $m$, $b$, and $\\dot{\\eta}$. No numerical evaluation is required. If you find multiple stationary points, identify the one that minimizes $T_{\\text{total}}$.",
            "solution": "The problem asks for the optimal repartition frequency, $f^{\\star}$, that minimizes the total runtime, $T_{\\text{total}}$, over a simulation window of duration $H$. The total runtime is the sum of the baseline runtime, the cumulative overhead from load imbalance, and the cumulative cost of migration from repartitioning.\n\nLet's model each component of the total time as a function of the repartition frequency, $f$.\n\n1.  **Cumulative Migration Cost ($T_{\\text{mig}}$)**\n    The simulation is repartitioned periodically with a frequency $f$. The period is $\\tau = 1/f$. Over a time window of length $H$, the total number of repartition events is $N_{\\text{rep}} = H / \\tau = Hf$.\n    The cost of a single repartition event is given as $C_{m} = \\alpha m + \\beta b$.\n    Therefore, the total migration cost over the window $H$ is:\n    $$ T_{\\text{mig}}(f) = N_{\\text{rep}} \\times C_{m} = Hf(\\alpha m + \\beta b) $$\n\n2.  **Computational Time (Baseline + Imbalance Overhead)**\n    The instantaneous runtime is increased by a factor of $(1+\\eta(t))$, where $\\eta(t)$ is the load imbalance. Between repartitions, $\\eta(t)$ grows linearly from 0 at a rate $\\dot{\\eta}$.\n    Let's consider the computational work done in one period of length $\\tau$. The wall-clock time for this period is the integral of the instantaneous runtime rate. We can set the baseline rate to 1.\n    $$ T_{\\text{period}} = \\int_{0}^{\\tau} (1 + \\eta(t)) dt = \\int_{0}^{\\tau} (1 + \\dot{\\eta}t) dt = \\left[ t + \\frac{1}{2}\\dot{\\eta}t^2 \\right]_{0}^{\\tau} = \\tau + \\frac{1}{2}\\dot{\\eta}\\tau^2 $$\n    The total computational time, $T_{\\text{comp}}$, is the time per period multiplied by the number of periods, $N_{\\text{rep}} = H/\\tau$:\n    $$ T_{\\text{comp}} = \\frac{H}{\\tau} \\left( \\tau + \\frac{1}{2}\\dot{\\eta}\\tau^2 \\right) = H + \\frac{1}{2}H\\dot{\\eta}\\tau $$\n    Substituting $\\tau = 1/f$, this becomes:\n    $$ T_{\\text{comp}}(f) = H + \\frac{H\\dot{\\eta}}{2f} $$\n    Here, $H$ is the baseline runtime, and the second term is the total overhead from imbalance.\n\n3.  **Total Runtime and Optimization**\n    The total runtime $T_{\\text{total}}(f)$ is the sum of the computational time and the migration cost:\n    $$ T_{\\text{total}}(f) = T_{\\text{comp}}(f) + T_{\\text{mig}}(f) = \\left(H + \\frac{H\\dot{\\eta}}{2f}\\right) + Hf(\\alpha m + \\beta b) $$\n    To find the optimal frequency $f^{\\star}$ that minimizes $T_{\\text{total}}$, we take the derivative with respect to $f$ and set it to zero:\n    $$ \\frac{dT_{\\text{total}}}{df} = \\frac{d}{df} \\left( H + \\frac{H\\dot{\\eta}}{2}f^{-1} + H(\\alpha m + \\beta b)f \\right) = -\\frac{H\\dot{\\eta}}{2f^2} + H(\\alpha m + \\beta b) $$\n    Setting the derivative to zero:\n    $$ H(\\alpha m + \\beta b) = \\frac{H\\dot{\\eta}}{2(f^{\\star})^2} $$\n    Since $H > 0$, we can cancel it from both sides:\n    $$ \\alpha m + \\beta b = \\frac{\\dot{\\eta}}{2(f^{\\star})^2} $$\n    Solving for $(f^{\\star})^2$:\n    $$ (f^{\\star})^2 = \\frac{\\dot{\\eta}}{2(\\alpha m + \\beta b)} $$\n    Since frequency must be positive, we take the positive square root:\n    $$ f^{\\star} = \\sqrt{\\frac{\\dot{\\eta}}{2(\\alpha m + \\beta b)}} $$\n    To confirm this is a minimum, we check the second derivative:\n    $$ \\frac{d^2T_{\\text{total}}}{df^2} = \\frac{d}{df} \\left( -\\frac{H\\dot{\\eta}}{2}f^{-2} + H(\\alpha m + \\beta b) \\right) = (-2)\\left(-\\frac{H\\dot{\\eta}}{2}\\right)f^{-3} = \\frac{H\\dot{\\eta}}{f^3} $$\n    Since $H > 0$, $\\dot{\\eta} > 0$, and $f > 0$, the second derivative is always positive. This confirms that $f^{\\star}$ is indeed the frequency that minimizes the total runtime.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{\\dot{\\eta}}{2(\\alpha m + \\beta b)}}}\n$$"
        }
    ]
}