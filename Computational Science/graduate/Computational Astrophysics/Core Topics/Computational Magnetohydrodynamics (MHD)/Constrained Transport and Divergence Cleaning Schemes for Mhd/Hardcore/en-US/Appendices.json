{
    "hands_on_practices": [
        {
            "introduction": "Before applying any correction, we must first learn how to accurately detect and quantify violations of the $\\nabla \\cdot \\mathbf{B} = 0$ constraint. This exercise is foundational, guiding you to develop a computational diagnostic to measure the severity of divergence errors on a discrete grid. By implementing finite-difference operators and a normalized error metric, you will build essential tools for monitoring simulation health and triggering corrective actions like divergence cleaning or adaptive mesh refinement .",
            "id": "3506882",
            "problem": "You are asked to construct and test a computational diagnostic for spurious divergence-induced accelerations in Magnetohydrodynamics (MHD). The diagnostic must be based on first principles and implemented on a uniform Cartesian grid. The goal is to define a proxy acceleration that quantifies numerical violations of Gauss’s law for magnetism and to design threshold criteria that trigger local divergence cleaning or mesh refinement.\n\nStarting from Gauss’s law for magnetism, which states that the magnetic field must satisfy $\\nabla \\cdot \\mathbf{B} = 0$ in the continuum, you will derive and implement discrete diagnostics on a uniform two-dimensional grid. Use only the following fundamental base and well-tested facts as starting points:\n\n- Maxwell’s equation (Gauss’s law for magnetism): $\\nabla \\cdot \\mathbf{B} = 0$.\n- Uniform Cartesian grid with spacings $\\Delta x$ and $\\Delta y$.\n- Smooth fields admit Taylor expansions; central finite differences can approximate first derivatives to second-order accuracy in the interior; one-sided differences provide first-order boundary closures.\n- The magnetic field magnitude is $|\\mathbf{B}| = \\sqrt{B_x^2 + B_y^2}$.\n- A spurious monopole-like force appears in the discrete momentum equation when $\\nabla \\cdot \\mathbf{B} \\neq 0$; a commonly used proxy for the resulting acceleration is proportional to $-(\\nabla \\cdot \\mathbf{B})\\mathbf{B}$ up to a constant.\n\nYour tasks are:\n\n- Derive and implement a second-order accurate finite-difference approximation to $\\nabla \\cdot \\mathbf{B}$ on a uniform grid in the interior using central stencils, and a consistent one-sided approximation on the boundaries.\n- From this discrete divergence, define and implement a dimensionless normalized divergence indicator $\\varepsilon$ that scales with resolution as $\\varepsilon \\sim h |\\nabla \\cdot \\mathbf{B}| / (|\\mathbf{B}| + B_{\\text{floor}})$, where $h$ is a characteristic cell size and $B_{\\text{floor}}$ is a small regularization constant to prevent division by zero. Choose $h = \\min(\\Delta x, \\Delta y)$ and consider $B_{\\text{floor}}$ as a given parameter.\n- Define and compute the proxy spurious acceleration vector $\\mathbf{f}_{\\mathrm{div}} \\propto -(\\nabla \\cdot \\mathbf{B}) \\mathbf{B}$.\n- Design boolean triggers for cleaning and refinement from local diagnostics:\n  - A cleaning trigger $T_{\\mathrm{clean}}$ is set to true if $\\varepsilon > \\theta_{\\mathrm{clean}}$.\n  - A refinement trigger $T_{\\mathrm{ref}}$ is set to true if either $\\varepsilon > \\theta_{\\mathrm{ref}}$ or $|\\nabla \\cdot \\mathbf{B}| > \\phi_{\\mathrm{abs}}$.\n  Use given thresholds $\\theta_{\\mathrm{clean}}$, $\\theta_{\\mathrm{ref}}$, and $\\phi_{\\mathrm{abs}}$.\n\nYou must implement a program that, for each test case below, computes:\n- The maximum value of $\\varepsilon$ over the grid, denoted $\\varepsilon_{\\max}$ (unitless).\n- The total number of cells where $T_{\\mathrm{clean}}$ is true (integer count).\n- The total number of cells where $T_{\\mathrm{ref}}$ is true (integer count).\n\nReport $\\varepsilon_{\\max}$ rounded to six decimal places. All quantities in this problem are to be treated as unitless (dimensionless). Angles, where relevant, must be interpreted in radians.\n\nDiscrete operators and boundary conditions:\n- Use a uniform two-dimensional grid of size $N_x \\times N_y$ with spacings $\\Delta x$ and $\\Delta y$.\n- Use second-order central finite differences to approximate $\\partial B_x/\\partial x$ and $\\partial B_y/\\partial y$ at interior cells.\n- Use consistent first-order one-sided finite differences at the domain boundaries.\n\nThresholds and regularization:\n- Use $\\theta_{\\mathrm{clean}} = 0.2$, $\\theta_{\\mathrm{ref}} = 0.6$, $\\phi_{\\mathrm{abs}} = 0.8$, and $B_{\\mathrm{floor}} = 10^{-6}$.\n\nTest suite:\nProvide results for the following four test cases. In all cases, the grid is cell-centered and $\\mathbf{B}$ is given at the cell centers.\n\n- Test case A (smooth, nearly divergence-free by construction): $N_x = 33$, $N_y = 33$, domain $x \\in [0, 2\\pi]$, $y \\in [0, 2\\pi]$, so that $\\Delta x = 2\\pi/(N_x - 1)$ and $\\Delta y = 2\\pi/(N_y - 1)$. Define a stream function $\\psi(x,y) = \\sin(k_x x) \\sin(k_y y)$ with $k_x = 2$ and $k_y = 3$. Construct $\\mathbf{B}$ from $\\psi$ via $B_x = \\partial \\psi/\\partial y$ and $B_y = -\\partial \\psi/\\partial x$ evaluated analytically at the cell centers.\n- Test case B (monopole-like interface): $N_x = 9$, $N_y = 9$, $\\Delta x = 1$, $\\Delta y = 1$. Define $B_x(i,j) = 1$ for all indices with $i \\ge \\lceil N_x/2 \\rceil$ and $B_x(i,j) = 0$ otherwise; set $B_y(i,j) = 0$ for all cells. Here $i$ and $j$ are zero-based indices in code, but your derivation must be independent of indexing conventions.\n- Test case C (near-vacuum noise): $N_x = 7$, $N_y = 7$, $\\Delta x = 1$, $\\Delta y = 1$. Define $B_x$ and $B_y$ as independent small-amplitude fields with deterministic pseudorandom values uniformly distributed in $[-10^{-12}, 10^{-12}]$, produced by a fixed pseudorandom seed. Use $B_{\\mathrm{floor}} = 10^{-6}$ as above.\n- Test case D (strong uniform field with localized perturbation): $N_x = 17$, $N_y = 17$, $\\Delta x = 1$, $\\Delta y = 1$. Define $B_x(i,j) = 5$ for all cells. Define $B_y(i,j) = 0$ for all cells initially, then add a step in $y$: for all cells with $j \\ge \\lfloor N_y/2 \\rfloor$, set $B_y(i,j) \\leftarrow B_y(i,j) + 1$, leaving other cells unchanged.\n\nFor each test case, compute and return a list containing three items $[\\varepsilon_{\\max}, n_{\\mathrm{clean}}, n_{\\mathrm{ref}}]$, where $n_{\\mathrm{clean}}$ is the number of cells with $T_{\\mathrm{clean}}$ true, and $n_{\\mathrm{ref}}$ is the number of cells with $T_{\\mathrm{ref}}$ true. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list. For example, you should print a line of the form\n\"[[eA,ncA,nrA],[eB,ncB,nrB],[eC,ncC,nrC],[eD,ncD,nrD]]\"\nwith no spaces, where $eX$ are floats rounded to six decimal places and $ncX$, $nrX$ are integers.",
            "solution": "The objective is to construct and test a computational diagnostic for spurious magnetic divergence in numerical magnetohydrodynamics (MHD). This involves deriving and implementing discrete operators on a uniform Cartesian grid to quantify violations of Gauss's law for magnetism, $\\nabla \\cdot \\mathbf{B} = 0$, and to establish criteria for corrective actions like divergence cleaning or mesh refinement.\n\n### Step 1: Discretization of the Divergence Operator\n\nThe divergence of a magnetic field $\\mathbf{B} = (B_x, B_y)$ in two dimensions is given by the continuum expression:\n$$ \\nabla \\cdot \\mathbf{B} = \\frac{\\partial B_x}{\\partial x} + \\frac{\\partial B_y}{\\partial y} $$\nWe are tasked with discretizing this operator on a uniform cell-centered Cartesian grid defined by points $(x_i, y_j) = (i\\Delta x, j\\Delta y)$ for integer indices $i \\in [0, N_x-1]$ and $j \\in [0, N_y-1]$. The field components $B_x(i, j)$ and $B_y(i, j)$ are known at these cell centers.\n\nFor interior grid cells, where $0 < i < N_x-1$ and $0 < j < N_y-1$, we employ a second-order accurate central finite difference scheme. The partial derivatives are approximated as:\n$$ \\left(\\frac{\\partial B_x}{\\partial x}\\right)_{i,j} \\approx \\frac{B_x(i+1, j) - B_x(i-1, j)}{2 \\Delta x} $$\n$$ \\left(\\frac{\\partial B_y}{\\partial y}\\right)_{i,j} \\approx \\frac{B_y(i, j+1) - B_y(i, j-1)}{2 \\Delta y} $$\nThe discrete divergence at an interior cell $(i,j)$ is therefore:\n$$ (\\nabla \\cdot \\mathbf{B})_{i,j} \\approx \\frac{B_x(i+1, j) - B_x(i-1, j)}{2 \\Delta x} + \\frac{B_y(i, j+1) - B_y(i, j-1)}{2 \\Delta y} $$\n\nFor cells on the domain boundary, a first-order accurate one-sided difference is required for the derivative normal to that boundary, while the tangential derivative can still use a central stencil. For example, at a cell on the left boundary ($i=0$) but not at a corner ($0 < j < N_y-1$), the approximation becomes:\n$$ (\\nabla \\cdot \\mathbf{B})_{0,j} \\approx \\underbrace{\\frac{B_x(1, j) - B_x(0, j)}{\\Delta x}}_{\\text{1st-order forward}} + \\underbrace{\\frac{B_y(0, j+1) - B_y(0, j-1)}{2 \\Delta y}}_{\\text{2nd-order central}} $$\nAt a corner, for example the bottom-left corner ($i=0, j=0$), both derivatives must be approximated with first-order forward differences:\n$$ (\\nabla \\cdot \\mathbf{B})_{0,0} \\approx \\frac{B_x(1, 0) - B_x(0, 0)}{\\Delta x} + \\frac{B_y(0, 1) - B_y(0, 0)}{\\Delta y} $$\nAnalogous backward-difference stencils are used for the top and right boundaries. This mixed-accuracy scheme is a standard approach for discretizing derivatives on a bounded grid.\n\n### Step 2: Diagnostic Indicators and Triggers\n\nFrom the computed discrete divergence, $(\\nabla \\cdot \\mathbf{B})_{i,j}$, we define several diagnostic quantities.\n\nThe primary diagnostic is the dimensionless normalized divergence indicator, $\\varepsilon$, defined as:\n$$ \\varepsilon_{i,j} = \\frac{h |(\\nabla \\cdot \\mathbf{B})_{i,j}|}{|\\mathbf{B}|_{i,j} + B_{\\text{floor}}} $$\nHere, $h = \\min(\\Delta x, \\Delta y)$ is a characteristic cell size, which makes $\\varepsilon$ a measure of the divergence error relative to the grid scale. $|\\mathbf{B}|_{i,j} = \\sqrt{B_x(i,j)^2 + B_y(i,j)^2}$ is the local magnetic field magnitude. The normalization by $|\\mathbf{B}|$ makes $\\varepsilon$ a relative error metric, which is often more meaningful than the absolute divergence. The constant $B_{\\text{floor}}$ is a small positive number ($10^{-6}$ in this problem) that regularizes the expression, preventing division by zero or spuriously large values of $\\varepsilon$ in regions where the magnetic field is vanishingly weak (near-vacuum).\n\nBased on the values of $\\varepsilon$ and $|\\nabla \\cdot \\mathbf{B}|$, we define two boolean triggers for numerical control:\n\n1.  **Cleaning Trigger ($T_{\\mathrm{clean}}$):** This trigger indicates the need for a local divergence cleaning operation (e.g., using a hyperbolic or elliptic cleaning scheme). It is activated when the normalized divergence exceeds a moderate threshold:\n    $$ T_{\\mathrm{clean}} \\text{ is true if } \\varepsilon > \\theta_{\\mathrm{clean}} $$\n    For this problem, $\\theta_{\\mathrm{clean}} = 0.2$.\n\n2.  **Refinement Trigger ($T_{\\mathrm{ref}}$):** This trigger signals a more severe problem, suggesting that the local grid resolution may be insufficient to resolve the field structure, necessitating adaptive mesh refinement (AMR). It is activated by either a very large normalized divergence or a large absolute divergence:\n    $$ T_{\\mathrm{ref}} \\text{ is true if } (\\varepsilon > \\theta_{\\mathrm{ref}}) \\lor (|\\nabla \\cdot \\mathbf{B}| > \\phi_{\\mathrm{abs}}) $$\n    For this problem, $\\theta_{\\mathrm{ref}} = 0.6$ and $\\phi_{\\mathrm{abs}} = 0.8$.\n\n### Step 3: Computational Procedure\n\nThe overall algorithm proceeds as follows for each test case:\n1.  **Grid and Field Setup:** An $N_x \\times N_y$ grid is defined with spacings $\\Delta x$ and $\\Delta y$. The magnetic field components $B_x$ and $B_y$ are populated on this grid according to the specific test case definition.\n2.  **Divergence Calculation:** The discrete divergence, $(\\nabla \\cdot \\mathbf{B})_{i,j}$, is computed for all cells using the mixed-stencil finite difference scheme described above.\n3.  **Diagnostic Calculation:** The field magnitude $|\\mathbf{B}|_{i,j}$ and the normalized divergence $\\varepsilon_{i,j}$ are computed for all cells.\n4.  **Trigger Evaluation:** The boolean conditions for $T_{\\mathrm{clean}}$ and $T_{\\mathrm{ref}}$ are evaluated at each cell, yielding two boolean arrays.\n5.  **Result Aggregation:** The final outputs are computed from the full-grid diagnostic arrays:\n    - The maximum value of the normalized divergence, $\\varepsilon_{\\max} = \\max_{i,j}(\\varepsilon_{i,j})$.\n    - The total number of cells requiring cleaning, $n_{\\mathrm{clean}} = \\sum_{i,j} T_{\\mathrm{clean}}(i,j)$.\n    - The total number of cells requiring refinement, $n_{\\mathrm{ref}} = \\sum_{i,j} T_{\\mathrm{ref}}(i,j)$.\n\nThis procedure is applied to each of the four specified test cases, which are designed to probe the diagnostic's response to different field configurations: a smooth, analytically divergence-free field; a sharp discontinuity emulating a magnetic monopole; a near-vacuum region with random noise; and a step function in one field component superposed on a strong, uniform background field.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for MHD divergence diagnostics.\n    \"\"\"\n\n    def compute_diagnostics(Nx, Ny, Bx, By, dx, dy):\n        \"\"\"\n        Computes divergence diagnostics for a given magnetic field on a Cartesian grid.\n\n        Args:\n            Nx (int): Number of grid points in x.\n            Ny (int): Number of grid points in y.\n            Bx (np.ndarray): 2D array of the x-component of the magnetic field.\n            By (np.ndarray): 2D array of the y-component of the magnetic field.\n            dx (float): Grid spacing in x.\n            dy (float): Grid spacing in y.\n\n        Returns:\n            list: A list containing [eps_max, n_clean, n_ref].\n        \"\"\"\n        # Define given thresholds and regularization constant\n        theta_clean = 0.2\n        theta_ref = 0.6\n        phi_abs = 0.8\n        B_floor = 1e-6\n        \n        # Characteristic cell size\n        h = min(dx, dy)\n\n        # Compute partial derivatives using a second-order central difference\n        # for the interior and a first-order one-sided difference at the boundaries.\n        # numpy.gradient implements this exact scheme.\n        # axis=1 for d/dx, axis=0 for d/dy for a (Nx, Ny) array will not work with\n        # default 'xy' indexing. We use 'ij' indexing throughout, so for an\n        # array of shape (Nx, Ny), axis=0 corresponds to x and axis=1 to y.\n        # However, np.gradient(f, axis=0) gives d/dx0, and np.gradient(f, axis=1) gives d/dx1.\n        # For a 2D array A[i, j], i is row (y), j is col (x) by default plotting convention.\n        # To avoid confusion, we'll pass dx and dy as a tuple for coordinate spacing.\n        # np.gradient(data, dy, dx) assumes data[y,x] ordering.\n        \n        # In this implementation, we will use shape (Nx, Ny) and 'ij' indexing,\n        # so axis=0 is the x-axis (index_i) and axis=1 is the y-axis (index_j).\n        d_Bx_dx = np.gradient(Bx, dx, axis=0)\n        d_By_dy = np.gradient(By, dy, axis=1)\n\n        # Compute divergence of B\n        div_B = d_Bx_dx + d_By_dy\n\n        # Compute magnetic field magnitude\n        B_mag = np.sqrt(Bx**2 + By**2)\n\n        # Compute dimensionless normalized divergence indicator epsilon\n        epsilon = h * np.abs(div_B) / (B_mag + B_floor)\n        \n        # Calculate maximum epsilon over the grid\n        eps_max = np.max(epsilon)\n\n        # Evaluate cleaning and refinement triggers\n        T_clean = epsilon > theta_clean\n        T_ref = (epsilon > theta_ref) | (np.abs(div_B) > phi_abs)\n\n        # Count the number of cells where triggers are true\n        n_clean = np.sum(T_clean)\n        n_ref = np.sum(T_ref)\n\n        return [round(eps_max, 6), int(n_clean), int(n_ref)]\n\n    results = []\n\n    # Test Case A: Smooth, nearly divergence-free field\n    Nx_A, Ny_A = 33, 33\n    x_A = np.linspace(0, 2 * np.pi, Nx_A)\n    y_A = np.linspace(0, 2 * np.pi, Ny_A)\n    dx_A, dy_A = x_A[1] - x_A[0], y_A[1] - y_A[0]\n    xx_A, yy_A = np.meshgrid(x_A, y_A, indexing='ij')\n    kx, ky = 2, 3\n    Bx_A = ky * np.sin(kx * xx_A) * np.cos(ky * yy_A)\n    By_A = -kx * np.cos(kx * xx_A) * np.sin(ky * yy_A)\n    results.append(compute_diagnostics(Nx_A, Ny_A, Bx_A, By_A, dx_A, dy_A))\n\n    # Test Case B: Monopole-like interface\n    Nx_B, Ny_B = 9, 9\n    dx_B, dy_B = 1.0, 1.0\n    Bx_B = np.zeros((Nx_B, Ny_B))\n    i_step_B = int(np.ceil(Nx_B / 2.0)) # For Nx=9, ceil(4.5)=5. 0-indexed: 5,6,7,8\n    Bx_B[i_step_B:, :] = 1.0\n    By_B = np.zeros((Nx_B, Ny_B))\n    results.append(compute_diagnostics(Nx_B, Ny_B, Bx_B, By_B, dx_B, dy_B))\n\n    # Test Case C: Near-vacuum noise\n    Nx_C, Ny_C = 7, 7\n    dx_C, dy_C = 1.0, 1.0\n    rng = np.random.default_rng(seed=0)\n    Bx_C = rng.uniform(-1e-12, 1e-12, size=(Nx_C, Ny_C))\n    By_C = rng.uniform(-1e-12, 1e-12, size=(Nx_C, Ny_C))\n    results.append(compute_diagnostics(Nx_C, Ny_C, Bx_C, By_C, dx_C, dy_C))\n\n    # Test Case D: Strong uniform field with localized perturbation\n    Nx_D, Ny_D = 17, 17\n    dx_D, dy_D = 1.0, 1.0\n    Bx_D = np.full((Nx_D, Ny_D), 5.0)\n    By_D = np.zeros((Nx_D, Ny_D))\n    j_step_D = int(np.floor(Ny_D / 2.0)) # For Ny=17, floor(8.5)=8. 0-indexed: 8,...,16\n    By_D[:, j_step_D:] = 1.0\n    results.append(compute_diagnostics(Nx_D, Ny_D, Bx_D, By_D, dx_D, dy_D))\n    \n    # Format the final output string\n    # e.g., [[1.0,2,3],[4.0,5,6]]\n    output_str = f\"[{','.join(str(r).replace(' ', '') for r in results)}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "With a method to diagnose divergence errors, the next step is to correct them. This practice compares the two dominant families of cleaning techniques: the instantaneous, global projection method and the local, iterative Generalized Lagrange Multiplier (GLM) method. By implementing both an elliptic solver for a Poisson equation and a hyperbolic-parabolic scheme, you will gain direct insight into the fundamental trade-offs between accuracy, computational cost, and algorithmic complexity for different physical scenarios .",
            "id": "3506842",
            "problem": "You are to implement and compare two divergence-cleaning strategies for the magnetic field in Magnetohydrodynamics (MHD): a spectral projection method and a Generalized Lagrange Multiplier (GLM) cleaning method. The comparison must be made in terms of accuracy, measured by the post-cleaning discrete divergence norm, and computational cost, modeled by simple operation counts. All quantities are dimensionless.\n\nThe fundamental base for this problem is the divergence-free constraint from Maxwell’s equations in MHD, namely that the magnetic field satisfies $\\nabla \\cdot \\mathbf{B} = 0$. In practice, numerical schemes in shock-dominated flows can violate this constraint, leading to a nonzero discrete divergence that must be cleaned. Two widely used strategies are:\n\n- A projection method that enforces the constraint by solving a Poisson equation for a potential $\\phi$ and projecting $\\mathbf{B}$ via $\\mathbf{B} \\leftarrow \\mathbf{B} - \\nabla \\phi$.\n- A Generalized Lagrange Multiplier (GLM) method that introduces an auxiliary scalar field $\\psi$ and couples it hyperbolically and parabolically to $\\nabla \\cdot \\mathbf{B}$ to damp and advect divergence.\n\nYou must implement both methods on a periodic two-dimensional square domain $[0,1) \\times [0,1)$, discretized with a uniform grid of $N_x \\times N_y$ cells, with $\\Delta x = 1/N_x$ and $\\Delta y = 1/N_y$. The comparison shall be executed for a set of prescribed test cases.\n\nField construction for shock-dominated pre-cleaning state:\n- Construct a synthetic pre-cleaning magnetic field $\\mathbf{B}^*$ that mimics a shock-dominated situation by combining a smooth solenoidal component and a controlled non-solenoidal component:\n  - Let $\\psi(x,y) = A \\sin(2\\pi x)\\sin(2\\pi y)$ with $A = 0.1$.\n  - Let $\\phi_{\\text{shock}}(x,y) = s \\tanh\\left(\\frac{x - 1/2}{w}\\right)$, where $s$ controls shock severity and $w$ controls shock thickness.\n  - Define\n    $$\n    B_x^*(x,y) = \\frac{\\partial \\psi}{\\partial y}(x,y) + \\frac{\\partial \\phi_{\\text{shock}}}{\\partial x}(x,y), \\quad\n    B_y^*(x,y) = -\\frac{\\partial \\psi}{\\partial x}(x,y),\n    $$\n    so that $\\nabla \\cdot \\mathbf{B}^* = \\nabla^2 \\phi_{\\text{shock}}$ is nonzero and concentrated at the shock. Derivatives appearing in the construction of $\\mathbf{B}^*$ must be computed analytically:\n    $$\n    \\frac{\\partial \\psi}{\\partial y} = A \\sin(2\\pi x)\\, 2\\pi \\cos(2\\pi y), \\quad\n    \\frac{\\partial \\psi}{\\partial x} = A\\, 2\\pi \\cos(2\\pi x)\\, \\sin(2\\pi y),\n    $$\n    $$\n    \\frac{\\partial \\phi_{\\text{shock}}}{\\partial x} = \\frac{s}{w} \\operatorname{sech}^2\\!\\left(\\frac{x - 1/2}{w}\\right), \\quad \\frac{\\partial \\phi_{\\text{shock}}}{\\partial y} = 0.\n    $$\n\nDiscrete diagnostics:\n- Use second-order central differences with periodic wrapping to define the discrete divergence of a grid field $\\mathbf{B}$:\n  $$\n  (\\nabla \\cdot \\mathbf{B})_{i,j} = \\frac{B_{x,i+1,j} - B_{x,i-1,j}}{2\\Delta x} + \\frac{B_{y,i,j+1} - B_{y,i,j-1}}{2\\Delta y}.\n  $$\n- Define the discrete $L^2$ divergence norm as\n  $$\n  E(\\mathbf{B}) = \\sqrt{\\frac{1}{N_x N_y} \\sum_{i,j} \\left[(\\nabla \\cdot \\mathbf{B})_{i,j}\\right]^2 }.\n  $$\n\nMethod 1: Spectral projection (Poisson solve):\n- Compute a scalar potential $\\phi$ on the periodic grid by solving\n  $$\n  \\nabla^2 \\phi = \\nabla \\cdot \\mathbf{B}^*\n  $$\n  with zero-mean condition for $\\phi$ (i.e., the Fourier mode at zero wavenumber set to zero). Implement this solve spectrally using the Fast Fourier Transform (FFT): in Fourier space, for wavenumbers $k_x = 2\\pi n_x$ and $k_y = 2\\pi n_y$, solve\n  $$\n  \\widehat{\\phi}(\\mathbf{k}) = -\\frac{\\widehat{\\nabla \\cdot \\mathbf{B}^*}(\\mathbf{k})}{k_x^2 + k_y^2} \\quad \\text{for} \\quad \\mathbf{k} \\neq \\mathbf{0}, \\quad \\widehat{\\phi}(\\mathbf{0}) = 0,\n  $$\n  and obtain $\\nabla \\phi$ spectrally via multiplication by $\\mathrm{i}\\mathbf{k}$ and inverse FFTs. The cleaned field is\n  $$\n  \\mathbf{B}^{\\mathrm{proj}} = \\mathbf{B}^* - \\nabla \\phi.\n  $$\n- Cost model for the projection method: use a count of FFT operations. Assume that computing the projection requires $n_{\\mathrm{FFT}} = 4$ two-dimensional FFTs. The cost per two-dimensional FFT is modeled as\n  $$\n  C_{\\mathrm{FFT}} = \\gamma \\, N_x N_y \\, \\log_2(N_x N_y),\n  $$\n  where $\\gamma$ is a positive constant. Thus the total cost is\n  $$\n  \\text{Cost}_{\\mathrm{proj}} = n_{\\mathrm{FFT}} \\, C_{\\mathrm{FFT}}.\n  $$\n\nMethod 2: GLM cleaning (Generalized Lagrange Multiplier):\n- Initialize $\\psi = 0$. For $m$ substeps, perform the explicit update\n  $$\n  \\psi^{n+1} = \\psi^n - \\Delta t \\, c_h^2 \\, (\\nabla \\cdot \\mathbf{B}^n) - \\Delta t \\, \\frac{c_h^2}{c_p^2} \\, \\psi^n,\n  $$\n  $$\n  \\mathbf{B}^{n+1} = \\mathbf{B}^n - \\Delta t \\, \\nabla \\psi^{n+1},\n  $$\n  where $c_h$ is the hyperbolic cleaning speed, $c_p$ is the parabolic damping parameter, and $\\nabla$ is discretized by second-order central differences with periodic wrapping (consistent with the divergence operator above). Set the number of substeps to\n  $$\n  m = \\lceil c_h \\rceil,\n  $$\n  and choose the substep size\n  $$\n  \\Delta t = \\mathrm{CFL} \\cdot \\frac{\\min(\\Delta x,\\Delta y)}{\\max(c_h, 10^{-8})},\n  $$\n  with $\\mathrm{CFL} = 0.8$. The cleaned field after $m$ substeps is denoted $\\mathbf{B}^{\\mathrm{glm}}$.\n- Cost model for the GLM method: count local stencil operations. Assume each substep costs\n  $$\n  C_{\\mathrm{local}} = \\beta \\, N_x N_y\n  $$\n  operations, so the total cost is\n  $$\n  \\text{Cost}_{\\mathrm{glm}} = m \\, C_{\\mathrm{local}}.\n  $$\n\nComparison metric and decision rule:\n- Define an absolute accuracy tolerance\n  $$\n  \\tau = 10^{-3}.\n  $$\n- For each method, compute the post-cleaning divergence norms $E(\\mathbf{B}^{\\mathrm{proj}})$ and $E(\\mathbf{B}^{\\mathrm{glm}})$, and the costs $\\text{Cost}_{\\mathrm{proj}}$ and $\\text{Cost}_{\\mathrm{glm}}$ using the models above with $\\gamma$ and $\\beta$ specified below.\n- Determine which method dominates according to the following rule:\n  - If exactly one method achieves $E \\le \\tau$, select that method.\n  - If both methods achieve $E \\le \\tau$, select the one with the lower cost.\n  - If neither method achieves $E \\le \\tau$, select the one with the smaller $E$; if $E$ is equal within a relative tolerance of $10^{-6}$, select the one with lower cost.\n- Encode the selection per test case as an integer:\n  - Output $1$ if the projection method is selected.\n  - Output $-1$ if the GLM method is selected.\n  - Output $0$ if the tie-breaking leads to no strict preference (only applicable when both methods have equal $E$ within the relative tolerance and equal cost under the cost model).\n\nConstants for the cost model:\n- Use $\\gamma = 5.0$ and $\\beta = 20.0$.\n\nTest suite:\nImplement the program to execute exactly the following five test cases, where each tuple is $(N_x, N_y, s, w, c_h, c_p)$:\n- Case $1$: $(128, 128, 1.0, 0.02, 1.0, 1.0)$\n- Case $2$: $(128, 128, 3.0, 0.01, 1.0, 1.0)$\n- Case $3$: $(256, 256, 1.0, 0.02, 1.0, 1.0)$\n- Case $4$: $(128, 128, 1.0, 0.02, 5.0, 1.0)$\n- Case $5$: $(64, 64, 0.5, 0.05, 0.5, 1.0)$\n\nRequired final output:\n- Your program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets, in the same order as the test cases, for example, \"[$r_1, r_2, r_3, r_4, r_5$]\". No other output is permitted.",
            "solution": "The problem requires the implementation and comparison of two distinct numerical methods for enforcing the divergence-free constraint, $\\nabla \\cdot \\mathbf{B} = 0$, in Magnetohydrodynamics (MHD) on a two-dimensional periodic domain. The methods to be compared are a spectral projection method and a Generalized Lagrange Multiplier (GLM) cleaning scheme. The comparison is based on post-cleaning accuracy, measured by a discrete divergence norm, and computational cost, estimated through simplified operational models.\n\nFirst, we establish the computational domain and the initial state. The domain is a periodic square $[0,1) \\times [0,1)$ discretized by a uniform grid of $N_x \\times N_y$ cells, with grid spacings $\\Delta x = 1/N_x$ and $\\Delta y = 1/N_y$. The cell centers are located at $(x_i, y_j) = ((i+0.5)\\Delta x, (j+0.5)\\Delta y)$ for $i \\in \\{0, \\dots, N_x-1\\}$ and $j \\in \\{0, \\dots, N_y-1\\}$.\n\nThe initial, pre-cleaning magnetic field, denoted $\\mathbf{B}^*$, is analytically defined to contain both a solenoidal (divergence-free) part and a non-solenoidal part, mimicking numerical errors from a shock. It is constructed from a vector potential $\\psi(x,y) = A \\sin(2\\pi x)\\sin(2\\pi y)$ and a scalar potential $\\phi_{\\text{shock}}(x,y) = s \\tanh\\left(\\frac{x - 1/2}{w}\\right)$, with $A=0.1$. The components are:\n$$\nB_x^*(x,y) = \\frac{\\partial \\psi}{\\partial y}(x,y) + \\frac{\\partial \\phi_{\\text{shock}}}{\\partial x}(x,y) = 2\\pi A \\sin(2\\pi x) \\cos(2\\pi y) + \\frac{s}{w} \\operatorname{sech}^2\\!\\left(\\frac{x - 1/2}{w}\\right)\n$$\n$$\nB_y^*(x,y) = -\\frac{\\partial \\psi}{\\partial x}(x,y) = -2\\pi A \\cos(2\\pi x) \\sin(2\\pi y)\n$$\nThe divergence of this field is analytically $\\nabla \\cdot \\mathbf{B}^* = \\nabla^2\\phi_{\\text{shock}}$, which is non-zero.\n\nThe accuracy of each cleaning method is quantified by the discrete $L^2$ norm of the divergence of the cleaned field, defined as:\n$$\nE(\\mathbf{B}) = \\sqrt{\\frac{1}{N_x N_y} \\sum_{i,j} \\left[(\\nabla \\cdot \\mathbf{B})_{i,j}\\right]^2 }\n$$\nThe discrete divergence operator $(\\nabla \\cdot \\mathbf{B})_{i,j}$ is approximated using second-order central differences with periodic boundary conditions:\n$$\n(\\nabla \\cdot \\mathbf{B})_{i,j} = \\frac{B_{x,i+1,j} - B_{x,i-1,j}}{2\\Delta x} + \\frac{B_{y,i,j+1} - B_{y,i,j-1}}{2\\Delta y}\n$$\n\n**Method 1: Spectral Projection**\n\nThe principle of the projection method is to decompose the initial field $\\mathbf{B}^*$ into a divergence-free part and the gradient of a scalar potential, $\\mathbf{B}^* = \\mathbf{B}^{\\mathrm{proj}} + \\nabla \\phi$. To make the cleaned field $\\mathbf{B}^{\\mathrm{proj}}$ divergence-free (i.e., $\\nabla \\cdot \\mathbf{B}^{\\mathrm{proj}} = 0$), the potential $\\phi$ must satisfy the Poisson equation:\n$$\n\\nabla^2 \\phi = \\nabla \\cdot \\mathbf{B}^*\n$$\nOn a periodic domain, this equation is efficiently solved in Fourier space. Taking the Fourier transform yields:\n$$\n-(k_x^2 + k_y^2) \\widehat{\\phi}(\\mathbf{k}) = \\widehat{\\nabla \\cdot \\mathbf{B}^*}(\\mathbf{k})\n$$\nwhere $\\mathbf{k}=(k_x, k_y)$ is the wavenumber vector, with components $k_x = 2\\pi n_x$ and $k_y = 2\\pi n_y$ for integer mode numbers $n_x, n_y$. The solution for the Fourier coefficients of the potential is:\n$$\n\\widehat{\\phi}(\\mathbf{k}) = -\\frac{\\widehat{\\nabla \\cdot \\mathbf{B}^*}(\\mathbf{k})}{k_x^2 + k_y^2} \\quad \\text{for} \\quad \\mathbf{k} \\neq \\mathbf{0}\n$$\nFor $\\mathbf{k} = \\mathbf{0}$, the denominator is zero. A unique solution for $\\phi$ is obtained by imposing the zero-mean condition, $\\widehat{\\phi}(\\mathbf{0}) = 0$.\n\nThe implementation consistently uses the spectral method for all derivatives. First, the Fourier transforms of $B_x^*$ and $B_y^*$ are computed. Then, the divergence is computed in Fourier space via $\\widehat{\\nabla \\cdot \\mathbf{B}^*} = i k_x \\widehat{B_x^*} + i k_y \\widehat{B_y^*}$. After solving for $\\widehat{\\phi}$, the gradient $\\nabla \\phi$ is also found spectrally: $\\widehat{\\nabla\\phi} = i\\mathbf{k}\\widehat{\\phi}$. The components of $\\nabla\\phi$ are then recovered via inverse Fourier transforms. The cleaned field is $\\mathbf{B}^{\\mathrm{proj}} = \\mathbf{B}^* - \\nabla\\phi$. This entire procedure requires two forward Fast Fourier Transforms (FFTs) for $\\mathbf{B}^*$ and two inverse FFTs for $\\nabla\\phi$, confirming the given parameter $n_{\\mathrm{FFT}} = 4$. The computational cost is modeled as:\n$$\n\\text{Cost}_{\\mathrm{proj}} = n_{\\mathrm{FFT}} \\, \\gamma \\, N_x N_y \\, \\log_2(N_x N_y)\n$$\nwith $\\gamma = 5.0$.\n\n**Method 2: Generalized Lagrange Multiplier (GLM)**\n\nThe GLM method introduces an auxiliary scalar field, $\\psi$, which is coupled to the magnetic field. The system of equations is designed to transport divergence errors out of the domain at a speed $c_h$ and simultaneously damp them on a timescale related to a parameter $c_p$. The iterative update scheme for a single cleaning sweep is given by:\n$$\n\\psi^{n+1} = \\psi^n - \\Delta t \\, c_h^2 \\, (\\nabla \\cdot \\mathbf{B}^n) - \\Delta t \\, \\frac{c_h^2}{c_p^2} \\, \\psi^n\n$$\n$$\n\\mathbf{B}^{n+1} = \\mathbf{B}^n - \\Delta t \\, \\nabla \\psi^{n+1}\n$$\nStarting with $\\psi^0 = 0$ and $\\mathbf{B}^0 = \\mathbf{B}^*$, this scheme is applied for a total of $m = \\lceil c_h \\rceil$ substeps. The time step $\\Delta t$ is determined by a Courant-Friedrichs-Lewy (CFL) condition:\n$$\n\\Delta t = \\mathrm{CFL} \\cdot \\frac{\\min(\\Delta x,\\Delta y)}{\\max(c_h, 10^{-8})}\n$$\nwith $\\mathrm{CFL} = 0.8$. The discrete gradient $\\nabla$ and divergence $\\nabla \\cdot$ operators are implemented using second-order central differences, consistent with the diagnostic calculation. The cost, based on local stencil operations, is modeled as:\n$$\n\\text{Cost}_{\\mathrm{glm}} = m \\, \\beta \\, N_x N_y\n$$\nwith $\\beta = 20.0$.\n\n**Comparison and Decision**\n\nFor each test case, both methods are executed to obtain the cleaned fields $\\mathbf{B}^{\\mathrm{proj}}$ and $\\mathbf{B}^{\\mathrm{glm}}$, their respective divergence norms $E(\\mathbf{B}^{\\mathrm{proj}})$ and $E(\\mathbf{B}^{\\mathrm{glm}})$, and their costs $\\text{Cost}_{\\mathrm{proj}}$ and $\\text{Cost}_{\\mathrm{glm}}$. A decision is made based on a pre-defined rule set involving an accuracy tolerance $\\tau = 10^{-3}$:\n1.  If one method is accurate ($E \\le \\tau$) and the other is not, the accurate one is chosen.\n2.  If both are accurate, the one with the lower cost is chosen.\n3.  If neither is accurate, the one with the lower divergence norm $E$ is chosen. A tie in $E$ (within a relative tolerance of $10^{-6}$) is broken by choosing the lower-cost method.\n\nThe final choice is encoded as $1$ for the projection method, $-1$ for the GLM method, and $0$ for a perfect tie in both accuracy (within tolerance) and cost.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares spectral projection and GLM divergence cleaning methods for MHD.\n    \"\"\"\n\n    # --- Problem Constants ---\n    A = 0.1\n    GAMMA = 5.0\n    BETA = 20.0\n    CFL = 0.8\n    TAU = 1e-3\n    REL_TOL_E = 1e-6\n\n    # --- Test Suite ---\n    test_cases = [\n        # (Nx, Ny, s, w, ch, cp)\n        (128, 128, 1.0, 0.02, 1.0, 1.0),\n        (128, 128, 3.0, 0.01, 1.0, 1.0),\n        (256, 256, 1.0, 0.02, 1.0, 1.0),\n        (128, 128, 1.0, 0.02, 5.0, 1.0),\n        (64, 64, 0.5, 0.05, 0.5, 1.0),\n    ]\n\n    def calculate_divergence(Bx, By, dx, dy):\n        \"\"\"Computes discrete divergence using 2nd-order central differences.\"\"\"\n        d_Bx_dx = (np.roll(Bx, -1, axis=1) - np.roll(Bx, 1, axis=1)) / (2 * dx)\n        d_By_dy = (np.roll(By, -1, axis=0) - np.roll(By, 1, axis=0)) / (2 * dy)\n        return d_Bx_dx + d_By_dy\n\n    def calculate_gradient(phi, dx, dy):\n        \"\"\"Computes discrete gradient using 2nd-order central differences.\"\"\"\n        d_phi_dx = (np.roll(phi, -1, axis=1) - np.roll(phi, 1, axis=1)) / (2 * dx)\n        d_phi_dy = (np.roll(phi, -1, axis=0) - np.roll(phi, 1, axis=0)) / (2 * dy)\n        return d_phi_dx, d_phi_dy\n\n    def run_projection_method(B_star_x, B_star_y, Nx, Ny, dx, dy):\n        \"\"\"Implements the spectral projection method.\"\"\"\n        # Wavenumbers\n        kx = 2 * np.pi * np.fft.fftfreq(Nx, d=dx)\n        ky = 2 * np.pi * np.fft.fftfreq(Ny, d=dy)\n        kx_grid, ky_grid = np.meshgrid(kx, ky)\n        k_squared = kx_grid**2 + ky_grid**2\n\n        # FFT of initial field\n        B_hat_x = np.fft.fft2(B_star_x)\n        B_hat_y = np.fft.fft2(B_star_y)\n\n        # Spectral divergence\n        div_B_hat = 1j * kx_grid * B_hat_x + 1j * ky_grid * B_hat_y\n\n        # Solve for phi in Fourier space\n        phi_hat = np.zeros_like(div_B_hat)\n        non_zero_k = k_squared != 0\n        phi_hat[non_zero_k] = -div_B_hat[non_zero_k] / k_squared[non_zero_k]\n\n        # Spectral gradient of phi\n        grad_phi_x_hat = 1j * kx_grid * phi_hat\n        grad_phi_y_hat = 1j * ky_grid * phi_hat\n\n        # Inverse FFT to get gradient in real space\n        grad_phi_x = np.fft.ifft2(grad_phi_x_hat).real\n        grad_phi_y = np.fft.ifft2(grad_phi_y_hat).real\n\n        # Cleaned field\n        B_proj_x = B_star_x - grad_phi_x\n        B_proj_y = B_star_y - grad_phi_y\n\n        # Diagnostics\n        div_proj = calculate_divergence(B_proj_x, B_proj_y, dx, dy)\n        E_proj = np.sqrt(np.mean(div_proj**2))\n        cost_proj = 4 * GAMMA * Nx * Ny * np.log2(Nx * Ny)\n        \n        return E_proj, cost_proj\n\n    def run_glm_method(B_star_x, B_star_y, Nx, Ny, dx, dy, ch, cp):\n        \"\"\"Implements the GLM cleaning method.\"\"\"\n        # Parameters\n        m = int(np.ceil(ch))\n        dt = CFL * min(dx, dy) / max(ch, 1e-8)\n\n        # Initialization\n        psi = np.zeros((Ny, Nx))\n        Bx = B_star_x.copy()\n        By = B_star_y.copy()\n\n        # Iterative cleaning\n        for _ in range(m):\n            div_B = calculate_divergence(Bx, By, dx, dy)\n            psi_new = psi - dt * ch**2 * div_B - dt * (ch**2 / cp**2) * psi\n            grad_psi_x, grad_psi_y = calculate_gradient(psi_new, dx, dy)\n            Bx_new = Bx - dt * grad_psi_x\n            By_new = By - dt * grad_psi_y\n            \n            psi, Bx, By = psi_new, Bx_new, By_new\n        \n        # Diagnostics\n        B_glm_x, B_glm_y = Bx, By\n        div_glm = calculate_divergence(B_glm_x, B_glm_y, dx, dy)\n        E_glm = np.sqrt(np.mean(div_glm**2))\n        cost_glm = m * BETA * Nx * Ny\n\n        return E_glm, cost_glm\n        \n    results = []\n    for case in test_cases:\n        Nx, Ny, s, w, ch, cp = case\n        \n        # Grid setup\n        dx = 1.0 / Nx\n        dy = 1.0 / Ny\n        x = (np.arange(Nx) + 0.5) * dx\n        y = (np.arange(Ny) + 0.5) * dy\n        xx, yy = np.meshgrid(x, y)\n\n        # Initial field construction\n        B_star_x = (A * 2 * np.pi * np.sin(2 * np.pi * xx) * np.cos(2 * np.pi * yy) + \n                    (s / w) / (np.cosh((xx - 0.5) / w)**2))\n        B_star_y = -A * 2 * np.pi * np.cos(2 * np.pi * xx) * np.sin(2 * np.pi * yy)\n        \n        # Run methods\n        E_proj, cost_proj = run_projection_method(B_star_x, B_star_y, Nx, Ny, dx, dy)\n        E_glm, cost_glm = run_glm_method(B_star_x, B_star_y, Nx, Ny, dx, dy, ch, cp)\n\n        # Comparison logic\n        proj_ok = E_proj <= TAU\n        glm_ok = E_glm <= TAU\n        \n        result = 0\n        if proj_ok and not glm_ok:\n            result = 1\n        elif not proj_ok and glm_ok:\n            result = -1\n        elif proj_ok and glm_ok:\n            if cost_proj < cost_glm:\n                result = 1\n            elif cost_glm < cost_proj:\n                result = -1\n            else:\n                result = 0\n        else: # Neither is OK\n            if np.isclose(E_proj, E_glm, rtol=REL_TOL_E):\n                if cost_proj < cost_glm:\n                    result = 1\n                elif cost_glm < cost_proj:\n                    result = -1\n                else: \n                    result = 0\n            elif E_proj < E_glm:\n                result = 1\n            else:\n                result = -1\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Standard cleaning methods can be suboptimal on the anisotropic grids commonly used in astrophysical simulations. This advanced exercise challenges you to enhance the GLM method by designing a damping term, $c_p$, that adapts to the grid's local geometry and the magnetic field's orientation. Through a combination of discrete Fourier analysis and implementation, you will explore how tailoring numerical schemes to specific grid properties can significantly improve their performance, demonstrating the power of physics-aware algorithm design .",
            "id": "3506840",
            "problem": "Consider Generalized Lagrange Multiplier (GLM) divergence cleaning for Magnetohydrodynamics (MHD), which augments the induction equation with an auxiliary scalar field to transport and damp divergence errors. Assume the hyperbolic-parabolic cleaning form in two spatial dimensions with constant coefficients, given by the pair of linearized equations\n$$\n\\frac{\\partial \\mathbf{B}}{\\partial t} + \\nabla \\times \\mathbf{E} + \\nabla \\psi = \\mathbf{0}, \\quad\n\\frac{\\partial \\psi}{\\partial t} + c_h^2 \\nabla \\cdot \\mathbf{B} = - c_p \\, \\psi,\n$$\nwhere $\\mathbf{B}$ is the magnetic field, $\\psi$ is the cleaning field, $c_h$ is the cleaning wave speed, and $c_p$ is a damping rate. All quantities are nondimensional. Angles must be expressed in radians.\n\nStarting only from the above equations and standard discrete-Fourier analysis for second-order central differences on a uniform but anisotropic grid with spacings $\\Delta x$ and $\\Delta y$, do the following:\n\n1) Derive the evolution equation for the Fourier-mode amplitude of the discrete divergence $D = \\nabla \\cdot \\mathbf{B}$ in the form of a damped linear oscillator for a single mode with wavevector $\\mathbf{k} = (k_x, k_y)$, where the semi-discrete discrete Laplacian enters through the effective wavenumbers\n$$\nk_{x,\\mathrm{eff}} = \\frac{2}{\\Delta x} \\sin\\!\\left(\\frac{k_x \\Delta x}{2}\\right), \\quad\nk_{y,\\mathrm{eff}} = \\frac{2}{\\Delta y} \\sin\\!\\left(\\frac{k_y \\Delta y}{2}\\right).\n$$\nShow that the mode amplitude $A(t)$ of $D$ satisfies for constant coefficients an ordinary differential equation of the form\n$$\nA''(t) + c_p \\, A'(t) + \\omega_0^2 \\, A(t) = 0,\n$$\nwith $\\omega_0^2 = c_h^2 \\left(k_{x,\\mathrm{eff}}^2 + k_{y,\\mathrm{eff}}^2\\right)$, and adopt the initial conditions $A(0) = 1$ and $A'(0) = 0$.\n\n2) Propose an anisotropic damping model that depends on the local magnetic-field orientation and grid anisotropy. Let $\\theta$ be the angle between the magnetic field and the $x$-axis, and let $r = \\Delta x / \\Delta y$ be the grid anisotropy ratio. Define the anisotropic damping rate $c_p(\\theta)$ by\n$$\nc_p(\\theta) = c_{p0} \\, \\frac{\\sqrt{(\\Delta x \\cos\\theta)^2 + (\\Delta y \\sin\\theta)^2}}{\\sqrt{\\Delta x \\, \\Delta y}} = c_{p0} \\sqrt{ r \\cos^2\\theta + \\frac{1}{r} \\sin^2\\theta },\n$$\nwhere $c_{p0}$ is a baseline isotropic damping rate. Justify this choice based on the directional metric induced by the stretched mesh and the desire to strengthen damping along directions with larger effective spacing.\n\n3) For a single Fourier mode of magnitude $k_0$ aligned with the magnetic field, i.e., $k_x = k_0 \\cos\\theta$ and $k_y = k_0 \\sin\\theta$, compute the exact closed-form amplitude $A(t)$ at time $t = T$ using the ordinary differential equation in part $1$ for both the isotropic case with $c_p = c_{p0}$ and the anisotropic case with $c_p = c_p(\\theta)$.\n\n4) Define the performance metric for divergence control as the ratio\n$$\nR = \\frac{A_{\\mathrm{aniso}}(T)}{A_{\\mathrm{iso}}(T)},\n$$\nwhere $A_{\\mathrm{aniso}}(T)$ and $A_{\\mathrm{iso}}(T)$ are the amplitudes at time $T$ for the anisotropic and isotropic damping choices, respectively. Values $R < 1$ indicate improved control from the anisotropic model at the chosen parameters.\n\nYour task is to write a complete program that, for each test case in the suite below, computes $R$ using the exact closed-form solution for $A(t)$ under the initial conditions $A(0) = 1$ and $A'(0) = 0$, taking proper account of the three regimes of the damped oscillator (underdamped, critically damped, overdamped) determined by the sign of $c_p^2 - 4 \\omega_0^2$.\n\nUse the following test suite, where each test case is a tuple $(\\Delta x, \\Delta y, \\theta, k_0, c_h, c_{p0}, T)$:\n\n- Case $1$ (happy path, isotropic grid so $R = 1$): $(\\Delta x, \\Delta y, \\theta, k_0, c_h, c_{p0}, T) = (1.0, 1.0, 0.7, 1.2, 1.0, 1.0, 1.0)$.\n- Case $2$ (stretched in $x$, field aligned with stretch): $(4.0, 1.0, 0.0, 1.0, 1.0, 0.5, 2.0)$.\n- Case $3$ (stretched in $x$, field perpendicular to stretch): $(4.0, 1.0, \\pi/2, 1.0, 1.0, 0.5, 2.0)$.\n- Case $4$ (strongly stretched, small angle): $(10.0, 1.0, 0.1, 2.2, 1.0, 0.7, 3.0)$.\n- Case $5$ (zero wavenumber edge case, no decay of the mean mode): $(2.0, 1.0, 0.3, 0.0, 1.0, 0.9, 1.5)$.\n- Case $6$ (near critical damping for the isotropic choice): $(1.0, 0.5, 0.0, 2.6, 1.0, 3.8, 0.7)$.\n\nYour program must:\n\n- Implement the exact closed-form solution for $A(T)$ in each regime using the initial conditions $A(0) = 1$ and $A'(0) = 0$.\n- Use the discrete effective wavenumbers $k_{x,\\mathrm{eff}}$ and $k_{y,\\mathrm{eff}}$ as specified above.\n- Compute and output a single line containing a list with the values of $R$ for all test cases in the given order, rounded to six decimal places, as a comma-separated list enclosed in square brackets (e.g., $[0.123456,0.234567,\\dots]$).\n\nAll quantities are nondimensional, and angles are in radians. No external input should be read; the test suite is fixed as specified above and embedded in your program. The output must be exactly the single line described, with no additional text.",
            "solution": "The problem is valid. It is a well-posed problem in computational astrophysics, grounded in the standard mathematical framework of numerical methods for partial differential equations, specifically magnetohydrodynamics (MHD). All necessary equations, parameters, and initial conditions are provided, and there are no internal contradictions, scientific inaccuracies, or ambiguities.\n\nHerein, we derive the requested results and formulate the algorithm for computation.\n\n### Part 1: Derivation of the Damped Oscillator Equation for Divergence\n\nWe are given the linearized, two-dimensional Generalized Lagrange Multiplier (GLM) equations with constant coefficients:\n$$ \\frac{\\partial \\mathbf{B}}{\\partial t} + \\nabla \\times \\mathbf{E} + \\nabla \\psi = \\mathbf{0} \\quad (1) $$\n$$ \\frac{\\partial \\psi}{\\partial t} + c_h^2 \\nabla \\cdot \\mathbf{B} = - c_p \\, \\psi \\quad (2) $$\n\nOur goal is to derive an evolution equation for the divergence of the magnetic field, $D = \\nabla \\cdot \\mathbf{B}$. We begin by taking the divergence of equation $(1)$:\n$$ \\nabla \\cdot \\left( \\frac{\\partial \\mathbf{B}}{\\partial t} \\right) + \\nabla \\cdot (\\nabla \\times \\mathbf{E}) + \\nabla \\cdot (\\nabla \\psi) = 0 $$\nAssuming sufficient smoothness, we can commute the spatial and temporal derivatives, $\\nabla \\cdot (\\frac{\\partial \\mathbf{B}}{\\partial t}) = \\frac{\\partial}{\\partial t}(\\nabla \\cdot \\mathbf{B})$. The divergence of a curl is identically zero, $\\nabla \\cdot (\\nabla \\times \\mathbf{E}) \\equiv 0$. The divergence of a gradient is the Laplacian operator, $\\nabla \\cdot (\\nabla \\psi) = \\nabla^2 \\psi$. The equation thus simplifies to:\n$$ \\frac{\\partial D}{\\partial t} + \\nabla^2 \\psi = 0 \\quad (3) $$\nThis equation shows that a non-zero divergence of the cleaning field $\\psi$ acts as a source for changes in the magnetic field divergence.\n\nTo obtain a second-order equation for $D$, we differentiate equation $(3)$ with respect to time:\n$$ \\frac{\\partial^2 D}{\\partial t^2} + \\frac{\\partial}{\\partial t}(\\nabla^2 \\psi) = 0 $$\nAgain, assuming smoothness, we commute derivatives:\n$$ \\frac{\\partial^2 D}{\\partial t^2} + \\nabla^2 \\left(\\frac{\\partial \\psi}{\\partial t}\\right) = 0 \\quad (4) $$\nNow, we substitute the expression for $\\frac{\\partial \\psi}{\\partial t}$ from equation $(2)$ into equation $(4)$:\n$$ \\frac{\\partial^2 D}{\\partial t^2} + \\nabla^2 (-c_h^2 \\nabla \\cdot \\mathbf{B} - c_p \\psi) = 0 $$\nSubstituting $D = \\nabla \\cdot \\mathbf{B}$ and distributing the Laplacian (since $c_h$ and $c_p$ are constants):\n$$ \\frac{\\partial^2 D}{\\partial t^2} - c_h^2 \\nabla^2 D - c_p \\nabla^2 \\psi = 0 \\quad (5) $$\nFinally, we use equation $(3)$ to replace $\\nabla^2 \\psi$ with $-\\frac{\\partial D}{\\partial t}$. Substituting this into equation $(5)$ yields:\n$$ \\frac{\\partial^2 D}{\\partial t^2} - c_h^2 \\nabla^2 D - c_p \\left(-\\frac{\\partial D}{\\partial t}\\right) = 0 $$\nRearranging gives the damped wave equation for the divergence $D$:\n$$ \\frac{\\partial^2 D}{\\partial t^2} + c_p \\frac{\\partial D}{\\partial t} - c_h^2 \\nabla^2 D = 0 $$\n\nTo find the equation for a single Fourier mode, we consider a solution of the form $D(\\mathbf{x}, t) = A(t) e^{i \\mathbf{k} \\cdot \\mathbf{x}}$, where $\\mathbf{k} = (k_x, k_y)$ is the wavevector and $A(t)$ is the mode's time-dependent amplitude. For a continuous system, the Laplacian operator $\\nabla^2$ acting on this mode corresponds to multiplication by $-k^2 = -(k_x^2 + k_y^2)$.\n\nFor the semi-discrete system using second-order central differences on a grid with spacings $(\\Delta x, \\Delta y)$, the Laplacian operator's action on the mode $e^{i(k_x x + k_y y)}$ is modified. The second partial derivative $\\frac{\\partial^2}{\\partial x^2}$ is approximated by the central difference operator $\\delta_x^2 f(x) = \\frac{f(x+\\Delta x) - 2f(x) + f(x-\\Delta x)}{(\\Delta x)^2}$. Its action on $e^{i k_x x}$ yields a multiplicative factor:\n$$ \\frac{e^{i k_x(x+\\Delta x)} - 2e^{i k_x x} + e^{i k_x(x-\\Delta x)}}{(\\Delta x)^2} = \\frac{e^{i k_x x}(e^{i k_x \\Delta x} - 2 + e^{-i k_x \\Delta x})}{(\\Delta x)^2} = \\frac{e^{i k_x x}(2\\cos(k_x \\Delta x) - 2)}{(\\Delta x)^2} $$\nUsing the half-angle identity $1 - \\cos(\\alpha) = 2\\sin^2(\\alpha/2)$, this becomes:\n$$ \\frac{e^{i k_x x}(-4\\sin^2(k_x \\Delta x/2))}{(\\Delta x)^2} = - \\left( \\frac{2}{\\Delta x} \\sin\\left(\\frac{k_x \\Delta x}{2}\\right) \\right)^2 e^{i k_x x} = -k_{x,\\mathrm{eff}}^2 e^{i k_x x} $$\nThe discrete Laplacian operator $\\nabla_d^2 = \\delta_x^2 + \\delta_y^2$ therefore acts on the mode $A(t) e^{i \\mathbf{k} \\cdot \\mathbf{x}}$ by multiplying it by $-(k_{x,\\mathrm{eff}}^2 + k_{y,\\mathrm{eff}}^2)$.\n\nSubstituting this into the wave equation for $D$ gives the ordinary differential equation for the amplitude $A(t)$:\n$$ A''(t) + c_p A'(t) + c_h^2 (k_{x,\\mathrm{eff}}^2 + k_{y,\\mathrm{eff}}^2) A(t) = 0 $$\nThis is precisely the required form $A''(t) + c_p A'(t) + \\omega_0^2 A(t) = 0$, with the natural frequency squared given by $\\omega_0^2 = c_h^2 (k_{x,\\mathrm{eff}}^2 + k_{y,\\mathrm{eff}}^2)$.\n\n### Part 2: Justification of the Anisotropic Damping Model\n\nThe proposed anisotropic damping rate is:\n$$ c_p(\\theta) = c_{p0} \\sqrt{ r \\cos^2\\theta + \\frac{1}{r} \\sin^2\\theta } $$\nwhere $r = \\Delta x / \\Delta y$ is the grid anisotropy ratio and $\\theta$ is the angle of the magnetic field relative to the $x$-axis.\n\nThe rationale for this model is to adapt the damping strength to the local grid resolution along the direction of the magnetic field (and consequently, the direction of the divergence wave propagation). The term under the square root, $M(\\theta, r) = r \\cos^2\\theta + \\frac{1}{r} \\sin^2\\theta$, can be rewritten as:\n$$ M(\\theta, r) = \\frac{\\Delta x}{\\Delta y} \\cos^2\\theta + \\frac{\\Delta y}{\\Delta x} \\sin^2\\theta = \\frac{(\\Delta x \\cos\\theta)^2 + (\\Delta y \\sin\\theta)^2}{\\Delta x \\Delta y} $$\nThe numerator represents the squared length of a cell-dimension-scaled vector pointing in the direction $\\theta$. This quantity serves as a measure of the effective grid spacing in that direction. The damping rate $c_p(\\theta)$ is made proportional to the square root of this effective spacing measure.\n\nConsider the behavior at the axes:\n- Along the $x$-axis ($\\theta = 0$): $c_p(0) = c_{p0} \\sqrt{r} = c_{p0} \\sqrt{\\Delta x / \\Delta y}$.\n- Along the $y$-axis ($\\theta = \\pi/2$): $c_p(\\pi/2) = c_{p0} \\sqrt{1/r} = c_{p0} \\sqrt{\\Delta y / \\Delta x}$.\nIf the grid is stretched in the $x$-direction ($\\Delta x > \\Delta y$, so $r > 1$), then $c_p(0) > c_{p0}$ and $c_p(\\pi/2) < c_{p0}$. This means damping is strengthened in the direction of coarser resolution ($\\Delta x$) and weakened in the direction of finer resolution ($\\Delta y$). This heuristic is sensible: numerical errors are often more pronounced or propagate differently in directions with larger cell sizes, so increasing the damping in these directions can improve the stability and accuracy of the divergence cleaning scheme.\n\nFor an isotropic grid ($r=1$), $c_p(\\theta) = c_{p0} \\sqrt{\\cos^2\\theta+\\sin^2\\theta} = c_{p0}$, recovering the isotropic case as required.\n\n### Part 3: Closed-Form Solution for Amplitude $A(t)$\n\nWe must solve the ODE $A''(t) + c_p A'(t) + \\omega_0^2 A(t) = 0$ subject to initial conditions $A(0)=1$ and $A'(0)=0$. The characteristic equation is $\\lambda^2 + c_p\\lambda + \\omega_0^2 = 0$, with roots $\\lambda_{1,2} = \\frac{-c_p \\pm \\sqrt{c_p^2 - 4\\omega_0^2}}{2}$. The solution form depends on the sign of the discriminant $\\Delta = c_p^2 - 4\\omega_0^2$.\n\n1.  **Overdamped Case ($\\Delta > 0$):**\n    The general solution is $A(t) = e^{-c_p t/2} (C_1 e^{\\Omega_d t} + C_2 e^{-\\Omega_d t})$, where $\\Omega_d = \\frac{\\sqrt{\\Delta}}{2}$. Applying initial conditions $A(0)=1$ and $A'(0)=0$ yields coefficients, resulting in the specific solution:\n    $$ A(t) = e^{-c_p t/2} \\left[ \\cosh(\\Omega_d t) + \\frac{c_p}{2\\Omega_d} \\sinh(\\Omega_d t) \\right] $$\n\n2.  **Critically Damped Case ($\\Delta = 0$):**\n    The characteristic equation has a repeated root $\\lambda = -c_p/2$. The general solution is $A(t) = (C_1 + C_2 t)e^{-c_p t/2}$. Applying the initial conditions gives:\n    $$ A(t) = \\left(1 + \\frac{c_p}{2} t\\right) e^{-c_p t/2} $$\n\n3.  **Underdamped Case ($\\Delta < 0$):**\n    The roots are complex conjugates, $\\lambda_{1,2} = -c_p/2 \\pm i\\omega_d$, where $\\omega_d = \\frac{\\sqrt{-\\Delta}}{2}$. The general solution is $A(t) = e^{-c_p t/2} (C_1 \\cos(\\omega_d t) + C_2 \\sin(\\omega_d t))$. Applying initial conditions yields:\n    $$ A(t) = e^{-c_p t/2} \\left[ \\cos(\\omega_d t) + \\frac{c_p}{2\\omega_d} \\sin(\\omega_d t) \\right] $$\n\nAn important edge case is when $k_0 = 0$, which implies $k_{x,\\mathrm{eff}} = k_{y,\\mathrm{eff}} = 0$ and thus $\\omega_0^2 = 0$. The ODE becomes $A''(t) + c_p A'(t) = 0$. Given $A'(0)=0$, the solution is $A'(t)=0$ for all $t$, which implies $A(t)$ is constant. With $A(0)=1$, we have $A(t)=1$. Our derived formulas correctly handle this: if $\\omega_0^2=0$, then $\\Delta = c_p^2 \\ge 0$. For $c_p>0$, this is the overdamped case with $\\Omega_d = c_p/2$, and the formula simplifies to $A(t)=1$. For $c_p=0$, this is the critically damped case, which also gives $A(t)=1$.\n\n### Part 4: Algorithmic Implementation\n\nThe task reduces to implementing these closed-form solutions and calculating the ratio $R = A_{\\mathrm{aniso}}(T) / A_{\\mathrm{iso}}(T)$ for each test case.\n\nThe algorithm is as follows:\nFor each test case tuple $(\\Delta x, \\Delta y, \\theta, k_0, c_h, c_{p0}, T)$:\n1.  Calculate ancillary parameters: grid ratio $r = \\Delta x / \\Delta y$, and wavevector components $k_x = k_0 \\cos\\theta$ and $k_y = k_0 \\sin\\theta$.\n2.  Calculate the effective wavenumbers:\n    $$ k_{x,\\mathrm{eff}} = \\frac{2}{\\Delta x} \\sin\\left(\\frac{k_x \\Delta x}{2}\\right), \\quad k_{y,\\mathrm{eff}} = \\frac{2}{\\Delta y} \\sin\\left(\\frac{k_y \\Delta y}{2}\\right) $$\n3.  Calculate the natural frequency squared common to both scenarios: $\\omega_0^2 = c_h^2(k_{x,\\mathrm{eff}}^2 + k_{y,\\mathrm{eff}}^2)$.\n4.  Define a function, `compute_amplitude(T, c_p, omega_0_sq)`, that takes the time $T$, damping rate $c_p$, and frequency squared $\\omega_0^2$, and returns $A(T)$. This function will:\n    a. Calculate the discriminant $\\Delta = c_p^2 - 4\\omega_0^2$.\n    b. Use a small tolerance to check if $\\Delta$ is positive, negative, or zero.\n    c. Apply the appropriate formula (overdamped, critically damped, or underdamped) to compute $A(T)$.\n5.  **Isotropic case**:\n    a. Set $c_p = c_{p0}$.\n    b. Call `compute_amplitude` to get $A_{\\mathrm{iso}}(T)$.\n6.  **Anisotropic case**:\n    a. Calculate $c_p(\\theta) = c_{p0} \\sqrt{r \\cos^2\\theta + (1/r) \\sin^2\\theta}$.\n    b. Call `compute_amplitude` to get $A_{\\mathrm{aniso}}(T)$.\n7.  Calculate the performance ratio $R = A_{\\mathrm{aniso}}(T) / A_{\\mathrm{iso}}(T)$. If $A_{\\mathrm{iso}}(T)$ happens to be zero, this ratio is treated by standard floating-point division rules.\n8.  Store the rounded value of $R$.\nAfter processing all cases, print the results in the specified format.\n\nThis procedure constitutes the basis for the provided Python program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the divergence control performance metric R for a set of test cases\n    based on the GLM-MHD divergence cleaning equations.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (dx, dy, theta, k0, ch, cp0, T)\n    test_cases = [\n        (1.0, 1.0, 0.7, 1.2, 1.0, 1.0, 1.0),\n        (4.0, 1.0, 0.0, 1.0, 1.0, 0.5, 2.0),\n        (4.0, 1.0, np.pi/2, 1.0, 1.0, 0.5, 2.0),\n        (10.0, 1.0, 0.1, 2.2, 1.0, 0.7, 3.0),\n        (2.0, 1.0, 0.3, 0.0, 1.0, 0.9, 1.5),\n        (1.0, 0.5, 0.0, 2.6, 1.0, 3.8, 0.7),\n    ]\n\n    results = []\n    \n    # A small tolerance for floating point comparisons to handle the\n    # critically damped case robustly.\n    TOLERANCE = 1e-12\n\n    def calculate_A(T, c_p, omega_0_sq):\n        \"\"\"\n        Calculates the amplitude A(T) of the divergence mode for a given time T,\n        damping coefficient c_p, and natural frequency squared omega_0_sq.\n        \n        The function handles the three regimes of a damped linear oscillator:\n        underdamped, critically damped, and overdamped.\n        \n        Args:\n            T (float): The time at which to evaluate the amplitude.\n            c_p (float): The damping coefficient.\n            omega_0_sq (float): The square of the natural frequency.\n            \n        Returns:\n            float: The amplitude A at time T.\n        \"\"\"\n        # If omega_0_sq is basically zero (k=0 mode), the amplitude does not decay.\n        if abs(omega_0_sq) < TOLERANCE:\n            return 1.0\n            \n        discriminant = c_p**2 - 4 * omega_0_sq\n\n        if discriminant > TOLERANCE:  # Overdamped case\n            Omega_d = 0.5 * np.sqrt(discriminant)\n            # Avoid division by zero if Omega_d is tiny, though c_p > 0 and T > 0\n            # should prevent this from being an issue in the full expression.\n            # The case omega_0_sq=0 and c_p=0 is handled by critical damping.\n            # If omega_0_sq > 0 and c_p=2*omega_0, then Omega_d=0, handled by critical.\n            # So Omega_d > 0 here.\n            term1 = np.cosh(Omega_d * T)\n            term2 = (0.5 * c_p / Omega_d) * np.sinh(Omega_d * T)\n            amplitude = np.exp(-0.5 * c_p * T) * (term1 + term2)\n        elif discriminant < -TOLERANCE:  # Underdamped case\n            omega_d = 0.5 * np.sqrt(-discriminant)\n            # omega_d cannot be zero here.\n            term1 = np.cos(omega_d * T)\n            term2 = (0.5 * c_p / omega_d) * np.sin(omega_d * T)\n            amplitude = np.exp(-0.5 * c_p * T) * (term1 + term2)\n        else:  # Critically damped case\n            amplitude = (1 + 0.5 * c_p * T) * np.exp(-0.5 * c_p * T)\n            \n        return amplitude\n\n    for case in test_cases:\n        dx, dy, theta, k0, ch, cp0, T = case\n\n        # Calculate grid ratio and wavevector components\n        r = dx / dy\n        kx = k0 * np.cos(theta)\n        ky = k0 * np.sin(theta)\n\n        # Calculate effective wavenumbers, handling k=0 case to avoid sin(0)/0\n        k_x_eff = (2.0 / dx) * np.sin(kx * dx / 2.0) if abs(kx) > TOLERANCE else 0.0\n        k_y_eff = (2.0 / dy) * np.sin(ky * dy / 2.0) if abs(ky) > TOLERANCE else 0.0\n        \n        # This explicit check is safer than relying on kx->0 making sin(kx*dx/2)->0.\n        # Although for the given problems, only k0=0 causes this.\n        if k0 == 0.0:\n            k_x_eff = 0.0\n            k_y_eff = 0.0\n        \n        # Calculate natural frequency squared\n        omega_0_sq = ch**2 * (k_x_eff**2 + k_y_eff**2)\n        \n        # --- Isotropic case ---\n        c_p_iso = cp0\n        A_iso = calculate_A(T, c_p_iso, omega_0_sq)\n\n        # --- Anisotropic case ---\n        c_p_aniso = cp0 * np.sqrt(r * np.cos(theta)**2 + (1.0/r) * np.sin(theta)**2)\n        A_aniso = calculate_A(T, c_p_aniso, omega_0_sq)\n        \n        # --- Performance Metric R ---\n        # Handle cases where A_iso might be zero or very close to it.\n        if abs(A_iso) < TOLERANCE:\n            # If both are zero, ratio is 1 (or nan). If only denominator is zero,\n            # ratio is infinite. The problem implies finite results.\n            # For k0=0, A_iso=1. For k0>0, A_iso is typically non-zero for small T.\n            # Assuming test cases avoid A_iso=0.\n            if abs(A_aniso) < TOLERANCE:\n                R = 1.0\n            else:\n                R = np.inf\n        else:\n            R = A_aniso / A_iso\n        \n        results.append(R)\n\n    # Format output as specified, rounding to 6 decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}