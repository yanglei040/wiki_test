{
    "hands_on_practices": [
        {
            "introduction": "This problem gets to the heart of how implicit methods handle stiffness. We will dissect a single time step of a powerful Implicit Runge-Kutta method, the two-stage Radau IIA, applied to the fundamental stiff test equation, $y' = \\lambda y$. By manually solving the internal stage equations and calculating the resulting amplification factor, you will gain a concrete understanding of how these methods achieve strong damping of stiff components, a property known as L-stability .",
            "id": "3535957",
            "problem": "In modeling the rapid approach to equilibrium of a single reactive species embedded in a nuclear reaction network relevant to explosive astrophysical environments, a common linearized stiff mode is represented by the scalar ordinary differential equation (ODE) $y'(t) = \\lambda\\, y(t)$ with $\\lambda \\ll 0$. Consider advancing this mode over one time step using the two-stage Radau Implicit Rungeâ€“Kutta (IRK) method of type IIA, which is a collocation method at the right Radau points. The method is specified by the Butcher coefficients\n$$\nA = \\begin{pmatrix}\n\\frac{5}{12}  -\\frac{1}{12} \\\\\n\\frac{3}{4}  \\frac{1}{4}\n\\end{pmatrix}, \n\\quad\n\\boldsymbol{b} = \\begin{pmatrix}\n\\frac{3}{4} \\\\ \\frac{1}{4}\n\\end{pmatrix}, \n\\quad\n\\boldsymbol{c} = \\begin{pmatrix}\n\\frac{1}{3} \\\\ 1\n\\end{pmatrix}.\n$$\nStarting from the standard initial value formulation, the IRK stage equations for a general autonomous system $y'(t) = f(y)$ are given by\n$$\n\\boldsymbol{Y} = y_n \\boldsymbol{e} + h\\, A\\, \\boldsymbol{f}(\\boldsymbol{Y}),\n$$\nwhere $y_n$ is the current solution value, $h$ is the time step, $\\boldsymbol{e}$ is the vector of ones, and $\\boldsymbol{f}(\\boldsymbol{Y})$ is the vector of stage derivatives. For the linear stiff test problem $f(y) = \\lambda y$, the stage system becomes a linear $2 \\times 2$ system in the stage vector $\\boldsymbol{Y}$.\n\nUsing only these definitions and the given Butcher coefficients, carry out one Radau IIA step for the scalar ODE $y'(t) = \\lambda y(t)$ with $\\lambda = -10^{4}$ and step size $h = 10^{-2}$. Solve the $2 \\times 2$ stage system exactly and determine the amplification factor $R(z)$ defined by $y_{n+1} = R(z)\\, y_n$, where $z = h \\lambda$. Express the final amplification factor as an exact rational number (no rounding).",
            "solution": "The Implicit Runge-Kutta (IRK) method for an autonomous ODE $y'(t) = f(y(t))$ is defined by the stage equations and the solution update. For the linear scalar test problem $y'(t) = \\lambda y(t)$, the function is $f(y) = \\lambda y$. The stage equations become a linear system for the stage vector $\\boldsymbol{Y} = (Y_1, Y_2)^T$:\n$$\n\\boldsymbol{Y} = y_n \\boldsymbol{e} + h A (\\lambda \\boldsymbol{Y}) = y_n \\boldsymbol{e} + (h \\lambda) A \\boldsymbol{Y}\n$$\nwhere $\\boldsymbol{e}=(1, 1)^T$. Let $z = h\\lambda$. The equation is:\n$$\n\\boldsymbol{Y} = y_n \\boldsymbol{e} + z A \\boldsymbol{Y}\n$$\nRearranging to solve for $\\boldsymbol{Y}$, we get the $2 \\times 2$ stage system:\n$$\n(\\boldsymbol{I} - zA) \\boldsymbol{Y} = y_n \\boldsymbol{e}\n$$\nUsing the provided matrix $A$, the system matrix is:\n$$\n\\boldsymbol{I} - zA = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - z \\begin{pmatrix} \\frac{5}{12}  -\\frac{1}{12} \\\\ \\frac{3}{4}  \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{5z}{12}  \\frac{z}{12} \\\\ -\\frac{3z}{4}  1 - \\frac{z}{4} \\end{pmatrix}\n$$\nTo solve for $\\boldsymbol{Y}$, we find the inverse of this matrix. First, we compute its determinant:\n$$\n\\det(\\boldsymbol{I} - zA) = \\left(1 - \\frac{5z}{12}\\right)\\left(1 - \\frac{z}{4}\\right) - \\left(\\frac{z}{12}\\right)\\left(-\\frac{3z}{4}\\right) = 1 - \\frac{z}{4} - \\frac{5z}{12} + \\frac{5z^2}{48} + \\frac{3z^2}{48}\n$$\n$$\n\\det(\\boldsymbol{I} - zA) = 1 - \\frac{8z}{12} + \\frac{z^2}{6} = 1 - \\frac{2z}{3} + \\frac{z^2}{6}\n$$\nThe solution for the stage vector is $\\boldsymbol{Y} = (\\boldsymbol{I} - zA)^{-1} y_n \\boldsymbol{e}$. The inverse is:\n$$\n(\\boldsymbol{I} - zA)^{-1} = \\frac{1}{1 - \\frac{2z}{3} + \\frac{z^2}{6}} \\begin{pmatrix} 1 - \\frac{z}{4}  -\\frac{z}{12} \\\\ \\frac{3z}{4}  1 - \\frac{5z}{12} \\end{pmatrix}\n$$\nMultiplying by $y_n \\boldsymbol{e}$:\n$$\n\\boldsymbol{Y} = \\frac{y_n}{1 - \\frac{2z}{3} + \\frac{z^2}{6}} \\begin{pmatrix} 1 - \\frac{z}{4} - \\frac{z}{12} \\\\ \\frac{3z}{4} + 1 - \\frac{5z}{12} \\end{pmatrix} = \\frac{y_n}{1 - \\frac{2z}{3} + \\frac{z^2}{6}} \\begin{pmatrix} 1 - \\frac{z}{3} \\\\ 1 + \\frac{z}{3} \\end{pmatrix}\n$$\nThe solution is updated via $y_{n+1} = y_n + h \\boldsymbol{b}^T \\boldsymbol{f}(\\boldsymbol{Y}) = y_n + z \\boldsymbol{b}^T \\boldsymbol{Y}$. The amplification factor is $R(z) = y_{n+1}/y_n$:\n$$\nR(z) = 1 + z \\boldsymbol{b}^T \\frac{\\boldsymbol{Y}}{y_n} = 1 + \\frac{z}{1 - \\frac{2z}{3} + \\frac{z^2}{6}} \\begin{pmatrix} \\frac{3}{4}  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 1 - \\frac{z}{3} \\\\ 1 + \\frac{z}{3} \\end{pmatrix}\n$$\nThe inner product is:\n$$\n\\frac{3}{4}\\left(1 - \\frac{z}{3}\\right) + \\frac{1}{4}\\left(1 + \\frac{z}{3}\\right) = 1 - \\frac{z}{6}\n$$\nSubstituting this back into the expression for $R(z)$:\n$$\nR(z) = 1 + \\frac{z(1 - z/6)}{1 - \\frac{2z}{3} + \\frac{z^2}{6}} = \\frac{(1 - \\frac{2z}{3} + \\frac{z^2}{6}) + (z - \\frac{z^2}{6})}{1 - \\frac{2z}{3} + \\frac{z^2}{6}} = \\frac{1 + \\frac{z}{3}}{1 - \\frac{2z}{3} + \\frac{z^2}{6}}\n$$\nWe are given $\\lambda = -10^4$ and $h=10^{-2}$, therefore $z = h\\lambda = -100$. We substitute this value into the expression for $R(z)$:\n$$\nR(-100) = \\frac{1 + \\frac{-100}{3}}{1 - \\frac{2(-100)}{3} + \\frac{(-100)^2}{6}} = \\frac{1 - \\frac{100}{3}}{1 + \\frac{200}{3} + \\frac{10000}{6}}\n$$\nThe numerator is $1 - \\frac{100}{3} = -\\frac{97}{3}$. The denominator is $1 + \\frac{200}{3} + \\frac{5000}{3} = \\frac{3+200+5000}{3} = \\frac{5203}{3}$.\nThus, the amplification factor is:\n$$\nR(-100) = \\frac{-\\frac{97}{3}}{\\frac{5203}{3}} = -\\frac{97}{5203}\n$$\nThis fraction is in its simplest form.",
            "answer": "$$\n\\boxed{-\\frac{97}{5203}}\n$$"
        },
        {
            "introduction": "Moving from single-step to multi-step methods, this exercise focuses on the widely used Backward Differentiation Formulas (BDF). You will first derive the second-order BDF method (BDF2) from its definition based on polynomial interpolation. Then, you will apply it to a two-dimensional linear system, which requires you to analyze stability not with a scalar factor, but with a $4 \\times 4$ amplification matrix, revealing the dynamics of multi-step methods on coupled systems .",
            "id": "3535986",
            "problem": "In computational astrophysics, stiff ordinary differential equations arise when multiple physical processes relax on widely separated timescales. Consider a simplified, nondimensionalized linearization of a two-variable relaxation model near equilibrium, with state vector $y(t) \\in \\mathbb{R}^{2}$ governed by the autonomous system $y'(t) = J y(t) + b$, where $J \\in \\mathbb{R}^{2 \\times 2}$ is constant and $b \\in \\mathbb{R}^{2}$ is constant. Assume a constant time step $h  0$ and uniform grid $t_{n} = t_{0} + n h$. The second-order backward differentiation formula (BDF2) is defined by differentiating at $t_{n+1}$ the unique quadratic interpolant that passes through the three points $\\{(t_{n-1}, y_{n-1}), (t_{n}, y_{n}), (t_{n+1}, y_{n+1})\\}$ on the solution trajectory, and equating this derivative to the right-hand side $f(t_{n+1}, y_{n+1}) = J y_{n+1} + b$.\n\nStarting from this definition and the basic facts of polynomial interpolation and differentiation, derive the BDF2 step relation that connects $y_{n+1}$, $y_{n}$, and $y_{n-1}$ for a constant step size $h$. Then, for the specific stiff linear system with\n$$\nJ = \\begin{pmatrix}\n-6  2 \\\\\n-3  -12\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n3 \\\\\n-1\n\\end{pmatrix}, \\quad\nh = \\frac{1}{6},\n$$\nwrite the BDF2 step as a linear solve for $y_{n+1}$ in terms of $y_{n}$ and $y_{n-1}$, and compute explicitly the $4 \\times 4$ amplification matrix $G$ that maps the augmented state $\\begin{pmatrix} y_{n} \\\\ y_{n-1} \\end{pmatrix}$ to $\\begin{pmatrix} y_{n+1} \\\\ y_{n} \\end{pmatrix}$ when $b$ is set to zero. You must show all steps from first principles to obtain the coefficients of the BDF2 relation, and then perform the required matrix algebra to express the amplification matrix in closed form. Provide the amplification matrix without numerical rounding.",
            "solution": "The problem requires the derivation of the second-order backward differentiation formula (BDF2) from first principles, its application to a specific linear system of ordinary differential equations (ODEs), and the calculation of the corresponding amplification matrix.\n\nFirst, we derive the BDF2 formula. The method is defined by approximating the derivative $y'(t_{n+1})$ using the derivative of a unique quadratic polynomial $P(t)$ that interpolates the numerical solution at three points: $(t_{n-1}, y_{n-1})$, $(t_{n}, y_{n})$, and $(t_{n+1}, y_{n+1})$. The time steps are uniform, so $t_k = t_0 + k h$ for a constant step size $h  0$.\n\nTo simplify the derivation, we introduce a local, non-dimensional time coordinate $\\tau = (t - t_{n+1})/h$. The interpolation points in this coordinate system are $(\\tau, y) = (-2, y_{n-1})$, $(-1, y_n)$, and $(0, y_{n+1})$.\nLet the quadratic interpolant be $P(\\tau) = a\\tau^2 + b\\tau + c$. We find the coefficients $a$, $b$, and $c$ by enforcing the interpolation conditions:\n$P(0) = c = y_{n+1}$\n$P(-1) = a - b + c = y_n$\n$P(-2) = 4a - 2b + c = y_{n-1}$\n\nFrom the first equation, we substitute $c = y_{n+1}$ into the others:\n$a - b = y_n - y_{n+1}$\n$4a - 2b = y_{n-1} - y_{n+1} \\implies 2a - b = \\frac{1}{2}(y_{n-1} - y_{n+1})$\n\nSubtracting the first of these new equations from the second gives the coefficient $a$:\n$(2a - b) - (a - b) = a = \\frac{1}{2}(y_{n-1} - y_{n+1}) - (y_n - y_{n+1}) = \\frac{1}{2}y_{n-1} - y_n + \\frac{1}{2}y_{n+1}$\n\nNow we solve for $b$:\n$b = a - (y_n - y_{n+1}) = \\left(\\frac{1}{2}y_{n-1} - y_n + \\frac{1}{2}y_{n+1}\\right) - (y_n - y_{n+1}) = \\frac{1}{2}y_{n-1} - 2y_n + \\frac{3}{2}y_{n+1}$\n\nThe derivative of the solution $y(t)$ is approximated by the derivative of the interpolating polynomial $P(t)$. Using the chain rule, $y'(t) = \\frac{d P}{d\\tau} \\frac{d\\tau}{dt}$.\nWe have $\\frac{d\\tau}{dt} = \\frac{1}{h}$ and $\\frac{dP}{d\\tau} = 2a\\tau + b$.\nThe derivative at $t_{n+1}$ corresponds to $\\tau = 0$:\n$y'(t_{n+1}) \\approx \\left.\\frac{dP}{dt}\\right|_{t=t_{n+1}} = \\frac{1}{h} (2a(0) + b) = \\frac{b}{h}$\nSubstituting the expression for $b$:\n$y'(t_{n+1}) \\approx \\frac{1}{h} \\left(\\frac{1}{2}y_{n-1} - 2y_n + \\frac{3}{2}y_{n+1}\\right) = \\frac{3y_{n+1} - 4y_n + y_{n-1}}{2h}$\n\nThe BDF2 method equates this derivative approximation to the right-hand side of the ODE, $f(t_{n+1}, y_{n+1}) = J y_{n+1} + b$:\n$$\n\\frac{3y_{n+1} - 4y_n + y_{n-1}}{2h} = J y_{n+1} + b\n$$\nRearranging the terms to isolate $y_{n+1}$, we obtain the BDF2 step relation for this linear system:\n$$\n\\left(\\frac{3}{2h}I - J\\right) y_{n+1} = \\frac{4y_n - y_{n-1}}{2h} + b\n$$\nMultiplying by $2h$:\n$$\n(3I - 2hJ) y_{n+1} = 4y_n - y_{n-1} + 2hb\n$$\nThis can be written as a linear solve for $y_{n+1}$:\n$$\ny_{n+1} = (3I - 2hJ)^{-1} (4y_n - y_{n-1} + 2hb)\n$$\n\nNext, we are asked to find the $4 \\times 4$ amplification matrix $G$ for the homogeneous system (i.e., with $b = 0$). The homogeneous recurrence relation is:\n$$\ny_{n+1} = (3I - 2hJ)^{-1} (4y_n - y_{n-1})\n$$\nLet's define the matrix $A = (3I - 2hJ)^{-1}$. The relation becomes:\n$$\ny_{n+1} = 4A y_n - A y_{n-1}\n$$\nThe problem defines an augmented state vector $\\begin{pmatrix} y_{n} \\\\ y_{n-1} \\end{pmatrix}$. The evolution of this augmented state is given by $Z_{n+1} = G Z_n$, where $Z_k = \\begin{pmatrix} y_k \\\\ y_{k-1} \\end{pmatrix}$.\n$$\nZ_{n+1} = \\begin{pmatrix} y_{n+1} \\\\ y_n \\end{pmatrix} = \\begin{pmatrix} 4A y_n - A y_{n-1} \\\\ I y_n + 0 y_{n-1} \\end{pmatrix}\n$$\nThis can be written in block matrix form:\n$$\n\\begin{pmatrix} y_{n+1} \\\\ y_n \\end{pmatrix} = \\begin{pmatrix} 4A  -A \\\\ I_{2}  0_{2} \\end{pmatrix} \\begin{pmatrix} y_n \\\\ y_{n-1} \\end{pmatrix}\n$$\nwhere $I_2$ and $0_2$ are the $2 \\times 2$ identity and zero matrices, respectively.\nThe amplification matrix $G$ is therefore:\n$$\nG = \\begin{pmatrix} 4A  -A \\\\ I_{2}  0_{2} \\end{pmatrix} = \\begin{pmatrix} 4(3I - 2hJ)^{-1}  -(3I - 2hJ)^{-1} \\\\ I_{2}  0_{2} \\end{pmatrix}\n$$\n\nNow we substitute the given values:\n$J = \\begin{pmatrix} -6  2 \\\\ -3  -12 \\end{pmatrix}$ and $h = \\frac{1}{6}$.\nFirst, we compute the matrix to be inverted, which we will call $M = 3I - 2hJ$:\n$2h = 2(\\frac{1}{6}) = \\frac{1}{3}$\n$$\nM = 3\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\frac{1}{3}\\begin{pmatrix} -6  2 \\\\ -3  -12 \\end{pmatrix} = \\begin{pmatrix} 3  0 \\\\ 0  3 \\end{pmatrix} - \\begin{pmatrix} -2  2/3 \\\\ -1  -4 \\end{pmatrix} = \\begin{pmatrix} 3 - (-2)  0 - 2/3 \\\\ 0 - (-1)  3 - (-4) \\end{pmatrix} = \\begin{pmatrix} 5  -2/3 \\\\ 1  7 \\end{pmatrix}\n$$\nWe find the inverse of $M$. The determinant is:\n$\\det(M) = (5)(7) - (-\\frac{2}{3})(1) = 35 + \\frac{2}{3} = \\frac{105+2}{3} = \\frac{107}{3}$\nThe inverse is $A = M^{-1}$:\n$$\nA = M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} 7  2/3 \\\\ -1  5 \\end{pmatrix} = \\frac{3}{107} \\begin{pmatrix} 7  2/3 \\\\ -1  5 \\end{pmatrix} = \\frac{1}{107} \\begin{pmatrix} 21  2 \\\\ -3  15 \\end{pmatrix}\n$$\nNow we compute the blocks of $G$:\nBlock (1,1) is $4A$:\n$$\n4A = \\frac{4}{107} \\begin{pmatrix} 21  2 \\\\ -3  15 \\end{pmatrix} = \\frac{1}{107} \\begin{pmatrix} 84  8 \\\\ -12  60 \\end{pmatrix}\n$$\nBlock (1,2) is $-A$:\n$$\n-A = -\\frac{1}{107} \\begin{pmatrix} 21  2 \\\\ -3  15 \\end{pmatrix} = \\frac{1}{107} \\begin{pmatrix} -21  -2 \\\\ 3  -15 \\end{pmatrix}\n$$\nBlock (2,1) is $I_2 = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$.\nBlock (2,2) is $0_2 = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$.\n\nAssembling the $4 \\times 4$ amplification matrix $G$:\n$$\nG = \\begin{pmatrix} \\frac{84}{107}  \\frac{8}{107}  \\frac{-21}{107}  \\frac{-2}{107} \\\\ \\frac{-12}{107}  \\frac{60}{107}  \\frac{3}{107}  \\frac{-15}{107} \\\\ 1  0  0  0 \\\\ 0  1  0  0 \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{84}{107}  \\frac{8}{107}  -\\frac{21}{107}  -\\frac{2}{107} \\\\\n-\\frac{12}{107}  \\frac{60}{107}  \\frac{3}{107}  -\\frac{15}{107} \\\\\n1  0  0  0 \\\\\n0  1  0  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "This final practice bridges theory and application by using a computational approach to analyze a stiff, nonlinear ODE modeling power-law radiative cooling in astrophysics. You will use a high-quality BDF solver to investigate a subtle but critical phenomenon: order reduction, where a method's practical accuracy can be lower than its theoretical order under extreme stiffness. This exercise demonstrates how to benchmark a numerical solver against an analytical solution and critically assess its performance in a realistic scientific context .",
            "id": "3535987",
            "problem": "Consider the nondimensional initial value problem for power-law radiative cooling, modeled as the ordinary differential equation (ODE) $\\frac{dT}{dt}=-\\Lambda_{0} T^{\\alpha}$ with initial condition $T(0)=T_{0}$, where $T$ is a nondimensional temperature, $t$ is a nondimensional time, $\\Lambda_{0}0$ is a nondimensional cooling coefficient, and $\\alpha \\ge 1$ is a nondimensional power-law index that can induce stiffness for large $\\alpha$. Your task is to construct a semi-analytic benchmark for this ODE, use it to validate the accuracy of an implicit stiff solver, and assess potential order reduction as the stiffness increases.\n\nStarting from the fundamental definition of the derivative $\\frac{dT}{dt}$ and separation of variables, derive the closed-form solution $T(t)$ appropriate for this class of right-hand sides (without introducing any untested formulas). Use that solution as the reference to quantify global errors of a stiff solver at a specified final time. Use a Backward Differentiation Formula (BDF) method (Backward Differentiation Formula (BDF) is a family of implicit linear multi-step methods widely used for stiff problems) with a user-supplied analytic Jacobian to integrate the ODE and obtain $T(t_{\\mathrm{f}})$ for a sequence of tight relative tolerances. For each test case, compute the observed convergence rate by performing a least-squares fit of $\\log$ of the final-time error versus $\\log$ of the relative tolerance across three tolerance levels. Interpret the fitted slope as an empirical indicator of the effective order and assess order reduction when $\\alpha$ is large.\n\nAll variables and parameters are nondimensional, so no physical units are required. Angles are not involved.\n\nImplement a program that performs the following for each test case:\n- Integrate the ODE from $t=0$ to $t=t_{\\mathrm{f}}$ using a BDF solver with a supplied analytic Jacobian $\\partial f/\\partial T$.\n- Use three relative tolerances $10^{-3}$, $10^{-5}$, and $10^{-7}$, with a fixed absolute tolerance of $10^{-12}$.\n- For each tolerance, compute the absolute error at $t=t_{\\mathrm{f}}$ relative to the exact solution derived by separation of variables.\n- Fit a straight line to $(\\log(\\text{rtol}), \\log(\\text{error}))$ across the three tolerance levels and report the absolute value of the fitted slope as a single floating-point number for that test case.\n\nTest Suite:\n- Case A (baseline linear, non-stiff reference): $\\Lambda_{0}=1$, $\\alpha=1$, $T_{0}=2$, $t_{\\mathrm{f}}=1$.\n- Case B (moderately stiff): $\\Lambda_{0}=1$, $\\alpha=4$, $T_{0}=2$, $t_{\\mathrm{f}}=1$.\n- Case C (very stiff transient): $\\Lambda_{0}=1$, $\\alpha=20$, $T_{0}=2$, $t_{\\mathrm{f}}=1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the three fitted slopes for Cases A, B, and C, expressed as decimal numbers. For example: $[s_A,s_B,s_C]$.\n\nThe final output must therefore be a single line in the exact format: $[s_A,s_B,s_C]$, where each $s$ is a floating-point number computed as specified above.",
            "solution": "The problem presents a well-posed initial value problem (IVP) from computational astrophysics, suitable for a rigorous numerical analysis. It is scientifically grounded, using a standard power-law model for radiative cooling, and is methodologically sound, proposing a standard technique for evaluating the convergence and order of a stiff ODE solver. The problem statement is complete and unambiguous, allowing for a direct and verifiable solution.\n\nThe core of the problem is the nondimensional ODE for temperature $T$ as a function of time $t$:\n$$\n\\frac{dT}{dt} = -\\Lambda_{0} T^{\\alpha}\n$$\nwith an initial condition $T(0)=T_{0}$. The parameters are the cooling coefficient $\\Lambda_{0} > 0$ and the power-law index $\\alpha \\ge 1$.\n\nThe first step is to derive the exact analytical solution $T(t)$, which will serve as the ground truth for evaluating the numerical solver's accuracy. The ODE is separable and can be integrated as follows:\n$$\n\\int_{T_{0}}^{T(t)} T'^{-\\alpha} dT' = \\int_{0}^{t} -\\Lambda_{0} dt'\n$$\nThe solution depends on the value of the index $\\alpha$.\n\nFor the case $\\alpha = 1$, the ODE is linear:\n$$\n\\int_{T_{0}}^{T(t)} \\frac{1}{T'} dT' = [\\ln T']_{T_{0}}^{T(t)} = \\ln\\left(\\frac{T(t)}{T_{0}}\\right)\n$$\nThe right-hand side evaluates to $-\\Lambda_{0} t$. Equating the two sides and solving for $T(t)$ yields the exponential decay solution:\n$$\nT(t) = T_{0} e^{-\\Lambda_{0} t} \\quad (\\text{for } \\alpha=1)\n$$\n\nFor the case $\\alpha > 1$, the integration gives:\n$$\n\\int_{T_{0}}^{T(t)} T'^{-\\alpha} dT' = \\left[\\frac{T'^{1-\\alpha}}{1-\\alpha}\\right]_{T_{0}}^{T(t)} = \\frac{T(t)^{1-\\alpha} - T_{0}^{1-\\alpha}}{1-\\alpha}\n$$\nSetting this equal to $-\\Lambda_{0} t$ and rearranging for $T(t)$ gives:\n$$\nT(t)^{1-\\alpha} = T_{0}^{1-\\alpha} - (1-\\alpha)\\Lambda_{0} t = T_{0}^{1-\\alpha} + (\\alpha-1)\\Lambda_{0} t\n$$\n$$\nT(t) = \\left[ T_{0}^{1-\\alpha} + (\\alpha-1)\\Lambda_{0} t \\right]^{\\frac{1}{1-\\alpha}} \\quad (\\text{for } \\alpha1)\n$$\nThese closed-form solutions provide the exact benchmark required for the analysis.\n\nThe numerical solution will be obtained using a Backward Differentiation Formula (BDF) method, which is specifically designed for stiff ODEs. We will use the implementation provided by `scipy.integrate.solve_ivp` with `method='BDF'`. For nonlinear stiff problems, providing the analytical Jacobian of the right-hand side function significantly improves the solver's efficiency and robustness. For the function $f(t, T) = -\\Lambda_{0} T^{\\alpha}$, the Jacobian $J$ is the scalar derivative with respect to $T$:\n$$\nJ = \\frac{\\partial f}{\\partial T} = -\\Lambda_{0} \\alpha T^{\\alpha-1}\n$$\n\nThe analysis of the solver's performance is based on measuring its convergence rate. For a well-behaved adaptive solver, the global error $\\epsilon$ at the final time $t_{\\mathrm{f}}$ is expected to be proportional to the requested relative tolerance, `rtol`. This implies a relationship $\\epsilon \\approx C \\cdot (\\text{rtol})$, where the exponent on the tolerance is $1$. Taking the logarithm of this relationship, we get $\\log(\\epsilon) \\approx \\log(\\text{rtol}) + \\log(C)$, which describes a line with a slope of $1$. By fitting a line to empirical data points $(\\log(\\text{rtol}), \\log(\\epsilon))$, we can measure the effective order of convergence. A slope deviating from $1$ indicates non-ideal behavior.\n\nThe stiffness of the ODE is characterized by the magnitude of the Jacobian, $|\\lambda| = |J| = \\Lambda_{0} \\alpha T^{\\alpha-1}$.\n-   For Case A ($\\alpha=1$), $\\lambda$ is constant, and the problem is non-stiff. We expect the measured slope to be close to $1$.\n-   For Cases B ($\\alpha=4$) and C ($\\alpha=20$), the initial value of $|\\lambda|$ at $t=0$ is large, indicating a fast, stiff transient. For the extremely stiff Case C, a phenomenon known as *order reduction* is anticipated. This occurs when high-order implicit methods fail to achieve their theoretical convergence order on very stiff problems, resulting in a measured slope significantly less than $1$.\n\nThe algorithm for each test case is as follows:\n$1$. Set the parameters $\\Lambda_{0}$, $\\alpha$, $T_{0}$, and $t_{\\mathrm{f}}$.\n$2$. For each relative tolerance in the set $\\{10^{-3}, 10^{-5}, 10^{-7}\\}$, integrate the ODE from $t=0$ to $t=t_{\\mathrm{f}}$ using the BDF solver with the analytical Jacobian and a fixed absolute tolerance of $10^{-12}$.\n$3$. At $t=t_{\\mathrm{f}}$, compute the absolute error between the numerical solution and the exact analytical solution.\n$4$. Perform a linear least-squares fit to the three data points $(\\log(\\text{rtol}), \\log(\\text{error}))$.\n$5$. The absolute value of the slope from this fit is the result for the test case.\nThis procedure is repeated for all three test cases to generate the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Computes the empirical convergence rate for a stiff ODE solver\n    on a power-law cooling problem across three test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (Lambda0, alpha, T0, t_final)\n    test_cases = [\n        (1.0, 1.0, 2.0, 1.0),  # Case A: Non-stiff\n        (1.0, 4.0, 2.0, 1.0),  # Case B: Moderately stiff\n        (1.0, 20.0, 2.0, 1.0) # Case C: Very stiff\n    ]\n\n    # Define the set of relative tolerances for the convergence study.\n    rtols = np.array([1e-3, 1e-5, 1e-7])\n    # Define the fixed absolute tolerance.\n    atol = 1e-12\n\n    results = []\n    for case in test_cases:\n        Lambda0, alpha, T0, tf = case\n\n        # Define the ODE function, f(t, y), where y is a vector.\n        # The problem is scalar, so we use y[0].\n        def ode_func(t, y):\n            return [-Lambda0 * y[0]**alpha]\n\n        # Define the analytical Jacobian of the ODE function, df/dy.\n        # For a scalar problem, this is a 1x1 matrix.\n        def jac_func(t, y):\n            return [[-Lambda0 * alpha * y[0]**(alpha - 1.0)]]\n\n        # Define the exact analytical solution T(t).\n        def analytic_solution(t):\n            if np.isclose(alpha, 1.0):\n                # Solution for alpha = 1\n                return T0 * np.exp(-Lambda0 * t)\n            else:\n                # Solution for alpha > 1\n                # The exponent (1.0 - alpha) is negative, so this is well-defined.\n                base = T0**(1.0 - alpha) + (alpha - 1.0) * Lambda0 * t\n                return base**(1.0 / (1.0 - alpha))\n\n        # Calculate the exact solution at the final time for error computation.\n        T_exact_f = analytic_solution(tf)\n\n        errors = []\n        for rtol in rtols:\n            # Integrate the ODE using the BDF method with the supplied Jacobian.\n            # We only need the solution at the final time t_f.\n            sol = solve_ivp(\n                fun=ode_func,\n                t_span=[0, tf],\n                y0=[T0],\n                method='BDF',\n                jac=jac_func,\n                rtol=rtol,\n                atol=atol,\n                t_eval=[tf]\n            )\n            \n            # Extract the numerical solution at t_f.\n            T_num_f = sol.y[0, -1]\n            \n            # Compute the absolute error against the exact solution.\n            error = np.abs(T_num_f - T_exact_f)\n            errors.append(error)\n        \n        # To find the convergence rate, we fit a line to log(error) vs. log(rtol).\n        # The slope of this line is the empirical order of convergence.\n        log_rtols = np.log(rtols)\n        log_errors = np.log(np.array(errors))\n        \n        # Perform a linear least-squares fit (degree 1 polynomial).\n        # np.polyfit returns coefficients [slope, intercept].\n        slope, _ = np.polyfit(log_rtols, log_errors, 1)\n        \n        # The problem asks for the absolute value of the slope.\n        results.append(abs(slope))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}