{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of numerical analysis is not just implementing formulas, but understanding and systematically improving their accuracy. This practice introduces Richardson extrapolation, a powerful and general technique for enhancing the precision of a numerical estimate. By combining results from a lower-order formula computed at two different step sizes, you will derive and implement a higher-order approximation, effectively canceling the dominant error term without needing to design a more complex finite difference stencil from scratch .",
            "id": "3525596",
            "problem": "In computational astrophysics, gradients of dimensionless model functions often need to be estimated numerically when closed-form expressions are unavailable or expensive to evaluate. Consider the dimensionless scalar function $f(x) = \\ln(1+x)$ and the task of estimating the derivative $f'(x)$ at the point $x_0 = 1$. The fundamental base for numerical differentiation is the definition of the derivative and the Taylor series expansion. Starting from the definition $f'(x_0) = \\lim_{h \\to 0} \\frac{f(x_0+h) - f(x_0-h)}{2h}$ and from Taylor expansions about $x_0$, derive the second-order central difference formula for $f'(x_0)$, identify its leading truncation error term, and explain why that error scales as a power of the step size $h$. Then, using two second-order central difference approximations computed at two step sizes $h_1$ and $h_2$ with $h_1 = 2 h_2$, derive a single combined estimate that cancels the leading truncation error term by constructing an appropriate linear combination of the two approximations. This combined estimate is obtained without assuming or using any pre-stated shortcut formula.\n\nWrite a program that implements the derived procedure to compute the improved estimate of $f'(1)$ for the function $f(x) = \\ln(1+x)$ using the following test suite of step sizes:\n- Case A (moderate steps): $h_1 = 10^{-2}$ and $h_2 = 5 \\times 10^{-3}$.\n- Case B (coarse steps): $h_1 = 10^{-1}$ and $h_2 = 5 \\times 10^{-2}$.\n- Case C (very fine steps): $h_1 = 10^{-6}$ and $h_2 = 5 \\times 10^{-7}$.\n\nFor each case, compute:\n1. The second-order central difference approximations at $h_1$ and at $h_2$.\n2. The improved derivative estimate at $x_0=1$ that cancels the leading truncation error term using the derived linear combination.\n3. The absolute error of the improved estimate with respect to the exact analytic derivative $f'(x) = \\frac{1}{1+x}$ evaluated at $x_0 = 1$, which equals $\\frac{1}{2}$.\n\nAll quantities in this problem are dimensionless; report the absolute error in dimensionless units. Your program should produce a single line of output containing the absolute errors for the improved estimates for the three cases, rounded to twelve decimal places, as a comma-separated list enclosed in square brackets (for example, $[e_A,e_B,e_C]$ where $e_A$, $e_B$, and $e_C$ are the absolute errors for Cases A, B, and C, respectively).",
            "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **Function:** The dimensionless scalar function is $f(x) = \\ln(1+x)$.\n- **Task:** Estimate the derivative $f'(x)$ at the point $x_0 = 1$.\n- **Starting Point 1 (Formula):** The definition of the central difference is provided as a limit: $f'(x_0) = \\lim_{h \\to 0} \\frac{f(x_0+h) - f(x_0-h)}{2h}$.\n- **Starting Point 2 (Method):** Use Taylor expansions about $x_0$ to derive the numerical formulas.\n- **Requirement 1:** Derive the second-order central difference formula for $f'(x_0)$.\n- **Requirement 2:** Identify its leading truncation error term and explain its scaling with step size $h$.\n- **Requirement 3:** Using two approximations at step sizes $h_1$ and $h_2$ where $h_1 = 2 h_2$, derive a combined estimate that cancels the leading truncation error term.\n- **Test Cases:**\n    - Case A: $h_1 = 10^{-2}$, $h_2 = 5 \\times 10^{-3}$.\n    - Case B: $h_1 = 10^{-1}$, $h_2 = 5 \\times 10^{-2}$.\n    - Case C: $h_1 = 10^{-6}$, $h_2 = 5 \\times 10^{-7}$.\n- **Comparison Value:** The exact analytic derivative is $f'(x) = \\frac{1}{1+x}$, which at $x_0 = 1$ is $f'(1) = \\frac{1}{2}$.\n- **Output:** The absolute errors of the improved estimates for the three cases, rounded to twelve decimal places, in the format `[e_A,e_B,e_C]`.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is firmly rooted in the fundamental principles of numerical analysis, specifically numerical differentiation using finite differences and Richardson extrapolation. The use of Taylor series to analyze truncation error is a standard and core concept in this field. The function $f(x) = \\ln(1+x)$ is a standard, well-behaved analytic function. The problem is a classic example from computational science. It is scientifically and factually sound.\n2.  **Well-Posed:** The problem is clearly defined. It asks for the derivation of a specific numerical method and its application to a concrete function with specified parameters. The existence of a unique, stable, and meaningful solution is guaranteed.\n3.  **Objective:** The problem is stated in precise, objective, and formal mathematical language. It is free of any ambiguity, subjectivity, or opinion.\n4.  **Completeness and Consistency:** The problem is self-contained. It provides the function, the point of evaluation, the step sizes, the relationship between step sizes, the exact value for error computation, and a precise output format. All provided information is consistent.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-posed, scientifically grounded problem in numerical methods. I will proceed with the solution.\n\n## Theoretical Derivation and Solution\n\nThe objective is to first derive the second-order central difference formula and its error term, and then use this result to derive a higher-order approximation through Richardson extrapolation.\n\n### Part 1: Derivation of the Second-Order Central Difference Formula\n\nWe start with the Taylor series expansions of $f(x_0 + h)$ and $f(x_0 - h)$ around the point $x_0$. Assuming the function $f$ is sufficiently smooth (which $f(x)=\\ln(1+x)$ is for $x>-1$), we can write:\n$$\nf(x_0 + h) = f(x_0) + h f'(x_0) + \\frac{h^2}{2!} f''(x_0) + \\frac{h^3}{3!} f'''(x_0) + \\frac{h^4}{4!} f^{(4)}(x_0) + \\frac{h^5}{5!} f^{(5)}(x_0) + O(h^6)\n$$\n$$\nf(x_0 - h) = f(x_0) - h f'(x_0) + \\frac{h^2}{2!} f''(x_0) - \\frac{h^3}{3!} f'''(x_0) + \\frac{h^4}{4!} f^{(4)}(x_0) - \\frac{h^5}{5!} f^{(5)}(x_0) + O(h^6)\n$$\nTo isolate the first derivative term, $f'(x_0)$, we subtract the second expansion from the first. Notice that all terms with even powers of $h$ (including the $f(x_0)$ term, which is $h^0$) will cancel out.\n$$\nf(x_0 + h) - f(x_0 - h) = 2h f'(x_0) + 2 \\frac{h^3}{3!} f'''(x_0) + 2 \\frac{h^5}{5!} f^{(5)}(x_0) + O(h^7)\n$$\nNow, we rearrange this equation to solve for $f'(x_0)$:\n$$\n2h f'(x_0) = \\left( f(x_0 + h) - f(x_0 - h) \\right) - \\frac{h^3}{3} f'''(x_0) - \\frac{h^5}{60} f^{(5)}(x_0) - O(h^7)\n$$\n$$\nf'(x_0) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h} - \\frac{h^2}{6} f'''(x_0) - \\frac{h^4}{120} f^{(5)}(x_0) - O(h^6)\n$$\nThis equation reveals two key pieces of information:\n1.  The **second-order central difference formula** is the approximation $D_c(h) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h}$.\n2.  The true derivative $f'(x_0)$ is equal to this approximation plus a series of error terms. The **leading truncation error term** is $- \\frac{h^2}{6} f'''(x_0)$. Because this leading error is proportional to $h^2$, the approximation is said to be \"second-order accurate\". The error scales as $O(h^2)$.\n\n### Part 2: Derivation of the Improved Estimate via Richardson Extrapolation\n\nLet $A$ be the exact value of the derivative, $A = f'(x_0)$, and let $D(h)$ denote the central difference approximation $D_c(h)$. From the previous step, we can express the approximation as a series in powers of $h^2$:\n$$\nD(h) = A + C_1 h^2 + C_2 h^4 + C_3 h^6 + \\dots\n$$\nwhere $C_1 = \\frac{1}{6} f'''(x_0)$, $C_2 = \\frac{1}{120} f^{(5)}(x_0)$, and so on. We have two approximations computed with step sizes $h_1$ and $h_2$, where $h_1 = 2h_2$:\n$$\nD(h_1) = A + C_1 h_1^2 + C_2 h_1^4 + \\dots\n$$\n$$\nD(h_2) = A + C_1 h_2^2 + C_2 h_2^4 + \\dots\n$$\nOur goal is to find a linear combination of $D(h_1)$ and $D(h_2)$ that eliminates the leading error term, which is proportional to $C_1 h^2$. Let the improved estimate be $A_{imp}$.\nWe can substitute $h_1 = 2h_2$ into the first equation:\n$$\nD(h_1) = A + C_1 (2h_2)^2 + C_2 (2h_2)^4 + \\dots = A + 4C_1 h_2^2 + 16C_2 h_2^4 + \\dots\n$$\nWe now have a system of two equations with two \"unknowns\" ($A$ and $C_1$):\n1.  $D(h_1) \\approx A + 4C_1 h_2^2$\n2.  $D(h_2) \\approx A + C_1 h_2^2$\n\nTo eliminate the $C_1$ term, we can multiply the second equation by $4$ and subtract the first equation from it:\n$$\n4D(h_2) - D(h_1) \\approx (4A + 4C_1 h_2^2) - (A + 4C_1 h_2^2)\n$$\n$$\n4D(h_2) - D(h_1) \\approx 3A\n$$\nSolving for $A$, we get the formula for the improved estimate:\n$$\nA \\approx A_{imp} = \\frac{4D(h_2) - D(h_1)}{3}\n$$\nThis is the required combined estimate. Let's analyze its error. Substituting the full series expansions:\n$$\nA_{imp} = \\frac{4(A + C_1 h_2^2 + C_2 h_2^4 + \\dots) - (A + C_1 h_1^2 + C_2 h_1^4 + \\dots)}{3}\n$$\nUsing $h_1 = 2h_2$:\n$$\nA_{imp} = \\frac{4(A + C_1 h_2^2 + C_2 h_2^4 + \\dots) - (A + 4C_1 h_2^2 + 16C_2 h_2^4 + \\dots)}{3}\n$$\n$$\nA_{imp} = \\frac{(4A - A) + (4C_1 - 4C_1)h_2^2 + (4C_2 - 16C_2)h_2^4 + \\dots}{3}\n$$\n$$\nA_{imp} = \\frac{3A - 12C_2 h_2^4 + \\dots}{3} = A - 4C_2 h_2^4 + \\dots\n$$\nThe leading error term is now $-4C_2 h_2^4$, which is proportional to $h^4$. The method is now fourth-order accurate.\n\n### Part 3: Algorithmic Implementation\n\nThe program will implement this derived procedure. For each test case $(h_1, h_2)$:\n1.  Define the function $f(x) = \\ln(1+x)$ and the point $x_0 = 1$. The exact derivative is $f'(1) = 0.5$.\n2.  Define a function to compute the central difference approximation $D(h) = \\frac{f(x_0+h) - f(x_0-h)}{2h}$.\n3.  Calculate the two second-order approximations: $D_1 = D(h_1)$ and $D_2 = D(h_2)$.\n4.  Calculate the improved, fourth-order estimate using the derived formula: $A_{imp} = \\frac{4D_2 - D_1}{3}$.\n5.  Compute the absolute error: $E = |A_{imp} - 0.5|$.\n6.  The errors for all three cases will be collected and printed in the specified format, rounded to $12$ decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies a fourth-order finite difference scheme to estimate\n    the derivative of f(x) = ln(1+x) at x=1.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (moderate steps)\n        (1e-2, 5e-3),\n        # Case B (coarse steps)\n        (1e-1, 5e-2),\n        # Case C (very fine steps)\n        (1e-6, 5e-7),\n    ]\n\n    # Define the point of interest and the function.\n    x0 = 1.0\n    # The function f(x) = ln(1+x)\n    f = lambda x: np.log(1.0 + x)\n\n    # The exact value of the derivative f'(x) = 1/(1+x) at x_0 = 1.\n    exact_derivative = 0.5\n\n    # Helper function for the second-order central difference approximation.\n    def central_difference(h, x_val, func):\n        \"\"\"\n        Computes the second-order central difference approximation for f'(x_val).\n        \"\"\"\n        return (func(x_val + h) - func(x_val - h)) / (2.0 * h)\n\n    # List to store the absolute errors of the improved estimates.\n    absolute_errors = []\n\n    for h1, h2 in test_cases:\n        # 1. Compute the two second-order central difference approximations.\n        # D(h1) is the approximation with the larger step size.\n        d1 = central_difference(h1, x0, f)\n        # D(h2) is the approximation with the smaller step size.\n        d2 = central_difference(h2, x0, f)\n\n        # 2. Compute the improved derivative estimate (Richardson Extrapolation).\n        # This formula was derived to cancel the O(h^2) error term,\n        # resulting in an O(h^4) approximation.\n        improved_estimate = (4.0 * d2 - d1) / 3.0\n\n        # 3. Compute the absolute error of the improved estimate.\n        error = np.abs(improved_estimate - exact_derivative)\n        absolute_errors.append(error)\n\n    # Final print statement in the exact required format.\n    # The errors are rounded to 12 decimal places.\n    print(f\"[{','.join(f'{err:.12f}' for err in absolute_errors)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Numerical models must contend with two primary sources of error: truncation error from the discretization and random error from noisy data. This exercise shifts our focus to the latter, a crucial consideration when analyzing observational data from instruments like photometers. You will analytically derive how uncorrelated white noise in a time series propagates through a high-order finite difference operator, demonstrating the critical trade-off between accurately resolving derivatives and amplifying measurement noise .",
            "id": "3525628",
            "problem": "An observatory monitors the specific luminosity time series $L(t)$ of a variable active galactic nucleus using a space-borne photometer at a uniform cadence $\\Delta t$. Each discrete sample $L_{i} \\equiv L(t_{i})$ is contaminated by instrument-dominated white photon noise that is zero-mean, independent between samples, and has variance $\\sigma^{2}$ per sample. To estimate the instantaneous time derivative $\\frac{dL}{dt}$ at time $t_{i}$, you adopt the $5$-point $4$th-order central finite-difference operator defined by\n$$\n\\left.\\frac{dL}{dt}\\right|_{t_{i}} \\approx \\frac{-L_{i+2} + 8 L_{i+1} - 8 L_{i-1} + L_{i-2}}{12\\,\\Delta t}.\n$$\nStarting from first principles about variances of independent, zero-mean random variables and the linear structure of the finite-difference estimator, quantify how the white noise with variance $\\sigma^{2}$ propagates through this derivative operator and derive a closed-form analytic expression for the variance of the derivative estimate, expressed in terms of $\\sigma$ and $\\Delta t$ only. Provide your final answer as a single simplified symbolic expression in terms of $\\sigma$ and $\\Delta t$; do not include units.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in computational astrophysics and statistics, requiring the application of first principles of error propagation to a standard numerical differentiation formula. All necessary information is provided, and the terminology is precise.\n\nThe task is to determine the variance of the estimate of the time derivative, which we will denote as $\\hat{D}_{i}$. The estimator is given by the $5$-point, $4$th-order central finite-difference operator applied to the noisy measurements $L_j$:\n$$\n\\hat{D}_{i} = \\left.\\frac{dL}{dt}\\right|_{t_{i}} \\approx \\frac{-L_{i+2} + 8 L_{i+1} - 8 L_{i-1} + L_{i-2}}{12\\,\\Delta t}\n$$\nEach measurement $L_j$ is a random variable. It can be modeled as the sum of the true, deterministic specific luminosity $L_{\\text{true}}(t_j)$ and a random noise term $\\epsilon_j$:\n$$\nL_j = L_{\\text{true}}(t_j) + \\epsilon_j\n$$\nThe problem states that the noise $\\epsilon_j$ is zero-mean, independent between samples, and has a constant variance $\\sigma^2$. That is, for any indices $j$ and $k$:\n$$\nE[\\epsilon_j] = 0\n$$\n$$\n\\text{Var}(\\epsilon_j) = E[\\epsilon_j^2] - (E[\\epsilon_j])^2 = E[\\epsilon_j^2] = \\sigma^2\n$$\n$$\n\\text{Cov}(\\epsilon_j, \\epsilon_k) = 0 \\quad \\text{for} \\quad j \\neq k\n$$\nThe variance of a measured sample $L_j$ is therefore:\n$$\n\\text{Var}(L_j) = \\text{Var}(L_{\\text{true}}(t_j) + \\epsilon_j)\n$$\nSince $L_{\\text{true}}(t_j)$ is a deterministic (non-random) quantity, it acts as a constant in the variance calculation. The variance of a random variable plus a constant is equal to the variance of the random variable itself. Thus:\n$$\n\\text{Var}(L_j) = \\text{Var}(\\epsilon_j) = \\sigma^2\n$$\nThe derivative estimator $\\hat{D}_{i}$ is a linear combination of the random variables $L_{i+2}$, $L_{i+1}$, $L_{i-1}$, and $L_{i-2}$. We can write $\\hat{D}_{i}$ as:\n$$\n\\hat{D}_{i} = c_{i+2} L_{i+2} + c_{i+1} L_{i+1} + c_{i-1} L_{i-1} + c_{i-2} L_{i-2}\n$$\nwhere the coefficients are:\n$$\nc_{i+2} = \\frac{-1}{12\\,\\Delta t}, \\quad c_{i+1} = \\frac{8}{12\\,\\Delta t}, \\quad c_{i-1} = \\frac{-8}{12\\,\\Delta t}, \\quad c_{i-2} = \\frac{1}{12\\,\\Delta t}\n$$\nThe fundamental principle for the propagation of variance states that for a linear combination of independent random variables $X_j$ with coefficients $a_j$, $Y = \\sum_j a_j X_j$, the variance of $Y$ is given by:\n$$\n\\text{Var}(Y) = \\sum_j a_j^2 \\text{Var}(X_j)\n$$\nIn our case, the random variables are the measurements $L_j$, which are independent because their noise components $\\epsilon_j$ are independent. The variance of each $L_j$ is $\\sigma^2$. Applying this principle to our estimator $\\hat{D}_{i}$:\n$$\n\\text{Var}(\\hat{D}_{i}) = c_{i+2}^2 \\text{Var}(L_{i+2}) + c_{i+1}^2 \\text{Var}(L_{i+1}) + c_{i-1}^2 \\text{Var}(L_{i-1}) + c_{i-2}^2 \\text{Var}(L_{i-2})\n$$\nSubstituting the coefficients and the variance $\\sigma^2$:\n$$\n\\text{Var}(\\hat{D}_{i}) = \\left(\\frac{-1}{12\\,\\Delta t}\\right)^2 \\sigma^2 + \\left(\\frac{8}{12\\,\\Delta t}\\right)^2 \\sigma^2 + \\left(\\frac{-8}{12\\,\\Delta t}\\right)^2 \\sigma^2 + \\left(\\frac{1}{12\\,\\Delta t}\\right)^2 \\sigma^2\n$$\nWe can factor out the common term $\\frac{\\sigma^2}{(12\\,\\Delta t)^2}$:\n$$\n\\text{Var}(\\hat{D}_{i}) = \\frac{\\sigma^2}{(12\\,\\Delta t)^2} \\left[ (-1)^2 + 8^2 + (-8)^2 + 1^2 \\right]\n$$\nNow, we compute the sum of the squares of the coefficients:\n$$\n(-1)^2 + 8^2 + (-8)^2 + 1^2 = 1 + 64 + 64 + 1 = 130\n$$\nSubstitute this sum back into the expression for the variance:\n$$\n\\text{Var}(\\hat{D}_{i}) = \\frac{\\sigma^2}{(12\\,\\Delta t)^2} (130) = \\frac{130 \\sigma^2}{144 (\\Delta t)^2}\n$$\nFinally, we simplify the numerical fraction $\\frac{130}{144}$. Both the numerator and the denominator are divisible by $2$:\n$$\n\\frac{130}{144} = \\frac{65}{72}\n$$\nThe numbers $65$ and $72$ are coprime, so this fraction is in its simplest form. The final expression for the variance of the derivative estimate is:\n$$\n\\text{Var}(\\hat{D}_{i}) = \\frac{65 \\sigma^2}{72 (\\Delta t)^2}\n$$\nThis expression quantifies how the variance $\\sigma^2$ of the input samples propagates through the specified finite-difference operator, showing that the resulting variance is amplified by a numerical factor and is inversely proportional to the square of the time step $\\Delta t$.",
            "answer": "$$\\boxed{\\frac{65 \\sigma^2}{72 \\Delta t^2}}$$"
        },
        {
            "introduction": "Many fundamental laws of physics, such as the Poisson equation for gravity, are expressed using the Laplacian operator, $\\nabla^2$. When simulating these laws on a grid, it is vital that our discrete operator does not introduce unphysical directional biases; it must be as isotropic as possible. This practice challenges you to derive a fourth-order accurate discrete Laplacian from first principles and then analyze its isotropy error by examining how it acts on a plane wave, a key skill in designing robust and physically faithful simulation codes .",
            "id": "3525603",
            "problem": "In many computational astrophysics applications, such as solving the Newtonian Poisson equation $\\nabla^{2}\\Phi = 4\\pi G \\rho$ for self-gravity on a uniform Cartesian mesh, minimizing directional bias in discrete differential operators is critical for preserving physical symmetry across scales and directions of wave-like perturbations. Consider a three-dimensional ($3$D) uniform Cartesian grid with spacing $h$ in each coordinate direction. Let $u(\\boldsymbol{x})$ be a sufficiently smooth scalar field, sampled at grid points $\\boldsymbol{x}_{i,j,k} = (x_{i},y_{j},z_{k})$.\n\nStarting from Taylor expansions and the core definition of the second derivative as the limit of difference quotients, and without employing pre-tabulated stencil coefficients, derive a fourth-order accurate discrete Laplacian $\\Delta_{h}$ in three dimensions that uses grid points up to two steps away along each coordinate axis and is consistent with the properties that (i) constants map to zero and (ii) the truncation error is $\\mathcal{O}(h^{4})$.\n\nThen, to quantify the directional (isotropy) error of this discrete operator, apply $\\Delta_{h}$ to the plane wave $u(\\boldsymbol{x}) = \\exp\\!\\left(i\\,\\boldsymbol{k}\\cdot\\boldsymbol{x}\\right)$ with $\\boldsymbol{k} = (k_{x},k_{y},k_{z})$, and define the isotropy error as\n$$\n\\epsilon(\\boldsymbol{k};h) \\equiv \\frac{\\Delta_{h}\\,\\exp\\!\\left(i\\,\\boldsymbol{k}\\cdot\\boldsymbol{x}\\right)}{-\\,|\\boldsymbol{k}|^{2}\\,\\exp\\!\\left(i\\,\\boldsymbol{k}\\cdot\\boldsymbol{x}\\right)} - 1,\n$$\nwhere $|\\boldsymbol{k}|^{2} = k_{x}^{2}+k_{y}^{2}+k_{z}^{2}$. Provide $\\epsilon(\\boldsymbol{k};h)$ in a single closed-form analytic expression involving only $k_{x}$, $k_{y}$, $k_{z}$, and $h$. No numerical evaluation is required, and no rounding is necessary. Express your final answer as a single analytical expression.",
            "solution": "The problem requires the derivation of a fourth-order accurate discrete Laplacian operator, $\\Delta_h$, on a three-dimensional uniform Cartesian grid, and subsequently, the calculation of its isotropy error, $\\epsilon(\\boldsymbol{k};h)$. The derivation must be based on Taylor series expansions.\n\nFirst, we derive the operator. The Laplacian is $\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2} + \\frac{\\partial^2}{\\partial z^2}$. Due to the symmetry of the Cartesian grid, we can derive the finite difference approximation for the second partial derivative with respect to a single coordinate, say $x$, and then sum the operators for all three coordinates. Let $D_{xx}$ be the discrete operator approximating $\\frac{\\partial^2}{\\partial x^2}$.\n\nThe problem specifies using grid points up to two steps away from the central point. For a symmetric central difference scheme, we consider a linear combination of function values at points $x_i-2h$, $x_i-h$, $x_i$, $x_i+h$, and $x_i+2h$. Let $u_j = u(x_i+jh)$. The operator can be written as:\n$$ D_{xx} u_i = \\frac{1}{h^2} \\left( c_{-2}u_{-2} + c_{-1}u_{-1} + c_0 u_0 + c_1 u_1 + c_2 u_2 \\right) $$\nFor a symmetric (non-biasing) stencil, we must have $c_{-j} = c_j$. The expression simplifies to:\n$$ D_{xx} u_i = \\frac{1}{h^2} \\left( c_2(u_{-2} + u_2) + c_1(u_{-1} + u_1) + c_0 u_0 \\right) $$\nWe use Taylor series to expand $u(x_i + \\delta x)$ around $x_i$:\n$u(x_i \\pm sh) = u_i \\pm (sh)u'_i + \\frac{(sh)^2}{2!}u''_i \\pm \\frac{(sh)^3}{3!}u'''_i + \\frac{(sh)^4}{4!}u^{(4)}_i + \\frac{(sh)^5}{5!}u^{(5)}_i + \\frac{(sh)^6}{6!}u^{(6)}_i + \\mathcal{O}(h^7)$.\nThe symmetric sums are:\n$$ u_{1} + u_{-1} = 2u_i + h^2 u''_i + \\frac{h^4}{12}u^{(4)}_i + \\frac{h^6}{360}u^{(6)}_i + \\mathcal{O}(h^8) $$\n$$ u_{2} + u_{-2} = 2u_i + 4h^2 u''_i + \\frac{16h^4}{12}u^{(4)}_i + \\frac{64h^6}{360}u^{(6)}_i + \\mathcal{O}(h^8) = 2u_i + 4h^2 u''_i + \\frac{4h^4}{3}u^{(4)}_i + \\frac{8h^6}{45}u^{(6)}_i + \\mathcal{O}(h^8) $$\nSubstituting these into the expression for $D_{xx} u_i$:\n$$ D_{xx} u_i = \\frac{1}{h^2} \\left[ c_2(2u_i + 4h^2 u''_i + \\frac{4h^4}{3}u^{(4)}_i + \\dots) + c_1(2u_i + h^2 u''_i + \\frac{h^4}{12}u^{(4)}_i + \\dots) + c_0 u_i \\right] $$\nGrouping terms by the derivatives of $u$ at $x_i$:\n$$ D_{xx} u_i = \\frac{1}{h^2}(2c_2+2c_1+c_0)u_i + (4c_2+c_1)u''_i + h^2\\left(\\frac{4}{3}c_2+\\frac{1}{12}c_1\\right)u^{(4)}_i + \\mathcal{O}(h^4) $$\nTo make this operator a fourth-order accurate approximation of $u''_i$, we must match the coefficients of the derivatives:\n1.  Coefficient of $u_i$: $\\frac{1}{h^2}(2c_2+2c_1+c_0) = 0 \\implies 2c_2+2c_1+c_0 = 0$. This ensures that a constant field maps to zero.\n2.  Coefficient of $u''_i$: $4c_2+c_1 = 1$. This normalizes the operator to approximate the second derivative.\n3.  Coefficient of $u^{(4)}_i$: $\\frac{4}{3}c_2+\\frac{1}{12}c_1 = 0 \\implies 16c_2+c_1 = 0$. This eliminates the $\\mathcal{O}(h^2)$ error term, yielding $\\mathcal{O}(h^4)$ accuracy.\n\nWe solve this system of three linear equations for $c_0, c_1, c_2$.\nFrom (3), $c_1 = -16c_2$.\nSubstituting into (2): $4c_2 + (-16c_2) = 1 \\implies -12c_2 = 1 \\implies c_2 = -\\frac{1}{12}$.\nThen $c_1 = -16(-\\frac{1}{12}) = \\frac{16}{12} = \\frac{4}{3}$.\nSubstituting into (1): $2(-\\frac{1}{12}) + 2(\\frac{4}{3}) + c_0 = 0 \\implies -\\frac{1}{6} + \\frac{8}{3} + c_0 = 0 \\implies -\\frac{1}{6} + \\frac{16}{6} + c_0 = 0 \\implies \\frac{15}{6} + c_0 = 0 \\implies c_0 = -\\frac{15}{6} = -\\frac{5}{2}$.\n\nThe resulting fourth-order operator for $\\frac{\\partial^2 u}{\\partial x^2}$ is:\n$$ D_{xx} u_i = \\frac{1}{h^2} \\left[ -\\frac{1}{12}(u_{i-2}+u_{i+2}) + \\frac{4}{3}(u_{i-1}+u_{i+1}) - \\frac{5}{2}u_i \\right] $$\n$$ D_{xx} u_{i,j,k} = \\frac{1}{12h^2} \\left[ -u_{i-2,j,k} + 16u_{i-1,j,k} - 30u_{i,j,k} + 16u_{i+1,j,k} - u_{i+2,j,k} \\right] $$\nThe $3$D discrete Laplacian $\\Delta_h$ is the sum of such operators for each coordinate direction:\n$$ \\Delta_h = D_{xx} + D_{yy} + D_{zz} $$\nNext, we apply this operator to the plane wave $u(\\boldsymbol{x}) = \\exp(i\\,\\boldsymbol{k}\\cdot\\boldsymbol{x})$, where $\\boldsymbol{k}=(k_x, k_y, k_z)$ and $\\boldsymbol{x}=(x,y,z)$. The value of the field at a shifted grid point is:\n$$ u(\\boldsymbol{x} + s h \\hat{e}_x) = \\exp(i \\boldsymbol{k} \\cdot (\\boldsymbol{x} + s h \\hat{e}_x)) = \\exp(i(\\boldsymbol{k}\\cdot\\boldsymbol{x} + s k_x h)) = u(\\boldsymbol{x}) \\exp(i s k_x h) $$\nApplying $D_{xx}$ to $u(\\boldsymbol{x})$:\n$$ D_{xx} u(\\boldsymbol{x}) = \\frac{u(\\boldsymbol{x})}{12h^2} \\left[ -\\exp(-i2k_x h) + 16\\exp(-ik_x h) - 30 + 16\\exp(ik_x h) - \\exp(i2k_x h) \\right] $$\nUsing Euler's identity $\\cos\\theta = \\frac{e^{i\\theta} + e^{-i\\theta}}{2}$, we group terms:\n$$ D_{xx} u(\\boldsymbol{x}) = \\frac{u(\\boldsymbol{x})}{12h^2} \\left[ 16(\\exp(ik_x h) + \\exp(-ik_x h)) - (\\exp(i2k_x h) + \\exp(-i2k_x h)) - 30 \\right] $$\n$$ D_{xx} u(\\boldsymbol{x}) = \\frac{u(\\boldsymbol{x})}{12h^2} \\left[ 32\\cos(k_x h) - 2\\cos(2k_x h) - 30 \\right] $$\nUsing the double-angle identity $\\cos(2\\theta) = 2\\cos^2\\theta - 1$:\n$$ D_{xx} u(\\boldsymbol{x}) = \\frac{u(\\boldsymbol{x})}{12h^2} \\left[ 32\\cos(k_x h) - 2(2\\cos^2(k_x h) - 1) - 30 \\right] $$\n$$ D_{xx} u(\\boldsymbol{x}) = \\frac{u(\\boldsymbol{x})}{12h^2} \\left[ -4\\cos^2(k_x h) + 32\\cos(k_x h) - 28 \\right] $$\n$$ \\frac{D_{xx} u(\\boldsymbol{x})}{u(\\boldsymbol{x})} = \\frac{1}{3h^2} \\left[ -\\cos^2(k_x h) + 8\\cos(k_x h) - 7 \\right] $$\nThe action of the full Laplacian $\\Delta_h$ is the sum of the actions of the directional operators:\n$$ \\frac{\\Delta_h u(\\boldsymbol{x})}{u(\\boldsymbol{x})} = \\sum_{\\alpha \\in \\{x,y,z\\}} \\frac{1}{3h^2} \\left[ -\\cos^2(k_\\alpha h) + 8\\cos(k_\\alpha h) - 7 \\right] $$\nThe isotropy error is defined as $\\epsilon(\\boldsymbol{k};h) = \\frac{\\Delta_{h}\\,u(\\boldsymbol{x})}{-\\,|\\boldsymbol{k}|^{2}\\,u(\\boldsymbol{x})} - 1$, where $|\\boldsymbol{k}|^2 = k_x^2+k_y^2+k_z^2$. Substituting the derived expression:\n$$ \\epsilon(\\boldsymbol{k};h) = \\frac{1}{-|\\boldsymbol{k}|^2} \\left( \\sum_{\\alpha \\in \\{x,y,z\\}} \\frac{1}{3h^2} [-\\cos^2(k_\\alpha h) + 8\\cos(k_\\alpha h) - 7] \\right) - 1 $$\n$$ \\epsilon(\\boldsymbol{k};h) = -\\frac{1}{3h^2(k_x^2+k_y^2+k_z^2)} \\sum_{\\alpha \\in \\{x,y,z\\}} (8\\cos(k_\\alpha h) - \\cos^2(k_\\alpha h) - 7) - 1 $$\nExpanding the sum yields the final expression for the isotropy error in terms of $k_x$, $k_y$, $k_z$, and $h$:\n$$ \\epsilon(\\boldsymbol{k};h) = -\\frac{(8\\cos(k_x h) - \\cos^2(k_x h) - 7) + (8\\cos(k_y h) - \\cos^2(k_y h) - 7) + (8\\cos(k_z h) - \\cos^2(k_z h) - 7)}{3h^2(k_x^2+k_y^2+k_z^2)} - 1 $$\nThis is the required single closed-form analytic expression.",
            "answer": "$$ \\boxed{-\\frac{(8\\cos(k_x h) - \\cos^2(k_x h) - 7) + (8\\cos(k_y h) - \\cos^2(k_y h) - 7) + (8\\cos(k_z h) - \\cos^2(k_z h) - 7)}{3h^2(k_x^2+k_y^2+k_z^2)} - 1} $$"
        }
    ]
}