{
    "hands_on_practices": [
        {
            "introduction": "A deep understanding of numerical computation begins with the representation of numbers themselves. Before analyzing complex error propagation, we must be able to determine if a given number can even be stored exactly. This practice  challenges you to apply the first principles of the IEEE 754 standard to determine if a large integer power of ten is exactly representable in binary64, requiring a careful dissection of its binary significand and exponent.",
            "id": "3510985",
            "problem": "In a three-dimensional radiation-hydrodynamics solver for accretion disk simulations, you adopt dimensional normalization that leads to fluxes and opacities whose magnitudes span many orders of magnitude. Consider the dimensionless scalar $x = 10^{20}$ arising as a characteristic source amplitude in a timestep stability estimate. The code uses the Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 (double-precision) format with round-to-nearest, ties-to-even.\n\nStarting from the formal definitions of the IEEE 754 binary64 representation (one sign bit, an $11$-bit biased exponent with bias $1023$, and a $52$-bit fraction with an implicit leading $1$ for normalized numbers) and the structure of normalized numbers $x = m \\times 2^{e}$ with $m \\in [1,2)$ and integer $e$, determine, by first principles, whether $x$ is exactly representable in binary64. Use only foundational facts: $10^{20} = 2^{20} \\times 5^{20}$, the identity $\\log_{2}(10) = \\ln(10)/\\ln(2)$, and the spacing of representable numbers near a given exponent. If $x$ is not exactly representable, identify the two nearest representable binary64 numbers and give their $64$-bit patterns in sign/exponent/fraction fields. If $x$ is exactly representable, give its $64$-bit pattern in sign/exponent/fraction fields. You may additionally provide the hexadecimal $64$-bit word.\n\nFinally, compute the absolute rounding error $| \\operatorname{fl}(x) - x |$ under round-to-nearest, ties-to-even. Express your final answer as a single exact real number with no units.",
            "solution": "The problem asks for an analysis of the representability of the number $x = 10^{20}$ in the IEEE 754 binary64 (double-precision) floating-point format, and for the computation of its absolute rounding error.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n-   The number to be represented is $x = 10^{20}$.\n-   The floating-point format is IEEE 754 binary64 (double-precision).\n-   The rounding mode is round-to-nearest, ties-to-even.\n-   Binary64 representation details: $1$ sign bit, $11$-bit biased exponent with a bias of $1023$, and a $52$-bit fraction with an implicit leading $1$ for normalized numbers.\n-   Normalized number structure: $v = m \\times 2^{e}$, with $m \\in [1, 2)$ and integer $e$.\n-   A provided identity: $10^{20} = 2^{20} \\times 5^{20}$.\n-   A provided relation: $\\log_{2}(10) = \\ln(10)/\\ln(2)$.\n-   The task is to determine if $x$ is exactly representable, find its bit pattern if so (or the patterns of its neighbors if not), and compute the absolute rounding error $| \\operatorname{fl}(x) - x |$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, concerning the fundamental principles of floating-point arithmetic as defined by the IEEE 754 standard. It is well-posed, providing all necessary information to determine a unique solution. The language is objective and precise. The problem is a standard exercise in numerical analysis and computational science, and its astrophysical framing is simply context. The problem is valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\nA number $v$ is exactly representable as a normalized binary64 floating-point number if it can be written in the form:\n$$v = (-1)^{S} \\times m \\times 2^{e}$$\nwhere $S$ is the sign bit ($0$ for positive, $1$ for negative), $e$ is the integer exponent in the range $[-1022, 1023]$, and $m$ is the significand in the range $[1, 2)$. The significand $m$ must have a terminating binary representation of the form $m = (1.f_1 f_2 \\dots f_{52})_2 = 1 + \\sum_{i=1}^{52} f_i 2^{-i}$. This is equivalent to stating that $m$ must be writable as $1 + \\frac{F}{2^{52}}$, where $F$ is an integer in the range $0 \\le F < 2^{52}$.\n\nWe analyze the number $x = 10^{20}$.\n1.  **Sign:** $x$ is positive, so the sign bit $S=0$.\n\n2.  **Exponent:** We express $x$ in binary scientific notation, $x = m \\times 2^e$, where $1 \\le m < 2$. The exponent $e$ is found by taking the base-2 logarithm:\n    $$\\log_2(x) = \\log_2(m \\times 2^e) = \\log_2(m) + e$$\n    Since $1 \\le m < 2$, we have $0 \\le \\log_2(m) < 1$. Therefore, the exponent $e$ is the integer part of $\\log_2(x)$:\n    $$e = \\lfloor \\log_2(x) \\rfloor = \\lfloor \\log_2(10^{20}) \\rfloor = \\lfloor 20 \\log_2(10) \\rfloor$$\n    Using the identity $\\log_2(10) = \\frac{\\ln(10)}{\\ln(2)} \\approx \\frac{2.302585}{0.693147} \\approx 3.321928$, we find:\n    $$e = \\lfloor 20 \\times 3.321928\\dots \\rfloor = \\lfloor 66.43856\\dots \\rfloor = 66$$\n    The exponent $e=66$ is within the allowable range for normalized binary64 numbers, which is $[-1022, 1023]$.\n\n3.  **Significand:** The significand $m$ is given by:\n    $$m = \\frac{x}{2^e} = \\frac{10^{20}}{2^{66}}$$\n    Using the provided identity $10^{20} = 2^{20} \\times 5^{20}$:\n    $$m = \\frac{2^{20} \\times 5^{20}}{2^{66}} = \\frac{5^{20}}{2^{46}}$$\n    For $x$ to be exactly representable, $m$ must have a terminating binary representation with at most $52$ fractional bits. A rational number has a terminating binary representation if and only if its denominator is a power of $2$. Our significand $m$ is of the form $\\frac{K}{2^N}$ with $K=5^{20}$ and $N=46$. It is a dyadic rational and thus has a terminating binary representation. The number of fractional bits required is $N=46$. Since the binary64 format provides $52$ fractional bits, and $46 \\le 52$, the significand can be represented exactly.\n\n    We must also confirm that $m$ is in the range $[1, 2)$:\n    $$1 \\le \\frac{5^{20}}{2^{46}} < 2 \\implies 2^{46} \\le 5^{20} < 2^{47}$$\n    Taking the base-2 logarithm of all parts:\n    $$46 \\le \\log_2(5^{20}) < 47 \\implies 46 \\le 20 \\log_2(5) < 47$$\n    Since $\\log_2(5) = \\log_2(10) - 1 \\approx 2.321928$, we have $20 \\log_2(5) \\approx 46.43856$. The inequality $46 \\le 46.43856 < 47$ is true. Thus, $m$ is a valid normalized significand.\n\nSince all conditions are met, the number $x=10^{20}$ is exactly representable in the binary64 format.\n\nNow we determine the 64-bit pattern.\n-   **Sign bit ($1$ bit):** $S=0$.\n\n-   **Exponent field ($11$ bits):** The true exponent is $e=66$. The biased exponent is $E_b = e + 1023 = 66 + 1023 = 1089$. We convert $1089$ to an $11$-bit binary number:\n    $$1089 = 1024 + 64 + 1 = 2^{10} + 2^6 + 2^0$$\n    The $11$-bit pattern is `10001000001`.\n\n-   **Fraction field ($52$ bits):** The fraction field stores the binary representation of the fractional part of the significand, $f = m-1$.\n    $$f = m-1 = \\frac{5^{20}}{2^{46}} - 1 = \\frac{5^{20} - 2^{46}}{2^{46}}$$\n    The fraction field contains the top $52$ bits of $f$. We can find the integer value $F$ that these bits represent by computing $F = f \\times 2^{52}$:\n    $$F = \\left(\\frac{5^{20} - 2^{46}}{2^{46}}\\right) \\times 2^{52} = (5^{20} - 2^{46}) \\times 2^6$$\n    We calculate the necessary integer values:\n    $5^{20} = 95367431640625$\n    $2^{46} = 70368744177664$\n    $2^6 = 64$\n    $$F = (95367431640625 - 70368744177664) \\times 64 = 24998687462961 \\times 64 = 1599915997629504$$\n    This integer $F$ must be represented as a $52$-bit binary number. In hexadecimal, $F = 5\\text{B}0\\text{B}2\\text{E}3\\text{D}84800_{16}$. The $52$-bit binary pattern for the fraction field is:\n    `0101101100001011001011100011110110000100100000000000`.\n\nThe bit pattern for $10^{20}$ is:\n-   Sign field: `0`\n-   Exponent field: `10001000001`\n-   Fraction field: `0101101100001011001011100011110110000100100000000000`\n\nThe full $64$-bit word can be represented in hexadecimal by concatenating the fields:\n-   `0` (sign) + `10001000001` (exponent) + `0101...` (fraction)\n-   `0100` `0100` `0001` `0101` `1011` `0000` `1011` `0010` `1110` `0011` `1101` `1000` `0100` `1000` `0000` `0000`\n-   This corresponds to the hexadecimal word: `4415B0B2E3D84800`.\n\nFinally, we compute the absolute rounding error $| \\operatorname{fl}(x) - x |$. Since we have demonstrated that $x=10^{20}$ is exactly representable, its floating-point representation, $\\operatorname{fl}(x)$, is equal to $x$ itself.\n$$\\operatorname{fl}(10^{20}) = 10^{20}$$\nTherefore, the absolute rounding error is:\n$$| \\operatorname{fl}(10^{20}) - 10^{20} | = | 10^{20} - 10^{20} | = 0$$\nThe rounding error is exactly zero.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "One of the most common pitfalls in numerical computing is assuming that floating-point arithmetic obeys the familiar axioms of real numbers, such as the associative law of addition. This exercise  provides concrete examples where $(a+b)+c \\ne a+(b+c)$ and introduces the \"unit in the last place\" (ULP) distance, a rigorous and powerful tool for quantifying the exact, bit-level difference between two floating-point values.",
            "id": "3511001",
            "problem": "You will implement and evaluate floating-point non-associativity in the Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 format. The foundational base for this exercise is the finite-precision model of floating-point arithmetic: real-number operations are rounded to the nearest representable value in a fixed-format encoding, which implies that algebraic properties such as associativity need not hold. You must reason from the binary64 representation and the definition of rounding to nearest, ties to even.\n\nTask specification:\n1. Let $(a,b,c)$ be real numbers represented in IEEE 754 binary64. Compute the two parenthesized sums $(a+b)+c$ and $a+(b+c)$ using binary64 arithmetic, and determine whether they differ.\n2. Define the exact unit in the last place distance between two binary64 results $x$ and $y$ as the integer count of distinct representable binary64 numbers between $x$ and $y$ in the total order of finite binary64 values (including subnormals), plus zero when $x=y$. Formally, if $r(x)$ denotes the rank of $x$ in the total ordering of finite binary64 numbers, then the distance is $\\Delta_{\\mathrm{ulp}}(x,y) = \\lvert r(x) - r(y) \\rvert$. You must compute $\\Delta_{\\mathrm{ulp}}(x,y)$ exactly by operating on the $64$-bit encoding of $x$ and $y$ to obtain $r(\\cdot)$.\n3. All arithmetic must be performed in IEEE 754 binary64. No physical units are involved; the outputs are unitless integers.\n\nTest suite:\n- Case 1 (happy path, large cancellation): $(a,b,c) = (1, 10^{16}, -10^{16})$.\n- Case 2 (boundary condition, subnormal sensitivity): $(a,b,c) = (2^{-1074}, 1, -1)$, where $2^{-1074}$ is the smallest positive subnormal binary64 number.\n- Case 3 (extreme magnitudes without overflow): $(a,b,c) = (10^{-308}, 10^{308}, -10^{308})$.\n\nFor each case, compute:\n- The two results $x_{\\mathrm{L}} = (a+b)+c$ and $x_{\\mathrm{R}} = a+(b+c)$ using binary64 arithmetic.\n- The exact integer $\\Delta_{\\mathrm{ulp}}(x_{\\mathrm{L}}, x_{\\mathrm{R}})$ as defined above.\n\nFinal output format:\nYour program should produce a single line of output containing the three $\\Delta_{\\mathrm{ulp}}$ values for the cases listed above, in order, as a comma-separated list enclosed in square brackets. For example, the output must have the form $\\texttt{[d_1,d_2,d_3]}$ where each $d_i$ is an integer.",
            "solution": "The problem requires an analysis of the non-associativity of floating-point addition under the IEEE 754 binary64 standard. We must compute the difference between the expressions $(a+b)+c$ and $a+(b+c)$ for three specific sets of inputs $(a, b, c)$. The difference is to be quantified by the exact Unit in the Last Place (ULP) distance, which is formally defined as the absolute difference of the integer ranks of the two floating-point results in the total ordering of finite binary64 numbers.\n\nThe fundamental principle at play is that floating-point arithmetic is an approximation of real arithmetic. Operations are subject to rounding errors because the result of an operation on two representable numbers may not itself be exactly representable. In binary64, a number is represented with a sign, an $11$-bit exponent, and a $52$-bit fraction (mantissa), affording approximately $15$ to $17$ decimal digits of precision. When numbers of vastly different magnitudes are added, information from the smaller number is often lost, a phenomenon known as absorption or swamping. This loss of information is the source of non-associativity.\n\nTo quantify the difference between two binary64 numbers $x$ and $y$, we use the ULP distance, $\\Delta_{\\mathrm{ulp}}(x,y) = \\lvert r(x) - r(y) \\rvert$. Here, $r(f)$ is a function that maps a floating-point number $f$ to a unique integer rank that is monotonic with respect to the total ordering of floats defined in IEEE 754. In this ordering, numbers are sorted from $-\\infty$ to $+\\infty$, and notably, $-0 < +0$.\n\nTo implement the rank function $r(f)$, we operate on the $64$-bit integer representation of the float $f$, which we denote as $u$. This representation can be obtained in Python using `struct.pack` and `struct.unpack`. The $64$-bit pattern $u$ is composed of a sign bit $S$ (bit $63$), an exponent $E$ (bits $62$-$52$), and a mantissa $M$ (bits $51$-$0$).\n\nThe integer rank $r(u)$ must be constructed to be monotonic over the entire range of finite floats.\nFor positive numbers (including $+0$), where the sign bit is $0$, the unsigned integer representation $u$ is already ordered correctly. Thus, for a float $f \\ge 0$, its $64$-bit representation $u$ has $S=0$, and we can define its rank as:\n$r(u) = u$, for $u < 2^{63}$.\nThis gives $r(+0.0) = 0$, $r(\\text{smallest positive subnormal}) = 1$, and so on.\n\nFor negative numbers (including $-0$), where the sign bit is $1$, the standard integer representation is ordered inversely to the floating-point value (e.g., the bit pattern for $-1.0$ is larger than for $-2.0$). To create a single monotonic sequence, we map negative floats to negative integer ranks. The float $-0.0$ must have a rank just below $+0.0$, which is $-1$. The next float, the smallest-magnitude negative number (largest value), must have rank $-2$, and so on. The bit pattern for $-0.0$ is $u=2^{63}$. For any negative float with representation $u$, its rank can be defined as:\n$r(u) = -((u - 2^{63}) + 1)$, for $u \\ge 2^{63}$.\nFor $u=2^{63}$ ($-0.0$), this gives $r = -((2^{63}-2^{63})+1) = -1$. For $u=2^{63}+1$ (smallest-magnitude negative subnormal), this gives $r = -(((2^{63}+1)-2^{63})+1) = -2$. This mapping correctly establishes the desired total order.\n\nWith this rank function, we can analyze the three test cases.\n\nCase 1: $(a,b,c) = (1, 10^{16}, -10^{16})$\nLet $a=1.0$, $b=10^{16}$, $c=-10^{16}$.\n$x_{\\mathrm{L}} = (a+b)+c$. The first sum is $1.0 + 10^{16}$. The magnitude of $10^{16}$ is such that its ULP is greater than $1.0$. Specifically, $\\text{ulp}(10^{16}) \\approx 10^{16} \\cdot 2^{-52} \\approx 2.22$. The addition of $1.0$ is lost due to rounding.\nThus, $\\operatorname{fl}(a+b) = \\operatorname{fl}(1.0+10^{16}) = 10^{16}$.\nThen, $x_{\\mathrm{L}} = \\operatorname{fl}(10^{16} + (-10^{16})) = 0.0$.\n$x_{\\mathrm{R}} = a+(b+c)$. The sum in parentheses is $b+c = 10^{16} + (-10^{16}) = 0.0$. This cancellation is exact.\nThen, $x_{\\mathrm{R}} = \\operatorname{fl}(1.0 + 0.0) = 1.0$.\nThe ULP distance is $\\Delta_{\\mathrm{ulp}}(0.0, 1.0) = \\lvert r(0.0) - r(1.0) \\rvert$.\n$r(0.0) = 0$. The binary64 representation of $1.0$ is $u = \\text{0x3FF0000000000000}$.\n$r(1.0) = \\text{0x3FF0000000000000} = 4607182418800017408$.\n$\\Delta_{\\mathrm{ulp},1} = \\lvert 0 - 4607182418800017408 \\rvert = 4607182418800017408$.\n\nCase 2: $(a,b,c) = (2^{-1074}, 1, -1)$\nLet $a=2^{-1074}$, $b=1.0$, $c=-1.0$. The value $a$ is the smallest positive subnormal number.\n$x_{\\mathrm{L}} = (a+b)+c$. The first sum is $2^{-1074} + 1.0$. The magnitude of $a$ is vastly smaller than the ULP of $1.0$ ($\\text{ulp}(1.0) = 2^{-52}$). The addition of $a$ is lost to rounding.\nThus, $\\operatorname{fl}(a+b) = \\operatorname{fl}(2^{-1074}+1.0) = 1.0$.\nThen, $x_{\\mathrm{L}} = \\operatorname{fl}(1.0 + (-1.0)) = 0.0$.\n$x_{\\mathrm{R}} = a+(b+c)$. The sum in parentheses is $b+c = 1.0 + (-1.0) = 0.0$.\nThen, $x_{\\mathrm{R}} = \\operatorname{fl}(2^{-1074} + 0.0) = 2^{-1074}$.\nThe ULP distance is $\\Delta_{\\mathrm{ulp}}(0.0, 2^{-1074}) = \\lvert r(0.0) - r(2^{-1074}) \\rvert$.\n$r(0.0) = 0$. The value $2^{-1074}$ is the smallest positive subnormal number, whose representation is $u=\\text{0x0000000000000001}$.\n$r(2^{-1074}) = 1$.\n$\\Delta_{\\mathrm{ulp},2} = \\lvert 0 - 1 \\rvert = 1$. This is expected, as they are adjacent in the total ordering.\n\nCase 3: $(a,b,c) = (10^{-308}, 10^{308}, -10^{308})$\nLet $a=10^{-308}$, $b=10^{308}$, $c=-10^{308}$.\n$x_{\\mathrm{L}} = (a+b)+c$. The first sum is $10^{-308} + 10^{308}$. The value of $a$ is minuscule compared to the ULP of $b$. The addition is lost to absorption.\nThus, $\\operatorname{fl}(a+b) = \\operatorname{fl}(10^{-308} + 10^{308}) = 10^{308}$.\nThen, $x_{\\mathrm{L}} = \\operatorname{fl}(10^{308} + (-10^{308})) = 0.0$.\n$x_{\\mathrm{R}} = a+(b+c)$. The sum in parentheses is $b+c = 10^{308} + (-10^{308}) = 0.0$.\nThen, $x_{\\mathrm{R}} = \\operatorname{fl}(10^{-308} + 0.0) = 10^{-308}$.\nThe ULP distance is $\\Delta_{\\mathrm{ulp}}(0.0, 10^{-308}) = \\lvert r(0.0) - r(10^{-308}) \\rvert$.\n$r(0.0) = 0$. The value $10^{-308}$ is a subnormal number, as it is less than the smallest normal number ($2^{-1022} \\approx 2.225 \\times 10^{-308}$). Its value is $k \\cdot 2^{-1074}$ for some integer $k$. This integer $k$ is its bit representation $u$, which is also its rank.\nThe value of $k$ is found by rounding $10^{-308} / 2^{-1074}$. This is approximately $\\text{round}(2.0240225...\\times 10^{15})$, which gives the integer rank $k=2024022533832796$.\nThe representation is $u = 2024022533832796$. Since the number is positive, its rank is $u$.\n$r(10^{-308}) = 2024022533832796$.\n$\\Delta_{\\mathrm{ulp},3} = \\lvert 0 - 2024022533832796 \\rvert = 2024022533832796$.\n\nThe final results are the ULP distances for the three cases, computed by implementing this logic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport struct\n\ndef get_rank(f: np.float64) -> int:\n    \"\"\"\n    Computes a monotonic integer rank for a binary64 floating-point number.\n    This rank corresponds to the position of the number in the total ordering\n    of finite binary64 values, from -infinity to +infinity.\n    \"\"\"\n    if not isinstance(f, np.float64):\n        f = np.float64(f)\n    \n    # Pack the float into 8 bytes (64 bits) and then unpack it as an\n    # unsigned 64-bit integer. This gives the bit-level representation.\n    # 'd' is for double-precision float, 'Q' is for unsigned long long.\n    try:\n        u = struct.unpack('Q', struct.pack('d', f))[0]\n    except (struct.error, OverflowError):\n        # Handle cases where f might be Inf or NaN, though not in test cases\n        # This is for robustness; problem statement guarantees finite values.\n        if np.isinf(f):\n            return (1 << 63) - 1 if f > 0 else -((1 << 63) - 1)\n        # For NaN, rank is undefined, but for completeness:\n        return 0\n\n    # The most significant bit (bit 63) is the sign bit.\n    sign_bit_mask = 1 << 63\n    \n    if (u & sign_bit_mask) == 0:\n        # For positive numbers (including +0.0), the integer representation `u`\n        # is already ordered monotonically with the float value.\n        # r(+0.0) = 0, r(smallest_positive_subnormal) = 1, etc.\n        return int(u)\n    else:\n        # For negative numbers (including -0.0), the integer representation's\n        # order is inverse to the float value's order. To create a single\n        # monotonic sequence across all floats, we map them to negative integers.\n        # The rank of -0.0 (u = 0x800...0) should be -1, just below +0.0 (rank=0).\n        # The rank of -min_subnormal (u = 0x800...1) should be -2, and so on.\n        # This mapping is achieved by r(u) = -( (u - 2^63) + 1 ).\n        return -int((u - sign_bit_mask) + 1)\n\ndef compute_ulp_distance(x: np.float64, y: np.float64) -> int:\n    \"\"\"\n    Computes the exact ULP distance between two binary64 floats.\n    The distance is defined as the absolute difference of their integer ranks.\n    \"\"\"\n    rank_x = get_rank(x)\n    rank_y = get_rank(y)\n    return abs(rank_x - rank_y)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Large cancellation\n        (1.0, 1.0e16, -1.0e16),\n        # Case 2: Subnormal sensitivity\n        (2.0**(-1074), 1.0, -1.0),\n        # Case 3: Extreme magnitudes\n        (1.0e-308, 1.0e308, -1.0e308),\n    ]\n\n    results = []\n    for case in test_cases:\n        # Explicitly cast to numpy's float64 to ensure IEEE 754 binary64 arithmetic.\n        a, b, c = (np.float64(v) for v in case)\n        \n        # Compute x_L = (a+b)+c using binary64 arithmetic.\n        x_L = (a + b) + c\n        \n        # Compute x_R = a+(b+c) using binary64 arithmetic.\n        x_R = a + (b + c)\n        \n        # Compute the exact ULP distance between the two results.\n        delta = compute_ulp_distance(x_L, x_R)\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The theoretical limits of machine precision have tangible consequences for the stability and accuracy of astrophysical simulations. This practice  guides you through writing code to numerically measure your system's fundamental precision constants, such as machine epsilon. You will then use this knowledge to investigate and verify the error bounds of catastrophic cancellation, a critical source of numerical error that arises when subtracting nearly equal numbers, as often happens when calculating differences in orbital energies.",
            "id": "3510974",
            "problem": "In computational astrophysics, numerical simulations depend on the behavior of floating-point arithmetic. The Institute of Electrical and Electronics Engineers (IEEE) 754 standard defines a binary floating-point system characterized by a base $\\,\\beta\\,$, a precision $\\,p\\,$ (number of significant bits in the significand, including the implicit leading bit for normalized numbers), and a rounding rule. When rounding to nearest with ties to even, the spacing between consecutive normalized floating-point numbers near $\\,1\\,$ is $\\,\\varepsilon_{\\mathrm{mach}} = \\beta^{1-p}\\,$, and the unit roundoff $\\,u\\,$ (the bound appearing in the standard rounding model $\\,\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)\\,$ with $\\,|\\delta| \\le u\\,$ for basic operations $\\,\\circ \\in \\{+, -, \\times, \\div\\}$) is $\\,u = \\frac{1}{2}\\beta^{1-p}\\,$.\n\nYou are to implement a robust numerical algorithm to compute, on the execution platform, both the unit roundoff $\\,u\\,$ and the machine epsilon $\\,\\varepsilon_{\\mathrm{mach}}\\,$ for two IEEE 754 binary formats:\n- Binary64 (double precision), with $\\,\\beta = 2\\,$ and $\\,p = 53\\,$.\n- Binary32 (single precision), with $\\,\\beta = 2\\,$ and $\\,p = 24\\,$.\n\nThen, compare your computed values to the theoretical values derived from $\\,\\beta\\,$ and $\\,p\\,$, and interpret any discrepancies in terms of definitions and rounding behavior. Finally, assess the impact of machine precision on a numerically sensitive astrophysical computation involving cancellation.\n\nDefinitions to use:\n- Machine epsilon $\\,\\varepsilon_{\\mathrm{mach}}\\,$ is the gap $\\,\\operatorname{nextafter}(1, 2) - 1\\,$, where $\\,\\operatorname{nextafter}\\,$ returns the next representable floating-point number in the direction of $\\,2\\,$.\n- Unit roundoff $\\,u\\,$ is the smallest positive $\\,\\eta\\,$ such that $\\,1 + \\eta\\,$ rounds to a floating-point number strictly larger than $\\,1\\,$. Under rounding to nearest ties to even, this threshold equals $\\,\\varepsilon_{\\mathrm{mach}}/2\\,$.\n\nAlgorithmic requirements:\n1. Implement a robust loop to compute $\\,u\\,$ for a given floating-point type: start with $\\,\\eta = 1\\,$ and repeatedly halve $\\,\\eta\\,$ until $\\,1 + \\eta = 1\\,$; the previous $\\,\\eta\\,$ value (just before equality holds) is $\\,u\\,$. This method must use the target data type, not higher precision intermediates.\n2. Compute $\\,\\varepsilon_{\\mathrm{mach}}\\,$ using $\\,\\operatorname{nextafter}\\,$ at $\\,1\\,$ in the target type to avoid dependence on arithmetic sequences of halving.\n3. Compute theoretical values using $\\,\\varepsilon_{\\mathrm{th}} = \\beta^{1-p}\\,$ and $\\,u_{\\mathrm{th}} = \\frac{1}{2}\\beta^{1-p} = \\beta^{-p}\\,$ for the specified $\\,(\\beta, p)\\,$.\n\nAstrophysical cancellation assessment:\nConsider the specific orbital energy per unit mass in Newtonian gravity for circular orbits, $\\,E(r) = -\\frac{G M}{2 r}\\,$. For the purposes of this numerical assessment, set $\\,G M = 1\\,$ so that $\\,E(r) = -\\frac{1}{2 r}\\,$, which is dimensionless in these normalized units. The difference in specific energy between radii $\\,r_1\\,$ and $\\,r_2\\,$ is\n$$\n\\Delta E = E(r_2) - E(r_1) = -\\frac{1}{2}\\left(\\frac{1}{r_2} - \\frac{1}{r_1}\\right).\n$$\nComputing $\\,\\Delta E\\,$ naively with floating-point arithmetic incurs cancellation when $\\,r_1 \\approx r_2\\,$. Under the standard first-order rounding model, the relative error in computing $\\,\\Delta E\\,$ with two divisions and one subtraction satisfies the bound\n$$\n\\frac{\\left|\\Delta E_{\\mathrm{comp}} - \\Delta E_{\\mathrm{true}}\\right|}{\\left|\\Delta E_{\\mathrm{true}}\\right|} \\;\\lesssim\\; \\left(\\frac{\\left|\\frac{1}{r_1}\\right| + \\left|\\frac{1}{r_2}\\right|}{\\left|\\frac{1}{r_2} - \\frac{1}{r_1}\\right|} + 1\\right) u,\n$$\nignoring second-order terms in $\\,u\\,$. You must verify this inequality numerically up to a factor of $\\,2\\,$ for the double-precision (binary64) format.\n\nTest suite:\n- Test case 1 (binary64): compute the absolute errors $\\,|u_{\\mathrm{num}} - u_{\\mathrm{th}}|\\,$ and $\\,|\\varepsilon_{\\mathrm{num}} - \\varepsilon_{\\mathrm{th}}|\\,$.\n- Test case 2 (binary32): compute the absolute errors $\\,|u_{\\mathrm{num}} - u_{\\mathrm{th}}|\\,$ and $\\,|\\varepsilon_{\\mathrm{num}} - \\varepsilon_{\\mathrm{th}}|\\,$.\n- Test case 3 (cancellation, moderate): $\\,r_1 = 10^8\\,$, $\\,r_2 = 10^8 + 1\\,$, compute whether the measured relative error in $\\,\\Delta E\\,$ is less than or equal to $\\,2\\,$ times the bound above, using binary64 arithmetic for $\\,\\Delta E_{\\mathrm{comp}}\\,$ and a high-precision reference for $\\,\\Delta E_{\\mathrm{true}}\\,$.\n- Test case 4 (cancellation, extreme): $\\,r_1 = 1\\,$, $\\,r_2 = \\operatorname{nextafter}(1, 2)\\,$, perform the same bound check as in Test case 3.\n\nHigh-precision reference:\nUse a high-precision arithmetic from the programming language's standard library to compute $\\,\\Delta E_{\\mathrm{true}}\\,$ so that any floating-point roundoff error is negligible compared to the binary64 errors being assessed.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\;|u_{64,\\mathrm{num}} - u_{64,\\mathrm{th}}|,\\;|\\varepsilon_{64,\\mathrm{num}} - \\varepsilon_{64,\\mathrm{th}}|,\\;|u_{32,\\mathrm{num}} - u_{32,\\mathrm{th}}|,\\;|\\varepsilon_{32,\\mathrm{num}} - \\varepsilon_{32,\\mathrm{th}}|,\\;\\text{bound\\_ok\\_case3},\\;\\text{bound\\_ok\\_case4}\\;],\n$$\nwhere the first four entries are floats and the last two entries are booleans. No physical units are involved because of the normalization $\\,G M = 1\\,$; all quantities are dimensionless. The program must be self-contained and require no input.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of IEEE 754 floating-point arithmetic and Newtonian mechanics, is well-posed with all necessary information provided, and is objective in its formulation. The core of the problem involves implementing and verifying standard definitions and error bounds related to machine precision, a fundamental topic in computational science. The apparent discrepancy in the provided algorithm for the unit roundoff, $u$, is a deliberate feature designed to test the understanding of subtle but critical details of floating-point behavior, specifically the rounding-to-nearest-ties-to-even rule.\n\nThe solution proceeds in two main parts. First, we compute the machine precision constants, unit roundoff $u$ and machine epsilon $\\varepsilon_{\\mathrm{mach}}$, for both binary64 and binary32 formats, and compare the numerically obtained values with their theoretical counterparts. Second, we assess the impact of catastrophic cancellation on a practical astrophysical calculation using binary64 arithmetic and verify a standard first-order error bound.\n\n**Part 1: Computation of Machine Precision Constants**\n\nThe problem specifies two formats: binary64 (double precision) with base $\\beta=2$ and precision $p=53$, and binary32 (single precision) with $\\beta=2$ and $p=24$.\n\nThe theoretical values for machine epsilon ($\\varepsilon_{\\mathrm{th}}$) and unit roundoff ($u_{\\mathrm{th}}$) are derived from these parameters:\n- Machine epsilon: $\\varepsilon_{\\mathrm{th}} = \\beta^{1-p}$.\n- Unit roundoff: $u_{\\mathrm{th}} = \\frac{1}{2}\\beta^{1-p} = \\beta^{-p}$ for $\\beta=2$.\n\nFor binary64, this gives:\n- $\\varepsilon_{64,\\mathrm{th}} = 2^{1-53} = 2^{-52} \\approx 2.22 \\times 10^{-16}$.\n- $u_{64,\\mathrm{th}} = 2^{-53} \\approx 1.11 \\times 10^{-16}$.\n\nFor binary32, this gives:\n- $\\varepsilon_{32,\\mathrm{th}} = 2^{1-24} = 2^{-23} \\approx 1.19 \\times 10^{-7}$.\n- $u_{32,\\mathrm{th}} = 2^{-24} \\approx 5.96 \\times 10^{-8}$.\n\nThe numerical computation of these values follows two distinct, specified algorithms.\n\nFirst, the numerical machine epsilon, $\\varepsilon_{\\mathrm{num}}$, is computed using the function $\\operatorname{nextafter}(x, y)$, which gives the next representable floating-point number after $x$ in the direction of $y$. By definition, for normalized numbers around $1$, $\\varepsilon_{\\mathrm{mach}}$ is the distance from $1$ to the next larger representable number. Thus, we compute $\\varepsilon_{\\mathrm{num}} = \\operatorname{nextafter}(1.0, 2.0) - 1.0$. This computation is performed for both `numpy.float64` and `numpy.float32` types. We expect $\\varepsilon_{\\mathrm{num}}$ to be exactly equal to $\\varepsilon_{\\mathrm{th}}$ since this is the definition of the gap.\n\nSecond, the numerical unit roundoff, $u_{\\mathrm{num}}$, is computed via a specified loop algorithm: start with $\\eta = 1$ and repeatedly halve it until the floating-point addition $1+\\eta$ evaluates to $1$. The value of $\\eta$ from the iteration *before* this equality occurs is taken as $u_{\\mathrm{num}}$. A critical analysis of this algorithm under the IEEE 754 \"round-to-nearest, ties-to-even\" rule reveals a subtle but important detail. The sum $1.0 + \\eta$ is a rounding tie when $\\eta$ is exactly half the gap between two consecutive floating-point numbers. For the number $1.0$, the next representable value is $1.0 + \\varepsilon_{\\mathrm{mach}}$. The midpoint is $1.0 + \\varepsilon_{\\mathrm{mach}}/2 = 1.0 + u_{\\mathrm{th}}$. Since the significand of $1.0$ (which is $1.0...0 \\times 2^0$) is even, the tie-breaking rule dictates that $1.0 + u_{\\mathrm{th}}$ rounds down to $1.0$. Consequently, the loop condition $1.0 + \\eta > 1.0$ will be false when $\\eta$ has been halved to the value of $u_{\\mathrm{th}}$. The last value of $\\eta$ for which the condition was true was $2 \\cdot u_{\\mathrm{th}} = \\varepsilon_{\\mathrm{mach}}$. The algorithm, as stated, thus returns $\\varepsilon_{\\mathrm{mach}}$. Therefore, we predict that the numerically computed value, denoted $u_{\\mathrm{num}}$, will be equal to $\\varepsilon_{\\mathrm{mach}}$, not $u_{\\mathrm{th}}$. The absolute error $|u_{\\mathrm{num}} - u_{\\mathrm{th}}|$ will therefore be $| \\varepsilon_{\\mathrm{mach}} - u_{\\mathrm{th}} | = | 2u_{\\mathrm{th}} - u_{\\mathrm{th}} | = u_{\\mathrm{th}} \\neq 0$. This discrepancy is not a flaw in the computation but an illustration of the definitional ambiguity and the precise behavior of floating-point rounding.\n\n**Part 2: Astrophysical Cancellation Assessment**\n\nThe second part of the problem examines catastrophic cancellation in the calculation of the change in specific orbital energy, $\\Delta E = E(r_2) - E(r_1)$, where $E(r) = -1/(2r)$ in normalized units. The naive computational formula is $\\Delta E_{\\mathrm{comp}} = -0.5 \\times (1/r_2 - 1/r_1)$. When $r_1 \\approx r_2$, the terms $1/r_1$ and $1/r_2$ are nearly equal, and their subtraction leads to a significant loss of relative precision.\n\nThe problem provides a first-order relative error bound:\n$$ \\frac{|\\Delta E_{\\mathrm{comp}} - \\Delta E_{\\mathrm{true}}|}{|\\Delta E_{\\mathrm{true}}|} \\lesssim \\left(\\frac{|1/r_1| + |1/r_2|}{|1/r_2 - 1/r_1|} + 1\\right) u $$\nwhere $u$ is the unit roundoff, which we take as $u_{64,\\mathrm{th}} = 2^{-53}$ for binary64 arithmetic. The term multiplying $u$ is the condition number of the subtraction. For $r_1, r_2 > 0$, this simplifies to $C(r_1, r_2) = \\frac{r_1+r_2}{|r_2-r_1|} + 1$. The bound is then approximately $C(r_1, r_2) \\cdot u$. We are asked to verify this bound numerically up to a factor of $2$.\n\nTo perform this verification, we need three quantities:\n1.  $\\Delta E_{\\mathrm{comp}}$: Computed using the naive formula with binary64 (`numpy.float64`) arithmetic.\n2.  $\\Delta E_{\\mathrm{true}}$: A high-precision reference value. This is computed using Python's `decimal` module with a precision of $100$ digits to ensure its own roundoff error is negligible.\n3.  The error bound: Computed using the simplified condition number and $u_{64,\\mathrm{th}}$.\n\nWe perform this check for two test cases in binary64:\n- Case 3 (moderate cancellation): $r_1 = 10^8$, $r_2 = 10^8 + 1$. Here, $|r_2-r_1|=1$, so the condition number is large, $C \\approx 2 \\times 10^8$.\n- Case 4 (extreme cancellation): $r_1 = 1$, $r_2 = \\operatorname{nextafter}(1, 2) = 1 + \\varepsilon_{64,\\mathrm{mach}}$. Here, $|r_2-r_1| = \\varepsilon_{64,\\mathrm{mach}}$, the smallest possible gap. The condition number is maximal, $C \\approx 2 / \\varepsilon_{64,\\mathrm{mach}} \\approx 0.9 \\times 10^{16}$.\n\nFor each case, we compute the measured relative error $|\\Delta E_{\\mathrm{comp}} - \\Delta E_{\\mathrm{true}}|/|\\Delta E_{\\mathrm{true}}|$ and check if it is less than or equal to $2$ times the theoretical bound.\n\nThe final output will consist of the four absolute error values from Part 1 and two boolean results from Part 2, formatted as a list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing machine precision constants and assessing\n    numerical cancellation in an astrophysical context.\n    \"\"\"\n\n    # Part 1: Computation of Machine Precision Constants\n\n    def get_constant_from_loop(dtype):\n        \"\"\"\n        Implements the loop-based algorithm described in the problem to find\n        the smallest power of 2, eta, such that 1.0 + eta > 1.0. As explained\n        in the solution, this algorithm computes machine epsilon, not unit roundoff.\n        \n        The problem asks for \"the previous eta value (just before equality holds)\".\n        \"\"\"\n        one = dtype(1.0)\n        two = dtype(2.0)\n        eta = dtype(1.0)\n        eta_prev = eta\n        \n        while True:\n            # Under round-to-nearest, ties-to-even, 1.0 + u rounds to 1.0.\n            # So, the loop terminates when eta becomes u or smaller.\n            if one + eta == one:\n                # The previous eta was the last value for which 1.0+eta > 1.0.\n                return eta_prev\n            eta_prev = eta\n            eta = eta / two\n\n    # Constants for binary64 (double precision)\n    p64 = 53\n    beta = 2\n    # Theoretical unit roundoff u = 1/2 * beta^(1-p) = beta^(-p) for beta=2\n    u_th_64 = np.float64(beta**(-p64))\n    # Theoretical machine epsilon eps = beta^(1-p)\n    eps_th_64 = np.float64(beta**(1 - p64))\n    \n    # Numerical computation for binary64\n    u_num_64 = get_constant_from_loop(np.float64) # This will be eps_mach\n    eps_num_64 = np.nextafter(np.float64(1), np.float64(2)) - np.float64(1)\n\n    abs_err_u64 = abs(u_num_64 - u_th_64)\n    abs_err_eps64 = abs(eps_num_64 - eps_th_64)\n\n    # Constants for binary32 (single precision)\n    p32 = 24\n    u_th_32 = np.float32(beta**(-p32))\n    eps_th_32 = np.float32(beta**(1 - p32))\n\n    # Numerical computation for binary32\n    u_num_32 = get_constant_from_loop(np.float32) # This will be eps_mach\n    eps_num_32 = np.nextafter(np.float32(1), np.float32(2)) - np.float32(1)\n    \n    abs_err_u32 = abs(u_num_32 - u_th_32)\n    abs_err_eps32 = abs(eps_num_32 - eps_th_32)\n\n    # Part 2: Astrophysical Cancellation Assessment\n\n    # Set precision for high-precision reference calculation\n    getcontext().prec = 100\n\n    def compute_delta_E_naive(r1, r2, dtype):\n        \"\"\"Computes Delta_E using the naive, cancellation-prone formula.\"\"\"\n        r1_f = dtype(r1)\n        r2_f = dtype(r2)\n        return -dtype(0.5) * (dtype(1.0) / r2_f - dtype(1.0) / r1_f)\n\n    def compute_delta_E_true(r1_str, r2_str):\n        \"\"\"Computes Delta_E using high-precision decimal arithmetic.\"\"\"\n        r1_d = Decimal(r1_str)\n        r2_d = Decimal(r2_str)\n        return -Decimal('0.5') * (Decimal('1') / r2_d - Decimal('1') / r1_d)\n\n    def check_error_bound(r1, r2):\n        \"\"\"\n        Verifies the relative error bound for a given test case.\n        Returns True if the measured error is within 2x the theoretical bound.\n        \"\"\"\n        unit_roundoff_64 = u_th_64\n\n        # 1. Compute Delta_E with standard float64 arithmetic\n        delta_E_comp = compute_delta_E_naive(r1, r2, np.float64)\n\n        # 2. Compute high-precision \"true\" Delta_E\n        # Use string representation to avoid float inaccuracies upon conversion\n        delta_E_true_decimal = compute_delta_E_true(str(r1), str(r2))\n        delta_E_true_float = float(delta_E_true_decimal)\n        \n        # 3. Calculate measured relative error\n        if delta_E_true_float == 0.0: return False # Avoid division by zero\n        measured_rel_error = abs(delta_E_comp - delta_E_true_float) / abs(delta_E_true_float)\n\n        # 4. Calculate theoretical error bound\n        r1_f = np.float64(r1)\n        r2_f = np.float64(r2)\n        # Simplified and stable form of the condition number term\n        condition_term = (r1_f + r2_f) / abs(r2_f - r1_f)\n        error_bound = (condition_term + 1.0) * unit_roundoff_64\n\n        # 5. Check if the measured error is within tolerance of the bound\n        return measured_rel_error <= 2.0 * error_bound\n\n    # Test Case 3: Cancellation, moderate\n    r1_c3 = 1e8\n    r2_c3 = 1e8 + 1\n    bound_ok_case3 = check_error_bound(r1_c3, r2_c3)\n\n    # Test Case 4: Cancellation, extreme\n    r1_c4 = 1.0\n    # The next representable float64 after 1.0\n    r2_c4 = np.nextafter(np.float64(1.0), np.float64(2.0))\n    bound_ok_case4 = check_error_bound(r1_c4, r2_c4)\n    \n    # Collate results in the specified order\n    results = [\n        abs_err_u64,\n        abs_err_eps64,\n        abs_err_u32,\n        abs_err_eps32,\n        bound_ok_case3,\n        bound_ok_case4\n    ]\n    \n    # Print the final result in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}