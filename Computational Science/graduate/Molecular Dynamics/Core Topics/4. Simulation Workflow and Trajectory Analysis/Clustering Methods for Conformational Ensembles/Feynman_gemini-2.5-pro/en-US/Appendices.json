{
    "hands_on_practices": [
        {
            "introduction": "Meaningful clustering begins with a meaningful distance metric. This first practice challenges you to construct a composite distance that intelligently blends global structural similarity, measured by RMSD, with local conformational changes described by dihedral angles . By implementing a circular representation for periodic angles, you will directly address a common pitfall in molecular analysis and appreciate how a well-designed metric leads to more robust clustering results.",
            "id": "3401829",
            "problem": "Design and implement a composite distance for comparing molecular conformations that blends Cartesian Root Mean Square Deviation (RMSD) with torsional periodicity by mapping dihedral angles to the unit circle. The goal is to probe clustering sensitivity when angles cross the $\\pm\\pi$ boundary. Work entirely in a purely mathematical specification that can be implemented in any programming language.\n\nDefinitions and requirements:\n- Each conformation is specified by a set of $M$ three-dimensional Cartesian coordinates (in Angstroms) and a vector of $K$ torsion angles (in radians). Angles must be treated in radians.\n- The Root Mean Square Deviation (RMSD) between two coordinate sets must be computed after optimal rigid-body superposition (no reflections), obtained by the orthogonal rotation that minimizes the Euclidean sum of squared distances. This is the classical Kabsch alignment using the Singular Value Decomposition (SVD), followed by translation to align centroids.\n- To handle torsional periodicity, map each torsion angle $\\theta \\in \\mathbb{R}$ to the unit circle via $\\phi(\\theta) = (\\cos \\theta, \\sin \\theta) \\in \\mathbb{R}^2$. For a torsion vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^K$, define its embedding as $\\Phi(\\boldsymbol{\\theta}) = (\\cos \\theta_1, \\sin \\theta_1, \\ldots, \\cos \\theta_K, \\sin \\theta_K) \\in \\mathbb{R}^{2K}$.\n- Define the circular torsional distance between two torsion vectors $\\boldsymbol{\\theta}^{(A)}$ and $\\boldsymbol{\\theta}^{(B)}$ as the Euclidean norm $\\left\\|\\Phi(\\boldsymbol{\\theta}^{(A)}) - \\Phi(\\boldsymbol{\\theta}^{(B)})\\right\\|_2$, which is invariant to wrapping at the $\\pm\\pi$ boundary.\n- Define the naive linear torsional distance as $\\left\\|\\boldsymbol{\\theta}^{(A)} - \\boldsymbol{\\theta}^{(B)}\\right\\|_2$ (no modular wrapping).\n- Define the composite circular distance between conformations $A$ and $B$ as\n$$\nd_{\\mathrm{circ}}(A,B) = \\sqrt{\\alpha \\cdot \\mathrm{RMSD}(A,B)^2 + \\beta \\cdot \\left\\|\\Phi(\\boldsymbol{\\theta}^{(A)}) - \\Phi(\\boldsymbol{\\theta}^{(B)})\\right\\|_2^2},\n$$\nand the composite linear-angle distance as\n$$\nd_{\\mathrm{lin}}(A,B) = \\sqrt{\\alpha \\cdot \\mathrm{RMSD}(A,B)^2 + \\beta \\cdot \\left\\|\\boldsymbol{\\theta}^{(A)} - \\boldsymbol{\\theta}^{(B)}\\right\\|_2^2}.\n$$\nHere $\\alpha$ and $\\beta$ are nonnegative weights.\n\nClustering rule (single-linkage with threshold):\n- Given a set of conformations, a distance function $d(\\cdot,\\cdot)$, and a threshold $\\tau \\ge 0$, construct an undirected graph on the conformations with an edge between nodes $i$ and $j$ if and only if $d(i,j) \\le \\tau$. Clusters are the connected components of this graph.\n- The cluster labeling for an ordered set of $N$ conformations must be a list of $N$ integers, where labels are assigned in order of first appearance and start at $0$; that is, the first conformation gets label $0$, and each subsequent conformation gets the smallest nonnegative integer not yet used by any of its already labeled connected-component members.\n\nData set (shared across all tests):\n- Number of atoms per conformation: $M = 4$.\n- Number of torsions per conformation: $K = 2$.\n- Six conformations indexed $0$ through $5$. For each conformation, the Cartesian coordinates (in Angstroms) are listed as $4 \\times 3$ arrays, and torsions (in radians) as $2$-vectors.\n\nConformation $0$:\n- Coordinates:\n  $[ [\\,0.00,\\, 0.00,\\, 0.00\\,], [\\,1.54,\\, 0.00,\\, 0.00\\,], [\\,3.08,\\, 0.10,\\, 0.00\\,], [\\,4.62,\\, 0.10,\\, 0.05\\,] ]$\n- Torsions:\n  $[\\,3.091592653589793,\\, 0.30\\,]$.\n\nConformation $1$:\n- Coordinates:\n  $[ [\\,0.00,\\, 0.00,\\, 0.00\\,], [\\,1.54,\\, 0.01,\\, 0.00\\,], [\\,3.08,\\, 0.08,\\, 0.00\\,], [\\,4.62,\\, 0.12,\\, 0.05\\,] ]$\n- Torsions:\n  $[\\, -3.101592653589793,\\, 0.31\\,]$.\n\nConformation $2$:\n- Coordinates:\n  $[ [\\,0.00,\\, 0.00,\\, 0.00\\,], [\\,1.54,\\, -0.01,\\, 0.00\\,], [\\,3.08,\\, 0.11,\\, 0.00\\,], [\\,4.62,\\, 0.09,\\, 0.05\\,] ]$\n- Torsions:\n  $[\\,3.081592653589793,\\, 0.29\\,]$.\n\nConformation $3$:\n- Coordinates:\n  $[ [\\,0.00,\\, 0.00,\\, 0.00\\,], [\\,1.54,\\, 0.05,\\, 0.00\\,], [\\,3.08,\\, 0.25,\\, 0.05\\,], [\\,4.62,\\, 0.35,\\, 0.10\\,] ]$\n- Torsions:\n  $[\\,1.20,\\, -2.00\\,]$.\n\nConformation $4$:\n- Coordinates:\n  $[ [\\,0.00,\\, 0.00,\\, 0.00\\,], [\\,1.54,\\, 0.06,\\, 0.00\\,], [\\,3.08,\\, 0.26,\\, 0.04\\,], [\\,4.62,\\, 0.36,\\, 0.12\\,] ]$\n- Torsions:\n  $[\\,1.22,\\, -2.02\\,]$.\n\nConformation $5$ (identical to conformation $4$):\n- Coordinates:\n  $[ [\\,0.00,\\, 0.00,\\, 0.00\\,], [\\,1.54,\\, 0.06,\\, 0.00\\,], [\\,3.08,\\, 0.26,\\, 0.04\\,], [\\,4.62,\\, 0.36,\\, 0.12\\,] ]$\n- Torsions:\n  $[\\,1.22,\\, -2.02\\,]$.\n\nTest suite:\n- Test $1$ (happy path, two well-separated clusters under circular metric):\n  - Subset indices: $[\\,0,\\,1,\\,2,\\,3,\\,4\\,]$.\n  - Weights: $\\alpha = 1.0$, $\\beta = 1.0$.\n  - Threshold: $\\tau = 1.0$.\n  - Distance: $d_{\\mathrm{circ}}$.\n  - Output: the integer number of clusters.\n\n- Test $2$ (boundary sensitivity at $\\pm\\pi$):\n  - Subset indices: $[\\,0,\\,1\\,]$.\n  - Weights: $\\alpha = 1.0$, $\\beta = 1.0$.\n  - Threshold: $\\tau = 0.3$.\n  - Distances: compare $d_{\\mathrm{lin}}$ and $d_{\\mathrm{circ}}$.\n  - Output: the integer difference in the number of clusters, defined as $\\text{clusters}_{\\mathrm{lin}} - \\text{clusters}_{\\mathrm{circ}}$.\n\n- Test $3$ (degenerate duplicates):\n  - Subset indices: $[\\,4,\\,5\\,]$.\n  - Weights: $\\alpha = 1.0$, $\\beta = 1.0$.\n  - Threshold: $\\tau = 10^{-12}$.\n  - Distance: either $d_{\\mathrm{lin}}$ or $d_{\\mathrm{circ}}$ (they coincide for identical conformations).\n  - Output: the integer number of clusters.\n\n- Test $4$ (weight sensitivity under circular metric):\n  - Subset indices: $[\\,0,\\,2,\\,3\\,]$.\n  - Threshold: $\\tau = 0.3$.\n  - Distances: $d_{\\mathrm{circ}}$ with two weightings:\n    - Case A: $\\alpha = 1.0$, $\\beta = 1.0$.\n    - Case B: $\\alpha = 1.0$, $\\beta = 0.0$.\n  - Output: a boolean indicating whether the cluster labelings from Case A and Case B are identical (True if identical, False otherwise).\n\nFinal output format:\n- Your program should produce a single line containing the results of Tests $1$ through $4$ as a comma-separated Python-style list, for example $[\\,r_1, r_2, r_3, r_4\\,]$, where $r_1$ and $r_3$ are integers, $r_2$ is an integer, and $r_4$ is a boolean. There must be no additional text before or after this list on the output line.",
            "solution": "The user has provided a problem that requires the design and implementation of a composite distance metric for molecular conformations and a subsequent clustering algorithm. The problem is to be validated for scientific soundness, well-posedness, and objectivity before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Conformation Representation**: Each conformation consists of $M$ Cartesian coordinates in $\\mathbb{R}^3$ and $K$ torsion angles in $\\mathbb{R}$.\n- **Root Mean Square Deviation (RMSD)**: Calculated after optimal rigid-body superposition using the Kabsch algorithm (SVD-based, with handling for reflections).\n- **Torsional Angle Mapping**: An angle $\\theta$ is mapped to the unit circle as $\\phi(\\theta) = (\\cos \\theta, \\sin \\theta) \\in \\mathbb{R}^2$. A vector of $K$ angles $\\boldsymbol{\\theta}$ is mapped to $\\Phi(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{2K}$.\n- **Distance Metrics**:\n    - **Circular Torsional Distance**: $d_{\\mathrm{tors,circ}}(\\boldsymbol{\\theta}^{(A)}, \\boldsymbol{\\theta}^{(B)}) = \\left\\|\\Phi(\\boldsymbol{\\theta}^{(A)}) - \\Phi(\\boldsymbol{\\theta}^{(B)})\\right\\|_2$.\n    - **Linear Torsional Distance**: $d_{\\mathrm{tors,lin}}(\\boldsymbol{\\theta}^{(A)}, \\boldsymbol{\\theta}^{(B)}) = \\left\\|\\boldsymbol{\\theta}^{(A)} - \\boldsymbol{\\theta}^{(B)}\\right\\|_2$.\n    - **Composite Circular Distance**: $d_{\\mathrm{circ}}(A,B) = \\sqrt{\\alpha \\cdot \\mathrm{RMSD}(A,B)^2 + \\beta \\cdot d_{\\mathrm{tors,circ}}(\\boldsymbol{\\theta}^{(A)}, \\boldsymbol{\\theta}^{(B)})^2}$.\n    - **Composite Linear-Angle Distance**: $d_{\\mathrm{lin}}(A,B) = \\sqrt{\\alpha \\cdot \\mathrm{RMSD}(A,B)^2 + \\beta \\cdot d_{\\mathrm{tors,lin}}(\\boldsymbol{\\theta}^{(A)}, \\boldsymbol{\\theta}^{(B)})^2}$.\n- **Clustering Rule**:\n    - A graph is built where nodes are conformations and an edge exists between conformations $i$ and $j$ if their distance $d(i,j) \\le \\tau$, where $\\tau$ is a given threshold.\n    - Clusters are the connected components of this graph.\n    - Labeling: For an ordered set of conformations, labels are integers starting from $0$. The label for a conformation is determined by the first-encountered label of its connected component during an ordered traversal of the conformations.\n- **Data Set**: $M=4$, $K=2$. Six conformations (indexed $0$ to $5$) are provided with their Cartesian coordinates and torsion angles. Conformations $4$ and $5$ are identical.\n- **Test Suite**: Four specific tests are defined, each with a subset of conformations, parameters ($\\alpha, \\beta, \\tau$), the distance metric to use, and a specific output to calculate (number of clusters, difference in cluster counts, or a boolean comparison of labelings).\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in computational biophysics and structural biology. RMSD, Kabsch algorithm, torsion angles, and clustering are all standard concepts. The use of a circular metric for periodic data (angles) is a common and valid technique. The problem is scientifically sound.\n2.  **Well-Posed**: The problem provides all necessary definitions, data, and parameters. The formulas for distances are explicit. The clustering algorithm, based on finding connected components in a thresholded distance graph, is a well-defined procedure. The labeling rule, while phrased in a slightly complex manner, corresponds to a standard deterministic algorithm: find the connected components, then assign labels to these components in the order they are first encountered when iterating through the input conformations. This ensures a unique solution for each test case. The problem is well-posed.\n3.  **Objective**: The language is precise and mathematical. All requirements are stated formally and without ambiguity or subjective elements. The problem is objective.\n4.  **Completeness and Consistency**: The problem is self-contained. All data and constants are provided. There are no contradictions. The fact that conformations 4 and 5 are identical is a deliberate part of the design for Test 3, not an error.\n5.  **Feasibility**: The specified calculations (SVD, matrix operations, graph component analysis) are computationally feasible and standard in scientific computing libraries. The data values are realistic for molecular structures.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-formulated computational task based on established scientific principles. I will proceed with providing a solution.\n\n### Principle-Based Design of the Solution\nThe solution is structured around creating a series of functions that encapsulate the core concepts defined in the problem, followed by a main routine to execute the specified tests.\n\n1.  **RMSD Calculation (`calculate_rmsd`)**: This function implements the Kabsch algorithm for optimal molecular superposition.\n    -   **Input**: Two point sets, $P$ and $Q$, as $M \\times 3$ NumPy arrays.\n    -   **Principle**: The goal is to find a rotation matrix $R$ and translation vector $t$ that minimize the root mean square deviation, $\\mathrm{RMSD}(P, Q) = \\min_{R,t} \\sqrt{\\frac{1}{M}\\sum_{i=1}^M \\| (R P_i + t) - Q_i \\|^2}$.\n    -   **Algorithm**:\n        1.  The optimal translation aligns the centroids of the two point sets. We first translate both $P$ and $Q$ to have their centroids at the origin: $P' = P - \\bar{P}$, $Q' = Q - \\bar{Q}$.\n        2.  The optimal rotation is found by maximizing the trace of $R^T C$, where $C = P'^T Q'$ is the covariance matrix. This maximization is solved using Singular Value Decomposition (SVD) of $C$.\n        3.  Let $C = U S V^T$. The optimal rotation matrix is $R = V U^T$.\n        4.  A special case arises if $\\det(R) = -1$, which corresponds to a reflection (an improper rotation). To ensure a proper rotation ($\\det(R)=+1$), we must correct $R$. This is done by inverting the sign of the column of $V$ corresponding to the smallest singular value before computing $R$.\n        5.  Once $R$ is found, we apply it to the centered coordinates of $P$, i.e., $P'' = P' R$.\n        6.  The RMSD is then calculated as the Euclidean distance between the transformed points $P''$ and the centered points $Q'$, averaged over all points: $\\mathrm{RMSD} = \\sqrt{\\frac{1}{M} \\sum_{i=1}^M \\| P''_i - Q'_i \\|^2}$.\n\n2.  **Distance Metrics (`calculate_composite_distance`)**: This function computes the composite distance between two conformations based on the selected metric type.\n    -   **Input**: Two conformations (each containing coordinates and torsions), weights $\\alpha$ and $\\beta$, and a flag indicating whether to use the circular or linear torsional distance.\n    -   **Principle**: It combines Cartesian and internal coordinate distances into a single metric. The circular metric correctly handles the $2\\pi$ periodicity of torsion angles.\n    -   **Algorithm**:\n        1.  Calculates $\\mathrm{RMSD}(A,B)$ using the `calculate_rmsd` function.\n        2.  Calculates the torsional distance term. For the linear distance, it's the $L^2$-norm of the difference between the torsion vectors. For the circular distance, it first maps each torsion vector $\\boldsymbol{\\theta}$ to its $2K$-dimensional representation $\\Phi(\\boldsymbol{\\theta})$ on the unit circle and then calculates the $L^2$-norm of the difference between these representations.\n        3.  Combines the squared RMSD and squared torsional distance using the weights $\\alpha$ and $\\beta$ as per the formulas for $d_{\\mathrm{circ}}$ or $d_{\\mathrm{lin}}$.\n\n3.  **Clustering (`perform_clustering`)**: This function implements the specified single-linkage clustering with a threshold.\n    -   **Input**: A list of conformations, a distance function, a threshold $\\tau$, and any parameters for the distance function (e.g., $\\alpha, \\beta$).\n    -   **Principle**: Clustering groups similar objects. Here, \"similarity\" is defined by the distance between two conformations being less than or equal to a threshold $\\tau$. This defines a graph, and the clusters are its connected components.\n    -   **Algorithm**:\n        1.  An $N \\times N$ pairwise distance matrix is computed for the given subset of conformations.\n        2.  An adjacency matrix is constructed where an entry $(i,j)$ is $1$ if the distance between conformations $i$ and $j$ is $\\le \\tau$, and $0$ otherwise.\n        3.  The `scipy.sparse.csgraph.connected_components` function is used to find the connected components of the graph represented by the adjacency matrix. This function returns the number of components and an array of raw integer labels for each conformation.\n        4.  To satisfy the problem's labeling rule (\"order of first appearance\"), these raw labels are re-mapped. We iterate through the conformations in their original input order. The first time a raw component label is encountered, it is assigned the next available final label, starting from $0$. This mapping is then used to generate the final list of labels.\n\n4.  **Main Execution Logic (`solve`)**: This function sets up the data and orchestrates the execution of the four tests described in the problem statement, printing the results in the specified format. It calls the helper functions with the appropriate parameters for each test.",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.csgraph import connected_components\n\ndef calculate_rmsd(coords_p, coords_q):\n    \"\"\"\n    Calculates the Root Mean Square Deviation (RMSD) between two sets of\n    3D coordinates (P and Q) after optimal rigid-body superposition.\n    \"\"\"\n    # Number of atoms must be the same\n    if coords_p.shape[0] != coords_q.shape[0]:\n        raise ValueError(\"Coordinate sets must have the same number of atoms.\")\n\n    # 1. Center coordinates by subtracting their centroids\n    p_centroid = coords_p.mean(axis=0)\n    q_centroid = coords_q.mean(axis=0)\n    p_centered = coords_p - p_centroid\n    q_centered = coords_q - q_centroid\n\n    # 2. Compute the covariance matrix\n    cov_matrix = p_centered.T @ q_centered\n\n    # 3. Compute the SVD of the covariance matrix\n    U, S, Vt = np.linalg.svd(cov_matrix)\n\n    # 4. Determine the optimal rotation matrix, correcting for reflections\n    # The determinant of Vt.T @ U.T can be -1 (reflection).\n    # If so, we must invert the sign of the last column of V (or last row of Vt)\n    # to ensure a proper rotation.\n    det_check = np.linalg.det(Vt.T @ U.T)\n    if det_check < 0:\n        Vt[-1, :] *= -1\n\n    R = Vt.T @ U.T\n\n    # 5. Apply the rotation to the centered coordinates of P\n    p_rotated = p_centered @ R\n\n    # 6. Calculate the squared differences and the RMSD\n    diff = p_rotated - q_centered\n    rmsd_sq = np.sum(diff * diff) / coords_p.shape[0]\n    return np.sqrt(rmsd_sq)\n\ndef calculate_composite_distance(conf_a, conf_b, alpha, beta, use_circular_torsion):\n    \"\"\"\n    Calculates the composite distance between two conformations A and B.\n    \"\"\"\n    rmsd_val = calculate_rmsd(conf_a['coords'], conf_b['coords'])\n    \n    if use_circular_torsion:\n        # Map torsions to the unit circle\n        phi_a = np.empty(2 * len(conf_a['torsions']))\n        phi_a[0::2] = np.cos(conf_a['torsions'])\n        phi_a[1::2] = np.sin(conf_a['torsions'])\n        \n        phi_b = np.empty(2 * len(conf_b['torsions']))\n        phi_b[0::2] = np.cos(conf_b['torsions'])\n        phi_b[1::2] = np.sin(conf_b['torsions'])\n        \n        torsion_dist_sq = np.sum((phi_a - phi_b)**2)\n    else:\n        torsion_dist_sq = np.sum((conf_a['torsions'] - conf_b['torsions'])**2)\n        \n    composite_dist_sq = alpha * (rmsd_val**2) + beta * torsion_dist_sq\n    return np.sqrt(composite_dist_sq)\n\ndef perform_clustering(subset_indices, conformations, alpha, beta, tau, use_circular_torsion):\n    \"\"\"\n    Performs single-linkage clustering with a threshold and returns the\n    number of clusters and the final labels.\n    \"\"\"\n    num_confs = len(subset_indices)\n    if num_confs == 0:\n        return 0, []\n\n    dist_matrix = np.zeros((num_confs, num_confs))\n    \n    for i in range(num_confs):\n        for j in range(i + 1, num_confs):\n            conf_i = conformations[subset_indices[i]]\n            conf_j = conformations[subset_indices[j]]\n            dist = calculate_composite_distance(conf_i, conf_j, alpha, beta, use_circular_torsion)\n            dist_matrix[i, j] = dist_matrix[j, i] = dist\n            \n    # Build adjacency matrix based on the threshold\n    adjacency_matrix = dist_matrix <= tau\n    \n    # Find connected components, which are the clusters\n    graph = csr_matrix(adjacency_matrix)\n    n_components, raw_labels = connected_components(csgraph=graph, directed=False, return_labels=True)\n    \n    # Re-label according to the \"first appearance\" rule\n    final_labels = -np.ones(num_confs, dtype=int)\n    label_map = {}\n    next_label = 0\n    for i in range(num_confs):\n        raw_label = raw_labels[i]\n        if raw_label not in label_map:\n            label_map[raw_label] = next_label\n            next_label += 1\n        final_labels[i] = label_map[raw_label]\n        \n    return n_components, final_labels.tolist()\n\ndef solve():\n    \"\"\"\n    Main function to define data, run tests, and print results.\n    \"\"\"\n    conformations_data = {\n        0: {'coords': np.array([[0.00, 0.00, 0.00], [1.54, 0.00, 0.00], [3.08, 0.10, 0.00], [4.62, 0.10, 0.05]]),\n            'torsions': np.array([3.091592653589793, 0.30])},\n        1: {'coords': np.array([[0.00, 0.00, 0.00], [1.54, 0.01, 0.00], [3.08, 0.08, 0.00], [4.62, 0.12, 0.05]]),\n            'torsions': np.array([-3.101592653589793, 0.31])},\n        2: {'coords': np.array([[0.00, 0.00, 0.00], [1.54, -0.01, 0.00], [3.08, 0.11, 0.00], [4.62, 0.09, 0.05]]),\n            'torsions': np.array([3.081592653589793, 0.29])},\n        3: {'coords': np.array([[0.00, 0.00, 0.00], [1.54, 0.05, 0.00], [3.08, 0.25, 0.05], [4.62, 0.35, 0.10]]),\n            'torsions': np.array([1.20, -2.00])},\n        4: {'coords': np.array([[0.00, 0.00, 0.00], [1.54, 0.06, 0.00], [3.08, 0.26, 0.04], [4.62, 0.36, 0.12]]),\n            'torsions': np.array([1.22, -2.02])},\n        5: {'coords': np.array([[0.00, 0.00, 0.00], [1.54, 0.06, 0.00], [3.08, 0.26, 0.04], [4.62, 0.36, 0.12]]),\n            'torsions': np.array([1.22, -2.02])},\n    }\n\n    # Test 1\n    subset1 = [0, 1, 2, 3, 4]\n    alpha1, beta1, tau1 = 1.0, 1.0, 1.0\n    num_clusters1, _ = perform_clustering(subset1, conformations_data, alpha1, beta1, tau1, use_circular_torsion=True)\n    result1 = num_clusters1\n\n    # Test 2\n    subset2 = [0, 1]\n    alpha2, beta2, tau2 = 1.0, 1.0, 0.3\n    num_clusters_lin, _ = perform_clustering(subset2, conformations_data, alpha2, beta2, tau2, use_circular_torsion=False)\n    num_clusters_circ, _ = perform_clustering(subset2, conformations_data, alpha2, beta2, tau2, use_circular_torsion=True)\n    result2 = num_clusters_lin - num_clusters_circ\n\n    # Test 3\n    subset3 = [4, 5]\n    alpha3, beta3, tau3 = 1.0, 1.0, 1e-12\n    # Distance function doesn't matter since conformations are identical (dist=0)\n    num_clusters3, _ = perform_clustering(subset3, conformations_data, alpha3, beta3, tau3, use_circular_torsion=True)\n    result3 = num_clusters3\n\n    # Test 4\n    subset4 = [0, 2, 3]\n    tau4 = 0.3\n    # Case A\n    alpha4a, beta4a = 1.0, 1.0\n    _, labels_A = perform_clustering(subset4, conformations_data, alpha4a, beta4a, tau4, use_circular_torsion=True)\n    # Case B\n    alpha4b, beta4b = 1.0, 0.0\n    _, labels_B = perform_clustering(subset4, conformations_data, alpha4b, beta4b, tau4, use_circular_torsion=True)\n    result4 = labels_A == labels_B\n    \n    results = [result1, result2, result3, result4]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With a robust distance metric in hand, the next challenge is applying it to the massive datasets generated by molecular dynamics simulations. This practice simulates a real-world computational scenario, requiring you to select the most efficient $k$-medoids clustering pipeline based on memory constraints and algorithmic complexity . This exercise will sharpen your ability to analyze trade-offs between computational cost and algorithmic accuracy, a crucial skill for tackling large-scale scientific data.",
            "id": "3401895",
            "problem": "You are given a scenario from Molecular Dynamics (MD) analysis where a conformational ensemble of protein structures is represented by a pairwise Root Mean Square Deviation (RMSD) matrix. Your goal is to design and analyze a computational pipeline for clustering these structures into conformational states using the $k$-medoids algorithm. The matrix corresponds to $N$ snapshots (structures), and the task is to provide a computationally efficient approach for $N = 10000$ that includes algorithmic complexity analysis and memory considerations. Root Mean Square Deviation (RMSD) is a metric distance between structures, and the $k$-medoids objective is to minimize the sum of distances between each point and its nearest medoid. You will not compute RMSD values; instead, you must count distance matrix lookups and elementary arithmetic operations under a standardized counting model.\n\nFundamental base and definitions:\n- The $k$-medoids objective chooses a set $M$ of $k$ indices that minimizes the sum of distances to the nearest medoid, i.e., $\\sum_{i=1}^{N} \\min_{m \\in M} d(i,m)$ for metric $d(\\cdot,\\cdot)$.\n- You are given a precomputed pairwise RMSD matrix, so using it is considered a constant-time lookup for any pair. Operations are counted as follows:\n  1. One distance lookup equals accessing $d(i,j)$ once from memory.\n  2. One elementary addition equals adding two scalar values once.\n- The classic Partitioning Around Medoids (PAM) method has a BUILD step to pick initial medoids and a SWAP step to refine them; modern accelerations reduce the dominant per-iteration cost to a term on the order of $N^{2}$ via multi-swap evaluation with cached nearest and second-nearest medoid distances.\n- A full symmetric matrix can be stored in a condensed layout of size $N(N-1)/2$ entries. If the element size is $b$ bytes, then the condensed matrix uses $b \\cdot N(N-1)/2$ bytes. Auxiliary arrays for nearest and second-nearest distances, and current assignments, scale as $\\mathcal{O}(N)$ and are counted explicitly below.\n- Subsampling (as in Clustering Large Applications, CLARA) evaluates PAM on a sample of size $s \\ll N$, repeated $r$ times, then assigns all $N$ points to the best-found medoids.\n- Landmark-based initialization uses $L$ landmark structures and $N \\cdot L$ precomputed landmark distances to seed medoids, followed by a small number of refinement iterations on the full set.\n\nCounting and memory model to be used in your program:\n- For a full condensed distance matrix approach with a multi-swap $k$-medoids refinement (one “FastPAM-like” iteration):\n  - Distance lookups per iteration: $N \\cdot k + N \\cdot (N - k)$, where the second term counts a single scan over all non-medoids as candidates using cached nearest and second-nearest distances.\n  - Additions per iteration: equal to the number of distance lookups (assume one addition per lookup when updating objective deltas).\n  - Total over $t$ iterations: multiply by $t$.\n  - Memory in bytes: $b \\cdot \\left( \\frac{N(N-1)}{2} + 3N \\right) + 8k$. The $3N$ accounts for two cached distance arrays and one assignment array; medoid indices are counted as $8k$ bytes.\n- For CLARA-style subsampling with sample size $s$ and $r$ restarts:\n  - Per-iteration distance lookups on the sample: $s \\cdot k + s \\cdot (s - k)$.\n  - Total sample lookups: $r \\cdot t \\cdot \\left( s \\cdot k + s \\cdot (s - k) \\right)$.\n  - Full-data evaluation of each run plus final assignment: $r \\cdot (N \\cdot k) + (N \\cdot k)$ lookups.\n  - Additions: equal to the total number of lookups.\n  - Memory in bytes: $b \\cdot \\left( \\frac{s(s-1)}{2} + 3s \\right) + 8k$ (store the sample condensed matrix and $\\mathcal{O}(s)$ caches; full-data evaluation is streaming and does not store $\\mathcal{O}(N)$ arrays).\n- For exact $k=1$ medoid by streaming over the upper triangle:\n  - Distance lookups: $\\frac{N(N-1)}{2}$ (each pair once).\n  - Additions: $N(N-1)$ (two additions per pair to accumulate two row sums symmetrically).\n  - Memory in bytes: $b \\cdot N$ (to store the running row sums) $+ 8$ (for the index of the best medoid).\n- For a landmark-initialized approach with $L$ landmarks and $t_{\\mathrm{refine}}$ FastPAM-like refinement iterations:\n  - Landmark distance lookups: $N \\cdot L$.\n  - Refinement lookups: $t_{\\mathrm{refine}} \\cdot \\left( N \\cdot k + N \\cdot (N - k) \\right)$.\n  - Final assignment lookups: $N \\cdot k$.\n  - Additions: equal to the total number of lookups.\n  - Memory in bytes: $b \\cdot \\left( N \\cdot L + 3N \\right) + 8k$.\n\nPipeline selection rule to implement:\n- If $k = 1$, choose the exact streaming $k=1$ medoid pipeline.\n- Else, if the full condensed matrix with caches fits in memory (bytes $\\leq B$), choose the full FastPAM-like pipeline.\n- Else, if a landmark plan with given $L$ fits in memory (bytes $\\leq B$), choose the landmark-initialized pipeline with the specified $t_{\\mathrm{refine}}$.\n- Else, choose the CLARA subsampling pipeline with given $s$ and $r$.\n\nYour task:\n- Implement a program that, for each test case, determines the chosen pipeline under the selection rule, computes whether the full condensed matrix is feasible within budget, computes the memory used by the chosen pipeline in bytes, and computes the total number of distance lookups and additions under the above counting model.\n\nAngle units are not applicable. Physical units are not applicable. All memory must be reported in bytes. All counts must be integers.\n\nTest suite:\n- Case $1$: $N = 10000$, $k = 20$, $t = 5$, $b = 4$, $B = 250000000$.\n- Case $2$: $N = 10000$, $k = 100$, $t = 5$, $b = 8$, $B = 300000000$, CLARA parameters: $s = 2000$, $r = 5$.\n- Case $3$: $N = 10000$, $k = 1$, $t = 3$, $b = 4$, $B = 50000000$.\n- Case $4$: $N = 10000$, $k = 50$, $t = 4$, $b = 4$, $B = 180000000$, landmark parameters: $L = 100$, $t_{\\mathrm{refine}} = 1$.\n\nOutput format:\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the same order as above. Each test case result must itself be a list with five integers:\n  1. Feasibility of full condensed matrix within budget as $0$ or $1$.\n  2. Chosen pipeline identifier: $0$ for full FastPAM-like, $1$ for CLARA subsampling, $2$ for exact $k=1$ streaming, $3$ for landmark-initialized with refinement.\n  3. Memory used by the chosen pipeline in bytes.\n  4. Total number of distance lookups.\n  5. Total number of additions.\n- For example, your output should look like $[[a_{1},a_{2},a_{3},a_{4},a_{5}],[b_{1},b_{2},b_{3},b_{4},b_{5}],\\dots]$ with no whitespace characters.",
            "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded problem in computational science.\n\n### Step 1: Extract Givens\n\nThe problem provides the following data, definitions, and rules for analyzing computational pipelines for $k$-medoids clustering of molecular dynamics trajectory data.\n\n**General Parameters:**\n- $N$: The number of snapshots (structures).\n- $k$: The number of clusters (medoids).\n- $t$: The number of refinement iterations for iterative algorithms.\n- $b$: The size in bytes of a single distance matrix element (e.g., a floating-point number).\n- $B$: The total memory budget in bytes.\n\n**Algorithm-Specific Parameters:**\n- For CLARA: $s$ (sample size), $r$ (number of restarts).\n- For Landmark-based initialization: $L$ (number of landmarks), $t_{\\mathrm{refine}}$ (number of refinement iterations).\n\n**Fundamental Definitions:**\n- The $k$-medoids objective is to find a set of $k$ medoids $M$ that minimizes $\\sum_ {i=1}^{N} \\min_ {m \\in M} d(i,m)$, where $d(\\cdot, \\cdot)$ is a metric distance.\n- A distance matrix lookup is accessing $d(i,j)$ and is counted as $1$ operation.\n- An elementary addition is adding two scalars and is counted as $1$ operation.\n\n**Cost Models:**\n1.  **Full Condensed Matrix (FastPAM-like):**\n    - Distance lookups per iteration: $N \\cdot k + N \\cdot (N - k)$.\n    - Additions per iteration: Equal to distance lookups.\n    - Total operations over $t$ iterations: $t \\times (\\text{per-iteration cost})$.\n    - Memory (bytes): $b \\cdot \\left( \\frac{N(N-1)}{2} + 3N \\right) + 8k$.\n\n2.  **CLARA-style Subsampling:**\n    - Sample-based lookups: $r \\cdot t \\cdot \\left( s \\cdot k + s \\cdot (s - k) \\right)$.\n    - Full-data evaluation lookups: $r \\cdot (N \\cdot k) + (N \\cdot k)$.\n    - Total lookups: Sum of the two terms above.\n    - Total additions: Equal to total distance lookups.\n    - Memory (bytes): $b \\cdot \\left( \\frac{s(s-1)}{2} + 3s \\right) + 8k$.\n\n3.  **Exact $k=1$ Streaming:**\n    - Distance lookups: $\\frac{N(N-1)}{2}$.\n    - Additions: $N(N-1)$.\n    - Memory (bytes): $b \\cdot N + 8$.\n\n4.  **Landmark-Initialized:**\n    - Landmark distance lookups: $N \\cdot L$.\n    - Refinement lookups: $t_{\\mathrm{refine}} \\cdot \\left( N \\cdot k + N \\cdot (N - k) \\right)$.\n    - Final assignment lookups: $N \\cdot k$.\n    - Total lookups: Sum of the three terms above.\n    - Total additions: Equal to total distance lookups.\n    - Memory (bytes): $b \\cdot \\left( N \\cdot L + 3N \\right) + 8k$.\n\n**Pipeline Selection Rule:**\nA strict hierarchical decision process must be followed:\n1.  If $k = 1$, choose the \"Exact $k=1$ Streaming\" pipeline.\n2.  Else, if the memory for the \"Full Condensed Matrix\" pipeline is within budget ($\\le B$), choose it.\n3.  Else, if a landmark plan is defined (parameters $L, t_{\\mathrm{refine}}$ are given) and its memory is within budget ($\\le B$), choose the \"Landmark-Initialized\" pipeline.\n4.  Else, choose the \"CLARA-style Subsampling\" pipeline.\n\n**Test Suite:**\n- Case 1: $N = 10000$, $k = 20$, $t = 5$, $b = 4$, $B = 250000000$.\n- Case 2: $N = 10000$, $k = 100$, $t = 5$, $b = 8$, $B = 300000000$, $s = 2000$, $r = 5$.\n- Case 3: $N = 10000$, $k = 1$, $t = 3$, $b = 4$, $B = 50000000$.\n- Case 4: $N = 10000$, $k = 50$, $t = 4$, $b = 4$, $B = 180000000$, $L = 100$, $t_{\\mathrm{refine}} = 1$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe provided problem is a well-defined exercise in computational analysis and algorithmic decision-making.\n- **Scientifically Grounded (Critical):** The problem is rooted in the common task of clustering conformational ensembles from molecular dynamics simulations, a standard procedure in computational biophysics. The use of RMSD, $k$-medoids, and associated algorithms like PAM and CLARA is entirely conventional and scientifically sound. The cost models, while simplified, are explicitly defined and based on reasonable approximations of algorithmic complexity.\n- **Well-Posed:** The problem is structured with deterministic rules. A clear set of inputs is provided for each test case, and the \"Pipeline selection rule\" is an unambiguous decision-making algorithm. The formulas for calculating the output metrics (memory, lookups, additions) are explicit. This structure guarantees a unique and stable solution for each test case.\n- **Objective (Critical):** The problem statement is purely quantitative and algorithmic. It avoids any subjective language, ambiguity, or opinion-based claims.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or reliance on non-formalizable concepts. It is a valid computational problem.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe solution requires implementing the specified pipeline selection logic and the corresponding cost calculations for each test case. For each case, we will first determine if the full matrix approach is feasible and then apply the selection rule to identify the chosen pipeline. Finally, we compute the memory usage, total distance lookups, and total additions for the selected pipeline.\n\n**Analysis for Case 1:**\n- Parameters: $N = 10000$, $k = 20$, $t = 5$, $b = 4$, $B = 250000000$.\n- Memory for full matrix: $M_{\\text{full}} = b \\cdot (\\frac{N(N-1)}{2} + 3N) + 8k = 4 \\cdot (\\frac{10000 \\cdot 9999}{2} + 3 \\cdot 10000) + 8 \\cdot 20 = 4 \\cdot (49995000 + 30000) + 160 = 200100160$ bytes.\n- Feasibility of full matrix: $200100160 \\le 250000000$, which is true. Feasibility is $1$.\n- Selection: Since $k \\neq 1$ and the full matrix is feasible, the \"Full FastPAM-like\" pipeline (ID $0$) is chosen.\n- Memory used: $200100160$ bytes.\n- Total lookups: $L_{\\text{total}} = t \\cdot (N \\cdot k + N \\cdot (N - k)) = t \\cdot N^2 = 5 \\cdot (10000)^2 = 500000000$.\n- Total additions: $A_{\\text{total}} = L_{\\text{total}} = 500000000$.\n- Result: $[1, 0, 200100160, 500000000, 500000000]$.\n\n**Analysis for Case 2:**\n- Parameters: $N = 10000$, $k = 100$, $t = 5$, $b = 8$, $B = 300000000$, $s = 2000$, $r = 5$.\n- Memory for full matrix: $M_{\\text{full}} = 8 \\cdot (\\frac{10000 \\cdot 9999}{2} + 3 \\cdot 10000) + 8 \\cdot 100 = 8 \\cdot (50025000) + 800 = 400200800$ bytes.\n- Feasibility of full matrix: $400200800 \\le 300000000$, which is false. Feasibility is $0$.\n- Selection: $k \\neq 1$, full matrix is not feasible. Landmark parameters are not given. The rule defaults to the \"CLARA subsampling\" pipeline (ID $1$).\n- Memory used: $M_{\\text{CLARA}} = b \\cdot (\\frac{s(s-1)}{2} + 3s) + 8k = 8 \\cdot (\\frac{2000 \\cdot 1999}{2} + 3 \\cdot 2000) + 8 \\cdot 100 = 8 \\cdot (1999000 + 6000) + 800 = 16040800$ bytes.\n- Total lookups: $L_{\\text{total}} = r \\cdot t \\cdot (s \\cdot k + s \\cdot (s - k)) + (r+1) \\cdot N \\cdot k = r \\cdot t \\cdot s^2 + (r+1) \\cdot N \\cdot k = 5 \\cdot 5 \\cdot 2000^2 + (5+1) \\cdot 10000 \\cdot 100 = 100000000 + 6000000 = 106000000$.\n- Total additions: $A_{\\text{total}} = L_{\\text{total}} = 106000000$.\n- Result: $[0, 1, 16040800, 106000000, 106000000]$.\n\n**Analysis for Case 3:**\n- Parameters: $N = 10000$, $k = 1$, $t = 3$, $b = 4$, $B = 50000000$.\n- Memory for full matrix: $M_{\\text{full}} = 4 \\cdot (\\frac{10000 \\cdot 9999}{2} + 3 \\cdot 10000) + 8 \\cdot 1 = 200100008$ bytes.\n- Feasibility of full matrix: $200100008 \\le 50000000$, which is false. Feasibility is $0$.\n- Selection: Since $k = 1$, the \"Exact $k=1$ Streaming\" pipeline (ID $2$) is chosen, irrespective of other conditions.\n- Memory used: $M_{k=1} = b \\cdot N + 8 = 4 \\cdot 10000 + 8 = 40008$ bytes.\n- Total lookups: $L_{\\text{total}} = \\frac{N(N-1)}{2} = \\frac{10000 \\cdot 9999}{2} = 49995000$.\n- Total additions: $A_{\\text{total}} = N(N-1) = 99980000$.\n- Result: $[0, 2, 40008, 49995000, 99980000]$.\n\n**Analysis for Case 4:**\n- Parameters: $N = 10000$, $k = 50$, $t = 4$, $b = 4$, $B = 180000000$, $L = 100$, $t_{\\mathrm{refine}} = 1$.\n- Memory for full matrix: $M_{\\text{full}} = 4 \\cdot (\\frac{10000 \\cdot 9999}{2} + 3 \\cdot 10000) + 8 \\cdot 50 = 200100400$ bytes.\n- Feasibility of full matrix: $200100400 \\le 180000000$, which is false. Feasibility is $0$.\n- Selection: $k \\neq 1$, full matrix is not feasible. We next check the landmark-based approach.\n- Memory for landmark: $M_{\\text{landmark}} = b \\cdot (N \\cdot L + 3N) + 8k = 4 \\cdot (10000 \\cdot 100 + 3 \\cdot 10000) + 8 \\cdot 50 = 4 \\cdot (1000000 + 30000) + 400 = 4120400$ bytes.\n- Landmark feasibility: $4120400 \\le 180000000$, which is true. The \"Landmark-Initialized\" pipeline (ID $3$) is chosen.\n- Memory used: $4120400$ bytes.\n- Total lookups: $L_{\\text{total}} = N \\cdot L + t_{\\mathrm{refine}} \\cdot (N \\cdot k + N \\cdot (N-k)) + N \\cdot k = N \\cdot L + t_{\\mathrm{refine}} \\cdot N^2 + N \\cdot k = 10000 \\cdot 100 + 1 \\cdot (10000)^2 + 10000 \\cdot 50 = 1000000 + 100000000 + 500000 = 101500000$.\n- Total additions: $A_{\\text{total}} = L_{\\text{total}} = 101500000$.\n- Result: $[0, 3, 4120400, 101500000, 101500000]$.\n\nThese calculations will be implemented in the provided Python code structure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are imported from scipy or elsewhere.\n\ndef solve():\n    \"\"\"\n    Analyzes computational pipelines for k-medoids clustering based on a given set of rules\n    and cost models, for several test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: N=10000, k=20, t=5, b=4, B=250000000\n        {'N': 10000, 'k': 20, 't': 5, 'b': 4, 'B': 250000000},\n        # Case 2: N=10000, k=100, t=5, b=8, B=300000000, CLARA params: s=2000, r=5\n        {'N': 10000, 'k': 100, 't': 5, 'b': 8, 'B': 300000000, 's': 2000, 'r': 5},\n        # Case 3: N=10000, k=1, t=3, b=4, B=50000000\n        {'N': 10000, 'k': 1, 't': 3, 'b': 4, 'B': 50000000},\n        # Case 4: N=10000, k=50, t=4, b=4, B=180000000, landmark params: L=100, t_refine=1\n        {'N': 10000, 'k': 50, 't': 4, 'b': 4, 'B': 180000000, 'L': 100, 't_refine': 1},\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case['N']\n        k = case['k']\n        b = case['b']\n        B = case['B']\n        t = case.get('t', 0) # Not used for all pipelines, default to 0\n\n        # Calculate memory for the full matrix approach to determine feasibility\n        mem_full = b * (N * (N - 1) // 2 + 3 * N) + 8 * k\n        is_full_feasible = 1 if mem_full <= B else 0\n\n        chosen_pipeline_id = -1\n        mem_used = -1\n        lookups = -1\n        additions = -1\n\n        # Apply the pipeline selection rule\n        if k == 1:\n            # Pipeline 2: Exact k=1 streaming\n            chosen_pipeline_id = 2\n            mem_used = b * N + 8\n            lookups = N * (N - 1) // 2\n            additions = N * (N - 1)\n        elif is_full_feasible:\n            # Pipeline 0: Full FastPAM-like\n            chosen_pipeline_id = 0\n            mem_used = mem_full\n            # Lookups/Additions: t * (N*k + N*(N-k)) simplifies to t * N^2\n            ops_count = t * (N * k + N * (N - k))\n            lookups = ops_count\n            additions = ops_count\n        elif 'L' in case:\n            # Check landmark pipeline\n            L = case['L']\n            t_refine = case['t_refine']\n            mem_landmark = b * (N * L + 3 * N) + 8 * k\n            if mem_landmark <= B:\n                # Pipeline 3: Landmark-initialized\n                chosen_pipeline_id = 3\n                mem_used = mem_landmark\n                # Ops: N*L + t_refine * (N*k + N*(N-k)) + N*k\n                ops_count = N * L + t_refine * (N * k + N * (N - k)) + N * k\n                lookups = ops_count\n                additions = ops_count\n            else:\n                # Fallback to CLARA\n                chosen_pipeline_id = 1\n        else:\n            # Default to CLARA if full and landmark are not feasible/available\n            chosen_pipeline_id = 1\n\n        # If CLARA was chosen (either by direct fallback or because landmark failed)\n        if chosen_pipeline_id == 1:\n            s = case['s']\n            r = case['r']\n            mem_used = b * (s * (s - 1) // 2 + 3 * s) + 8 * k\n            # Ops: r*t*(s*k + s*(s-k)) + r*N*k + N*k\n            ops_count = r * t * (s * k + s * (s - k)) + r * N * k + N * k\n            lookups = ops_count\n            additions = ops_count\n\n        results.append([is_full_feasible, chosen_pipeline_id, mem_used, lookups, additions])\n\n    # Format the final output string as specified\n    # Example: [[a,b,c],[d,e,f]] with no whitespace.\n    result_str = str(results).replace(\" \", \"\")\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice moves beyond purely geometric similarity to explore the dynamics of the system. Often, the most meaningful conformational states are not those that are geometrically distinct, but those that are kinetically persistent. This exercise guides you through the derivation of Time-lagged Independent Component Analysis (TICA), a powerful method for identifying the slowest, most functionally relevant motions in a trajectory . Mastering this concept allows you to define clusters that represent truly metastable states, a cornerstone of modern kinetic modeling.",
            "id": "3401861",
            "problem": "Consider a molecular dynamics trajectory of a biomolecule sampled at discrete times, producing a mean-free feature vector $x_t \\in \\mathbb{R}^{d}$ for each time $t$, where $d$ is the feature dimension. Assume the process is stationary and ergodic. Define the zero-lag covariance matrix $C_0$ and the time-lagged covariance matrix $C_{\\tau}$ at lag $\\tau$ by $C_0 = \\mathbb{E}[x_t x_t^{\\top}]$ and $C_{\\tau} = \\mathbb{E}[x_t x_{t+\\tau}^{\\top}]$, respectively, where $\\mathbb{E}[\\cdot]$ denotes the expectation with respect to the stationary distribution. Time-lagged Independent Component Analysis (TICA) seeks linear collective variables that maximize time-lagged autocorrelation under unit variance at zero lag.\n\nStarting from these definitions and the physical principle that slow collective variables maximize autocorrelation at lag $\\tau$ while maintaining unit variance at lag $0$, formulate the TICA optimization problem over a direction $w \\in \\mathbb{R}^{d}$ and derive the corresponding stationarity condition that takes the form of a generalized eigenvalue problem, suitable for identifying slow collective variables in conformational ensembles.\n\nThen, consider an illustrative $d=2$ case with\n$$\nC_0 = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix}, \\qquad\nC_{\\tau} = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}.\n$$\nCompute the largest TICA eigenvalue $\\lambda_{\\max}$ implied by the generalized eigenvalue problem. Express your final answer exactly (no rounding). The final answer must be a single real number or a single closed-form analytic expression without units.",
            "solution": "### 1. Formulation of the TICA Optimization Problem\n\nWe seek a linear projection of the feature vector $x_t$ onto a direction vector $w \\in \\mathbb{R}^d$. The resulting scalar time series, or collective variable, is $y_t = w^\\top x_t$.\n\nThe goal of TICA is to find the direction $w$ that corresponds to the slowest process. A slow process is characterized by high autocorrelation over a time lag $\\tau$. The time-lagged autocorrelation of the projected variable $y_t$ is given by:\n$$\n\\mathbb{E}[y_t y_{t+\\tau}] = \\mathbb{E}[(w^\\top x_t)(w^\\top x_{t+\\tau})] = \\mathbb{E}[w^\\top x_t x_{t+\\tau}^\\top w] = w^\\top \\mathbb{E}[x_t x_{t+\\tau}^\\top] w = w^\\top C_\\tau w\n$$\nTo make the problem well-posed and select for significant motions, we add the constraint that the projected variable $y_t$ must have unit variance at zero lag. Since the input data $x_t$ is mean-free, the mean of $y_t$ is also zero ($\\mathbb{E}[y_t] = w^\\top \\mathbb{E}[x_t] = 0$). Therefore, the variance is:\n$$\n\\text{Var}(y_t) = \\mathbb{E}[y_t^2] - (\\mathbb{E}[y_t])^2 = \\mathbb{E}[(w^\\top x_t)^2] = \\mathbb{E}[w^\\top x_t x_t^\\top w] = w^\\top C_0 w\n$$\nThe constraint is thus $w^\\top C_0 w = 1$.\n\nCombining the objective and the constraint, the TICA optimization problem is to find the vector $w$ that solves:\n$$\n\\max_w w^\\top C_\\tau w \\quad \\text{subject to} \\quad w^\\top C_0 w = 1\n$$\nThis is an instance of maximizing a Rayleigh quotient.\n\n### 2. Derivation of the Generalized Eigenvalue Problem\n\nWe solve this constrained optimization problem using the method of Lagrange multipliers. The Lagrangian $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(w, \\lambda) = w^\\top C_\\tau w - \\lambda(w^\\top C_0 w - 1)\n$$\nTo find the stationary points, we take the derivative of $\\mathcal{L}$ with respect to $w$ and set it to zero. Using matrix calculus identities $\\nabla_w (w^\\top A w) = 2Aw$ for symmetric $A$:\n$$\n\\nabla_w \\mathcal{L} = 2C_\\tau w - 2\\lambda C_0 w = 0\n$$\nThis simplifies to:\n$$\nC_\\tau w = \\lambda C_0 w\n$$\nThis is a **generalized eigenvalue problem**. The eigenvectors $w$ are the TICA components (the directions of slow motion), and the corresponding eigenvalues $\\lambda$ are the maximized time-lagged autocorrelations. The largest eigenvalue corresponds to the slowest process.\n\n### 3. Calculation for the 2x2 Case\n\nGiven the matrices:\n$$\nC_0 = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix}, \\qquad\nC_{\\tau} = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\nWe need to solve the generalized eigenvalue problem $C_\\tau w = \\lambda C_0 w$. Since $C_0$ is invertible, we can transform this into a standard eigenvalue problem by left-multiplying by $C_0^{-1}$:\n$$\nC_0^{-1} C_\\tau w = \\lambda w\n$$\nFirst, we compute the inverse of $C_0$:\n$$\n\\det(C_0) = (4)(3) - (1)(1) = 11\n$$\n$$\nC_0^{-1} = \\frac{1}{11} \\begin{pmatrix} 3 & -1 \\\\ -1 & 4 \\end{pmatrix}\n$$\nNext, we compute the product $M = C_0^{-1} C_\\tau$:\n$$\nM = \\frac{1}{11} \\begin{pmatrix} 3 & -1 \\\\ -1 & 4 \\end{pmatrix} \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} (3)(3)+(-1)(1) & (3)(1)+(-1)(2) \\\\ (-1)(3)+(4)(1) & (-1)(1)+(4)(2) \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} 8 & 1 \\\\ 1 & 7 \\end{pmatrix}\n$$\nNow we find the eigenvalues of $M$ by solving the characteristic equation $\\det(M - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} \\frac{8}{11} - \\lambda & \\frac{1}{11} \\\\ \\frac{1}{11} & \\frac{7}{11} - \\lambda \\end{pmatrix} = 0\n$$\n$$\n\\left(\\frac{8}{11} - \\lambda\\right)\\left(\\frac{7}{11} - \\lambda\\right) - \\left(\\frac{1}{11}\\right)^2 = 0\n$$\n$$\n\\frac{56}{121} - \\frac{8}{11}\\lambda - \\frac{7}{11}\\lambda + \\lambda^2 - \\frac{1}{121} = 0\n$$\n$$\n\\lambda^2 - \\frac{15}{11}\\lambda + \\frac{55}{121} = 0\n$$\nSimplifying the constant term gives:\n$$\n\\lambda^2 - \\frac{15}{11}\\lambda + \\frac{5}{11} = 0\n$$\nWe solve this quadratic equation for $\\lambda$ using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{\\frac{15}{11} \\pm \\sqrt{\\left(-\\frac{15}{11}\\right)^2 - 4(1)\\left(\\frac{5}{11}\\right)}}{2} = \\frac{\\frac{15}{11} \\pm \\sqrt{\\frac{225}{121} - \\frac{20}{11}}}{2}\n$$\n$$\n\\lambda = \\frac{\\frac{15}{11} \\pm \\sqrt{\\frac{225 - 220}{121}}}{2} = \\frac{\\frac{15}{11} \\pm \\sqrt{\\frac{5}{121}}}{2} = \\frac{\\frac{15}{11} \\pm \\frac{\\sqrt{5}}{11}}{2}\n$$\n$$\n\\lambda = \\frac{15 \\pm \\sqrt{5}}{22}\n$$\nThe two eigenvalues are $\\lambda_1 = \\frac{15 + \\sqrt{5}}{22}$ and $\\lambda_2 = \\frac{15 - \\sqrt{5}}{22}$. The problem asks for the largest TICA eigenvalue, $\\lambda_{\\max}$.\n$$\n\\lambda_{\\max} = \\frac{15 + \\sqrt{5}}{22}\n$$",
            "answer": "$$\n\\boxed{\\frac{15 + \\sqrt{5}}{22}}\n$$"
        }
    ]
}