{
    "hands_on_practices": [
        {
            "introduction": "The concept of a constraint force, represented by a Lagrange multiplier $\\lambda$, can often feel abstract. This first exercise provides a concrete link between theory and practice by asking you to reconstruct this force from simulation output. By analyzing the position corrections generated by the popular SHAKE algorithm within a Velocity Verlet integrator, you will derive a direct relationship between a tangible numerical quantity, $\\Delta \\mathbf{r}_i$, and the underlying constraint force, $\\mathbf{F}_i^{\\mathrm{c}}$. This practice solidifies the understanding that constraint algorithms are not magic; they are direct, physical consequences of the integration scheme used to enforce geometric restrictions .",
            "id": "2453554",
            "problem": "You have run a classical molecular dynamics simulation using the velocity Verlet integrator with the SHAKE constraint algorithm to enforce a holonomic bond-length constraint between atoms $i$ and $j$, expressed as $\\sigma(\\mathbf{r}) = 0$ with $\\sigma(\\mathbf{r}) = \\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert^2 - d^2$. At each time step, the integrator first computes unconstrained positions and then applies SHAKE position corrections $\\Delta \\mathbf{r}_i$ and $\\Delta \\mathbf{r}_j$ so that the constraint is satisfied at the end of the step. The simulation saved the atom masses $m_i$ and $m_j$, the time step $\\Delta t$, and the SHAKE position corrections $\\Delta \\mathbf{r}_i$ and $\\Delta \\mathbf{r}_j$ for each step, but did not store any Lagrange multipliers. Without rerunning the simulation, how can you reconstruct the instantaneous constraint force vector exerted on atom $i$ by the constraint at a given step?\n\nChoose the best expression or procedure below. Assume the standard formulation of holonomic constraints with Lagrange multipliers and that forces are approximated as constant over a single time step in the velocity Verlet position update.\n\nA. Use the recorded SHAKE position correction and compute $\\mathbf{F}_i^{\\mathrm{c}}$ directly as $\\mathbf{F}_i^{\\mathrm{c}} = \\dfrac{2 m_i}{\\Delta t^2}\\,\\Delta \\mathbf{r}_i$.\n\nB. Infer an effective spring constant $k$ by matching the distribution of bond-length deviations to a harmonic model, then compute $\\mathbf{F}_i^{\\mathrm{c}} = -k\\left(\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert - d\\right)\\,\\dfrac{\\mathbf{r}_i - \\mathbf{r}_j}{\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert}$.\n\nC. It is impossible to reconstruct $\\mathbf{F}_i^{\\mathrm{c}}$ without rerunning the trajectory while explicitly outputting Lagrange multipliers.\n\nD. Project the unconstrained interatomic force onto the bond direction and negate it: $\\mathbf{F}_i^{\\mathrm{c}} = -\\left[\\mathbf{F}_i \\cdot \\dfrac{\\mathbf{r}_i - \\mathbf{r}_j}{\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert}\\right]\\dfrac{\\mathbf{r}_i - \\mathbf{r}_j}{\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert}$, because the constraint cancels the component that would change the bond length.\n\nOnly one option is correct. Provide your reasoning from fundamental principles to justify your choice.",
            "solution": "The problem statement has been validated as scientifically grounded, well-posed, and objective. It provides a solvable scenario rooted in the standard principles of computational chemistry.\n\nThe objective is to find an expression for the instantaneous constraint force, $\\mathbf{F}_i^{\\mathrm{c}}$, on atom $i$ at a given time step, using the provided data: atom mass $m_i$, time step $\\Delta t$, and the SHAKE position correction $\\Delta \\mathbf{r}_i$.\n\nWe begin with the Lagrangian equations of motion for a constrained system. For atom $i$, the total force is the sum of the unconstrained force $\\mathbf{F}_i^{\\text{un}}$ (derived from the potential energy function) and the constraint force $\\mathbf{F}_i^{\\mathrm{c}}$:\n$$ m_i \\ddot{\\mathbf{r}}_i = \\mathbf{F}_i(t) = \\mathbf{F}_i^{\\text{un}}(t) + \\mathbf{F}_i^{\\mathrm{c}}(t) $$\nThe velocity Verlet algorithm, which is a second-order accurate integrator, approximates the position of atom $i$ at time $t+\\Delta t$ based on its state at time $t$. The position update formula is derived from a Taylor series expansion of the position $\\mathbf{r}_i(t)$:\n$$ \\mathbf{r}_i(t+\\Delta t) = \\mathbf{r}_i(t) + \\mathbf{v}_i(t)\\Delta t + \\frac{1}{2}\\mathbf{a}_i(t)(\\Delta t)^2 + \\mathcal{O}((\\Delta t)^3) $$\nwhere $\\mathbf{v}_i(t)$ is the velocity and $\\mathbf{a}_i(t) = \\ddot{\\mathbf{r}}_i(t)$ is the acceleration at time $t$. Substituting the equation of motion, we have $\\mathbf{a}_i(t) = \\mathbf{F}_i(t)/m_i$. The Verlet algorithm truncates the series, using the force at time $t$ as constant over the step $\\Delta t$ for the position update. This is consistent with the assumption provided in the problem statement.\n$$ \\mathbf{r}_i(t+\\Delta t) = \\mathbf{r}_i(t) + \\mathbf{v}_i(t)\\Delta t + \\frac{(\\Delta t)^2}{2m_i}\\mathbf{F}_i(t) $$\nSubstituting the partitioned force $\\mathbf{F}_i(t) = \\mathbf{F}_i^{\\text{un}}(t) + \\mathbf{F}_i^{\\mathrm{c}}(t)$:\n$$ \\mathbf{r}_i(t+\\Delta t) = \\mathbf{r}_i(t) + \\mathbf{v}_i(t)\\Delta t + \\frac{(\\Delta t)^2}{2m_i} \\left( \\mathbf{F}_i^{\\text{un}}(t) + \\mathbf{F}_i^{\\mathrm{c}}(t) \\right) $$\nThis equation can be separated into two parts, which reflects the procedure of the SHAKE algorithm:\n$$ \\mathbf{r}_i(t+\\Delta t) = \\left[ \\mathbf{r}_i(t) + \\mathbf{v}_i(t)\\Delta t + \\frac{(\\Delta t)^2}{2m_i} \\mathbf{F}_i^{\\text{un}}(t) \\right] + \\left[ \\frac{(\\Delta t)^2}{2m_i} \\mathbf{F}_i^{\\mathrm{c}}(t) \\right] $$\nThe first bracketed term represents the updated position of atom $i$ considering only the unconstrained forces. This is precisely what the problem describes as the \"unconstrained positions\" computed by the integrator, let us call them $\\mathbf{r}'_i(t+\\Delta t)$.\n$$ \\mathbf{r}'_i(t+\\Delta t) = \\mathbf{r}_i(t) + \\mathbf{v}_i(t)\\Delta t + \\frac{(\\Delta t)^2}{2m_i} \\mathbf{F}_i^{\\text{un}}(t) $$\nThe second bracketed term represents the additional displacement due to the constraint force, which is applied to restore the bond length to its constrained value $d$. The problem defines this displacement as the \"SHAKE position correction\", $\\Delta \\mathbf{r}_i$.\n$$ \\Delta \\mathbf{r}_i = \\mathbf{r}_i(t+\\Delta t) - \\mathbf{r}'_i(t+\\Delta t) = \\frac{(\\Delta t)^2}{2m_i} \\mathbf{F}_i^{\\mathrm{c}}(t) $$\nThis equation establishes a direct relationship between the constraint force at the beginning of the step, $\\mathbf{F}_i^{\\mathrm{c}}(t)$, and the position correction $\\Delta \\mathbf{r}_i$ applied during that step. We can rearrange this equation to solve for the constraint force:\n$$ \\mathbf{F}_i^{\\mathrm{c}}(t) = \\frac{2 m_i}{(\\Delta t)^2} \\Delta \\mathbf{r}_i $$\nSince the values for $m_i$, $\\Delta t$, and $\\Delta \\mathbf{r}_i$ are saved from the simulation, it is possible to reconstruct the instantaneous constraint force $\\mathbf{F}_i^{\\mathrm{c}}$ at each time step.\n\nNow, we evaluate each of the given options.\n\nA. Use the recorded SHAKE position correction and compute $\\mathbf{F}_i^{\\mathrm{c}}$ directly as $\\mathbf{F}_i^{\\mathrm{c}} = \\dfrac{2 m_i}{\\Delta t^2}\\,\\Delta \\mathbf{r}_i$.\nThis expression is identical to the one derived from the fundamental principles of the velocity Verlet integrator and the SHAKE algorithm. The derivation shows that the position correction $\\Delta \\mathbf{r}_i$ is the integrated effect of the constraint force $\\mathbf{F}_i^{\\mathrm{c}}$ over the time step $\\Delta t$, within the second-order approximation of the integrator.\nVerdict: **Correct**.\n\nB. Infer an effective spring constant $k$ by matching the distribution of bond-length deviations to a harmonic model, then compute $\\mathbf{F}_i^{\\mathrm{c}} = -k\\left(\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert - d\\right)\\,\\dfrac{\\mathbf{r}_i - \\mathbf{r}_j}{\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert}$.\nThis option is fundamentally incorrect. The SHAKE algorithm enforces a *rigid* constraint, meaning the bond length is fixed (ideally, $\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert = d$). This option proposes to replace the rigid constraint model with a flexible harmonic spring model. The force in a harmonic model is proportional to the deviation from the equilibrium distance. In a perfectly constrained system, this deviation is zero, which would incorrectly imply zero force. The constraint force is not a function of bond length deviation; it is the force required to counteract all other influences (unconstrained forces and inertial effects) that would alter the bond length. This method describes an approximation, not a reconstruction of the original simulation's constraint force.\nVerdict: **Incorrect**.\n\nC. It is impossible to reconstruct $\\mathbf{F}_i^{\\mathrm{c}}$ without rerunning the trajectory while explicitly outputting Lagrange multipliers.\nThis statement is proven false by the derivation for option A. The SHAKE position correction $\\Delta \\mathbf{r}_i$ is a direct consequence of the constraint force (which is determined by the Lagrange multiplier $\\lambda$). Having $\\Delta \\mathbf{r}_i$ allows us to calculate $\\mathbf{F}_i^{\\mathrm{c}}$ directly, without needing to know the intermediate value of $\\lambda$. The necessary information is encoded in the position corrections.\nVerdict: **Incorrect**.\n\nD. Project the unconstrained interatomic force onto the bond direction and negate it: $\\mathbf{F}_i^{\\mathrm{c}} = -\\left[\\mathbf{F}_i \\cdot \\dfrac{\\mathbf{r}_i - \\mathbf{r}_j}{\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert}\\right]\\dfrac{\\mathbf{r}_i - \\mathbf{r}_j}{\\left\\lVert \\mathbf{r}_i - \\mathbf{r}_j \\right\\rVert}$, because the constraint cancels the component that would change the bond length.\nThis is a common but incorrect simplification. The constraint force must counteract not only the component of the unconstrained force along the bond, but also inertial effects. The correct Lagrange multiplier expression, derived from the condition $\\ddot\\sigma=0$, shows that the constraint force depends on the unconstrained forces on *both* atoms ($i$ and $j$), their masses ($m_i, m_j$), and a velocity-dependent term $\\left\\lVert \\mathbf{v}_i - \\mathbf{v}_j \\right\\rVert^2$ which acts like a centrifugal force. Option D neglects the contribution from atom $j$, the masses, and the velocity term entirely.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "While the laws of physics are independent of the coordinate system we choose, the numerical stability and efficiency of our simulations are not. This exercise explores the profound impact of coordinate choice by comparing constraint enforcement in Cartesian coordinates versus generalized internal coordinates for a simple molecule. Through a combination of analytical derivation and numerical implementation, you will discover how a poorly chosen coordinate system can lead to ill-conditioned linear algebra problems, especially near certain geometries. This hands-on practice demonstrates that a thoughtful problem formulation is as critical as a correct physical model for achieving accurate and stable simulations .",
            "id": "3416352",
            "problem": "Consider a planar triatomic molecule with atoms labeled $1$, $2$, and $3$. Let the generalized internal coordinates be $q = [r_{12}, r_{23}, \\theta]^\\top$, where $r_{12}$ is the bond length between atom $1$ and atom $2$ expressed in ångström, $r_{23}$ is the bond length between atom $2$ and atom $3$ expressed in ångström, and $\\theta$ is the angle formed at atom $2$ (between the vector from atom $2$ to atom $1$ and the vector from atom $2$ to atom $3$) expressed in radians. Define a fixed body frame mapping from internal coordinates $q$ to Cartesian coordinates $x \\in \\mathbb{R}^{6}$ by setting atom $1$ at the origin, atom $2$ on the $x$-axis, and atom $3$ at a position determined by $r_{23}$ and $\\theta$. Specifically, use the mapping\n$$\nx(q) = \\begin{bmatrix}\nx_{1x} \\\\ x_{1y} \\\\ x_{2x} \\\\ x_{2y} \\\\ x_{3x} \\\\ x_{3y}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0 \\\\\nr_{12} \\\\ 0 \\\\\nr_{12} + r_{23}\\cos\\theta \\\\\nr_{23}\\sin\\theta\n\\end{bmatrix}.\n$$\nConsider two holonomic constraints that keep the bond lengths near target values:\n$$\ng_1(x) = \\left\\| x_2 - x_1 \\right\\| - r_{12}^{\\text{target}}, \\quad\ng_2(x) = \\left\\| x_3 - x_2 \\right\\| - r_{23}^{\\text{target}},\n$$\nwhere $r_{12}^{\\text{target}}$ and $r_{23}^{\\text{target}}$ are target bond lengths in ångström. These constraints are enforced via Lagrange multipliers in the context of Molecular Dynamics (MD). Define the constraint Jacobian in Cartesian coordinates $C_{\\text{cartesian}}(x) = \\partial g/\\partial x \\in \\mathbb{R}^{2\\times 6}$ and the Jacobian of the coordinate mapping $J(q) = \\partial x/\\partial q \\in \\mathbb{R}^{6\\times 3}$.\n\nYour tasks are:\n- Starting from the definition of holonomic constraints and the chain rule for multivariate functions, derive the relationship between the constraint gradient expressed in generalized internal coordinates and the constraint gradient expressed in Cartesian coordinates for the mapping $x(q)$.\n- For the given planar triatomic mapping, compute the Jacobian $J(q)$ explicitly.\n- Implement a numerical procedure to construct $C_{\\text{cartesian}}(x)$, $J(q)$, and the constraint Jacobian in internal coordinates computed via the chain rule. Verify numerically that the internal-coordinate constraint Jacobian matches the analytical internal-coordinate gradient for this specific mapping.\n- Compare enforcing the linearized constraints in Cartesian versus internal coordinates by solving the minimal-norm linearized correction problems:\n    - Cartesian-space correction: find $\\delta x$ that solves $C_{\\text{cartesian}}\\delta x = -g(x)$ in the least-norm sense.\n    - Internal-space correction: find $\\delta q$ that solves $C_{\\text{internal}}\\delta q = -g(x)$ in the least-norm sense, then map to Cartesian via $\\delta x = J \\delta q$.\n  Use a small Tikhonov regularization term added to the $2\\times 2$ Gram matrix to handle potential near-singular geometries.\n\nAnalyze numerical conditioning and accuracy:\n- Compute the condition number of the $2\\times 2$ Gram matrices $G_{\\text{cartesian}} = C_{\\text{cartesian}}C_{\\text{cartesian}}^\\top$ and $G_{\\text{internal}} = C_{\\text{internal}}C_{\\text{internal}}^\\top$.\n- Evaluate an amplification factor defined as $\\|\\delta x\\|_2 / \\|g\\|_2$ for both Cartesian and internal enforcement.\n- Evaluate the exact post-correction constraint residual norm $\\|g(x+\\delta x)\\|_2$ for both methods.\n- Evaluate the Frobenius-norm difference between the chain-rule computed $C_{\\text{internal}}$ and the analytically derived internal-coordinate gradient for this mapping.\n\nUse the following test suite of parameters. For each test case, set $r_{12}^{\\text{target}} = r_{12}(1+\\varepsilon_1)$ and $r_{23}^{\\text{target}} = r_{23}(1+\\varepsilon_2)$, where $\\varepsilon_1$ and $\\varepsilon_2$ are given. Angles must be in radians and lengths must be in ångström. Add a small deterministic noise vector $n = [s, -s]^\\top$ with $s = 10^{-6}$ ångström to the constraint residual vector to probe sensitivity.\n\nTest suite (each case is $(r_{12}, r_{23}, \\theta, \\varepsilon_1, \\varepsilon_2)$):\n- Case A (happy path): $(1.5, 1.2, 1.0, 0.02, -0.015)$.\n- Case B (near-collinear): $(1.0, 1.0, 0.001, 0.05, 0.05)$.\n- Case C (near $\\pi$): $(1.0, 1.0, \\pi - 0.001, -0.02, 0.03)$.\n- Case D (bond-length disparity): $(2.0, 0.5, 1.5, 0.01, -0.01)$.\n\nYour program must, for each test case, compute and output the following seven floating-point results in a fixed order:\n1. The condition number of $G_{\\text{cartesian}}$ (dimensionless).\n2. The condition number of $G_{\\text{internal}}$ (dimensionless).\n3. The amplification factor in Cartesian coordinates $\\|\\delta x_{\\text{cartesian}}\\|_2 / \\|g\\|_2$ (dimensionless).\n4. The amplification factor in internal coordinates $\\|\\delta x_{\\text{internal}}\\|_2 / \\|g\\|_2$ (dimensionless).\n5. The exact post-correction residual norm in Cartesian coordinates $\\|g(x+\\delta x_{\\text{cartesian}})\\|_2$ (in ångström).\n6. The exact post-correction residual norm in internal coordinates $\\|g(x+\\delta x_{\\text{internal}})\\|_2$ (in ångström).\n7. The Frobenius-norm difference between the chain-rule computed internal constraint Jacobian and the analytical internal constraint Jacobian (dimensionless).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets. Each inner list corresponds to one test case, in the order of the test suite, and contains the seven values specified above, e.g., \"[[v11,v12,...,v17],[v21,v22,...,v27],...]\".",
            "solution": "The problem requires a detailed analysis and comparison of two methods for enforcing holonomic constraints on a planar triatomic molecule: one operating in Cartesian coordinates and the other in generalized internal coordinates. The analysis involves deriving the relevant Jacobians, solving linearized constraint equations, and evaluating numerical stability and accuracy metrics.\n\n### Step 1: Theoretical Derivations\n\nFirst, we establish the mathematical framework connecting the Cartesian and internal coordinate representations.\n\n**Relationship between Jacobians via Chain Rule**\nLet the holonomic constraints be a vector function of Cartesian coordinates, $g(x) \\in \\mathbb{R}^2$. The Cartesian coordinates $x \\in \\mathbb{R}^6$ are themselves a function of the generalized internal coordinates $q \\in \\mathbb{R}^3$, given by the mapping $x(q)$. The constraints can be expressed in terms of internal coordinates by composition, $g(x(q))$.\n\nThe chain rule for multivariate functions provides the relationship between the constraint Jacobian in internal coordinates, $C_{\\text{internal}} = \\partial g / \\partial q$, and the constraint Jacobian in Cartesian coordinates, $C_{\\text{cartesian}} = \\partial g / \\partial x$.\n$$\n\\frac{\\partial g}{\\partial q} = \\frac{\\partial g}{\\partial x} \\frac{\\partial x}{\\partial q}\n$$\nUsing the problem's notation, where $J(q) = \\partial x / \\partial q$ is the Jacobian of the coordinate mapping, this relationship is:\n$$\nC_{\\text{internal}}(q) = C_{\\text{cartesian}}(x(q)) J(q)\n$$\nHere, $C_{\\text{cartesian}}$ is a $2 \\times 6$ matrix, $J(q)$ is a $6 \\times 3$ matrix, and their product $C_{\\text{internal}}$ is a $2 \\times 3$ matrix, as expected.\n\n**Explicit Jacobian of the Coordinate Mapping, $J(q)$**\nThe generalized coordinates are $q = [r_{12}, r_{23}, \\theta]^\\top$. The mapping to Cartesian coordinates $x = [x_{1x}, x_{1y}, x_{2x}, x_{2y}, x_{3x}, x_{3y}]^\\top$ is:\n$$\nx(q) = [0, 0, r_{12}, 0, r_{12} + r_{23}\\cos\\theta, r_{23}\\sin\\theta]^\\top\n$$\nThe Jacobian $J(q) = \\partial x / \\partial q$ is a $6 \\times 3$ matrix whose columns are the partial derivatives of $x$ with respect to each component of $q$.\n$$\n\\frac{\\partial x}{\\partial r_{12}} = [0, 0, 1, 0, 1, 0]^\\top\n$$\n$$\n\\frac{\\partial x}{\\partial r_{23}} = [0, 0, 0, 0, \\cos\\theta, \\sin\\theta]^\\top\n$$\n$$\n\\frac{\\partial x}{\\partial \\theta} = [0, 0, 0, 0, -r_{23}\\sin\\theta, r_{23}\\cos\\theta]^\\top\n$$\nAssembling these columns gives the Jacobian matrix:\n$$\nJ(q) = \\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n1 & \\cos\\theta & -r_{23}\\sin\\theta \\\\\n0 & \\sin\\theta & r_{23}\\cos\\theta\n\\end{bmatrix}\n$$\n\n**Explicit Cartesian Constraint Jacobian, $C_{\\text{cartesian}}(x)$**\nThe constraints are $g_1(x) = \\|x_2 - x_1\\| - r_{12}^{\\text{target}}$ and $g_2(x) = \\|x_3 - x_2\\| - r_{23}^{\\text{target}}$, where $x_i = [x_{ix}, x_{iy}]^\\top$. The rows of $C_{\\text{cartesian}} = \\partial g / \\partial x$ are the gradients of $g_1$ and $g_2$. The gradient of a norm $\\|v\\|$ is $v/\\|v\\|$.\n$$\n\\nabla_x g_1 = \\left[ \\frac{-(x_2-x_1)^\\top}{\\|x_2-x_1\\|}, \\frac{(x_2-x_1)^\\top}{\\|x_2-x_1\\|}, \\mathbf{0}^\\top \\right]\n$$\n$$\n\\nabla_x g_2 = \\left[ \\mathbf{0}^\\top, \\frac{-(x_3-x_2)^\\top}{\\|x_3-x_2\\|}, \\frac{(x_3-x_2)^\\top}{\\|x_3-x_2\\|} \\right]\n$$\nEvaluating at the configuration $x=x(q)$, we have $x_2-x_1 = [r_{12}, 0]^\\top$ and $x_3-x_2 = [r_{23}\\cos\\theta, r_{23}\\sin\\theta]^\\top$. Their norms are $r_{12}$ and $r_{23}$ respectively.\nSubstituting these into the gradient expressions yields:\n$$\nC_{\\text{cartesian}}(x(q)) = \\begin{bmatrix}\n-1 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & -\\cos\\theta & -\\sin\\theta & \\cos\\theta & \\sin\\theta\n\\end{bmatrix}\n$$\n\n**Internal Constraint Jacobian, $C_{\\text{internal}}(q)$**\nWe can now compute $C_{\\text{internal}}$ using the chain rule:\n$$\nC_{\\text{internal}} = C_{\\text{cartesian}}(x(q)) J(q) = \n\\begin{bmatrix}\n-1 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & -\\cos\\theta & -\\sin\\theta & \\cos\\theta & \\sin\\theta\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n1 & \\cos\\theta & -r_{23}\\sin\\theta \\\\\n0 & \\sin\\theta & r_{23}\\cos\\theta\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{bmatrix}\n$$\nThis result, derived via the chain rule, must be compared to the analytical Jacobian.\nAnalytically, the constraints expressed directly in internal coordinates are $g_1(q) = r_{12} - r_{12}^{\\text{target}}$ and $g_2(q) = r_{23} - r_{23}^{\\text{target}}$. The analytical Jacobian $\\partial g / \\partial q$ is therefore:\n$$\nC_{\\text{internal}}^{\\text{analytical}} = \\frac{\\partial}{\\partial (r_{12}, r_{23}, \\theta)} \\begin{bmatrix} r_{12} - r_{12}^{\\text{target}} \\\\ r_{23} - r_{23}^{\\text{target}} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\n$$\nThe chain rule result perfectly matches the analytical derivation, as expected. The numerical implementation will verify this to within machine precision.\n\n### Step 2: Numerical Procedure and Analysis\n\nThe core of the task is to solve the linearized constraint equation $C \\delta y = -g$ for a correction vector $\\delta y$, where $(C, \\delta y, g)$ can be in either Cartesian or internal coordinates. Since the systems are underdetermined, we seek the minimum-norm solution, which is found by solving for Lagrange multipliers.\n\n**Correction in Cartesian Coordinates**\nWe solve $C_{\\text{cartesian}} \\delta x = -g(x)$ for the minimum-norm $\\delta x$. The solution is formulated via Lagrange multipliers $\\lambda \\in \\mathbb{R}^2$:\n$$\n\\delta x = C_{\\text{cartesian}}^\\top \\lambda, \\quad \\text{where} \\quad (C_{\\text{cartesian}}C_{\\text{cartesian}}^\\top) \\lambda = -g(x)\n$$\nThe matrix $G_{\\text{cartesian}} = C_{\\text{cartesian}}C_{\\text{cartesian}}^\\top$ is the Gram matrix. With the derived $C_{\\text{cartesian}}$, we have:\n$$\nG_{\\text{cartesian}} = \\begin{bmatrix} 2 & -\\cos\\theta \\\\ -\\cos\\theta & 2 \\end{bmatrix}\n$$\nTo handle potential near-singularity, we introduce a Tikhonov regularization parameter $\\alpha > 0$ (chosen as $\\alpha = 10^{-14}$) and solve $(G_{\\text{cartesian}} + \\alpha I) \\lambda = -(g(x)+n)$, where $n$ is the specified noise term.\n\n**Correction in Internal Coordinates**\nSimilarly, we solve $C_{\\text{internal}} \\delta q = -g(x(q))$ for the minimum-norm $\\delta q$. The solution is:\n$$\n\\delta q = C_{\\text{internal}}^\\top \\mu, \\quad \\text{where} \\quad (C_{\\text{internal}}C_{\\text{internal}}^\\top) \\mu = -g(x(q))\n$$\nThe Gram matrix is $G_{\\text{internal}} = C_{\\text{internal}}C_{\\text{internal}}^\\top = I_{2 \\times 2}$, the $2 \\times 2$ identity matrix. This matrix is perfectly conditioned. We solve the regularized system $(G_{\\text{internal}} + \\alpha I) \\mu = -(g(x(q))+n)$. The Cartesian correction is then found by the linear mapping $\\delta x = J(q) \\delta q$.\n\n**Analysis Metrics**\nThe numerical analysis focuses on the following metrics:\n1.  **Condition Number:** $\\kappa(G) = \\|\\lambda_{\\max}\\|/\\|\\lambda_{\\min}\\|$. For $G_{\\text{cartesian}}$, $\\kappa = (2+|\\cos\\theta|)/(2-|\\cos\\theta|)$, which degrades as $\\theta$ approaches $0$ or $\\pi$ (collinear geometries). For $G_{\\text{internal}}$, $\\kappa=1$ for all geometries.\n2.  **Amplification Factor:** $\\|\\delta x\\|_2 / \\|g+n\\|_2$. This measures the sensitivity of the output correction to the input constraint violation.\n3.  **Post-correction Residual:** $\\|g(x+\\delta x)\\|_2$. Since the constraints are non-linear, a linear correction step will not perfectly zero the residual. A smaller value indicates a more accurate correction.\n4.  **Jacobian Verification:** $\\|C_{\\text{internal}}^{\\text{chain}} - C_{\\text{internal}}^{\\text{analytical}}\\|_F$, which should be close to floating-point zero.\n\nThe implementation will compute these seven quantities for each test case, providing a quantitative comparison between the two coordinate systems for constraint enforcement. The internal coordinates are expected to show superior numerical stability and efficiency due to the perfectly conditioned Gram matrix.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the molecular dynamics constraint problem for a series of test cases.\n    \"\"\"\n\n    def perform_analysis(r12, r23, theta, eps1, eps2):\n        \"\"\"\n        Performs the full numerical analysis for a single test case.\n        \"\"\"\n        # A small Tikhonov regularization parameter\n        alpha = 1e-14\n        \n        # Small deterministic noise vector\n        s = 1e-6\n        noise = np.array([s, -s])\n\n        # 1. Define internal coordinates and map to Cartesian\n        q = np.array([r12, r23, theta])\n        \n        c, s_theta = np.cos(theta), np.sin(theta)\n\n        # Mapping x(q)\n        x = np.array([\n            0.0,\n            0.0,\n            r12,\n            0.0,\n            r12 + r23 * c,\n            r23 * s_theta\n        ])\n\n        # 2. Define constraint function and target values\n        r12_target = r12 * (1.0 + eps1)\n        r23_target = r23 * (1.0 + eps2)\n\n        def g(x_vec):\n            # x_vec is a 6-element NumPy array\n            x1 = x_vec[0:2]\n            x2 = x_vec[2:4]\n            x3 = x_vec[4:6]\n            g1 = np.linalg.norm(x2 - x1) - r12_target\n            g2 = np.linalg.norm(x3 - x2) - r23_target\n            return np.array([g1, g2])\n\n        # Evaluate initial constraint violation vector and add noise\n        g_val = g(x)\n        g_noisy = g_val + noise\n\n        # 3. Compute Jacobians\n        # Mapping Jacobian J(q) = dx/dq\n        J = np.array([\n            [0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0],\n            [1.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0],\n            [1.0, c, -r23 * s_theta],\n            [0.0, s_theta, r23 * c]\n        ])\n\n        # Cartesian constraint Jacobian C_cartesian(x(q))\n        C_cart = np.array([\n            [-1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, -c, -s_theta, c, s_theta]\n        ])\n\n        # Internal constraint Jacobian via chain rule\n        C_int_chain = C_cart @ J\n        \n        # Analytical internal constraint Jacobian\n        C_int_analytical = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n        \n        # Metric 7: Frobenius norm difference\n        fro_diff = np.linalg.norm(C_int_chain - C_int_analytical, 'fro')\n\n        # 4. Cartesian-space correction\n        G_cart = C_cart @ C_cart.T\n        cond_G_cart = np.linalg.cond(G_cart) # Metric 1\n        \n        # Solve for Lagrange multipliers lambda\n        rhs = -g_noisy\n        lambda_p = np.linalg.solve(G_cart + alpha * np.identity(2), rhs)\n        \n        # Compute correction dx_cart\n        dx_cart = C_cart.T @ lambda_p\n        \n        # Compute amplification factor\n        g_norm = np.linalg.norm(g_noisy)\n        amp_factor_cart = np.linalg.norm(dx_cart) / g_norm # Metric 3\n        \n        # Compute post-correction residual\n        residual_norm_cart = np.linalg.norm(g(x + dx_cart)) # Metric 5\n        \n        # 5. Internal-space correction\n        G_int = C_int_chain @ C_int_chain.T\n        cond_G_int = np.linalg.cond(G_int) # Metric 2\n        \n        # Solve for Lagrange multipliers mu\n        mu_p = np.linalg.solve(G_int + alpha * np.identity(2), rhs)\n        \n        # Compute correction dq and map to dx_int\n        dq = C_int_chain.T @ mu_p\n        dx_int = J @ dq\n        \n        # Compute amplification factor\n        amp_factor_int = np.linalg.norm(dx_int) / g_norm # Metric 4\n        \n        # Compute post-correction residual\n        residual_norm_int = np.linalg.norm(g(x + dx_int)) # Metric 6\n        \n        return [\n            cond_G_cart,\n            cond_G_int,\n            amp_factor_cart,\n            amp_factor_int,\n            residual_norm_cart,\n            residual_norm_int,\n            fro_diff\n        ]\n\n    # Test suite from the problem description\n    test_cases = [\n        # (r12, r23, theta, eps1, eps2)\n        (1.5, 1.2, 1.0, 0.02, -0.015),          # Case A (happy path)\n        (1.0, 1.0, 0.001, 0.05, 0.05),         # Case B (near-collinear)\n        (1.0, 1.0, np.pi - 0.001, -0.02, 0.03), # Case C (near pi)\n        (2.0, 0.5, 1.5, 0.01, -0.01)           # Case D (bond-length disparity)\n    ]\n\n    results = []\n    for case in test_cases:\n        result_vector = perform_analysis(*case)\n        results.append(result_vector)\n\n    # Format the final output string\n    output_str_parts = []\n    for res_vec in results:\n        # Format each float to a consistent representation\n        formatted_vec = [f\"{v:.7e}\" for v in res_vec]\n        output_str_parts.append(f\"[{','.join(formatted_vec)}]\")\n    \n    print(f\"[{','.join(output_str_parts)}]\")\n\n# Execute the main function\nsolve()\n```"
        },
        {
            "introduction": "As molecular models grow in complexity, so does the network of constraints used to define their structure. It is surprisingly easy to define a set of constraints that are mathematically redundant, leading to a singular or \"overconstrained\" system that is numerically unsolvable. This final practice tackles this advanced, system-level problem by combining concepts from linear algebra and graph theory. You will implement an algorithm to detect these redundancies by identifying cycles in the constraint graph and propose robust solutions, such as targeted constraint pruning or numerical regularization. This exercise provides a powerful diagnostic toolkit essential for building and simulating complex, highly constrained molecular systems .",
            "id": "3416369",
            "problem": "Consider a molecular dynamics system of $N$ point particles with positions $q = (r_{1,x}, r_{1,y}, r_{1,z}, \\dots, r_{N,x}, r_{N,y}, r_{N,z}) \\in \\mathbb{R}^{3N}$ subject to holonomic constraints, enforced via Lagrange multipliers, that fix relative displacements along edges of an undirected constraint graph. Each constraint is associated with an undirected edge $(i,j)$ of the graph and a Cartesian component $\\alpha \\in \\{x,y,z\\}$, and is given by $g_{(i,j),\\alpha}(q) = r_{i,\\alpha} - r_{j,\\alpha} - d_{(i,j),\\alpha} = 0$, where $d_{(i,j),\\alpha} \\in \\mathbb{R}$ is a specified target displacement component for the edge $(i,j)$. The constraint Jacobian $C(q) = \\partial g / \\partial q$ has one row per scalar constraint and $3N$ columns; for the constraint $g_{(i,j),\\alpha}$, the corresponding row has entries $+1$ at the coordinate $(i,\\alpha)$ and $-1$ at $(j,\\alpha)$, with zeros elsewhere. Let $m$ denotes the total number of scalar constraints, that is $m = 3|E|$ where $|E|$ is the number of edges in the constraint graph.\n\nThe system is consistent if the constraints are independent and the Lagrange multiplier system associated with $C$ is solvable. Overconstraint scenarios arise when constraints are algebraically dependent, typically due to cycles in the constraint graph. The rank $\\operatorname{rank} C$ detects the number of independent constraints, and if $\\operatorname{rank} C < m$ then some constraints are redundant. The matrix governing Lagrange multipliers in mass-weighted form is $G = C M^{-1} C^{\\top}$, where $M$ is the diagonal mass matrix with particle masses along each coordinate. In overconstraint scenarios, $G$ becomes singular or ill-conditioned. Two remedies are: (i) Tikhonov regularization adding $\\lambda I$ to $G$ to raise its smallest eigenvalue above a numerical threshold, and (ii) constraint pruning, removing a minimal set of constraints along cycles to restore independence.\n\nYour task is to derive, implement, and test an algorithm that:\n\n1. Constructs the constraint Jacobian $C$ for the specified set of displacement constraints.\n2. Computes a numerically robust estimate of $\\operatorname{rank} C$ using singular value decomposition with a tolerance.\n3. Detects cycles in the undirected constraint graph and computes the cyclomatic number $\\mu = |E| - |V| + c$, where $|V|$ is the number of vertices and $c$ is the number of connected components.\n4. Determines whether an overconstraint exists specifically due to cycles by checking whether $\\operatorname{rank} C < m$ and $\\mu > 0$.\n5. Proposes a regularization parameter $\\lambda \\ge 0$ such that the smallest eigenvalue of $G + \\lambda I$ is at least a target threshold $\\tau$, where $\\tau$ is chosen based on the largest eigenvalue of $G$ and machine precision. Use $M^{-1}$ formed from particle masses; the mass matrix $M$ is diagonal with blocks $(m_i, m_i, m_i)$ for each particle $i$.\n6. Proposes a constraint pruning strategy that removes exactly one undirected edge per independent cycle (thus removing $3$ scalar constraints per removed edge) by dropping non-tree edges discovered during a spanning forest construction, then recomputes $\\operatorname{rank} C$ on the pruned set to verify independence.\n\nStart from the following base principles: Newton’s second law $\\mathbf{F} = m \\mathbf{a}$; holonomic constraints $g(q) = 0$ enforced via Lagrange multipliers; the definition of the constraint Jacobian $C = \\partial g / \\partial q$. Use singular value decomposition for rank determination and cyclomatic number for cycle detection. Do not assume shortcut formulas beyond these foundations.\n\nImplement the algorithm and evaluate it on the following test suite. Each test case provides $N$, positions $r_i$ (used to construct consistent $d_{(i,j)}$ values via $d_{(i,j)} = r_i - r_j$), an undirected edge list $E$, and a mass array $(m_1, \\dots, m_N)$.\n\n- Test Case 1 (happy path, no cycles):\n  - $N = 4$\n  - Positions: $r_0 = (0,0,0)$, $r_1 = (1,0,0)$, $r_2 = (2,0,0)$, $r_3 = (3,0,0)$\n  - Edges: $(0,1), (1,2), (2,3)$\n  - Masses: $(1.0, 1.0, 1.0, 1.0)$\n\n- Test Case 2 (single cycle, square ring):\n  - $N = 4$\n  - Positions: $r_0 = (0,0,0)$, $r_1 = (1,0,0)$, $r_2 = (1,1,0)$, $r_3 = (0,1,0)$\n  - Edges: $(0,1), (1,2), (2,3), (3,0)$\n  - Masses: $(1.0, 2.0, 1.5, 1.0)$\n\n- Test Case 3 (two cycles, figure-eight):\n  - $N = 5$\n  - Positions: $r_0 = (0,0,0)$, $r_1 = (1,0,0)$, $r_2 = (1,1,0)$, $r_3 = (2,1,0)$, $r_4 = (2,2,0)$\n  - Edges: $(0,1), (1,2), (2,0), (2,3), (3,4), (4,2)$\n  - Masses: $(1.0, 1.2, 1.0, 0.8, 1.5)$\n\n- Test Case 4 (boundary case, broken ring):\n  - $N = 4$\n  - Positions: $r_0 = (0,0,0)$, $r_1 = (1,0,0)$, $r_2 = (1,1,0)$, $r_3 = (0,1,0)$\n  - Edges: $(0,1), (1,2), (2,3)$\n  - Masses: $(1.0, 3.0, 1.0, 2.0)$\n\nFor each test case, your program must compute the following quantities:\n\n- $\\operatorname{rank} C$ (as an integer),\n- $m$ (the total number of scalar constraints, as an integer),\n- $\\mu$ (the cyclomatic number, as an integer),\n- the number of scalar constraints removed by pruning (as an integer),\n- the $\\operatorname{rank} C$ after pruning (as an integer),\n- the regularization parameter $\\lambda$ (as a float),\n- a boolean indicating whether the overconstraint is due to cycles (true if $\\operatorname{rank} C < m$ and $\\mu > 0$, otherwise false).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element in the list should itself be a list in the order described above. For example, the output format must be of the form $[\\,[r_1, m_1, \\mu_1, \\Delta m_1, r^{\\text{pruned}}_1, \\lambda_1, b_1], \\dots, [r_4, m_4, \\mu_4, \\Delta m_4, r^{\\text{pruned}}_4, \\lambda_4, b_4]\\,]$. No physical units are required, and angles are not involved. All boolean values should be printed in their programming language canonical form (for example, true or false).",
            "solution": "The problem requires an analysis of a molecular system with holonomic constraints, focusing on the detection and resolution of overconstraints arising from cycles in the constraint graph. The solution involves a multi-step algorithm that integrates concepts from linear algebra, graph theory, and numerical analysis, grounded in the principles of classical mechanics.\n\nThe foundational principle is the enforcement of holonomic constraints $g(q) = 0$ on a system of $N$ particles with generalized coordinates $q \\in \\mathbb{R}^{3N}$. These constraints are incorporated into the equations of motion using Lagrange multipliers. The system of equations for the Lagrange multipliers $\\lambda$ involves the constraint Jacobian, $C(q) = \\partial g / \\partial q$, and the mass matrix $M$. Specifically, the matrix $G = C M^{-1} C^{\\top}$ must be inverted. If the constraints are not linearly independent, this matrix becomes singular, leading to an overconstrained system.\n\nOur algorithm systematically addresses the tasks specified in the problem statement for each test case.\n\n**1. Constraint Graph and Jacobian Construction**\n\nThe constraints are defined on a graph where vertices represent particles and edges $(i,j)$ represent a fixed relative displacement. Each edge $(i,j)$ corresponds to three scalar constraints, one for each Cartesian component $\\alpha \\in \\{x,y,z\\}$:\n$$g_{(i,j),\\alpha}(q) = r_{i,\\alpha} - r_{j,\\alpha} - d_{(i,j),\\alpha} = 0$$\nwhere $r_{i,\\alpha}$ is the $\\alpha$-coordinate of particle $i$ and $d_{(i,j),\\alpha}$ is a constant target displacement. The total number of scalar constraints is $m = 3|E|$, where $|E|$ is the number of edges in the constraint graph.\n\nThe constraint Jacobian $C$ is an $m \\times 3N$ matrix. Each row of $C$ corresponds to the gradient of one scalar constraint function. For the constraint $g_{(i,j),\\alpha}$, the corresponding row vector has a $+1$ at the column index for coordinate $r_{i,\\alpha}$ and a $-1$ at the column index for $r_{j,\\alpha}$. All other entries are zero. The specific values of the positions $r_i$ and target displacements $d_{(i,j),\\alpha}$ do not affect the Jacobian matrix $C$.\n\n**2. Rank Analysis via Singular Value Decomposition (SVD)**\n\nThe number of linearly independent constraints is given by the rank of the Jacobian matrix, $\\operatorname{rank} C$. A numerically robust method to compute the rank is through SVD. The number of singular values of $C$ that are greater than a small tolerance determines the rank. If $\\operatorname{rank} C < m$, the set of constraints is linearly dependent, indicating redundancy and an overconstrained system.\n\n**3. Graph Cycle Detection and Cyclomatic Number**\n\nRedundancies in this type of constraint system are directly related to cycles in the underlying constraint graph. The sum of relative displacement vectors around any closed loop of particles must be zero, e.g., $(r_i-r_j) + (r_j-r_k) + (r_k-r_i) = 0$. This geometric identity implies a linear dependency among the corresponding constraint functions for each Cartesian component.\n\nTo quantify the number of cycles, we compute the cyclomatic number of the constraint graph G=($V$, $E$):\n$$\\mu = |E| - |V| + c$$\nwhere $|V|=N$ is the number of vertices (particles), $|E|$ is the number of edges (displacement constraints), and $c$ is the number of connected components in the graph. A value $\\mu > 0$ indicates the presence of one or more independent cycles. We determine $c$ by performing a graph traversal, such as a Breadth-First Search (BFS), to count the distinct subgraphs that are internally connected.\n\n**4. Identification of Cycle-Induced Overconstraint**\n\nAn overconstraint is specifically identified as being caused by cycles if both conditions are met: the system is mathematically overconstrained ($\\operatorname{rank} C < m$) and the constraint graph contains cycles ($\\mu > 0$).\n\n**5. Tikhonov Regularization**\n\nOne method to handle the singular matrix $G = C M^{-1} C^{\\top}$ is Tikhonov regularization. We seek a parameter $\\lambda \\ge 0$ to form a regularized matrix $G' = G + \\lambda I$ that is non-singular and well-conditioned. The eigenvalues of $G'$ are $\\lambda'_k = \\lambda_k + \\lambda$, where $\\lambda_k$ are the eigenvalues of $G$. Since $G$ is symmetric positive semi-definite, its eigenvalues are real and non-negative. If $G$ is singular, its smallest eigenvalue $\\lambda_{\\text{min}}$ is $0$.\n\nWe aim to ensure the smallest eigenvalue of $G'$, $\\lambda_{\\text{min}} + \\lambda$, is above a numerical stability threshold $\\tau$. A reasonable choice for this threshold is $\\tau = \\lambda_{\\text{max}} \\cdot \\epsilon_{\\text{machine}}$, where $\\lambda_{\\text{max}}$ is the largest eigenvalue of $G$ and $\\epsilon_{\\text{machine}}$ is machine precision. The required regularization parameter is then:\n$$\\lambda = \\max(0, \\tau - \\lambda_{\\text{min}})$$\nThe mass matrix $M$ is a $3N \\times 3N$ diagonal matrix where the diagonal entries corresponding to particle $i$ are all equal to its mass $m_i$. Its inverse $M^{-1}$ is also diagonal, with entries $1/m_i$.\n\n**6. Constraint Pruning**\n\nA direct approach to eliminate redundancies is to remove constraints until they are linearly independent. For cycle-based redundancies, this corresponds to making the constraint graph acyclic. This is achieved by finding a spanning forest of the graph. A spanning forest contains the maximum number of edges possible without creating a cycle. The edges from the original graph that are not in the spanning forest are the ones that complete the cycles.\n\nWe construct a spanning forest using a graph traversal (e.g., BFS), adding edges to the forest as we discover unvisited vertices. The number of edges in a spanning forest of a graph with $N$ vertices and $c$ components is $N-c$. The number of edges to remove is $|E| - (N-c) = \\mu$. For each edge removed, we remove its three associated scalar constraints. The total number of removed scalar constraints is $3\\mu$. We then construct a new, pruned Jacobian matrix $C_{\\text{pruned}}$ using only the edges in the spanning forest and verify that its rank equals its number of rows, confirming that all redundancies have been removed.\n\nBy implementing these steps, we can fully characterize each test case as requested.",
            "answer": "```python\nimport numpy as np\nfrom collections import deque\n\ndef process_case(N, edges, masses):\n    \"\"\"\n    Processes a single test case according to the problem description.\n    \"\"\"\n    \n    # Task 3 (Part 1): Graph Analysis to find connected components and spanning forest\n    adj = {i: [] for i in range(N)}\n    unique_edges = set()\n    for u, v in edges:\n        adj[u].append(v)\n        adj[v].append(u)\n        unique_edges.add(tuple(sorted((u, v))))\n    \n    num_edges = len(unique_edges)\n    visited = [False] * N\n    num_components = 0\n    spanning_forest_edges = []\n    \n    for i in range(N):\n        if not visited[i]:\n            num_components += 1\n            q = deque([i])\n            visited[i] = True\n            while q:\n                u = q.popleft()\n                for v in adj[u]:\n                    if not visited[v]:\n                        visited[v] = True\n                        q.append(v)\n                        spanning_forest_edges.append(tuple(sorted((u, v))))\n\n    # Task 3 (Part 2): Compute cyclomatic number\n    mu = num_edges - N + num_components\n\n    # Helper function to construct the Jacobian\n    def construct_jacobian(n_particles, edge_list):\n        n_constraints = 3 * len(edge_list)\n        if n_constraints == 0:\n            return np.zeros((0, 3 * n_particles)), 0\n        \n        C = np.zeros((n_constraints, 3 * n_particles))\n        constraint_idx = 0\n        for u, v in edge_list:\n            for alpha in range(3):\n                col_i = 3 * u + alpha\n                col_j = 3 * v + alpha\n                C[constraint_idx, col_i] = 1\n                C[constraint_idx, col_j] = -1\n                constraint_idx += 1\n        return C, n_constraints\n\n    # Task 1: Construct the full constraint Jacobian\n    C, m = construct_jacobian(N, unique_edges)\n\n    # Task 2: Compute rank of the full Jacobian\n    if m > 0:\n        rank_C = np.linalg.matrix_rank(C)\n    else:\n        rank_C = 0\n\n    # Task 4: Detect overconstraint due to cycles\n    is_overconstrained_by_cycles = (rank_C  m) and (mu > 0)\n\n    # Task 6: Propose and evaluate pruning strategy\n    num_removed_scalar_constraints = 3 * mu\n    C_pruned, m_pruned = construct_jacobian(N, spanning_forest_edges)\n    if m_pruned > 0:\n        rank_C_pruned = np.linalg.matrix_rank(C_pruned)\n    else:\n        rank_C_pruned = 0\n\n    # Task 5: Propose regularization parameter lambda\n    if m == 0:\n        lambda_reg = 0.0\n    else:\n        masses_array = np.array(masses, dtype=float)\n        inv_masses_rep = np.repeat(1.0 / masses_array, 3)\n        \n        # G = C M^-1 C^T, computed efficiently\n        # C_scaled_by_mass = C * inv_masses_rep (broadcasting)\n        # G = C_scaled_by_mass @ C.T\n        temp = C * inv_masses_rep\n        G = temp @ C.T\n        \n        eigvals_G = np.linalg.eigvalsh(G)\n        lambda_min = eigvals_G[0]\n        lambda_max = eigvals_G[-1]\n        \n        # Set threshold tau\n        tau = lambda_max * np.finfo(float).eps\n        \n        # Compute lambda\n        lambda_reg = max(0.0, tau - lambda_min)\n\n    return [\n        int(rank_C),\n        int(m),\n        int(mu),\n        int(num_removed_scalar_constraints),\n        int(rank_C_pruned),\n        float(lambda_reg),\n        is_overconstrained_by_cycles\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 4,\n            \"positions\": {0:(0,0,0), 1:(1,0,0), 2:(2,0,0), 3:(3,0,0)},\n            \"edges\": [(0,1), (1,2), (2,3)],\n            \"masses\": [1.0, 1.0, 1.0, 1.0]\n        },\n        {\n            \"N\": 4,\n            \"positions\": {0:(0,0,0), 1:(1,0,0), 2:(1,1,0), 3:(0,1,0)},\n            \"edges\": [(0,1), (1,2), (2,3), (3,0)],\n            \"masses\": [1.0, 2.0, 1.5, 1.0]\n        },\n        {\n            \"N\": 5,\n            \"positions\": {0:(0,0,0), 1:(1,0,0), 2:(1,1,0), 3:(2,1,0), 4:(2,2,0)},\n            \"edges\": [(0,1), (1,2), (2,0), (2,3), (3,4), (4,2)],\n            \"masses\": [1.0, 1.2, 1.0, 0.8, 1.5]\n        },\n        {\n            \"N\": 4,\n            \"positions\": {0:(0,0,0), 1:(1,0,0), 2:(1,1,0), 3:(0,1,0)},\n            \"edges\": [(0,1), (1,2), (2,3)],\n            \"masses\": [1.0, 3.0, 1.0, 2.0]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case['N'], case['edges'], case['masses'])\n        all_results.append(result)\n\n    # Format the output as specified\n    formatted_results = []\n    for res in all_results:\n        res[-1] = str(res[-1]).lower()  # Convert boolean to \"true\"/\"false\"\n        str_res = [str(x) for x in res]\n        formatted_results.append(f\"[{','.join(str_res)}]\")\n    \n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}