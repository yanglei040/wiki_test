{
    "hands_on_practices": [
        {
            "introduction": "A primary goal of parallel computing is to solve a fixed-size problem faster by using more processors. This concept, known as strong scaling, is a critical benchmark for performance. However, ideal speedup is rarely achieved in practice because as computational work is divided among more processors, the cost of communication between them often becomes a dominant bottleneck. This exercise provides a practical method for quantifying this trade-off by calculating the strong scaling efficiency from measured computation and communication timings . Mastering this analysis is the first step toward diagnosing and optimizing the performance of any large-scale molecular dynamics simulation.",
            "id": "3448088",
            "problem": "A three-dimensional molecular dynamics simulation of a dense Lennard–Jones fluid with periodic boundaries is executed using spatial domain decomposition and Message Passing Interface (MPI) halo exchanges. The force calculation employs a cutoff radius $r_c$ with Verlet neighbor lists. Per-timestep wall-clock times are measured on a distributed-memory system for two processor counts, $P=64$ and $P=512$. The per-timestep time is decomposed into a computation part $T_c(P)$ (force and integration work within subdomains) and a communication part $T_m(P)$ (halo exchange and collective reductions). Assume the total per-timestep time satisfies $T(P)=T_c(P)+T_m(P)$ and neglect input/output.\n\nThe measured times are:\n- At $P=64$: $T_c(64)=2.40\\,\\mathrm{ms}$, $T_m(64)=0.35\\,\\mathrm{ms}$.\n- At $P=512$: $T_c(512)=0.34\\,\\mathrm{ms}$, $T_m(512)=0.62\\,\\mathrm{ms}$.\n\nStarting from the fundamental definitions of strong scaling speedup and efficiency, and without introducing any additional empirical models, derive an expression for the strong scaling efficiency at $P=512$ relative to the baseline $P_0=64$ in terms of $T(64)$, $T(512)$, $P$, and $P_0$. Use the measurements above to evaluate this efficiency numerically. As part of your reasoning, identify whether communication dominates at $P=512$ according to the criterion $T_m(P)>T_c(P)$, but report only the efficiency value.\n\nExpress your final answer as a dimensionless decimal rounded to four significant figures.",
            "solution": "The primary task is to derive and calculate the strong scaling efficiency for a molecular dynamics simulation when scaling from a baseline processor count, $P_0$, to a larger count, $P$.\n\nStrong scaling analysis assumes the total problem size (in this case, the number of atoms and the simulation volume) is held constant while the number of processors is increased.\n\nThe speedup, $S(P)$, is defined as the ratio of the serial execution time, $T_{serial}$ (on one processor), to the parallel execution time on $P$ processors, $T(P)$.\n$$ S(P) = \\frac{T_{serial}}{T(P)} $$\nIn many practical scenarios, a true serial run on a single processor is either unavailable or impractical. Instead, performance is measured relative to a baseline run on $P_0$ processors. The relative speedup, $S_{P_0 \\to P}$, when moving from $P_0$ processors to $P$ processors is the ratio of their execution times:\n$$ S_{P_0 \\to P} = \\frac{T(P_0)}{T(P)} $$\nFor ideal or linear scaling, the speedup gained by increasing the processor count from $P_0$ to $P$ would be equal to the ratio of the processor counts, $\\frac{P}{P_0}$.\n\nThe strong scaling efficiency, $E_{P_0 \\to P}$, is the ratio of the observed relative speedup to the ideal linear speedup. This provides a measure of how effectively the additional computational resources are utilized.\n$$ E_{P_0 \\to P} = \\frac{\\text{Observed Speedup}}{\\text{Ideal Speedup}} = \\frac{S_{P_0 \\to P}}{P/P_0} $$\nSubstituting the expression for the relative speedup, we obtain the formula for relative strong scaling efficiency:\n$$ E_{P_0 \\to P} = \\frac{T(P_0)/T(P)}{P/P_0} = \\frac{T(P_0) P_0}{T(P) P} $$\nThis expression for efficiency is derived from fundamental definitions as required and is expressed in terms of the specified variables.\n\nNow, we apply this formula to the given data. The baseline and final processor counts are $P_0 = 64$ and $P = 512$, respectively.\nThe total per-timestep execution time, $T(P)$, is the sum of the computation time, $T_c(P)$, and the communication time, $T_m(P)$:\n$$ T(P) = T_c(P) + T_m(P) $$\nUsing the provided measurements:\nFor the baseline run at $P_0=64$:\n$$ T(64) = T_c(64) + T_m(64) = 2.40\\,\\mathrm{ms} + 0.35\\,\\mathrm{ms} = 2.75\\,\\mathrm{ms} $$\nFor the scaled-up run at $P=512$:\n$$ T(512) = T_c(512) + T_m(512) = 0.34\\,\\mathrm{ms} + 0.62\\,\\mathrm{ms} = 0.96\\,\\mathrm{ms} $$\nBefore calculating the efficiency, we address the secondary question of whether communication dominates at $P=512$. The criterion is $T_m(P) > T_c(P)$.\nAt $P=512$, we have $T_m(512) = 0.62\\,\\mathrm{ms}$ and $T_c(512) = 0.34\\,\\mathrm{ms}$. Since $0.62\\,\\mathrm{ms} > 0.34\\,\\mathrm{ms}$, the condition is met, and communication time indeed dominates over computation time at this scale. This is a common cause of diminishing returns in strong scaling.\n\nWe now substitute the total times and processor counts into the derived efficiency formula:\n$$ E_{64 \\to 512} = \\frac{T(64) \\times 64}{T(512) \\times 512} = \\frac{2.75 \\times 64}{0.96 \\times 512} $$\nThe units of time (ms) cancel, yielding a dimensionless efficiency value.\n$$ E_{64 \\to 512} = \\frac{176}{491.52} $$\nPerforming the division:\n$$ E_{64 \\to 512} \\approx 0.3580729166... $$\nThe problem requires the result to be rounded to four significant figures.\n$$ E_{64 \\to 512} \\approx 0.3581 $$\nThis value signifies that the parallel job achieved approximately $35.81\\%$ of the ideal linear speedup when scaling from $64$ to $512$ processors.",
            "answer": "$$\\boxed{0.3581}$$"
        },
        {
            "introduction": "As we've seen, communication overhead is a primary impediment to achieving ideal parallel scaling. In spatial decomposition schemes, this overhead is largely determined by the halo exchange, where each processor sends and receives particle data from its neighbors. The volume of this data exchange is proportional to the surface area of the subdomains. This raises a crucial design question: for a fixed subdomain volume (and thus a fixed amount of computation), how can we shape the domain to minimize its surface area and, consequently, the communication cost? This practice  offers a quantitative comparison between a \"slab\" decomposition, which has a large surface-to-volume ratio, and a more compact \"pencil\" decomposition, demonstrating the profound impact of domain geometry on communication efficiency.",
            "id": "3448083",
            "problem": "Consider a three-dimensional periodic molecular dynamics system with a uniform number density of particles, where short-range pair forces are evaluated with a spherical cutoff. Let the total number of particles be $N=10^6$ in a cubic domain of side length $L$, and let the force cutoff be $r_c$, implemented by a spatial domain decomposition with halo exchange. The mesh for particle-mesh operations is uniform with $M_x \\times M_y \\times M_z = 256^3$ points, and the cutoff is set to $r_c = 3h$, where $h=L/256$ is the mesh spacing.\n\nAssume the following two decompositions across $P=512$ processors:\n1. Slab decomposition: $P_x=1$, $P_y=1$, $P_z=512$, so each subdomain has dimensions $l_x=L$, $l_y=L$, $l_z=L/512$.\n2. Pencil decomposition: $P_x=16$, $P_y=32$, $P_z=1$, so each subdomain has dimensions $l_x=L/16$, $l_y=L/32$, $l_z=L$.\n\nAssume a standard three-stage halo exchange protocol that guarantees each process receives all particles within a union halo of thickness $r_c$ around its subdomain, so that the expected communicated particle count per process equals the number density $\\rho=N/L^3$ times the union halo volume \n$$\nV_{\\mathrm{halo}}=(l_x+2r_c)(l_y+2r_c)(l_z+2r_c)-l_x l_y l_z.\n$$\nDefine the total expected communication volume as the total expected number of particles sent across the network in one force-exchange step, equal to $P \\rho V_{\\mathrm{halo}}$ when summed over all processes.\n\nUsing only the above assumptions and definitions, compute the ratio $\\mathcal{R}$ of the total expected communication volume for slab decomposition to that for pencil decomposition. Express your final answer as a single real number rounded to four significant figures.",
            "solution": "The objective is to compute the ratio $\\mathcal{R}$ of the total expected communication volume for a slab decomposition to that for a pencil decomposition. The total expected communication volume $C$ is defined as $P \\rho V_{\\mathrm{halo}}$, where $P$ is the number of processors, $\\rho$ is the particle number density, and $V_{\\mathrm{halo}}$ is the volume of the halo region for a single processor's subdomain.\n\nLet $C_S$ and $V_{\\mathrm{halo}, S}$ be the total communication volume and single-subdomain halo volume for the slab decomposition, respectively.\nLet $C_P$ and $V_{\\mathrm{halo}, P}$ be the total communication volume and single-subdomain halo volume for the pencil decomposition, respectively.\n\nThe ratio $\\mathcal{R}$ is given by:\n$$\n\\mathcal{R} = \\frac{C_S}{C_P} = \\frac{P \\rho V_{\\mathrm{halo}, S}}{P \\rho V_{\\mathrm{halo}, P}}\n$$\nSince the number of processors $P$ and the density $\\rho$ are the same for both scenarios, they cancel out, simplifying the ratio to:\n$$\n\\mathcal{R} = \\frac{V_{\\mathrm{halo}, S}}{V_{\\mathrm{halo}, P}}\n$$\nThe problem provides the formula for the halo volume:\n$$\nV_{\\mathrm{halo}} = (l_x+2r_c)(l_y+2r_c)(l_z+2r_c) - l_x l_y l_z\n$$\nwhere $l_x$, $l_y$, and $l_z$ are the dimensions of the subdomain and $r_c$ is the force cutoff radius. Note that the total number of particles $N=10^6$ is not required for this calculation.\n\nTo facilitate the calculation, we express all lengths in terms of the mesh spacing $h$. The problem states:\n- The cubic domain has side length $L$.\n- The mesh has $256^3$ points, so the mesh spacing is $h = L/256$. This means $L = 256h$.\n- The force cutoff is $r_c = 3h$.\n\nFirst, we analyze the slab decomposition.\nThe processors are arranged as $P_x=1$, $P_y=1$, $P_z=512$.\nThe subdomain dimensions for the slab case ($S$) are:\n$l_{x,S} = \\frac{L}{P_x} = \\frac{L}{1} = L = 256h$\n$l_{y,S} = \\frac{L}{P_y} = \\frac{L}{1} = L = 256h$\n$l_{z,S} = \\frac{L}{P_z} = \\frac{L}{512} = \\frac{256h}{512} = 0.5h$\n\nNow we calculate the halo volume, $V_{\\mathrm{halo}, S}$:\n$$\nV_{\\mathrm{halo}, S} = (l_{x,S}+2r_c)(l_{y,S}+2r_c)(l_{z,S}+2r_c) - l_{x,S}l_{y,S}l_{z,S}\n$$\nSubstituting the dimensions in terms of $h$:\n$$\nV_{\\mathrm{halo}, S} = (256h + 2(3h))(256h + 2(3h))(0.5h + 2(3h)) - (256h)(256h)(0.5h)\n$$\n$$\nV_{\\mathrm{halo}, S} = (256h + 6h)(256h + 6h)(0.5h + 6h) - (256 \\times 256 \\times 0.5)h^3\n$$\n$$\nV_{\\mathrm{halo}, S} = (262h)(262h)(6.5h) - (32768)h^3\n$$\n$$\nV_{\\mathrm{halo}, S} = (262^2 \\times 6.5)h^3 - 32768h^3\n$$\n$$\nV_{\\mathrm{halo}, S} = (68644 \\times 6.5)h^3 - 32768h^3\n$$\n$$\nV_{\\mathrm{halo}, S} = 446186h^3 - 32768h^3\n$$\n$$\nV_{\\mathrm{halo}, S} = 413418h^3\n$$\n\nNext, we analyze the pencil decomposition.\nThe processors are arranged as $P_x=16$, $P_y=32$, $P_z=1$.\nThe subdomain dimensions for the pencil case ($P$) are:\n$l_{x,P} = \\frac{L}{P_x} = \\frac{L}{16} = \\frac{256h}{16} = 16h$\n$l_{y,P} = \\frac{L}{P_y} = \\frac{L}{32} = \\frac{256h}{32} = 8h$\n$l_{z,P} = \\frac{L}{P_z} = \\frac{L}{1} = L = 256h$\n\nNow we calculate the halo volume, $V_{\\mathrm{halo}, P}$:\n$$\nV_{\\mathrm{halo}, P} = (l_{x,P}+2r_c)(l_{y,P}+2r_c)(l_{z,P}+2r_c) - l_{x,P}l_{y,P}l_{z,P}\n$$\nSubstituting the dimensions in terms of $h$:\n$$\nV_{\\mathrm{halo}, P} = (16h + 2(3h))(8h + 2(3h))(256h + 2(3h)) - (16h)(8h)(256h)\n$$\n$$\nV_{\\mathrm{halo}, P} = (16h + 6h)(8h + 6h)(256h + 6h) - (16 \\times 8 \\times 256)h^3\n$$\n$$\nV_{\\mathrm{halo}, P} = (22h)(14h)(262h) - (32768)h^3\n$$\n$$\nV_{\\mathrm{halo}, P} = (22 \\times 14 \\times 262)h^3 - 32768h^3\n$$\n$$\nV_{\\mathrm{halo}, P} = (308 \\times 262)h^3 - 32768h^3\n$$\n$$\nV_{\\mathrm{halo}, P} = 80696h^3 - 32768h^3\n$$\n$$\nV_{\\mathrm{halo}, P} = 47928h^3\n$$\n\nFinally, we compute the ratio $\\mathcal{R}$:\n$$\n\\mathcal{R} = \\frac{V_{\\mathrm{halo}, S}}{V_{\\mathrm{halo}, P}} = \\frac{413418h^3}{47928h^3} = \\frac{413418}{47928}\n$$\n$$\n\\mathcal{R} \\approx 8.625801201552328\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\n\\mathcal{R} \\approx 8.626\n$$\nThis result confirms the computational principle that domain decompositions producing subdomains that are more \"cubic\" (like the pencil decomposition here, where dimensions are $16h \\times 8h \\times 256h$) are more efficient in terms of communication volume than highly anisotropic decompositions (like the slab, with dimensions $256h \\times 256h \\times 0.5h$), because they minimize the surface-to-volume ratio of the subdomains.",
            "answer": "$$\n\\boxed{8.626}\n$$"
        },
        {
            "introduction": "The previous practices assumed a spatially uniform system, where dividing the simulation box into equal volumes effectively balances the computational work. However, many scientifically interesting systems are highly non-uniform, such as a liquid droplet coexisting with its vapor or the formation of molecular clusters. In these cases, a simple geometric decomposition leads to severe load imbalance: processors assigned to dense regions are overworked, while those in sparse regions sit idle. The solution is to create domains of unequal volume but equal computational load. This advanced practice  guides you through the implementation of a powerful load-balancing algorithm, weighted recursive bisection (WRB), which partitions the domain based on a physical model of work density, ensuring that every processor receives a fair share of the work.",
            "id": "3448085",
            "problem": "A cubic simulation box of side length $L$ (in $\\mathrm{nm}$) contains a dense spherical droplet of radius $R$ (in $\\mathrm{nm}$) centered at position $x_0$ along the $x$-axis and a sparse vapor elsewhere. Consider a short-range pairwise interaction potential where, by Newton's laws and the locality of forces at a fixed cutoff, the computational work of a Molecular Dynamics (MD) step is dominated by pair evaluations. In a continuum approximation with local number density $n(\\mathbf{r})$ (in $\\mathrm{nm}^{-3}$), the local work density per unit volume is proportional to $n(\\mathbf{r})^2$ because the number of interacting pairs in a small neighborhood scales as the square of the number of particles in that neighborhood. Assume a two-phase approximation with $n(\\mathbf{r}) = n_d$ inside the droplet and $n(\\mathbf{r}) = n_v$ in the vapor, with $n_d \\gg n_v$.\n\nTo construct a one-dimensional decomposition along the $x$-axis for $P$ processors, estimate the per-processor work by integrating the work density over $x$-aligned slabs. Let the box be periodic and the droplet fully contained with $R \\le L/2$ so that cross-sections are unambiguous. The cross-sectional area at a given $x$ that lies inside the droplet is the area of a circle of radius $\\sqrt{R^2 - (x - x_0)^2}$ when $|x - x_0| \\le R$ and zero otherwise, and the remainder of the $y$–$z$ plane up to area $L^2$ is vapor. The slab work per infinitesimal width $\\mathrm{d}x$ is therefore proportional to $n_d^2$ times the inside area plus $n_v^2$ times the outside area. Your goal is to partition the interval $[0,L]$ into $P$ contiguous subdomains such that the estimated total work in each subdomain is equal, using a simple weighted recursive bisection (WRB): recursively split an interval into two subintervals in proportion to the number of processors assigned to each side, placing the split at the $x$ where the cumulative estimated work reaches the required proportion.\n\nImplement the following steps from first principles:\n- Discretize $[0,L]$ into $N_x$ uniform cells with edges $x_i$ and width $\\Delta x = L/N_x$. Approximate the slab work in cell $i$ by evaluating the cross-sectional areas at the cell center and multiplying by $\\Delta x$ to obtain a nonnegative weight $w_i$.\n- Implement weighted recursive bisection on the discrete weights $w_i$ and edges $x_i$ to produce domain boundaries. For a split of an interval assigned $P$ processors into $P_\\ell = \\lfloor P/2 \\rfloor$ and $P_r = P - P_\\ell$, place the split position so that the cumulative weight equals $(P_\\ell/P)$ of the interval’s total. Recurse on each side with $P_\\ell$ and $P_r$, respectively. When splitting inside a cell, linearly interpolate within that cell to place the split and apportion the cell’s weight accordingly, so that the subsequent recursion operates on exact subinterval weights consistent with the split.\n- Return the complete list of domain boundaries including $0$ and $L$.\n\nPhysical units: All length inputs and outputs must be in $\\mathrm{nm}$. Number densities are in $\\mathrm{nm}^{-3}$. The final domain boundary coordinates must be reported in $\\mathrm{nm}$, rounded to $6$ decimals.\n\nTest suite:\n- Case $1$ (asymmetric droplet, power-of-two processors): $L = 10.0$, $R = 2.0$, $x_0 = 6.0$, $n_d = 30.0$, $n_v = 1.0$, $N_x = 2000$, $P = 4$.\n- Case $2$ (asymmetric droplet, non-power-of-two processors): $L = 10.0$, $R = 2.0$, $x_0 = 6.0$, $n_d = 30.0$, $n_v = 1.0$, $N_x = 2000$, $P = 3$.\n- Case $3$ (no droplet, uniform vapor): $L = 10.0$, $R = 0.0$, $x_0 = 6.0$, $n_d = 30.0$, $n_v = 1.0$, $N_x = 2000$, $P = 4$.\n- Case $4$ (centered droplet, two processors): $L = 10.0$, $R = 2.0$, $x_0 = 5.0$, $n_d = 30.0$, $n_v = 1.0$, $N_x = 2000$, $P = 2$.\n\nYour program should compute the boundaries for each case and produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, where each result is itself a list of boundary coordinates from $0$ to $L$ in increasing order, each rounded to $6$ decimals. For example, an output with two cases would look like `[[0.000000,2.500000,5.000000],[0.000000,5.000000,10.000000]]`; you must produce exactly one line in this format with the four cases listed in order.",
            "solution": "The core of the problem is to implement a weighted recursive bisection algorithm on a work profile defined over the spatial domain $[0, L]$. A robust way to implement this is to work with a continuous representation of the cumulative work, approximated from a fine-grained discretization.\n\n**1. Discretized Work Profile**\nFirst, we discretize the domain $[0,L]$ into $N_x$ cells of width $\\Delta x = L/N_x$. For each cell $i \\in \\{0, 1, ..., N_x-1\\}$, we calculate its computational weight, $w_i$. The weight is proportional to the work done in the corresponding slab of volume $L^2 \\Delta x$. We approximate this by evaluating the work density at the center of the cell, $x_{c,i} = (i+0.5)\\Delta x$.\n\nThe work per unit volume is proportional to $n(\\mathbf{r})^2$. The work in a slab of thickness $\\mathrm{d}x$ is found by integrating over its cross-section:\n$$ \\mathrm{d}W \\propto [n_d^2 A_{droplet}(x) + n_v^2 (L^2 - A_{droplet}(x))] \\mathrm{d}x $$\nLet the work profile function be $W'(x) = (n_d^2 - n_v^2)A_{droplet}(x) + n_v^2 L^2$, where $A_{droplet}(x) = \\pi(R^2 - (x - x_0)^2)$ for $|x-x_0| \\le R$ and $0$ otherwise. The weight of cell $i$ is then $w_i = W'(x_{c,i}) \\Delta x$.\n\n**2. Continuous Cumulative Work and its Inverse**\nFrom the discrete weights $w_i$, we can construct an approximate continuous cumulative work function, $C(x)$, representing the total work in the interval $[0,x]$. This is done by summing the weights of all cells fully to the left of $x$ and adding the proportional weight of the cell that $x$ falls into.\n$$ C(x) \\approx \\sum_{j=0}^{i-1} w_j + w_i \\frac{x - x_i}{\\Delta x}, \\quad \\text{where } x \\in [x_i, x_{i+1}] $$\nHere, $x_i = i \\Delta x$ is the left edge of cell $i$. This function can be computed efficiently using a pre-calculated array of cumulative weights, $S_i = \\sum_{j=0}^{i} w_j$.\n\nConversely, we need the inverse function, $C^{-1}(W)$, which gives the coordinate $x$ corresponding to a given cumulative work $W$. This can be found by first identifying the cell $i$ in which the cumulative work $W$ is reached (i.e., $S_{i-1} \\le W < S_i$) and then using linear interpolation within that cell to find the precise coordinate.\n\n**3. Weighted Recursive Bisection (WRB) Algorithm**\nThe WRB algorithm is implemented by managing a queue of tasks. Each task is a tuple `(x_start, x_end, P_current)`, representing an interval $[x_{start}, x_{end}]$ to be partitioned among $P_{current}$ processors.\n\nThe process begins with a single task: $(0, L, P)$. For each task:\n- If $P_{current} \\le 1$, no further splits are needed for this interval.\n- If $P_{current} > 1$, we calculate the number of processors for the left and right sub-problems: $P_\\ell = \\lfloor P_{current}/2 \\rfloor$ and $P_r = P_{current} - P_\\ell$.\n- The interval $[x_{start}, x_{end}]$ must be split at a coordinate $x_{split}$ such that the work is partitioned in the ratio $P_\\ell : P_r$. The fraction of work for the left interval is $f = P_\\ell / P_{current}$.\n- We find the total work in the current interval, $W_{interval} = C(x_{end}) - C(x_{start})$.\n- The target cumulative work for the split point, measured from the origin, is $W_{target} = C(x_{start}) + f \\cdot W_{interval}$.\n- The split coordinate is then found using the inverse cumulative work function: $x_{split} = C^{-1}(W_{target})$.\n- This new boundary $x_{split}$ is recorded, and two new tasks, $(x_{start}, x_{split}, P_\\ell)$ and $(x_{split}, x_{end}, P_r)$, are added to the queue.\n\nThis process continues until all intervals are assigned to a single processor. The final result is the sorted list of unique boundary coordinates, augmented with $0$ and $L$.",
            "answer": "```\n[[0.000000,4.717036,6.000000,7.282964,10.000000],[0.000000,5.127264,6.872736,10.000000],[0.000000,2.500000,5.000000,7.500000,10.000000],[0.000000,5.000000,10.000000]]\n```"
        }
    ]
}