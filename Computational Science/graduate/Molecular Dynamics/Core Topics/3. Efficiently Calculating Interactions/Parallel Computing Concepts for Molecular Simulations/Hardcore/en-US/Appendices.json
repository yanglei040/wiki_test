{
    "hands_on_practices": [
        {
            "introduction": "Spatial decomposition is the cornerstone of parallelizing molecular simulations with short-range forces. This first practice focuses on the fundamental step of setting up the computational grid. By calculating the optimal number of cells based on physical parameters like the interaction cutoff $r_c$ and a safety margin $\\Delta$, you will gain a concrete understanding of how geometric constraints directly influence the granularity of parallel tasks .",
            "id": "3431951",
            "problem": "A three-dimensional molecular dynamics simulation of a simple fluid of uniform number density is run in a cubic box of side length $L$ with periodic boundary conditions. To enable efficient parallel neighbor searching with a cell list, the domain is partitioned into a uniform grid of cubic cells. The cell edge length $a$ must be chosen such that it satisfies the neighbor-list safety condition $a \\geq l_c$, where the list cell size $l_c$ equals the sum of a conservative cutoff radius $r_c$ for pair interactions and a nonbonded skin distance $\\Delta$ used to amortize neighbor-list rebuilds. The implementation selects an integer number of cells $n$ per edge so that $a = L/n \\geq l_c$ while maximizing $n$ under this constraint, and the total number of cells is $N_{\\text{cell}} = n^3$. The average number of atoms per cell is the total atoms divided by the number of cells.\n\nGiven $r_c = 1.0$ nm, $\\Delta = 0.2$ nm, $\\rho = 33$ nm$^{-3}$, and a cubic box of side $L = 20$ nm, compute:\n- the total number of cells $N_{\\text{cell}}$ in the grid, and\n- the average number of atoms per cell.\n\nUse the list cell size $l_c = r_c + \\Delta$, treat the number density $\\rho$ as uniform, and assume the total number of atoms is $N = \\rho L^3$. Report the number of cells as a dimensionless integer and the average number of atoms per cell as a dimensionless quantity rounded to four significant figures.",
            "solution": "The problem is validated as self-contained, consistent, and scientifically sound. It represents a standard procedure in setting up a molecular dynamics simulation with domain decomposition. We proceed with the solution.\n\nThe objective is to compute $2$ quantities: the total number of cells, $N_{\\text{cell}}$, and the average number of atoms per cell.\n\nFirst, we determine the optimal number of cells per edge, $n$. The implementation requires that the cell edge length, $a$, must satisfy the safety condition $a \\geq l_c$, where $l_c$ is the list cell size. The list cell size is the sum of the cutoff radius, $r_c$, and the nonbonded skin distance, $\\Delta$. Given $r_c = 1.0$ nm and $\\Delta = 0.2$ nm, we calculate $l_c$:\n$$l_c = r_c + \\Delta = 1.0 \\text{ nm} + 0.2 \\text{ nm} = 1.2 \\text{ nm}$$\nThe cell edge length $a$ is defined by the total box length $L$ and the number of cells per edge $n$ as $a = L/n$. Substituting this into the safety condition:\n$$\\frac{L}{n} \\geq l_c$$\nThe problem specifies that $n$ must be an integer and maximized under this constraint. We can rearrange the inequality to solve for $n$:\n$$n \\leq \\frac{L}{l_c}$$\nUsing the given box side length $L = 20$ nm and the calculated value of $l_c = 1.2$ nm:\n$$n \\leq \\frac{20 \\text{ nm}}{1.2 \\text{ nm}} = \\frac{200}{12} = \\frac{50}{3} \\approx 16.667$$\nSince $n$ must be an integer, the maximum value for $n$ that satisfies this condition is the floor of $16.667$:\n$$n = \\lfloor 16.667 \\rfloor = 16$$\nNow that we have determined the number of cells per edge, we can calculate the total number of cells in the $3$-dimensional grid, $N_{\\text{cell}}$.\n$$N_{\\text{cell}} = n^3 = 16^3 = 4096$$\nThis is the first required quantity.\n\nSecond, we compute the average number of atoms per cell. This is given by the total number of atoms, $N$, divided by the total number of cells, $N_{\\text{cell}}$. The total number of atoms is determined by the uniform number density, $\\rho$, and the volume of the simulation box, $V = L^3$. Given $\\rho = 33 \\text{ nm}^{-3}$ and $L = 20$ nm:\n$$N = \\rho V = \\rho L^3 = (33 \\text{ nm}^{-3}) \\times (20 \\text{ nm})^3 = 33 \\times 8000 = 264000$$\nThe average number of atoms per cell, which we denote as $\\langle N_{\\text{atoms/cell}} \\rangle$, is therefore:\n$$\\langle N_{\\text{atoms/cell}} \\rangle = \\frac{N}{N_{\\text{cell}}} = \\frac{264000}{4096}$$\nPerforming the division gives the exact value:\n$$\\langle N_{\\text{atoms/cell}} \\rangle = 64.453125$$\nThe problem requires this value to be reported rounded to $4$ significant figures. The first $4$ significant figures are $6$, $4$, $4$, and $5$. The fifth significant digit is $3$, which is less than $5$, so we round down.\n$$\\langle N_{\\text{atoms/cell}} \\rangle \\approx 64.45$$\nThus, the $2$ computed quantities are $N_{\\text{cell}} = 4096$ and the average number of atoms per cell is approximately $64.45$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 4096  64.45 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Effective parallelization extends beyond simply dividing the work; it requires optimizing how each processor handles its data. This exercise delves into the critical impact of memory layout on performance by comparing the Array-of-Structures (AoS) and Structure-of-Arrays (SoA) schemes. By developing a simple but powerful performance model, you will learn to analyze the trade-offs associated with memory access patterns like unit-stride versus gather/scatter, a crucial skill for writing high-performance, hardware-aware scientific code .",
            "id": "3431984",
            "problem": "Consider a classical short-range pairwise molecular dynamics kernel that, for each central particle $i$, evaluates pair interactions with $w$ neighbors $\\{j_{1},\\dots,j_{w}\\}$ using Single Instruction Multiple Data (SIMD) width $w$. Assume the following well-tested performance modeling bases:\n\n1. The time per SIMD vector iteration is the sum of an arithmetic term and a memory traffic term, where the latter is proportional to the number of cache lines moved. This is consistent with the roofline-style decomposition in which arithmetic and data movement contribute additively to execution time when neither is fully overlapped.\n2. For memory, the cost to transfer one cache line is a constant $c_{L}$ (in cycles per line) from the dominant memory hierarchy level, and non-unit-stride vector loads (gather) and non-unit-stride vector stores (scatter) add per-line overheads $c_{g}$ and $c_{s}$, respectively. Unit-stride loads and stores incur no gather/scatter overhead.\n3. Cache lines have size $B$ bytes. Scalar values (e.g., one coordinate component) have size $a$ bytes. For an Array of Structures (AoS) layout, each particle occupies a structure of size $s$ bytes containing at least three position components and three force components. For a Structure of Arrays (SoA) layout, positions and forces are stored in separate arrays, one per component (e.g., $x$, $y$, $z$ for positions and $f_{x}$, $f_{y}$, $f_{z}$ for forces), each element occupying $a$ bytes.\n\nAssume a short-range neighbor list such that, within one SIMD vector iteration, the $w$ neighbor indices $\\{j_{k}\\}$ are effectively uncorrelated in memory (no reuse within the vector), so that:\n- Under SoA, the number of cache lines touched to load the $w$ neighbor positions is $L_{\\mathrm{SoA},j}^{\\mathrm{load}} = 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil$, and to store the $w$ neighbor force updates is $L_{\\mathrm{SoA},j}^{\\mathrm{store}} = 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil$.\n- Under AoS, the number of cache lines touched to load the $w$ neighbor positions (as structures) is $L_{\\mathrm{AoS},j}^{\\mathrm{load}} = \\left\\lceil \\frac{w s}{B} \\right\\rceil$, and to store the $w$ neighbor force updates is $L_{\\mathrm{AoS},j}^{\\mathrm{store}} = \\left\\lceil \\frac{w s}{B} \\right\\rceil$.\n\nFor the central particle $i$, assume its data are accessed with unit stride so that:\n- Under SoA, the positions of $i$ touch $L_{\\mathrm{SoA},i} = \\left\\lceil \\frac{3 a}{B} \\right\\rceil$ cache lines.\n- Under AoS, the structure of $i$ touches $L_{\\mathrm{AoS},i} = \\left\\lceil \\frac{s}{B} \\right\\rceil$ cache lines.\n\nLet the arithmetic cost per pair interaction be $c_{c}$ cycles (this includes floating-point force evaluation but excludes memory costs). Assume no other memory traffic dominates (e.g., neighbor indices are negligible in comparison to positions and forces for the purposes of this question), and that there is no overlap between arithmetic and memory costs.\n\nDefine throughput as the number of pair interactions completed per processor cycle. Using only the above modeling assumptions and definitions, derive closed-form expressions for the throughput of the SIMD vector iteration under AoS and SoA layouts, denoted $T_{\\mathrm{AoS}}(w,B,a,s,c_{c},c_{L},c_{g},c_{s})$ and $T_{\\mathrm{SoA}}(w,B,a,s,c_{c},c_{L},c_{g},c_{s})$, respectively, each measured in interactions per cycle. Express your final answer as analytical expressions in terms of $w$, $B$, $a$, $s$, $c_{c}$, $c_{L}$, $c_{g}$, and $c_{s}$. Do not include any units in your final expressions. No numerical evaluation is required.",
            "solution": "We begin with the roofline-style principle that the time per SIMD vector iteration is the sum of an arithmetic contribution and a data movement contribution. Let one SIMD vector iteration process $w$ pair interactions, namely the interactions of the central particle $i$ with the $w$ neighbors $\\{j_{1},\\dots,j_{w}\\}$. The throughput, measured in interactions per cycle, is the number of interactions completed divided by the total cycles taken for that iteration. Therefore, if the total cycles per iteration is $C_{\\mathrm{iter}}$, then the throughput is $T = \\frac{w}{C_{\\mathrm{iter}}}$.\n\nWe now construct $C_{\\mathrm{iter}}$ under each memory layout from first principles.\n\nArithmetic cost. By assumption, the arithmetic cost per pair interaction is $c_{c}$ cycles and is independent of layout. For $w$ pairs per iteration, the arithmetic cost is\n$$\nC_{\\mathrm{comp}} = c_{c} \\, w.\n$$\n\nMemory cost model. Each cache line transfer costs $c_{L}$ cycles, independently of whether it is a load or a store, when counted at the dominant memory hierarchy level. Additionally, non-unit-stride vector loads (gather) incur an overhead $c_{g}$ per cache line touched by the gather, and non-unit-stride vector stores (scatter) incur an overhead $c_{s}$ per cache line touched by the scatter. Unit-stride accesses do not incur these gather/scatter overheads.\n\nAccess pattern characterization for a short-range neighbor list. Within one SIMD vector iteration, the neighbor indices $\\{j_{k}\\}$ are assumed uncorrelated in memory. This implies that, at vector granularity, the $w$ elements drawn from arrays (SoA) or from structures (AoS) span enough addresses that the number of distinct cache lines touched is well approximated by the total bytes addressed divided by the cache line size $B$, rounded up.\n\nUnder Structure of Arrays (SoA), each coordinate component is stored in a dedicated array with element size $a$ bytes. To load the $w$ neighbor positions, we must gather from the $x$, $y$, and $z$ arrays, $w$ elements each. The number of cache lines touched to load these is\n$$\nL_{\\mathrm{SoA},j}^{\\mathrm{load}} = 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil.\n$$\nSimilarly, to scatter the force updates for the $w$ neighbors to the $f_{x}$, $f_{y}$, and $f_{z}$ arrays, the number of cache lines touched is\n$$\nL_{\\mathrm{SoA},j}^{\\mathrm{store}} = 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil.\n$$\nFor the central particle $i$, whose data are accessed with unit stride, the number of cache lines touched to load its position components is\n$$\nL_{\\mathrm{SoA},i} = \\left\\lceil \\frac{3 a}{B} \\right\\rceil.\n$$\nBecause these are unit-stride, there is no gather overhead for $i$.\n\nUnder Array of Structures (AoS), each particle occupies a structure of size $s$ bytes that includes at least the three position components and three force components. The $w$ neighbor positions are gathered by reading $w$ structures. The number of cache lines touched to load is\n$$\nL_{\\mathrm{AoS},j}^{\\mathrm{load}} = \\left\\lceil \\frac{w s}{B} \\right\\rceil,\n$$\nand the number of cache lines touched to store the $w$ neighbor force updates is\n$$\nL_{\\mathrm{AoS},j}^{\\mathrm{store}} = \\left\\lceil \\frac{w s}{B} \\right\\rceil.\n$$\nFor the central particle $i$, accessed with unit stride, the number of cache lines touched to load its structure is\n$$\nL_{\\mathrm{AoS},i} = \\left\\lceil \\frac{s}{B} \\right\\rceil.\n$$\n\nCycle accounting under SoA. The total number of cache lines moved (loads plus stores) per iteration is\n$$\nL_{\\mathrm{SoA}}^{\\mathrm{tot}} = L_{\\mathrm{SoA},j}^{\\mathrm{load}} + L_{\\mathrm{SoA},j}^{\\mathrm{store}} + L_{\\mathrm{SoA},i} = 2 \\cdot 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil + \\left\\lceil \\frac{3 a}{B} \\right\\rceil.\n$$\nThe gather overhead applies to the neighbor loads only, adding $c_{g}$ per cache line touched in $L_{\\mathrm{SoA},j}^{\\mathrm{load}}$. The scatter overhead applies to the neighbor stores only, adding $c_{s}$ per cache line touched in $L_{\\mathrm{SoA},j}^{\\mathrm{store}}$. There is no gather/scatter overhead for the unit-stride central particle $i$ loads. Therefore, the total cycles per iteration under SoA are\n$$\nC_{\\mathrm{iter}}^{\\mathrm{SoA}} = c_{c} \\, w \\;+\\; c_{L} \\left( 2 \\cdot 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil + \\left\\lceil \\frac{3 a}{B} \\right\\rceil \\right) \\;+\\; c_{g} \\cdot 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil \\;+\\; c_{s} \\cdot 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil.\n$$\nIt is convenient to combine the gather and scatter overheads for neighbors:\n$$\nC_{\\mathrm{iter}}^{\\mathrm{SoA}} = c_{c} \\, w \\;+\\; c_{L} \\left( 6 \\left\\lceil \\frac{w a}{B} \\right\\rceil + \\left\\lceil \\frac{3 a}{B} \\right\\rceil \\right) \\;+\\; (c_{g} + c_{s}) \\cdot 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil.\n$$\nThus the SoA throughput, in interactions per cycle, is\n$$\nT_{\\mathrm{SoA}}(w,B,a,s,c_{c},c_{L},c_{g},c_{s}) \\;=\\; \\frac{w}{\\,c_{c} \\, w \\;+\\; c_{L} \\left( 6 \\left\\lceil \\frac{w a}{B} \\right\\rceil + \\left\\lceil \\frac{3 a}{B} \\right\\rceil \\right) \\;+\\; (c_{g} + c_{s}) \\cdot 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil\\,}.\n$$\n\nCycle accounting under AoS. The total number of cache lines moved (loads plus stores) per iteration is\n$$\nL_{\\mathrm{AoS}}^{\\mathrm{tot}} = L_{\\mathrm{AoS},j}^{\\mathrm{load}} + L_{\\mathrm{AoS},j}^{\\mathrm{store}} + L_{\\mathrm{AoS},i} = 2 \\left\\lceil \\frac{w s}{B} \\right\\rceil + \\left\\lceil \\frac{s}{B} \\right\\rceil.\n$$\nAs above, the neighbor loads incur gather overhead and the neighbor stores incur scatter overhead, each per cache line. There is no gather/scatter overhead for the unit-stride central particle $i$. Therefore, the total cycles per iteration under AoS are\n$$\nC_{\\mathrm{iter}}^{\\mathrm{AoS}} = c_{c} \\, w \\;+\\; c_{L} \\left( 2 \\left\\lceil \\frac{w s}{B} \\right\\rceil + \\left\\lceil \\frac{s}{B} \\right\\rceil \\right) \\;+\\; c_{g} \\left\\lceil \\frac{w s}{B} \\right\\rceil \\;+\\; c_{s} \\left\\lceil \\frac{w s}{B} \\right\\rceil,\n$$\nor equivalently,\n$$\nC_{\\mathrm{iter}}^{\\mathrm{AoS}} = c_{c} \\, w \\;+\\; c_{L} \\left( 2 \\left\\lceil \\frac{w s}{B} \\right\\rceil + \\left\\lceil \\frac{s}{B} \\right\\rceil \\right) \\;+\\; (c_{g} + c_{s}) \\left\\lceil \\frac{w s}{B} \\right\\rceil.\n$$\nThus the AoS throughput, in interactions per cycle, is\n$$\nT_{\\mathrm{AoS}}(w,B,a,s,c_{c},c_{L},c_{g},c_{s}) \\;=\\; \\frac{w}{\\,c_{c} \\, w \\;+\\; c_{L} \\left( 2 \\left\\lceil \\frac{w s}{B} \\right\\rceil + \\left\\lceil \\frac{s}{B} \\right\\rceil \\right) \\;+\\; (c_{g} + c_{s}) \\left\\lceil \\frac{w s}{B} \\right\\rceil\\,}.\n$$\n\nThese expressions explicitly exhibit the dependence of throughput on the SIMD width $w$, cache line size $B$, scalar size $a$, structure size $s$, arithmetic cost $c_{c}$, cache line transfer cost $c_{L}$, and gather/scatter overheads $c_{g}$ and $c_{s}$, under the stated short-range, uncorrelated-neighbor assumptions.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\dfrac{w}{\\,c_{c} \\, w \\;+\\; c_{L} \\left( 2 \\left\\lceil \\dfrac{w s}{B} \\right\\rceil + \\left\\lceil \\dfrac{s}{B} \\right\\rceil \\right) \\;+\\; (c_{g} + c_{s}) \\left\\lceil \\dfrac{w s}{B} \\right\\rceil\\,} \n\\dfrac{w}{\\,c_{c} \\, w \\;+\\; c_{L} \\left( 6 \\left\\lceil \\dfrac{w a}{B} \\right\\rceil + \\left\\lceil \\dfrac{3 a}{B} \\right\\rceil \\right) \\;+\\; (c_{g} + c_{s}) \\cdot 3 \\left\\lceil \\dfrac{w a}{B} \\right\\rceil\\,}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In many simulations, particle distributions are not uniform and change over time, leading to load imbalance that cripples parallel efficiency. This advanced practice addresses this challenge by having you model a dynamic load balancing strategy using a Hilbert Space-Filling Curve (SFC) to re-partition the domain. You will implement a hysteresis mechanism to prevent \"domain thrashing,\" exploring the sophisticated interplay between maintaining perfect balance and ensuring algorithmic stability .",
            "id": "3431963",
            "problem": "Consider a two-dimensional Molecular Dynamics (MD) domain with short-range interactions and spatially nonuniform particle density. The domain is a square of side length $L=\\,1$ (in meters), discretized into a uniform grid of $M \\times M$ cells, where $M=2^k$ for some integer $k$. The total number of particles is $N$, and the instantaneous particle number in cell $i$ at time $t$ is $n_i(t)$. For short-range interactions with a fixed cutoff and a typical neighbor count, the computational cost per particle is approximately constant locally. However, for nonuniform densities, neighbor list maintenance and force evaluations induce an additional local complexity that scales with local occupancy. A widely used load model is\n\n$$\nc_i(t) = \\alpha\\, n_i(t) + \\beta\\, n_i^2(t),\n$$\n\nwhere $\\alpha0$ and $\\beta\\ge 0$ are constants measuring, respectively, linear per-particle cost and superlinear overhead due to neighbor-list updates and cache effects.\n\nAssume a spatial density field\n\n$$\n\\rho(x,y,t) = \\rho_0 \\left( 1 + A \\sin\\!\\left( 2\\pi \\left( \\frac{x}{L} + \\frac{y}{L} \\right) + \\omega t \\right) \\right),\n$$\n\nwith $L=\\;1$ meter, base density $\\rho_0 = \\frac{N}{L^2}$, amplitude $A \\in [0,1)$, and angular frequency $\\omega$ (in radians per second). Let cell centers be at $(x_{p}, y_{q}) = \\left( \\frac{p+1/2}{M}L, \\frac{q+1/2}{M}L \\right)$ for $p,q \\in \\{0,\\dots,M-1\\}$. Define a nonnegative field\n\n$$\nf_{p,q}(t) = 1 + A \\sin\\!\\left( 2\\pi \\left( \\frac{x_{p}}{L} + \\frac{y_{q}}{L} \\right) + \\omega t \\right),\n$$\n\nand set expected occupancy per cell as\n\n$$\nn_{p,q}(t) = N \\cdot \\frac{f_{p,q}(t)}{\\sum_{p',q'} f_{p',q'}(t)}.\n$$\n\nThus, $\\sum_{p,q} n_{p,q}(t) = N$ for all $t$, and the computational load per cell $c_{p,q}(t)$ follows from the given $c_i(t)$ model.\n\nParallel decomposition uses a Hilbert Space-Filling Curve (SFC): map each cell $(p,q)$ to a one-dimensional Hilbert index $h(p,q) \\in \\{0,\\dots,M^2-1\\}$. Let $\\ell_d(t)$ be the load sequence ordered by increasing Hilbert index, with cumulative loads $\\mathcal{L}_d(t) = \\sum_{u=0}^{d} \\ell_u(t)$. For $P$ processing elements, a perfectly balanced partition at time $t$ places contiguous Hilbert segments so that the segment boundaries $\\{b_j(t)\\}_{j=1}^{P}$, with $b_P(t)=M^2$, satisfy\n\n$$\n\\mathcal{L}_{b_j(t)-1}(t) \\le j \\cdot \\frac{\\mathcal{L}_{M^2-1}(t)}{P} \\quad \\text{and} \\quad \\mathcal{L}_{b_j(t)}(t) \\ge j \\cdot \\frac{\\mathcal{L}_{M^2-1}(t)}{P},\n$$\n\nfor $j=1,\\dots,P-1$, ensuring contiguous segments from index $0$ to $b_1(t)-1$, $b_1(t)$ to $b_2(t)-1$, and so on.\n\nAdaptive redistribution operates at discrete update times $t_k = k \\tau_u$ with update interval $\\tau_u$ (in seconds). Between updates, the partition boundaries remain unchanged. At an update time $t_k$, define the current partition boundaries as $\\{b_j^{\\text{prev}}\\}$ and compute the instantaneous perfectly balanced candidate boundaries $\\{b_j^{\\text{new}}(t_k)\\}$. Let the segment loads under $\\{b_j^{\\text{prev}}\\}$ at time $t_k$ be $\\{L_j^{\\text{prev}}(t_k)\\}_{j=1}^{P}$ and under $\\{b_j^{\\text{new}}(t_k)\\}$ be $\\{L_j^{\\text{new}}(t_k)\\}_{j=1}^{P}$. Define the instantaneous mean segment load\n\n$$\n\\bar{L}(t_k) = \\frac{1}{P} \\sum_{j=1}^{P} L_j^{\\text{prev}}(t_k),\n$$\n\nand the relative imbalance under the current boundaries\n\n$$\nI_{\\text{prev}}(t_k) = \\frac{\\max_j L_j^{\\text{prev}}(t_k)}{\\bar{L}(t_k)} - 1.\n$$\n\nLet the normalized boundary movement be\n\n$$\nD(t_k) = \\frac{1}{P\\, M^2} \\sum_{j=1}^{P} \\left| b_j^{\\text{new}}(t_k) - b_j^{\\text{prev}} \\right|.\n$$\n\nWith hysteresis threshold $\\delta \\in [0,1)$, the update rule is:\n- If $I_{\\text{prev}}(t_k) \\le \\delta$ and $D(t_k) \\le \\delta$, then keep $\\{b_j^{\\text{prev}}\\}$.\n- Otherwise, apply $\\{b_j^{\\text{new}}(t_k)\\}$.\n\nDefine domain thrashing as changes of the applied boundaries at an update time. Let the thrashing count over the simulation be $\\Theta$ (an integer equal to the number of updates where boundaries change), and let the total number of updates be $K$. Define the stability index\n\n$$\nS = 1 - \\frac{\\Theta}{K},\n$$\n\nwhich lies in $[0,1]$, and the average thrashing severity\n\n$$\nT = \\frac{1}{K} \\sum_{k=1}^{K} D(t_k),\n$$\n\nwhere $D(t_k)$ is computed with respect to the applied update decision at $t_k$. Additionally, define the time-averaged relative imbalance across all time steps (including non-update times) as\n\n$$\n\\overline{I} = \\frac{1}{N_t} \\sum_{m=1}^{N_t} \\left( \\frac{\\max_j L_j(t_m)}{\\frac{1}{P} \\sum_{j=1}^{P} L_j(t_m)} - 1 \\right),\n$$\n\nwhere $t_m = m\\, \\Delta t$, $\\Delta t$ is the simulation time step (in seconds), and $N_t$ is the number of time steps.\n\nImplement an algorithm that:\n- Computes $\\ell_d(t)$ from $c_{p,q}(t)$ via Hilbert ordering at each time $t$.\n- Initializes boundaries at $t_0 = 0$ using the perfectly balanced partition.\n- Applies the hysteresis rule at update times $t_k$ with interval $\\tau_u$.\n- Tracks $\\Theta$, $S$, $T$, and $\\overline{I}$.\n\nYour program must process the following test suite. Each case specifies $(M, N, P, \\alpha, \\beta, A, \\omega, \\Delta t, T_{\\text{end}}, \\tau_u, \\delta)$:\n\n- Case $1$: $(M=\\;32, N=\\;4096, P=\\;8, \\alpha=\\;1.0, \\beta=\\;0.001, A=\\;0.5, \\omega=\\;\\pi \\text{ rad/s}, \\Delta t=\\;0.05 \\text{ s}, T_{\\text{end}}=\\;5.0 \\text{ s}, \\tau_u=\\;0.1 \\text{ s}, \\delta=\\;0.0)$.\n- Case $2$: $(M=\\;32, N=\\;4096, P=\\;8, \\alpha=\\;1.0, \\beta=\\;0.001, A=\\;0.5, \\omega=\\;\\pi \\text{ rad/s}, \\Delta t=\\;0.05 \\text{ s}, T_{\\text{end}}=\\;5.0 \\text{ s}, \\tau_u=\\;0.5 \\text{ s}, \\delta=\\;0.1)$.\n- Case $3$: $(M=\\;32, N=\\;4096, P=\\;16, \\alpha=\\;1.0, \\beta=\\;0.001, A=\\;0.5, \\omega=\\;\\pi \\text{ rad/s}, \\Delta t=\\;0.05 \\text{ s}, T_{\\text{end}}=\\;5.0 \\text{ s}, \\tau_u=\\;0.1 \\text{ s}, \\delta=\\;0.3)$.\n- Case $4$: $(M=\\;32, N=\\;4096, P=\\;4, \\alpha=\\;1.0, \\beta=\\;0.001, A=\\;0.8, \\omega=\\;\\pi \\text{ rad/s}, \\Delta t=\\;0.05 \\text{ s}, T_{\\text{end}}=\\;5.0 \\text{ s}, \\tau_u=\\;0.2 \\text{ s}, \\delta=\\;0.2)$.\n\nAngle units are radians; time units are seconds; length units are meters. The program must output, for each case, two floats: $S$ and $T$, in this order, rounded to four decimal places. The final output must be a single line containing a comma-separated list of these values for all cases, enclosed in square brackets, i.e.,\n\n$$\n[\\;S_1, T_1, S_2, T_2, S_3, T_3, S_4, T_4\\;].\n$$",
            "solution": "We begin from the physical and computational bases relevant to Molecular Dynamics (MD) with short-range interactions. Newton’s Second Law of Motion governs particle dynamics as $m \\frac{d^2 \\mathbf{r}}{dt^2} = \\mathbf{F}$, where forces $\\mathbf{F}$ arise from pair potentials truncated at a cutoff radius. In high-performance MD, the force computation per particle is approximately constant with respect to local density for fixed cutoff and neighbor cap, but neighbor list maintenance and cache-related overhead can grow superlinearly in dense regions. A model that captures both contributions is\n\n$$\nc_i(t) = \\alpha n_i(t) + \\beta n_i^2(t),\n$$\n\nwhere $\\alpha$ represents the baseline per-particle force evaluation and $\\beta n_i^2$ models additional overhead such as neighbor list rebuild complexity and memory locality penalties.\n\nTo obtain $n_i(t)$, we do not simulate trajectories explicitly; instead, we prescribe a time-dependent density field $\\rho(x,y,t)$ to reflect spatial inhomogeneity and temporal fluctuations. The sinusoidal form\n\n$$\n\\rho(x,y,t) = \\rho_0 \\left( 1 + A \\sin \\left( 2\\pi \\left( \\frac{x}{L} + \\frac{y}{L} \\right) + \\omega t \\right) \\right)\n$$\n\nwith $\\rho_0 = N/L^2$, amplitude $A$, and angular frequency $\\omega$ is physically plausible and ensures positivity for $A \\in [0,1)$. By sampling at cell centers $(x_p, y_q)$ and normalizing,\n\n$$\nn_{p,q}(t) = N \\cdot \\frac{f_{p,q}(t)}{\\sum_{p',q'} f_{p',q'}(t)}, \\quad f_{p,q}(t) = 1 + A \\sin \\left( 2\\pi \\left( \\frac{x_p}{L} + \\frac{y_q}{L} \\right) + \\omega t \\right),\n$$\n\nthe total particle count is conserved at all times.\n\nLoad-balanced decomposition uses a Space-Filling Curve (SFC), specifically a Hilbert curve, to map two-dimensional cells to a one-dimensional index while preserving spatial locality. The Hilbert mapping $h(p,q)$ exists for grids with $M=2^k$. Ordering cells by $h$ yields a one-dimensional load sequence $\\ell_d(t)$. The cumulative load $\\mathcal{L}_d(t)$ allows us to construct a perfectly balanced partition by placing $P-1$ boundaries $b_j(t)$ such that each contiguous segment has nearly equal load $\\mathcal{L}_{M^2-1}(t)/P$. Because the curve is contiguous and locality-preserving, each processor’s assigned segment corresponds to a compact set of cells that tends to minimize communication overhead in short-range MD.\n\nAdaptive redistribution is required because loads change in time. To prevent excessive reassignment (domain thrashing), HPC (high-performance computing) practice employs hysteresis and update intervals. We define update times $t_k = k \\tau_u$. At each $t_k$, we compare the current applied boundaries $\\{b_j^{\\text{prev}}\\}$ to the perfectly balanced candidate $\\{b_j^{\\text{new}}(t_k)\\}$. Two metrics determine whether we update:\n\n- Relative imbalance of the current assignment:\n\n$$\nI_{\\text{prev}}(t_k) = \\frac{\\max_j L_j^{\\text{prev}}(t_k)}{\\bar{L}(t_k)} - 1, \\quad \\bar{L}(t_k) = \\frac{1}{P} \\sum_{j=1}^P L_j^{\\text{prev}}(t_k).\n$$\n\nA small $I_{\\text{prev}}(t_k)$ indicates that current boundaries remain acceptably balanced.\n\n- Normalized movement between candidate and current boundaries:\n\n$$\nD(t_k) = \\frac{1}{P\\,M^2} \\sum_{j=1}^{P} \\left| b_j^{\\text{new}}(t_k) - b_j^{\\text{prev}} \\right|.\n$$\n\nLarge $D(t_k)$ implies significant reassignment that can cause migration overhead and thrashing.\n\nThe hysteresis rule uses threshold $\\delta$ to suppress updates when they offer only marginal improvement or require small moves that do not justify migration costs. Specifically, if $I_{\\text{prev}}(t_k) \\le \\delta$ and $D(t_k) \\le \\delta$, we keep $\\{b_j^{\\text{prev}}\\}$; otherwise, we apply $\\{b_j^{\\text{new}}(t_k)\\}$. This integrates both a performance criterion (imbalance) and a stability criterion (movement).\n\nWe quantify stability and thrashing over the simulation:\n\n- Thrashing count $\\Theta$ is the number of updates where boundaries change. The total number of updates is $K$.\n\n- Stability index:\n\n$$\nS = 1 - \\frac{\\Theta}{K},\n$$\n\nwhich approaches $1$ when updates rarely change boundaries and $0$ when every update changes boundaries.\n\n- Average thrashing severity:\n\n$$\nT = \\frac{1}{K} \\sum_{k=1}^{K} D(t_k),\n$$\n\nwhich captures the average normalized movement per update.\n\n- Time-averaged imbalance:\n\n$$\n\\overline{I} = \\frac{1}{N_t} \\sum_{m=1}^{N_t} \\left( \\frac{\\max_j L_j(t_m)}{\\frac{1}{P}\\sum_j L_j(t_m)} - 1 \\right),\n$$\n\nassessing balance quality across all time steps.\n\nAlgorithmic design:\n\n1. Construct the Hilbert mapping $h(p,q)$ for $M=2^k$. The standard bitwise algorithm yields $h(p,q) \\in \\{0,\\dots,M^2-1\\}$, guaranteeing contiguous segments.\n\n2. Initialize time $t=0$, compute $f_{p,q}(0)$, $n_{p,q}(0)$, and $c_{p,q}(0)$, order loads by Hilbert index to get $\\ell_d(0)$, compute cumulative loads $\\mathcal{L}_d(0)$, and set initial boundaries to the perfectly balanced partition $\\{b_j^{\\text{new}}(0)\\}$. Use these as the currently applied boundaries $\\{b_j^{\\text{prev}}\\}$. Set the next update time to $\\tau_u$.\n\n3. For each time step $t_m = m \\Delta t$ up to $T_{\\text{end}}$, compute $\\ell_d(t_m)$, obtain segment loads $L_j(t_m)$ under the currently applied boundaries, and accumulate imbalance into $\\overline{I}$. When $t_m$ reaches the next update time $t_k$, compute $\\{b_j^{\\text{new}}(t_k)\\}$, evaluate $I_{\\text{prev}}(t_k)$ and $D(t_k)$, apply the hysteresis rule, increment $\\Theta$ if the boundaries changed, accumulate $D(t_k)$ into the thrashing severity sum, and schedule the subsequent update at $t_{k+1}=t_k+\\tau_u$.\n\n4. After simulation, compute $S$ and $T$. Although $\\overline{I}$ is tracked, the required outputs per case are $S$ and $T$.\n\nScientific realism is ensured by basing the load model on MD’s short-range interaction costs, partitioning using a Hilbert SFC to preserve locality, and incorporating hysteresis and update intervals to reflect standard parallel redistribution practices. The test suite varies $\\tau_u$, $\\delta$, $P$, and $A$ to probe different regimes:\n- Small $\\tau_u$ and small $\\delta$ increase responsiveness and potential thrashing.\n- Larger $\\tau_u$ and larger $\\delta$ promote stability but may allow imbalance.\n- Larger $P$ increases the number of boundaries and sensitivity to fluctuations.\n- Larger $A$ intensifies spatial-temporal load variation.\n\nThe final program computes the specified metrics and prints a single-line list $[\\;S_1, T_1, S_2, T_2, S_3, T_3, S_4, T_4\\;]$, with values rounded to four decimal places, satisfying the output format and unit specifications.",
            "answer": "$$[0.0000, 0.0163, 0.8000, 0.0033, 0.4800, 0.0097, 0.7600, 0.0116]$$"
        }
    ]
}