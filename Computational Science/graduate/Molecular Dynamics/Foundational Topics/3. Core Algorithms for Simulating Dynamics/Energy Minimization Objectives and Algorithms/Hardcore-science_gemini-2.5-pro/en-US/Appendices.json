{
    "hands_on_practices": [
        {
            "introduction": "The fundamental goal of energy minimization is to locate stable molecular configurations, which correspond to local and global minima on the potential energy surface (PES). This exercise provides direct, analytical practice in identifying and classifying the critical points of a representative PES . By solving for stationary points and using the Hessian matrix to distinguish minima from saddle points, you will build a concrete understanding of the complex landscape that minimization algorithms are designed to navigate.",
            "id": "3410230",
            "problem": "In molecular dynamics energy minimization, one seeks stationary points of the potential energy surface and classifies them by the local curvature to determine minima and their basins of attraction, which inform algorithmic initialization for local minimizers such as steepest descent or Newton-type methods. Consider the two-dimensional toy potential\n$$\nU(x,y) = (x^{2}-1)^{2} + (y^{2}-1)^{2} + 0.2\\, x y.\n$$\nStarting only from the definitions that stationary points satisfy vanishing gradient and that the Hessian matrix encodes local curvature for classification, do the following:\n- Derive the stationarity conditions and solve exactly for all stationary points.\n- Compute the Hessian matrix and evaluate it at each stationary point to classify each as a local minimum, local maximum, or saddle using definiteness of the Hessian.\n- Using the energy values, determine which minima are globally lower and discuss which minima are relevant as targets for initialization of local energy minimization algorithms in molecular dynamics.\n\nFor your final answer, report only the coordinates of the four local minima, ordered as follows: first the global minimum with $x0$, then its sign-flipped counterpart, followed by the same-sign minimum in the first quadrant, then its sign-flipped counterpart. Express all entries exactly in terms of radicals, with no rounding, and format as a single row matrix $\\big[x_{1},y_{1},x_{2},y_{2},x_{3},y_{3},x_{4},y_{4}\\big]$. No units are required and no rounding is permitted.",
            "solution": "The user-provided problem is validated as scientifically grounded, well-posed, objective, and self-contained. It represents a standard exercise in multivariable calculus and optimization theory, applied to a concept central in computational physics and chemistry. The problem is solvable using established mathematical principles.\n\nThe potential energy is given by the function $U(x,y)$:\n$$\nU(x,y) = (x^{2}-1)^{2} + (y^{2}-1)^{2} + 0.2\\, x y\n$$\n\n**1. Derivation and Solution for Stationary Points**\n\nStationary points are locations $(x,y)$ where the gradient of the potential energy, $\\nabla U(x,y)$, is the zero vector. We begin by computing the first partial derivatives of $U(x,y)$.\n\n$$\n\\frac{\\partial U}{\\partial x} = \\frac{\\partial}{\\partial x} \\left( (x^{2}-1)^{2} + (y^{2}-1)^{2} + 0.2xy \\right) = 2(x^{2}-1)(2x) + 0.2y = 4x^{3} - 4x + 0.2y\n$$\n$$\n\\frac{\\partial U}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( (x^{2}-1)^{2} + (y^{2}-1)^{2} + 0.2xy \\right) = 2(y^{2}-1)(2y) + 0.2x = 4y^{3} - 4y + 0.2x\n$$\n\nThe stationarity conditions are $\\frac{\\partial U}{\\partial x} = 0$ and $\\frac{\\partial U}{\\partial y} = 0$. This yields a system of two coupled polynomial equations:\n$$\n(1) \\quad 4x^{3} - 4x + 0.2y = 0\n$$\n$$\n(2) \\quad 4y^{3} - 4y + 0.2x = 0\n$$\n\nTo solve this system, we can exploit its symmetries. Let's subtract equation $(2)$ from equation $(1)$:\n$$\n(4x^{3} - 4y^{3}) - (4x - 4y) + (0.2y - 0.2x) = 0\n$$\n$$\n4(x^{3} - y^{3}) - 4.2(x - y) = 0\n$$\n$$\n4(x-y)(x^{2}+xy+y^{2}) - 4.2(x-y) = 0\n$$\n$$\n(x-y) \\left[ 4(x^{2}+xy+y^{2}) - 4.2 \\right] = 0\n$$\nThis implies that either $x = y$ or $4(x^{2}+xy+y^{2}) = 4.2$, which simplifies to $x^{2}+xy+y^{2} = 1.05 = \\frac{21}{20}$.\n\nNow, let's add equation $(1)$ and equation $(2)$:\n$$\n(4x^{3} + 4y^{3}) - (4x + 4y) + (0.2y + 0.2x) = 0\n$$\n$$\n4(x^{3} + y^{3}) - 3.8(x + y) = 0\n$$\n$$\n4(x+y)(x^{2}-xy+y^{2}) - 3.8(x+y) = 0\n$$\n$$\n(x+y) \\left[ 4(x^{2}-xy+y^{2}) - 3.8 \\right] = 0\n$$\nThis implies that either $x = -y$ or $4(x^{2}-xy+y^{2}) = 3.8$, which simplifies to $x^{2}-xy+y^{2} = 0.95 = \\frac{19}{20}$.\n\nA stationary point must satisfy one condition from the subtraction result and one condition from the addition result. We analyze the four possible cases:\n\nCase A: $x=y$ and $x=-y$. This implies $x=0$ and $y=0$. We find the stationary point $(0,0)$.\n\nCase B: $x=y$ and $x^{2}-xy+y^{2} = \\frac{19}{20}$. Substituting $y=x$ into the second equation gives $x^{2}-x^{2}+x^{2} = \\frac{19}{20}$, so $x^{2} = \\frac{19}{20}$. This yields two stationary points: $(\\sqrt{\\frac{19}{20}}, \\sqrt{\\frac{19}{20}})$ and $(-\\sqrt{\\frac{19}{20}}, -\\sqrt{\\frac{19}{20}})$.\n\nCase C: $x=-y$ and $x^{2}+xy+y^{2} = \\frac{21}{20}$. Substituting $y=-x$ into the second equation gives $x^{2}-x^{2}+x^{2} = \\frac{21}{20}$, so $x^{2} = \\frac{21}{20}$. This yields two stationary points: $(\\sqrt{\\frac{21}{20}}, -\\sqrt{\\frac{21}{20}})$ and $(-\\sqrt{\\frac{21}{20}}, \\sqrt{\\frac{21}{20}})$.\n\nCase D: $x^{2}+xy+y^{2} = \\frac{21}{20}$ and $x^{2}-xy+y^{2} = \\frac{19}{20}$. This is a system of two equations. Adding them gives $2(x^{2}+y^{2}) = \\frac{21}{20} + \\frac{19}{20} = \\frac{40}{20} = 2$, so $x^{2}+y^{2} = 1$. Subtracting the second from the first gives $2xy = \\frac{21}{20} - \\frac{19}{20} = \\frac{2}{20} = \\frac{1}{10}$, so $xy = \\frac{1}{20}$.\nSubstitute $y=\\frac{1}{20x}$ into $x^{2}+y^{2}=1$: $x^{2} + (\\frac{1}{20x})^{2} = 1 \\implies x^{2} + \\frac{1}{400x^{2}} = 1$.\nLet $u=x^{2}$. Then $u + \\frac{1}{400u} = 1 \\implies 400u^{2} - 400u + 1 = 0$.\nThe solutions for $u$ are $u = \\frac{400 \\pm \\sqrt{400^{2} - 4(400)(1)}}{800} = \\frac{400 \\pm \\sqrt{160000-1600}}{800} = \\frac{400 \\pm \\sqrt{158400}}{800} = \\frac{400 \\pm 120\\sqrt{11}}{800} = \\frac{10 \\pm 3\\sqrt{11}}{20}$.\nThis gives two possible pairs for $(x^{2}, y^{2})$: $(\\frac{10+3\\sqrt{11}}{20}, \\frac{10-3\\sqrt{11}}{20})$ and vice-versa. Since $xy=\\frac{1}{20}0$, $x$ and $y$ must have the same sign. This yields four stationary points:\n$(\\pm\\sqrt{\\frac{10+3\\sqrt{11}}{20}}, \\pm\\sqrt{\\frac{10-3\\sqrt{11}}{20}})$ and $(\\pm\\sqrt{\\frac{10-3\\sqrt{11}}{20}}, \\pm\\sqrt{\\frac{10+3\\sqrt{11}}{20}})$.\n\nIn total, we have $1+2+2+4=9$ stationary points.\n\n**2. Hessian Matrix and Classification of Stationary Points**\n\nTo classify these points, we compute the Hessian matrix $H(x,y)$:\n$$\nH(x,y) = \\begin{pmatrix} \\frac{\\partial^{2} U}{\\partial x^{2}}  \\frac{\\partial^{2} U}{\\partial x \\partial y} \\\\ \\frac{\\partial^{2} U}{\\partial y \\partial x}  \\frac{\\partial^{2} U}{\\partial y^{2}} \\end{pmatrix} = \\begin{pmatrix} 12x^{2}-4  0.2 \\\\ 0.2  12y^{2}-4 \\end{pmatrix}\n$$\nThe nature of a stationary point is determined by the eigenvalues of the Hessian, which can be inferred from its determinant, $\\det(H)$, and trace, $\\text{Tr}(H)$.\n- Local minimum: $\\det(H)  0$, $\\text{Tr}(H)  0$.\n- Local maximum: $\\det(H)  0$, $\\text{Tr}(H)  0$.\n- Saddle point: $\\det(H)  0$.\n\nClassification of the 9 points:\n- For $(0,0)$: $H(0,0) = \\begin{pmatrix} -4  0.2 \\\\ 0.2  -4 \\end{pmatrix}$. $\\det(H) = 16 - 0.04 = 15.96  0$. $\\text{Tr}(H) = -8  0$. This is a **local maximum**.\n\n- For $(\\pm\\sqrt{\\frac{19}{20}}, \\pm\\sqrt{\\frac{19}{20}})$: $x^{2}=y^{2}=\\frac{19}{20}$.\n$12x^{2}-4 = 12(\\frac{19}{20})-4 = \\frac{57}{5}-4 = \\frac{37}{5} = 7.4$.\n$H = \\begin{pmatrix} 7.4  0.2 \\\\ 0.2  7.4 \\end{pmatrix}$. $\\det(H) = (7.4)^{2} - 0.04 = 54.76-0.04 = 54.72  0$. $\\text{Tr}(H) = 14.8  0$. These two points are **local minima**.\n\n- For $(\\pm\\sqrt{\\frac{21}{20}}, \\mp\\sqrt{\\frac{21}{20}})$: $x^{2}=y^{2}=\\frac{21}{20}$.\n$12x^{2}-4 = 12(\\frac{21}{20})-4 = \\frac{63}{5}-4 = \\frac{43}{5} = 8.6$.\n$H = \\begin{pmatrix} 8.6  0.2 \\\\ 0.2  8.6 \\end{pmatrix}$. $\\det(H) = (8.6)^{2} - 0.04 = 73.96-0.04 = 73.92  0$. $\\text{Tr}(H) = 17.2  0$. These two points are **local minima**.\n\n- For the four points from Case D: We had $x^{2}+y^{2}=1$ and $xy=\\frac{1}{20}$.\n$\\det(H) = (12x^{2}-4)(12y^{2}-4) - 0.04 = 144x^{2}y^{2} - 48(x^{2}+y^{2}) + 16 - 0.04$.\n$\\det(H) = 144(xy)^{2} - 48(x^{2}+y^{2}) + 15.96 = 144(\\frac{1}{20})^{2} - 48(1) + 15.96 = \\frac{144}{400} - 48 + 15.96 = 0.36 - 32.04 = -31.68  0$.\nThese four points are **saddle points**.\n\nSo, there are four local minima in total.\n\n**3. Global Minima and Discussion**\n\nWe evaluate the potential energy $U(x,y)$ at the four local minima to identify the global minima.\n$U(x,y) = (x^{2}-1)^{2} + (y^{2}-1)^{2} + 0.2xy$.\n\nFor the minima at $(\\pm\\sqrt{\\frac{19}{20}}, \\pm\\sqrt{\\frac{19}{20}})$:\n$x^{2}=y^{2}=\\frac{19}{20}$, and for both points, $xy=x^2=\\frac{19}{20}$.\n$U = (\\frac{19}{20}-1)^{2} + (\\frac{19}{20}-1)^{2} + 0.2(\\frac{19}{20}) = (-\\frac{1}{20})^{2} + (-\\frac{1}{20})^{2} + \\frac{1}{5}\\frac{19}{20} = \\frac{1}{400} + \\frac{1}{400} + \\frac{19}{100} = \\frac{2}{400} + \\frac{76}{400} = \\frac{78}{400} = \\frac{39}{200}$.\n\nFor the minima at $(\\pm\\sqrt{\\frac{21}{20}}, \\mp\\sqrt{\\frac{21}{20}})$:\n$x^{2}=y^{2}=\\frac{21}{20}$, and for both points, $xy=-x^2=-\\frac{21}{20}$.\n$U = (\\frac{21}{20}-1)^{2} + (\\frac{21}{20}-1)^{2} - 0.2(\\frac{21}{20}) = (\\frac{1}{20})^{2} + (\\frac{1}{20})^{2} - \\frac{1}{5}\\frac{21}{20} = \\frac{1}{400} + \\frac{1}{400} - \\frac{21}{100} = \\frac{2}{400} - \\frac{84}{400} = -\\frac{82}{400} = -\\frac{41}{200}$.\n\nComparing energies, $-\\frac{41}{200}  \\frac{39}{200}$. Therefore, the two points $(\\pm\\sqrt{\\frac{21}{20}}, \\mp\\sqrt{\\frac{21}{20}})$ are the degenerate global minima. The other two minima are higher-energy local minima.\n\nIn molecular dynamics, all minima are physically relevant. The global minima represent the most stable configurations (ground states) of the molecular system. Local energy minimization algorithms like steepest descent will converge to one of these states. The higher-energy local minima represent metastable states, where the system can be trapped. The specific minimum found by an algorithm depends on its starting point (initialization), as it will typically converge to the minimum within whose basin of attraction it began. Finding global minima is crucial for identifying the ground state structure, while identifying all local minima is important for characterizing the conformational landscape and metastable states of a molecule. Thus, all four minima are relevant targets for initialization strategies.\n\nThe final answer requires the coordinates of the four local minima, ordered as specified. We first rationalize the coordinates:\n$\\sqrt{\\frac{21}{20}} = \\frac{\\sqrt{21}}{\\sqrt{20}} = \\frac{\\sqrt{21}}{2\\sqrt{5}} = \\frac{\\sqrt{105}}{10}$.\n$\\sqrt{\\frac{19}{20}} = \\frac{\\sqrt{19}}{\\sqrt{20}} = \\frac{\\sqrt{19}}{2\\sqrt{5}} = \\frac{\\sqrt{95}}{10}$.\n\nThe ordering is:\n1. Global minimum with $x0$: $(\\frac{\\sqrt{105}}{10}, -\\frac{\\sqrt{105}}{10})$.\n2. Its sign-flipped counterpart: $(-\\frac{\\sqrt{105}}{10}, \\frac{\\sqrt{105}}{10})$.\n3. Same-sign minimum in the first quadrant: $(\\frac{\\sqrt{95}}{10}, \\frac{\\sqrt{95}}{10})$.\n4. Its sign-flipped counterpart: $(-\\frac{\\sqrt{95}}{10}, -\\frac{\\sqrt{95}}{10})$.\nThis provides the eight coordinates for the final answer.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sqrt{105}}{10}  -\\frac{\\sqrt{105}}{10}  -\\frac{\\sqrt{105}}{10}  \\frac{\\sqrt{105}}{10}  \\frac{\\sqrt{95}}{10}  \\frac{\\sqrt{95}}{10}  -\\frac{\\sqrt{95}}{10}  -\\frac{\\sqrt{95}}{10}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Once a descent direction is chosen, such as the path of steepest descent, an optimization algorithm must decide how far to step along it. This sub-problem is solved by a line search, which finds a step length $\\alpha$ that guarantees a sufficient decrease in energy while making reasonable progress toward the minimum. This coding exercise guides you through implementing a backtracking line search, a core component of nearly all modern minimization routines, and introduces the essential Armijo and Wolfe conditions that ensure robust convergence .",
            "id": "3410242",
            "problem": "Consider energy minimization in Molecular Dynamics, where potential energy is modeled by differentiable functions and forces follow Newton’s Second Law, with force equal to the negative gradient of potential energy. In steepest descent, one updates a configuration along the direction of the negative gradient of the potential energy. A line search chooses a step length that balances sufficient decrease in the energy with efficient progress. For a one-dimensional surrogate potential energy, let the function be $U(x) = \\sin(x) + 0.1 x^{2}$, with the angle specified in radians. The direction of descent at a point $x$ is taken to be the negative gradient of the potential energy at $x$. The step length is chosen by a backtracking strategy that enforces a sufficient-decrease criterion. Once a step is accepted, a curvature criterion is checked to assess whether the step also satisfies a strong condition on directional derivative attenuation.\n\nYour task is to write a complete program that, for a given starting position $x_{0}$, simulates a backtracking line search to find the smallest step length (from a geometric sequence initialized at $1$) that satisfies the sufficient-decrease condition, and then evaluates whether the strong curvature condition is also satisfied. Specifically, the following requirements apply:\n\n- Fundamental base: Use Newton’s Second Law stating that force is the negative gradient of potential energy and the definition of the gradient for the direction of steepest descent, along with first-order directional derivatives to define descent and sufficient decrease.\n- Energy function and gradient: The potential energy is $U(x) = \\sin(x) + 0.1 x^{2}$, and its gradient (first derivative) is $U'(x)$, defined by standard calculus. All angles are in radians.\n- Descent direction: At the starting position $x_{0}$, define the search direction $p = -U'(x_{0})$.\n- Backtracking line search parameters: Use the sufficient-decrease parameter $c_{1} = 10^{-4}$ and the backtracking factor $\\tau = 0.5$. Initialize the trial step length at $\\alpha = 1$. At each backtracking iteration, if the sufficient-decrease criterion is not satisfied, update the step length by $\\alpha \\leftarrow \\tau \\alpha$, and repeat. Stop at the first $\\alpha$ for which the sufficient-decrease criterion is satisfied.\n- Strong curvature condition parameter: After finding the first $\\alpha$ that satisfies sufficient decrease, evaluate whether the strong curvature condition holds with $c_{2} = 0.9$.\n- Units: Angles in all trigonometric evaluations must be in radians. No physical units beyond this angle specification are required.\n- Output per test case: For each test case, return a list containing the chosen step length $\\alpha$ as a floating-point number, a boolean indicating whether the sufficient-decrease criterion was satisfied at that $\\alpha$, and a boolean indicating whether the strong curvature condition holds at that same $\\alpha$.\n- Test suite: Your program must process the following starting positions with the fixed parameters defined above:\n  - Case $1$: $x_{0} = 1$, $c_{1} = 10^{-4}$, $\\tau = 0.5$, $c_{2} = 0.9$.\n  - Case $2$: $x_{0} = -5$, $c_{1} = 10^{-4}$, $\\tau = 0.5$, $c_{2} = 0.9$.\n  - Case $3$: $x_{0} = -1.32$, $c_{1} = 10^{-4}$, $\\tau = 0.5$, $c_{2} = 0.9$.\n  - Case $4$: $x_{0} = 10$, $c_{1} = 10^{-4}$, $\\tau = 0.5$, $c_{2} = 0.9$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where each entry corresponds to a test case and is itself a list in the form $[\\alpha,\\text{ArmijoSatisfied},\\text{StrongWolfeSatisfied}]$. For example, the printed output should look like $[[\\alpha_{1},\\text{True},\\text{False}],[\\alpha_{2},\\text{True},\\text{True}],\\ldots]$.\n\nThe sufficient-decrease and strong curvature conditions are defined in terms of directional derivatives and the potential energy function; ensure the program uses the one-dimensional forms derived from these principles for $U(x)$ given above.",
            "solution": "The problem requires the implementation of a backtracking line search algorithm for energy minimization in a one-dimensional system and the subsequent evaluation of a curvature condition. The process is grounded in the fundamental principles of numerical optimization as applied to molecular dynamics.\n\nFirst, we establish the mathematical framework. The system's potential energy is described by the function $U(x) = \\sin(x) + 0.1 x^2$, where $x$ is the position and the argument to the sine function is in radians. In classical mechanics, the force on the particle is the negative gradient of the potential energy. For this $1$-dimensional system, the gradient is the first derivative, $U'(x) = \\frac{d}{dx}U(x)$. By standard calculus, this is $U'(x) = \\cos(x) + 0.2 x$. The force is thus $F(x) = -U'(x)$.\n\nThe objective is to find a local minimum of the potential energy function $U(x)$. The steepest descent method is an iterative optimization algorithm that moves a point $x_k$ to a new point $x_{k+1}$ by taking a step in the direction of the negative gradient, which is the direction of the greatest local decrease in the function's value. The search direction $p_k$ at a point $x_k$ is therefore defined as $p_k = -U'(x_k)$.\n\nThe new position is given by $x_{k+1} = x_k + \\alpha_k p_k$, where $\\alpha_k  0$ is the step length. A crucial part of the algorithm is to determine an appropriate value for $\\alpha_k$. An overly large step can overshoot the minimum and increase the energy, while an excessively small step leads to slow convergence. The backtracking line search is a strategy to find a suitable $\\alpha_k$.\n\nThe backtracking line search algorithm begins with an initial trial step length, here specified as $\\alpha = 1$, and iteratively reduces it until a specific criterion is met. This problem uses the sufficient decrease condition, also known as the Armijo condition. This condition ensures that the step length $\\alpha$ produces a meaningful reduction in the potential energy. For a starting position $x_0$ and a search direction $p_0 = -U'(x_0)$, the Armijo condition is:\n$$U(x_0 + \\alpha p_0) \\le U(x_0) + c_1 \\alpha U'(x_0) p_0$$\nSubstituting $p_0 = -U'(x_0)$, the condition becomes:\n$$U(x_0 - \\alpha U'(x_0)) \\le U(x_0) - c_1 \\alpha (U'(x_0))^2$$\nThe constant $c_1$ is a small positive number, given here as $c_1 = 10^{-4}$, which controls how much of a decrease is considered \"sufficient\". The algorithm proceeds as follows:\n1. Initialize the step length $\\alpha = 1$.\n2. Check if the Armijo condition is satisfied for the current $\\alpha$.\n3. If it is satisfied, the search terminates, and this $\\alpha$ is chosen as the step length.\n4. If it is not satisfied, the step length is reduced by a backtracking factor $\\tau$, i.e., $\\alpha \\leftarrow \\tau \\alpha$. The problem specifies $\\tau = 0.5$. Steps $2-4$ are repeated until a suitable $\\alpha$ is found. For a descent direction and a function that is bounded below, this process is guaranteed to terminate.\n\nAfter the backtracking search successfully identifies a step length $\\alpha$ that satisfies the Armijo condition, a second condition, the strong curvature condition, is evaluated. This condition is part of the strong Wolfe conditions and ensures that the slope at the new point has been reduced sufficiently. This helps to avoid taking very small steps. The strong curvature condition is:\n$$|U'(x_0 + \\alpha p_0)| \\le c_2 |U'(x_0)|$$\nSubstituting $p_0 = -U'(x_0)$, this is:\n$$|U'(x_0 - \\alpha U'(x_0))| \\le c_2 |U'(x_0)|$$\nThe constant $c_2$ is given as $c_2 = 0.9$. Note that this is a check performed on the step length $\\alpha$ obtained from the backtracking search; it does not influence the selection of $\\alpha$ in this problem.\n\nFor each test case with a given starting position $x_0$, the program will execute this defined procedure:\n1. Calculate the gradient $U'(x_0)$.\n2. Initialize $\\alpha = 1$.\n3. Iteratively check the Armijo condition, updating $\\alpha \\leftarrow 0.5 \\alpha$ upon failure, until the condition is met.\n4. The first $\\alpha$ that satisfies the condition is the result of the line search. By design, the sufficient-decrease criterion is satisfied for this $\\alpha$, so the corresponding boolean output will be True.\n5. Using this final $\\alpha$, evaluate the strong curvature condition to obtain the second boolean output.\n6. Package the final step length $\\alpha$ and the two boolean results into a list for output.\n\nThis procedure will be systematically applied to the specified initial positions: $x_0 = 1$, $x_0 = -5$, $x_0 = -1.32$, and $x_0 = 10$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates a backtracking line search for energy minimization on a 1D potential\n    for a set of starting positions.\n    \"\"\"\n\n    # Define the potential energy function and its derivative.\n    def potential_energy(x):\n        \"\"\"\n        Calculates the potential energy U(x) = sin(x) + 0.1 * x^2.\n        \"\"\"\n        return np.sin(x) + 0.1 * x**2\n\n    def potential_gradient(x):\n        \"\"\"\n        Calculates the gradient (derivative) U'(x) = cos(x) + 0.2 * x.\n        \"\"\"\n        return np.cos(x) + 0.2 * x\n\n    def run_line_search(x0, c1, tau, c2):\n        \"\"\"\n        Performs the backtracking line search and evaluates the curvature condition.\n        \n        Args:\n            x0 (float): The starting position.\n            c1 (float): The sufficient-decrease (Armijo) parameter.\n            tau (float): The backtracking factor.\n            c2 (float): The strong curvature (Wolfe) parameter.\n            \n        Returns:\n            list: A list containing [alpha, armijo_satisfied, strong_wolfe_satisfied].\n        \"\"\"\n        # Calculate initial potential energy and gradient at the starting point x0.\n        u_x0 = potential_energy(x0)\n        grad_x0 = potential_gradient(x0)\n\n        # If the gradient is zero, we are at a stationary point. The search direction is zero.\n        # No step can be taken. This is an edge case not expected from the problem's test data.\n        if np.isclose(grad_x0, 0.0):\n            return [0.0, True, np.abs(grad_x0) = c2 * np.abs(grad_x0)]\n\n        # Initialize the step length for the backtracking line search.\n        alpha = 1.0\n\n        # Loop until the sufficient-decrease (Armijo) condition is met.\n        while True:\n            # The new position is x_new = x0 + alpha * p0, where p0 = -grad_x0.\n            # So, x_new = x0 - alpha * grad_x0.\n            u_new = potential_energy(x0 - alpha * grad_x0)\n            \n            # The Armijo condition: U(x_new) = U(x0) - c1 * alpha * grad_x0^2\n            armijo_rhs = u_x0 - c1 * alpha * grad_x0**2\n\n            if u_new = armijo_rhs:\n                # Condition satisfied, break the loop.\n                armijo_satisfied = True\n                break\n            \n            # If the condition is not met, reduce the step length.\n            alpha *= tau\n            \n            # A failsafe to prevent potential infinite loops with badly behaved functions,\n            # though not expected for this problem.\n            if alpha  1e-16:\n                armijo_satisfied = False\n                break\n\n        # After finding the step length alpha, check the strong curvature condition.\n        # This condition is |U'(x_new)| = c2 * |U'(x0)|.\n        grad_new = potential_gradient(x0 - alpha * grad_x0)\n        strong_wolfe_satisfied = np.abs(grad_new) = c2 * np.abs(grad_x0)\n\n        return [alpha, armijo_satisfied, strong_wolfe_satisfied]\n\n    # Define the test cases and fixed parameters from the problem statement.\n    test_cases = [1.0, -5.0, -1.32, 10.0]\n    c1 = 1e-4\n    tau = 0.5\n    c2 = 0.9\n\n    # Run the simulation for each test case and collect the results.\n    results = []\n    for x0_val in test_cases:\n        result = run_line_search(x0_val, c1, tau, c2)\n        results.append(result)\n\n    # Format the final output string as specified: [[r1_alpha,r1_b1,r1_b2],[r2_alpha,r2_b1,r2_b2],...]\n    # str(list) includes spaces, which need to be removed.\n    output_str = f\"[{','.join(str(r).replace(' ', '') for r in results)}]\"\n\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "For large molecular systems, explicitly computing, storing, and inverting the full Hessian matrix is computationally intractable. The Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm is the industry-standard solution, approximating the inverse Hessian's action on the gradient using only a handful of recent step and gradient-difference vectors. This practice problem has you implement the famous L-BFGS two-loop recursion, demystifying the elegant mechanism that provides its remarkable efficiency and makes it the workhorse for large-scale energy minimization .",
            "id": "3410324",
            "problem": "You are to implement a program that computes search directions for Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) using the canonical two-loop recursion, and then evaluates the effect of limited memory on the computed directions for a set of small, controlled inputs. The context is energy minimization in molecular dynamics, where the goal is to minimize a differentiable potential energy function $U(\\mathbf{x})$ over the configuration space of atomic coordinates. The fundamental base is as follows: gradients are forces derived from potential energy by $\\nabla U(\\mathbf{x})$, Newton’s method builds search directions by applying the inverse Hessian to the gradient, and quasi-Newton methods construct an approximation of the inverse Hessian using curvature information from successive steps. In L-BFGS, only a limited number $m$ of recent curvature pairs are retained. The program must implement the two-loop recursion to compute the search direction and use it for fixed inputs.\n\nThe target computation is purely mathematical. Let $\\mathbf{g} \\in \\mathbb{R}^n$ be the current gradient, where $n$ is the dimension (here $n = 3$). Let $\\{(\\mathbf{s}_i,\\mathbf{y}_i)\\}$ be the most recent step and gradient-difference pairs, with $\\mathbf{s}_i = \\mathbf{x}_{i+1} - \\mathbf{x}_i$ and $\\mathbf{y}_i = \\nabla U(\\mathbf{x}_{i+1}) - \\nabla U(\\mathbf{x}_i)$. The L-BFGS two-loop recursion constructs an approximate inverse Hessian action $H_k \\mathbf{g}$ using at most $m$ of the most recent pairs, producing the search direction $\\mathbf{p}_k = - H_k \\mathbf{g}$. You must implement the standard two-loop recursion with the following robustness rule: skip any pair for which $ \\mathbf{y}_i^\\top \\mathbf{s}_i \\le \\epsilon $, with $\\epsilon = 10^{-10}$, to avoid division instabilities. The initial inverse Hessian $H_0$ must be a scaled identity $H_0 = \\gamma I$, with scalar $\\gamma$ chosen from the last valid pair as $\\gamma = (\\mathbf{s}_{\\ell}^\\top \\mathbf{y}_{\\ell}) / (\\mathbf{y}_{\\ell}^\\top \\mathbf{y}_{\\ell})$, where $\\ell$ indexes the most recent valid pair; if there is no valid pair, use $\\gamma = 1$.\n\nYou will compute search directions for the following test suite. In every case, the curvature pairs $\\mathbf{y}_i$ are generated from a symmetric positive definite matrix $B$ via $\\mathbf{y}_i = B \\mathbf{s}_i$, ensuring the secant relation $\\mathbf{y}_i^\\top \\mathbf{s}_i  0$ holds in well-conditioned cases. All vectors and matrices are in $\\mathbb{R}^3$.\n\n- Test case $1$ (happy path):\n  - $B^{(1)} = \\begin{bmatrix} 4.0  0.1  0.0 \\\\ 0.1  2.0  0.2 \\\\ 0.0  0.2  3.0 \\end{bmatrix}$.\n  - $\\mathbf{s}_1^{(1)} = [\\,0.10,\\,-0.20,\\,0.05\\,]$, $\\mathbf{s}_2^{(1)} = [\\,-0.05,\\,0.10,\\,-0.02\\,]$.\n  - $\\mathbf{y}_i^{(1)} = B^{(1)} \\mathbf{s}_i^{(1)}$ for $i \\in \\{1,2\\}$.\n  - $\\mathbf{g}^{(1)} = [\\,1.00,\\,-2.00,\\,0.50\\,]$, $m^{(1)} = 2$.\n\n- Test case $2$ (boundary condition with no memory):\n  - No pairs are available, so the set $\\{(\\mathbf{s}_i,\\mathbf{y}_i)\\}$ is empty.\n  - $\\mathbf{g}^{(2)} = [\\,-0.30,\\,0.40,\\,0.10\\,]$, $m^{(2)} = 0$.\n  - The result should reduce to scaled gradient descent with $H_0 = I$.\n\n- Test case $3$ (ill-conditioned pair, skip by threshold):\n  - $B^{(2)} = \\begin{bmatrix} 2.0  0.05  0.0 \\\\ 0.05  1.8  0.03 \\\\ 0.0  0.03  4.0 \\end{bmatrix}$.\n  - $\\mathbf{s}_1^{(2)} = [\\,10^{-6},\\,-10^{-6},\\,2 \\cdot 10^{-6}\\,]$.\n  - $\\mathbf{y}_1^{(2)} = B^{(2)} \\mathbf{s}_1^{(2)}$.\n  - $\\mathbf{g}^{(3)} = [\\,0.30,\\,-0.10,\\,0.05\\,]$, $m^{(3)} = 1$.\n  - Apply the skip rule with $\\epsilon = 10^{-10}$; if the pair is skipped, fall back to $H_0 = I$.\n\n- Test case $4$ (limited memory truncation effects):\n  - $B^{(3)} = \\begin{bmatrix} 3.0  0.1  0.2 \\\\ 0.1  2.5  0.0 \\\\ 0.2  0.0  1.8 \\end{bmatrix}$.\n  - $\\mathbf{s}_1^{(3)} = [\\,0.20,\\,-0.10,\\,0.00\\,]$, $\\mathbf{s}_2^{(3)} = [\\,-0.10,\\,0.05,\\,0.10\\,]$, $\\mathbf{s}_3^{(3)} = [\\,0.00,\\,-0.15,\\,0.20\\,]$, $\\mathbf{s}_4^{(3)} = [\\,0.05,\\,0.00,\\,-0.10\\,]$.\n  - $\\mathbf{y}_i^{(3)} = B^{(3)} \\mathbf{s}_i^{(3)}$ for $i \\in \\{1,2,3,4\\}$.\n  - $\\mathbf{g}^{(4)} = [\\,0.50,\\,0.40,\\,-0.30\\,]$, $m^{(4)} = 3$.\n  - Use only the last $m^{(4)}$ curvature pairs among the four provided.\n\n- Test case $5$ (memory parameter larger than available pairs):\n  - $B^{(4)} = \\begin{bmatrix} 1.5  0.0  0.0 \\\\ 0.0  1.2  0.1 \\\\ 0.0  0.1  2.2 \\end{bmatrix}$.\n  - $\\mathbf{s}_1^{(4)} = [\\,0.30,\\,-0.20,\\,0.10\\,]$, $\\mathbf{s}_2^{(4)} = [\\,-0.20,\\,0.10,\\,-0.05\\,]$.\n  - $\\mathbf{y}_i^{(4)} = B^{(4)} \\mathbf{s}_i^{(4)}$ for $i \\in \\{1,2\\}$.\n  - $\\mathbf{g}^{(5)} = [\\,-1.00,\\,0.50,\\,0.20\\,]$, $m^{(5)} = 5$.\n  - Use only the available pairs, i.e., $\\min(m,\\text{number of pairs})$.\n\nYour program must compute the L-BFGS search direction $\\mathbf{p} \\in \\mathbb{R}^3$ for each test case and aggregate the results into a single line of output. The required final output format is a single line containing a comma-separated list of the resulting direction vectors, each represented as a Python-style list of three floating-point numbers, all enclosed in outer square brackets (for example, $[\\,[p_{11},p_{12},p_{13}],\\,[p_{21},p_{22},p_{23}]\\,]$). No physical units are involved, and angles are not applicable. The numerical values should be computed exactly as specified by the algorithm, with no rounding requirement beyond the default floating-point representation.",
            "solution": "The problem requires the implementation of the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) two-loop recursion algorithm to compute a search direction for nonlinear optimization. The context is energy minimization in molecular dynamics, where the objective is to find a local minimum of a potential energy function $U(\\mathbf{x})$ of atomic coordinates $\\mathbf{x} \\in \\mathbb{R}^n$. The search direction $\\mathbf{p}_k$ at an iteration $k$ is computed to solve for a step that decreases the energy.\n\nIn quasi-Newton methods, the search direction is given by $\\mathbf{p}_k = -H_k \\mathbf{g}_k$, where $\\mathbf{g}_k = \\nabla U(\\mathbf{x}_k)$ is the gradient of the potential energy (the negative of the forces on the atoms) and $H_k$ is an approximation to the inverse of the Hessian matrix $\\nabla^2 U(\\mathbf{x}_k)$. The L-BFGS method is a specific quasi-Newton method that does not store the dense $n \\times n$ matrix $H_k$. Instead, it stores a limited history of $m$ recent step vectors $\\mathbf{s}_i = \\mathbf{x}_{i+1} - \\mathbf{x}_i$ and gradient difference vectors $\\mathbf{y}_i = \\mathbf{g}_{i+1} - \\mathbf{g}_i$. These pairs $(\\mathbf{s}_i, \\mathbf{y}_i)$ capture curvature information about the potential energy surface.\n\nThe core of the task is to implement the L-BFGS two-loop recursion, which computes the matrix-vector product $H_k \\mathbf{g}_k$ algorithmically using the stored pairs.\n\nThe algorithm proceeds as follows:\nLet the current gradient be $\\mathbf{g}$, and let the set of stored history pairs be $\\{(\\mathbf{s}_i, \\mathbf{y}_i)\\}_{i=1}^M$, where $M$ is the number of valid pairs being used, ordered from oldest to most recent. The number of pairs to consider is determined by the memory parameter $m$ and the number of available pairs. A pair $(\\mathbf{s}_i, \\mathbf{y}_i)$ is deemed valid only if it satisfies the curvature condition $\\mathbf{y}_i^\\top \\mathbf{s}_i  \\epsilon$, where $\\epsilon = 10^{-10}$ is a small positive threshold to ensure stability and that the Hessian approximation remains positive definite.\n\nThe two-loop recursion algorithm is:\n1.  Initialize a vector $\\mathbf{q} = \\mathbf{g}$.\n2.  **First Loop (Backward Pass):** This loop iterates backward from the most recent pair ($i=M$) to the oldest ($i=1$). In this loop, we successively update $\\mathbf{q}$ based on the effect of each BFGS update.\n    For $i = M, M-1, \\dots, 1$:\n    -   Store the scalar $\\alpha_i = \\rho_i \\mathbf{s}_i^\\top \\mathbf{q}$, where $\\rho_i = 1 / (\\mathbf{y}_i^\\top \\mathbf{s}_i)$.\n    -   Update the vector: $\\mathbf{q} \\leftarrow \\mathbf{q} - \\alpha_i \\mathbf{y}_i$.\n    The list of computed $\\alpha_i$ values is saved for the second loop.\n\n3.  **Initial Hessian Approximation:** An initial approximation for the inverse Hessian, $H_0$, is required. A common and effective choice is a scaled identity matrix, $H_0 = \\gamma I$. The scaling factor $\\gamma$ is chosen to approximate the magnitude of the true inverse Hessian based on the most recent curvature information. It is computed as $\\gamma = (\\mathbf{s}_M^\\top \\mathbf{y}_M) / (\\mathbf{y}_M^\\top \\mathbf{y}_M)$, using the most recent valid pair $(s_M, y_M)$. If no valid history pairs are available, a default value of $\\gamma=1$ is used, which reduces the method to steepest descent for that step. The vector $\\mathbf{q}$ from the first loop is then scaled by $\\gamma$ to produce a new vector $\\mathbf{r} = \\gamma \\mathbf{q}$. This step is equivalent to computing $\\mathbf{r} = H_0 \\mathbf{q}$.\n\n4.  **Second Loop (Forward Pass):** This loop iterates forward from the oldest pair ($i=1$) to the most recent ($i=M$), reconstructing the final search vector.\n    For $i = 1, 2, \\dots, M$:\n    -   Compute an intermediate scalar $\\beta = \\rho_i \\mathbf{y}_i^\\top \\mathbf{r}$.\n    -   Update the vector: $\\mathbf{r} \\leftarrow \\mathbf{r} + (\\alpha_i - \\beta) \\mathbf{s}_i$. The $\\alpha_i$ values are those saved from the first loop.\n\n5.  **Final Search Direction:** The resulting vector $\\mathbf{r}$ is the approximation of $H_k \\mathbf{g}_k$. The final search direction is its negative: $\\mathbf{p} = - \\mathbf{r}$.\n\nThis procedure is implemented for each of the $5$ test cases. The inputs for each case include the gradient $\\mathbf{g}$, a set of step vectors $\\{\\mathbf{s}_i\\}$, a method for generating the corresponding gradient differences $\\{\\mathbf{y}_i\\}$ via a matrix $B$, and the memory parameter $m$. The implementation must correctly handle the logic for selecting pairs based on $m$, filtering for the curvature condition, and applying the fallback rules for empty history.",
            "answer": "```python\nimport numpy as np\n\ndef compute_lbfgs_direction(g, s_list, y_list, m, epsilon):\n    \"\"\"\n    Computes the L-BFGS search direction using the two-loop recursion.\n\n    Args:\n        g (np.ndarray): The current gradient vector.\n        s_list (list of np.ndarray): List of step vectors, oldest to newest.\n        y_list (list of np.ndarray): List of gradient difference vectors, oldest to newest.\n        m (int): The memory parameter, number of pairs to store.\n        epsilon (float): Threshold for the curvature condition y.s > epsilon.\n\n    Returns:\n        list: The computed search direction vector as a Python list.\n    \"\"\"\n    # 1. Determine which pairs to use based on memory m and availability.\n    num_available = len(s_list)\n    # Use the min(m, num_available) most recent pairs.\n    num_to_consider = min(m, num_available)\n    \n    if num_to_consider > 0:\n        s_to_consider = s_list[-num_to_consider:]\n        y_to_consider = y_list[-num_to_consider:]\n    else:\n        s_to_consider = []\n        y_to_consider = []\n\n    # 2. Filter these pairs for validity (curvature condition).\n    history = []\n    for s, y in zip(s_to_consider, y_to_consider):\n        ys = np.dot(y, s)\n        if ys > epsilon:\n            history.append((s, y))\n    \n    M_hist = len(history)\n    q = np.copy(g)\n\n    # 3. Handle case with no valid history.\n    if M_hist == 0:\n        # Per problem spec, if no valid pairs, use gamma = 1.\n        p = -q\n        return p.tolist()\n\n    # 4. Calculate scaling factor gamma for H_0.\n    s_last, y_last = history[-1]\n    gamma = np.dot(s_last, y_last) / np.dot(y_last, y_last)\n    \n    # 5. First loop (backward pass) to update q.\n    alphas = [0.0] * M_hist\n    for i in range(M_hist - 1, -1, -1):\n        s_i, y_i = history[i]\n        rho_i = 1.0 / np.dot(y_i, s_i)\n        alphas[i] = rho_i * np.dot(s_i, q)\n        q = q - alphas[i] * y_i\n        \n    # 6. Apply initial Hessian approximation.\n    r = gamma * q\n    \n    # 7. Second loop (forward pass) to build the final direction.\n    for i in range(M_hist):\n        s_i, y_i = history[i]\n        rho_i = 1.0 / np.dot(y_i, s_i)\n        beta = rho_i * np.dot(y_i, r)\n        r = r + (alphas[i] - beta) * s_i\n\n    # 8. Final search direction is the negative of the result.\n    p = -r\n    return p.tolist()\n\ndef solve():\n    \"\"\"\n    Sets up and solves the test cases defined in the problem.\n    \"\"\"\n    epsilon = 1e-10\n\n    test_cases = []\n\n    # Case 1\n    B1 = np.array([[4.0, 0.1, 0.0], [0.1, 2.0, 0.2], [0.0, 0.2, 3.0]])\n    s1_list = [np.array([0.10, -0.20, 0.05]), np.array([-0.05, 0.10, -0.02])]\n    y1_list = [B1 @ s for s in s1_list]\n    g1 = np.array([1.00, -2.00, 0.50])\n    m1 = 2\n    test_cases.append({'g': g1, 's_list': s1_list, 'y_list': y1_list, 'm': m1})\n\n    # Case 2\n    g2 = np.array([-0.30, 0.40, 0.10])\n    m2 = 0\n    test_cases.append({'g': g2, 's_list': [], 'y_list': [], 'm': m2})\n\n    # Case 3\n    B2 = np.array([[2.0, 0.05, 0.0], [0.05, 1.8, 0.03], [0.0, 0.03, 4.0]])\n    s2_list = [np.array([1e-6, -1e-6, 2e-6])]\n    y2_list = [B2 @ s for s in s2_list]\n    g3 = np.array([0.30, -0.10, 0.05])\n    m3 = 1\n    test_cases.append({'g': g3, 's_list': s2_list, 'y_list': y2_list, 'm': m3})\n\n    # Case 4\n    B3 = np.array([[3.0, 0.1, 0.2], [0.1, 2.5, 0.0], [0.2, 0.0, 1.8]])\n    s3_list = [\n        np.array([0.20, -0.10, 0.00]),\n        np.array([-0.10, 0.05, 0.10]),\n        np.array([0.00, -0.15, 0.20]),\n        np.array([0.05, 0.00, -0.10]),\n    ]\n    y3_list = [B3 @ s for s in s3_list]\n    g4 = np.array([0.50, 0.40, -0.30])\n    m4 = 3\n    test_cases.append({'g': g4, 's_list': s3_list, 'y_list': y3_list, 'm': m4})\n    \n    # Case 5\n    B4 = np.array([[1.5, 0.0, 0.0], [0.0, 1.2, 0.1], [0.0, 0.1, 2.2]])\n    s4_list = [\n        np.array([0.30, -0.20, 0.10]),\n        np.array([-0.20, 0.10, -0.05]),\n    ]\n    y4_list = [B4 @ s for s in s4_list]\n    g5 = np.array([-1.00, 0.50, 0.20])\n    m5 = 5\n    test_cases.append({'g': g5, 's_list': s4_list, 'y_list': y4_list, 'm': m5})\n\n    results = []\n    for case in test_cases:\n        p = compute_lbfgs_direction(case['g'], case['s_list'], case['y_list'], case['m'], epsilon)\n        results.append(p)\n    \n    # Format the output to be a single line, with no spaces in the lists.\n    print(f\"[{','.join(str(r).replace(' ', '') for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}