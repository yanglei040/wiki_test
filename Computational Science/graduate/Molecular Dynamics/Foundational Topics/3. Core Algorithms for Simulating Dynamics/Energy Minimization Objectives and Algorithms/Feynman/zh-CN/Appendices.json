{
    "hands_on_practices": [
        {
            "introduction": "能量最小化的目标是在势能面上找到能量最低的稳定构象。在我们运用算法去寻找这些最低点之前，理解势能面的地貌特征至关重要。这个练习将引导你对一个简化的二维势能函数进行“纸笔”分析，通过求解其所有驻点（包括局部最小值、局部最大值和鞍点）并进行分类，你将对最小化算法所探索的复杂“地形”建立起一个具体而直观的认识。",
            "id": "3410230",
            "problem": "在分子动力学能量最小化中，研究者寻找势能面的驻点，并通过局部曲率对其进行分类，以确定极小值点及其吸引盆，这为局部最小化方法（如最速下降法或牛顿类方法）的算法初始化提供了信息。考虑以下二维示例势：\n$$\nU(x,y) = (x^{2}-1)^{2} + (y^{2}-1)^{2} + 0.2\\, x y.\n$$\n仅从“驻点满足梯度为零”以及“Hessian矩阵编码了用于分类的局部曲率”这两个定义出发，完成以下任务：\n- 推导驻点条件，并精确求解所有驻点。\n- 计算Hessian矩阵，并在每个驻点处求值，利用Hessian矩阵的正定性将每个驻点分类为局部极小值、局部极大值或鞍点。\n- 利用能量值，确定哪些极小值点是全局最低的，并讨论在分子动力学中，哪些极小值点可作为局部能量最小化算法的初始化目标。\n\n对于最终答案，仅报告四个局部极小值点的坐标，并按以下顺序排列：首先是 $x>0$ 的全局极小值点，然后是其符号相反的对应点；接着是第一象限中符号相同的极小值点，然后是其符号相反的对应点。所有条目都必须用根式精确表示，不得四舍五入，并格式化为单行矩阵 $\\big[x_{1},y_{1},x_{2},y_{2},x_{3},y_{3},x_{4},y_{4}\\big]$。无需单位，不允许四舍五入。",
            "solution": "用户提供的问题经验证具有科学依据、提法恰当、客观且自洽。它代表了多变量微积分和优化理论中的一个标准练习，并应用于计算物理和计算化学的核心概念。该问题可以使用既定的数学原理来解决。\n\n势能由函数 $U(x,y)$ 给出：\n$$\nU(x,y) = (x^{2}-1)^{2} + (y^{2}-1)^{2} + 0.2\\, x y\n$$\n\n**1. 驻点的推导与求解**\n\n驻点是势能梯度 $\\nabla U(x,y)$ 为零向量的位置 $(x,y)$。我们首先计算 $U(x,y)$ 的一阶偏导数。\n\n$$\n\\frac{\\partial U}{\\partial x} = \\frac{\\partial}{\\partial x} \\left( (x^{2}-1)^{2} + (y^{2}-1)^{2} + 0.2xy \\right) = 2(x^{2}-1)(2x) + 0.2y = 4x^{3} - 4x + 0.2y\n$$\n$$\n\\frac{\\partial U}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( (x^{2}-1)^{2} + (y^{2}-1)^{2} + 0.2xy \\right) = 2(y^{2}-1)(2y) + 0.2x = 4y^{3} - 4y + 0.2x\n$$\n\n驻点条件为 $\\frac{\\partial U}{\\partial x} = 0$ 和 $\\frac{\\partial U}{\\partial y} = 0$。这产生了一个由两个耦合的多项式方程组成的方程组：\n$$\n(1) \\quad 4x^{3} - 4x + 0.2y = 0\n$$\n$$\n(2) \\quad 4y^{3} - 4y + 0.2x = 0\n$$\n\n为了求解该方程组，我们可以利用其对称性。将方程 $(1)$ 减去方程 $(2)$：\n$$\n(4x^{3} - 4y^{3}) - (4x - 4y) + (0.2y - 0.2x) = 0\n$$\n$$\n4(x^{3} - y^{3}) - 4.2(x - y) = 0\n$$\n$$\n4(x-y)(x^{2}+xy+y^{2}) - 4.2(x-y) = 0\n$$\n$$\n(x-y) \\left[ 4(x^{2}+xy+y^{2}) - 4.2 \\right] = 0\n$$\n这意味着 $x = y$ 或 $4(x^{2}+xy+y^{2}) = 4.2$，可简化为 $x^{2}+xy+y^{2} = 1.05 = \\frac{21}{20}$。\n\n现在，将方程 $(1)$ 和方程 $(2)$ 相加：\n$$\n(4x^{3} + 4y^{3}) - (4x + 4y) + (0.2y + 0.2x) = 0\n$$\n$$\n4(x^{3} + y^{3}) - 3.8(x + y) = 0\n$$\n$$\n4(x+y)(x^{2}-xy+y^{2}) - 3.8(x+y) = 0\n$$\n$$\n(x+y) \\left[ 4(x^{2}-xy+y^{2}) - 3.8 \\right] = 0\n$$\n这意味着 $x = -y$ 或 $4(x^{2}-xy+y^{2}) = 3.8$，可简化为 $x^{2}-xy+y^{2} = 0.95 = \\frac{19}{20}$。\n\n一个驻点必须同时满足相减结果得出的一个条件和相加结果得出的一个条件。我们分析以下四种可能的情况：\n\n情况A：$x=y$ 且 $x=-y$。这意味着 $x=0$ 且 $y=0$。我们得到驻点 $(0,0)$。\n\n情况B：$x=y$ 且 $x^{2}-xy+y^{2} = \\frac{19}{20}$。将 $y=x$ 代入第二个方程得到 $x^{2}-x^{2}+x^{2} = \\frac{19}{20}$，所以 $x^{2} = \\frac{19}{20}$。这得到两个驻点： $(\\sqrt{\\frac{19}{20}}, \\sqrt{\\frac{19}{20}})$ 和 $(-\\sqrt{\\frac{19}{20}}, -\\sqrt{\\frac{19}{20}})$。\n\n情况C：$x=-y$ 且 $x^{2}+xy+y^{2} = \\frac{21}{20}$。将 $y=-x$ 代入第二个方程得到 $x^{2}-x^{2}+x^{2} = \\frac{21}{20}$，所以 $x^{2} = \\frac{21}{20}$。这得到两个驻点： $(\\sqrt{\\frac{21}{20}}, -\\sqrt{\\frac{21}{20}})$ 和 $(-\\sqrt{\\frac{21}{20}}, \\sqrt{\\frac{21}{20}})$。\n\n情况D：$x^{2}+xy+y^{2} = \\frac{21}{20}$ 且 $x^{2}-xy+y^{2} = \\frac{19}{20}$。这是一个由两个方程组成的方程组。将它们相加得到 $2(x^{2}+y^{2}) = \\frac{21}{20} + \\frac{19}{20} = \\frac{40}{20} = 2$，所以 $x^{2}+y^{2} = 1$。用第一个方程减去第二个方程得到 $2xy = \\frac{21}{20} - \\frac{19}{20} = \\frac{2}{20} = \\frac{1}{10}$，所以 $xy = \\frac{1}{20}$。\n将 $y=\\frac{1}{20x}$ 代入 $x^{2}+y^{2}=1$： $x^{2} + (\\frac{1}{20x})^{2} = 1 \\implies x^{2} + \\frac{1}{400x^{2}} = 1$。\n令 $u=x^{2}$。则 $u + \\frac{1}{400u} = 1 \\implies 400u^{2} - 400u + 1 = 0$。\n$u$ 的解为 $u = \\frac{400 \\pm \\sqrt{400^{2} - 4(400)(1)}}{800} = \\frac{400 \\pm \\sqrt{160000-1600}}{800} = \\frac{400 \\pm \\sqrt{158400}}{800} = \\frac{400 \\pm 120\\sqrt{11}}{800} = \\frac{10 \\pm 3\\sqrt{11}}{20}$。\n这给出了 $(x^{2}, y^{2})$ 的两种可能配对： $(\\frac{10+3\\sqrt{11}}{20}, \\frac{10-3\\sqrt{11}}{20})$ 反之亦然。因为 $xy=\\frac{1}{20}>0$，所以 $x$ 和 $y$ 必须同号。这得到四个驻点：\n$(\\pm\\sqrt{\\frac{10+3\\sqrt{11}}{20}}, \\pm\\sqrt{\\frac{10-3\\sqrt{11}}{20}})$ 和 $(\\pm\\sqrt{\\frac{10-3\\sqrt{11}}{20}}, \\pm\\sqrt{\\frac{10+3\\sqrt{11}}{20}})$。\n\n总共有 $1+2+2+4=9$ 个驻点。\n\n**2. Hessian矩阵与驻点分类**\n\n为了对这些点进行分类，我们计算Hessian矩阵 $H(x,y)$：\n$$\nH(x,y) = \\begin{pmatrix} \\frac{\\partial^{2} U}{\\partial x^{2}} & \\frac{\\partial^{2} U}{\\partial x \\partial y} \\\\ \\frac{\\partial^{2} U}{\\partial y \\partial x} & \\frac{\\partial^{2} U}{\\partial y^{2}} \\end{pmatrix} = \\begin{pmatrix} 12x^{2}-4 & 0.2 \\\\ 0.2 & 12y^{2}-4 \\end{pmatrix}\n$$\n驻点的性质由Hessian矩阵的特征值决定，而特征值可以通过其行列式 $\\det(H)$ 和迹 $\\text{Tr}(H)$ 来推断。\n- 局部极小值：$\\det(H) > 0$，$\\text{Tr}(H) > 0$。\n- 局部极大值：$\\det(H) > 0$，$\\text{Tr}(H) < 0$。\n- 鞍点：$\\det(H) < 0$。\n\n对9个点的分类：\n- 对于 $(0,0)$：$H(0,0) = \\begin{pmatrix} -4 & 0.2 \\\\ 0.2 & -4 \\end{pmatrix}$。 $\\det(H) = 16 - 0.04 = 15.96 > 0$。 $\\text{Tr}(H) = -8 < 0$。这是一个**局部极大值**。\n\n- 对于 $(\\pm\\sqrt{\\frac{19}{20}}, \\pm\\sqrt{\\frac{19}{20}})$：$x^{2}=y^{2}=\\frac{19}{20}$。\n$12x^{2}-4 = 12(\\frac{19}{20})-4 = \\frac{57}{5}-4 = \\frac{37}{5} = 7.4$。\n$H = \\begin{pmatrix} 7.4 & 0.2 \\\\ 0.2 & 7.4 \\end{pmatrix}$。 $\\det(H) = (7.4)^{2} - 0.04 = 54.76-0.04 = 54.72 > 0$。 $\\text{Tr}(H) = 14.8 > 0$。这两个点是**局部极小值**。\n\n- 对于 $(\\pm\\sqrt{\\frac{21}{20}}, \\mp\\sqrt{\\frac{21}{20}})$：$x^{2}=y^{2}=\\frac{21}{20}$。\n$12x^{2}-4 = 12(\\frac{21}{20})-4 = \\frac{63}{5}-4 = \\frac{43}{5} = 8.6$。\n$H = \\begin{pmatrix} 8.6 & 0.2 \\\\ 0.2 & 8.6 \\end{pmatrix}$。 $\\det(H) = (8.6)^{2} - 0.04 = 73.96-0.04 = 73.92 > 0$。 $\\text{Tr}(H) = 17.2 > 0$。这两个点是**局部极小值**。\n\n- 对于情况D中的四个点：我们有 $x^{2}+y^{2}=1$ 和 $xy=\\frac{1}{20}$。\n$\\det(H) = (12x^{2}-4)(12y^{2}-4) - 0.04 = 144x^{2}y^{2} - 48(x^{2}+y^{2}) + 16 - 0.04$。\n$\\det(H) = 144(xy)^{2} - 48(x^{2}+y^{2}) + 15.96 = 144(\\frac{1}{20})^{2} - 48(1) + 15.96 = \\frac{144}{400} - 48 + 15.96 = 0.36 - 32.04 = -31.68 < 0$。\n这四个点是**鞍点**。\n\n所以，总共有四个局部极小值点。\n\n**3. 全局极小值与讨论**\n\n我们在四个局部极小值点处计算势能 $U(x,y)$ 的值，以确定全局极小值点。\n$U(x,y) = (x^{2}-1)^{2} + (y^{2}-1)^{2} + 0.2xy$。\n\n对于极小值点 $(\\pm\\sqrt{\\frac{19}{20}}, \\pm\\sqrt{\\frac{19}{20}})$：\n$x^{2}=y^{2}=\\frac{19}{20}$，对于这两个点， $xy=x^2=\\frac{19}{20}$。\n$U = (\\frac{19}{20}-1)^{2} + (\\frac{19}{20}-1)^{2} + 0.2(\\frac{19}{20}) = (-\\frac{1}{20})^{2} + (-\\frac{1}{20})^{2} + \\frac{1}{5}\\frac{19}{20} = \\frac{1}{400} + \\frac{1}{400} + \\frac{19}{100} = \\frac{2}{400} + \\frac{76}{400} = \\frac{78}{400} = \\frac{39}{200}$。\n\n对于极小值点 $(\\pm\\sqrt{\\frac{21}{20}}, \\mp\\sqrt{\\frac{21}{20}})$：\n$x^{2}=y^{2}=\\frac{21}{20}$，对于这两个点， $xy=-x^2=-\\frac{21}{20}$。\n$U = (\\frac{21}{20}-1)^{2} + (\\frac{21}{20}-1)^{2} - 0.2(\\frac{21}{20}) = (\\frac{1}{20})^{2} + (\\frac{1}{20})^{2} - \\frac{1}{5}\\frac{21}{20} = \\frac{1}{400} + \\frac{1}{400} - \\frac{21}{100} = \\frac{2}{400} - \\frac{84}{400} = -\\frac{82}{400} = -\\frac{41}{200}$。\n\n比较能量值，$-\\frac{41}{200} < \\frac{39}{200}$。因此，两个点 $(\\pm\\sqrt{\\frac{21}{20}}, \\mp\\sqrt{\\frac{21}{20}})$ 是简并的全局极小值点。另外两个极小值点是能量较高的局部极小值点。\n\n在分子动力学中，所有极小值点都具有物理意义。全局极小值点代表了分子系统最稳定的构型（基态）。像最速下降法这样的局部能量最小化算法会收敛到这些状态之一。能量较高的局部极小值点代表亚稳态，系统可能会被困在这些状态中。算法找到的具体极小值点取决于其起始点（初始化），因为它通常会收敛到其起始位置所在的吸引盆内的极小值点。找到全局极小值点对于确定基态结构至关重要，而识别所有局部极小值点对于表征分子的构象景观和亚稳态非常重要。因此，所有四个极小值点都是初始化策略的相关目标。\n\n最终答案要求按指定顺序排列四个局部极小值点的坐标。我们首先对坐标进行化简：\n$\\sqrt{\\frac{21}{20}} = \\frac{\\sqrt{21}}{\\sqrt{20}} = \\frac{\\sqrt{21}}{2\\sqrt{5}} = \\frac{\\sqrt{105}}{10}$。\n$\\sqrt{\\frac{19}{20}} = \\frac{\\sqrt{19}}{\\sqrt{20}} = \\frac{\\sqrt{19}}{2\\sqrt{5}} = \\frac{\\sqrt{95}}{10}$。\n\n顺序如下：\n1. $x>0$ 的全局极小值点： $(\\frac{\\sqrt{105}}{10}, -\\frac{\\sqrt{105}}{10})$。\n2. 其符号相反的对应点： $(-\\frac{\\sqrt{105}}{10}, \\frac{\\sqrt{105}}{10})$。\n3. 第一象限中符号相同的极小值点： $(\\frac{\\sqrt{95}}{10}, \\frac{\\sqrt{95}}{10})$。\n4. 其符号相反的对应点： $(-\\frac{\\sqrt{95}}{10}, -\\frac{\\sqrt{95}}{10})$。\n这就为最终答案提供了八个坐标。",
            "answer": "$$\n\\boxed{\n\\begin{bmatrix}\n\\frac{\\sqrt{105}}{10} & -\\frac{\\sqrt{105}}{10} & -\\frac{\\sqrt{105}}{10} & \\frac{\\sqrt{105}}{10} & \\frac{\\sqrt{95}}{10} & \\frac{\\sqrt{95}}{10} & -\\frac{\\sqrt{95}}{10} & -\\frac{\\sqrt{95}}{10}\n\\end{bmatrix}\n}\n$$"
        },
        {
            "introduction": "确定了下降方向后，下一步的关键是决定沿着该方向走多远，即选择一个合适的步长。过大的步长可能会“越过”最小值点，导致能量反而上升；而过小的步长则会使收敛速度变得极为缓慢。这个编程练习将指导你实现回溯线搜索算法，它是一种确保每一步都满足Armijo充分下降条件的稳健方法，从而保证优化过程高效地向能量更低处进行。",
            "id": "3410242",
            "problem": "考虑分子动力学中的能量最小化，其中势能由可微函数建模，力遵循牛顿第二定律，力等于势能的负梯度。在最速下降法中，沿势能负梯度的方向更新构型。线搜索选择一个步长，以在能量充分下降与高效进展之间取得平衡。对于一维代理势能，设函数为 $U(x) = \\sin(x) + 0.1 x^{2}$，其中角度以弧度为单位。在点 $x$ 处的下降方向取为该点势能的负梯度。步长通过实施充分下降准则的回溯策略来选择。一旦一个步长被接受，就会检查曲率准则，以评估该步长是否也满足关于方向导数衰减的强条件。\n\n您的任务是编写一个完整的程序，对于给定的起始位置 $x_{0}$，模拟回溯线搜索以找到满足充分下降条件的最小步长（来自一个以1初始化的几何序列），然后评估强曲率条件是否也得到满足。具体而言，适用以下要求：\n\n- 基本基础：使用牛顿第二定律（即力是势能的负梯度）和最速下降方向的梯度定义，以及一阶方向导数来定义下降和充分下降。\n- 能量函数和梯度：势能为 $U(x) = \\sin(x) + 0.1 x^{2}$，其梯度（一阶导数）为 $U'(x)$，由标准微积分定义。所有角度均以弧度为单位。\n- 下降方向：在起始位置 $x_{0}$ 处，定义搜索方向 $p = -U'(x_{0})$。\n- 回溯线搜索参数：使用充分下降参数 $c_{1} = 10^{-4}$ 和回溯因子 $\\tau = 0.5$。将试验步长初始化为 $\\alpha = 1$。在每次回溯迭代中，如果未满足充分下降准则，则通过 $\\alpha \\leftarrow \\tau \\alpha$ 更新步长，并重复此过程。在第一个满足充分下降准则的 $\\alpha$ 处停止。\n- 强曲率条件参数：找到第一个满足充分下降条件的 $\\alpha$ 后，评估强曲率条件是否在 $c_{2} = 0.9$ 的情况下成立。\n- 单位：所有三角函数求值的角度都必须以弧度为单位。除此角度规定外，不需要其他物理单位。\n- 每个测试用例的输出：对于每个测试用例，返回一个列表，其中包含所选步长 $\\alpha$（浮点数）、一个布尔值（指示在该 $\\alpha$ 处是否满足充分下降准则），以及一个布尔值（指示在同一 $\\alpha$ 处强曲率条件是否成立）。\n- 测试套件：您的程序必须使用上面定义的固定参数处理以下起始位置：\n  - 情况 1：$x_{0} = 1$，$c_{1} = 10^{-4}$，$\\tau = 0.5$，$c_{2} = 0.9$。\n  - 情况 2：$x_{0} = -5$，$c_{1} = 10^{-4}$，$\\tau = 0.5$，$c_{2} = 0.9$。\n  - 情况 3：$x_{0} = -1.32$，$c_{1} = 10^{-4}$，$\\tau = 0.5$，$c_{2} = 0.9$。\n  - 情况 4：$x_{0} = 10$，$c_{1} = 10^{-4}$，$\\tau = 0.5$，$c_{2} = 0.9$。\n- 最终输出格式：您的程序应生成单行输出，其中包含结果，格式为方括号内无空格的逗号分隔列表，其中每个条目对应一个测试用例，本身是 $[\\alpha,\\text{ArmijoSatisfied},\\text{StrongWolfeSatisfied}]$ 形式的列表。例如，打印的输出应类似于 $[[\\alpha_{1},\\text{True},\\text{False}],[\\alpha_{2},\\text{True},\\text{True}],\\ldots]$。\n\n充分下降和强曲率条件是根据方向导数和势能函数定义的；请确保程序使用从这些原理为上述 $U(x)$ 推导出来的一维形式。",
            "solution": "该问题要求实现一个用于一维系统中能量最小化的回溯线搜索算法，并随后评估曲率条件。该过程基于应用于分子动力学的数值优化的基本原理。\n\n首先，我们建立数学框架。系统的势能由函数 $U(x) = \\sin(x) + 0.1 x^2$ 描述，其中 $x$ 是位置，正弦函数的参数以弧度为单位。在经典力学中，作用在粒子上的力是势能的负梯度。对于这个一维系统，梯度是一阶导数，$U'(x) = \\frac{d}{dx}U(x)$。根据标准微积分，这是 $U'(x) = \\cos(x) + 0.2 x$。因此，力是 $F(x) = -U'(x)$。\n\n目标是找到势能函数 $U(x)$ 的一个局部最小值。最速下降法是一种迭代优化算法，它通过沿负梯度方向（即函数值局部下降最快的方向）迈出一步，将点 $x_k$ 移动到新点 $x_{k+1}$。因此，在点 $x_k$ 处的搜索方向 $p_k$ 定义为 $p_k = -U'(x_k)$。\n\n新位置由 $x_{k+1} = x_k + \\alpha_k p_k$ 给出，其中 $\\alpha_k > 0$ 是步长。该算法的一个关键部分是确定一个合适的 $\\alpha_k$ 值。过大的步长可能会越过最小值并增加能量，而过小的步长则会导致收敛缓慢。回溯线搜索是找到一个合适的 $\\alpha_k$ 的策略。\n\n回溯线搜索算法从一个初始试验步长（此处指定为 $\\alpha = 1$）开始，并迭代地减小它，直到满足特定准则。本问题使用充分下降条件，也称为 Armijo 条件。该条件确保步长 $\\alpha$ 能在势能上产生有意义的减少。对于起始位置 $x_0$ 和搜索方向 $p_0 = -U'(x_0)$，Armijo 条件为：\n$$U(x_0 + \\alpha p_0) \\le U(x_0) + c_1 \\alpha U'(x_0) p_0$$\n代入 $p_0 = -U'(x_0)$，该条件变为：\n$$U(x_0 - \\alpha U'(x_0)) \\le U(x_0) - c_1 \\alpha (U'(x_0))^2$$\n常数 $c_1$ 是一个小的正数，此处给出为 $c_1 = 10^{-4}$，它控制多大的下降被认为是“充分的”。该算法按以下步骤进行：\n1. 初始化步长 $\\alpha = 1$。\n2. 检查当前 $\\alpha$ 是否满足 Armijo 条件。\n3. 如果满足，搜索终止，并选择此 $\\alpha$ 作为步长。\n4. 如果不满足，步长将通过回溯因子 $\\tau$ 进行缩减，即 $\\alpha \\leftarrow \\tau \\alpha$。问题指定 $\\tau = 0.5$。重复步骤2-4，直到找到一个合适的 $\\alpha$。对于下降方向和有下界的函数，此过程保证会终止。\n\n在回溯搜索成功确定一个满足 Armijo 条件的步长 $\\alpha$ 之后，将评估第二个条件，即强曲率条件。该条件是强 Wolfe 条件的一部分，确保新点的斜率已充分减小。这有助于避免采取非常小的步长。强曲率条件是：\n$$|U'(x_0 + \\alpha p_0)| \\le c_2 |U'(x_0)|$$\n代入 $p_0 = -U'(x_0)$，该条件变为：\n$$|U'(x_0 - \\alpha U'(x_0))| \\le c_2 |U'(x_0)|$$\n常数 $c_2$ 给出为 $c_2 = 0.9$。请注意，这是对从回溯搜索中获得的步长 $\\alpha$ 执行的检查；它不影响本问题中 $\\alpha$ 的选择。\n\n对于每个具有给定起始位置 $x_0$ 的测试用例，程序将执行此定义的过程：\n1. 计算梯度 $U'(x_0)$。\n2. 初始化 $\\alpha = 1$。\n3. 迭代检查 Armijo 条件，失败时更新 $\\alpha \\leftarrow 0.5 \\alpha$，直到条件满足为止。\n4. 第一个满足条件的 $\\alpha$ 是线搜索的结果。根据设计，此 $\\alpha$ 满足充分下降准则，因此相应的布尔值输出将为 True。\n5. 使用这个最终的 $\\alpha$，评估强曲率条件以获得第二个布尔值输出。\n6. 将最终步长 $\\alpha$ 和两个布尔值结果打包到一个列表中以供输出。\n\n此过程将系统地应用于指定的初始位置：$x_0 = 1$，$x_0 = -5$，$x_0 = -1.32$ 和 $x_0 = 10$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates a backtracking line search for energy minimization on a 1D potential\n    for a set of starting positions.\n    \"\"\"\n\n    # Define the potential energy function and its derivative.\n    def potential_energy(x):\n        \"\"\"\n        Calculates the potential energy U(x) = sin(x) + 0.1 * x^2.\n        \"\"\"\n        return np.sin(x) + 0.1 * x**2\n\n    def potential_gradient(x):\n        \"\"\"\n        Calculates the gradient (derivative) U'(x) = cos(x) + 0.2 * x.\n        \"\"\"\n        return np.cos(x) + 0.2 * x\n\n    def run_line_search(x0, c1, tau, c2):\n        \"\"\"\n        Performs the backtracking line search and evaluates the curvature condition.\n        \n        Args:\n            x0 (float): The starting position.\n            c1 (float): The sufficient-decrease (Armijo) parameter.\n            tau (float): The backtracking factor.\n            c2 (float): The strong curvature (Wolfe) parameter.\n            \n        Returns:\n            list: A list containing [alpha, armijo_satisfied, strong_wolfe_satisfied].\n        \"\"\"\n        # Calculate initial potential energy and gradient at the starting point x0.\n        u_x0 = potential_energy(x0)\n        grad_x0 = potential_gradient(x0)\n\n        # If the gradient is zero, we are at a stationary point. The search direction is zero.\n        # No step can be taken. This is an edge case not expected from the problem's test data.\n        if np.isclose(grad_x0, 0.0):\n            return [0.0, True, np.abs(grad_x0) = c2 * np.abs(grad_x0)]\n\n        # Initialize the step length for the backtracking line search.\n        alpha = 1.0\n\n        # Loop until the sufficient-decrease (Armijo) condition is met.\n        while True:\n            # The new position is x_new = x0 + alpha * p0, where p0 = -grad_x0.\n            # So, x_new = x0 - alpha * grad_x0.\n            u_new = potential_energy(x0 - alpha * grad_x0)\n            \n            # The Armijo condition: U(x_new) = U(x0) - c1 * alpha * grad_x0^2\n            armijo_rhs = u_x0 - c1 * alpha * grad_x0**2\n\n            if u_new = armijo_rhs:\n                # Condition satisfied, break the loop.\n                armijo_satisfied = True\n                break\n            \n            # If the condition is not met, reduce the step length.\n            alpha *= tau\n            \n            # A failsafe to prevent potential infinite loops with badly behaved functions,\n            # though not expected for this problem.\n            if alpha  1e-16:\n                armijo_satisfied = False\n                break\n\n        # After finding the step length alpha, check the strong curvature condition.\n        # This condition is |U'(x_new)| = c2 * |U'(x0)|.\n        grad_new = potential_gradient(x0 - alpha * grad_x0)\n        strong_wolfe_satisfied = np.abs(grad_new) = c2 * np.abs(grad_x0)\n\n        return [alpha, armijo_satisfied, strong_wolfe_satisfied]\n\n    # Define the test cases and fixed parameters from the problem statement.\n    test_cases = [1.0, -5.0, -1.32, 10.0]\n    c1 = 1e-4\n    tau = 0.5\n    c2 = 0.9\n\n    # Run the simulation for each test case and collect the results.\n    results = []\n    for x0_val in test_cases:\n        result = run_line_search(x0_val, c1, tau, c2)\n        results.append(result)\n\n    # Format the final output string as specified: [[r1_alpha,r1_b1,r1_b2],[r2_alpha,r2_b1,r2_b2],...]\n    # str(list) includes spaces, which need to be removed.\n    output_str = f\"[{','.join(str(r).replace(' ', '') for r in results)}]\"\n\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "最速下降法虽然简单，但对于复杂的势能面，其收敛效率往往不尽人意。更高级的算法，如L-BFGS，通过引入关于势能面曲率的近似信息来构造更优的搜索方向，从而实现更快的收敛。这项高级编程练习将带你实现L-BFGS方法的核心——双循环递归算法，让你亲手构建一个强大且内存高效的优化引擎，这正是如今大规模分子动力学能量最小化中的首选方法之一。",
            "id": "3410324",
            "problem": "您的任务是实现一个程序，该程序使用经典的双循环递归算法计算有限内存Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) 的搜索方向，然后在一组小规模、受控的输入上评估有限内存对所计算方向的影响。其背景是分子动力学中的能量最小化，目标是在原子坐标的构型空间上最小化一个可微的势能函数 $U(\\mathbf{x})$。基本原理如下：梯度是通过 $\\nabla U(\\mathbf{x})$ 从势能中导出的力，牛顿法通过将Hessian矩阵的逆矩阵应用于梯度来构建搜索方向，而拟牛顿法则利用连续步骤中的曲率信息来构造Hessian逆矩阵的近似。在L-BFGS方法中，仅保留有限数量 $m$ 个最近的曲率对。程序必须实现双循环递归来计算搜索方向，并将其应用于固定的输入。\n\n目标计算是纯数学的。设 $\\mathbf{g} \\in \\mathbb{R}^n$ 为当前梯度，其中 $n$ 是维度（此处 $n=3$）。设 $\\{(\\mathbf{s}_i,\\mathbf{y}_i)\\}$ 为最近的步长和梯度差对，其中 $\\mathbf{s}_i = \\mathbf{x}_{i+1} - \\mathbf{x}_i$ 且 $\\mathbf{y}_i = \\nabla U(\\mathbf{x}_{i+1}) - \\nabla U(\\mathbf{x}_i)$。L-BFGS双循环递归算法使用最多 $m$ 个最近的曲率对来构造近似的Hessian逆矩阵作用 $H_k \\mathbf{g}$，从而产生搜索方向 $\\mathbf{p}_k = - H_k \\mathbf{g}$。您必须实现标准的双循环递归，并遵循以下鲁棒性规则：为避免除法不稳定，跳过任何满足 $ \\mathbf{y}_i^\\top \\mathbf{s}_i \\le \\epsilon $（其中 $\\epsilon = 10^{-10}$）的曲率对。初始Hessian逆矩阵 $H_0$ 必须是缩放的单位矩阵 $H_0 = \\gamma I$，其中标量 $\\gamma$ 根据最后一个有效曲率对选择，计算公式为 $\\gamma = (\\mathbf{s}_{\\ell}^\\top \\mathbf{y}_{\\ell}) / (\\mathbf{y}_{\\ell}^\\top \\mathbf{y}_{\\ell})$，$\\ell$ 是最近的有效曲率对的索引；如果没有有效的曲率对，则使用 $\\gamma = 1$。\n\n您将为以下测试套件计算搜索方向。在每种情况下，曲率对 $\\mathbf{y}_i$ 都是由一个对称正定矩阵 $B$ 通过 $\\mathbf{y}_i = B \\mathbf{s}_i$ 生成的，以确保在良态情况下割线关系 $\\mathbf{y}_i^\\top \\mathbf{s}_i  0$ 成立。所有向量和矩阵都在 $\\mathbb{R}^3$ 中。\n\n- 测试用例1（理想情况）：\n  - $B^{(1)} = \\begin{bmatrix} 4.0  0.1  0.0 \\\\ 0.1  2.0  0.2 \\\\ 0.0  0.2  3.0 \\end{bmatrix}$。\n  - $\\mathbf{s}_1^{(1)} = [\\,0.10,\\,-0.20,\\,0.05\\,]$, $\\mathbf{s}_2^{(1)} = [\\,-0.05,\\,0.10,\\,-0.02\\,]$。\n  - $\\mathbf{y}_i^{(1)} = B^{(1)} \\mathbf{s}_i^{(1)}$ 对于 $i \\in \\{1,2\\}$。\n  - $\\mathbf{g}^{(1)} = [\\,1.00,\\,-2.00,\\,0.50\\,]$, $m^{(1)} = 2$。\n\n- 测试用例2（无内存的边界条件）：\n  - 无可用曲率对，因此集合 $\\{(\\mathbf{s}_i,\\mathbf{y}_i)\\}$ 为空。\n  - $\\mathbf{g}^{(2)} = [\\,-0.30,\\,0.40,\\,0.10\\,]$, $m^{(2)} = 0$。\n  - 结果应退化为使用 $H_0 = I$ 的缩放梯度下降。\n\n- 测试用例3（病态对，因阈值而跳过）：\n  - $B^{(2)} = \\begin{bmatrix} 2.0  0.05  0.0 \\\\ 0.05  1.8  0.03 \\\\ 0.0  0.03  4.0 \\end{bmatrix}$。\n  - $\\mathbf{s}_1^{(2)} = [\\,10^{-6},\\,-10^{-6},\\,2 \\cdot 10^{-6}\\,]$。\n  - $\\mathbf{y}_1^{(2)} = B^{(2)} \\mathbf{s}_1^{(2)}$。\n  - $\\mathbf{g}^{(3)} = [\\,0.30,\\,-0.10,\\,0.05\\,]$, $m^{(3)} = 1$。\n  - 应用 $\\epsilon = 10^{-10}$ 的跳过规则；如果该对被跳过，则回退到 $H_0 = I$。\n\n- 测试用例4（有限内存截断效应）：\n  - $B^{(3)} = \\begin{bmatrix} 3.0  0.1  0.2 \\\\ 0.1  2.5  0.0 \\\\ 0.2  0.0  1.8 \\end{bmatrix}$。\n  - $\\mathbf{s}_1^{(3)} = [\\,0.20,\\,-0.10,\\,0.00\\,]$, $\\mathbf{s}_2^{(3)} = [\\,-0.10,\\,0.05,\\,0.10\\,]$, $\\mathbf{s}_3^{(3)} = [\\,0.00,\\,-0.15,\\,0.20\\,]$, $\\mathbf{s}_4^{(3)} = [\\,0.05,\\,0.00,\\,-0.10\\,]$。\n  - $\\mathbf{y}_i^{(3)} = B^{(3)} \\mathbf{s}_i^{(3)}$ 对于 $i \\in \\{1,2,3,4\\}$。\n  - $\\mathbf{g}^{(4)} = [\\,0.50,\\,0.40,\\,-0.30\\,]$, $m^{(4)} = 3$。\n  - 在提供的四个曲率对中，仅使用最后的 $m^{(4)}$ 个。\n\n- 测试用例5（内存参数大于可用对数）：\n  - $B^{(4)} = \\begin{bmatrix} 1.5  0.0  0.0 \\\\ 0.0  1.2  0.1 \\\\ 0.0  0.1  2.2 \\end{bmatrix}$。\n  - $\\mathbf{s}_1^{(4)} = [\\,0.30,\\,-0.20,\\,0.10\\,]$, $\\mathbf{s}_2^{(4)} = [\\,-0.20,\\,0.10,\\,-0.05\\,]$。\n  - $\\mathbf{y}_i^{(4)} = B^{(4)} \\mathbf{s}_i^{(4)}$ 对于 $i \\in \\{1,2\\}$。\n  - $\\mathbf{g}^{(5)} = [\\,-1.00,\\,0.50,\\,0.20\\,]$, $m^{(5)} = 5$。\n  - 仅使用可用的曲率对，即 $\\min(m,\\text{可用对数})$。\n\n您的程序必须为每个测试用例计算L-BFGS搜索方向 $\\mathbf{p} \\in \\mathbb{R}^3$，并将结果汇总到单行输出中。要求的最终输出格式是单行字符串，其中包含一个逗号分隔的结果方向向量列表，每个向量表示为一个包含三个浮点数的Python风格列表，所有内容都包含在外层方括号中（例如，$[\\,[p_{11},p_{12},p_{13}],\\,[p_{21},p_{22},p_{23}]\\,]$）。计算不涉及物理单位，也不适用角度。数值应完全按照算法规定进行计算，除了默认的浮点数表示外，没有额外的舍入要求。",
            "solution": "该问题要求实现有限内存Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) 的双循环递归算法，以计算用于非线性优化的搜索方向。其背景是分子动力学中的能量最小化，目标是找到原子坐标 $\\mathbf{x} \\in \\mathbb{R}^n$ 的势能函数 $U(\\mathbf{x})$ 的一个局部最小值。在第 $k$ 次迭代中计算搜索方向 $\\mathbf{p}_k$，以求解一个能降低能量的步长。\n\n在拟牛顿法中，搜索方向由 $\\mathbf{p}_k = -H_k \\mathbf{g}_k$ 给出，其中 $\\mathbf{g}_k = \\nabla U(\\mathbf{x}_k)$ 是势能的梯度（原子所受作用力的负值），而 $H_k$ 是Hessian矩阵 $\\nabla^2 U(\\mathbf{x}_k)$ 的逆矩阵的近似。L-BFGS方法是一种特殊的拟牛顿法，它不存储稠密的 $n \\times n$ 矩阵 $H_k$。相反，它存储了有限的 $m$ 组最近的步长向量 $\\mathbf{s}_i = \\mathbf{x}_{i+1} - \\mathbf{x}_i$ 和梯度差向量 $\\mathbf{y}_i = \\mathbf{g}_{i+1} - \\mathbf{g}_i$ 的历史记录。这些曲率对 $(\\mathbf{s}_i, \\mathbf{y}_i)$ 捕捉了关于势能面的曲率信息。\n\n任务的核心是实现L-BFGS双循环递归，该递归利用存储的曲率对通过算法计算矩阵-向量乘积 $H_k \\mathbf{g}_k$。\n\n算法流程如下：\n设当前梯度为 $\\mathbf{g}$，存储的历史对集合为 $\\{(\\mathbf{s}_i, \\mathbf{y}_i)\\}_{i=1}^M$，其中 $M$ 是正在使用的有效对的数量，按从最旧到最新的顺序排列。需要考虑的对数由内存参数 $m$ 和可用对的数量决定。只有当一个曲率对 $(\\mathbf{s}_i, \\mathbf{y}_i)$ 满足曲率条件 $\\mathbf{y}_i^\\top \\mathbf{s}_i  \\epsilon$ 时，才被视为有效。其中 $\\epsilon = 10^{-10}$ 是一个小的正阈值，以确保稳定性并保证Hessian近似保持正定。\n\n双循环递归算法是：\n1.  初始化向量 $\\mathbf{q} = \\mathbf{g}$。\n2.  **第一个循环（反向传递）：** 此循环从最新的对 ($i=M$) 向后迭代到最旧的对 ($i=1$)。在此循环中，我们根据每次BFGS更新的效果，相继更新 $\\mathbf{q}$。\n    对于 $i = M, M-1, \\dots, 1$：\n    -   存储标量 $\\alpha_i = \\rho_i \\mathbf{s}_i^\\top \\mathbf{q}$，其中 $\\rho_i = 1 / (\\mathbf{y}_i^\\top \\mathbf{s}_i)$。\n    -   更新向量：$\\mathbf{q} \\leftarrow \\mathbf{q} - \\alpha_i \\mathbf{y}_i$。计算出的 $\\alpha_i$ 值列表将被保存以用于第二个循环。\n\n3.  **初始Hessian近似：** 需要一个Hessian逆矩阵的初始近似 $H_0$。一个常用且有效的选择是缩放的单位矩阵 $H_0 = \\gamma I$。缩放因子 $\\gamma$ 的选择是为了根据最新的曲率信息来近似真实Hessian逆矩阵的量级。它使用最新的有效对 $(s_M, y_M)$ 计算得出，公式为 $\\gamma = (\\mathbf{s}_M^\\top \\mathbf{y}_M) / (\\mathbf{y}_M^\\top \\mathbf{y}_M)$。如果没有可用的有效历史对，则使用默认值 $\\gamma=1$，这将使该步骤的方法退化为最速下降法。然后，将第一个循环中得到的向量 $\\mathbf{q}$ 乘以 $\\gamma$ 得到新向量 $\\mathbf{r} = \\gamma \\mathbf{q}$。此步骤等价于计算 $\\mathbf{r} = H_0 \\mathbf{q}$。\n\n4.  **第二个循环（正向传递）：** 此循环从最旧的对 ($i=1$) 向前迭代到最新的对 ($i=M$)，以重构最终的搜索向量。\n    对于 $i = 1, 2, \\dots, M$：\n    -   计算一个中间标量 $\\beta = \\rho_i \\mathbf{y}_i^\\top \\mathbf{r}$。\n    -   更新向量：$\\mathbf{r} \\leftarrow \\mathbf{r} + (\\alpha_i - \\beta) \\mathbf{s}_i$。其中的 $\\alpha_i$ 值是从第一个循环中保存的。\n\n5.  **最终搜索方向：** 得到的向量 $\\mathbf{r}$ 是 $H_k \\mathbf{g}_k$ 的近似值。最终的搜索方向是它的负值：$\\mathbf{p} = - \\mathbf{r}$。\n\n此过程将为5个测试用例中的每一个实现。每个用例的输入包括梯度 $\\mathbf{g}$、一组步长向量 $\\{\\mathbf{s}_i\\}$、一个通过矩阵 $B$ 生成相应梯度差 $\\{\\mathbf{y}_i\\}$ 的方法，以及内存参数 $m$。实现必须正确处理基于 $m$ 选择曲率对、根据曲率条件进行筛选以及应用针对空历史记录的回退规则的逻辑。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_lbfgs_direction(g, s_list, y_list, m, epsilon):\n    \"\"\"\n    Computes the L-BFGS search direction using the two-loop recursion.\n\n    Args:\n        g (np.ndarray): The current gradient vector.\n        s_list (list of np.ndarray): List of step vectors, oldest to newest.\n        y_list (list of np.ndarray): List of gradient difference vectors, oldest to newest.\n        m (int): The memory parameter, number of pairs to store.\n        epsilon (float): Threshold for the curvature condition y.s > epsilon.\n\n    Returns:\n        list: The computed search direction vector as a Python list.\n    \"\"\"\n    # 1. Determine which pairs to use based on memory m and availability.\n    num_available = len(s_list)\n    # The problem specifies using min(m, num_available) of the MOST RECENT pairs.\n    num_to_consider = min(m, num_available)\n    \n    if num_to_consider > 0:\n        s_to_consider = s_list[-num_to_consider:]\n        y_to_consider = y_list[-num_to_consider:]\n    else:\n        s_to_consider = []\n        y_to_consider = []\n\n    # 2. Filter these pairs for validity (curvature condition).\n    history = []\n    for s, y in zip(s_to_consider, y_to_consider):\n        ys = np.dot(y, s)\n        if ys > epsilon:\n            history.append((s, y))\n    \n    M_hist = len(history)\n    q = np.copy(g)\n\n    # 3. Handle case with no valid history.\n    if M_hist == 0:\n        # Per problem spec, if no valid pairs, use gamma = 1.\n        p = -q\n        return p.tolist()\n\n    # 4. Calculate scaling factor gamma for H_0.\n    s_last, y_last = history[-1]\n    gamma = np.dot(s_last, y_last) / np.dot(y_last, y_last)\n    \n    # 5. First loop (backward pass) to update q.\n    alphas = [0.0] * M_hist\n    for i in range(M_hist - 1, -1, -1):\n        s_i, y_i = history[i]\n        rho_i = 1.0 / np.dot(y_i, s_i)\n        alphas[i] = rho_i * np.dot(s_i, q)\n        q = q - alphas[i] * y_i\n        \n    # 6. Apply initial Hessian approximation.\n    r = gamma * q\n    \n    # 7. Second loop (forward pass) to build the final direction.\n    for i in range(M_hist):\n        s_i, y_i = history[i]\n        rho_i = 1.0 / np.dot(y_i, s_i)\n        beta = rho_i * np.dot(y_i, r)\n        r = r + (alphas[i] - beta) * s_i\n\n    # 8. Final search direction is the negative of the result.\n    p = -r\n    return p.tolist()\n\ndef solve():\n    \"\"\"\n    Sets up and solves the test cases defined in the problem.\n    \"\"\"\n    epsilon = 1e-10\n\n    test_cases = []\n\n    # Case 1\n    B1 = np.array([[4.0, 0.1, 0.0], [0.1, 2.0, 0.2], [0.0, 0.2, 3.0]])\n    s1_list = [np.array([0.10, -0.20, 0.05]), np.array([-0.05, 0.10, -0.02])]\n    y1_list = [B1 @ s for s in s1_list]\n    g1 = np.array([1.00, -2.00, 0.50])\n    m1 = 2\n    test_cases.append({'g': g1, 's_list': s1_list, 'y_list': y1_list, 'm': m1})\n\n    # Case 2\n    g2 = np.array([-0.30, 0.40, 0.10])\n    m2 = 0\n    test_cases.append({'g': g2, 's_list': [], 'y_list': [], 'm': m2})\n\n    # Case 3\n    B2 = np.array([[2.0, 0.05, 0.0], [0.05, 1.8, 0.03], [0.0, 0.03, 4.0]])\n    s2_list = [np.array([1e-6, -1e-6, 2e-6])]\n    y2_list = [B2 @ s for s in s2_list]\n    g3 = np.array([0.30, -0.10, 0.05])\n    m3 = 1\n    test_cases.append({'g': g3, 's_list': s2_list, 'y_list': y2_list, 'm': m3})\n\n    # Case 4\n    B3 = np.array([[3.0, 0.1, 0.2], [0.1, 2.5, 0.0], [0.2, 0.0, 1.8]])\n    s3_list = [\n        np.array([0.20, -0.10, 0.00]),\n        np.array([-0.10, 0.05, 0.10]),\n        np.array([0.00, -0.15, 0.20]),\n        np.array([0.05, 0.00, -0.10]),\n    ]\n    y3_list = [B3 @ s for s in s3_list]\n    g4 = np.array([0.50, 0.40, -0.30])\n    m4 = 3\n    test_cases.append({'g': g4, 's_list': s3_list, 'y_list': y3_list, 'm': m4})\n    \n    # Case 5\n    B4 = np.array([[1.5, 0.0, 0.0], [0.0, 1.2, 0.1], [0.0, 0.1, 2.2]])\n    s4_list = [\n        np.array([0.30, -0.20, 0.10]),\n        np.array([-0.20, 0.10, -0.05]),\n    ]\n    y4_list = [B4 @ s for s in s4_list]\n    g5 = np.array([-1.00, 0.50, 0.20])\n    m5 = 5\n    test_cases.append({'g': g5, 's_list': s4_list, 'y_list': y4_list, 'm': m5})\n\n    results = []\n    for case in test_cases:\n        p = compute_lbfgs_direction(case['g'], case['s_list'], case['y_list'], case['m'], epsilon)\n        results.append(p)\n\n    # The default string representation of a list is exactly what is required.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}