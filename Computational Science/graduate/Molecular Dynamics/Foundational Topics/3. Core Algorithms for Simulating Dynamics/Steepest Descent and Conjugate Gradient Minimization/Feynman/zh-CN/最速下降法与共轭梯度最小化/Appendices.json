{
    "hands_on_practices": [
        {
            "introduction": "所有基于梯度的优化算法，从最速下降法到共轭梯度法，都始于一个关键步骤：计算势能的梯度。这个向量是每个原子所受作用力的负值，指向势能最陡峭的上升方向。这项基础练习将指导你推导无处不在的 Lennard-Jones 势的梯度，锻炼你的矢量微积分技能，并将抽象的数学与物理作用力联系起来。",
            "id": "3449099",
            "problem": "在分子动力学 (MD) 的能量最小化算法中，例如最速下降法和共轭梯度法 (CG)，搜索方向和步长接受准则需要计算能量相对于原子笛卡尔坐标的梯度。考虑一对原子，其位置分别为 $x_i \\in \\mathbb{R}^3$ 和 $x_j \\in \\mathbb{R}^3$，它们通过 Lennard-Jones 势 $U(r) = 4\\epsilon\\left[\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]$ 相互作用，其中 $r = \\|x_i - x_j\\|$，$ \\epsilon > 0$ 是深度参数，$\\sigma > 0$ 是尺寸参数。从欧几里得范数的定义、矢量微积分中的链式法则以及标量场与其梯度之间的基本关系出发，推导笛卡尔梯度 $\\nabla_{x_i} U$ 和 $\\nabla_{x_j} U$ 的闭合形式，并将其完全简化为 $x_i$、$x_j$、$\\epsilon$、$\\sigma$ 和 $r$ 的函数。\n\n将您的最终结果表示为一个包含两个条目的单行矩阵，其中第一个条目是 $\\nabla_{x_i} U$ 的闭合形式表达式，第二个条目是 $\\nabla_{x_j} U$ 的闭合形式表达式。无需进行数值计算，也无需包含单位。无需四舍五入。",
            "solution": "该问题要求推导 Lennard-Jones 势 $U$ 相对于原子位置 $x_i$ 和 $x_j$ 的笛卡尔梯度。势能 $U$ 是关于两个原子间标量距离 $r = \\|x_i - x_j\\|$ 的函数。位置 $x_i$ 和 $x_j$ 是 $\\mathbb{R}^3$ 中的矢量。\n\nLennard-Jones 势为：\n$$U(r) = 4\\epsilon\\left[\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]$$\n其中 $\\epsilon > 0$ 和 $\\sigma > 0$ 是常数。\n\n势能 $U$ 是一个复合函数。它是一个定义在原子位置 $U(x_i, x_j)$ 上的标量场。它对 $x_i$ 和 $x_j$ 的依赖性是通过标量距离 $r(x_i, x_j) = \\|x_i - x_j\\|$ 来介导的。为了求 $U$ 对 $x_i$ 的梯度，记作 $\\nabla_{x_i} U$，我们应用矢量微积分的链式法则：\n$$\\nabla_{x_i} U(r(x_i, x_j)) = \\frac{dU}{dr} \\nabla_{x_i} r$$\n这需要两个部分：$U$ 对 $r$ 的常导数 $\\frac{dU}{dr}$，以及标量距离 $r$ 对矢量 $x_i$ 的梯度 $\\nabla_{x_i} r$。\n\n首先，我们计算 $\\frac{dU}{dr}$。为清晰起见，我们将 $U(r)$ 重写为：\n$$U(r) = 4\\epsilon(\\sigma^{12}r^{-12} - \\sigma^6r^{-6})$$\n使用求导的幂法则 $\\frac{d}{dr}(r^n) = nr^{n-1}$：\n$$\\frac{dU}{dr} = 4\\epsilon \\left[ \\sigma^{12}(-12r^{-13}) - \\sigma^6(-6r^{-7}) \\right]$$\n$$\\frac{dU}{dr} = 4\\epsilon \\left[ -12\\frac{\\sigma^{12}}{r^{13}} + 6\\frac{\\sigma^6}{r^7} \\right]$$\n提取公因式以简化表达式：\n$$\\frac{dU}{dr} = 24\\epsilon \\left[ \\frac{\\sigma^6}{r^7} - 2\\frac{\\sigma^{12}}{r^{13}} \\right]$$\n这也可以用无量纲比率表示：\n$$\\frac{dU}{dr} = \\frac{24\\epsilon}{r} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right]$$\n\n其次，我们计算 $\\nabla_{x_i} r$。距离 $r$ 是分离矢量 $\\vec{r}_{ij} = x_i - x_j$ 的欧几里得范数：\n$$r = \\|x_i - x_j\\| = \\sqrt{(x_i - x_j) \\cdot (x_i - x_j)}$$\n通常先计算 $r^2$ 的梯度会更容易：\n$$r^2 = (x_i - x_j) \\cdot (x_i - x_j)$$\n梯度算子 $\\nabla_{x_i}$ 作用于 $x_i$ 的函数。使用点积的乘积法则 $\\nabla(A \\cdot B) = (\\nabla A)B + (\\nabla B)A$：\n$$\\nabla_{x_i} r^2 = \\nabla_{x_i} ((x_i - x_j) \\cdot (x_i - x_j)) = 2 (\\nabla_{x_i} (x_i - x_j)) \\cdot (x_i - x_j)$$\n$x_i$ 相对于自身的梯度是单位张量 $\\mathbf{I}$，而 $x_j$ 相对于 $x_i$ 的微分是常数。因此，$\\nabla_{x_i} (x_i - x_j) = \\mathbf{I}$。\n$$\\nabla_{x_i} r^2 = 2 (x_i - x_j)$$\n或者，在左侧使用链式法则，$\\nabla_{x_i} r^2 = 2r \\nabla_{x_i} r$。\n令关于 $\\nabla_{x_i} r^2$ 的两个表达式相等：\n$$2r \\nabla_{x_i} r = 2(x_i - x_j)$$\n$$\\nabla_{x_i} r = \\frac{x_i - x_j}{r}$$\n\n现在我们结合这两个部分来求 $\\nabla_{x_i} U$：\n$$\\nabla_{x_i} U = \\frac{dU}{dr} \\nabla_{x_i} r = \\left( \\frac{24\\epsilon}{r} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] \\right) \\left( \\frac{x_i - x_j}{r} \\right)$$\n简化此表达式，我们得到：\n$$\\nabla_{x_i} U = \\frac{24\\epsilon}{r^2} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_i - x_j)$$\n\n接下来，我们推导相对于 $x_j$ 的梯度 $\\nabla_{x_j} U$。势能 $U$ 仅取决于分离矢量的模，$r = \\|x_i - x_j\\| = \\|x_j - x_i\\|$。因此，该势能对于粒子交换是对称的。对于这种仅取决于分离距离的对势，根据平移不变性（牛顿第三定律），作用在该对粒子上的总力必须为零。作用在粒子 $k$ 上的力是 $F_k = -\\nabla_{x_k} U$。\n$$F_i + F_j = 0 \\implies -\\nabla_{x_i} U - \\nabla_{x_j} U = 0$$\n因此，\n$$\\nabla_{x_j} U = -\\nabla_{x_i} U$$\n代入 $\\nabla_{x_i} U$ 的表达式：\n$$\\nabla_{x_j} U = - \\left( \\frac{24\\epsilon}{r^2} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_i - x_j) \\right)$$\n将负号分配到矢量部分 $(x_i - x_j)$，我们得到：\n$$\\nabla_{x_j} U = \\frac{24\\epsilon}{r^2} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_j - x_i)$$\n\n所需的两个笛卡尔梯度是：\n$$\\nabla_{x_i} U = \\frac{24\\epsilon}{r^2} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_i - x_j)$$\n$$\\nabla_{x_j} U = \\frac{24\\epsilon}{r^2} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_j - x_i)$$\n这些表达式是所需的闭合形式，并已完全简化为 $x_i$、$x_j$、$\\epsilon$、$\\sigma$ 和 $r$ 的函数。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{24\\epsilon}{r^{2}} \\left[ \\left(\\frac{\\sigma}{r}\\right)^{6} - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_i - x_j)  \\frac{24\\epsilon}{r^{2}} \\left[ \\left(\\frac{\\sigma}{r}\\right)^{6} - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_j - x_i) \\end{pmatrix}}$$"
        },
        {
            "introduction": "最速下降法是最直观的能量最小化方法，它简单地沿着负梯度方向移动。然而，在分子体系中常见的各向异性势能面上，其性能可能出人意料地差，导致效率低下的“之”字形收敛路径。这个实践性的编程练习不仅要求你实现带有至关重要的 Armijo 线搜索的最速下降算法，还让你能亲手量化并观察到这种低效现象。",
            "id": "3449160",
            "problem": "考虑一个二维势能函数 $U:\\mathbb{R}^2\\to\\mathbb{R}$，其定义为 $U(x,y)=\\left(x^2-1\\right)^2+k\\,y^2$，其中 $k0$ 是一个刚度参数，用于控制 $x$ 和 $y$ 方向之间的曲率各向异性。在分子动力学能量最小化的背景下，负梯度 $-\\nabla U$ 与力成正比，最速下降法更新沿着负梯度方向移动以降低势能。\n\n从给定的初始位置 $(x_0,y_0)$ 开始，使用 Armijo 回溯线搜索执行恰好 $2$ 次最速下降法迭代。在第 $n$ 次迭代时，最速下降法的更新形式为 $(x_{n+1},y_{n+1})=(x_n,y_n)-t_n\\,\\nabla U(x_n,y_n)$，其中步长 $t_n0$ 通过 Armijo 回溯法选择，以保证充分下降。Armijo 充分下降准则是一个不等式 $U\\big((x_n,y_n)-t\\,\\nabla U(x_n,y_n)\\big)\\le U(x_n,y_n)-\\sigma\\,t\\,\\|\\nabla U(x_n,y_n)\\|_2^2$，其中 $t$ 从几何序列 $t=t_0\\,\\beta^m$（$m\\in\\{0,1,2,\\dots\\}$）中选取，$t_00$ 是初始步长，$\\beta\\in(0,1)$ 是回溯收缩因子，$\\sigma\\in\\left(0,\\tfrac{1}{2}\\right)$ 是 Armijo 参数。梯度根据第一性原理定义为 $\\nabla U(x,y)=\\left(\\tfrac{\\partial U}{\\partial x}(x,y),\\tfrac{\\partial U}{\\partial y}(x,y)\\right)$。\n\n定义两步位移向量为 $\\Delta \\mathbf{r}_1=(x_1-x_0,\\,y_1-y_0)$ 和 $\\Delta \\mathbf{r}_2=(x_2-x_1,\\,y_2-y_1)$。为了量化曲率各向异性对路径的影响，计算无量纲的路径各向异性比率 $R=\\dfrac{|\\Delta y_1|+|\\Delta y_2|}{|\\Delta x_1|+|\\Delta x_2|}$，其中 $\\Delta x_i$ 和 $\\Delta y_i$ 是 $\\Delta \\mathbf{r}_i$ 的分量。如果分母恰好为零，则定义 $R=+\\infty$。\n\n编写一个完整的、可运行的程序，对下述每个测试用例，执行恰好 $2$ 次带 Armijo 回溯的最速下降迭代，并输出相应的 $R$ 值（以浮点数表示）。不涉及任何物理单位；$R$ 是无量纲的。你的程序必须按照描述实现 Armijo 回溯，且仅使用提供的参数。为保证数值稳健性，你可以在 Armijo 条件可满足时，通过设置一个足够大的整数来限制回溯缩减的最大次数，同时保持计算的正确性。\n\n测试套件：\n- 用例 $1$：$k=10$，$(x_0,y_0)=(0.5,0.5)$，$t_0=1.0$，$\\beta=0.5$，$\\sigma=10^{-4}$。\n- 用例 $2$：$k=1$，$(x_0,y_0)=(0.5,0.5)$，$t_0=1.0$，$\\beta=0.5$，$\\sigma=10^{-4}$。\n- 用例 $3$：$k=1000$，$(x_0,y_0)=(0.5,0.5)$，$t_0=1.0$，$\\beta=0.5$，$\\sigma=10^{-4}$。\n- 用例 $4$：$k=10$，$(x_0,y_0)=(0.5,10^{-6})$，$t_0=1.0$，$\\beta=0.5$，$\\sigma=10^{-4}$。\n- 用例 $5$：$k=10$，$(x_0,y_0)=(0.0,0.5)$，$t_0=1.0$，$\\beta=0.5，$\\sigma=10^{-4}$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含按测试用例顺序排列、用逗号分隔并用方括号括起来的结果，例如 [result1,result2,result3,result4,result5]。每个结果必须是为该测试用例计算出的相应 $R$ 值，并表示为浮点数。如果出现 $+\\infty$，应将其打印为有效的浮点无穷大。",
            "solution": "用户提供了一个有效且适定的数值优化问题。任务是计算一个粒子在二维势能面上，通过恰好 2 次带 Armijo 回溯线搜索的最速下降法迭代后，其路径各向异性比率 $R$。\n\n解决方案的制定过程是：首先建立数学和算法框架，然后通过代码实现该框架，以解决所提供的测试用例。\n\n**1. 势能与梯度**\n势能函数 $U: \\mathbb{R}^2 \\to \\mathbb{R}$ 由下式给出：\n$$\nU(x,y) = \\left(x^2-1\\right)^2+k\\,y^2\n$$\n其中 $k0$ 是给定的刚度参数。在分子动力学和优化中，势能的负梯度 $-\\nabla U$ 对应于作用在粒子上的力。系统旨在最小化 $U$。\n\n梯度 $\\nabla U(x,y)$ 是 $U$ 对每个坐标的偏导数组成的向量：\n$$\n\\nabla U(x,y) = \\left( \\frac{\\partial U}{\\partial x}, \\frac{\\partial U}{\\partial y} \\right)\n$$\n计算偏导数，我们得到：\n$$\n\\frac{\\partial U}{\\partial x} = 2(x^2-1) \\cdot (2x) = 4x(x^2-1) = 4x^3 - 4x\n$$\n$$\n\\frac{\\partial U}{\\partial y} = 2ky\n$$\n因此，梯度向量为：\n$$\n\\nabla U(x,y) = (4x^3 - 4x, 2ky)\n$$\n\n**2. 最速下降算法**\n最速下降法是一种迭代优化算法，在每一步都将点沿着负梯度的方向移动。设 $\\mathbf{r}_n = (x_n, y_n)$ 为第 $n$ 次迭代时的位置。下一个位置 $\\mathbf{r}_{n+1}$ 通过以下更新规则找到：\n$$\n\\mathbf{r}_{n+1} = \\mathbf{r}_n - t_n \\nabla U(\\mathbf{r}_n)\n$$\n这里，$t_n  0$ 是第 $n$ 次迭代的步长。步进方向 $\\mathbf{d}_n = -\\nabla U(\\mathbf{r}_n)$ 是势能面在 $\\mathbf{r}_n$ 处的最速下降方向。\n\n**3. Armijo 回溯线搜索**\n必须仔细选择步长 $t_n$ 以确保势能有充分的下降。Armijo 回溯线搜索是寻找这样步长的一个过程。对于每次迭代 $n$，我们如下寻找 $t_n$：\n\n- **参数**：该搜索使用三个参数：初始试验步长 $t_{init}$（在问题描述中记为 $t_0$）、收缩因子 $\\beta \\in (0,1)$ 和充分下降参数 $\\sigma \\in (0, 1/2)$。对于本问题，这些值为 $t_{init}=1.0$, $\\beta=0.5$ 和 $\\sigma=10^{-4}$。\n- **过程**：我们从试验步长 $t = t_{init}$ 开始。然后我们检查它是否满足 Armijo 条件：\n$$\nU(\\mathbf{r}_n - t\\,\\nabla U(\\mathbf{r}_n)) \\le U(\\mathbf{r}_n) - \\sigma\\,t\\,\\|\\nabla U(\\mathbf{r}_n)\\|_2^2\n$$\n- 如果条件满足，我们接受这个 $t$ 作为我们的步长 $t_n$。\n- 如果条件不满足，我们将步长按收缩因子减小，$t \\leftarrow \\beta t$，然后重复检查。\n\n对于有下界的光滑函数，例如给定的势能 $U(x,y)$，该过程保证会终止。\n\n**4. 两步迭代与路径各向异性比率计算**\n问题要求从初始位置 $\\mathbf{r}_0 = (x_0, y_0)$ 开始，执行恰好 2 次迭代。\n\n- **迭代 1**：\n  1. 设置 $n=0$。从 $\\mathbf{r}_0 = (x_0, y_0)$ 开始。\n  2. 计算梯度 $\\mathbf{g}_0 = \\nabla U(\\mathbf{r}_0)$。\n  3. 使用 Armijo 回溯法（如第 3 节所述）找到步长 $t_0$。\n  4. 更新位置：$\\mathbf{r}_1 = \\mathbf{r}_0 - t_0 \\mathbf{g}_0$。\n  5. 计算第一个位移向量：$\\Delta \\mathbf{r}_1 = \\mathbf{r}_1 - \\mathbf{r}_0 = (\\Delta x_1, \\Delta y_1)$。\n\n- **迭代 2**：\n  1. 设置 $n=1$。从 $\\mathbf{r}_1$ 开始。\n  2. 计算梯度 $\\mathbf{g}_1 = \\nabla U(\\mathbf{r}_1)$。\n  3. 使用 Armijo 回溯法找到步长 $t_1$。\n  4. 更新位置：$\\mathbf{r}_2 = \\mathbf{r}_1 - t_1 \\mathbf{g}_1$。\n  5. 计算第二个位移向量：$\\Delta \\mathbf{r}_2 = \\mathbf{r}_2 - \\mathbf{r}_1 = (\\Delta x_2, \\Delta y_2)$。\n\n最后，使用这些位移向量的分量计算无量纲的路径各向异性比率 $R$：\n$$\nR = \\frac{|\\Delta y_1| + |\\Delta y_2|}{|\\Delta x_1| + |\\Delta x_2|}\n$$\n如果分母 $|\\Delta x_1| + |\\Delta x_2|$ 恰好为零，则该比率定义为 $R=+\\infty$。例如，如果梯度始终垂直于 $x$ 轴，导致所有运动都纯粹在 $y$ 方向上，就会发生这种情况。\n\n所提供的 Python 代码为五个测试用例中的每一个实现了这个完整过程，计算并报告 $R$ 的值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: k=10, (x0,y0)=(0.5,0.5), t0=1.0, beta=0.5, sigma=10^{-4}.\n        {'k': 10, 'r0': (0.5, 0.5), 't_init': 1.0, 'beta': 0.5, 'sigma': 1e-4},\n        # Case 2: k=1, (x0,y0)=(0.5,0.5), t0=1.0, beta=0.5, sigma=10^{-4}.\n        {'k': 1, 'r0': (0.5, 0.5), 't_init': 1.0, 'beta': 0.5, 'sigma': 1e-4},\n        # Case 3: k=1000, (x0,y0)=(0.5,0.5), t0=1.0, beta=0.5, sigma=10^{-4}.\n        {'k': 1000, 'r0': (0.5, 0.5), 't_init': 1.0, 'beta': 0.5, 'sigma': 1e-4},\n        # Case 4: k=10, (x0,y0)=(0.5,10^{-6}), t0=1.0, beta=0.5, sigma=10^{-4}.\n        {'k': 10, 'r0': (0.5, 1e-6), 't_init': 1.0, 'beta': 0.5, 'sigma': 1e-4},\n        # Case 5: k=10, (x0,y0)=(0.0,0.5), t0=1.0, beta=0.5, sigma=10^{-4}.\n        {'k': 10, 'r0': (0.0, 0.5), 't_init': 1.0, 'beta': 0.5, 'sigma': 1e-4},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_steepest_descent_for_case(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_steepest_descent_for_case(k, r0, t_init, beta, sigma):\n    \"\"\"\n    Performs 2 iterations of steepest descent with Armijo backtracking\n    and computes the path-anisotropy ratio R.\n    \n    Args:\n        k (float): Stiffness parameter.\n        r0 (tuple): Initial position (x0, y0).\n        t_init (float): Initial step size for Armijo search.\n        beta (float): Backtracking shrinkage factor.\n        sigma (float): Armijo sufficient decrease parameter.\n        \n    Returns:\n        float: The computed path-anisotropy ratio R.\n    \"\"\"\n    r = np.array(r0, dtype=float)\n\n    def U(pos):\n        x, y = pos\n        return (x**2 - 1)**2 + k * y**2\n\n    def grad_U(pos):\n        x, y = pos\n        grad_x = 4 * x * (x**2 - 1)\n        grad_y = 2 * k * y\n        return np.array([grad_x, grad_y])\n\n    displacements = []\n    \n    # Perform exactly 2 iterations\n    for _ in range(2):\n        r_current = r\n        U_current = U(r_current)\n        g_current = grad_U(r_current)\n        g_norm_sq = np.dot(g_current, g_current)\n\n        # Armijo backtracking line search\n        t = t_init\n        \n        # Max backtracking iterations for numerical stability\n        max_backtrack_iters = 100 \n        for _ in range(max_backtrack_iters):\n            r_next_trial = r_current - t * g_current\n            U_next_trial = U(r_next_trial)\n            \n            # Armijo sufficient-decrease condition\n            armijo_rhs = U_current - sigma * t * g_norm_sq\n            if U_next_trial = armijo_rhs:\n                break\n            \n            t *= beta\n        # The 'else' on a for-loop executes if the loop completed without a 'break'.\n        # This case should not be reached for a well-posed problem.\n        # else:\n        #    # Handle failure to find a step size, e.g., by raising an error.\n        #    # For this problem, a step size is guaranteed to exist.\n        #    pass\n\n        # Update position using the found step size\n        r_next = r_current - t * g_current\n        \n        # Store displacement vector for this step\n        displacements.append(r_next - r_current)\n        \n        # Update current position for the next iteration\n        r = r_next\n\n    # Unpack displacements\n    delta_r1 = displacements[0]\n    delta_r2 = displacements[1]\n    \n    delta_x1, delta_y1 = delta_r1\n    delta_x2, delta_y2 = delta_r2\n\n    # Calculate the path-anisotropy ratio R\n    sum_delta_y = np.abs(delta_y1) + np.abs(delta_y2)\n    sum_delta_x = np.abs(delta_x1) + np.abs(delta_x2)\n\n    if sum_delta_x == 0.0:\n        R = float('inf')\n    else:\n        R = sum_delta_y / sum_delta_x\n        \n    return R\n\nsolve()\n```"
        },
        {
            "introduction": "共轭梯度（CG）方法通过利用先前梯度的历史信息来构建更有效的搜索方向，极大地改善了最速下降法的缓慢收敛问题。对于理想的二次势能面，CG 能保证在至多 $n$ 步内收敛到一个 $n$ 维问题的最小值。这项练习提供了一个共轭梯度算法的逐步计算过程，揭示了其背后的工作机理，并阐明了为何它是在行为良好的势能盆地中进行最小化的更优选择。",
            "id": "3449130",
            "problem": "在分子动力学 (MD) 中，对局部盆地周围势能的谐波近似进行能量最小化，可以建模为一个二次函数形式：$$U(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x,$$ 其中 $x \\in \\mathbb{R}^{n}$ 汇集了系统在平衡点附近的广义坐标，$A \\in \\mathbb{R}^{n \\times n}$ 是势能的对称正定Hessian矩阵，而 $b \\in \\mathbb{R}^{n}$ 代表由外力或围绕参考构型的梯度线性化所产生的线性项。极小点 $x^{\\star}$ 满足平稳性条件 $$\\nabla U(x^{\\star}) = A x^{\\star} - b = 0,$$ 这是一个线性方程组。共轭梯度 (CG) 方法在分子动力学中被广泛用于能量最小化，因为它利用对称正定系统的结构来快速收敛，而无需存储或分解矩阵。\n\n考虑能量函数 $$U(x) = \\frac{1}{2} x^{\\top} \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} x - \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}^{\\top} x,$$ 其中 $x \\in \\mathbb{R}^{2}$。目标是使用共轭梯度 (CG) 方法来最小化 $U(x)$，该方法应用于等价的线性系统 $$A x = b,$$ 其中 $$A = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.$$ 使用标准的共轭梯度 (CG) 算法，初始猜测为 $$x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},$$ 残差定义为 $$r_{k} = b - A x_{k}.$$ 执行恰好两次CG迭代以获得 $x_{2}$ 和 $r_{2}$。设第 $k$ 次迭代的误差为 $$e_{k} = x^{\\star} - x_{k},$$ 其中 $x^{\\star}$ 是 $A x = b$ 的精确解，并定义误差的A-范数为 $$\\| e_{k} \\|_{A} = \\sqrt{ e_{k}^{\\top} A e_{k} }.$$ 将 $x_{2}$、$r_{2}$ 和 $\\| e_{2} \\|_{A}$ 这三个量报告为精确值 (无舍入)。按指定顺序将您的最终答案表示为包含这三项的单个复合行矩阵。",
            "solution": "用户希望我遵循严谨的验证过程来解决问题，然后提供详细的解决方案。\n\n### 问题验证\n\n**第1步：提取已知条件**\n\n问题提供了以下数据和定义：\n- 势能函数：$U(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$，其中 $x \\in \\mathbb{R}^{2}$。\n- 矩阵 $A$：$A = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}$。\n- 向量 $b$：$b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$。\n- 解的初始猜测：$x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n- 第 $k$ 次迭代时残差的定义：$r_{k} = b - A x_{k}$。\n- 第 $k$ 次迭代时误差的定义：$e_{k} = x^{\\star} - x_{k}$，其中 $x^{\\star}$ 是 $A x = b$ 的精确解。\n- 误差的 A-范数的定义：$\\| e_{k} \\|_{A} = \\sqrt{ e_{k}^{\\top} A e_{k} }$。\n- 任务是执行恰好两次共轭梯度 (CG) 方法的迭代来找到 $x_{2}$、$r_{2}$ 和 $\\| e_{2} \\|_{A}$。\n\n**第2步：使用提取的已知条件进行验证**\n\n- **科学依据**：该问题是共轭梯度法在最小化二次函数上的应用，这等价于求解线性系统 $A x = b$。这是数值线性代数和优化中的一个标准和基本问题，在分子动力学等科学领域有直接应用，正如问题所述。矩阵 $A$ 是对称的。要成为标准CG方法的有效问题，$A$ 还必须是正定的。$A$ 的顺序主子式为 $\\det(4) = 4  0$ 和 $\\det(A) = 4 \\times 3 - 1 \\times 1 = 11  0$。由于所有顺序主子式都为正，矩阵 $A$ 是正定的。因此，该问题在科学上和数学上都是合理的。\n- **良态性**：由于 $A$ 是对称正定的，它是可逆的。因此，线性系统 $A x = b$ 有一个唯一解 $x^{\\star} = A^{-1} b$。$U(x)$ 的最小化问题也有一个唯一的全局最小值。该问题是良态的。\n- **客观性**：问题使用精确的数学定义和语言陈述，没有任何主观性或歧愈。\n- **完整性**：执行CG算法两个步骤所需的所有必要信息都已提供。\n- **其他缺陷**：该问题没有表现出验证标准中列出的任何其他缺陷。这是一个定义明确的、标准的数值方法练习。\n\n**第3步：结论和行动**\n\n问题有效。我将继续进行求解。\n\n### 求解过程\n\n共轭梯度算法是求解线性方程组 $A x = b$ 的一种迭代方法，其中 $A$ 是一个对称正定矩阵。该算法从一个初始猜测 $x_{0}$ 开始，流程如下：\n\n初始化：\n$k = 0$\n$r_{0} = b - A x_{0}$\n$p_{0} = r_{0}$\n\n对于 $k = 0, 1, 2, \\dots$:\n1. 计算步长：$\\alpha_{k} = \\frac{r_{k}^{\\top} r_{k}}{p_{k}^{\\top} A p_{k}}$\n2. 更新解：$x_{k+1} = x_{k} + \\alpha_{k} p_{k}$\n3. 更新残差：$r_{k+1} = r_{k} - \\alpha_{k} A p_{k}$\n4. 如果 $r_{k+1}$ 足够小，则停止。\n5. 计算搜索方向的改进因子：$\\beta_{k} = \\frac{r_{k+1}^{\\top} r_{k+1}}{r_{k}^{\\top} r_{k}}$\n6. 更新搜索方向：$p_{k+1} = r_{k+1} + \\beta_{k} p_{k}$\n\n我们被要求从 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，执行恰好两次迭代（$k=0$ 和 $k=1$）。\n\n**初始化 ($k=0$)**\n\n初始猜测是 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n初始残差是：\n$$r_{0} = b - A x_{0} = b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\n初始搜索方向与初始残差相同：\n$$p_{0} = r_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\n\n**第一次迭代 ($k=0$)**\n\n1. 计算步长 $\\alpha_{0}$：\n分子是 $r_{0}^{\\top} r_{0} = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = 1^{2} + 2^{2} = 5$。\n首先，我们计算 $A p_{0}$：\n$$A p_{0} = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4(1) + 1(2) \\\\ 1(1) + 3(2) \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}$$\n分母是 $p_{0}^{\\top} A p_{0} = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = 1(6) + 2(7) = 20$。\n所以，$\\alpha_{0} = \\frac{5}{20} = \\frac{1}{4}$。\n\n2. 更新解得到 $x_{1}$：\n$$x_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{2}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}$$\n\n3. 更新残差得到 $r_{1}$：\n$$r_{1} = r_{0} - \\alpha_{0} A p_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{6}{4} \\\\ 2 - \\frac{7}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{4-6}{4} \\\\ \\frac{8-7}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{2}{4} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}$$\n\n5. 计算因子 $\\beta_{0}$：\n分子是 $r_{1}^{\\top} r_{1} = \\begin{pmatrix} -\\frac{1}{2}  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} = (-\\frac{1}{2})^{2} + (\\frac{1}{4})^{2} = \\frac{1}{4} + \\frac{1}{16} = \\frac{4+1}{16} = \\frac{5}{16}$。\n分母是 $r_{0}^{\\top} r_{0} = 5$。\n所以，$\\beta_{0} = \\frac{r_{1}^{\\top} r_{1}}{r_{0}^{\\top} r_{0}} = \\frac{5/16}{5} = \\frac{1}{16}$。\n\n6. 更新搜索方向得到 $p_{1}$：\n$$p_{1} = r_{1} + \\beta_{0} p_{0} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} + \\frac{1}{16} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -\\frac{8}{16} + \\frac{1}{16} \\\\ \\frac{4}{16} + \\frac{2}{16} \\end{pmatrix} = \\begin{pmatrix} -\\frac{7}{16} \\\\ \\frac{6}{16} \\end{pmatrix}$$\n\n**第二次迭代 ($k=1$)**\n\n1. 计算步长 $\\alpha_{1}$：\n分子是 $r_{1}^{\\top} r_{1} = \\frac{5}{16}$。\n首先，我们计算 $A p_{1}$：\n$$A p_{1} = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} -\\frac{7}{16} \\\\ \\frac{6}{16} \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} 4(-7) + 1(6) \\\\ 1(-7) + 3(6) \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} -22 \\\\ 11 \\end{pmatrix}$$\n分母是 $p_{1}^{\\top} A p_{1} = \\begin{pmatrix} -\\frac{7}{16}  \\frac{6}{16} \\end{pmatrix} \\left( \\frac{1}{16} \\begin{pmatrix} -22 \\\\ 11 \\end{pmatrix} \\right) = \\frac{1}{256} ((-7)(-22) + 6(11)) = \\frac{1}{256} (154 + 66) = \\frac{220}{256}$。\n所以，$\\alpha_{1} = \\frac{r_{1}^{\\top} r_{1}}{p_{1}^{\\top} A p_{1}} = \\frac{5/16}{220/256} = \\frac{5}{16} \\times \\frac{256}{220} = 5 \\times \\frac{16}{220} = \\frac{80}{220} = \\frac{8}{22} = \\frac{4}{11}$。\n\n2. 更新解得到 $x_{2}$：\n$$x_{2} = x_{1} + \\alpha_{1} p_{1} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix} + \\frac{4}{11} \\begin{pmatrix} -\\frac{7}{16} \\\\ \\frac{6}{16} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix} + \\begin{pmatrix} -\\frac{7}{11 \\times 4} \\\\ \\frac{6}{11 \\times 4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} - \\frac{7}{44} \\\\ \\frac{1}{2} + \\frac{6}{44} \\end{pmatrix}$$\n$$x_{2} = \\begin{pmatrix} \\frac{11-7}{44} \\\\ \\frac{22+6}{44} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{44} \\\\ \\frac{28}{44} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{11} \\\\ \\frac{7}{11} \\end{pmatrix}$$\n这是第一个要求的量。\n\n3. 更新残差得到 $r_{2}$：\n$$r_{2} = r_{1} - \\alpha_{1} A p_{1} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} - \\frac{4}{11} \\left( \\frac{1}{16} \\begin{pmatrix} -22 \\\\ 11 \\end{pmatrix} \\right) = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} - \\frac{1}{44} \\begin{pmatrix} -22 \\\\ 11 \\end{pmatrix}$$\n$$r_{2} = \\begin{pmatrix} -\\frac{1}{2} + \\frac{22}{44} \\\\ \\frac{1}{4} - \\frac{11}{44} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} + \\frac{1}{2} \\\\ \\frac{1}{4} - \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n这是第二个要求的量。\n\n**误差计算**\n\n$A x = b$ 的精确解 $x^{\\star}$ 满足该方程。残差定义为 $r_{k} = b - A x_{k}$。如果 $r_{k} = 0$，则 $b - A x_{k} = 0$，这意味着 $A x_{k} = b$。因此，$x_k$ 是精确解，$x_{k} = x^{\\star}$。\n在我们的例子中，$r_{2} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，所以 $x_{2}$ 是精确解：$x_{2} = x^{\\star}$。这是符合预期的，因为在精确算术中，对于一个 $n \\times n$ 的系统，CG方法保证在至多 $n$ 次迭代内找到精确解。这里，$n=2$。\n\n第 k=2 次迭代的误差是 $e_{2} = x^{\\star} - x_{2}$。由于 $x^{\\star} = x_{2}$，误差为：\n$$e_{2} = x_{2} - x_{2} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n误差 $e_{2}$ 的A-范数定义为 $\\| e_{2} \\|_{A} = \\sqrt{ e_{2}^{\\top} A e_{2} }$。\n$$\\| e_{2} \\|_{A} = \\sqrt{ \\begin{pmatrix} 0  0 \\end{pmatrix} \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} } = \\sqrt{0} = 0$$\n这是第三个要求的量。\n\n这三个量是：\n$x_{2} = \\begin{pmatrix} \\frac{1}{11} \\\\ \\frac{7}{11} \\end{pmatrix}$\n$r_{2} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n$\\| e_{2} \\|_{A} = 0$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\begin{pmatrix} \\frac{1}{11} \\\\ \\frac{7}{11} \\end{pmatrix}  \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}  0 \\end{pmatrix}}\n$$"
        }
    ]
}