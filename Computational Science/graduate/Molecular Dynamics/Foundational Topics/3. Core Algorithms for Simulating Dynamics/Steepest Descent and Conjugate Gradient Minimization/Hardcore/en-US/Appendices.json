{
    "hands_on_practices": [
        {
            "introduction": "Before we can apply any gradient-based minimization algorithm, we must first compute the gradient of the potential energy function. The negative of this gradient, $-\\nabla U$, gives the force on each atom, which points in the direction of steepest descent on the energy landscape. This foundational exercise guides you through the analytical derivation of the gradient for the ubiquitous Lennard-Jones potential, connecting the scalar potential energy to the vector forces that drive the minimization process. ",
            "id": "3449099",
            "problem": "In molecular dynamics (MD) energy minimization algorithms such as steepest descent and conjugate gradient (CG), the search directions and step acceptance criteria require the energy gradient with respect to atomic Cartesian coordinates. Consider a pair of atoms at positions $x_i \\in \\mathbb{R}^3$ and $x_j \\in \\mathbb{R}^3$ interacting via the Lennard–Jones potential $U(r) = 4\\epsilon\\left[\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]$, where $r = \\|x_i - x_j\\|$, $\\epsilon  0$ is the depth parameter, and $\\sigma  0$ is the size parameter. Starting from the definitions of the Euclidean norm, the chain rule from vector calculus, and the fundamental relationship between a scalar field and its gradient, derive the Cartesian gradients $\\nabla_{x_i} U$ and $\\nabla_{x_j} U$ in closed form, fully simplified as functions of $x_i$, $x_j$, $\\epsilon$, $\\sigma$, and $r$.\n\nReport your final result as a single row matrix with two entries, where the first entry is the closed-form expression for $\\nabla_{x_i} U$ and the second entry is the closed-form expression for $\\nabla_{x_j} U$. There is no numerical evaluation required and no units are to be included. No rounding is required.",
            "solution": "The problem requires the derivation of the Cartesian gradients of the Lennard-Jones potential, $U$, with respect to the atomic positions $x_i$ and $x_j$. The potential $U$ is given as a function of the scalar distance $r = \\|x_i - x_j\\|$ between the two atoms. The positions $x_i$ and $x_j$ are vectors in $\\mathbb{R}^3$.\n\nThe Lennard-Jones potential is:\n$$U(r) = 4\\epsilon\\left[\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]$$\nwhere $\\epsilon  0$ and $\\sigma  0$ are constants.\n\nThe potential $U$ is a composition of functions. It is a scalar field defined over the positions of the atoms, $U(x_i, x_j)$. Its dependence on $x_i$ and $x_j$ is mediated through the scalar distance $r(x_i, x_j) = \\|x_i - x_j\\|$. To find the gradient of $U$ with respect to $x_i$, denoted as $\\nabla_{x_i} U$, we apply the chain rule for vector calculus:\n$$\\nabla_{x_i} U(r(x_i, x_j)) = \\frac{dU}{dr} \\nabla_{x_i} r$$\nThis requires two components: the ordinary derivative of $U$ with respect to $r$, $\\frac{dU}{dr}$, and the gradient of the scalar distance $r$ with respect to the vector $x_i$, $\\nabla_{x_i} r$.\n\nFirst, let us calculate $\\frac{dU}{dr}$. For clarity, we rewrite $U(r)$ as:\n$$U(r) = 4\\epsilon(\\sigma^{12}r^{-12} - \\sigma^6r^{-6})$$\nUsing the power rule for differentiation, $\\frac{d}{dr}(r^n) = nr^{n-1}$:\n$$\\frac{dU}{dr} = 4\\epsilon \\left[ \\sigma^{12}(-12r^{-13}) - \\sigma^6(-6r^{-7}) \\right]$$\n$$\\frac{dU}{dr} = 4\\epsilon \\left[ -12\\frac{\\sigma^{12}}{r^{13}} + 6\\frac{\\sigma^6}{r^7} \\right]$$\nFactoring out common terms to simplify the expression:\n$$\\frac{dU}{dr} = 24\\epsilon \\left[ \\frac{\\sigma^6}{r^7} - 2\\frac{\\sigma^{12}}{r^{13}} \\right]$$\nThis can also be expressed in terms of dimensionless ratios:\n$$\\frac{dU}{dr} = \\frac{24\\epsilon}{r} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right]$$\n\nSecond, we calculate $\\nabla_{x_i} r$. The distance $r$ is the Euclidean norm of the separation vector $\\vec{r}_{ij} = x_i - x_j$:\n$$r = \\|x_i - x_j\\| = \\sqrt{(x_i - x_j) \\cdot (x_i - x_j)}$$\nIt is often easier to first compute the gradient of $r^2$:\n$$r^2 = (x_i - x_j) \\cdot (x_i - x_j)$$\nThe gradient operator $\\nabla_{x_i}$ acts on functions of $x_i$. Using the product rule for dot products, $\\nabla(A \\cdot B) = (\\nabla A)B + (\\nabla B)A$:\n$$\\nabla_{x_i} r^2 = \\nabla_{x_i} ((x_i - x_j) \\cdot (x_i - x_j)) = 2 (\\nabla_{x_i} (x_i - x_j)) \\cdot (x_i - x_j)$$\nThe gradient of $x_i$ with respect to itself is the identity tensor $\\mathbf{I}$, and $x_j$ is constant with respect to differentiation by $x_i$. Thus, $\\nabla_{x_i} (x_i - x_j) = \\mathbf{I}$.\n$$\\nabla_{x_i} r^2 = 2 (x_i - x_j)$$\nAlternatively, using the chain rule on the left side, $\\nabla_{x_i} r^2 = 2r \\nabla_{x_i} r$.\nEquating the two expressions for $\\nabla_{x_i} r^2$:\n$$2r \\nabla_{x_i} r = 2(x_i - x_j)$$\n$$\\nabla_{x_i} r = \\frac{x_i - x_j}{r}$$\n\nNow we combine the two components to find $\\nabla_{x_i} U$:\n$$\\nabla_{x_i} U = \\frac{dU}{dr} \\nabla_{x_i} r = \\left( \\frac{24\\epsilon}{r} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] \\right) \\left( \\frac{x_i - x_j}{r} \\right)$$\nSimplifying this expression, we get:\n$$\\nabla_{x_i} U = \\frac{24\\epsilon}{r^2} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_i - x_j)$$\n\nNext, we derive the gradient with respect to $x_j$, $\\nabla_{x_j} U$. The potential $U$ depends only on the magnitude of the separation vector, $r = \\|x_i - x_j\\| = \\|x_j - x_i\\|$. Therefore, the potential is symmetric with respect to particle exchange. For such a pairwise potential that depends only on the separation distance, the total force on the pair of particles must be zero due to translational invariance (Newton's third law). The force on particle $k$ is $F_k = -\\nabla_{x_k} U$.\n$$F_i + F_j = 0 \\implies -\\nabla_{x_i} U - \\nabla_{x_j} U = 0$$\nTherefore,\n$$\\nabla_{x_j} U = -\\nabla_{x_i} U$$\nSubstituting the expression for $\\nabla_{x_i} U$:\n$$\\nabla_{x_j} U = - \\left( \\frac{24\\epsilon}{r^2} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_i - x_j) \\right)$$\nBy distributing the negative sign into the vector part $(x_i - x_j)$, we obtain:\n$$\\nabla_{x_j} U = \\frac{24\\epsilon}{r^2} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_j - x_i)$$\n\nThe two required Cartesian gradients are:\n$$\\nabla_{x_i} U = \\frac{24\\epsilon}{r^2} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_i - x_j)$$\n$$\\nabla_{x_j} U = \\frac{24\\epsilon}{r^2} \\left[ \\left(\\frac{\\sigma}{r}\\right)^6 - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_j - x_i)$$\nThese expressions are in the required closed form, fully simplified as functions of $x_i$, $x_j$, $\\epsilon$, $\\sigma$, and $r$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{24\\epsilon}{r^{2}} \\left[ \\left(\\frac{\\sigma}{r}\\right)^{6} - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_i - x_j)  \\frac{24\\epsilon}{r^{2}} \\left[ \\left(\\frac{\\sigma}{r}\\right)^{6} - 2\\left(\\frac{\\sigma}{r}\\right)^{12} \\right] (x_j - x_i) \\end{pmatrix}}$$"
        },
        {
            "introduction": "The conjugate gradient (CG) method is renowned for its efficiency, particularly when applied to quadratic potential energy surfaces, which are excellent approximations of the energy landscape near a local minimum. This exercise provides a step-by-step walkthrough of the CG algorithm for a simple two-dimensional quadratic system. By performing the calculations manually, you will gain a concrete understanding of how the algorithm constructs its unique set of A-orthogonal search directions to find the exact minimum in a finite number of steps. ",
            "id": "3449130",
            "problem": "In Molecular Dynamics (MD), energy minimization of a harmonic approximation to the potential energy around a local basin can be modeled by a quadratic function of the form $$U(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x,$$ where $x \\in \\mathbb{R}^{n}$ collects the generalized coordinates of the system near an equilibrium, $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive-definite Hessian matrix of the potential, and $b \\in \\mathbb{R}^{n}$ represents the linear term arising from external forces or linearization of the gradient around a reference configuration. The minimizer $x^{\\star}$ satisfies the stationarity condition $$\\nabla U(x^{\\star}) = A x^{\\star} - b = 0,$$ which is a linear system of equations. Conjugate Gradient (CG) is widely used in MD for energy minimization because it exploits the structure of the symmetric positive-definite system to converge rapidly without storing or factorizing the matrix.\n\nConsider the energy function $$U(x) = \\frac{1}{2} x^{\\top} \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} x - \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}^{\\top} x,$$ where $x \\in \\mathbb{R}^{2}$. The goal is to minimize $U(x)$ using the Conjugate Gradient (CG) method applied to the equivalent linear system $$A x = b,$$ with $$A = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.$$ Use the standard Conjugate Gradient (CG) algorithm with initial guess $$x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},$$ and residual defined by $$r_{k} = b - A x_{k}.$$ Perform exactly two CG iterations to obtain $x_{2}$ and $r_{2}$. Let the error at iteration $k$ be $$e_{k} = x^{\\star} - x_{k},$$ where $x^{\\star}$ is the exact solution to $A x = b$, and define the $A$-norm of the error by $$\\| e_{k} \\|_{A} = \\sqrt{ e_{k}^{\\top} A e_{k} }.$$ Report the three quantities $x_{2}$, $r_{2}$, and $\\| e_{2} \\|_{A}$ as exact values (no rounding). Express your final answer as a single composite row matrix containing these three items in the order specified.",
            "solution": "The Conjugate Gradient algorithm solves the system $A x = b$ where $A$ is a symmetric positive-definite matrix. The algorithm proceeds as follows, starting with an initial guess $x_{0}$:\n\nInitialize:\n$k = 0$\n$r_{0} = b - A x_{0}$\n$p_{0} = r_{0}$\n\nFor $k = 0, 1, 2, \\dots$:\n1. Calculate the step size: $\\alpha_{k} = \\frac{r_{k}^{\\top} r_{k}}{p_{k}^{\\top} A p_{k}}$\n2. Update the solution: $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$\n3. Update the residual: $r_{k+1} = r_{k} - \\alpha_{k} A p_{k}$\n4. If $r_{k+1}$ is sufficiently small, stop.\n5. Calculate the improvement factor: $\\beta_{k} = \\frac{r_{k+1}^{\\top} r_{k+1}}{r_{k}^{\\top} r_{k}}$\n6. Update the search direction: $p_{k+1} = r_{k+1} + \\beta_{k} p_{k}$\n\nWe are asked to perform exactly two iterations ($k=0$ and $k=1$) starting from $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\n**Initialization ($k=0$)**\nThe initial guess is $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe initial residual is $r_{0} = b - A x_{0} = b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe initial search direction is $p_{0} = r_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n\n**First Iteration ($k=0$)**\n1.  Calculate $\\alpha_{0}$:\n    $r_{0}^{\\top} r_{0} = 1^{2} + 2^{2} = 5$.\n    $A p_{0} = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}$.\n    $p_{0}^{\\top} A p_{0} = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = 6 + 14 = 20$.\n    $\\alpha_{0} = \\frac{5}{20} = \\frac{1}{4}$.\n\n2.  Update the solution to get $x_{1}$:\n    $x_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1/4 \\\\ 1/2 \\end{pmatrix}$.\n\n3.  Update the residual to get $r_{1}$:\n    $r_{1} = r_{0} - \\alpha_{0} A p_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 1 - 3/2 \\\\ 2 - 7/4 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ 1/4 \\end{pmatrix}$.\n\n4.  Calculate $\\beta_{0}$:\n    $r_{1}^{\\top} r_{1} = (-\\frac{1}{2})^{2} + (\\frac{1}{4})^{2} = \\frac{1}{4} + \\frac{1}{16} = \\frac{5}{16}$.\n    $\\beta_{0} = \\frac{r_{1}^{\\top} r_{1}}{r_{0}^{\\top} r_{0}} = \\frac{5/16}{5} = \\frac{1}{16}$.\n\n5.  Update the search direction to get $p_{1}$:\n    $p_{1} = r_{1} + \\beta_{0} p_{0} = \\begin{pmatrix} -1/2 \\\\ 1/4 \\end{pmatrix} + \\frac{1}{16} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -8/16 + 1/16 \\\\ 4/16 + 2/16 \\end{pmatrix} = \\begin{pmatrix} -7/16 \\\\ 6/16 \\end{pmatrix}$.\n\n**Second Iteration ($k=1$)**\n1.  Calculate $\\alpha_{1}$:\n    $A p_{1} = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} -7/16 \\\\ 6/16 \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} -28 + 6 \\\\ -7 + 18 \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} -22 \\\\ 11 \\end{pmatrix}$.\n    $p_{1}^{\\top} A p_{1} = \\frac{1}{16}\\begin{pmatrix} -7  6 \\end{pmatrix} \\frac{1}{16}\\begin{pmatrix} -22 \\\\ 11 \\end{pmatrix} = \\frac{1}{256} (154 + 66) = \\frac{220}{256}$.\n    $\\alpha_{1} = \\frac{r_{1}^{\\top} r_{1}}{p_{1}^{\\top} A p_{1}} = \\frac{5/16}{220/256} = \\frac{5}{16} \\frac{256}{220} = \\frac{5 \\times 16}{220} = \\frac{80}{220} = \\frac{4}{11}$.\n\n2.  Update the solution to get $x_{2}$:\n    $x_{2} = x_{1} + \\alpha_{1} p_{1} = \\begin{pmatrix} 1/4 \\\\ 1/2 \\end{pmatrix} + \\frac{4}{11} \\begin{pmatrix} -7/16 \\\\ 6/16 \\end{pmatrix} = \\begin{pmatrix} 1/4 - 7/44 \\\\ 1/2 + 6/44 \\end{pmatrix} = \\begin{pmatrix} 11/44 - 7/44 \\\\ 22/44 + 6/44 \\end{pmatrix} = \\begin{pmatrix} 4/44 \\\\ 28/44 \\end{pmatrix} = \\begin{pmatrix} 1/11 \\\\ 7/11 \\end{pmatrix}$.\n    This is the first required quantity, $x_2$.\n\n3.  Update the residual to get $r_{2}$:\n    $r_{2} = r_{1} - \\alpha_{1} A p_{1} = \\begin{pmatrix} -1/2 \\\\ 1/4 \\end{pmatrix} - \\frac{4}{11} \\left( \\frac{1}{16} \\begin{pmatrix} -22 \\\\ 11 \\end{pmatrix} \\right) = \\begin{pmatrix} -1/2 \\\\ 1/4 \\end{pmatrix} - \\frac{1}{44} \\begin{pmatrix} -22 \\\\ 11 \\end{pmatrix} = \\begin{pmatrix} -1/2 + 1/2 \\\\ 1/4 - 1/4 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n    This is the second required quantity, $r_2$.\n\n**Error Calculation**\nThe exact solution $x^{\\star}$ is found when the residual is zero. Since $r_{2} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $x_{2}$ is the exact solution ($x_{2} = x^{\\star}$). This is expected, as the CG method is guaranteed to find the exact solution in at most $n=2$ iterations.\nThe error at iteration $k=2$ is $e_{2} = x^{\\star} - x_{2} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe $A$-norm of the error $e_{2}$ is:\n$$\\| e_{2} \\|_{A} = \\sqrt{ e_{2}^{\\top} A e_{2} } = \\sqrt{ \\begin{pmatrix} 0  0 \\end{pmatrix} \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} } = \\sqrt{0} = 0$$\nThis is the third required quantity, $\\| e_{2} \\|_{A}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\begin{pmatrix} \\frac{1}{11} \\\\ \\frac{7}{11} \\end{pmatrix}  \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}  0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While the conjugate gradient method is guaranteed to converge for quadratic functions, its application to the complex, non-quadratic energy landscapes of real molecular systems introduces new challenges. The theoretical assumptions that guarantee success in the quadratic case no longer hold perfectly. This advanced practice explores a critical issue with the popular Polak-Ribière (PR) formulation of nonlinear CG, where the calculated search direction can fail to be a descent direction, and demonstrates how the simple \"PR+\" modification ensures the algorithm's robustness. ",
            "id": "3449183",
            "problem": "Consider energy minimization in Molecular Dynamics (MD) for a coarse-grained two-degree-of-freedom system with potential energy $U(\\mathbf{x})$, where $\\mathbf{x} \\in \\mathbb{R}^{2}$ and the gradient $\\mathbf{g}(\\mathbf{x}) = \\nabla U(\\mathbf{x})$ provides the forces driving minimization. A direction $\\mathbf{p}$ is a descent direction if and only if $\\mathbf{p}^{\\top} \\mathbf{g}  0$. Nonlinear Conjugate Gradient (NCG) methods construct successive search directions by combining steepest descent with a memory term: at iteration $k$, a new direction is formed by $\\mathbf{p}_{k+1} = -\\mathbf{g}_{k+1} + \\beta_{k} \\mathbf{p}_{k}$, where $\\mathbf{g}_{k} = \\mathbf{g}(\\mathbf{x}_{k})$.\n\nStarting from the definitions above and the quadratic-model conjugacy condition for symmetric positive definite Hessians (i.e., $A$-conjugacy $ \\mathbf{p}_{k+1}^{\\top} A \\mathbf{p}_{k} = 0$ together with the exact line-search property $ \\mathbf{g}_{k+1} - \\mathbf{g}_{k} = \\alpha_{k} A \\mathbf{p}_{k}$ for some step length $\\alpha_{k}$), derive the Polak–Ribiere (PR) expression for $\\beta_{k}$ purely in terms of the gradients $\\mathbf{g}_{k}$ and $\\mathbf{g}_{k+1}$, and use it to compute $\\beta_{k}$ for the following concrete MD minimization snapshot:\n- Previous gradient $\\mathbf{g}_{k} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$.\n- Current gradient $\\mathbf{g}_{k+1} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$.\n- Previous direction $\\mathbf{p}_{k} = \\begin{pmatrix} -10 \\\\ -10 \\end{pmatrix}$ (obtained from earlier iterations on the same rugged energy surface).\n\nThen, using the computed $\\beta_{k}$, evaluate the directional derivative $\\mathbf{p}_{k+1}^{\\top} \\mathbf{g}_{k+1}$ with $\\mathbf{p}_{k+1} = -\\mathbf{g}_{k+1} + \\beta_{k} \\mathbf{p}_{k}$ to assess descent. Finally, apply the Polak–Ribiere Plus (PR+) modification, which truncates the parameter to $\\beta_{k}^{+} = \\max\\{0, \\beta_{k}\\}$, and argue that this restores descent by yielding $\\mathbf{p}_{k+1}^{(+)} = -\\mathbf{g}_{k+1}$. \n\nProvide the truncated PR+ parameter $\\beta_{k}^{+}$ as your final answer. No rounding is required, and no units are needed.",
            "solution": "The problem asks for the derivation of the Polak-Ribière (PR) parameter $\\beta_k$, its calculation for a specific case, an analysis of the resulting search direction, and the application of the Polak-Ribière Plus (PR+) modification.\n\nFirst, we derive the expression for the Polak-Ribière parameter $\\beta_k$. The derivation starts from the conjugacy condition for a quadratic model with a symmetric positive definite Hessian matrix $A$. The search directions must be $A$-conjugate, meaning:\n$$ \\mathbf{p}_{k+1}^{\\top} A \\mathbf{p}_{k} = 0 $$\nThe Nonlinear Conjugate Gradient (NCG) update rule for the search direction is given as:\n$$ \\mathbf{p}_{k+1} = -\\mathbf{g}_{k+1} + \\beta_{k} \\mathbf{p}_{k} $$\nSubstituting this expression for $\\mathbf{p}_{k+1}$ into the conjugacy condition yields:\n$$ (-\\mathbf{g}_{k+1} + \\beta_{k} \\mathbf{p}_{k})^{\\top} A \\mathbf{p}_{k} = 0 $$\nExpanding this expression gives:\n$$ -\\mathbf{g}_{k+1}^{\\top} A \\mathbf{p}_{k} + \\beta_{k} (\\mathbf{p}_{k}^{\\top} A \\mathbf{p}_{k}) = 0 $$\nSolving for $\\beta_k$, we obtain:\n$$ \\beta_{k} = \\frac{\\mathbf{g}_{k+1}^{\\top} A \\mathbf{p}_{k}}{\\mathbf{p}_{k}^{\\top} A \\mathbf{p}_{k}} $$\nTo eliminate the explicit dependence on the Hessian $A$, we use the property derived from an exact line search along $\\mathbf{p}_k$:\n$$ \\mathbf{g}_{k+1} - \\mathbf{g}_{k} = \\alpha_{k} A \\mathbf{p}_{k} $$\nwhere $\\alpha_k$ is the step length. Let's define the gradient difference vector $\\mathbf{y}_k = \\mathbf{g}_{k+1} - \\mathbf{g}_{k}$. Then, $A \\mathbf{p}_{k} = \\frac{1}{\\alpha_k}\\mathbf{y}_k$. Substituting this into the expression for $\\beta_k$:\n$$ \\beta_{k} = \\frac{\\mathbf{g}_{k+1}^{\\top} (\\frac{1}{\\alpha_k}\\mathbf{y}_k)}{\\mathbf{p}_{k}^{\\top} (\\frac{1}{\\alpha_k}\\mathbf{y}_k)} = \\frac{\\mathbf{g}_{k+1}^{\\top} \\mathbf{y}_k}{\\mathbf{p}_{k}^{\\top} \\mathbf{y}_k} $$\nThis is the Hestenes-Stiefel formula. To arrive at the Polak-Ribière formula, we manipulate the denominator. For conjugate gradient methods on a quadratic function with exact line searches, successive gradients are orthogonal, and the gradient at step $k+1$ is orthogonal to the search direction at step $k$. That is, $\\mathbf{g}_{k+1}^{\\top}\\mathbf{p}_{k} = 0$. Also, the search direction update $\\mathbf{p}_k = -\\mathbf{g}_k + \\beta_{k-1}\\mathbf{p}_{k-1}$ and the orthogonality of the gradient $\\mathbf{g}_k$ to the previous search direction $\\mathbf{p}_{k-1}$ (due to exact line search in the previous step) implies $\\mathbf{p}_k^\\top \\mathbf{g}_k = -\\mathbf{g}_k^\\top \\mathbf{g}_k = -\\|\\mathbf{g}_k\\|^2$.\nUsing these properties, the denominator becomes:\n$$ \\mathbf{p}_{k}^{\\top} \\mathbf{y}_k = \\mathbf{p}_{k}^{\\top} (\\mathbf{g}_{k+1} - \\mathbf{g}_{k}) = \\mathbf{p}_{k}^{\\top} \\mathbf{g}_{k+1} - \\mathbf{p}_{k}^{\\top} \\mathbf{g}_{k} = 0 - (-\\|\\mathbf{g}_k\\|^2) = \\|\\mathbf{g}_k\\|^2 = \\mathbf{g}_{k}^{\\top}\\mathbf{g}_{k} $$\nSubstituting this into the Hestenes-Stiefel formula gives the Polak-Ribière formula:\n$$ \\beta_k^{\\text{PR}} = \\frac{\\mathbf{g}_{k+1}^{\\top} (\\mathbf{g}_{k+1} - \\mathbf{g}_{k})}{\\mathbf{g}_{k}^{\\top} \\mathbf{g}_{k}} $$\n\nNow, we compute $\\beta_k$ using the provided data:\n$\\mathbf{g}_{k} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$ and $\\mathbf{g}_{k+1} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$.\nFirst, calculate the terms needed for the formula:\nThe denominator is the squared norm of $\\mathbf{g}_k$:\n$$ \\mathbf{g}_{k}^{\\top} \\mathbf{g}_{k} = (3)^2 + (4)^2 = 9 + 16 = 25 $$\nThe gradient difference vector is:\n$$ \\mathbf{y}_k = \\mathbf{g}_{k+1} - \\mathbf{g}_{k} = \\begin{pmatrix} 2 - 3 \\\\ 2 - 4 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -2 \\end{pmatrix} $$\nThe numerator is the dot product of $\\mathbf{g}_{k+1}$ and $\\mathbf{y}_k$:\n$$ \\mathbf{g}_{k+1}^{\\top} (\\mathbf{g}_{k+1} - \\mathbf{g}_{k}) = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}^{\\top} \\begin{pmatrix} -1 \\\\ -2 \\end{pmatrix} = (2)(-1) + (2)(-2) = -2 - 4 = -6 $$\nThus, the Polak-Ribière parameter is:\n$$ \\beta_k = \\frac{-6}{25} $$\n\nNext, we compute the new search direction $\\mathbf{p}_{k+1}$ and check if it is a descent direction. The previous direction is given as $\\mathbf{p}_{k} = \\begin{pmatrix} -10 \\\\ -10 \\end{pmatrix}$.\n$$ \\mathbf{p}_{k+1} = -\\mathbf{g}_{k+1} + \\beta_{k} \\mathbf{p}_{k} = -\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} + \\left(\\frac{-6}{25}\\right) \\begin{pmatrix} -10 \\\\ -10 \\end{pmatrix} $$\n$$ \\mathbf{p}_{k+1} = \\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix} + \\begin{pmatrix} \\frac{60}{25} \\\\ \\frac{60}{25} \\end{pmatrix} = \\begin{pmatrix} -2 + \\frac{12}{5} \\\\ -2 + \\frac{12}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{-10+12}{5} \\\\ \\frac{-10+12}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{pmatrix} $$\nA direction $\\mathbf{p}$ is a descent direction if $\\mathbf{p}^{\\top} \\mathbf{g}  0$. We check this condition for $\\mathbf{p}_{k+1}$ and $\\mathbf{g}_{k+1}$:\n$$ \\mathbf{p}_{k+1}^{\\top} \\mathbf{g}_{k+1} = \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{pmatrix}^{\\top} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\left(\\frac{2}{5}\\right)(2) + \\left(\\frac{2}{5}\\right)(2) = \\frac{4}{5} + \\frac{4}{5} = \\frac{8}{5} $$\nSince $\\frac{8}{5}  0$, the direction $\\mathbf{p}_{k+1}$ is not a descent direction. This can happen with the PR method when far from a local minimum.\n\nFinally, we apply the Polak-Ribière Plus (PR+) modification. This modification is designed to ensure descent by truncating $\\beta_k$:\n$$ \\beta_{k}^{+} = \\max\\{0, \\beta_{k}\\} $$\nUsing our calculated value $\\beta_k = -6/25$:\n$$ \\beta_{k}^{+} = \\max\\left\\{0, \\frac{-6}{25}\\right\\} = 0 $$\nThe new search direction using the PR+ parameter is $\\mathbf{p}_{k+1}^{(+)}$:\n$$ \\mathbf{p}_{k+1}^{(+)} = -\\mathbf{g}_{k+1} + \\beta_{k}^{+} \\mathbf{p}_{k} = -\\mathbf{g}_{k+1} + (0)\\mathbf{p}_{k} = -\\mathbf{g}_{k+1} $$\nThis modification effectively resets the CG algorithm to a steepest descent step. To verify that descent is restored, we check the directional derivative:\n$$ (\\mathbf{p}_{k+1}^{(+)})^{\\top} \\mathbf{g}_{k+1} = (-\\mathbf{g}_{k+1})^{\\top} \\mathbf{g}_{k+1} = -(\\mathbf{g}_{k+1}^{\\top} \\mathbf{g}_{k+1}) = -\\|\\mathbf{g}_{k+1}\\|^2 $$\nAs long as the gradient $\\mathbf{g}_{k+1}$ is non-zero (i.e., we are not at a stationary point), $\\|\\mathbf{g}_{k+1}\\|^2  0$, and thus $-\\|\\mathbf{g}_{k+1}\\|^2  0$. In our case, $\\|\\mathbf{g}_{k+1}\\|^2 = 2^2 + 2^2 = 8$. The directional derivative is $-8  0$, which confirms that $\\mathbf{p}_{k+1}^{(+)}$ is a descent direction. The PR+ modification successfully restored the descent property.\n\nThe question asks for the value of the truncated parameter $\\beta_{k}^{+}$.\n$$ \\beta_{k}^{+} = 0 $$",
            "answer": "$$\n\\boxed{0}\n$$"
        }
    ]
}