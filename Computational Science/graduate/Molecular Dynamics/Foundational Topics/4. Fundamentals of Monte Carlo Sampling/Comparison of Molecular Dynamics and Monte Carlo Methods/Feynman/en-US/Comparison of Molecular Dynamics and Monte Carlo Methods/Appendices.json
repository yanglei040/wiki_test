{
    "hands_on_practices": [
        {
            "introduction": "Molecular Dynamics simulations rely on numerical integrators to approximate the continuous equations of motion, with the velocity-Verlet algorithm being a popular choice due to its stability and symplecticity. This analytical exercise delves into the heart of this integrator by examining its behavior for a simple harmonic oscillator. By deriving the stability condition from first principles, you will gain a deep understanding of why the integration time step in MD is fundamentally limited by the fastest frequencies in the system, a constraint that has no direct parallel in Monte Carlo methods. ",
            "id": "3403189",
            "problem": "Consider a one-dimensional particle of mass $m$ in a harmonic potential $U(x) = \\frac{1}{2} k x^{2}$. Let $\\omega = \\sqrt{\\frac{k}{m}}$. A Molecular Dynamics (MD) trajectory generated by the velocity-Verlet integrator updates positions and velocities at discrete times $t_{n} = n \\Delta t$ according to Newton’s second law. Starting from first principles (Newton’s second law and the definition of the velocity-Verlet method), do the following:\n\n- Derive the linear, homogeneous recurrence that advances the state vector $y_{n} = \\begin{pmatrix} x_{n} \\\\ v_{n} \\end{pmatrix}$ to $y_{n+1}$ for a time step $\\Delta t$, and express it as $y_{n+1} = M(\\Delta t) \\, y_{n}$ with an explicit $2 \\times 2$ amplification matrix $M(\\Delta t)$ in terms of $\\omega$ and $\\Delta t$.\n\n- Using only the stability criterion for linear recurrences with constant coefficients (that the spectral radius of $M(\\Delta t)$ must not exceed $1$ for linear stability), derive the condition on $\\Delta t$ under which the velocity-Verlet integration of this harmonic oscillator is linearly stable. Report the maximum stable time step as a closed-form expression in terms of $\\omega$.\n\n- From the eigenvalues of $M(\\Delta t)$, obtain the discrete oscillation frequency $\\Omega(\\Delta t, \\omega)$ and determine the leading-order phase error of the integrator for small $\\Delta t$. Explain how the stability bound influences MD accuracy for configurational sampling and energy conservation, referring to the existence of a shadow (modified) Hamiltonian.\n\n- In the context of comparison with Monte Carlo (MC), briefly justify why a Metropolis algorithm targeting the Boltzmann distribution at temperature $T$ for the same harmonic system has no analogous linear stability limit on its proposal step scale, and explain the different mechanism by which proposal size affects MC accuracy and efficiency.\n\nExpress your final answer as the maximum stable time step in closed form in terms of $\\omega$. Do not include units in your final numeric answer. No rounding is required.",
            "solution": "The posed problem is valid as it is scientifically grounded in the principles of classical mechanics and numerical analysis, is well-posed with sufficient information for a unique solution, and is stated objectively. We may proceed with the solution.\n\nThe problem requires a multi-part analysis of the velocity-Verlet algorithm applied to a one-dimensional harmonic oscillator, followed by a conceptual comparison with the Metropolis Monte Carlo method.\n\nThe system is a particle of mass $m$ in a harmonic potential $U(x) = \\frac{1}{2} k x^2$. The equation of motion is given by Newton's second law:\n$$F = m \\ddot{x} = -\\frac{dU}{dx} = -kx$$\nThis can be rewritten in terms of the angular frequency $\\omega = \\sqrt{k/m}$ as:\n$$\\ddot{x} + \\omega^2 x = 0$$\nThe acceleration at any time is a function of position: $a(x) = -\\omega^2 x$. We denote the acceleration at time $t_n = n \\Delta t$ as $a_n = a(x_n) = -\\omega^2 x_n$.\n\n**Part 1: Derivation of the Amplification Matrix**\n\nThe velocity-Verlet integrator consists of the following update steps for position $x$ and velocity $v$ over a time step $\\Delta t$:\n$$x_{n+1} = x_n + v_n \\Delta t + \\frac{1}{2} a_n (\\Delta t)^2$$\n$$v_{n+1} = v_n + \\frac{1}{2} (a_n + a_{n+1}) \\Delta t$$\nWe will express these updates in the form of a matrix transformation on the state vector $y_n = \\begin{pmatrix} x_n \\\\ v_n \\end{pmatrix}$.\n\nFirst, we substitute $a_n = -\\omega^2 x_n$ into the position update equation:\n$$x_{n+1} = x_n + v_n \\Delta t - \\frac{1}{2} \\omega^2 x_n (\\Delta t)^2$$\n$$x_{n+1} = \\left(1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2\\right) x_n + (\\Delta t) v_n$$\nThis equation provides the first row of the amplification matrix $M(\\Delta t)$.\n\nNext, we address the velocity update. This requires the acceleration at the next time step, $a_{n+1} = -\\omega^2 x_{n+1}$. We substitute the expression for $x_{n+1}$ we just derived:\n$$a_{n+1} = -\\omega^2 \\left[ \\left(1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2\\right) x_n + (\\Delta t) v_n \\right]$$\nNow, substitute both $a_n$ and $a_{n+1}$ into the velocity update equation:\n$$v_{n+1} = v_n + \\frac{1}{2} \\Delta t \\left( -\\omega^2 x_n - \\omega^2 \\left[ \\left(1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2\\right) x_n + (\\Delta t) v_n \\right] \\right)$$\n$$v_{n+1} = v_n - \\frac{\\omega^2 \\Delta t}{2} \\left( x_n + \\left(1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2\\right) x_n + (\\Delta t) v_n \\right)$$\n$$v_{n+1} = v_n - \\frac{\\omega^2 \\Delta t}{2} \\left( \\left(2 - \\frac{1}{2}\\omega^2 (\\Delta t)^2\\right) x_n + (\\Delta t) v_n \\right)$$\nDistributing the terms yields:\n$$v_{n+1} = -\\omega^2 \\Delta t \\left(1 - \\frac{1}{4}\\omega^2 (\\Delta t)^2\\right) x_n + \\left(1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2\\right) v_n$$\nThis provides the second row of the amplification matrix.\n\nCombining the expressions for $x_{n+1}$ and $v_{n+1}$, we can write the full recurrence relation:\n$$ y_{n+1} = \\begin{pmatrix} x_{n+1} \\\\ v_{n+1} \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2 & \\Delta t \\\\ -\\omega^2 \\Delta t \\left(1 - \\frac{1}{4}\\omega^2 (\\Delta t)^2\\right) & 1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2 \\end{pmatrix} \\begin{pmatrix} x_n \\\\ v_n \\end{pmatrix} $$\nThe amplification matrix is therefore:\n$$ M(\\Delta t) = \\begin{pmatrix} 1 - \\frac{1}{2}(\\omega \\Delta t)^2 & \\Delta t \\\\ -\\omega^2 \\Delta t \\left(1 - \\frac{1}{4}(\\omega \\Delta t)^2\\right) & 1 - \\frac{1}{2}(\\omega \\Delta t)^2 \\end{pmatrix} $$\n\n**Part 2: Derivation of the Stability Condition**\n\nLinear stability requires that the spectral radius of the amplification matrix, $\\rho(M) = \\max_i |\\lambda_i|$, does not exceed $1$. The eigenvalues $\\lambda$ of $M$ are found from the characteristic equation $\\det(M - \\lambda I) = 0$.\nLet $c = 1 - \\frac{1}{2}(\\omega \\Delta t)^2$. The characteristic equation is:\n$$ (c - \\lambda)^2 - (\\Delta t) \\left[ -\\omega^2 \\Delta t \\left(1 - \\frac{1}{4}(\\omega \\Delta t)^2\\right) \\right] = 0 $$\n$$ (c - \\lambda)^2 = - (\\omega \\Delta t)^2 \\left(1 - \\frac{1}{4}(\\omega \\Delta t)^2\\right) $$\n$$ \\lambda - c = \\pm i (\\omega \\Delta t) \\sqrt{1 - \\frac{1}{4}(\\omega \\Delta t)^2} $$\nThe eigenvalues are:\n$$ \\lambda_{\\pm} = 1 - \\frac{1}{2}(\\omega \\Delta t)^2 \\pm i (\\omega \\Delta t) \\sqrt{1 - \\frac{1}{4}(\\omega \\Delta t)^2} $$\nThe stability depends on the nature of the term under the square root.\n\nCase 1: $1 - \\frac{1}{4}(\\omega \\Delta t)^2 \\ge 0$, which means $(\\omega \\Delta t)^2 \\le 4$, or $\\omega \\Delta t \\le 2$.\nIn this case, the eigenvalues are a complex conjugate pair. The modulus squared of each eigenvalue is:\n$$ |\\lambda|^2 = \\left(1 - \\frac{1}{2}(\\omega \\Delta t)^2\\right)^2 + \\left((\\omega \\Delta t) \\sqrt{1 - \\frac{1}{4}(\\omega \\Delta t)^2}\\right)^2 $$\n$$ |\\lambda|^2 = \\left(1 - (\\omega \\Delta t)^2 + \\frac{1}{4}(\\omega \\Delta t)^4\\right) + (\\omega \\Delta t)^2 \\left(1 - \\frac{1}{4}(\\omega \\Delta t)^2\\right) $$\n$$ |\\lambda|^2 = 1 - (\\omega \\Delta t)^2 + \\frac{1}{4}(\\omega \\Delta t)^4 + (\\omega \\Delta t)^2 - \\frac{1}{4}(\\omega \\Delta t)^4 = 1 $$\nThus, $|\\lambda| = 1$ for all $\\omega \\Delta t \\le 2$. The method is stable in this regime.\n\nCase 2: $\\omega \\Delta t > 2$.\nIn this case, the term under the square root is negative, and the eigenvalues become real:\n$$ \\lambda_{\\pm} = 1 - \\frac{1}{2}(\\omega \\Delta t)^2 \\pm (\\omega \\Delta t) \\sqrt{\\frac{1}{4}(\\omega \\Delta t)^2 - 1} $$\nConsider the eigenvalue $\\lambda_{-}$. Since $\\omega \\Delta t > 2$, the term $1 - \\frac{1}{2}(\\omega \\Delta t)^2$ is less than $1 - \\frac{1}{2}(4) = -1$. The second term is also negative. Thus, $\\lambda_{-} < -1$, and $|\\lambda_{-}| > 1$.\nThe spectral radius $\\rho(M)$ is greater than $1$, and the integration is unstable.\n\nThe stability boundary is precisely at $\\omega \\Delta t = 2$. Therefore, the condition for linear stability is $\\omega \\Delta t \\le 2$. The maximum stable time step is:\n$$ \\Delta t_{\\text{max}} = \\frac{2}{\\omega} $$\nThis corresponds to approximately $3$ integration steps per oscillation period $T = 2\\pi/\\omega$, since $ T / \\Delta t_{\\text{max}} = \\pi \\approx 3.14$.\n\n**Part 3: Discrete Frequency, Phase Error, and Implications**\n\nIn the stable regime, the eigenvalues are $\\lambda_{\\pm} = \\exp(\\pm i \\Omega \\Delta t)$, where $\\Omega$ is the discrete (numerical) frequency. The trace of the matrix is $\\text{Tr}(M) = \\lambda_{+} + \\lambda_{-} = 2 \\cos(\\Omega \\Delta t)$. From our matrix $M$, we have $\\text{Tr}(M) = 2 \\left(1 - \\frac{1}{2}(\\omega \\Delta t)^2\\right)$. Equating these gives:\n$$ 2 \\cos(\\Omega \\Delta t) = 2 - (\\omega \\Delta t)^2 $$\n$$ \\cos(\\Omega \\Delta t) = 1 - \\frac{1}{2}(\\omega \\Delta t)^2 $$\nThis equation implicitly defines $\\Omega$. For small $\\Delta t$, we can find the leading-order error. We use the Taylor series for $\\cos(\\theta) = 1 - \\theta^2/2! + \\theta^4/4! - \\dots$. Let $\\theta = \\Omega \\Delta t$ and $z = \\omega \\Delta t$.\n$$ 1 - \\frac{(\\Omega \\Delta t)^2}{2} + \\frac{(\\Omega \\Delta t)^4}{24} - \\dots = 1 - \\frac{(\\omega \\Delta t)^2}{2} $$\nAssuming $\\Omega \\approx \\omega$ for small $\\Delta t$, we can approximate $(\\Omega \\Delta t)^4 \\approx (\\omega \\Delta t)^4$ in the first-order correction term:\n$$ \\frac{(\\Omega \\Delta t)^2}{2} \\approx \\frac{(\\omega \\Delta t)^2}{2} + \\frac{(\\omega \\Delta t)^4}{24} $$\n$$ \\Omega^2 \\approx \\omega^2 \\left(1 + \\frac{(\\omega \\Delta t)^2}{12}\\right) $$\n$$ \\Omega \\approx \\omega \\sqrt{1 + \\frac{(\\omega \\Delta t)^2}{12}} \\approx \\omega \\left(1 + \\frac{(\\omega \\Delta t)^2}{24}\\right) $$\nThe leading-order relative phase error is $(\\Omega - \\omega) / \\omega = (\\omega \\Delta t)^2 / 24$. This is a phase lead, meaning the numerical oscillator is slightly faster than the true oscillator.\n\nThe stability bound and accuracy have significant implications. The velocity-Verlet algorithm is symplectic, which implies that for any $\\Delta t$ in the stable regime, the numerical trajectory exactly conserves a \"shadow Hamiltonian\" $H_{\\text{shadow}}$ that is close to the true Hamiltonian $H$. The error is $H_{\\text{shadow}} - H = O((\\Delta t)^2)$. This ensures excellent long-term energy conservation (no drift), with only bounded oscillations of the true energy $H$.\nHowever, accuracy for configurational sampling requires that $H_{\\text{shadow}}$ is a good approximation of $H$, which is only true for $\\Delta t \\ll \\Delta t_{\\text{max}}$. As $\\Delta t$ approaches the stability limit $2/\\omega$, the phase error becomes large, and the trajectory deviates significantly from the true dynamics, even though it remains on a perturbed (shadow) constant-energy surface. For calculating static properties this may be tolerable, but for time-dependent properties, this phase error introduces significant inaccuracies.\n\n**Part 4: Comparison with Monte Carlo**\n\nA Metropolis Monte Carlo (MC) simulation of the same harmonic system aims to generate a sequence of positions $x$ that follow the Boltzmann distribution $P(x) \\propto \\exp(-U(x)/k_B T)$. A trial move is proposed, $x_{\\text{trial}} = x_{\\text{old}} + \\delta$, where $\\delta$ is a random displacement. The move is accepted with probability $p_{\\text{acc}} = \\min(1, \\exp(-\\Delta U/k_B T))$.\n\nThis process has no analogue to the linear stability limit of MD for the following reasons:\n1.  **Stochastic vs. Deterministic Dynamics:** MD integrates a deterministic equation of motion. A large time step causes discretization errors to amplify catastrophically, leading to instability. MC generates a stochastic Markov chain. There is no \"dynamics\" to become unstable.\n2.  **Built-in Stabilizing Mechanism:** The acceptance criterion $\\min(1, \\exp(-\\Delta U/k_B T))$ inherently prevents the system from \"blowing up\". A very large proposed step $\\delta$ will likely lead to a large, positive energy change $\\Delta U$, making the acceptance probability $\\exp(-\\Delta U/k_B T)$ approach zero. The system will simply reject the move and remain in its current, finite-energy state. There is no mechanism for unbounded energy growth.\n\nThe proposal step size in MC does not affect stability but profoundly affects **efficiency**.\n-   If the proposal size is too small, nearly all moves are accepted, but the system explores configuration space very slowly (high correlation between successive states).\n-   If the proposal size is too large, nearly all moves are rejected, and the system also explores configuration space very slowly (it remains stuck in the same state).\nThe efficiency of an MC simulation is optimized by tuning the proposal size to achieve a moderate acceptance rate (typically in the $20-50\\%$ range), which maximizes the rate of decorrelation of states and allows for rapid exploration of the phase space. This impacts the practical \"accuracy\" of a finite-length simulation by reducing the statistical error on calculated averages.",
            "answer": "$$\\boxed{\\frac{2}{\\omega}}$$"
        },
        {
            "introduction": "A key challenge in comparing Molecular Dynamics and Monte Carlo methods is their different conceptions of \"time\": MD propagates a system through physical time, while MC generates a stochastic sequence of configurations. This practice builds a crucial conceptual bridge by showing how to map the abstract steps of an MC simulation to the physical timescale of Brownian Dynamics. By calibrating the MC proposal step size to match the physical diffusion rate, you will derive an effective time mapping that makes the two methods directly comparable and imbues the MC trajectory with a meaningful dynamic interpretation. ",
            "id": "3403190",
            "problem": "Consider a single particle of mass $m$ undergoing overdamped stochastic motion in $d$ spatial dimensions. In the overdamped limit, Brownian Dynamics (BD) emerges from the Langevin equation under the assumption of high friction and negligible inertia, and the Einstein relation yields the mean squared displacement $\\langle r^{2}(t)\\rangle = 2 d D t$, where $D$ is the diffusion coefficient and $t$ is time. In a Markov Chain Monte Carlo (MC) simulation, one can propose trial displacements $\\delta \\mathbf{r}$ drawn independently from an isotropic distribution. Suppose the proposal for each component is a Gaussian with zero mean and variance $\\Delta^{2}$ (so the total squared step has expectation $d \\Delta^{2}$), and the acceptance probability per attempt is a constant $0 < \\alpha \\leq 1$ that summarizes the interaction effects. Each MC attempt incurs a constant computational cost $c_{\\mathrm{MC}}$ in seconds, regardless of acceptance.\n\nYour tasks are:\n\n- From fundamental definitions, derive the expected mean squared displacement accumulated by MC attempts per unit of central processing unit (CPU) time, as a function of $d$, $D$, $\\Delta$, $\\alpha$, and $c_{\\mathrm{MC}}$.\n- Enforce the requirement that the MC algorithm emulate BD by matching the MC mean squared displacement per unit CPU time with the BD mean squared displacement rate. From this requirement, derive a calibration for the proposal variance parameter $\\Delta$ in terms of $D$, $\\alpha$, and $c_{\\mathrm{MC}}$.\n- Using the Einstein relation, derive an effective time mapping $t_{\\mathrm{eff}}(N)$ that assigns an equivalent BD time to a trajectory consisting of $N$ MC attempts, in terms of $D$, $\\Delta$, $\\alpha$, and $N$. Explain any dependence or independence on the dimension $d$.\n- Show that with the calibrated $\\Delta$, $t_{\\mathrm{eff}}(N)$ simplifies to a form that enables direct interpretability of MC trajectories in wall-clock units.\n\nFor implementation, write a complete runnable program that, for each parameter tuple $(d, D, c_{\\mathrm{MC}}, \\alpha, N)$ in the test suite below, computes two outputs: the calibrated proposal scale $\\Delta$ and the effective time $t_{\\mathrm{eff}}(N)$ corresponding to $N$ MC attempts. Use the following test suite, where $D$ is provided in $\\mathrm{m}^{2}/\\mathrm{s}$, $c_{\\mathrm{MC}}$ in $\\mathrm{s}$ per attempt, $\\alpha$ as a unitless decimal fraction, and $N$ as an integer:\n\n- Case $1$: $d = 3$, $D = 1.0 \\times 10^{-12}\\ \\mathrm{m}^{2}/\\mathrm{s}$, $c_{\\mathrm{MC}} = 5.0 \\times 10^{-7}\\ \\mathrm{s}$, $\\alpha = 0.8$, $N = 100000$.\n- Case $2$: $d = 2$, $D = 2.5 \\times 10^{-9}\\ \\mathrm{m}^{2}/\\mathrm{s}$, $c_{\\mathrm{MC}} = 1.0 \\times 10^{-5}\\ \\mathrm{s}$, $\\alpha = 0.2$, $N = 5000$.\n- Case $3$: $d = 1$, $D = 1.0 \\times 10^{-10}\\ \\mathrm{m}^{2}/\\mathrm{s}$, $c_{\\mathrm{MC}} = 1.0 \\times 10^{-4}\\ \\mathrm{s}$, $\\alpha = 0.01$, $N = 100$.\n- Case $4$: $d = 3$, $D = 1.0 \\times 10^{-11}\\ \\mathrm{m}^{2}/\\mathrm{s}$, $c_{\\mathrm{MC}} = 1.0 \\times 10^{-6}\\ \\mathrm{s}$, $\\alpha = 0.5$, $N = 0$.\n\nYour program should output a single line containing a comma-separated list of pairs, one per test case, with each pair itself written as a comma-separated list in square brackets. The first element of each pair must be $\\Delta$ in $\\mathrm{m}$ and the second element must be $t_{\\mathrm{eff}}(N)$ in $\\mathrm{s}$. For example, your output format must look like $[[\\Delta_{1},t_{\\mathrm{eff},1}],[\\Delta_{2},t_{\\mathrm{eff},2}],\\ldots]$ on one line, with no additional spaces.",
            "solution": "The problem requires the derivation of a relationship between Monte Carlo (MC) simulation parameters and the physical parameters of Brownian Dynamics (BD), followed by the implementation of the derived formulas. The validation of the problem statement confirms that it is scientifically grounded, well-posed, and contains all necessary information.\n\nThe derivation proceeds in four steps as requested.\n\n**1. Expected Mean Squared Displacement per Unit CPU Time in MC**\n\nIn a Markov Chain Monte Carlo (MC) simulation, a particle's position is updated through a series of trial moves. For each attempt, a trial displacement $\\delta \\mathbf{r}$ is proposed. This move is accepted with a constant probability $\\alpha$, resulting in a new position $\\mathbf{r}' = \\mathbf{r} + \\delta \\mathbf{r}$. The move is rejected with probability $1 - \\alpha$, and the position remains unchanged, $\\mathbf{r}' = \\mathbf{r}$.\n\nLet us calculate the expected squared displacement for a single MC attempt. The squared displacement is $|\\delta \\mathbf{r}|^2$ if the move is accepted and $0$ if rejected. The expectation value, $\\langle (\\Delta \\mathbf{r})^2 \\rangle_{\\text{attempt}}$, over one attempt is thus:\n$$ \\langle (\\Delta \\mathbf{r})^2 \\rangle_{\\text{attempt}} = \\alpha \\cdot E[|\\delta \\mathbf{r}|^2] + (1 - \\alpha) \\cdot 0 = \\alpha E[|\\delta \\mathbf{r}|^2] $$\nThe problem states that each component of the trial displacement, $\\delta r_i$ for $i=1, \\ldots, d$, is drawn from an isotropic Gaussian distribution with zero mean and variance $\\Delta^2$. The expectation of the square of such a component is $E[(\\delta r_i)^2] = \\text{Var}(\\delta r_i) + (E[\\delta r_i])^2 = \\Delta^2 + 0^2 = \\Delta^2$.\n\nThe total squared magnitude of the proposed step is $|\\delta \\mathbf{r}|^2 = \\sum_{i=1}^{d} (\\delta r_i)^2$. By the linearity of expectation, its expected value is:\n$$ E[|\\delta \\mathbf{r}|^2] = E\\left[\\sum_{i=1}^{d} (\\delta r_i)^2\\right] = \\sum_{i=1}^{d} E[(\\delta r_i)^2] = \\sum_{i=1}^{d} \\Delta^2 = d\\Delta^2 $$\nSubstituting this back, the expected squared displacement per MC attempt is:\n$$ \\langle (\\Delta \\mathbf{r})^2 \\rangle_{\\text{attempt}} = \\alpha d \\Delta^2 $$\nEach attempt incurs a computational cost of $c_{\\mathrm{MC}}$ seconds. Therefore, the expected mean squared displacement (MSD) accumulated per unit of CPU time, denoted $\\mathcal{R}_{\\mathrm{MC}}$, is the expected MSD per attempt divided by the cost per attempt:\n$$ \\mathcal{R}_{\\mathrm{MC}} = \\frac{\\langle (\\Delta \\mathbf{r})^2 \\rangle_{\\text{attempt}}}{c_{\\mathrm{MC}}} = \\frac{\\alpha d \\Delta^2}{c_{\\mathrm{MC}}} $$\nThis expression gives the rate of MSD accumulation in the MC simulation in units of $(\\text{length})^2 / (\\text{CPU time})$.\n\n**2. Calibration of the Proposal Variance Parameter $\\Delta$**\n\nTo emulate Brownian Dynamics (BD), we must match the long-term diffusive behavior of the MC simulation to that of BD. For BD, the Einstein relation gives the mean squared displacement as a function of physical time $t$:\n$$ \\langle r^2(t) \\rangle_{\\mathrm{BD}} = 2 d D t $$\nwhere $D$ is the diffusion coefficient. The rate of MSD accumulation in physical time for BD is:\n$$ \\mathcal{R}_{\\mathrm{BD}} = \\frac{d}{dt} \\langle r^2(t) \\rangle_{\\mathrm{BD}} = \\frac{d}{dt}(2 d D t) = 2dD $$\nThe problem requires calibrating the MC simulation by equating the MC rate of MSD accumulation per CPU time with the BD rate of MSD accumulation per physical time. This establishes a correspondence where one unit of CPU time in the simulation corresponds to one unit of physical time.\n$$ \\mathcal{R}_{\\mathrm{MC}} = \\mathcal{R}_{\\mathrm{BD}} $$\n$$ \\frac{\\alpha d \\Delta^2}{c_{\\mathrm{MC}}} = 2dD $$\nWe can solve this equation for the proposal variance parameter $\\Delta$. The dimension $d$ cancels from both sides:\n$$ \\alpha \\Delta^2 = 2 D c_{\\mathrm{MC}} $$\n$$ \\Delta^2 = \\frac{2 D c_{\\mathrm{MC}}}{\\alpha} $$\nSince $\\Delta$ represents a scale (standard deviation), we take the positive square root:\n$$ \\Delta = \\sqrt{\\frac{2 D c_{\\mathrm{MC}}}{\\alpha}} $$\nThis is the calibrated value for the proposal scale $\\Delta$ that makes the MC simulation's diffusive behavior, measured in CPU time, equivalent to the physical BD process.\n\n**3. Effective Time Mapping $t_{\\mathrm{eff}}(N)$**\n\nAn effective physical time $t_{\\mathrm{eff}}(N)$ can be assigned to a trajectory of $N$ MC attempts by equating the total MSD accumulated in the MC simulation to the MSD of a BD process over that effective time.\n\nThe total expected MSD after $N$ independent MC attempts is:\n$$ \\langle r^2(N) \\rangle_{\\mathrm{MC}} = N \\cdot \\langle (\\Delta \\mathbf{r})^2 \\rangle_{\\text{attempt}} = N \\alpha d \\Delta^2 $$\nThe MSD for a BD process of duration $t_{\\mathrm{eff}}$ is:\n$$ \\langle r^2(t_{\\mathrm{eff}}) \\rangle_{\\mathrm{BD}} = 2 d D t_{\\mathrm{eff}}(N) $$\nEquating the two MSDs yields:\n$$ N \\alpha d \\Delta^2 = 2 d D t_{\\mathrm{eff}}(N) $$\nSolving for $t_{\\mathrm{eff}}(N)$:\n$$ t_{\\mathrm{eff}}(N) = \\frac{N \\alpha \\Delta^2}{2D} $$\nThis expression for the effective time depends on the MC parameters $N$, $\\alpha$, and $\\Delta$, as well as the physical parameter $D$. Notably, it is independent of the spatial dimension $d$. This independence arises because both the MC and BD mean squared displacements scale linearly with $d$ for an isotropic system, causing the factor of $d$ to cancel during the derivation.\n\n**4. Simplification of $t_{\\mathrm{eff}}(N)$ with Calibrated $\\Delta$**\n\nWe now substitute the calibrated expression for $\\Delta^2$ from Step 2 into the formula for $t_{\\mathrm{eff}}(N)$ from Step 3.\nThe calibrated $\\Delta^2$ is $\\Delta^2 = \\frac{2 D c_{\\mathrm{MC}}}{\\alpha}$.\nThe effective time is $t_{\\mathrm{eff}}(N) = \\frac{N \\alpha \\Delta^2}{2D}$.\nSubstituting:\n$$ t_{\\mathrm{eff}}(N) = \\frac{N \\alpha}{2D} \\left( \\frac{2 D c_{\\mathrm{MC}}}{\\alpha} \\right) $$\nThe terms $2D$ in the numerator and denominator cancel, as does the acceptance probability $\\alpha$. This leads to a remarkably simple result:\n$$ t_{\\mathrm{eff}}(N) = N c_{\\mathrm{MC}} $$\nThis result shows that when the MC proposal width $\\Delta$ is calibrated correctly, the effective physical time simulated by $N$ MC attempts is precisely equal to the total CPU time spent on those attempts ($T_{\\mathrm{CPU}} = N c_{\\mathrm{MC}}$). This direct correspondence allows one to interpret the computational cost of an MC simulation in physical time units, a crucial link for comparing simulation results to experimental timescales.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the calibrated MC proposal scale (Delta) and the effective\n    time (t_eff) for a series of test cases based on derived formulas\n    linking Monte Carlo and Brownian Dynamics simulations.\n    \"\"\"\n    # Test suite: (d, D, c_MC, alpha, N)\n    # D is in m^2/s, c_MC is in s, alpha is unitless, N is an integer.\n    test_cases = [\n        # Case 1\n        (3, 1.0e-12, 5.0e-7, 0.8, 100000),\n        # Case 2\n        (2, 2.5e-9, 1.0e-5, 0.2, 5000),\n        # Case 3\n        (1, 1.0e-10, 1.0e-4, 0.01, 100),\n        # Case 4\n        (3, 1.0e-11, 1.0e-6, 0.5, 0),\n    ]\n\n    results = []\n    for case in test_cases:\n        d, D, c_mc, alpha, N = case\n\n        # Task 2: Calculate the calibrated proposal scale Delta.\n        # Formula: Delta = sqrt(2 * D * c_mc / alpha)\n        # The dimension 'd' does not appear in this formula.\n        delta_squared = (2 * D * c_mc) / alpha\n        delta = np.sqrt(delta_squared)\n\n        # Task 4: Calculate the effective time t_eff(N) using the simplified form.\n        # Formula: t_eff(N) = N * c_mc\n        t_eff = N * c_mc\n\n        results.append([delta, t_eff])\n\n    # Format the output string as specified: [[Δ₁,t_eff,₁],[Δ₂,t_eff,₂],...]\n    # without any extra spaces.\n    output_parts = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_output = f\"[{','.join(output_parts)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Ultimately, the choice between MD and MC often comes down to a practical question: which method provides more useful information for a given amount of computational effort? This computational exercise tackles this question head-on by introducing the concept of Effective Sample Size (ESS) as the gold standard for measuring sampling efficiency. By implementing a model that accounts for both the computational cost per step and the statistical correlation between samples, you will learn to perform a quantitative, head-to-head comparison of MD and MC for sampling different observables under a fixed computational budget. ",
            "id": "3403186",
            "problem": "You are to write a complete, runnable program that compares the sampling efficiency of Molecular Dynamics (MD) and Monte Carlo (MC) methods under a fixed computational budget, for two observables: the configurational energy and a scalar order parameter. The comparison must be carried out by computing the Effective Sample Size (ESS) for each observable and method, starting from first principles and explicitly accounting for autocorrelation.\n\nThe fundamental base is as follows. For a stationary time series observable with mean-square integrable autocorrelation function (ACF), the variance of the sample mean over $N$ correlated samples is inflated relative to independent sampling by a factor that depends on the integrated autocorrelation time. Let the discrete-lag autocorrelation at lag $k$ be $\\rho(k)$. Define the integrated autocorrelation time in units of samples as\n$$\n\\tau_{\\mathrm{int}} \\equiv \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\rho(k).\n$$\nThis leads to the well-tested relation for the variance of the mean,\n$$\n\\mathrm{Var}(\\bar{X}_N) \\approx \\frac{\\sigma_X^2}{N}\\, 2 \\tau_{\\mathrm{int}},\n$$\nwhere $\\sigma_X^2$ is the variance of the observable and $N$ is the number of collected samples. The Effective Sample Size (ESS) is defined by matching $\\mathrm{Var}(\\bar{X}_N)$ to that of $N_{\\mathrm{eff}}$ independent samples, yielding\n$$\nN_{\\mathrm{eff}} \\approx \\frac{N}{2 \\tau_{\\mathrm{int}}}.\n$$\n\nAssume the following physically and statistically plausible conditions.\n\n- Molecular Dynamics (MD) generates a time series at a constant time step $\\Delta t$ (in reduced, dimensionless units), and you record a sample every $s_{\\mathrm{MD}}$ steps. The ACF for each observable under MD decays exponentially in continuous time, $C_{\\mathrm{MD}}(t) \\propto \\exp(-t/\\theta_{\\mathrm{MD}})$, with a characteristic decorrelation time $\\theta_{\\mathrm{MD}}$ (in the same time units as $\\Delta t$). This implies a one-step discrete correlation coefficient at the recording cadence of\n$$\n\\rho_{\\mathrm{MD}}(1) = \\exp\\!\\left(-\\frac{s_{\\mathrm{MD}} \\Delta t}{\\theta_{\\mathrm{MD}}}\\right).\n$$\n\n- Monte Carlo (MC) is a Markov chain driven by proposal moves of unit computational cost, and you record a sample every $s_{\\mathrm{MC}}$ proposals. The ACF for each observable under MC decays exponentially in the number of proposals, $C_{\\mathrm{MC}}(n) \\propto \\exp(-n/\\lambda_{\\mathrm{MC}})$, with a characteristic decorrelation length $\\lambda_{\\mathrm{MC}}$ measured in proposals. This implies a one-step discrete correlation coefficient at the recording cadence of\n$$\n\\rho_{\\mathrm{MC}}(1) = \\exp\\!\\left(-\\frac{s_{\\mathrm{MC}}}{\\lambda_{\\mathrm{MC}}}\\right).\n$$\n\n- For an exponentially decaying ACF with one-step correlation coefficient $\\rho(1)$, the discrete ACF is $\\rho(k) = \\rho(1)^k$ for integer $k \\ge 1$, and the integrated autocorrelation time is finite.\n\n- A fixed computational budget $B$ counts the number of force or energy evaluations available. Each MD step consumes $c_{\\mathrm{MD}}$ evaluations, and each MC proposal consumes $c_{\\mathrm{MC}}$ evaluations. The total number of MD steps you can perform is $\\left\\lfloor B / c_{\\mathrm{MD}} \\right\\rfloor$, and the total number of MC proposals you can perform is $\\left\\lfloor B / c_{\\mathrm{MC}} \\right\\rfloor$. Recording every $s_{\\mathrm{MD}}$ steps in MD yields\n$$\nN_{\\mathrm{MD}} = \\left\\lfloor \\frac{\\left\\lfloor B / c_{\\mathrm{MD}} \\right\\rfloor}{s_{\\mathrm{MD}}} \\right\\rfloor\n$$\nsamples, and recording every $s_{\\mathrm{MC}}$ proposals in MC yields\n$$\nN_{\\mathrm{MC}} = \\left\\lfloor \\frac{\\left\\lfloor B / c_{\\mathrm{MC}} \\right\\rfloor}{s_{\\mathrm{MC}}} \\right\\rfloor\n$$\nsamples.\n\nYour program must implement the following tasks from these foundations, without any further assumptions.\n\n- For each method (MD and MC) and for each observable (energy and order parameter), compute the one-step discrete correlation coefficient at the recording cadence using the exponential models above, then evaluate the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ and finally compute the Effective Sample Size $N_{\\mathrm{eff}}$.\n\n- If the number of collected samples $N$ is zero, define $N_{\\mathrm{eff}}$ to be $0$.\n\n- Compare the total efficiency between MD and MC by summing the energy and order parameter ESS values for each method and setting a winner flag that is $1$ if MD has a strictly larger total ESS than MC and $0$ otherwise.\n\nAll quantities are in reduced, dimensionless units; no physical units are to be reported. Angles do not appear. Percentages do not appear. You must numerically evaluate exponentials via the natural base.\n\nTest suite. Use the following parameter sets, where each case is specified by\n$$\n(B,\\ c_{\\mathrm{MD}},\\ c_{\\mathrm{MC}},\\ \\Delta t,\\ s_{\\mathrm{MD}},\\ s_{\\mathrm{MC}},\\ \\theta_{\\mathrm{MD}}^{(E)},\\ \\theta_{\\mathrm{MD}}^{(Q)},\\ \\lambda_{\\mathrm{MC}}^{(E)},\\ \\lambda_{\\mathrm{MC}}^{(Q)}).\n$$\nHere superscripts $(E)$ and $(Q)$ denote the energy and order parameter, respectively.\n\n- Case $1$: $(10^6,\\ 1,\\ 1,\\ 0.005,\\ 10,\\ 5,\\ 0.2,\\ 1.0,\\ 50.0,\\ 300.0)$.\n- Case $2$: $(500,\\ 1,\\ 1,\\ 0.005,\\ 50,\\ 50,\\ 5.0,\\ 5.0,\\ 1000.0,\\ 1000.0)$.\n- Case $3$: $(90,\\ 1,\\ 1,\\ 0.002,\\ 100,\\ 10,\\ 0.1,\\ 0.1,\\ 20.0,\\ 20.0)$.\n- Case $4$: $(3 \\times 10^5,\\ 3,\\ 1,\\ 0.004,\\ 5,\\ 2,\\ 0.05,\\ 0.20,\\ 4.0,\\ 40.0)$.\n- Case $5$: $(2 \\times 10^5,\\ 1,\\ 2,\\ 0.01,\\ 50,\\ 20,\\ 0.1,\\ 0.15,\\ 500.0,\\ 1000.0)$.\n\nRequired outputs. For each case, produce a list\n$$\n\\left[ N_{\\mathrm{eff,MD}}^{(E)},\\ N_{\\mathrm{eff,MD}}^{(Q)},\\ N_{\\mathrm{eff,MC}}^{(E)},\\ N_{\\mathrm{eff,MC}}^{(Q)},\\ \\mathrm{win} \\right],\n$$\nwhere the four ESS entries are floats rounded to $3$ decimals and $\\mathrm{win}$ is an integer that equals $1$ if the sum of the two MD ESS values strictly exceeds the sum of the two MC ESS values, and equals $0$ otherwise.\n\nFinal output format. Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, where each per-case result is itself a bracketed, comma-separated list in the order specified above. For example, the shape must look like\n$$\n\\left[ [x_{1,1}, x_{1,2}, x_{1,3}, x_{1,4}, w_1], [x_{2,1}, x_{2,2}, x_{2,3}, x_{2,4}, w_2], \\ldots \\right],\n$$\nwith all $x_{i,j}$ rounded to $3$ decimals and each $w_i$ an integer.",
            "solution": "The user has provided a computational problem to compare the sampling efficiency of Molecular Dynamics (MD) and Monte Carlo (MC) simulations. The problem is to be solved from first principles as defined in the prompt.\n\n### Step 1: Problem Validation\n\nThe problem statement is critically evaluated against the specified criteria.\n\n#### Givens Extraction\n\nThe following quantities and relations are explicitly provided:\n- **Integrated Autocorrelation Time:** $\\tau_{\\mathrm{int}} \\equiv \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\rho(k)$, where $\\rho(k)$ is the discrete-lag autocorrelation at lag $k$.\n- **Effective Sample Size (ESS):** $N_{\\mathrm{eff}} \\approx \\frac{N}{2 \\tau_{\\mathrm{int}}}$, where $N$ is the number of collected samples.\n- **Autocorrelation Function (ACF) Model:** The ACF is exponential, $\\rho(k) = \\rho(1)^k$ for integer $k \\ge 1$.\n- **MD One-Step Correlation:** $\\rho_{\\mathrm{MD}}(1) = \\exp\\!\\left(-\\frac{s_{\\mathrm{MD}} \\Delta t}{\\theta_{\\mathrm{MD}}}\\right)$, with time step $\\Delta t$, recording stride $s_{\\mathrm{MD}}$, and decorrelation time $\\theta_{\\mathrm{MD}}$.\n- **MC One-Step Correlation:** $\\rho_{\\mathrm{MC}}(1) = \\exp\\!\\left(-\\frac{s_{\\mathrm{MC}}}{\\lambda_{\\mathrm{MC}}}\\right)$, with recording stride $s_{\\mathrm{MC}}$ and decorrelation length $\\lambda_{\\mathrm{MC}}$.\n- **Computational Budget:** A fixed budget $B$ of force/energy evaluations.\n- **Computational Costs:** $c_{\\mathrm{MD}}$ evaluations per MD step, $c_{\\mathrm{MC}}$ evaluations per MC proposal.\n- **Number of MD Samples:** $N_{\\mathrm{MD}} = \\left\\lfloor \\frac{\\left\\lfloor B / c_{\\mathrm{MD}} \\right\\rfloor}{s_{\\mathrm{MD}}} \\right\\rfloor$.\n- **Number of MC Samples:** $N_{\\mathrm{MC}} = \\left\\lfloor \\frac{\\left\\lfloor B / c_{\\mathrm{MC}} \\right\\rfloor}{s_{\\mathrm{MC}}} \\right\\rfloor$.\n- **Special Condition:** If $N=0$, then $N_{\\mathrm{eff}}=0$.\n- **Winning Condition:** A winner flag, $\\mathrm{win}$, is $1$ if the sum of MD ESS values is strictly greater than the sum of MC ESS values, and $0$ otherwise.\n- **Test Suite:** Five parameter sets $(B,\\ c_{\\mathrm{MD}},\\ c_{\\mathrm{MC}},\\ \\Delta t,\\ s_{\\mathrm{MD}},\\ s_{\\mathrm{MC}},\\ \\theta_{\\mathrm{MD}}^{(E)},\\ \\theta_{\\mathrm{MD}}^{(Q)},\\ \\lambda_{\\mathrm{MC}}^{(E)},\\ \\lambda_{\\mathrm{MC}}^{(Q)})$.\n- **Output Format:** For each case, a list $[ N_{\\mathrm{eff,MD}}^{(E)},\\ N_{\\mathrm{eff,MD}}^{(Q)},\\ N_{\\mathrm{eff,MC}}^{(E)},\\ N_{\\mathrm{eff,MC}}^{(Q)},\\ \\mathrm{win} ]$, with floats rounded to $3$ decimal places.\n\n#### Validation against Criteria\n\n1.  **Scientific Grounding:** The problem is firmly rooted in the statistical mechanics of molecular simulations and standard time-series analysis. The concepts of autocorrelation, integrated autocorrelation time, and effective sample size are fundamental to the field. The exponential-decay model for the ACF is a common, physically-motivated approximation. The problem is scientifically and mathematically sound.\n2.  **Well-Posedness:** The problem is well-posed. It provides all necessary equations, parameters, and conditions to compute a unique numerical result for each test case. The logic is self-contained and free of ambiguity.\n3.  **Objectivity:** The problem is stated in precise, objective, and mathematical language. There are no subjective or opinion-based elements.\n4.  **Completeness and Consistency:** The problem is complete and self-consistent. All required data are provided in the test suite. The formulas are explicit, and there are no contradictions in the setup. The special case of zero samples is handled.\n5.  **Realism and Feasibility:** The parameters are given in reduced, dimensionless units, which is standard practice. The numerical values are within plausible ranges for typical simulation models. The task is computationally feasible.\n6.  **Other Flaws:** The problem is not trivial, tautological, or unverifiable. It constitutes a direct and meaningful application of core principles to a practical comparison scenario.\n\n#### Verdict\n\nThe problem is **valid**. It is a well-defined, scientifically sound, and solvable computational task. I will proceed with the solution.\n\n### Step 2: Principle-Based Solution Design\n\nThe core of the problem is to compute the Effective Sample Size ($N_{\\mathrm{eff}}$) for each method and observable. A direct implementation of the provided formulas is required.\n\nFirst, we simplify the expression for $N_{\\mathrm{eff}}$. The problem states that the autocorrelation function (ACF) follows an exponential decay, $\\rho(k) = \\rho(1)^k$ for $k \\ge 1$. The integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, is given by:\n$$ \\tau_{\\mathrm{int}} = \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\rho(k) $$\nThe sum is a geometric series:\n$$ \\sum_{k=1}^{\\infty} \\rho(1)^k = \\frac{\\rho(1)}{1 - \\rho(1)} $$\nThis series converges because for any physical system, the correlation must decay, implying $|\\rho(1)| < 1$. In our specific model, $\\rho(1)$ is the exponential of a negative number, so $0 < \\rho(1) < 1$. Substituting the sum back into the expression for $\\tau_{\\mathrm{int}}$ yields:\n$$ \\tau_{\\mathrm{int}} = \\frac{1}{2} + \\frac{\\rho(1)}{1 - \\rho(1)} = \\frac{1(1-\\rho(1)) + 2\\rho(1)}{2(1 - \\rho(1))} = \\frac{1 + \\rho(1)}{2(1 - \\rho(1))} $$\nNow, we substitute this simplified expression for $\\tau_{\\mathrm{int}}$ into the formula for $N_{\\mathrm{eff}}$:\n$$ N_{\\mathrm{eff}} = \\frac{N}{2 \\tau_{\\mathrm{int}}} = \\frac{N}{2 \\left( \\frac{1 + \\rho(1)}{2(1 - \\rho(1))} \\right)} = N \\frac{1 - \\rho(1)}{1 + \\rho(1)} $$\nThis compact formula for $N_{\\mathrm{eff}}$ is computationally efficient and will be used for all calculations. If the total number of samples $N$ is $0$, $N_{\\mathrm{eff}}$ is defined to be $0$.\n\nThe overall algorithm is as follows:\nFor each test case, we are given a set of parameters $(B,\\ c_{\\mathrm{MD}},\\ c_{\\mathrm{MC}},\\ \\Delta t,\\ s_{\\mathrm{MD}},\\ s_{\\mathrm{MC}},\\ \\theta_{\\mathrm{MD}}^{(E)},\\ \\theta_{\\mathrm{MD}}^{(Q)},\\ \\lambda_{\\mathrm{MC}}^{(E)},\\ \\lambda_{\\mathrm{MC}}^{(Q)})$. We perform the following calculations:\n\n**1. Molecular Dynamics (MD) Efficiency:**\n- Calculate the total number of MD steps possible within the budget $B$: $N_{\\mathrm{steps}} = \\lfloor B / c_{\\mathrm{MD}} \\rfloor$.\n- Calculate the total number of samples collected: $N_{\\mathrm{MD}} = \\lfloor N_{\\mathrm{steps}} / s_{\\mathrm{MD}} \\rfloor$.\n- If $N_{\\mathrm{MD}} = 0$, then $N_{\\mathrm{eff,MD}}^{(E)} = 0$ and $N_{\\mathrm{eff,MD}}^{(Q)} = 0$.\n- If $N_{\\mathrm{MD}} > 0$:\n  - For the energy (E):\n    - Compute the one-step correlation: $\\rho_{\\mathrm{MD}}^{(E)}(1) = \\exp\\!\\left(-\\frac{s_{\\mathrm{MD}} \\Delta t}{\\theta_{\\mathrm{MD}}^{(E)}}\\right)$.\n    - Compute the ESS: $N_{\\mathrm{eff,MD}}^{(E)} = N_{\\mathrm{MD}} \\frac{1 - \\rho_{\\mathrm{MD}}^{(E)}(1)}{1 + \\rho_{\\mathrm{MD}}^{(E)}(1)}$.\n  - For the order parameter (Q):\n    - Compute the one-step correlation: $\\rho_{\\mathrm{MD}}^{(Q)}(1) = \\exp\\!\\left(-\\frac{s_{\\mathrm{MD}} \\Delta t}{\\theta_{\\mathrm{MD}}^{(Q)}}\\right)$.\n    - Compute the ESS: $N_{\\mathrm{eff,MD}}^{(Q)} = N_{\\mathrm{MD}} \\frac{1 - \\rho_{\\mathrm{MD}}^{(Q)}(1)}{1 + \\rho_{\\mathrm{MD}}^{(Q)}(1)}$.\n\n**2. Monte Carlo (MC) Efficiency:**\n- Calculate the total number of MC proposals possible: $N_{\\mathrm{proposals}} = \\lfloor B / c_{\\mathrm{MC}} \\rfloor$.\n- Calculate the total number of samples collected: $N_{\\mathrm{MC}} = \\lfloor N_{\\mathrm{proposals}} / s_{\\mathrm{MC}} \\rfloor$.\n- If $N_{\\mathrm{MC}} = 0$, then $N_{\\mathrm{eff,MC}}^{(E)} = 0$ and $N_{\\mathrm{eff,MC}}^{(Q)} = 0$.\n- If $N_{\\mathrm{MC}} > 0$:\n  - For the energy (E):\n    - Compute the one-step correlation: $\\rho_{\\mathrm{MC}}^{(E)}(1) = \\exp\\!\\left(-\\frac{s_{\\mathrm{MC}}}{\\lambda_{\\mathrm{MC}}^{(E)}}\\right)$.\n    - Compute the ESS: $N_{\\mathrm{eff,MC}}^{(E)} = N_{\\mathrm{MC}} \\frac{1 - \\rho_{\\mathrm{MC}}^{(E)}(1)}{1 + \\rho_{\\mathrm{MC}}^{(E)}(1)}$.\n  - For the order parameter (Q):\n    - Compute the one-step correlation: $\\rho_{\\mathrm{MC}}^{(Q)}(1) = \\exp\\!\\left(-\\frac{s_{\\mathrm{MC}}}{\\lambda_{\\mathrm{MC}}^{(Q)}}\\right)$.\n    - Compute the ESS: $N_{\\mathrm{eff,MC}}^{(Q)} = N_{\\mathrm{MC}} \\frac{1 - \\rho_{\\mathrm{MC}}^{(Q)}(1)}{1 + \\rho_{\\mathrm{MC}}^{(Q)}(1)}$.\n\n**3. Comparison and Output:**\n- Calculate the total ESS for each method:\n  - $S_{\\mathrm{MD}} = N_{\\mathrm{eff,MD}}^{(E)} + N_{\\mathrm{eff,MD}}^{(Q)}$\n  - $S_{\\mathrm{MC}} = N_{\\mathrm{eff,MC}}^{(E)} + N_{\\mathrm{eff,MC}}^{(Q)}$\n- Determine the winner flag: $\\mathrm{win} = 1$ if $S_{\\mathrm{MD}} > S_{\\mathrm{MC}}$, otherwise $\\mathrm{win} = 0$.\n- Assemble the final list for the case: $[ N_{\\mathrm{eff,MD}}^{(E)},\\ N_{\\mathrm{eff,MD}}^{(Q)},\\ N_{\\mathrm{eff,MC}}^{(E)},\\ N_{\\mathrm{eff,MC}}^{(Q)},\\ \\mathrm{win} ]$. The float values are formatted to $3$ decimal places.\n\nThis procedure will be implemented in a Python program to process all test cases and generate the final output string in the specified format. The use of `numpy.exp` is appropriate for numerical evaluation of the exponential function.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_ess(N, rho1):\n    \"\"\"\n    Calculates the Effective Sample Size (ESS) from the number of samples N\n    and the one-step autocorrelation coefficient rho1.\n    \"\"\"\n    if N == 0:\n        return 0.0\n    # The ACF is exponential, so N_eff = N * (1 - rho1) / (1 + rho1)\n    if rho1 >= 1.0: # Should not happen with valid physical parameters\n        return 0.0\n    return N * (1 - rho1) / (1 + rho1)\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple:\n    # (B, c_MD, c_MC, dt, s_MD, s_MC, theta_MD_E, theta_MD_Q, lambda_MC_E, lambda_MC_Q)\n    test_cases = [\n        (1e6, 1, 1, 0.005, 10, 5, 0.2, 1.0, 50.0, 300.0),\n        (500, 1, 1, 0.005, 50, 50, 5.0, 5.0, 1000.0, 1000.0),\n        (90, 1, 1, 0.002, 100, 10, 0.1, 0.1, 20.0, 20.0),\n        (3e5, 3, 1, 0.004, 5, 2, 0.05, 0.20, 4.0, 40.0),\n        (2e5, 1, 2, 0.01, 50, 20, 0.1, 0.15, 500.0, 1000.0),\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        B, c_md, c_mc, dt, s_md, s_mc, theta_md_e, theta_md_q, lambda_mc_e, lambda_mc_q = case\n\n        # --- Molecular Dynamics (MD) Calculations ---\n        total_md_steps = int(B // c_md)\n        N_md = total_md_steps // s_md\n\n        # ESS for Energy\n        rho1_md_e = np.exp(-(s_md * dt) / theta_md_e)\n        n_eff_md_e = calculate_ess(N_md, rho1_md_e)\n\n        # ESS for Order Parameter\n        rho1_md_q = np.exp(-(s_md * dt) / theta_md_q)\n        n_eff_md_q = calculate_ess(N_md, rho1_md_q)\n\n        # --- Monte Carlo (MC) Calculations ---\n        total_mc_proposals = int(B // c_mc)\n        N_mc = total_mc_proposals // s_mc\n\n        # ESS for Energy\n        rho1_mc_e = np.exp(-s_mc / lambda_mc_e)\n        n_eff_mc_e = calculate_ess(N_mc, rho1_mc_e)\n\n        # ESS for Order Parameter\n        rho1_mc_q = np.exp(-s_mc / lambda_mc_q)\n        n_eff_mc_q = calculate_ess(N_mc, rho1_mc_q)\n\n        # --- Comparison ---\n        total_ess_md = n_eff_md_e + n_eff_md_q\n        total_ess_mc = n_eff_mc_e + n_eff_mc_q\n        win_flag = 1 if total_ess_md > total_ess_mc else 0\n\n        # --- Format and Store Results for this case ---\n        case_output = [\n            f\"{n_eff_md_e:.3f}\",\n            f\"{n_eff_md_q:.3f}\",\n            f\"{n_eff_mc_e:.3f}\",\n            f\"{n_eff_mc_q:.3f}\",\n            str(win_flag)\n        ]\n        all_results_str.append(f\"[{','.join(case_output)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        }
    ]
}