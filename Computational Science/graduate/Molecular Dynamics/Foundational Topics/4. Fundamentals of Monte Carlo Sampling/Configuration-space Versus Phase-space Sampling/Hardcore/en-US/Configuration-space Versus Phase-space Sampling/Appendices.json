{
    "hands_on_practices": [
        {
            "introduction": "In molecular dynamics, our goal is often to sample from a thermodynamic ensemble, like the canonical ensemble, to compute average properties. While deterministic Hamiltonian dynamics perfectly conserves energy, this very property can prevent a system from exploring all accessible configurations, a problem known as non-ergodicity. This exercise  challenges you to analyze a classic case of this failure—a particle in a double-well potential—and to distinguish between various thermostatting methods to find one that correctly introduces stochasticity to ensure the entire phase space is sampled according to the canonical distribution.",
            "id": "3403533",
            "problem": "Consider Molecular Dynamics (MD) of a single particle of mass $m$ moving in one spatial dimension with coordinate $q$ and momentum $p$, obeying Hamiltonian dynamics with Hamiltonian $H(q,p)=\\frac{p^{2}}{2m}+U(q)$ and equations of motion $\\dot{q}=\\frac{\\partial H}{\\partial p}=\\frac{p}{m}$ and $\\dot{p}=-\\frac{\\partial H}{\\partial q}=-U^{\\prime}(q)$. Let the configuration space be the set of positions $q$, and the phase space be the set of pairs $(q,p)$. Assume a double-well potential $U(q)=\\frac{\\lambda}{4}\\left(q^{2}-a^{2}\\right)^{2}$ with minima at $q=\\pm a$ and a barrier at $q=0$ of height $U(0)=\\frac{\\lambda}{4}a^{4}$. The MD is initialized with total energy $E$ satisfying $0<E<U(0)$ and evolves deterministically under the Hamiltonian flow. The target ensemble for sampling is the canonical ensemble at temperature $T$, with inverse temperature $\\beta=\\frac{1}{k_{\\mathrm{B}}T}$, where $k_{\\mathrm{B}}$ denotes the Boltzmann constant.\n\nStarting only from the definitions and laws above (Hamiltonian dynamics, energy conservation, and the canonical distribution), reason whether the deterministic MD will be able to sample both wells in configuration space and identify a concrete remedy that can restore canonical ensemble ergodicity by introducing stochastic perturbations consistent with the target distribution. In particular, determine which option correctly provides:\n\n(i) a scientifically sound example of a nonergodic Hamiltonian MD scenario in which the trajectory is dynamically confined to a proper subset of the energy surface in phase space, thereby leading to incomplete sampling of configuration space, and\n\n(ii) a remedy that, by adding appropriately chosen stochastic perturbations, makes the canonical distribution $\\rho(q,p)\\propto\\exp\\left(-\\beta H(q,p)\\right)$ invariant and accessible, enabling barrier crossing and complete sampling of both wells.\n\nChoose the single best option.\n\nA. An example: a system of two uncoupled one-dimensional harmonic oscillators with Hamiltonian $H=\\sum_{i=1}^{2}\\left(\\frac{p_{i}^{2}}{2m}+\\frac{k}{2}q_{i}^{2}\\right)$ and commensurate frequencies, where deterministic Nosé–Hoover chain (NHC) thermostats with a finite chain length are claimed to guarantee canonical ergodicity. Remedy: add deterministic NHC variables without any stochastic noise.\n\nB. An example: a one-dimensional particle in a symmetric double-well potential $U(q)=\\frac{\\lambda}{4}\\left(q^{2}-a^{2}\\right)^{2}$ with initial energy $E$ satisfying $0<E<U(0)$, so the trajectory remains confined to one well on the constant-energy manifold. Remedy: add a Langevin thermostat, i.e., modify the equations to $\\dot{q}=\\frac{p}{m}$ and $\\dot{p}=-U^{\\prime}(q)-\\gamma p+\\sqrt{2\\gamma m k_{\\mathrm{B}}T}\\,\\xi(t)$, where $\\gamma>0$ is a friction coefficient and $\\xi(t)$ is Gaussian white noise with $\\langle\\xi(t)\\rangle=0$ and $\\langle\\xi(t)\\xi(t^{\\prime})\\rangle=\\delta(t-t^{\\prime})$, which has the canonical density as a stationary solution.\n\nC. An example: the same double-well potential as above with $0<E<U(0)$, but claim that continuous velocity-rescaling via the Berendsen thermostat alone ensures correct canonical sampling and overcomes the barrier deterministically. Remedy: rescale $p\\mapsto\\alpha p$ at each step with $\\alpha$ chosen to relax the kinetic temperature toward $T$.\n\nD. An example: a central potential $U(r)$ in two dimensions with conserved angular momentum $L$, where the motion is integrable and claimed to be generically ergodic over the energy surface. Remedy: impose an isokinetic Gaussian thermostat that constrains $\\frac{p^{2}}{2m}$ to equal $k_{\\mathrm{B}}T$ at all times without any random forcing, asserting that it restores ergodicity and canonical sampling.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and self-contained. It describes a canonical scenario in statistical mechanics and molecular dynamics (MD) where deterministic, energy-conserving dynamics fails to sample the desired thermodynamic ensemble. The definitions of the Hamiltonian, equations of motion, potential energy function, and the canonical ensemble are standard and correct. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe problem requires an analysis of two distinct aspects: (i) identifying a valid example of non-ergodic sampling in Hamiltonian MD, and (ii) identifying a correct remedy that uses stochastic perturbations to restore sampling of the canonical ensemble.\n\nFirst, let's analyze the physical system described. We have a particle of mass $m$ in one dimension $q$ with a double-well potential $U(q) = \\frac{\\lambda}{4}(q^2 - a^2)^2$. The Hamiltonian is $H(q,p) = \\frac{p^2}{2m} + U(q)$. The dynamics are governed by Hamilton's equations, which conserve the total energy $E = H(q,p)$. The problem specifies that the initial energy $E$ is less than the barrier height, i.e., $0 < E < U(0) = \\frac{\\lambda}{4}a^4$.\n\nA key principle of Hamiltonian dynamics is the conservation of energy. The system's trajectory is forever confined to the constant-energy surface in phase space defined by $H(q,p) = E$. For the given potential, the condition $U(q) \\le E$ determines the classically allowed regions in the configuration space. Since $E < U(0)$, the particle lacks the energy to be at the top of the barrier ($q=0$). This means the region around $q=0$ is classically forbidden. The equation $U(q) = E$ has four real roots for $q$, defining two disjoint allowed intervals for the position $q$: one corresponding to the left well and one to the right well. If the particle is initialized in one well (e.g., with $q(0) < 0$), it will oscillate within that well and can never cross the potential barrier to enter the other well.\n\nIn phase space, the constant-energy surface $H(q,p) = E$ consists of two disconnected, closed curves (ovals), one for each well. A single deterministic trajectory starting on one curve will remain on that curve for all time.\n\nThe target, however, is to sample the canonical ensemble, described by the probability density $\\rho(q,p) \\propto \\exp(-\\beta H(q,p))$, where $\\beta = (k_\\mathrm{B}T)^{-1}$. This distribution is non-zero for all $(q,p)$ and assigns probabilities to states of all energies. A fundamental requirement for an algorithm to correctly sample a distribution is ergodicity: the trajectory generated by the algorithm must, over a long time, visit all accessible states in proportion to their probability. The deterministic MD simulation at a fixed low energy fails this requirement spectacularly. It only samples a tiny, disconnected subset of phase space and completely fails to sample states in the other well or states with different energies, both of which have non-zero probability in the canonical ensemble. This is a classic example of non-ergodicity leading to incomplete sampling.\n\nTo remedy this, the dynamics must be modified. The system must be able to exchange energy with its surroundings, which are represented by a conceptual \"heat bath\" at temperature $T$. This coupling should introduce a mechanism for energy to fluctuate, enabling barrier crossings. The prompt specifically asks for a remedy involving *stochastic perturbations*. The modified dynamics must be such that its stationary probability distribution is precisely the canonical distribution $\\rho(q,p)$.\n\nNow we evaluate each option based on this understanding.\n\n**A. An example: a system of two uncoupled one-dimensional harmonic oscillators with Hamiltonian $H=\\sum_{i=1}^{2}\\left(\\frac{p_{i}^{2}}{2m}+\\frac{k}{2}q_{i}^{2}\\right)$ and commensurate frequencies, where deterministic Nosé–Hoover chain (NHC) thermostats with a finite chain length are claimed to guarantee canonical ergodicity. Remedy: add deterministic NHC variables without any stochastic noise.**\n\n(i) The example of two uncoupled harmonic oscillators is a valid illustration of non-ergodicity. Since the oscillators are uncoupled, the energy of each, $E_1$ and $E_2$, is individually conserved. The system has two degrees of freedom and two independent constants of motion (any two of $E_1, E_2, E = E_1+E_2$). Such a system is integrable. Its trajectory is confined to a lower-dimensional manifold (a torus) within the constant total energy surface and is not ergodic on that surface, especially for commensurate frequencies where the trajectory forms a closed loop. This part is scientifically sound.\n\n(ii) The remedy proposed is the Nosé–Hoover chain (NHC) thermostat. NHC is a *deterministic* algorithm. It extends the phase space with fictitious variables that are supposed to mimic a heat bath. While often effective, it is known to fail for certain systems, including harmonic oscillators, where the thermostat degrees of freedom can decouple from the physical system. The claim that it \"guarantees\" ergodicity is false. More importantly, it is a deterministic method and does not involve the \"stochastic perturbations\" required by the problem statement.\n\nVerdict: **Incorrect**. The remedy is deterministic, not stochastic, and its effectiveness is not guaranteed as claimed.\n\n**B. An example: a one-dimensional particle in a symmetric double-well potential $U(q)=\\frac{\\lambda}{4}\\left(q^{2}-a^{2}\\right)^{2}$ with initial energy $E$ satisfying $0<E<U(0)$, so the trajectory remains confined to one well on the constant-energy manifold. Remedy: add a Langevin thermostat, i.e., modify the equations to $\\dot{q}=\\frac{p}{m}$ and $\\dot{p}=-U^{\\prime}(q)-\\gamma p+\\sqrt{2\\gamma m k_{\\mathrm{B}}T}\\,\\xi(t)$, where $\\gamma>0$ is a friction coefficient and $\\xi(t)$ is Gaussian white noise with $\\langle\\xi(t)\\rangle=0$ and $\\langle\\xi(t)\\xi(t^{\\prime})\\rangle=\\delta(t-t^{\\prime})$, which has the canonical density as a stationary solution.**\n\n(i) The example is precisely the one analyzed in the problem statement and our initial discussion. As established, it is a standard and physically clear instance where deterministic, constant-energy dynamics leads to non-ergodic sampling by trapping the trajectory in a subset of the configuration space. This part is correct.\n\n(ii) The remedy is a Langevin thermostat. The equations of motion are modified to include a frictional drag term $-\\gamma p$ and a stochastic force term $\\sqrt{2\\gamma m k_{\\mathrm{B}}T}\\,\\xi(t)$. This models the particle's interaction with a heat bath. The stochastic force term provides random \"kicks\" that cause the system's energy to fluctuate, enabling it to gain sufficient energy to cross the potential barrier. The friction term provides dissipation, ensuring the energy does not grow without bound and the system can relax towards equilibrium. The relation between the friction coefficient $\\gamma$ and the noise strength is a direct consequence of the fluctuation-dissipation theorem, and it is crucial for ensuring that the system equilibrates to the correct temperature $T$. It is a standard result of statistical physics that the probability distribution of the system governed by these Langevin equations converges to the canonical distribution $\\rho(q,p) \\propto \\exp(-\\beta H(q,p))$ as its unique stationary state. This remedy satisfies all requirements: it is stochastic, it allows for barrier crossing, and it correctly generates the canonical ensemble.\n\nVerdict: **Correct**. Both the example and the remedy are scientifically sound, consistent with the problem statement, and directly address the core issue of sampling.\n\n**C. An example: the same double-well potential as above with $0<E<U(0)$, but claim that continuous velocity-rescaling via the Berendsen thermostat alone ensures correct canonical sampling and overcomes the barrier deterministically. Remedy: rescale $p\\mapsto\\alpha p$ at each step with $\\alpha$ chosen to relax the kinetic temperature toward $T$.**\n\n(i) The example is the same as in option B and is correct.\n\n(ii) The remedy is the Berendsen thermostat. This thermostat rescales the momenta to guide the kinetic energy toward its canonical average, $\\langle p^2/(2m) \\rangle = k_\\mathrm{B}T/2$. However, a major and well-known flaw of the Berendsen thermostat is that it does *not* generate a rigorous canonical ensemble. The phase space distribution it samples deviates from the correct Boltzmann distribution, particularly in its suppression of energy fluctuations. Therefore, the claim that it \"ensures correct canonical sampling\" is false. Furthermore, the standard Berendsen algorithm is deterministic (the scaling factor $\\alpha$ is a deterministic function of the system's state), so it does not introduce the \"stochastic perturbations\" required by the prompt.\n\nVerdict: **Incorrect**. The proposed remedy is known to be theoretically flawed for generating a canonical ensemble, and it is deterministic, not stochastic.\n\n**D. An example: a central potential $U(r)$ in two dimensions with conserved angular momentum $L$, where the motion is integrable and claimed to be generically ergodic over the energy surface. Remedy: impose an isokinetic Gaussian thermostat that constrains $\\frac{p^{2}}{2m}$ to equal $k_{\\mathrm{B}}T$ at all times without any random forcing, asserting that it restores ergodicity and canonical sampling.**\n\n(i) A particle in a 2D central potential is an integrable system because both energy $E$ and angular momentum $L$ are conserved. The existence of an additional constant of motion besides energy prevents the trajectory from exploring the entire energy surface. Therefore, such a system is *not* ergodic on the energy surface. The option correctly identifies it as integrable but then incorrectly claims it is \"generically ergodic over the energy surface\". This is a direct contradiction.\n\n(ii) The remedy is a Gaussian isokinetic thermostat. This thermostat enforces a strict constraint on the kinetic energy, $K = \\text{const}$. While this technique can be used to sample the configurational part of the canonical ensemble (i.e., it generates $\\rho(q) \\propto \\exp(-\\beta U(q))$), it does not sample the full phase-space canonical distribution, as the momenta are not distributed according to the Maxwell-Boltzmann distribution. Thus, the claim that it restores \"canonical sampling\" is misleading. Most importantly, like the NHC and Berendsen thermostats, the Gaussian thermostat is a *deterministic* algorithm and involves no \"random forcing\".\n\nVerdict: **Incorrect**. The example part contains a fundamental scientific contradiction, and the remedy is deterministic and does not generate the full canonical ensemble.\n\nIn summary, only option B provides a correct example of non-ergodicity within the context of the problem and a valid, standard, stochastic remedy that correctly generates the canonical ensemble.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "When we compute free energy profiles along a reaction coordinate, we often use holonomic constraints to force the system to sample specific configurations. This practice, exemplified by methods like the Blue Moon ensemble, introduces a subtle but critical issue related to the measure used for sampling. This practice problem  guides you through the derivation of a geometric correction factor that arises from the difference between the native constrained phase-space measure and the desired configuration-space marginal distribution, demonstrating why naive sampling on a manifold is biased and how to correct for it.",
            "id": "3403498",
            "problem": "You are given a holonomically constrained system suitable for Molecular Dynamics (MD) in two spatial dimensions with mass matrix equal to the identity. Let the generalized coordinates be $q=(x,y) \\in \\mathbb{R}^2$, and consider a single smooth constraint defined implicitly by the level set $\\sigma(q)=0$, which defines a closed curve (one-dimensional manifold) embedded in the plane. The Hamiltonian is $H(q,p)=\\tfrac{1}{2}p^\\top p + U(q)$, where $U(q)$ is a smooth potential energy and $p$ are the momenta. The canonical phase-space density at inverse temperature $\\beta$ is proportional to $\\exp(-\\beta H(q,p))$.\n\nConstrained MD methods (such as SHAKE/RATTLE) restrict the dynamics to the manifold $\\sigma(q)=0$ and the corresponding constrained momentum subspace $\\dot{\\sigma}(q,p)=\\nabla \\sigma(q) \\cdot p = 0$. It is known that the invariant constrained phase-space measure induces a biased configuration distribution along $\\sigma(q)=0$ when projected to positions, relative to the configuration-space marginal one would obtain by first integrating out momenta and then restricting to $\\sigma(q)=0$. Your task is to:\n\n1) Starting from fundamental canonical principles and the definition of holonomic constraints (including the kinematic constraint $\\dot{\\sigma}(q,p)=0$), derive quantitatively how the constrained phase-space measure biases the sampling along the curve $\\sigma(q)=0$ in terms of the function $G(q)=\\nabla \\sigma(q)^\\top \\nabla \\sigma(q)$, and show that with the induced surface element $\\mathrm{d}S$ (the one-dimensional arc-length element on the curve), the induced configuration density from constrained phase-space is proportional to $\\exp(-\\beta U(q)) \\sqrt{G(q)}$ on the manifold. Then, define the configuration-space marginal along the constraint as the restriction of the unconstrained configuration Gibbs measure to the manifold with the induced arc-length element, whose density is proportional to $\\exp(-\\beta U(q))$ on the manifold. Conclude that the reweighting factor needed to convert averages computed under the constrained phase-space induced distribution to the desired configuration marginal along the constraint is $w(q)=1/\\sqrt{G(q)}$. In the special case of identity mass matrix in two dimensions, this reduces to $w(q)=1/\\lVert \\nabla \\sigma(q) \\rVert$, where $\\lVert \\cdot \\rVert$ denotes the Euclidean norm.\n\n2) Implement a deterministic numerical test that demonstrates the effect of the bias and its correction on a set of three closed-curve constraints. Parameterize the curve by a polar angle $\\theta \\in [0,2\\pi)$ (in radians), define the radial coordinate $r(\\theta)>0$ implicitly by $\\sigma(r(\\theta)\\cos\\theta, r(\\theta)\\sin\\theta)=0$, and use implicit differentiation to obtain $\\mathrm{d}s/\\mathrm{d}\\theta$ (the arc length rate) along the curve at each angle, where $\\mathrm{d}s$ is the arc-length differential. For each $\\theta$:\n- Compute $q(\\theta)=(x(\\theta),y(\\theta))=(r(\\theta)\\cos\\theta,r(\\theta)\\sin\\theta)$.\n- Compute $\\lVert \\nabla \\sigma(q(\\theta)) \\rVert$.\n- Form three unnormalized angular densities (with respect to $\\mathrm{d}\\theta$):\n  - The target configuration marginal along the constraint: $\\rho_{\\mathrm{conf}}(\\theta) \\propto \\exp(-\\beta U(q(\\theta))) \\, \\frac{\\mathrm{d}s}{\\mathrm{d}\\theta}$.\n  - The constrained phase-space induced marginal: $\\rho_{\\mathrm{bm}}(\\theta) \\propto \\exp(-\\beta U(q(\\theta))) \\, \\lVert \\nabla \\sigma(q(\\theta)) \\rVert \\, \\frac{\\mathrm{d}s}{\\mathrm{d}\\theta}$.\n  - The corrected marginal: $\\rho_{\\mathrm{corr}}(\\theta) \\propto \\rho_{\\mathrm{bm}}(\\theta) \\, \\frac{1}{\\lVert \\nabla \\sigma(q(\\theta)) \\rVert}$.\nNormalize each density over $\\theta \\in [0,2\\pi)$ and compute the discrete $\\ell_1$ distance between the normalized target and:\n- the naive constrained phase-space induced marginal,\n- the corrected marginal.\n\nUse sufficiently fine angular discretization to make the numerical errors negligible compared to the bias.\n\n3) Provide the program output as a single line containing the two errors for each test case as a list of lists of floats in the exact format specified below.\n\nAll quantities in this problem are dimensionless; angles must be in radians. The test suite must use the following cases and parameters:\n\n- Case 1 (baseline, constant gradient magnitude): Constraint $\\sigma(x,y)=x^2+y^2-1$, potential $U(x,y)=\\tfrac{1}{2}(1.0\\, x^2+1.0\\, y^2)$, inverse temperature $\\beta=1.0$.\n- Case 2 (anisotropic ellipse): Constraint $\\sigma(x,y)=x^2+4 y^2-1$, potential $U(x,y)=\\tfrac{1}{2}(1.5\\, x^2+0.5\\, y^2)$, inverse temperature $\\beta=1.0$.\n- Case 3 (nonlinear star-shaped): Constraint $\\sigma(x,y)=x^4+y^2-1$, potential $U(x,y)=\\tfrac{1}{2}(0.7\\, x^2+2.0\\, y^2)$, inverse temperature $\\beta=1.0$.\n\nYour program must discretize $\\theta \\in [0,2\\pi)$, solve for $r(\\theta)$ as the unique positive root of $\\sigma(r\\cos\\theta,r\\sin\\theta)=0$, compute $\\mathrm{d}s/\\mathrm{d}\\theta$ using implicit differentiation of $\\sigma(q(\\theta))=0$, and then compute the normalized densities and $\\ell_1$ distances as specified.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of three items, one per case, where each item is a two-element list with the naive and corrected $\\ell_1$ distances, respectively. For example: \"[[e1_naive,e1_corrected],[e2_naive,e2_corrected],[e3_naive,e3_corrected]]\".",
            "solution": "The problem asks for a theoretical derivation concerning biased sampling in constrained molecular dynamics and a numerical implementation to verify the effect.\n\n### Part 1: Theoretical Derivation\n\nThe analysis begins with the principles of classical statistical mechanics in the canonical ensemble. The system is described by a Hamiltonian $H(q,p) = K(p) + U(q)$, where $q$ are the generalized coordinates, $p$ are the conjugate momenta, $K(p) = \\frac{1}{2}p^\\top M^{-1}p$ is the kinetic energy, and $U(q)$ is the potential energy. For this problem, the mass matrix $M$ is the identity matrix ($M=I$), so $K(p) = \\frac{1}{2}p^\\top p$. The system is subject to a holonomic constraint $\\sigma(q)=0$.\n\nThe canonical phase-space probability density for an unconstrained system is proportional to $\\exp(-\\beta H(q,p))$, where $\\beta=1/(k_B T)$ is the inverse temperature. The phase-space measure is $\\mathrm{d}\\mu = \\exp(-\\beta H(q,p)) \\mathrm{d}q \\mathrm{d}p$.\n\nWhen constraints are present, the dynamics, and thus the equilibrium sampling, are restricted to a submanifold of the phase space. The holonomic constraint $\\sigma(q)=0$ confines the configurations to a specific manifold. The time derivative of the constraint must also be zero, which imposes a condition on the momenta. This is the kinematic constraint:\n$$\n\\dot{\\sigma}(q, \\dot{q}) = \\frac{\\mathrm{d}\\sigma}{\\mathrm{d}t} = \\nabla_q \\sigma(q) \\cdot \\dot{q} = 0\n$$\nSince the momenta are related to velocities by $p = M\\dot{q}$ (or $\\dot{q}=M^{-1}p$), and with $M=I$, we have $p=\\dot{q}$. The kinematic constraint becomes:\n$$\n\\nabla\\sigma(q) \\cdot p = 0\n$$\nThis constrains the momenta to the tangent space of the constraint manifold. Constrained Molecular Dynamics algorithms, such as SHAKE and RATTLE, are designed to generate trajectories that sample the canonical distribution on this constrained phase-space submanifold.\n\nThe objective is to find the marginal probability distribution for the configurations $q$ that is induced by this constrained phase-space sampling. This distribution is obtained by integrating the constrained phase-space density over the allowed momenta. The derivation of the precise form of this induced density is a subtle topic in statistical mechanics, with the result depending on the specific formulation of the measure on the constraint surface. The problem statement directs us to show that this leads to a density proportional to $\\exp(-\\beta U(q)) \\sqrt{G(q)}$. This result aligns with the theoretical framework established by Fixman and later clarified by Sprik and Ciccotti for describing the statistics of constrained polymers.\n\nFollowing this framework, the projection from the full phase space onto the constrained configuration manifold introduces a Jacobian-like factor related to the metric of the constrained coordinates. This factor is identified as $\\sqrt{\\det(Z(q))}$, where $Z(q)$ is a metric tensor matrix given by $Z_{ab}(q) = (\\nabla_q \\sigma_a)^\\top M^{-1} (\\nabla_q \\sigma_b)$. For a single constraint $\\sigma(q)=0$ and identity mass matrix $M=I$, this matrix is a $1 \\times 1$ matrix whose single element is:\n$$\nG(q) = (\\nabla\\sigma(q))^\\top I^{-1} (\\nabla\\sigma(q)) = (\\nabla\\sigma(q))^\\top (\\nabla\\sigma(q)) = \\lVert \\nabla\\sigma(q) \\rVert^2\n$$\nThe factor is thus $\\sqrt{G(q)}$. Consequently, the configuration-space probability density induced by the constrained phase-space sampling, expressed with respect to the natural arc-length element $\\mathrm{d}S$ on the manifold $\\sigma(q)=0$, is given by:\n$$\n\\rho_{\\text{bm}}(q) \\propto \\exp(-\\beta U(q)) \\sqrt{G(q)}\n$$\nThe subscript \"bm\" denotes this \"blue moon\" or biased marginal distribution.\n\nThe problem then defines the desired or \"target\" configuration-space distribution. This is obtained by considering the unconstrained configurational Gibbs measure, $\\exp(-\\beta U(q)) \\mathrm{d}q$, and simply restricting it to the constraint manifold. This approach corresponds to defining a free energy surface or potential of mean force along the manifold. The density, with respect to the same arc-length element $\\mathrm{d}S$, is:\n$$\n\\rho_{\\text{conf}}(q) \\propto \\exp(-\\beta U(q))\n$$\nThis represents the physically intuitive distribution where each configuration on the manifold is weighted only by its Boltzmann factor $\\exp(-\\beta U(q))$.\n\nAverages of an observable $A(q)$ computed from a constrained MD simulation will converge to $\\langle A \\rangle_{\\text{bm}}$, the average over the biased distribution $\\rho_{\\text{bm}}$. To obtain the correct average over the target distribution $\\langle A \\rangle_{\\text{conf}}$, a reweighting must be applied.\n$$\n\\langle A \\rangle_{\\text{conf}} = \\frac{\\int A(q) \\rho_{\\text{conf}}(q) \\mathrm{d}S}{\\int \\rho_{\\text{conf}}(q) \\mathrm{d}S}\n$$\nAn identity for reweighting states that $\\langle A \\rangle_{\\text{conf}} = \\frac{\\langle A \\cdot w \\rangle_{\\text{bm}}}{\\langle w \\rangle_{\\text{bm}}}$, where $w(q)$ is the reweighting factor. This requires $A(q) \\rho_{\\text{conf}}(q) \\propto (A(q) w(q)) \\rho_{\\text{bm}}(q)$ for any $A(q)$, which simplifies to $\\rho_{\\text{conf}}(q) \\propto w(q) \\rho_{\\text{bm}}(q)$.\nSubstituting the derived forms of the densities:\n$$\n\\exp(-\\beta U(q)) \\propto w(q) \\left( \\exp(-\\beta U(q)) \\sqrt{G(q)} \\right)\n$$\nThis immediately implies that the reweighting factor $w(q)$ must be:\n$$\nw(q) \\propto \\frac{1}{\\sqrt{G(q)}}\n$$\nFor the specific case of a 2D system with $M=I$, this becomes:\n$$\nw(q) = \\frac{1}{\\sqrt{\\lVert \\nabla\\sigma(q) \\rVert^2}} = \\frac{1}{\\lVert \\nabla\\sigma(q) \\rVert}\n$$\n(up to a normalization constant). This factor corrects the sampling bias introduced by projecting the constrained phase-space measure onto configuration space.\n\n### Part 2: Numerical Implementation Strategy\n\nThe numerical test is designed to demonstrate this effect. The core of the implementation involves parameterizing the constraint curve $\\sigma(x,y)=0$ by a polar angle $\\theta \\in [0, 2\\pi)$. For each $\\theta$, the position $q(\\theta) = (r(\\theta)\\cos\\theta, r(\\theta)\\sin\\theta)$ is determined by solving $\\sigma(r\\cos\\theta, r\\sin\\theta)=0$ for the radius $r(\\theta)$.\n\nThe three probability densities are expressed as functions of $\\theta$. A density with respect to the arc length $S$ relates to a density with respect to the angle $\\theta$ via the chain rule: $\\rho(S) \\mathrm{d}S = \\rho(S(\\theta)) \\frac{\\mathrm{d}S}{\\mathrm{d}\\theta} \\mathrm{d}\\theta$. So, the unnormalized angular densities are:\n1.  **Target density**: $\\rho_{\\text{conf}}(\\theta) \\propto \\exp(-\\beta U(q(\\theta))) \\frac{\\mathrm{d}S}{\\mathrm{d}\\theta}$\n2.  **Biased density**: $\\rho_{\\text{bm}}(\\theta) \\propto \\exp(-\\beta U(q(\\theta))) \\lVert \\nabla\\sigma(q(\\theta)) \\rVert \\frac{\\mathrm{d}S}{\\mathrm{d}\\theta}$\n3.  **Corrected density**: $\\rho_{\\text{corr}}(\\theta) \\propto \\rho_{\\text{bm}}(\\theta) w(q(\\theta)) = \\rho_{\\text{bm}}(\\theta) \\frac{1}{\\lVert\\nabla\\sigma(q(\\theta))\\rVert} = \\rho_{\\text{conf}}(\\theta)$\n\nThe arc-length derivative $\\frac{\\mathrm{d}S}{\\mathrm{d}\\theta}$ is found from the parameterization $q(\\theta)$:\n$$\n\\frac{\\mathrm{d}S}{\\mathrm{d}\\theta} = \\left\\lVert \\frac{\\mathrm{d}q}{\\mathrm{d}\\theta} \\right\\rVert = \\sqrt{ \\left(\\frac{\\mathrm{d}x}{\\mathrm{d}\\theta}\\right)^2 + \\left(\\frac{\\mathrm{d}y}{\\mathrm{d}\\theta}\\right)^2 }\n$$\nWith $x(\\theta)=r(\\theta)\\cos\\theta$ and $y(\\theta)=r(\\theta)\\sin\\theta$, this simplifies to $\\frac{\\mathrm{d}S}{\\mathrm{d}\\theta} = \\sqrt{r(\\theta)^2 + (\\mathrm{d}r/\\mathrm{d}\\theta)^2}$. The derivative $\\mathrm{d}r/\\mathrm{d}\\theta$ is obtained by implicit differentiation of the constraint equation $\\sigma(r(\\theta)\\cos\\theta, r(\\theta)\\sin\\theta)=0$ with respect to $\\theta$.\n\nThe algorithm proceeds as follows for each test case:\n1.  Create a fine grid of $N$ points for $\\theta$ in $[0, 2\\pi)$.\n2.  For each $\\theta_i$, solve for $r_i=r(\\theta_i)$.\n3.  Compute the derivative $r'_i = \\mathrm{d}r/\\mathrm{d}\\theta$ at each point.\n4.  Calculate the geometric factors: positions $q_i$, norms of the gradient $\\lVert \\nabla\\sigma(q_i) \\rVert$, and arc-length derivatives $(\\mathrm{d}S/\\mathrm{d}\\theta)_i$.\n5.  Evaluate the potential energy $U(q_i)$.\n6.  Construct the three unnormalized angular density vectors $\\rho_{\\text{conf}}$, $\\rho_{\\text{bm}}$, and $\\rho_{\\text{corr}}$.\n7.  Normalize each density vector by dividing by its sum (the integration element $\\Delta\\theta$ cancels).\n8.  Compute the $\\ell_1$ distance between the normalized target density and the normalized biased density (naive error).\n9.  Compute the $\\ell_1$ distance between the normalized target density and the normalized corrected density. By construction, this error should be zero, up to floating-point precision, providing a robust check of the implementation.\n\nThe three test cases provided allow for a clear demonstration:\n- **Case 1 (Circle):** $\\lVert\\nabla\\sigma\\rVert$ is constant on the curve. The biasing factor is uniform, leading to no distortion of the probability distribution. Both naive and corrected errors are expected to be zero.\n- **Case 2 (Ellipse) and Case 3 (Star-shape):** $\\lVert\\nabla\\sigma\\rVert$ varies along the curve, which introduces a non-trivial bias. The naive error will be significant, while the corrected error will remain near zero.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the molecular dynamics sampling bias problem.\n    \"\"\"\n    \n    # Define test cases\n    # Each case is a dictionary containing the symbolic functions and parameters.\n    test_cases = [\n        {\n            \"sigma\": lambda x, y: x**2 + y**2 - 1.0,\n            \"grad_sigma\": lambda x, y: np.array([2.0 * x, 2.0 * y]),\n            \"U\": lambda x, y: 0.5 * (1.0 * x**2 + 1.0 * y**2),\n            \"beta\": 1.0,\n            \"r_solver\": lambda c, s: 1.0,\n            \"r_prime_solver\": lambda r, c, s, r_solver_func: 0.0,\n        },\n        {\n            \"sigma\": lambda x, y: x**2 + 4.0 * y**2 - 1.0,\n            \"grad_sigma\": lambda x, y: np.array([2.0 * x, 8.0 * y]),\n            \"U\": lambda x, y: 0.5 * (1.5 * x**2 + 0.5 * y**2),\n            \"beta\": 1.0,\n            \"r_solver\": lambda c, s: 1.0 / np.sqrt(c**2 + 4.0 * s**2),\n            \"r_prime_solver\": lambda r, c, s, r_solver_func: (3.0 * r**3 * s * c) / (r**2 * c**2 + 4.0 * r**2 * s**2),\n        },\n        {\n            \"sigma\": lambda x, y: x**4 + y**2 - 1.0,\n            \"grad_sigma\": lambda x, y: np.array([4.0 * x**3, 2.0 * y]),\n            \"U\": lambda x, y: 0.5 * (0.7 * x**2 + 2.0 * y**2),\n            \"beta\": 1.0,\n            \"r_solver\": lambda c, s: np.sqrt(\n                (-s**2 + np.sqrt(s**4 + 4.0 * c**4)) / (2.0 * c**4)\n            ) if abs(c) > 1e-9 else 1.0,\n            \"r_prime_solver\": lambda r, c, s, r_solver_func: \n                (r * (2*r**2 * c**3 * s - s*c)) / (2*r**2 * c**4 + s**2) if abs(c) > 1e-9 else 0.0,\n        }\n    ]\n\n    all_results = []\n    N_points = 10000\n\n    for case in test_cases:\n        sigma = case[\"sigma\"]\n        grad_sigma = case[\"grad_sigma\"]\n        U = case[\"U\"]\n        beta = case[\"beta\"]\n        r_solver = case[\"r_solver\"]\n        r_prime_solver = case[\"r_prime_solver\"]\n\n        # 1. Discretize theta\n        theta = np.linspace(0, 2 * np.pi, N_points, endpoint=False)\n        c, s = np.cos(theta), np.sin(theta)\n\n        # 2. Solve for r(theta)\n        r = np.array([r_solver(ci, si) for ci, si in zip(c, s)])\n\n        # 3. Compute r'(theta) for ds/dtheta\n        r_prime = np.array([r_prime_solver(ri, ci, si, r_solver) for ri, ci, si in zip(r, c, s)])\n\n        # 4. Compute ds/dtheta\n        ds_dtheta = np.sqrt(r**2 + r_prime**2)\n\n        # 5. Compute quantities on the curve\n        x, y = r * c, r * s\n        \n        # Potential energy\n        U_vals = U(x, y)\n        \n        # Gradient norm\n        grad_sigma_vals = grad_sigma(x, y)\n        grad_sigma_norm = np.linalg.norm(grad_sigma_vals, axis=0)\n\n        # 6. Form unnormalized densities\n        boltzmann_factor = np.exp(-beta * U_vals)\n        \n        # Target configuration marginal\n        rho_conf = boltzmann_factor * ds_dtheta\n        \n        # Biased phase-space induced marginal\n        rho_bm = boltzmann_factor * grad_sigma_norm * ds_dtheta\n        \n        # Corrected marginal\n        rho_corr = rho_bm / grad_sigma_norm\n\n        # 7. Normalize densities\n        # The integration element dtheta is constant and cancels out.\n        P_conf = rho_conf / np.sum(rho_conf)\n        P_bm = rho_bm / np.sum(rho_bm)\n        P_corr = rho_corr / np.sum(rho_corr)\n        \n        # 8. Compute L1 distances\n        # The sum of absolute differences of the probability mass functions\n        error_naive = np.sum(np.abs(P_conf - P_bm))\n        error_corrected = np.sum(np.abs(P_conf - P_corr))\n\n        all_results.append([error_naive, error_corrected])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'[{e_n},{e_c}]' for e_n, e_c in all_results])}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "A correct sampling algorithm should, in theory, generate the target canonical distribution, but the use of a finite time-step $\\Delta t$ in numerical integration inevitably introduces errors. This exercise  introduces the concept of the configurational temperature, a sensitive probe for the accuracy of sampling that is distinct from the familiar kinetic temperature. By comparing these two estimators, you will quantitatively diagnose how a discrete-time integrator can create a bias in the momentum distribution, even while preserving a near-correct distribution in configuration space, offering deep insight into the fidelity of your simulation.",
            "id": "3403553",
            "problem": "Consider a system of $N$ identical particles of mass $m$ moving in $d$ Cartesian dimensions under a separable harmonic potential $U(\\mathbf{q})=\\tfrac{1}{2}k\\sum_{i=1}^{Nd} q_i^2$, where $k$ is the stiffness and $\\mathbf{q}\\in\\mathbb{R}^{Nd}$ is the configuration vector. In continuous-time canonical equilibrium at temperature $T$ with Boltzmann constant $k_B$, two operational temperature estimators coincide: the kinetic temperature $T_k=\\tfrac{2\\langle K\\rangle}{Nd\\,k_B}$, where $K=\\sum_{i=1}^{Nd}\\tfrac{p_i^2}{2m}$ is the kinetic energy, and the configurational temperature $T_c=\\tfrac{\\langle \\lVert\\nabla U\\rVert^2\\rangle}{k_B\\langle\\nabla^2 U\\rangle}$, where $\\nabla U$ is the gradient of the potential and $\\nabla^2 U$ is its Laplacian. Under discrete-time Molecular Dynamics (MD) integration with nonzero timestep $\\Delta t$, phase-space sampling can be biased such that the configuration-space marginal $\\rho(\\mathbf{q})$ is close to the target canonical distribution, yet the momentum-space marginal is distorted, leading to discrepancies between $T_k$ and $T_c$.\n\nStarting from fundamental laws and core definitions, including Newton’s equations of motion, the Langevin equation with linear friction coefficient $\\gamma$, and the definitions of $T_k$ and $T_c$ above, construct a program that models a single degree of freedom ($Nd=1$) linear harmonic oscillator integrated by a discrete-time Langevin splitting scheme of the form $B$-$A$-$O$-$A$-$B$ (BAOAB). Here, $B$ denotes a half-step update of momentum using forces, $A$ denotes a half-step update of positions using momenta, and $O$ denotes a full-step Ornstein–Uhlenbeck (OU) update of momentum representing friction and stochastic thermalization at target temperature $T$. Use dimensionless units where $k_B=1$.\n\nYour program must:\n- Derive the one-step linear update mapping for the BAOAB integrator applied to a one-dimensional harmonic potential $U(q)=\\tfrac{1}{2}k q^2$, expressing $(q',p')$ as an affine function of $(q,p)$ with additive Gaussian noise induced by the $O$ step.\n- Use the resulting linear stochastic map to compute the stationary phase-space covariance matrix $\\Sigma$ for $(q,p)$ by solving the discrete-time Lyapunov equation $\\Sigma = A\\Sigma A^\\top + Q$, where $A$ is the deterministic linear update matrix and $Q$ is the noise covariance induced by the OU step.\n- From $\\Sigma$, compute the configurational temperature $T_c = \\tfrac{\\langle \\lVert\\nabla U\\rVert^2\\rangle}{\\langle\\nabla^2 U\\rangle}$ and the kinetic temperature $T_k = \\tfrac{2\\langle K\\rangle}{Nd}$, both in dimensionless units with $k_B=1$; for $Nd=1$, these reduce to $T_c = k\\langle q^2\\rangle$ and $T_k = \\tfrac{\\langle p^2\\rangle}{m}$.\n- Report for each test case the discrepancy $\\delta = T_k - T_c$ as a float, which quantifies phase-space bias that still leaves $\\rho(q)$ near-correct.\n\nExpress all angles, if any appear, in radians, and all physical quantities in the specified dimensionless units. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").\n\nTest Suite:\nUse the following four parameter sets to probe different regimes, ensuring coverage of a small timestep “happy path,” moderate timestep, near-stability boundary, and high-friction overdamped limit. In each case, $Nd=1$ and $k_B=1$.\n\n1. $(m=1,\\;k=1,\\;T=1,\\;\\gamma=1,\\;\\Delta t=0.01)$\n2. $(m=1,\\;k=1,\\;T=1,\\;\\gamma=1,\\;\\Delta t=0.5)$\n3. $(m=1,\\;k=1,\\;T=1,\\;\\gamma=0.5,\\;\\Delta t=1.9)$\n4. $(m=1,\\;k=1,\\;T=1,\\;\\gamma=10,\\;\\Delta t=1.0)$\n\nFor each test case, output the single float $\\delta=T_k-T_c$ in dimensionless units. The final output must be a single line in the exact format \"[delta1,delta2,delta3,delta4]\".",
            "solution": "The problem requires an analysis of the discrepancy between the kinetic and configurational temperature estimators for a one-dimensional harmonic oscillator simulated using a discrete-time Langevin integrator, specifically the $BAOAB$ scheme. We will first validate the problem statement and then proceed to derive the analytical solution.\n\n### Step 1: Extract Givens\n- **System**: A single degree of freedom ($Nd=1$) with mass $m$ and potential $U(q) = \\frac{1}{2}kq^2$.\n- **Dynamics**: Governed by the Langevin equation with target temperature $T$, friction coefficient $\\gamma$, and Boltzmann constant $k_B$.\n- **Integrator**: A discrete-time $BAOAB$ splitting scheme with timestep $\\Delta t$.\n- **Temperature Estimators ($k_B=1$)**:\n    - Kinetic Temperature: $T_k = \\frac{\\langle p^2 \\rangle}{m}$\n    - Configurational Temperature: $T_c = \\frac{\\langle (\\nabla U)^2 \\rangle}{\\langle \\nabla^2 U \\rangle} = k\\langle q^2 \\rangle$\n- **Task**: Derive the one-step update map for the BAOAB integrator, use it to find the stationary phase-space covariance matrix $\\Sigma$ by solving the discrete-time Lyapunov equation $\\Sigma = A\\Sigma A^\\top + Q$, and compute the temperature discrepancy $\\delta = T_k - T_c$.\n- **Units**: Dimensionless, with $k_B=1$.\n- **Test Suite**: Four parameter sets for $(m, k, T, \\gamma, \\Delta t)$:\n    1. $(1, 1, 1, 1, 0.01)$\n    2. $(1, 1, 1, 1, 0.5)$\n    3. $(1, 1, 1, 0.5, 1.9)$\n    4. $(1, 1, 1, 10, 1.0)$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in established principles of statistical mechanics and numerical analysis of stochastic differential equations. The definitions of the temperature estimators, the Langevin equation, and the concept of integrator-induced bias are all standard in the field of molecular simulation. The problem is well-posed, providing all necessary parameters and a clear, formalizable path to a unique solution. It is objective and free of ambiguity. All terms are standard or explicitly defined. The task is non-trivial and probes a key concept in simulation fidelity.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full, reasoned solution follows.\n\n### Principle-Based Solution Derivation\n\nThe state of the system is described by the phase-space vector $\\mathbf{x} = \\begin{pmatrix} q \\\\ p \\end{pmatrix}$. The $BAOAB$ integrator is a sequence of five transformations applied to this state vector over a single timestep $\\Delta t$. We will derive the matrix representation for each step.\n\n**1. The Individual Propagator Steps**\n\n- **Step B (Force integration)**: A half-step update of momentum due to the conservative force $F(q) = -\\nabla U = -kq$.\n  $$ p' = p - kq \\frac{\\Delta t}{2}, \\quad q' = q $$\n  This corresponds to a linear transformation $\\mathbf{x}' = M_B \\mathbf{x}$, where $M_B = \\begin{pmatrix} 1 & 0 \\\\ -k \\frac{\\Delta t}{2} & 1 \\end{pmatrix}$.\n\n- **Step A (Free-particle motion)**: A half-step update of position due to the momentum.\n  $$ q' = q + \\frac{p}{m} \\frac{\\Delta t}{2}, \\quad p' = p $$\n  This corresponds to a linear transformation $\\mathbf{x}' = M_A \\mathbf{x}$, where $M_A = \\begin{pmatrix} 1 & \\frac{\\Delta t}{2m} \\\\ 0 & 1 \\end{pmatrix}$.\n\n- **Step O (Ornstein-Uhlenbeck integration)**: A full-step exact solution for the momentum under friction and stochastic forces, $\\dot{p} = -\\gamma p + \\sqrt{2m\\gamma T} R(t)$ (with $k_B=1$), where $R(t)$ is Gaussian white noise.\n  $$ p' = e^{-\\gamma \\Delta t} p + \\sqrt{mT(1 - e^{-2\\gamma \\Delta t})} \\mathcal{N}(0,1), \\quad q' = q $$\n  where $\\mathcal{N}(0,1)$ is a standard normal random variable. This is an affine transformation $\\mathbf{x}' = M_O \\mathbf{x} + \\eta_O$, where $M_O = \\begin{pmatrix} 1 & 0 \\\\ 0 & c_1 \\end{pmatrix}$, $c_1 = e^{-\\gamma \\Delta t}$, and $\\eta_O = \\begin{pmatrix} 0 \\\\ \\eta_p \\end{pmatrix}$. The random term $\\eta_p$ has zero mean and variance $\\langle \\eta_p^2 \\rangle = \\sigma_p^2 = mT(1 - c_1^2)$.\n\n**2. The Full One-Step Propagator**\n\nThe BAOAB scheme applies these steps in sequence: $B(\\frac{\\Delta t}{2})$, $A(\\frac{\\Delta t}{2})$, $O(\\Delta t)$, $A(\\frac{\\Delta t}{2})$, $B(\\frac{\\Delta t}{2})$. Let the state at the beginning of the step be $\\mathbf{x}_n$.\nThe state is propagated as follows:\n$\\mathbf{x}_{mid} = M_A M_B \\mathbf{x}_n$\n$\\mathbf{x}_{O} = M_O \\mathbf{x}_{mid} + \\eta_O$\n$\\mathbf{x}_{n+1} = M_B M_A \\mathbf{x}_O$\n\nSubstituting these gives the full one-step map:\n$$ \\mathbf{x}_{n+1} = (M_B M_A) (M_O (M_A M_B \\mathbf{x}_n) + \\eta_O) $$\n$$ \\mathbf{x}_{n+1} = (M_B M_A M_O M_A M_B) \\mathbf{x}_n + (M_B M_A) \\eta_O $$\nThis is a linear stochastic map of the form $\\mathbf{x}_{n+1} = M \\mathbf{x}_n + \\eta_n$, where:\n- The deterministic propagator is $M = M_{BA} M_O M_{AB}$, with $M_{AB} = M_A M_B$ and $M_{BA} = M_B M_A$.\n- The noise vector is $\\eta_n = M_{BA} \\eta_O$.\n\nExplicitly, the matrices are:\n$M_{AB} = \\begin{pmatrix} 1 - \\frac{k (\\Delta t)^2}{4m} & \\frac{\\Delta t}{2m} \\\\ -k \\frac{\\Delta t}{2} & 1 \\end{pmatrix}$\n$M_{BA} = \\begin{pmatrix} 1 & \\frac{\\Delta t}{2m} \\\\ -k \\frac{\\Delta t}{2} & 1 - \\frac{k (\\Delta t)^2}{4m} \\end{pmatrix}$\n$M_O = \\begin{pmatrix} 1 & 0 \\\\ 0 & e^{-\\gamma \\Delta t} \\end{pmatrix}$\n\nThe final propagator $M$ is the product $M_{BA} M_O M_{AB}$. The noise vector is:\n$$ \\eta_n = M_{BA} \\begin{pmatrix} 0 \\\\ \\eta_p \\end{pmatrix} = \\begin{pmatrix} \\frac{\\Delta t}{2m} \\eta_p \\\\ (1 - \\frac{k(\\Delta t)^2}{4m}) \\eta_p \\end{pmatrix} $$\n\n**3. Stationary Covariance and The Lyapunov Equation**\n\nAt statistical stationarity, the covariance matrix $\\Sigma = \\langle \\mathbf{x}_n \\mathbf{x}_n^T \\rangle$ is constant. Taking the expectation of $\\mathbf{x}_{n+1}\\mathbf{x}_{n+1}^T$:\n$$ \\langle \\mathbf{x}_{n+1}\\mathbf{x}_{n+1}^T \\rangle = \\langle (M \\mathbf{x}_n + \\eta_n)(M \\mathbf{x}_n + \\eta_n)^T \\rangle $$\nSince the noise $\\eta_n$ at step $n$ is uncorrelated with the state $\\mathbf{x}_n$, this simplifies to:\n$$ \\Sigma = M \\langle \\mathbf{x}_n \\mathbf{x}_n^T \\rangle M^T + \\langle \\eta_n \\eta_n^T \\rangle = M \\Sigma M^T + Q $$\nThis is the discrete-time Lyapunov equation for $\\Sigma$. The noise covariance matrix $Q = \\langle \\eta_n \\eta_n^T \\rangle$ is given by:\n$$ Q = \\langle \\eta_n \\eta_n^T \\rangle = \\sigma_p^2 \\begin{pmatrix} \\frac{\\Delta t}{2m} \\\\ 1 - \\frac{k(\\Delta t)^2}{4m} \\end{pmatrix} \\begin{pmatrix} \\frac{\\Delta t}{2m} & 1 - \\frac{k(\\Delta t)^2}{4m} \\end{pmatrix} $$\nwhere $\\sigma_p^2 = mT(1 - e^{-2\\gamma \\Delta t})$.\n\n**4. Temperature Calculation and Discrepancy**\n\nBy solving the Lyapunov equation for $\\Sigma = \\begin{pmatrix} \\langle q^2 \\rangle & \\langle qp \\rangle \\\\ \\langle qp \\rangle & \\langle p^2 \\rangle \\end{pmatrix}$, we obtain the stationary second moments of position and momentum. The temperatures are then calculated as:\n- $T_c = k \\langle q^2 \\rangle = k \\Sigma_{00}$\n- $T_k = \\frac{\\langle p^2 \\rangle}{m} = \\frac{1}{m} \\Sigma_{11}$\n\nThe required discrepancy is $\\delta = T_k - T_c$. The solution procedure is to construct the matrices $M$ and $Q$ from the input parameters, solve the Lyapunov equation for $\\Sigma$ numerically, and then compute $\\delta$. This will be implemented in the provided Python code.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_discrete_lyapunov\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n\n    The function iterates through each parameter set, constructs the one-step\n    BAOAB propagator matrix and noise covariance matrix, solves the\n    discrete-time Lyapunov equation for the stationary phase-space covariance,\n    and then computes the kinetic and configurational temperatures to find their\n    discrepancy.\n    \"\"\"\n    \n    # Test cases defined in the problem statement.\n    # Each tuple is (m, k, T, gamma, dt).\n    test_cases = [\n        (1.0, 1.0, 1.0, 1.0, 0.01),\n        (1.0, 1.0, 1.0, 1.0, 0.5),\n        (1.0, 1.0, 1.0, 0.5, 1.9),\n        (1.0, 1.0, 1.0, 10.0, 1.0),\n    ]\n\n    results = []\n    for m, k, T, gamma, dt in test_cases:\n        # Step 1: Define constants and intermediate values from the derivation.\n        # c1 is the momentum damping factor from the Ornstein-Uhlenbeck step.\n        c1 = np.exp(-gamma * dt)\n        \n        # sigma_p_sq is the variance of the stochastic noise added to momentum.\n        # k_B is 1 in the specified dimensionless units.\n        sigma_p_sq = m * T * (1.0 - np.exp(-2.0 * gamma * dt))\n\n        # Step 2: Construct the matrices for the operator splitting scheme.\n        \n        # M_AB corresponds to the first half-step B (force) then A (position).\n        M_AB = np.array([\n            [1.0 - k * dt**2 / (4.0 * m), dt / (2.0 * m)],\n            [-0.5 * k * dt, 1.0]\n        ])\n\n        # M_O corresponds to the full-step Ornstein-Uhlenbeck update.\n        M_O = np.array([\n            [1.0, 0.0],\n            [0.0, c1]\n        ])\n\n        # M_BA corresponds to the second half-step A (position) then B (force).\n        M_BA = np.array([\n            [1.0, dt / (2.0 * m)],\n            [-0.5 * k * dt, 1.0 - k * dt**2 / (4.0 * m)]\n        ])\n\n        # Step 3: Compute the full one-step deterministic propagator matrix M.\n        # M = M_BA * M_O * M_AB\n        M = M_BA @ M_O @ M_AB\n\n        # Step 4: Construct the noise covariance matrix Q.\n        # The noise vector is eta = M_BA * [0, eta_p]^T, where <eta_p^2> = sigma_p_sq.\n        # This results in a rank-1 covariance matrix Q = <eta * eta^T>.\n        a = dt / (2.0 * m)\n        b = 1.0 - k * dt**2 / (4.0 * m)\n        noise_vector_coeffs = np.array([a, b])\n        Q = sigma_p_sq * np.outer(noise_vector_coeffs, noise_vector_coeffs)\n\n        # Step 5: Solve the discrete-time Lyapunov equation for stationary covariance.\n        # Sigma = M * Sigma * M^T + Q\n        Sigma = solve_discrete_lyapunov(M, Q)\n\n        # Step 6: Extract the mean square position and momentum.\n        # Sigma[0,0] = <q^2>, Sigma[1,1] = <p^2>\n        q2_avg = Sigma[0, 0]\n        p2_avg = Sigma[1, 1]\n\n        # Step 7: Calculate configurational and kinetic temperatures.\n        # (with k_B = 1)\n        Tc = k * q2_avg\n        Tk = p2_avg / m\n\n        # Step 8: Compute the discrepancy and store the result.\n        delta = Tk - Tc\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}