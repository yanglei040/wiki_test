{
    "hands_on_practices": [
        {
            "introduction": "Before tackling complex molecular systems, it is essential to build intuition on a model that is analytically tractable. This exercise provides a foundational \"sanity check\" for the methods of Thermodynamic Integration (TI) and Free Energy Perturbation (FEP) by applying them to a one-dimensional harmonic oscillator, a cornerstone of statistical mechanics. By deriving the free energy difference through direct calculation and then showing that both TI and FEP yield the identical, exact result, you will solidify your understanding of the formalisms and gain confidence in their application .",
            "id": "2642319",
            "problem": "Consider a classical, one-dimensional harmonic oscillator with potential energy $U(x;k)=\\tfrac{1}{2}k x^{2}$, where $k>0$ is the force constant and $x$ is the Cartesian coordinate. Work at fixed temperature $T$ in the canonical ensemble. Assume that only configurational contributions are considered (kinetic terms are independent of $k$ and cancel in free-energy differences between states with different $k$). Let $k_{B}$ denote the Boltzmann constant and $\\beta \\equiv 1/(k_{B}T)$.\n\na) Starting from the canonical configurational partition function definition, $Z_{\\mathrm{conf}}(k,T)=\\int_{-\\infty}^{\\infty}\\exp\\!\\left(-\\beta U(x;k)\\right)\\,\\mathrm{d}x$, derive the exact configurational Helmholtz free energy $F_{\\mathrm{conf}}(k,T)$.\n\nb) Consider changing the force constant from $k_{0}>0$ to $k_{1}>0$ at fixed $T$. Introduce a coupling parameter $\\lambda\\in[0,1]$ that defines an interpolating Hamiltonian $U_{\\lambda}(x)=\\tfrac{1}{2}k(\\lambda)x^{2}$ with $k(\\lambda)=(1-\\lambda)k_{0}+\\lambda k_{1}$. Starting from fundamental ensemble definitions, derive an expression for the free-energy difference $\\Delta F \\equiv F_{\\mathrm{conf}}(k_{1},T)-F_{\\mathrm{conf}}(k_{0},T)$ using Thermodynamic Integration (TI), where TI denotes Thermodynamic Integration.\n\nc) Starting from the canonical ratio of partition functions, derive and then evaluate exactly the Free Energy Perturbation (FEP) estimator, where FEP denotes Free Energy Perturbation, for the same transformation. That is, express $\\Delta F$ in terms of an average over the reference ensemble at $k_{0}$ and evaluate the average analytically.\n\nYour final answer should be a single, closed-form analytic expression for $\\Delta F$ in terms of $k_{0}$, $k_{1}$, $k_{B}$, and $T$. No numerical evaluation is required. Express the final result as a single analytic expression. Do not include units in the final boxed answer.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is based on standard principles of classical statistical mechanics and presents a canonical problem readily solvable by established methods. There are no logical contradictions, missing information, or factual inaccuracies. The problem is valid. We proceed with the solution.\n\nThe problem requires the calculation of the Helmholtz free energy difference, $\\Delta F$, for a one-dimensional classical harmonic oscillator when its force constant is changed from $k_{0}$ to $k_{1}$ at constant temperature $T$. We will derive this quantity through three distinct methods as requested.\n\n(a) Exact configurational Helmholtz free energy $F_{\\mathrm{conf}}(k,T)$\n\nThe configurational partition function $Z_{\\mathrm{conf}}$ for a system with potential energy $U(x;k)$ at temperature $T$ is given by\n$$\nZ_{\\mathrm{conf}}(k,T) = \\int_{-\\infty}^{\\infty} \\exp(-\\beta U(x;k)) \\, \\mathrm{d}x\n$$\nwhere $\\beta = 1/(k_{B}T)$. For the harmonic oscillator, the potential energy is $U(x;k) = \\frac{1}{2}kx^{2}$. Substituting this into the definition of the partition function gives\n$$\nZ_{\\mathrm{conf}}(k,T) = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{1}{2}\\beta k x^{2}\\right) \\, \\mathrm{d}x\n$$\nThis is a standard Gaussian integral of the form $\\int_{-\\infty}^{\\infty} \\exp(-ax^{2}) \\, \\mathrm{d}x = \\sqrt{\\frac{\\pi}{a}}$. In our case, the constant $a$ is $\\frac{\\beta k}{2}$. Therefore, the integral evaluates to\n$$\nZ_{\\mathrm{conf}}(k,T) = \\sqrt{\\frac{\\pi}{\\frac{\\beta k}{2}}} = \\sqrt{\\frac{2\\pi}{\\beta k}}\n$$\nThe Helmholtz free energy $F$ is related to the partition function $Z$ by the fundamental equation $F = -k_{B}T \\ln Z = -\\frac{1}{\\beta} \\ln Z$. The configurational contribution to the Helmholtz free energy is thus\n$$\nF_{\\mathrm{conf}}(k,T) = -\\frac{1}{\\beta} \\ln Z_{\\mathrm{conf}}(k,T) = -\\frac{1}{\\beta} \\ln\\left(\\sqrt{\\frac{2\\pi}{\\beta k}}\\right)\n$$\nUsing the property of logarithms $\\ln(\\sqrt{y}) = \\frac{1}{2}\\ln(y)$, we obtain\n$$\nF_{\\mathrm{conf}}(k,T) = -\\frac{1}{2\\beta} \\ln\\left(\\frac{2\\pi}{\\beta k}\\right) = \\frac{1}{2\\beta} \\ln\\left(\\frac{\\beta k}{2\\pi}\\right)\n$$\nThis is the exact expression for the configurational Helmholtz free energy of the one-dimensional harmonic oscillator.\n\n(b) Free energy difference via Thermodynamic Integration (TI)\n\nThermodynamic Integration (TI) provides a method to calculate the free energy difference between two states by integrating the ensemble average of the derivative of the Hamiltonian with respect to a coupling parameter $\\lambda$. The free energy difference is given by\n$$\n\\Delta F = \\int_{0}^{1} \\left\\langle \\frac{\\partial U_{\\lambda}(x)}{\\partial \\lambda} \\right\\rangle_{\\lambda} \\, \\mathrm{d}\\lambda\n$$\nwhere $U_{\\lambda}(x)$ is the potential energy of the system interpolated between state $0$ ($\\lambda=0$) and state $1$ ($\\lambda=1$), and $\\langle \\dots \\rangle_{\\lambda}$ denotes an ensemble average over the system with potential $U_{\\lambda}(x)$.\n\nThe interpolating potential is given as $U_{\\lambda}(x) = \\frac{1}{2}k(\\lambda)x^{2}$, with $k(\\lambda) = (1-\\lambda)k_{0} + \\lambda k_{1}$. First, we compute the partial derivative of $U_{\\lambda}(x)$ with respect to $\\lambda$:\n$$\n\\frac{\\partial U_{\\lambda}(x)}{\\partial \\lambda} = \\frac{1}{2} \\frac{\\mathrm{d}k(\\lambda)}{\\mathrm{d}\\lambda} x^{2} = \\frac{1}{2} (k_{1} - k_{0}) x^{2}\n$$\nNext, we must compute the ensemble average of this quantity, $\\langle \\frac{\\partial U_{\\lambda}(x)}{\\partial \\lambda} \\rangle_{\\lambda}$, in the canonical ensemble defined by the potential $U_{\\lambda}(x)$:\n$$\n\\left\\langle \\frac{\\partial U_{\\lambda}(x)}{\\partial \\lambda} \\right\\rangle_{\\lambda} = \\left\\langle \\frac{1}{2} (k_{1} - k_{0}) x^{2} \\right\\rangle_{\\lambda} = \\frac{1}{2} (k_{1} - k_{0}) \\langle x^{2} \\rangle_{\\lambda}\n$$\nThe quantity $\\langle x^{2} \\rangle_{\\lambda}$ is the mean-squared displacement of the oscillator with force constant $k(\\lambda)$. For a classical one-dimensional harmonic oscillator, the equipartition theorem states that the average potential energy is $\\langle \\frac{1}{2}k(\\lambda)x^{2} \\rangle_{\\lambda} = \\frac{1}{2}k_{B}T$. From this, we directly find the mean-squared displacement:\n$$\n\\langle x^{2} \\rangle_{\\lambda} = \\frac{k_{B}T}{k(\\lambda)} = \\frac{1}{\\beta k(\\lambda)}\n$$\nSubstituting this result into the expression for the ensemble average derivative gives\n$$\n\\left\\langle \\frac{\\partial U_{\\lambda}(x)}{\\partial \\lambda} \\right\\rangle_{\\lambda} = \\frac{1}{2} (k_{1} - k_{0}) \\frac{1}{\\beta k(\\lambda)} = \\frac{k_{1} - k_{0}}{2\\beta((1-\\lambda)k_{0} + \\lambda k_{1})}\n$$\nNow we can perform the integration over $\\lambda$ from $0$ to $1$:\n$$\n\\Delta F = \\int_{0}^{1} \\frac{k_{1} - k_{0}}{2\\beta((1-\\lambda)k_{0} + \\lambda k_{1})} \\, \\mathrm{d}\\lambda\n$$\nLet the denominator be $f(\\lambda) = (1-\\lambda)k_{0} + \\lambda k_{1} = k_{0} + \\lambda(k_{1}-k_{0})$. The derivative is $f'(\\lambda) = k_{1} - k_{0}$. The integral is of the form $\\int \\frac{f'(\\lambda)}{f(\\lambda)}\\mathrm{d}\\lambda = \\ln(f(\\lambda))$.\n$$\n\\Delta F = \\frac{1}{2\\beta} \\left[ \\ln\\left((1-\\lambda)k_{0} + \\lambda k_{1}\\right) \\right]_{0}^{1} = \\frac{1}{2\\beta} (\\ln(k_{1}) - \\ln(k_{0})) = \\frac{1}{2\\beta} \\ln\\left(\\frac{k_{1}}{k_{0}}\\right)\n$$\n\n(c) Free energy difference via Free Energy Perturbation (FEP)\n\nThe Free Energy Perturbation (FEP) formula, also known as the Zwanzig equation, expresses the free energy difference in terms of an ensemble average of the potential energy difference, taken over a reference state. Choosing the initial state with force constant $k_{0}$ as the reference, the formula is\n$$\n\\Delta F = F_{1} - F_{0} = -\\frac{1}{\\beta} \\ln \\left\\langle \\exp\\left(-\\beta (U_{1}(x) - U_{0}(x))\\right) \\right\\rangle_{0}\n$$\nHere, $U_{0}(x) = \\frac{1}{2}k_{0}x^{2}$, $U_{1}(x) = \\frac{1}{2}k_{1}x^{2}$, and $\\langle \\dots \\rangle_{0}$ denotes an ensemble average over the reference state $0$. The potential energy difference is $\\Delta U(x) = U_{1}(x) - U_{0}(x) = \\frac{1}{2}(k_{1}-k_{0})x^{2}$.\n\nThe ensemble average is computed as\n$$\n\\left\\langle \\exp(-\\beta \\Delta U(x)) \\right\\rangle_{0} = \\frac{\\int_{-\\infty}^{\\infty} \\exp(-\\beta \\Delta U(x)) \\exp(-\\beta U_{0}(x)) \\, \\mathrm{d}x}{\\int_{-\\infty}^{\\infty} \\exp(-\\beta U_{0}(x)) \\, \\mathrm{d}x}\n$$\nThe argument of the exponential in the numerator's integrand is\n$$\n-\\beta \\Delta U(x) - \\beta U_{0}(x) = -\\beta \\left(\\frac{1}{2}(k_{1}-k_{0})x^{2} + \\frac{1}{2}k_{0}x^{2}\\right) = -\\frac{1}{2}\\beta k_{1}x^{2} = -\\beta U_1(x)\n$$\nThus, the ensemble average is the ratio of two partition functions:\n$$\n\\left\\langle \\exp(-\\beta \\Delta U(x)) \\right\\rangle_{0} = \\frac{\\int_{-\\infty}^{\\infty} \\exp(-\\beta U_{1}(x)) \\, \\mathrm{d}x}{\\int_{-\\infty}^{\\infty} \\exp(-\\beta U_{0}(x)) \\, \\mathrm{d}x} = \\frac{Z_{\\mathrm{conf}}(k_1, T)}{Z_{\\mathrm{conf}}(k_0, T)}\n$$\nUsing the result for $Z_{\\mathrm{conf}}$ from part (a), $Z_{\\mathrm{conf}}(k,T) = \\sqrt{\\frac{2\\pi}{\\beta k}}$, we have\n$$\n\\frac{Z_{\\mathrm{conf}}(k_1, T)}{Z_{\\mathrm{conf}}(k_0, T)} = \\frac{\\sqrt{\\frac{2\\pi}{\\beta k_{1}}}}{\\sqrt{\\frac{2\\pi}{\\beta k_{0}}}} = \\sqrt{\\frac{k_{0}}{k_{1}}}\n$$\nSubstituting this back into the FEP formula for $\\Delta F$:\n$$\n\\Delta F = -\\frac{1}{\\beta} \\ln\\left(\\sqrt{\\frac{k_{0}}{k_{1}}}\\right) = -\\frac{1}{2\\beta} \\ln\\left(\\frac{k_{0}}{k_{1}}\\right) = \\frac{1}{2\\beta} \\ln\\left(\\frac{k_{1}}{k_{0}}\\right)\n$$\nAll three methods—direct calculation from the definition of $F$, thermodynamic integration, and free energy perturbation—yield the identical result, as required by consistency. The final expression for the change in Helmholtz free energy is derived by substituting $\\beta = 1/(k_{B}T)$:\n$$\n\\Delta F = \\frac{1}{2}k_{B}T \\ln\\left(\\frac{k_{1}}{k_{0}}\\right)\n$$\nThis is the required single, closed-form analytic expression for $\\Delta F$.",
            "answer": "$$\n\\boxed{\\frac{1}{2} k_{B} T \\ln\\left(\\frac{k_{1}}{k_{0}}\\right)}\n$$"
        },
        {
            "introduction": "Moving from analytical models to realistic simulations reveals practical challenges, chief among them the \"endpoint problem\" where the creation or annihilation of an atom leads to singularities in the potential energy. This practice delves into the standard solution: the use of \"soft-core\" potentials that regularize the interaction at short distances. You will derive the thermodynamic integration integrand for a soft-core Lennard-Jones potential and computationally investigate how different parameter choices affect the integrand's smoothness near the decoupled endpoint ($\\lambda \\to 1$), providing direct insight into designing stable and efficient alchemical transformations .",
            "id": "3454192",
            "problem": "Consider a single-site Lennard–Jones solute immersed in a continuum of solvent, studied in reduced Lennard–Jones units where $\\sigma = 1$ and $\\epsilon = 1$. The temperature is fixed such that the inverse thermal energy $\\beta = 1$, and distances and energies are expressed in units of $\\sigma$ and $\\epsilon$, respectively. In this setting, we want to analyze Thermodynamic Integration (TI) for decoupling the solute from the solvent under a soft-core scheme. The canonical ensemble average of the TI integrand is the expectation of the derivative of the Hamiltonian with respect to the coupling parameter, computed at fixed $\\lambda$. The free energy difference between the fully coupled state and the decoupled state is the integral over $\\lambda$ of this canonical average.\n\nStart from the following fundamental base:\n- The canonical ensemble probability density for a configuration with energy $U$ at inverse thermal energy $\\beta$ is proportional to $\\exp(-\\beta U)$.\n- The free energy difference between two states parameterized by $\\lambda$ follows from integrating the canonical expectation of the parametric derivative: $\\Delta F = \\int_{0}^{1} \\langle \\partial U / \\partial \\lambda \\rangle_{\\lambda} \\, \\mathrm{d}\\lambda$, where $\\langle \\cdot \\rangle_{\\lambda}$ denotes the ensemble average at fixed $\\lambda$.\n- A Lennard–Jones pair interaction in reduced units is given by $u_{\\mathrm{LJ}}(r) = 4 \\left( r^{-12} - r^{-6} \\right)$, with $r$ the separation.\n\nWe adopt a soft-core mapping that regularizes the repulsive singularity during decoupling. The radial soft-core function is specified by an exponent $p \\geq 1$ and a soft-core parameter $\\alpha \\geq 0$ through a modified sixth-power distance,\n$$\nr_{\\mathrm{sc}}^{6}(\\lambda) = r^{6} + \\alpha \\left(1 - \\lambda\\right)^{p} ,\n$$\nand the $\\lambda$-dependent potential energy is scaled linearly in $\\lambda$ by evaluating the Lennard–Jones form at $r_{\\mathrm{sc}}(\\lambda)$,\n$$\nU(r;\\lambda) = 4 \\lambda \\left[ \\left(r_{\\mathrm{sc}}^{6}(\\lambda)\\right)^{-2} - \\left(r_{\\mathrm{sc}}^{6}(\\lambda)\\right)^{-1} \\right] .\n$$\nThis model reproduces the standard Lennard–Jones potential at $\\lambda = 1$ and achieves zero interaction at $\\lambda = 0$.\n\nAssume a spherically symmetric, single-pair effective description of the solute–solvent interaction. In this description, an effective radial probability density for the separation $r$ at fixed $\\lambda$ is proportional to the three-dimensional measure and the Boltzmann factor,\n$$\nP(r \\mid \\lambda) \\propto r^{2} \\exp\\left( -\\beta U(r;\\lambda) \\right) ,\n$$\nover the domain $r \\in [0, r_{\\max}]$ with $r_{\\max} = 5$. Under this approximation, the canonical ensemble average of the TI integrand at fixed $\\lambda$ is computed as the radial average with this weight,\n$$\n\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle_{\\lambda} = \\frac{ \\int_{0}^{r_{\\max}} r^{2} \\exp\\left( -\\beta U(r;\\lambda) \\right) \\left( \\frac{\\partial U}{\\partial \\lambda}(r;\\lambda) \\right) \\, \\mathrm{d}r }{ \\int_{0}^{r_{\\max}} r^{2} \\exp\\left( -\\beta U(r;\\lambda) \\right) \\, \\mathrm{d}r } .\n$$\n\nYour task is to:\n1. Derive from first principles the explicit expression for $\\partial U / \\partial \\lambda$ for the soft-core mapping given above and explain the endpoint behavior as $\\lambda \\to 1$ for different choices of $p$ and $\\alpha$.\n2. Implement a self-contained, numerically stable program to compute the canonical average $\\langle \\partial U/\\partial\\lambda \\rangle_{\\lambda}$ at four near-endpoint values of $\\lambda$: $0.90$, $0.99$, $0.999$, and $0.9999$, using the radial average specified above.\n3. Define a smoothness index quantifying the near-endpoint oscillations of the integrand as\n$$\nS = \\max_{j \\in \\{1,2,3\\}} \\left| I(\\lambda_{j+1}) - I(\\lambda_{j}) \\right| ,\n$$\nwhere $I(\\lambda)$ denotes the computed canonical average and $\\lambda_{1} = 0.90$, $\\lambda_{2} = 0.99$, $\\lambda_{3} = 0.999$, $\\lambda_{4} = 0.9999$. Report $S$ in energy units of $\\epsilon$.\n\nUse reduced Lennard–Jones units throughout. Energies must be in units of $\\epsilon$ and distances in units of $\\sigma$. No angles are involved. The inverse thermal energy is fixed to $\\beta = 1$.\n\nTest suite:\n- Case A (happy path): $(\\alpha, p) = (1.0, 1)$.\n- Case B (boundary, no soft-core): $(\\alpha, p) = (0.0, 1)$.\n- Case C (more smoothing): $(\\alpha, p) = (2.0, 1)$.\n- Case D (endpoint-regularizing exponent): $(\\alpha, p) = (1.0, 2)$.\n\nFor each case, compute and return a list of three floating-point numbers: $[I(0.99), I(0.999), S]$, where $I(\\lambda)$ is the canonical average of the integrand at the specified $\\lambda$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, aggregating all test cases in order A, B, C, D. For example, the output format must be like:\n\"[[I_A(0.99),I_A(0.999),S_A],[I_B(0.99),I_B(0.999),S_B],[I_C(0.99),I_C(0.999),S_C],[I_D(0.99),I_D(0.999),S_D]]\".",
            "solution": "The problem is valid as it is scientifically grounded in statistical mechanics and computational chemistry, well-posed with all necessary parameters and functions defined, and objective. We can proceed with a solution. The solution involves two main parts: first, an analytical derivation of the thermodynamic integration (TI) integrand and an analysis of its behavior, and second, a numerical implementation to compute the required quantities.\n\n### Part 1: Analytical Derivation and Endpoint Analysis\n\nThe potential energy $U(r;\\lambda)$ is given by\n$$\nU(r;\\lambda) = 4 \\lambda \\left[ \\left(r_{\\mathrm{sc}}^{6}(\\lambda)\\right)^{-2} - \\left(r_{\\mathrm{sc}}^{6}(\\lambda)\\right)^{-1} \\right]\n$$\nwhere the soft-core modified sixth-power distance is\n$$\nr_{\\mathrm{sc}}^{6}(\\lambda) = r^{6} + \\alpha \\left(1 - \\lambda\\right)^{p}.\n$$\nTo simplify notation, let $g(\\lambda) \\equiv r_{\\mathrm{sc}}^{6}(\\lambda)$. The TI integrand is the partial derivative of the potential energy $U$ with respect to the coupling parameter $\\lambda$. We compute this using the product rule and chain rule:\n$$\n\\frac{\\partial U}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left( 4 \\lambda \\left[ g(\\lambda)^{-2} - g(\\lambda)^{-1} \\right] \\right)\n$$\n$$\n\\frac{\\partial U}{\\partial \\lambda} = 4 \\left[ g(\\lambda)^{-2} - g(\\lambda)^{-1} \\right] + 4 \\lambda \\frac{d}{d g} \\left[ g^{-2} - g^{-1} \\right] \\frac{\\partial g}{\\partial \\lambda}.\n$$\nThe first term is simply $U(r;\\lambda)/\\lambda$. The derivatives of the components are:\n$$\n\\frac{d}{d g} \\left[ g^{-2} - g^{-1} \\right] = -2 g^{-3} + g^{-2}\n$$\nand\n$$\n\\frac{\\partial g}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left[ r^{6} + \\alpha (1 - \\lambda)^{p} \\right] = \\alpha \\cdot p (1 - \\lambda)^{p-1} \\cdot (-1) = -\\alpha p (1 - \\lambda)^{p-1}.\n$$\nSubstituting these back into the expression for $\\partial U / \\partial \\lambda$:\n$$\n\\frac{\\partial U}{\\partial \\lambda} = \\frac{U(r;\\lambda)}{\\lambda} + 4 \\lambda \\left[ g(\\lambda)^{-2} - 2g(\\lambda)^{-3} \\right] \\left[ -\\alpha p (1 - \\lambda)^{p-1} \\right]\n$$\n$$\n\\frac{\\partial U}{\\partial \\lambda} = \\frac{U(r;\\lambda)}{\\lambda} - 4 \\lambda \\alpha p (1 - \\lambda)^{p-1} \\left[ \\left(r_{\\mathrm{sc}}^{6}(\\lambda)\\right)^{-2} - 2\\left(r_{\\mathrm{sc}}^{6}(\\lambda)\\right)^{-3} \\right].\n$$\nThis is the explicit expression for the TI integrand.\n\nNext, we analyze the endpoint behavior as $\\lambda \\to 1$. In this limit, the soft-core term $\\alpha(1-\\lambda)^p \\to 0$ (for $p \\ge 1$), so $g(\\lambda) \\to r^6$. The first term, $U(r;\\lambda)/\\lambda$, approaches the standard Lennard-Jones potential, $u_{\\mathrm{LJ}}(r) = 4(r^{-12} - r^{-6})$. The behavior of the second term depends critically on the exponent $p$ through the factor $(1-\\lambda)^{p-1}$:\n\n1.  **Case B (no soft-core, $\\alpha=0$):** In this case, the second term in the expression for $\\partial U / \\partial \\lambda$ is always zero. The potential is $U(r;\\lambda) = \\lambda u_{\\mathrm{LJ}}(r)$, and its derivative is simply $\\partial U / \\partial \\lambda = u_{\\mathrm{LJ}}(r)$. Although the derivative function itself is independent of $\\lambda$, the canonical average $\\langle u_{\\mathrm{LJ}}(r) \\rangle_{\\lambda}$ depends strongly on $\\lambda$ through the Boltzmann weight $\\exp(-\\beta \\lambda u_{\\mathrm{LJ}}(r))$. As $\\lambda \\to 1$, the repulsive wall at small $r$ grows, drastically changing the probability distribution $P(r|\\lambda)$ and causing a sharp variation in the average, which is the well-known \"endpoint problem\" of TI.\n\n2.  **Cases A and C ($p=1$):** For $p=1$, the exponent in the second term becomes $p-1=0$. Thus, $(1-\\lambda)^{p-1} \\to 1$ as $\\lambda \\to 1$. The derivative $\\partial U / \\partial \\lambda$ converges to a limit that is different from $u_{\\mathrm{LJ}}(r)$:\n    $$\n    \\lim_{\\lambda \\to 1} \\frac{\\partial U}{\\partial \\lambda} = u_{\\mathrm{LJ}}(r) - 4 \\alpha (r^{-12} - 2r^{-18}).\n    $$\n    This implies a discontinuity in the definition of the integrand's functional form at $\\lambda=1$. The canonical average $\\langle \\partial U / \\partial \\lambda \\rangle_{\\lambda}$ is therefore expected to exhibit significant variation as $\\lambda$ approaches $1$, leading to a relatively large smoothness index $S$.\n\n3.  **Case D ($p=2$):** For $p=2$, the exponent becomes $p-1=1 > 0$. Therefore, the factor $(1-\\lambda)^{p-1} \\to 0$ as $\\lambda \\to 1$. This causes the entire second term of $\\partial U / \\partial \\lambda$ to vanish at the endpoint.\n    $$\n    \\lim_{\\lambda \\to 1} \\frac{\\partial U}{\\partial \\lambda} = u_{\\mathrm{LJ}}(r).\n    $$\n    In this scenario, the derivative itself smoothly approaches the standard Lennard-Jones potential. It can be shown that higher derivatives with respect to $\\lambda$ also vanish, leading to a much flatter profile for the canonical average $\\langle \\partial U / \\partial \\lambda \\rangle_{\\lambda}$ near $\\lambda=1$. This choice of exponent is designed to regularize the endpoint and significantly improves the numerical stability and accuracy of the overall free energy calculation. Consequently, we anticipate a very small smoothness index $S$ for this case.\n\n### Part 2: Numerical Computation\n\nThe canonical average $\\langle \\partial U / \\partial \\lambda \\rangle_{\\lambda}$, which we denote as $I(\\lambda)$, is computed by numerically evaluating the ratio of two integrals:\n$$\nI(\\lambda) = \\frac{N(\\lambda)}{D(\\lambda)} = \\frac{ \\int_{0}^{r_{\\max}} r^{2} \\exp\\left( -\\beta U(r;\\lambda) \\right) \\left( \\frac{\\partial U}{\\partial \\lambda}(r;\\lambda) \\right) \\, \\mathrm{d}r }{ \\int_{0}^{r_{\\max}} r^{2} \\exp\\left( -\\beta U(r;\\lambda) \\right) \\, \\mathrm{d}r }.\n$$\nWe implement functions for $U(r;\\lambda)$ and $\\partial U / \\partial \\lambda(r;\\lambda)$ based on the expressions derived above. The given problem parameters are $\\beta=1$ and $r_{\\max}=5$. The integrals for the numerator $N(\\lambda)$ and denominator $D(\\lambda)$ are computed using numerical quadrature, specifically the `quad` function from Python's SciPy library. For robustness, we must handle potential numerical issues such as division by zero for $r=0$, although the analysis shows the integrands are well-behaved at this point due to the $r^2$ volume element and the rapid decay of the Boltzmann factor.\n\nThe computational procedure is as follows:\n1.  For each test case $(\\alpha, p)$, we iterate through the specified lambda values: $\\lambda_1=0.90$, $\\lambda_2=0.99$, $\\lambda_3=0.999$, and $\\lambda_4=0.9999$.\n2.  For each $\\lambda$, we compute $I(\\lambda)$ by numerically integrating the numerator and denominator from $r=0$ to $r=5$.\n3.  After computing the four values of $I(\\lambda_j)$, we calculate the smoothness index $S$ as the maximum absolute difference between consecutive values:\n    $$\n    S = \\max \\left( |I(\\lambda_2) - I(\\lambda_1)|, |I(\\lambda_3) - I(\\lambda_2)|, |I(\\lambda_4) - I(\\lambda_3)| \\right).\n    $$\n4.  The results for each case are compiled into a list of three numbers: $[I(0.99), I(0.999), S]$. The final output aggregates these lists for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the results.\n    \"\"\"\n    BETA = 1.0\n    R_MAX = 5.0\n\n    test_cases = [\n        (1.0, 1),  # Case A\n        (0.0, 1),  # Case B\n        (2.0, 1),  # Case C\n        (1.0, 2),  # Case D\n    ]\n\n    def U_func(r, lam, alpha, p):\n        \"\"\"Calculates the soft-core potential U(r; lambda).\"\"\"\n        if r < 1e-12:  # Avoid r=0 issues\n            if alpha > 0 and lam < 1.0:\n                g = alpha * np.power(1.0 - lam, p)\n                if g == 0: return np.inf\n                return 4.0 * lam * (np.power(g, -2) - np.power(g, -1))\n            return np.inf\n\n        r6 = np.power(r, 6)\n        one_minus_lam = 1.0 - lam\n        # Handle lam very close to 1\n        if abs(one_minus_lam) < 1e-15:\n            one_minus_lam = 0.0\n\n        g = r6 + alpha * np.power(one_minus_lam, p)\n        if g <= 0: return np.inf\n        \n        g_inv = 1.0 / g\n        g_inv2 = g_inv * g_inv\n\n        return 4.0 * lam * (g_inv2 - g_inv)\n\n    def dU_dlam_func(r, lam, alpha, p):\n        \"\"\"Calculates the derivative dU/dlambda.\"\"\"\n        if r < 1e-12: # Integrand will be zero due to r^2 term\n            return 0.0\n\n        r6 = np.power(r, 6)\n        one_minus_lam = 1.0 - lam\n        # Handle lam very close to 1\n        if abs(one_minus_lam) < 1e-15:\n            one_minus_lam = 0.0\n\n        g = r6 + alpha * np.power(one_minus_lam, p)\n        if g <= 0: return np.inf\n\n        g_inv = 1.0 / g\n        g_inv2 = g_inv * g_inv\n        \n        # First term: U/lambda\n        u_over_lam = 4.0 * (g_inv2 - g_inv)\n\n        if alpha == 0 or lam == 0:\n            return u_over_lam\n\n        # Second term\n        # np.power(0.0, 0.0) correctly evaluates to 1.0 for p=1, lam=1\n        one_minus_lam_pow = np.power(one_minus_lam, p - 1)\n        \n        g_inv3 = g_inv2 * g_inv\n        \n        second_term_factor = -4.0 * lam * alpha * p * one_minus_lam_pow\n        g_related_term = g_inv2 - 2.0 * g_inv3\n        \n        return u_over_lam + second_term_factor * g_related_term\n\n    def calculate_average(lam, alpha, p):\n        \"\"\"Computes the canonical average <dU/dlambda>.\"\"\"\n        \n        def numerator_integrand(r):\n            potential = U_func(r, lam, alpha, p)\n            if np.isinf(potential):\n                return 0.0\n            \n            boltzmann_factor = np.exp(-BETA * potential)\n            derivative = dU_dlam_func(r, lam, alpha, p)\n            if np.isinf(derivative):\n                return 0.0\n                \n            return r**2 * boltzmann_factor * derivative\n\n        def denominator_integrand(r):\n            potential = U_func(r, lam, alpha, p)\n            if np.isinf(potential):\n                return 0.0\n            \n            boltzmann_factor = np.exp(-BETA * potential)\n            return r**2 * boltzmann_factor\n\n        quad_opts = {'epsabs': 1e-10, 'epsrel': 1e-10, 'limit': 100}\n        num_integral, _ = quad(numerator_integrand, 0, R_MAX, **quad_opts)\n        den_integral, _ = quad(denominator_integrand, 0, R_MAX, **quad_opts)\n\n        return num_integral / den_integral if den_integral != 0 else np.nan\n\n    all_results = []\n    lambdas = [0.90, 0.99, 0.999, 0.9999]\n\n    for alpha, p in test_cases:\n        I_vals = [calculate_average(l, alpha, p) for l in lambdas]\n        \n        diffs = [abs(I_vals[i+1] - I_vals[i]) for i in range(len(I_vals) - 1)]\n        S = max(diffs)\n        \n        # Result must be [I(0.99), I(0.999), S]\n        # I(0.99) is I_vals[1], I(0.999) is I_vals[2]\n        result_for_case = [I_vals[1], I_vals[2], S]\n        all_results.append(result_for_case)\n    \n    # Format the final output string exactly as requested\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]}]\" for r in all_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The precision of a free energy calculation is determined not only by the physical model but also by the statistical efficiency of the sampling strategy. Since thermodynamic integration is performed by running independent simulations at discrete $\\lambda$ windows, a crucial question arises: how should one distribute a finite computational budget across these windows to minimize the statistical error in the final result? This advanced practice guides you through solving this optimization problem, from deriving the optimal allocation scheme using Lagrange multipliers to implementing a practical algorithm that distributes sampling effort proportionally to the integrand's variance and the cost of sampling .",
            "id": "3454240",
            "problem": "Consider an alchemical transformation in molecular dynamics with a coupling parameter $\\lambda \\in [0,1]$. The Helmholtz free energy difference $\\Delta F$ between two end states is expressed via thermodynamic integration as an integral of an ensemble average along $\\lambda$. Using a fixed grid of windows at $\\{\\lambda_i\\}_{i=1}^M$ and deterministic quadrature weights $\\{w_i\\}_{i=1}^M$ (for example, the trapezoidal rule on an equally spaced grid), a Monte Carlo estimator of $\\Delta F$ is built from independent simulations at each window that produce sample means of the generalized force $X_i=\\partial U/\\partial \\lambda$ at $\\lambda_i$. Assume the following foundational bases:\n- The estimator of the free energy difference is $\\widehat{\\Delta F}=\\sum_{i=1}^M w_i \\overline{X}_i$, where $\\overline{X}_i$ is the sample mean of $X_i$ from $n_i$ independent draws at window $i$.\n- Under the Central Limit Theorem and independence across windows, the variance of the estimator obeys $\\mathrm{Var}[\\widehat{\\Delta F}]=\\sum_{i=1}^M w_i^2\\,\\mathrm{Var}[\\overline{X}_i]$, and $\\mathrm{Var}[\\overline{X}_i] \\approx \\sigma_i^2/n_i$, where $\\sigma_i^2$ denotes the per-sample variance of $X_i$ at $\\lambda_i$.\n- Each independent sample at window $i$ incurs a positive computational cost $c_i$ (in arbitrary but fixed cost units), so the total computational budget consumed is $\\sum_{i=1}^M c_i n_i$.\n\nYou must:\n1. Starting from these bases and no other shortcuts, pose and solve the constrained continuous optimization that minimizes the estimator variance subject to a fixed total cost constraint. Explicitly derive the stationarity condition and the proportionality of the optimal sampling effort across windows in terms of $\\{w_i\\}$, $\\{\\sigma_i^2\\}$, and $\\{c_i\\}$.\n2. Convert the continuous solution into an implementable integer allocation that exactly satisfies the cost constraint $\\sum_{i=1}^M c_i n_i=C_{\\mathrm{tot}}$, while respecting $n_i \\ge 1$ for all $i$. Your algorithm must:\n   - Identify any windows with $\\sigma_i^2=0$ and allocate only the minimum $n_i=1$ there unless needed to satisfy the exact cost constraint.\n   - Determine the continuous solution for the remaining active windows by enforcing the cost constraint and the minimum-sample constraint iteratively.\n   - Produce an integer allocation by rounding down the continuous allocation and greedily distributing the remaining budget to the windows that yield the largest marginal reduction of the objective per unit cost until the budget is exactly exhausted.\n3. Compute and report the predicted minimized variance $\\sum_{i=1}^M w_i^2 \\sigma_i^2 / n_i$ using your integer allocation.\n\nTest Suite. Implement a program that solves the above for the following four cases. All quantities are dimensionless, and all costs are abstract units. Use $n_i \\ge 1$ for all windows. The final output must aggregate the results of all test cases in a single line, printed as a list of lists, where each inner list contains the integer allocations followed by the minimized predicted variance as a floating-point number, in the order $[n_1,n_2,\\dots,n_M,V_{\\min}]$.\n\n- Case A (happy path):\n  - $M=4$, $\\sigma_i^2 = [4,1,9,16]$, $c_i=[1,1,2,3]$, $C_{\\mathrm{tot}}=50$, uniform quadrature weights $w_i=[1,1,1,1]$.\n\n- Case B (boundary with a zero-variance window):\n  - $M=3$, $\\sigma_i^2 = [1,0,4]$, $c_i=[2,3,1]$, $C_{\\mathrm{tot}}=20$, uniform quadrature weights $w_i=[1,1,1]$.\n\n- Case C (nonuniform quadrature weights, heterogeneous costs):\n  - $M=5$, $\\sigma_i^2 = [1,9,4,16,25]$, $c_i=[1,10,1,10,1]$, $C_{\\mathrm{tot}}=100$, equal-spacing trapezoidal weights on $[0,1]$: with $\\Delta \\lambda = 1/(M-1)$, the weights are $w_i = \\Delta \\lambda \\cdot [0.5,1,1,1,0.5]=[0.125,0.25,0.25,0.25,0.125]$.\n\n- Case D (minimal budget equals minimal feasible cost):\n  - $M=2$, $\\sigma_i^2 = [1,1]$, $c_i=[4,6]$, $C_{\\mathrm{tot}}=10$, uniform $w_i=[1,1]$.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list of lists, enclosed in square brackets, for example, $[[\\dots],[\\dots],[\\dots],[\\dots]]$. Each inner list must be the integer allocation for that case followed by the minimized predicted variance computed from that allocation, in the exact order of the test suite above. No input is to be read, and no other text should be printed. The angle unit is not applicable, and no physical units are involved. Percentages are not used anywhere; any ratios must be expressed as decimals if needed.",
            "solution": "The problem asks for the derivation and implementation of an optimal sampling strategy for thermodynamic integration, aiming to minimize the variance of the estimated free energy difference, $\\widehat{\\Delta F}$, subject to a fixed total computational cost. The solution is presented in three parts as requested: analytical derivation, description of the integer allocation algorithm, and the final computation of minimized variance.\n\n### **Problem Validation**\n\n**Step 1: Extract Givens**\n- **Objective Function (Variance to Minimize):** $V \\equiv \\mathrm{Var}[\\widehat{\\Delta F}] = \\sum_{i=1}^M w_i^2\\,\\mathrm{Var}[\\overline{X}_i] \\approx \\sum_{i=1}^M \\frac{w_i^2 \\sigma_i^2}{n_i}$, where $n_i$ is the number of samples at window $i$.\n- **Constraint (Total Cost):** $C = \\sum_{i=1}^M c_i n_i = C_{\\mathrm{tot}}$.\n- **Constraint (Minimum Samples):** $n_i \\ge 1$ for all $i \\in \\{1, \\dots, M\\}$.\n- **Variables:**\n    - $M$: Number of windows.\n    - $\\{\\lambda_i\\}_{i=1}^M$: Grid of coupling parameter values.\n    - $\\{w_i\\}_{i=1}^M$: Deterministic quadrature weights.\n    - $\\{\\overline{X}_i\\}_{i=1}^M$: Sample means of the generalized force $\\partial U/\\partial \\lambda$ at $\\lambda_i$.\n    - $\\{n_i\\}_{i=1}^M$: Number of samples to be optimized for each window.\n    - $\\{\\sigma_i^2\\}_{i=1}^M$: Per-sample variance of $X_i$ at $\\lambda_i$.\n    - $\\{c_i\\}_{i=1}^M$: Computational cost per sample at window $i$.\n    - $C_{\\mathrm{tot}}$: Fixed total computational budget.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in statistical mechanics and computational science. The formula for the variance of the thermodynamic integration estimator is standard. The optimization of sample allocation to minimize variance under a cost constraint is a classic and fundamentally important problem in Monte Carlo simulations.\n- **Well-Posed:** The problem defines a clear objective function (a convex function of $n_i$) and a linear equality constraint, along with boundary conditions ($n_i \\ge 1$). This structure typically leads to a unique, stable, and meaningful solution.\n- **Objective:** The problem is stated using precise mathematical and scientific language, free of ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-defined constrained optimization problem in a relevant scientific context.\n\n### **Part 1: Analytical Derivation of the Optimal Continuous Allocation**\n\nWe seek to minimize the estimator variance, $V(\\{n_i\\}) = \\sum_{i=1}^M \\frac{w_i^2 \\sigma_i^2}{n_i}$, subject to the constraint that the total cost is fixed, $\\sum_{i=1}^M c_i n_i = C_{\\mathrm{tot}}$. We first treat the sample counts $n_i$ as continuous variables. This constrained optimization problem is ideally suited for the method of Lagrange multipliers.\n\nThe Lagrangian function $\\mathcal{L}$ is constructed as:\n$$\n\\mathcal{L}(\\{n_i\\}, \\mu) = \\sum_{i=1}^M \\frac{w_i^2 \\sigma_i^2}{n_i} + \\mu \\left( \\left( \\sum_{i=1}^M c_i n_i \\right) - C_{\\mathrm{tot}} \\right)\n$$\nwhere $\\mu$ is the Lagrange multiplier associated with the cost constraint.\n\nFor an optimal allocation, the gradient of the Lagrangian with respect to each $n_k$ must be zero. This gives the stationarity conditions for $k=1, \\dots, M$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial n_k} = \\frac{\\partial}{\\partial n_k} \\left( \\frac{w_k^2 \\sigma_k^2}{n_k} \\right) + \\mu c_k = 0\n$$\n$$\n-\\frac{w_k^2 \\sigma_k^2}{n_k^2} + \\mu c_k = 0\n$$\nAssuming $\\sigma_k > 0$, we can rearrange this equation to solve for $n_k$:\n$$\n\\mu c_k n_k^2 = w_k^2 \\sigma_k^2 \\implies n_k^2 = \\frac{w_k^2 \\sigma_k^2}{\\mu c_k} \\implies n_k = \\frac{|w_k| \\sigma_k}{\\sqrt{\\mu c_k}}\n$$\nThis equation reveals the proportionality of the optimal sampling effort: the number of samples $n_k$ is directly proportional to the product of the magnitude of the quadrature weight $|w_k|$ and the standard deviation of the observable $\\sigma_k$, and inversely proportional to the square root of the cost per sample $\\sqrt{c_k}$.\n\nTo find the value of the constant of proportionality (which depends on $\\mu$), we substitute this expression for $n_k$ back into the cost constraint equation:\n$$\n\\sum_{i=1}^M c_i n_i = \\sum_{i=1}^M c_i \\left( \\frac{|w_i| \\sigma_i}{\\sqrt{\\mu c_i}} \\right) = C_{\\mathrm{tot}}\n$$\n$$\n\\frac{1}{\\sqrt{\\mu}} \\sum_{i=1}^M \\sqrt{c_i} |w_i| \\sigma_i = C_{\\mathrm{tot}}\n$$\nSolving for the term containing the multiplier:\n$$\n\\frac{1}{\\sqrt{\\mu}} = \\frac{C_{\\mathrm{tot}}}{\\sum_{j=1}^M \\sqrt{c_j} |w_j| \\sigma_j}\n$$\nFinally, we substitute this back into the expression for $n_k$ to obtain the optimal continuous allocation in terms of known quantities:\n$$\nn_k^{\\mathrm{opt}} = \\frac{|w_k| \\sigma_k}{\\sqrt{c_k}} \\left( \\frac{C_{\\mathrm{tot}}}{\\sum_{j=1}^M \\sqrt{c_j} |w_j| \\sigma_j} \\right)\n$$\nThis formula provides the optimal continuous number of samples for each window, assuming no lower bound on $n_k$ and $\\sigma_k>0$ for all $k$.\n\n### **Part 2: Algorithm for Integer Allocation**\n\nThe continuous solution must be converted into a practical integer allocation $\\{n_i\\}$ that satisfies $n_i \\ge 1$ for all $i$ and exactly meets the budget, $\\sum_i c_i n_i = C_{\\mathrm{tot}}$. The following algorithm achieves this.\n\n1.  **Handle Zero-Variance Windows:** For any window $i$ where the variance $\\sigma_i^2 = 0$, the corresponding term in the total variance is already zero. Sampling beyond the minimum is wasteful. Therefore, for each such window $i$, we set its allocation to the minimum required, $n_i=1$. The cost incurred by these windows is subtracted from the total budget $C_{\\mathrm{tot}}$. These windows are then removed from further optimization.\n\n2.  **Iterative Continuous Allocation for Active Windows:** For the remaining \"active\" windows (where $\\sigma_i > 0$), we must find a continuous allocation that respects the $n_i \\ge 1$ constraint. An iterative procedure is employed:\n    a. Using the current remaining budget and the set of currently active windows, compute the optimal continuous allocation $\\{n_i^{\\mathrm{cont}}\\}$ using the formula derived in Part 1.\n    b. Identify any windows where $n_i^{\\mathrm{cont}} < 1$. Such a result indicates that, under the model, the optimal allocation for this window is less than one sample. Since we must take at least one sample, we fix the allocation for these \"under-sampled\" windows to $n_i=1$.\n    c. These newly fixed windows are removed from the active set, and their cost ($c_i \\cdot 1$) is subtracted from the remaining budget.\n    d. The process (steps a-c) is repeated with the reduced active set and budget until a continuous allocation is found where all $n_i^{\\mathrm{cont}} \\ge 1$. This yields the final continuous plan for the remaining active windows.\n\n3.  **Integer Truncation and Greedy Distribution:** At this stage, we have a final (possibly mixed) allocation: $n_i=1$ for zero-variance and under-sampled windows, and a real-valued $n_i^{\\mathrm{cont}} \\ge 1$ for all others.\n    a. **Truncation:** An initial integer allocation is obtained by taking the floor of the continuous values: $n_i^{\\mathrm{int}} = \\lfloor n_i^{\\mathrm{cont}} \\rfloor$ for the active windows. The allocation for previously fixed windows remains $1$.\n    b. **Budget Remainder:** Calculate the cost of this initial integer allocation and find the remaining budget, $C_{\\mathrm{rem}} = C_{\\mathrm{tot}} - \\sum_i c_i n_i^{\\mathrm{int}}$.\n    c. **Greedy Allocation:** The remaining budget $C_{\\mathrm{rem}}$ is distributed one cost unit at a time. In each step, we identify the window that provides the largest marginal reduction in variance per unit of cost. The variance reduction for adding one sample to window $k$ (from $n_k$ to $n_k+1$) is $\\Delta V_k = \\frac{w_k^2 \\sigma_k^2}{n_k} - \\frac{w_k^2 \\sigma_k^2}{n_k+1} = \\frac{w_k^2 \\sigma_k^2}{n_k(n_k+1)}$. The reduction per unit cost is $g_k = \\Delta V_k / c_k$.\n    The algorithm proceeds as follows: while $C_{\\mathrm{rem}} > 0$:\n        i.  For all windows $i$ with $c_i \\le C_{\\mathrm{rem}}$, calculate the marginal gain $g_i$.\n        ii. Find the window $k$ with the maximum gain $g_k$.\n        iii. Increment its sample count, $n_k \\leftarrow n_k + 1$.\n        iv. Decrement the remaining budget, $C_{\\mathrm{rem}} \\leftarrow C_{\\mathrm{rem}} - c_k$.\n    This greedy process continues until the budget is exactly exhausted ($C_{\\mathrm{rem}}=0$). The resulting $\\{n_i\\}$ is the final integer allocation.\n\n### **Part 3: Final Variance Calculation**\n\nWith the final integer allocation $\\{n_i^{\\mathrm{final}}\\}$ determined, the minimized predicted variance is computed by substituting these values back into the objective function:\n$$\nV_{\\min} = \\sum_{i=1}^M \\frac{w_i^2 \\sigma_i^2}{n_i^{\\mathrm{final}}}\n$$\n\nThis completes the theoretical and algorithmic description of the solution.",
            "answer": "```python\nimport numpy as np\n\ndef solve_allocation(M, sigma_sq, c, C_tot, w):\n    \"\"\"\n    Computes the optimal integer sample allocation to minimize estimator variance\n    subject to a total cost constraint.\n    \"\"\"\n    sigma_sq = np.array(sigma_sq, dtype=float)\n    c = np.array(c, dtype=float)\n    w = np.array(w, dtype=float)\n    sigma = np.sqrt(sigma_sq)\n    \n    n_final = np.zeros(M, dtype=int)\n    is_fixed = np.zeros(M, dtype=bool)\n    C_rem = float(C_tot)\n\n    # Step 1: Handle zero-variance windows\n    zero_var_indices = np.where(sigma == 0)[0]\n    for i in zero_var_indices:\n        n_final[i] = 1\n        C_rem -= c[i]\n        is_fixed[i] = True\n\n    if C_rem < 0:\n        # This case suggests the budget is insufficient even for minimal sampling.\n        # The problem constraints should prevent this. We return a placeholder\n        # for malformed inputs, though test cases won't trigger this.\n        # Here we just re-allocate to satisfy cost if possible.\n        n_final.fill(1)\n        # In a more complex scenario, might need to increase n_i on zero-var windows if others cost more.\n        # The greedy part below can handle distributing any negative remainder if needed.\n        # But for the given problem, this branch is not expected to be taken.\n        pass\n\n    # Step 2: Iterative continuous allocation for active windows\n    n_continuous = np.zeros(M)\n    while True:\n        active_indices = np.where(~is_fixed)[0]\n        if len(active_indices) == 0:\n            break\n\n        min_cost_for_active = np.sum(c[active_indices])\n        if C_rem < min_cost_for_active:\n             # If budget is not enough for n_i=1 on all remaining, we fix all to 1\n             # and let the greedy algorithm handle the (negative) remainder.\n             # This can happen if C_tot is very small.\n             n_final[active_indices] = 1\n             C_rem -= min_cost_for_active\n             is_fixed[active_indices] = True\n             break\n\n        # Calculate sum for denominator\n        S = np.sum(np.sqrt(c[active_indices]) * np.abs(w[active_indices]) * sigma[active_indices])\n\n        if S == 0: # All remaining active windows have sigma=0, already handled\n            break\n\n        # Calculate continuous allocation for current active set\n        n_cont_active = (np.abs(w[active_indices]) * sigma[active_indices] / np.sqrt(c[active_indices])) * (C_rem / S)\n        \n        # Identify under-sampled windows\n        under_sampled_mask = n_cont_active < 1.0\n        under_sampled_indices = active_indices[under_sampled_mask]\n\n        if len(under_sampled_indices) == 0:\n            # All remaining windows are well-sampled, store continuous result and break\n            n_continuous[active_indices] = n_cont_active\n            break\n        else:\n            # Fix under-sampled windows to n=1 and iterate\n            for i in under_sampled_indices:\n                n_final[i] = 1\n                C_rem -= c[i]\n                is_fixed[i] = True\n                \n    # Step 3: Integer truncation and greedy distribution\n    active_indices = np.where(~is_fixed)[0]\n    if len(active_indices) > 0:\n        n_final[active_indices] = np.floor(n_continuous[active_indices]).astype(int)\n\n    C_used = np.sum(c * n_final)\n    C_leftover = C_tot - C_used\n\n    # Greedy allocation of remaining budget\n    # We use a small tolerance for floating point comparisons of the budget\n    while C_leftover > 1e-9:\n        gains = np.zeros(M)\n        eligible_mask = (c <= C_leftover) & (sigma > 0) # Only consider affordable increments and non-zero sigma\n        \n        if not np.any(eligible_mask):\n            break # No affordable increments left\n\n        eligible_indices = np.where(eligible_mask)[0]\n        \n        # Calculate marginal gain per unit cost\n        current_n = n_final[eligible_indices]\n        gains[eligible_indices] = (w[eligible_indices]**2 * sigma_sq[eligible_indices]) / \\\n                                  (c[eligible_indices] * current_n * (current_n + 1))\n        \n        best_idx = np.argmax(gains)\n        \n        n_final[best_idx] += 1\n        C_leftover -= c[best_idx]\n\n    # Final variance calculation\n    # To avoid division by zero if n_final has a 0 (should not happen with n_i >= 1)\n    non_zero_n = np.maximum(n_final, 1)\n    minimized_variance = np.sum((w**2 * sigma_sq) / non_zero_n)\n    \n    result = list(n_final) + [minimized_variance]\n    return result\n\ndef solve():\n    test_cases = [\n        # Case A\n        {'M': 4, 'sigma_sq': [4, 1, 9, 16], 'c': [1, 1, 2, 3], 'C_tot': 50, 'w': [1, 1, 1, 1]},\n        # Case B\n        {'M': 3, 'sigma_sq': [1, 0, 4], 'c': [2, 3, 1], 'C_tot': 20, 'w': [1, 1, 1]},\n        # Case C\n        {'M': 5, 'sigma_sq': [1, 9, 4, 16, 25], 'c': [1, 10, 1, 10, 1], 'C_tot': 100, \n         'w': [0.125, 0.25, 0.25, 0.25, 0.125]},\n        # Case D\n        {'M': 2, 'sigma_sq': [1, 1], 'c': [4, 6], 'C_tot': 10, 'w': [1, 1]}\n    ]\n\n    results = []\n    for case in test_cases:\n        res = solve_allocation(**case)\n        results.append(str(res))\n\n    # Format output as a list of lists string\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}