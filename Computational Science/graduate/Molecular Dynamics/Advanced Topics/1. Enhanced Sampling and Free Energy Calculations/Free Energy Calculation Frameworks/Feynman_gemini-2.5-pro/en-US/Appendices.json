{
    "hands_on_practices": [
        {
            "introduction": "Thermodynamic Integration (TI) is a powerful and widely used method for computing free energy differences. It relies on designing a non-physical path, parameterized by a coupling parameter $\\lambda$, that connects a known initial state to a final state of interest. This first exercise will guide you through an exact, analytical application of the TI formula for a simple harmonic system, making the abstract theory concrete and providing a foundational understanding of how the ensemble-averaged derivative of the potential energy is integrated to yield a free energy difference .",
            "id": "3414384",
            "problem": "Consider a one-dimensional system in the canonical ensemble at temperature $T$, with inverse thermal energy $\\beta = 1/(k_{B} T)$, where $k_{B}$ is the Boltzmann constant. A coupling parameter $\\lambda \\in [0,1]$ defines a mixing potential\n$$\nU_{\\lambda}(x) = \\frac{k}{2}\\big[(1-\\lambda)x^{2} + \\lambda (x-a)^{2}\\big],\n$$\nwith stiffness $k>0$ and displacement $a \\in \\mathbb{R}$. The canonical ensemble average at fixed $\\lambda$ is defined by\n$$\n\\langle A\\rangle_{\\lambda} = \\frac{\\int_{-\\infty}^{\\infty} A(x)\\exp\\big(-\\beta U_{\\lambda}(x)\\big)\\,\\mathrm{d}x}{\\int_{-\\infty}^{\\infty} \\exp\\big(-\\beta U_{\\lambda}(x)\\big)\\,\\mathrm{d}x}.\n$$\nUsing only these definitions, first compute the exact analytical expression for the ensemble average $\\left\\langle \\frac{\\partial U_{\\lambda}}{\\partial \\lambda}\\right\\rangle_{\\lambda}$ for each $\\lambda \\in [0,1]$. Then perform the thermodynamic integration,\n$$\n\\Delta F = \\int_{0}^{1} \\left\\langle \\frac{\\partial U_{\\lambda}}{\\partial \\lambda}\\right\\rangle_{\\lambda}\\,\\mathrm{d}\\lambda,\n$$\nto obtain the free energy difference $\\Delta F = F_{1} - F_{0}$ between the states at $\\lambda=1$ and $\\lambda=0$. Assume the coordinate $x$ spans the entire real line and there are no boundary effects. Express the final free energy difference in Joules. No rounding is necessary; provide the exact value as your final answer.",
            "solution": "We begin from the canonical ensemble definition for a system at temperature $T$ with inverse thermal energy $\\beta = 1/(k_B T)$. The potential energy is\n$$\nU_{\\lambda}(x) = \\frac{k}{2}\\left[(1-\\lambda)x^{2} + \\lambda (x-a)^{2}\\right].\n$$\nWe need $\\left\\langle \\frac{\\partial U_{\\lambda}}{\\partial \\lambda}\\right\\rangle_{\\lambda}$. First compute the derivative of the potential with respect to $\\lambda$:\n$$\n\\frac{\\partial U_{\\lambda}}{\\partial \\lambda} = \\frac{k}{2}\\left[-x^{2} + (x-a)^{2}\\right] = \\frac{k}{2}\\left[-x^{2} + x^{2} - 2 a x + a^{2}\\right] = \\frac{k}{2}\\left(a^{2} - 2 a x\\right).\n$$\nTherefore,\n$$\n\\left\\langle \\frac{\\partial U_{\\lambda}}{\\partial \\lambda}\\right\\rangle_{\\lambda} = \\frac{k}{2}\\left(a^{2} - 2 a \\langle x\\rangle_{\\lambda}\\right).\n$$\nTo evaluate $\\langle x\\rangle_{\\lambda}$, we analyze the $\\lambda$-dependent quadratic potential. Expand and complete the square:\n$$\nU_{\\lambda}(x) = \\frac{k}{2}\\left[x^{2} - 2 \\lambda a x + \\lambda a^{2}\\right] = \\frac{k}{2}\\left[(x - \\lambda a)^{2} + \\lambda(1-\\lambda)a^{2}\\right].\n$$\nThe Boltzmann weight then factorizes as\n$$\n\\exp\\big(-\\beta U_{\\lambda}(x)\\big) = \\exp\\left(-\\beta \\frac{k}{2} (x - \\lambda a)^{2}\\right)\\exp\\left(-\\beta \\frac{k}{2}\\lambda(1-\\lambda)a^{2}\\right).\n$$\nThe factor $\\exp\\left(-\\beta \\frac{k}{2}\\lambda(1-\\lambda)a^{2}\\right)$ is independent of $x$ and cancels between numerator and denominator in any canonical average. The $x$-dependent part is a Gaussian centered at $x = \\lambda a$ with curvature $k$. By symmetry of the Gaussian,\n$$\n\\langle x\\rangle_{\\lambda} = \\lambda a.\n$$\nSubstituting this into the expression for the averaged derivative yields\n$$\n\\left\\langle \\frac{\\partial U_{\\lambda}}{\\partial \\lambda}\\right\\rangle_{\\lambda} = \\frac{k}{2}\\left(a^{2} - 2 a \\cdot \\lambda a\\right) = \\frac{k}{2} a^{2}(1 - 2\\lambda).\n$$\nWe now perform the thermodynamic integration from $\\lambda=0$ to $\\lambda=1$ to obtain the free energy difference $\\Delta F = F_1 - F_0$:\n$$\n\\Delta F = \\int_{0}^{1} \\left\\langle \\frac{\\partial U_{\\lambda}}{\\partial \\lambda}\\right\\rangle_{\\lambda}\\,\\mathrm{d}\\lambda = \\int_{0}^{1} \\frac{k}{2} a^{2}(1 - 2\\lambda)\\,\\mathrm{d}\\lambda.\n$$\nCarrying out the integral,\n$$\n\\Delta F = \\frac{k}{2} a^{2}\\int_{0}^{1} (1 - 2\\lambda)\\,\\mathrm{d}\\lambda = \\frac{k}{2} a^{2}\\left[\\lambda - \\lambda^{2}\\right]_{0}^{1} = \\frac{k}{2} a^{2}(1 - 1) = 0.\n$$\nThus, the free energy difference between the two states is exactly zero. This outcome is consistent with the physical interpretation: the two endpoint potentials, $U_{0}(x) = \\frac{k}{2}x^{2}$ and $U_{1}(x) = \\frac{k}{2}(x-a)^{2}$, differ only by a translation in $x$, and over the entire real line such a translation does not change the partition function, hence no change in free energy.\n\nAs an internal consistency check, we can also compute the $\\lambda$-dependent free energy directly from the partition function. The configurational partition function factorizes as\n$$\nZ_{\\lambda}^{\\text{conf}} = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\beta \\frac{k}{2}(x - \\lambda a)^{2}\\right)\\,\\mathrm{d}x \\times \\exp\\left(-\\beta \\frac{k}{2}\\lambda(1-\\lambda)a^{2}\\right),\n$$\nwhere the Gaussian integral is independent of $\\lambda$, so\n$$\nF_{\\lambda} = -\\frac{1}{\\beta}\\ln Z_{\\lambda}^{\\text{conf}} = \\text{const} + \\frac{k}{2}\\lambda(1-\\lambda)a^{2}.\n$$\nTherefore,\n$$\nF_{1} - F_{0} = \\frac{k}{2}\\left[1\\cdot 0 - 0\\cdot 1\\right]a^{2} = 0,\n$$\nin agreement with the thermodynamic integration result. The final answer is an exact zero in Joules, with no rounding required.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "While the concept of Thermodynamic Integration is elegant, its application to real molecular systems presents practical challenges, most notably the \"endpoint singularity.\" This issue arises when particles are created or annihilated, leading to infinite potential energies and a divergent TI integrand. This practice explores the standard solution: the use of soft-core potentials that smooth out these singularities, challenging you to derive the integrand for such a potential and analyze how its parameters ensure a well-behaved and convergent free energy calculation .",
            "id": "3414367",
            "problem": "In molecular dynamics free energy calculations using Thermodynamic Integration (TI), the integrand is the ensemble average of the partial derivative of the Hamiltonian with respect to a coupling parameter. Consider a soft-core Lennard–Jones potential, introduced to regularize singularities as interactions are turned off and on. Let the configuration-space contribution to the Hamiltonian be a potential defined by\n$$U_{\\text{sc}}(\\lambda,r) = \\lambda \\, U_{\\text{LJ}}(r_{\\text{sc}}),$$\nwith the soft-core mapping given by\n$$r_{\\text{sc}}^{6} = r^{6} + \\alpha \\left(1 - \\lambda\\right)^{m},$$\nwhere $r > 0$, $\\lambda \\in [0,1]$, $\\alpha > 0$, and $m \\geq 1$. The underlying Lennard–Jones potential is\n$$U_{\\text{LJ}}(r) = 4 \\epsilon \\left[ \\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^{6} \\right],$$\nwith parameters $\\epsilon > 0$ and $\\sigma > 0$.\n\nStarting from fundamental calculus and the definitions above, derive a closed-form analytic expression for the partial derivative $\\partial U_{\\text{sc}}/\\partial \\lambda$ at fixed $r$. Express your final result solely in terms of $r$, $\\lambda$, $\\epsilon$, $\\sigma$, $\\alpha$, and $m$, and, where helpful, powers of $r^{6} + \\alpha (1-\\lambda)^{m}$. Then, using the derived expression as your basis, reason about how the choices of $\\alpha$ and $m$ influence the boundedness and smoothness of the TI integrand $\\partial U_{\\text{sc}}/\\partial \\lambda$ near $\\lambda \\to 0^{+}$ and $\\lambda \\to 1^{-}$, especially for small $r$.\n\nYour final answer must be the single analytic expression for $\\partial U_{\\text{sc}}/\\partial \\lambda$. Do not include any discussion in the final answer. No numerical evaluation is required, and no rounding is needed. Express the expression without units.",
            "solution": "The objective is to derive the partial derivative $\\frac{\\partial U_{\\text{sc}}}{\\partial \\lambda}$ and analyze its behavior at the endpoints of the coupling parameter $\\lambda$. The soft-core potential is given by\n$$U_{\\text{sc}}(\\lambda,r) = \\lambda \\, U_{\\text{LJ}}(r_{\\text{sc}})$$\nwhere the Lennard-Jones potential is $U_{\\text{LJ}}(r) = 4 \\epsilon \\left[ \\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^{6} \\right]$ and the soft-core distance $r_{\\text{sc}}$ is defined through the relation $r_{\\text{sc}}^{6} = r^{6} + \\alpha \\left(1 - \\lambda\\right)^{m}$.\n\nTo simplify the derivatives, let us define an intermediate variable $S(\\lambda) = r_{\\text{sc}}^{6} = r^{6} + \\alpha \\left(1 - \\lambda\\right)^{m}$. The Lennard-Jones potential can then be written as a function of $S$:\n$$U_{\\text{LJ}}(S) = 4 \\epsilon \\left[ \\sigma^{12} S^{-2} - \\sigma^{6} S^{-1} \\right]$$\nThe soft-core potential is then $U_{\\text{sc}}(\\lambda, r) = \\lambda \\, U_{\\text{LJ}}(S(\\lambda))$. We are asked to find the partial derivative with respect to $\\lambda$ at fixed $r$. We apply the product rule for differentiation:\n$$\\frac{\\partial U_{\\text{sc}}}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left( \\lambda \\cdot U_{\\text{LJ}}(S) \\right) = \\frac{\\partial \\lambda}{\\partial \\lambda} U_{\\text{LJ}}(S) + \\lambda \\frac{\\partial U_{\\text{LJ}}(S)}{\\partial \\lambda}$$\n$$\\frac{\\partial U_{\\text{sc}}}{\\partial \\lambda} = U_{\\text{LJ}}(S) + \\lambda \\frac{\\partial U_{\\text{LJ}}(S)}{\\partial \\lambda}$$\nThe second term requires the chain rule, as $U_{\\text{LJ}}$ depends on $\\lambda$ through $S$:\n$$\\frac{\\partial U_{\\text{LJ}}(S)}{\\partial \\lambda} = \\frac{d U_{\\text{LJ}}}{dS} \\frac{\\partial S}{\\partial \\lambda}$$\nLet's compute the two derivatives on the right-hand side. First, the derivative of $U_{\\text{LJ}}$ with respect to $S$:\n$$\\frac{d U_{\\text{LJ}}}{dS} = \\frac{d}{dS} \\left( 4 \\epsilon \\left[ \\sigma^{12} S^{-2} - \\sigma^{6} S^{-1} \\right] \\right) = 4 \\epsilon \\left[ \\sigma^{12} (-2 S^{-3}) - \\sigma^{6} (-1 S^{-2}) \\right] = 4 \\epsilon \\left[ -2\\sigma^{12} S^{-3} + \\sigma^{6} S^{-2} \\right]$$\nNext, the derivative of $S$ with respect to $\\lambda$:\n$$\\frac{\\partial S}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left( r^{6} + \\alpha (1 - \\lambda)^{m} \\right) = \\alpha \\cdot m(1-\\lambda)^{m-1} \\cdot (-1) = -m\\alpha(1-\\lambda)^{m-1}$$\nNow, we substitute these back into the expression for $\\frac{\\partial U_{\\text{sc}}}{\\partial \\lambda}$:\n$$\\frac{\\partial U_{\\text{sc}}}{\\partial \\lambda} = U_{\\text{LJ}}(S) + \\lambda \\left( 4 \\epsilon \\left[ -2\\sigma^{12} S^{-3} + \\sigma^{6} S^{-2} \\right] \\right) \\left( -m\\alpha(1-\\lambda)^{m-1} \\right)$$\nSubstituting the expression for $U_{\\text{LJ}}(S)$ and distributing the terms:\n$$\\frac{\\partial U_{\\text{sc}}}{\\partial \\lambda} = 4 \\epsilon \\left[ \\frac{\\sigma^{12}}{S^{2}} - \\frac{\\sigma^{6}}{S} \\right] + 4 \\epsilon \\lambda m \\alpha (1-\\lambda)^{m-1} \\left[ 2\\sigma^{12} S^{-3} - \\sigma^{6} S^{-2} \\right]$$\nTo obtain a more structured expression, we can group the terms by powers of $\\sigma$:\n$$\\frac{\\partial U_{\\text{sc}}}{\\partial \\lambda} = 4 \\epsilon \\sigma^{12} \\left[ \\frac{1}{S^{2}} + \\frac{2\\lambda m \\alpha (1-\\lambda)^{m-1}}{S^{3}} \\right] - 4 \\epsilon \\sigma^{6} \\left[ \\frac{1}{S} + \\frac{\\lambda m \\alpha (1-\\lambda)^{m-1}}{S^{2}} \\right]$$\nCombining the fractions within each bracket:\n$$\\frac{\\partial U_{\\text{sc}}}{\\partial \\lambda} = 4 \\epsilon \\sigma^{12} \\left[ \\frac{S + 2\\lambda m \\alpha (1-\\lambda)^{m-1}}{S^{3}} \\right] - 4 \\epsilon \\sigma^{6} \\left[ \\frac{S + \\lambda m \\alpha (1-\\lambda)^{m-1}}{S^{2}} \\right]$$\nFinally, substituting back $S = r^{6} + \\alpha (1-\\lambda)^{m}$ gives the desired closed-form expression:\n$$\\frac{\\partial U_{\\text{sc}}}{\\partial \\lambda} = 4 \\epsilon \\sigma^{12} \\frac{r^{6} + \\alpha(1-\\lambda)^{m} + 2\\lambda m \\alpha (1-\\lambda)^{m-1}}{\\left(r^{6} + \\alpha (1-\\lambda)^{m}\\right)^{3}} - 4 \\epsilon \\sigma^{6} \\frac{r^{6} + \\alpha(1-\\lambda)^{m} + \\lambda m \\alpha (1-\\lambda)^{m-1}}{\\left(r^{6} + \\alpha (1-\\lambda)^{m}\\right)^{2}}$$\nThis is the analytic expression for the TI integrand.\n\nNow we analyze the influence of $\\alpha$ and $m$ on the boundedness and smoothness of this integrand, denoted $I(\\lambda, r) = \\frac{\\partial U_{\\text{sc}}}{\\partial \\lambda}$, near the endpoints $\\lambda \\to 0^{+}$ and $\\lambda \\to 1^{-}$. The key issue in TI is ensuring the ensemble average $\\langle I(\\lambda, r) \\rangle_{\\lambda}$ is well-behaved across the entire integration path. This requires that $I(\\lambda, r)$ itself remains finite for configurations $r$ that are accessible at a given $\\lambda$.\n\nAnalysis near $\\lambda \\to 1^{-}$:\nAs $\\lambda \\to 1^{-}$, the term $(1-\\lambda) \\to 0$. Consequently, $r_{\\text{sc}}^{6} = r^{6} + \\alpha(1-\\lambda)^{m} \\to r^{6}$. The potential $U_{\\text{sc}}$ smoothly transforms into the standard Lennard-Jones potential $U_{\\text{LJ}}(r)$. The integrand $I(\\lambda, r)$ approaches a limiting function. For $m>1$, $\\lim_{\\lambda \\to 1^{-}} I(\\lambda,r) = U_{\\text{LJ}}(r)$. For $m=1$, the limit is a combination of $U_{\\text{LJ}}(r)$ and its derivative with respect to $r^{6}$. In either case, as $r \\to 0$, $I(1,r)$ is unbounded, diverging like $r^{-12}$ or faster. This is not a practical problem because at $\\lambda \\approx 1$, the potential energy is highly repulsive at small $r$, so the Boltzmann factor $\\exp(-U_{\\text{sc}}/k_B T)$ ensures that configurations with small $r$ are not sampled. Thus, the ensemble average $\\langle I(\\lambda, r) \\rangle_{\\lambda \\to 1^{-}}$ remains finite. The choice of $\\alpha$ or $m$ does not alter the fundamental divergence of the integrand itself at $r=0$ at this endpoint.\n\nAnalysis near $\\lambda \\to 0^{+}$:\nThis is the critical endpoint. At $\\lambda=0$, the particles are non-interacting, so configurations with small inter-particle distances, including $r \\to 0$, are physically accessible. For the TI integral to be numerically tractable, the integrand must be bounded for all $r$ as $\\lambda \\to 0^{+}$.\nLet's examine the limit of our derived expression as $\\lambda \\to 0^{+}$. The term $\\lambda$ multiplies the derivative part of the expression, so we focus on the leading term:\n$$\\lim_{\\lambda \\to 0^{+}} I(\\lambda,r) = \\lim_{\\lambda \\to 0^{+}} U_{\\text{LJ}}(r_{\\text{sc}}) = U_{\\text{LJ}}(r_{\\text{sc}}|_{\\lambda=0})$$\nAt $\\lambda=0$, $S(0) = r_{\\text{sc}}^{6}|_{\\lambda=0} = r^{6} + \\alpha(1-0)^{m} = r^{6} + \\alpha$. So the limit is:\n$$\\lim_{\\lambda \\to 0^{+}} I(\\lambda,r) = 4 \\epsilon \\left[ \\frac{\\sigma^{12}}{(r^{6}+\\alpha)^{2}} - \\frac{\\sigma^{6}}{r^{6}+\\alpha} \\right]$$\nThis expression is finite for all $r \\geq 0$ because $\\alpha > 0$. Even when $r=0$, the denominator becomes $\\alpha$ or $\\alpha^{2}$, preventing a singularity. The value of the integrand at the critical point $(\\lambda, r) = (0, 0)$ is:\n$$\\lim_{r \\to 0} \\left( \\lim_{\\lambda \\to 0^{+}} I(\\lambda,r) \\right) = 4 \\epsilon \\left[ \\frac{\\sigma^{12}}{\\alpha^{2}} - \\frac{\\sigma^{6}}{\\alpha} \\right]$$\nThis is a finite value. Therefore, the soft-core potential successfully regularizes the singularity.\nThe role of the parameters is as follows:\n- $\\alpha$: The parameter $\\alpha > 0$ is essential for ensuring the boundedness of the integrand at $\\lambda=0$. It sets the value of the \"floor\" of the soft-core radius, $r_{\\text{sc}} \\geq \\alpha^{1/6}$, preventing the potential and its derivative from diverging even at $r=0$. It directly controls the magnitude of the integrand at this endpoint.\n- $m$: The parameter $m \\geq 1$ does not affect the value of the integrand at $\\lambda=0$. It influences the \"shape\" of the integrand curve as a function of $\\lambda$. For example, the smoothness (higher-order derivatives with respect to $\\lambda$) at the endpoints is affected by the choice of $m$. However, for the crucial task of ensuring the integrand is bounded at $\\lambda=0$, any $m \\geq 1$ suffices. The derivatives with respect to $\\lambda$ are also finite at $\\lambda=0$, ensuring the integrand is a smooth function near this endpoint.",
            "answer": "$$\n\\boxed{\n4 \\epsilon \\sigma^{12} \\frac{r^{6} + \\alpha(1-\\lambda)^{m} + 2 \\lambda m \\alpha (1-\\lambda)^{m-1}}{\\left(r^{6} + \\alpha (1-\\lambda)^{m}\\right)^{3}} - 4 \\epsilon \\sigma^{6} \\frac{r^{6} + \\alpha(1-\\lambda)^{m} + \\lambda m \\alpha (1-\\lambda)^{m-1}}{\\left(r^{6} + \\alpha (1-\\lambda)^{m}\\right)^{2}}\n}\n$$"
        },
        {
            "introduction": "Beyond equilibrium methods like TI, free energy differences can also be determined from explicitly non-equilibrium processes via the remarkable Jarzynski equality. This principle states that the equilibrium free energy difference $\\Delta F$ can be recovered by performing an exponential average over the work $W$ performed during irreversible transformations. This exercise delves into the practical aspects of this framework, where you will optimize a non-equilibrium protocol by balancing the systematic bias from fast switching with the statistical variance from a finite number of measurements, a core trade-off in modern non-equilibrium simulation .",
            "id": "3414390",
            "problem": "Consider a one-dimensional overdamped Langevin particle of coordinate $x(t)$ in a harmonic potential whose minimum (trap center) $\\lambda(t)$ is translated from $0$ to $L$ in a total protocol time $\\tau$, with a constant translation speed $v=L/\\tau$. The dynamics obeys $ \\gamma \\,\\dot{x}(t) = -\\kappa \\left[x(t)-\\lambda(t)\\right] + \\xi(t)$, where $\\gamma$ is the friction coefficient, $\\kappa$ is the trap stiffness, and $\\xi(t)$ is a zero-mean Gaussian random force with covariance $\\langle \\xi(t)\\xi(t')\\rangle = 2 \\gamma k_B T \\,\\delta(t-t')$. Work performed on the system along the protocol is the time integral of generalized force times the protocol rate. The Helmholtz free energy is defined by $F(\\lambda) = -\\beta^{-1} \\ln Z(\\lambda)$, with inverse temperature $\\beta = (k_B T)^{-1}$ and partition function $Z(\\lambda)$.\n\nYour task is to implement an optimizer for nonequilibrium switching protocols estimating the free energy difference between $\\lambda(0)=0$ and $\\lambda(\\tau)=L$ using an exponential work-based estimator. You must proceed from fundamental definitions as follows.\n\nDerivation requirements to guide algorithm design:\n- Begin from the definition of equilibrium free energy $F(\\lambda) = -\\beta^{-1}\\ln Z(\\lambda)$ and the microscopic definition of work as the path integral of force along the protocol.\n- Use the assumption of linear response for the overdamped harmonic system under constant-velocity dragging to argue that the work distribution is Gaussian for any fixed protocol time $\\tau$, with mean and variance related by a fluctuation-dissipation balance as a function of $\\tau$. Use $k_B T = \\beta^{-1}$ throughout. From this base, deduce an explicit dependence of the work variance on $\\tau$ for the dragged harmonic oscillator, of the form $\\sigma_W^2(\\tau) \\propto \\gamma L^2\\,\\beta^{-1}/\\tau$.\n- Using the above ingredients and the definition of the exponential work-based free-energy estimator formed from $M$ independent repeated nonequilibrium trajectories, derive an asymptotic large-$M$ expression for the mean-squared error as a function of $\\tau$ and $M$ that includes both variance and bias contributions. The derivation should combine the Gaussian-work assumption with a Taylor expansion (delta method) for the nonlinear estimator to obtain a closed-form expression that depends on $\\beta$, $\\gamma$, $L$, and $\\tau$.\n\nComputational cost model and optimization objective:\n- Each trajectory has a real-time cost proportional to its protocol duration. In addition, a fixed overhead time per trajectory, $t_{\\mathrm{over}}$, accounts for setup and equilibration. Given a total compute-time budget $B$, the number of trajectories $M$ and the protocol time $\\tau$ must satisfy $M(\\tau + t_{\\mathrm{over}}) \\le B$ with $M \\in \\mathbb{Z}_{\\ge 1}$ and $\\tau > 0$.\n- Under these constraints, choose $M$ and $\\tau$ to minimize the asymptotic mean-squared error you derived. Argue whether the optimal solution saturates the budget (i.e., achieves $M(\\tau + t_{\\mathrm{over}})=B$), and use this to reduce the search to a one-dimensional discrete optimization over $M$ with $\\tau$ determined by the budget.\n\nImplementation details:\n- Adopt reduced, dimensionless units throughout so that all quantities are unitless. Report all times and errors in these reduced units.\n- For the overdamped dragged harmonic oscillator, you may treat the unknown trap stiffness $\\kappa$ as absorbed into an effective constant that only appears through the combination consistent with linear response in your variance formula. Your implementation should use the explicit dependence of the work variance on $\\tau$ implied by your derivation for this model, expressed in terms of $\\beta$, $\\gamma$, and $L$ alone.\n- For numerical stability, ensure your code can handle very small and very large arguments to the exponential functions that appear in your asymptotic mean-squared error expression. You may freely choose numerically stable rearrangements as long as they are mathematically equivalent to the expression derived from first principles.\n\nTest suite:\nFor each parameter set $(\\beta, L, \\gamma, B, t_{\\mathrm{over}})$ below, compute the optimal protocol time $\\tau^\\star$, the optimal integer number of trajectories $M^\\star$, and the corresponding minimized asymptotic mean-squared error $\\mathrm{MSE}^\\star$.\n- Case $1$: $(\\beta, L, \\gamma, B, t_{\\mathrm{over}}) = (1.0, 1.0, 1.0, 100.0, 0.0)$.\n- Case $2$: $(\\beta, L, \\gamma, B, t_{\\mathrm{over}}) = (2.0, 2.0, 0.5, 10.0, 1.0)$.\n- Case $3$: $(\\beta, L, \\gamma, B, t_{\\mathrm{over}}) = (1.0, 0.5, 5.0, 1.5, 0.4)$.\n- Case $4$: $(\\beta, L, \\gamma, B, t_{\\mathrm{over}}) = (5.0, 1.0, 1.0, 20.0, 0.2)$.\n\nOutput format:\n- Your program must produce a single line containing a list with one entry per test case. Each entry must itself be a list of three values in the order $[\\tau^\\star, M^\\star, \\mathrm{MSE}^\\star]$.\n- Report $\\tau^\\star$ and $\\mathrm{MSE}^\\star$ as decimal floating-point numbers rounded to six decimal places, and $M^\\star$ as an integer. For example, an output with two cases could look like $[[1.234000,1,0.056789],[2.500000,7,0.012345]]$.",
            "solution": "The solution to this problem involves a multi-step derivation followed by a numerical optimization. We will first derive the expression for the Mean-Squared Error (MSE) of the free energy estimator from fundamental principles and then devise an algorithm to minimize this MSE under the given computational budget. We will use dimensionless units throughout, as specified.\n\nThe system is a one-dimensional particle in a harmonic potential $U(x, \\lambda) = \\frac{1}{2}\\kappa(x-\\lambda)^2$. The equilibrium Helmholtz free energy is $F(\\lambda) = -\\beta^{-1}\\ln Z(\\lambda)$, where $\\beta=(k_B T)^{-1}$ and $Z(\\lambda) = \\int_{-\\infty}^{\\infty} e^{-\\beta U(x,\\lambda)} dx$. This Gaussian integral evaluates to $Z(\\lambda) = \\sqrt{2\\pi/(\\beta\\kappa)}$, which is independent of the trap center $\\lambda$. Consequently, the free energy difference for any protocol that changes $\\lambda$ from $\\lambda(0)=0$ to $\\lambda(\\tau)=L$ is $\\Delta F = F(L) - F(0) = 0$.\n\nThe work done on the system during the protocol $\\lambda(t) = (L/\\tau)t$ is defined as $W = \\int_0^\\tau \\frac{\\partial U}{\\partial\\lambda} \\dot{\\lambda} dt$. Substituting the potential and protocol gives $W = \\int_0^\\tau [-\\kappa(x(t)-\\lambda(t))](L/\\tau) dt$. Since the Langevin equation is linear and the noise is Gaussian, the work $W$ is a Gaussian random variable. Its statistical properties can be determined using linear response theory, particularly in the slow-driving limit ($\\tau \\gg \\gamma/\\kappa$). The average work corresponds to the dissipated heat, $\\langle W \\rangle = \\langle W_{diss} \\rangle$, as $\\Delta F=0$. For a constant velocity protocol, the average dissipated work is given by $\\langle W \\rangle \\approx \\gamma L^2/\\tau$ for large $\\tau$.\n\nThe problem states that the work variance and mean are related by a fluctuation-dissipation relation, which for this system manifests as $\\sigma_W^2 = 2k_B T \\langle W_{diss} \\rangle = 2\\beta^{-1}\\langle W \\rangle$. Combining these results, we establish the parameters of the Gaussian work distribution $P(W) \\sim \\mathcal{N}(\\mu_W, \\sigma_W^2)$:\n$$\n\\mu_W(\\tau) = \\frac{\\gamma L^2}{\\tau}\n$$\n$$\n\\sigma_W^2(\\tau) = \\frac{2\\gamma L^2}{\\beta\\tau}\n$$\nThese expressions conform to the functional dependencies specified in the problem statement. A key relation here is $\\sigma_W^2 = 2\\beta^{-1}\\mu_W$.\n\nThe free energy difference is estimated from $M$ independent trajectories using the exponential work estimator, which is a direct application of the Jarzynski equality $\\langle e^{-\\beta(W-\\Delta F)} \\rangle = 1$. With $\\Delta F = 0$, the estimator is:\n$$\n\\widehat{\\Delta F}_M = -\\beta^{-1} \\ln\\left( \\frac{1}{M} \\sum_{i=1}^M e^{-\\beta W_i} \\right)\n$$\nWe seek the Mean-Squared Error, $\\mathrm{MSE} = \\mathbb{E}[(\\widehat{\\Delta F}_M - \\Delta F)^2] = \\mathbb{E}[\\widehat{\\Delta F}_M^2]$. We derive this using the delta method. Let $Y = \\frac{1}{M}\\sum_i e^{-\\beta W_i}$. For large $M$, $Y$ converges to $\\mathbb{E}[e^{-\\beta W}]$. For a Gaussian variable $W$, $\\mathbb{E}[e^{-\\beta W}] = e^{-\\beta\\mu_W + \\beta^2\\sigma_W^2/2}$. Using our derived moments, the exponent is $-\\beta(\\gamma L^2/\\tau) + \\beta^2(2\\gamma L^2/\\beta\\tau)/2 = 0$, so $\\mathbb{E}[Y]=1$, consistent with the Jarzynski equality.\n\nTo find the MSE, we expand $\\widehat{\\Delta F}_M = -\\beta^{-1}\\ln Y$ around $Y=\\mathbb{E}[Y]=1$. Let $Y=1+\\delta Y$. Using $\\ln(1+\\delta Y) \\approx \\delta Y - (\\delta Y)^2/2$:\n$\\widehat{\\Delta F}_M \\approx -\\beta^{-1}(\\delta Y - (\\delta Y)^2/2)$.\nThe bias is $\\mathbb{B} = \\mathbb{E}[\\widehat{\\Delta F}_M] \\approx \\frac{1}{2\\beta}\\mathbb{E}[(\\delta Y)^2] = \\frac{1}{2\\beta}\\mathrm{Var}(Y)$.\nThe variance is $\\mathrm{Var}(\\widehat{\\Delta F}_M) \\approx \\frac{1}{\\beta^2}\\mathrm{Var}(\\delta Y) = \\frac{1}{\\beta^2}\\mathrm{Var}(Y)$.\nThe variance of $Y$ is $\\mathrm{Var}(Y) = \\frac{1}{M}\\mathrm{Var}(e^{-\\beta W}) = \\frac{1}{M}(\\mathbb{E}[e^{-2\\beta W}] - (\\mathbb{E}[e^{-\\beta W}])^2)$.\n$\\mathbb{E}[e^{-\\beta W}]=1$.\n$\\mathbb{E}[e^{-2\\beta W}] = e^{-2\\beta\\mu_W + \\beta^2(2\\sigma_W^2)/2} = e^{-2\\beta\\mu_W + 2\\beta^2\\sigma_W^2}$. Substituting the moments, the exponent is $-2\\beta(\\gamma L^2/\\tau) + 2\\beta^2(2\\gamma L^2/\\beta\\tau) = 2\\beta\\gamma L^2/\\tau$.\nLet's define $A(\\tau) = 2\\beta\\gamma L^2/\\tau$. Then $\\mathbb{E}[e^{-2\\beta W}] = e^{A(\\tau)}$.\nSo, $\\mathrm{Var}(Y) = \\frac{1}{M}(e^{A(\\tau)}-1)$.\nThe asymptotic bias and variance of the estimator are:\n$$\n\\mathbb{B}(\\widehat{\\Delta F}_M) \\approx \\frac{1}{2M\\beta}(e^{A(\\tau)}-1)\n$$\n$$\n\\mathrm{Var}(\\widehat{\\Delta F}_M) \\approx \\frac{1}{M\\beta^2}(e^{A(\\tau)}-1)\n$$\nThe MSE is the sum of the variance and the squared bias:\n$$\n\\mathrm{MSE}(M,\\tau) = \\mathrm{Var}(\\widehat{\\Delta F}_M) + \\mathbb{B}(\\widehat{\\Delta F}_M)^2 \\approx \\frac{e^{A(\\tau)}-1}{M\\beta^2} + \\left(\\frac{e^{A(\\tau)}-1}{2M\\beta}\\right)^2\n$$\n$$\n\\mathrm{MSE}(M,\\tau) = \\frac{1}{\\beta^2}\\left[ \\frac{e^{A(\\tau)}-1}{M} + \\frac{(e^{A(\\tau)}-1)^2}{4M^2} \\right] \\quad \\text{with} \\quad A(\\tau) = \\frac{2\\beta\\gamma L^2}{\\tau}\n$$\nThis is the function to be minimized. The optimization is subject to the total time budget $M(\\tau+t_{\\mathrm{over}}) \\le B$. Since the MSE is a monotonically decreasing function of both $M$ and $\\tau$, the minimum must lie on the boundary of the feasible region, where the budget is saturated: $M(\\tau+t_{\\mathrm{over}}) = B$. This allows us to express $\\tau$ as a function of $M$: $\\tau(M) = B/M - t_{\\mathrm{over}}$. This relation is valid for integers $M \\geq 1$ such that $\\tau(M) > 0$, implying $M < B/t_{\\mathrm{over}}$.\n\nThe problem is now reduced to a one-dimensional discrete optimization over the integer number of trajectories $M$:\n$$\n\\min_{M \\in \\mathbb{Z}^+, M < B/t_{\\mathrm{over}}} \\mathrm{MSE}(M, B/M - t_{\\mathrm{over}})\n$$\nTwo cases arise for the optimization strategy:\n1.  If $t_{\\mathrm{over}}=0$, then $\\tau=B/M$. The argument of the exponential becomes $A(M) = (2\\beta\\gamma L^2/B)M$. The function to be minimized, $g(M) = \\mathrm{MSE}(M, B/M)$, can be shown to be strictly increasing for all $M>0$. Therefore, the optimal choice is the smallest possible integer value for $M$, which is $M^\\star=1$.\n2.  If $t_{\\mathrm{over}}>0$, the search space for $M$ is $1 \\le M < B/t_{\\mathrm{over}}$. The argument $A(M) = \\frac{2\\beta\\gamma L^2 M}{B - Mt_{\\mathrm{over}}}$ diverges as $M \\to B/t_{\\mathrm{over}}$, causing the MSE to diverge. At $M=1$, the MSE is finite. This guarantees that a minimum exists for some integer $M$ in the allowed range. Due to the complex dependency on $M$, we find the optimum by a direct search over all valid integer values of $M$, from $1$ to $\\lfloor (B-10^{-9})/t_{\\mathrm{over}} \\rfloor$. For each $M$, we calculate the corresponding $\\tau$ and MSE, and identify the pair $(M^\\star, \\tau^\\star)$ that yields the minimum MSE. Numerical stability is ensured by checking for potential overflows when calculating $e^{A(\\tau)}$ for values of $M$ close to the upper bound, as the MSE in that region will be non-optimal.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes the optimal protocol time (tau*), number of trajectories (M*),\n    and minimum Mean-Squared Error (MSE*) for estimating free energy difference\n    under a computational budget.\n    \"\"\"\n    test_cases = [\n        (1.0, 1.0, 1.0, 100.0, 0.0),\n        (2.0, 2.0, 0.5, 10.0, 1.0),\n        (1.0, 0.5, 5.0, 1.5, 0.4),\n        (5.0, 1.0, 1.0, 20.0, 0.2),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        beta, L, gamma, B, t_over = case\n\n        best_m = -1\n        min_mse = float('inf')\n\n        def calculate_mse(M, tau, beta, gamma, L):\n            \"\"\"Calculates the asymptotic MSE.\"\"\"\n            if tau <= 0:\n                return float('inf')\n            \n            # Argument for the exponential term\n            A = (2 * beta * gamma * L**2) / tau\n            \n            # Numerical stability: if A is too large, exp(A) will overflow.\n            # The MSE will be gigantic, so this M cannot be the optimum.\n            # We can stop searching as MSE will only increase for larger M.\n            # math.exp overflows for args > 709.78\n            if A > 700:\n                return float('inf')\n\n            try:\n                exp_A = np.exp(A)\n            except OverflowError:\n                return float('inf')\n\n            exp_A_minus_1 = exp_A - 1\n            \n            # Variance term\n            variance = (exp_A_minus_1) / (M * beta**2)\n            \n            # Bias squared term\n            bias_sq = ((exp_A_minus_1)**2) / (4 * M**2 * beta**2)\n            \n            return variance + bias_sq\n\n        if t_over == 0.0:\n            # For t_over = 0, the MSE function is monotonically increasing with M.\n            # Thus, the optimum is always at M=1.\n            best_m = 1\n            tau_opt = B / best_m\n            min_mse = calculate_mse(best_m, tau_opt, beta, gamma, L)\n        else:\n            # Search for the optimal integer M\n            # The maximum possible value for M is when tau > 0, so M*t_over < B.\n            # We use floor((B - epsilon)/t_over) to get a safe integer upper bound.\n            max_m = int((B - 1e-9) / t_over)\n            \n            if max_m < 1:\n                # No valid trajectories possible under this budget\n                results.append([float('nan'), 0, float('nan')])\n                continue\n\n            for M_current in range(1, max_m + 1):\n                tau_current = B / M_current - t_over\n                current_mse = calculate_mse(M_current, tau_current, beta, gamma, L)\n\n                if current_mse == float('inf') and M_current > 1:\n                    # MSE is diverging, further M values will be worse.\n                    break\n                \n                if current_mse < min_mse:\n                    min_mse = current_mse\n                    best_m = M_current\n\n        tau_star = B / best_m - t_over if best_m > 0 else float('nan')\n        mse_star = min_mse if best_m > 0 else float('nan')\n\n        results.append([tau_star, best_m, mse_star])\n\n    # Format the output as a single string\n    final_strings = [f\"[{t:.6f},{m},{e:.6f}]\" for t, m, e in results]\n    print(f\"[{','.join(final_strings)}]\")\n\nsolve()\n```"
        }
    ]
}