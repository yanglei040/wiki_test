{
    "hands_on_practices": [
        {
            "introduction": "Before tackling complex free energy calculations, it is essential to build intuition by examining a limiting case. This practice considers two identical thermodynamic states, where the true free energy difference is known to be zero . By demonstrating from first principles that both BAR and MBAR correctly recover $\\Delta f = 0$ with zero variance, you will solidify your understanding of how these estimators function and how diagnostic quantities reflect the ideal scenario of perfect phase space overlap.",
            "id": "3397190",
            "problem": "Consider two thermodynamic states, labeled $0$ and $1$, each defined by a canonical distribution at inverse temperature $\\beta$. The reduced potential is $u_i(x) = \\beta U_i(x)$, where $U_i(x)$ is the potential energy function of state $i$ and $x$ denotes a configuration in phase space. The canonical density is $p_i(x) = Z_i^{-1} \\exp(-u_i(x))$, with partition function $Z_i = \\int \\exp(-u_i(x)) \\, dx$ and reduced free energy $f_i = -\\ln Z_i$. The free energy difference is $\\Delta f = f_1 - f_0$.\n\nYou are given the trivial case in which the potentials and temperatures are identical, meaning $U_0(x) = U_1(x)$ for all $x$ and both states are sampled at the same inverse temperature $\\beta$. Suppose that $N_0 = 41$ independent samples are drawn from $p_0(x)$ and $N_1 = 59$ independent samples are drawn from $p_1(x)$.\n\nUsing only the fundamental statistical mechanics definitions above and the fact that the Bennett Acceptance Ratio (BAR) and the Multistate Bennett Acceptance Ratio (MBAR) estimators are consistent estimators of $\\Delta f$, do the following:\n\n1. Demonstrate from first principles that $p_0(x) = p_1(x)$ and hence $Z_0 = Z_1$, concluding that the true free energy difference is $\\Delta f = 0$.\n\n2. Argue why, in this trivial case, both the BAR estimate and the MBAR estimate of $\\Delta f$ return exactly $0$ and possess vanishing estimator variance, i.e., the estimator outputs are non-random constants given the data.\n\n3. Compute the following diagnostic quantities that reflect perfect overlap between the two states:\n   - The overlap integral $O = \\int \\min\\{p_0(x), p_1(x)\\} \\, dx$.\n   - The Kullback–Leibler divergence $D_{\\mathrm{KL}}(p_0 \\Vert p_1) = \\int p_0(x) \\ln\\big(p_0(x)/p_1(x)\\big) \\, dx$.\n   - The Bhattacharyya coefficient $B = \\int \\sqrt{p_0(x) p_1(x)} \\, dx$.\n   - The Multistate Bennett Acceptance Ratio (MBAR) effective sample size defined by $N_{\\mathrm{eff}} = \\dfrac{\\left(\\sum_{n=1}^{N} w_n\\right)^{2}}{\\sum_{n=1}^{N} w_n^{2}}$ for the weights $\\{w_n\\}_{n=1}^{N}$ used to estimate expectations in either state when reweighting the pooled $N = N_0 + N_1$ samples, and where the weights can be taken to be proportional to the importance weights for either state under MBAR in this trivial scenario.\n   - The coefficient of variation of the MBAR weights, $\\mathrm{CV}_w = \\dfrac{\\sqrt{\\mathrm{Var}(w)}}{\\mathbb{E}[w]}$, for the same pooled weights.\n\nExpress the final answer as a row matrix containing, in order, the BAR estimate of $\\Delta f$, the MBAR estimate of $\\Delta f$, the variance of the BAR estimator, the overlap integral $O$, the Kullback–Leibler divergence $D_{\\mathrm{KL}}(p_0 \\Vert p_1)$, the Bhattacharyya coefficient $B$, the MBAR effective sample size $N_{\\mathrm{eff}}$, and the coefficient of variation $\\mathrm{CV}_w$.\n\nNo rounding is required. All quantities are dimensionless and should be expressed using natural logarithms, so that reduced free energies are dimensionless.",
            "solution": "The user has provided a problem to be validated and solved.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Two thermodynamic states, $0$ and $1$.\n- Inverse temperature: $\\beta$.\n- Reduced potential: $u_i(x) = \\beta U_i(x)$.\n- Potential energy function: $U_i(x)$.\n- Canonical probability density: $p_i(x) = Z_i^{-1} \\exp(-u_i(x))$.\n- Partition function: $Z_i = \\int \\exp(-u_i(x)) \\, dx$.\n- Reduced free energy: $f_i = -\\ln Z_i$.\n- Free energy difference: $\\Delta f = f_1 - f_0$.\n- Condition for the trivial case: $U_0(x) = U_1(x)$ for all configurations $x$.\n- Number of independent samples from state $0$: $N_0 = 41$.\n- Number of independent samples from state $1$: $N_1 = 59$.\n- The Bennett Acceptance Ratio (BAR) and Multistate Bennett Acceptance Ratio (MBAR) estimators are consistent estimators of $\\Delta f$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on fundamental principles of statistical mechanics (canonical ensemble, partition function, free energy) and well-established methods in computational chemistry (BAR, MBAR). All definitions and concepts are standard and scientifically sound.\n- **Well-Posed:** The problem provides a clear, self-contained set of conditions ($U_0(x)=U_1(x)$) and a series of specific, answerable questions. A unique solution exists for all parts of the problem.\n- **Objective:** The problem is stated in precise, formal language, free of ambiguity or subjective claims.\n\nThe problem does not violate any of the invalidity criteria. It is a valid, though intentionally simplified, problem designed to test the understanding of free energy estimators in a limiting case.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n### Solution\n\nThe solution proceeds in three parts as requested by the problem statement.\n\n**1. Demonstration of Identical States and Zero Free Energy Difference**\n\nWe are given that the potential energy functions for the two states are identical for all configurations $x$:\n$$ U_0(x) = U_1(x) $$\nThe reduced potential for state $i$ is defined as $u_i(x) = \\beta U_i(x)$. Since both states are at the same inverse temperature $\\beta$, it follows directly that their reduced potentials are also identical:\n$$ u_0(x) = \\beta U_0(x) = \\beta U_1(x) = u_1(x) $$\nThe partition function for state $i$, $Z_i$, is defined as the integral of the Boltzmann factor over all configurations:\n$$ Z_i = \\int \\exp(-u_i(x)) \\, dx $$\nBecause $u_0(x) = u_1(x)$, the integrands for $Z_0$ and $Z_1$ are identical. Therefore, the partition functions must be equal:\n$$ Z_0 = \\int \\exp(-u_0(x)) \\, dx = \\int \\exp(-u_1(x)) \\, dx = Z_1 $$\nThe canonical probability density for state $i$ is given by $p_i(x) = Z_i^{-1} \\exp(-u_i(x))$. Since we have shown that $u_0(x) = u_1(x)$ and $Z_0 = Z_1$, the probability densities for the two states must be identical:\n$$ p_0(x) = \\frac{\\exp(-u_0(x))}{Z_0} = \\frac{\\exp(-u_1(x))}{Z_1} = p_1(x) $$\nFinally, the reduced free energy difference $\\Delta f$ is defined as $\\Delta f = f_1 - f_0$. Using the definition $f_i = -\\ln Z_i$, we have:\n$$ \\Delta f = (-\\ln Z_1) - (-\\ln Z_0) = \\ln Z_0 - \\ln Z_1 = \\ln\\left(\\frac{Z_0}{Z_1}\\right) $$\nSince $Z_0 = Z_1$, their ratio is $1$, and the true free energy difference is:\n$$ \\Delta f = \\ln(1) = 0 $$\n\n**2. BAR and MBAR Estimators in the Trivial Case**\n\nThe key insight for this part is that the reduced potential energy difference, $\\Delta u(x) = u_1(x) - u_0(x)$, is zero for all $x$ because $u_0(x) = u_1(x)$.\n\n**Bennett Acceptance Ratio (BAR):**\nThe BAR estimate for the free energy difference, $\\Delta \\hat{f}$, is found by solving the following implicit equation:\n$$ \\sum_{n=1}^{N_1} \\frac{1}{1 + \\frac{N_1}{N_0} \\exp(\\Delta u(x_{1,n}) - \\Delta \\hat{f})} = \\sum_{m=1}^{N_0} \\frac{1}{1 + \\frac{N_0}{N_1} \\exp(-\\Delta u(x_{0,m}) + \\Delta \\hat{f})} $$\nwhere $\\{x_{1,n}\\}$ are samples from state $1$ and $\\{x_{0,m}\\}$ are samples from state $0$.\nSubstituting $\\Delta u(x)=0$ for all $x$, the equation simplifies significantly:\n$$ \\sum_{n=1}^{N_1} \\frac{1}{1 + \\frac{N_1}{N_0} \\exp(- \\Delta \\hat{f})} = \\sum_{m=1}^{N_0} \\frac{1}{1 + \\frac{N_0}{N_1} \\exp(\\Delta \\hat{f})} $$\nThe terms inside the summations are constant and do not depend on the specific samples drawn. The equation becomes:\n$$ N_1 \\left( \\frac{1}{1 + \\frac{N_1}{N_0} \\exp(- \\Delta \\hat{f})} \\right) = N_0 \\left( \\frac{1}{1 + \\frac{N_0}{N_1} \\exp(\\Delta \\hat{f})} \\right) $$\nThis equation can be solved for $\\Delta \\hat{f}$. We can test the solution $\\Delta \\hat{f} = 0$. If $\\Delta \\hat{f} = 0$, then $\\exp(-\\Delta \\hat{f}) = 1$ and $\\exp(\\Delta \\hat{f}) = 1$. The equation becomes:\n$$ \\frac{N_1}{1 + \\frac{N_1}{N_0}} = \\frac{N_0}{1 + \\frac{N_0}{N_1}} \\implies \\frac{N_1 N_0}{N_0 + N_1} = \\frac{N_0 N_1}{N_1 + N_0} $$\nThis is an identity, confirming that $\\Delta \\hat{f} = 0$ is the unique solution. Because the determining equation for $\\Delta \\hat{f}$ does not involve the random samples, the resulting estimator is a non-random constant. Therefore, the BAR estimate is exactly $\\Delta \\hat{f}_{\\mathrm{BAR}} = 0$, and its variance is consequently $0$.\n\n**Multistate Bennett Acceptance Ratio (MBAR):**\nThe MBAR equations determine the set of free energies $\\{f_k\\}$ by solving:\n$$ \\exp(-f_k) = \\sum_{i=0}^{1} \\sum_{n=1}^{N_i} \\frac{\\exp(-u_k(x_{i,n}))}{\\sum_{j=0}^{1} N_j \\exp(f_j - u_j(x_{i,n}))}, \\quad k \\in \\{0, 1\\} $$\nSetting the reference free energy $f_0=0$, we solve for $f_1$. Since $u_0(x)=u_1(x)=u(x)$, the equation for $f_1$ is:\n$$ \\exp(-f_1) = \\sum_{i=0}^{1} \\sum_{n=1}^{N_i} \\frac{\\exp(-u(x_{i,n}))}{N_0 \\exp(f_0 - u(x_{i,n})) + N_1 \\exp(f_1 - u(x_{i,n}))} $$\nSubstituting $f_0=0$ and factoring out $\\exp(-u(x_{i,n}))$ from the denominator:\n$$ \\exp(-f_1) = \\sum_{i=0}^{1} \\sum_{n=1}^{N_i} \\frac{\\exp(-u(x_{i,n}))}{\\exp(-u(x_{i,n}))(N_0 + N_1 \\exp(f_1))} = \\sum_{i=0}^{1} \\sum_{n=1}^{N_i} \\frac{1}{N_0 + N_1 \\exp(f_1)} $$\nThe summand is constant for all samples. The sum over all $N_0+N_1$ samples gives:\n$$ \\exp(-f_1) = \\frac{N_0 + N_1}{N_0 + N_1 \\exp(f_1)} $$\nTesting the solution $f_1=0$ gives $\\exp(-0) = \\frac{N_0+N_1}{N_0+N_1\\exp(0)}$, which simplifies to $1 = \\frac{N_0+N_1}{N_0+N_1} = 1$. This confirms that $\\hat{f}_1=0$ is the unique solution.\nThe estimated free energy difference is $\\Delta \\hat{f}_{\\mathrm{MBAR}} = \\hat{f}_1 - \\hat{f}_0 = 0 - 0 = 0$. As with BAR, the result is independent of the samples, so the estimate is a constant with zero variance.\n\n**3. Calculation of Diagnostic Quantities**\n\n- **Overlap Integral ($O$):**\n  $O = \\int \\min\\{p_0(x), p_1(x)\\} \\, dx$. Since $p_0(x) = p_1(x)$, we have $\\min\\{p_0(x), p_1(x)\\} = p_0(x)$. The integral of a probability density function over its domain is $1$.\n  $$ O = \\int p_0(x) \\, dx = 1 $$\n\n- **Kullback–Leibler Divergence ($D_{\\mathrm{KL}}$):**\n  $D_{\\mathrm{KL}}(p_0 \\Vert p_1) = \\int p_0(x) \\ln\\left(\\frac{p_0(x)}{p_1(x)}\\right) \\, dx$. Since $p_0(x) = p_1(x)$, the ratio is $1$, and $\\ln(1)=0$.\n  $$ D_{\\mathrm{KL}}(p_0 \\Vert p_1) = \\int p_0(x) \\cdot 0 \\, dx = 0 $$\n\n- **Bhattacharyya Coefficient ($B$):**\n  $B = \\int \\sqrt{p_0(x) p_1(x)} \\, dx$. Since $p_0(x) = p_1(x)$, the integrand is $\\sqrt{p_0(x)^2} = p_0(x)$.\n  $$ B = \\int p_0(x) \\, dx = 1 $$\n\n- **MBAR Effective Sample Size ($N_{\\mathrm{eff}}$):**\n  The MBAR weight for estimating an expectation in any state $k$ for a sample $x_n$ (from the pooled set of $N=N_0+N_1$ samples) is proportional to $w_n \\propto (\\sum_j N_j \\exp(f_j - u_j(x_n)))^{-1}$. In this case, $u_0=u_1=u$ and $f_0=f_1=0$, so the denominator for every sample simplifies to $(N_0+N_1) \\exp(-u(x_n))$. The full weight used for estimating averages in state $k$ is $w_{nk} = \\frac{\\exp(-u_k(x_n))}{\\sum_j N_j \\exp(f_j-u_j(x_n))}$. Normalizing these weights such that they sum to $1$ to form an average, we find that the actual value of each weight becomes a constant, $w_n = 1/N$, where $N=N_0+N_1=41+59=100$.\n  The effective sample size is defined as $N_{\\mathrm{eff}} = \\left(\\sum_{n=1}^{N} w_n\\right)^{2} / \\left(\\sum_{n=1}^{N} w_n^{2}\\right)$.\n  With $w_n = 1/N$:\n  $$ \\sum_{n=1}^{N} w_n = \\sum_{n=1}^{100} \\frac{1}{100} = 100 \\times \\frac{1}{100} = 1 $$\n  $$ \\sum_{n=1}^{N} w_n^2 = \\sum_{n=1}^{100} \\left(\\frac{1}{100}\\right)^2 = 100 \\times \\frac{1}{10000} = \\frac{1}{100} $$\n  $$ N_{\\mathrm{eff}} = \\frac{(1)^2}{1/100} = 100 $$\n  This indicates no loss of samples due to reweighting, as expected for identical distributions. $N_{\\mathrm{eff}} = N_0+N_1=100$.\n\n- **Coefficient of Variation of Weights ($\\mathrm{CV}_w$):**\n  $\\mathrm{CV}_w = \\sqrt{\\mathrm{Var}(w)}/\\mathbb{E}[w]$. The weights $\\{w_n\\}_{n=1}^N$ are a set of identical constants, $w_n = 1/N$. The variance of a set of constant values is $0$.\n  $$ \\mathrm{Var}(w) = 0 \\implies \\mathrm{CV}_w = 0 $$\n  This signifies that all samples are weighted equally, which is the hallmark of perfect overlap.\n\n**Summary of Results**\n- BAR estimate of $\\Delta f$: $0$\n- MBAR estimate of $\\Delta f$: $0$\n- Variance of the BAR estimator: $0$\n- Overlap integral $O$: $1$\n- Kullback–Leibler divergence $D_{\\mathrm{KL}}(p_0 \\Vert p_1)$: $0$\n- Bhattacharyya coefficient $B$: $1$\n- MBAR effective sample size $N_{\\mathrm{eff}}$: $100$\n- Coefficient of variation $\\mathrm{CV}_w$: $0$\nThese values will be assembled into the final answer matrix.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & 0 & 0 & 1 & 0 & 1 & 100 & 0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The efficiency of a free energy estimator is critically dependent on the degree of phase space overlap between the states. This practice moves beyond the ideal case to quantitatively explore this relationship using a tunable synthetic model . By numerically computing and comparing the asymptotic variance of the Bennett Acceptance Ratio (BAR) estimator against the simpler Free Energy Perturbation (FEP), you will gain a concrete understanding of BAR's statistical optimality and the conditions under which its performance degrades.",
            "id": "3397194",
            "problem": "You are given a synthetic two-state equilibrium model constructed to analyze the statistical efficiency of free energy estimators used in molecular dynamics. The model is defined entirely in terms of reduced units, so no physical units are required in the final numerical answers. Your task is to compute the asymptotic variance of the Bennett Acceptance Ratio (BAR) estimator and compare it to one-sided exponential averaging (also known as Free Energy Perturbation (FEP)), as a function of a tunable overlap parameter.\n\nFundamental base and construction:\n- Consider two reduced potentials $u_0(x)$ and $u_1(x)$, with the reduced potential difference defined as $\\Delta u(x) = u_1(x) - u_0(x)$.\n- In equilibrium statistical mechanics (canonical ensemble), the distributions of $\\Delta u$ when sampling from states $0$ and $1$ are related by exponential tilting:\n  $$ P_1(\\Delta u) = \\frac{P_0(\\Delta u)\\, e^{-(\\Delta u - \\Delta f)}}{\\int P_0(\\Delta u)\\, e^{-(\\Delta u - \\Delta f)}\\, d\\Delta u}, $$\n  where $\\Delta f$ is the reduced free energy difference satisfying\n  $$ \\Delta f = -\\log \\mathbb{E}_{0}\\left[ e^{-\\Delta u} \\right], $$\n  and $\\mathbb{E}_{0}[\\cdot]$ denotes expectation with respect to $P_0$.\n- Define a one-parameter family of models indexed by $\\kappa > 0$:\n  - Let $P_0$ be a Gaussian distribution $P_0(\\Delta u) = \\mathcal{N}(m_0, s^2)$ with $s = \\kappa$ and $m_0 = s^2/2$.\n  - Using the exponential-tilt relation above and the property of the moment-generating function of a Gaussian, this construction yields $\\Delta f = 0$, and $P_1(\\Delta u)$ becomes Gaussian with the same variance $s^2$ and mean $m_1 = -s^2/2$. Thus $P_1(\\Delta u) = \\mathcal{N}(m_1, s^2)$.\n\nEstimators to compare:\n- One-sided exponential averaging (Free Energy Perturbation (FEP), forward direction from state $0$): With $n_0$ samples $\\{\\Delta u_i\\}_{i=1}^{n_0}$ drawn independently from $P_0$, the estimator is\n  $$ \\widehat{\\Delta f}_{\\text{FEP}} = -\\log \\left( \\frac{1}{n_0} \\sum_{i=1}^{n_0} e^{-\\Delta u_i} \\right). $$\n- Bennett Acceptance Ratio (BAR): With $n_0$ samples from $P_0$ and $n_1$ samples from $P_1$, BAR estimates $\\Delta f$ by solving for the unique root $C$ of the estimating equation\n  $$ \\frac{1}{n_0}\\sum_{i=1}^{n_0} \\frac{1}{1 + e^{\\Delta u_i - C}} - \\frac{1}{n_1}\\sum_{j=1}^{n_1} \\frac{1}{1 + e^{-(\\Delta u'_j - C)}} = 0, $$\n  where $\\{\\Delta u'_j\\}_{j=1}^{n_1}$ are independent samples from $P_1$, and then setting $\\widehat{\\Delta f}_{\\text{BAR}} = C$. In the present symmetric construction, the true value is $\\Delta f = 0$.\n\nVariance analysis tasks:\n- Starting from the Law of Large Numbers and the Central Limit Theorem, and using the delta method for smooth transformations of sample means where appropriate, you must:\n  1) Derive the asymptotic variance of the one-sided FEP estimator (forward direction from state $0$). In terms of the variance $s^2$ and sample size $n_0$, show a formula that depends only on $s^2$ and $n_0$.\n  2) Derive the asymptotic variance of the BAR estimator using the theory of $M$-estimators for roots of estimating equations. Let $f(u; C) = \\left(1 + e^{u - C}\\right)^{-1}$. Show that in the large-sample limit with $n_0 = n_1 = n$, the variance has the form\n     $$ \\mathrm{Var}(\\widehat{\\Delta f}_{\\text{BAR}}) \\approx \\frac{ \\mathrm{Var}_0\\!\\left[f(\\Delta u; \\Delta f)\\right]/n + \\mathrm{Var}_1\\!\\left[f(\\Delta u; \\Delta f)\\right]/n }{ \\left( \\mathbb{E}_0\\!\\left[ f(\\Delta u; \\Delta f)\\left(1 - f(\\Delta u; \\Delta f)\\right) \\right] + \\mathbb{E}_1\\!\\left[ f(\\Delta u; \\Delta f)\\left(1 - f(\\Delta u; \\Delta f)\\right) \\right] \\right)^2 }, $$\n     where expectations and variances with subscripts $0$ and $1$ are taken with respect to $P_0$ and $P_1$, respectively, and the evaluation is at the true $C = \\Delta f = 0$.\n- Interpret “BAR instability” in this Gaussian family as the condition where the sensitivity of the estimating equation vanishes, i.e., the denominator\n  $$ D(\\kappa) \\equiv \\mathbb{E}_0\\!\\left[ f(\\Delta u; 0)\\left(1 - f(\\Delta u; 0)\\right) \\right] + \\mathbb{E}_1\\!\\left[ f(\\Delta u; 0)\\left(1 - f(\\Delta u; 0)\\right) \\right] $$\n  becomes numerically negligible, causing the asymptotic variance to blow up. For numerical purposes, declare BAR “unstable” when $D(\\kappa) < 10^{-6}$.\n\nNumerical procedure constraints:\n- All integrals over $\\Delta u$ with respect to $P_0$ and $P_1$ must be evaluated numerically to high accuracy using quadrature appropriate for Gaussian measures. Gauss–Hermite quadrature is suitable: for $U \\sim \\mathcal{N}(m, s^2)$ and any function $g$, compute\n  $$ \\mathbb{E}[g(U)] = \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{M} w_i\\, g\\!\\left(m + s\\sqrt{2}\\, x_i\\right), $$\n  where $\\{x_i, w_i\\}_{i=1}^{M}$ are $M$-point Gauss–Hermite nodes and weights, and $M$ must be chosen large enough to ensure numerical stability for all test cases below.\n\nTest suite:\n- Use identical sample sizes for both states: $n_0 = n_1 = n = 10000$.\n- Evaluate at the following six values of the overlap parameter $\\kappa$:\n  - $\\kappa = 0.25$\n  - $\\kappa = 0.75$\n  - $\\kappa = 1.50$\n  - $\\kappa = 2.50$\n  - $\\kappa = 4.00$\n  - $\\kappa = 6.00$\n- For each $\\kappa$, you must compute:\n  - The asymptotic variance of BAR, as a float.\n  - The asymptotic variance of forward one-sided FEP, as a float.\n  - A boolean flag indicating BAR instability at that $\\kappa$ (as defined above).\n\nFinal output format:\n- Your program must output a single line containing a comma-separated Python list aggregating the results for the six test cases in order, each case contributing three entries: $[\\mathrm{Var}_{\\text{BAR}}(\\kappa), \\mathrm{Var}_{\\text{FEP}}(\\kappa), \\text{unstable}(\\kappa), \\dots]$.\n- After these eighteen entries, append two more entries:\n  1) The smallest $\\kappa$ (from the given test set) at which $\\mathrm{Var}_{\\text{BAR}}(\\kappa) > \\mathrm{Var}_{\\text{FEP}}(\\kappa)$; if none, output $-1$.\n  2) The smallest $\\kappa$ (from the given test set) flagged as unstable; if none, output $-1$.\n- The final printed string must therefore be a single-line Python list of length $20$. All floats are dimensionless and must be rounded to six decimal places in the output.\n\nYour program must be self-contained, must not read input, and must not access any external resources. It must implement the Gauss–Hermite quadrature internally and adhere to the numerical stability requirement across all specified $\\kappa$ values. The output must be exactly one line in the specified format (e.g., “[..., ...]”).",
            "solution": "The problem statement has been meticulously validated and is deemed **valid**. It is scientifically grounded in the principles of statistical mechanics, specifically the theory of free energy estimation. The problem is well-posed, self-contained, and provides a clear, objective task for deriving and computing the statistical properties of two key estimators, the Bennett Acceptance Ratio (BAR) and Free Energy Perturbation (FEP), within a synthetic but rigorously defined Gaussian model. All parameters, definitions, and numerical constraints are specified without ambiguity or contradiction.\n\nThe solution proceeds in two stages: theoretical derivation followed by numerical implementation.\n\n### Theoretical Derivations\n\nThe problem requires the derivation of the asymptotic variances for the one-sided FEP estimator and the BAR estimator.\n\n**1. Asymptotic Variance of the FEP Estimator**\n\nThe forward FEP estimator for the free energy difference $\\Delta f$ is given by:\n$$ \\widehat{\\Delta f}_{\\text{FEP}} = -\\log \\left( \\frac{1}{n_0} \\sum_{i=1}^{n_0} e^{-\\Delta u_i} \\right) $$\nwhere the samples $\\{\\Delta u_i\\}$ are drawn from the distribution $P_0(\\Delta u) = \\mathcal{N}(m_0, s^2)$. The problem specifies $s = \\kappa$ and $m_0 = s^2/2$.\n\nLet $Y_i = e^{-\\Delta u_i}$. The estimator is a function of the sample mean $\\bar{Y} = \\frac{1}{n_0}\\sum Y_i$, specifically $\\widehat{\\Delta f}_{\\text{FEP}} = g(\\bar{Y})$ where $g(y) = -\\log(y)$. The true free energy is $\\Delta f = g(\\mathbb{E}_0[Y])$.\nThe mean of $Y$ is $\\mu_Y = \\mathbb{E}_0[e^{-\\Delta u}]$. This is the moment-generating function of a Gaussian variable $\\Delta u \\sim \\mathcal{N}(m_0, s^2)$ evaluated at $t=-1$:\n$$ \\mu_Y = \\mathbb{E}_0[e^{-\\Delta u}] = \\exp\\left(m_0(-1) + \\frac{s^2(-1)^2}{2}\\right) = \\exp\\left(-m_0 + \\frac{s^2}{2}\\right) $$\nSubstituting $m_0 = s^2/2$, we find $\\mu_Y = \\exp(-s^2/2 + s^2/2) = \\exp(0) = 1$. This confirms the problem's statement that $\\Delta f = -\\log(1) = 0$.\n\nUsing the delta method, the asymptotic variance of $\\widehat{\\Delta f}_{\\text{FEP}}$ is:\n$$ \\mathrm{Var}(\\widehat{\\Delta f}_{\\text{FEP}}) \\approx [g'(\\mu_Y)]^2 \\mathrm{Var}(\\bar{Y}) = [g'(\\mu_Y)]^2 \\frac{\\mathrm{Var}_0(Y)}{n_0} $$\nWith $g'(y) = -1/y$, we have $g'(1) = -1$. The variance of $Y$ is $\\mathrm{Var}_0(Y) = \\mathbb{E}_0[Y^2] - (\\mathbb{E}_0[Y])^2$.\nWe need $\\mathbb{E}_0[Y^2] = \\mathbb{E}_0[e^{-2\\Delta u}]$, which is the MGF evaluated at $t=-2$:\n$$ \\mathbb{E}_0[e^{-2\\Delta u}] = \\exp\\left(m_0(-2) + \\frac{s^2(-2)^2}{2}\\right) = \\exp\\left(-2m_0 + 2s^2\\right) $$\nSubstituting $m_0 = s^2/2$, we get $\\mathbb{E}_0[e^{-2\\Delta u}] = \\exp(-s^2 + 2s^2) = \\exp(s^2)$.\nThus, $\\mathrm{Var}_0(Y) = \\exp(s^2) - 1^2 = \\exp(s^2) - 1$.\nThe asymptotic variance of the FEP estimator is:\n$$ \\mathrm{Var}(\\widehat{\\Delta f}_{\\text{FEP}}) \\approx \\frac{1}{n_0} (\\exp(s^2) - 1) = \\frac{\\exp(\\kappa^2) - 1}{n_0} $$\nThis expression is exact for this model and depends only on $s^2 = \\kappa^2$ and $n_0$.\n\n**2. Asymptotic Variance of the BAR Estimator**\n\nThe problem provides the formula for the asymptotic variance of the BAR estimator for $n_0 = n_1 = n$ and at the true value $\\Delta f = 0$:\n$$ \\mathrm{Var}(\\widehat{\\Delta f}_{\\text{BAR}}) \\approx \\frac{ \\mathrm{Var}_0[f(\\Delta u; 0)]/n + \\mathrm{Var}_1[f(\\Delta u; 0)]/n }{ \\left( \\mathbb{E}_0[f(\\Delta u; 0)(1 - f(\\Delta u; 0))] + \\mathbb{E}_1[f(\\Delta u; 0)(1 - f(\\Delta u; 0))] \\right)^2 } $$\nwhere $f(u; 0) = \\sigma(u) = (1 + e^u)^{-1}$.\nThe model is constructed with symmetric distributions: $P_0 = \\mathcal{N}(m_0, s^2)$ and $P_1 = \\mathcal{N}(m_1, s^2)$, where $m_1 = -m_0 = -s^2/2$. This implies that for any function $h$, $\\mathbb{E}_1[h(\\Delta u)] = \\mathbb{E}_0[h(-\\Delta u)]$.\n\nThis symmetry greatly simplifies the variance formula's components:\n- Let $g_3(u) = \\sigma(u)(1-\\sigma(u))$. This function is even, $g_3(-u) = g_3(u)$. Therefore, $\\mathbb{E}_1[g_3(\\Delta u)] = \\mathbb{E}_0[g_3(-\\Delta u)] = \\mathbb{E}_0[g_3(\\Delta u)]$.\nThe denominator's core, $D(\\kappa)$, becomes $D(\\kappa) = 2 \\mathbb{E}_0[\\sigma(\\Delta u)(1 - \\sigma(\\Delta u))]$.\n- For the numerator, we have $\\mathrm{Var}_1[\\sigma(\\Delta u)] = \\mathbb{E}_1[\\sigma^2(\\Delta u)] - (\\mathbb{E}_1[\\sigma(\\Delta u)])^2$.\nUsing the relations $\\sigma(-u) = 1-\\sigma(u)$ and $\\sigma^2(-u) = (1-\\sigma(u))^2$, we find $\\mathrm{Var}_1[\\sigma(\\Delta u)] = \\mathrm{Var}_0[\\sigma(\\Delta u)]$ after algebraic simplification.\n\nThe BAR variance formula thus simplifies to:\n$$ \\mathrm{Var}(\\widehat{\\Delta f}_{\\text{BAR}}) \\approx \\frac{ 2 \\mathrm{Var}_0[\\sigma(\\Delta u)]/n }{ \\left( 2 \\mathbb{E}_0[\\sigma(\\Delta u)(1 - \\sigma(\\Delta u))] \\right)^2 } = \\frac{ \\mathrm{Var}_0[\\sigma(\\Delta u)] }{ 2n \\left( \\mathbb{E}_0[\\sigma(\\Delta u)(1 - \\sigma(\\Delta u))] \\right)^2 } $$\nwhere $\\mathrm{Var}_0[\\sigma(\\Delta u)] = \\mathbb{E}_0[\\sigma^2(\\Delta u)] - (\\mathbb{E}_0[\\sigma(\\Delta u)])^2$.\n\n### Numerical Evaluation\n\nThe remaining task is to compute the expectations $\\mathbb{E}_0[\\sigma(\\Delta u)]$, $\\mathbb{E}_0[\\sigma^2(\\Delta u)]$, and $\\mathbb{E}_0[\\sigma(\\Delta u)(1 - \\sigma(\\Delta u))]$ with respect to the Gaussian distribution $P_0 = \\mathcal{N}(m_0, s^2)$. This is done using Gauss-Hermite quadrature as specified. For a function $g$ and a random variable $U \\sim \\mathcal{N}(m, s^2)$, the expectation is computed as:\n$$ \\mathbb{E}[g(U)] = \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{M} w_i\\, g(m + s\\sqrt{2}\\, x_i) $$\nwhere $\\{x_i, w_i\\}_{i=1}^{M}$ are the nodes and weights for the physicist's Hermite polynomials. The implementation will use `numpy.polynomial.hermite.hermgauss` to obtain these nodes and weights. A sufficiently large number of points ($M=200$) is chosen to ensure numerical accuracy across the range of $\\kappa$ values.\n\nThe final calculations are:\n- $\\mathrm{Var}_{\\text{FEP}}$ is computed analytically.\n- $\\mathrm{Var}_{\\text{BAR}}$ is computed using the simplified formula and numerically evaluated expectations.\n- The BAR instability flag is set to `True` if $D(\\kappa) = 2 \\mathbb{E}_0[\\sigma(\\Delta u)(1 - \\sigma(\\Delta u))] < 10^{-6}$.\n\nIt is a known theoretical result that the variance of the BAR estimator is less than or equal to that of one-sided FEP estimators. Therefore, the condition $\\mathrm{Var}_{\\text{BAR}}(\\kappa) > \\mathrm{Var}_{\\text{FEP}}(\\kappa)$ is not expected to be met for any $\\kappa > 0$. The corresponding summary statistic will be $-1$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the asymptotic variances of BAR and FEP estimators\n    for a synthetic Gaussian model.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    # List of overlap parameters kappa to test\n    KAPPAS = [0.25, 0.75, 1.50, 2.50, 4.00, 6.00]\n    # Number of samples from each state\n    N_SAMPLES = 10000\n    # Number of Gauss-Hermite quadrature points for numerical integration\n    N_QUAD_POINTS = 200\n    # Threshold for BAR instability\n    INSTABILITY_THRESHOLD = 1e-6\n\n    def compute_estimator_variances(kappa: float, n: int, M: int) -> tuple[float, float, bool]:\n        \"\"\"\n        Calculates the asymptotic variances for BAR and FEP for a given kappa.\n\n        Args:\n            kappa: The overlap parameter.\n            n: The number of samples (n0=n1=n).\n            M: The number of quadrature points.\n\n        Returns:\n            A tuple containing (Var_BAR, Var_FEP, is_unstable).\n        \"\"\"\n        s = kappa\n        s2 = s**2\n        \n        # --- 1. Asymptotic Variance of FEP (Analytic) ---\n        # Var(FEP_fwd) = (exp(s^2) - 1) / n\n        var_fep = (np.exp(s2) - 1) / n\n\n        # --- 2. Asymptotic Variance of BAR (Numerical Quadrature) ---\n        m0 = s2 / 2.0\n        \n        # Get Gauss-Hermite nodes (x_gh) and weights (w_gh) for the\n        # physicist's weight function exp(-x^2).\n        try:\n            x_gh, w_gh = np.polynomial.hermite.hermgauss(M)\n        except TypeError: # Compatibility for older numpy versions\n             x_gh, w_gh = np.polynomial.hermite.hermgauss(M)\n\n\n        # Create the quadrature evaluation points for U ~ N(m0, s^2)\n        # using the transformation u = m0 + s*sqrt(2)*x\n        u_pts = m0 + s * np.sqrt(2.0) * x_gh\n\n        # Define integrand functions (vectorized)\n        # The fermi function f(u;0) = sigma(u)\n        sigma = 1.0 / (1.0 + np.exp(u_pts))\n        # f^2\n        sigma_sq = sigma**2\n        # f(1-f)\n        sigma_1_minus_sigma = sigma * (1.0 - sigma)\n\n        # Compute expectations with respect to P0 using the specified quadrature rule:\n        # E[g(U)] = (1/sqrt(pi)) * sum(w_i * g(m+s*sqrt(2)*x_i))\n        E0_sigma = np.sum(w_gh * sigma) / np.sqrt(np.pi)\n        E0_sigma_sq = np.sum(w_gh * sigma_sq) / np.sqrt(np.pi)\n        E0_sigma_1_minus_sigma = np.sum(w_gh * sigma_1_minus_sigma) / np.sqrt(np.pi)\n\n        # Variance of sigma(Delta u) under P0\n        var0_sigma = E0_sigma_sq - E0_sigma**2\n        \n        # Denominator term D(kappa) and instability check\n        D_kappa = 2.0 * E0_sigma_1_minus_sigma\n        is_unstable = D_kappa < INSTABILITY_THRESHOLD\n        \n        # Asymptotic variance of BAR, using the simplified formula for symmetric systems\n        # Var(BAR) = [2 * Var0(f) / n] / [D_kappa]^2\n        # Use a small epsilon to prevent division by zero for unstable cases.\n        var_bar = (2.0 * var0_sigma / n) / (D_kappa**2 + 1e-30)\n\n        return var_bar, var_fep, is_unstable\n\n    # --- Main Calculation Loop ---\n    all_results = []\n    first_crossover_kappa = -1.0\n    first_unstable_kappa = -1.0\n\n    for kappa in KAPPAS:\n        var_bar, var_fep, is_unstable = compute_estimator_variances(\n            kappa, N_SAMPLES, N_QUAD_POINTS\n        )\n        \n        all_results.append(f\"{var_bar:.6f}\")\n        all_results.append(f\"{var_fep:.6f}\")\n        all_results.append(str(is_unstable))\n        \n        # Check for crossover (Var_BAR > Var_FEP)\n        # Based on theory, this should not happen, so first_crossover_kappa will remain -1.\n        if var_bar > var_fep and first_crossover_kappa == -1.0:\n            first_crossover_kappa = kappa\n        \n        # Check for first occurrence of instability\n        if is_unstable and first_unstable_kappa == -1.0:\n            first_unstable_kappa = kappa\n\n    # --- Final Output Formatting ---\n    all_results.append(str(first_crossover_kappa))\n    all_results.append(str(first_unstable_kappa))\n    \n    final_output_string = f\"[{','.join(all_results)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "The true power of the acceptance ratio method is realized in its multistate formulation, MBAR, which can simultaneously analyze data from many thermodynamic states. This capstone exercise challenges you to implement a numerically robust MBAR solver from its foundational statistical principles . By incorporating essential techniques like the log-sum-exp trick and damped fixed-point iteration, you will build a practical tool and understand the key ingredients for obtaining stable and accurate free energy estimates in real-world applications.",
            "id": "3397216",
            "problem": "Implement a numerically stable solver for the Multistate Bennett Acceptance Ratio (MBAR), suitable for estimating dimensionless reduced free energies from pooled configurations drawn from multiple thermodynamic states in molecular dynamics. Start from the following fundamental base: In the canonical ensemble, the unnormalized target density of a thermodynamic state is proportional to the Boltzmann factor $e^{-u_k(x)}$, where $u_k(x)$ is a reduced potential energy. Given pooled samples and the principle of maximum likelihood under importance sampling, the stationary conditions define a set of self-consistent equations for the unknown reduced free energies that must be solved under the constraint that an arbitrary additive constant does not affect observables. Do not assume any pre-derived formula specific to Multistate Bennett Acceptance Ratio (MBAR) or Bennett Acceptance Ratio (BAR); instead, reason from the likelihood of the pooled data and the normalization requirements of importance sampling. Your implementation must:\n- Treat the denominator of the importance weights as a log-sum-exp computation to avoid underflow and overflow in floating-point arithmetic.\n- Enforce an anchoring convention to fix the additive gauge freedom, e.g., $ \\hat f_0 = 0 $.\n- Include at least two scaling strategies to improve numerical stability when the estimates $ \\hat f_m $ vary widely:\n  - Re-centering of the vector $ \\hat f $ at every iteration (e.g., by fixing one component to $ 0 $).\n  - Normalizing the sample counts in logarithmic form to remove irrelevant global scale factors, which only shift all $ \\hat f_k $ by a constant that is eliminated by the anchoring.\n- Use a damped fixed-point iteration with a convergence tolerance to ensure robust convergence.\n\nData model for test construction:\n- Consider $ 1 $-dimensional harmonic oscillators at inverse temperature $ \\beta = 1 $, with potential energy $ U_k(x) = \\tfrac{1}{2} K_k x^2 $, where $ K_k $ is the force constant for state $ k $. The reduced potential is then $ u_k(x) = \\beta U_k(x) + c_k = \\tfrac{1}{2} K_k x^2 + c_k $, where $ c_k $ is a constant offset that can vary by state.\n- For each thermodynamic state $ k $, draw $ N_k $ independent samples $ x $ from the corresponding equilibrium distribution of that state. For a $ 1 $-dimensional harmonic oscillator at $ \\beta = 1 $, $ x $ is distributed as a normal distribution with mean $ 0 $ and variance $ \\sigma_k^2 = 1 / K_k $.\n- For each test case, pool all samples from all states into a single set, and compute $ u_m(x) $ for every pooled configuration $ x $ and every state $ m $ to obtain a matrix $ u_{m i} $ of shape $ K \\times N $, where $ K $ is the number of states and $ N = \\sum_{k=0}^{K-1} N_k $ is the total number of pooled samples.\n\nAlgorithmic requirements:\n- Let $ N_k $ be the count of samples drawn from state $ k $. Let $ \\hat f_k $ be the current estimate of the reduced free energy of state $ k $. Define the per-sample denominator for pooled sample index $ i $ as the stable log-sum-exp of the terms $ \\log N_m + \\hat f_m - u_m(x_i) $ over $ m $. Use this to evaluate the importance weights and update $ \\hat f_k $ via the fixed-point equations implied by the stationary conditions of the pooled-data likelihood. Apply damping to the fixed-point updates and re-center the vector $ \\hat f $ to enforce $ \\hat f_0 = 0 $ at every iteration. Normalize $ N_k $ in logarithmic form (by subtracting $ \\log \\sum_m N_m $) so that any global multiplicative factor in the counts induces only an additive constant in all $ \\hat f_k $, which is removed by the anchoring.\n- Implement a convergence check that stops when the maximum absolute change in $ \\hat f_k $ across $ k $ is below a tolerance. Use a reasonable default tolerance such as $ 10^{-12} $ and damping factor such as $ 0.5 $.\n\nTest suite:\nConstruct the following $ 4 $ test cases using a deterministic random number generator with seed $ 12345 $ so that all results are reproducible. For each test case, generate normal variates as specified above and construct the reduced potential matrix $ u_{m i} $ for all pooled samples and states. Then run your MBAR solver on each test case to estimate $ \\hat f_k $, anchored such that $ \\hat f_0 = 0 $. For each test case, report the vector $ [ \\hat f_0 - \\hat f_0, \\hat f_1 - \\hat f_0, \\ldots, \\hat f_{K-1} - \\hat f_0 ] $ so the first entry is always $ 0 $.\n\n- Test case $ 1 $ (happy path): $ K = 3 $, $ (N_0, N_1, N_2) = (200, 150, 150) $, $ (K_0, K_1, K_2) = (1.0, 4.0, 16.0) $, $ (c_0, c_1, c_2) = (0.0, 0.0, 0.0) $.\n- Test case $ 2 $ (imbalanced counts and moderate offsets): $ K = 3 $, $ (N_0, N_1, N_2) = (400, 10, 10) $, $ (K_0, K_1, K_2) = (1.0, 2.0, 8.0) $, $ (c_0, c_1, c_2) = (0.0, 5.0, 10.0) $.\n- Test case $ 3 $ (extreme offset stress-test): $ K = 2 $, $ (N_0, N_1) = (100, 100) $, $ (K_0, K_1) = (1.0, 1.0) $, $ (c_0, c_1) = (0.0, 500.0) $.\n- Test case $ 4 $ (boundary case): $ K = 1 $, $ (N_0) = (100) $, $ (K_0) = (2.0) $, $ (c_0) = (0.0) $.\n\nAngle or physical units do not apply; all quantities are dimensionless reduced units. For numerical reporting, round each output entry to $ 6 $ decimal places.\n\nFinal output format:\n- Your program should produce a single line containing the concatenation of the anchored free energies for all test cases, each test case contributing a list $ [ \\hat f_0 - \\hat f_0, \\ldots, \\hat f_{K-1} - \\hat f_0 ] $, flattened and printed as a single comma-separated list enclosed in square brackets. For example, if test case $ 1 $ yields $ [0.0, a, b] $ and test case $ 2 $ yields $ [0.0, c, d] $, the final output should be $ [0.0,a,b,0.0,c,d] $ with each number rounded to $ 6 $ decimals.",
            "solution": "The user-provided problem is valid as it is scientifically grounded in the principles of statistical mechanics, specifically the calculation of free energies from molecular simulation data. It is well-posed, providing a clear objective, all necessary data, and a verifiable structure. The problem requires the implementation of the Multistate Bennett Acceptance Ratio (MBAR) method, derived from fundamental principles rather than a pre-packaged formula.\n\nThe derivation of the MBAR equations stems from the self-consistency requirement that the number of samples observed from a given thermodynamic state must match the model's prediction for that number. Let there be $K$ thermodynamic states, indexed by $k$. From each state $k$, a set of $N_k$ configurations is sampled. All $N = \\sum_{k=0}^{K-1} N_k$ configurations $\\{x_i\\}_{i=1}^N$ are pooled. The reduced potential energy of configuration $x_i$ in state $k$ is $u_k(x_i)$. The unknown dimensionless reduced free energy of state $k$ is $f_k$.\n\nThe probability that a given configuration $x_i$ from the pooled set originated from state $k$, given the set of free energies $\\{f_m\\}$, is given by a posterior probability:\n$$ P(k | x_i, \\{f_m, N_m\\}) = \\frac{P(x_i | k) P(k)}{\\sum_{m=0}^{K-1} P(x_i | m) P(m)} $$\nThe probability of observing configuration $x_i$ given it came from state $k$ is proportional to the Boltzmann factor, $P(x_i | k) \\propto e^{-u_k(x_i)}$. More precisely, the probability density is $p_k(x_i) = e^{-u_k(x_i) + f_k}$, where $e^{-f_k} = Z_k$ is the partition function that normalizes the distribution. The prior probability of drawing a sample from state $k$ is proportional to the number of samples drawn from it, $P(k) \\propto N_k$. Substituting these into the posterior gives:\n$$ P(k | x_i) = \\frac{N_k e^{-u_k(x_i) + f_k}}{\\sum_{m=0}^{K-1} N_m e^{-u_m(x_i) + f_m}} $$\nThe self-consistency condition requires that the total number of samples from state $k$, $N_k$, must be equal to the sum of the probabilities that each sample in the entire pooled dataset originated from state $k$:\n$$ N_k = \\sum_{i=1}^N P(k | x_i) = \\sum_{i=1}^N \\frac{N_k e^{-u_k(x_i) + f_k}}{\\sum_{m=0}^{K-1} N_m e^{-u_m(x_i) + f_m}} $$\nAssuming $N_k > 0$, we can divide by $N_k$ and rearrange to obtain a fixed-point iteration for $f_k$. Let $f_k^{(t)}$ be the estimate at iteration $t$. The estimate for the next iteration, $f_k^{(t+1)}$, is given by:\n$$ e^{-f_k^{(t+1)}} = \\sum_{i=1}^N \\frac{e^{-u_k(x_i)}}{\\sum_{m=0}^{K-1} N_m e^{-u_m(x_i) + f_m^{(t)}}} $$\nTaking the natural logarithm of both sides gives the update rule for $f_k$:\n$$ f_k^{(t+1)} = -\\ln\\left( \\sum_{i=1}^N \\frac{e^{-u_k(x_i)}}{\\sum_{m=0}^{K-1} N_m e^{f_m^{(t)} - u_m(x_i)}} \\right) $$\nTo implement this equation in a numerically stable manner, all sums of exponentials are computed using the log-sum-exp trick. Let $A_{mi}^{(t)} = \\ln N_m + f_m^{(t)} - u_m(x_i)$. The log of the inner summation (the denominator) for sample $i$ is $L_i^{(t)} = \\text{log-sum-exp}_m(A_{mi}^{(t)})$. The update equation becomes:\n$$ f_k^{(t+1)} = -\\text{log-sum-exp}_i \\left( -u_k(x_i) - L_i^{(t)} \\right) $$\nThis iterative process is subject to an arbitrary additive constant, as adding a constant $C$ to all $f_m$ leaves the equations unchanged. This gauge freedom is fixed by anchoring one free energy, for instance by imposing $\\hat f_0 = 0$ at each step of the iteration. This is a re-centering strategy.\nA second stability enhancement involves normalizing the sample counts, $N_k$. Using normalized counts $N_k' = N_k / N$ is equivalent to subtracting a constant $\\ln N$ from all initial $\\ln N_k$. This makes the calculation invariant to a global scaling of all $N_k$, a shift which is in any case absorbed by the anchoring step.\nFinally, to ensure robust convergence, a damped update is used. If $\\hat{f}_{\\text{new}}$ is the vector of free energies computed from the equation above and subsequently anchored, the next iterate $\\hat{f}^{(t+1)}$ is computed as a weighted average of the old and new estimates:\n$$ \\hat{f}^{(t+1)} = (1-\\alpha) \\hat{f}^{(t)} + \\alpha \\hat{f}_{\\text{new}} $$\nwhere $\\alpha \\in (0, 1]$ is a damping factor. The iteration proceeds until the maximum absolute change in any component of $\\hat{f}$ falls below a specified tolerance, $\\epsilon$.\n\nThe algorithmic procedure is as follows:\n1. Initialize the free energy estimates, $\\hat{f}_k$, to $0$ for all $k$.\n2. Pre-calculate the normalized logarithms of the sample counts, $\\ln(N_k/N)$, handling cases where $N_k=0$.\n3. Enter the iteration loop:\n   a. Compute the matrix of exponents for the denominator, $A_{mi} = \\ln(N_m/N) + \\hat{f}_m - u_{mi}$.\n   b. Use log-sum-exp along the state-axis ($m$) to find the log-denominator for each sample, $L_i = \\text{log-sum-exp}_m(A_{mi})$.\n   c. Compute the matrix of exponents for the outer sum, $B_{ki} = -u_{ki} - L_i$.\n   d. Use log-sum-exp along the sample-axis ($i$) to find the new un-anchored free energies, $\\hat{f}_{k, \\text{raw}} = -\\text{log-sum-exp}_i(B_{ki})$.\n   e. Anchor the new estimates by subtracting the first element: $\\hat{f}_{k, \\text{new}} = \\hat{f}_{k, \\text{raw}} - \\hat{f}_{0, \\text{raw}}$.\n   f. Apply the damped update to get the new estimate $\\hat{f}^{(t+1)}$.\n   g. Check for convergence by comparing $\\hat{f}^{(t+1)}$ with $\\hat{f}^{(t)}$. If converged, exit; otherwise, continue.\n4. Return the final anchored free energy vector $\\hat{f}$.\nThis procedure is implemented to solve for the free energies for the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef mbar_solver(u_kn, N_k, tol=1e-12, alpha=0.5, max_iter=10000):\n    \"\"\"\n    Solves the Multistate Bennett Acceptance Ratio (MBAR) equations for the\n    dimensionless reduced free energies.\n\n    This function implements a numerically stable, damped fixed-point iteration\n    to find the free energies f_k that satisfy the MBAR equations.\n\n    Args:\n        u_kn (np.ndarray): A matrix of shape (K, N) where u_kn[k, n] is the\n                           reduced potential energy of configuration n evaluated\n                           at the potential of state k. K is the number of states\n                           and N is the total number of configurations.\n        N_k (np.ndarray): An array of shape (K,) containing the number of\n                          configurations sampled from each state.\n        tol (float): The convergence tolerance. Iteration stops when the\n                     maximum absolute change in any f_k is less than this value.\n        alpha (float): The damping factor for the fixed-point iteration.\n        max_iter (int): The maximum number of iterations to perform.\n\n    Returns:\n        np.ndarray: An array of shape (K,) containing the estimated reduced free\n                    energies, anchored such that f_0 = 0.\n    \"\"\"\n    K = u_kn.shape[0]\n\n    # Handle trivial cases: one state or no states. The free energy difference\n    # is by definition 0.\n    if K <= 1:\n        return np.zeros(K)\n\n    # Initialize free energies to zero.\n    f_k = np.zeros(K)\n\n    # Calculate normalized log sample counts. This is numerically stable and\n    # handles states with zero samples (N_k=0 -> log(N_k)=-inf).\n    log_N_k = np.full(K, -np.inf)\n    active_states = N_k > 0\n    if np.any(active_states):\n        log_N_k[active_states] = np.log(N_k[active_states])\n    \n    # Normalizing the log-counts makes the solver invariant to global scaling\n    # of sample counts.\n    log_N_norm = log_N_k - logsumexp(log_N_k[active_states])\n\n    for _ in range(max_iter):\n        f_old = f_k.copy()\n        \n        # The argument for the log-sum-exp over states (k) is:\n        # A_kn = log(N_k) + f_k - u_kn\n        # We use broadcasting to efficiently compute this matrix.\n        log_N_f_k = log_N_norm[:, np.newaxis] + f_k[:, np.newaxis]\n        A_kn = log_N_f_k - u_kn\n        \n        # The log of the denominator term, for each configuration n:\n        # log_D_n = log(sum_k exp(A_kn))\n        # This summation is performed over the state index (axis=0).\n        log_D_n = logsumexp(A_kn, axis=0)\n        \n        # The argument for the log-sum-exp over configurations (n) is:\n        # B_kn = -u_kn - log_D_n\n        B_kn = -u_kn - log_D_n[np.newaxis, :]\n        \n        # Solve for the new, un-anchored free energies:\n        # f_new_k = -log(sum_n exp(B_kn))\n        # This summation is performed over the configuration index (axis=1).\n        f_new_k = -logsumexp(B_kn, axis=1)\n        \n        # Enforce anchoring convention f_0 = 0 by re-centering.\n        f_new_anchored = f_new_k - f_new_k[0]\n        \n        # Apply damped update to improve convergence stability.\n        f_k = f_k + alpha * (f_new_anchored - f_k)\n        \n        # Check for convergence.\n        max_abs_change = np.max(np.abs(f_k - f_old))\n        if max_abs_change < tol:\n            break\n            \n    # Final anchor to ensure f_0 is exactly 0.\n    f_k -= f_k[0]\n    return f_k\n\ndef solve():\n    \"\"\"\n    Main function to construct test cases, run the MBAR solver,\n    and print the results in the specified format.\n    \"\"\"\n    test_cases = [\n        # K, (N_counts), (K_springs), (c_offsets)\n        (3, (200, 150, 150), (1.0, 4.0, 16.0), (0.0, 0.0, 0.0)),\n        (3, (400, 10, 10), (1.0, 2.0, 8.0), (0.0, 5.0, 10.0)),\n        (2, (100, 100), (1.0, 1.0), (0.0, 500.0)),\n        (1, (100,), (2.0,), (0.0,)),\n    ]\n\n    rng = np.random.default_rng(seed=12345)\n    all_results = []\n\n    for case in test_cases:\n        K, N_counts, K_springs, c_offsets = case\n        N_counts_arr = np.array(N_counts)\n        \n        # Generate configurations for each state from its equilibrium distribution.\n        x_samples = []\n        if K > 0:\n            for k in range(K):\n                Nk = N_counts_arr[k]\n                if Nk > 0:\n                    Kk = K_springs[k]\n                    # For a 1D harmonic oscillator with beta=1, distribution is\n                    # Normal(0, 1/K_k).\n                    sigma_k = np.sqrt(1.0 / Kk)\n                    samples_k = rng.normal(loc=0.0, scale=sigma_k, size=Nk)\n                    x_samples.append(samples_k)\n        \n        # Pool all configurations into a single dataset.\n        if not x_samples:\n            x_pooled = np.array([])\n        else:\n            x_pooled = np.concatenate(x_samples)\n        \n        # Construct the reduced potential energy matrix u_kn.\n        N_total = x_pooled.shape[0]\n        u_kn = np.zeros((K, N_total))\n        if K > 0 and N_total > 0:\n            for m in range(K):\n                Km = K_springs[m]\n                cm = c_offsets[m]\n                u_kn[m, :] = 0.5 * Km * x_pooled**2 + cm\n        \n        # Run the MBAR solver.\n        f_k_estimated = mbar_solver(u_kn, N_counts_arr)\n        all_results.extend(f_k_estimated)\n\n    # Format the final results as a single comma-separated list.\n    # Each number is rounded to 6 decimal places.\n    formatted_results = [f\"{val:.6f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}