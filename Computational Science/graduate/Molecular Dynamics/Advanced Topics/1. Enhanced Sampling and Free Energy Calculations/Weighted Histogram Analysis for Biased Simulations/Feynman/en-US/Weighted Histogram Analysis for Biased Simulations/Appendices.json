{
    "hands_on_practices": [
        {
            "introduction": "Many important processes in chemistry and biology, such as the rotation of molecular bonds, are described by periodic collective variables. Incorrectly handling this periodicity is a common and critical source of error in free energy calculations. This exercise  challenges you to think through the physical and algorithmic principles required to correctly treat these systems, ensuring your analysis respects the underlying circular topology and avoids artificial discontinuities in the potential of mean force.",
            "id": "3461108",
            "problem": "A one-dimensional collective variable $s$ measuring a torsional angle is strictly periodic with period $L$, so that physically distinct states correspond to $s \\in [0,L)$ and the distance between two values is defined on the circle. In umbrella sampling, window $i$ applies a harmonic bias $w_i(s)$ centered at $s_{0,i}$, and configurations are sampled according to the canonical ensemble at inverse temperature $\\beta$ with total potential energy $U(s) + w_i(s)$. The canonically biased probability density in window $i$ is therefore proportional to $\\exp\\!\\left[-\\beta\\left(U(s)+w_i(s)\\right)\\right]$, and the unbiased target density $p(s)$ is proportional to $\\exp\\!\\left[-\\beta U(s)\\right]$. Weighted histogram analysis seeks to reconstruct $p(s)$ from many windows by reweighting each sampled configurationâ€™s contribution to bins and then estimating the free energy $F(s)$ via $F(s) = -k_{\\mathrm{B}} T \\ln p(s)$ up to an additive constant, where $k_{\\mathrm{B}}$ is the Boltzmann constant and $T$ is the absolute temperature.\n\nConsider umbrella windows with centers $s_{0,i}$ placed near the periodic boundary, for example $s_{0,i} \\approx L - \\Delta$ for small $\\Delta > 0$. During sampling, many configurations have $s$ values that cross the boundary, e.g., values slightly larger than $L$ or slightly negative when expressed continuously before wrapping. You are tasked with deciding, from first principles, how contributions from such samples should be counted into histogram bins that straddle the boundary and how to prevent artificial discontinuities in the reconstructed $F(s)$.\n\nWhich of the following procedures are correct in ensuring scientifically consistent counting across the boundary and preventing artificial discontinuities in $F(s)$?\n\nA. Build the histogram on $[0,L)$ without any wrapping, assign any sample with $s>L$ to a bin beyond $L$, and discard these contributions during normalization to avoid double counting at the boundary.\n\nB. Map each sampled $s$ to its representative $s' \\in [0,L)$ via $s' = s - L \\left\\lfloor s/L \\right\\rfloor$, assign histogram bins using $s'$, and compute the harmonic bias with the minimum-image distance $d(s',s_{0,i}) = \\min_{n \\in \\mathbb{Z}} \\left| s' - s_{0,i} + nL \\right|$ so that the reweighting factor depends smoothly across the boundary.\n\nC. To enforce continuity, duplicate any sample whose $s$ lies within a small threshold $\\delta$ of either boundary into both the first and last bins, and average its weight across the two to reduce boundary artifacts.\n\nD. When estimating $F(s)$ from the reweighted histogram, impose the periodicity constraint $F(0) = F(L)$ explicitly and, if smoothing is applied to $p(s)$ or $F(s)$, use kernels defined on the circle with geodesic distance on the manifold (i.e., periodic kernels) rather than Euclidean kernels on an interval.\n\nE. Shift any window center $s_{0,i}$ near $L$ by $\\pm L$ during analysis so that $|s - s_{0,i}|$ appears small without using a minimum-image convention, and leave all sampled $s$ values unwrapped so that the histogram has a clear boundary at $s=0$ and $s=L$.\n\nSelect all that apply.",
            "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   **System**: A one-dimensional collective variable $s$ that is strictly periodic with period $L$.\n-   **State Space**: Physically distinct states correspond to $s \\in [0,L)$. The distance metric is defined on a circle.\n-   **Simulation Method**: Umbrella sampling is used.\n-   **Bias Potential**: Window $i$ applies a harmonic bias $w_i(s)$ centered at $s_{0,i}$.\n-   **Ensemble**: Sampling is performed in the canonical ensemble at inverse temperature $\\beta = (k_{\\mathrm{B}} T)^{-1}$.\n-   **Biased Probability Density**: In window $i$, the probability density is proportional to $\\exp[-\\beta(U(s)+w_i(s))]$.\n-   **Unbiased Probability Density**: The target quantity is $p(s) \\propto \\exp[-\\beta U(s)]$.\n-   **Free Energy**: The free energy is defined as $F(s) = -k_{\\mathrm{B}} T \\ln p(s)$ (up to an additive constant).\n-   **Analysis Method**: Weighted histogram analysis (WHAM) is used to reconstruct $p(s)$.\n-   **Challenge**: For windows with centers $s_{0,i}$ near the periodic boundary (e.g., $s_{0,i} \\approx L$), samples cross the boundary (e.g., $s > L$ or $s  0$). The task is to identify correct procedures for handling these samples to ensure consistent counting and prevent artificial discontinuities in $F(s)$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is firmly rooted in the principles of statistical mechanics and computational physics. Umbrella sampling, the weighted histogram analysis method (WHAM), the concept of a periodic collective variable (like a dihedral angle), and the definition of free energy are all standard and accurately described. The challenge of handling periodic boundary conditions is a real and important issue in molecular simulations.\n-   **Well-Posed**: The problem is well-defined. It asks for the correct procedures to handle a specific, common technical challenge in data analysis for molecular simulations. The goal is clear, and the context is specified with sufficient detail.\n-   **Objective**: The problem statement is expressed in precise, objective, and standard scientific terminology. There are no subjective or ambiguous statements.\n\nThe problem does not exhibit any of the invalidity flaws. It is a standard, valid problem in the field of computational molecular science.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation of Principles for Periodic Systems\n\nThe core of the problem lies in correctly treating a system whose coordinate $s$ is periodic. The space of the collective variable is not the real line $\\mathbb{R}$, but a circle $S^1$ of circumference $L$. All physical quantities and mathematical operations must respect this topology.\n\n1.  **Topological Equivalence**: Any two points $s_1$ and $s_2$ are physically identical if $s_1 = s_2 + nL$ for some integer $n \\in \\mathbb{Z}$. Consequently, any single-valued function of the physical state, such as the unbiased potential energy $U(s)$ and the free energy $F(s)$, must be periodic with period $L$: $U(s) = U(s+L)$ and $F(s) = F(s+L)$.\n\n2.  **Distance Metric**: The distance between two points $s_a$ and $s_b$ on a circle is the shortest arc length between them. This is known as the minimum-image distance, given by $d(s_a, s_b) = \\min_{n \\in \\mathbb{Z}} |s_a - s_b + nL|$. A harmonic bias potential, $w_i(s) = \\frac{1}{2}k_i (\\text{distance from } s_{0,i})^2$, must use this metric to be a smooth and physically meaningful function on the circle: $w_i(s) = \\frac{1}{2}k_i [d(s, s_{0,i})]^2$. Using the simple Euclidean distance $|s - s_{0,i}|$ would create an unphysical energy cusp at the periodic boundary for windows centered near it.\n\n3.  **Histogram Binning**: A simulation trajectory yields a sequence of $s$ values that are typically \"unwrapped\" and may fall outside the fundamental domain $[0, L)$. To construct a histogram of the probability density $p(s)$ on the circle, each sampled value of $s$ must be mapped to its unique representative $s'$ in the chosen fundamental domain, e.g., $s' \\in [0, L)$. The correct mapping is $s' = s \\pmod L$, which can be implemented as $s' = s - L \\lfloor s/L \\rfloor$. The histogram is populated using these wrapped coordinates $s'$.\n\n4.  **Continuity and Post-processing**: The resulting free energy profile $F(s)$ must be periodic, meaning $F(0) = F(L)$. While a perfect analysis of infinite data would guarantee this, finite data and numerical methods may introduce a small mismatch. It is a valid step to enforce this periodicity. Furthermore, if any smoothing is applied to the data (e.g., to $p(s)$ or $F(s)$), the smoothing kernel must also respect the circular topology. This requires a periodic kernel (e.g., a wrapped Gaussian), which correctly handles points near the boundary (e.g., at $s=\\epsilon$) by acknowledging their proximity to points at the other end of the interval (e.g., at $s=L-\\epsilon$).\n\nBased on these first principles, we can now evaluate each option.\n\n### Option-by-Option Analysis\n\n**A. Build the histogram on $[0,L)$ without any wrapping, assign any sample with $s>L$ to a bin beyond $L$, and discard these contributions during normalization to avoid double counting at the boundary.**\n\nThis procedure is fundamentally flawed. A sample at $s = L + \\epsilon$ is physically identical to a sample at $s=\\epsilon$. Discarding the sample at $s=L+\\epsilon$ means throwing away valid statistical information. This artificially depletes the sampling of bins near $s=0$, leading to an incorrect probability density and a corresponding artificial barrier in the free energy profile. This approach treats the periodic variable as if it were confined to a box, which contradicts the problem definition.\n**Verdict: Incorrect.**\n\n**B. Map each sampled $s$ to its representative $s' \\in [0,L)$ via $s' = s - L \\left\\lfloor s/L \\right\\rfloor$, assign histogram bins using $s'$, and compute the harmonic bias with the minimum-image distance $d(s',s_{0,i}) = \\min_{n \\in \\mathbb{Z}} \\left| s' - s_{0,i} + nL \\right|$ so that the reweighting factor depends smoothly across the boundary.**\n\nThis procedure correctly addresses the core issues.\n1.  The mapping $s' = s - L \\lfloor s/L \\rfloor$ correctly folds all sampled coordinates into the fundamental domain $[0,L)$, consistent with Principle 3. This ensures every sample is counted in its physically correct location.\n2.  Using the minimum-image distance $d(s', s_{0,i})$ to define the harmonic bias potential is crucial, as explained in Principle 2. This ensures the bias energy $w_i(s)$ is a continuous and smooth function on the circle, which is necessary for the reweighting factor $\\exp[\\beta w_i(s)]$ to be well-behaved across the periodic boundary. This prevents artificial energy jumps and ensures the physics of the biased simulation is correct.\nThis option describes the standard, correct implementation for umbrella sampling with periodic variables.\n**Verdict: Correct.**\n\n**C. To enforce continuity, duplicate any sample whose $s$ lies within a small threshold $\\delta$ of either boundary into both the first and last bins, and average its weight across the two to reduce boundary artifacts.**\n\nThis is an ad-hoc, heuristic procedure that is not founded on rigorous statistical principles. It violates the basic rule of histogramming, where each sample contributes to exactly one bin. Duplicating and averaging sample contributions is a form of data manipulation that distorts the underlying probability distribution. It attempts to cosmetically smooth a boundary artifact that arises from an incorrect treatment, rather than solving the problem at its root by respecting the system's topology. The correct way to achieve continuity is by using a periodic representation from the start.\n**Verdict: Incorrect.**\n\n**D. When estimating $F(s)$ from the reweighted histogram, impose the periodicity constraint $F(0) = F(L)$ explicitly and, if smoothing is applied to $p(s)$ or $F(s)$, use kernels defined on the circle with geodesic distance on the manifold (i.e., periodic kernels) rather than Euclidean kernels on an interval.**\n\nThis option describes correct and often necessary post-processing steps, consistent with Principle 4.\n1.  The true free energy $F(s)$ must be periodic. Imposing the constraint $F(0) = F(L)$ ensures the final result is physically meaningful, correcting for minor discrepancies that can arise from finite sampling.\n2.  If smoothing is performed, using a periodic kernel is essential. A standard Euclidean kernel would treat the points at $s=0$ and $s=L$ as far apart, creating smoothing artifacts at the boundaries. A periodic kernel correctly recognizes that the domain is circular and that $s=0$ is adjacent to $s=L$, thus preserving the continuity and periodicity of the profile.\nBoth practices are sound and consistent with a rigorous treatment of periodic data.\n**Verdict: Correct.**\n\n**E. Shift any window center $s_{0,i}$ near $L$ by $\\pm L$ during analysis so that $|s - s_{0,i}|$ appears small without using a minimum-image convention, and leave all sampled $s$ values unwrapped so that the histogram has a clear boundary at $s=0$ and $s=L$.**\n\nThis is an incorrect and cumbersome workaround that fails to be a general solution. Leaving $s$ values unwrapped constructs a histogram on the real line, which is the wrong topology. Shifting the center $s_{0,i}$ is a \"trick\" to make a simple Euclidean distance calculation $|s-s_{0,i}|$ give the correct minimum-image distance, but it only works for samples $s$ that are close to the shifted center. It does not work for all possible $s$ values and is not equivalent to a proper minimum-image calculation. For example, if $s_{0,i}=0.9L$ is shifted to $-0.1L$, a sample at $s=0.4L$ would have an incorrectly calculated large distance to the center. The robust method is to always compute the minimum-image distance. This option explicitly avoids the correct convention and also fails to bin the data correctly.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{BD}$$"
        },
        {
            "introduction": "The equations of the Weighted Histogram Analysis Method (WHAM) involve sums of exponential terms, which can easily lead to numerical overflow when implemented directly, especially for systems at low temperatures or with strong biasing potentials. This practice  guides you through the derivation of the \"log-sum-exp\" trick, a fundamental technique for ensuring numerical stability. Mastering this method is essential for writing robust scientific code that can handle the wide dynamic range of probabilities encountered in statistical mechanics.",
            "id": "3461097",
            "problem": "Consider a set of $K$ biased Molecular Dynamics (MD) simulations performed at constant temperature $T$ and inverse thermal energy $\\beta = 1/(k_B T)$, where $k_B$ is the Boltzmann constant. Each simulation $k$ imposes a bias potential $W_k(s)$ on a scalar collective variable $s$. Let $s$ be discretized into bins indexed by $i$, with bin centers $s_i$. The Weighted Histogram Analysis Method (WHAM) combines these simulations to estimate the unbiased probability distribution over $s$. In this context, the WHAM reweighting for bin $i$ involves a denominator that aggregates contributions across simulations, defined by\n$$\nD_i = \\sum_{k=1}^{K} n_k \\exp\\!\\left[\\beta f_k - \\beta W_k(s_i)\\right],\n$$\nwhere $n_k \\geq 1$ is the number of uncorrelated samples contributed by simulation $k$ and $f_k$ are reduced free energy offsets determined by self-consistency within WHAM. For large $\\beta$ or stiff biases $W_k(s)$, direct evaluation of $D_i$ can overflow due to exponential terms with large positive arguments. Starting from the canonical ensemble and the basic properties of exponentials and logarithms (monotonicity of $\\exp(\\cdot)$ and the identity $\\ln(\\exp(x)) = x$), derive a numerically stable expression for $\\ln D_i$ that avoids overflow and underflow by recentering the exponents, without altering $D_i$'s value. Your derivation must introduce only quantities already present in the definition of $D_i$ and use elementary operations available in standard numerical libraries. Provide the final closed-form analytical expression for $\\ln D_i$ expressed solely in terms of $\\beta$, $f_k$, $W_k(s_i)$, and $n_k$. No rounding is required and no physical units should be included in the final answer.",
            "solution": "The problem asks for a numerically stable expression for the natural logarithm of the Weighted Histogram Analysis Method (WHAM) denominator, $\\ln(D_i)$. The given expression for $D_i$ is:\n$$\nD_i = \\sum_{k=1}^{K} n_k \\exp\\!\\left[\\beta f_k - \\beta W_k(s_i)\\right]\n$$\nHere, $\\beta$ is the inverse thermal energy, $n_k$ is the number of samples from simulation $k$, $f_k$ is the reduced free energy offset for simulation $k$, and $W_k(s_i)$ is the bias potential of simulation $k$ evaluated at the center of bin $i$.\n\nThe challenge arises when the argument of the exponential function, $\\beta f_k - \\beta W_k(s_i)$, is a large positive number for any simulation $k$. In such cases, the term $\\exp[\\beta f_k - \\beta W_k(s_i)]$ can exceed the floating-point representation capabilities of a computer, a condition known as numerical overflow. A direct computation of $D_i$ followed by taking the logarithm would fail.\n\nTo derive a stable expression for $\\ln(D_i)$, we can use a common algebraic technique often referred to as the \"log-sum-exp\" trick. This involves identifying the largest exponent in the sum, factoring it out, and then applying the properties of logarithms. This procedure does not alter the mathematical value of $D_i$ but recasts the calculation into a numerically stable form.\n\nLet's define the exponent for the $k$-th term as $A_k(s_i) = \\beta f_k - \\beta W_k(s_i)$. The expression for $D_i$ becomes:\n$$\nD_i = \\sum_{k=1}^{K} n_k \\exp(A_k(s_i))\n$$\nNow, let's define a constant $C_i$ for each bin $i$ as the maximum value of these exponents across all $K$ simulations:\n$$\nC_i = \\max_{k' \\in \\{1, \\dots, K\\}} \\left\\{ A_{k'}(s_i) \\right\\} = \\max_{k' \\in \\{1, \\dots, K\\}} \\left\\{ \\beta f_{k'} - \\beta W_{k'}(s_i) \\right\\}\n$$\nWe use $k'$ as a dummy index for the maximization to distinguish it from the summation index $k$.\n\nWe can now rewrite the expression for $D_i$ by adding and subtracting $C_i$ inside each exponent. This is a null operation that preserves the value of each term.\n$$\nD_i = \\sum_{k=1}^{K} n_k \\exp\\left[ A_k(s_i) - C_i + C_i \\right]\n$$\nUsing the property of exponentials, $\\exp(a+b) = \\exp(a)\\exp(b)$, we can split the exponential:\n$$\nD_i = \\sum_{k=1}^{K} n_k \\exp\\left[ A_k(s_i) - C_i \\right] \\exp(C_i)\n$$\nSince $\\exp(C_i)$ is a constant with respect to the summation index $k$, it can be factored out of the sum:\n$$\nD_i = \\exp(C_i) \\left( \\sum_{k=1}^{K} n_k \\exp\\left[ A_k(s_i) - C_i \\right] \\right)\n$$\nThis expression is mathematically identical to the original definition of $D_i$. Now, we can take the natural logarithm of both sides to find $\\ln(D_i)$:\n$$\n\\ln(D_i) = \\ln\\left( \\exp(C_i) \\left( \\sum_{k=1}^{K} n_k \\exp\\left[ A_k(s_i) - C_i \\right] \\right) \\right)\n$$\nUsing the property of logarithms, $\\ln(xy) = \\ln(x) + \\ln(y)$, we separate the two factors:\n$$\n\\ln(D_i) = \\ln\\left( \\exp(C_i) \\right) + \\ln\\left( \\sum_{k=1}^{K} n_k \\exp\\left[ A_k(s_i) - C_i \\right] \\right)\n$$\nBy the fundamental identity $\\ln(\\exp(x)) = x$, the first term simplifies to $C_i$:\n$$\n\\ln(D_i) = C_i + \\ln\\left( \\sum_{k=1}^{K} n_k \\exp\\left[ A_k(s_i) - C_i \\right] \\right)\n$$\nThis form is numerically stable. The argument of the exponential function inside the sum is now $A_k(s_i) - C_i = A_k(s_i) - \\max_{k'}\\{A_{k'}(s_i)\\}$. By the definition of the maximum, this difference is always less than or equal to zero for all $k$. Therefore, the value of $\\exp[A_k(s_i) - C_i]$ will be between $0$ and $1$, preventing overflow. The term for which the exponent is maximized will have an adjusted exponent of $0$, resulting in $\\exp(0)=1$, which guarantees that the argument of the final logarithm is at least $1$ (since $n_k \\ge 1$), thus preventing underflow in the logarithm.\n\nTo provide the final expression solely in terms of the initial quantities, we substitute the full expressions for $A_k(s_i)$ and $C_i$. Since $\\beta > 0$, it can be factored out of the maximization:\n$$\nC_i = \\beta \\max_{k' \\in \\{1, \\dots, K\\}} \\left\\{ f_{k'} - W_{k'}(s_i) \\right\\}\n$$\nSubstituting this and $A_k(s_i)$ into our derived expression yields the final answer:\n$$\n\\ln(D_i) = \\beta \\max_{k' \\in \\{1, \\dots, K\\}} \\left\\{ f_{k'} - W_{k'}(s_i) \\right\\} + \\ln\\left( \\sum_{k=1}^{K} n_k \\exp\\left[ \\beta(f_k - W_k(s_i)) - \\beta \\max_{k'' \\in \\{1, \\dots, K\\}} \\left\\{ f_{k''} - W_{k''}(s_i) \\right\\} \\right] \\right)\n$$\nThis expression provides a numerically robust method for calculating $\\ln(D_i)$, avoiding overflow and underflow issues inherent in the direct evaluation of the original formula.",
            "answer": "$$\n\\boxed{\\beta \\max_{k' \\in \\{1, \\dots, K\\}} \\left( f_{k'} - W_{k'}(s_i) \\right) + \\ln\\left( \\sum_{k=1}^{K} n_k \\exp\\left( \\beta \\left( f_k - W_k(s_i) \\right) - \\beta \\max_{k'' \\in \\{1, \\dots, K\\}} \\left( f_{k''} - W_{k''}(s_i) \\right) \\right) \\right)}\n$$"
        },
        {
            "introduction": "A powerful way to solidify your understanding of an algorithm is to implement it and test its validity. This comprehensive exercise  serves as a capstone project where you will not only implement the core iterative WHAM equations but also design a crucial self-consistency check. By comparing the original biased histograms to those reconstructed from the final unbiased free energy profile, you will learn how to build a powerful diagnostic test to verify that your simulation data are mutually consistent and that your analysis is trustworthy.",
            "id": "3461103",
            "problem": "You are given a collection of biased molecular dynamics windows sampling a scalar reaction coordinate $s$, each subject to a known harmonic umbrella potential. The task is to design and implement a numerical self-consistency test grounded in first principles of statistical mechanics to verify that per-window distributions reconstructed from a single, globally unbiased distribution match observed histograms within statistical error.\n\nBegin from the canonical ensemble and the concept of importance sampling for biased simulations. Consider an unknown unbiased probability density $p(s)$ over a discretized set of $M$ bins with centers $s_i$ and uniform bin width $\\Delta s$. Each window $k$ applies a known bias potential $W_k(s)$ and generates a histogram of counts $n_{k,i}$ over the bins, with a total sample count $N_k = \\sum_{i=1}^M n_{k,i}$. The inverse thermal energy is $\\beta$, with units $1/\\text{energy}$, and all energies are in consistent arbitrary units. The unbiased free energy along $s$ is $F(s)$ such that $p(s) \\propto \\exp[-\\beta F(s)]$. The bias potential is harmonic, $W_k(s) = \\frac{1}{2} K (s - c_k)^2$, with stiffness $K$ and center $c_k$.\n\nYour program must:\n- From the canonical ensemble and importance sampling principles, derive a practical algorithm to estimate the global unbiased discrete distribution $p_i \\approx p(s_i)$ from all windows jointly, and the per-window normalization constants $f_k$ that ensure each biased distribution normalizes.\n- Compute for each window $k$ a back-computed discrete distribution $\\tilde{p}_{k,i}$ that would be expected in window $k$ if the global $p_i$ is correct and the bias $W_k$ is applied at inverse temperature $\\beta$. This back-computed distribution must be normalized to one over $s$ with the measure $\\Delta s$, that is $\\sum_i \\tilde{p}_{k,i} \\Delta s = 1$.\n- Compare $\\tilde{p}_{k,i}$ to the observed empirical per-window histograms $\\hat{p}_{k,i} = n_{k,i} / N_k$ using a statistically principled criterion that accounts for finite sampling noise. Assume counts are independent and approximately Poisson distributed with mean $N_k \\tilde{p}_{k,i}$ per bin, so the standard deviation per bin is $\\sqrt{N_k \\tilde{p}_{k,i}}$. Define a decision rule that declares a window $k$ self-consistent if and only if for all bins $i$,\n$$\n\\left| n_{k,i} - N_k \\tilde{p}_{k,i} \\right| \\le z \\sqrt{N_k \\tilde{p}_{k,i}},\n$$\nwith $z = 3$ as a fixed threshold. A test case passes if every window in that case is self-consistent.\n\nYour implementation must be fully deterministic and use only the provided parameters. For reproducibility, whenever synthetic counts are needed, use a fixed seed for the random number generator to draw counts from a multinomial distribution per window; otherwise, round expected counts deterministically and adjust the final bin to preserve $N_k$. The algorithm you derive must be scientifically sound and start from first principles; do not rely on black-box formulas without justification.\n\nTest Suite:\nFor all cases, the unbiased free energy is $F(s) = \\frac{1}{2} k_{\\text{ub}} s^2$ and the bias potentials are harmonic $W_k(s) = \\frac{1}{2} K (s - c_k)^2$. Use a uniform grid with $M$ bins, centers $s_i$ spanning $[s_{\\min}, s_{\\max}]$, and $\\Delta s = (s_{\\max} - s_{\\min})/M$. All energies are in the same arbitrary units and $\\beta$ has units $1/\\text{energy}$.\n\n- Case A (well-sampled and consistent):\n    - $\\beta = 1.0$\n    - $k_{\\text{ub}} = 3.0$\n    - $K = 10.0$\n    - $c_k = [-1.0, 0.0, 1.0]$ for $k = 1,2,3$\n    - $N_k = [8000, 6000, 7000]$ for $k = 1,2,3$\n    - $s_{\\min} = -2.5$, $s_{\\max} = 2.5$, $M = 81$\n    - Use random seed $12345$ for generating synthetic counts.\n    - All windows are physically consistent with the given $\\beta$.\n\n- Case B (lower sampling but consistent):\n    - $\\beta = 1.0$\n    - $k_{\\text{ub}} = 3.0$\n    - $K = 10.0$\n    - $c_k = [-1.0, 0.0, 1.0]$\n    - $N_k = [1500, 800, 1000]$\n    - $s_{\\min} = -2.5$, $s_{\\max} = 2.5$, $M = 81$\n    - Use random seed $24680$ for generating synthetic counts.\n    - All windows are physically consistent with the given $\\beta$.\n\n- Case C (intentionally inconsistent to induce failure):\n    - Analysis assumes $\\beta = 1.0$\n    - $k_{\\text{ub}} = 3.0$\n    - $K = 10.0$\n    - $c_k = [-1.0, 0.0, 1.0]$\n    - $N_k = [7000, 9000, 7000]$\n    - $s_{\\min} = -2.5$, $s_{\\max} = 2.5$, $M = 81$\n    - Use random seed $13579$ for generating synthetic counts.\n    - Synthetic counts for window $k=2$ are generated at $\\beta_{\\text{gen},2} = 0.5$ (mismatch), while $k=1$ and $k=3$ use $\\beta_{\\text{gen},1} = \\beta_{\\text{gen},3} = 1.0$.\n\nProgram Requirements:\n- Implement a scientifically justified algorithm to estimate $p_i$ and $f_k$ jointly from the observed histograms across windows.\n- Construct back-computed per-window distributions $\\tilde{p}_{k,i}$ and evaluate the Poisson deviation rule with $z=3$.\n- For each case, output a boolean indicating whether the case passes (all windows self-consistent) or fails.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True]\").\n\nNo angles are involved; there are no additional physical units in the output, which is purely boolean. The final results must be computed from the given parameters without external input or files.",
            "solution": "The user has requested the design and implementation of a numerical self-consistency test for data from biased molecular simulations, based on the principles of the Weighted Histogram Analysis Method (WHAM). The problem is deemed valid as it is scientifically grounded in established statistical mechanics, well-posed, and all necessary parameters for a unique, verifiable solution are provided. A minor notational ambiguity in the problem's decision rule will be resolved based on dimensional consistency and first principles.\n\n### 1. Theoretical Foundation\n\nThe problem centers on combining data from multiple simulations (windows), each biased with a potential $W_k(s)$, to reconstruct a single, underlying unbiased probability distribution $p(s)$ for a reaction coordinate $s$.\n\nIn the canonical ensemble, the unbiased probability density of finding the system at coordinate $s$ is related to the free energy $F(s)$ by:\n$$p(s) = \\frac{e^{-\\beta F(s)}}{\\int e^{-\\beta F(s')} ds'}$$\nwhere $\\beta$ is the inverse thermal energy. When a bias potential $W_k(s)$ is applied in simulation $k$, the observed (biased) probability density $p_k(s)$ becomes:\n$$p_k(s) = \\frac{e^{-\\beta (F(s) + W_k(s))}}{\\int e^{-\\beta (F(s') + W_k(s'))} ds'}$$\nFrom these two equations, we can relate the biased and unbiased densities:\n$$p_k(s) = p(s) e^{-\\beta W_k(s)} \\frac{\\int e^{-\\beta F(s')} ds'}{\\int e^{-\\beta (F(s') + W_k(s'))} ds'} = p(s) e^{-\\beta W_k(s)} e^{\\beta f_k}$$\nHere, $f_k = -\\frac{1}{\\beta} \\ln \\left( \\int p(s') e^{-\\beta W_k(s')} ds' \\right)$ is the free energy change associated with applying the bias potential $W_k(s)$. The term $e^{\\beta f_k}$ serves as the normalization factor.\n\n### 2. The Weighted Histogram Analysis Method (WHAM)\n\nWHAM provides a self-consistent set of equations to find the optimal estimates for the unbiased probability distribution and the free energy shifts $f_k$ from a set of histograms. We discretize the coordinate $s$ into $M$ bins, indexed by $i$, with centers $s_i$ and width $\\Delta s$. Let $P_i$ be the probability *mass* in bin $i$ (such that $\\sum_i P_i = 1$), $n_{k,i}$ be the number of samples from simulation $k$ in bin $i$, and $N_k = \\sum_i n_{k,i}$ be the total samples in simulation $k$.\n\nThe WHAM equations, derived from maximizing the likelihood of observing the given histograms $\\{n_{k,i}\\}$, are a set of self-consistent equations for the probability masses $P_i$ and the free energies $f_k$:\n\n1.  **Estimate for Unbiased Probability Mass:** The probability mass of bin $i$ is estimated from the total counts in that bin across all simulations, normalized by an effective total number of samples.\n    $$P_i = \\frac{\\sum_{k} n_{k,i}}{\\sum_{j} N_j e^{\\beta(f_j - W_{j,i})}}$$\n    where $W_{j,i} = W_j(s_i)$ is the bias potential of simulation $j$ evaluated at the center of bin $i$.\n\n2.  **Estimate for Free Energy Shifts:** The free energy shift $f_k$ for each simulation is determined by the normalization condition of its biased distribution.\n    $$e^{-\\beta f_k} = \\sum_{i} P_i e^{-\\beta W_{k,i}}$$\n\nThese two equations depend on each other and must be solved iteratively until $P_i$ and $f_k$ converge.\n\n### 3. Algorithmic Implementation\n\n**Step 1: Data Generation**\nFor each test case, synthetic histogram data $n_{k,i}$ must be generated first.\n- The true unbiased free energy is given as $F_{\\text{true}}(s) = \\frac{1}{2} k_{\\text{ub}} s^2$.\n- The true biased probability mass for window $k$ in bin $i$ is $P_{k, \\text{true}, i} \\propto e^{-\\beta_{\\text{gen}}(F_{\\text{true}}(s_i) + W_k(s_i))}$. Note that for Case C, the generation temperature $\\beta_{\\text{gen}}$ for one window differs from the analysis temperature $\\beta$.\n- These probabilities are normalized such that $\\sum_i P_{k, \\text{true}, i} = 1$.\n- For each window $k$, $N_k$ samples are drawn from a multinomial distribution with probabilities $P_{k, \\text{true}, i}$ using a seeded random number generator to produce the counts $n_{k,i}$.\n\n**Step 2: Iterative WHAM Solution**\nThe WHAM equations are solved self-consistently:\n1.  Initialize all $f_k = 0$.\n2.  Begin an iteration loop:\n    a. Calculate $P_i$ for all bins using the current $f_k$ values and the provided $n_{k,i}$ histograms via Equation (1).\n    b. Normalize the resulting $P_i$ such that $\\sum_i P_i = 1$. This is a crucial step for stability.\n    c. Calculate new values $f_k^{\\text{new}}$ for all windows using the updated $P_i$ via Equation (2). To prevent a uniform drift in the $f_k$ values, one value is pinned (e.g., $f_1 = 0$) by shifting all $f_k^{\\text{new}}$ by a constant.\n    d. Check if the change in $f_k$ values, e.g., $\\max_k |f_k^{\\text{new}} - f_k|$, is below a small tolerance. If so, exit the loop. Otherwise, update $f_k \\leftarrow f_k^{\\text{new}}$ and continue.\nNumerical stability is ensured by using the log-sum-exp trick to handle the summations of exponential terms, preventing overflow errors.\n\n**Step 3: Self-Consistency Validation**\nOnce the converged global probability distribution $P_i$ is obtained, we perform the consistency check for each window $k$:\n1.  Obtain the unbiased probability *density* $p_i = P_i / \\Delta s$.\n2.  For each window $k$, calculate the \"back-computed\" or reconstructed biased probability density $\\tilde{p}_{k,i}$ that would be expected given our model:\n    $$\\tilde{p}_{k,i} = \\frac{p_i e^{-\\beta W_{k,i}}}{\\sum_{j} p_j e^{-\\beta W_{k,j}} \\Delta s}$$\n    The denominator ensures that $\\sum_i \\tilde{p}_{k,i} \\Delta s = 1$, as required.\n3.  Calculate the expected number of counts in bin $i$ for window $k$: $\\langle n_{k,i} \\rangle = N_k \\tilde{p}_{k,i} \\Delta s$.\n4.  Apply the statistical decision rule. The problem specifies the rule as $\\left| n_{k,i} - N_k \\tilde{p}_{k,i} \\right| \\le z \\sqrt{N_k \\tilde{p}_{k,i}}$. This expression is dimensionally inconsistent, as $n_{k,i}$ is a dimensionless count, while $N_k \\tilde{p}_{k,i}$ has units of $1/\\text{length}$. The clear scientific intent, grounded in Poisson statistics for count data, is to compare the observed counts $n_{k,i}$ to the expected counts $\\langle n_{k,i} \\rangle$. The rule is therefore corrected to:\n    $$|n_{k,i} - \\langle n_{k,i} \\rangle| \\le z \\sqrt{\\langle n_{k,i} \\rangle}$$\n    where $z=3$. A window $k$ is deemed self-consistent if and only if this inequality holds for all bins $i$.\n5.  A test case passes if all its constituent windows are found to be self-consistent. For Case C, the inconsistent data from window 2 (generated at a different temperature) is expected to cause a large deviation between observed and expected counts for that window, leading to a failure of the test.\n\nThis procedure provides a rigorous, first-principles-based method to validate the internal consistency of a set of biased simulation data.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(case_params):\n    \"\"\"\n    Processes a single test case for WHAM self-consistency.\n\n    This function performs three main steps:\n    1. Generates synthetic histogram data based on a true underlying free energy profile.\n    2. Uses the Weighted Histogram Analysis Method (WHAM) to compute a global,\n       unbiased probability distribution from the biased histograms.\n    3. Performs a self-consistency check by back-calculating the expected distribution\n       in each window and comparing it to the observed histogram using a statistical criterion.\n    \"\"\"\n    # Unpack parameters for the test case\n    beta_gen = case_params['beta']\n    analysis_beta = case_params['analysis_beta']\n    k_ub = case_params['k_ub']\n    K = case_params['K']\n    c_k = np.array(case_params['c_k'])\n    N_k = np.array(case_params['N_k'])\n    s_min, s_max, M = case_params['s_min'], case_params['s_max'], case_params['M']\n    seed = case_params['seed']\n\n    num_windows = len(c_k)\n\n    # Step 1: Setup grid and generate synthetic count data (n_ki)\n    s_i, delta_s = np.linspace(s_min, s_max, M, retstep=True)\n    if M == 1 and delta_s == 0: delta_s = s_max - s_min\n    \n    n_ki = np.zeros((num_windows, M))\n    rng = np.random.default_rng(seed)\n\n    F_true_i = 0.5 * k_ub * s_i**2\n    W_ki_matrix = 0.5 * K * (s_i[np.newaxis, :] - c_k[:, np.newaxis])**2\n\n    for k in range(num_windows):\n        current_gen_beta = beta_gen[k] if isinstance(beta_gen, list) else beta_gen\n        \n        # Biased free energy for data generation\n        F_biased_true_i = F_true_i + W_ki_matrix[k, :]\n        log_prob_mass = -current_gen_beta * F_biased_true_i\n        \n        # Stabilize with log-sum-exp before taking exp\n        log_prob_mass -= np.max(log_prob_mass)\n        prob_mass = np.exp(log_prob_mass)\n        prob_mass /= np.sum(prob_mass)\n        \n        # Generate counts using multinomial distribution\n        n_ki[k, :] = rng.multinomial(N_k[k], prob_mass)\n\n    # Step 2: WHAM analysis using the specified analysis_beta\n    beta = analysis_beta\n    f_k = np.zeros(num_windows)\n    \n    max_iter = 5000\n    tol = 1e-10\n\n    for _ in range(max_iter):\n        f_k_old = f_k.copy()\n        \n        # Update unbiased probability mass P_i\n        log_denom_terms = beta * (f_k[:, np.newaxis] - W_ki_matrix)\n        max_log_denom = np.max(log_denom_terms, axis=0)\n        denom_sum = np.sum(N_k[:, np.newaxis] * np.exp(log_denom_terms - max_log_denom), axis=0)\n        \n        n_i = np.sum(n_ki, axis=0)\n        \n        P_i = np.zeros(M)\n        valid_bins = denom_sum > 0\n        P_i[valid_bins] = n_i[valid_bins] / (np.exp(max_log_denom[valid_bins]) * denom_sum[valid_bins])\n        \n        # Normalize probability mass\n        P_i_sum = np.sum(P_i)\n        if P_i_sum > 0:\n            P_i /= P_i_sum\n\n        # Update free energy shifts f_k\n        log_sum_terms = -beta * W_ki_matrix\n        max_log_sum = np.max(log_sum_terms, axis=1)\n        sum_val = np.sum(P_i[np.newaxis, :] * np.exp(log_sum_terms - max_log_sum[:, np.newaxis]), axis=1)\n        \n        f_k_new = - (1 / beta) * (max_log_sum + np.log(sum_val))\n        f_k_new -= f_k_new[0]  # Pin f_1 to 0 to prevent drift\n        \n        if np.max(np.abs(f_k_new - f_k_old))  tol:\n            break\n        f_k = f_k_new\n\n    # Step 3: Self-consistency check\n    z = 3.0\n    case_is_consistent = True\n    p_i = P_i / delta_s\n\n    for k in range(num_windows):\n        log_p_tilde_unnorm = np.log(p_i, where=p_i > 0, out=np.full_like(p_i, -np.inf)) - beta * W_ki_matrix[k, :]\n        \n        max_log_p = np.max(log_p_tilde_unnorm)\n        if np.isinf(max_log_p):\n            p_tilde_ki = np.zeros_like(p_i)\n        else:\n            log_Z_k = max_log_p + np.log(np.sum(np.exp(log_p_tilde_unnorm - max_log_p)) * delta_s)\n            p_tilde_ki = np.exp(log_p_tilde_unnorm - log_Z_k)\n            \n        expected_n_ki = N_k[k] * p_tilde_ki * delta_s\n        \n        diff = np.abs(n_ki[k, :] - expected_n_ki)\n        threshold = z * np.sqrt(expected_n_ki)\n        \n        if not np.all(diff = threshold):\n            case_is_consistent = False\n            break\n            \n    return case_is_consistent\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'beta': 1.0,\n            'analysis_beta': 1.0,\n            'k_ub': 3.0, 'K': 10.0, 'c_k': [-1.0, 0.0, 1.0], 'N_k': [8000, 6000, 7000],\n            's_min': -2.5, 's_max': 2.5, 'M': 81, 'seed': 12345\n        },\n        {\n            'beta': 1.0,\n            'analysis_beta': 1.0,\n            'k_ub': 3.0, 'K': 10.0, 'c_k': [-1.0, 0.0, 1.0], 'N_k': [1500, 800, 1000],\n            's_min': -2.5, 's_max': 2.5, 'M': 81, 'seed': 24680\n        },\n        {\n            'beta': [1.0, 0.5, 1.0],\n            'analysis_beta': 1.0,\n            'k_ub': 3.0, 'K': 10.0, 'c_k': [-1.0, 0.0, 1.0], 'N_k': [7000, 9000, 7000],\n            's_min': -2.5, 's_max': 2.5, 'M': 81, 'seed': 13579\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}