{
    "hands_on_practices": [
        {
            "introduction": "Principal Component Analysis serves as a powerful tool for reducing the immense dimensionality of a molecular trajectory to a few essential collective variables. This exercise provides a foundational, hands-on application by connecting the mathematical abstraction of PCA to the physical process of barrier crossing in a simple bistable system . By simulating dynamics on a 2D toy potential and evaluating how well the leading principal components reconstruct a known reaction coordinate, you will build concrete intuition for how PCA identifies and quantifies large-amplitude conformational changes.",
            "id": "3437411",
            "problem": "You are given a two-dimensional bistable toy potential intended to mimic barrier crossing in molecular dynamics. Your task is to simulate samples from the overdamped Langevin dynamics in this potential, perform Principal Component Analysis (PCA) of the sampled molecular trajectory, and determine, for each test case, the minimum number of principal components needed to reconstruct the barrier-crossing coordinate within a specified mean squared error threshold.\n\nUse the following fundamental and widely accepted base in your derivation and implementation:\n- Overdamped Langevin dynamics for a particle at inverse temperature $\\beta$ in a potential $U(\\mathbf{q})$ evolves according to the stochastic differential equation $d \\mathbf{q}_t = - \\nabla U(\\mathbf{q}_t) \\, dt + \\sqrt{2 \\beta^{-1}} \\, d \\mathbf{W}_t$, where $\\mathbf{W}_t$ is a standard Wiener process. In discrete time with time step $\\Delta t$, the Euler–Maruyama update reads $\\mathbf{q}_{n+1} = \\mathbf{q}_n - \\Delta t \\, \\nabla U(\\mathbf{q}_n) + \\sqrt{2 \\Delta t / \\beta} \\, \\boldsymbol{\\xi}_n$, where $\\boldsymbol{\\xi}_n$ is a vector of independent standard normal variables.\n- The barrier-crossing coordinate is defined as the projection onto the unstable eigenvector of the Hessian of the potential at the saddle point. For a saddle at $\\mathbf{q}^\\ast = \\mathbf{0}$, the Hessian is the matrix of second derivatives $H_{ij} = \\partial^2 U / \\partial q_i \\partial q_j \\big|_{\\mathbf{q}=\\mathbf{0}}$. Its eigenvector associated with the smallest eigenvalue gives the direction of negative curvature, which we denote $\\mathbf{v}_{\\mathrm{b}}$. The barrier-crossing scalar coordinate is $s = \\mathbf{v}_{\\mathrm{b}}^\\top \\mathbf{q}$.\n- Principal Component Analysis (PCA) is defined as follows: given mean-centered data matrix $X \\in \\mathbb{R}^{N \\times d}$, compute its singular value decomposition $X = U \\Sigma V^\\top$, with columns of $V$ as principal axes. The $k$-component reconstruction of centered data is $X_k = X V_k V_k^\\top$, where $V_k$ contains the first $k$ columns of $V$. The full reconstruction in the original coordinates is obtained by adding back the mean.\n\nConsider the two-dimensional potential\n$$\nU(x,y) = \\left(x^2 - a^2\\right)^2 + \\frac{k_y}{2} y^2 + c \\, x y,\n$$\nwhere $a > 0$ controls the positions of the wells along the $x$-axis, $k_y > 0$ is the stiffness along $y$, and $c$ couples $x$ and $y$. All quantities are dimensionless; no physical units are required.\n\nTasks to implement for each test case:\n1. Simulate an overdamped Langevin trajectory using the Euler–Maruyama scheme with the specified parameters $(a, k_y, c, \\beta, \\Delta t, N_{\\mathrm{steps}}, N_{\\mathrm{burn}})$. Use initial condition $\\mathbf{q}_0 = (a, 0)$, discard the first $N_{\\mathrm{burn}}$ steps as burn-in, and keep the remaining samples without subsampling.\n2. Compute the Hessian $H$ of $U$ at $(x,y) = (0,0)$ and obtain the normalized eigenvector $\\mathbf{v}_{\\mathrm{b}}$ associated with the smallest eigenvalue. For any sample $\\mathbf{q}$, define $s = \\mathbf{v}_{\\mathrm{b}}^\\top \\mathbf{q}$ as the barrier-crossing scalar coordinate.\n3. Perform PCA on the centered samples. For each $k \\in \\{0,1,2\\}$, form the $k$-component reconstruction $\\widehat{\\mathbf{q}}^{(k)}$ of each sample. For $k = 0$, the reconstruction is the sample mean. For $k \\ge 1$, use the projection onto the first $k$ principal axes. For each $k$, compute the reconstructed scalar coordinate $\\hat{s}^{(k)} = \\mathbf{v}_{\\mathrm{b}}^\\top \\widehat{\\mathbf{q}}^{(k)}$.\n4. For each $k$, compute the mean squared error\n$$\n\\mathrm{MSE}(k) = \\frac{1}{N} \\sum_{i=1}^N \\left(\\hat{s}^{(k)}_i - s_i\\right)^2.\n$$\n5. Let $\\varepsilon$ be the error threshold for the test case. Determine the minimal $k \\in \\{0,1,2\\}$ such that $\\mathrm{MSE}(k) \\le \\varepsilon$. If multiple $k$ satisfy the inequality, choose the smallest. Report that minimal $k$.\n\nThe test suite consists of three cases. Each case specifies the tuple $(a, k_y, c, \\beta, \\Delta t, N_{\\mathrm{steps}}, N_{\\mathrm{burn}}, \\varepsilon)$:\n- Case $1$: $(1.2, 5.0, 1.0, 0.7, 5 \\times 10^{-4}, 200000, 20000, 10.0)$.\n- Case $2$: $(1.2, 5.0, 1.0, 0.7, 5 \\times 10^{-4}, 200000, 20000, 0.2)$.\n- Case $3$: $(1.2, 5.0, 1.0, 0.7, 5 \\times 10^{-4}, 200000, 20000, 10^{-10})$.\n\nNotes and constraints:\n- All computations must be done in dimensionless form; no physical units are required in the output.\n- The initial condition, numerical time step, and number of steps are specified above and must be used exactly as given.\n- Randomness must be controlled with a fixed seed so that results are reproducible; use a distinct integer seed for each test case derived deterministically from the case index.\n- The final program must not read any input; it must construct the test cases internally and output a single line with the results.\n\nFinal output format:\n- Your program should produce a single line of output containing the minimal $k$ for each test case as a comma-separated list enclosed in square brackets, for example, $[k_1,k_2,k_3]$. The numbers $k_i$ must be integers.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of statistical mechanics and linear algebra, well-posed with a clear objective and sufficient data, and formulated objectively. It constitutes a standard computational exercise in the field of molecular dynamics. We proceed with the solution.\n\nThe core task is to determine the minimum number of principal components required to reconstruct a specific collective variable—the barrier-crossing coordinate—from a simulated molecular trajectory within a given error tolerance. This involves a sequence of steps: simulating the system's dynamics, defining the coordinate of interest, performing Principal Component Analysis (PCA), and evaluating the reconstruction error.\n\nFirst, we define the system. The particle's motion occurs in a two-dimensional potential given by\n$$\nU(x,y) = \\left(x^2 - a^2\\right)^2 + \\frac{k_y}{2} y^2 + c \\, x y\n$$\nThis potential features two wells, making it a suitable model for studying barrier-crossing events. The dynamics are governed by the overdamped Langevin equation, which describes the motion of a particle subject to a potential force, friction, and a random thermal force. In its discretized form, the Euler-Maruyama scheme gives the update rule for the particle's position vector $\\mathbf{q}_n = (x_n, y_n)$ at step $n$:\n$$\n\\mathbf{q}_{n+1} = \\mathbf{q}_n - \\Delta t \\, \\nabla U(\\mathbf{q}_n) + \\sqrt{\\frac{2 \\Delta t}{\\beta}} \\, \\boldsymbol{\\xi}_n\n$$\nwhere $\\Delta t$ is the time step, $\\beta$ is the inverse temperature, $\\boldsymbol{\\xi}_n$ is a vector of independent random numbers drawn from a standard normal distribution, and $\\nabla U(\\mathbf{q})$ is the gradient of the potential:\n$$\n\\nabla U(x,y) = \\begin{pmatrix} \\frac{\\partial U}{\\partial x} \\\\ \\frac{\\partial U}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 4x(x^2 - a^2) + c y \\\\ k_y y + c x \\end{pmatrix}\n$$\nFollowing the problem specification, we simulate a trajectory starting from $\\mathbf{q}_0 = (a, 0)$ for $N_{\\mathrm{steps}}$ steps, and discard the initial $N_{\\mathrm{burn}}$ steps to ensure the system has reached equilibrium. This leaves $N = N_{\\mathrm{steps}} - N_{\\mathrm{burn}}$ samples for analysis.\n\nNext, we define the barrier-crossing coordinate, $s$. This coordinate is physically interpreted as the direction of slowest motion away from the transition state, which lies along the path of highest energy between the two stable states (wells). Mathematically, it is defined as the projection of the position vector $\\mathbf{q}$ onto the direction of maximum instability at the saddle point of the potential. The primary saddle point is located at the origin, $\\mathbf{q}^\\ast = (0,0)$. The local geometry of the potential is characterized by the Hessian matrix of second derivatives at this point:\n$$\nH = \\left. \\begin{pmatrix} \\frac{\\partial^2 U}{\\partial x^2} & \\frac{\\partial^2 U}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 U}{\\partial y \\partial x} & \\frac{\\partial^2 U}{\\partial y^2} \\end{pmatrix} \\right|_{\\mathbf{q}=\\mathbf{0}} = \\begin{pmatrix} -4a^2 & c \\\\ c & k_y \\end{pmatrix}\n$$\nSince $H$ is a real symmetric matrix, its eigenvalues are real and its eigenvectors are orthogonal. A saddle point has at least one negative eigenvalue. The eigenvector $\\mathbf{v}_{\\mathrm{b}}$ corresponding to the smallest (most negative) eigenvalue points along the direction of negative curvature, which represents the barrier-crossing direction. We find this eigenvector and normalize it to unit length. The barrier-crossing scalar coordinate for any sample position $\\mathbf{q}_i$ is then given by the projection $s_i = \\mathbf{v}_{\\mathrm{b}}^\\top \\mathbf{q}_i$.\n\nThe third step is to perform Principal Component Analysis (PCA) on the set of $N$ trajectory samples, which we denote as the data matrix $Q \\in \\mathbb{R}^{N \\times 2}$. PCA identifies the principal axes of variation in the data. First, we compute the mean position $\\boldsymbol{\\mu} = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{q}_i$ and form the mean-centered data matrix $X$, where each row is $X_i = \\mathbf{q}_i - \\boldsymbol{\\mu}$. The singular value decomposition (SVD) of $X$ is computed as $X = U \\Sigma V^\\top$. The columns of the orthogonal matrix $V \\in \\mathbb{R}^{2 \\times 2}$ are the principal components (axes) of the data.\n\nWe then reconstruct the trajectory using a limited number of principal components, $k \\in \\{0, 1, 2\\}$.\nFor $k=0$, the reconstruction is simply the mean of the data: $\\widehat{\\mathbf{q}}_i^{(0)} = \\boldsymbol{\\mu}$ for all $i$. This represents the best zero-dimensional approximation.\nFor $k \\in \\{1, 2\\}$, the reconstruction of the centered data is obtained by projecting the data onto the first $k$ principal axes and then transforming back to the original coordinate space: $X_k = X V_k V_k^\\top$, where $V_k$ is the matrix containing the first $k$ columns of $V$. The full reconstruction is obtained by adding the mean back: $\\widehat{\\mathbf{q}}_i^{(k)} = \\boldsymbol{\\mu} + (X_k)_i$. For $k=2$, we use all principal components, so $V_2=V$. Since $V$ is orthogonal ($V V^\\top = I$), the reconstruction is perfect: $\\widehat{\\mathbf{q}}_i^{(2)} = \\boldsymbol{\\mu} + (\\mathbf{q}_i - \\boldsymbol{\\mu}) = \\mathbf{q}_i$.\n\nFinally, for each reconstruction level $k$, we compute the reconstructed scalar coordinate $\\hat{s}_i^{(k)} = \\mathbf{v}_{\\mathrm{b}}^\\top \\widehat{\\mathbf{q}}_i^{(k)}$ and evaluate its quality using the Mean Squared Error (MSE):\n$$\n\\mathrm{MSE}(k) = \\frac{1}{N} \\sum_{i=1}^N \\left(\\hat{s}^{(k)}_i - s_i\\right)^2\n$$\nFor a given error threshold $\\varepsilon$, we seek the smallest integer $k \\in \\{0, 1, 2\\}$ such that $\\mathrm{MSE}(k) \\le \\varepsilon$. Since $\\mathrm{MSE}(2)=0$ (up to numerical precision), a solution always exists for $\\varepsilon \\ge 0$.\n\nThis entire procedure is applied to each of the three test cases specified, using a distinct, deterministic random seed for each case to ensure reproducibility. The minimal value of $k$ is reported for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a, k_y, c, beta, dt, n_steps, n_burn, epsilon)\n        (1.2, 5.0, 1.0, 0.7, 5e-4, 200000, 20000, 10.0),\n        (1.2, 5.0, 1.0, 0.7, 5e-4, 200000, 20000, 0.2),\n        (1.2, 5.0, 1.0, 0.7, 5e-4, 200000, 20000, 1e-10),\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        # Unpack parameters\n        a, k_y, c, beta, dt, n_steps, n_burn, epsilon = case\n        \n        # Set a deterministic seed for reproducibility for each case\n        np.random.seed(i)\n\n        # Task 1: Simulate an overdamped Langevin trajectory\n        q = np.zeros((n_steps, 2))\n        q[0] = [a, 0.0]\n        noise_factor = np.sqrt(2 * dt / beta)\n\n        for n in range(n_steps - 1):\n            x, y = q[n]\n            grad_x = 4 * x * (x**2 - a**2) + c * y\n            grad_y = k_y * y + c * x\n            grad = np.array([grad_x, grad_y])\n            xi = np.random.randn(2)\n            q[n+1] = q[n] - dt * grad + noise_factor * xi\n        \n        # Discard burn-in samples\n        samples = q[n_burn:]\n        N = len(samples)\n\n        # Task 2: Compute the barrier-crossing eigenvector\n        Hessian = np.array([[-4 * a**2, c], [c, k_y]])\n        # np.linalg.eigh sorts eigenvalues in ascending order\n        # and returns normalized eigenvectors.\n        eigvals, eigvecs = np.linalg.eigh(Hessian)\n        v_b = eigvecs[:, 0]\n        \n        # True barrier-crossing scalar coordinate for all samples\n        s = samples @ v_b\n\n        # Task 3: Perform PCA and reconstruct\n        q_mean = np.mean(samples, axis=0)\n        centered_samples = samples - q_mean\n        \n        # SVD: a is centered_samples, vh is V.T\n        u, s_vals, vh = np.linalg.svd(centered_samples, full_matrices=False)\n        V = vh.T\n        \n        # Task 4: Compute Mean Squared Error for each k\n        mse_values = []\n\n        # k = 0: Reconstruction is the mean\n        q_hat_0 = q_mean\n        s_hat_0 = q_hat_0 @ v_b\n        mse_0 = np.mean((s - s_hat_0)**2)\n        mse_values.append(mse_0)\n\n        # k = 1: Reconstruction using the first PC\n        V1 = V[:, 0:1] # Shape (2, 1) to ensure correct matrix multiplication\n        reconstructed_centered_1 = (centered_samples @ V1) @ V1.T\n        q_hat_1 = reconstructed_centered_1 + q_mean\n        s_hat_1 = q_hat_1 @ v_b\n        mse_1 = np.mean((s - s_hat_1)**2)\n        mse_values.append(mse_1)\n\n        # k = 2: Perfect reconstruction\n        # MSE is theoretically 0, practically close to machine epsilon.\n        mse_2 = 0.0\n        mse_values.append(mse_2)\n\n        # Task 5: Determine the minimal k\n        min_k = -1\n        for k, mse in enumerate(mse_values):\n            if mse <= epsilon:\n                min_k = k\n                break\n        \n        results.append(min_k)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Before applying PCA to data from realistic simulations, it is crucial to address the technical challenges posed by Periodic Boundary Conditions (PBCs). This practice explores how different choices for handling atomic coordinates across the periodic box—a process known as imaging—can introduce significant artifacts into your analysis . By comparing PCA results derived from naively wrapped, minimum-image, and unwrapped coordinates, you will see firsthand how improper PBC treatment can distort the covariance structure and lead to physically meaningless principal components.",
            "id": "3437455",
            "problem": "You are asked to implement a complete program that constructs synthetic molecular dynamics trajectories under periodic boundary conditions, computes pairwise distance fluctuations under different imaging choices, and performs Principal Component Analysis (PCA) of these fluctuations. The goal is to quantify how periodic boundary unwrapping and imaging choices affect the covariance eigenstructure of pairwise distance features.\n\nBegin from the following fundamental base and definitions:\n\n- Molecular dynamics positions evolve according to Newton’s laws in a periodic cubic simulation box of side length $L$, but for the purposes of this problem you will directly construct synthetic, time-discrete positions.\n- A periodic boundary condition (PBC) in a cubic box identifies a particle at position $\\mathbf{r}$ with any image $\\mathbf{r} + \\mathbf{n} L$, where $\\mathbf{n} \\in \\mathbb{Z}^{3}$.\n- The wrapped coordinate $\\mathbf{r}^{\\mathrm{wrap}}$ is the representative reduced into the interval $[0, L)$ along each Cartesian axis using modulo $L$.\n- The minimum image convention (MIC) defines the displacement between two wrapped positions $\\mathbf{r}_{i}^{\\mathrm{wrap}}$ and $\\mathbf{r}_{j}^{\\mathrm{wrap}}$ as $\\Delta \\mathbf{r}_{ij}^{\\mathrm{MIC}} = \\mathbf{r}_{i}^{\\mathrm{wrap}} - \\mathbf{r}_{j}^{\\mathrm{wrap}} - L \\,\\mathrm{round}\\!\\left( \\dfrac{\\mathbf{r}_{i}^{\\mathrm{wrap}} - \\mathbf{r}_{j}^{\\mathrm{wrap}}}{L} \\right)$, applied component-wise, which brings the displacement into $[-L/2, L/2)$.\n- An unwrapped trajectory $\\mathbf{r}_{i}^{\\mathrm{unw}}(t)$ reconstructs continuous atomic paths by accumulating minimum-image displacements frame-to-frame: if $\\mathbf{r}_{i}^{\\mathrm{wrap}}(t)$ are wrapped positions, then $\\Delta \\mathbf{d}_{i}(t) = \\mathbf{r}_{i}^{\\mathrm{wrap}}(t) - \\mathbf{r}_{i}^{\\mathrm{wrap}}(t-1)$, $\\Delta \\mathbf{d}_{i}^{\\mathrm{MIC}}(t) = \\Delta \\mathbf{d}_{i}(t) - L \\,\\mathrm{round}\\!\\left( \\dfrac{\\Delta \\mathbf{d}_{i}(t)}{L} \\right)$, and $\\mathbf{r}_{i}^{\\mathrm{unw}}(t) = \\mathbf{r}_{i}^{\\mathrm{unw}}(t-1) + \\Delta \\mathbf{d}_{i}^{\\mathrm{MIC}}(t)$ with $\\mathbf{r}_{i}^{\\mathrm{unw}}(0) = \\mathbf{r}_{i}^{\\mathrm{wrap}}(0)$.\n- For $N$ atoms, define the feature vector at time $t$ as the list of all unique pairwise Euclidean distances $d_{ij}(t)$ for $1 \\le i < j \\le N$, arranged in a fixed order. For a time series of $T$ frames, assemble a $T \\times M$ matrix $\\mathbf{F}$ of these features where $M = N(N-1)/2$.\n- Compute fluctuations by subtracting the temporal mean: $\\tilde{\\mathbf{F}} = \\mathbf{F} - \\mathbf{1}\\,\\boldsymbol{\\mu}^{\\top}$ where $\\boldsymbol{\\mu}$ is the mean across time and $\\mathbf{1}$ is a length-$T$ vector of ones. The covariance matrix across features is $\\mathbf{C} = \\dfrac{1}{T-1}\\,\\tilde{\\mathbf{F}}^{\\top}\\tilde{\\mathbf{F}}$.\n- Principal Component Analysis (PCA) of covariance $\\mathbf{C}$ is given by its eigen-decomposition $\\mathbf{C}\\mathbf{v}_{k} = \\lambda_{k}\\mathbf{v}_{k}$ with eigenvalues $\\lambda_{k}$ sorted in nonincreasing order. The leading eigenvalue $\\lambda_{\\max}$ quantifies the dominant variance mode. The trace $\\mathrm{tr}(\\mathbf{C})$ equals the sum of feature variances.\n\nYou must implement three imaging choices to compute the pairwise distances $d_{ij}(t)$:\n- Wrapped-naive: use $\\|\\mathbf{r}^{\\mathrm{wrap}}_{i}(t) - \\mathbf{r}^{\\mathrm{wrap}}_{j}(t)\\|_{2}$ without MIC.\n- Minimum image: use $\\|\\Delta \\mathbf{r}_{ij}^{\\mathrm{MIC}}(t)\\|_{2}$.\n- Unwrapped: compute $\\mathbf{r}^{\\mathrm{unw}}(t)$ as above, then use $\\|\\mathbf{r}^{\\mathrm{unw}}_{i}(t) - \\mathbf{r}^{\\mathrm{unw}}_{j}(t)\\|_{2}$.\n\nAll distances are to be treated in ångström units (Å), but the requested outputs are dimensionless ratios.\n\nConstruct the following test suite of three trajectories, each in a cubic box and each given as wrapped positions generated from initial positions and constant per-frame displacements, all in three spatial dimensions:\n\n- Test case $1$ (no boundary crossing, baseline):\n  - Box length $L = 10$.\n  - Number of atoms $N = 3$, number of frames $T = 5$ with frames indexed by $t \\in \\{0,1,2,3,4\\}$.\n  - For atom $0$: initial position $(1.0, 2.0, 3.0)$ and per-frame increment $(0.1, -0.05, 0.0)$.\n  - For atom $1$: initial position $(2.0, 2.5, 3.5)$ and per-frame increment $(0.0, 0.05, 0.0)$.\n  - For atom $2$: initial position $(8.5, 1.0, 2.0)$ and per-frame increment $(0.0, 0.0, 0.1)$.\n  - Wrapped positions are obtained by applying modulo $L$ per component after adding $t$ increments; in this case no component reaches $L$.\n\n- Test case $2$ (single-atom boundary crossing, artifact-prone for naive wrapping):\n  - Box length $L = 10$.\n  - Number of atoms $N = 3$, number of frames $T = 5$ with $t \\in \\{0,1,2,3,4\\}$.\n  - For atom $0$: initial position $(9.8, 1.0, 1.0)$ and per-frame increment $(0.3, 0.0, 0.0)$, wrapped modulo $L$ per frame.\n  - For atom $1$: initial position $(1.5, 1.0, 1.0)$ and per-frame increment $(0.0, 0.0, 0.0)$.\n  - For atom $2$: initial position $(5.0, 5.0, 5.0)$ and per-frame increment $(0.0, 0.0, 0.0)$.\n\n- Test case $3$ (counter-propagation across the boundary, MIC stability versus unwrapped divergence):\n  - Box length $L = 10$.\n  - Number of atoms $N = 3$, number of frames $T = 6$ with $t \\in \\{0,1,2,3,4,5\\}$.\n  - For atom $0$: initial position $(9.5, 0.0, 0.0)$ and per-frame increment $(0.4, 0.0, 0.0)$, wrapped modulo $L$ per frame.\n  - For atom $1$: initial position $(0.5, 0.0, 0.0)$ and per-frame increment $(-0.4, 0.0, 0.0)$, wrapped modulo $L$ per frame.\n  - For atom $2$: initial position $(5.0, 0.0, 0.0)$ and per-frame increment $(0.0, 0.0, 0.0)$.\n\nFor each test case, you must:\n- Construct the wrapped trajectory $\\mathbf{r}^{\\mathrm{wrap}}_{i}(t)$ from the specified initial positions and increments using modulo $L$ per component.\n- Compute three feature matrices $\\mathbf{F}^{\\mathrm{wrap}}$, $\\mathbf{F}^{\\mathrm{MIC}}$, and $\\mathbf{F}^{\\mathrm{unw}}$ composed of all unique pairwise distances at each frame according to the three imaging choices.\n- Compute the corresponding covariance matrices $\\mathbf{C}^{\\mathrm{wrap}}$, $\\mathbf{C}^{\\mathrm{MIC}}$, and $\\mathbf{C}^{\\mathrm{unw}}$ across features as defined above.\n- Compute the leading eigenvalues $\\lambda_{\\max}^{\\mathrm{wrap}}$, $\\lambda_{\\max}^{\\mathrm{MIC}}$, and $\\lambda_{\\max}^{\\mathrm{unw}}$, and the traces $\\tau^{\\mathrm{wrap}} = \\mathrm{tr}(\\mathbf{C}^{\\mathrm{wrap}})$ and $\\tau^{\\mathrm{MIC}} = \\mathrm{tr}(\\mathbf{C}^{\\mathrm{MIC}})$.\n\nYour program must output, for each test case in order $\\{1,2,3\\}$, the following three quantities:\n- $r_{w} = \\dfrac{\\lambda_{\\max}^{\\mathrm{wrap}}}{\\lambda_{\\max}^{\\mathrm{MIC}}}$,\n- $r_{u} = \\dfrac{\\lambda_{\\max}^{\\mathrm{unw}}}{\\lambda_{\\max}^{\\mathrm{MIC}}}$,\n- $r_{\\tau} = \\dfrac{\\tau^{\\mathrm{wrap}}}{\\tau^{\\mathrm{MIC}}}$.\n\nNumerical output requirements:\n- Express each of the above three ratios as a decimal rounded to exactly $6$ digits after the decimal point.\n- Aggregate the results for the three test cases into a single line in the following exact format: a comma-separated list enclosed in square brackets, flattening the triplets per test case in order. For example, the output should look like $[r_{w}^{(1)},r_{u}^{(1)},r_{\\tau}^{(1)},r_{w}^{(2)},r_{u}^{(2)},r_{\\tau}^{(2)},r_{w}^{(3)},r_{u}^{(3)},r_{\\tau}^{(3)}]$ with each entry rounded to exactly six decimal places.\n\nAngles do not appear in this problem. No physical unit conversion is required for the final output because all ratios are dimensionless. Your program must be fully self-contained, take no input, and use only the specified libraries.",
            "solution": "The user-provided problem statement has been meticulously validated and is determined to be scientifically grounded, well-posed, and objective. It provides a complete and consistent set of definitions and data to solve a problem relevant to computational chemistry and molecular biophysics. The problem requires the implementation of an algorithm to quantify the effect of periodic boundary condition (PBC) imaging choices on the statistical properties of pairwise molecular distances, specifically through Principal Component Analysis (PCA).\n\nThe solution proceeds systematically through the following steps:\n1.  Generation of synthetic atomic trajectories under periodic boundary conditions.\n2.  Calculation of pairwise distance feature matrices using three distinct imaging methods.\n3.  Computation of the covariance matrix for each feature set and its subsequent eigen-decomposition (PCA).\n4.  Calculation of specified ratios of eigenvalues and traces to quantify the differences between imaging methods.\n\nEach step is detailed below, adhering to the mathematical formalism defined in the problem statement.\n\nFirst, we construct the time-series of atomic positions. For each of the $N$ atoms, a wrapped trajectory $\\mathbf{r}_{i}^{\\mathrm{wrap}}(t)$ is generated for $T$ time frames, indexed by $t \\in \\{0, 1, \\dots, T-1\\}$. Given an initial position $\\mathbf{r}_{i}(0)$ and a constant per-frame displacement vector $\\Delta\\mathbf{r}_{i}$, the position at time $t$ is calculated before wrapping as $\\mathbf{r}_{i}^{\\mathrm{unbounded}}(t) = \\mathbf{r}_{i}(0) + t\\Delta\\mathbf{r}_{i}$. The wrapped trajectory is then obtained by applying the modulo-$L$ operation component-wise, where $L$ is the side length of the cubic simulation box:\n$$\n\\mathbf{r}_{i}^{\\mathrm{wrap}}(t) = \\mathbf{r}_{i}^{\\mathrm{unbounded}}(t) \\pmod L\n$$\nThis places each coordinate of each atom into the principal simulation box, defined by the interval $[0, L)$.\n\nFrom the wrapped trajectory $\\mathbf{r}^{\\mathrm{wrap}}(t)$, we derive the unwrapped trajectory $\\mathbf{r}^{\\mathrm{unw}}(t)$. This process reconstructs a continuous path for each atom, accounting for jumps across the periodic boundary. It is defined recursively. The initial condition is $\\mathbf{r}_{i}^{\\mathrm{unw}}(0) = \\mathbf{r}_{i}^{\\mathrm{wrap}}(0)$. For subsequent frames $t > 0$, the update rule is:\n$$\n\\mathbf{r}_{i}^{\\mathrm{unw}}(t) = \\mathbf{r}_{i}^{\\mathrm{unw}}(t-1) + \\Delta \\mathbf{d}_{i}^{\\mathrm{MIC}}(t)\n$$\nwhere $\\Delta \\mathbf{d}_{i}^{\\mathrm{MIC}}(t)$ is the minimum image displacement of the atom $i$ between frame $t-1$ and $t$:\n$$\n\\Delta \\mathbf{d}_{i}^{\\mathrm{MIC}}(t) = \\left( \\mathbf{r}_{i}^{\\mathrm{wrap}}(t) - \\mathbf{r}_{i}^{\\mathrm{wrap}}(t-1) \\right) - L \\,\\mathrm{round}\\!\\left( \\frac{\\mathbf{r}_{i}^{\\mathrm{wrap}}(t) - \\mathbf{r}_{i}^{\\mathrm{wrap}}(t-1)}{L} \\right)\n$$\nThe `round` function is applied component-wise to the vector argument. This ensures that any single-frame displacement greater than $L/2$ in magnitude is interpreted as motion across the boundary into the nearest periodic image.\n\nNext, we define the feature vectors. The features are the $M = N(N-1)/2$ unique pairwise Euclidean distances $d_{ij}(t)$ between atoms $i$ and $j$ at each time frame $t$. We compute these distances using three different imaging choices, resulting in three distinct feature matrices: $\\mathbf{F}^{\\mathrm{wrap}}$, $\\mathbf{F}^{\\mathrm{MIC}}$, and $\\mathbf{F}^{\\mathrm{unw}}$, each of size $T \\times M$.\n1.  **Wrapped-naive distance ($d_{ij}^{\\mathrm{wrap}}$)**: This method ignores PBCs and computes the distance between coordinates as they are stored in the wrapped trajectory file. This can lead to large, artifactual distances when a pair of atoms is separated by a boundary.\n    $$\n    d_{ij}^{\\mathrm{wrap}}(t) = \\left\\| \\mathbf{r}_{i}^{\\mathrm{wrap}}(t) - \\mathbf{r}_{j}^{\\mathrm{wrap}}(t) \\right\\|_{2}\n    $$\n2.  **Minimum image distance ($d_{ij}^{\\mathrm{MIC}}$)**: This method correctly computes the shortest distance between two atoms in a periodic system by applying the minimum image convention (MIC) to their displacement vector.\n    $$\n    d_{ij}^{\\mathrm{MIC}}(t) = \\left\\| \\Delta \\mathbf{r}_{ij}^{\\mathrm{MIC}}(t) \\right\\|_{2} = \\left\\| \\left( \\mathbf{r}_{i}^{\\mathrm{wrap}}(t) - \\mathbf{r}_{j}^{\\mathrm{wrap}}(t) \\right) - L \\,\\mathrm{round}\\!\\left( \\frac{\\mathbf{r}_{i}^{\\mathrm{wrap}}(t) - \\mathbf{r}_{j}^{\\mathrm{wrap}}(t)}{L} \\right) \\right\\|_{2}\n    $$\n3.  **Unwrapped distance ($d_{ij}^{\\mathrm{unw}}$)**: This method computes the distance between atoms in the unwrapped, continuous coordinate system.\n    $$\n    d_{ij}^{\\mathrm{unw}}(t) = \\left\\| \\mathbf{r}_{i}^{\\mathrm{unw}}(t) - \\mathbf{r}_{j}^{\\mathrm{unw}}(t) \\right\\|_{2}\n    $$\n\nWith the feature matrices assembled, we proceed to the statistical analysis. For each feature matrix $\\mathbf{F}$ (representing any of $\\mathbf{F}^{\\mathrm{wrap}}$, $\\mathbf{F}^{\\mathrm{MIC}}$, or $\\mathbf{F}^{\\mathrm{unw}}$), we compute its covariance matrix $\\mathbf{C}$. First, the temporal mean vector $\\boldsymbol{\\mu}$ of the features is calculated.\n$$\n\\boldsymbol{\\mu} = \\frac{1}{T} \\sum_{t=0}^{T-1} \\mathbf{F}_{t,:}\n$$\nThen, the feature matrix is mean-centered: $\\tilde{\\mathbf{F}} = \\mathbf{F} - \\mathbf{1}\\boldsymbol{\\mu}^{\\top}$, where $\\mathbf{1}$ is a column vector of ones of length $T$. The sample covariance matrix is then computed as:\n$$\n\\mathbf{C} = \\frac{1}{T-1} \\tilde{\\mathbf{F}}^{\\top} \\tilde{\\mathbf{F}}\n$$\nThis $M \\times M$ matrix describes the variance of each feature along its diagonal and the covariance between pairs of features in its off-diagonal elements.\n\nThe core of the analysis is Principal Component Analysis (PCA), which involves the eigen-decomposition of the covariance matrix:\n$$\n\\mathbf{C}\\mathbf{v}_{k} = \\lambda_{k}\\mathbf{v}_{k}\n$$\nwhere $\\mathbf{v}_k$ are the eigenvectors (principal components) and $\\lambda_k$ are the corresponding eigenvalues. The eigenvalues, sorted non-increasingly $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_M \\ge 0$, represent the amount of variance captured by each principal component. The leading eigenvalue, $\\lambda_{\\max} = \\lambda_1$, quantifies the variance of the most dominant mode of fluctuation in the feature set. The trace of the covariance matrix, $\\mathrm{tr}(\\mathbf{C}) = \\sum_{k=1}^{M} \\lambda_k$, represents the total variance across all features.\n\nFinally, for each test case, we compute the following dimensionless ratios to compare the statistical signatures of the three imaging methods:\n-   $r_{w} = \\dfrac{\\lambda_{\\max}^{\\mathrm{wrap}}}{\\lambda_{\\max}^{\\mathrm{MIC}}}$: This ratio measures the inflation of the dominant variance mode due to naive-wrapped PBC artifacts relative to the physically correct MIC.\n-   $r_{u} = \\dfrac{\\lambda_{\\max}^{\\mathrm{unw}}}{\\lambda_{\\max}^{\\mathrm{MIC}}}$: This ratio compares the dominant variance of the globally continuous unwrapped trajectory to that of the locally correct MIC. This can highlight artifacts from long-timescale diffusion in the unwrapped representation.\n-   $r_{\\tau} = \\dfrac{\\tau^{\\mathrm{wrap}}}{\\tau^{\\mathrm{MIC}}}$, where $\\tau = \\mathrm{tr}(\\mathbf{C})$: This ratio measures the inflation of the total feature variance due to naive-wrapped PBC artifacts.\n\nThe implementation encapsulates this entire procedure, applying it to each of the three specified test cases and producing the required ratios, formatted to six decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the final result.\n    \"\"\"\n\n    def mic_displacement(delta_r, L):\n        \"\"\"\n        Computes the minimum image convention displacement vector(s).\n        \n        Args:\n            delta_r (np.ndarray): The displacement vector(s) without PBC.\n            L (float): The box length.\n        \n        Returns:\n            np.ndarray: The displacement vector(s) under MIC.\n        \"\"\"\n        return delta_r - L * np.round(delta_r / L)\n\n    def perform_pca(F):\n        \"\"\"\n        Computes the covariance matrix, its leading eigenvalue, and its trace.\n        \n        Args:\n            F (np.ndarray): The T x M feature matrix.\n        \n        Returns:\n            tuple[float, float]: The leading eigenvalue and the trace of the covariance matrix.\n        \"\"\"\n        T = F.shape[0]\n        if T < 2:\n            return 0.0, 0.0\n\n        # rowvar=False: columns are variables (features), rows are observations (time)\n        # ddof=1: use sample covariance (divide by T-1)\n        C = np.cov(F, rowvar=False, ddof=1)\n        \n        # If F has only one feature column, np.cov returns a 0-dim array (scalar)\n        if C.ndim == 0:\n            eigenvalues = np.array([C.item()])\n        else:\n            # eigh returns eigenvalues in ascending order for symmetric matrices\n            eigenvalues = np.linalg.eigh(C)[0]\n        \n        lambda_max = eigenvalues[-1] if len(eigenvalues) > 0 else 0.0\n        trace = np.trace(C)\n        \n        return lambda_max, trace\n\n    def process_case(case_params):\n        \"\"\"\n        Processes a single test case from trajectory generation to ratio calculation.\n        \n        Args:\n            case_params (tuple): A tuple containing L, N, T, initial_pos, and increments.\n        \n        Returns:\n            tuple[float, float, float]: The calculated ratios r_w, r_u, r_tau.\n        \"\"\"\n        L, N, T, initial_pos, increments = case_params\n\n        # 1. Generate wrapped trajectory\n        t_vals = np.arange(T).reshape(T, 1, 1)\n        initial_pos_b = np.expand_dims(initial_pos, 0)\n        increments_b = np.expand_dims(increments, 0)\n        unbounded_traj = initial_pos_b + t_vals * increments_b\n        wrapped_traj = np.mod(unbounded_traj, L)\n\n        # 2. Generate unwrapped trajectory\n        unwrapped_traj = np.zeros_like(wrapped_traj)\n        unwrapped_traj[0] = wrapped_traj[0]\n        for t in range(1, T):\n            delta_d = wrapped_traj[t] - wrapped_traj[t-1]\n            delta_d_mic = mic_displacement(delta_d, L)\n            unwrapped_traj[t] = unwrapped_traj[t-1] + delta_d_mic\n\n        # 3. Compute feature matrices\n        M = N * (N - 1) // 2\n        F_wrap = np.zeros((T, M))\n        F_mic = np.zeros((T, M))\n        F_unw = np.zeros((T, M))\n\n        pairs = [(i, j) for i in range(N) for j in range(i + 1, N)]\n\n        for t in range(T):\n            r_wrap_t = wrapped_traj[t]\n            r_unw_t = unwrapped_traj[t]\n            for k, (i, j) in enumerate(pairs):\n                # Wrapped-naive\n                delta_r_wrap_naive = r_wrap_t[i] - r_wrap_t[j]\n                F_wrap[t, k] = np.linalg.norm(delta_r_wrap_naive)\n\n                # MIC\n                delta_r_mic = mic_displacement(delta_r_wrap_naive, L)\n                F_mic[t, k] = np.linalg.norm(delta_r_mic)\n\n                # Unwrapped\n                delta_r_unw = r_unw_t[i] - r_unw_t[j]\n                F_unw[t, k] = np.linalg.norm(delta_r_unw)\n\n        # 4. Perform PCA for each feature matrix\n        lambda_max_wrap, trace_wrap = perform_pca(F_wrap)\n        lambda_max_mic, trace_mic = perform_pca(F_mic)\n        lambda_max_unw, _ = perform_pca(F_unw)\n\n        # 5. Calculate and return ratios\n        # Handle potential division by zero if MIC variance is zero\n        r_w = (lambda_max_wrap / lambda_max_mic) if lambda_max_mic != 0 else 0.0\n        r_u = (lambda_max_unw / lambda_max_mic) if lambda_max_mic != 0 else 0.0\n        r_tau = (trace_wrap / trace_mic) if trace_mic != 0 else 0.0\n        \n        return r_w, r_u, r_tau\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (10.0, 3, 5, \n         np.array([[1.0, 2.0, 3.0], [2.0, 2.5, 3.5], [8.5, 1.0, 2.0]]),\n         np.array([[0.1, -0.05, 0.0], [0.0, 0.05, 0.0], [0.0, 0.0, 0.1]])),\n        (10.0, 3, 5,\n         np.array([[9.8, 1.0, 1.0], [1.5, 1.0, 1.0], [5.0, 5.0, 5.0]]),\n         np.array([[0.3, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])),\n        (10.0, 3, 6,\n         np.array([[9.5, 0.0, 0.0], [0.5, 0.0, 0.0], [5.0, 0.0, 0.0]]),\n         np.array([[0.4, 0.0, 0.0], [-0.4, 0.0, 0.0], [0.0, 0.0, 0.0]]))\n    ]\n\n    results = []\n    for case in test_cases:\n        ratios = process_case(case)\n        results.extend(ratios)\n\n    # Format the final output string with rounding to 6 decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once you have computed the principal components, a final practical challenge often emerges in time-resolved analysis: the arbitrary sign of eigenvectors. Since an eigenvector $\\mathbf{v}$ and its negative $-\\mathbf{v}$ are mathematically equivalent, numerical solvers may produce a time series of PC vectors with spurious sign flips, obscuring the underlying smooth evolution of collective motions . This exercise guides you through implementing a standard sign-continuity alignment algorithm and quantifying its impact on time-correlation functions, a critical step for any analysis that depends on the temporal coherence of the principal components.",
            "id": "3437370",
            "problem": "You are given a sequence of time-ordered principal component (PC) direction vectors arising from a sliding-window Principal Component Analysis (PCA) of a Molecular Dynamics (MD) trajectory. In time-resolved PCA of molecular displacements, each window yields a leading eigenvector whose sign is undetermined, because if $\\mathbf{v}$ is an eigenvector, then so is $-\\mathbf{v}$. As a result, the raw time series $\\{\\mathbf{v}_t\\}_{t=1}^{T}$ may contain arbitrary sign flips that are not physically meaningful. These flips corrupt orientation-based time-correlation analysis because the inner products between adjacent directions can turn negative purely due to the sign ambiguity rather than any actual change in molecular motion.\n\nFrom first principles, consider that MD evolves atomic positions $\\mathbf{r}_i(t)$ via Newton's laws, and PCA on the covariance of displacements identifies dominant collective fluctuation directions. The orientation of these directions over time should vary smoothly when the underlying collective motion is coherent, but the sign ambiguity introduces artificial discontinuities. A principled way to enforce temporal sign consistency is to ensure that the orientation at time $t$ is chosen to be the one that maximizes its agreement with the previous aligned orientation, as measured by the Euclidean inner product in the space of atomic displacements.\n\nYour tasks are:\n\n- Implement a sign-continuity alignment for the time series $\\{\\mathbf{v}_t\\}_{t=1}^{T}$, where each $\\mathbf{v}_t \\in \\mathbb{R}^d$ is a unit vector representing the leading PC direction at time step $t$. The aligned sequence $\\{\\tilde{\\mathbf{v}}_t\\}_{t=1}^{T}$ must be constructed so that for each $t \\ge 2$, the choice between $\\mathbf{v}_t$ and $-\\mathbf{v}_t$ maximizes the inner product with $\\tilde{\\mathbf{v}}_{t-1}$. In particular, if the inner product is negative, you must reverse the sign at time $t$; if it is zero, you must leave the vector unchanged. All vectors must be treated as elements of $\\mathbb{R}^d$ with the standard Euclidean inner product. Angles used below are specified in radians.\n\n- Assess the effect of sign alignment on the time-correlation function of the PC directions. Define the time-correlation function at lag $\\tau$ as\n$$\nC(\\tau) \\;=\\; \\frac{1}{T-\\tau}\\sum_{t=1}^{T-\\tau} \\mathbf{u}_t^\\top \\mathbf{u}_{t+\\tau},\n$$\nwhere $\\{\\mathbf{u}_t\\}$ is a time series of unit vectors (either the raw $\\{\\mathbf{v}_t\\}$ or the aligned $\\{\\tilde{\\mathbf{v}}_t\\}$). Compute $C_{\\text{raw}}(\\tau)$ for the raw series and $C_{\\text{aligned}}(\\tau)$ for the aligned series, for lags $\\tau \\in \\{1,5\\}$. Report, for each test case, the improvements\n$$\n\\Delta C(\\tau) \\;=\\; C_{\\text{aligned}}(\\tau) \\;-\\; C_{\\text{raw}}(\\tau).\n$$\n\n- All vectors in each time series must be unit-normalized before correlation analysis.\n\nConstruct the following four deterministic test cases. In each case, you must generate the time series $\\{\\mathbf{v}_t\\}_{t=1}^{T}$ according to the definition provided, ensuring all vectors are unit length. The dimension $d$, the number of time steps $T$, and any parameters are explicitly specified. For any trigonometric function, the angle unit is radians.\n\n- Test Case A (slowly rotating direction in $\\mathbb{R}^2$ with sparse sign inversions):\n    - Dimension: $d = 2$.\n    - Number of steps: $T = 200$.\n    - Base angle process: $\\theta_t = 0.03 \\times (t-1)$ for $t \\in \\{1,\\dots,200\\}$.\n    - Base vector before inversions: $\\mathbf{w}_t = [\\cos(\\theta_t), \\sin(\\theta_t)]$.\n    - Introduce artificial sign inversions at time indices $t \\in \\{50,120,170\\}$ by setting $\\mathbf{v}_t = -\\mathbf{w}_t$ for those $t$ and $\\mathbf{v}_t = \\mathbf{w}_t$ otherwise. Normalize each $\\mathbf{v}_t$ to unit length.\n\n- Test Case B (smoothly precessing direction in $\\mathbb{R}^3$ with frequent sign inversions):\n    - Dimension: $d = 3$.\n    - Number of steps: $T = 300$.\n    - Base, unnormalized vector: $\\mathbf{w}_t = [\\cos(0.07 \\times (t-1)), \\sin(0.07 \\times (t-1)), 0.3 \\cos(0.02 \\times (t-1))]$.\n    - Normalize $\\mathbf{w}_t$ to unit length to get $\\hat{\\mathbf{w}}_t$.\n    - Introduce artificial sign inversions at indices $t \\in \\{20,40,60,80,100,140,180,220,260\\}$ by setting $\\mathbf{v}_t = -\\hat{\\mathbf{w}}_t$ for those $t$ and $\\mathbf{v}_t = \\hat{\\mathbf{w}}_t$ otherwise.\n\n- Test Case C (orthogonal walk in $\\mathbb{R}^2$ producing zero inner products at unit lags):\n    - Dimension: $d = 2$.\n    - Number of steps: $T = 12$.\n    - Set $\\theta_t = \\frac{\\pi}{2} \\times (t-1)$ and $\\mathbf{v}_t = [\\cos(\\theta_t), \\sin(\\theta_t)]$. Normalize each $\\mathbf{v}_t$.\n\n- Test Case D (constant direction in $\\mathbb{R}^3$):\n    - Dimension: $d = 3$.\n    - Number of steps: $T = 60$.\n    - Set $\\mathbf{v}_t = [1,0,0]$ for all $t$, then normalize.\n\nFor each test case, compute $\\Delta C(1)$ and $\\Delta C(5)$ as defined above. Aggregate the results across all four test cases into a single list in the following order:\n- Test Case A: $\\Delta C(1)$, then $\\Delta C(5)$.\n- Test Case B: $\\Delta C(1)$, then $\\Delta C(5)$.\n- Test Case C: $\\Delta C(1)$, then $\\Delta C(5)$.\n- Test Case D: $\\Delta C(1)$, then $\\Delta C(5)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\Delta C_A(1),\\Delta C_A(5),\\Delta C_B(1),\\Delta C_B(5),\\Delta C_C(1),\\Delta C_C(5),\\Delta C_D(1),\\Delta C_D(5)]$.\n- Each entry must be a real number. You may format the numbers to a reasonable fixed number of decimal places.",
            "solution": "The user has provided a well-defined computational problem grounded in the analysis of molecular dynamics (MD) simulations. The problem focuses on correcting a known numerical artifact in time-resolved Principal Component Analysis (PCA) and quantifying the impact of this correction.\n\n### Scientific Context and Principle\n\nIn MD simulations, PCA is a powerful technique for identifying dominant modes of collective atomic motion from a trajectory of atomic coordinates. When applied in a sliding-window fashion, it yields a time series of principal components (eigenvectors of the covariance matrix of atomic displacements), where the first principal component, $\\mathbf{v}_t$, represents the most significant collective motion in the time window centered at time $t$.\n\nA fundamental property of eigenvectors is that their sign is arbitrary: if $\\mathbf{v}$ is an eigenvector of a matrix $A$ with eigenvalue $\\lambda$, so that $A\\mathbf{v} = \\lambda\\mathbf{v}$, then $-\\mathbf{v}$ is also an eigenvector with the same eigenvalue, since $A(-\\mathbf{v}) = -A\\mathbf{v} = -\\lambda\\mathbf{v} = \\lambda(-\\mathbf{v})$. Numerical eigensolvers may return $\\mathbf{v}_t$ or $-\\mathbf{v}_t$ unpredictably. This leads to artificial sign flips in the time series $\\{\\mathbf{v}_t\\}_{t=1}^{T}$, which corrupt measures of temporal correlation.\n\nThe physical principle guiding the correction is that collective motions in a system evolving under Newton's laws should be temporally continuous. Therefore, the direction of the dominant motion at time $t$ should be \"as close as possible\" to the direction at time $t-1$. This continuity is restored by ensuring the sequence of eigenvectors is smoothly oriented.\n\n### Step 1: Sign-Continuity Alignment\n\nThe problem mandates an alignment procedure to construct an aligned time series $\\{\\tilde{\\mathbf{v}}_t\\}_{t=1}^{T}$ from the raw series $\\{\\mathbf{v}_t\\}_{t=1}^{T}$. Each $\\mathbf{v}_t$ and $\\tilde{\\mathbf{v}}_t$ is a unit vector in $\\mathbb{R}^d$.\n\nThe alignment is performed sequentially:\n1.  The first vector of the aligned series is initialized with the first raw vector: $\\tilde{\\mathbf{v}}_1 = \\mathbf{v}_1$.\n2.  For each subsequent time step $t$ from $2$ to $T$, the vector $\\mathbf{v}_t$ is compared to the *previously aligned* vector $\\tilde{\\mathbf{v}}_{t-1}$. The orientation of $\\tilde{\\mathbf{v}}_t$ is chosen to maximize its overlap with $\\tilde{\\mathbf{v}}_{t-1}$, as measured by the Euclidean inner product, $\\tilde{\\mathbf{v}}_{t-1}^\\top \\mathbf{v}_t$.\n    -   If $\\tilde{\\mathbf{v}}_{t-1}^\\top \\mathbf{v}_t \\ge 0$, the orientation of $\\mathbf{v}_t$ is already consistent with the previous aligned vector. We set $\\tilde{\\mathbf{v}}_t = \\mathbf{v}_t$. The case of the inner product being exactly zero is handled here, leaving the vector unchanged as per the problem specification.\n    -   If $\\tilde{\\mathbf{v}}_{t-1}^\\top \\mathbf{v}_t < 0$, the orientation of $\\mathbf{v}_t$ is opposite to the established trend. We reverse its sign to restore continuity: $\\tilde{\\mathbf{v}}_t = -\\mathbf{v}_t$.\n\nThis iterative process ensures that $\\tilde{\\mathbf{v}}_t^\\top \\tilde{\\mathbf{v}}_{t-1} \\ge 0$ for all $t \\ge 2$, effectively removing the non-physical sign discontinuities.\n\n### Step 2: Time-Correlation Function and a Metric for Improvement\n\nTo quantify the effect of the alignment, we use a time-correlation function. For a time series of unit vectors $\\{\\mathbf{u}_t\\}_{t=1}^T$, the correlation at a time lag $\\tau$ is defined as the average inner product between vectors separated by that lag:\n$$\nC(\\tau) \\;=\\; \\frac{1}{T-\\tau}\\sum_{t=1}^{T-\\tau} \\mathbf{u}_t^\\top \\mathbf{u}_{t+\\tau}\n$$\nThe inner product $\\mathbf{u}_t^\\top \\mathbf{u}_{t+\\tau}$ is the cosine of the angle between the two unit vectors, so $C(\\tau)$ measures the average cosine of the angle between PC directions at times $t$ and $t+\\tau$.\n\nWe compute this function for both the raw series, $C_{\\text{raw}}(\\tau)$, and the aligned series, $C_{\\text{aligned}}(\\tau)$. The improvement due to the alignment is then measured by the difference:\n$$\n\\Delta C(\\tau) \\;=\\; C_{\\text{aligned}}(\\tau) \\;-\\; C_{\\text{raw}}(\\tau)\n$$\nA positive $\\Delta C(\\tau)$ indicates that the alignment process has increased the measured temporal correlation, which is expected as it removes artificial anticorrelations caused by sign flips. This calculation is performed for lags $\\tau=1$ and $\\tau=5$.\n\n### Step 3: Test Case Generation and Analysis\n\nThe methodology is applied to four deterministic test cases. For each case, the raw vector series $\\{\\mathbf{v}_t\\}$ is generated, then aligned to get $\\{\\tilde{\\mathbf{v}}_t\\}$, and finally $\\Delta C(1)$ and $\\Delta C(5)$ are computed.\n\n- **Test Case A:** A vector rotating slowly in a 2D plane with three artificial sign flips. The alignment algorithm is expected to detect and correct these three flips. This will significantly increase the value of the correlation function, especially at small lags, resulting in a positive $\\Delta C(\\tau)$.\n\n- **Test Case B:** A vector undergoing a more complex precession motion in 3D with frequent sign flips. Similar to Case A, the alignment will correct these flips, restoring the underlying smooth evolution of the vector. The frequent flips in the raw series will lead to a very low, possibly negative, $C_{\\text{raw}}(\\tau)$, while $C_{\\text{aligned}}(\\tau)$ will be high and positive. Thus, a large positive $\\Delta C(\\tau)$ is expected.\n\n- **Test Case C:** An \"orthogonal walk\" where each vector is orthogonal to the next, i.e., $\\mathbf{v}_t^\\top \\mathbf{v}_{t+1} = 0$. According to the alignment rule, if the inner product with the previous vector is zero, no flip is performed. Therefore, the aligned series will be identical to the raw series: $\\tilde{\\mathbf{v}}_t = \\mathbf{v}_t$ for all $t$. Consequently, $C_{\\text{aligned}}(\\tau) = C_{\\text{raw}}(\\tau)$, and the improvement must be zero: $\\Delta C(\\tau) = 0$.\n\n- **Test Case D:** A constant vector series in 3D with no introduced sign flips. The inner product between any two vectors is $1$. The alignment algorithm will find that $\\tilde{\\mathbf{v}}_{t-1}^\\top \\mathbf{v}_t = 1 > 0$ for all $t \\ge 2$ and will perform no flips. The aligned series is identical to the raw series, and as in Case C, the improvement must be zero: $\\Delta C(\\tau) = 0$.\n\nThe following code implements this full procedure. It generates each test case, applies the alignment algorithm, computes the correlation functions, and reports the required $\\Delta C(\\tau)$ values.",
            "answer": "```python\nimport numpy as np\n\ndef generate_vectors(case_params):\n    \"\"\"\n    Generates the raw time series of vectors for a given test case.\n    \"\"\"\n    d = case_params['d']\n    T = case_params['T']\n    name = case_params['name']\n    \n    vectors = np.zeros((T, d))\n\n    if name == 'A':\n        # Test Case A: slowly rotating direction in R^2 with sparse sign inversions\n        flip_indices = {50, 120, 170}\n        for t_idx in range(T):\n            t = t_idx + 1\n            theta_t = 0.03 * (t - 1)\n            w_t = np.array([np.cos(theta_t), np.sin(theta_t)])\n            if t in flip_indices:\n                v_t = -w_t\n            else:\n                v_t = w_t\n            # Normalization is technically redundant as w_t is already unit length\n            norm = np.linalg.norm(v_t)\n            vectors[t_idx] = v_t / norm if norm > 0 else v_t\n    \n    elif name == 'B':\n        # Test Case B: smoothly precessing direction in R^3 with frequent sign inversions\n        flip_indices = {20, 40, 60, 80, 100, 140, 180, 220, 260}\n        for t_idx in range(T):\n            t = t_idx + 1\n            w_t_unnormalized = np.array([\n                np.cos(0.07 * (t - 1)),\n                np.sin(0.07 * (t - 1)),\n                0.3 * np.cos(0.02 * (t - 1))\n            ])\n            # Must normalize the base vector\n            norm = np.linalg.norm(w_t_unnormalized)\n            w_hat_t = w_t_unnormalized / norm if norm > 0 else w_t_unnormalized\n            \n            if t in flip_indices:\n                v_t = -w_hat_t\n            else:\n                v_t = w_hat_t\n            vectors[t_idx] = v_t\n\n    elif name == 'C':\n        # Test Case C: orthogonal walk in R^2\n        for t_idx in range(T):\n            t = t_idx + 1\n            theta_t = (np.pi / 2.0) * (t - 1)\n            v_t = np.array([np.cos(theta_t), np.sin(theta_t)])\n            # Normalization for floating point precision\n            norm = np.linalg.norm(v_t)\n            vectors[t_idx] = v_t / norm if norm > 0 else v_t\n\n    elif name == 'D':\n        # Test Case D: constant direction in R^3\n        v = np.array([1.0, 0.0, 0.0])\n        # Normalization is redundant\n        norm = np.linalg.norm(v)\n        v_normalized = v / norm if norm > 0 else v\n        for t_idx in range(T):\n            vectors[t_idx] = v_normalized\n    \n    return vectors\n\ndef align_vectors(raw_vectors):\n    \"\"\"\n    Applies sign-continuity alignment to a time series of vectors.\n    \"\"\"\n    T, d = raw_vectors.shape\n    aligned_vectors = np.copy(raw_vectors)\n    for t in range(1, T):\n        # Compare current vector with the *previous aligned* vector\n        dot_product = np.dot(aligned_vectors[t - 1], aligned_vectors[t])\n        if dot_product < 0:\n            aligned_vectors[t] *= -1.0\n    return aligned_vectors\n\ndef calculate_correlation(vectors, tau):\n    \"\"\"\n    Computes the time-correlation function C(tau).\n    \"\"\"\n    T = vectors.shape[0]\n    if tau >= T:\n        return 0.0\n    \n    # Sum of dot products u_t^T u_{t+tau}\n    correlation_sum = np.sum([np.dot(vectors[t], vectors[t + tau]) for t in range(T - tau)])\n    \n    return correlation_sum / (T - tau)\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print the final result.\n    \"\"\"\n    test_case_params = [\n        {'name': 'A', 'd': 2, 'T': 200},\n        {'name': 'B', 'd': 3, 'T': 300},\n        {'name': 'C', 'd': 2, 'T': 12},\n        {'name': 'D', 'd': 3, 'T': 60},\n    ]\n\n    lags = [1, 5]\n    all_results = []\n\n    for params in test_case_params:\n        # 1. Generate the raw vector series\n        raw_vectors = generate_vectors(params)\n        \n        # 2. Generate the aligned vector series\n        aligned_vectors = align_vectors(raw_vectors)\n        \n        case_results = []\n        for tau in lags:\n            # 3. Calculate correlation for raw and aligned series\n            C_raw = calculate_correlation(raw_vectors, tau)\n            C_aligned = calculate_correlation(aligned_vectors, tau)\n            \n            # 4. Calculate the improvement\n            delta_C = C_aligned - C_raw\n            case_results.append(delta_C)\n        \n        all_results.extend(case_results)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.8f}' for r in all_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}