{
    "hands_on_practices": [
        {
            "introduction": "This exercise provides a complete, end-to-end experience of applying PCA to a problem in chemical physics. You will simulate dynamics in a simple two-dimensional potential, a common model for barrier-crossing events, and then use PCA to discover the system's most important collective motion . By comparing the dominant principal component to the true reaction coordinate, you will gain a concrete understanding of how PCA can reveal the underlying physics of a complex system.",
            "id": "3437411",
            "problem": "You are given a two-dimensional bistable toy potential intended to mimic barrier crossing in molecular dynamics. Your task is to simulate samples from the overdamped Langevin dynamics in this potential, perform Principal Component Analysis (PCA) of the sampled molecular trajectory, and determine, for each test case, the minimum number of principal components needed to reconstruct the barrier-crossing coordinate within a specified mean squared error threshold.\n\nUse the following fundamental and widely accepted base in your derivation and implementation:\n- Overdamped Langevin dynamics for a particle at inverse temperature $\\beta$ in a potential $U(\\mathbf{q})$ evolves according to the stochastic differential equation $d \\mathbf{q}_t = - \\nabla U(\\mathbf{q}_t) \\, dt + \\sqrt{2 \\beta^{-1}} \\, d \\mathbf{W}_t$, where $\\mathbf{W}_t$ is a standard Wiener process. In discrete time with time step $\\Delta t$, the Euler–Maruyama update reads $\\mathbf{q}_{n+1} = \\mathbf{q}_n - \\Delta t \\, \\nabla U(\\mathbf{q}_n) + \\sqrt{2 \\Delta t / \\beta} \\, \\boldsymbol{\\xi}_n$, where $\\boldsymbol{\\xi}_n$ is a vector of independent standard normal variables.\n- The barrier-crossing coordinate is defined as the projection onto the unstable eigenvector of the Hessian of the potential at the saddle point. For a saddle at $\\mathbf{q}^\\ast = \\mathbf{0}$, the Hessian is the matrix of second derivatives $H_{ij} = \\partial^2 U / \\partial q_i \\partial q_j \\big|_{\\mathbf{q}=\\mathbf{0}}$. Its eigenvector associated with the smallest eigenvalue gives the direction of negative curvature, which we denote $\\mathbf{v}_{\\mathrm{b}}$. The barrier-crossing scalar coordinate is $s = \\mathbf{v}_{\\mathrm{b}}^\\top \\mathbf{q}$.\n- Principal Component Analysis (PCA) is defined as follows: given mean-centered data matrix $X \\in \\mathbb{R}^{N \\times d}$, compute its singular value decomposition $X = U \\Sigma V^\\top$, with columns of $V$ as principal axes. The $k$-component reconstruction of centered data is $X_k = X V_k V_k^\\top$, where $V_k$ contains the first $k$ columns of $V$. The full reconstruction in the original coordinates is obtained by adding back the mean.\n\nConsider the two-dimensional potential\n$$\nU(x,y) = \\left(x^2 - a^2\\right)^2 + \\frac{k_y}{2} y^2 + c \\, x y,\n$$\nwhere $a > 0$ controls the positions of the wells along the $x$-axis, $k_y > 0$ is the stiffness along $y$, and $c$ couples $x$ and $y$. All quantities are dimensionless; no physical units are required.\n\nTasks to implement for each test case:\n1. Simulate an overdamped Langevin trajectory using the specified parameters $(a, k_y, c, \\beta, \\Delta t, N_{\\mathrm{steps}}, N_{\\mathrm{burn}})$. Use initial condition $\\mathbf{q}_0 = (a, 0)$, discard the first $N_{\\mathrm{burn}}$ steps as burn-in, and keep the remaining samples without subsampling.\n2. Compute the Hessian $H$ of $U$ at $(x,y) = (0,0)$ and obtain the normalized eigenvector $\\mathbf{v}_{\\mathrm{b}}$ associated with the smallest eigenvalue. For any sample $\\mathbf{q}$, define $s = \\mathbf{v}_{\\mathrm{b}}^\\top \\mathbf{q}$ as the barrier-crossing scalar coordinate.\n3. Perform PCA on the centered samples. For each $k \\in \\{0,1,2\\}$, form the $k$-component reconstruction $\\widehat{\\mathbf{q}}^{(k)}$ of each sample. For $k = 0$, the reconstruction is the sample mean. For $k \\ge 1$, use the projection onto the first $k$ principal axes. For each $k$, compute the reconstructed scalar coordinate $\\hat{s}^{(k)} = \\mathbf{v}_{\\mathrm{b}}^\\top \\widehat{\\mathbf{q}}^{(k)}$.\n4. For each $k$, compute the mean squared error\n$$\n\\mathrm{MSE}(k) = \\frac{1}{N} \\sum_{i=1}^N \\left(\\hat{s}^{(k)}_i - s_i\\right)^2.\n$$\n5. Let $\\varepsilon$ be the error threshold for the test case. Determine the minimal $k \\in \\{0,1,2\\}$ such that $\\mathrm{MSE}(k) \\le \\varepsilon$. If multiple $k$ satisfy the inequality, choose the smallest. Report that minimal $k$.\n\nThe test suite consists of three cases. Each case specifies the tuple $(a, k_y, c, \\beta, \\Delta t, N_{\\mathrm{steps}}, N_{\\mathrm{burn}}, \\varepsilon)$:\n- Case $1$: $(1.2, 5.0, 1.0, 0.7, 5 \\times 10^{-4}, 200000, 20000, 10.0)$.\n- Case $2$: $(1.2, 5.0, 1.0, 0.7, 5 \\times 10^{-4}, 200000, 20000, 0.2)$.\n- Case $3$: $(1.2, 5.0, 1.0, 0.7, 5 \\times 10^{-4}, 200000, 20000, 10^{-10})$.\n\nNotes and constraints:\n- All computations must be done in dimensionless form; no physical units are required in the output.\n- The initial condition, numerical time step, and number of steps are specified above and must be used exactly as given.\n- Randomness must be controlled with a fixed seed so that results are reproducible; use a distinct integer seed for each test case derived deterministically from the case index.\n- The final program must not read any input; it must construct the test cases internally and output a single line with the results.\n\nFinal output format:\n- Your program should produce a single line of output containing the minimal $k$ for each test case as a comma-separated list enclosed in square brackets, for example, $[k_1,k_2,k_3]$. The numbers $k_i$ must be integers.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of statistical mechanics and linear algebra, well-posed with a clear objective and sufficient data, and formulated objectively. It constitutes a standard computational exercise in the field of molecular dynamics. We proceed with the solution.\n\nThe core task is to determine the minimum number of principal components required to reconstruct a specific collective variable—the barrier-crossing coordinate—from a simulated molecular trajectory within a given error tolerance. This involves a sequence of steps: simulating the system's dynamics, defining the coordinate of interest, performing Principal Component Analysis (PCA), and evaluating the reconstruction error.\n\nFirst, we define the system. The particle's motion occurs in a two-dimensional potential given by\n$$\nU(x,y) = \\left(x^2 - a^2\\right)^2 + \\frac{k_y}{2} y^2 + c \\, x y\n$$\nThis potential features two wells, making it a suitable model for studying barrier-crossing events. The dynamics are governed by the overdamped Langevin equation, which describes the motion of a particle subject to a potential force, friction, and a random thermal force. In its discretized form, the Euler-Maruyama scheme gives the update rule for the particle's position vector $\\mathbf{q}_n = (x_n, y_n)$ at step $n$:\n$$\n\\mathbf{q}_{n+1} = \\mathbf{q}_n - \\Delta t \\, \\nabla U(\\mathbf{q}_n) + \\sqrt{\\frac{2 \\Delta t}{\\beta}} \\, \\boldsymbol{\\xi}_n\n$$\nwhere $\\Delta t$ is the time step, $\\beta$ is the inverse temperature, $\\boldsymbol{\\xi}_n$ is a vector of independent random numbers drawn from a standard normal distribution, and $\\nabla U(\\mathbf{q})$ is the gradient of the potential:\n$$\n\\nabla U(x,y) = \\begin{pmatrix} \\frac{\\partial U}{\\partial x} \\\\ \\frac{\\partial U}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 4x(x^2 - a^2) + c y \\\\ k_y y + c x \\end{pmatrix}\n$$\nFollowing the problem specification, we simulate a trajectory starting from $\\mathbf{q}_0 = (a, 0)$ for $N_{\\mathrm{steps}}$ steps, and discard the initial $N_{\\mathrm{burn}}$ steps to ensure the system has reached equilibrium. This leaves $N = N_{\\mathrm{steps}} - N_{\\mathrm{burn}}$ samples for analysis.\n\nNext, we define the barrier-crossing coordinate, $s$. This coordinate is physically interpreted as the direction of slowest motion away from the transition state, which lies along the path of highest energy between the two stable states (wells). Mathematically, it is defined as the projection of the position vector $\\mathbf{q}$ onto the direction of maximum instability at the saddle point of the potential. The primary saddle point is located at the origin, $\\mathbf{q}^\\ast = (0,0)$. The local geometry of the potential is characterized by the Hessian matrix of second derivatives at this point:\n$$\nH = \\left. \\begin{pmatrix} \\frac{\\partial^2 U}{\\partial x^2} & \\frac{\\partial^2 U}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 U}{\\partial y \\partial x} & \\frac{\\partial^2 U}{\\partial y^2} \\end{pmatrix} \\right|_{\\mathbf{q}=\\mathbf{0}} = \\begin{pmatrix} -4a^2 & c \\\\ c & k_y \\end{pmatrix}\n$$\nSince $H$ is a real symmetric matrix, its eigenvalues are real and its eigenvectors are orthogonal. A saddle point has at least one negative eigenvalue. The eigenvector $\\mathbf{v}_{\\mathrm{b}}$ corresponding to the smallest (most negative) eigenvalue points along the direction of negative curvature, which represents the barrier-crossing direction. We find this eigenvector and normalize it to unit length. The barrier-crossing scalar coordinate for any sample position $\\mathbf{q}_i$ is then given by the projection $s_i = \\mathbf{v}_{\\mathrm{b}}^\\top \\mathbf{q}_i$.\n\nThe third step is to perform Principal Component Analysis (PCA) on the set of $N$ trajectory samples, which we denote as the data matrix $Q \\in \\mathbb{R}^{N \\times 2}$. PCA identifies the principal axes of variation in the data. First, we compute the mean position $\\boldsymbol{\\mu} = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{q}_i$ and form the mean-centered data matrix $X$, where each row is $X_i = \\mathbf{q}_i - \\boldsymbol{\\mu}$. The singular value decomposition (SVD) of $X$ is computed as $X = U \\Sigma V^\\top$. The columns of the orthogonal matrix $V \\in \\mathbb{R}^{2 \\times 2}$ are the principal components (axes) of the data.\n\nWe then reconstruct the trajectory using a limited number of principal components, $k \\in \\{0, 1, 2\\}$.\nFor $k=0$, the reconstruction is simply the mean of the data: $\\widehat{\\mathbf{q}}_i^{(0)} = \\boldsymbol{\\mu}$ for all $i$. This represents the best zero-dimensional approximation.\nFor $k \\in \\{1, 2\\}$, the reconstruction of the centered data is obtained by projecting the data onto the first $k$ principal axes and then transforming back to the original coordinate space: $X_k = X V_k V_k^\\top$, where $V_k$ is the matrix containing the first $k$ columns of $V$. The full reconstruction is obtained by adding the mean back: $\\widehat{\\mathbf{q}}_i^{(k)} = \\boldsymbol{\\mu} + (X_k)_i$. For $k=2$, we use all principal components, so $V_2=V$. Since $V$ is orthogonal ($V V^\\top = I$), the reconstruction is perfect: $\\widehat{\\mathbf{q}}_i^{(2)} = \\boldsymbol{\\mu} + (\\mathbf{q}_i - \\boldsymbol{\\mu}) = \\mathbf{q}_i$.\n\nFinally, for each reconstruction level $k$, we compute the reconstructed scalar coordinate $\\hat{s}_i^{(k)} = \\mathbf{v}_{\\mathrm{b}}^\\top \\widehat{\\mathbf{q}}_i^{(k)}$ and evaluate its quality using the Mean Squared Error (MSE):\n$$\n\\mathrm{MSE}(k) = \\frac{1}{N} \\sum_{i=1}^N \\left(\\hat{s}^{(k)}_i - s_i\\right)^2\n$$\nFor a given error threshold $\\varepsilon$, we seek the smallest integer $k \\in \\{0, 1, 2\\}$ such that $\\mathrm{MSE}(k) \\le \\varepsilon$. Since $\\mathrm{MSE}(2)=0$ (up to numerical precision), a solution always exists for $\\varepsilon \\ge 0$.\n\nThis entire procedure is applied to each of the three test cases specified, using a distinct, deterministic random seed for each case to ensure reproducibility. The minimal value of $k$ is reported for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a, k_y, c, beta, dt, n_steps, n_burn, epsilon)\n        (1.2, 5.0, 1.0, 0.7, 5e-4, 200000, 20000, 10.0),\n        (1.2, 5.0, 1.0, 0.7, 5e-4, 200000, 20000, 0.2),\n        (1.2, 5.0, 1.0, 0.7, 5e-4, 200000, 20000, 1e-10),\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        # Unpack parameters\n        a, k_y, c, beta, dt, n_steps, n_burn, epsilon = case\n        \n        # Set a deterministic seed for reproducibility for each case\n        np.random.seed(i)\n\n        # Task 1: Simulate an overdamped Langevin trajectory\n        q = np.zeros((n_steps, 2))\n        q[0] = [a, 0.0]\n        noise_factor = np.sqrt(2 * dt / beta)\n\n        for n in range(n_steps - 1):\n            x, y = q[n]\n            grad_x = 4 * x * (x**2 - a**2) + c * y\n            grad_y = k_y * y + c * x\n            grad = np.array([grad_x, grad_y])\n            xi = np.random.randn(2)\n            q[n+1] = q[n] - dt * grad + noise_factor * xi\n        \n        # Discard burn-in samples\n        samples = q[n_burn:]\n        N = len(samples)\n\n        # Task 2: Compute the barrier-crossing eigenvector\n        Hessian = np.array([[-4 * a**2, c], [c, k_y]])\n        # np.linalg.eigh sorts eigenvalues in ascending order\n        # and returns normalized eigenvectors.\n        eigvals, eigvecs = np.linalg.eigh(Hessian)\n        v_b = eigvecs[:, 0]\n        \n        # True barrier-crossing scalar coordinate for all samples\n        s = samples @ v_b\n\n        # Task 3: Perform PCA and reconstruct\n        q_mean = np.mean(samples, axis=0)\n        centered_samples = samples - q_mean\n        \n        # SVD: a is centered_samples, vh is V.T\n        u, s_vals, vh = np.linalg.svd(centered_samples, full_matrices=False)\n        V = vh.T\n        \n        # Task 4: Compute Mean Squared Error for each k\n        mse_values = []\n\n        # k = 0: Reconstruction is the mean\n        q_hat_0 = q_mean\n        s_hat_0 = q_hat_0 @ v_b\n        mse_0 = np.mean((s - s_hat_0)**2)\n        mse_values.append(mse_0)\n\n        # k = 1: Reconstruction using the first PC\n        V1 = V[:, 0:1] # Shape (2, 1) to ensure correct matrix multiplication\n        reconstructed_centered_1 = (centered_samples @ V1) @ V1.T\n        q_hat_1 = reconstructed_centered_1 + q_mean\n        s_hat_1 = q_hat_1 @ v_b\n        mse_1 = np.mean((s - s_hat_1)**2)\n        mse_values.append(mse_1)\n\n        # k = 2: Perfect reconstruction\n        # MSE is theoretically 0, practically close to machine epsilon.\n        mse_2 = 0.0\n        mse_values.append(mse_2)\n\n        # Task 5: Determine the minimal k\n        min_k = -1\n        for k, mse in enumerate(mse_values):\n            if mse = epsilon:\n                min_k = k\n                break\n        \n        results.append(min_k)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world molecular dynamics simulations are almost always performed under periodic boundary conditions (PBCs), which can introduce serious artifacts if not handled correctly. This practice  challenges you to explore how different PBC imaging choices—from a naive approach to the physically correct minimum image convention—can distort the covariance structure of molecular features. This exercise highlights the critical importance of proper data preprocessing before any PCA is performed, driving home the principle of \"garbage in, garbage out.\"",
            "id": "3437455",
            "problem": "You are asked to implement a complete program that constructs synthetic molecular dynamics trajectories under periodic boundary conditions, computes pairwise distance fluctuations under different imaging choices, and performs Principal Component Analysis (PCA) of these fluctuations. The goal is to quantify how periodic boundary unwrapping and imaging choices affect the covariance eigenstructure of pairwise distance features.\n\nBegin from the following fundamental base and definitions:\n\n- Molecular dynamics positions evolve according to Newton’s laws in a periodic cubic simulation box of side length $L$, but for the purposes of this problem you will directly construct synthetic, time-discrete positions.\n- A periodic boundary condition (PBC) in a cubic box identifies a particle at position $\\mathbf{r}$ with any image $\\mathbf{r} + \\mathbf{n} L$, where $\\mathbf{n} \\in \\mathbb{Z}^{3}$.\n- The wrapped coordinate $\\mathbf{r}^{\\mathrm{wrap}}$ is the representative reduced into the interval $[0, L)$ along each Cartesian axis using modulo $L$.\n- The minimum image convention (MIC) defines the displacement between two wrapped positions $\\mathbf{r}_{i}^{\\mathrm{wrap}}$ and $\\mathbf{r}_{j}^{\\mathrm{wrap}}$ as $\\Delta \\mathbf{r}_{ij}^{\\mathrm{MIC}} = \\mathbf{r}_{i}^{\\mathrm{wrap}} - \\mathbf{r}_{j}^{\\mathrm{wrap}} - L \\,\\mathrm{round}\\!\\left( \\dfrac{\\mathbf{r}_{i}^{\\mathrm{wrap}} - \\mathbf{r}_{j}^{\\mathrm{wrap}}}{L} \\right)$, applied component-wise, which brings the displacement into $[-L/2, L/2)$.\n- An unwrapped trajectory $\\mathbf{r}_{i}^{\\mathrm{unw}}(t)$ reconstructs continuous atomic paths by accumulating minimum-image displacements frame-to-frame: if $\\mathbf{r}_{i}^{\\mathrm{wrap}}(t)$ are wrapped positions, then $\\Delta \\mathbf{d}_{i}(t) = \\mathbf{r}_{i}^{\\mathrm{wrap}}(t) - \\mathbf{r}_{i}^{\\mathrm{wrap}}(t-1)$, $\\Delta \\mathbf{d}_{i}^{\\mathrm{MIC}}(t) = \\Delta \\mathbf{d}_{i}(t) - L \\,\\mathrm{round}\\!\\left( \\dfrac{\\Delta \\mathbf{d}_{i}(t)}{L} \\right)$, and $\\mathbf{r}_{i}^{\\mathrm{unw}}(t) = \\mathbf{r}_{i}^{\\mathrm{unw}}(t-1) + \\Delta \\mathbf{d}_{i}^{\\mathrm{MIC}}(t)$ with $\\mathbf{r}_{i}^{\\mathrm{unw}}(0) = \\mathbf{r}_{i}^{\\mathrm{wrap}}(0)$.\n- For $N$ atoms, define the feature vector at time $t$ as the list of all unique pairwise Euclidean distances $d_{ij}(t)$ for $1 \\le i  j \\le N$, arranged in a fixed order. For a time series of $T$ frames, assemble a $T \\times M$ matrix $\\mathbf{F}$ of these features where $M = N(N-1)/2$.\n- Compute fluctuations by subtracting the temporal mean: $\\tilde{\\mathbf{F}} = \\mathbf{F} - \\mathbf{1}\\,\\boldsymbol{\\mu}^{\\top}$ where $\\boldsymbol{\\mu}$ is the mean across time and $\\mathbf{1}$ is a length-$T$ vector of ones. The covariance matrix across features is $\\mathbf{C} = \\dfrac{1}{T-1}\\,\\tilde{\\mathbf{F}}^{\\top}\\tilde{\\mathbf{F}}$.\n- Principal Component Analysis (PCA) of covariance $\\mathbf{C}$ is given by its eigen-decomposition $\\mathbf{C}\\mathbf{v}_{k} = \\lambda_{k}\\mathbf{v}_{k}$ with eigenvalues $\\lambda_{k}$ sorted in nonincreasing order. The leading eigenvalue $\\lambda_{\\max}$ quantifies the dominant variance mode. The trace $\\mathrm{tr}(\\mathbf{C})$ equals the sum of feature variances.\n\nYou must implement three imaging choices to compute the pairwise distances $d_{ij}(t)$:\n- Wrapped-naive: use $\\|\\mathbf{r}^{\\mathrm{wrap}}_{i}(t) - \\mathbf{r}^{\\mathrm{wrap}}_{j}(t)\\|_{2}$ without MIC.\n- Minimum image: use $\\|\\Delta \\mathbf{r}_{ij}^{\\mathrm{MIC}}(t)\\|_{2}$.\n- Unwrapped: compute $\\mathbf{r}^{\\mathrm{unw}}(t)$ as above, then use $\\|\\mathbf{r}^{\\mathrm{unw}}_{i}(t) - \\mathbf{r}^{\\mathrm{unw}}_{j}(t)\\|_{2}$.\n\nAll distances are to be treated in ångström units (Å), but the requested outputs are dimensionless ratios.\n\nConstruct the following test suite of three trajectories, each in a cubic box and each given as wrapped positions generated from initial positions and constant per-frame displacements, all in three spatial dimensions:\n\n- Test case $1$ (no boundary crossing, baseline):\n  - Box length $L = 10$.\n  - Number of atoms $N = 3$, number of frames $T = 5$ with frames indexed by $t \\in \\{0,1,2,3,4\\}$.\n  - For atom $0$: initial position $(1.0, 2.0, 3.0)$ and per-frame increment $(0.1, -0.05, 0.0)$.\n  - For atom $1$: initial position $(2.0, 2.5, 3.5)$ and per-frame increment $(0.0, 0.05, 0.0)$.\n  - For atom $2$: initial position $(8.5, 1.0, 2.0)$ and per-frame increment $(0.0, 0.0, 0.1)$.\n  - Wrapped positions are obtained by applying modulo $L$ per component after adding $t$ increments; in this case no component reaches $L$.\n\n- Test case $2$ (single-atom boundary crossing, artifact-prone for naive wrapping):\n  - Box length $L = 10$.\n  - Number of atoms $N = 3$, number of frames $T = 5$ with $t \\in \\{0,1,2,3,4\\}$.\n  - For atom $0$: initial position $(9.8, 1.0, 1.0)$ and per-frame increment $(0.3, 0.0, 0.0)$, wrapped modulo $L$ per frame.\n  - For atom $1$: initial position $(1.5, 1.0, 1.0)$ and per-frame increment $(0.0, 0.0, 0.0)$.\n  - For atom $2$: initial position $(5.0, 5.0, 5.0)$ and per-frame increment $(0.0, 0.0, 0.0)$.\n\n- Test case $3$ (counter-propagation across the boundary, MIC stability versus unwrapped divergence):\n  - Box length $L = 10$.\n  - Number of atoms $N = 3$, number of frames $T = 6$ with $t \\in \\{0,1,2,3,4,5\\}$.\n  - For atom $0$: initial position $(9.5, 0.0, 0.0)$ and per-frame increment $(0.4, 0.0, 0.0)$, wrapped modulo $L$ per frame.\n  - For atom $1$: initial position $(0.5, 0.0, 0.0)$ and per-frame increment $(-0.4, 0.0, 0.0)$, wrapped modulo $L$ per frame.\n  - For atom $2$: initial position $(5.0, 0.0, 0.0)$ and per-frame increment $(0.0, 0.0, 0.0)$.\n\nFor each test case, you must:\n- Construct the wrapped trajectory $\\mathbf{r}^{\\mathrm{wrap}}_{i}(t)$ from the specified initial positions and increments using modulo $L$ per component.\n- Compute three feature matrices $\\mathbf{F}^{\\mathrm{wrap}}$, $\\mathbf{F}^{\\mathrm{MIC}}$, and $\\mathbf{F}^{\\mathrm{unw}}$ composed of all unique pairwise distances at each frame according to the three imaging choices.\n- Compute the corresponding covariance matrices $\\mathbf{C}^{\\mathrm{wrap}}$, $\\mathbf{C}^{\\mathrm{MIC}}$, and $\\mathbf{C}^{\\mathrm{unw}}$ across features as defined above.\n- Compute the leading eigenvalues $\\lambda_{\\max}^{\\mathrm{wrap}}$, $\\lambda_{\\max}^{\\mathrm{MIC}}$, and $\\lambda_{\\max}^{\\mathrm{unw}}$, and the traces $\\tau^{\\mathrm{wrap}} = \\mathrm{tr}(\\mathbf{C}^{\\mathrm{wrap}})$ and $\\tau^{\\mathrm{MIC}} = \\mathrm{tr}(\\mathbf{C}^{\\mathrm{MIC}})$.\n\nYour program must output, for each test case in order $\\{1,2,3\\}$, the following three quantities:\n- $r_{w} = \\dfrac{\\lambda_{\\max}^{\\mathrm{wrap}}}{\\lambda_{\\max}^{\\mathrm{MIC}}}$,\n- $r_{u} = \\dfrac{\\lambda_{\\max}^{\\mathrm{unw}}}{\\lambda_{\\max}^{\\mathrm{MIC}}}$,\n- $r_{\\tau} = \\dfrac{\\tau^{\\mathrm{wrap}}}{\\tau^{\\mathrm{MIC}}}$.\n\nNumerical output requirements:\n- Express each of the above three ratios as a decimal rounded to exactly $6$ digits after the decimal point.\n- Aggregate the results for the three test cases into a single line in the following exact format: a comma-separated list enclosed in square brackets, flattening the triplets per test case in order. For example, the output should look like $[r_{w}^{(1)},r_{u}^{(1)},r_{\\tau}^{(1)},r_{w}^{(2)},r_{u}^{(2)},r_{\\tau}^{(2)},r_{w}^{(3)},r_{u}^{(3)},r_{\\tau}^{(3)}]$ with each entry rounded to exactly six decimal places.\n\nAngles do not appear in this problem. No physical unit conversion is required for the final output because all ratios are dimensionless. Your program must be fully self-contained, take no input, and use only the specified libraries.",
            "solution": "The user-provided problem statement has been meticulously validated and is determined to be scientifically grounded, well-posed, and objective. It provides a complete and consistent set of definitions and data to solve a problem relevant to computational chemistry and molecular biophysics. The problem requires the implementation of an algorithm to quantify the effect of periodic boundary condition (PBC) imaging choices on the statistical properties of pairwise molecular distances, specifically through Principal Component Analysis (PCA).\n\nThe solution proceeds systematically through the following steps:\n1.  Generation of synthetic atomic trajectories under periodic boundary conditions.\n2.  Calculation of pairwise distance feature matrices using three distinct imaging methods.\n3.  Computation of the covariance matrix for each feature set and its subsequent eigen-decomposition (PCA).\n4.  Calculation of specified ratios of eigenvalues and traces to quantify the differences between imaging methods.\n\nEach step is detailed below, adhering to the mathematical formalism defined in the problem statement.\n\nFirst, we construct the time-series of atomic positions. For each of the $N$ atoms, a wrapped trajectory $\\mathbf{r}_{i}^{\\mathrm{wrap}}(t)$ is generated for $T$ time frames, indexed by $t \\in \\{0, 1, \\dots, T-1\\}$. Given an initial position $\\mathbf{r}_{i}(0)$ and a constant per-frame displacement vector $\\Delta\\mathbf{r}_{i}$, the position at time $t$ is calculated before wrapping as $\\mathbf{r}_{i}^{\\mathrm{unbounded}}(t) = \\mathbf{r}_{i}(0) + t\\Delta\\mathbf{r}_{i}$. The wrapped trajectory is then obtained by applying the modulo-$L$ operation component-wise, where $L$ is the side length of the cubic simulation box:\n$$\n\\mathbf{r}_{i}^{\\mathrm{wrap}}(t) = \\mathbf{r}_{i}^{\\mathrm{unbounded}}(t) \\pmod L\n$$\nThis places each coordinate of each atom into the principal simulation box, defined by the interval $[0, L)$.\n\nFrom the wrapped trajectory $\\mathbf{r}^{\\mathrm{wrap}}(t)$, we derive the unwrapped trajectory $\\mathbf{r}^{\\mathrm{unw}}(t)$. This process reconstructs a continuous path for each atom, accounting for jumps across the periodic boundary. It is defined recursively. The initial condition is $\\mathbf{r}_{i}^{\\mathrm{unw}}(0) = \\mathbf{r}_{i}^{\\mathrm{wrap}}(0)$. For subsequent frames $t  0$, the update rule is:\n$$\n\\mathbf{r}_{i}^{\\mathrm{unw}}(t) = \\mathbf{r}_{i}^{\\mathrm{unw}}(t-1) + \\Delta \\mathbf{d}_{i}^{\\mathrm{MIC}}(t)\n$$\nwhere $\\Delta \\mathbf{d}_{i}^{\\mathrm{MIC}}(t)$ is the minimum image displacement of the atom $i$ between frame $t-1$ and $t$:\n$$\n\\Delta \\mathbf{d}_{i}^{\\mathrm{MIC}}(t) = \\left( \\mathbf{r}_{i}^{\\mathrm{wrap}}(t) - \\mathbf{r}_{i}^{\\mathrm{wrap}}(t-1) \\right) - L \\,\\mathrm{round}\\!\\left( \\frac{\\mathbf{r}_{i}^{\\mathrm{wrap}}(t) - \\mathbf{r}_{i}^{\\mathrm{wrap}}(t-1)}{L} \\right)\n$$\nThe `round` function is applied component-wise to the vector argument. This ensures that any single-frame displacement greater than $L/2$ in magnitude is interpreted as motion across the boundary into the nearest periodic image.\n\nNext, we define the feature vectors. The features are the $M = N(N-1)/2$ unique pairwise Euclidean distances $d_{ij}(t)$ between atoms $i$ and $j$ at each time frame $t$. We compute these distances using three different imaging choices, resulting in three distinct feature matrices: $\\mathbf{F}^{\\mathrm{wrap}}$, $\\mathbf{F}^{\\mathrm{MIC}}$, and $\\mathbf{F}^{\\mathrm{unw}}$, each of size $T \\times M$.\n1.  **Wrapped-naive distance ($d_{ij}^{\\mathrm{wrap}}$)**: This method ignores PBCs and computes the distance between coordinates as they are stored in the wrapped trajectory file. This can lead to large, artifactual distances when a pair of atoms is separated by a boundary.\n    $$\n    d_{ij}^{\\mathrm{wrap}}(t) = \\left\\| \\mathbf{r}_{i}^{\\mathrm{wrap}}(t) - \\mathbf{r}_{j}^{\\mathrm{wrap}}(t) \\right\\|_{2}\n    $$\n2.  **Minimum image distance ($d_{ij}^{\\mathrm{MIC}}$)**: This method correctly computes the shortest distance between two atoms in a periodic system by applying the minimum image convention (MIC) to their displacement vector.\n    $$\n    d_{ij}^{\\mathrm{MIC}}(t) = \\left\\| \\Delta \\mathbf{r}_{ij}^{\\mathrm{MIC}}(t) \\right\\|_{2} = \\left\\| \\left( \\mathbf{r}_{i}^{\\mathrm{wrap}}(t) - \\mathbf{r}_{j}^{\\mathrm{wrap}}(t) \\right) - L \\,\\mathrm{round}\\!\\left( \\frac{\\mathbf{r}_{i}^{\\mathrm{wrap}}(t) - \\mathbf{r}_{j}^{\\mathrm{wrap}}(t)}{L} \\right) \\right\\|_{2}\n    $$\n3.  **Unwrapped distance ($d_{ij}^{\\mathrm{unw}}$)**: This method computes the distance between atoms in the unwrapped, continuous coordinate system.\n    $$\n    d_{ij}^{\\mathrm{unw}}(t) = \\left\\| \\mathbf{r}_{i}^{\\mathrm{unw}}(t) - \\mathbf{r}_{j}^{\\mathrm{unw}}(t) \\right\\|_{2}\n    $$\n\nWith the feature matrices assembled, we proceed to the statistical analysis. For each feature matrix $\\mathbf{F}$ (representing any of $\\mathbf{F}^{\\mathrm{wrap}}$, $\\mathbf{F}^{\\mathrm{MIC}}$, or $\\mathbf{F}^{\\mathrm{unw}}$), we compute its covariance matrix $\\mathbf{C}$. First, the temporal mean vector $\\boldsymbol{\\mu}$ of the features is calculated.\n$$\n\\boldsymbol{\\mu} = \\frac{1}{T} \\sum_{t=0}^{T-1} \\mathbf{F}_{t,:}\n$$\nThen, the feature matrix is mean-centered: $\\tilde{\\mathbf{F}} = \\mathbf{F} - \\mathbf{1}\\boldsymbol{\\mu}^{\\top}$, where $\\mathbf{1}$ is a column vector of ones of length $T$. The sample covariance matrix is then computed as:\n$$\n\\mathbf{C} = \\frac{1}{T-1} \\tilde{\\mathbf{F}}^{\\top} \\tilde{\\mathbf{F}}\n$$\nThis $M \\times M$ matrix describes the variance of each feature along its diagonal and the covariance between pairs of features in its off-diagonal elements.\n\nThe core of the analysis is Principal Component Analysis (PCA), which involves the eigen-decomposition of the covariance matrix:\n$$\n\\mathbf{C}\\mathbf{v}_{k} = \\lambda_{k}\\mathbf{v}_{k}\n$$\nwhere $\\mathbf{v}_k$ are the eigenvectors (principal components) and $\\lambda_k$ are the corresponding eigenvalues. The eigenvalues, sorted non-increasingly $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_M \\ge 0$, represent the amount of variance captured by each principal component. The leading eigenvalue, $\\lambda_{\\max} = \\lambda_1$, quantifies the variance of the most dominant mode of fluctuation in the feature set. The trace of the covariance matrix, $\\mathrm{tr}(\\mathbf{C}) = \\sum_{k=1}^{M} \\lambda_k$, represents the total variance across all features.\n\nFinally, for each test case, we compute the following dimensionless ratios to compare the statistical signatures of the three imaging methods:\n-   $r_{w} = \\dfrac{\\lambda_{\\max}^{\\mathrm{wrap}}}{\\lambda_{\\max}^{\\mathrm{MIC}}}$: This ratio measures the inflation of the dominant variance mode due to naive-wrapped PBC artifacts relative to the physically correct MIC.\n-   $r_{u} = \\dfrac{\\lambda_{\\max}^{\\mathrm{unw}}}{\\lambda_{\\max}^{\\mathrm{MIC}}}$: This ratio compares the dominant variance of the globally continuous unwrapped trajectory to that of the locally correct MIC. This can highlight artifacts from long-timescale diffusion in the unwrapped representation.\n-   $r_{\\tau} = \\dfrac{\\tau^{\\mathrm{wrap}}}{\\tau^{\\mathrm{MIC}}}$, where $\\tau = \\mathrm{tr}(\\mathbf{C})$: This ratio measures the inflation of the total feature variance due to naive-wrapped PBC artifacts.\n\nThe implementation encapsulates this entire procedure, applying it to each of the three specified test cases and producing the required ratios, formatted to six decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the final result.\n    \"\"\"\n\n    def mic_displacement(delta_r, L):\n        \"\"\"\n        Computes the minimum image convention displacement vector(s).\n        \n        Args:\n            delta_r (np.ndarray): The displacement vector(s) without PBC.\n            L (float): The box length.\n        \n        Returns:\n            np.ndarray: The displacement vector(s) under MIC.\n        \"\"\"\n        return delta_r - L * np.round(delta_r / L)\n\n    def perform_pca(F):\n        \"\"\"\n        Computes the covariance matrix, its leading eigenvalue, and its trace.\n        \n        Args:\n            F (np.ndarray): The T x M feature matrix.\n        \n        Returns:\n            tuple[float, float]: The leading eigenvalue and the trace of the covariance matrix.\n        \"\"\"\n        T = F.shape[0]\n        if T  2:\n            return 0.0, 0.0\n\n        # rowvar=False: columns are variables (features), rows are observations (time)\n        # ddof=1: use sample covariance (divide by T-1)\n        C = np.cov(F, rowvar=False, ddof=1)\n        \n        # If F has only one feature column, np.cov returns a 0-dim array (scalar)\n        if C.ndim == 0:\n            eigenvalues = np.array([C.item()])\n        else:\n            # eigh returns eigenvalues in ascending order for symmetric matrices\n            eigenvalues = np.linalg.eigh(C)[0]\n        \n        lambda_max = eigenvalues[-1] if len(eigenvalues)  0 else 0.0\n        trace = np.trace(C)\n        \n        return lambda_max, trace\n\n    def process_case(case_params):\n        \"\"\"\n        Processes a single test case from trajectory generation to ratio calculation.\n        \n        Args:\n            case_params (tuple): A tuple containing L, N, T, initial_pos, and increments.\n        \n        Returns:\n            tuple[float, float, float]: The calculated ratios r_w, r_u, r_tau.\n        \"\"\"\n        L, N, T, initial_pos, increments = case_params\n\n        # 1. Generate wrapped trajectory\n        t_vals = np.arange(T).reshape(T, 1, 1)\n        initial_pos_b = np.expand_dims(initial_pos, 0)\n        increments_b = np.expand_dims(increments, 0)\n        unbounded_traj = initial_pos_b + t_vals * increments_b\n        wrapped_traj = np.mod(unbounded_traj, L)\n\n        # 2. Generate unwrapped trajectory\n        unwrapped_traj = np.zeros_like(wrapped_traj)\n        unwrapped_traj[0] = wrapped_traj[0]\n        for t in range(1, T):\n            delta_d = wrapped_traj[t] - wrapped_traj[t-1]\n            delta_d_mic = mic_displacement(delta_d, L)\n            unwrapped_traj[t] = unwrapped_traj[t-1] + delta_d_mic\n\n        # 3. Compute feature matrices\n        M = N * (N - 1) // 2\n        F_wrap = np.zeros((T, M))\n        F_mic = np.zeros((T, M))\n        F_unw = np.zeros((T, M))\n\n        pairs = [(i, j) for i in range(N) for j in range(i + 1, N)]\n\n        for t in range(T):\n            r_wrap_t = wrapped_traj[t]\n            r_unw_t = unwrapped_traj[t]\n            for k, (i, j) in enumerate(pairs):\n                # Wrapped-naive\n                delta_r_wrap_naive = r_wrap_t[i] - r_wrap_t[j]\n                F_wrap[t, k] = np.linalg.norm(delta_r_wrap_naive)\n\n                # MIC\n                delta_r_mic = mic_displacement(delta_r_wrap_naive, L)\n                F_mic[t, k] = np.linalg.norm(delta_r_mic)\n\n                # Unwrapped\n                delta_r_unw = r_unw_t[i] - r_unw_t[j]\n                F_unw[t, k] = np.linalg.norm(delta_r_unw)\n\n        # 4. Perform PCA for each feature matrix\n        lambda_max_wrap, trace_wrap = perform_pca(F_wrap)\n        lambda_max_mic, trace_mic = perform_pca(F_mic)\n        lambda_max_unw, _ = perform_pca(F_unw)\n\n        # 5. Calculate and return ratios\n        # Handle potential division by zero if MIC variance is zero\n        r_w = (lambda_max_wrap / lambda_max_mic) if lambda_max_mic != 0 else 0.0\n        r_u = (lambda_max_unw / lambda_max_mic) if lambda_max_mic != 0 else 0.0\n        r_tau = (trace_wrap / trace_mic) if trace_mic != 0 else 0.0\n        \n        return r_w, r_u, r_tau\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (10.0, 3, 5, \n         np.array([[1.0, 2.0, 3.0], [2.0, 2.5, 3.5], [8.5, 1.0, 2.0]]),\n         np.array([[0.1, -0.05, 0.0], [0.0, 0.05, 0.0], [0.0, 0.0, 0.1]])),\n        (10.0, 3, 5,\n         np.array([[9.8, 1.0, 1.0], [1.5, 1.0, 1.0], [5.0, 5.0, 5.0]]),\n         np.array([[0.3, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])),\n        (10.0, 3, 6,\n         np.array([[9.5, 0.0, 0.0], [0.5, 0.0, 0.0], [5.0, 0.0, 0.0]]),\n         np.array([[0.4, 0.0, 0.0], [-0.4, 0.0, 0.0], [0.0, 0.0, 0.0]]))\n    ]\n\n    results = []\n    for case in test_cases:\n        ratios = process_case(case)\n        results.extend(ratios)\n\n    # Format the final output string with rounding to 6 decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A key challenge in PCA is distinguishing meaningful collective motions from the vast background of random thermal fluctuations. This exercise introduces a powerful tool from random matrix theory, the Marchenko-Pastur law, to establish a principled threshold for significance . By deriving and applying this theoretical \"noise floor,\" you will learn how to move beyond simple scree plot heuristics and make statistically robust judgments about which principal components represent true signals.",
            "id": "3437421",
            "problem": "An all-atom molecular dynamics trajectory has been aligned to a reference structure to remove overall rotation and translation, and its Cartesian displacements have been mean-centered. Let the centered data matrix be $X_{c} \\in \\mathbb{R}^{3N \\times (T-1)}$, whose $j$-th column contains the $3N$ Cartesian displacements at time $t_{j}$ relative to the time-average. The sample coordinate covariance matrix is defined by\n\n$$\nC \\;=\\; \\frac{1}{T-1}\\,X_{c}\\,X_{c}^{\\top} \\in \\mathbb{R}^{3N \\times 3N}.\n$$\n\nAssume an idealized null model in which the entries of $X_{c}$ are independent and identically distributed with mean $0$ and variance $\\sigma^{2}$, and that the high-dimensional limit is taken with the aspect ratio $\\gamma = \\frac{3N}{T-1}$ held fixed. Using Marchenko–Pastur theory, derive the asymptotic bounds that delimit the support of the noise-only eigenvalue spectrum of $C$ as explicit functions of $\\sigma^{2}$ and $\\gamma$. Then apply these bounds to threshold significant Principal Components (PCs) in the following setting.\n\nConsider a trajectory with $N=1000$ atoms and $T=1200$ frames, so that $\\gamma = \\frac{3N}{T-1}$. Suppose the per-coordinate variance is known to be $\\sigma^{2} = 0.01\\ \\text{nm}^2$. The top $12$ sample eigenvalues (variances along the first $12$ PCs) of $C$, expressed in $\\text{nm}^2$, are observed to be\n\n$$\n0.500,\\;\\;0.200,\\;\\;0.100,\\;\\;0.080,\\;\\;0.070,\\;\\;0.0685,\\;\\;0.0671,\\;\\;0.0610,\\;\\;0.0580,\\;\\;0.0560,\\;\\;0.0520,\\;\\;0.0490.\n$$\n\nAssume all remaining $3N-12$ eigenvalues are strictly less than $0.060\\ \\text{nm}^2$. Using the noise-only upper edge from your derived bounds as a hard threshold, determine how many PCs are significant, i.e., how many eigenvalues exceed the noise-only upper edge. For the purpose of the threshold comparison, compute the upper edge numerically and round it to four significant figures. Report the final answer as a single integer (the count of significant PCs). Do not include units in your final answer.",
            "solution": "The problem asks for two main tasks: first, to derive the bounds of the eigenvalue spectrum for a sample covariance matrix under a specific null model using Marchenko-Pastur theory; second, to apply these bounds to a given dataset to determine the number of statistically significant principal components.\n\nFirst, we address the derivation of the spectral bounds.\nThe problem defines a centered data matrix $X_c \\in \\mathbb{R}^{3N \\times (T-1)}$ and a sample coordinate covariance matrix $C \\in \\mathbb{R}^{3N \\times 3N}$ as:\n$$\nC = \\frac{1}{T-1} X_c X_c^{\\top}\n$$\nLet us define the dimensions of the data matrix as $p = 3N$ and $n = T-1$. The covariance matrix is thus $C = \\frac{1}{n} X_c X_c^{\\top}$, which is a $p \\times p$ matrix. The aspect ratio is given as $\\gamma = \\frac{p}{n} = \\frac{3N}{T-1}$.\n\nThe null model assumes that the entries of $X_c$, denoted $x_{ij}$, are independent and identically distributed (i.i.d.) random variables with mean $\\mathbb{E}[x_{ij}] = 0$ and variance $\\text{Var}(x_{ij}) = \\sigma^2$.\n\nThe Marchenko-Pastur theorem describes the limiting eigenvalue distribution of sample covariance matrices. The standard form of the theorem applies to a matrix whose elements have unit variance. To use it, we first rescale the data matrix $X_c$. Let us define a new matrix $Y \\in \\mathbb{R}^{p \\times n}$ with entries $y_{ij} = x_{ij}/\\sigma$. The entries of $Y$ are i.i.d. random variables with mean $\\mathbb{E}[y_{ij}] = 0$ and variance $\\text{Var}(y_{ij}) = \\mathbb{E}[(x_{ij}/\\sigma)^2] = \\frac{1}{\\sigma^2}\\mathbb{E}[x_{ij}^2] = \\frac{\\sigma^2}{\\sigma^2} = 1$.\n\nWe can express the matrix $X_c$ in terms of $Y$ as $X_c = \\sigma Y$. Substituting this into the definition of $C$:\n$$\nC = \\frac{1}{n} (\\sigma Y)(\\sigma Y)^{\\top} = \\sigma^2 \\left( \\frac{1}{n} Y Y^{\\top} \\right)\n$$\nLet us define the standard Wishart matrix $S = \\frac{1}{n} Y Y^{\\top}$. The eigenvalues of $C$, denoted $\\lambda_C$, are directly proportional to the eigenvalues of $S$, denoted $\\lambda_S$, via the relation $\\lambda_C = \\sigma^2 \\lambda_S$.\n\nAccording to the Marchenko-Pastur theorem, in the high-dimensional limit where $p,n \\to \\infty$ such that the ratio $p/n \\to \\gamma$, the probability density function of the eigenvalues of $S$ has a support defined by a compact interval $[\\lambda_-, \\lambda_+]$. The bounds of this support are given by:\n$$\n\\lambda_{\\pm} = (1 \\pm \\sqrt{\\gamma})^2\n$$\nThis formula holds for any $\\gamma  0$. If $\\gamma  1$, the spectrum of $S$ also contains a point mass of $1-1/\\gamma$ at eigenvalue $0$, but the continuous part of the spectrum remains within the same interval.\n\nSince the eigenvalues of $C$ are scaled versions of the eigenvalues of $S$ (i.e., $\\lambda_C = \\sigma^2 \\lambda_S$), the support of the eigenvalue spectrum of $C$ under the null model, denoted $[\\lambda_{C,-}, \\lambda_{C,+}]$, is given by:\n$$\n\\lambda_{C,-} = \\sigma^2 \\lambda_- = \\sigma^2 (1 - \\sqrt{\\gamma})^2\n$$\n$$\n\\lambda_{C,+} = \\sigma^2 \\lambda_+ = \\sigma^2 (1 + \\sqrt{\\gamma})^2\n$$\nThese are the derived asymptotic bounds for the noise-only eigenvalue spectrum of $C$. The upper bound, $\\lambda_{C,+}$, represents the maximum eigenvalue expected from random noise alone under the specified model conditions. Any observed eigenvalue significantly exceeding this threshold can be considered to represent a non-random, or significant, signal.\n\nNext, we apply this result to the specific case provided. The parameters are:\n- Number of atoms, $N = 1000$.\n- Number of trajectory frames, $T = 1200$.\n- Per-coordinate variance, $\\sigma^2 = 0.01 \\text{ nm}^2$.\n\nFrom these parameters, we compute the matrix dimensions and the aspect ratio $\\gamma$:\n- The number of rows is $p = 3N = 3 \\times 1000 = 3000$.\n- The number of columns is $n = T-1 = 1200 - 1 = 1199$.\n- The aspect ratio is $\\gamma = \\frac{p}{n} = \\frac{3000}{1199}$.\n\nNow, we calculate the upper edge of the noise spectrum, $\\lambda_{C,+}$, using the derived formula:\n$$\n\\lambda_{C,+} = \\sigma^2 (1 + \\sqrt{\\gamma})^2 = 0.01 \\times \\left(1 + \\sqrt{\\frac{3000}{1199}}\\right)^2\n$$\nWe compute the numerical value:\n$$\n\\gamma = \\frac{3000}{1199} \\approx 2.502085\n$$\n$$\n\\sqrt{\\gamma} \\approx \\sqrt{2.502085} \\approx 1.581798\n$$\n$$\n\\lambda_{C,+} \\approx 0.01 \\times (1 + 1.581798)^2 = 0.01 \\times (2.581798)^2 \\approx 0.01 \\times 6.665675 \\approx 0.06665675 \\text{ nm}^2\n$$\nThe problem requires rounding this value to four significant figures. This yields:\n$$\n\\lambda_{\\text{thresh}} = 0.06666 \\text{ nm}^2\n$$\nThis value serves as our hard threshold. An observed eigenvalue $\\lambda_{\\text{obs}}$ from the data is considered significant if $\\lambda_{\\text{obs}}  \\lambda_{\\text{thresh}}$.\n\nThe top $12$ observed eigenvalues are:\n$0.500, 0.200, 0.100, 0.080, 0.070, 0.0685, 0.0671, 0.0610, 0.0580, 0.0560, 0.0520, 0.0490$.\n\nWe compare each eigenvalue to the threshold $\\lambda_{\\text{thresh}} = 0.06666$:\n1. $0.500  0.06666$ (Significant)\n2. $0.200  0.06666$ (Significant)\n3. $0.100  0.06666$ (Significant)\n4. $0.080  0.06666$ (Significant)\n5. $0.070  0.06666$ (Significant)\n6. $0.0685  0.06666$ (Significant)\n7. $0.0671  0.06666$ (Significant)\n8. $0.0610  0.06666$ (Not significant)\n9. $0.0580  0.06666$ (Not significant)\n10. $0.0560  0.06666$ (Not significant)\n11. $0.0520  0.06666$ (Not significant)\n12. $0.0490  0.06666$ (Not significant)\n\nThe remaining $3N-12$ eigenvalues are all strictly less than $0.060 \\text{ nm}^2$, which is also below the threshold of $0.06666 \\text{ nm}^2$. Therefore, none of the remaining eigenvalues are significant.\n\nBy counting the eigenvalues that exceed the threshold, we find there are $7$ significant principal components.",
            "answer": "$$\n\\boxed{7}\n$$"
        }
    ]
}