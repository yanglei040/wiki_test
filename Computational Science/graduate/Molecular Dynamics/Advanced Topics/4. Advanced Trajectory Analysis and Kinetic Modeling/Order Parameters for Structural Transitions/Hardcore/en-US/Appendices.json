{
    "hands_on_practices": [
        {
            "introduction": "The static structure factor, $S(\\mathbf{k})$, provides a direct bridge between atomic-scale configurations and experimental diffraction patterns, making it a fundamental tool for phase identification. This exercise guides you through the process of computing $S(\\mathbf{k})$ directly from particle coordinates in a periodic system. By analyzing the resulting peak patterns in reciprocal space, you will learn to distinguish between a disordered liquid and common crystalline phases like body-centered cubic (BCC) and face-centered cubic (FCC), a foundational skill for analyzing simulation data.",
            "id": "3430930",
            "problem": "You are given a small set of atomic coordinates in a three-dimensional periodic cell and asked to compute the static structure factor on a grid of discrete wavevectors, then classify the phase based on the resulting peak pattern. The static structure factor is defined in terms of the microscopic number density and must be computed from first principles.\n\nFundamental base:\n- The microscopic number density is defined as $\\rho(\\mathbf{r}) = \\sum_{j=1}^{N} \\delta(\\mathbf{r} - \\mathbf{r}_j)$ where $N$ is the number of atoms and $\\mathbf{r}_j$ are their positions.\n- Its Fourier transform is $\\rho_{\\mathbf{k}} = \\int \\rho(\\mathbf{r}) e^{i \\mathbf{k} \\cdot \\mathbf{r}} \\, d^3\\mathbf{r} = \\sum_{j=1}^{N} e^{i \\mathbf{k} \\cdot \\mathbf{r}_j}$.\n- The static structure factor is defined as $S(\\mathbf{k}) = \\frac{1}{N} \\left| \\rho_{\\mathbf{k}} \\right|^2 = \\frac{1}{N} \\left| \\sum_{j=1}^{N} e^{i \\mathbf{k} \\cdot \\mathbf{r}_j} \\right|^2$.\n\nPeriodic boundary conditions imply that for a cubic cell of linear dimensions $L_x = L_y = L_z = a$, the discrete wavevectors consistent with the periodic box are $\\mathbf{k} = \\frac{2\\pi}{a} (m_x, m_y, m_z)$, where $m_x, m_y, m_z$ are integers. In this problem, use a uniform grid of integer triplets $(m_x, m_y, m_z)$ in the range $m_\\alpha \\in \\{-M, -M+1, \\dots, M-1, M\\}$ for $\\alpha \\in \\{x,y,z\\}$, excluding the zero vector $(0,0,0)$. All positions $\\mathbf{r}_j$ are expressed in Angstroms (Å), and all wavevector components $k_\\alpha$ are in inverse Angstroms (Å$^{-1}$).\n\nPhase classification objective:\n- Use the computed $S(\\mathbf{k})$ values to classify the configuration as one of the following three classes, encoded as integers:\n  - $0$: disordered (liquid-like),\n  - $1$: body-centered cubic (BCC),\n  - $2$: face-centered cubic (FCC).\n- The classification must be based solely on $S(\\mathbf{k})$ peak patterns and reciprocal-lattice selection rules derived from basis-induced interference:\n  - For a body-centered cubic basis with atoms at $\\mathbf{0}$ and $\\frac{a}{2}(1,1,1)$, the structure factor of the basis is $F(h,k,l) = 1 + e^{i\\pi(h+k+l)}$, so non-zero peaks occur when $h+k+l$ is even. The lowest-magnitude non-zero peaks satisfy $h^2 + k^2 + l^2 = 2$ (the $\\{110\\}$ family).\n  - For a face-centered cubic basis with atoms at $\\mathbf{0}$, $\\frac{a}{2}(0,1,1)$, $\\frac{a}{2}(1,0,1)$, and $\\frac{a}{2}(1,1,0)$, the structure factor of the basis is $F(h,k,l) = 1 + e^{i\\pi(k+l)} + e^{i\\pi(h+l)} + e^{i\\pi(h+k)}$, so non-zero peaks occur when $h,k,l$ are all even or all odd. The lowest-magnitude non-zero peaks satisfy $h^2 + k^2 + l^2 = 3$ (the $\\{111\\}$ family).\n- To decide between disordered versus crystalline, compare the maximum of $S(\\mathbf{k})$ to $N$ as follows. Define $R = \\frac{\\max_{\\mathbf{k}} S(\\mathbf{k})}{N}$. If $R  \\tau$ with $\\tau = 0.6$, classify as disordered ($0$). Otherwise, if crystalline, determine if the strongest peaks follow the parity rule of BCC or FCC and whether their lowest non-zero $h^2+k^2+l^2$ matches $2$ or $3$, respectively. Use these criteria to classify as BCC ($1$) or FCC ($2$). If the criteria conflict, fall back to the parity rule that matches the majority of the top peaks.\n\nAlgorithmic tasks:\n1. Compute $S(\\mathbf{k})$ over the specified grid for each test case, using the above definition. Use a grid range parameter $M=3$.\n2. For each case, compute $R = \\max_{\\mathbf{k}} S(\\mathbf{k}) / N$. If $R  0.6$, output $0$.\n3. If $R \\ge 0.6$, identify the set of top peaks defined as those $\\mathbf{k}$ with $S(\\mathbf{k}) \\ge 0.99 \\cdot \\max_{\\mathbf{k}} S(\\mathbf{k})$. Among these, compute:\n   - The minimal non-zero value of $h^2+k^2+l^2$ where $(h,k,l) = (m_x, m_y, m_z)$, and check whether it equals $2$ or $3$.\n   - The parity rule counts: number of top peaks with $h+k+l$ even (BCC-allowed) and number with $h,k,l$ all even or all odd (FCC-allowed).\n4. Classify BCC ($1$) if the minimal non-zero $h^2+k^2+l^2$ among the top peaks is $2$ and the majority of top peaks satisfy $h+k+l$ even; classify FCC ($2$) if the minimal non-zero $h^2+k^2+l^2$ among the top peaks is $3$ and the majority satisfy the all-even-or-all-odd rule. In any other crystalline case, choose the class by the parity rule with the larger count (BCC or FCC). If neither rule has a strict majority, classify as disordered ($0$).\n\nTest suite:\nUse the following four test cases. In all cases, the cubic box has $a = 3.0$ Å and the wavevector grid uses $M=3$.\n- Case $1$ (ideal BCC): $N=2$ atoms at positions $\\mathbf{r}_1 = (0,0,0)$ Å and $\\mathbf{r}_2 = \\left(\\frac{a}{2},\\frac{a}{2},\\frac{a}{2}\\right)$ Å.\n- Case $2$ (ideal FCC): $N=4$ atoms at positions $\\mathbf{r}_1 = (0,0,0)$ Å, $\\mathbf{r}_2 = \\left(0,\\frac{a}{2},\\frac{a}{2}\\right)$ Å, $\\mathbf{r}_3 = \\left(\\frac{a}{2},0,\\frac{a}{2}\\right)$ Å, and $\\mathbf{r}_4 = \\left(\\frac{a}{2},\\frac{a}{2},0\\right)$ Å.\n- Case $3$ (disordered): $N=64$ atoms with positions sampled independently and uniformly from $[0,a)$ along each Cartesian axis using a pseudorandom number generator with fixed seed $s=42$; that is, $\\mathbf{r}_j$ has components $x_j, y_j, z_j \\sim \\mathrm{Uniform}(0,a)$ for $j=1,\\dots,64$ with the stated seed.\n- Case $4$ (noisy BCC): Start with Case $1$ BCC positions and add independent Gaussian displacements of zero mean and standard deviation $\\sigma = 0.1$ Å to each Cartesian component of each atom, i.e., $\\mathbf{r}_j \\leftarrow \\mathbf{r}_j + \\boldsymbol{\\eta}_j$, where $\\boldsymbol{\\eta}_j$ has components sampled from $\\mathcal{N}(0,\\sigma^2)$, using seed $s=7$.\n\nFinal output format:\nYour program should produce a single line of output containing the classification results for Cases $1$ to $4$ as a comma-separated list of integers enclosed in square brackets (e.g., \"[1,2,0,1]\"). No additional text should be printed. The integers must be in the order of the cases listed above.",
            "solution": "The problem requires the classification of atomic configurations into disordered, body-centered cubic (BCC), or face-centered cubic (FCC) phases. This is achieved by computing the static structure factor, $S(\\mathbf{k})$, and analyzing its peak pattern in reciprocal space. The procedure is based on fundamental principles of crystallography and statistical mechanics.\n\nThe first step is to compute the static structure factor, $S(\\mathbf{k})$, for a given set of $N$ atomic positions $\\{\\mathbf{r}_j\\}$. The structure factor is defined via the Fourier transform of the microscopic number density, $\\rho(\\mathbf{r}) = \\sum_{j=1}^{N} \\delta(\\mathbf{r} - \\mathbf{r}_j)$. The Fourier component $\\rho_{\\mathbf{k}}$ at a given wavevector $\\mathbf{k}$ is:\n$$\n\\rho_{\\mathbf{k}} = \\int \\rho(\\mathbf{r}) e^{i \\mathbf{k} \\cdot \\mathbf{r}} \\, d^3\\mathbf{r} = \\sum_{j=1}^{N} e^{i \\mathbf{k} \\cdot \\mathbf{r}_j}\n$$\nThe static structure factor $S(\\mathbf{k})$ is then defined as the normalized squared magnitude of this complex quantity:\n$$\nS(\\mathbf{k}) = \\frac{1}{N} \\left| \\rho_{\\mathbf{k}} \\right|^2 = \\frac{1}{N} \\left| \\sum_{j=1}^{N} e^{i \\mathbf{k} \\cdot \\mathbf{r}_j} \\right|^2\n$$\nFor a system within a cubic periodic cell of side length $a$, the wavevectors $\\mathbf{k}$ that are compatible with the periodic boundary conditions form a discrete grid in reciprocal space:\n$$\n\\mathbf{k} = \\frac{2\\pi}{a} (m_x, m_y, m_z)\n$$\nwhere $m_x$, $m_y$, and $m_z$ are integers. The problem specifies that we compute $S(\\mathbf{k})$ on a grid where each integer component $m_\\alpha$ ranges from $-M$ to $M$, with $M=3$. The central point $\\mathbf{k}=\\mathbf{0}$, corresponding to $(m_x, m_y, m_z)=(0,0,0)$, is excluded as $S(\\mathbf{0})=N$, which carries no structural information.\n\nThe second step is the classification algorithm, which analyzes the computed $S(\\mathbf{k})$ values.\nFirst, a distinction between a disordered (liquid-like) and an ordered (crystalline) state is made. In a perfect crystal, constructive interference leads to sharp Bragg peaks where $S(\\mathbf{k})$ is proportional to $N$. In a liquid, $S(\\mathbf{k})$ is typically of order $1$ for $\\mathbf{k} \\neq \\mathbf{0}$. We quantify this by computing the ratio $R = \\frac{\\max_{\\mathbf{k}} S(\\mathbf{k})}{N}$. If $R$ is below a threshold $\\tau=0.6$, the system is classified as disordered (class $0$).\n\nIf $R \\ge 0.6$, the configuration is considered crystalline, and we proceed to distinguish between BCC and FCC structures. This is done by examining the locations of the most intense peaks in reciprocal space. These locations are governed by selection rules determined by the crystal's basis.\n- For a BCC lattice, constructive interference (non-zero peaks) occurs for Miller indices $(h,k,l)$—which correspond to our integer triplets $(m_x,m_y,m_z)$—when their sum $h+k+l$ is an even number. The set of lowest-magnitude reciprocal lattice vectors corresponds to the $\\{110\\}$ family, where $h^2+k^2+l^2=2$.\n- For an FCC lattice, constructive interference occurs when the indices $(h,k,l)$ are all even or all odd (unmixed parity). The set of lowest-magnitude non-zero reciprocal lattice vectors is the $\\{111\\}$ family, where $h^2+k^2+l^2=3$.\n\nThe algorithm identifies the set of \"top peaks\" where $S(\\mathbf{k})$ is at least $99\\%$ of the global maximum value. Using this set of peaks, it checks two conditions:\n1.  The minimum non-zero value of $m_x^2+m_y^2+m_z^2$ among the top peaks.\n2.  The adherence of the top peaks' indices to the BCC and FCC parity selection rules.\n\nThe final classification follows a hierarchical logic:\n- A configuration is classified as BCC (class $1$) if the minimum squared-magnitude of its top peak indices is $2$ and the majority of its top peaks satisfy the BCC parity rule ($m_x+m_y+m_z$ is even).\n- It is classified as FCC (class $2$) if the minimum squared-magnitude is $3$ and the majority of its top peaks satisfy the FCC parity rule (all indices even or all odd).\n- If neither of these primary conditions is met, a fallback mechanism is used: the structure is assigned to the class (BCC or FCC) whose parity rule is satisfied by a larger number of top peaks.\n- If the counts for both parity rules are equal, the structure cannot be unambiguously classified and is assigned the disordered class $0$.\n\nThis procedure is applied to four test cases: an ideal BCC basis, an ideal FCC basis, a randomly generated disordered configuration, and a BCC basis with added thermal-like noise. The implementation involves generating the wavevector grid, computing $S(\\mathbf{k})$ for each case, and applying the classification logic to determine the final phase assignment.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Computes the static structure factor for several atomic configurations\n    and classifies them as disordered, BCC, or FCC.\n    \"\"\"\n\n    # Define common parameters from the problem statement.\n    a = 3.0  # Angstrom\n    M = 3\n    tau = 0.6\n    \n    # --- Test Case Definitions ---\n    test_cases = []\n\n    # Case 1: Ideal BCC\n    pos1 = np.array([[0.0, 0.0, 0.0], [a / 2, a / 2, a / 2]])\n    test_cases.append(pos1)\n\n    # Case 2: Ideal FCC\n    pos2 = np.array([[0.0, 0.0, 0.0], [0.0, a / 2, a / 2], [a / 2, 0.0, a / 2], [a / 2, a / 2, 0.0]])\n    test_cases.append(pos2)\n\n    # Case 3: Disordered\n    rng_disordered = np.random.default_rng(seed=42)\n    pos3 = rng_disordered.uniform(0, a, size=(64, 3))\n    test_cases.append(pos3)\n\n    # Case 4: Noisy BCC\n    rng_noisy = np.random.default_rng(seed=7)\n    pos4_ideal = np.array([[0.0, 0.0, 0.0], [a / 2, a / 2, a / 2]])\n    noise = rng_noisy.normal(loc=0.0, scale=0.1, size=pos4_ideal.shape)\n    pos4 = pos4_ideal + noise\n    test_cases.append(pos4)\n\n    results = []\n    for positions in test_cases:\n        classification = process_case(positions, a, M, tau)\n        results.append(classification)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef process_case(positions: np.ndarray, a: float, M: int, tau: float) -> int:\n    \"\"\"\n    Performs classification for a single set of atomic positions.\n    \"\"\"\n    N = positions.shape[0]\n    if N == 0:\n        return 0\n\n    # 1. Generate k-vector grid and compute S(k)\n    m_range = range(-M, M + 1)\n    m_triplets = [m for m in itertools.product(m_range, repeat=3) if m != (0, 0, 0)]\n    m_array = np.array(m_triplets, dtype=np.float64)\n    \n    k_vectors = (2 * np.pi / a) * m_array\n    \n    # Vectorized computation of S(k)\n    k_dot_r = k_vectors @ positions.T\n    rho_k = np.sum(np.exp(1j * k_dot_r), axis=1)\n    s_k_values = (1.0 / N) * np.abs(rho_k)**2\n\n    # 2. Compute R = max(S(k)) / N and check for disordered state\n    s_max = np.max(s_k_values)\n    R = s_max / N\n    if R  tau:\n        return 0  # Classify as disordered\n        \n    # 3. Crystalline analysis\n    # Identify top peaks\n    top_peaks_indices = np.where(s_k_values >= 0.99 * s_max)[0]\n    top_m_triplets = [m_triplets[i] for i in top_peaks_indices]\n\n    if not top_m_triplets:\n        return 0 # Should not happen, but as a safeguard\n\n    # Compute minimal non-zero h^2+k^2+l^2 for top peaks\n    hkl_sq_sums = [h**2 + k**2 + l**2 for h, k, l in top_m_triplets]\n    min_h2k2l2 = min(hkl_sq_sums)\n\n    # Count peaks satisfying BCC and FCC parity rules\n    bcc_count = 0\n    fcc_count = 0\n    for h, k, l in top_m_triplets:\n        # BCC rule: h+k+l is even\n        if (h + k + l) % 2 == 0:\n            bcc_count += 1\n        # FCC rule: h, k, l all even or all odd\n        if (h % 2 == k % 2) and (k % 2 == l % 2):\n            fcc_count += 1\n            \n    # 4. Apply hierarchical classification logic\n    num_top_peaks = len(top_m_triplets)\n    \n    # Primary classification rules\n    if min_h2k2l2 == 2 and bcc_count > num_top_peaks / 2.0:\n        return 1\n    if min_h2k2l2 == 3 and fcc_count > num_top_peaks / 2.0:\n        return 2\n\n    # Fallback rules based on larger count\n    if bcc_count > fcc_count:\n        return 1\n    if fcc_count > bcc_count:\n        return 2\n        \n    # Final tie-breaker: if counts are equal, classify as disordered\n    return 0\n\nsolve()\n```"
        },
        {
            "introduction": "While the structure factor excels at identifying bulk crystalline order, many transitions involve the formation of local motifs within a disordered environment, which requires real-space methods like bond-orientational order parameters. The Steinhardt parameter $q_6$ is an essential tool for this, but its calculation is sensitive to the definition of a particle's local neighborhood. This exercise challenges you to think critically about these practical subtleties, exploring how the choice of neighbor cutoff can introduce biases and affect the discrimination between liquid-like and crystal-like environments.",
            "id": "3430898",
            "problem": "Consider a supercooled monoatomic liquid simulated by Molecular Dynamics (MD) at number density $\\rho$ and temperature $T$, near the onset of crystallization into close-packed structures. Let the local bond-orientational order parameter of order $l=6$ be defined in the standard Steinhardt form. For each particle $i$, define the neighbor set $\\mathcal{N}_i(r_c)$ as all particles $j$ with center-to-center distance $r_{ij} \\le r_c$, where $r_c$ is a chosen distance cutoff. Define\n$$\nq_{6m}(i; r_c) \\equiv \\frac{1}{N_b(i; r_c)} \\sum_{j \\in \\mathcal{N}_i(r_c)} Y_{6m}(\\hat{\\mathbf{r}}_{ij}) \\quad \\text{for } m=-6,\\ldots,6,\n$$\nwith $N_b(i; r_c) \\equiv |\\mathcal{N}_i(r_c)|$ and $\\hat{\\mathbf{r}}_{ij}$ the unit vector from particle $i$ to $j$. The local magnitude is\n$$\nq_6(i; r_c) \\equiv \\left[\\frac{4\\pi}{2\\cdot 6 + 1} \\sum_{m=-6}^{6} \\left| q_{6m}(i; r_c) \\right|^2 \\right]^{1/2}.\n$$\nLet the radial distribution function (RDF) $g(r)$ of the liquid have a pronounced first peak and a first minimum at $r=r_{\\min}$ that separates the first coordination shell from the second shell. You measure $q_6(i; r_c)$ for a range of cutoffs $r_c$ that vary from just below $r_{\\min}$ (excluding most second-shell neighbors) to values that include a substantial fraction of second-shell neighbors. You also consider an alternative neighbor definition using the $k$ nearest neighbors for a fixed $k$.\n\nWhich of the following statements about how varying $r_c$ affects $q_6$, and about biases introduced by including second-shell neighbors, are correct in a supercooled liquid near crystallization? Select all that apply.\n\nA. Increasing $r_c$ so that second-shell neighbors are included will systematically increase the mean $q_6$ of both liquid-like and crystalline-like particles because the larger neighbor count reduces statistical variance and therefore biases the magnitude upward.\n\nB. Including second-shell neighbors generally introduces a downward bias in $q_6(i; r_c)$ for both crystalline-like and liquid-like environments when the standard normalization by $N_b(i; r_c)$ is used, because additional bonds with weaker angular correlation dilute the orientational signal by averaging over less-correlated directions.\n\nC. With a fixed distance cutoff $r_c$, spatial variations of local number density induce a finite-neighbor bias: particles with smaller $N_b(i; r_c)$ (e.g., at interfaces or in slightly lower-density regions) have larger finite-$N_b$ noise in $q_6$, leading to an elevated noise floor in the mean and variance compared to particles with larger $N_b(i; r_c)$.\n\nD. The global order parameter $Q_6$ obtained by averaging $q_{6m}(i; r_c)$ over all particles before computing the norm is invariant with respect to the choice of $r_c$ in the thermodynamic limit, provided $g(r)$ decays on the scale of the first few shells.\n\nE. In a perfect face-centered cubic (FCC) or hexagonal close-packed (HCP) crystal, $q_6(i; r_c)$ computed with the first shell (approximately $12$ neighbors) is unchanged by adding the second shell, because spherical harmonics contributions from different shells are orthogonal and thus do not affect the norm.\n\nF. Using a Voronoi tessellation to define neighbors instead of a fixed $r_c$ makes $q_6$ less sensitive to thermal expansion and local density fluctuations, thereby providing more consistent discrimination between liquid-like and crystalline-like environments near the transition.",
            "solution": "This problem tests the understanding of practical nuances in calculating Steinhardt bond-orientational order parameters, particularly the choice of the neighbor definition. An accurate analysis of the options requires considering both the mathematical definition of $q_6$ and the physical nature of atomic correlations in condensed phases.\n\n**Analysis of Correct Statements:**\n\n*   **B is correct.** The value of $q_6(i)$ is an average of the bond orientations in the neighborhood of particle $i$. The first coordination shell contains neighbors with the strongest geometric correlation. The second shell neighbors are, by definition, further away and their angular positions are less correlated with the local symmetry of the first shell. When a larger cutoff $r_c$ is used to include these second-shell neighbors, their less-correlated bond vectors are added to the average. This acts to \"dilute\" the strong orientational signal from the first shell, generally causing the magnitude of $q_6(i)$ to decrease. This effect applies to both crystalline-like environments (where a strong signal is diluted) and liquid-like environments (where a weak signal is diluted further).\n\n*   **C is correct.** For a completely random arrangement of $N_b$ neighbors, the value of $q_l$ does not average to zero but has a finite-size noise floor that scales approximately as $1/\\sqrt{N_b}$. When using a fixed distance cutoff $r_c$, local density fluctuations cause the number of neighbors, $N_b(i; r_c)$, to vary. Particles in lower-density regions will have fewer neighbors. Consequently, the noise contribution to their calculated $q_6$ value will be larger. This artificially elevates the $q_6$ \"noise floor\" for genuinely disordered particles in these regions, making it harder to distinguish them from particles with weak but real structural order.\n\n*   **F is correct.** Using a fixed distance cutoff $r_c$ makes the neighbor definition sensitive to the system's average density. If the system undergoes thermal expansion, for example, all interparticle distances increase, and a previously optimal $r_c$ may no longer correctly capture the first neighbor shell. Voronoi tessellation defines neighbors based on shared polyhedral faces, which is a topological criterion. This definition is adaptive to local density fluctuations and is invariant to uniform scaling of the system (e.g., thermal expansion). This robustness makes the resulting $q_6$ values more consistent and reliable for discriminating between local structures across different temperatures, pressures, or in heterogeneous systems near a phase transition.\n\n**Analysis of Incorrect Statements:**\n\n*   **A is incorrect.** This statement claims the opposite of statement B. The primary physical effect of including more distant, less-correlated neighbors is signal dilution, which *decreases* the value of $q_6$, not increases it. The reasoning about statistical variance is misapplied; while variance is reduced, the bias introduced is downward, not upward.\n\n*   **D is incorrect.** The parameter $q_6(i; r_c)$ is fundamentally a measure of local order on the length scale defined by $r_c$. Changing $r_c$ changes the physical quantity being measured. The global average of this local, length-scale-dependent quantity will itself depend on that length scale. There is no physical principle that would make the global average invariant to the choice of $r_c$.\n\n*   **E is incorrect.** The orthogonality of spherical harmonics applies to integrals over a continuous sphere, not to discrete sums over a finite number of bond vectors. Direct calculations for perfect FCC or HCP crystals show that the value of $q_6$ computed using only the first shell (12 neighbors) is different from the value computed using both the first and second shells. For example, in a perfect FCC crystal, $q_6$ is approximately 0.575 for the first shell and 0.485 for the first two shells combined, demonstrating the dilution effect.\n\nTherefore, the correct statements are B, C, and F.",
            "answer": "$$\n\\boxed{BCF}\n$$"
        },
        {
            "introduction": "Structural transitions are not always about the emergence of crystallographic symmetry; they can also involve changes in large-scale connectivity, such as in percolation or gelation. This practice introduces a powerful approach from spectral graph theory to quantify the network structure of particle systems. By constructing the graph Laplacian and calculating its smallest non-zero eigenvalue, the algebraic connectivity $\\lambda_2$, you will develop an order parameter that is highly sensitive to the formation and merging of particle clusters.",
            "id": "3430923",
            "problem": "You are given a sequence of particle configurations in a cubic box with Periodic Boundary Conditions (PBC) defined using the Minimum Image Convention and a fixed neighbor cutoff distance. The goal is to define a spectral order parameter from the graph Laplacian of the neighbor graph and to track the smallest nonzero eigenvalue across configurations to relate to changes in connectivity during a structural transition. Implement a program that, for each configuration, constructs the neighbor graph, forms its combinatorial Laplacian, and returns the smallest strictly positive eigenvalue as the spectral order parameter. Use the following fundamental base to derive your implementation: Euclidean distance under PBC via the Minimum Image Convention, graph adjacency derived from a cutoff criterion, the degree matrix and the combinatorial Laplacian, and the spectrum of a real symmetric matrix.\n\nDefinitions to use:\n- Let there be $N$ particles with positions $\\{\\mathbf{r}_i\\}_{i=1}^{N}$ in a cubic box of edge length $L$ (in nanometers). PBC applies in each coordinate direction. For any pair $(i,j)$, the PBC-adjusted displacement $\\Delta \\mathbf{r}_{ij}$ is obtained from the component-wise operation $\\Delta r_{ij,\\alpha} \\leftarrow \\Delta r_{ij,\\alpha} - L \\cdot \\mathrm{round}\\!\\left(\\frac{\\Delta r_{ij,\\alpha}}{L}\\right)$ for each Cartesian component index $\\alpha$, where $\\Delta \\mathbf{r}_{ij} = \\mathbf{r}_j - \\mathbf{r}_i$. The PBC Euclidean distance is $d_{ij} = \\|\\Delta \\mathbf{r}_{ij}\\|_2$.\n- Given a cutoff $r_c$ (in nanometers), define the unweighted adjacency matrix $A$ by $A_{ij} = 1$ if $i \\neq j$ and $d_{ij} \\le r_c$, and $A_{ij} = 0$ otherwise.\n- Define the degree matrix $D$ by $D_{ii} = \\sum_{j=1}^{N} A_{ij}$ and $D_{ij} = 0$ for $i \\neq j$.\n- Define the combinatorial Laplacian $L_{\\mathrm{graph}} = D - A$.\n- Let the real symmetric matrix $L_{\\mathrm{graph}}$ have eigenvalues $\\{\\lambda_k\\}_{k=1}^{N}$ sorted in nondecreasing order. Define the spectral order parameter as the smallest strictly positive eigenvalue, denoted $\\lambda_2$, where all eigenvalues $\\lambda_k$ satisfying $\\lambda_k \\le \\varepsilon$ for a small numerical tolerance $\\varepsilon$ are considered zero. If no strictly positive eigenvalue exists (for example, a completely disconnected graph), define $\\lambda_2 = 0.0$. Report $\\lambda_2$ as a dimensionless real number.\n\nImplement the above in a single program that processes the following test suite of configurations. Distances and box lengths are in nanometers; return $\\lambda_2$ in dimensionless units. Use the same numerical tolerance $\\varepsilon = 10^{-10}$ for all eigenvalue positivity checks.\n\nTest suite:\n- Test case $1$ (disconnected configuration, sanity check):\n  - Box length $L = 10.0$.\n  - Cutoff $r_c = 0.5$.\n  - Positions for $N=6$: $(0.0,0.0)$, $(5.0,5.0)$, $(9.0,1.0)$, $(1.0,9.0)$, $(2.0,7.0)$, $(7.0,2.0)$.\n  - Expected qualitative behavior: no edges; $\\lambda_2$ should be $0.0$.\n- Test case $2$ (path graph configuration, typical connected but sparse):\n  - Box length $L = 3.0$.\n  - Cutoff $r_c = 0.5$.\n  - Positions for $N=6$: $(0.1,1.5)$, $(0.5,1.5)$, $(0.9,1.5)$, $(1.3,1.5)$, $(1.7,1.5)$, $(2.1,1.5)$.\n  - Expected qualitative behavior: a linear chain with edges only between nearest neighbors; $\\lambda_2$ should be small but strictly positive.\n- Test case $3$ (two-particle PBC proximity, PBC edge case):\n  - Box length $L = 1.0$.\n  - Cutoff $r_c = 0.2$.\n  - Positions for $N=2$: $(0.95,0.50)$, $(0.05,0.50)$.\n  - Expected qualitative behavior: connected via PBC across the boundary; $\\lambda_2$ should be strictly positive.\n- Test case $4$ (trajectory across a connectivity transition, sequence of frames):\n  - Box length $L = 2.0$.\n  - Cutoff $r_c = 0.35$.\n  - Each frame has $N=8$ particles arranged as two clusters of $4$ points each. The cluster pattern is the set of offsets $\\{(0.00,0.00),(0.10,0.00),(0.00,0.10),(0.10,0.10)\\}$ added to the cluster center. The first cluster center is fixed at $(0.20,0.20)$. The second cluster center moves across frames:\n    - Frame $1$: $(1.60,1.60)$,\n    - Frame $2$: $(1.70,1.70)$,\n    - Frame $3$: $(1.80,1.80)$,\n    - Frame $4$: $(1.90,1.90)$,\n    - Frame $5$: $(1.95,1.95)$.\n  - Expected qualitative behavior: initially disconnected clusters, then emergence of cross-cluster edges as the moving cluster approaches under PBC; report a list of $\\lambda_2$ values, one per frame, in order.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must have four elements corresponding to the four test cases, where the first three elements are real numbers and the fourth element is a list of real numbers corresponding to the frames in test case $4$. For instance, the output should have the structural form $[\\lambda_2^{(1)},\\lambda_2^{(2)},\\lambda_2^{(3)},[\\lambda_2^{(4,1)},\\lambda_2^{(4,2)},\\lambda_2^{(4,3)},\\lambda_2^{(4,4)},\\lambda_2^{(4,5)}]]$ with numeric values computed by your program.",
            "solution": "The problem is valid as it is scientifically sound, well-posed, and objective. It presents a clear, algorithmically-defined task based on established principles in computational physics and spectral graph theory.\n\nThe goal is to compute a spectral order parameter, denoted $\\lambda_2$, for several particle configurations. This parameter is defined as the smallest strictly positive eigenvalue of the combinatorial graph Laplacian of the system. The value of $\\lambda_2$ quantifies the connectivity of the particle system, providing insight into structural transitions. The methodology involves constructing a graph representation of the particle system, calculating its Laplacian matrix, and then finding the required eigenvalue from its spectrum.\n\n**1. System Representation and Periodic Boundary Conditions (PBC)**\n\nA system of $N$ particles is described by their positions $\\{\\mathbf{r}_i\\}_{i=1}^{N}$ within a cubic box of side length $L$. Although the problem refers to a cubic box, the provided test cases use 2D coordinates. This is handled by performing all calculations in 2D, effectively modeling a 2D system within a square periodic box.\n\nThe distance between any two particles $i$ and $j$ must account for the periodic nature of the simulation box. This is done using the Minimum Image Convention. The displacement vector $\\Delta \\mathbf{r}_{ij} = \\mathbf{r}_j - \\mathbf{r}_i$ is first calculated. Then, each Cartesian component $\\Delta r_{ij, \\alpha}$ of this vector is adjusted according to the rule:\n$$\n\\Delta r'_{ij, \\alpha} = \\Delta r_{ij, \\alpha} - L \\cdot \\mathrm{round}\\!\\left(\\frac{\\Delta r_{ij, \\alpha}}{L}\\right)\n$$\nThis operation maps each displacement component into the interval $[-L/2, L/2]$, ensuring that we find the shortest vector connecting the two particles (or their periodic images). The PBC-adjusted distance $d_{ij}$ is then the Euclidean norm of the adjusted displacement vector, $d_{ij} = \\|\\Delta \\mathbf{r}'_{ij}\\|_2$.\n\n**2. Neighbor Graph and Adjacency Matrix**\n\nBased on the calculated distances, a neighbor graph is constructed. The particles serve as vertices, and an edge exists between two particles if they are closer than a specified cutoff distance $r_c$. This relationship is captured by the unweighted adjacency matrix $A$, an $N \\times N$ matrix defined as:\n$$\nA_{ij} = \n\\begin{cases} \n1  \\text{if } i \\neq j \\text{ and } d_{ij} \\le r_c \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nBy construction, $A$ is a symmetric matrix with a zero diagonal.\n\n**3. Combinatorial Graph Laplacian**\n\nThe combinatorial Laplacian, $L_{\\mathrm{graph}}$, is a matrix representation of the graph that is central to spectral graph theory. It is derived from the adjacency matrix $A$ and the degree matrix $D$.\n\nThe degree matrix $D$ is a diagonal matrix where each diagonal element $D_{ii}$ represents the degree of vertex $i$—that is, the number of neighbors it has. It is calculated as:\n$$\nD_{ii} = \\sum_{j=1}^{N} A_{ij}\n$$\nThe combinatorial Laplacian is then defined as the difference between the degree and adjacency matrices:\n$$\nL_{\\mathrm{graph}} = D - A\n$$\n$L_{\\mathrm{graph}}$ is a real, symmetric, and positive semi-definite matrix.\n\n**4. Spectral Analysis and the Order Parameter $\\boldsymbol{\\lambda_2}$**\n\nThe eigenvalues of $L_{\\mathrm{graph}}$, denoted $\\{\\lambda_k\\}_{k=1}^{N}$ and sorted in non-decreasing order ($0 = \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_N$), form the spectrum of the graph. The number of zero eigenvalues is equal to the number of connected components in the graph. For any connected graph, $\\lambda_1 = 0$ and all other eigenvalues are strictly positive. The second-smallest eigenvalue, $\\lambda_2$, is known as the algebraic connectivity and measures how well-connected the graph is.\n\nThe problem re-defines the symbol $\\lambda_2$ to mean the **smallest strictly positive eigenvalue**. To find this value, we compute the full spectrum of $L_{\\mathrm{graph}}$. Since $L_{\\mathrm{graph}}$ is real and symmetric, `numpy.linalg.eigh` is an appropriate and efficient numerical tool. It returns the eigenvalues sorted in non-decreasing order. We iterate through this sorted list and identify the first eigenvalue $\\lambda_k$ that is greater than a given numerical tolerance, $\\varepsilon = 10^{-10}$. This value is our order parameter. If all eigenvalues are less than or equal to $\\varepsilon$ (e.g., for a graph with no edges), the order parameter is defined as $0.0$.\n\nThis procedure is encapsulated in a Python function. The function is then applied to each test case, including the multi-frame trajectory of test case 4, to generate the required results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating the spectral order parameter for each test case.\n    The order parameter is the smallest strictly positive eigenvalue of the graph Laplacian.\n    \"\"\"\n\n    # Numerical tolerance for checking if an eigenvalue is positive.\n    EPSILON = 1e-10\n\n    def calculate_lambda2(positions: np.ndarray, L: float, r_c: float) -> float:\n        \"\"\"\n        Calculates the smallest strictly positive eigenvalue of the graph Laplacian.\n\n        Args:\n            positions (np.ndarray): An (N, D) array of particle positions.\n            L (float): The length of the cubic/square box.\n            r_c (float): The neighbor cutoff distance.\n        \n        Returns:\n            float: The smallest strictly positive eigenvalue, or 0.0 if none exists.\n        \"\"\"\n        N = positions.shape[0]\n        \n        if N == 0:\n            return 0.0\n\n        # Construct the adjacency matrix A\n        A = np.zeros((N, N), dtype=int)\n        for i in range(N):\n            for j in range(i + 1, N):\n                # Calculate displacement vector\n                delta_r = positions[j] - positions[i]\n                \n                # Apply Minimum Image Convention for Periodic Boundary Conditions\n                delta_r = delta_r - L * np.round(delta_r / L)\n                \n                # Calculate Euclidean distance\n                distance = np.linalg.norm(delta_r)\n                \n                if distance = r_c:\n                    A[i, j] = 1\n                    A[j, i] = 1\n\n        # Construct the degree matrix D\n        degrees = np.sum(A, axis=1)\n        D = np.diag(degrees)\n\n        # Construct the combinatorial Laplacian L = D - A\n        L_graph = D - A\n\n        # Calculate eigenvalues of the real symmetric Laplacian matrix\n        # numpy.linalg.eigh returns eigenvalues sorted in non-decreasing order\n        eigenvalues = np.linalg.eigh(L_graph)[0]\n\n        # Find the smallest strictly positive eigenvalue\n        smallest_positive_eigenvalue = 0.0\n        for val in eigenvalues:\n            if val > EPSILON:\n                smallest_positive_eigenvalue = float(val)\n                break\n        \n        return smallest_positive_eigenvalue\n\n    # Process test cases\n    results = []\n\n    # Test case 1\n    L1 = 10.0\n    rc1 = 0.5\n    pos1 = np.array([(0.0, 0.0), (5.0, 5.0), (9.0, 1.0), (1.0, 9.0), (2.0, 7.0), (7.0, 2.0)])\n    results.append(calculate_lambda2(pos1, L1, rc1))\n\n    # Test case 2\n    L2 = 3.0\n    rc2 = 0.5\n    pos2 = np.array([(0.1, 1.5), (0.5, 1.5), (0.9, 1.5), (1.3, 1.5), (1.7, 1.5), (2.1, 1.5)])\n    results.append(calculate_lambda2(pos2, L2, rc2))\n\n    # Test case 3\n    L3 = 1.0\n    rc3 = 0.2\n    pos3 = np.array([(0.95, 0.50), (0.05, 0.50)])\n    results.append(calculate_lambda2(pos3, L3, rc3))\n\n    # Test case 4\n    L4 = 2.0\n    rc4 = 0.35\n    cluster_offsets = np.array([(0.00, 0.00), (0.10, 0.00), (0.00, 0.10), (0.10, 0.10)])\n    cluster1_center = np.array([0.20, 0.20])\n    cluster2_centers = [\n        np.array([1.60, 1.60]),\n        np.array([1.70, 1.70]),\n        np.array([1.80, 1.80]),\n        np.array([1.90, 1.90]),\n        np.array([1.95, 1.95])\n    ]\n    \n    pos_cluster1 = cluster1_center + cluster_offsets\n    \n    lambda2_4_list = []\n    for c2_center in cluster2_centers:\n        pos_cluster2 = c2_center + cluster_offsets\n        pos4 = np.concatenate((pos_cluster1, pos_cluster2))\n        lambda2_frame = calculate_lambda2(pos4, L4, rc4)\n        lambda2_4_list.append(lambda2_frame)\n    results.append(lambda2_4_list)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}