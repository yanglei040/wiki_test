{
    "hands_on_practices": [
        {
            "introduction": "The first and most critical decision in building a Markov State Model is choosing the lag time, $\\tau$. This choice embodies a fundamental trade-off in statistical modeling: a lag time that is too short may lead to a model that is not truly Markovian, introducing systematic bias, while a lag time that is too long reduces the amount of statistical data, increasing variance. This exercise provides a hands-on implementation of a principled approach to navigate this dilemma by evaluating a risk score that combines metrics of model bias and variance, such as the convergence of implied timescales.",
            "id": "3423391",
            "problem": "You are asked to implement a complete, runnable program that selects a Markov State Model (MSM) lag time by a statistically principled approach grounded in core definitions of discrete-time Markov processes and the Chapman-Kolmogorov equation (CK). The goal is to choose a lag time $\\tau$ that balances estimation variance and Markovian bias by analyzing convergence of implied timescales and the stability of $T(\\tau)^n$ compared to $T(n\\tau)$, where $T(\\tau)$ is a transition probability matrix at lag $\\tau$, and $n$ is a positive integer.\n\nBegin from the following fundamental base:\n- The Markov property for a discrete-time Markov chain: for states $X_t$ in a finite state space, $\\mathbb{P}(X_{t+\\tau}=j\\mid X_t=i, \\text{history}) = \\mathbb{P}(X_{t+\\tau}=j\\mid X_t=i) = T_{ij}(\\tau)$ for all states $i,j$.\n- The Chapman-Kolmogorov equation (CK): for a Markov chain, $T(\\tau)^n = T(n\\tau)$ for any positive integer $n$, when $T$ denotes the exact propagator at lag $\\tau$.\n- Spectral characterization of relaxation: for a diagonalizable stochastic matrix $T(\\tau)$ with eigenvalues $\\lambda_1(\\tau)=1, \\lambda_2(\\tau),\\dots,$ the implied timescales $t_i(\\tau)$ for $i\\ge 2$ are defined by $t_i(\\tau) = -\\tau / \\ln|\\lambda_i(\\tau)|$ and reflect relaxation times associated with the slow modes of the dynamics.\n\nYour program must implement the following algorithmic tasks for each provided test case:\n1. Construct a continuous-time Markov chain (CTMC) generator $Q$ on a finite state space with off-diagonal elements $Q_{ij}\\ge 0$ and diagonals $Q_{ii}=-\\sum_{j\\neq i}Q_{ij}$. Compute the stationary distribution $\\pi$ satisfying $Q^\\top \\pi = 0$ and $\\sum_i \\pi_i = 1$.\n2. For each candidate lag time $\\tau$ in a provided list, compute the exact transition matrix $T_{\\text{true}}(\\tau) = \\exp(Q\\,\\tau)$. Construct integer transition counts $C(\\tau)$ by allocating a row-wise total exposure $N_i(\\tau) = \\left\\lfloor \\frac{L}{\\tau}\\,\\pi_i \\right\\rfloor$ for a provided trajectory duration $L$ (in seconds), and set $C_{ij}(\\tau)\\approx N_i(\\tau)\\,T_{\\text{true},ij}(\\tau)$ with rounding so that $\\sum_j C_{ij}(\\tau)=N_i(\\tau)$. Estimate a row-stochastic transition matrix $\\widehat{T}(\\tau)$ by adding a small Dirichlet-type pseudocount $\\alpha > 0$ uniformly to $C(\\tau)$ and normalizing rows, to mitigate zero-count and numerical issues.\n3. For each $\\tau$, compute the nontrivial implied timescales $t_i(\\tau) = -\\tau / \\ln|\\lambda_i(\\tau)|$ for the leading $m$ slow modes, where $\\lambda_i(\\tau)$ are eigenvalues of $\\widehat{T}(\\tau)$ ordered by decreasing $|\\lambda_i|$ and $m$ is chosen up to $S-1$, with $S$ the number of states.\n4. Quantify implied timescale convergence by a drift score at $\\tau_k$ relative to the previous candidate $\\tau_{k-1}$:\n   $$d(\\tau_k) = \\frac{1}{m}\\sum_{i=2}^{m+1} \\frac{\\left|t_i(\\tau_k)-t_i(\\tau_{k-1})\\right|}{\\max\\left(t_i(\\tau_{k-1}),\\varepsilon\\right)},$$\n   with a small $\\varepsilon>0$ to avoid division by zero. For the smallest $\\tau$ in the list (no previous value), assign a conservative default drift, for example $d(\\tau_1)=1$.\n5. Quantify CK stability for each $\\tau$ when $n\\tau$ is present among candidates by computing\n   $$e_{\\text{CK}}(\\tau) = \\frac{\\| \\widehat{T}(\\tau)^n - \\widehat{T}(n\\tau) \\|_F}{\\max\\left(\\|\\widehat{T}(n\\tau)\\|_F,\\varepsilon\\right)},$$\n   with Frobenius norm $\\|\\cdot\\|_F$ and small $\\varepsilon>0$. If $n\\tau$ is not available, assign a conservative penalty, for example $e_{\\text{CK}}(\\tau)=1$.\n6. Quantify variance using the total exposure $M(\\tau) = \\sum_i N_i(\\tau)$ via the proxy\n   $$v(\\tau) = \\frac{\\sqrt{\\min_{\\tau'} M(\\tau')}}{\\sqrt{M(\\tau)}},$$\n   which rescales sampling uncertainty across candidate lag times.\n7. Form a combined risk score that balances bias and variance,\n   $$R(\\tau) = w_d\\,d(\\tau) + w_{\\text{CK}}\\,e_{\\text{CK}}(\\tau) + w_v\\,v(\\tau),$$\n   with provided nonnegative weights $w_d, w_{\\text{CK}}, w_v$. Impose decision constraints with thresholds $d(\\tau)\\le \\theta_d$ and $e_{\\text{CK}}(\\tau)\\le \\theta_{\\text{CK}}$. Select $\\tau^\\star$ as the $\\tau$ that minimizes $R(\\tau)$ among those meeting the constraints; if no candidate satisfies the constraints, choose the global minimizer of $R(\\tau)$.\n\nPhysical units: The lag time $\\tau$ and trajectory duration $L$ are in seconds. Your program must report the selected $\\tau^\\star$ for each test case in seconds. Express each final $\\tau^\\star$ as a floating-point number rounded to three decimal places.\n\nTest suite. Use the following three test cases:\n- Test case $1$ (three-state metastable chain):\n  - States $S=3$, generator $Q$ with off-diagonal elements $Q_{01}=0.02$, $Q_{10}=0.02$, $Q_{12}=0.3$, $Q_{21}=0.3$, and all other off-diagonals zero; diagonals $Q_{ii}=-\\sum_{j\\neq i}Q_{ij}$.\n  - Candidate lag times $\\tau\\in\\{0.2,\\,0.5,\\,1.0,\\,2.0\\}$ seconds.\n  - CK check power $n=2$.\n  - Trajectory duration $L=10000$ seconds.\n  - Weights $w_d=w_{\\text{CK}}=w_v=1$, thresholds $\\theta_d=0.2$, $\\theta_{\\text{CK}}=0.1$.\n- Test case $2$ (four-state fast-mixing chain):\n  - States $S=4$, generator $Q$ with off-diagonal elements $Q_{ij}=1.0$ for $i\\neq j$; diagonals $Q_{ii}=-\\sum_{j\\neq i}Q_{ij}=-3.0$.\n  - Candidate lag times $\\tau\\in\\{0.05,\\,0.1,\\,0.2,\\,0.5\\}$ seconds.\n  - CK check power $n=2$.\n  - Trajectory duration $L=200$ seconds.\n  - Weights $w_d=w_{\\text{CK}}=w_v=1$, thresholds $\\theta_d=0.2$, $\\theta_{\\text{CK}}=0.1$.\n- Test case $3$ (four-state cyclic non-reversible dynamics):\n  - States $S=4$, generator $Q$ with off-diagonals $Q_{01}=0.4$, $Q_{12}=0.4$, $Q_{23}=0.4$, $Q_{30}=0.4$, and reverse directions $Q_{10}=0.1$, $Q_{21}=0.1$, $Q_{32}=0.1$, $Q_{03}=0.1$; diagonals $Q_{ii}=-\\sum_{j\\neq i}Q_{ij}$.\n  - Candidate lag times $\\tau\\in\\{0.1,\\,0.3,\\,0.6,\\,1.2\\}$ seconds.\n  - CK check power $n=2$.\n  - Trajectory duration $L=5000$ seconds.\n  - Weights $w_d=w_{\\text{CK}}=w_v=1$, thresholds $\\theta_d=0.2$, $\\theta_{\\text{CK}}=0.1$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the selected $\\tau^\\star$ in seconds for the corresponding test case, rounded to three decimal places. For example, the output must have the exact form $[\\tau^\\star_1,\\tau^\\star_2,\\tau^\\star_3]$ with no spaces.",
            "solution": "The solution will be structured as a sequence of computations for each test case, following the algorithmic tasks outlined in the problem statement. The core of the problem is to select an optimal lag time $\\tau^\\star$ for a Markov State Model (MSM) by balancing estimation bias, which decreases with longer $\\tau$, and statistical variance, which increases with longer $\\tau$. This is achieved by evaluating a composite risk score $R(\\tau)$ for a set of candidate lag times.\n\nFirst, let us define the shared parameters for the implementation. A pseudocount $\\alpha=1$ is added to the transition counts to prevent zero-probability transitions in the estimated model, a standard practice known as Laplace or add-one smoothing. A small constant $\\varepsilon=10^{-9}$ is used to prevent division by zero in score calculations.\n\nThe overall procedure for each test case is as follows:\n\n**Step 1: System Definition**\nFor each test case, we are given the number of states $S$ and the off-diagonal elements of a continuous-time Markov chain (CTMC) generator matrix $Q$. The diagonal elements are determined by the condition that rows of $Q$ must sum to zero: $Q_{ii} = -\\sum_{j\\neq i} Q_{ij}$.\n\n**Step 2: Stationary Distribution Calculation**\nThe stationary distribution $\\pi$ is a probability vector (elements are non-negative and sum to $1$) that represents the equilibrium population of the states. It satisfies the equation $Q^\\top \\pi = 0$. This is equivalent to finding the eigenvector of $Q^\\top$ corresponding to the eigenvalue $0$. Numerically, we compute the eigenvalues and eigenvectors of $Q^\\top$. The eigenvector corresponding to the eigenvalue with the smallest magnitude is selected. Its real part is taken, and it is normalized to sum to $1$. For all three test cases provided, the matrix $Q^\\top$ has column sums equal to zero ($Q^\\top \\mathbf{1} = 0$), which implies that the stationary distribution is uniform, i.e., $\\pi_i = 1/S$ for all states $i$.\n\n**Step 3: Data Generation and Model Estimation for Each Lag Time**\nFor each candidate lag time $\\tau$ in the provided list:\n- The exact discrete-time transition probability matrix is computed via the matrix exponential: $T_{\\text{true}}(\\tau) = \\exp(Q\\tau)$. The `scipy.linalg.expm` function is used for this calculation.\n- The total number of observed transitions from each state $i$ is determined by the total simulation length $L$, the lag time $\\tau$, and the stationary probability $\\pi_i$. This exposure is given by $N_i(\\tau) = \\lfloor \\frac{L}{\\tau} \\pi_i \\rfloor$.\n- Integer transition counts $C_{ij}(\\tau)$ are generated to simulate experimental or simulation data. The expected number of transitions from state $i$ to $j$ is $N_i(\\tau) T_{\\text{true},ij}(\\tau)$. To obtain integer counts $C_{ij}(\\tau)$ that sum to $N_i(\\tau)$ for each row $i$, we use a deterministic rounding scheme: initial counts are obtained by flooring the expected values. The deficit, $N_i(\\tau) - \\sum_j \\lfloor N_i(\\tau) T_{\\text{true},ij}(\\tau) \\rfloor$, is then distributed one by one to the transitions with the largest fractional parts in their expected counts.\n- An estimated transition matrix $\\widehat{T}(\\tau)$ is constructed from the counts. First, the pseudocount $\\alpha=1$ is added to each element of the count matrix $C(\\tau)$. Then, each row is normalized to sum to $1$, yielding a row-stochastic matrix $\\widehat{T}_{ij}(\\tau) = (C_{ij}(\\tau) + \\alpha) / \\sum_k (C_{ik}(\\tau) + \\alpha)$.\n\n**Step 4: Calculation of Validation Metrics**\nAfter processing all candidate lag times, we compute the three scores for each $\\tau$.\n- **Implied Timescales and Drift Score ($d(\\tau)$):** For each estimated matrix $\\widehat{T}(\\tau)$, its eigenvalues $\\lambda_i(\\tau)$ are computed. They are sorted by decreasing magnitude, $|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_S|$. The trivial eigenvalue $\\lambda_1=1$ is discarded. The implied timescales for the $S-1$ slow processes are calculated as $t_i(\\tau) = -\\tau / \\ln|\\lambda_i(\\tau)|$ for $i=2, \\dots, S$. The drift score $d(\\tau_k)$ measures the stability of these timescales as a function of the lag time, comparing them to the previous lag time in the sorted list, $\\tau_{k-1}$: $d(\\tau_k) = \\frac{1}{S-1}\\sum_{i=2}^{S} \\frac{|t_i(\\tau_k)-t_i(\\tau_{k-1})|}{\\max(t_i(\\tau_{k-1}), \\varepsilon)}$. For the first lag time $\\tau_1$, $d(\\tau_1)$ is set to a default penalty of $1$.\n\n- **Chapman-Kolmogorov (CK) Test Score ($e_{\\text{CK}}(\\tau)$):** This score validates the Markovianity of the estimated model by checking if $\\widehat{T}(\\tau)^n \\approx \\widehat{T}(n\\tau)$, a consequence of the CK equation. For a given integer $n$, we check if the lag time $n\\tau$ exists in our candidate list. If it does, we compute the error as the Frobenius norm of the difference, normalized by the norm of the target matrix: $e_{\\text{CK}}(\\tau) = \\frac{\\| \\widehat{T}(\\tau)^n - \\widehat{T}(n\\tau) \\|_F}{\\max(\\|\\widehat{T}(n\\tau)\\|_F, \\varepsilon)}$. If $n\\tau$ is not in the list, a penalty score of $e_{\\text{CK}}(\\tau)=1$ is assigned.\n\n- **Variance Proxy ($v(\\tau)$):** The statistical uncertainty of the model increases as the amount of data decreases. Longer lag times lead to fewer total transition counts, $M(\\tau) = \\sum_i N_i(\\tau)$. The variance proxy is defined as $v(\\tau) = \\sqrt{\\min_{\\tau'} M(\\tau')} / \\sqrt{M(\\tau)}$, which is $1$ for the lag time with the most data and decreases for lag times with less data.\n\n**Step 5: Optimal Lag Time Selection**\nA combined risk score, $R(\\tau) = w_d d(\\tau) + w_{\\text{CK}} e_{\\text{CK}}(\\tau) + w_v v(\\tau)$, is computed for each $\\tau$ using the provided weights ($w_d=w_{\\text{CK}}=w_v=1$ for all cases). The selection of the optimal lag time $\\tau^\\star$ is based on this score and the provided thresholds ($\\theta_d=0.2$, $\\theta_{\\text{CK}}=0.1$). We first identify the subset of lag times that satisfy both constraints: $d(\\tau) \\le \\theta_d$ and $e_{\\text{CK}}(\\tau) \\le \\theta_{\\text{CK}}$. If this subset is not empty, $\\tau^\\star$ is chosen as the lag time that minimizes $R(\\tau)$ within this subset. If no lag time satisfies the constraints, $\\tau^\\star$ is chosen as the one that minimizes $R(\\tau)$ over all candidates.\n\nThis procedure is applied to each of the three test cases, and the selected $\\tau^\\star$ values are reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"three-state metastable\",\n            \"S\": 3,\n            \"Q\": np.array([\n                [-0.02, 0.02, 0.0],\n                [0.02, -0.32, 0.3],\n                [0.0, 0.3, -0.3]\n            ]),\n            \"taus\": [0.2, 0.5, 1.0, 2.0],\n            \"n\": 2,\n            \"L\": 10000.0,\n            \"weights\": (1.0, 1.0, 1.0),\n            \"thresholds\": (0.2, 0.1)\n        },\n        {\n            \"name\": \"four-state fast-mixing\",\n            \"S\": 4,\n            \"Q\": np.array([\n                [-3.0, 1.0, 1.0, 1.0],\n                [1.0, -3.0, 1.0, 1.0],\n                [1.0, 1.0, -3.0, 1.0],\n                [1.0, 1.0, 1.0, -3.0]\n            ]),\n            \"taus\": [0.05, 0.1, 0.2, 0.5],\n            \"n\": 2,\n            \"L\": 200.0,\n            \"weights\": (1.0, 1.0, 1.0),\n            \"thresholds\": (0.2, 0.1)\n        },\n        {\n            \"name\": \"four-state cyclic non-reversible\",\n            \"S\": 4,\n            \"Q\": np.array([\n                [-0.5, 0.4, 0.0, 0.1],\n                [0.1, -0.5, 0.4, 0.0],\n                [0.0, 0.1, -0.5, 0.4],\n                [0.4, 0.0, 0.1, -0.5]\n            ]),\n            \"taus\": [0.1, 0.3, 0.6, 1.2],\n            \"n\": 2,\n            \"L\": 5000.0,\n            \"weights\": (1.0, 1.0, 1.0),\n            \"thresholds\": (0.2, 0.1)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        tau_star = _process_case(**case)\n        results.append(f\"{tau_star:.3f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef _get_counts(expected_counts, total_count):\n    \"\"\"\n    Generates integer counts from expected fractional counts while preserving the sum.\n    \"\"\"\n    if total_count == 0:\n        return np.zeros_like(expected_counts, dtype=int)\n    \n    counts = np.floor(expected_counts).astype(int)\n    n_missing = total_count - np.sum(counts)\n    \n    if n_missing > 0:\n        residuals = expected_counts - counts\n        indices_to_add = np.argsort(residuals)[-n_missing:]\n        counts[indices_to_add] += 1\n    \n    # Final check to ensure sum is correct due to possible float precision errors\n    final_diff = total_count - np.sum(counts)\n    if final_diff != 0:\n        counts[np.argmax(expected_counts)] += final_diff\n        \n    return counts\n\ndef _process_case(name, S, Q, taus, n, L, weights, thresholds, **kwargs):\n    \"\"\"\n    Processes a single test case for lag time selection.\n    \"\"\"\n    alpha = 1.0\n    epsilon = 1e-9\n\n    # Step 1: Compute stationary distribution\n    eigvals, eigvecs = np.linalg.eig(Q.T)\n    pi = np.real(eigvecs[:, np.argmin(np.abs(eigvals))])\n    pi /= np.sum(pi)\n    \n    tau_data = {}\n    \n    # Step 2: Generate data for each lag time\n    for tau in taus:\n        T_true = expm(Q * tau)\n        \n        N_i = np.floor((L / tau) * pi).astype(int)\n        C = np.zeros((S, S), dtype=int)\n        for i in range(S):\n            C[i, :] = _get_counts(N_i[i] * T_true[i, :], N_i[i])\n            \n        C_alpha = C + alpha\n        T_hat = C_alpha / np.sum(C_alpha, axis=1, keepdims=True)\n        \n        # Compute implied timescales\n        eigvals_T_hat = np.linalg.eigvals(T_hat)\n        # Sort by magnitude, discard the trivial eigenvalue\n        sorted_indices = np.argsort(np.abs(eigvals_T_hat))[::-1]\n        slow_eigvals = eigvals_T_hat[sorted_indices[1:S]]\n        \n        # Handle cases where magnitude is close to 0 or 1\n        magnitudes = np.abs(slow_eigvals)\n        timescales = np.full(S - 1, np.inf)\n        valid_mask = (magnitudes > epsilon)  (magnitudes  1.0 - epsilon)\n        timescales[valid_mask] = -tau / np.log(magnitudes[valid_mask])\n        \n        tau_data[tau] = {\n            'T_hat': T_hat,\n            'total_counts': np.sum(N_i),\n            'timescales': timescales\n        }\n        \n    # Step 3: Calculate scores\n    min_total_counts = min(data['total_counts'] for data in tau_data.values()) if tau_data else 0\n    \n    score_results = []\n    sorted_taus = sorted(taus)\n\n    for i, tau in enumerate(sorted_taus):\n        # Drift score\n        if i == 0:\n            d = 1.0\n        else:\n            prev_tau = sorted_taus[i-1]\n            t_curr = tau_data[tau]['timescales']\n            t_prev = tau_data[prev_tau]['timescales']\n            \n            # Handle infinite timescales by setting their ratio term to 0\n            # if they are both inf, or 1 if one is inf.\n            # A more stable approach is to filter them out\n            finite_mask = np.isfinite(t_curr)  np.isfinite(t_prev)\n            if np.any(finite_mask):\n                diffs = np.abs(t_curr[finite_mask] - t_prev[finite_mask])\n                denominators = np.maximum(t_prev[finite_mask], epsilon)\n                d = np.mean(diffs / denominators)\n            else:\n                d = 0.0\n\n        # CK error\n        target_tau_val = n * tau\n        target_tau = None\n        for t in taus:\n            if np.isclose(t, target_tau_val):\n                target_tau = t\n                break\n        \n        if target_tau is not None:\n            T_hat_n = np.linalg.matrix_power(tau_data[tau]['T_hat'], n)\n            T_hat_ntau = tau_data[target_tau]['T_hat']\n            norm_diff = np.linalg.norm(T_hat_n - T_hat_ntau, 'fro')\n            norm_target = np.linalg.norm(T_hat_ntau, 'fro')\n            e_ck = norm_diff / max(norm_target, epsilon)\n        else:\n            e_ck = 1.0\n\n        # Variance proxy\n        M_tau = tau_data[tau]['total_counts']\n        v = np.sqrt(min_total_counts) / np.sqrt(M_tau) if M_tau > 0 else np.inf\n        \n        # Risk score\n        w_d, w_ck, w_v = weights\n        R = w_d * d + w_ck * e_ck + w_v * v\n        \n        score_results.append({'tau': tau, 'd': d, 'e_ck': e_ck, 'v': v, 'R': R})\n        \n    # Step 4: Select best tau\n    theta_d, theta_ck = thresholds\n    \n    constrained_results = [res for res in score_results if res['d'] = theta_d and res['e_ck'] = theta_ck]\n    \n    if constrained_results:\n        best_res = min(constrained_results, key=lambda x: x['R'])\n    else:\n        best_res = min(score_results, key=lambda x: x['R'])\n        \n    return best_res['tau']\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Once a candidate lag time is selected and an MSM is built, its core assumption—the Markov property—must be rigorously validated. The Chapman-Kolmogorov (CK) test is the cornerstone of this validation process, asserting that a true Markov model's predictions must be consistent across different timescales. In this practice, you will formalize and implement the CK test by quantitatively comparing the transition matrix propagated over multiple lag steps, $T(\\tau)^n$, with a matrix estimated directly from data at a lag of $n\\tau$.",
            "id": "3423468",
            "problem": "You are given the task of formalizing and implementing a quantitative Chapman–Kolmogorov consistency test for discrete-state, time-homogeneous Markov state models (MSMs) constructed from molecular dynamics trajectory counts. The test compares the direct estimate of the transition matrix at lag time $n\\tau$, denoted $T(n\\tau)$, with the $n$-fold matrix power of the transition matrix estimated at lag time $\\tau$, denoted $T(\\tau)^n$. Your goal is to derive the test from first principles and implement a program that computes a weighted discrepancy between $T(n\\tau)$ and $T(\\tau)^n$ for a provided test suite of count matrices.\n\nFundamental base:\n- The Markov property for a time-homogeneous Markov chain implies the Chapman–Kolmogorov identity: for any nonnegative times $s$ and $t$, the transition kernel satisfies $T(t+s) = T(t) T(s)$. In discrete time with lag $\\tau$, this implies $T(n\\tau) = T(\\tau)^n$ for any integer $n \\ge 1$.\n- Given a count matrix $C(\\tau)$ where $C(\\tau)_{ij}$ counts observed transitions from state $i$ to state $j$ over lag $\\tau$, the maximum likelihood estimator for the row-stochastic transition matrix $T(\\tau)$ is obtained by row-normalization: $T(\\tau)_{ij} = C(\\tau)_{ij} / \\sum_k C(\\tau)_{ik}$ whenever $\\sum_k C(\\tau)_{ik} > 0$.\n- The stationary distribution $\\pi$ is defined by the left eigenvector equation $\\pi^\\top T(\\tau) = \\pi^\\top$ with the normalization $\\sum_i \\pi_i = 1$ and $\\pi_i \\ge 0$.\n\nYour program must implement the following steps for each test case:\n- Input: two square count matrices $C(\\tau)$ and $C(n\\tau)$ of the same size $m \\times m$, and an integer $n \\ge 1$.\n- Estimate $T(\\tau)$ by row-normalizing $C(\\tau)$. For any row $i$ with zero total count $\\sum_j C(\\tau)_{ij} = 0$, define the corresponding row of $T(\\tau)$ as the uniform distribution over $m$ states, i.e., $T(\\tau)_{ij} = 1/m$ for all $j$.\n- Estimate $T(n\\tau)$ by row-normalizing $C(n\\tau)$ using the same rule for zero rows (uniform rows).\n- Compute an estimate of the stationary distribution $\\pi$ as any nonnegative solution of $\\pi^\\top T(\\tau) = \\pi^\\top$ with $\\sum_i \\pi_i = 1$.\n- Form the discrepancy matrix $\\Delta = T(n\\tau) - T(\\tau)^n$ and compute the $\\pi$-weighted Frobenius norm discrepancy:\n$$\nd = \\left\\| \\Delta \\right\\|_{F,\\pi} = \\sqrt{ \\sum_{i=1}^m \\pi_i \\sum_{j=1}^m \\left( \\Delta_{ij} \\right)^2 }.\n$$\nThis weighting emphasizes errors in frequently occupied states and deemphasizes errors in rare states.\n\nTest suite:\nImplement your program to process the following four test cases in order. For each case, report the scalar value $d$ as a floating-point number.\n\n- Case $1$ (exactly consistent, $m=3$, $n=2$):\n  - $C(\\tau) = \\begin{bmatrix} 32  32  0 \\\\ 16  32  16 \\\\ 0  32  32 \\end{bmatrix}$.\n  - $C(2\\tau) = \\begin{bmatrix} 24  32  8 \\\\ 16  32  16 \\\\ 8  32  24 \\end{bmatrix}$.\n\n- Case $2$ (edge case with a zero row in $C(2\\tau)$, $m=3$, $n=2$):\n  - $C(\\tau) = \\begin{bmatrix} 32  32  0 \\\\ 16  32  16 \\\\ 0  32  32 \\end{bmatrix}$.\n  - $C(2\\tau) = \\begin{bmatrix} 24  32  8 \\\\ 16  32  16 \\\\ 0  0  0 \\end{bmatrix}$.\n\n- Case $3$ (near-absorbing two-state system with sampling noise for $n=3$, $m=2$, $n=3$):\n  - $C(\\tau) = \\begin{bmatrix} 99  1 \\\\ 2  98 \\end{bmatrix}$.\n  - $C(3\\tau) = \\begin{bmatrix} 97  3 \\\\ 6  94 \\end{bmatrix}$.\n\n- Case $4$ (higher-dimensional exactly consistent case, $m=4$, $n=2$):\n  - $C(\\tau) = \\begin{bmatrix} 70  30  0  0 \\\\ 10  80  10  0 \\\\ 0  20  70  10 \\\\ 0  0  30  70 \\end{bmatrix}$.\n  - $C(2\\tau) = \\begin{bmatrix} 52  45  3  0 \\\\ 15  69  15  1 \\\\ 2  30  54  14 \\\\ 0  6  42  52 \\end{bmatrix}$.\n\nFinal output format:\n- Your program must compute $d$ for each of the four cases and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each value rounded to exactly $8$ decimal places, and with no spaces. For example, the output must look like $\\left[\\text{d}_1,\\text{d}_2,\\text{d}_3,\\text{d}_4\\right]$ where each $\\text{d}_k$ is a floating-point number with exactly $8$ digits after the decimal point.\n\nConstraints and clarifications:\n- All matrices are real-valued and finite.\n- The stationary distribution $\\pi$ must be computed from $T(\\tau)$ only.\n- If the solution method produces small negative components in $\\pi$ due to numerical error, they must be clipped to nonnegative values and renormalized to sum to $1$.\n- Angles and physical units are not involved in this problem.",
            "solution": "The objective is to implement a quantitative test of the Chapman–Kolmogorov consistency for a time-homogeneous Markov state model (MSM). The test evaluates the discrepancy between two estimates of a transition matrix at a lag time $n\\tau$: one estimated directly from data at lag $n\\tau$, denoted $T(n\\tau)$, and another predicted by propagating the model from lag time $\\tau$, denoted $T(\\tau)^n$. The discrepancy is quantified using a weighted Frobenius norm.\n\nThe procedure is as follows:\n\n1.  **Estimation of Transition Matrices from Count Data**\n    Given a count matrix $C(t)$, where $C(t)_{ij}$ is the number of observed transitions from a discrete state $i$ to state $j$ over a lag time $t$, the maximum likelihood estimator for the corresponding row-stochastic transition probability matrix $T(t)$ is given by normalizing each row by its sum. Specifically, for each element $T(t)_{ij}$:\n    $$\n    T(t)_{ij} = \\frac{C(t)_{ij}}{\\sum_{k=1}^m C(t)_{ik}}\n    $$\n    where $m$ is the number of states. This is valid only if the total number of outgoing transitions from state $i$, $\\sum_k C(t)_{ik}$, is greater than zero.\n    \n    A special condition is defined for states with no observed outgoing transitions (i.e., $\\sum_k C(t)_{ik} = 0$). In such cases, the corresponding row of the transition matrix is defined as a uniform distribution over all $m$ states:\n    $$\n    T(t)_{ij} = \\frac{1}{m} \\quad \\text{for all } j=1, \\dots, m.\n    $$\n    This procedure is applied to the count matrix $C(\\tau)$ to obtain the transition matrix $T(\\tau)$ and to the count matrix $C(n\\tau)$ to obtain $T(n\\tau)$.\n\n2.  **Propagation of the Transition Matrix**\n    The Chapman–Kolmogorov equation for a time-homogeneous Markov process states that $T(s+t) = T(s)T(t)$. By induction, this implies that for a discrete lag time $\\tau$ and an integer $n \\ge 1$:\n    $$\n    T(n\\tau) = T(\\tau)T((n-1)\\tau) = \\dots = \\underbrace{T(\\tau) T(\\tau) \\cdots T(\\tau)}_{n \\text{ times}} = T(\\tau)^n.\n    $$\n    This equation provides the theoretical prediction for the transition matrix at lag $n\\tau$ based on the model at lag $\\tau$. We compute this matrix, $T(\\tau)^n$, using numerical matrix exponentiation.\n\n3.  **Estimation of the Stationary Distribution**\n    The stationary distribution, denoted by the row vector $\\pi = [\\pi_1, \\pi_2, \\dots, \\pi_m]$, is a probability distribution over the states that remains invariant under the application of the transition matrix. It is defined by the eigenvalue equation:\n    $$\n    \\pi^\\top T(\\tau) = \\pi^\\top\n    $$\n    subject to the constraints $\\sum_{i=1}^m \\pi_i = 1$ and $\\pi_i \\ge 0$ for all $i$. This equation shows that $\\pi^\\top$ is a left eigenvector of $T(\\tau)$ with an eigenvalue of $\\lambda = 1$. Equivalently, $\\pi$ is the right eigenvector of the transposed matrix $T(\\tau)^\\top$ corresponding to the eigenvalue $\\lambda=1$.\n    \n    For a well-behaved MSM (i.e., one that is irreducible and aperiodic), the Perron-Frobenius theorem guarantees that a unique such $\\pi$ exists with all positive components. We compute $\\pi$ by finding the eigenvectors of $T(\\tau)^\\top$. The eigenvector corresponding to the eigenvalue closest to $1$ is selected. Due to potential numerical floating-point inaccuracies, the computed eigenvector components may be complex or slightly negative. We take the real part of the eigenvector, clip any negative values to $0$, and re-normalize the vector so that its components sum to $1$.\n\n4.  **Calculation of the Discrepancy Metric**\n    The discrepancy between the directly estimated matrix $T(n\\tau)$ and the propagated matrix $T(\\tau)^n$ is first captured by the difference matrix:\n    $$\n    \\Delta = T(n\\tau) - T(\\tau)^n.\n    $$\n    To quantify this matrix of differences into a single scalar value, we use the $\\pi$-weighted Frobenius norm, defined as:\n    $$\n    d = \\left\\| \\Delta \\right\\|_{F,\\pi} = \\sqrt{ \\sum_{i=1}^m \\pi_i \\sum_{j=1}^m \\left( \\Delta_{ij} \\right)^2 }.\n    $$\n    This metric can be interpreted as a root-mean-square error. The contribution of the error from each starting state $i$ (i.e., the squared Euclidean norm of the $i$-th row of $\\Delta$) is weighted by its stationary probability $\\pi_i$. This gives more importance to errors in transitions from frequently visited states and less importance to those from rarely visited states, providing a physically meaningful measure of the model's self-consistency.\n    \n    The algorithm proceeds by implementing these four steps for each provided test case.",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Solves the Chapman-Kolmogorov consistency test problem for a suite of test cases.\n    \"\"\"\n\n    def normalize_counts(C):\n        \"\"\"\n        Estimates a transition matrix T from a count matrix C via row-normalization.\n        \n        Args:\n            C (np.ndarray): The m x m count matrix.\n\n        Returns:\n            np.ndarray: The m x m row-stochastic transition matrix.\n        \"\"\"\n        m = C.shape[0]\n        T = C.astype(float)\n        \n        row_sums = T.sum(axis=1)\n        \n        # Identify rows with non-zero counts\n        non_zero_rows = row_sums > 0\n        \n        # Normalize non-zero rows\n        if np.any(non_zero_rows):\n             T[non_zero_rows, :] /= row_sums[non_zero_rows, np.newaxis]\n        \n        # Identify rows with zero counts\n        zero_rows = ~non_zero_rows\n        \n        # Set zero-count rows to uniform distribution\n        if np.any(zero_rows):\n            T[zero_rows, :] = 1.0 / m\n            \n        return T\n\n    def compute_stationary_distribution(T):\n        \"\"\"\n        Computes the stationary distribution pi for a transition matrix T.\n\n        Args:\n            T (np.ndarray): The m x m transition matrix.\n\n        Returns:\n            np.ndarray: The m-dimensional stationary distribution vector.\n        \"\"\"\n        # We need the left eigenvector of T for eigenvalue 1,\n        # which is the right eigenvector of T.T for eigenvalue 1.\n        eigenvalues, eigenvectors = scipy.linalg.eig(T.T)\n        \n        # Find the index of the eigenvalue closest to 1.\n        # Eigenvalues of a stochastic matrix have magnitude = 1.\n        idx = np.argmin(np.abs(eigenvalues - 1.0))\n        \n        # The eigenvector is the corresponding column.\n        pi_vector = eigenvectors[:, idx]\n        \n        # Take the real part (eigenvalues/vectors can be complex due to numerical precision).\n        pi = np.real(pi_vector)\n        \n        # Clip any small negative values that might arise from numerical error.\n        pi[pi  0] = 0\n        \n        # Normalize to ensure it's a probability distribution.\n        pi_sum = pi.sum()\n        if pi_sum > 0:\n            pi /= pi_sum\n        else:\n            # Fallback for an unlikely case of a zero vector\n            m = T.shape[0]\n            pi = np.full(m, 1.0 / m)\n\n        return pi\n\n    def compute_discrepancy(C_tau, C_ntau, n):\n        \"\"\"\n        Computes the pi-weighted Frobenius norm discrepancy for a single test case.\n\n        Args:\n            C_tau (np.ndarray): The count matrix at lag tau.\n            C_ntau (np.ndarray): The count matrix at lag n*tau.\n            n (int): The lag time multiplier.\n\n        Returns:\n            float: The scalar discrepancy value d.\n        \"\"\"\n        # Step 1: Estimate Transition Matrices\n        T_tau = normalize_counts(C_tau)\n        T_ntau = normalize_counts(C_ntau)\n        \n        # Step 2: Compute the Propagated Transition Matrix\n        T_tau_n = np.linalg.matrix_power(T_tau, n)\n        \n        # Step 3: Estimate the Stationary Distribution from T(tau)\n        pi = compute_stationary_distribution(T_tau)\n\n        # Step 4: Compute the Discrepancy\n        delta = T_ntau - T_tau_n\n        \n        # sum_{j} (Delta_ij)^2 for each i\n        row_wise_sq_sum = np.sum(delta**2, axis=1)\n        \n        # sum_{i} pi_i * (sum_{j} (Delta_ij)^2)\n        weighted_sum_of_squares = np.dot(pi, row_wise_sq_sum)\n        \n        d = np.sqrt(weighted_sum_of_squares)\n        \n        return d\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([[32, 32, 0], [16, 32, 16], [0, 32, 32]]), # C(tau)\n            np.array([[24, 32, 8], [16, 32, 16], [8, 32, 24]]), # C(2tau)\n            2 # n\n        ),\n        (\n            np.array([[32, 32, 0], [16, 32, 16], [0, 32, 32]]), # C(tau)\n            np.array([[24, 32, 8], [16, 32, 16], [0, 0, 0]]), # C(2tau)\n            2 # n\n        ),\n        (\n            np.array([[99, 1], [2, 98]]), # C(tau)\n            np.array([[97, 3], [6, 94]]), # C(3tau)\n            3 # n\n        ),\n        (\n            np.array([[70, 30, 0, 0], [10, 80, 10, 0], [0, 20, 70, 10], [0, 0, 30, 70]]), # C(tau)\n            np.array([[52, 45, 3, 0], [15, 69, 15, 1], [2, 30, 54, 14], [0, 6, 42, 52]]), # C(2tau)\n            2 # n\n        ),\n    ]\n\n    results = []\n    for C_tau, C_ntau, n in test_cases:\n        result = compute_discrepancy(C_tau, C_ntau, n)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate purpose of constructing an MSM is often to calculate macroscopic kinetic observables that are difficult to measure directly. This advanced exercise bridges the gap from a validated model to a physical prediction by focusing on the Mean First-Passage Time (MFPT). You will not only compute the MFPT but also quantify its statistical uncertainty, a crucial step for any robust scientific claim, by propagating the posterior uncertainty of the transition matrix—derived from a Bayesian framework with Dirichlet priors—to the final kinetic quantity.",
            "id": "3423406",
            "problem": "A reversible Markov State Model (MSM) is constructed from observed transition counts between discrete microstates. Consider a finite, row-stochastic transition matrix $T \\in \\mathbb{R}^{n \\times n}$ estimated from a count matrix $N \\in \\mathbb{N}_0^{n \\times n}$ with a conjugate Dirichlet prior on each row. For each row $i$, assume a Dirichlet prior with parameters $\\alpha_{i1}, \\dots, \\alpha_{in}$, and let the posterior over row $i$ be $\\mathrm{Dirichlet}(a_{i1}, \\dots, a_{in})$ with $a_{ij} = N_{ij} + \\alpha_{ij}$. Let the posterior mean transition matrix be the row-normalized matrix of $A = N + \\alpha$, i.e., $T_{ij} = a_{ij} / A_i$ with $A_i = \\sum_{j=1}^n a_{ij}$. For each row $i$, the posterior covariance of the entries of the row-stochastic vector $(T_{i1}, \\dots, T_{in})$ is given by the well-tested Dirichlet formulas\n$$\n\\mathbb{E}[T_{ij}] = \\frac{a_{ij}}{A_i}, \\quad \n\\mathrm{Var}(T_{ij}) = \\frac{a_{ij}(A_i - a_{ij})}{A_i^2 (A_i + 1)}, \\quad\n\\mathrm{Cov}(T_{ij}, T_{ik}) = - \\frac{a_{ij} a_{ik}}{A_i^2 (A_i + 1)} \\quad \\text{for } j \\neq k.\n$$\nLet $B \\subset \\{1,\\dots,n\\}$ be a nonempty target (absorbing) set of states, and let $A \\subset \\{1,\\dots,n\\}$ be a nonempty source set of states, with $A \\cap B = \\varnothing$. Define the complement $R = \\{1,\\dots,n\\} \\setminus B$. The mean first-passage time (MFPT) from $A$ to $B$ for a Markov chain with transition matrix $T$ and lag time $\\tau_{\\mathrm{lag}}$ is defined as follows. Let $T_{RR}$ be the restriction of $T$ to rows and columns in $R$. Assume that the spectral radius of $T_{RR}$ is strictly less than $1$ so that absorption into $B$ occurs almost surely. Let $t_R \\in \\mathbb{R}^{|R|}$ solve the linear system\n$$\n(\\mathbf{I}_{|R|} - T_{RR}) \\, t_R = \\mathbf{1}_{|R|},\n$$\nwhich yields the expected number of steps to reach $B$ starting from each state in $R$. Define the MFPT from $A$ to $B$, averaged over the uniform distribution on $A$, as\n$$\n\\tau_{A \\to B}(T) = \\frac{1}{|A|} \\sum_{i \\in A} (t_R)_i.\n$$\nThe physical MFPT in seconds is $\\tau_{\\mathrm{phys}} = \\tau_{\\mathrm{lag}} \\, \\tau_{A \\to B}(T)$.\n\nUsing first-order eigenvalue perturbation theory for row-stochastic matrices and the delta method, one can approximate the posterior uncertainty of $\\tau_{A \\to B}(T)$ by linearizing $\\tau_{A \\to B}$ with respect to the entries of $T$ around the posterior mean $T$ (the row-normalized $N+\\alpha$). Specifically, let $M = (\\mathbf{I}_{|R|} - T_{RR})^{-1}$, let $v = t_R$, and let $s_A \\in \\mathbb{R}^{|R|}$ be the indicator vector on $R$ with $(s_A)_i = 1$ if $i \\in A$ and $(s_A)_i = 0$ otherwise. The first-order sensitivity of $\\tau_{A \\to B}$ to an entry $T_{kl}$ with $k,l \\in R$ is encoded by the gradient entries\n$$\n\\frac{\\partial \\tau_{A \\to B}}{\\partial T_{kl}} = \\frac{1}{|A|} \\, \\big( (M^\\top s_A)_k \\big) \\, v_l, \\quad \\text{and} \\quad \\frac{\\partial \\tau_{A \\to B}}{\\partial T_{kl}} = 0 \\ \\text{if } k \\notin R \\ \\text{or} \\ l \\notin R,\n$$\nwhich is a consequence of the implicit function theorem combined with the spectral representation $M = \\sum_{m} \\frac{1}{1 - \\lambda_m} r_m \\ell_m^\\top$ of the resolvent of $T_{RR}$ when diagonalizable, where $\\lambda_m$ are the eigenvalues of $T_{RR}$ and $r_m, \\ell_m$ are corresponding right and left eigenvectors satisfying $\\ell_m^\\top r_m = 1$. The posterior variance of $\\tau_{A \\to B}$ under the independent row-wise Dirichlet posterior is then approximated as\n$$\n\\mathrm{Var}[\\tau_{A \\to B}] \\approx \\sum_{i=1}^n \\nabla_{T_{i*}} \\tau_{A \\to B}^\\top \\, \\mathrm{Cov}(T_{i*}) \\, \\nabla_{T_{i*}} \\tau_{A \\to B},\n$$\nwhere $\\nabla_{T_{i*}} \\tau_{A \\to B}$ is the gradient restricted to row $i$ (a vector of length $n$ with zero entries for $l \\notin R$), and $\\mathrm{Cov}(T_{i*})$ is the Dirichlet covariance matrix for row $i$. Using a normal approximation to the posterior of $\\tau_{A \\to B}$, a two-sided $(1 - \\gamma)$ credible interval is\n$$\n\\left[ \\tau_{\\mathrm{phys}} - z_{1-\\gamma/2} \\, \\tau_{\\mathrm{lag}} \\, \\sqrt{\\mathrm{Var}[\\tau_{A \\to B}]}, \\ \\ \\tau_{\\mathrm{phys}} + z_{1-\\gamma/2} \\, \\tau_{\\mathrm{lag}} \\, \\sqrt{\\mathrm{Var}[\\tau_{A \\to B}]} \\right],\n$$\nwhere $z_{1-\\gamma/2}$ is the quantile of the standard normal distribution.\n\nTask. Implement a complete, self-contained program to:\n- Construct the posterior mean transition matrix by row-normalizing $N + \\alpha$.\n- Compute $\\tau_{A \\to B}(T)$ in number of steps and convert to seconds using $\\tau_{\\mathrm{lag}}$.\n- Compute the gradient via the linear-response formula above by solving for $M^\\top s_A$ and $t_R$.\n- Assemble the row-wise Dirichlet covariance matrices and compute the delta-method variance of $\\tau_{A \\to B}$.\n- Return the two-sided credible interval in seconds for a credibility level of $0.95$ (i.e., use $z_{0.975}$), without rounding.\n\nUse the following test suite. For each case, output the interval for the MFPT from $A$ to $B$ in seconds as a list $[\\text{lower}, \\text{upper}]$:\n- Case $1$ (happy path): \n  - $N = \\begin{bmatrix} 90  10  5 \\\\ 5  85  10 \\\\ 1  2  97 \\end{bmatrix}$,\n  - $\\alpha = 0.5$ applied to every entry (i.e., $\\alpha_{ij} = 0.5$),\n  - $A = \\{1\\}$, $B = \\{3\\}$, $\\tau_{\\mathrm{lag}} = 1 \\times 10^{-9}$ seconds.\n- Case $2$ (edge, zero direct counts to $B$ from some states):\n  - $N = \\begin{bmatrix} 50  50  0 \\\\ 0  50  50 \\\\ 0  0  100 \\end{bmatrix}$,\n  - $\\alpha = 0.2$ applied to every entry,\n  - $A = \\{1\\}$, $B = \\{3\\}$, $\\tau_{\\mathrm{lag}} = 5 \\times 10^{-10}$ seconds.\n- Case $3$ (nearly decomposable, slow escape):\n  - $N = \\begin{bmatrix} 495  495  10 \\\\ 495  495  10 \\\\ 5  5  990 \\end{bmatrix}$,\n  - $\\alpha = 0.1$ applied to every entry,\n  - $A = \\{1,2\\}$, $B = \\{3\\}$, $\\tau_{\\mathrm{lag}} = 1 \\times 10^{-6}$ seconds.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following format:\n$$\n\\text{[}[ \\text{lower}_1, \\text{upper}_1 ],[ \\text{lower}_2, \\text{upper}_2 ],[ \\text{lower}_3, \\text{upper}_3 ]\\text{]}\n$$\nAll outputs must be in seconds (SI unit), with no additional text.",
            "solution": "This problem requires the implementation of a Bayesian uncertainty quantification for the Mean First-Passage Time (MFPT) derived from a Markov State Model (MSM). The solution relies on the delta method to propagate the posterior uncertainty of the transition matrix, modeled by a Dirichlet distribution, to the MFPT.\n\nThe overall procedure can be broken down into the following key steps:\n\n1.  **Posterior Mean Transition Matrix:** First, we establish the posterior distribution for the transition matrix $T$. Given the transition counts $N$ and a uniform Dirichlet prior $\\alpha$, the parameters of the posterior Dirichlet distribution for each row are $a_{ij} = N_{ij} + \\alpha$. The posterior mean transition matrix, which we denote as $T$, is then computed by row-normalizing the matrix of posterior parameters, $A = [a_{ij}]$.\n\n2.  **Mean First-Passage Time (MFPT) Calculation:** With the posterior mean matrix $T$, we compute the MFPT from a source set $A$ to a target set $B$. This involves identifying the set of transient (non-target) states $R$. The expected number of steps to reach $B$ from any state in $R$, denoted by the vector $t_R$, is found by solving the linear system $(\\mathbf{I}_{|R|} - T_{RR}) t_R = \\mathbf{1}_{|R|}$, where $T_{RR}$ is the submatrix of $T$ for transitions within $R$. The MFPT in units of lag steps, $\\tau_{A \\to B}$, is the average of the elements of $t_R$ that correspond to the source states in $A$. This is then converted to a physical time, $\\tau_{\\mathrm{phys}}$, by multiplying by the lag time $\\tau_{\\mathrm{lag}}$.\n\n3.  **Gradient of the MFPT:** To apply the delta method, we need the sensitivity of the MFPT to changes in the transition matrix elements, which is captured by its gradient, $\\nabla \\tau_{A \\to B}$. The problem provides a linear-response formula for this gradient. The non-zero elements of the gradient (for transitions between transient states) are calculated as $\\frac{\\partial \\tau_{A \\to B}}{\\partial T_{kl}} = \\frac{1}{|A|} (w_k v_l)$, where $v=t_R$ is the vector of passage times from the previous step, and the vector $w$ is found by solving a related linear system: $(\\mathbf{I}_{|R|} - T_{RR})^\\top w = s_A$, where $s_A$ is an indicator vector for the source states.\n\n4.  **Posterior Variance of the MFPT:** The delta method approximates the posterior variance of the MFPT as a quadratic form involving the gradient and the covariance of the transition matrix elements: $\\mathrm{Var}[\\tau_{A \\to B}] \\approx \\sum_i (\\nabla_{T_{i*}})^\\top \\mathrm{Cov}(T_{i*}) (\\nabla_{T_{i*}})$. Since the Dirichlet posteriors for each row are independent, we can sum the contributions to the variance from each row. For each row $i$, we construct its $n \\times n$ covariance matrix, $\\mathrm{Cov}(T_{i*})$, using the standard formulas for a Dirichlet distribution provided in the problem. The total variance is the sum of the row-wise quadratic forms.\n\n5.  **Credible Interval Construction:** Finally, assuming the posterior distribution of the MFPT is approximately normal, we construct the $95\\%$ credible interval. This is given by $\\tau_{\\mathrm{phys}} \\pm z_{0.975} \\cdot \\sigma_{\\mathrm{phys}}$, where $\\sigma_{\\mathrm{phys}} = \\tau_{\\mathrm{lag}} \\sqrt{\\mathrm{Var}[\\tau_{A \\to B}]}$ is the standard deviation of the physical MFPT, and $z_{0.975}$ is the 97.5th percentile of the standard normal distribution, approximately 1.95996.\n\nThis complete procedure is implemented for each of the provided test cases to compute the final credible intervals.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_mfpt_interval(N, alpha, A_1based, B_1based, tau_lag):\n    \"\"\"\n    Computes the 95% credible interval for the MFPT from source set A to target set B.\n    \n    Args:\n        N (list of lists): The n x n transition count matrix.\n        alpha (float): The uniform Dirichlet prior parameter.\n        A_1based (set): The set of 1-based source state indices.\n        B_1based (set): The set of 1-based target state indices.\n        tau_lag (float): The lag time of the Markov model in seconds.\n        \n    Returns:\n        list: A list [lower_bound, upper_bound] for the credible interval in seconds.\n    \"\"\"\n    # Step 1: Setup and Index Conversion\n    N_mat = np.array(N, dtype=float)\n    n = N_mat.shape[0]\n    all_states = set(range(n))\n    A_0based = {x - 1 for x in A_1based}\n    B_0based = {x - 1 for x in B_1based}\n    R_0based = all_states - B_0based\n    R_indices = sorted(list(R_0based))\n    num_R = len(R_indices)\n    R_map = {state_idx: i for i, state_idx in enumerate(R_indices)}\n\n    # Step 2: Posterior Mean Transition Matrix\n    alpha_mat = np.full((n, n), alpha, dtype=float)\n    A_post_mat = N_mat + alpha_mat\n    A_sums = A_post_mat.sum(axis=1)\n    T = A_post_mat / A_sums[:, np.newaxis]\n\n    # Step 3: Mean First-Passage Time (MFPT)\n    T_RR = T[np.ix_(R_indices, R_indices)]\n    I_RR = np.eye(num_R)\n    M_matrix = I_RR - T_RR\n    # v = t_R in problem statement, vector of expected steps to absorption from R\n    v = np.linalg.solve(M_matrix, np.ones(num_R))\n\n    A_in_R_indices = [R_map[i] for i in A_0based]\n    tau_steps = np.mean(v[A_in_R_indices])\n    tau_phys = tau_lag * tau_steps\n\n    # Step 4: Gradient Calculation\n    s_A = np.zeros(num_R)\n    s_A[A_in_R_indices] = 1.0\n    # w = M^T * s_A, solved via linear system\n    w = np.linalg.solve(M_matrix.T, s_A)\n\n    grad_tau = np.zeros((n, n))\n    # grad_tau_RR = (1/|A|) * w * v^T (outer product)\n    grad_tau_RR = (1.0 / len(A_0based)) * np.outer(w, v)\n    grad_tau[np.ix_(R_indices, R_indices)] = grad_tau_RR\n\n    # Step 5: Variance Calculation via Delta Method\n    total_var_tau = 0.0\n    # Sum over rows i in R. Gradient is zero for rows i not in R.\n    for i in R_indices:\n        grad_row_i = grad_tau[i, :]\n        a_i = A_post_mat[i, :]\n        A_i_sum = A_sums[i]\n        \n        # Dirichlet covariance matrix for row i\n        const = 1.0 / (A_i_sum**2 * (A_i_sum + 1.0))\n        diag_part = np.diag(a_i * A_i_sum)\n        outer_part = np.outer(a_i, a_i)\n        Cov_i = const * (diag_part - outer_part)\n\n        row_var = grad_row_i @ Cov_i @ grad_row_i\n        total_var_tau += row_var\n\n    # Step 6: Credible Interval\n    z_val = norm.ppf(0.975)  # for 95% interval\n    std_dev_tau_steps = np.sqrt(total_var_tau)\n    # Uncertainty in physical time\n    uncertainty_phys = z_val * tau_lag * std_dev_tau_steps\n\n    lower_bound = tau_phys - uncertainty_phys\n    upper_bound = tau_phys + uncertainty_phys\n\n    return [lower_bound, upper_bound]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"N\": [[90, 10, 5], [5, 85, 10], [1, 2, 97]],\n            \"alpha\": 0.5,\n            \"A\": {1},\n            \"B\": {3},\n            \"tau_lag\": 1e-9\n        },\n        # Case 2 (edge, zero direct counts to B)\n        {\n            \"N\": [[50, 50, 0], [0, 50, 50], [0, 0, 100]],\n            \"alpha\": 0.2,\n            \"A\": {1},\n            \"B\": {3},\n            \"tau_lag\": 5e-10\n        },\n        # Case 3 (nearly decomposable)\n        {\n            \"N\": [[495, 495, 10], [495, 495, 10], [5, 5, 990]],\n            \"alpha\": 0.1,\n            \"A\": {1, 2},\n            \"B\": {3},\n            \"tau_lag\": 1e-6\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        result = calculate_mfpt_interval(\n            case[\"N\"], case[\"alpha\"], case[\"A\"], case[\"B\"], case[\"tau_lag\"]\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    result_str = \",\".join([f\"[{res[0]},{res[1]}]\" for res in results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        }
    ]
}