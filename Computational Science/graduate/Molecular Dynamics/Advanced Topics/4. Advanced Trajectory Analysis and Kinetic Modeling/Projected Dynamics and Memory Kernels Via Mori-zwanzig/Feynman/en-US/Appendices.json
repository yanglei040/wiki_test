{
    "hands_on_practices": [
        {
            "introduction": "The first step in applying the Mori-Zwanzig formalism is understanding how to compute the memory kernel from a known microscopic model. This exercise explores the core of the formalism: defining a projection operator and applying the orthogonal dynamics generator, $\\mathcal{Q}\\mathcal{L}$, to calculate the time evolution of the \"random\" force. By implementing and comparing two different computational routes, you will gain a deep, practical understanding of why the orthogonal dynamics are distinct from simply projecting the full dynamics, a crucial concept in projected dynamics. ",
            "id": "3438295",
            "problem": "Consider a microscopic Hamiltonian system modeling a coarse-grained protein folding coordinate as a linear collective variable in a high-dimensional harmonic environment. The microscopic positions are $\\mathbf{x} \\in \\mathbb{R}^N$ with velocities $\\mathbf{v} \\in \\mathbb{R}^N$, masses $m_i$ collected in a diagonal mass matrix $\\mathbf{M} = \\operatorname{diag}(m_1,\\dots,m_N)$ and stiffness matrix $\\mathbf{K} \\in \\mathbb{R}^{N \\times N}$ that is symmetric positive definite. The potential energy is $U(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^\\top \\mathbf{K} \\mathbf{x}$. The resolved collective coordinate is $Q = \\mathbf{c}^\\top \\mathbf{x}$, where $\\mathbf{c} \\in \\mathbb{R}^N$ is a fixed coefficient vector. The microdynamics obey Newton’s laws: $\\dot{\\mathbf{x}} = \\mathbf{v}$ and $\\dot{\\mathbf{v}} = -\\mathbf{M}^{-1} \\mathbf{K} \\mathbf{x}$, and the canonical equilibrium at temperature $T$ has Boltzmann factor proportional to $\\exp\\left(-\\beta H(\\mathbf{x},\\mathbf{v})\\right)$ with $H(\\mathbf{x},\\mathbf{v}) = \\tfrac{1}{2}\\mathbf{x}^\\top \\mathbf{K}\\mathbf{x} + \\tfrac{1}{2}\\mathbf{v}^\\top \\mathbf{M}\\mathbf{v}$ and $\\beta = 1/(k_{\\mathrm{B}} T)$, where $k_{\\mathrm{B}}$ is Boltzmann’s constant.\n\nDefine the phase-space vector $\\mathbf{z} = [\\mathbf{x}; \\mathbf{v}] \\in \\mathbb{R}^{2N}$ and the linear system matrix\n$$\n\\mathbf{A}_{\\mathrm{sys}} = \\begin{bmatrix}\n\\mathbf{0} & \\mathbf{I} \\\\\n-\\mathbf{M}^{-1}\\mathbf{K} & \\mathbf{0}\n\\end{bmatrix},\n$$\nso that $\\dot{\\mathbf{z}} = \\mathbf{A}_{\\mathrm{sys}} \\mathbf{z}$. The canonical covariance of $\\mathbf{z}$ is\n$$\n\\boldsymbol{\\Sigma} = \\begin{bmatrix}\nk_{\\mathrm{B}} T\\, \\mathbf{K}^{-1} & \\mathbf{0} \\\\\n\\mathbf{0} & k_{\\mathrm{B}} T\\, \\mathbf{M}^{-1}\n\\end{bmatrix}.\n$$\nConsider the linear observables $Q(\\mathbf{z}) = \\mathbf{q}^\\top \\mathbf{z}$ and $\\dot{Q}(\\mathbf{z}) = \\mathbf{d}^\\top \\mathbf{z}$ with coefficient vectors $\\mathbf{q} = [\\mathbf{c}; \\mathbf{0}]$ and $\\mathbf{d} = [\\mathbf{0}; \\mathbf{c}]$. The microscopic generalized force conjugate to $Q$ is $F(\\mathbf{z}) = -\\partial U / \\partial Q = -\\mathbf{c}^\\top \\mathbf{K} \\mathbf{x}$, which is also a linear observable with coefficient vector $\\mathbf{f} = [-\\mathbf{K}\\mathbf{c}; \\mathbf{0}]$.\n\nLet $\\mathcal{V}$ be the two-dimensional subspace of linear observables spanned by $Q$ and $\\dot{Q}$. Using the canonical inner product $\\langle A, B \\rangle = \\mathbb{E}[A B]$ in equilibrium, the orthogonal projector $\\mathbf{P}$ acting on observable coefficient vectors is the $\\boldsymbol{\\Sigma}$-weighted projector onto the span of $\\mathbf{q}$ and $\\mathbf{d}$:\n$$\n\\mathbf{P} = \\mathbf{V}\\left(\\mathbf{V}^\\top \\boldsymbol{\\Sigma} \\mathbf{V}\\right)^{-1}\\mathbf{V}^\\top \\boldsymbol{\\Sigma}, \\quad \\text{with } \\mathbf{V} = [\\mathbf{q}, \\mathbf{d}] \\in \\mathbb{R}^{2N \\times 2}.\n$$\nThe perpendicular component of the force is $\\mathbf{f}_\\perp = (\\mathbf{I} - \\mathbf{P}) \\mathbf{f}$.\n\nImplement a program that, for each given test case, performs the following tasks:\n\n1. Construct $\\mathbf{K}$ as a sum of a diagonal stiffness and a nearest-neighbor chain Laplacian to ensure a symmetric positive definite matrix. Explicitly, let the chain Laplacian be the $N \\times N$ tridiagonal matrix with diagonal entries $2$ except the endpoints which are $1$, and $-1$ on the immediate off-diagonals. Let $\\mathbf{K} = k_{\\mathrm{main}} \\mathbf{I} + k_{\\mathrm{couple}} \\mathbf{L}_{\\mathrm{chain}}$, where $k_{\\mathrm{main}}$ and $k_{\\mathrm{couple}}$ are given scalars. All stiffness values must be expressed in newtons per meter ($\\mathrm{N}/\\mathrm{m}$).\n\n2. Build $\\mathbf{M}$, $\\mathbf{A}_{\\mathrm{sys}}$, $\\boldsymbol{\\Sigma}$, $\\mathbf{P}$, and $\\mathbf{f}_\\perp$ as specified above. All masses must be expressed in kilograms ($\\mathrm{kg}$), the temperature in kelvin ($\\mathrm{K}$), and time in seconds ($\\mathrm{s}$).\n\n3. Orthogonal dynamics route: Evolve the perpendicular force coefficients using the orthogonal dynamics generator acting on observables as a linear operator on coefficients, i.e., use the matrix $\\mathbf{G}_{\\mathrm{OD}} = (\\mathbf{I} - \\mathbf{P}) \\mathbf{A}_{\\mathrm{sys}}^\\top$. For a time $t$, the evolved coefficients are $\\mathbf{a}_{\\mathrm{OD}}(t) = \\exp\\left(\\mathbf{G}_{\\mathrm{OD}} t\\right)\\mathbf{f}_\\perp$. From these, compute the time series $K_{\\mathrm{OD}}(t)$ of the memory kernel using the canonical inner product with $\\boldsymbol{\\Sigma}$ and scaling by $\\beta$. Express $K_{\\mathrm{OD}}(t)$ in $\\mathrm{N}/\\mathrm{m}$.\n\n4. Force-autocorrelation estimate route: Evolve the microscopic force coefficients under the full dynamics using $\\mathbf{G}_{\\mathrm{full}} = \\mathbf{A}_{\\mathrm{sys}}^\\top$, i.e., $\\mathbf{a}_{\\mathrm{full}}(t) = \\exp\\left(\\mathbf{G}_{\\mathrm{full}} t\\right)\\mathbf{f}$. Project the evolved coefficients onto the perpendicular subspace to obtain $\\mathbf{a}_{\\mathrm{FAC}}(t) = (\\mathbf{I} - \\mathbf{P}) \\mathbf{a}_{\\mathrm{full}}(t)$. From these, compute the time series $K_{\\mathrm{FAC}}(t)$ in $\\mathrm{N}/\\mathrm{m}$ using the canonical inner product with $\\boldsymbol{\\Sigma}$ and scaling by $\\beta$.\n\n5. On a uniform time grid $t \\in [0, t_{\\max}]$ with $N_t$ points, compute the maximum absolute difference over the grid:\n$$\n\\Delta = \\max_{t \\in \\{t_k\\}} \\left| K_{\\mathrm{OD}}(t) - K_{\\mathrm{FAC}}(t) \\right|.\n$$\nReturn $\\Delta$ as a floating-point number in $\\mathrm{N}/\\mathrm{m}$.\n\nYour program must implement the above for the following test suite and produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order of the tests, for example $[r_1,r_2,r_3]$:\n\n- Test Case $1$ (happy path):\n  - $N = 4$\n  - Masses $\\mathbf{m} = [2.0 \\times 10^{-26}, 3.0 \\times 10^{-26}, 2.5 \\times 10^{-26}, 2.2 \\times 10^{-26}]$ in $\\mathrm{kg}$\n  - $k_{\\mathrm{main}} = 5.0$ in $\\mathrm{N}/\\mathrm{m}$\n  - $k_{\\mathrm{couple}} = 2.0$ in $\\mathrm{N}/\\mathrm{m}$\n  - Temperature $T = 300.0$ in $\\mathrm{K}$\n  - Collective coordinate coefficients $\\mathbf{c}$ as uniform weights: $c_i = 1/N$ for $i = 1,\\dots,N$\n  - Time grid: $t_{\\max} = 5.0 \\times 10^{-12}$ in $\\mathrm{s}$, $N_t = 51$\n\n- Test Case $2$ (boundary alignment of $Q$ with a coordinate axis):\n  - $N = 3$\n  - Masses $\\mathbf{m} = [2.0 \\times 10^{-26}, 2.0 \\times 10^{-26}, 2.0 \\times 10^{-26}]$ in $\\mathrm{kg}$\n  - $k_{\\mathrm{main}} = 10.0$ in $\\mathrm{N}/\\mathrm{m}$\n  - $k_{\\mathrm{couple}} = 0.5$ in $\\mathrm{N}/\\mathrm{m}$\n  - Temperature $T = 300.0$ in $\\mathrm{K}$\n  - Collective coordinate coefficients $\\mathbf{c} = [1.0, 0.0, 0.0]$\n  - Time grid: $t_{\\max} = 5.0 \\times 10^{-12}$ in $\\mathrm{s}$, $N_t = 51$\n\n- Test Case $3$ (edge case with strong coupling and heterogeneous masses):\n  - $N = 5$\n  - Masses $\\mathbf{m} = [1.5 \\times 10^{-26}, 2.0 \\times 10^{-26}, 2.5 \\times 10^{-26}, 2.8 \\times 10^{-26}, 3.0 \\times 10^{-26}]$ in $\\mathrm{kg}$\n  - $k_{\\mathrm{main}} = 8.0$ in $\\mathrm{N}/\\mathrm{m}$\n  - $k_{\\mathrm{couple}} = 6.0$ in $\\mathrm{N}/\\mathrm{m}$\n  - Temperature $T = 300.0$ in $\\mathrm{K}$\n  - Collective coordinate coefficients $\\mathbf{c}$ with alternating signs: $c_i = (-1)^{i-1} / \\sqrt{N}$ for $i = 1,\\dots,N$\n  - Time grid: $t_{\\max} = 5.0 \\times 10^{-12}$ in $\\mathrm{s}$, $N_t = 51$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3]$), where each $r_i$ is the maximum absolute difference $\\Delta$ for the corresponding test case expressed in $\\mathrm{N}/\\mathrm{m}$.",
            "solution": "The problem statement submitted for consideration is valid. It presents a well-posed and scientifically grounded problem within the domain of statistical mechanics and molecular dynamics, specifically focusing on the numerical implementation of the Mori-Zwanzig projection operator formalism for a linear system. The problem asks for the comparison between two computational routes for the memory kernel, a task that is non-trivial and probes a key subtlety in the application of projection methods. All parameters and definitions are provided, ensuring the problem is self-contained and allows for a unique, verifiable solution. We may therefore proceed with a complete and reasoned solution.\n\nThe core of the problem lies in computing the memory kernel, $K(t)$, of a generalized Langevin equation for a coarse-grained variable $Q$. The kernel is defined as the time-autocorrelation of the \"random\" or \"orthogonal\" force, which is the component of the microscopic force evolving in the subspace orthogonal to the resolved variables. For a linear system, this formalism can be translated into matrix algebra on the coefficient vectors of linear observables. We will construct the necessary matrices, implement the two specified dynamical evolution schemes, and compute the resulting difference in the memory kernels.\n\n**1. System and Phase-Space Matrix Construction**\nThe system consists of $N$ particles in a harmonic potential. We first construct the fundamental matrices in configuration space.\n- The mass matrix $\\mathbf{M} \\in \\mathbb{R}^{N \\times N}$ is a diagonal matrix of the given particle masses, $\\mathbf{M} = \\operatorname{diag}(m_1, \\dots, m_N)$.\n- The stiffness matrix $\\mathbf{K} \\in \\mathbb{R}^{N \\times N}$ is given by $\\mathbf{K} = k_{\\mathrm{main}} \\mathbf{I} + k_{\\mathrm{couple}} \\mathbf{L}_{\\mathrm{chain}}$. The chain Laplacian matrix $\\mathbf{L}_{\\mathrm{chain}}$ is a symmetric positive semi-definite matrix. As $k_{\\mathrm{main}} > 0$ and $k_{\\mathrm{couple}} \\ge 0$, the resulting matrix $\\mathbf{K}$ is symmetric and strictly positive definite, which guarantees that its inverse $\\mathbf{K}^{-1}$ exists.\n\nThe dynamics are formulated in a $2N$-dimensional phase space with state vector $\\mathbf{z} = [\\mathbf{x}; \\mathbf{v}]$. The time evolution is governed by the linear system of ordinary differential equations $\\dot{\\mathbf{z}} = \\mathbf{A}_{\\mathrm{sys}} \\mathbf{z}$, where the system matrix $\\mathbf{A}_{\\mathrm{sys}} \\in \\mathbb{R}^{2N \\times 2N}$ is defined as:\n$$\n\\mathbf{A}_{\\mathrm{sys}} = \\begin{bmatrix}\n\\mathbf{0} & \\mathbf{I} \\\\\n-\\mathbf{M}^{-1}\\mathbf{K} & \\mathbf{0}\n\\end{bmatrix}\n$$\nThe canonical equilibrium ensemble average of the outer product of the phase-space vector with itself defines the covariance matrix $\\boldsymbol{\\Sigma} = \\mathbb{E}[\\mathbf{z}\\mathbf{z}^\\top]$. For this harmonic system, it is given by:\n$$\n\\boldsymbol{\\Sigma} = \\begin{bmatrix}\nk_{\\mathrm{B}} T\\, \\mathbf{K}^{-1} & \\mathbf{0} \\\\\n\\mathbf{0} & k_{\\mathrm{B}} T\\, \\mathbf{M}^{-1}\n\\end{bmatrix}\n$$\nwhere $k_{\\mathrm{B}}$ is the Boltzmann constant and $T$ is the temperature.\n\n**2. Projection Operator Formalism**\nThe space of observables is restricted to the two-dimensional subspace spanned by the coarse-grained coordinate $Q(\\mathbf{z}) = \\mathbf{q}^\\top \\mathbf{z}$ and its time derivative $\\dot{Q}(\\mathbf{z}) = \\mathbf{d}^\\top \\mathbf{z}$. The corresponding coefficient vectors are $\\mathbf{q} = [\\mathbf{c}; \\mathbf{0}]$ and $\\mathbf{d} = [\\mathbf{0}; \\mathbf{c}]$. The canonical inner product between two linear observables $A = \\mathbf{a}^\\top\\mathbf{z}$ and $B = \\mathbf{b}^\\top\\mathbf{z}$ is $\\langle A, B \\rangle = \\mathbb{E}[AB] = \\mathbf{a}^\\top \\boldsymbol{\\Sigma} \\mathbf{b}$.\n\nThe projector $\\mathbf{P}$ which projects an observable's coefficient vector onto the subspace spanned by $\\mathbf{q}$ and $\\mathbf{d}$ is defined as:\n$$\n\\mathbf{P} = \\mathbf{V}\\left(\\mathbf{V}^\\top \\boldsymbol{\\Sigma} \\mathbf{V}\\right)^{-1}\\mathbf{V}^\\top \\boldsymbol{\\Sigma}, \\quad \\text{with } \\mathbf{V} = [\\mathbf{q}, \\mathbf{d}]\n$$\nThis is an oblique projector in the Euclidean sense, but it is orthogonal (self-adjoint) with respect to the $\\boldsymbol{\\Sigma}$-weighted inner product, satisfying $\\mathbf{a}^\\top \\boldsymbol{\\Sigma} (\\mathbf{P}\\mathbf{b}) = (\\mathbf{P}\\mathbf{a})^\\top \\boldsymbol{\\Sigma} \\mathbf{b}$. The complementary orthogonal projector is $\\mathcal{Q}_{\\text{proj}} = \\mathbf{I} - \\mathbf{P}$.\n\nThe microscopic generalized force $F = -\\mathbf{c}^\\top \\mathbf{K} \\mathbf{x}$ corresponds to the coefficient vector $\\mathbf{f} = [-\\mathbf{K}\\mathbf{c}; \\mathbf{0}]$. The \"random force\" in the Mori-Zwanzig formalism is the component of this force orthogonal to the resolved subspace. Its coefficient vector at time $t=0$ is $\\mathbf{f}_\\perp = (\\mathbf{I} - \\mathbf{P})\\mathbf{f}$.\n\n**3. Dynamics and Memory Kernel Calculation**\nThe evolution of an observable's coefficient vector $\\mathbf{a}$ in the Heisenberg picture is given by $\\mathbf{a}(t) = \\exp(\\mathbf{A}_{\\mathrm{sys}}^\\top t) \\mathbf{a}(0)$. The two routes for computing the memory kernel differ in how they handle the dynamics. The memory kernel is defined as $K(t) = \\beta \\mathbb{E}[R(t) R(0)]$, where $R(t)$ is the random force at time $t$ and $\\beta = (k_{\\mathrm{B}}T)^{-1}$.\n\n**Route 1: Orthogonal Dynamics (OD)**\nThis is the theoretically exact Mori-Zwanzig formulation. The random force evolves according to the projected dynamics, generated by $\\mathcal{Q}_{\\text{proj}}\\mathbf{A}_{\\mathrm{sys}}^\\top$.\n- Evolved random force coefficients: $\\mathbf{a}_{\\mathrm{OD}}(t) = \\exp\\left( (\\mathbf{I} - \\mathbf{P})\\mathbf{A}_{\\mathrm{sys}}^\\top t \\right) \\mathbf{f}_\\perp$.\n- Memory Kernel: $K_{\\mathrm{OD}}(t) = \\beta \\, \\mathbf{a}_{\\mathrm{OD}}(t)^\\top \\boldsymbol{\\Sigma} \\, \\mathbf{f}_\\perp$.\n\n**Route 2: Force-Autocorrelation Estimate (FAC)**\nThis route uses an alternative formulation, which is often an approximation. The full force is evolved under the full dynamics, and the result is projected onto the orthogonal subspace *after* evolution.\n- Evolved full force coefficients: $\\mathbf{a}_{\\mathrm{full}}(t) = \\exp(\\mathbf{A}_{\\mathrm{sys}}^\\top t) \\mathbf{f}$.\n- Projected coefficients: $\\mathbf{a}_{\\mathrm{FAC}}(t) = (\\mathbf{I} - \\mathbf{P}) \\mathbf{a}_{\\mathrm{full}}(t)$.\n- Memory Kernel: $K_{\\mathrm{FAC}}(t) = \\beta \\, \\mathbf{a}_{\\mathrm{FAC}}(t)^\\top \\boldsymbol{\\Sigma} \\, \\mathbf{f}_\\perp$.\n\nThe two routes are not mathematically identical because the projection operator $\\mathbf{P}$ and the dynamics generator $\\mathbf{A}_{\\mathrm{sys}}^\\top$ do not commute. The problem requires computing the maximum absolute difference $\\Delta = \\max_t |K_{\\mathrm{OD}}(t) - K_{\\mathrm{FAC}}(t)|$ over a discrete time grid.\n\n**4. Numerical Implementation Synopsis**\nFor each test case, the algorithm proceeds as follows:\n1. Construct the matrices $\\mathbf{M}$, $\\mathbf{K}$, $\\mathbf{A}_{\\mathrm{sys}}$, and $\\boldsymbol{\\Sigma}$ from the given parameters $N$, $\\mathbf{m}$, $k_{\\mathrm{main}}$, $k_{\\mathrm{couple}}$, and $T$.\n2. Construct the coefficient vectors $\\mathbf{q}$, $\\mathbf{d}$, and $\\mathbf{f}$ from the vector $\\mathbf{c}$.\n3. Form the projector $\\mathbf{P}$ and the initial random force vector $\\mathbf{f}_\\perp$.\n4. Define the dynamics generators $\\mathbf{G}_{\\mathrm{OD}} = (\\mathbf{I} - \\mathbf{P})\\mathbf{A}_{\\mathrm{sys}}^\\top$ and $\\mathbf{G}_{\\mathrm{full}} = \\mathbf{A}_{\\mathrm{sys}}^\\top$.\n5. Iterate over the time grid points $t_k \\in [0, t_{\\max}]$. In each iteration:\n   a. Compute the matrix exponentials $\\exp(\\mathbf{G}_{\\mathrm{OD}} t_k)$ and $\\exp(\\mathbf{G}_{\\mathrm{full}} t_k)$ using a robust numerical algorithm (`scipy.linalg.expm`).\n   b. Calculate $\\mathbf{a}_{\\mathrm{OD}}(t_k)$ and $\\mathbf{a}_{\\mathrm{FAC}}(t_k)$.\n   c. Compute the kernel values $K_{\\mathrm{OD}}(t_k)$ and $K_{\\mathrm{FAC}}(t_k)$.\n6. Determine the maximum absolute difference between the two computed kernel time series. This value, $\\Delta$, is the result for the test case.\nAll calculations are performed using floating-point arithmetic with values in standard SI units.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases and prints the final result.\n    \"\"\"\n    k_B = 1.380649e-23  # Boltzmann constant in J/K\n\n    test_cases = [\n        {\n            \"N\": 4,\n            \"m_vals\": np.array([2.0e-26, 3.0e-26, 2.5e-26, 2.2e-26]),\n            \"k_main\": 5.0,\n            \"k_couple\": 2.0,\n            \"T\": 300.0,\n            \"c_rule\": lambda N: np.ones(N) / N,\n            \"t_max\": 5.0e-12,\n            \"N_t\": 51,\n        },\n        {\n            \"N\": 3,\n            \"m_vals\": np.array([2.0e-26, 2.0e-26, 2.0e-26]),\n            \"k_main\": 10.0,\n            \"k_couple\": 0.5,\n            \"T\": 300.0,\n            \"c_rule\": lambda N: np.array([1.0] + [0.0] * (N - 1)),\n            \"t_max\": 5.0e-12,\n            \"N_t\": 51,\n        },\n        {\n            \"N\": 5,\n            \"m_vals\": np.array([1.5e-26, 2.0e-26, 2.5e-26, 2.8e-26, 3.0e-26]),\n            \"k_main\": 8.0,\n            \"k_couple\": 6.0,\n            \"T\": 300.0,\n            \"c_rule\": lambda N: ((-1.0) ** np.arange(N)) / np.sqrt(N),\n            \"t_max\": 5.0e-12,\n            \"N_t\": 51,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        delta = calculate_delta(\n            case[\"N\"],\n            case[\"m_vals\"],\n            case[\"k_main\"],\n            case[\"k_couple\"],\n            case[\"T\"],\n            case[\"c_rule\"],\n            case[\"t_max\"],\n            case[\"N_t\"],\n            k_B,\n        )\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef calculate_delta(N, m_vals, k_main, k_couple, T, c_rule, t_max, N_t, k_B):\n    \"\"\"\n    Calculates the maximum absolute difference Delta for a single test case.\n    \"\"\"\n    # 1. Construct Basic and Phase-Space Matrices\n    # Mass matrix M and its inverse\n    M = np.diag(m_vals)\n    M_inv = np.diag(1.0 / m_vals)\n\n    # Stiffness matrix K\n    L_chain = (\n        np.diag(np.full(N, 2.0))\n        - np.diag(np.ones(N - 1), k=1)\n        - np.diag(np.ones(N - 1), k=-1)\n    )\n    if N > 0:\n        L_chain[0, 0] = 1.0\n    if N > 1:\n        L_chain[N - 1, N - 1] = 1.0\n    \n    K = k_main * np.identity(N) + k_couple * L_chain\n    K_inv = np.linalg.inv(K)\n\n    # Phase-space matrices A_sys and Sigma\n    dim = 2 * N\n    A_sys = np.zeros((dim, dim))\n    A_sys[:N, N:] = np.identity(N)\n    A_sys[N:, :N] = -M_inv @ K\n\n    Sigma = np.zeros((dim, dim))\n    Sigma[:N, :N] = k_B * T * K_inv\n    Sigma[N:, N:] = k_B * T * M_inv\n\n    # 2. Construct Vectors and Projector\n    c_vec = c_rule(N)\n    q = np.concatenate([c_vec, np.zeros(N)])\n    d = np.concatenate([np.zeros(N), c_vec])\n    f = np.concatenate([-K @ c_vec, np.zeros(N)])\n\n    V = np.vstack([q, d]).T  # Shape (2N, 2)\n    \n    # Gram matrix for projection\n    G_mat = V.T @ Sigma @ V\n    G_inv = np.linalg.inv(G_mat)\n\n    # Projection operator P\n    P = V @ G_inv @ V.T @ Sigma\n    Q_proj = np.identity(dim) - P\n\n    # Perpendicular force vector f_perp\n    f_perp = Q_proj @ f\n\n    # 3. Time Evolution and Kernel Calculation\n    t_grid = np.linspace(0, t_max, N_t)\n    beta = 1.0 / (k_B * T)\n\n    K_OD = np.zeros(N_t)\n    K_FAC = np.zeros(N_t)\n\n    G_full = A_sys.T\n    G_OD = Q_proj @ A_sys.T\n    \n    for i, t in enumerate(t_grid):\n        # Orthogonal Dynamics (OD) Route\n        exp_G_OD_t = linalg.expm(G_OD * t)\n        a_OD_t = exp_G_OD_t @ f_perp\n        K_OD[i] = beta * (a_OD_t.T @ Sigma @ f_perp)\n\n        # Force-Autocorrelation (FAC) Route\n        exp_G_full_t = linalg.expm(G_full * t)\n        a_full_t = exp_G_full_t @ f\n        a_FAC_t = Q_proj @ a_full_t\n        K_FAC[i] = beta * (a_FAC_t.T @ Sigma @ f_perp)\n\n    # 4. Calculate Final Difference\n    delta = np.max(np.abs(K_OD - K_FAC))\n    return delta\n\nsolve()\n```"
        },
        {
            "introduction": "The power of the Mori-Zwanzig framework lies in its ability to systematically derive coarse-grained models, but the quality of these models depends heavily on the choice of the resolved variables. This practice investigates the connection between the choice of a coarse-grained coordinate and the properties of the resulting memory kernel and random force, where a \"good\" coordinate should capture the slow dynamics, leaving a weaker, faster-decaying residual noise. You will learn to quantitatively evaluate different coarse-graining choices, a key skill for developing effective reduced models of complex systems. ",
            "id": "3438303",
            "problem": "Consider a deterministic, linear microscopic dynamics for a state vector $x(t) \\in \\mathbb{R}^n$ governed by the ordinary differential equation $ \\dot{x}(t) = A x(t) $, where $A \\in \\mathbb{R}^{n \\times n}$ is a constant matrix. Assume that the equilibrium ensemble of initial conditions is Gaussian with zero mean and covariance matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$, which is symmetric positive definite. For a scalar coarse-grained observable (reaction coordinate) defined as $ \\xi(x) = c^\\top x $ with $c \\in \\mathbb{R}^n$, consider the one-dimensional Mori-Zwanzig (MZ) projection with respect to the inner product $(u,v) = \\mathbb{E}[u v]$ under the equilibrium distribution. Using the Mori-Zwanzig formalism and the covariance-weighted orthogonal projection onto the span of $\\xi$, the projected dynamics of $\\xi(t)$ admits a Generalized Langevin Equation (GLE) with a memory kernel $K(t)$ and a residual (random) force $R(t)$.\n\nStarting from the deterministic microscopic law $ \\dot{x} = A x $, the Liouville operator acting on linear observables is $ i \\mathcal{L} \\xi = c^\\top A x = d^\\top x $, where $ d = A^\\top c $. Let the projection operator onto $\\operatorname{span}(\\xi)$ be $ \\mathcal{P} b = \\frac{(b,\\xi)}{(\\xi,\\xi)} \\xi $. For linear observables $ b(x) = u^\\top x $ with $ u \\in \\mathbb{R}^n $, this induces the coefficient-space projection $ P u = c \\frac{u^\\top \\Sigma c}{c^\\top \\Sigma c} $, and the corresponding orthogonal projector $ Q = I - P $. Define the initial residual-force coefficient $ g = d - \\alpha c $, where $ \\alpha = \\frac{d^\\top \\Sigma c}{c^\\top \\Sigma c} $. The orthogonal dynamics for coefficients evolves by $ u(t) = e^{t Q A^\\top} u(0) $. Hence, the residual force is $ R(t) = u(t)^\\top x(t_0) $ with $ u(0) = g $, and its equilibrium mean-squared magnitude at time $t$ is $ \\mathbb{E}[R(t)^2] = u(t)^\\top \\Sigma u(t) $. The memory kernel for the scalar projected dynamics is\n$$\nK(t) = \\frac{(R(0), R(t))}{(\\xi,\\xi)} = \\frac{g^\\top \\Sigma \\, e^{t Q A^\\top} g}{c^\\top \\Sigma c}.\n$$\n\nYour task is to implement a program that, for each specified test case, computes the memory kernels $K_1(t)$ and $K_2(t)$ corresponding to two different reaction coordinates $\\xi_1(x) = c_1^\\top x$ and $\\xi_2(x) = c_2^\\top x$, and evaluates which coordinate yields the smaller time-averaged, covariance-normalized residual noise\n$$\n\\langle R_j^2 \\rangle_T = \\frac{1}{T} \\int_0^T \\frac{u_j(t)^\\top \\Sigma \\, u_j(t)}{c_j^\\top \\Sigma c_j} \\, dt, \\quad j \\in \\{1,2\\},\n$$\nwhere $u_j(t) = e^{t Q_j A^\\top} g_j$, $g_j = d_j - \\alpha_j c_j$, $d_j = A^\\top c_j$, $\\alpha_j = \\frac{d_j^\\top \\Sigma c_j}{c_j^\\top \\Sigma c_j}$, and $Q_j = I - c_j \\frac{(\\Sigma c_j)^\\top}{c_j^\\top \\Sigma c_j}$. For numerical implementation, approximate the integral by a uniform average over $N$ time points on $[0,T]$ using the matrix exponential $e^{t Q_j A^\\top}$.\n\nAll quantities are dimensionless (no physical units). Use the following test suite, where each case specifies $(A,\\Sigma,c_1,c_2,T,N)$:\n\n- Test case 1 (two-dimensional slow-fast coupling; mixed second coordinate):\n$$\nA = \\begin{bmatrix}\n-0.10 & 1.00 \\\\\n-4.00 & -1.00\n\\end{bmatrix}, \\quad\n\\Sigma = \\begin{bmatrix}\n1.0 & 0.0 \\\\\n0.0 & 1.0\n\\end{bmatrix}, \\quad\nc_1 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\quad\nc_2 = \\begin{bmatrix} 1.0 \\\\ 0.5 \\end{bmatrix}, \\quad\nT = 5.0, \\quad N = 200.\n$$\n\n- Test case 2 (identical coordinates; tie expected):\n$$\nA = \\begin{bmatrix}\n-0.10 & 1.00 \\\\\n-4.00 & -1.00\n\\end{bmatrix}, \\quad\n\\Sigma = \\begin{bmatrix}\n1.0 & 0.0 \\\\\n0.0 & 1.0\n\\end{bmatrix}, \\quad\nc_1 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\quad\nc_2 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\quad\nT = 5.0, \\quad N = 200.\n$$\n\n- Test case 3 (two-dimensional with a strongly fast second coordinate):\n$$\nA = \\begin{bmatrix}\n-0.05 & 10.0 \\\\\n-0.10 & -2.0\n\\end{bmatrix}, \\quad\n\\Sigma = \\begin{bmatrix}\n1.0 & 0.0 \\\\\n0.0 & 1.0\n\\end{bmatrix}, \\quad\nc_1 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\quad\nc_2 = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}, \\quad\nT = 4.0, \\quad N = 200.\n$$\n\n- Test case 4 (three-dimensional with nontrivial covariance):\n$$\nA = \\begin{bmatrix}\n-0.20 & 0.80 & 0.00 \\\\\n-1.00 & -0.50 & 2.00 \\\\\n0.00 & -1.50 & -0.30\n\\end{bmatrix}, \\quad\n\\Sigma = \\begin{bmatrix}\n2.0 & 0.0 & 0.0 \\\\\n0.0 & 1.0 & 0.0 \\\\\n0.0 & 0.0 & 0.5\n\\end{bmatrix}, \\quad\nc_1 = \\begin{bmatrix} 1.0 \\\\ 0.2 \\\\ 0.0 \\end{bmatrix}, \\quad\nc_2 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.7 \\end{bmatrix}, \\quad\nT = 6.0, \\quad N = 300.\n$$\n\nFor each test case, compute $K_1(t)$ and $K_2(t)$ over the specified time grid, then compute $\\langle R_1^2 \\rangle_T$ and $\\langle R_2^2 \\rangle_T$ using the arithmetic average over the $N$ points. Decide which coordinate minimizes the normalized residual noise. If the two averages are equal within a numerical tolerance of $10^{-12}$, choose the first coordinate.\n\nFinal output format: Your program should produce a single line of output containing the chosen coordinate indices for the four test cases as a comma-separated list enclosed in square brackets (e.g., $[1,2,1,2]$).",
            "solution": "The user wants to solve a problem based on the Mori-Zwanzig formalism for projected dynamics.\n\n### Step 1: Extract Givens\nThe problem provides the following data and definitions for a linear dynamical system $\\dot{x}(t) = A x(t)$:\n-   State vector: $x(t) \\in \\mathbb{R}^n$\n-   Dynamics matrix: $A \\in \\mathbb{R}^{n \\times n}$\n-   Equilibrium distribution: Gaussian with zero mean and covariance $\\Sigma \\in \\mathbb{R}^{n \\times n}$, symmetric positive definite.\n-   Scalar observable (reaction coordinate): $\\xi(x) = c^\\top x$ for a vector $c \\in \\mathbb{R}^n$.\n-   Inner product: $(u,v) = \\mathbb{E}[u v]$. For linear observables $u=u^\\top x$ and $v=v^\\top x$, this is $(u,v) = u^\\top \\Sigma v$.\n-   Liouville operator on linear observables: $i \\mathcal{L} \\xi = c^\\top A x = d^\\top x$, with $d = A^\\top c$.\n-   Projection operator on coefficients: $P u = c \\frac{u^\\top \\Sigma c}{c^\\top \\Sigma c}$.\n-   Orthogonal projection operator: $Q = I - P$. The problem provides the matrix form $Q_j = I - c_j \\frac{(\\Sigma c_j)^\\top}{c_j^\\top \\Sigma c_j}$.\n-   Initial residual-force coefficient: $g = d - \\alpha c$, where $\\alpha = \\frac{d^\\top \\Sigma c}{c^\\top \\Sigma c}$.\n-   Orthogonal dynamics of coefficients: $u(t) = e^{t Q A^\\top} u(0)$. For the residual force, $u(0) = g$.\n-   Time-averaged, covariance-normalized residual noise: $\\langle R_j^2 \\rangle_T = \\frac{1}{T} \\int_0^T \\frac{u_j(t)^\\top \\Sigma \\, u_j(t)}{c_j^\\top \\Sigma c_j} \\, dt$ for coordinate $j \\in \\{1,2\\}$.\n-   Numerical approximation: The integral is approximated by an arithmetic average over $N$ uniformly spaced time points on the interval $[0,T]$.\n-   Decision rule: For each test case, choose the coordinate $j$ that minimizes $\\langle R_j^2 \\rangle_T$. If the values are equal within a tolerance of $10^{-12}$, choose coordinate $1$.\n-   Test Cases: Four test cases are provided, each specifying the tuples $(A, \\Sigma, c_1, c_2, T, N)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the established Mori-Zwanzig theory of projected dynamics, a standard topic in statistical mechanics. The application to a linear system with Gaussian statistics is a common and well-understood model. All mathematical definitions for the projectors, propagators, and derived quantities are standard and self-consistent.\n\nThe problem is well-posed. All necessary inputs ($A, \\Sigma, c_1, c_2, T, N$) are provided for each case. The definitions lead to a unique computational procedure. The use of a symmetric positive definite covariance matrix $\\Sigma$ ensures that denominators like $c^\\top \\Sigma c$ are non-zero for any non-zero vector $c$, preventing divisions by zero. The objective is clear and unambiguous: to compute and compare two values and make a decision based on a precise rule. The language is objective and formal. There are no contradictions, missing information, or pseudoscientific claims. The problem is a non-trivial but solvable numerical task.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Algorithmic Solution\nThe core of the problem is to compute the time-averaged normalized residual noise, $\\langle R_j^2 \\rangle_T$, for two reaction coordinates, $\\xi_1(x) = c_1^\\top x$ and $\\xi_2(x) = c_2^\\top x$. The procedure for each coordinate $c_j$ corresponding to index $j$ is as follows:\n\n1.  **Compute Preliminary Quantities**:\n    Given the matrices $A$ and $\\Sigma$, and the vector $c_j$:\n    -   Calculate the coefficient vector of the time derivative of the observable: $d_j = A^\\top c_j$.\n    -   Calculate the normalization constant, which is the variance of the observable $\\xi_j$: $\\text{var}(\\xi_j) = (\\xi_j, \\xi_j) = c_j^\\top \\Sigma c_j$.\n    -   Calculate the frequency factor $\\alpha_j = \\frac{(i\\mathcal{L}\\xi_j, \\xi_j)}{(\\xi_j, \\xi_j)} = \\frac{d_j^\\top \\Sigma c_j}{c_j^\\top \\Sigma c_j}$.\n    -   Calculate the initial coefficient of the orthogonal force: $g_j = d_j - \\alpha_j c_j$. This vector is orthogonal to $c_j$ with respect to the $\\Sigma$-weighted inner product, i.e., $g_j^\\top \\Sigma c_j = 0$.\n\n2.  **Define the Orthogonal Dynamics**:\n    -   Construct the projection matrix $P_j$ that projects coefficient vectors onto the direction of $c_j$. Its matrix form is $P_j = \\frac{c_j c_j^\\top \\Sigma}{c_j^\\top \\Sigma c_j}$.\n    -   The orthogonal projector is $Q_j = I - P_j$.\n    -   The generator for the orthogonal dynamics of the coefficients is the matrix $M_j = Q_j A^\\top$.\n\n3.  **Compute the Time-Averaged Noise**:\n    -   A uniform time grid is defined with $N$ points over the interval $[0, T]$: $t_k = k \\frac{T}{N-1}$ for $k=0, 1, \\dots, N-1$.\n    -   For each time point $t_k$ on the grid, the evolution of the orthogonal coefficient is computed: $u_j(t_k) = e^{t_k M_j} g_j$. The matrix exponential $e^{t_k M_j}$ is calculated numerically.\n    -   At each time point $t_k$, the instantaneous value of the normalized squared residual noise is calculated: $f_j(t_k) = \\frac{u_j(t_k)^\\top \\Sigma \\, u_j(t_k)}{c_j^\\top \\Sigma c_j}$.\n    -   The time average $\\langle R_j^2 \\rangle_T$ is approximated by the arithmetic mean of these values over the $N$ points: $S_j = \\frac{1}{N} \\sum_{k=0}^{N-1} f_j(t_k)$.\n\n4.  **Compare and Decide**:\n    -   The two computed averages, $S_1$ and $S_2$, are compared.\n    -   Following the specified rule, the coordinate with index $1$ is chosen if $S_1 - S_2 < 10^{-12}$. This condition is satisfied if $S_1$ is significantly smaller than $S_2$, or if $S_1$ and $S_2$ are equal within the given tolerance. Otherwise, coordinate $2$ is chosen.\n\nThis procedure is applied to each test case to determine the optimal coordinate. The final output is a list of the chosen indices.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef calculate_average_noise(A, Sigma, c, T, N):\n    \"\"\"\n    Calculates the time-averaged, covariance-normalized residual noise for a given reaction coordinate.\n\n    Args:\n        A (np.ndarray): The dynamics matrix.\n        Sigma (np.ndarray): The covariance matrix of the equilibrium distribution.\n        c (np.ndarray): The coefficient vector defining the reaction coordinate.\n        T (float): The total integration time.\n        N (int): The number of time points for the numerical average.\n\n    Returns:\n        float: The calculated average residual noise.\n    \"\"\"\n    n = A.shape[0]\n    Id = np.identity(n)\n    At = A.T\n\n    # 1. Compute preliminary quantities\n    d = At @ c\n    denom = c.T @ Sigma @ c\n    \n    # Handle the trivial case where c is the zero vector, though not expected from problem spec.\n    if np.isclose(denom, 0):\n        return np.inf\n\n    num_alpha = d.T @ Sigma @ c\n    alpha = num_alpha / denom\n    g = d - alpha * c\n\n    # 2. Define the orthogonal dynamics\n    # The problem specifies Q_j = I - c_j * (Sigma c_j)^T / (c_j^T Sigma c_j).\n    # Since Sigma is symmetric, (Sigma c_j)^T = c_j^T Sigma.\n    # The projection matrix P_j is c_j * (c_j^T Sigma) / (c_j^T Sigma c_j).\n    # In numpy, this is np.outer(c_j, c_j @ Sigma) / denom.\n    P = np.outer(c, c @ Sigma) / denom\n    Q = Id - P\n    \n    M = Q @ At\n\n    # 3. Compute the time-averaged noise\n    time_points = np.linspace(0.0, T, N)\n    integrand_values = []\n\n    for t in time_points:\n        # Evolve the orthogonal coefficient\n        # u(t) = exp(t * M) @ g\n        exp_tM = expm(t * M)\n        u_t = exp_tM @ g\n\n        # Calculate the instantaneous normalized squared residual noise\n        # (u(t)^T * Sigma * u(t)) / (c^T * Sigma * c)\n        numerator = u_t.T @ Sigma @ u_t\n        integrand_value = numerator / denom\n        integrand_values.append(integrand_value)\n\n    # Approximate the integral by the arithmetic mean\n    avg_noise = np.mean(integrand_values)\n    \n    return avg_noise\n\ndef solve():\n    \"\"\"\n    Solves the Mori-Zwanzig problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        (\n            np.array([[-0.10, 1.00], [-4.00, -1.00]]),\n            np.array([[1.0, 0.0], [0.0, 1.0]]),\n            np.array([1.0, 0.0]),\n            np.array([1.0, 0.5]),\n            5.0,\n            200\n        ),\n        # Test case 2\n        (\n            np.array([[-0.10, 1.00], [-4.00, -1.00]]),\n            np.array([[1.0, 0.0], [0.0, 1.0]]),\n            np.array([1.0, 0.0]),\n            np.array([1.0, 0.0]),\n            5.0,\n            200\n        ),\n        # Test case 3\n        (\n            np.array([[-0.05, 10.0], [-0.10, -2.0]]),\n            np.array([[1.0, 0.0], [0.0, 1.0]]),\n            np.array([1.0, 0.0]),\n            np.array([0.0, 1.0]),\n            4.0,\n            200\n        ),\n        # Test case 4\n        (\n            np.array([[-0.20, 0.80, 0.00], [-1.00, -0.50, 2.00], [0.00, -1.50, -0.30]]),\n            np.array([[2.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.5]]),\n            np.array([1.0, 0.2, 0.0]),\n            np.array([0.5, 0.5, 0.7]),\n            6.0,\n            300\n        )\n    ]\n\n    results = []\n    tolerance = 1e-12\n\n    for A, Sigma, c1, c2, T, N in test_cases:\n        avg_noise_1 = calculate_average_noise(A, Sigma, c1, T, N)\n        avg_noise_2 = calculate_average_noise(A, Sigma, c2, T, N)\n        \n        # Decision rule: choose 1 if S1 is smaller or if they are equal within tolerance.\n        if avg_noise_1 - avg_noise_2  tolerance:\n            results.append(1)\n        else:\n            results.append(2)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While deriving kernels from analytical models is instructive, in real-world research we often start with simulation data, and this exercise bridges the gap between theory and practical data analysis. It demonstrates how the convolution structure of the Generalized Langevin Equation (GLE) can be transformed into a linear regression problem, allowing modern techniques like sparse regression (LASSO) to be used to estimate the memory kernel from a trajectory. This practice provides hands-on experience with a powerful method for building data-driven coarse-grained models, connecting the abstract Mori-Zwanzig formalism to the domain of statistical learning. ",
            "id": "3438307",
            "problem": "Consider a single resolved coordinate $q(t)$ from a high-dimensional biomolecular system described by the Mori-Zwanzig projection formalism, which yields a Generalized Langevin Equation (GLE) for projected dynamics. In this description, the frictional memory force associated with unresolved degrees of freedom is represented by a convolution of a memory kernel with the resolved velocity. A common parametric approximation for the memory kernel in practical computations is to represent it as a sparse combination of basis functions. Your task is to derive a discrete estimator for the coefficients of such a kernel from first principles and implement a sparse regression procedure to recover them from synthetic data.\n\nStarting point and physical context:\n- Assume a one-dimensional GLE for a resolved coordinate $q(t)$ with memory force $F_{\\text{mem}}(t)$ that depends on the history of the resolved velocity $\\dot{q}(t)$ via a causal convolution.\n- Let the memory kernel be approximated as a weighted sum of causal basis functions. Denote these basis functions by $\\{\\phi_i(t)\\}_{i=1}^p$ where each $\\phi_i(t)$ vanishes for $t0$, and the kernel approximation is a linear combination of these basis functions with unknown coefficients $\\{c_i\\}$.\n- Time $t$ is expressed in picoseconds (ps), the coordinate $q(t)$ is in nanometers (nm), the velocity $\\dot{q}(t)$ in $\\text{nm}/\\text{ps}$, and the memory force $F_{\\text{mem}}(t)$ in piconewtons (pN). Any output requested from your program must comply with these units if physical quantities are involved. In this task, the final outputs are dimensionless booleans, so no unit conversion is needed for outputs.\n\nDerivation requirements:\n- Beginning from the causal structure implied by the Mori-Zwanzig formalism and the convolution nature of the memory force, derive a discrete-time representation that expresses sampled values of $F_{\\text{mem}}(t)$ as a linear function of the unknown coefficients $\\{c_i\\}$ and time-sampled, basis-function-filtered velocities. Use a Riemann sum approximation to convert the temporal integrals into discrete sums on a uniform grid with step size $\\Delta t$.\n- Show how this leads to a linear system suitable for regression, where the design matrix encodes the convolution of each basis function $\\phi_i(t)$ with the sampled velocity $\\dot{q}(t)$.\n- Formulate a sparse regression estimator based on the Least Absolute Shrinkage and Selection Operator (LASSO), which minimizes a convex objective that balances data fidelity and an $\\ell_1$ penalty on the coefficients $\\{c_i\\}$.\n- Provide an algorithmic strategy to solve the LASSO, selecting a proximal gradient method such as the Iterative Soft-Thresholding Algorithm (ISTA), and explain how to choose a stable step size using the Lipschitz constant of the gradient.\n\nImplementation requirements:\n- Implement a program that:\n  1. Generates synthetic, deterministic time series for $q(t)$ and $\\dot{q}(t)$ over a specified interval using trigonometric functions with physically plausible frequencies and amplitudes.\n  2. Constructs a set of causal basis functions $\\phi_i(t)$ and builds the discrete design matrix by convolving each basis function with the sampled velocity using a uniform time grid.\n  3. Synthesizes $F_{\\text{mem}}(t)$ using known, ground-truth sparse coefficients and the design matrix, optionally without noise to isolate estimation accuracy.\n  4. Solves the LASSO problem via ISTA to estimate the coefficients, using a step size determined by the spectral norm of the design matrix.\n  5. Evaluates the reconstruction quality using the normalized root-mean-square error between the predicted and ground-truth memory force signals; for cases where the ground-truth signal is identically zero, treat the error as zero by convention. Report a boolean indicating whether the normalized error is below a specified tolerance.\n\nTest suite specification:\n- Use the following common deterministic trajectory for all test cases:\n  - Time grid: $t_n = n \\Delta t$ for $n=0,1,\\dots,N-1$, with $\\Delta t = 0.01\\,\\text{ps}$ and total duration $T = 20\\,\\text{ps}$, hence $N = \\lfloor T/\\Delta t \\rfloor + 1$.\n  - Coordinate: $q(t) = 0.5\\,\\text{nm}\\,\\sin(2\\pi \\cdot 0.05\\,\\text{ps}^{-1}\\, t) + 0.3\\,\\text{nm}\\,\\sin(2\\pi \\cdot 0.20\\,\\text{ps}^{-1}\\, t) + 0.2\\,\\text{nm}\\,\\sin(2\\pi \\cdot 0.50\\,\\text{ps}^{-1}\\, t)$.\n  - Velocity: $\\dot{q}(t)$ is the time derivative of $q(t)$, computed exactly from the above expression, yielding units of $\\text{nm}/\\text{ps}$.\n- Basis functions are causal exponentials, $\\phi_i(t) = e^{-t/\\tau_i}$ for $t \\ge 0$ and $0$ for $t  0$, with timescales $\\tau_i$ in picoseconds.\n- Construct the design matrix by causal discrete convolution on the uniform grid using the Riemann sum factor $\\Delta t$.\n\nDefine four test cases covering different scenarios:\n1. General case (sparse, multiple timescales, well-separated):\n   - Timescales: $\\tau = [0.1,\\,0.3,\\,1.0,\\,3.0,\\,7.0]\\,\\text{ps}$.\n   - True coefficients: $c = [0.0,\\,2.0,\\,-1.5,\\,0.0,\\,0.5]$ (units implicit in the discretization yielding $F_{\\text{mem}}(t)$ in $\\text{pN}$).\n   - LASSO parameter: $\\lambda = 10^{-4}$.\n   - Tolerance for success: normalized root-mean-square error less than $10^{-3}$.\n2. Boundary case (zero kernel):\n   - Timescales: $\\tau = [1.0,\\,2.0,\\,3.0,\\,4.0]\\,\\text{ps}$.\n   - True coefficients: $c = [0.0,\\,0.0,\\,0.0,\\,0.0]$.\n   - LASSO parameter: $\\lambda = 10^{-3}$.\n   - Tolerance: normalized error less than $10^{-8}$.\n3. Edge case (highly correlated basis functions):\n   - Timescales: $\\tau = [0.50,\\,0.51,\\,5.0]\\,\\text{ps}$.\n   - True coefficients: $c = [1.0,\\,0.0,\\,0.0]$.\n   - LASSO parameter: $\\lambda = 10^{-3}$.\n   - Tolerance: normalized error less than $10^{-3}$.\n4. Single-basis case:\n   - Timescales: $\\tau = [2.0]\\,\\text{ps}$.\n   - True coefficient: $c = [3.0]$.\n   - LASSO parameter: $\\lambda = 10^{-4}$.\n   - Tolerance: normalized error less than $10^{-3}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one boolean per test case indicating whether the reconstruction succeeded according to its tolerance (e.g., \"[True,False,True,True]\").",
            "solution": "The problem requires the derivation and implementation of a sparse regression method to estimate the coefficients of a memory kernel in a Generalized Langevin Equation (GLE). The validation and subsequent solution are presented below.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **System Model:** A single resolved coordinate $q(t)$ governed by a GLE arising from the Mori-Zwanzig formalism.\n- **Memory Force:** $F_{\\text{mem}}(t)$ is a causal convolution of a memory kernel $K(t)$ and the resolved velocity $\\dot{q}(t)$.\n- **Kernel Approximation:** The memory kernel is approximated as a sparse linear combination of $p$ causal basis functions, $K(t) \\approx \\sum_{i=1}^{p} c_i \\phi_i(t)$, where $\\phi_i(t) = 0$ for $t  0$.\n- **Discretization:** A uniform time grid $t_n = n \\Delta t$ is used, and integrals are approximated by Riemann sums.\n- **Regression Problem:** Estimate the sparse coefficients $\\{c_i\\}$ using the Least Absolute Shrinkage and Selection Operator (LASSO).\n- **Solver:** The Iterative Soft-Thresholding Algorithm (ISTA) is to be used.\n- **Synthetic Data:**\n    - Time grid: $\\Delta t = 0.01\\,\\text{ps}$, total duration $T = 20\\,\\text{ps}$, $N = \\lfloor T/\\Delta t \\rfloor + 1 = 2001$.\n    - Coordinate: $q(t) = 0.5\\,\\text{nm}\\,\\sin(2\\pi \\cdot 0.05\\,\\text{ps}^{-1}\\, t) + 0.3\\,\\text{nm}\\,\\sin(2\\pi \\cdot 0.20\\,\\text{ps}^{-1}\\, t) + 0.2\\,\\text{nm}\\,\\sin(2\\pi \\cdot 0.50\\,\\text{ps}^{-1}\\, t)$.\n    - Velocity $\\dot{q}(t)$: The exact time derivative of $q(t)$.\n    - Basis functions: $\\phi_i(t) = e^{-t/\\tau_i}$ for $t \\ge 0$.\n- **Test Cases:** Four specific test cases are defined with different sets of timescales $\\{\\tau_i\\}$, true coefficients $\\{c_i\\}$, LASSO parameters $\\lambda$, and error tolerances.\n- **Evaluation Metric:** The normalized root-mean-square error (NRMSE) between the true and reconstructed memory force. A boolean is reported based on whether the NRMSE is below a given tolerance. For a zero ground-truth signal, the error is defined as zero.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientific Grounding:** The problem is firmly rooted in the established principles of statistical mechanics and computational physics. The Mori-Zwanzig formalism, the GLE, the concept of memory kernels, and their approximation using basis functions are standard tools in the study of complex systems, particularly in molecular dynamics. The use of sparse regression techniques like LASSO to identify such models from data is a current and valid research topic.\n- **Well-Posedness:** The problem is well-posed. It details a clear computational task: given a set of inputs (trajectory, basis functions, true coefficients), generate synthetic data and then apply a specified algorithm (ISTA for LASSO) to estimate the coefficients. The existence and uniqueness of the LASSO solution are guaranteed by the convexity of its objective function.\n- **Objectivity:** The problem is stated in precise, objective, and quantitative terms. All parameters are specified, and the criteria for success are unambiguously defined by a numerical tolerance.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is scientifically sound, self-contained, and well-posed. It presents a non-trivial but feasible computational task that is directly relevant to the specified topic in molecular dynamics. There are no contradictions, ambiguities, or factual errors. The problem is deemed **valid**. We may proceed with the solution.\n\n### Derivation and Algorithmic Design\n\n**1. From Continuous Convolution to a Discrete Linear System**\n\nThe memory force $F_{\\text{mem}}(t)$ arises from the history of interactions between the resolved coordinate and the unresolved degrees of freedom. In the GLE formalism, this is expressed as a convolution with a memory kernel $K(t')$. Given the causality of physical interactions, the kernel vanishes for negative time arguments, $K(t')=0$ for $t'0$. The force at time $t$ is thus:\n$$ F_{\\text{mem}}(t) = \\int_{0}^{\\infty} K(\\tau) \\dot{q}(t-\\tau) d\\tau $$\nWe are given that the kernel is approximated by a linear combination of $p$ basis functions, $K(\\tau) \\approx \\sum_{i=1}^{p} c_i \\phi_i(\\tau)$. Substituting this into the integral and exploiting linearity, we obtain:\n$$ F_{\\text{mem}}(t) = \\sum_{i=1}^{p} c_i \\left( \\int_{0}^{\\infty} \\phi_i(\\tau) \\dot{q}(t-\\tau) d\\tau \\right) $$\nEach term in the parentheses is a convolution of a basis function $\\phi_i$ with the velocity $\\dot{q}$. Let's denote the velocity as $v(t) = \\dot{q}(t)$ and the convolution integral for the $i$-th basis as $X_i(t) = (\\phi_i * v)(t)$. Then the expression simplifies to:\n$$ F_{\\text{mem}}(t) = \\sum_{i=1}^{p} c_i X_i(t) $$\nThis shows that the memory force is a linear function of the unknown coefficients $\\{c_i\\}$. To turn this into a system solvable on a computer, we discretize in time. Let $t_n = n \\Delta t$ for $n = 0, 1, \\dots, N-1$. We evaluate the equation at each time point $t_n$. The convolution integral is approximated by a Riemann sum:\n$$ X_i(t_n) = \\int_{0}^{t_n} \\phi_i(\\tau) v(t_n-\\tau) d\\tau \\approx \\sum_{k=0}^{n} \\phi_i(k\\Delta t) v(t_n - k\\Delta t) \\Delta t $$\nLet $F_{\\text{mem}, n} = F_{\\text{mem}}(t_n)$, $v_m = v(t_m)$, and $\\phi_{i, k} = \\phi_i(k\\Delta t)$. Let the result of the discrete convolution sum be $(v * \\phi_i)_n = \\sum_{k=0}^{n} v_{n-k} \\phi_{i, k}$. Then the discretized expression for the features is $X_{ni} = \\Delta t (v * \\phi_i)_n$. The full system of equations for all time points can be written in matrix form:\n$$ \\mathbf{F}_{\\text{mem}} = \\mathbf{X} \\mathbf{c} $$\nwhere $\\mathbf{F}_{\\text{mem}}$ is an $N$-dimensional column vector of memory force samples $[F_{\\text{mem}, 0}, \\dots, F_{\\text{mem}, N-1}]^T$, $\\mathbf{c}$ is the $p$-dimensional column vector of coefficients $[c_1, \\dots, c_p]^T$, and $\\mathbf{X}$ is the $N \\times p$ design matrix whose elements are $X_{ni}$. Each column of $\\mathbf{X}$ is the time series of the velocity convolved with one of the basis functions.\n\n**2. Sparse Regression via LASSO**\n\nThe problem specifies that the kernel representation is sparse, meaning most coefficients $c_i$ are zero. To recover this structure, we employ the LASSO estimator, which finds the coefficients $\\mathbf{c}$ by minimizing a penalized least-squares objective function:\n$$ \\hat{\\mathbf{c}} = \\arg\\min_{\\mathbf{c}} \\left\\{ \\frac{1}{2} || \\mathbf{X}\\mathbf{c} - \\mathbf{F}_{\\text{mem}} ||_2^2 + \\lambda ||\\mathbf{c}||_1 \\right\\} $$\nThe first term, $\\frac{1}{2} || \\mathbf{X}\\mathbf{c} - \\mathbf{F}_{\\text{mem}} ||_2^2$, is the data fidelity term, which encourages the solution to fit the observed data. The second term, $\\lambda ||\\mathbf{c}||_1 = \\lambda \\sum_{i=1}^p |c_i|$, is the $\\ell_1$-regularization penalty. This penalty promotes sparsity by driving some coefficients exactly to zero. The parameter $\\lambda \\ge 0$ controls the trade-off between data fidelity and sparsity.\n\n**3. The Iterative Soft-Thresholding Algorithm (ISTA)**\n\nThe LASSO objective function is convex but not differentiable everywhere due to the absolute value in the $\\ell_1$-norm. Proximal gradient methods are well-suited for such problems. ISTA is one such method. It iteratively performs a gradient descent step on the smooth part of the objective, followed by a \"proximal\" step that accounts for the non-smooth part.\n\nThe objective is $J(\\mathbf{c}) = f(\\mathbf{c}) + g(\\mathbf{c})$, with $f(\\mathbf{c}) = \\frac{1}{2} || \\mathbf{X}\\mathbf{c} - \\mathbf{F}_{\\text{mem}} ||_2^2$ and $g(\\mathbf{c}) = \\lambda ||\\mathbf{c}||_1$.\nThe gradient of the smooth part is $\\nabla f(\\mathbf{c}) = \\mathbf{X}^T (\\mathbf{X}\\mathbf{c} - \\mathbf{F}_{\\text{mem}})$.\nThe ISTA update rule for the $(k+1)$-th iteration is:\n$$ \\mathbf{c}^{(k+1)} = \\text{prox}_{\\alpha g}(\\mathbf{c}^{(k)} - \\alpha \\nabla f(\\mathbf{c}^{(k)})) $$\nwhere $\\alpha$ is the step size. The proximal operator for the $\\ell_1$-norm scaled by $\\alpha\\lambda$ is the element-wise soft-thresholding operator, $S_{\\kappa}(z) = \\text{sgn}(z) \\max(|z| - \\kappa, 0)$.\nThe complete update rule is:\n$$ \\mathbf{c}^{(k+1)} = S_{\\alpha\\lambda} \\left( \\mathbf{c}^{(k)} - \\alpha \\mathbf{X}^T (\\mathbf{X}\\mathbf{c}^{(k)} - \\mathbf{F}_{\\text{mem}}) \\right) $$\n\nFor convergence, the step size $\\alpha$ must be chosen such that $0  \\alpha  2/L$, where $L$ is the Lipschitz constant of the gradient $\\nabla f$. The Lipschitz constant is the largest eigenvalue (spectral norm) of the Hessian $\\nabla^2 f(\\mathbf{c}) = \\mathbf{X}^T\\mathbf{X}$. This is given by $L = \\lambda_{\\max}(\\mathbf{X}^T\\mathbf{X}) = ||\\mathbf{X}||_2^2$, where $||\\mathbf{X}||_2$ is the spectral norm of $\\mathbf{X}$ (its largest singular value). A safe and standard choice is $\\alpha = 1/L$.\n\n**4. Evaluation**\n\nAfter obtaining the estimated coefficients $\\hat{\\mathbf{c}}$ from ISTA, the memory force is reconstructed as $\\hat{\\mathbf{F}}_{\\text{mem}} = \\mathbf{X}\\hat{\\mathbf{c}}$. The quality of the reconstruction is quantified by the normalized root-mean-square error (NRMSE):\n$$ \\text{NRMSE} = \\frac{\\sqrt{\\frac{1}{N}\\sum_{n=0}^{N-1} (F_{\\text{mem}, n} - \\hat{F}_{\\text{mem}, n})^2}}{\\sqrt{\\frac{1}{N}\\sum_{n=0}^{N-1} F_{\\text{mem}, n}^2}} = \\frac{||\\mathbf{F}_{\\text{mem}} - \\hat{\\mathbf{F}}_{\\text{mem}}||_2}{||\\mathbf{F}_{\\text{mem}}||_2} $$\nAs specified, if the denominator is zero (i.e., $\\mathbf{F}_{\\text{mem}} = \\mathbf{0}$), the NRMSE is considered to be zero. A test case is successful if its NRMSE is below the specified tolerance.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Mori-Zwanzig memory kernel reconstruction problem for a set of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: General sparse case\n        {\n            \"taus\": [0.1, 0.3, 1.0, 3.0, 7.0],\n            \"c_true\": [0.0, 2.0, -1.5, 0.0, 0.5],\n            \"lambda\": 1e-4,\n            \"tolerance\": 1e-3,\n        },\n        # Case 2: Zero kernel\n        {\n            \"taus\": [1.0, 2.0, 3.0, 4.0],\n            \"c_true\": [0.0, 0.0, 0.0, 0.0],\n            \"lambda\": 1e-3,\n            \"tolerance\": 1e-8,\n        },\n        # Case 3: Highly correlated basis functions\n        {\n            \"taus\": [0.50, 0.51, 5.0],\n            \"c_true\": [1.0, 0.0, 0.0],\n            \"lambda\": 1e-3,\n            \"tolerance\": 1e-3,\n        },\n        # Case 4: Single-basis case\n        {\n            \"taus\": [2.0],\n            \"c_true\": [3.0],\n            \"lambda\": 1e-4,\n            \"tolerance\": 1e-3,\n        },\n    ]\n\n    results = []\n\n    # Common parameters for all test cases\n    dt = 0.01  # ps\n    T = 20.0   # ps\n    N = int(T / dt) + 1\n    t = np.linspace(0, T, N)\n\n    # Generate reference trajectory q(t) and velocity v(t) = dq/dt\n    w1, w2, w3 = 2 * np.pi * 0.05, 2 * np.pi * 0.20, 2 * np.pi * 0.50\n    a1, a2, a3 = 0.5, 0.3, 0.2\n    \n    # q(t) in nm\n    q_t = a1 * np.sin(w1 * t) + a2 * np.sin(w2 * t) + a3 * np.sin(w3 * t) \n    # v(t) in nm/ps\n    v_t = a1 * w1 * np.cos(w1 * t) + a2 * w2 * np.cos(w2 * t) + a3 * w3 * np.cos(w3 * t)\n\n    def soft_threshold(z, kappa):\n        \"\"\"Soft-thresholding operator for ISTA.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - kappa, 0)\n\n    for case in test_cases:\n        taus = case[\"taus\"]\n        c_true = np.array(case[\"c_true\"])\n        lamb = case[\"lambda\"]\n        tolerance = case[\"tolerance\"]\n\n        p = len(taus)\n        \n        # 1. Construct the design matrix X by convolving velocity with basis functions\n        X = np.zeros((N, p))\n        for i in range(p):\n            # Causal exponential basis function\n            phi = np.exp(-t / taus[i])\n            # Discrete convolution using Riemann sum factor dt\n            # (phi * v)(t_n) approx sum_{k=0 to n} phi(k*dt)v(t_n-k*dt)dt\n            convolution_result = np.convolve(v_t, phi, mode=\"full\")[:N]\n            X[:, i] = convolution_result * dt\n\n        # 2. Synthesize the ground-truth memory force F_mem (in pN)\n        F_mem = X @ c_true\n\n        # 3. Solve for coefficients using ISTA\n        # Determine stable step size from Lipschitz constant\n        # L = ||X^T X||_2 = ||X||_2^2 (spectral norm)\n        L = np.linalg.norm(X, ord=2) ** 2\n        # A condition number check to handle the case of X being a zero matrix\n        if L == 0:\n            alpha = 1.0\n        else:\n            alpha = 1.0 / L\n\n        # ISTA implementation\n        c_est = np.zeros(p)\n        n_iterations = 10000  # Number of iterations for convergence\n        for _ in range(n_iterations):\n            gradient = X.T @ (X @ c_est - F_mem)\n            c_update = c_est - alpha * gradient\n            c_est = soft_threshold(c_update, alpha * lamb)\n            \n        # 4. Evaluate reconstruction quality\n        F_pred = X @ c_est\n        \n        # Calculate normalized root-mean-square error (NRMSE)\n        norm_F_mem = np.linalg.norm(F_mem)\n        \n        if norm_F_mem == 0:\n            # Per problem spec, if ground truth is zero, NRMSE is zero if prediction is also zero.\n            # ISTA with c_true=0 will yield c_est=0, so F_pred=0 and error is zero.\n            nrmse = 0.0\n        else:\n            error = F_pred - F_mem\n            nrmse = np.linalg.norm(error) / norm_F_mem\n            \n        # Record if the reconstruction was successful\n        results.append(nrmse  tolerance)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}