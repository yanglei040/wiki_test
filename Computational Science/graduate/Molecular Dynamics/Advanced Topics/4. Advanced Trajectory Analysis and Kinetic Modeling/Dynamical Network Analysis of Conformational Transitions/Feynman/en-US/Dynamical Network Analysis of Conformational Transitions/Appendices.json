{
    "hands_on_practices": [
        {
            "introduction": "The first practical step in analyzing conformational dynamics with a network approach is to translate continuous trajectory data into a discrete set of states. This process, known as microstate decomposition, forms the foundation of a Markov State Model (MSM). This exercise will guide you through the fundamental pipeline of constructing a dynamical network from scratch, starting from low-dimensional trajectory data, clustering it into microstates, and estimating the transition probabilities between them .",
            "id": "3408877",
            "problem": "You are given time-ordered configurations in a low-dimensional embedding that mimics diffusion map coordinates of a molecular dynamics trajectory undergoing rare conformational transitions. Your task is to implement, from first principles, a pipeline that constructs microstates by clustering in the leading diffusion coordinates and then builds a directed dynamical network from observed transitions between these microstates. You must produce a single program that generates deterministic synthetic embedded trajectories, performs clustering using the $k$-means algorithm in the leading diffusion coordinates, estimates a discrete-time Markov chain from observed transitions between microstates, and reports specified scalar diagnostics per test case.\n\nFundamental starting points and definitions:\n- A microstate decomposition is a partition of the configuration space into $K$ disjoint clusters. The $k$-means algorithm seeks a partition $\\{C_1,\\dots,C_K\\}$ and centroids $\\{\\mu_1,\\dots,\\mu_K\\}$ that minimize $\\sum_{i=1}^{K} \\sum_{x \\in C_i} \\lVert x - \\mu_i \\rVert^2$, where $\\lVert \\cdot \\rVert$ denotes the Euclidean norm.\n- Given a time-ordered sequence of microstate labels $\\{s_t\\}_{t=0}^{T-1}$, the empirical transition count matrix $N \\in \\mathbb{N}^{K \\times K}$ is defined by $N_{ij} = \\#\\{t \\in \\{0,\\dots,T-2\\} : s_t = i, s_{t+1} = j\\}$. The row-stochastic transition probability matrix $P \\in \\mathbb{R}^{K \\times K}$ is estimated by $P_{ij} = N_{ij} / \\sum_{j'} N_{ij'}$ if $\\sum_{j'} N_{ij'} > 0$ and $P_{ij} = 0$ otherwise.\n- The eigenvalues of $P$ satisfy $\\lambda_1 = 1$ for any row-stochastic $P$. The second-largest eigenvalue modulus (SLEM) is defined as $\\max\\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } P, \\lambda \\neq 1\\}$ when at least two eigenvalues exist; if $K = 1$ so that only one eigenvalue exists, take the SLEM to be $0$ by convention.\n\nData generation for each test case: You will synthesize a deterministic, time-ordered embedded trajectory $X \\in \\mathbb{R}^{T \\times d}$ with $d = 3$ as follows. Define a slow switching signal $s(t)$ with two possible basins (values near $-1$ and $+1$) and at most two ramps:\n- Let $t \\in \\{0, 1, \\dots, T-1\\}$.\n- Initialize $s(t) = -1$ for all $t$.\n- Define an upward ramp starting at $t_{\\text{up}}$ of duration $R$ frames: for $t_{\\text{up}} \\le t < \\min(t_{\\text{up}} + R, T)$, set $s(t) = -1 + 2\\,(t - t_{\\text{up}})/R$; for $t \\ge \\min(t_{\\text{up}} + R, T)$, set $s(t) = +1$ (unless modified by the downward ramp below).\n- Define an optional downward ramp starting at $t_{\\text{down}}$ of duration $R$ frames: for $t_{\\text{down}} \\le t < \\min(t_{\\text{down}} + R, T)$, set $s(t) = +1 - 2\\,(t - t_{\\text{down}})/R$; for $t \\ge \\min(t_{\\text{down}} + R, T)$, set $s(t) = -1$. If $t_{\\text{down}} \\ge T$, then no downward ramp occurs.\n- Define angular frequency $\\omega = 2\\pi/M$ in radians per frame. Construct coordinates\n$$\n\\begin{aligned}\nx_1(t) &= s(t) + a \\sin(\\omega t),\\\\\nx_2(t) &= \\tfrac{1}{2}\\,s(t) + a \\cos(\\omega t),\\\\\nx_3(t) &= \\tfrac{1}{4}\\,\\sin(2 \\omega t).\n\\end{aligned}\n$$\nSet $X(t,:) = (x_1(t), x_2(t), x_3(t))$ for each $t$.\n\nClustering and network construction:\n- Use only the first $q$ coordinates of $X$ (the “leading diffusion coordinates”) for clustering.\n- Standardize these $q$ coordinates to zero mean and unit variance along each coordinate before clustering. If a coordinate has zero variance, leave it unchanged by treating its standard deviation as $1$ to avoid division by zero.\n- Implement $k$-means with $K$ clusters using Lloyd’s algorithm and deterministic farthest-point initialization:\n  1. Choose the first center as the data point with the smallest Euclidean norm.\n  2. For each subsequent center, choose the data point that maximizes the distance to its nearest already chosen center.\n  3. Iterate assignment (by nearest center) and update (centroids as arithmetic means) until labels do not change or a maximum of $M_{\\text{it}}$ iterations is reached. If an empty cluster occurs, reinitialize its center to the data point with the largest distance to its currently assigned center and continue.\n- From the cluster labels in time order, construct the empirical transition count matrix $N$ and the row-stochastic transition matrix $P$ including self-transitions.\n\nRequired outputs per test case:\n- Let $E$ be the number of directed edges with strictly positive transition probability, i.e., the count of entries with $P_{ij} > 0$.\n- Let $\\theta$ be the second-largest eigenvalue modulus (SLEM) of $P$ as defined above.\n- Let $f_{\\max}$ be the fraction of time spent in the most populated microstate, computed as the maximum occupancy count divided by $T$.\n- Report for each test case the list $[E, \\mathrm{round}(\\theta, 6), \\mathrm{round}(f_{\\max}, 6)]$, where rounding is to $6$ decimal places. Angles for trigonometric functions must be computed in radians.\n\nTest suite:\n- Case A: $(T, t_{\\text{up}}, t_{\\text{down}}, R, a, M, q, K) = (300, 80, 200, 20, 0.05, 15, 2, 2)$.\n- Case B (degenerate clustering boundary): $(T, t_{\\text{up}}, t_{\\text{down}}, R, a, M, q, K) = (180, 60, 140, 10, 0.02, 12, 2, 1)$.\n- Case C (one-way transition edge case): $(T, t_{\\text{up}}, t_{\\text{down}}, R, a, M, q, K) = (250, 160, 1000, 20, 0.04, 20, 2, 2)$.\nUse a maximum of $M_{\\text{it}} = 100$ iterations for $k$-means.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a bracketed list of three numbers for a test case, in the same order as listed above. For example, the output format must be exactly like\n$[ [E_A,\\theta_A,f_{\\max,A}],[E_B,\\theta_B,f_{\\max,B}],[E_C,\\theta_C,f_{\\max,C}] ]$\nwith no additional spaces required beyond those shown and no extra text. All quantities are dimensionless, and all trigonometric arguments are in radians.",
            "solution": "The problem requires the implementation of a deterministic computational pipeline to analyze synthetic molecular dynamics data. The pipeline involves synthesizing a trajectory, partitioning it into microstates using $k$-means clustering, constructing a Markovian dynamical network, and calculating specific network diagnostics. The solution is presented step-by-step, adhering to the provided definitions and principles.\n\n### 1. Trajectory Synthesis\nFor each test case, a time-ordered trajectory $X \\in \\mathbb{R}^{T \\times d}$ with dimension $d=3$ and length $T$ frames is generated. The time index is $t \\in \\{0, 1, \\dots, T-1\\}$.\n\nFirst, a slow switching signal $s(t)$ is constructed, which emulates transitions between two metastable basins ($s \\approx -1$ and $s \\approx +1$). The signal is generated as follows:\n1.  Initialize an array $s$ of length $T$ with the value $-1$.\n2.  An upward ramp is introduced starting at time $t_{\\text{up}}$ for a duration of $R$ frames. For time steps $t$ in the range $[t_{\\text{up}}, \\min(t_{\\text{up}} + R, T))$, the signal is linearly interpolated from $-1$ to $+1$:\n    $$s(t) = -1 + 2 \\cdot \\frac{t - t_{\\text{up}}}{R}$$\n    For all subsequent times $t \\ge \\min(t_{\\text{up}} + R, T)$, the signal is set to $s(t) = +1$.\n3.  An optional downward ramp, symmetric to the upward ramp, starts at $t_{\\text{down}}$. If $t_{\\text{down}} < T$, then for $t \\in [t_{\\text{down}}, \\min(t_{\\text{down}} + R, T))$, the signal is linearly interpolated from $+1$ to $-1$:\n    $$s(t) = +1 - 2 \\cdot \\frac{t - t_{\\text{down}}}{R}$$\n    For all subsequent times $t \\ge \\min(t_{\\text{down}} + R, T)$, the signal is set to $s(t) = -1$. This downward ramp and its aftermath overwrite any values set by the upward ramp logic.\n\nWith the slow signal $s(t)$ defined, the three-dimensional coordinates $X(t,:) = (x_1(t), x_2(t), x_3(t))$ are constructed. These coordinates couple the slow transition with faster, periodic local motions, governed by an angular frequency $\\omega = 2\\pi/M$:\n$$\n\\begin{aligned}\nx_1(t) &= s(t) + a \\sin(\\omega t) \\\\\nx_2(t) &= \\tfrac{1}{2}\\,s(t) + a \\cos(\\omega t) \\\\\nx_3(t) &= \\tfrac{1}{4}\\,\\sin(2 \\omega t)\n\\end{aligned}\n$$\nThe parameter $a$ controls the amplitude of the fast fluctuations.\n\n### 2. Microstate Clustering\nThe continuous trajectory is discretized into a sequence of microstates using $k$-means clustering.\n\nFirst, the data for clustering is prepared. Only the first $q$ coordinates of the trajectory $X$ are used, as specified. This sub-matrix $X_{:,:q} \\in \\mathbb{R}^{T \\times q}$ is then standardized. For each of the $q$ coordinates, the mean is shifted to zero and the variance is scaled to one. If a coordinate has zero variance (i.e., its standard deviation is $0$), its standard deviation is treated as $1$ to prevent division by zero, leaving the coordinate unchanged as it is already centered.\n\nNext, $k$-means clustering is applied to partition the $T$ standardized data points into $K$ clusters. The implementation uses Lloyd's algorithm with a deterministic initialization scheme to ensure a reproducible outcome.\n-   **Initialization (Farthest-Point)**:\n    1.  The first centroid is chosen as the data point with the smallest Euclidean norm.\n    2.  For each subsequent centroid $i$ from $2$ to $K$, the data point that maximizes the minimum squared Euclidean distance to all previously chosen centroids ($1, \\dots, i-1$) is selected.\n-   **Iteration (Lloyd's Algorithm)**: The algorithm iterates through two steps until the cluster assignments no longer change or a maximum of $M_{\\text{it}} = 100$ iterations is reached.\n    1.  **Assignment Step**: Each data point is assigned to the cluster corresponding to the nearest centroid, based on Euclidean distance. This yields a time-ordered sequence of cluster labels $\\{z_t\\}_{t=0}^{T-1}$, where $z_t \\in \\{0, 1, \\dots, K-1\\}$.\n    2.  **Update Step**: The centroid of each cluster is re-calculated as the arithmetic mean of all data points assigned to it.\n-   **Empty Cluster Handling**: If a cluster becomes empty during an iteration (i.e., no points are assigned to it), its centroid is re-initialized. The new centroid is chosen to be the data point that has the largest squared Euclidean distance to its own currently assigned centroid. This procedure \"steals\" a poorly-fit point to seed the empty cluster, ensuring all $K$ clusters remain active. This happens within the update step before the next iteration begins.\n\n### 3. Dynamical Network Analysis\nThe time series of microstate labels $\\{z_t\\}$ is used to construct a discrete-time Markov chain, represented by a transition probability matrix $P$.\n\n-   **Transition Count Matrix ($N$)**: A $K \\times K$ matrix $N$ is constructed, where the entry $N_{ij}$ counts the number of observed transitions from microstate $i$ to microstate $j$ at a time lag of one frame:\n    $$N_{ij} = \\#\\{t \\in \\{0, \\dots, T-2\\} : z_t = i, z_{t+1} = j\\}$$\n-   **Transition Probability Matrix ($P$)**: The count matrix $N$ is row-normalized to yield the row-stochastic transition probability matrix $P \\in \\mathbb{R}^{K \\times K}$. The probability of transitioning from state $i$ to state $j$ is estimated as:\n    $$P_{ij} = \\frac{N_{ij}}{\\sum_{k=0}^{K-1} N_{ik}}$$\n    If a state $i$ has no observed outgoing transitions (i.e., $\\sum_{k} N_{ik} = 0$, which can only happen if state $i$ is visited exclusively at the final time step $t=T-1$), its corresponding row $P_{i,:}$ is set to all zeros.\n\nFinally, three scalar diagnostics are computed from this model for each test case:\n1.  **Number of Edges ($E$)**: This is the number of directed edges with positive transition probability in the network, calculated as the count of strictly positive entries in the matrix $P$: $E = \\#\\{(i,j) : P_{ij} > 0\\}$.\n2.  **Second-Largest Eigenvalue Modulus ($\\theta$)**: This is the SLEM of the matrix $P$, defined as $\\theta = \\max\\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } P, \\lambda \\neq 1\\}$. For the case $K=1$, the SLEM is taken to be $0$ by convention. Computationally, this is found by calculating all eigenvalues of $P$, taking their absolute values, sorting them, and selecting the second-largest value. Since $P$ is constructed to be row-stochastic (or sub-stochastic), its largest eigenvalue modulus is guaranteed to be $1$ (if the graph of states is non-trivial and connected).\n3.  **Maximum Fractional Occupancy ($f_{\\max}$)**: This measures the fraction of simulation time spent in the most populated microstate. It is calculated by finding the microstate $k$ with the maximum number of assigned frames, $\\text{count}(k)$, and dividing by the total number of frames $T$:\n    $$f_{\\max} = \\frac{\\max_{k \\in \\{0, \\dots, K-1\\}} \\text{count}(k)}{T}$$\n\nThe calculated values of $\\theta$ and $f_{\\max}$ are rounded to $6$ decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef generate_trajectory(T, t_up, t_down, R, a, M):\n    \"\"\"\n    Generates a deterministic synthetic trajectory based on the problem specification.\n\n    Args:\n        T (int): Total number of frames.\n        t_up (int): Start time of the upward ramp.\n        t_down (int): Start time of the downward ramp.\n        R (int): Duration of the ramps.\n        a (float): Amplitude of fast fluctuations.\n        M (int): Period of fast fluctuations.\n\n    Returns:\n        np.ndarray: The generated trajectory of shape (T, 3).\n    \"\"\"\n    t_vals = np.arange(T)\n    s = np.full(T, -1.0)\n\n    # Upward ramp and subsequent plateau\n    if t_up < T:\n        up_ramp_end = min(t_up + R, T)\n        ramp_indices = np.arange(t_up, up_ramp_end)\n        if len(ramp_indices) > 0:\n            s[ramp_indices] = -1.0 + 2.0 * (ramp_indices - t_up) / R\n        if up_ramp_end < T:\n            s[up_ramp_end:T] = 1.0\n\n    # Downward ramp and subsequent plateau (overwrites previous values)\n    if t_down < T:\n        down_ramp_end = min(t_down + R, T)\n        ramp_indices = np.arange(t_down, down_ramp_end)\n        if len(ramp_indices) > 0:\n            s[ramp_indices] = 1.0 - 2.0 * (ramp_indices - t_down) / R\n        if down_ramp_end < T:\n            s[down_ramp_end:T] = -1.0\n        \n    omega = 2 * np.pi / M\n    x1 = s + a * np.sin(omega * t_vals)\n    x2 = 0.5 * s + a * np.cos(omega * t_vals)\n    x3 = 0.25 * np.sin(2 * omega * t_vals)\n    \n    return np.stack([x1, x2, x3], axis=1)\n\ndef k_means(data, K, M_it):\n    \"\"\"\n    Performs k-means clustering with deterministic farthest-point initialization.\n\n    Args:\n        data (np.ndarray): The data to cluster, shape (n_samples, n_features).\n        K (int): The number of clusters.\n        M_it (int): The maximum number of iterations.\n\n    Returns:\n        np.ndarray: The final cluster labels for each data point.\n    \"\"\"\n    n_samples, n_features = data.shape\n    \n    if K == 1:\n        return np.zeros(n_samples, dtype=int)\n\n    # 1. Initialization: Farthest-point\n    centers = np.zeros((K, n_features))\n    norms_sq = np.sum(data**2, axis=1)\n    first_center_idx = np.argmin(norms_sq)\n    centers[0] = data[first_center_idx]\n    \n    min_dist_sq = np.full(n_samples, np.inf)\n    for i in range(1, K):\n        dist_sq_to_last_center = np.sum((data - centers[i-1])**2, axis=1)\n        min_dist_sq = np.minimum(min_dist_sq, dist_sq_to_last_center)\n        next_center_idx = np.argmax(min_dist_sq)\n        centers[i] = data[next_center_idx]\n\n    # 2. Lloyd's Algorithm\n    labels = -np.ones(n_samples, dtype=int)\n    for _ in range(M_it):\n        # Assignment step\n        dist_sq = np.sum((data[:, np.newaxis, :] - centers)**2, axis=2)\n        new_labels = np.argmin(dist_sq, axis=1)\n\n        # Convergence check\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # Update step\n        new_centers = np.zeros((K, n_features))\n        empty_clusters = []\n        for k in range(K):\n            points_in_cluster = data[labels == k]\n            if len(points_in_cluster) == 0:\n                empty_clusters.append(k)\n            else:\n                new_centers[k] = points_in_cluster.mean(axis=0)\n\n        # Empty cluster handling\n        if empty_clusters:\n            point_distances_sq = np.sum((data - centers[labels])**2, axis=1)\n            farthest_point_indices = np.argsort(point_distances_sq)[::-1]\n            \n            used_farthest_points = set()\n            farthest_idx_ptr = 0\n            for k in empty_clusters:\n                point_to_reseed_idx = farthest_point_indices[farthest_idx_ptr]\n                while point_to_reseed_idx in used_farthest_points:\n                    farthest_idx_ptr += 1\n                    point_to_reseed_idx = farthest_point_indices[farthest_idx_ptr]\n\n                new_centers[k] = data[point_to_reseed_idx]\n                used_farthest_points.add(point_to_reseed_idx)\n                \n        centers = new_centers\n        \n    return labels\n\ndef calculate_diagnostics(labels, K, T):\n    \"\"\"\n    Calculates network diagnostics E, theta, and f_max.\n\n    Args:\n        labels (np.ndarray): Time-ordered cluster labels.\n        K (int): Number of clusters.\n        T (int): Total number of frames.\n\n    Returns:\n        tuple: A tuple containing (E, theta, f_max).\n    \"\"\"\n    if K == 1:\n        return 1, 0.0, 1.0\n\n    N = np.zeros((K, K), dtype=int)\n    for i in range(T - 1):\n        N[labels[i], labels[i+1]] += 1\n\n    P = np.zeros((K, K))\n    row_sums = N.sum(axis=1)\n    non_zero_rows = row_sums > 0\n    P[non_zero_rows] = N[non_zero_rows] / row_sums[non_zero_rows, np.newaxis]\n\n    E = np.count_nonzero(P)\n\n    eigenvalues = np.linalg.eigvals(P)\n    eigenvalue_moduli = np.abs(eigenvalues)\n    eigenvalue_moduli.sort() \n    theta = eigenvalue_moduli[-2] if len(eigenvalue_moduli) >= 2 else 0.0\n\n    if T > 0:\n        occupancies = np.bincount(labels, minlength=K)\n        f_max = np.max(occupancies) / T\n    else:\n        f_max = 0.0\n        \n    return E, theta, f_max\n\ndef solve():\n    \"\"\"\n    Main function to run the full pipeline for all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        (300, 80, 200, 20, 0.05, 15, 2, 2),  # Case A\n        (180, 60, 140, 10, 0.02, 12, 2, 1),  # Case B\n        (250, 160, 1000, 20, 0.04, 20, 2, 2), # Case C\n    ]\n    M_it = 100\n\n    all_results = []\n    for params in test_cases:\n        T, t_up, t_down, R, a, M, q, K = params\n        \n        X = generate_trajectory(T, t_up, t_down, R, a, M)\n        \n        data_for_clustering = X[:, :q]\n        \n        mean = data_for_clustering.mean(axis=0)\n        std = data_for_clustering.std(axis=0)\n        std[std == 0] = 1.0\n        standardized_data = (data_for_clustering - mean) / std\n        \n        labels = k_means(standardized_data, K, M_it)\n        \n        E, theta, f_max = calculate_diagnostics(labels, K, T)\n        \n        result = [E, round(theta, 6), round(f_max, 6)]\n        all_results.append(result)\n\n    result_str = ','.join([str(r).replace(' ', '') for r in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A powerful feature of Markov State Models is their ability to connect with equilibrium thermodynamics, but this requires the model to satisfy the principle of detailed balance. An empirically estimated model may violate this condition due to finite sampling, so it is crucial to verify and, if necessary, enforce this property. This practice explores how to calculate the stationary distribution of a network, test for detailed balance, and apply a standard symmetrization procedure to construct a physically consistent, reversible model .",
            "id": "3408814",
            "problem": "You are given a framework to analyze conformational transitions in Molecular Dynamics using Markov State Models (MSM). In MSM, a discrete-time Markov chain represents transitions between metastable states. Let $T$ denote a row-stochastic transition probability matrix with entries $T_{ij} \\ge 0$ and $\\sum_{j} T_{ij} = 1$ for all states $i$. The stationary distribution $\\pi$ of the chain satisfies the left eigenvector relation $\\pi^{\\top} T = \\pi^{\\top}$ with the normalization $\\sum_{i} \\pi_{i} = 1$. A reversible Markov chain obeys detailed balance: for all $i,j$, $\\pi_{i} T_{ij} = \\pi_{j} T_{ji}$. In practice, from trajectory data, one estimates a count matrix $C$ whose entries $C_{ij}$ record the number of observed transitions from state $i$ to state $j$ in a chosen lag time. Empirical non-reversibility can arise due to finite sampling; a common remedy is to enforce reversibility by symmetrizing counts via $C^{\\mathrm{sym}}_{ij} = C_{ij} + C_{ji}$ and then constructing a reversible transition matrix by row normalization. To mitigate zero-count artifacts, it is often advisable to add a symmetric pseudocount $\\alpha > 0$ to all entries before normalization.\n\nStarting from the fundamental base:\n\n- Discrete-time Markov chain definition: $T$ is row-stochastic, $\\sum_{j} T_{ij} = 1$, with $T_{ij} \\ge 0$.\n- Stationary distribution definition: $\\pi^{\\top} T = \\pi^{\\top}$ and $\\sum_{i} \\pi_{i} = 1$.\n- Detailed balance (reversibility) condition: $\\pi_{i} T_{ij} = \\pi_{j} T_{ji}$ for all $i,j$.\n- Empirical count matrix construction: $C_{ij}$ is the observed count of transitions from $i$ to $j$.\n- Reversible estimator by symmetric counts: $C^{\\mathrm{sym}}_{ij} = C_{ij} + C_{ji}$, optionally with pseudocounts $\\alpha$, and $T^{\\mathrm{rev}}_{ij} = \\frac{C^{\\mathrm{sym}}_{ij} + \\alpha}{\\sum_{k} \\left(C^{\\mathrm{sym}}_{ik} + \\alpha \\right)}$.\n\nYour task is to write a program that, for each test case, performs these steps:\n\n1. Given an estimated transition matrix $T$, compute the stationary distribution $\\pi$ as the normalized left eigenvector with eigenvalue $1$, by solving the linear system implied by $\\pi^{\\top} T = \\pi^{\\top}$ and $\\sum_{i} \\pi_{i} = 1$.\n2. Quantify detailed balance violation by computing the maximum absolute flux asymmetry\n   $$d = \\max_{i,j} \\left| \\pi_{i} T_{ij} - \\pi_{j} T_{ji} \\right|.$$\n3. Decide whether detailed balance holds within a tolerance $\\tau$ by checking if $d \\le \\tau$.\n4. If detailed balance is violated, construct a reversible estimate $T^{\\mathrm{rev}}$ by symmetrizing counts $C$ and adding a symmetric pseudocount $\\alpha$ to all entries, then row-normalize. Compute the stationary distribution $\\pi^{\\mathrm{rev}}$ for $T^{\\mathrm{rev}}$, and recompute the asymmetry $d^{\\mathrm{rev}}$ and the boolean check for detailed balance within the same tolerance $\\tau$.\n5. If detailed balance is not violated, set $\\pi^{\\mathrm{rev}} = \\pi$, $d^{\\mathrm{rev}} = d$, and the detailed balance boolean after symmetrization equal to the pre-symmetrization check.\n\nAll answers are unitless probabilities and counts; there are no physical units involved. Angles are not applicable. Percentages must not be used anywhere; probabilities must be expressed as decimals in $[0,1]$.\n\nTest Suite:\n\nUse the following three test cases, each specified by a tuple $(T, C, \\alpha)$:\n\n- Case 1 (reversible baseline):\n  $$\n  T^{(1)} = \\begin{pmatrix}\n  0 & \\frac{2}{3} & \\frac{1}{3} \\\\\n  \\frac{2}{3} & 0 & \\frac{1}{3} \\\\\n  \\frac{1}{2} & \\frac{1}{2} & 0\n  \\end{pmatrix}, \\quad\n  C^{(1)} = \\begin{pmatrix}\n  0 & 2 & 1 \\\\\n  3 & 0 & 1 \\\\\n  1 & 0 & 0\n  \\end{pmatrix}, \\quad\n  \\alpha^{(1)} = 0.\n  $$\n- Case 2 (non-reversible due to asymmetric counts and transitions):\n  $$\n  T^{(2)} = \\begin{pmatrix}\n  0 & 0.9 & 0.1 \\\\\n  0.3 & 0 & 0.7 \\\\\n  0.2 & 0.8 & 0\n  \\end{pmatrix}, \\quad\n  C^{(2)} = \\begin{pmatrix}\n  0 & 90 & 10 \\\\\n  30 & 0 & 70 \\\\\n  20 & 80 & 0\n  \\end{pmatrix}, \\quad\n  \\alpha^{(2)} = 0.\n  $$\n- Case 3 (edge case with zero counts; pseudocount regularization):\n  $$\n  T^{(3)} = \\begin{pmatrix}\n  0 & 1 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 1 & 0\n  \\end{pmatrix}, \\quad\n  C^{(3)} = \\begin{pmatrix}\n  0 & 0 & 0 \\\\\n  0 & 0 & 0 \\\\\n  0 & 0 & 0\n  \\end{pmatrix}, \\quad\n  \\alpha^{(3)} = 1.\n  $$\n\nUse tolerance $\\tau = 10^{-12}$ for the detailed balance check.\n\nOutput specification:\n\nFor each test case, your program must output a list containing six elements in the following order:\n- The stationary distribution before symmetrization as a list of floats, each rounded to $6$ decimal places, in the order of states $[1,2,3]$.\n- A boolean indicating whether detailed balance holds before symmetrization using tolerance $\\tau$.\n- The float $d$ before symmetrization.\n- The stationary distribution after symmetrization as a list of floats, each rounded to $6$ decimal places.\n- A boolean indicating whether detailed balance holds after symmetrization using tolerance $\\tau$.\n- The float $d^{\\mathrm{rev}}$ after symmetrization.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result_case_1,result_case_2,result_case_3]\"), where each result_case_k follows the specified six-element list format.",
            "solution": "The user has provided a problem that requires the implementation of an analysis pipeline for Markov State Models (MSMs) as used in molecular dynamics. I will first validate the problem statement and then provide a detailed solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Model:** Discrete-time Markov chain for conformational transitions.\n-   **Transition Matrix $T$:** Row-stochastic ($T_{ij} \\ge 0$, $\\sum_{j} T_{ij} = 1$).\n-   **Stationary Distribution $\\pi$:** Defined by $\\pi^{\\top} T = \\pi^{\\top}$ and $\\sum_{i} \\pi_{i} = 1$.\n-   **Detailed Balance (Reversibility):** $\\pi_{i} T_{ij} = \\pi_{j} T_{ji}$ for all $i,j$.\n-   **Count Matrix $C$:** $C_{ij}$ is the observed number of transitions from state $i$ to $j$.\n-   **Reversible Estimator $T^{\\mathrm{rev}}$:** Constructed from a symmetrized count matrix with an optional pseudocount $\\alpha$: $T^{\\mathrm{rev}}_{ij} = \\frac{C_{ij} + C_{ji} + \\alpha}{\\sum_{k} (C_{ik} + C_{ki} + \\alpha)}$.\n-   **Task Steps:**\n    1.  For a given $T$, compute its stationary distribution $\\pi$.\n    2.  Compute the maximum absolute flux asymmetry $d = \\max_{i,j} \\left| \\pi_{i} T_{ij} - \\pi_{j} T_{ji} \\right|$.\n    3.  Check if detailed balance holds: $d \\le \\tau$.\n    4.  If violated, use the given $C$ and $\\alpha$ to construct $T^{\\mathrm{rev}}$, then re-calculate its stationary distribution $\\pi^{\\mathrm{rev}}$ and asymmetry $d^{\\mathrm{rev}}$.\n    5.  If not violated, copy the initial results ($\\pi, d$) to the \"after symmetrization\" results.\n-   **Tolerance:** $\\tau = 10^{-12}$.\n-   **Test Cases:** Three cases are provided, each with a transition matrix $T$, a count matrix $C$, and a pseudocount $\\alpha$.\n    -   Case 1: $T^{(1)} = \\begin{pmatrix} 0 & \\frac{2}{3} & \\frac{1}{3} \\\\ \\frac{2}{3} & 0 & \\frac{1}{3} \\\\ \\frac{1}{2} & \\frac{1}{2} & 0 \\end{pmatrix}$, $C^{(1)} = \\begin{pmatrix} 0 & 2 & 1 \\\\ 3 & 0 & 1 \\\\ 1 & 0 & 0 \\end{pmatrix}$, $\\alpha^{(1)} = 0$.\n    -   Case 2: $T^{(2)} = \\begin{pmatrix} 0 & 0.9 & 0.1 \\\\ 0.3 & 0 & 0.7 \\\\ 0.2 & 0.8 & 0 \\end{pmatrix}$, $C^{(2)} = \\begin{pmatrix} 0 & 90 & 10 \\\\ 30 & 0 & 70 \\\\ 20 & 80 & 0 \\end{pmatrix}$, $\\alpha^{(2)} = 0$.\n    -   Case 3: $T^{(3)} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}$, $C^{(3)} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}$, $\\alpha^{(3)} = 1$.\n-   **Output:** A single-line string representing a list of lists. Each inner list contains 6 items for one test case: $[\\pi_{\\text{before}}, \\text{db}_{\\text{before}}, d_{\\text{before}}, \\pi_{\\text{after}}, \\text{db}_{\\text{after}}, d_{\\text{after}}]$.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded:** The problem is firmly based on the established mathematical theory of Markov chains and their application in Markov State Models, a standard methodology in computational chemistry and biophysics. All definitions and procedures are standard and correct.\n-   **Well-Posed:** The problem is well-posed. The task is to implement a deterministic algorithm. The computation of a stationary distribution for a finite-state Markov chain is a well-defined linear algebra problem. The existence and uniqueness of the stationary distribution are guaranteed for the types of systems typically modeled (irreducible, aperiodic chains), and the provided matrices, while some are reducible, are handled correctly by the specified linear algebra approach.\n-   **Objective:** The problem statement is precise, quantitative, and free of ambiguity or subjectivity. All terms are mathematically defined.\n-   **Consistency:** The problem is internally consistent. The transition matrix $T$ and count matrix $C$ are provided as separate inputs for the \"before\" and \"after\" stages of the analysis. This is a realistic scenario where one might start with an initial, possibly non-reversible, estimate of $T$ and then use the underlying raw counts $C$ to produce a corrected, reversible estimate. Thus, the fact that $T$ is not always derivable from $C$ in the test cases is not a contradiction.\n\n**Step 3: Verdict and Action**\n\nThe problem is scientifically sound, well-posed, objective, and internally consistent. It is a valid problem. I will proceed with the solution.\n\n### Algorithmic Solution\n\nThe solution will systematically implement the steps outlined in the problem description for each test case.\n\n**1. Computation of the Stationary Distribution ($\\pi$)**\n\nThe stationary distribution $\\pi$ is the normalized left eigenvector of the transition matrix $T$ corresponding to the eigenvalue $\\lambda=1$. It satisfies two conditions:\n$$ \\pi^{\\top} T = \\pi^{\\top} \\quad \\text{and} \\quad \\sum_{i=1}^{N} \\pi_i = 1 $$\nwhere $N$ is the number of states. The first condition can be rewritten as a homogeneous system of linear equations:\n$$ \\pi^{\\top} (T - I) = \\mathbf{0}^{\\top} $$\nwhere $I$ is the identity matrix and $\\mathbf{0}$ is the zero vector. Transposing this equation gives:\n$$ (T^{\\top} - I) \\pi = \\mathbf{0} $$\nThis system has a non-trivial solution because $\\lambda=1$ is an eigenvalue of $T$, which implies that $\\det(T^{\\top} - I) = 0$. To find the unique solution for $\\pi$ that represents a probability distribution, we must incorporate the normalization constraint $\\sum_{i} \\pi_i = 1$. We can achieve this by replacing one of the (linearly dependent) equations in the system with the normalization equation. For an $N \\times N$ system, we construct a new matrix $A'$ by taking the first $N-1$ rows of $(T^{\\top} - I)$ and setting the last row to all ones. We also construct a new vector $b'$ which is zero for the first $N-1$ elements and one for the last element. The unique stationary distribution $\\pi$ is then the solution to the non-singular system:\n$$ A' \\pi = b' $$\nThis system can be solved using a standard linear algebra solver.\n\n**2. Detailed Balance Violation Analysis**\n\nWith the stationary distribution $\\pi$ and the transition matrix $T$, we quantify the violation of detailed balance. The detailed balance condition, $\\pi_i T_{ij} = \\pi_j T_{ji}$, states that the probability flux from state $i$ to $j$ must equal the flux from $j$ to $i$ in equilibrium. The net flux, or flux asymmetry, between any two states is $F_{ij} = \\pi_i T_{ij} - \\pi_j T_{ji}$. We compute the maximum absolute flux asymmetry over all pairs of states:\n$$ d = \\max_{i,j} | \\pi_i T_{ij} - \\pi_j T_{ji} | $$\nThis value $d$ serves as a metric for the degree of non-reversibility. The system is considered to satisfy detailed balance if this deviation is within a given numerical tolerance, i.e., $d \\le \\tau$.\n\n**3. Construction of the Reversible Estimator ($T^{\\mathrm{rev}}$)**\n\nIf the initial matrix $T$ is found to violate detailed balance ($d > \\tau$), a new, reversible transition matrix $T^{\\mathrm{rev}}$ is constructed based on the raw transition counts $C$ and the pseudocount $\\alpha$.\nFirst, the count matrix is symmetrized to remove directional bias from finite sampling:\n$$ C^{\\mathrm{sym}}_{ij} = C_{ij} + C_{ji} $$\nThis ensures that the total number of observed transitions between any pair of states $(i,j)$ is the same in both directions. To handle cases with zero counts, which can lead to zero probabilities and disconnected states, a small positive pseudocount $\\alpha$ is added to every entry:\n$$ C'_{ij} = C^{\\mathrm{sym}}_{ij} + \\alpha $$\nThe reversible transition matrix $T^{\\mathrm{rev}}$ is then obtained by row-normalizing this adjusted count matrix:\n$$ T^{\\mathrm{rev}}_{ij} = \\frac{C'_{ij}}{\\sum_k C'_{ik}} $$\nBy construction, this matrix $T^{\\mathrm{rev}}$ is guaranteed to satisfy the detailed balance condition with respect to a stationary distribution $\\pi^{\\mathrm{rev}}$ whose components are proportional to the row sums of $C'$ (i.e., $\\pi^{\\mathrm{rev}}_i \\propto \\sum_k C'_{ik}$). After constructing $T^{\\mathrm{rev}}$, its stationary distribution $\\pi^{\\mathrm{rev}}$ and asymmetry $d^{\\mathrm{rev}}$ are computed using the same methods described in steps 1 and 2. Due to the construction, $d^{\\mathrm{rev}}$ is expected to be zero (or a very small number due to floating-point precision).\n\nIf the initial matrix $T$ already satisfies detailed balance ($d \\le \\tau$), this entire construction procedure is skipped. The \"after\" results ($\\pi^{\\mathrm{rev}}$, $d^{\\mathrm{rev}}$) are simply set to be identical to the \"before\" results ($\\pi, d$). This logic will be applied to each test case to generate the required output.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve as solve_linear_system\n\ndef analyze_msm(T, C, alpha, tau):\n    \"\"\"\n    Performs MSM analysis for a given test case.\n    \n    Args:\n        T (np.ndarray): The initial transition probability matrix.\n        C (np.ndarray): The transition count matrix.\n        alpha (float): The pseudocount value.\n        tau (float): The tolerance for detailed balance check.\n        \n    Returns:\n        list: A 6-element list containing the results.\n    \"\"\"\n    \n    # 1. Compute stationary distribution for the initial matrix T\n    def compute_stationary_dist(T_matrix):\n        \"\"\"Computes the stationary distribution of a transition matrix.\"\"\"\n        n_states = T_matrix.shape[0]\n        # System is (T.T - I) * pi = 0\n        A = T_matrix.T - np.identity(n_states)\n        # Replace last row with normalization constraint sum(pi) = 1\n        A[-1, :] = 1.0\n        b = np.zeros(n_states)\n        b[-1] = 1.0\n        pi = solve_linear_system(A, b, assume_a='gen')\n        # Ensure positivity and re-normalize due to potential floating point errors\n        pi[pi < 0] = 0\n        return pi / np.sum(pi)\n\n    pi = compute_stationary_dist(T)\n    \n    # 2. Quantify detailed balance violation\n    flux_matrix = np.outer(pi, np.ones_like(pi)) * T - (np.outer(pi, np.ones_like(pi)) * T).T\n    d = np.max(np.abs(flux_matrix))\n    \n    # 3. Decide if detailed balance holds\n    is_db = d <= tau\n    \n    pi_rounded = [round(p, 6) for p in pi]\n    \n    # 4. If detailed balance is violated, construct reversible estimate\n    if not is_db:\n        # Symmetrize counts\n        C_sym = C + C.T\n        # Add pseudocount\n        C_prime = C_sym + alpha\n        # Row-normalize to get T_rev\n        row_sums = C_prime.sum(axis=1)\n        # Handle case where a row sum could be zero if alpha=0 and counts are all zero for a row\n        if np.any(row_sums == 0):\n             # This case shouldn't happen with the test data, but is a safeguard.\n             # A row of zeros means it's an absorbing state with no way out.\n             # Cannot form a valid stochastic matrix. Let's make it uniform to avoid crash.\n             n_s = C_prime.shape[0]\n             T_rev = np.full((n_s, n_s), 1.0/n_s)\n        else:\n             T_rev = C_prime / row_sums[:, np.newaxis]\n        \n        # Compute stationary distribution and asymmetry for T_rev\n        pi_rev = compute_stationary_dist(T_rev)\n        flux_matrix_rev = np.outer(pi_rev, np.ones_like(pi_rev)) * T_rev - (np.outer(pi_rev, np.ones_like(pi_rev)) * T_rev).T\n        d_rev = np.max(np.abs(flux_matrix_rev))\n        is_db_rev = d_rev <= tau\n        pi_rev_rounded = [round(p, 6) for p in pi_rev]\n    else:\n        # 5. If detailed balance holds, copy results\n        pi_rev_rounded = pi_rounded\n        d_rev = d\n        is_db_rev = is_db\n        \n    return [pi_rounded, is_db, d, pi_rev_rounded, is_db_rev, d_rev]\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (\n            np.array([[0, 2/3, 1/3], [2/3, 0, 1/3], [1/2, 1/2, 0]]),\n            np.array([[0, 2, 1], [3, 0, 1], [1, 0, 0]]),\n            0.0\n        ),\n        (\n            np.array([[0, 0.9, 0.1], [0.3, 0, 0.7], [0.2, 0.8, 0]]),\n            np.array([[0, 90, 10], [30, 0, 70], [20, 80, 0]]),\n            0.0\n        ),\n        (\n            np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]]),\n            np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]]),\n            1.0\n        )\n    ]\n    \n    tau = 1e-12\n    \n    results = []\n    for T, C, alpha in test_cases:\n        result_case = analyze_msm(T, C, alpha, tau)\n        results.append(result_case)\n\n    # The required output is a single line string representing a list of lists.\n    # The template 'f\"[{','.join(map(str, results))}]\"' correctly formats this.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Scientific conclusions drawn from dynamical networks are only as reliable as the data they are built upon. This advanced practice moves beyond simple point estimates of network properties by introducing a Bayesian framework to handle statistical uncertainty. You will learn to propagate uncertainty from transition counts to a meaningful node centrality metric, allowing for the statistically robust identification of key \"hotspot\" states that play a critical role in the system's overall dynamics .",
            "id": "3408823",
            "problem": "You are given a discrete-state Markov State Model (MSM) arising from molecular dynamics simulations of conformational transitions. The MSM is a discrete-time Markov chain on a finite set of nodes, each node representing a metastable conformational state (or a coarse-grained residue-level community relevant for allostery). Empirical transition counts are available, and the transition probabilities for each row are modeled with independent Dirichlet priors. Your task is to propagate Bayesian uncertainty in the transition probabilities into uncertainty in node centralities relevant to dynamical network analysis, and to identify statistically robust allosteric hotspots under a decision rule defined below.\n\nFrom first principles, use the following fundamental bases:\n- The definition of a discrete-time Markov chain with a row-stochastic transition matrix $P$ on $n$ states.\n- The conjugacy of the Dirichlet prior to the multinomial likelihood for each row of $P$ given count data.\n- The definition of the stationary distribution $\\pi$ of an irreducible, aperiodic Markov chain, satisfying $\\pi^{\\top} P = \\pi^{\\top}$ and $\\sum_{i=1}^{n} \\pi_i = 1$.\n- The definition of mean first passage times (MFPTs) and their relation to the fundamental matrix of a finite irreducible Markov chain.\n\nDefine the centrality of a node $j$ as the inverse of the stationary-averaged mean first passage time to $j$, that is, the reciprocal of the expected hitting time of node $j$ when the starting node is distributed according to the stationary distribution. Conceptually: sample $X_0 \\sim \\pi$, then define $T_j = \\min\\{ t \\ge 0 : X_t = j \\}$, and define the centrality $C_j = 1 / \\mathbb{E}[T_j]$. You must derive an implementable expression for $C_j$ in terms of $P$ and $\\pi$ from the fundamental bases above, without using any black-box centrality formulas.\n\nUncertainty propagation requirement:\n- Let $C$ be an $n \\times n$ matrix of nonnegative integers where $C_{ij}$ is the observed number of transitions from state $i$ to state $j$ in a long trajectory, and let $\\alpha > 0$ be a scalar pseudo-count prior. For each row $i$, let the prior be $\\mathrm{Dirichlet}(\\alpha, \\alpha, \\dots, \\alpha)$ on the $n$-dimensional probability simplex, independent across rows. Use the multinomial-Dirichlet conjugacy to obtain the posterior for each row $i$ as $\\mathrm{Dirichlet}(\\alpha + C_{i1}, \\dots, \\alpha + C_{in})$. \n- To propagate uncertainty, generate $S$ independent samples of the full transition matrix $P$ by independently drawing each row $i$ from its Dirichlet posterior.\n\nDecision rule for statistically robust allosteric hotspots:\n- For each sampled transition matrix, compute the vector of node centralities $(C_1, \\dots, C_n)$ as defined above. Rank nodes by descending centrality; to break ties deterministically, use the lexicographic rule that if two nodes $a$ and $b$ have identical centralities in floating-point up to equality, prefer the smaller index. Extract the set of the top-$k$ nodes from this ordering. \n- Across $S$ posterior samples, compute for each node $j$ the empirical frequency $f_j$ that it appears in the top-$k$ set. A node is declared a statistically robust allosteric hotspot if $f_j \\ge \\tau$, where $\\tau \\in (0,1)$ is a credibility threshold.\n- Report the set of robust hotspot indices for each test case as a list of $0$-indexed integers sorted in ascending order.\n\nImplement a program that performs the above steps and produces the required output for the following test suite. You must not hard-code any results; compute them by implementing the described procedure. All random sampling must be reproducible using the provided seeds.\n\nTest Suite:\n- Case A (general connectivity, two basins):\n  - Counts matrix $C^{(A)} \\in \\mathbb{N}_0^{5 \\times 5}$:\n    $$\n    C^{(A)} =\n    \\begin{bmatrix}\n    0 & 30 & 5 & 0 & 0 \\\\\n    20 & 0 & 10 & 0 & 0 \\\\\n    3 & 12 & 0 & 15 & 0 \\\\\n    0 & 0 & 8 & 0 & 22 \\\\\n    0 & 0 & 0 & 18 & 0\n    \\end{bmatrix}.\n    $$\n  - Prior pseudo-count $\\alpha^{(A)} = 0.5$.\n  - Number of posterior samples $S^{(A)} = 2000$.\n  - Top-$k^{(A)} = 2$.\n  - Credibility threshold $\\tau^{(A)} = 0.95$.\n  - Pseudorandom seed $s^{(A)} = 42$.\n\n- Case B (sparse and nearly deterministic transitions):\n  - Counts matrix $C^{(B)} \\in \\mathbb{N}_0^{4 \\times 4}$:\n    $$\n    C^{(B)} =\n    \\begin{bmatrix}\n    0 & 1 & 0 & 0 \\\\\n    0 & 0 & 50 & 0 \\\\\n    0 & 0 & 0 & 1 \\\\\n    25 & 0 & 0 & 0\n    \\end{bmatrix}.\n    $$\n  - Prior pseudo-count $\\alpha^{(B)} = 0.1$.\n  - Number of posterior samples $S^{(B)} = 3000$.\n  - Top-$k^{(B)} = 1$.\n  - Credibility threshold $\\tau^{(B)} = 0.90$.\n  - Pseudorandom seed $s^{(B)} = 123$.\n\n- Case C (symmetric transitions):\n  - Counts matrix $C^{(C)} \\in \\mathbb{N}_0^{3 \\times 3}$:\n    $$\n    C^{(C)} =\n    \\begin{bmatrix}\n    0 & 10 & 10 \\\\\n    10 & 0 & 10 \\\\\n    10 & 10 & 0\n    \\end{bmatrix}.\n    $$\n  - Prior pseudo-count $\\alpha^{(C)} = 1.0$.\n  - Number of posterior samples $S^{(C)} = 1000$.\n  - Top-$k^{(C)} = 1$.\n  - Credibility threshold $\\tau^{(C)} = 0.80$.\n  - Pseudorandom seed $s^{(C)} = 7$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element is itself a list of $0$-indexed integers corresponding to the robust hotspot indices for one test case, ordered as Case A, Case B, Case C. For example, a syntactically valid output with placeholder values would be of the form $[ [0,2], [1], [] ]$. Your program must print exactly one line in this format.\n\nAngle units and physical units are not applicable in this problem. All probabilities are unitless real numbers in $[0,1]$, and all counts are nonnegative integers. Ensure that any stochastic computations are controlled by the provided seeds for reproducibility and that numerical computations are stable for small matrices.",
            "solution": "The problem requires the identification of statistically robust allosteric hotspots in a discrete-state Markov State Model (MSM) by propagating Bayesian uncertainty from transition probabilities to a specially defined node centrality measure. The solution involves three main parts: first, a derivation of the centrality measure from fundamental principles of Markov chain theory; second, an outline of the Bayesian uncertainty propagation procedure; and third, the specification of the algorithm to identify robust hotspots based on the given decision rule.\n\n### 1. Derivation of the Centrality Measure\n\nThe centrality of a node $j$, denoted $C_j$, is defined as the reciprocal of the stationary-averaged mean first passage time to $j$. Let the discrete-time Markov chain have $n$ states, a row-stochastic transition matrix $P$, and a unique stationary distribution $\\pi$ satisfying $\\pi^{\\top}P = \\pi^{\\top}$ and $\\sum_{i=0}^{n-1} \\pi_i = 1$. The hitting time for state $j$ is $T_j = \\min\\{ t \\ge 0 : X_t = j \\}$. The centrality is:\n\n$$ C_j = \\frac{1}{\\mathbb{E}_{\\pi}[T_j]} $$\n\nThe expectation $\\mathbb{E}_{\\pi}[T_j]$ is taken over initial states $X_0$ drawn from the stationary distribution $\\pi$. It can be expanded as:\n\n$$ \\mathbb{E}_{\\pi}[T_j] = \\sum_{i=0}^{n-1} \\pi_i \\mathbb{E}[T_j | X_0 = i] = \\sum_{i=0}^{n-1} \\pi_i m_{ij} $$\n\nwhere $m_{ij}$ is the Mean First Passage Time (MFPT) from state $i$ to state $j$. By the problem's definition of $T_j$, if the process starts in state $j$ (i.e., $i=j$), the hitting time is $0$. Thus, $m_{jj} = \\mathbb{E}[T_j | X_0 = j] = 0$.\n\nTo find the MFPTs $m_{ij}$ for $i \\neq j$, we use the fundamental matrix method. Consider a modified Markov chain where state $j$ is made an absorbing state. The other states $\\{0, 1, \\dots, n-1\\} \\setminus \\{j\\}$ become transient. Let $P_j$ be the $(n-1) \\times (n-1)$ submatrix of $P$ corresponding to these transient states. For any starting state $i \\neq j$, the MFPT $m_{ij}$ is the expected time to be absorbed into state $j$. The MFPTs from the transient states satisfy the linear system:\n\n$$ m_{ij} = 1 + \\sum_{k \\neq j} P_{ik} m_{kj} \\quad \\text{for } i \\neq j $$\n\nIn vector form, let $\\vec{m}_j$ be the column vector of MFPTs to state $j$ from all states $i \\neq j$, and $\\mathbf{1}$ be a column vector of ones. The system is $\\vec{m}_j = \\mathbf{1} + P_j \\vec{m}_j$. Rearranging gives $(I - P_j)\\vec{m}_j = \\mathbf{1}$, where $I$ is the $(n-1) \\times (n-1)$ identity matrix.\n\nThe matrix $N_j = (I - P_j)^{-1}$ is known as the fundamental matrix for this absorbing chain. Its existence is guaranteed for an irreducible original chain, as the spectral radius of the sub-stochastic matrix $P_j$ is less than $1$. The solution for the MFPTs is thus:\n\n$$ \\vec{m}_j = (I - P_j)^{-1} \\mathbf{1} = N_j \\mathbf{1} $$\n\nThis means that for $i \\neq j$, $m_{ij}$ is the sum of the entries in the row of $N_j$ corresponding to state $i$.\n\nCombining these results, the stationary-averaged MFPT to state $j$, which we denote $H_j$, is:\n$$ H_j = \\mathbb{E}_{\\pi}[T_j] = \\sum_{i \\neq j} \\pi_i m_{ij} $$\nAnd the centrality is $C_j = 1/H_j$. For an irreducible chain, $\\pi_i > 0$ for all $i$ and $m_{ij} > 0$ for $i \\neq j$, ensuring $H_j > 0$.\n\n### 2. Bayesian Uncertainty Propagation\n\nThe transition probabilities are uncertain. Given an empirical transition count matrix $C$ and a scalar Dirichlet prior pseudo-count $\\alpha > 0$, the posterior distribution for each row $i$ of the transition matrix $P$ is independent and follows a Dirichlet distribution:\n\n$$ P_{i,:} | C_{i,:} \\sim \\mathrm{Dirichlet}(\\alpha + C_{i,0}, \\alpha + C_{i,1}, \\dots, \\alpha + C_{i,n-1}) $$\n\nTo propagate this uncertainty to the centrality measures, we employ a Monte Carlo sampling approach:\n1. Generate $S$ independent samples of the transition matrix, $\\{P^{(1)}, P^{(2)}, \\dots, P^{(S)}\\}$. Each matrix $P^{(s)}$ is constructed by independently drawing its rows $P^{(s)}_{i,:}$ from their respective posterior Dirichlet distributions.\n2. For each sampled matrix $P^{(s)}$, compute the full vector of centralities $(C_0^{(s)}, \\dots, C_{n-1}^{(s)})$ using the derivation from Section 1.\n\n### 3. Algorithmic Procedure and Decision Rule\n\nThe complete algorithm to identify robust allosteric hotspots for a given test case is as follows:\n\n1.  **Initialization**: Set the random number generator seed for reproducibility. Initialize a counter array of size $n$ to zeros, which will store the frequency of each node appearing in the top-$k$ set.\n2.  **Posterior Sampling Loop**: Iterate $S$ times:\n    a. **Sample $P$**: Construct a transition matrix $P$ by drawing each row from its posterior Dirichlet distribution.\n    b. **Compute Stationary Distribution $\\pi$**: Numerically find the stationary distribution of $P$. This is achieved by finding the right eigenvector of $P^{\\top}$ corresponding to the eigenvalue $1$. The eigenvector is then normalized to sum to $1$.\n    c. **Compute MFPTs**: For each target state $j \\in \\{0, \\dots, n-1\\}$, construct the submatrix $P_j$, compute the fundamental matrix $N_j=(I-P_j)^{-1}$, and calculate the MFPTs $m_{ij}$ for all $i \\neq j$.\n    d. **Compute Centralities**: Calculate the stationary-averaged MFPTs $H_j = \\sum_{i \\neq j} \\pi_i m_{ij}$, and then the centralities $C_j = 1/H_j$.\n    e. **Rank Nodes**: Rank the nodes in descending order of their centrality values. To break ties, the node with the smaller index is preferred. This is implemented by sorting based on the tuple $(-C_j, j)$.\n    f. **Update Counters**: Identify the set of top-$k$ nodes from the ranking and increment the corresponding counters.\n3.  **Identify Hotspots**: After the loop, compute the empirical frequency $f_j$ for each node $j$ by dividing its count by $S$. A node $j$ is declared a robust hotspot if its frequency $f_j$ meets or exceeds the given credibility threshold $\\tau$.\n4.  **Final Output**: Collect the 0-indexed indices of all robust hotspots and present them in a sorted list. This procedure is repeated for each test case provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    It iterates through each case, performs the Bayesian analysis of node centrality,\n    and prints the final results in the specified format.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"C\": np.array([\n                [0, 30, 5, 0, 0],\n                [20, 0, 10, 0, 0],\n                [3, 12, 0, 15, 0],\n                [0, 0, 8, 0, 22],\n                [0, 0, 0, 18, 0]\n            ]),\n            \"alpha\": 0.5, \"S\": 2000, \"k\": 2, \"tau\": 0.95, \"seed\": 42\n        },\n        {\n            \"C\": np.array([\n                [0, 1, 0, 0],\n                [0, 0, 50, 0],\n                [0, 0, 0, 1],\n                [25, 0, 0, 0]\n            ]),\n            \"alpha\": 0.1, \"S\": 3000, \"k\": 1, \"tau\": 0.90, \"seed\": 123\n        },\n        {\n            \"C\": np.array([\n                [0, 10, 10],\n                [10, 0, 10],\n                [10, 10, 0]\n            ]),\n            \"alpha\": 1.0, \"S\": 1000, \"k\": 1, \"tau\": 0.80, \"seed\": 7\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        hotspots = analyze_case(\n            case[\"C\"], case[\"alpha\"], case[\"S\"], case[\"k\"], case[\"tau\"], case[\"seed\"]\n        )\n        results.append(hotspots)\n\n    # Format the final output string exactly as specified.\n    # str([0, 2]) -> '[0, 2]'. The join creates '[0, 2],[1],[]'.\n    # The f-string adds the outer brackets making '[[0, 2],[1],[]]'.\n    final_output_string = f\"[{','.join(map(str, results))}]\"\n    print(final_output_string)\n\ndef get_stationary_distribution(P):\n    \"\"\"\n    Computes the stationary distribution of a Markov chain given its transition matrix P.\n    It corresponds to the left eigenvector for the eigenvalue 1.\n    \"\"\"\n    # Solve for the right eigenvector of P.T for the eigenvalue 1.\n    eigvals, eigvecs = linalg.eig(P.T)\n    # Find the eigenvector corresponding to the eigenvalue closest to 1.0.\n    idx = np.argmin(np.abs(eigvals - 1.0))\n    pi_vec = np.real(eigvecs[:, idx])\n    # Normalize to obtain a probability distribution.\n    pi_vec /= np.sum(pi_vec)\n    return pi_vec\n\ndef get_centralities(P, pi):\n    \"\"\"\n    Computes the node centralities for a given transition matrix P and stationary distribution pi.\n    \"\"\"\n    n = P.shape[0]\n    mfpt_matrix = np.zeros((n, n))\n    all_indices = np.arange(n)\n\n    for j in range(n):\n        # Define transient states for the absorbing chain with absorbing state j.\n        transient_indices = np.delete(all_indices, j)\n        \n        # Extract the submatrix P_j for transient states.\n        P_j = P[np.ix_(transient_indices, transient_indices)]\n        \n        # Compute the fundamental matrix N_j = (I - P_j)^-1.\n        I_sub = np.identity(n - 1)\n        try:\n            N_j = linalg.inv(I_sub - P_j)\n        except linalg.LinAlgError:\n            # This should be rare given the problem setup but is good practice.\n            return np.full(n, -1.0) # Indicate failure\n\n        # MFPTs to state j are the row sums of the fundamental matrix.\n        mfpts_to_j = N_j.sum(axis=1)\n        \n        # Populate the MFPT matrix. m_{jj} is 0.\n        mfpt_matrix[transient_indices, j] = mfpts_to_j\n    \n    # Calculate stationary-averaged MFPT H_j = sum_i(pi_i * m_ij).\n    # This is equivalent to M.T @ pi for a column vector pi.\n    H = mfpt_matrix.T @ pi\n    \n    # Centrality C_j = 1 / H_j. Add a small epsilon to avoid division by zero.\n    centralities = 1.0 / (H + 1e-12)\n    return centralities\n\ndef analyze_case(C, alpha, S, k, tau, seed):\n    \"\"\"\n    Performs the full analysis for a single test case.\n    \"\"\"\n    n = C.shape[0]\n    rng = np.random.default_rng(seed)\n    \n    # Pre-compute posterior Dirichlet parameters.\n    posterior_params = C + alpha\n    \n    top_k_counts = np.zeros(n, dtype=int)\n    \n    for _ in range(S):\n        # 1. Sample a transition matrix P from the posterior.\n        P = np.array([rng.dirichlet(params) for params in posterior_params])\n        \n        # 2. Compute stationary distribution.\n        pi = get_stationary_distribution(P)\n        \n        # 3. Compute centralities.\n        centralities = get_centralities(P, pi)\n        if np.any(centralities < 0): continue # Skip if computation failed.\n        \n        # 4. Rank nodes by (-centrality, index) and find top-k.\n        node_indices = np.arange(n)\n        sorted_indices = sorted(node_indices, key=lambda i: (-centralities[i], i))\n        top_k_nodes = sorted_indices[:k]\n        \n        # 5. Update counts for nodes in the top-k set.\n        top_k_counts[top_k_nodes] += 1\n        \n    # 6. Identify robust hotspots based on the frequency threshold tau.\n    frequencies = top_k_counts / S\n    robust_hotspots = np.where(frequencies >= tau)[0]\n    \n    return sorted(robust_hotspots.tolist())\n\n# The problem requires the script to be runnable \"as is\", so call solve().\nsolve()\n```"
        }
    ]
}