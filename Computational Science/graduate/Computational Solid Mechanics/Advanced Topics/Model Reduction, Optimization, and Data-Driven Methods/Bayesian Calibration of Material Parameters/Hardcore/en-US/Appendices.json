{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of Bayesian modeling is the assignment of prior distributions that reflect existing knowledge about the parameters. In mechanics, many material parameters like Young's modulus $E$ or bulk modulus $K$ are physically constrained to be strictly positive. This exercise  demonstrates a crucial technique for handling such constraints: reparameterization. By working through the transformation of a Gaussian prior on a latent, unconstrained parameter (e.g., $\\eta = \\ln(E)$) to the corresponding log-normal prior on the physical, positive parameter, you will master the change of variables formula for probability densities, an essential skill for constructing valid and effective priors.",
            "id": "3547099",
            "problem": "In computational solid mechanics, consider Bayesian calibration of an isotropic linear elastic material's Young's modulus, denoted by $E$, where $E$ is strictly positive due to physical constraints. To enforce positivity and stabilize inference, introduce the logarithmic reparameterization $\\eta = \\ln(E)$. Suppose the prior belief on the latent parameter $\\eta$ is Gaussian with mean $\\mu$ and standard deviation $\\sigma$, where $\\sigma  0$, that is, the prior density on $\\eta$ is\n$$\np_{\\eta}(\\eta) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(\\eta - \\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nStarting from the conservation of probability under reparameterization and the definition of a probability density, derive the induced prior density $p_{E}(E)$ on the physical modulus $E$ implied by the mapping $E = \\exp(\\eta)$. Determine the normalization constant so that\n$$\n\\int_{0}^{\\infty} p_{E}(E)\\,\\mathrm{d}E = 1.\n$$\nExpress your final answer as a single closed-form analytic expression for $p_{E}(E)$ in terms of $E$, $\\mu$, and $\\sigma$. No numerical evaluation is required. Do not include units in your final expression. The final answer must be a single expression only.",
            "solution": "The problem requires the derivation of the probability density function (PDF) for the Young's modulus, $E$, given the PDF for its logarithm, $\\eta = \\ln(E)$. The relationship between the two PDFs is governed by the principle of conservation of probability, which leads to the change of variables formula for probability densities.\n\nLet $\\eta$ be a random variable with a known PDF $p_{\\eta}(\\eta)$, and let $E = g(\\eta)$ be a new random variable defined by an invertible, differentiable transformation $g$. The inverse transformation is $\\eta = g^{-1}(E)$. The PDF of $E$, denoted $p_{E}(E)$, can be found using the following relationship:\n$$\np_{E}(E) = p_{\\eta}\\left(g^{-1}(E)\\right) \\left| \\frac{\\mathrm{d}}{\\mathrm{d}E} g^{-1}(E) \\right|\n$$\nThis formula ensures that the probability element is conserved, i.e., $|p_{E}(E)\\,\\mathrm{d}E| = |p_{\\eta}(\\eta)\\,\\mathrm{d}\\eta|$.\n\nIn the context of this problem, we are given:\n1.  The transformation from the latent parameter $\\eta$ to the physical parameter $E$: $E = \\exp(\\eta)$.\n2.  The inverse transformation is therefore $\\eta = \\ln(E)$. So, $g^{-1}(E) = \\ln(E)$.\n3.  The prior density on $\\eta$, which is a Gaussian distribution with mean $\\mu$ and standard deviation $\\sigma$:\n$$\np_{\\eta}(\\eta) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\eta - \\mu)^{2}}{2\\sigma^{2}}\\right)\n$$\nThe domain for $\\eta$ is $(-\\infty, \\infty)$. Because $E = \\exp(\\eta)$, the domain for $E$ is $(0, \\infty)$, which is consistent with the physical requirement that the Young's modulus must be strictly positive.\n\nFirst, we calculate the Jacobian determinant of the transformation, which is the absolute value of the derivative of the inverse transformation, $\\left| \\frac{\\mathrm{d}\\eta}{\\mathrm{d}E} \\right|$.\n$$\n\\eta(E) = \\ln(E)\n$$\nThe derivative is:\n$$\n\\frac{\\mathrm{d}\\eta}{\\mathrm{d}E} = \\frac{\\mathrm{d}}{\\mathrm{d}E} \\left( \\ln(E) \\right) = \\frac{1}{E}\n$$\nSince $E  0$, the absolute value is simply:\n$$\n\\left| \\frac{\\mathrm{d}\\eta}{\\mathrm{d}E} \\right| = \\left| \\frac{1}{E} \\right| = \\frac{1}{E}\n$$\n\nNext, we substitute $\\eta = \\ln(E)$ into the expression for $p_{\\eta}(\\eta)$:\n$$\np_{\\eta}(\\ln(E)) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln(E) - \\mu)^{2}}{2\\sigma^{2}}\\right)\n$$\n\nNow, we assemble the final expression for $p_{E}(E)$ by multiplying $p_{\\eta}(\\ln(E))$ by the Jacobian term $\\left| \\frac{\\mathrm{d}\\eta}{\\mathrm{d}E} \\right|$:\n$$\np_{E}(E) = p_{\\eta}(\\ln(E)) \\left| \\frac{\\mathrm{d}\\eta}{\\mathrm{d}E} \\right| = \\left( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln(E) - \\mu)^{2}}{2\\sigma^{2}}\\right) \\right) \\cdot \\left( \\frac{1}{E} \\right)\n$$\nRearranging the terms gives the final closed-form expression for the prior density on $E$:\n$$\np_{E}(E) = \\frac{1}{E \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln(E) - \\mu)^{2}}{2\\sigma^{2}}\\right)\n$$\nThis is the probability density function of a log-normal distribution, which is defined for $E  0$.\n\nThe problem asks to determine the normalization constant such that $\\int_{0}^{\\infty} p_{E}(E)\\,\\mathrm{d}E = 1$. The derivation using the change of variables formula automatically preserves normalization. If $p_{\\eta}(\\eta)$ is normalized to $1$, then the resulting $p_{E}(E)$ is also normalized to $1$. The integral of the Gaussian prior $p_{\\eta}(\\eta)$ from $-\\infty$ to $\\infty$ is $1$. By performing a change of variables in the integral $\\int_{0}^{\\infty} p_{E}(E)\\,\\mathrm{d}E$ with the substitution $u = \\ln(E)$ (so $\\mathrm{d}u = \\frac{1}{E} \\mathrm{d}E$, and the limits transform from $(0, \\infty)$ to $(-\\infty, \\infty)$), the integral becomes $\\int_{-\\infty}^{\\infty} p_{\\eta}(u)\\,\\mathrm{d}u$, which confirms that the integral is indeed $1$. Therefore, the expression derived is already correctly normalized, and the \"normalization constant\" is implicitly contained within the expression, specifically the factor $\\frac{1}{\\sigma\\sqrt{2\\pi}}$.",
            "answer": "$$\\boxed{\\frac{1}{E \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{\\left(\\ln(E) - \\mu\\right)^{2}}{2\\sigma^{2}}\\right)}$$"
        },
        {
            "introduction": "While the Gaussian noise model is a common starting point for defining a likelihood function, real-world experimental data is often contaminated with outliers that can disproportionately influence parameter estimates. This practice  introduces a powerful strategy for building robust statistical models by replacing the Gaussian likelihood with a Student-$t$ likelihood. You will derive this more resilient distribution by marginalizing a hierarchical model, providing deep insight into how to construct flexible and outlier-resistant likelihoods that better capture the complexities of experimental measurements.",
            "id": "3547105",
            "problem": "A quasi-static uniaxial tension test on a ductile metal provides $N$ pairs of measured engineering strain and Cauchy stress, denoted $\\{(\\varepsilon_i, \\sigma_i)\\}_{i=1}^N$. A computational forward model $m(\\varepsilon; \\boldsymbol{\\theta})$ maps strain $ \\varepsilon $ to predicted stress based on a chosen constitutive law with material parameters $\\boldsymbol{\\theta}$ (for example, a rate-independent isotropic $J_2$ elastoplastic model integrated with a standard return-mapping algorithm). Suppose observational errors are caused by occasional grip slippage and sensor nonlinearities, producing outlier stresses. To resist outliers in a Bayesian calibration of $\\boldsymbol{\\theta}$, replace the usual additive Gaussian noise with an additive Student-$t$ noise model. Assume the following hierarchical measurement model for residuals $r_i(\\boldsymbol{\\theta}) = \\sigma_i - m(\\varepsilon_i;\\boldsymbol{\\theta})$:\n1) Conditional on a latent variance $ \\tau_i $, each residual is Gaussian: $r_i \\mid \\tau_i \\sim \\mathcal{N}(0, \\tau_i)$.\n2) The latent variance is independently distributed across observations as inverse-gamma with shape $ \\alpha = \\nu/2 $ and scale $ \\beta = \\nu s^2/2 $, where $ \\nu  0 $ is the degrees of freedom (DoF) of the Student-$t$ model and $ s  0 $ is a scale parameter.\n3) Residuals are independent across $ i $ conditioned on $(\\boldsymbol{\\theta}, s, \\nu)$.\n\nDerive, by marginalizing the latent variances $ \\{\\tau_i\\} $, the closed-form likelihood function of the stress observations $ \\{\\sigma_i\\}_{i=1}^N $ given the forward model predictions $ \\{m(\\varepsilon_i;\\boldsymbol{\\theta})\\}_{i=1}^N $ under this Student-$t$ noise model, expressed explicitly in terms of $ \\boldsymbol{\\theta} $, $ s $, $ \\nu $, and the data. Provide a single, closed-form analytic expression for the likelihood $L(\\boldsymbol{\\theta}, s \\mid \\mathcal{D}, \\nu)$, where $ \\mathcal{D} = \\{(\\varepsilon_i, \\sigma_i)\\}_{i=1}^N $. Do not simplify to a log-likelihood. Your final answer must be a single analytical expression. No rounding is required.",
            "solution": "The problem is valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The task is to derive the likelihood function for a set of experimental data under a specified hierarchical Bayesian model, which is a standard procedure in statistical inference and uncertainty quantification.\n\nThe objective is to derive the likelihood function $L(\\boldsymbol{\\theta}, s \\mid \\mathcal{D}, \\nu) = p(\\{\\sigma_i\\}_{i=1}^N \\mid \\boldsymbol{\\theta}, s, \\nu)$, where $\\mathcal{D} = \\{(\\varepsilon_i, \\sigma_i)\\}_{i=1}^N$. The problem states that the residuals $r_i(\\boldsymbol{\\theta}) = \\sigma_i - m(\\varepsilon_i;\\boldsymbol{\\theta})$ are independent across observations $i$ when conditioned on the parameters $(\\boldsymbol{\\theta}, s, \\nu)$. Therefore, the total likelihood is the product of the likelihoods for each individual observation:\n$$\nL(\\boldsymbol{\\theta}, s \\mid \\mathcal{D}, \\nu) = \\prod_{i=1}^{N} p(\\sigma_i \\mid \\boldsymbol{\\theta}, s, \\nu)\n$$\nSince the noise model is additive, i.e., $\\sigma_i = m(\\varepsilon_i;\\boldsymbol{\\theta}) + r_i$, the probability density of an observation $\\sigma_i$ is equivalent to the probability density of its corresponding residual $r_i$:\n$$\np(\\sigma_i \\mid \\boldsymbol{\\theta}, s, \\nu) = p(r_i(\\boldsymbol{\\theta}) \\mid s, \\nu)\n$$\nOur task thus reduces to finding the marginal probability density function of a single residual, $p(r_i)$, by integrating out the latent variance $\\tau_i$.\n\nFor a single residual $r_i$, the hierarchical model is given by:\n$1$. The conditional distribution of the residual is Gaussian: $r_i \\mid \\tau_i \\sim \\mathcal{N}(0, \\tau_i)$. The probability density function (PDF) is:\n$$\np(r_i \\mid \\tau_i) = \\frac{1}{\\sqrt{2\\pi\\tau_i}} \\exp\\left(-\\frac{r_i^2}{2\\tau_i}\\right)\n$$\n$2$. The latent variance $\\tau_i$ follows an Inverse-Gamma distribution: $\\tau_i \\sim \\text{Inverse-Gamma}(\\alpha, \\beta)$, with shape $\\alpha = \\nu/2$ and scale $\\beta = \\nu s^2/2$. The PDF is:\n$$\np(\\tau_i \\mid s, \\nu) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\tau_i^{-\\alpha-1} \\exp\\left(-\\frac{\\beta}{\\tau_i}\\right) = \\frac{\\left(\\frac{\\nu s^2}{2}\\right)^{\\frac{\\nu}{2}}}{\\Gamma\\left(\\frac{\\nu}{2}\\right)} \\tau_i^{-\\frac{\\nu}{2}-1} \\exp\\left(-\\frac{\\nu s^2}{2\\tau_i}\\right)\n$$\nwhere $\\Gamma(\\cdot)$ is the gamma function.\n\nThe marginal probability of $r_i$ is found by integrating the joint probability $p(r_i, \\tau_i) = p(r_i \\mid \\tau_i) p(\\tau_i)$ over all possible values of $\\tau_i \\in (0, \\infty)$:\n$$\np(r_i \\mid s, \\nu) = \\int_{0}^{\\infty} p(r_i \\mid \\tau_i) p(\\tau_i \\mid s, \\nu) \\,d\\tau_i\n$$\nSubstituting the PDFs:\n$$\np(r_i \\mid s, \\nu) = \\int_{0}^{\\infty} \\left[ \\frac{1}{\\sqrt{2\\pi\\tau_i}} \\exp\\left(-\\frac{r_i^2}{2\\tau_i}\\right) \\right] \\left[ \\frac{\\left(\\frac{\\nu s^2}{2}\\right)^{\\frac{\\nu}{2}}}{\\Gamma\\left(\\frac{\\nu}{2}\\right)} \\tau_i^{-\\frac{\\nu}{2}-1} \\exp\\left(-\\frac{\\nu s^2}{2\\tau_i}\\right) \\right] \\,d\\tau_i\n$$\nCombine the terms involving $\\tau_i$:\n$$\np(r_i \\mid s, \\nu) = \\frac{\\left(\\frac{\\nu s^2}{2}\\right)^{\\frac{\\nu}{2}}}{\\sqrt{2\\pi}\\Gamma\\left(\\frac{\\nu}{2}\\right)} \\int_{0}^{\\infty} \\tau_i^{-\\frac{1}{2}} \\tau_i^{-\\frac{\\nu}{2}-1} \\exp\\left(-\\frac{r_i^2}{2\\tau_i} - \\frac{\\nu s^2}{2\\tau_i}\\right) \\,d\\tau_i\n$$\n$$\np(r_i \\mid s, \\nu) = \\frac{\\left(\\frac{\\nu s^2}{2}\\right)^{\\frac{\\nu}{2}}}{\\sqrt{2\\pi}\\Gamma\\left(\\frac{\\nu}{2}\\right)} \\int_{0}^{\\infty} \\tau_i^{-\\left(\\frac{\\nu+1}{2}\\right)-1} \\exp\\left(-\\frac{1}{\\tau_i}\\left(\\frac{r_i^2 + \\nu s^2}{2}\\right)\\right) \\,d\\tau_i\n$$\nThe integral is of the form $\\int_{0}^{\\infty} x^{-a-1} \\exp(-b/x) \\,dx$, which is related to the normalization constant of an Inverse-Gamma distribution. The value of this integral is $\\frac{\\Gamma(a)}{b^a}$.\nIn our case, the parameters for the integral are $a = \\frac{\\nu+1}{2}$ and $b = \\frac{r_i^2 + \\nu s^2}{2}$.\nEvaluating the integral gives:\n$$\n\\int_{0}^{\\infty} \\tau_i^{-\\left(\\frac{\\nu+1}{2}\\right)-1} \\exp\\left(-\\frac{1}{\\tau_i}\\left(\\frac{r_i^2 + \\nu s^2}{2}\\right)\\right) \\,d\\tau_i = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\left(\\frac{r_i^2 + \\nu s^2}{2}\\right)^{\\frac{\\nu+1}{2}}}\n$$\nSubstituting this result back into the expression for $p(r_i \\mid s, \\nu)$:\n$$\np(r_i \\mid s, \\nu) = \\frac{\\left(\\frac{\\nu s^2}{2}\\right)^{\\frac{\\nu}{2}}}{\\sqrt{2\\pi}\\Gamma\\left(\\frac{\\nu}{2}\\right)} \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\left(\\frac{r_i^2 + \\nu s^2}{2}\\right)^{\\frac{\\nu+1}{2}}}\n$$\nRearranging the terms to obtain a standard form:\n$$\np(r_i \\mid s, \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{2\\pi}} \\frac{\\left(\\frac{\\nu s^2}{2}\\right)^{\\frac{\\nu}{2}}}{\\left(\\frac{\\nu s^2}{2}\\left(1 + \\frac{r_i^2}{\\nu s^2}\\right)\\right)^{\\frac{\\nu+1}{2}}}\n$$\n$$\np(r_i \\mid s, \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{2\\pi}} \\frac{\\left(\\frac{\\nu s^2}{2}\\right)^{\\frac{\\nu}{2}}}{\\left(\\frac{\\nu s^2}{2}\\right)^{\\frac{\\nu+1}{2}}\\left(1 + \\frac{r_i^2}{\\nu s^2}\\right)^{\\frac{\\nu+1}{2}}}\n$$\n$$\np(r_i \\mid s, \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{2\\pi}} \\frac{1}{\\left(\\frac{\\nu s^2}{2}\\right)^{\\frac{1}{2}}} \\left(1 + \\frac{r_i^2}{\\nu s^2}\\right)^{-\\frac{\\nu+1}{2}}\n$$\n$$\np(r_i \\mid s, \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu\\pi} s} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{r_i}{s}\\right)^2\\right)^{-\\frac{\\nu+1}{2}}\n$$\nThis is the PDF of a location-scale Student-$t$ distribution with degrees of freedom $\\nu$, location $\\mu=0$, and scale $s$.\n\nSubstituting $r_i = \\sigma_i - m(\\varepsilon_i;\\boldsymbol{\\theta})$, the likelihood for a single observation is:\n$$\np(\\sigma_i \\mid \\boldsymbol{\\theta}, s, \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{s \\sqrt{\\nu\\pi} \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{\\left(\\sigma_i - m(\\varepsilon_i; \\boldsymbol{\\theta})\\right)^2}{\\nu s^2}\\right)^{-\\frac{\\nu+1}{2}}\n$$\nFinally, the full likelihood for the $N$ independent observations is the product of the individual likelihoods:\n$$\nL(\\boldsymbol{\\theta}, s \\mid \\mathcal{D}, \\nu) = \\prod_{i=1}^{N} \\left[ \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{s \\sqrt{\\nu\\pi} \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{\\left(\\sigma_i - m(\\varepsilon_i; \\boldsymbol{\\theta})\\right)^2}{\\nu s^2}\\right)^{-\\frac{\\nu+1}{2}} \\right]\n$$\nThis can be written more compactly by factoring out the constant term:\n$$\nL(\\boldsymbol{\\theta}, s \\mid \\mathcal{D}, \\nu) = \\left( \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{s \\sqrt{\\nu\\pi} \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\right)^{N} \\prod_{i=1}^{N} \\left(1 + \\frac{\\left(\\sigma_i - m(\\varepsilon_i; \\boldsymbol{\\theta})\\right)^2}{\\nu s^2}\\right)^{-\\frac{\\nu+1}{2}}\n$$\nThis is the desired closed-form analytic expression for the likelihood function.",
            "answer": "$$\n\\boxed{\\left( \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{s \\sqrt{\\nu\\pi} \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\right)^{N} \\prod_{i=1}^{N} \\left(1 + \\frac{\\left(\\sigma_i - m(\\varepsilon_i; \\boldsymbol{\\theta})\\right)^2}{\\nu s^2}\\right)^{-\\frac{\\nu+1}{2}}}\n$$"
        },
        {
            "introduction": "The true power of Bayesian calibration is realized when it is integrated with sophisticated computational models, such as those from the Finite Element (FE) method. However, the computational cost of gradient-based inference can become prohibitive when the model is expensive and the parameter sensitivities are needed. This comprehensive practice  guides you through the derivation and implementation of the adjoint method to compute the log-likelihood gradient efficiently. By solving a single adjoint system, you can obtain the gradient with respect to all parameters at a cost independent of the parameter space dimension, a state-of-the-art technique that makes Bayesian inference for complex FE models computationally tractable.",
            "id": "3547183",
            "problem": "Consider a one-dimensional, nondimensionalized hyperelastic bar occupying the reference domain $[0,L]$ with $L=1$. The bar is discretized using the Finite Element (FE) method with $N_e$ linear elements and $N_n=N_e+1$ nodes. The nodal displacement field $u(X)$ defines the current position $x=X+u(X)$ and the deformation gradient $F=1+\\frac{du}{dX}$. The cross-sectional area is $A=1$ and all quantities are nondimensional, so no physical units are involved.\n\nAssume a compressible Neo-Hookean stored energy density\n$$\nW(F;\\theta)=\\frac{\\mu}{2}\\left(F^2-1\\right)-\\mu\\ln F+\\frac{\\lambda}{2}\\left(\\ln F\\right)^2,\n$$\nwhere the material parameters are $\\theta=\\{\\lambda,\\mu\\}$. The first Piola-Kirchhoff stress is\n$$\nP(F;\\theta)=\\frac{\\partial W}{\\partial F}=\\mu F-\\frac{\\mu}{F}+\\frac{\\lambda \\ln F}{F}.\n$$\nThe bar is subject to prescribed displacement boundary conditions $u(0)=0$ and $u(L)=U_0$, with no body forces and no applied tractions. The FE internal force residual $R(u,\\theta)$ for the free nodal degrees of freedom is obtained from the principle of virtual work. Using one-point quadrature and linear shape functions on each element of length $h=L/N_e$, the elemental internal force contributions are $[-P(F;\\theta),+P(F;\\theta)]$ assembled to the global residual. The tangent matrix $A=\\frac{\\partial R}{\\partial u}$ is obtained by differentiating the residual with respect to the free nodal displacements and involves $\\frac{\\partial P}{\\partial F}$ via $\\frac{\\partial F}{\\partial u}$.\n\nYou observe stresses at the element midpoints, collected in the data vector $D \\in \\mathbb{R}^m$ with $m=N_e$. The forward map $f(\\theta)$ computes the FE solution $u(\\theta)$ consistent with the boundary conditions and returns the predicted stress values $P(F_e;\\theta)$ for each element $e=1,\\dots,N_e$. The likelihood of the data given the parameters is Gaussian, with independent noise of variance $\\sigma^2$ at each measurement:\n$$\np(D\\mid \\theta)\\propto \\exp\\left(-\\frac{1}{2}\\left(D-f(\\theta)\\right)^{\\!\\top}\\Sigma^{-1}\\left(D-f(\\theta)\\right)\\right),\\quad \\Sigma=\\sigma^2 I_m.\n$$\nYour tasks are:\n- Starting from the principle of virtual work and the Gaussian likelihood definition, derive the adjoint-based gradient $\\frac{\\partial \\log p(D\\mid\\theta)}{\\partial \\theta}$ that avoids explicit computation of $\\frac{du}{d\\theta}$. Express the adjoint equation and the final gradient formula in terms of $\\frac{\\partial f}{\\partial \\theta}$, $\\frac{\\partial f}{\\partial u}$, $\\frac{\\partial R}{\\partial u}$, and $\\frac{\\partial R}{\\partial \\theta}$.\n- Implement a complete program that:\n  1. Builds the FE residual $R(u,\\theta)$ and tangent $A=\\frac{\\partial R}{\\partial u}$ for the $1$D bar using the given $W(F;\\theta)$ and $P(F;\\theta)$.\n  2. Solves the nonlinear FE equilibrium $R(u,\\theta)=0$ with the specified displacement boundary conditions using Newton's method.\n  3. Evaluates the forward map $f(\\theta)$, its sensitivities $\\frac{\\partial f}{\\partial \\theta}$ and $\\frac{\\partial f}{\\partial u}$, assembles the adjoint system, solves for the adjoint, and computes $\\frac{\\partial \\log p(D\\mid\\theta)}{\\partial \\theta}$ using your derived formula.\n\nUse the following test suite of cases, with all quantities nondimensional:\n- Case $1$ (general): $N_e=4$, $U_0=0.2$, $\\sigma=0.05$, candidate parameters $\\theta=\\{\\lambda=8.0,\\mu=4.0\\}$, and data $D=[3.0274117,\\,2.9974117,\\,3.0324117,\\,3.0124117]$.\n- Case $2$ (finite strain): $N_e=3$, $U_0=0.5$, $\\sigma=0.08$, candidate parameters $\\theta=\\{\\lambda=12.0,\\mu=6.0\\}$, and data $D=[8.545232495,\\,8.515232495,\\,8.525232495]$.\n- Case $3$ (edge, near identity stretch): $N_e=2$, $U_0=0.0$, $\\sigma=0.03$, candidate parameters $\\theta=\\{\\lambda=10.0,\\mu=5.0\\}$, and data $D=[0.003,\\,-0.004]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be the two-component gradient vector $[\\frac{\\partial \\log p}{\\partial \\lambda},\\frac{\\partial \\log p}{\\partial \\mu}]$ for that case, so the overall output is a list of lists of floats in the order of the test suite cases, for example $[[g_{\\lambda,1},g_{\\mu,1}],[g_{\\lambda,2},g_{\\mu,2}],[g_{\\lambda,3},g_{\\mu,3}]]$.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of computational solid mechanics and Bayesian inference, well-posed, objective, and self-contained. All necessary data and definitions for a unique solution are provided.\n\nThe core of the problem is to derive and implement the adjoint-based gradient of the log-likelihood function for a set of material parameters $\\theta$. This allows for efficient sensitivity analysis, which is fundamental to gradient-based optimization and sampling methods in Bayesian calibration.\n\n### Derivation of the Adjoint-Based Gradient\n\nLet the vector of material parameters be $\\theta = \\{\\lambda, \\mu\\}$. The observations are collected in the data vector $D$. The forward model $f(\\theta)$ predicts these observations. The state of the system, represented by the vector of free nodal displacements $u$, is implicitly dependent on $\\theta$ through the nonlinear finite element equilibrium equation, $R(u, \\theta) = 0$.\n\nThe log-likelihood function, neglecting constant terms, is given by:\n$$\nJ(\\theta) \\equiv \\log p(D \\mid \\theta) = -\\frac{1}{2\\sigma^2} (D - f(u(\\theta), \\theta))^T (D - f(u(\\theta), \\theta))\n$$\nOur goal is to compute the total derivative of $J$ with respect to $\\theta$, denoted as $\\frac{dJ}{d\\theta}$. Applying the chain rule, we obtain:\n$$\n\\frac{dJ}{d\\theta} = \\frac{\\partial J}{\\partial \\theta} + \\frac{\\partial J}{\\partial u} \\frac{du}{d\\theta}\n$$\nwhere $\\frac{\\partial J}{\\partial \\theta}$ and $\\frac{\\partial J}{\\partial u}$ are partial derivatives, and $\\frac{du}{d\\theta}$ is the sensitivity of the displacements with respect to the parameters. The term $\\frac{du}{d\\theta}$ is computationally expensive to evaluate directly. The adjoint method provides an elegant way to bypass its explicit computation.\n\nWe find an expression for $\\frac{du}{d\\theta}$ by taking the total derivative of the equilibrium equation $R(u, \\theta) = 0$ with respect to $\\theta$:\n$$\n\\frac{dR}{d\\theta} = \\frac{\\partial R}{\\partial \\theta} + \\frac{\\partial R}{\\partial u} \\frac{du}{d\\theta} = 0\n$$\nAssuming the tangent stiffness matrix $A = \\frac{\\partial R}{\\partial u}$ is invertible, we can write:\n$$\n\\frac{du}{d\\theta} = - \\left(\\frac{\\partial R}{\\partial u}\\right)^{-1} \\frac{\\partial R}{\\partial \\theta}\n$$\nSubstituting this into the expression for $\\frac{dJ}{d\\theta}$ gives the direct sensitivity expression:\n$$\n\\frac{dJ}{d\\theta} = \\frac{\\partial J}{\\partial \\theta} - \\frac{\\partial J}{\\partial u} \\left(\\frac{\\partial R}{\\partial u}\\right)^{-1} \\frac{\\partial R}{\\partial \\theta}\n$$\nThis expression still requires solving a linear system for each parameter in $\\theta$. To obtain the adjoint formulation, we rearrange the terms. Let us define a new vector, the adjoint vector $\\psi$, as the solution to the following linear system, called the adjoint equation:\n$$\n\\left(\\frac{\\partial R}{\\partial u}\\right)^T \\psi = \\left(\\frac{\\partial J}{\\partial u}\\right)^T\n$$\nThis definition allows us to write $\\frac{\\partial J}{\\partial u} = \\psi^T \\frac{\\partial R}{\\partial u}$. Substituting this into the gradient expression:\n$$\n\\frac{dJ}{d\\theta} = \\frac{\\partial J}{\\partial \\theta} - \\left(\\psi^T \\frac{\\partial R}{\\partial u}\\right) \\left(\\frac{\\partial R}{\\partial u}\\right)^{-1} \\frac{\\partial R}{\\partial \\theta} = \\frac{\\partial J}{\\partial \\theta} - \\psi^T \\frac{\\partial R}{\\partial \\theta}\n$$\nThis is the final adjoint-based formula for the gradient. To use it, we need the partial derivatives of $J$. From the definition of $J$:\n$$\n\\frac{\\partial J}{\\partial u} = \\frac{1}{\\sigma^2} (D - f)^T \\frac{\\partial f}{\\partial u}\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\frac{1}{\\sigma^2} (D - f)^T \\frac{\\partial f}{\\partial \\theta}\n$$\nSubstituting these into our main formulas gives the operational equations:\n\n1.  **Adjoint Equation**:\n    $$\n    \\left(\\frac{\\partial R}{\\partial u}\\right)^T \\psi = \\frac{1}{\\sigma^2} \\left(\\frac{\\partial f}{\\partial u}\\right)^T (D - f)\n    $$\n2.  **Gradient Formula**:\n    $$\n    \\frac{dJ}{d\\theta} = \\frac{1}{\\sigma^2} (D - f)^T \\frac{\\partial f}{\\partial \\theta} - \\psi^T \\frac{\\partial R}{\\partial \\theta}\n    $$\n\n### Algorithmic Implementation\n\nThe computation proceeds in the following steps:\n1.  **Forward Solve**: For a given set of parameters $\\theta$, solve the nonlinear system of equations $R(u, \\theta) = 0$ for the displacement vector $u$ of the free degrees of freedom. This is accomplished using Newton's method, which iteratively solves $A \\Delta u = -R$, where $A = \\frac{\\partial R}{\\partial u}$ is the tangent stiffness matrix.\n2.  **Compute Model Prediction**: Using the converged displacement field $u$, calculate the deformation gradient $F_e$ and the first Piola-Kirchhoff stress $P_e$ for each element $e$. The vector of these stresses constitutes the forward model prediction, $f(\\theta) = [P_1, \\dots, P_{N_e}]^T$.\n3.  **Compute Sensitivities**: Evaluate all required partial derivatives at the converged state $(u, \\theta)$:\n    - $\\frac{\\partial R}{\\partial u}$: The tangent stiffness matrix $A$, which is available from the final step of the Newton solver.\n    - $\\frac{\\partial R}{\\partial \\theta}$: The sensitivity of the residual to the parameters.\n    - $\\frac{\\partial f}{\\partial u}$: The sensitivity of the predicted stresses to the displacements.\n    - $\\frac{\\partial f}{\\partial \\theta}$: The sensitivity of the predicted stresses to the parameters.\n4.  **Adjoint Solve**: Assemble the right-hand side of the adjoint equation, $b_{adj} = \\frac{1}{\\sigma^2} (\\frac{\\partial f}{\\partial u})^T (D - f)$, and solve the linear system $A^T \\psi = b_{adj}$ for the adjoint vector $\\psi$.\n5.  **Gradient Assembly**: Compute the two terms of the gradient formula and combine them to get the final gradient vector $\\frac{dJ}{d\\theta}$.\n\nThis procedure is implemented for each test case provided in the problem statement. For the 1D bar with linear elements, the displacement field is piecewise linear, resulting in a deformation gradient $F$ that is constant within each element but can vary between elements. The partial derivative of the residual with respect to the parameters, $\\frac{\\partial R}{\\partial \\theta}$, is generally non-zero. The implementation provided correctly computes the full adjoint-based gradient without making simplifying assumptions.",
            "answer": "```python\nimport numpy as np\n\ndef solve_fe(params, Ne, U0):\n    \"\"\"\n    Solves the nonlinear FE equilibrium equation R(u, theta) = 0 using Newton's method.\n    \"\"\"\n    lambda_, mu = params\n    L = 1.0\n    h = L / Ne\n    num_free_dof = Ne - 1\n\n    if num_free_dof = 0:\n        # Case with no free DOFs (e.g., Ne=1)\n        u = np.linspace(0, U0, Ne + 1)\n        F = 1.0 + np.diff(u) / h\n        lnF = np.log(F)\n        P_prime = mu + mu / F**2 + lambda_ * (1 - lnF) / F**2\n        A = np.array([[]]) # No tangent for free dofs\n        return u, A, True\n\n    # Initial guess: linear displacement profile\n    u_free = np.linspace(0, U0, Ne + 1)[1:-1]\n    \n    max_iter = 20\n    tol = 1e-10\n\n    A = np.zeros((num_free_dof, num_free_dof))\n\n    for _ in range(max_iter):\n        u = np.concatenate(([0], u_free, [U0]))\n        \n        u_diff = np.diff(u)\n        F = 1.0 + u_diff / h\n        \n        if np.any(F = 0):\n            return None, None, False\n\n        lnF = np.log(F)\n        P = mu * F - mu / F + lambda_ * lnF / F\n        \n        R_free = P[1:] - P[:-1]\n        \n        norm_R = np.linalg.norm(R_free)\n        if norm_R  tol:\n            # Converged. Re-compute tangent for return.\n            P_prime = mu + mu / F**2 + lambda_ * (1 - lnF) / F**2\n            diag = -(P_prime[1:] + P_prime[:-1]) / h\n            sup_diag = P_prime[1:-1] / h\n            sub_diag = P_prime[1:-1] / h\n            A = np.diag(diag) + np.diag(sup_diag, k=1) + np.diag(sub_diag, k=-1)\n            return u, A, True\n\n        P_prime = mu + mu / F**2 + lambda_ * (1 - lnF) / F**2\n        \n        diag = -(P_prime[1:] + P_prime[:-1]) / h\n        A = np.diag(diag)\n        if num_free_dof  1:\n            sup_diag = P_prime[1:-1] / h\n            sub_diag = P_prime[1:-1] / h\n            A += np.diag(sup_diag, k=1) + np.diag(sub_diag, k=-1)\n\n        delta_u = np.linalg.solve(A, -R_free)\n        u_free += delta_u\n            \n    return None, None, False # Did not converge\n\ndef calculate_gradient(params, Ne, U0, sigma, D):\n    \"\"\"\n    Calculates the adjoint-based gradient of the log-likelihood.\n    \"\"\"\n    lambda_, mu = params\n    h = 1.0 / Ne\n    num_free_dof = Ne - 1\n    \n    # 1. Forward Solve\n    u, A, converged = solve_fe(params, Ne, U0)\n    if not converged:\n        raise RuntimeError(\"FE solver did not converge for the given parameters.\")\n\n    # 2. Compute Model Prediction f(theta) and its derivatives\n    u_diff = np.diff(u)\n    F = 1.0 + u_diff / h\n    lnF = np.log(F)\n    \n    f_theta = mu * F - mu / F + lambda_ * lnF / F\n    misfit = D - f_theta\n    \n    # 3. Compute Sensitivities\n    # Partial derivative of P wrt F\n    P_prime = mu + mu / F**2 + lambda_ * (1 - lnF) / F**2\n\n    # df/dtheta (Ne x 2 matrix)\n    df_dlambda = lnF / F\n    df_dmu = F - 1.0 / F\n    df_dtheta = np.vstack((df_dlambda, df_dmu)).T\n    \n    if num_free_dof  0:\n        # dR/dtheta ((Ne-1) x 2 matrix)\n        dR_dlambda = df_dlambda[1:] - df_dlambda[:-1]\n        dR_dmu = df_dmu[1:] - df_dmu[:-1]\n        dR_dtheta = np.vstack((dR_dlambda, dR_dmu)).T\n\n        # df/du_free (Ne x (Ne-1) matrix)\n        # u_free[j] corresponds to nodal displacement u_{j+1}\n        # df_e/du_free[j] = dP_e/dF_e * dF_e/du_{j+1}\n        df_du_free = np.zeros((Ne, num_free_dof))\n        for e in range(Ne): # over elements (rows)\n            # Contribution from u_{e+1} dependency\n            if e  num_free_dof: # u_{e+1} is u_free[e]\n                df_du_free[e, e] += P_prime[e] / h\n            # Contribution from u_e dependency\n            if e  0 and (e - 1)  num_free_dof: # u_e is u_free[e-1]\n                df_du_free[e, e - 1] -= P_prime[e] / h\n        \n        # 4. Adjoint Solve\n        b_adj = df_du_free.T @ misfit / (sigma**2)\n        psi = np.linalg.solve(A.T, b_adj)\n        \n        # 5. Gradient Assembly\n        term1 = (misfit @ df_dtheta) / (sigma**2)\n        term2 = psi.T @ dR_dtheta\n        grad = term1 - term2\n    else: # No free dofs, gradient has no adjoint part\n        term1 = (misfit @ df_dtheta) / (sigma**2)\n        grad = term1\n\n    return grad.tolist()\n\ndef solve():\n    test_cases = [\n        {'Ne': 4, 'U0': 0.2, 'sigma': 0.05, 'params': (8.0, 4.0), \n         'D': np.array([3.0274117, 2.9974117, 3.0324117, 3.0124117])},\n        {'Ne': 3, 'U0': 0.5, 'sigma': 0.08, 'params': (12.0, 6.0), \n         'D': np.array([8.545232495, 8.515232495, 8.525232495])},\n        {'Ne': 2, 'U0': 0.0, 'sigma': 0.03, 'params': (10.0, 5.0), \n         'D': np.array([0.003, -0.004])},\n    ]\n\n    results = []\n    for case in test_cases:\n        grad = calculate_gradient(case['params'], case['Ne'], case['U0'], case['sigma'], case['D'])\n        results.append(grad)\n\n    # Format the output exactly as required.\n    # e.g., [[g_lam1,g_mu1],[g_lam2,g_mu2],[g_lam3,g_mu3]]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}