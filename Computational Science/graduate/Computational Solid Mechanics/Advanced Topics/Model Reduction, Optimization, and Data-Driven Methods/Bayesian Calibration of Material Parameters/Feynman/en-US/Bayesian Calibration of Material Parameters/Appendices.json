{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in Bayesian calibration is defining the prior distribution, which encodes our knowledge about parameters before observing data. Many material parameters, such as Young's modulus, are physically constrained to be positive. This practice demonstrates how to rigorously enforce such constraints through reparameterization, a fundamental technique in statistical modeling. By deriving the log-normal prior distribution for the modulus $E$ from a Gaussian prior on its logarithm, you will master the change of variables formula and learn to build physically consistent prior models .",
            "id": "3547099",
            "problem": "In computational solid mechanics, consider Bayesian calibration of an isotropic linear elastic material's Young's modulus, denoted by $E$, where $E$ is strictly positive due to physical constraints. To enforce positivity and stabilize inference, introduce the logarithmic reparameterization $\\eta = \\ln(E)$. Suppose the prior belief on the latent parameter $\\eta$ is Gaussian with mean $\\mu$ and standard deviation $\\sigma$, where $\\sigma > 0$, that is, the prior density on $\\eta$ is\n$$\np_{\\eta}(\\eta) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(\\eta - \\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nStarting from the conservation of probability under reparameterization and the definition of a probability density, derive the induced prior density $p_{E}(E)$ on the physical modulus $E$ implied by the mapping $E = \\exp(\\eta)$. Determine the normalization constant so that\n$$\n\\int_{0}^{\\infty} p_{E}(E)\\,\\mathrm{d}E = 1.\n$$\nExpress your final answer as a single closed-form analytic expression for $p_{E}(E)$ in terms of $E$, $\\mu$, and $\\sigma$. No numerical evaluation is required. Do not include units in your final expression. The final answer must be a single expression only.",
            "solution": "The problem requires the derivation of the probability density function (PDF) for the Young's modulus, $E$, given the PDF for its logarithm, $\\eta = \\ln(E)$. The relationship between the two PDFs is governed by the principle of conservation of probability, which leads to the change of variables formula for probability densities.\n\nLet $\\eta$ be a random variable with a known PDF $p_{\\eta}(\\eta)$, and let $E = g(\\eta)$ be a new random variable defined by an invertible, differentiable transformation $g$. The inverse transformation is $\\eta = g^{-1}(E)$. The PDF of $E$, denoted $p_{E}(E)$, can be found using the following relationship:\n$$\np_{E}(E) = p_{\\eta}\\left(g^{-1}(E)\\right) \\left| \\frac{\\mathrm{d}}{\\mathrm{d}E} g^{-1}(E) \\right|\n$$\nThis formula ensures that the probability element is conserved, i.e., $|p_{E}(E)\\,\\mathrm{d}E| = |p_{\\eta}(\\eta)\\,\\mathrm{d}\\eta|$.\n\nIn the context of this problem, we are given:\n1.  The transformation from the latent parameter $\\eta$ to the physical parameter $E$: $E = \\exp(\\eta)$.\n2.  The inverse transformation is therefore $\\eta = \\ln(E)$. So, $g^{-1}(E) = \\ln(E)$.\n3.  The prior density on $\\eta$, which is a Gaussian distribution with mean $\\mu$ and standard deviation $\\sigma$:\n$$\np_{\\eta}(\\eta) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\eta - \\mu)^{2}}{2\\sigma^{2}}\\right)\n$$\nThe domain for $\\eta$ is $(-\\infty, \\infty)$. Because $E = \\exp(\\eta)$, the domain for $E$ is $(0, \\infty)$, which is consistent with the physical requirement that the Young's modulus must be strictly positive.\n\nFirst, we calculate the Jacobian determinant of the transformation, which is the absolute value of the derivative of the inverse transformation, $\\left| \\frac{\\mathrm{d}\\eta}{\\mathrm{d}E} \\right|$.\n$$\n\\eta(E) = \\ln(E)\n$$\nThe derivative is:\n$$\n\\frac{\\mathrm{d}\\eta}{\\mathrm{d}E} = \\frac{\\mathrm{d}}{\\mathrm{d}E} \\left( \\ln(E) \\right) = \\frac{1}{E}\n$$\nSince $E > 0$, the absolute value is simply:\n$$\n\\left| \\frac{\\mathrm{d}\\eta}{\\mathrm{d}E} \\right| = \\left| \\frac{1}{E} \\right| = \\frac{1}{E}\n$$\n\nNext, we substitute $\\eta = \\ln(E)$ into the expression for $p_{\\eta}(\\eta)$:\n$$\np_{\\eta}(\\ln(E)) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln(E) - \\mu)^{2}}{2\\sigma^{2}}\\right)\n$$\n\nNow, we assemble the final expression for $p_{E}(E)$ by multiplying $p_{\\eta}(\\ln(E))$ by the Jacobian term $\\left| \\frac{\\mathrm{d}\\eta}{\\mathrm{d}E} \\right|$:\n$$\np_{E}(E) = p_{\\eta}(\\ln(E)) \\left| \\frac{\\mathrm{d}\\eta}{\\mathrm{d}E} \\right| = \\left( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln(E) - \\mu)^{2}}{2\\sigma^{2}}\\right) \\right) \\cdot \\left( \\frac{1}{E} \\right)\n$$\nRearranging the terms gives the final closed-form expression for the prior density on $E$:\n$$\np_{E}(E) = \\frac{1}{E \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln(E) - \\mu)^{2}}{2\\sigma^{2}}\\right)\n$$\nThis is the probability density function of a log-normal distribution, which is defined for $E > 0$.\n\nThe problem asks to determine the normalization constant such that $\\int_{0}^{\\infty} p_{E}(E)\\,\\mathrm{d}E = 1$. The derivation using the change of variables formula automatically preserves normalization. If $p_{\\eta}(\\eta)$ is normalized to $1$, then the resulting $p_{E}(E)$ is also normalized to $1$. The integral of the Gaussian prior $p_{\\eta}(\\eta)$ from $-\\infty$ to $\\infty$ is $1$. By performing a change of variables in the integral $\\int_{0}^{\\infty} p_{E}(E)\\,\\mathrm{d}E$ with the substitution $u = \\ln(E)$ (so $\\mathrm{d}u = \\frac{1}{E} \\mathrm{d}E$, and the limits transform from $(0, \\infty)$ to $(-\\infty, \\infty)$), the integral becomes $\\int_{-\\infty}^{\\infty} p_{\\eta}(u)\\,\\mathrm{d}u$, which confirms that the integral is indeed $1$. Therefore, the expression derived is already correctly normalized, and the \"normalization constant\" is implicitly contained within the expression, specifically the factor $\\frac{1}{\\sigma\\sqrt{2\\pi}}$.",
            "answer": "$$\\boxed{\\frac{1}{E \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{\\left(\\ln(E) - \\mu\\right)^{2}}{2\\sigma^{2}}\\right)}$$"
        },
        {
            "introduction": "The likelihood function quantifies how probable the observed data are for a given set of model parameters, forming the bridge between your computational model and experimental reality. While assuming constant measurement error is simple, it is often unrealistic, as experimental noise can vary with the magnitude of the signal. This exercise guides you through the derivation of a heteroscedastic likelihood function, where the noise variance is not constant but depends on the model's prediction . This skill is essential for constructing more accurate statistical models that better reflect the complexities of physical experiments.",
            "id": "3547176",
            "problem": "In a uniaxial tensile test of a metallic coupon, let the applied engineering strain data be $\\{\\varepsilon_{i}\\}_{i=1}^{n}$ and the corresponding measured Cauchy stress data be $\\{\\sigma_{i}^{\\text{obs}}\\}_{i=1}^{n}$. Consider a deterministic constitutive response map $\\sigma^{\\text{mod}}(\\varepsilon;\\theta)$, where $\\theta$ is a vector of unknown material parameters to be calibrated (for example, moduli and hardening parameters), and suppose the measurement model is \n$$\n\\sigma_{i}^{\\text{obs}} \\;=\\; \\sigma^{\\text{mod}}(\\varepsilon_{i};\\theta) \\;+\\; \\eta_{i},\n$$\nwhere the noise term $\\eta_{i}$ represents the discrepancy between the measured and modeled stress. Assume the following heteroscedastic noise structure: $\\eta_{i}$ are mutually independent, mean-zero, Gaussian random variables whose variances depend on both the strain $\\varepsilon_{i}$ and the parameters $\\theta$ through a known positive scaling function $f(\\varepsilon_{i},\\theta)$ and a positive scalar amplitude $\\alpha$, namely\n$$\n\\operatorname{Var}(\\eta_{i}) \\;=\\; \\alpha^{2}\\,f(\\varepsilon_{i},\\theta)^{2}.\n$$\nAssume that $\\alpha>0$ is known. Starting from the definition of independent Gaussian errors and the product form of the joint Probability Density Function (PDF), derive the heteroscedastic likelihood function $p(\\{\\sigma_{i}^{\\text{obs}}\\}_{i=1}^{n}\\mid \\theta)$ implied by the above model, and then write the corresponding log-likelihood function $L(\\theta)$ as a function of $\\theta$. Your final result should be a single closed-form analytic expression for $L(\\theta)$ in terms of $\\{\\sigma_{i}^{\\text{obs}}\\}_{i=1}^{n}$, $\\{\\varepsilon_{i}\\}_{i=1}^{n}$, $\\sigma^{\\text{mod}}(\\cdot;\\theta)$, $f(\\cdot,\\theta)$, and $\\alpha$. No numerical evaluation is required. If you drop any additive constants that do not depend on $\\theta$, clearly indicate this in your derivation, but your final expression for $L(\\theta)$ should include all terms. The final answer must be a single expression without units.",
            "solution": "The problem is valid as it presents a well-posed, scientifically grounded, and standard task in the field of statistical inference for material model calibration. It is self-contained and free of contradictions or ambiguities. We proceed with the derivation.\n\nThe objective is to derive the log-likelihood function $L(\\theta)$ for the material parameters $\\theta$ given the observed stress data $\\{\\sigma_{i}^{\\text{obs}}\\}_{i=1}^{n}$ corresponding to applied strains $\\{\\varepsilon_{i}\\}_{i=1}^{n}$.\n\nThe measurement model is given by\n$$\n\\sigma_{i}^{\\text{obs}} \\;=\\; \\sigma^{\\text{mod}}(\\varepsilon_{i};\\theta) \\;+\\; \\eta_{i}\n$$\nwhere $i = 1, 2, \\ldots, n$. The noise term $\\eta_{i}$ for each measurement is a random variable. The properties of the noise terms are specified as:\n1.  They are mutually independent.\n2.  They are drawn from a Gaussian (Normal) distribution with a mean of zero, $E[\\eta_{i}] = 0$.\n3.  Their variance is heteroscedastic, given by $\\operatorname{Var}(\\eta_{i}) = \\alpha^{2}\\,f(\\varepsilon_{i},\\theta)^{2}$, where $\\alpha > 0$ is a known constant and $f(\\varepsilon_{i},\\theta)$ is a known positive function.\n\nFrom these properties, we can characterize the probability distribution of each noise term $\\eta_{i}$. The probability density function (PDF) of a Gaussian random variable $x$ with mean $\\mu$ and variance $\\sigma^2$ is\n$$\np(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right).\n$$\nFor the noise term $\\eta_i$, we have $\\mu=0$ and $\\sigma^2 = \\operatorname{Var}(\\eta_{i}) = \\alpha^{2}\\,f(\\varepsilon_{i},\\theta)^{2}$. Therefore, the PDF of $\\eta_i$ is\n$$\np(\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\alpha^{2}\\,f(\\varepsilon_{i},\\theta)^{2}}} \\exp\\left(-\\frac{\\eta_i^2}{2\\alpha^{2}\\,f(\\varepsilon_{i},\\theta)^{2}}\\right).\n$$\nThe observed stress $\\sigma_{i}^{\\text{obs}}$ is a linear transformation of the random variable $\\eta_{i}$: $\\sigma_{i}^{\\text{obs}} = \\eta_{i} + C$, where $C = \\sigma^{\\text{mod}}(\\varepsilon_{i};\\theta)$ is a constant for a given $\\theta$ and $\\varepsilon_i$. A linear transformation of a Gaussian random variable results in another Gaussian random variable. The mean of $\\sigma_{i}^{\\text{obs}}$ is\n$$\nE[\\sigma_{i}^{\\text{obs}}] = E[\\sigma^{\\text{mod}}(\\varepsilon_{i};\\theta) + \\eta_{i}] = \\sigma^{\\text{mod}}(\\varepsilon_{i};\\theta) + E[\\eta_{i}] = \\sigma^{\\text{mod}}(\\varepsilon_{i};\\theta).\n$$\nThe variance of $\\sigma_{i}^{\\text{obs}}$ is\n$$\n\\operatorname{Var}(\\sigma_{i}^{\\text{obs}}) = \\operatorname{Var}(\\sigma^{\\text{mod}}(\\varepsilon_{i};\\theta) + \\eta_{i}) = \\operatorname{Var}(\\eta_{i}) = \\alpha^{2}\\,f(\\varepsilon_{i},\\theta)^{2}.\n$$\nThus, for a given value of the parameter vector $\\theta$, each observation $\\sigma_{i}^{\\text{obs}}$ follows a Gaussian distribution with mean $\\sigma^{\\text{mod}}(\\varepsilon_{i};\\theta)$ and variance $\\alpha^{2}\\,f(\\varepsilon_{i},\\theta)^{2}$. The PDF for a single observation $\\sigma_{i}^{\\text{obs}}$, conditioned on $\\theta$, is the individual likelihood:\n$$\np(\\sigma_{i}^{\\text{obs}} \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\alpha^{2}\\,f(\\varepsilon_{i},\\theta)^{2}}} \\exp\\left(-\\frac{(\\sigma_{i}^{\\text{obs}} - \\sigma^{\\text{mod}}(\\varepsilon_{i};\\theta))^2}{2\\alpha^{2}\\,f(\\varepsilon_{i},\\theta)^{2}}\\right).\n$$\nSince $\\alpha > 0$ and $f(\\cdot,\\cdot) > 0$, the term in the square root can be simplified: $\\sqrt{2\\pi\\alpha^{2}\\,f(\\varepsilon_{i},\\theta)^{2}} = \\alpha f(\\varepsilon_{i},\\theta) \\sqrt{2\\pi}$.\n$$\np(\\sigma_{i}^{\\text{obs}} \\mid \\theta) = \\frac{1}{\\alpha f(\\varepsilon_{i},\\theta) \\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2\\alpha^2} \\left[ \\frac{\\sigma_{i}^{\\text{obs}} - \\sigma^{\\text{mod}}(\\varepsilon_{i},\\theta)}{f(\\varepsilon_{i},\\theta)} \\right]^2 \\right).\n$$\nThe problem states that the noise terms $\\eta_i$ are mutually independent. This implies that the observations $\\{\\sigma_{i}^{\\text{obs}}\\}_{i=1}^{n}$ are also mutually independent, conditioned on $\\theta$. Therefore, the joint likelihood function for the entire set of observations is the product of the individual likelihoods:\n$$\np(\\{\\sigma_{i}^{\\text{obs}}\\}_{i=1}^{n} \\mid \\theta) = \\prod_{i=1}^{n} p(\\sigma_{i}^{\\text{obs}} \\mid \\theta) = \\prod_{i=1}^{n} \\left( \\frac{1}{\\alpha f(\\varepsilon_{i},\\theta) \\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2\\alpha^2} \\left[ \\frac{\\sigma_{i}^{\\text{obs}} - \\sigma^{\\text{mod}}(\\varepsilon_{i},\\theta)}{f(\\varepsilon_{i},\\theta)} \\right]^2 \\right) \\right).\n$$\nThe log-likelihood function $L(\\theta)$ is the natural logarithm of the likelihood function:\n$$\nL(\\theta) = \\ln\\left( p(\\{\\sigma_{i}^{\\text{obs}}\\}_{i=1}^{n} \\mid \\theta) \\right) = \\ln\\left( \\prod_{i=1}^{n} p(\\sigma_{i}^{\\text{obs}} \\mid \\theta) \\right).\n$$\nUsing the property of logarithms that $\\ln(\\prod a_i) = \\sum \\ln(a_i)$, we get:\n$$\nL(\\theta) = \\sum_{i=1}^{n} \\ln\\left( p(\\sigma_{i}^{\\text{obs}} \\mid \\theta) \\right) = \\sum_{i=1}^{n} \\ln\\left( \\frac{1}{\\alpha f(\\varepsilon_{i},\\theta) \\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2\\alpha^2} \\left[ \\frac{\\sigma_{i}^{\\text{obs}} - \\sigma^{\\text{mod}}(\\varepsilon_{i},\\theta)}{f(\\varepsilon_{i},\\theta)} \\right]^2 \\right) \\right).\n$$\nUsing the properties $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(e^x)=x$:\n$$\nL(\\theta) = \\sum_{i=1}^{n} \\left[ \\ln\\left( \\frac{1}{\\alpha f(\\varepsilon_{i},\\theta) \\sqrt{2\\pi}} \\right) - \\frac{1}{2\\alpha^2} \\left( \\frac{\\sigma_{i}^{\\text{obs}} - \\sigma^{\\text{mod}}(\\varepsilon_{i},\\theta)}{f(\\varepsilon_{i},\\theta)} \\right)^2 \\right].\n$$\nFurther simplifying the logarithm term using $\\ln(1/a) = -\\ln(a)$ and $\\ln(abc) = \\ln(a)+\\ln(b)+\\ln(c)$:\n$$\n\\ln\\left( \\frac{1}{\\alpha f(\\varepsilon_{i},\\theta) \\sqrt{2\\pi}} \\right) = - \\ln(\\alpha f(\\varepsilon_{i},\\theta) \\sqrt{2\\pi}) = -\\left( \\ln(\\alpha) + \\ln(f(\\varepsilon_{i},\\theta)) + \\ln(\\sqrt{2\\pi}) \\right).\n$$\nSince $\\ln(\\sqrt{2\\pi}) = \\frac{1}{2}\\ln(2\\pi)$, this becomes:\n$$\n-\\ln(\\alpha) - \\ln(f(\\varepsilon_{i},\\theta)) - \\frac{1}{2}\\ln(2\\pi).\n$$\nSubstituting this back into the expression for $L(\\theta)$:\n$$\nL(\\theta) = \\sum_{i=1}^{n} \\left[ -\\ln(\\alpha) - \\ln(f(\\varepsilon_{i},\\theta)) - \\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2\\alpha^2} \\left( \\frac{\\sigma_{i}^{\\text{obs}} - \\sigma^{\\text{mod}}(\\varepsilon_{i},\\theta)}{f(\\varepsilon_{i},\\theta)} \\right)^2 \\right].\n$$\nFinally, we can separate the terms inside the summation. The terms $-\\ln(\\alpha)$ and $-\\frac{1}{2}\\ln(2\\pi)$ do not depend on the index $i$, so their sum over $n$ terms is simply $n$ times the term.\n$$\nL(\\theta) = -n\\ln(\\alpha) - \\frac{n}{2}\\ln(2\\pi) - \\sum_{i=1}^{n} \\ln(f(\\varepsilon_{i},\\theta)) - \\frac{1}{2\\alpha^2} \\sum_{i=1}^{n} \\left( \\frac{\\sigma_{i}^{\\text{obs}} - \\sigma^{\\text{mod}}(\\varepsilon_{i},\\theta)}{f(\\varepsilon_{i},\\theta)} \\right)^2.\n$$\nCombining the constant terms, we have $-n(\\ln(\\alpha) + \\frac{1}{2}\\ln(2\\pi)) = -n\\ln(\\alpha\\sqrt{2\\pi}) = -\\frac{n}{2}\\ln(2\\pi\\alpha^2)$. This gives the final expression for the log-likelihood function, including all terms as requested.\n$$\nL(\\theta) = - \\frac{n}{2}\\ln(2\\pi\\alpha^2) - \\sum_{i=1}^{n} \\ln(f(\\varepsilon_{i},\\theta)) - \\frac{1}{2\\alpha^2} \\sum_{i=1}^{n} \\left[ \\frac{\\sigma_{i}^{\\text{obs}} - \\sigma^{\\text{mod}}(\\varepsilon_{i},\\theta)}{f(\\varepsilon_{i},\\theta)} \\right]^2.\n$$\nThis is the required closed-form analytic expression for the log-likelihood function $L(\\theta)$.",
            "answer": "$$\n\\boxed{\nL(\\theta) = - \\frac{n}{2}\\ln(2\\pi\\alpha^2) - \\sum_{i=1}^{n} \\ln(f(\\varepsilon_{i},\\theta)) - \\frac{1}{2\\alpha^2} \\sum_{i=1}^{n} \\left( \\frac{\\sigma_{i}^{\\text{obs}} - \\sigma^{\\text{mod}}(\\varepsilon_{i},\\theta)}{f(\\varepsilon_{i},\\theta)} \\right)^2\n}\n$$"
        },
        {
            "introduction": "The power of Bayesian inference truly shines when applied to complex, high-fidelity computational models, such as those built using the Finite Element (FE) method. A major challenge, however, is efficiently computing the gradient of the log-likelihood with respect to the model parameters, which is essential for modern sampling algorithms. This capstone practice introduces the adjoint method, a powerful and computationally efficient technique for calculating these gradients for implicit models . By deriving the adjoint equations and implementing a full computational pipeline for a nonlinear hyperelastic bar, you will integrate advanced numerical mechanics with statistical inference, a key skill for state-of-the-art research in the field.",
            "id": "3547183",
            "problem": "Consider a one-dimensional, nondimensionalized hyperelastic bar occupying the reference domain $[0,L]$ with $L=1$. The bar is discretized using the Finite Element (FE) method with $N_e$ linear elements and $N_n=N_e+1$ nodes. The nodal displacement field $u(X)$ defines the current position $x=X+u(X)$ and the deformation gradient $F=1+\\frac{du}{dX}$. The cross-sectional area is $A=1$ and all quantities are nondimensional, so no physical units are involved.\n\nAssume a compressible Neo-Hookean stored energy density\n$$\nW(F;\\theta)=\\frac{\\mu}{2}\\left(F^2-1\\right)-\\mu\\ln F+\\frac{\\lambda}{2}\\left(\\ln F\\right)^2,\n$$\nwhere the material parameters are $\\theta=\\{\\lambda,\\mu\\}$. The first Piola-Kirchhoff stress is\n$$\nP(F;\\theta)=\\frac{\\partial W}{\\partial F}=\\mu F-\\frac{\\mu}{F}+\\frac{\\lambda \\ln F}{F}.\n$$\nThe bar is subject to prescribed displacement boundary conditions $u(0)=0$ and $u(L)=U_0$, with no body forces and no applied tractions. The FE internal force residual $R(u,\\theta)$ for the free nodal degrees of freedom is obtained from the principle of virtual work. Using one-point quadrature and linear shape functions on each element of length $h=L/N_e$, the elemental internal force contributions are $[-P(F;\\theta),+P(F;\\theta)]$ assembled to the global residual. The tangent matrix $A=\\frac{\\partial R}{\\partial u}$ is obtained by differentiating the residual with respect to the free nodal displacements and involves $\\frac{\\partial P}{\\partial F}$ via $\\frac{\\partial F}{\\partial u}$.\n\nYou observe stresses at the element midpoints, collected in the data vector $D\\in\\mathbb{R}^m$ with $m=N_e$. The forward map $f(\\theta)$ computes the FE solution $u(\\theta)$ consistent with the boundary conditions and returns the predicted stress values $P(F_e;\\theta)$ for each element $e=1,\\dots,N_e$. The likelihood of the data given the parameters is Gaussian, with independent noise of variance $\\sigma^2$ at each measurement:\n$$\np(D\\mid \\theta)\\propto \\exp\\left(-\\frac{1}{2}\\left(D-f(\\theta)\\right)^{\\!\\top}\\Sigma^{-1}\\left(D-f(\\theta)\\right)\\right),\\quad \\Sigma=\\sigma^2 I_m.\n$$\nYour tasks are:\n- Starting from the principle of virtual work and the Gaussian likelihood definition, derive the adjoint-based gradient $\\frac{\\partial \\log p(D\\mid\\theta)}{\\partial \\theta}$ that avoids explicit computation of $\\frac{du}{d\\theta}$. Express the adjoint equation and the final gradient formula in terms of $\\frac{\\partial f}{\\partial \\theta}$, $\\frac{\\partial f}{\\partial u}$, $\\frac{\\partial R}{\\partial u}$, and $\\frac{\\partial R}{\\partial \\theta}$.\n- Implement a complete program that:\n  1. Builds the FE residual $R(u,\\theta)$ and tangent $A=\\frac{\\partial R}{\\partial u}$ for the $1$D bar using the given $W(F;\\theta)$ and $P(F;\\theta)$.\n  2. Solves the nonlinear FE equilibrium $R(u,\\theta)=0$ with the specified displacement boundary conditions using Newton's method.\n  3. Evaluates the forward map $f(\\theta)$, its sensitivities $\\frac{\\partial f}{\\partial \\theta}$ and $\\frac{\\partial f}{\\partial u}$, assembles the adjoint system, solves for the adjoint, and computes $\\frac{\\partial \\log p(D\\mid\\theta)}{\\partial \\theta}$ using your derived formula.\n\nUse the following test suite of cases, with all quantities nondimensional:\n- Case $1$ (general): $N_e=4$, $U_0=0.2$, $\\sigma=0.05$, candidate parameters $\\theta=\\{\\lambda=8.0,\\mu=4.0\\}$, and data $D=[3.0274117,\\,2.9974117,\\,3.0324117,\\,3.0124117]$.\n- Case $2$ (finite strain): $N_e=3$, $U_0=0.5$, $\\sigma=0.08$, candidate parameters $\\theta=\\{\\lambda=12.0,\\mu=6.0\\}$, and data $D=[8.545232495,\\,8.515232495,\\,8.525232495]$.\n- Case $3$ (edge, near identity stretch): $N_e=2$, $U_0=0.0$, $\\sigma=0.03$, candidate parameters $\\theta=\\{\\lambda=10.0,\\mu=5.0\\}$, and data $D=[0.003,\\,-0.004]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be the two-component gradient vector $[\\frac{\\partial \\log p}{\\partial \\lambda},\\frac{\\partial \\log p}{\\partial \\mu}]$ for that case, so the overall output is a list of lists of floats in the order of the test suite cases, for example $[[g_{\\lambda,1},g_{\\mu,1}],[g_{\\lambda,2},g_{\\mu,2}],[g_{\\lambda,3},g_{\\mu,3}]]$.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of computational solid mechanics and Bayesian inference, well-posed, objective, and self-contained. All necessary data and definitions for a unique solution are provided.\n\nThe core of the problem is to derive and implement the adjoint-based gradient of the log-likelihood function for a set of material parameters $\\theta$. This allows for efficient sensitivity analysis, which is fundamental to gradient-based optimization and sampling methods in Bayesian calibration.\n\n### Derivation of the Adjoint-Based Gradient\n\nLet the vector of material parameters be $\\theta = \\{\\lambda, \\mu\\}$. The observations are collected in the data vector $D$. The forward model $f(\\theta)$ predicts these observations. The state of the system, represented by the vector of free nodal displacements $u$, is implicitly dependent on $\\theta$ through the nonlinear finite element equilibrium equation, $R(u, \\theta) = 0$.\n\nThe log-likelihood function, neglecting constant terms, is given by:\n$$\nJ(\\theta) \\equiv \\log p(D \\mid \\theta) = -\\frac{1}{2\\sigma^2} (D - f(u(\\theta), \\theta))^T (D - f(u(\\theta), \\theta))\n$$\nOur goal is to compute the total derivative of $J$ with respect to $\\theta$, denoted as $\\frac{dJ}{d\\theta}$. Applying the chain rule, we obtain:\n$$\n\\frac{dJ}{d\\theta} = \\frac{\\partial J}{\\partial \\theta} + \\frac{\\partial J}{\\partial u} \\frac{du}{d\\theta}\n$$\nwhere $\\frac{\\partial J}{\\partial \\theta}$ and $\\frac{\\partial J}{\\partial u}$ are partial derivatives, and $\\frac{du}{d\\theta}$ is the sensitivity of the displacements with respect to the parameters. The term $\\frac{du}{d\\theta}$ is computationally expensive to evaluate directly. The adjoint method provides an elegant way to bypass its explicit computation.\n\nWe find an expression for $\\frac{du}{d\\theta}$ by taking the total derivative of the equilibrium equation $R(u, \\theta) = 0$ with respect to $\\theta$:\n$$\n\\frac{dR}{d\\theta} = \\frac{\\partial R}{\\partial \\theta} + \\frac{\\partial R}{\\partial u} \\frac{du}{d\\theta} = 0\n$$\nAssuming the tangent stiffness matrix $A = \\frac{\\partial R}{\\partial u}$ is invertible, we can write:\n$$\n\\frac{du}{d\\theta} = - \\left(\\frac{\\partial R}{\\partial u}\\right)^{-1} \\frac{\\partial R}{\\partial \\theta}\n$$\nSubstituting this into the expression for $\\frac{dJ}{d\\theta}$ gives the direct sensitivity expression:\n$$\n\\frac{dJ}{d\\theta} = \\frac{\\partial J}{\\partial \\theta} - \\frac{\\partial J}{\\partial u} \\left(\\frac{\\partial R}{\\partial u}\\right)^{-1} \\frac{\\partial R}{\\partial \\theta}\n$$\nThis expression still requires solving a linear system for each parameter in $\\theta$. To obtain the adjoint formulation, we rearrange the terms. Let us define a new vector, the adjoint vector $\\psi$, as the solution to the following linear system, called the adjoint equation:\n$$\n\\left(\\frac{\\partial R}{\\partial u}\\right)^T \\psi = \\left(\\frac{\\partial J}{\\partial u}\\right)^T\n$$\nThis definition allows us to write $\\frac{\\partial J}{\\partial u} = \\psi^T \\frac{\\partial R}{\\partial u}$. Substituting this into the gradient expression:\n$$\n\\frac{dJ}{d\\theta} = \\frac{\\partial J}{\\partial \\theta} - \\left(\\psi^T \\frac{\\partial R}{\\partial u}\\right) \\left(\\frac{\\partial R}{\\partial u}\\right)^{-1} \\frac{\\partial R}{\\partial \\theta} = \\frac{\\partial J}{\\partial \\theta} - \\psi^T \\frac{\\partial R}{\\partial \\theta}\n$$\nThis is the final adjoint-based formula for the gradient. To use it, we need the partial derivatives of $J$. From the definition of $J$:\n$$\n\\frac{\\partial J}{\\partial u} = \\frac{1}{\\sigma^2} (D - f)^T \\frac{\\partial f}{\\partial u}\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\frac{1}{\\sigma^2} (D - f)^T \\frac{\\partial f}{\\partial \\theta}\n$$\nSubstituting these into our main formulas gives the operational equations:\n\n1.  **Adjoint Equation**:\n    $$\n    \\left(\\frac{\\partial R}{\\partial u}\\right)^T \\psi = \\frac{1}{\\sigma^2} \\left(\\frac{\\partial f}{\\partial u}\\right)^T (D - f)\n    $$\n2.  **Gradient Formula**:\n    $$\n    \\frac{dJ}{d\\theta} = \\frac{1}{\\sigma^2} (D - f)^T \\frac{\\partial f}{\\partial \\theta} - \\psi^T \\frac{\\partial R}{\\partial \\theta}\n    $$\n\n### Algorithmic Implementation\n\nThe computation proceeds in the following steps:\n1.  **Forward Solve**: For a given set of parameters $\\theta$, solve the nonlinear system of equations $R(u, \\theta) = 0$ for the displacement vector $u$ of the free degrees of freedom. This is accomplished using Newton's method, which iteratively solves $A \\Delta u = -R$, where $A = \\frac{\\partial R}{\\partial u}$ is the tangent stiffness matrix.\n2.  **Compute Model Prediction**: Using the converged displacement field $u$, calculate the deformation gradient $F_e$ and the first Piola-Kirchhoff stress $P_e$ for each element $e$. The vector of these stresses constitutes the forward model prediction, $f(\\theta) = [P_1, \\dots, P_{N_e}]^T$.\n3.  **Compute Sensitivities**: Evaluate all required partial derivatives at the converged state $(u, \\theta)$:\n    - $\\frac{\\partial R}{\\partial u}$: The tangent stiffness matrix $A$, which is available from the final step of the Newton solver.\n    - $\\frac{\\partial R}{\\partial \\theta}$: The sensitivity of the residual to the parameters.\n    - $\\frac{\\partial f}{\\partial u}$: The sensitivity of the predicted stresses to the displacements.\n    - $\\frac{\\partial f}{\\partial \\theta}$: The sensitivity of the predicted stresses to the parameters.\n4.  **Adjoint Solve**: Assemble the right-hand side of the adjoint equation, $b_{adj} = \\frac{1}{\\sigma^2} (\\frac{\\partial f}{\\partial u})^T (D - f)$, and solve the linear system $A^T \\psi = b_{adj}$ for the adjoint vector $\\psi$.\n5.  **Gradient Assembly**: Compute the two terms of the gradient formula and combine them to get the final gradient vector $\\frac{dJ}{d\\theta}$.\n\nFor the specific 1D bar problem with prescribed end displacements and no body forces, the system's behavior simplifies considerably. The equilibrium solution is a linear displacement field, leading to a uniform deformation gradient $F$ across all elements. Crucially, this uniform strain state is determined solely by the boundary conditions ($F = 1 + U_0/L$) and is therefore independent of the material parameters $\\theta$. As a result, the forward model prediction $f(\\theta)$ (the vector of element stresses) is also independent of the free nodal displacements $u$. This implies that the sensitivity of the prediction to the free displacements, $\\frac{\\partial f}{\\partial u}$, is zero. When substituted into the adjoint equation, this forces the adjoint vector $\\psi$ to be zero. Consequently, the gradient term $\\psi^T \\frac{\\partial R}{\\partial \\theta}$ vanishes, and the total gradient simplifies to $\\frac{dJ}{d\\theta} = \\frac{\\partial J}{\\partial \\theta}$. The provided Python code implements the general adjoint formulation, which correctly and naturally handles this special case without requiring this a priori analysis.",
            "answer": "```python\nimport numpy as np\n\ndef solve_fe(params, Ne, U0):\n    \"\"\"\n    Solves the nonlinear FE equilibrium equation R(u, theta) = 0 using Newton's method.\n    \"\"\"\n    lambda_, mu = params\n    L = 1.0\n    h = L / Ne\n    num_free_dof = Ne - 1\n\n    if num_free_dof <= 0:\n        # Case with no free DOFs (e.g., Ne=1)\n        u = np.linspace(0, U0, Ne + 1)\n        F = 1.0 + np.diff(u) / h\n        lnF = np.log(F)\n        P_prime = mu + mu / F**2 + lambda_ * (1 - lnF) / F**2\n        A = np.array([[]]) # No tangent for free dofs\n        return u, A, True\n\n    # Initial guess: linear displacement profile\n    u_free = np.linspace(0, U0, Ne + 1)[1:-1]\n    \n    max_iter = 20\n    tol = 1e-10\n\n    A = np.zeros((num_free_dof, num_free_dof))\n\n    for _ in range(max_iter):\n        u = np.concatenate(([0], u_free, [U0]))\n        \n        u_diff = np.diff(u)\n        F = 1.0 + u_diff / h\n        \n        if np.any(F <= 0):\n            return None, None, False\n\n        lnF = np.log(F)\n        P = mu * F - mu / F + lambda_ * lnF / F\n        \n        R_free = P[1:] - P[:-1]\n        \n        norm_R = np.linalg.norm(R_free)\n        if norm_R < tol:\n            # Converged. Re-compute tangent for return.\n            P_prime = mu + mu / F**2 + lambda_ * (1 - lnF) / F**2\n            diag = -(P_prime[1:] + P_prime[:-1]) / h\n            sup_diag = P_prime[1:-1] / h\n            sub_diag = P_prime[1:-1] / h\n            A = np.diag(diag) + np.diag(sup_diag, k=1) + np.diag(sub_diag, k=-1)\n            return u, A, True\n\n        P_prime = mu + mu / F**2 + lambda_ * (1 - lnF) / F**2\n        \n        diag = -(P_prime[1:] + P_prime[:-1]) / h\n        A = np.diag(diag)\n        if num_free_dof > 1:\n            sup_diag = P_prime[1:-1] / h\n            sub_diag = P_prime[1:-1] / h\n            A += np.diag(sup_diag, k=1) + np.diag(sub_diag, k=-1)\n\n        delta_u = np.linalg.solve(A, -R_free)\n        u_free += delta_u\n            \n    return None, None, False # Did not converge\n\ndef calculate_gradient(params, Ne, U0, sigma, D):\n    \"\"\"\n    Calculates the adjoint-based gradient of the log-likelihood.\n    \"\"\"\n    lambda_, mu = params\n    h = 1.0 / Ne\n    num_free_dof = Ne - 1\n    \n    # 1. Forward Solve\n    u, A, converged = solve_fe(params, Ne, U0)\n    if not converged:\n        raise RuntimeError(\"FE solver did not converge for the given parameters.\")\n\n    # 2. Compute Model Prediction f(theta) and its derivatives\n    u_diff = np.diff(u)\n    F = 1.0 + u_diff / h\n    lnF = np.log(F)\n    \n    f_theta = mu * F - mu / F + lambda_ * lnF / F\n    misfit = D - f_theta\n    \n    # 3. Compute Sensitivities\n    # Partial derivative of P wrt F\n    P_prime = mu + mu / F**2 + lambda_ * (1 - lnF) / F**2\n\n    # df/dtheta (Ne x 2 matrix)\n    df_dlambda = lnF / F\n    df_dmu = F - 1.0 / F\n    df_dtheta = np.vstack((df_dlambda, df_dmu)).T\n    \n    if num_free_dof > 0:\n        # dR/dtheta ((Ne-1) x 2 matrix)\n        dR_dlambda = df_dlambda[1:] - df_dlambda[:-1]\n        dR_dmu = df_dmu[1:] - df_dmu[:-1]\n        dR_dtheta = np.vstack((dR_dlambda, dR_dmu)).T\n\n        # df/du_free (Ne x (Ne-1) matrix)\n        # u_free[j] corresponds to nodal displacement u_{j+1}\n        # df_e/du_free[j] = dP_e/dF_e * dF_e/du_{j+1}\n        df_du_free = np.zeros((Ne, num_free_dof))\n        for e in range(Ne): # over elements (rows)\n            # Contribution from u_{e+1} dependency\n            if e < num_free_dof: # u_{e+1} is u_free[e]\n                df_du_free[e, e] += P_prime[e] / h\n            # Contribution from u_e dependency\n            if e > 0 and (e - 1) < num_free_dof: # u_e is u_free[e-1]\n                df_du_free[e, e - 1] -= P_prime[e] / h\n        \n        # 4. Adjoint Solve\n        b_adj = df_du_free.T @ misfit / (sigma**2)\n        psi = np.linalg.solve(A.T, b_adj)\n        \n        # 5. Gradient Assembly\n        term1 = (misfit @ df_dtheta) / (sigma**2)\n        term2 = psi.T @ dR_dtheta\n        grad = term1 - term2\n    else: # No free dofs, gradient has no adjoint part\n        term1 = (misfit @ df_dtheta) / (sigma**2)\n        grad = term1\n\n    return grad.tolist()\n\ndef solve():\n    test_cases = [\n        {'Ne': 4, 'U0': 0.2, 'sigma': 0.05, 'params': (8.0, 4.0), \n         'D': np.array([3.0274117, 2.9974117, 3.0324117, 3.0124117])},\n        {'Ne': 3, 'U0': 0.5, 'sigma': 0.08, 'params': (12.0, 6.0), \n         'D': np.array([8.545232495, 8.515232495, 8.525232495])},\n        {'Ne': 2, 'U0': 0.0, 'sigma': 0.03, 'params': (10.0, 5.0), \n         'D': np.array([0.003, -0.004])},\n    ]\n\n    results = []\n    for case in test_cases:\n        grad = calculate_gradient(case['params'], case['Ne'], case['U0'], case['sigma'], case['D'])\n        results.append(grad)\n\n    # Format the output exactly as required.\n    # e.g., [[g_lam1,g_mu1],[g_lam2,g_mu2],[g_lam3,g_mu3]]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}