{
    "hands_on_practices": [
        {
            "introduction": "The core challenge that hyperreduction aims to solve is the efficient approximation of high-dimensional nonlinear vectors, such as the internal force in a finite element model. This foundational exercise contrasts two seminal approaches: gappy Proper Orthogonal Decomposition (POD), which relies on a least-squares fit over a set of sampled entries, and the Discrete Empirical Interpolation Method (DEIM), which uses direct interpolation. By working through the calculations for a small, illustrative system, you will gain direct insight into how these distinct mathematical philosophies operate and how they result in different approximation errors .",
            "id": "3572677",
            "problem": "Consider a nonlinear finite element internal force vector $f(u) \\in \\mathbb{R}^{8}$ arising from the spatial discretization of a one-dimensional hyperelastic bar in computational solid mechanics. In a reduced-order modeling context, approximate the nonlinear internal force $f(u)$ in the span of a nonlinear term basis $U \\in \\mathbb{R}^{8 \\times 3}$ constructed from snapshots. Hyperreduction techniques seek to recover the coefficients using only a subset of entries of $f(u)$.\n\nStart from the principle of virtual work and the finite element residual, which yields a discrete internal force $f(u)$ satisfying equilibrium $f(u) - F_{\\mathrm{ext}} = 0$, where $F_{\\mathrm{ext}}$ is the external force vector. A nonlinear reduced model represents the internal force as $f(u) \\approx U c(u)$ for some coefficient vector $c(u) \\in \\mathbb{R}^{3}$. Let a row-sampling operator be given by a selection matrix $P \\in \\mathbb{R}^{8 \\times 4}$ formed from standard basis vectors so that $P^{\\top} f(u)$ extracts $4$ entries of $f(u)$. In gappy Proper Orthogonal Decomposition (gappy POD), the coefficients are found by a least-squares fit that minimizes the sampled misfit $\\|P^{\\top}(Uc - f)\\|_{2}$. In the Discrete Empirical Interpolation Method (DEIM), the coefficients are instead determined by an interpolation at $r_{f}$ selected entries such that $P_{\\mathrm{DEIM}}^{\\top} U c = P_{\\mathrm{DEIM}}^{\\top} f$.\n\nUsing the following specific data that are consistent with the above framework, compute the gappy POD reconstruction $\\hat f$ and the DEIM reconstruction $\\tilde f$, and then compute the ratio of the Euclidean norms of their reconstruction errors:\n- Dimension $N = 8$, reduced nonlinear basis size $r_{f} = 3$, and $4$ sampled entries.\n- Nonlinear term basis $U = [u_{1}\\ u_{2}\\ u_{3}] \\in \\mathbb{R}^{8 \\times 3}$ with columns\n  $u_{1} = [1, 1, 1, 1, 1, 1, 1, 1]^{\\top}$,\n  $u_{2} = [1, 2, 3, 4, 5, 6, 7, 8]^{\\top}$,\n  $u_{3} = [1, 0, 1, 0, 1, 0, 1, 0]^{\\top}$.\n- Sampling selection $\\mathcal{S} = \\{2, 4, 6, 7\\}$, so $P = [e_{2}\\ e_{4}\\ e_{6}\\ e_{7}] \\in \\mathbb{R}^{8 \\times 4}$ where $e_{i}$ is the $i$-th standard basis vector in $\\mathbb{R}^{8}$.\n- Internal force vector $f \\in \\mathbb{R}^{8}$ given by\n  $f = U c^{\\star} + w$ with $c^{\\star} = [1, -\\tfrac{1}{2}, 2]^{\\top}$ and $w = \\tfrac{1}{2}[0, 1, 0, -1, 0, 1, 0, -1]^{\\top}$.\n- For the Discrete Empirical Interpolation Method (DEIM), use interpolation indices $\\{2, 6, 7\\}$, i.e., $P_{\\mathrm{DEIM}} = [e_{2}\\ e_{6}\\ e_{7}] \\in \\mathbb{R}^{8 \\times 3}$.\n\nDerive the coefficient recovery formulas from the least-squares principle for gappy Proper Orthogonal Decomposition (gappy POD) and the interpolation condition for the Discrete Empirical Interpolation Method (DEIM). Then compute explicitly:\n- The gappy POD reconstruction $\\hat f = U \\hat c$, where $\\hat c$ minimizes $\\|P^{\\top}(Uc - f)\\|_{2}$.\n- The DEIM reconstruction $\\tilde f = U \\tilde c$, where $\\tilde c$ satisfies $P_{\\mathrm{DEIM}}^{\\top} U \\tilde c = P_{\\mathrm{DEIM}}^{\\top} f$.\n\nFinally, evaluate the errors $e_{g} = \\|f - \\hat f\\|_{2}$ and $e_{d} = \\|f - \\tilde f\\|_{2}$, and report the ratio $e_{g}/e_{d}$ as a single closed-form analytic expression. No rounding is required. Express the final answer without units.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is based on established principles of reduced-order modeling and hyperreduction, specifically gappy Proper Orthogonal Decomposition (gappy POD) and the Discrete Empirical Interpolation Method (DEIM). All data and conditions required for a unique solution are provided and are self-consistent.\n\nThe problem asks for the computation of two approximations to a vector $f \\in \\mathbb{R}^{8}$, denoted $\\hat{f}$ (from gappy POD) and $\\tilde{f}$ (from DEIM), and the ratio of their reconstruction errors. Both approximations lie in the subspace spanned by the columns of a matrix $U \\in \\mathbb{R}^{8 \\times 3}$.\n\nFirst, we define the given quantities.\nThe nonlinear term basis is $U = [u_{1}\\ u_{2}\\ u_{3}]$, where\n$$\nu_{1} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nu_{2} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\end{pmatrix}, \\quad\nu_{3} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n\\implies\nU = \\begin{pmatrix}\n1  1  1 \\\\\n1  2  0 \\\\\n1  3  1 \\\\\n1  4  0 \\\\\n1  5  1 \\\\\n1  6  0 \\\\\n1  7  1 \\\\\n1  8  0\n\\end{pmatrix}\n$$\nThe target internal force vector is $f = U c^{\\star} + w$, with $c^{\\star} = \\begin{pmatrix} 1 \\\\ -1/2 \\\\ 2 \\end{pmatrix}$ and $w = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$. Let's compute $f$:\n$$\nU c^{\\star} = \\begin{pmatrix}\n1  1  1 \\\\\n1  2  0 \\\\\n1  3  1 \\\\\n1  4  0 \\\\\n1  5  1 \\\\\n1  6  0 \\\\\n1  7  1 \\\\\n1  8  0\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ -1/2 \\\\ 2 \\end{pmatrix}\n= \\begin{pmatrix}\n1 - 1/2 + 2 \\\\\n1 - 1 + 0 \\\\\n1 - 3/2 + 2 \\\\\n1 - 2 + 0 \\\\\n1 - 5/2 + 2 \\\\\n1 - 3 + 0 \\\\\n1 - 7/2 + 2 \\\\\n1 - 4 + 0\n\\end{pmatrix}\n= \\begin{pmatrix} 5/2 \\\\ 0 \\\\ 3/2 \\\\ -1 \\\\ 1/2 \\\\ -2 \\\\ -1/2 \\\\ -3 \\end{pmatrix}\n$$\n$$\nf = U c^{\\star} + w = \\begin{pmatrix} 5/2 \\\\ 0 \\\\ 3/2 \\\\ -1 \\\\ 1/2 \\\\ -2 \\\\ -1/2 \\\\ -3 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ -1/2 \\\\ 0 \\\\ 1/2 \\\\ 0 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} 5/2 \\\\ 1/2 \\\\ 3/2 \\\\ -3/2 \\\\ 1/2 \\\\ -3/2 \\\\ -1/2 \\\\ -7/2 \\end{pmatrix}\n$$\n\n**1. Gappy Proper Orthogonal Decomposition (Gappy POD)**\n\nThe gappy POD approximation is $\\hat f = U \\hat c$, where the coefficient vector $\\hat c \\in \\mathbb{R}^{3}$ is the solution to the least-squares problem:\n$$\n\\hat c = \\underset{c \\in \\mathbb{R}^{3}}{\\operatorname{argmin}} \\|P^{\\top}(Uc - f)\\|_{2}^{2}\n$$\nThe sampling matrix $P = [e_{2}\\ e_{4}\\ e_{6}\\ e_{7}]$ implies that the operator $P^{\\top}$ selects rows $2, 4, 6, 7$. Let's define the sampled matrix $U_{S} = P^{\\top}U$ and sampled vector $f_{S} = P^{\\top}f$.\n$$\nU_{S} = \\begin{pmatrix}\n1  2  0 \\\\\n1  4  0 \\\\\n1  6  0 \\\\\n1  7  1\n\\end{pmatrix}, \\quad\nf_{S} = \\begin{pmatrix} 1/2 \\\\ -3/2 \\\\ -3/2 \\\\ -1/2 \\end{pmatrix}\n$$\nThe least-squares solution is given by the normal equations:\n$$\n(U_{S}^{\\top} U_{S}) \\hat c = U_{S}^{\\top} f_{S}\n$$\nWe compute the components:\n$$\nU_{S}^{\\top} U_{S} = \\begin{pmatrix}\n1  1  1  1 \\\\\n2  4  6  7 \\\\\n0  0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n1  2  0 \\\\\n1  4  0 \\\\\n1  6  0 \\\\\n1  7  1\n\\end{pmatrix}\n= \\begin{pmatrix}\n4  19  1 \\\\\n19  105  7 \\\\\n1  7  1\n\\end{pmatrix}\n$$\n$$\nU_{S}^{\\top} f_{S} = \\begin{pmatrix}\n1  1  1  1 \\\\\n2  4  6  7 \\\\\n0  0  0  1\n\\end{pmatrix}\n\\begin{pmatrix} 1/2 \\\\ -3/2 \\\\ -3/2 \\\\ -1/2 \\end{pmatrix}\n= \\begin{pmatrix}\n1/2 - 3/2 - 3/2 - 1/2 \\\\\n1 - 6 - 9 - 7/2 \\\\\n-1/2\n\\end{pmatrix}\n= \\begin{pmatrix} -3 \\\\ -35/2 \\\\ -1/2 \\end{pmatrix}\n$$\nThe determinant of $U_{S}^{\\top} U_{S}$ is $4(105-49) - 19(19-7) + 1(133-105) = 224 - 228 + 28 = 24$. The matrix is invertible. The inverse is:\n$$\n(U_{S}^{\\top} U_{S})^{-1} = \\frac{1}{24} \\begin{pmatrix}\n56  -12  28 \\\\\n-12  3  -9 \\\\\n28  -9  59\n\\end{pmatrix}\n$$\nNow we solve for $\\hat c = (U_{S}^{\\top} U_{S})^{-1} (U_{S}^{\\top} f_{S})$:\n$$\n\\hat c = \\frac{1}{24} \\begin{pmatrix}\n56  -12  28 \\\\\n-12  3  -9 \\\\\n28  -9  59\n\\end{pmatrix}\n\\begin{pmatrix} -3 \\\\ -35/2 \\\\ -1/2 \\end{pmatrix}\n= \\frac{1}{24} \\begin{pmatrix}\n56(-3) - 12(-35/2) + 28(-1/2) \\\\\n-12(-3) + 3(-35/2) - 9(-1/2) \\\\\n28(-3) - 9(-35/2) + 59(-1/2)\n\\end{pmatrix}\n= \\frac{1}{24} \\begin{pmatrix}\n-168 + 210 - 14 \\\\\n36 - 105/2 + 9/2 \\\\\n-84 + 315/2 - 59/2\n\\end{pmatrix}\n$$\n$$\n\\hat c = \\frac{1}{24} \\begin{pmatrix}\n28 \\\\\n36 - 48 \\\\\n-84 + 128\n\\end{pmatrix}\n= \\frac{1}{24} \\begin{pmatrix}\n28 \\\\\n-12 \\\\\n44\n\\end{pmatrix}\n= \\begin{pmatrix} 7/6 \\\\ -1/2 \\\\ 11/6 \\end{pmatrix}\n$$\n\n**2. Discrete Empirical Interpolation Method (DEIM)**\n\nThe DEIM approximation is $\\tilde f = U \\tilde c$, where the coefficient vector $\\tilde c \\in \\mathbb{R}^{3}$ is the solution to the linear system:\n$$\nP_{\\mathrm{DEIM}}^{\\top} U \\tilde c = P_{\\mathrm{DEIM}}^{\\top} f\n$$\nThe DEIM selection matrix is $P_{\\mathrm{DEIM}} = [e_{2}\\ e_{6}\\ e_{7}]$, so $P_{\\mathrm{DEIM}}^{\\top}$ selects rows $2, 6, 7$. Let's define the interpolation matrix $U_{I} = P_{\\mathrm{DEIM}}^{\\top}U$ and interpolated vector $f_{I} = P_{\\mathrm{DEIM}}^{\\top}f$.\n$$\nU_{I} = \\begin{pmatrix}\n1  2  0 \\\\\n1  6  0 \\\\\n1  7  1\n\\end{pmatrix}, \\quad\nf_{I} = \\begin{pmatrix} 1/2 \\\\ -3/2 \\\\ -1/2 \\end{pmatrix}\n$$\nWe solve the system $U_{I} \\tilde c = f_{I}$ for $\\tilde c$. The determinant of $U_{I}$ is $1(6-0) - 2(1-0) + 0 = 4$. The matrix is invertible.\n$$\nU_{I}^{-1} = \\frac{1}{4} \\operatorname{adj}(U_{I}) = \\frac{1}{4} \\begin{pmatrix}\n6  -2  0 \\\\\n-1  1  0 \\\\\n1  -5  4\n\\end{pmatrix}\n$$\nNow we solve for $\\tilde c = U_{I}^{-1} f_{I}$:\n$$\n\\tilde c = \\frac{1}{4} \\begin{pmatrix}\n6  -2  0 \\\\\n-1  1  0 \\\\\n1  -5  4\n\\end{pmatrix}\n\\begin{pmatrix} 1/2 \\\\ -3/2 \\\\ -1/2 \\end{pmatrix}\n= \\frac{1}{4} \\begin{pmatrix}\n6(1/2) - 2(-3/2) \\\\\n-1(1/2) + 1(-3/2) \\\\\n1(1/2) - 5(-3/2) + 4(-1/2)\n\\end{pmatrix}\n= \\frac{1}{4} \\begin{pmatrix}\n3 + 3 \\\\\n-1/2 - 3/2 \\\\\n1/2 + 15/2 - 2\n\\end{pmatrix}\n$$\n$$\n\\tilde c = \\frac{1}{4} \\begin{pmatrix}\n6 \\\\\n-2 \\\\\n6\n\\end{pmatrix}\n= \\begin{pmatrix} 3/2 \\\\ -1/2 \\\\ 3/2 \\end{pmatrix}\n$$\n\n**3. Reconstruction Errors**\n\nThe reconstruction error is the difference between the original vector $f$ and its approximation.\nThe error can be expressed as $f - U c = (U c^{\\star} + w) - U c = U(c^{\\star} - c) + w$.\n\nFor gappy POD, the error is $e_{g, \\text{vec}} = f - \\hat{f} = U(c^{\\star} - \\hat c) + w$.\n$$\nc^{\\star} - \\hat c = \\begin{pmatrix} 1 \\\\ -1/2 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 7/6 \\\\ -1/2 \\\\ 11/6 \\end{pmatrix} = \\begin{pmatrix} -1/6 \\\\ 0 \\\\ 1/6 \\end{pmatrix}\n$$\n$$\nU(c^{\\star} - \\hat c) = \\frac{1}{6} U \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} -1+0+1 \\\\ -1+0+0 \\\\ -1+0+1 \\\\ -1+0+0 \\\\ -1+0+1 \\\\ -1+0+0 \\\\ -1+0+1 \\\\ -1+0+0 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 0  -1  0  -1  0  -1  0  -1 \\end{pmatrix}^{\\top}\n$$\n$$\ne_{g, \\text{vec}} = \\frac{1}{6} \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\end{pmatrix} + \\frac{1}{6} \\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\\\ -3 \\\\ 0 \\\\ 3 \\\\ 0 \\\\ -3 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\\\ -4 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1/3 \\\\ 0 \\\\ -2/3 \\\\ 0 \\\\ 1/3 \\\\ 0 \\\\ -2/3 \\end{pmatrix}\n$$\nThe squared Euclidean norm of the gappy POD error is:\n$$\ne_{g}^{2} = \\|f - \\hat f\\|_{2}^{2} = 0^{2} + (\\frac{1}{3})^{2} + 0^{2} + (-\\frac{2}{3})^{2} + 0^{2} + (\\frac{1}{3})^{2} + 0^{2} + (-\\frac{2}{3})^{2} = \\frac{1}{9} + \\frac{4}{9} + \\frac{1}{9} + \\frac{4}{9} = \\frac{10}{9}\n$$\nSo, $e_{g} = \\sqrt{\\frac{10}{9}} = \\frac{\\sqrt{10}}{3}$.\n\nFor DEIM, the error is $e_{d, \\text{vec}} = f - \\tilde{f} = U(c^{\\star} - \\tilde c) + w$.\n$$\nc^{\\star} - \\tilde c = \\begin{pmatrix} 1 \\\\ -1/2 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 3/2 \\\\ -1/2 \\\\ 3/2 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ 0 \\\\ 1/2 \\end{pmatrix}\n$$\n$$\nU(c^{\\star} - \\tilde c) = \\frac{1}{2} U \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\end{pmatrix}\n$$\n$$\ne_{d, \\text{vec}} = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ -2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}\n$$\nThe squared Euclidean norm of the DEIM error is:\n$$\ne_{d}^{2} = \\|f - \\tilde f\\|_{2}^{2} = 0^{2} + 0^{2} + 0^{2} + (-1)^{2} + 0^{2} + 0^{2} + 0^{2} + (-1)^{2} = 1 + 1 = 2\n$$\nSo, $e_{d} = \\sqrt{2}$.\n\n**4. Ratio of Errors**\n\nFinally, the ratio of the error norms is:\n$$\n\\frac{e_{g}}{e_{d}} = \\frac{\\frac{\\sqrt{10}}{3}}{\\sqrt{2}} = \\frac{\\sqrt{10}}{3\\sqrt{2}} = \\frac{\\sqrt{5 \\cdot 2}}{3\\sqrt{2}} = \\frac{\\sqrt{5}\\sqrt{2}}{3\\sqrt{2}} = \\frac{\\sqrt{5}}{3}\n$$",
            "answer": "$$\n\\boxed{\\frac{\\sqrt{5}}{3}}\n$$"
        },
        {
            "introduction": "The true utility of hyperreduction is revealed when it is embedded within an iterative nonlinear solver, where it can drastically reduce the cost of computing state updates. This practice moves from static approximation to a dynamic application by demonstrating how to build and use a hyperreduced Jacobian matrix within a single step of a Newton-Raphson algorithm. Implementing this procedure will illuminate how the choice of sampling points and weights directly impacts the quality of the solution update and the overall convergence behavior of the nonlinear analysis .",
            "id": "3572702",
            "problem": "Consider a discrete nonlinear equilibrium system in computational solid mechanics defined by the residual vector $r(u) \\in \\mathbb{R}^n$ and its Jacobian matrix $J(u) \\in \\mathbb{R}^{n \\times n}$, where $u \\in \\mathbb{R}^n$ is the nodal unknown vector. The residual is given by\n$$\nr(u) = K u + \\alpha\\, u^{\\circ 3} - f,\n$$\nwhere $K \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite stiffness-like matrix, $f \\in \\mathbb{R}^n$ is a load vector, $\\alpha \\in \\mathbb{R}$ is a scalar nonlinearity parameter, and $u^{\\circ 3}$ denotes the element-wise cubic of $u$. The Jacobian of the residual is\n$$\nJ(u) = K + \\operatorname{diag}\\!\\left(3\\alpha\\, u^{\\circ 2}\\right),\n$$\nwith $u^{\\circ 2}$ the element-wise square of $u$.\n\nA reduced model is constructed with a two-dimensional reduced basis $\\Phi \\in \\mathbb{R}^{n \\times r}$, $r = 2$, so that the state is approximated as $u \\approx \\Phi a$, with $a \\in \\mathbb{R}^r$ the reduced coordinates. To perform one Newton step in reduced coordinates, a hyperreduced Jacobian-action is used. Specifically, define a sampling set of row indices $S \\subset \\{0,1,\\dots,n-1\\}$ and positive weights $w \\in \\mathbb{R}^{|S|}$; let $W = \\operatorname{diag}(w) \\in \\mathbb{R}^{|S| \\times |S|}$. Denote by $X_S$ the matrix obtained by selecting the rows of a matrix $X$ indexed by $S$, and by $v_S$ the vector obtained by selecting the entries of a vector $v$ indexed by $S$. The hyperreduced Jacobian-action constructs the reduced linear system\n$$\nA_{\\mathrm{HR}}(u) \\,\\Delta a = b_{\\mathrm{HR}}(u),\n$$\nwhere\n$$\nA_{\\mathrm{HR}}(u) = \\Phi_S^\\top W \\left(J(u)\\Phi\\right)_S \\in \\mathbb{R}^{r \\times r}, \\quad \nb_{\\mathrm{HR}}(u) = -\\Phi_S^\\top W \\, r(u)_S \\in \\mathbb{R}^{r}.\n$$\nThe Newton update is $a^{+} = a + \\Delta a$, and the updated full state is $u^{+} = \\Phi a^{+}$. The residual decrease is measured by the Euclidean norm ratio $\\|r(u^{+})\\|_2 / \\|r(u)\\|_2$ and a boolean indicating whether the norm strictly decreases.\n\nUse the following data, with $n = 5$ and $r = 2$ fixed for all test cases. The stiffness matrix $K$ is the tridiagonal matrix\n$$\nK = \\begin{bmatrix}\n2  -1  0  0  0 \\\\\n-1  2  -1  0  0 \\\\\n0  -1  2  -1  0 \\\\\n0  0  -1  2  -1 \\\\\n0  0  0  -1  2\n\\end{bmatrix}.\n$$\nThe reduced basis $\\Phi$ has columns\n$$\n\\phi_1 = \\frac{1}{\\sqrt{5}}\\begin{bmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{bmatrix}, \\quad\n\\phi_2 = \\frac{1}{\\sqrt{3}}\\begin{bmatrix}1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 1\\end{bmatrix}, \\quad\n\\Phi = \\begin{bmatrix}\\phi_1  \\phi_2\\end{bmatrix}.\n$$\n\nThere are three test cases, each specifying $(\\alpha, f, a, S, w)$, with indices in $S$ given in $0$-based convention:\n\n- Test case $1$ (moderate nonlinearity, well-chosen sampling):\n  $$\n  \\alpha = 0.5,\\quad \n  f = \\begin{bmatrix}1 \\\\ 0 \\\\ 0.5 \\\\ 0 \\\\ -1\\end{bmatrix},\\quad\n  a = \\begin{bmatrix}0.1 \\\\ -0.2\\end{bmatrix},\\quad\n  S = \\{0, 2, 4\\},\\quad\n  w = \\begin{bmatrix}1.6 \\\\ 1.2 \\\\ 1.2\\end{bmatrix}.\n  $$\n\n- Test case $2$ (linear case, full sampling):\n  $\n  \\alpha = 0.0,\\quad \n  f = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0.5\\end{bmatrix},\\quad\n  a = \\begin{bmatrix}0.0 \\\\ 0.0\\end{bmatrix},\\quad\n  S = \\{0, 1, 2, 3, 4\\},\\quad\n  w = \\begin{bmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{bmatrix}.\n  $\n\n- Test case $3$ (strong nonlinearity, poor sampling):\n  $\n  \\alpha = 1.5,\\quad \n  f = \\begin{bmatrix}-0.5 \\\\ 0.2 \\\\ 0 \\\\ 0.1 \\\\ -0.1\\end{bmatrix},\\quad\n  a = \\begin{bmatrix}-0.3 \\\\ 0.15\\end{bmatrix},\\quad\n  S = \\{3, 4\\},\\quad\n  w = \\begin{bmatrix}2.5 \\\\ 2.5\\end{bmatrix}.\n  $\n\nProgram requirements:\n- For each test case, compute $u = \\Phi a$, form $r(u)$ and $J(u)$, assemble $A_{\\mathrm{HR}}(u)$ and $b_{\\mathrm{HR}}(u)$, solve for $\\Delta a$, update $a^{+}$ and $u^{+}$, and evaluate $\\|r(u)\\|_2$, $\\|r(u^{+})\\|_2$, the ratio $\\|r(u^{+})\\|_2 / \\|r(u)\\|_2$, and a boolean indicating whether $\\|r(u^{+})\\|_2  \\|r(u)\\|_2$.\n- If $\\|r(u)\\|_2 = 0$, define the ratio as a Not-a-Number indicator; otherwise compute the ratio as specified. In all cases, report the boolean based on strict inequality.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is an inner list of the form $[$norm\\_before, norm\\_after, ratio, improved$]$. Angles are not used and no physical units apply; all outputs are dimensionless real numbers. Use floating-point numbers for norms and ratios and a boolean for the improvement indicator. For example, the output format must look like $[[x_1,y_1,z_1,\\mathrm{True}],[x_2,y_2,z_2,\\mathrm{False}],\\dots]$.",
            "solution": "The problem is subjected to validation against the stipulated criteria.\n\n### Step 1: Extract Givens\nThe problem statement provides the following definitions and data:\n- **System Equations**:\n  - Residual vector: $r(u) = K u + \\alpha\\, u^{\\circ 3} - f$, where $u \\in \\mathbb{R}^n$, $K \\in \\mathbb{R}^{n \\times n}$, $\\alpha \\in \\mathbb{R}$, $f \\in \\mathbb{R}^n$, and $u^{\\circ 3}$ is the element-wise cubic power.\n  - Jacobian matrix: $J(u) = K + \\operatorname{diag}\\!\\left(3\\alpha\\, u^{\\circ 2}\\right)$, where $u^{\\circ 2}$ is the element-wise square.\n- **Reduced-Order Model**:\n  - State approximation: $u \\approx \\Phi a$, with a reduced basis $\\Phi \\in \\mathbb{R}^{n \\times r}$ and reduced coordinates $a \\in \\mathbb{R}^r$.\n- **Hyperreduction for Newton Step**:\n  - The linear system for the update $\\Delta a$ is $A_{\\mathrm{HR}}(u) \\,\\Delta a = b_{\\mathrm{HR}}(u)$.\n  - Hyperreduced matrix: $A_{\\mathrm{HR}}(u) = \\Phi_S^\\top W \\left(J(u)\\Phi\\right)_S$.\n  - Hyperreduced right-hand side: $b_{\\mathrm{HR}}(u) = -\\Phi_S^\\top W \\, r(u)_S$.\n  - Here, $S$ is a set of sample row indices, $W = \\operatorname{diag}(w)$ is a diagonal matrix of weights $w$, and $X_S$ denotes row-sampling of a matrix/vector $X$.\n- **Update and Evaluation**:\n  - Update rule: $a^{+} = a + \\Delta a$, followed by $u^{+} = \\Phi a^{+}$.\n  - Performance metric: The ratio of Euclidean norms $\\|r(u^{+})\\|_2 / \\|r(u)\\|_2$ and a boolean flag for strict decrease, $\\|r(u^{+})\\|_2  \\|r(u)\\|_2$. If $\\|r(u)\\|_2 = 0$, the ratio is defined as Not-a-Number (NaN).\n- **Fixed Parameters**:\n  - System dimension: $n=5$.\n  - Reduced dimension: $r=2$.\n  - Stiffness matrix: $K = \\operatorname{tridiag}(-1, 2, -1) \\in \\mathbb{R}^{5 \\times 5}$.\n  - Reduced basis: $\\Phi = [\\phi_1, \\phi_2]$, with $\\phi_1 = \\frac{1}{\\sqrt{5}}[1, 1, 1, 1, 1]^\\top$ and $\\phi_2 = \\frac{1}{\\sqrt{3}}[1, 0, -1, 0, 1]^\\top$.\n- **Test Cases**: Three cases are provided, each with a specific set of $(\\alpha, f, a, S, w)$.\n  - **Case 1**: $\\alpha = 0.5$, $f = [1, 0, 0.5, 0, -1]^\\top$, $a = [0.1, -0.2]^\\top$, $S = \\{0, 2, 4\\}$, $w = [1.6, 1.2, 1.2]^\\top$.\n  - **Case 2**: $\\alpha = 0.0$, $f = [0, 0, 0, 0, 0.5]^\\top$, $a = [0.0, 0.0]^\\top$, $S = \\{0, 1, 2, 3, 4\\}$, $w = [1, 1, 1, 1, 1]^\\top$.\n  - **Case 3**: $\\alpha = 1.5$, $f = [-0.5, 0.2, 0, 0.1, -0.1]^\\top$, $a = [-0.3, 0.15]^\\top$, $S = \\{3, 4\\}$, $w = [2.5, 2.5]^\\top$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is validated against the required criteria:\n- **Scientifically Grounded**: The problem is situated in the established field of computational solid mechanics, specifically addressing reduced-order modeling and hyperreduction. The governing equations represent a standard discrete nonlinear mechanical system, and the hyperreduced Newton method is a well-documented technique (related to GNAT or ECSW methods). All concepts are scientifically sound.\n- **Well-Posed**: For each test case, all necessary inputs are provided. The task is a direct computation of a single Newton step. The existence of a solution for $\\Delta a$ depends on the invertibility of the $2 \\times 2$ matrix $A_{\\mathrm{HR}}(u)$, which is part of the calculation itself and not a flaw in the problem statement. The problem is well-posed.\n- **Objective**: The problem is formulated with precise mathematical definitions and numerical data. It is free of ambiguity, subjectivity, or opinion.\n- **Completeness and Consistency**: The problem is self-contained. All dimensions of matrices and vectors are consistent. For example, for $A_{\\mathrm{HR}} \\in \\mathbb{R}^{2 \\times 2}$, the product is $\\Phi_S^\\top W (J\\Phi)_S$, with dimensions $(2 \\times |S|) \\times (|S| \\times |S|) \\times (|S| \\times 2)$, which correctly results in a $2 \\times 2$ matrix. All data required for the computation are explicitly given.\n- **No Other Flaws**: The problem is not unrealistic, ill-posed, trivial, or unverifiable. It is a concrete computational exercise.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be provided by executing the specified computational procedure for each test case.\n\n### Solution Procedure\n\nThe solution involves a systematic application of the given formulas for each test case. The steps are as follows:\n\n1.  **Initialization**: For a given test case $(\\alpha, f, a, S, w)$, define the constant matrices $K$ and $\\Phi$.\n2.  **Compute Initial State**: Calculate the full-order state vector $u = \\Phi a$ and the corresponding residual vector $r(u) = K u + \\alpha u^{\\circ 3} - f$. Compute its Euclidean norm, $N_{\\text{before}} = \\|r(u)\\|_2$.\n3.  **Assemble Hyperreduced System**:\n    a. Form the Jacobian matrix $J(u) = K + \\operatorname{diag}(3\\alpha u^{\\circ 2})$.\n    b. Extract the sampled rows of the basis, $\\Phi_S$, and the sampled rows of the Jacobian-basis product, $(J(u)\\Phi)_S$.\n    c. Extract the sampled entries of the residual, $r(u)_S$.\n    d. Form the diagonal weight matrix $W = \\operatorname{diag}(w)$.\n    e. Assemble the hyperreduced matrix $A_{\\mathrm{HR}}(u) = \\Phi_S^\\top W (J(u)\\Phi)_S$ and the hyperreduced right-hand side vector $b_{\\mathrm{HR}}(u) = -\\Phi_S^\\top W r(u)_S$.\n4.  **Solve for Update**: Solve the $r \\times r$ linear system $A_{\\mathrm{HR}}(u) \\Delta a = b_{\\mathrm{HR}}(u)$ for the reduced coordinate update $\\Delta a$.\n5.  **Compute Updated State**:\n    a. Update the reduced coordinates: $a^{+} = a + \\Delta a$.\n    b. Compute the new full-order state vector: $u^{+} = \\Phi a^{+}$.\n6.  **Evaluate Performance**:\n    a. Compute the new residual vector $r(u^{+}) = K u^{+} + \\alpha (u^{+})^{\\circ 3} - f$ and its Euclidean norm, $N_{\\text{after}} = \\|r(u^{+})\\|_2$.\n    b. Determine if the residual norm has strictly decreased: $\\textit{improved} = (N_{\\text{after}}  N_{\\text{before}})$.\n    c. Calculate the ratio. If $N_{\\text{before}} = 0$, the ratio is NaN. Otherwise, $\\textit{ratio} = N_{\\text{after}} / N_{\\text{before}}$.\n7.  **Store Results**: The results for the test case are compiled into the list $[N_{\\text{before}}, N_{\\text{after}}, \\textit{ratio}, \\textit{improved}]$.\n\nThis procedure is repeated for all three test cases to generate the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes one hyperreduced Newton step for a nonlinear system for three test cases.\n    \"\"\"\n    \n    # Define fixed problem data\n    n = 5\n    r = 2\n    \n    # Stiffness matrix K\n    K = np.diag(np.full(n, 2.0)) + np.diag(np.full(n - 1, -1.0), k=1) + np.diag(np.full(n - 1, -1.0), k=-1)\n    \n    # Reduced basis Phi\n    phi1 = np.ones(n) / np.sqrt(n)\n    phi2 = np.array([1.0, 0.0, -1.0, 0.0, 1.0]) / np.sqrt(3.0)\n    Phi = np.vstack((phi1, phi2)).T\n\n    # Define the three test cases\n    test_cases = [\n        # Case 1: Moderate nonlinearity, well-chosen sampling\n        {\n            \"alpha\": 0.5,\n            \"f\": np.array([1.0, 0.0, 0.5, 0.0, -1.0]),\n            \"a\": np.array([0.1, -0.2]),\n            \"S\": np.array([0, 2, 4]),\n            \"w\": np.array([1.6, 1.2, 1.2])\n        },\n        # Case 2: Linear case, full sampling\n        {\n            \"alpha\": 0.0,\n            \"f\": np.array([0.0, 0.0, 0.0, 0.0, 0.5]),\n            \"a\": np.array([0.0, 0.0]),\n            \"S\": np.array([0, 1, 2, 3, 4]),\n            \"w\": np.array([1.0, 1.0, 1.0, 1.0, 1.0])\n        },\n        # Case 3: Strong nonlinearity, poor sampling\n        {\n            \"alpha\": 1.5,\n            \"f\": np.array([-0.5, 0.2, 0.0, 0.1, -0.1]),\n            \"a\": np.array([-0.3, 0.15]),\n            \"S\": np.array([3, 4]),\n            \"w\": np.array([2.5, 2.5])\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        alpha = case[\"alpha\"]\n        f = case[\"f\"]\n        a = case[\"a\"]\n        S = case[\"S\"]\n        w = case[\"w\"]\n\n        # 1. Compute initial state and residual norm\n        u = Phi @ a\n        r_u = K @ u + alpha * (u**3) - f\n        norm_before = np.linalg.norm(r_u)\n\n        # 2. Assemble hyperreduced Jacobian system\n        # Jacobian J(u)\n        J_u = K + np.diag(3 * alpha * (u**2))\n        \n        # Sampled components\n        Phi_S = Phi[S, :]\n        J_Phi = J_u @ Phi\n        J_Phi_S = J_Phi[S, :]\n        r_u_S = r_u[S]\n        \n        # Weight matrix W\n        W = np.diag(w)\n        \n        # Hyperreduced system matrix A_HR and right-hand side b_HR\n        A_HR = Phi_S.T @ W @ J_Phi_S\n        b_HR = -Phi_S.T @ W @ r_u_S\n        \n        # 3. Solve for the update in reduced coordinates\n        delta_a = np.linalg.solve(A_HR, b_HR)\n        \n        # 4. Update the state\n        a_plus = a + delta_a\n        u_plus = Phi @ a_plus\n        \n        # 5. Compute the new residual norm\n        r_u_plus = K @ u_plus + alpha * (u_plus**3) - f\n        norm_after = np.linalg.norm(r_u_plus)\n        \n        # 6. Calculate performance metrics\n        if norm_before == 0.0:\n            ratio = np.nan\n        else:\n            ratio = norm_after / norm_before\n        \n        improved = bool(norm_after  norm_before)\n        \n        # Store results for this case\n        all_results.append([norm_before, norm_after, ratio, improved])\n\n    # Format the final output string exactly as required\n    # str() on a list containing booleans and nan will produce the correct text representation\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond understanding the mechanics of hyperreduction, designing an effective and robust reduced-order model requires a higher-level, systems-thinking approach. This advanced practice elevates your perspective by framing hyperreduction as an engineering trade-off problem involving computational cost, approximation accuracy, and numerical stability. By navigating these competing objectives to find an optimal sampling configuration, you will practice the critical skill of making principled design decisions that balance performance with reliability for large-scale simulations .",
            "id": "3572683",
            "problem": "Consider a three-dimensional large-deformation hyperelastic cantilever beam discretized by the Finite Element Method (FEM) with a total of $N_{q} = 1.6 \\times 10^{5}$ quadrature points per time step. The reduced basis is built from snapshots using Proper Orthogonal Decomposition (POD), and a Galerkin Reduced Order Model (ROM) of dimension $r = 30$ is constructed. To accelerate the evaluation of the nonlinear internal force, a hyperreduction (HR) technique is employed that approximates the full quadrature by a sampled subset $\\mathcal{S}$ of size $m$ with positive weights $\\{w_{i}\\}_{i \\in \\mathcal{S}}$, $w_{i}  0$. Assume the following foundational elements.\n\n1. Fundamental model: The semi-discrete balance of linear momentum in weak form reads\n$$\n\\mathbf{M} \\ddot{\\mathbf{u}} + \\mathbf{f}_{\\text{int}}(\\mathbf{u}) = \\mathbf{f}_{\\text{ext}}(t),\n$$\nwhere $\\mathbf{M}$ is the consistent mass matrix, $\\mathbf{u}$ is the displacement vector, $\\mathbf{f}_{\\text{int}}$ is the internal force vector arising from the hyperelastic constitutive law via FEM quadrature, and $\\mathbf{f}_{\\text{ext}}$ is the external force. A Galerkin projection onto a POD basis $\\mathbf{V} \\in \\mathbb{R}^{n \\times r}$ yields the ROM in generalized coordinates $\\mathbf{q}$ with $\\mathbf{u} \\approx \\mathbf{V} \\mathbf{q}$.\n\n2. Hyperreduction approximation: The FEM quadrature sum for $\\mathbf{f}_{\\text{int}}(\\mathbf{V}\\mathbf{q})$ is replaced by a weighted sum over sampled points $\\mathcal{S}$:\n$$\n\\mathbf{f}_{\\text{int}}(\\mathbf{V}\\mathbf{q}) \\approx \\sum_{i \\in \\mathcal{S}} w_{i} \\, \\mathbf{g}_{i}(\\mathbf{V}\\mathbf{q}),\n$$\nwhere $\\mathbf{g}_{i}$ denotes the elemental/quadrature contribution at point $i$. The weights are constrained to be positive to preserve energy stability.\n\n3. Error and conditioning model: Let the relative projection error at $r=30$ be bounded as $\\varepsilon_{p} = 4.0 \\times 10^{-3}$. The additional hyperreduction relative error is controlled by the interpolation conditioning of the sampling operator and scales as $\\kappa(m) \\, \\varepsilon_{q}$, where $\\varepsilon_{q} = 1.0 \\times 10^{-3}$. Assume that the sampling matrix is built from a POD interpolation basis with row coherence $\\mu = 3.0$ and that the interpolation condition number admits the bound\n$$\n\\kappa(m) \\leq \\frac{1 + \\delta(m)}{1 - \\delta(m)}, \\quad \\delta(m) = \\sqrt{\\frac{\\mu \\, r}{m}},\n$$\nvalid for $m  \\mu \\, r$. To ensure robust time integration, impose the conditioning cap $\\kappa(m) \\leq \\kappa_{\\max} = 10$.\n\n4. Positivity and feasibility: To robustly compute strictly positive hyperreduction weights via a nonnegative least-squares Empirical Cubature Method (ECM), enforce the design constraint $m \\geq m_{+} = 120$.\n\n5. Cost and speed model: Per time step, the full-order cost is modeled as\n$$\nC_{\\text{FOM}} = \\alpha \\, N_{q},\n$$\nand the hyperreduced ROM cost is modeled as\n$$\nC_{\\text{ROM}}(m) = \\alpha \\, m + \\beta \\, r^{3},\n$$\nwith $\\alpha = 50$ and $\\beta = 4.0$. Define the speedup as $S(m) = C_{\\text{FOM}} / C_{\\text{ROM}}(m)$.\n\nTarget requirement: Impose a total relative error tolerance $\\tau = 1.0 \\times 10^{-2}$ on the ROM with hyperreduction, i.e.,\n$$\n\\varepsilon_{p} + \\kappa(m) \\, \\varepsilon_{q} \\leq \\tau.\n$$\n\nTask: Starting from the above foundations, derive a trade-off relation that links the sampling size $m$, the positivity and conditioning constraints, and the interpolation error to the achievable speedup $S(m)$. Then determine the maximum achievable speedup $S^{\\star}$ by choosing $m$ to satisfy all constraints while maximizing $S(m)$. Round your final answer to four significant figures. The answer must be a single real number with no units.",
            "solution": "The problem is well-posed and scientifically sound, grounded in established principles of computational mechanics and model order reduction. All necessary data and constraints are provided. The problem is therefore deemed valid.\n\nThe central task is to find the maximum achievable speedup, $S^{\\star}$, by selecting an optimal number of hyperreduction sampling points, $m$. The speedup, $S(m)$, is defined as the ratio of the full-order model (FOM) computational cost to the hyperreduced reduced-order model (ROM) cost:\n$$\nS(m) = \\frac{C_{\\text{FOM}}}{C_{\\text{ROM}}(m)} = \\frac{\\alpha \\, N_{q}}{\\alpha \\, m + \\beta \\, r^{3}}\n$$\nThe parameters are given as $\\alpha = 50$, $\\beta = 4.0$, $N_{q} = 1.6 \\times 10^{5}$, and $r = 30$. Since $\\alpha  0$, the denominator is a monotonically increasing function of $m$. Consequently, the speedup $S(m)$ is a monotonically decreasing function of $m$. To maximize $S(m)$, we must find the minimum allowable integer value of $m$ that satisfies all stated constraints.\n\nFirst, we derive the trade-off relation. The core trade-off links the sampling size $m$ to the total error tolerance $\\tau$. The total relative error is constrained by:\n$$\n\\varepsilon_{p} + \\kappa(m) \\, \\varepsilon_{q} \\leq \\tau\n$$\nSubstituting the given values $\\varepsilon_{p} = 4.0 \\times 10^{-3}$, $\\varepsilon_{q} = 1.0 \\times 10^{-3}$, and $\\tau = 1.0 \\times 10^{-2}$ yields:\n$$\n4.0 \\times 10^{-3} + \\kappa(m) \\times (1.0 \\times 10^{-3}) \\leq 10.0 \\times 10^{-3}\n$$\nThis inequality simplifies to a constraint on the conditioning number $\\kappa(m)$:\n$$\n\\kappa(m) \\leq \\frac{10.0 \\times 10^{-3} - 4.0 \\times 10^{-3}}{1.0 \\times 10^{-3}} = 6.0\n$$\nThe problem provides an upper bound for $\\kappa(m)$ based on the sampling size $m$:\n$$\n\\kappa(m) \\leq \\frac{1 + \\delta(m)}{1 - \\delta(m)}, \\quad \\text{where} \\quad \\delta(m) = \\sqrt{\\frac{\\mu \\, r}{m}}\n$$\nWe enforce this upper bound to be less than or equal to $6.0$:\n$$\n\\frac{1 + \\delta(m)}{1 - \\delta(m)} \\leq 6.0\n$$\nSolving for $\\delta(m)$:\n$$\n1 + \\delta(m) \\leq 6.0(1 - \\delta(m)) \\implies 1 + \\delta(m) \\leq 6.0 - 6.0\\delta(m) \\implies 7.0\\delta(m) \\leq 5.0 \\implies \\delta(m) \\leq \\frac{5}{7}\n$$\nNow, we substitute the expression for $\\delta(m)$ with $\\mu = 3.0$ and $r = 30$:\n$$\n\\sqrt{\\frac{\\mu \\, r}{m}} = \\sqrt{\\frac{3.0 \\times 30}{m}} = \\sqrt{\\frac{90}{m}} \\leq \\frac{5}{7}\n$$\nSquaring both sides and solving for $m$ gives the trade-off relation between sampling size and the imposed accuracy constraint:\n$$\n\\frac{90}{m} \\leq \\left(\\frac{5}{7}\\right)^{2} = \\frac{25}{49} \\implies m \\geq 90 \\times \\frac{49}{25} = \\frac{882}{5} = 176.4\n$$\nThis relation shows that achieving the target accuracy requires a minimum sampling size. To achieve higher accuracy (a smaller $\\tau$), a smaller $\\kappa(m)$ would be needed, leading to a larger required $m$ and thus a lower speedup. This is the fundamental trade-off.\n\nNow we must consolidate all constraints on $m$ to find the minimal feasible value, $m^{\\star}$:\n1.  **Error Tolerance Constraint:** As derived above, $m \\geq 176.4$.\n2.  **Conditioning Bound Validity:** The bound for $\\kappa(m)$ is valid for $m  \\mu \\, r = 3.0 \\times 30 = 90$.\n3.  **Positivity Constraint:** The design choice for the cubature method requires $m \\geq m_{+} = 120$.\n4.  **Conditioning Cap:** The problem imposes $\\kappa(m) \\leq \\kappa_{\\max} = 10$. The error tolerance constraint ($\\kappa(m) \\leq 6.0$) is stricter, so this cap is automatically satisfied.\n\nSince $m$ must be an integer, we combine the constraints: $m \\geq \\lceil 176.4 \\rceil = 177$, $m  90$, and $m \\geq 120$. The most restrictive condition is $m \\geq 177$. To maximize the monotonically decreasing speedup function $S(m)$, we must choose the smallest possible integer value for $m$.\nTherefore, the optimal sampling size is $m^{\\star} = 177$.\n\nFinally, we calculate the maximum achievable speedup $S^{\\star} = S(m^{\\star})$:\n$$\nS^{\\star} = S(177) = \\frac{\\alpha \\, N_{q}}{\\alpha \\, (177) + \\beta \\, r^{3}}\n$$\nSubstitute the given numerical values:\n$$\nS^{\\star} = \\frac{50 \\times (1.6 \\times 10^{5})}{50 \\times 177 + 4.0 \\times (30)^{3}}\n$$\nFirst, calculate the cost of the FOM (numerator):\n$$\nC_{\\text{FOM}} = 50 \\times 160000 = 8000000\n$$\nNext, calculate the cost of the hyperreduced ROM (denominator):\n$$\nC_{\\text{ROM}}(177) = (50 \\times 177) + (4.0 \\times 27000) = 8850 + 108000 = 116850\n$$\nThe maximum speedup is the ratio:\n$$\nS^{\\star} = \\frac{8000000}{116850} \\approx 68.46384253\n$$\nRounding to four significant figures as requested, we obtain the final answer.\n$$\nS^{\\star} \\approx 68.46\n$$",
            "answer": "$$\n\\boxed{68.46}\n$$"
        }
    ]
}