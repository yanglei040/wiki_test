{
    "hands_on_practices": [
        {
            "introduction": "In computational mechanics, the correctness of our numerical tools is paramount. This exercise provides a hands-on approach to verifying fundamental tensor operations by implementing and testing key identities. You will explore three different methods for computing the determinant and use classical relations involving the cofactor tensor and the inverse transpose, such as $\\mathrm{cof}(F) = \\det(F)\\, F^{-T}$, to build a suite of unit tests . This practice reinforces the theoretical underpinnings of tensor algebra while instilling the crucial engineering habit of validating numerical implementations against known truths.",
            "id": "3605426",
            "problem": "Consider the deformation gradient tensor $F \\in \\mathbb{R}^{3 \\times 3}$ in computational solid mechanics, where the Jacobian $J = \\det(F)$ quantifies local volume change. Robust numerical evaluation of $\\det(F)$ must account for pivoting in Lower-Upper (LU) factorization to avoid sign errors caused by row permutations. The cofactor (adjugate) tensor $\\mathrm{cof}(F)$ and transpose/inverse identities provide principled unit tests to detect implementation defects, particularly those related to erroneous transposition.\n\nStarting from fundamental linear algebra definitions and facts applicable to tensors:\n- The transpose $F^T$ is defined by $(F^T)_{ij} = F_{ji}$.\n- The inverse $F^{-1}$ of an invertible $F$ satisfies $F F^{-1} = F^{-1} F = I$, where $I$ is the identity tensor.\n- The inverse-transpose identity holds for all invertible $F$: $F^{-T} = (F^{-1})^T$.\n- The cofactor (adjugate) tensor $\\mathrm{cof}(F)$ is the matrix of signed minors, and for invertible $F$ satisfies $\\mathrm{cof}(F) = \\det(F)\\, F^{-T}$.\n- Cramer's rule implies $F\\, \\mathrm{cof}(F)^T = \\det(F)\\, I$ for all $F \\in \\mathbb{R}^{n \\times n}$ (including singular $F$).\n- LU factorization with partial pivoting yields $P F = L U$, where $P$ is a permutation matrix, $L$ is unit lower triangular, and $U$ is upper triangular, implying $\\det(F) = \\det(P)\\, \\det(U)$ because $\\det(L) = 1$.\n\nYour task is to write a program that, for each given $3 \\times 3$ test tensor $F$, does the following:\n1. Compute $\\det(F)$ using a general-purpose backend call (library determinant).\n2. Compute $\\det(F)$ using LU factorization with partial pivoting via the identity $\\det(F) = \\det(P)\\, \\prod_i U_{ii}$, where $P F = L U$ and $U_{ii}$ are diagonal entries of $U$.\n3. Compute $\\mathrm{cof}(F)$ by its definition as the matrix of signed minors, then compute $\\det(F)$ again using the trace identity derived from Cramer's rule: $\\det(F) = \\frac{1}{3}\\, \\mathrm{tr}\\left(F\\, \\mathrm{cof}(F)^T\\right)$.\n4. Verify the inverse-transpose identity $F^{-T} = (F^{-1})^T$ for invertible $F$ only (skip or mark as false for singular $F$).\n5. Verify the cofactor-adjugate identity $\\mathrm{cof}(F) = \\det(F)\\, F^{-T}$ for invertible $F$ only.\n6. Verify the Cramer's rule identity $F\\, \\mathrm{cof}(F)^T = \\det(F)\\, I$ for all $F$.\n7. Compare the three determinant computations to examine any differences that could arise from pivoting and rounding.\n\nUse a tolerance of $\\varepsilon = 10^{-11}$ for relative comparisons and $\\delta = 10^{-12}$ for absolute comparisons in matrix equality checks (i.e., two matrices $A$ and $B$ are considered equal if $\\|A-B\\|_{\\infty} \\le \\delta + \\varepsilon \\|B\\|_{\\infty}$).\n\nTest Suite (five $3 \\times 3$ tensors):\n- General non-singular tensor (happy path):\n$$\nF_1 = \\begin{bmatrix}\n1.2 & -0.3 & 0.5 \\\\\n0.4 & 2.1 & -1.0 \\\\\n-0.8 & 0.7 & 1.5\n\\end{bmatrix}\n$$\n- Symmetric positive definite tensor:\n$$\nF_2 = \\begin{bmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{bmatrix}\n$$\n- Negative determinant tensor constructed by left-multiplying $F_2$ with $D = \\mathrm{diag}(-1,1,1)$:\n$$\nF_3 = D F_2 = \\begin{bmatrix}\n-2 & 1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{bmatrix}\n$$\n- Nearly singular tensor (boundary condition):\n$$\nF_4 = \\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 + 10^{-8} & 1 + 10^{-8} & 1 + 10^{-8} \\\\\n1 & 2 & 3\n\\end{bmatrix}\n$$\n- Singular tensor (exact rank deficiency):\n$$\nF_5 = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 6 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n$$\n\nFinal Output Format:\nFor each test case $F_k$, produce a list containing six entries in the exact order:\n$[\\det_{\\mathrm{lib}}, \\det_{\\mathrm{pivot}}, \\det_{\\mathrm{trace}}, \\text{invT\\_ok}, \\text{cof\\_adj\\_ok}, \\text{cramer\\_ok}]$, where:\n- $\\det_{\\mathrm{lib}}$ is the determinant from the backend call,\n- $\\det_{\\mathrm{pivot}}$ is the determinant from LU with pivoting,\n- $\\det_{\\mathrm{trace}}$ is the determinant from the trace identity,\n- $\\text{invT\\_ok}$ is a boolean for $F^{-T} = (F^{-1})^T$ (only for invertible $F$, otherwise false),\n- $\\text{cof\\_adj\\_ok}$ is a boolean for $\\mathrm{cof}(F) = \\det(F)\\, F^{-T}$ (only for invertible $F$, otherwise false),\n- $\\text{cramer\\_ok}$ is a boolean for $F\\, \\mathrm{cof}(F)^T = \\det(F)\\, I$ (for all $F$).\n\nYour program should produce a single line of output containing a list of the five case results as a comma-separated list enclosed in square brackets, with each case itself represented as a list in the order above. For example:\n$[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ldots]$.\nNo physical units enter this computation. Angles are not involved.\n\nThe goal is to use these identities as principled unit tests to catch transpose bugs and to examine differences attributable to pivoting and numerical rounding in computing $\\det(F)$.",
            "solution": "We begin from linear algebra foundations and identities that apply to deformation gradient tensors. For $F \\in \\mathbb{R}^{3 \\times 3}$, $\\det(F)$ can be obtained in several consistent ways; mismatches reveal numerical or implementation defects.\n\nDeterminant with pivoting via factorization: In Lower-Upper (LU) factorization with partial pivoting, we obtain $P F = L U$ where $P$ is a permutation matrix, $L$ is unit lower triangular (diagonal entries equal to $1$), and $U$ is upper triangular. Taking determinants,\n$$\n\\det(P)\\, \\det(F) = \\det(L)\\, \\det(U) \\quad \\Rightarrow \\quad \\det(F) = \\det(P)\\, \\det(U),\n$$\nbecause $\\det(L) = 1$. This expression makes explicit the effect of pivoting: row permutations contribute a sign factor $\\det(P) \\in \\{-1, +1\\}$ that must be accounted for to avoid sign errors in $\\det(F)$.\n\nCofactor and Cramer's rule identities: The cofactor (adjugate) matrix $\\mathrm{cof}(F)$ is defined as the matrix of signed minors:\n$$\n\\mathrm{cof}(F)_{ij} = (-1)^{i+j} \\det(M_{ij}),\n$$\nwhere $M_{ij}$ is the minor obtained by deleting row $i$ and column $j$. Cramer's rule yields the identity valid for all square matrices (including singular):\n$$\nF\\, \\mathrm{cof}(F)^T = \\det(F)\\, I.\n$$\nTaking the trace on both sides for $3 \\times 3$ matrices gives\n$$\n\\mathrm{tr}\\!\\left(F\\, \\mathrm{cof}(F)^T\\right) = \\det(F)\\, \\mathrm{tr}(I) = 3\\, \\det(F),\n$$\nwhich implies the trace-based determinant formula\n$$\n\\det(F) = \\frac{1}{3}\\, \\mathrm{tr}\\!\\left(F\\, \\mathrm{cof}(F)^T\\right).\n$$\nThis formula depends only on the cofactor definition and avoids inversion, making it robust even when $F$ is singular.\n\nInverse-transpose identity and cofactor-adjugate identity: For invertible $F$, the inverse-transpose identity\n$$\nF^{-T} = (F^{-1})^T\n$$\nfollows immediately from $F F^{-1} = I$ and properties of matrix transposition. Moreover, the classical adjugate identity\n$$\n\\mathrm{cof}(F) = \\det(F)\\, F^{-T}\n$$\nholds for invertible $F$ and is equivalent to $F\\, \\mathrm{cof}(F)^T = \\det(F)\\, I$.\n\nNumerical verification strategy and tolerances: We adopt a relative tolerance $\\varepsilon = 10^{-11}$ and an absolute tolerance $\\delta = 10^{-12}$ for equality checks. For matrix equalities $A = B$, we test\n$$\n\\|A - B\\|_{\\infty} \\le \\delta + \\varepsilon \\, \\|B\\|_{\\infty},\n$$\nwhere $\\|\\cdot\\|_{\\infty}$ is the maximum absolute entry norm. In code, we use $\\mathrm{allclose}$-style checks with these tolerances.\n\nAlgorithmic plan per test case $F$:\n1. Compute $\\det_{\\mathrm{lib}}$ via a standard backend call to the determinant.\n2. Compute $\\det_{\\mathrm{pivot}}$ from LU factorization with pivoting by evaluating $\\det(P)$ and $\\prod_i U_{ii}$.\n3. Compute $\\mathrm{cof}(F)$ using explicit $3 \\times 3$ signed minors, then obtain $\\det_{\\mathrm{trace}} = \\frac{1}{3}\\, \\mathrm{tr}(F\\, \\mathrm{cof}(F)^T)$.\n4. Determine invertibility by rank: if $\\mathrm{rank}(F) = 3$, proceed; otherwise treat as non-invertible.\n5. If invertible, verify $F^{-T} = (F^{-1})^T$ by separately computing $(F^{-1})^T$ and $(F^T)^{-1}$ and checking equality with tolerances.\n6. If invertible, verify $\\mathrm{cof}(F) = \\det_{\\mathrm{lib}}\\, F^{-T}$.\n7. For all $F$, verify $F\\, \\mathrm{cof}(F)^T = \\det_{\\mathrm{lib}}\\, I$.\n8. Record the tuple $[\\det_{\\mathrm{lib}}, \\det_{\\mathrm{pivot}}, \\det_{\\mathrm{trace}}, \\text{invT\\_ok}, \\text{cof\\_adj\\_ok}, \\text{cramer\\_ok}]$.\n\nCoverage of edge cases:\n- The general non-singular case tests consistency across methods.\n- The symmetric positive definite case checks behavior when $F = F^T$ (transpose symmetry).\n- The negative determinant case highlights the effect of orientation reversal, ensuring sign handling in pivoting is correct.\n- The nearly singular case examines numerical stability (rounding and conditioning).\n- The singular case avoids inversion and tests the Cramer's rule identity, which remains valid.\n\nThe final output aggregates the five case results into one top-level list and prints it as a single line. Differences between determinant computations, if any, stem from pivoting sign handling and numerical rounding; the unit tests based on $F^{-T} = (F^{-1})^T$ and $\\mathrm{cof}(F) = \\det(F)\\, F^{-T}$ directly expose transpose-related implementation bugs and adjugate misuse, while $F\\, \\mathrm{cof}(F)^T = \\det(F)\\, I$ provides a robust check even when $F$ is singular.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import lu\nimport json\n\ndef det_pivoting(A: np.ndarray) -> float:\n    \"\"\"\n    Compute det(A) using LU factorization with partial pivoting.\n    Given P @ A = L @ U, with L unit lower triangular, det(A) = det(P) * det(U).\n    \"\"\"\n    P, L, U = lu(A)\n    # det(P) is +/-1 for a permutation matrix; compute robustly\n    detP = np.linalg.det(P)\n    # Round to nearest integer to avoid tiny floating error\n    signP = int(np.round(detP))\n    detU = float(np.prod(np.diag(U)))\n    return signP * detU\n\ndef cofactor_3x3(A: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the cofactor (adjugate) matrix of a 3x3 matrix A via signed minors.\n    \"\"\"\n    a = A\n    cof = np.empty((3, 3), dtype=float)\n    # Row 0\n    cof[0,0] = a[1,1]*a[2,2] - a[1,2]*a[2,1]\n    cof[0,1] = -(a[1,0]*a[2,2] - a[1,2]*a[2,0])\n    cof[0,2] = a[1,0]*a[2,1] - a[1,1]*a[2,0]\n    # Row 1\n    cof[1,0] = -(a[0,1]*a[2,2] - a[0,2]*a[2,1])\n    cof[1,1] = a[0,0]*a[2,2] - a[0,2]*a[2,0]\n    cof[1,2] = -(a[0,0]*a[2,1] - a[0,1]*a[2,0])\n    # Row 2\n    cof[2,0] = a[0,1]*a[1,2] - a[0,2]*a[1,1]\n    cof[2,1] = -(a[0,0]*a[1,2] - a[0,2]*a[1,0])\n    cof[2,2] = a[0,0]*a[1,1] - a[0,1]*a[1,0]\n    return cof\n\ndef det_trace_from_cof(A: np.ndarray, cofA: np.ndarray) -> float:\n    \"\"\"\n    Compute det(A) via the trace identity det(A) = (1/3) * tr(A @ cof(A).T) for 3x3.\n    \"\"\"\n    return float(np.trace(A @ cofA.T) / 3.0)\n\ndef is_invertible(A: np.ndarray) -> bool:\n    \"\"\"\n    Determine invertibility by rank for 3x3: rank == 3 => invertible.\n    \"\"\"\n    r = np.linalg.matrix_rank(A)\n    return r == 3\n\ndef allclose_inf_norm(A: np.ndarray, B: np.ndarray, rtol: float, atol: float) -> bool:\n    \"\"\"\n    Check max-norm closeness: ||A-B||_inf <= atol + rtol * ||B||_inf\n    \"\"\"\n    diff = np.max(np.abs(A - B))\n    normB = np.max(np.abs(B))\n    return diff <= (atol + rtol * normB)\n\ndef make_test_cases():\n    # F1: general non-singular\n    F1 = np.array([\n        [1.2, -0.3, 0.5],\n        [0.4,  2.1, -1.0],\n        [-0.8, 0.7, 1.5]\n    ], dtype=float)\n\n    # F2: symmetric positive definite\n    F2 = np.array([\n        [ 2.0, -1.0,  0.0],\n        [-1.0,  2.0, -1.0],\n        [ 0.0, -1.0,  2.0]\n    ], dtype=float)\n\n    # F3: negative determinant via left-multiplication by diag(-1,1,1)\n    D = np.diag([-1.0, 1.0, 1.0])\n    F3 = D @ F2\n\n    # F4: nearly singular\n    eps = 1e-8\n    F4 = np.array([\n        [1.0, 1.0, 1.0],\n        [1.0 + eps, 1.0 + eps, 1.0 + eps],\n        [1.0, 2.0, 3.0]\n    ], dtype=float)\n\n    # F5: singular (second row is 2x first row)\n    F5 = np.array([\n        [1.0, 2.0, 3.0],\n        [2.0, 4.0, 6.0],\n        [0.0, 0.0, 1.0]\n    ], dtype=float)\n\n    return [F1, F2, F3, F4, F5]\n\ndef solve():\n    rtol = 1e-11\n    atol = 1e-12\n\n    test_cases = make_test_cases()\n    results = []\n\n    for F in test_cases:\n        # Determinants\n        det_lib = float(np.linalg.det(F))\n        det_piv = det_pivoting(F)\n        cofF = cofactor_3x3(F)\n        det_trace = det_trace_from_cof(F, cofF)\n\n        # Invertibility check\n        invertible = is_invertible(F)\n\n        # Identity checks\n        if invertible:\n            invF = np.linalg.inv(F)\n            invT_from_inv = invF.T\n            invT_from_transpose = np.linalg.inv(F.T)\n            invT_ok = allclose_inf_norm(invT_from_inv, invT_from_transpose, rtol, atol)\n\n            # cof(F) = det(F) * F^{-T}\n            cof_adj_rhs = det_lib * invT_from_inv\n            cof_adj_ok = allclose_inf_norm(cofF, cof_adj_rhs, rtol, atol)\n        else:\n            invT_ok = False\n            cof_adj_ok = False\n\n        # Cramer's rule: F @ cof(F).T = det(F) * I for all F\n        lhs = F @ cofF.T\n        rhs = det_lib * np.eye(3)\n        cramer_ok = allclose_inf_norm(lhs, rhs, rtol, atol)\n\n        case_result = [det_lib, det_piv, det_trace, bool(invT_ok), bool(cof_adj_ok), bool(cramer_ok)]\n        results.append(case_result)\n\n    # Final print statement in the exact required format: single line, JSON-like brackets, no spaces.\n    print(json.dumps(results, separators=(',',':')))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While multiple algorithms may be mathematically equivalent, their performance in floating-point arithmetic can differ dramatically. This practice investigates the numerical stability of two common methods for computing the determinant—direct Laplace expansion and the product of eigenvalues—against a robust baseline. By systematically constructing tensors with varying degrees of ill-conditioning and non-symmetry, you will quantify how these properties amplify computational errors . This analysis is essential for selecting appropriate algorithms in simulations where tensors can become nearly singular or deviate from expected symmetries.",
            "id": "3605420",
            "problem": "Let $C \\in \\mathbb{R}^{3 \\times 3}$ denote a second-order tensor (a real $3 \\times 3$ matrix) arising, for example, as a discrete representation of the right Cauchy–Green tensor in computational solid mechanics. The transpose of $C$ is denoted $C^{\\mathsf{T}}$. The symmetric part of $C$ is defined as $S = \\tfrac{1}{2}\\left(C + C^{\\mathsf{T}}\\right)$ and the skew-symmetric part is defined as $W = \\tfrac{1}{2}\\left(C - C^{\\mathsf{T}}\\right)$. The Frobenius norm is defined as $\\lVert X \\rVert_{\\mathrm{F}} = \\sqrt{\\sum_{i,j} x_{ij}^{2}}$. The spectral condition number in the matrix $2$-norm is $\\kappa_{2}(C) = \\lVert C \\rVert_{2}\\,\\lVert C^{-1} \\rVert_{2}$, where $\\lVert \\cdot \\rVert_{2}$ denotes the operator $2$-norm. The determinant is $\\det(C)$ and the trace is $\\mathrm{tr}(C)$.\n\nYour task is to write a program that, for a prescribed test suite of tensors $C$, compares two computational strategies for $\\det(C)$—a direct Laplace expansion and the product of eigenvalues—and quantifies the floating-point error of each method as a function of the conditioning and the deviation from symmetry in $C$.\n\nFundamental base and definitions to be used:\n- The determinant is a multilinear alternating form that equals the oriented volume scaling factor of the linear map represented by $C$.\n- The determinant equals the product of eigenvalues (counted with algebraic multiplicity), i.e., if $\\{\\lambda_{i}\\}_{i=1}^{3}$ are the eigenvalues of $C$, then $\\det(C) = \\prod_{i=1}^{3} \\lambda_{i}$.\n- For an orthogonal matrix $Q$ with $Q^{\\mathsf{T}}Q = I$, the determinant is invariant under orthogonal similarity, $\\det\\!\\left(Q \\Lambda Q^{\\mathsf{T}}\\right) = \\det(\\Lambda)$, and the singular values of $Q \\Lambda Q^{\\mathsf{T}}$ equal those of $\\Lambda$.\n- The differential identity $\\mathrm{d} \\log \\det(C) = \\mathrm{tr}\\!\\left(C^{-1} \\mathrm{d}C\\right)$ implies first-order sensitivity bounds of $\\det(\\cdot)$ to perturbations.\n\nImplementation requirements:\n1. For each specified test case, construct the tensor $C$ as\n   $$C = Q \\,\\mathrm{diag}(\\ell_{1},\\ell_{2},\\ell_{3})\\, Q^{\\mathsf{T}} + \\varepsilon A,$$\n   where $Q$ is a prescribed orthogonal matrix, $\\mathrm{diag}(\\ell_{1},\\ell_{2},\\ell_{3})$ is a real diagonal matrix with positive diagonal entries (defining a symmetric positive definite (SPD) base), $A$ is a fixed skew-symmetric matrix normalized to unit Frobenius norm, and $\\varepsilon \\ge 0$ is a prescribed scalar that controls deviation from symmetry. The deviation from symmetry must be reported as the relative skew measure\n   $$\\delta_{\\mathrm{skew}}(C) = \\frac{\\lVert \\tfrac{1}{2}(C - C^{\\mathsf{T}}) \\rVert_{\\mathrm{F}}}{\\lVert C \\rVert_{\\mathrm{F}}}.$$\n2. Compute $\\det(C)$ using:\n   - A direct Laplace expansion along the first row (i.e., cofactor expansion using minors).\n   - The product of eigenvalues computed by a general eigenvalue routine (suitable for possibly non-symmetric $C$).\n   Use a numerically stable factorization-based routine as a reference determinant (e.g., one that is mathematically equivalent to Gaussian elimination with partial pivoting).\n3. Quantify the relative error of each method with respect to the reference as\n   $$e_{\\mathrm{L}} = \\frac{|\\det_{\\mathrm{Laplace}}(C) - \\det_{\\mathrm{ref}}(C)|}{|\\det_{\\mathrm{ref}}(C)|}, \\quad e_{\\mathrm{E}} = \\frac{|\\det_{\\mathrm{eigprod}}(C) - \\det_{\\mathrm{ref}}(C)|}{|\\det_{\\mathrm{ref}}(C)|}.$$\n   If $|\\det_{\\mathrm{ref}}(C)|$ underflows to values below machine-representable thresholds, switch to absolute error for robustness, but still report the numerical value computed by the program.\n4. For each $C$, also compute the spectral condition number $\\kappa_{2}(C)$ using the singular values (by the Singular Value Decomposition (SVD)).\n5. Angles used to define rotations must be interpreted in radians.\n\nTest suite:\n- Define the fixed skew-symmetric matrix $A_{0}$ by\n  $$A_{0} = \\begin{bmatrix} 0 & 1 & -2 \\\\ -1 & 0 & 3 \\\\ 2 & -3 & 0 \\end{bmatrix}, \\quad A = \\frac{A_{0}}{\\lVert A_{0} \\rVert_{\\mathrm{F}}}.$$\n- Define the rotation matrices:\n  - About the $z$-axis by angle $\\theta$: \n    $$R_{z}(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta & 0 \\\\ \\sin\\theta & \\cos\\theta & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}.$$\n  - About the $x$-axis by angle $\\phi$:\n    $$R_{x}(\\phi) = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\cos\\phi & -\\sin\\phi \\\\ 0 & \\sin\\phi & \\cos\\phi \\end{bmatrix}.$$\n  - About a unit axis $u \\in \\mathbb{R}^{3}$ by angle $\\alpha$ using Rodrigues’ formula:\n    $$R(u,\\alpha) = I \\cos\\alpha + (1-\\cos\\alpha)\\,u u^{\\mathsf{T}} + [u]_{\\times} \\sin\\alpha,$$\n    where $[u]_{\\times}$ is the skew-symmetric cross-product matrix associated with $u$.\n- Specify the orthogonal matrices and diagonal spectra:\n  - $Q_{0} = I$.\n  - $Q_{1} = R_{z}(\\theta_{z})$ with $\\theta_{z} = \\pi/3$.\n  - $Q_{2} = R(u,\\alpha)$ with $u = \\tfrac{1}{\\sqrt{3}}[1,1,1]^{\\mathsf{T}}$ and $\\alpha = \\pi/5$.\n  - $Q_{3} = R_{z}(\\pi/7)\\,R_{x}(\\pi/9)$.\n  - $\\Lambda_{1} = \\mathrm{diag}(1,1,1)$.\n  - $\\Lambda_{2} = \\mathrm{diag}(10^{6},1,1)$.\n  - $\\Lambda_{3} = \\mathrm{diag}(1,1,10^{-6})$.\n  - $\\Lambda_{6} = \\mathrm{diag}(10,1,10^{-1})$.\n- Construct the six test cases as follows:\n  1. $(\\Lambda_{1}, Q_{0}, \\varepsilon = 0)$.\n  2. $(\\Lambda_{2}, Q_{1}, \\varepsilon = 0)$.\n  3. $(\\Lambda_{3}, Q_{2}, \\varepsilon = 0)$.\n  4. $(\\Lambda_{2}, Q_{1}, \\varepsilon = 10^{-8})$.\n  5. $(\\Lambda_{3}, Q_{2}, \\varepsilon = 10^{-4})$.\n  6. $(\\Lambda_{6}, Q_{3}, \\varepsilon = 10^{-2})$.\n\nProgram output specification:\n- For each test case, compute and record the list\n  $$\\left[ \\kappa_{2}(C),\\ \\delta_{\\mathrm{skew}}(C),\\ e_{\\mathrm{L}},\\ e_{\\mathrm{E}} \\right].$$\n- The final program output must be a single line containing the results for all six test cases as a comma-separated list of these four-entry lists, enclosed in a single pair of square brackets, for example,\n  $$\\big[\\,[\\kappa_{2}^{(1)},\\delta_{\\mathrm{skew}}^{(1)},e_{\\mathrm{L}}^{(1)},e_{\\mathrm{E}}^{(1)}],\\ldots,[\\kappa_{2}^{(6)},\\delta_{\\mathrm{skew}}^{(6)},e_{\\mathrm{L}}^{(6)},e_{\\mathrm{E}}^{(6)}]\\,\\big].$$\nNo physical units are involved. Angles must be in radians. All numerical values must be output as decimal floating-point numbers.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in numerical linear algebra and computational mechanics, well-posed with a complete and consistent set of definitions and data, and objective in its formulation. The task is to perform a numerical experiment comparing the accuracy of two methods for computing the determinant of a $3 \\times 3$ matrix against a baseline reference, as a function of the matrix's condition number and its deviation from symmetry. This constitutes a standard and meaningful exercise in numerical analysis.\n\nThe solution proceeds by first algorithmically constructing the test matrices, then computing the required quantities using specified methods, and finally reporting the results in the prescribed format.\n\nThe core of the problem involves constructing a series of test tensors $C \\in \\mathbb{R}^{3 \\times 3}$ according to the formula:\n$$C = Q \\Lambda Q^{\\mathsf{T}} + \\varepsilon A$$\nHere, $Q$ is an orthogonal matrix ($Q^{\\mathsf{T}}Q=I$), $\\Lambda = \\mathrm{diag}(\\ell_{1},\\ell_{2},\\ell_{3})$ is a diagonal matrix with positive entries, $\\varepsilon \\ge 0$ is a scalar controlling the magnitude of a non-symmetric perturbation, and $A$ is a fixed, unit-norm skew-symmetric matrix. The term $Q \\Lambda Q^{\\mathsf{T}}$ represents a symmetric positive definite (SPD) tensor, which is then perturbed by the skew-symmetric term $\\varepsilon A$.\n\nThe fixed skew-symmetric matrix $A_0$ is given as\n$$A_{0} = \\begin{bmatrix} 0 & 1 & -2 \\\\ -1 & 0 & 3 \\\\ 2 & -3 & 0 \\end{bmatrix}$$\nThis matrix must be normalized with respect to the Frobenius norm, $\\lVert X \\rVert_{\\mathrm{F}} = \\sqrt{\\mathrm{tr}(X^{\\mathsf{T}}X)} = \\sqrt{\\sum_{i,j} x_{ij}^{2}}$. The squared Frobenius norm of $A_0$ is $\\lVert A_0 \\rVert_{\\mathrm{F}}^2 = 0^2 + 1^2 + (-2)^2 + (-1)^2 + 0^2 + 3^2 + 2^2 + (-3)^2 + 0^2 = 1+4+1+9+4+9 = 28$. Thus, the normalized matrix is $A = A_0 / \\sqrt{28}$.\n\nFor each test case specified by a triplet $(\\Lambda, Q, \\varepsilon)$, the matrix $C$ is constructed. Following this, we compute four key metrics:\n\n1.  **Spectral Condition Number, $\\kappa_{2}(C)$**: This quantifies the sensitivity of the matrix inverse to perturbations. For a general matrix $C$, it is defined as $\\kappa_{2}(C) = \\lVert C \\rVert_{2}\\,\\lVert C^{-1} \\rVert_{2}$. This is most robustly computed using the singular values of $C$, $\\{\\sigma_i\\}$, obtained from the Singular Value Decomposition (SVD). The condition number is the ratio of the largest to the smallest singular value: $\\kappa_{2}(C) = \\sigma_{\\max} / \\sigma_{\\min}$.\n\n2.  **Relative Skew Measure, $\\delta_{\\mathrm{skew}}(C)$**: This measures the deviation of $C$ from being a symmetric matrix. It is defined as the ratio of the Frobenius norm of the skew-symmetric part of $C$ to the Frobenius norm of $C$ itself:\n    $$\\delta_{\\mathrm{skew}}(C) = \\frac{\\lVert W \\rVert_{\\mathrm{F}}}{\\lVert C \\rVert_{\\mathrm{F}}} \\quad \\text{where} \\quad W = \\frac{1}{2}(C - C^{\\mathsf{T}})$$\n    For the constructed matrices, $W = \\frac{1}{2}((Q \\Lambda Q^{\\mathsf{T}} + \\varepsilon A) - (Q \\Lambda Q^{\\mathsf{T}} + \\varepsilon A)^{\\mathsf{T}}) = \\frac{1}{2}(\\varepsilon A - \\varepsilon A^{\\mathsf{T}})$. Since $A$ is skew-symmetric ($A^{\\mathsf{T}} = -A$), this simplifies to $W = \\varepsilon A$.\n\n3.  **Laplace Expansion Error, $e_{\\mathrm{L}}$**: The determinant is first computed using the Laplace (or cofactor) expansion along the first row of $C=[c_{ij}]$:\n    $$\\det_{\\mathrm{Laplace}}(C) = c_{11}(c_{22}c_{33} - c_{23}c_{32}) - c_{12}(c_{21}c_{33} - c_{23}c_{31}) + c_{13}(c_{21}c_{32} - c_{22}c_{31})$$\n    This method is known to be numerically unstable due to subtractive cancellation, especially for larger or ill-conditioned matrices.\n\n4.  **Eigenvalue Product Error, $e_{\\mathrm{E}}$**: The determinant is also computed as the product of its eigenvalues, $\\det_{\\mathrm{eigprod}}(C) = \\prod_{i=1}^{3} \\lambda_{i}$. Since $C$ can be non-symmetric, its eigenvalues $\\{\\lambda_i\\}$ may be complex, and a general-purpose eigenvalue solver is required.\n\nThese two methods are compared against a reference determinant, $\\det_{\\mathrm{ref}}(C)$, computed using a standard, numerically stable library function. Such functions typically employ a matrix factorization like LU decomposition with partial pivoting, which is generally robust and efficient. The relative errors are then calculated as:\n$$e_{\\mathrm{L}} = \\frac{|\\det_{\\mathrm{Laplace}}(C) - \\det_{\\mathrm{ref}}(C)|}{|\\det_{\\mathrm{ref}}(C)|}, \\quad e_{\\mathrm{E}} = \\frac{|\\det_{\\mathrm{eigprod}}(C) - \\det_{\\mathrm{ref}}(C)|}{|\\det_{\\mathrm{ref}}(C)|}$$\nIf $\\det_{\\mathrm{ref}}(C)$ is zero, this formula is ill-defined. However, the matrices constructed in this problem are designed to be invertible, although some may be nearly singular (i.e., have a very large condition number), making the denominator very small.\n\nThe orthogonal matrices $Q$ are constructed using standard rotation matrix formulas. For a rotation about a unit axis $u$ by an angle $\\alpha$, Rodrigues' formula is used:\n$$R(u,\\alpha) = I \\cos\\alpha + (1-\\cos\\alpha)\\,u u^{\\mathsf{T}} + [u]_{\\times} \\sin\\alpha$$\nwhere $[u]_{\\times}$ is the skew-symmetric matrix representation of the cross-product with $u$:\n$$[u]_{\\times} = \\begin{bmatrix} 0 & -u_3 & u_2 \\\\ u_3 & 0 & -u_1 \\\\ -u_2 & u_1 & 0 \\end{bmatrix}$$\nWe will implement helper functions for each rotation type and for Rodrigues' formula to construct the test cases. The final implementation will systematically proceed through the six specified test cases, perform all calculations for each, and format the quartet of results $[\\kappa_{2}(C), \\delta_{\\mathrm{skew}}(C), e_{\\mathrm{L}}, e_{\\mathrm{E}}]$ into the final required output structure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the problem by constructing test matrices, computing determinants\n    via three methods, and evaluating errors and matrix properties.\n    \"\"\"\n\n    def get_laplace_determinant(C):\n        \"\"\"Computes the determinant of a 3x3 matrix using Laplace expansion.\"\"\"\n        # C[i,j] corresponds to c_{i+1, j+1}\n        det = (C[0, 0] * (C[1, 1] * C[2, 2] - C[1, 2] * C[2, 1]) -\n               C[0, 1] * (C[1, 0] * C[2, 2] - C[1, 2] * C[2, 0]) +\n               C[0, 2] * (C[1, 0] * C[2, 1] - C[1, 1] * C[2, 0]))\n        return det\n\n    def get_eigenvalue_determinant(C):\n        \"\"\"Computes the determinant from the product of eigenvalues.\"\"\"\n        eigenvalues = np.linalg.eigvals(C)\n        return np.prod(eigenvalues)\n\n    def rz_matrix(theta):\n        \"\"\"Returns the rotation matrix around the z-axis.\"\"\"\n        c, s = np.cos(theta), np.sin(theta)\n        return np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n\n    def rx_matrix(phi):\n        \"\"\"Returns the rotation matrix around the x-axis.\"\"\"\n        c, s = np.cos(phi), np.sin(phi)\n        return np.array([[1, 0, 0], [0, c, -s], [0, s, c]])\n\n    def rodrigues_rotation(u, alpha):\n        \"\"\"\n        Computes the rotation matrix for a rotation about axis u by angle alpha.\n        \"\"\"\n        u = np.asarray(u)\n        u = u / np.linalg.norm(u)\n        I = np.identity(3)\n        u_outer = np.outer(u, u)\n        u_cross_matrix = np.array([\n            [0, -u[2], u[1]],\n            [u[2], 0, -u[0]],\n            [-u[1], u[0], 0]\n        ])\n        cos_a = np.cos(alpha)\n        sin_a = np.sin(alpha)\n        R = I * cos_a + (1 - cos_a) * u_outer + u_cross_matrix * sin_a\n        return R\n\n    # Define the fixed skew-symmetric matrix A\n    A0 = np.array([[0, 1, -2], [-1, 0, 3], [2, -3, 0]])\n    A = A0 / np.linalg.norm(A0, 'fro')\n\n    # Define the orthogonal matrices Q\n    Q0 = np.identity(3)\n    Q1 = rz_matrix(np.pi / 3)\n    u2 = (1 / np.sqrt(3)) * np.array([1, 1, 1])\n    Q2 = rodrigues_rotation(u2, np.pi / 5)\n    Q3 = rz_matrix(np.pi / 7) @ rx_matrix(np.pi / 9)\n\n    # Define the diagonal matrices Lambda\n    Lambda1 = np.diag([1, 1, 1])\n    Lambda2 = np.diag([1e6, 1, 1])\n    Lambda3 = np.diag([1, 1, 1e-6])\n    Lambda6 = np.diag([10, 1, 1e-1])\n\n    # Define the test suite\n    test_cases = [\n        (Lambda1, Q0, 0.0),\n        (Lambda2, Q1, 0.0),\n        (Lambda3, Q2, 0.0),\n        (Lambda2, Q1, 1e-8),\n        (Lambda3, Q2, 1e-4),\n        (Lambda6, Q3, 1e-2)\n    ]\n\n    results = []\n    for Lambda, Q, epsilon in test_cases:\n        # 1. Construct the tensor C\n        C = Q @ Lambda @ Q.T + epsilon * A\n\n        # 2. Compute determinants\n        det_ref = np.linalg.det(C)\n        det_laplace = get_laplace_determinant(C)\n        det_eig = get_eigenvalue_determinant(C)\n\n        # 3. Quantify relative errors\n        if np.abs(det_ref) > np.finfo(float).tiny:\n            e_L = np.abs(det_laplace - det_ref) / np.abs(det_ref)\n            e_E = np.abs(det_eig - det_ref) / np.abs(det_ref)\n        else: # Handle denormalized or zero determinant\n            e_L = np.abs(det_laplace - det_ref)\n            e_E = np.abs(det_eig - det_ref)\n\n        # 4. Compute spectral condition number kappa_2(C)\n        singular_values = np.linalg.svd(C, compute_uv=False)\n        kappa_2 = singular_values[0] / singular_values[-1]\n\n        # 5. Compute relative skew measure delta_skew(C)\n        W = 0.5 * (C - C.T)\n        delta_skew = np.linalg.norm(W, 'fro') / np.linalg.norm(C, 'fro')\n        \n        # Ensure outputs are floats\n        case_results = [float(kappa_2), float(delta_skew), float(e_L), float(np.real(e_E))]\n        results.append(case_results)\n\n    # Final print statement in the exact required format\n    # Using f-strings with general format specifier 'g' for clean output\n    output_str = \"[\" + \",\".join(\n        f\"[{res[0]:.15g},{res[1]:.15g},{res[2]:.15g},{res[3]:.15g}]\" for res in results\n    ) + \"]\"\n    print(output_str)\n\nsolve()\n\n```"
        },
        {
            "introduction": "For simulations involving large deformations, the determinant of the deformation gradient can span many orders of magnitude, making it susceptible to numerical overflow or underflow. This advanced exercise tackles this challenge by asking you to design and implement a highly stable algorithm to compute the natural logarithm of the determinant, $\\ln(\\det F)$. You will derive the expression based on the sum of the logarithms of singular values and incorporate scaling techniques to handle extreme-scale matrices, a task where naive methods would fail . This demonstrates a powerful technique for developing robust computational tools capable of handling the demanding conditions of nonlinear solid mechanics.",
            "id": "3605427",
            "problem": "Design a numerically stable algorithm to compute the natural logarithm of the Jacobian determinant, $\\,\\ln J\\,$, where $\\,J = \\det F\\,$ and $\\,F \\in \\mathbb{R}^{n \\times n}\\,$ is a real square matrix representing a deformation gradient in computational solid mechanics. Assume that $\\,F\\,$ may be ill-conditioned, and that in physically meaningful cases $\\,J>0\\,$, but allow that $\\,J<0\\,$ can arise in artificial tests. Your solution must proceed from first principles and core definitions. Begin only from the following foundational facts: (i) the existence of the Singular Value Decomposition (SVD) stating that any real matrix $\\,F\\,$ admits $\\,F = U \\Sigma V^{T}\\,$ with $\\,U, V\\,$ orthogonal and $\\,\\Sigma\\,$ diagonal with nonnegative entries called singular values, (ii) the determinant of an orthogonal matrix has magnitude $\\,1\\,$, (iii) the definition of the determinant as a multilinear, alternating form equal to the product of eigenvalues for normal matrices, and (iv) the Fréchet differential of the determinant satisfies $\\,\\mathrm{d}(\\det F)[\\delta F] = \\det(F)\\,\\mathrm{tr}(F^{-1}\\delta F)\\,$ for invertible $\\,F\\,$, implying $\\,\\mathrm{d}(\\ln \\det F)[\\delta F] = \\mathrm{tr}(F^{-1}\\delta F)\\,$. Do not assume or state any closed-form shortcut for $\\,\\ln \\det F\\,$ in terms of the singular values; instead, derive any such expression you use from these bases.\n\nYour task has three parts:\n\n1) Derivation and conditioning. Using only the foundational facts above, derive an expression for $\\,\\ln | \\det F |\\,$ in terms of quantities from the Singular Value Decomposition (SVD) and justify, using perturbation analysis and the Fréchet differential, the conditioning of this computation. Your justification must identify which aspects of $\\,F\\,$ control sensitivity and must connect to the smallest singular value. Provide a bound for the absolute perturbation $\\,|\\delta(\\ln |\\det F|)|\\,$ in terms of a matrix norm of $\\,F^{-1}\\,$ and a norm of $\\,\\delta F\\,$.\n\n2) Numerically stable algorithm. Propose a numerically stable algorithm to compute $\\,\\ln |\\det F|\\,$ for ill-conditioned $\\,F\\,$ by using only operations that do not suffer from catastrophic underflow or overflow. Your algorithm must:\n- Use the Singular Value Decomposition (SVD) $\\,F = U \\Sigma V^{T}\\,$ explicitly in its design.\n- Employ dynamic scaling by a power of two to prevent intermediate overflow or underflow in floating-point arithmetic. Let $\\,k = \\lfloor \\log_{2}\\|F\\|_{F}\\rfloor\\,$, scale by $\\,a = 2^{-k}\\,$, and relate $\\,\\ln |\\det F|\\,$ to the singular values of $\\,aF\\,$ without multiplying singular values together.\n- Also compute the orientation sign $\\,s = \\operatorname{sign}(\\det F)\\in\\{-1,+1\\}\\,$ from orthogonal factors.\n\n3) Implementation and test suite. Implement your algorithm as a program that takes no input and evaluates a fixed set of test matrices $\\,F_i\\,$, producing the required outputs. Angles are to be interpreted in radians, and logarithms are natural logarithms. The program shall evaluate the following five square matrices in $\\,\\mathbb{R}^{3\\times 3}\\,$:\n- Case $\\,1\\,$ (well-conditioned, orientation-preserving): $F_1 = R_z(\\theta_1)\\,\\mathrm{diag}(2.0,\\,0.5,\\,1.5)\\,R_y(\\phi_1)^{T}$ with $\\theta_1=0.7$ and $\\phi_1=-0.4$.\n- Case $\\,2\\,$ (highly ill-conditioned but with unit volume change): $F_2 = R_x(\\alpha_2)\\,\\mathrm{diag}(10^{-12},\\,1.0,\\,10^{12})\\,R_z(\\beta_2)^{T}$ with $\\alpha_2=0.3$ and $\\beta_2=1.2$.\n- Case $\\,3\\,$ (very large scales, risk of overflow if naively multiplied): $F_3 = R_y(\\alpha_3)\\,\\mathrm{diag}(10^{100},\\,2\\cdot 10^{100},\\,5\\cdot 10^{99})\\,R_x(\\beta_3)^{T}$ with $\\alpha_3=0.9$ and $\\beta_3=-1.1$.\n- Case $\\,4\\,$ (extreme scale separation, risk of underflow if naively multiplied): $F_4 = R_z(\\theta_4)\\,\\mathrm{diag}(10^{-300},\\,10^{-10},\\,10^{20})\\,R_y(\\phi_4)^{T}$ with $\\theta_4=-0.8$ and $\\phi_4=0.6$.\n- Case $\\,5\\,$ (orientation-reversing): $F_5 = R_x(\\alpha_5)\\,\\mathrm{diag}(-2.0,\\,3.0,\\,5.0)\\,R_y(\\beta_5)^{T}$ with $\\alpha_5=0.5$ and $\\beta_5=-0.7$.\n\nHere, for $\\,\\gamma \\in \\mathbb{R}\\,$,\n$$\nR_x(\\gamma)=\\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & \\cos\\gamma & -\\sin\\gamma\\\\\n0 & \\sin\\gamma & \\cos\\gamma\n\\end{bmatrix},\\quad\nR_y(\\gamma)=\\begin{bmatrix}\n\\cos\\gamma & 0 & \\sin\\gamma\\\\\n0 & 1 & 0\\\\\n-\\sin\\gamma & 0 & \\cos\\gamma\n\\end{bmatrix},\\quad\nR_z(\\gamma)=\\begin{bmatrix}\n\\cos\\gamma & -\\sin\\gamma & 0\\\\\n\\sin\\gamma & \\cos\\gamma & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}.\n$$\n\nFor each case, your program must output a list $\\,[\\ln|\\det F_i|,\\,s_i]\\,$, where $\\,s_i \\in\\{-1,+1\\}\\,$ is the orientation sign computed from orthogonal SVD factors. Aggregate all five results into one list. The final output format must be a single line containing a comma-separated list of the five inner lists enclosed in square brackets, for example $\\,\\big[\\,[\\ell_1,s_1],[\\ell_2,s_2],\\dots,[\\ell_5,s_5]\\,\\big]\\,$, with no spaces.\n\nYour implementation must use the natural logarithm and radians for angles and must not read any input. The outputs are floating-point numbers for $\\,\\ln|\\det F_i|\\,$ and integers for $\\,s_i\\,$. This constitutes the test suite and output specification.",
            "solution": "The task is to design, justify, and implement a numerically stable algorithm for computing the natural logarithm of the Jacobian determinant, $\\ln J = \\ln(\\det F)$, for a real square matrix $F \\in \\mathbb{R}^{n \\times n}$ which may be ill-conditioned. The solution is presented in three parts as requested: derivation and conditioning analysis, algorithm design, and implementation.\n\n**Part 1: Derivation and Conditioning Analysis**\n\nThis part derives an expression for $\\ln|\\det F|$ based on the Singular Value Decomposition (SVD) and analyzes the conditioning of the problem.\n\n**Derivation of $\\ln|\\det F|$**\n\nWe are given the following foundational facts:\n(i) Any real matrix $F \\in \\mathbb{R}^{n \\times n}$ admits a Singular Value Decomposition (SVD), $F = U \\Sigma V^{T}$, where $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix with non-negative entries $\\sigma_i \\ge 0$, known as the singular values of $F$.\n(ii) The determinant of any orthogonal matrix has a magnitude of $1$.\n(iii) The determinant is a multilinear, alternating form, and for a normal matrix, it is the product of its eigenvalues.\n\nUsing the multiplicative property of determinants, we can write the determinant of $F$ as:\n$$\n\\det F = \\det(U \\Sigma V^{T}) = (\\det U) (\\det \\Sigma) (\\det V^{T})\n$$\nFrom fact (ii), as $U$ and $V$ are orthogonal, their determinants are $\\det U = \\pm 1$ and $\\det V = \\pm 1$. Since the determinant of a transpose of a matrix is equal to the determinant of the matrix itself, $\\det V^T = \\det V = \\pm 1$.\n\nThe matrix $\\Sigma$ is a diagonal matrix, which is a special case of a normal matrix. From fact (iii), its determinant is the product of its eigenvalues. The eigenvalues of a diagonal matrix are its diagonal entries, which in this case are the singular values $\\sigma_i$ of $F$. Therefore:\n$$\n\\det \\Sigma = \\prod_{i=1}^{n} \\sigma_i\n$$\nCombining these results, we obtain an expression for the determinant of $F$ in terms of its SVD components:\n$$\n\\det F = (\\det U) (\\det V^T) \\prod_{i=1}^{n} \\sigma_i\n$$\nTo find an expression for $\\ln|\\det F|$, we first take the absolute value of the determinant:\n$$\n|\\det F| = |(\\det U) (\\det V^T)| \\left| \\prod_{i=1}^{n} \\sigma_i \\right|\n$$\nSince $|\\det U| = 1$, $|\\det V^T| = 1$, and singular values $\\sigma_i$ are non-negative, the product $\\prod_{i=1}^{n} \\sigma_i$ is also non-negative. Thus, we have:\n$$\n|\\det F| = \\prod_{i=1}^{n} \\sigma_i\n$$\nThis expression demonstrates that the volume-change factor $|\\det F|$ is determined solely by the product of the singular values. Applying the natural logarithm to both sides, and using the property that the logarithm of a product is the sum of the logarithms, we arrive at the desired expression:\n$$\n\\ln|\\det F| = \\ln\\left(\\prod_{i=1}^{n} \\sigma_i\\right) = \\sum_{i=1}^{n} \\ln \\sigma_i\n$$\nThis derivation relies only on the foundational facts provided. This formula is the basis for a numerically stable computation, as it avoids the explicit multiplication of singular values, which could lead to numerical overflow or underflow.\n\n**Conditioning Analysis**\n\nWe are given fact (iv), the Fréchet differential of the determinant, which for an invertible matrix $F$ leads to the differential of its natural logarithm:\n$$\n\\mathrm{d}(\\ln \\det F)[\\delta F] = \\mathrm{tr}(F^{-1} \\delta F)\n$$\nwhere $\\delta F$ is a perturbation to the matrix $F$. For a small perturbation, this implies that the change in $\\ln(\\det F)$, denoted $\\delta(\\ln \\det F)$, can be approximated by:\n$$\n\\delta(\\ln \\det F) \\approx \\mathrm{tr}(F^{-1} \\delta F)\n$$\nThe problem concerns $\\ln|\\det F|$. If we assume the perturbation $\\delta F$ is small enough such that $\\mathrm{sign}(\\det(F+\\delta F)) = \\mathrm{sign}(\\det F)$, then $\\delta(\\ln|\\det F|) = \\delta(\\ln(\\det F))$ if $\\det F > 0$, and $\\delta(\\ln(-\\det F))$ if $\\det F < 0$. In both cases, the differential is $\\mathrm{tr}(F^{-1}\\delta F)$. So, we can analyze the perturbation of $\\ln|\\det F|$ as:\n$$\n|\\delta(\\ln|\\det F|)| \\approx |\\mathrm{tr}(F^{-1} \\delta F)|\n$$\nTo bound this expression, we can use the Cauchy-Schwarz inequality for the Frobenius inner product of matrices, which is defined for real matrices $A, B \\in \\mathbb{R}^{n \\times n}$ as $\\langle A, B \\rangle_F = \\mathrm{tr}(A^T B)$. The inequality states $|\\langle A, B \\rangle_F| \\le \\|A\\|_F \\|B\\|_F$. Letting $A = (F^{-1})^T$ and $B = \\delta F$, we get:\n$$\n|\\mathrm{tr}(((F^{-1})^T)^T \\delta F)| = |\\mathrm{tr}(F^{-1} \\delta F)| \\le \\|(F^{-1})^T\\|_F \\|\\delta F\\|_F\n$$\nSince the Frobenius norm is invariant under transposition, $\\|(F^{-1})^T\\|_F = \\|F^{-1}\\|_F$, we obtain the bound:\n$$\n|\\delta(\\ln|\\det F|)| \\lesssim \\|F^{-1}\\|_F \\|\\delta F\\|_F\n$$\nThis inequality relates the absolute perturbation in $\\ln|\\det F|$ to the norms of the inverse of $F$ and the perturbation $\\delta F$.\n\nTo connect this sensitivity to the singular values of $F$, we express $\\|F^{-1}\\|_F$ in terms of the singular values of $F$. The SVD of $F^{-1}$ is $F^{-1} = (U \\Sigma V^T)^{-1} = V \\Sigma^{-1} U^T$. The singular values of $F^{-1}$ are the reciprocals of the singular values of $F$, i.e., $1/\\sigma_i$. The Frobenius norm of $F^{-1}$ is the square root of the sum of squares of its singular values:\n$$\n\\|F^{-1}\\|_F = \\sqrt{\\sum_{i=1}^{n} \\left(\\frac{1}{\\sigma_i}\\right)^2}\n$$\nLet $\\sigma_{\\min} = \\min_{i} \\{\\sigma_i\\}$ be the smallest singular value of $F$. The norm of the inverse is then bounded below by $\\|F^{-1}\\|_F \\ge \\sqrt{(1/\\sigma_{\\min})^2} = 1/\\sigma_{\\min}$. If $\\sigma_{\\min}$ is very small, $F$ is ill-conditioned (close to singular), and $\\|F^{-1}\\|_F$ becomes very large. Consequently, the term $\\|F^{-1}\\|_F \\|\\delta F\\|_F$ can be large even for a small perturbation $\\|\\delta F\\|_F$, indicating that the computation of $\\ln|\\det F|$ is sensitive to perturbations in $F$. The conditioning of the problem is thus critically controlled by the smallest singular value of $F$.\n\n**Part 2: Numerically Stable Algorithm**\n\nA naive computation of $\\ln|\\det F|$ by first calculating $J = \\det F$ and then taking the logarithm is prone to numerical issues. If $F$ has singular values that are very large or very small, their product $J$ can easily overflow or underflow standard floating-point representations before the logarithm can be taken. The derived expression $\\ln|\\det F| = \\sum_i \\ln \\sigma_i$ circumvents this by converting the product into a sum, which is a much more stable operation.\n\nThe problem, however, specifies that the SVD computation itself might be affected by extreme scaling of $F$. To preempt this, a dynamic scaling procedure is required.\n\nThe proposed algorithm is as follows:\n\n1.  **Scaling**: Given the matrix $F \\in \\mathbb{R}^{n \\times n}$, compute its Frobenius norm, $\\|F\\|_F = \\sqrt{\\sum_{i,j=1}^{n} F_{ij}^2}$. To bring the matrix entries into a numerically favorable range (of order $1$), we scale $F$ by a power of $2$.\n    - Calculate the scaling exponent $k = \\lfloor \\log_2 \\|F\\|_F \\rfloor$. This can be computed using `floor(log2(norm))`.\n    - Define the scaling factor $a = 2^{-k}$.\n    - Form the scaled matrix $F' = aF$. The norm of this matrix will be in the range $1 \\le \\|F'\\|_F < 2$.\n\n2.  **SVD of Scaled Matrix**: Compute the SVD of the well-scaled matrix $F' = U' \\Sigma' (V')^T$. This yields the orthogonal matrices $U'$ and $(V')^T$, and the diagonal matrix $\\Sigma'$ containing the singular values $\\sigma'_i$ of $F'$.\n\n3.  **Compute $\\ln|\\det F|$**: We relate $\\det F$ to $\\det F'$. From $F' = aF$, we have $\\det(F') = \\det(aF) = a^n \\det F$. Thus, $\\det F = a^{-n} \\det F'$.\n    Taking the natural logarithm of the absolute value:\n    $$\n    \\ln|\\det F| = \\ln|a^{-n} \\det F'| = \\ln(a^{-n}) + \\ln|\\det F'| = -n \\ln(a) + \\sum_{i=1}^{n} \\ln \\sigma'_i\n    $$\n    Substituting $a = 2^{-k}$, we have $\\ln a = \\ln(2^{-k}) = -k \\ln 2$. The final expression for the computation is:\n    $$\n    \\ln|\\det F| = -n(-k \\ln 2) + \\sum_{i=1}^{n} \\ln \\sigma'_i = nk \\ln 2 + \\sum_{i=1}^{n} \\ln \\sigma'_i\n    $$\n    This computation is numerically stable, as it involves summing logarithms of the well-behaved singular values $\\sigma'_i$ and adding a scaled term. It completely avoids multiplying the original singular values $\\sigma_i = \\sigma'_i/a$.\n\n4.  **Compute Orientation Sign**: The orientation sign is $s = \\mathrm{sign}(\\det F)$. Since the scaling factor $a = 2^{-k}$ is positive, $\\mathrm{sign}(\\det F) = \\mathrm{sign}(\\det F')$.\n    From the SVD of $F'$, we have $\\det F' = (\\det U') (\\det \\Sigma') (\\det (V')^T)$. For a non-singular matrix, all singular values $\\sigma'_i$ are positive, so $\\det \\Sigma' = \\prod_i \\sigma'_i > 0$.\n    The sign is therefore determined by the determinants of the orthogonal factors:\n    $$\n    s = \\mathrm{sign}(\\det F) = \\mathrm{sign}((\\det U')(\\det(V')^T)) = (\\det U')(\\det(V')^T)\n    $$\n    The determinants of $U'$ and $(V')^T$ are computed numerically. Since they are known to be $\\pm 1$, the product can be rounded to the nearest integer to yield a robust sign of $\\pm 1$.\n\n**Summary of the Algorithm**\nFor a given matrix $F \\in \\mathbb{R}^{n \\times n}$:\na. If $F$ is the zero matrix, $|\\det F| = 0$ and $\\ln|\\det F| = -\\infty$. The sign is undefined, but can be taken as $0$ or $1$. We assume $F$ is non-zero.\nb. Compute the Frobenius norm $\\|F\\|_F$.\nc. Calculate the scaling exponent $k = \\lfloor \\log_2 \\|F\\|_F \\rfloor$ and the factor $a = 2^{-k}$.\nd. Construct the scaled matrix $F' = aF$.\ne. Perform SVD on $F'$ to obtain $U'$, singular values $\\sigma'_i$, and $V'^T$.\nf. Calculate $\\ln|\\det F| = n k \\ln(2) + \\sum_i \\ln(\\sigma'_i)$.\ng. Calculate the sign $s = \\mathrm{round}(\\det(U') \\cdot \\det(V'^T))$.\nh. Return the pair $[\\ln|\\det F|, s]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Designs and executes a numerically stable algorithm to compute ln|det(F)| and sign(det(F))\n    for a given set of test matrices F.\n    \"\"\"\n\n    def Rx(gamma):\n        \"\"\"Rotation matrix about x-axis.\"\"\"\n        c = np.cos(gamma)\n        s = np.sin(gamma)\n        return np.array([[1, 0, 0], [0, c, -s], [0, s, c]], dtype=np.float64)\n\n    def Ry(gamma):\n        \"\"\"Rotation matrix about y-axis.\"\"\"\n        c = np.cos(gamma)\n        s = np.sin(gamma)\n        return np.array([[c, 0, s], [0, 1, 0], [-s, 0, c]], dtype=np.float64)\n\n    def Rz(gamma):\n        \"\"\"Rotation matrix about z-axis.\"\"\"\n        c = np.cos(gamma)\n        s = np.sin(gamma)\n        return np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]], dtype=np.float64)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (well-conditioned, orientation-preserving)\n        Rz(0.7) @ np.diag([2.0, 0.5, 1.5]) @ Ry(-0.4).T,\n        # Case 2 (highly ill-conditioned but with unit volume change)\n        Rx(0.3) @ np.diag([1e-12, 1.0, 1e12]) @ Rz(1.2).T,\n        # Case 3 (very large scales, risk of overflow)\n        Ry(0.9) @ np.diag([1e100, 2e100, 5e99]) @ Rx(-1.1).T,\n        # Case 4 (extreme scale separation, risk of underflow)\n        Rz(-0.8) @ np.diag([1e-300, 1e-10, 1e20]) @ Ry(0.6).T,\n        # Case 5 (orientation-reversing)\n        Rx(0.5) @ np.diag([-2.0, 3.0, 5.0]) @ Ry(-0.7).T\n    ]\n\n    results = []\n    \n    # Process each test case using the derived numerically stable algorithm\n    for F in test_cases:\n        n = F.shape[0]\n\n        # Step 1: Scaling\n        norm_F = np.linalg.norm(F, 'fro')\n        \n        # Handle the case of a zero matrix, though not in test suite.\n        if norm_F == 0:\n            log_det_abs = -np.inf\n            sign_det = 0 # Or undefined. Problem asks for +/-1. Assume non-singular.\n            results.append([log_det_abs, sign_det])\n            continue\n            \n        k = np.floor(np.log2(norm_F))\n        a = 2.0**(-k)\n        \n        # Step 2: SVD of Scaled Matrix\n        F_scaled = a * F\n        U, s_scaled, Vt = np.linalg.svd(F_scaled)\n\n        # Step 3: Compute ln|det F|\n        # ln|det F| = nk*ln(2) + sum(ln(sigma_i_scaled))\n        log_det_abs = n * k * np.log(2) + np.sum(np.log(s_scaled))\n\n        # Step 4: Compute Orientation Sign\n        # sign(det F) = det(U) * det(Vt)\n        det_U = np.linalg.det(U)\n        det_Vt = np.linalg.det(Vt)\n        sign_det = int(np.round(det_U * det_Vt))\n        \n        results.append([log_det_abs, sign_det])\n\n    # Final print statement in the exact required format.\n    # Convert list of lists to string and remove spaces.\n    output_str = str(results).replace(' ', '')\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}