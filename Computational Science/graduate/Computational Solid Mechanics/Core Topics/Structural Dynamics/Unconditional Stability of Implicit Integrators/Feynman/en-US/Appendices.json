{
    "hands_on_practices": [
        {
            "introduction": "A deep understanding of unconditional stability begins with the rigorous analysis of an integrator's behavior on a simple test problem. This practice guides you through the fundamental derivation of stability properties for the versatile generalized-$\\alpha$ method, a cornerstone of modern computational dynamics . By deriving the amplification factor from first principles and mapping its parameters to the requirements of A-stability and L-stability, you will gain the essential tools to classify and critically evaluate any implicit time integration scheme.",
            "id": "3608611",
            "problem": "Consider the generalized-$\\alpha$ time integrator for first-order systems in computational solid mechanics applied to the Dahlquist test equation $ \\dot{y} = \\lambda y $ with $ \\lambda \\in \\mathbb{C} $. Let $ \\Delta t  0 $, $ z = \\lambda \\Delta t $, and the method be defined by the update and stage relations\n\n$$\ny^{n+1} = y^{n} + \\Delta t \\left[ (1 - \\gamma)\\,\\dot{y}^{n} + \\gamma\\,\\dot{y}^{n+1} \\right],\\quad\n\\dot{y}^{n+\\alpha_m} = (1 - \\alpha_m)\\,\\dot{y}^{n} + \\alpha_m\\,\\dot{y}^{n+1},\\quad\ny^{n+\\alpha_f} = (1 - \\alpha_f)\\,y^{n} + \\alpha_f\\,y^{n+1},\n$$\n\ntogether with the equilibrium at the generalized time level,\n\n$$\n\\dot{y}^{n+\\alpha_m} = \\lambda\\, y^{n+\\alpha_f}.\n$$\n\nAssume $ 0  \\alpha_f \\leq 1 $ and impose the constraint $ \\gamma = \\alpha_m $ to eliminate spurious underdetermination and ensure a one-parameter family of implicit schemes. Starting from these definitions alone (no pre-tabulated stability functions), carry out the following steps:\n\n1) Derive the scalar amplification factor $ R(z) $ defined by $ y^{n+1} = R(z)\\,y^{n} $ as an explicit rational function of $ z $ and $ \\alpha_f $ under the constraint $ \\gamma = \\alpha_m $.\n\n2) Using only the definition of $A$-stability, namely that a one-step method is $A$-stable if $ |R(z)| \\leq 1 $ for all $ z $ with $ \\operatorname{Re}(z) \\leq 0 $, determine the condition on $ \\alpha_f $ ensuring $A$-stability of the derived $ R(z) $.\n\n3) The method is called $L$-stable if it is $A$-stable and $ \\lim_{|z|\\to\\infty,\\, \\operatorname{Re}(z)0} R(z) = 0 $. Determine whether this can occur within the family obtained in step 1), and if so, at which value(s) of $ \\alpha_f $.\n\n4) Lift the scalar analysis to a matrix ordinary differential equation $ \\dot{\\mathbf{y}} = \\mathbf{A}\\,\\mathbf{y} $ with $ \\mathbf{A} \\in \\mathbb{C}^{N\\times N} $ having a Jordan decomposition with at least one defective Jordan block, and all eigenvalues $ \\lambda $ satisfying $ \\operatorname{Re}(\\lambda)  0 $. Show, starting from the functional calculus for Jordan blocks, that unconditional stability (asymptotic decay $ \\mathbf{y}^{n} \\to \\mathbf{0} $ as $ n \\to \\infty $ for any fixed $ \\Delta t  0 $) follows under the same condition on $ \\alpha_f $ you obtained in step 2), even in the presence of nilpotent parts.\n\nYour final task is to compute, in exact closed form, the high-frequency amplification factor (also called the spectral radius at infinity)\n\n$$\n\\rho_{\\infty} \\equiv \\lim_{|z|\\to\\infty,\\, \\operatorname{Re}(z)0} \\big|R(z)\\big|\n$$\n\nfor the family defined by $ \\gamma = \\alpha_m $; express $ \\rho_{\\infty} $ purely in terms of $ \\alpha_f $. Provide this $ \\rho_{\\infty} $ as your final answer. No rounding is required, and no units are involved.",
            "solution": "The problem is subjected to validation and is deemed valid. It is scientifically grounded in the theory of numerical methods for ordinary differential equations, well-posed, objective, and contains no discernible flaws. The solution proceeds as follows.\n\nThe problem asks for an analysis of the generalized-$\\alpha$ method for first-order systems, subject to the constraint $\\gamma = \\alpha_m$. The analysis proceeds in several steps, culminating in the calculation of the high-frequency amplification factor $\\rho_{\\infty}$.\n\n**1) Derivation of the Amplification Factor $R(z)$**\n\nThe method is defined by the following set of equations applied to the Dahlquist test equation $\\dot{y} = \\lambda y$:\n1. Update rule: $y^{n+1} = y^{n} + \\Delta t [ (1 - \\gamma)\\,\\dot{y}^{n} + \\gamma\\,\\dot{y}^{n+1} ]$\n2. Stage value for the derivative: $\\dot{y}^{n+\\alpha_m} = (1 - \\alpha_m)\\,\\dot{y}^{n} + \\alpha_m\\,\\dot{y}^{n+1}$\n3. Stage value for the solution: $y^{n+\\alpha_f} = (1 - \\alpha_f)\\,y^{n} + \\alpha_f\\,y^{n+1}$\n4. Equilibrium at the intermediate point: $\\dot{y}^{n+\\alpha_m} = \\lambda\\, y^{n+\\alpha_f}$\n\nWe begin by substituting the stage values (2) and (3) into the equilibrium equation (4):\n$$\n(1 - \\alpha_m)\\,\\dot{y}^{n} + \\alpha_m\\,\\dot{y}^{n+1} = \\lambda [ (1 - \\alpha_f)\\,y^{n} + \\alpha_f\\,y^{n+1} ]\n$$\nFor the test equation, we have $\\dot{y}^n = \\lambda y^n$. Substituting this into the equation above, we obtain an equation relating the unknowns $y^{n+1}$ and $\\dot{y}^{n+1}$ to the known value $y^n$:\n$$\n(1 - \\alpha_m)\\,\\lambda y^{n} + \\alpha_m\\,\\dot{y}^{n+1} = \\lambda (1 - \\alpha_f)\\,y^{n} + \\lambda \\alpha_f\\,y^{n+1}\n$$\nRearranging to group unknowns on one side:\n$$\n\\alpha_m\\,\\dot{y}^{n+1} - \\lambda \\alpha_f\\,y^{n+1} = \\lambda (1 - \\alpha_f - (1 - \\alpha_m))\\,y^{n} = \\lambda (\\alpha_m - \\alpha_f)\\,y^{n}\n$$\nThis is our first equation for the unknowns. The second is the update rule (1). Let's use the definition $z = \\lambda \\Delta t$ and substitute $\\dot{y}^n = \\lambda y^n = (z/\\Delta t) y^n$:\n$$\ny^{n+1} = y^{n} + \\Delta t (1 - \\gamma)\\,\\lambda y^{n} + \\Delta t \\gamma\\,\\dot{y}^{n+1} = (1 + (1-\\gamma)z)y^n + \\gamma \\Delta t \\dot{y}^{n+1}\n$$\nRearranging this second equation:\n$$\n-y^{n+1} + \\gamma \\Delta t \\dot{y}^{n+1} = -(1 + (1-\\gamma)z)y^n\n$$\nWe now have a $2 \\times 2$ linear system for $\\dot{y}^{n+1}$ and $y^{n+1}$:\n$$\n\\begin{cases}\n\\alpha_m\\,\\dot{y}^{n+1} - \\lambda \\alpha_f\\,y^{n+1} = \\lambda (\\alpha_m - \\alpha_f)\\,y^{n} \\\\\n\\gamma \\Delta t \\dot{y}^{n+1} - y^{n+1} = -(1 + (1-\\gamma)z)y^n\n\\end{cases}\n$$\nWe seek $y^{n+1} = R(z)y^n$, so we eliminate $\\dot{y}^{n+1}$. Multiply the first equation by $\\gamma \\Delta t$ and the second by $\\alpha_m$:\n$$\n\\begin{cases}\n\\alpha_m \\gamma \\Delta t\\,\\dot{y}^{n+1} - \\lambda \\alpha_f \\gamma \\Delta t\\,y^{n+1} = \\lambda (\\alpha_m - \\alpha_f) \\gamma \\Delta t\\,y^{n} \\\\\n\\alpha_m \\gamma \\Delta t \\dot{y}^{n+1} - \\alpha_m y^{n+1} = -\\alpha_m(1 + (1-\\gamma)z)y^n\n\\end{cases}\n$$\nSubtracting the second equation from the first yields:\n$$\n(-\\lambda \\alpha_f \\gamma \\Delta t + \\alpha_m) y^{n+1} = [\\lambda (\\alpha_m - \\alpha_f) \\gamma \\Delta t + \\alpha_m(1 + (1-\\gamma)z)] y^n\n$$\nUsing $z = \\lambda \\Delta t$:\n$$\n(-\\alpha_f \\gamma z + \\alpha_m) y^{n+1} = [(\\alpha_m - \\alpha_f) \\gamma z + \\alpha_m + \\alpha_m(1-\\gamma)z] y^n\n$$\n$$\n(\\alpha_m - \\alpha_f \\gamma z) y^{n+1} = [\\alpha_m + (\\alpha_m \\gamma - \\alpha_f \\gamma + \\alpha_m - \\alpha_m \\gamma)z] y^n\n$$\n$$\n(\\alpha_m - \\alpha_f \\gamma z) y^{n+1} = [\\alpha_m + (\\alpha_m - \\alpha_f \\gamma)z] y^n\n$$\nThe amplification factor is therefore $R(z) = \\frac{\\alpha_m + (\\alpha_m - \\alpha_f \\gamma)z}{\\alpha_m - \\alpha_f \\gamma z}$.\nNow, we apply the constraint $\\gamma = \\alpha_m$:\n$$\nR(z) = \\frac{\\alpha_m + (\\alpha_m - \\alpha_f \\alpha_m)z}{\\alpha_m - \\alpha_f \\alpha_m z} = \\frac{\\alpha_m(1 + (1-\\alpha_f)z)}{\\alpha_m(1 - \\alpha_f z)} = \\frac{1 + (1-\\alpha_f)z}{1 - \\alpha_f z}\n$$\n\n**2) A-stability Analysis**\n\nA method is A-stable if its stability region includes the entire left half-plane of complex numbers, i.e., $|R(z)| \\le 1$ for all $z \\in \\mathbb{C}$ with $\\operatorname{Re}(z) \\le 0$. Since $R(z)$ is a rational function analytic in $\\operatorname{Re}(z) \\le 0$ (its pole is at $z_p = 1/\\alpha_f$, which is on the positive real axis since $\\alpha_f  0$), the maximum modulus principle allows us to check the condition on the boundary of the domain, i.e., for $z = i\\omega$ where $\\omega \\in \\mathbb{R}$.\n$$\n|R(i\\omega)|^2 = \\left| \\frac{1 + (1-\\alpha_f)i\\omega}{1 - \\alpha_f i\\omega} \\right|^2 = \\frac{|1 + (1-\\alpha_f)i\\omega|^2}{|1 - \\alpha_f i\\omega|^2} = \\frac{1^2 + (1-\\alpha_f)^2\\omega^2}{1^2 + (-\\alpha_f)^2\\omega^2} = \\frac{1 + (1-\\alpha_f)^2\\omega^2}{1 + \\alpha_f^2\\omega^2}\n$$\nThe condition $|R(i\\omega)|^2 \\le 1$ translates to:\n$$\n\\frac{1 + (1-\\alpha_f)^2\\omega^2}{1 + \\alpha_f^2\\omega^2} \\le 1\n$$\nSince $1 + \\alpha_f^2\\omega^2  0$, we can multiply both sides by it:\n$$\n1 + (1-\\alpha_f)^2\\omega^2 \\le 1 + \\alpha_f^2\\omega^2 \\implies (1-\\alpha_f)^2\\omega^2 \\le \\alpha_f^2\\omega^2\n$$\nThis inequality must hold for all $\\omega \\in \\mathbb{R}$. This is true if and only if $(1-\\alpha_f)^2 \\le \\alpha_f^2$.\n$$\n1 - 2\\alpha_f + \\alpha_f^2 \\le \\alpha_f^2 \\implies 1 - 2\\alpha_f \\le 0 \\implies 1 \\le 2\\alpha_f \\implies \\alpha_f \\ge \\frac{1}{2}\n$$\nGiven the problem constraint $0  \\alpha_f \\le 1$, the condition for A-stability is $\\frac{1}{2} \\le \\alpha_f \\le 1$.\n\n**3) L-stability Analysis**\n\nA method is L-stable if it is A-stable and, additionally, the amplification factor vanishes at infinity in the left half-plane: $\\lim_{|z|\\to\\infty, \\operatorname{Re}(z)0} R(z) = 0$.\nLet's compute this limit for the derived $R(z)$:\n$$\n\\lim_{|z|\\to\\infty} R(z) = \\lim_{|z|\\to\\infty} \\frac{1 + (1-\\alpha_f)z}{1 - \\alpha_f z} = \\lim_{|z|\\to\\infty} \\frac{1/z + (1-\\alpha_f)}{1/z - \\alpha_f} = \\frac{1-\\alpha_f}{-\\alpha_f} = \\frac{\\alpha_f - 1}{\\alpha_f}\n$$\nFor L-stability, this limit must be zero.\n$$\n\\frac{\\alpha_f-1}{\\alpha_f} = 0 \\implies \\alpha_f-1 = 0 \\implies \\alpha_f = 1\n$$\nThis value lies within the A-stability range $[\\frac{1}{2}, 1]$. Thus, the method is L-stable if and only if $\\alpha_f = 1$. In this case, $R(z) = (1-z)^{-1}$, which is the amplification factor for the backward Euler method, known to be L-stable.\n\n**4) Unconditional Stability for Matrix Systems**\n\nFor a linear system $\\dot{\\mathbf{y}} = \\mathbf{A}\\,\\mathbf{y}$, the numerical scheme can be written as $\\mathbf{y}^{n+1} = R(\\mathbf{A}\\Delta t)\\mathbf{y}^n$, where $R(\\mathbf{A}\\Delta t)$ is the matrix amplification operator, defined through a functional calculus. Let $\\mathbf{z} = \\mathbf{A}\\Delta t$.\n$$\nR(\\mathbf{z}) = (I + (1-\\alpha_f)\\mathbf{z})(I - \\alpha_f \\mathbf{z})^{-1}\n$$\nThe asymptotic behavior of the solution, $\\mathbf{y}^n \\to \\mathbf{0}$ as $n \\to \\infty$, is governed by the spectral radius of the amplification matrix, $\\rho(R(\\mathbf{z}))$. Unconditional stability requires $\\rho(R(\\mathbf{z}))  1$ for any choice of time step $\\Delta t  0$.\n\nLet $\\mathbf{A} = \\mathbf{PJP}^{-1}$ be the Jordan normal form of $\\mathbf{A}$, where $\\mathbf{J}$ is a block-diagonal matrix of Jordan blocks $J_k$. Then $\\mathbf{z} = \\mathbf{P}(\\mathbf{J}\\Delta t)\\mathbf{P}^{-1}$. The amplification matrix is $R(\\mathbf{z}) = \\mathbf{P}R(\\mathbf{J}\\Delta t)\\mathbf{P}^{-1}$.\nThe spectral radius is invariant under similarity transformations, so $\\rho(R(\\mathbf{z})) = \\rho(R(\\mathbf{J}\\Delta t))$. Since $R(\\mathbf{J}\\Delta t)$ is block-diagonal, its spectral radius is the maximum of the spectral radii of its diagonal blocks, $R(J_k \\Delta t)$.\nEach matrix $J_k \\Delta t$ corresponds to an eigenvalue $\\lambda_k$ of $\\mathbf{A}$ and is upper triangular. The matrix function $R(J_k \\Delta t)$ is also upper triangular, and its diagonal entries (which are its eigenvalues) are all equal to $R(\\lambda_k \\Delta t)$. Therefore, the spectral radius of each block is $\\rho(R(J_k \\Delta t)) = |R(\\lambda_k \\Delta t)|$.\nThe overall spectral radius is $\\rho(R(\\mathbf{z})) = \\max_k |R(\\lambda_k \\Delta t)|$.\n\nThe problem states that all eigenvalues $\\lambda_k$ of $\\mathbf{A}$ satisfy $\\operatorname{Re}(\\lambda_k)  0$. Since $\\Delta t  0$, this means $\\operatorname{Re}(\\lambda_k \\Delta t)  0$.\nIn the A-stability analysis, we found that for $\\alpha_f \\ge 1/2$, we have $|R(z)| \\le 1$ for $\\operatorname{Re}(z) \\le 0$. A closer look at the derivation reveals that strict inequality holds when $\\operatorname{Re}(z)  0$.\nSpecifically, we showed that $|R(z)|^2  1$ is equivalent to $2x + (1-2\\alpha_f)(x^2+\\omega^2)  0$ for $z=x+i\\omega$. For $\\alpha_f \\ge 1/2$, we have $1-2\\alpha_f \\le 0$. For $x = \\operatorname{Re}(z)  0$, both terms $2x$ and $(1-2\\alpha_f)(x^2+\\omega^2)$ are non-positive, and their sum is strictly negative (unless $z=0$, which is excluded by $\\operatorname{Re}(z)0$).\nThus, for all eigenvalues $\\lambda_k$ of $\\mathbf{A}$, we have $\\operatorname{Re}(\\lambda_k \\Delta t)  0$, which implies $|R(\\lambda_k \\Delta t)|  1$.\nConsequently, $\\rho(R(\\mathbf{z})) = \\max_k |R(\\lambda_k \\Delta t)|  1$.\n\nA well-known result from matrix theory states that if the spectral radius of a matrix $\\mathbf{M}$ is less than $1$, then $\\lim_{n \\to \\infty} \\mathbf{M}^n = \\mathbf{0}$. Since $\\mathbf{y}^n = (R(\\mathbf{z}))^n \\mathbf{y}^0$, it follows that $\\mathbf{y}^n \\to \\mathbf{0}$ as $n \\to \\infty$. This holds for any $\\Delta t  0$, which is the definition of unconditional stability. The presence of defective Jordan blocks (nilpotent parts) does not alter this conclusion, as the asymptotic convergence is determined solely by the spectral radius being strictly less than one. This stability holds under the same condition as A-stability, $\\alpha_f \\ge 1/2$.\n\n**5) Computation of the High-Frequency Amplification Factor $\\rho_{\\infty}$**\n\nThe final task is to compute the high-frequency amplification factor, defined as $\\rho_{\\infty} \\equiv \\lim_{|z|\\to\\infty, \\operatorname{Re}(z)0} |R(z)|$.\nFrom part (3), we already computed the limit of the function $R(z)$:\n$$\n\\lim_{|z|\\to\\infty} R(z) = \\frac{\\alpha_f - 1}{\\alpha_f}\n$$\nThe limit of the magnitude is the magnitude of the limit:\n$$\n\\rho_{\\infty} = \\left| \\lim_{|z|\\to\\infty} R(z) \\right| = \\left| \\frac{\\alpha_f - 1}{\\alpha_f} \\right|\n$$\nThe problem specifies the constraint $0  \\alpha_f \\le 1$. Within this interval:\n- The denominator $\\alpha_f$ is positive.\n- The numerator $\\alpha_f - 1$ is non-positive (it is zero for $\\alpha_f=1$ and negative for $0  \\alpha_f  1$).\nTherefore, the fraction $\\frac{\\alpha_f - 1}{\\alpha_f}$ is non-positive. Its absolute value is:\n$$\n\\rho_{\\infty} = -\\left( \\frac{\\alpha_f - 1}{\\alpha_f} \\right) = \\frac{1 - \\alpha_f}{\\alpha_f}\n$$\nThis is the final expression for the high-frequency amplification factor in terms of $\\alpha_f$.",
            "answer": "$$\n\\boxed{\\frac{1 - \\alpha_f}{\\alpha_f}}\n$$"
        },
        {
            "introduction": "Theoretical stability properties, such as energy conservation versus numerical dissipation, have profound and visible effects on simulation outcomes. This practice transitions from analysis to implementation, challenging you to code two canonical integrators: the energy-conserving implicit midpoint rule and the dissipative backward Euler method . By applying these schemes to both linear and geometrically nonlinear problems, you will directly observe how their mathematical nature translates into practical behavior and build core skills in developing robust nonlinear solvers.",
            "id": "3608605",
            "problem": "Consider elastodynamics in one spatial dimension under the assumptions of no external forces and no damping. The fundamental base is Newton's second law and the definition of mechanical energy. Let the generalized displacement be $q(t)$, velocity $v(t) = \\dot{q}(t)$, and mass $m \\gt 0$. The equation of motion is\n$$\nm \\, \\ddot{q}(t) + \\nabla V(q(t)) = 0,\n$$\nwith total mechanical energy\n$$\nE(t) = \\tfrac{1}{2} m \\, v(t)^2 + V(q(t)).\n$$\nYou will compare the discrete energy evolution $E^{n+1} - E^n$ generated by two implicit time integrators applied to two systems: a linear conservative system and a geometrically nonlinear system arising from finite strain.\n\nTime integrators to implement:\n- Backward Euler (fully implicit) applied to the first-order system in $[q, v]$:\n  - Discrete equations for step size $\\Delta t$:\n    - Position update constraint uses the advanced velocity $v^{n+1}$.\n    - Momentum balance uses the internal force evaluated at $q^{n+1}$.\n- Implicit midpoint (also known as the Gauss–Legendre two-stage method):\n  - Discrete equations for step size $\\Delta t$:\n    - Position update uses the midpoint velocity.\n    - Momentum balance uses the internal force evaluated at the midpoint configuration.\n\nBoth schemes must be solved at each step by a consistent nonlinear solver when the force is nonlinear. You must implement a Newton–Raphson method with an analytically derived Jacobian. The Jacobian must come from differentiating the discrete residuals with respect to the unknowns at the new time step. Stop Newton iterations when the Euclidean norm of the residual is less than $10^{-14}$ or after $50$ iterations, whichever occurs first. If Newton fails to converge, your code should still return a result using the last iterate to ensure the program always runs to completion. Use nondimensional units throughout; no physical units should be reported.\n\nSystems to simulate:\n- Linear elastic oscillator:\n  - Potential $V(q) = \\tfrac{1}{2} k q^2$ with stiffness $k \\gt 0$.\n  - Internal force $\\nabla V(q) = k q$.\n  - This is a linear conservative system.\n- Geometrically nonlinear finite-strain bar (compressible neo-Hookean in one dimension under homogeneous stretch):\n  - Let the reference length be $L_0 \\gt 0$, the stretch be $\\lambda = 1 + q/L_0$, and the strain energy density be\n    $$\n    \\Psi(\\lambda) = \\tfrac{1}{2} \\mu \\left(\\lambda^2 - 2 \\ln \\lambda - 1\\right) + \\tfrac{1}{2} \\kappa (\\ln \\lambda)^2,\n    $$\n    for shear modulus $\\mu \\gt 0$ and bulk modulus $\\kappa \\gt 0$.\n  - Let the cross-sectional area be $A = 1$ and define the total potential as $V(q) = A L_0 \\Psi(\\lambda)$.\n  - The internal force is $\\nabla V(q) = A \\, \\Psi'(\\lambda)$ and the tangent is $\\nabla^2 V(q) = A \\, \\Psi''(\\lambda) / L_0$, where\n    $$\n    \\Psi'(\\lambda) = \\mu \\left(\\lambda - \\lambda^{-1}\\right) + \\kappa \\, \\frac{\\ln \\lambda}{\\lambda}, \\quad\n    \\Psi''(\\lambda) = \\mu \\left(1 + \\lambda^{-2}\\right) + \\kappa \\, \\frac{1 - \\ln \\lambda}{\\lambda^2}.\n    $$\n  - Restrict to configurations with $\\lambda \\gt 0$.\n\nInitial conditions and energy accounting:\n- For each simulation, initialize with given $(q^0, v^0)$.\n- At each time step, after computing $(q^{n+1}, v^{n+1})$, compute the discrete energy difference $\\Delta E^n = E^{n+1} - E^n$.\n- To assess conservation or dissipation, evaluate statistics over the computed $\\Delta E^n$ sequence.\n\nTest suite:\nImplement the following four test cases. All parameters are nondimensional.\n\n- Test $1$ (linear, implicit midpoint):\n  - Parameters: $m = 1$, $k = 10$, $\\Delta t = 0.5$, number of steps $N = 40$, initial conditions $q^0 = 0.3$, $v^0 = -0.1$.\n  - Compute the maximum absolute stepwise energy change $\\max_n |\\Delta E^n|$.\n  - Output boolean $\\mathrm{T}_1$: True if $\\max_n |\\Delta E^n| \\le 10^{-10}$, else False.\n\n- Test $2$ (linear, backward Euler):\n  - Parameters: $m = 1$, $k = 10$, $\\Delta t = 2.0$, number of steps $N = 25$, initial conditions $q^0 = 0.3$, $v^0 = -0.1$.\n  - Compute the maximum stepwise energy increase $\\max_n \\Delta E^n$.\n  - Output boolean $\\mathrm{T}_2$: True if $\\max_n \\Delta E^n \\le 10^{-12}$, else False.\n\n- Test $3$ (geometrically nonlinear, implicit midpoint):\n  - Parameters: $m = 1$, $\\mu = 2$, $\\kappa = 100$, $L_0 = 1$, $\\Delta t = 0.1$, number of steps $N = 60$, initial conditions $q^0 = 0.2$, $v^0 = 0.0$.\n  - Compute $\\max_n |\\Delta E^n|$.\n  - Output boolean $\\mathrm{T}_3$: True if $\\max_n |\\Delta E^n| \\gt 10^{-12}$ (i.e., not exactly energy-conserving), else False.\n\n- Test $4$ (geometrically nonlinear, backward Euler):\n  - Parameters: $m = 1$, $\\mu = 2$, $\\kappa = 100$, $L_0 = 1$, $\\Delta t = 0.3$, number of steps $N = 50$, initial conditions $q^0 = 0.2$, $v^0 = 0.0$.\n  - Compute $\\max_n \\Delta E^n$.\n  - Output boolean $\\mathrm{T}_4$: True if $\\max_n \\Delta E^n \\le 10^{-12}$, else False.\n\nFinal output format:\n- Your program should produce a single line of output containing the four boolean results as a comma-separated list enclosed in square brackets in the order $[\\mathrm{T}_1,\\mathrm{T}_2,\\mathrm{T}_3,\\mathrm{T}_4]$. For example, an output could look like \"[True,True,False,True]\".\n- All computations must be performed in nondimensional units; no physical units are to be printed.",
            "solution": "We start from Newton's second law for a single degree of freedom with mass $m \\gt 0$:\n$$\nm \\, \\ddot{q}(t) + \\nabla V(q(t)) = 0.\n$$\nDefine the state vector $y(t) = \\begin{bmatrix} q(t) \\\\ v(t) \\end{bmatrix}$ with $v(t) = \\dot{q}(t)$. Then the first-order system is\n$$\n\\dot{y}(t) = f(y(t)) = \\begin{bmatrix} v(t) \\\\ -\\frac{1}{m} \\, \\nabla V(q(t)) \\end{bmatrix}.\n$$\nThe mechanical energy is\n$$\nE(t) = \\tfrac{1}{2} m \\, v(t)^2 + V(q(t)).\n$$\nWe consider two time integrators.\n\nImplicit midpoint:\nThe implicit midpoint rule for advancing from $t^n$ to $t^{n+1} = t^n + \\Delta t$ is\n$$\ny^{n+1} = y^n + \\Delta t \\, f\\Big(\\tfrac{1}{2}\\big(y^n + y^{n+1}\\big)\\Big).\n$$\nWriting components yields the coupled equations\n$$\nq^{n+1} - q^n - \\Delta t \\, \\tfrac{1}{2}\\big(v^n + v^{n+1}\\big) = 0,\n$$\n$$\nv^{n+1} - v^n + \\Delta t \\, \\frac{1}{m} \\, \\nabla V\\Big(\\tfrac{1}{2}\\big(q^n + q^{n+1}\\big)\\Big) = 0.\n$$\nFor nonlinear $\\nabla V$, we solve for $\\big(q^{n+1}, v^{n+1}\\big)$ by Newton–Raphson. Define the residual\n$$\nR(q^{n+1}, v^{n+1}) =\n\\begin{bmatrix}\nq^{n+1} - q^n - \\Delta t \\, \\tfrac{1}{2}\\big(v^n + v^{n+1}\\big) \\\\\nv^{n+1} - v^n + \\Delta t \\, \\frac{1}{m} \\, \\nabla V\\big(q^{\\mathrm{mid}}\\big)\n\\end{bmatrix}, \\quad\nq^{\\mathrm{mid}} = \\tfrac{1}{2}\\big(q^n + q^{n+1}\\big).\n$$\nThe Jacobian is\n$$\nJ = \\frac{\\partial R}{\\partial (q^{n+1}, v^{n+1})} =\n\\begin{bmatrix}\n1  -\\tfrac{1}{2}\\Delta t \\\\\n\\tfrac{1}{2}\\Delta t \\, \\tfrac{1}{m} \\, \\nabla^2 V\\big(q^{\\mathrm{mid}}\\big)  1\n\\end{bmatrix}.\n$$\nThe Newton update solves $J \\, \\delta = -R$ and sets $(q^{n+1}, v^{n+1}) \\leftarrow (q^{n+1}, v^{n+1}) + \\delta$ until convergence.\n\nBackward Euler:\nThe backward Euler method is\n$$\ny^{n+1} = y^n + \\Delta t \\, f(y^{n+1}),\n$$\nor in components,\n$$\nq^{n+1} - q^n - \\Delta t \\, v^{n+1} = 0,\n$$\n$$\nv^{n+1} - v^n + \\Delta t \\, \\frac{1}{m} \\, \\nabla V(q^{n+1}) = 0.\n$$\nDefine the residual\n$$\nR(q^{n+1}, v^{n+1}) =\n\\begin{bmatrix}\nq^{n+1} - q^n - \\Delta t \\, v^{n+1} \\\\\nv^{n+1} - v^n + \\Delta t \\, \\tfrac{1}{m} \\, \\nabla V(q^{n+1})\n\\end{bmatrix},\n$$\nwith Jacobian\n$$\nJ =\n\\begin{bmatrix}\n1  -\\Delta t \\\\\n\\Delta t \\, \\tfrac{1}{m} \\, \\nabla^2 V(q^{n+1})  1\n\\end{bmatrix}.\n$$\nAgain, we solve $J \\, \\delta = -R$ for the Newton update.\n\nEnergy properties in the linear conservative case:\nLet $V(q) = \\tfrac{1}{2} k q^2$, so $\\nabla V(q) = k q$ and $\\nabla^2 V(q) = k$. In first-order form,\n$$\n\\dot{y} = A y, \\quad A = \\begin{bmatrix} 0  1 \\\\ -\\tfrac{k}{m}  0 \\end{bmatrix}.\n$$\nThe implicit midpoint step becomes\n$$\n\\big(I - \\tfrac{\\Delta t}{2} A\\big) y^{n+1} = \\big(I + \\tfrac{\\Delta t}{2} A\\big) y^n,\n$$\ni.e., $y^{n+1} = M y^n$ with the Cayley transform $M = \\big(I - \\tfrac{\\Delta t}{2} A\\big)^{-1}\\big(I + \\tfrac{\\Delta t}{2} A\\big)$. The energy can be written as a quadratic form $E = \\tfrac{1}{2} y^\\top H y$ with\n$$\nH = \\begin{bmatrix} k  0 \\\\ 0  m \\end{bmatrix}.\n$$\nOne checks that $A$ is $H$-skew-symmetric, i.e., $A^\\top H + H A = 0$. This implies $M^\\top H M = H$, hence\n$$\nE^{n+1} = \\tfrac{1}{2} (y^{n})^\\top M^\\top H M y^n = \\tfrac{1}{2} (y^{n})^\\top H y^n = E^n,\n$$\nso the implicit midpoint method exactly conserves the quadratic energy for any $\\Delta t$. In floating-point arithmetic, deviations are at roundoff level.\n\nFor backward Euler, the step is $(I - \\Delta t A) y^{n+1} = y^n$, or $y^{n+1} = B y^n$ with $B = (I - \\Delta t A)^{-1}$. A direct calculation shows\n$$\nE^{n+1} - E^n = - \\tfrac{1}{2} \\Delta t \\, (y^{n+1} + y^n)^\\top (A^\\top H + H A) (y^{n+1} - y^n) - \\tfrac{1}{2} \\Delta t^2 \\, (y^{n+1})^\\top A^\\top H A \\, y^{n+1}.\n$$\nUsing $A^\\top H + H A = 0$ and $A^\\top H A$ positive definite for $k, m \\gt 0$, one obtains\n$$\nE^{n+1} - E^n = - \\tfrac{1}{2} \\Delta t^2 \\, (y^{n+1})^\\top A^\\top H A \\, y^{n+1} \\le 0,\n$$\nwith equality only at the trivial state. Thus backward Euler is unconditionally dissipative on the linear conservative oscillator: the energy monotonically decreases for any $\\Delta t \\gt 0$.\n\nGeometric nonlinearity:\nFor the compressible neo-Hookean bar, let $\\lambda = 1 + q/L_0$ and\n$$\n\\Psi(\\lambda) = \\tfrac{1}{2} \\mu \\left(\\lambda^2 - 2 \\ln \\lambda - 1\\right) + \\tfrac{1}{2} \\kappa (\\ln \\lambda)^2,\n$$\nso $V(q) = A L_0 \\Psi(\\lambda)$ with $A = 1$, and\n$$\n\\nabla V(q) = \\Psi'(\\lambda), \\quad \\nabla^2 V(q) = \\frac{\\Psi''(\\lambda)}{L_0},\n$$\nwith\n$$\n\\Psi'(\\lambda) = \\mu \\left(\\lambda - \\lambda^{-1}\\right) + \\kappa \\, \\frac{\\ln \\lambda}{\\lambda}, \\quad\n\\Psi''(\\lambda) = \\mu \\left(1 + \\lambda^{-2}\\right) + \\kappa \\, \\frac{1 - \\ln \\lambda}{\\lambda^2}.\n$$\nFor this nonlinear conservative system, the implicit midpoint method is symplectic and exactly preserves all quadratic invariants but not the true nonlinear energy $E$ unless $V$ is quadratic. Therefore, one expects bounded, oscillatory deviations in $E^{n+1} - E^n$ of order $\\mathcal{O}(\\Delta t^3)$ per step and a bounded $\\mathcal{O}(\\Delta t^2)$ energy error over long times, rather than exact conservation. Backward Euler remains a strongly dissipative method and typically yields $E^{n+1} - E^n \\le 0$ for these parameters, although for general nonlinear Hamiltonian systems a strict per-step monotonic decrease is not guaranteed by a general invariant, it is observed here due to the numerical viscosity introduced by the fully implicit update and the convexity of $V$ near the operating range.\n\nAlgorithmic design:\n- Implement potentials as objects providing $V(q)$, $\\nabla V(q)$, and $\\nabla^2 V(q)$.\n- Implement a Newton–Raphson solver for each time step for both implicit midpoint and backward Euler using the residuals and Jacobians above.\n- For each simulation, accumulate $E^n$ and $\\Delta E^n = E^{n+1} - E^n$.\n- For the linear midpoint test, compute $\\max_n |\\Delta E^n|$ and compare to $10^{-10}$.\n- For the linear backward Euler test, compute $\\max_n \\Delta E^n$ and assert it is $\\le 10^{-12}$.\n- For the nonlinear midpoint test, check that $\\max_n |\\Delta E^n| \\gt 10^{-12}$ to confirm that exact conservation does not persist under geometric nonlinearity.\n- For the nonlinear backward Euler test, compute $\\max_n \\Delta E^n$ and assert it is $\\le 10^{-12}$.\n\nTest suite parameters:\n- Test $1$: $m = 1$, $k = 10$, $\\Delta t = 0.5$, $N = 40$, $q^0 = 0.3$, $v^0 = -0.1$.\n- Test $2$: $m = 1$, $k = 10$, $\\Delta t = 2.0$, $N = 25$, $q^0 = 0.3$, $v^0 = -0.1$.\n- Test $3$: $m = 1$, $\\mu = 2$, $\\kappa = 100$, $L_0 = 1$, $\\Delta t = 0.1$, $N = 60$, $q^0 = 0.2$, $v^0 = 0.0$.\n- Test $4$: $m = 1$, $\\mu = 2$, $\\kappa = 100$, $L_0 = 1$, $\\Delta t = 0.3$, $N = 50$, $q^0 = 0.2$, $v^0 = 0.0$.\n\nFinal output:\n- Produce a single line with $[\\mathrm{T}_1,\\mathrm{T}_2,\\mathrm{T}_3,\\mathrm{T}_4]$ as booleans.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Potentials with energy, gradient, and Hessian\nclass LinearPotential:\n    def __init__(self, k: float):\n        self.k = float(k)\n\n    def energy(self, q: float) - float:\n        return 0.5 * self.k * q * q\n\n    def grad(self, q: float) - float:\n        return self.k * q\n\n    def hess(self, q: float) - float:\n        return self.k\n\n\nclass NeoHookean1D:\n    def __init__(self, mu: float, kappa: float, L0: float):\n        self.mu = float(mu)\n        self.kappa = float(kappa)\n        self.L0 = float(L0)\n\n    def _lambda(self, q: float) - float:\n        return 1.0 + q / self.L0\n\n    def energy(self, q: float) - float:\n        lam = self._lambda(q)\n        if lam = 0.0:\n            # Penalize non-physical states; return large energy to discourage\n            return np.inf\n        # A = 1, total V = A * L0 * Psi(lam) = L0 * Psi(lam)\n        psi = 0.5 * self.mu * (lam * lam - 2.0 * np.log(lam) - 1.0) + 0.5 * self.kappa * (np.log(lam) ** 2)\n        return self.L0 * psi\n\n    def grad(self, q: float) - float:\n        lam = self._lambda(q)\n        if lam = 0.0:\n            # Large force driving back to admissible region\n            return np.sign(q) * np.inf\n        # dV/dq = A * Psi'(lam) = Psi'(lam) since A=1\n        psi_p = self.mu * (lam - 1.0 / lam) + self.kappa * (np.log(lam) / lam)\n        return psi_p\n\n    def hess(self, q: float) - float:\n        lam = self._lambda(q)\n        if lam = 0.0:\n            return np.inf\n        # d2V/dq2 = A * Psi''(lam) / L0 = Psi''(lam) / L0\n        psi_pp = self.mu * (1.0 + 1.0 / (lam * lam)) + self.kappa * ((1.0 - np.log(lam)) / (lam * lam))\n        return psi_pp / self.L0\n\n\ndef newton_solve_midpoint(qn, vn, dt, m, potential, tol=1e-14, maxit=50):\n    # Unknowns: q1, v1\n    q1 = qn\n    v1 = vn\n    for _ in range(maxit):\n        qmid = 0.5 * (qn + q1)\n        vmid = 0.5 * (vn + v1)\n        # Residual\n        r1 = q1 - qn - dt * vmid\n        gmid = potential.grad(qmid)\n        hmid = potential.hess(qmid)\n        r2 = v1 - vn + dt * (gmid / m)\n        # Check convergence\n        resn = np.sqrt(r1 * r1 + r2 * r2)\n        if not np.isfinite(resn):\n            break\n        if resn  tol:\n            break\n        # Jacobian\n        # dr1/dq1 = 1, dr1/dv1 = -0.5*dt\n        # dr2/dq1 = dt * (0.5 * hmid / m), dr2/dv1 = 1\n        a11 = 1.0\n        a12 = -0.5 * dt\n        a21 = dt * (0.5 * hmid / m)\n        a22 = 1.0\n        # Solve 2x2 linear system J * dx = -r\n        det = a11 * a22 - a12 * a21\n        if det == 0 or not np.isfinite(det):\n            break\n        inv11 = a22 / det\n        inv12 = -a12 / det\n        inv21 = -a21 / det\n        inv22 = a11 / det\n        dx1 = -(inv11 * r1 + inv12 * r2)\n        dx2 = -(inv21 * r1 + inv22 * r2)\n        q1 += dx1\n        v1 += dx2\n        # safeguard: stay in admissible region for neo-Hookean\n        # if lambda = 0, backtrack a bit\n        if isinstance(potential, NeoHookean1D):\n            lam = potential._lambda(q1)\n            if lam = 0.0 or not np.isfinite(lam):\n                # simple backtracking\n                q1 = 0.5 * (q1 + qn)\n    return q1, v1\n\n\ndef newton_solve_backward_euler(qn, vn, dt, m, potential, tol=1e-14, maxit=50):\n    q1 = qn\n    v1 = vn\n    for _ in range(maxit):\n        g1 = potential.grad(q1)\n        h1 = potential.hess(q1)\n        r1 = q1 - qn - dt * v1\n        r2 = v1 - vn + dt * (g1 / m)\n        resn = np.sqrt(r1 * r1 + r2 * r2)\n        if not np.isfinite(resn):\n            break\n        if resn  tol:\n            break\n        # Jacobian\n        # dr1/dq1 = 1, dr1/dv1 = -dt\n        # dr2/dq1 = dt * (h1 / m), dr2/dv1 = 1\n        a11 = 1.0\n        a12 = -dt\n        a21 = dt * (h1 / m)\n        a22 = 1.0\n        det = a11 * a22 - a12 * a21\n        if det == 0 or not np.isfinite(det):\n            break\n        inv11 = a22 / det\n        inv12 = -a12 / det\n        inv21 = -a21 / det\n        inv22 = a11 / det\n        dx1 = -(inv11 * r1 + inv12 * r2)\n        dx2 = -(inv21 * r1 + inv22 * r2)\n        q1 += dx1\n        v1 += dx2\n        if isinstance(potential, NeoHookean1D):\n            lam = potential._lambda(q1)\n            if lam = 0.0 or not np.isfinite(lam):\n                q1 = 0.5 * (q1 + qn)\n    return q1, v1\n\n\ndef simulate(method, potential, m, dt, N, q0, v0):\n    q = q0\n    v = v0\n    E_hist = []\n    dE_hist = []\n    # initial energy\n    E = 0.5 * m * v * v + potential.energy(q)\n    E_hist.append(E)\n    for _ in range(N):\n        if method == \"midpoint\":\n            q_new, v_new = newton_solve_midpoint(q, v, dt, m, potential)\n        elif method == \"backward_euler\":\n            q_new, v_new = newton_solve_backward_euler(q, v, dt, m, potential)\n        else:\n            raise ValueError(\"Unknown method\")\n        E_new = 0.5 * m * v_new * v_new + potential.energy(q_new)\n        dE = E_new - E\n        dE_hist.append(dE)\n        q, v, E = q_new, v_new, E_new\n        E_hist.append(E)\n    return np.array(E_hist), np.array(dE_hist)\n\n\ndef solve():\n    results = []\n\n    # Tolerances\n    tol_conserve = 1e-10\n    tol_dissip = 1e-12\n    tol_exact_conserve = 1e-12\n\n    # Test 1: Linear, implicit midpoint\n    m = 1.0\n    k = 10.0\n    dt = 0.5\n    N = 40\n    q0 = 0.3\n    v0 = -0.1\n    pot_lin = LinearPotential(k)\n    _, dE = simulate(\"midpoint\", pot_lin, m, dt, N, q0, v0)\n    T1 = np.max(np.abs(dE)) = tol_conserve\n    results.append(bool(T1))\n\n    # Test 2: Linear, backward Euler\n    dt2 = 2.0\n    N2 = 25\n    _, dE2 = simulate(\"backward_euler\", pot_lin, m, dt2, N2, q0, v0)\n    T2 = np.max(dE2) = tol_dissip\n    results.append(bool(T2))\n\n    # Test 3: Nonlinear, implicit midpoint\n    mu = 2.0\n    kappa = 100.0\n    L0 = 1.0\n    pot_nl = NeoHookean1D(mu, kappa, L0)\n    dt3 = 0.1\n    N3 = 60\n    q0_nl = 0.2\n    v0_nl = 0.0\n    _, dE3 = simulate(\"midpoint\", pot_nl, m, dt3, N3, q0_nl, v0_nl)\n    T3 = np.max(np.abs(dE3))  tol_exact_conserve\n    results.append(bool(T3))\n\n    # Test 4: Nonlinear, backward Euler\n    dt4 = 0.3\n    N4 = 50\n    _, dE4 = simulate(\"backward_euler\", pot_nl, m, dt4, N4, q0_nl, v0_nl)\n    T4 = np.max(dE4) = tol_dissip\n    results.append(bool(T4))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Stability analysis is full of important subtleties, and one of the most critical is that A-stability does not automatically guarantee monotonic decay in every norm. This exercise reveals this non-intuitive fact through a carefully constructed counterexample involving the A-stable trapezoidal rule . By calculating the one-step amplification of a specific energy-like functional, you will demonstrate that transient growth is possible, providing a deeper appreciation for what stability conditions do—and do not—guarantee in a dynamic simulation.",
            "id": "3608634",
            "problem": "A single-degree-of-freedom linear elastic oscillator with viscous damping, representative of a small-strain linear solid, satisfies Newton’s second law\n$$\nm\\,\\ddot{x}(t) + c\\,\\dot{x}(t) + k\\,x(t) = 0,\n$$\nwith displacement $x(t)$, velocity $v(t) = \\dot{x}(t)$, mass $m0$, damping $c \\ge 0$, and stiffness $k0$. Introduce the first-order state $y(t) = \\begin{pmatrix} x(t) \\\\ v(t) \\end{pmatrix}$, so that\n$$\n\\dot{y}(t) = A\\,y(t), \\quad A = \\begin{pmatrix} 0  1 \\\\ -\\frac{k}{m}  -\\frac{c}{m} \\end{pmatrix}.\n$$\nConsider the trapezoidal rule (also known as the Crank–Nicolson method or the implicit midpoint rule for linear systems) with time step $\\Delta t  0$,\n$$\ny^{n+1} = y^{n} + \\frac{\\Delta t}{2}\\left(A\\,y^{n} + A\\,y^{n+1}\\right),\n$$\nwhich defines the one-step amplification matrix $G(\\Delta t)$ via\n$$\ny^{n+1} = G(\\Delta t)\\,y^{n}, \\quad G(\\Delta t) = \\left(I - \\frac{\\Delta t}{2}A\\right)^{-1}\\left(I + \\frac{\\Delta t}{2}A\\right).\n$$\nIt is a well-known result from the definition of $A$-stability (absolute stability) that the trapezoidal rule is $A$-stable, meaning its scalar stability function maps the open left-half complex plane into the closed unit disk.\n\nDespite $A$-stability, monotone decay in a given norm (or “energy-like” quadratic functional) is not guaranteed for discrete trajectories. To exhibit this, consider the oscillator with parameters $m = 1$, $c = 10$, $k = 100$, and the time step $\\Delta t = 0.3$. Define the quadratic energy-like functional\n$$\nE_{\\mathrm{euc}}(y) = \\frac{1}{2}\\,y^{\\top} y,\n$$\nwhich corresponds to the Euclidean norm squared.\n\nStarting only from the governing equation, the state-space form, and the trapezoidal rule definition, derive $G(\\Delta t)$ for the given parameters, and then compute the maximal one-step amplification factor of $E_{\\mathrm{euc}}$ under the trapezoidal update, namely\n$$\n\\alpha_{\\max} = \\max_{y \\neq 0}\\frac{E_{\\mathrm{euc}}(y^{n+1})}{E_{\\mathrm{euc}}(y^{n})}.\n$$\nYour derivation must proceed from these fundamentals and must not assume any pre-tabulated energy estimates. Express the final answer as a dimensionless number. Round your answer to four significant figures.",
            "solution": "The problem asks for the maximal one-step amplification factor, $\\alpha_{\\max}$, of the quadratic functional $E_{\\mathrm{euc}}(y) = \\frac{1}{2}y^{\\top}y$ for a linear oscillator discretized by the trapezoidal rule. The parameters are given as mass $m = 1$, damping $c = 10$, stiffness $k = 100$, and time step $\\Delta t = 0.3$.\n\nThe one-step update is given by $y^{n+1} = G(\\Delta t)y^{n}$. The ratio of the energy-like functional between consecutive steps is\n$$\n\\frac{E_{\\mathrm{euc}}(y^{n+1})}{E_{\\mathrm{euc}}(y^{n})} = \\frac{\\frac{1}{2}(y^{n+1})^{\\top}y^{n+1}}{\\frac{1}{2}(y^{n})^{\\top}y^{n}} = \\frac{(G y^{n})^{\\top}(G y^{n})}{(y^{n})^{\\top}y^{n}} = \\frac{(y^{n})^{\\top}G^{\\top}G y^{n}}{(y^{n})^{\\top}y^{n}}.\n$$\nThe quantity to be computed, $\\alpha_{\\max}$, is the maximum value of this ratio over all non-zero state vectors $y^{n}$. This maximum corresponds to the maximum of the Rayleigh quotient for the matrix $G^{\\top}G$. The maximum value of the Rayleigh quotient is the largest eigenvalue of the matrix. Therefore,\n$$\n\\alpha_{\\max} = \\lambda_{\\max}(G^{\\top}G).\n$$\nThe eigenvalues $\\lambda$ of a $2 \\times 2$ matrix $H = G^{\\top}G$ are the roots of its characteristic equation:\n$$\n\\lambda^2 - \\mathrm{tr}(H)\\lambda + \\det(H) = 0.\n$$\nOur strategy is to first compute the amplification matrix $G$ and then find the determinant and trace of $H=G^{\\top}G$ to solve for its eigenvalues.\n\nFirst, we construct the state matrix $A$ with the given parameters $m=1$, $c=10$, and $k=100$:\n$$\nA = \\begin{pmatrix} 0  1 \\\\ -\\frac{k}{m}  -\\frac{c}{m} \\end{pmatrix} = \\begin{pmatrix} 0  1 \\\\ -100  -10 \\end{pmatrix}.\n$$\nThe time step is $\\Delta t = 0.3$, so $\\frac{\\Delta t}{2} = 0.15$. The amplification matrix $G$ is given by\n$$\nG = \\left(I - \\frac{\\Delta t}{2}A\\right)^{-1}\\left(I + \\frac{\\Delta t}{2}A\\right).\n$$\nLet's define $B_{-} = I - \\frac{\\Delta t}{2}A$ and $B_{+} = I + \\frac{\\Delta t}{2}A$.\n$$\n\\frac{\\Delta t}{2}A = 0.15 \\begin{pmatrix} 0  1 \\\\ -100  -10 \\end{pmatrix} = \\begin{pmatrix} 0  0.15 \\\\ -15  -1.5 \\end{pmatrix}.\n$$\nSo,\n$$\nB_{+} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 0  0.15 \\\\ -15  -1.5 \\end{pmatrix} = \\begin{pmatrix} 1  0.15 \\\\ -15  -0.5 \\end{pmatrix}.\n$$\n$$\nB_{-} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\begin{pmatrix} 0  0.15 \\\\ -15  -1.5 \\end{pmatrix} = \\begin{pmatrix} 1  -0.15 \\\\ 15  2.5 \\end{pmatrix}.\n$$\nTo find the determinant of $H = G^{\\top}G$, we use the property $\\det(H) = \\det(G^{\\top})\\det(G) = (\\det(G))^2$.\n$$\n\\det(G) = \\det(B_{-}^{-1}B_{+}) = \\frac{\\det(B_{+})}{\\det(B_{-})}.\n$$\nThe determinants of $B_{+}$ and $B_{-}$ are:\n$$\n\\det(B_{+}) = (1)(-0.5) - (0.15)(-15) = -0.5 + 2.25 = 1.75 = \\frac{7}{4}.\n$$\n$$\n\\det(B_{-}) = (1)(2.5) - (-0.15)(15) = 2.5 + 2.25 = 4.75 = \\frac{19}{4}.\n$$\nThus, the determinant of $G$ is:\n$$\n\\det(G) = \\frac{7/4}{19/4} = \\frac{7}{19}.\n$$\nAnd the determinant of $H = G^{\\top}G$ is:\n$$\n\\det(H) = \\left(\\frac{7}{19}\\right)^2 = \\frac{49}{361}.\n$$\nNext, we compute the trace of $H$. The trace of $G^{\\top}G$ is equal to the square of the Frobenius norm of $G$, i.e., $\\mathrm{tr}(G^{\\top}G) = \\|G\\|_{\\mathrm{F}}^2 = \\sum_{i,j} G_{ij}^2$. To find this, we must first compute the matrix $G$.\nWe need the inverse of $B_{-}$:\n$$\nB_{-}^{-1} = \\frac{1}{\\det(B_{-})} \\begin{pmatrix} 2.5  0.15 \\\\ -15  1 \\end{pmatrix} = \\frac{1}{4.75} \\begin{pmatrix} 2.5  0.15 \\\\ -15  1 \\end{pmatrix} = \\frac{4}{19} \\begin{pmatrix} 2.5  0.15 \\\\ -15  1 \\end{pmatrix}.\n$$\nNow we compute $G = B_{-}^{-1}B_{+}$:\n$$\nG = \\frac{4}{19} \\begin{pmatrix} 2.5  0.15 \\\\ -15  1 \\end{pmatrix} \\begin{pmatrix} 1  0.15 \\\\ -15  -0.5 \\end{pmatrix} = \\frac{4}{19} \\begin{pmatrix} (2.5)(1)+(0.15)(-15)  (2.5)(0.15)+(0.15)(-0.5) \\\\ (-15)(1)+(1)(-15)  (-15)(0.15)+(1)(-0.5) \\end{pmatrix}\n$$\n$$\nG = \\frac{4}{19} \\begin{pmatrix} 2.5 - 2.25  0.375 - 0.075 \\\\ -15 - 15  -2.25 - 0.5 \\end{pmatrix} = \\frac{4}{19} \\begin{pmatrix} 0.25  0.3 \\\\ -30  -2.75 \\end{pmatrix}.\n$$\nTo facilitate exact arithmetic, we convert the decimals to fractions: $0.25=\\frac{1}{4}$, $0.3=\\frac{3}{10}$, $-2.75=-\\frac{11}{4}$.\n$$\nG = \\frac{4}{19} \\begin{pmatrix} \\frac{1}{4}  \\frac{3}{10} \\\\ -30  -\\frac{11}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{19}\\frac{1}{4}  \\frac{4}{19}\\frac{3}{10} \\\\ \\frac{4}{19}(-30)  \\frac{4}{19}(-\\frac{11}{4}) \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{19}  \\frac{6}{95} \\\\ -\\frac{120}{19}  -\\frac{11}{19} \\end{pmatrix}.\n$$\nNow we compute $\\mathrm{tr}(H) = \\|G\\|_{\\mathrm{F}}^2$:\n$$\n\\mathrm{tr}(H) = \\left(\\frac{1}{19}\\right)^2 + \\left(\\frac{6}{95}\\right)^2 + \\left(-\\frac{120}{19}\\right)^2 + \\left(-\\frac{11}{19}\\right)^2.\n$$\nSince $95 = 5 \\times 19$, we have $95^2 = 25 \\times 19^2 = 25 \\times 361 = 9025$.\n$$\n\\mathrm{tr}(H) = \\frac{1}{361} + \\frac{36}{9025} + \\frac{14400}{361} + \\frac{121}{361} = \\frac{1+14400+121}{361} + \\frac{36}{9025}\n$$\n$$\n\\mathrm{tr}(H) = \\frac{14522}{361} + \\frac{36}{9025} = \\frac{14522 \\times 25}{361 \\times 25} + \\frac{36}{9025} = \\frac{363050}{9025} + \\frac{36}{9025} = \\frac{363086}{9025}.\n$$\nThe eigenvalues $\\lambda$ of $H$ satisfy $\\lambda^2 - \\mathrm{tr}(H)\\lambda + \\det(H) = 0$:\n$$\n\\lambda^2 - \\frac{363086}{9025}\\lambda + \\frac{49}{361} = 0.\n$$\nUsing the quadratic formula, $\\lambda = \\frac{-\\beta \\pm \\sqrt{\\beta^2 - 4\\gamma}}{2}$, with $\\beta = -\\frac{363086}{9025}$ and $\\gamma = \\frac{49}{361}$:\n$$\n\\lambda = \\frac{1}{2} \\left(\\frac{363086}{9025} \\pm \\sqrt{\\left(\\frac{363086}{9025}\\right)^2 - 4\\left(\\frac{49}{361}\\right)}\\right).\n$$\nNumerically evaluating the terms:\n$\\mathrm{tr}(H) = \\frac{363086}{9025} \\approx 40.23113573$\n$\\det(H) = \\frac{49}{361} \\approx 0.13573407$\n$$\n\\lambda = \\frac{1}{2} \\left(40.23113573 \\pm \\sqrt{(40.23113573)^2 - 4(0.13573407)}\\right)\n$$\n$$\n\\lambda = \\frac{1}{2} \\left(40.23113573 \\pm \\sqrt{1618.544257 - 0.542936}\\right)\n$$\n$$\n\\lambda = \\frac{1}{2} \\left(40.23113573 \\pm \\sqrt{1618.001321}\\right)\n$$\n$$\n\\lambda = \\frac{1}{2} \\left(40.23113573 \\pm 40.22438708\\right).\n$$\nThe two eigenvalues are:\n$$\n\\lambda_1 = \\frac{1}{2} (40.23113573 + 40.22438708) = \\frac{80.45552281}{2} = 40.227761405.\n$$\n$$\n\\lambda_2 = \\frac{1}{2} (40.23113573 - 40.22438708) = \\frac{0.00674865}{2} = 0.003374325.\n$$\nThe maximal one-step amplification factor is the larger eigenvalue, $\\alpha_{\\max} = \\lambda_{\\max}(G^{\\top}G) = \\lambda_1$.\n$$\n\\alpha_{\\max} \\approx 40.227761405.\n$$\nRounding to four significant figures, we get $40.23$.\nThis result, being significantly greater than $1$, demonstrates that although the trapezoidal rule is $A$-stable (the spectral radius of $G$ is $\\rho(G)=\\sqrt{7/19}  1$), it is not strictly contractive in the Euclidean norm for this choice of time step, and can transiently amplify the energy-like functional $E_{\\mathrm{euc}}$.",
            "answer": "$$\\boxed{40.23}$$"
        }
    ]
}