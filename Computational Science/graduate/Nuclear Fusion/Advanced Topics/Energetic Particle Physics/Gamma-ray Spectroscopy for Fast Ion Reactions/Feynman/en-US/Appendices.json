{
    "hands_on_practices": [
        {
            "introduction": "Any quantitative analysis of radiation begins with a simple question: what fraction of the emitted particles actually reaches the detector? This is determined by the solid angle, a fundamental concept that describes the detector's field of view. This exercise  will guide you through deriving the solid angle for a typical collimated detector from first principles, providing a crucial link between the instrument's design and its measurement capability.",
            "id": "3700975",
            "problem": "A gamma-ray spectroscopy line-of-sight diagnostic for fast ion reactions in a tokamak uses a lead collimator with a flat circular aperture to define the viewing geometry of a small plasma element. The aperture has total area $A$, its surface carries a constant unit normal $\\hat{\\boldsymbol{n}}$, and its center is located at a distance $d$ from the plasma element. The line segment from the plasma element to the aperture center defines a unit vector $\\hat{\\boldsymbol{r}}$, and the angle between $\\hat{\\boldsymbol{n}}$ and $\\hat{\\boldsymbol{r}}$ is $\\theta$. Assume that the linear dimension of the aperture is much smaller than $d$, so that variations of both the distance to and the orientation relative to the plasma element across the aperture are negligible to leading order. Ignore scattering and attenuation.\n\nStarting only from the fundamental definition of solid angle as the area it subtends on the unit sphere, and the geometric relationship between a surface element and its projected area, derive the leading-order expression for the solid angle $\\Omega$ subtended by the aperture at the plasma element in terms of $A$, $d$, and $\\theta$. State clearly the approximation you use and its regime of validity in terms of the ratio of the aperture’s characteristic size to $d$.\n\nThen evaluate the resulting expression numerically for $A = 1.00\\ \\mathrm{cm}^2$, $d = 2.00\\ \\mathrm{m}$, and $\\theta = 60^\\circ$ (angles are specified in degrees). Round your final numerical answer for $\\Omega$ to four significant figures and express it in steradians.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **System**: A gamma-ray spectroscopy line-of-sight diagnostic for fast ion reactions in a tokamak.\n- **Geometry**: A lead collimator with a flat circular aperture defines the view of a plasma element.\n- **Aperture Properties**:\n    - Total area: $A$.\n    - Surface normal: a constant unit vector $\\hat{\\boldsymbol{n}}$.\n- **Positional Properties**:\n    - Distance from plasma element to aperture center: $d$.\n    - Unit vector from plasma element to aperture center: $\\hat{\\boldsymbol{r}}$.\n    - Angle between $\\hat{\\boldsymbol{n}}$ and $\\hat{\\boldsymbol{r}}$: $\\theta$.\n- **Approximation**: The linear dimension of the aperture is much smaller than $d$. Variations of distance and orientation across the aperture are negligible to leading order.\n- **Simplifications**: Scattering and attenuation are ignored.\n- **Task 1**: Starting from the fundamental definition of solid angle, derive the leading-order expression for the solid angle $\\Omega$ subtended by the aperture at the plasma element in terms of $A$, $d$, and $\\theta$.\n- **Task 2**: State the approximation used and its regime of validity.\n- **Task 3**: Numerically evaluate the expression for $A = 1.00\\ \\mathrm{cm}^2$, $d = 2.00\\ \\mathrm{m}$, and $\\theta = 60^\\circ$.\n- **Task 4**: Round the numerical answer to four significant figures and express it in steradians.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the required criteria for validity.\n\n- **Scientifically Grounded**: The problem describes a standard setup for a collimated radiation detector, a widely used technique in experimental physics, particularly in plasma diagnostics for fusion research. The concept of solid angle and its geometric calculation are fundamental in physics and engineering. The premises are factually and scientifically sound.\n- **Well-Posed**: The problem provides all necessary variables ($A, d, \\theta$) and a clear physical approximation (aperture size $\\ll$ distance) to derive a unique, leading-order expression for the solid angle. The subsequent numerical calculation is based on specified values. The problem is unambiguous and has a single, stable solution under the given approximation.\n- **Objective**: The problem is stated in precise, technical language, free from subjectivity, ambiguity, or opinion-based claims.\n\nBased on this analysis, the problem does not exhibit any flaws such as scientific unsoundness, incompleteness, or ambiguity. The specified physical parameters are realistic for a tokamak experiment.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution Derivation\nThe solid angle $\\Omega$ subtended by a surface $S$ at a point $O$ (the plasma element) is defined as the flux of the vector field $\\vec{F} = \\frac{\\hat{r}}{r^2}$ through the surface, where $\\vec{r}$ is the position vector from $O$ to a point on the surface, $r = |\\vec{r}|$, and $\\hat{r} = \\vec{r}/r$. The differential solid angle $d\\Omega$ subtended by a differential area element $d\\vec{A}$ of the surface is given by:\n$$d\\Omega = \\frac{d A_{\\perp}}{r^2} = \\frac{\\hat{r} \\cdot d\\vec{A}}{r^2}$$\nHere, $d A_{\\perp}$ is the area of the element projected onto a plane perpendicular to $\\hat{r}$. The total solid angle is found by integrating over the entire aperture surface $S$:\n$$\\Omega = \\iint_S \\frac{\\hat{r} \\cdot d\\vec{A}}{r^2}$$\nFor the given flat aperture, the differential area vector is $d\\vec{A} = \\hat{\\boldsymbol{n}} dA$, where $\\hat{\\boldsymbol{n}}$ is the constant unit normal to the aperture plane and $dA$ is the scalar area element. The expression for the solid angle becomes:\n$$\\Omega = \\iint_S \\frac{\\hat{r} \\cdot \\hat{\\boldsymbol{n}}}{r^2} dA$$\nThe problem states that the linear dimension of the aperture is much smaller than the distance $d$. Let $L$ be a characteristic linear dimension of the aperture, such that its area $A \\sim L^2$. The condition is $L/d \\ll 1$, or equivalently, $\\sqrt{A}/d \\ll 1$. This approximation is the regime of validity for the leading-order solution.\n\nUnder this condition, for any point on the aperture surface, the distance $r$ to the plasma element and the unit vector $\\hat{r}$ can be approximated by their values at the center of the aperture.\n1. The distance $r$ is approximately constant and equal to $d$.\n2. The unit vector $\\hat{r}$ is approximately constant and equal to the unit vector $\\hat{\\boldsymbol{r}}$ that points from the plasma element to the aperture's center.\n\nThese approximations mean that the integrand $\\frac{\\hat{r} \\cdot \\hat{\\boldsymbol{n}}}{r^2}$ can be treated as a constant over the surface $S$ of the aperture. We can therefore take it outside the integral:\n$$\\Omega \\approx \\left( \\frac{\\hat{\\boldsymbol{r}} \\cdot \\hat{\\boldsymbol{n}}}{d^2} \\right) \\iint_S dA$$\nThe integral of the scalar area element $dA$ over the surface $S$ is simply the total area of the aperture, $A$:\n$$\\iint_S dA = A$$\nThe dot product of the two unit vectors $\\hat{\\boldsymbol{r}}$ and $\\hat{\\boldsymbol{n}}$ is, by definition, the cosine of the angle $\\theta$ between them:\n$$\\hat{\\boldsymbol{r}} \\cdot \\hat{\\boldsymbol{n}} = |\\hat{\\boldsymbol{r}}| |\\hat{\\boldsymbol{n}}| \\cos\\theta = (1)(1)\\cos\\theta = \\cos\\theta$$\nSubstituting these results back into the expression for $\\Omega$, we obtain the leading-order approximation for the solid angle:\n$$\\Omega \\approx \\frac{A \\cos\\theta}{d^2}$$\nThis expression is valid for physical configurations where the aperture is visible from the plasma element, which corresponds to $0 \\le \\theta < \\pi/2$. For this range, $\\cos\\theta \\ge 0$, and the solid angle is non-negative, as it must be.\n\n### Numerical Evaluation\nThe problem provides the following values:\n- $A = 1.00\\ \\mathrm{cm}^2$\n- $d = 2.00\\ \\mathrm{m}$\n- $\\theta = 60^\\circ$\n\nFor consistency, we must convert all quantities to SI units. The area $A$ is converted from square centimeters to square meters:\n$$A = 1.00\\ \\mathrm{cm}^2 = 1.00 \\times (10^{-2}\\ \\mathrm{m})^2 = 1.00 \\times 10^{-4}\\ \\mathrm{m}^2$$\nThe distance $d$ is already in SI units. The angle $\\theta$ is used in the cosine function:\n$$\\cos(60^\\circ) = \\frac{1}{2} = 0.5$$\nNow, we substitute these values into the derived expression for $\\Omega$:\n$$\\Omega = \\frac{(1.00 \\times 10^{-4}\\ \\mathrm{m}^2) \\times \\cos(60^\\circ)}{(2.00\\ \\mathrm{m})^2}$$\n$$\\Omega = \\frac{(1.00 \\times 10^{-4}) \\times 0.5}{4.00}\\ \\mathrm{sr}$$\n$$\\Omega = \\frac{0.500 \\times 10^{-4}}{4.00}\\ \\mathrm{sr}$$\n$$\\Omega = 0.125 \\times 10^{-4}\\ \\mathrm{sr}$$\nExpressing this in standard scientific notation gives:\n$$\\Omega = 1.25 \\times 10^{-5}\\ \\mathrm{sr}$$\nThe problem requires the final numerical answer to be rounded to four significant figures.\n$$\\Omega = 1.250 \\times 10^{-5}\\ \\mathrm{sr}$$\n\nWe can check the validity of the approximation for these values. The characteristic size of the aperture is $L \\approx \\sqrt{A} = \\sqrt{1.00\\ \\mathrm{cm}^2} = 1.00\\ \\mathrm{cm} = 0.01\\ \\mathrm{m}$. The ratio $\\sqrt{A}/d$ is:\n$$\\frac{\\sqrt{A}}{d} = \\frac{0.01\\ \\mathrm{m}}{2.00\\ \\mathrm{m}} = 0.005$$\nSince $0.005 \\ll 1$, the approximation is highly justified.",
            "answer": "$$\\boxed{1.250 \\times 10^{-5}}$$"
        },
        {
            "introduction": "In the high-flux environment of a fusion plasma, detectors can be overwhelmed, leading to instrumental artifacts like pulse pile-up, where two separate gamma-rays are mistakenly recorded as a single, higher-energy event. This practice  delves into the statistical nature of this phenomenon, challenging you to model the probability of such false events and to devise a statistical test to confirm a true signal against the background. Mastering this is essential for ensuring the fidelity of high-rate spectroscopic measurements.",
            "id": "3700977",
            "problem": "A high-efficiency gamma-ray spectrometer is deployed on a magnetic confinement fusion device to study fast-ion reactions that produce a narrow nuclear gamma-ray line at energy $E_{\\gamma}$. The detector operates in a high-rate environment where a strong continuum arises from bremsstrahlung and inelastic nuclear processes. The data acquisition electronics have a semi-Gaussian shaping with characteristic shaping time $\\tau$ and, when two pulses arrive within an interval shorter than $\\tau$, the electronics record a single pulse whose measured energy is the sum of the two pulse energies. Assume the following:\n\n- Photon arrivals follow a stationary Poisson process with rate $\\lambda$ (events per unit time).\n- The single-photon true energy distribution of the continuum is described by a normalized probability density function $f(E)$ on $E \\in [0,\\infty)$, so that $\\int_{0}^{\\infty} f(E)\\, dE = 1$.\n- The detector’s energy response for a true deposited energy $X$ is Gaussian with standard deviation $\\sigma$, i.e., the measured energy $M$ conditioned on $X$ has density $\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{(M-X)^{2}}{2\\sigma^{2}}\\right)$.\n- The counting window for identifying the narrow line is the interval $[E_{\\gamma}-w,\\, E_{\\gamma}+w]$, where $w>0$ is small compared to $E_{\\gamma}$ but not necessarily small compared to $\\sigma$.\n- The environment is “high-rate” in the sense that pile-up must be accounted for, but triple and higher-order pile-up can be neglected; formally assume $\\lambda \\tau \\ll 1$ so that two-fold pile-up dominates.\n\nPart 1. Starting from the definitions of a Poisson process and the Gaussian response model, derive a closed-form analytic expression for the probability $P_{\\text{false}}$ that a recorded event falls within the window $[E_{\\gamma}-w,\\, E_{\\gamma}+w]$ solely due to two-fold pile-up of continuum photons. Your expression must be exact under the stated assumptions and expressed in terms of $\\lambda$, $\\tau$, $\\sigma$, $w$, $E_{\\gamma}$, and $f(E)$, using standard functions as needed. Clearly define any special functions you use.\n\nPart 2. To verify the presence of the narrow line against the continuum background in the counting window, consider a hypothesis test based on the number of counts observed in that window. Model the number of counts $N$ observed in a fixed live time as Poisson with mean $\\mu = s + b$, where $b$ is the known expected mean number of background counts in the window (from continuum processes including the pile-up contribution from Part 1), and $s \\ge 0$ is the mean number of signal counts from the narrow line. Derive the asymptotic (large-sample) log-likelihood ratio test statistic $q_{\\mathrm{A}}$ for testing $H_{0}: s=0$ versus $H_{1}: s>0$ evaluated on the “Asimov” dataset (i.e., the dataset equal to its expectation), expressed only in terms of $s$ and $b$. Your final expression must be a closed-form analytic expression.\n\nProvide your final answers as two expressions, the first for $P_{\\text{false}}$ and the second for $q_{\\mathrm{A}}$, combined into a single row matrix. No numerical evaluation is required.",
            "solution": "The user has provided a two-part problem concerning gamma-ray spectroscopy in a fusion environment. The problem will first be validated against the specified criteria before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Gamma-ray line energy:** $E_{\\gamma}$\n- **Detector shaping time:** $\\tau$\n- **Continuum photon arrival rate:** $\\lambda$ (Poisson process)\n- **Single-photon true energy distribution (continuum):** $f(E)$, a normalized probability density function on $E \\in [0,\\infty)$.\n- **Detector energy response:** Gaussian with standard deviation $\\sigma$. The measured energy $M$ for a true deposited energy $X$ has density $p(M|X) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-\\frac{(M-X)^2}{2\\sigma^2})$.\n- **Pile-up condition:** Two pulses arriving within an interval shorter than $\\tau$ are recorded as a single pulse with summed energy.\n- **Counting window:** $[E_{\\gamma}-w, E_{\\gamma}+w]$, with $w>0$.\n- **High-rate assumption:** $\\lambda \\tau \\ll 1$, allowing neglect of triple and higher-order pile-up.\n- **Part 2 Model:** Number of counts $N$ in a fixed live time is Poisson distributed with mean $\\mu = s+b$.\n- **Signal and Background:** $s$ is the mean number of signal counts, $s \\ge 0$. $b$ is the known expected mean number of background counts.\n- **Hypothesis Test:** Test $H_0: s=0$ versus $H_1: s>0$.\n- **Test Statistic:** Asymptotic log-likelihood ratio test statistic $q_{\\mathrm{A}}$ evaluated on the Asimov dataset.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is firmly rooted in the principles of nuclear physics, experimental particle physics, and statistical data analysis. The concepts of Poisson processes for particle arrivals, Gaussian detector response, pulse pile-up, and likelihood-ratio tests are standard and fundamental in these fields. The scenario described is a realistic challenge in experimental fusion research.\n2.  **Well-Posed:** All necessary parameters and functional forms ($\\lambda, \\tau, \\sigma, E_{\\gamma}, w, s, b, f(E)$, Gaussian response, Poisson counts) are defined. The objectives for both parts are clear and lead to unique analytical expressions under the stated assumptions.\n3.  **Objective:** The language is formal, technical, and free of any subjective or ambiguous terminology.\n4.  **No other flaws detected:** The problem does not violate any of the other criteria for invalidity (e.g., it is not incomplete, contradictory, unrealistic, ill-posed, or trivial).\n\n### Step 3: Verdict and Action\nThe problem is valid. A detailed solution will be provided for both parts.\n\n---\n\n### Part 1: Derivation of $P_{\\text{false}}$\n\nThe quantity $P_{\\text{false}}$ is the probability that a recorded event falls within the energy window $[E_{\\gamma}-w, E_{\\gamma}+w]$ due to two-fold pile-up of continuum photons. This can be expressed as the product of two probabilities:\n1.  The probability that a randomly selected recorded event is a two-fold pile-up event, $P(\\text{pile-up})$.\n2.  The probability that the measured energy of such a pile-up event falls within the specified window, $P(\\text{energy in window} | \\text{pile-up})$.\n\n$$P_{\\text{false}} = P(\\text{pile-up}) \\times P(E_{\\gamma}-w \\le M \\le E_{\\gamma}+w | \\text{pile-up})$$\n\n**1. Probability of Pile-up, $P(\\text{pile-up})$**\nFor a stationary Poisson process with rate $\\lambda$, the probability of an event occurring in a small time interval $dt$ is $\\lambda dt$. Given an event at time $t_0$, the probability of a second event occurring within the pile-up window $(t_0, t_0+\\tau]$ is given by the CDF of the exponential inter-arrival time distribution, $P(T \\le \\tau) = 1 - \\exp(-\\lambda\\tau)$.\nUnder the assumption $\\lambda\\tau \\ll 1$, we can approximate this probability by its first-order Taylor expansion: $1 - (1 - \\lambda\\tau + O((\\lambda\\tau)^2)) \\approx \\lambda\\tau$.\nThe rate of single (non-piled-up) recorded events is $\\lambda_1 = \\lambda \\exp(-\\lambda\\tau) \\approx \\lambda(1-\\lambda\\tau)$. The rate of two-fold pile-up events is $\\lambda_2 = \\lambda(1 - \\exp(-\\lambda\\tau)) \\approx \\lambda^2\\tau$. The total rate of recorded events is $\\lambda_{\\text{rec}} = \\lambda_1 + \\lambda_2 \\approx \\lambda$.\nThe probability that a recorded event is a two-fold pile-up is the ratio of the pile-up rate to the total recorded rate:\n$$P(\\text{pile-up}) = \\frac{\\lambda_2}{\\lambda_1 + \\lambda_2} \\approx \\frac{\\lambda^2\\tau}{\\lambda} = \\lambda\\tau$$\n\n**2. Energy Distribution of Pile-up Events**\nLet $E_1$ and $E_2$ be the true energies of two independent continuum photons. Both are random variables with probability density function (PDF) $f(E)$. The true energy deposited by the pile-up event is $X = E_1 + E_2$. The PDF of $X$, denoted $g(X)$, is the convolution of $f(E)$ with itself:\n$$g(X) = (f * f)(X) = \\int_{-\\infty}^{\\infty} f(E_1) f(X - E_1) \\, dE_1$$\nSince the energies are non-negative ($f(E)=0$ for $E<0$), this becomes:\n$$g(X) = \\int_{0}^{X} f(E_1) f(X - E_1) \\, dE_1$$\n\n**3. Measured Energy Distribution and Window Probability**\nThe detector response to a true energy deposition $X$ is a Gaussian distribution for the measured energy $M$ with mean $X$ and standard deviation $\\sigma$. The PDF is $p(M|X) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-\\frac{(M-X)^2}{2\\sigma^2})$.\nThe overall PDF for the measured energy $M$ of a pile-up event, $h(M)$, is obtained by marginalizing over all possible true summed energies $X$:\n$$h(M) = \\int_{0}^{\\infty} p(M|X) g(X) \\, dX = \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(M-X)^2}{2\\sigma^2}\\right) g(X) \\, dX$$\nThe probability that $M$ falls in the window $[E_{\\gamma}-w, E_{\\gamma}+w]$ is the integral of $h(M)$ over this interval:\n$$P(E_{\\gamma}-w \\le M \\le E_{\\gamma}+w | \\text{pile-up}) = \\int_{E_{\\gamma}-w}^{E_{\\gamma}+w} h(M) \\, dM$$\nSubstituting the expression for $h(M)$ and changing the order of integration (justified by Fubini's theorem as all functions are non-negative):\n$$P(\\dots) = \\int_{0}^{\\infty} g(X) \\left( \\int_{E_{\\gamma}-w}^{E_{\\gamma}+w} \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(M-X)^2}{2\\sigma^2}\\right) \\, dM \\right) \\, dX$$\nThe inner integral is the probability for a normal random variable with mean $X$ and standard deviation $\\sigma$ to fall in the window. This can be expressed using the error function, $\\text{erf}(z) = \\frac{2}{\\sqrt{\\pi}}\\int_0^z \\exp(-t^2)dt$. The CDF of a normal distribution $N(\\mu, \\sigma^2)$ is $\\Phi(x) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{x-\\mu}{\\sqrt{2}\\sigma}\\right)\\right)$. The integral evaluates to:\n$$ \\Phi(E_{\\gamma}+w) - \\Phi(E_{\\gamma}-w) = \\frac{1}{2}\\left[\\text{erf}\\left(\\frac{E_{\\gamma}+w-X}{\\sqrt{2}\\sigma}\\right) - \\text{erf}\\left(\\frac{E_{\\gamma}-w-X}{\\sqrt{2}\\sigma}\\right)\\right] $$\nSubstituting this back, we get:\n$$P(\\dots) = \\int_{0}^{\\infty} g(X) \\frac{1}{2}\\left[\\text{erf}\\left(\\frac{E_{\\gamma}+w-X}{\\sqrt{2}\\sigma}\\right) - \\text{erf}\\left(\\frac{E_{\\gamma}-w-X}{\\sqrt{2}\\sigma}\\right)\\right] \\, dX$$\n\n**4. Final Expression for $P_{\\text{false}}$**\nCombining the results, we have:\n$$P_{\\text{false}} = \\lambda\\tau \\int_{0}^{\\infty} (f*f)(X) \\frac{1}{2}\\left[\\text{erf}\\left(\\frac{E_{\\gamma}+w-X}{\\sqrt{2}\\sigma}\\right) - \\text{erf}\\left(\\fracE_{\\gamma}-w-X}{\\sqrt{2}\\sigma}\\right)\\right] \\, dX$$\nwhere $(f*f)(X) = \\int_{0}^{X} f(E)f(X-E)dE$ is the convolution operator and $\\text{erf}(z)$ is the error function. This is the required closed-form analytic expression.\n\n### Part 2: Derivation of $q_{\\mathrm{A}}$\n\nWe are given a Poisson model for the number of counts $N$ in the window, $N \\sim \\text{Poisson}(\\mu = s+b)$. We test the null hypothesis $H_0: s=0$ against the alternative $H_1: s>0$. The background mean $b$ is known.\n\nThe likelihood function is $L(s|N) = \\frac{(s+b)^N \\exp(-(s+b))}{N!}$.\nThe log-likelihood is $\\ln L(s|N) = N\\ln(s+b) - (s+b) - \\ln(N!)$.\n\nThe log-likelihood ratio test statistic for testing $H_0: s=0$ is:\n$$q_0 = -2 \\ln \\frac{L(s=0|N)}{L(\\hat{s}|N)}$$\nwhere $\\hat{s}$ is the maximum likelihood estimator (MLE) of $s$. To find $\\hat{s}$, we maximize $\\ln L(s|N)$ under the constraint $s \\ge 0$.\nThe derivative with respect to $s$ is $\\frac{\\partial \\ln L}{\\partial s} = \\frac{N}{s+b} - 1$. Setting this to zero gives $s = N-b$. Since $s$ must be non-negative, the MLE is $\\hat{s} = \\max(0, N-b)$.\n\nThe statistic $q_0$ is therefore:\n$$q_0 = \\begin{cases} -2 \\left[ \\ln L(0|N) - \\ln L(N-b|N) \\right] & \\text{if } N > b \\\\ 0 & \\text{if } N \\le b \\end{cases}$$\nFor $N > b$, we have $\\hat{s} = N-b$ and $\\hat{s}+b = N$.\n$$q_0 = -2 [ (N\\ln b - b - \\ln N!) - (N\\ln N - N - \\ln N!) ]$$\n$$q_0 = -2 [ N\\ln b - b - N\\ln N + N ] = 2 [ N\\ln(N/b) - (N-b) ]$$\n\nThe problem asks for this statistic evaluated on the \"Asimov\" dataset. For testing a hypothesis against an alternative with signal strength $s > 0$, the Asimov dataset is defined by setting the observation $N$ to its expectation value under the alternative hypothesis.\n$$N_{\\text{Asimov}} = E[N|s,b] = s+b$$\nSince we are interested in $H_1: s>0$, we have $s>0$, which implies $N_{\\text{Asimov}} = s+b > b$. We can therefore use the expression for $q_0$ with $N=s+b$.\nSubstituting $N = s+b$ into the expression for $q_0$ gives the Asimov test statistic $q_{\\mathrm{A}}$:\n$$q_{\\mathrm{A}} = 2 \\left[ (s+b)\\ln\\left(\\frac{s+b}{b}\\right) - ((s+b)-b) \\right]$$\n$$q_{\\mathrm{A}} = 2 \\left[ (s+b)\\ln\\left(1 + \\frac{s}{b}\\right) - s \\right]$$\nThis is the final closed-form analytic expression for $q_{\\mathrm{A}}$ in terms of $s$ and $b$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{\\lambda\\tau}{2} \\int_{0}^{\\infty} (f*f)(X) \\left[\\text{erf}\\left(\\frac{E_{\\gamma}+w-X}{\\sqrt{2}\\sigma}\\right) - \\text{erf}\\left(\\frac{E_{\\gamma}-w-X}{\\sqrt{2}\\sigma}\\right)\\right] \\, dX & 2 \\left[ (s+b)\\ln\\left(1 + \\frac{s}{b}\\right) - s \\right] \\end{pmatrix} } $$"
        },
        {
            "introduction": "The ultimate purpose of gamma-ray spectroscopy in fusion research is to infer the properties of the fast-ion population, such as its density and energy distribution. This final practice  simulates this end-to-end process, showing how to combine a linearized physical model, measurement data, and prior knowledge within a Bayesian statistical framework. By solving this problem, you will implement a modern inference technique to turn raw detector counts into quantitative estimates of key plasma parameters like the fast-ion density $n_f$ and their associated uncertainties.",
            "id": "3700982",
            "problem": "You are tasked with implementing a Bayesian inference program for gamma-ray spectroscopy of fast-ion reactions in a nuclear fusion plasma. Consider a set of line-of-sight gamma-ray count measurements modeled as Poisson processes with high expected counts. Each measurement channel observes counts over a known integration time, and the reaction rate is approximated by a linearized response to two latent parameters: the fast-ion density and an anisotropy parameter.\n\nFundamental starting point and modeling assumptions:\n- The counts in channel $i$, denoted $K_i$, follow a Poisson distribution with mean $\\lambda_i$, where $\\lambda_i = T_i \\,\\mu_i$, $T_i$ is the known integration time (in $\\mathrm{s}$), and $\\mu_i$ is the count rate (in $\\mathrm{s}^{-1}$).\n- For sufficiently large counts, the Poisson distribution can be approximated by a Normal distribution with the same mean and variance.\n- The linearized response model around a nominal operating point is $\\boldsymbol{\\mu} \\approx \\mathbf{H}\\,\\boldsymbol{\\theta} + \\mathbf{b}$, where $\\boldsymbol{\\theta} = [n_f,\\ a]^\\top$, $n_f$ is the fast-ion density (in $\\mathrm{m}^{-3}$), $a$ is a dimensionless anisotropy parameter, $\\mathbf{H}$ is the sensitivity matrix of partial derivatives evaluated at the nominal point (with the first column having units $\\mathrm{s}^{-1}\\,\\mathrm{m}^{3}$ and the second column having units $\\mathrm{s}^{-1}$ per unit $a$), and $\\mathbf{b}$ is the baseline count rate vector (in $\\mathrm{s}^{-1}$). The linearization offsets at the nominal point are absorbed into $\\mathbf{b}$.\n- Define the observed rates $\\mathbf{r}$ component-wise as $r_i = K_i/T_i$ (in $\\mathrm{s}^{-1}$). Under the Normal approximation and linearization, take a diagonal noise covariance with entries $\\sigma_i^2 \\approx K_i / T_i^2$, evaluated from the observed counts.\n\nBayesian prior:\n- Assume a Gaussian prior on $\\boldsymbol{\\theta}$ with mean $\\boldsymbol{\\mu}_0$ and covariance $\\boldsymbol{\\Lambda}_0$ (units consistent with $n_f$ and $a$), with zero off-diagonal entries.\n\nTask:\n- Using the assumptions above, implement a program that computes the Gaussian posterior distribution of $\\boldsymbol{\\theta}$ given the data, returning the posterior mean and the marginal equal-tailed credible intervals at credibility level $\\alpha = 0.95$ for both $n_f$ and $a$.\n- Express $n_f$ in $\\mathrm{m}^{-3}$ and $a$ as dimensionless values. No angular units are used. Output all numeric values as floating-point numbers without units. No rounding to a fixed number of digits is required.\n- Use the following test suite of three cases. In each case, there are $N$ views. Provide all parameters exactly as listed.\n\nCase A (well-conditioned, high-count regime):\n- Number of views: $N=3$.\n- Integration times $\\mathbf{T}$ (in $\\mathrm{s}$): $[\\,2.0,\\ 1.5,\\ 2.5\\,]$.\n- Observed counts $\\mathbf{K}$: $[\\,7680,\\ 4530,\\ 11350\\,]$.\n- Sensitivity matrix columns:\n  - First column $\\mathbf{S}$ (units $\\mathrm{s}^{-1}$ per $\\mathrm{m}^{-3}$): $[\\,1.1\\times 10^{-14},\\ 0.9\\times 10^{-14},\\ 1.3\\times 10^{-14}\\,]$.\n  - Second column $\\mathbf{A}$ (units $\\mathrm{s}^{-1}$ per unit $a$): $[\\,800.0,\\ -400.0,\\ 1200.0\\,]$.\n- Baseline rate vector $\\mathbf{b}$ (in $\\mathrm{s}^{-1}$): $[\\,200.0,\\ 200.0,\\ 200.0\\,]$.\n- Prior mean $\\boldsymbol{\\mu}_0 = [\\,2.5\\times 10^{17},\\ 0.0\\,]^\\top$.\n- Prior standard deviations: $\\sigma_{n_f,0} = 0.8\\times 10^{17}$ and $\\sigma_{a,0} = 0.5$; prior covariance is diagonal with these variances.\n\nCase B (short integration times, lower-count regime):\n- Number of views: $N=3$.\n- Integration times $\\mathbf{T}$ (in $\\mathrm{s}$): $[\\,0.2,\\ 0.1,\\ 0.15\\,]$.\n- Observed counts $\\mathbf{K}$: $[\\,768,\\ 302,\\ 681\\,]$.\n- Sensitivity matrix columns:\n  - First column $\\mathbf{S}$ (units $\\mathrm{s}^{-1}$ per $\\mathrm{m}^{-3}$): $[\\,1.1\\times 10^{-14},\\ 0.9\\times 10^{-14},\\ 1.3\\times 10^{-14}\\,]$.\n  - Second column $\\mathbf{A}$ (units $\\mathrm{s}^{-1}$ per unit $a$): $[\\,800.0,\\ -400.0,\\ 1200.0\\,]$.\n- Baseline rate vector $\\mathbf{b}$ (in $\\mathrm{s}^{-1}$): $[\\,200.0,\\ 200.0,\\ 200.0\\,]$.\n- Prior mean $\\boldsymbol{\\mu}_0 = [\\,2.5\\times 10^{17},\\ 0.0\\,]^\\top$.\n- Prior standard deviations: $\\sigma_{n_f,0} = 0.8\\times 10^{17}$ and $\\sigma_{a,0} = 0.5$; prior covariance is diagonal.\n\nCase C (nearly colinear sensitivities, identifiability stress test):\n- Number of views: $N=3$.\n- Integration times $\\mathbf{T}$ (in $\\mathrm{s}$): $[\\,1.0,\\ 1.2,\\ 0.8\\,]$.\n- Observed counts $\\mathbf{K}$: $[\\,6150,\\ 11700,\\ 10680\\,]$.\n- Sensitivity matrix columns:\n  - First column $\\mathbf{S}$ (units $\\mathrm{s}^{-1}$ per $\\mathrm{m}^{-3}$): $[\\,1.0\\times 10^{-14},\\ 1.6\\times 10^{-14},\\ 2.2\\times 10^{-14}\\,]$.\n  - Define a proportionality constant $\\alpha = 3.0\\times 10^{18}$ and set the second column $\\mathbf{A} = \\alpha\\,\\mathbf{S}$ (units $\\mathrm{s}^{-1}$ per unit $a$), so $\\mathbf{A} = [\\,3.0\\times 10^{4},\\ 4.8\\times 10^{4},\\ 6.6\\times 10^{4}\\,]$.\n- Baseline rate vector $\\mathbf{b}$ (in $\\mathrm{s}^{-1}$): $[\\,150.0,\\ 150.0,\\ 150.0\\,]$.\n- Prior mean $\\boldsymbol{\\mu}_0 = [\\,2.5\\times 10^{17},\\ 0.0\\,]^\\top$.\n- Prior standard deviations: $\\sigma_{n_f,0} = 0.8\\times 10^{17}$ and $\\sigma_{a,0} = 0.5$; prior covariance is diagonal.\n\nAlgorithmic requirements:\n- Construct $\\mathbf{r}$ from $\\mathbf{K}$ and $\\mathbf{T}$ as $r_i = K_i/T_i$.\n- Construct the diagonal covariance of $\\mathbf{r}$ with entries $\\sigma_i^2 = K_i/T_i^2$.\n- Form the $N\\times 2$ matrix $\\mathbf{H}$ by concatenating $\\mathbf{S}$ and $\\mathbf{A}$ as its columns, and use $\\mathbf{b}$ to shift the mean of $\\mathbf{r}$ by computing $\\mathbf{y} = \\mathbf{r} - \\mathbf{b}$.\n- Combine the Gaussian likelihood with the Gaussian prior to obtain the Gaussian posterior for $\\boldsymbol{\\theta}$ and its covariance.\n- From the marginal posterior variances, compute the symmetric equal-tailed credible intervals at credibility level $\\alpha = 0.95$ for each component.\n\nFinal output format:\n- For each case, output a list containing six floating-point numbers in the following order: $[\\ \\text{mean}(n_f),\\ \\text{lower}_{0.95}(n_f),\\ \\text{upper}_{0.95}(n_f),\\ \\text{mean}(a),\\ \\text{lower}_{0.95}(a),\\ \\text{upper}_{0.95}(a)\\ ]$.\n- Your program should produce a single line of output containing the results for Cases A, B, and C as a comma-separated list of these three lists enclosed in square brackets (e.g., $[[\\dots],[\\dots],[\\dots]]$).",
            "solution": "The problem is deemed valid after a thorough review of its components. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique solution. The core of the problem is the application of Bayesian inference to a linearized physical model with Gaussian noise and a Gaussian prior, a standard technique in data analysis.\n\nThe objective is to infer the posterior distribution of a two-dimensional parameter vector $\\boldsymbol{\\theta} = [n_f, a]^\\top$, where $n_f$ is the fast-ion density and $a$ is an anisotropy parameter. The inference is based on a set of gamma-ray count measurements.\n\nFirst, we formalize the statistical model. The problem states that the observed counts $K_i$ in channel $i$ follow a Poisson distribution, which, for large counts, can be approximated by a Normal distribution. The observed rate in channel $i$ is $r_i = K_i / T_i$, where $T_i$ is the integration time. The mean of this rate is $\\mu_i$, and its variance is $\\sigma_i^2 = \\text{Var}(K_i/T_i) = \\text{Var}(K_i)/T_i^2$. For a Poisson distribution, $\\text{Var}(K_i) = \\mathbb{E}[K_i] = \\lambda_i = T_i \\mu_i$. Using the observed counts $K_i$ as an estimate for $\\lambda_i$, the variance of the observed rate $r_i$ is approximated as $\\sigma_i^2 \\approx K_i / T_i^2$.\n\nThe set of observed rates, $\\mathbf{r} = [r_1, \\dots, r_N]^\\top$, can thus be modeled by a multivariate Normal distribution with mean vector $\\boldsymbol{\\mu}$ and a diagonal covariance matrix $\\mathbf{C}$ with diagonal entries $C_{ii} = \\sigma_i^2$.\nThe relationship between the true rates $\\boldsymbol{\\mu}$ and the parameters $\\boldsymbol{\\theta}$ is given by the linearized model:\n$$\n\\boldsymbol{\\mu} \\approx \\mathbf{H}\\boldsymbol{\\theta} + \\mathbf{b}\n$$\nwhere $\\mathbf{H}$ is the $N \\times 2$ sensitivity matrix and $\\mathbf{b}$ is a baseline rate vector.\nThe data available for inference is the vector $\\mathbf{y} = \\mathbf{r} - \\mathbf{b}$, which, according to the model, is related to $\\boldsymbol{\\theta}$ by:\n$$\n\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{H}\\boldsymbol{\\theta}, \\mathbf{C})\n$$\nThis constitutes the likelihood function for $\\boldsymbol{\\theta}$, given the data $\\mathbf{y}$. The probability density function is:\n$$\np(\\mathbf{y} | \\boldsymbol{\\theta}) \\propto \\exp\\left(-\\frac{1}{2} (\\mathbf{y} - \\mathbf{H}\\boldsymbol{\\theta})^\\top \\mathbf{C}^{-1} (\\mathbf{y} - \\mathbf{H}\\boldsymbol{\\theta})\\right)\n$$\nThe prior knowledge about $\\boldsymbol{\\theta}$ is also modeled as a Gaussian distribution:\n$$\np(\\boldsymbol{\\theta}) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\boldsymbol{\\Lambda}_0)\n$$\nwhere $\\boldsymbol{\\mu}_0$ is the prior mean and $\\boldsymbol{\\Lambda}_0$ is the prior covariance matrix. The probability density is:\n$$\np(\\boldsymbol{\\theta}) \\propto \\exp\\left(-\\frac{1}{2} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0)^\\top \\boldsymbol{\\Lambda}_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0)\\right)\n$$\nAccording to Bayes' theorem, the posterior probability distribution $p(\\boldsymbol{\\theta} | \\mathbf{y})$ is proportional to the product of the likelihood and the prior:\n$$\np(\\boldsymbol{\\theta} | \\mathbf{y}) \\propto p(\\mathbf{y} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\n$$\nSince the product of two Gaussian distributions is another Gaussian distribution, the posterior will be Gaussian: $p(\\boldsymbol{\\theta} | \\mathbf{y}) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_p, \\boldsymbol{\\Lambda}_p)$. The exponent of the posterior density is the sum of the exponents of the likelihood and the prior (ignoring constant terms):\n$$\n-\\frac{1}{2} \\left( (\\mathbf{y} - \\mathbf{H}\\boldsymbol{\\theta})^\\top \\mathbf{C}^{-1} (\\mathbf{y} - \\mathbf{H}\\boldsymbol{\\theta}) + (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0)^\\top \\boldsymbol{\\Lambda}_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0) \\right)\n$$\nBy expanding the quadratic forms and completing the square with respect to $\\boldsymbol{\\theta}$, one can derive the posterior mean $\\boldsymbol{\\mu}_p$ and posterior covariance $\\boldsymbol{\\Lambda}_p$. The standard result for this Bayesian linear model is:\n$$\n\\boldsymbol{\\Lambda}_p^{-1} = \\mathbf{H}^\\top \\mathbf{C}^{-1} \\mathbf{H} + \\boldsymbol{\\Lambda}_0^{-1}\n$$\n$$\n\\boldsymbol{\\mu}_p = \\boldsymbol{\\Lambda}_p (\\mathbf{H}^\\top \\mathbf{C}^{-1} \\mathbf{y} + \\boldsymbol{\\Lambda}_0^{-1} \\boldsymbol{\\mu}_0)\n$$\nHere, $\\boldsymbol{\\Lambda}_p^{-1}$ is the posterior precision matrix, which is the sum of the likelihood precision ($\\mathbf{H}^\\top \\mathbf{C}^{-1} \\mathbf{H}$) and the prior precision ($\\boldsymbol{\\Lambda}_0^{-1}$). The posterior covariance is found by inverting this matrix: $\\boldsymbol{\\Lambda}_p = (\\boldsymbol{\\Lambda}_p^{-1})^{-1}$.\n\nIn Case C, the columns of $\\mathbf{H}$ are collinear, making the matrix $\\mathbf{H}^\\top \\mathbf{C}^{-1} \\mathbf{H}$ singular. A purely likelihood-based inference would fail. However, the addition of the positive-definite prior precision $\\boldsymbol{\\Lambda}_0^{-1}$ regularizes the problem, ensuring that the posterior precision $\\boldsymbol{\\Lambda}_p^{-1}$ is invertible and thus the posterior distribution is well-defined.\n\nOnce the posterior mean vector $\\boldsymbol{\\mu}_p$ and covariance matrix $\\boldsymbol{\\Lambda}_p$ are computed, the marginal distribution for each parameter $\\theta_j$ (where $j \\in \\{1, 2\\}$) is a Gaussian with mean $(\\boldsymbol{\\mu}_p)_j$ and variance $(\\boldsymbol{\\Lambda}_p)_{jj}$. Let $\\mu_{p,j}$ be the posterior mean and $\\sigma_{p,j} = \\sqrt{(\\boldsymbol{\\Lambda}_p)_{jj}}$ be the posterior standard deviation for parameter $j$.\n\nThe $100(1-\\delta)\\%$ equal-tailed credible interval for $\\theta_j$ is given by:\n$$\n[\\mu_{p,j} - z_{\\delta/2} \\sigma_{p,j}, \\mu_{p,j} + z_{\\delta/2} \\sigma_{p,j}]\n$$\nwhere $z_{\\delta/2}$ is the upper $\\delta/2$ critical value of the standard normal distribution. For the specified credibility level $\\alpha=0.95$, we have $\\delta = 0.05$, so we need $z_{0.025}$. This value is $\\Phi^{-1}(1-0.025) = \\Phi^{-1}(0.975) \\approx 1.959964$.\n\nThe algorithm for each test case is as follows:\n1.  Construct the vectors $\\mathbf{T}$ and $\\mathbf{K}$.\n2.  Calculate the observed rates $\\mathbf{r}$ with elements $r_i = K_i / T_i$.\n3.  Calculate the shifted data vector $\\mathbf{y} = \\mathbf{r} - \\mathbf{b}$.\n4.  Construct the diagonal data covariance matrix $\\mathbf{C}$ with diagonal elements $C_{ii} = K_i / T_i^2$, and its inverse $\\mathbf{C}^{-1}$ with diagonal elements $T_i^2 / K_i$.\n5.  Construct the sensitivity matrix $\\mathbf{H}$ by concatenating the column vectors $\\mathbf{S}$ and $\\mathbf{A}$.\n6.  Construct the prior mean vector $\\boldsymbol{\\mu}_0$ and the diagonal prior covariance matrix $\\boldsymbol{\\Lambda}_0$ from the given prior standard deviations (variances are the squares). Compute the inverse $\\boldsymbol{\\Lambda}_0^{-1}$.\n7.  Compute the posterior precision matrix $\\boldsymbol{\\Lambda}_p^{-1} = \\mathbf{H}^\\top \\mathbf{C}^{-1} \\mathbf{H} + \\boldsymbol{\\Lambda}_0^{-1}$.\n8.  Invert it to get the posterior covariance matrix $\\boldsymbol{\\Lambda}_p = (\\boldsymbol{\\Lambda}_p^{-1})^{-1}$.\n9.  Compute the posterior mean vector $\\boldsymbol{\\mu}_p = \\boldsymbol{\\Lambda}_p (\\mathbf{H}^\\top \\mathbf{C}^{-1} \\mathbf{y} + \\boldsymbol{\\Lambda}_0^{-1} \\boldsymbol{\\mu}_0)$.\n10. Extract the posterior means for $n_f$ and $a$ from $\\boldsymbol{\\mu}_p$.\n11. Extract the posterior variances from the diagonal of $\\boldsymbol{\\Lambda}_p$ and compute the standard deviations.\n12. Calculate the lower and upper bounds for the $95\\%$ credible intervals for both parameters using the $z_{0.025}$ critical value.\n13. Assemble the six required floating-point numbers into a list for the final output.\nThis procedure is applied to each of the three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final results.\n    \"\"\"\n\n    # Define the credibility level and the corresponding z-score.\n    alpha = 0.95\n    z_score = norm.ppf(1 - (1 - alpha) / 2)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: well-conditioned, high-count regime\n        {\n            \"T\": np.array([2.0, 1.5, 2.5]),\n            \"K\": np.array([7680, 4530, 11350]),\n            \"S\": np.array([1.1e-14, 0.9e-14, 1.3e-14]),\n            \"A\": np.array([800.0, -400.0, 1200.0]),\n            \"b\": np.array([200.0, 200.0, 200.0]),\n            \"mu0\": np.array([2.5e17, 0.0]),\n            \"sigma0\": np.array([0.8e17, 0.5]),\n        },\n        # Case B: short integration times, lower-count regime\n        {\n            \"T\": np.array([0.2, 0.1, 0.15]),\n            \"K\": np.array([768, 302, 681]),\n            \"S\": np.array([1.1e-14, 0.9e-14, 1.3e-14]),\n            \"A\": np.array([800.0, -400.0, 1200.0]),\n            \"b\": np.array([200.0, 200.0, 200.0]),\n            \"mu0\": np.array([2.5e17, 0.0]),\n            \"sigma0\": np.array([0.8e17, 0.5]),\n        },\n        # Case C: nearly colinear sensitivities, identifiability stress test\n        {\n            \"T\": np.array([1.0, 1.2, 0.8]),\n            \"K\": np.array([6150, 11700, 10680]),\n            \"S\": np.array([1.0e-14, 1.6e-14, 2.2e-14]),\n            \"A\": 3.0e18 * np.array([1.0e-14, 1.6e-14, 2.2e-14]),\n            \"b\": np.array([150.0, 150.0, 150.0]),\n            \"mu0\": np.array([2.5e17, 0.0]),\n            \"sigma0\": np.array([0.8e17, 0.5]),\n        },\n    ]\n\n    results = []\n    for case_data in test_cases:\n        # Pass data to the core computation function\n        result = compute_posterior(case_data, z_score)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list of floats is what is needed.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_posterior(data, z_score):\n    \"\"\"\n    Computes the posterior mean and credible intervals for a given case.\n    \n    Args:\n        data (dict): A dictionary containing all the parameters for a single case.\n        z_score (float): The critical value for the credible interval calculation.\n        \n    Returns:\n        list: A list of 6 floats: [mean_nf, lower_nf, upper_nf, mean_a, lower_a, upper_a].\n    \"\"\"\n    T = data[\"T\"]\n    K = data[\"K\"]\n    S = data[\"S\"]\n    A = data[\"A\"]\n    b = data[\"b\"]\n    mu0 = data[\"mu0\"]\n    sigma0 = data[\"sigma0\"]\n\n    # 1. Construct observed rate vector r and shifted data vector y\n    r = K / T\n    y = r - b\n\n    # 2. Construct diagonal data covariance matrix C and its inverse C_inv\n    # C_ii = K_i / T_i^2\n    # C_inv_ii = T_i^2 / K_i\n    C_inv_diag = (T**2) / K\n    C_inv = np.diag(C_inv_diag)\n\n    # 3. Construct sensitivity matrix H\n    H = np.column_stack((S, A))\n    \n    # 4. Construct prior covariance matrix Lambda0 and its inverse Lambda0_inv\n    Lambda0 = np.diag(sigma0**2)\n    Lambda0_inv = np.diag(1 / (sigma0**2))\n    \n    # 5. Compute posterior precision matrix Lambda_p_inv\n    # Lambda_p_inv = H^T * C_inv * H + Lambda0_inv\n    Lambda_p_inv = H.T @ C_inv @ H + Lambda0_inv\n    \n    # 6. Compute posterior covariance matrix Lambda_p\n    Lambda_p = np.linalg.inv(Lambda_p_inv)\n    \n    # 7. Compute posterior mean vector mu_p\n    # mu_p = Lambda_p * (H^T * C_inv * y + Lambda0_inv * mu0)\n    term1 = H.T @ C_inv @ y\n    term2 = Lambda0_inv @ mu0\n    mu_p = Lambda_p @ (term1 + term2)\n    \n    # 8. Extract posterior means and standard deviations\n    mean_nf, mean_a = mu_p[0], mu_p[1]\n    std_nf = np.sqrt(Lambda_p[0, 0])\n    std_a = np.sqrt(Lambda_p[1, 1])\n    \n    # 9. Compute credible intervals\n    lower_nf = mean_nf - z_score * std_nf\n    upper_nf = mean_nf + z_score * std_nf\n    lower_a = mean_a - z_score * std_a\n    upper_a = mean_a + z_score * std_a\n    \n    return [mean_nf, lower_nf, upper_nf, mean_a, lower_a, upper_a]\n\nsolve()\n```"
        }
    ]
}