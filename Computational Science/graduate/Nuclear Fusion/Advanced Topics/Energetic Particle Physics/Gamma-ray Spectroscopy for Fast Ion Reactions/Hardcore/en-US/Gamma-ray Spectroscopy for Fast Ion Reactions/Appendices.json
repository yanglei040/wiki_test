{
    "hands_on_practices": [
        {
            "introduction": "Before we can interpret any measurement, we must first build a \"forward model\" that quantitatively describes how our instrument observes the plasma. A critical component of this model is the geometry of the detector's line-of-sight (LOS). This exercise asks you to develop a computational tool to calculate the path length of a collimated detector's view through a specific, geometrically-defined region of the plasma. Mastering this calculation is a fundamental step in building the response matrix, $\\mathbf{R}$, which is the cornerstone of tomographic reconstructions and quantitative analysis .",
            "id": "3700995",
            "problem": "A tokamak poloidal cross-section is modeled in the Euclidean plane by cylindrical coordinates reduced to a two-dimensional Cartesian system with radial coordinate $R$ (horizontal axis) and vertical coordinate $Z$ (vertical axis). Assume toroidal symmetry. A gamma-ray line-of-sight measurement for fast ion reactions is performed by a collimated detector whose Line Of Sight (LOS) is modeled as a half-line starting at the detector position $(R_{\\mathrm{d}}, Z_{\\mathrm{d}})$ and extending in the forward direction with angle $\\alpha$ (in radians) measured counterclockwise from the positive $R$ axis. The LOS parametrization is the mapping $t \\mapsto (R(t), Z(t))$ for $t \\ge 0$ given by\n$$\nR(t) = R_{\\mathrm{d}} + t \\cos \\alpha,\\quad Z(t) = Z_{\\mathrm{d}} + t \\sin \\alpha.\n$$\nThe plasma region of interest (ROI) for gamma-ray emission from fast ions is modeled as an annular ellipse (shell) centered at $(R_0, Z_0)$, bounded by an outer ellipse with semi-axes $(a_{\\mathrm{out}}, b_{\\mathrm{out}})$ and an inner ellipse with semi-axes $(a_{\\mathrm{in}}, b_{\\mathrm{in}})$, with $a_{\\mathrm{in}}  a_{\\mathrm{out}}$ and $b_{\\mathrm{in}}  b_{\\mathrm{out}}$. The ellipses are axis-aligned, with equations\n$$\n\\frac{(R - R_0)^2}{a^2} + \\frac{(Z - Z_0)^2}{b^2} = 1,\n$$\nwhere $(a,b)$ is $(a_{\\mathrm{out}}, b_{\\mathrm{out}})$ for the outer boundary and $(a_{\\mathrm{in}}, b_{\\mathrm{in}})$ for the inner boundary. The ROI is the set of points that are inside the outer ellipse and outside the inner ellipse.\n\nTask:\n- From first principles, construct the mapping between detector coordinates (defined by $(R_{\\mathrm{d}}, Z_{\\mathrm{d}})$ and $\\alpha$) and plasma coordinates along the LOS as a function of the path parameter $t \\ge 0$.\n- Using this mapping, determine the total intersection length of the LOS with the ROI. If the LOS passes through the ROI in multiple disjoint segments for $t \\ge 0$, sum the lengths of all such segments. If the LOS is tangent, the intersection length is zero. If the LOS misses the ROI, the intersection length is zero. Express the final length in meters, rounded to six decimals.\n\nBase assumptions you may use:\n- Euclidean geometry in the poloidal plane.\n- The definition of an ellipse as the locus satisfying $\\frac{(R - R_0)^2}{a^2} + \\frac{(Z - Z_0)^2}{b^2} = 1$.\n- The LOS parametric equation defined above.\n- The detector only measures along the forward half-line $t \\ge 0$.\n\nAngle unit requirement:\n- All angles $\\alpha$ are given in radians and must be treated in radians.\n\nOutput format requirement:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets (e.g., $[x_1,x_2,\\dots]$), where each entry is a float representing the total intersection length in meters, rounded to six decimals.\n\nTest suite:\nUse the following five test cases. In all cases, the ROI parameters are:\n- Center $(R_0, Z_0) = (\\,1.7,\\;0.0\\,)$,\n- Outer semi-axes $(a_{\\mathrm{out}}, b_{\\mathrm{out}}) = (\\,0.6,\\;1.02\\,)$,\n- Inner semi-axes $(a_{\\mathrm{in}}, b_{\\mathrm{in}}) = (\\,0.3,\\;0.51\\,)$.\n\nEach test case specifies $(R_{\\mathrm{d}}, Z_{\\mathrm{d}}, \\alpha)$:\n1. $(\\,2.5,\\;0.0,\\;\\pi\\,) = (\\,2.5,\\;0.0,\\;3.141592653589793\\,)$.\n2. $(\\,3.0,\\;1.02,\\;\\pi\\,) = (\\,3.0,\\;1.02,\\;3.141592653589793\\,)$.\n3. $(\\,3.0,\\;1.2,\\;\\pi\\,) = (\\,3.0,\\;1.2,\\;3.141592653589793\\,)$.\n4. $(\\,2.15,\\;0.0,\\;0\\,) = (\\,2.15,\\;0.0,\\;0.0\\,)$.\n5. $(\\,2.6,\\;0.4,\\;\\pi + \\pi/6\\,) = (\\,2.6,\\;0.4,\\;3.665191429188092\\,)$.\n\nYour program must compute, for each test case, the total LOS intersection length through the ROI in meters, rounded to six decimals, and print a single line $[L_1,L_2,L_3,L_4,L_5]$ with no spaces.",
            "solution": "The problem is well-posed and scientifically grounded, representing a simplified but valid geometric model for a common diagnostic technique in experimental fusion physics. We can proceed with a formal derivation of the solution from first principles.\n\nThe objective is to compute the total length of the intersection of a half-line, representing the detector's Line of Sight (LOS), with an annular elliptical region, representing the plasma's Region of Interest (ROI).\n\nThe fundamental strategy is to first determine the intersection length of the LOS with a single, solid ellipse. Since the ROI is defined as the region inside an outer ellipse and outside a concentric inner ellipse, the total intersection length with the ROI can be calculated by finding the intersection length with the solid outer ellipse ($L_{\\mathrm{out}}$) and subtracting the intersection length with the solid inner ellipse ($L_{\\mathrm{in}}$). This is valid because the ellipses are nested, meaning any segment of the LOS passing through the inner ellipse is necessarily also passing through the outer ellipse. Thus, the total length is $L = L_{\\mathrm{out}} - L_{\\mathrm{in}}$.\n\nLet's derive the intersection length for a general case of the LOS with a single solid ellipse.\n\nThe LOS is defined parametrically for $t \\ge 0$:\n$$\nR(t) = R_{\\mathrm{d}} + t \\cos \\alpha \\\\\nZ(t) = Z_{\\mathrm{d}} + t \\sin \\alpha\n$$\nwhere $(R_{\\mathrm{d}}, Z_{\\mathrm{d}})$ is the starting point of the LOS and $\\alpha$ is its angle.\n\nThe equation of a generic axis-aligned ellipse centered at $(R_0, Z_0)$ with semi-axes $(a, b)$ is:\n$$\n\\frac{(R - R_0)^2}{a^2} + \\frac{(Z - Z_0)^2}{b^2} = 1\n$$\n\nTo find the points of intersection, we substitute the parametric equations of the LOS into the ellipse equation:\n$$\n\\frac{((R_{\\mathrm{d}} + t \\cos \\alpha) - R_0)^2}{a^2} + \\frac{((Z_{\\mathrm{d}} + t \\sin \\alpha) - Z_0)^2}{b^2} = 1\n$$\n\nTo simplify, let's define the detector's starting position relative to the ellipse center:\n$R'_{\\mathrm{d}} = R_{\\mathrm{d}} - R_0$\n$Z'_{\\mathrm{d}} = Z_{\\mathrm{d}} - Z_0$\n\nAnd use shorthand for the trigonometric functions:\n$c_{\\alpha} = \\cos \\alpha$\n$s_{\\alpha} = \\sin \\alpha$\n\nThe equation becomes:\n$$\n\\frac{(R'_{\\mathrm{d}} + t c_{\\alpha})^2}{a^2} + \\frac{(Z'_{\\mathrm{d}} + t s_{\\alpha})^2}{b^2} = 1\n$$\n\nThis is a quadratic equation in the path parameter $t$. We expand and group terms by powers of $t$ to obtain the standard form $At^2 + Bt + C = 0$:\n$$\nt^2 \\left( \\frac{c_{\\alpha}^2}{a^2} + \\frac{s_{\\alpha}^2}{b^2} \\right) + t \\left( \\frac{2 R'_{\\mathrm{d}} c_{\\alpha}}{a^2} + \\frac{2 Z'_{\\mathrmd}} s_{\\alpha}}{b^2} \\right) + \\left( \\frac{(R'_{\\mathrm{d}})^2}{a^2} + \\frac{(Z'_{\\mathrm{d}})^2}{b^2} - 1 \\right) = 0\n$$\n\nThe coefficients $A$, $B$, and $C$ are:\n$$\nA = \\frac{c_{\\alpha}^2}{a^2} + \\frac{s_{\\alpha}^2}{b^2} = \\frac{b^2 c_{\\alpha}^2 + a^2 s_{\\alpha}^2}{a^2 b^2}\n$$\n$$\nB = \\frac{2 R'_{\\mathrm{d}} c_{\\alpha}}{a^2} + \\frac{2 Z'_{\\mathrm{d}} s_{\\alpha}}{b^2} = \\frac{2 (b^2 R'_{\\mathrm{d}} c_{\\alpha} + a^2 Z'_{\\mathrm{d}} s_{\\alpha})}{a^2 b^2}\n$$\n$$\nC = \\frac{(R'_{\\mathrm{d}})^2}{a^2} + \\frac{(Z'_{\\mathrm{d}})^2}{b^2} - 1 = \\frac{b^2 (R'_{\\mathrm{d}})^2 + a^2 (Z'_{\\mathrm{d}})^2 - a^2 b^2}{a^2 b^2}\n$$\n\nFor simplicity, we can work with the equation multiplied by $a^2 b^2$, which yields the same roots for $t$. The coefficients for the simpler quadratic form are:\n$$\nA' = b^2 c_{\\alpha}^2 + a^2 s_{\\alpha}^2\n$$\n$$\nB' = 2 (b^2 R'_{\\mathrm{d}} c_{\\alpha} + a^2 Z'_{\\mathrm{d}} s_{\\alpha})\n$$\n$$\nC' = b^2 (R'_{\\mathrm{d}})^2 + a^2 (Z'_{\\mathrm{d}})^2 - a^2 b^2\n$$\n\nThe solutions for $t$ are found using the quadratic formula, based on the discriminant $\\Delta = (B')^2 - 4A'C'$:\n$$\nt = \\frac{-B' \\pm \\sqrt{\\Delta}}{2A'}\n$$\n\nThe value of the discriminant determines the nature of the intersection:\n-   If $\\Delta  0$: No real roots exist. The LOS does not intersect the ellipse. The length of intersection is $0$.\n-   If $\\Delta = 0$: One real root exists. The LOS is tangent to the ellipse. The intersection length is $0$.\n-   If $\\Delta  0$: Two distinct real roots exist, $t_1$ and $t_2$. Let's order them such that $t_1  t_2$. These are the values of $t$ where the full line (for $t \\in (-\\infty, \\infty)$) crosses the ellipse boundary. The LOS is contained within the ellipse for $t \\in [t_1, t_2]$.\n\nThe total length of the intersection segment for the full line is $t_2 - t_1$. However, the problem specifies a half-line detector, which is sensitive only for $t \\ge 0$. We must therefore find the intersection of the interval $[t_1, t_2]$ with the interval $[0, \\infty)$.\n\nLet the roots be $t_1 = \\frac{-B' - \\sqrt{\\Delta}}{2A'}$ and $t_2 = \\frac{-B' + \\sqrt{\\Delta}}{2A'}$.\nThe logic to determine the intersection length for the half-line is as follows:\n1.  If $t_2  0$, both intersection points lie on the backward extension of the LOS (i.e., for $t  0$). The valid intersection length is $0$.\n2.  If $t_1  0$ and $t_2 \\ge 0$, the starting point of the LOS ($t=0$) is inside the ellipse. The LOS emerges from the ellipse at $t=t_2$. The valid intersection is over the interval $[0, t_2]$. The length is $t_2$.\n3.  If $t_1 \\ge 0$, both intersection points lie on the forward path of the LOS. The detector starts outside the ellipse, enters at $t=t_1$, and exits at $t=t_2$. The valid intersection is over the interval $[t_1, t_2]$. The length is $t_2 - t_1$.\n\nThis case analysis provides the intersection length for a single solid ellipse. To solve the problem, we apply this procedure twice:\n1.  Calculate $L_{\\mathrm{out}}$ using the parameters of the outer ellipse: $(R_0, Z_0)$, $(a_{\\mathrm{out}}, b_{\\mathrm{out}})$.\n2.  Calculate $L_{\\mathrm{in}}$ using the parameters of the inner ellipse: $(R_0, Z_0)$, $(a_{\\mathrm{in}}, b_{\\mathrm{in}})$.\n3.  The final result is the total intersection length with the annular region, $L = L_{\\mathrm{out}} - L_{\\mathrm{in}}$.\n\nThis algorithm will be implemented for each of the provided test cases. Numerical stability near $\\Delta = 0$ will be handled by treating any small positive or negative discriminant as zero, ensuring tangent lines correctly yield zero length.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_single_ellipse_length(ellipse_params, los_params):\n    \"\"\"\n    Calculates the intersection length of a half-line (LOS) with a single solid ellipse.\n\n    Args:\n        ellipse_params (tuple): (R0, Z0, a, b) for the ellipse.\n        los_params (tuple): (Rd, Zd, alpha) for the Line of Sight.\n\n    Returns:\n        float: The intersection length.\n    \"\"\"\n    R0, Z0, a, b = ellipse_params\n    Rd, Zd, alpha = los_params\n\n    cos_a = np.cos(alpha)\n    sin_a = np.sin(alpha)\n    \n    R_d_prime = Rd - R0\n    Z_d_prime = Zd - Z0\n\n    # Coefficients of the quadratic equation At^2 + Bt + C = 0\n    # The A coefficient is always positive for non-degenerate ellipses.\n    A = b**2 * cos_a**2 + a**2 * sin_a**2\n    B = 2 * (b**2 * R_d_prime * cos_a + a**2 * Z_d_prime * sin_a)\n    C = b**2 * R_d_prime**2 + a**2 * Z_d_prime**2 - a**2 * b**2\n\n    discriminant = B**2 - 4 * A * C\n\n    # No real intersection (misses) or one-point intersection (tangent).\n    # Use a small tolerance for floating-point comparisons.\n    if discriminant = 1e-12:\n        return 0.0\n\n    sqrt_discriminant = np.sqrt(discriminant)\n    \n    # Solutions for t where the full line intersects the ellipse\n    t1 = (-B - sqrt_discriminant) / (2 * A)\n    t2 = (-B + sqrt_discriminant) / (2 * A)\n\n    # We are only interested in the forward-looking half-line, t = 0.\n    # The intersection interval is [t1, t2]. We intersect this with [0, infinity).\n    \n    # Case 1: Both intersection points are \"behind\" the detector (t  0).\n    if t2  1e-12:\n        return 0.0\n    \n    # Case 2: The detector is inside the ellipse.\n    # The LOS starts at t=0 and exits at t=t2.\n    if t1  0:\n        return t2\n    \n    # Case 3: The detector is outside the ellipse.\n    # The LOS enters at t=t1 and exits at t=t2.\n    return t2 - t1\n\ndef solve():\n    \"\"\"\n    Main solver function to process test cases and print results.\n    \"\"\"\n    # ROI parameters are constant for all test cases.\n    R0, Z0 = 1.7, 0.0\n    a_out, b_out = 0.6, 1.02\n    a_in, b_in = 0.3, 0.51\n    \n    outer_ellipse = (R0, Z0, a_out, b_out)\n    inner_ellipse = (R0, Z0, a_in, b_in)\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (2.5, 0.0, np.pi),\n        (3.0, 1.02, np.pi),\n        (3.0, 1.2, np.pi),\n        (2.15, 0.0, 0.0),\n        (2.6, 0.4, np.pi + np.pi / 6.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        los_params = case\n        \n        # Calculate intersection length with the solid outer ellipse.\n        len_out = calculate_single_ellipse_length(outer_ellipse, los_params)\n        \n        # Calculate intersection length with the solid inner ellipse.\n        len_in = calculate_single_ellipse_length(inner_ellipse, los_params)\n        \n        # The length in the annular region is the difference.\n        total_length = len_out - len_in\n        results.append(total_length)\n\n    # Format the results into a single string as required.\n    # Each result is formatted to six decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With a forward model in hand, we can now address the \"inverse problem\": deducing the underlying physical source from the measured signal. This practice introduces a powerful iterative method to solve this challenge based on the principle of maximum likelihood. You will derive and implement the Maximum Likelihood Expectation-Maximization (MLEM) algorithm, a cornerstone technique for problems governed by Poisson statistics, to \"unfold\" a measured gamma-ray spectrum into the contributing reaction intensities . This exercise will give you direct experience with a robust algorithm widely used in medical imaging and astrophysics, adapted here for fusion plasma diagnostics.",
            "id": "3700970",
            "problem": "A gamma-ray spectrometer deployed on a magnetically confined nuclear fusion device measures counts per energy bin arising from fast ion reactions. Let $y_i$ denote the observed counts in bin $i$ and let $\\lambda_i$ denote the expected counts in bin $i$. Assume independent Poisson statistics across bins, with $$\\lambda_i = \\sum_{j=1}^{n} R_{ij} x_j + b_i,$$ where $R_{ij}$ is a known detector-response matrix mapping $n$ reaction-channel intensities $x_j$ to $m$ energy-bin expectations, and $b_i \\ge 0$ is a known background expectation. The objective is to compute the maximum likelihood estimate of the nonnegative reaction-channel intensities $x_j \\ge 0$ by maximizing the Poisson log-likelihood $$L(\\mathbf{x}) = \\sum_{i=1}^{m}\\left( y_i \\ln \\lambda_i - \\lambda_i \\right),$$ subject to $\\lambda_i$ as defined above.\n\nStarting from the independence of Poisson-distributed counts and the definition of the Poisson probability mass function, derive the condition that characterizes the stationary point of $L(\\mathbf{x})$ with respect to $\\mathbf{x}$ under the nonnegativity constraint. Using only fundamental principles, outline an iterative algorithm that converges to the maximum likelihood solution with $x_j \\ge 0$ for all $j$, and explain how the iteration guarantees $\\lambda_i  0$. The iteration must terminate when the relative change in the objective $L(\\mathbf{x})$ is below a prescribed tolerance or when a maximum iteration count is reached.\n\nThen, implement the derived iteration and apply it to the following test suite. In every case, $R$ is an $m \\times n$ real matrix with nonnegative entries, $b$ is an $m$-vector with nonnegative entries, and $y$ is an $m$-vector of nonnegative integers. Initialize $x_j$ with a positive value for all $j$ and enforce $x_j \\ge 0$ during iteration. Use a fixed tolerance of $\\varepsilon = 10^{-8}$ and a maximum of $N_{\\max} = 10000$ iterations. Use $\\lambda_i \\leftarrow \\max(\\lambda_i, \\delta)$ with a small $\\delta = 10^{-12}$ to guarantee strict positivity inside logarithms and divisions. No physical units are required; all quantities are treated as dimensionless expected counts.\n\nTest suite:\n1. Happy path ($m=3$, $n=2$):\n$$\nR = \\begin{bmatrix}\n0.85  0.15 \\\\\n0.10  0.90 \\\\\n0.50  0.50\n\\end{bmatrix},\\quad\nb = \\begin{bmatrix}\n3 \\\\\n2 \\\\\n1\n\\end{bmatrix},\\quad\ny = \\begin{bmatrix}\n130 \\\\\n95 \\\\\n110\n\\end{bmatrix}.\n$$\n\n2. Zero counts and sparse response ($m=4$, $n=3$):\n$$\nR = \\begin{bmatrix}\n0.90  0.00  0.10 \\\\\n0.20  0.70  0.10 \\\\\n0.00  0.60  0.40 \\\\\n0.10  0.10  0.80\n\\end{bmatrix},\\quad\nb = \\begin{bmatrix}\n1 \\\\\n1 \\\\\n0 \\\\\n0\n\\end{bmatrix},\\quad\ny = \\begin{bmatrix}\n0 \\\\\n60 \\\\\n40 \\\\\n80\n\\end{bmatrix}.\n$$\n\n3. Nearly colinear columns ($m=3$, $n=2$):\n$$\nR = \\begin{bmatrix}\n0.60  0.59 \\\\\n0.40  0.41 \\\\\n0.50  0.50\n\\end{bmatrix},\\quad\nb = \\begin{bmatrix}\n0.50 \\\\\n0.50 \\\\\n0.50\n\\end{bmatrix},\\quad\ny = \\begin{bmatrix}\n100 \\\\\n90 \\\\\n95\n\\end{bmatrix}.\n$$\n\n4. Background-dominated regime ($m=3$, $n=2$):\n$$\nR = \\begin{bmatrix}\n0.30  0.70 \\\\\n0.60  0.40 \\\\\n0.20  0.80\n\\end{bmatrix},\\quad\nb = \\begin{bmatrix}\n150 \\\\\n120 \\\\\n130\n\\end{bmatrix},\\quad\ny = \\begin{bmatrix}\n152 \\\\\n123 \\\\\n135\n\\end{bmatrix}.\n$$\n\nYour program must implement the iterative maximum likelihood algorithm you derived and produce, for each test case in the order given, the final estimated vector $\\mathbf{x}$ as a list of floating-point numbers. The required output format is a single line containing a comma-separated list of the four results, each itself a comma-separated list enclosed in square brackets. For example, if there were two test cases, the format would be $$[[\\,[x_{1,1},x_{1,2}],\\,[x_{2,1},x_{2,2}]\\,].$$ In this problem, your program should produce exactly one line of output containing the four results in the specified nested list format.",
            "solution": "The problem requires the derivation and implementation of an iterative algorithm to find the maximum likelihood estimate (MLE) of nonnegative reaction-channel intensities, $\\mathbf{x}$, given observed counts, $\\mathbf{y}$, in a gamma-ray spectrometer.\n\n### 1. Problem Formulation and Log-Likelihood\n\nThe observed counts $y_i$ in each energy bin $i$ are assumed to be independent random variables following a Poisson distribution with an expected count $\\lambda_i$. The probability mass function for a single bin is:\n$$ P(y_i|\\lambda_i) = \\frac{\\lambda_i^{y_i} e^{-\\lambda_i}}{y_i!} $$\nGiven the independence of the $m$ bins, the total likelihood of observing the count vector $\\mathbf{y} = [y_1, \\dots, y_m]$ given the expectation vector $\\mathbf{\\lambda} = [\\lambda_1, \\dots, \\lambda_m]$ is the product of the individual probabilities:\n$$ \\mathcal{L}(\\mathbf{\\lambda}) = P(\\mathbf{y}|\\mathbf{\\lambda}) = \\prod_{i=1}^{m} \\frac{\\lambda_i^{y_i} e^{-\\lambda_i}}{y_i!} $$\nThe expectation $\\lambda_i$ is modeled as a linear function of the unknown reaction-channel intensities $\\mathbf{x} = [x_1, \\dots, x_n]$, plus a known background $\\mathbf{b} = [b_1, \\dots, b_m]$:\n$$ \\lambda_i(\\mathbf{x}) = \\sum_{j=1}^{n} R_{ij} x_j + b_i $$\nHere, $R_{ij}$ are the elements of a known detector-response matrix $\\mathbf{R}$.\n\nTo find the MLE of $\\mathbf{x}$, it is more convenient to maximize the log-likelihood function, $L(\\mathbf{x}) = \\ln \\mathcal{L}(\\mathbf{\\lambda}(\\mathbf{x}))$. Taking the logarithm and dropping the term $\\sum_i \\ln(y_i!)$, which is constant with respect to $\\mathbf{x}$ and does not affect the location of the maximum, we obtain the objective function given in the problem:\n$$ L(\\mathbf{x}) = \\sum_{i=1}^{m} \\left( y_i \\ln \\lambda_i(\\mathbf{x}) - \\lambda_i(\\mathbf{x}) \\right) $$\nThe optimization problem is to maximize $L(\\mathbf{x})$ subject to the nonnegativity constraint $x_j \\ge 0$ for all $j=1, \\dots, n$.\n\n### 2. Derivation of the Stationarity Condition\n\nThis is a constrained optimization problem. The Karush-Kuhn-Tucker (KKT) conditions provide the necessary conditions for a solution to be optimal. To apply them, we first need the gradient of the log-likelihood function with respect to each component $x_k$ of $\\mathbf{x}$.\n\nThe partial derivative of $L(\\mathbf{x})$ with respect to $x_k$ is:\n$$ \\frac{\\partial L}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\sum_{i=1}^{m} \\left( y_i \\ln \\lambda_i - \\lambda_i \\right) = \\sum_{i=1}^{m} \\left( \\frac{y_i}{\\lambda_i} \\frac{\\partial \\lambda_i}{\\partial x_k} - \\frac{\\partial \\lambda_i}{\\partial x_k} \\right) $$\nFrom the definition of $\\lambda_i$, we have $\\frac{\\partial \\lambda_i}{\\partial x_k} = R_{ik}$. Substituting this into the gradient expression yields:\n$$ \\frac{\\partial L}{\\partial x_k} = \\sum_{i=1}^{m} \\left( \\frac{y_i}{\\lambda_i} - 1 \\right) R_{ik} $$\nThe KKT conditions for maximizing $L(\\mathbf{x})$ subject to $x_k \\ge 0$ require that at the optimal solution $\\mathbf{x}^*$, for each $k \\in \\{1, \\dots, n\\}$:\n1.  Primal feasibility: $x_k^* \\ge 0$.\n2.  Stationarity/Complementary Slackness: $x_k^* \\cdot \\frac{\\partial L}{\\partial x_k}\\bigg|_{\\mathbf{x}=\\mathbf{x}^*} = 0$ and $\\frac{\\partial L}{\\partial x_k}\\bigg|_{\\mathbf{x}=\\mathbf{x}^*} \\le 0$.\n\nThis implies two cases for the an optimal solution $\\mathbf{x}^*$:\n- If $x_k^*  0$, then the gradient must be zero: $\\frac{\\partial L}{\\partial x_k}\\bigg|_{\\mathbf{x}=\\mathbf{x}^*} = \\sum_{i=1}^{m} R_{ik} \\left( \\frac{y_i}{\\lambda_i^*} - 1 \\right) = 0$.\n- If $x_k^* = 0$, then the gradient must be non-positive: $\\frac{\\partial L}{\\partial x_k}\\bigg|_{\\mathbf{x}=\\mathbf{x}^*} = \\sum_{i=1}^{m} R_{ik} \\left( \\frac{y_i}{\\lambda_i^*} - 1 \\right) \\le 0$.\n\nThese two conditions characterize the stationary point of the log-likelihood function under the nonnegativity constraint.\n\n### 3. Derivation of the Iterative Algorithm\n\nThe stationarity condition for an active component ($x_k  0$) can be rewritten as:\n$$ \\sum_{i=1}^{m} R_{ik} = \\sum_{i=1}^{m} R_{ik} \\frac{y_i}{\\lambda_i} $$\nSolving for $\\mathbf{x}$ directly from this system of nonlinear equations is generally intractable. However, this form suggests a fixed-point iteration. Let's multiply both sides by $x_k$ and divide by $\\sum_{i'=1}^{m} R_{i'k}$ (assuming this sum is positive, which is true if channel $k$ contributes to at least one bin):\n$$ x_k = x_k \\frac{\\sum_{i=1}^{m} R_{ik} \\frac{y_i}{\\lambda_i}}{\\sum_{i'=1}^{m} R_{i'k}} $$\nThis equation inspires the following iterative update rule, known as the Maximum Likelihood Expectation-Maximization (MLEM) algorithm (or Richardson-Lucy algorithm in optics):\n$$ x_k^{(t+1)} = x_k^{(t)} \\frac{\\sum_{i=1}^{m} R_{ik} \\frac{y_i}{\\lambda_i^{(t)}}}{\\sum_{i'=1}^{m} R_{i'k}} $$\nwhere $t$ is the iteration index and $\\lambda_i^{(t)} = \\sum_{j=1}^{n} R_{ij} x_j^{(t)} + b_i$.\n\n### 4. Algorithm Properties\n\n- **Nonnegativity**: If we initialize with $x_k^{(0)}  0$, then since $y_i \\ge 0$, $R_{ij} \\ge 0$, and $\\lambda_i^{(t)}$ is maintained to be positive, the multiplicative factor is always non-negative. Thus, $x_k^{(t+1)} \\ge 0$ for all $t$, satisfying the constraint naturally.\n\n- **Convergence**: The MLEM algorithm is proven to monotonically non-decrease the log-likelihood function at each iteration, i.e., $L(\\mathbf{x}^{(t+1)}) \\ge L(\\mathbf{x}^{(t)})$. Since the Poisson log-likelihood function is strictly concave over the feasible set, and bounded above for a given $\\mathbf{y}$, the sequence of log-likelihood values converges. The algorithm converges to the unique global maximum of the likelihood function.\n\n- **Guaranteeing $\\lambda_i  0$**: The expected counts are $\\lambda_i = \\sum_j R_{ij} x_j + b_i$. Since all $R_{ij}$, $x_j$, and $b_i$ are non-negative, $\\lambda_i$ is always non-negative. To prevent division by zero or taking the logarithm of zero, which can occur if $\\lambda_i$ becomes computationally zero (e.g., if $b_i=0$ and all contributing $x_j$ are driven to zero), we enforce strict positivity by applying a floor: $\\lambda_i \\leftarrow \\max(\\lambda_i, \\delta)$ for a small positive constant $\\delta$. This ensures numerical stability.\n\n### 5. Summary of the Algorithm for Implementation\n\n1.  **Initialization**:\n    - Set tolerance $\\varepsilon = 10^{-8}$, max iterations $N_{\\max} = 10000$, and floor $\\delta = 10^{-12}$.\n    - Initialize the iteration counter $t=0$.\n    - Initialize intensities with a positive value, e.g., $x_j^{(0)} = 1$ for all $j=1, \\dots, n$.\n    - Pre-calculate the normalization vector $s_k = \\sum_{i=1}^{m} R_{ik}$ for each $k$.\n    - Initialize the previous log-likelihood $L_{\\text{prev}}$ to a very small number (e.g., negative infinity).\n\n2.  **Iteration Loop**: Repeat for $t=0, 1, 2, \\dots$ up to $N_{\\max}-1$:\n    a. Calculate the expected counts: $\\lambda_i^{(t)} = \\sum_{j=1}^{n} R_{ij} x_j^{(t)} + b_i$.\n    b. Apply the floor: $\\lambda_i^{\\text{safe}} = \\max(\\lambda_i^{(t)}, \\delta)$.\n    c. Calculate the current log-likelihood: $L_{\\text{current}} = \\sum_{i=1}^{m} \\left( y_i \\ln(\\lambda_i^{\\text{safe}}) - \\lambda_i^{\\text{safe}} \\right)$. Handle the $y_i=0$ case where $y_i\\ln(\\cdot)=0$.\n    d. Check for convergence: If $\\left| L_{\\text{current}} - L_{\\text{prev}} \\right| / (\\left| L_{\\text{current}} \\right| + \\varepsilon)  \\varepsilon$, terminate the loop.\n    e. Update $L_{\\text{prev}} = L_{\\text{current}}$.\n    f. Calculate the update factors: The core of the update is the ratio $y_i / \\lambda_i^{\\text{safe}}$.\n    g. Update the intensities:\n       $$ x_k^{(t+1)} = x_k^{(t)} \\cdot \\frac{1}{s_k} \\sum_{i=1}^{m} R_{ik} \\frac{y_i}{\\lambda_i^{\\text{safe}}} $$\n       This must be implemented carefully to avoid division by zero if any $s_k=0$. In such a case, $x_k$ is unobservable and should not be updated. For the given problem, all $s_k0$.\n\n3.  **Output**: Return the final estimated intensity vector $\\mathbf{x}^{(t+1)}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the MLEM algorithm on the provided test suite.\n    \"\"\"\n\n    def solve_mlem(R, b, y, tol=1e-8, max_iter=10000, delta=1e-12):\n        \"\"\"\n        Implements the Maximum Likelihood Expectation-Maximization (MLEM) algorithm.\n\n        Args:\n            R (np.ndarray): Detector-response matrix (m x n).\n            b (np.ndarray): Background expectation vector (m,).\n            y (np.ndarray): Observed counts vector (m,).\n            tol (float): Relative tolerance for log-likelihood convergence.\n            max_iter (int): Maximum number of iterations.\n            delta (float): Small positive constant to ensure lambda  0.\n\n        Returns:\n            np.ndarray: The estimated nonnegative reaction-channel intensities x (n,).\n        \"\"\"\n        m, n = R.shape\n        \n        # Initialize intensities x_j to 1.0 for all j\n        x = np.ones(n, dtype=float)\n\n        # Pre-calculate normalization factor for each channel k\n        s = R.sum(axis=0)\n        \n        # Guard against unobservable channels (s_k = 0)\n        s[s == 0] = 1.0\n\n        # Initialize log-likelihood for convergence check\n        l_prev = -np.inf\n\n        for _ in range(max_iter):\n            # Calculate expected counts (forward projection)\n            # lambda_i = sum_j(R_ij * x_j) + b_i\n            # In vector form: lambda = R @ x + b\n            lambda_exp = R @ x + b\n            \n            # Apply floor to ensure lambda is strictly positive\n            lambda_safe = np.maximum(lambda_exp, delta)\n\n            # Calculate the log-likelihood L(x) = sum(y * log(lambda) - lambda)\n            # We must handle y_i=0 case where y_i*log(lambda) is 0\n            log_lambda = np.log(lambda_safe)\n            l_current = np.sum(np.where(y > 0, y * log_lambda, 0) - lambda_safe)\n\n            # Check for convergence based on relative change in log-likelihood\n            if np.abs(l_current - l_prev) / (np.abs(l_current) + tol)  tol:\n                break\n            l_prev = l_current\n\n            # Calculate the update factor (back projection)\n            # c_k = sum_i(R_ik * y_i / lambda_i)\n            # In vector form: correction = R.T @ (y / lambda)\n            ratio = y / lambda_safe\n            correction_factor = R.T @ ratio\n            \n            # Apply multiplicative update\n            x = x * correction_factor / s\n\n        return x\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([[0.85, 0.15], [0.10, 0.90], [0.50, 0.50]]),\n            np.array([3, 2, 1]),\n            np.array([130, 95, 110])\n        ),\n        (\n            np.array([[0.90, 0.00, 0.10], [0.20, 0.70, 0.10], [0.00, 0.60, 0.40], [0.10, 0.10, 0.80]]),\n            np.array([1, 1, 0, 0]),\n            np.array([0, 60, 40, 80])\n        ),\n        (\n            np.array([[0.60, 0.59], [0.40, 0.41], [0.50, 0.50]]),\n            np.array([0.50, 0.50, 0.50]),\n            np.array([100, 90, 95])\n        ),\n        (\n            np.array([[0.30, 0.70], [0.60, 0.40], [0.20, 0.80]]),\n            np.array([150, 120, 130]),\n            np.array([152, 123, 135])\n        )\n    ]\n\n    results = []\n    for R, b, y in test_cases:\n        x_mle = solve_mlem(R, b, y)\n        results.append(x_mle)\n\n    # Format the results into the required string format\n    formatted_results = [f\"[{','.join(map(str, r))}]\" for r in results]\n    final_output = f\"[{','.join(formatted_results)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "We now explore an alternative and increasingly powerful approach to the inverse problem: Bayesian inference. Unlike methods that yield a single best-fit estimate, the Bayesian framework provides a full posterior probability distribution for the parameters of interest, naturally incorporating prior knowledge and quantifying uncertainty. In this exercise, you will implement a Bayesian analysis to infer key physical parameters, such as the fast-ion density $n_f$ and anisotropy $a$, from noisy count data . This practice demonstrates how to combine a linearized physical model with prior information to obtain not just point estimates, but also statistically rigorous credible intervals.",
            "id": "3700982",
            "problem": "You are tasked with implementing a Bayesian inference program for gamma-ray spectroscopy of fast-ion reactions in a nuclear fusion plasma. Consider a set of line-of-sight gamma-ray count measurements modeled as Poisson processes with high expected counts. Each measurement channel observes counts over a known integration time, and the reaction rate is approximated by a linearized response to two latent parameters: the fast-ion density and an anisotropy parameter.\n\nFundamental starting point and modeling assumptions:\n- The counts in channel $i$, denoted $K_i$, follow a Poisson distribution with mean $\\lambda_i$, where $\\lambda_i = T_i \\,\\mu_i$, $T_i$ is the known integration time (in $\\mathrm{s}$), and $\\mu_i$ is the count rate (in $\\mathrm{s}^{-1}$).\n- For sufficiently large counts, the Poisson distribution can be approximated by a Normal distribution with the same mean and variance.\n- The linearized response model around a nominal operating point is $\\boldsymbol{\\mu} \\approx \\mathbf{H}\\,\\boldsymbol{\\theta} + \\mathbf{b}$, where $\\boldsymbol{\\theta} = [n_f,\\ a]^\\top$, $n_f$ is the fast-ion density (in $\\mathrm{m}^{-3}$), $a$ is a dimensionless anisotropy parameter, $\\mathbf{H}$ is the sensitivity matrix of partial derivatives evaluated at the nominal point (with the first column having units $\\mathrm{s}^{-1}\\,\\mathrm{m}^{3}$ and the second column having units $\\mathrm{s}^{-1}$ per unit $a$), and $\\mathbf{b}$ is the baseline count rate vector (in $\\mathrm{s}^{-1}$). The linearization offsets at the nominal point are absorbed into $\\mathbf{b}$.\n- Define the observed rates $\\mathbf{r}$ component-wise as $r_i = K_i/T_i$ (in $\\mathrm{s}^{-1}$). Under the Normal approximation and linearization, take a diagonal noise covariance with entries $\\sigma_i^2 \\approx K_i / T_i^2$, evaluated from the observed counts.\n\nBayesian prior:\n- Assume a Gaussian prior on $\\boldsymbol{\\theta}$ with mean $\\boldsymbol{\\mu}_0$ and covariance $\\boldsymbol{\\Lambda}_0$ (units consistent with $n_f$ and $a$), with zero off-diagonal entries.\n\nTask:\n- Using the assumptions above, implement a program that computes the Gaussian posterior distribution of $\\boldsymbol{\\theta}$ given the data, returning the posterior mean and the marginal equal-tailed credible intervals at credibility level $\\alpha = 0.95$ for both $n_f$ and $a$.\n- Express $n_f$ in $\\mathrm{m}^{-3}$ and $a$ as dimensionless values. No angular units are used. Output all numeric values as floating-point numbers without units. No rounding to a fixed number of digits is required.\n- Use the following test suite of three cases. In each case, there are $N$ views. Provide all parameters exactly as listed.\n\nCase A (well-conditioned, high-count regime):\n- Number of views: $N=3$.\n- Integration times $\\mathbf{T}$ (in $\\mathrm{s}$): $[\\,2.0,\\ 1.5,\\ 2.5\\,]$.\n- Observed counts $\\mathbf{K}$: $[\\,7680,\\ 4530,\\ 11350\\,]$.\n- Sensitivity matrix columns:\n  - First column $\\mathbf{S}$ (units $\\mathrm{s}^{-1}$ per $\\mathrm{m}^{-3}$): $[\\,1.1\\times 10^{-14},\\ 0.9\\times 10^{-14},\\ 1.3\\times 10^{-14}\\,]$.\n  - Second column $\\mathbf{A}$ (units $\\mathrm{s}^{-1}$ per unit $a$): $[\\,800.0,\\ -400.0,\\ 1200.0\\,]$.\n- Baseline rate vector $\\mathbf{b}$ (in $\\mathrm{s}^{-1}$): $[\\,200.0,\\ 200.0,\\ 200.0\\,]$.\n- Prior mean $\\boldsymbol{\\mu}_0 = [\\,2.5\\times 10^{17},\\ 0.0\\,]^\\top$.\n- Prior standard deviations: $\\sigma_{n_f,0} = 0.8\\times 10^{17}$ and $\\sigma_{a,0} = 0.5$; prior covariance is diagonal with these variances.\n\nCase B (short integration times, lower-count regime):\n- Number of views: $N=3$.\n- Integration times $\\mathbf{T}$ (in $\\mathrm{s}$): $[\\,0.2,\\ 0.1,\\ 0.15\\,]$.\n- Observed counts $\\mathbf{K}$: $[\\,768,\\ 302,\\ 681\\,]$.\n- Sensitivity matrix columns:\n  - First column $\\mathbf{S}$ (units $\\mathrm{s}^{-1}$ per $\\mathrm{m}^{-3}$): $[\\,1.1\\times 10^{-14},\\ 0.9\\times 10^{-14},\\ 1.3\\times 10^{-14}\\,]$.\n  - Second column $\\mathbf{A}$ (units $\\mathrm{s}^{-1}$ per unit $a$): $[\\,800.0,\\ -400.0,\\ 1200.0\\,]$.\n- Baseline rate vector $\\mathbf{b}$ (in $\\mathrm{s}^{-1}$): $[\\,200.0,\\ 200.0,\\ 200.0\\,]$.\n- Prior mean $\\boldsymbol{\\mu}_0 = [\\,2.5\\times 10^{17},\\ 0.0\\,]^\\top$.\n- Prior standard deviations: $\\sigma_{n_f,0} = 0.8\\times 10^{17}$ and $\\sigma_{a,0} = 0.5$; prior covariance is diagonal.\n\nCase C (nearly colinear sensitivities, identifiability stress test):\n- Number of views: $N=3$.\n- Integration times $\\mathbf{T}$ (in $\\mathrm{s}$): $[\\,1.0,\\ 1.2,\\ 0.8\\,]$.\n- Observed counts $\\mathbf{K}$: $[\\,6150,\\ 11700,\\ 10680\\,]$.\n- Sensitivity matrix columns:\n  - First column $\\mathbf{S}$ (units $\\mathrm{s}^{-1}$ per $\\mathrm{m}^{-3}$): $[\\,1.0\\times 10^{-14},\\ 1.6\\times 10^{-14},\\ 2.2\\times 10^{-14}\\,]$.\n  - Define a proportionality constant $\\alpha = 3.0\\times 10^{18}$ and set the second column $\\mathbf{A} = \\alpha\\,\\mathbf{S}$ (units $\\mathrm{s}^{-1}$ per unit $a$), so $\\mathbf{A} = [\\,3.0\\times 10^{4},\\ 4.8\\times 10^{4},\\ 6.6\\times 10^{4}\\,]$.\n- Baseline rate vector $\\mathbf{b}$ (in $\\mathrm{s}^{-1}$): $[\\,150.0,\\ 150.0,\\ 150.0\\,]$.\n- Prior mean $\\boldsymbol{\\mu}_0 = [\\,2.5\\times 10^{17},\\ 0.0\\,]^\\top$.\n- Prior standard deviations: $\\sigma_{n_f,0} = 0.8\\times 10^{17}$ and $\\sigma_{a,0} = 0.5$; prior covariance is diagonal.\n\nAlgorithmic requirements:\n- Construct $\\mathbf{r}$ from $\\mathbf{K}$ and $\\mathbf{T}$ as $r_i = K_i/T_i$.\n- Construct the diagonal covariance of $\\mathbf{r}$ with entries $\\sigma_i^2 = K_i/T_i^2$.\n- Form the $N\\times 2$ matrix $\\mathbf{H}$ by concatenating $\\mathbf{S}$ and $\\mathbf{A}$ as its columns, and use $\\mathbf{b}$ to shift the mean of $\\mathbf{r}$ by computing $\\mathbf{y} = \\mathbf{r} - \\mathbf{b}$.\n- Combine the Gaussian likelihood with the Gaussian prior to obtain the Gaussian posterior for $\\boldsymbol{\\theta}$ and its covariance.\n- From the marginal posterior variances, compute the symmetric equal-tailed credible intervals at credibility level $\\alpha = 0.95$ for each component.\n\nFinal output format:\n- For each case, output a list containing six floating-point numbers in the following order: $[\\ \\text{mean}(n_f),\\ \\text{lower}_{0.95}(n_f),\\ \\text{upper}_{0.95}(n_f),\\ \\text{mean}(a),\\ \\text{lower}_{0.95}(a),\\ \\text{upper}_{0.95}(a)\\ ]$.\n- Your program should produce a single line of output containing the results for Cases A, B, and C as a comma-separated list of these three lists enclosed in square brackets (e.g., $[[\\dots],[\\dots],[\\dots]]$).",
            "solution": "The problem is deemed valid after a thorough review of its components. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique solution. The core of the problem is the application of Bayesian inference to a linearized physical model with Gaussian noise and a Gaussian prior, a standard technique in data analysis.\n\nThe objective is to infer the posterior distribution of a two-dimensional parameter vector $\\boldsymbol{\\theta} = [n_f, a]^\\top$, where $n_f$ is the fast-ion density and $a$ is an anisotropy parameter. The inference is based on a set of gamma-ray count measurements.\n\nFirst, we formalize the statistical model. The problem states that the observed counts $K_i$ in channel $i$ follow a Poisson distribution, which, for large counts, can be approximated by a Normal distribution. The observed rate in channel $i$ is $r_i = K_i / T_i$, where $T_i$ is the integration time. The mean of this rate is $\\mu_i$, and its variance is $\\sigma_i^2 = \\text{Var}(K_i/T_i) = \\text{Var}(K_i)/T_i^2$. For a Poisson distribution, $\\text{Var}(K_i) = \\mathbb{E}[K_i] = \\lambda_i = T_i \\mu_i$. Using the observed counts $K_i$ as an estimate for $\\lambda_i$, the variance of the observed rate $r_i$ is approximated as $\\sigma_i^2 \\approx K_i / T_i^2$.\n\nThe set of observed rates, $\\mathbf{r} = [r_1, \\dots, r_N]^\\top$, can thus be modeled by a multivariate Normal distribution with mean vector $\\boldsymbol{\\mu}$ and a diagonal covariance matrix $\\mathbf{C}$ with diagonal entries $C_{ii} = \\sigma_i^2$.\nThe relationship between the true rates $\\boldsymbol{\\mu}$ and the parameters $\\boldsymbol{\\theta}$ is given by the linearized model:\n$$\n\\boldsymbol{\\mu} \\approx \\mathbf{H}\\boldsymbol{\\theta} + \\mathbf{b}\n$$\nwhere $\\mathbf{H}$ is the $N \\times 2$ sensitivity matrix and $\\mathbf{b}$ is a baseline rate vector.\nThe data available for inference is the vector $\\mathbf{y} = \\mathbf{r} - \\mathbf{b}$, which, according to the model, is related to $\\boldsymbol{\\theta}$ by:\n$$\n\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{H}\\boldsymbol{\\theta}, \\mathbf{C})\n$$\nThis constitutes the likelihood function for $\\boldsymbol{\\theta}$, given the data $\\mathbf{y}$. The probability density function is:\n$$\np(\\mathbf{y} | \\boldsymbol{\\theta}) \\propto \\exp\\left(-\\frac{1}{2} (\\mathbf{y} - \\mathbf{H}\\boldsymbol{\\theta})^\\top \\mathbf{C}^{-1} (\\mathbf{y} - \\mathbf{H}\\boldsymbol{\\theta})\\right)\n$$\nThe prior knowledge about $\\boldsymbol{\\theta}$ is also modeled as a Gaussian distribution:\n$$\np(\\boldsymbol{\\theta}) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\boldsymbol{\\Lambda}_0)\n$$\nwhere $\\boldsymbol{\\mu}_0$ is the prior mean and $\\boldsymbol{\\Lambda}_0$ is the prior covariance matrix. The probability density is:\n$$\np(\\boldsymbol{\\theta}) \\propto \\exp\\left(-\\frac{1}{2} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0)^\\top \\boldsymbol{\\Lambda}_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0)\\right)\n$$\nAccording to Bayes' theorem, the posterior probability distribution $p(\\boldsymbol{\\theta} | \\mathbf{y})$ is proportional to the product of the likelihood and the prior:\n$$\np(\\boldsymbol{\\theta} | \\mathbf{y}) \\propto p(\\mathbf{y} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\n$$\nSince the product of two Gaussian distributions is another Gaussian distribution, the posterior will be Gaussian: $p(\\boldsymbol{\\theta} | \\mathbf{y}) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_p, \\boldsymbol{\\Lambda}_p)$. The exponent of the posterior density is the sum of the exponents of the likelihood and the prior (ignoring constant terms):\n$$\n-\\frac{1}{2} \\left( (\\mathbf{y} - \\mathbf{H}\\boldsymbol{\\theta})^\\top \\mathbf{C}^{-1} (\\mathbf{y} - \\mathbf{H}\\boldsymbol{\\theta}) + (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0)^\\top \\boldsymbol{\\Lambda}_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0) \\right)\n$$\nBy expanding the quadratic forms and completing the square with respect to $\\boldsymbol{\\theta}$, one can derive the posterior mean $\\boldsymbol{\\mu}_p$ and posterior covariance $\\boldsymbol{\\Lambda}_p$. The standard result for this Bayesian linear model is:\n$$\n\\boldsymbol{\\Lambda}_p^{-1} = \\mathbf{H}^\\top \\mathbf{C}^{-1} \\mathbf{H} + \\boldsymbol{\\Lambda}_0^{-1}\n$$\n$$\n\\boldsymbol{\\mu}_p = \\boldsymbol{\\Lambda}_p (\\mathbf{H}^\\top \\mathbf{C}^{-1} \\mathbf{y} + \\boldsymbol{\\Lambda}_0^{-1} \\boldsymbol{\\mu}_0)\n$$\nHere, $\\boldsymbol{\\Lambda}_p^{-1}$ is the posterior precision matrix, which is the sum of the likelihood precision ($\\mathbf{H}^\\top \\mathbf{C}^{-1} \\mathbf{H}$) and the prior precision ($\\boldsymbol{\\Lambda}_0^{-1}$). The posterior covariance is found by inverting this matrix: $\\boldsymbol{\\Lambda}_p = (\\boldsymbol{\\Lambda}_p^{-1})^{-1}$.\n\nIn Case C, the columns of $\\mathbf{H}$ are collinear, making the matrix $\\mathbf{H}^\\top \\mathbf{C}^{-1} \\mathbf{H}$ singular. A purely likelihood-based inference would fail. However, the addition of the positive-definite prior precision $\\boldsymbol{\\Lambda}_0^{-1}$ regularizes the problem, ensuring that the posterior precision $\\boldsymbol{\\Lambda}_p^{-1}$ is invertible and thus the posterior distribution is well-defined.\n\nOnce the posterior mean vector $\\boldsymbol{\\mu}_p$ and covariance matrix $\\boldsymbol{\\Lambda}_p$ are computed, the marginal distribution for each parameter $\\theta_j$ (where $j \\in \\{1, 2\\}$) is a Gaussian with mean $(\\boldsymbol{\\mu}_p)_j$ and variance $(\\boldsymbol{\\Lambda}_p)_{jj}$. Let $\\mu_{p,j}$ be the posterior mean and $\\sigma_{p,j} = \\sqrt{(\\boldsymbol{\\Lambda}_p)_{jj}}$ be the posterior standard deviation for parameter $j$.\n\nThe $100(1-\\delta)\\%$ equal-tailed credible interval for $\\theta_j$ is given by:\n$$\n[\\mu_{p,j} - z_{\\delta/2} \\sigma_{p,j}, \\mu_{p,j} + z_{\\delta/2} \\sigma_{p,j}]\n$$\nwhere $z_{\\delta/2}$ is the upper $\\delta/2$ critical value of the standard normal distribution. For the specified credibility level $\\alpha=0.95$, we have $\\delta = 0.05$, so we need $z_{0.025}$. This value is $\\Phi^{-1}(1-0.025) = \\Phi^{-1}(0.975) \\approx 1.959964$.\n\nThe algorithm for each test case is as follows:\n1.  Construct the vectors $\\mathbf{T}$ and $\\mathbf{K}$.\n2.  Calculate the observed rates $\\mathbf{r}$ with elements $r_i = K_i / T_i$.\n3.  Calculate the shifted data vector $\\mathbf{y} = \\mathbf{r} - \\mathbf{b}$.\n4.  Construct the diagonal data covariance matrix $\\mathbf{C}$ with diagonal elements $C_{ii} = K_i / T_i^2$, and its inverse $\\mathbf{C}^{-1}$ with diagonal elements $T_i^2 / K_i$.\n5.  Construct the sensitivity matrix $\\mathbf{H}$ by concatenating the column vectors $\\mathbf{S}$ and $\\mathbf{A}$.\n6.  Construct the prior mean vector $\\boldsymbol{\\mu}_0$ and the diagonal prior covariance matrix $\\boldsymbol{\\Lambda}_0$ from the given prior standard deviations (variances are the squares). Compute the inverse $\\boldsymbol{\\Lambda}_0^{-1}$.\n7.  Compute the posterior precision matrix $\\boldsymbol{\\Lambda}_p^{-1} = \\mathbf{H}^\\top \\mathbf{C}^{-1} \\mathbf{H} + \\boldsymbol{\\Lambda}_0^{-1}$.\n8.  Invert it to get the posterior covariance matrix $\\boldsymbol{\\Lambda}_p = (\\boldsymbol{\\Lambda}_p^{-1})^{-1}$.\n9.  Compute the posterior mean vector $\\boldsymbol{\\mu}_p = \\boldsymbol{\\Lambda}_p (\\mathbf{H}^\\top \\mathbf{C}^{-1} \\mathbf{y} + \\boldsymbol{\\Lambda}_0^{-1} \\boldsymbol{\\mu}_0)$.\n10. Extract the posterior means for $n_f$ and $a$ from $\\boldsymbol{\\mu}_p$.\n11. Extract the posterior variances from the diagonal of $\\boldsymbol{\\Lambda}_p$ and compute the standard deviations.\n12. Calculate the lower and upper bounds for the $95\\%$ credible intervals for both parameters using the $z_{0.025}$ critical value.\n13. Assemble the six required floating-point numbers into a list for the final output.\nThis procedure is applied to each of the three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final results.\n    \"\"\"\n\n    # Define the credibility level and the corresponding z-score.\n    alpha = 0.95\n    z_score = norm.ppf(1 - (1 - alpha) / 2)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: well-conditioned, high-count regime\n        {\n            \"T\": np.array([2.0, 1.5, 2.5]),\n            \"K\": np.array([7680, 4530, 11350]),\n            \"S\": np.array([1.1e-14, 0.9e-14, 1.3e-14]),\n            \"A\": np.array([800.0, -400.0, 1200.0]),\n            \"b\": np.array([200.0, 200.0, 200.0]),\n            \"mu0\": np.array([2.5e17, 0.0]),\n            \"sigma0\": np.array([0.8e17, 0.5]),\n        },\n        # Case B: short integration times, lower-count regime\n        {\n            \"T\": np.array([0.2, 0.1, 0.15]),\n            \"K\": np.array([768, 302, 681]),\n            \"S\": np.array([1.1e-14, 0.9e-14, 1.3e-14]),\n            \"A\": np.array([800.0, -400.0, 1200.0]),\n            \"b\": np.array([200.0, 200.0, 200.0]),\n            \"mu0\": np.array([2.5e17, 0.0]),\n            \"sigma0\": np.array([0.8e17, 0.5]),\n        },\n        # Case C: nearly colinear sensitivities, identifiability stress test\n        {\n            \"T\": np.array([1.0, 1.2, 0.8]),\n            \"K\": np.array([6150, 11700, 10680]),\n            \"S\": np.array([1.0e-14, 1.6e-14, 2.2e-14]),\n            \"A\": 3.0e18 * np.array([1.0e-14, 1.6e-14, 2.2e-14]),\n            \"b\": np.array([150.0, 150.0, 150.0]),\n            \"mu0\": np.array([2.5e17, 0.0]),\n            \"sigma0\": np.array([0.8e17, 0.5]),\n        },\n    ]\n\n    results = []\n    for case_data in test_cases:\n        # Pass data to the core computation function\n        result = compute_posterior(case_data, z_score)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list of floats is what is needed.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_posterior(data, z_score):\n    \"\"\"\n    Computes the posterior mean and credible intervals for a given case.\n    \n    Args:\n        data (dict): A dictionary containing all the parameters for a single case.\n        z_score (float): The critical value for the credible interval calculation.\n        \n    Returns:\n        list: A list of 6 floats: [mean_nf, lower_nf, upper_nf, mean_a, lower_a, upper_a].\n    \"\"\"\n    T = data[\"T\"]\n    K = data[\"K\"]\n    S = data[\"S\"]\n    A = data[\"A\"]\n    b = data[\"b\"]\n    mu0 = data[\"mu0\"]\n    sigma0 = data[\"sigma0\"]\n\n    # 1. Construct observed rate vector r and shifted data vector y\n    r = K / T\n    y = r - b\n\n    # 2. Construct diagonal data covariance matrix C and its inverse C_inv\n    # C_ii = K_i / T_i^2\n    # C_inv_ii = T_i^2 / K_i\n    C_inv_diag = (T**2) / K\n    C_inv = np.diag(C_inv_diag)\n\n    # 3. Construct sensitivity matrix H\n    H = np.column_stack((S, A))\n    \n    # 4. Construct prior covariance matrix Lambda0 and its inverse Lambda0_inv\n    Lambda0 = np.diag(sigma0**2)\n    Lambda0_inv = np.diag(1 / (sigma0**2))\n    \n    # 5. Compute posterior precision matrix Lambda_p_inv\n    # Lambda_p_inv = H^T * C_inv * H + Lambda0_inv\n    Lambda_p_inv = H.T @ C_inv @ H + Lambda0_inv\n    \n    # 6. Compute posterior covariance matrix Lambda_p\n    Lambda_p = np.linalg.inv(Lambda_p_inv)\n    \n    # 7. Compute posterior mean vector mu_p\n    # mu_p = Lambda_p * (H^T * C_inv * y + Lambda0_inv * mu0)\n    term1 = H.T @ C_inv @ y\n    term2 = Lambda0_inv @ mu0\n    mu_p = Lambda_p @ (term1 + term2)\n    \n    # 8. Extract posterior means and standard deviations\n    mean_nf, mean_a = mu_p[0], mu_p[1]\n    std_nf = np.sqrt(Lambda_p[0, 0])\n    std_a = np.sqrt(Lambda_p[1, 1])\n    \n    # 9. Compute credible intervals\n    lower_nf = mean_nf - z_score * std_nf\n    upper_nf = mean_nf + z_score * std_nf\n    lower_a = mean_a - z_score * std_a\n    upper_a = mean_a + z_score * std_a\n    \n    return [mean_nf, lower_nf, upper_nf, mean_a, lower_a, upper_a]\n\nsolve()\n```"
        }
    ]
}