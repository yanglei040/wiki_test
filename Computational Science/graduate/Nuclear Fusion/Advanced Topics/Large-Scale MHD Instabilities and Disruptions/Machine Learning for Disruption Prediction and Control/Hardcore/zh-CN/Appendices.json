{
    "hands_on_practices": [
        {
            "introduction": "机器学习模型为我们提供了等离子体破裂的概率。我们应如何利用这个数值来做出一个二元决策：是触发缓解措施，还是不触发？这个决策必须在误报（不必要的缓解）的成本与漏报（破裂造成的损害）的成本之间取得平衡。这个练习  将机器学习模型的抽象输出置于一个具体的决策论框架中。通过最小化预期总成本，你将推导出一个用于触发缓解行动的最优的、经济上合理的阈值。",
            "id": "3707575",
            "problem": "托卡马克的破裂预测系统使用机器学习 (ML) 在每个控制周期输出一个校准概率 $p \\in [0,1]$，表示在未来 $30\\,\\mathrm{ms}$ 内发生主要等离子体破裂的可能性。一个名为“大量气体注入”(MGI) 的紧急缓解执行器可以被触发，以终止放电并抑制损害。当 MGI 被触发时，它能可靠地防止损害，但会中断等离子体放电。设触发 MGI 的成本为一个恒定的单次驱动成本 $C_{A}$，该成本计入了损失的实验价值和设备磨损。如果不触发 MGI 且破裂发生，设导致的损害成本为一个恒定的 $C_{D}$，该成本汇总了硬件风险和停机时间。假设 ML 预测器是完美校准的：在给定一个 $p$ 的条件下，决策窗口内发生破裂的真实概率恰好为 $p$，并且如果应用驱动，则会消除该窗口内的破裂损害。\n\n一项操作策略在 $p \\ge \\tau$ 时触发 MGI，其中 $\\tau \\in [0,1]$ 是一个固定阈值。在所有控制周期中，ML 模型预测的概率 $p$ 服从参数为 $\\alpha$ 和 $\\beta$ 的贝塔分布，其密度函数为\n$$\nf(p) \\;=\\; \\frac{1}{B(\\alpha,\\beta)}\\,p^{\\alpha-1}\\,(1-p)^{\\beta-1}, \\quad p \\in (0,1),\n$$\n其中 $B(\\alpha,\\beta)$ 是贝塔函数。在一次特定的实验活动中，历史数据显示 $\\alpha = 3$ 和 $\\beta = 97$。对于当前设备和实验活动的经济性而言，单次驱动成本和损害成本分别为 $C_{A} = 1.8 \\times 10^{4}$ 和 $C_{D} = 3.2 \\times 10^{6}$，两者均采用一致的成本单位。\n\n从阈值策略下的期望成本定义和全期望定律出发，从第一性原理推导出在对 $p$ 的分布进行平均时，使每次决策的期望总成本最小化的阈值 $\\tau^{\\star}$。不要假设任何特定的风险启发式方法；使用微积分推导最优性的必要条件。然后，使用给定的 $C_{A}$ 和 $C_{D}$ 的数值，计算 $\\tau^{\\star}$ 的数值。将您计算出的 $\\tau^{\\star}$ 的最终数值结果四舍五入到四位有效数字。最终答案应该是一个无量纲的单一数字。",
            "solution": "我们通过对预测概率 $p \\in [0,1]$ 进行条件化来为每个决策周期建模。根据完美校准的假设，该概率等于决策窗口内发生破裂的真实概率。一个阈值为 $\\tau \\in [0,1]$ 的阈值策略当且仅当 $p \\ge \\tau$ 时触发驱动。\n\n对于固定的 $p$ 和阈值 $\\tau$，决策及其瞬时期望成本如下：\n- 如果 $p  \\tau$，我们不进行驱动。破裂以概率 $p$ 发生，并产生成本 $C_{D}$；以概率 $1-p$ 不发生破裂，也不产生驱动成本。此区域的条件期望成本为 $p\\,C_{D}$。\n- 如果 $p \\ge \\tau$，我们进行驱动，这会抑制破裂损害，但会产生驱动成本 $C_{A}$。此区域的条件期望成本为 $C_{A}$。\n\n令 $f(p)$ 表示在操作周期内 $p$ 的密度函数。那么，在阈值 $\\tau$ 下，每次决策的期望总成本为\n$$\n\\mathcal{J}(\\tau)\n\\;=\\;\n\\int_{0}^{\\tau} \\big(p\\,C_{D}\\big)\\, f(p)\\, dp\n\\;+\\;\n\\int_{\\tau}^{1} \\big(C_{A}\\big)\\, f(p)\\, dp.\n$$\n这个表达式遵循全期望定律，通过将定义域划分为不驱动和驱动两个区域得出。\n\n为了在 $\\tau \\in [0,1]$ 上最小化 $\\mathcal{J}(\\tau)$，我们使用含参变量积分的莱布尼茨法则对 $\\tau$ 求导。假设 $f$ 是连续的（这对于在 $(0,1)$ 上的贝塔密度函数是成立的），我们得到\n$$\n\\frac{d\\mathcal{J}}{d\\tau}\n=\n\\frac{d}{d\\tau}\\left[\\int_{0}^{\\tau} p\\,C_{D}\\, f(p)\\, dp\\right]\n+\n\\frac{d}{d\\tau}\\left[\\int_{\\tau}^{1} C_{A}\\, f(p)\\, dp\\right]\n=\n\\tau\\,C_{D}\\, f(\\tau)\\;-\\; C_{A}\\, f(\\tau).\n$$\n因此，\n$$\n\\frac{d\\mathcal{J}}{d\\tau}\n=\nf(\\tau)\\,\\big(\\tau\\,C_{D} - C_{A}\\big).\n$$\n临界点满足 $\\frac{d\\mathcal{J}}{d\\tau}=0$。由于对于贝塔密度函数，当 $\\tau \\in (0,1)$ 时有 $f(\\tau) > 0$（并且在边界处，该导数条件通过连续性来解释），因此必要条件简化为\n$$\n\\tau\\,C_{D} - C_{A} \\;=\\; 0\n\\quad\\Rightarrow\\quad\n\\tau^{\\star} \\;=\\; \\frac{C_{A}}{C_{D}}.\n$$\n为验证该临界点是一个极小值点，我们计算二阶导数：\n$$\n\\frac{d^{2}\\mathcal{J}}{d\\tau^{2}}\n=\n\\frac{d}{d\\tau}\\Big[f(\\tau)\\,(\\tau\\,C_{D} - C_{A})\\Big]\n=\nf'(\\tau)\\,(\\tau\\,C_{D} - C_{A})\n+\nf(\\tau)\\,C_{D}.\n$$\n在 $\\tau^{\\star} = \\frac{C_{A}}{C_{D}}$ 处，项 $(\\tau\\,C_{D} - C_{A})$ 变为零，得到\n$$\n\\left.\\frac{d^{2}\\mathcal{J}}{d\\tau^{2}}\\right|_{\\tau=\\tau^{\\star}}\n=\nf(\\tau^{\\star})\\,C_{D}\n\\;\\; 0,\n$$\n因为 $f(\\tau^{\\star}) > 0$ 且 $C_{D} > 0$。因此，$\\tau^{\\star}$ 是在 $[0,1]$ 上的唯一全局最小值点。值得注意的是，最优阈值除了依赖于分布 $f(p)$ 在 $(0,1)$ 上为正之外，不依赖于该分布的其他任何性质。\n\n我们现在代入给定的数值 $C_{A} = 1.8 \\times 10^{4}$ 和 $C_{D} = 3.2 \\times 10^{6}$：\n$$\n\\tau^{\\star}\n=\n\\frac{1.8 \\times 10^{4}}{3.2 \\times 10^{6}}\n=\n\\frac{18{,}000}{3{,}200{,}000}\n=\n\\frac{9}{1600}\n=\n0.005625.\n$$\n四舍五入到四位有效数字（从第一个非零数字开始计算），该值仍为 $0.005625$。",
            "answer": "$$\\boxed{0.005625}$$"
        },
        {
            "introduction": "我们已经有了一个理论上的最优策略。但是，我们的实时系统真的能够执行它吗？控制系统具有有限的速度和资源，其设计必须在预测范围、模型复杂性、计算硬件和执行器物理特性之间进行权衡。这项实践  将带你从理论策略走向实际部署。你将为一个模型预测控制器计算出一组可行的参数，确保整个“感知-计算-执行”循环在满足其严格的截止时间的同时，也尊重硬件的物理限制。",
            "id": "3707561",
            "problem": "托卡马克的一种实时破裂规避控制器采用模型预测控制（MPC），并使用一个学习到的代理模型来模拟等离子体-执行器的相互作用。该控制器必须每 $1~\\text{ms}$ 完成一次闭环，这意味着包括传感、计算和驱动在内的端到端周期时间被限制为 $T_{c} = 1 \\times 10^{-3}~\\text{s}$。目标是为MPC优化器确定一个可行的内部预测离散化（采样时间）$T_{s}$ 和预测时域长度 $N$，使其在满足 $1~\\text{ms}$ 控制周期的同时，也能满足计算约束和执行器动态约束。\n\n学习到的代理模型是一个前馈神经网络，它将 $(x_{k}, u_{k})$ 映射到 $x_{k+1}$，并带有全连接层。其架构的输入维度为 $n_{\\text{in}} = 14$，有两个宽度为 $128$ 的隐藏层，输出维度为 $n_{\\text{out}} = 10$。假设每次评估的前向传播浮点运算（FLOP）次数主要由稠密矩阵-向量乘法和每个神经元的激活决定，并近似为每层中乘法和加法的总和，再加上隐藏层中每个神经元的一次激活。因此，单次前向传播的成本为\n$$\nF_{\\text{fp}} = 2 \\, n_{\\text{in}} \\cdot 128 \\;+\\; 2 \\cdot 128 \\cdot 128 \\;+\\; 2 \\cdot 128 \\cdot n_{\\text{out}} \\;+\\; 128 \\;+\\; 128.\n$$\n假设用于梯度计算的反向模式自动微分每次评估的成本为 $3 F_{\\text{fp}}$。MPC优化器在每个控制周期内使用 $P = 6$ 次迭代；每次迭代对每个时域步长执行一次推演和一次反向传播，因此每次迭代、每个步长的成本为 $F_{\\text{fp}} + 3 F_{\\text{fp}}$。持续计算吞吐量为 $S = 2.0 \\times 10^{10}$ FLOP/s。周期的非计算开销包括传感延迟 $t_{\\text{sens}} = 1.0 \\times 10^{-4}~\\text{s}$、驱动接口延迟 $t_{\\text{act}} = 5.0 \\times 10^{-5}~\\text{s}$ 和内存传输时间 $t_{\\text{copy}} = 5.0 \\times 10^{-5}~\\text{s}$，因此总开销为 $t_{\\text{oh}} = t_{\\text{sens}} + t_{\\text{act}} + t_{\\text{copy}}$，每个周期的可用计算预算为 $t_{\\text{bud}} = T_{c} - t_{\\text{oh}}$。\n\n垂直位置控制线圈执行器的主导一阶时间常数为 $\\tau_{a} = 4.0 \\times 10^{-4}~\\text{s}$，最大指令摆率为 $r_{\\max} = 2.0 \\times 10^{5}~\\text{A/s}$。出于安全约束，控制器的每样本指令增量被限制在 $\\Delta u_{\\max} = 20~\\text{A}$ 以内。为避免指令变化速度快于执行器物理上能实现的速度，内部预测离散化必须满足下界\n$$\nT_{s} \\geq \\max\\!\\left(t_{\\text{act}}, \\frac{\\Delta u_{\\max}}{r_{\\max}}\\right).\n$$\n为充分预测执行器响应，要求预测时域至少覆盖两个执行器时间常数：\n$$\nN \\, T_{s} \\geq 2 \\, \\tau_{a}.\n$$\n在计算方面，设每个周期的总计算时间为\n$$\nt_{\\text{comp}}(N) = \\frac{P \\, N \\, (F_{\\text{fp}} + 3 F_{\\text{fp}})}{S}.\n$$\n可行性要求 $t_{\\text{comp}}(N) \\leq t_{\\text{bud}}$。\n\n在这些约束条件下，选择满足执行器下界的最小 $T_{s}$ 值以最大化模型保真度，并选择同时满足 (i) 时域覆盖范围 $N T_{s} \\geq 2 \\tau_{a}$ 和 (ii) 计算预算 $t_{\\text{comp}}(N) \\leq t_{\\text{bud}}$ 的最小正整数 $N$。计算得到的可行对 $(T_{s}, N)$。\n\n将最终的 $T_{s}$ 四舍五入到四位有效数字，并以秒为单位表示。将 $N$ 报告为无量纲整数。最终答案必须是一个单行矩阵，按顺序包含 $T_{s}$ 和 $N$。",
            "solution": "首先，我们计算求解此问题所需的常数和中间值。\n\n1.  **计算单次前向传播的浮点运算（FLOP）次数 $F_{\\text{fp}}$**:\n    根据给定的网络架构 ($n_{\\text{in}} = 14, n_{\\text{out}} = 10$, 两个隐藏层宽度为 128)，我们有：\n    $$ F_{\\text{fp}} = 2 \\cdot 14 \\cdot 128 + 2 \\cdot 128 \\cdot 128 + 2 \\cdot 128 \\cdot 10 + 128 + 128 = 3584 + 32768 + 2560 + 256 = 39168 \\, \\text{FLOPs} $$\n\n2.  **计算总开销时间 $t_{\\text{oh}}$ 和可用计算预算 $t_{\\text{bud}}$**:\n    总开销是传感、驱动和内存传输延迟的总和：\n    $$ t_{\\text{oh}} = t_{\\text{sens}} + t_{\\text{act}} + t_{\\text{copy}} = (1.0 \\times 10^{-4}) + (5.0 \\times 10^{-5}) + (5.0 \\times 10^{-5}) = 2.0 \\times 10^{-4}~\\text{s} $$\n    因此，每个控制周期 $T_c = 1 \\times 10^{-3}~\\text{s}$ 内的可用计算时间为：\n    $$ t_{\\text{bud}} = T_{c} - t_{\\text{oh}} = (1 \\times 10^{-3}) - (2.0 \\times 10^{-4}) = 8.0 \\times 10^{-4}~\\text{s} $$\n\n接下来，我们确定采样时间 $T_s$。问题要求我们选择满足执行器下界的最小 $T_s$ 值。\n$$ T_{s} \\geq \\max\\!\\left(t_{\\text{act}}, \\frac{\\Delta u_{\\max}}{r_{\\max}}\\right) $$\n代入数值：\n$$ \\frac{\\Delta u_{\\max}}{r_{\\max}} = \\frac{20~\\text{A}}{2.0 \\times 10^{5}~\\text{A/s}} = 1.0 \\times 10^{-4}~\\text{s} $$\n$$ T_{s} \\geq \\max(5.0 \\times 10^{-5}~\\text{s}, 1.0 \\times 10^{-4}~\\text{s}) = 1.0 \\times 10^{-4}~\\text{s} $$\n因此，我们选择最小允许值 $T_s = 1.0 \\times 10^{-4}~\\text{s}$。\n\n然后，我们确定预测时域长度 $N$ 的约束条件。$N$ 必须是满足以下两个条件的最小正整数：\n\n1.  **时域覆盖约束**: 预测时域必须至少覆盖两个执行器时间常数。\n    $$ N T_{s} \\geq 2 \\tau_{a} \\implies N \\cdot (1.0 \\times 10^{-4}) \\geq 2 \\cdot (4.0 \\times 10^{-4}) \\implies N \\geq 8 $$\n\n2.  **计算预算约束**: 总计算时间不能超过可用预算。\n    每个周期的总计算时间为：\n    $$ t_{\\text{comp}}(N) = \\frac{P \\cdot N \\cdot (F_{\\text{fp}} + 3 F_{\\text{fp}})}{S} = \\frac{4 P N F_{\\text{fp}}}{S} $$\n    代入数值：\n    $$ t_{\\text{comp}}(N) = \\frac{4 \\cdot 6 \\cdot N \\cdot 39168}{2.0 \\times 10^{10}} = N \\cdot \\frac{939024}{2.0 \\times 10^{10}} \\approx N \\cdot 4.69512 \\times 10^{-5}~\\text{s} $$\n    应用约束 $t_{\\text{comp}}(N) \\leq t_{\\text{bud}}$:\n    $$ N \\cdot 4.69512 \\times 10^{-5} \\leq 8.0 \\times 10^{-4} \\implies N \\leq \\frac{8.0 \\times 10^{-4}}{4.69512 \\times 10^{-5}} \\approx 17.039 $$\n    由于 $N$ 必须是整数，所以 $N \\leq 17$。\n\n结合两个约束条件，我们得到 $N$ 的可行范围是 $8 \\leq N \\leq 17$。问题要求我们选择最小的正整数 $N$，即 $N=8$。\n\n因此，满足所有约束的可行对是 $(T_s, N) = (1.0 \\times 10^{-4}~\\text{s}, 8)$。",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 1.000 \\times 10^{-4}  8 \\end{pmatrix} } $$"
        },
        {
            "introduction": "我们设计了一个新的、可能更优的控制策略，但在价值数百万美元的托卡马克上直接部署它风险极高。我们如何能在上线前就评估其性能？离线策略评估（OPE）方法，如逆倾向加权（IPW），能够利用旧策略下记录的历史数据来估计新策略的价值。这个编程练习  将指导你实现和分析关键的 OPE 估计器。你将学会如何处理高方差和模型设定错误等挑战，从而掌握在高风险环境中安全验证和迭代改进控制策略的实用技能。",
            "id": "3707515",
            "problem": "您将处理一个基于仿真的评估任务，该任务为托卡马克（tokamak）中的破裂控制建立了一个二元缓解触发决策模型。目标是使用在随机行为策略下记录的历史数据，为一个确定性目标策略实现使用逆倾向加权（IPW）的离策略评估（OPE）。您必须设计并实现一个程序，该程序计算基于IPW的估计值，分析偏差和方差的权衡（包括自归一化和裁剪），并在一个测试场景套件中汇总结果。\n\n假设一个上下文赌博机抽象，它捕捉了破裂缓解决策过程中一个简化但科学上合理的片段。对于每个具有协变量 $X \\in \\mathbb{R}^2$ 的等离子体放电快照，一个历史行为策略 $\\pi_b$ 选择一个动作 $A \\in \\{0,1\\}$ （其中 $A=1$ 意味着触发缓解执行器，如大量气体注入，而 $A=0$ 意味着不触发），并观察到一个奖励 $R \\in \\{0,1\\}$ （其中 $R=1$ 表示有利结果， $R=0$ 表示不利结果）。记录的倾向性被记为 $p_b(A \\mid X)$ 或一个可能被错误记录的变体。目标是评估一个确定性目标策略 $\\pi_e$ 的期望奖励，该策略根据风险得分的阈值进行触发。\n\n基本假设与定义：\n- 离策略评估（OPE）：目标策略的价值为 $V(\\pi_e) = \\mathbb{E}[R \\mid \\pi_e]$。\n- 逆倾向加权（IPW）：对于记录的数据 $\\{(X_i, A_i, R_i, p_i)\\}_{i=1}^n$ 且行为倾向性为 $p_i = \\pi_b(A_i \\mid X_i)$，确定性目标策略 $\\pi_e$ 的 IPW 估计量为\n  $$\\widehat{V}_{\\mathrm{IPW}} = \\frac{1}{n}\\sum_{i=1}^n \\frac{\\mathbb{I}\\{A_i = \\pi_e(X_i)\\}}{\\pi_b(A_i \\mid X_i)} R_i.$$\n- 自归一化逆倾向加权（SNIPW）：自归一化估计量为\n  $$\\widehat{V}_{\\mathrm{SNIPW}} = \\frac{\\sum_{i=1}^n w_i R_i}{\\sum_{i=1}^n w_i}, \\quad \\text{其中 } w_i = \\frac{\\mathbb{I}\\{A_i = \\pi_e(X_i)\\}}{\\pi_b(A_i \\mid X_i)}.$$\n- 裁剪的IPW：为控制方差，定义裁剪后的权重 $w_i^{(\\tau)} = \\min\\{w_i, \\tau\\}$（对于阈值 $\\tau > 0$），估计量为\n  $$\\widehat{V}_{\\mathrm{CLIP}} = \\frac{1}{n}\\sum_{i=1}^n w_i^{(\\tau)} R_i.$$\n\n仿真模型：\n- 协变量：$X = (X_1, X_2)$ 是独立同分布的，服从标准二维正态分布：$X \\sim \\mathcal{N}(0, I_2)$。\n- 风险得分：$s(X) = w_0 + w_1 X_1 + w_2 X_2$，固定系数为 $w_0 = 0.1$，$w_1 = 1.0$ 和 $w_2 = -0.8$。\n- Sigmoid函数：$\\sigma(z) = 1/(1 + e^{-z})$。\n- 行为策略：对于尖锐度参数 $k > 0$，定义动作 $A=1$ 的行为倾向性为 $p_b(1 \\mid X) = \\mathrm{clip}(\\sigma(k \\cdot s(X)), \\epsilon, 1-\\epsilon)$，其中 $\\mathrm{clip}(p,\\epsilon,1-\\epsilon) = \\min(\\max(p,\\epsilon), 1-\\epsilon)$ 且 $\\epsilon \\in (0, 1/2)$ 是一个小的数值下限。那么 $p_b(0 \\mid X) = 1 - p_b(1 \\mid X)$。\n- 目标策略：确定性阈值策略 $\\pi_e(X) = \\mathbb{I}\\{ s(X) \\ge t \\}$，其中 $t \\in \\mathbb{R}$。\n- 等离子体风险代理：$h(X) = \\sigma(\\theta_0 + \\theta_1 X_1 + \\theta_2 X_2 + \\theta_{12} X_1 X_2)$，固定参数为 $\\theta_0 = -0.2$, $\\theta_1 = 1.0$, $\\theta_2 = 1.2$, 和 $\\theta_{12} = 0.5$。\n- 奖励模型：条件平均奖励为 $m(X,A) = \\sigma(\\eta_0 - \\alpha \\cdot h(X) + \\gamma A + \\delta A X_2)$，固定参数为 $\\eta_0 = 0.2$, $\\alpha = 3.0$, $\\gamma = 1.2$, 和 $\\delta = 0.8$。二元奖励 $R$ 从均值为 $m(X,A)$ 的伯努利分布中抽取。\n\n记录的倾向性：\n- 真实记录：$p_i = p_b(A_i \\mid X_i)$。\n- 错误记录的倾向性：对于噪声水平 $\\sigma_{\\log} \\ge 0$，设置 $\\tilde{p}_b(1 \\mid X) = \\mathrm{clip}(p_b(1 \\mid X) \\cdot \\exp(\\varepsilon), \\epsilon, 1-\\epsilon)$，其中每个样本的 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\log}^2)$ 独立抽取，并在估计量中使用 $p_i = \\tilde{p}_b(A_i \\mid X_i)$。当 $A_i = 0$ 时，使用 $p_i = 1 - \\tilde{p}_b(1 \\mid X_i)$。\n\n真实值：\n- 由于数据生成过程是已知的，通过蒙特卡洛直接积分来近似真实值 $V(\\pi_e)$。从 $\\mathcal{N}(0, I_2)$ 中抽取 $N_{\\mathrm{true}}$ 个独立的协变量样本，并计算 $V(\\pi_e) \\approx \\frac{1}{N_{\\mathrm{true}}} \\sum_{j=1}^{N_{\\mathrm{true}}} m(X_j, \\pi_e(X_j))$。使用一个大的 $N_{\\mathrm{true}}$ 值，使得此任务中的蒙特卡洛误差可以忽略不计。\n\n方差估计：\n- 根据中心极限定理，对于IPW估计量，其渐进方差为 $\\mathrm{Var}(\\widehat{V}_{\\mathrm{IPW}}) \\approx \\mathrm{Var}(Z)/n$，其中 $Z = w R$ 且 $w = \\mathbb{I}\\{A = \\pi_e(X)\\}/\\pi_b(A \\mid X)$。使用 $\\{Z_i\\}_{i=1}^n$ 的无偏样本方差来估计 $\\mathrm{Var}(Z)$，并将估计的标准差报告为 $\\sqrt{\\widehat{\\mathrm{Var}}(Z)/n}$。\n\n您的任务：\n- 精确按照规定实现仿真和OPE估计量。\n- 对每个测试案例，估计以下量：\n  $1)$ $\\widehat{V}_{\\mathrm{IPW}}$，\n  $2)$ $\\widehat{V}_{\\mathrm{SNIPW}}$，\n  $3)$ 使用裁剪阈值 $\\tau$ 的 $\\widehat{V}_{\\mathrm{CLIP}}$，\n  $4)$ $\\widehat{V}_{\\mathrm{IPW}}$ 的估计标准差，\n  $5)$ 真实值 $V(\\pi_e)$，\n  $6)$ 偏差 $\\widehat{V}_{\\mathrm{IPW}} - V(\\pi_e)$，$\\widehat{V}_{\\mathrm{SNIPW}} - V(\\pi_e)$，和 $\\widehat{V}_{\\mathrm{CLIP}} - V(\\pi_e)$。\n- 将所有报告的浮点数输出四舍五入到 $6$ 位小数。\n- 不涉及物理单位；所有输出都是无单位的。\n\n测试套件：\n使用以下四个案例来测试估计量的不同特性。在所有案例中，固定的模型参数如上所述，风险得分系数为 $(w_0, w_1, w_2) = (0.1, 1.0, -0.8)$，Sigmoid函数为 $\\sigma(z) = 1/(1+e^{-z})$。\n\n- 案例1（记录规范，中度重叠）：\n  - 样本量：$n = 50000$。\n  - 行为策略尖锐度：$k = 2.0$。\n  - 目标策略阈值：$t = 0.0$。\n  - 倾向性下限：$\\epsilon = 10^{-3}$。\n  - 记录噪声：$\\sigma_{\\log} = 0.0$（真实记录）。\n  - 裁剪阈值：$\\tau = 10.0$。\n  - 数据种子：$10$，真实值种子：$101$，真实值蒙特卡洛大小：$N_{\\mathrm{true}} = 300000$。\n\n- 案例2（记录规范，行为近乎确定性且方差高）：\n  - 样本量：$n = 50000$。\n  - 行为策略尖锐度：$k = 8.0$。\n  - 目标策略阈值：$t = 0.0$。\n  - 倾向性下限：$\\epsilon = 10^{-4}$。\n  - 记录噪声：$\\sigma_{\\log} = 0.0$（真实记录）。\n  - 裁剪阈值：$\\tau = 10.0$。\n  - 数据种子：$20$，真实值种子：$202$，真实值蒙特卡洛大小：$N_{\\mathrm{true}} = 300000$。\n\n- 案例3（倾向性记录错误导致偏差）：\n  - 样本量：$n = 50000$。\n  - 行为策略尖锐度：$k = 2.0$。\n  - 目标策略阈值：$t = 0.5$。\n  - 倾向性下限：$\\epsilon = 10^{-3}$。\n  - 记录噪声：$\\sigma_{\\log} = 0.25$（错误记录的倾向性）。\n  - 裁剪阈值：$\\tau = 10.0$。\n  - 数据种子：$30$，真实值种子：$303$，真实值蒙特卡洛大小：$N_{\\mathrm{true}} = 300000$。\n\n- 案例4（记录规范，目标动作在尾部区域支持有限）：\n  - 样本量：$n = 80000$。\n  - 行为策略尖锐度：$k = 3.0$。\n  - 目标策略阈值：$t = 1.5$。\n  - 倾向性下限：$\\epsilon = 10^{-3}$。\n  - 记录噪声：$\\sigma_{\\log} = 0.0$（真实记录）。\n  - 裁剪阈值：$\\tau = 10.0$。\n  - 数据种子：$40$，真实值种子：$404$，真实值蒙特卡洛大小：$N_{\\mathrm{true}} = 300000$。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含方括号内的所有结果，以逗号分隔。\n- 对于每个案例，按此顺序输出8个浮点数序列：\n  $[\\widehat{V}_{\\mathrm{IPW}}, \\widehat{V}_{\\mathrm{SNIPW}}, \\widehat{V}_{\\mathrm{CLIP}}, \\widehat{\\mathrm{Std}}_{\\mathrm{IPW}}, V(\\pi_e), \\widehat{V}_{\\mathrm{IPW}} - V(\\pi_e), \\widehat{V}_{\\mathrm{SNIPW}} - V(\\pi_e), \\widehat{V}_{\\mathrm{CLIP}} - V(\\pi_e)]$。\n- 将四个案例的结果连接成一个单一的扁平列表，每个浮点数四舍五入到6位小数。例如，输出应如下所示：\n```\n[c1_v_ipw,c1_v_snipw,...,c1_bias_clip,c2_v_ipw,...,c4_bias_clip],\n```\n  不含任何附加文本。\n\n科学真实性与基本原理：\n- 使用无意识统计学家法则进行期望转换，并使用重要性采样恒等式推导IPW。\n- 使用中心极限定理对独立项样本均值进行方差估计。\n- 确保所有概率满足 $0  \\epsilon \\le p_b(A \\mid X) \\le 1 - \\epsilon  1$，并且仿真与评估是自洽的。\n\n不涉及角度单位、物理单位和百分比。所有结果都是无量纲的浮点值。您解决方案的正确性将通过指定测试套件的数值输出以及与此处提供的定义和推导的内部一致性来评估。您的最终程序必须无需任何用户输入即可运行，并且必须以指定格式精确生成一行输出。",
            "solution": "该问题要求在一个模拟环境中为确定性目标策略实现并评估多种离策略评估（OPE）估计量，该环境模拟了托卡马克中的破裂缓解决策。解决方案涉及精确实现数据生成过程、OPE估计量和真实值的计算，然后在一个指定的测试套件上分析偏差和方差。\n\n此问题的方法论基础是重要性采样原理，该原理允许使用从一个不同的提议分布中抽取的样本来估计目标分布下的期望值。在OPE的背景下，目标策略 $\\pi_e$ 定义了目标分布，而行为策略 $\\pi_b$ 是记录历史数据时所依据的提议分布。目标策略的价值为 $V(\\pi_e) = \\mathbb{E}_{\\pi_e}[R] = \\mathbb{E}_{A \\sim \\pi_e(\\cdot|X), X \\sim p(X)}[R(X,A)]$。使用重要性采样，这可以重写为对行为策略分布的期望：$V(\\pi_e) = \\mathbb{E}_{A \\sim \\pi_b(\\cdot|X), X \\sim p(X)}\\left[\\frac{\\pi_e(A|X)}{\\pi_b(A|X)}R(X,A)\\right]$。对于确定性目标策略 $\\pi_e(X)$，比率 $\\frac{\\pi_e(A|X)}{\\pi_b(A|X)}$ 简化为 $\\frac{\\mathbb{I}\\{A = \\pi_e(X)\\}}{\\pi_b(A|X)}$，即重要性权重 $w$。逆倾向加权（IPW）估计量 $\\widehat{V}_{\\mathrm{IPW}}$ 是这些重要性加权奖励在记录数据集上的经验均值。\n\n该实现被分为几个部分，全部通过向量化的 `numpy` 操作实现，以提高计算效率。\n\n首先，仿真模型的组件被编码为不同的函数。Sigmoid函数 $\\sigma(z) = 1/(1+e^{-z})$ 使用 `scipy.special.expit` 实现以保证数值稳定性。用于风险得分 $s(X)$、等离子体风险代理 $h(X)$ 和平均奖励 $m(X,A)$ 的函数直接转换了它们各自的数学定义。\n\n其次，一个 `calculate_ground_truth` 函数计算目标策略的“真实”价值 $V(\\pi_e)$。由于数据生成过程是完全已知的，我们可以使用蒙特卡洛积分高精度地近似这个值。这包括从指定的协变量分布 $X \\sim \\mathcal{N}(0, I_2)$ 中抽取大量样本 $X_j$，确定相应的目标策略动作 $A_{e_j} = \\pi_e(X_j) = \\mathbb{I}\\{s(X_j) \\ge t\\}$，为每个样本计算期望奖励 $m(X_j, A_{e_j})$，并对这些值求平均。这作为评判OPE估计量的基准。\n\n第三，主要的仿真和评估逻辑位于 `run_ope_simulation` 函数中。该函数以向量化的方式执行以下步骤：\n1.  生成一个大小为 $n$ 的数据集。协变量 $X_i$ 从 $\\mathcal{N}(0, I_2)$ 中采样。\n2.  根据风险得分 $s(X_i)$、尖锐度参数 $k$ 和倾向性下限 $\\epsilon$ 计算真实的行为倾向性 $\\pi_b(A=1|X_i)$。\n3.  从具有真实倾向性的伯努利分布中采样动作 $A_i$，即 $A_i \\sim \\mathrm{Bernoulli}(\\pi_b(A=1|X_i))$。\n4.  从 $R_i \\sim \\mathrm{Bernoulli}(m(X_i, A_i))$ 中采样奖励 $R_i$。\n5.  确定记录的倾向性 $p_i$。对于真实记录（$\\sigma_{\\log} = 0$），$p_i$ 就是真实倾向性 $\\pi_b(A_i|X_i)$。对于错误记录的情况（$\\sigma_{\\log} > 0$），将一个乘性噪声项 $\\exp(\\varepsilon_i)$（其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\log}^2)$）应用于动作1的倾向性，然后进行裁剪。动作0的记录倾向性取为1减去这个带噪声、裁剪后的动作1的倾向性。估计量中使用的值 $p_i$ 对应于实际采取的动作 $A_i$。\n6.  计算重要性权重 $w_i = \\frac{\\mathbb{I}\\{A_i = \\pi_e(X_i)\\}}{\\pi_b(A_i \\mid X_i)}$，在分母中使用记录的倾向性 $p_i$。\n7.  然后从收集的样本、奖励和权重中计算估计量：\n    -   $\\widehat{V}_{\\mathrm{IPW}} = \\frac{1}{n} \\sum_{i=1}^n w_i R_i$。如果倾向性被正确指定，这是无偏的。\n    -   $\\widehat{V}_{\\mathrm{SNIPW}} = (\\sum_{i=1}^n w_i R_i) / (\\sum_{i=1}^n w_i)$。这个估计量通常是有偏的，但方差往往低于 $\\widehat{V}_{\\mathrm{IPW}}$。\n    -   $\\widehat{V}_{\\mathrm{CLIP}} = \\frac{1}{n} \\sum_{i=1}^n \\min(w_i, \\tau) R_i$。这通过裁剪大的权重引入了偏差，但是一种控制方差的常用启发式方法。\n8.  估计IPW估计量的标准差。根据中心极限定理，估计量 $\\widehat{V}_{\\mathrm{IPW}} = \\bar{Z}$（其中 $Z_i = w_i R_i$）的方差为 $\\mathrm{Var}(\\bar{Z}) \\approx \\mathrm{Var}(Z)/n$。我们使用计算出的 $Z_i$ 值的无偏样本方差（分母为 $n-1$）来估计 $\\mathrm{Var}(Z)$。报告的标准差是这个估计的均值方差的平方根，即 $\\sqrt{\\widehat{\\mathrm{Var}}(Z)/n}$。\n\n最后，一个 `solve` 函数协调整个过程。它遍历问题中定义的四个测试案例。对于每个案例，它首先调用 `calculate_ground_truth`，然后调用 `run_ope_simulation`，使用指定的种子以确保可复现性。然后，它通过从每个估计值中减去真实值来计算三个估计量的偏差。每个案例所需的所有八个浮点数被收集起来，四舍五入到六位小数，并根据问题规范格式化为单个输出字符串。",
            "answer": "```python\nimport numpy as np\nfrom scipy import special\n\ndef solve():\n    \"\"\"\n    Solves the Off-Policy Evaluation problem as specified, calculating various estimators\n    and their statistics across a suite of test cases.\n    \"\"\"\n\n    # The problem specifies a sigmoid function sigma(z) = 1/(1+e^-z), which is equivalent\n    # to scipy.special.expit and is used for numerical stability.\n    sigmoid = special.expit\n\n    # Fixed model parameters as defined in the problem statement\n    W_PARAMS = {'w0': 0.1, 'w1': 1.0, 'w2': -0.8}\n    THETA_PARAMS = {'th0': -0.2, 'th1': 1.0, 'th2': 1.2, 'th12': 0.5}\n    REWARD_PARAMS = {'eta0': 0.2, 'alpha': 3.0, 'gamma': 1.2, 'delta': 0.8}\n\n    # Helper functions for the simulation model, implemented to work on batches of data.\n    def risk_score(X, w_params):\n        \"\"\"Calculates the risk score s(X) for a batch of covariates.\"\"\"\n        return w_params['w0'] + X[:, 0] * w_params['w1'] + X[:, 1] * w_params['w2']\n\n    def plasma_hazard(X, theta_params):\n        \"\"\"Calculates the plasma hazard proxy h(X) for a batch of covariates.\"\"\"\n        arg = theta_params['th0'] + theta_params['th1'] * X[:, 0] + theta_params['th2'] * X[:, 1] + theta_params['th12'] * X[:, 0] * X[:, 1]\n        return sigmoid(arg)\n\n    def mean_reward(X, A, reward_params, theta_params):\n        \"\"\"Calculates the conditional mean reward m(X, A) for a batch.\"\"\"\n        h = plasma_hazard(X, theta_params)\n        arg = reward_params['eta0'] - reward_params['alpha'] * h + reward_params['gamma'] * A + reward_params['delta'] * A * X[:, 1]\n        return sigmoid(arg)\n\n    def behavior_propensity_p1(s, k, epsilon):\n        \"\"\"Calculates the behavior policy propensity for action A=1.\"\"\"\n        p_unclipped = sigmoid(k * s)\n        return np.clip(p_unclipped, epsilon, 1 - epsilon)\n\n    def target_policy_action(s, t):\n        \"\"\"Calculates the deterministic target policy action.\"\"\"\n        return (s >= t).astype(int)\n    \n    def calculate_ground_truth(w_params, theta_params, reward_params, t, N_true, seed):\n        \"\"\"\n        Approximates the true value V(pi_e) via Monte Carlo integration.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.normal(size=(N_true, 2))\n        \n        s = risk_score(X, w_params)\n        A_e = target_policy_action(s, t)\n        \n        m = mean_reward(X, A_e, reward_params, theta_params)\n        \n        return np.mean(m)\n\n    def run_ope_simulation(w_params, theta_params, reward_params, n, k, t, epsilon, sigma_log, tau, seed):\n        \"\"\"\n        Runs the full simulation to generate data and compute OPE estimators.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Generate covariates\n        X = rng.normal(size=(n, 2))\n        \n        # Calculate risk score\n        s = risk_score(X, w_params)\n        \n        # --- Data Generation (Behavior Policy) ---\n        true_p1 = behavior_propensity_p1(s, k, epsilon)\n        A = rng.binomial(1, true_p1)\n        m = mean_reward(X, A, reward_params, theta_params)\n        R = rng.binomial(1, m)\n        \n        # --- Logged Propensities ---\n        if sigma_log > 0:\n            noise = rng.normal(loc=0.0, scale=sigma_log, size=n)\n            noisy_p1_unclipped = true_p1 * np.exp(noise)\n            logged_p1 = np.clip(noisy_p1_unclipped, epsilon, 1 - epsilon)\n        else:\n            logged_p1 = true_p1  # Truthful logging\n            \n        logged_p = np.where(A == 1, logged_p1, 1 - logged_p1)\n        \n        # --- Off-Policy Evaluation ---\n        A_e = target_policy_action(s, t)\n        indicator = (A == A_e).astype(float)\n        w = indicator / logged_p\n\n        # --- Compute Estimators ---\n        Z = w * R\n        V_ipw = np.mean(Z)\n        \n        sum_w = np.sum(w)\n        V_snipw = np.sum(Z) / sum_w if sum_w > 0 else 0.0\n        \n        w_clipped = np.minimum(w, tau)\n        V_clip = np.mean(w_clipped * R)\n        \n        # --- Variance Estimation for IPW ---\n        var_Z = np.var(Z, ddof=1)\n        std_V_ipw = np.sqrt(var_Z / n)\n        \n        return V_ipw, V_snipw, V_clip, std_V_ipw\n\n    # Define the test suite from the problem statement\n    test_cases = [\n        {'n': 50000, 'k': 2.0, 't': 0.0, 'epsilon': 1e-3, 'sigma_log': 0.0, 'tau': 10.0, 'data_seed': 10, 'gt_seed': 101, 'N_true': 300000},\n        {'n': 50000, 'k': 8.0, 't': 0.0, 'epsilon': 1e-4, 'sigma_log': 0.0, 'tau': 10.0, 'data_seed': 20, 'gt_seed': 202, 'N_true': 300000},\n        {'n': 50000, 'k': 2.0, 't': 0.5, 'epsilon': 1e-3, 'sigma_log': 0.25, 'tau': 10.0, 'data_seed': 30, 'gt_seed': 303, 'N_true': 300000},\n        {'n': 80000, 'k': 3.0, 't': 1.5, 'epsilon': 1e-3, 'sigma_log': 0.0, 'tau': 10.0, 'data_seed': 40, 'gt_seed': 404, 'N_true': 300000},\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        V_true = calculate_ground_truth(\n            w_params=W_PARAMS, theta_params=THETA_PARAMS, reward_params=REWARD_PARAMS,\n            t=case['t'], N_true=case['N_true'], seed=case['gt_seed']\n        )\n        \n        V_ipw, V_snipw, V_clip, std_V_ipw = run_ope_simulation(\n            w_params=W_PARAMS, theta_params=THETA_PARAMS, reward_params=REWARD_PARAMS,\n            n=case['n'], k=case['k'], t=case['t'], epsilon=case['epsilon'],\n            sigma_log=case['sigma_log'], tau=case['tau'], seed=case['data_seed']\n        )\n        \n        bias_ipw = V_ipw - V_true\n        bias_snipw = V_snipw - V_true\n        bias_clip = V_clip - V_true\n        \n        case_results = [V_ipw, V_snipw, V_clip, std_V_ipw, V_true, bias_ipw, bias_snipw, bias_clip]\n        all_results.extend(case_results)\n        \n    output_str = ','.join([f'{x:.6f}' for x in all_results])\n    print(f'[{output_str}]')\n\nsolve()\n```"
        }
    ]
}