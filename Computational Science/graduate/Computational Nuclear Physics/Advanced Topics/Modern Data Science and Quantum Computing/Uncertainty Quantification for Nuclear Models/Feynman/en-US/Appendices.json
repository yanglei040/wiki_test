{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of uncertainty quantification is understanding how uncertainty in model parameters propagates to predictions. This exercise introduces Polynomial Chaos Expansion (PCE), a powerful technique for creating a computationally cheap \"surrogate\" of a complex model. By implementing PCE from the ground up, you will gain hands-on experience with projection methods and see how a surrogate can be used to efficiently analyze the statistical properties of a model's output .",
            "id": "3610367",
            "problem": "Construct a complete, runnable program that builds a Polynomial Chaos Expansion (PCE) for a nuclear-model observable as a function of uncertain parameters and computes the expansion coefficients via projection with respect to the prior measure. The context is uncertainty quantification for computational nuclear physics. Begin from foundational definitions: a PCE represents a square-integrable random observable as an expansion in an orthonormal polynomial basis with respect to its prior probability measure, and the coefficients are obtained by projection using the inner product induced by that measure. Assume independent priors for uncertain parameters, each supported on a closed interval, and use the orthonormal polynomial family associated with the uniform probability measure for each parameter. In multiple dimensions, use the tensor-product basis and truncate by total degree. Consistently, treat parameters as random variables on their support intervals and construct a multi-dimensional product measure. Implement coefficient computation by numerically approximating the required projections using tensor-product Gaussian Quadrature (GQ) on the canonical interval. The observable definitions below are physically plausible reductions of the semi-empirical mass formula and are expressed for fixed nuclei. All outputs that represent the observable or its PCE coefficients must be expressed in megalelectronvolt (MeV).\n\nDefine the observable for a fixed nucleus by the following deterministic mapping from the uncertain parameters to the observable:\n- Case definitions use the standard semi-empirical mass model components: a volume energy coefficient, a surface energy coefficient, and a Coulomb energy coefficient, denoted by $a_v$, $a_s$, and $a_c$ respectively. Let $A$ be the mass number and $Z$ be the proton number. Consider the simplified binding energy observable $B$ in megalelectronvolt (MeV) given by $B(a_v,a_s,a_c;A,Z) = a_v A - a_s A^{2/3} - a_c Z^2 A^{-1/3}$, with reduced forms when fewer coefficients are modeled as uncertain.\n\nModeling and basis assumptions:\n- Treat each uncertain parameter as independently uniformly distributed on its specified interval, and use the orthonormal polynomial basis associated with this uniform probability measure for each parameter. Use the total-degree truncation of the multi-dimensional tensor-product basis, i.e., keep all multi-indices whose component-wise degree sum is less than or equal to the specified maximum degree.\n- Implement numerical projection integrals by tensor-product Gaussian Quadrature on the canonical interval, and apply the appropriate linear mapping from parameter supports to the canonical interval. Choose a sufficient number of quadrature nodes per dimension to integrate products of the observable and basis functions up to the required polynomial degree without aliasing. The implementation must ensure correct normalization consistent with the orthonormal basis and the prior probability measure.\n\nTest suite specification and required outputs:\nFor each test case, compute and output the full list of PCE coefficients ordered by increasing total degree and, within each total degree, by lexicographic order of the multi-index (first component varying slowest). The coefficients must be reported in MeV as floating-point numbers.\n\n- Test Case $1$: One-dimensional, fixed nucleus with mass number $A=56$ and no Coulomb term. Observable $B(a_v;A)=a_v A$. Prior for $a_v$: uniform on $[14.5,16.5]$. Maximum total degree $p=3$.\n\n- Test Case $2$: Two-dimensional, fixed nucleus with mass number $A=100$ and no Coulomb term. Observable $B(a_v,a_s;A) = a_v A - a_s A^{2/3}$. Priors: $a_v$ uniform on $[14.0,16.0]$, $a_s$ uniform on $[16.0,18.0]$. Maximum total degree $p=2$.\n\n- Test Case $3$: Three-dimensional, fixed nucleus with mass number $A=208$ and proton number $Z=82$. Observable $B(a_v,a_s,a_c;A,Z) = a_v A - a_s A^{2/3} - a_c Z^2 A^{-1/3}$. Priors: $a_v$ uniform on $[14.0,16.0]$, $a_s$ uniform on $[16.0,18.0]$, $a_c$ uniform on $[0.6,0.8]$. Maximum total degree $p=2$.\n\n- Test Case $4$: Boundary case to test constant-term projection in three dimensions. Same observable and priors as Test Case $3$, with maximum total degree $p=0$.\n\nOutput format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test caseâ€™s result is itself a comma-separated list enclosed in square brackets. For example, if there are $4$ test cases, the final output must look like $[[c_{1,1},c_{1,2},\\dots],[c_{2,1},\\dots],[c_{3,1},\\dots],[c_{4,1},\\dots]]$ without any spaces. Each $c_{i,j}$ is a floating-point number in MeV.\n\nAngle units are not applicable. All numeric outputs are unit-bearing in megalelectronvolt (MeV).",
            "solution": "The task is to construct a Polynomial Chaos Expansion (PCE) for a simplified nuclear binding energy observable and compute the expansion coefficients via projection. The problem is well-defined, scientifically grounded in uncertainty quantification and nuclear physics, and provides a complete specification for its solution. We proceed by first establishing the mathematical framework and then detailing the numerical implementation.\n\n### 1. Mathematical Formulation\n\nA square-integrable random observable $M(\\mathbf{x})$, which is a function of a $d$-dimensional vector of independent random parameters $\\mathbf{x} = (x_1, \\dots, x_d)$, can be represented by a Polynomial Chaos Expansion. Each parameter $x_i$ is assumed to be uniformly distributed on a known interval $[a_i, b_i]$, i.e., $x_i \\sim U(a_i, b_i)$.\n\nTo work with a standard set of basis polynomials, we transform each physical parameter $x_i$ to a canonical random variable $\\xi_i$ that is uniformly distributed on the interval $[-1, 1]$. The corresponding probability density function is $\\rho(\\xi_i) = 1/2$ for $\\xi_i \\in [-1, 1]$. The affine transformation is:\n$$x_i(\\xi_i) = \\frac{b_i-a_i}{2} \\xi_i + \\frac{a_i+b_i}{2}$$\nThe observable $M(\\mathbf{x})$ can now be expressed as a function of the canonical variables, $\\tilde{M}(\\boldsymbol{\\xi}) = M(\\mathbf{x}(\\boldsymbol{\\xi}))$.\n\nThe orthonormal polynomial basis for a random variable uniformly distributed on $[-1, 1]$ consists of scaled Legendre polynomials. The standard Legendre polynomials $P_k(\\xi)$ are orthogonal with respect to the standard weight function $w(\\xi)=1$ on $[-1, 1]$: $\\int_{-1}^1 P_k(\\xi) P_j(\\xi) d\\xi = \\frac{2}{2k+1}\\delta_{kj}$.\nThe inner product induced by our probability measure $\\rho(\\xi)d\\xi$ is $\\langle f, g \\rangle = \\int_{-1}^1 f(\\xi) g(\\xi) \\frac{1}{2} d\\xi$. The correctly-normalized basis polynomials $\\Psi_k(\\xi)$ must satisfy $\\langle \\Psi_k, \\Psi_j \\rangle = \\delta_{kj}$. Setting $\\Psi_k(\\xi) = c_k P_k(\\xi)$, we find:\n$$\\langle \\Psi_k, \\Psi_j \\rangle = c_k c_j \\int_{-1}^1 P_k(\\xi) P_j(\\xi) \\frac{1}{2} d\\xi = \\frac{c_k c_j}{2} \\frac{2}{2k+1} \\delta_{kj} = \\frac{c_k^2}{2k+1} \\delta_{kj}$$\nFor this to equal $\\delta_{kj}$, we require $c_k = \\sqrt{2k+1}$. Thus, the one-dimensional orthonormal basis functions are:\n$$\\Psi_k(\\xi) = \\sqrt{2k+1} P_k(\\xi)$$\n\nFor the $d$-dimensional case with independent parameters, the basis is formed by the tensor product of the one-dimensional bases. A basis function is indexed by a multi-index $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_d) \\in \\mathbb{N}_0^d$:\n$$\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi}) = \\prod_{i=1}^d \\Psi_{\\alpha_i}(\\xi_i) = \\prod_{i=1}^d \\sqrt{2\\alpha_i+1} P_{\\alpha_i}(\\xi_i)$$\nThe PCE is truncated using a total-degree scheme, retaining all basis functions for which the sum of the degrees of the components does not exceed a maximum total degree $p$: $|\\boldsymbol{\\alpha}| = \\sum_{i=1}^d \\alpha_i \\le p$. The truncated PCE for the observable is:\n$$\\tilde{M}(\\boldsymbol{\\xi}) \\approx \\sum_{|\\boldsymbol{\\alpha}| \\le p} c_{\\boldsymbol{\\alpha}} \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$$\n\nThe coefficients $c_{\\boldsymbol{\\alpha}}$ are found by projecting the observable onto each basis function using the inner product induced by the joint probability measure $\\rho(\\boldsymbol{\\xi})d\\boldsymbol{\\xi} = (1/2)^d d\\boldsymbol{\\xi}$ on the hypercube $[-1, 1]^d$:\n$$c_{\\boldsymbol{\\alpha}} = \\langle \\tilde{M}, \\Psi_{\\boldsymbol{\\alpha}} \\rangle = \\int_{[-1, 1]^d} \\tilde{M}(\\boldsymbol{\\xi}) \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi}) \\rho(\\boldsymbol{\\xi}) d\\boldsymbol{\\xi} = \\left(\\frac{1}{2}\\right)^d \\int_{[-1, 1]^d} \\tilde{M}(\\boldsymbol{\\xi}) \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi}) d\\boldsymbol{\\xi}$$\n\n### 2. Numerical Implementation\n\nThe multi-dimensional integral for $c_{\\boldsymbol{\\alpha}}$ is computed numerically using a tensor-product Gaussian Quadrature (GQ) rule. Specifically, we use Gauss-Legendre quadrature. A one-dimensional $N_q$-point Gauss-Legendre rule approximates an integral as $\\int_{-1}^1 f(\\xi) d\\xi \\approx \\sum_{j=1}^{N_q} f(\\xi_j) w_j$, where $\\xi_j$ are the quadrature nodes and $w_j$ are the corresponding weights. The tensor-product rule for $d$ dimensions is:\n$$\\int_{[-1, 1]^d} g(\\boldsymbol{\\xi}) d\\boldsymbol{\\xi} \\approx \\sum_{j_1=1}^{N_q} \\dots \\sum_{j_d=1}^{N_q} g(\\xi_{j_1}, \\dots, \\xi_{j_d}) \\prod_{k=1}^d w_{j_k}$$\nApplying this to the coefficient integral, we get:\n$$c_{\\boldsymbol{\\alpha}} \\approx \\left(\\frac{1}{2}\\right)^d \\sum_{j_1, \\dots, j_d} \\tilde{M}(\\xi_{j_1}, \\dots, \\xi_{j_d}) \\Psi_{\\boldsymbol{\\alpha}}(\\xi_{j_1}, \\dots, \\xi_{j_d}) \\prod_{k=1}^d w_{j_k}$$\n\nTo ensure accuracy, the number of quadrature points $N_q$ must be chosen large enough to exactly integrate the polynomial integrand $\\tilde{M}(\\boldsymbol{\\xi}) \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$. The observable $B$ is a linear function of its parameters $(a_v, a_s, a_c)$. Since the mapping from $\\xi_i$ to $x_i$ is linear, $\\tilde{M}(\\boldsymbol{\\xi})$ is a polynomial of total degree $1$ in $\\boldsymbol{\\xi}$. The basis function $\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$ is a polynomial of total degree $|\\boldsymbol{\\alpha}|$. The highest degree of the integrand in any single dimension $i$ is $1 + \\alpha_i$. Since the maximum value of any $\\alpha_i$ for a total degree $p$ expansion is $p$, the maximum polynomial degree to be integrated in any one dimension is $1+p$. An $N_q$-point Gauss-Legendre rule can integrate polynomials of degree up to $2N_q - 1$ exactly. Therefore, we require $2N_q - 1 \\ge p + 1$, which simplifies to $N_q \\ge (p+2)/2$. For robustness, we select $N_q = p+1$, which satisfies this condition for all non-negative $p$.\n\nThe overall algorithm is as follows:\n1.  For each test case (dimension $d$, max degree $p$, observable $M$, and priors):\n2.  Generate the set of multi-indices $\\boldsymbol{\\alpha}$ such that $|\\boldsymbol{\\alpha}| \\le p$. Sort these indices first by total degree $|\\boldsymbol{\\alpha}|$, then lexicographically.\n3.  Set the number of quadrature points per dimension, $N_q = p + 1$.\n4.  Obtain the $N_q$ one-dimensional Gauss-Legendre nodes and weights.\n5.  Construct the $d$-dimensional grid of quadrature nodes $\\boldsymbol{\\xi}^{(j)}$ and the corresponding product weights $W^{(j)}$.\n6.  For each node $\\boldsymbol{\\xi}^{(j)}$ on the grid, transform it to the physical parameter space $\\mathbf{x}^{(j)}$.\n7.  Evaluate the model observable $M(\\mathbf{x}^{(j)})$ at each physical grid point to obtain a grid of model values.\n8.  For each multi-index $\\boldsymbol{\\alpha}$ in the sorted list:\n    a. Evaluate the basis function $\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi}^{(j)})$ at each canonical grid node.\n    b. Compute the integrand by taking the element-wise product of the model values grid and the basis function values grid.\n    c. Approximate the integral by taking the dot product of the flattened integrand grid with the flattened grid of product weights.\n    d. Calculate the coefficient $c_{\\boldsymbol{\\alpha}}$ by multiplying the integral result by $(0.5)^d$.\n9.  Collect the computed coefficients for each test case and format the output as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import legendre, roots_legendre\nimport itertools\n\ndef solve():\n    \"\"\"\n    Computes Polynomial Chaos Expansion coefficients for a simplified nuclear binding energy model.\n    \"\"\"\n\n    def generate_multi_indices(dim, max_degree):\n        \"\"\"\n        Generates multi-indices for a given dimension and maximum total degree.\n        The indices are sorted by total degree, then lexicographically.\n        \"\"\"\n        if dim == 0:\n            if max_degree >= 0:\n                return [()]\n            else:\n                return []\n        \n        if dim == 1:\n            return [(i,) for i in range(max_degree + 1)]\n\n        indices = []\n        for i in range(max_degree + 1):\n            sub_indices = generate_multi_indices(dim - 1, max_degree - i)\n            for sub_index in sub_indices:\n                indices.append((i,) + sub_index)\n        \n        # Sort by total degree, then lexicographically\n        indices.sort(key=lambda idx: (sum(idx), idx))\n        return indices\n\n    def compute_pce_coefficients(model_func, priors, dim, max_degree):\n        \"\"\"\n        Computes the PCE coefficients for a given model, priors, dimension, and max degree.\n        \"\"\"\n        # Step 1: Generate multi-indices\n        multi_indices = generate_multi_indices(dim, max_degree)\n\n        # Step 2: Determine quadrature rule\n        num_quad_points = max_degree + 1\n        nodes_1d, weights_1d = roots_legendre(num_quad_points)\n\n        # Step 3: Construct multi-dimensional quadrature grid\n        # 'ij' indexing creates grids that can be stacked correctly for our purpose.\n        xi_grids = np.meshgrid(*([nodes_1d] * dim), indexing='ij')\n        w_grids = np.meshgrid(*([weights_1d] * dim), indexing='ij')\n\n        # Create a single grid of product weights\n        product_weights_grid = np.prod(np.stack(w_grids, axis=-1), axis=-1)\n\n        # Step 4: Transform canonical nodes to physical parameter space\n        centers = np.array([(p[0] + p[1]) / 2.0 for p in priors])\n        half_widths = np.array([(p[1] - p[0]) / 2.0 for p in priors])\n        \n        physical_params_grid = []\n        for i in range(dim):\n             physical_params_grid.append(centers[i] + half_widths[i] * xi_grids[i])\n\n        # Step 5: Evaluate the model on the grid\n        model_values_grid = model_func(*physical_params_grid)\n\n        # Step 6: Compute coefficients\n        coefficients = []\n        for alpha in multi_indices:\n            # Evaluate the multi-dimensional basis polynomial on the grid\n            basis_values_grid = np.ones_like(model_values_grid)\n            for i in range(dim):\n                degree = alpha[i]\n                # Orthonormal Legendre polynomials: sqrt(2k+1) * P_k(x)\n                poly = legendre(degree)\n                norm_factor = np.sqrt(2 * degree + 1)\n                basis_values_grid *= (norm_factor * poly(xi_grids[i]))\n            \n            # Form the integrand\n            integrand_grid = model_values_grid * basis_values_grid\n            \n            # Approximate the integral using the quadrature rule\n            integral_val = np.sum(integrand_grid * product_weights_grid)\n            \n            # Final coefficient calculation\n            # c_alpha = (1/2)^d * integral\n            c_alpha = (0.5**dim) * integral_val\n            coefficients.append(c_alpha)\n        \n        return coefficients\n\n    # Define test cases\n    # (observable_lambda, priors_list, dimension, max_pce_degree)\n    test_cases = [\n        (lambda a_v: 56.0 * a_v, [(14.5, 16.5)], 1, 3),\n        (lambda a_v, a_s: 100.0 * a_v - (100.0**(2.0/3.0)) * a_s, \n         [(14.0, 16.0), (16.0, 18.0)], 2, 2),\n        (lambda a_v, a_s, a_c: 208.0 * a_v - (208.0**(2.0/3.0)) * a_s - (82.0**2 * 208.0**(-1.0/3.0)) * a_c,\n         [(14.0, 16.0), (16.0, 18.0), (0.6, 0.8)], 3, 2),\n        (lambda a_v, a_s, a_c: 208.0 * a_v - (208.0**(2.0/3.0)) * a_s - (82.0**2 * 208.0**(-1.0/3.0)) * a_c,\n         [(14.0, 16.0), (16.0, 18.0), (0.6, 0.8)], 3, 0),\n    ]\n\n    all_results = []\n    for model, priors, dim, p_max in test_cases:\n        coeffs = compute_pce_coefficients(model, priors, dim, p_max)\n        all_results.append(coeffs)\n    \n    # Format the final output string\n    result_str = \",\".join([f\"[{','.join([f'{c:.15g}' for c in case_coeffs])}]\" for case_coeffs in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Calibrating a model against experimental data is only half the battle; we must also rigorously validate its predictive power. This practice explores a crucial challenge in nuclear modeling: the presence of systematic errors that simple validation schemes can miss, leading to over-optimistic performance estimates. By comparing two different cross-validation strategies, you will see how a physically-informed approach is essential for robustly assessing model performance and avoiding overfitting .",
            "id": "3610428",
            "problem": "You are tasked with writing a complete and runnable program that compares two cross-validation schemes for calibrating a linear energy density functional (EDF)-like surrogate model used to predict nuclear binding energy residuals. Your program must estimate predictive risk and detect potential overfitting differences between leave-one-nucleus-out and leave-one-isotopic-chain-out schemes.\n\nThe model and data generation are as follows.\n\n1. Fundamental base and modeling assumptions:\n   - Assume that the observable is a binding-energy residual per nucleus, denoted by $y$ and measured in megaelectronvolts (MeV). Assume a linear surrogate model with a polynomial feature map up to second order in proton number $Z$ and neutron number $N$. For a given nucleus $(Z, N)$, define the feature vector\n     $$\\phi(Z,N) = \\begin{bmatrix} 1 & Z & N & Z^2 & N^2 & ZN \\end{bmatrix}^{\\top}.$$\n   - Assume additive independent Gaussian noise with zero mean and standard deviation $\\sigma$ measured in MeV, that is, $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$.\n   - The data are generated from a fixed but unknown linear parameter vector $\\beta^{\\star} \\in \\mathbb{R}^6$, plus an unmodeled isotopic-chain offset $g(Z)$ that depends only on $Z$, and an independent noise $\\epsilon$. That is,\n     $$ y = \\phi(Z,N)^{\\top} \\beta^{\\star} + g(Z) + \\epsilon.$$\n   - The calibration uses ridge-regularized least squares with a Gaussian prior interpretation on the coefficients, where the intercept (the first component corresponding to the constant term) is not penalized. For a given regularization strength $\\lambda \\ge 0$, the estimator $\\hat{\\beta}_{\\lambda}$ minimizes\n     $$ \\sum_{i=1}^{n} \\left(y_i - \\phi(Z_i,N_i)^{\\top}\\beta\\right)^2 + \\lambda \\sum_{j=2}^{6} \\beta_j^2.$$\n\n2. Cross-validation schemes and predictive risk:\n   - Leave-one-nucleus-out cross-validation (LONO-CV): For each data point $i$, fit $\\hat{\\beta}_{\\lambda}^{(-i)}$ on the $n-1$ other data points and compute the squared prediction error on the held-out point $i$. The LONO-CV predictive risk for $\\lambda$ is the mean of these squared prediction errors over all $n$ hold-outs.\n   - Leave-one-isotopic-chain-out cross-validation (LOICO-CV): For each distinct isotopic chain identified by proton number $Z$, fit on all nuclei not in that chain, then predict the held-out chain and accumulate the squared prediction errors for all nuclei in that chain. The LOICO-CV predictive risk for $\\lambda$ is the mean of all squared prediction errors across all held-out chains. An isotopic chain is defined here as the set of nuclei with the same proton number $Z$ and varying neutron number $N$.\n\n3. Regularization grid and selection:\n   - Consider the discrete grid of regularization strengths\n     $$ \\Lambda = \\{\\, 0,\\, 10^{-6},\\, 10^{-3},\\, 10^{-1},\\, 1,\\, 10 \\,\\}. $$\n   - For each cross-validation scheme, select the $\\lambda \\in \\Lambda$ that minimizes the corresponding predictive risk. In case of ties within an absolute tolerance of $10^{-12}$ in the risk value, choose the smallest $\\lambda$ among those tied.\n\n4. Overfitting detection criterion:\n   - Define an overfitting detection indicator as a boolean computed as\n     $$ \\text{overfit\\_detected} = \\left( \\lambda_{\\text{LONO}}^{\\star} = 0 \\right) \\wedge \\left( \\lambda_{\\text{LOICO}}^{\\star} > 0 \\right), $$\n     where $\\lambda_{\\text{LONO}}^{\\star}$ and $\\lambda_{\\text{LOICO}}^{\\star}$ are the optimal choices under LONO-CV and LOICO-CV respectively.\n\n5. Data set, true parameters, and isotopic-chain offsets:\n   - Use the following set of nuclei $(Z,N)$ grouped into isotopic chains by $Z$:\n     - $Z=20$: $N \\in \\{\\, 20,\\, 22,\\, 24,\\, 26 \\,\\}$,\n     - $Z=28$: $N \\in \\{\\, 28,\\, 30,\\, 32,\\, 34 \\,\\}$,\n     - $Z=50$: $N \\in \\{\\, 64,\\, 66,\\, 68,\\, 70 \\,\\}$,\n     - $Z=82$: $N \\in \\{\\, 120,\\, 122,\\, 124,\\, 126 \\,\\}$.\n   - Use the true parameter vector\n     $$ \\beta^{\\star} = \\begin{bmatrix} 5.0 & -0.08 & -0.10 & 5.0\\times 10^{-4} & 3.0\\times 10^{-4} & 2.0\\times 10^{-4} \\end{bmatrix}^{\\top} \\text{ MeV}, $$\n     and the isotopic-chain offsets\n     $$ g(20)=1.2 \\text{ MeV},\\quad g(28)=-1.0 \\text{ MeV},\\quad g(50)=0.5 \\text{ MeV},\\quad g(82)=-2.0 \\text{ MeV}.$$\n\n6. Test suite:\n   - You must evaluate three distinct test cases that only differ in the noise level $\\sigma$ and the random seed used to generate the Gaussian noise. For each test case $t \\in \\{1,2,3\\}$, generate the outputs $y$ by drawing independent $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ with the specified seed for reproducibility. Use:\n     - Case $1$: $\\sigma = 0.20$ MeV, random seed $123$,\n     - Case $2$: $\\sigma = 0.00$ MeV, random seed $456$,\n     - Case $3$: $\\sigma = 2.00$ MeV, random seed $789$.\n\n7. Required computations and outputs per test case:\n   - For each test case, compute:\n     - the optimal LONO-CV predictive risk (mean squared prediction error) in megaelectronvolt squared, denoted $R_{\\text{LONO}}^{\\star}$,\n     - the optimal LOICO-CV predictive risk (mean squared prediction error) in megaelectronvolt squared, denoted $R_{\\text{LOICO}}^{\\star}$,\n     - the corresponding optimal regularization strengths $\\lambda_{\\text{LONO}}^{\\star}$ and $\\lambda_{\\text{LOICO}}^{\\star}$ from the grid $\\Lambda$,\n     - the overfitting detection boolean as defined above.\n   - All risks must be expressed as real numbers in megaelectronvolt squared (MeV$^2$). All regularization strengths must be expressed as real numbers without units. The boolean must be either the literal True or False.\n\n8. Final output format:\n   - Your program should produce a single line of output containing the results of all three test cases as a comma-separated list of lists, with no spaces, enclosed in a single pair of square brackets. For each test case, output the list\n     $$ \\left[ R_{\\text{LONO}}^{\\star},\\ R_{\\text{LOICO}}^{\\star},\\ \\lambda_{\\text{LONO}}^{\\star},\\ \\lambda_{\\text{LOICO}}^{\\star},\\ \\text{overfit\\_detected} \\right]. $$\n   - The final output should therefore look like\n     $$ \\big[ [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot] \\big], $$\n     with every numeric entry printed as a standard floating-point literal and the boolean printed as either True or False, and with no spaces anywhere in the line.\n\nYour solution must start from the fundamental assumptions above and implement the two cross-validation schemes precisely as defined. No external inputs are allowed; all constants and data are specified here. The program must be deterministic given the random seeds. The entirety of the logic must be embedded in the program, and it must produce exactly one line as specified.",
            "solution": "The user has provided a computational problem in the domain of nuclear physics, specifically focusing on uncertainty quantification for energy density functional (EDF) surrogate models. The task is to compare two cross-validation (CV) schemes, leave-one-nucleus-out (LONO-CV) and leave-one-isotopic-chain-out (LOICO-CV), for calibrating a linear model and detecting potential overfitting.\n\n### Step 1: Extract Givens\n- **Observable**: Binding-energy residual per nucleus, $y$, in units of megaelectronvolts (MeV).\n- **Surrogate Model**: A linear model with a polynomial feature map.\n- **Feature Vector**: For a nucleus $(Z,N)$, $\\phi(Z,N) = \\begin{bmatrix} 1 & Z & N & Z^2 & N^2 & ZN \\end{bmatrix}^{\\top}$.\n- **Data Generation Model**: $y = \\phi(Z,N)^{\\top} \\beta^{\\star} + g(Z) + \\epsilon$.\n- **Noise Model**: $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$, independent and identically distributed Gaussian noise.\n- **True Parameters**: $\\beta^{\\star} = \\begin{bmatrix} 5.0 & -0.08 & -0.10 & 5.0\\times 10^{-4} & 3.0\\times 10^{-4} & 2.0\\times 10^{-4} \\end{bmatrix}^{\\top}$ MeV.\n- **Isotopic-Chain Offsets**: $g(20)=1.2$ MeV, $g(28)=-1.0$ MeV, $g(50)=0.5$ MeV, $g(82)=-2.0$ MeV.\n- **Calibration Method**: Ridge-regularized least squares minimizing $\\sum_{i=1}^{n} (y_i - \\phi(Z_i,N_i)^{\\top}\\beta)^2 + \\lambda \\sum_{j=2}^{6} \\beta_j^2$. The intercept term $\\beta_1$ is not penalized.\n- **Cross-Validation Schemes**:\n    - **LONO-CV**: Leave-one-nucleus-out cross-validation. Risk is the mean of squared prediction errors over all $n$ hold-outs.\n    - **LOICO-CV**: Leave-one-isotopic-chain-out cross-validation. An isotopic chain is the set of nuclei with the same $Z$. Risk is the mean of squared prediction errors across all held-out chains.\n- **Regularization Grid**: $\\Lambda = \\{\\, 0,\\, 10^{-6},\\, 10^{-3},\\, 10^{-1},\\, 1,\\, 10 \\,\\}$.\n- **Optimal $\\lambda$ Selection**: Choose $\\lambda \\in \\Lambda$ that minimizes the CV risk. In case of a tie (risk values within an absolute tolerance of $10^{-12}$), choose the smallest $\\lambda$.\n- **Overfitting Indicator**: $\\text{overfit\\_detected} = (\\lambda_{\\text{LONO}}^{\\star} = 0) \\wedge (\\lambda_{\\text{LOICO}}^{\\star} > 0)$.\n- **Dataset**:\n    - $Z=20$: $N \\in \\{20, 22, 24, 26\\}$\n    - $Z=28$: $N \\in \\{28, 30, 32, 34\\}$\n    - $Z=50$: $N \\in \\{64, 66, 68, 70\\}$\n    - $Z=82$: $N \\in \\{120, 122, 124, 126\\}$\n- **Test Cases**:\n    - Case 1: $\\sigma = 0.20$ MeV, random seed $123$.\n    - Case 2: $\\sigma = 0.00$ MeV, random seed $456$.\n    - Case 3: $\\sigma = 2.00$ MeV, random seed $789$.\n- **Required Output**: For each case, a list $[R_{\\text{LONO}}^{\\star}, R_{\\text{LOICO}}^{\\star}, \\lambda_{\\text{LONO}}^{\\star}, \\lambda_{\\text{LOICO}}^{\\star}, \\text{overfit\\_detected}]$, where $R$ are risks in MeV$^2$.\n- **Final Format**: A single line `[[...],[...],[...]]` with no spaces.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined computational task in statistical modeling and machine learning, set within a nuclear physics context.\n- **Scientifically Grounded**: The problem uses concepts like energy density functionals, surrogate modeling, and cross-validation, which are standard in computational physics. The model is a specified simplification for a computational exercise, not a claim about fundamental physics. This is scientifically sound as a modeling study.\n- **Well-Posed**: All components are specified: the data-generating process, the model to be fitted, the optimization objective (ridge regression with an unpenalized intercept), the cross-validation procedures, the hyperparameter grid, and the evaluation metrics. The use of random seeds ensures reproducibility. The problem admits a unique and deterministically computable solution.\n- **Objective**: The problem is stated with precise mathematical and algorithmic definitions, free of ambiguity or subjective language.\n- **No Flaws**: The problem does not violate any of the invalidity criteria. It is self-contained, consistent, numerically feasible, and clearly structured. All parameters, constants, and data are provided.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Solution Design\n\nThe core of the problem is to implement and compare two cross-validation strategies for a ridge-regularized linear model. The key distinction lies in the unmodeled systematic error term, $g(Z)$, which introduces correlations among data points within the same isotopic chain (same $Z$). The two CV schemes will handle this correlation differently.\n\n**1. Data Generation and Model Setup**\nFirst, we construct the dataset of $n=16$ nuclei $(Z,N)$ and the corresponding design matrix $\\mathbf{X}$, where each row is the feature vector $\\phi(Z,N)^{\\top}$. The response vector $\\mathbf{y}$ is generated for each test case according to the model $y_i = \\phi(Z_i, N_i)^{\\top} \\beta^{\\star} + g(Z_i) + \\epsilon_i$, using the specified true parameters $\\beta^{\\star}$, isotopic offsets $g(Z)$, noise level $\\sigma$, and random seed.\n\n**2. Ridge Regression Solver**\nThe calibration requires solving a ridge regression problem where the intercept, $\\beta_1$, is not regularized. The objective function to minimize with respect to $\\beta \\in \\mathbb{R}^6$ is:\n$$ L(\\beta) = \\|\\mathbf{y} - \\mathbf{X}\\beta\\|_2^2 + \\lambda \\|\\beta_{2:6}\\|_2^2 = (\\mathbf{y} - \\mathbf{X}\\beta)^{\\top}(\\mathbf{y} - \\mathbf{X}\\beta) + \\beta^{\\top}\\mathbf{P}\\beta $$\nwhere $\\mathbf{P}$ is a diagonal penalty matrix $\\mathbf{P} = \\lambda \\cdot \\text{diag}(0, 1, 1, 1, 1, 1)$.\nTaking the gradient with respect to $\\beta$ and setting it to zero gives the normal equations:\n$$ (\\mathbf{X}^{\\top}\\mathbf{X} + \\mathbf{P})\\beta = \\mathbf{X}^{\\top}\\mathbf{y} $$\nThis linear system is solved for $\\hat{\\beta}_{\\lambda}$ for each training fold and regularization strength $\\lambda$. We use a robust linear algebra solver for this purpose.\n\n**3. Cross-Validation Schemes**\nThe goal is to estimate the model's predictive risk for different values of $\\lambda$ from the grid $\\Lambda$.\n\n- **Leave-One-Nucleus-Out (LONO-CV)**: This is the standard leave-one-out cross-validation. For each of the $n$ data points, we train the model on the other $n-1$ points and calculate the squared prediction error on the single held-out point. The LONO-CV risk for a given $\\lambda$ is the average of these $n$ squared errors.\n$$ R_{\\text{LONO}}(\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\phi(Z_i, N_i)^{\\top}\\hat{\\beta}_{\\lambda}^{(-i)})^2 $$\nwhere $\\hat{\\beta}_{\\lambda}^{(-i)}$ is the parameter vector estimated without data point $i$. Because the training set for each fold almost certainly contains other nuclei from the same isotopic chain as the held-out point, this scheme allows the model to \"learn\" the systematic offset $g(Z)$ for that chain. This can lead to an overly optimistic estimate of the true generalization error, potentially favoring an unregularized, overfitted model ($\\lambda=0$).\n\n- **Leave-One-Isotopic-Chain-Out (LOICO-CV)**: This scheme is designed to account for the data correlation structure. Instead of holding out one nucleus, we hold out all nuclei belonging to the same isotopic chain (i.e., all nuclei with the same $Z$). The model is trained on the remaining chains and tested on the entire held-out chain. The process is repeated for each isotopic chain. The LOICO-CV risk for a given $\\lambda$ is the average of the squared errors over all $n$ nuclei.\n$$ R_{\\text{LOICO}}(\\lambda) = \\frac{1}{n} \\sum_{Z_k \\in \\text{Chains}} \\sum_{i \\in \\text{Chain } Z_k} (y_i - \\phi(Z_i, N_i)^{\\top}\\hat{\\beta}_{\\lambda}^{(-Z_k)})^2 $$\nwhere $\\hat{\\beta}_{\\lambda}^{(-Z_k)}$ is estimated without any data from the chain with proton number $Z_k$. Since the model is tested on a chain for which the systematic offset $g(Z_k)$ is completely unseen during training, this provides a more realistic estimate of the model's ability to extrapolate to new physical regions (i.e., new proton numbers). This will likely reveal the model's misspecification and favor a non-zero $\\lambda$ to prevent overfitting to the specific offsets present in the training data.\n\n**4. Optimal Parameter Selection and Overfitting Detection**\nFor each CV scheme, we compute the risk for every $\\lambda \\in \\Lambda$. We then identify the minimum risk and select the corresponding $\\lambda$ as the optimal one ($\\lambda^{\\star}$). The problem specifies a tie-breaking rule: if multiple $\\lambda$ values yield a risk within $10^{-12}$ of the minimum, the smallest $\\lambda$ among them is chosen. The risk associated with this chosen $\\lambda^{\\star}$ is the optimal risk $R^{\\star}$.\n\nFinally, the overfitting indicator is computed as the logical AND of two conditions: the optimal $\\lambda$ from LONO-CV is zero, and the optimal $\\lambda$ from LOICO-CV is greater than zero. This condition formally captures the scenario where the naive CV scheme (LONO) suggests no regularization is needed, while the more robust scheme (LOICO) correctly identifies the need for regularization to improve generalization, a classic sign of overfitting due to unmodeled systematic errors.\n\nThe entire procedure is repeated for the three test cases, varying the noise level $\\sigma$ and the random seed.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that runs the three test cases and prints the final result.\n    \"\"\"\n\n    def phi(Z, N):\n        \"\"\"Constructs the feature vector for a given nucleus (Z, N).\"\"\"\n        return np.array([1, Z, N, Z**2, N**2, Z*N], dtype=float)\n\n    def solve_ridge(X_train, y_train, lambda_reg):\n        \"\"\"\n        Solves ridge regression with a non-penalized intercept.\n        The objective is: ||y - X*beta||^2 + lambda * ||beta[1:]||^2\n        \"\"\"\n        num_features = X_train.shape[1]\n        # Create the penalty matrix P = lambda * diag(0, 1, 1, ...)\n        P = np.diag([0.0] + [lambda_reg] * (num_features - 1))\n        \n        # Solve the normal equations: (X.T*X + P)*beta = X.T*y\n        A = X_train.T @ X_train + P\n        b = X_train.T @ y_train\n        \n        try:\n            beta_hat = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if solve fails (for singular matrices. e.g. when lambda=0 and X is not full rank)\n            # This is generally not expected here but is good practice.\n            A_inv = np.linalg.pinv(A)\n            beta_hat = A_inv @ b\n            \n        return beta_hat\n\n    def select_optimal_lambda(lambdas, risks):\n        \"\"\"\n        Selects the optimal lambda according to the problem's criteria.\n        Chooses the smallest lambda in case of a tie in risk values.\n        \"\"\"\n        tolerance = 1e-12\n        min_risk_val = np.min(risks)\n        \n        # Find all lambdas whose risk is within tolerance of the minimum\n        candidate_lambdas = []\n        for lam, risk in zip(lambdas, risks):\n            if abs(risk - min_risk_val) <= tolerance:\n                candidate_lambdas.append(lam)\n        \n        # From the candidates, choose the smallest lambda\n        optimal_lambda = min(candidate_lambdas)\n        \n        # Find the index of this optimal lambda to get its corresponding risk\n        optimal_lambda_index = lambdas.index(optimal_lambda)\n        optimal_risk = risks[optimal_lambda_index]\n        \n        return optimal_risk, optimal_lambda\n\n    def run_case(sigma, seed):\n        \"\"\"\n        Executes the full analysis for a single test case (sigma, seed).\n        \"\"\"\n        # 1. Define constants, data structures, and nuclei\n        nuclei_chains = {\n            20: [20, 22, 24, 26],\n            28: [28, 30, 32, 34],\n            50: [64, 66, 68, 70],\n            82: [120, 122, 124, 126]\n        }\n        nuclei = []\n        for z, ns in nuclei_chains.items():\n            for n in ns:\n                nuclei.append((z, n))\n        \n        num_nuclei = len(nuclei)\n\n        beta_star = np.array([5.0, -0.08, -0.10, 5.0e-4, 3.0e-4, 2.0e-4])\n        g_offsets = {20: 1.2, 28: -1.0, 50: 0.5, 82: -2.0}\n        lambdas = [0.0, 1e-6, 1e-3, 1e-1, 1.0, 10.0]\n\n        # 2. Generate data based on the model\n        rng = np.random.default_rng(seed)\n        X = np.array([phi(z, n) for z, n in nuclei])\n        y = np.zeros(num_nuclei)\n        for i, (z, n) in enumerate(nuclei):\n            true_model_val = X[i, :] @ beta_star\n            chain_offset = g_offsets[z]\n            noise = rng.normal(0, sigma) if sigma > 0 else 0.0\n            y[i] = true_model_val + chain_offset + noise\n\n        # 3. Leave-One-Nucleus-Out Cross-Validation (LONO-CV)\n        lono_risks = []\n        for lam in lambdas:\n            squared_errors = []\n            for i in range(num_nuclei):\n                X_train = np.delete(X, i, axis=0)\n                y_train = np.delete(y, i)\n                X_test_row = X[i, :]\n                y_test_val = y[i]\n                \n                beta_hat = solve_ridge(X_train, y_train, lam)\n                \n                y_pred = X_test_row @ beta_hat\n                squared_errors.append((y_pred - y_test_val)**2)\n            lono_risks.append(np.mean(squared_errors))\n        \n        R_lono_star, lambda_lono_star = select_optimal_lambda(lambdas, lono_risks)\n\n        # 4. Leave-One-Isotopic-Chain-Out Cross-Validation (LOICO-CV)\n        unique_Zs = sorted(nuclei_chains.keys())\n        loico_risks = []\n        for lam in lambdas:\n            all_fold_errors = []\n            for z_out in unique_Zs:\n                test_indices = [i for i, (z, n) in enumerate(nuclei) if z == z_out]\n                train_indices = [i for i, (z, n) in enumerate(nuclei) if z != z_out]\n                \n                X_train, y_train = X[train_indices], y[train_indices]\n                X_test, y_test = X[test_indices], y[test_indices]\n                \n                beta_hat = solve_ridge(X_train, y_train, lam)\n                \n                y_pred = X_test @ beta_hat\n                all_fold_errors.extend((y_pred - y_test)**2)\n            loico_risks.append(np.mean(all_fold_errors))\n            \n        R_loico_star, lambda_loico_star = select_optimal_lambda(lambdas, loico_risks)\n        \n        # 5. Overfitting Detection\n        overfit_detected = (lambda_lono_star == 0.0) and (lambda_loico_star > 0.0)\n        \n        return [R_lono_star, R_loico_star, lambda_lono_star, lambda_loico_star, overfit_detected]\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        (0.20, 123),  # Case 1\n        (0.00, 456),  # Case 2\n        (2.00, 789),  # Case 3\n    ]\n\n    results = []\n    for sigma, seed in test_cases:\n        case_result = run_case(sigma, seed)\n        results.append(case_result)\n\n    # Format the final output string as specified\n    formatted_results = []\n    for res_list in results:\n        str_list = list(map(str, res_list))\n        formatted_results.append(f\"[{','.join(str_list)}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate goal of uncertainty quantification is often to guide future research by identifying the most informative experiments to perform next. This exercise puts you in the role of a decision-maker, using Bayesian decision theory to select the next measurement that optimally balances expected scientific gain against practical constraints like experimental cost and risk. You will implement a utility function that formalizes this trade-off, providing a powerful framework for closing the loop between theory, experiment, and uncertainty .",
            "id": "3610437",
            "problem": "You are tasked with writing a complete, runnable program that implements a one-step Bayesian decision rule to choose the next mass measurement that maximizes the expected reduction in predictive loss for the two-neutron separation energy, denoted $S_{2n}$, in a simplified Gaussian-process surrogate model under squared error loss. The decision utility must include a measurement cost penalty and a risk-budget penalty. You must produce decisions for a fixed set of test cases. All mathematical objects and scalars must be interpreted as real-valued, and all numerical values provided are to be treated as exact input parameters without unit conversion.\n\nFundamental base. Use the following principles.\n\n- Bayesian linear-Gaussian update for a multivariate normal model: If a latent vector $y \\in \\mathbb{R}^M$ has prior $y \\sim \\mathcal{N}(\\mu, K)$ and we observe a single component $y_i$ with independent Gaussian noise variance $\\tau_i^2$, then the posterior covariance of any subvector $y_T$ for an index set $T$ is\n$$\n\\Sigma_{T \\mid i} \\;=\\; K_{TT} \\;-\\; K_{T i} \\,\\frac{1}{K_{ii} + \\tau_i^2}\\, K_{iT},\n$$\nwhich is independent of the observed value due to conjugacy.\n- Bayes estimator under squared error loss: For a target subvector $y_T$ and any deterministic predictor $a \\in \\mathbb{R}^{|T|}$, the expected weighted squared error with a symmetric positive semidefinite weight matrix $W \\in \\mathbb{R}^{|T| \\times |T|}$ is\n$$\n\\mathbb{E}\\left[(y_T - a)^\\top W (y_T - a)\\right] \\;=\\; \\operatorname{tr}\\left(W \\,\\operatorname{Cov}(y_T)\\right) \\;+\\; (\\mathbb{E}[y_T] - a)^\\top W (\\mathbb{E}[y_T] - a).\n$$\nThe Bayes-optimal choice is $a = \\mathbb{E}[y_T]$, yielding Bayes risk equal to $\\operatorname{tr}\\left(W \\,\\operatorname{Cov}(y_T)\\right)$.\n- Decision-theoretic utility: With a single prospective measurement at index $i$, define the expected loss reduction as\n$$\n\\Delta \\mathcal{R}(i) \\;=\\; \\operatorname{tr}\\!\\left(W\\,K_{TT}\\right) \\;-\\; \\operatorname{tr}\\!\\left(W\\,\\Sigma_{T \\mid i}\\right).\n$$\nDefine a measurement cost $c_i \\ge 0$ and a risk measure\n$$\n\\mathcal{Q}(i) \\;=\\; \\max_{j \\in T} \\left[\\Sigma_{T \\mid i}\\right]_{jj},\n$$\nwith risk budget $R_{\\max} \\ge 0$. For nonnegative penalty multipliers $\\lambda_{\\text{cost}}$ and $\\lambda_{\\text{risk}}$, the utility is\n$$\nU(i) \\;=\\; \\Delta \\mathcal{R}(i) \\;-\\; \\lambda_{\\text{cost}}\\,c_i \\;-\\; \\lambda_{\\text{risk}}\\,\\max\\!\\left(0,\\,\\mathcal{Q}(i) - R_{\\max}\\right).\n$$\nYou must also support an abstain choice, denoted by the special index $-1$, meaning no measurement is taken. The abstain utility is defined to be $U(-1) = 0$.\n- Selection rule and tie-breaking: For each test case, select the index $i^\\star \\in \\{-1,0,1,\\dots,M-1\\}$ that maximizes $U(i)$. If all $U(i)$ for $i \\in \\{0,1,\\dots,M-1\\}$ are strictly less than $0$, then select $-1$. In case of a tie among measurement indices with the same maximal utility, choose the smallest index.\n\nModel specification. Consider a discretized chain of $M$ candidate nuclei represented by an index grid $x = [0,1,2,3,4,5]$ of length $M = 6$. The latent vector $y$ collects the values of $S_{2n}$ on this grid. Use a zero-mean Gaussian Process (GP) prior with squared-exponential kernel\n$$\nK_{ij} \\;=\\; \\sigma_f^2 \\exp\\!\\left(-\\frac{(x_i - x_j)^2}{2\\ell^2}\\right) \\;+\\; \\sigma_{\\text{nug}}^2 \\,\\delta_{ij},\n$$\nfor given amplitude $\\sigma_f > 0$, length scale $\\ell > 0$, and a small nugget $\\sigma_{\\text{nug}}^2 > 0$ ensuring numerical stability. There is no existing data to condition on before the next measurement. The observation noise variance at index $i$ is $\\tau_i^2 > 0$. The target set for prediction is $T = [2,3,4]$ and the weight matrix $W$ is diagonal with entries given per test case. All energies are to be treated in mega-electronvolts (MeV), while the loss has units of $\\text{MeV}^2$. The final output, being a list of indices, is unitless.\n\nYour program must implement the above utility and selection rule and produce results for the following test suite. For all cases, use the same grid $x = [0,1,2,3,4,5]$ and target set $T = [2,3,4]$.\n\n- Test case $1$ (happy path, moderate correlation and uniform low cost):\n  - Kernel hyperparameters: $\\sigma_f = 1.5$, $\\ell = 1.0$, $\\sigma_{\\text{nug}}^2 = 10^{-8}$.\n  - Observation noise: $\\tau^2 = [0.05, 0.05, 0.05, 0.05, 0.05, 0.05]$.\n  - Weights: $W = \\operatorname{diag}([1.0, 1.0, 2.0])$.\n  - Costs: $c = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]$.\n  - Penalties: $\\lambda_{\\text{cost}} = 0.05$, $\\lambda_{\\text{risk}} = 0.0$.\n  - Risk budget: $R_{\\max} = 10.0$.\n- Test case $2$ (cost-dominated for a central candidate):\n  - Kernel hyperparameters: $\\sigma_f = 1.5$, $\\ell = 1.0$, $\\sigma_{\\text{nug}}^2 = 10^{-8}$.\n  - Observation noise: $\\tau^2 = [0.05, 0.05, 0.05, 0.05, 0.05, 0.05]$.\n  - Weights: $W = \\operatorname{diag}([1.0, 1.0, 2.0])$.\n  - Costs: $c = [0.1, 0.1, 0.1, 20.0, 0.1, 0.1]$.\n  - Penalties: $\\lambda_{\\text{cost}} = 0.05$, $\\lambda_{\\text{risk}} = 0.0$.\n  - Risk budget: $R_{\\max} = 10.0$.\n- Test case $3$ (tight risk budget with short correlation length):\n  - Kernel hyperparameters: $\\sigma_f = 1.0$, $\\ell = 0.3$, $\\sigma_{\\text{nug}}^2 = 10^{-8}$.\n  - Observation noise: $\\tau^2 = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01]$.\n  - Weights: $W = \\operatorname{diag}([1.0, 1.0, 2.0])$.\n  - Costs: $c = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$.\n  - Penalties: $\\lambda_{\\text{cost}} = 0.0$, $\\lambda_{\\text{risk}} = 5.0$.\n  - Risk budget: $R_{\\max} = 0.7$.\n- Test case $4$ (high-noise, cost-dominated, prefer abstain):\n  - Kernel hyperparameters: $\\sigma_f = 1.0$, $\\ell = 1.0$, $\\sigma_{\\text{nug}}^2 = 10^{-8}$.\n  - Observation noise: $\\tau^2 = [1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0]$.\n  - Weights: $W = \\operatorname{diag}([1.0, 1.0, 1.0])$.\n  - Costs: $c = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]$.\n  - Penalties: $\\lambda_{\\text{cost}} = 0.1$, $\\lambda_{\\text{risk}} = 0.0$.\n  - Risk budget: $R_{\\max} = 10.0$.\n\nImplementation requirements.\n\n- Your code must compute, for each test case, the index $i^\\star$ that maximizes $U(i)$ over $i \\in \\{0,1,2,3,4,5\\}$ with the abstain rule defined above. Use the smallest-index tie-breaking rule.\n- Numerical stability: When inverting the scalar $K_{ii} + \\tau_i^2$, treat it as an exact scalar division and do not perform any matrix inversion.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets (e.g., $[3,2,4,-1]$), where each entry is the selected index for the corresponding test case, and $-1$ denotes abstain. No other text should be printed.",
            "solution": "The problem is scientifically and mathematically well-posed, providing a complete and consistent set of definitions, parameters, and objectives. It is a standard problem in the field of Bayesian optimal experimental design, applied to a stylized nuclear physics context. All provided formulas are correct and all numerical values are specified, allowing for a unique and verifiable solution. We may therefore proceed with the derivation and implementation.\n\nThe primary objective is to select a measurement index $i^\\star$ from the set of candidate nuclei, indexed by $\\{0, 1, \\dots, M-1\\}$, or to abstain from measurement, denoted by index $-1$. The selection must maximize the utility function $U(i)$, defined as:\n$$\nU(i) \\;=\\; \\Delta \\mathcal{R}(i) \\;-\\; \\lambda_{\\text{cost}}\\,c_i \\;-\\; \\lambda_{\\text{risk}}\\,\\max\\!\\left(0,\\,\\mathcal{Q}(i) - R_{\\max}\\right)\n$$\nfor $i \\in \\{0, 1, \\dots, M-1\\}$, and $U(-1) = 0$. The selection rule is to choose $i^\\star = \\operatorname{argmax}_{i \\in \\{-1, 0, \\dots, M-1\\}} U(i)$, with a tie-breaking rule to select the smallest positive index.\n\nThe calculation of $U(i)$ for a given measurement candidate $i$ involves three components: the expected reduction in predictive loss, $\\Delta \\mathcal{R}(i)$; a penalty for the measurement cost, $\\lambda_{\\text cost} c_i$; and a penalty for exceeding a risk budget, $\\lambda_{\\text risk} \\max(0, \\mathcal{Q}(i) - R_{\\max})$. We will detail the calculation of each component.\n\nThe model is a zero-mean Gaussian Process, so the prior distribution of the latent vector $y \\in \\mathbb{R}^M$ is $\\mathcal{N}(0, K)$, where $K$ is the prior covariance matrix. The problem states there is no existing data, so the predictive covariance for any subvector $y_T$ is simply its prior covariance, $K_{TT}$. The Bayes risk before any measurement is made is $\\mathcal{R}_{\\text{prior}} = \\operatorname{tr}(W K_{TT})$. After a measurement at index $i$, the posterior covariance of $y_T$ is given by $\\Sigma_{T \\mid i}$. The posterior risk is $\\mathcal{R}_{\\text{post}}(i) = \\operatorname{tr}(W \\Sigma_{T \\mid i})$.\n\n1.  **Expected Loss Reduction $\\Delta \\mathcal{R}(i)$**\n\nThe expected loss reduction is the difference between the prior and posterior Bayes risk:\n$$\n\\Delta \\mathcal{R}(i) = \\mathcal{R}_{\\text{prior}} - \\mathcal{R}_{\\text{post}}(i) = \\operatorname{tr}(W K_{TT}) - \\operatorname{tr}(W \\Sigma_{T \\mid i})\n$$\nSubstituting the expression for the posterior covariance, $\\Sigma_{T \\mid i} = K_{TT} - K_{T i} \\frac{1}{K_{ii} + \\tau_i^2} K_{iT}$, we have:\n$$\n\\Delta \\mathcal{R}(i) = \\operatorname{tr}(W K_{TT}) - \\operatorname{tr}\\left(W \\left(K_{TT} - K_{T i} \\frac{1}{K_{ii} + \\tau_i^2} K_{iT}\\right)\\right)\n$$\nUsing the linearity of the trace operator, $\\operatorname{tr}(A-B) = \\operatorname{tr}(A) - \\operatorname{tr}(B)$:\n$$\n\\Delta \\mathcal{R}(i) = \\operatorname{tr}(W K_{TT}) - \\left(\\operatorname{tr}(W K_{TT}) - \\operatorname{tr}\\left(W K_{T i} \\frac{1}{K_{ii} + \\tau_i^2} K_{iT}\\right)\\right)\n$$\nThis simplifies to:\n$$\n\\Delta \\mathcal{R}(i) = \\frac{1}{K_{ii} + \\tau_i^2} \\operatorname{tr}\\left(W K_{T i} K_{iT}\\right)\n$$\nThe term $W K_{T i} K_{iT}$ is a matrix of size $|T| \\times |T|$. We can use the cyclic property of the trace, $\\operatorname{tr}(ABC) = \\operatorname{tr}(CAB)$. Let $A=W$, $B=K_{Ti}$, and $C=K_{iT}$. Then $\\operatorname{tr}(W K_{Ti} K_{iT}) = \\operatorname{tr}(K_{iT} W K_{Ti})$. The product $K_{iT} W K_{Ti}$ is a $1 \\times 1$ matrix (a scalar), and its trace is the scalar value itself. Since the weight matrix $W$ is diagonal with entries $W_{kk}$ for $k \\in T$, this scalar product is:\n$$\nK_{iT} W K_{Ti} = \\sum_{j \\in T} \\sum_{k \\in T} K_{ik} W_{kj} K_{ji} = \\sum_{j \\in T} W_{jj} (K_{ij})^2\n$$\nwhere we have used the fact that $W$ is diagonal and $K$ is symmetric ($K_{ij} = K_{ji}$). The final efficient formula for the loss reduction is:\n$$\n\\Delta \\mathcal{R}(i) = \\frac{\\sum_{j \\in T} W_{jj} (K_{ij})^2}{K_{ii} + \\tau_i^2}\n$$\n\n2.  **Risk Penalty Term**\n\nThis penalty is non-zero only if $\\lambda_{\\text{risk}} > 0$. It depends on the risk measure $\\mathcal{Q}(i)$, defined as the maximum posterior variance over the target set $T$:\n$$\n\\mathcal{Q}(i) = \\max_{j \\in T} \\left[\\Sigma_{T \\mid i}\\right]_{jj}\n$$\nThe diagonal elements of the posterior covariance matrix $\\Sigma_{T \\mid i}$ represent the posterior variances of the individual target components. For a target $j \\in T$, the posterior variance is:\n$$\n[\\Sigma_{T \\mid i}]_{jj} = K_{jj} - \\frac{(K_{ji})^2}{K_{ii} + \\tau_i^2}\n$$\nNote that the indices of $\\Sigma_{T \\mid i}$ correspond to the elements of $T$. A careful implementation must map these back to the global indices $j \\in T$. The risk measure is thus:\n$$\n\\mathcal{Q}(i) = \\max_{j \\in T} \\left( K_{jj} - \\frac{(K_{ji})^2}{K_{ii} + \\tau_i^2} \\right)\n$$\nThe penalty is then $\\lambda_{\\text{risk}} \\max(0, \\mathcal{Q}(i) - R_{\\max})$.\n\n**Algorithmic Procedure**\n\nFor each of the four test cases, we perform the following steps:\n1.  Set the physical and model parameters: grid $x$, target indices $T$, kernel hyperparameters ($\\sigma_f, \\ell, \\sigma_{\\text{nug}}^2$), observation noise variances $\\tau^2$, weight matrix diagonal $W_{\\text{diag}}$, measurement costs $c$, and penalty parameters ($\\lambda_{\\text{cost}}, \\lambda_{\\text{risk}}, R_{\\max}$).\n2.  Construct the $M \\times M$ prior covariance matrix $K$ using the squared-exponential kernel formula: $K_{ij} = \\sigma_f^2 \\exp(-\\frac{(x_i - x_j)^2}{2\\ell^2}) + \\sigma_{\\text{nug}}^2 \\delta_{ij}$. For our problem, $M=6$.\n3.  Iterate through each possible measurement index $i \\in \\{0, 1, ..., 5\\}$.\n    a. Calculate the denominator $d_i = K_{ii} + \\tau_i^2$.\n    b. Calculate the expected loss reduction $\\Delta \\mathcal{R}(i)$ using the formula derived above.\n    c. Calculate the cost penalty term $\\lambda_{\\text{cost}} c_i$.\n    d. If $\\lambda_{\\text{risk}} > 0$, calculate the risk measure $\\mathcal{Q}(i)$ and the corresponding risk penalty.\n    e. Combine these terms to find the total utility $U(i)$.\n4.  After computing $U(i)$ for all $i \\in \\{0, 1, ..., 5\\}$, determine the maximum utility among them, $U_{\\max} = \\max_i U(i)$.\n5.  Apply the selection rule:\n    a. If $U_{\\max} < 0$, the optimal choice is to abstain, $i^\\star = -1$.\n    b. Otherwise, find the set of indices $I^\\star = \\{ i \\mid U(i) = U_{\\max} \\}$. The optimal choice is the smallest index in this set, $i^\\star = \\min(I^\\star)$.\n6.  The final result for the test suite is the list of $i^\\star$ for each case.\n\nThis procedure is implemented in the following Python program, adhering to the specified environment and output format.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the Bayesian decision problem for all test cases.\n    \"\"\"\n\n    # Grid and target set are common to all test cases.\n    x = np.array([0, 1, 2, 3, 4, 5], dtype=float)\n    target_indices = np.array([2, 3, 4])\n\n    test_cases = [\n        # Test case 1 (happy path, moderate correlation and uniform low cost)\n        {\n            \"sigma_f\": 1.5, \"l\": 1.0, \"sigma_nug_sq\": 1e-8,\n            \"tau_sq\": np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05]),\n            \"W_diag\": np.array([1.0, 1.0, 2.0]),\n            \"c\": np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1]),\n            \"lambda_cost\": 0.05, \"lambda_risk\": 0.0, \"R_max\": 10.0\n        },\n        # Test case 2 (cost-dominated for a central candidate)\n        {\n            \"sigma_f\": 1.5, \"l\": 1.0, \"sigma_nug_sq\": 1e-8,\n            \"tau_sq\": np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05]),\n            \"W_diag\": np.array([1.0, 1.0, 2.0]),\n            \"c\": np.array([0.1, 0.1, 0.1, 20.0, 0.1, 0.1]),\n            \"lambda_cost\": 0.05, \"lambda_risk\": 0.0, \"R_max\": 10.0\n        },\n        # Test case 3 (tight risk budget with short correlation length)\n        {\n            \"sigma_f\": 1.0, \"l\": 0.3, \"sigma_nug_sq\": 1e-8,\n            \"tau_sq\": np.array([0.01, 0.01, 0.01, 0.01, 0.01, 0.01]),\n            \"W_diag\": np.array([1.0, 1.0, 2.0]),\n            \"c\": np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"lambda_cost\": 0.0, \"lambda_risk\": 5.0, \"R_max\": 0.7\n        },\n        # Test case 4 (high-noise, cost-dominated, prefer abstain)\n        {\n            \"sigma_f\": 1.0, \"l\": 1.0, \"sigma_nug_sq\": 1e-8,\n            \"tau_sq\": np.array([1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0]),\n            \"W_diag\": np.array([1.0, 1.0, 1.0]),\n            \"c\": np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"lambda_cost\": 0.1, \"lambda_risk\": 0.0, \"R_max\": 10.0\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = find_optimal_measurement(x, target_indices, case_params)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef find_optimal_measurement(x, target_indices, params):\n    \"\"\"\n    Computes the optimal measurement index for a single test case.\n    \"\"\"\n    M = len(x)\n    \n    # Unpack parameters\n    sigma_f = params[\"sigma_f\"]\n    l = params[\"l\"]\n    sigma_nug_sq = params[\"sigma_nug_sq\"]\n    tau_sq = params[\"tau_sq\"]\n    W_diag = params[\"W_diag\"]\n    c = params[\"c\"]\n    lambda_cost = params[\"lambda_cost\"]\n    lambda_risk = params[\"lambda_risk\"]\n    R_max = params[\"R_max\"]\n\n    # Construct the prior covariance matrix K\n    x_col = x[:, np.newaxis]\n    sq_dist = (x_col - x_col.T)**2\n    K = sigma_f**2 * np.exp(-sq_dist / (2.0 * l**2)) + sigma_nug_sq * np.eye(M)\n\n    utilities = np.zeros(M)\n    for i in range(M):\n        # Denominator for all calculations involving posterior update\n        denominator = K[i, i] + tau_sq[i]\n        \n        # 1. Calculate expected loss reduction, Delta_R(i)\n        # K_Ti is the vector of covariances between target points and candidate point i\n        K_Ti = K[target_indices, i]\n        # This is sum_{j in T} W_jj * (K_ij)^2, efficient with dot product\n        sum_weighted_sq_cov = np.dot(W_diag, K_Ti**2)\n        delta_R_i = sum_weighted_sq_cov / denominator\n\n        # 2. Calculate cost penalty\n        cost_penalty = lambda_cost * c[i]\n\n        # 3. Calculate risk penalty\n        risk_penalty = 0.0\n        if lambda_risk > 0:\n            # Q(i) = max_{j in T} [Sigma_{T|i}]_{jj}\n            # [Sigma_{T|i}]_{jj} = K_jj - (K_ji)^2 / (K_ii + tau_i^2)\n            K_TT_diag = np.diag(K)[target_indices]\n            posterior_variances = K_TT_diag - (K_Ti**2) / denominator\n            Q_i = np.max(posterior_variances)\n            risk_penalty = lambda_risk * max(0.0, Q_i - R_max)\n        \n        # 4. Calculate total utility U(i)\n        utilities[i] = delta_R_i - cost_penalty - risk_penalty\n\n    # 5. Apply selection rule\n    # Find the maximum utility among all non-abstain choices\n    max_utility = np.max(utilities)\n\n    # If the best non-abstain choice has negative utility, abstain (U(-1)=0)\n    if max_utility < 0:\n        return -1\n    else:\n        # Find all indices that achieve the maximal utility\n        best_indices = np.where(np.isclose(utilities, max_utility))[0]\n        # Tie-breaking rule: choose the smallest index\n        return int(best_indices[0])\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}