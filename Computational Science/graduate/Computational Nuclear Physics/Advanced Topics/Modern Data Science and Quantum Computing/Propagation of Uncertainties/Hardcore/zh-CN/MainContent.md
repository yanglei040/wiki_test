## 引言
在现代计算科学中，尤其是在[计算核物理](@entry_id:747629)这一前沿领域，任何理论预测的价值都取决于其可靠性。然而，模型、参数和测量中固有的不确定性是不可避免的。因此，如何系统地量化和传播这些不确定性，已成为评估模拟结果可信度的核心挑战。许多实践者将不确定性量化（UQ）简单地视为在最终结果上附加一个误差棒，而忽略了其背后严谨的数学框架和深刻的物理内涵。这种知识上的差距可能导致对预测过度自信或错误解读，从而阻碍科学进展。

本文旨在填补这一空白，为[不确定性传播](@entry_id:146574)提供一个从理论到实践的全面指南。在接下来的内容中，我们将遵循一个逻辑递进的结构：

*   **原理与机制**: 在本章中，我们将深入剖析不确定性的分类、核心传播方法（如蒙特卡罗和[Delta方法](@entry_id:276272)）、信息论视角以及统一的贝叶斯框架等基本原理。
*   **应用与[交叉](@entry_id:147634)学科联系**: 随后，我们将通过[计算核物理](@entry_id:747629)、[分析化学](@entry_id:137599)等领域的丰富案例，展示这些原理的强大实践价值，并探讨它们在不同学科间的联系。
*   **动手实践**: 最后，通过一系列精心设计的实践问题，读者将有机会亲手应用所学知识，解决具体的计算挑战，从而将理论理解转化为实用技能。

通过这一结构化的学习路径，本文旨在帮助读者建立对[不确定性传播](@entry_id:146574)的深刻理解，并掌握在科研实践中进行严谨[不确定性分析](@entry_id:149482)的能力。

## 原理与机制

在[计算核物理](@entry_id:747629)领域，对不确定性的严谨处理是确保模拟预测可靠性的基石。[不确定性量化](@entry_id:138597)（UQ）不仅仅是为最终结果附加一个[误差棒](@entry_id:268610)，它是一个系统性的框架，用于识别、表征、传播和分析模型与数据中所有已知的不确定性来源。本章旨在深入探讨[不确定性传播](@entry_id:146574)的核心原理与关键机制，从基本概念出发，逐步延伸至[计算建模](@entry_id:144775)中遇到的高级挑战。

### 不确定性的分类：[偶然不确定性与认知不确定性](@entry_id:746346)

在进行任何[不确定性分析](@entry_id:149482)之前，首要任务是对[不确定性的来源](@entry_id:164809)进行分类。不确定性主要分为两大类：**偶然不确定性（Aleatory Uncertainty）** 和 **[认知不确定性](@entry_id:149866)（Epistemic Uncertainty）**。

**[偶然不确定性](@entry_id:154011)**，又称随机不确定性或不可约不确定性，源于系统内在的、固有的随机性。即便我们拥有关于系统的完美知识，这种不确定性依然存在。它描述的是在相同条件下重复进行实验或模拟时，结果依然会表现出的自然变异。在概率论中，[偶然不确定性](@entry_id:154011)通常由一个定义在结果空间上的[概率分布](@entry_id:146404)来描述。

**认知不确定性**，又称系统不确定性或可约不确定性，源于我们对系统知识的缺乏。这可能是由于测量不精确、模型参数未知、或模型结构本身不完善所致。原则上，通过更多的实验、更高精度的测量或更完善的理论，[认知不确定性](@entry_id:149866)是可以被减小甚至消除的。在概率论中，认知不确定性通常由一个定义在模型[参数空间](@entry_id:178581)上的[概率分布](@entry_id:146404)来表示，这个[分布](@entry_id:182848)反映了我们对参数真实值所处位置的信念程度。

为了具体理解这两种不确定性，我们考虑一个计算[中子输运](@entry_id:159564)问题的实例 。假设我们需要计算一个基准装置中的中子感生反应率 $R$，其模型为：
$R = \int_{\mathcal{V}}\int_{0}^{\infty} \phi(E,\mathbf{r};\omega)\,\sigma(E;\boldsymbol{\theta})\,N(\mathbf{r};\boldsymbol{\theta})\,\mathrm{d}E\,\mathrm{d}^3\mathbf{r}$
其中 $\phi$ 是中子通量，$\sigma$ 是微观[截面](@entry_id:154995)，$N$ 是靶核的数密度。在这个模型中：
- **[偶然不确定性](@entry_id:154011)** 的来源包括：
    1.  **源的随机性**：即使在名义上相同的脉冲之间，每次脉冲发射的中子总数也可能因量子过程而波动，这种波动可以用泊松（Poisson）[分布](@entry_id:182848)来描述。
    2.  **粒子输运的随机性**：在蒙特卡罗（[Monte Carlo](@entry_id:144354)）模拟中，每个中子的飞行、碰撞和散射过程都是随机抽样的。对于给定的输入参数，每次模拟运行（即不同的随机数种子）都会产生一个略有不同的中子通量场 $\phi(E,\mathbf{r};\omega)$，其中 $\omega$ 代表了一次随机历史的实现。

- **认知不确定性** 的来源包括：
    1.  **核数据的不确定性**：微观[反应截面](@entry_id:191218) $\sigma(E)$ 是自然的固定常数，但我们的测量和评估是不完美的。核数据库（如ENDF）通常会提供一个[均值向量](@entry_id:266544) $\boldsymbol{\mu}_{\sigma}$ 和一个协方差矩阵 $\mathbf{C}_{\sigma}$ 来量化我们对这些[截面](@entry_id:154995)值的“知识状态不确定性”。这些[截面](@entry_id:154995)值是固定但未知的，因此构成了参数 $\boldsymbol{\theta}$ 的一部分。
    2.  **材料属性的不确定性**：靶材料的[数密度](@entry_id:268986) $N(\mathbf{r})$ 在实验期间是固定的，但其制造和计量过程存在公差，导致我们对其精确值缺乏完美知识。
    3.  **探测器参数的不确定性**：探测器模型的参数，如[死时间](@entry_id:273487) $\tau$，对于给定的探测器是固定的，但其校准值存在不确定性，这通常由一个校准实验得到的后验分布来描述。

这两种不确定性的分离对于正确的传播至关重要。这可以通过 **[全方差定律](@entry_id:184705)（Law of Total Variance）** 来数学化地表述。对于任何关心量 $Q$，其总[方差](@entry_id:200758)可以分解为：
$\mathrm{Var}(Q) = \mathbb{E}_{\boldsymbol{\theta}}[\mathrm{Var}(Q \mid \boldsymbol{\theta})] + \mathrm{Var}_{\boldsymbol{\theta}}(\mathbb{E}[Q \mid \boldsymbol{\theta}])$
这个公式提供了一个清晰的计算蓝图：
1.  $\mathrm{Var}(Q \mid \boldsymbol{\theta})$ 项代表在**给定一组确定的认知参数** $\boldsymbol{\theta}$ 的情况下，仅由偶然不确定性（例如，蒙特卡罗模拟中的[随机抽样](@entry_id:175193)）引起的[方差](@entry_id:200758)。
2.  $\mathbb{E}_{\boldsymbol{\theta}}[\cdot]$ 算子随后对这个偶然[方差](@entry_id:200758)在所有可能的认知参数 $\boldsymbol{\theta}$ 上进行期望。
3.  $\mathbb{E}[Q \mid \boldsymbol{\theta}]$ 项是在给定 $\boldsymbol{\theta}$ 的情况下对所有偶然性取期望得到的结果。
4.  $\mathrm{Var}_{\boldsymbol{\theta}}(\cdot)$ 算子则计算这个期望结果如何随着我们对认知参数 $\boldsymbol{\theta}$ 的不确定性而变化。

在实践中，这通常通过一个嵌套的抽样循环来实现：外循环对认知参数 $\boldsymbol{\theta}$ 的[分布](@entry_id:182848)进行抽样，而内循环则对每个抽出的 $\boldsymbol{\theta}$ 值，多次运行[随机模拟](@entry_id:168869)（即对偶然变量 $\omega$ 抽样）以估计内层的期望和[方差](@entry_id:200758)。

### [前向传播](@entry_id:193086)方法

一旦我们识别并用[概率分布](@entry_id:146404)来表征了输入的不确定性，下一步就是将这些不确定性通过计算模型 $y = f(x)$ 向前传播，以量化输出 $y$ 的不确定性。本节介绍两种主流的[前向传播](@entry_id:193086)方法。

#### 蒙特卡罗传播

最直接、最通用的[不确定性传播](@entry_id:146574)方法是 **蒙特卡罗（[Monte Carlo](@entry_id:144354), MC）方法**。其基本思想非常直观：通过从输入的[概率分布](@entry_id:146404) $p(x)$ 中反复抽样，将每个样本 $x^{(i)}$ 通过模型计算得到一个输出 $y^{(i)} = f(x^{(i)})$，然后通过分析输出样本集合 $\\{y^{(i)}\\}$ 来估计输出的统计特性（如均值、[方差](@entry_id:200758)和完整的[分布](@entry_id:182848)）。

对于估计输出的期望 $\mu = \mathbb{E}_p[y]$，标准的蒙特卡罗估计量是样本均值：
$\hat{\mu}_N = \frac{1}{N} \sum_{i=1}^{N} y^{(i)}$

这个估计量具有一些非常优良的统计性质 ：
1.  **无偏性**：只要输出的期望存在且有限（即 $\mathbb{E}_p[|y|] \lt \infty$），无论样本量 $N$ 多大，$\hat{\mu}_N$ 的[期望值](@entry_id:153208)都精确等于真实的[期望值](@entry_id:153208) $\mu$。即 $\mathbb{E}[\hat{\mu}_N] = \mu$。这个性质不要求模型 $f(x)$ 或[分布](@entry_id:182848) $p(x)$ 具有任何特殊形式（如连续性或高斯性）。
2.  **[中心极限定理](@entry_id:143108)（CLT）**：如果输出的二阶矩也是有限的（即 $\mathbb{E}_p[y^2] \lt \infty$，这保证了[方差](@entry_id:200758) $\sigma_y^2 = \mathrm{Var}_p(y)$ 有限），那么当样本量 $N$ 足够大时，样本均值 $\hat{\mu}_N$ 的[分布](@entry_id:182848)将近似于一个高斯分布。更精确地说，$\sqrt{N}(\hat{\mu}_N - \mu)$ 在[分布](@entry_id:182848)上收敛于一个均值为 $0$、[方差](@entry_id:200758)为 $\sigma_y^2$ 的正态分布 $\mathcal{N}(0, \sigma_y^2)$。这为构造置信区间提供了理论基础。
3.  **[方差](@entry_id:200758)收敛性**：$\hat{\mu}_N$ 的[方差](@entry_id:200758)为 $\mathrm{Var}(\hat{\mu}_N) = \sigma_y^2 / N$。这意味着估计的[方差](@entry_id:200758)以 $\mathcal{O}(N^{-1})$ 的速率下降，而[标准误差](@entry_id:635378)（即标准差）则以 $\mathcal{O}(N^{-1/2})$ 的速率下降。

值得注意的是，即使输出的[方差](@entry_id:200758)是无限的（例如，当 $f$ 和 $p$ 的组合导致 $y$ 具有[重尾分布](@entry_id:142737)时），只要期望存在，$\hat{\mu}_N$ 仍然是无偏的，但经典的[中心极限定理](@entry_id:143108)可能不适用，收敛速度也可能慢于 $N^{-1/2}$ 。此外，对于从马尔可夫链蒙特卡罗（MCMC）等方法产生的相关样本，只要满足某些条件，类似的无偏性和中心极限定理也成立，但[方差](@entry_id:200758)的计算需要考虑样本间的自相关性 。

蒙特卡罗方法的巨大优势在于其通用性和易于实施，但其收敛速度可能较慢，对于计算成本高昂的模型，可能需要大量的模拟次数才能获得可接受的精度。

#### 线性化传播（[Delta方法](@entry_id:276272)）

当模型 $f(x)$ 的计算成本极高，或者当输入不确定性较小时，一种高效的替代方法是 **线性化传播**，也称为 **[Delta方法](@entry_id:276272)**。该方法基于模型在输入均值 $\mu_x$ 附近的一阶泰勒展开。

我们首先考虑一个将 $p$ 维参数 $\boldsymbol{\theta}$ 映射到标量输出 $Q(\boldsymbol{\theta})$ 的模型。其一阶泰勒展开为：
$Q(\boldsymbol{\theta}) \approx Q(\overline{\boldsymbol{\theta}}) + \nabla_{\boldsymbol{\theta}} Q |_{\overline{\boldsymbol{\theta}}} \cdot (\boldsymbol{\theta} - \overline{\boldsymbol{\theta}})$
其中 $\overline{\boldsymbol{\theta}}$ 是参数的[均值向量](@entry_id:266544)。我们定义 **局部敏感度（local sensitivities）** 向量 $S$ 为 $Q$ 对 $\boldsymbol{\theta}$ 的梯度在均值点处的[转置](@entry_id:142115)，它是一个 $1 \times p$ 的行向量，其分量 $S_j = \left.\frac{\partial Q}{\partial \theta_j}\right|_{\overline{\boldsymbol{\theta}}}$ 量化了输出 $Q$ 对每个参数 $\theta_j$ 的局部响应程度 。

基于这个线性近似，我们可以推导出输出[方差](@entry_id:200758)的近似表达式。输出的波动 $\Delta Q = Q - \mathbb{E}[Q]$ 约等于 $S (\boldsymbol{\theta} - \overline{\boldsymbol{\theta}})$。因此，输出[方差](@entry_id:200758)为：
$\mathrm{Var}(Q) = \mathbb{E}[(\Delta Q)^2] \approx \mathbb{E}[ (S (\boldsymbol{\theta} - \overline{\boldsymbol{\theta}})) (S (\boldsymbol{\theta} - \overline{\boldsymbol{\theta}}))^{\top} ]$
$\mathrm{Var}(Q) \approx S \, \mathbb{E}[(\boldsymbol{\theta} - \overline{\boldsymbol{\theta}})(\boldsymbol{\theta} - \overline{\boldsymbol{\theta}})^{\top}] \, S^{\top}$
$\mathbb{E}[(\boldsymbol{\theta} - \overline{\boldsymbol{\theta}})(\boldsymbol{\theta} - \overline{\boldsymbol{\theta}})^{\top}]$ 正是输入参数的[协方差矩阵](@entry_id:139155) $C_{\theta}$ 的定义。因此，我们得到了一阶[方差](@entry_id:200758)传播公式：
$\mathrm{Var}(Q) \approx S C_{\theta} S^{\top}$
这个公式优雅地展示了输出[方差](@entry_id:200758)是如何由输入参数的[方差](@entry_id:200758)（$C_{\theta}$ 的对角[线元](@entry_id:196833)素）、相关性（$C_{\theta}$ 的非对角[线元](@entry_id:196833)素）以及模型对这些参数的敏感度（$S$）共同决定的。

这个思想可以自然地推广到[向量值函数](@entry_id:261164) $y = f(x)$，其中 $x \in \mathbb{R}^p$，$y \in \mathbb{R}^m$ 。在这种情况下，敏感度向量 $S$ 被替换为 $m \times p$ 的 **[雅可比矩阵](@entry_id:264467)（Jacobian matrix）** $J$，其元素 $J_{ij} = \left.\frac{\partial f_i}{\partial x_j}\right|_{\mu_x}$。线性近似变为 $y - \mu_y \approx J(x - \mu_x)$。通过类似的推导，输出的[协方差矩阵](@entry_id:139155) $C_y$ 可以近似为：
$C_y \approx J C_x J^{\top}$
这个公式是多维[不确定性传播](@entry_id:146574)的核心。它表明，[雅可比矩阵](@entry_id:264467) $J$ 扮演着一个[线性变换](@entry_id:149133)的角色，将输入空间的协[方差](@entry_id:200758)“椭球”映射并旋转到输出空间，从而确定了输出变量的[方差](@entry_id:200758)和相关性结构。这种方法的准确性依赖于模型在输入不确定性范围内近似线性的假设。对于高度[非线性](@entry_id:637147)的模型，这种一阶近似可能会引入偏差。

### 信息论视角与[参数可辨识性](@entry_id:197485)

[不确定性传播](@entry_id:146574)与我们能从数据中学到多少信息密切相关。信息论提供了一个深刻的视角来理解[参数估计](@entry_id:139349)的极限，以及模型结构如何影响不确定性的传播。

#### [费雪信息](@entry_id:144784)与[克拉默-拉奥下界](@entry_id:154412)

**[费雪信息矩阵](@entry_id:750640)（Fisher Information Matrix, FIM）** $I(\theta)$ 是一个核心概念，它量化了观测数据 $D$ 中包含的关于未知参数 $\theta$ 的[信息量](@entry_id:272315)。在满足一定[正则性条件](@entry_id:166962)下，FIM有两种等价定义 ：
1.  作为[对数似然函数](@entry_id:168593)梯度的协[方差](@entry_id:200758)：$I(\theta) = \mathbb{E}_{\theta}[ (\nabla_{\theta} \log p(D \mid \theta)) (\nabla_{\theta} \log p(D \mid \theta))^{\top} ]$
2.  作为[对数似然函数](@entry_id:168593)Hessian矩阵期望的负值：$I(\theta) = \mathbb{E}_{\theta}[- \nabla_{\theta}^{2} \log p(D \mid \theta)]$

从直观上看，如果似然函数 $p(D|\theta)$ 随着参数 $\theta$ 的变化而急剧变化（即梯度大，曲率大），那么不同 $\theta$ 值的[似然](@entry_id:167119)度差异显著，数据中包含的关于 $\theta$ 的信息就越多，FIM也就越大。

FIM的重要性体现在 **[克拉默-拉奥下界](@entry_id:154412)（Cramér-Rao Lower Bound, CRLB）** 中。该定理指出，对于任何对 $\theta$ 的[无偏估计量](@entry_id:756290) $\hat{\theta}$，其协方差矩阵 $\mathrm{Cov}_{\theta}(\hat{\theta})$ 存在一个下界，这个下界由[费雪信息矩阵](@entry_id:750640)的逆决定：
$\operatorname{Cov}_{\theta}(\hat{\theta}) \succeq I(\theta)^{-1}$
其中 $\succeq$ 表示矩阵的半正定序，即 $\operatorname{Cov}_{\theta}(\hat{\theta}) - I(\theta)^{-1}$ 是一个[半正定矩阵](@entry_id:155134)。这意味着，一个参数分量的估计[方差](@entry_id:200758)不可能小于由FIM的逆所决定的某个值。FIM越大（信息越多），其逆矩阵就越小，参数估计的最小可能[方差](@entry_id:200758)也越小。因此，FIM的逆 $I(\theta)^{-1}$ 为我们提供了一个关于[参数不确定性](@entry_id:264387)的“最佳情况”的度量。

#### 不确定性的几何学：“松弛”与“刚性”方向

FIM不仅是一个[信息量](@entry_id:272315)的度量，它还在[参数空间](@entry_id:178581)上定义了一个 **度规张量（metric tensor）**，赋予了参数空间一种几何结构 。通过分析FIM的谱结构（[特征值](@entry_id:154894)和[特征向量](@entry_id:151813)），我们可以发现模型参数的不同组合在被数据约束的程度上存在巨大差异。
- **刚性方向（Stiff directions）**：对应于FIM的**大[特征值](@entry_id:154894)**的[特征向量](@entry_id:151813)方向。沿着这些方向移动参数，模型预测会发生显著变化，因此数据对这些参数组合的约束很强，其不确定性很小。
- **松弛方向（Sloppy directions）**：对应于FIM的**小[特征值](@entry_id:154894)**的[特征向量](@entry_id:151813)方向。沿着这些方向移动参数，模型预测几乎不变，因此数据对这些参数组合的约束很弱，其不确定性很大。

这种“松弛性”是复杂多参数模型的一个普遍特征。参数的协方差矩阵 $C_{\theta} \approx I(\theta)^{-1}$ 的[特征向量](@entry_id:151813)与FIM相同，但[特征值](@entry_id:154894)为FIM[特征值](@entry_id:154894)的倒数。这意味着，松弛方向（$I(\theta)$的小[特征值](@entry_id:154894)）对应于参数协方差矩阵的大[特征值](@entry_id:154894)，即参数空间中不确定性最大的方向。

这种结构直接影响不确定性的传播。对于一个预测量 $g(\theta)$，其传播[方差](@entry_id:200758)由 $\mathrm{Var}(g) \approx (\nabla g)^{\top} C_{\theta} (\nabla g)$ 给出。如果预测量 $g$ 的梯度 $\nabla g$ 主要指向一个松弛方向，那么即使所有单个参数的边缘不确定性看起来很小，其组合效应也会导致 $g$ 的预测不确定性被急剧放大 。

#### [统计不确定性](@entry_id:267672)、系统不确定性与可辨识性

在实验物理中，我们经常需要区分随机的[统计误差](@entry_id:755391)和固定的系统误差。例如，在一个中子反应计数实验中 ，观测到的事件数 $N$ 遵循泊松分布 $N \sim \text{Poisson}(\lambda)$，这是**[统计不确定性](@entry_id:267672)**。然而，真实的平均计数率 $\lambda$ 本身可能受到**系统不确定性**的影响，例如探测器效率存在一个未知的、乘性的校准偏差 $s$。此时，模型变为 $N \mid \sigma, s \sim \text{Poisson}(L \sigma s)$，其中 $\sigma$ 是我们关心的物理[截面](@entry_id:154995)，$s$ 是一个系统效应的**滋扰参数（nuisance parameter）**。

这里出现了一个关键问题：**可辨识性（identifiability）**。从[似然函数](@entry_id:141927) $p(N \mid \sigma, s)$ 中，我们只能确定乘积 $\sigma s$ 的值，而无法单独分辨出 $\sigma$ 和 $s$。这就是参数不可辨识。为了解决这个问题，我们必须引入外部信息来打破简并。在贝叶斯框架中，这通常通过为滋扰参数 $s$ 设置一个信息丰富的先验分布来实现（例如，基于独立的校准实验信息）。这个先验约束了 $s$ 的可能取值范围，从而使得从数据中分离出 $\sigma$ 成为可能。

### 贝叶斯[不确定性传播](@entry_id:146574)框架

贝叶斯推断为处理和传播不确定性提供了一个统一且强大的框架。其核心是利用[贝叶斯定理](@entry_id:151040)，通过观测数据 $D$ 来更新我们对模型参数 $\theta$ 的知识。

整个过程遵循以下逻辑流程 ：
1.  **[先验分布](@entry_id:141376) $p(\theta)$**：我们首先用一个[先验概率](@entry_id:275634)[分布](@entry_id:182848)来表征在观测到任何数据之前我们对参数 $\theta$ 的知识或信念。
2.  **[似然函数](@entry_id:141927) $p(D|\theta)$**：这是一个以参数 $\theta$ 为条件的、观测到数据 $D$ 的概率。它将模型与数据联系起来。
3.  **[后验分布](@entry_id:145605) $p(\theta|D)$**：根据贝叶斯定理，[后验分布](@entry_id:145605)正比于先验与似然的乘积：$p(\theta|D) \propto p(D|\theta)p(\theta)$。[后验分布](@entry_id:145605) $p(\theta|D)$ 融合了先验知识和数据信息，代表了我们在看到数据后对参数 $\theta$ 的更新的知识状态。它完整地刻画了参数的认知不确定性。
4.  **[后验预测分布](@entry_id:167931) $p(y^*|D)$**：为了对一个新的、未观测的量 $y^*$ 进行预测，我们不能简单地将参数的某个[点估计](@entry_id:174544)（如[后验均值](@entry_id:173826)）代入模型。正确的做法是，通过对所有可能的参数值进行加权平均来预测 $y^*$，权重就是参数的后验概率。这个过程称为**边缘化（marginalization）**：
    $p(y^*|D) = \int p(y^*|\theta) p(\theta|D) \,d\theta$

[后验预测分布](@entry_id:167931) $p(y^*|D)$ 是贝叶斯[不确定性传播](@entry_id:146574)的最终产物。它自然地包含了所有来源的不确定性。我们可以再次使用[全方差定律](@entry_id:184705)来分解预测[方差](@entry_id:200758) ：
$\mathrm{Var}(y^*|D) = \mathbb{E}_{\theta|D}[\mathrm{Var}(y^*|\theta,D)] + \mathrm{Var}_{\theta|D}(\mathbb{E}[y^*|\theta,D])$
在一个典型的预测问题中（例如，给定校准参数 $\theta$，新测量 $y^*$ 包含噪声 $\epsilon$），$\mathbb{E}[y^*|\theta,D]$ 是给定参数时的预测值，而 $\mathrm{Var}(y^*|\theta,D)$ 是新测量的[偶然不确定性](@entry_id:154011)（如[测量噪声](@entry_id:275238)[方差](@entry_id:200758) $\sigma^2$）。因此，上式右边的第一项代表了未来测量的[偶然不确定性](@entry_id:154011)，而第二项则代表了由于我们对参数 $\theta$ 仍存在的认知不确定性（由后验分布 $p(\theta|D)$ 描述）所导致的预测值的变化。这个分解清晰地展示了贝叶斯框架如何将学习到的[参数不确定性](@entry_id:264387)传播到未来的预测中。

### 高级[不确定性建模](@entry_id:268420)专题

在实际的计算物理建模中，除了[参数不确定性](@entry_id:264387)，我们还面临着更复杂的挑战，如系统效应、模型缺陷和数值计算引入的误差。

#### 处理系统不确定性：边缘化 vs. 剖面化

如前所述，模型中经常包含我们不直接关心、但会影响主要[参数推断](@entry_id:753157)的滋扰参数（例如，描述探测器效率或背景噪声的参数）。处理这些滋扰参数主要有两种策略 ：
- **[边缘化](@entry_id:264637)（Marginalization）**：这是标准的贝叶斯方法，如上一节所述。我们通过积分（或[高维MCMC](@entry_id:750279)抽样）将滋扰参数从后验分布中“积分掉”，以获得我们关心的参数的边缘后验分布。这样做的好处是，它考虑了滋扰参数所有可能的值，并根据其后验概率进行加权，从而将它们的不确定性完全、诚实地传播到最终结果中。
- **剖面化（Profiling）**：这是一种在频率学派统计中更常见的方法。对于我们关心的参数的每一个固定值 $\theta_0$，我们通过优化找到使[似然函数](@entry_id:141927)最大化的滋扰参数的值。然后，我们构造一个只依赖于 $\theta$ 的“[剖面似然](@entry_id:269700)函数”。这种方法本质上是用一个[点估计](@entry_id:174544)来代替对滋扰参数的完整不确定性描述。

[边缘化](@entry_id:264637)通常被认为是更稳健的方法，因为它完整地考虑了滋扰参数的不确定性，从而得到更保守（通常也更宽）的置信区间。而剖面化可能会因为忽略了滋扰参数在最优值附近的不确定性而低估总的不确定性。

#### 模型缺陷

我们使用的所有模型都是现实的简化。一个关键的[认知不确定性](@entry_id:149866)来源是模型本身可能是不完善的，即存在 **模型缺陷（model discrepancy）**。标准的[不确定性传播](@entry_id:146574)假设模型结构 $f(x, \theta)$ 是完美的，这可能导致过于自信的预测。

现代UQ框架通过在模型中明确引入一个缺陷项 $\delta(x)$ 来解决这个问题 ：
$y = f(x, \theta) + \delta(x) + \epsilon$
这里，$y$ 是观测值， $f(x, \theta)$ 是我们的物理模型，$\epsilon$ 是测量噪声，而 $\delta(x)$ 代表了模型与真实物理过程之间的系统性偏差。由于我们不知道 $\delta(x)$ 的具体形式，我们通常会为其赋予一个灵活的非参数先验，最常用的是 **[高斯过程](@entry_id:182192)（Gaussian Process, GP）** 先验。GP提供了一个在函数空间上的[分布](@entry_id:182848)，允许 $\delta(x)$ 呈现出各种平滑的函数形态。

引入模型缺陷项会带来深刻的后果。现在，模型有了额外的“自由度”来解释数据。这可能导致物理参数 $\theta$ 和缺陷项 $\delta(x)$ 之间的可辨识性问题。如果GP先验过于灵活，它可能会“吸收”掉本应由 $f(x, \theta)$ 解释的结构，使得数据对 $\theta$ 的约束能力减弱。这通常表现为 $\theta$ 和 $\delta$ 之间强烈的负后验相关性。尽管这会使参数的推断变得更具挑战性，但它提供了一个更诚实的不确定性评估，特别是在模型需要外推到数据稀疏的区域时。

#### [数值离散化](@entry_id:752782)误差

计算物理模型几乎总是求解连续的数学方程（如[微分方程](@entry_id:264184)）的[数值近似](@entry_id:161970)。这个近似过程，例如使用[有限差分](@entry_id:167874)或有限元方法，会引入 **[数值离散化](@entry_id:752782)误差**。这种误差与之前讨论的不确定性类型有着本质区别 ：
- 对于给定的模型、参数 $\theta$ 和网格尺寸 $h$，数值解 $Q_h(\theta)$ 是一个确定的值。[离散化误差](@entry_id:748522) $\delta_h(\theta) = Q_h(\theta) - Q_{\infty}(\theta)$ 也是一个固定的、有偏的量，而不是一个[随机变量](@entry_id:195330)。
- 然而，我们不知道这个误差的确切大小，因此它是一种[认知不确定性](@entry_id:149866)。
- 这种误差具有明确的结构：对于一个 $p$ 阶精度的数值方法，当网格尺寸 $h$ 足够小时，误差可以[渐近展开](@entry_id:173196)为 $\delta_h(\theta) \approx c(\theta)h^p + \mathcal{O}(h^{p+\delta})$，其中 $c(\theta)$ 是一个依赖于解的光滑度的系数。

一个严谨的UQ框架必须明确地建模这种误差，而不是将其与物理参数的不确定性混为一谈。常见的一些[启发式方法](@entry_id:637904)，如运行在“足够精细”的网格上然后忽略[数值误差](@entry_id:635587)，或者用[理查森外推法](@entry_id:137237)得到一个更高阶的[点估计](@entry_id:174544)，都未能恰当地量化残余的[数值误差](@entry_id:635587)不确定性。

正确的做法是，将渐近误差模型整合到一个贝叶斯框架中。通过在不同网格分辨率 $\\{h_1, h_2, \dots\\}$ 上运行模拟，我们可以利用这些数据来同时推断物理参数 $\theta$ 和误差模型参数（如 $c(\theta)$）。这样，对真实解 $Q_{\infty}(\theta)$ 的预测就能自然地包含由于离散化而引入的[认知不确定性](@entry_id:149866)。