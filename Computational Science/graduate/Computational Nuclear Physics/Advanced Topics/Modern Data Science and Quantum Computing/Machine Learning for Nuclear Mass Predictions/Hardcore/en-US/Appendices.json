{
    "hands_on_practices": [
        {
            "introduction": "This exercise delves into a fundamental aspect of applying machine learning to experimental sciences: handling data with reported uncertainties. You will start from first principles of Gaussian measurement error to derive the maximum likelihood estimate for a quantity measured multiple times. This process reveals the statistical origin of inverse-variance weighting, a crucial technique for properly designing loss functions that respect the precision of the training data .",
            "id": "3568202",
            "problem": "Consider a single nuclide whose mass excess is measured independently by two experiments. Let the latent true mass excess be $m \\in \\mathbb{R}$. Each reported value $y_{i}$ is modeled by the Gaussian measurement equation $y_{i} = m + \\epsilon_{i}$, where $\\epsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{i}^{2})$ and $(\\epsilon_{1}, \\epsilon_{2})$ are independent. You are given the following two reports for the same nucleus: $y_{1} = -72382.6\\,\\mathrm{keV}$ with standard uncertainty $\\sigma_{1} = 4.0\\,\\mathrm{keV}$, and $y_{2} = -72385.1\\,\\mathrm{keV}$ with standard uncertainty $\\sigma_{2} = 2.5\\,\\mathrm{keV}$. Starting from the Gaussian likelihood and without assuming any shortcut formulas, derive the maximum likelihood estimator $\\hat{m}$ for $m$ and its standard uncertainty $\\sigma_{\\hat{m}}$. Then, in the context of Machine Learning (ML) with a weighted least squares training objective, where the training loss takes the form $L = \\sum_{i} w_{i} \\left(y_{i} - f(x_{i})\\right)^{2}$, derive the choice of sample weights $w_{i}$ that recovers the Gaussian maximum likelihood principle, and determine the single equivalent loss weight $w_{\\mathrm{agg}}$ that should be assigned if the two measurements are first aggregated into one label at $y = \\hat{m}$ and used as a single training sample. Evaluate $\\hat{m}$, $\\sigma_{\\hat{m}}$, and $w_{\\mathrm{agg}}$ numerically for the given data. Express $\\hat{m}$ in $\\mathrm{keV}$ rounded to five significant figures, express $\\sigma_{\\hat{m}}$ in $\\mathrm{keV}$ rounded to four significant figures, and express $w_{\\mathrm{agg}}$ in $\\mathrm{keV}^{-2}$ rounded to four significant figures.",
            "solution": "The problem is well-posed and scientifically grounded. We proceed with the derivation and calculation.\n\nThe problem states that each measurement $y_i$ of the true mass excess $m$ is modeled by a Gaussian distribution. The probability density function (PDF) for observing a value $y_i$ given the true value $m$ and standard uncertainty $\\sigma_i$ is:\n$$p(y_i|m, \\sigma_i) = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp\\left(-\\frac{(y_i - m)^2}{2\\sigma_i^2}\\right)$$\nWe have two independent measurements, $(y_1, \\sigma_1)$ and $(y_2, \\sigma_2)$. Due to independence, the joint probability of observing both $y_1$ and $y_2$, known as the likelihood function $L(m)$, is the product of the individual PDFs:\n$$L(m; y_1, y_2) = p(y_1|m, \\sigma_1) \\cdot p(y_2|m, \\sigma_2) = \\frac{1}{2\\pi\\sigma_1\\sigma_2} \\exp\\left(-\\frac{(y_1 - m)^2}{2\\sigma_1^2} - \\frac{(y_2 - m)^2}{2\\sigma_2^2}\\right)$$\nThe maximum likelihood estimator (MLE) $\\hat{m}$ is the value of $m$ that maximizes $L(m)$. It is equivalent and computationally simpler to maximize the log-likelihood function, $\\ell(m) = \\ln(L(m))$:\n$$\\ell(m) = \\ln\\left(\\frac{1}{2\\pi\\sigma_1\\sigma_2}\\right) - \\frac{(y_1 - m)^2}{2\\sigma_1^2} - \\frac{(y_2 - m)^2}{2\\sigma_2^2}$$\nTo find the maximum, we take the derivative of $\\ell(m)$ with respect to $m$ and set it to zero. The constant term vanishes upon differentiation.\n$$\\frac{d\\ell(m)}{dm} = -\\frac{2(y_1 - m)(-1)}{2\\sigma_1^2} - \\frac{2(y_2 - m)(-1)}{2\\sigma_2^2} = \\frac{y_1 - m}{\\sigma_1^2} + \\frac{y_2 - m}{\\sigma_2^2}$$\nSetting the derivative to zero to find the estimator $\\hat{m}$:\n$$\\frac{y_1 - \\hat{m}}{\\sigma_1^2} + \\frac{y_2 - \\hat{m}}{\\sigma_2^2} = 0$$\n$$\\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2} = \\hat{m}\\left(\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}\\right)$$\nSolving for $\\hat{m}$:\n$$\\hat{m} = \\frac{\\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2}}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}} = \\frac{\\sum_{i=1}^{2} \\frac{y_i}{\\sigma_i^2}}{\\sum_{i=1}^{2} \\frac{1}{\\sigma_i^2}}$$\nThis is the inverse-variance weighted mean of the measurements.\n\nNext, we derive the standard uncertainty of the estimator, $\\sigma_{\\hat{m}}$. The variance of the MLE, $\\sigma_{\\hat{m}}^2$, is given by the Cramér-Rao lower bound, which for an unbiased estimator is the inverse of the Fisher information $I(m)$. The Fisher information is $I(m) = -E\\left[\\frac{d^2\\ell(m)}{dm^2}\\right]$. We compute the second derivative of the log-likelihood:\n$$\\frac{d^2\\ell(m)}{dm^2} = \\frac{d}{dm}\\left(\\frac{y_1 - m}{\\sigma_1^2} + \\frac{y_2 - m}{\\sigma_2^2}\\right) = -\\frac{1}{\\sigma_1^2} - \\frac{1}{\\sigma_2^2} = -\\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}$$\nSince the second derivative is a constant and does not depend on the data $y_i$, its expectation is simply its value.\n$$I(m) = -E\\left[-\\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}\\right] = \\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}$$\nThe variance of $\\hat{m}$ is the inverse of the Fisher information:\n$$\\sigma_{\\hat{m}}^2 = [I(m)]^{-1} = \\left(\\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}\\right)^{-1} = \\frac{1}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}$$\nThe standard uncertainty $\\sigma_{\\hat{m}}$ is the square root of the variance:\n$$\\sigma_{\\hat{m}} = \\sqrt{\\sigma_{\\hat{m}}^2} = \\left(\\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}\\right)^{-1/2}$$\n\nNow, we connect this to the weighted least squares training objective in Machine Learning (ML). The loss function is given as $L = \\sum_{i} w_{i} \\left(y_{i} - f(x_{i})\\right)^{2}$. In our case, we are estimating a single parameter $m$ for one nuclide, so the model prediction is constant, $f(x_i) = m$. The loss function simplifies to:\n$$L(m) = \\sum_{i=1}^{2} w_i (y_i - m)^2$$\nMinimizing this loss function should be equivalent to maximizing the Gaussian likelihood. Maximizing the likelihood is equivalent to minimizing the negative log-likelihood. Ignoring constant terms, the objective to minimize is:\n$$-\\ell(m) \\propto \\sum_{i=1}^{2} \\frac{(y_i - m)^2}{2\\sigma_i^2}$$\nFor the ML loss $L(m)$ to be equivalent to this objective, their functional forms with respect to $m$ must match. Comparing the two expressions:\n$$\\sum_{i=1}^{2} w_i (y_i - m)^2 \\quad \\text{vs.} \\quad \\sum_{i=1}^{2} \\frac{1}{2\\sigma_i^2} (y_i - m)^2$$\nThis equivalence holds if the weights $w_i$ are proportional to $1/\\sigma_i^2$. The conventional choice in statistical weighting is to set $w_i = 1/\\sigma_i^2$. This makes the loss function equal to the chi-squared statistic, $\\chi^2$.\n\nFinally, we consider aggregating the two measurements into a single data point $(y, \\sigma_y)$, where $y = \\hat{m}$ and $\\sigma_y = \\sigma_{\\hat{m}}$. The training loss for this single sample would be $L_{\\mathrm{agg}} = w_{\\mathrm{agg}}(y - m)^2 = w_{\\mathrm{agg}}(\\hat{m} - m)^2$. Following the principle just derived, the appropriate weight $w_{\\mathrm{agg}}$ is the inverse of the variance of this aggregated data point. The variance of our aggregated point $\\hat{m}$ is $\\sigma_{\\hat{m}}^2$. Therefore:\n$$w_{\\mathrm{agg}} = \\frac{1}{\\sigma_{\\hat{m}}^2}$$\nSubstituting the expression for $\\sigma_{\\hat{m}}^2$:\n$$w_{\\mathrm{agg}} = \\frac{1}{\\left(\\frac{1}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}\\right)} = \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}$$\nSince $w_i = 1/\\sigma_i^2$, we see that $w_{\\mathrm{agg}} = w_1 + w_2$. The aggregated weight is the sum of the individual weights.\n\nNumerical evaluation:\nGiven data: $y_{1} = -72382.6\\,\\mathrm{keV}$ with $\\sigma_{1} = 4.0\\,\\mathrm{keV}$, and $y_{2} = -72385.1\\,\\mathrm{keV}$ with $\\sigma_{2} = 2.5\\,\\mathrm{keV}$.\nFirst, calculate the variances and weights:\n$\\sigma_1^2 = (4.0)^2 = 16.0\\,\\mathrm{keV}^2 \\implies w_1 = 1/16.0 = 0.0625\\,\\mathrm{keV}^{-2}$\n$\\sigma_2^2 = (2.5)^2 = 6.25\\,\\mathrm{keV}^2 \\implies w_2 = 1/6.25 = 0.16\\,\\mathrm{keV}^{-2}$\n\nCalculate $\\hat{m}$:\n$$\\hat{m} = \\frac{w_1 y_1 + w_2 y_2}{w_1 + w_2} = \\frac{(0.0625)(-72382.6) + (0.16)(-72385.1)}{0.0625 + 0.16} = \\frac{-4523.9125 - 11581.616}{0.2225} = \\frac{-16105.5285}{0.2225} \\approx -72384.4000\\,\\mathrm{keV}$$\nRounding to five significant figures, $\\hat{m} = -72384\\,\\mathrm{keV}$.\n\nCalculate $\\sigma_{\\hat{m}}$:\n$$\\sigma_{\\hat{m}} = (w_1 + w_2)^{-1/2} = (0.2225)^{-1/2} \\approx \\sqrt{4.494382...} \\approx 2.119995...\\,\\mathrm{keV}$$\nRounding to four significant figures, $\\sigma_{\\hat{m}} = 2.120\\,\\mathrm{keV}$.\n\nCalculate $w_{\\mathrm{agg}}$:\n$$w_{\\mathrm{agg}} = w_1 + w_2 = 0.0625 + 0.16 = 0.2225\\,\\mathrm{keV}^{-2}$$\nRounding to four significant figures, $w_{\\mathrm{agg}} = 0.2225\\,\\mathrm{keV}^{-2}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} -72384 & 2.120 & 0.2225 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Effective model development requires evaluation metrics that are consistent with the statistical properties of the data. Building upon the principle of inverse-variance weighting, this practice challenges you to implement and compare a standard Root Mean Squared Error ($RMSE$) with its weighted counterpart ($WRMSE$). Through hands-on coding with several test cases, you will explore how weighting residuals by their measurement precision provides a more robust and statistically sound assessment of a model's predictive accuracy .",
            "id": "3568178",
            "problem": "In computational nuclear physics, machine learning models that predict nuclear masses are assessed by comparing predictions to evaluated reference values. When reference masses are accompanied by reported one standard deviation uncertainties, it is statistically appropriate to account for heteroscedastic measurement noise. Assume an independent Gaussian noise model with known standard deviations. Specifically, let the evaluated mass excess values be denoted by $\\{y_i\\}_{i=1}^N$ (in megaelectronvolts), the model predictions by $\\{\\hat{y}_i\\}_{i=1}^N$, and the reported standard deviations by $\\{\\sigma_i\\}_{i=1}^N$, with $\\sigma_i > 0$.\n\nTask. From first principles, starting from the Gaussian likelihood for independent observations with possibly different variances, derive a principled weighting scheme for residuals that reflects measurement precision. Then, implement two metrics:\n- the unweighted Root Mean Squared Error (RMSE), and\n- a weighted Root Mean Squared Error (WRMSE) consistent with the Gaussian heteroscedastic noise model.\n\nBoth metrics must be expressed in megaelectronvolts. The final numerical answers must be rounded to $6$ decimal places.\n\nTest Suite. For each test case below, compute the unweighted RMSE and the weighted RMSE. Use mass excess values in megaelectronvolts (MeV). For all arrays, index alignment corresponds to nuclide alignment.\n\n- Test case A (varied uncertainties, typical residuals):\n  - $y = [-60.234, -45.876, -30.112, -15.995, -8.321]$\n  - $\\hat{y} = [-60.100, -45.700, -30.500, -16.100, -8.400]$\n  - $\\sigma = [0.05, 0.10, 0.20, 0.15, 0.08]$\n- Test case B (equal uncertainties, sanity check):\n  - $y = [-12.000, -25.000, -37.000, -49.000]$\n  - $\\hat{y} = [-12.100, -25.100, -36.900, -49.300]$\n  - $\\sigma = [0.10, 0.10, 0.10, 0.10]$\n- Test case C (large residual outliers paired with large uncertainties):\n  - $y = [-80.000, -70.000, -65.000, -60.000, -55.000]$\n  - $\\hat{y} = [-79.900, -69.800, -66.500, -58.500, -53.000]$\n  - $\\sigma = [0.10, 0.10, 1.50, 1.20, 2.50]$\n- Test case D (a very precise point dominates weighting):\n  - $y = [-10.000, -20.000, -30.000, -40.000]$\n  - $\\hat{y} = [-10.200, -20.150, -29.900, -39.850]$\n  - $\\sigma = [0.01, 0.20, 0.20, 0.20]$\n\nOutput requirements.\n- For each test case, produce two floating-point results in megaelectronvolts: first the unweighted RMSE, then the weighted RMSE, each rounded to $6$ decimal places.\n- Aggregate the results for all test cases into a single list in the following order: A-unweighted, A-weighted, B-unweighted, B-weighted, C-unweighted, C-weighted, D-unweighted, D-weighted.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots,r_8]$).\n\nAngle units are not applicable. No percentages are involved. All required quantities must be expressed in megaelectronvolts (MeV), rounded to $6$ decimal places as specified. The code must be self-contained and must not read any input.",
            "solution": "The problem is valid. It is scientifically grounded in standard statistical practices for model evaluation in the physical sciences, is well-posed with all necessary information provided, and is expressed objectively.\n\nThe task is to derive a principled weighting scheme for evaluating model predictions against experimental data with known, heteroscedastic uncertainties, and then to implement two metrics: the unweighted Root Mean Squared Error (RMSE) and a corresponding weighted version (WRMSE).\n\n### Derivation from First Principles\n\nThe foundation of our derivation is the assumption that each experimental measurement $y_i$ is a random variable drawn from a Gaussian (Normal) distribution. The mean of this distribution is the true, unknown physical value, which the model aims to predict with $\\hat{y}_i$. The standard deviation of this distribution is the reported experimental uncertainty, $\\sigma_i$. Under this assumption, the probability density of observing the value $y_i$, given the model's prediction $\\hat{y}_i$ and the uncertainty $\\sigma_i$, is:\n$$\nP(y_i | \\hat{y}_i, \\sigma_i) = \\frac{1}{\\sigma_i \\sqrt{2\\pi}} \\exp\\left( -\\frac{(y_i - \\hat{y}_i)^2}{2\\sigma_i^2} \\right)\n$$\nThe problem states that the observations are independent. Therefore, the total likelihood $\\mathcal{L}$ of observing the entire dataset $\\{y_i\\}_{i=1}^N$ is the product of the individual probabilities:\n$$\n\\mathcal{L}(\\{y_i\\} | \\{\\hat{y}_i, \\sigma_i\\}) = \\prod_{i=1}^{N} P(y_i | \\hat{y}_i, \\sigma_i) = \\prod_{i=1}^{N} \\frac{1}{\\sigma_i \\sqrt{2\\pi}} \\exp\\left( -\\frac{(y_i - \\hat{y}_i)^2}{2\\sigma_i^2} \\right)\n$$\nIn model fitting and evaluation, it is computationally and analytically more convenient to work with the logarithm of the likelihood, the log-likelihood $\\ln(\\mathcal{L})$. Maximizing the likelihood is equivalent to maximizing the log-likelihood.\n$$\n\\ln(\\mathcal{L}) = \\ln\\left( \\prod_{i=1}^{N} \\frac{1}{\\sigma_i \\sqrt{2\\pi}} \\exp\\left( -\\frac{(y_i - \\hat{y}_i)^2}{2\\sigma_i^2} \\right) \\right)\n$$\nUsing the properties of logarithms, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(e^x) = x$, we get:\n$$\n\\ln(\\mathcal{L}) = \\sum_{i=1}^{N} \\ln\\left( \\frac{1}{\\sigma_i \\sqrt{2\\pi}} \\right) + \\sum_{i=1}^{N} \\left( -\\frac{(y_i - \\hat{y}_i)^2}{2\\sigma_i^2} \\right)\n$$\n$$\n\\ln(\\mathcal{L}) = -\\frac{N}{2}\\ln(2\\pi) - \\sum_{i=1}^{N}\\ln(\\sigma_i) - \\frac{1}{2} \\sum_{i=1}^{N} \\frac{(y_i - \\hat{y}_i)^2}{\\sigma_i^2}\n$$\nWhen assessing a model, we seek to find model parameters that maximize this likelihood. The terms $-\\frac{N}{2}\\ln(2\\pi)$ and $-\\sum_{i=1}^{N}\\ln(\\sigma_i)$ are constant with respect to the model's predictions $\\hat{y}_i$. Therefore, maximizing $\\ln(\\mathcal{L})$ is equivalent to minimizing the remaining term, which is proportional to the chi-squared statistic, $\\chi^2$:\n$$\n\\chi^2 = \\sum_{i=1}^{N} \\frac{(y_i - \\hat{y}_i)^2}{\\sigma_i^2} = \\sum_{i=1}^{N} \\left(\\frac{y_i - \\hat{y}_i}{\\sigma_i}\\right)^2\n$$\nThis expression is a sum of squared residuals, where each squared residual $(y_i - \\hat{y}_i)^2$ is multiplied by a factor of $1/\\sigma_i^2$. This provides the principled weighting scheme: the weight $w_i$ for each squared residual should be the inverse of the variance of the corresponding measurement.\n$$\nw_i = \\frac{1}{\\sigma_i^2}\n$$\nThis is intuitively correct: measurements with higher precision (smaller $\\sigma_i$) have larger variance $\\sigma_i^2$, hence a larger weight $w_i$, and thus contribute more significantly to the overall metric.\n\n### Metric Definitions\n\nWith the weighting scheme established, we can define the required metrics. Let $N$ be the number of data points.\n\n1.  **Unweighted Root Mean Squared Error (RMSE)**: This metric treats all residuals equally. It is the square root of the mean of the squared residuals.\n    $$\n    \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}\n    $$\n    The units of RMSE are the same as $y_i$ and $\\hat{y}_i$, which are megaelectronvolts (MeV).\n\n2.  **Weighted Root Mean Squared Error (WRMSE)**: This metric incorporates the derived weights $w_i = 1/\\sigma_i^2$. It is the square root of the weighted mean of the squared residuals. The weighted mean is calculated by summing the weighted values and dividing by the sum of the weights.\n    $$\n    \\text{WRMSE} = \\sqrt{\\frac{\\sum_{i=1}^{N} w_i (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N} w_i}} = \\sqrt{\\frac{\\sum_{i=1}^{N} \\frac{1}{\\sigma_i^2} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N} \\frac{1}{\\sigma_i^2}}}\n    $$\n    The units of WRMSE are also in MeV. To verify this, examine the argument of the square root. The numerator term is $\\frac{(y_i - \\hat{y}_i)^2}{\\sigma_i^2}$, which has units of $\\frac{(\\text{MeV})^2}{(\\text{MeV})^2}$ and is thus dimensionless. The sum is also dimensionless. The denominator term is $1/\\sigma_i^2$, which has units of $1/(\\text{MeV})^2$. The sum has units of $1/(\\text{MeV})^2$. The fraction thus has units of $\\frac{\\text{dimensionless}}{1/(\\text{MeV})^2} = (\\text{MeV})^2$. The square root correctly yields MeV.\n\nAs a consistency check, if all uncertainties are equal, i.e., $\\sigma_i = \\sigma_0$ for all $i$, then $w_i = 1/\\sigma_0^2$ is a constant. The WRMSE formula becomes:\n$$\n\\text{WRMSE} = \\sqrt{\\frac{\\sum_{i=1}^{N} \\frac{1}{\\sigma_0^2} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N} \\frac{1}{\\sigma_0^2}}} = \\sqrt{\\frac{\\frac{1}{\\sigma_0^2} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}{\\frac{N}{\\sigma_0^2}}} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2} = \\text{RMSE}\n$$\nThis confirms that for the homoscedastic case (constant variance), the WRMSE correctly reduces to the unweighted RMSE.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates unweighted and weighted RMSE for nuclear mass predictions\n    based on a Gaussian heteroscedastic noise model.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case A (varied uncertainties, typical residuals)\n        (\n            np.array([-60.234, -45.876, -30.112, -15.995, -8.321]),\n            np.array([-60.100, -45.700, -30.500, -16.100, -8.400]),\n            np.array([0.05, 0.10, 0.20, 0.15, 0.08])\n        ),\n        # Test case B (equal uncertainties, sanity check)\n        (\n            np.array([-12.000, -25.000, -37.000, -49.000]),\n            np.array([-12.100, -25.100, -36.900, -49.300]),\n            np.array([0.10, 0.10, 0.10, 0.10])\n        ),\n        # Test case C (large residual outliers paired with large uncertainties)\n        (\n            np.array([-80.000, -70.000, -65.000, -60.000, -55.000]),\n            np.array([-79.900, -69.800, -66.500, -58.500, -53.000]),\n            np.array([0.10, 0.10, 1.50, 1.20, 2.50])\n        ),\n        # Test case D (a very precise point dominates weighting)\n        (\n            np.array([-10.000, -20.000, -30.000, -40.000]),\n            np.array([-10.200, -20.150, -29.900, -39.850]),\n            np.array([0.01, 0.20, 0.20, 0.20])\n        )\n    ]\n\n    results = []\n    for y_true, y_pred, sigma in test_cases:\n        \n        # Calculate the residuals (differences between true and predicted values)\n        residuals = y_true - y_pred\n        \n        # 1. Unweighted Root Mean Squared Error (RMSE)\n        # RMSE = sqrt(mean(residuals^2))\n        mse = np.mean(np.square(residuals))\n        rmse = np.sqrt(mse)\n        results.append(rmse)\n        \n        # 2. Weighted Root Mean Squared Error (WRMSE)\n        # Weights are the inverse of the variance (sigma^2)\n        weights = 1.0 / np.square(sigma)\n        \n        # Weighted Mean Squared Error (WMSE) = sum(weights * residuals^2) / sum(weights)\n        weighted_squared_errors = weights * np.square(residuals)\n        wmse = np.sum(weighted_squared_errors) / np.sum(weights)\n        \n        # WRMSE = sqrt(WMSE)\n        wrmse = np.sqrt(wmse)\n        results.append(wrmse)\n\n    # Format the final list of results to 6 decimal places and print.\n    # The required output is a single line, comma-separated list in brackets.\n    output_str = f\"[{','.join(f'{r:.6f}' for r in results)}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "In many areas of physics, we have access to abundant but less accurate theoretical data and sparse but highly accurate experimental data. This exercise introduces a powerful technique for fusing these sources: multi-fidelity Gaussian Process regression. You will implement a co-kriging model to see how low-fidelity information can be used to improve predictions and reduce uncertainty where high-fidelity data is unavailable, a key strategy for building powerful, data-efficient models .",
            "id": "3568164",
            "problem": "You are asked to implement a statistically principled multi-fidelity Gaussian process predictor for nuclear mass residuals and to compare its predictive uncertainty against a single-fidelity baseline. Consider a new nucleus characterized by proton number $Z$ and neutron number $N$, and let the target quantity be a residual nuclear mass observable in mega-electron-volts (MeV). The low-fidelity model is a computational theory residual $f_{L}(x)$, and the high-fidelity model is an experimental residual $f_{H}(x)$, where $x = [Z, N]$. The multi-fidelity relationship follows the autoregressive co-kriging structure introduced by Kennedy and O’Hagan: \n$$\nf_{H}(x) = \\rho f_{L}(x) + \\delta(x),\n$$\nwhere $f_{L}(x)$ and $\\delta(x)$ are independent Gaussian processes (GPs), each with a squared-exponential covariance. Observations include additive independent Gaussian noise. You must compute the predictive mean and variance for the high-fidelity residual at specified new nuclei using both the multi-fidelity model (which leverages both low- and high-fidelity data) and a single-fidelity baseline (which uses only high-fidelity data with the implied composite kernel), and then quantify the reduction in uncertainty due to the multi-fidelity approach.\n\nFoundational base and model definitions:\n- Gaussian Process (GP): A collection of random variables, any finite number of which have a joint Gaussian distribution. The predictive distribution under a GP prior and Gaussian likelihood is Gaussian with analytically computable mean and variance.\n- Multi-fidelity autoregressive model: $f_{H}(x) = \\rho f_{L}(x) + \\delta(x)$, with $f_{L} \\sim \\mathcal{GP}(0, k_{L})$ and $\\delta \\sim \\mathcal{GP}(0, k_{\\delta})$, independent.\n- Squared-exponential kernel with anisotropic length-scales: \n$$\nk_{\\text{SE}}(x, x'; \\sigma^{2}, \\ell_{Z}, \\ell_{N}) = \\sigma^{2} \\exp\\left(-\\frac{(Z-Z')^{2}}{\\ell_{Z}^{2}} - \\frac{(N-N')^{2}}{\\ell_{N}^{2}}\\right).\n$$\n\nObservation models:\n- Low-fidelity observations: $y_{L}(x) = f_{L}(x) + \\epsilon_{L}(x)$, with $\\epsilon_{L}(x) \\sim \\mathcal{N}(0, \\sigma_{L,n}^{2})$.\n- High-fidelity observations: $y_{H}(x) = f_{H}(x) + \\epsilon_{H}(x)$, with $\\epsilon_{H}(x) \\sim \\mathcal{N}(0, \\sigma_{H,n}^{2})$.\n\nGiven hyperparameters are:\n- Autoregressive coefficient: $\\rho = 0.8$.\n- Low-fidelity kernel parameters: $\\sigma_{L,f}^{2} = 25$ $\\text{MeV}^{2}$, $\\ell_{Z,L} = 15$, $\\ell_{N,L} = 18$.\n- High-fidelity discrepancy kernel parameters: $\\sigma_{\\delta,f}^{2} = 9$ $\\text{MeV}^{2}$, $\\ell_{Z,\\delta} = 12$, $\\ell_{N,\\delta} = 10$.\n- Observation noise variances: $\\sigma_{L,n}^{2} = 4$ $\\text{MeV}^{2}$, $\\sigma_{H,n}^{2} = 1$ $\\text{MeV}^{2}$.\n\nTraining data (all residuals in MeV):\n- Low-fidelity training inputs $X_{L}$ and outputs $y_{L}$:\n  $$\n  X_{L} = \\{(20,20), (26,30), (28,34), (50,62), (82,126), (92,146)\\},\n  $$\n  $$\n  y_{L} = \\{-3.0, -1.5, -0.5, 2.0, 4.0, 5.5\\}.\n  $$\n- High-fidelity training inputs $X_{H}$ and outputs $y_{H}$:\n  $$\n  X_{H} = \\{(26,30), (50,62), (82,126), (92,146)\\},\n  $$\n  $$\n  y_{H} = \\{-1.8, 2.1, 4.4, 6.0\\}.\n  $$\n\nPredictive distributions to compute:\n1. Multi-fidelity predictive distribution for $f_{H}(x^{*})$ at a new input $x^{*}$ using the joint low- and high-fidelity training data. Let $K$ denote the joint covariance of stacked observations $[y_{L}; y_{H}]$, and let $k$ denote the cross-covariance vector between $[y_{L}; y_{H}]$ and $f_{H}(x^{*})$. The prior variance for $f_{H}(x^{*})$ is $\\rho^{2} k_{L}(x^{*}, x^{*}) + k_{\\delta}(x^{*}, x^{*})$. The posterior mean and variance are:\n   $$\n   \\mu_{\\text{MF}}(x^{*}) = k^{\\top} K^{-1} \\begin{bmatrix} y_{L} \\\\ y_{H} \\end{bmatrix},\n   \\quad\n   \\sigma^{2}_{\\text{MF}}(x^{*}) = \\rho^{2} k_{L}(x^{*}, x^{*}) + k_{\\delta}(x^{*}, x^{*}) - k^{\\top} K^{-1} k.\n   $$\n   Here,\n   $$\n   K = \n   \\begin{bmatrix}\n   K_{LL} + \\sigma_{L,n}^{2} I & \\rho K_{LH} \\\\\n   \\rho K_{HL} & \\rho^{2} K_{HH}^{(L)} + K_{HH}^{(\\delta)} + \\sigma_{H,n}^{2} I\n   \\end{bmatrix},\n   $$\n   with $K_{LL} = k_{L}(X_{L}, X_{L})$, $K_{LH} = k_{L}(X_{L}, X_{H})$, $K_{HH}^{(L)} = k_{L}(X_{H}, X_{H})$, $K_{HH}^{(\\delta)} = k_{\\delta}(X_{H}, X_{H})$, and\n   $$\n   k = \n   \\begin{bmatrix}\n   \\rho \\, k_{L}(x^{*}, X_{L}) \\\\\n   \\rho^{2} \\, k_{L}(x^{*}, X_{H}) + k_{\\delta}(x^{*}, X_{H})\n   \\end{bmatrix}.\n   $$\n2. Single-fidelity predictive distribution for $f_{H}(x^{*})$ using only high-fidelity data, with the implied composite kernel $k_{H}(x, x') = \\rho^{2} k_{L}(x, x') + k_{\\delta}(x, x')$. Let $K_{H} = k_{H}(X_{H}, X_{H}) + \\sigma_{H,n}^{2} I$, $k_{H,*} = k_{H}(x^{*}, X_{H})$, and $v_{H} = k_{H}(x^{*}, x^{*})$. Then:\n   $$\n   \\mu_{\\text{SF}}(x^{*}) = k_{H,*}^{\\top} K_{H}^{-1} y_{H},\n   \\quad\n   \\sigma^{2}_{\\text{SF}}(x^{*}) = v_{H} - k_{H,*}^{\\top} K_{H}^{-1} k_{H,*}.\n   $$\n\nUncertainty reduction metric to report for each $x^{*}$:\n$$\nr(x^{*}) = \\frac{\\sigma^{2}_{\\text{SF}}(x^{*}) - \\sigma^{2}_{\\text{MF}}(x^{*})}{\\sigma^{2}_{\\text{SF}}(x^{*})},\n$$\nexpressed as a decimal.\n\nUnits and numerical reporting:\n- All means are in MeV, all variances are in $\\text{MeV}^{2}$, and the reduction $r$ is unitless.\n- Report each floating-point number rounded to six decimal places.\n\nTest suite (new nuclei to evaluate):\n- Case $1$: $x^{*} = (28, 34)$.\n- Case $2$: $x^{*} = (90, 150)$.\n- Case $3$: $x^{*} = (20, 20)$.\n\nProgram requirements:\n- Implement the above predictive computations using the given hyperparameters and training data.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case in order, append the five numbers $[\\mu_{\\text{MF}}, \\sigma^{2}_{\\text{MF}}, \\mu_{\\text{SF}}, \\sigma^{2}_{\\text{SF}}, r]$ (rounded to six decimal places), resulting in a flat list over all test cases. For example, the output format must be exactly like:\n$$\n[\\mu_{\\text{MF}}^{(1)}, \\sigma_{\\text{MF}}^{2\\,(1)}, \\mu_{\\text{SF}}^{(1)}, \\sigma_{\\text{SF}}^{2\\,(1)}, r^{(1)}, \\mu_{\\text{MF}}^{(2)}, \\ldots, r^{(3)}].\n$$\nNo additional text may be printed.",
            "solution": "The problem requires us to implement and compare a multi-fidelity Gaussian Process (GP) model against a single-fidelity GP baseline for predicting nuclear mass residuals. The core of any GP model is the covariance function, or kernel, which encodes our prior beliefs about the function's properties, such as smoothness.\n\n**1. Kernel Function**\n\nThe specified kernel is the anisotropic squared-exponential (SE) kernel. Given two sets of input points, $X_1 \\in \\mathbb{R}^{N_1 \\times 2}$ and $X_2 \\in \\mathbb{R}^{N_2 \\times 2}$, the covariance matrix $K \\in \\mathbb{R}^{N_1 \\times N_2}$ is computed element-wise. For any two points $x=[Z, N]$ and $x'=[Z', N']$, the kernel is:\n$$\nk(x, x'; \\sigma_{f}^{2}, \\ell_{Z}, \\ell_{N}) = \\sigma_{f}^{2} \\exp\\left(-\\frac{(Z-Z')^{2}}{\\ell_{Z}^{2}} - \\frac{(N-N')^{2}}{\\ell_{N}^{2}}\\right)\n$$\nwhere $\\sigma_{f}^{2}$ is the signal variance and $\\ell_{Z}, \\ell_{N}$ are the characteristic length-scales along the proton ($Z$) and neutron ($N$) axes, respectively. We will implement this function as a reusable module. Two distinct kernels are used: $k_L$ for the low-fidelity process $f_L(x)$, and $k_\\delta$ for the discrepancy process $\\delta(x)$.\n\n**2. Single-Fidelity (SF) Predictive Distribution**\n\nThe SF model uses only the high-fidelity data $\\{X_H, y_H\\}$ for prediction. The model assumes that the high-fidelity function $f_H(x)$ is a GP. The problem specifies that the kernel for this GP is the implied composite kernel from the multi-fidelity structure:\n$$\nk_{H}(x, x') = \\text{Cov}(f_H(x), f_H(x')) = \\text{Cov}(\\rho f_L(x) + \\delta(x), \\rho f_L(x') + \\delta(x'))\n$$\nDue to the independence of $f_L$ and $\\delta$, this simplifies to:\n$$\nk_{H}(x, x') = \\rho^2 \\text{Cov}(f_L(x), f_L(x')) + \\text{Cov}(\\delta(x), \\delta(x')) = \\rho^2 k_L(x, x') + k_\\delta(x, x')\n$$\nThe observation model is $y_H(x) = f_H(x) + \\epsilon_H(x)$. Let $Y_H$ be the vector of high-fidelity observations. The predictive distribution for $f_H(x^*)$ at a new point $x^*$ is Gaussian, $p(f_H(x^*) | Y_H, X_H) = \\mathcal{N}(\\mu_{\\text{SF}}(x^*), \\sigma^2_{\\text{SF}}(x^*))$, with mean and variance given by the standard GP conditioning equations:\n$$\n\\mu_{\\text{SF}}(x^{*}) = k_{H,*}^{\\top} K_{H}^{-1} Y_{H}\n$$\n$$\n\\sigma^{2}_{\\text{SF}}(x^{*}) = v_{H} - k_{H,*}^{\\top} K_{H}^{-1} k_{H,*}\n$$\nwhere:\n- $K_{H} = k_{H}(X_{H}, X_{H}) + \\sigma_{H,n}^{2} I$ is the $(N_H \\times N_H)$ covariance matrix of the observations $y_H$.\n- $k_{H,*} = k_{H}(X_{H}, x^{*})$ is the $(N_H \\times 1)$ vector of covariances between the training points and the test point.\n- $v_{H} = k_{H}(x^{*}, x^{*}) = \\rho^2 k_L(x^*, x^*) + k_\\delta(x^*, x^*) = \\rho^2 \\sigma_{L,f}^2 + \\sigma_{\\delta,f}^2$ is the prior variance of $f_H$ at $x^*$.\nTo compute this, we first construct the composite kernel matrix $k_H(X_H, X_H)$, add the noise variance to its diagonal, and then solve linear systems to find the mean and variance.\n\n**3. Multi-Fidelity (MF) Predictive Distribution**\n\nThe MF model leverages both low-fidelity data $\\{X_L, y_L\\}$ and high-fidelity data $\\{X_H, y_H\\}$. We stack the observations into a single vector $Y = [Y_L; Y_H]$. The goal is to find the predictive distribution for $f_H(x^*)$ conditioned on this joint dataset. The predictive distribution is again Gaussian, $p(f_H(x^*) | Y, X_L, X_H) = \\mathcal{N}(\\mu_{\\text{MF}}(x^*), \\sigma^2_{\\text{MF}}(x^*))$. The general structure is the same, but the covariance matrices become block matrices reflecting the correlations between all data points.\n\nThe joint covariance matrix $K = \\text{Cov}(Y, Y)$ is a $( (N_L+N_H) \\times (N_L+N_H) )$ block matrix:\n$$\nK = \\begin{bmatrix} \\text{Cov}(Y_L, Y_L) & \\text{Cov}(Y_L, Y_H) \\\\ \\text{Cov}(Y_H, Y_L) & \\text{Cov}(Y_H, Y_H) \\end{bmatrix}\n$$\nThe blocks are derived from the model structure:\n- $\\text{Cov}(Y_L, Y_L) = k_L(X_L, X_L) + \\sigma_{L,n}^2 I = K_{LL} + \\sigma_{L,n}^2 I$.\n- $\\text{Cov}(Y_L, Y_H) = \\text{Cov}(f_L(X_L), \\rho f_L(X_H) + \\delta(X_H)) = \\rho k_L(X_L, X_H) = \\rho K_{LH}$.\n- $\\text{Cov}(Y_H, Y_H) = \\text{Cov}(\\rho f_L(X_H) + \\delta(X_H), \\rho f_L(X_H) + \\delta(X_H)) + \\sigma_{H,n}^2 I = \\rho^2 k_L(X_H, X_H) + k_\\delta(X_H, X_H) + \\sigma_{H,n}^2 I = \\rho^2 K_{HH}^{(L)} + K_{HH}^{(\\delta)} + \\sigma_{H,n}^2 I$.\n\nThe cross-covariance vector $k = \\text{Cov}(Y, f_H(x^*))$ is a $( (N_L+N_H) \\times 1 )$ block vector:\n$$\nk = \\begin{bmatrix} \\text{Cov}(Y_L, f_H(x^*)) \\\\ \\text{Cov}(Y_H, f_H(x^*)) \\end{bmatrix}\n$$\n- $\\text{Cov}(Y_L, f_H(x^*)) = \\text{Cov}(f_L(X_L), \\rho f_L(x^*) + \\delta(x^*)) = \\rho k_L(X_L, x^*)$.\n- $\\text{Cov}(Y_H, f_H(x^*)) = \\text{Cov}(\\rho f_L(X_H) + \\delta(X_H), \\rho f_L(x^*) + \\delta(x^*)) = \\rho^2 k_L(X_H, x^*) + k_\\delta(X_H, x^*)$.\n\nThe prior variance of the target quantity is $v_H = k_H(x^*, x^*)$, which is identical to the single-fidelity case. The predictive mean and variance are then:\n$$\n\\mu_{\\text{MF}}(x^{*}) = k^{\\top} K^{-1} Y\n$$\n$$\n\\sigma^{2}_{\\text{MF}}(x^{*}) = v_{H} - k^{\\top} K^{-1} k\n$$\n\n**4. Implementation and Calculation**\n\nThe algorithm proceeds as follows for each test point $x^*$:\n1.  Define all hyperparameters and datasets.\n2.  Implement the SE kernel function.\n3.  **For the SF model**:\n    a. Construct the composite kernel matrix $k_H(X_H, X_H)$ and cross-covariance vector $k_H(X_H, x^*)$.\n    b. Form the final covariance matrix $K_H$ by adding noise.\n    c. Solve the necessary linear systems to find $\\mu_{\\text{SF}}(x^*)$ and $\\sigma^2_{\\text{SF}}(x^*)$. Numerically, it is more stable to solve $K_H \\alpha = Y_H$ for $\\alpha$ and compute $\\mu = k^\\top \\alpha$, rather than explicitly inverting $K_H$.\n4.  **For the MF model**:\n    a. Construct all required kernel sub-matrices: $K_{LL}$, $K_{LH}$, $K_{HH}^{(L)}$, $K_{HH}^{(\\delta)}$.\n    b. Assemble the full block matrix $K$ and block vector $k$.\n    c. Solve the linear systems involving $K$ to find $\\mu_{\\text{MF}}(x^*)$ and $\\sigma^2_{\\text{MF}}(x^*)$.\n5.  Calculate the uncertainty reduction ratio $r(x^*)$.\n6.  Collect and format all five resulting values for each test point.\n\nThe low-fidelity data acts as an additional constraint on the shared underlying process $f_L(x)$, which, through the autoregressive structure, helps to better constrain the high-fidelity process $f_H(x)$. This typically leads to $\\sigma^2_{\\text{MF}} < \\sigma^2_{\\text{SF}}$, resulting in a positive uncertainty reduction $r > 0$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares single-fidelity and multi-fidelity Gaussian Process\n    predictors for nuclear mass residuals based on the problem specification.\n    \"\"\"\n\n    # 1. Define all given hyperparameters, data, and model constants.\n    rho = 0.8\n    # Low-fidelity kernel parameters\n    sigma_L_f_sq = 25.0\n    l_Z_L = 15.0\n    l_N_L = 18.0\n    # Discrepancy kernel parameters\n    sigma_delta_f_sq = 9.0\n    l_Z_delta = 12.0\n    l_N_delta = 10.0\n    # Observation noise variances\n    sigma_L_n_sq = 4.0\n    sigma_H_n_sq = 1.0\n\n    # Low-fidelity training data\n    X_L = np.array([\n        [20.0, 20.0], [26.0, 30.0], [28.0, 34.0],\n        [50.0, 62.0], [82.0, 126.0], [92.0, 146.0]\n    ])\n    y_L = np.array([-3.0, -1.5, -0.5, 2.0, 4.0, 5.5])\n\n    # High-fidelity training data\n    X_H = np.array([\n        [26.0, 30.0], [50.0, 62.0], [82.0, 126.0], [92.0, 146.0]\n    ])\n    y_H = np.array([-1.8, 2.1, 4.4, 6.0])\n\n    # Test cases (new nuclei to evaluate)\n    test_cases = np.array([\n        [28.0, 34.0], [90.0, 150.0], [20.0, 20.0]\n    ])\n    \n    n_L = X_L.shape[0]\n    n_H = X_H.shape[0]\n\n    def sq_exp_kernel(X1, X2, sigma_f_sq, l_z, l_n):\n        \"\"\"\n        Computes the anisotropic squared-exponential kernel matrix.\n        X1: (N1, 2) array of inputs.\n        X2: (N2, 2) array of inputs.\n        Returns: (N1, N2) covariance matrix.\n        \"\"\"\n        # Pairwise squared distances for Z and N, using broadcasting\n        dist_Z_sq = np.sum(X1[:, 0:1]**2, 1, keepdims=True) - 2 * X1[:, 0:1] @ X2[:, 0:1].T + np.sum(X2[:, 0:1]**2, 1, keepdims=True).T\n        dist_N_sq = np.sum(X1[:, 1:2]**2, 1, keepdims=True) - 2 * X1[:, 1:2] @ X2[:, 1:2].T + np.sum(X2[:, 1:2]**2, 1, keepdims=True).T\n        return sigma_f_sq * np.exp(-dist_Z_sq / (l_z**2) - dist_N_sq / (l_n**2))\n\n    results = []\n\n    # 2. Pre-calculate common matrices to speed up the loop.\n    \n    # Kernel matrices for the f_L process\n    K_LL_L = sq_exp_kernel(X_L, X_L, sigma_L_f_sq, l_Z_L, l_N_L)\n    K_LH_L = sq_exp_kernel(X_L, X_H, sigma_L_f_sq, l_Z_L, l_N_L)\n    K_HL_L = K_LH_L.T\n    K_HH_L = sq_exp_kernel(X_H, X_H, sigma_L_f_sq, l_Z_L, l_N_L)\n    \n    # Kernel matrix for the delta process\n    K_HH_delta = sq_exp_kernel(X_H, X_H, sigma_delta_f_sq, l_Z_delta, l_N_delta)\n\n    # Assemble the full covariance matrix K for the multi-fidelity model\n    K_mf = np.zeros((n_L + n_H, n_L + n_H))\n    K_mf[:n_L, :n_L] = K_LL_L + sigma_L_n_sq * np.eye(n_L)\n    K_mf[:n_L, n_L:] = rho * K_LH_L\n    K_mf[n_L:, :n_L] = rho * K_HL_L\n    K_mf[n_L:, n_L:] = rho**2 * K_HH_L + K_HH_delta + sigma_H_n_sq * np.eye(n_H)\n\n    # Stacked observation vector for the multi-fidelity model\n    Y_mf = np.concatenate([y_L, y_H])\n    \n    # Covariance matrix for the single-fidelity model\n    K_sf = rho**2 * K_HH_L + K_HH_delta + sigma_H_n_sq * np.eye(n_H)\n    \n    # Prior variance of f_H(x*). It is constant for any x*.\n    prior_var_H_at_xstar = rho**2 * sigma_L_f_sq + sigma_delta_f_sq\n\n    # 3. Loop over each test case to compute predictions.\n    for x_star_vec in test_cases:\n        x_star = x_star_vec.reshape(1, -1)\n\n        # --- Single-Fidelity (SF) Calculation ---\n        k_L_star_H = sq_exp_kernel(x_star, X_H, sigma_L_f_sq, l_Z_L, l_N_L)\n        k_delta_star_H = sq_exp_kernel(x_star, X_H, sigma_delta_f_sq, l_Z_delta, l_N_delta)\n        \n        # Cross-covariance vector k_{H,*}^T, shape (1, n_H)\n        k_H_star_T = rho**2 * k_L_star_H + k_delta_star_H\n        k_H_star = k_H_star_T.T # shape (n_H, 1)\n\n        # Solve for predictive mean\n        alpha_sf = np.linalg.solve(K_sf, y_H)\n        mu_sf = (k_H_star_T @ alpha_sf)[0]\n        \n        # Solve for predictive variance\n        beta_sf = np.linalg.solve(K_sf, k_H_star)\n        var_sf = (prior_var_H_at_xstar - k_H_star_T @ beta_sf)[0, 0]\n        \n        # --- Multi-Fidelity (MF) Calculation ---\n        k_L_star_L = sq_exp_kernel(x_star, X_L, sigma_L_f_sq, l_Z_L, l_N_L)\n        \n        # Assemble cross-covariance vector k, shape (n_L+n_H, 1)\n        k_mf_upper = rho * k_L_star_L.T\n        k_mf_lower = (rho**2 * k_L_star_H + k_delta_star_H).T\n        k_mf_star = np.concatenate([k_mf_upper, k_mf_lower])\n        \n        # Solve for predictive mean\n        alpha_mf = np.linalg.solve(K_mf, Y_mf)\n        mu_mf = (k_mf_star.T @ alpha_mf)[0]\n\n        # Solve for predictive variance\n        beta_mf = np.linalg.solve(K_mf, k_mf_star)\n        var_mf = (prior_var_H_at_xstar - k_mf_star.T @ beta_mf)[0, 0]\n        \n        # --- Uncertainty Reduction Calculation ---\n        reduction = (var_sf - var_mf) / var_sf if var_sf != 0 else 0.0\n\n        # Append results for the current test case\n        results.extend([mu_mf, var_mf, mu_sf, var_sf, reduction])\n\n    # 4. Format and print the final output string.\n    formatted_results = [f\"{x:.6f}\" for x in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}