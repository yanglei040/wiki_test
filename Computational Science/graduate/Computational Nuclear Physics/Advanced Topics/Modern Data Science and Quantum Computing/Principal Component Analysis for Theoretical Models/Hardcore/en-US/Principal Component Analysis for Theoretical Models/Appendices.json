{
    "hands_on_practices": [
        {
            "introduction": "Complex theoretical models in nuclear physics, such as Skyrme Energy Density Functionals, often involve many parameters that are calibrated to experimental data. A common challenge is 'parameter degeneracy,' where different combinations of parameters produce nearly identical predictions, making the model ill-conditioned. This hands-on practice  guides you to formulate and implement a powerful diagnostic test using PCA to uncover these degeneracies by identifying directions of vanishing variance in the space of model observables.",
            "id": "3581425",
            "problem": "Consider a simplified calibration setting for a Skyrme Energy Density Functional (EDF) where a vector of model parameters $p \\in \\mathbb{R}^d$ determines a vector of observables $y \\in \\mathbb{R}^m$. Assume that near a calibration point $p_0$, small perturbations $\\delta p$ induce changes in observables that can be described by a linearized sensitivity model, and that the parameter variations are random with a known covariance. All quantities are treated as dimensionless by prior normalization, so no physical units are required.\n\nYour task is to design and implement a Principal Component Analysis (PCA)-based test to detect parameter degeneracies by analyzing near-zero eigenvalues in the covariance of observables induced by parameter variations. The test should be derived from first principles using the following foundational bases: linear response around a calibration point, random variables with covariance, and spectral decomposition of symmetric positive semidefinite matrices. You must not use any pre-specified formulas for the induced observable covariance or for PCA within the problem statement; instead, derive them in your solution from the stated bases.\n\nDefine a degeneracy detection criterion as follows. Let the eigenvalues of the observable covariance matrix be $\\{\\lambda_i\\}_{i=1}^m$ with $\\lambda_{\\max} = \\max_i \\lambda_i$. Given a threshold parameter $\\tau > 0$, declare a principal component to be \"degenerate\" if its eigenvalue satisfies $\\lambda_i \\le \\tau \\lambda_{\\max}$. The total number of degeneracies $D$ is the count of such eigenvalues. Larger values of $D$ indicate stronger degeneracy.\n\nImplement a program that, for each provided test case, computes the induced observable covariance matrix from the given sensitivity matrix and parameter covariance, performs PCA, and returns the degeneracy count $D$ based on the criterion above.\n\nThe test suite consists of four cases. In each case, $m = d = 6$. For clarity, denote the sensitivity matrix by $S \\in \\mathbb{R}^{6 \\times 6}$ and the parameter covariance by $C_p \\in \\mathbb{R}^{6 \\times 6}$. The threshold is $\\tau = 10^{-9}$.\n\nCase 1 (well-conditioned, no degeneracy expected):\n- Sensitivity matrix $S_1$:\n$$\nS_1 = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 \\\\\n0.5 & 0.2 & 0.1 & 0.3 & 1.0 & 0.0 \\\\\n0.1 & 0.4 & 0.3 & 0.2 & 0.0 & 1.0\n\\end{pmatrix}\n$$\n- Parameter covariance $C_{p,1}$:\n$$\nC_{p,1} = \\mathrm{diag}(1.0, 0.8, 1.2, 0.9, 1.1, 1.0)\n$$\n\nCase 2 (exact linear dependence between two parameter directions):\n- Sensitivity matrix $S_2$ obtained by copying column $5$ of $S_1$ into column $6$, so columns $5$ and $6$ are identical:\n$$\nS_2 = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 \\\\\n0.5 & 0.2 & 0.1 & 0.3 & 1.0 & 1.0 \\\\\n0.1 & 0.4 & 0.3 & 0.2 & 0.0 & 0.0\n\\end{pmatrix}\n$$\n- Parameter covariance $C_{p,2}$:\n$$\nC_{p,2} = \\mathrm{diag}(1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n$$\n\nCase 3 (anisotropic parameter covariance with a nearly frozen direction):\n- Sensitivity matrix $S_3$ is equal to $S_1$.\n- Parameter covariance $C_{p,3}$:\n$$\nC_{p,3} = \\mathrm{diag}(1.0, 1.0, 1.0, 1.0, 1.0, 10^{-12})\n$$\n\nCase 4 (near-degeneracy: two parameter directions almost linearly dependent):\n- Sensitivity matrix $S_4$ obtained by perturbing column $6$ of $S_2$ slightly so that it is nearly, but not exactly, equal to column $5$. Concretely, let the perturbation be of order $10^{-10}$ in rows $5$ and $6$:\n$$\nS_4 = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 \\\\\n0.5 & 0.2 & 0.1 & 0.3 & 1.0 & 1.0 + 3 \\times 10^{-11} \\\\\n0.1 & 0.4 & 0.3 & 0.2 & 0.0 & -3 \\times 10^{-11}\n\\end{pmatrix}\n$$\n- Parameter covariance $C_{p,4}$:\n$$\nC_{p,4} = \\mathrm{diag}(1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n$$\n\nYour program must, for each case $i \\in \\{1,2,3,4\\}$, compute the degeneracy count $D_i$ using the PCA-based criterion with threshold $\\tau = 10^{-9}$ and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, `[d_1,d_2,d_3,d_4]`, where each $d_i$ is an integer equal to $D_i$ for case $i$. The program must be entirely self-contained, must not read any external input, and must compute results using the specified matrices and threshold exactly as given. Angles do not appear in this problem, and percentages must not be used anywhere; any ratio should be expressed as a decimal.",
            "solution": "The problem requires the design and implementation of a Principal Component Analysis (PCA)-based test to detect parameter degeneracies in a linearized theoretical model, deriving the method from first principles. The core of the task is to analyze the covariance structure of model observables that is induced by random variations in the model parameters.\n\nThe foundational elements are:\n1.  A linear response model: Small perturbations in model parameters, $\\delta p \\in \\mathbb{R}^d$, cause changes in model observables, $\\delta y \\in \\mathbb{R}^m$, according to the relationship $\\delta y = S \\delta p$. Here, $S \\in \\mathbb{R}^{m \\times d}$ is the sensitivity matrix, whose elements $S_{ij} = \\frac{\\partial y_i}{\\partial p_j}$ represent the partial derivative of the $i$-th observable with respect to the $j$-th parameter, evaluated at a reference point $p_0$.\n2.  Stochastic parameter variations: The parameter perturbations $\\delta p$ are treated as a random vector with a known covariance matrix $C_p \\in \\mathbb{R}^{d \\times d}$. The covariance matrix is defined as $C_p = E[(\\delta p - E[\\delta p]) (\\delta p - E[\\delta p])^T]$, where $E[\\cdot]$ denotes the expectation value. For perturbations around a central value, it is standard to assume a zero mean, $E[\\delta p] = 0$, in which case $C_p = E[\\delta p \\, \\delta p^T]$.\n\nThe first step is to derive the covariance matrix of the observables, $C_y$, from the given parameter covariance $C_p$ and the linear model. The covariance matrix of $\\delta y$ is defined as:\n$$C_y = E[(\\delta y - E[\\delta y]) (\\delta y - E[\\delta y])^T]$$\nGiven the linear model, the expectation of $\\delta y$ is $E[\\delta y] = E[S \\delta p] = S E[\\delta p]$. Substituting this into the definition of $C_y$:\n$$C_y = E[(S \\delta p - S E[\\delta p]) (S \\delta p - S E[\\delta p])^T]$$\nUsing the properties of the transpose and the linearity of the expectation operator, we can refactor the expression:\n$$C_y = E[S (\\delta p - E[\\delta p]) (S (\\delta p - E[\\delta p]))^T]$$\n$$C_y = E[S (\\delta p - E[\\delta p]) (\\delta p - E[\\delta p])^T S^T]$$\nSince the sensitivity matrix $S$ is constant, it can be factored out of the expectation:\n$$C_y = S E[(\\delta p - E[\\delta p]) (\\delta p - E[\\delta p])^T] S^T$$\nRecognizing the term inside the expectation as the definition of the parameter covariance matrix $C_p$, we arrive at the final expression:\n$$C_y = S C_p S^T$$\nThis $m \\times m$ matrix encapsulates the variance and correlation of the observables induced by the parameter variations. Since $C_p$ is a symmetric positive semidefinite matrix, $C_y$ is also symmetric positive semidefinite. This guarantees that its eigenvalues are real and non-negative.\n\nPrincipal Component Analysis (PCA) of the observables is equivalent to performing a spectral decomposition of their covariance matrix $C_y$. According to the spectral theorem, any real symmetric matrix like $C_y$ can be diagonalized by an orthogonal matrix:\n$$C_y = V \\Lambda V^T$$\nHere, $\\Lambda$ is a diagonal matrix whose entries $\\{\\lambda_i\\}_{i=1}^m$ are the eigenvalues of $C_y$. The columns of the orthogonal matrix $V$ are the corresponding orthonormal eigenvectors $\\{v_i\\}_{i=1}^m$. These eigenvectors represent the principal components: a new basis of orthogonal directions in the observable space. The eigenvalue $\\lambda_i$ is the variance of the observable data when projected onto the principal direction $v_i$.\n\nA parameter degeneracy arises when different combinations of parameter variations lead to nearly indistinguishable changes in the observables. This manifests as a direction in the observable space having near-zero variance. An eigenvalue $\\lambda_i$ of $C_y$ that is zero or close to zero indicates that the corresponding principal component $v_i$ has almost no variance. Such an occurrence is a clear signal of degeneracy, as it implies a constraint or redundancy in how parameter variations propagate to observable variations.\n\nThe problem provides a quantitative criterion for identifying these \"degenerate\" components. Given a threshold $\\tau > 0$, an eigenvalue $\\lambda_i$ is considered to signify a degeneracy if it is exceptionally small relative to the largest eigenvalue, $\\lambda_{\\max} = \\max_j \\lambda_j$. The condition is:\n$$\\lambda_i \\le \\tau \\lambda_{\\max}$$\nThe total number of degeneracies, $D$, is the count of eigenvalues that satisfy this inequality. For this problem, the threshold is given as $\\tau = 10^{-9}$.\n\nThe algorithm to be implemented for each test case is as follows:\n1.  Given the sensitivity matrix $S$ and the parameter covariance matrix $C_p$. In the provided cases, $m=d=6$.\n2.  Compute the observable covariance matrix using the derived formula: $C_y = S C_p S^T$.\n3.  Calculate the eigenvalues of the symmetric matrix $C_y$. Let these be $\\{\\lambda_i\\}_{i=1}^6$.\n4.  Determine the maximum eigenvalue, $\\lambda_{\\max} = \\max_i \\lambda_i$.\n5.  Count the number of eigenvalues $D$ that satisfy $\\lambda_i \\le 10^{-9} \\lambda_{\\max}$. This count is the final result for the test case.\n\nThis procedure will be applied to the four test cases, each designed to probe a different scenario:\n-   **Case 1**: A well-conditioned system where no degeneracies are expected ($D_1=0$).\n-   **Case 2**: A system with an exact linear dependence in the columns of the sensitivity matrix $S_2$. This introduces a rank deficiency, which should result in exactly one zero eigenvalue for $C_y$, leading to $D_2=1$.\n-   **Case 3**: A system where one parameter's variance is artificially suppressed to a very small value ($10^{-12}$). This \"freezing\" of a parameter direction is expected to induce a near-zero variance in the observables, resulting in one very small eigenvalue and $D_3=1$.\n-   **Case 4**: A system with a near-linear dependence in the columns of $S_4$. This is a perturbation of Case 2, designed to be full-rank but ill-conditioned. This should produce one very small, but non-zero, eigenvalue, which should still be small enough to be flagged as a degeneracy, yielding $D_4=1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs PCA-based degeneracy detection on a set of test cases for a \n    linearized model calibration problem.\n    \"\"\"\n    \n    # Define the fixed threshold parameter tau\n    tau = 1e-9\n\n    # Case 1: Well-conditioned, no degeneracy expected\n    S1 = np.array([\n        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.5, 0.2, 0.1, 0.3, 1.0, 0.0],\n        [0.1, 0.4, 0.3, 0.2, 0.0, 1.0]\n    ])\n    Cp1 = np.diag([1.0, 0.8, 1.2, 0.9, 1.1, 1.0])\n    \n    # Case 2: Exact linear dependence in sensitivity matrix\n    S2 = np.array([\n        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.5, 0.2, 0.1, 0.3, 1.0, 1.0],\n        [0.1, 0.4, 0.3, 0.2, 0.0, 0.0]\n    ])\n    Cp2 = np.identity(6)\n    \n    # Case 3: Anisotropic parameter covariance, nearly frozen direction\n    S3 = S1\n    Cp3 = np.diag([1.0, 1.0, 1.0, 1.0, 1.0, 1e-12])\n    \n    # Case 4: Near-degeneracy in sensitivity matrix\n    S4 = np.array([\n        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.5, 0.2, 0.1, 0.3, 1.0, 1.0 + 3e-11],\n        [0.1, 0.4, 0.3, 0.2, 0.0, -3e-11]\n    ])\n    Cp4 = np.identity(6)\n    \n    test_cases = [\n        (S1, Cp1),\n        (S2, Cp2),\n        (S3, Cp3),\n        (S4, Cp4)\n    ]\n    \n    results = []\n    \n    for S, Cp in test_cases:\n        # Step 1: Compute the observable covariance matrix Cy = S * Cp * S^T\n        Cy = S @ Cp @ S.T\n        \n        # Step 2: Compute the eigenvalues of the symmetric matrix Cy.\n        # Use eigvalsh for numerical stability and efficiency with symmetric matrices.\n        eigenvalues = np.linalg.eigvalsh(Cy)\n        \n        # Step 3: Find the maximum eigenvalue.\n        # Check against a small positive number to handle the zero matrix case.\n        lambda_max = np.max(eigenvalues) if eigenvalues.size > 0 else 0.0\n        \n        # Step 4: Count degeneracies based on the criterion.\n        # Ensure lambda_max is not zero to avoid division by zero or spurious results.\n        if lambda_max > 1e-30:  # A small tolerance for a non-zero max eigenvalue\n            degeneracy_count = np.sum(eigenvalues = tau * lambda_max)\n        else:\n            # If all eigenvalues are effectively zero, they are all degenerate.\n            degeneracy_count = len(eigenvalues)\n            \n        results.append(int(degeneracy_count))\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond diagnosing static model structures, PCA is an invaluable tool for analyzing the propagation of correlated uncertainties. In many-body theories, corrections from different orders of a perturbative expansion are often strongly correlated. This exercise  demonstrates how to use PCA to decompose the total uncertainty in a final observable into contributions from orthogonal 'principal correction pathways,' revealing the dominant modes of theoretical error.",
            "id": "3581450",
            "problem": "You are given ensembles of many-body perturbative corrections across truncation orders for a theoretical nuclear model. For each ensemble element indexed by $s \\in \\{0,1,\\dots,N-1\\}$ and order index $n \\in \\{1,2,\\dots,d\\}$, the correction is denoted $ \\Delta E_{s}^{(n)} $. Collect these into a data matrix $ X \\in \\mathbb{R}^{N \\times d} $ with entries $ X_{s,n} = \\Delta E_{s}^{(n)} $. From the data, you are to perform a Principal Component Analysis (PCA) and quantify how uncertainty in the final observable propagates along principal correction pathways.\n\nStart from the foundational definitions:\n- The sample mean vector $ \\mu \\in \\mathbb{R}^{d} $ has components $ \\mu_n = \\frac{1}{N} \\sum_{s=0}^{N-1} X_{s,n} $.\n- The centered data matrix is $ Y = X - \\mathbf{1}\\mu^{\\top} $, where $ \\mathbf{1} \\in \\mathbb{R}^{N} $ is the vector of ones.\n- The unbiased sample covariance matrix is $ \\Sigma = \\frac{1}{N-1} Y^{\\top} Y \\in \\mathbb{R}^{d \\times d} $.\n- Principal Component Analysis diagonalizes $ \\Sigma $ via $ \\Sigma = U \\Lambda U^{\\top} $, where $ U \\in \\mathbb{R}^{d \\times d} $ has orthonormal columns $ u_i $ and $ \\Lambda = \\operatorname{diag}(\\lambda_1,\\dots,\\lambda_d) $ with eigenvalues sorted so that $ \\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_d \\ge 0 $.\n- The explained variance ratio for component $ i $ is $ r_i = \\lambda_i / \\sum_{j=1}^{d} \\lambda_j $ if the denominator is positive, and $ r_i = 0 $ otherwise.\n\nAssume the final observable energy is of the form $ E_{\\mathrm{final}} = E_{\\mathrm{ref}} + \\sum_{n=1}^{d} w_n \\Delta E^{(n)} $, where $ w \\in \\mathbb{R}^{d} $ is a specified sensitivity vector. The propagated variance of the error in the final observable due to the ensemble fluctuations of $ \\Delta E^{(n)} $ is\n$$\n\\sigma^2 = w^{\\top} \\Sigma w.\n$$\nUsing the PCA basis, decompose this variance into contributions from each principal component as\n$$\nc_i = \\begin{cases}\n\\frac{\\lambda_i \\left( u_i^{\\top} w \\right)^2}{\\sigma^2},  \\text{if } \\sigma^2  0, \\\\\n0,  \\text{if } \\sigma^2 = 0,\n\\end{cases}\n$$\nso that $ c_i \\ge 0 $ and $ \\sum_{i=1}^{d} c_i = 1 $ when $ \\sigma^2  0 $. Define the minimal number $ m $ of principal components required to reach a target fraction $ \\tau $ of the propagated variance as the smallest $ m $ such that $ \\sum_{i=1}^{m} c_i \\ge \\tau $ (if $ \\sigma^2 = 0 $, adopt the convention $ m = 0 $ and $ c_i = 0 $ for all $ i $).\n\nYour tasks for each test case are:\n- Construct $ X $ deterministically as specified.\n- Compute $ \\Sigma $ from the centered data.\n- Perform PCA to obtain $ \\lambda_i $ and $ u_i $ sorted by descending $ \\lambda_i $.\n- Compute the explained variance ratios $ r_i $.\n- Compute the propagated standard deviation $ \\sigma = \\sqrt{\\sigma^2} $ in mega-electronvolts (MeV).\n- Compute the contribution fractions $ c_i $ and the minimal $ m $ such that $ \\sum_{i=1}^{m} c_i \\ge \\tau $.\n\nAll floating-point outputs must be rounded to six decimal places. The final standard deviation $ \\sigma $ must be expressed in mega-electronvolts (MeV). Angles in the deterministic constructions use the radian unit. Percentages must be expressed as decimals.\n\nTest Suite:\n- Case A (general, multi-factor structure): $ N = 100 $, $ d = 5 $. For $ s \\in \\{0,1,\\dots,99\\} $ and $ n \\in \\{1,2,3,4,5\\} $, define\n  - $ L_1(s) = \\cos(0.11\\, s) $,\n  - $ L_2(s) = \\sin(0.07\\, s + 0.3) $,\n  - $ T(s) = \\frac{s}{99} - 0.5 $,\n  - $ \\mathrm{noise}(s,n) = 0.05 \\sin(0.31\\, s + 0.77\\, n) $,\n  - amplitudes $ A = [2.0, 1.0, 0.55, 0.3, 0.15] $,\n  - $ X_{s,n} = A_n \\left( L_1(s) + 0.6\\, L_2(s) + 0.2\\, T(s) \\right) + \\mathrm{noise}(s,n) $.\n  Use $ w = [1, 1, 1, 1, 1] $ and $ \\tau = 0.9 $.\n- Case B (highly collinear across orders): $ N = 80 $, $ d = 4 $. For $ s \\in \\{0,1,\\dots,79\\} $ and $ n \\in \\{1,2,3,4\\} $, define\n  - $ H(s) = \\cos(0.05\\, s) + 0.1 \\sin(0.13\\, s) $,\n  - $ \\mathrm{noise}(s,n) = 0.02 \\sin(0.2\\, s + 0.5\\, n) $,\n  - coefficients $ C = [1.5, 0.75, 0.38, 0.19] $,\n  - $ X_{s,n} = C_n\\, H(s) + \\mathrm{noise}(s,n) $.\n  Use $ w = [1, 1, 1, 1] $ and $ \\tau = 0.95 $.\n- Case C (one zero-variance order): $ N = 60 $, $ d = 3 $. For $ s \\in \\{0,1,\\dots,59\\} $ and $ n \\in \\{1,2,3\\} $, define\n  - $ G_1(s) = 0.8 \\cos(0.09\\, s) $,\n  - $ G_2(s) = 0.6 \\sin(0.04\\, s + 0.2) $,\n  - $ X_{s,1} = 1.2\\, G_1(s) + 0.1 \\sin(0.23\\, s) $,\n  - $ X_{s,2} = 0.6\\, G_2(s) + 0.1 \\cos(0.17\\, s) $,\n  - $ X_{s,3} = 0.3 $ (constant across $ s $, hence zero variance after centering).\n  Use $ w = [1, 0.5, 3.0] $ and $ \\tau = 0.8 $.\n\nRequired program output format:\n- For each case, output a list $ [m, \\sigma, c\\_list, r\\_list] $, where $ m $ is an integer, $ \\sigma $ is a float rounded to six decimals (in MeV), $ c\\_list $ is the list of $ d $ floats $ [c_1,\\dots,c_d] $ rounded to six decimals, and $ r\\_list $ is the list of $ d $ floats $ [r_1,\\dots,r_d] $ rounded to six decimals.\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets, for example $ [\\mathrm{caseA\\_result}, \\mathrm{caseB\\_result}, \\mathrm{caseC\\_result}] $.",
            "solution": "The problem requires the application of Principal Component Analysis (PCA) to analyze the propagation of uncertainty in a theoretical nuclear model. Given an ensemble of many-body perturbative corrections, we are to construct a data matrix, perform PCA on its covariance structure, and quantify how the variance of a final observable is distributed across the principal components.\n\nThe solution proceeds through a sequence of well-defined statistical and linear algebraic operations.\n\n**1. Data Specification and Centering**\n\nThe starting point is the data matrix $X \\in \\mathbb{R}^{N \\times d}$, where $X_{s,n}$ represents the $n$-th order perturbative correction for the $s$-th ensemble member. $N$ is the number of ensemble members and $d$ is the maximum order of the perturbation theory.\n\nFirst, we compute the sample mean vector $\\mu \\in \\mathbb{R}^{d}$, which represents the average correction at each order over the entire ensemble. Its components are given by:\n$$\n\\mu_n = \\frac{1}{N} \\sum_{s=0}^{N-1} X_{s,n}\n$$\nThe data is then centered by subtracting this mean from each ensemble member's correction vector. This yields the centered data matrix $Y \\in \\mathbb{R}^{N \\times d}$:\n$$\nY = X - \\mathbf{1}\\mu^{\\top}\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^{N}$ is a column vector of ones. Each row of $Y$ represents the fluctuation of an ensemble member around the mean behavior.\n\n**2. Covariance Matrix and Principal Component Analysis**\n\nThe correlations between the fluctuations at different perturbative orders are captured by the unbiased sample covariance matrix, $\\Sigma \\in \\mathbb{R}^{d \\times d}$:\n$$\n\\Sigma = \\frac{1}{N-1} Y^{\\top} Y\n$$\nThe factor of $\\frac{1}{N-1}$ ensures an unbiased estimate of the true population covariance.\n\nPrincipal Component Analysis (PCA) is performed by finding the eigendecomposition of the symmetric, positive semi-definite covariance matrix $\\Sigma$:\n$$\n\\Sigma = U \\Lambda U^{\\top}\n$$\nHere, $U$ is an orthogonal matrix whose columns $u_i$ are the eigenvectors of $\\Sigma$, and $\\Lambda$ is a diagonal matrix containing the corresponding non-negative eigenvalues $\\lambda_i$. The eigenvectors $u_i$ are the principal components (PCs), which form an orthonormal basis for the space of correction vectors. The eigenvalues $\\lambda_i$ represent the variance of the data projected onto the corresponding principal components. By convention, the eigenvalues are sorted in descending order, $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_d \\ge 0$, so that $u_1$ corresponds to the direction of maximum variance in the data.\n\n**3. Explained Variance Ratio**\n\nThe fraction of the total variance in the dataset captured by the $i$-th principal component is given by the explained variance ratio, $r_i$:\n$$\nr_i = \\frac{\\lambda_i}{\\sum_{j=1}^{d} \\lambda_j}\n$$\nThis is defined only if the total variance, $\\operatorname{Tr}(\\Sigma) = \\sum_{j=1}^{d} \\lambda_j$, is positive. If the total variance is zero (i.e., all data points are identical), then $r_i=0$ for all $i$.\n\n**4. Uncertainty Propagation and Decomposition**\n\nThe final observable, $E_{\\mathrm{final}}$, is a linear combination of the perturbative corrections, weighted by a sensitivity vector $w \\in \\mathbb{R}^{d}$. The variance of $E_{\\mathrm{final}}$ due to the ensemble fluctuations is given by the standard formula for propagation of uncertainty:\n$$\n\\sigma^2 = w^{\\top} \\Sigma w\n$$\nThe quantity $\\sigma = \\sqrt{\\sigma^2}$ is the propagated standard deviation.\n\nTo understand how this total propagated variance is composed, we can decompose it into contributions from each principal component. By substituting $\\Sigma = U \\Lambda U^{\\top} = \\sum_{i=1}^{d} \\lambda_i u_i u_i^{\\top}$ into the expression for $\\sigma^2$, we obtain:\n$$\n\\sigma^2 = w^{\\top} \\left( \\sum_{i=1}^{d} \\lambda_i u_i u_i^{\\top} \\right) w = \\sum_{i=1}^{d} \\lambda_i (w^{\\top} u_i) (u_i^{\\top} w) = \\sum_{i=1}^{d} \\lambda_i (u_i^{\\top} w)^2\n$$\nThe fractional contribution, $c_i$, of the $i$-th principal component to the total propagated variance $\\sigma^2$ is thus:\n$$\nc_i = \\begin{cases}\n\\frac{\\lambda_i \\left( u_i^{\\top} w \\right)^2}{\\sigma^2},  \\text{if } \\sigma^2  0 \\\\\n0,  \\text{if } \\sigma^2 = 0\n\\end{cases}\n$$\nBy construction, these contributions are non-negative and sum to unity, $\\sum_{i=1}^{d} c_i = 1$, provided $\\sigma^2  0$.\n\n**5. Minimal Set of Principal Components**\n\nFinally, we determine the minimal number of leading principal components, $m$, required to account for at least a target fraction $\\tau$ of the total propagated variance. This is the smallest integer $m \\in \\{1, \\dots, d\\}$ such that:\n$$\n\\sum_{i=1}^{m} c_i \\ge \\tau\n$$\nIf $\\sigma^2 = 0$, the contributions $c_i$ are all zero, and we adopt the convention that $m=0$. This value $m$ provides a measure of the effective dimensionality of the uncertainty propagation for the given observable sensitivity $w$.\n\n**Algorithmic Procedure**\n\nFor each test case, the following computational steps are executed:\n1.  The data matrix $X$ is constructed deterministically according to the specified functions for $N$ samples and $d$ dimensions.\n2.  The unbiased sample covariance matrix $\\Sigma$ is computed from $X$.\n3.  The eigenvalues $\\lambda_i$ and eigenvectors $u_i$ of $\\Sigma$ are calculated and sorted in descending order of $\\lambda_i$.\n4.  The explained variance ratios $r_i$ are computed.\n5.  The propagated variance $\\sigma^2$ and standard deviation $\\sigma$ are calculated using the provided sensitivity vector $w$.\n6.  The variance contribution fractions $c_i$ are computed.\n7.  The minimal number of components $m$ is found by identifying the point at which the cumulative sum of the $c_i$ first exceeds the threshold $\\tau$.\n8.  All floating-point results ($\\sigma$, $c_i$, $r_i$) are rounded to six decimal places as required. The final output is assembled into the specified list format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_pca_metrics(X, w, tau):\n    \"\"\"\n    Performs PCA and uncertainty propagation analysis on the data matrix X.\n\n    Args:\n        X (np.ndarray): The N x d data matrix.\n        w (list or np.ndarray): The d-dimensional sensitivity vector.\n        tau (float): The target fraction for cumulative variance contribution.\n\n    Returns:\n        list: A list containing [m, sigma, c_list, r_list] with rounded values.\n    \"\"\"\n    N, d = X.shape\n    w = np.array(w)\n\n    # Handle case with insufficient data for unbiased covariance\n    if N = 1:\n        # Sigma would be undefined or zero.\n        m = 0\n        sigma_rounded = 0.0\n        c_list_rounded = [0.0] * d\n        r_list_rounded = [0.0] * d\n        return [m, sigma_rounded, c_list_rounded, r_list_rounded]\n\n    # Compute the unbiased sample covariance matrix\n    # rowvar=False because columns are variables (orders n)\n    # ddof=1 for the 1/(N-1) factor\n    Sigma = np.cov(X, rowvar=False, ddof=1)\n\n    # Perform eigendecomposition of the covariance matrix\n    # eigh is used for symmetric matrices\n    lambdas, U = np.linalg.eigh(Sigma)\n\n    # Sort eigenvalues and corresponding eigenvectors in descending order\n    sorted_indices = np.argsort(lambdas)[::-1]\n    lambdas = lambdas[sorted_indices]\n    U = U[:, sorted_indices]\n\n    # Compute explained variance ratios r_i\n    total_lambda = np.sum(lambdas)\n    if total_lambda > 1e-12:  # Use a tolerance for floating point comparison\n        r = lambdas / total_lambda\n    else:\n        r = np.zeros(d)\n        \n    # Compute propagated variance sigma^2 and standard deviation sigma\n    sigma_sq = w.T @ Sigma @ w\n    sigma = np.sqrt(sigma_sq) if sigma_sq > 0 else 0.0\n\n    # Compute variance contribution fractions c_i\n    if sigma_sq > 1e-12:\n        # U.T gives eigenvectors as rows, which is convenient for iteration\n        c = np.array([lam * (u.T @ w)**2 / sigma_sq for lam, u in zip(lambdas, U.T)])\n    else:\n        c = np.zeros(d)\n\n    # Find the minimal number of components m\n    if sigma_sq > 1e-12:\n        cumulative_c = np.cumsum(c)\n        # Find the first index where the cumulative sum is >= tau\n        # A small tolerance is added for floating point comparisons\n        m_search = np.where(cumulative_c >= tau - 1e-9)\n        if m_search[0].size > 0:\n            m = m_search[0][0] + 1\n        else:\n            # If tau is not reached (e.g., due to precision with tau=1)\n            # we consider all components to be necessary.\n            m = d\n    else: # sigma_sq is zero\n        m = 0\n\n    # Round all floating point results to six decimal places\n    sigma_rounded = np.round(sigma, 6)\n    c_list_rounded = np.round(c, 6).tolist()\n    r_list_rounded = np.round(r, 6).tolist()\n\n    return [m, sigma_rounded, c_list_rounded, r_list_rounded]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case A: (N, d, w, tau, 'A')\n        (100, 5, [1, 1, 1, 1, 1], 0.9, 'A'),\n        # Case B: (N, d, w, tau, 'B')\n        (80, 4, [1, 1, 1, 1], 0.95, 'B'),\n        # Case C: (N, d, w, tau, 'C')\n        (60, 3, [1, 0.5, 3.0], 0.8, 'C')\n    ]\n\n    results = []\n    for case in test_cases:\n        N, d, w, tau, case_id = case\n        \n        X = np.zeros((N, d))\n        s_vals = np.arange(N)\n\n        if case_id == 'A':\n            A = np.array([2.0, 1.0, 0.55, 0.3, 0.15])\n            L1 = np.cos(0.11 * s_vals)\n            L2 = np.sin(0.07 * s_vals + 0.3)\n            # s/(N-1) for s in {0,...,N-1} spans [0,1]. N-1 is 99.\n            T = s_vals / (N - 1) - 0.5\n            base_structure = L1 + 0.6 * L2 + 0.2 * T\n            for n_idx in range(d):\n                n = n_idx + 1\n                noise = 0.05 * np.sin(0.31 * s_vals + 0.77 * n)\n                X[:, n_idx] = A[n_idx] * base_structure + noise\n\n        elif case_id == 'B':\n            C = np.array([1.5, 0.75, 0.38, 0.19])\n            H = np.cos(0.05 * s_vals) + 0.1 * np.sin(0.13 * s_vals)\n            for n_idx in range(d):\n                n = n_idx + 1\n                noise = 0.02 * np.sin(0.2 * s_vals + 0.5 * n)\n                X[:, n_idx] = C[n_idx] * H + noise\n\n        elif case_id == 'C':\n            G1 = 0.8 * np.cos(0.09 * s_vals)\n            G2 = 0.6 * np.sin(0.04 * s_vals + 0.2)\n            X[:, 0] = 1.2 * G1 + 0.1 * np.sin(0.23 * s_vals)\n            X[:, 1] = 0.6 * G2 + 0.1 * np.cos(0.17 * s_vals)\n            X[:, 2] = 0.3 # Constant value\n\n        result = calculate_pca_metrics(X, w, tau)\n        results.append(result)\n\n    # Format the final output string\n    # str() on a list converts it to '[...]' format, which is what is needed.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The power of PCA often comes from truncationâ€”retaining only the leading components to create a low-dimensional surrogate. However, this simplification is not without consequences, as it can affect the statistical properties of individual observables. This practice  delves into the statistical impact of this truncation, guiding you to derive rigorous bounds on how the variance of a single observable can be distorted, which is a critical step for robust uncertainty quantification.",
            "id": "3581437",
            "problem": "Consider a computational nuclear physics workflow in which a theoretical model predicts a vector of $p$ nuclear observables (for example, a set of binding energies along an isotopic chain). Let the model discrepancy be described by a zero-mean multivariate normal distribution with covariance matrix $\\Sigma \\in \\mathbb{R}^{p \\times p}$. Assume that Principal Component Analysis (PCA) has been performed on $\\Sigma$, yielding the spectral decomposition $\\Sigma = U \\Lambda U^{\\top}$, where $U$ is an orthogonal matrix whose columns are eigenvectors, and $\\Lambda = \\operatorname{diag}(\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{p})$ contains the ordered eigenvalues satisfying $\\lambda_{1} \\geq \\lambda_{2} \\geq \\dots \\geq \\lambda_{p}  0$. In practice, one often truncates to the leading $k$ principal components to report posterior predictive intervals. Consider the following setting:\n\n- The dimension is $p = 5$, with eigenvalues $\\lambda_{1} = 3.2$, $\\lambda_{2} = 1.5$, $\\lambda_{3} = 0.7$, $\\lambda_{4} = 0.3$, $\\lambda_{5} = 0.1$ (all in $\\mathrm{MeV}^{2}$).\n- A truncated PCA approximation retains $k = 3$ components.\n- Focus on a single observable indexed by $j \\in \\{1, \\dots, p\\}$. Write the standard basis vector $e_{j} \\in \\mathbb{R}^{p}$ in the PCA eigenbasis as $e_{j} = \\sum_{i=1}^{p} c_{i} u_{i}$, where $u_{i}$ is the $i$-th eigenvector and $c_{i} = u_{i}^{\\top} e_{j}$. Define the subspace-alignment fraction $A_{\\mathrm{top}} = \\sum_{i=1}^{k} c_{i}^{2}$ and the complementary tail fraction $A_{\\mathrm{tail}} = 1 - A_{\\mathrm{top}}$. Suppose $A_{\\mathrm{top}} = 0.6$.\n\nLet $s_{\\mathrm{true}} = e_{j}^{\\top} \\Sigma e_{j}$ denote the true marginal posterior variance for the observable under the full covariance, and let $s_{\\mathrm{rep}}$ denote the reported marginal variance under the truncated PCA ($k$ components). Define the variance ratio $q = s_{\\mathrm{rep}} / s_{\\mathrm{true}}$. \n\nTask:\n1. Starting from the definitions of the multivariate normal distribution, eigen-decomposition, and linear projection, derive an explicit expression for the nominal $(1-\\alpha)$ posterior coverage of the observable $j$ when intervals are computed using $s_{\\mathrm{rep}}$ while the true variability is $s_{\\mathrm{true}}$. Express this coverage in terms of $q$, the standard normal cumulative distribution function, and the usual quantile $z_{1-\\alpha/2}$.\n2. Using only the ordering of eigenvalues and the given value of $A_{\\mathrm{top}}$, derive tight lower and upper bounds on $q$ that follow from extremal allocations of $c_{i}^{2}$ among the retained and omitted principal components.\n3. Evaluate these bounds numerically for the given eigenvalues and $A_{\\mathrm{top}} = 0.6$. Round your answers to four significant figures. Express the final answer as the pair of bounds on $q$ with no units.",
            "solution": "### Task 1: Derivation of Posterior Coverage\n\nLet the vector of model discrepancies be $\\delta \\in \\mathbb{R}^p$, with $\\delta \\sim \\mathcal{N}(0, \\Sigma)$. We are interested in a single observable, which corresponds to the $j$-th component of this vector, $\\delta_j = e_j^\\top \\delta$. As a linear transformation of a multivariate normal random variable, $\\delta_j$ is itself normally distributed.\nThe mean of $\\delta_j$ is $E[\\delta_j] = E[e_j^\\top \\delta] = e_j^\\top E[\\delta] = e_j^\\top 0 = 0$.\nThe true variance of $\\delta_j$ is given by:\n$$s_{\\mathrm{true}} = \\mathrm{Var}(\\delta_j) = \\mathrm{Var}(e_j^\\top \\delta) = e_j^\\top \\mathrm{Var}(\\delta) e_j = e_j^\\top \\Sigma e_j$$\nSo, the true distribution of the observable's discrepancy is $\\delta_j \\sim \\mathcal{N}(0, s_{\\mathrm{true}})$.\n\nA nominal $(1-\\alpha)$ posterior interval is constructed using the reported variance, $s_{\\mathrm{rep}}$. The interval is given by $[-\\sqrt{s_{\\mathrm{rep}}} z_{1-\\alpha/2}, \\sqrt{s_{\\mathrm{rep}}} z_{1-\\alpha/2}]$, where $z_{1-\\alpha/2}$ is the upper $(1-\\alpha/2)$ quantile of the standard normal distribution $\\mathcal{N}(0,1)$, i.e., $\\Phi(z_{1-\\alpha/2}) = 1-\\alpha/2$, where $\\Phi$ is the standard normal cumulative distribution function (CDF).\n\nThe actual coverage probability is the probability that the true random variable $\\delta_j$ falls within this reported interval:\n$$P(-\\sqrt{s_{\\mathrm{rep}}} z_{1-\\alpha/2} \\leq \\delta_j \\leq \\sqrt{s_{\\mathrm{rep}}} z_{1-\\alpha/2})$$\nTo evaluate this probability, we standardize $\\delta_j$ using its true variance $s_{\\mathrm{true}}$. Let $Z = \\delta_j / \\sqrt{s_{\\mathrm{true}}}$, so $Z \\sim \\mathcal{N}(0,1)$. The inequality becomes:\n$$P(-\\sqrt{s_{\\mathrm{rep}}} z_{1-\\alpha/2} \\leq Z\\sqrt{s_{\\mathrm{true}}} \\leq \\sqrt{s_{\\mathrm{rep}}} z_{1-\\alpha/2})$$\nDividing by $\\sqrt{s_{\\mathrm{true}}}$ inside the probability statement yields:\n$$P\\left(-\\frac{\\sqrt{s_{\\mathrm{rep}}}}{\\sqrt{s_{\\mathrm{true}}}} z_{1-\\alpha/2} \\leq Z \\leq \\frac{\\sqrt{s_{\\mathrm{rep}}}}{\\sqrt{s_{\\mathrm{true}}}} z_{1-\\alpha/2}\\right)$$\nUsing the definition of the variance ratio $q = s_{\\mathrm{rep}} / s_{\\mathrm{true}}$, the expression simplifies to:\n$$P(-\\sqrt{q} z_{1-\\alpha/2} \\leq Z \\leq \\sqrt{q} z_{1-\\alpha/2})$$\nThis probability is evaluated using the CDF of $Z \\sim \\mathcal{N}(0,1)$:\n$$\\mathrm{Coverage} = \\Phi(\\sqrt{q} z_{1-\\alpha/2}) - \\Phi(-\\sqrt{q} z_{1-\\alpha/2})$$\nUsing the symmetry property of the standard normal CDF, $\\Phi(-x) = 1 - \\Phi(x)$, we obtain the final expression for the coverage probability:\n$$\\mathrm{Coverage} = \\Phi(\\sqrt{q} z_{1-\\alpha/2}) - (1 - \\Phi(\\sqrt{q} z_{1-\\alpha/2})) = 2\\Phi(\\sqrt{q} z_{1-\\alpha/2}) - 1$$\n\n### Task 2: Derivation of Bounds on the Variance Ratio $q$\n\nFirst, we express $s_{\\mathrm{true}}$ and $s_{\\mathrm{rep}}$ in terms of the eigenvalues and the projection coefficients.\nThe true variance is $s_{\\mathrm{true}} = e_j^\\top \\Sigma e_j$. Using the spectral decomposition $\\Sigma = \\sum_{i=1}^p \\lambda_i u_i u_i^\\top$:\n$$s_{\\mathrm{true}} = e_j^\\top \\left(\\sum_{i=1}^p \\lambda_i u_i u_i^\\top\\right) e_j = \\sum_{i=1}^p \\lambda_i (e_j^\\top u_i) (u_i^\\top e_j)$$\nWith $c_i = u_i^\\top e_j$, this becomes:\n$$s_{\\mathrm{true}} = \\sum_{i=1}^p \\lambda_i c_i^2$$\nThe truncated approximation of the covariance matrix using $k$ principal components is $\\Sigma_k = \\sum_{i=1}^k \\lambda_i u_i u_i^\\top$. The reported variance is:\n$$s_{\\mathrm{rep}} = e_j^\\top \\Sigma_k e_j = e_j^\\top \\left(\\sum_{i=1}^k \\lambda_i u_i u_i^\\top\\right) e_j = \\sum_{i=1}^k \\lambda_i c_i^2$$\nThe variance ratio is therefore:\n$$q = \\frac{s_{\\mathrm{rep}}}{s_{\\mathrm{true}}} = \\frac{\\sum_{i=1}^k \\lambda_i c_i^2}{\\sum_{i=1}^p \\lambda_i c_i^2} = \\frac{\\sum_{i=1}^k \\lambda_i c_i^2}{\\sum_{i=1}^k \\lambda_i c_i^2 + \\sum_{i=k+1}^p \\lambda_i c_i^2}$$\nThe coefficients $c_i^2$ are non-negative and sum to unity, because $e_j$ is a unit vector: $\\sum_{i=1}^p c_i^2 = \\sum_{i=1}^p (e_j^\\top u_i)^2 = \\|e_j\\|^2 = 1$.\nWe are given the constraints:\n1. $\\sum_{i=1}^k c_i^2 = A_{\\mathrm{top}}$\n2. $\\sum_{i=k+1}^p c_i^2 = A_{\\mathrm{tail}} = 1 - A_{\\mathrm{top}}$\n\nTo find the bounds on $q$, we must find the extremal values of $q$ with respect to the allocation of the \"masses\" $c_i^2$ within their respective sums. Let $N = \\sum_{i=1}^k \\lambda_i c_i^2$ and $D_{\\mathrm{tail}} = \\sum_{i=k+1}^p \\lambda_i c_i^2$. Then $q = N / (N + D_{\\mathrm{tail}})$.\n\nTo find the lower bound $q_{\\min}$, we need to minimize $N$ and maximize $D_{\\mathrm{tail}}$.\n-   To minimize $N = \\sum_{i=1}^k \\lambda_i c_i^2$ subject to $\\sum_{i=1}^k c_i^2 = A_{\\mathrm{top}}$, we must allocate the entire mass $A_{\\mathrm{top}}$ to the smallest eigenvalue in the set $\\{\\lambda_1, \\dots, \\lambda_k\\}$, which is $\\lambda_k$ (since $\\lambda_1 \\geq \\dots \\geq \\lambda_k$). This is achieved by setting $c_k^2 = A_{\\mathrm{top}}$ and $c_i^2=0$ for $i=1, \\dots, k-1$. Thus, $N_{\\min} = \\lambda_k A_{\\mathrm{top}}$.\n-   To maximize $D_{\\mathrm{tail}} = \\sum_{i=k+1}^p \\lambda_i c_i^2$ subject to $\\sum_{i=k+1}^p c_i^2 = A_{\\mathrm{tail}}$, we must allocate the entire mass $A_{\\mathrm{tail}}$ to the largest eigenvalue in the set $\\{\\lambda_{k+1}, \\dots, \\lambda_p\\}$, which is $\\lambda_{k+1}$. This is achieved by setting $c_{k+1}^2 = A_{\\mathrm{tail}}$ and $c_i^2=0$ for $i=k+2, \\dots, p$. Thus, $D_{\\mathrm{tail, max}} = \\lambda_{k+1} A_{\\mathrm{tail}}$.\nThe lower bound is:\n$$q_{\\min} = \\frac{N_{\\min}}{N_{\\min} + D_{\\mathrm{tail, max}}} = \\frac{\\lambda_k A_{\\mathrm{top}}}{\\lambda_k A_{\\mathrm{top}} + \\lambda_{k+1} A_{\\mathrm{tail}}} = \\frac{\\lambda_k A_{\\mathrm{top}}}{\\lambda_k A_{\\mathrm{top}} + \\lambda_{k+1} (1-A_{\\mathrm{top}})}$$\n\nTo find the upper bound $q_{\\max}$, we need to maximize $N$ and minimize $D_{\\mathrm{tail}}$.\n-   To maximize $N = \\sum_{i=1}^k \\lambda_i c_i^2$ subject to $\\sum_{i=1}^k c_i^2 = A_{\\mathrm{top}}$, we allocate the mass $A_{\\mathrm{top}}$ to the largest eigenvalue, $\\lambda_1$. This gives $N_{\\max} = \\lambda_1 A_{\\mathrm{top}}$.\n-   To minimize $D_{\\mathrm{tail}} = \\sum_{i=k+1}^p \\lambda_i c_i^2$ subject to $\\sum_{i=k+1}^p c_i^2 = A_{\\mathrm{tail}}$, we allocate the mass $A_{\\mathrm{tail}}$ to the smallest eigenvalue in the tail, which is $\\lambda_p$. This gives $D_{\\mathrm{tail, min}} = \\lambda_p A_{\\mathrm{tail}}$.\nThe upper bound is:\n$$q_{\\max} = \\frac{N_{\\max}}{N_{\\max} + D_{\\mathrm{tail, min}}} = \\frac{\\lambda_1 A_{\\mathrm{top}}}{\\lambda_1 A_{\\mathrm{top}} + \\lambda_p A_{\\mathrm{tail}}} = \\frac{\\lambda_1 A_{\\mathrm{top}}}{\\lambda_1 A_{\\mathrm{top}} + \\lambda_p (1-A_{\\mathrm{top}})}$$\n\n### Task 3: Numerical Evaluation of Bounds\n\nWe are given the following values:\n-   $p=5$, $k=3$\n-   $\\lambda_1 = 3.2$, $\\lambda_2 = 1.5$, $\\lambda_3 = 0.7$, $\\lambda_4 = 0.3$, $\\lambda_5 = 0.1$\n-   $A_{\\mathrm{top}} = 0.6$, so $A_{\\mathrm{tail}} = 1 - 0.6 = 0.4$\n\nThe relevant eigenvalues for the bounds are:\n-   $\\lambda_1 = 3.2$\n-   $\\lambda_k = \\lambda_3 = 0.7$\n-   $\\lambda_{k+1} = \\lambda_4 = 0.3$\n-   $\\lambda_p = \\lambda_5 = 0.1$\n\nWe can now compute the numerical values for the bounds.\n\nFor the lower bound, $q_{\\min}$:\n$$q_{\\min} = \\frac{\\lambda_3 A_{\\mathrm{top}}}{\\lambda_3 A_{\\mathrm{top}} + \\lambda_4 A_{\\mathrm{tail}}} = \\frac{0.7 \\times 0.6}{0.7 \\times 0.6 + 0.3 \\times 0.4} = \\frac{0.42}{0.42 + 0.12} = \\frac{0.42}{0.54} = \\frac{7}{9}$$\nNumerically, $q_{\\min} \\approx 0.77777...$. Rounding to four significant figures, we get $q_{\\min} = 0.7778$.\n\nFor the upper bound, $q_{\\max}$:\n$$q_{\\max} = \\frac{\\lambda_1 A_{\\mathrm{top}}}{\\lambda_1 A_{\\mathrm{top}} + \\lambda_5 A_{\\mathrm{tail}}} = \\frac{3.2 \\times 0.6}{3.2 \\times 0.6 + 0.1 \\times 0.4} = \\frac{1.92}{1.92 + 0.04} = \\frac{1.92}{1.96} = \\frac{48}{49}$$\nNumerically, $q_{\\max} \\approx 0.9795918...$. Rounding to four significant figures, we get $q_{\\max} = 0.9796$.\n\nThe tight bounds on $q$ are $[0.7778, 0.9796]$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.7778  0.9796\n\\end{pmatrix}\n}\n$$"
        }
    ]
}