{
    "hands_on_practices": [
        {
            "introduction": "A core task in Bayesian analysis is understanding how our prior assumptions influence our conclusions, a concept known as prior sensitivity. This practice explores this concept in the context of a low-count nuclear physics experiment, where the choice of prior is particularly influential. By directly comparing posterior distributions derived from a flat prior and a log-uniform (or Jeffreys) prior, you will gain hands-on experience with numerical Bayesian computation and develop a quantitative intuition for how priors shape inference when data is sparse .",
            "id": "3544521",
            "problem": "Consider a nuclear activation counting experiment in which an observed number of counts $k$ over a counting interval arises from a Poisson process with mean $\\mu = r E + b$, where $r$ is the nonnegative reaction rate to be inferred, $E$ is a known effective exposure (a known positive constant proportional to the product of fluence, efficiency, live time, and number of target nuclei) that maps rate $r$ to expected signal counts $rE$, and $b$ is a known expected background counts contribution (a known nonnegative constant). Assume the likelihood for $k$ given $r$ is the Poisson probability mass function with mean $\\mu = rE + b$. You will compare two priors for $r$: a flat prior and a log-uniform prior, and quantify posterior sensitivity for low-count regimes. The task is to compute posterior probabilities and credible bounds purely from first principles using Bayes’ rule and numerical integration, without using any closed-form posterior expressions.\n\nBase the solution on the following well-tested principles and definitions:\n- The Poisson likelihood for $k$ conditioned on a mean $\\mu$ is $P(k \\mid \\mu) = \\exp(-\\mu)\\,\\mu^{k}/k!$ for nonnegative integer $k$ and $\\mu \\ge 0$.\n- Bayes’ rule states $p(r \\mid k) \\propto P(k \\mid r)\\,p(r)$, with normalization $\\int p(r \\mid k)\\,dr = 1$ over the support of $r$.\n- A flat prior on $r$ over a bounded support $[0, r_{\\max}]$ is $p_{\\mathrm{flat}}(r) \\propto 1$ for $r \\in [0, r_{\\max}]$ and $0$ otherwise.\n- A log-uniform prior (also called a Jeffreys-type scale prior) on $r$ over a bounded support $[r_{\\min}, r_{\\max}]$ is $p_{\\log}(r) \\propto 1/r$ for $r \\in [r_{\\min}, r_{\\max}]$ and $0$ otherwise, which is equivalent to a uniform prior on $\\log r$ between $\\log r_{\\min}$ and $\\log r_{\\max}$.\n\nFor each test case below, you must:\n1. Construct the unnormalized posterior density under each prior:\n   - $p_{\\mathrm{flat}}(r \\mid k) \\propto \\exp\\!\\left(-\\left(rE + b\\right)\\right)\\,\\left(rE + b\\right)^{k}\\,\\mathbf{1}_{[0, r_{\\max}]}(r)$.\n   - $p_{\\log}(r \\mid k) \\propto \\exp\\!\\left(-\\left(rE + b\\right)\\right)\\,\\left(rE + b\\right)^{k}\\,\\dfrac{1}{r}\\,\\mathbf{1}_{[r_{\\min}, r_{\\max}]}(r)$.\n   Here $\\mathbf{1}_{A}(r)$ is the indicator function of set $A$.\n2. Normalize each posterior by numerically evaluating the appropriate integral over $r$ to ensure the posterior integrates to $1$ on its support.\n3. Define the threshold $r_{\\mathrm{thr}} = 1/E$, which corresponds to the signal rate that would yield an expected signal of $1$ count at the given exposure $E$.\n4. Compute the following quantities:\n   - The posterior cumulative probability under the flat prior at $r_{\\mathrm{thr}}$: $F_{\\mathrm{flat}}(r_{\\mathrm{thr}}) = \\int_{0}^{\\min(r_{\\mathrm{thr}}, r_{\\max})} p_{\\mathrm{flat}}(r \\mid k)\\,dr$.\n   - The posterior cumulative probability under the log-uniform prior at $r_{\\mathrm{thr}}$: $F_{\\log}(r_{\\mathrm{thr}}) = \\int_{r_{\\min}}^{\\min(r_{\\mathrm{thr}}, r_{\\max})} p_{\\log}(r \\mid k)\\,dr$, with the understanding that this value is $0$ if $r_{\\mathrm{thr}} < r_{\\min}$.\n   - The sensitivity difference $S = F_{\\log}(r_{\\mathrm{thr}}) - F_{\\mathrm{flat}}(r_{\\mathrm{thr}})$.\n   - The $95$-percent upper credible bound under each prior, defined as the unique values $r^{95}_{\\mathrm{flat}}$ and $r^{95}_{\\log}$ satisfying $\\int_{0}^{r^{95}_{\\mathrm{flat}}} p_{\\mathrm{flat}}(r \\mid k)\\,dr = 0.95$ and $\\int_{r_{\\min}}^{r^{95}_{\\log}} p_{\\log}(r \\mid k)\\,dr = 0.95$, respectively, each constrained to the respective support intervals. Report the ratio $R_{95} = r^{95}_{\\log} / r^{95}_{\\mathrm{flat}}$.\nAll reported quantities ($F_{\\mathrm{flat}}(r_{\\mathrm{thr}})$, $F_{\\log}(r_{\\mathrm{thr}})$, $S$, $R_{95}$) are dimensionless.\n\nNumerical requirements:\n- All integrals must be computed numerically with sufficient accuracy to resolve the results to at least $6$ significant digits.\n- Root-finding for credible bounds must be performed using a robust bracketing method on the support intervals $[0, r_{\\max}]$ for the flat prior and $[r_{\\min}, r_{\\max}]$ for the log-uniform prior.\n\nTest suite:\nUse the following five test cases, each specified by $(k, E, b)$:\n- Case $1$: $(k, E, b) = (0, 10^{5}, 0)$.\n- Case $2$: $(k, E, b) = (1, 10^{5}, 0.5)$.\n- Case $3$: $(k, E, b) = (3, 5\\times 10^{4}, 2)$.\n- Case $4$: $(k, E, b) = (0, 10^{3}, 0)$.\n- Case $5$: $(k, E, b) = (0, 10^{5}, 5)$.\nUse the same bounded supports for all cases: $r_{\\min} = 10^{-12}$ and $r_{\\max} = 10^{-3}$.\n\nFinal output specification:\n- For each case, in the given order, compute and output a sequence of four real numbers: $F_{\\mathrm{flat}}(r_{\\mathrm{thr}})$, $F_{\\log}(r_{\\mathrm{thr}})$, $S$, and $R_{95}$.\n- Your program should produce a single line of output containing all $5$ case results concatenated in order as a comma-separated list enclosed in square brackets. For example, the format is $[x_{1,1}, x_{1,2}, x_{1,3}, x_{1,4}, x_{2,1}, \\dots, x_{5,4}]$, where $x_{i,j}$ denotes the $j$-th quantity for the $i$-th case.\n- All outputs must be decimal numbers (not fractions) and must not include any percentage signs; for example, a cumulative probability of $95$ percent must be printed as $0.95$.",
            "solution": "The problem is valid as it is scientifically grounded in Bayesian statistical inference and nuclear physics, is well-posed with all necessary information provided, and is computationally feasible. The problem asks for the comparison of posterior inferences derived from two different priors (flat and log-uniform) for a reaction rate parameter in a Poisson counting experiment, a standard task in computational physics.\n\nThe solution proceeds by first principles as specified. For each test case defined by the observed counts $k$, effective exposure $E$, and background counts $b$, we construct the posterior probability density functions (PDFs) for the reaction rate $r$. Then, we numerically compute the required quantities: posterior cumulative probabilities, their difference, and a ratio of $95\\%$ credible upper bounds.\n\nThe likelihood of observing $k$ counts for a Poisson process with mean $\\mu = rE + b$ is given by\n$$\nP(k \\mid r, E, b) = \\frac{(rE+b)^k e^{-(rE+b)}}{k!}\n$$\nwhere $r \\ge 0$, $E > 0$, and $b \\ge 0$.\n\nAccording to Bayes' rule, the posterior PDF for $r$ is proportional to the product of the likelihood and the prior PDF, $p(r \\mid k) \\propto P(k \\mid r) p(r)$. The term $1/k!$ is a constant with respect to $r$ and can be absorbed into the normalization constant.\n\nWe consider two different priors for the rate $r$.\n\n1.  **Flat Prior**: The prior is uniform over the interval $[0, r_{\\max}]$.\n    $$\n    p_{\\mathrm{flat}}(r) = \\begin{cases} C_1 & \\text{if } 0 \\le r \\le r_{\\max} \\\\ 0 & \\text{otherwise} \\end{cases}\n    $$\n    where $C_1$ is a normalization constant. The unnormalized posterior is:\n    $$\n    \\tilde{p}_{\\mathrm{flat}}(r \\mid k) = (rE+b)^k e^{-(rE+b)}, \\quad \\text{for } r \\in [0, r_{\\max}]\n    $$\n\n2.  **Log-Uniform Prior**: The prior is proportional to $1/r$ over the interval $[r_{\\min}, r_{\\max}]$. This corresponds to a uniform prior on $\\log r$.\n    $$\n    p_{\\log}(r) = \\begin{cases} C_2/r & \\text{if } r_{\\min} \\le r \\le r_{\\max} \\\\ 0 & \\text{otherwise} \\end{cases}\n    $$\n    where $C_2$ is a normalization constant, and $r_{\\min} > 0$. The unnormalized posterior is:\n    $$\n    \\tilde{p}_{\\log}(r \\mid k) = \\frac{(rE+b)^k e^{-(rE+b)}}{r}, \\quad \\text{for } r \\in [r_{\\min}, r_{\\max}]\n    $$\n\nTo obtain the normalized posterior PDFs, we must compute the normalization constants by integrating the unnormalized posteriors over their respective supports:\n$$\nZ_{\\mathrm{flat}} = \\int_0^{r_{\\max}} \\tilde{p}_{\\mathrm{flat}}(r \\mid k) \\, dr = \\int_0^{r_{\\max}} (rE+b)^k e^{-(rE+b)} \\, dr\n$$\n$$\nZ_{\\log} = \\int_{r_{\\min}}^{r_{\\max}} \\tilde{p}_{\\log}(r \\mid k) \\, dr = \\int_{r_{\\min}}^{r_{\\max}} \\frac{(rE+b)^k e^{-(rE+b)}}{r} \\, dr\n$$\nThese integrals are evaluated numerically. The normalized posterior PDFs are then:\n$$\np_{\\mathrm{flat}}(r \\mid k) = \\frac{1}{Z_{\\mathrm{flat}}} \\tilde{p}_{\\mathrm{flat}}(r \\mid k), \\quad \\text{for } r \\in [0, r_{\\max}]\n$$\n$$\np_{\\log}(r \\mid k) = \\frac{1}{Z_{\\log}} \\tilde{p}_{\\log}(r \\mid k), \\quad \\text{for } r \\in [r_{\\min}, r_{\\max}]\n$$\nThe posterior cumulative distribution function (CDF) for a generic posterior $p(r|k)$ with support $[r_a, r_b]$ is $F(r) = \\int_{r_a}^{r} p(r'|k) dr'$.\n\nThe required quantities for each test case are computed as follows:\n\n1.  **Posterior Cumulative Probability at $r_{\\mathrm{thr}}$**: The threshold rate is $r_{\\mathrm{thr}} = 1/E$. We compute the CDF values at this threshold for both posteriors. The upper integration limit is $\\min(r_{\\mathrm{thr}}, r_{\\max})$.\n    $$\n    F_{\\mathrm{flat}}(r_{\\mathrm{thr}}) = \\int_{0}^{\\min(r_{\\mathrm{thr}}, r_{\\max})} p_{\\mathrm{flat}}(r \\mid k) \\, dr\n    $$\n    $$\n    F_{\\log}(r_{\\mathrm{thr}}) = \\int_{r_{\\min}}^{\\min(r_{\\mathrm{thr}}, r_{\\max})} p_{\\log}(r \\mid k) \\, dr\n    $$\n    For all given test cases, $r_{\\mathrm{thr}} > r_{\\min}$, so the second integral is well-defined over a positive-length interval. These integrals are also computed numerically.\n\n2.  **Sensitivity Difference**: This is the simple arithmetic difference between the two cumulative probabilities.\n    $$\n    S = F_{\\log}(r_{\\mathrm{thr}}) - F_{\\mathrm{flat}}(r_{\\mathrm{thr}})\n    $$\n\n3.  **$95\\%$ Upper Credible Bounds**: The upper bound $r^{95}$ is the value of $r$ for which the CDF is $0.95$. We find $r^{95}_{\\mathrm{flat}}$ and $r^{95}_{\\log}$ by solving the following equations for $r^{95}$:\n    $$\n    \\int_{0}^{r^{95}_{\\mathrm{flat}}} p_{\\mathrm{flat}}(r \\mid k) \\, dr = 0.95\n    $$\n    $$\n    \\int_{r_{\\min}}^{r^{95}_{\\log}}} p_{\\log}(r \\mid k) \\, dr = 0.95\n    $$\n    These equations are solved for $r^{95}_{\\mathrm{flat}} \\in [0, r_{\\max}]$ and $r^{95}_{\\log} \\in [r_{\\min}, r_{\\max}]$ using a numerical root-finding algorithm. Specifically, we find the root of the function $g(x) = F(x) - 0.95 = 0$. Since the CDF $F(x)$ is monotonic, a robust bracketing method like Brent's method is suitable. The search interval is the support of the respective prior.\n\n4.  **Ratio of Upper Bounds**: The final quantity is the ratio of these two credible bounds.\n    $$\n    R_{95} = \\frac{r^{95}_{\\log}}{r^{95}_{\\mathrm{flat}}}\n    $$\n\nThe overall computational procedure for each test case $(k, E, b)$ is:\n-   Define the unnormalized posterior functions $\\tilde{p}_{\\mathrm{flat}}(r \\mid k)$ and $\\tilde{p}_{\\log}(r \\mid k)$.\n-   Compute normalization constants $Z_{\\mathrm{flat}}$ and $Z_{\\log}$ using numerical quadrature (`scipy.integrate.quad`).\n-   Compute $F_{\\mathrm{flat}}(r_{\\mathrm{thr}})$ and $F_{\\log}(r_{\\mathrm{thr}})$ using numerical quadrature.\n-   Calculate $S$.\n-   Set up the CDF equations for the credible bounds and solve for $r^{95}_{\\mathrm{flat}}$ and $r^{95}_{\\log}$ using a numerical root-finder (`scipy.optimize.brentq`).\n-   Calculate $R_{95}$.\n\nThis procedure is implemented in the provided Python script, which iterates through the specified test cases and formats the results as required.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian inference problem for all test cases.\n    \"\"\"\n    \n    # Define the bounded supports for the rate r\n    r_min = 1e-12\n    r_max = 1e-3\n\n    # Define the test cases (k, E, b)\n    test_cases = [\n        (0, 1e5, 0),\n        (1, 1e5, 0.5),\n        (3, 5e4, 2),\n        (0, 1e3, 0),\n        (0, 1e5, 5),\n    ]\n\n    all_results = []\n\n    for k, E, b in test_cases:\n        # Define the threshold rate\n        r_thr = 1.0 / E\n\n        # Define unnormalized posterior functions\n        # The (rE+b)**k term is handled carefully for k=0.\n        # np.power(0, 0) correctly evaluates to 1.\n        def unnormalized_posterior_flat(r, k_val, E_val, b_val):\n            mean_val = r * E_val + b_val\n            # The likelihood is proportional to mean_val**k * exp(-mean_val)\n            # The flat prior is constant, so it's absorbed into normalization\n            return np.power(mean_val, k_val) * np.exp(-mean_val)\n\n        def unnormalized_posterior_log(r, k_val, E_val, b_val):\n            # The log prior is proportional to 1/r\n            if r == 0:\n                # Should not be called at r=0 as support is [r_min, r_max]\n                return 0\n            return unnormalized_posterior_flat(r, k_val, E_val, b_val) / r\n\n        # --- Normalization ---\n        # Integrate the unnormalized posteriors over their supports\n        norm_flat, _ = quad(unnormalized_posterior_flat, 0, r_max, args=(k, E, b))\n        norm_log, _ = quad(unnormalized_posterior_log, r_min, r_max, args=(k, E, b))\n\n        # Define normalized posterior PDFs\n        def pdf_flat(r, k_val, E_val, b_val):\n            return unnormalized_posterior_flat(r, k_val, E_val, b_val) / norm_flat\n\n        def pdf_log(r, k_val, E_val, b_val):\n            return unnormalized_posterior_log(r, k_val, E_val, b_val) / norm_log\n\n        # --- Compute F(r_thr) and S ---\n        integration_upper_bound = min(r_thr, r_max)\n        \n        # F_flat(r_thr)\n        F_flat_thr, _ = quad(pdf_flat, 0, integration_upper_bound, args=(k, E, b))\n        \n        # F_log(r_thr)\n        # Note: Problem states if r_thr < r_min, F_log is 0. Here, r_thr >= r_min for all cases.\n        # quad handles the case where lower bound > upper bound, returning 0.\n        F_log_thr, _ = quad(pdf_log, r_min, integration_upper_bound, args=(k, E, b))\n        \n        # Sensitivity Difference S\n        S = F_log_thr - F_flat_thr\n\n        # --- Compute 95% Upper Credible Bounds and R_95 ---\n        target_quantile = 0.95\n\n        # Function for root-finding: CDF(r) - target_quantile = 0\n        def cdf_minus_target_flat(r_val):\n            integral, _ = quad(pdf_flat, 0, r_val, args=(k, E, b))\n            return integral - target_quantile\n        \n        def cdf_minus_target_log(r_val):\n            integral, _ = quad(pdf_log, r_min, r_val, args=(k, E, b))\n            return integral - target_quantile\n\n        # Use brentq to find the roots (credible bounds)\n        # The function values at the endpoints of the support [a,b] are\n        # F(a)-0.95 = -0.95 and F(b)-0.95 = 1-0.95 = 0.05, so a root is bracketed.\n        r95_flat = brentq(cdf_minus_target_flat, 0, r_max)\n        r95_log = brentq(cdf_minus_target_log, r_min, r_max)\n\n        # Ratio of upper bounds\n        R95 = r95_log / r95_flat\n        \n        # Append results for this case\n        all_results.extend([F_flat_thr, F_log_thr, S, R95])\n\n    # Print results in the specified single-line format\n    print(f\"[{','.join(f'{x:.8f}' for x in all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world experimental data is often imperfect and may include measurements that fall below a detection threshold, known as censored data. This exercise demonstrates how to correctly incorporate such data into a Bayesian model by modifying the likelihood function, a necessary step that often moves the analysis beyond simple conjugate models. You will implement a Bayesian analysis for a Poisson counting process with both fully observed and censored data points, learning how to handle the resulting non-conjugate posterior distribution that requires robust numerical integration .",
            "id": "3544560",
            "problem": "A nuclear counting experiment observes arrivals of independent emission quanta over fixed acquisition gates, which under the assumption of a stationary source are modeled as a Poisson process. Let the count in gate $i$ be the random variable $Y_i$ with $Y_i \\sim \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = r\\,t_i$, $r$ is the unknown constant rate in counts per second ($s^{-1}$), and $t_i$ is the gate duration in seconds. The instrument applies a detection limit, logging a resolved count only when $Y_i \\ge L_i$; otherwise it records a censored event indicating $Y_i < L_i$. To perform calibration with low-count data, we adopt a Gamma prior for the rate, $r \\sim \\text{Gamma}(\\alpha,\\beta)$ in the shape–rate parameterization, with density $p(r) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} r^{\\alpha - 1} e^{-\\beta r}$ for $r > 0$. Given observed counts for those gates where $Y_i \\ge L_i$ and censored observations for those gates where $Y_i < L_i$, derive a Bayesian estimator for the posterior mean of $r$ and compute the posterior tail probability that $r$ exceeds a specified calibration threshold $r_0$. Your derivation must start from the definition of the Poisson process and Bayes’ rule and should not assume conjugacy because censoring breaks the usual Poisson–Gamma conjugacy. You must obtain the posterior normalization by numerical integration over $r \\in (0,\\infty)$.\n\nYour program must:\n- Implement the unnormalized posterior density $p(r \\mid \\text{data}) \\propto p(r)\\,\\prod_{i \\in \\mathcal{O}} \\Pr(Y_i = y_i \\mid r t_i)\\,\\prod_{j \\in \\mathcal{C}} \\Pr(Y_j < L_j \\mid r t_j)$, where $\\mathcal{O}$ is the index set of observed counts with $y_i \\ge L_i$ and $\\mathcal{C}$ is the index set of censored gates.\n- Use numerical quadrature to compute the posterior normalizing constant, the posterior mean $\\mathbb{E}[r \\mid \\text{data}]$, and the tail probability $\\Pr(r > r_0 \\mid \\text{data})$.\n- Express the final answers for rates in counts per second ($s^{-1}$) and tail probabilities as decimals (not percentages).\n\nFundamental base:\n- The Poisson process with rate $r$ implies that for a gate of duration $t$, the count distribution is $Y \\sim \\text{Poisson}(\\lambda)$ with $\\lambda = r t$. Its probability mass function is $\\Pr(Y = y \\mid \\lambda) = e^{-\\lambda}\\frac{\\lambda^y}{y!}$ for integer $y \\ge 0$.\n- For censored gates with detection limit $L$, the likelihood contribution is $\\Pr(Y < L \\mid \\lambda) = \\sum_{k=0}^{L-1} e^{-\\lambda}\\frac{\\lambda^k}{k!}$, which is the Poisson cumulative distribution function evaluated at $L-1$.\n- Bayes’ rule implies $p(r \\mid \\text{data}) \\propto p(r)\\cdot \\text{likelihood}(\\text{data} \\mid r)$.\n\nTest suite:\nProvide a set of three scientifically plausible test cases covering typical and edge conditions. For each case, specify $(\\alpha,\\beta)$, the threshold $r_0$ in $s^{-1}$, the observed counts and gate times for resolved events, and the detection limits and gate times for censored events.\n\n- Case $1$ (mixed resolved and censored, moderate rates): $\\alpha = 2.0$, $\\beta = 10.0$, $r_0 = 0.25$, observed gates: $(t, y)$ pairs $[(10.0, 3), (20.0, 6), (15.0, 4)]$ with $L = 3$ for these; censored gates: $(t, L)$ pairs $[(5.0, 3), (2.0, 3)]$.\n- Case $2$ (all censored, low rates boundary): $\\alpha = 1.5$, $\\beta = 8.0$, $r_0 = 0.20$, observed gates: none; censored gates: $(t, L)$ pairs $[(10.0, 2), (10.0, 2), (10.0, 2)]$.\n- Case $3$ (mostly resolved, higher rates): $\\alpha = 3.0$, $\\beta = 5.0$, $r_0 = 1.0$, observed gates: $(t, y)$ pairs $[(5.0, 10), (5.0, 14)]$ with $L = 3$ for these; censored gates: $(t, L)$ pairs $[(2.0, 5)]$.\n\nRequired outputs per case:\n- The posterior mean $\\mathbb{E}[r \\mid \\text{data}]$ in $s^{-1}$.\n- The posterior tail probability $\\Pr(r > r_0 \\mid \\text{data})$ as a decimal.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists, one list per case, each inner list containing the posterior mean and the tail probability in that order. For example, the format must be like $[[m_1,p_1],[m_2,p_2],[m_3,p_3]]$, where each $m_i$ and $p_i$ is a floating-point number. No other text should be printed.",
            "solution": "The objective is to perform Bayesian inference on the rate parameter $r$ of a Poisson process, given a dataset comprised of both fully observed counts and censored observations. We will derive the posterior distribution for $r$ and use it to compute the posterior mean and a tail probability.\n\n### 1. Bayesian Framework\nThe posterior probability density function (PDF) for the rate $r$, given the observed data, is derived from Bayes' rule:\n$$\np(r \\mid \\text{data}) = \\frac{p(\\text{data} \\mid r) \\, p(r)}{p(\\text{data})} \\propto p(\\text{data} \\mid r) \\, p(r)\n$$\nwhere $p(r)$ is the prior distribution of the rate, $p(\\text{data} \\mid r)$ is the likelihood of observing the data given the rate, and the denominator $p(\\text{data})$ is the marginal likelihood or evidence, which acts as a normalization constant. Our analysis will focus on the unnormalized posterior, $\\tilde{p}(r \\mid \\text{data}) \\propto p(\\text{data} \\mid r) \\, p(r)$.\n\n### 2. Prior Distribution\nAs specified, the rate parameter $r$ is assigned a Gamma prior distribution, $r \\sim \\text{Gamma}(\\alpha, \\beta)$, in the shape-rate parameterization. The PDF is given by:\n$$\np(r) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} r^{\\alpha - 1} e^{-\\beta r} \\quad \\text{for } r > 0\n$$\nwhere $\\alpha$ is the shape parameter and $\\beta$ is the rate parameter.\n\n### 3. Likelihood Function\nThe experimental data consists of two distinct sets of independent observations:\n1.  A set of resolved counts, $\\mathcal{O} = \\{ (t_i, y_i) \\}$, where the count $y_i$ in a gate of duration $t_i$ is known and $y_i \\ge L_i$.\n2.  A set of censored gates, $\\mathcal{C} = \\{ (t_j, L_j) \\}$, where the count $Y_j$ in a gate of duration $t_j$ is only known to be less than the detection limit $L_j$.\n\nThe total likelihood function $\\mathcal{L}(\\text{data} \\mid r)$ is the product of the probabilities of each independent observation:\n$$\n\\mathcal{L}(\\text{data} \\mid r) = \\left( \\prod_{i \\in \\mathcal{O}} \\Pr(Y_i = y_i \\mid r) \\right) \\left( \\prod_{j \\in \\mathcal{C}} \\Pr(Y_j < L_j \\mid r) \\right)\n$$\n\nFor a resolved count $y_i$ from a gate of duration $t_i$, the count $Y_i$ follows a Poisson distribution with mean $\\lambda_i = r t_i$. The probability is given by the Poisson probability mass function (PMF):\n$$\n\\Pr(Y_i = y_i \\mid r) = \\frac{(r t_i)^{y_i} e^{-r t_i}}{y_i!}\n$$\n\nFor a censored observation from a gate of duration $t_j$ with limit $L_j$, we know that the count $Y_j < L_j$. The probability of this event is the sum of the Poisson probabilities for all integer counts from $0$ to $L_j-1$:\n$$\n\\Pr(Y_j < L_j \\mid r) = \\sum_{k=0}^{L_j-1} \\Pr(Y_j = k \\mid r) = \\sum_{k=0}^{L_j-1} \\frac{(r t_j)^k e^{-r t_j}}{k!}\n$$\nThis expression is the cumulative distribution function (CDF) of the Poisson distribution with mean $\\lambda_j = r t_j$, evaluated at the integer $L_j-1$. We denote this as $F_{\\text{Poisson}}(L_j-1; r t_j)$.\n\n### 4. Unnormalized Posterior Distribution\nThe unnormalized posterior density, $\\tilde{p}(r \\mid \\text{data})$, is proportional to the product of the prior and the likelihood. We can drop any factors that do not depend on $r$.\n$$\n\\tilde{p}(r \\mid \\text{data}) \\propto (r^{\\alpha - 1} e^{-\\beta r}) \\times \\left( \\prod_{i \\in \\mathcal{O}} (r t_i)^{y_i} e^{-r t_i} \\right) \\times \\left( \\prod_{j \\in \\mathcal{C}} F_{\\text{Poisson}}(L_j-1; r t_j) \\right)\n$$\nTo facilitate numerical computation, it is more stable to work with the logarithm of the posterior. The log-unnormalized posterior is:\n$$\n\\ln \\tilde{p}(r \\mid \\text{data}) = \\ln p(r) + \\sum_{i \\in \\mathcal{O}} \\ln \\Pr(Y_i = y_i \\mid r) + \\sum_{j \\in \\mathcal{C}} \\ln \\Pr(Y_j < L_j \\mid r) + \\text{const.}\n$$\nSubstituting the expressions and grouping terms involving $r$:\n$$\n\\ln \\tilde{p}(r \\mid \\text{data}) = (\\alpha-1)\\ln r - \\beta r + \\sum_{i \\in \\mathcal{O}}(y_i \\ln r - r t_i) + \\sum_{j \\in \\mathcal{C}} \\ln F_{\\text{Poisson}}(L_j-1; r t_j) + \\text{const.}\n$$\nLet $Y_{obs} = \\sum_{i \\in \\mathcal{O}} y_i$ and $T_{obs} = \\sum_{i \\in \\mathcal{O}} t_i$. The expression simplifies to:\n$$\n\\ln \\tilde{p}(r \\mid \\text{data}) = (\\alpha + Y_{obs} - 1)\\ln r - (\\beta + T_{obs})r + \\sum_{j \\in \\mathcal{C}} \\ln F_{\\text{Poisson}}(L_j-1; r t_j)\n$$\nThe term $\\ln F_{\\text{Poisson}}(k; \\lambda)$ is implemented in scientific libraries as a log-CDF function, which internally computes $\\ln(\\sum_{i=0}^k e^{-\\lambda} \\lambda^i/i!) = -\\lambda + \\ln(\\sum_{i=0}^k \\lambda^i/i!)$. The exponential terms from the censored data are thus correctly incorporated by using the log-CDF.\n\n### 5. Computation of Posterior Quantities\nThe normalized posterior density is $p(r \\mid \\text{data}) = \\tilde{p}(r \\mid \\text{data}) / Z$, where the normalization constant (evidence) is $Z = \\int_0^\\infty \\tilde{p}(r \\mid \\text{data}) \\, dr$. The quantities of interest are computed as ratios of integrals, which makes them independent of the normalization of $\\tilde{p}(r \\mid \\text{data})$.\n\nThe posterior mean of $r$ is its expectation with respect to the posterior distribution:\n$$\n\\mathbb{E}[r \\mid \\text{data}] = \\int_0^\\infty r \\, p(r \\mid \\text{data}) \\, dr = \\frac{\\int_0^\\infty r \\, \\tilde{p}(r \\mid \\text{data}) \\, dr}{\\int_0^\\infty \\tilde{p}(r \\mid \\text{data}) \\, dr}\n$$\n\nThe posterior tail probability that $r$ exceeds a threshold $r_0$ is:\n$$\n\\Pr(r > r_0 \\mid \\text{data}) = \\int_{r_0}^\\infty p(r \\mid \\text{data}) \\, dr = \\frac{\\int_{r_0}^\\infty \\tilde{p}(r \\mid \\text{data}) \\, dr}{\\int_0^\\infty \\tilde{p}(r \\mid \\text{data}) \\, dr}\n$$\n\n### 6. Numerical Strategy\nThe presence of the Poisson CDF terms breaks the conjugacy of the Gamma-Poisson model, and a closed-form solution for these integrals is not available. We thus employ numerical quadrature. The computational procedure is:\n1.  Implement a function for the log-unnormalized posterior, $\\ln \\tilde{p}(r \\mid \\text{data})$, using robust library functions for the log-CDF of the Poisson distribution.\n2.  To enhance numerical stability for the integration, especially to avoid underflow from exponentiating large negative numbers, we scale the integrand. We first find the mode of the posterior, $r_{MAP}$, by numerically maximizing $\\ln \\tilde{p}(r \\mid \\text{data})$. Let the maximum value be $L_{max} = \\ln \\tilde{p}(r_{MAP} \\mid \\text{data})$.\n3.  A scaled, numerically stable, unnormalized posterior is defined as $\\tilde{p}_{scaled}(r) = \\exp(\\ln \\tilde{p}(r \\mid \\text{data}) - L_{max})$. This function is guaranteed to have a maximum value of $1$.\n4.  The required integrals are computed using $\\tilde{p}_{scaled}(r)$. The scaling constant $e^{-L_{max}}$ cancels in the ratios for the posterior mean and tail probability. For instance, the posterior mean is computed as:\n$$\n\\mathbb{E}[r \\mid \\text{data}] = \\frac{\\int_0^\\infty r \\, \\tilde{p}_{scaled}(r) \\, dr}{\\int_0^\\infty \\tilde{p}_{scaled}(r) \\, dr}\n$$\nThis approach ensures accurate computation of the posterior quantities.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import poisson\nfrom scipy.integrate import quad\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Main solver function that processes test cases and prints the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"alpha\": 2.0, \"beta\": 10.0, \"r0\": 0.25,\n            \"observed\": [(10.0, 3), (20.0, 6), (15.0, 4)],\n            \"censored\": [(5.0, 3), (2.0, 3)],\n        },\n        {\n            \"alpha\": 1.5, \"beta\": 8.0, \"r0\": 0.20,\n            \"observed\": [],\n            \"censored\": [(10.0, 2), (10.0, 2), (10.0, 2)],\n        },\n        {\n            \"alpha\": 3.0, \"beta\": 5.0, \"r0\": 1.0,\n            \"observed\": [(5.0, 10), (5.0, 14)],\n            \"censored\": [(2.0, 5)],\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Calculate the posterior mean and tail probability for each case\n        result = _process_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _process_case(case_data):\n    \"\"\"\n    Computes the posterior mean and tail probability for a single case.\n    \"\"\"\n    alpha = case_data[\"alpha\"]\n    beta = case_data[\"beta\"]\n    r0 = case_data[\"r0\"]\n    observed = case_data[\"observed\"]\n    censored = case_data[\"censored\"]\n\n    # Pre-compute sums from the data for efficiency\n    Y_obs = sum(y for t, y in observed)\n    T_obs = sum(t for t, y in observed)\n\n    def log_unnorm_posterior(r):\n        \"\"\"\n        Computes the log of the unnormalized posterior density.\n        The function is proportional to p(r) * L(data|r).\n        Constant terms independent of r are dropped.\n        \"\"\"\n        if r <= 0:\n            return -np.inf\n        \n        # Contribution from prior and observed data likelihood\n        log_p = (alpha + Y_obs - 1) * np.log(r) - (beta + T_obs) * r\n        \n        # Contribution from censored data likelihood\n        for t_j, L_j in censored:\n            lambda_j = r * t_j\n            # The likelihood contribution is P(Y < L_j) = P(Y <= L_j - 1),\n            # which is the Poisson CDF evaluated at L_j - 1.\n            # Using logcdf provides better numerical stability.\n            log_p += poisson.logcdf(L_j - 1, mu=lambda_j)\n            \n        return log_p\n\n    # Find the posterior mode to determine a scaling factor for numerical integration.\n    # This scaling prevents numerical underflow in the integrator.\n    if T_obs > 0 and Y_obs > 0:\n        crude_rate_est = Y_obs / T_obs\n    else:\n        crude_rate_est = alpha / beta\n    \n    # Use a bounded optimizer to find the maximum of the log posterior (minimum of its negative).\n    opt_res = minimize_scalar(\n        lambda r: -log_unnorm_posterior(r),\n        bounds=(0, crude_rate_est * 20 + 1), # A generous but safe upper bound.\n        method='bounded'\n    )\n    \n    log_max_val = -opt_res.fun\n\n    # Define the scaled posterior to have a maximum value of 1.\n    def scaled_posterior(r):\n        val = log_unnorm_posterior(r)\n        if np.isneginf(val):\n            return 0.0\n        return np.exp(val - log_max_val)\n\n    # Define the integrand for calculating the numerator of the posterior mean.\n    def mean_integrand(r):\n        return r * scaled_posterior(r)\n\n    # Perform numerical integrations over r in (0, inf).\n    # The 'limit' parameter is increased for potentially complex integrands.\n    Z, _ = quad(scaled_posterior, 0, np.inf, limit=100)\n    mean_num, _ = quad(mean_integrand, 0, np.inf, limit=100)\n    tail_num, _ = quad(scaled_posterior, r0, np.inf, limit=100)\n\n    # Calculate final results by taking ratios of the integrals.\n    posterior_mean = mean_num / Z\n    tail_probability = tail_num / Z\n\n    return [posterior_mean, tail_probability]\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Fitting a model is only part of a Bayesian analysis; we must also assess whether the model provides a good description of the data and generates realistic predictions. This practice introduces posterior predictive checking (PPC), a powerful technique for model validation that involves simulating replicated data from the fitted model and comparing it to the observed data. By implementing several standard PPC diagnostics for a surrogate optical model potential, you will learn the essential workflow for critically evaluating and validating your Bayesian models .",
            "id": "3544558",
            "problem": "You will implement posterior predictive checks for a surrogate of nucleon–nucleus elastic-scattering optical model potentials across laboratory energy. The goal is to quantify calibration and coverage of posterior predictive distributions across energy ranges using a mathematically defined discrepancy measure and uniformity diagnostics. All computations must be fully specified and reproducible.\n\nFundamental base and model. Assume a linear-Gaussian surrogate for an energy-dependent observable (e.g., a differential elastic cross section at fixed angle) in millibarns per steradian. Let the laboratory energies be $E_{j}$ (in $\\text{MeV}$) for $j \\in \\{1,\\dots,n\\}$. Define the regressor vector at energy $E$ as\n$$\n\\mathbf{x}(E) = \\begin{bmatrix} 1 \\\\ E^{-1} \\\\ \\exp(-E/E_{0}) \\end{bmatrix},\n$$\nwith known scale $E_{0} > 0$. Let $\\boldsymbol{\\theta} \\in \\mathbb{R}^{3}$ denote unknown regression coefficients. The observation model is\n$$\ny_{j} = \\mathbf{x}(E_{j})^{\\top} \\boldsymbol{\\theta} + \\varepsilon_{j}, \\quad \\varepsilon_{j} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^{2}),\n$$\nwhere $\\sigma > 0$ is known and $y_{j}$ is the observed cross section (in $\\text{mb}\\,\\text{sr}^{-1}$). Place a Gaussian prior on $\\boldsymbol{\\theta}$:\n$$\n\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\mathbf{m}_{0}, \\mathbf{V}_{0}),\n$$\nwith $\\mathbf{V}_{0}$ a diagonal covariance matrix with strictly positive diagonal entries. Let $\\mathbf{X} \\in \\mathbb{R}^{n \\times 3}$ be the design matrix with rows $\\mathbf{x}(E_{j})^{\\top}$ and $\\mathbf{y} \\in \\mathbb{R}^{n}$ the data vector. Under this conjugate linear-Gaussian model, the posterior is\n$$\n\\boldsymbol{\\theta} \\mid \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{m}_{n}, \\mathbf{V}_{n}), \\quad \\mathbf{V}_{n} = \\left(\\mathbf{V}_{0}^{-1} + \\frac{1}{\\sigma^{2}} \\mathbf{X}^{\\top} \\mathbf{X}\\right)^{-1}, \\quad \\mathbf{m}_{n} = \\mathbf{V}_{n} \\left(\\mathbf{V}_{0}^{-1} \\mathbf{m}_{0} + \\frac{1}{\\sigma^{2}} \\mathbf{X}^{\\top} \\mathbf{y}\\right).\n$$\nThe posterior predictive distribution at energy $E_{j}$ is Gaussian:\n$$\ny_{j}^{\\ast} \\mid \\mathbf{y} \\sim \\mathcal{N}\\!\\left(\\mu_{j}^{\\text{pred}}, \\, s_{j}^{2,\\text{pred}}\\right), \\quad \\mu_{j}^{\\text{pred}} = \\mathbf{x}(E_{j})^{\\top} \\mathbf{m}_{n}, \\quad s_{j}^{2,\\text{pred}} = \\sigma^{2} + \\mathbf{x}(E_{j})^{\\top} \\mathbf{V}_{n} \\mathbf{x}(E_{j}).\n$$\n\nDiscrepancy measure for posterior predictive checks. Define a discrepancy indexed by a parameter draw $\\boldsymbol{\\theta}$:\n$$\nT(\\mathbf{y}, \\boldsymbol{\\theta}) = \\sum_{j=1}^{n} \\frac{\\left(y_{j} - \\mathbf{x}(E_{j})^{\\top} \\boldsymbol{\\theta}\\right)^{2}}{\\sigma^{2}},\n$$\nand, for a posterior predictive replicate $\\tilde{\\mathbf{y}}$ drawn at the same energies, define\n$$\nT(\\tilde{\\mathbf{y}}, \\boldsymbol{\\theta}) = \\sum_{j=1}^{n} \\frac{\\left(\\tilde{y}_{j} - \\mathbf{x}(E_{j})^{\\top} \\boldsymbol{\\theta}\\right)^{2}}{\\sigma^{2}}.\n$$\nThe posterior predictive $p$-value is\n$$\np_{B} = \\mathbb{P}\\!\\left( T(\\tilde{\\mathbf{y}}, \\boldsymbol{\\theta}) \\ge T(\\mathbf{y}, \\boldsymbol{\\theta}) \\mid \\mathbf{y} \\right),\n$$\napproximated via Monte Carlo by drawing $\\boldsymbol{\\theta}^{(i)} \\sim \\mathcal{N}(\\mathbf{m}_{n}, \\mathbf{V}_{n})$ and $\\tilde{\\mathbf{y}}^{(i)} \\mid \\boldsymbol{\\theta}^{(i)} \\sim \\mathcal{N}(\\mathbf{X}\\boldsymbol{\\theta}^{(i)}, \\sigma^{2}\\mathbf{I}_{n})$, then averaging indicators $\\mathbb{I}\\{T(\\tilde{\\mathbf{y}}^{(i)}, \\boldsymbol{\\theta}^{(i)}) \\ge T(\\mathbf{y}, \\boldsymbol{\\theta}^{(i)})\\}$.\n\nCoverage and calibration across energy. For a nominal central predictive interval level $c \\in (0,1)$, define the one-sided tail probability $\\alpha = (1-c)/2$. The central predictive interval at energy $E_{j}$ is\n$$\n\\left[\\mu_{j}^{\\text{pred}} - z_{1-\\alpha} s_{j}^{\\text{pred}},\\ \\mu_{j}^{\\text{pred}} + z_{1-\\alpha} s_{j}^{\\text{pred}}\\right],\n$$\nwhere $z_{q}$ is the $q$-quantile of the standard normal distribution. The empirical coverage at level $c$ is the fraction of $j \\in \\{1,\\dots,n\\}$ for which $y_{j}$ lies inside this interval.\n\nCalibration via the probability integral transform. Define the probability integral transform for each energy $E_{j}$ as\n$$\nu_{j} = \\Phi\\!\\left(\\frac{y_{j} - \\mu_{j}^{\\text{pred}}}{s_{j}^{\\text{pred}}}\\right),\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function. Under well-calibrated predictions, the multiset $\\{u_{j}\\}_{j=1}^{n}$ is approximately independent and identically distributed Uniform on $[0,1]$. Quantify deviation from uniformity using the Kolmogorov–Smirnov statistic\n$$\nD_{n} = \\sup_{t \\in [0,1]} \\left| \\hat{F}_{n}(t) - t \\right|,\n$$\nwhere $\\hat{F}_{n}$ is the empirical cumulative distribution function of $\\{u_{j}\\}_{j=1}^{n}$.\n\nData generation and units. For each test case below, generate synthetic data from a fixed “true” parameter vector $\\boldsymbol{\\theta}^{\\text{true}}$ and the specified noise level $\\sigma$. Specifically, for each case, define $n$ energies $E_{j}$ equally spaced from $E_{\\min}$ to $E_{\\max}$ (both in $\\text{MeV}$), construct $\\mathbf{X}$, compute the noise-free mean $\\mathbf{X}\\boldsymbol{\\theta}^{\\text{true}}$, and add Gaussian noise with standard deviation $\\sigma$ to obtain $\\mathbf{y}$. Cross sections $y_{j}$ are in $\\text{mb}\\,\\text{sr}^{-1}$; energies $E_{j}$ are in $\\text{MeV}$. The required outputs are dimensionless decimals (no physical units). Angles do not appear. All random number generation must be reproducible using the specified seeds.\n\nTest suite. Use the following $3$ cases, each with energies $E_{j}$ on an equally spaced grid from $E_{\\min}$ to $E_{\\max}$ with $n$ points and the same scale $E_{0}$. For all cases, set $E_{0} = 20\\,\\text{MeV}$ and $\\boldsymbol{\\theta}^{\\text{true}} = \\begin{bmatrix} 10 \\\\ 350 \\\\ 90 \\end{bmatrix}$ (which yields a physically plausible decreasing trend with energy). For the Monte Carlo approximation of $p_{B}$, use $M = 8000$ posterior draws. Use a NumPy random number generator with a fixed seed for data simulation and a separate fixed seed for posterior predictive simulation as specified.\n\n- Case A (happy path): $n = 21$, $E_{\\min} = 5\\,\\text{MeV}$, $E_{\\max} = 40\\,\\text{MeV}$, $\\sigma = 3\\,\\text{mb}\\,\\text{sr}^{-1}$, prior mean $\\mathbf{m}_{0} = \\begin{bmatrix} 12 \\\\ 300 \\\\ 100 \\end{bmatrix}$, prior standard deviations $\\begin{bmatrix} 30 \\\\ 100 \\\\ 50 \\end{bmatrix}$ (so $\\mathbf{V}_{0} = \\mathrm{diag}(30^{2}, 100^{2}, 50^{2})$). Data-generation seed $s_{\\text{data}} = 2468$, posterior-predictive seed $s_{\\text{ppc}} = 112233$.\n\n- Case B (boundary undercoverage via tight biased prior and low noise): $n = 21$, $E_{\\min} = 5\\,\\text{MeV}$, $E_{\\max} = 40\\,\\text{MeV}$, $\\sigma = 1\\,\\text{mb}\\,\\text{sr}^{-1}$, prior mean $\\mathbf{m}_{0} = \\begin{bmatrix} 30 \\\\ 150 \\\\ 30 \\end{bmatrix}$, prior standard deviations $\\begin{bmatrix} 5 \\\\ 20 \\\\ 5 \\end{bmatrix}$ (so $\\mathbf{V}_{0} = \\mathrm{diag}(5^{2}, 20^{2}, 5^{2})$). Data-generation seed $s_{\\text{data}} = 1357$, posterior-predictive seed $s_{\\text{ppc}} = 223344$.\n\n- Case C (edge case with high noise and diffuse prior): $n = 21$, $E_{\\min} = 5\\,\\text{MeV}$, $E_{\\max} = 40\\,\\text{MeV}$, $\\sigma = 8\\,\\text{mb}\\,\\text{sr}^{-1}$, prior mean $\\mathbf{m}_{0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, prior standard deviations $\\begin{bmatrix} 100 \\\\ 300 \\\\ 120 \\end{bmatrix}$ (so $\\mathbf{V}_{0} = \\mathrm{diag}(100^{2}, 300^{2}, 120^{2})$). Data-generation seed $s_{\\text{data}} = 9876$, posterior-predictive seed $s_{\\text{ppc}} = 334455$.\n\nComputational requirements. For each case, do the following.\n\n- Construct $E_{j}$ for $j \\in \\{1,\\dots,n\\}$ as an equally spaced grid from $E_{\\min}$ to $E_{\\max}$ (inclusive) in $\\text{MeV}$. Form $\\mathbf{X}$ with rows $\\mathbf{x}(E_{j})^{\\top}$. Generate $\\mathbf{y}$ using the specified $s_{\\text{data}}$ and the true parameter $\\boldsymbol{\\theta}^{\\text{true}}$ and noise $\\sigma$.\n\n- Compute $(\\mathbf{m}_{n}, \\mathbf{V}_{n})$.\n\n- Compute the posterior predictive mean $\\mu_{j}^{\\text{pred}}$ and variance $s_{j}^{2,\\text{pred}}$ for each $j$.\n\n- Compute the posterior predictive $p$-value $p_{B}$ using $M$ draws and the specified $s_{\\text{ppc}}$.\n\n- Compute the empirical coverage at the central level $c = 0.9$ using the analytic Gaussian predictive intervals, i.e., with $z_{0.95}$.\n\n- Compute the Kolmogorov–Smirnov statistic $D_{n}$ for the probability integral transforms $u_{j}$, using the exact Gaussian predictive distributions.\n\nNumerical output specification. Your program must produce a single line of output containing the $9$ numbers\n$$\n\\left[p_{B}^{(A)},\\ \\text{cov}_{0.9}^{(A)},\\ D_{n}^{(A)},\\ p_{B}^{(B)},\\ \\text{cov}_{0.9}^{(B)},\\ D_{n}^{(B)},\\ p_{B}^{(C)},\\ \\text{cov}_{0.9}^{(C)},\\ D_{n}^{(C)}\\right],\n$$\nrounded to $6$ decimal places, where the superscripts denote the test cases A, B, and C. Each is a dimensionless decimal number. The program must print exactly this single list as a comma-separated sequence enclosed in square brackets, with no additional text.\n\nAngle units are not used. Energies must be handled in $\\text{MeV}$ and cross sections in $\\text{mb}\\,\\text{sr}^{-1}$ internally, but the final outputs are dimensionless decimals as specified.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result}_{1},\\text{result}_{2},\\text{result}_{3},\\dots]$).",
            "solution": "The problem requires the implementation of a series of posterior predictive checks for a Bayesian linear-Gaussian surrogate model of nucleon–nucleus elastic-scattering data. The goal is to compute three diagnostic quantities for three distinct test cases: a posterior predictive $p$-value ($p_B$), the empirical coverage of $90\\%$ predictive intervals ($\\text{cov}_{0.9}$), and the Kolmogorov-Smirnov statistic for uniformity of probability integral transforms ($D_n$). The solution adheres to a principled, step-by-step computational procedure based on the provided model and definitions.\n\nThe underlying model is a Bayesian linear regression. The observable $y_j$ at energy $E_j$ is modeled as a linear function of a regressor vector $\\mathbf{x}(E_j) = [1, E^{-1}, \\exp(-E/E_0)]^\\top$ with coefficients $\\boldsymbol{\\theta}$, plus Gaussian noise. The model is specified as:\n$$y_j = \\mathbf{x}(E_j)^\\top \\boldsymbol{\\theta} + \\varepsilon_j, \\quad \\varepsilon_j \\sim \\mathcal{N}(0, \\sigma^2)$$\nA conjugate Gaussian prior is placed on the coefficients: $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\mathbf{m}_0, \\mathbf{V}_0)$. The problem provides the analytical forms for the posterior distribution $\\boldsymbol{\\theta} | \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{m}_n, \\mathbf{V}_n)$ and the posterior predictive distribution $y_j^* | \\mathbf{y} \\sim \\mathcal{N}(\\mu_j^{\\text{pred}}, s_j^{2, \\text{pred}})$.\n\nThe computational workflow for each test case is as follows:\n\n1.  **Data Generation and Model Setup**: For each case, we first establish the experimental conditions. The laboratory energies $E_j$ are generated as a grid of $n=21$ points equally spaced between $E_{\\min}=5\\,\\text{MeV}$ and $E_{\\max}=40\\,\\text{MeV}$. Using the given scale $E_0 = 20\\,\\text{MeV}$, the $n \\times 3$ design matrix $\\mathbf{X}$ is constructed, where the $j$-th row is $\\mathbf{x}(E_j)^\\top$. Synthetic data for the cross section, $\\mathbf{y}$, are then generated. This is achieved by computing the true mean values $\\mathbf{X}\\boldsymbol{\\theta}^{\\text{true}}$ with the provided $\\boldsymbol{\\theta}^{\\text{true}} = [10, 350, 90]^\\top$ and adding Gaussian noise with standard deviation $\\sigma$, specific to each case. A fixed random seed ($s_{\\text{data}}$) ensures that this data generation process is reproducible.\n\n2.  **Posterior Parameter Inference**: With the data $\\mathbf{y}$ and design matrix $\\mathbf{X}$ established, we compute the parameters of the posterior distribution of $\\boldsymbol{\\theta}$. The prior mean $\\mathbf{m}_0$ and diagonal covariance matrix $\\mathbf{V}_0$ (constructed from the given prior standard deviations) are used alongside the known noise variance $\\sigma^2$ and the data. The posterior covariance $\\mathbf{V}_n$ and mean $\\mathbf{m}_n$ are calculated using the provided formulas:\n    $$ \\mathbf{V}_n = \\left(\\mathbf{V}_0^{-1} + \\frac{1}{\\sigma^2} \\mathbf{X}^{\\top} \\mathbf{X}\\right)^{-1} $$\n    $$ \\mathbf{m}_n = \\mathbf{V}_n \\left(\\mathbf{V}_0^{-1} \\mathbf{m}_0 + \\frac{1}{\\sigma^2} \\mathbf{X}^{\\top} \\mathbf{y}\\right) $$\n    These calculations involve standard matrix operations, including inversion and multiplication. For the diagonal prior covariance $\\mathbf{V}_0$, its inverse $\\mathbf{V}_0^{-1}$ is trivially found by inverting its diagonal elements.\n\n3.  **Calculation of Diagnostic Quantities**:\n\n    **a. Posterior Predictive $p$-value ($p_B$):** This quantity assesses a general model misfit. It is the probability that a replicated dataset $\\tilde{\\mathbf{y}}$ is more extreme than the observed dataset $\\mathbf{y}$, according to a discrepancy measure $T(\\mathbf{d}, \\boldsymbol{\\theta})$. The problem specifies a Monte Carlo procedure to estimate $p_B = \\mathbb{P}(T(\\tilde{\\mathbf{y}}, \\boldsymbol{\\theta}) \\ge T(\\mathbf{y}, \\boldsymbol{\\theta}) \\mid \\mathbf{y})$. This is implemented as follows:\n    -   Draw $M=8000$ samples $\\boldsymbol{\\theta}^{(i)}$ from the posterior distribution $\\mathcal{N}(\\mathbf{m}_n, \\mathbf{V}_n)$. A dedicated random seed ($s_{\\text{ppc}}$) is used for all posterior simulations.\n    -   For each sample $\\boldsymbol{\\theta}^{(i)}$, calculate the discrepancy for the observed data: $T(\\mathbf{y}, \\boldsymbol{\\theta}^{(i)}) = \\sum_{j=1}^n (y_j - \\mathbf{x}_j^\\top \\boldsymbol{\\theta}^{(i)})^2 / \\sigma^2$.\n    -   For each $\\boldsymbol{\\theta}^{(i)}$, generate a replicated dataset $\\tilde{\\mathbf{y}}^{(i)}$ by drawing from $\\mathcal{N}(\\mathbf{X}\\boldsymbol{\\theta}^{(i)}, \\sigma^2 \\mathbf{I}_n)$.\n    -   For each pair $(\\tilde{\\mathbf{y}}^{(i)}, \\boldsymbol{\\theta}^{(i)})$, calculate the discrepancy for the replicated data: $T(\\tilde{\\mathbf{y}}^{(i)}, \\boldsymbol{\\theta}^{(i)}) = \\sum_{j=1}^n (\\tilde{y}_j^{(i)} - \\mathbf{x}_j^\\top \\boldsymbol{\\theta}^{(i)})^2 / \\sigma^2$.\n    -   The $p$-value, $p_B$, is the fraction of samples for which $T(\\tilde{\\mathbf{y}}^{(i)}, \\boldsymbol{\\theta}^{(i)}) \\ge T(\\mathbf{y}, \\boldsymbol{\\theta}^{(i)})$.\n\n    **b. Empirical Coverage ($\\text{cov}_{0.9}$):** This diagnostic checks if the predictive uncertainty is correctly quantified. First, we compute the analytic posterior predictive mean $\\mu_j^{\\text{pred}} = \\mathbf{x}(E_j)^\\top \\mathbf{m}_n$ and variance $s_j^{2, \\text{pred}} = \\sigma^2 + \\mathbf{x}(E_j)^\\top \\mathbf{V}_n \\mathbf{x}(E_j)$ for each data point $j$. For a nominal level $c=0.9$, the tail probability is $\\alpha = (1-0.9)/2 = 0.05$. The $90\\%$ central predictive interval for each $y_j$ is $[\\mu_j^{\\text{pred}} - z_{0.95} s_j^{\\text{pred}}, \\mu_j^{\\text{pred}} + z_{0.95} s_j^{\\text{pred}}]$, where $s_j^{\\text{pred}}$ is the predictive standard deviation and $z_{0.95} \\approx 1.645$ is the $95$-th percentile of the standard normal distribution. The empirical coverage is the fraction of observed data points $y_j$ that fall within their corresponding interval.\n\n    **c. Kolmogorov-Smirnov Statistic ($D_n$):** This assesses the calibration of the predictive distributions. For each observation $y_j$, we compute the probability integral transform (PIT) value $u_j = \\Phi((y_j - \\mu_j^{\\text{pred}})/s_j^{\\text{pred}})$, where $\\Phi$ is the standard normal cumulative distribution function (CDF). If the model is perfectly calibrated, the set of values $\\{u_j\\}_{j=1}^n$ should be indistinguishable from a sample from a Uniform$[0,1]$ distribution. The Kolmogorov-Smirnov statistic $D_n$ measures the maximum absolute difference between the empirical CDF of the $\\{u_j\\}$ and the uniform CDF, $F(t)=t$. It is computed using `scipy.stats.kstest`.\n\nThis entire sequence of computations is performed for each of the three test cases (A, B, and C). The resulting nine values ($p_B$, $\\text{cov}_{0.9}$, and $D_n$ for each case) are collected, formatted to six decimal places, and printed as a single comma-separated list enclosed in brackets. The implementation uses `numpy` for efficient, vectorized numerical computations and `scipy.stats` for statistical functions like `norm.ppf`, `norm.cdf`, and `kstest`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Define common parameters across all cases\n    E0 = 20.0\n    theta_true = np.array([10.0, 350.0, 90.0])\n    M = 8000  # Number of Monte Carlo draws for p-value\n    c = 0.9  # Nominal coverage level\n\n    # Define the three test cases\n    test_cases = [\n        # Case A: Happy path with a reasonable prior and moderate noise\n        {\n            'name': 'A', 'n': 21, 'E_min': 5.0, 'E_max': 40.0, 'sigma': 3.0,\n            'm0': np.array([12.0, 300.0, 100.0]),\n            'prior_stds': np.array([30.0, 100.0, 50.0]),\n            's_data': 2468, 's_ppc': 112233\n        },\n        # Case B: Boundary undercoverage due to a tight, biased prior and low noise\n        {\n            'name': 'B', 'n': 21, 'E_min': 5.0, 'E_max': 40.0, 'sigma': 1.0,\n            'm0': np.array([30.0, 150.0, 30.0]),\n            'prior_stds': np.array([5.0, 20.0, 5.0]),\n            's_data': 1357, 's_ppc': 223344\n        },\n        # Case C: Edge case with high noise and a diffuse prior\n        {\n            'name': 'C', 'n': 21, 'E_min': 5.0, 'E_max': 40.0, 'sigma': 8.0,\n            'm0': np.array([0.0, 0.0, 0.0]),\n            'prior_stds': np.array([100.0, 300.0, 120.0]),\n            's_data': 9876, 's_ppc': 334455\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # Run the full analysis for one case\n        p_B, cov_09, D_n = run_case(case, E0, theta_true, M, c)\n        all_results.extend([p_B, cov_09, D_n])\n\n    # Format the results to 6 decimal places and print in the specified format\n    formatted_results = [f\"{res:.6f}\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_case(params, E0, theta_true, M, c):\n    \"\"\"\n    Performs the full Bayesian analysis for a single test case.\n\n    Args:\n        params (dict): A dictionary containing all parameters for the case.\n        E0 (float): The energy scale parameter.\n        theta_true (np.ndarray): The true parameter vector for data generation.\n        M (int): Number of Monte Carlo samples.\n        c (float): Nominal coverage level.\n\n    Returns:\n        tuple: A tuple containing (p_B, cov_0.9, D_n) for the case.\n    \"\"\"\n    n, E_min, E_max = params['n'], params['E_min'], params['E_max']\n    sigma, m0, prior_stds = params['sigma'], params['m0'], params['prior_stds']\n    s_data, s_ppc = params['s_data'], params['s_ppc']\n\n    # 1. Data Generation and Model Setup\n    data_rng = np.random.default_rng(s_data)\n    E_j = np.linspace(E_min, E_max, n, dtype=np.float64)\n    \n    # Construct the design matrix X\n    X = np.vstack([np.ones(n), 1.0 / E_j, np.exp(-E_j / E0)]).T\n\n    # Generate synthetic data y\n    y_mean_true = X @ theta_true\n    noise = data_rng.normal(0.0, sigma, n)\n    y = y_mean_true + noise\n\n    # 2. Posterior Parameter Inference\n    V0 = np.diag(prior_stds**2)\n    # Since V0 is diagonal, its inverse is the reciprocal of the diagonal elements\n    V0_inv = np.diag(1.0 / (prior_stds**2))\n    XTX = X.T @ X\n    \n    # Posterior covariance and mean\n    Vn = np.linalg.inv(V0_inv + (1.0 / sigma**2) * XTX)\n    mn = Vn @ (V0_inv @ m0 + (1.0 / sigma**2) * X.T @ y)\n\n    # 3. Analytic Posterior Predictive Mean and Variance\n    mu_pred = X @ mn\n    # Efficient calculation of diag(X @ Vn @ X.T)\n    s2_pred = sigma**2 + np.sum((X @ Vn) * X, axis=1)\n    s_pred = np.sqrt(s2_pred)\n\n    # 4. Compute Diagnostic Quantities\n\n    # 4a. Posterior Predictive p-value (p_B)\n    ppc_rng = np.random.default_rng(s_ppc)\n    # Draw M samples from the posterior distribution of theta\n    theta_samples = ppc_rng.multivariate_normal(mn, Vn, size=M)  # Shape: (M, 3)\n\n    # Predictions for each theta sample, shape: (n, M)\n    mu_pred_samples = X @ theta_samples.T\n    \n    # Discrepancy for observed data for each theta sample\n    residuals_y = y[:, np.newaxis] - mu_pred_samples  # Broadcasting y\n    T_y_samples = np.sum(residuals_y**2 / sigma**2, axis=0)  # Shape: (M,)\n\n    # Replicate data and compute discrepancy for replicated data\n    y_tilde_samples = ppc_rng.normal(loc=mu_pred_samples, scale=sigma)\n    residuals_y_tilde = y_tilde_samples - mu_pred_samples\n    T_y_tilde_samples = np.sum(residuals_y_tilde**2 / sigma**2, axis=0)\n\n    p_B = np.mean(T_y_tilde_samples >= T_y_samples)\n    \n    # 4b. Empirical Coverage (cov_0.9)\n    alpha = (1.0 - c) / 2.0\n    z_q = stats.norm.ppf(1.0 - alpha)  # Quantile for the central interval\n    \n    lower_bound = mu_pred - z_q * s_pred\n    upper_bound = mu_pred + z_q * s_pred\n    \n    # Count how many observed y_j are within their predictive intervals\n    covered_count = np.sum((y >= lower_bound) & (y <= upper_bound))\n    cov_09 = covered_count / n\n    \n    # 4c. Kolmogorov-Smirnov Statistic (D_n)\n    # Calculate Probability Integral Transform (PIT) values\n    u_j = stats.norm.cdf((y - mu_pred) / s_pred)\n    \n    # Compute the KS statistic against a uniform distribution\n    D_n = stats.kstest(u_j, 'uniform').statistic\n\n    return p_B, cov_09, D_n\n\nsolve()\n```"
        }
    ]
}