{
    "hands_on_practices": [
        {
            "introduction": "The stiffness inherent in nuclear reaction networks mandates the use of implicit integration schemes. Unlike explicit methods, implicit solvers require solving a system of (often nonlinear) algebraic equations at each time step to advance the solution. This exercise demystifies this core process by guiding you through a single, detailed step of the second-order Backward Differentiation Formula (BDF2), a classic stiffly stable method. By manually constructing the residual, computing the Jacobian, and performing a Newton-Raphson update, you will gain a concrete understanding of the mechanics that lie at the heart of sophisticated reaction network solvers .",
            "id": "3576963",
            "problem": "Consider a two-species nuclear reaction subnetwork with species $Y_{P}$ and $Y_{D}$ whose abundances (number fractions per baryon, dimensionless) evolve by a conversion and decay scheme. The parent species $Y_{P}$ converts to the daughter $Y_{D}$ at a constant rate $k$ and independently decays to a sink at a constant rate $\\lambda_{P}$. The daughter species $Y_{D}$ independently decays to a sink at a constant rate $\\lambda_{D}$. The deterministic evolution obeys the ordinary differential equation (ODE) system\n$$\n\\frac{d}{dt}\\begin{pmatrix}Y_{P} \\\\ Y_{D}\\end{pmatrix} \\;=\\; \\begin{pmatrix} -(\\lambda_{P}+k) & 0 \\\\ k & -\\lambda_{D} \\end{pmatrix}\\begin{pmatrix}Y_{P} \\\\ Y_{D}\\end{pmatrix},\n$$\nwhich is a stiff, linear reaction network. Take the rate constants to be $k=900\\,\\mathrm{s}^{-1}$, $\\lambda_{P}=40\\,\\mathrm{s}^{-1}$, and $\\lambda_{D}=5\\,\\mathrm{s}^{-1}$.\n\nAssume a uniform time step $\\Delta t = 1.0\\times 10^{-3}\\,\\mathrm{s}$, with two already-computed states at $t_{n-1}$ and $t_{n}$ given by\n$$\n\\mathbf{Y}_{n-1}=\\begin{pmatrix}0.500 \\\\ 0.300\\end{pmatrix}, \\qquad \\mathbf{Y}_{n}=\\begin{pmatrix}0.195 \\\\ 0.700\\end{pmatrix}.\n$$\n\nUsing the fully implicit Backward Differentiation Formula of order two (BDF2), advance one step to $t_{n+1}$ by:\n- Starting from the definition of the method as a two-step, implicit linear multistep approximation to the time derivative at $t_{n+1}$ constructed from a quadratic interpolant through $\\mathbf{Y}_{n-1}$, $\\mathbf{Y}_{n}$, and $\\mathbf{Y}$ evaluated at $t_{n+1}$.\n- Forming the nonlinear discrete residual $\\mathbf{R}(\\mathbf{Y})$ at $t_{n+1}$, and its Jacobian with respect to $\\mathbf{Y}$.\n- Linearizing and solving the resulting system via one Newton update with the initial guess $\\mathbf{Y}^{(0)}=\\mathbf{Y}_{n}$, yielding the updated $\\mathbf{Y}_{n+1}$.\n\nCarry out these steps explicitly for the given data, and report the abundance vector $\\mathbf{Y}_{n+1}=(Y_{P,n+1},\\,Y_{D,n+1})$ rounded to six significant figures. Express the final answer as dimensionless abundances.",
            "solution": "The governing system is linear and stiff:\n$$\n\\frac{d}{dt}\\mathbf{Y} \\;=\\; \\mathbf{f}(\\mathbf{Y}) \\;=\\; \\mathbf{M}\\,\\mathbf{Y}, \\qquad \\mathbf{M} \\;=\\; \\begin{pmatrix}-(\\lambda_{P}+k) & 0 \\\\ k & -\\lambda_{D}\\end{pmatrix}.\n$$\nWith $k=900\\,\\mathrm{s}^{-1}$, $\\lambda_{P}=40\\,\\mathrm{s}^{-1}$, and $\\lambda_{D}=5\\,\\mathrm{s}^{-1}$, we have\n$$\n\\mathbf{M} \\;=\\; \\begin{pmatrix}-940 & 0 \\\\ 900 & -5\\end{pmatrix}.\n$$\n\nWe use the Backward Differentiation Formula of order two (BDF2), which is a two-step implicit linear multistep method. To derive its discrete time derivative at $t_{n+1}$ from first principles, approximate $\\mathbf{Y}(t)$ on $[t_{n-1},t_{n+1}]$ by the unique quadratic polynomial $\\mathbf{p}(t)$ that interpolates $\\mathbf{Y}_{n-1}$ at $t_{n-1}=t_{n}-\\Delta t$, $\\mathbf{Y}_{n}$ at $t_{n}$, and a to-be-determined $\\mathbf{Y}$ at $t_{n+1}=t_{n}+\\Delta t$. The derivative of the Lagrange interpolant at $t_{n+1}$ yields the standard BDF2 backward difference for the time derivative:\n$$\n\\left.\\frac{d\\mathbf{Y}}{dt}\\right|_{t_{n+1}} \\;\\approx\\; \\frac{3\\,\\mathbf{Y}_{n+1} - 4\\,\\mathbf{Y}_{n} + \\mathbf{Y}_{n-1}}{2\\,\\Delta t}.\n$$\nThe fully implicit BDF2 scheme enforces this discrete derivative to equal the right-hand side evaluated at $t_{n+1}$:\n$$\n\\frac{3\\,\\mathbf{Y}_{n+1} - 4\\,\\mathbf{Y}_{n} + \\mathbf{Y}_{n-1}}{2\\,\\Delta t} \\;=\\; \\mathbf{f}(\\mathbf{Y}_{n+1}).\n$$\n\nDefine the nonlinear residual at $t_{n+1}$ by\n$$\n\\mathbf{R}(\\mathbf{Y}) \\;=\\; \\frac{3\\,\\mathbf{Y} - 4\\,\\mathbf{Y}_{n} + \\mathbf{Y}_{n-1}}{2\\,\\Delta t} \\;-\\; \\mathbf{f}(\\mathbf{Y}).\n$$\nA Newton update from an initial guess $\\mathbf{Y}^{(0)}$ satisfies\n$$\n\\left[\\frac{\\partial \\mathbf{R}}{\\partial \\mathbf{Y}}(\\mathbf{Y}^{(0)})\\right]\\delta \\;=\\; -\\,\\mathbf{R}(\\mathbf{Y}^{(0)}), \\qquad \\mathbf{Y}^{(1)} \\;=\\; \\mathbf{Y}^{(0)} + \\delta.\n$$\nBecause $\\mathbf{f}(\\mathbf{Y})=\\mathbf{M}\\mathbf{Y}$ is linear, the Jacobian $\\mathbf{J}=\\partial \\mathbf{f}/\\partial \\mathbf{Y}$ is constant and equal to $\\mathbf{M}$. Hence\n$$\n\\frac{\\partial \\mathbf{R}}{\\partial \\mathbf{Y}} \\;=\\; \\frac{3}{2\\Delta t}\\,\\mathbf{I} \\;-\\; \\mathbf{J} \\;=\\; \\frac{3}{2\\Delta t}\\,\\mathbf{I} \\;-\\; \\mathbf{M}.\n$$\nWith $\\Delta t = 1.0\\times 10^{-3}\\,\\mathrm{s}$, we have\n$$\n\\alpha \\;=\\; \\frac{3}{2\\Delta t} \\;=\\; \\frac{3}{2\\times 10^{-3}} \\;=\\; 1500,\n$$\nand therefore\n$$\n\\mathbf{A} \\;=\\; \\alpha\\,\\mathbf{I} - \\mathbf{M} \\;=\\; \\begin{pmatrix}1500 - (-940) & 0 - 0 \\\\ 0 - 900 & 1500 - (-5)\\end{pmatrix} \\;=\\; \\begin{pmatrix}2440 & 0 \\\\ -900 & 1505\\end{pmatrix}.\n$$\n\nChoose the Newton initial guess $\\mathbf{Y}^{(0)}=\\mathbf{Y}_{n}$. Evaluate the residual:\n$$\n\\mathbf{R}(\\mathbf{Y}_{n}) \\;=\\; \\frac{\\mathbf{Y}_{n-1} - \\mathbf{Y}_{n}}{2\\Delta t} \\;-\\; \\mathbf{f}(\\mathbf{Y}_{n}).\n$$\nCompute each term. First,\n$$\n\\frac{\\mathbf{Y}_{n-1} - \\mathbf{Y}_{n}}{2\\Delta t} \\;=\\; \\frac{1}{2\\times 10^{-3}}\\begin{pmatrix}0.500 - 0.195 \\\\ 0.300 - 0.700\\end{pmatrix} \\;=\\; \\frac{1}{0.002}\\begin{pmatrix}0.305 \\\\ -0.400\\end{pmatrix} \\;=\\; \\begin{pmatrix}152.5 \\\\ -200.0\\end{pmatrix}.\n$$\nNext, compute explicitly:\n$$\n\\mathbf{f}(\\mathbf{Y}_{n}) \\;=\\; \\begin{pmatrix}-940\\cdot 0.195 + 0\\cdot 0.700 \\\\ 900\\cdot 0.195 - 5\\cdot 0.700\\end{pmatrix} \\;=\\; \\begin{pmatrix}-183.3 \\\\ 175.5 - 3.5\\end{pmatrix} \\;=\\; \\begin{pmatrix}-183.3 \\\\ 172.0\\end{pmatrix}.\n$$\nThus the discrete residual at the initial guess is\n$$\n\\mathbf{R}(\\mathbf{Y}_{n}) \\;=\\; \\begin{pmatrix}152.5 \\\\ -200.0\\end{pmatrix} \\;-\\; \\begin{pmatrix}-183.3 \\\\ 172.0\\end{pmatrix} \\;=\\; \\begin{pmatrix}335.8 \\\\ -372.0\\end{pmatrix}.\n$$\n\nSolve the Newton system\n$$\n\\mathbf{A}\\,\\delta \\;=\\; -\\,\\mathbf{R}(\\mathbf{Y}_{n}) \\;=\\; \\begin{pmatrix}-335.8 \\\\ 372.0\\end{pmatrix}.\n$$\nBecause $\\mathbf{A}$ is lower triangular in this case, solve componentwise. The first row gives\n$$\n2440\\,\\delta_{1} \\;=\\; -335.8 \\;\\;\\Rightarrow\\;\\; \\delta_{1} \\;=\\; -\\frac{335.8}{2440} \\;\\approx\\; -0.13762295.\n$$\nThe second row gives\n$$\n-900\\,\\delta_{1} + 1505\\,\\delta_{2} \\;=\\; 372.0 \\;\\;\\Rightarrow\\;\\; 1505\\,\\delta_{2} \\;=\\; 372.0 + 900\\,\\delta_{1}.\n$$\nSubstituting $\\delta_{1}$,\n$$\n900\\,\\delta_{1} \\;\\approx\\; -123.860656 \\;\\;\\Rightarrow\\;\\; 1505\\,\\delta_{2} \\;\\approx\\; 248.139344 \\;\\;\\Rightarrow\\;\\; \\delta_{2} \\;\\approx\\; 0.16487664.\n$$\nUpdate to obtain the new state:\n$$\n\\mathbf{Y}_{n+1} \\;=\\; \\mathbf{Y}_{n} + \\delta \\;=\\; \\begin{pmatrix}0.195 \\\\ 0.700\\end{pmatrix} + \\begin{pmatrix}-0.13762295 \\\\ 0.16487664\\end{pmatrix} \\;=\\; \\begin{pmatrix}0.05737705 \\\\ 0.86487664\\end{pmatrix}.\n$$\n\nFor a linear system $\\mathbf{f}(\\mathbf{Y})=\\mathbf{M}\\mathbf{Y}$, this single Newton update is the exact BDF2 solution because the Jacobian is constant and the residual is affine. One can equivalently write the BDF2 step as the linear system\n$$\n\\left(\\frac{3}{2\\Delta t}\\mathbf{I} - \\mathbf{M}\\right)\\mathbf{Y}_{n+1} \\;=\\; \\frac{4\\,\\mathbf{Y}_{n} - \\mathbf{Y}_{n-1}}{2\\Delta t},\n$$\nwhich, with $\\mathbf{A}$ as above and right-hand side\n$$\n\\frac{4\\,\\mathbf{Y}_{n} - \\mathbf{Y}_{n-1}}{2\\Delta t} \\;=\\; \\frac{1}{0.002}\\begin{pmatrix}4\\cdot 0.195 - 0.500 \\\\ 4\\cdot 0.700 - 0.300\\end{pmatrix} \\;=\\; \\frac{1}{0.002}\\begin{pmatrix}0.280 \\\\ 2.500\\end{pmatrix} \\;=\\; \\begin{pmatrix}140 \\\\ 1250\\end{pmatrix},\n$$\nyields the same solution. Solving\n$$\n\\begin{pmatrix}2440 & 0 \\\\ -900 & 1505\\end{pmatrix}\\begin{pmatrix}Y_{P,n+1} \\\\ Y_{D,n+1}\\end{pmatrix} \\;=\\; \\begin{pmatrix}140 \\\\ 1250\\end{pmatrix}\n$$\ngives $Y_{P,n+1} = 140/2440 = 7/122 \\approx 0.057377049...$ and\n$$\nY_{D,n+1} \\;=\\; \\frac{1250 + 900\\,Y_{P,n+1}}{1505} \\;=\\; \\frac{1250 + 900\\cdot (7/122)}{1505} \\;=\\; \\frac{79400}{91805} \\;\\approx\\; 0.86487664,\n$$\nin agreement with the Newton update. Rounding each component to six significant figures produces the requested result:\n$$\n\\mathbf{Y}_{n+1} \\;\\approx\\; \\begin{pmatrix}0.0573770 \\\\ 0.864877\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}0.0573770 & 0.864877\\end{pmatrix}}$$"
        },
        {
            "introduction": "A critical challenge in simulating reaction networks is ensuring that species abundances $Y_i$ remain non-negative, a physical constraint that standard numerical methods can easily violate, especially with large time steps. This practice moves from a single step to a complete solver, tackling this positivity problem head-on. You will implement and test a powerful technique: transforming the abundance variables via $Y_i = \\exp(z_i)$ and solving the implicit system for the logarithmic abundances $z_i$, which guarantees $Y_i \\gt 0$ by construction . This exercise demonstrates how a clever change of variables can enforce essential physical constraints elegantly and robustly.",
            "id": "3576947",
            "problem": "Construct a program that integrates small nuclear reaction networks over a single, possibly large, timestep while guaranteeing nonnegative species abundances. The abundances are the nuclear abundances per baryon, denoted by $Y_i$, which are dimensionless. Time is measured in seconds. You must do the following within your program.\n\n- Model the network as a system of ordinary differential equations (ODEs) derived from mass-action kinetics. If there are $n$ species and $m$ reactions, let the stoichiometric change matrix be $N \\in \\mathbb{R}^{n \\times m}$ with entries $N_{i r} = \\nu^{\\mathrm{prod}}_{i r} - \\nu^{\\mathrm{reac}}_{i r}$, where $\\nu^{\\mathrm{prod}}_{i r}$ and $\\nu^{\\mathrm{reac}}_{i r}$ are the product and reactant stoichiometric coefficients of species $i$ in reaction $r$. For mass-action kinetics, the reaction rate for reaction $r$ is $R_r(\\mathbf{Y}) = k_r \\prod_{i=1}^{n} Y_i^{\\nu^{\\mathrm{reac}}_{i r}}$, with rate constant $k_r$ given and $\\mathbf{Y} = (Y_1,\\dots,Y_n)^{\\top}$. The ODE system is $d\\mathbf{Y}/dt = \\mathbf{f}(\\mathbf{Y}) = N \\,\\mathbf{R}(\\mathbf{Y})$, where $\\mathbf{R} = (R_1,\\dots,R_m)^{\\top}$.\n\n- Implement two one-step methods that advance the state from $\\mathbf{Y}^{n}$ at time $t^{n}$ to $\\mathbf{Y}^{n+1}$ at time $t^{n+1} = t^{n} + \\Delta t$:\n  1. A baseline forward (explicit) Euler step, $\\mathbf{Y}^{n+1}_{\\mathrm{FE}} = \\mathbf{Y}^{n} + \\Delta t \\,\\mathbf{f}(\\mathbf{Y}^{n})$, without any clipping or postprocessing.\n  2. A positivity-guaranteeing implicit backward Euler step solved in logarithmic variables. Introduce transformed variables $\\mathbf{z} \\in \\mathbb{R}^{n}$ with $z_i = \\log Y_i$. Solve for $\\mathbf{z}^{n+1}$ such that $\\exp(\\mathbf{z}^{n+1}) = \\mathbf{Y}^{n} + \\Delta t \\,\\mathbf{f}(\\exp(\\mathbf{z}^{n+1}))$. The solver must use Newton’s method with a suitable globalization strategy to ensure convergence from a physically meaningful initial guess. The mapping back to abundances is $\\mathbf{Y}^{n+1}_{\\mathrm{BE}} = \\exp(\\mathbf{z}^{n+1})$, which guarantees $Y^{n+1}_i > 0$ by construction.\n\n- For Newton’s method on the transformed residual, derive and implement the Jacobian using the chain rule from first principles starting from the definition of $\\mathbf{f}(\\mathbf{Y})$ and the properties of mass-action kinetics. Your implementation must be robust for the provided test suite.\n\n- For any provided network, define the conserved baryon-number-like quantity using integer mass numbers $A_i$ for each species. The conserved quantity for any physically closed set of reactions is $B(\\mathbf{Y}) = \\sum_{i=1}^{n} A_i Y_i$. Your program must check conservation by comparing $B(\\mathbf{Y}^{n+1}_{\\mathrm{BE}})$ to $B(\\mathbf{Y}^{n})$.\n\n- Numerical details:\n  - Use a Newton termination criterion based on the residual norm. Terminate when $\\lVert \\mathbf{G}(\\mathbf{z}) \\rVert_2 \\le \\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\lVert \\exp(\\mathbf{z}) \\rVert_2$, where $\\mathbf{G}(\\mathbf{z}) = \\exp(\\mathbf{z}) - \\mathbf{Y}^{n} - \\Delta t\\,\\mathbf{f}(\\exp(\\mathbf{z}))$, with tolerances $\\varepsilon_{\\mathrm{abs}} = 10^{-12}$ and $\\varepsilon_{\\mathrm{rel}} = 10^{-10}$. Use a maximum of $100$ Newton iterations and backtracking line search with standard Armijo decrease to ensure convergence.\n  - All abundances are dimensionless. Time is in seconds. Rate constants $k_r$ should be treated as consistent with these units; you do not need to report units for $k_r$ in the output.\n  - For any logarithm operation, ensure its argument is strictly positive by using the current positive iterate. For the initial guess of the implicit solve, use $\\mathbf{z}^{0} = \\log(\\mathbf{Y}^{n})$ with a small positive offset added componentwise if needed to avoid $\\log 0$.\n\n- Test suite:\n  Provide four test cases, each defined by its species, reactions, rate constants, mass numbers $A_i$, initial abundances $\\mathbf{Y}^{n}$, and timestep $\\Delta t$.\n  - Case $1$ (two-body fusion prone to negativity under large $\\Delta t$): species $\\{A,B,C\\}$ with mass numbers $(1,1,2)$ and reaction $A + B \\rightarrow C$ having $k = 50$. Initial abundances $\\mathbf{Y}^{n} = (0.6, 0.6, 0.0)$, $\\Delta t = 0.2$.\n  - Case $2$ (stiff reversible interconversion): species $\\{A,B\\}$ with mass numbers $(1,1)$ and reactions $A \\rightarrow B$ with $k_{f} = 100$ and $B \\rightarrow A$ with $k_{r} = 40$. Initial abundances $\\mathbf{Y}^{n} = (0.99, 0.01)$, $\\Delta t = 0.5$.\n  - Case $3$ (stiff chain): species $\\{A,B,C\\}$ with mass numbers $(1,1,1)$ and reactions $A \\rightarrow B$ with $k_{1} = 20$ and $B \\rightarrow C$ with $k_{2} = 30$. Initial abundances $\\mathbf{Y}^{n} = (0.8, 0.2, 0.0)$, $\\Delta t = 0.2$.\n  - Case $4$ (small $\\Delta t$ baseline): identical to Case $3$ but with $\\Delta t = 0.001$.\n\n- Required checks and final output specification:\n  For each case $j \\in \\{1,2,3,4\\}$, compute the following three predicates:\n  1. $P^{(j)}_{\\mathrm{FE\\_neg}}$: whether any component of $\\mathbf{Y}^{n+1}_{\\mathrm{FE}}$ is negative.\n  2. $P^{(j)}_{\\mathrm{BE\\_pos}}$: whether every component of $\\mathbf{Y}^{n+1}_{\\mathrm{BE}}$ is strictly positive to within a tolerance of $10^{-14}$.\n  3. $P^{(j)}_{\\mathrm{cons}}$: whether $\\left| B(\\mathbf{Y}^{n+1}_{\\mathrm{BE}}) - B(\\mathbf{Y}^{n}) \\right| \\le 10^{-10}$.\n  Your program must produce a single line of output containing a list of four boolean values $Q^{(j)}$, one per case, where $Q^{(j)}$ is true if and only if $P^{(j)}_{\\mathrm{FE\\_neg}}$ is true, $P^{(j)}_{\\mathrm{BE\\_pos}}$ is true, and $P^{(j)}_{\\mathrm{cons}}$ is true. The required final output format is exactly a single line:\n  \"[Q1,Q2,Q3,Q4]\"",
            "solution": "The problem requires the implementation and comparison of two numerical integration methods for a system of ordinary differential equations (ODEs) describing a nuclear reaction network under the assumption of mass-action kinetics. The primary objectives are to demonstrate the positivity-violating nature of the explicit forward Euler method for large timesteps and to implement a positivity-guaranteeing implicit backward Euler solver using a logarithmic variable transformation.\n\nA reaction network with $n$ species and $m$ reactions is described by the time evolution of the species abundances, $\\mathbf{Y} = (Y_1, \\dots, Y_n)^\\top$. The ODE system is given by:\n$$\n\\frac{d\\mathbf{Y}}{dt} = \\mathbf{f}(\\mathbf{Y}) = N \\mathbf{R}(\\mathbf{Y})\n$$\nwhere $N \\in \\mathbb{R}^{n \\times m}$ is the stoichiometric change matrix, with entries $N_{ir} = \\nu^{\\mathrm{prod}}_{ir} - \\nu^{\\mathrm{reac}}_{ir}$ representing the net change in species $i$ due to reaction $r$. The vector $\\mathbf{R}(\\mathbf{Y}) = (R_1, \\dots, R_m)^\\top$ contains the rates of the $m$ reactions. For mass-action kinetics, the rate of reaction $r$ is:\n$$\nR_r(\\mathbf{Y}) = k_r \\prod_{i=1}^{n} Y_i^{\\nu^{\\mathrm{reac}}_{ir}}\n$$\nwhere $k_r$ is the rate constant and $\\nu^{\\mathrm{reac}}_{ir}$ is the stoichiometric coefficient of species $i$ as a reactant in reaction $r$.\n\nThe task is to advance the system from a state $\\mathbf{Y}^n$ at time $t^n$ to a new state $\\mathbf{Y}^{n+1}$ at $t^{n+1} = t^n + \\Delta t$.\n\nFirst, we consider the baseline forward (explicit) Euler method. This is the simplest explicit one-step method, defined by:\n$$\n\\mathbf{Y}^{n+1}_{\\mathrm{FE}} = \\mathbf{Y}^{n} + \\Delta t \\, \\mathbf{f}(\\mathbf{Y}^{n})\n$$\nThis method is computationally inexpensive as it only requires one evaluation of the right-hand side function $\\mathbf{f}$. However, it is known to have a limited stability region. For stiff systems, which are common in reaction networks, the timestep $\\Delta t$ must be prohibitively small to maintain stability and, relevant to this problem, to prevent abundances from becoming negative, a physically nonsensical result.\n\nSecond, we implement a backward (implicit) Euler method. This method is A-stable and thus well-suited for stiff systems. The update formula is:\n$$\n\\mathbf{Y}^{n+1} = \\mathbf{Y}^{n} + \\Delta t \\, \\mathbf{f}(\\mathbf{Y}^{n+1})\n$$\nThis is an implicit equation for the unknown state $\\mathbf{Y}^{n+1}$. To guarantee that the solution components remain positive, we introduce a change of variables: $Y_i = e^{z_i}$, or in vector form, $\\mathbf{Y} = \\exp(\\mathbf{z})$. Since the exponential function maps any real number to a positive one, any solution for $\\mathbf{z}$ will yield a strictly positive abundance vector $\\mathbf{Y}$. Substituting this into the backward Euler equation, we get an equation for $\\mathbf{z}^{n+1}$:\n$$\n\\exp(\\mathbf{z}^{n+1}) = \\mathbf{Y}^{n} + \\Delta t \\, \\mathbf{f}(\\exp(\\mathbf{z}^{n+1}))\n$$\nThis is a system of nonlinear algebraic equations for $\\mathbf{z}^{n+1}$. We can solve it using Newton's method. Let $\\mathbf{z} \\equiv \\mathbf{z}^{n+1}$ be the variable we are solving for. We define a residual function $\\mathbf{G}(\\mathbf{z})$ whose root we seek:\n$$\n\\mathbf{G}(\\mathbf{z}) = \\exp(\\mathbf{z}) - \\mathbf{Y}^{n} - \\Delta t \\, \\mathbf{f}(\\exp(\\mathbf{z})) = \\mathbf{0}\n$$\nNewton's method finds the root iteratively. Starting from an initial guess $\\mathbf{z}_0$, each iteration updates the solution via:\n$$\n\\mathbf{z}_{k+1} = \\mathbf{z}_k + \\Delta \\mathbf{z}_k\n$$\nwhere the update step $\\Delta \\mathbf{z}_k$ is the solution to the linear system:\n$$\n\\mathbf{J}_{\\mathbf{G}}(\\mathbf{z}_k) \\Delta \\mathbf{z}_k = -\\mathbf{G}(\\mathbf{z}_k)\n$$\nHere, $\\mathbf{J}_{\\mathbf{G}}(\\mathbf{z}_k)$ is the Jacobian matrix of the residual function $\\mathbf{G}$ evaluated at $\\mathbf{z}_k$. We must derive this Jacobian. Using the chain rule, $\\mathbf{J}_{\\mathbf{G}} = \\frac{d\\mathbf{G}}{d\\mathbf{z}} = \\frac{d\\mathbf{G}}{d\\mathbf{Y}} \\frac{d\\mathbf{Y}}{d\\mathbf{z}}$.\n\nLet's compute the two parts. First, $\\frac{d\\mathbf{Y}}{d\\mathbf{z}}$:\nSince $Y_i = e^{z_i}$, the derivative $\\frac{\\partial Y_i}{\\partial z_j} = \\delta_{ij} e^{z_i} = \\delta_{ij} Y_i$. This means $\\frac{d\\mathbf{Y}}{d\\mathbf{z}}$ is a diagonal matrix:\n$$\n\\frac{d\\mathbf{Y}}{d\\mathbf{z}} = \\mathrm{diag}(e^{z_1}, \\dots, e^{z_n}) = \\mathrm{diag}(\\mathbf{Y})\n$$\n\nNext, $\\frac{d\\mathbf{G}}{d\\mathbf{Y}}$, where $\\mathbf{G}(\\mathbf{Y}) = \\mathbf{Y} - \\mathbf{Y}^n - \\Delta t \\, \\mathbf{f}(\\mathbf{Y})$:\n$$\n\\frac{d\\mathbf{G}}{d\\mathbf{Y}} = \\mathbf{I} - \\Delta t \\, \\frac{d\\mathbf{f}}{d\\mathbf{Y}} = \\mathbf{I} - \\Delta t \\, \\mathbf{J}_{\\mathbf{f}}(\\mathbf{Y})\n$$\nwhere $\\mathbf{I}$ is the identity matrix and $\\mathbf{J}_{\\mathbf{f}}$ is the Jacobian of the ODE right-hand side function $\\mathbf{f}$.\n\nThe elements of $\\mathbf{J}_{\\mathbf{f}}$ are $(J_{\\mathbf{f}})_{ij} = \\frac{\\partial f_i}{\\partial Y_j}$. We have $f_i = \\sum_{r=1}^m N_{ir} R_r$, so:\n$$\n(J_{\\mathbf{f}})_{ij} = \\frac{\\partial f_i}{\\partial Y_j} = \\sum_{r=1}^m N_{ir} \\frac{\\partial R_r}{\\partial Y_j}\n$$\nThe derivative of the reaction rate $R_r = k_r \\prod_{l=1}^n Y_l^{\\nu^{\\mathrm{reac}}_{lr}}$ with respect to $Y_j$ is:\n$$\n\\frac{\\partial R_r}{\\partial Y_j} = k_r \\nu^{\\mathrm{reac}}_{jr} Y_j^{\\nu^{\\mathrm{reac}}_{jr}-1} \\prod_{l \\neq j} Y_l^{\\nu^{\\mathrm{reac}}_{lr}} = \\frac{\\nu^{\\mathrm{reac}}_{jr}}{Y_j} R_r(\\mathbf{Y})\n$$\nThis expression is valid for $Y_j > 0$, which is guaranteed since we evaluate it at $\\mathbf{Y} = \\exp(\\mathbf{z})$.\nThus, we have $(J_{\\mathbf{f}})_{ij} = \\sum_{r=1}^m N_{ir} \\frac{\\nu^{\\mathrm{reac}}_{jr}}{Y_j} R_r(\\mathbf{Y})$.\n\nCombining these parts, the Jacobian of the Newton system is:\n$$\n\\mathbf{J}_{\\mathbf{G}}(\\mathbf{z}) = \\left( \\mathbf{I} - \\Delta t \\, \\mathbf{J}_{\\mathbf{f}}(\\exp(\\mathbf{z})) \\right) \\mathrm{diag}(\\exp(\\mathbf{z}))\n$$\n\nTo ensure convergence of Newton's method from an initial guess that may be far from the solution, a globalization strategy is employed. We use a backtracking line search on the update step: $\\mathbf{z}_{k+1} = \\mathbf{z}_k + \\alpha \\Delta \\mathbf{z}_k$, where $\\alpha \\in (0, 1]$ is a step length. We start with $\\alpha=1$ and reduce it until the Armijo-Goldstein condition is met. This condition ensures a sufficient decrease in a merit function, typically $F(\\mathbf{z}) = \\frac{1}{2}\\lVert \\mathbf{G}(\\mathbf{z}) \\rVert_2^2$. The condition is $F(\\mathbf{z}_k + \\alpha \\Delta \\mathbf{z}_k) \\le F(\\mathbf{z}_k) + c \\alpha \\nabla F(\\mathbf{z}_k)^T \\Delta \\mathbf{z}_k$ for a small constant $c \\in (0,1)$. Given $\\nabla F^T\\Delta\\mathbf{z}_k = -\\lVert \\mathbf{G}(\\mathbf{z}_k) \\rVert_2^2$, this simplifies to:\n$$\n\\frac{1}{2}\\lVert \\mathbf{G}(\\mathbf{z}_k + \\alpha \\Delta \\mathbf{z}_k) \\rVert_2^2 \\le \\frac{1}{2}(1 - 2c\\alpha)\\lVert \\mathbf{G}(\\mathbf{z}_k) \\rVert_2^2\n$$\nA simpler, practical variant is to ensure sufficient decrease in the residual norm directly: $\\lVert \\mathbf{G}(\\mathbf{z}_k + \\alpha \\Delta \\mathbf{z}_k) \\rVert_2 \\le (1-c\\alpha)\\lVert \\mathbf{G}(\\mathbf{z}_k) \\rVert_2$. We will use this in the implementation.\n\nFinally, we must check the conservation of a baryon-number-like quantity, $B(\\mathbf{Y})=\\sum_{i=1}^n A_i Y_i$, where $A_i$ are integer mass numbers. The time derivative is $\\frac{dB}{dt} = \\sum_i A_i \\frac{dY_i}{dt} = \\sum_i A_i f_i(\\mathbf{Y})$. Substituting the definition of $\\mathbf{f}$:\n$$\n\\frac{dB}{dt} = \\sum_{i=1}^n A_i \\left( \\sum_{r=1}^m N_{ir} R_r(\\mathbf{Y}) \\right) = \\sum_{r=1}^m \\left( \\sum_{i=1}^n A_i N_{ir} \\right) R_r(\\mathbf{Y})\n$$\nFor any valid nuclear reaction, the total mass number is conserved, meaning $\\sum_i A_i N_{ir} = \\sum_i A_i (\\nu^{\\mathrm{prod}}_{ir} - \\nu^{\\mathrm{reac}}_{ir}) = 0$ for every reaction $r$. Thus, $\\frac{dB}{dt}=0$ analytically. A numerical scheme is \"conservative\" if it respects this property. The backward Euler method does. We verify this by checking if $|B(\\mathbf{Y}^{n+1}_{\\mathrm{BE}}) - B(\\mathbf{Y}^n)|$ is smaller than a given tolerance, which accounts for floating-point and solver inaccuracies.\n\nThe solution proceeds by implementing these methods and applying them to the four test cases specified. For each case, we determine the truth value of the combined predicate $Q^{(j)} = P^{(j)}_{\\mathrm{FE\\_neg}} \\land P^{(j)}_{\\mathrm{BE\\_pos}} \\land P^{(j)}_{\\mathrm{cons}}$, which tests if the forward Euler method yields unphysical negative abundances while the backward Euler method provides a physically valid (positive) and conservative result.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# The problem statement specifies scipy in the environment, but the solver\n# is implemented from first principles using only numpy as required.\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    class ReactionNetwork:\n        \"\"\"\n        Represents a nuclear reaction network.\n        \"\"\"\n        def __init__(self, species, mass_numbers, reactions, rate_constants):\n            self.species = list(species)\n            self.n_species = len(self.species)\n            self.mass_numbers = np.array(mass_numbers, dtype=float)\n            self.rate_constants = np.array(rate_constants, dtype=float)\n            \n            self.n_reactions = len(reactions)\n            self.stoich_reac = np.zeros((self.n_species, self.n_reactions))\n            self.stoich_prod = np.zeros((self.n_species, self.n_reactions))\n            \n            species_map = {name: i for i, name in enumerate(self.species)}\n            \n            for r_idx, reaction in enumerate(reactions):\n                reac_str, prod_str = reaction.split('->')\n                \n                for part in reac_str.strip().split('+'):\n                    part = part.strip()\n                    if not part: continue\n                    # Assuming coefficients are 1 if not specified\n                    coeff = 1\n                    name = part\n                    if ' ' in part:\n                        coeff_str, name = part.split(' ', 1)\n                        coeff = int(coeff_str)\n                    s_idx = species_map[name]\n                    self.stoich_reac[s_idx, r_idx] += coeff\n                \n                for part in prod_str.strip().split('+'):\n                    part = part.strip()\n                    if not part: continue\n                    coeff = 1\n                    name = part\n                    if ' ' in part:\n                        coeff_str, name = part.split(' ', 1)\n                        coeff = int(coeff_str)\n                    s_idx = species_map[name]\n                    self.stoich_prod[s_idx, r_idx] += coeff\n\n            self.N = self.stoich_prod - self.stoich_reac\n\n        def get_rates(self, Y):\n            \"\"\"Computes the reaction rates R(Y).\"\"\"\n            # Add a small floor to Y to handle Y=0 in reactants for np.power\n            Y_floored = np.maximum(Y, 1e-100)\n            # Y[:, np.newaxis] broadcasts species abundances across reactions\n            rates = self.rate_constants * np.prod(Y_floored[:, np.newaxis]**self.stoich_reac, axis=0)\n            return rates\n\n        def f(self, Y):\n            \"\"\"Computes the ODE right-hand side f(Y) = N * R(Y).\"\"\"\n            return self.N @ self.get_rates(Y)\n\n        def jacobian_f(self, Y):\n            \"\"\"Computes the Jacobian of the ODE RHS, J_f.\"\"\"\n            Jf = np.zeros((self.n_species, self.n_species))\n            rates = self.get_rates(Y)\n            \n            # Use a small value to avoid division by zero, but since Y comes from\n            # exp(z) in the BE solver, it will be strictly positive.\n            Y_safe = np.maximum(Y, 1e-100)\n\n            for j in range(self.n_species):\n                # dR_r/dY_j = (nu_reac_jr / Y_j) * R_r\n                dRdYj = (self.stoich_reac[j, :] / Y_safe[j]) * rates\n                Jf[:, j] = self.N @ dRdYj\n            return Jf\n\n    def forward_euler_step(network, Y_n, dt):\n        \"\"\"Performs a single forward Euler step.\"\"\"\n        f_n = network.f(Y_n)\n        Y_fe = Y_n + dt * f_n\n        return Y_fe\n\n    def backward_euler_step(network, Y_n, dt):\n        \"\"\"\n        Solves for Y_{n+1} using backward Euler with a logarithmic transformation\n        and Newton's method.\n        \"\"\"\n        n_species = network.n_species\n        \n        # Numerical parameters for Newton's method\n        eps_abs = 1e-12\n        eps_rel = 1e-10\n        max_iter = 100\n        armijo_c = 1e-4\n        min_alpha = 1e-8\n        \n        # Initial guess in log-space, handle Y_n[i] = 0\n        z_k = np.log(np.maximum(Y_n, 1e-40))\n\n        for _ in range(max_iter):\n            Y_k = np.exp(z_k)\n            \n            # Calculate residual G(z_k)\n            G_k = Y_k - Y_n - dt * network.f(Y_k)\n            G_norm = np.linalg.norm(G_k)\n            \n            # Check for convergence\n            Y_norm = np.linalg.norm(Y_k)\n            if G_norm <= eps_abs + eps_rel * Y_norm:\n                return Y_k\n\n            # Calculate Jacobian of the Newton system J_G(z_k)\n            J_f_k = network.jacobian_f(Y_k)\n            I = np.identity(n_species)\n            J_G = (I - dt * J_f_k) @ np.diag(Y_k)\n            \n            # Solve linear system for Newton step: J_G * dz = -G\n            try:\n                delta_z = np.linalg.solve(J_G, -G_k)\n            except np.linalg.LinAlgError:\n                # Fallback to pseudo-inverse if Jacobian is singular\n                delta_z = np.linalg.pinv(J_G) @ -G_k\n\n            # Backtracking line search\n            alpha = 1.0\n            G_norm_sq = G_norm**2\n            \n            while True:\n                z_trial = z_k + alpha * delta_z\n                Y_trial = np.exp(z_trial)\n                G_trial = Y_trial - Y_n - dt * network.f(Y_trial)\n                \n                # Armijo condition check on residual norm\n                if np.linalg.norm(G_trial) <= (1 - armijo_c * alpha) * G_norm:\n                    z_k = z_trial\n                    break\n                \n                alpha /= 2.0\n                if alpha < min_alpha:\n                    # Line search failed, should not happen for these problems\n                    return None \n            \n        return None # Max iterations reached\n\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"species\": [\"A\", \"B\", \"C\"],\n            \"mass_numbers\": [1, 1, 2],\n            \"reactions\": [\"A + B -> C\"],\n            \"k\": [50.0],\n            \"Y0\": np.array([0.6, 0.6, 0.0]),\n            \"dt\": 0.2\n        },\n        {\n            \"name\": \"Case 2\",\n            \"species\": [\"A\", \"B\"],\n            \"mass_numbers\": [1, 1],\n            \"reactions\": [\"A -> B\", \"B -> A\"],\n            \"k\": [100.0, 40.0],\n            \"Y0\": np.array([0.99, 0.01]),\n            \"dt\": 0.5\n        },\n        {\n            \"name\": \"Case 3\",\n            \"species\": [\"A\", \"B\", \"C\"],\n            \"mass_numbers\": [1, 1, 1],\n            \"reactions\": [\"A -> B\", \"B -> C\"],\n            \"k\": [20.0, 30.0],\n            \"Y0\": np.array([0.8, 0.2, 0.0]),\n            \"dt\": 0.2\n        },\n        {\n            \"name\": \"Case 4\",\n            \"species\": [\"A\", \"B\", \"C\"],\n            \"mass_numbers\": [1, 1, 1],\n            \"reactions\": [\"A -> B\", \"B -> C\"],\n            \"k\": [20.0, 30.0],\n            \"Y0\": np.array([0.8, 0.2, 0.0]),\n            \"dt\": 0.001\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        net = ReactionNetwork(case[\"species\"], case[\"mass_numbers\"], case[\"reactions\"], case[\"k\"])\n        Y0 = case[\"Y0\"]\n        dt = case[\"dt\"]\n        \n        # 1. Forward Euler\n        Y_fe = forward_euler_step(net, Y0, dt)\n        P_fe_neg = np.any(Y_fe < 0)\n        \n        # 2. Backward Euler\n        Y_be = backward_euler_step(net, Y0, dt)\n        \n        P_be_pos = False\n        P_cons = False\n        \n        if Y_be is not None:\n            # Check for positivity\n            P_be_pos = np.all(Y_be > 1e-14)\n            \n            # Check for conservation\n            B0 = np.dot(net.mass_numbers, Y0)\n            B_be = np.dot(net.mass_numbers, Y_be)\n            P_cons = np.abs(B_be - B0) <= 1e-10\n\n        # 3. Final Predicate Q\n        Q = P_fe_neg and P_be_pos and P_cons\n        results.append(Q)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once a numerical solver is written, how can we be confident it is implemented correctly and understand its limitations? This practice introduces the Method of Manufactured Solutions (MMS), a rigorous code verification technique. By designing an ODE system with a known, non-trivial analytical solution, we can precisely measure our solver's numerical error and empirically determine its order of convergence. This exercise tasks you with using MMS to assess two common integrators, revealing how stiffness can cause \"order reduction\"—a phenomenon where a method fails to achieve its theoretical accuracy—and providing you with a critical tool for validating your own computational work .",
            "id": "3577003",
            "problem": "Consider a simplified nuclear reaction network with three species and sequential decays that captures key stiffness and multiple-timescale effects typical in computational nuclear physics. Let the abundance vector be $\\mathbf{y}(t) = [y_1(t), y_2(t), y_3(t)]^\\top$ with $y_i(t) \\ge 0$ denoting the abundance per baryon of species $i$, and let the network be modeled by a linear ordinary differential equation (ODE) of the form\n$$\n\\frac{d\\mathbf{y}}{dt} = \\mathbf{M}\\,\\mathbf{y}(t) + \\mathbf{g}(t),\n$$\nwhere $\\mathbf{M}$ is the stoichiometric-rate matrix encoding decay reactions and $\\mathbf{g}(t)$ is a time-dependent forcing term. For sequential one-body decays with rates $\\lambda_1$ and $\\lambda_2$ (in $\\mathrm{s}^{-1}$), use\n$$\n\\mathbf{M} = \\begin{bmatrix}\n-\\lambda_1 & 0 & 0 \\\\\n\\lambda_1 & -\\lambda_2 & 0 \\\\\n0 & \\lambda_2 & 0\n\\end{bmatrix}.\n$$\nThe forcing $\\mathbf{g}(t)$ is to be constructed so that a manufactured solution with multiple timescales is satisfied exactly. Define the manufactured abundances by\n$$\ny_1(t) = 0.4\\,e^{-\\alpha t} + 0.2\\,e^{-\\beta t} + 0.1\\,e^{-\\gamma t},\n$$\n$$\ny_2(t) = 0.3\\left(1 - e^{-\\alpha t}\\right)e^{-\\beta t} + 0.1\\,e^{-\\gamma t} + 0.1\\,e^{-\\delta t},\n$$\n$$\ny_3(t) = 1 - y_1(t) - y_2(t),\n$$\nwith $\\alpha = 800\\,\\mathrm{s}^{-1}$, $\\beta = 80\\,\\mathrm{s}^{-1}$, $\\gamma = 8\\,\\mathrm{s}^{-1}$, and $\\delta = 0.8\\,\\mathrm{s}^{-1}$. Using the fundamental definition $\\frac{d}{dt}e^{-\\kappa t} = -\\kappa e^{-\\kappa t}$ and the linear network model above, derive $\\mathbf{g}(t)$ explicitly from the identity\n$$\n\\mathbf{g}(t) = \\frac{d\\mathbf{y}}{dt} - \\mathbf{M}\\,\\mathbf{y}(t),\n$$\nso that the manufactured $\\mathbf{y}(t)$ satisfies the ODE for all $t$.\n\nYour task is to implement two time-integration methods for the ODE $\\frac{d\\mathbf{y}}{dt} = \\mathbf{M}\\,\\mathbf{y}(t) + \\mathbf{g}(t)$:\n- The classical fourth-order Runge–Kutta method (RK), defined as a one-step explicit method with nominal order $4$.\n- The trapezoidal rule (also known as the Crank–Nicolson method), defined as an implicit one-step method with nominal order $2$ and update\n$$\n\\mathbf{y}^{n+1} = \\mathbf{y}^{n} + \\frac{\\Delta t}{2}\\left(\\mathbf{M}\\,\\mathbf{y}^{n} + \\mathbf{g}(t^{n}) + \\mathbf{M}\\,\\mathbf{y}^{n+1} + \\mathbf{g}(t^{n+1})\\right),\n$$\nwhich can be written as a linear solve\n$$\n\\left(\\mathbf{I} - \\frac{\\Delta t}{2}\\mathbf{M}\\right)\\mathbf{y}^{n+1} = \\left(\\mathbf{I} + \\frac{\\Delta t}{2}\\mathbf{M}\\right)\\mathbf{y}^{n} + \\frac{\\Delta t}{2}\\left(\\mathbf{g}(t^{n}) + \\mathbf{g}(t^{n+1})\\right),\n$$\nwhere $\\mathbf{I}$ is the identity matrix.\n\nUse the manufactured solution and forcing to perform a grid-based convergence study of the final-time error for each method. For each test, integrate from $t_0 = 0\\,\\mathrm{s}$ to final time $t_f$ using a uniform time step $\\Delta t$ such that $t_f/\\Delta t$ is an integer. The initial condition must be set to the exact manufactured abundances at $t_0$, namely $\\mathbf{y}(0) = \\mathbf{y}_{\\mathrm{exact}}(0)$.\n\nCompute the Euclidean norm of the final-time error\n$$\nE(\\Delta t) = \\left\\lVert \\mathbf{y}_{\\mathrm{num}}(t_f;\\Delta t) - \\mathbf{y}_{\\mathrm{exact}}(t_f) \\right\\rVert_2,\n$$\nfor a grid of $\\Delta t$ values. Estimate the observed order $p$ for each method by fitting a line to $\\log_{10}(E(\\Delta t))$ versus $\\log_{10}(\\Delta t)$ using least squares, and report the slope as the observed order. Declare that “order reduction” is detected if the observed order $p$ falls below the method’s nominal order $p_{\\mathrm{nom}}$ by at least $0.5$, i.e., if $p < p_{\\mathrm{nom}} - 0.5$.\n\nTime is measured in seconds; the numerical outputs (orders and flags) are dimensionless. Design the study for the following test suite of parameter sets $(\\lambda_1, \\lambda_2, t_f, \\{\\Delta t\\})$:\n- Case $1$: $(\\lambda_1, \\lambda_2, t_f, \\{\\Delta t\\}) = (10^2, 10, 2\\times 10^{-2}, \\{2\\times 10^{-3}, 10^{-3}, 5\\times 10^{-4}, 2.5\\times 10^{-4}\\})$.\n- Case $2$: $(\\lambda_1, \\lambda_2, t_f, \\{\\Delta t\\}) = (10^4, 10^3, 2\\times 10^{-3}, \\{2\\times 10^{-4}, 10^{-4}, 5\\times 10^{-5}, 2.5\\times 10^{-5}\\})$.\n- Case $3$: $(\\lambda_1, \\lambda_2, t_f, \\{\\Delta t\\}) = (2\\times 10^4, 5\\times 10^3, 10^{-3}, \\{2\\times 10^{-4}, 10^{-4}, 5\\times 10^{-5}, 2.5\\times 10^{-5}\\})$.\n\nFor each case, compute:\n- The observed order for the fourth-order Runge–Kutta method, $p_{\\mathrm{RK4}}$.\n- The observed order for the trapezoidal rule, $p_{\\mathrm{TR}}$.\n- An order reduction flag for the fourth-order Runge–Kutta method, $r_{\\mathrm{RK4}}$, which must be $1$ if order reduction is detected and $0$ otherwise.\n- An order reduction flag for the trapezoidal rule, $r_{\\mathrm{TR}}$, defined analogously.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n[p_{\\mathrm{RK4}}^{(1)}, p_{\\mathrm{TR}}^{(1)}, r_{\\mathrm{RK4}}^{(1)}, r_{\\mathrm{TR}}^{(1)}, p_{\\mathrm{RK4}}^{(2)}, p_{\\mathrm{TR}}^{(2)}, r_{\\mathrm{RK4}}^{(2)}, r_{\\mathrm{TR}}^{(2)}, p_{\\mathrm{RK4}}^{(3)}, p_{\\mathrm{TR}}^{(3)}, r_{\\mathrm{RK4}}^{(3)}, r_{\\mathrm{TR}}^{(3)}],\n$$\nwhere the superscript indicates the case number. The entries must be real numbers for the orders and integers ($0$ or $1$) for the flags. No additional text should be printed.",
            "solution": "The user-provided problem is assessed to be valid. It constitutes a well-posed numerical analysis problem using the Method of Manufactured Solutions (MMS) to study the convergence properties of numerical integrators on a stiff system of ordinary differential equations (ODEs). The problem is scientifically grounded, self-contained, and objective.\n\nThe solution process involves three primary stages:\n1.  Derivation of the forcing term $\\mathbf{g}(t)$ required for the manufactured solution to exactly satisfy the given ODE system.\n2.  Implementation of the two specified numerical integration schemes: the explicit fourth-order Runge-Kutta (RK4) method and the implicit trapezoidal rule (TR).\n3.  Execution of a numerical convergence study to determine the observed order of accuracy for each method and to detect any \"order reduction\" under the stiff conditions specified in the test cases.\n\n**1. Derivation of the Forcing Term $\\mathbf{g}(t)$**\n\nThe ODE system is given by $\\frac{d\\mathbf{y}}{dt} = \\mathbf{M}\\,\\mathbf{y}(t) + \\mathbf{g}(t)$. To ensure the manufactured solution $\\mathbf{y}(t)$ is an exact solution, the forcing term $\\mathbf{g}(t)$ must be defined as:\n$$\n\\mathbf{g}(t) = \\frac{d\\mathbf{y}}{dt} - \\mathbf{M}\\,\\mathbf{y}(t)\n$$\nThe components of the manufactured solution $\\mathbf{y}(t) = [y_1(t), y_2(t), y_3(t)]^\\top$ are:\n$$\ny_1(t) = 0.4\\,e^{-\\alpha t} + 0.2\\,e^{-\\beta t} + 0.1\\,e^{-\\gamma t}\n$$\n$$\ny_2(t) = 0.3\\left(1 - e^{-\\alpha t}\\right)e^{-\\beta t} + 0.1\\,e^{-\\gamma t} + 0.1\\,e^{-\\delta t} = 0.3 e^{-\\beta t} - 0.3 e^{-(\\alpha+\\beta)t} + 0.1 e^{-\\gamma t} + 0.1 e^{-\\delta t}\n$$\n$$\ny_3(t) = 1 - y_1(t) - y_2(t)\n$$\nwith constants $\\alpha = 800\\,\\mathrm{s}^{-1}$, $\\beta = 80\\,\\mathrm{s}^{-1}$, $\\gamma = 8\\,\\mathrm{s}^{-1}$, and $\\delta = 0.8\\,\\mathrm{s}^{-1}$.\n\nFirst, we find the time derivatives $\\frac{dy_1}{dt}$ and $\\frac{dy_2}{dt}$:\n$$\n\\frac{dy_1}{dt} = -0.4\\alpha e^{-\\alpha t} - 0.2\\beta e^{-\\beta t} - 0.1\\gamma e^{-\\gamma t}\n$$\n$$\n\\frac{dy_2}{dt} = -0.3\\beta e^{-\\beta t} + 0.3(\\alpha+\\beta) e^{-(\\alpha+\\beta)t} - 0.1\\gamma e^{-\\gamma t} - 0.1\\delta e^{-\\delta t}\n$$\nNext, we compute the product $\\mathbf{M}\\,\\mathbf{y}(t)$ using the given matrix $\\mathbf{M}$:\n$$\n\\mathbf{M}\\,\\mathbf{y}(t) = \\begin{bmatrix}\n-\\lambda_1 & 0 & 0 \\\\\n\\lambda_1 & -\\lambda_2 & 0 \\\\\n0 & \\lambda_2 & 0\n\\end{bmatrix} \\begin{bmatrix} y_1(t) \\\\ y_2(t) \\\\ y_3(t) \\end{bmatrix} = \\begin{bmatrix} -\\lambda_1 y_1(t) \\\\ \\lambda_1 y_1(t) - \\lambda_2 y_2(t) \\\\ \\lambda_2 y_2(t) \\end{bmatrix}\n$$\nThe components of $\\mathbf{g}(t)$ are then:\n$$\ng_1(t) = \\frac{dy_1}{dt} - (-\\lambda_1 y_1(t)) = 0.4(\\lambda_1 - \\alpha)e^{-\\alpha t} + 0.2(\\lambda_1 - \\beta)e^{-\\beta t} + 0.1(\\lambda_1 - \\gamma)e^{-\\gamma t}\n$$\n$$\ng_2(t) = \\frac{dy_2}{dt} - (\\lambda_1 y_1(t) - \\lambda_2 y_2(t))\n$$\nSubstituting the expressions for the derivatives and abundances yields:\n$$\n\\begin{align*}\ng_2(t) = &(-0.3\\beta e^{-\\beta t} + 0.3(\\alpha+\\beta) e^{-(\\alpha+\\beta)t} - 0.1\\gamma e^{-\\gamma t} - 0.1\\delta e^{-\\delta t}) \\\\\n&- \\lambda_1(0.4 e^{-\\alpha t} + 0.2 e^{-\\beta t} + 0.1 e^{-\\gamma t}) \\\\\n&+ \\lambda_2(0.3 e^{-\\beta t} - 0.3 e^{-(\\alpha+\\beta)t} + 0.1 e^{-\\gamma t} + 0.1 e^{-\\delta t})\n\\end{align*}\n$$\nCollecting terms with common exponentials:\n$$\n\\begin{align*}\ng_2(t) = &-0.4\\lambda_1 e^{-\\alpha t} + (-0.3\\beta - 0.2\\lambda_1 + 0.3\\lambda_2) e^{-\\beta t} \\\\\n&+ (-0.1\\gamma - 0.1\\lambda_1 + 0.1\\lambda_2) e^{-\\gamma t} + (-0.1\\delta + 0.1\\lambda_2) e^{-\\delta t} \\\\\n&+ (0.3(\\alpha+\\beta) - 0.3\\lambda_2) e^{-(\\alpha+\\beta)t}\n\\end{align*}\n$$\nFor $g_3(t)$, we note that the sum of abundances is constant: $\\sum_{i=1}^3 y_i(t) = 1$. Differentiating with respect to time gives $\\sum \\frac{dy_i}{dt} = 0$. Summing the rows of the ODE system gives $\\sum \\frac{dy_i}{dt} = \\sum (\\mathbf{M}\\mathbf{y})_i + \\sum g_i(t)$. The sum of the components of $\\mathbf{M}\\mathbf{y}$ is $(-\\lambda_1 y_1) + (\\lambda_1 y_1 - \\lambda_2 y_2) + (\\lambda_2 y_2) = 0$. Therefore, it must be that $\\sum g_i(t) = 0$, which implies $g_3(t) = -g_1(t) - g_2(t)$. This provides a simple way to compute $g_3(t)$ and serves as a consistency check.\n\n**2. Numerical Integration Methods**\n\nLet the ODE be written as $\\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(t, \\mathbf{y})$, where $\\mathbf{f}(t, \\mathbf{y}) = \\mathbf{M}\\,\\mathbf{y} + \\mathbf{g}(t)$.\n\n**Fourth-Order Runge-Kutta (RK4):** This is an explicit one-step method. The solution at time $t^{n+1} = t^n + \\Delta t$ is computed from the solution at $t^n$ via:\n$$\n\\mathbf{y}^{n+1} = \\mathbf{y}^{n} + \\frac{\\Delta t}{6}(\\mathbf{k}_1 + 2\\mathbf{k}_2 + 2\\mathbf{k}_3 + \\mathbf{k}_4)\n$$\nwhere the stages $\\mathbf{k}_i$ are:\n$$\n\\begin{aligned}\n\\mathbf{k}_1 &= \\mathbf{f}(t^n, \\mathbf{y}^n) \\\\\n\\mathbf{k}_2 &= \\mathbf{f}(t^n + \\frac{\\Delta t}{2}, \\mathbf{y}^n + \\frac{\\Delta t}{2}\\mathbf{k}_1) \\\\\n\\mathbf{k}_3 &= \\mathbf{f}(t^n + \\frac{\\Delta t}{2}, \\mathbf{y}^n + \\frac{\\Delta t}{2}\\mathbf{k}_2) \\\\\n\\mathbf{k}_4 &= \\mathbf{f}(t^n + \\Delta t, \\mathbf{y}^n + \\Delta t\\mathbf{k}_3)\n\\end{aligned}\n$$\nThe nominal order of accuracy is $p_{\\mathrm{nom}} = 4$.\n\n**Trapezoidal Rule (TR):** This is an implicit one-step method, also known as the Crank-Nicolson method. The update rule is:\n$$\n\\mathbf{y}^{n+1} = \\mathbf{y}^{n} + \\frac{\\Delta t}{2}\\left(\\mathbf{f}(t^n, \\mathbf{y}^n) + \\mathbf{f}(t^{n+1}, \\mathbf{y}^{n+1})\\right)\n$$\nSubstituting $\\mathbf{f}(t, \\mathbf{y}) = \\mathbf{M}\\mathbf{y} + \\mathbf{g}(t)$ and rearranging to solve for $\\mathbf{y}^{n+1}$:\n$$\n\\mathbf{y}^{n+1} - \\frac{\\Delta t}{2}\\mathbf{M}\\mathbf{y}^{n+1} = \\mathbf{y}^{n} + \\frac{\\Delta t}{2}\\mathbf{M}\\mathbf{y}^{n} + \\frac{\\Delta t}{2}\\left(\\mathbf{g}(t^n) + \\mathbf{g}(t^{n+1})\\right)\n$$\nThis is the linear system provided in the problem statement, which is solved for $\\mathbf{y}^{n+1}$ at each time step:\n$$\n\\left(\\mathbf{I} - \\frac{\\Delta t}{2}\\mathbf{M}\\right)\\mathbf{y}^{n+1} = \\left(\\mathbf{I} + \\frac{\\Delta t}{2}\\mathbf{M}\\right)\\mathbf{y}^{n} + \\frac{\\Delta t}{2}\\left(\\mathbf{g}(t^{n}) + \\mathbf{g}(t^{n+1})\\right)\n$$\nwhere $\\mathbf{I}$ is the identity matrix. The nominal order of accuracy is $p_{\\mathrm{nom}} = 2$.\n\n**3. Convergence Study and Order Estimation**\n\nFor each parameter set, the system is integrated from $t_0=0$ to $t_f$ using a grid of decreasing time steps $\\Delta t$. The initial condition is set to the exact value $\\mathbf{y}(0) = \\mathbf{y}_{\\mathrm{exact}}(0)$. The Euclidean norm of the final-time error is computed:\n$$\nE(\\Delta t) = \\left\\lVert \\mathbf{y}_{\\mathrm{num}}(t_f; \\Delta t) - \\mathbf{y}_{\\mathrm{exact}}(t_f) \\right\\rVert_2\n$$\nFor a method of order $p$, the error is expected to scale as $E(\\Delta t) \\approx C(\\Delta t)^p$ for some constant $C$ and sufficiently small $\\Delta t$. Taking the base-$10$ logarithm gives a linear relationship:\n$$\n\\log_{10}(E(\\Delta t)) \\approx \\log_{10}(C) + p \\log_{10}(\\Delta t)\n$$\nThe observed order of convergence, $p$, is estimated as the slope of a line fitted to the data points $(\\log_{10}(\\Delta t), \\log_{10}(E))$ using a linear least-squares regression.\n\nOrder reduction is a phenomenon, particularly in stiff systems, where the observed order $p$ is lower than the nominal order $p_{\\mathrm{nom}}$ of the method. According to the problem definition, order reduction is detected if $p < p_{\\mathrm{nom}} - 0.5$. This translates to $p < 3.5$ for RK4 and $p < 1.5$ for TR. The analysis is performed for each of the three test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the specified ODE convergence study problem.\n    \"\"\"\n    \n    # Global constants for the manufactured solution\n    ALPHA = 800.0\n    BETA = 80.0\n    GAMMA = 8.0\n    DELTA = 0.8\n    T0 = 0.0\n\n    def y_exact(t):\n        \"\"\"Computes the exact manufactured solution vector y(t).\"\"\"\n        t = np.asarray(t)\n        exp_at = np.exp(-ALPHA * t)\n        exp_bt = np.exp(-BETA * t)\n        exp_gt = np.exp(-GAMMA * t)\n        exp_dt = np.exp(-DELTA * t)\n\n        y1 = 0.4 * exp_at + 0.2 * exp_bt + 0.1 * exp_gt\n        y2 = 0.3 * (1.0 - exp_at) * exp_bt + 0.1 * exp_gt + 0.1 * exp_dt\n        y3 = 1.0 - y1 - y2\n        \n        # Ensure output is (N, 3) for N time points or (1, 3) for a scalar t\n        return np.array([y1, y2, y3]).T.reshape(-1, 3)\n\n    def g_forcing(t, lam1, lam2):\n        \"\"\"Computes the forcing term vector g(t).\"\"\"\n        t = np.asarray(t)\n\n        exp_at = np.exp(-ALPHA * t)\n        exp_bt = np.exp(-BETA * t)\n        exp_gt = np.exp(-GAMMA * t)\n        exp_dt = np.exp(-DELTA * t)\n        exp_abt = np.exp(-(ALPHA + BETA) * t)\n\n        y1_exact = 0.4 * exp_at + 0.2 * exp_bt + 0.1 * exp_gt\n        y2_exact = 0.3 * exp_bt - 0.3 * exp_abt + 0.1 * exp_gt + 0.1 * exp_dt\n        \n        dy1_dt = -0.4 * ALPHA * exp_at - 0.2 * BETA * exp_bt - 0.1 * GAMMA * exp_gt\n        dy2_dt = -0.3 * BETA * exp_bt + 0.3 * (ALPHA + BETA) * exp_abt - 0.1 * GAMMA * exp_gt - 0.1 * DELTA * exp_dt\n        \n        # g = dy/dt - M*y\n        g1 = dy1_dt - (-lam1 * y1_exact)\n        g2 = dy2_dt - (lam1 * y1_exact - lam2 * y2_exact)\n        g3 = -g1 - g2\n        \n        return np.array([g1, g2, g3]).T.reshape(-1, 3)\n\n    def f_ode(t, y, M, lam1, lam2):\n        \"\"\"RHS of the ODE: f(t, y) = M*y + g(t)\"\"\"\n        return M @ y + g_forcing(t, lam1, lam2).flatten()\n\n    def solve_rk4(M, lam1, lam2, y0, tf, dt):\n        \"\"\"Integrates the ODE system using the classical RK4 method.\"\"\"\n        t = T0\n        y = y0.copy()\n        num_steps = int(round((tf - T0) / dt))\n        \n        for _ in range(num_steps):\n            k1 = f_ode(t, y, M, lam1, lam2)\n            k2 = f_ode(t + 0.5 * dt, y + 0.5 * dt * k1, M, lam1, lam2)\n            k3 = f_ode(t + 0.5 * dt, y + 0.5 * dt * k2, M, lam1, lam2)\n            k4 = f_ode(t + dt, y + dt * k3, M, lam1, lam2)\n            y += (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n            t += dt\n        \n        return y\n\n    def solve_tr(M, lam1, lam2, y0, tf, dt):\n        \"\"\"Integrates the ODE system using the Trapezoidal Rule.\"\"\"\n        t = T0\n        y = y0.copy()\n        num_steps = int(round((tf - T0) / dt))\n        \n        I = np.identity(3)\n        A = I - (dt / 2.0) * M\n        B = I + (dt / 2.0) * M\n        \n        for _ in range(num_steps):\n            t_n = t\n            t_np1 = t + dt\n            \n            g_n = g_forcing(t_n, lam1, lam2).flatten()\n            g_np1 = g_forcing(t_np1, lam1, lam2).flatten()\n            \n            rhs = B @ y + (dt / 2.0) * (g_n + g_np1)\n            y = np.linalg.solve(A, rhs)\n            t = t_np1\n            \n        return y\n\n    test_cases = [\n        # Case 1: (lambda1, lambda2, t_f, {Delta t})\n        (10**2, 10, 2*10**-2, np.array([2e-3, 1e-3, 5e-4, 2.5e-4])),\n        # Case 2\n        (10**4, 10**3, 2*10**-3, np.array([2e-4, 1e-4, 5e-5, 2.5e-5])),\n        # Case 3\n        (2*10**4, 5*10**3, 10**-3, np.array([2e-4, 1e-4, 5e-5, 2.5e-5])),\n    ]\n\n    results = []\n    \n    for lam1, lam2, tf, dt_list in test_cases:\n        M = np.array([[-lam1, 0, 0], [lam1, -lam2, 0], [0, lam2, 0]], dtype=float)\n        y0 = y_exact(T0).flatten()\n        y_final_exact = y_exact(tf).flatten()\n        \n        errors_rk4 = []\n        errors_tr = []\n        \n        for dt in dt_list:\n            y_final_rk4 = solve_rk4(M, lam1, lam2, y0, tf, dt)\n            error_rk4 = np.linalg.norm(y_final_rk4 - y_final_exact)\n            errors_rk4.append(error_rk4)\n            \n            y_final_tr = solve_tr(M, lam1, lam2, y0, tf, dt)\n            error_tr = np.linalg.norm(y_final_tr - y_final_exact)\n            errors_tr.append(error_tr)\n\n        log_dt = np.log10(dt_list)\n        \n        log_err_rk4 = np.log10(np.array(errors_rk4, dtype=float))\n        if np.any(np.isinf(log_err_rk4)) or np.any(np.isnan(log_err_rk4)):\n            p_rk4 = 0.0\n        else:\n            p_rk4 = np.polyfit(log_dt, log_err_rk4, 1)[0]\n        \n        log_err_tr = np.log10(np.array(errors_tr, dtype=float))\n        if np.any(np.isinf(log_err_tr)) or np.any(np.isnan(log_err_tr)):\n            p_tr = 0.0\n        else:\n            p_tr = np.polyfit(log_dt, log_err_tr, 1)[0]\n        \n        r_rk4 = 1 if p_rk4 < (4.0 - 0.5) else 0\n        r_tr = 1 if p_tr < (2.0 - 0.5) else 0\n        \n        results.extend([p_rk4, p_tr, r_rk4, r_tr])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}