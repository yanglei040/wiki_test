{
    "hands_on_practices": [
        {
            "introduction": "Lattice gauge theory calculations are performed at a finite lattice spacing $a$, which introduces discretization artifacts. To extract physical predictions, one must perform a continuum extrapolation to the limit where $a \\to 0$. This practice introduces the essential technique of using a Symanzik-inspired effective theory to model these artifacts and fit data from multiple simulations to determine the true continuum value of an observable. Mastering this procedure is a fundamental step in connecting raw numerical data to the physical predictions of quantum chromodynamics. ",
            "id": "3560462",
            "problem": "You are given measurements of a dimensionless lattice observable $O(a)$ computed at several nonzero lattice spacings $a$, together with independent standard deviations for each measurement. Motivated by the Symanzik effective theory for cutoff effects in lattice gauge theory, assume the following model for discretization errors: $$O(a) = O(0) + c_1 \\, a^2 + c_2 \\, a^4$$ where $O(0)$ is the continuum-limit value to be estimated, and $c_1$ and $c_2$ are unknown coefficients. Suppose each measured value $y_i$ at lattice spacing $a_i$ satisfies $$y_i = O(a_i) + \\varepsilon_i$$ where the $\\varepsilon_i$ are independent Gaussian random variables with zero mean and known variances $\\sigma_i^2$. Work in units where all quantities are dimensionless.\n\nYour task is to estimate $O(0)$ for each dataset using maximum likelihood estimation under the Gaussian noise model, which is equivalent to a weighted least squares fit of the model above.\n\nImplement a program that performs the weighted least squares estimation of $O(0)$ for each of the following three datasets. Each dataset provides a list of lattice spacings $a_i$, observed values $y_i$, and standard deviations $\\sigma_i$. The three datasets are:\n\n- Dataset A (general case, multiple spacings with moderate discretization effects):\n  - $a$: [$0.14$, $0.12$, $0.10$, $0.08$, $0.06$]\n  - $y$: [$0.52000792$, $0.51449632$, $0.50965$, $0.50627952$, $0.50399352$]\n  - $\\sigma$: [$0.0015$, $0.0012$, $0.0010$, $0.0010$, $0.0009$]\n\n- Dataset B (coverage from coarse to fine spacings with stronger $a^4$ effects):\n  - $a$: [$0.20$, $0.16$, $0.12$, $0.08$, $0.04$]\n  - $y$: [$0.7279$, $0.73662304$, $0.74187104$, $0.74562144$, $0.74914384$]\n  - $\\sigma$: [$0.0020$, $0.0016$, $0.0013$, $0.0011$, $0.0010$]\n\n- Dataset C (edge case with the minimal number of points to identify all three parameters):\n  - $a$: [$0.18$, $0.10$, $0.05$]\n  - $y$: [$1.25139952$, $1.2505$, $1.2498125$]\n  - $\\sigma$: [$0.0012$, $0.0008$, $0.0006$]\n\nMathematical and algorithmic requirements:\n\n- Treat the parameter vector as $\\boldsymbol{\\beta} = \\big(O(0), c_1, c_2\\big)$ and the design matrix with rows $\\big(1, a_i^2, a_i^4\\big)$.\n- Use weighted least squares under the assumption of independent Gaussian errors with known variances $\\sigma_i^2$.\n- The estimator must be computed by solving the normal equations in a numerically stable way using standard linear algebra routines.\n- For each dataset, output only the estimated $O(0)$, rounded to $6$ decimal places.\n\nFinal output requirement:\n\n- Your program should produce a single line of output containing the three estimated continuum values $O(0)$ for Dataset A, Dataset B, and Dataset C, in that order, as a comma-separated list enclosed in square brackets. For example, the format must be exactly like this (the numbers below are illustrative only): \n  - Example format: [$0.123456$, $0.234567$, $0.345678$]\n- The numbers in your output must be rounded to exactly $6$ decimal places.\n\nTest suite coverage rationale:\n\n- Dataset A is a general multi-point case testing robust estimation with moderate discretization effects.\n- Dataset B emphasizes stronger higher-order effects and a broader range of lattice spacings to test sensitivity to the $a^4$ term.\n- Dataset C is an edge case with exactly three points, stressing identifiability and matrix conditioning when the model is just-determined.\n\nYour solution should be universally implementable in any modern programming language and must follow the weighted least squares principle as described above. No physical units are required because all quantities are dimensionless.",
            "solution": "The user wants me to solve the problem. I will now proceed with generating the solution.\n### Step 1: Extract Givens\n\nThe problem provides the following information:\n- A model for the lattice observable $O(a)$ as a function of lattice spacing $a$:\n$$O(a) = O(0) + c_1 \\, a^2 + c_2 \\, a^4$$\n- The parameters to be estimated are $O(0)$, $c_1$, and $c_2$.\n- The relationship between a measurement $y_i$ at lattice spacing $a_i$ and the model is:\n$$y_i = O(a_i) + \\varepsilon_i$$\n- The errors $\\varepsilon_i$ are independent Gaussian random variables with mean $0$ and known variances $\\sigma_i^2$.\n- The parameter vector is defined as $\\boldsymbol{\\beta} = \\big(O(0), c_1, c_2\\big)$.\n- The design matrix has rows of the form $\\big(1, a_i^2, a_i^4\\big)$.\n- Three datasets are provided:\n    - **Dataset A**:\n        - $a$: [$0.14$, $0.12$, $0.10$, $0.08$, $0.06$]\n        - $y$: [$0.52000792$, $0.51449632$, $0.50965$, $0.50627952$, $0.50399352$]\n        - $\\sigma$: [$0.0015$, $0.0012$, $0.0010$, $0.0010$, $0.0009$]\n    - **Dataset B**:\n        - $a$: [$0.20$, $0.16$, $0.12$, $0.08$, $0.04$]\n        - $y$: [$0.7279$, $0.73662304$, $0.74187104$, $0.74562144$, $0.74914384$]\n        - $\\sigma$: [$0.0020$, $0.0016$, $0.0013$, $0.0011$, $0.0010$]\n    - **Dataset C**:\n        - $a$: [$0.18$, $0.10$, $0.05$]\n        - $y$: [$1.25139952$, $1.2505$, $1.2498125$]\n        - $\\sigma$: [$0.0012$, $0.0008$, $0.0006$]\n- The task is to estimate $O(0)$ for each dataset using maximum likelihood estimation, which is equivalent to weighted least squares.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The model $O(a) = O(0) + c_1 \\, a^2 + c_2 \\, a^4$ is a standard expansion used in Symanzik effective theory to describe discretization errors for non-improved Wilson-fermion or gauge-action observables in lattice gauge theory. The use of a Gaussian noise model and maximum likelihood estimation (equivalent to weighted least squares for this case) is a standard and robust statistical method for parameter estimation from experimental or numerical data. The problem is firmly grounded in computational physics principles.\n- **Well-Posed**: The problem is to solve a linear regression problem for the parameters $\\boldsymbol{\\beta} = \\big(O(0), c_1, c_2\\big)^T$. A unique solution exists if the normal equations matrix is invertible. For Datasets A and B, there are $5$ data points to determine $3$ parameters, making the system over-determined and generally well-posed. For Dataset C, there are exactly $3$ data points for $3$ parameters, making the system just-determined. The design matrix $X$ for Dataset C has rows $\\big(1, a_i^2, a_i^4\\big)$. Its determinant is proportional to $(a_2^2 - a_1^2)(a_3^2 - a_1^2)(a_3^2 - a_2^2)$. Since all given $a_i$ values are distinct and positive, the determinant is non-zero, the matrix is invertible, and a unique solution exists. The problem is well-posed for all datasets.\n- **Objective**: The problem is stated in precise, unambiguous mathematical language. The data and requirements are objective and clearly specified.\n\n### Step 3: Verdict and Action\n\n**Verdict**: The problem is valid, scientifically sound, well-posed, and objective. I will proceed with the solution.\n\n###\nThe goal is to find the continuum-limit value $O(0)$ of an observable by fitting the model $O(a) = O(0) + c_1 a^2 + c_2 a^4$ to data points $(a_i, y_i)$ with associated uncertainties $\\sigma_i$. This is a linear regression problem, as the model is linear in its parameters $\\boldsymbol{\\beta} = \\big(O(0), c_1, c_2\\big)^T$. The system of equations for $n$ data points can be expressed in matrix form as:\n$$ \\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} $$\nwhere $\\mathbf{y}$ is the $n \\times 1$ vector of observed values, $X$ is the $n \\times 3$ design matrix, $\\boldsymbol{\\beta}$ is the $3 \\times 1$ parameter vector, and $\\boldsymbol{\\varepsilon}$ is the $n \\times 1$ vector of random errors.\n\nThe $i$-th row of the design matrix $X$ corresponds to the $i$-th measurement and is given by $\\big(1, a_i^2, a_i^4\\big)$. Thus,\n$$\nX = \\begin{pmatrix}\n1 & a_1^2 & a_1^4 \\\\\n1 & a_2^2 & a_2^4 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & a_n^2 & a_n^4\n\\end{pmatrix}\n, \\quad\n\\boldsymbol{\\beta} = \\begin{pmatrix}\nO(0) \\\\ c_1 \\\\ c_2\n\\end{pmatrix}\n, \\quad\n\\mathbf{y} = \\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{pmatrix}\n$$\nThe problem states that the errors $\\varepsilon_i$ are independent and normally distributed with mean $0$ and variance $\\sigma_i^2$. Maximizing the likelihood of observing the data $\\mathbf{y}$ given the parameters $\\boldsymbol{\\beta}$ is equivalent to minimizing the weighted sum of squared residuals, also known as the chi-squared, $\\chi^2$:\n$$ \\chi^2(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( \\frac{y_i - (O(0) + c_1 a_i^2 + c_2 a_i^4)}{\\sigma_i} \\right)^2 $$\nThis can be expressed in matrix form as:\n$$ \\chi^2(\\boldsymbol{\\beta}) = (\\mathbf{y} - X\\boldsymbol{\\beta})^T W (\\mathbf{y} - X\\boldsymbol{\\beta}) $$\nwhere $W$ is the weight matrix, which is the inverse of the error covariance matrix. Since the errors are independent, $W$ is a diagonal matrix with elements $W_{ii} = 1/\\sigma_i^2$.\n\nThe parameter vector $\\hat{\\boldsymbol{\\beta}}$ that minimizes $\\chi^2$ is found by solving the weighted least squares normal equations:\n$$ (X^T W X) \\hat{\\boldsymbol{\\beta}} = X^T W \\mathbf{y} $$\nThe solution is formally given by:\n$$ \\hat{\\boldsymbol{\\beta}} = (X^T W X)^{-1} X^T W \\mathbf{y} $$\nFor numerical stability, it is preferable to solve this system without explicitly computing the inverse. A standard method is to transform the weighted least squares (WLS) problem into an ordinary least squares (OLS) problem. We define a scaled vector $\\mathbf{y}'$ and a scaled design matrix $X'$:\n$$ \\mathbf{y}' = W^{1/2} \\mathbf{y} \\quad \\text{and} \\quad X' = W^{1/2} X $$\nwhere $W^{1/2}$ is a diagonal matrix with entries $(W^{1/2})_{ii} = 1/\\sigma_i$. This means the $i$-th row of $X'$ is the $i$-th row of $X$ divided by $\\sigma_i$, and the $i$-th element of $\\mathbf{y}'$ is $y_i/\\sigma_i$. The $\\chi^2$ expression becomes the unweighted sum of squares for the transformed variables:\n$$ \\chi^2(\\boldsymbol{\\beta}) = (\\mathbf{y}' - X'\\boldsymbol{\\beta})^T (\\mathbf{y}' - X'\\boldsymbol{\\beta}) $$\nThis is a standard OLS problem, whose solution $\\hat{\\boldsymbol{\\beta}}$ can be found using robust numerical techniques like QR decomposition. This is the procedure implemented by standard library functions such as `numpy.linalg.lstsq`.\n\nThe algorithm for each dataset is as follows:\n1. Construct the vector of observations $\\mathbf{y}$ from the provided $y_i$ values.\n2. Construct the design matrix $X$, where the $i$-th row is $(1, a_i^2, a_i^4)$.\n3. Construct the scaled matrix $X'$ by dividing each row $i$ of $X$ by the corresponding standard deviation $\\sigma_i$.\n4. Construct the scaled vector $\\mathbf{y}'$ by dividing each element $y_i$ by $\\sigma_i$.\n5. Solve the linear least squares system $X'\\boldsymbol{\\beta} \\approx \\mathbf{y}'$ for the parameter vector $\\hat{\\boldsymbol{\\beta}} = \\big(\\hat{O}(0), \\hat{c}_1, \\hat{c}_2\\big)^T$.\n6. The first element of the resulting vector, $\\hat{\\boldsymbol{\\beta}}_0$, is the estimated value of $O(0)$.\n7. Round the estimate to $6$ decimal places as required. This procedure is repeated for all three datasets.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs weighted least squares estimation for continuum limit extrapolation\n    on three datasets as specified in the problem.\n    \"\"\"\n    \n    # Define the three datasets from the problem statement.\n    test_cases = [\n        # Dataset A (general case)\n        {\n            \"a\": [0.14, 0.12, 0.10, 0.08, 0.06],\n            \"y\": [0.52000792, 0.51449632, 0.50965, 0.50627952, 0.50399352],\n            \"sigma\": [0.0015, 0.0012, 0.0010, 0.0010, 0.0009]\n        },\n        # Dataset B (stronger a^4 effects)\n        {\n            \"a\": [0.20, 0.16, 0.12, 0.08, 0.04],\n            \"y\": [0.7279, 0.73662304, 0.74187104, 0.74562144, 0.74914384],\n            \"sigma\": [0.0020, 0.0016, 0.0013, 0.0011, 0.0010]\n        },\n        # Dataset C (minimal number of points)\n        {\n            \"a\": [0.18, 0.10, 0.05],\n            \"y\": [1.25139952, 1.2505, 1.2498125],\n            \"sigma\": [0.0012, 0.0008, 0.0006]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Extract data and convert to numpy arrays for calculation\n        a = np.array(case[\"a\"])\n        y = np.array(case[\"y\"])\n        sigma = np.array(case[\"sigma\"])\n        \n        # The model is O(a) = beta_0 + beta_1 * a^2 + beta_2 * a^4\n        # where beta_0 = O(0), beta_1 = c1, beta_2 = c2.\n        # This is a linear model. We perform a weighted least squares fit.\n        \n        # Construct the design matrix X.\n        # The columns are the basis functions: 1, a^2, a^4.\n        X = np.vstack([np.ones_like(a), a**2, a**4]).T\n        \n        # A weighted least squares problem with weights w_i = 1/sigma_i^2 is\n        # equivalent to an ordinary least squares problem on variables scaled\n        # by 1/sigma_i.\n        \n        # Scale the rows of the design matrix X by 1/sigma.\n        # The inv_sigma must be a column vector to broadcast correctly.\n        inv_sigma = 1.0 / sigma\n        X_prime = X * inv_sigma[:, np.newaxis]\n        \n        # Scale the measurement vector y by 1/sigma.\n        y_prime = y * inv_sigma\n        \n        # Solve the linear least squares system X_prime * beta = y_prime\n        # for the parameter vector beta = [O(0), c1, c2].\n        # np.linalg.lstsq is a robust solver, typically using QR decomposition.\n        beta_hat, _, _, _ = np.linalg.lstsq(X_prime, y_prime, rcond=None)\n        \n        # The first element of the solution vector is the estimate for O(0).\n        O0_estimate = beta_hat[0]\n        \n        results.append(O0_estimate)\n\n    # Format the results to exactly 6 decimal places and print in the required format.\n    results_formatted = [\"{:.6f}\".format(r) for r in results]\n    print(f\"[{','.join(results_formatted)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "For certain physical systems, such as those involving heavy quarks or thermodynamics, it is computationally efficient to use an anisotropic lattice with different temporal ($a_t$) and spatial ($a_s$) spacings. This exercise demonstrates how to calibrate the renormalized anisotropy, $\\xi = a_s/a_t$, by requiring that the simulation correctly reproduces the underlying Euclidean spacetime symmetries. You will derive and implement estimators for $\\xi$ from both hadron dispersion relations and Wilson loop ratios, learning how different physical observables are used to tune and cross-validate simulation parameters. ",
            "id": "3560441",
            "problem": "Consider Euclidean Lattice Quantum Chromodynamics (LQCD) with anisotropic discretization, where spatial lattice spacing is $a_s$ and temporal lattice spacing is $a_t$. The renormalized anisotropy is defined by $\\xi = a_s / a_t$. In a finite spatial box of $N_s$ sites with periodic boundary conditions, allowed spatial momenta are labeled by integers $n \\in \\{0,1,2,\\dots\\}$ and satisfy $p_{\\ell}(n) = a_s p_{\\mathrm{phys}}(n) = \\frac{2\\pi n}{N_s}$, expressed in dimensionless lattice units. Energies $E_{\\ell}$ are also dimensionless, with $E_{\\ell} = a_t E_{\\mathrm{phys}}$.\n\nStarting from the relativistic energy-momentum relation and the definition of Euclidean correlators and Wilson loops, derive principled estimators of the renormalized anisotropy $\\xi$ from two independent observables:\n- the single-hadron dispersion relation, using measured energies $E_{\\ell}(n)$ at several spatial momenta $p_{\\ell}(n)$, and\n- rectangular Wilson loop ratios, using temporal Wilson loops $W(R,T)$ and purely spatial Wilson loops $W_s(R,S)$.\n\nYour program must implement both estimators and return them for each test case specified below. All quantities are to be treated in dimensionless lattice units, and all angles are in radians. The final outputs are dimensionless numbers.\n\nFundamental base for derivation:\n- The relativistic energy-momentum relation in natural units is $E_{\\mathrm{phys}}^2 = m_{\\mathrm{phys}}^2 + \\vec{p}_{\\mathrm{phys}}^{\\,2}$.\n- In Euclidean LQCD, a zero-temperature rectangular Wilson loop $W(R,T)$ with spatial extent $R$ and temporal extent $T$ decays approximately exponentially with the temporal extent, governed by the static potential; similarly, an all-spatial rectangular loop $W_s(R,S)$ decays exponentially with its spatial extent.\n- Lattice quantities satisfy $E_{\\ell} = a_t E_{\\mathrm{phys}}$ and $p_{\\ell} = a_s p_{\\mathrm{phys}}$.\n\nEstimator design requirements:\n1. From hadron dispersion: Use a statistically consistent fitting procedure on $E_{\\ell}(n)$ measured at several $p_{\\ell}(n)$ to estimate the coefficient that encodes $\\xi$ and hence determine $\\xi$. Do not introduce any unmotivated shortcut formulas; begin at the fundamental relations and map them to lattice variables to design the estimator.\n2. From Wilson loops: Construct effective exponents from ratios $W(R,T+1)/W(R,T)$ at fixed $R$ for temporal loops and $W_s(R,S+1)/W_s(R,S)$ at fixed $R$ for spatial loops. Use these effective exponents to form an estimator for $\\xi$ that is robust across $R$ when the large-$T$ and large-$S$ behavior is reached.\n\nTest suite specification:\nFor each test case $k \\in \\{1,2,3\\}$, you are given:\n- A spatial size $N_s^{(k)}$, a set of momentum indices $n^{(k)}$, and measured dimensionless hadron energies $E_{\\ell}^{(k)}(n)$.\n- For Wilson loops, three spatial extents $R \\in \\{1,2,3\\}$, amplitude lists $A^{(k)}(R)$ and $B^{(k)}(R)$, temporal exponents $E_t^{(k)}(R)$, spatial exponents $E_s^{(k)}(R)$, and extent sets $T^{(k)}$ and $S^{(k)}$. The temporal Wilson loops are defined by $W^{(k)}(R,T) = A^{(k)}(R) \\exp(-E_t^{(k)}(R)\\, T)$ for $T \\in T^{(k)}$, and the spatial Wilson loops by $W_s^{(k)}(R,S) = B^{(k)}(R) \\exp(-E_s^{(k)}(R)\\, S)$ for $S \\in S^{(k)}$.\n\nProvide estimators $\\xi_{\\mathrm{had}}^{(k)}$ from hadron dispersion and $\\xi_{\\mathrm{W}}^{(k)}$ from Wilson loop ratios. For each test case, also report a boolean consistency flag that is true if the absolute difference between the two estimates is below a tolerance $\\delta = 10^{-3}$.\n\nThe required final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- Each element corresponds to one test case and is itself a sublist of the form $[\\xi_{\\mathrm{had}}^{(k)}, \\xi_{\\mathrm{W}}^{(k)}, \\mathrm{consistent}^{(k)}]$.\n- Example shape (not actual values): $[[x_1,y_1,\\mathrm{True}], [x_2,y_2,\\mathrm{False}], [x_3,y_3,\\mathrm{True}]]$.\n\nTest cases:\n- Test case $1$ (happy path, near-isotropic):\n  - $N_s^{(1)} = 32$\n  - $n^{(1)} = \\{0,1,2,3\\}$\n  - $E_{\\ell}^{(1)}(n)$ provided as the list $\\{0.600000,\\,0.631303,\\,0.717145,\\,0.841129\\}$\n  - Temporal Wilson loops:\n    - $R = \\{1,2,3\\}$, $T^{(1)} = \\{1,2,3,4\\}$\n    - $A^{(1)}(R) = \\{0.900000,\\,0.850000,\\,0.800000\\}$\n    - $E_t^{(1)}(R) = \\{0.500000,\\,0.700000,\\,0.900000\\}$\n    - $W^{(1)}(R,T) = A^{(1)}(R) \\exp(-E_t^{(1)}(R)\\, T)$\n  - Spatial Wilson loops:\n    - $S^{(1)} = \\{1,2,3,4\\}$\n    - $B^{(1)}(R) = \\{0.880000,\\,0.830000,\\,0.780000\\}$\n    - $E_s^{(1)}(R) = \\{0.500000,\\,0.700000,\\,0.900000\\}$\n    - $W_s^{(1)}(R,S) = B^{(1)}(R) \\exp(-E_s^{(1)}(R)\\, S)$\n- Test case $2$ (anisotropic, $\\xi > 1$):\n  - $N_s^{(2)} = 24$\n  - $n^{(2)} = \\{0,1,2,3\\}$\n  - $E_{\\ell}^{(2)}(n)$ provided as the list $\\{0.500000,\\,0.513967,\\,0.553764,\\,0.614430\\}$\n  - Temporal Wilson loops:\n    - $R = \\{1,2,3\\}$, $T^{(2)} = \\{1,2,3,4\\}$\n    - $A^{(2)}(R) = \\{0.920000,\\,0.870000,\\,0.830000\\}$\n    - $E_t^{(2)}(R) = \\{0.450000,\\,0.700000,\\,0.950000\\}$\n    - $W^{(2)}(R,T) = A^{(2)}(R) \\exp(-E_t^{(2)}(R)\\, T)$\n  - Spatial Wilson loops:\n    - $S^{(2)} = \\{1,2,3,4\\}$\n    - $B^{(2)}(R) = \\{0.890000,\\,0.840000,\\,0.790000\\}$\n    - $E_s^{(2)}(R) = \\{0.990000,\\,1.540000,\\,2.090000\\}$\n    - $W_s^{(2)}(R,S) = B^{(2)}(R) \\exp(-E_s^{(2)}(R)\\, S)$\n- Test case $3$ (anisotropic, $\\xi < 1$, edge case with fewer momenta):\n  - $N_s^{(3)} = 16$\n  - $n^{(3)} = \\{0,1,2\\}$\n  - $E_{\\ell}^{(3)}(n)$ provided as the list $\\{0.300000,\\,0.575392,\\,1.026926\\}$\n  - Temporal Wilson loops:\n    - $R = \\{1,2,3\\}$, $T^{(3)} = \\{1,2,3,4\\}$\n    - $A^{(3)}(R) = \\{0.930000,\\,0.880000,\\,0.830000\\}$\n    - $E_t^{(3)}(R) = \\{0.300000,\\,0.500000,\\,0.700000\\}$\n    - $W^{(3)}(R,T) = A^{(3)}(R) \\exp(-E_t^{(3)}(R)\\, T)$\n  - Spatial Wilson loops:\n    - $S^{(3)} = \\{1,2,3,4\\}$\n    - $B^{(3)}(R) = \\{0.910000,\\,0.860000,\\,0.810000\\}$\n    - $E_s^{(3)}(R) = \\{0.240000,\\,0.400000,\\,0.560000\\}$\n    - $W_s^{(3)}(R,S) = B^{(3)}(R) \\exp(-E_s^{(3)}(R)\\, S)$\n\nAlgorithmic goals:\n- Implement a robust linear estimator for the hadron-based $\\xi$ using $E_{\\ell}^2$ versus $p_{\\ell}^2$ across the provided momenta for each test case.\n- Implement effective exponent extraction from loop ratios and an estimator for $\\xi$ from the ratio of spatial to temporal effective exponents aggregated over $R$.\n- Return the final results in the required format, with a tolerance $\\delta = 10^{-3}$ for the boolean consistency flag, i.e., report $\\mathrm{True}$ if $|\\xi_{\\mathrm{had}} - \\xi_{\\mathrm{W}}| \\le \\delta$ and $\\mathrm{False}$ otherwise.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the exact form $[[\\xi_{\\mathrm{had}}^{(1)},\\xi_{\\mathrm{W}}^{(1)},\\mathrm{consistent}^{(1)}],[\\xi_{\\mathrm{had}}^{(2)},\\xi_{\\mathrm{W}}^{(2)},\\mathrm{consistent}^{(2)}],[\\xi_{\\mathrm{had}}^{(3)},\\xi_{\\mathrm{W}}^{(3)},\\mathrm{consistent}^{(3)}]]$.",
            "solution": "The problem as stated is scientifically grounded, objective, and well-posed. It provides a complete set of data and definitions required to derive and implement estimators for the renormalized anisotropy, $\\xi$, in the context of anisotropic lattice QCD. The problem's basis in the relativistic dispersion relation and the behavior of Wilson loops is fundamental to the field. The provided test cases are simplified (idealized exponential decay for Wilson loops) but well-defined, making the problem a valid and tractable exercise in applying theoretical principles. We may therefore proceed with the derivation of the estimators.\n\nThe analysis is based on two independent physical observables: the hadron dispersion relation and the ratios of rectangular Wilson loops.\n\n**1. Estimator from the Hadron Dispersion Relation ($\\xi_{\\mathrm{had}}$)**\n\nThe fundamental principle is the relativistic energy-momentum relation in the continuum, expressed in natural units ($\\hbar=c=1$):\n$$\nE_{\\mathrm{phys}}^2 = m_{\\mathrm{phys}}^2 + \\vec{p}_{\\mathrm{phys}}^{\\,2}\n$$\nwhere $E_{\\mathrm{phys}}$ is the energy, $m_{\\mathrm{phys}}$ is the rest mass, and $\\vec{p}_{\\mathrm{phys}}$ is the momentum of the hadron.\n\nIn an anisotropic lattice discretization, physical quantities are related to dimensionless lattice quantities via the spatial and temporal lattice spacings, $a_s$ and $a_t$ respectively. The relations are given as:\n$$\nE_{\\ell} = a_t E_{\\mathrm{phys}} \\implies E_{\\mathrm{phys}} = \\frac{E_{\\ell}}{a_t}\n$$\n$$\np_{\\ell} = a_s p_{\\mathrm{phys}} \\implies p_{\\mathrm{phys}} = \\frac{p_{\\ell}}{a_s}\n$$\nHere, $E_{\\ell}$ and $p_{\\ell}$ are the dimensionless energy and momentum in lattice units. The momenta are quantized on a finite lattice of $N_s$ sites with periodic boundary conditions, $p_{\\ell}(n) = \\frac{2\\pi n}{N_s}$ for integer $n$.\n\nSubstituting the lattice-unit expressions into the continuum energy-momentum relation yields:\n$$\n\\left(\\frac{E_{\\ell}}{a_t}\\right)^2 = m_{\\mathrm{phys}}^2 + \\left(\\frac{p_{\\ell}}{a_s}\\right)^2\n$$\nTo make the renormalized anisotropy $\\xi = a_s / a_t$ appear, we multiply the entire equation by $a_t^2$:\n$$\nE_{\\ell}^2 = (a_t m_{\\mathrm{phys}})^2 + \\frac{a_t^2}{a_s^2} p_{\\ell}^2\n$$\nRecognizing that $(a_t/a_s)^2 = (1/\\xi)^2$ and defining the dimensionless mass in temporal lattice units as $m_{\\ell,t} = a_t m_{\\mathrm{phys}}$, we arrive at the lattice dispersion relation:\n$$\nE_{\\ell}^2 = m_{\\ell,t}^2 + \\frac{1}{\\xi^2} p_{\\ell}^2\n$$\nThis equation establishes a linear relationship between $E_{\\ell}^2$ and $p_{\\ell}^2$. If we consider a set of measurements of hadron energies $E_{\\ell}(n)$ at different momenta $p_{\\ell}(n)$, we can model this as a linear equation $Y = C + M X$, where:\n- $Y = E_{\\ell}^2$\n- $X = p_{\\ell}^2$\n- $C = m_{\\ell,t}^2$ is the intercept, corresponding to the squared hadron mass in temporal lattice units.\n- $M = 1/\\xi^2$ is the slope.\n\nTo determine $\\xi$, we can perform a linear regression on the data points $(p_{\\ell}(n)^2, E_{\\ell}(n)^2)$ for the given momentum modes $n$. The slope $M$ of the best-fit line directly yields an estimate for the anisotropy. From $M = 1/\\xi^2$, we can solve for $\\xi$. Since $\\xi$ is a ratio of positive definite lattice spacings, we take the positive root:\n$$\n\\xi_{\\mathrm{had}} = \\sqrt{\\frac{1}{M}} = \\frac{1}{\\sqrt{M}}\n$$\nThis provides a principled, statistically robust estimator for the anisotropy based on the measured hadron spectrum.\n\n**2. Estimator from Wilson Loop Ratios ($\\xi_{\\mathrm{W}}$)**\n\nThe second estimator is derived from the behavior of rectangular Wilson loops. In a confining gauge theory like QCD, the expectation value of a large Wilson loop exhibits an area law. For a rectangular loop, this manifests as an exponential decay.\n\nFor a temporal Wilson loop $W(R,T)$, with spatial extent $R$ and temporal extent $T$ (in lattice units), the decay for large $T$ is governed by the static quark-antiquark potential, $V_{\\mathrm{phys}}$. The physical area is $(R a_s) \\times (T a_t)$. However, the standard formulation is that the loop's expectation value evolves in time, so $W(R,T) \\propto \\exp(-V_{\\mathrm{phys}}(r) \\cdot t)$, where $r = R a_s$ is the physical separation and $t = T a_t$ is the physical time. This gives:\n$$\nW(R,T) \\sim \\exp(-V_{\\mathrm{phys}}(R a_s) \\cdot T a_t)\n$$\nThe problem provides a functional form $W(R,T) = A(R) \\exp(-E_t(R) T)$. Comparing the exponents, we identify the dimensionless temporal exponent $E_t(R)$ as:\n$$\nE_t(R) = a_t V_{\\mathrm{phys}}(R a_s)\n$$\nThis quantity is the static potential in temporal lattice units. The problem requests the construction of effective exponents from ratios, which for large $T$ are defined as $V_{t, \\text{eff}}(R) = -\\ln(W(R,T+1)/W(R,T))$. For the given ideal exponential form, this calculation confirms $V_{t, \\text{eff}}(R) = E_t(R)$ for all $T$.\n\nFor a purely spatial Wilson loop $W_s(R,S)$, with extents $R$ and $S$ in a spatial plane, the underlying physics is governed by the same static potential $V_{\\mathrm{phys}}$, as dictated by Euclidean invariance. The physical lengths are $R_{phys} = R a_s$ and $S_{phys} = S a_s$. The behavior is thus:\n$$\nW_s(R,S) \\sim \\exp(-V_{\\mathrm{phys}}(R a_s) \\cdot S a_s)\n$$\nThe problem provides the form $W_s(R,S) = B(R) \\exp(-E_s(R) S)$. Comparing exponents, we identify the dimensionless spatial exponent $E_s(R)$ as:\n$$\nE_s(R) = a_s V_{\\mathrm{phys}}(R a_s)\n$$\nThis is the static potential in spatial lattice units. Again, the effective exponent from ratios $V_{s, \\text{eff}}(R) = -\\ln(W_s(R,S+1)/W_s(R,S))$ is trivially $E_s(R)$ for all $S$.\n\nWe now have two expressions for the same physical quantity, $V_{\\mathrm{phys}}(R a_s)$, measured in different dimensionless units. The ratio of these two expressions eliminates the unknown potential $V_{\\mathrm{phys}}$:\n$$\n\\frac{E_s(R)}{E_t(R)} = \\frac{a_s V_{\\mathrm{phys}}(R a_s)}{a_t V_{\\mathrm{phys}}(R a_s)} = \\frac{a_s}{a_t} = \\xi\n$$\nThis relationship should hold for any separation $R$ for which a potential description is valid. To obtain a single robust estimator, we can average a set of determinations of $\\xi$ over the available values of $R$. For each test case, we are given exponents for $R \\in \\{1,2,3\\}$. The estimator $\\xi_{\\mathrm{W}}$ is therefore the arithmetic mean:\n$$\n\\xi_{\\mathrm{W}} = \\frac{1}{N_R} \\sum_{R} \\frac{E_s(R)}{E_t(R)}\n$$\nwhere the sum is over the provided $R$ values and $N_R$ is the number of such values.\n\nThe computational implementation will follow these two derived procedures for each test case. A consistency check is performed by comparing the two estimators, $\\xi_{\\mathrm{had}}$ and $\\xi_{\\mathrm{W}}$, against a given tolerance $\\delta = 10^{-3}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the lattice QCD anisotropy problem by implementing two estimators\n    for the renormalized anisotropy xi and comparing them.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1 (happy path, near-isotropic)\n        {\n            \"Ns\": 32,\n            \"n\": [0, 1, 2, 3],\n            \"El\": [0.600000, 0.631303, 0.717145, 0.841129],\n            \"Et\": [0.500000, 0.700000, 0.900000],\n            \"Es\": [0.500000, 0.700000, 0.900000]\n        },\n        # Test case 2 (anisotropic, xi > 1)\n        {\n            \"Ns\": 24,\n            \"n\": [0, 1, 2, 3],\n            \"El\": [0.500000, 0.513967, 0.553764, 0.614430],\n            \"Et\": [0.450000, 0.700000, 0.950000],\n            \"Es\": [0.990000, 1.540000, 2.090000]\n        },\n        # Test case 3 (anisotropic, xi < 1, edge case with fewer momenta)\n        {\n            \"Ns\": 16,\n            \"n\": [0, 1, 2],\n            \"El\": [0.300000, 0.575392, 1.026926],\n            \"Et\": [0.300000, 0.500000, 0.700000],\n            \"Es\": [0.240000, 0.400000, 0.560000]\n        }\n    ]\n    \n    delta = 1e-3\n    all_results = []\n\n    for case in test_cases:\n        # 1. Hadron Dispersion Estimator\n        Ns = case[\"Ns\"]\n        n_vals = np.array(case[\"n\"])\n        El_vals = np.array(case[\"El\"])\n\n        # Calculate squared momenta: p_l^2 = (2*pi*n/Ns)^2\n        pl_sq_vals = ((2 * np.pi * n_vals) / Ns)**2\n        \n        # Calculate squared energies: E_l^2\n        El_sq_vals = El_vals**2\n\n        # Perform a linear fit of E_l^2 vs p_l^2.\n        # E_l^2 = m_l^2 + (1/xi^2) * p_l^2\n        # The slope M of the fit is 1/xi^2.\n        slope, intercept = np.polyfit(pl_sq_vals, El_sq_vals, 1)\n        \n        # Avoid division by zero or negative slope, though not expected here.\n        if slope <= 0:\n            xi_had = np.inf\n        else:\n            xi_had = np.sqrt(1.0 / slope)\n\n        # 2. Wilson Loop Estimator\n        Et_vals = np.array(case[\"Et\"])\n        Es_vals = np.array(case[\"Es\"])\n\n        # xi = Es(R) / Et(R) for each R.\n        # Average over all R values.\n        xi_ratios = Es_vals / Et_vals\n        xi_W = np.mean(xi_ratios)\n\n        # 3. Consistency Check\n        is_consistent = np.abs(xi_had - xi_W) <= delta\n        \n        # Format the result for this case as a list\n        case_result = [xi_had, xi_W, is_consistent]\n        all_results.append(case_result)\n\n    # Format the final output string exactly as specified, without spaces\n    # in the sublists.\n    result_strings = []\n    for res in all_results:\n        # Convert boolean to a lowercase string if needed, but 'True'/'False' is fine.\n        res_str = f\"[{res[0]},{res[1]},{str(res[2])}]\"\n        result_strings.append(res_str)\n        \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Lattice QCD simulations are exceptionally computationally intensive, making performance optimization a critical aspect of the field. This practice introduces the roofline model, a powerful framework for analyzing and predicting the performance of algorithms on modern hardware like GPUs. You will perform a roofline analysis for a key part of the Hybrid Monte Carlo (HMC) algorithm, evaluating the arithmetic intensity and the potential performance gains from kernel fusion, thereby gaining crucial skills in identifying and alleviating computational bottlenecks. ",
            "id": "3560466",
            "problem": "You are tasked with implementing a complete, runnable program that performs a principled roofline analysis for a simplified, yet scientifically sound, model of a Hybrid Monte Carlo (HMC) integrator step for lattice gauge theory with special unitary group of degree three ($SU(3)$). The goal is to compute, for several parameter sets, the arithmetic intensity, attainable performance under the roofline model, and the expected performance impact of kernel fusion between a gauge-force accumulation kernel and a link-update kernel on a graphics processing unit (GPU). The program must produce results that are universally applicable in computational terms and avoid reliance on hardware-specific micro-details beyond the provided peak compute and bandwidth parameters.\n\nStart from the following fundamental base:\n\n- In lattice gauge theory for the strong interaction, the gauge field is represented by link variables $U \\in SU(3)$ stored on edges of a hypercubic lattice. A link is a $3 \\times 3$ special unitary complex matrix. The Lie algebra element (the conjugate momentum in the Hamiltonian dynamics of Hybrid Monte Carlo) lives in $\\mathfrak{su}(3)$ and can be represented by $8$ real parameters corresponding to the eight generators of $SU(3)$. \n- The Hybrid Monte Carlo (HMC) algorithm evolves $(U,P)$ using Hamiltonian dynamics integrating gauge forces computed from local staples of neighboring links. A simplified model of a single integrator substep consists of:\n  1. A gauge-force kernel that reads neighbor $SU(3)$ link matrices to accumulate a force contribution and updates the momentum $P$.\n  2. A link-update kernel that constructs an exponential representation from $P$ and multiplies it into $U$.\n- The roofline model bounds attainable floating-point performance by the minimum of compute peak and the product of memory bandwidth and arithmetic intensity. If the memory bandwidth is $B$ in gigabytes per second and the arithmetic intensity is $I$ in floating-point operations per byte, then the attainable performance is $P_{\\mathrm{att}} = \\min\\{P_{\\mathrm{peak}}, B \\times I\\}$, where $P_{\\mathrm{peak}}$ is the peak floating-point throughput in gigaflops per second.\n- For a dense complex $3 \\times 3$ matrix-matrix product, we model the floating-point cost as $F_M = 198$ real floating-point operations. For an elementwise $3 \\times 3$ complex matrix addition, we model the floating-point cost as $F_A = 18$ real floating-point operations. These costs are widely used in lattice quantum chromodynamics (QCD) performance modeling.\n- Memory sizes follow from the data layout:\n  - Each $SU(3)$ link matrix is stored as $18$ real numbers; its size is $S_U = 18 s$ bytes, where $s$ is the bytes per real number ($s = 4$ for single precision or $s = 8$ for double precision).\n  - Each $\\mathfrak{su}(3)$ momentum is stored as $8$ real numbers; its size is $S_P = 8 s$ bytes.\n- The arithmetic intensity is $I = \\dfrac{F}{M}$, where $F$ is the total floating-point operation count and $M$ is the total data movement from or to global memory in bytes.\n- Kernel fusion that computes the force and immediately applies the update can reduce data movement by eliminating intermediate global-memory traffic for temporary results, raising $I$ and hence $P_{\\mathrm{att}}$.\n\nYour task is to compute, for each test case, the following quantities:\n\n1. For the separate gauge-force kernel:\n   - Floating-point count $F_{\\mathrm{force}} = M_{\\mathrm{force}} F_M + A_{\\mathrm{force}} F_A + F_{\\mathrm{proj}}$, where $M_{\\mathrm{force}}$ is the number of $SU(3)$ matrix-matrix multiplications per link, $A_{\\mathrm{force}}$ is the number of $SU(3)$ matrix additions per link, and $F_{\\mathrm{proj}}$ models any additional flops to project the computed object into $\\mathfrak{su}(3)$.\n   - Memory traffic $M_{\\mathrm{force}}^{\\mathrm{bytes}} = c_{\\mathrm{sep}} \\left(R_{\\mathrm{force},U} S_U + R_{\\mathrm{force},P} S_P + W_{\\mathrm{force}} S_P\\right)$, where $R_{\\mathrm{force},U}$ is the number of $SU(3)$ link reads, $R_{\\mathrm{force},P}$ is the number of momentum reads, $W_{\\mathrm{force}}$ is the number of momentum writes, and $c_{\\mathrm{sep}} \\geq 1$ models coalescing and cache inefficiency for separate kernels.\n   - Arithmetic intensity $I_{\\mathrm{force}} = \\dfrac{F_{\\mathrm{force}}}{M_{\\mathrm{force}}^{\\mathrm{bytes}}}$.\n   - Attainable performance $P_{\\mathrm{force}} = \\min\\{P_{\\mathrm{peak}}, B I_{\\mathrm{force}}\\}$ in gigaflops per second (GFLOP/s).\n   - Memory-bound boolean flag $b_{\\mathrm{force}}$, which is true if $B I_{\\mathrm{force}} < P_{\\mathrm{peak}}$ and false otherwise.\n\n2. For the separate link-update kernel:\n   - Floating-point count $F_{\\mathrm{update}} = M_{\\mathrm{update}} F_M + F_{\\mathrm{exp}}$, where $M_{\\mathrm{update}}$ is the number of $SU(3)$ matrix-matrix multiplications and $F_{\\mathrm{exp}}$ models the flops to construct the exponential from the momentum.\n   - Memory traffic $M_{\\mathrm{update}}^{\\mathrm{bytes}} = c_{\\mathrm{sep}} \\left(R_{\\mathrm{update},U} S_U + R_{\\mathrm{update},P} S_P + W_{\\mathrm{update},U} S_U\\right)$, where $R_{\\mathrm{update},U}$ is the number of $SU(3)$ link reads, $R_{\\mathrm{update},P}$ is the number of momentum reads, and $W_{\\mathrm{update},U}$ is the number of $SU(3)$ link writes.\n   - Arithmetic intensity $I_{\\mathrm{update}} = \\dfrac{F_{\\mathrm{update}}}{M_{\\mathrm{update}}^{\\mathrm{bytes}}}$.\n   - Attainable performance $P_{\\mathrm{update}} = \\min\\{P_{\\mathrm{peak}}, B I_{\\mathrm{update}}\\}$ in GFLOP/s.\n   - Memory-bound boolean flag $b_{\\mathrm{update}}$, defined analogously to $b_{\\mathrm{force}}$.\n\n3. For a fused gauge-force-plus-update kernel:\n   - Floating-point count $F_{\\mathrm{fused}} = F_{\\mathrm{force}} + F_{\\mathrm{update}}$.\n   - Memory traffic $M_{\\mathrm{fused}}^{\\mathrm{bytes}} = c_{\\mathrm{fused}} \\left(R_{\\mathrm{force},U} S_U + R_{\\mathrm{force},P} S_P + W_{\\mathrm{update},U} S_U\\right)$, which removes the intermediate momentum write and its subsequent read, and also removes the separate redundant link read because the force kernel already reads the required link matrix. The factor $c_{\\mathrm{fused}} \\geq 1$ allows for improved memory locality under fusion when $c_{\\mathrm{fused}} < c_{\\mathrm{sep}}$.\n   - Arithmetic intensity $I_{\\mathrm{fused}} = \\dfrac{F_{\\mathrm{fused}}}{M_{\\mathrm{fused}}^{\\mathrm{bytes}}}$.\n   - Attainable performance $P_{\\mathrm{fused}} = \\min\\{P_{\\mathrm{peak}}, B I_{\\mathrm{fused}}\\}$ in GFLOP/s.\n   - Memory-bound boolean flag $b_{\\mathrm{fused}}$.\n\n4. The predicted performance impact of fusion on the end-to-end substep time for one link update. Using the roofline-predicted performance for each kernel, define the model time per link for each kernel as $T = \\dfrac{F}{P_{\\mathrm{att}}}$ where $F$ is in floating-point operations and $P_{\\mathrm{att}}$ is in GFLOP/s; the common gigascale factor cancels in ratios. Then the separate-kernel time is $T_{\\mathrm{sep}} = \\dfrac{F_{\\mathrm{force}}}{P_{\\mathrm{force}}} + \\dfrac{F_{\\mathrm{update}}}{P_{\\mathrm{update}}}$ and the fused-kernel time is $T_{\\mathrm{fused}} = \\dfrac{F_{\\mathrm{fused}}}{P_{\\mathrm{fused}}}$. Report the speedup factor $S = \\dfrac{T_{\\mathrm{sep}}}{T_{\\mathrm{fused}}}$ as a floating-point number, with $S > 1$ indicating that fusion is beneficial.\n\nUnits and numerical conventions:\n\n- Treat memory bandwidth $B$ as gigabytes per second (GB/s).\n- Treat peak compute throughput $P_{\\mathrm{peak}}$ as gigaflops per second (GFLOP/s).\n- Arithmetic intensity $I$ is in floating-point operations per byte.\n- The attainable performance $P_{\\mathrm{att}}$ should be reported in GFLOP/s.\n- There are no angles involved. No physical units beyond those specified are required in the output.\n- All outputs should be provided as plain numbers without unit symbols.\n\nTest suite:\n\nImplement your program to compute the above quantities for the following four parameter sets. Each parameter set is defined by the tuple $(s, B, P_{\\mathrm{peak}}, M_{\\mathrm{force}}, A_{\\mathrm{force}}, F_{\\mathrm{proj}}, M_{\\mathrm{update}}, F_{\\mathrm{exp}}, R_{\\mathrm{force},U}, R_{\\mathrm{force},P}, W_{\\mathrm{force}}, R_{\\mathrm{update},U}, R_{\\mathrm{update},P}, W_{\\mathrm{update},U}, c_{\\mathrm{sep}}, c_{\\mathrm{fused}})$ with all parameters interpreted as described above.\n\n- Case $1$ (single precision, typical high-bandwidth GPU):\n  - $s = 4$, $B = 900$, $P_{\\mathrm{peak}} = 19000$, $M_{\\mathrm{force}} = 8$, $A_{\\mathrm{force}} = 6$, $F_{\\mathrm{proj}} = 120$, $M_{\\mathrm{update}} = 1$, $F_{\\mathrm{exp}} = 300$, $R_{\\mathrm{force},U} = 12$, $R_{\\mathrm{force},P} = 1$, $W_{\\mathrm{force}} = 1$, $R_{\\mathrm{update},U} = 1$, $R_{\\mathrm{update},P} = 1$, $W_{\\mathrm{update},U} = 1$, $c_{\\mathrm{sep}} = 1.0$, $c_{\\mathrm{fused}} = 1.0$.\n\n- Case $2$ (double precision on the same GPU class):\n  - $s = 8$, $B = 900$, $P_{\\mathrm{peak}} = 9700$, $M_{\\mathrm{force}} = 8$, $A_{\\mathrm{force}} = 6$, $F_{\\mathrm{proj}} = 200$, $M_{\\mathrm{update}} = 1$, $F_{\\mathrm{exp}} = 600$, $R_{\\mathrm{force},U} = 12$, $R_{\\mathrm{force},P} = 1$, $W_{\\mathrm{force}} = 1$, $R_{\\mathrm{update},U} = 1$, $R_{\\mathrm{update},P} = 1$, $W_{\\mathrm{update},U} = 1$, $c_{\\mathrm{sep}} = 1.0$, $c_{\\mathrm{fused}} = 1.0$.\n\n- Case $3$ (bandwidth-limited environment, single precision):\n  - $s = 4$, $B = 300$, $P_{\\mathrm{peak}} = 19000$, $M_{\\mathrm{force}} = 8$, $A_{\\mathrm{force}} = 6$, $F_{\\mathrm{proj}} = 120$, $M_{\\mathrm{update}} = 1$, $F_{\\mathrm{exp}} = 300$, $R_{\\mathrm{force},U} = 12$, $R_{\\mathrm{force},P} = 1$, $W_{\\mathrm{force}} = 1$, $R_{\\mathrm{update},U} = 1$, $R_{\\mathrm{update},P} = 1$, $W_{\\mathrm{update},U} = 1$, $c_{\\mathrm{sep}} = 1.0$, $c_{\\mathrm{fused}} = 1.0$.\n\n- Case $4$ (imperfect coalescing in separate kernels, improved locality under fusion, single precision):\n  - $s = 4$, $B = 900$, $P_{\\mathrm{peak}} = 19000$, $M_{\\mathrm{force}} = 8$, $A_{\\mathrm{force}} = 6$, $F_{\\mathrm{proj}} = 120$, $M_{\\mathrm{update}} = 1$, $F_{\\mathrm{exp}} = 300$, $R_{\\mathrm{force},U} = 12$, $R_{\\mathrm{force},P} = 1$, $W_{\\mathrm{force}} = 1$, $R_{\\mathrm{update},U} = 1$, $R_{\\mathrm{update},P} = 1$, $W_{\\mathrm{update},U} = 1$, $c_{\\mathrm{sep}} = 1.3$, $c_{\\mathrm{fused}} = 1.05$.\n\nRequired final output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list of per-case result lists, with no embedded spaces. For each case, output the list\n$[I_{\\mathrm{force}}, I_{\\mathrm{update}}, I_{\\mathrm{fused}}, P_{\\mathrm{force}}, P_{\\mathrm{update}}, P_{\\mathrm{fused}}, b_{\\mathrm{force}}, b_{\\mathrm{update}}, b_{\\mathrm{fused}}, S]$\nin this exact order. The booleans $b_{\\mathrm{force}}$, $b_{\\mathrm{update}}$, and $b_{\\mathrm{fused}}$ must be printed as $True$ or $False$. For example, the overall output has the form\n$[[r_{1,1},\\dots,r_{1,10}],[r_{2,1},\\dots,r_{2,10}],[r_{3,1},\\dots,r_{3,10}],[r_{4,1},\\dots,r_{4,10}]]$\nwith no spaces anywhere.",
            "solution": "The user has provided a problem statement that requires a roofline analysis for a simplified Hybrid Monte Carlo (HMC) integrator step in lattice gauge theory. The analysis is to be performed for separate and fused computational kernels.\n\n### Step 1: Extract Givens\n\nThe problem statement provides the following data, definitions, and constants:\n\n**Constants and Model Parameters:**\n-   $F_M$: Floating-point cost for a dense complex $3 \\times 3$ matrix-matrix product, modeled as $198$ real floating-point operations.\n-   $F_A$: Floating-point cost for an elementwise $3 \\times 3$ complex matrix addition, modeled as $18$ real floating-point operations.\n-   $s$: Bytes per real number ($4$ for single precision, $8$ for double precision).\n-   $S_U$: Memory size of an $SU(3)$ link matrix, defined as $18s$ bytes.\n-   $S_P$: Memory size of an $\\mathfrak{su}(3)$ momentum matrix, defined as $8s$ bytes.\n-   $B$: Memory bandwidth in gigabytes per second (GB/s).\n-   $P_{\\mathrm{peak}}$: Peak floating-point throughput in gigaflops per second (GFLOP/s).\n-   $c_{\\mathrm{sep}}$: Memory access inefficiency factor for separate kernels.\n-   $c_{\\mathrm{fused}}$: Memory access inefficiency factor for the fused kernel.\n\n**Kernel-Specific Parameters (per link):**\n-   $M_{\\mathrm{force}}$: Number of $SU(3)$ matrix-matrix multiplications in the force kernel.\n-   $A_{\\mathrm{force}}$: Number of $SU(3)$ matrix additions in the force kernel.\n-   $F_{\\mathrm{proj}}$: Additional flops for projection in the force kernel.\n-   $R_{\\mathrm{force},U}$: Number of $SU(3)$ link reads in the force kernel.\n-   $R_{\\mathrm{force},P}$: Number of momentum reads in the force kernel.\n-   $W_{\\mathrm{force}}$: Number of momentum writes in the force kernel.\n-   $M_{\\mathrm{update}}$: Number of $SU(3)$ matrix-matrix multiplications in the update kernel.\n-   $F_{\\mathrm{exp}}$: Additional flops for exponential construction in the update kernel.\n-   $R_{\\mathrm{update},U}$: Number of $SU(3)$ link reads in the update kernel.\n-   $R_{\\mathrm{update},P}$: Number of momentum reads in the update kernel.\n-   $W_{\\mathrm{update},U}$: Number of $SU(3)$ link writes in the update kernel.\n\n**Formulas:**\n-   Arithmetic Intensity: $I = \\dfrac{F}{M}$, where $F$ is total flops and $M$ is total memory traffic in bytes.\n-   Attainable Performance: $P_{\\mathrm{att}} = \\min\\{P_{\\mathrm{peak}}, B \\times I\\}$.\n-   Memory-bound flag, $b$: True if $B \\times I < P_{\\mathrm{peak}}$.\n-   Force kernel flops: $F_{\\mathrm{force}} = M_{\\mathrm{force}} F_M + A_{\\mathrm{force}} F_A + F_{\\mathrm{proj}}$.\n-   Force kernel memory: $M_{\\mathrm{force}}^{\\mathrm{bytes}} = c_{\\mathrm{sep}} \\left(R_{\\mathrm{force},U} S_U + R_{\\mathrm{force},P} S_P + W_{\\mathrm{force}} S_P\\right)$.\n-   Update kernel flops: $F_{\\mathrm{update}} = M_{\\mathrm{update}} F_M + F_{\\mathrm{exp}}$.\n-   Update kernel memory: $M_{\\mathrm{update}}^{\\mathrm{bytes}} = c_{\\mathrm{sep}} \\left(R_{\\mathrm{update},U} S_U + R_{\\mathrm{update},P} S_P + W_{\\mathrm{update},U} S_U\\right)$.\n-   Fused kernel flops: $F_{\\mathrm{fused}} = F_{\\mathrm{force}} + F_{\\mathrm{update}}$.\n-   Fused kernel memory: $M_{\\mathrm{fused}}^{\\mathrm{bytes}} = c_{\\mathrm{fused}} \\left(R_{\\mathrm{force},U} S_U + R_{\\mathrm{force},P} S_P + W_{\\mathrm{update},U} S_U\\right)$.\n-   Time for a kernel: $T = \\dfrac{F}{P_{\\mathrm{att}}}$.\n-   Separate kernel time: $T_{\\mathrm{sep}} = \\dfrac{F_{\\mathrm{force}}}{P_{\\mathrm{force}}} + \\dfrac{F_{\\mathrm{update}}}{P_{\\mathrm{update}}}$.\n-   Fused kernel time: $T_{\\mathrm{fused}} = \\dfrac{F_{\\mathrm{fused}}}{P_{\\mathrm{fused}}}$.\n-   Speedup: $S = \\dfrac{T_{\\mathrm{sep}}}{T_{\\mathrm{fused}}}$.\n\n**Test Cases:**\nFour test cases are provided as tuples of the above parameters.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is set in the context of computational lattice gauge theory, a subfield of nuclear and particle physics. The concepts used—$SU(3)$ matrices for gauge links, $\\mathfrak{su}(3)$ elements for momenta, Hybrid Monte Carlo, kernel fusion, and the roofline performance model—are all standard, valid, and central to the field. The flop counts for $SU(3)$ operations are standard approximations used in performance modeling. The problem is scientifically sound.\n-   **Well-Posed**: The problem provides a complete set of deterministic formulas and a complete set of input parameters for each of the four required test cases. The objective is to compute a list of specific quantities. For each set of inputs, the formulas yield a unique, well-defined numerical result. The problem is well-posed.\n-   **Objective**: The problem is stated in precise, quantitative terms. All variables are defined, and all required computations are specified by explicit mathematical formulas. The problem is free of subjective language or opinion.\n-   **No Other Flaws**: The problem is formalizable, complete, and internally consistent. The provided hardware parameters ($B$, $P_{\\mathrm{peak}}$) are realistic for modern GPUs. The required computations are non-trivial and represent a standard performance analysis exercise.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. It is scientifically grounded, well-posed, objective, and complete. A complete, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe solution involves a systematic application of the provided formulas to each of the four test cases. We will structure the calculation by first defining the fundamental constants and then processing each case to compute the required ten output quantities: $I_{\\mathrm{force}}$, $I_{\\mathrm{update}}$, $I_{\\mathrm{fused}}$, $P_{\\mathrm{force}}$, $P_{\\mathrm{update}}$, $P_{\\mathrm{fused}}$, $b_{\\mathrm{force}}$, $b_{\\mathrm{update}}$, $b_{\\mathrm{fused}}$, and $S$.\n\nLet the input parameters for a given case be $(s, B, P_{\\mathrm{peak}}, M_{\\mathrm{force}}, \\dots, c_{\\mathrm{fused}})$. The computational procedure is as follows:\n\n**1. Preliminary Calculations:**\nFirst, we define the constant flop counts for matrix operations:\n-   $F_M = 198$\n-   $F_A = 18$\n\nNext, we calculate the byte sizes of the data structures, which depend on the precision parameter $s$:\n-   $S_U = 18s$\n-   $S_P = 8s$\n\n**2. Analysis of Separate Kernels:**\n\n**Gauge-Force Kernel:**\n-   The total number of floating-point operations is calculated as:\n    $$F_{\\mathrm{force}} = M_{\\mathrm{force}} F_M + A_{\\mathrm{force}} F_A + F_{\\mathrm{proj}}$$\n-   The total memory traffic in bytes, including the inefficiency factor, is:\n    $$M_{\\mathrm{force}}^{\\mathrm{bytes}} = c_{\\mathrm{sep}} \\left(R_{\\mathrm{force},U} S_U + R_{\\mathrm{force},P} S_P + W_{\\mathrm{force}} S_P\\right)$$\n-   The arithmetic intensity is the ratio of flops to bytes:\n    $$I_{\\mathrm{force}} = \\frac{F_{\\mathrm{force}}}{M_{\\mathrm{force}}^{\\mathrm{bytes}}}$$\n-   The attainable performance is determined by the roofline model:\n    $$P_{\\mathrm{force}} = \\min\\left(P_{\\mathrm{peak}}, B \\cdot I_{\\mathrm{force}}\\right)$$\n-   The kernel is memory-bound if the performance is limited by the memory ceiling:\n    $$b_{\\mathrm{force}} = (B \\cdot I_{\\mathrm{force}} < P_{\\mathrm{peak}})$$\n\n**Link-Update Kernel:**\n-   The total number of floating-point operations is:\n    $$F_{\\mathrm{update}} = M_{\\mathrm{update}} F_M + F_{\\mathrm{exp}}$$\n-   The total memory traffic in bytes is:\n    $$M_{\\mathrm{update}}^{\\mathrm{bytes}} = c_{\\mathrm{sep}} \\left(R_{\\mathrm{update},U} S_U + R_{\\mathrm{update},P} S_P + W_{\\mathrm{update},U} S_U\\right)$$\n-   The arithmetic intensity is:\n    $$I_{\\mathrm{update}} = \\frac{F_{\\mathrm{update}}}{M_{\\mathrm{update}}^{\\mathrm{bytes}}}$$\n-   The attainable performance is:\n    $$P_{\\mathrm{update}} = \\min\\left(P_{\\mathrm{peak}}, B \\cdot I_{\\mathrm{update}}\\right)$$\n-   The memory-bound status is:\n    $$b_{\\mathrm{update}} = (B \\cdot I_{\\mathrm{update}} < P_{\\mathrm{peak}})$$\n\n**3. Analysis of the Fused Kernel:**\n\n-   The total flop count is the sum of the individual kernel flops:\n    $$F_{\\mathrm{fused}} = F_{\\mathrm{force}} + F_{\\mathrm{update}}$$\n-   The memory traffic is reduced by eliminating the intermediate write and read of the momentum $P$ and the redundant read of the link $U$ that is being updated. The formula provided is:\n    $$M_{\\mathrm{fused}}^{\\mathrm{bytes}} = c_{\\mathrm{fused}} \\left(R_{\\mathrm{force},U} S_U + R_{\\mathrm{force},P} S_P + W_{\\mathrm{update},U} S_U\\right)$$\n-   The arithmetic intensity for the fused kernel is:\n    $$I_{\\mathrm{fused}} = \\frac{F_{\\mathrm{fused}}}{M_{\\mathrm{fused}}^{\\mathrm{bytes}}}$$\n-   The attainable performance is:\n    $$P_{\\mathrm{fused}} = \\min\\left(P_{\\mathrm{peak}}, B \\cdot I_{\\mathrm{fused}}\\right)$$\n-   The memory-bound status is:\n    $$b_{\\mathrm{fused}} = (B \\cdot I_{\\mathrm{fused}} < P_{\\mathrm{peak}})$$\n\n**4. Calculation of Speedup:**\n\nThe performance improvement from fusion is quantified by the speedup factor $S$, which is the ratio of the total time for separate kernels to the time for the fused kernel.\n\n-   The time for the separate-kernel execution is the sum of the individual kernel times:\n    $$T_{\\mathrm{sep}} = \\frac{F_{\\mathrm{force}}}{P_{\\mathrm{force}}} + \\frac{F_{\\mathrm{update}}}{P_{\\mathrm{update}}}$$\n-   The time for the fused-kernel execution is:\n    $$T_{\\mathrm{fused}} = \\frac{F_{\\mathrm{fused}}}{P_{\\mathrm{fused}}}$$\n-   The speedup is their ratio:\n    $$S = \\frac{T_{\\mathrm{sep}}}{T_{\\mathrm{fused}}}$$\nThe units of $F$ (GigaFLOP) and $P_{\\mathrm{att}}$ (GFLOP/s) result in time $T$ in seconds, but the giga-prefix cancels in the ratio, simplifying implementation.\n\nThis complete procedure will be implemented in the provided Python environment to process the four given test cases and generate the output in the specified format. The code will follow these steps precisely for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries outside the Python standard library are permitted.\n\ndef solve():\n    \"\"\"\n    Performs a roofline analysis for a simplified HMC integrator step in lattice gauge theory.\n    Computes arithmetic intensity, attainable performance, and kernel fusion speedup\n    for several parameter sets.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each tuple contains:\n    # (s, B, P_peak, M_force, A_force, F_proj, M_update, F_exp, R_force_U,\n    # R_force_P, W_force, R_update_U, R_update_P, W_update_U, c_sep, c_fused)\n    test_cases = [\n        # Case 1: single precision, typical high-bandwidth GPU\n        (4, 900, 19000, 8, 6, 120, 1, 300, 12, 1, 1, 1, 1, 1, 1.0, 1.0),\n        # Case 2: double precision on the same GPU class\n        (8, 900, 9700, 8, 6, 200, 1, 600, 12, 1, 1, 1, 1, 1, 1.0, 1.0),\n        # Case 3: bandwidth-limited environment, single precision\n        (4, 300, 19000, 8, 6, 120, 1, 300, 12, 1, 1, 1, 1, 1, 1.0, 1.0),\n        # Case 4: imperfect coalescing, single precision\n        (4, 900, 19000, 8, 6, 120, 1, 300, 12, 1, 1, 1, 1, 1, 1.3, 1.05),\n    ]\n\n    # Fundamental constants for FLOP counts\n    F_M = 198  # FLOPs for 3x3 complex matrix-matrix product\n    F_A = 18   # FLOPs for 3x3 complex matrix addition\n\n    all_results = []\n    \n    for case in test_cases:\n        (s, B, P_peak, M_force, A_force, F_proj, M_update, F_exp,\n         R_force_U, R_force_P, W_force, R_update_U, R_update_P, W_update_U,\n         c_sep, c_fused) = case\n\n        # --- 1. Preliminary Calculations ---\n        S_U = 18 * s  # Size of SU(3) matrix in bytes\n        S_P = 8 * s   # Size of su(3) matrix in bytes\n\n        # --- 2. Separate Gauge-Force Kernel Analysis ---\n        F_force = M_force * F_M + A_force * F_A + F_proj\n        M_force_bytes = c_sep * (R_force_U * S_U + R_force_P * S_P + W_force * S_P)\n        I_force = F_force / M_force_bytes\n        P_force_mem_ceil = B * I_force\n        P_force = min(P_peak, P_force_mem_ceil)\n        b_force = P_force_mem_ceil  P_peak\n\n        # --- 3. Separate Link-Update Kernel Analysis ---\n        F_update = M_update * F_M + F_exp\n        M_update_bytes = c_sep * (R_update_U * S_U + R_update_P * S_P + W_update_U * S_U)\n        I_update = F_update / M_update_bytes\n        P_update_mem_ceil = B * I_update\n        P_update = min(P_peak, P_update_mem_ceil)\n        b_update = P_update_mem_ceil  P_peak\n\n        # --- 4. Fused Kernel Analysis ---\n        F_fused = F_force + F_update\n        M_fused_bytes = c_fused * (R_force_U * S_U + R_force_P * S_P + W_update_U * S_U)\n        I_fused = F_fused / M_fused_bytes\n        P_fused_mem_ceil = B * I_fused\n        P_fused = min(P_peak, P_fused_mem_ceil)\n        b_fused = P_fused_mem_ceil  P_peak\n\n        # --- 5. Speedup Calculation ---\n        # Time T = F / P_att. Units are consistent, so giga- prefix cancels.\n        T_sep = (F_force / P_force) + (F_update / P_update)\n        T_fused = F_fused / P_fused\n        S = T_sep / T_fused if T_fused > 0 else float('inf')\n\n        # Store results for this case\n        case_results = [\n            I_force, I_update, I_fused,\n            P_force, P_update, P_fused,\n            b_force, b_update, b_fused,\n            S\n        ]\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # Produces a string like [[r1,r2,...],[s1,s2,...]] with no spaces.\n    inner_strings = []\n    for r_list in all_results:\n        # Convert each item in the list to its string representation\n        r_str_list = [str(item) for item in r_list]\n        # Join with commas and wrap in brackets\n        inner_strings.append(f\"[{','.join(r_str_list)}]\")\n    \n    # Join all case strings with commas and wrap in brackets\n    final_output_string = f\"[{','.join(inner_strings)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}