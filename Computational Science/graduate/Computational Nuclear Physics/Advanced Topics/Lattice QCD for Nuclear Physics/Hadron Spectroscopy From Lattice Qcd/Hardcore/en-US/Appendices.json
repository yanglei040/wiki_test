{
    "hands_on_practices": [
        {
            "introduction": "A fundamental check in any hadron spectroscopy calculation is to ensure that the computed energy-momentum pairs respect the principles of special relativity. This practice guides you through verifying the relativistic dispersion relation, $E^2 = m^2 + p^2$, for discrete data from a lattice simulation. By fitting an effective speed of light from the data, you will learn a crucial method for identifying and quantifying discretization artifacts. ",
            "id": "3562992",
            "problem": "You are given discrete-energy data for heavy-light mesons computed on finite, cubic, possibly anisotropic lattices. The goal is to check consistency with the continuum relativistic dispersion relation by extracting an effective speed-of-light parameter and evaluating goodness-of-fit for the dispersion using only the provided data. All quantities in this problem are dimensionless lattice quantities, so no physical unit conversion is needed.\n\nThe fundamental base is the relativistic energy-momentum relation in natural units where the speed of light is set to unity, which states that for a single-particle state of mass $m$ and spatial momentum magnitude $p$,\n$$\nE^2 = m^2 + p^2.\n$$\nOn a lattice with temporal lattice spacing $a_t$ and spatial lattice spacing $a_s$, define the dimensionless energy $e \\equiv a_t E$, dimensionless mass $m_0 \\equiv a_t m$, and dimensionless spatial momentum $q \\equiv a_s p$. Let the spatial-to-temporal anisotropy be $\\xi \\equiv a_s/a_t$. Quantization of spatial momentum on a cubic box of linear extent $L$ (in lattice sites per spatial direction) implies\n$$\nq(\\mathbf{n}) = \\frac{2\\pi}{L} \\left\\lVert \\mathbf{n} \\right\\rVert, \\quad \\mathbf{n} = (n_x,n_y,n_z), \\quad n_x,n_y,n_z \\in \\mathbb{Z}.\n$$\nCombining these, the continuum dispersion relation in lattice units becomes\n$$\ne^2 = m_0^2 + \\frac{1}{\\xi^2} \\, q^2.\n$$\nDefine the per-momentum effective speed-of-light squared by\n$$\nc_{\\mathrm{eff}}^2(\\mathbf{n}) = \\xi^2 \\, \\frac{e(\\mathbf{n})^2 - m_0^2}{q(\\mathbf{n})^2},\n$$\nfor $\\mathbf{n}$ such that $q(\\mathbf{n}) \\neq 0$. In the continuum limit, $c_{\\mathrm{eff}}^2(\\mathbf{n}) \\to 1$ for all nonzero momenta.\n\nYou must implement the following tasks for each test case:\n- Given $L$, $\\xi$, $m_0$, a list of momentum vectors $\\{\\mathbf{n}_i\\}$, corresponding measured energies $\\{e_i\\}$, and corresponding energy uncertainties $\\{\\delta e_i\\}$, compute $q_i = \\frac{2\\pi}{L} \\|\\mathbf{n}_i\\|$ for each momentum entry.\n- Exclude entries with $q_i = 0$ from any division or fitting procedure.\n- For each nonzero momentum, compute $c_{\\mathrm{eff}}^2(\\mathbf{n}_i)$.\n- Perform a weighted least-squares fit of the form\n$$\ny_i = c_{\\mathrm{lat}}^2 \\, x_i,\n$$\nwith\n$$\ny_i = e_i^2 - m_0^2, \\quad x_i = \\frac{q_i^2}{\\xi^2},\n$$\nand weights\n$$\nw_i = \\frac{1}{\\sigma_{y_i}^2}, \\quad \\sigma_{y_i} \\approx 2\\, e_i \\, \\delta e_i,\n$$\nassuming uncorrelated energy uncertainties and negligible mass uncertainty. The weighted slope through the origin $c_{\\mathrm{lat}}^2$ is the estimator of the lattice effective speed-of-light squared that best matches the data.\n- Compute the reduced chi-square for the fit,\n$$\n\\chi^2_{\\mathrm{red}} = \\frac{1}{N - 1} \\sum_{i=1}^{N} w_i \\left( y_i - c_{\\mathrm{lat}}^2 x_i \\right)^2,\n$$\nwhere $N$ is the number of nonzero-momentum data points used in the fit.\n- Compute the maximum absolute deviation of the per-momentum effective speed-of-light squared from unity,\n$$\n\\Delta_{\\max} = \\max_{i: q_i \\neq 0} \\left| c_{\\mathrm{eff}}^2(\\mathbf{n}_i) - 1 \\right|.\n$$\n- Determine a pass/fail boolean according to the criteria\n$$\n\\left| c_{\\mathrm{lat}}^2 - 1 \\right| \\le 0.05 \\quad \\text{and} \\quad \\chi^2_{\\mathrm{red}} \\le 1.5.\n$$\n\nTest Suite:\nProvide results for the following four test cases. Each test case is specified by $(L, \\xi, m_0, \\{\\mathbf{n}_i\\}, \\{e_i\\}, \\{\\delta e_i\\})$.\n\n- Case 1 (isotropic lattice, near-continuum energies):\n  - $L = 32$, $\\xi = 1.0$, $m_0 = 0.85$.\n  - Momenta and data:\n    - $\\mathbf{n}_1 = (1,0,0)$, $e_1 = 0.87299$, $\\delta e_1 = 0.0012$.\n    - $\\mathbf{n}_2 = (1,1,0)$, $e_2 = 0.89530$, $\\delta e_2 = 0.0013$.\n    - $\\mathbf{n}_3 = (1,1,1)$, $e_3 = 0.91526$, $\\delta e_3 = 0.0012$.\n    - $\\mathbf{n}_4 = (0,0,0)$, $e_4 = 0.85080$, $\\delta e_4 = 0.0008$.\n\n- Case 2 (anisotropic lattice, near-continuum energies):\n  - $L = 24$, $\\xi = 3.5$, $m_0 = 0.65$.\n  - Momenta and data:\n    - $\\mathbf{n}_1 = (1,0,0)$, $e_1 = 0.65403$, $\\delta e_1 = 0.0012$.\n    - $\\mathbf{n}_2 = (1,1,0)$, $e_2 = 0.65895$, $\\delta e_2 = 0.0012$.\n    - $\\mathbf{n}_3 = (2,0,0)$, $e_3 = 0.667495$, $\\delta e_3 = 0.0012$.\n    - $\\mathbf{n}_4 = (0,0,0)$, $e_4 = 0.65080$, $\\delta e_4 = 0.0010$.\n\n- Case 3 (isotropic lattice, deliberately inflated nonzero-momentum energies to mimic discretization artifacts):\n  - $L = 28$, $\\xi = 1.0$, $m_0 = 1.20$.\n  - Momenta and data:\n    - $\\mathbf{n}_1 = (1,0,0)$, $e_1 = 1.26979$, $\\delta e_1 = 0.0015$.\n    - $\\mathbf{n}_2 = (1,1,0)$, $e_2 = 1.29091$, $\\delta e_2 = 0.0015$.\n    - $\\mathbf{n}_3 = (2,0,0)$, $e_3 = 1.33231$, $\\delta e_3 = 0.0015$.\n    - $\\mathbf{n}_4 = (0,0,0)$, $e_4 = 1.20080$, $\\delta e_4 = 0.0010$.\n\n- Case 4 (anisotropic lattice, includes zero momentum, larger uncertainties):\n  - $L = 16$, $\\xi = 2.0$, $m_0 = 0.90$.\n  - Momenta and data:\n    - $\\mathbf{n}_1 = (1,0,0)$, $e_1 = 0.92200$, $\\delta e_1 = 0.0050$.\n    - $\\mathbf{n}_2 = (2,0,0)$, $e_2 = 0.98250$, $\\delta e_2 = 0.0050$.\n    - $\\mathbf{n}_3 = (0,0,0)$, $e_3 = 0.90050$, $\\delta e_3 = 0.0030$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a list of the form\n$$\n[c_{\\mathrm{lat}}^2, \\Delta_{\\max}, \\chi^2_{\\mathrm{red}}, \\text{pass}]\n$$\nwhere $c_{\\mathrm{lat}}^2$, $\\Delta_{\\max}$, and $\\chi^2_{\\mathrm{red}}$ are floats rounded to six decimal places, and $\\text{pass}$ is a boolean. For example, the overall output should look like\n$$\n[[c_1,\\Delta_1,\\chi_1,\\text{pass}_1],[c_2,\\Delta_2,\\chi_2,\\text{pass}_2],[c_3,\\Delta_3,\\chi_3,\\text{pass}_3],[c_4,\\Delta_4,\\chi_4,\\text{pass}_4]].\n$$",
            "solution": "The problem is valid. It is scientifically grounded in the principles of special relativity and lattice field theory, is well-posed with all necessary information provided, and is formulated objectively.\n\nThe objective is to validate discrete energy-momentum data, obtained from a lattice Quantum Chromodynamics (QCD) simulation, against the continuum relativistic dispersion relation. This is a standard procedure in hadron spectroscopy to assess and control systematic errors arising from the discretization of spacetime. The analysis involves extracting an effective speed-of-light parameter from the data and evaluating the goodness-of-fit.\n\nThe foundation of this analysis is the relativistic energy-momentum relation for a particle of mass $m$ and momentum $p$:\n$$E^2 = p^2c^2 + m^2c^4$$\nIn natural units ($c=1$), this simplifies to $E^2 = p^2 + m^2$. On a spacetime lattice, physical quantities are made dimensionless by scaling with the lattice spacings. Let $a_t$ be the temporal lattice spacing and $a_s$ be the spatial lattice spacing. The dimensionless energy is $e \\equiv a_t E$, the dimensionless rest mass is $m_0 \\equiv a_t m$, and the dimensionless spatial momentum is $q \\equiv a_s p$. The ratio $\\xi \\equiv a_s/a_t$ is the lattice anisotropy. Substituting these into the continuum relation yields the target form on the lattice:\n$$ (e/a_t)^2 = (q/a_s)^2 + (m_0/a_t)^2 $$\n$$ e^2/a_t^2 = q^2/a_s^2 + m_0^2/a_t^2 $$\nMultiplying by $a_t^2$ gives:\n$$ e^2 = m_0^2 + (a_t/a_s)^2 q^2 = m_0^2 + \\frac{1}{\\xi^2} q^2 $$\nOn a finite cubic lattice of linear size $L$ (in lattice sites) with periodic boundary conditions, the allowed spatial momenta are quantized. A particle's momentum vector is $\\mathbf{p} = \\frac{2\\pi}{L a_s} \\mathbf{n}$, where $\\mathbf{n} = (n_x, n_y, n_z)$ is a vector of integers. The dimensionless momentum magnitude $q$ is therefore:\n$$ q(\\mathbf{n}) = a_s |\\mathbf{p}| = a_s \\frac{2\\pi}{L a_s} \\|\\mathbf{n}\\| = \\frac{2\\pi}{L} \\|\\mathbf{n}\\| $$\nThe analysis proceeds through the following steps for each test case.\n\n1.  **Data Preparation**: For each supplied data point, consisting of a momentum vector $\\mathbf{n}_i$, a measured energy $e_i$, and its uncertainty $\\delta e_i$, we first compute the corresponding dimensionless momentum magnitude $q_i = \\frac{2\\pi}{L} \\sqrt{n_{ix}^2 + n_{iy}^2 + n_{iz}^2}$. Data points for which $\\mathbf{n}_i = (0,0,0)$, and thus $q_i=0$, are used to determine the rest mass but are excluded from the fit of the dispersion, as the relation $e^2 = m_0^2$ contains no information about the speed of light. The problem provides $m_0$ directly, so we simply filter out these zero-momentum points.\n\n2.  **Linearization of the Model**: To determine the effective speed-of-light from the data, we rearrange the lattice dispersion relation. If the speed of light were not unity but some lattice-effective value $c_{\\mathrm{lat}}$, the relation would be $e^2 = m_0^2 + c_{\\mathrm{lat}}^2 \\frac{1}{\\xi^2} q^2$. This can be cast into a linear model of the form $y = a x$ (regression through the origin) by defining:\n    $$ y_i = e_i^2 - m_0^2 $$\n    $$ x_i = \\frac{q_i^2}{\\xi^2} $$\n    The slope of this line is the lattice-effective speed-of-light squared, $c_{\\mathrm{lat}}^2$.\n\n3.  **Weighted Least-Squares Fit**: Since the energies $e_i$ are measured with uncertainties $\\delta e_i$, a weighted least-squares fit is appropriate to give more influence to more precise data points. The uncertainty on $y_i$, denoted $\\sigma_{y_i}$, is found via standard error propagation. Assuming the uncertainty on $m_0$ is negligible, $\\sigma_{y_i}^2 = (\\frac{\\partial y_i}{\\partial e_i})^2 (\\delta e_i)^2 = (2e_i)^2 (\\delta e_i)^2$. Thus, $\\sigma_{y_i} = 2 e_i \\delta e_i$. The weight for each data point is the inverse of the variance of $y_i$:\n    $$ w_i = \\frac{1}{\\sigma_{y_i}^2} = \\frac{1}{(2 e_i \\delta e_i)^2} $$\n    For a linear model $y = a x$ passing through the origin, the weighted least-squares estimate for the slope $a$ (which is $c_{\\mathrm{lat}}^2$ in our case) is given by:\n    $$ c_{\\mathrm{lat}}^2 = \\frac{\\sum_i w_i x_i y_i}{\\sum_i w_i x_i^2} $$\n    The sum is over all $N$ data points with non-zero momentum.\n\n4.  **Goodness-of-Fit**: The quality of the fit is quantified by the reduced chi-square, $\\chi^2_{\\mathrm{red}}$. It measures the weighted mean squared deviation of the data from the fitted model, normalized by the number of degrees of freedom. For a fit with $N$ points and $1$ free parameter ($c_{\\mathrm{lat}}^2$), there are $N-1$ degrees of freedom.\n    $$ \\chi^2_{\\mathrm{red}} = \\frac{1}{N-1} \\sum_{i=1}^{N} w_i (y_i - c_{\\mathrm{lat}}^2 x_i)^2 $$\n    A value of $\\chi^2_{\\mathrm{red}} \\approx 1$ indicates that the scatter of the data around the model is consistent with the estimated uncertainties.\n\n5.  **Per-Momentum Effective Speed of Light**: The problem also asks for a per-momentum measure of the speed of light, $c_{\\mathrm{eff}}^2(\\mathbf{n}_i)$. This is calculated by solving the dispersion relation for the \"speed of light\" term for each individual non-zero momentum data point:\n    $$ c_{\\mathrm{eff}}^2(\\mathbf{n}_i) = \\frac{e_i^2 - m_0^2}{q_i^2 / \\xi^2} = \\xi^2 \\frac{e_i^2 - m_0^2}{q_i^2} $$\n    The maximum absolute deviation of these values from the expected continuum value of $1$ provides another measure of discretization effects:\n    $$ \\Delta_{\\max} = \\max_{i: q_i \\neq 0} |c_{\\mathrm{eff}}^2(\\mathbf{n}_i) - 1| $$\n\n6.  **Validation Criteria**: Finally, a boolean `pass` flag is determined based on two conditions, which test if the data are consistent with the continuum relativistic dispersion relation within acceptable tolerances:\n    -   $|c_{\\mathrm{lat}}^2 - 1| \\le 0.05$: The overall fitted speed of light must be within $5\\%$ of unity.\n    -   $\\chi^2_{\\mathrm{red}} \\le 1.5$: The fit must be statistically good.\n\nThis complete procedure will be applied to each test case to generate the required results.",
            "answer": "```python\nimport numpy as np\n\ndef analyze_dispersion(L, xi, m0, data):\n    \"\"\"\n    Analyzes discrete energy-momentum data from a lattice simulation.\n    \n    Args:\n        L (int): Linear extent of the cubic lattice.\n        xi (float): Spatial-to-temporal anisotropy (a_s/a_t).\n        m0 (float): Dimensionless rest mass (a_t * m).\n        data (list of tuples): Each tuple contains (n_vec, e, delta_e),\n                               where n_vec is a momentum vector (nx, ny, nz),\n                               e is the dimensionless energy, and delta_e is its uncertainty.\n\n    Returns:\n        list: A list containing [c_lat_sq, delta_max, chi2_red, pass_bool].\n    \"\"\"\n    \n    # Filter out zero-momentum points and prepare lists for fitting\n    fit_data = []\n    for n_vec, e, delta_e in data:\n        n_squared = np.sum(np.square(n_vec))\n        if n_squared == 0:\n            continue\n        \n        q = (2 * np.pi / L) * np.sqrt(n_squared)\n        fit_data.append({'q': q, 'e': e, 'delta_e': delta_e, 'n_vec': n_vec})\n\n    if not fit_data:\n        # Handle cases with no non-zero momentum points if necessary\n        # Though the test cases provided have them.\n        return [np.nan, np.nan, np.nan, False]\n\n    N = len(fit_data)\n    m0_sq = m0**2\n    xi_sq = xi**2\n\n    # Calculate quantities for the linear fit y = a*x\n    x_i = np.array([p['q']**2 / xi_sq for p in fit_data])\n    y_i = np.array([p['e']**2 - m0_sq for p in fit_data])\n    \n    # Calculate weights\n    e_i = np.array([p['e'] for p in fit_data])\n    delta_e_i = np.array([p['delta_e'] for p in fit_data])\n    sigma_y_i = 2 * e_i * delta_e_i\n    w_i = 1 / (sigma_y_i**2)\n\n    # --- 1. Calculate c_lat^2 (weighted least-squares slope) ---\n    numerator = np.sum(w_i * x_i * y_i)\n    denominator = np.sum(w_i * x_i**2)\n    c_lat_sq = numerator / denominator\n\n    # --- 2. Calculate reduced chi-square (chi^2_red) ---\n    residuals = y_i - c_lat_sq * x_i\n    chi_sq = np.sum(w_i * residuals**2)\n    degrees_of_freedom = N - 1\n    if degrees_of_freedom = 0:\n        chi2_red = np.inf # Or handle as an error\n    else:\n        chi2_red = chi_sq / degrees_of_freedom\n\n    # --- 3. Calculate Delta_max ---\n    q_sq = np.array([p['q']**2 for p in fit_data])\n    c_eff_sq_i = xi_sq * y_i / q_sq\n    delta_max = np.max(np.abs(c_eff_sq_i - 1))\n\n    # --- 4. Determine pass/fail status ---\n    pass_c_lat = np.abs(c_lat_sq - 1) = 0.05\n    pass_chi2 = chi2_red = 1.5\n    pass_bool = pass_c_lat and pass_chi2\n\n    return [c_lat_sq, delta_max, chi2_red, pass_bool]\n\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the dispersion relation analysis.\n    \"\"\"\n    test_cases = [\n        {\n            \"L\": 32, \"xi\": 1.0, \"m0\": 0.85,\n            \"data\": [\n                ((1, 0, 0), 0.87299, 0.0012),\n                ((1, 1, 0), 0.89530, 0.0013),\n                ((1, 1, 1), 0.91526, 0.0012),\n                ((0, 0, 0), 0.85080, 0.0008),\n            ]\n        },\n        {\n            \"L\": 24, \"xi\": 3.5, \"m0\": 0.65,\n            \"data\": [\n                ((1, 0, 0), 0.65403, 0.0012),\n                ((1, 1, 0), 0.65895, 0.0012),\n                ((2, 0, 0), 0.667495, 0.0012),\n                ((0, 0, 0), 0.65080, 0.0010),\n            ]\n        },\n        {\n            \"L\": 28, \"xi\": 1.0, \"m0\": 1.20,\n            \"data\": [\n                ((1, 0, 0), 1.26979, 0.0015),\n                ((1, 1, 0), 1.29091, 0.0015),\n                ((2, 0, 0), 1.33231, 0.0015),\n                ((0, 0, 0), 1.20080, 0.0010),\n            ]\n        },\n        {\n            \"L\": 16, \"xi\": 2.0, \"m0\": 0.90,\n            \"data\": [\n                ((1, 0, 0), 0.92200, 0.0050),\n                ((2, 0, 0), 0.98250, 0.0050),\n                ((0, 0, 0), 0.90050, 0.0030),\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = analyze_dispersion(case[\"L\"], case[\"xi\"], case[\"m0\"], case[\"data\"])\n        results.append(result)\n\n    def format_result(res):\n        c_lat_sq_str = f\"{res[0]:.6f}\"\n        delta_max_str = f\"{res[1]:.6f}\"\n        chi2_red_str = f\"{res[2]:.6f}\"\n        pass_str = str(res[3])\n        return f\"[{c_lat_sq_str},{delta_max_str},{chi2_red_str},{pass_str}]\"\n    \n    formatted_strings = [format_result(r) for r in results]\n    print(f\"[{','.join(formatted_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Extracting a clean signal for a hadron's mass requires designing an interpolating operator that has a large overlap with the desired state's wavefunction. This exercise explores the concept of \"smearing,\" a standard technique to achieve this, using a tractable quantum mechanical model. You will optimize a smearing parameter to maximize the signal for a specific state, providing concrete insight into how operator construction impacts the quality of lattice QCD results. ",
            "id": "3562967",
            "problem": "Consider Euclidean two-point correlation functions for hadron spectroscopy extracted from Lattice Quantum Chromodynamics (QCD). For a smeared interpolating operator with smearing radius parameter $\\sigma$, the spectral decomposition of the zero-momentum correlator is given by\n$$\nC(t;\\sigma) = \\sum_{n=0}^{N-1} |Z_n(\\sigma)|^2 e^{-E_n t},\n$$\nwhere $E_n$ are energy levels and $Z_n(\\sigma) = \\langle 0 | O(\\sigma) | n \\rangle$ are state-dependent overlaps that depend on the operator’s smearing profile.\n\nTo study how the smearing radius modifies overlap factors and redistributes spectral weight among excited states, adopt the following physically motivated continuum model for the overlap. Assume the spatial profile of the smeared source is a normalized three-dimensional Gaussian\n$$\nS(\\mathbf{r};\\sigma) = \\pi^{-3/4}\\,\\sigma^{-3/2} \\exp\\left(-\\frac{r^2}{2\\sigma^2}\\right),\n$$\nand that the hadron’s $s$-wave radial states can be qualitatively approximated by three-dimensional isotropic harmonic oscillator eigenfunctions (which form a complete orthonormal basis in $L^2(\\mathbb{R}^3)$) with characteristic length scale $a0$. The normalized $s$-wave radial eigenfunctions are\n$$\n\\psi_n(\\mathbf{r};a) = C_n(a)\\,L_n^{(1/2)}\\!\\left(\\frac{r^2}{a^2}\\right)\\exp\\left(-\\frac{r^2}{2a^2}\\right),\n$$\nwhere $L_n^{(1/2)}(x)$ are generalized Laguerre polynomials with parameter $1/2$, and $C_n(a)$ is chosen such that $\\int d^3r\\,|\\psi_n(\\mathbf{r};a)|^2 = 1$. Using the orthogonality of generalized Laguerre polynomials with the appropriate weight, one finds\n$$\nC_n(a) = \\left(\\frac{n!}{2\\pi a^3\\,\\Gamma\\!\\left(n+\\frac{3}{2}\\right)}\\right)^{1/2}.\n$$\nThe overlap factor is defined by the spatial integral\n$$\nZ_n(\\sigma) = \\int d^3r\\,\\psi_n(\\mathbf{r};a)\\,S(\\mathbf{r};\\sigma),\n$$\nand the spectral weights (the fractional contribution of each state to the correlator amplitude at $t=0$) are\n$$\nw_n(\\sigma) = \\frac{|Z_n(\\sigma)|^2}{\\sum_{m=0}^{N-1} |Z_m(\\sigma)|^2}.\n$$\n\nYour tasks are:\n- Derive, from first principles, an explicit computable expression for $Z_n(\\sigma)$ in terms of $a$, $\\sigma$, and known special functions for the first three $s$-wave states $n\\in\\{0,1,2\\}$. Start from the definitions above, use spherical symmetry, and reduce the three-dimensional integral to radial form. Use the fact that $L_n^{(1/2)}(x)$ are polynomials in $x$ so that the integrals reduce to Gaussian moments with $r^{2k}$.\n- Design and implement a gradient-based optimization method (gradient ascent with a backtracking line search) to maximize the squared overlap objective $F_n(\\sigma) = |Z_n(\\sigma)|^2$ with respect to $\\sigma$ over a bounded interval $\\sigma\\in[\\sigma_{\\min},\\sigma_{\\max}]$. Use a numerically stable central-difference approximation for the gradient $\\partial F_n/\\partial \\sigma$ that respects the bounds. Ensure convergence criteria are well-defined in terms of gradient magnitude and step size.\n- Quantify how the smearing radius redistributes spectral weight among excited states by computing $w_n(\\sigma)$ at the optimized $\\sigma$ for each target state, and comparing it to a fixed reference smearing $\\sigma_{\\mathrm{ref}}$.\n\nUse the following scientifically consistent parameters for the test suite:\n- Harmonic oscillator length scale $a = 1.0$ (dimensionless units; all $\\sigma$ are to be expressed in units of $a$, thus dimensionless).\n- Number of states $N=3$ with $n\\in\\{0,1,2\\}$; energies $E_n$ are not needed for the required outputs because all requested quantities are based on $Z_n(\\sigma)$ only.\n- Reference smearing $\\sigma_{\\mathrm{ref}} = 1.0$.\n- Bounds for the optimization $\\sigma_{\\min} = 0.05$, $\\sigma_{\\max} = 5.0$.\n- Initial guesses and target states for five test cases:\n    1. Target $n=0$, initial $\\sigma_0=0.5$.\n    2. Target $n=1$, initial $\\sigma_0=0.5$.\n    3. Target $n=2$, initial $\\sigma_0=0.5$.\n    4. Target $n=1$, initial $\\sigma_0=0.05$ (boundary start).\n    5. Target $n=0$, initial $\\sigma_0=5.0$ (boundary start).\n\nFor each test case, compute:\n- The optimized smearing radius $\\sigma_{\\mathrm{opt}}$ (dimensionless, in units of $a$).\n- The optimized target spectral weight $w_n(\\sigma_{\\mathrm{opt}})$ (dimensionless).\n- The change in the target spectral weight $\\Delta w_n = w_n(\\sigma_{\\mathrm{opt}}) - w_n(\\sigma_{\\mathrm{ref}})$ (dimensionless).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For the above five test cases, output the sequence\n$$\n[\\sigma_{\\mathrm{opt}}^{(1)},\\, w_{n_1}(\\sigma_{\\mathrm{opt}}^{(1)}),\\, \\Delta w_{n_1}^{(1)},\\, \\sigma_{\\mathrm{opt}}^{(2)},\\, w_{n_2}(\\sigma_{\\mathrm{opt}}^{(2)}),\\, \\Delta w_{n_2}^{(2)},\\, \\dots,\\, \\sigma_{\\mathrm{opt}}^{(5)},\\, w_{n_5}(\\sigma_{\\mathrm{opt}}^{(5)}),\\, \\Delta w_{n_5}^{(5)}],\n$$\nwhere $n_j$ is the target state index for test case $j$. All quantities are floats and dimensionless.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of quantum mechanics and lattice QCD modeling, is mathematically well-posed, objective, and self-contained. All necessary parameters and definitions for a unique solution are provided. We may therefore proceed with the solution.\n\nThe solution is presented in three parts as requested: first, the derivation of the overlap factors $Z_n(\\sigma)$; second, the design of the optimization algorithm; and third, the method for computing spectral weights and their changes.\n\n### Part 1: Derivation of the Overlap Factor $Z_n(\\sigma)$\n\nThe overlap factor $Z_n(\\sigma)$ is defined by the spatial integral:\n$$\nZ_n(\\sigma) = \\int d^3r\\,\\psi_n(\\mathbf{r};a)\\,S(\\mathbf{r};\\sigma)\n$$\nwhere $\\psi_n(\\mathbf{r};a)$ are the harmonic oscillator s-wave eigenfunctions and $S(\\mathbf{r};\\sigma)$ is the Gaussian smeared source. Both functions are spherically symmetric, depending only on the radial coordinate $r = |\\mathbf{r}|$. We can therefore reduce the $3$-dimensional integral to a $1$-dimensional radial integral by using spherical coordinates, where the volume element is $d^3r = 4\\pi r^2 dr$.\n\nThe explicit forms for the functions are:\n$$\nS(r;\\sigma) = \\pi^{-3/4}\\,\\sigma^{-3/2} \\exp\\left(-\\frac{r^2}{2\\sigma^2}\\right)\n$$\n$$\n\\psi_n(r;a) = C_n(a)\\,L_n^{(1/2)}\\!\\left(\\frac{r^2}{a^2}\\right)\\exp\\left(-\\frac{r^2}{2a^2}\\right)\n$$\nwith the normalization constant $C_n(a) = \\left(\\frac{n!}{2\\pi a^3\\,\\Gamma(n+3/2)}\\right)^{1/2}$.\n\nSubstituting these into the integral for $Z_n(\\sigma)$:\n$$\nZ_n(\\sigma) = 4\\pi \\int_0^\\infty r^2 dr \\left[ C_n(a)\\,L_n^{(1/2)}\\!\\left(\\frac{r^2}{a^2}\\right)\\exp\\left(-\\frac{r^2}{2a^2}\\right) \\right] \\left[ \\pi^{-3/4}\\,\\sigma^{-3/2} \\exp\\left(-\\frac{r^2}{2\\sigma^2}\\right) \\right]\n$$\nCombining constant prefactors and the exponential terms yields:\n$$\nZ_n(\\sigma) = 4\\pi^{1/4} C_n(a) \\sigma^{-3/2} \\int_0^\\infty dr\\, r^2 L_n^{(1/2)}\\!\\left(\\frac{r^2}{a^2}\\right) \\exp\\left(-\\frac{r^2}{2}\\left(\\frac{1}{a^2} + \\frac{1}{\\sigma^2}\\right)\\right)\n$$\nLet us define a combined length scale $\\beta$ such that $\\frac{1}{2\\beta^2} = \\frac{1}{2a^2} + \\frac{1}{2\\sigma^2}$, which gives $\\beta^2 = \\frac{a^2\\sigma^2}{a^2+\\sigma^2}$. The integral becomes:\n$$\nZ_n(\\sigma) = 4\\pi^{1/4} C_n(a) \\sigma^{-3/2} \\int_0^\\infty dr\\, r^2 L_n^{(1/2)}\\!\\left(\\frac{r^2}{a^2}\\right) \\exp\\left(-\\frac{r^2}{2\\beta^2}\\right)\n$$\nThe functions $L_n^{(1/2)}(x)$ for $n \\in \\{0,1,2\\}$ are polynomials:\n$L_0^{(1/2)}(x) = 1$\n$L_1^{(1/2)}(x) = -x + 3/2$\n$L_2^{(1/2)}(x) = \\frac{1}{2}x^2 - \\frac{5}{2}x + \\frac{15}{8}$\n\nSince $L_n^{(1/2)}(r^2/a^2)$ is a polynomial in $r^2$, the integral reduces to a sum of Gaussian moments of the form $I_{2k} = \\int_0^\\infty r^{2k} \\exp(-c r^2) dr$ with $c = 1/(2\\beta^2)$. The general formula for these moments is $\\int_0^\\infty x^{2m} e^{-\\gamma x^2} dx = \\frac{\\Gamma(m+1/2)}{2\\gamma^{m+1/2}}$.\nFor our case, with $m \\in \\{1, 2, 3\\}$ (from $r^2, r^4, r^6$ terms):\n$I_2 = \\int_0^\\infty r^2 e^{-r^2/(2\\beta^2)} dr = \\frac{\\Gamma(3/2)}{2(1/(2\\beta^2))^{3/2}} = \\frac{\\sqrt{2\\pi}\\beta^3}{2}$\n$I_4 = \\int_0^\\infty r^4 e^{-r^2/(2\\beta^2)} dr = \\frac{\\Gamma(5/2)}{2(1/(2\\beta^2))^{5/2}} = \\frac{3\\sqrt{2\\pi}\\beta^5}{2}$\n$I_6 = \\int_0^\\infty r^6 e^{-r^2/(2\\beta^2)} dr = \\frac{\\Gamma(7/2)}{2(1/(2\\beta^2))^{7/2}} = \\frac{15\\sqrt{2\\pi}\\beta^7}{2}$\n\nWe evaluate the integral part, let's call it $J_n$, for each $n$:\nFor $n=0$: $J_0 = I_2 = \\frac{\\sqrt{2\\pi}\\beta^3}{2}$.\nFor $n=1$: $J_1 = \\int_0^\\infty r^2 (-\\frac{r^2}{a^2} + \\frac{3}{2}) e^{-r^2/(2\\beta^2)} dr = \\frac{3}{2}I_2 - \\frac{1}{a^2}I_4 = \\frac{3\\sqrt{2\\pi}\\beta^3}{4} - \\frac{3\\sqrt{2\\pi}\\beta^5}{2a^2} = \\frac{3\\sqrt{2\\pi}\\beta^3}{4} \\left(1-\\frac{2\\beta^2}{a^2}\\right)$.\nFor $n=2$: $J_2 = \\int_0^\\infty r^2 (\\frac{r^4}{2a^4} - \\frac{5r^2}{2a^2} + \\frac{15}{8}) e^{-r^2/(2\\beta^2)} dr = \\frac{1}{2a^4}I_6 - \\frac{5}{2a^2}I_4 + \\frac{15}{8}I_2 = \\frac{15\\sqrt{2\\pi}\\beta^3}{16} \\left(\\frac{4\\beta^4}{a^4} - \\frac{4\\beta^2}{a^2} + 1\\right) = \\frac{15\\sqrt{2\\pi}\\beta^3}{16} \\left(1-\\frac{2\\beta^2}{a^2}\\right)^2$.\n\nThe normalization constants $C_n(a)$ are:\n$C_0(a) = \\pi^{-3/4}a^{-3/2}$\n$C_1(a) = \\sqrt{2/3}\\,\\pi^{-3/4}a^{-3/2}$\n$C_2(a) = \\sqrt{8/15}\\,\\pi^{-3/4}a^{-3/2}$\n\nCombining these pieces gives the final expressions for $Z_n(\\sigma)$. Let's use the simplification $1-\\frac{2\\beta^2}{a^2} = 1-\\frac{2\\sigma^2}{a^2+\\sigma^2} = \\frac{a^2-\\sigma^2}{a^2+\\sigma^2}$.\n\nFor $n=0$:\n$Z_0(\\sigma) = 4\\pi^{1/4}(\\pi^{-3/4}a^{-3/2})\\sigma^{-3/2} \\frac{\\sqrt{2\\pi}\\beta^3}{2} = 2\\sqrt{2}a^{-3/2}\\sigma^{-3/2}\\beta^3 = 2\\sqrt{2}a^{-3/2}\\sigma^{-3/2}\\frac{a^3\\sigma^3}{(a^2+\\sigma^2)^{3/2}} = 2\\sqrt{2}\\left(\\frac{a\\sigma}{a^2+\\sigma^2}\\right)^{3/2}$.\n\nFor $n=1$:\n$Z_1(\\sigma) = 4\\pi^{1/4}(\\sqrt{2/3}\\,\\pi^{-3/4}a^{-3/2})\\sigma^{-3/2} \\frac{3\\sqrt{2\\pi}\\beta^3}{4}(1-\\frac{2\\beta^2}{a^2}) = 2\\sqrt{3}a^{-3/2}\\sigma^{-3/2}\\beta^3\\left(\\frac{a^2-\\sigma^2}{a^2+\\sigma^2}\\right) = 2\\sqrt{3}\\left(\\frac{a\\sigma}{a^2+\\sigma^2}\\right)^{3/2}\\left(\\frac{a^2-\\sigma^2}{a^2+\\sigma^2}\\right)$.\n\nFor $n=2$:\n$Z_2(\\sigma) = 4\\pi^{1/4}(\\sqrt{8/15}\\,\\pi^{-3/4}a^{-3/2})\\sigma^{-3/2} \\frac{15\\sqrt{2\\pi}\\beta^3}{16}(1-\\frac{2\\beta^2}{a^2})^2 = \\sqrt{15}a^{-3/2}\\sigma^{-3/2}\\beta^3\\left(\\frac{a^2-\\sigma^2}{a^2+\\sigma^2}\\right)^2 = \\sqrt{15}\\left(\\frac{a\\sigma}{a^2+\\sigma^2}\\right)^{3/2}\\left(\\frac{a^2-\\sigma^2}{a^2+\\sigma^2}\\right)^2$.\n\nSetting $a=1.0$ as specified, we obtain the objective functions $F_n(\\sigma) = |Z_n(\\sigma)|^2$:\n$$F_0(\\sigma) = 8 \\left(\\frac{\\sigma}{1+\\sigma^2}\\right)^3$$\n$$F_1(\\sigma) = 12 \\left(\\frac{\\sigma}{1+\\sigma^2}\\right)^3 \\left(\\frac{1-\\sigma^2}{1+\\sigma^2}\\right)^2$$\n$$F_2(\\sigma) = 15 \\left(\\frac{\\sigma}{1+\\sigma^2}\\right)^3 \\left(\\frac{1-\\sigma^2}{1+\\sigma^2}\\right)^4$$\n\n### Part 2: Optimization Algorithm\n\nWe are tasked to maximize the objective function $F_n(\\sigma)$ for a target state $n$ with respect to the smearing radius $\\sigma$ within the interval $[\\sigma_{\\min}, \\sigma_{\\max}]$. We will implement a projected gradient ascent algorithm with a backtracking line search.\n\nThe algorithm proceeds as follows:\n1.  **Initialization**: Start with an initial guess $\\sigma_0$ for the target state $n$. Set iteration counter $k=0$.\n2.  **Gradient Calculation**: At each iteration $k$, compute the gradient of the objective function, $g_k = \\frac{dF_n}{d\\sigma}|_{\\sigma_k}$. A numerically stable central-difference approximation is used: $g_k \\approx \\frac{F_n(\\sigma_k+h) - F_n(\\sigma_k-h)}{2h}$ for a small step $h$. To respect the bounds, if $\\sigma_k$ is too close to a boundary for the central difference to be valid, a one-sided difference (forward or backward) is used.\n3.  **Line Search**: Find an appropriate step size $\\alpha_k$ using backtracking.\n    a. Initialize a trial step size, e.g., $\\alpha = 1.0$.\n    b. Propose an updated value: $\\sigma' = \\sigma_k + \\alpha g_k$.\n    c. Project the proposal onto the valid interval: $\\sigma_{k+1} = \\max(\\sigma_{\\min}, \\min(\\sigma_{\\max}, \\sigma'))$.\n    d. Check for sufficient ascent using an Armijo-like condition: $F_n(\\sigma_{k+1}) \\ge F_n(\\sigma_k) + c_1 \\alpha g_k (\\sigma_{k+1} - \\sigma_k)$, where $c_1$ is a small constant (e.g., $10^{-4}$).\n    e. If the condition is not met, reduce the step size $\\alpha \\leftarrow \\tau \\alpha$ with a backtracking factor $\\tau \\in (0,1)$ (e.g., $0.5$) and repeat from 3b.\n4.  **Update**: Set $\\sigma_k \\leftarrow \\sigma_{k+1}$.\n5.  **Convergence**: The process is iterated until the change in $\\sigma$ between successive iterations is below a tolerance, $|\\sigma_{k+1} - \\sigma_k|  \\epsilon_{\\text{step}}$, or a maximum number of iterations is reached.\n\nThis iterative procedure will find a local maximum of $F_n(\\sigma)$ within the specified bounds.\n\n### Part 3: Spectral Weight Calculation\n\nThe spectral weights $w_n(\\sigma)$ quantify the contribution of each state $n$ to the total signal at Euclidean time $t=0$. They are defined as:\n$$\nw_n(\\sigma) = \\frac{|Z_n(\\sigma)|^2}{\\sum_{m=0}^{N-1} |Z_m(\\sigma)|^2} = \\frac{F_n(\\sigma)}{\\sum_{m=0}^{N-1} F_m(\\sigma)}\n$$\nwhere the sum in the denominator runs over the states being considered, here $N=3$ ($m \\in \\{0, 1, 2\\}$).\n\nFor each test case with a target state $n_j$, after finding the optimized smearing radius $\\sigma_{\\mathrm{opt}}^{(j)}$ by maximizing $F_{n_j}(\\sigma)$, we compute the following quantities:\n1.  The optimized target spectral weight, $w_{n_j}(\\sigma_{\\mathrm{opt}}^{(j)})$. This is calculated by first computing $F_m(\\sigma_{\\mathrm{opt}}^{(j)})$ for all $m \\in \\{0, 1, 2\\}$ and then applying the formula for $w_{n_j}$.\n2.  The reference spectral weight, $w_{n_j}(\\sigma_{\\mathrm{ref}})$, using the fixed reference smearing $\\sigma_{\\mathrm{ref}}=1.0$.\n3.  The change in the target spectral weight, $\\Delta w_{n_j} = w_{n_j}(\\sigma_{\\mathrm{opt}}^{(j)}) - w_{n_j}(\\sigma_{\\mathrm{ref}})$. This measures the effectiveness of the optimization procedure in enhancing the signal of the target state compared to a reference smearing profile.\n\nThe following Python code implements this complete procedure for the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global parameters defined in the problem statement\nA_SCALE = 1.0  # Harmonic oscillator length scale\nSIGMA_REF = 1.0 # Reference smearing radius\nSIGMA_MIN = 0.05 # Minimum bound for optimization\nSIGMA_MAX = 5.0 # Maximum bound for optimization\nN_STATES = 3    # Number of states in the model\n\ndef get_F(n, sigma, a=A_SCALE):\n    \"\"\"\n    Computes the squared overlap F_n(sigma) = |Z_n(sigma)|^2 for a given state n.\n    The formulas are derived for and assume a=1.0.\n    \"\"\"\n    if a != 1.0:\n        raise ValueError(\"This implementation is specific to a=1.0.\")\n    if not (SIGMA_MIN = sigma = SIGMA_MAX):\n        # We handle this robustly, but avoid calls outside the domain.\n        return 0.0\n\n    # Common terms in the derived expressions for F_n\n    term1_base = sigma / (1.0 + sigma**2)\n    term2 = (1.0 - sigma**2) / (1.0 + sigma**2)\n    \n    # The term (sigma/(1+sigma^2))^3, which is prone to underflow for small sigma.\n    # However, Python's float64 is sufficient here.\n    term1_cubed = term1_base**3\n\n    if n == 0:\n        # F_0(sigma) = 8 * (sigma / (1+sigma^2))^3\n        val = 8.0 * term1_cubed\n    elif n == 1:\n        # F_1(sigma) = 12 * (sigma / (1+sigma^2))^3 * ((1-sigma^2)/(1+sigma^2))^2\n        val = 12.0 * term1_cubed * (term2**2)\n    elif n == 2:\n        # F_2(sigma) = 15 * (sigma / (1+sigma^2))^3 * ((1-sigma^2)/(1+sigma^2))^4\n        val = 15.0 * term1_cubed * (term2**4)\n    else:\n        raise ValueError(f\"State n={n} is not supported. Must be in [0, {N_STATES-1}]\")\n    \n    return val\n\ndef get_gradient(n, sigma, a=A_SCALE, h=1e-7):\n    \"\"\"\n    Computes the gradient of F_n with respect to sigma using a finite difference\n    method that respects the optimization bounds.\n    \"\"\"\n    if sigma - h  SIGMA_MIN and sigma + h > SIGMA_MAX: # sigma is the whole interval\n        return 0.0\n    elif sigma - h  SIGMA_MIN: # Near left boundary, use forward difference\n        return (get_F(n, sigma + h, a) - get_F(n, sigma, a)) / h\n    elif sigma + h > SIGMA_MAX: # Near right boundary, use backward difference\n        return (get_F(n, sigma, a) - get_F(n, sigma - h, a)) / h\n    else: # Central difference is safe\n        return (get_F(n, sigma + h, a) - get_F(n, sigma - h, a)) / (2 * h)\n\ndef optimize_sigma(target_n, sigma_0):\n    \"\"\"\n    Maximizes F_n(sigma) using projected gradient ascent with a backtracking line search.\n    \n    Args:\n        target_n (int): The index of the state to optimize for.\n        sigma_0 (float): The initial guess for the smearing radius sigma.\n        \n    Returns:\n        float: The optimized smearing radius sigma_opt.\n    \"\"\"\n    sigma = sigma_0\n    \n    # Optimization parameters\n    max_iter = 1000\n    step_tolerance = 1e-9\n    \n    # Backtracking line search parameters\n    initial_alpha = 1.0  # Initial step size for each iteration\n    c1 = 1e-4          # Sufficient ascent condition constant\n    tau = 0.5          # Step size reduction factor\n\n    for _ in range(max_iter):\n        grad = get_gradient(target_n, sigma)\n        \n        # If gradient is zero, we are at a stationary point\n        if np.abs(grad)  1e-15:\n            break\n\n        # Backtracking line search for step size alpha\n        alpha = initial_alpha\n        current_F = get_F(target_n, sigma)\n        \n        while True:\n            sigma_new_unproj = sigma + alpha * grad\n            sigma_new = np.clip(sigma_new_unproj, SIGMA_MIN, SIGMA_MAX)\n            \n            # Use Armijo-like condition for sufficient ascent\n            if get_F(target_n, sigma_new) >= current_F + c1 * alpha * grad * (sigma_new - sigma):\n                break\n            \n            alpha *= tau\n            if alpha  1e-12:  # Step size is too small, convergence likely\n                return sigma\n\n        sigma_prev = sigma\n        sigma = sigma_new\n        \n        # Check for convergence\n        if np.abs(sigma - sigma_prev)  step_tolerance:\n            break\n            \n    return sigma\n\ndef get_weights(sigma, a=A_SCALE, N=N_STATES):\n    \"\"\"\n    Computes the spectral weights w_n(sigma) for n=0..N-1.\n    \"\"\"\n    F_values = np.array([get_F(n, sigma, a) for n in range(N)])\n    F_sum = np.sum(F_values)\n    if F_sum  1e-15: # Avoid division by zero\n        return np.zeros(N)\n    return F_values / F_sum\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, 0.5),   # Target n=0, initial sigma_0=0.5\n        (1, 0.5),   # Target n=1, initial sigma_0=0.5\n        (2, 0.5),   # Target n=2, initial sigma_0=0.5\n        (1, 0.05),  # Target n=1, initial sigma_0=0.05 (boundary start)\n        (0, 5.0),   # Target n=0, initial sigma_0=5.0 (boundary start)\n    ]\n\n    results = []\n    \n    for n_target, sigma_0 in test_cases:\n        # 1. Optimize sigma\n        sigma_opt = optimize_sigma(n_target, sigma_0)\n        \n        # 2. Compute optimized spectral weight\n        weights_opt = get_weights(sigma_opt)\n        w_n_opt = weights_opt[n_target]\n        \n        # 3. Compute change in spectral weight\n        weights_ref = get_weights(SIGMA_REF)\n        w_n_ref = weights_ref[n_target]\n        delta_w_n = w_n_opt - w_n_ref\n        \n        results.extend([sigma_opt, w_n_opt, delta_w_n])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.8f}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate goal of a lattice calculation is to determine physical quantities in the continuum limit ($a \\to 0$), where discretization artifacts vanish. This advanced practice introduces the powerful technique of a joint continuum extrapolation, where data from different lattice fermion formulations are combined to achieve a more robust and precise result. By enforcing the universality of the continuum limit while allowing for scheme-dependent discretization effects, you will practice a state-of-the-art method for controlling systematic uncertainties. ",
            "id": "3562999",
            "problem": "You are given two discretizations of quark actions commonly used in Lattice Quantum Chromodynamics (LQCD): the $\\mathcal{O}(a)$-improved Sheikholeslami–Wohlert (clover) action and the Domain-Wall Fermion (DWF) action. According to the Symanzik effective theory for lattice artifacts, a hadron mass extracted at lattice spacing $a$ may be expanded as $M(a)=M_0+c_1 a+c_2 a^2+\\mathcal{O}(a^3)$, where $M_0$ is the continuum limit in Giga-electronvolts (GeV), $a$ is in femtometers (fm), and $c_1$, $c_2$ are coefficients that depend on the discretization. The coefficients for different discretizations are scheme dependent, but the continuum limit $M_0$ is scheme independent. Domain-Wall Fermions (DWF) preserve chiral symmetry at nonzero lattice spacing, making $c_1$ naturally small or vanishing, while for $\\mathcal{O}(a)$-improved clover fermions the linear term is suppressed by improvement but not guaranteed to be exactly zero.\n\nStarting from the following fundamental base:\n- The spectral decomposition of Euclidean two-point correlation functions in LQCD implies ground-state hadron masses are well-defined in the continuum limit $a \\to 0$.\n- The Symanzik effective theory organizes discretization effects as an expansion in powers of $a$, so that for sufficiently small $a$, $M(a)$ may be described by a low-order polynomial in $a$, with action-dependent coefficients.\n- Weighted least squares with uncorrelated measurements minimizes $\\chi^2=\\sum_i (y_i - f(x_i;\\theta))^2/\\sigma_i^2$, where $y_i$ are measurements with standard deviations $\\sigma_i$, $x_i$ are inputs, and $\\theta$ are model parameters.\n\nYour task is to implement a joint continuum extrapolation of hadron masses for clover and DWF discretizations under the model $M(a)=M_0+c_1 a+c_2 a^2$ using weighted linear least squares, with the following requirements:\n\n$1.$ Construct a joint fit in which clover and DWF data share a common $M_0$ but have independent cutoff coefficients. That is, fit a model with parameters $\\{M_0, c_{1,\\mathrm{clover}}, c_{2,\\mathrm{clover}}, c_{1,\\mathrm{DWF}}, c_{2,\\mathrm{DWF}}\\}$, where clover rows contribute $\\{1,a,a^2,0,0\\}$ and DWF rows contribute $\\{1,0,0,a,a^2\\}$ to the design matrix. For a specified test case, you must also be able to impose the chiral-symmetry motivated constraint $c_{1,\\mathrm{DWF}}=0$, in which case the DWF row becomes $\\{1,0,0,0,a^2\\}$ and the parameter set reduces accordingly.\n\n$2.$ In addition to the joint fit, perform separate weighted quadratic fits for the clover data and for the DWF data independently, each of the form $M(a)=M_{0,\\mathrm{scheme}}+c_{1,\\mathrm{scheme}} a+c_{2,\\mathrm{scheme}} a^2$.\n\n$3.$ Quantify “scheme dependence” in two ways:\n- Test whether the separate-scheme continuum limits agree by checking if $|M_{0,\\mathrm{clover}}-M_{0,\\mathrm{DWF}}|\\leq 2\\sqrt{\\sigma^2(M_{0,\\mathrm{clover}})+\\sigma^2(M_{0,\\mathrm{DWF}})}$, where $\\sigma^2(\\cdot)$ are the variances from the separate fits. Report a boolean that is $\\mathrm{True}$ if this inequality holds and $\\mathrm{False}$ otherwise.\n- Test whether $c_{1,\\mathrm{DWF}}$ from the separate DWF fit is statistically consistent with zero at the two-standard-deviation level by checking $|c_{1,\\mathrm{DWF}}|\\leq 2\\,\\sigma(c_{1,\\mathrm{DWF}})$. Report a boolean.\n\n$4.$ Report goodness-of-fit for the joint fit via $\\chi^2/\\mathrm{dof}$, where $\\mathrm{dof}=N-p$, $N$ is the total number of data points across both schemes, and $p$ is the number of fitted parameters in the joint fit. Use the unconstrained joint fit when the test case does not request the constraint, and use the constrained version when it does.\n\n$5.$ Units and rounding:\n- All masses must be treated in GeV and all lattice spacings in fm.\n- Your program must output the joint-fit estimate of $M_0$ in GeV for each test case, rounded to $6$ decimal places.\n- Your program must output $\\chi^2/\\mathrm{dof}$ for the joint fit for each test case, rounded to $3$ decimal places.\n\n$6.$ Test suite:\nImplement your program to compute results for the following three test cases. In all cases, treat the quoted values as central values with uncorrelated Gaussian standard deviations as given. The arrays give the lattice spacings, measured masses, and their standard deviations.\n\n- Case $1$ (unconstrained joint fit):\n  Clover:\n  $a_{\\mathrm{C}}=\\{0.12,0.09,0.06,0.045\\}\\,\\mathrm{fm}$,\n  $M_{\\mathrm{C}}=\\{0.9640,0.95765,0.9537,0.9499375\\}\\,\\mathrm{GeV}$,\n  $\\sigma_{\\mathrm{C}}=\\{0.0025,0.0020,0.0018,0.0016\\}\\,\\mathrm{GeV}$.\n  DWF:\n  $a_{\\mathrm{D}}=\\{0.12,0.09,0.06,0.045\\}\\,\\mathrm{fm}$,\n  $M_{\\mathrm{D}}=\\{0.94372,0.94333,0.94068,0.9408075\\}\\,\\mathrm{GeV}$,\n  $\\sigma_{\\mathrm{D}}=\\{0.0020,0.0018,0.0016,0.0015\\}\\,\\mathrm{GeV}$.\n\n- Case $2$ (constrained joint fit with $c_{1,\\mathrm{DWF}}=0$):\n  Clover:\n  $a_{\\mathrm{C}}=\\{0.10,0.08,0.06\\}\\,\\mathrm{fm}$,\n  $M_{\\mathrm{C}}=\\{1.1925,1.19298,1.19492\\}\\,\\mathrm{GeV}$,\n  $\\sigma_{\\mathrm{C}}=\\{0.0015,0.0013,0.0012\\}\\,\\mathrm{GeV}$.\n  DWF:\n  $a_{\\mathrm{D}}=\\{0.12,0.09,0.07\\}\\,\\mathrm{fm}$,\n  $M_{\\mathrm{D}}=\\{1.20032,1.200605,1.200545\\}\\,\\mathrm{GeV}$,\n  $\\sigma_{\\mathrm{D}}=\\{0.0014,0.0013,0.0012\\}\\,\\mathrm{GeV}$.\n\n- Case $3$ (unconstrained joint fit, finer lattices):\n  Clover:\n  $a_{\\mathrm{C}}=\\{0.08,0.06,0.04,0.03,0.02\\}\\,\\mathrm{fm}$,\n  $M_{\\mathrm{C}}=\\{0.5017,0.5011,0.50085,0.50052,0.50042\\}\\,\\mathrm{GeV}$,\n  $\\sigma_{\\mathrm{C}}=\\{0.0005,0.0004,0.00035,0.0003,0.00025\\}\\,\\mathrm{GeV}$.\n  DWF:\n  $a_{\\mathrm{D}}=\\{0.08,0.06,0.04,0.03,0.02\\}\\,\\mathrm{fm}$,\n  $M_{\\mathrm{D}}=\\{0.499812,0.499998,0.499948,0.500012,0.499982\\}\\,\\mathrm{GeV}$,\n  $\\sigma_{\\mathrm{D}}=\\{0.0005,0.0004,0.00035,0.0003,0.00025\\}\\,\\mathrm{GeV}$.\n\n$7.$ Final output format:\nYour program should produce a single line of output containing the results aggregated over the three cases as a comma-separated Python-style list. For each case in order, output the following four items:\n- the joint-fit estimate of $M_0$ in GeV rounded to $6$ decimals,\n- the joint-fit $\\chi^2/\\mathrm{dof}$ rounded to $3$ decimals,\n- the boolean for $M_{0,\\mathrm{clover}}$ and $M_{0,\\mathrm{DWF}}$ agreement within two standard deviations from the separate fits,\n- the boolean for whether $c_{1,\\mathrm{DWF}}$ is consistent with zero within two standard deviations from the separate DWF fit.\n\nThus your program must print a single line of the form\n$[M0\\_1,\\chi2dof\\_1,\\mathrm{bool1},\\mathrm{bool2},M0\\_2,\\chi2dof\\_2,\\mathrm{bool3},\\mathrm{bool4},M0\\_3,\\chi2dof\\_3,\\mathrm{bool5},\\mathrm{bool6}]$,\nwith $M0\\_k$ in $\\mathrm{GeV}$ and $\\chi2dof\\_k$ dimensionless.",
            "solution": "The problem requires performing a joint continuum extrapolation of hadron mass data from two different lattice QCD discretizations, Clover and Domain-Wall Fermions (DWF), and comparing the results with separate extrapolations. This is a standard procedure in lattice QCD for removing discretization artifacts and determining physical quantities in the continuum limit ($a \\to 0$). The process relies on several fundamental principles.\n\nFirst, the Symanzik effective theory provides a theoretical foundation for the extrapolation model. It states that for a lattice action improved to a certain order, the discretization errors in a physical observable, such as a hadron mass $M(a)$, can be expressed as a power series in the lattice spacing $a$. For the actions considered, a quadratic model $M(a) = M_0 + c_1 a + c_2 a^2$ is an appropriate low-order truncation for sufficiently small $a$. The parameter $M_0$ represents the true physical mass in the continuum, which must be independent of the regularization scheme (i.e., the same for both Clover and DWF). The coefficients $c_1$ and $c_2$, however, are scheme-dependent. DWF actions possess an approximate chiral symmetry that suppresses the linear term, suggesting that $c_{1,\\mathrm{DWF}}$ should be small and statistically consistent with zero.\n\nSecond, the fitting procedure to determine the model parameters $\\{M_0, c_1, c_2, \\dots\\}$ is based on the method of weighted least squares. Given a set of $N$ data points $(a_i, M_i)$ with uncorrelated Gaussian uncertainties $\\sigma_i$, this method finds the parameter vector $\\hat{\\theta}$ that minimizes the chi-squared function, $\\chi^2$:\n\n$$\n\\chi^2 = \\sum_{i=1}^{N} \\frac{(M_i - M(a_i; \\theta))^2}{\\sigma_i^2}\n$$\n\nMinimizing $\\chi^2$ is equivalent to the principle of maximum likelihood for Gaussian-distributed errors. For a linear model of the form $M(a; \\theta) = \\sum_j X_{ij} \\theta_j$, where $X$ is the design matrix, the solution is given by the normal equations:\n\n$$\n(X^T W X) \\hat{\\theta} = X^T W y\n$$\n\nHere, $y$ is the vector of measured masses $M_i$, $\\hat{\\theta}$ is the vector of best-fit parameters, and $W$ is a diagonal weight matrix with entries $W_{ii} = 1/\\sigma_i^2$. The solution for the parameters is $\\hat{\\theta} = (X^T W X)^{-1} X^T W y$. The statistical uncertainties on the fitted parameters are derived from their covariance matrix, given by $C = (X^T W X)^{-1}$. The variance of the $j$-th parameter is $\\sigma^2(\\hat{\\theta}_j) = C_{jj}$.\n\nThe implementation will proceed by defining a general function to solve these normal equations. This function will take the design matrix $X$, the data vector $y$, and the vector of uncertainties $\\sigma$ as inputs, and will return the parameter vector $\\hat{\\theta}$ and the covariance matrix $C$.\n\nThe analysis involves three types of fits:\n\n1.  **Separate Clover Fit**:\n    The model is $M(a) = M_{0,\\mathrm{clover}} + c_{1,\\mathrm{clover}} a + c_{2,\\mathrm{clover}} a^2$. The parameter vector is $\\theta_C = [M_{0,C}, c_{1,C}, c_{2,C}]^T$. For a set of $N_C$ Clover data points, the design matrix $X_C$ is an $N_C \\times 3$ matrix where the $i$-th row is $[1, a_i, a_i^2]$.\n\n2.  **Separate DWF Fit**:\n    Similarly, the model is $M(a) = M_{0,\\mathrm{DWF}} + c_{1,\\mathrm{DWF}} a + c_{2,\\mathrm{DWF}} a^2$. The parameter vector is $\\theta_D = [M_{0,D}, c_{1,D}, c_{2,D}]^T$, and the $N_D \\times 3$ design matrix $X_D$ has rows $[1, a_i, a_i^2]$.\n\n3.  **Joint Fit**:\n    The key physical principle here is that the continuum mass $M_0$ is universal. We thus perform a combined fit to all $N = N_C + N_D$ data points with a single $M_0$ but separate, scheme-dependent coefficients.\n    -   **Unconstrained Fit**: The parameter vector is $\\theta_J = [M_0, c_{1,C}, c_{2,C}, c_{1,D}, c_{2,D}]^T$. The design matrix $X_J$ is an $N \\times 5$ matrix. For a Clover data point at $a_i$, the corresponding row is $[1, a_i, a_i^2, 0, 0]$. For a DWF data point at $a_j$, the row is $[1, 0, 0, a_j, a_j^2]$.\n    -   **Constrained Fit**: In test cases where the chiral symmetry of DWF is assumed to be exact enough to enforce $c_{1,D}=0$, this parameter is removed from the fit. The parameter vector becomes $\\theta_{J,c} = [M_0, c_{1,C}, c_{2,C}, c_{2,D}]^T$. The design matrix $X_{J,c}$ is an $N \\times 4$ matrix. The row for a Clover point is $[1, a_i, a_i^2, 0]$, while the row for a DWF point is $[1, 0, 0, a_j^2]$.\n\nFor each test case, the algorithm will:\n-   Perform the separate fits for Clover and DWF data to obtain $M_{0,C}$, $\\sigma^2(M_{0,C})$, $M_{0,D}$, $\\sigma^2(M_{0,D})$, $c_{1,D}$, and $\\sigma(c_{1,D})$.\n-   Perform the specified joint fit (constrained or unconstrained) to obtain the primary result for the continuum mass, $M_0$.\n-   Calculate the goodness-of-fit $\\chi^2/\\mathrm{dof}$ for the joint fit. The number of degrees of freedom is $\\mathrm{dof} = N - p$, where $N$ is the total number of data points and $p$ is the number of parameters in the joint fit ($p=5$ for unconstrained, $p=4$ for constrained).\n-   Evaluate two boolean conditions based on the results from the *separate fits*:\n    1.  Test for agreement of continuum limits: $|M_{0,C}-M_{0,D}| \\leq 2\\sqrt{\\sigma^2(M_{0,C})+\\sigma^2(M_{0,D})}$. This checks if the two independent determinations of $M_0$ are consistent within two standard deviations of their difference.\n    2.  Test for chiral symmetry behavior: $|c_{1,D}| \\leq 2\\,\\sigma(c_{1,D})$. This checks if the linear discretization term for DWF is consistent with zero at the $95\\%$ confidence level.\n\nThe final results combining the joint fit $M_0$, its $\\chi^2/\\mathrm{dof}$, and the two boolean checks from the separate fits will be aggregated for all test cases and formatted as specified. All numerical calculations will be performed using the `numpy` library for matrix operations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the hadron spectroscopy continuum extrapolation problem.\n    It defines the test cases, processes each one, and prints the final combined result.\n    \"\"\"\n\n    def weighted_least_squares(X, y, sigma):\n        \"\"\"\n        Performs weighted linear least squares fitting.\n        \n        Args:\n            X (np.ndarray): The design matrix.\n            y (np.ndarray): The vector of observed values.\n            sigma (np.ndarray): The vector of standard deviations for the observations.\n\n        Returns:\n            tuple: A tuple containing:\n                - np.ndarray: The vector of best-fit parameters.\n                - np.ndarray: The covariance matrix of the parameters.\n        \"\"\"\n        w = 1.0 / sigma**2\n        # Numerically stable way to compute (X.T * W) @ X and (X.T * W) @ y\n        # where W is a diagonal matrix of weights.\n        XT_W = X.T * w\n        XTWX = XT_W @ X\n        XTWy = XT_W @ y\n        \n        # Solve the normal equations: (X.T W X) theta = X.T W y\n        parameters = np.linalg.solve(XTWX, XTWy)\n        \n        # The covariance matrix is the inverse of (X.T W X)\n        covariance_matrix = np.linalg.inv(XTWX)\n        \n        return parameters, covariance_matrix\n\n    def process_case(clover_data, dwf_data, constrained):\n        \"\"\"\n        Processes a single test case, performing separate and joint fits.\n        \n        Args:\n            clover_data (tuple): Contains a, M, sigma arrays for Clover.\n            dwf_data (tuple): Contains a, M, sigma arrays for DWF.\n            constrained (bool): True if the joint fit is constrained (c1_DWF=0).\n\n        Returns:\n            list: A list containing [M0_joint, chi2dof, bool_M0_agree, bool_c1_is_zero].\n        \"\"\"\n        a_c, M_c, s_c = clover_data\n        a_d, M_d, s_d = dwf_data\n\n        # 1. Separate Clover Fit\n        X_c = np.vstack([np.ones_like(a_c), a_c, a_c**2]).T\n        params_c, cov_c = weighted_least_squares(X_c, M_c, s_c)\n        M0_clover = params_c[0]\n        var_M0_clover = cov_c[0, 0]\n\n        # 2. Separate DWF Fit\n        X_d = np.vstack([np.ones_like(a_d), a_d, a_d**2]).T\n        params_d, cov_d = weighted_least_squares(X_d, M_d, s_d)\n        M0_dwf = params_d[0]\n        var_M0_dwf = cov_d[0, 0]\n        c1_dwf = params_d[1]\n        sigma_c1_dwf = np.sqrt(cov_d[1, 1])\n\n        # 3. Joint Fit (constrained or unconstrained)\n        y_joint = np.concatenate([M_c, M_d])\n        s_joint = np.concatenate([s_c, s_d])\n        n_c = len(a_c)\n        n_d = len(a_d)\n        N = n_c + n_d\n\n        if not constrained:\n            # Unconstrained joint fit: {M0, c1_C, c2_C, c1_D, c2_D}\n            p = 5\n            X_joint = np.zeros((N, p))\n            # Clover rows\n            X_joint[:n_c, 0] = 1\n            X_joint[:n_c, 1] = a_c\n            X_joint[:n_c, 2] = a_c**2\n            # DWF rows\n            X_joint[n_c:, 0] = 1\n            X_joint[n_c:, 3] = a_d\n            X_joint[n_c:, 4] = a_d**2\n        else:\n            # Constrained joint fit (c1_DWF = 0): {M0, c1_C, c2_C, c2_D}\n            p = 4\n            X_joint = np.zeros((N, p))\n            # Clover rows\n            X_joint[:n_c, 0] = 1\n            X_joint[:n_c, 1] = a_c\n            X_joint[:n_c, 2] = a_c**2\n            # DWF rows\n            X_joint[n_c:, 0] = 1\n            X_joint[n_c:, 3] = a_d**2\n\n        params_joint, _ = weighted_least_squares(X_joint, y_joint, s_joint)\n        M0_joint = params_joint[0]\n        \n        # 4. Goodness-of-fit for joint fit\n        residuals = y_joint - (X_joint @ params_joint)\n        chi2 = np.sum((residuals / s_joint)**2)\n        dof = N - p\n        chi2_dof = chi2 / dof if dof > 0 else 0.0\n\n        # 5. Scheme dependence tests from separate fits\n        # Test 1: Agreement of continuum limits\n        m0_diff = np.abs(M0_clover - M0_dwf)\n        m0_err_sum = np.sqrt(var_M0_clover + var_M0_dwf)\n        bool_M0_agree = m0_diff = 2 * m0_err_sum\n\n        # Test 2: Consistency of c1_DWF with zero\n        bool_c1_is_zero = np.abs(c1_dwf) = 2 * sigma_c1_dwf\n\n        return [\n            round(M0_joint, 6),\n            round(chi2_dof, 3),\n            bool_M0_agree,\n            bool_c1_is_zero\n        ]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (unconstrained)\n        {\n            \"clover_data\": (\n                np.array([0.12, 0.09, 0.06, 0.045]),\n                np.array([0.9640, 0.95765, 0.9537, 0.9499375]),\n                np.array([0.0025, 0.0020, 0.0018, 0.0016])\n            ),\n            \"dwf_data\": (\n                np.array([0.12, 0.09, 0.06, 0.045]),\n                np.array([0.94372, 0.94333, 0.94068, 0.9408075]),\n                np.array([0.0020, 0.0018, 0.0016, 0.0015])\n            ),\n            \"constrained\": False\n        },\n        # Case 2 (constrained)\n        {\n            \"clover_data\": (\n                np.array([0.10, 0.08, 0.06]),\n                np.array([1.1925, 1.19298, 1.19492]),\n                np.array([0.0015, 0.0013, 0.0012])\n            ),\n            \"dwf_data\": (\n                np.array([0.12, 0.09, 0.07]),\n                np.array([1.20032, 1.200605, 1.200545]),\n                np.array([0.0014, 0.0013, 0.0012])\n            ),\n            \"constrained\": True\n        },\n        # Case 3 (unconstrained)\n        {\n            \"clover_data\": (\n                np.array([0.08, 0.06, 0.04, 0.03, 0.02]),\n                np.array([0.5017, 0.5011, 0.50085, 0.50052, 0.50042]),\n                np.array([0.0005, 0.0004, 0.00035, 0.0003, 0.00025])\n            ),\n            \"dwf_data\": (\n                np.array([0.08, 0.06, 0.04, 0.03, 0.02]),\n                np.array([0.499812, 0.499998, 0.499948, 0.500012, 0.499982]),\n                np.array([0.0005, 0.0004, 0.00035, 0.0003, 0.00025])\n            ),\n            \"constrained\": False\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        case_results = process_case(case[\"clover_data\"], case[\"dwf_data\"], case[\"constrained\"])\n        results.extend(case_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}