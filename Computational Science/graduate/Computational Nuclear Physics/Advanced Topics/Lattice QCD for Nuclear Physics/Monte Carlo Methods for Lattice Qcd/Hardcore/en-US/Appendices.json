{
    "hands_on_practices": [
        {
            "introduction": "In Hybrid Monte Carlo simulations, the most computationally intensive step is often the repeated solution of a large linear system involving the fermion matrix. The Conjugate Gradient (CG) algorithm is the standard iterative method for this task. This exercise  provides a foundational understanding of the CG method by guiding you through a first-principles derivation of the algorithm and its famous convergence rate, which depends critically on the condition number $\\kappa$ of the matrix.",
            "id": "3571187",
            "problem": "In Hybrid Monte Carlo (HMC) simulations of lattice Quantum Chromodynamics (QCD) with Wilson-like fermions, one frequently solves linear systems involving the normal equations of the lattice Dirac operator. Let $D$ denote a Wilson-like lattice Dirac operator, and consider the Hermitian positive-definite (HPD) matrix $A \\equiv D^{\\dagger} D$. Solving $A x = b$ is central to the evaluation of the pseudofermion action and forces in HMC trajectories.\n\n(a) Starting from the definition of a Krylov subspace and the requirement that search directions be $A$-conjugate, define the Conjugate Gradient (CG) algorithm for an HPD system $A x = b$. Your definition must specify the initialization, the recurrence relations for the iterates, the residuals, the search directions, and the scalar coefficients that ensure $A$-conjugacy.\n\n(b) Using only foundational facts from linear algebra and approximation theory, derive a bound on the $A$-norm of the CG error after $k$ iterations in terms of the spectrum of $A$. Let $e_k \\equiv x^{\\ast} - x_k$ denote the error, where $x^{\\ast}$ is the exact solution and $x_k$ is the CG iterate after $k$ steps. Express your bound in terms of the condition number $\\kappa \\equiv \\lambda_{\\max}/\\lambda_{\\min}$ of $A$, where $\\lambda_{\\min}$ and $\\lambda_{\\max}$ denote the smallest and largest eigenvalues of $A$, respectively. You may use well-tested facts about Chebyshev polynomials, but you must derive how they enter the CG error bound via polynomial approximation on the spectrum of $A$.\n\n(c) From your bound in part (b), extract the asymptotic per-iteration convergence factor $\\rho(\\kappa)$, defined as the smallest scalar $\\rho(\\kappa)$ such that there exists a constant $C$ independent of $k$ with\n$$\n\\| e_k \\|_{A} \\le C \\, \\rho(\\kappa)^{\\,k} \\, \\| e_0 \\|_{A} \\quad \\text{for all } k \\ge 0 .\n$$\nReport $\\rho(\\kappa)$ as a single simplified analytic expression in terms of $\\kappa$ only. No units are required. Provide your final answer as the expression for $\\rho(\\kappa)$ only.",
            "solution": "(a) The Conjugate Gradient (CG) algorithm is an iterative method for solving a linear system $A x = b$, where the matrix $A$ is Hermitian positive-definite (HPD). The algorithm constructs the $k$-th iterate $x_k$ in an affine subspace $x_0 + \\mathcal{K}_k(A, r_0)$, where $x_0$ is an initial guess, $r_0 = b - A x_0$ is the initial residual, and $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$ is the $k$-th Krylov subspace generated by $A$ and $r_0$. The core principle of CG is to find the unique $x_k \\in x_0 + \\mathcal{K}_k(A, r_0)$ that minimizes the $A$-norm of the error, $\\|e_k\\|_A \\equiv \\sqrt{e_k^\\dagger A e_k}$, where $e_k = x^\\ast - x_k$ and $x^\\ast=A^{-1}b$ is the exact solution.\n\nThis minimization is achieved efficiently by generating a sequence of search directions $\\{p_0, p_1, \\dots, p_{k-1}\\}$ that are $A$-conjugate, meaning $p_i^\\dagger A p_j = 0$ for $i \\neq j$. These directions form an $A$-orthogonal basis for the Krylov subspace $\\mathcal{K}_k(A, r_0)$. The iterate is updated along these directions: $x_{k+1} = x_k + \\alpha_k p_k$. The coefficient $\\alpha_k$ is chosen to minimize $\\|e_{k+1}\\|_A$. This is equivalent to enforcing that the new residual $r_{k+1} = b - A x_{k+1}$ is orthogonal to the current search direction $p_k$. The new search direction $p_{k+1}$ is then constructed from the new residual $r_{k+1}$ and the previous search direction $p_k$ in a way that ensures it is $A$-conjugate to all previous search directions. Due to the properties of the algorithm, a simple two-term recurrence is sufficient.\n\nThe complete algorithm is defined as follows:\n\n1.  **Initialization**:\n    Choose an initial guess $x_0$.\n    Compute the initial residual: $r_0 = b - A x_0$.\n    Set the initial search direction: $p_0 = r_0$.\n    Let $k=0$.\n\n2.  **Iteration**:\n    While a convergence criterion is not met:\n    a.  Compute the step size $\\alpha_k$:\n        $$ \\alpha_k = \\frac{r_k^\\dagger r_k}{p_k^\\dagger A p_k} $$\n    b.  Update the solution iterate:\n        $$ x_{k+1} = x_k + \\alpha_k p_k $$\n    c.  Update the residual using a short recurrence:\n        $$ r_{k+1} = r_k - \\alpha_k A p_k $$\n    d.  Check for convergence (e.g., using $\\|r_{k+1}\\|$). If converged, stop.\n    e.  Compute the coefficient for the new search direction:\n        $$ \\beta_k = \\frac{r_{k+1}^\\dagger r_{k+1}}{r_k^\\dagger r_k} $$\n    f.  Update the search direction:\n        $$ p_{k+1} = r_{k+1} + \\beta_k p_k $$\n    g.  Increment the iteration counter: $k \\leftarrow k+1$.\n\n(b) To derive a bound on the $A$-norm of the error, we leverage the minimization property of the CG algorithm. The iterate $x_k$ is constructed from the Krylov subspace $\\mathcal{K}_k(A, r_0)$, so we can write $x_k = x_0 + Q_{k-1}(A) r_0$, where $Q_{k-1}$ is a polynomial of degree at most $k-1$.\n\nThe error after $k$ steps is $e_k = x^\\ast - x_k$. Using $r_0 = b - Ax_0 = A(x^\\ast - x_0) = A e_0$, we can express the error in terms of the initial error $e_0$:\n$$ e_k = x^\\ast - (x_0 + Q_{k-1}(A) r_0) = (x^\\ast - x_0) - Q_{k-1}(A) A e_0 = e_0 - A Q_{k-1}(A) e_0 $$\nLet $P_k(\\lambda) = 1 - \\lambda Q_{k-1}(\\lambda)$. $P_k$ is a polynomial of degree at most $k$ satisfying the constraint $P_k(0) = 1$. With this definition, the error becomes:\n$$ e_k = P_k(A) e_0 $$\nThe CG algorithm finds the iterate $x_k$ that minimizes $\\|e_k\\|_A$ over all possible choices of $x_k \\in x_0 + \\mathcal{K}_k(A, r_0)$. This is equivalent to finding the polynomial $P_k$ (of degree at most $k$ with $P_k(0)=1$) that minimizes $\\|P_k(A) e_0\\|_A$.\n\nWe can bound this minimized norm by considering any valid polynomial. Let $\\mathcal{P}_k$ be the set of polynomials of degree at most $k$ with $P_k(0)=1$. Then,\n$$ \\|e_k\\|_A = \\min_{P_k \\in \\mathcal{P}_k} \\|P_k(A) e_0\\|_A $$\nTo get an upper bound, we express $e_0$ in the eigenbasis of $A$. Let $A v_j = \\lambda_j v_j$ for $j=1, \\dots, n$, where the eigenvalues $\\lambda_j$ are positive. Let $e_0 = \\sum_{j=1}^n c_j v_j$. Then:\n$$ \\|P_k(A) e_0\\|_A^2 = \\left\\| \\sum_{j=1}^n c_j P_k(\\lambda_j) v_j \\right\\|_A^2 = \\sum_{j=1}^n \\lambda_j |c_j|^2 |P_k(\\lambda_j)|^2 \\le \\left( \\max_{j} |P_k(\\lambda_j)| \\right)^2 \\sum_{j=1}^n \\lambda_j |c_j|^2 $$\nThe last sum is $\\|e_0\\|_A^2$. The maximum is taken over the spectrum $\\sigma(A)$ of $A$. This gives:\n$$ \\|e_k\\|_A \\le \\left( \\min_{P_k \\in \\mathcal{P}_k} \\max_{\\lambda \\in \\sigma(A)} |P_k(\\lambda)| \\right) \\|e_0\\|_A $$\nThe problem is now reduced to a polynomial approximation problem: find the polynomial of degree $k$ that is $1$ at $\\lambda=0$ and has the smallest possible maximum magnitude over the spectrum of $A$. Since the spectrum is only known to lie in the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$, we bound the maximum over this continuous interval:\n$$ \\min_{P_k \\in \\mathcal{P}_k} \\max_{\\lambda \\in \\sigma(A)} |P_k(\\lambda)| \\le \\min_{P_k \\in \\mathcal{P}_k} \\max_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} |P_k(\\lambda)| $$\nThe solution to this classic approximation problem is given by a scaled and shifted Chebyshev polynomial of the first kind, $T_k(z)$. Recall that $T_k(z)$ minimizes the maximum magnitude on $[-1, 1]$ among polynomials of degree $k$ with a fixed leading coefficient. To satisfy $P_k(0)=1$, we choose the polynomial:\n$$ P_k(\\lambda) = \\frac{T_k\\left(\\frac{\\lambda_{\\max}+\\lambda_{\\min}-2\\lambda}{\\lambda_{\\max}-\\lambda_{\\min}}\\right)}{T_k\\left(\\frac{\\lambda_{\\max}+\\lambda_{\\min}}{\\lambda_{\\max}-\\lambda_{\\min}}\\right)} $$\nThe argument of $T_k$ in the numerator maps the interval $\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]$ to $[-1, 1]$. On this interval, $|T_k(z)| \\le 1$. Therefore, the maximum magnitude of $P_k(\\lambda)$ on $[\\lambda_{\\min}, \\lambda_{\\max}]$ is given by the reciprocal of the denominator:\n$$ \\max_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} |P_k(\\lambda)| = \\frac{1}{\\left|T_k\\left(\\frac{\\lambda_{\\max}+\\lambda_{\\min}}{\\lambda_{\\max}-\\lambda_{\\min}}\\right)\\right|} = \\frac{1}{T_k\\left(\\frac{\\kappa+1}{\\kappa-1}\\right)} $$\nwhere $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ is the condition number of $A$. The argument is greater than $1$, so $T_k$ is positive. For $z > 1$, $T_k(z) = \\cosh(k \\cdot \\arccosh(z))$. Let $z_0 = \\frac{\\kappa+1}{\\kappa-1}$. Then $\\arccosh(z_0) = \\ln(z_0 + \\sqrt{z_0^2-1})$.\n$$ z_0^2 - 1 = \\left(\\frac{\\kappa+1}{\\kappa-1}\\right)^2 - 1 = \\frac{(\\kappa^2+2\\kappa+1) - (\\kappa^2-2\\kappa+1)}{(\\kappa-1)^2} = \\frac{4\\kappa}{(\\kappa-1)^2} $$\n$$ \\sqrt{z_0^2 - 1} = \\frac{2\\sqrt{\\kappa}}{\\kappa-1} $$\n$$ z_0 + \\sqrt{z_0^2 - 1} = \\frac{\\kappa+1+2\\sqrt{\\kappa}}{\\kappa-1} = \\frac{(\\sqrt{\\kappa}+1)^2}{(\\sqrt{\\kappa}-1)(\\sqrt{\\kappa}+1)} = \\frac{\\sqrt{\\kappa}+1}{\\sqrt{\\kappa}-1} $$\nSo $\\arccosh(z_0) = \\ln\\left(\\frac{\\sqrt{\\kappa}+1}{\\sqrt{\\kappa}-1}\\right)$. Using $T_k(z_0) = \\cosh(k \\ln(\\dots))$, and the definition $\\cosh(u) = (e^u + e^{-u})/2$:\n$$ T_k\\left(\\frac{\\kappa+1}{\\kappa-1}\\right) = \\frac{1}{2}\\left[ \\left(\\frac{\\sqrt{\\kappa}+1}{\\sqrt{\\kappa}-1}\\right)^k + \\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^k \\right] $$\nSince the second term is positive, we can establish a useful lower bound: $T_k\\left(\\frac{\\kappa+1}{\\kappa-1}\\right) > \\frac{1}{2}\\left(\\frac{\\sqrt{\\kappa}+1}{\\sqrt{\\kappa}-1}\\right)^k$.\nPutting everything together, the error bound on $\\|e_k\\|_A$ is:\n$$ \\|e_k\\|_A \\le \\frac{1}{T_k\\left(\\frac{\\kappa+1}{\\kappa-1}\\right)} \\|e_0\\|_A < 2 \\left( \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1} \\right)^k \\|e_0\\|_A $$\nThe final bound, conventionally expressed with the constant $2$, is:\n$$ \\|e_k\\|_A \\le 2 \\left( \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1} \\right)^k \\|e_0\\|_A $$\n\n(c) The asymptotic per-iteration convergence factor $\\rho(\\kappa)$ is defined by the inequality $\\| e_k \\|_{A} \\le C \\, \\rho(\\kappa)^{\\,k} \\, \\| e_0 \\|_{A}$ for some constant $C$ independent of $k$. By comparing this definition with the bound derived in part (b),\n$$ \\|e_k\\|_A \\le 2 \\left( \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1} \\right)^k \\|e_0\\|_A $$\nwe can directly identify the constant $C=2$ and the convergence factor $\\rho(\\kappa)$. The convergence factor is the base of the exponential term that governs the rate of error reduction per iteration. Therefore, the expression for $\\rho(\\kappa)$ is:\n$$ \\rho(\\kappa) = \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1} $$\nThis expression quantifies how the convergence rate of the CG algorithm deteriorates as the condition number $\\kappa$ of the matrix $A$ increases. For $\\kappa \\gg 1$, we have $\\rho(\\kappa) \\approx 1 - 2/\\sqrt{\\kappa}$, indicating that the number of iterations required to achieve a certain accuracy scales roughly with $\\sqrt{\\kappa}$.",
            "answer": "$$\\boxed{\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}}$$"
        },
        {
            "introduction": "The validity of the Hybrid Monte Carlo algorithm rests on the time-reversibility of its molecular dynamics integration. However, the use of iterative solvers for the fermion force introduces an inexactness that can break this fundamental symmetry and lead to a drift in the Hamiltonian. This practice  offers a direct look at this crucial issue by having you implement a leapfrog integrator and perform a reversibility test, allowing you to quantify how solver tolerance impacts both reversibility and energy conservation.",
            "id": "3571194",
            "problem": "You are to implement and validate a reversibility test for the Hybrid Monte Carlo (HMC) molecular dynamics integrator used in Lattice Quantum Chromodynamics (Lattice QCD) by simulating a compact gauge field and a toy pseudofermion sector that requires an iterative linear solver. The goal is to quantify the breakdown of exact reversibility and the microcanonical energy drift as functions of the solver residual tolerance. Your program must be a complete, runnable program that carries out the molecular dynamics evolution forward and backward in time, evaluates the microcanonical energy drift, and reports reversibility errors for a small, fully specified test suite. All angles must be in radians.\n\nStart from the following fundamental base. The HMC method for Lattice Quantum Chromodynamics introduces an auxiliary canonical momentum for each gauge degree of freedom and integrates Hamilton’s equations for a fictitious molecular dynamics time. For a compact abelian gauge group, consider a two-dimensional periodic lattice of size $L_x \\times L_y$ with $L_x = 2$ and $L_y = 2$. Each link variable is a compact angle $\\theta_{x,\\mu} \\in (-\\pi,\\pi]$ representing a $\\mathrm{U}(1)$ link $U_{x,\\mu} = e^{i \\theta_{x,\\mu}}$, where $\\mu \\in \\{0,1\\}$ denotes the $x$- and $y$-directions, respectively. Introduce canonical momenta $p_{x,\\mu} \\in \\mathbb{R}$ conjugate to $\\theta_{x,\\mu}$. The Hamiltonian is\n$$\nH(\\theta,p) = T(p) + S_g(\\theta) + V_f(\\theta),\n$$\nwhere $T(p) = \\tfrac{1}{2} \\sum_{x,\\mu} p_{x,\\mu}^2$ is the kinetic term, $S_g(\\theta)$ is the Wilson pure gauge action for $\\mathrm{U}(1)$,\n$$\nS_g(\\theta) = -\\beta \\sum_{x} \\cos \\phi_x,\n$$\nwith plaquette angle $\\phi_x = \\theta_{x,0} + \\theta_{x+\\hat{0},1} - \\theta_{x,1} - \\theta_{x+\\hat{1},0}$ and periodic boundary conditions, and $V_f(\\theta)$ is a toy pseudofermion potential constructed to be positive-definite and to require a linear solve at each force evaluation:\n$$\nV_f(\\theta) = \\tfrac{1}{2} \\, b^\\top A(\\theta)^{-1} b, \\quad A(\\theta) = I + c_0 \\sum_{\\ell=1}^{N_\\ell} \\theta_\\ell^2 \\, B_\\ell.\n$$\nHere $I$ is the $n \\times n$ identity matrix with $n = 12$, $N_\\ell = 8$ is the number of oriented links on the $2 \\times 2$ lattice, $c_0 = 0.1$ is a fixed positive scalar, and each $B_\\ell$ is a symmetric positive semidefinite matrix defined as $B_\\ell = v_\\ell v_\\ell^\\top$ for a random vector $v_\\ell \\in \\mathbb{R}^n$. The vector $b \\in \\mathbb{R}^n$ is a fixed random vector. The link index $\\ell$ is ordered lexicographically by $(x,y,\\mu)$ with $x \\in \\{0,1\\}$ outermost, then $y \\in \\{0,1\\}$, then direction $\\mu \\in \\{0,1\\}$.\n\nMolecular dynamics is performed with the time-reversible, symplectic leapfrog integrator. If the forces are computed exactly, the map is area-preserving and reversible, and the microcanonical Hamiltonian $H$ is conserved up to integration errors controlled by the step size. In practice, for Lattice Quantum Chromodynamics with dynamical fermions, the fermion force requires solving a large linear system, which is typically done with the Conjugate Gradient method and a stopping tolerance. An inexact solve breaks the exact reversibility and can cause an increase in the absolute microcanonical energy drift.\n\nYour task is to:\n\n- Implement the leapfrog integrator for the compact $\\mathrm{U}(1)$ gauge field $\\theta_{x,\\mu}$ and canonical momenta $p_{x,\\mu}$, using Hamilton’s equations $\\dot{\\theta}_{x,\\mu} = \\partial H / \\partial p_{x,\\mu}$ and $\\dot{p}_{x,\\mu} = - \\partial H / \\partial \\theta_{x,\\mu}$. Use half-step momentum updates and full-step link updates per the standard leapfrog scheme.\n- Use exact analytical gauge forces from the Wilson action on the $2 \\times 2$ lattice, with periodic boundaries. Angles must be wrapped back to $(-\\pi,\\pi]$ after each update.\n- For the fermion-like force term, construct $A(\\theta)$ as above and use the Conjugate Gradient method to compute an approximate solution $x \\approx A(\\theta)^{-1} b$ with a specified solver tolerance $\\epsilon$ based on the norm of the residual. Do not precondition. Use the same $x$ to compute the fermion force consistently throughout a given force evaluation.\n- The microcanonical energy $H(\\theta,p)$ must be evaluated with high accuracy using a direct dense linear solve for $A(\\theta)^{-1}$ (that is, do not use the iterative solver for the energy). This separates force inexactness from energy evaluation.\n- Perform a reversibility test: from a fixed initial condition $(\\theta^{(0)}, p^{(0)})$, integrate forward for $N_{\\text{steps}}$ steps of size $\\delta \\tau$, then reverse the momenta $p \\to -p$ and integrate for $N_{\\text{steps}}$ steps of size $\\delta \\tau$ again. Compare $(\\theta^{(b)}, p^{(b)})$ to $(\\theta^{(0)}, -p^{(0)})$ after the backward leg. Measure the maximum absolute wrapped angle difference and the maximum absolute momentum difference.\n- Quantify the absolute microcanonical energy drift after the forward leg, $\\Delta H = H(\\theta^{(f)}, p^{(f)}) - H(\\theta^{(0)}, p^{(0)})$, where $(\\theta^{(f)}, p^{(f)})$ is the state after the forward leg.\n\nInitialization and data:\n\n- Use a single random number generator seed $s = 314159$ to make the experiment reproducible. Draw data in the following order: the vectors $v_\\ell \\in \\mathbb{R}^{n}$ for each $\\ell = 1,\\dots,N_\\ell$, the vector $b \\in \\mathbb{R}^{n}$, the initial links $\\theta^{(0)}_{x,\\mu}$, and the initial momenta $p^{(0)}_{x,\\mu}$.\n- Each component of $v_\\ell$ and $b$ is independently sampled from a normal distribution with mean $0$ and variance $1$.\n- Each initial link angle $\\theta^{(0)}_{x,\\mu}$ is sampled independently from a normal distribution with mean $0$ and variance $\\sigma^2$ with $\\sigma = 0.3$, then wrapped into $(-\\pi,\\pi]$. Each initial momentum $p^{(0)}_{x,\\mu}$ is sampled independently from a normal distribution with mean $0$ and variance $1$.\n\nIntegrator and action parameters:\n\n- Lattice sizes $L_x = 2$, $L_y = 2$.\n- Gauge coupling $\\beta = 3.0$.\n- Fermion matrix parameter $c_0 = 0.1$.\n- Pseudofermion dimension $n = 12$.\n- Leapfrog step size $\\delta \\tau = 0.1$ and number of steps $N_{\\text{steps}} = 10$.\n\nThe Conjugate Gradient solver stopping criterion must be based on the relative residual $\\|r_k\\|_2 / \\|b\\|_2 \\le \\epsilon$, with a maximum of $1000$ iterations. The linear operator is the dense symmetric positive-definite matrix $A(\\theta)$. No preconditioning is allowed.\n\nTest suite:\n\nRun the entire forward-backward reversibility protocol for the following four solver tolerances $\\epsilon$ while keeping all other parameters fixed to the values above:\n\n- $\\epsilon = 10^{-12}$.\n- $\\epsilon = 10^{-8}$.\n- $\\epsilon = 10^{-5}$.\n- $\\epsilon = 10^{-3}$.\n\nFor each solver tolerance $\\epsilon$, compute and report:\n\n- The absolute microcanonical energy drift after the forward leg, $|\\Delta H|$, computed with $H$ evaluated using a dense direct solve for $A(\\theta)^{-1}$.\n- The maximum absolute wrapped angle difference after the round trip, $E_\\theta = \\max_{\\ell} \\left| \\operatorname{wrap}\\left(\\theta_\\ell^{(b)} - \\theta_\\ell^{(0)}\\right) \\right|$, with angles wrapped into $(-\\pi,\\pi]$.\n- The maximum absolute momentum difference after the round trip, $E_p = \\max_{\\ell} \\left| p_\\ell^{(b)} + p_\\ell^{(0)} \\right|$.\n\nAngle unit specification: all angles are in radians.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a three-element list $[|\\Delta H|, E_\\theta, E_p]$. All floating-point numbers must be formatted in scientific notation with eight digits after the decimal point. For example, the output must look like\n$$\n\\big[ [a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_4] \\big],\n$$\nbut with actual numbers and no spaces. The output must be a single line and nothing else.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded, well-posed, objective, and provides a complete and consistent set of specifications for a numerical experiment in computational physics. The model, while simplified, is based on established principles of Lattice Quantum Chromodynamics (Lattice QCD) and Hamiltonian dynamics. The task involves implementing a standard Hybrid Monte Carlo (HMC) integrator and performing a reversibility test, which is a routine and meaningful diagnostic procedure in this field. All parameters, initial conditions, and procedures are explicitly defined, making the problem fully determined and verifiable.\n\nThe solution proceeds by first constructing the necessary components of the simulation—the Hamiltonian, its constituent potential and kinetic terms, and the forces derived from the potential—and then integrating the equations of motion using the specified leapfrog algorithm. The core of the task is to observe how inexactness in the force calculation, arising from the iterative solution of a linear system, affects a fundamental property of the integrator: time reversibility.\n\nThe Hamiltonian for the system is given by\n$$\nH(\\theta,p) = T(p) + S_g(\\theta) + V_f(\\theta)\n$$\nwhere $\\theta = \\{\\theta_{x,\\mu}\\}$ are the compact link angles on a $L_x \\times L_y = 2 \\times 2$ lattice and $p = \\{p_{x,\\mu}\\}$ are their conjugate momenta. The total number of links is $N_\\ell = L_x \\times L_y \\times 2 = 8$.\n\nThe kinetic energy is $T(p) = \\tfrac{1}{2} \\sum_{\\ell=1}^{N_\\ell} p_\\ell^2$. The potential energy is composed of a pure gauge part $S_g(\\theta)$ and a pseudofermion part $V_f(\\theta)$.\n\nThe gauge action is the Wilson action for a $\\mathrm{U}(1)$ gauge group:\n$$\nS_g(\\theta) = -\\beta \\sum_{x} \\cos \\phi_x\n$$\nwhere $\\beta=3.0$ is the coupling constant and the sum is over all $4$ lattice sites $x=(i_x, i_y)$. The plaquette angle $\\phi_x$ at site $x$ is defined as\n$$\n\\phi_x = \\theta_{x,0} + \\theta_{x+\\hat{0},1} - \\theta_{x,1} - \\theta_{x+\\hat{1},0}\n$$\nwith periodic boundary conditions, where $\\mu=0$ denotes the x-direction and $\\mu=1$ denotes the y-direction. The force due to this action on a link variable $\\theta_{x,\\mu}$ is $F_{g,x,\\mu} = -\\partial S_g / \\partial \\theta_{x,\\mu}$. A given link $\\theta_{x,\\mu}$ contributes to two plaquettes. For a link $\\theta_{x,0}$ at site $x=(i_x,i_y)$, its contribution to the force is\n$$\nF_{g,x,0} = -\\beta (\\sin \\phi_x - \\sin \\phi_{x-\\hat{1}})\n$$\nwhere $x-\\hat{1}$ is the site $(i_x, (i_y-1) \\pmod{L_y})$. For a link $\\theta_{x,1}$, the force is\n$$\nF_{g,x,1} = -(-\\sin \\phi_x + \\sin \\phi_{x-\\hat{0}})\n$$\nwhere $x-\\hat{0}$ is the site $((i_x-1) \\pmod{L_x}, i_y)$.\n\nThe pseudofermion potential $V_f(\\theta)$ mimics the effect of a fermion determinant and is the source of the computational complexity and inexactness. It is defined as\n$$\nV_f(\\theta) = \\tfrac{1}{2} \\, b^\\top A(\\theta)^{-1} b\n$$\nwhere $b \\in \\mathbb{R}^n$ (with $n=12$) is a fixed random vector, and $A(\\theta)$ is an $n \\times n$ matrix that depends on the gauge field configuration $\\theta$:\n$$\nA(\\theta) = I + c_0 \\sum_{\\ell=1}^{N_\\ell} \\theta_\\ell^2 \\, B_\\ell\n$$\nHere, $c_0=0.1$ is a constant, $\\theta_\\ell$ is the $\\ell$-th link angle in a flattened lexicographical ordering, and $B_\\ell = v_\\ell v_\\ell^\\top$ are fixed symmetric positive semidefinite matrices constructed from random vectors $v_\\ell \\in \\mathbb{R}^n$. Since $I$ is positive definite and each $c_0 \\theta_\\ell^2 B_\\ell$ is positive semidefinite, the matrix $A(\\theta)$ is guaranteed to be symmetric and positive definite (SPD), a crucial property for the stability of the Conjugate Gradient (CG) algorithm.\n\nThe pseudofermion force $F_{f,\\ell} = -\\partial V_f / \\partial \\theta_\\ell$ is derived using matrix calculus. By letting $x(\\theta) = A(\\theta)^{-1}b$, we have $V_f(\\theta) = \\tfrac{1}{2} b^\\top x(\\theta)$. The derivative is:\n$$\n\\frac{\\partial V_f}{\\partial \\theta_\\ell} = -\\frac{1}{2} b^\\top A^{-1} \\frac{\\partial A}{\\partial \\theta_\\ell} A^{-1} b = -\\frac{1}{2} x^\\top \\left( 2 c_0 \\theta_\\ell B_\\ell \\right) x = -c_0 \\theta_\\ell \\, (x^\\top B_\\ell x)\n$$\nThus, the force is\n$$\nF_{f,\\ell} = c_0 \\theta_\\ell \\, (x^\\top B_\\ell x)\n$$\nCalculation of this force requires finding $x = A(\\theta)^{-1}b$. This is performed using the CG algorithm, which is halted when the relative residual norm $\\|A x_k - b\\|_2 / \\|b\\|_2$ drops below a specified tolerance $\\epsilon$. The inexactness of this solution is the central focus of the problem.\n\nThe system is evolved using a standard second-order leapfrog integrator, which is time-reversible and symplectic when forces are calculated exactly. The integration for a single step $\\delta\\tau=0.1$ proceeds as:\n1. Update momenta by a half step: $p \\to p + F(\\theta) \\frac{\\delta\\tau}{2}$.\n2. Update positions by a full step: $\\theta \\to \\theta + p \\, \\delta\\tau$. Angles are then wrapped into the interval $(-\\pi, \\pi]$.\n3. Update momenta by a final half step: $p \\to p + F(\\theta_{new}) \\frac{\\delta\\tau}{2}$.\nThis process is repeated for $N_{\\text{steps}}=10$.\n\nThe reversibility test consists of a forward-in-time trajectory of $N_{\\text{steps}}$, followed by a momentum reversal $p \\to -p$, and a backward-in-time trajectory of $N_{\\text{steps}}$. The final state $(\\theta^{(b)}, p^{(b)})$ is compared to the initial state $(\\theta^{(0)}, -p^{(0)})$. Three quantities are measured for each solver tolerance $\\epsilon$:\n1.  **Energy Drift $|\\Delta H|$**: The absolute change in the Hamiltonian, $|H(\\theta^{(f)}, p^{(f)}) - H(\\theta^{(0)}, p^{(0)})|$, after the forward trajectory. This is a measure of energy conservation. For this calculation, $H$ is computed with a high-accuracy direct linear solve for $A(\\theta)^{-1}$ to isolate integration errors from energy measurement errors.\n2.  **Angle Reversibility Error $E_\\theta$**: The maximum absolute deviation of the final angles from the initial angles, $\\max_\\ell |\\operatorname{wrap}(\\theta_\\ell^{(b)} - \\theta_\\ell^{(0)})|$.\n3.  **Momentum Reversibility Error $E_p$**: The maximum absolute deviation of the final momenta from the negated initial momenta, $\\max_\\ell |p_\\ell^{(b)} + p_\\ell^{(0)}|$.\n\nThis numerical experiment is performed for a test suite of tolerances $\\epsilon \\in \\{10^{-12}, 10^{-8}, 10^{-5}, 10^{-3}\\}$, demonstrating how a looser (larger $\\epsilon$) solver tolerance leads to a more significant breakdown of reversibility and energy conservation, as expected. All random numbers are generated from a fixed seed for reproducibility.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a reversibility test for a Hybrid Monte Carlo integrator\n    on a toy U(1) lattice gauge theory model.\n    \"\"\"\n    # Problem Parameters\n    L_X, L_Y = 2, 2\n    N_LINKS = L_X * L_Y * 2\n    BETA = 3.0\n    C0 = 0.1\n    N_PF = 12\n    DT = 0.1\n    N_STEPS = 10\n    SIGMA_THETA = 0.3\n    SEED = 314159\n    CG_MAX_ITER = 1000\n    TEST_TOLERANCES = [1e-12, 1e-8, 1e-5, 1e-3]\n\n    # --- Initialization ---\n    rng = np.random.default_rng(SEED)\n\n    # Generate fixed random data for the pseudofermion sector\n    # Vectors v_l used to construct matrices B_l\n    v_vectors = rng.normal(size=(N_LINKS, N_PF))\n    # Matrices B_l = v_l v_l^T\n    B_matrices = np.array([np.outer(v, v) for v in v_vectors])\n    # Fixed pseudofermion source vector b\n    b_vector = rng.normal(size=N_PF)\n    b_norm = np.linalg.norm(b_vector)\n\n    # Generate fixed initial conditions\n    theta_initial = (rng.normal(scale=SIGMA_THETA, size=(L_X, L_Y, 2)) + np.pi) % (2 * np.pi) - np.pi\n    p_initial = rng.normal(size=(L_X, L_Y, 2))\n\n    # --- Helper Functions ---\n    def wrap_angle(angles):\n        \"\"\"Wraps angles to the interval (-pi, pi].\"\"\"\n        return (angles + np.pi) % (2 * np.pi) - np.pi\n\n    def get_plaquettes(theta):\n        \"\"\"Computes plaquette angles for a given gauge field configuration.\"\"\"\n        plaquettes = np.zeros((L_X, L_Y))\n        for ix in range(L_X):\n            for iy in range(L_Y):\n                # phi_x = theta_{x,0} + theta_{x+0,1} - theta_{x,1} - theta_{x+1,0}\n                # with periodic boundary conditions\n                plaquettes[ix, iy] = (\n                    theta[ix, iy, 0]\n                    + theta[(ix + 1) % L_X, iy, 1]\n                    - theta[ix, iy, 1]\n                    - theta[ix, (iy + 1) % L_Y, 0]\n                )\n        return plaquettes\n\n    def cg_solve(A, b, b_norm_val, rel_tol):\n        \"\"\"Solves Ax = b using Conjugate Gradient for a symmetric positive-definite A.\"\"\"\n        x = np.zeros_like(b)\n        r = b.copy()\n        p = r.copy()\n        rs_old = r @ r\n        \n        abs_tol_sq = (rel_tol * b_norm_val)**2\n        if rs_old  abs_tol_sq:\n            return x\n\n        for _ in range(CG_MAX_ITER):\n            Ap = A @ p\n            alpha = rs_old / (p @ Ap)\n            x += alpha * p\n            r -= alpha * Ap\n            rs_new = r @ r\n            if rs_new  abs_tol_sq:\n                break\n            p = r + (rs_new / rs_old) * p\n            rs_old = rs_new\n        return x\n\n    # --- Physics and Integrator Functions ---\n    def compute_A_matrix(theta_flat):\n        \"\"\"Computes the pseudofermion matrix A(theta).\"\"\"\n        A = np.identity(N_PF)\n        theta_sq = theta_flat**2\n        # Sum over links: c0 * theta_l^2 * B_l\n        A += C0 * np.einsum('l,lij->ij', theta_sq, B_matrices)\n        return A\n        \n    def total_force(theta, epsilon):\n        \"\"\"Computes the total force F = -dV/dtheta.\"\"\"\n        # Gauge force\n        plaquettes = get_plaquettes(theta)\n        force_g = np.zeros_like(theta)\n        for ix in range(L_X):\n            for iy in range(L_Y):\n                # F_g,x,0 = -beta * (sin(phi_x) - sin(phi_{x-y_hat}))\n                force_g[ix, iy, 0] = -BETA * (\n                    np.sin(plaquettes[ix, iy]) - np.sin(plaquettes[ix, (iy - 1) % L_Y])\n                )\n                # F_g,x,1 = -(-sin(phi_x) + sin(phi_{x-x_hat}))\n                force_g[ix, iy, 1] = -BETA * (\n                    -np.sin(plaquettes[ix, iy]) + np.sin(plaquettes[(ix - 1) % L_X, iy])\n                )\n        \n        # Pseudofermion force\n        theta_flat = theta.flatten()\n        A = compute_A_matrix(theta_flat)\n        x_sol = cg_solve(A, b_vector, b_norm, epsilon)\n        \n        # F_f,l = c0 * theta_l * (x^T B_l x)\n        xT_B = np.einsum('i,lij->lj', x_sol, B_matrices)\n        xT_B_x = np.einsum('lj,j->l', xT_B, x_sol)\n        force_f_flat = C0 * theta_flat * xT_B_x\n        \n        force_total = force_g.flatten() + force_f_flat\n        return force_total.reshape(L_X, L_Y, 2)\n\n    def calculate_hamiltonian(theta, p):\n        \"\"\"Calculates the Hamiltonian with high precision.\"\"\"\n        # Kinetic energy\n        T = 0.5 * np.sum(p**2)\n        \n        # Gauge action\n        plaquettes = get_plaquettes(theta)\n        S_g = -BETA * np.sum(np.cos(plaquettes))\n        \n        # Pseudofermion potential (high precision solve)\n        theta_flat = theta.flatten()\n        A = compute_A_matrix(theta_flat)\n        x_sol_exact = np.linalg.solve(A, b_vector)\n        V_f = 0.5 * (b_vector @ x_sol_exact)\n        \n        return T + S_g + V_f\n\n    def run_trajectory(theta_start, p_start, epsilon):\n        \"\"\"Integrates the system for N_steps using the leapfrog algorithm.\"\"\"\n        theta, p = theta_start.copy(), p_start.copy()\n        \n        # Standard leapfrog: p(t+dt/2), theta(t+dt), p(t+dt)\n        force_current = total_force(theta, epsilon)\n        p += 0.5 * DT * force_current\n        for _ in range(N_STEPS - 1):\n            theta = wrap_angle(theta + DT * p)\n            force_current = total_force(theta, epsilon)\n            p += DT * force_current\n        \n        # Final full theta step and half p step\n        theta = wrap_angle(theta + DT * p)\n        force_current = total_force(theta, epsilon)\n        p += 0.5 * DT * force_current\n        \n        return theta, p\n\n    # --- Main Test Loop ---\n    results = []\n    H_initial = calculate_hamiltonian(theta_initial, p_initial)\n\n    for epsilon in TEST_TOLERANCES:\n        # Forward trajectory\n        theta_fwd, p_fwd = run_trajectory(theta_initial, p_initial, epsilon)\n        \n        # Energy drift\n        H_final_fwd = calculate_hamiltonian(theta_fwd, p_fwd)\n        delta_H = np.abs(H_final_fwd - H_initial)\n        \n        # Backward trajectory\n        theta_bwd, p_bwd = run_trajectory(theta_fwd, -p_fwd, epsilon)\n        \n        # Reversibility errors\n        E_theta = np.max(np.abs(wrap_angle(theta_bwd - theta_initial)))\n        E_p = np.max(np.abs(p_bwd + p_initial))\n        \n        results.append([delta_H, E_theta, E_p])\n\n    # --- Format and Print Output ---\n    output_str = \",\".join(\n        f\"[{res[0]:.8e},{res[1]:.8e},{res[2]:.8e}]\" for res in results\n    )\n    print(f\"[{output_str}]\")\n\n\nsolve()\n\n```"
        },
        {
            "introduction": "Data from Markov Chain Monte Carlo simulations are not independent; they exhibit autocorrelations that must be properly handled to obtain reliable error estimates. This is especially true for nonlinear functions of the data, such as effective masses. This computational exercise  guides you through building a complete data analysis pipeline, using a binning and jackknife procedure on synthetic data to see empirically how the estimated error saturates, providing a robust method for uncertainty quantification.",
            "id": "3571154",
            "problem": "You are to construct a complete, runnable program that implements a binning plus jackknife procedure for a nonlinear estimator commonly used in Lattice Quantum Chromodynamics (Lattice QCD), the effective mass. The goal is to connect the statistical behavior of Markov Chain Monte Carlo data, which are typically autocorrelated, to how binning and the jackknife estimators behave as the bin size increases. Your task is to begin from first principles of Markov Chain Monte Carlo and autocorrelation, implement a scientifically sound synthetic data generator that mimics a Euclidean two-point correlator, and then build the binning plus jackknife estimator pipeline for the effective mass. You must then quantify whether the estimated jackknife error saturates as a function of bin size for various autocorrelation scenarios.\n\nBegin from the following fundamental base, which you must use to justify your design and reasoning:\n\n- Markov Chain Monte Carlo generates a time-ordered sequence of samples with an autocorrelation function $C_{\\mathcal{O}}(k)$ for an observable $\\mathcal{O}$ measured at Monte Carlo time separation $k$, and an integrated autocorrelation time $\\tau_{\\text{int}} = \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\rho_{\\mathcal{O}}(k)$, where $\\rho_{\\mathcal{O}}(k) = C_{\\mathcal{O}}(k)/C_{\\mathcal{O}}(0)$.\n- For correlated samples, the variance of the sample mean scales as $\\sigma^2_{\\bar{\\mathcal{O}}} \\approx \\frac{\\sigma^2_{\\mathcal{O}}}{N} \\, 2 \\, \\tau_{\\text{int}}$ in the limit of large number of samples $N$.\n- Binning adjacent samples into non-overlapping bins of size $B$ reduces autocorrelation between bin means. If $B \\gg \\tau_{\\text{int}}$, the bin means are approximately independent and identically distributed.\n- The delete-one-bin jackknife applied to a scalar estimator $\\theta$ computed from bin-averaged data yields a consistent estimator for the variance of $\\theta$, even for nonlinear $\\theta$.\n\nYou must implement the following components:\n\n1) Synthetic model of a Euclidean two-point correlator at two adjacent Euclidean times. For a fixed time slice $t_0$, assume a single-state correlator model with configuration-dependent amplitude. Let the true correlator be $G(t) = A \\, e^{-m t}$ with mass $m$ and a positive amplitude $A$ that fluctuates across configurations due to gauge noise. The measured correlator at Monte Carlo configuration index $n$ is:\n$$\nC_t^{(n)} = A^{(n)} e^{-m t_0} \\, \\epsilon_t^{(n)}, \\qquad C_{t+1}^{(n)} = A^{(n)} e^{-m (t_0+1)} \\, \\epsilon_{t+1}^{(n)},\n$$\nwhere $A^{(n)}$ is a positive, correlated random process along $n$ and $\\epsilon_t^{(n)}$, $\\epsilon_{t+1}^{(n)}$ are small, independent, mean-one multiplicative noises. Model $\\ln A^{(n)}$ as an autoregressive process of order one (AR($1$)) with coefficient $\\phi$, that is,\n$$\nX_n = \\phi X_{n-1} + \\eta_n, \\quad \\eta_n \\sim \\mathcal{N}(0, \\sigma_\\eta^2), \\quad A^{(n)} = A_0 \\, e^{s X_n},\n$$\nwith the stationary normalization $\\sigma_\\eta^2 = 1 - \\phi^2$ so that $\\operatorname{Var}(X_n) = 1$ for $|\\phi| lt; 1$. The multiplicative measurement noises satisfy $\\epsilon \\sim 1 + \\mathcal{N}(0, \\sigma_{\\text{noise}}^2)$ with small $\\sigma_{\\text{noise}}$ and must be enforced to keep strictly positive correlators.\n\n2) Nonlinear estimator of the effective mass at $t_0$:\n$$\nm_{\\text{eff}}(t_0) = \\ln \\left( \\frac{\\langle C_t \\rangle}{\\langle C_{t+1} \\rangle} \\right),\n$$\nwhere $\\langle \\cdot \\rangle$ denotes the ensemble mean over configurations. This is a nonlinear function of the data since it involves a ratio and a logarithm.\n\n3) Binning and delete-one-bin jackknife variance estimation. Partition the time-ordered configurations into $N_{\\text{bin}}$ non-overlapping, contiguous bins of equal size $B$. Let $\\widehat{m}_{\\text{eff}}^{(-i)}$ be the effective mass estimator computed from all bins except bin $i$ (delete-one-bin jackknife replicates). Use these replicates to construct the jackknife estimate of the standard error for $m_{\\text{eff}}(t_0)$ at bin size $B$. You must ensure that only bin sizes with at least $8$ bins (that is, $N_{\\text{bin}} \\ge 8$) are used to produce a jackknife error.\n\n4) Saturation detection criterion. Consider a geometric sequence of bin sizes $B \\in \\{ 1, 2, 4, \\dots \\}$ up to the largest $B$ for which there are at least $8$ bins. Let $\\sigma(B)$ denote the jackknife standard error at bin size $B$. Define the relative change at the two largest admissible bin sizes $B_{K-1}$ and $B_K$ as\n$$\nr = \\frac{|\\sigma(B_K) - \\sigma(B_{K-1})|}{\\max(\\sigma(B_K), \\varepsilon)},\n$$\nwith a small regularizer $\\varepsilon$ to avoid division by zero. Declare that the error has saturated if $r lt; \\delta$ for a specified tolerance $\\delta$.\n\nYour program must generate the synthetic data, apply the binning plus jackknife procedure, and evaluate the saturation criterion for each of the following test cases. In all cases, use the same exponential mass and noise parameters, and vary only the sequence length and AR($1$) coefficient:\n\n- Test case $1$ (happy path, moderate autocorrelation): seed $= 12345$, $N = 4096$, $\\phi = 0.8$, $A_0 = 1.0$, $s = 0.2$, $m = 0.5$, $t_0 = 5$, $\\sigma_{\\text{noise}} = 0.02$, detection tolerance $\\delta = 0.07$.\n- Test case $2$ (nearly uncorrelated): seed $= 54321$, $N = 4096$, $\\phi = 0.0$, $A_0 = 1.0$, $s = 0.2$, $m = 0.5$, $t_0 = 5$, $\\sigma_{\\text{noise}} = 0.02$, detection tolerance $\\delta = 0.07$.\n- Test case $3$ (strong autocorrelation, no saturation within available bins): seed $= 999$, $N = 1024$, $\\phi = 0.995$, $A_0 = 1.0$, $s = 0.2$, $m = 0.5$, $t_0 = 5$, $\\sigma_{\\text{noise}} = 0.02$, detection tolerance $\\delta = 0.07$.\n- Test case $4$ (boundary condition with fewer samples but sufficient bins): seed $= 2024$, $N = 256$, $\\phi = 0.9$, $A_0 = 1.0$, $s = 0.2$, $m = 0.5$, $t_0 = 5$, $\\sigma_{\\text{noise}} = 0.02$, detection tolerance $\\delta = 0.07$.\n\nYour program must:\n\n- Simulate the data exactly as described above for each test case, using the specified seeds.\n- Enumerate bin sizes $B \\in \\{ 1, 2, 4, \\dots \\}$ such that the number of whole bins $N_{\\text{bin}} = \\lfloor N / B \\rfloor$ satisfies $N_{\\text{bin}} \\ge 8$. Discard any leftover configurations at the end that do not fill a complete bin.\n- Compute the jackknife standard error $\\sigma(B)$ for each admissible $B$.\n- Determine whether the error has saturated at the largest bin size according to the criterion with tolerance $\\delta$ stated above, using $\\varepsilon = 10^{-16}$.\n- Produce, as the only output, a single line containing a list of booleans for the four test cases in order $[1,2,3,4]$, where each boolean is $true$ if saturation is detected and $false$ otherwise. Use the exact string representations $True$ and $False$.\n\nNo physical units are involved in the final outputs, which must be unitless booleans. Angles do not appear. Percentages do not appear. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$).",
            "solution": "We proceed by integrating the statistical physics of Markov Chain Monte Carlo with a concrete algorithmic construction for binning and jackknife estimation of a nonlinear observable relevant to Lattice Quantum Chromodynamics.\n\nFirst, we identify the fundamental statistical base. In Markov Chain Monte Carlo, sequential measurements $\\{\\mathcal{O}_n\\}_{n=1}^N$ typically possess nonzero autocorrelation, encoded by the autocovariance $C_{\\mathcal{O}}(k) = \\mathbb{E}\\left[(\\mathcal{O}_n - \\mu)(\\mathcal{O}_{n+k} - \\mu)\\right]$. The normalized autocorrelation function is $\\rho_{\\mathcal{O}}(k) = C_{\\mathcal{O}}(k)/C_{\\mathcal{O}}(0)$. The integrated autocorrelation time,\n$$\n\\tau_{\\text{int}} = \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\rho_{\\mathcal{O}}(k),\n$$\ndetermines the variance inflation of the sample mean, a well-tested fact: for large $N$,\n$$\n\\operatorname{Var}(\\bar{\\mathcal{O}}) \\approx \\frac{\\sigma^2_{\\mathcal{O}}}{N} \\, 2 \\, \\tau_{\\text{int}}.\n$$\nThis shows that correlations effectively reduce the number of independent samples by approximately a factor $2 \\tau_{\\text{int}}$.\n\nSecond, we implement binning to reduce correlations. Grouping adjacent samples into non-overlapping bins of size $B$ forms $N_{\\text{bin}} = \\lfloor N/B \\rfloor$ bin averages. If $B \\gg \\tau_{\\text{int}}$, the bin means are approximately independent and identically distributed. Thus, estimators computed from the bin means will have errors that no longer change significantly with further increases in $B$. This plateau or saturation in the estimated error as a function of $B$ is an empirical indicator that $B$ exceeds the correlation scale, yielding decorrelated effective samples.\n\nThird, we define the nonlinear estimator, the effective mass at a fixed Euclidean time $t_0$:\n$$\nm_{\\text{eff}}(t_0) = \\ln \\left( \\frac{\\langle C_t \\rangle}{\\langle C_{t+1} \\rangle} \\right),\n$$\nwhere $C_t$ and $C_{t+1}$ denote correlator measurements at times $t_0$ and $t_0 + 1$ respectively, and $\\langle \\cdot \\rangle$ is an ensemble average. This estimator is nonlinear in the data due to the ratio and logarithm, thus requiring resampling techniques that respect nonlinearity to estimate its uncertainty robustly.\n\nTo propagate errors through this nonlinear estimator, we employ the delete-one-bin jackknife. Let the sequential measurements be partitioned into $N_{\\text{bin}}$ contiguous, non-overlapping bins of equal size $B$. For each bin $i \\in \\{1,\\dots,N_{\\text{bin}}\\}$, compute the leave-one-out estimator $\\widehat{m}_{\\text{eff}}^{(-i)}$, defined as the value of $m_{\\text{eff}}(t_0)$ computed from all configurations except those in bin $i$. The jackknife estimate of the variance of $m_{\\text{eff}}$ is then constructed from the fluctuations of these replicates about their mean. The delete-one-bin jackknife is known to provide a consistent variance estimate for smooth nonlinear functionals of means. In practice, we compute the jackknife standard error $\\sigma(B)$ for each bin size $B$. For small $B$, residual autocorrelation between bins inflates $\\sigma(B)$ less than its asymptotic value; as $B$ increases so that $B \\gtrsim \\tau_{\\text{int}}$, $\\sigma(B)$ increases and saturates to a plateau corresponding to the true error after accounting for correlations. If $B$ never reaches $\\tau_{\\text{int}}$ due to limited $N$, saturation may not be observed.\n\nTo connect this reasoning to a controlled setting, we generate synthetic data. We model amplitude fluctuations via an autoregressive process of order one, $X_n = \\phi X_{n-1} + \\eta_n$, with $\\eta_n \\sim \\mathcal{N}(0, \\sigma_\\eta^2)$ and $\\sigma_\\eta^2 = 1 - \\phi^2$ so that the stationary variance is $\\operatorname{Var}(X_n) = 1$. The amplitude is $A^{(n)} = A_0 e^{s X_n}$, ensuring positivity. The measured correlators at times $t_0$ and $t_0 + 1$ are then\n$$\nC_t^{(n)} = A^{(n)} e^{-m t_0} \\, \\epsilon_t^{(n)}, \\qquad C_{t+1}^{(n)} = A^{(n)} e^{-m (t_0 + 1)} \\, \\epsilon_{t+1}^{(n)},\n$$\nwith multiplicative noises $\\epsilon \\sim 1 + \\mathcal{N}(0, \\sigma_{\\text{noise}}^2)$. We constrain the data to be strictly positive by clipping at a tiny positive floor, which is a negligible correction given small $\\sigma_{\\text{noise}}$.\n\nAlgorithmic design:\n\n- For each test case, and for a fixed random seed, simulate $N$ samples of $X_n$ and hence $A^{(n)}$, then generate $C_t^{(n)}$ and $C_{t+1}^{(n)}$.\n- Consider bin sizes $B \\in \\{ 1, 2, 4, \\dots \\}$ such that $N_{\\text{bin}} = \\lfloor N / B \\rfloor \\ge 8$. For each such $B$, compute the delete-one-bin jackknife standard error $\\sigma(B)$ of $m_{\\text{eff}}(t_0)$ constructed from the ensemble means of $C_t$ and $C_{t+1}$ with bin $i$ removed.\n- Define the relative change between the errors at the two largest admissible bin sizes $B_{K-1}$ and $B_K$ as $r = \\frac{|\\sigma(B_K) - \\sigma(B_{K-1})|}{\\max(\\sigma(B_K), \\varepsilon)}$ with $\\varepsilon = 10^{-16}$, and declare saturation if $r lt; \\delta$ for the designated $\\delta$.\n- The expected outcomes grounded in the fundamental base are:\n  - For moderate or weak autocorrelation, $\\sigma(B)$ will increase from $B = 1$ and plateau by the largest $B$, so saturation should be detected.\n  - For nearly uncorrelated data, $\\sigma(B)$ is nearly constant from $B = 1$ onward, so the saturation criterion still holds because the relative change is small.\n  - For very strong autocorrelation with limited $N$, $B$ may never reach the scale of $\\tau_{\\text{int}}$, hence $\\sigma(B)$ may not plateau by the largest admissible $B$, and saturation should not be detected.\n\nWe now apply this procedure to the prescribed test suite:\n\n- Test case $1$: seed $= 12345$, $N = 4096$, $\\phi = 0.8$, $A_0 = 1.0$, $s = 0.2$, $m = 0.5$, $t_0 = 5$, $\\sigma_{\\text{noise}} = 0.02$, $\\delta = 0.07$. Here $\\phi = 0.8$ corresponds to a moderate integrated autocorrelation time $\\tau_{\\text{int}} \\approx \\frac{1+\\phi}{2(1-\\phi)} = \\frac{1.8}{0.4} = 4.5$. With bin sizes up to $B = 512$, one expects saturation and thus a $True$ result.\n- Test case $2$: seed $= 54321$, $N = 4096$, $\\phi = 0.0$ yields $\\tau_{\\text{int}} \\approx 0.5$; the jackknife error is nearly constant across $B$, and the saturation criterion is satisfied, resulting in $True$.\n- Test case $3$: seed $= 999$, $N = 1024$, $\\phi = 0.995$ gives $\\tau_{\\text{int}} \\approx \\frac{1+0.995}{2(1-0.995)} \\approx \\frac{1.995}{0.01} \\approx 199.5$. The largest admissible bin is $B = 128$ (with $N_{\\text{bin}} = 8$), which is smaller than $\\tau_{\\text{int}}$. Thus, saturation is unlikely, and the result should be $False$.\n- Test case $4$: seed $= 2024$, $N = 256$, $\\phi = 0.9$ gives $\\tau_{\\text{int}} \\approx \\frac{1+0.9}{2(1-0.9)} = \\frac{1.9}{0.2} = 9.5$. The largest admissible bin is $B = 32$ (with $N_{\\text{bin}} = 8$), which exceeds $\\tau_{\\text{int}}$, so saturation should be observed, giving $True$.\n\nThe program collects the four booleans in order and prints a single line $[b_1,b_2,b_3,b_4]$ with $b_i \\in \\{\\text{True}, \\text{False}\\}$. This demonstrates that saturation of the jackknife error with increasing bin size is an empirical diagnostic of decorrelation: when $B$ is much larger than the autocorrelation scale, bin means are effectively independent and the jackknife error stabilizes, whereas if $B$ remains too small relative to the correlation length, the estimated error continues to change and saturation is not detected.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef ar1_series(N: int, phi: float, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generate an AR(1) series X_n = phi * X_{n-1} + eta_n with stationary Var(X)=1.\n    eta_n ~ N(0, 1 - phi^2). Start from stationary distribution.\n    \"\"\"\n    sigma_eta = np.sqrt(max(0.0, 1.0 - phi * phi))\n    X = np.empty(N)\n    # Start at stationary distribution\n    X[0] = rng.normal(0.0, 1.0)\n    for n in range(1, N):\n        X[n] = phi * X[n - 1] + rng.normal(0.0, sigma_eta)\n    return X\n\ndef simulate_correlators(seed: int, N: int, phi: float,\n                         A0: float, s_logamp: float,\n                         m: float, t0: int, sigma_noise: float) - tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Simulate C_t and C_{t+1} arrays for a single-state correlator with\n    log-normal amplitude fluctuations following an AR(1) process.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    X = ar1_series(N, phi, rng)\n    A = A0 * np.exp(s_logamp * X)  # positive amplitudes\n    base_t = np.exp(-m * t0)\n    base_tp1 = np.exp(-m * (t0 + 1))\n    # Multiplicative noise around 1 with small sigma\n    eps_t = 1.0 + sigma_noise * rng.standard_normal(N)\n    eps_tp1 = 1.0 + sigma_noise * rng.standard_normal(N)\n    Ct = A * base_t * eps_t\n    Ctp1 = A * base_tp1 * eps_tp1\n    # Enforce strict positivity\n    Ct = np.clip(Ct, 1e-12, None)\n    Ctp1 = np.clip(Ctp1, 1e-12, None)\n    return Ct, Ctp1\n\ndef jackknife_error_effective_mass(Ct: np.ndarray, Ctp1: np.ndarray, bin_size: int) - float:\n    \"\"\"\n    Compute the delete-one-bin jackknife standard error for the nonlinear estimator:\n    m_eff = log( mean(Ct) / mean(Ctp1) ), using contiguous non-overlapping bins of size bin_size.\n    Requires at least 2 bins; caller ensures =8 for robustness.\n    \"\"\"\n    N = len(Ct)\n    n_bins = N // bin_size\n    if n_bins  2:\n        return np.nan\n    keep = n_bins * bin_size\n    Ct_kept = Ct[:keep]\n    Ctp1_kept = Ctp1[:keep]\n    # Reshape into bins\n    Ct_bins = Ct_kept.reshape(n_bins, bin_size)\n    Ctp1_bins = Ctp1_kept.reshape(n_bins, bin_size)\n    # Precompute total sums\n    total_Ct = Ct_kept.sum()\n    total_Ctp1 = Ctp1_kept.sum()\n    total_count = keep\n    # Bin sums\n    bin_sum_Ct = Ct_bins.sum(axis=1)\n    bin_sum_Ctp1 = Ctp1_bins.sum(axis=1)\n    # Jackknife replicates\n    jk_vals = np.empty(n_bins)\n    count_excl = total_count - bin_size\n    for i in range(n_bins):\n        sum_excl_Ct = total_Ct - bin_sum_Ct[i]\n        sum_excl_Ctp1 = total_Ctp1 - bin_sum_Ctp1[i]\n        mean_excl_Ct = sum_excl_Ct / count_excl\n        mean_excl_Ctp1 = sum_excl_Ctp1 / count_excl\n        # Safety: avoid division by zero (should not occur due to positivity)\n        mean_excl_Ct = max(mean_excl_Ct, 1e-300)\n        mean_excl_Ctp1 = max(mean_excl_Ctp1, 1e-300)\n        jk_vals[i] = np.log(mean_excl_Ct / mean_excl_Ctp1)\n    jk_mean = jk_vals.mean()\n    # Standard jackknife variance: (n_bins - 1) * mean((theta_i - mean(theta_i))^2)\n    var_jk = (n_bins - 1) * np.mean((jk_vals - jk_mean) ** 2)\n    return float(np.sqrt(max(var_jk, 0.0)))\n\ndef bin_sizes_geometric(N: int, min_bins: int = 8) - list[int]:\n    \"\"\"\n    Generate powers-of-two bin sizes B = 1,2,4,... such that floor(N/B) = min_bins.\n    \"\"\"\n    sizes = []\n    B = 1\n    while N // B = min_bins:\n        sizes.append(B)\n        B *= 2\n    return sizes\n\ndef detect_saturation(Ct: np.ndarray, Ctp1: np.ndarray, delta_tol: float) - bool:\n    \"\"\"\n    Compute jackknife errors over admissible bin sizes and detect saturation using\n    the relative change criterion at the two largest bin sizes.\n    \"\"\"\n    N = len(Ct)\n    B_list = bin_sizes_geometric(N, min_bins=8)\n    if len(B_list)  2:\n        return False\n    errors = []\n    for B in B_list:\n        err = jackknife_error_effective_mass(Ct, Ctp1, B)\n        errors.append(err)\n    # Relative change at the end\n    eps = 1e-16\n    rel = abs(errors[-1] - errors[-2]) / max(errors[-1], eps)\n    return rel  delta_tol\n\ndef run_case(seed: int, N: int, phi: float,\n             A0: float, s: float,\n             m: float, t0: int, sigma_noise: float,\n             delta_tol: float) - bool:\n    Ct, Ctp1 = simulate_correlators(seed, N, phi, A0, s, m, t0, sigma_noise)\n    return detect_saturation(Ct, Ctp1, delta_tol)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (seed, N, phi, A0, s, m, t0, sigma_noise, delta_tol)\n    test_cases = [\n        (12345, 4096, 0.8,   1.0, 0.2, 0.5, 5, 0.02, 0.07),   # Case 1: moderate autocorrelation\n        (54321, 4096, 0.0,   1.0, 0.2, 0.5, 5, 0.02, 0.07),   # Case 2: nearly uncorrelated\n        (999,   1024, 0.995, 1.0, 0.2, 0.5, 5, 0.02, 0.07),   # Case 3: strong autocorr, no saturation\n        (2024,   256, 0.9,   1.0, 0.2, 0.5, 5, 0.02, 0.07),   # Case 4: boundary, should saturate\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, N, phi, A0, s, m, t0, sigma_noise, delta_tol = case\n        res = run_case(seed, N, phi, A0, s, m, t0, sigma_noise, delta_tol)\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}