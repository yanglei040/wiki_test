{
    "hands_on_practices": [
        {
            "introduction": "Effective Monte Carlo integration hinges on reducing statistical variance, and importance sampling achieves this by concentrating samples where the integrand is largest. This first exercise provides a foundational look at this principle by tackling a common radial integral found in nuclear physics. By analytically deriving the proposal distribution that minimizes variance, you will gain a clear, quantitative understanding of how matching the sampler to the integrand leads to dramatic gains in efficiency, even achieving the ideal zero-variance limit .",
            "id": "3570766",
            "problem": "In computational nuclear physics, radial integrals of the form $\\int_{0}^{\\infty} e^{-\\alpha r} r^{2} \\, dr$ arise when evaluating expectation values in spherical coordinates, where $r$ is the radial coordinate in femtometers ($\\mathrm{fm}$) and $\\alpha$ is a positive rate parameter with units $\\mathrm{fm}^{-1}$. Consider estimating the integral by Monte Carlo (MC) integration with importance sampling, using a proposal probability density function (PDF) $g(r)$ proportional to $r^{2} e^{-\\beta r}$ on $[0,\\infty)$, with $\\beta0$ and $g(r) = c \\, r^{2} e^{-\\beta r}$, where $c$ is a normalization constant to be determined. The target integrand is $f(r) = e^{-\\alpha r} r^{2}$.\n\nStarting only from core definitions, proceed as follows:\n- Use the definition of a probability density function to determine the constant $c$ so that $g(r)$ is normalized, by enforcing $\\int_{0}^{\\infty} g(r) \\, dr = 1$.\n- Using the definition of importance sampling, derive the unbiased MC estimator for the integral $\\int_{0}^{\\infty} f(r) \\, dr$ based on independent samples from $g(r)$, and derive the corresponding weight function $w(r)$ such that the estimator is the sample average of $w(r)$.\n- Derive the variance of the importance sampling estimator under $g(r)$ and express it in closed form as a function of $\\alpha$ and $\\beta$. Determine the range of $\\beta$ values for which this variance is finite. Then, choose $\\beta$ to minimize the variance and justify your choice rigorously.\n- Implement a complete, runnable program that:\n  1. Uses Monte Carlo importance sampling with the proposal $g(r)$, drawing independent samples from $g(r)$ by sampling from a Gamma distribution with shape parameter $k=3$ and rate parameter $\\beta$ (equivalently, scale parameter $1/\\beta$).\n  2. For each test case, computes two estimates of the integral: one using the given proposal parameter $\\beta$ and one using the variance-minimizing choice of $\\beta$ implied by your derivation.\n  3. Uses fixed random number seeds for reproducibility.\n  4. Produces a single line of output containing the results for all test cases as a comma-separated list of lists enclosed in square brackets, where each inner list contains two floating-point numbers corresponding to the estimate with the given $\\beta$ and the estimate with the optimal $\\beta$. The values must be expressed in cubic femtometers ($\\mathrm{fm}^{3}$). No rounding requirements are imposed beyond standard double-precision behavior.\n\nUse the following test suite, which is designed to probe typical behavior, low-variance optimal sampling, and near-boundary high-variance regimes:\n- Test case $1$: $\\alpha = 0.5 \\ \\mathrm{fm}^{-1}$, $\\beta = 0.5 \\ \\mathrm{fm}^{-1}$, $N = 2000$ samples, seed $= 123$.\n- Test case $2$: $\\alpha = 1.0 \\ \\mathrm{fm}^{-1}$, $\\beta = 0.7 \\ \\mathrm{fm}^{-1}$, $N = 50000$ samples, seed $= 456$.\n- Test case $3$: $\\alpha = 1.0 \\ \\mathrm{fm}^{-1}$, $\\beta = 1.99 \\ \\mathrm{fm}^{-1}$, $N = 50000$ samples, seed $= 789$.\n- Test case $4$: $\\alpha = 2.0 \\ \\mathrm{fm}^{-1}$, $\\beta = 0.2 \\ \\mathrm{fm}^{-1}$, $N = 100000$ samples, seed $= 321$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each element being an inner list of the form $[\\text{estimate\\_given\\_}\\beta,\\text{estimate\\_optimal\\_}\\beta]$. For example, the output format must be exactly like $[[x_{1},y_{1}],[x_{2},y_{2}],[x_{3},y_{3}],[x_{4},y_{4}]]$, where each $x_{i}$ and $y_{i}$ is a floating-point number in $\\mathrm{fm}^{3}$.",
            "solution": "We set up the integral and importance sampling estimator from first principles. The target integral is\n$$\nI(\\alpha) = \\int_{0}^{\\infty} f(r) \\, dr = \\int_{0}^{\\infty} e^{-\\alpha r} r^{2} \\, dr,\n$$\nwith $r \\ge 0$, $\\alpha  0$, and the units of $r$ in femtometers ($\\mathrm{fm}$) and $\\alpha$ in inverse femtometers ($\\mathrm{fm}^{-1}$), yielding $I(\\alpha)$ in cubic femtometers ($\\mathrm{fm}^{3}$). We introduce a proposal probability density function $g(r)$ proportional to $r^{2} e^{-\\beta r}$ with $\\beta0$:\n$$\ng(r) = c \\, r^{2} e^{-\\beta r}, \\quad r \\in [0,\\infty).\n$$\nBy the definition of a probability density function, $g$ must satisfy $\\int_{0}^{\\infty} g(r) \\, dr = 1$. Therefore,\n$$\n1 = \\int_{0}^{\\infty} g(r) \\, dr = c \\int_{0}^{\\infty} r^{2} e^{-\\beta r} \\, dr.\n$$\nThe integral $\\int_{0}^{\\infty} r^{n} e^{-\\lambda r} \\, dr$ is a standard Gamma function-based integral. Using the Gamma function $\\Gamma(z)$, defined by $\\Gamma(z) = \\int_{0}^{\\infty} t^{z-1} e^{-t} \\, dt$ for $z0$, and the substitution $t = \\lambda r$ yields\n$$\n\\int_{0}^{\\infty} r^{n} e^{-\\lambda r} \\, dr = \\frac{n!}{\\lambda^{n+1}}, \\quad \\lambda  0, \\ n \\in \\mathbb{N}.\n$$\nFor $n = 2$ and $\\lambda = \\beta$, we have\n$$\n\\int_{0}^{\\infty} r^{2} e^{-\\beta r} \\, dr = \\frac{2!}{\\beta^{3}} = \\frac{2}{\\beta^{3}}.\n$$\nThus,\n$$\n1 = c \\cdot \\frac{2}{\\beta^{3}} \\quad \\Rightarrow \\quad c = \\frac{\\beta^{3}}{2}.\n$$\nThe normalized proposal is therefore\n$$\ng(r) = \\frac{\\beta^{3}}{2} r^{2} e^{-\\beta r}, \\quad r \\ge 0,\n$$\nwhich is the probability density function of a Gamma distribution with shape parameter $k = 3$ and rate parameter $\\beta$ (equivalently, scale parameter $1/\\beta$), since the general Gamma PDF is\n$$\n\\text{Gamma}(k,\\beta): \\quad g(r) = \\frac{\\beta^{k}}{\\Gamma(k)} r^{k-1} e^{-\\beta r}.\n$$\nFor $k=3$, $\\Gamma(3) = 2$, which matches $g(r) = (\\beta^{3}/2) r^{2} e^{-\\beta r}$.\n\nIn importance sampling, the exact integral can be written as an expectation under $g$:\n$$\nI(\\alpha) = \\int_{0}^{\\infty} f(r) \\, dr = \\int_{0}^{\\infty} \\frac{f(r)}{g(r)} g(r) \\, dr = \\mathbb{E}_{g}[w(r)],\n$$\nwhere the weight function $w(r)$ is defined by\n$$\nw(r) = \\frac{f(r)}{g(r)} = \\frac{e^{-\\alpha r} r^{2}}{(\\beta^{3}/2) r^{2} e^{-\\beta r}} = \\frac{2}{\\beta^{3}} \\exp\\left( -(\\alpha - \\beta) r \\right).\n$$\nLet $r_{1}, r_{2}, \\dots, r_{N}$ be independent and identically distributed samples drawn from $g(r)$. The Monte Carlo estimator is the sample mean of the weights:\n$$\n\\widehat{I}_{N} = \\frac{1}{N} \\sum_{i=1}^{N} w(r_{i}),\n$$\nwhich is unbiased by construction, since $\\mathbb{E}_{g}[\\widehat{I}_{N}] = \\mathbb{E}_{g}[w(r)] = I(\\alpha)$.\n\nTo study and minimize the variance, we compute\n$$\n\\operatorname{Var}_{g}[w(r)] = \\mathbb{E}_{g}[w(r)^{2}] - I(\\alpha)^{2}.\n$$\nFirst,\n$$\nw(r)^{2} = \\left( \\frac{2}{\\beta^{3}} \\right)^{2} \\exp\\left( -2(\\alpha - \\beta) r \\right),\n$$\nand\n$$\n\\mathbb{E}_{g}[w(r)^{2}] = \\int_{0}^{\\infty} w(r)^{2} g(r) \\, dr = \\int_{0}^{\\infty} \\left( \\frac{4}{\\beta^{6}} \\exp\\left( -2(\\alpha - \\beta) r \\right) \\right) \\left( \\frac{\\beta^{3}}{2} r^{2} e^{-\\beta r} \\right) \\, dr.\n$$\nSimplifying the integrand,\n$$\n\\mathbb{E}_{g}[w(r)^{2}] = \\frac{2}{\\beta^{3}} \\int_{0}^{\\infty} r^{2} \\exp\\left( -(2\\alpha - \\beta) r \\right) \\, dr.\n$$\nApplying the Gamma integral with $n=2$ and $\\lambda = 2\\alpha - \\beta$,\n$$\n\\int_{0}^{\\infty} r^{2} e^{-(2\\alpha - \\beta) r} \\, dr = \\frac{2}{(2\\alpha - \\beta)^{3}},\n$$\nwhich is finite only if $2\\alpha - \\beta  0$, i.e., $\\beta \\in (0, 2\\alpha)$. Therefore,\n$$\n\\mathbb{E}_{g}[w(r)^{2}] = \\frac{4}{\\beta^{3} (2\\alpha - \\beta)^{3}},\n$$\nand\n$$\n\\operatorname{Var}_{g}[w(r)] = \\frac{4}{\\beta^{3} (2\\alpha - \\beta)^{3}} - I(\\alpha)^{2}, \\quad \\text{for } \\beta \\in (0, 2\\alpha).\n$$\nTo minimize the variance with respect to $\\beta$, it suffices to minimize $\\mathbb{E}_{g}[w(r)^{2}]$ because $I(\\alpha)$ does not depend on $\\beta$. Consider the function\n$$\n\\phi(\\beta) = \\frac{4}{\\beta^{3} (2\\alpha - \\beta)^{3}}, \\quad \\beta \\in (0, 2\\alpha).\n$$\nIntroduce the dimensionless parameter $y = \\beta / (2\\alpha)$, so $y \\in (0,1)$, and rewrite\n$$\n\\phi(y) = \\frac{4}{(2\\alpha)^{6}} \\cdot \\frac{1}{y^{3} (1 - y)^{3}}.\n$$\nThe factor $4/(2\\alpha)^{6}$ is constant with respect to $y$. Thus minimizing $\\phi(y)$ over $y \\in (0,1)$ is equivalent to maximizing $y^{3} (1 - y)^{3}$, which is symmetric and attains its maximum at $y = 1/2$. Therefore the variance is minimized at\n$$\n\\beta^{\\star} = \\alpha.\n$$\nAt this choice, the weight simplifies to\n$$\nw(r) = \\frac{2}{\\alpha^{3}} \\exp(0) = \\frac{2}{\\alpha^{3}},\n$$\nwhich is constant and yields zero variance, since $\\mathbb{E}_{g}[w(r)^{2}] = I(\\alpha)^{2}$ and hence $\\operatorname{Var}_{g}[w(r)] = 0$. This is a manifestation of the ideal importance sampler where $g(r)$ is proportional to the target integrand $f(r)$.\n\nAlgorithmic design for the program:\n- For each test case, compute two estimates of $I(\\alpha)$:\n  1. Using the given proposal parameter $\\beta$.\n  2. Using the optimal proposal parameter $\\beta^{\\star} = \\alpha$.\n- To sample from $g(r)$, use a Gamma distribution with shape $k = 3$ and scale $\\theta = 1/\\beta$. The probability density function of this Gamma distribution matches $g(r)$ as derived.\n- For each sample $r_{i}$, compute the weight $w(r_{i}) = (2/\\beta^{3}) \\exp( -(\\alpha - \\beta) r_{i} )$ and average over $N$ samples to obtain $\\widehat{I}_{N}$.\n- Use the fixed random number seeds provided with each test case to ensure reproducibility.\n- Aggregate the results into the specified single-line output format. Each reported floating-point number has units $\\mathrm{fm}^{3}$, consistent with $r$ in $\\mathrm{fm}$ and $\\alpha$ in $\\mathrm{fm}^{-1}$.\n\nThis approach respects the foundational definitions of probability density functions, expectations, and variance, and leverages the well-tested properties of the Gamma function to perform exact normalization and variance analysis, ensuring scientifically sound and numerically stable implementation across the provided test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef mc_importance_integral(alpha, beta, N, seed):\n    \"\"\"\n    Monte Carlo importance sampling estimator for I(alpha) = ∫_0^∞ e^{-alpha r} r^2 dr\n    using proposal g(r) = (beta^3/2) r^2 e^{-beta r}, i.e., Gamma(k=3, rate=beta).\n    Returns the estimate in fm^3.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Sample from Gamma with shape k=3 and scale theta=1/beta (rate=beta).\n    r_samples = rng.gamma(shape=3.0, scale=1.0 / beta, size=N)\n    # Importance weight: w(r) = (2 / beta^3) * exp(-(alpha - beta) * r)\n    weights = (2.0 / (beta ** 3)) * np.exp(-(alpha - beta) * r_samples)\n    return float(np.mean(weights))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (alpha [fm^-1], beta_given [fm^-1], N samples, seed)\n    test_cases = [\n        (0.5, 0.5, 2000, 123),      # Case 1: optimal beta equals alpha\n        (1.0, 0.7, 50000, 456),     # Case 2: typical case\n        (1.0, 1.99, 50000, 789),    # Case 3: near boundary beta  2*alpha\n        (2.0, 0.2, 100000, 321),    # Case 4: small beta relative to alpha\n    ]\n\n    results = []\n    for alpha, beta_given, N, seed in test_cases:\n        # Estimate with the given beta\n        estimate_given = mc_importance_integral(alpha, beta_given, N, seed)\n        # Estimate with the optimal beta (beta_star = alpha)\n        estimate_opt = mc_importance_integral(alpha, alpha, N, seed)\n        results.append([estimate_given, estimate_opt])\n\n    # Final print statement in the exact required format.\n    # Single line output: list of lists of floats.\n    print(f\"[{','.join('[' + ','.join(map(str, pair)) + ']' for pair in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While a single, well-chosen proposal works for simple integrands, many physical phenomena, such as particle scattering, involve both broad backgrounds and sharp resonances. This practice addresses these multi-scale problems, where a single proposal is doomed to be inefficient, either by missing the narrow peak or wasting samples on the background. You will implement and compare single-proposal strategies against a Multiple Importance Sampling (MIS) estimator, providing a hands-on demonstration of how MIS robustly handles complex integrands and drastically reduces variance .",
            "id": "3570791",
            "problem": "Consider the task of estimating the one-dimensional integral of a dimensionless integrand $f(E)$ over an energy interval $E \\in [0,E_{\\max}]$ motivated by resonance behavior in computational nuclear physics. The integrand is a superposition of a broad background and a narrow resonance peak, specified by\n$$\nf(E) = B \\,\\exp\\!\\left(-\\frac{E}{E_b}\\right) + A \\,\\frac{1}{\\pi \\Gamma \\left(1 + \\left(\\frac{E - E_0}{\\Gamma}\\right)^2\\right)} ,\n$$\nwhere $B  0$ controls the amplitude of the background, $E_b  0$ is the background scale, $A \\ge 0$ controls the resonance strength, $E_0$ is the resonance centroid, and $\\Gamma  0$ is the resonance width. The functional form of $f(E)$ is dimensionless, and there is no requirement to express the answer in physical units.\n\nYou will compare three importance sampling strategies for Monte Carlo integration of the integral $I = \\int_0^{E_{\\max}} f(E) \\, dE$:\n- a single broad proposal $g_1(E)$ targeted at the background,\n- a single narrow proposal $g_2(E)$ targeted at the peak,\n- a Multiple Importance Sampling (MIS) estimator that balances both proposals with equal sampling fractions.\n\nThe proposals are given by properly normalized probability density functions on $[0, E_{\\max}]$:\n1. The truncated exponential proposal for the background,\n$$\ng_1(E) = C_1 \\,\\exp\\!\\left(-\\frac{E}{E_b}\\right), \\quad C_1 = \\frac{1}{E_b \\left(1 - \\exp\\!\\left(-\\frac{E_{\\max}}{E_b}\\right)\\right)} .\n$$\n2. The truncated Cauchy (Lorentzian) proposal centered at the resonance,\n$$\ng_2(E) = \\frac{1}{Z_2}\\,\\frac{1}{\\pi \\Gamma \\left(1 + \\left(\\frac{E - E_0}{\\Gamma}\\right)^2\\right)} , \\quad Z_2 = \\frac{1}{\\pi}\\left[ \\arctan\\!\\left(\\frac{E_{\\max}-E_0}{\\Gamma}\\right) - \\arctan\\!\\left(\\frac{-E_0}{\\Gamma}\\right) \\right] .\n$$\n\nYou must implement Monte Carlo importance sampling using the following:\n- For a single proposal $g(E)$, draw $N$ independent samples $\\{E_n\\}_{n=1}^N \\sim g$, and form an unbiased estimator for $I$ based on $f(E_n)$ and $g(E_n)$.\n- For the Multiple Importance Sampling case, draw $N_1$ samples from $g_1$ and $N_2$ samples from $g_2$, with $N_1 = \\lfloor \\alpha_1 N \\rfloor$ and $N_2 = N - N_1$ using equal fractions $\\alpha_1 = \\alpha_2 = \\tfrac{1}{2}$, and combine them using the balance heuristic to obtain an unbiased estimator. Your implementation must use the balance heuristic in a way that is consistent with a mixture sampling density on $[0,E_{\\max}]$.\n\nFor each strategy, estimate the variance of the Monte Carlo estimator from the generated sample weights. Use the conventional estimator variance scaling with the sample count $N$: for each strategy, compute the empirical variance of the corresponding per-sample weights and divide by $N$ to obtain an estimate of the variance of the estimator of $I$. Use a fixed random seed to ensure deterministic outputs.\n\nYou must implement exact sampling from the proposals on the truncated domain:\n- For $g_1(E)$, sample $E$ via inverse transform sampling for the truncated exponential on $[0,E_{\\max}]$.\n- For $g_2(E)$, sample $E$ via inverse transform sampling for the truncated Cauchy on $[0,E_{\\max}]$, which can be constructed by sampling a uniform angle between the truncated inverse tangent bounds and mapping through the tangent.\n\nTest Suite:\nEvaluate the three strategies for five parameter sets. For each case, use the specified parameters and sample counts. The energy interval limit $E_{\\max}$ is included for each case. The sampling fractions must be $\\alpha_1 = \\alpha_2 = \\tfrac{1}{2}$ in all cases.\n\n- Case $1$ (happy path, separated scales): $E_{\\max} = 10$, $B = 1$, $E_b = 3$, $A = 4$, $E_0 = 3$, $\\Gamma = 0.2$, $N = 300000$.\n- Case $2$ (very narrow peak, strong resonance): $E_{\\max} = 10$, $B = 1$, $E_b = 3$, $A = 20$, $E_0 = 5$, $\\Gamma = 0.01$, $N = 500000$.\n- Case $3$ (broader peak, moderate strength): $E_{\\max} = 10$, $B = 1$, $E_b = 3$, $A = 2$, $E_0 = 4$, $\\Gamma = 1.0$, $N = 250000$.\n- Case $4$ (peak near lower boundary): $E_{\\max} = 10$, $B = 1$, $E_b = 3$, $A = 8$, $E_0 = 0.2$, $\\Gamma = 0.05$, $N = 400000$.\n- Case $5$ (no peak present): $E_{\\max} = 10$, $B = 1$, $E_b = 3$, $A = 0$, $E_0 = 4$, $\\Gamma = 0.5$, $N = 200000$.\n\nRandomness requirement:\nUse a fixed pseudorandom seed $s = 123456$ for all random draws to produce deterministic results.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the five test cases as a list of lists. For each test case, output the list\n$$\n\\left[ \\widehat{\\operatorname{Var}}_{g_1}, \\widehat{\\operatorname{Var}}_{g_2}, \\widehat{\\operatorname{Var}}_{\\mathrm{MIS}}, \\frac{\\widehat{\\operatorname{Var}}_{g_1}}{\\widehat{\\operatorname{Var}}_{\\mathrm{MIS}}}, \\frac{\\widehat{\\operatorname{Var}}_{g_2}}{\\widehat{\\operatorname{Var}}_{\\mathrm{MIS}}} \\right] ,\n$$\nwhere each variance is the empirical estimator variance as described above. Aggregate the five case-specific lists into one list in order from Case $1$ to Case $5$, and print that aggregate list exactly in Python list syntax, for example\n$$\n\\left[ [v_{1,1}, v_{1,2}, v_{1,3}, v_{1,4}, v_{1,5}], \\ldots, [v_{5,1}, v_{5,2}, v_{5,3}, v_{5,4}, v_{5,5}] \\right] .\n$$",
            "solution": "The user-provided problem has been analyzed and validated as being scientifically sound, well-posed, and objective. It presents a standard, albeit detailed, task in computational physics that is free of contradictions or ambiguities.\n\nHerein, a complete solution is presented. The core of the problem is to evaluate the performance of three distinct Monte Carlo importance sampling strategies in estimating the integral of a function $f(E)$ that models a physical process with both a background component and a sharp resonance. The integral to be estimated is:\n$$\nI = \\int_0^{E_{\\max}} f(E) \\, dE\n$$\nThe integrand $f(E)$ is a dimensionless function of energy $E$, given by the superposition of an exponential background and a Lorentzian (Cauchy) peak:\n$$\nf(E) = B \\,\\exp\\!\\left(-\\frac{E}{E_b}\\right) + A \\,\\frac{1}{\\pi \\Gamma \\left(1 + \\left(\\frac{E - E_0}{\\Gamma}\\right)^2\\right)}\n$$\nwhere $B  0$ is the background amplitude, $E_b  0$ is the background decay constant, $A \\ge 0$ is the resonance strength, $E_0$ is the energy of the resonance centroid, and $\\Gamma  0$ is the half-width at half-maximum of the resonance.\n\nThe goal is to compute the variance of the Monte Carlo estimator for $I$ for each of the three proposed sampling strategies. A lower variance indicates a more efficient estimator for a given number of samples $N$.\n\nThe fundamental principle of importance sampling is to estimate the integral $I = \\int f(x) dx$ by drawing samples $\\{x_i\\}$ from a proposal probability density function (PDF) $g(x)$ and forming the estimator $\\hat{I} = \\frac{1}{N} \\sum_{i=1}^N \\frac{f(x_i)}{g(x_i)}$. The variance of this estimator is minimized when the proposal PDF $g(x)$ is proportional to the absolute value of the integrand, i.e., $g(x) \\propto |f(x)|$. Since our $f(E)$ is a sum of two distinct functional forms, a single proposal PDF cannot be optimal for both parts simultaneously. This motivates the comparison of different strategies.\n\nThe variance of the estimator $\\hat{I}$ is given by $\\operatorname{Var}(\\hat{I}) = \\frac{1}{N} \\operatorname{Var}\\left( \\frac{f(E)}{g(E)} \\right)$. We will estimate this quantity empirically using the sample variance of the calculated weights $w_n = f(E_n)/g(E_n)$:\n$$\n\\widehat{\\operatorname{Var}}(\\hat{I}) = \\frac{1}{N} \\cdot \\frac{1}{N-1} \\sum_{n=1}^N (w_n - \\bar{w})^2\n$$\nwhere $\\bar{w} = \\frac{1}{N} \\sum_{n=1}^N w_n$.\n\nWe will now detail the specific formulation for each of the three strategies.\n\n**Strategy 1: Background-Targeted Proposal $g_1(E)$**\n\nThis strategy uses a proposal PDF that mimics the exponential background term of $f(E)$. The proposal is a truncated exponential distribution on the interval $[0, E_{\\max}]$:\n$$\ng_1(E) = C_1 \\,\\exp\\!\\left(-\\frac{E}{E_b}\\right), \\quad \\text{for } E \\in [0, E_{\\max}]\n$$\nThe normalization constant $C_1$ is determined by the condition $\\int_0^{E_{\\max}} g_1(E) dE = 1$, which yields:\n$$\nC_1 = \\left( \\int_0^{E_{\\max}} \\exp\\!\\left(-\\frac{E}{E_b}\\right) dE \\right)^{-1} = \\frac{1}{E_b \\left(1 - \\exp\\!\\left(-\\frac{E_{\\max}}{E_b}\\right)\\right)}\n$$\nTo draw samples from $g_1(E)$, we use the inverse transform sampling method. The cumulative distribution function (CDF) is:\n$$\nF_1(E) = \\int_0^E g_1(t) dt = \\frac{1 - \\exp(-E/E_b)}{1 - \\exp(-E_{\\max}/E_b)}\n$$\nSetting $F_1(E) = u$ for a uniform random number $u \\in [0,1]$ and inverting for $E$, we get the sampling formula:\n$$\nE = -E_b \\ln\\left(1 - u\\left(1 - \\exp\\left(-\\frac{E_{\\max}}{E_b}\\right)\\right)\\right)\n$$\nFor $N$ samples $\\{E_n\\}_{n=1}^N$ drawn using this formula, the importance weights are $w_{1,n} = f(E_n)/g_1(E_n)$. The estimator variance, $\\widehat{\\operatorname{Var}}_{g_1}$, is computed from these weights.\n\n**Strategy 2: Peak-Targeted Proposal $g_2(E)$**\n\nThis strategy uses a proposal PDF that matches the Lorentzian (Cauchy) resonance peak. The proposal is a truncated Cauchy distribution:\n$$\ng_2(E) = \\frac{1}{Z_2}\\,\\frac{1}{\\pi \\Gamma \\left(1 + \\left(\\frac{E - E_0}{\\Gamma}\\right)^2\\right)}, \\quad \\text{for } E \\in [0, E_{\\max}]\n$$\nThe normalization constant $Z_2$ is found by integrating the unnormalized Cauchy distribution over the domain $[0, E_{\\max}]$:\n$$\nZ_2 = \\int_0^{E_{\\max}} \\frac{1}{\\pi \\Gamma \\left(1 + \\left(\\frac{E - E_0}{\\Gamma}\\right)^2\\right)} dE = \\frac{1}{\\pi}\\left[ \\arctan\\!\\left(\\frac{E_{\\max}-E_0}{\\Gamma}\\right) - \\arctan\\!\\left(\\frac{-E_0}{\\Gamma}\\right) \\right]\n$$\nSampling is again done via the inverse transform method. The CDF is $F_2(E) = \\frac{\\arctan((E-E_0)/\\Gamma) - \\theta_{\\min}}{\\theta_{\\max} - \\theta_{\\min}}$, where $\\theta_{\\min} = \\arctan(-E_0/\\Gamma)$ and $\\theta_{\\max} = \\arctan((E_{\\max}-E_0)/\\Gamma)$. Setting $F_2(E)=u$ and inverting gives:\n$$\nE = E_0 + \\Gamma \\tan(\\theta_{\\min} + u(\\theta_{\\max}-\\theta_{\\min}))\n$$\nThis is equivalent to sampling a uniform angle $\\theta$ in the range $[\\theta_{\\min}, \\theta_{\\max}]$ and applying the transformation. For $N$ samples $\\{E_n\\}_{n=1}^N$ drawn this way, the weights are $w_{2,n} = f(E_n)/g_2(E_n)$, from which the estimator variance, $\\widehat{\\operatorname{Var}}_{g_2}$, is computed.\n\n**Strategy 3: Multiple Importance Sampling (MIS)**\n\nThis strategy combines both proposals to robustly handle both the background and the peak. The problem specifies a method consistent with sampling from a mixture PDF:\n$$\ng_{\\text{mix}}(E) = \\alpha_1 g_1(E) + \\alpha_2 g_2(E)\n$$\nwith equal sampling fractions $\\alpha_1 = \\alpha_2 = 1/2$. The procedure is to draw $N_1 = \\lfloor N/2 \\rfloor$ samples from $g_1(E)$ and $N_2 = N - N_1$ samples from $g_2(E)$, and then treat the combined set of $N=N_1+N_2$ samples as if they were drawn from $g_{\\text{mix}}(E)$.\n\nFor each sample $E_n$ in this combined set, the weight is calculated using the balance heuristic, which for a mixture PDF corresponds to:\n$$\nw_{\\text{MIS},n} = \\frac{f(E_n)}{g_{\\text{mix}}(E_n)} = \\frac{f(E_n)}{\\frac{1}{2} g_1(E_n) + \\frac{1}{2} g_2(E_n)}\n$$\nThis formulation is powerful because the denominator is large (and thus the weight is small and stable) whenever either $g_1(E_n)$ or $g_2(E_n)$ is a good match for $f(E_n)$. The estimator variance, $\\widehat{\\operatorname{Var}}_{\\text{MIS}}$, is computed from the sample variance of these $N$ weights.\n\nBy comparing the three variances ($\\widehat{\\operatorname{Var}}_{g_1}$, $\\widehat{\\operatorname{Var}}_{g_2}$, $\\widehat{\\operatorname{Var}}_{\\text{MIS}}$) across different physical scenarios (the five test cases), we can gain quantitative insight into the effectiveness of each strategy and the power of the MIS approach. The ratios $\\widehat{\\operatorname{Var}}_{g_1}/\\widehat{\\operatorname{Var}}_{\\text{MIS}}$ and $\\widehat{\\operatorname{Var}}_{g_2}/\\widehat{\\operatorname{Var}}_{\\text{MIS}}$ provide a direct measure of the efficiency gain offered by MIS over the single-proposal strategies.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy is not strictly needed as np covers all required functions.\n\ndef solve():\n    \"\"\"\n    Main function to run the Monte Carlo integration comparison for all test cases.\n    \"\"\"\n    \n    # Use a fixed pseudorandom seed for all random draws to produce deterministic results.\n    rng = np.random.default_rng(123456)\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: happy path, separated scales\n        {'E_max': 10.0, 'B': 1.0, 'E_b': 3.0, 'A': 4.0, 'E_0': 3.0, 'Gamma': 0.2, 'N': 300000},\n        # Case 2: very narrow peak, strong resonance\n        {'E_max': 10.0, 'B': 1.0, 'E_b': 3.0, 'A': 20.0, 'E_0': 5.0, 'Gamma': 0.01, 'N': 500000},\n        # Case 3: broader peak, moderate strength\n        {'E_max': 10.0, 'B': 1.0, 'E_b': 3.0, 'A': 2.0, 'E_0': 4.0, 'Gamma': 1.0, 'N': 250000},\n        # Case 4: peak near lower boundary\n        {'E_max': 10.0, 'B': 1.0, 'E_b': 3.0, 'A': 8.0, 'E_0': 0.2, 'Gamma': 0.05, 'N': 400000},\n        # Case 5: no peak present\n        {'E_max': 10.0, 'B': 1.0, 'E_b': 3.0, 'A': 0.0, 'E_0': 4.0, 'Gamma': 0.5, 'N': 200000},\n    ]\n\n    results = []\n    \n    # Integrand f(E)\n    def integrand(E, B, E_b, A, E_0, Gamma):\n        background = B * np.exp(-E / E_b)\n        # Avoid division by zero if Gamma is very small, although problem constraints ensure Gamma  0\n        cauchy_part = A / (np.pi * Gamma * (1 + ((E - E_0) / Gamma)**2))\n        return background + cauchy_part\n\n    # Proposal g1(E) - Truncated Exponential\n    def g1(E, E_b, E_max):\n        C1 = 1.0 / (E_b * (1.0 - np.exp(-E_max / E_b)))\n        return C1 * np.exp(-E / E_b)\n    \n    # Sampler for g1(E) using inverse transform sampling\n    def sample_g1(num_samples, E_b, E_max):\n        u = rng.uniform(0.0, 1.0, num_samples)\n        return -E_b * np.log(1.0 - u * (1.0 - np.exp(-E_max / E_b)))\n\n    # Proposal g2(E) - Truncated Cauchy\n    def g2(E, E_0, Gamma, E_max):\n        theta_min = np.arctan(-E_0 / Gamma)\n        theta_max = np.arctan((E_max - E_0) / Gamma)\n        Z2 = (theta_max - theta_min) / np.pi\n        return (1.0 / Z2) * (1.0 / (np.pi * Gamma * (1.0 + ((E - E_0) / Gamma)**2)))\n    \n    # Sampler for g2(E) using inverse transform sampling\n    def sample_g2(num_samples, E_0, Gamma, E_max):\n        u = rng.uniform(0.0, 1.0, num_samples)\n        theta_min = np.arctan(-E_0 / Gamma)\n        theta_max = np.arctan((E_max - E_0) / Gamma)\n        theta = theta_min + u * (theta_max - theta_min)\n        return E_0 + Gamma * np.tan(theta)\n\n    for case in test_cases:\n        p = case\n        N = p['N']\n\n        # --- Strategy 1: Importance sampling with g1 ---\n        E1_samples = sample_g1(N, p['E_b'], p['E_max'])\n        f_vals = integrand(E1_samples, p['B'], p['E_b'], p['A'], p['E_0'], p['Gamma'])\n        g1_vals = g1(E1_samples, p['E_b'], p['E_max'])\n        weights1 = f_vals / g1_vals\n        var_g1 = np.var(weights1, ddof=1) / N\n        \n        # --- Strategy 2: Importance sampling with g2 ---\n        E2_samples = sample_g2(N, p['E_0'], p['Gamma'], p['E_max'])\n        f_vals = integrand(E2_samples, p['B'], p['E_b'], p['A'], p['E_0'], p['Gamma'])\n        g2_vals = g2(E2_samples, p['E_0'], p['Gamma'], p['E_max'])\n        weights2 = f_vals / g2_vals\n        var_g2 = np.var(weights2, ddof=1) / N\n\n        # --- Strategy 3: Multiple Importance Sampling (MIS) ---\n        alpha1 = 0.5\n        alpha2 = 0.5\n        N1 = int(np.floor(alpha1 * N))\n        N2 = N - N1\n\n        E_mis_1 = sample_g1(N1, p['E_b'], p['E_max'])\n        E_mis_2 = sample_g2(N2, p['E_0'], p['Gamma'], p['E_max'])\n        E_mis_samples = np.concatenate((E_mis_1, E_mis_2))\n        \n        f_vals_mis = integrand(E_mis_samples, p['B'], p['E_b'], p['A'], p['E_0'], p['Gamma'])\n        g1_vals_mis = g1(E_mis_samples, p['E_b'], p['E_max'])\n        g2_vals_mis = g2(E_mis_samples, p['E_0'], p['Gamma'], p['E_max'])\n        \n        g_mix = alpha1 * g1_vals_mis + alpha2 * g2_vals_mis\n        weights_mis = f_vals_mis / g_mix\n        var_mis = np.var(weights_mis, ddof=1) / N\n\n        # Calculate ratios\n        ratio1_mis = var_g1 / var_mis\n        ratio2_mis = var_g2 / var_mis\n\n        case_results = [var_g1, var_g2, var_mis, ratio1_mis, ratio2_mis]\n        results.append(case_results)\n    \n    # Final print statement in the exact required format.\n    print(results)\n\nsolve()\n```"
        },
        {
            "introduction": "This exercise delves into the critical, practical issue of robustness against model mismatch, a frequent concern when our knowledge of the integrand is incomplete. The variance of an importance sampling estimator can become pathologically large if the proposal distribution fails to cover important regions of the integrand. You will analyze a scenario where an unexpected feature threatens the calculation and learn how to bound the variance by mixing in a heavy-tailed \"safeguard\" proposal, a key technique for designing resilient Monte Carlo estimators .",
            "id": "3570816",
            "problem": "In computational nuclear physics, consider the dimensionless reaction-rate integral over dimensionless energy $e \\in [0,\\infty)$,\n$$\nI \\;=\\; \\int_{0}^{\\infty} f(e) \\, de,\n$$\nwhere $f(e)$ represents the product of a neutron flux factor and a material cross section. A baseline importance sampling proposal $q_{0}(e)$ has been tuned to the smooth background, but a previously unknown narrow resonance appears in the cross section, centered at $e_{r}$ with width $\\gamma$. Model the resonance contribution to the integrand by a Lorentzian (single-level Breit–Wigner form),\n$$\nf_{r}(e) \\;=\\; S \\,\\frac{\\gamma^{2}}{(e-e_{r})^{2}+\\gamma^{2}},\n$$\nwhere the amplitude $S$ is fixed by the integrated resonance area $A = \\int_{-\\infty}^{\\infty} f_{r}(e)\\,de$. Assume the resonance is sufficiently narrow so that the neutron flux factor is approximately constant across the resonance region and is absorbed into $S$. The unknown resonance is poorly represented by $q_{0}(e)$; at the resonance center $q_{0}(e_{r})$ is small but nonzero.\n\nA Monte Carlo (MC) estimator of $I$ using importance sampling (IS) with proposal $q(e)$ has per-sample variance\n$$\nV(q) \\;=\\; \\int_{0}^{\\infty} \\frac{f(e)^{2}}{q(e)} \\, de \\;-\\; I^{2}.\n$$\nYou are tasked with analyzing, under worst-case mismatch when the resonance dominates the variance, how to cap the resonance-induced variance by mixing in a heavy-tailed safeguard. Specifically, consider the mixture family\n$$\nq_{\\epsilon}(e) \\;=\\; (1-\\epsilon)\\,q_{0}(e) \\;+\\; \\epsilon\\,r(e),\n$$\nwith $0 \\le \\epsilon \\le 1$ and a Cauchy safeguard\n$$\nr(e) \\;=\\; \\frac{1}{\\pi}\\,\\frac{\\gamma_{s}}{(e-e_{r})^{2}+\\gamma_{s}^{2}},\n$$\ncentered at $e_{r}$ with scale $\\gamma_{s}$. Assume, for bounding purposes, that the resonance contribution to $\\int f(e)^{2}/q_{\\epsilon}(e)\\,de$ is dominated by the neighborhood of $e_{r}$ and that $q_{\\epsilon}(e)$ is approximately constant across the narrow resonance, so that the resonance variance contribution satisfies\n$$\n\\int_{-\\infty}^{\\infty} \\frac{f_{r}(e)^{2}}{q_{\\epsilon}(e_{r})} \\, de \\;\\approx\\; \\frac{1}{q_{\\epsilon}(e_{r})}\\int_{-\\infty}^{\\infty} f_{r}(e)^{2} \\, de.\n$$\nThe design requirement is to cap this resonance contribution by a specified tolerance $V_{\\max}$:\n$$\n\\frac{1}{q_{\\epsilon}(e_{r})}\\int_{-\\infty}^{\\infty} f_{r}(e)^{2} \\, de \\;\\le\\; V_{\\max}.\n$$\n\nStarting from the fundamental definitions above, derive a closed-form expression for the minimal mixture weight $\\epsilon$ required to enforce the bound, and then evaluate it numerically for the following scientifically realistic parameters:\n- Resonance center $e_{r} = 6.67$,\n- Width $\\gamma = 0.3$,\n- Integrated resonance area $A = 12$,\n- Baseline proposal density at the resonance center $q_{0}(e_{r}) = 0.001$,\n- Safeguard Cauchy scale $\\gamma_{s} = 0.1$,\n- Resonance variance tolerance $V_{\\max} = 100$.\n\nAssume the resonance extension to the entire real line is valid for the purpose of bounding because $\\gamma \\ll e_{r}$, and ignore the $I^{2}$ term since the worst-case analysis is dominated by the first term. Express your final answer as a dimensionless decimal, rounded to four significant figures.",
            "solution": "The problem statement has been critically validated and is deemed to be self-contained, scientifically grounded in computational physics and statistical methods, and well-posed. All provided data, definitions, and assumptions are clear, consistent, and sufficient to derive a unique solution. The problem requires the derivation of a minimal mixture weight $\\epsilon$ for an importance sampling proposal density to cap the variance arising from a narrow resonance. The solution process will proceed.\n\nThe objective is to find the minimum value of $\\epsilon$ that satisfies the inequality:\n$$\n\\frac{1}{q_{\\epsilon}(e_{r})}\\int_{-\\infty}^{\\infty} f_{r}(e)^{2} \\, de \\le V_{\\max}\n$$\nThis requires deriving expressions for the integral term in the numerator and the proposal density term in the denominator.\n\nFirst, we determine the term $\\int_{-\\infty}^{\\infty} f_{r}(e)^{2} \\, de$. The resonance contribution is given by the Lorentzian function:\n$$\nf_{r}(e) = S \\frac{\\gamma^{2}}{(e-e_{r})^{2}+\\gamma^{2}}\n$$\nThe amplitude $S$ is related to the integrated resonance area $A = \\int_{-\\infty}^{\\infty} f_{r}(e)\\,de$. We compute this integral:\n$$\nA = \\int_{-\\infty}^{\\infty} S \\frac{\\gamma^{2}}{(e-e_{r})^{2}+\\gamma^{2}} \\, de = S\\gamma^{2} \\int_{-\\infty}^{\\infty} \\frac{1}{(e-e_{r})^{2}+\\gamma^{2}} \\, de\n$$\nUsing the standard integral $\\int \\frac{1}{x^2+a^2} dx = \\frac{1}{a}\\arctan(\\frac{x}{a})$, we have:\n$$\nA = S\\gamma^{2} \\left[ \\frac{1}{\\gamma}\\arctan\\left(\\frac{e-e_{r}}{\\gamma}\\right) \\right]_{-\\infty}^{\\infty} = S\\gamma \\left( \\frac{\\pi}{2} - \\left(-\\frac{\\pi}{2}\\right) \\right) = \\pi\\gamma S\n$$\nFrom this, we solve for the amplitude $S$:\n$$\nS = \\frac{A}{\\pi\\gamma}\n$$\nNext, we compute the integral of $f_{r}(e)^{2}$:\n$$\n\\int_{-\\infty}^{\\infty} f_{r}(e)^{2} \\, de = \\int_{-\\infty}^{\\infty} \\left( \\frac{A}{\\pi\\gamma} \\frac{\\gamma^{2}}{(e-e_{r})^{2}+\\gamma^{2}} \\right)^{2} \\, de = \\frac{A^{2}\\gamma^{4}}{\\pi^{2}\\gamma^{2}} \\int_{-\\infty}^{\\infty} \\frac{1}{\\left((e-e_{r})^{2}+\\gamma^{2}\\right)^{2}} \\, de\n$$\n$$\n\\int_{-\\infty}^{\\infty} f_{r}(e)^{2} \\, de = \\frac{A^{2}\\gamma^{2}}{\\pi^{2}} \\int_{-\\infty}^{\\infty} \\frac{1}{\\left((e-e_{r})^{2}+\\gamma^{2}\\right)^{2}} \\, de\n$$\nUsing the standard integral result $\\int_{-\\infty}^{\\infty} \\frac{1}{(x^2+a^2)^2} dx = \\frac{\\pi}{2a^3}$, with $x = e-e_r$ and $a = \\gamma$, we find:\n$$\n\\int_{-\\infty}^{\\infty} \\frac{1}{\\left((e-e_{r})^{2}+\\gamma^{2}\\right)^{2}} \\, de = \\frac{\\pi}{2\\gamma^{3}}\n$$\nSubstituting this back gives the value for the numerator of the variance expression:\n$$\n\\int_{-\\infty}^{\\infty} f_{r}(e)^{2} \\, de = \\frac{A^{2}\\gamma^{2}}{\\pi^{2}} \\left( \\frac{\\pi}{2\\gamma^{3}} \\right) = \\frac{A^{2}}{2\\pi\\gamma}\n$$\nSecond, we evaluate the mixture proposal density $q_{\\epsilon}(e)$ at the resonance center $e_{r}$:\n$$\nq_{\\epsilon}(e) = (1-\\epsilon)q_{0}(e) + \\epsilon r(e)\n$$\nAt $e=e_{r}$, this becomes:\n$$\nq_{\\epsilon}(e_{r}) = (1-\\epsilon)q_{0}(e_{r}) + \\epsilon r(e_{r})\n$$\nThe safeguard distribution $r(e)$ is a Cauchy distribution:\n$$\nr(e) = \\frac{1}{\\pi}\\frac{\\gamma_{s}}{(e-e_{r})^{2}+\\gamma_{s}^{2}}\n$$\nAt $e=e_{r}$, we have:\n$$\nr(e_{r}) = \\frac{1}{\\pi}\\frac{\\gamma_{s}}{(e_{r}-e_{r})^{2}+\\gamma_{s}^{2}} = \\frac{1}{\\pi}\\frac{\\gamma_{s}}{\\gamma_{s}^{2}} = \\frac{1}{\\pi\\gamma_{s}}\n$$\nTherefore, the proposal density at the resonance center is:\n$$\nq_{\\epsilon}(e_{r}) = (1-\\epsilon)q_{0}(e_{r}) + \\frac{\\epsilon}{\\pi\\gamma_{s}}\n$$\nNow, we substitute the derived expressions for the numerator and denominator back into the design inequality:\n$$\n\\frac{A^{2}/(2\\pi\\gamma)}{(1-\\epsilon)q_{0}(e_{r}) + \\epsilon/(\\pi\\gamma_{s})} \\le V_{\\max}\n$$\nTo find the minimal $\\epsilon$, we solve for $\\epsilon$ when the equality holds. Rearranging the inequality:\n$$\n\\frac{A^{2}}{2\\pi\\gamma V_{\\max}} \\le (1-\\epsilon)q_{0}(e_{r}) + \\frac{\\epsilon}{\\pi\\gamma_{s}}\n$$\n$$\n\\frac{A^{2}}{2\\pi\\gamma V_{\\max}} \\le q_{0}(e_{r}) - \\epsilon q_{0}(e_{r}) + \\frac{\\epsilon}{\\pi\\gamma_{s}}\n$$\n$$\n\\frac{A^{2}}{2\\pi\\gamma V_{\\max}} - q_{0}(e_{r}) \\le \\epsilon \\left( \\frac{1}{\\pi\\gamma_{s}} - q_{0}(e_{r}) \\right)\n$$\nAssuming $\\left( \\frac{1}{\\pi\\gamma_{s}} - q_{0}(e_{r}) \\right)  0$, which is expected as the safeguard peak density should be much larger than the background proposal, we can divide without changing the inequality's direction:\n$$\n\\epsilon \\ge \\frac{\\frac{A^{2}}{2\\pi\\gamma V_{\\max}} - q_{0}(e_{r})}{\\frac{1}{\\pi\\gamma_{s}} - q_{0}(e_{r})}\n$$\nThe minimal mixture weight, $\\epsilon_{\\min}$, is given by the equality:\n$$\n\\epsilon_{\\min} = \\frac{\\frac{A^{2}}{2\\pi\\gamma V_{\\max}} - q_{0}(e_{r})}{\\frac{1}{\\pi\\gamma_{s}} - q_{0}(e_{r})}\n$$\nWe now substitute the given numerical values: $A=12$, $\\gamma=0.3$, $q_{0}(e_{r})=0.001$, $\\gamma_{s}=0.1$, and $V_{\\max}=100$.\n\nFirst, we calculate the terms in the numerator of the expression for $\\epsilon_{\\min}$:\n$$\n\\frac{A^{2}}{2\\pi\\gamma V_{\\max}} = \\frac{12^{2}}{2\\pi(0.3)(100)} = \\frac{144}{60\\pi} = \\frac{2.4}{\\pi}\n$$\nThe numerator is:\n$$\n\\frac{2.4}{\\pi} - 0.001 \\approx 0.7639437 - 0.001 = 0.7629437\n$$\nNext, we calculate the terms in the denominator:\n$$\n\\frac{1}{\\pi\\gamma_{s}} = \\frac{1}{\\pi(0.1)} = \\frac{10}{\\pi}\n$$\nThe denominator is:\n$$\n\\frac{10}{\\pi} - 0.001 \\approx 3.1830989 - 0.001 = 3.1820989\n$$\nFinally, we compute $\\epsilon_{\\min}$:\n$$\n\\epsilon_{\\min} = \\frac{0.7629437}{3.1820989} \\approx 0.2397503\n$$\nRounding to four significant figures as required, the minimal mixture weight is $0.2398$.",
            "answer": "$$\\boxed{0.2398}$$"
        }
    ]
}