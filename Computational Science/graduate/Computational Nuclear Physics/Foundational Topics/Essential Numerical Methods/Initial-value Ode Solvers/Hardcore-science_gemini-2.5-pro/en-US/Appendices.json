{
    "hands_on_practices": [
        {
            "introduction": "Before deploying complex \"black-box\" solvers, it is crucial to understand the fundamental behavior of the underlying numerical methods. This practice provides a controlled environment to test theoretical predictions of stability and accuracy for basic one-step solvers like the Forward Euler, Backward Euler, and Trapezoidal methods. By confronting these methods with a stiff linear system, you will gain hands-on experience verifying their stability boundaries and orders of convergence, building essential intuition for the challenges posed by stiff ODEs in nuclear physics .",
            "id": "3565661",
            "problem": "Consider the linear, time-invariant initial-value problem for an Ordinary Differential Equation (ODE), $\\dfrac{d\\mathbf{y}}{dt} = J\\,\\mathbf{y}$ with initial condition $\\mathbf{y}(0)=\\mathbf{y}_0$, where $J\\in\\mathbb{R}^{n\\times n}$ has a known real spectrum that mimics the wide separation of nuclear-reaction time scales (many negative real eigenvalues spanning orders of magnitude). You will analyze one-step methods for advancing the solution by a time step $h>0$, compare their absolute-stability predictions to observed amplification, and verify their error orders from first principles.\n\nStarting from the fundamental base that: \n- the exact solution of a linear constant-coefficient system is $\\mathbf{y}(t+h)=\\exp(h J)\\,\\mathbf{y}(t)$, \n- a one-step method produces an update of the form $\\mathbf{y}_{n+1} = \\mathcal{R}_h(J)\\,\\mathbf{y}_n$ for some method-dependent amplification operator $\\mathcal{R}_h(J)$,\n- the one-step local truncation error scales as $\\mathcal{O}(h^{p+1})$ for a method of global order $p$ when starting from the exact state at $t$,\n\nderive for each of the following methods:\n- the amplification operator $\\mathcal{R}_h(J)$ from the defining discrete update, without using pre-memorized shortcut formulas,\n- the absolute-stability prediction stated in terms of the eigenvalues $\\lambda\\in\\Lambda(J)$ and step size $h$,\n- the expected one-step error order when starting from the exact state at $t$.\n\nThe methods to analyze are:\n- Forward Euler (explicit Euler),\n- Backward Euler (implicit Euler),\n- Trapezoidal rule (Crank–Nicolson).\n\nTo ensure scientific realism, use symmetric matrices $J$ with strictly negative real eigenvalues so that $J$ is diagonalizable by an orthogonal matrix. Construct $J$ deterministically as $J = Q\\,\\mathrm{diag}(\\Lambda)\\,Q^\\top$ with $Q\\in\\mathbb{R}^{3\\times 3}$ obtained from a fixed $3\\times 3$ real matrix via a unique orthogonal factor from the QR decomposition with a positive diagonal in $R$:\n- Let $M \\in \\mathbb{R}^{3\\times 3}$ be\n$$\nM=\\begin{bmatrix}\n1  2  3\\\\\n4  5  6\\\\\n7  8  10\n\\end{bmatrix}.\n$$\nDefine $Q R = M$ by the QR factorization with $Q^\\top Q = I$ and the condition that all diagonal entries of $R$ are positive. Use this same $Q$ to build each $J$ below, and take the initial condition to be the unit-norm vector $\\mathbf{y}_0 = \\dfrac{1}{\\|\\mathbf{v}\\|_2}\\,\\mathbf{v}$ with $\\mathbf{v}=[1,-1,2]^\\top$.\n\nImplement and test the following cases:\n\n- Case A (boundary and beyond for explicit stability): Let $\\Lambda_1=\\{-1,-10,-100\\}$ and $J_1=Q\\,\\mathrm{diag}(\\Lambda_1)\\,Q^\\top$. \n  - Use step sizes $h_1=10^{-2}$ and $h_2=3\\times 10^{-2}$ with the Forward Euler method. \n  - Define observed one-step amplification as the induced spectral norm $\\|\\mathcal{R}_{h}(J_1)\\|_2$ and call the method “observationally stable” if $\\|\\mathcal{R}_{h}(J_1)\\|_21$. \n  - Compare to the theoretical absolute-stability prediction stated in terms of $\\Lambda_1$ and $h$. Report two booleans: whether the method is observationally stable at $h_1$, and whether it is observationally stable at $h_2$.\n\n- Case B (absolute-stability for an implicit method): With the same $J_1$, use Backward Euler with $h_3=5$. Report a boolean indicating whether the method is observationally stable (again using $\\|\\mathcal{R}_{h_3}(J_1)\\|_21$).\n\n- Case C (large negative eigenvalue and damping of the stiffest mode): Let $\\Lambda_2=\\{-1,-10,-10^6\\}$ and $J_2=Q\\,\\mathrm{diag}(\\Lambda_2)\\,Q^\\top$. With time step $h_4=10^{-2}$, consider the amplification factor on the eigendirection associated with the stiffest eigenvalue $\\lambda_\\ast=-10^6$. \n  - Report the scalar one-step amplification magnitude for Backward Euler evaluated at $z_\\ast = h_4\\,\\lambda_\\ast$. \n  - Report the scalar one-step amplification magnitude for the Trapezoidal rule evaluated at the same $z_\\ast$.\n  Both numbers must be reported as floating-point values in unitless form.\n\n- Case D (verification of one-step local error orders): Let $\\Lambda_3=\\{-\\tfrac{1}{2},-1,-2\\}$ and $J_3=Q\\,\\mathrm{diag}(\\Lambda_3)\\,Q^\\top$. For each method among Forward Euler, Backward Euler, and Trapezoidal rule, compute the one-step error when starting from the exact state at $t=0$:\n  - For $h_{\\mathrm{big}}=10^{-1}$ compute $\\mathbf{y}_{\\mathrm{num}}=\\mathcal{R}_{h_{\\mathrm{big}}}(J_3)\\,\\mathbf{y}_0$ and $\\mathbf{y}_{\\mathrm{exact}}=\\exp(h_{\\mathrm{big}} J_3)\\,\\mathbf{y}_0$, and take the error $e_{\\mathrm{big}}=\\|\\mathbf{y}_{\\mathrm{num}}-\\mathbf{y}_{\\mathrm{exact}}\\|_2$.\n  - For $h_{\\mathrm{small}}=5\\times 10^{-2}$ repeat to obtain $e_{\\mathrm{small}}$.\n  - Report the three ratios $\\rho_{\\mathrm{FE}}=\\dfrac{e_{\\mathrm{big}}}{e_{\\mathrm{small}}}$, $\\rho_{\\mathrm{BE}}=\\dfrac{e_{\\mathrm{big}}}{e_{\\mathrm{small}}}$, and $\\rho_{\\mathrm{TR}}=\\dfrac{e_{\\mathrm{big}}}{e_{\\mathrm{small}}}$ for Forward Euler, Backward Euler, and Trapezoidal rule respectively. \n  - Also report three booleans indicating whether each ratio is within a relative tolerance of $\\tau=0.15$ of the theoretically expected factor $2^{p+1}$ for the method’s one-step local truncation error, where $p$ is the global order. That is, check $\\left|\\dfrac{\\rho}{2^{p+1}}-1\\right|\\tau$.\n\nAngle units are not involved. All reported quantities are unitless. Your program must implement the above construction and produce the following single line of output containing all results as a comma-separated list enclosed in square brackets, in this exact order:\n- Case A: observational stability at $h_1$ (boolean), observational stability at $h_2$ (boolean);\n- Case B: observational stability at $h_3$ (boolean);\n- Case C: stiff-mode amplification for Backward Euler (float), stiff-mode amplification for Trapezoidal rule (float);\n- Case D: $\\rho_{\\mathrm{FE}}$ (float), $\\rho_{\\mathrm{BE}}$ (float), $\\rho_{\\mathrm{TR}}$ (float), ratio-within-tolerance for Forward Euler (boolean), for Backward Euler (boolean), and for Trapezoidal rule (boolean).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\ldots]$). No additional text should be printed. All computations must be performed numerically using the definitions above, and comparisons must be made precisely as specified.",
            "solution": "The problem requires an analysis of three one-step numerical methods for the linear initial value problem $\\frac{d\\mathbf{y}}{dt} = J\\,\\mathbf{y}$, where $J$ is a diagonalizable real matrix with negative real eigenvalues. The analysis involves deriving the amplification operator, stating the stability condition, and determining the local error order for each method. These theoretical findings are then tested numerically in a series of specified cases.\n\n**Theoretical Derivations**\n\nA one-step method approximates the solution at time $t_{n+1}=t_n+h$ as $\\mathbf{y}_{n+1} = \\mathcal{R}_h(J)\\,\\mathbf{y}_n$, where $\\mathcal{R}_h(J)$ is the amplification operator. The method is absolutely stable if its application does not amplify any component of the numerical solution. For a normal matrix $J$ (which includes symmetric matrices), this is equivalent to requiring the spectral radius of the amplification operator to be at most one: $\\rho(\\mathcal{R}_h(J)) \\le 1$. This condition, in turn, simplifies to $|R(h\\lambda)| \\le 1$ for all eigenvalues $\\lambda$ of $J$, where $R(z)$ is the scalar stability function corresponding to the operator.\n\n**1. Forward Euler (Explicit) Method**\nThe method is defined by approximating the derivative at time $t_n$:\n$$ \\frac{\\mathbf{y}_{n+1} - \\mathbf{y}_n}{h} = J\\,\\mathbf{y}_n $$\n\n- **Amplification Operator**: Solving for $\\mathbf{y}_{n+1}$ gives the explicit update $\\mathbf{y}_{n+1} = \\mathbf{y}_n + hJ\\,\\mathbf{y}_n = (I + hJ)\\mathbf{y}_n$. The amplification operator is thus:\n  $$ \\mathcal{R}_h(J) = I + hJ $$\n\n- **Absolute-Stability Prediction**: The scalar stability function is $R(z) = 1+z$. Stability requires $|1+h\\lambda| \\le 1$ for all $\\lambda \\in \\Lambda(J)$. As $\\lambda$ is real and negative, let $\\lambda  0$. The condition becomes $-1 \\le 1+h\\lambda \\le 1$, which simplifies to $-2 \\le h\\lambda \\le 0$. Since $h0$ and $\\lambda0$, the condition $h\\lambda \\le 0$ is always met. The remaining condition is $h\\lambda \\ge -2$, or $h \\le -2/\\lambda$. This must hold for the eigenvalue with the largest magnitude, which imposes a limit on the step size $h$.\n\n- **One-Step Error Order**: The one-step error, assuming $\\mathbf{y}_n$ is exact, is the difference between the exact solution $\\mathbf{y}(t_n+h) = \\exp(hJ)\\mathbf{y}(t_n)$ and the numerical approximation $\\mathbf{y}_{n+1}$. Taylor expanding the exponential gives $\\exp(hJ) = I + hJ + \\frac{1}{2}h^2J^2 + \\mathcal{O}(h^3)$. The error operator is $\\exp(hJ) - \\mathcal{R}_h(J) = \\frac{1}{2}h^2J^2 + \\mathcal{O}(h^3)$. The one-step local truncation error is $\\mathcal{O}(h^2)$, which corresponds to a global order of convergence of $p=1$.\n\n**2. Backward Euler (Implicit) Method**\nThe method is defined by approximating the derivative at time $t_{n+1}$:\n$$ \\frac{\\mathbf{y}_{n+1} - \\mathbf{y}_n}{h} = J\\,\\mathbf{y}_{n+1} $$\n\n- **Amplification Operator**: To find $\\mathbf{y}_{n+1}$, one must solve a linear system: $(I - hJ)\\mathbf{y}_{n+1} = \\mathbf{y}_n$. This gives $\\mathbf{y}_{n+1} = (I - hJ)^{-1}\\mathbf{y}_n$. The amplification operator is:\n  $$ \\mathcal{R}_h(J) = (I - hJ)^{-1} $$\n\n- **Absolute-Stability Prediction**: The scalar stability function is $R(z) = (1-z)^{-1}$. Stability requires $|1-h\\lambda| \\ge 1$. For any $\\lambda$ with $\\text{Re}(\\lambda) \\le 0$ and $h0$, this condition holds. In particular, for real negative $\\lambda$, $1-h\\lambda  1$, so the condition is always satisfied. The method is A-stable, imposing no step-size restriction for stability on this class of problems.\n\n- **One-Step Error Order**: The error operator is $\\exp(hJ) - (I - hJ)^{-1}$. A careful expansion shows this to be $-\\frac{1}{2}h^2J^2 + \\mathcal{O}(h^3)$. The one-step local truncation error is $\\mathcal{O}(h^2)$, corresponding to a global order of $p=1$.\n\n**3. Trapezoidal Rule (Crank-Nicolson)**\nThis method averages the right-hand side evaluation at $t_n$ and $t_{n+1}$:\n$$ \\frac{\\mathbf{y}_{n+1} - \\mathbf{y}_n}{h} = \\frac{1}{2} (J\\,\\mathbf{y}_n + J\\,\\mathbf{y}_{n+1}) $$\n\n- **Amplification Operator**: Rearranging yields $(I - \\frac{h}{2}J)\\mathbf{y}_{n+1} = (I + \\frac{h}{2}J)\\mathbf{y}_n$. The amplification operator is:\n  $$ \\mathcal{R}_h(J) = \\left(I - \\frac{h}{2}J\\right)^{-1} \\left(I + \\frac{h}{2}J\\right) $$\n\n- **Absolute-Stability Prediction**: The scalar stability function is $R(z) = (1+z/2)/(1-z/2)$. The stability condition $|R(h\\lambda)| \\le 1$ is satisfied for all $\\lambda$ with $\\text{Re}(\\lambda) \\le 0$. The method is A-stable.\n\n- **One-Step Error Order**: The error operator is $\\exp(hJ) - \\mathcal{R}_h(J)$. The stability function $R(z)$ is the $(1,1)$-Padé approximant to $\\exp(z)$. The error series starts at the third-order term: $\\exp(z) - R(z) = -\\frac{1}{12}z^3 + \\mathcal{O}(z^4)$. The one-step local truncation error is $\\mathcal{O}(h^3)$, corresponding to a global order of $p=2$.\n\n**Numerical Implementation**\nThe numerical tests are carried out following the problem's specifications. The matrix $Q$ is obtained from the QR decomposition of the given matrix $M$, with the diagonal of $R$ forced to be positive. The matrices $J_k$ are constructed as $J_k = Q\\,\\mathrm{diag}(\\Lambda_k)\\,Q^\\top$. For each case, the required quantities are calculated numerically using these components. Error ratios in Case D are expected to follow $\\rho = (h_{\\mathrm{big}}/h_{\\mathrm{small}})^{p+1} = 2^{p+1}$, which is $4$ for the Euler methods ($p=1$) and $8$ for the Trapezoidal rule ($p=2$).",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Solves the specified numerical analysis problem regarding ODE solvers.\n    \"\"\"\n    \n    # --- Common Setup ---\n    M = np.array([\n        [1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 10.]\n    ])\n    \n    # Unique QR factorization with R having a positive diagonal\n    Q, R = np.linalg.qr(M)\n    s = np.diag(np.sign(np.diag(R)))\n    Q = Q @ s\n    # R_new = s @ R # not needed for problem but completes the factorization\n    \n    v = np.array([1., -1., 2.])\n    y0 = v / np.linalg.norm(v)\n\n    results = []\n\n    # --- Case A: Forward Euler Stability ---\n    Lambda1 = np.diag([-1., -10., -100.])\n    J1 = Q @ Lambda1 @ Q.T\n    h1 = 1e-2\n    h2 = 3e-2\n    I = np.identity(3)\n\n    # h1 test\n    R_h1_J1 = I + h1 * J1\n    norm1 = np.linalg.norm(R_h1_J1, ord=2)\n    stable_h1 = norm1  1.0\n    results.append(stable_h1)\n\n    # h2 test\n    R_h2_J1 = I + h2 * J1\n    norm2 = np.linalg.norm(R_h2_J1, ord=2)\n    stable_h2 = norm2  1.0\n    results.append(stable_h2)\n\n    # --- Case B: Backward Euler Stability ---\n    h3 = 5.0\n    # R_h3_J1 = inv(I - h3 * J1)\n    # Using np.linalg.solve is more stable than explicit inversion.\n    # We need the operator matrix itself for the norm, so inversion is appropriate here.\n    R_h3_J1 = np.linalg.inv(I - h3 * J1)\n    norm3 = np.linalg.norm(R_h3_J1, ord=2)\n    stable_h3 = norm3  1.0\n    results.append(stable_h3)\n\n    # --- Case C: Stiff Mode Amplification ---\n    lambda_star = -1e6\n    h4 = 1e-2\n    z_star = h4 * lambda_star\n\n    # Backward Euler amplification magnitude\n    amp_be = np.abs(1.0 / (1.0 - z_star))\n    results.append(amp_be)\n\n    # Trapezoidal rule amplification magnitude\n    amp_tr = np.abs((1.0 + z_star/2.0) / (1.0 - z_star/2.0))\n    results.append(amp_tr)\n    \n    # --- Case D: Verification of Error Orders ---\n    Lambda3 = np.diag([-0.5, -1.0, -2.0])\n    J3 = Q @ Lambda3 @ Q.T\n    h_big = 1e-1\n    h_small = 5e-2\n    tau = 0.15\n\n    methods_spec = {\n        'FE': {'p': 1, 'op': lambda J, h: I + h * J},\n        'BE': {'p': 1, 'op': lambda J, h: np.linalg.inv(I - h * J)},\n        'TR': {'p': 2, 'op': lambda J, h: np.linalg.inv(I - h*J/2.0) @ (I + h*J/2.0)},\n    }\n    \n    error_ratios = []\n    ratio_checks = []\n\n    method_order = ['FE', 'BE', 'TR']\n    \n    for method_key in method_order:\n        spec = methods_spec[method_key]\n        p = spec['p']\n        op_func = spec['op']\n\n        # Calculate numerical and exact solutions for h_big\n        R_big = op_func(J3, h_big)\n        y_num_big = R_big @ y0\n        y_exact_big = expm(h_big * J3) @ y0\n        e_big = np.linalg.norm(y_num_big - y_exact_big)\n\n        # Calculate numerical and exact solutions for h_small\n        R_small = op_func(J3, h_small)\n        y_num_small = R_small @ y0\n        y_exact_small = expm(h_small * J3) @ y0\n        e_small = np.linalg.norm(y_num_small - y_exact_small)\n\n        # Calculate error ratio\n        rho = e_big / e_small\n        error_ratios.append(rho)\n        \n        # Verify ratio against theory\n        theoretical_ratio = (h_big / h_small)**(p + 1)\n        check = np.abs(rho / theoretical_ratio - 1.0)  tau\n        ratio_checks.append(check)\n\n    results.extend(error_ratios)\n    results.extend(ratio_checks)\n    \n    # --- Final Output ---\n    # Convert booleans to lowercase 'true'/'false' as per standard Python str()\n    # and format floats. The problem implies standard string conversion is fine.\n    # Example format: [True,False,1.23] -> [true,false,1.23] which is JSON-like.\n    # Python str(True) is 'True'. The example in the prompt is math, not code output.\n    # We will let python's default string conversion for bools handle it.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many physical systems, such as those relaxing towards thermal or chemical equilibrium, are inherently dissipative. Their evolution is often described by a decreasing free-energy or Lyapunov function, a crucial physical property that a robust numerical solver should preserve. This practice demonstrates how advanced implicit methods, essential for stiff systems, can be designed to be algebraically stable, ensuring the numerical solution respects the underlying dissipative physics . You will implement and test several implicit schemes, including the L-stable Radau IIA method, to verify their ability to maintain discrete Lyapunov decay.",
            "id": "3565705",
            "problem": "Consider a non-dimensional dissipative model for near-equilibrium relaxation in a reduced nuclear reaction network. Let $y(t) \\in \\mathbb{R}^3$ denote deviations of species abundances from equilibrium, and suppose the dynamics are governed by the autonomous Ordinary Differential Equation (ODE)\n$$\n\\dot{y}(t) = f(y(t)) = -M \\nabla V(y(t)),\n$$\nwhere $V(y)$ is a smooth free-energy-like Lyapunov function and $M$ is a constant, symmetric positive definite mobility matrix. Assume a quadratic Lyapunov function\n$$\nV(y) = \\tfrac{1}{2} y^\\top H y,\n$$\nwith $H$ a constant, symmetric positive definite matrix representing the local curvature (Hessian) of the free energy near equilibrium. In this setting,\n$$\n\\nabla V(y) = H y, \\quad f(y) = -M H y.\n$$\nAll quantities are dimensionless.\n\nThe foundational facts for dissipative gradient flows are:\n- If $M$ and $H$ are symmetric positive definite, then $V$ is a strict Lyapunov function for the continuous-time system, satisfying $\\frac{d}{dt} V(y(t)) \\le 0$ along solutions.\n- Algebraically stable implicit time integration schemes applied to such monotone dissipative systems are expected to produce a discrete-time sequence $V(y_n)$ that is nonincreasing in $n$ under suitable conditions.\n\nYour task is to implement three implicit initial-value ODE solvers and numerically verify the discrete Lyapunov decay $V(y_{n+1}) \\le V(y_n)$ across multiple time steps for a fixed stiff linear gradient-flow instance. The solvers to be implemented are:\n- Backward Euler (implicit Euler).\n- Implicit midpoint (trapezoidal rule).\n- Two-stage Radau IIA Runge–Kutta method of order $3$.\n\nDo not use any specialized formulas stated here; instead, derive and implement these implicit schemes from their standard definitions for autonomous systems. Because $f(y)$ is linear for the chosen $V(y)$, each implicit step reduces to solving a linear system that depends on the method, the step size, and the matrix $A = M H$.\n\nUse the following specific, scientifically plausible matrices and initial condition:\n$$\nH = \\begin{bmatrix}\n4.0  0.5  0.0 \\\\\n0.5  2.0  0.3 \\\\\n0.0  0.3  10.0\n\\end{bmatrix}, \\quad\nM = \\begin{bmatrix}\n1.5  0.1  0.0 \\\\\n0.1  0.5  0.0 \\\\\n0.0  0.0  3.0\n\\end{bmatrix}, \\quad\ny_0 = \\begin{bmatrix}\n1.0 \\\\ -0.8 \\\\ 0.5\n\\end{bmatrix}.\n$$\nDefine $A = M H$ and $V(y) = \\tfrac{1}{2} y^\\top H y$.\n\nFor each test case, simulate $N$ steps with constant step size $h$ using the specified method, starting from $y_0$. After the simulation, evaluate whether the discrete Lyapunov sequence is nonincreasing at every step, that is, whether $V(y_{n+1}) \\le V(y_n)$ holds for all $n$ from $0$ to $N-1$. To allow for floating-point roundoff, treat violations smaller than a tolerance $\\varepsilon = 10^{-10}$ as non-violations; formally, require $V(y_{n+1}) \\le V(y_n) + \\varepsilon$ for all steps.\n\nImplement and test the following parameter sets (method, step size, number of steps), forming the test suite:\n- Backward Euler, $h = 0.05$, $N = 40$.\n- Implicit midpoint (trapezoidal rule), $h = 0.5$, $N = 20$.\n- Two-stage Radau IIA, $h = 5.0$, $N = 6$.\n- Backward Euler, $h = 10.0$, $N = 4$.\n- Implicit midpoint (trapezoidal rule), $h = 10.0$, $N = 4$.\n- Two-stage Radau IIA, $h = 20.0$, $N = 3$.\n\nFor each test case, produce a boolean result: `True` if the discrete Lyapunov sequence $V(y_n)$ is nonincreasing for all steps under the tolerance $\\varepsilon$, and `False` otherwise.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, `[r_1,r_2,...,r_6]`). Each $r_i$ must be a boolean corresponding to the $i$-th test case in the order listed above. Since the problem is non-dimensional, no physical units are required, and angles are not involved.",
            "solution": "The core of the problem is to analyze the behavior of three implicit time integration schemes applied to a stiff linear system of ODEs, $\\dot{y}(t) = -A y(t)$, where $A = MH$. The matrix $A$ is computed as:\n$$\nA = M H =\n\\begin{bmatrix}\n1.5  0.1  0.0 \\\\\n0.1  0.5  0.0 \\\\\n0.0  0.0  3.0\n\\end{bmatrix}\n\\begin{bmatrix}\n4.0  0.5  0.0 \\\\\n0.5  2.0  0.3 \\\\\n0.0  0.3  10.0\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n6.05  0.95  0.03 \\\\\n0.65  1.05  0.15 \\\\\n0.0  0.9  30.0\n\\end{bmatrix}\n$$\nThe Lyapunov function is $V(y) = \\frac{1}{2} y^\\top H y$. The task is to verify if $V(y_{n+1}) \\le V(y_n) + \\varepsilon$ for a sequence of timesteps, where $\\varepsilon = 10^{-10}$ is a numerical tolerance.\n\nThe derivation of the update step for each numerical method is as follows.\n\n**1. Backward Euler Method**\nThe Backward Euler method for an autonomous ODE $\\dot{y}=f(y)$ is defined by the implicit equation:\n$$\ny_{n+1} = y_n + h f(y_{n+1})\n$$\nFor the linear system $\\dot{y} = -Ay$, this becomes:\n$$\ny_{n+1} = y_n + h (-A y_{n+1}) = y_n - h A y_{n+1}\n$$\nRearranging the terms to solve for $y_{n+1}$ yields a linear system:\n$$\n(I + hA) y_{n+1} = y_n\n$$\nwhere $I$ is the identity matrix. The new state $y_{n+1}$ is found by solving this system for a given $y_n$.\n\n**2. Implicit Midpoint Method (Trapezoidal Rule)**\nThe Trapezoidal Rule for $\\dot{y}=f(y)$ is given by:\n$$\ny_{n+1} = y_n + \\frac{h}{2} \\left( f(y_n) + f(y_{n+1}) \\right)\n$$\nFor $\\dot{y} = -Ay$, substitutes into the formula:\n$$\ny_{n+1} = y_n + \\frac{h}{2} \\left( -A y_n - A y_{n+1} \\right)\n$$\nCollecting terms involving $y_{n+1}$ on the left-hand side and terms involving $y_n$ on the right-hand side, we get:\n$$\n\\left(I + \\frac{h}{2}A\\right) y_{n+1} = \\left(I - \\frac{h}{2}A\\right) y_n\n$$\nThis linear system is solved for $y_{n+1}$ at each time step. This method is A-stable but not L-stable, which may lead to persistent, undamped oscillations for stiff components when the step size $h$ is large.\n\n**3. Two-Stage Radau IIA Method (Order 3)**\nThis is an implicit Runge-Kutta method with $s=2$ stages. Its Butcher tableau is:\n$$\n\\begin{array}{c|cc}\n1/3  5/12  -1/12 \\\\\n1    3/4  1/4 \\\\\n\\hline\n     3/4  1/4\n\\end{array}\n$$\nThe stage equations for $\\dot{y} = -Ay$ are:\n$$\nk_1 = -A \\left( y_n + h \\left( \\frac{5}{12}k_1 - \\frac{1}{12}k_2 \\right) \\right)\n$$\n$$\nk_2 = -A \\left( y_n + h \\left( \\frac{3}{4}k_1 + \\frac{1}{4}k_2 \\right) \\right)\n$$\nThese equations form a coupled linear system for the stage vectors $k_1$ and $k_2$:\n$$\n\\left(I + \\frac{5h}{12}A\\right) k_1 - \\frac{h}{12}A k_2 = -A y_n\n$$\n$$\n-\\frac{3h}{4}A k_1 + \\left(I + \\frac{h}{4}A\\right) k_2 = -A y_n\n$$\nThis $6 \\times 6$ block system can be written as:\n$$\n\\begin{bmatrix}\nI + \\frac{5h}{12}A  -\\frac{h}{12}A \\\\\n-\\frac{3h}{4}A  I + \\frac{h}{4}A\n\\end{bmatrix}\n\\begin{pmatrix} k_1 \\\\ k_2 \\end{pmatrix}\n=\n\\begin{pmatrix} -A y_n \\\\ -A y_n \\end{pmatrix}\n$$\nAfter solving this system for $k_1, k_2 \\in \\mathbb{R}^3$, the state is updated using the weights $b_i$:\n$$\ny_{n+1} = y_n + h \\left( \\frac{3}{4}k_1 + \\frac{1}{4}k_2 \\right)\n$$\nThis method is L-stable, ensuring strong damping of high-frequency components, which is desirable for stiff systems.\n\nFor each test case, the corresponding method is applied for $N$ steps with step size $h$, starting from $y_0$. At each step $n \\in \\{0, \\dots, N-1\\}$, $V_n = V(y_n)$ and $V_{n+1} = V(y_{n+1})$ are computed, and the condition $V_{n+1} \\le V_n + \\varepsilon$ is checked. If the condition is violated at any step, the result for that test case is `False`; otherwise, it is `True`. The L-stable methods (Backward Euler, Radau IIA) are expected to preserve the Lyapunov decay for any step size, while the A-stable but not L-stable Trapezoidal rule might fail for large step sizes where $h\\lambda_{\\max}$ is large, as its stability function approaches $-1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests three implicit ODE solvers on a stiff linear gradient-flow system\n    to verify the discrete Lyapunov decay property.\n    \"\"\"\n    \n    # Define problem constants and initial conditions\n    H = np.array([\n        [4.0, 0.5, 0.0],\n        [0.5, 2.0, 0.3],\n        [0.0, 0.3, 10.0]\n    ])\n    M = np.array([\n        [1.5, 0.1, 0.0],\n        [0.1, 0.5, 0.0],\n        [0.0, 0.0, 3.0]\n    ])\n    y0 = np.array([1.0, -0.8, 0.5])\n    epsilon = 1e-10\n    \n    # System matrix and dimension\n    A = M @ H\n    dim = A.shape[0]\n    I = np.identity(dim)\n\n    # Lyapunov function V(y) = 0.5 * y^T * H * y\n    def lyapunov_V(y, H_mat):\n        return 0.5 * y.T @ H_mat @ y\n\n    # Define the six test cases: (method, step size h, number of steps N)\n    test_cases = [\n        ('be', 0.05, 40),\n        ('im', 0.5, 20),\n        ('r2', 5.0, 6),\n        ('be', 10.0, 4),\n        ('im', 10.0, 4),\n        ('r2', 20.0, 3),\n    ]\n\n    results = []\n\n    for method, h, N in test_cases:\n        y = y0.copy()\n        is_nonincreasing = True\n        \n        # Pre-compute solver matrices if they are constant over the steps\n        if method == 'be':\n            solver_mat = I + h * A\n        elif method == 'im':\n            solver_mat = I + h * 0.5 * A\n            rhs_mat = I - h * 0.5 * A\n        elif method == 'r2':\n            # Butcher tableau for 2-stage Radau IIA\n            a11, a12 = 5/12, -1/12\n            a21, a22 = 3/4, 1/4\n            b1, b2 = 3/4, 1/4\n\n            # Construct the 2d x 2d block matrix for stage equations\n            M_radau = np.block([\n                [I + h * a11 * A, h * a12 * A],\n                [h * a21 * A, I + h * a22 * A]\n            ])\n\n        for n in range(N):\n            V_prev = lyapunov_V(y, H)\n            \n            if method == 'be':\n                y_next = np.linalg.solve(solver_mat, y)\n            \n            elif method == 'im':\n                rhs_vec = rhs_mat @ y\n                y_next = np.linalg.solve(solver_mat, rhs_vec)\n            \n            elif method == 'r2':\n                # Solve for stage derivatives k1, k2\n                rhs_k = -A @ y\n                rhs_block = np.concatenate([rhs_k, rhs_k])\n                \n                k_block = np.linalg.solve(M_radau, rhs_block)\n                k1 = k_block[:dim]\n                k2 = k_block[dim:]\n\n                # Update step\n                y_next = y + h * (b1 * k1 + b2 * k2)\n            \n            y = y_next\n            V_next = lyapunov_V(y, H)\n            \n            if V_next > V_prev + epsilon:\n                is_nonincreasing = False\n                break\n        \n        results.append(is_nonincreasing)\n\n    # Print the final results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Nuclear reaction networks are classic examples of multiscale systems, where reactions with vastly different timescales—such as fast neutron captures and slow $\\beta$-decays—occur simultaneously. Treating such a system with a single, monolithic solver can be computationally prohibitive. This exercise introduces the powerful Implicit-Explicit (IMEX) strategy, where the system's governing operator is split, allowing stiff components to be handled implicitly while non-stiff parts are treated explicitly . By constructing a multirate integrator from first principles, you will learn to tackle these challenging problems efficiently and analyze the resulting partitioning errors.",
            "id": "3565707",
            "problem": "Consider a minimal, scientifically reasonable stylized two-isotope waiting-point subsystem within a neutron-rich nucleosynthesis flow. Let the abundances be denoted by the vector $\\mathbf{N}(t) = [N_1(t), N_2(t)]^\\top$ with $N_1(t)$ representing the population of a pre-waiting-point isotope and $N_2(t)$ representing the population of the waiting-point isotope. Assume a constant neutron environment and effective one-neutron-capture and $\\beta$-decay rates that do not depend on time. The coupled rate equations are\n$$\n\\frac{d}{dt}\\mathbf{N}(t) = \\mathbf{R}_{\\text{cap}}(\\mathbf{N}(t)) + \\mathbf{R}_{\\beta}(\\mathbf{N}(t)),\n$$\nwhere capture drives $N_1$ forward to $N_2$, and $\\beta$-decay returns $N_2$ to $N_1$. The components are defined by the fundamental reaction-rate law for well-mixed populations (linear mass-action with constant rate coefficients in a static thermodynamic environment):\n$$\n\\mathbf{R}_{\\text{cap}}(\\mathbf{N}) = \\begin{bmatrix} -\\lambda_{\\text{cap}} N_1 \\\\ +\\lambda_{\\text{cap}} N_1 \\end{bmatrix}, \\quad\n\\mathbf{R}_{\\beta}(\\mathbf{N}) = \\begin{bmatrix} +\\lambda_{\\beta} N_2 \\\\ -\\lambda_{\\beta} N_2 \\end{bmatrix},\n$$\nwith $\\lambda_{\\text{cap}}$ in $\\mathrm{s}^{-1}$ and $\\lambda_{\\beta}$ in $\\mathrm{s}^{-1}$. This simplified pair forms a conservative closed subsystem with $N_1(t)+N_2(t)$ constant, suitable for isolating waiting-point partitioning effects.\n\nYou are to construct an Implicit-Explicit (IMEX) multirate integrator for this initial-value problem based on first principles. In this construction:\n- Treat $\\mathbf{R}_{\\beta}$ as the \"slow\" stiff component to be advanced implicitly over a macro-step $\\Delta t_s$ via the backward-Euler method.\n- Treat $\\mathbf{R}_{\\text{cap}}$ as the \"fast\" component to be advanced explicitly via $m$ micro-steps of the forward-Euler method of size $\\Delta t_f = \\Delta t_s/m$ per macro-step.\n- Within each macro-step, first apply the implicit slow update, then advance the fast component using the $m$ explicit micro-steps.\n\nStarting from the definitions of the ordinary differential equation, the backward-Euler update for the slow component, and the forward-Euler update for the fast component, derive the discrete update map for one macro-step and implement it. Do not use pre-assembled or shortcut update formulas; derive any algebra needed for the implicit solve directly from the definitions above.\n\nTo quantify partitioning errors near waiting points, compare the IMEX multirate solution to a reference solution obtained by integrating the full unsplit system with a high-accuracy stiff solver using the Backward Differentiation Formula (BDF) method. Use the same initial conditions and physical parameters for both integrations and the same final time. Define the bias in the final waiting-point abundance as\n$$\n\\delta N_2 = N_{2,\\text{IMEX}}(T) - N_{2,\\text{ref}}(T),\n$$\nwhich you must report for each test case.\n\nInitial condition for all test cases: $N_1(0) = 1$ (dimensionless number fraction), $N_2(0) = 0$ (dimensionless number fraction). Express all times in $\\mathrm{s}$ and all rates in $\\mathrm{s}^{-1}$. The quantity $\\delta N_2$ is dimensionless.\n\nImplement the program to run the following test suite of parameter sets that probe a variety of dynamical regimes, including a general case, waiting-point dominated regimes, balanced regimes, and a coarse multirate case to expose partitioning errors:\n\n- Test case $1$ (general \"happy path\"): $\\lambda_{\\text{cap}} = 10$, $\\lambda_{\\beta} = 1$, $T = 1$, $\\Delta t_s = 0.1$, $m = 10$.\n- Test case $2$ (waiting point dominated by fast capture and slow decay): $\\lambda_{\\text{cap}} = 1000$, $\\lambda_{\\beta} = 0.1$, $T = 0.1$, $\\Delta t_s = 0.05$, $m = 100$.\n- Test case $3$ (balanced rates): $\\lambda_{\\text{cap}} = 5$, $\\lambda_{\\beta} = 5$, $T = 1$, $\\Delta t_s = 0.2$, $m = 5$.\n- Test case $4$ (coarse multirate micro-step exposing partitioning error): $\\lambda_{\\text{cap}} = 100$, $\\lambda_{\\beta} = 0.5$, $T = 0.5$, $\\Delta t_s = 0.25$, $m = 25$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[x_1,x_2,x_3,x_4]$) where each $x_i$ is the numerical value of $\\delta N_2$ for test case $i$ expressed as a decimal number. Use a high-accuracy reference integration to ensure that observed biases are attributable to partitioning and multirate effects, not to the baseline solver. Ensure numerical non-negativity of abundances throughout the explicit updates by choosing $\\Delta t_f$ and $m$ as given; do not add artificial clipping.",
            "solution": "The core of this problem is to derive and implement a specific first-order Implicit-Explicit (IMEX) multirate integration scheme for a system of coupled ordinary differential equations (ODEs), and to compare its results against a high-accuracy reference solution. The system describes the evolution of two isotope abundances, $N_1(t)$ and $N_2(t)$, governed by neutron capture and $\\beta$-decay.\n\nThe governing ODE system is given by $\\frac{d}{dt}\\mathbf{N}(t) = \\mathbf{F}(\\mathbf{N}(t))$, where $\\mathbf{N}(t) = [N_1(t), N_2(t)]^\\top$. The total rate of change $\\mathbf{F}(\\mathbf{N})$ is split into a \"fast\" component, $\\mathbf{R}_{\\text{cap}}(\\mathbf{N})$, and a \"slow\" component, $\\mathbf{R}_{\\beta}(\\mathbf{N})$.\n$$\n\\frac{d}{dt}\\mathbf{N}(t) = \\mathbf{R}_{\\text{cap}}(\\mathbf{N}(t)) + \\mathbf{R}_{\\beta}(\\mathbf{N}(t))\n$$\nThe components are:\n$$\n\\mathbf{R}_{\\text{cap}}(\\mathbf{N}) = \\begin{bmatrix} -\\lambda_{\\text{cap}} N_1 \\\\ +\\lambda_{\\text{cap}} N_1 \\end{bmatrix}, \\quad\n\\mathbf{R}_{\\beta}(\\mathbf{N}) = \\begin{bmatrix} +\\lambda_{\\beta} N_2 \\\\ -\\lambda_{\\beta} N_2 \\end{bmatrix}\n$$\nThe IMEX scheme advances the solution over a macro-step of size $\\Delta t_s$. Let $\\mathbf{N}^k$ be the numerical solution at time $t_k$. The solution at time $t_{k+1} = t_k + \\Delta t_s$ is denoted $\\mathbf{N}^{k+1}$. The specified scheme involves two stages per macro-step: an implicit update for the slow component followed by explicit updates for the fast component.\n\n**Stage 1: Implicit Slow Update**\n\nThe slow component, $\\mathbf{R}_{\\beta}$, is treated implicitly using the backward-Euler method over the macro-step $\\Delta t_s$. This updates the state from $\\mathbf{N}^k$ to an intermediate state $\\mathbf{N}^*$. The backward-Euler formula is:\n$$\n\\frac{\\mathbf{N}^* - \\mathbf{N}^k}{\\Delta t_s} = \\mathbf{R}_{\\beta}(\\mathbf{N}^*)\n$$\nWriting this in component form for $\\mathbf{N}^* = [N_1^*, N_2^*]^\\top$ and $\\mathbf{N}^k = [N_1^k, N_2^k]^\\top$:\n$$\n\\frac{N_1^* - N_1^k}{\\Delta t_s} = \\lambda_{\\beta} N_2^*\n$$\n$$\n\\frac{N_2^* - N_2^k}{\\Delta t_s} = -\\lambda_{\\beta} N_2^*\n$$\nThis is a system of linear equations for $N_1^*$ and $N_2^*$. We first solve the second equation for $N_2^*$, as it is independent of $N_1^*$:\n$$\nN_2^* - N_2^k = -\\Delta t_s \\lambda_{\\beta} N_2^* \\implies N_2^*(1 + \\Delta t_s \\lambda_{\\beta}) = N_2^k\n$$\n$$\nN_2^* = \\frac{N_2^k}{1 + \\Delta t_s \\lambda_{\\beta}}\n$$\nSubstituting this result into the first equation allows us to solve for $N_1^*$:\n$$\nN_1^* = N_1^k + \\Delta t_s \\lambda_{\\beta} N_2^*\n$$\nThis defines the intermediate state $\\mathbf{N}^*$. Note that summing the two component equations yields $N_1^* + N_2^* = N_1^k + N_2^k$, confirming that the total abundance is conserved by this implicit step, as it should be for the closed $\\mathbf{R}_{\\beta}$ subsystem. Thus, a numerically robust way to calculate $N_1^*$ is:\n$$\nN_1^* = N_1^k + N_2^k - N_2^*\n$$\n\n**Stage 2: Explicit Fast Updates**\n\nThe fast component, $\\mathbf{R}_{\\text{cap}}$, is treated explicitly using $m$ micro-steps of the forward-Euler method. The size of each micro-step is $\\Delta t_f = \\Delta t_s / m$. This sequence of updates advances the state from $\\mathbf{N}^*$ to the final state $\\mathbf{N}^{k+1}$.\n\nLet $\\mathbf{N}^{(j)}$ be the state after $j$ micro-steps, with the initial state being $\\mathbf{N}^{(0)} = \\mathbf{N}^*$. The forward-Euler update for one micro-step from state $\\mathbf{N}^{(j-1)}$ to $\\mathbf{N}^{(j)}$ is:\n$$\n\\frac{\\mathbf{N}^{(j)} - \\mathbf{N}^{(j-1)}}{\\Delta t_f} = \\mathbf{R}_{\\text{cap}}(\\mathbf{N}^{(j-1)})\n$$\nIn component form:\n$$\nN_1^{(j)} = N_1^{(j-1)} - \\Delta t_f \\lambda_{\\text{cap}} N_1^{(j-1)} = (1 - \\Delta t_f \\lambda_{\\text{cap}}) N_1^{(j-1)}\n$$\n$$\nN_2^{(j)} = N_2^{(j-1)} + \\Delta t_f \\lambda_{\\text{cap}} N_1^{(j-1)}\n$$\nWe apply this update $m$ times, starting from $\\mathbf{N}^{(0)} = \\mathbf{N}^*$. The equation for $N_1$ can be solved by repeated application:\n$$\nN_1^{(m)} = (1 - \\Delta t_f \\lambda_{\\text{cap}})^m N_1^{(0)} = (1 - \\Delta t_f \\lambda_{\\text{cap}})^m N_1^*\n$$\nSince the operator $\\mathbf{R}_{\\text{cap}}$ is also conservative, we have $N_1^{(j)} + N_2^{(j)} = N_1^{(j-1)} + N_2^{(j-1)}$ for each micro-step. This implies that the total abundance is conserved over the entire sequence of $m$ micro-steps: $N_1^{(m)} + N_2^{(m)} = N_1^{(0)} + N_2^{(0)}$.\nUsing this conservation property, we can find $N_2^{(m)}$ directly:\n$$\nN_2^{(m)} = N_1^{(0)} + N_2^{(0)} - N_1^{(m)} = N_1^* + N_2^* - N_1^{(m)}\n$$\nThe state at the end of the macro-step is $\\mathbf{N}^{k+1} = \\mathbf{N}^{(m)}$.\n\n**Complete IMEX Macro-step Update Map**\n\nCombining both stages, the full update from $\\mathbf{N}^k$ to $\\mathbf{N}^{k+1}$ is:\n1.  **Implicit Part:**\n    $$\n    N_2^* = \\frac{N_2^k}{1 + \\Delta t_s \\lambda_{\\beta}}\n    $$\n    $$\n    N_1^* = N_1^k + N_2^k - N_2^*\n    $$\n2.  **Explicit Part:**\n    $$\n    N_1^{k+1} = \\left(1 - \\frac{\\Delta t_s \\lambda_{\\text{cap}}}{m}\\right)^m N_1^*\n    $$\n    $$\n    N_2^{k+1} = N_1^* + N_2^* - N_1^{k+1}\n    $$\nThis set of equations defines the algorithm to be implemented for the IMEX solver.\n\n**Reference Solution and Error Metric**\nTo assess the accuracy, the IMEX solution at the final time $T$, denoted $\\mathbf{N}_{\\text{IMEX}}(T)$, is compared to a high-accuracy reference solution, $\\mathbf{N}_{\\text{ref}}(T)$. The reference solution is obtained by integrating the full, unsplit ODE system using a stiff solver based on the Backward Differentiation Formula (BDF), specifically `scipy.integrate.solve_ivp` with `method='BDF'` and stringent error tolerances. The full system is:\n$$\n\\frac{dN_1}{dt} = -\\lambda_{\\text{cap}} N_1 + \\lambda_{\\beta} N_2\n$$\n$$\n\\frac{dN_2}{dt} = +\\lambda_{\\text{cap}} N_1 - \\lambda_{\\beta} N_2\n$$\nThe comparison is quantified by the bias in the final waiting-point abundance:\n$$\n\\delta N_2 = N_{2,\\text{IMEX}}(T) - N_{2,\\text{ref}}(T)\n$$\nThis procedure is performed for each of the specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the specified test suite to compare\n    a custom IMEX integrator with a high-accuracy reference solver.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Format: (lambda_cap, lambda_beta, T_final, dt_s, m)\n    test_cases = [\n        (10.0, 1.0, 1.0, 0.1, 10),\n        (1000.0, 0.1, 0.1, 0.05, 100),\n        (5.0, 5.0, 1.0, 0.2, 5),\n        (100.0, 0.5, 0.5, 0.25, 25),\n    ]\n\n    # Initial condition for all cases\n    N0 = np.array([1.0, 0.0])\n\n    results = []\n    for case in test_cases:\n        l_cap, l_beta, T_final, dt_s, m = case\n\n        # Run the IMEX multirate solver\n        N_imex_final = run_imex_solver(l_cap, l_beta, T_final, dt_s, m, N0)\n\n        # Run the high-accuracy reference solver\n        N_ref_final = run_reference_solver(l_cap, l_beta, T_final, N0)\n\n        # Calculate the bias in the waiting-point abundance\n        delta_N2 = N_imex_final[1] - N_ref_final[1]\n        results.append(delta_N2)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\ndef run_imex_solver(l_cap, l_beta, T_final, dt_s, m, N0):\n    \"\"\"\n    Implements the derived IMEX multirate integrator.\n    \n    - Slow component (beta-decay) is advanced implicitly with backward-Euler.\n    - Fast component (capture) is advanced explicitly with m forward-Euler steps.\n    \"\"\"\n    # Ensure a whole number of steps\n    num_steps = int(round(T_final / dt_s))\n    \n    dt_f = dt_s / m\n    N = np.array(N0, dtype=float)\n\n    for _ in range(num_steps):\n        # Store current state\n        N_k = N\n\n        # Stage 1: Implicit slow update (Backward Euler for R_beta)\n        # N2_star = N_k[1] / (1 + dt_s * l_beta)\n        # N1_star = N_k[0] + dt_s * l_beta * N2_star\n        # Using the conservative form for N1_star for better numerical stability:\n        N2_star = N_k[1] / (1.0 + dt_s * l_beta)\n        N1_star = N_k[0] + N_k[1] - N2_star\n\n        N_star = np.array([N1_star, N2_star])\n\n        # Stage 2: Explicit fast updates (m steps of Forward Euler for R_cap)\n        # The m-step update can be expressed in a closed form derived in the solution.\n        # This avoids an inner loop for micro-steps.\n        factor = (1.0 - dt_f * l_cap)**m\n        N1_next = N_star[0] * factor\n        # Use conservation of particles for N2\n        N2_next = N_star[0] + N_star[1] - N1_next\n\n        N = np.array([N1_next, N2_next])\n\n    return N\n\ndef run_reference_solver(l_cap, l_beta, T_final, N0):\n    \"\"\"\n    Integrates the full unsplit system using a high-accuracy stiff (BDF) solver.\n    \"\"\"\n    def ode_system(t, N):\n        \"\"\" The right-hand side of the full ODE system. \"\"\"\n        N1, N2 = N\n        dN1_dt = -l_cap * N1 + l_beta * N2\n        dN2_dt =  l_cap * N1 - l_beta * N2\n        return np.array([dN1_dt, dN2_dt])\n\n    # `solve_ivp` with BDF method and tight tolerances for high accuracy.\n    sol = solve_ivp(\n        ode_system,\n        (0, T_final),\n        N0,\n        method='BDF',\n        rtol=1e-13,\n        atol=1e-13\n    )\n\n    # Return the state at the final time T_final\n    return sol.y[:, -1]\n\nsolve()\n```"
        }
    ]
}