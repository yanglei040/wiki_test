{
    "hands_on_practices": [
        {
            "introduction": "The simple Breit-Wigner formula provides an intuitive picture of a resonance, but it can fail to respect the fundamental principle of unitarity, especially when resonances overlap or couple to strong backgrounds. A more rigorous approach is the K-matrix formalism, which guarantees a unitary scattering amplitude by construction. This practice guides you through the implementation of this powerful method, showing how a real-valued K-matrix generates a physically correct, unitary scattering amplitude and provides a stable description of complex resonant structures. ",
            "id": "3596485",
            "problem": "You are asked to implement a single-channel elastic-scattering unitarization via a real-valued $K$-matrix for a background-plus-pole model in energy $E$ and to demonstrate how the mapping from $K(E)$ to the scattering matrix exhibits Breit–Wigner-like behavior while stabilizing the description in the presence of overlapping structures. Work in reduced units where the phase-space factor is a constant equal to $1$ (dimensionless), and treat the scattering amplitude as dimensionless. Energy must be expressed in mega-electron volts ($\\mathrm{MeV}$). Your program must produce a single line of output containing the results for the prescribed test suite as a comma-separated list enclosed in square brackets.\n\nStart from fundamental principles for single-channel, elastic scattering:\n\n- The scattering matrix ($S$-matrix) is unitary for real energies above threshold, so it has the form $S(E)=e^{2 i \\delta(E)}$ with a real phase shift $\\delta(E)$.\n- The partial-wave scattering amplitude $T(E)$ must be constructed such that unitarity holds and, for sufficiently small real interaction strength, $T(E)$ tends to the real interaction strength.\n\nUsing these principles, derive the unique algebraic mapping that takes a real-valued $K$-matrix $K(E)$ into a unitary $S$-matrix $S(E)$ and a corresponding amplitude $T(E)$ that matches $K(E)$ at small magnitude and enforces exact single-channel unitarity. Do not assume any specific form of $K(E)$ during the derivation; only use the unitarity condition and the small-interaction limit to fix the mapping.\n\nThen implement the following background-plus-pole model for the $K$-matrix:\n$$\nK(E) \\;=\\; a \\;+\\; b \\,\\big(E - E_{\\mathrm{ref}}\\big) \\;+\\; \\sum_{i=1}^{N} \\frac{g_i}{E_i - E}\\,,\n$$\nwhere $a$ and $b$ are real background parameters, $E_{\\mathrm{ref}}$ is a fixed reference energy, and the pole parameters are $\\{(E_i,g_i)\\}_{i=1}^{N}$ with $E_i$ the pole energy and $g_i$ a positive real number with dimensions of energy. All energies must be provided in $\\mathrm{MeV}$, and $K(E)$ is dimensionless in the chosen reduced units.\n\nFor reference and for the purpose of numerical comparison, also define a naive non-unitary amplitude by taking $T_{\\mathrm{naive}}(E) := K(E)$ and $S_{\\mathrm{naive}}(E) := 1 + 2 i \\, K(E)$.\n\nYour program must:\n\n- Derive and implement the unitarizing map from $K(E)$ to $S(E)$ and $T(E)$ in a manner that exactly enforces single-channel unitarity for real $E$.\n- Implement $K(E)$ for the above background-plus-pole model.\n- Evaluate the following test suite on specified uniform energy grids. Avoid numerical overflow or division by zero by handling or excluding energies exactly coinciding with $E_i$; all operations must remain robust and finite.\n\nDefinitions to be used in the metrics:\n\n- Let $I(E) := |T(E)|^2$, which is dimensionless.\n- Define the full width at half maximum (FWHM) of a single dominant peak in $I(E)$ as the difference between the two energies where $I(E)$ equals one half of its maximum value, using linear interpolation between grid points if needed.\n- Define the unitarity deviation as $\\max_{E}\\big||S(E)| - 1\\big|$ over the grid.\n- Define the stabilization ratio as\n$$\nR \\;=\\; \\frac{\\max_E |K(E)|}{\\max_E |T(E)|}\\,,\n$$\nwhere the maxima are taken over the evaluation grid after excluding any undefined or infinite values.\n\nTest suite:\n\n1. Isolated pole with weak background (Breit–Wigner-like line shape):\n   - Parameters: $a = 0.02$, $b = 0$, $E_{\\mathrm{ref}} = 1500\\,\\mathrm{MeV}$, one pole with $(E_1,g_1)=(1500\\,\\mathrm{MeV}, 25\\,\\mathrm{MeV})$.\n   - Grid: $E \\in [1450\\,\\mathrm{MeV}, 1550\\,\\mathrm{MeV}]$ with uniform spacing $\\Delta E = 0.2\\,\\mathrm{MeV}$.\n   - Required metric: the relative error between the numerically extracted $\\mathrm{FWHM}$ of $I(E)$ and the expected width $2 g_1$, i.e.,\n     $$\n     \\varepsilon_{\\Gamma} \\;=\\; \\frac{\\big|\\mathrm{FWHM} - 2 g_1\\big|}{2 g_1}\\,,\n     $$\n     reported as a float.\n\n2. Two overlapping poles (unitarity and stabilization):\n   - Parameters: $a = 0.1$, $b = 0$, $E_{\\mathrm{ref}} = 1000\\,\\mathrm{MeV}$, two poles $(E_1,g_1)=(1000\\,\\mathrm{MeV}, 60\\,\\mathrm{MeV})$ and $(E_2,g_2)=(1040\\,\\mathrm{MeV}, 40\\,\\mathrm{MeV})$.\n   - Grid: $E \\in [900\\,\\mathrm{MeV}, 1100\\,\\mathrm{MeV}]$ with uniform spacing $\\Delta E = 0.2\\,\\mathrm{MeV}$.\n   - Required metrics:\n     - The unitarity deviation $\\max_{E}\\big||S(E)| - 1\\big|$, reported as a float.\n     - The stabilization ratio $R$ as defined above, reported as a float.\n\n3. Strongly overlapping poles with sloped background (peak shift due to interference and background):\n   - Parameters: $a = 0.2$, $b = 0.001\\,\\mathrm{MeV}^{-1}$, $E_{\\mathrm{ref}} = 1300\\,\\mathrm{MeV}$, two poles $(E_1,g_1)=(1300\\,\\mathrm{MeV}, 70\\,\\mathrm{MeV})$ and $(E_2,g_2)=(1320\\,\\mathrm{MeV}, 60\\,\\mathrm{MeV})$.\n   - Grid: $E \\in [1200\\,\\mathrm{MeV}, 1400\\,\\mathrm{MeV}]$ with uniform spacing $\\Delta E = 0.2\\,\\mathrm{MeV}$.\n   - Required metric: the shift of the dominant peak near $E_1$ defined as $\\Delta E_{\\mathrm{peak}} = E_{\\mathrm{peak}} - E_1$, where $E_{\\mathrm{peak}}$ is the location (on the grid, with sub-grid refinement by local linear interpolation around the maximum if desired) of the local maximum of $I(E)$ within the window $[E_1 - 50\\,\\mathrm{MeV}, E_1 + 50\\,\\mathrm{MeV}]$. Report $\\Delta E_{\\mathrm{peak}}$ in $\\mathrm{MeV}$ as a float.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order:\n  - Test $1$: the float $\\varepsilon_{\\Gamma}$.\n  - Test $2$: the float unitarity deviation followed by the float stabilization ratio $R$.\n  - Test $3$: the float peak shift $\\Delta E_{\\mathrm{peak}}$ in $\\mathrm{MeV}$.\n- For example: [$x_1,x_2,x_3,x_4$], where each $x_i$ is a float. All computations must use energies in $\\mathrm{MeV}$, and all reported quantities are dimensionless except for the last, which must be in $\\mathrm{MeV}$.",
            "solution": "The user has provided a problem in computational nuclear physics concerning the unitarization of a scattering amplitude using the K-matrix formalism. The problem is scientifically well-grounded, self-contained, and algorithmically specified. It represents a standard and valid exercise in scattering theory.\n\nThe problem requires a derivation of the mapping from a real K-matrix, $K(E)$, to a unitary single-channel S-matrix, $S(E)$, and the corresponding scattering amplitude, $T(E)$. This derivation must be based on fundamental principles. Subsequently, this formalism is to be applied to a specific background-plus-pole model for $K(E)$, and several numerical metrics are to be computed for defined test cases.\n\nLet us begin with the derivation.\n\nIn single-channel elastic scattering, the S-matrix, which relates the initial and final states of a scattering process, is a complex function of energy, $S(E)$. For real energies $E$ above the production threshold, the conservation of probability flux requires the S-matrix to be unitary. For a single channel, this means $S(E)^{\\dagger}S(E) = |S(E)|^2 = 1$. This implies that $S(E)$ can be parameterized by a single real quantity, the phase shift $\\delta(E)$, as $S(E) = e^{2i\\delta(E)}$.\n\nThe scattering amplitude, $T(E)$, is related to the S-matrix. In the normalization specified by the problem, where the phase-space factor $\\rho(E)$ is set to a constant value of $1$, the relationship is:\n$$\nS(E) = 1 + 2iT(E)\n$$\nThe unitarity of $S(E)$ imposes a constraint on $T(E)$. Substituting the expression for $S(E)$ into the unitarity condition $|S(E)|^2 = 1$:\n$$\n|1 + 2iT(E)|^2 = 1\n$$\n$$\n(1 - 2iT(E)^{\\dagger})(1 + 2iT(E)) = 1\n$$\n$$\n1 + 2iT(E) - 2iT(E)^{\\dagger} + 4T(E)^{\\dagger}T(E) = 1\n$$\n$$\n2i(T(E) - T(E)^{\\dagger}) + 4|T(E)|^2 = 0\n$$\nSince $T - T^{\\dagger} = 2i\\,\\mathrm{Im}(T)$, this simplifies to:\n$$\n-4\\,\\mathrm{Im}(T(E)) + 4|T(E)|^2 = 0\n$$\n$$\n\\mathrm{Im}(T(E)) = |T(E)|^2\n$$\nThis is the single-channel unitarity condition, also known as the optical theorem in this normalization.\n\nThe K-matrix, $K(E)$, is introduced as a real, symmetric matrix (for multiple channels) that parameterizes the interaction. For a single channel, $K(E)$ is a real-valued function for real energies. The problem requires that the resulting scattering amplitude $T(E)$ must reduce to $K(E)$ in the limit of a small interaction strength, i.e., $T(E) \\approx K(E)$ for $|K(E)| \\ll 1$.\n\nTo find the mapping from $K(E)$ to $T(E)$, we seek a functional form $T(K)$ that satisfies the two conditions: $\\mathrm{Im}(T) = |T|^2$ and $T \\approx K$ for small $K$. A common and successful ansatz relates the inverse of the T-matrix to the inverse of the K-matrix:\n$$\nT(E)^{-1} = K(E)^{-1} - i\n$$\nSolving for $T(E)$:\n$$\nT(E) = \\left(K(E)^{-1} - i\\right)^{-1} = \\left(\\frac{1 - iK(E)}{K(E)}\\right)^{-1} = \\frac{K(E)}{1 - iK(E)}\n$$\nLet us verify that this mapping satisfies the required conditions.\n1.  **Small interaction limit**: If $|K(E)| \\ll 1$, the denominator $(1 - iK(E)) \\approx 1$. Thus, $T(E) \\approx K(E)$, which satisfies the condition.\n2.  **Unitarity**: We check if $\\mathrm{Im}(T) = |T|^2$. First, we rationalize the expression for $T(E)$:\n    $$\n    T(E) = \\frac{K(E)}{1 - iK(E)} \\cdot \\frac{1 + iK(E)}{1 + iK(E)} = \\frac{K(E) + iK(E)^2}{1 + K(E)^2}\n    $$\n    The imaginary part is $\\mathrm{Im}(T(E)) = \\frac{K(E)^2}{1 + K(E)^2}$.\n    The squared magnitude is $|T(E)|^2 = \\left|\\frac{K(E)}{1 - iK(E)}\\right|^2 = \\frac{|K(E)|^2}{|1 - iK(E)|^2} = \\frac{K(E)^2}{1^2 + (-K(E))^2} = \\frac{K(E)^2}{1 + K(E)^2}$.\n    Since $\\mathrm{Im}(T(E)) = |T(E)|^2$, the mapping correctly enforces single-channel unitarity.\n\nThe S-matrix can now be expressed in terms of $K(E)$:\n$$\nS(E) = 1 + 2iT(E) = 1 + 2i\\frac{K(E)}{1 - iK(E)} = \\frac{1 - iK(E) + 2iK(E)}{1 - iK(E)} = \\frac{1 + iK(E)}{1 - iK(E)}\n$$\nThis form makes unitarity manifest, as for real $K(E)$, the numerator is the complex conjugate of the denominator, so $|S(E)|=1$.\n\nThe connection to a Breit-Wigner resonance form is apparent for a single-pole K-matrix without background, $K(E) = \\frac{g_1}{E_1 - E}$. Substituting this into the formula for $T(E)$:\n$$\nT(E) = \\frac{\\frac{g_1}{E_1 - E}}{1 - i\\frac{g_1}{E_1 - E}} = \\frac{g_1}{(E_1 - E) - ig_1}\n$$\nThe intensity $I(E) = |T(E)|^2$ is then:\n$$\nI(E) = \\frac{g_1^2}{(E_1 - E)^2 + g_1^2}\n$$\nThis is the standard Breit-Wigner lineshape. The maximum intensity is $I(E_1)=1$. The full width at half maximum (FWHM) is found by setting $I(E) = 1/2$, which gives $(E_1 - E)^2 = g_1^2$, or $E = E_1 \\pm g_1$. The FWHM is therefore $(E_1+g_1) - (E_1-g_1) = 2g_1$. This justifies the reference width in Test Case 1.\n\nThe K-matrix formalism is a powerful tool because it guarantees unitarity. A naive model where one simply adds amplitudes, for example $T_{\\mathrm{naive}}(E) \\approx K(E)$, can easily violate the unitarity bound $|T(E)| \\leq 1$, especially for overlapping resonances where $|K(E)|$ can become large. The unitarization map $T = K/(1-iK)$ naturally \"stabilizes\" the amplitude, ensuring $|T(E)| \\leq 1$ for all real $E$. This is the physical and mathematical basis for the computations that follow.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import interp1d\n\ndef solve():\n    \"\"\"\n    Solves the computational nuclear physics problem involving K-matrix unitarization.\n    It processes three distinct test cases and computes the required metrics for each.\n    \"\"\"\n    \n    def get_K(E, a, b, E_ref, poles):\n        \"\"\"\n        Calculates the K-matrix for a given energy or array of energies.\n        \n        Args:\n            E (np.ndarray or float): Energy/ies in MeV.\n            a (float): Background parameter.\n            b (float): Background slope parameter in MeV^-1.\n            E_ref (float): Reference energy in MeV.\n            poles (list of tuples): List of (E_i, g_i) pole parameters in MeV.\n            \n        Returns:\n            np.ndarray or float: The value of the K-matrix.\n        \"\"\"\n        k_val = a + b * (E - E_ref)\n        for E_i, g_i in poles:\n            # The calling context ensures that E != E_i.\n            k_val += g_i / (E_i - E)\n        return k_val\n\n    results = []\n\n    # Test Case 1: Isolated pole with weak background\n    a1, b1, E_ref1 = 0.02, 0.0, 1500.0\n    poles1 = [(1500.0, 25.0)]\n    g1 = poles1[0][1]\n    E_grid1_full = np.arange(1450.0, 1550.0 + 0.1, 0.2)\n    \n    pole_energies1 = [p[0] for p in poles1]\n    mask1 = ~np.isin(E_grid1_full, pole_energies1)\n    E_grid1 = E_grid1_full[mask1]\n\n    K1 = get_K(E_grid1, a1, b1, E_ref1, poles1)\n    I1 = K1**2 / (1 + K1**2)\n\n    I_max1 = np.max(I1)\n    I_half1 = I_max1 / 2.0\n    \n    peak_grid_idx = np.searchsorted(E_grid1, poles1[0][0])\n    \n    # Left side of the peak\n    E_left = E_grid1[:peak_grid_idx]\n    I_left = I1[:peak_grid_idx]\n    E_low = np.nan\n    if len(I_left) > 1 and I_left[0] < I_half1 < I_left[-1]:\n        f_left = interp1d(I_left, E_left, kind='linear')\n        E_low = f_left(I_half1)\n\n    # Right side of the peak\n    E_right = E_grid1[peak_grid_idx:]\n    I_right = I1[peak_grid_idx:]\n    E_high = np.nan\n    if len(I_right) > 1 and I_right[-1] < I_half1 < I_right[0]:\n        f_right = interp1d(I_right[::-1], E_right[::-1], kind='linear')\n        E_high = f_right(I_half1)\n\n    fwhm = E_high - E_low\n    expected_width = 2 * g1\n    eps_Gamma = np.abs(fwhm - expected_width) / expected_width\n    results.append(eps_Gamma)\n\n    # Test Case 2: Two overlapping poles\n    a2, b2, E_ref2 = 0.1, 0.0, 1000.0\n    poles2 = [(1000.0, 60.0), (1040.0, 40.0)]\n    E_grid2_full = np.arange(900.0, 1100.0 + 0.1, 0.2)\n    \n    pole_energies2 = [p[0] for p in poles2]\n    mask2 = ~np.isin(E_grid2_full, pole_energies2)\n    E_grid2 = E_grid2_full[mask2]\n    \n    K2 = get_K(E_grid2, a2, b2, E_ref2, poles2)\n    \n    # Metric 1: Unitarity deviation\n    S2 = (1 + 1j * K2) / (1 - 1j * K2)\n    unitarity_dev = np.max(np.abs(np.abs(S2) - 1))\n    results.append(unitarity_dev)\n    \n    # Metric 2: Stabilization ratio\n    max_K_abs = np.max(np.abs(K2))\n    T2 = K2 / (1 - 1j * K2)\n    max_T_abs = np.max(np.abs(T2))\n    stabilization_ratio = max_K_abs / max_T_abs\n    results.append(stabilization_ratio)\n\n    # Test Case 3: Strongly overlapping poles with sloped background\n    a3, b3, E_ref3 = 0.2, 0.001, 1300.0\n    poles3 = [(1300.0, 70.0), (1320.0, 60.0)]\n    E1_3 = poles3[0][0]\n    grid_step = 0.2\n    E_grid3_full = np.arange(1200.0, 1400.0 + 0.1, grid_step)\n\n    pole_energies3 = [p[0] for p in poles3]\n    mask3 = ~np.isin(E_grid3_full, pole_energies3)\n    E_grid3 = E_grid3_full[mask3]\n\n    K3 = get_K(E_grid3, a3, b3, E_ref3, poles3)\n    I3 = K3**2 / (1 + K3**2)\n    \n    # Metric: Peak shift\n    window_mask = (E_grid3 >= (E1_3 - 50.0)) & (E_grid3 <= (E1_3 + 50.0))\n    E_window = E_grid3[window_mask]\n    I_window = I3[window_mask]\n    \n    peak_idx_window = np.argmax(I_window)\n    \n    # Quadratic interpolation for sub-grid peak position\n    if 0 < peak_idx_window < len(I_window) - 1:\n        p_idx = peak_idx_window\n        I_m1, I_0, I_p1 = I_window[p_idx-1], I_window[p_idx], I_window[p_idx+1]\n        E_0 = E_window[p_idx]\n        \n        denominator = I_m1 - 2*I_0 + I_p1\n        if np.abs(denominator) > 1e-12: # Avoid division by zero for linear regions\n            p_offset = 0.5 * (I_m1 - I_p1) / denominator\n            E_peak = E_0 + p_offset * grid_step\n        else:\n            E_peak = E_0 \n    else:\n        # Peak is at the edge of the window, no interpolation\n        E_peak = E_window[peak_idx_window]\n\n    delta_E_peak = E_peak - E1_3\n    results.append(delta_E_peak)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Extracting resonance parameters is more than just finding the best-fit curve; it requires a careful assessment of the uncertainties and interdependencies of those parameters. This practice focuses on the crucial post-fit step of analyzing the parameter covariance matrix, which is the key to understanding the precision of your results. By fitting a resonance model under different simulated experimental conditions, you will explore how factors like data sampling and background levels can introduce strong correlations, particularly between the resonance's energy ($E_R$) and width ($\\Gamma$), impacting their identifiability. ",
            "id": "3596468",
            "problem": "You are asked to write a complete and runnable program that performs a weighted nonlinear least-squares fit to a physically motivated resonance model in a nuclear scattering cross section and then analyzes the parameter correlations and identifiability of the resonance energy and width. The model consists of a single isolated resonance described by a Lorentzian line shape consistent with probability conservation and unitarity, plus a smooth linear background. Your program must begin from the definitions of the scattering matrix and the basic structure of resonant scattering and justify the model used for the fit within the code comments. The fitting should be conducted for synthetic datasets with known measurement uncertainties, treated as independent and Gaussian, and the parameter covariance must be estimated from the Fisher information associated with the weighted least-squares objective. The program must compute the covariance matrix for the resonance energy and width, the corresponding correlation coefficient, and an identifiability flag based on a numerically specified criterion.\n\nDefinitions and requirements:\n\n- Let the independent variable be the center-of-mass energy $E$ in $\\mathrm{MeV}$ and the observable be the cross section $y(E)$ in barns. The resonant structure is characterized by a pole in the scattering matrix close to the real energy axis with pole position $E_R - i \\Gamma/2$, where $E_R$ is the resonance energy in $\\mathrm{MeV}$ and $\\Gamma$ is the total width in $\\mathrm{MeV}$. The background is assumed to be slowly varying over the energy window.\n- Define the parameter vector as $\\theta = (E_R, \\Gamma, A, B_0, B_1)$, where $A$ is a positive amplitude parameter in barns, $B_0$ is a background intercept in barns, and $B_1$ is a background slope in barns per $\\mathrm{MeV}$.\n- Given data points $\\{(E_i, y_i, \\sigma_i)\\}_{i=1}^N$ with $N$ points, where $\\sigma_i$ are known standard deviations in barns, the weighted nonlinear least-squares objective is\n$$\n\\chi^2(\\theta) = \\sum_{i=1}^N \\frac{\\left[y_i - y(E_i;\\theta)\\right]^2}{\\sigma_i^2}.\n$$\n- At the best-fit $\\hat{\\theta}$, the parameter covariance matrix is approximated by the inverse of the Fisher information for independent Gaussian errors,\n$$\n\\mathrm{Cov}(\\hat{\\theta}) \\approx \\left(J^\\top W J\\right)^{-1},\n$$\nwhere $J$ is the Jacobian matrix with $J_{ij} = \\partial y(E_i;\\theta)/\\partial \\theta_j$ evaluated at $\\hat{\\theta}$, and $W$ is diagonal with $W_{ii} = 1/\\sigma_i^2$.\n- Extract the $2\\times 2$ covariance submatrix corresponding to $(E_R, \\Gamma)$, compute its correlation coefficient\n$$\n\\rho_{E_R,\\Gamma} = \\frac{\\mathrm{Cov}(E_R,\\Gamma)}{\\sqrt{\\mathrm{Var}(E_R)\\,\\mathrm{Var}(\\Gamma)}},\n$$\nand compute the $2$-norm condition number of this $2\\times 2$ covariance submatrix,\n$$\n\\kappa = \\|C\\|_2 \\,\\|C^{-1}\\|_2,\n$$\nwhere $C$ is the covariance submatrix for $(E_R,\\Gamma)$.\n- Define an identifiability flag $I$ for $(E_R,\\Gamma)$ as follows: set $I$ to $\\mathrm{False}$ (not identifiable) if either $|\\rho_{E_R,\\Gamma}| > 0.95$ or $\\kappa > 10^8$, and set $I$ to $\\mathrm{True}$ otherwise.\n\nYour program must generate synthetic datasets using a resonance line shape consistent with single-level resonant scattering and a linear background. Energies must be in $\\mathrm{MeV}$, and cross sections must be in barns. Use evenly spaced energy grids symmetric about $E_R$, with the half-width of the window proportional to $\\Gamma$. Do not add random noise; use the model to generate $y_i$ exactly and assign a constant $\\sigma_i$ per test case.\n\nImplement a weighted nonlinear least-squares fit for each test case with the absolute weighting of the uncertainties enforced, and then compute:\n- The correlation coefficient $\\rho_{E_R,\\Gamma}$ (dimensionless).\n- The condition number $\\kappa$ (dimensionless).\n- The identifiability flag $I$ (boolean).\n\nTest suite:\n\nProvide and solve the following three test cases. For each case, energies must be expressed in $\\mathrm{MeV}$, widths in $\\mathrm{MeV}$, cross sections in barns, and uncertainties in barns. Each energy grid must run from $E_{\\min} = E_R - m\\,\\Gamma$ to $E_{\\max} = E_R + m\\,\\Gamma$ with $N$ evenly spaced points, where $m$ is a dimensionless multiplier:\n\n- Case $1$ (well-sampled resonance, moderate background):\n  - $E_R = 5.0\\,\\mathrm{MeV}$, $\\Gamma = 0.2\\,\\mathrm{MeV}$, $A = 10.0\\,\\mathrm{barns}$, $B_0 = 1.0\\,\\mathrm{barns}$, $B_1 = 0.2\\,\\mathrm{barns}/\\mathrm{MeV}$,\n  - $m = 8$, $N = 121$, $\\sigma_i \\equiv 0.05\\,\\mathrm{barns}$ for all $i$.\n\n- Case $2$ (narrow resonance with coarse sampling):\n  - $E_R = 5.0\\,\\mathrm{MeV}$, $\\Gamma = 0.01\\,\\mathrm{MeV}$, $A = 10.0\\,\\mathrm{barns}$, $B_0 = 1.0\\,\\mathrm{barns}$, $B_1 = 0.2\\,\\mathrm{barns}/\\mathrm{MeV}$,\n  - $m = 6$, $N = 21$, $\\sigma_i \\equiv 0.05\\,\\mathrm{barns}$ for all $i$.\n\n- Case $3$ (limited window with strong background slope):\n  - $E_R = 5.0\\,\\mathrm{MeV}$, $\\Gamma = 0.5\\,\\mathrm{MeV}$, $A = 5.0\\,\\mathrm{barns}$, $B_0 = 1.0\\,\\mathrm{barns}$, $B_1 = 5.0\\,\\mathrm{barns}/\\mathrm{MeV}$,\n  - $m = 2$, $N = 41$, $\\sigma_i \\equiv 0.05\\,\\mathrm{barns}$ for all $i$.\n\nAlgorithmic expectations:\n\n- Your code must justify the resonance line shape from fundamental principles of resonant scattering and implement the weighted nonlinear least-squares fit using a modern numerical routine.\n- Compute the parameter covariance using the appropriate weighting with $\\sigma_i$ treated as absolute uncertainties.\n- Extract the $(E_R,\\Gamma)$ covariance submatrix, compute $\\rho_{E_R,\\Gamma}$ and $\\kappa$, and set the identifiability flag $I$ as per the criterion above.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the form $[\\rho_{E_R,\\Gamma}, \\kappa, I]$. For example, the printed line must look like\n$[[r_1, c_1, b_1],[r_2, c_2, b_2],[r_3, c_3, b_3]]$,\nwhere $r_k$ and $c_k$ are floats and $b_k$ is a boolean for case $k$.",
            "solution": "The problem requires the implementation of a weighted nonlinear least-squares fit to a physically motivated resonance model and a subsequent analysis of parameter correlations and identifiability. The solution is structured around three core components: the definition of the physical model, the numerical fitting procedure, and the statistical analysis of the parameter uncertainties.\n\n**1. The Physical Model: Breit-Wigner Resonance**\n\nThe model for the nuclear scattering cross section is based on the fundamental principles of resonant scattering in quantum mechanics. A resonance corresponds to the formation of a quasi-stable intermediate state with a characteristic energy $E_R$ and a finite lifetime, which is inversely proportional to its decay width $\\Gamma$. The presence of such a state leads to a sharp peak in the energy-dependent cross section.\n\nThis behavior is formally described by the scattering matrix ($S$-matrix). To conserve probability, the $S$-matrix must be unitary, i.e., $S^\\dagger S = I$. For a single, isolated resonance in a single scattering channel, the $S$-matrix element can be parameterized in the vicinity of the resonance by a pole in the complex energy plane at $E = E_R - i\\Gamma/2$. This leads to the Breit-Wigner formula for the $S$-matrix element:\n$$\nS(E) \\approx e^{2i\\delta_{bg}} \\frac{E - E_R - i\\Gamma/2}{E - E_R + i\\Gamma/2}\n$$\nwhere $\\delta_{bg}$ represents a slowly varying background phase shift. For our phenomenological model, we can simplify by setting $\\delta_{bg}=0$. The resonant part of the elastic scattering cross section, $\\sigma_{\\text{res}}(E)$, is proportional to $|1 - S(E)|^2$. A direct calculation yields:\n$$\n\\sigma_{\\text{res}}(E) \\propto \\left|1 - \\frac{E - E_R - i\\Gamma/2}{E - E_R + i\\Gamma/2}\\right|^2 = \\left|\\frac{(E - E_R + i\\Gamma/2) - (E - E_R - i\\Gamma/2)}{E - E_R + i\\Gamma/2}\\right|^2 = \\left|\\frac{i\\Gamma}{E - E_R + i\\Gamma/2}\\right|^2 = \\frac{\\Gamma^2}{(E - E_R)^2 + (\\Gamma/2)^2}\n$$\nThis function has a Lorentzian shape. The problem specifies a model combining this resonant structure with a simple linear background. The complete model for the cross section $y(E)$ is therefore defined as:\n$$\ny(E; \\theta) = A \\frac{(\\Gamma/2)^2}{(E - E_R)^2 + (\\Gamma/2)^2} + B_0 + B_1 E\n$$\nThe parameter vector is $\\theta = (E_R, \\Gamma, A, B_0, B_1)$, where $E_R$ is the resonance energy, $\\Gamma$ is its full width at half maximum (FWHM), $A$ is the peak amplitude of the resonant part, and $B_0$ and $B_1$ are the intercept and slope of the linear background, respectively.\n\n**2. Weighted Nonlinear Least-Squares Fitting**\n\nThe task is to find the best-fit parameters $\\hat{\\theta}$ for a given set of synthetic data points $\\{(E_i, y_i, \\sigma_i)\\}_{i=1}^N$. This is achieved by minimizing the weighted sum of squared residuals, the chi-squared statistic $\\chi^2$:\n$$\n\\chi^2(\\theta) = \\sum_{i=1}^N \\left(\\frac{y_i - y(E_i;\\theta)}{\\sigma_i}\\right)^2\n$$\nMinimizing this value is a nonlinear least-squares problem. The algorithm will utilize `scipy.optimize.curve_fit`, a standard routine based on the Levenberg-Marquardt algorithm. The synthetic data for each test case is generated from the model itself without adding random noise, meaning the true parameters $\\theta_{\\text{true}}$ correspond to the global minimum where $\\chi^2(\\theta_{\\text{true}}) = 0$. The fit is initialized with $\\theta_{\\text{true}}$ to ensure immediate and exact convergence. The `absolute_sigma=True` argument is passed to `curve_fit` to ensure the uncertainties $\\sigma_i$ are treated as absolute values, which is critical for the correct scaling of the output covariance matrix.\n\n**3. Parameter Covariance and Identifiability Analysis**\n\nThe final step is to assess the identifiability of the resonance energy $E_R$ and width $\\Gamma$. This is done by analyzing the parameter covariance matrix, $\\mathrm{Cov}(\\hat{\\theta})$, which quantifies the uncertainties and correlations of the best-fit parameters. In nonlinear least-squares, this matrix is approximated by the inverse of the Fisher information matrix. For independent Gaussian errors, this is:\n$$\n\\mathrm{Cov}(\\hat{\\theta}) \\approx (J^\\top W J)^{-1}\n$$\nwhere $J$ is the Jacobian matrix of the model with respect to the parameters, $J_{ij} = \\frac{\\partial y(E_i;\\theta)}{\\partial \\theta_j}$, evaluated at the best-fit solution $\\hat{\\theta}$, and $W$ is the diagonal weight matrix with elements $W_{ii} = 1/\\sigma_i^2$. The `curve_fit` function with `absolute_sigma=True` returns exactly this matrix.\n\nThe analysis focuses on the $2 \\times 2$ submatrix $C$ of $\\mathrm{Cov}(\\hat{\\theta})$ corresponding to the parameters $(E_R, \\Gamma)$:\n$$\nC = \\begin{pmatrix} \\mathrm{Var}(E_R) & \\mathrm{Cov}(E_R, \\Gamma) \\\\ \\mathrm{Cov}(\\Gamma, E_R) & \\mathrm{Var}(\\Gamma) \\end{pmatrix}\n$$\nFrom this submatrix, two metrics are computed:\na) The **correlation coefficient** $\\rho_{E_R,\\Gamma}$, which measures the linear interdependence between the estimators for $E_R$ and $\\Gamma$:\n$$\n\\rho_{E_R,\\Gamma} = \\frac{C_{12}}{\\sqrt{C_{11}C_{22}}}\n$$\nA value of $|\\rho_{E_R,\\Gamma}|$ approaching $1$ indicates a strong correlation, which makes it difficult to disentangle the effects of the two parameters.\n\nb) The **$2$-norm condition number** $\\kappa$ of the covariance submatrix $C$, defined as $\\kappa = \\|C\\|_2 \\|C^{-1}\\|_2$. For a symmetric positive definite matrix like $C$, this is the ratio of its largest to its smallest eigenvalue, $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$. A large condition number signifies that the problem of determining $E_R$ and $\\Gamma$ is ill-conditioned, meaning a small perturbation in the data could cause a large change in the parameter estimates.\n\nFinally, an **identifiability flag** $I$ is determined based on the specified criteria: $I$ is set to $\\mathrm{False}$ if $|\\rho_{E_R,\\Gamma}| > 0.95$ or if $\\kappa > 10^8$, and to $\\mathrm{True}$ otherwise. The program implements this full workflow for the three provided test cases, each representing a different experimental scenario, and outputs the resulting tuple $[\\rho_{E_R,\\Gamma}, \\kappa, I]$ for each.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nfrom numpy.linalg import cond\n\ndef solve():\n    \"\"\"\n    Performs a weighted nonlinear least-squares fit of a Breit-Wigner resonance model\n    to synthetic data and analyzes the parameter identifiability.\n    \"\"\"\n\n    # --- Model Definition and Justification ---\n    # In nuclear scattering, an isolated resonance can be described by the Breit-Wigner formula.\n    # This arises from parameterizing the S-matrix near a pole E_R - i*Gamma/2 on the complex energy plane,\n    # a consequence of a quasi-bound state with finite lifetime. For single-channel scattering,\n    # the cross-section exhibits a Lorentzian energy dependence. The phenomenological model used here\n    # combines a Lorentzian peak with a linear background, which is a common approach for analyzing\n    # experimental data. The model is:\n    # y(E) = A * (Gamma/2)^2 / ((E - E_R)^2 + (Gamma/2)^2) + B_0 + B_1 * E\n    # The parameters are theta = (E_R, Gamma, A, B_0, B_1).\n    def resonance_model(E, E_R, Gamma, A, B0, B1):\n        \"\"\"\n        Defines the resonance model: a Breit-Wigner-like Lorentzian plus a linear background.\n\n        Args:\n            E (float or array): Center-of-mass energy in MeV.\n            E_R (float): Resonance energy in MeV.\n            Gamma (float): Resonance total width in MeV.\n            A (float): Resonance amplitude in barns.\n            B0 (float): Background intercept in barns.\n            B1 (float): Background slope in barns/MeV.\n\n        Returns:\n            float or array: Cross section in barns.\n        \"\"\"\n        lorentzian = A * (Gamma / 2.0)**2 / ((E - E_R)**2 + (Gamma / 2.0)**2)\n        background = B0 + B1 * E\n        return lorentzian + background\n\n    test_cases = [\n        # Case 1: Well-sampled resonance, moderate background.\n        {\"params\": (5.0, 0.2, 10.0, 1.0, 0.2), \"m\": 8.0, \"N\": 121, \"sigma\": 0.05},\n        # Case 2: Narrow resonance with coarse sampling.\n        {\"params\": (5.0, 0.01, 10.0, 1.0, 0.2), \"m\": 6.0, \"N\": 21, \"sigma\": 0.05},\n        # Case 3: Limited window with strong background slope.\n        {\"params\": (5.0, 0.5, 5.0, 1.0, 5.0), \"m\": 2.0, \"N\": 41, \"sigma\": 0.05},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        p_true = case[\"params\"]\n        E_R_true, Gamma_true, _, _, _ = p_true\n        m, N, sigma_val = case[\"m\"], case[\"N\"], case[\"sigma\"]\n\n        # --- Step 1: Generate Synthetic Data ---\n        # The data is generated without noise, so y_i = y(E_i; theta_true).\n        E_min = E_R_true - m * Gamma_true\n        E_max = E_R_true + m * Gamma_true\n        E_data = np.linspace(E_min, E_max, N)\n        y_data = resonance_model(E_data, *p_true)\n        sigma_data = np.full_like(y_data, sigma_val)\n\n        # --- Step 2: Perform Weighted Nonlinear Least-Squares Fit ---\n        # We use the true parameters as the initial guess to ensure convergence\n        # to the exact solution, as the data is noise-free.\n        # 'absolute_sigma=True' is crucial. It ensures the covariance matrix pcov\n        # is calculated as (J^T W J)^-1, where W_ii = 1/sigma_i^2, as required.\n        try:\n            _, pcov = curve_fit(\n                resonance_model,\n                E_data,\n                y_data,\n                p0=p_true,\n                sigma=sigma_data,\n                absolute_sigma=True,\n                check_finite=True\n            )\n        except RuntimeError:\n            # In ill-conditioned cases, the fit might fail.\n            # Represent this scenario with NaN values.\n            all_results.append([np.nan, np.nan, False])\n            continue\n        \n        # --- Step 3: Analyze Parameter Covariance and Identifiability ---\n        # Extract the 2x2 covariance submatrix for (E_R, Gamma).\n        # The order of parameters is (E_R, Gamma, A, B0, B1).\n        C_ER_Gamma = pcov[0:2, 0:2]\n\n        # Compute the correlation coefficient rho_{E_R, Gamma}.\n        var_ER = C_ER_Gamma[0, 0]\n        var_Gamma = C_ER_Gamma[1, 1]\n        cov_ER_Gamma = C_ER_Gamma[0, 1]\n        \n        # Avoid division by zero if variances are zero or negative (numerical instability)\n        if var_ER <= 0 or var_Gamma <= 0:\n            rho = np.nan\n        else:\n            rho = cov_ER_Gamma / np.sqrt(var_ER * var_Gamma)\n\n        # Compute the 2-norm condition number kappa of the submatrix.\n        kappa = cond(C_ER_Gamma, 2)\n\n        # Determine the identifiability flag I based on the given criteria.\n        is_identifiable = not (np.abs(rho) > 0.95 or kappa > 1e8)\n\n        # Append results for this case. Ensure boolean is a native Python bool.\n        all_results.append([rho, kappa, bool(is_identifiable)])\n\n    # --- Final Output Formatting ---\n    # The output must be a single line: [[r1, c1, b1],[r2, c2, b2],...].\n    # The `str` representation of a list of lists matches this format.\n    formatted_results = \",\".join(map(str, all_results))\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Nonlinear least-squares fitting is a cornerstone of data analysis, but its algorithms can be numerically sensitive and prone to instability. This practice takes you under the hood of the fitting process to diagnose its health by examining the Jacobian matrix, which maps small changes in parameters to changes in the model. You will learn to identify and quantify ill-conditioning—a primary cause of slow or failed convergence—and apply powerful techniques like parameter scaling and regularization to stabilize the fit and ensure reliable results. ",
            "id": "3596455",
            "problem": "You are given a nonlinear least squares model for a single nuclear resonance with a polynomial background, where the dependent variable is a cross section as a function of energy. The model assumes the observed data points $\\{(E_i, y_i, \\sigma_i)\\}_{i=1}^N$ with energy $E_i$ in $\\mathrm{MeV}$, cross section $y_i$ in $\\mathrm{barn}$, and measurement uncertainty $\\sigma_i$ in $\\mathrm{barn}$. The parametric model is a sum of a Breit-Wigner resonance and a polynomial background,\n$$\n\\hat{y}(E;\\boldsymbol{\\theta}) = A\\,S(E;E_0,\\Gamma) + \\sum_{k=0}^{m} c_k\\,x(E)^k,\n$$\nwhere $A$ is the resonance amplitude in $\\mathrm{barn}$, $E_0$ is the resonance centroid in $\\mathrm{MeV}$, $\\Gamma$ is the total width in $\\mathrm{MeV}$, and the background coefficients $c_k$ are in $\\mathrm{barn}$. The normalized background basis uses\n$$\nx(E) = \\frac{E - E_c}{E_s},\n$$\nwith $E_c$ the center of the energy window (in $\\mathrm{MeV}$) and $E_s$ its half-width (in $\\mathrm{MeV}$) so that $x(E) \\in [-1,1]$. The nonrelativistic Breit-Wigner shape is\n$$\nS(E;E_0,\\Gamma) = \\frac{(\\Gamma/2)^2}{(E - E_0)^2 + (\\Gamma/2)^2}.\n$$\n\nConsider the residual vector of the unweighted least squares,\n$$\n\\mathbf{r}(\\boldsymbol{\\theta}) = \\mathbf{y} - \\hat{\\mathbf{y}}(\\boldsymbol{\\theta}),\n$$\nand its Jacobian matrix $J(\\boldsymbol{\\theta}) = \\partial \\mathbf{r}/\\partial \\boldsymbol{\\theta} \\in \\mathbb{R}^{N \\times P}$ where $P = m+4$ is the number of parameters. The conditioning of $J$ directly affects Gauss-Newton convergence. Your tasks are to compute the Jacobian, evaluate its conditioning, and investigate how scaling and simple Gaussian priors improve the conditioning and the positive definiteness of the approximate Hessian. Use the following fundamental bases:\n\n- The least squares residual definition $\\mathbf{r}(\\boldsymbol{\\theta}) = \\mathbf{y} - \\hat{\\mathbf{y}}(\\boldsymbol{\\theta})$.\n- The Jacobian definition $J(\\boldsymbol{\\theta}) = \\partial \\mathbf{r}/\\partial \\boldsymbol{\\theta}$.\n- The singular value decomposition condition number $\\kappa(J) = \\sigma_{\\max}(J)/\\sigma_{\\min}(J)$ with $\\sigma_{\\max}$, $\\sigma_{\\min}$ the largest and smallest singular values.\n- The Gauss-Newton approximate Hessian $H \\approx J^\\top J$ for unweighted least squares, and $H \\approx J_w^\\top J_w$ for weighted least squares with $J_w = WJ$ and $W = \\mathrm{diag}(1/\\sigma_i)$.\n\nProceed from first principles and compute the Jacobian entries by direct differentiation of $\\hat{y}(E;\\boldsymbol{\\theta})$. Do not assume any provided shortcuts. Then:\n\n1. Compute the unweighted Jacobian $J(\\boldsymbol{\\theta})$ at a specified initial guess $\\boldsymbol{\\theta}_0$ and the corresponding condition number $\\kappa_{\\mathrm{raw}}$.\n2. Define a scaled Jacobian $J_{\\mathrm{scaled}} = W J C^{-1}$ where $W = \\mathrm{diag}(1/\\sigma_i)$ whitens the residuals, and $C$ is a diagonal matrix of column norms of $WJ$ such that each column of $J_{\\mathrm{scaled}}$ has unit Euclidean norm. Compute $\\kappa_{\\mathrm{scaled}}$.\n3. Construct a Gaussian prior precision matrix $\\Lambda = \\mathrm{diag}(\\lambda_j)$ for the parameters with independent priors:\n   - $A \\sim \\mathcal{N}(0, \\sigma_A^2)$ with $\\sigma_A = 10\\,\\mathrm{barn}$, so $\\lambda_A = 1/\\sigma_A^2$.\n   - $E_0 \\sim \\mathcal{N}(0, \\sigma_{E_0}^2)$ with $\\sigma_{E_0} = 0.5\\,\\mathrm{MeV}$, so $\\lambda_{E_0} = 1/\\sigma_{E_0}^2$.\n   - $\\Gamma \\sim \\mathcal{N}(0, \\sigma_{\\Gamma}^2)$ with $\\sigma_{\\Gamma} = 0.2\\,\\mathrm{MeV}$, so $\\lambda_{\\Gamma} = 1/\\sigma_{\\Gamma}^2$.\n   - $c_k \\sim \\mathcal{N}(0, \\sigma_{c,k}^2)$ with $\\sigma_{c,k} = \\sigma_c/(k+1)$, $\\sigma_c = 0.5\\,\\mathrm{barn}$, so $\\lambda_{c,k} = 1/\\sigma_{c,k}^2$.\n   Evaluate the smallest eigenvalue of the unregularized approximate Hessian $H_0 = J^\\top J$ and report whether it is strictly positive (boolean), and the smallest eigenvalue of the regularized approximate Hessian $H_{\\mathrm{prior}} = J_w^\\top J_w + \\Lambda$ and report whether it is strictly positive (boolean). Use a strict positivity threshold of $\\varepsilon = 10^{-12}$ in the corresponding parameter units.\n\nDesign a program that performs the above computations for the following test suite of cases. Each case specifies the energy window and sampling, the true parameters used to generate synthetic data (for realism, although the Jacobian only depends on the current parameter values), and the initial guess $\\boldsymbol{\\theta}_0$ where the Jacobian is evaluated. All energies must be in $\\mathrm{MeV}$, widths in $\\mathrm{MeV}$, cross sections and background coefficients in $\\mathrm{barn}$.\n\n- Case $1$ (happy path, well-conditioned):\n  - Energy window: $E \\in [3,7]\\,\\mathrm{MeV}$ sampled at $N=200$ equally spaced points.\n  - True parameters: $E_0 = 5\\,\\mathrm{MeV}$, $\\Gamma = 0.5\\,\\mathrm{MeV}$, $A = 2.0\\,\\mathrm{barn}$, polynomial degree $m=2$, $c_0 = 0.5\\,\\mathrm{barn}$, $c_1 = 0.1\\,\\mathrm{barn}$, $c_2 = -0.02\\,\\mathrm{barn}$.\n  - Measurement uncertainty: $\\sigma_i = 0.02\\,\\mathrm{barn}$ for all $i$.\n  - Initial guess: $E_0^{(0)} = 5.1\\,\\mathrm{MeV}$, $\\Gamma^{(0)} = 0.55\\,\\mathrm{MeV}$, $A^{(0)} = 1.8\\,\\mathrm{barn}$, $c_k^{(0)} = c_k$ for all $k$.\n\n- Case $2$ (boundary case, very narrow width, potential undersampling):\n  - Energy window: $E \\in [4.8,5.2]\\,\\mathrm{MeV}$ sampled at $N=100$ equally spaced points.\n  - True parameters: $E_0 = 5.0\\,\\mathrm{MeV}$, $\\Gamma = 0.05\\,\\mathrm{MeV}$, $A = 2.0\\,\\mathrm{barn}$, polynomial degree $m=2$, $c_0 = 0.5\\,\\mathrm{barn}$, $c_1 = 0.1\\,\\mathrm{barn}$, $c_2 = -0.02\\,\\mathrm{barn}$.\n  - Measurement uncertainty: $\\sigma_i = 0.02\\,\\mathrm{barn}$ for all $i$.\n  - Initial guess: $E_0^{(0)} = 5.02\\,\\mathrm{MeV}$, $\\Gamma^{(0)} = 0.06\\,\\mathrm{MeV}$, $A^{(0)} = 1.9\\,\\mathrm{barn}$, $c_k^{(0)} = c_k$ for all $k$.\n\n- Case $3$ (edge case, higher-degree background with potential collinearity):\n  - Energy window: $E \\in [0,10]\\,\\mathrm{MeV}$ sampled at $N=300$ equally spaced points.\n  - True parameters: $E_0 = 5.0\\,\\mathrm{MeV}$, $\\Gamma = 0.3\\,\\mathrm{MeV}$, $A = 1.5\\,\\mathrm{barn}$, polynomial degree $m=5$, $c_0 = 0.3\\,\\mathrm{barn}$, $c_1 = -0.15\\,\\mathrm{barn}$, $c_2 = 0.05\\,\\mathrm{barn}$, $c_3 = -0.02\\,\\mathrm{barn}$, $c_4 = 0.01\\,\\mathrm{barn}$, $c_5 = -0.005\\,\\mathrm{barn}$.\n  - Measurement uncertainty: $\\sigma_i = 0.03\\,\\mathrm{barn}$ for all $i$.\n  - Initial guess: $E_0^{(0)} = 5.2\\,\\mathrm{MeV}$, $\\Gamma^{(0)} = 0.27\\,\\mathrm{MeV}$, $A^{(0)} = 1.35\\,\\mathrm{barn}$, $c_k^{(0)} = c_k$ for all $k$.\n\n- Case $4$ (edge case, low-amplitude resonance overshadowed by background):\n  - Energy window: $E \\in [3,7]\\,\\mathrm{MeV}$ sampled at $N=200$ equally spaced points.\n  - True parameters: $E_0 = 5.0\\,\\mathrm{MeV}$, $\\Gamma = 0.5\\,\\mathrm{MeV}$, $A = 0.1\\,\\mathrm{barn}$, polynomial degree $m=2$, $c_0 = 0.5\\,\\mathrm{barn}$, $c_1 = 0.1\\,\\mathrm{barn}$, $c_2 = -0.02\\,\\mathrm{barn}$.\n  - Measurement uncertainty: $\\sigma_i = 0.02\\,\\mathrm{barn}$ for all $i$.\n  - Initial guess: $E_0^{(0)} = 5.1\\,\\mathrm{MeV}$, $\\Gamma^{(0)} = 0.55\\,\\mathrm{MeV}$, $A^{(0)} = 0.12\\,\\mathrm{barn}$, $c_k^{(0)} = c_k$ for all $k$.\n\nFor each case, your program must compute and return a list containing:\n- $\\kappa_{\\mathrm{raw}}$ (float),\n- $\\kappa_{\\mathrm{scaled}}$ (float),\n- the improvement ratio $\\rho = \\kappa_{\\mathrm{raw}}/\\kappa_{\\mathrm{scaled}}$ (float),\n- a boolean indicating whether $\\min\\operatorname{eig}(H_0) > \\varepsilon$,\n- a boolean indicating whether $\\min\\operatorname{eig}(H_{\\mathrm{prior}}) > \\varepsilon$.\n\nFinal output format: Your program should produce a single line of output containing the results for the four cases as a comma-separated list of four sublists, each sublist formatted as specified above and enclosed in square brackets, with no spaces (for example, $\\big[\\,[1.0,0.5,2.0,\\mathrm{True},\\mathrm{True}],\\ldots\\,\\big]$). The values of $\\kappa_{\\mathrm{raw}}$, $\\kappa_{\\mathrm{scaled}}$, and $\\rho$ are dimensionless floats. The positive definiteness booleans must be exact logical values. Express all physical quantities internally in the units provided above; the output is unitless or boolean as specified.",
            "solution": "The problem requires an analysis of the conditioning of the Jacobian matrix for a nonlinear least squares model of a nuclear resonance. The analysis involves computing the Jacobian at a given parameter guess, evaluating its condition number with and without scaling, and assessing the positive definiteness of the corresponding approximate Hessian matrix, both with and without Tikhonov regularization derived from Gaussian priors.\n\nThe parametric model for the cross section $\\hat{y}$ as a function of energy $E$ is given by the sum of a Breit-Wigner resonance term and a polynomial background term:\n$$\n\\hat{y}(E;\\boldsymbol{\\theta}) = A\\,S(E;E_0,\\Gamma) + \\sum_{k=0}^{m} c_k\\,x(E)^k\n$$\nThe parameter vector $\\boldsymbol{\\theta}$ of size $P=m+4$ is composed of the resonance amplitude $A$, the resonance energy $E_0$, the resonance width $\\Gamma$, and the $m+1$ background coefficients $\\{c_k\\}_{k=0}^m$. We order the parameter vector as $\\boldsymbol{\\theta} = [A, E_0, \\Gamma, c_0, c_1, \\dots, c_m]^\\top$.\n\nThe Breit-Wigner lineshape function $S(E;E_0,\\Gamma)$ is defined as:\n$$\nS(E;E_0,\\Gamma) = \\frac{(\\Gamma/2)^2}{(E - E_0)^2 + (\\Gamma/2)^2}\n$$\nThe background polynomial is expressed in a normalized basis $x(E)$:\n$$\nx(E) = \\frac{E - E_c}{E_s}\n$$\nwhere $E_c$ is the center of the energy interval and $E_s$ is its half-width, ensuring $x(E) \\in [-1,1]$ over the fitting window.\n\nFor a set of $N$ energy points $\\{E_i\\}_{i=1}^N$, the least squares problem aims to minimize the sum of squared residuals. The residual vector is $\\mathbf{r}(\\boldsymbol{\\theta}) = \\mathbf{y} - \\hat{\\mathbf{y}}(\\boldsymbol{\\theta})$, where $\\mathbf{y}$ is the vector of observed cross sections and $\\hat{\\mathbf{y}}(\\boldsymbol{\\theta})$ is the vector of model predictions. The Jacobian matrix of the residual vector is a key component in iterative solvers like the Gauss-Newton algorithm. Its definition is:\n$$\nJ(\\boldsymbol{\\theta}) = \\frac{\\partial \\mathbf{r}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = - \\frac{\\partial \\hat{\\mathbf{y}}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\n$$\nThe entry $J_{ij}$ corresponds to the partial derivative of the $i$-th residual with respect to the $j$-th parameter, evaluated at $E_i$. We compute these derivatives from first principles.\n\nThe partial derivatives of $\\hat{y}(E; \\boldsymbol{\\theta})$ with respect to each parameter are:\n1.  Derivative with respect to amplitude $A$:\n    $$ \\frac{\\partial \\hat{y}}{\\partial A} = S(E;E_0,\\Gamma) $$\n2.  Derivative with respect to resonance energy $E_0$:\n    $$ \\frac{\\partial \\hat{y}}{\\partial E_0} = A \\frac{\\partial S}{\\partial E_0} = A \\left( (\\Gamma/2)^2 \\frac{-1}{((E-E_0)^2+(\\Gamma/2)^2)^2} (2(E-E_0)(-1)) \\right) = A \\frac{2(E-E_0)(\\Gamma/2)^2}{((E-E_0)^2+(\\Gamma/2)^2)^2} $$\n3.  Derivative with respect to resonance width $\\Gamma$:\n    $$ \\frac{\\partial \\hat{y}}{\\partial \\Gamma} = A \\frac{\\partial S}{\\partial \\Gamma} = A \\frac{ \\frac{\\partial}{\\partial\\Gamma}((\\Gamma/2)^2) \\cdot ((E-E_0)^2+(\\Gamma/2)^2) - (\\Gamma/2)^2 \\cdot \\frac{\\partial}{\\partial\\Gamma}((E-E_0)^2+(\\Gamma/2)^2) }{ ((E-E_0)^2+(\\Gamma/2)^2)^2 } $$\n    $$ \\frac{\\partial \\hat{y}}{\\partial \\Gamma} = A \\frac{ (\\Gamma/2) \\cdot ((E-E_0)^2+(\\Gamma/2)^2) - (\\Gamma/2)^2 \\cdot (\\Gamma/2) }{ ((E-E_0)^2+(\\Gamma/2)^2)^2 } = A \\frac{(\\Gamma/2)(E-E_0)^2}{((E-E_0)^2+(\\Gamma/2)^2)^2} $$\n4.  Derivative with respect to background coefficient $c_k$:\n    $$ \\frac{\\partial \\hat{y}}{\\partial c_k} = \\frac{\\partial}{\\partial c_k} \\sum_{j=0}^{m} c_j\\,x(E)^j = x(E)^k $$\n\nThe columns of the Jacobian matrix $J$ at a specific parameter guess $\\boldsymbol{\\theta}_0$ are constructed using the negative of these expressions evaluated at each energy $E_i$ and for each parameter in $\\boldsymbol{\\theta}_0$.\n\nThe first task is to compute the condition number $\\kappa_{\\mathrm{raw}}$ of this unweighted Jacobian $J$. This is given by the ratio of its largest to its smallest singular value: $\\kappa_{\\mathrm{raw}}(J) = \\sigma_{\\max}(J) / \\sigma_{\\min}(J)$. A large condition number indicates that the least squares problem is ill-conditioned.\n\nThe second task is to improve a related condition number through scaling. We define a weighted Jacobian $J_w = WJ$, where $W = \\mathrm{diag}(1/\\sigma_i)$ is a diagonal matrix of inverse measurement uncertainties. This transformation is equivalent to whitening the residuals. A further scaling is applied to the columns of $J_w$. We define a diagonal matrix $C$ whose diagonal elements $C_{jj}$ are the Euclidean norms of the columns of $J_w$, i.e., $C_{jj} = \\|(J_w)_{:,j}\\|_2$. The fully scaled Jacobian is $J_{\\mathrm{scaled}} = J_w C^{-1}$. By construction, each column of $J_{\\mathrm{scaled}}$ has a unit Euclidean norm. We then compute its condition number $\\kappa_{\\mathrm{scaled}}$, which is typically much improved over $\\kappa_{\\mathrm{raw}}$. The improvement is quantified by the ratio $\\rho = \\kappa_{\\mathrm{raw}} / \\kappa_{\\mathrm{scaled}}$.\n\nThe third task concerns the Gauss-Newton approximation of the Hessian matrix. For unweighted least squares, this is $H_0 \\approx J^\\top J$. For the algorithm to be stable, $H_0$ must be positive definite. We check this by computing its eigenvalues and verifying if the minimum eigenvalue is strictly positive, i.e., $\\min\\operatorname{eig}(H_0) > \\varepsilon$ for a small threshold $\\varepsilon = 10^{-12}$.\nWhen $H_0$ is ill-conditioned or singular, regularization can be introduced. Using Gaussian priors for the parameters leads to a regularized or maximum a posteriori (MAP) estimation problem. The Hessian of the corresponding objective function is approximated by $H_{\\mathrm{prior}} = J_w^\\top J_w + \\Lambda$, where $\\Lambda$ is the prior precision matrix (inverse of the prior covariance matrix). For the specified independent Gaussian priors, $\\Lambda = \\mathrm{diag}(\\lambda_j)$ with $\\lambda_j = 1/\\sigma_j^2$, where $\\sigma_j$ is the standard deviation of the prior on parameter $\\theta_j$. The specific values are $\\lambda_A = 1/\\sigma_A^2 = 1/10^2 = 0.01$, $\\lambda_{E_0} = 1/\\sigma_{E_0}^2 = 1/0.5^2 = 4$, $\\lambda_{\\Gamma} = 1/\\sigma_{\\Gamma}^2 = 1/0.2^2 = 25$, and $\\lambda_{c,k} = 1/\\sigma_{c,k}^2 = ((k+1)/\\sigma_c)^2 = (2(k+1))^2$ with $\\sigma_c=0.5$. Adding the diagonal, positive definite matrix $\\Lambda$ to the positive semi-definite $J_w^\\top J_w$ typically ensures that the resulting regularized Hessian $H_{\\mathrm{prior}}$ is strictly positive definite. We verify this by checking if $\\min\\operatorname{eig}(H_{\\mathrm{prior}}) > \\varepsilon$.\n\nThe computational procedure for each test case is as follows:\n1.  Set up the energy grid $\\{E_i\\}$ and the initial parameter guess $\\boldsymbol{\\theta}_0$.\n2.  Construct the $N \\times P$ unweighted Jacobian matrix $J$ using the derived partial derivatives evaluated at $\\boldsymbol{\\theta}_0$.\n3.  Compute $\\kappa_{\\mathrm{raw}} = \\operatorname{cond}(J)$.\n4.  Construct the weighting matrix $W$ and the weighted Jacobian $J_w = WJ$.\n5.  Compute the column norms of $J_w$ to form the diagonal matrix $C$.\n6.  Construct the scaled Jacobian $J_{\\mathrm{scaled}} = J_w C^{-1}$ and compute $\\kappa_{\\mathrm{scaled}} = \\operatorname{cond}(J_{\\mathrm{scaled}})$.\n7.  Calculate the improvement ratio $\\rho = \\kappa_{\\mathrm{raw}} / \\kappa_{\\mathrm{scaled}}$.\n8.  Compute the unregularized Hessian $H_0 = J^\\top J$ and determine if its minimum eigenvalue is greater than $\\varepsilon = 10^{-12}$.\n9.  Construct the prior precision matrix $\\Lambda$.\n10. Compute the regularized Hessian $H_{\\mathrm{prior}} = J_w^\\top J_w + \\Lambda$ and determine if its minimum eigenvalue is greater than $\\varepsilon = 10^{-12}$.\n11. Collect these five results: $\\kappa_{\\mathrm{raw}}$, $\\kappa_{\\mathrm{scaled}}$, $\\rho$, and the two booleans for positive definiteness.\nThis procedure is repeated for all four specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the final result.\n    \"\"\"\n    \n    # Test cases defined as a list of dictionaries.\n    test_cases = [\n        {\n            \"E_range\": (3.0, 7.0), \"N\": 200, \"m\": 2,\n            \"sigma\": 0.02,\n            \"theta0\": np.array([1.8, 5.1, 0.55, 0.5, 0.1, -0.02])\n        },\n        {\n            \"E_range\": (4.8, 5.2), \"N\": 100, \"m\": 2,\n            \"sigma\": 0.02,\n            \"theta0\": np.array([1.9, 5.02, 0.06, 0.5, 0.1, -0.02])\n        },\n        {\n            \"E_range\": (0.0, 10.0), \"N\": 300, \"m\": 5,\n            \"sigma\": 0.03,\n            \"theta0\": np.array([1.35, 5.2, 0.27, 0.3, -0.15, 0.05, -0.02, 0.01, -0.005])\n        },\n        {\n            \"E_range\": (3.0, 7.0), \"N\": 200, \"m\": 2,\n            \"sigma\": 0.02,\n            \"theta0\": np.array([0.12, 5.1, 0.55, 0.5, 0.1, -0.02])\n        }\n    ]\n\n    # Prior definitions\n    prior_params = {\n        \"sigma_A\": 10.0,\n        \"sigma_E0\": 0.5,\n        \"sigma_Gamma\": 0.2,\n        \"sigma_c\": 0.5\n    }\n\n    epsilon = 1e-12\n    \n    results = []\n    for case in test_cases:\n        result = process_case(case, prior_params, epsilon)\n        results.append(result)\n\n    # Format output to be a list of lists string with no spaces.\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef get_jacobian(E, theta0, m, E_c, E_s):\n    \"\"\"\n    Computes the Jacobian matrix for the given model.\n    J = -d(y_hat)/d(theta)\n    \"\"\"\n    N = len(E)\n    P = m + 4\n    J = np.zeros((N, P))\n    \n    A, E0, Gamma = theta0[0], theta0[1], theta0[2]\n    c_coeffs = theta0[3:]\n    \n    # Breit-Wigner common terms\n    G2 = Gamma / 2.0\n    G2_sq = G2**2\n    E_minus_E0 = E - E0\n    E_minus_E0_sq = E_minus_E0**2\n    den = E_minus_E0_sq + G2_sq\n    den_sq = den**2\n    \n    # Column for A\n    # d(y_hat)/dA = S(E)\n    S = G2_sq / den\n    J[:, 0] = -S\n    \n    # Column for E0\n    # d(y_hat)/dE0 = A * 2 * (E - E0) * (Gamma/2)^2 / ((E-E0)^2 + (Gamma/2)^2)^2\n    J[:, 1] = -A * 2 * E_minus_E0 * G2_sq / den_sq\n    \n    # Column for Gamma\n    # d(y_hat)/dGamma = A * (Gamma/2) * (E-E0)^2 / ((E-E0)^2 + (Gamma/2)^2)^2\n    J[:, 2] = -A * G2 * E_minus_E0_sq / den_sq\n    \n    # Columns for c_k\n    # d(y_hat)/dc_k = x(E)^k\n    x = (E - E_c) / E_s\n    for k in range(m + 1):\n        J[:, 3 + k] = -np.power(x, k)\n        \n    return J\n\ndef process_case(case_params, prior_params, epsilon):\n    \"\"\"\n    Processes a single test case to compute all required values.\n    \"\"\"\n    E_min, E_max = case_params[\"E_range\"]\n    N = case_params[\"N\"]\n    m = case_params[\"m\"]\n    sigma = case_params[\"sigma\"]\n    theta0 = case_params[\"theta0\"]\n    \n    E = np.linspace(E_min, E_max, N)\n    E_c = (E_max + E_min) / 2.0\n    E_s = (E_max - E_min) / 2.0\n    \n    # 1. Unweighted Jacobian and condition number\n    J = get_jacobian(E, theta0, m, E_c, E_s)\n    kappa_raw = np.linalg.cond(J)\n    \n    # 2. Scaled Jacobian and condition number\n    W = np.diag(np.full(N, 1.0/sigma))\n    J_w = W @ J\n    \n    col_norms = np.linalg.norm(J_w, axis=0)\n    # Handle potential zero columns to avoid division by zero\n    col_norms[col_norms == 0] = 1.0\n    C_inv = np.diag(1.0 / col_norms)\n    \n    J_scaled = J_w @ C_inv\n    kappa_scaled = np.linalg.cond(J_scaled)\n    \n    improvement_ratio = kappa_raw / kappa_scaled if kappa_scaled > 0 else float('inf')\n\n    # 3. Hessian analysis\n    # Unregularized Hessian H0\n    H0 = J.T @ J\n    eigvals_H0 = np.linalg.eigvalsh(H0)\n    is_H0_pos_def = bool(np.min(eigvals_H0) > epsilon)\n    \n    # Regularized Hessian H_prior\n    sigma_A = prior_params[\"sigma_A\"]\n    sigma_E0 = prior_params[\"sigma_E0\"]\n    sigma_Gamma = prior_params[\"sigma_Gamma\"]\n    sigma_c = prior_params[\"sigma_c\"]\n    \n    lambda_A = 1.0 / sigma_A**2\n    lambda_E0 = 1.0 / sigma_E0**2\n    lambda_Gamma = 1.0 / sigma_Gamma**2\n    lambda_c = [((k + 1) / sigma_c)**2 for k in range(m + 1)]\n    \n    Lambda_diag = np.concatenate(([lambda_A, lambda_E0, lambda_Gamma], lambda_c))\n    Lambda = np.diag(Lambda_diag)\n    \n    H_prior = J_w.T @ J_w + Lambda\n    eigvals_H_prior = np.linalg.eigvalsh(H_prior)\n    is_H_prior_pos_def = bool(np.min(eigvals_H_prior) > epsilon)\n    \n    return [kappa_raw, kappa_scaled, improvement_ratio, is_H0_pos_def, is_H_prior_pos_def]\n    \n\n# The problem defines a specific program structure. Calling solve() will execute it.\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}