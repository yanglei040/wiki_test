{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Cholesky decomposition, we must move beyond the matrix algebra and build an intuition for how it operates on random variables. This first exercise provides a direct, hands-on experience with this mechanism. By focusing on a simple bivariate case , you will see precisely how the off-diagonal element of the Cholesky factor $L$ mixes two independent standard normal variables to create a correlated pair, and you will quantify how this mixing influences the probability of joint extreme events.",
            "id": "3294994",
            "problem": "You are tasked with constructing and executing a numerical experiment to quantify how small off-diagonal entries in a Cholesky factor alter the joint tail behavior of a multivariate normal vector. The experiment must adhere to fundamental principles of stochastic simulation and Monte Carlo (MC), and use the definition of the multivariate normal distribution through linear transformation of independent standard normal variables.\n\nConsider a bivariate random vector $X \\in \\mathbb{R}^2$ generated by $X = L Z$, where $Z \\sim \\mathcal{N}(0, I_2)$ has independent standard normal components and $L$ is the lower-triangular Cholesky factor of a covariance matrix $\\Sigma$ satisfying $\\Sigma = L L^\\top$. For a given correlation parameter $\\rho$ and unit variances, let\n$$\n\\Sigma(\\rho) = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix},\n$$\nwhich is positive definite for $|\\rho| < 1$. The Cholesky factor $L(\\rho)$ obtained from $\\Sigma(\\rho)$ induces linear mixing via its off-diagonal entry, which directly controls the dependence between the components of $X$.\n\nDefine a high threshold $t$ by the quantile $q \\in (0,1)$ of the standard normal distribution, i.e., $t$ is such that $P(Z_1 \\le t) = q$. For each pair $(\\rho, q)$, estimate the joint exceedance probability $p_{12}(\\rho, q) \\approx P(X_1 > t, X_2 > t)$ by MC sampling, and compare it to the independence baseline $(1-q)^2$. To quantify the tail dependence sensitivity, use the inflation ratio\n$$\nR(\\rho, q) = \\frac{p_{12}(\\rho, q)}{(1-q)^2},\n$$\nwhich equals $1$ under independence. Values $R(\\rho, q) > 1$ indicate increased co-occurrence of joint extremes due to positive mixing in $L$, while $R(\\rho, q) < 1$ indicates suppression of joint extremes due to negative mixing.\n\nYour program must:\n- Use the Cholesky factor $L(\\rho)$ of $\\Sigma(\\rho)$ to generate $X = L(\\rho) Z$ from $Z \\sim \\mathcal{N}(0, I_2)$.\n- Use a fixed Monte Carlo sample size of $N = 2 \\times 10^6$ and a fixed random seed of $123456789$ to ensure reproducibility.\n- For each test case, compute the empirical fraction of samples with $X_1 > t$ and $X_2 > t$ to estimate $p_{12}(\\rho, q)$, then compute $R(\\rho, q)$ using the theoretical baseline $(1-q)^2$.\n- Round each reported $R(\\rho, q)$ to six decimal places.\n\nUse the following test suite of parameter pairs $(\\rho, q)$, designed to probe independence, weak positive correlation, weak negative correlation, and variation in tail severity:\n- $(0.0, 0.99)$\n- $(0.05, 0.99)$\n- $(0.10, 0.99)$\n- $(-0.05, 0.99)$\n- $(0.0, 0.995)$\n- $(0.05, 0.995)$\n- $(0.10, 0.995)$\n- $(-0.05, 0.995)$\n\nScientific realism requirements:\n- The covariance matrix $\\Sigma(\\rho)$ must be positive definite, which holds for $|\\rho| < 1$, satisfied by the above test suite.\n- The threshold $t$ must be computed via the inverse cumulative distribution function of the standard normal distribution, ensuring that $P(Z_1 \\le t) = q$.\n\nFinal output format:\n- Your program should produce a single line of output containing the computed inflation ratios $R(\\rho, q)$ for the test suite, in the exact order given above, as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,\\dots,r_8]$. Report each $r_i$ as a float rounded to six decimal places.",
            "solution": "The problem is valid. It is a well-posed numerical experiment grounded in the fundamental principles of multivariate statistics and Monte Carlo simulation. All necessary parameters and conditions are provided, and the task is scientifically sound and computationally feasible.\n\nThe objective is to numerically quantify how the correlation $\\rho$ in a bivariate normal distribution affects the probability of joint tail events. The experiment is designed around the generative model of a multivariate normal vector and employs Monte Carlo (MC) methods for probability estimation.\n\nThe core principle is that a mean-zero bivariate normal random vector $X = (X_1, X_2)^\\top$ with covariance matrix $\\Sigma$ can be generated via a linear transformation of a vector $Z = (Z_1, Z_2)^\\top$ of independent standard normal random variables. The transformation is given by $X = LZ$, where $L$ is the lower-triangular Cholesky factor of $\\Sigma$, satisfying $\\Sigma = LL^\\top$.\n\nFor this problem, the covariance matrix is specified by the correlation parameter $\\rho$ as\n$$\n\\Sigma(\\rho) = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\n$$\nThis matrix is symmetric and positive definite for all $|\\rho| < 1$, a condition met by all test cases. The Cholesky decomposition of $\\Sigma(\\rho)$ yields the lower-triangular matrix $L(\\rho)$:\n$$\nL(\\rho) = \\begin{pmatrix} 1 & 0 \\\\ \\rho & \\sqrt{1 - \\rho^2} \\end{pmatrix}\n$$\nThe generative model $X = L(\\rho)Z$ can be expressed component-wise as:\n$$\nX_1 = Z_1\n$$\n$$\nX_2 = \\rho Z_1 + \\sqrt{1 - \\rho^2} Z_2\n$$\nThis formulation explicitly shows how the correlation $\\rho$ introduces dependence. While $X_1$ and $X_2$ are each standard normal variables, they are not independent when $\\rho \\neq 0$. The off-diagonal term $\\rho$ in $L(\\rho)$ acts as a mixing parameter, coupling $X_2$ to $Z_1$ (and thus to $X_1$).\n\nThe experiment aims to estimate the joint exceedance probability $p_{12}(\\rho, q) = P(X_1 > t, X_2 > t)$, where the threshold $t$ is defined by the $q$-th quantile of the standard normal distribution. That is, $t$ satisfies $P(Z_1 \\le t) = q$, or $t = \\Phi^{-1}(q)$, where $\\Phi$ is the standard normal cumulative distribution function (CDF). This implies $P(Z_1 > t) = 1-q$.\n\nThe estimation is performed using a Monte Carlo simulation. By the Law of Large Numbers, the probability of an event can be approximated by the relative frequency of its occurrence in a large number of independent trials. The algorithm is as follows:\n1.  Generate a large number $N = 2 \\times 10^6$ of independent samples $(Z_{i,1}, Z_{i,2})$ for $i=1, \\dots, N$ from the standard normal distribution. For reproducibility, a fixed random seed of $123456789$ is used. For each test case, the pseudo-random number generator is re-seeded, ensuring that the same set of underlying standard normal samples $Z$ is used to evaluate the effect of different $\\rho$ and $q$ values. This is a variance reduction technique known as common random numbers, which sharpens the comparison between different parameter settings.\n2.  For each sample $Z_i$, compute the corresponding sample $X_i = L(\\rho)Z_i$.\n3.  Count the number of samples, $N_{ext}$, for which both components of $X_i$ exceed the threshold $t$: $N_{ext} = \\sum_{i=1}^N \\mathbb{I}(X_{i,1} > t \\text{ and } X_{i,2} > t)$, where $\\mathbb{I}(\\cdot)$ is the indicator function.\n4.  The MC estimate of the joint probability is $\\hat{p}_{12}(\\rho, q) = N_{ext} / N$.\n\nTo quantify the deviation from independence, we compute the inflation ratio:\n$$\nR(\\rho, q) = \\frac{\\hat{p}_{12}(\\rho, q)}{(1-q)^2}\n$$\nThe denominator, $(1-q)^2$, is the theoretical probability of the joint exceedance event under independence (i.e., when $\\rho=0$), since $P(X_1 > t, X_2 > t) = P(X_1 > t)P(X_2 > t) = (1-q)(1-q)$. Therefore, $R(\\rho, q)$ measures the factor by which correlation amplifies ($R>1$) or suppresses ($R<1$) the likelihood of simultaneous extreme events compared to the independent case. For $\\rho=0$, we expect $R(0, q) \\approx 1$, with any deviation being attributable to MC sampling error.\n\nThe implementation will utilize `numpy` for efficient array operations and linear algebra (specifically, `numpy.linalg.cholesky`) and `scipy.stats.norm.ppf` to accurately compute the threshold $t$ from the quantile $q$. The final ratios are rounded to six decimal places as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Constructs and executes a numerical experiment to quantify how small off-diagonal\n    entries in a Cholesky factor alter the joint tail behavior of a multivariate\n    normal vector.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 0.99),\n        (0.05, 0.99),\n        (0.10, 0.99),\n        (-0.05, 0.99),\n        (0.0, 0.995),\n        (0.05, 0.995),\n        (0.10, 0.995),\n        (-0.05, 0.995),\n    ]\n\n    # Define fixed parameters for the Monte Carlo simulation.\n    N = 2_000_000\n    seed = 123456789\n\n    results = []\n    for rho, q in test_cases:\n        # 1. Compute the threshold t from the quantile q of the standard normal distribution.\n        t = norm.ppf(q)\n\n        # 2. Construct the covariance matrix Sigma(rho) and compute its Cholesky factor L(rho).\n        # Sigma is guaranteed to be positive definite for the given rho values.\n        Sigma = np.array([[1.0, rho], \n                          [rho, 1.0]])\n        L = np.linalg.cholesky(Sigma)\n\n        # 3. Initialize the random number generator with a fixed seed for reproducibility.\n        # Re-seeding for each case ensures comparisons are based on the same underlying Z samples.\n        rng = np.random.default_rng(seed)\n\n        # 4. Generate N samples from the bivariate standard normal distribution Z ~ N(0, I_2).\n        # Z has shape (N, 2).\n        Z = rng.standard_normal(size=(N, 2))\n\n        # 5. Transform the standard normal samples Z to correlated samples X using X = L Z.\n        # For a matrix Z where each row is a sample z^T, the transformed samples X are\n        # given by X = Z @ L.T, where each row of X is x^T = (L z)^T.\n        X = Z @ L.T\n\n        # 6. Estimate the joint exceedance probability p_12 by counting samples.\n        # Count the number of samples where both X_1 > t and X_2 > t.\n        joint_exceedances = np.sum((X[:, 0] > t) & (X[:, 1] > t))\n        \n        # The MC estimate for p_12.\n        p12_estimated = joint_exceedances / N\n        \n        # 7. Compute the independence baseline probability.\n        p_independent = (1.0 - q)**2\n\n        # 8. Compute the inflation ratio R(rho, q).\n        # A check for p_independent being zero is good practice but not necessary for the given q values.\n        if p_independent == 0:\n            # This case will not occur for the given problem inputs.\n            # If it did, R would be infinity if p12_estimated > 0, and undefined/NaN if both are 0.\n            # Let's assign NaN in this edge case.\n            R = np.nan\n        else:\n            R = p12_estimated / p_independent\n        \n        # 9. Append the rounded result to the list.\n        results.append(round(R, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In theoretical work, covariance matrices are perfectly symmetric and positive definite. In practice, however, matrices derived from data or numerical models can fail to meet this standard, making Cholesky decomposition impossible. This exercise  tackles this common issue by introducing \"diagonal jitter,\" a pragmatic technique for ensuring numerical stability. Your task is to go beyond simply applying the fix and to rigorously analyze its consequences, deriving the exact statistical bias introduced by this regularization.",
            "id": "3294961",
            "problem": "Consider generating samples from a $d$-dimensional zero-mean multivariate normal distribution with target covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$ that is symmetric and positive semidefinite. To ensure numerical stability of the Cholesky decomposition used for simulation, practitioners often apply a diagonal jitter regularization, defining the perturbed covariance \n$$\n\\Sigma_{\\epsilon} \\leftarrow \\Sigma + \\epsilon I,\n$$\nwhere $\\epsilon > 0$ and $I$ is the $d \\times d$ identity matrix. You will work from the following fundamental bases: the spectral theorem for real symmetric matrices, the definition of covariance as $\\operatorname{Cov}(X)=\\mathbb{E}[XX^{\\top}]$ for zero-mean $X$, and the definition of the unbiased sample covariance estimator. Do not use any pre-stated shortcut facts beyond these bases.\n\n1. Starting from the spectral theorem, express the eigenvalues of $\\Sigma_{\\epsilon}$ in terms of the eigenvalues of $\\Sigma$. Justify each step from first principles.\n2. Suppose we simulate $n$ independent and identically distributed draws $X_{1},\\dots,X_{n}$ from $\\mathcal{N}(0,\\Sigma_{\\epsilon})$ and compute the unbiased sample covariance\n$$\n\\widehat{\\Sigma}_{n} = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)\\left(X_{i}-\\overline{X}\\right)^{\\top}, \\quad \\overline{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}.\n$$\nDerive $\\mathbb{E}[\\widehat{\\Sigma}_{n}]$ and thus the deterministic bias \n$$\nB = \\mathbb{E}[\\widehat{\\Sigma}_{n}] - \\Sigma\n$$\nintroduced by the jitter. Then derive a closed-form expression for the Frobenius norm $\\|B\\|_{F}$ in terms of $d$ and $\\epsilon$ only.\n3. Apply your general results to the concrete case with $d=4$ and\n$$\n\\Sigma =\n\\begin{pmatrix}\n1 & 0.999 & 0 & 0\\\\\n0.999 & 1 & 0 & 0\\\\\n0 & 0 & 2 & 0\\\\\n0 & 0 & 0 & 0.5\n\\end{pmatrix}, \\qquad \\epsilon = 2.5 \\times 10^{-4}.\n$$\nCompute the numerical value of the Frobenius norm $\\|B\\|_{F}$ arising from the jitter regularization in this setup. Round your final numerical answer to four significant figures.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in multivariate statistics and linear algebra, well-posed with a clear path to a unique solution, and objective in its formulation. The provided data is complete and consistent. We may therefore proceed with the solution.\n\nThe problem is divided into three parts. We will address each in sequence.\n\n**Part 1: Eigenvalues of the regularized covariance matrix $\\Sigma_{\\epsilon}$**\n\nWe are asked to express the eigenvalues of $\\Sigma_{\\epsilon} = \\Sigma + \\epsilon I$ in terms of the eigenvalues of $\\Sigma$. The starting point is the spectral theorem for real symmetric matrices.\n\nSince $\\Sigma \\in \\mathbb{R}^{d \\times d}$ is a real symmetric matrix, the spectral theorem guarantees that there exists an orthonormal basis of $\\mathbb{R}^d$ consisting of eigenvectors of $\\Sigma$. Let $v \\in \\mathbb{R}^d$ be an eigenvector of $\\Sigma$ with a corresponding eigenvalue $\\lambda \\in \\mathbb{R}$. By the definition of an eigenvector and eigenvalue, we have:\n$$\n\\Sigma v = \\lambda v\n$$\nNow, we consider the action of the perturbed matrix $\\Sigma_{\\epsilon}$ on this same eigenvector $v$:\n$$\n\\Sigma_{\\epsilon} v = (\\Sigma + \\epsilon I) v\n$$\nUsing the distributive property of matrix multiplication, we get:\n$$\n\\Sigma_{\\epsilon} v = \\Sigma v + (\\epsilon I)v\n$$\nBy definition, $I v = v$, so the second term is $\\epsilon v$. Substituting the eigenvalue relation $\\Sigma v = \\lambda v$ for the first term yields:\n$$\n\\Sigma_{\\epsilon} v = \\lambda v + \\epsilon v\n$$\nFactoring out the vector $v$, we obtain:\n$$\n\\Sigma_{\\epsilon} v = (\\lambda + \\epsilon) v\n$$\nThis resulting equation is the definition of an eigenvalue-eigenvector relationship for the matrix $\\Sigma_{\\epsilon}$. It shows that $v$ is also an eigenvector of $\\Sigma_{\\epsilon}$, and its corresponding eigenvalue is $\\lambda + \\epsilon$.\n\nSince the set of eigenvectors of $\\Sigma$ forms a complete basis for $\\mathbb{R}^d$, we have found all the eigenvectors of $\\Sigma_{\\epsilon}$, which are the same as those of $\\Sigma$. Consequently, if the eigenvalues of $\\Sigma$ are denoted by $\\lambda_1, \\lambda_2, \\dots, \\lambda_d$, then the eigenvalues of $\\Sigma_{\\epsilon}$ are precisely $\\lambda_1 + \\epsilon, \\lambda_2 + \\epsilon, \\dots, \\lambda_d + \\epsilon$.\n\n**Part 2: Bias of the sample covariance estimator and its Frobenius norm**\n\nWe are given $n$ independent and identically distributed (i.i.d.) samples $X_1, \\dots, X_n$ drawn from a multivariate normal distribution $\\mathcal{N}(0, \\Sigma_{\\epsilon})$. The task is to first find the expectation of the unbiased sample covariance estimator $\\widehat{\\Sigma}_n$, then find the bias $B = \\mathbb{E}[\\widehat{\\Sigma}_n] - \\Sigma$, and finally compute the Frobenius norm of this bias, $\\|B\\|_F$.\n\nThe unbiased sample covariance estimator is defined as:\n$$\n\\widehat{\\Sigma}_{n} = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)\\left(X_{i}-\\overline{X}\\right)^{\\top}\n$$\nwhere $\\overline{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ is the sample mean. By its very construction, the expectation of the unbiased sample covariance estimator for samples drawn from a distribution with covariance matrix $\\Sigma_{\\text{true}}$ is $\\Sigma_{\\text{true}}$. In our case, the samples are drawn from $\\mathcal{N}(0, \\Sigma_{\\epsilon})$, so the true covariance is $\\Sigma_{\\epsilon}$. Thus, $\\mathbb{E}[\\widehat{\\Sigma}_{n}] = \\Sigma_{\\epsilon}$.\n\nLet us derive this from first principles as directed. First, we expand the term in the sum:\n$$\n\\sum_{i=1}^{n}(X_{i}-\\overline{X})(X_{i}-\\overline{X})^{\\top} = \\sum_{i=1}^{n} (X_i X_i^\\top - X_i \\overline{X}^\\top - \\overline{X} X_i^\\top + \\overline{X} \\overline{X}^\\top) = \\sum_{i=1}^{n} X_i X_i^\\top - n\\overline{X}\\overline{X}^\\top\n$$\nTaking the expectation, we get:\n$$\n\\mathbb{E}\\left[ \\sum_{i=1}^{n} (X_{i}-\\overline{X})(X_{i}-\\overline{X})^{\\top} \\right] = \\sum_{i=1}^{n} \\mathbb{E}[X_i X_i^\\top] - n\\mathbb{E}[\\overline{X}\\overline{X}^\\top]\n$$\nFor any sample $X_i \\sim \\mathcal{N}(0, \\Sigma_{\\epsilon})$, we have $\\mathbb{E}[X_i] = 0$. The covariance matrix is $\\operatorname{Cov}(X_i) = \\mathbb{E}[(X_i - \\mathbb{E}[X_i])(X_i - \\mathbb{E}[X_i])^\\top] = \\mathbb{E}[X_i X_i^\\top]$. Therefore, $\\mathbb{E}[X_i X_i^\\top] = \\Sigma_{\\epsilon}$.\nThe sum becomes $\\sum_{i=1}^{n} \\Sigma_{\\epsilon} = n\\Sigma_{\\epsilon}$.\n\nNext, we evaluate $\\mathbb{E}[\\overline{X}\\overline{X}^\\top]$. The expectation of the sample mean is $\\mathbb{E}[\\overline{X}] = \\mathbb{E}[\\frac{1}{n}\\sum_i X_i] = \\frac{1}{n}\\sum_i \\mathbb{E}[X_i] = 0$.\nThe covariance of the sample mean is $\\operatorname{Cov}(\\overline{X}) = \\mathbb{E}[\\overline{X}\\overline{X}^\\top]$. Since the $X_i$ are independent,\n$$\n\\operatorname{Cov}(\\overline{X}) = \\operatorname{Cov}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Cov}(X_i) = \\frac{1}{n^2} (n \\Sigma_{\\epsilon}) = \\frac{1}{n}\\Sigma_{\\epsilon}\n$$\nThus, $\\mathbb{E}[\\overline{X}\\overline{X}^\\top] = \\frac{1}{n}\\Sigma_{\\epsilon}$.\n\nSubstituting these results back into the expectation of the sum:\n$$\n\\mathbb{E}\\left[ \\sum_{i=1}^{n} (X_{i}-\\overline{X})(X_{i}-\\overline{X})^{\\top} \\right] = n\\Sigma_{\\epsilon} - n\\left(\\frac{1}{n}\\Sigma_{\\epsilon}\\right) = (n-1)\\Sigma_{\\epsilon}\n$$\nFinally, we find the expectation of $\\widehat{\\Sigma}_n$:\n$$\n\\mathbb{E}[\\widehat{\\Sigma}_n] = \\frac{1}{n-1} \\mathbb{E}\\left[ \\sum_{i=1}^{n} (X_{i}-\\overline{X})(X_{i}-\\overline{X})^{\\top} \\right] = \\frac{1}{n-1} (n-1)\\Sigma_{\\epsilon} = \\Sigma_{\\epsilon}\n$$\nNow, we can compute the bias $B$. The bias is the difference between the expected value of the estimator and the true target parameter $\\Sigma$.\n$$\nB = \\mathbb{E}[\\widehat{\\Sigma}_n] - \\Sigma = \\Sigma_{\\epsilon} - \\Sigma\n$$\nSubstituting the definition $\\Sigma_{\\epsilon} = \\Sigma + \\epsilon I$:\n$$\nB = (\\Sigma + \\epsilon I) - \\Sigma = \\epsilon I\n$$\nThe bias introduced by the jitter regularization is a deterministic matrix equal to $\\epsilon I$, a diagonal matrix with $\\epsilon$ on the diagonal.\n\nNext, we derive the Frobenius norm of the bias, $\\|B\\|_F$. The Frobenius norm of a matrix $A \\in \\mathbb{R}^{d \\times d}$ is given by $\\|A\\|_F = \\sqrt{\\sum_{i=1}^d \\sum_{j=1}^d A_{ij}^2}$. For our bias matrix $B = \\epsilon I$, the elements are $B_{ij} = \\epsilon \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n$$\n\\|B\\|_F^2 = \\sum_{i=1}^d \\sum_{j=1}^d (\\epsilon \\delta_{ij})^2 = \\sum_{i=1}^d (\\epsilon \\delta_{ii})^2 = \\sum_{i=1}^d \\epsilon^2\n$$\nSince this is a sum of $d$ identical terms, we have:\n$$\n\\|B\\|_F^2 = d \\epsilon^2\n$$\nTaking the square root and noting that $\\epsilon > 0$, we find the closed-form expression for the Frobenius norm of the bias:\n$$\n\\|B\\|_F = \\sqrt{d \\epsilon^2} = \\epsilon \\sqrt{d}\n$$\nThis result is notably independent of the original covariance matrix $\\Sigma$.\n\n**Part 3: Application to a specific case**\n\nWe are asked to compute the numerical value of $\\|B\\|_F$ for the specific case where the dimension is $d=4$ and the jitter parameter is $\\epsilon = 2.5 \\times 10^{-4}$. The specific form of the provided $\\Sigma$ matrix does not affect the calculation of the bias norm, as established in Part 2.\n\nUsing the formula $\\|B\\|_F = \\epsilon \\sqrt{d}$:\n$$\n\\|B\\|_F = (2.5 \\times 10^{-4}) \\times \\sqrt{4}\n$$\n$$\n\\|B\\|_F = (2.5 \\times 10^{-4}) \\times 2\n$$\n$$\n\\|B\\|_F = 5 \\times 10^{-4}\n$$\nIn decimal form, this is $0.0005$. The problem requires the answer to be rounded to four significant figures. To express this, we write it as:\n$$\n\\|B\\|_F = 5.000 \\times 10^{-4}\n$$",
            "answer": "$$\n\\boxed{5.000 \\times 10^{-4}}\n$$"
        },
        {
            "introduction": "Choosing a method to regularize a non-positive definite matrix is not merely a numerical prerequisite; it is a modeling decision with real consequences for any downstream analysis. This advanced practice  treats this choice with the seriousness it deserves, asking you to build a test harness to compare two common regularization methods: uniform diagonal jitter and eigenvalue clipping. By using both resulting matrices as priors in a Bayesian linear regression model, you will quantify the \"contamination\" each method introduces into the final posterior inference, using principled information-theoretic metrics like the Kullback-Leibler divergence.",
            "id": "3294984",
            "problem": "You are to implement a complete, deterministic program that acts as a test harness for detecting hidden non-Symmetric Positive Definite (SPD) inputs in multivariate normal generation via Cholesky decomposition, and quantifying how the minimum additive jitter contaminates the posterior in Bayesian linear regression. The harness must systematically enforce strict positive definiteness, generate one multivariate normal sample, and compute a contamination metric between two posterior constructions.\n\nThe following are the foundational bases you must use:\n- The definition of a Symmetric Positive Definite (SPD) matrix: a real symmetric matrix $\\Sigma$ is SPD if and only if all its eigenvalues are strictly positive. Equivalently, for all nonzero vectors $v$, $v^\\top \\Sigma v \\gt 0$.\n- The definition of the multivariate normal distribution $\\mathcal{N}(\\mu,\\Sigma)$ and the use of Cholesky decomposition to generate samples: if $\\Sigma$ is SPD, then there exists a unique lower-triangular matrix $L$ with positive diagonal entries such that $\\Sigma = L L^\\top$. If $z \\sim \\mathcal{N}(0,I)$, then $x = \\mu + L z$ generates a sample from $\\mathcal{N}(\\mu,\\Sigma)$.\n- The Gaussian prior and Gaussian likelihood model for Bayesian linear regression: a prior $w \\sim \\mathcal{N}(0,\\Sigma_{\\text{prior}})$ and a likelihood $y \\mid w \\sim \\mathcal{N}(\\Phi w, \\sigma^2 I)$ yield a Gaussian posterior for $w$.\n- The Kullback–Leibler divergence (defined on first appearance) between two multivariate Gaussian distributions, used to quantify contamination.\n\nYou must not use any shortcut formulas in the problem statement; instead, derive the required computational procedures from these definitions and facts in your solution.\n\nYour program must perform the following steps for each provided test case:\n1. Given a real square input matrix $\\Sigma_{\\text{raw}}$, form its symmetric part $S = \\frac{1}{2}\\left(\\Sigma_{\\text{raw}} + \\Sigma_{\\text{raw}}^\\top\\right)$.\n2. Compute the spectral norm $\\lVert S \\rVert_2$ (the largest singular value) and define a numerical tolerance $\\delta = \\eta \\,\\lVert S \\rVert_2$ with $\\eta = 10^{-8}$.\n3. Compute the minimal scalar jitter $\\epsilon$ that enforces strict positive definiteness after uniform diagonal augmentation, by finding the smallest real number $\\epsilon \\ge 0$ such that $S + \\epsilon I$ has all eigenvalues strictly greater than $\\delta$. Express $\\epsilon$ via the minimal eigenvalue of $S$ by choosing $\\epsilon = \\max\\{0,\\, \\delta - \\lambda_{\\min}(S)\\}$.\n4. Form the jittered covariance $\\Sigma_{\\epsilon} = S + \\epsilon I$ and generate one multivariate normal sample $x \\sim \\mathcal{N}(0,\\Sigma_{\\epsilon})$ using Cholesky decomposition. Use a fixed pseudorandom seed equal to $123$ to ensure reproducibility.\n5. Construct a Bayesian linear regression model with prior $w \\sim \\mathcal{N}(0,\\Sigma_{\\text{prior}})$ and likelihood $y \\mid w \\sim \\mathcal{N}(\\Phi w, \\sigma^2 I)$, where $\\Sigma_{\\text{prior}} = \\Sigma_{\\epsilon}$, design matrix $\\Phi$ and observation vector $y$ are given below, and $\\sigma^2$ is specified below. Compute the posterior mean and covariance of $w$.\n6. Construct a reference SPD prior $\\Sigma_{\\text{ref}}$ by eigenvalue clipping of $S$ (the symmetric part): if $S = Q \\Lambda Q^\\top$ is the spectral decomposition with real eigenvalues, define $\\Lambda_{\\text{clip}} = \\operatorname{diag}(\\max\\{\\lambda_i, \\delta\\})$ and $\\Sigma_{\\text{ref}} = Q \\Lambda_{\\text{clip}} Q^\\top$. Use this reference prior in the same Bayesian linear regression to compute the reference posterior mean and covariance.\n7. Quantify contamination due to the uniform jitter $\\epsilon$ by computing:\n   - The Kullback–Leibler divergence from the jitter-based posterior to the reference posterior, denoted $\\operatorname{KL}(\\mathcal{N}(\\mu_{\\epsilon},\\Sigma_{\\epsilon}^{\\text{post}})\\,\\Vert\\,\\mathcal{N}(\\mu_{\\text{ref}},\\Sigma_{\\text{ref}}^{\\text{post}}))$.\n   - The Euclidean norm of the difference in posterior means, $\\lVert \\mu_{\\epsilon} - \\mu_{\\text{ref}} \\rVert_2$.\n\nYour program must implement the above steps directly from the stated definitions, not by calling any black-box routines beyond linear algebra decompositions.\n\nUse the following fixed design and observation data (no physical units are involved, so none are required):\n- Dimension of weight vector $w$ is $k = 3$.\n- Number of observations is $n = 6$.\n- Design matrix $\\Phi \\in \\mathbb{R}^{n \\times k}$ is\n  $\\Phi = \\begin{bmatrix}\n  1.0 & 0.0 & -1.0 \\\\\n  0.5 & 1.0 & 0.0 \\\\\n  1.5 & -0.5 & 0.5 \\\\\n  0.0 & 1.0 & 1.0 \\\\\n  -1.0 & 0.5 & 2.0 \\\\\n  2.0 & -1.0 & 0.0\n  \\end{bmatrix}$.\n- Observation vector $y \\in \\mathbb{R}^{n}$ is $y = \\begin{bmatrix} 0.1 & 1.0 & -0.5 & 2.0 & -1.5 & 0.0 \\end{bmatrix}^\\top$.\n- Observation noise variance is $\\sigma^2 = 0.25$.\n\nTest suite:\nProvide four test cases for $\\Sigma_{\\text{raw}}$ that exercise the harness across different regimes:\n- Case $1$ (SPD baseline): $\\Sigma_{\\text{raw}} = \\begin{bmatrix} 2.0 & 0.6 & 0.0 \\\\ 0.6 & 1.5 & 0.1 \\\\ 0.0 & 0.1 & 1.0 \\end{bmatrix}$.\n- Case $2$ (nearly positive semidefinite with a tiny negative eigenvalue): let $B = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$, set $A = B B^\\top$ and define $\\Sigma_{\\text{raw}} = A - 10^{-12} I$, i.e., $\\Sigma_{\\text{raw}} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix} - \\begin{bmatrix} 10^{-12} & 0 & 0 \\\\ 0 & 10^{-12} & 0 \\\\ 0 & 0 & 10^{-12} \\end{bmatrix}$.\n- Case $3$ (indefinite): $\\Sigma_{\\text{raw}} = \\begin{bmatrix} 1.0 & 2.0 & 0.0 \\\\ 2.0 & 1.0 & 0.0 \\\\ 0.0 & 0.0 & -0.1 \\end{bmatrix}$.\n- Case $4$ (non-symmetric input): $\\Sigma_{\\text{raw}} = \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 0.0 & 1.0 & 4.0 \\\\ 5.0 & 0.0 & 1.0 \\end{bmatrix}$.\n\nFor each case, your program must compute and return:\n- The scalar jitter $\\epsilon$ used.\n- The Kullback–Leibler divergence value.\n- The Euclidean norm of posterior mean difference.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each per-case result is itself a list with three floating-point numbers in the order specified above. For example: $[\\,[\\epsilon_1,\\mathrm{KL}_1,\\Delta\\mu_1],\\,[\\epsilon_2,\\mathrm{KL}_2,\\Delta\\mu_2],\\,[\\epsilon_3,\\mathrm{KL}_3,\\Delta\\mu_3],\\,[\\epsilon_4,\\mathrm{KL}_4,\\Delta\\mu_4]\\,]$. No additional text should be printed.",
            "solution": "The proposed problem is a well-posed and scientifically grounded exercise in numerical linear algebra and computational statistics. It is complete, internally consistent, and addresses a legitimate and non-trivial issue: the handling of potentially non-Symmetric Positive Definite (SPD) covariance matrices in the context of stochastic simulation and Bayesian inference. The steps are algorithmically defined, and all required data and constants are provided. The problem is therefore deemed valid.\n\nThe solution proceeds by first deriving the requisite mathematical formulas from the foundational principles stipulated, and then specifying an algorithm that implements these derivations.\n\n### Theoretical Derivations and Algorithmic Steps\n\nThe task requires executing a sequence of seven steps for each input matrix $\\Sigma_{\\text{raw}}$. We will detail the principles and computations for each.\n\n**1. Symmetrization**\nA covariance matrix must be symmetric. The input $\\Sigma_{\\text{raw}}$ may not be. We construct its symmetric part, $S$, which represents the closest symmetric matrix to $\\Sigma_{\\text{raw}}$ in the Frobenius norm.\n$$S = \\frac{1}{2}\\left(\\Sigma_{\\text{raw}} + \\Sigma_{\\text{raw}}^\\top\\right)$$\n\n**2. Numerical Tolerance Definition**\nFor a matrix to be strictly positive definite, its eigenvalues must be strictly greater than zero. In floating-point arithmetic, we require them to be greater than a small positive tolerance, $\\delta$. This tolerance is scaled relative to the magnitude of the matrix, defined via its spectral norm $\\lVert S \\rVert_2$. For a symmetric matrix $S$, the spectral norm is its largest singular value, which is equivalent to the largest absolute eigenvalue, $\\lVert S \\rVert_2 = \\max_i |\\lambda_i(S)|$. The tolerance $\\delta$ is set as:\n$$\\delta = \\eta \\,\\lVert S \\rVert_2, \\quad \\text{with } \\eta = 10^{-8}$$\n\n**3. Enforcing Strict Positive Definiteness: Additive Jitter**\nIf $S$ is not strictly positive definite (i.e., if its minimal eigenvalue $\\lambda_{\\min}(S)$ is not greater than $\\delta$), we must regularize it. The specified method is uniform diagonal augmentation, or adding \"jitter\". We form a new matrix $\\Sigma_{\\epsilon} = S + \\epsilon I$, where $I$ is the identity matrix and $\\epsilon$ is a scalar. The eigenvalues of $\\Sigma_{\\epsilon}$ are $\\lambda_i(S) + \\epsilon$. To ensure strict positive definiteness, we must satisfy $\\lambda_i(S) + \\epsilon > \\delta$ for all $i$. This is equivalent to satisfying the condition for the minimum eigenvalue:\n$$\\lambda_{\\min}(S) + \\epsilon > \\delta \\implies \\epsilon > \\delta - \\lambda_{\\min}(S)$$\nThe problem asks for the smallest non-negative jitter $\\epsilon \\ge 0$. We therefore select:\n$$\\epsilon = \\max\\{0, \\delta - \\lambda_{\\min}(S)\\}$$\nThe resulting matrix, $\\Sigma_{\\epsilon} = S + \\epsilon I$, is guaranteed to be SPD with all eigenvalues $>\\delta$.\n\n**4. Multivariate Normal Sample Generation**\nTo generate a sample $x$ from a multivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$, we can use the Cholesky decomposition of the covariance matrix $\\Sigma$. If $\\Sigma$ is SPD, there exists a unique lower-triangular matrix $L$ with positive diagonal entries such that $\\Sigma = LL^\\top$. Given a vector $z$ of independent standard normal random variates, $z \\sim \\mathcal{N}(0, I)$, a sample $x$ is generated by the affine transformation:\n$$x = \\mu + Lz$$\nIn our case, we generate one sample from $\\mathcal{N}(0, \\Sigma_{\\epsilon})$. Thus, we compute the Cholesky factor $L_{\\epsilon}$ of $\\Sigma_{\\epsilon}$ and then compute $x = L_{\\epsilon} z$ with a fixed random seed for reproducibility.\n\n**5. Bayesian Linear Regression Posterior**\nThe problem specifies a Bayesian linear regression model. The posterior distribution of the weights $w$ is derived from the prior and the likelihood via Bayes' theorem: $p(w|y) \\propto p(y|w)p(w)$.\n- Prior: $w \\sim \\mathcal{N}(w|0, \\Sigma_0)$\n- Likelihood: $y \\mid w \\sim \\mathcal{N}(y|\\Phi w, \\sigma^2 I)$\n\nThe log-posterior is the sum of the log-prior and log-likelihood, ignoring constants:\n$$\\ln p(w|y) = -\\frac{1}{2} w^\\top \\Sigma_0^{-1} w - \\frac{1}{2\\sigma^2}(y - \\Phi w)^\\top(y - \\Phi w) + \\text{const.}$$\nExpanding the likelihood term:\n$$\\ln p(w|y) = -\\frac{1}{2} w^\\top \\Sigma_0^{-1} w - \\frac{1}{2\\sigma^2}(y^\\top y - 2y^\\top \\Phi w + w^\\top \\Phi^\\top \\Phi w) + \\text{const.}$$\nGrouping terms quadratic and linear in $w$:\n$$\\ln p(w|y) = -\\frac{1}{2} w^\\top \\left(\\Sigma_0^{-1} + \\frac{1}{\\sigma^2}\\Phi^\\top \\Phi\\right) w + \\left(\\frac{1}{\\sigma^2} y^\\top \\Phi\\right) w + \\text{const.}$$\nThis is the logarithm of an unnormalized Gaussian density. By completing the square, or by matching terms with the canonical form $\\ln \\mathcal{N}(w|\\mu, \\Sigma) = -\\frac{1}{2}w^\\top\\Sigma^{-1}w + w^\\top\\Sigma^{-1}\\mu + \\text{const.}$, we identify the posterior precision $\\Lambda_{\\text{post}}$ and covariance $\\Sigma_{\\text{post}}$:\n$$\\Lambda_{\\text{post}} = \\Sigma_{\\text{post}}^{-1} = \\Sigma_0^{-1} + \\frac{1}{\\sigma^2}\\Phi^\\top \\Phi$$\nAnd the posterior mean $\\mu_{\\text{post}}$:\n$$\\Sigma_{\\text{post}}^{-1} \\mu_{\\text{post}} = \\frac{1}{\\sigma^2} \\Phi^\\top y \\implies \\mu_{\\text{post}} = \\Sigma_{\\text{post}} \\left(\\frac{1}{\\sigma^2} \\Phi^\\top y\\right)$$\nThese formulas will be used to compute the posterior distribution $(\\mu_{\\epsilon}, \\Sigma^{\\text{post}}_{\\epsilon})$ by setting the prior covariance $\\Sigma_0 = \\Sigma_{\\epsilon}$.\n\n**6. Reference Posterior Construction**\nA reference posterior is constructed using an alternative SPD matrix, $\\Sigma_{\\text{ref}}$, derived by eigenvalue clipping. Given the spectral decomposition of the symmetric matrix $S = Q \\Lambda Q^\\top$, where $Q$ is the orthogonal matrix of eigenvectors and $\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_k)$ is the diagonal matrix of eigenvalues, we clip any eigenvalue below the tolerance $\\delta$:\n$$\\Lambda_{\\text{clip}} = \\operatorname{diag}(\\max\\{\\lambda_i, \\delta\\})$$\nThe reference covariance is then reconstructed:\n$$\\Sigma_{\\text{ref}} = Q \\Lambda_{\\text{clip}} Q^\\top$$\nThis matrix is SPD by construction. We then use this as the prior covariance, $\\Sigma_0 = \\Sigma_{\\text{ref}}$, in the Bayesian regression formulas derived above to find the reference posterior mean $\\mu_{\\text{ref}}$ and covariance $\\Sigma^{\\text{post}}_{\\text{ref}}$.\n\n**7. Contamination Quantification**\nThe effect of the jitter-based regularization is quantified by comparing the jittered posterior $\\mathcal{P}_{\\epsilon} = \\mathcal{N}(\\mu_{\\epsilon}, \\Sigma^{\\text{post}}_{\\epsilon})$ with the reference posterior $\\mathcal{P}_{\\text{ref}} = \\mathcal{N}(\\mu_{\\text{ref}}, \\Sigma^{\\text{post}}_{\\text{ref}})$.\n\na. **Kullback-Leibler (KL) Divergence**: The KL divergence from a Gaussian $P_1 = \\mathcal{N}(\\mu_1, \\Sigma_1)$ to $P_2 = \\mathcal{N}(\\mu_2, \\Sigma_2)$ in $k$ dimensions is given by:\n$$\\operatorname{KL}(P_1 \\,\\Vert\\, P_2) = \\frac{1}{2} \\left[ \\operatorname{tr}(\\Sigma_2^{-1} \\Sigma_1) + (\\mu_2 - \\mu_1)^\\top \\Sigma_2^{-1} (\\mu_2 - \\mu_1) - k + \\log\\left(\\frac{\\det \\Sigma_2}{\\det \\Sigma_1}\\right) \\right]$$\nWe apply this formula with $P_1 = \\mathcal{P}_{\\epsilon}$ and $P_2 = \\mathcal{P}_{\\text{ref}}$.\n\nb. **Posterior Mean Difference**: We compute the Euclidean distance between the two posterior means:\n$$\\Delta\\mu = \\lVert \\mu_{\\epsilon} - \\mu_{\\text{ref}} \\rVert_2$$\n\nThese steps form a complete, deterministic algorithm to be executed for each test case.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky\n\ndef solve():\n    \"\"\"\n    Implements a test harness for analyzing the effect of jitter on Bayesian linear regression.\n    \"\"\"\n    \n    # Fixed design and observation data\n    k = 3\n    n = 6\n    Phi = np.array([\n        [1.0, 0.0, -1.0],\n        [0.5, 1.0, 0.0],\n        [1.5, -0.5, 0.5],\n        [0.0, 1.0, 1.0],\n        [-1.0, 0.5, 2.0],\n        [2.0, -1.0, 0.0]\n    ])\n    y = np.array([0.1, 1.0, -0.5, 2.0, -1.5, 0.0])\n    sigma_sq = 0.25\n    eta = 1e-8\n    \n    # Pre-computation for Bayesian update\n    Phi_T_Phi = Phi.T @ Phi\n    Phi_T_y = Phi.T @ y\n    beta = 1.0 / sigma_sq\n\n    # Test suite\n    case1_S_raw = np.array([\n        [2.0, 0.6, 0.0], \n        [0.6, 1.5, 0.1], \n        [0.0, 0.1, 1.0]\n    ])\n    \n    B = np.array([[1, 0], [0, 1], [1, 1]])\n    A = B @ B.T\n    case2_S_raw = A - 1e-12 * np.eye(k)\n    \n    case3_S_raw = np.array([\n        [1.0, 2.0, 0.0],\n        [2.0, 1.0, 0.0],\n        [0.0, 0.0, -0.1]\n    ])\n    \n    case4_S_raw = np.array([\n        [1.0, 2.0, 3.0],\n        [0.0, 1.0, 4.0],\n        [5.0, 0.0, 1.0]\n    ])\n    \n    test_cases = [case1_S_raw, case2_S_raw, case3_S_raw, case4_S_raw]\n    \n    results = []\n    \n    rng = np.random.default_rng(123)\n\n    for Sigma_raw in test_cases:\n        # Step 1: Symmetrize the input matrix\n        S = 0.5 * (Sigma_raw + Sigma_raw.T)\n        \n        # Step 2: Compute tolerance delta\n        eigvals, eigvecs = np.linalg.eigh(S)\n        spectral_norm = np.max(np.abs(eigvals))\n        delta = eta * spectral_norm\n        \n        # Step 3: Computejitter epsilon\n        lambda_min = np.min(eigvals)\n        epsilon = np.maximum(0, delta - lambda_min)\n        \n        # Jittered Covariance\n        Sigma_eps = S + epsilon * np.eye(k)\n        \n        # Step 4: Generate one multivariate normal sample (for completeness)\n        L_eps = cholesky(Sigma_eps, lower=True)\n        z = rng.standard_normal(k)\n        x_sample = L_eps @ z # This sample is not used in subsequent calculations\n        \n        # Step 5: Compute posterior for jittered prior\n        inv_Sigma_eps = np.linalg.inv(Sigma_eps)\n        precision_post_eps = inv_Sigma_eps + beta * Phi_T_Phi\n        Sigma_post_eps = np.linalg.inv(precision_post_eps)\n        mu_post_eps = beta * (Sigma_post_eps @ Phi_T_y)\n\n        # Step 6: Compute reference posterior via eigenvalue clipping\n        Lambda_clip_diag = np.maximum(eigvals, delta)\n        inv_Lambda_clip_diag = 1.0 / Lambda_clip_diag\n        Sigma_ref = eigvecs @ np.diag(Lambda_clip_diag) @ eigvecs.T\n        inv_Sigma_ref = eigvecs @ np.diag(inv_Lambda_clip_diag) @ eigvecs.T\n\n        precision_post_ref = inv_Sigma_ref + beta * Phi_T_Phi\n        Sigma_post_ref = np.linalg.inv(precision_post_ref)\n        mu_post_ref = beta * (Sigma_post_ref @ Phi_T_y)\n\n        # Step 7: Quantify contamination\n        # KL Divergence: KL(eps || ref)\n        inv_Sigma_post_ref = np.linalg.inv(Sigma_post_ref)\n        trace_term = np.trace(inv_Sigma_post_ref @ Sigma_post_eps)\n        mu_diff_vec = mu_post_ref - mu_post_eps\n        mahalanobis_term = mu_diff_vec.T @ inv_Sigma_post_ref @ mu_diff_vec\n        \n        _, logdet_post_eps = np.linalg.slogdet(Sigma_post_eps)\n        _, logdet_post_ref = np.linalg.slogdet(Sigma_post_ref)\n        log_det_ratio = logdet_post_ref - logdet_post_eps\n        \n        kl_divergence = 0.5 * (trace_term + mahalanobis_term - k + log_det_ratio)\n        \n        # Posterior mean difference norm\n        mean_diff_norm = np.linalg.norm(mu_post_eps - mu_post_ref)\n        \n        results.append([epsilon, kl_divergence, mean_diff_norm])\n\n    # Format the final output string to be a compact list of lists\n    output_str = f\"[{','.join([str(res).replace(' ', '') for res in results])}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}