{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of any rejection sampler is fundamentally tied to the tightness of its proposal envelope. This practice provides a concrete, hands-on opportunity to explore this relationship by designing an envelope for a common log-concave target distribution. By working through the problem , you will derive the optimal placement of support points that maximizes the acceptance rate $\\alpha$, a foundational skill in building efficient adaptive samplers.",
            "id": "3335755",
            "problem": "Consider a target probability density function $f(x)$ on $\\mathbb{R}$ that is log-concave, meaning $\\log f(x)$ is a concave function. In rejection sampling, one constructs an envelope function $g(x)$ such that $g(x) \\geq f(x)$ for all $x$, draws $X$ from the normalized envelope, and accepts $X$ with probability $f(X)/g(X)$. The average acceptance rate $\\alpha$ equals the ratio of the integral of $f$ to the integral of $g$. A standard way to construct efficient envelopes for log-concave targets is to use tangent lines to $\\log f$ to build a piecewise linear upper hull $u(x)$, and then take the envelope $g(x) = \\exp(u(x))$. This is the core idea behind Adaptive Rejection Sampling (ARS).\n\nYou will analyze the trade-off between the number and placement of support points (where tangents are taken) and the resulting acceptance rate $\\alpha$ for a specific, well-studied log-concave target and a restricted class of envelopes, using first principles from rejection sampling.\n\nLet the target be the standard normal density $f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$, which satisfies log-concavity. Consider constructing a piecewise-exponential envelope $g(x)$ from tangent lines to $\\log f(x)$ at two symmetric support points $\\{-a, +a\\}$ for some $a > 0$. Let $l(x) = \\log f(x)$, and let $u_{-a}(x)$ and $u_{+a}(x)$ be the tangent lines to $l$ at $x = -a$ and $x = +a$, respectively. Define the upper hull $u(x) = \\min\\{u_{-a}(x), u_{+a}(x)\\}$ and set $g(x) = \\exp(u(x))$. Because $l$ is concave, $u(x) \\geq l(x)$ and hence $g(x) \\geq f(x)$.\n\nStarting from the fundamental rejection sampling fact that the average acceptance rate equals the ratio of the integrals of the target and the envelope, and using only the definitions above (without introducing any shortcut formulas), do the following:\n\n1. Derive a closed-form expression for the acceptance rate $\\alpha(a)$ as a function of $a$ for the envelope $g$ defined by the two symmetric support points $\\{-a, +a\\}$.\n2. Using the acceptance-rate-as-integral-ratio principle, define the expected number of target evaluations per accepted draw as $J(a) = 1/\\alpha(a)$, under the simplifying assumption that the overhead to sample from the piecewise-exponential envelope and to determine the active segment is negligible compared to evaluating $f(x)$; this isolates the core trade-off driven by the envelopeâ€™s tightness.\n3. Determine the value $a^{\\star} > 0$ that minimizes $J(a)$ over this class of two-support-point envelopes, and the corresponding optimal acceptance rate $\\alpha^{\\star} = \\alpha(a^{\\star})$. Provide both $a^{\\star}$ and $\\alpha^{\\star}$ as exact expressions.\n\nYour final answer must be a single row matrix containing $a^{\\star}$ and $\\alpha^{\\star}$. No numerical rounding is required, and no units are to be included in the final expressions. Explain, based on your derivation, how the placement of support points affects $\\alpha$ and thus the evaluation cost $J$, thereby quantifying the trade-off in this restricted two-point design. Also briefly comment, conceptually, on how increasing the number of support points generally impacts $\\alpha$ and $J$ for log-concave targets constructed via tangent envelopes and how a squeeze function would further affect evaluation cost without changing $\\alpha$.",
            "solution": "The problem is first validated to ensure it is scientifically sound, self-contained, and well-posed.\n\n**Step 1: Extract Givens**\n-   Target probability density function: $f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)$ on $\\mathbb{R}$.\n-   Property: $f(x)$ is log-concave.\n-   Envelope construction: The envelope $g(x)$ is constructed from tangent lines to $l(x) = \\log f(x)$ at two symmetric support points $\\{-a, +a\\}$ for $a > 0$.\n-   Function definitions:\n    -   $l(x) = \\log f(x)$.\n    -   $u_{-a}(x)$ and $u_{+a}(x)$ are the tangent lines to $l(x)$ at $x=-a$ and $x=+a$.\n    -   $u(x) = \\min\\{u_{-a}(x), u_{+a}(x)\\}$.\n    -   $g(x) = \\exp(u(x))$.\n-   Core principle: The average acceptance rate is $\\alpha = \\frac{\\int_{-\\infty}^{\\infty} f(x) \\,dx}{\\int_{-\\infty}^{\\infty} g(x) \\,dx}$.\n-   Cost function: Expected number of target evaluations per accepted draw is $J(a) = 1/\\alpha(a)$.\n-   Objectives:\n    1.  Derive the closed-form expression for the acceptance rate $\\alpha(a)$.\n    2.  Define $J(a)$.\n    3.  Find the optimal value $a^{\\star} > 0$ that minimizes $J(a)$ and the corresponding optimal acceptance rate $\\alpha^{\\star} = \\alpha(a^{\\star})$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is valid. It is a well-posed mathematical exercise firmly grounded in the theory of rejection sampling and Adaptive Rejection Sampling (ARS). The target function is the standard normal density, and its log-concavity is a known property. The construction of the envelope from tangents is a standard technique for log-concave densities. All terms are clearly defined, the objectives are unambiguous, and no scientific or logical flaws are present. The simplifying assumption about the cost $J(a)$ is explicitly stated and serves to isolate the primary trade-off related to the envelope's tightness.\n\n**Step 3: Verdict and Action**\nThe problem is valid and a full solution will be provided.\n\n**Derivation of the Solution**\n\nThe solution proceeds by first constructing the envelope function $g(x)$, then calculating its integral to find the acceptance rate $\\alpha(a)$, and finally optimizing this rate with respect to the support point parameter $a$.\n\nFirst, we define the logarithm of the target density, $l(x)$:\n$$l(x) = \\ln(f(x)) = \\ln\\left(\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)\\right) = -\\frac{1}{2}\\ln(2\\pi) - \\frac{x^2}{2}$$\nThe derivative, $l'(x)$, is required to define the tangent lines:\n$$l'(x) = -x$$\nThe equation of a tangent line to $l(x)$ at a point $x_0$ is given by $y(x) = l(x_0) + l'(x_0)(x - x_0)$. We find the two tangent lines at the support points $x_0 = a$ and $x_0 = -a$.\n\nFor the support point $x_0 = a$:\n$l(a) = -\\frac{1}{2}\\ln(2\\pi) - \\frac{a^2}{2}$\n$l'(a) = -a$\nThe tangent line $u_{+a}(x)$ is:\n$$u_{+a}(x) = \\left(-\\frac{1}{2}\\ln(2\\pi) - \\frac{a^2}{2}\\right) - a(x-a) = -\\frac{1}{2}\\ln(2\\pi) + \\frac{a^2}{2} - ax$$\n\nFor the support point $x_0 = -a$:\n$l(-a) = -\\frac{1}{2}\\ln(2\\pi) - \\frac{a^2}{2}$\n$l'(-a) = -(-a) = a$\nThe tangent line $u_{-a}(x)$ is:\n$$u_{-a}(x) = \\left(-\\frac{1}{2}\\ln(2\\pi) - \\frac{a^2}{2}\\right) + a(x-(-a)) = -\\frac{1}{2}\\ln(2\\pi) + \\frac{a^2}{2} + ax$$\n\nThe upper hull $u(x)$ is the minimum of these two lines:\n$$u(x) = \\min\\{u_{-a}(x), u_{+a}(x)\\} = \\min\\left\\{-\\frac{1}{2}\\ln(2\\pi) + \\frac{a^2}{2} + ax, -\\frac{1}{2}\\ln(2\\pi) + \\frac{a^2}{2} - ax\\right\\}$$\nThis simplifies to:\n$$u(x) = -\\frac{1}{2}\\ln(2\\pi) + \\frac{a^2}{2} - a|x|$$\nThe two tangent lines intersect when $u_{-a}(x) = u_{+a}(x)$, which implies $ax = -ax$, so $2ax=0$. Since $a>0$, this occurs at $x=0$. For $x < 0$, $u_{-a}(x) < u_{+a}(x)$, and for $x > 0$, $u_{+a}(x) < u_{-a}(x)$.\n\nThe envelope function $g(x)$ is the exponential of the upper hull $u(x)$:\n$$g(x) = \\exp(u(x)) = \\exp\\left(-\\frac{1}{2}\\ln(2\\pi) + \\frac{a^2}{2} - a|x|\\right) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(\\frac{a^2}{2}\\right)\\exp(-a|x|)$$\n\nThe average acceptance rate $\\alpha(a)$ is given by the ratio of the integrals of $f(x)$ and $g(x)$. The integral of the target density $f(x)$ over $\\mathbb{R}$ is, by definition, $1$:\n$$\\int_{-\\infty}^{\\infty} f(x) \\,dx = 1$$\nNext, we calculate the integral of the envelope function $g(x)$:\n$$\\int_{-\\infty}^{\\infty} g(x) \\,dx = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(\\frac{a^2}{2}\\right) \\int_{-\\infty}^{\\infty} \\exp(-a|x|) \\,dx$$\nThe integral of $\\exp(-a|x|)$ can be computed by exploiting symmetry:\n$$\\int_{-\\infty}^{\\infty} \\exp(-a|x|) \\,dx = 2 \\int_{0}^{\\infty} \\exp(-ax) \\,dx = 2 \\left[-\\frac{1}{a}\\exp(-ax)\\right]_{0}^{\\infty} = 2\\left(0 - \\left(-\\frac{1}{a}\\right)\\right) = \\frac{2}{a}$$\nSubstituting this result back, the integral of the envelope is:\n$$I_g(a) = \\int_{-\\infty}^{\\infty} g(x) \\,dx = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(\\frac{a^2}{2}\\right) \\frac{2}{a} = \\frac{2}{a\\sqrt{2\\pi}}\\exp\\left(\\frac{a^2}{2}\\right)$$\n\nNow we can write the expression for the acceptance rate $\\alpha(a)$:\n$$\\alpha(a) = \\frac{\\int f(x) \\,dx}{\\int g(x) \\,dx} = \\frac{1}{I_g(a)} = \\frac{a\\sqrt{2\\pi}}{2\\exp\\left(\\frac{a^2}{2}\\right)} = \\frac{a\\sqrt{2\\pi}}{2} \\exp\\left(-\\frac{a^2}{2}\\right)$$\nThis is the closed-form expression for the acceptance rate as a function of $a$. The cost function is $J(a) = 1/\\alpha(a)$.\n\nTo find the optimal support point placement $a^{\\star}$, we must minimize $J(a)$, which is equivalent to maximizing $\\alpha(a)$ for $a > 0$. We compute the derivative of $\\alpha(a)$ with respect to $a$ and set it to zero.\n$$\\frac{d\\alpha}{da} = \\frac{d}{da}\\left[\\frac{\\sqrt{2\\pi}}{2} \\left(a \\exp\\left(-\\frac{a^2}{2}\\right)\\right)\\right]$$\nUsing the product rule:\n$$\\frac{d\\alpha}{da} = \\frac{\\sqrt{2\\pi}}{2} \\left[1 \\cdot \\exp\\left(-\\frac{a^2}{2}\\right) + a \\cdot \\exp\\left(-\\frac{a^2}{2}\\right) \\cdot (-a)\\right] = \\frac{\\sqrt{2\\pi}}{2} (1-a^2) \\exp\\left(-\\frac{a^2}{2}\\right)$$\nSetting the derivative to zero:\n$$\\frac{d\\alpha}{da} = 0 \\implies (1-a^2) = 0$$\nSince we require $a>0$, the only critical point is $a=1$. To confirm this is a maximum, we examine the sign of the first derivative. For $0 < a < 1$, $1-a^2 > 0$ and $\\frac{d\\alpha}{da} > 0$, so $\\alpha(a)$ is increasing. For $a > 1$, $1-a^2 < 0$ and $\\frac{d\\alpha}{da} < 0$, so $\\alpha(a)$ is decreasing. Thus, $a=1$ yields the global maximum for $\\alpha(a)$ over $a>0$.\n\nThe optimal support point parameter is therefore:\n$$a^{\\star} = 1$$\nThe corresponding optimal acceptance rate, $\\alpha^{\\star}$, is found by substituting $a^{\\star}=1$ into the expression for $\\alpha(a)$:\n$$\\alpha^{\\star} = \\alpha(1) = \\frac{1 \\cdot \\sqrt{2\\pi}}{2} \\exp\\left(-\\frac{1^2}{2}\\right) = \\frac{\\sqrt{2\\pi}}{2} \\exp\\left(-\\frac{1}{2}\\right) = \\frac{\\sqrt{2\\pi}}{2\\sqrt{e}} = \\sqrt{\\frac{2\\pi}{4e}} = \\sqrt{\\frac{\\pi}{2e}}$$\n\n**Analysis of Trade-offs and Extensions**\n\nThe parameter $a$ controls the placement of the support points. The expression $\\alpha(a) = \\frac{a\\sqrt{2\\pi}}{2} \\exp\\left(-\\frac{a^2}{2}\\right)$ quantifies the trade-off. If $a$ is very small (close to $0$), the support points are near the mode. The tangents are nearly horizontal, creating a loose envelope with a large area, leading to a low acceptance rate ($\\alpha(a) \\to 0$ as $a \\to 0$). Conversely, if $a$ is very large, the support points are far in the tails. The tangents are very steep, creating a tight fit in the tails but forming an extremely high and narrow peak at $x=0$. The area of the envelope, dominated by this peak, becomes large again, and the acceptance rate also approaches zero ($\\alpha(a) \\to 0$ as $a \\to \\infty$). The optimal value $a^{\\star}=1$ represents an optimal balance, placing the support points at one standard deviation from the mean, which minimizes the total area under the envelope $g(x)$ and thereby maximizes the acceptance rate $\\alpha$. This minimizes the expected number of draws $J(a)$ needed to obtain one accepted sample.\n\nIncreasing the number of support points would allow for a more refined piecewise linear upper hull $u(x)$. Since the new hull is the minimum of a larger set of tangent lines, it must lie at or below the two-point hull: $u_{\\text{new}}(x) \\leq u(x)$ for all $x$. This results in a tighter envelope, $g_{\\text{new}}(x) \\leq g(x)$, a smaller envelope integral $\\int g_{\\text{new}}(x)dx$, and thus a higher acceptance rate $\\alpha$. This reduces the cost $J$. This is the principle behind Adaptive Rejection Sampling, where points are added iteratively to improve the envelope.\n\nA squeeze function $s(x)$ is a function satisfying $0 \\le s(x) \\le f(x)$ for all $x$. In a rejection sampler, one can perform a cheap pre-test: if a uniform deviate $U$ satisfies $U \\cdot g(X) \\le s(X)$, the sample $X$ is accepted without evaluating $f(X)$. This is possible because $s(X) \\le f(X)$ guarantees that $U \\cdot g(X) \\le f(X)$ is also true. The total acceptance rate $\\alpha$ remains unchanged, as it is ultimately determined by the ratio of the areas under $f(x)$ and $g(x)$. However, the squeeze reduces the *average number of evaluations of the target function $f(x)$* per accepted sample. By allowing some samples to be accepted \"for free,\" an efficient squeeze function (one whose integral is close to that of $f(x)$) can substantially lower the overall computational cost, especially when $f(x)$ is expensive to compute, without altering the fundamental acceptance probability $\\alpha$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 1 & \\sqrt{\\frac{\\pi}{2e}} \\end{pmatrix} } $$"
        },
        {
            "introduction": "Beyond pure computational efficiency, modern algorithms must often satisfy external constraints like data privacy. This practice delves into the increasingly important intersection of stochastic simulation and differential privacy, a key technique in trustworthy machine learning. You will analyze a scenario  where a sampler's parameters depend on sensitive data and quantify the exact trade-off between the strength of the privacy guarantee, controlled by budget $\\varepsilon$, and the resulting utility, measured by the sampler's acceptance probability.",
            "id": "3335750",
            "problem": "Consider a one-dimensional target density that is log-concave and data-dependent through a location parameter, specifically a Laplace distribution with known scale parameter $b_0 > 0$ and data-dependent location parameter $\\mu(D)$:\n$$\nf(x \\mid D) \\;=\\; \\frac{1}{2 b_0} \\exp\\!\\Big(-\\frac{|x - \\mu(D)|}{b_0}\\Big), \\quad x \\in \\mathbb{R}.\n$$\nAssume the dataset $D$ consists of $n$ real-valued observations bounded in the unit interval, so that any neighboring datasets differ in exactly one entry and the empirical mean has global sensitivity $\\Delta_1 = 1/n$. To construct an adaptive envelope for rejection sampling with a proposal that tracks the data-dependent location, we privatize the support-point update for $\\mu(D)$ via the Laplace mechanism: we release\n$$\n\\widetilde{\\mu} \\;=\\; \\widehat{\\mu}(D) \\;+\\; \\eta, \\quad \\eta \\sim \\text{Laplace}(s), \\quad s \\;=\\; \\frac{\\Delta_1}{\\varepsilon} \\;=\\; \\frac{1}{n \\varepsilon},\n$$\nwhere $\\varepsilon > 0$ is the privacy budget and $\\widehat{\\mu}(D)$ is any location estimator of $\\mu(D)$ with sensitivity bounded by $\\Delta_1$ (for example, the empirical mean). Use the proposal density\n$$\nq(x \\mid \\widetilde{\\mu}) \\;=\\; \\frac{1}{2 b_0} \\exp\\!\\Big(-\\frac{|x - \\widetilde{\\mu}|}{b_0}\\Big),\n$$\nand define the rejection sampler envelope constant $M(\\eta)$ as the smallest constant such that\n$$\nf(x \\mid D) \\;\\le\\; M(\\eta)\\, q(x \\mid \\widetilde{\\mu}) \\quad \\text{for all } x \\in \\mathbb{R}.\n$$\nAssume $n \\varepsilon b_0 > 1$ so that the expectations below are finite.\n\nUsing only foundational definitions from rejection sampling, log-concavity, and $\\varepsilon$-differential privacy (DP) together with the Laplace mechanism, derive from first principles:\n\n1. An explicit expression for $M(\\eta)$ in terms of $b_0$ and $|\\eta|$ obtained by comparing $f(\\cdot \\mid D)$ and $q(\\cdot \\mid \\widetilde{\\mu})$ pointwise.\n2. The expected envelope inflation $\\mathbb{E}[M(\\eta)]$ with respect to the noise $\\eta \\sim \\text{Laplace}(s)$.\n3. The expected acceptance probability of the rejection sampler, $\\mathbb{E}[1/M(\\eta)]$, with respect to the noise $\\eta$.\n\nYou may use that the acceptance probability of rejection sampling with envelope $M(\\eta)\\,q(x \\mid \\widetilde{\\mu})$ equals $\\int q(x \\mid \\widetilde{\\mu}) \\frac{f(x \\mid D)}{M(\\eta)\\,q(x \\mid \\widetilde{\\mu})}\\,dx \\,=\\, \\frac{1}{M(\\eta)}$. Express your final answer as a closed-form row matrix containing, in this order, the expressions for $\\mathbb{E}[M(\\eta)]$ and $\\mathbb{E}[1/M(\\eta)]$ as functions of $n$, $\\varepsilon$, and $b_0$. No numerical rounding is required.",
            "solution": "The user has asked for a detailed derivation of the expected envelope inflation and the expected acceptance probability for a rejection sampler using a data-dependent Laplace target density and a privatized Laplace proposal density.\n\nThe problem will be solved in three stages as requested, following a validation step.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Target Density**: $f(x \\mid D) = \\frac{1}{2 b_0} \\exp\\left(-\\frac{|x - \\mu(D)|}{b_0}\\right)$, with $b_0 > 0$.\n- **Dataset Properties**: $D$ has $n$ real-valued observations in $[0, 1]$. Neighboring datasets differ in one entry.\n- **Estimator Sensitivity**: The empirical mean has global sensitivity $\\Delta_1 = 1/n$.\n- **Privatized Location**: $\\widetilde{\\mu} = \\widehat{\\mu}(D) + \\eta$.\n- **Noise Distribution**: $\\eta \\sim \\text{Laplace}(s)$, where $s = \\frac{\\Delta_1}{\\varepsilon} = \\frac{1}{n \\varepsilon}$ for a privacy budget $\\varepsilon > 0$.\n- **Estimator**: $\\widehat{\\mu}(D)$ is a location estimator of $\\mu(D)$ with sensitivity bounded by $\\Delta_1$.\n- **Proposal Density**: $q(x \\mid \\widetilde{\\mu}) = \\frac{1}{2 b_0} \\exp\\left(-\\frac{|x - \\widetilde{\\mu}|}{b_0}\\right)$.\n- **Envelope Constant**: $M(\\eta)$ is the smallest constant such that $f(x \\mid D) \\le M(\\eta)\\, q(x \\mid \\widetilde{\\mu})$ for all $x \\in \\mathbb{R}$.\n- **Assumption**: $n \\varepsilon b_0 > 1$.\n- **Acceptance Probability**: The acceptance probability is $1/M(\\eta)$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded, well-posed, and objective. It draws on standard, verifiable principles from statistics (Laplace distribution, rejection sampling) and computer science (differential privacy, Laplace mechanism). The setup is self-contained and logically consistent. The one point of ambiguity is the relationship between $\\mu(D)$ and $\\widehat{\\mu}(D)$. However, the requirement that the first result, $M(\\eta)$, be expressed solely in terms of $b_0$ and $|\\eta|$ implicitly resolves this ambiguity by requiring that the difference $\\widehat{\\mu}(D) - \\mu(D)$ be zero. Without this assumption, the result for $M(\\eta)$ would depend on this difference, which is not provided. This interpretation makes the problem well-posed. The problem is formalizable and directly pertains to the specified topic. All constraints and data are consistent.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Derivation of the quantities of interest\n\nThe solution proceeds by first deriving an expression for the envelope constant $M(\\eta)$, and then using this expression to compute the required expectations $\\mathbb{E}[M(\\eta)]$ and $\\mathbb{E}[1/M(\\eta)]$.\n\n**1. Expression for the envelope constant $M(\\eta)$**\n\nBy definition, $M(\\eta)$ is the smallest constant satisfying the inequality $f(x \\mid D) \\le M(\\eta) q(x \\mid \\widetilde{\\mu})$, which implies that $M(\\eta)$ must be the supremum of the ratio of the two densities over their support:\n$$\nM(\\eta) = \\sup_{x \\in \\mathbb{R}} \\frac{f(x \\mid D)}{q(x \\mid \\widetilde{\\mu})}\n$$\nSubstituting the expressions for the target and proposal densities, we get:\n$$\nM(\\eta) = \\sup_{x \\in \\mathbb{R}} \\frac{\\frac{1}{2 b_0} \\exp\\left(-\\frac{|x - \\mu(D)|}{b_0}\\right)}{\\frac{1}{2 b_0} \\exp\\left(-\\frac{|x - \\widetilde{\\mu}|}{b_0}\\right)} = \\sup_{x \\in \\mathbb{R}} \\exp\\left( \\frac{|x - \\widetilde{\\mu}| - |x - \\mu(D)|}{b_0} \\right)\n$$\nSince the exponential function is monotonically increasing, we can find the supremum by maximizing the argument of the exponential:\n$$\n\\ln(M(\\eta)) = \\frac{1}{b_0} \\sup_{x \\in \\mathbb{R}} \\left( |x - \\widetilde{\\mu}| - |x - \\mu(D)| \\right)\n$$\nBy the reverse triangle inequality, for any real numbers $a$, $b$, and $c$, we have $|a - b| \\le |a-c| + |c-b|$, which can be rearranged to $|a-c| - |b-c| \\le |a-b|$. Applying this with $c=x$, $a=\\widetilde{\\mu}$, and $b=\\mu(D)$, we have:\n$$\n|x - \\widetilde{\\mu}| - |x - \\mu(D)| \\le |\\widetilde{\\mu} - \\mu(D)|\n$$\nThis upper bound is attained, for example, for any $x$ on the ray extending from $\\mu(D)$ away from $\\widetilde{\\mu}$. Thus, the supremum is exactly $|\\widetilde{\\mu} - \\mu(D)|$.\n$$\n\\sup_{x \\in \\mathbb{R}} \\left( |x - \\widetilde{\\mu}| - |x - \\mu(D)| \\right) = |\\widetilde{\\mu} - \\mu(D)|\n$$\nThe problem states that $M(\\eta)$ must be expressed in terms of $b_0$ and $|\\eta|$. We have $\\widetilde{\\mu} = \\widehat{\\mu}(D) + \\eta$. This implies that $|\\widetilde{\\mu} - \\mu(D)| = |\\widehat{\\mu}(D) - \\mu(D) + \\eta|$. For this expression to depend only on $|\\eta|$, we must assume that the estimator is exact for the given data $D$, i.e., $\\widehat{\\mu}(D) = \\mu(D)$. This is a reasonable simplifying assumption given the phrasing of the problem. Under this assumption, we have:\n$$\n|\\widetilde{\\mu} - \\mu(D)| = |\\mu(D) + \\eta - \\mu(D)| = |\\eta|\n$$\nTherefore, the envelope constant is:\n$$\nM(\\eta) = \\exp\\left(\\frac{|\\eta|}{b_0}\\right)\n$$\n\n**2. Expected envelope inflation $\\mathbb{E}[M(\\eta)]$**\n\nThe noise $\\eta$ is drawn from a Laplace distribution with scale $s$, $\\eta \\sim \\text{Laplace}(s)$, which has the probability density function $p(\\eta) = \\frac{1}{2s} \\exp\\left(-\\frac{|\\eta|}{s}\\right)$. The expectation of $M(\\eta)$ is found by integrating $M(\\eta)$ against this density:\n$$\n\\mathbb{E}[M(\\eta)] = \\int_{-\\infty}^{\\infty} M(\\eta) p(\\eta) d\\eta = \\int_{-\\infty}^{\\infty} \\exp\\left(\\frac{|\\eta|}{b_0}\\right) \\frac{1}{2s} \\exp\\left(-\\frac{|\\eta|}{s}\\right) d\\eta\n$$\nWe can simplify the integrand:\n$$\n\\mathbb{E}[M(\\eta)] = \\frac{1}{2s} \\int_{-\\infty}^{\\infty} \\exp\\left(|\\eta|\\left(\\frac{1}{b_0} - \\frac{1}{s}\\right)\\right) d\\eta\n$$\nThe integrand is an even function of $\\eta$, so we can integrate over the positive real axis and multiply by $2$:\n$$\n\\mathbb{E}[M(\\eta)] = 2 \\cdot \\frac{1}{2s} \\int_{0}^{\\infty} \\exp\\left(\\eta\\left(\\frac{1}{b_0} - \\frac{1}{s}\\right)\\right) d\\eta = \\frac{1}{s} \\int_{0}^{\\infty} \\exp\\left(\\eta\\frac{s - b_0}{s b_0}\\right) d\\eta\n$$\nFor this integral to converge, the coefficient of $\\eta$ in the exponent must be negative. This requires $s - b_0 < 0$, or $s < b_0$. Substituting $s = \\frac{1}{n\\varepsilon}$, this condition is $\\frac{1}{n\\varepsilon} < b_0$, which is equivalent to $n\\varepsilon b_0 > 1$. This is precisely the assumption given in the problem statement.\nEvaluating the integral:\n$$\n\\mathbb{E}[M(\\eta)] = \\frac{1}{s} \\left[ \\frac{1}{\\frac{s - b_0}{s b_0}} \\exp\\left(\\eta\\frac{s - b_0}{s b_0}\\right) \\right]_{0}^{\\infty} = \\frac{1}{s} \\left( 0 - \\frac{s b_0}{s - b_0} \\right) = \\frac{b_0}{b_0 - s}\n$$\nFinally, we substitute $s = \\frac{1}{n\\varepsilon}$ to express the result in terms of $n$, $\\varepsilon$, and $b_0$:\n$$\n\\mathbb{E}[M(\\eta)] = \\frac{b_0}{b_0 - \\frac{1}{n\\varepsilon}} = \\frac{b_0}{\\frac{n\\varepsilon b_0 - 1}{n\\varepsilon}} = \\frac{n\\varepsilon b_0}{n\\varepsilon b_0 - 1}\n$$\n\n**3. Expected acceptance probability $\\mathbb{E}[1/M(\\eta)]$**\n\nThe acceptance probability for a given $\\eta$ is $1/M(\\eta)$. We need to find its expectation with respect to the distribution of $\\eta$. We have $1/M(\\eta) = \\exp\\left(-\\frac{|\\eta|}{b_0}\\right)$.\n$$\n\\mathbb{E}[1/M(\\eta)] = \\int_{-\\infty}^{\\infty} \\frac{1}{M(\\eta)} p(\\eta) d\\eta = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{|\\eta|}{b_0}\\right) \\frac{1}{2s} \\exp\\left(-\\frac{|\\eta|}{s}\\right) d\\eta\n$$\nSimplifying the integrand:\n$$\n\\mathbb{E}[1/M(\\eta)] = \\frac{1}{2s} \\int_{-\\infty}^{\\infty} \\exp\\left(-|\\eta|\\left(\\frac{1}{b_0} + \\frac{1}{s}\\right)\\right) d\\eta\n$$\nAgain, the integrand is an even function:\n$$\n\\mathbb{E}[1/M(\\eta)] = 2 \\cdot \\frac{1}{2s} \\int_{0}^{\\infty} \\exp\\left(-\\eta\\left(\\frac{1}{b_0} + \\frac{1}{s}\\right)\\right) d\\eta = \\frac{1}{s} \\int_{0}^{\\infty} \\exp\\left(-\\eta\\frac{s+b_0}{sb_0}\\right) d\\eta\n$$\nThe coefficient in the exponent is always negative since $s, b_0 > 0$, so the integral always converges. Evaluating the integral:\n$$\n\\mathbb{E}[1/M(\\eta)] = \\frac{1}{s} \\left[ \\frac{1}{-\\frac{s+b_0}{sb_0}} \\exp\\left(-\\eta\\frac{s+b_0}{sb_0}\\right) \\right]_{0}^{\\infty} = \\frac{1}{s} \\left( 0 - \\frac{-sb_0}{s+b_0} \\right) = \\frac{b_0}{s+b_0}\n$$\nSubstituting $s = \\frac{1}{n\\varepsilon}$:\n$$\n\\mathbb{E}[1/M(\\eta)] = \\frac{b_0}{\\frac{1}{n\\varepsilon} + b_0} = \\frac{b_0}{\\frac{1 + n\\varepsilon b_0}{n\\varepsilon}} = \\frac{n\\varepsilon b_0}{1 + n\\varepsilon b_0}\n$$\nThe final answer requires a row matrix containing $\\mathbb{E}[M(\\eta)]$ and $\\mathbb{E}[1/M(\\eta)]$ in that order.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{n \\varepsilon b_0}{n \\varepsilon b_0 - 1} & \\frac{n \\varepsilon b_0}{n \\varepsilon b_0 + 1} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Achieving high performance requires designing algorithms that are aware of the underlying hardware architecture, a principle especially true for parallel processors like GPUs where control flow divergence can severely degrade performance. In this exercise , you will design piecewise-constant envelopes and squeezes specifically to minimize branch divergence in a Single Instruction, Multiple Data (SIMD) execution model. This practice will allow you to analytically derive the resulting efficiency, bridging the gap between algorithmic theory and high-performance implementation.",
            "id": "3335770",
            "problem": "You are to design and analyze rejection sampling with proposal selection, envelopes, and squeezes so as to be friendly to Graphics Processing Unit (GPU) Single Instruction, Multiple Data (SIMD) execution. The setting is the following. A target probability density function $f_{\\alpha}(x)$ on $[0,1]$ is given by the Beta family $f_{\\alpha}(x) = \\alpha x^{\\alpha - 1}$ with parameter $\\alpha \\ge 1$. A proposal $g(x)$ is chosen as the uniform density on $[0,1]$. Let $M$ be a finite constant satisfying $f_{\\alpha}(x) \\le M g(x)$ for all $x \\in [0,1]$. Consider rejection sampling in the form: sample $x \\sim g$, sample $u \\sim \\mathrm{Uniform}(0,1)$, accept $x$ if $u \\le f_{\\alpha}(x)/(M g(x))$ and reject otherwise. To reduce evaluation cost and to align branch decisions across threads, define a lower bounding squeeze $s(x)$ and an upper bounding envelope $e(x)$ for $f_{\\alpha}(x)$ such that $s(x) \\le f_{\\alpha}(x) \\le e(x)$ for all $x \\in [0,1]$. Then implement the following branch structure, which is standard in accelerated rejection sampling:\n- If $u \\le s(x)/(M g(x))$, accept $x$ without evaluating $f_{\\alpha}(x)$.\n- Else if $u > e(x)/(M g(x))$, reject $x$ without evaluating $f_{\\alpha}(x)$.\n- Else evaluate $f_{\\alpha}(x)$ and accept $x$ if $u \\le f_{\\alpha}(x)/(M g(x))$.\n\nDesign $s(x)$ and $e(x)$ to align branch decisions across threads and reduce divergence by quantizing the domain. Specifically, partition $[0,1]$ into $K$ equal-width bins $B_k = [\\frac{k-1}{K}, \\frac{k}{K}]$ for $k \\in \\{1,2,\\dots,K\\}$, and define $s(x)$ and $e(x)$ to be piecewise constant on each bin, using bin-wise extremal values of $f_{\\alpha}(x)$ for the squeeze and envelope. That is, for $x \\in B_k$, let $s(x)$ be equal to the minimum of $f_{\\alpha}(x)$ on $B_k$ and let $e(x)$ be equal to the maximum of $f_{\\alpha}(x)$ on $B_k$. Under $g(x)$ uniform on $[0,1]$, this construction yields branch probabilities that depend only on the bin endpoints and on $\\alpha$.\n\nDefine a warp of size $W$ threads proceeding in lockstep as in Single Instruction, Multiple Data (SIMD). Let the per-thread costs of executing each branch be $\\tau_a$ for the early-accept branch, $\\tau_r$ for the early-reject branch, and $\\tau_m$ for the middle branch that evaluates $f_{\\alpha}(x)$. Threads draw their $(x,u)$ pairs independently. Define the SIMD efficiency as the ratio of the expected total per-thread work to the expected warp-time work, where warp-time aggregates the cost contributed by each branch if at least one thread in the warp executes that branch. The efficiency should be a number in $[0,1]$ and must be derived from first principles starting from the independence of threads, the definition of rejection sampling, and the definitions of envelope and squeeze.\n\nYour tasks are:\n- Starting from the formal definitions above, derive expressions for the early-accept probability, the early-reject probability, and the middle-region probability in terms of $\\alpha$ and $K$ under the uniform proposal $g(x)$ and the bin-wise constant $s(x)$ and $e(x)$ construction.\n- Derive an analytical expression for the expected SIMD efficiency as a function of $\\alpha$, $K$, $W$, $\\tau_a$, $\\tau_r$, and $\\tau_m$ under the independence of threads and the described branch structure. The derivation must begin from the core definitions and must not assume any pre-specified shortcut formulas.\n- Implement a complete, runnable program that computes the SIMD efficiency for the specified test cases. The program must perform only deterministic computations and must not use any randomness.\n\nUse the following test suite, each case specified as $(\\alpha, K, W, \\tau_a, \\tau_r, \\tau_m)$:\n- Case $1$: $(1, 64, 32, 1.0, 1.0, 8.0)$, boundary case where the target is uniform.\n- Case $2$: $(2, 32, 32, 1.0, 1.0, 8.0)$, moderate shape and moderate bin count.\n- Case $3$: $(64, 32, 32, 1.0, 1.0, 8.0)$, sharply increasing target with moderate bin count.\n- Case $4$: $(4, 1, 32, 1.0, 1.0, 8.0)$, edge case with a single bin.\n- Case $5$: $(4, 64, 64, 1.0, 1.0, 8.0)$, larger warp size with aligned bin count.\n\nYour program should produce a single line of output containing the results as a comma-separated list of floating-point numbers enclosed in square brackets, ordered by the cases above (for example, $[r_1,r_2,r_3,r_4,r_5]$). No physical units or angle units are involved in this problem; all outputs are dimensionless floats.",
            "solution": "The problem is to derive and compute the Single Instruction, Multiple Data (SIMD) efficiency of an accelerated rejection sampling algorithm. The derivation must be from first principles.\n\nFirst, we formalize the components of the rejection sampling algorithm.\nThe target probability density function (PDF) is from the Beta family, given by $f_{\\alpha}(x) = \\alpha x^{\\alpha - 1}$ for $x \\in [0,1]$ and parameter $\\alpha \\ge 1$.\nThe proposal PDF is the uniform density on $[0,1]$, $g(x) = 1$.\nThe rejection sampling method requires a constant $M$ such that $f_{\\alpha}(x) \\le M g(x)$ for all $x \\in [0,1]$. Given $g(x)=1$, this inequality is $f_{\\alpha}(x) \\le M$. To find the smallest such $M$, we find the maximum of $f_{\\alpha}(x)$. The derivative is $f'_{\\alpha}(x) = \\alpha(\\alpha-1)x^{\\alpha-2}$. Since $\\alpha \\ge 1$, $f'_{\\alpha}(x) \\ge 0$ for $x \\in [0,1]$, which means $f_{\\alpha}(x)$ is a non-decreasing function. Its maximum value on $[0,1]$ occurs at $x=1$, where $f_{\\alpha}(1) = \\alpha(1)^{\\alpha-1} = \\alpha$. We choose the optimal constant $M = \\alpha$.\nThe core rejection condition is $u \\le \\frac{f_{\\alpha}(x)}{M g(x)}$, where $u \\sim \\mathrm{Uniform}(0,1)$. Substituting our functions, this becomes $u \\le \\frac{\\alpha x^{\\alpha-1}}{\\alpha \\cdot 1} = x^{\\alpha-1}$.\n\nNext, we define the squeeze function $s(x)$ and envelope function $e(x)$. The domain $[0,1]$ is partitioned into $K$ equal-width bins, $B_k = [\\frac{k-1}{K}, \\frac{k}{K}]$ for $k \\in \\{1, 2, \\dots, K\\}$. For any $x \\in B_k$, $s(x)$ and $e(x)$ are defined as the minimum and maximum of $f_{\\alpha}(x)$ on $B_k$, respectively. Since $f_{\\alpha}(x)$ is non-decreasing, these extrema occur at the bin endpoints:\nFor $x \\in B_k$:\n$s(x) = \\min_{z \\in B_k} f_{\\alpha}(z) = f_{\\alpha}\\left(\\frac{k-1}{K}\\right) = \\alpha \\left(\\frac{k-1}{K}\\right)^{\\alpha-1}$\n$e(x) = \\max_{z \\in B_k} f_{\\alpha}(z) = f_{\\alpha}\\left(\\frac{k}{K}\\right) = \\alpha \\left(\\frac{k}{K}\\right)^{\\alpha-1}$\n\nThe accelerated rejection sampling algorithm uses a three-stage branch structure. A sample pair $(x, u)$ is drawn, with $x \\sim g$ and $u \\sim \\mathrm{Uniform}(0,1)$.\n$1$. Early Accept if $u \\le \\frac{s(x)}{M g(x)} = \\frac{s(x)}{\\alpha}$.\n$2$. Early Reject if $u > \\frac{e(x)}{M g(x)} = \\frac{e(x)}{\\alpha}$.\n$3$. Middle Branch otherwise, where $f_{\\alpha}(x)$ is evaluated.\n\nThe probabilities for a single thread to enter each branch ($p_a, p_r, p_m$) are determined by the joint distribution of $(x, u)$, which is uniform on the unit square $[0,1] \\times [0,1]$. The probabilities are the areas of the corresponding regions.\nThe early-accept probability $p_a$ is:\n$$p_a = P\\left(u \\le \\frac{s(x)}{\\alpha}\\right) = \\int_0^1 \\frac{s(x)}{\\alpha} dx$$\nSince $s(x)$ is piecewise constant, we sum the integrals over each bin:\n$$p_a = \\sum_{k=1}^K \\int_{\\frac{k-1}{K}}^{\\frac{k}{K}} \\frac{1}{\\alpha} \\left(\\alpha \\left(\\frac{k-1}{K}\\right)^{\\alpha-1}\\right) dx = \\sum_{k=1}^K \\frac{1}{K} \\left(\\frac{k-1}{K}\\right)^{\\alpha-1} = \\frac{1}{K^\\alpha} \\sum_{k=1}^K (k-1)^{\\alpha-1}$$\nBy re-indexing with $j=k-1$, we get $p_a = \\frac{1}{K^\\alpha} \\sum_{j=0}^{K-1} j^{\\alpha-1}$. Note that for $\\alpha=1$, $0^0=1$.\n\nThe early-reject probability $p_r$ is:\n$$p_r = P\\left(u > \\frac{e(x)}{\\alpha}\\right) = \\int_0^1 \\left(1 - \\frac{e(x)}{\\alpha}\\right) dx$$\n$$p_r = \\sum_{k=1}^K \\int_{\\frac{k-1}{K}}^{\\frac{k}{K}} \\left(1 - \\frac{1}{\\alpha} \\alpha \\left(\\frac{k}{K}\\right)^{\\alpha-1}\\right) dx = \\sum_{k=1}^K \\frac{1}{K} \\left(1 - \\left(\\frac{k}{K}\\right)^{\\alpha-1}\\right) = 1 - \\frac{1}{K^\\alpha} \\sum_{k=1}^K k^{\\alpha-1}$$\n\nThe middle-branch probability $p_m$ is for the region between the squeeze and envelope. Since the branches are mutually exclusive and cover all possibilities, $p_m = 1 - p_a - p_r$.\n$$p_m = 1 - \\left(\\frac{1}{K^\\alpha} \\sum_{j=0}^{K-1} j^{\\alpha-1}\\right) - \\left(1 - \\frac{1}{K^\\alpha} \\sum_{k=1}^K k^{\\alpha-1}\\right) = \\frac{1}{K^\\alpha} \\left(\\sum_{k=1}^K k^{\\alpha-1} - \\sum_{j=0}^{K-1} j^{\\alpha-1}\\right)$$\nThe difference of the sums is $(1^{\\alpha-1} + \\dots + K^{\\alpha-1}) - (0^{\\alpha-1} + \\dots + (K-1)^{\\alpha-1}) = K^{\\alpha-1} - 0^{\\alpha-1}$.\nThus, $p_m = \\frac{K^{\\alpha-1} - 0^{\\alpha-1}}{K^\\alpha} = \\frac{1}{K} - \\frac{0^{\\alpha-1}}{K^\\alpha}$. For $\\alpha>1$, $p_m=1/K$. For $\\alpha=1$, $0^{\\alpha-1}=0^0=1$, so $p_m=0$.\n\nNow, we derive the SIMD efficiency.\nLet $C_i$ be the execution cost for thread $i$, a random variable taking values from $\\{\\tau_a, \\tau_r, \\tau_m\\}$. The expected per-thread work is:\n$$E_{thread} = E[C_i] = \\tau_a p_a + \\tau_r p_r + \\tau_m p_m$$\nThe warp-time work, as defined, aggregates costs for all branches taken by at least one thread in a warp of size $W$. Let $A_W$, $R_W$, and $M_W$ be indicator variables that are $1$ if at least one thread takes the early-accept, early-reject, or middle branch, respectively. The warp-time work is $C_{warp} = \\tau_a A_W + \\tau_r R_W + \\tau_m M_W$.\nThe expected warp-time work is $E_{warp} = E[C_{warp}] = \\tau_a E[A_W] + \\tau_r E[R_W] + \\tau_m E[M_W]$.\nThe expectation of an indicator is the probability of the event. Since threads are independent:\n$E[A_W] = P(A_W=1) = 1 - P(\\text{no thread takes early-accept}) = 1 - (1 - p_a)^W$.\nSimilarly, $E[R_W] = 1 - (1-p_r)^W$ and $E[M_W] = 1 - (1-p_m)^W$.\nSo, the expected warp-time work is:\n$$E_{warp} = \\tau_a(1 - (1-p_a)^W) + \\tau_r(1 - (1-p_r)^W) + \\tau_m(1 - (1-p_m)^W)$$\nThe SIMD efficiency $\\eta$ is defined as the ratio of expected total per-thread work to expected warp-time work. A standard interpretation of efficiency, which gives a value in $[0,1]$, is the ratio of ideal work to actual resources used. The problem statement defines efficiency as the ratio of expected *total* per-thread work to expected warp-time work. This phrasing is ambiguous. If \"total per-thread work\" means the work of the entire warp ($W \\times E_{thread}$), the efficiency would be $W \\times E_{thread} / E_{warp}$, which is the speedup and can be greater than 1. A more standard definition of efficiency is speedup per worker, which would be $(W \\times E_{thread} / E_{warp}) / W = E_{thread} / E_{warp}$. This ratio is guaranteed to be in $[0,1]$ because $E_{warp}$ is the sum of terms like $\\tau_k(1-(1-p_k)^W)$, and by Bernoulli's inequality, for $p_k \\in [0,1]$, $1-p_k W \\le (1-p_k)^W$, so $1-(1-p_k)^W \\le p_k W$. Therefore $E_{warp} \\le \\sum \\tau_k p_k W = W \\cdot E_{thread}$. We adopt the interpretation $\\eta = E_{thread} / E_{warp}$ as it fits the expected $[0,1]$ range.\n$$\\eta = \\frac{\\tau_a p_a + \\tau_r p_r + \\tau_m p_m}{\\tau_a(1 - (1-p_a)^W) + \\tau_r(1 - (1-p_r)^W) + \\tau_m(1 - (1-p_m)^W)}$$\nThese derived expressions are implemented to solve for the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the SIMD efficiency for rejection sampling based on a derived analytical formula.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (alpha, K, W, tau_a, tau_r, tau_m)\n    test_cases = [\n        (1.0, 64, 32, 1.0, 1.0, 8.0),   # Case 1\n        (2.0, 32, 32, 1.0, 1.0, 8.0),   # Case 2\n        (64.0, 32, 32, 1.0, 1.0, 8.0),  # Case 3\n        (4.0, 1, 32, 1.0, 1.0, 8.0),    # Case 4\n        (4.0, 64, 64, 1.0, 1.0, 8.0)    # Case 5\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, K, W, tau_a, tau_r, tau_m = case\n\n        # The derivation of probabilities is robust for alpha >= 1, including alpha = 1\n        # where 0^0 is handled as 1 by numpy.power, which is the correct convention here.\n        # No special case for alpha=1 is needed.\n        \n        # Calculate branch probabilities p_a, p_r, p_m\n        \n        # p_a = (1/K^alpha) * sum_{j=0}^{K-1} j^(alpha-1)\n        j = np.arange(K, dtype=np.float64)\n        alpha_minus_1 = alpha - 1.0\n        \n        # Use np.power for robust handling of 0^0\n        sum_for_pa = np.sum(np.power(j, alpha_minus_1))\n        K_to_alpha = np.power(float(K), alpha)\n        \n        p_a = sum_for_pa / K_to_alpha\n\n        # p_r = 1 - (1/K^alpha) * sum_{k=1}^{K} k^(alpha-1)\n        k = np.arange(1, K + 1, dtype=np.float64)\n        sum_for_pr = np.sum(np.power(k, alpha_minus_1))\n        \n        p_r = 1.0 - (sum_for_pr / K_to_alpha)\n        \n        # p_m = 1 - p_a - p_r for numerical stability\n        p_m = 1.0 - p_a - p_r\n        \n        # If K=1, then p_a=0, p_r=0, p_m=1. Let's check for small numerical errors.\n        if K == 1:\n            p_a, p_r, p_m = 0.0, 0.0, 1.0\n        \n        # Calculate expected per-thread work\n        E_thread = tau_a * p_a + tau_r * p_r + tau_m * p_m\n        \n        # If E_thread is zero, efficiency is zero (unless E_warp is also zero)\n        if E_thread == 0.0:\n            results.append(0.0)\n            continue\n\n        # Calculate expected warp-time work\n        # E_warp = tau_a(1-(1-p_a)^W) + tau_r(1-(1-p_r)^W) + tau_m(1-(1-p_m)^W)\n        prob_no_a = np.power(1.0 - p_a, W)\n        prob_no_r = np.power(1.0 - p_r, W)\n        prob_no_m = np.power(1.0 - p_m, W)\n        \n        E_warp = tau_a * (1.0 - prob_no_a) + tau_r * (1.0 - prob_no_r) + tau_m * (1.0 - prob_no_m)\n        \n        # Calculate SIMD efficiency\n        if E_warp == 0.0:\n            # This case occurs if all costs are 0, or if all probabilities are 0, etc.\n            # If E_thread is also 0, efficiency is indeterminate. Let's define it as 1.0 for perfect efficiency.\n            # If E_thread is non-zero, this is division by zero, an error.\n            # In our setup, taus are positive, so E_warp > 0.\n            eta = 1.0\n        else:\n            eta = E_thread / E_warp\n        \n        results.append(eta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}