{
    "hands_on_practices": [
        {
            "introduction": "Before relying on pre-built library functions, it is essential for a practitioner of stochastic simulation to understand how to construct fundamental random variate generators from first principles. This exercise guides you through implementing a complete and statistically exact generator for the gamma distribution. By building the celebrated Marsaglia-Tsang algorithm for shape parameters $k1$ and the augmentation method for $0k1$, you will gain a deep, practical understanding of state-of-the-art sampling techniques and solidify your knowledge of the relationships between the gamma, beta, and chi-squared distributions through a rigorous verification process .",
            "id": "3056410",
            "problem": "Implement a statistically exact random variate generator for the gamma distribution using acceptance–rejection sampling and augmentation, and verify its correctness through a set of tests grounded in fundamental distributional relationships. Specifically, construct a program that simulates independent and identically distributed samples from the gamma distribution with shape parameter $k$ and scale parameter $\\theta$, denoted by $\\Gamma(k,\\theta)$, relying on the following foundational base only: (i) the definition of the gamma distribution as a family of distributions with probability density function on $(0,\\infty)$, (ii) the law of large numbers for verifying sample moments, (iii) the acceptance–rejection method with a valid proposal distribution, (iv) the definition of the beta distribution as the distribution of a ratio of independent gamma random variables with the same scale, and (v) the definition of the chi-squared distribution as a special case of the gamma distribution. Do not use any pre-built gamma, beta, or chi-squared samplers.\n\nYour implementation must satisfy the following specification.\n- For $k1$, use an acceptance–rejection sampler based on a transformation of a standard normal proposal and a “squeeze” acceptance region. The method must be fully coded from first principles using independent uniform and standard normal variates.\n- For $0k1$, use augmentation: construct a $\\Gamma(k,\\theta)$ variate from a $\\Gamma(k+1,\\theta)$ variate and an independent $\\operatorname{Uniform}(0,1)$ variate via an appropriate transformation. You must handle the boundary case $k=1$ correctly.\n- The generator must accept arbitrary positive real $k$ and $\\theta$, and a requested sample size $n\\in\\mathbb{N}$, and return $n$ independent samples.\n\nUse the following well-tested facts as the fundamental base for your derivations and checks:\n- If $X\\sim\\Gamma(k,\\theta)$, then $\\mathbb{E}[X]=k\\,\\theta$ and $\\operatorname{Var}(X)=k\\,\\theta^{2}$.\n- If $X\\sim\\Gamma(a,1)$ and $Y\\sim\\Gamma(b,1)$ are independent, then $U=\\dfrac{X}{X+Y}$ follows the beta distribution with parameters $(a,b)$, with mean $\\dfrac{a}{a+b}$.\n- If $Z\\sim\\chi^{2}_{\\nu}$, then $Z\\sim\\Gamma\\!\\left(\\dfrac{\\nu}{2},2\\right)$, with $\\mathbb{E}[Z]=\\nu$ and $\\operatorname{Var}(Z)=2\\,\\nu$.\n- The acceptance–rejection method: if $(X,U)$ are generated from a proposal distribution and a uniform variable with an acceptance region designed so that accepted $X$ have the target density, then the distribution of accepted $X$ equals the target.\n\nTest suite and required outputs:\n- Use a fixed seed for the pseudorandom number generator to ensure reproducibility. Let the seed be the integer $123456789$.\n- For each test below, generate $n=200000$ independent samples and compute the indicated statistics. Each test returns one or two boolean values. Your program must aggregate these booleans in the order listed and print them in a single line as a comma-separated list enclosed in square brackets. No other output is permitted.\n\nTests:\n1. Gamma, shape greater than one: Let $k=3.5$, $\\theta=2.0$. Compute the sample mean $\\hat{m}$ and sample variance $\\hat{v}$ and compare to the targets $m^{\\star}=k\\,\\theta$ and $v^{\\star}=k\\,\\theta^{2}$. Output two booleans: $\\left|\\hat{m}-m^{\\star}\\right|0.03$ and $\\left|\\hat{v}-v^{\\star}\\right|0.30$.\n2. Gamma, shape less than one (augmentation): Let $k=0.7$, $\\theta=1.5$. Output two booleans: $\\left|\\hat{m}-m^{\\star}\\right|0.03$ and $\\left|\\hat{v}-v^{\\star}\\right|0.10$ with $m^{\\star}=k\\,\\theta$ and $v^{\\star}=k\\,\\theta^{2}$.\n3. Beta via ratio of independent gammas: Let $a=2.1$, $b=3.3$. Generate independent $X\\sim\\Gamma(a,1)$ and $Y\\sim\\Gamma(b,1)$, set $U=\\dfrac{X}{X+Y}$. Output one boolean: $\\left|\\hat{m}_{U}-\\dfrac{a}{a+b}\\right|0.01$, where $\\hat{m}_{U}$ is the sample mean of $U$.\n4. Chi-squared as a gamma: Let $\\nu=10$, which corresponds to $\\Gamma\\!\\left(\\dfrac{\\nu}{2},2\\right)$ with $k=\\dfrac{\\nu}{2}=5$ and $\\theta=2$. Output two booleans: $\\left|\\hat{m}-\\nu\\right|0.03$ and $\\left|\\hat{v}-2\\,\\nu\\right|0.20$.\n5. Boundary case $k=1$ (exponential): Let $k=1.0$, $\\theta=0.8$. Output two booleans: $\\left|\\hat{m}-k\\,\\theta\\right|0.02$ and $\\left|\\hat{v}-k\\,\\theta^{2}\\right|0.05$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[true1,true2,...]”), where each entry is the boolean result of the corresponding check in the exact order: test $1$ (mean, variance), test $2$ (mean, variance), test $3$, test $4$ (mean, variance), test $5$ (mean, variance).\n- No angles or physical units are involved in this problem; all outputs are unitless booleans.",
            "solution": "The user-provided problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and internally consistent. It poses a standard, albeit non-trivial, problem in computational statistics: implementing a random variate generator for the gamma distribution from first principles and verifying its properties. The problem statement provides all necessary definitions, algorithms, and test conditions for a unique and verifiable solution.\n\nThe solution is implemented by first constructing a generator for the standard gamma distribution, $\\Gamma(k,1)$, which is then extended to the general $\\Gamma(k,\\theta)$ distribution by exploiting the scaling property: if $X \\sim \\Gamma(k,1)$, then $\\theta X \\sim \\Gamma(k,\\theta)$. The generation of standard gamma variates is handled by three distinct methods, selected based on the value of the shape parameter $k$.\n\nThe required standard normal variates, $Z \\sim N(0,1)$, are synthesized from pairs of independent uniform variates, $U_1, U_2 \\sim \\operatorname{Uniform}(0,1)$, using the Box-Muller transform: $Z_1 = \\sqrt{-2 \\ln(U_1)} \\cos(2\\pi U_2)$ and $Z_2 = \\sqrt{-2 \\ln(U_1)} \\sin(2\\pi U_2)$. This transform produces two independent $N(0,1)$ samples for each pair of uniform samples.\n\nThe three cases for the shape parameter $k$ are as follows:\n\n1.  **For $k  1$**: The Marsaglia and Tsang (2000) acceptance-rejection method is used. This is a highly efficient algorithm.\n    Let the target distribution be the standard gamma distribution $\\Gamma(k,1)$. The method uses a clever transformation of a standard normal variate $Z \\sim N(0,1)$ to propose a candidate value $X$. The algorithm is as follows:\n    a. Pre-calculate constants $d = k - 1/3$ and $c = 1/\\sqrt{9d}$.\n    b. Generate a standard normal variate $Z$ and a uniform variate $U \\sim \\operatorname{Uniform}(0,1)$.\n    c. Calculate $V = (1+cZ)^3$. If $V \\le 0$, reject and return to step (b).\n    d. The proposed gamma-distributed value is $X = dV$.\n    e. Accept $X$ if the following condition holds: $\\ln(U)  \\frac{Z^2}{2} + d - X + d \\ln(V)$. Otherwise, reject $X$ and return to step (b).\n    This process is repeated until the desired number of samples is generated.\n\n2.  **For $0  k  1$**: An augmentation method, as described by Ahrens and Dieter (1974), is employed. This method leverages the generator for shape parameters greater than $1$. It is based on the property that if $Y \\sim \\Gamma(k+1,1)$ and $U \\sim \\operatorname{Uniform}(0,1)$ are independent, then the random variable $X = Y \\cdot U^{1/k}$ is distributed as $\\Gamma(k,1)$.\n    Since $0  k  1$, the shape parameter $k+1$ of the required variate $Y$ will be in the range $(1,2)$. Therefore, we can generate $Y$ using the acceptance-rejection method for $k  1$ described above. This is a direct method (no rejection), so one $Y$ and one $U$ produce one $X$.\n\n3.  **For $k = 1$**: This is a boundary case. The $\\Gamma(1,1)$ distribution is the standard exponential distribution, $\\operatorname{Exp}(1)$. This can be sampled efficiently and exactly using the inverse transform method. If $U \\sim \\operatorname{Uniform}(0,1)$, then $X = -\\ln(U)$ is distributed as $\\operatorname{Exp}(1)$.\n\nThe final implementation consolidates these methods into a single generator function. This generator is then used to perform the five specified tests. For each test, a large number of samples ($n=200000$) is generated, and their sample moments (mean and variance) are compared against the theoretical values derived from the properties of the gamma, beta, and chi-squared distributions. The results of these comparisons, which are boolean values, are collected and printed in the specified format. The use of a fixed seed for the random number generator ensures that the results are reproducible.",
            "answer": "```python\nimport numpy as np\n\ndef _my_normal_sampler(size: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Generates standard normal variates using the Box-Muller transform.\"\"\"\n    if size == 0:\n        return np.array([])\n    \n    num_pairs = (size + 1) // 2\n    u1 = rng.uniform(size=num_pairs)\n    # Avoid u1=0 for log\n    u1[u1 == 0] = 1e-100 \n    u2 = rng.uniform(size=num_pairs)\n    \n    r = np.sqrt(-2.0 * np.log(u1))\n    theta = 2.0 * np.pi * u2\n    \n    z1 = r * np.cos(theta)\n    z2 = r * np.sin(theta)\n    \n    all_z = np.empty(2 * num_pairs)\n    all_z[0::2] = z1\n    all_z[1::2] = z2\n    \n    return all_z[:size]\n\ndef _gamma_sampler_gt1(k: float, n: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generates n samples from a standard Gamma(k, 1) distribution for k  1.\n    Uses the Marsaglia and Tsang (2000) acceptance-rejection method.\n    \"\"\"\n    d = k - 1.0/3.0\n    c = 1.0 / np.sqrt(9.0 * d)\n    \n    samples = np.empty(n)\n    count = 0\n    \n    while count  n:\n        needed = n - count\n        \n        # Generate candidate Zs and Us\n        z = _my_normal_sampler(needed, rng)\n        u = rng.uniform(size=needed)\n        \n        v = (1.0 + c * z)**3\n        \n        # Filter for valid V  0\n        mask_valid_v = v  0\n        z_valid = z[mask_valid_v]\n        u_valid = u[mask_valid_v]\n        v_valid = v[mask_valid_v]\n        \n        if len(v_valid) == 0:\n            continue\n            \n        x_cand = d * v_valid\n        \n        # Squeeze test and main acceptance condition\n        log_u_valid = np.log(u_valid)\n        accept_mask = log_u_valid  (0.5 * z_valid**2 + d - x_cand + d * np.log(v_valid))\n        \n        accepted_samples = x_cand[accept_mask]\n        num_accepted = len(accepted_samples)\n        \n        if num_accepted  0:\n            fill_count = min(num_accepted, needed)\n            samples[count:count + fill_count] = accepted_samples[:fill_count]\n            count += fill_count\n            \n    return samples\n\ndef _gamma_sampler_eq1(n: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generates n samples from a standard Gamma(1, 1) distribution (Exponential).\n    Uses the inverse transform method.\n    \"\"\"\n    u = rng.uniform(size=n)\n    # Avoid u=0 for log\n    u[u == 0] = 1e-100\n    return -np.log(u)\n\ndef _gamma_sampler_lt1(k: float, n: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generates n samples from a standard Gamma(k, 1) distribution for 0  k  1.\n    Uses augmentation method: Y * U^(1/k) where Y ~ Gamma(k+1, 1).\n    \"\"\"\n    # Generate Y ~ Gamma(k+1, 1)\n    # Since k+1 is in (1, 2), we use the k1 sampler.\n    y_samples = _gamma_sampler_gt1(k + 1.0, n, rng)\n    \n    # Generate U ~ Uniform(0, 1)\n    u_samples = rng.uniform(size=n)\n    \n    return y_samples * (u_samples ** (1.0/k))\n\ndef gamma_sampler(k: float, theta: float, n: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generates n samples from a Gamma(k, theta) distribution.\n    \"\"\"\n    if k = 0 or theta = 0 or not isinstance(n, int) or n  0:\n        raise ValueError(\"k and theta must be positive, n must be a non-negative integer.\")\n\n    if k  1.0:\n        std_samples = _gamma_sampler_gt1(k, n, rng)\n    elif k == 1.0:\n        std_samples = _gamma_sampler_eq1(n, rng)\n    else:  # 0  k  1\n        std_samples = _gamma_sampler_lt1(k, n, rng)\n        \n    return std_samples * theta\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    seed = 123456789\n    n = 200000\n    rng = np.random.default_rng(seed)\n    \n    results = []\n\n    # Test 1: Gamma, shape greater than one\n    k, theta = 3.5, 2.0\n    samples_1 = gamma_sampler(k, theta, n, rng)\n    m_star_1, v_star_1 = k * theta, k * theta**2\n    m_hat_1, v_hat_1 = np.mean(samples_1), np.var(samples_1, ddof=1)\n    results.append(np.abs(m_hat_1 - m_star_1)  0.03)\n    results.append(np.abs(v_hat_1 - v_star_1)  0.30)\n    \n    # Test 2: Gamma, shape less than one (augmentation)\n    k, theta = 0.7, 1.5\n    samples_2 = gamma_sampler(k, theta, n, rng)\n    m_star_2, v_star_2 = k * theta, k * theta**2\n    m_hat_2, v_hat_2 = np.mean(samples_2), np.var(samples_2, ddof=1)\n    results.append(np.abs(m_hat_2 - m_star_2)  0.03)\n    results.append(np.abs(v_hat_2 - v_star_2)  0.10)\n\n    # Test 3: Beta via ratio of independent gammas\n    a, b = 2.1, 3.3\n    x_samples_3 = gamma_sampler(a, 1.0, n, rng)\n    y_samples_3 = gamma_sampler(b, 1.0, n, rng)\n    u_samples_3 = x_samples_3 / (x_samples_3 + y_samples_3)\n    u_mean_star_3 = a / (a + b)\n    u_mean_hat_3 = np.mean(u_samples_3)\n    results.append(np.abs(u_mean_hat_3 - u_mean_star_3)  0.01)\n\n    # Test 4: Chi-squared as a gamma\n    nu = 10\n    k, theta = nu / 2.0, 2.0\n    samples_4 = gamma_sampler(k, theta, n, rng)\n    m_star_4, v_star_4 = nu, 2.0 * nu\n    m_hat_4, v_hat_4 = np.mean(samples_4), np.var(samples_4, ddof=1)\n    results.append(np.abs(m_hat_4 - m_star_4)  0.03)\n    results.append(np.abs(v_hat_4 - v_star_4)  0.20)\n\n    # Test 5: Boundary case k=1 (exponential)\n    k, theta = 1.0, 0.8\n    samples_5 = gamma_sampler(k, theta, n, rng)\n    m_star_5, v_star_5 = k * theta, k * theta**2\n    m_hat_5, v_hat_5 = np.mean(samples_5), np.var(samples_5, ddof=1)\n    results.append(np.abs(m_hat_5 - m_star_5)  0.02)\n    results.append(np.abs(v_hat_5 - v_star_5)  0.05)\n\n    print(f\"[{','.join(map(str, results))}]\".lower())\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond correct implementation, designing an efficient sampler requires intelligent algorithmic choices. This practice moves from implementation to the theoretical design of an acceptance-rejection sampler, one of the most fundamental tools in simulation. You will design and optimize a proposal distribution from the gamma family itself, using exponential tilting to create an efficient sampler for cases where the shape parameter $k$ is less than one, a traditionally challenging regime . This exercise provides a concrete application of theoretical concepts such as convex duality and moment-matching to the tangible goal of minimizing variance and maximizing sampler efficiency.",
            "id": "3309202",
            "problem": "Consider a target gamma distribution with shape parameter $k \\in (0,1)$ and rate parameter $r0$, whose probability density function is\n$$\nf(x) \\;=\\; \\frac{r^{k}}{\\Gamma(k)} \\, x^{k-1} \\exp(-r x), \\quad x0.\n$$\nYou are tasked with designing an acceptance-rejection scheme to generate variates from $f$ using an exponentially tilted proposal that preserves the power-law behavior near the origin. Specifically, consider the proposal family\n$$\nq_{t}(x) \\;=\\; \\frac{t^{k}}{\\Gamma(k)} \\, x^{k-1} \\exp(-t x), \\quad x0,\n$$\nwith tilt parameter $t0$, so that $q_{t}$ is a gamma distribution with the same shape parameter $k$ and rate parameter $t$. For acceptance-rejection, you must choose a finite constant $M(t)$ such that $f(x) \\leq M(t)\\, q_{t}(x)$ for all $x0$, and you accept a draw $X \\sim q_{t}$ with probability $f(X)/(M(t) q_{t}(X))$. Let $I_{t}$ denote the acceptance indicator, which is a Bernoulli random variable with success probability $p(t) = 1/M(t)$.\n\nStarting from the foundational definitions of acceptance-rejection and the gamma family, and without invoking any shortcut formulas, perform the following:\n\n- Derive an explicit expression for $M(t)$ as a function of $t$, and characterize the values of $t$ for which $M(t)$ is finite.\n\n- Express $p(t)$ and $\\operatorname{Var}(I_{t})$ in terms of $t$, $r$, and $k$.\n\n- Using convex duality by analyzing the supremum of the log-likelihood ratio and by interpreting the solution via moment-matching, determine the optimal tilt $t^{\\ast}$ that minimizes $\\operatorname{Var}(I_{t})$ over all admissible $t$.\n\nYour final answer must be the optimal tilt $t^{\\ast}$ expressed in terms of $r$. No numerical approximation is required; provide the exact symbolic expression. If you find multiple candidates, select the one that achieves the global minimum of $\\operatorname{Var}(I_{t})$ subject to the admissibility constraint you derived. The final answer should be a single closed-form expression.",
            "solution": "The acceptance-rejection algorithm requires a constant $M(t)$ such that $f(x) \\leq M(t) q_t(x)$ for all $x > 0$. The most efficient choice for $M(t)$ is the smallest such constant, given by the supremum of the ratio of the densities, $M(t) = \\sup_{x>0} \\frac{f(x)}{q_t(x)}$.\nThe ratio of the densities is:\n$$\n\\frac{f(x)}{q_t(x)} = \\frac{\\frac{r^{k}}{\\Gamma(k)} x^{k-1} \\exp(-r x)}{\\frac{t^{k}}{\\Gamma(k)} x^{k-1} \\exp(-t x)} = \\left(\\frac{r}{t}\\right)^{k} \\exp((t-r)x)\n$$\nTo find the supremum of this expression over $x>0$, we analyze the sign of the coefficient $t-r$:\n1.  If $t > r$, then $t-r > 0$. The function $\\exp((t-r)x)$ is unbounded as $x \\to \\infty$. Thus, $M(t) = \\infty$, and this choice of $t$ is not admissible.\n2.  If $0  t \\le r$, then $t-r \\le 0$. The function $\\exp((t-r)x)$ is non-increasing. Its supremum over $x>0$ is achieved as $x \\to 0^{+}$, where it equals 1.\nTherefore, a finite constant $M(t)$ exists only for $t \\in (0, r]$, for which the optimal constant is:\n$$\nM(t) = \\left(\\frac{r}{t}\\right)^{k}\n$$\nThe acceptance probability for a proposed sample is $p(t) = 1/M(t) = (t/r)^k$. The acceptance indicator $I_t$ is a Bernoulli random variable with variance $\\operatorname{Var}(I_t) = p(t)(1-p(t))$. To optimize the sampler, we aim to maximize the acceptance probability $p(t)$, which is equivalent to minimizing the variance $\\operatorname{Var}(I_t)$ towards 0.\nWe need to maximize $p(t) = (t/r)^k$ over the admissible range $t \\in (0, r]$. Since $k>0$, this function is increasing in $t$. The maximum is achieved at the boundary $t=r$.\nAt $t=r$, we have $p(r) = (r/r)^k = 1$, and $\\operatorname{Var}(I_r) = 1(1-1) = 0$. This represents the most efficient sampler possible, where every proposed sample is accepted. The optimal tilt is therefore $t^\\ast = r$.\n\nThis result can be interpreted through convex duality and moment-matching. The problem is equivalent to minimizing $M(t)$ over $t \\in (0,r]$. The objective is $\\min_{t \\in (0,r]} \\sup_{x>0} (f(x)/q_t(x))$. By strong duality, we can swap the operators. The globally optimal solution that minimizes the uniform bound $M(t)$ corresponds to matching the global properties of the distributions. By setting $t=r$, the proposal distribution $q_r(x)$ becomes identical to the target distribution $f(x)$. This can be seen as matching all moments, but most simply, it is matching the rate parameter, which implies matching the mean $\\mathbb{E}[X] = k/r = k/t$.",
            "answer": "$$\\boxed{r}$$"
        },
        {
            "introduction": "The ability to generate gamma variates is not an end in itself, but a means to solve more complex problems. This practice demonstrates their power as a building block in advanced Monte Carlo methods by tasking you with designing an importance sampler to estimate a rare-event probability—a common challenge where naive simulation is computationally infeasible. You will use an exponentially tilted gamma distribution as a proposal to concentrate sampling effort in the tail region of interest, a powerful variance reduction technique . This exercise illuminates the connection between generating random variates and their application, showing you how to analytically optimize the sampler and quantify its efficiency gain using the Effective Sample Size (ESS).",
            "id": "3309211",
            "problem": "Consider a random variable $X$ distributed as a Gamma distribution with shape $k0$ and scale $\\theta0$, denoted $X\\sim\\Gamma(k,\\theta)$. Let $\\beta\\equiv 1/\\theta$ denote the rate. The objective is to estimate the right-tail probability $p(t)\\equiv \\mathbb{P}(Xt)$ for a fixed threshold $t0$ by importance sampling. You are asked to design and analyze an importance sampler based on exponential tilting of the target density. Specifically, consider the proposal family $\\{g_{\\lambda}:\\lambda\\in[0,\\beta)\\}$ defined by\n$$\ng_{\\lambda}(x)\\propto \\exp(\\lambda x) f(x),\n$$\nwhere $f(x)$ is the probability density function of $X$, and the proportionality constant normalizes $g_{\\lambda}$ to a density. Using this proposal, the standard importance sampling estimator of $p(t)$ is\n$$\n\\widehat{p}_{n}(t;\\lambda)\\equiv \\frac{1}{n}\\sum_{i=1}^{n} w_{\\lambda}(X_{i})\\,\\mathbf{1}\\{X_{i}t\\},\\quad X_{i}\\stackrel{\\text{i.i.d.}}{\\sim} g_{\\lambda},\n$$\nwith importance weight $w_{\\lambda}(x)\\equiv f(x)/g_{\\lambda}(x)$.\n\nTasks:\n1. Starting from first principles in importance sampling, derive the exact form of $g_{\\lambda}(x)$ (including its normalizing constant) and the exact weight function $w_{\\lambda}(x)$. Then, derive an analytic expression for the per-sample second moment $\\mathbb{E}_{g_{\\lambda}}[w_{\\lambda}(X)^{2}\\,\\mathbf{1}\\{Xt\\}]$ in terms of $k$, $\\beta$, $t$, and $\\lambda$ only, and use it to obtain the objective whose minimizer in $\\lambda\\in[0,\\beta)$ gives the variance-minimizing choice for the exponential tilting.\n2. Compute the first-order optimality condition for the variance-minimizing $\\lambda$, express it in terms of standard special functions, and use asymptotic analysis in the rare-event regime $t\\to\\infty$ to obtain a leading-order asymptotic expansion for the optimal tilting parameter $\\lambda^{\\star}$.\n3. Define the Effective Sample Size (ESS) for the weighted estimator as $\\mathrm{ESS} \\equiv n\\cdot \\left(\\mathbb{E}_{g_{\\lambda}}[w_{\\lambda}(X)\\,\\mathbf{1}\\{Xt\\}]\\right)^{2}\\big/\\mathbb{E}_{g_{\\lambda}}[w_{\\lambda}(X)^{2}\\,\\mathbf{1}\\{Xt\\}]$, and derive a closed-form expression for the ESS fraction $\\mathrm{ESS}/n$ in terms of $k$, $\\beta$, $t$, and $\\lambda$. Then, evaluate this expression at the asymptotically optimal $\\lambda^{\\star}$ obtained in Task 2 and simplify to its leading asymptotic order for large $t$.\n\nYour final answer must consist of two items:\n- The leading-order asymptotic expression for $\\lambda^{\\star}$ as $t\\to\\infty$.\n- The leading-order asymptotic expression for $\\mathrm{ESS}/n$ at $\\lambda^{\\star}$ as $t\\to\\infty$.\n\nExpress the final answer as a single closed-form analytic expression using a row matrix with two entries. No numerical rounding is required.",
            "solution": "### Task 1: Derivation of the Proposal Density, Weights, and the Second Moment Objective\n\nThe target variable $X \\sim \\Gamma(k, \\theta)$ has a rate $\\beta=1/\\theta$ and PDF $f(x) = \\frac{\\beta^k}{\\Gamma(k)} x^{k-1} e^{-\\beta x}$. The proposal density $g_{\\lambda}(x)$ is defined by exponential tilting: $g_{\\lambda}(x) \\propto e^{\\lambda x} f(x)$. The normalizing constant is the reciprocal of the moment generating function (MGF) of $X$: $M_X(\\lambda) = \\mathbb{E}[\\exp(\\lambda X)] = \\left(\\frac{\\beta}{\\beta-\\lambda}\\right)^k$, valid for $\\lambda  \\beta$. The normalized proposal density is:\n$$\ng_{\\lambda}(x) = \\frac{e^{\\lambda x} f(x)}{M_X(\\lambda)} = \\frac{(\\beta-\\lambda)^k}{\\Gamma(k)} x^{k-1} e^{-(\\beta-\\lambda)x}\n$$\nThis is the PDF of a Gamma distribution with shape $k$ and rate $\\beta' = \\beta-\\lambda$. The importance weight is $w_{\\lambda}(x) = f(x)/g_{\\lambda}(x) = M_X(\\lambda) e^{-\\lambda x} = \\left(\\frac{\\beta}{\\beta-\\lambda}\\right)^k e^{-\\lambda x}$. To minimize the variance of the estimator, which has a mean $p(t)$ independent of $\\lambda$, we minimize the second moment:\n$$\nV(\\lambda) \\equiv \\mathbb{E}_{g_{\\lambda}}[w_{\\lambda}(X)^2 \\mathbf{1}\\{X>t\\}] = \\int_t^\\infty w_{\\lambda}(x)^2 g_{\\lambda}(x) dx\n$$\nSubstituting the expressions, we get:\n$$\nV(\\lambda) = M_X(\\lambda) \\int_t^\\infty e^{-\\lambda x} f(x) dx = \\left(\\frac{\\beta}{\\beta-\\lambda}\\right)^k \\frac{\\beta^k}{\\Gamma(k)} \\int_t^\\infty x^{k-1} e^{-(\\beta+\\lambda)x} dx\n$$\nThe integral is $\\frac{\\Gamma(k, (\\beta+\\lambda)t)}{(\\beta+\\lambda)^k}$, where $\\Gamma(s,z)$ is the upper incomplete gamma function. Thus, the objective to minimize is:\n$$\nV(\\lambda) = \\frac{\\beta^{2k}}{(\\beta^2-\\lambda^2)^k \\Gamma(k)} \\Gamma(k, (\\beta+\\lambda)t)\n$$\n\n### Task 2: Optimality Condition and Asymptotic Analysis\n\nTo find the optimal $\\lambda$, we set the derivative of $\\log V(\\lambda)$ to zero. This gives the optimality condition:\n$$\n\\frac{2k\\lambda}{\\beta^2-\\lambda^2} - \\frac{t^k (\\beta+\\lambda)^{k-1} e^{-(\\beta+\\lambda)t}}{\\Gamma(k, (\\beta+\\lambda)t)} = 0\n$$\nFor the rare-event regime $t\\to\\infty$, we use the asymptotic expansion $\\Gamma(s,z) \\sim z^{s-1}e^{-z}$ for large $z$. The second term in the condition becomes approximately $t$. The asymptotic optimality condition is $\\frac{2k\\lambda}{\\beta^2-\\lambda^2} \\approx t$, which is the quadratic equation $t\\lambda^2 + 2k\\lambda - t\\beta^2 = 0$. The valid solution in $[0, \\beta)$ is $\\lambda^{\\star} \\approx \\frac{-k + \\sqrt{k^2+t^2\\beta^2}}{t}$. For large $t$, we expand the square root:\n$$\n\\lambda^{\\star} \\approx \\frac{-k + t\\beta\\sqrt{1+k^2/(t^2\\beta^2)}}{t} \\approx \\frac{-k + t\\beta(1 + \\frac{k^2}{2t^2\\beta^2})}{t} = \\beta - \\frac{k}{t} + O(t^{-2})\n$$\nThe leading-order asymptotic expression is $\\lambda^{\\star} \\approx \\beta - \\frac{k}{t}$.\n\n### Task 3: Effective Sample Size (ESS) Analysis\n\nThe ESS fraction is $\\frac{\\mathrm{ESS}}{n} = \\frac{p(t)^2}{V(\\lambda)}$. With $p(t) = \\frac{\\Gamma(k, \\beta t)}{\\Gamma(k)}$, this is:\n$$\n\\frac{\\mathrm{ESS}}{n} = \\frac{\\Gamma(k, \\beta t)^2 (\\beta^2-\\lambda^2)^k}{\\Gamma(k) \\beta^{2k} \\Gamma(k, (\\beta+\\lambda)t)}\n$$\nWe evaluate this at the asymptotic optimum $\\lambda^{\\star} \\approx \\beta - k/t$. For large $t$, we have $\\beta^2 - (\\lambda^{\\star})^2 \\approx 2k\\beta/t$ and $(\\beta+\\lambda^\\star)t \\approx 2\\beta t - k$. Using the asymptotic form $\\Gamma(s,z) \\sim z^{s-1}e^{-z}$:\n$$\n\\frac{\\mathrm{ESS}}{n}(\\lambda^\\star) \\approx \\frac{\\left[(\\beta t)^{k-1}e^{-\\beta t}\\right]^2 \\left(\\frac{2k\\beta}{t}\\right)^k}{\\Gamma(k) \\beta^{2k} \\left[(2\\beta t - k)^{k-1}e^{-(2\\beta t - k)}\\right]} \\approx \\frac{[(\\beta t)^{k-1}e^{-\\beta t}]^2 (\\frac{2k\\beta}{t})^k}{\\Gamma(k) \\beta^{2k} [(2\\beta t)^{k-1}e^{-2\\beta t}e^k]}\n$$\nThis simplifies to:\n$$\n\\frac{\\mathrm{ESS}}{n} \\approx \\frac{(\\beta t)^{2k-2} e^{-2\\beta t} \\cdot (2k\\beta)^k t^{-k}}{\\Gamma(k) \\beta^{2k} \\cdot (2\\beta t)^{k-1} e^{-2\\beta t} e^k} = \\frac{2 k^k e^{-k}}{\\Gamma(k) \\beta t}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\beta - \\frac{k}{t}  \\frac{2 k^k \\exp(-k)}{\\Gamma(k) \\beta t} \\end{pmatrix}}\n$$"
        }
    ]
}