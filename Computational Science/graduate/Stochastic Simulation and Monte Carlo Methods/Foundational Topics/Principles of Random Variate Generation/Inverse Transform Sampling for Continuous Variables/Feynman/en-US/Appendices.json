{
    "hands_on_practices": [
        {
            "introduction": "This first exercise grounds the theory of inverse transform sampling in a concrete and important application. We will derive the quantile function, $Q(u)$, for the Pareto distribution, a model crucial for describing phenomena with heavy tails, such as wealth distribution or internet traffic . This practice not only reinforces the mechanics of inverting a cumulative distribution function but also provides direct insight into how the mathematical form of a distribution governs the scaling of its extreme values in a simulation.",
            "id": "3314460",
            "problem": "Consider a Pareto distribution with scale parameter $x_m>0$ and shape parameter $\\alpha0$. Let $X$ be a continuous random variable with probability density function $f_X(x)=\\alpha x_m^{\\alpha} x^{-(\\alpha+1)}$ for $x\\ge x_m$ and $f_X(x)=0$ for $xx_m$. In inverse transform sampling for continuous variables, the quantile function is defined by the generalized inverse of the cumulative distribution function, $Q(u)=F_X^{-1}(u)=\\inf\\{x\\in\\mathbb{R}:F_X(x)\\ge u\\}$ for $u\\in(0,1)$.\n\n(a) Starting from the given density and the definition of the cumulative distribution function $F_X(x)=\\mathbb{P}(X\\le x)$, derive the explicit closed-form expression for the quantile function $Q(u)$ for $u\\in(0,1)$, carefully justifying each transformation.\n\n(b) In a Monte Carlo (MC) simulation using inverse transform sampling, one draws $U\\sim\\mathrm{Uniform}(0,1)$ and sets $X^{\\star}=Q(U)$. For a given sample size $n\\in\\mathbb{N}$, define the high-quantile threshold $\\tau_n$ implicitly by $\\mathbb{P}(X^{\\star}\\tau_n)=1/n$. Using part (a) and the definition of the quantile function, determine a closed-form analytic expression for $\\tau_n$ as a function of $x_m$, $\\alpha$, and $n$, and explain how this expression captures the scaling of extreme values with $n$ in inverse-transform-based sampling of heavy-tailed distributions.\n\nYour final reported answer must be the single closed-form expression for $\\tau_n$. No rounding is required.",
            "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Distribution:** Pareto distribution.\n- **Scale parameter:** $x_m > 0$.\n- **Shape parameter:** $\\alpha > 0$.\n- **Random variable:** $X$.\n- **Probability Density Function (PDF):** $f_X(x) = \\alpha x_m^{\\alpha} x^{-(\\alpha+1)}$ for $x \\ge x_m$ and $f_X(x) = 0$ for $x  x_m$.\n- **Cumulative Distribution Function (CDF):** $F_X(x) = \\mathbb{P}(X \\le x)$.\n- **Quantile Function:** $Q(u) = F_X^{-1}(u) = \\inf\\{x \\in \\mathbb{R}: F_X(x) \\ge u\\}$ for $u \\in (0, 1)$.\n- **Part (a):** Derive the explicit closed-form expression for $Q(u)$.\n- **Part (b):** In a simulation, a random variate is generated as $X^{\\star} = Q(U)$, where $U \\sim \\mathrm{Uniform}(0, 1)$. For a sample size $n \\in \\mathbb{N}$, a high-quantile threshold $\\tau_n$ is defined by the relation $\\mathbb{P}(X^{\\star}  \\tau_n) = 1/n$. The task is to find a closed-form expression for $\\tau_n$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard exercise in probability theory and stochastic simulation. It concerns the well-defined Pareto distribution and the established method of inverse transform sampling. The problem is well-posed, as all necessary parameters ($x_m, \\alpha$), definitions (PDF, CDF, quantile function), and conditions are provided, allowing for a unique solution. The definitions are standard and the language is objective and precise. The PDF provided is correctly normalized for the given parameter constraints, as $\\int_{x_m}^{\\infty} \\alpha x_m^{\\alpha} x^{-(\\alpha+1)} \\, dx = \\alpha x_m^{\\alpha} [-\\frac{1}{\\alpha}x^{-\\alpha}]_{x_m}^{\\infty} = -x_m^{\\alpha}(0 - x_m^{-\\alpha}) = 1$. The problem setup is self-contained and logically consistent.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n**Part (a): Derivation of the Quantile Function $Q(u)$**\n\nThe cumulative distribution function (CDF), $F_X(x)$, is defined as the integral of the probability density function (PDF), $f_X(t)$, from $-\\infty$ to $x$. The support of the Pareto distribution is $[x_m, \\infty)$, so for any $x  x_m$, the PDF is $f_X(x) = 0$, which implies $F_X(x) = 0$.\n\nFor $x \\ge x_m$, the CDF is calculated as follows:\n$$F_X(x) = \\int_{-\\infty}^{x} f_X(t) \\, dt = \\int_{x_m}^{x} \\alpha x_m^{\\alpha} t^{-(\\alpha+1)} \\, dt$$\nWe can factor out the constants and perform the integration:\n$$F_X(x) = \\alpha x_m^{\\alpha} \\int_{x_m}^{x} t^{-(\\alpha+1)} \\, dt$$\nThe integral of $t^k$ is $\\frac{t^{k+1}}{k+1}$. Here, $k = -(\\alpha+1)$, so $k+1 = -\\alpha$.\n$$F_X(x) = \\alpha x_m^{\\alpha} \\left[ \\frac{t^{-\\alpha}}{-\\alpha} \\right]_{x_m}^{x}$$\n$$F_X(x) = -x_m^{\\alpha} \\left[ t^{-\\alpha} \\right]_{x_m}^{x}$$\n$$F_X(x) = -x_m^{\\alpha} (x^{-\\alpha} - x_m^{-\\alpha})$$\n$$F_X(x) = -x_m^{\\alpha} x^{-\\alpha} + x_m^{\\alpha} x_m^{-\\alpha}$$\n$$F_X(x) = 1 - \\left(\\frac{x_m}{x}\\right)^{\\alpha}$$\nSo, the complete CDF is:\n$$F_X(x) = \\begin{cases} 0  \\text{for } x  x_m \\\\ 1 - \\left(\\frac{x_m}{x}\\right)^{\\alpha}  \\text{for } x \\ge x_m \\end{cases}$$\nThe quantile function $Q(u)$ is the inverse of the CDF, $F_X^{-1}(u)$. Since $F_X(x)$ is continuous and strictly increasing on its support, we can find the inverse by setting $u = F_X(x)$ for $u \\in (0, 1)$ and solving for $x$. This corresponds to the case where $x \\ge x_m$.\n$$u = 1 - \\left(\\frac{x_m}{x}\\right)^{\\alpha}$$\nRearranging the terms to solve for $x$:\n$$\\left(\\frac{x_m}{x}\\right)^{\\alpha} = 1 - u$$\nTaking the $1/\\alpha$ power of both sides:\n$$\\frac{x_m}{x} = (1 - u)^{1/\\alpha}$$\nFinally, isolating $x$:\n$$x = \\frac{x_m}{(1-u)^{1/\\alpha}} = x_m (1 - u)^{-1/\\alpha}$$\nThus, the quantile function for the Pareto distribution is:\n$$Q(u) = x_m (1 - u)^{-1/\\alpha}, \\quad u \\in (0, 1)$$\n\n**Part (b): Derivation of the High-Quantile Threshold $\\tau_n$**\n\nWe are given the condition defining the high-quantile threshold $\\tau_n$:\n$$\\mathbb{P}(X^{\\star}  \\tau_n) = \\frac{1}{n}$$\nIn inverse transform sampling, the random variate $X^{\\star}$ is generated by transforming a standard uniform random variable $U \\sim \\mathrm{Uniform}(0, 1)$ using the quantile function derived in part (a), i.e., $X^{\\star} = Q(U)$. Substituting this into the probability statement:\n$$\\mathbb{P}(Q(U)  \\tau_n) = \\frac{1}{n}$$\nThe quantile function $Q(u)$ is a strictly increasing function for $u \\in (0,1)$. Therefore, we can apply its inverse function, the CDF $F_X$, to both sides of the inequality inside the probability without changing the direction of the inequality:\n$$Q(U)  \\tau_n \\iff U  F_X(\\tau_n)$$\nThe probability statement becomes:\n$$\\mathbb{P}(U  F_X(\\tau_n)) = \\frac{1}{n}$$\nFor a standard uniform random variable $U$, the probability $\\mathbb{P}(U  p)$ is equal to $1 - p$ for any $p \\in [0, 1]$. Let $p = F_X(\\tau_n)$. Then:\n$$1 - F_X(\\tau_n) = \\frac{1}{n}$$\nThis implies:\n$$F_X(\\tau_n) = 1 - \\frac{1}{n}$$\nThis equation states that $\\tau_n$ is the quantile of the distribution $X$ corresponding to the probability level $1 - 1/n$. By the definition of the quantile function, this means $\\tau_n = Q(1 - 1/n)$.\n\nWe can now use the explicit formula for $Q(u)$ derived in part (a) by substituting $u = 1 - 1/n$:\n$$\\tau_n = Q\\left(1 - \\frac{1}{n}\\right) = x_m \\left(1 - \\left(1 - \\frac{1}{n}\\right)\\right)^{-1/\\alpha}$$\nSimplifying the expression inside the parentheses:\n$$\\tau_n = x_m \\left(\\frac{1}{n}\\right)^{-1/\\alpha}$$\nUsing the property of exponents $(a^b)^c = a^{bc}$:\n$$\\tau_n = x_m (n^{-1})^{-1/\\alpha} = x_m n^{(-1)(-1/\\alpha)}$$\n$$\\tau_n = x_m n^{1/\\alpha}$$\nThis is the closed-form analytic expression for the high-quantile threshold $\\tau_n$.\n\nThis expression reveals how extreme values drawn from a Pareto distribution scale with the sample size $n$. The threshold $\\tau_n$, which is the value expected to be exceeded on average once in a sample of size $n$, grows as a power law of $n$, with the exponent being the reciprocal of the tail index, $1/\\alpha$. This polynomial growth is a hallmark of heavy-tailed distributions. A smaller $\\alpha$ (heavier tail) leads to a larger exponent $1/\\alpha$, indicating that extreme values grow more rapidly with sample size compared to distributions with larger $\\alpha$ (lighter tails). This contrasts sharply with light-tailed distributions (e.g., exponential, normal), where quantiles typically grow much more slowly, often logarithmically with $n$.",
            "answer": "$$\\boxed{x_m n^{1/\\alpha}}$$"
        },
        {
            "introduction": "Real-world modeling often requires restricting a distribution to a specific interval, a process known as truncation. This practice moves beyond single-distribution examples to the design of a powerful, general-purpose tool: an inverse transform sampler for any truncated continuous distribution . You will learn how to correctly adapt the quantile function for this common scenario and tackle cases where a closed-form inverse is unavailable, a frequent and practical challenge in computational statistics that requires numerical root-finding.",
            "id": "3314454",
            "problem": "You are asked to design and implement an inverse transform sampler for truncated continuous distributions. Begin from the fundamental definitions: if a continuous random variable $X$ has cumulative distribution function (CDF) $F(x)$, then for a Uniform$(0,1)$ variable $U$, the random variable $X = F^{-1}(U)$ has CDF $F(x)$. For a truncation interval $[a,b]$ with $-\\infty  a  b  \\infty$ and $F$ strictly increasing on $[a,b]$, the truncated CDF is defined by\n$$\nF_T(x) = \\frac{F(x) - F(a)}{F(b) - F(a)}, \\quad x \\in [a,b].\n$$\nThe corresponding truncated quantile function is obtained by solving $F_T(x) = u$ for $x$ and is given by\n$$\nQ_T(u) = F^{-1}\\Big(F(a) + u\\big(F(b)-F(a)\\big)\\Big), \\quad u \\in [0,1].\n$$\nYour task is to:\n- Derive the truncated CDF and truncated quantile function from first principles based on the definitions above, clearly articulating the reasoning.\n- Implement a general-purpose inverse transform sampler for truncated continuous distributions that, given a base $F$, its inverse $F^{-1}$ when available, and truncation bounds $a$ and $b$, computes $Q_T(u)$ for requested $u$ values. When a closed-form or library implementation of $F^{-1}$ is unavailable, perform numerical inversion by solving $F(x) = p$ for $x$ using a robust root-finding method on the interval $[a,b]$, where $p = F(a) + u\\big(F(b)-F(a)\\big)$.\n- Ensure that the implementation handles boundary cases $u=0$ and $u=1$ exactly as $Q_T(0)=a$ and $Q_T(1)=b$.\n\nUse the following test suite. For each case, compute the list of truncated quantiles $[Q_T(u_1), Q_T(u_2), \\dots]$ for the specified values of $u$.\n\n- Test Case 1 (happy path with closed-form inverse): Base distribution is the exponential with rate $\\lambda = 0.7$, with base CDF $F(x) = 1 - e^{-\\lambda x}$ on $[0,\\infty)$. Truncate to $[a,b]=[0.2,1.5]$. Evaluate at $u \\in \\{0,\\,0.3,\\,0.6,\\,1\\}$.\n- Test Case 2 (library inverse, asymmetric truncation): Base distribution is the standard normal $\\mathcal{N}(0,1)$ with base CDF $F(x)$, truncated to $[a,b]=[-0.5,\\,1.0]$. Evaluate at $u \\in \\{0,\\,0.5,\\,1\\}$.\n- Test Case 3 (analytic polynomial CDF): Base probability density function (PDF) is $f(x)=3x^2$ on $[0,1]$, so the base CDF is $F(x)=x^3$. Truncate to $[a,b]=[0.2,\\,0.8]$. Evaluate at $u \\in \\{0,\\,0.25,\\,0.5,\\,0.75,\\,1\\}$.\n- Test Case 4 (numerical inversion only): Base PDF is proportional to $e^{-x^3}$ on $[0,1]$, with base CDF\n$$\nF(x) = \\frac{\\int_0^x e^{-t^3}\\,dt}{\\int_0^1 e^{-t^3}\\,dt}.\n$$\nEquivalently, using the lower incomplete gamma function $\\gamma(s,z)$, $F(x) = \\frac{\\gamma\\!\\left(\\frac{1}{3},\\,x^3\\right)}{\\gamma\\!\\left(\\frac{1}{3},\\,1\\right)}$. Truncate to $[a,b]=[0.1,\\,0.9]$. Evaluate at $u \\in \\{0,\\,0.5,\\,1\\}$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes one inner list of floats. The exact format is:\n- For the four test cases in order, print a single line:\n$$\n\\big[ [Q_{T,1}(u_1),\\dots], [Q_{T,2}(u_1),\\dots], [Q_{T,3}(u_1),\\dots], [Q_{T,4}(u_1),\\dots] \\big]\n$$\nwith no extra whitespace or text. For example, a valid structural format is\n$$\n[[x_{11},x_{12}],[x_{21},x_{22}],[x_{31},x_{32},x_{33}],[x_{41},x_{42}]]\n$$\nwhere each $x_{ij}$ is a float.",
            "solution": "The starting point is the definition of the Cumulative Distribution Function (CDF) $F(x)$ for a continuous random variable $X$, which is $F(x) = \\mathbb{P}(X \\le x)$. The inverse transform sampling principle states that if $U \\sim \\text{Uniform}(0,1)$, then $X = F^{-1}(U)$ has CDF $F(x)$. This follows from monotonicity: for any $x$, $\\mathbb{P}(X \\le x) = \\mathbb{P}(F^{-1}(U) \\le x) = \\mathbb{P}(U \\le F(x)) = F(x)$.\n\nWhen truncating a base distribution with CDF $F$ to an interval $[a,b]$, we consider the conditional distribution $\\mathcal{L}(X \\mid a \\le X \\le b)$. For any $x \\in [a,b]$, by the definition of conditional probability,\n$$\nF_T(x) = \\mathbb{P}(X \\le x \\mid a \\le X \\le b) = \\frac{\\mathbb{P}(a \\le X \\le x)}{\\mathbb{P}(a \\le X \\le b)} = \\frac{F(x) - F(a)}{F(b) - F(a)}.\n$$\nThis proves the truncated CDF formula and ensures $F_T(a)=0$ and $F_T(b)=1$. To obtain the truncated quantile function, set $F_T(x) = u$ and solve for $x$:\n$$\nu = \\frac{F(x) - F(a)}{F(b) - F(a)} \\quad \\Longrightarrow \\quad F(x) = F(a) + u\\big(F(b) - F(a)\\big).\n$$\nSince $F$ is strictly increasing on $[a,b]$, we can invert it to get\n$$\nQ_T(u) = F^{-1}\\Big(F(a) + u\\big(F(b)-F(a)\\big)\\Big), \\quad u \\in [0,1].\n$$\nThis representation recovers the boundary cases directly: for $u=0$, $Q_T(0) = F^{-1}(F(a)) = a$, and for $u=1$, $Q_T(1) = F^{-1}(F(b)) = b$.\n\nAlgorithmic design for implementation:\n- Inputs: base CDF $F$, optional base quantile $F^{-1}$ if available, truncation bounds $ab$, and a list of $u$ values in $[0,1]$.\n- Compute $F(a)$ and $F(b)$ once. For each $u$, compute $p = F(a) + u\\big(F(b)-F(a)\\big)$.\n- If a closed-form or library implementation of $F^{-1}$ is available, set $x = F^{-1}(p)$.\n- If $F^{-1}$ is not available, numerically invert $F$ on $[a,b]$ by solving $F(x)-p=0$ using a robust root-finding method. Brent’s method is a standard choice because it combines bisection, secant, and inverse quadratic interpolation, guaranteeing convergence for continuous monotone functions when the bracket contains a root. We use the bracket $[a,b]$, which contains the unique solution because $F$ is strictly increasing on $[a,b]$ and $p \\in [F(a),F(b)]$ by construction. Handle $u=0$ and $u=1$ explicitly to avoid unnecessary numerical work and to ensure exact endpoints.\n\nTest suite construction:\n- Test Case 1 uses the exponential base CDF $F(x)=1-e^{-\\lambda x}$ with $\\lambda=0.7$; the inverse is $F^{-1}(p) = -\\frac{1}{\\lambda}\\ln(1-p)$. Truncate to $[a,b]=[0.2,1.5]$ and evaluate at $u \\in \\{0,0.3,0.6,1\\}$. This checks general behavior and both endpoints.\n- Test Case 2 uses the standard normal distribution with CDF $F(x)$ and inverse quantile given by the percent point function provided by a numerical library. Truncate to $[a,b]=[-0.5,1.0]$ and evaluate at $u \\in \\{0,0.5,1\\}$. This exercises asymmetric truncation and a library inverse.\n- Test Case 3 uses a polynomial base with $f(x)=3x^2$ on $[0,1]$, $F(x)=x^3$, so $F^{-1}(p)=p^{1/3}$. Truncate to $[a,b]=[0.2,0.8]$ and evaluate at $u \\in \\{0,0.25,0.5,0.75,1\\}$. This provides a clean analytic test that also includes multiple interior points.\n- Test Case 4 uses a base PDF proportional to $e^{-x^3}$ on $[0,1]$. The base CDF can be written via the lower incomplete gamma function as $F(x)=\\frac{\\gamma\\!\\left(\\frac{1}{3},x^3\\right)}{\\gamma\\!\\left(\\frac{1}{3},1\\right)}$. Truncate to $[a,b]=[0.1,0.9]$ and evaluate at $u \\in \\{0,0.5,1\\}$. Here we use numerical inversion (Brent’s method) since a simple closed-form inverse is not available.\n\nOutput specification:\n- Aggregate the four lists of computed quantiles into a single outer list and print them as a single line with the exact structure: one outer list of four inner lists, with no extraneous whitespace or labels. Each entry must be a float.\n\nThis approach is scientifically grounded in the definitions of the Cumulative Distribution Function (CDF), Probability Density Function (PDF), conditional distributions, and the inverse transform method. The numerical inversion step uses established root-finding with guaranteed convergence under monotonicity, ensuring correctness and robustness for continuous distributions on bounded truncation intervals.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import brentq\nfrom scipy.special import gammainc\n\ndef truncated_quantiles(F, Finv_or_none, a, b, u_values):\n    \"\"\"\n    Compute truncated quantiles Q_T(u) = F^{-1}(F(a) + u*(F(b)-F(a))) for u in u_values.\n    If Finv_or_none is None, perform numerical inversion on [a,b].\n    \"\"\"\n    Fa = F(a)\n    Fb = F(b)\n    results = []\n    for u in u_values:\n        # Handle exact endpoints\n        if u == 0.0:\n            results.append(float(a))\n            continue\n        if u == 1.0:\n            results.append(float(b))\n            continue\n        p = Fa + u * (Fb - Fa)\n        if Finv_or_none is not None:\n            x = Finv_or_none(p)\n        else:\n            # Numerically invert F(x) = p on [a,b] via Brent's method\n            # F is strictly increasing on [a,b], so a unique root exists.\n            func = lambda x: F(x) - p\n            # If func(a) == 0 or func(b) == 0, brentq will handle it, but we guarded above.\n            x = brentq(func, a, b, maxiter=100, rtol=1e-12)\n        results.append(float(x))\n    return results\n\ndef format_nested_list(nested):\n    \"\"\"\n    Format nested list of floats without spaces: [[a,b],[c,d,e],...]\n    \"\"\"\n    def fmt_float(x):\n        # Use repr to avoid trailing spaces and ensure a precise representation.\n        return repr(float(x))\n    inner_strs = []\n    for inner in nested:\n        inner_str = \"[\" + \",\".join(fmt_float(x) for x in inner) + \"]\"\n        inner_strs.append(inner_str)\n    return \"[\" + \",\".join(inner_strs) + \"]\"\n\ndef solve():\n    # Test Case 1: Exponential with rate lambda=0.7, truncated [0.2, 1.5], u in {0,0.3,0.6,1}\n    lam = 0.7\n    F_exp = lambda x: 1.0 - np.exp(-lam * x)\n    Finv_exp = lambda p: -np.log(1.0 - p) / lam\n    a1, b1 = 0.2, 1.5\n    u1 = [0.0, 0.3, 0.6, 1.0]\n    res1 = truncated_quantiles(F_exp, Finv_exp, a1, b1, u1)\n\n    # Test Case 2: Standard Normal, truncated [-0.5, 1.0], u in {0,0.5,1}\n    F_norm = lambda x: norm.cdf(x)\n    Finv_norm = lambda p: norm.ppf(p)\n    a2, b2 = -0.5, 1.0\n    u2 = [0.0, 0.5, 1.0]\n    res2 = truncated_quantiles(F_norm, Finv_norm, a2, b2, u2)\n\n    # Test Case 3: Polynomial base f(x)=3x^2 on [0,1], F(x)=x^3, truncated [0.2,0.8], u in {0,0.25,0.5,0.75,1}\n    F_poly = lambda x: x**3\n    Finv_poly = lambda p: p**(1.0/3.0)\n    a3, b3 = 0.2, 0.8\n    u3 = [0.0, 0.25, 0.5, 0.75, 1.0]\n    res3 = truncated_quantiles(F_poly, Finv_poly, a3, b3, u3)\n\n    # Test Case 4: Base PDF ∝ exp(-x^3) on [0,1], F(x) = gammainc(1/3, x^3) / gammainc(1/3, 1), truncated [0.1,0.9], u in {0,0.5,1}\n    denom = gammainc(1.0/3.0, 1.0)  # gammainc is the regularized lower incomplete gamma; denom is P(1/3,1)\n    F_expnegx3 = lambda x: gammainc(1.0/3.0, x**3) / denom\n    a4, b4 = 0.1, 0.9\n    u4 = [0.0, 0.5, 1.0]\n    # No closed-form inverse; use numerical inversion on [a,b]\n    res4 = truncated_quantiles(F_expnegx3, None, a4, b4, u4)\n\n    results = [res1, res2, res3, res4]\n\n    # Final print statement in the exact required format.\n    print(format_nested_list(results))\n\nsolve()\n```"
        },
        {
            "introduction": "When we rely on numerical approximations of quantile functions, how can we trust that our generated samples are correct? This final exercise addresses the critical task of validation by focusing on a fundamental property: a valid quantile function must be non-decreasing . You will implement a test to detect violations of monotonicity and quantify the resulting sampling bias, providing a practical toolkit for ensuring the reliability and accuracy of your simulation code.",
            "id": "3314471",
            "problem": "You are given the task of assessing whether a numerically implemented quantile function for a continuous distribution is nondecreasing and of quantifying how deviations from nondecreasing behavior induce sampling bias when using inverse transform sampling.\n\nAssume the target distribution has support on the interval $[0,1]$ with cumulative distribution function $F(x) = x^{\\alpha}$ for $x \\in [0,1]$ and shape parameter $\\alpha = 2$. The corresponding exact quantile function is $Q^{\\star}(u) = u^{1/\\alpha}$ for $u \\in [0,1]$. Inverse transform sampling is defined as generating $U \\sim \\mathrm{Uniform}(0,1)$, and setting $X = Q(U)$ for a quantile function $Q$. If $Q$ is a right-continuous, nondecreasing function satisfying the generalized-inverse property relative to $F$, then the distribution of $X$ coincides with the target. If $Q$ is not nondecreasing, the pushforward distribution of $U$ under $Q$ deviates from the target, causing sampling bias.\n\nDesign and implement a program to perform the following:\n\n1. For a grid of $u$ values, verify the monotonicity of a given numerical quantile function $Q(u)$ by checking that the discrete forward differences are nonnegative. Specifically, for a grid $\\{u_i\\}_{i=0}^{N-1}$ with $u_0 = 0$, $u_{N-1} = 1$, and $u_i$ evenly spaced, compute $\\Delta_i = Q(u_{i+1}) - Q(u_i)$ for $i = 0, \\dots, N-2$ and define a nondecreasing test with tolerance $\\tau$ as follows:\n   - The function passes the monotonicity test if and only if $\\Delta_i \\ge -\\tau$ for all $i$.\n   - Define the violation count as the number of indices $i$ such that $\\Delta_i lt; -\\tau$.\n   - Define the violation magnitude as $\\sum_{\\{i: \\Delta_i lt; -\\tau\\}} (-\\Delta_i)$.\n\n2. Quantify the sampling bias induced by using $Q(u)$ with inverse transform sampling, measured by the total variation distance between the induced distribution and the target distribution over a specified set of histogram bins in $x$-space. Let the histogram consist of $B$ bins with edges $\\{b_j\\}_{j=0}^{B}$ covering the interval $[x_{\\min}, x_{\\max}]$, where $x_{\\min} \\le 0$ and $x_{\\max} \\ge 1$. Define the target bin probabilities by\n   $$p^{\\text{true}}_j = F(\\min\\{1, b_{j+1}\\}) - F(\\max\\{0, b_j\\}), \\quad j = 0, \\dots, B-1,$$\n   with the convention that $F(x) = 0$ for $x \\le 0$ and $F(x) = 1$ for $x \\ge 1$. This assigns zero mass outside $[0,1]$. Define the induced bin probabilities by mapping a uniform grid in $u$ to $x$ via $Q(u)$ and counting the fraction falling in each bin (including any mass outside $[0,1]$ if $Q(u)$ produces such values). Compute the total variation distance\n   $$\\mathrm{TV} = \\frac{1}{2} \\sum_{j=0}^{B-1} \\left| p^{\\text{gen}}_j - p^{\\text{true}}_j \\right|,$$\n   where $p^{\\text{gen}}_j$ are the induced bin probabilities.\n\n3. Evaluate three numerical quantile implementations $Q(u)$ against the above tests, all within the same target distribution with $\\alpha = 2$:\n   - Case A (baseline, exact): $Q_{\\mathrm{A}}(u) = u^{1/2}$.\n   - Case B (oscillatory perturbation): $Q_{\\mathrm{B}}(u) = u^{1/2} + \\varepsilon \\sin(2\\pi k u)$ with $\\varepsilon = 0.02$ and $k = 5$.\n   - Case C (segment reversal): For parameters $a = 0.3$ and $b = 0.5$, define\n     $$Q_{\\mathrm{C}}(u) = \\begin{cases}\n     u^{1/2},  u \\notin [a,b], \\\\\n     \\left(a + b - u\\right)^{1/2},  u \\in [a,b].\n     \\end{cases}$$\n     This introduces a strictly decreasing segment on $[a,b]$ while preserving continuity.\n\nUse the following fixed parameters to constitute the test suite:\n- Monotonicity grid: $N = 200001$ evenly spaced points on $[0,1]$ and tolerance $\\tau = 10^{-12}$.\n- Bias evaluation:\n  - Use $M = 200000$ evenly spaced points on $[0,1]$ as a proxy for a large inverse-transform sample without randomness by mapping via $Q(u)$.\n  - Histogram specification: $B = 400$ bins with edges uniformly spaced on $[x_{\\min}, x_{\\max}]$ where $x_{\\min} = -0.05$ and $x_{\\max} = 1.05$.\n\nYour program must compute, for each of the three cases $\\mathrm{A}$, $\\mathrm{B}$, and $\\mathrm{C}$, the following four outputs:\n- A monotonicity flag equal to $1$ if the nondecreasing test passes and $0$ otherwise.\n- The violation count as an integer.\n- The violation magnitude as a floating-point number.\n- The total variation distance $\\mathrm{TV}$ as a floating-point number.\n\nFinal output format: Your program should produce a single line of output containing a list of three lists, one per case in the order $\\mathrm{A}$, $\\mathrm{B}$, $\\mathrm{C}$, where each inner list is of the form $[\\text{flag}, \\text{count}, \\text{violation\\_magnitude}, \\text{TV}]$. The entire output must be a single line in the format\n$[ [\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot] ]$\nwith numerical entries printed as standard decimal strings.",
            "solution": "The objective is to validate three numerical implementations of a quantile function, denoted $Q(u)$, for a continuous probability distribution and to quantify the sampling bias introduced when these functions are not nondecreasing. This assessment is fundamental to the reliable application of the inverse transform sampling method, a cornerstone of Monte Carlo simulation.\n\nThe theoretical basis for this problem is the probability integral transform and its inverse. For a continuous random variable $X$ with a strictly increasing cumulative distribution function (CDF) $F(x)$, the random variable $U = F(X)$ is uniformly distributed on $[0,1]$. The inverse transform sampling method leverages the converse: if $U$ is drawn from a uniform distribution $\\mathrm{Uniform}(0,1)$, then the random variable $X = F^{-1}(U)$ will have the CDF $F(x)$. The function $F^{-1}(u)$ is the quantile function, $Q(u)$. A crucial property for the validity of this method is that the quantile function $Q(u)$ must be nondecreasing. If this condition is violated, the mapping from $U$ to $X$ does not preserve the probability measure correctly, and the distribution of the generated samples $X$ will deviate from the target distribution $F(x)$.\n\nThe target distribution for this analysis is defined by the CDF $F(x) = x^{\\alpha}$ for $x \\in [0,1]$ with the parameter $\\alpha=2$. The support of this distribution is the interval $[0,1]$. The CDF is defined to be $F(x)=0$ for $x \\le 0$ and $F(x)=1$ for $x \\ge 1$. The corresponding exact quantile function is $Q^{\\star}(u) = F^{-1}(u) = u^{1/\\alpha} = u^{1/2}$ for $u \\in [0,1]$.\n\nThe analysis is performed in two main steps for each of the three provided quantile function implementations.\n\n**Step 1: Monotonicity Validation**\n\nA function $Q(u)$ is nondecreasing if for any $u_1  u_2$, it holds that $Q(u_1) \\le Q(u_2)$. To verify this property numerically, we discretize the domain $[0,1]$ into a fine, uniform grid of $N=200001$ points, $\\{u_i\\}_{i=0}^{N-1}$, where $u_i = i/(N-1)$. We then compute the sequence of forward differences, $\\Delta_i = Q(u_{i+1}) - Q(u_i)$ for $i=0, \\dots, N-2$. In theory, all $\\Delta_i$ must be non-negative. However, to accommodate for finite-precision floating-point arithmetic, we introduce a small tolerance $\\tau = 10^{-12}$. The function $Q(u)$ is considered to pass the monotonicity test if and only if $\\Delta_i \\ge -\\tau$ for all $i$.\n\nFrom this test, we derive three metrics:\n1.  A binary monotonicity flag: This is $1$ if the test passes (all $\\Delta_i \\ge -\\tau$) and $0$ otherwise.\n2.  The violation count: This is an integer representing the total number of indices $i$ for which the condition fails, i.e., $\\Delta_i  -\\tau$.\n3.  The violation magnitude: This is a floating-point number that quantifies the total extent of the decreases, calculated as the sum $\\sum_{\\{i: \\Delta_i  -\\tau\\}} (-\\Delta_i)$.\n\n**Step 2: Sampling Bias Quantification**\n\nWhen a quantile function $Q(u)$ is not nondecreasing, using it for inverse transform sampling introduces bias. We quantify this bias by measuring the statistical distance between the distribution induced by $Q(u)$ and the true target distribution. The metric used is the Total Variation (TV) distance, defined as:\n$$\n\\mathrm{TV} = \\frac{1}{2} \\sum_{j=0}^{B-1} | p^{\\text{gen}}_j - p^{\\text{true}}_j |\n$$\nwhere $\\{p^{\\text{gen}}_j\\}$ and $\\{p^{\\text{true}}_j\\}$ are the probabilities assigned to $B$ histogram bins across a specified range.\n\nThe bin probabilities are calculated as follows:\n-   **Target Probabilities ($p^{\\text{true}}$)**: These are calculated analytically from the exact CDF, $F(x)$. For a set of $B=400$ histogram bins with edges $\\{b_j\\}_{j=0}^{B}$ uniformly spaced on $[x_{\\min}, x_{\\max}] = [-0.05, 1.05]$, the true probability in the $j$-th bin is $p^{\\text{true}}_j = F(b_{j+1}) - F(b_j)$. The problem's specification of $F(x)$ outside $[0,1]$, captured by $p^{\\text{true}}_j = F(\\min\\{1, b_{j+1}\\}) - F(\\max\\{0, b_j\\})$, correctly assigns all probability mass to the interval $[0,1]$.\n-   **Generated Probabilities ($p^{\\text{gen}}$)**: These are determined by mapping a deterministic, uniform grid of $M=200000$ points $\\{u_k\\}$ from $[0,1]$ through the numerical quantile function $Q(u)$ to obtain a set of generated values $\\{x_k = Q(u_k)\\}$. The probabilities $p^{\\text{gen}}_j$ are the fractions of these $M$ points that fall into each histogram bin $j$. This deterministic procedure isolates the structural bias of the function $Q(u)$ from the stochasticity of random sampling.\n\n**Analysis of Quantile Function Cases**\n\nThe procedure is applied to three distinct functions:\n\n1.  **Case A: $Q_{\\mathrm{A}}(u) = u^{1/2}$**. This is the exact, correct quantile function. It is strictly increasing on $(0,1]$, so we expect it to pass the monotonicity test (flag=$1$, count=$0$, magnitude=$0.0$) and to exhibit a TV distance that is non-zero but very small, attributable solely to numerical discretization and binning effects.\n\n2.  **Case B: $Q_{\\mathrm{B}}(u) = u^{1/2} + \\varepsilon \\sin(2\\pi k u)$** with $\\varepsilon = 0.02$ and $k=5$. This function adds a small, high-frequency oscillation to the true quantile function. The derivative is $Q'_{\\mathrm{B}}(u) = \\frac{1}{2}u^{-1/2} + 2\\pi k \\varepsilon \\cos(2\\pi k u)$. The second term can be negative, and for certain values of $u$, it can overwhelm the first term, causing $Q'_{\\mathrm{B}}(u)  0$. We therefore expect this function to fail the monotonicity test, resulting in a non-zero violation count and magnitude, and a corresponding non-zero TV distance.\n\n3.  **Case C: $Q_{\\mathrm{C}}(u)$**, which is $u^{1/2}$ for $u \\notin [0.3, 0.5]$ and $(0.3 + 0.5 - u)^{1/2}$ for $u \\in [0.3, 0.5]$. This function replaces a segment of the correct quantile function with a segment that maps the interval $[0.3, 0.5]$ in reverse. On this interval, $Q_{\\mathrm{C}}(u) = (0.8 - u)^{1/2}$, which has a strictly negative derivative. This represents a significant, structural violation of the nondecreasing property. We anticipate a large violation count and magnitude and a substantial TV distance, reflecting a severe sampling bias.\n\nThe following program implements this entire validation and quantification procedure. It defines the three quantile functions, calculates the four specified metrics for each, and presents the results in the required format. `NumPy` is used for its powerful and efficient array operations, which are essential for processing the large grids of points required by the problem's specification.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs monotonicity and bias analysis on three numerical quantile functions.\n    \"\"\"\n    # Problem parameters\n    ALPHA = 2.0\n    # Case B parameters\n    EPSILON = 0.02\n    K = 5\n    # Case C parameters\n    A = 0.3\n    B = 0.5\n    # Monotonicity test parameters\n    N_MONO = 200001\n    TAU = 1e-12\n    # Bias evaluation parameters\n    M_BIAS = 200000\n    B_HIST = 400\n    X_MIN = -0.05\n    X_MAX = 1.05\n\n    # --- Define Quantile Functions ---\n    def Q_A(u):\n        # Case A: Exact quantile function\n        return np.sqrt(u)\n\n    def Q_B(u):\n        # Case B: Oscillatory perturbation\n        return np.sqrt(u) + EPSILON * np.sin(2 * np.pi * K * u)\n\n    def Q_C(u):\n        # Case C: Segment reversal\n        res = np.sqrt(u)\n        mask = (u = A)  (u = B)\n        res[mask] = np.sqrt(A + B - u[mask])\n        return res\n\n    # --- Define Target CDF ---\n    def F_target(x):\n        # Target CDF: F(x) = x^alpha on [0,1], 0 below, 1 above.\n        # np.clip handles the domain correctly.\n        x_clipped = np.clip(x, 0, 1)\n        return x_clipped**ALPHA\n\n    def evaluate_quantile_function(Q_func):\n        \"\"\"\n        Runs the full analysis for a given quantile function Q.\n        \n        Returns:\n            list: [monotonicity_flag, violation_count, violation_magnitude, tv_distance]\n        \"\"\"\n        # --- 1. Monotonicity Test ---\n        u_mono = np.linspace(0.0, 1.0, N_MONO)\n        x_vals_mono = Q_func(u_mono)\n        \n        # Compute forward differences\n        deltas = np.diff(x_vals_mono)\n        \n        # Find violations\n        violations_mask = deltas  -TAU\n        violation_count = int(np.sum(violations_mask))\n        \n        monotonicity_flag = 1 if violation_count == 0 else 0\n        \n        # Sum of magnitudes of violations\n        if violation_count  0:\n            violation_magnitude = float(-np.sum(deltas[violations_mask]))\n        else:\n            violation_magnitude = 0.0\n\n        # --- 2. Bias Evaluation ---\n        # Define histogram bins\n        bin_edges = np.linspace(X_MIN, X_MAX, B_HIST + 1)\n\n        # Calculate target bin probabilities (p_true)\n        # The problem defines F(min{1,b_{j+1}})-F(max{0,b_j}). Our F_target does this.\n        # F_target applies np.clip internally.\n        p_true = F_target(bin_edges[1:]) - F_target(bin_edges[:-1])\n\n        # Calculate generated bin probabilities (p_gen)\n        # Use a deterministic grid for u as a proxy for a large sample\n        u_bias = np.linspace(0.0, 1.0, M_BIAS)\n        x_generated = Q_func(u_bias)\n        \n        # Histogram the generated values\n        counts, _ = np.histogram(x_generated, bins=bin_edges)\n        p_gen = counts / M_BIAS\n\n        # Calculate Total Variation Distance\n        tv_distance = float(0.5 * np.sum(np.abs(p_gen - p_true)))\n\n        return [monotonicity_flag, violation_count, violation_magnitude, tv_distance]\n\n    # --- Run Analysis for All Cases ---\n    results = []\n    for Q_func in [Q_A, Q_B, Q_C]:\n        result_vector = evaluate_quantile_function(Q_func)\n        results.append(result_vector)\n        \n    # --- Format and Print Output ---\n    # The output must be a single line in the specified format.\n    # Ex: [[1,0,0.0,1.2e-5],[0,3,0.1,0.2],[...]]\n    output_str = \",\".join(f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results)\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```"
        }
    ]
}