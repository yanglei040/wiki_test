{
    "hands_on_practices": [
        {
            "introduction": "Before we can trust a sequence of numbers for simulation, we must verify they follow the intended distribution. The Kolmogorov-Smirnov (KS) test is a cornerstone for this task, providing a rigorous way to measure the discrepancy between an empirical distribution and a hypothesized one, in this case, the standard uniform distribution. This practice guides you through implementing the KS test from its theoretical definition, revealing how the calculation of the supremum statistic $D_n = \\sup_{u \\in [0,1]} | F_n(u) - u |$ can be efficiently performed in $O(n \\log n)$ time, a crucial insight for practical application .",
            "id": "3347511",
            "problem": "You are given a finite sample $u_1,\\dots,u_n \\in [0,1]$ and tasked with assessing the null hypothesis that the data are independent and identically distributed from the continuous Uniform distribution on $[0,1]$. Your program must implement an $O(n\\log n)$ algorithm to compute the exact Kolmogorov–Smirnov statistic $D_n$ and then compute a two-sided $p$-value for the one-sample test of uniformity using either an exact finite-sample method or an asymptotic method, as specified per test case.\n\nFundamental definitions to be used:\n- The empirical cumulative distribution function (empirical CDF) is $F_n(u) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{u_i \\le u\\}$.\n- The Kolmogorov–Smirnov statistic is $D_n = \\sup_{u \\in [0,1]} \\lvert F_n(u) - u \\rvert$.\n\nRequirements:\n1. Compute $D_n$ exactly by an algorithm with $O(n\\log n)$ time complexity. The intended approach should be derived from first principles of the empirical CDF and should not rely on numerical approximation of the supremum over a grid. The algorithm must handle ties in the sample.\n2. Compute the two-sided $p$-value $p = \\mathbb{P}(D_n \\ge d_{\\text{obs}})$ under the null hypothesis that $u_1,\\dots,u_n$ are independent and identically distributed Uniform on $[0,1]$. For this, use either:\n   - An exact finite-sample computation based on recursive counting of admissible paths or an equivalent numerically stable implementation, or\n   - An asymptotic approximation based on the weak convergence of $\\sqrt{n} D_n$ to the Kolmogorov distribution.\n3. Your program must implement both options and select which one to use based on a per-test-case directive.\n\nTest suite:\nImplement the following four test cases. For each case, compute and report both $D_n$ and the two-sided $p$-value according to the specified method. For clarity, define $\\operatorname{frac}(x)$ as the fractional part of $x$.\n\n- Case A (small $n$, exact):\n  - $n = 5$\n  - Sample: $[0.01, 0.20, 0.24, 0.90, 0.95]$\n  - Method: exact finite-sample\n\n- Case B (ties and endpoints, exact):\n  - $n = 10$\n  - Sample: $[0.00, 1.00, 0.12, 0.12, 0.30, 0.31, 0.55, 0.56, 0.78, 0.79]$\n  - Method: exact finite-sample\n\n- Case C (deterministic grid, asymptotic):\n  - $n = 2000$\n  - Sample: $u_i = \\frac{i}{n}$ for $i \\in \\{1,2,\\dots,n\\}$\n  - Method: asymptotic\n\n- Case D (deterministic quasi-random, asymptotic):\n  - $n = 100$\n  - Sample: $u_i = \\operatorname{frac}(i\\sqrt{2})$ for $i \\in \\{1,2,\\dots,n\\}$\n  - Method: asymptotic\n\nOutput specification:\n- For each test case, your program must produce a pair $[d, p]$ where $d$ is the computed $D_n$ and $p$ is the two-sided $p$-value for the specified method.\n- Round both $d$ and $p$ to $10$ decimal places.\n- Your program should produce a single line of output containing a list of these pairs, in order of the cases A, B, C, D, formatted as a comma-separated list enclosed in square brackets, with each pair itself enclosed in square brackets. For example, the output must look like:\n  - $[[d_A,p_A],[d_B,p_B],[d_C,p_C],[d_D,p_D]]$\n- All reported values are dimensionless real numbers and must be given as decimals.\n\nConstraints and notes:\n- The algorithm for $D_n$ must be $O(n\\log n)$ in time, which is achieved by sorting followed by a single linear pass.\n- The exact finite-sample method must target the true null distribution of $D_n$ for finite $n$. The asymptotic method must rely on the convergence in distribution of $\\sqrt{n} D_n$ to the Kolmogorov distribution.\n- No randomness is allowed in constructing the test samples; all samples are deterministic as specified.",
            "solution": "The user requests the implementation of the one-sample Kolmogorov-Smirnov test for uniformity. The problem is valid as it is scientifically grounded in well-established statistical theory, is well-posed with all necessary information provided, and is expressed in objective, formal language. We will proceed with a solution.\n\nThe solution requires two main components:\n1.  An algorithm to compute the Kolmogorov-Smirnov (KS) statistic, $D_n$.\n2.  Algorithms to compute the two-sided p-value using both an exact finite-sample method and an asymptotic approximation.\n\n### 1. Computation of the KS Statistic $D_n$\n\nThe KS statistic $D_n$ is defined as the supremum of the absolute difference between the empirical cumulative distribution function (ECDF) $F_n(u)$ and the CDF of the uniform distribution, $F(u)=u$.\n$$\nD_n = \\sup_{u \\in [0,1]} |F_n(u) - u|\n$$\nThe ECDF for a sample $u_1, u_2, \\dots, u_n$ is given by:\n$$\nF_n(u) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{u_i \\le u\\}\n$$\n$F_n(u)$ is a step function that increases by $1/n$ at each sample point. The function $g(u) = F_n(u) - u$ is piecewise linear with a slope of $-1$, and it has jumps at the sample points. Therefore, the supremum of $|g(u)|$ must be attained immediately before or at one of these jump points.\n\nLet the sample be sorted in non-decreasing order: $u_{(1)} \\le u_{(2)} \\le \\dots \\le u_{(n)}$.\nThe ECDF has the value $F_n(u) = (i-1)/n$ for $u \\in [u_{(i-1)}, u_{(i)})$ and jumps to $F_n(u_{(i)}) = i/n$ if there are no ties at $u_{(i)}$. If there are ties, the jump is larger.\n\nA general and robust algorithm that correctly handles ties is to check the deviation at each sorted sample point $u_{(i)}$. At each point $u_{(i)}$, we evaluate the difference just before the jump and at the jump.\n- The value of the ECDF as $u$ approaches $u_{(i)}$ from the left is $F_n(u_{(i)}^{-}) = (i-1)/n$. The deviation is $|(i-1)/n - u_{(i)}| = u_{(i)} - (i-1)/n$ (since $u_{(i-1)} < u_{(i)}$ and $(i-1)/n \\approx u_{(i-1)}$).\n- The value of the ECDF at $u_{(i)}$ is $F_n(u_{(i)})$. If there are no ties, this is $i/n$. The deviation is $|i/n - u_{(i)}|$.\n\nThis suggests that the supremum can be found by calculating two quantities for each $i \\in \\{1, \\dots, n\\}$:\n$$\nd_i^+ = \\frac{i}{n} - u_{(i)} \\quad \\text{and} \\quad d_i^- = u_{(i)} - \\frac{i-1}{n}\n$$\nThe statistic $D_n$ is the maximum over all these values.\n$$\nD_n = \\max_{i=1,\\dots,n} \\left( d_i^+, d_i^- \\right)\n$$\nThis is equivalent to:\n$$\nD_n = \\max \\left( \\max_{i=1,\\dots,n} \\left( \\frac{i}{n} - u_{(i)} \\right), \\max_{i=1,\\dots,n} \\left( u_{(i)} - \\frac{i-1}{n} \\right) \\right)\n$$\nThe algorithm is as follows:\n1.  Sort the sample $u_1, \\dots, u_n$. This step has a time complexity of $O(n \\log n)$.\n2.  Perform a single linear pass over the sorted sample, calculating $d_i^+$ and $d_i^-$ for each $i$ and tracking the maximum value. This step is $O(n)$.\n\nThe total time complexity is dominated by the sorting step, resulting in an $O(n \\log n)$ algorithm as required. This procedure correctly handles ties in the data, as the indices $i$ will correctly reflect the cumulative counts.\n\n### 2. Computation of the p-value\n\nThe p-value is the probability of observing a test statistic as extreme as or more extreme than the one computed ($d_{obs}$), assuming the null hypothesis is true: $p = \\mathbb{P}(D_n \\ge d_{obs})$.\n\n#### 2.1. Exact Finite-Sample Method\nFor small sample sizes, the exact distribution of $D_n$ can be computed. This is a complex combinatorial problem, often solved with recursive algorithms based on counting admissible paths or using matrix-based methods. These algorithms compute the probability $\\mathbb{P}(D_n < d)$ by integrating the joint density of the order statistics of a uniform sample over the region defined by the constraints $|F_n(u) - u| < d$.\n\nThe problem is equivalent to computing the volume of the polytope defined by:\n$0 < u_{(1)} < \\dots < u_{(n)} < 1$\nand\n$\\frac{i}{n} - d < u_{(i)} < \\frac{i-1}{n} + d \\quad \\text{for } i=1, \\dots, n$.\n\nDirect implementation of the associated recursive integral is highly non-trivial. Standard statistical libraries like `scipy` contain numerically stable, well-vetted implementations of these exact methods (e.g., based on the work of Marsaglia, Tsang, and Wang, or Abas and Veraverbeke). Given the complexity and need for robustness, we will leverage the `scipy.stats.ks_1samp` function with the `mode='exact'` option. This is a sound scientific practice, as it uses a peer-reviewed algorithm, satisfying the problem's requirement for an \"exact finite-sample computation\". The function takes the sample as input and returns the statistic and the exact p-value.\n\n#### 2.2. Asymptotic Method\nFor large sample sizes ($n$), the distribution of the random variable $\\sqrt{n}D_n$ converges to the Kolmogorov distribution. Let $K$ be a random variable following this distribution.\n$$\n\\sqrt{n}D_n \\xrightarrow{d} K\n$$\nThe p-value can be approximated by:\n$$\np = \\mathbb{P}(D_n \\ge d_{obs}) \\approx \\mathbb{P}(K \\ge \\sqrt{n} d_{obs})\n$$\nThe survival function of the Kolmogorov distribution, $\\mathbb{P}(K > x)$, is given by the well-known series:\n$$\n\\mathbb{P}(K > x) = 1 - \\left( 1 - 2\\sum_{j=1}^{\\infty} (-1)^{j-1} e^{-2j^2x^2} \\right) = 2\\sum_{j=1}^{\\infty} (-1)^{j-1} e^{-2j^2x^2}\n$$\nThis series converges rapidly for moderate to large values of $x$. For very small $x$, an alternative series based on Jacobi theta functions is used for better numerical stability. The function `scipy.special.kolmogorov` provides an accurate implementation for computing this probability.\n\nThe algorithm is:\n1.  Compute $d_{obs}$ from the sample.\n2.  Calculate the scaled statistic $x = \\sqrt{n} d_{obs}$.\n3.  Compute the p-value using the asymptotic formula, for instance via `scipy.special.kolmogorov(x)`.\n\nWe will now implement these methods to solve the four test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import ks_1samp\nfrom scipy.special import kolmogorov\n\ndef solve():\n    \"\"\"\n    Implements the Kolmogorov-Smirnov test for uniformity for a set of predefined test cases.\n    \"\"\"\n\n    def calculate_dn(sample: list[float]) -> float:\n        \"\"\"\n        Calculates the Kolomogorov-Smirnov statistic D_n in O(n log n) time.\n        \n        The statistic is D_n = sup_u |F_n(u) - u|, where F_n is the ECDF.\n        The calculation is based on the formula:\n        D_n = max(max_{i=1..n}(i/n - u_{(i)}), max_{i=1..n}(u_{(i)} - (i-1)/n))\n        where u_{(i)} are the sorted sample values.\n        \"\"\"\n        n = len(sample)\n        if n == 0:\n            return 0.0\n        \n        u_sorted = np.sort(np.asarray(sample, dtype=np.float64))\n        \n        d_max = 0.0\n        for i in range(n):\n            # i is 0-indexed (0 to n-1), so we use (i+1) for the formula's 1-indexed i.\n            # u_{(i+1)} corresponds to u_sorted[i].\n            # Deviation from above: F_n(u_{(i+1)}) - u_{(i+1)} = (i+1)/n - u_sorted[i]\n            d_plus = (i + 1) / n - u_sorted[i]\n            # Deviation from below: u_{(i+1)} - F_n(u_{(i+1)}^{-}) = u_sorted[i] - i/n\n            d_minus = u_sorted[i] - i / n\n            \n            d_max = max(d_max, d_plus, d_minus)\n            \n        return d_max\n\n    def p_value_asymptotic(d: float, n: int) -> float:\n        \"\"\"\n        Calculates the two-sided p-value using the asymptotic Kolmogorov distribution.\n        p = P(D_n >= d) approx P(K >= sqrt(n)*d).\n        \"\"\"\n        if n == 0:\n            return 1.0\n        x = np.sqrt(n) * d\n        # scipy.special.kolmogorov(x) computes P(K > x), which is what we need.\n        return kolmogorov(x)\n\n    def solve_case(sample: list[float], method: str) -> list[float, float]:\n        \"\"\"\n        Solves one test case by computing D_n and the p-value.\n        \"\"\"\n        n = len(sample)\n        d = calculate_dn(sample)\n        \n        p = 0.0\n        if method == 'exact':\n            # We use scipy's vetted implementation for the exact p-value.\n            # It takes the sample and computes both statistic and p-value.\n            # While it recomputes d, it guarantees correctness for this complex calculation.\n            # The CDF is F(x)=x for the uniform distribution on [0,1].\n            _, p = ks_1samp(sample, lambda x: x, mode='exact')\n        elif method == 'asymptotic':\n            p = p_value_asymptotic(d, n)\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n            \n        return [round(d, 10), round(p, 10)]\n\n    # Define the test cases from the problem statement.\n    test_cases_spec = [\n        {\n            'name': 'A',\n            'n': 5,\n            'sample_gen': lambda n: [0.01, 0.20, 0.24, 0.90, 0.95],\n            'method': 'exact'\n        },\n        {\n            'name': 'B',\n            'n': 10,\n            'sample_gen': lambda n: [0.00, 1.00, 0.12, 0.12, 0.30, 0.31, 0.55, 0.56, 0.78, 0.79],\n            'method': 'exact'\n        },\n        {\n            'name': 'C',\n            'n': 2000,\n            'sample_gen': lambda n: [(i + 1) / n for i in range(n)],\n            'method': 'asymptotic'\n        },\n        {\n            'name': 'D',\n            'n': 100,\n            'sample_gen': lambda n: [(i * np.sqrt(2)) % 1.0 for i in range(1, n + 1)],\n            'method': 'asymptotic'\n        }\n    ]\n\n    results = []\n    for case in test_cases_spec:\n        sample = case['sample_gen'](case['n'])\n        result = solve_case(sample, case['method'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # e.g., [[d_A,p_A],[d_B,p_B],[d_C,p_C],[d_D,p_D]]\n    result_str = \",\".join([f\"[{d},{p}]\" for d, p in results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Uniformity is necessary but not sufficient; the numbers in our sequence must also be independent. A simple yet powerful method to detect serial dependence is the runs test, which examines the ordering of the values rather than the values themselves. In this exercise, you will implement a runs test and delve into its theoretical underpinnings by deriving the exact null distribution using Eulerian numbers, providing a fascinating look at the interplay between combinatorics and non-parametric statistics .",
            "id": "3347502",
            "problem": "You must design and implement a complete, runnable program that constructs and applies a test for serial independence in a sequence of realizations $U_1,U_2,\\dots,U_n$ from the distribution $\\mathrm{Unif}(0,1)$ by using the distribution of ascending runs. The program must compute the exact null distribution for small $n$ and use it to produce an exact two-sided $p$-value. The test must be based only on the relative order structure implied by independence and continuity, and not on any shortcut formulas. The foundational base you must adhere to is as follows.\n\nFundamental base:\n- If $U_1,U_2,\\dots,U_n$ are independent and identically distributed from a continuous distribution, then the vector of ranks of $(U_1,\\dots,U_n)$ is uniformly distributed over all $n!$ permutations. This follows from the Probability Integral Transform: if $X$ has continuous cumulative distribution function $F$, then $F(X)\\sim\\mathrm{Unif}(0,1)$, and independence is preserved under measurable transformations.\n- For any finite sequence of distinct real numbers $(x_1,\\dots,x_n)$, an ascending run is defined as a maximal contiguous block of indices on which the sequence is strictly increasing. The number of ascending runs $R$ equals $1$ plus the number of descents, where a descent is an index $i\\in\\{1,\\dots,n-1\\}$ such that $x_i>x_{i+1}$.\n\nTask requirements:\n1. Define precisely the count of ascending runs $R$ in the observed order of $U_1,\\dots,U_n$ as the number of maximal contiguous strictly increasing segments. Formally, for $n\\geq 1$, start with $R=1$, and for each $i\\in\\{1,\\dots,n-1\\}$ add $1$ to $R$ whenever $U_i\\geq U_{i+1}$. Because $\\mathrm{Unif}(0,1)$ is continuous, equalities occur with probability $0$, but your algorithm must treat $U_i\\geq U_{i+1}$ as a run break.\n2. Under the null hypothesis $\\mathsf{H}_0$ that the sample is independent and identically distributed $\\mathrm{Unif}(0,1)$, derive the exact null distribution of $R$ for small $n$ from first principles. Specifically, use the fact that the ranks of $(U_1,\\dots,U_n)$ form a uniformly random permutation, and the number of ascending runs $R$ depends only on the permutation’s descents. Let $\\langle n,k\\rangle$ denote the Eulerian number that counts permutations of $\\{1,\\dots,n\\}$ with exactly $k$ ascents. Compute $\\langle n,k\\rangle$ for $n\\leq 10$ using the recurrence derived from insertion arguments on permutations:\n$$\n\\langle n,k\\rangle=(k+1)\\langle n-1,k\\rangle + (n-k)\\langle n-1,k-1\\rangle,\n$$\nwith the convention that $\\langle n-1,-1\\rangle=\\langle n-1,n-1\\rangle=0$.\nRelate runs to ascents via descents: if a permutation has $d$ descents, then it has $R=d+1$ runs and $k=n-1-d$ ascents. Therefore, the null distribution of $R$ is:\n$$\n\\mathbb{P}_{\\mathsf{H}_0}(R=r)=\\frac{\\langle n,\\,n-r\\rangle}{n!},\\quad r\\in\\{1,2,\\dots,n\\}.\n$$\n3. Define the exact two-sided $p$-value for an observed $r$ using probability ordering on the discrete null distribution of $R$:\n$$\np\\text{-value}=\\sum_{r':\\,\\mathbb{P}_{\\mathsf{H}_0}(R=r')\\leq \\mathbb{P}_{\\mathsf{H}_0}(R=r)} \\mathbb{P}_{\\mathsf{H}_0}(R=r').\n$$\nReject $\\mathsf{H}_0$ at significance level $\\alpha$ if and only if the $p$-value is less than or equal to $\\alpha$. Use $\\alpha$ supplied in each test case.\n4. Implement the algorithm to compute $R$, compute $\\langle n,k\\rangle$ for all $k\\in\\{0,1,\\dots,n-1\\}$ using the recurrence, construct the null distribution $\\mathbb{P}_{\\mathsf{H}_0}(R=r)$ for $r\\in\\{1,\\dots,n\\}$, compute the exact two-sided $p$-value, and return the decision to reject or not.\n5. The program must be fully deterministic and use no randomization. All input is internal to the program.\n\nTest suite:\n- Case $1$: $n=1$, sequence $[0.42]$, $\\alpha=0.05$.\n- Case $2$: $n=2$, sequence $[0.7,0.6]$, $\\alpha=0.50$.\n- Case $3$: $n=5$, sequence $[0.1,0.8,0.2,0.7,0.3]$, $\\alpha=0.05$.\n- Case $4$: $n=8$, sequence $[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8]$, $\\alpha=0.05$.\n- Case $5$: $n=8$, sequence $[0.9,0.1,0.8,0.2,0.7,0.3,0.6,0.4]$, $\\alpha=0.05$.\n\nAnswer specification:\n- For each test case, compute and return a list $[n,R,p,\\text{reject}]$, where $n$ is the integer length, $R$ is the integer number of ascending runs, $p$ is the exact two-sided $p$-value as a decimal, and $\\text{reject}$ is a boolean indicating whether to reject $\\mathsf{H}_0$ at the given $\\alpha$.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, with each test case’s result in its own bracket. For example, the format must be like $[[n_1,R_1,p_1,\\text{reject}_1],[n_2,R_2,p_2,\\text{reject}_2],\\dots]$.",
            "solution": "The problem requires the design and implementation of a statistical test for the serial independence of a sequence of random variables, purportedly drawn from a $\\mathrm{Unif}(0,1)$ distribution. The test is to be based on the number of ascending runs in the sequence. The solution involves deriving and using the exact null distribution of the test statistic for small sample sizes.\n\nThe fundamental principle underlying this test is that if $U_1, U_2, \\dots, U_n$ are independent and identically distributed (i.i.d.) from any continuous distribution (such as $\\mathrm{Unif}(0,1)$), then any permutation of their values is equally likely. Consequently, the vector of their ranks is uniformly distributed over the $n!$ possible permutations of $\\{1, 2, \\dots, n\\}$. The number of ascending runs, $R$, depends only on the relative ordering of the $U_i$, i.e., on their rank permutation, and is thus a distribution-free statistic under the null hypothesis of independence.\n\nThe solution proceeds in five principal steps:\n1.  **Count the observed number of ascending runs, $R_{obs}$**: Define and compute the test statistic from the given sequence.\n2.  **Compute Eulerian Numbers**: Programmatically compute the table of Eulerian numbers, $\\langle n, k \\rangle$, which are essential for defining the null distribution.\n3.  **Construct the Null Distribution**: Use the Eulerian numbers to calculate the exact probability mass function (PMF) of $R$ under the null hypothesis, $\\mathbb{P}_{\\mathsf{H}_0}(R=r)$.\n4.  **Calculate the Two-Sided $p$-value**: Based on the observed statistic $R_{obs}$ and the null distribution, compute the exact two-sided $p$-value.\n5.  **Make a Decision**: Compare the $p$-value to the given significance level $\\alpha$ to decide whether to reject the null hypothesis.\n\n### Step 1: Counting Ascending Runs ($R$)\nAn ascending run is a maximal contiguous subsequence of increasing values. The number of such runs, $R$, is determined by the number of \"descents\" in the sequence. A descent occurs at index $i \\in \\{1, \\dots, n-1\\}$ if $U_i > U_{i+1}$. The problem statement specifies that a run break occurs when $U_i \\geq U_{i+1}$. For a continuous distribution, the probability of an equality $U_i = U_{i+1}$ is zero, so this distinction is typically negligible. The number of runs is one more than the number of descents.\n\nThe algorithm to count runs in a sequence $(U_1, \\dots, U_n)$ is:\n1. Initialize the number of runs, $R$, to $1$ (for $n \\geq 1$; for $n=0$, $R=0$).\n2. Iterate from $i=1$ to $n-1$.\n3. If $U_i \\geq U_{i+1}$, increment $R$ by $1$.\n\nFor example, in the sequence $[0.1, 0.8, 0.2, 0.7, 0.3]$, there are descents at $U_2=0.8 > U_3=0.2$ and $U_4=0.7 > U_5=0.3$. Starting with $R=1$, we add $2$ for the two descents, yielding an observed run count of $R_{obs}=3$.\n\n### Step 2: Computing Eulerian Numbers\nThe Eulerian number $\\langle n, k \\rangle$ counts the number of permutations of $\\{1, \\dots, n\\}$ with exactly $k$ ascents. An ascent is an index $i \\in \\{1, \\dots, n-1\\}$ where the permutation value at $i$ is less than the value at $i+1$. The number of descents, $d$, and ascents, $k$, in a permutation of length $n$ are related by $d+k = n-1$.\n\nThe problem provides the recurrence relation for Eulerian numbers:\n$$\n\\langle n, k \\rangle = (k+1) \\langle n-1, k \\rangle + (n-k) \\langle n-1, k-1 \\rangle\n$$\nfor $n \\geq 2$ and $0 \\leq k \\leq n-1$. The base case is $\\langle 1, 0 \\rangle = 1$. The boundary conditions are $\\langle n-1, -1 \\rangle = 0$ and $\\langle n-1, n-1 \\rangle = 0$. These can be computed and stored in a two-dimensional array for efficient lookup. For instance, to compute the row for $n$, we only need the values from the row for $n-1$.\n\n### Step 3: Constructing the Null Distribution of $R$\nThe number of runs $R$ is related to the number of descents $d$ by $R = d+1$. The number of ascents $k$ is $k = n-1-d$. Substituting $d = R-1$, we find the relationship between runs and ascents:\n$$\nk = n-1 - (R-1) = n-R\n$$\nA permutation with $R$ runs has $k = n-R$ ascents. Under the null hypothesis $\\mathsf{H}_0$, each of the $n!$ permutations is equally likely. The number of permutations with $R=r$ runs is therefore equal to the number of permutations with $k=n-r$ ascents, which is $\\langle n, n-r \\rangle$.\n\nThe probability of observing exactly $r$ runs is the ratio of favorable outcomes to the total number of outcomes:\n$$\n\\mathbb{P}_{\\mathsf{H}_0}(R=r) = \\frac{\\langle n, n-r \\rangle}{n!}, \\quad \\text{for } r \\in \\{1, 2, \\dots, n\\}.\n$$\nWe can pre-compute factorials and use the table of Eulerian numbers from Step 2 to build the PMF of $R$ for a given $n$.\n\n### Step 4: Calculating the Two-Sided $p$-value\nFor a discrete test statistic, the two-sided $p$-value is defined as the sum of probabilities of all outcomes that are as extreme or more extreme than the one observed. \"More extreme\" is interpreted as having a probability under $\\mathsf{H}_0$ that is less than or equal to the probability of the observed outcome.\n\nGiven an observed run count $R_{obs} = r$, its probability under the null is $P_{obs} = \\mathbb{P}_{\\mathsf{H}_0}(R=r)$. The $p$-value is then calculated as:\n$$\np\\text{-value} = \\sum_{r' \\in \\{1, \\dots, n\\} \\text{ s.t. } \\mathbb{P}_{\\mathsf{H}_0}(R=r') \\leq P_{obs}} \\mathbb{P}_{\\mathsf{H}_0}(R=r')\n$$\nThis involves iterating through all possible values of $R$ from $1$ to $n$, calculating their probabilities, and summing them up if they meet the criterion.\n\nAs an example, consider $n=5$. The null distribution, derived from the Eulerian numbers, is:\n$\\mathbb{P}(R=1) = 1/120$, $\\mathbb{P}(R=2) = 26/120$, $\\mathbb{P}(R=3) = 66/120$, $\\mathbb{P}(R=4) = 26/120$, $\\mathbb{P}(R=5) = 1/120$.\nIf we observe $R_{obs}=2$, then $P_{obs} = 26/120$. The outcomes $r'$ with $\\mathbb{P}(R=r') \\le 26/120$ are $r'=1$, $r'=2$, $r'=4$, and $r'=5$.\nThe $p$-value would be $\\mathbb{P}(R=1) + \\mathbb{P}(R=2) + \\mathbb{P}(R=4) + \\mathbb{P}(R=5) = (1+26+26+1)/120 = 54/120 = 0.45$.\n\n### Step 5: Making a Decision\nThe final step is to apply the standard decision rule. Given a significance level $\\alpha$, we compare the computed $p$-value to $\\alpha$:\n- If $p\\text{-value} \\leq \\alpha$, we reject the null hypothesis $\\mathsf{H}_0$. This suggests there is statistically significant evidence against the independence of the sequence.\n- If $p\\text{-value} > \\alpha$, we fail to reject $\\mathsf{H}_0$. This implies there is insufficient evidence to conclude that the sequence is not independent.\n\nThe entire procedure is deterministic and relies on established combinatorial and statistical principles, as stipulated by the problem. The implementation will combine these steps into a single program to process the specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the runs test for all specified test cases.\n    \"\"\"\n    test_cases = [\n        {'n': 1, 'sequence': [0.42], 'alpha': 0.05},\n        {'n': 2, 'sequence': [0.7, 0.6], 'alpha': 0.50},\n        {'n': 5, 'sequence': [0.1, 0.8, 0.2, 0.7, 0.3], 'alpha': 0.05},\n        {'n': 8, 'sequence': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8], 'alpha': 0.05},\n        {'n': 8, 'sequence': [0.9, 0.1, 0.8, 0.2, 0.7, 0.3, 0.6, 0.4], 'alpha': 0.05},\n    ]\n\n    max_n = max(case['n'] for case in test_cases)\n    \n    # Pre-compute Eulerian numbers and factorials to avoid redundant calculations.\n    eulerian_table = _compute_eulerian_table(max_n)\n    factorials = {i: math.factorial(i) for i in range(max_n + 1)}\n\n    results = []\n    for case in test_cases:\n        result = _perform_runs_test(\n            case['n'],\n            case['sequence'],\n            case['alpha'],\n            eulerian_table,\n            factorials\n        )\n        results.append(result)\n    \n    # Format the final output precisely as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _compute_eulerian_table(max_n):\n    \"\"\"\n    Computes Eulerian numbers <n, k> for 1 <= n <= max_n and 0 <= k < n.\n    The table stores <n, k> at index [n][k].\n    \"\"\"\n    # max_n+1 rows for n=0..max_n, max_n columns for k=0..max_n-1\n    table = np.zeros((max_n + 1, max_n), dtype=np.double)\n    \n    if max_n >= 1:\n        table[1, 0] = 1.0\n\n    for n in range(2, max_n + 1):\n        for k in range(n):\n            # Term 1: (k+1) * <n-1, k>\n            term1 = 0.0\n            if k < n - 1: # k for n-1 goes up to n-2\n                term1 = (k + 1) * table[n - 1, k]\n            \n            # Term 2: (n-k) * <n-1, k-1>\n            term2 = 0.0\n            if k > 0:\n                term2 = (n - k) * table[n - 1, k - 1]\n            \n            table[n, k] = term1 + term2\n            \n    return table\n\ndef _count_runs(sequence):\n    \"\"\"\n    Counts the number of ascending runs in a sequence.\n    \"\"\"\n    n = len(sequence)\n    if n <= 1:\n        return n\n    \n    runs = 1\n    for i in range(n - 1):\n        if sequence[i] >= sequence[i+1]:\n            runs += 1\n    return runs\n\ndef _perform_runs_test(n, sequence, alpha, eulerian_table, factorials):\n    \"\"\"\n    Performs the full runs test for a single case.\n    \"\"\"\n    if n == 0:\n        return [0, 0, 1.0, False]\n    \n    # Step 1: Count observed runs\n    r_obs = _count_runs(sequence)\n    \n    # Step 2 & 3: Get the null distribution from pre-computed tables\n    null_dist = {}\n    n_factorial = float(factorials[n])\n    if n_factorial == 0:  # Handles n=0 case, though guarded earlier\n        n_factorial = 1.0\n\n    for r in range(1, n + 1):\n        # k = n - r (number of ascents)\n        k = n - r\n        # P(R=r) = <n, n-r> / n! = <n, k> / n!\n        num_permutations = eulerian_table[n, k]\n        prob = num_permutations / n_factorial\n        null_dist[r] = prob\n\n    if not (1 <= r_obs <= n):\n        # This case should not be possible for n > 0\n        p_value = 1.0\n    else:\n        # Step 4: Compute the two-sided p-value\n        prob_obs = null_dist[r_obs]\n        p_value = 0.0\n        for r_val, prob in null_dist.items():\n            if prob <= prob_obs:\n                p_value += prob\n\n    # Step 5: Make decision\n    reject_h0 = p_value <= alpha\n    \n    return [n, r_obs, p_value, reject_h0]\n\nsolve()\n```"
        },
        {
            "introduction": "In multidimensional applications, the absence of pairwise correlation does not guarantee mutual independence, a subtle but critical distinction that can invalidate simulation results. This advanced practice challenges you to confront this issue directly by first constructing a dataset that exhibits pairwise but not mutual independence. You will then design and implement a more sophisticated test based on higher-order moments to uncover this hidden dependence structure, learning a valuable lesson in the limitations of simpler tests and the need for more powerful diagnostic tools .",
            "id": "3347518",
            "problem": "You are given the task of constructing explicit sequences of discrete random vectors that are pairwise independent but not mutually independent in dimension $d \\ge 3$, and of designing a higher-order test that can expose the hidden dependence. Your implementation must be purely algorithmic and mathematical, requiring no domain-specific physical units. All angles are irrelevant here, and no trigonometric units are involved.\n\nStarting point and definitions:\n- A collection of random variables $\\{X_1,\\dots,X_d\\}$ is said to be pairwise independent if every pair $(X_i,X_j)$ with $i \\ne j$ is independent. They are mutually independent if the joint distribution factors as the product of marginals.\n- For sequences, suppose $\\{(X_{t,1},\\dots,X_{t,d})\\}_{t=1}^n$ is an independent and identically distributed sample of a $d$-dimensional random vector.\n- Define the $\\{\\pm 1\\}$ coding by $Y_{t,i} = 2 X_{t,i} - 1$ so that $Y_{t,i} \\in \\{-1,+1\\}$ and $E[Y_{t,i}] = 0$ for fair Bernoulli input.\n- The joint cumulant provides a principled way to quantify higher-order dependence beyond pairwise interactions. Under mutual independence, all mixed joint cumulants of order at least $2$ vanish. In particular, for $m \\ge 3$, an $m$th-order multilinear interaction functional based on products $\\prod_{i \\in S} Y_{t,i}$ over index sets $S \\subset \\{1,\\dots,d\\}$ with $|S| = m$ has mean $0$ under mutual independence because $E\\left[\\prod_{i \\in S} Y_{t,i}\\right] = \\prod_{i \\in S} E[Y_{t,i}] = 0$.\n\nYour tasks:\n1. Construct data-generating mechanisms for sequences $\\{(X_{t,1},\\dots,X_{t,d})\\}_{t=1}^n$ with $X_{t,i} \\in \\{0,1\\}$ that are pairwise independent but not mutually independent, using parity constraints in dimension $d \\ge 3$. Specifically, use a parity construction where the last coordinate is the exclusive-or (sum modulo $2$) of the first $d-1$ coordinates. Also, include a noisy variant where, with probability $q$, the last coordinate is replaced by an independent fair bit; otherwise, it is set to the parity.\n2. From first principles and the above definitions, propose a higher-order interaction test for mutual independence based on the multilinear forms $\\prod_{i \\in S} Y_{t,i}$ for a set of orders $m \\in \\mathcal{M}$, where $\\mathcal{M}$ is a subset of $\\{3,4,\\dots,d\\}$. Your test must:\n   - For each order $m \\in \\mathcal{M}$ and each subset $S$ of indices with $|S| = m$, aggregate the empirical average over time $t = 1,\\dots,n$ of the product $\\prod_{i \\in S} Y_{t,i}$ and standardize it using a variance estimate derived from the sample of these products.\n   - Justify a large-sample decision rule using a normal approximation for the standardized mean of independent and identically distributed summands, and convert the standardized statistics into two-sided $p$-values.\n   - Combine all tests across all $m$ and all $S$ using a Bonferroni correction to control the family-wise error rate at level $\\alpha$.\n   - Return a boolean decision: output $1$ (true) if any interaction is detected at the Bonferroni-adjusted level, and $0$ (false) otherwise. In the final output, represent true/false as the programming language booleans.\n\nData construction details:\n- Independent benchmark: For any $d \\ge 1$, let all coordinates be independent fair Bernoulli with parameter $1/2$.\n- Parity construction in $d \\ge 3$: For each $t$, draw $\\{X_{t,1},\\dots,X_{t,d-1}\\}$ independently as fair Bernoulli with parameter $1/2$, and set $X_{t,d}$ equal to the parity (exclusive-or) of $X_{t,1},\\dots,X_{t,d-1}$. In the noisy version with noise level $q \\in [0,1]$, with probability $q$ replace $X_{t,d}$ by an independent fair Bernoulli bit; otherwise keep it as the parity. This yields pairwise independence but not mutual independence for $q = 0$ in $d \\ge 3$, and the higher-order interaction persists for small $q$.\n- Use the random seed $12345$ for reproducibility.\n\nTesting and inference specifics:\n- For each selected subset $S$ with $|S| = m$, form $W_{t,S} = \\prod_{i \\in S} Y_{t,i}$. Under mutual independence, $E[W_{t,S}] = 0$ for all $S$. Estimate the mean by the sample average $\\bar{W}_S = \\frac{1}{n}\\sum_{t=1}^n W_{t,S}$ and the variance by the sample variance of $\\{W_{t,S}\\}_{t=1}^n$. Use a standardized statistic based on $\\sqrt{n} \\, \\bar{W}_S$ and the variance estimate to obtain a two-sided $p$-value under a normal approximation.\n- Apply a Bonferroni correction over all tested subsets and orders by multiplying the smallest $p$-value by the total number of tests $K$, capping at $1$. If $K = 0$ because no subset size in $\\mathcal{M}$ is available in the given dimension $d$, then return false (no detection).\n\nTest suite and required outputs:\nImplement your program to evaluate the following six test cases. In each tuple, the parameters are $(d, n, \\text{model}, q, \\mathcal{M}, \\alpha)$ where $d$ is dimension, $n$ is sample size, $\\text{model}$ is either the string \"indep\" or \"xor\\_d\" as described above, $q$ is the noise level in $[0,1]$, $\\mathcal{M}$ is the set of orders to test, and $\\alpha$ is the family-wise error tolerance. Use the exact values as follows:\n- Case $1$: $(3,\\, 4096,\\, \\text{\"xor\\_d\"},\\, 0.0,\\, \\{3\\},\\, 0.01)$\n- Case $2$: $(3,\\, 512,\\, \\text{\"xor\\_d\"},\\, 0.0,\\, \\{3\\},\\, 0.01)$\n- Case $3$: $(3,\\, 8192,\\, \\text{\"xor\\_d\"},\\, 0.3,\\, \\{3\\},\\, 0.01)$\n- Case $4$: $(3,\\, 4096,\\, \\text{\"indep\"},\\, 0.0,\\, \\{3\\},\\, 0.01)$\n- Case $5$: $(4,\\, 4096,\\, \\text{\"xor\\_d\"},\\, 0.0,\\, \\{3,4\\},\\, 0.01)$\n- Case $6$: $(2,\\, 4096,\\, \\text{\"indep\"},\\, 0.0,\\, \\{3\\},\\, 0.01)$\n\nYour program must:\n- Generate the requested data according to the above constructions with the specified $d$, $n$, model, noise level $q$, and random seed $12345$.\n- Implement the higher-order interaction test across all subsets of sizes in $\\mathcal{M}$ with Bonferroni correction at level $\\alpha$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[true,false,true,...]\". Use the language-native boolean literals and preserve case sensitivity of those literals as printed by the programming language. The output list must contain exactly six booleans in the order of the six cases above.",
            "solution": "The problem requires the construction of discrete multidimensional random vectors that are pairwise independent but not mutually independent, and the design of a statistical test capable of detecting this higher-order dependence. The solution proceeds in two parts: first, a detailed description of the data-generating mechanisms, and second, a principled derivation of the statistical test for mutual independence.\n\n### Data Generation Models\n\nWe are tasked with generating an independent and identically distributed (i.i.d.) sample $\\{(X_{t,1}, \\dots, X_{t,d})\\}_{t=1}^n$ from a $d$-dimensional binary random vector, where each $X_{t,i} \\in \\{0, 1\\}$.\n\n1.  **Mutually Independent Model (\"indep\")**: This serves as a benchmark and the null case for our test. For each sample $t \\in \\{1, \\dots, n\\}$ and each coordinate $i \\in \\{1, \\dots, d\\}$, the random variable $X_{t,i}$ is drawn independently from a Bernoulli distribution with parameter $p=1/2$. The joint probability mass function is given by $P(X_{t,1}=x_1, \\dots, X_{t,d}=x_d) = \\prod_{i=1}^d P(X_{t,i}=x_i) = (1/2)^d$ for any configuration $(x_1, \\dots, x_d) \\in \\{0,1\\}^d$.\n\n2.  **Pairwise Independent Model (\"xor\\_d\")**: This model demonstrates a specific structure of dependence. For $d \\ge 3$, the first $d-1$ coordinates, $\\{X_{t,1}, \\dots, X_{t,d-1}\\}$, are generated as i.i.d. Bernoulli($1/2$) variables. The last coordinate, $X_{t,d}$, is deterministically set by the parity (sum modulo $2$, or exclusive-or) of the preceding $d-1$ coordinates:\n    $$X_{t,d} = \\left( \\sum_{i=1}^{d-1} X_{t,i} \\right) \\pmod 2 = X_{t,1} \\oplus X_{t,2} \\oplus \\dots \\oplus X_{t,d-1}$$\n    A noisy variant is introduced where, with probability $q \\in [0, 1]$, $X_{t,d}$ is replaced by a freshly drawn independent Bernoulli($1/2$) variable.\n\n    *   **Pairwise Independence for $d \\ge 3$ and $q=0$**: Any pair $(X_{t,i}, X_{t,j})$ with $i,j < d$ is independent by construction. Consider a pair $(X_{t,i}, X_{t,d})$ for $i < d$. $X_{t,i}$ is Bernoulli($1/2$). The variable $X_{t,d}$ is the sum modulo $2$ of $X_{t,i}$ and $S' = \\sum_{k \\ne i, k<d} X_{t,k}$. Since $d \\ge 3$, the sum for $S'$ is over at least one Bernoulli($1/2$) variable, making $S'$ itself a Bernoulli($1/2$) random variable that is independent of $X_{t,i}$. Thus, $P(X_{t,d}=1) = P(X_{t,i} \\oplus S' = 1) = P(X_{t,i}=1, S'=0) + P(X_{t,i}=0, S'=1) = (1/2)(1/2) + (1/2)(1/2) = 1/2$. So, $X_{t,d}$ is also a fair Bernoulli variable. The joint probability is $P(X_{t,i}=x_i, X_{t,d}=x_d) = P(X_{t,i}=x_i, S' = x_d \\oplus x_i) = P(X_{t,i}=x_i) P(S' = x_d \\oplus x_i) = (1/2)(1/2) = 1/4$. This equals $P(X_{t,i}=x_i) P(X_{t,d}=x_d)$, proving pairwise independence.\n\n    *   **Lack of Mutual Independence**: The variables are not mutually independent because $X_{t,d}$ is a deterministic function of the others. The joint probability $P(X_{t,1}=x_1, \\dots, X_{t,d}=x_d)$ is $(1/2)^{d-1}$ if $x_d = (\\sum_{i=1}^{d-1} x_i) \\pmod 2$ and $0$ otherwise. This is not equal to the product of marginals, $(1/2)^d$.\n\n### Higher-Order Interaction Test\n\nStandard tests for independence, like those based on pairwise correlations, would fail to detect the dependency in the \"xor\\_d\" model. We require a test sensitive to higher-order interactions.\n\n1.  **Test Statistic Formulation**:\n    First, we transform the binary variables $X_{t,i} \\in \\{0, 1\\}$ to $Y_{t,i} \\in \\{-1, +1\\}$ using the mapping $Y_{t,i} = 2X_{t,i} - 1$. If $X_{t,i}$ is a fair Bernoulli variable, then $E[Y_{t,i}] = 2 E[X_{t,i}] - 1 = 2(1/2) - 1 = 0$.\n\n    To probe for an $m$-th order interaction among a specific subset of variables indexed by $S \\subset \\{1, \\dots, d\\}$ with $|S|=m$, we form the product term $W_{t,S} = \\prod_{i \\in S} Y_{t,i}$.\n\n    The null hypothesis, $H_0$, is that the variables $\\{X_{t,1}, \\dots, X_{t,d}\\}$ are mutually independent. Under $H_0$, the variables $\\{Y_{t,1}, \\dots, Y_{t,d}\\}$ are also mutually independent, and each has an expectation of $0$. Consequently, for any subset $S$ with $|S|=m \\ge 2$, the expectation of the interaction term is:\n    $$E[W_{t,S}] = E\\left[\\prod_{i \\in S} Y_{t,i}\\right] = \\prod_{i \\in S} E[Y_{t,i}] = 0^m = 0$$\n    A non-zero expectation for $W_{t,S}$ is evidence against mutual independence.\n\n2.  **Inference from Sample Data**:\n    Given the sample $\\{W_{t,S}\\}_{t=1}^n$, we estimate the true mean $E[W_{t,S}]$ using the sample mean:\n    $$\\bar{W}_S = \\frac{1}{n} \\sum_{t=1}^n W_{t,S}$$\n    Under $H_0$, the Central Limit Theorem (CLT) states that for large $n$, the distribution of $\\sqrt{n}(\\bar{W}_S - E[W_{t,S}]) = \\sqrt{n}\\bar{W}_S$ is approximately normal with mean $0$ and variance $\\text{Var}(W_{t,S})$.\n    \n    Since $Y_{t,i} \\in \\{-1, +1\\}$, we have $W_{t,S} \\in \\{-1, +1\\}$. Therefore, $W_{t,S}^2 = 1$. Under $H_0$, the true variance is $\\text{Var}(W_{t,S}) = E[W_{t,S}^2] - (E[W_{t,S}])^2 = 1 - 0^2 = 1$. However, following the problem specification, we use the sample variance for standardization, which is more robust. The sample standard deviation is:\n    $$\\hat{\\sigma}_{W_S} = \\sqrt{\\frac{1}{n-1}\\sum_{t=1}^n (W_{t,S} - \\bar{W}_S)^2}$$\n    The standardized test statistic is:\n    $$Z_S = \\frac{\\sqrt{n} \\bar{W}_S}{\\hat{\\sigma}_{W_S}}$$\n    Under $H_0$, $Z_S$ follows an approximate standard normal distribution, $N(0,1)$. If $\\hat{\\sigma}_{W_S} = 0$, it implies all $W_{t,S}$ values are identical (either all $1$ or all $-1$). If $\\bar{W}_S \\ne 0$ in this case, it represents a maximal violation of $H_0$, and we assign a $p$-value of $0$.\n\n    The two-sided $p$-value for the test on subset $S$ is calculated as $p_S = 2 \\cdot P(Z > |Z_S|) = 2(1 - \\Phi(|Z_S|))$, where $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution.\n\n3.  **Multiple Testing Correction**:\n    The test is performed for all subsets $S$ of sizes $m \\in \\mathcal{M}$, where $\\mathcal{M}$ is a given set of orders (e.g., $\\{3, 4\\}$). Let $K$ be the total number of tests performed: $K = \\sum_{m \\in \\mathcal{M}, m \\le d} \\binom{d}{m}$. If $K=0$ (i.e., no valid subset sizes in $\\mathcal{M}$ for the given dimension $d$), we cannot perform any test and thus cannot reject $H_0$.\n\n    To control the family-wise error rate (FWER) at a significance level $\\alpha$, we employ the Bonferroni correction. This is one of the simplest and most general methods for multiple hypothesis testing. The decision rule is to reject the overall null hypothesis of mutual independence if any single test is significant at the adjusted level $\\alpha/K$. This is equivalent to checking if the minimum observed $p$-value, $p_{\\text{min}} = \\min_S p_S$, satisfies the condition:\n    $$p_{\\text{min}} < \\frac{\\alpha}{K} \\quad \\text{or equivalently} \\quad K \\cdot p_{\\text{min}} < \\alpha$$\n    The final output is a boolean value indicating whether dependence was detected (rejection of $H_0$).",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Solves the entire problem by running the specified test cases.\n    \"\"\"\n    # Use a single random number generator for reproducibility across all cases\n    rng = np.random.default_rng(12345)\n\n    test_cases = [\n        # (d, n, model, q, M_orders, alpha)\n        (3, 4096, \"xor_d\", 0.0, {3}, 0.01),\n        (3, 512, \"xor_d\", 0.0, {3}, 0.01),\n        (3, 8192, \"xor_d\", 0.3, {3}, 0.01),\n        (3, 4096, \"indep\", 0.0, {3}, 0.01),\n        (4, 4096, \"xor_d\", 0.0, {3, 4}, 0.01),\n        (2, 4096, \"indep\", 0.0, {3}, 0.01),\n    ]\n\n    results = []\n    for d, n, model, q, M_orders, alpha in test_cases:\n        decision = run_test(d, n, model, q, M_orders, alpha, rng)\n        results.append(decision)\n    \n    # Python's bool literals are True and False. str(True) -> \"True\"\n    # To match the required output format \"[true,false,...]\" we lowercase them.\n    results_str = [str(r).lower() for r in results]\n    print(f\"[{','.join(results_str)}]\")\n\ndef run_test(d, n, model, q, m_orders, alpha, rng):\n    \"\"\"\n    Generates data and performs the higher-order interaction test for one case.\n\n    Args:\n        d (int): Dimension of the random vector.\n        n (int): Sample size.\n        model (str): Data generation model, \"indep\" or \"xor_d\".\n        q (float): Noise level for the \"xor_d\" model.\n        m_orders (set): Set of interaction orders m to test.\n        alpha (float): Family-wise error rate.\n        rng (np.random.Generator): Random number generator.\n\n    Returns:\n        bool: True if dependence is detected, False otherwise.\n    \"\"\"\n    # 1. Data Generation\n    # Generate base i.i.d. fair Bernoulli variables\n    X = rng.integers(0, 2, size=(n, d), dtype=np.int8)\n\n    if model == \"xor_d\" and d >= 2: # XOR construction can apply for d>=2\n        # Calculate parity (sum modulo 2) of the first d-1 columns\n        parity = np.sum(X[:, :d-1], axis=1) % 2\n        \n        if q > 0.0:\n            # Noisy version\n            is_noisy = rng.random(n) < q\n            noise_bits = rng.integers(0, 2, size=n, dtype=np.int8)\n            X[:, d-1] = np.where(is_noisy, noise_bits, parity)\n        else:\n            # Deterministic version\n            X[:, d-1] = parity\n    \n    # 2. Test Implementation\n    # Transform from {0, 1} to {-1, +1}\n    Y = 2 * X - 1\n\n    # Get all subsets S of sizes m in m_orders\n    all_subsets = []\n    for m in m_orders:\n        if m <= d:\n            for subset in combinations(range(d), m):\n                all_subsets.append(list(subset))\n\n    K = len(all_subsets)\n    if K == 0:\n        return False\n\n    min_p_value = 1.0\n\n    for S in all_subsets:\n        # Form the interaction term W_t,S = product(Y_t,i for i in S)\n        W = np.prod(Y[:, S], axis=1)\n\n        # Calculate statistics\n        W_bar = np.mean(W)\n        \n        # Use sample standard deviation (ddof=1 for unbiased variance estimate)\n        sigma_hat_W = np.std(W, ddof=1)\n        \n        if sigma_hat_W < 1e-12: # Effectively zero\n            # If std dev is 0, W_t are all constant (+1 or -1).\n            # If W_bar is non-zero, this is maximal evidence against H0.\n            p_value = 0.0 if np.abs(W_bar) > 1e-9 else 1.0\n        else:\n            # Calculate the z-statistic\n            z_score = np.sqrt(n) * W_bar / sigma_hat_W\n            # Calculate two-sided p-value from standard normal distribution\n            p_value = 2 * norm.sf(np.abs(z_score))\n        \n        if p_value < min_p_value:\n            min_p_value = p_value\n            \n    # 3. Decision with Bonferroni correction\n    # Reject H0 if the Bonferroni-adjusted p-value is less than alpha\n    bonferroni_p = K * min_p_value\n    \n    return bonferroni_p < alpha\n\n# This block is for direct execution, not needed for the platform\n# but standard practice. The platform will call solve().\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}