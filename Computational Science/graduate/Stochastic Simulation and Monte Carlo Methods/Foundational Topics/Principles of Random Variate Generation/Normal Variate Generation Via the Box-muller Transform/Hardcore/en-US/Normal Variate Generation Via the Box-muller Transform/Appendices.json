{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering any numerical method is to implement it and verify its output against theoretical expectations. This practice  guides you through writing a Box-Muller generator and subjecting it to statistical testing. By calculating sample moments and constructing confidence intervals, you will gain hands-on experience in the essential practice of empirical validation, ensuring your implementation correctly produces variates matching the properties of a standard normal distribution.",
            "id": "3324049",
            "problem": "You are to implement and validate a generator of independent standard normal random variates using the Box–Muller transform in a reproducible program. The validation must be performed by estimating selected moments from a large sample, forming confidence intervals, and checking whether the known theoretical values for the standard normal distribution lie within those intervals. The program must implement the transformation from independent uniform random variables to independent standard normal random variables using a polar-to-Cartesian change of variables; any angles must be in radians.\n\nBegin from the following fundamental bases:\n- Let $U_1$ and $U_2$ be independent and identically distributed random variables with distribution $\\mathrm{Uniform}(0,1)$.\n- The standard normal distribution $\\mathcal{N}(0,1)$ has mean $0$, variance $1$, and even moments given by $E[X^{2m}] = (2m-1)!!$ for integer $m \\ge 1$, where $(2m-1)!!$ denotes the product of odd integers from $1$ to $(2m-1)$.\n- The Central Limit Theorem (CLT) implies that for independent and identically distributed observations $\\{Y_i\\}_{i=1}^n$ with $E[Y_i] = \\mu$ and $\\mathrm{Var}(Y_i) = \\sigma_Y^2 \\in (0,\\infty)$, the sample mean $\\bar{Y}_n$ satisfies $\\sqrt{n}\\,(\\bar{Y}_n - \\mu) \\overset{d}{\\longrightarrow} \\mathcal{N}(0,\\sigma_Y^2)$.\n- If $X_1,\\dots,X_n$ are independent and identically distributed as $\\mathcal{N}(0,1)$ and $S^2$ denotes the unbiased sample variance $S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2$, then $(n-1)S^2 \\sim \\chi^2_{n-1}$.\n\nYour program must:\n- Implement the Box–Muller transform to map independent uniforms into a sequence of independent standard normal variates. Ensure numerical robustness by avoiding taking the logarithm of $0$. The implementation must work for any positive integer sample size $n$, returning exactly $n$ normal variates. Use radians for any angular computations.\n- For each specified parameter set, generate a sample of size $n$, compute:\n  1. The sample mean $\\bar{X}_n$ and a two-sided $(1-\\alpha)$-confidence interval for the true mean $0$ using the CLT with known variance $1$.\n  2. The unbiased sample variance $S^2$ and a two-sided $(1-\\alpha)$-confidence interval for the true variance $1$ using the chi-square distribution with $n-1$ degrees of freedom.\n  3. The sample second raw moment $\\hat{m}_2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2$ and a two-sided $(1-\\alpha)$-confidence interval for the true second raw moment $E[X^2] = 1$ using the CLT with variance $\\mathrm{Var}(X^2) = E[X^4] - (E[X^2])^2$.\n  4. One selected higher raw moment $\\hat{m}_k = \\frac{1}{n}\\sum_{i=1}^n X_i^k$ for $k \\in \\{4\\}$ and a two-sided $(1-\\alpha)$-confidence interval for the true raw moment $E[X^k]$, using the CLT with variance $\\mathrm{Var}(X^k) = E[X^{2k}] - (E[X^k])^2$.\n- For items $3$ and $4$, compute any needed theoretical moments $E[X^{2m}]$ for $X \\sim \\mathcal{N}(0,1)$ using the identity $E[X^{2m}] = (2m-1)!!$.\n- For each confidence interval, report a boolean indicating whether the theoretical value lies within the computed interval.\n\nTest suite and required output:\n- Use the following three parameter sets, each a tuple $(n,\\alpha,k,\\mathrm{seed})$:\n  - Case $1$: $(n,\\alpha,k,\\mathrm{seed}) = (400000, 0.01, 4, 314159)$.\n  - Case $2$: $(n,\\alpha,k,\\mathrm{seed}) = (250000, 0.005, 4, 271828)$.\n  - Case $3$: $(n,\\alpha,k,\\mathrm{seed}) = (120000, 0.02, 4, 141421)$.\n- For each case, produce four booleans in the following order: mean-in-interval, variance-in-interval, second-raw-moment-in-interval, $k$th-raw-moment-in-interval. Aggregate the results for the three cases in order.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\mathrm{result1},\\mathrm{result2},\\mathrm{result3}]$), where each result is a boolean. The final output must therefore be a flat list of length $12$ containing the booleans for the three cases concatenated in order.\n\nNo physical units are involved. All angle computations must be in radians. The numerical results must be computed using double-precision floating-point arithmetic.",
            "solution": "The problem statement is valid. It is scientifically grounded in the theory of probability and statistics, well-posed with specific instructions and reproducible parameters, and objective in its language. The task is to implement the Box-Muller transform for generating standard normal variates and to validate the output using confidence intervals for several moments of the distribution.\n\nThe solution proceeds in two stages: first, the implementation of the random variate generator, and second, the statistical validation of the generated samples.\n\n### Part 1: The Box-Muller Transform\n\nThe Box-Muller transform is a method for generating pairs of independent standard normal random variates from a pair of independent uniform random variates. Let $U_1$ and $U_2$ be independent and identically distributed random variables from the $\\mathrm{Uniform}(0,1)$ distribution. The transformation is defined as:\n$$Z_1 = \\sqrt{-2 \\ln U_1} \\cos(2\\pi U_2)$$\n$$Z_2 = \\sqrt{-2 \\ln U_1} \\sin(2\\pi U_2)$$\nThe resulting random variables $Z_1$ and $Z_2$ are independent and identically distributed according to the standard normal distribution, $\\mathcal{N}(0,1)$.\n\nThis transformation can be understood as an inverse sampling method using a change of variables from Cartesian to polar coordinates. A two-dimensional standard normal vector $(Z_1, Z_2)$ has a squared radius $R^2 = Z_1^2 + Z_2^2$ that follows a chi-squared distribution with $2$ degrees of freedom ($\\chi^2_2$), which is equivalent to an exponential distribution with mean $2$. The angle $\\Theta = \\mathrm{atan2}(Z_2, Z_1)$ is uniformly distributed on $[0, 2\\pi)$. The Box-Muller transform synthesizes these properties. By setting $\\Theta = 2\\pi U_2$, we generate a correctly distributed angle. By setting $R^2 = -2 \\ln U_1$, we generate a variate with an exponential distribution with mean $2$. The transformation from the generated polar coordinates $(R, \\Theta)$ back to Cartesian coordinates $(Z_1, Z_2)$ yields the desired normal variates.\n\nTo generate a sample of size $n$, we will generate $\\lceil n/2 \\rceil$ pairs of uniform variates $(U_1, U_2)$, apply the transform to each pair to obtain $2 \\lceil n/2 \\rceil$ normal variates, and then select the first $n$ variates from the resulting sequence. For numerical robustness, we must ensure that the argument to the logarithm is strictly positive. Since $U_1 \\sim \\mathrm{Uniform}(0,1)$, its value can be arbitrarily close to $0$, and a computer's random number generator for the interval $[0,1)$ could theoretically produce an exact $0$. We will handle this by re-sampling any $U_1$ that is generated as $0$.\n\n### Part 2: Statistical Validation via Confidence Intervals\n\nFor each parameter set $(n, \\alpha, k, \\mathrm{seed})$, a sample $\\{X_i\\}_{i=1}^n$ is generated. We then construct four two-sided $(1-\\alpha)$-confidence intervals and check if they contain their respective theoretical values. Let $z_{\\alpha/2}$ be the upper $\\alpha/2$ quantile of the standard normal distribution, such that $P(Z > z_{\\alpha/2}) = \\alpha/2$ for $Z \\sim \\mathcal{N}(0,1)$.\n\n1.  **Confidence Interval for the Mean ($\\mu=0$)**:\n    The true mean is $\\mu=0$ and the true variance is $\\sigma^2=1$. The sample mean is $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$. By the Central Limit Theorem (CLT), for large $n$, the statistic $\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}}$ is approximately distributed as $\\mathcal{N}(0,1)$. A $(1-\\alpha)$-confidence interval for $\\mu$ is given by $\\bar{X}_n \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}$. We check if the theoretical mean $\\mu=0$ lies within $[\\bar{X}_n - z_{\\alpha/2}/\\sqrt{n}, \\bar{X}_n + z_{\\alpha/2}/\\sqrt{n}]$.\n\n2.  **Confidence Interval for the Variance ($\\sigma^2=1$)**:\n    The true variance is $\\sigma^2=1$. The unbiased sample variance is $S^2 = \\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\bar{X}_n)^2$. For a sample from a normal distribution, the quantity $\\frac{(n-1)S^2}{\\sigma^2}$ follows a chi-squared distribution with $n-1$ degrees of freedom, $\\chi^2_{n-1}$. Let $\\chi^2_{n-1, 1-\\alpha/2}$ and $\\chi^2_{n-1, \\alpha/2}$ be the lower and upper critical values from this distribution. The $(1-\\alpha)$-confidence interval for $\\sigma^2$ is $\\left[ \\frac{(n-1)S^2}{\\chi^2_{n-1, \\alpha/2}}, \\frac{(n-1)S^2}{\\chi^2_{n-1, 1-\\alpha/2}} \\right]$. We check if the theoretical variance $\\sigma^2=1$ falls into this interval.\n\n3.  **Confidence Interval for the Second Raw Moment ($E[X^2]=1$)**:\n    The second raw moment is $E[X^2]$. For $X \\sim \\mathcal{N}(0,1)$, $E[X^2] = \\mathrm{Var}(X) + (E[X])^2 = 1 + 0^2 = 1$. The sample second raw moment is $\\hat{m}_2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2$. We apply the CLT to the variables $Y_i = X_i^2$. The variance of $Y_i$ is $\\mathrm{Var}(X^2) = E[X^4] - (E[X^2])^2$. The theoretical even raw moments are given by $E[X^{2m}] = (2m-1)!!$. Thus, $E[X^2]=1!!=1$ and $E[X^4]=3!!=3$. So, $\\mathrm{Var}(X^2) = 3 - 1^2 = 2$. The $(1-\\alpha)$-confidence interval for $E[X^2]$ is $\\hat{m}_2 \\pm z_{\\alpha/2} \\sqrt{\\frac{\\mathrm{Var}(X^2)}{n}}$. We check if $1$ is within $[\\hat{m}_2 - z_{\\alpha/2}\\sqrt{2/n}, \\hat{m}_2 + z_{\\alpha/2}\\sqrt{2/n}]$.\n\n4.  **Confidence Interval for the $k$-th Raw Moment ($E[X^k]$ for $k=4$)**:\n    The problem specifies testing the $k=4$ raw moment. The theoretical value is $E[X^4] = 3!! = 3$. The sample $4$-th raw moment is $\\hat{m}_4 = \\frac{1}{n}\\sum_{i=1}^n X_i^4$. We apply the CLT to the variables $W_i = X_i^4$. The variance of $W_i$ is $\\mathrm{Var}(X^4) = E[X^8] - (E[X^4])^2$. Using the moment formula, $E[X^8] = 7!! = 105$. The variance is $\\mathrm{Var}(X^4) = 105 - 3^2 = 96$. The $(1-\\alpha)$-confidence interval for $E[X^4]$ is $\\hat{m}_4 \\pm z_{\\alpha/2} \\sqrt{\\frac{\\mathrm{Var}(X^4)}{n}}$. We check if the theoretical value $3$ is within $[\\hat{m}_4 - z_{\\alpha/2}\\sqrt{96/n}, \\hat{m}_4 + z_{\\alpha/2}\\sqrt{96/n}]$.\n\nThe program will execute these steps for each provided test case, using the specified random seed for reproducibility, and report a boolean for each of the four checks.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Implements and validates the Box-Muller transform for generating standard normal variates.\n    \"\"\"\n\n    # The test suite as specified in the problem statement.\n    test_cases = [\n        # (n, alpha, k, seed)\n        (400000, 0.01, 4, 314159),\n        (250000, 0.005, 4, 271828),\n        (120000, 0.02, 4, 141421),\n    ]\n\n    results = []\n\n    def box_muller_generate(n, rng):\n        \"\"\"\n        Generates n standard normal variates using the Box-Muller transform.\n        \n        Args:\n            n (int): The number of variates to generate.\n            rng (np.random.Generator): The random number generator instance.\n        \n        Returns:\n            np.ndarray: An array of n standard normal variates.\n        \"\"\"\n        if n = 0:\n            return np.array([])\n        \n        # We generate pairs of variates, so we need ceil(n/2) pairs.\n        num_pairs = (n + 1) // 2\n        \n        # Generate uniform random numbers in [0, 1).\n        u1 = rng.random(num_pairs)\n        \n        # Ensure u1 is not exactly 0 to avoid log(0).\n        # Although extremely unlikely, this ensures robustness.\n        while np.any(u1 == 0):\n            zero_indices = (u1 == 0)\n            u1[zero_indices] = rng.random(np.sum(zero_indices))\n            \n        u2 = rng.random(num_pairs)\n        \n        # Apply the Box-Muller transformation.\n        r = np.sqrt(-2.0 * np.log(u1))\n        theta = 2.0 * np.pi * u2\n        \n        z1 = r * np.cos(theta)\n        z2 = r * np.sin(theta)\n        \n        # Combine the pairs and truncate to the desired length n.\n        z = np.stack((z1, z2), axis=-1).flatten()\n        return z[:n]\n\n    for n, alpha, k, seed in test_cases:\n        # Initialize the random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        # Generate the sample of standard normal variates.\n        X = box_muller_generate(n, rng)\n        \n        # --- 1. Confidence Interval for the Mean (mu=0) ---\n        mu_true = 0.0\n        sigma_true = 1.0\n        x_bar = np.mean(X)\n        z_alpha_2 = stats.norm.ppf(1.0 - alpha / 2.0)\n        \n        ci_mean_half_width = z_alpha_2 * (sigma_true / np.sqrt(n))\n        ci_mean_lower = x_bar - ci_mean_half_width\n        ci_mean_upper = x_bar + ci_mean_half_width\n        mean_in_ci = (ci_mean_lower = mu_true = ci_mean_upper)\n        \n        # --- 2. Confidence Interval for the Variance (sigma^2=1) ---\n        var_true = 1.0\n        s_squared = np.var(X, ddof=1)\n        df = n - 1\n        \n        chi2_lower_crit = stats.chi2.ppf(alpha / 2.0, df)\n        chi2_upper_crit = stats.chi2.ppf(1.0 - alpha / 2.0, df)\n        \n        ci_var_lower = df * s_squared / chi2_upper_crit\n        ci_var_upper = df * s_squared / chi2_lower_crit\n        var_in_ci = (ci_var_lower = var_true = ci_var_upper)\n\n        # --- 3. Confidence Interval for the Second Raw Moment (E[X^2]=1) ---\n        m2_true = 1.0\n        # Var(X^2) = E[X^4] - (E[X^2])^2 = 3 - 1^2 = 2\n        var_x2 = 2.0\n        m2_hat = np.mean(X**2)\n        \n        ci_m2_half_width = z_alpha_2 * np.sqrt(var_x2 / n)\n        ci_m2_lower = m2_hat - ci_m2_half_width\n        ci_m2_upper = m2_hat + ci_m2_half_width\n        m2_in_ci = (ci_m2_lower = m2_true = ci_m2_upper)\n        \n        # --- 4. Confidence Interval for the k-th Raw Moment (k=4) ---\n        # For k=4, E[X^4] = 3.\n        # Var(X^4) = E[X^8] - (E[X^4])^2 = 105 - 3^2 = 96.\n        mk_true = 3.0\n        var_xk = 96.0\n        mk_hat = np.mean(X**k)\n        \n        ci_mk_half_width = z_alpha_2 * np.sqrt(var_xk / n)\n        ci_mk_lower = mk_hat - ci_mk_half_width\n        ci_mk_upper = mk_hat + ci_mk_half_width\n        mk_in_ci = (ci_mk_lower = mk_true = mk_upper)\n        \n        # Append the four boolean results for the current case.\n        results.extend([mean_in_ci, var_in_ci, m2_in_ci, mk_in_ci])\n\n    # Print the final aggregated results in the specified format.\n    print(f\"[{','.join(str(b).lower() for b in results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While checking moments provides a good first-pass validation, a truly robust generator must produce variates that are not only marginally correct but also jointly correct—in this case, independent. This exercise  delves deeper into validation by applying powerful goodness-of-fit tests and, crucially, by examining the joint structure of the output. By testing a deliberately flawed generator, you will see firsthand why marginal tests alone are insufficient and learn how to check for the all-important property of independence.",
            "id": "3323985",
            "problem": "Consider the generation of independent standard normal random variables via a transformation of independent uniform random variables on the interval $(0,1)$. The fundamental base to use is the following: when $(Z_1,Z_2)$ is a pair of independent standard normal random variables, the transformation to polar coordinates $(R,\\Theta)$ defined by $R = \\sqrt{Z_1^2 + Z_2^2}$ and $\\Theta = \\operatorname{atan2}(Z_2,Z_1)$ yields a radius $R$ with Rayleigh distribution of scale $1$ and an angle $\\Theta$ that is uniform on $[0,2\\pi)$ in radians, and $R$ and $\\Theta$ are independent. Conversely, the correct transformation of independent uniform random variables on $(0,1)$ can produce such a pair $(Z_1,Z_2)$. The Kolmogorov–Smirnov test (which compares empirical distribution functions to a specified continuous cumulative distribution function) and the Anderson–Darling test (which weights tail deviations more heavily) can be applied to one-dimensional marginals to assess whether each of $Z_1$ and $Z_2$ is standard normal. However, these tests do not directly assess the two-dimensional joint distribution or independence.\n\nYour task is to write a complete program that:\n\n1. Implements a generator of pairs $(Z_1,Z_2)$ intended to be independent standard normal random variables via the polar-coordinate transformation of independent uniform random variables on $(0,1)$ (commonly known as the Box–Muller transform), avoiding degenerate inputs by ensuring that all uniform draws are strictly positive.\n\n2. Applies one-dimensional goodness-of-fit tests to each marginal $Z_1$ and $Z_2$:\n   - Perform the Anderson–Darling test for normality at significance level $0.05$ (i.e., accept if the test statistic is strictly less than the tabulated critical value corresponding to $5$ percent).\n   - Perform the Kolmogorov–Smirnov test against the standard normal distribution with mean $0$ and variance $1$ at significance level $0.05$ (i.e., accept if the $p$-value is at least $0.05$).\n   Define the boolean `marginals_ok` to be true if and only if all four tests (Anderson–Darling and Kolmogorov–Smirnov on each of $Z_1$ and $Z_2$) accept.\n\n3. Addresses the limitation that one-dimensional tests do not assess the two-dimensional joint distribution by adding joint-structure checks derived from the fundamental base:\n   - Compute angles $\\Theta \\in [0,2\\pi)$ from $(Z_1,Z_2)$ using the two-argument arctangent and test their uniformity on $[0,2\\pi)$ in radians via the Kolmogorov–Smirnov test at significance level $0.05$.\n   - Compute radii $R = \\sqrt{Z_1^2 + Z_2^2}$ and test their Rayleigh distribution with scale $1$ via the Kolmogorov–Smirnov test at significance level $0.05$.\n   - Compute the sample Pearson correlation coefficient between $Z_1$ and $Z_2$ and accept the independence proxy if the absolute correlation is strictly less than $0.05$ and the two-sided $p$-value from the Pearson correlation test is at least $0.05$.\n   Define the boolean `joint_ok` to be true if and only if all three joint checks accept.\n\n4. Uses the following test suite of parameter values, each of which specifies a seed for the random number generator, a sample size, and a method:\n   - Test case 1 (happy path): seed $12345$, sample size $20000$, method “box\\_muller” with independent uniform inputs.\n   - Test case 2 (moderate size): seed $2024$, sample size $500$, method “box\\_muller” with independent uniform inputs.\n   - Test case 3 (edge case demonstrating limitation): seed $42$, sample size $8000$, method “fake\\_pair\\_equal” that constructs $Z_2 = Z_1$ from an otherwise valid “box\\_muller” generator, so the marginals remain standard normal but the joint is degenerate.\n   - Test case 4 (nonuniform input distortion): seed $777$, sample size $10000$, method “box\\_muller” with $U_1$ drawn from a nonuniform law $U_1 = V^2$ where $V \\sim \\operatorname{Uniform}(0,1)$ while $U_2 \\sim \\operatorname{Uniform}(0,1)$, to intentionally break the radial law without changing the angle law.\n\nYour program must produce, for each test case, a list containing:\n- The boolean `marginals_ok`,\n- The boolean `joint_ok`,\n- The floating-point value of the absolute sample Pearson correlation between $Z_1$ and $Z_2$.\n\nThe final output of the program must be a single line containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test result is itself a comma-separated list enclosed in square brackets. For example: “[ [true,false,0.01234], [true,true,0.00321], ... ]”. Angles must be handled in radians throughout. No physical units are involved in this problem. All numerical answers must be returned exactly in the specified final output format. The acceptance criteria for all tests are at significance level $0.05$ as stated above.",
            "solution": "The problem requires the implementation and statistical validation of a generator for independent standard normal random variables, $\\mathcal{N}(0,1)$, based on the Box-Muller transform. The validation process involves a comprehensive suite of tests assessing both the marginal distributions of the generated variates and their joint structure.\n\n### 1. The Box-Muller Transform: Principle and Implementation\n\nThe Box-Muller transform is a foundational method in Monte Carlo simulation for generating standard normal variates from uniform ones. It is derived from the observation that if $(Z_1, Z_2)$ are two independent standard normal random variables, their representation in polar coordinates $(R, \\Theta)$, where $Z_1 = R \\cos \\Theta$ and $Z_2 = R \\sin \\Theta$, results in a radius $R$ and an angle $\\Theta$ that are independent. Specifically, the squared radius $R^2 = Z_1^2 + Z_2^2$ follows an exponential distribution with rate $\\lambda = 1/2$, and the angle $\\Theta = \\operatorname{atan2}(Z_2, Z_1)$ follows a uniform distribution on $[0, 2\\pi)$.\n\nThe transform leverages this property in reverse. By generating $R$ and $\\Theta$ from their respective distributions and then converting back to Cartesian coordinates, we can produce the desired pair $(Z_1, Z_2)$.\nThe generation proceeds as follows:\n1.  Generate two independent random variables $U_1, U_2$ from the uniform distribution on $(0,1)$, i.e., $U_1, U_2 \\sim \\operatorname{Uniform}(0,1)$.\n2.  To generate the squared radius $R^2$ from an exponential distribution with CDF $F(x) = 1 - e^{-x/2}$, we use the inverse transform sampling method. Let $U_1 = F(R^2) = 1 - e^{-R^2/2}$. Solving for $R^2$ gives $R^2 = -2 \\ln(1 - U_1)$. Since $1-U_1$ is also distributed as $\\operatorname{Uniform}(0,1)$, we can simplify this to $R^2 = -2 \\ln U_1$. This gives the radius as $R = \\sqrt{-2 \\ln U_1}$.\n3.  To generate the angle $\\Theta$ uniformly from $[0, 2\\pi)$, we simply scale $U_2$: $\\Theta = 2\\pi U_2$.\n4.  Finally, we transform back to Cartesian coordinates to obtain the two independent standard normal variates:\n    $$Z_1 = R \\cos \\Theta = \\sqrt{-2 \\ln U_1} \\cos(2\\pi U_2)$$\n    $$Z_2 = R \\sin \\Theta = \\sqrt{-2 \\ln U_1} \\sin(2\\pi U_2)$$\nA critical implementation detail is that $U_1$ must be strictly positive to ensure that $\\ln U_1$ is well-defined. Our implementation will replace any occurrence of $U_1=0$ with a very small positive number, machine epsilon, to prevent numerical errors.\n\n### 2. Statistical Validation Framework\n\nA robust validation requires examining both the individual (marginal) distributions of $Z_1$ and $Z_2$ and their joint behavior.\n\n#### 2.1. Marginal Distribution Tests\nWe must verify that both $Z_1$ and $Z_2$ individually follow a standard normal distribution. This is done using two standard goodness-of-fit tests at a significance level of $\\alpha=0.05$.\n\n1.  **Anderson-Darling (AD) Test**: This test is powerful for detecting discrepancies from normality. The test statistic is computed and compared against a critical value. The null hypothesis (that the data is normal) is accepted if the test statistic is strictly less than the critical value for the $5\\%$ significance level.\n2.  **Kolmogorov-Smirnov (KS) Test**: This test compares the empirical cumulative distribution function (ECDF) of the sample with the theoretical CDF of the standard normal distribution. The null hypothesis is accepted if the test's $p$-value is greater than or equal to the significance level, i.e., $p \\ge 0.05$.\n\nThe boolean variable `marginals_ok` is true if and only if all four tests (AD and KS for $Z_1$; AD and KS for $Z_2$) pass.\n\n#### 2.2. Joint Structure Tests\nMarginal normality is a necessary but not sufficient condition for a pair of random variables to be *independent* and standard normal. The joint tests are designed to probe the two-dimensional structure.\n\n1.  **Angle Uniformity Test**: We reverse-transform the generated pairs $(Z_1, Z_2)$ back to polar coordinates to obtain the angles $\\Theta = \\operatorname{atan2}(Z_2, Z_1)$. According to the underlying theory, these angles should be uniformly distributed on an interval of length $2\\pi$, such as $[0, 2\\pi)$. We test this hypothesis using the KS test against a uniform distribution on $[0, 2\\pi)$. The test passes if the $p$-value is at least $0.05$.\n2.  **Radius Distribution Test**: Similarly, we compute the radii $R = \\sqrt{Z_1^2 + Z_2^2}$. These should follow a Rayleigh distribution with scale parameter $\\sigma=1$. This is tested using the KS test, which passes if the $p$-value is at least $0.05$.\n3.  **Independence Test (via Correlation)**: True independence is difficult to test. A common and necessary (though not sufficient) condition for the independence of normal variables is that they are uncorrelated. We compute the sample Pearson correlation coefficient, $\\rho$, between $Z_1$ and $Z_2$. We use a two-pronged check: the absolute value of the coefficient must be small ($|\\rho|  0.05$), and the $p$-value from the test for non-zero correlation must be insignificant ($p \\ge 0.05$).\n\nThe boolean variable `joint_ok` is true if and only if these three joint checks all pass.\n\n### 3. Analysis of Test Cases\n\nThe provided test suite is designed to demonstrate the efficacy and limitations of the validation framework.\n\n-   **Case 1  2 (Standard Box-Muller)**: These cases use the correct \"box\\_muller\" method with a large ($N=20000$) and moderate ($N=500$) sample size. We expect both `marginals_ok` and `joint_ok` to be true, as the generator is implemented correctly. The seed ensures a deterministic outcome.\n-   **Case 3 (Degenerate Joint, $Z_2=Z_1$)**: This case is constructed to fool the marginal tests. An initial set of standard normal variates $Z_1$ is generated correctly, but then $Z_2$ is set equal to $Z_1$. The marginal distribution of both $Z_1$ and $Z_2$ is, by construction, standard normal. Therefore, we expect `marginals_ok` to be true. However, the joint structure is completely degenerate. The correlation is $\\rho=1$, the radii $R = \\sqrt{2Z_1^2} = \\sqrt{2}|Z_1|$ are not Rayleigh-distributed, and the angles $\\Theta = \\operatorname{atan2}(Z_1, Z_1)$ are concentrated at $\\pi/4$ and $-3\\pi/4$. All joint tests should fail, leading to `joint_ok` being false.\n-   **Case 4 (Distorted Input)**: Here, the input uniform variable $U_1$ is generated from a non-uniform law, $U_1 = V^2$ where $V \\sim \\operatorname{Uniform}(0,1)$. The PDF of this $U_1$ is $f(u) = 1/(2\\sqrt{u})$, which is skewed towards $0$. This directly breaks the distribution of the radius $R=\\sqrt{-2\\ln U_1}$. Consequently, the radius test in the joint checks will fail. This distortion in $R$ will also make the marginal distributions of $Z_1 = R\\cos\\Theta$ and $Z_2 = R\\sin\\Theta$ non-normal, so the marginal tests should also fail. The angle $\\Theta=2\\pi U_2$ is unaffected as $U_2$ is still uniform, so the angle test may pass. The correlation structure is not obviously broken in a linear sense, but the fundamental distributional properties are incorrect. We expect both `marginals_ok` and `joint_ok` to be false.\n\nThe implementation will follow these principles, using `numpy` for numerical operations and `scipy.stats` for the statistical tests.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Implements and validates a standard normal random variate generator\n    based on the Box-Muller transform, using a suite of statistical tests.\n    \"\"\"\n    test_cases = [\n        {\"seed\": 12345, \"sample_size\": 20000, \"method\": \"box_muller\"},\n        {\"seed\": 2024, \"sample_size\": 500, \"method\": \"box_muller\"},\n        {\"seed\": 42, \"sample_size\": 8000, \"method\": \"fake_pair_equal\"},\n        {\"seed\": 777, \"sample_size\": 10000, \"method\": \"box_muller_distorted\"},\n    ]\n\n    results = []\n    \n    # Significance level for all tests\n    alpha = 0.05\n    \n    # Critical value for Anderson-Darling test at 5% significance\n    # scipy.stats.anderson returns critical values for [15%, 10%, 5%, 2.5%, 1%]\n    ad_crit_val_idx = 2\n\n    for case in test_cases:\n        seed = case[\"seed\"]\n        n = case[\"sample_size\"]\n        method = case[\"method\"]\n        \n        rng = np.random.default_rng(seed)\n\n        z1, z2 = None, None\n\n        if method == \"box_muller\":\n            u1 = rng.uniform(size=n)\n            # Ensure u1 is strictly positive for log calculation\n            u1[u1 == 0.0] = np.finfo(float).eps\n            u2 = rng.uniform(size=n)\n            \n            r = np.sqrt(-2 * np.log(u1))\n            theta_gen = 2 * np.pi * u2\n            \n            z1 = r * np.cos(theta_gen)\n            z2 = r * np.sin(theta_gen)\n\n        elif method == \"fake_pair_equal\":\n            # Generate one set of normal variates and set the other equal\n            # Need n/2 pairs of uniforms to make n variates\n            num_pairs = (n + 1) // 2 \n            u1 = rng.uniform(size=num_pairs)\n            u1[u1 == 0.0] = np.finfo(float).eps\n            u2 = rng.uniform(size=num_pairs)\n            \n            r = np.sqrt(-2 * np.log(u1))\n            theta_gen = 2 * np.pi * u2\n            \n            temp_z1 = r * np.cos(theta_gen)\n            temp_z2 = r * np.sin(theta_gen)\n            \n            z_full = np.concatenate([temp_z1, temp_z2])\n            z1 = z_full[:n]\n            z2 = np.copy(z1)\n\n        elif method == \"box_muller_distorted\":\n            # U1 is from a non-uniform law U1 = V^2\n            v = rng.uniform(size=n)\n            u1 = v**2\n            # Ensure u1 is strictly positive for log calculation\n            u1[u1 == 0.0] = np.finfo(float).eps\n            u2 = rng.uniform(size=n)\n\n            r = np.sqrt(-2 * np.log(u1))\n            theta_gen = 2 * np.pi * u2\n            \n            z1 = r * np.cos(theta_gen)\n            z2 = r * np.sin(theta_gen)\n\n        # 2. Perform marginal tests\n        ad_z1 = stats.anderson(z1, dist='norm')\n        ks_z1 = stats.kstest(z1, 'norm')\n        \n        ad_z2 = stats.anderson(z2, dist='norm')\n        ks_z2 = stats.kstest(z2, 'norm')\n\n        ad1_ok = ad_z1.statistic  ad_z1.critical_values[ad_crit_val_idx]\n        ks1_ok = ks_z1.pvalue >= alpha\n        ad2_ok = ad_z2.statistic  ad_z2.critical_values[ad_crit_val_idx]\n        ks2_ok = ks_z2.pvalue >= alpha\n        \n        marginals_ok = all([ad1_ok, ks1_ok, ad2_ok, ks2_ok])\n\n        # 3. Perform joint structure tests\n        # Angle uniformity\n        thetas = (np.arctan2(z2, z1) + 2 * np.pi) % (2 * np.pi)\n        # Normalize to [0,1) for testing against uniform(0,1)\n        thetas_norm = thetas / (2 * np.pi)\n        ks_theta = stats.kstest(thetas_norm, 'uniform')\n        theta_ok = ks_theta.pvalue >= alpha\n        \n        # Radius Rayleigh distribution\n        radii = np.sqrt(z1**2 + z2**2)\n        ks_radii = stats.kstest(radii, 'rayleigh') # scale=1 is default\n        radii_ok = ks_radii.pvalue >= alpha\n        \n        # Independence via Pearson correlation\n        corr_res = stats.pearsonr(z1, z2)\n        corr_coeff, corr_pvalue = corr_res.statistic, corr_res.pvalue\n        corr_ok = (abs(corr_coeff)  0.05) and (corr_pvalue >= alpha)\n        \n        joint_ok = all([theta_ok, radii_ok, corr_ok])\n\n        # Store results\n        results.append([marginals_ok, joint_ok, abs(corr_coeff)])\n\n    # Format the final output string\n    result_strings = []\n    for res in results:\n        # Convert booleans to lowercase 'true'/'false'\n        formatted_res = [str(v).lower() for v in res[:-1]] + [f\"{res[-1]:.10f}\"]\n        result_strings.append(f\"[{','.join(formatted_res)}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The standard Box-Muller transform relies on trigonometric functions, which can be computationally expensive. The Marsaglia polar method is a clever and widely-used alternative that avoids these calculations through rejection sampling. This analytical problem  challenges you to derive the efficiency of this method from first principles, calculating its acceptance probability and the expected number of attempts required to generate a valid pair of normal variates.",
            "id": "3324024",
            "problem": "Consider the Marsaglia polar method, a rejection-based variant of the Box–Muller transform for generating standard normal variates. In each iteration, two Independent and Identically Distributed (i.i.d.) random variables $U$ and $V$ are drawn uniformly from the interval $[-1,1]$. Define $S = U^{2} + V^{2}$. The iteration accepts the pair $(U,V)$ if $0  S  1$ and rejects otherwise; upon acceptance, the pair is transformed to a pair of independent standard normal variables via a radial mapping. Assume successive iterations are independent and that the transformation applied upon acceptance is measurable and well-defined for $0  S  1$.\n\nUsing only foundational measure-theoretic and probability concepts—namely, the definition of a uniform distribution on a bounded set, the notion of independence, and series identities for expectations—perform the following:\n\n1. Derive the acceptance probability $p$ by integrating the joint Probability Density Function (PDF) of $(U,V)$ over the acceptance region.\n2. Model the iteration count $N$ until the first accepted pair as a discrete random variable supported on the positive integers, and derive closed-form expressions for the expected number of iterations $\\mathbb{E}[N]$ and the variance $\\mathrm{Var}(N)$ from first principles.\n3. Express your final answers as exact analytic expressions in terms of $\\pi$. Do not round.\n\nYour final answer must be a single row matrix containing, in order, the acceptance probability $p$, the expected number of iterations $\\mathbb{E}[N]$, and the variance $\\mathrm{Var}(N)$. No numerical approximation or rounding is permitted.",
            "solution": "The problem is valid as it constitutes a well-posed, scientifically grounded question within the domain of stochastic simulation and probability theory. It is free from ambiguity, contradiction, and factual error. We proceed with a formal derivation.\n\n**Part 1: Derivation of the Acceptance Probability $p$**\n\nThe problem specifies two independent and identically distributed (i.i.d.) random variables, $U$ and $V$, drawn from a uniform distribution on the interval $[-1, 1]$. The Probability Density Function (PDF) for a random variable $X$ uniformly distributed on an interval $[a, b]$ is given by $f_X(x) = \\frac{1}{b-a}$ for $x \\in [a, b]$, and $f_X(x)=0$ otherwise. For $U$ and $V$, with $a = -1$ and $b = 1$, the individual PDFs are:\n$$f_U(u) = \\frac{1}{1 - (-1)} = \\frac{1}{2}, \\quad \\text{for } u \\in [-1, 1]$$\n$$f_V(v) = \\frac{1}{1 - (-1)} = \\frac{1}{2}, \\quad \\text{for } v \\in [-1, 1]$$\nDue to independence, the joint PDF of the pair $(U, V)$ is the product of their marginal PDFs:\n$$f_{U,V}(u,v) = f_U(u) f_V(v) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$$\nThis joint PDF is constant over the square region $\\mathcal{S} = \\{(u, v) \\in \\mathbb{R}^2 \\mid -1 \\le u \\le 1, -1 \\le v \\le 1\\}$. The area of this square is $(2) \\times (2) = 4$.\n\nAn iteration is accepted if $0  S  1$, where $S = U^2 + V^2$. This corresponds to the event that the point $(U, V)$ falls within the region $\\mathcal{A} = \\{(u, v) \\in \\mathbb{R}^2 \\mid 0  u^2 + v^2  1\\}$. This region is the interior of the unit circle centered at the origin, with the origin point $(0,0)$ excluded. The entire acceptance region $\\mathcal{A}$ is contained within the square $\\mathcal{S}$.\n\nThe acceptance probability, denoted by $p$, is the probability that a generated pair $(U, V)$ falls into $\\mathcal{A}$. This is found by integrating the joint PDF over the acceptance region:\n$$p = \\mathbb{P}((U, V) \\in \\mathcal{A}) = \\iint_{\\mathcal{A}} f_{U,V}(u,v) \\,du\\,dv$$\nSince the point $(0,0)$ is a set of measure zero in $\\mathbb{R}^2$, the condition $S  0$ does not affect the value of the integral for a continuous joint distribution. We can therefore integrate over the open unit disk $\\mathcal{D} = \\{(u, v) \\in \\mathbb{R}^2 \\mid u^2+v^2  1\\}$.\n$$p = \\iint_{u^2 + v^2  1} \\frac{1}{4} \\,du\\,dv$$\nBecause the integrand is a constant, the value of the integral is the constant multiplied by the area of the region of integration. The region is a disk of radius $r=1$, and its area is $\\pi r^2 = \\pi$.\n$$p = \\frac{1}{4} \\cdot (\\text{Area of unit disk}) = \\frac{1}{4} \\pi = \\frac{\\pi}{4}$$\n\n**Part 2: Derivation of $\\mathbb{E}[N]$ and $\\mathrm{Var}(N)$**\n\nThe random variable $N$ is the number of iterations required to obtain the first acceptance. Each iteration is an independent Bernoulli trial with a success probability of $p = \\frac{\\pi}{4}$. Thus, $N$ follows a geometric distribution with parameter $p$. Its probability mass function (PMF) is:\n$$\\mathbb{P}(N=k) = (1-p)^{k-1}p, \\quad k \\in \\{1, 2, 3, \\ldots\\}$$\nWe derive the expectation and variance from first principles as required. Let $q = 1-p$.\n\nThe expected value $\\mathbb{E}[N]$ is given by:\n$$\\mathbb{E}[N] = \\sum_{k=1}^{\\infty} k \\cdot \\mathbb{P}(N=k) = \\sum_{k=1}^{\\infty} k q^{k-1} p = p \\sum_{k=1}^{\\infty} k q^{k-1}$$\nFor $|q|1$, the geometric series is $\\sum_{k=0}^{\\infty} q^k = \\frac{1}{1-q}$. Differentiating with respect to $q$ yields:\n$$\\frac{d}{dq} \\left( \\sum_{k=0}^{\\infty} q^k \\right) = \\sum_{k=1}^{\\infty} k q^{k-1} = \\frac{d}{dq} \\left( \\frac{1}{1-q} \\right) = \\frac{1}{(1-q)^2}$$\nSubstituting this result into the expression for the expectation:\n$$\\mathbb{E}[N] = p \\cdot \\frac{1}{(1-q)^2} = p \\cdot \\frac{1}{(1-(1-p))^2} = p \\cdot \\frac{1}{p^2} = \\frac{1}{p}$$\nWith $p = \\frac{\\pi}{4}$, the expected number of iterations is:\n$$\\mathbb{E}[N] = \\frac{1}{\\pi/4} = \\frac{4}{\\pi}$$\n\nThe variance $\\mathrm{Var}(N)$ is given by $\\mathrm{Var}(N) = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2$. We first compute $\\mathbb{E}[N^2]$.\n$$\\mathbb{E}[N^2] = \\sum_{k=1}^{\\infty} k^2 \\mathbb{P}(N=k) = p \\sum_{k=1}^{\\infty} k^2 q^{k-1}$$\nTo find the sum, we start with the identity $\\sum_{k=1}^{\\infty} k q^{k-1} = \\frac{1}{(1-q)^2}$. Multiplying by $q$ gives $\\sum_{k=1}^{\\infty} k q^k = \\frac{q}{(1-q)^2}$. Differentiating this with respect to $q$:\n$$\\frac{d}{dq} \\left( \\sum_{k=1}^{\\infty} k q^k \\right) = \\sum_{k=1}^{\\infty} k^2 q^{k-1} = \\frac{d}{dq} \\left( \\frac{q}{(1-q)^2} \\right)$$\nUsing the quotient rule for differentiation:\n$$\\frac{(1)(1-q)^2 - q(2(1-q)(-1))}{(1-q)^4} = \\frac{(1-q)^2 + 2q(1-q)}{(1-q)^4} = \\frac{(1-q) + 2q}{(1-q)^3} = \\frac{1+q}{(1-q)^3}$$\nSubstituting this result into the expression for $\\mathbb{E}[N^2]$:\n$$\\mathbb{E}[N^2] = p \\cdot \\frac{1+q}{(1-q)^3} = p \\cdot \\frac{1+(1-p)}{(1-(1-p))^3} = p \\cdot \\frac{2-p}{p^3} = \\frac{2-p}{p^2}$$\nNow, we can compute the variance:\n$$\\mathrm{Var}(N) = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2 = \\frac{2-p}{p^2} - \\left(\\frac{1}{p}\\right)^2 = \\frac{2-p-1}{p^2} = \\frac{1-p}{p^2}$$\nSubstituting $p = \\frac{\\pi}{4}$:\n$$\\mathrm{Var}(N) = \\frac{1 - \\frac{\\pi}{4}}{(\\frac{\\pi}{4})^2} = \\frac{\\frac{4-\\pi}{4}}{\\frac{\\pi^2}{16}} = \\frac{4-\\pi}{4} \\cdot \\frac{16}{\\pi^2} = \\frac{4(4-\\pi)}{\\pi^2} = \\frac{16 - 4\\pi}{\\pi^2}$$\n\n**Part 3: Final Answer**\n\nThe requested quantities are the acceptance probability $p$, the expected number of iterations $\\mathbb{E}[N]$, and the variance of the number of iterations $\\mathrm{Var}(N)$.\n$p = \\frac{\\pi}{4}$\n$\\mathbb{E}[N] = \\frac{4}{\\pi}$\n$\\mathrm{Var}(N) = \\frac{16 - 4\\pi}{\\pi^2}$\nThese are presented as a row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\pi}{4}  \\frac{4}{\\pi}  \\frac{16 - 4\\pi}{\\pi^2}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}