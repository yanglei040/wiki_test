{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of any rejection sampling algorithm is determined by its acceptance probability. This exercise delves into the core performance characteristic of the Marsaglia polar method by modeling the generation process as a sequence of Bernoulli trials. By deriving the expected number of proposals and the variance of this count from first principles, you will solidify your understanding of the direct link between the method's elegant geometry and its real-world stochastic efficiency .",
            "id": "3324394",
            "problem": "Consider the acceptance stage of the Marsaglia polar method for generating independent standard normal variates. In each proposal, a pair $(V_1,V_2)$ is drawn uniformly over the square $[-1,1] \\times [-1,1]$, and the proposal is accepted if $S = V_1^2 + V_2^2 < 1$. Proposals are repeated independently until the first acceptance occurs. Let $N$ be the total number of proposal pairs required to obtain the first accepted pair.\n\nStarting from the definitions of the continuous uniform distribution and planar (Lebesgue) measure, derive the acceptance probability $p$ for a single proposal. Then, using the definition of the geometric distribution as the count of independent and identically distributed Bernoulli trials until the first success, and without invoking precomputed mean or variance formulas, derive closed-form analytic expressions for the expected value $\\mathbb{E}[N]$ and the variance $\\operatorname{Var}(N)$ in terms of $\\pi$. Your derivations should proceed from series identities that you justify from first principles.\n\nProvide your final answers for $\\mathbb{E}[N]$ and $\\operatorname{Var}(N)$ as exact analytical expressions. No rounding is required, and no units are involved.",
            "solution": "The user wants to solve the problem by starting with the problem statement validation.\n\n### Step 1: Extract Givens\n- A pair of random variables $(V_1, V_2)$ is drawn uniformly over the square $[-1, 1] \\times [-1, 1]$.\n- A proposal is accepted if $S = V_1^2 + V_2^2 < 1$.\n- Proposals are independent and repeated until the first acceptance.\n- The random variable $N$ is the total number of proposal pairs required to obtain the first accepted pair.\n- The derivation of the acceptance probability $p$ must start from the definitions of the continuous uniform distribution and planar measure.\n- The derivations of the expected value $\\mathbb{E}[N]$ and the variance $\\operatorname{Var}(N)$ must be from first principles, using justified series identities, and not by invoking precomputed formulas for the mean or variance of the geometric distribution.\n- The final answers for $\\mathbb{E}[N]$ and $\\operatorname{Var}(N)$ must be exact analytical expressions in terms of $\\pi$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is grounded in the well-established Marsaglia polar method, a standard algorithm in the field of computational statistics and Monte Carlo methods. All premises are factually correct.\n- **Well-Posed:** The problem is clearly stated, and all quantities are well-defined. The question asks for a derivation of specific statistical properties of a random variable defined by a clear probabilistic process. A unique, stable, and meaningful solution exists.\n- **Objective:** The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n- **Completeness and Consistency:** The problem is self-contained. It provides all necessary information (the sampling domain, the acceptance criterion, the definition of the random variable $N$) to derive the requested quantities. There are no contradictions.\n- **Other Flaws:** The problem does not exhibit any other flaws such as being unrealistic, non-formalizable, ill-posed, tautological, or unverifiable. The requirement to derive results from first principles is a standard pedagogical tool to test fundamental understanding.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will be provided as requested.\n\n### Solution Derivation\n\nThe solution is developed in two parts as required: first, the derivation of the acceptance probability $p$, and second, the derivation of the expectation $\\mathbb{E}[N]$ and variance $\\operatorname{Var}(N)$ from first principles.\n\n**Part 1: Derivation of the Acceptance Probability $p$**\n\nThe pair of random variables $(V_1, V_2)$ is drawn from a continuous uniform distribution over the square region $\\mathcal{C} = [-1, 1] \\times [-1, 1]$. The area of this square, which is a planar Lebesgue measure, is given by\n$$\n\\text{Area}(\\mathcal{C}) = (1 - (-1)) \\times (1 - (-1)) = 2 \\times 2 = 4\n$$\nBy the definition of a continuous uniform distribution over a region, the joint probability density function (PDF) $f_{V_1, V_2}(v_1, v_2)$ is constant within the region and zero outside. The value of the constant is the reciprocal of the area of the region.\n$$\nf_{V_1, V_2}(v_1, v_2) =\n\\begin{cases}\n\\frac{1}{\\text{Area}(\\mathcal{C})} = \\frac{1}{4} & \\text{if } (v_1, v_2) \\in \\mathcal{C} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nA proposal is accepted if the drawn pair $(V_1, V_2)$ satisfies the condition $S = V_1^2 + V_2^2 < 1$. This condition defines an open disk $\\mathcal{D}$ of radius $r=1$ centered at the origin $(0,0)$. The acceptance probability $p$ is the probability that a point uniformly chosen from the square $\\mathcal{C}$ lies within the disk $\\mathcal{D}$. This probability is calculated by integrating the joint PDF over the acceptance region $\\mathcal{D}$.\n$$\np = P(V_1^2 + V_2^2 < 1) = \\iint_{\\mathcal{D}} f_{V_1, V_2}(v_1, v_2) \\, dv_1 \\, dv_2\n$$\nThe disk $\\mathcal{D}$ is entirely contained within the square $\\mathcal{C}$ (since for any point $(v_1,v_2)$ in $\\mathcal{D}$, we have $v_1^2 < 1$ and $v_2^2 < 1$, which implies $|v_1| < 1$ and $|v_2| < 1$). Therefore, the PDF is constant at $\\frac{1}{4}$ over the entire integration domain $\\mathcal{D}$.\n$$\np = \\iint_{\\mathcal{D}} \\frac{1}{4} \\, dv_1 \\, dv_2 = \\frac{1}{4} \\iint_{\\mathcal{D}} \\, dv_1 \\, dv_2\n$$\nThe integral $\\iint_{\\mathcal{D}} \\, dv_1 \\, dv_2$ is simply the area of the disk $\\mathcal{D}$. The area of a disk with radius $r=1$ is $\\pi r^2 = \\pi(1)^2 = \\pi$.\nThus, the acceptance probability is the ratio of the area of the acceptance region to the area of the sampling region:\n$$\np = \\frac{\\text{Area}(\\mathcal{D})}{\\text{Area}(\\mathcal{C})} = \\frac{\\pi}{4}\n$$\n\n**Part 2: Derivation of $\\mathbb{E}[N]$ and $\\operatorname{Var}(N)$**\n\nThe random variable $N$ represents the number of independent and identically distributed Bernoulli trials required to obtain the first success. Each trial is a proposal, and a \"success\" is an accepted proposal. The probability of success in any single trial is $p = \\frac{\\pi}{4}$. This defines a geometric distribution for $N$. The probability mass function (PMF) of $N$ is given by\n$$\nP(N=k) = (1-p)^{k-1}p \\quad \\text{for } k \\in \\{1, 2, 3, \\ldots\\}\n$$\nFor convenience, let $q = 1-p$. The PMF is $P(N=k) = q^{k-1}p$.\n\nTo derive the expectation and variance from first principles, we rely on the formula for a geometric series and its derivatives. For any real number $|x| < 1$, the geometric series is\n$$\n\\sum_{k=0}^{\\infty} x^k = \\frac{1}{1-x}\n$$\nDifferentiating with respect to $x$ (which is valid for $|x|<1$) gives\n$$\n\\frac{d}{dx} \\sum_{k=0}^{\\infty} x^k = \\sum_{k=1}^{\\infty} kx^{k-1} = \\frac{d}{dx} (1-x)^{-1} = ( -1)(1-x)^{-2}(-1) = \\frac{1}{(1-x)^2}\n$$\nDifferentiating a second time gives\n$$\n\\frac{d}{dx} \\sum_{k=1}^{\\infty} kx^{k-1} = \\sum_{k=2}^{\\infty} k(k-1)x^{k-2} = \\frac{d}{dx} (1-x)^{-2} = (-2)(1-x)^{-3}(-1) = \\frac{2}{(1-x)^3}\n$$\n\n**Derivation of $\\mathbb{E}[N]$**\nBy the definition of expected value for a discrete random variable:\n$$\n\\mathbb{E}[N] = \\sum_{k=1}^{\\infty} k \\cdot P(N=k) = \\sum_{k=1}^{\\infty} k q^{k-1} p = p \\sum_{k=1}^{\\infty} k q^{k-1}\n$$\nSince $p = \\frac{\\pi}{4}$, we have $0 < p < 1$, which implies $0 < q < 1$. We can use the first derivative identity derived above with $x=q$:\n$$\n\\mathbb{E}[N] = p \\left( \\frac{1}{(1-q)^2} \\right)\n$$\nSubstituting $1-q = 1-(1-p) = p$, we obtain:\n$$\n\\mathbb{E}[N] = p \\left( \\frac{1}{p^2} \\right) = \\frac{1}{p}\n$$\nFinally, substituting $p = \\frac{\\pi}{4}$:\n$$\n\\mathbb{E}[N] = \\frac{1}{\\pi/4} = \\frac{4}{\\pi}\n$$\n\n**Derivation of $\\operatorname{Var}(N)$**\nThe variance is defined as $\\operatorname{Var}(N) = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2$. We first need to compute $\\mathbb{E}[N^2]$.\n$$\n\\mathbb{E}[N^2] = \\sum_{k=1}^{\\infty} k^2 \\cdot P(N=k) = \\sum_{k=1}^{\\infty} k^2 q^{k-1} p = p \\sum_{k=1}^{\\infty} k^2 q^{k-1}\n$$\nWe use the identity $k^2 = k(k-1) + k$ to decompose the sum:\n$$\n\\sum_{k=1}^{\\infty} k^2 q^{k-1} = \\sum_{k=1}^{\\infty} (k(k-1) + k) q^{k-1} = \\sum_{k=1}^{\\infty} k(k-1)q^{k-1} + \\sum_{k=1}^{\\infty} k q^{k-1}\n$$\nThe first term in the sum is zero for $k=1$, so a more precise form is $\\sum_{k=2}^{\\infty} k(k-1)q^{k-1}$.\nThe second sum is the one we evaluated for the expectation: $\\sum_{k=1}^{\\infty} k q^{k-1} = \\frac{1}{(1-q)^2} = \\frac{1}{p^2}$.\nFor the first sum, we use the second derivative identity.\n$$\n\\sum_{k=2}^{\\infty} k(k-1)q^{k-1} = q \\sum_{k=2}^{\\infty} k(k-1)q^{k-2}\n$$\nUsing our identity with $x=q$:\n$$\n\\sum_{k=2}^{\\infty} k(k-1)q^{k-1} = q \\left( \\frac{2}{(1-q)^3} \\right) = \\frac{2q}{(1-q)^3} = \\frac{2q}{p^3}\n$$\nNow, we can assemble the expression for $\\mathbb{E}[N^2]$:\n$$\n\\mathbb{E}[N^2] = p \\left( \\frac{2q}{p^3} + \\frac{1}{p^2} \\right) = \\frac{2q}{p^2} + \\frac{1}{p}\n$$\nNow, we compute the variance:\n$$\n\\operatorname{Var}(N) = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2 = \\left( \\frac{2q}{p^2} + \\frac{1}{p} \\right) - \\left( \\frac{1}{p} \\right)^2 = \\frac{2q}{p^2} + \\frac{1}{p} - \\frac{1}{p^2}\n$$\n$$\n\\operatorname{Var}(N) = \\frac{2q + p - 1}{p^2}\n$$\nSubstitute $q=1-p$:\n$$\n\\operatorname{Var}(N) = \\frac{2(1-p) + p - 1}{p^2} = \\frac{2 - 2p + p - 1}{p^2} = \\frac{1-p}{p^2}\n$$\nFinally, substituting $p = \\frac{\\pi}{4}$:\n$$\n\\operatorname{Var}(N) = \\frac{1 - \\frac{\\pi}{4}}{(\\frac{\\pi}{4})^2} = \\frac{\\frac{4-\\pi}{4}}{\\frac{\\pi^2}{16}} = \\frac{4-\\pi}{4} \\cdot \\frac{16}{\\pi^2} = \\frac{4(4-\\pi)}{\\pi^2} = \\frac{16-4\\pi}{\\pi^2}\n$$\nThe requested expressions for the expected value and variance have been derived from first principles.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{4}{\\pi} & \\frac{16-4\\pi}{\\pi^2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Theoretical algorithms must often be adapted to the practical limitations of computer arithmetic. The Marsaglia polar method's transformation involves terms like $\\ln(S)$ and $1/S$, which become numerically unstable as the squared radius $S$ approaches zero. This practice explores a common safeguard—imposing a small positive lower bound on $S$—and challenges you to analytically quantify the systematic error (bias) this introduces in the variance of the final output. Such analysis is a crucial skill in computational science, revealing the trade-offs between algorithmic purity and numerical robustness .",
            "id": "3324458",
            "problem": "Consider the Marsaglia polar method for generating independent standard normal variates. Let $U_1$ and $U_2$ be independent and identically distributed random variables, each uniform on the interval $(-1,1)$. Define $S = U_1^2 + U_2^2$ and accept the pair $(U_1,U_2)$ if and only if $S \\in (0,1)$. Conditional on acceptance, the method outputs\n$$\nZ_1 = U_1 \\sqrt{\\frac{-2 \\ln S}{S}}.\n$$\nIn practice, to avoid numerical instability for very small values of $S$, one may replace $S$ by the thresholded quantity $S_\\epsilon = \\max(S,\\epsilon)$ for some small $\\epsilon > 0$, and define the corresponding output as\n$$\nZ_1^{(\\epsilon)} = U_1 \\sqrt{\\frac{-2 \\ln S_\\epsilon}{S_\\epsilon}}.\n$$\nStarting from first principles in stochastic simulation and geometric probability, derive an exact expression for the bias in the estimated variance of $Z_1$ under this thresholding, defined by\n$$\n\\mathrm{Bias}(\\epsilon) = \\mathbb{E}\\!\\left[\\left(Z_1^{(\\epsilon)}\\right)^2\\right] - 1,\n$$\nwhere the expectation is over the accepted $(U_1,U_2)$ pairs. Your derivation must proceed by characterizing the joint distribution of $(U_1,U_2)$ conditional on acceptance, identifying the induced distribution of $S$, and computing $\\mathbb{E}\\!\\left[\\left(Z_1^{(\\epsilon)}\\right)^2\\right]$ via conditioning. Then, provide the asymptotic error rates of $\\mathrm{Bias}(\\epsilon)$ as $\\epsilon \\to 0^+$, identifying the leading and next-order terms. Express your final answer as a single closed-form analytic expression for $\\mathrm{Bias}(\\epsilon)$. No rounding is required; do not include units.",
            "solution": "The problem requires the derivation of an exact expression for the bias in the variance of a modified Marsaglia polar method variate. The bias is defined as $\\mathrm{Bias}(\\epsilon) = \\mathbb{E}\\!\\left[\\left(Z_1^{(\\epsilon)}\\right)^2\\right] - 1$, where the expectation is taken over the accepted pairs of random variables $(U_1, U_2)$.\n\nFirst, we must characterize the joint distribution of $(U_1, U_2)$ conditional on acceptance. The variables $U_1$ and $U_2$ are independent and identically distributed according to a uniform distribution on the interval $(-1,1)$. Their joint probability density function (PDF) is $f_{U_1,U_2}(u_1, u_2) = \\frac{1}{4}$ for $(u_1, u_2) \\in [-1,1] \\times [-1,1]$. The acceptance condition is $S = U_1^2 + U_2^2 \\in (0,1)$, which defines the interior of the unit disk, denoted $\\mathcal{D}$. The area of the initial sampling square is $4$, and the area of the acceptance disk $\\mathcal{D}$ is $\\pi(1)^2 = \\pi$. The probability of acceptance is the ratio of these areas, $P(S<1) = \\frac{\\pi}{4}$. Conditional on acceptance, the pair $(U_1, U_2)$ is uniformly distributed over the unit disk $\\mathcal{D}$. The conditional joint PDF is thus:\n$$\nf(u_1, u_2 | S < 1) = \\begin{cases} \\frac{1}{\\pi} & \\text{if } u_1^2 + u_2^2 < 1 \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe quantity to be computed is $\\mathbb{E}\\!\\left[\\left(Z_1^{(\\epsilon)}\\right)^2\\right]$, where the expectation is with respect to this conditional distribution. The expression for $\\left(Z_1^{(\\epsilon)}\\right)^2$ is:\n$$\n\\left(Z_1^{(\\epsilon)}\\right)^2 = \\left(U_1 \\sqrt{\\frac{-2 \\ln S_\\epsilon}{S_\\epsilon}}\\right)^2 = U_1^2 \\frac{-2 \\ln S_\\epsilon}{S_\\epsilon}\n$$\nwhere $S = U_1^2 + U_2^2$ and $S_\\epsilon = \\max(S, \\epsilon)$ for some small $\\epsilon > 0$.\n\nWe proceed by using the law of total expectation, conditioning on the value of $S=s$:\n$$\n\\mathbb{E}\\!\\left[\\left(Z_1^{(\\epsilon)}\\right)^2\\right] = \\mathbb{E}_S\\left[ \\mathbb{E}\\left[ U_1^2 \\frac{-2 \\ln S_\\epsilon}{S_\\epsilon} \\Big| S=s \\right] \\right] = \\mathbb{E}_S\\left[ \\frac{-2 \\ln s_\\epsilon}{s_\\epsilon} \\mathbb{E}\\left[ U_1^2 \\Big| S=s \\right] \\right]\n$$\nwhere $s_\\epsilon = \\max(s, \\epsilon)$.\n\nNext, we find the conditional expectation $\\mathbb{E}[U_1^2 | S=s]$. Given $U_1^2 + U_2^2 = s$, the point $(U_1, U_2)$ is uniformly distributed on a circle of radius $\\sqrt{s}$. We can represent this point in polar coordinates as $U_1 = \\sqrt{s}\\cos\\Theta$ and $U_2 = \\sqrt{s}\\sin\\Theta$, where $\\Theta$ is a random variable uniformly distributed on $[0, 2\\pi)$. By symmetry of the distributions of $U_1$ and $U_2$, we have $\\mathbb{E}[U_1^2 | S=s] = \\mathbb{E}[U_2^2 | S=s]$. Their sum is:\n$$\n\\mathbb{E}[U_1^2 | S=s] + \\mathbb{E}[U_2^2 | S=s] = \\mathbb{E}[U_1^2 + U_2^2 | S=s] = \\mathbb{E}[s | S=s] = s\n$$\nThis implies $2\\mathbb{E}[U_1^2 | S=s] = s$, so $\\mathbb{E}[U_1^2 | S=s] = \\frac{s}{2}$.\n\nSubstituting this back into the expression for the total expectation gives:\n$$\n\\mathbb{E}\\!\\left[\\left(Z_1^{(\\epsilon)}\\right)^2\\right] = \\mathbb{E}_S\\left[ \\frac{-2 \\ln s_\\epsilon}{s_\\epsilon} \\cdot \\frac{S}{2} \\right] = \\mathbb{E}_S\\left[ -S \\frac{\\ln S_\\epsilon}{S_\\epsilon} \\right]\n$$\nTo evaluate this expectation, we require the PDF of $S$ conditional on acceptance ($S<1$). The conditional cumulative distribution function (CDF) of $S$ for $s \\in [0,1)$ is the probability that a point chosen uniformly from the unit disk lies within a disk of radius $\\sqrt{s}$:\n$$\nF_S(s) = P(S \\le s | S < 1) = \\frac{\\text{Area of disk with radius } \\sqrt{s}}{\\text{Area of disk with radius } 1} = \\frac{\\pi (\\sqrt{s})^2}{\\pi (1)^2} = s\n$$\nThe PDF is the derivative of the CDF, $f_S(s) = \\frac{d}{ds}s = 1$ for $s \\in (0,1)$. Thus, conditional on acceptance, $S$ is uniformly distributed on $(0,1)$.\n\nWe can now compute the expectation as an integral:\n$$\n\\mathbb{E}\\!\\left[\\left(Z_1^{(\\epsilon)}\\right)^2\\right] = \\int_0^1 \\left(-s \\frac{\\ln(\\max(s,\\epsilon))}{\\max(s,\\epsilon)}\\right) \\cdot f_S(s) \\, ds = \\int_0^1 -s \\frac{\\ln(\\max(s,\\epsilon))}{\\max(s,\\epsilon)} \\, ds\n$$\nWe split the integral at $s=\\epsilon$, since the form of the integrand changes at this point:\n$$\n\\mathbb{E}\\!\\left[\\left(Z_1^{(\\epsilon)}\\right)^2\\right] = \\int_0^\\epsilon \\left(-s \\frac{\\ln \\epsilon}{\\epsilon}\\right) ds + \\int_\\epsilon^1 \\left(-s \\frac{\\ln s}{s}\\right) ds\n$$\nWe evaluate each integral separately. For the first integral:\n$$\n\\int_0^\\epsilon \\left(-\\frac{s \\ln \\epsilon}{\\epsilon}\\right) ds = -\\frac{\\ln \\epsilon}{\\epsilon} \\int_0^\\epsilon s \\, ds = -\\frac{\\ln \\epsilon}{\\epsilon} \\left[ \\frac{s^2}{2} \\right]_0^\\epsilon = -\\frac{\\ln \\epsilon}{\\epsilon} \\frac{\\epsilon^2}{2} = -\\frac{\\epsilon \\ln \\epsilon}{2}\n$$\nFor the second integral:\n$$\n\\int_\\epsilon^1 (-\\ln s) \\, ds\n$$\nUsing integration by parts with $u = -\\ln s$ and $dv = ds$, we have $du = -\\frac{1}{s} \\, ds$ and $v=s$.\n$$\n\\int (-\\ln s) \\, ds = s(-\\ln s) - \\int s\\left(-\\frac{1}{s}\\right)ds = -s\\ln s + \\int 1 \\, ds = -s\\ln s + s + C\n$$\nEvaluating the definite integral:\n$$\n[-s\\ln s + s]_\\epsilon^1 = (-1\\ln 1 + 1) - (-\\epsilon\\ln\\epsilon + \\epsilon) = (0 + 1) - (-\\epsilon\\ln\\epsilon + \\epsilon) = 1 + \\epsilon\\ln\\epsilon - \\epsilon\n$$\nCombining the results from both integrals gives the expected squared value:\n$$\n\\mathbb{E}\\!\\left[\\left(Z_1^{(\\epsilon)}\\right)^2\\right] = \\left(-\\frac{\\epsilon \\ln \\epsilon}{2}\\right) + (1 + \\epsilon\\ln\\epsilon - \\epsilon) = 1 + \\frac{\\epsilon \\ln \\epsilon}{2} - \\epsilon\n$$\nFinally, the bias is calculated by subtracting the true variance, which is $1$:\n$$\n\\mathrm{Bias}(\\epsilon) = \\mathbb{E}\\!\\left[\\left(Z_1^{(\\epsilon)}\\right)^2\\right] - 1 = \\left(1 + \\frac{\\epsilon \\ln \\epsilon}{2} - \\epsilon\\right) - 1 = \\frac{\\epsilon \\ln \\epsilon}{2} - \\epsilon\n$$\nAs requested, we analyze the asymptotic rates as $\\epsilon \\to 0^+$. The limit $\\lim_{\\epsilon \\to 0^+} \\frac{\\epsilon}{\\epsilon \\ln \\epsilon} = \\lim_{\\epsilon \\to 0^+} \\frac{1}{\\ln \\epsilon} = 0$, which shows that the term $-\\epsilon$ is of a smaller order of magnitude than $\\frac{1}{2}\\epsilon\\ln\\epsilon$. Thus, the leading-order term in the bias is $\\frac{1}{2}\\epsilon\\ln\\epsilon$ and the next-order term is $-\\epsilon$.",
            "answer": "$$\n\\boxed{\\frac{\\epsilon \\ln \\epsilon}{2} - \\epsilon}\n$$"
        },
        {
            "introduction": "To achieve high performance, modern Monte Carlo simulations are often executed on parallel computing architectures. This introduces subtle but critical challenges in managing random number streams to ensure statistical integrity. This exercise evaluates common strategies for handling the paired output of the Marsaglia polar method in a parallel setting, where streams may request an odd number of variates. Your analysis will focus on preserving two essential properties for reliable simulation: inter-stream independence and schedule-invariant reproducibility .",
            "id": "3324419",
            "problem": "Consider the Marsaglia polar method for generating independent standard normal variates. Let $(U,V)$ be two independent draws from the continuous uniform distribution on $(-1,1)$, and define $S = U^2 + V^2$. The method samples $(U,V)$ until $S < 1$, at which point a transformation is applied to obtain two independent standard normal variates $(Z_1,Z_2)$. It is a well-tested fact that, conditional on acceptance ($S<1$), the transformation yields $(Z_1,Z_2)$ as independent draws from the standard normal distribution $N(0,1)$. The acceptance probability for any attempt is the area ratio $p = \\pi/4$.\n\nIn a parallel Monte Carlo setting with $M$ streams, each stream has its own random number generator (RNG) seed, and each stream intermittently requests standard normal variates. To handle an odd number of requested normals per stream when using a pair-producing method such as the Marsaglia polar method, several caching strategies are considered. Define schedule-invariant reproducibility as the property that, for fixed per-stream seeds and fixed per-stream request counts $(n_1,\\dots,n_M)$, the sequence of outputs within each stream is a deterministic function of its own seed and request counts and does not depend on the interleaving or timing of requests across streams. Define inter-stream independence as the property that outputs from distinct streams are functions of disjoint sets of independent base variates, so that streams do not consume or influence each other’s base random draws.\n\nEvaluate the following strategies and statements. Select all statements that are correct.\n\nA. A local per-stream single-slot cache: whenever a stream accepts a pair $(Z_1,Z_2)$ but needs only one, it returns one and stores the other in that stream’s private cache for its next call. This strategy preserves both inter-stream independence and schedule-invariant reproducibility. Furthermore, if a stream produces $n$ normals in total, the expected number of independent uniform pairs $(U,V)$ it draws is $\\mathbb{E}[K] = \\lceil n/2 \\rceil / p$, where $p = \\pi/4$.\n\nB. A single global shared cache across all streams: whenever any stream accepts a pair $(Z_1,Z_2)$ but needs only one, it stores the other in a global cache, and any subsequent stream needing a single normal may consume the cached value. Because $(Z_1,Z_2)$ are independent $N(0,1)$, this sharing preserves both inter-stream independence and schedule-invariant reproducibility, and the expected number of $(U,V)$ pairs drawn per stream is unchanged.\n\nC. Discard the second variate whenever only one is requested: the stream always discards the unused element of $(Z_1,Z_2)$ instead of caching. This strategy preserves inter-stream independence and schedule-invariant reproducibility. However, for a stream producing $n$ normals, the expected number of $(U,V)$ pairs drawn increases to $\\mathbb{E}[K] = n / p$, which is asymptotically twice the cost of fully utilizing pairs as $n \\to \\infty$.\n\nD. Disjoint substreams with local caches: partition the base uniform generator into disjoint substreams (via skip-ahead or leapfrog) so that each stream consumes a dedicated, non-overlapping subsequence of base uniform variates, and combine this with a local per-stream single-slot cache. Then, for any interleaving of calls, the per-stream output sequences match those from a single-threaded execution with the same seeds and request counts. Moreover, for any fixed time index pairing across streams, the cross-stream covariance $\\operatorname{Cov}(Z^{(i)}_t, Z^{(j)}_s)$ equals $0$ for $i \\neq j$, since outputs are functions of disjoint independent base variates and each $Z$ has mean $0$.\n\nChoose all correct options.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains sufficient information to evaluate the given statements. We will proceed with the analysis of each option.\n\nThe core concepts are **schedule-invariant reproducibility** and **inter-stream independence**. Schedule-invariant reproducibility implies that the output of a stream depends only on its own initial state (seed) and its own history of requests, not on the timing or actions of other streams. Inter-stream independence requires that the random variates generated by different streams are statistically independent, which is typically achieved by ensuring they are functions of disjoint sets of base uniform random numbers.\n\nLet $p = \\pi/4$ be the acceptance probability of the Marsaglia polar method. The number of trials (pairs $(U,V)$ drawn) to obtain one accepted pair follows a geometric distribution with success probability $p$. The expected number of trials for one success is $1/p$.\n\n### Analysis of Option A\nThis strategy proposes a local, per-stream, single-slot cache.\n- **Inter-stream independence**: Each stream $i$ is described as having its own random number generator (RNG) seed. A local cache for stream $i$ is private and not accessible by any other stream $j \\neq i$. Therefore, all base uniform variates and consequently the generated normal variates for stream $i$ are derived from its own RNG. The outputs of stream $i$ and stream $j$ are functions of disjoint sets of base random numbers (assuming the seeds produce independent sequences, a standard assumption). This precisely matches the definition of inter-stream independence. Thus, this property is **preserved**.\n- **Schedule-invariant reproducibility**: The state of stream $i$ consists of its RNG state and its local cache. Both are modified only when stream $i$ requests a variate. The actions of other streams do not affect this state. Therefore, the sequence of variates produced by stream $i$ for a fixed number of requests $n_i$ is a deterministic function of its seed and $n_i$, regardless of how its calls are interleaved with calls from other streams. Thus, this property is **preserved**.\n- **Cost Analysis**: To produce a total of $n$ normal variates, a stream needs to generate $\\lceil n/2 \\rceil$ pairs of $(Z_1, Z_2)$. Each pair generation requires one accepted trial from the Marsaglia method. The expected number of drawn pairs $(U,V)$ to get one accepted pair is $1/p$. Since each of the $\\lceil n/2 \\rceil$ pair generations is an independent process, the total expected number of drawn pairs $(U,V)$ is the sum of the individual expectations, which is $\\mathbb{E}[K] = \\lceil n/2 \\rceil \\times (1/p) = \\lceil n/2 \\rceil / p$. This claim is **correct**.\n\nSince all parts of the statement are correct, option A is correct.\n**Verdict: Correct**\n\n### Analysis of Option B\nThis strategy proposes a single global cache shared among all streams.\n- **Inter-stream independence**: Suppose stream $i$ needs one variate. It checks the global cache, finds it empty, generates a pair $(Z_1, Z_2)$ using its own RNG, returns $Z_1$, and places $Z_2$ in the cache. Now, stream $j$ needs a variate. It checks the cache and finds $Z_2$. It consumes $Z_2$. Stream $j$'s output now includes a variate that was generated using base uniforms from stream $i$'s RNG. This violates the definition of inter-stream independence, which requires outputs to be functions of disjoint sets of base variates. The claim that this is preserved is **incorrect**.\n- **Schedule-invariant reproducibility**: Consider two streams, $i$ and $j$, each requesting one variate.\n    - Schedule 1: Stream $i$ calls first. It generates a pair $(Z_{i,1}, Z_{i,2})$ from its RNG, returns $Z_{i,1}$ and caches $Z_{i,2}$. Then stream $j$ calls, takes $Z_{i,2}$ from the cache, and returns it. The output sequence is $(Z_{i,1})$ for stream $i$ and $(Z_{i,2})$ for stream $j$.\n    - Schedule 2: Stream $j$ calls first. It generates a pair $(Z_{j,1}, Z_{j,2})$ from its RNG, returns $Z_{j,1}$ and caches $Z_{j,2}$. Then stream $i$ calls, takes $Z_{j,2}$ from the cache, and returns it. The output sequence is $(Z_{j,2})$ for stream $i$ and $(Z_{j,1})$ for stream $j$.\nThe output for stream $i$ is $Z_{i,1}$ in Schedule 1 but $Z_{j,2}$ in Schedule 2. Since the output depends on the interleaving of calls, the system is not schedule-invariant reproducible. The claim that this is preserved is **incorrect**.\nThe justification given in the option, \"Because $(Z_1,Z_2)$ are independent $N(0,1)$\", is a non-sequitur. While the two variates in a pair are statistically independent, sharing a specific realization of one variate across streams creates a dependency between the streams' outputs and their underlying random sources.\n\nSince the claims about preserving independence and reproducibility are false, the entire statement is incorrect.\n**Verdict: Incorrect**\n\n### Analysis of Option C\nThis strategy proposes discarding the second variate of a pair whenever only one is needed.\n- **Inter-stream independence**: In this strategy, there is no caching and no communication between streams. Each stream generates variates using only its own RNG. Therefore, the outputs of different streams are functions of disjoint sets of base uniform variates. This property is **preserved**.\n- **Schedule-invariant reproducibility**: Since there is no state carried over between calls (no cache), the generation of a variate depends only on the current state of the stream's private RNG. The RNG's state sequence is deterministic for a given seed. The behavior of other streams has no influence. Thus, this property is **preserved**.\n- **Cost Analysis**: To produce $n$ normal variates, this strategy effectively requires $n$ separate generation events. Each event generates a pair $(Z_1,Z_2)$ and uses one value. This means $n$ accepted pairs must be generated by the Marsaglia method. Each accepted pair requires an expected $1/p$ draws of $(U,V)$. Therefore, the total expected number of draws is $\\mathbb{E}[K] = n \\times (1/p) = n/p$. For large $n$, the efficient caching strategy from option A has an expected cost of $(\\lceil n/2 \\rceil)/p \\approx (n/2)/p$. The ratio of the cost of this discarding strategy to the efficient strategy is $(n/p) / ((n/2)/p) = 2$. Thus, the claim that the cost is asymptotically twice that of fully utilizing pairs is **correct**.\n\nSince all parts of the statement are correct, option C is correct.\n**Verdict: Correct**\n\n### Analysis of Option D\nThis strategy proposes using disjoint substreams for the base uniform generator for each stream, combined with local per-stream caching.\n- **Reproducibility claim**: \"for any interleaving of calls, the per-stream output sequences match those from a single-threaded execution...\". Using disjoint substreams is a formal mechanism to guarantee that each stream $i$ consumes a sequence of base uniform variates that is not accessed or affected by any other stream $j$. This, combined with a purely local cache, isolates each stream completely. The sequence of generated normals for stream $i$ is thus a deterministic function of its assigned substream and its request sequence, which is exactly what a single-threaded execution for that stream would produce. The claim is **correct**.\n- **Covariance claim**: \"$\\operatorname{Cov}(Z^{(i)}_t, Z^{(j)}_s)$ equals $0$ for $i \\neq j$\". Let $Z^{(i)}_t$ be the $t$-th variate from stream $i$ and $Z^{(j)}_s$ be the $s$-th from stream $j$, with $i \\neq j$. By construction, $Z^{(i)}_t$ is a function of the uniform variates from substream $i$, say $\\mathcal{U}_i$. Similarly, $Z^{(j)}_s$ is a function of the uniform variates from substream $j$, $\\mathcal{U}_j$. The substreams $\\mathcal{U}_i$ and $\\mathcal{U}_j$ are disjoint sets of independent base random variables. A fundamental theorem of probability states that functions of disjoint sets of independent random variables are themselves independent. Therefore, $Z^{(i)}_t$ and $Z^{(j)}_s$ are independent random variables. The covariance of any two independent random variables is $0$. The fact that each $Z$ has mean $0$ is consistent but not necessary to prove zero covariance; independence is sufficient. The claim is **correct**.\n\nSince both claims in the statement are correct, option D is correct.\n**Verdict: Correct**",
            "answer": "$$\\boxed{ACD}$$"
        }
    ]
}