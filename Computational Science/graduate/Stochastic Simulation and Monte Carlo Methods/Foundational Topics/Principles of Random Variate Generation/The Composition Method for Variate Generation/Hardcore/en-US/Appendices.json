{
    "hands_on_practices": [
        {
            "introduction": "Building a correct sampler begins with sound logic. This exercise moves beyond the idealized case and forces us to consider practical edge cases, such as components with zero weight or generators that might fail under certain parameterizations. By evaluating different implementation strategies , you will learn to build a robust logical foundation that prevents bias and ensures your sampler is correct by construction.",
            "id": "3351380",
            "problem": "A mixture distribution is defined by a finite collection of component distributions $\\{F_i\\}_{i=1}^m$ with densities $\\{f_i\\}_{i=1}^m$ and nonnegative weights $\\{w_i\\}_{i=1}^m$ satisfying $\\sum_{i=1}^m w_i = 1$. The target mixture density is $f(x) = \\sum_{i=1}^m w_i f_i(x)$. The composition method for variate generation constructs a sample $X$ by first drawing an index $I$ with $\\mathbb{P}(I = i) = w_i$ and then drawing $X \\mid I=i \\sim F_i$. Consider implementing the composition method in floating-point arithmetic when some components have zero weights and when some component generators are empty under the current parameterization, in the sense that their support is void or their sampling routine deterministically fails due to parameter constraints (for example, a shape parameter that makes $f_i(x) \\equiv 0$ almost everywhere or a domain restriction that precludes any valid draw).\n\nYou are asked to assess several implementation strategies with respect to correctness and unbiasedness of the resulting simulated mixture, under the following base principles:\n- The mixture is a probability measure: for any measurable set $A$, $\\mathbb{P}(X \\in A) = \\sum_{i=1}^m w_i \\,\\mathbb{P}(X \\in A \\mid I=i)$.\n- If $w_i = 0$, then $\\mathbb{P}(I=i) = 0$, and occurrences of $I=i$ have zero probability.\n- If a component is empty under the current parameters, a correct mathematical model must assign it weight $0$ or treat its generator’s failure as indicating an invalid model state; ad hoc reassignment of its weight is not justified unless derived from the model.\n\nWhich of the following implementation strategies avoid biased sampling while preserving correctness? Select all that apply.\n\nA. Pre-eliminate any index $i$ with $w_i = 0$ or with an empty generator under the current parameters; construct the categorical distribution over the remaining indices using weights $\\{w_i\\}_{i: w_i0}$, computing cumulative sums with compensated summation to reduce floating-point error; draw $I$ by comparing a uniform variate $U \\in [0,1)$ to the cumulative sums, mapping any rare $U=1$ to the last bin to avoid dropping mass; then draw $X \\mid I=i \\sim F_i$. If drawing $X \\mid I=i$ fails due to transient numerical issues but the component is theoretically nonempty, retry sampling $X$ while keeping $I$ fixed; if it deterministically fails, treat $F_i$ as empty and set $w_i=0$ before rebuilding the sampler.\n\nB. Keep all indices, including those with $w_i=0$, in the categorical distribution; if a uniform draw selects a zero-weight index due to rounding or cumulative sum discretization, fall back to the nearest nonzero-weight component (for example, the previous index), rather than redrawing $U$, to avoid resampling overhead.\n\nC. Replace each zero weight by a small positive constant $\\varepsilon$ (for example, machine epsilon) before building the categorical or alias sampler to avoid degenerate buckets; then renormalize the entire weight vector so that $\\sum_i w_i' = 1$ and proceed with composition sampling.\n\nD. Use the alias method for discrete sampling but construct the alias table only from indices with strictly positive weights; ensure numerical stabilization of the weight normalization using compensated summation or scaling; when a generated $X \\mid I=i$ returns a non-finite value due to a transient numerical problem in $F_i$, retry sampling $X$ from the same component $i$ until a valid draw is produced or a safety limit is reached, at which point classify $F_i$ as empty and remove it (set $w_i=0$) before rebuilding the alias table.",
            "solution": "The problem statement is assessed to be valid. It presents a clear, well-posed question regarding the practical implementation of a standard statistical algorithm, the composition method for variate generation. The problem is scientifically grounded in probability theory and computational statistics, objective in its formulation, and provides sufficient context through its \"base principles\" to allow for a rigorous evaluation of the proposed strategies.\n\nThe core of the composition method is to generate a random variate $X$ from a mixture distribution with density $f(x) = \\sum_{i=1}^m w_i f_i(x)$. This is achieved by a two-step process:\n$1$. Draw a discrete random index $I$ from the set $\\{1, 2, \\dots, m\\}$ with probabilities $\\mathbb{P}(I=i) = w_i$.\n$2$. Draw the continuous random variate $X$ from the component distribution $F_i$ corresponding to the drawn index $i$.\n\nA correct and unbiased implementation must ensure that these two steps are performed in a way that is faithful to the specified probabilities, even in the presence of finite-precision arithmetic and other practical complications. The fundamental principles are that if a weight $w_i=0$, the corresponding index $i$ must be selected with probability $0$, and if a component distribution $F_i$ is \"empty\" (i.e., its density $f_i(x)$ is zero almost everywhere), its contribution to the mixture is zero, and it should not be selected.\n\nLet's evaluate each option against these principles.\n\n**Option A: Pre-eliminate any index $i$ with $w_i = 0$ or with an empty generator under the current parameters; construct the categorical distribution over the remaining indices using weights $\\{w_i\\}_{i: w_i0}$, computing cumulative sums with compensated summation to reduce floating-point error; draw $I$ by comparing a uniform variate $U \\in [0,1)$ to the cumulative sums, mapping any rare $U=1$ to the last bin to avoid dropping mass; then draw $X \\mid I=i \\sim F_i$. If drawing $X \\mid I=i$ fails due to transient numerical issues but the component is theoretically nonempty, retry sampling $X$ while keeping $I$ fixed; if it deterministically fails, treat $F_i$ as empty and set $w_i=0$ before rebuilding the sampler.**\n\nThis strategy is a meticulously described, correct, and robust implementation.\n- **Pre-elimination**: Discarding components where $w_i = 0$ or the generator is empty is correct. These components have zero probability mass in the mixture and should never be sampled. The categorical sampler should be built only on the set of indices $S = \\{i \\mid w_i  0 \\text{ and } F_i \\text{ is not empty}\\}$. The probabilities for the sampler must be renormalized, i.e., $p_i = w_i / \\sum_{j \\in S} w_j$ for $i \\in S$. The option correctly implies this by stating the construction is over the remaining indices.\n- **Numerical Stability**: Using compensated summation (e.g., Kahan summation) to compute the cumulative distribution function for inverse transform sampling is a best practice that minimizes floating-point error, ensuring the discrete sampling step is as accurate as possible.\n- **Sampling Details**: The use of inverse transform sampling with a $U \\in [0,1)$ variate and handling the edge case of $U$ being near $1$ correctly accounts for the entire probability mass.\n- **Failure Handling**: The distinction between transient and deterministic failures is critical. Retrying on a transient numerical error (while keeping the chosen component $I=i$ fixed) is a correct way to obtain a valid sample from $F_i$ without introducing bias. Treating a deterministic failure as evidence of an empty component and subsequently rebuilding the sampler (which implies setting $w_i=0$ and renormalizing) is also a correct procedure that adapts the model to its discovered properties.\nThis strategy is fully consistent with the stated principles and avoids introducing bias.\n\nVerdict: **Correct**.\n\n**Option B: Keep all indices, including those with $w_i=0$, in the categorical distribution; if a uniform draw selects a zero-weight index due to rounding or cumulative sum discretization, fall back to the nearest nonzero-weight component (for example, the previous index), rather than redrawing $U$, to avoid resampling overhead.**\n\nThis strategy introduces bias.\n- The core flaw is the ad hoc fallback rule: \"fall back to the nearest nonzero-weight component\". This procedure is not derived from any probabilistic principle. It systematically allocates probability mass that might erroneously fall into a zero-weight bin to a specific neighbor. For example, if bin $i$ has $w_i=0$ and the rule is to fall back to bin $i-1$, then component $i-1$ will be systematically oversampled, while component $i+1$ will be undersampled relative to its proper weight. A correct sampler must ensure that the probability of selecting index $j$ is proportional to $w_j$. This rule violates that proportionality. While it might seem like a pragmatic fix for floating-point inaccuracies, its deterministic nature creates a systematic bias.\n\nVerdict: **Incorrect**.\n\n**Option C: Replace each zero weight by a small positive constant $\\varepsilon$ (for example, machine epsilon) before building the categorical or alias sampler to avoid degenerate buckets; then renormalize the entire weight vector so that $\\sum_i w_i' = 1$ and proceed with composition sampling.**\n\nThis strategy is fundamentally flawed because it knowingly samples from the wrong distribution.\n- The original problem specifies a mixture with certain weights $w_i$, some of which may be zero. By replacing $w_i=0$ with a small positive value $w_i'=\\varepsilon  0$, the implementer changes the definition of the target distribution. The sampler will now draw from a new mixture $f'(x) = \\sum w_i' f_i(x)$, which is not equal to the target mixture $f(x)$. This generates samples from a biased model. The motivation of avoiding \"degenerate buckets\" in a sampler is an implementation concern that must be solved without altering the mathematical specification of the problem. A correct algorithm must be able to handle zero-probability events by simply not generating them.\n\nVerdict: **Incorrect**.\n\n**Option D: Use the alias method for discrete sampling but construct the alias table only from indices with strictly positive weights; ensure numerical stabilization of the weight normalization using compensated summation or scaling; when a generated $X \\mid I=i$ returns a non-finite value due to a transient numerical problem in $F_i$, retry sampling $X$ from the same component $i$ until a valid draw is produced or a safety limit is reached, at which point classify $F_i$ as empty and remove it (set $w_i=0$) before rebuilding the alias table.**\n\nThis strategy, similar to A, describes a correct and robust implementation.\n- **Algorithm Choice**: The alias method is an efficient and exact algorithm for sampling from a categorical distribution, making it a suitable choice.\n- **Handling Zero Weights**: Constructing the alias table only from indices with strictly positive weights is the correct approach. This is equivalent to pre-eliminating zero-weight components, ensuring they are never selected, in accordance with $\\mathbb{P}(I=i)=0$ when $w_i=0$.\n- **Numerical Stability**: Acknowledging the need for numerically stable normalization (e.g., via compensated summation) demonstrates a commitment to correctness in floating-point arithmetic.\n- **Failure Handling**: The policy for handling failures is sound. Retrying for transient errors from the same component $i$ is correct. The addition of a safety limit is a practical and necessary safeguard against potential infinite loops. Classifying a component as empty after repeated failures is a reasonable and defensible heuristic for identifying a deterministically failing generator, and the subsequent action of rebuilding the sampler is correct.\n\nThis strategy is a valid and unbiased implementation of the composition method. It differs from A mainly in the choice of the discrete sampling algorithm (alias method vs. inverse transform), but the logical principles for handling the challenging cases are identical and correct.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "With a logically sound structure, the next challenge is numerical stability in finite-precision arithmetic. This practice explores what happens when component parameters are extreme, leading to values that can underflow to zero and corrupt your results. You will analyze advanced, numerically stable techniques  that are essential for creating professional-grade samplers that remain accurate even in challenging scenarios.",
            "id": "3351355",
            "problem": "You are tasked with implementing the composition method to draw samples from a truncated mixture distribution in a Monte Carlo simulation. Consider a two-component mixture of exponential distributions with density\n$$\nf(x) \\,=\\, \\pi_1 \\,\\lambda_1 \\,e^{-\\lambda_1 x} \\;+\\; \\pi_2 \\,\\lambda_2 \\,e^{-\\lambda_2 x}, \\quad x \\ge 0,\n$$\nwhere $0\\pi_11$, $\\pi_2 = 1-\\pi_1$, and the rates satisfy $\\lambda_1 \\ll \\lambda_2$. You need to sample from the conditional (truncated) distribution of $X$ given $Xt$ for a large threshold $t0$, using the composition method: first select a component index according to the conditional mixing probabilities given the truncation, and then sample from the corresponding truncated component distribution.\n\nIn finite-precision arithmetic, when $\\lambda_1 \\ll \\lambda_2$ and $t$ is large, direct implementations may suffer from severe underflow or loss of significance in both the component selection step and in the subsequent sampling of the truncated component. Which of the following implementation choices are numerically stable and unbiased in floating-point arithmetic when $\\lambda_1 \\ll \\lambda_2$ and $t$ is large? Select all that apply.\n\nA. Compute the post-truncation component weights as $w_k=\\pi_k \\, e^{-\\lambda_k t}$ in floating point, normalize by dividing by $w_1+w_2$, draw a uniform $U\\sim \\mathrm{Unif}(0,1)$ to select the component with these normalized weights, and then sample from the truncated exponential by drawing $U'\\sim \\mathrm{Unif}(0,1)$ and setting $X=t - \\frac{1}{\\lambda_K}\\log U'$. This direct implementation is numerically stable and unbiased.\n\nB. Work in the logarithmic domain for component selection: compute $\\ell_k=\\log \\pi_k - \\lambda_k t$, and sample the component index $K$ by the $\\mathrm{Gumbel}$-max method, i.e., draw $G_k \\stackrel{\\mathrm{iid}}{\\sim} \\mathrm{Gumbel}(0,1)$ and take $K=\\operatorname{argmax}_{k\\in\\{1,2\\}} \\{\\ell_k+G_k\\}$. Then sample from the truncated exponential as in option A. This avoids underflow in the selection step and is unbiased.\n\nC. Rescale the post-truncation weights by factoring out the smallest rate: let $\\lambda_\\star=\\min\\{\\lambda_1,\\lambda_2\\}$ and compute unnormalized weights $w_k'=\\pi_k \\exp\\{-(\\lambda_k-\\lambda_\\star)t\\}$; normalize $w_k'$ to form selection probabilities. Then sample from the truncated exponential as in option A. This rescaling prevents underflow in the selection step and is unbiased.\n\nD. If $\\lambda_2 t$ is so large that $e^{-\\lambda_2 t}$ would underflow to $0$ in double precision, set the corresponding weight to $0$, renormalize the remaining weight(s), and proceed. The resulting bias is negligible and can be ignored.\n\nE. In the truncated sampling step, to preserve small increments when $t \\gg \\frac{1}{\\lambda_K}$, compute the increment as $Y = -\\frac{1}{\\lambda_K}\\log\\!\\big(1-U\\big)$ but evaluate the logarithm with the compensated function $\\log1p(-U)$ to avoid subtractive cancellation, i.e., set $Y = -\\frac{1}{\\lambda_K}\\,\\log1p(-U)$ with $U\\sim \\mathrm{Unif}(0,1)$. If high relative accuracy near $t$ is needed, return the sample as the two-part representation $(t,Y)$ rather than the rounded floating-point sum $t+Y$. This maintains the law and improves numerical robustness for large $t$.\n\nSelect all that apply.",
            "solution": "The problem statement is critically evaluated for validity prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- The probability density function of a two-component mixture of exponential distributions is $f(x) \\,=\\, \\pi_1 \\,\\lambda_1 \\,e^{-\\lambda_1 x} \\;+\\; \\pi_2 \\,\\lambda_2 \\,e^{-\\lambda_2 x}$ for $x \\ge 0$.\n- The mixing probabilities satisfy $0\\pi_11$ and $\\pi_2 = 1-\\pi_1$.\n- The exponential rates satisfy $\\lambda_1 \\ll \\lambda_2$.\n- The task is to generate samples from the conditional distribution of the random variable $X$ given the event $X  t$, where $t  0$ is a large threshold.\n- The sampling procedure is the composition method:\n    1. Select a component index $K$ based on the conditional mixing probabilities given $Xt$.\n    2. Sample from the truncated distribution of the selected component $K$.\n- The context is finite-precision floating-point arithmetic, where underflow and loss of significance are potential issues, particularly for large $t$ under the condition $\\lambda_1 \\ll \\lambda_2$.\n- The question is to identify which of the provided implementation choices are numerically stable and unbiased.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It addresses a standard topic in computational statistics and Monte Carlo methods: generating random variates from a truncated mixture distribution. The numerical challenges described (underflow, loss of significance) are genuine concerns in floating-point arithmetic and are central to the field of numerical analysis. The setup is self-contained, with all necessary definitions provided. The problem is well-posed, asking for an evaluation of specific numerical strategies against the criteria of stability and unbiasedness. The terminology is precise and objective. The problem does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full solution and evaluation of options will be performed.\n\n### Principle-Based Derivation\nLet $X$ be a random variable with the given mixture density $f(x)$. The survival function is $S(x) = P(Xx) = \\pi_1 P(X_1x) + \\pi_2 P(X_2x)$, where $X_k \\sim \\mathrm{Exp}(\\lambda_k)$. The survival function for an exponential distribution is $S_k(x) = P(X_kx) = e^{-\\lambda_k x}$. Thus, the mixture survival function is $S(t) = \\pi_1 e^{-\\lambda_1 t} + \\pi_2 e^{-\\lambda_2 t}$.\n\nThe composition method for sampling from the truncated distribution $f(x|Xt)$ requires two steps:\n\n1.  **Component Selection**: Select a component index $K \\in \\{1, 2\\}$. The probability of selecting component $k$ is the conditional probability that the sample originated from component $k$, given that it is greater than $t$. This is given by Bayes' theorem:\n    $$\n    \\pi'_k = P(K=k | Xt) = \\frac{P(Xt|K=k)P(K=k)}{P(Xt)} = \\frac{S_k(t) \\pi_k}{S(t)} = \\frac{\\pi_k e^{-\\lambda_k t}}{\\pi_1 e^{-\\lambda_1 t} + \\pi_2 e^{-\\lambda_2 t}}\n    $$\n    Let $w_k = \\pi_k e^{-\\lambda_k t}$ be the unnormalized posterior weights. Then $\\pi'_k = w_k / (w_1 + w_2)$. Given $\\lambda_1 \\ll \\lambda_2$ and large $t$, the term $e^{-\\lambda_1 t}$ is much larger than $e^{-\\lambda_2 t}$. However, for sufficiently large $t$, both $e^{-\\lambda_1 t}$ and $e^{-\\lambda_2 t}$ can underflow to $0$ in floating-point arithmetic, causing the sum $w_1+w_2$ to become $0$ and leading to a division-by-zero error. This is a primary numerical stability challenge.\n\n2.  **Truncated Sampling**: After selecting a component $K$, we must sample from the distribution of $X_K$ conditioned on $X_K  t$. Due to the memoryless property of the exponential distribution, if $X_K \\sim \\mathrm{Exp}(\\lambda_K)$, then the distribution of $X_K - t$ conditioned on $X_K  t$ is also $\\mathrm{Exp}(\\lambda_K)$. Therefore, one can generate a sample $Y \\sim \\mathrm{Exp}(\\lambda_K)$ and set the final sample to $X = t+Y$. The standard way to generate $Y$ is via inverse transform sampling: $Y = -\\frac{1}{\\lambda_K} \\log(U)$, where $U \\sim \\mathrm{Unif}(0,1)$. A numerical stability challenge arises here when $t$ is large and $Y$ is small, as the sum $t+Y$ may be rounded to $t$, losing the information in the increment $Y$.\n\n### Option-by-Option Analysis\n\n**A. Compute the post-truncation component weights as $w_k=\\pi_k \\, e^{-\\lambda_k t}$ in floating point, normalize by dividing by $w_1+w_2$, draw a uniform $U\\sim \\mathrm{Unif}(0,1)$ to select the component with these normalized weights, and then sample from the truncated exponential by drawing $U'\\sim \\mathrm{Unif}(0,1)$ and setting $X=t - \\frac{1}{\\lambda_K}\\log U'$. This direct implementation is numerically stable and unbiased.**\n\nThis option describes the most direct, naive implementation. As established in the derivation, the computation of $w_k = \\pi_k e^{-\\lambda_k t}$ is susceptible to underflow. If $\\lambda_1 t$ is sufficiently large (e.g., $\\lambda_1 t  709.8$ for IEEE 754 double precision), $e^{-\\lambda_1 t}$ will evaluate to $0$. Since $\\lambda_2  \\lambda_1$, $e^{-\\lambda_2 t}$ will also be $0$. The denominator $w_1+w_2$ will then be $0$, causing a failure. Therefore, this implementation is fundamentally not numerically stable. The sampling formula $X=t - \\frac{1}{\\lambda_K}\\log U'$ is mathematically correct, as $1-U'$ for $U' \\sim \\mathrm{Unif}(0,1)$ is equivalent to a fresh uniform variate, and $X = t - \\frac{1}{\\lambda_K} \\log(1-U')$ is the correct inverse transform sampling formula for the truncated exponential.\nVerdict: **Incorrect**. The component selection step is not numerically stable.\n\n**B. Work in the logarithmic domain for component selection: compute $\\ell_k=\\log \\pi_k - \\lambda_k t$, and sample the component index $K$ by the $\\mathrm{Gumbel}$-max method, i.e., draw $G_k \\stackrel{\\mathrm{iid}}{\\sim} \\mathrm{Gumbel}(0,1)$ and take $K=\\operatorname{argmax}_{k\\in\\{1,2\\}} \\{\\ell_k+G_k\\}$. Then sample from the truncated exponential as in option A. This avoids underflow in the selection step and is unbiased.**\n\nThis option proposes a sophisticated and stable method for the component selection step. The selection probabilities are $\\pi'_k \\propto \\exp(\\log \\pi_k - \\lambda_k t) = \\exp(\\ell_k)$. The Gumbel-max trick is a standard and exactly correct (unbiased) method for sampling from a categorical distribution whose probabilities are proportional to $\\exp(\\ell_k)$ for given log-weights $\\ell_k$. By working in the logarithmic domain, this method avoids computing the small exponential terms directly, thus preventing the underflow that plagues the naive method in option A. The values of $\\ell_k$ are large negative numbers, which are well-represented in floating-point systems. This method is therefore numerically stable and unbiased for the selection step.\nVerdict: **Correct**.\n\n**C. Rescale the post-truncation weights by factoring out the smallest rate: let $\\lambda_\\star=\\min\\{\\lambda_1,\\lambda_2\\}$ and compute unnormalized weights $w_k'=\\pi_k \\exp\\{-(\\lambda_k-\\lambda_\\star)t\\}$; normalize $w_k'$ to form selection probabilities. Then sample from the truncated exponential as in option A. This rescaling prevents underflow in the selection step and is unbiased.**\n\nThis option proposes a rescaling of weights. The new weights $w'_k$ are related to the original weights $w_k$ by $w'_k = w_k \\cdot e^{\\lambda_\\star t}$. Since we normalize by the sum, this common factor $e^{\\lambda_\\star t}$ cancels out, meaning the resulting probabilities are identical to $\\pi'_k$. The method is thus unbiased.\nNumerically, since $\\lambda_1 \\ll \\lambda_2$, we have $\\lambda_\\star=\\lambda_1$. The new weights are $w'_1 = \\pi_1 \\exp\\{-(\\lambda_1-\\lambda_1)t\\} = \\pi_1$ and $w'_2 = \\pi_2 \\exp\\{-(\\lambda_2-\\lambda_1)t\\}$. The term for the dominant component becomes a stable constant. The exponential term for component $2$ can still underflow to $0$ if $(\\lambda_2-\\lambda_1)t$ is large, but this is not a catastrophic failure; it correctly reflects that the probability of selecting component $2$ is negligible. This a version of the \"log-sum-exp\" trick implemented in the linear domain by factoring out the largest term. The method is numerically stable and unbiased.\nVerdict: **Correct**.\n\n**D. If $\\lambda_2 t$ is so large that $e^{-\\lambda_2 t}$ would underflow to $0$ in double precision, set the corresponding weight to $0$, renormalize the remaining weight(s), and proceed. The resulting bias is negligible and can be ignored.**\n\nThis option describes a practical but ultimately biased approximation. The statement \"The resulting bias is negligible\" is an admission that there is, in fact, a bias. A method is unbiased if its expectation is exactly the target quantity. By intentionally setting a non-zero (albeit tiny) weight to zero, the sampling distribution is altered, and the method becomes biased. A meticulous analysis must distinguish between an algorithm that is mathematically exact (like B and C) versus one that is an explicit approximation (like D). While the bias is indeed extremely small and insignificant for most practical purposes, the procedure is not strictly \"unbiased\" as required by the question. Furthermore, this prescription is incomplete; it does not specify what to do if the weights for *all* components underflow, a scenario that methods B and C handle gracefully.\nVerdict: **Incorrect**. The method is explicitly, if negligibly, biased.\n\n**E. In the truncated sampling step, to preserve small increments when $t \\gg \\frac{1}{\\lambda_K}$, compute the increment as $Y = -\\frac{1}{\\lambda_K}\\log\\!\\big(1-U\\big)$ but evaluate the logarithm with the compensated function $\\log1p(-U)$ to avoid subtractive cancellation, i.e., set $Y = -\\frac{1}{\\lambda_K}\\,\\log1p(-U)$ with $U\\sim \\mathrm{Unif}(0,1)$. If high relative accuracy near $t$ is needed, return the sample as the two-part representation $(t,Y)$ rather than the rounded floating-point sum $t+Y$. This maintains the law and improves numerical robustness for large $t$.**\n\nThis option addresses numerical issues in the second step of the composition method: generating the sample $X = t+Y$.\nFirst, it proposes generating the exponential increment $Y$ via the formula $Y = -\\frac{1}{\\lambda_K} \\log(1-U)$. This is mathematically equivalent to the standard $Y = -\\frac{1}{\\lambda_K} \\log(U)$, as $U$ and $1-U$ are identically distributed. To generate a small increment $Y$, one needs $1-U$ to be close to $1$, meaning $U$ must be close to $0$. The option correctly suggests computing $\\log(1-U)$ via the function `log1p(-U)`. The `log1p(x)` function computes $\\log(1+x)$ accurately for small $|x|$, thus `log1p(-U)` accurately computes $\\log(1-U)$ for small $U$. This is a numerically robust technique.\nSecond, it correctly identifies that for large $t$, the floating-point addition $t+Y$ can suffer from \"swamping\", where the small increment $Y$ is lost due to rounding. Proposing to return the result as an unevaluated sum $(t,Y)$ is a standard and effective technique to preserve full precision. This option describes two valid and robust numerical improvements that \"maintain the law\" (i.e., are unbiased) and enhance stability.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{BCE}$$"
        },
        {
            "introduction": "The final step in developing a reliable sampler is rigorous validation: how can you prove your implementation is correct? This practice provides a complete blueprint for a unit-testing strategy, guiding you to use statistical goodness-of-fit tests to verify every aspect of the sampler's output. By implementing these tests , you will develop the critical skill of empirically validating that your simulated data faithfully represents the target theoretical distribution.",
            "id": "3351387",
            "problem": "You are tasked with designing and coding a unit-testing strategy for validating a composition sampler for mixture distributions using statistical tests on simulated data. The composition method for variate generation constructs a random variable $X$ by first selecting a discrete component index $I$ according to a categorical distribution with weights $\\{w_i\\}_{i=1}^k$ (where $\\sum_{i=1}^k w_i = 1$ and $w_i \\ge 0$), and then drawing $X \\mid (I=i)$ from the component distribution with probability density function $f_i(x)$ and cumulative distribution function $F_i(x)$. The resulting mixture distribution has density $f(x) = \\sum_{i=1}^k w_i f_i(x)$ and cumulative distribution function $F(x) = \\sum_{i=1}^k w_i F_i(x)$.\n\nStarting from the fundamental definitions of a categorical distribution, multinomial counting for independent trials, and cumulative distribution function properties, you must implement unit tests that validate both parts of the composition method:\n- The component selection mechanism, which should match the specified weights in frequency over many samples.\n- The conditional draws given the selected component, which should match the specified component distributions.\n\nAdditionally, validate the overall mixture distribution against its theoretical cumulative distribution function.\n\nYour program must implement the following statistical tests to provide objective acceptance criteria:\n1. A chi-square goodness-of-fit test for the component selection counts. Let $N$ be the total number of simulated samples, let $C_i$ be the observed count of selections of component $i$, and let $E_i = N w_i$ be the expected count. For all indices with $E_i  0$, compute the chi-square test statistic $\\chi^2 = \\sum_{i: E_i0} \\frac{(C_i - E_i)^2}{E_i}$ and the corresponding $p$-value against the chi-square distribution with $k' - 1$ degrees of freedom, where $k'$ is the number of indices with $E_i0$. Accept the selection mechanism if the $p$-value exceeds a significance level $\\alpha$.\n2. One-sample Kolmogorov–Smirnov tests for the conditional distributions. For each component $i$ with $w_i  0$, let $\\{X_j : I_j=i\\}$ be the subsample, and test that this subsample follows $F_i(x)$ using the Kolmogorov–Smirnov statistic $D_i = \\sup_x \\left| \\hat{F}_i(x) - F_i(x) \\right|$, where $\\hat{F}_i$ is the empirical cumulative distribution function of the subsample. Accept each conditional distribution if its $p$-value exceeds $\\alpha$; accept the entire conditional suite for a test case if all components with $w_i  0$ are accepted.\n3. A one-sample Kolmogorov–Smirnov test against the full mixture cumulative distribution $F(x) = \\sum_{i=1}^k w_i F_i(x)$. Accept if the $p$-value exceeds $\\alpha$.\n4. A zero-weight constraint check. For any component with $w_i = 0$, assert that $C_i = 0$ exactly. Accept if this holds for all such components.\n\nYou must implement a reusable composition sampler that supports the following component families:\n- Normal (Gaussian): $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ with parameters $\\mu$ and $\\sigma$.\n- Exponential: $X \\sim \\text{Exponential}(\\lambda)$ with rate parameter $\\lambda$, where $F(x) = 1 - e^{-\\lambda x}$ for $x \\ge 0$ and $F(x) = 0$ for $x  0$.\n\nUse a fixed random seed to ensure reproducibility, and use a single significance level $\\alpha = 10^{-6}$ for all tests.\n\nImplement the following test suite of parameter sets. For each test case, simulate $N$ samples and perform the four validations listed above:\n\n- Test Case A (Normal mixture, two components):\n  - $k = 2$.\n  - Weights: $w = [0.3, 0.7]$.\n  - Components: $\\mathcal{N}(\\mu_1=0, \\sigma_1=1)$ and $\\mathcal{N}(\\mu_2=3, \\sigma_2=0.5)$.\n  - Sample size: $N = 100000$.\n\n- Test Case B (Exponential mixture, three components):\n  - $k = 3$.\n  - Weights: $w = [0.2, 0.5, 0.3]$.\n  - Components: $\\text{Exponential}(\\lambda_1=1.0)$, $\\text{Exponential}(\\lambda_2=2.0)$, and $\\text{Exponential}(\\lambda_3=0.5)$.\n  - Sample size: $N = 100000$.\n\n- Test Case C (Normal mixture, near-zero weight component):\n  - $k = 2$.\n  - Weights: $w = [0.999, 0.001]$.\n  - Components: $\\mathcal{N}(\\mu_1=-2, \\sigma_1=0.8)$ and $\\mathcal{N}(\\mu_2=10, \\sigma_2=1.2)$.\n  - Sample size: $N = 200000$.\n\n- Test Case D (Normal mixture, zero-weight component):\n  - $k = 2$.\n  - Weights: $w = [1.0, 0.0]$.\n  - Components: $\\mathcal{N}(\\mu_1=0, \\sigma_1=1)$ and $\\mathcal{N}(\\mu_2=5, \\sigma_2=1)$.\n  - Sample size: $N = 100000$.\n\nYour program must produce a single line of output containing the aggregated boolean pass/fail results of the four validations for each of the four test cases, in order A, B, C, D. For each test case, output four booleans in the order: selection frequency test, conditional distribution tests, mixture distribution test, zero-weight constraint check. Aggregate these across all test cases into a single comma-separated list enclosed in square brackets. For example, the output format must be exactly:\n\"[$b_{A,1},$b_{A,2},$b_{A,3},$b_{A,4},$b_{B,1},$b_{B,2},$b_{B,3},$b_{B,4},$b_{C,1},$b_{C,2},$b_{C,3},$b_{C,4},$b_{D,1},$b_{D,2},$b_{D,3},$b_{D,4}]$\", where each $b_{\\cdot,\\cdot}$ is either the boolean value True or False.",
            "solution": "The exercise requires the design and implementation of a comprehensive unit-testing framework for a composition sampler, a fundamental algorithm in stochastic simulation. The scientific validity of any simulation study rests upon the correctness of its random number generation, and this problem formalizes the validation process using established statistical hypothesis tests. The solution is structured around two main parts: first, a robust implementation of the composition sampler itself, and second, a suite of four distinct validation procedures that test different aspects of the sampler's output.\n\nA random variable $X$ is said to follow a mixture distribution if its probability density function (PDF) $f(x)$ is a weighted sum of other density functions $f_i(x)$:\n$$f(x) = \\sum_{i=1}^{k} w_i f_i(x)$$\nwhere $k$ is the number of components, $\\{w_i\\}_{i=1}^k$ are the non-negative weights that sum to unity ($\\sum w_i = 1, w_i \\ge 0$), and each $f_i(x)$ is a component PDF. The cumulative distribution function (CDF) is similarly a weighted sum:\n$$F(x) = \\sum_{i=1}^{k} w_i F_i(x)$$\nThe composition method generates a variate from this distribution in two stages:\n1.  A component index $I$ is drawn from a categorical distribution with probabilities $P(I=i) = w_i$.\n2.  A random variate $X$ is drawn from the selected component distribution with CDF $F_I(x)$.\n\nOur implementation encapsulates this logic in a reusable sampler. To generate a dataset of size $N$, we first generate $N$ independent draws of the index $I$ using the specified weights $w_i$. This is efficiently accomplished using `numpy.random.choice`. Then, for each component index $i \\in \\{1, \\dots, k\\}$, we identify the subset of trials where $I=i$ and generate the required number of variates from the corresponding distribution, $f_i(x)$, using `numpy.random` functions for the Normal and Exponential families. The parameterization must be handled carefully: for an Exponential distribution with rate $\\lambda$, the `scale` parameter in both `numpy` and `scipy` is $1/\\lambda$.\n\nThe validation of this sampler is multifaceted, addressing four distinct properties of the generated data. A fixed random seed is employed to ensure the entire process is deterministic and reproducible. A stringent significance level of $\\alpha = 10^{-6}$ is used for all hypothesis tests to minimize the probability of Type I errors (i.e., falsely rejecting a correct sampler).\n\n1.  **Component Selection Frequencies (Chi-Square Test)**: The first test validates the component selection mechanism. Over a large number of trials $N$, the observed count $C_i$ for each component $i$ should be close to its expected count, $E_i = N w_i$. This is a goodness-of-fit problem, for which the Pearson's chi-square test is appropriate. The test statistic is calculated as:\n    $$\\chi^2 = \\sum_{i: E_i  0} \\frac{(C_i - E_i)^2}{E_i}$$\n    This statistic is compared against a chi-square distribution with $k' - 1$ degrees of freedom, where $k'$ is the number of components with a non-zero expected count ($E_i  0$). The null hypothesis, stating that the observed counts follow the expected distribution, is accepted if the resulting $p$-value is greater than $\\alpha$. The `scipy.stats.chi2.sf` function is used to compute this $p$-value.\n\n2.  **Conditional Distributions (Kolmogorov-Smirnov Tests)**: The second test verifies that the variates for each component are drawn from the correct conditional distribution. For each component $i$ with $w_i  0$, we isolate the subsample of variates $\\{X_j \\mid I_j=i\\}$ and perform a one-sample Kolmogorov-Smirnov (KS) test. The KS test compares the empirical CDF of this subsample, $\\hat{F}_i(x)$, against the theoretical component CDF, $F_i(x)$. The test statistic $D_i = \\sup_x |\\hat{F}_i(x) - F_i(x)|$ gives rise to a $p$-value. The null hypothesis is that the subsample is drawn from $F_i(x)$. The entire suite of conditional distributions is deemed correctly implemented only if the $p$-value for *every* component with $w_i  0$ exceeds $\\alpha$. This is implemented using `scipy.stats.kstest`, providing it with the appropriate theoretical CDF callable.\n\n3.  **Overall Mixture Distribution (Kolmogorov-Smirnov Test)**: The third test validates the final, aggregated output of the sampler. The entire sample of $N$ variates is tested against the theoretical mixture CDF, $F(x) = \\sum_{i=1}^k w_i F_i(x)$. A single one-sample KS test is performed. A callable function representing $F(x)$ is constructed, which computes the weighted sum of the component CDFs. The null hypothesis, that the generated sample follows the target mixture distribution, is accepted if the $p$-value from `scipy.stats.kstest` is greater than $\\alpha$.\n\n4.  **Zero-Weight Constraint**: The final validation is a deterministic check, not a statistical test. For any component $i$ with a specified weight of $w_i = 0$, it is a logical necessity that it is never selected. Therefore, the observed count $C_i$ must be exactly $0$. The test passes if this condition holds for all components with zero weight. If no components have zero weight, the test passes vacuously.\n\nThe program systematically applies these four tests to each of the specified test cases (A, B, C, D), which are designed to probe different aspects of the sampler's behavior, including balanced and skewed weights, different distribution families, and the crucial zero-weight scenario. The boolean result (pass/fail) of each of the four tests for each of the four cases is aggregated into a final list.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2, norm, expon, kstest\n\ndef solve():\n    \"\"\"\n    Implements and validates a composition sampler for mixture distributions.\n    \"\"\"\n    \n    # Global parameters for the test suite\n    SEED = 42\n    ALPHA = 1e-6\n    rng = np.random.default_rng(SEED)\n\n    # Test case definitions\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"N\": 100000,\n            \"weights\": np.array([0.3, 0.7]),\n            \"components\": [\n                {\"dist\": \"norm\", \"params\": {\"loc\": 0.0, \"scale\": 1.0}},\n                {\"dist\": \"norm\", \"params\": {\"loc\": 3.0, \"scale\": 0.5}},\n            ]\n        },\n        {\n            \"name\": \"B\",\n            \"N\": 100000,\n            \"weights\": np.array([0.2, 0.5, 0.3]),\n            \"components\": [\n                {\"dist\": \"expon\", \"params\": {\"rate\": 1.0}},\n                {\"dist\": \"expon\", \"params\": {\"rate\": 2.0}},\n                {\"dist\": \"expon\", \"params\": {\"rate\": 0.5}},\n            ]\n        },\n        {\n            \"name\": \"C\",\n            \"N\": 200000,\n            \"weights\": np.array([0.999, 0.001]),\n            \"components\": [\n                {\"dist\": \"norm\", \"params\": {\"loc\": -2.0, \"scale\": 0.8}},\n                {\"dist\": \"norm\", \"params\": {\"loc\": 10.0, \"scale\": 1.2}},\n            ]\n        },\n        {\n            \"name\": \"D\",\n            \"N\": 100000,\n            \"weights\": np.array([1.0, 0.0]),\n            \"components\": [\n                {\"dist\": \"norm\", \"params\": {\"loc\": 0.0, \"scale\": 1.0}},\n                {\"dist\": \"norm\", \"params\": {\"loc\": 5.0, \"scale\": 1.0}},\n            ]\n        },\n    ]\n\n    def composition_sampler(n_samples, weights, components, specific_rng):\n        \"\"\"\n        Generates samples from a mixture distribution using the composition method.\n        \"\"\"\n        k = len(weights)\n        indices = specific_rng.choice(np.arange(k), size=n_samples, p=weights)\n        samples = np.zeros(n_samples, dtype=float)\n\n        for i in range(k):\n            mask = (indices == i)\n            count = np.sum(mask)\n            if count  0:\n                comp_def = components[i]\n                if comp_def[\"dist\"] == \"norm\":\n                    samples[mask] = specific_rng.normal(size=count, **comp_def[\"params\"])\n                elif comp_def[\"dist\"] == \"expon\":\n                    rate = comp_def[\"params\"][\"rate\"]\n                    samples[mask] = specific_rng.exponential(size=count, scale=1.0 / rate)\n        return samples, indices\n\n    all_results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        weights = case[\"weights\"]\n        components = case[\"components\"]\n        k = len(weights)\n\n        samples, indices = composition_sampler(N, weights, components, rng)\n        observed_counts = np.bincount(indices, minlength=k)\n        \n        # Test 1: Chi-square test for component selection frequency\n        expected_counts = N * weights\n        positive_mask = expected_counts  0\n        chisq_passed = True\n        if np.any(positive_mask):\n            df = int(np.sum(positive_mask)) - 1\n            if df  0:\n                chisq_stat = np.sum(\n                    (observed_counts[positive_mask] - expected_counts[positive_mask])**2\n                    / expected_counts[positive_mask]\n                )\n                p_val_chisq = chi2.sf(chisq_stat, df)\n                chisq_passed = p_val_chisq  ALPHA\n\n        # Test 2: KS tests for conditional distributions\n        cond_ks_passed = True\n        for i in range(k):\n            if weights[i]  0:\n                sub_samples = samples[indices == i]\n                if len(sub_samples)  0:\n                    comp_def = components[i]\n                    if comp_def[\"dist\"] == \"norm\":\n                        scipy_cdf = lambda x, mu=comp_def[\"params\"][\"loc\"], s=comp_def[\"params\"][\"scale\"]: norm.cdf(x, loc=mu, scale=s)\n                    elif comp_def[\"dist\"] == \"expon\":\n                        rate = comp_def[\"params\"][\"rate\"]\n                        scipy_cdf = lambda x, r=rate: expon.cdf(x, scale=1.0/r)\n                    \n                    _, p_val = kstest(sub_samples, scipy_cdf)\n                    if p_val = ALPHA:\n                        cond_ks_passed = False\n                        break\n        \n        # Test 3: KS test for the full mixture distribution\n        def mixture_cdf(x):\n            total_cdf = np.zeros_like(np.asarray(x), dtype=float)\n            for i in range(k):\n                if weights[i]  0:\n                    comp_def = components[i]\n                    if comp_def[\"dist\"] == \"norm\":\n                        total_cdf += weights[i] * norm.cdf(x, **comp_def[\"params\"])\n                    elif comp_def[\"dist\"] == \"expon\":\n                        rate = comp_def[\"params\"][\"rate\"]\n                        total_cdf += weights[i] * expon.cdf(x, scale=1.0/rate)\n            return total_cdf\n        \n        _, p_val_mix = kstest(samples, mixture_cdf)\n        mix_ks_passed = p_val_mix  ALPHA\n\n        # Test 4: Zero-weight constraint check\n        zero_weight_passed = True\n        zero_weight_indices = np.where(weights == 0)[0]\n        if len(zero_weight_indices)  0:\n            if np.any(observed_counts[zero_weight_indices] != 0):\n                zero_weight_passed = False\n\n        all_results.extend([chisq_passed, cond_ks_passed, mix_ks_passed, zero_weight_passed])\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}