{
    "hands_on_practices": [
        {
            "introduction": "Our practical exploration begins with the most intuitive case: sampling uniformly from the unit circle, $S^1$, in the plane $\\mathbb{R}^2$. This exercise is foundational, as it directly connects the abstract concept of rotational invariance to a simple, concrete algorithm. By deriving the distribution of the polar angle and the resulting marginal distribution of a single coordinate, you will build the essential mathematical and computational skills needed for more complex, higher-dimensional problems .",
            "id": "3337220",
            "problem": "Consider a random vector $U=(U_1,U_2)$ that is uniformly distributed on the unit circle in $\\mathbb{R}^2$, that is, $U$ takes values in the unit sphere $\\mathbb{S}^1=\\{(x_1,x_2)\\in\\mathbb{R}^2:\\,x_1^2+x_2^2=1\\}$ with a distribution that is invariant under rotations about the origin. Let $\\Theta$ denote the polar angle in radians. The following tasks must be completed from first principles using only core definitions, the rotational invariance of the uniform distribution on $\\mathbb{S}^1$, and the change-of-variables formula.\n\n- Derive the probability density function of the first coordinate $U_1$.\n- Construct an exact sampling algorithm for $U$ that uses a single draw from the uniform distribution on an interval of angles measured in radians, and provide a clear mathematical justification that the output is exactly uniform on $\\mathbb{S}^1$.\n\nYour program must implement:\n- A sampler that generates independent samples of $U$ using a single uniform angle in radians.\n- Functions that evaluate the probability density function and cumulative distribution function of the marginal distribution of $U_1$ that you derived.\n- A test suite that verifies logical and numerical properties implied by your derivation and algorithm.\n\nAngle unit requirement: all angles must be handled in radians.\n\nTest suite and parameter values:\n1. Deterministic mapping sanity check: For angles $\\{0,\\tfrac{\\pi}{2},\\pi,\\tfrac{3\\pi}{2}\\}$, verify that mapping to $\\mathbb{S}^1$ via $(\\cos\\theta,\\sin\\theta)$ yields $\\{(1,0),(0,1),(-1,0),(0,-1)\\}$ to within absolute tolerance $10^{-12}$. Report a boolean indicating whether all four cases pass simultaneously.\n2. Unit-norm check (boundary condition): Generate $n=5$ samples of $U$ and verify that each has Euclidean norm exactly $1$ to within absolute tolerance $10^{-12}$. Report a single boolean that is true only if all $5$ samples pass.\n3. Density normalization: Numerically integrate the derived density of $U_1$ over $[-1,1]$ and verify that the integral equals $1$ within absolute tolerance $10^{-9}$. Report a boolean.\n4. Moment check for the first coordinate (happy path): Generate $n=100000$ independent samples of $U$. Let $\\widehat{m}_1$ be the empirical mean of $U_1$ and $\\widehat{m}_2$ the empirical second moment of $U_1$. Verify simultaneously that $|\\widehat{m}_1-0|\\le 5\\times 10^{-3}$ and $|\\widehat{m}_2-\\tfrac{1}{2}|\\le 10^{-2}$. Report a single boolean that is true only if both inequalities hold.\n5. Distributional check for the angle: Using $n=5000$ samples, recover angles $\\widehat{\\Theta}\\in[0,2\\pi)$ from the sampled points via the two-argument arctangent and test the null hypothesis that $\\widehat{\\Theta}/(2\\pi)$ is uniform on $[0,1]$ using the Kolmogorov–Smirnov test at significance level $\\alpha=0.05$. Report a boolean that is true if and only if the $p$-value is at least $\\alpha$.\n6. Distributional check for $U_1$: Using $n=5000$ samples, test the null hypothesis that the empirical distribution of $U_1$ matches the derived marginal distribution of $U_1$ using the Kolmogorov–Smirnov test at significance level $\\alpha=0.05$. Report a boolean that is true if and only if the $p$-value is at least $\\alpha$.\n\nFinal output format: Your program should produce a single line of output containing the six boolean results for the tests above as a comma-separated list enclosed in square brackets (for example, $[{\\rm True},{\\rm False},{\\rm True},{\\rm True},{\\rm True},{\\rm True}]$). No other output is permitted.",
            "solution": "The uniform distribution of a random vector $U=(U_1, U_2)$ on the unit circle $\\mathbb{S}^1 = \\{(x_1, x_2) \\in \\mathbb{R}^2 : x_1^2 + x_2^2 = 1\\}$ is defined by its rotational invariance. This principle mandates that the probability of $U$ falling into any measurable subset $A \\subset \\mathbb{S}^1$ remains the same if the set is rotated. Formally, $P(U \\in A) = P(U \\in R_\\phi(A))$ for any rotation operator $R_\\phi$ corresponding to an angle $\\phi$. This directly implies that the probability measure is proportional to the arc length on $\\mathbb{S}^1$.\n\nFirst, we construct the sampling algorithm. Any point on $\\mathbb{S}^1$ can be parameterized in polar coordinates $(r, \\theta)$ with radius $r=1$ and a polar angle $\\Theta$. We can define the angle to be in the interval $[0, 2\\pi)$. The Cartesian coordinates are then given by the transformation $U = (U_1, U_2) = (\\cos\\Theta, \\sin\\Theta)$, where $\\Theta$ is a random variable. A rotation of the vector $U$ by a fixed angle $\\phi$ results in a new vector $U'$ whose angle is $(\\Theta + \\phi) \\pmod{2\\pi}$. The rotational invariance of the distribution of $U$ thus requires that the distribution of $\\Theta$ be invariant under addition of a constant modulo $2\\pi$. The only probability distribution on a finite interval with this property is the continuous uniform distribution. Consequently, the angle $\\Theta$ must be uniformly distributed on $[0, 2\\pi)$. Its probability density function (PDF) is $f_\\Theta(\\theta) = \\frac{1}{2\\pi}$ for $\\theta \\in [0, 2\\pi)$, and $f_\\Theta(\\theta)=0$ otherwise.\n\nThis reasoning leads to an exact sampling algorithm for generating a point $U$ uniformly on $\\mathbb{S}^1$:\n$1$. Generate a random variate $\\Theta$ from the uniform distribution on the interval $[0, 2\\pi)$, denoted as $\\Theta \\sim \\mathcal{U}[0, 2\\pi)$.\n$2$. Construct the vector $U$ using the deterministic mapping: $U = (U_1, U_2) = (\\cos\\Theta, \\sin\\Theta)$.\nThis algorithm is exact because the uniform distribution of the angle $\\Theta$ corresponds directly to a distribution that is uniform with respect to the arc length for the point $U$ on the circle.\n\nNext, we derive the marginal PDF of the first coordinate, $U_1$. Let the random variable be $X = U_1 = \\cos\\Theta$. The support of $X$ is the interval $[-1, 1]$. We use the change-of-variables method by first finding the cumulative distribution function (CDF) of $X$, denoted by $F_X(x) = P(X \\le x)$. For any $x \\in [-1, 1]$, we have:\n$$F_X(x) = P(\\cos\\Theta \\le x)$$\nTo evaluate this probability, we must identify the set of angles $\\theta \\in [0, 2\\pi)$ for which the inequality $\\cos\\theta \\le x$ is satisfied. The function $\\cos\\theta$ is monotonically decreasing on $[0, \\pi]$ and monotonically increasing on $[\\pi, 2\\pi]$. The equation $\\cos\\theta = x$ has two solutions in the interval $[0, 2\\pi)$: $\\theta_1 = \\arccos(x)$ and $\\theta_2 = 2\\pi - \\arccos(x)$, where $\\arccos: [-1,1] \\to [0,\\pi]$ is the principal branch of the inverse cosine function. The inequality $\\cos\\theta \\le x$ holds for angles $\\theta$ within the closed interval $[\\theta_1, \\theta_2] = [\\arccos(x), 2\\pi - \\arccos(x)]$.\nSince $\\Theta$ is uniformly distributed on $[0, 2\\pi)$, the probability is the ratio of the length of this interval to the total length $2\\pi$:\n$$F_X(x) = \\frac{(2\\pi - \\arccos(x)) - \\arccos(x)}{2\\pi} = \\frac{2\\pi - 2\\arccos(x)}{2\\pi} = 1 - \\frac{\\arccos(x)}{\\pi}$$\nThe complete CDF for $X$ is therefore:\n$$\nF_X(x) = \\begin{cases}\n0 & \\text{if } x < -1 \\\\\n1 - \\frac{\\arccos(x)}{\\pi} & \\text{if } -1 \\le x \\le 1 \\\\\n1 & \\text{if } x > 1\n\\end{cases}\n$$\nThe PDF, $f_X(x)$, is obtained by differentiating the CDF with respect to $x$. For $x \\in (-1, 1)$, we have:\n$$f_X(x) = \\frac{d}{dx} F_X(x) = \\frac{d}{dx} \\left(1 - \\frac{\\arccos(x)}{\\pi}\\right) = -\\frac{1}{\\pi} \\frac{d}{dx}(\\arccos(x))$$\nUsing the standard derivative $\\frac{d}{dx}\\arccos(x) = -\\frac{1}{\\sqrt{1-x^2}}$, we find the PDF:\n$$f_X(x) = -\\frac{1}{\\pi} \\left(-\\frac{1}{\\sqrt{1-x^2}}\\right) = \\frac{1}{\\pi\\sqrt{1-x^2}}$$\nThe full PDF for $U_1$ is thus:\n$$\nf_{U_1}(x) = \\begin{cases}\n\\frac{1}{\\pi\\sqrt{1-x^2}} & \\text{if } x \\in (-1, 1) \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThis completes the required derivations from first principles.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats, integrate\n\n# --- Auxiliary Functions based on Derivation ---\n\ndef sampler(n_samples: int, seed: int = None) -> np.ndarray:\n    \"\"\"Generates n_samples of a 2D random vector uniformly on the unit circle.\"\"\"\n    rng = np.random.default_rng(seed)\n    # Sample angle uniformly from [0, 2*pi)\n    thetas = rng.uniform(0, 2 * np.pi, n_samples)\n    # Map angle to a point on the unit circle\n    samples = np.stack([np.cos(thetas), np.sin(thetas)], axis=1)\n    return samples\n\ndef pdf_U1(x: np.ndarray) -> np.ndarray:\n    \"\"\"Evaluates the PDF of the first coordinate U1.\"\"\"\n    x = np.asarray(x)\n    # Initialize result array with zeros\n    result = np.zeros_like(x, dtype=float)\n    # PDF is non-zero only for x in (-1, 1)\n    mask = (x > -1) & (x < 1)\n    # Calculate PDF value for valid inputs\n    result[mask] = 1 / (np.pi * np.sqrt(1 - x[mask]**2))\n    # Handle scalar input case\n    if result.ndim == 0:\n        return result.item()\n    return result\n\ndef cdf_U1(x: np.ndarray) -> np.ndarray:\n    \"\"\"Evaluates the CDF of the first coordinate U1.\"\"\"\n    x = np.asarray(x)\n    result = np.zeros_like(x, dtype=float)\n    \n    # CDF is 0 for x < -1\n    result[x < -1] = 0.0\n    # CDF is 1 for x > 1\n    result[x > 1] = 1.0\n    # For x in [-1, 1], CDF is 1 - arccos(x)/pi\n    mask = (x >= -1) & (x <= 1)\n    result[mask] = 1 - np.arccos(x[mask]) / np.pi\n    \n    # Handle scalar input case\n    if result.ndim == 0:\n        return result.item()\n    return result\n\n# --- Test Suite ---\n\ndef test_1_deterministic_mapping() -> bool:\n    \"\"\"Tests if specific angles map to the correct points.\"\"\"\n    angles = np.array([0, np.pi / 2, np.pi, 3 * np.pi / 2])\n    expected_points = np.array([[1., 0.], [0., 1.], [-1., 0.], [0., -1.]])\n    points = np.stack([np.cos(angles), np.sin(angles)], axis=1)\n    return np.allclose(points, expected_points, atol=1e-12, rtol=0)\n\ndef test_2_unit_norm() -> bool:\n    \"\"\"Tests if generated samples have a unit Euclidean norm.\"\"\"\n    n_samples = 5\n    samples = sampler(n_samples, seed=1)\n    norms = np.linalg.norm(samples, axis=1)\n    return np.all(np.isclose(norms, 1.0, atol=1e-12, rtol=0))\n\ndef test_3_density_normalization() -> bool:\n    \"\"\"Tests if the integral of the derived PDF over its support is 1.\"\"\"\n    # Define the core function for integration\n    integrand = lambda x: 1 / (np.pi * np.sqrt(1 - x**2))\n    integral_val, _ = integrate.quad(integrand, -1, 1)\n    return np.isclose(integral_val, 1.0, atol=1e-9, rtol=0)\n\ndef test_4_moment_check() -> bool:\n    \"\"\"Tests the empirical mean and second moment of U1.\"\"\"\n    n_samples = 100000\n    samples = sampler(n_samples, seed=2)\n    u1_samples = samples[:, 0]\n    \n    m1_hat = np.mean(u1_samples)\n    m2_hat = np.mean(u1_samples**2)\n    \n    check_m1 = np.abs(m1_hat - 0) <= 5e-3\n    check_m2 = np.abs(m2_hat - 0.5) <= 1e-2\n    return check_m1 and check_m2\n\ndef test_5_angle_distribution() -> bool:\n    \"\"\"Tests if the recovered angles follow a uniform distribution.\"\"\"\n    n_samples = 5000\n    alpha = 0.05\n    samples = sampler(n_samples, seed=3)\n    \n    # Recover angles --> range (-pi, pi]\n    recovered_thetas = np.arctan2(samples[:, 1], samples[:, 0])\n    # Shift to [0, 2*pi)\n    recovered_thetas[recovered_thetas < 0] += 2 * np.pi\n    # Normalize to [0, 1) for KS test against U[0, 1]\n    normalized_thetas = recovered_thetas / (2 * np.pi)\n    \n    _, p_value = stats.kstest(normalized_thetas, 'uniform')\n    return p_value >= alpha\n\ndef test_6_coordinate_distribution() -> bool:\n    \"\"\"Tests if the empirical distribution of U1 matches the derived CDF.\"\"\"\n    n_samples = 5000\n    alpha = 0.05\n    samples = sampler(n_samples, seed=4)\n    u1_samples = samples[:, 0]\n    \n    _, p_value = stats.kstest(u1_samples, cdf_U1)\n    return p_value >= alpha\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Using fixed seeds in the test functions for deterministic results.\n    test_results = [\n        test_1_deterministic_mapping(),\n        test_2_unit_norm(),\n        test_3_density_normalization(),\n        test_4_moment_check(),\n        test_5_angle_distribution(),\n        test_6_coordinate_distribution()\n    ]\n\n    print(f\"[{','.join(map(str, test_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having mastered the unit circle, we now generalize to higher dimensions, starting with the sphere $S^2$ in $\\mathbb{R}^3$ and then extending to any dimension $d$. This practice will guide you through deriving the correct angular distributions for $S^2$ and the general law for a coordinate's marginal distribution on $S^{d-1}$, revealing its connection to the Beta distribution. You will also implement the powerful and widely used Gaussian normalization method, a cornerstone technique for sphere sampling that elegantly bypasses the complexities of hyperspherical coordinates .",
            "id": "3337198",
            "problem": "You are tasked with designing, analyzing, and empirically validating algorithms for sampling random vectors uniformly on the unit sphere in $\\mathbb{R}^d$, denoted by $S^{d-1} = \\{x \\in \\mathbb{R}^d : \\|x\\|_2 = 1\\}$. Your work must begin from fundamental definitions: the surface area measure on $S^{d-1}$, rotational invariance, and the Jacobian of spherical coordinate transformations. No shortcut formulas may be assumed. All angles must be expressed in radians.\n\nYour program must implement the following, with rigorous derivations supporting each step:\n\n- Part A (Derivation on $S^2$): Starting from the rotational invariance of the uniform distribution on $S^2$ and the definition of spherical coordinates, derive the distribution of the zenith angle $\\theta \\in [0,\\pi]$ and azimuthal angle $\\phi \\in [0,2\\pi)$. Use this to derive an inverse-transform algorithm that samples points uniformly on $S^2$ by first sampling $\\phi$ and a suitable function of $\\theta$ from simple base distributions, and then computing the Cartesian coordinates.\n\n- Part B (Generalization to $S^{d-1}$): Let $X = (X_1,\\ldots,X_d)$ be uniformly distributed on $S^{d-1}$. Using first principles and the co-area (or Jacobian) argument for the surface measure on slices of $S^{d-1}$, derive the probability density function of $X_1$ on $[-1,1]$. Prove that the transformed variable $U = (X_1+1)/2$ follows a Beta distribution with symmetric parameters depending on $d$; identify these parameters explicitly as a function of $d$. Use this result to formulate a scalar inverse-transform step for $X_1$ if needed.\n\n- Part C (Independence of Angular Components): Introduce hyperspherical coordinates $(\\theta_1,\\ldots,\\theta_{d-2},\\phi)$ for $S^{d-1}$ via a standard $d$-dimensional spherical coordinate map. Derive the Jacobian determinant and use it to obtain the joint density of $(\\theta_1,\\ldots,\\theta_{d-2},\\phi)$. Determine whether these angular coordinates are mutually independent under the uniform distribution on $S^{d-1}$, and justify your answer entirely from the factorization structure of the joint density.\n\n- Algorithms to implement:\n  1. An $S^2$ sampler using inverse-transform on angles derived in Part A that outputs points in Cartesian coordinates.\n  2. A general $S^{d-1}$ sampler for any integer $d \\ge 2$ using the Gaussian normalization method: sample $Y \\sim \\mathcal{N}(0,I_d)$ and return $Y/\\|Y\\|_2$.\n  3. A routine to extract hyperspherical angles $(\\theta_1,\\theta_2)$ from Cartesian samples in $\\mathbb{R}^5$ according to a consistent convention, with $\\theta_i \\in [0,\\pi]$.\n\n- Statistical tests to perform:\n  1. On $S^2$, test the independence of the azimuthal angle $\\phi \\in [0,2\\pi)$ and the variable $\\cos\\theta \\in [-1,1]$ using a chi-square test of independence applied to a two-dimensional histogram. Use evenly spaced binning in each domain and a significance level $\\alpha = 0.01$.\n  2. On $S^{4}$ (i.e., $d=5$), test the marginal law of $U=(X_1+1)/2$ against the Beta distribution from Part B using the Kolmogorov–Smirnov test at significance level $\\alpha = 0.01$.\n  3. On $S^{4}$, test the independence of $(\\theta_1,\\theta_2)$ via a chi-square test of independence on a two-dimensional histogram with evenly spaced bins on $[0,\\pi]\\times[0,\\pi]$, using significance level $\\alpha = 0.01$.\n  4. On $S^{1}$ (i.e., $d=2$), validate the symmetry of the first coordinate $X_1$ by testing the null hypothesis $\\mathbb{E}[X_1]=0$ using a two-sided one-sample $t$-test at significance level $\\alpha = 0.01$.\n\nYour program must use a fixed random seed internally for reproducibility and must produce only a single line of output as specified below. All angles must be handled in radians. All statistical decisions must be boolean, where a result is true if the corresponding $p$-value exceeds the specified $\\alpha$.\n\nTest suite and required output:\n- Use the following test cases, in the exact order, with the specified parameters:\n  - Test $1$: $S^2$ angular sampler, sample size $N=200000$, histogram bin counts $b_\\phi=12$ on $[0,2\\pi)$ and $b_c=12$ on $[-1,1]$, chi-square independence test between $\\phi$ and $\\cos\\theta$ at $\\alpha=0.01$. Output a boolean indicating whether independence is not rejected.\n  - Test $2$: $S^{4}$ via Gaussian normalization with $d=5$, sample size $N=250000$. Compute $U=(X_1+1)/2$ and perform a Kolmogorov–Smirnov test against the Beta law from Part B at $\\alpha=0.01$. Output a boolean indicating whether the null is not rejected.\n  - Test $3$: $S^{4}$ via Gaussian normalization with $d=5$, sample size $N=250000$, histogram bin counts $b_{\\theta}=10$ on $[0,\\pi]$ for both $\\theta_1$ and $\\theta_2$. Perform a chi-square independence test at $\\alpha=0.01$; output a boolean indicating whether independence is not rejected.\n  - Test $4$: $S^{1}$ via Gaussian normalization with $d=2$, sample size $N=400000$. Perform a two-sided one-sample $t$-test for $\\mathbb{E}[X_1]=0$ at $\\alpha=0.01$; output a boolean indicating whether the null is not rejected.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[true1,true2,true3,true4]\" where each \"truei\" is replaced by the boolean result for test $i$ in order. The booleans must be unadorned Python boolean literals without spaces, e.g., \"[True,False,True,True]\".",
            "solution": "The problem of generating uniformly distributed random vectors on the surface of a $d$-dimensional unit sphere, $S^{d-1}$, is a fundamental task in stochastic simulation, with applications ranging from statistical physics to computer graphics. This task requires a precise understanding of the geometry of spheres and the properties of uniform surface measure. The following sections provide rigorous derivations for the required algorithms and statistical properties, starting from first principles as stipulated.\n\n### Part A: Derivation of the Inverse-Transform Sampler for $S^2$\n\nA point on the unit sphere $S^2 \\subset \\mathbb{R}^3$ can be parameterized by spherical coordinates $(\\theta, \\phi)$, where $\\theta \\in [0, \\pi]$ is the polar (zenith) angle and $\\phi \\in [0, 2\\pi)$ is the azimuthal angle. A standard mapping to Cartesian coordinates $(x_1, x_2, x_3)$ is:\n$$\n\\begin{cases}\nx_1 = \\sin\\theta \\cos\\phi \\\\\nx_2 = \\sin\\theta \\sin\\phi \\\\\nx_3 = \\cos\\theta\n\\end{cases}\n$$\nThe uniform distribution on $S^2$ is defined with respect to the surface area element, $d\\sigma$. For the given parameterization, the Jacobian of the transformation from spherical to Cartesian coordinates leads to the surface element $d\\sigma = \\sin\\theta \\, d\\theta \\, d\\phi$. The total surface area of $S^2$ is $\\mathcal{A}(S^2) = \\int_0^{2\\pi} \\int_0^{\\pi} \\sin\\theta \\, d\\theta \\, d\\phi = 4\\pi$.\n\nThe probability density function (PDF) for a uniform distribution over the angles $(\\theta, \\phi)$ is therefore:\n$$\nf(\\theta, \\phi) = \\frac{1}{4\\pi} \\sin\\theta, \\quad \\text{for } \\theta \\in [0, \\pi], \\phi \\in [0, 2\\pi)\n$$\nTo derive an algorithm for sampling $(\\theta, \\phi)$, we examine their joint and marginal distributions. The joint PDF can be factored:\n$$\nf(\\theta, \\phi) = \\left( \\frac{1}{2\\pi} \\right) \\left( \\frac{1}{2}\\sin\\theta \\right) = f_\\Phi(\\phi) f_\\Theta(\\theta)\n$$\nThis factorization demonstrates that $\\Theta$ and $\\Phi$ are independent random variables.\nThe marginal PDF for $\\phi$ is $f_\\Phi(\\phi) = \\int_0^\\pi \\frac{1}{4\\pi} \\sin\\theta \\, d\\theta = \\frac{1}{2\\pi}$, which is the uniform distribution on $[0, 2\\pi)$. Thus, $\\phi$ can be sampled by generating $U_1 \\sim \\mathcal{U}[0,1]$ and setting $\\phi = 2\\pi U_1$.\n\nThe marginal PDF for $\\theta$ is $f_\\Theta(\\theta) = \\int_0^{2\\pi} \\frac{1}{4\\pi} \\sin\\theta \\, d\\phi = \\frac{1}{2}\\sin\\theta$ for $\\theta \\in [0,\\pi]$. We use the inverse transform sampling method. The cumulative distribution function (CDF) of $\\Theta$ is:\n$$\nF_\\Theta(\\theta') = P(\\Theta \\le \\theta') = \\int_0^{\\theta'} \\frac{1}{2}\\sin\\theta \\, d\\theta = \\frac{1}{2}[-\\cos\\theta]_0^{\\theta'} = \\frac{1}{2}(1 - \\cos\\theta')\n$$\nSetting $F_\\Theta(\\theta) = U_2$ where $U_2 \\sim \\mathcal{U}[0,1]$, we solve for $\\theta$:\n$$\nU_2 = \\frac{1}{2}(1 - \\cos\\theta) \\implies \\cos\\theta = 1 - 2U_2\n$$\nThe variable $C = 1 - 2U_2$ is uniformly distributed on $[-1, 1]$. It is computationally and conceptually more direct to sample the variable $C = \\cos\\theta$ rather than $\\theta$ itself. Let's sample $U'_2 \\sim\\mathcal{U}[0,1]$ and set $C = 2U'_2-1$, which is uniform on $[-1,1]$. Then we can set $c = \\cos\\theta$. The independence of $\\phi$ and $\\cos\\theta$ is demonstrated by transforming the joint density $f(\\theta,\\phi)$ to variables $(\\phi,c)$:\n$$\ng(\\phi, c) = f(\\theta(c), \\phi) \\left| \\frac{d\\theta}{dc} \\right| = \\frac{\\sin(\\arccos(c))}{4\\pi} \\left| \\frac{-1}{\\sqrt{1-c^2}} \\right| = \\frac{\\sqrt{1-c^2}}{4\\pi} \\frac{1}{\\sqrt{1-c^2}} = \\frac{1}{4\\pi}\n$$\nThe domain is $\\phi \\in [0,2\\pi)$ and $c \\in [-1,1]$, with area $4\\pi$. The constant density confirms that $(\\Phi, C)$ are jointly uniform and hence independent. The algorithm is:\n1. Sample $\\phi \\sim \\mathcal{U}[0, 2\\pi)$.\n2. Sample $c \\sim \\mathcal{U}[-1, 1]$.\n3. Compute Cartesian coordinates: $x_1 = \\sqrt{1-c^2}\\cos\\phi$, $x_2 = \\sqrt{1-c^2}\\sin\\phi$, $x_3 = c$.\n\n### Part B: Marginal Distribution of a Coordinate on $S^{d-1}$\n\nLet $X=(X_1, \\ldots, X_d)$ be a random vector uniformly distributed on $S^{d-1}$. By rotational symmetry, the marginal distribution of each coordinate $X_i$ is identical. We derive the PDF of $X_1$ for $x_1 \\in [-1, 1]$.\n\nThe probability of an event is proportional to the surface area it occupies on the sphere. The probability density $f_{X_1}(x_1)$ is proportional to the surface area of the $(d-2)$-dimensional sphere formed by slicing $S^{d-1}$ at a fixed value of the first coordinate, $x_1$. The equation for this slice is $x_1^2 + x_2^2 + \\dots + x_d^2 = 1$, which implies $\\sum_{i=2}^d x_i^2 = 1 - x_1^2$. This is a $(d-2)$-sphere of radius $r = \\sqrt{1-x_1^2}$.\n\nThe surface area of a $k$-sphere of radius $R$ is $\\mathcal{A}_k(R) = \\mathcal{A}_k R^k$, where $\\mathcal{A}_k = \\frac{2\\pi^{(k+1)/2}}{\\Gamma((k+1)/2)}$ is the area of $S^k$. The area of the slice is $\\mathcal{A}_{d-2}(r) = \\mathcal{A}_{d-2}(1-x_1^2)^{(d-2)/2}$. The infinitesimal surface area of a thin band on $S^{d-1}$ between $x_1$ and $x_1+dx_1$ is this slice area multiplied by the band's infinitesimal arc length width, $dl$. The arc length is related to $dx_1$ by $dl = dx_1/\\sqrt{1-x_1^2}$. This line of reasoning can be made formal via the co-area formula. A more direct route is to note that the surface measure on $S^{d-1}$ can be integrated by first integrating over the $(d-2)$-sphere for a fixed $x_1$ and then integrating over $x_1$. The area element projects as $d\\sigma_{d-1} \\propto (1-x_1^2)^{(d-3)/2}dx_1 d\\sigma_{d-2}$. This leads to the PDF of $X_1$ being proportional to $(1-x_1^2)^{(d-3)/2}$.\n$$\nf_{X_1}(x_1) = K_d (1-x_1^2)^{(d-3)/2}, \\quad x_1 \\in [-1, 1]\n$$\nThe normalization constant $K_d$ is found by $\\int_{-1}^1 f_{X_1}(x_1)dx_1 = 1$. The integral is related to the Beta function: $\\int_{-1}^1 (1-x^2)^a dx = B(a+1, 1/2)$. Here $a = (d-3)/2$. So the integral is $B(\\frac{d-1}{2}, \\frac{1}{2}) = \\frac{\\Gamma(\\frac{d-1}{2})\\Gamma(\\frac{1}{2})}{\\Gamma(d/2)}$.\nThus, $K_d = \\frac{\\Gamma(d/2)}{\\Gamma(\\frac{1}{2})\\Gamma(\\frac{d-1}{2})}$.\n\nNow, consider the transformation $U = (X_1+1)/2$, mapping $x_1 \\in [-1, 1]$ to $u \\in [0,1]$. We have $x_1=2u-1$ and $dx_1=2du$. The PDF of $U$ is:\n$$\nf_U(u) = f_{X_1}(2u-1) \\left| \\frac{dx_1}{du} \\right| = 2 K_d (1 - (2u-1)^2)^{(d-3)/2}\n$$\nSince $1-(2u-1)^2 = 4u(1-u)$, we have:\n$$\nf_U(u) = 2 K_d (4u(1-u))^{(d-3)/2} = 2 K_d 4^{(d-3)/2} u^{(d-3)/2} (1-u)^{(d-3)/2}\n$$\n$2 \\cdot 4^{(d-3)/2} = 2 \\cdot 2^{d-3} = 2^{d-2}$. Rewriting exponents for a Beta distribution:\n$$\nf_U(u) = (K_d 2^{d-2}) u^{\\frac{d-1}{2}-1} (1-u)^{\\frac{d-1}{2}-1}\n$$\nThis is the form of a Beta distribution with parameters $\\alpha = \\beta = (d-1)/2$. The normalization constant for $\\operatorname{Beta}(\\alpha, \\beta)$ is $1/B(\\alpha, \\beta)$. Let's verify that $K_d 2^{d-2} = 1/B(\\frac{d-1}{2}, \\frac{d-1}{2})$.\nUsing a Legendre duplication formula for the Gamma function, $\\Gamma(z)\\Gamma(z+1/2) = 2^{1-2z}\\sqrt{\\pi}\\Gamma(2z)$, we can show:\n$$\nK_d = \\frac{2^{2-d}\\Gamma(d-1)}{\\Gamma((d-1)/2)^2}\n$$\nSo, $K_d 2^{d-2} = \\frac{\\Gamma(d-1)}{(\\Gamma((d-1)/2))^2} = \\frac{1}{B(\\frac{d-1}{2}, \\frac{d-1}{2})}$. This confirms that $U \\sim \\operatorname{Beta}(\\frac{d-1}{2}, \\frac{d-1}{2})$.\nFor inverse-transform sampling of $X_1$, one can sample $U$ from this Beta distribution and set $X_1 = 2U-1$.\n\n### Part C: Independence of Hyperspherical Angular Components\n\nWe use a standard hyperspherical coordinate system for a point $x \\in \\mathbb{R}^d$:\n$$\n\\begin{aligned}\nx_1 &= r \\cos\\theta_1 \\\\\nx_2 &= r \\sin\\theta_1 \\cos\\theta_2 \\\\\nx_3 &= r \\sin\\theta_1 \\sin\\theta_2 \\cos\\theta_3 \\\\\n\\vdots \\\\\nx_{d-1} &= r \\sin\\theta_1 \\cdots \\sin\\theta_{d-2} \\cos\\phi \\\\\nx_d &= r \\sin\\theta_1 \\cdots \\sin\\theta_{d-2} \\sin\\phi\n\\end{aligned}\n$$\nwhere $\\theta_i \\in [0,\\pi]$ for $i=1,\\dots,d-2$ and $\\phi \\in [0,2\\pi)$. On the unit sphere $S^{d-1}$, we have $r=1$. The Jacobian determinant of this transformation is a standard result:\n$$\n|J| = r^{d-1} \\sin^{d-2}\\theta_1 \\sin^{d-3}\\theta_2 \\cdots \\sin^1\\theta_{d-2}\n$$\nThe surface area element on $S^{d-1}$ is found by setting $r=1$ and dropping $dr$:\n$$\nd\\sigma = (\\sin^{d-2}\\theta_1) (\\sin^{d-3}\\theta_2) \\cdots (\\sin\\theta_{d-2}) \\cdot d\\theta_1 d\\theta_2 \\cdots d\\theta_{d-2} d\\phi\n$$\nFor a uniform distribution on $S^{d-1}$, the joint probability density of the angles $(\\theta_1, \\ldots, \\theta_{d-2}, \\phi)$ is proportional to this expression. Normalizing by the total surface area $\\mathcal{A}(S^{d-1})$ gives the PDF:\n$$\nf(\\theta_1, \\ldots, \\phi) = \\frac{1}{\\mathcal{A}(S^{d-1})} \\left( \\prod_{i=1}^{d-2} \\sin^{d-1-i}\\theta_i \\right)\n$$\nThis joint density function is a product of functions, where each function depends on only one of the angular variables:\n$$\nf(\\theta_1, \\ldots, \\phi) = C \\cdot g_1(\\theta_1) \\cdot g_2(\\theta_2) \\cdots g_{d-2}(\\theta_{d-2}) \\cdot g_\\phi(\\phi)\n$$\nwhere $g_i(\\theta_i) = \\sin^{d-1-i}\\theta_i$ and $g_\\phi(\\phi)=1$. A joint PDF that factorizes into a product of functions of its individual variables implies that the random variables are mutually independent. Therefore, the hyperspherical angular coordinates are mutually independent under the uniform distribution on $S^{d-1}$.\n\n### Implementation and Statistical Validation\n\nThe derived principles are now used to implement the specified sampling algorithms and statistical tests.\n- **Algorithm 1 ($S^2$ sampler):** Implements the inverse-transform method derived in Part A.\n- **Algorithm 2 ($S^{d-1}$ sampler):** Implements the Gaussian normalization method, which samples $Y \\sim \\mathcal{N}(0, I_d)$ and sets $X=Y/\\|Y\\|_2$. This method is known to produce a uniform distribution on $S^{d-1}$ due to the rotational symmetry of the multivariate normal distribution.\n- **Angle Extraction:** For samples $x \\in S^4 \\subset \\mathbb{R}^5$, angles are extracted via $\\theta_1 = \\arccos(x_1)$ and $\\theta_2 = \\arccos(x_2/\\sqrt{1-x_1^2})$, consistent with the convention in Part C.\n- **Statistical Tests:** The specified tests (chi-square for independence, Kolmogorov-Smirnov for goodness-of-fit, and t-test for mean) are performed using routines from `scipy.stats`. A fixed random seed ensures reproducibility. For the chi-square tests of independence, the standard procedure using `scipy.stats.chi2_contingency` is employed, which computes expected frequencies from the marginals of the observed data table. A test result is considered positive (the null hypothesis is not rejected) if the calculated $p$-value exceeds the significance level $\\alpha$.",
            "answer": "```python\nimport numpy as np\nfrom scipy import stats\nfrom scipy.special import betainc, gamma\n\ndef solve():\n    \"\"\"\n    Main function to run all derivations, algorithms, and statistical tests.\n    \"\"\"\n    RANDOM_SEED = 42\n    np.random.seed(RANDOM_SEED)\n\n    # Test cases as specified in the problem\n    test_cases = [\n        {'test_id': 1, 'd': 3, 'N': 200000, 'bins_phi': 12, 'bins_c': 12, 'alpha': 0.01},\n        {'test_id': 2, 'd': 5, 'N': 250000, 'alpha': 0.01},\n        {'test_id': 3, 'd': 5, 'N': 250000, 'bins_theta': 10, 'alpha': 0.01},\n        {'test_id': 4, 'd': 2, 'N': 400000, 'alpha': 0.01},\n    ]\n\n    results = []\n\n    # --- Helper Functions ---\n\n    def sample_S2_inverse_transform(N):\n        \"\"\"Samples N points on S^2 using inverse transform on angles (Part A).\"\"\"\n        # Sample phi uniformly from [0, 2*pi]\n        phi = np.random.uniform(0, 2 * np.pi, N)\n        # Sample cos(theta) uniformly from [-1, 1]\n        cos_theta = np.random.uniform(-1, 1, N)\n        sin_theta = np.sqrt(1 - cos_theta**2)\n        \n        # Convert to Cartesian coordinates\n        x1 = sin_theta * np.cos(phi)\n        x2 = sin_theta * np.sin(phi)\n        x3 = cos_theta\n        \n        # Return Cartesian points and the sampled angular variables for testing\n        return np.stack([x1, x2, x3], axis=-1), phi, cos_theta\n\n    def sample_Sd_minus_1_gaussian(N, d):\n        \"\"\"Samples N points on S^{d-1} using Gaussian normalization method.\"\"\"\n        Y = np.random.normal(size=(N, d))\n        norms = np.linalg.norm(Y, axis=1, keepdims=True)\n        # Handle potential zero norm, although highly unlikely for d>=2\n        norms[norms == 0] = 1\n        X = Y / norms\n        return X\n\n    def extract_hyperspherical_angles_d5(X):\n        \"\"\"Extracts angles theta_1 and theta_2 from Cartesian points in R^5.\"\"\"\n        x1 = X[:, 0]\n        x2 = X[:, 1]\n        \n        # theta_1 = arccos(x1)\n        theta_1 = np.arccos(np.clip(x1, -1.0, 1.0))\n        \n        # sin(theta_1) = sqrt(1-x1^2)\n        sin_theta_1 = np.sqrt(1 - x1**2)\n        \n        # cos(theta_2) = x2 / sin(theta_1)\n        # Avoid division by zero when sin(theta_1) is 0 (at poles)\n        # and clip to handle floating point inaccuracies\n        cos_theta_2 = np.divide(x2, sin_theta_1, out=np.zeros_like(x2), where=sin_theta_1!=0)\n        theta_2 = np.arccos(np.clip(cos_theta_2, -1.0, 1.0))\n\n        return theta_1, theta_2\n\n    # --- Execute Tests ---\n\n    # Test 1: Chi-square test for independence on S^2\n    case1 = test_cases[0]\n    _, phi_samples, cos_theta_samples = sample_S2_inverse_transform(case1['N'])\n    hist, _, _ = np.histogram2d(\n        phi_samples, cos_theta_samples,\n        bins=[case1['bins_phi'], case1['bins_c']],\n        range=[[0, 2 * np.pi], [-1, 1]]\n    )\n    chi2_stat, p_val, _, _ = stats.chi2_contingency(hist)\n    results.append(p_val > case1['alpha'])\n\n    # Test 2: KS test for marginal distribution on S^4 (d=5)\n    case2 = test_cases[1]\n    X_d5 = sample_Sd_minus_1_gaussian(case2['N'], case2['d'])\n    X1 = X_d5[:, 0]\n    U = (X1 + 1) / 2\n    # From Part B, U should follow Beta((d-1)/2, (d-1)/2) = Beta(2,2) for d=5\n    beta_params = (case2['d'] - 1) / 2\n    ks_stat, p_val = stats.kstest(U, 'beta', args=(beta_params, beta_params))\n    results.append(p_val > case2['alpha'])\n\n    # Test 3: Chi-square test for independence of angles on S^4 (d=5)\n    case3 = test_cases[2]\n    # It's better to regenerate data to keep tests independent\n    X_d5_test3 = sample_Sd_minus_1_gaussian(case3['N'], case3['d'])\n    theta1_samples, theta2_samples = extract_hyperspherical_angles_d5(X_d5_test3)\n    hist, _, _ = np.histogram2d(\n        theta1_samples, theta2_samples,\n        bins=[case3['bins_theta'], case3['bins_theta']],\n        range=[[0, np.pi], [0, np.pi]]\n    )\n    chi2_stat, p_val, _, _ = stats.chi2_contingency(hist)\n    results.append(p_val > case3['alpha'])\n\n    # Test 4: t-test for mean of X1 on S^1 (d=2)\n    case4 = test_cases[3]\n    X_d2 = sample_Sd_minus_1_gaussian(case4['N'], case4['d'])\n    X1 = X_d2[:, 0]\n    # Test H0: E[X1] = 0\n    t_stat, p_val = stats.ttest_1samp(X1, popmean=0)\n    results.append(p_val > case4['alpha'])\n    \n    # Final output formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "A critical skill in stochastic simulation is not just implementing algorithms, but rigorously verifying their correctness. This advanced practice addresses how to test if a set of points on a sphere is truly uniform by checking for the key property of rotational invariance. You will learn to sample a random rotation from the special orthogonal group $SO(d)$ and use it to construct a powerful diagnostic statistic, providing a hands-on method to quantify the quality of any sphere sampling algorithm .",
            "id": "3337173",
            "problem": "You are asked to design and implement a rotational invariance diagnostic for sampling random vectors uniformly on the unit sphere in $\\mathbb{R}^d$. The diagnostic must be derived from first principles in stochastic simulation and must quantify uniformity on the unit sphere via empirical spherical cap counts before and after a random rotation. Starting from a valid foundational base, you must establish rotational invariance of an algorithm to sample on the sphere and then construct an implementable test statistic.\n\nFundamental base:\n- The $d$-dimensional standard multivariate normal distribution with density $f(\\mathbf{z}) = (2\\pi)^{-d/2}\\exp(-\\|\\mathbf{z}\\|_2^2/2)$ is spherically symmetric, meaning for any rotation $\\mathbf{R}\\in SO(d)$, where $SO(d)$ denotes the Special Orthogonal Group (the set of $d\\times d$ orthogonal matrices with determinant $1$), the distribution of $\\mathbf{R}\\mathbf{z}$ is identical to that of $\\mathbf{z}$.\n- The unique rotation-invariant probability measure on $SO(d)$ is the Haar measure (left-right invariant probability measure on a compact group).\n- A spherical cap on the unit sphere $S^{d-1}=\\{\\mathbf{x}\\in\\mathbb{R}^d:\\|\\mathbf{x}\\|_2=1\\}$ centered at a unit vector $\\mathbf{u}\\in S^{d-1}$ with half-angle $\\theta\\in[0,\\pi]$ (in radians) is $C(\\mathbf{u},\\theta)=\\{\\mathbf{x}\\in S^{d-1}:\\arccos(\\mathbf{u}^\\top \\mathbf{x})\\le \\theta\\}$, equivalently $\\{\\mathbf{x}\\in S^{d-1}:\\mathbf{u}^\\top \\mathbf{x}\\ge \\cos\\theta\\}$.\n\nTask:\n- From the above base, derive and justify an algorithm to sample $\\mathbf{x}\\in S^{d-1}$ by transforming a rotationally invariant distribution in $\\mathbb{R}^d$ into a distribution on $S^{d-1}$.\n- From the uniqueness of the Haar measure, derive a practical procedure to sample a random rotation $\\mathbf{R}\\in SO(d)$.\n- Define a diagnostic that compares spherical cap counts before and after applying a Haar-distributed random rotation to a sampled point set on $S^{d-1}$. For a set of $m$ unit directions $\\{\\mathbf{u}_j\\}_{j=1}^m$ and a fixed half-angle $\\theta$, compute counts $c_{j}^{\\text{pre}}=\\sum_{i=1}^n \\mathbf{1}\\{\\mathbf{u}_j^\\top \\mathbf{x}^{(i)}\\ge \\cos\\theta\\}$ and $c_{j}^{\\text{post}}=\\sum_{i=1}^n \\mathbf{1}\\{\\mathbf{u}_j^\\top (\\mathbf{R}\\mathbf{x}^{(i)})\\ge \\cos\\theta\\}$. Construct a scalar test statistic $T$ that aggregates the differences across caps into a single, interpretable number reflecting rotational invariance. Your statistic must be numerically stable and must account for sampling variability; angles are to be treated in radians.\n\nImplementation requirements:\n- Use a fixed pseudorandom seed $1729$ for reproducibility.\n- Implement the following concrete choices:\n  - Sample $\\mathbf{x}^{(i)}$ by drawing $\\mathbf{z}^{(i)}\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}_d)$ independently and setting $\\mathbf{x}^{(i)}=\\mathbf{z}^{(i)}/\\|\\mathbf{z}^{(i)}\\|_2$.\n  - Sample $\\mathbf{R}\\in SO(d)$ using a method consistent with Haar measure.\n  - Sample cap directions $\\mathbf{u}_j$ as independent unit vectors on $S^{d-1}$ via the same sphere-sampling method.\n  - Define the diagnostic statistic as follows. For each cap $j$, let $\\widehat{p}_j=(c_j^{\\text{pre}}+c_j^{\\text{post}})/(2n)$, and the pooled standard error estimate $s_j=\\sqrt{2\\widehat{p}_j(1-\\widehat{p}_j)/n}$. Define the normalized difference $z_j=\\begin{cases} \\frac{\\left|c_j^{\\text{pre}}/n - c_j^{\\text{post}}/n\\right|}{s_j}, & s_j>0\\\\ 0, & s_j=0\\end{cases}$, and the diagnostic $T=\\max_{1\\le j\\le m} z_j$.\n\nTest suite:\n- For each test case, the input parameters are $(d,n,m,\\theta)$ with angles in radians. Use the following $5$ cases to exercise the diagnostic across typical, boundary, and high-dimensional regimes:\n  - Case $1$: $(d,n,m,\\theta)=(3,10000,12,0.5)$.\n  - Case $2$: $(d,n,m,\\theta)=(2,15000,16,0.2)$.\n  - Case $3$: $(d,n,m,\\theta)=(10,12000,24,0.3)$.\n  - Case $4$: $(d,n,m,\\theta)=(50,8000,40,0.25)$.\n  - Case $5$: $(d,n,m,\\theta)=(5,300,10,0.4)$.\n- For reproducibility across cases, use the fixed base seed $1729$ and, for case index $k\\in\\{1,2,3,4,5\\}$, use seed $1729+k$ for all randomness within case $k$.\n\nFinal output format:\n- Your program must output a single line containing the list of the diagnostic values $[T_1,T_2,T_3,T_4,T_5]$ for the $5$ test cases, as a comma-separated list enclosed in square brackets. No other output is permitted.",
            "solution": "The problem requires the derivation, justification, and implementation of a diagnostic to test for rotational invariance in algorithms that sample random vectors uniformly on the unit sphere in $\\mathbb{R}^d$. The solution is constructed from first principles as specified.\n\n### 1. Uniform Sampling on the Unit Sphere $S^{d-1}$\nThe foundational principle for sampling uniformly from the unit sphere $S^{d-1} = \\{\\mathbf{x} \\in \\mathbb{R}^d : \\|\\mathbf{x}\\|_2 = 1\\}$ is the spherical symmetry of the standard multivariate normal distribution. A random vector $\\mathbf{z} \\in \\mathbb{R}^d$ drawn from the standard normal distribution $\\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$ has the probability density function $f(\\mathbf{z}) = (2\\pi)^{-d/2}\\exp(-\\|\\mathbf{z}\\|_2^2/2)$. This density function depends only on the magnitude $\\|\\mathbf{z}\\|_2$ of the vector, not its direction. This property is known as spherical symmetry.\n\nThis symmetry implies that the direction of the vector $\\mathbf{z}$, given by the unit vector $\\mathbf{x} = \\mathbf{z} / \\|\\mathbf{z}\\|_2$, must be uniformly distributed on $S^{d-1}$. To formalize this, consider any rotation matrix $\\mathbf{R} \\in SO(d)$. Since the distribution of $\\mathbf{z}$ is spherically symmetric, the distribution of the rotated vector $\\mathbf{Rz}$ is identical to that of $\\mathbf{z}$. The direction of the rotated vector is $\\mathbf{Rz} / \\|\\mathbf{Rz}\\|_2$. As rotations are isometries, $\\|\\mathbf{Rz}\\|_2 = \\|\\mathbf{z}\\|_2$, so the new direction is $\\mathbf{R}(\\mathbf{z} / \\|\\mathbf{z}\\|_2) = \\mathbf{Rx}$. Because the distributions of $\\mathbf{z}$ and $\\mathbf{Rz}$ are identical, the distributions of their corresponding direction vectors, $\\mathbf{x}$ and $\\mathbf{Rx}$, must also be identical. The only probability distribution on $S^{d-1}$ that is invariant under all rotations is the uniform distribution. Therefore, the direction vector $\\mathbf{x}$ is uniformly distributed on $S^{d-1}$.\n\nThe resulting algorithm, often attributed to G. E. P. Box and M. E. Muller, is as follows:\n1. Generate a $d$-dimensional vector $\\mathbf{z} = (z_1, z_2, \\ldots, z_d)^\\top$ where each component $z_i$ is an independent draw from the standard normal distribution $\\mathcal{N}(0, 1)$.\n2. Calculate the Euclidean norm $\\|\\mathbf{z}\\|_2 = \\sqrt{\\sum_{i=1}^d z_i^2}$. For a continuous distribution, the probability of $\\mathbf{z} = \\mathbf{0}$ is $0$, so $\\|\\mathbf{z}\\|_2 > 0$ almost surely.\n3. The uniformly distributed random vector on $S^{d-1}$ is given by the normalization $\\mathbf{x} = \\mathbf{z} / \\|\\mathbf{z}\\|_2$.\n\n### 2. Sampling a Random Rotation from the Special Orthogonal Group $SO(d)$\nThe task requires sampling a random rotation matrix $\\mathbf{R}$ from the special orthogonal group $SO(d)$ according to its unique rotation-invariant probability measure, the Haar measure. A standard and computationally efficient method to achieve this utilizes the QR decomposition.\n\nLet $\\mathbf{A}$ be a $d \\times d$ matrix whose entries are independent and identically distributed (i.i.d.) random variables from $\\mathcal{N}(0, 1)$. The joint distribution of the entries of $\\mathbf{A}$ is spherically symmetric in $\\mathbb{R}^{d^2}$. The QR decomposition of $\\mathbf{A}$ expresses it as a product $\\mathbf{A} = \\mathbf{QR}_{\\text{upper}}$, where $\\mathbf{Q}$ is an orthogonal matrix ($\\mathbf{Q}^\\top\\mathbf{Q} = \\mathbf{I}_d$) and $\\mathbf{R}_{\\text{upper}}$ is an upper triangular matrix. It can be shown that the resulting matrix $\\mathbf{Q}$ is distributed according to the Haar measure on the orthogonal group $O(d)$.\n\nThe group $O(d)$ consists of matrices with determinant equal to $+1$ or $-1$. The special orthogonal group $SO(d)$ is the subgroup of $O(d)$ containing only matrices with determinant $+1$. To obtain a sample from $SO(d)$, we first sample $\\mathbf{Q}$ from $O(d)$ and then map it to $SO(d)$ if necessary. If $\\det(\\mathbf{Q}) = +1$, then $\\mathbf{Q}$ is already in $SO(d)$. If $\\det(\\mathbf{Q}) = -1$, we can convert it to an element of $SO(d)$ by multiplying any single column (or row) by $-1$. This operation flips the sign of the determinant while preserving the Haar measure property for the distribution on $SO(d)$.\n\nThe algorithm for sampling $\\mathbf{R} \\in SO(d)$ is:\n1. Generate a $d \\times d$ matrix $\\mathbf{A}$ with i.i.d. entries from $\\mathcal{N}(0, 1)$.\n2. Compute the QR decomposition of $\\mathbf{A}$ to obtain the orthogonal matrix $\\mathbf{Q}$.\n3. Calculate the determinant $s = \\det(\\mathbf{Q})$.\n4. If $s < 0$, multiply the first column of $\\mathbf{Q}$ by $-1$ to form the final rotation matrix $\\mathbf{R}$. Otherwise, set $\\mathbf{R} = \\mathbf{Q}$.\n\n### 3. The Rotational Invariance Diagnostic Statistic\nThe diagnostic is designed to verify the rotational invariance of a point set $\\{\\mathbf{x}^{(i)}\\}_{i=1}^n$ sampled on $S^{d-1}$. If the sampling method is truly uniform, the statistical properties of the point set should be unchanged by the application of a random rotation $\\mathbf{R} \\sim \\text{Haar}(SO(d))$.\n\nWe probe the distribution of points by measuring the number of points that fall into a collection of $m$ fixed spherical caps. A spherical cap $C_j$ is defined by a center direction $\\mathbf{u}_j \\in S^{d-1}$ and a half-angle $\\theta \\in [0, \\pi]$, such that $C_j = \\{\\mathbf{x} \\in S^{d-1} : \\mathbf{u}_j^\\top \\mathbf{x} \\ge \\cos\\theta\\}$. We compute the counts before and after rotation:\n- Pre-rotation count for cap $j$: $c_{j}^{\\text{pre}} = \\sum_{i=1}^n \\mathbf{1}\\{\\mathbf{u}_j^\\top \\mathbf{x}^{(i)} \\ge \\cos\\theta\\}$\n- Post-rotation count for cap $j$: $c_{j}^{\\text{post}} = \\sum_{i=1}^n \\mathbf{1}\\{\\mathbf{u}_j^\\top (\\mathbf{R}\\mathbf{x}^{(i)}) \\ge \\cos\\theta\\}$\n\nFor each cap $j$, we have two sample proportions, $\\hat{p}_j^{\\text{pre}} = c_{j}^{\\text{pre}}/n$ and $\\hat{p}_j^{\\text{post}} = c_{j}^{\\text{post}}/n$. The null hypothesis, $H_0$, is that the underlying sampling distribution is rotationally invariant, which implies that the true probability $p_j$ of a point falling into cap $C_j$ is the same for the original and rotated sets. This is a two-sample test for equality of proportions.\n\nThe diagnostic statistic $T$ is constructed as follows:\n1. Under $H_0$, the best estimate for the common proportion $p_j$ is the pooled proportion: $\\widehat{p}_j = (c_j^{\\text{pre}} + c_j^{\\text{post}}) / (2n)$.\n2. The variance of the difference between the two sample proportions, $\\hat{p}_j^{\\text{pre}} - \\hat{p}_j^{\\text{post}}$, is estimated using the pooled proportion. The standard error of this difference is given by $s_j = \\sqrt{\\frac{\\widehat{p}_j(1-\\widehat{p}_j)}{n} + \\frac{\\widehat{p}_j(1-\\widehat{p}_j)}{n}} = \\sqrt{2\\widehat{p}_j(1-\\widehat{p}_j)/n}$.\n3. The normalized difference, or Z-score, for each cap is $z_j = \\frac{|\\hat{p}_j^{\\text{pre}} - \\hat{p}_j^{\\text{post}}|}{s_j} = \\frac{|c_j^{\\text{pre}}/n - c_j^{\\text{post}}/n|}{s_j}$. If $s_j=0$ (which occurs if $\\widehat{p}_j$ is $0$ or $1$), the numerator is also $0$, and we define $z_j = 0$. For large $n$, $z_j$ approximately follows a half-normal distribution under $H_0$.\n4. To aggregate the results from all $m$ caps into a single number, the diagnostic is defined as the maximum observed Z-score: $T = \\max_{1 \\le j \\le m} z_j$. A large value of $T$ indicates a significant deviation from rotational invariance in at least one of the tested cap directions, casting doubt on the uniformity of the sampling method. Since the specified sampling method is theoretically sound, we expect $T$ to be small, consistent with random fluctuations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_diagnostic_case(d, n, m, theta, seed):\n    \"\"\"\n    Runs the rotational invariance diagnostic for a single test case.\n\n    Args:\n        d (int): Dimension of the space.\n        n (int): Number of points to sample on the sphere.\n        m (int): Number of spherical caps to use for the test.\n        theta (float): Half-angle of the spherical caps in radians.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        float: The computed diagnostic statistic T.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Helper function to sample k points uniformly on the unit sphere S^(d-1)\n    def sample_sphere(k, dim, random_state):\n        z = random_state.standard_normal(size=(k, dim))\n        norm = np.linalg.norm(z, axis=1, keepdims=True)\n        # Avoid division by zero, though Pr(norm=0) is 0 for continuous dist.\n        norm[norm == 0] = 1.0\n        return z / norm\n\n    # 1. Sample n data points X on S^(d-1)\n    X = sample_sphere(n, d, rng)\n\n    # 2. Sample m cap directions U on S^(d-1)\n    U = sample_sphere(m, d, rng)\n\n    # 3. Sample a random rotation R from SO(d) via QR decomposition\n    A = rng.standard_normal(size=(d, d))\n    Q, _ = np.linalg.qr(A)\n    # Ensure determinant is +1 for SO(d)\n    if np.linalg.det(Q) < 0:\n        Q[:, 0] *= -1\n    R = Q\n\n    # 4. Apply the rotation to the data points\n    # For row vectors x in X, the rotated vectors are x @ R.T\n    RX = X @ R.T\n\n    # 5. Compute the diagnostic statistic T\n    cos_theta = np.cos(theta)\n\n    # Vectorized computation of counts\n    # Dot products for pre-rotation points: X @ U.T gives an (n, m) matrix\n    dot_pre = X @ U.T\n    # Dot products for post-rotation points: RX @ U.T gives an (n, m) matrix\n    dot_post = RX @ U.T\n\n    # Counts for each cap (sum over points axis=0) -> vector of size m\n    c_pre_vec = np.sum(dot_pre >= cos_theta, axis=0)\n    c_post_vec = np.sum(dot_post >= cos_theta, axis=0)\n\n    # Pooled proportion estimate for each cap\n    p_hat_vec = (c_pre_vec + c_post_vec) / (2 * n)\n    \n    # Pooled standard error estimate for each cap\n    # The term inside sqrt can be negative due to float precision, so clip at 0.\n    s_vec_numerator = 2 * p_hat_vec * (1 - p_hat_vec) / n\n    s_vec = np.sqrt(np.maximum(0, s_vec_numerator))\n\n    # Difference in proportions for each cap\n    diff_vec = np.abs(c_pre_vec / n - c_post_vec / n)\n\n    # Normalized differences (z-scores)\n    z_vec = np.zeros_like(s_vec)\n    # Avoid division by zero where s_vec is 0\n    # If s_vec is 0, p_hat is 0 or 1, meaning c_pre and c_post were same (0 or n)\n    # so diff_vec is also 0. z_j=0 is correct in this case.\n    nonzero_s_mask = s_vec > 0\n    z_vec[nonzero_s_mask] = diff_vec[nonzero_s_mask] / s_vec[nonzero_s_mask]\n\n    # The final diagnostic is the maximum z-score\n    T = np.max(z_vec)\n    return T\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (d, n, m, theta)\n        (3, 10000, 12, 0.5),   # Case 1\n        (2, 15000, 16, 0.2),   # Case 2\n        (10, 12000, 24, 0.3),  # Case 3\n        (50, 8000, 40, 0.25),  # Case 4\n        (5, 300, 10, 0.4),     # Case 5\n    ]\n    \n    base_seed = 1729\n    results = []\n\n    for i, case in enumerate(test_cases):\n        d, n, m, theta = case\n        case_seed = base_seed + (i + 1)\n        # Using numpy 1.23.5, np.random.seed is legacy. Use default_rng.\n        # The behavior for a given seed is consistent.\n        T = run_diagnostic_case(d, n, m, theta, case_seed)\n        results.append(T)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}