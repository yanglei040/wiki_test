{
    "hands_on_practices": [
        {
            "introduction": "尽管标准的逆变换采样法在概念上很简单，但当分布的均值 $np$ 很大时，其性能会下降。本练习将指导您设计一个更精巧、更高效的精确采样器。您将首先推导二项概率的一个关键递推关系，然后利用它构建一个算法，从分布的众数（最可能的值）开始智能地搜索样本值，从而大大减少预期的计算步数。这种方法，当与允许我们假设 $p \\le 0.5$ 的对称性质 () 相结合时，代表了实用算法工程中的重要一步。",
            "id": "3292698",
            "problem": "考虑一个服从二项分布的离散随机变量 $X$，其概率质量函数定义为 $P(X=k)$，其中 $k \\in \\{0,1,\\dots,n\\}$，参数为 $n \\in \\mathbb{N}$ 和 $p \\in [0,1]$。从二项分布的基本定义出发，即 $n$ 次独立同分布的伯努利试验的总和，每次试验的成功概率为 $p$，并使用经过充分检验的二项概率质量函数公式 $P(X=k)$，即从 $n$ 中取 $k$ 的组合数乘以 $p^{k}$ 再乘以 $(1-p)^{n-k}$。从第一性原理出发，推导出一个形式如下的连续概率递推关系：\n$$\nP(X=k+1)=P(X=k)\\cdot\\frac{(n-k)}{(k+1)}\\cdot\\frac{p}{(1-p)}.\n$$\n在建立此递推关系后，设计并实现一个针对 $X$ 的基于逆变换的采样算法，该算法通过从众数开始向外累加概率质量函数值来生成单个二项分布变数。该算法必须：\n- 对于 $p \\in (0,1)$，计算众数 $m=\\lfloor (n+1)p \\rfloor$，并显式处理 $p=0$ 和 $p=1$ 的退化情况，此时所有概率质量分别集中在 $k=0$ 和 $k=n$。\n- 在 $k=m$ 处进行初始化，使用基于对数和伽马函数的数值稳定表达式计算 $P(X=m)$。\n- 使用推导出的递推关系从 $P(X=m)$ 计算 $P(X=m+1)$ 和 $P(X=m-1)$，然后继续向外扩展，交替地选择当前具有更大概率质量的一侧，从一个均匀分布的随机数 $U \\sim \\mathrm{Uniform}(0,1)$ 中减去每个访问过的概率质量，直到 $U$ 的剩余值落在当前概率质量区间内，此时返回对应的索引 $k$。\n- 确保每个概率质量 $P(X=k)$ 最多被访问一次，并且算法能以一个正确的样本值终止。讨论为什么期望运行时间与采样索引和众数之间的期望绝对距离 $|k-m|$ 同阶，其依据是二项概率质量函数远离众数时的单峰性和单调递减性。\n\n您的程序必须实现此算法，并将其应用于以下测试套件，其中指定了参数 $(n,p,U)$：\n- 测试用例 1：$(n,p,U)=(50,\\,0.3,\\,0.73)$。\n- 测试用例 2：$(n,p,U)=(1000,\\,0.5,\\,0.5)$。\n- 测试用例 3：$(n,p,U)=(20,\\,0.8,\\,0.999999)$。\n- 测试用例 4：$(n,p,U)=(10,\\,1.0,\\,0.42)$。\n- 测试用例 5：$(n,p,U)=(7,\\,0.0,\\,0.999)$。\n- 测试用例 6：$(n,p,U)=(200,\\,0.05,\\,0.2)$。\n\n每个测试用例产生一个代表采样值 $k$ 的整数输出。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表，例如 $[k_1,k_2,k_3,k_4,k_5,k_6]$。此问题不涉及物理单位，所有角度均无关。所有答案均为整数，不使用百分比。",
            "solution": "该问题被评估为有效，因为它科学地基于概率论的原理，特别是二项分布和逆变换采样方法。问题是适定的，为算法及其执行所需的数据提供了完整且一致的规范。语言客观、正式。\n\n### 推导概率递推关系\n\n设 $X$ 是一个参数为 $n \\in \\mathbb{N}$ 和 $p \\in [0,1]$ 的二项随机变量。其概率质量函数 (PMF) 由下式给出：\n$$\nP(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} = \\frac{n!}{k!(n-k)!} p^k (1-p)^{n-k}\n$$\n对于 $k \\in \\{0, 1, \\dots, n\\}$。\n\n我们希望找到连续概率 $P(X=k+1)$ 和 $P(X=k)$ 之间的递推关系。我们可以将 $k+1$ 的 PMF 表示为：\n$$\nP(X=k+1) = \\binom{n}{k+1} p^{k+1} (1-p)^{n-(k+1)} = \\frac{n!}{(k+1)!(n-k-1)!} p^{k+1} (1-p)^{n-k-1}\n$$\n为找到递推关系，我们计算比率 $\\frac{P(X=k+1)}{P(X=k)}$：\n$$\n\\frac{P(X=k+1)}{P(X=k)} = \\frac{\\frac{n!}{(k+1)!(n-k-1)!} p^{k+1} (1-p)^{n-k-1}}{\\frac{n!}{k!(n-k)!} p^k (1-p)^{n-k}}\n$$\n这可以分为两部分：二项式系数的比率和概率项的比率。\n\n二项式系数的比率简化为：\n$$\n\\frac{\\binom{n}{k+1}}{\\binom{n}{k}} = \\frac{n!}{(k+1)!(n-k-1)!} \\cdot \\frac{k!(n-k)!}{n!} = \\frac{k!(n-k)(n-k-1)!}{(k+1)k!(n-k-1)!} = \\frac{n-k}{k+1}\n$$\n概率项的比率简化为：\n$$\n\\frac{p^{k+1}(1-p)^{n-k-1}}{p^k(1-p)^{n-k}} = \\frac{p}{1-p}\n$$\n结合这两个结果，得到连续概率的比率：\n$$\n\\frac{P(X=k+1)}{P(X=k)} = \\frac{n-k}{k+1} \\cdot \\frac{p}{1-p}\n$$\n重新整理此方程，得到所需的前向递推关系：\n$$\nP(X=k+1) = P(X=k) \\cdot \\frac{n-k}{k+1} \\cdot \\frac{p}{1-p}\n$$\n由此，我们也可以推导出用于计算小于众数的值的概率的反向递推关系：\n$$\nP(X=k) = P(X=k+1) \\cdot \\frac{k+1}{n-k} \\cdot \\frac{1-p}{p}\n$$\n\n### 算法设计与分析\n\n该问题要求一种基于逆变换的采样算法，该算法从众数开始向外探索概率质量函数。这是一种有效的采样技术，因为对于一个随机变量 $U \\sim \\mathrm{Uniform}(0,1)$，算法在值 $k$ 处终止的概率恰好是 $P(X=k)$。虽然标准的逆变换采样是按 $k$ 的递增顺序对概率求和，但该算法对求和进行了重新排序，这不影响采样方法本身的有效性，尽管对于给定的 $U$，它可能产生与标准方法不同的样本。\n\n算法流程如下：\n\n1.  **处理退化情况**：如果 $p=1$，分布是确定性的，所有概率质量集中在 $k=n$。算法必须返回 $n$。如果 $p=0$，所有概率质量集中在 $k=0$，因此必须返回 $0$。\n\n2.  **计算众数**：对于 $p \\in (0,1)$，二项分布的 PMF 是单峰的。使 $P(X=k)$ 最大化的 $k$ 值，即众数 $m$，由 $m = \\lfloor(n+1)p\\rfloor$ 给出。这将是我们的起点。\n\n3.  **在众数处初始化**：必须计算众数处的概率 $P(X=m)$。为避免在 $n$ 很大时出现数值下溢或上溢，我们先计算 PMF 的对数，然后再取指数。对数 PMF 为：\n    $$\n    \\ln P(X=m) = \\ln\\left(\\binom{n}{m}\\right) + m\\ln(p) + (n-m)\\ln(1-p)\n    $$\n    二项式系数的对数 $\\ln(\\binom{n}{m}) = \\ln(n!) - \\ln(m!) - \\ln((n-m)!)$ 使用对数伽马函数 $\\ln(k!) = \\text{gammaln}(k+1)$ 计算以保证数值稳定性：\n    $$\n    \\ln P(X=m) = \\text{gammaln}(n+1) - \\text{gammaln}(m+1) - \\text{gammaln}(n-m+1) + m\\ln p + (n-m)\\ln(1-p)\n    $$\n    由此得到 $P(X=m) = \\exp(\\ln(P(X=m)))$。\n\n4.  **通过向外扩展进行逆变换采样**：给定一个均匀分布的随机数 $U \\sim \\mathrm{Uniform}(0,1)$，我们使用逆变换采样的“累加并检验”原理。\n    *   初始化累积概率和 $S = P(X=m)$。如果 $U \\le S$，则样本为 $m$。\n    *   否则，算法从众数向外扩展。我们维护两个指针 $k_{low} = m-1$ 和 $k_{high} = m+1$，以及使用推导出的递推关系计算出的相应概率 $P_{low} = P(X=k_{low})$ 和 $P_{high} = P(X=k_{high})$。\n    *   在一个循环中，我们比较 $P_{low}$ 和 $P_{high}$。问题规定我们必须选择具有较大概率质量的一侧。\n    *   如果 $P_{high}  P_{low}$ (且 $k_{high} \\le n$)，我们将 $P_{high}$ 加到和 $S$ 中。如果 $U \\le S$，我们返回 $k_{high}$。否则，我们使用前向递推将 $P_{high}$ 更新为 $k_{high}+1$ 处的概率，并递增 $k_{high}$。\n    *   如果 $P_{low} \\ge P_{high}$ (且 $k_{low} \\ge 0$)，我们将 $P_{low}$ 加到 $S$ 中。如果 $U \\le S$，我们返回 $k_{low}$。否则，我们使用反向递推将 $P_{low}$ 更新为 $k_{low}-1$ 处的概率，并递减 $k_{low}$。\n    *   此过程持续进行，直到满足条件 $U \\le S$。循环保证会终止，因为 $S$ 严格递增趋近于 $1$。\n\n5.  **运行时间分析**：循环中每一步的计算成本是常数。步数是我们必须求和的概率数量，直到累积和超过 $U$。由于我们从众数 $m$（最可能的值）开始，并向概率较小的值扩展，对于对应于众数附近 $k$ 值的 $U$，算法将更快终止。因此，期望迭代次数与随机变量与其众数的期望绝对偏差 $E[|X-m|]$ 相关。对于二项分布，其离散程度由标准差 $\\sigma = \\sqrt{np(1-p)}$ 表征。期望距离 $E[|X-m|]$ 与 $\\sigma$ 同阶。因此，该算法的期望运行时间为 $O(\\sqrt{np(1-p)})$，对于较大的 $n$，这比标准逆变换方法的 $O(np)$ 更高效。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef generate_binomial_variate(n, p, u):\n    \"\"\"\n    Generates a single binomial variate using an inversion-based method\n    that expands outward from the mode.\n    \n    Args:\n        n (int): The number of trials.\n        p (float): The success probability of each trial.\n        u (float): A uniform random variate from (0, 1).\n    \n    Returns:\n        int: The generated binomial variate k.\n    \"\"\"\n    # Step 1: Handle degenerate cases\n    if p == 0.0:\n        return 0\n    if p == 1.0:\n        return n\n\n    # Step 2: Compute the mode\n    m = int(np.floor((n + 1) * p))\n\n    # Step 3: Initialize at the mode with a numerically stable PMF calculation\n    log_p = np.log(p)\n    log_1_minus_p = np.log(1.0 - p)\n    \n    log_pmf_m = (gammaln(n + 1) - gammaln(m + 1) - gammaln(n - m + 1)\n                 + m * log_p + (n - m) * log_1_minus_p)\n    pmf_m = np.exp(log_pmf_m)\n\n    # Step 4: Inversion sampling by outward expansion\n    cum_prob = pmf_m\n    if u = cum_prob:\n        return m\n\n    # Initialize pointers and probabilities for the expansion\n    k_low = m - 1\n    k_high = m + 1\n    \n    # Pre-calculate recursion ratios for efficiency\n    p_ratio = p / (1.0 - p)\n\n    # Calculate initial probabilities for the two sides\n    # Use -1.0 as a sentinel for out-of-bounds indices\n    pmf_low = pmf_m * (m / (n - m + 1.0)) / p_ratio if m > 0 else -1.0\n    pmf_high = pmf_m * ((n - m) / (m + 1.0)) * p_ratio if m  n else -1.0\n\n    while True:\n        # Determine which side has the larger next probability mass.\n        # This logic correctly handles negative sentinels unless both are negative.\n        if pmf_high > pmf_low:\n            cum_prob += pmf_high\n            if u = cum_prob:\n                return k_high\n            \n            # Update for the next step on the high side\n            if k_high  n:\n                pmf_high *= ((n - k_high) / (k_high + 1.0)) * p_ratio\n                k_high += 1\n            else: # Boundary reached\n                pmf_high = -1.0\n        else:\n            # If low side is exhausted, high side must also be (or was smaller).\n            # This indicates all probabilities are summed.\n            if pmf_low  0:\n                break\n                \n            cum_prob += pmf_low\n            if u = cum_prob:\n                return k_low\n\n            # Update for the next step on the low side\n            if k_low > 0:\n                pmf_low *= (k_low / (n - k_low + 1.0)) / p_ratio\n                k_low -= 1\n            else: # Boundary reached\n                pmf_low = -1.0\n    # Fallback for u > sum(probs) due to floating point error. Return the largest possible value.\n    return n\n\ndef solve():\n    \"\"\"\n    Executes the binomial variate generation algorithm for the specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement: (n, p, U)\n    test_cases = [\n        (50, 0.3, 0.73),\n        (1000, 0.5, 0.5),\n        (20, 0.8, 0.999999),\n        (10, 1.0, 0.42),\n        (7, 0.0, 0.999),\n        (200, 0.05, 0.2),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, p, u = case\n        result = generate_binomial_variate(n, p, u)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "二项式随机变量的生成不仅仅是其本身的目的，它也是高级蒙特卡洛方法中的一个基本构建块。本练习将通过应用重要性采样来估计稀有事件概率（例如尾部概率 $\\mathbb{P}\\{X \\ge k\\}$），从而带您进入方差缩减的领域。您将发现一个优雅的性质，即指数倾斜后的二项分布仍然是一个二项分布，并利用这一见解构建一个高效的估计器，该估计器能够精确测量朴素模拟方法难以处理的极小概率。",
            "id": "3292775",
            "problem": "设计并实现一个重要性抽样估计器，使用指数倾斜（exponential tilting）来估计二项分布的右尾概率。目标是概率 $\\mathbb{P}\\{X \\ge k\\}$，其中 $X \\sim \\mathrm{Bin}(n,p)$。你必须从第一性原理出发，推导出倾斜后的抽样分布，并实现一个程序来高效地估计这个概率，即使该概率很小。\n\n从以下基本基础开始：\n- 二项分布 $\\mathrm{Bin}(n,p)$ 是 $n$ 次独立伯努利试验（Bernoulli trials）中成功次数的总和的分布，其中成功概率为 $p$，且 $0  p  1$，$n$ 为正整数。\n- 二项随机变量 $X \\sim \\mathrm{Bin}(n,p)$ 的矩生成函数（moment generating function）$M_{X}(\\theta)$ 为 $M_{X}(\\theta) = \\left( (1-p) + p e^{\\theta} \\right)^{n}$，累积量生成函数（cumulant generating function）为 $K(\\theta) = \\log M_{X}(\\theta)$。\n- 使用参数 $\\theta$ 的指数倾斜（也称为 Esscher transform）会产生一个新的分布，其概率质量函数与 $e^{\\theta x} \\mathbb{P}\\{X=x\\}$ 成正比。\n\n你的任务是：\n1. 使用累积量生成函数的定义和指数倾斜，对于 $X \\sim \\mathrm{Bin}(n,p)$，推导出倾斜后的概率质量函数 $q_{\\theta}(x)$（用原始质量函数 $p(x)$ 表示），并用代数方法证明 $q_{\\theta}$ 本身也是一个二项分布，其成功参数为某个 $p_{\\theta} \\in (0,1)$，你必须明确指出这个参数。\n2. 证明导数 $K'(\\theta)$ 等于倾斜测度下的均值，并推导出选择 $\\theta$ 的方程，使得 $K'(\\theta) = k$。以闭合形式解此方程，用 $n$、$p$ 和 $k$ 表示 $\\theta$（并因此表示 $p_{\\theta}$）。证明为何这种选择会使得事件 $\\{X \\ge k\\}$ 在倾斜分布下成为典型事件。\n3. 对于给定的 $\\theta$，推导原始分布与倾斜分布之间的似然比（Radon–Nikodym derivative）$w_{\\theta}(x)$，并展示如何使用从倾斜分布中抽取的样本来构建 $\\mathbb{P}\\{X \\ge k\\}$ 的无偏重要性抽样估计器。\n4. 实现一个程序，该程序：\n   - 对于下面的每个测试用例 $(n,p,k)$，计算倾斜参数 $\\theta$ 和相关的 $p_{\\theta}$。\n   - 从倾斜后的二项分布 $\\mathrm{Bin}(n,p_{\\theta})$ 中模拟 $M$ 个独立同分布的样本 $X_{1},\\dots,X_{M}$。\n   - 计算重要性抽样估计值 $\\widehat{\\alpha} = \\frac{1}{M} \\sum_{i=1}^{M} \\mathbf{1}\\{X_{i} \\ge k\\} w_{\\theta}(X_{i})$，其中 $w_{\\theta}(x)$ 是你推导出的似然比。\n   - 为了验证，同时使用数值稳定的方法计算精确概率 $\\alpha^{\\star} = \\mathbb{P}\\{X \\ge k\\}$。\n   - 为每个测试用例报告一对浮点数：估计值 $\\widehat{\\alpha}$ 和绝对误差 $|\\widehat{\\alpha} - \\alpha^{\\star}|$。\n5. 使用以下测试套件，并采用固定的模拟预算和随机种子：\n   - 测试 A: $(n,p,k) = (\\,100\\,,\\,0.30\\,,\\,45\\,)$。\n   - 测试 B: $(n,p,k) = (\\,1000\\,,\\,0.20\\,,\\,260\\,)$。\n   - 测试 C: $(n,p,k) = (\\,200\\,,\\,0.05\\,,\\,25\\,)$。\n   - 测试 D: $(n,p,k) = (\\,50\\,,\\,0.50\\,,\\,30\\,)$。\n   每个测试用例使用 $M = 100000$ 个倾斜样本，并将伪随机种子设置为 $123456789$ 以确保可复现性。\n\n实现细节和约束：\n- 你对倾斜二项变异的模拟必须利用这样一个事实：倾斜分布本身就是二项分布，其参数 $p_{\\theta}$ 是从 $\\theta$ 和原始的 $p$ 推导出来的。\n- 所有计算都必须以浮点数算术进行。不涉及角度。不涉及物理单位。任何比率都必须表示为小数。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素本身都是一个形式为 $[\\widehat{\\alpha},|\\widehat{\\alpha}-\\alpha^{\\star}|]$ 的双元素列表。例如，一个有效的输出格式是 $[[0.123,0.0012],[0.045,0.00003],[\\dots],[\\dots]]$。\n\n你的推导应避免使用启发式方法，并应遵循所述的基本基础，不援引任何未经证实的捷径。确保测试套件所需的所有数值都由你的程序计算得出，而不是硬编码。",
            "solution": "该问题是有效的，因为它在科学上基于概率论和蒙特卡洛方法，问题陈述清晰（well-posed）且提供了所有必要数据，并且表述客观。任务是推导并实现一个使用指数倾斜的重要性抽样估计器来估计二项分布的尾部概率，这是计算统计学中一个标准且不简单的（non-trivial）问题。\n\n我们旨在估计 $\\alpha = \\mathbb{P}\\{X \\ge k\\}$，其中 $X$ 是一个服从二项分布的随机变量，$X \\sim \\mathrm{Bin}(n,p)$。其概率质量函数（PMF）为 $p(x) = \\mathbb{P}\\{X=x\\} = \\binom{n}{x} p^x (1-p)^{n-x}$，对于 $x \\in \\{0, 1, \\dots, n\\}$。\n\n### 1. 倾斜分布的推导\n\n指数倾斜将原始的 PMF $p(x)$ 修改为一个新的 PMF $q_{\\theta}(x)$，定义为与 $e^{\\theta x} p(x)$ 成正比。为了成为一个有效的 PMF，$q_{\\theta}(x)$ 的总和必须为 1。\n$$\nq_{\\theta}(x) = \\frac{e^{\\theta x} p(x)}{\\sum_{j=0}^{n} e^{\\theta j} p(j)}\n$$\n分母是 $X$ 的矩生成函数（MGF），$M_X(\\theta) = \\mathbb{E}[e^{\\theta X}]$。对于二项分布，$M_X(\\theta) = ((1-p) + pe^{\\theta})^n$。因此，倾斜分布的 PMF 为：\n$$\nq_{\\theta}(x) = \\frac{e^{\\theta x} \\binom{n}{x} p^x (1-p)^{n-x}}{((1-p) + pe^{\\theta})^n}\n$$\n我们可以重新整理这个表达式：\n$$\nq_{\\theta}(x) = \\binom{n}{x} \\frac{(pe^{\\theta})^x (1-p)^{n-x}}{((1-p) + pe^{\\theta})^n} = \\binom{n}{x} \\left(\\frac{pe^{\\theta}}{(1-p) + pe^{\\theta}}\\right)^x \\left(\\frac{1-p}{(1-p) + pe^{\\theta}}\\right)^{n-x}\n$$\n这是一个二项分布 $\\mathrm{Bin}(n, p_{\\theta})$ 的 PMF，其新的成功概率参数为 $p_{\\theta}$。通过观察，我们确定 $p_{\\theta}$ 为：\n$$\np_{\\theta} = \\frac{pe^{\\theta}}{1-p+pe^{\\theta}}\n$$\n我们可以验证 $1-p_{\\theta} = 1 - \\frac{pe^{\\theta}}{1-p+pe^{\\theta}} = \\frac{1-p}{1-p+pe^{\\theta}}$，这与 PMF 中的第二项相匹配。由于 $p \\in (0,1)$ 且 $e^{\\theta}  0$，因此可以得出 $p_{\\theta} \\in (0,1)$。所以，倾斜分布也是二项分布，$q_{\\theta} \\sim \\mathrm{Bin}(n, p_\\theta)$。\n\n### 2. 倾斜参数 $\\theta$ 的最优选择\n\n对于稀有事件的重要性抽样，其目标是选择一个新的分布，使稀有事件变得更可能发生。为了估计 $\\mathbb{P}\\{X \\ge k\\}$，一个标准且有效的策略是选择倾斜参数 $\\theta$，使得倾斜分布的均值中心位于事件区域的边界 $k$。\n\n倾斜分布的均值 $\\mathbb{E}_{\\theta}[X]$ 与累积量生成函数（CGF）$K(\\theta) = \\log M_X(\\theta)$ 相关。\n$$\n\\mathbb{E}_{\\theta}[X] = \\sum_{x=0}^{n} x q_{\\theta}(x) = \\sum_{x=0}^{n} x \\frac{e^{\\theta x} p(x)}{M_X(\\theta)} = \\frac{1}{M_X(\\theta)} \\sum_{x=0}^{n} x e^{\\theta x} p(x)\n$$\nMGF 的导数为 $M'_X(\\theta) = \\frac{d}{d\\theta} \\sum_{x=0}^{n} e^{\\theta x} p(x) = \\sum_{x=0}^{n} x e^{\\theta x} p(x)$。\nCGF 的导数为 $K'(\\theta) = \\frac{M'_X(\\theta)}{M_X(\\theta)}$。\n比较这两者，我们可知 $\\mathbb{E}_{\\theta}[X] = K'(\\theta)$。\n\n我们将倾斜分布的均值设为 $k$：\n$$\nK'(\\theta) = k\n$$\n对于二项 CGF，$K(\\theta) = n \\log(1-p+pe^{\\theta})$。其导数为：\n$$\nK'(\\theta) = n \\frac{pe^{\\theta}}{1-p+pe^{\\theta}} = n p_{\\theta}\n$$\n设 $n p_{\\theta} = k$，可以为新的成功概率得到一个非常简洁的结果：\n$$\np_{\\theta} = \\frac{k}{n}\n$$\n现在我们可以通过将其代入 $p_{\\theta}$ 的方程来解出 $\\theta$：\n$$\n\\frac{k}{n} = \\frac{pe^{\\theta}}{1-p+pe^{\\theta}}\n$$\n解出 $e^{\\theta}$：\n$$\nk(1-p+pe^{\\theta}) = npe^{\\theta} \\\\\nk(1-p) = (np - kp)e^{\\theta} \\\\\nk(1-p) = p(n-k)e^{\\theta} \\\\\ne^{\\theta} = \\frac{k(1-p)}{p(n-k)}\n$$\n这意味着倾斜参数为：\n$$\n\\theta = \\log\\left(\\frac{k(1-p)}{p(n-k)}\\right)\n$$\n这个 $\\theta$ 的选择在 $0  k  n$ 时有效。所提供的测试用例都满足此条件。这种选择使得事件 $\\{X \\ge k\\}$ 在倾斜测度 $\\mathrm{Bin}(n, k/n)$ 下成为典型事件，从而减小了蒙特卡洛估计器的方差。\n\n### 3. 重要性抽样估计器\n\n$\\alpha = \\mathbb{P}\\{X \\ge k\\} = \\mathbb{E}_{p}[\\mathbf{1}\\{X \\ge k\\}]$ 的重要性抽样估计器是使用从倾斜分布中抽取的样本 $X_i \\sim q_{\\theta}$ 构建的：\n$$\n\\widehat{\\alpha} = \\frac{1}{M} \\sum_{i=1}^{M} \\mathbf{1}\\{X_i \\ge k\\} w_{\\theta}(X_i)\n$$\n其中 $w_{\\theta}(x)$ 是原始 PDF $p(x)$ 相对于倾斜 PDF $q_{\\theta}(x)$ 的似然比（或 Radon-Nikodym derivative）：\n$$\nw_{\\theta}(x) = \\frac{p(x)}{q_{\\theta}(x)} = \\frac{p(x)}{e^{\\theta x}p(x)/M_X(\\theta)} = M_X(\\theta)e^{-\\theta x}\n$$\n使用 CGF，这可以写作：\n$$\nw_{\\theta}(x) = e^{K(\\theta) - \\theta x}\n$$\n这个估计器是无偏的，因为 $\\mathbb{E}_{\\theta}[\\mathbf{1}\\{X \\ge k\\} w_{\\theta}(X)] = \\sum_{x=k}^{n} q_{\\theta}(x) \\frac{p(x)}{q_{\\theta}(x)} = \\sum_{x=k}^{n} p(x) = \\mathbb{P}\\{X \\ge k\\}$。\n\n### 4. 算法实现\n\n对于每个测试用例 $(n,p,k)$ 和 $M$ 个样本的模拟预算：\n1.  **计算参数**：计算最优倾斜参数 $\\theta = \\log\\left(\\frac{k(1-p)}{p(n-k)}\\right)$ 和相应的倾斜二项概率 $p_{\\theta} = k/n$。\n2.  **计算 CGF 值**：为了计算权重，我们需要 $K(\\theta)$。我们可以使用通过代入 $p_{\\theta}$ 得到的简化表达式：\n    $K(\\theta) = n \\log(1-p+pe^{\\theta}) = n \\log\\left(\\frac{n(1-p)}{n-k}\\right)$。\n3.  **模拟**：从倾斜分布 $\\mathrm{Bin}(n, p_{\\theta})$ 中抽取 $M$ 个独立样本 $X_1, \\dots, X_M$。\n4.  **估计**：计算重要性抽样估计值：\n    $$\n    \\widehat{\\alpha} = \\frac{1}{M} \\sum_{i=1}^{M} \\mathbf{1}\\{X_i \\ge k\\} e^{K(\\theta) - \\theta X_i}\n    $$\n5.  **验证**：使用二项分布的生存函数（survival function）计算精确概率 $\\alpha^{\\star} = \\mathbb{P}\\{X \\ge k\\}$，该函数通常在数值库中提供。\n6.  **报告**：每个用例的输出是一对值 $[\\widehat{\\alpha}, |\\widehat{\\alpha} - \\alpha^{\\star}|]$。\n\n此过程在以下程序中实现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import binom\n\ndef solve():\n    \"\"\"\n    Designs and implements an importance sampling estimator for the right-tail\n    probability of a binomial distribution using exponential tilting.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (100, 0.30, 45),\n        (1000, 0.20, 260),\n        (200, 0.05, 25),\n        (50, 0.50, 30),\n    ]\n    \n    # Simulation parameters\n    M = 100000\n    seed = 123456789\n    \n    # Initialize the random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n    \n    results = []\n    for n, p, k in test_cases:\n        # Step 1: Derive tilting parameters\n        # The optimal tilted probability p_theta that centers the distribution at k is k/n.\n        p_theta = k / n\n        \n        # Calculate the tilting parameter theta.\n        # This is derived from solving n * p_theta = K'(theta) for theta.\n        # theta = log( (k*(1-p)) / (p*(n-k)) )\n        # This formula is valid for 0  k  n.\n        theta = np.log((k * (1 - p)) / (p * (n - k)))\n        \n        # Calculate the cumulant generating function K(theta). Using the simplified form:\n        # K(theta) = n * log(n*(1-p) / (n-k))\n        K_theta = n * np.log((n * (1 - p)) / (n - k))\n        \n        # Step 2: Simulate M samples from the tilted distribution Bin(n, p_theta).\n        # We sample from Bin(n, k/n), which has a mean of k.\n        tilted_samples = rng.binomial(n, p_theta, size=M)\n        \n        # Step 3: Compute the importance sampling estimate.\n        # The estimate is the sample mean of I{X>=k} * w(X), where X~q_theta.\n        \n        # We only need to sum weights for samples that fall in the event region {X >= k}.\n        indicator = tilted_samples >= k\n        relevant_samples = tilted_samples[indicator]\n        \n        # The likelihood ratio (weight) is w(x) = exp(K(theta) - theta * x).\n        weights = np.exp(K_theta - theta * relevant_samples)\n        \n        # The final estimate is the sum of weights divided by the total number of samples M.\n        estimate = np.sum(weights) / M\n        \n        # Step 4: Compute the exact probability for verification.\n        # P(X >= k) is the survival function at k-1.\n        # scipy.stats.binom.sf(k, n, p) calculates P(X > k).\n        exact_prob = binom.sf(k - 1, n, p)\n        \n        # Step 5: Compute the absolute error.\n        abs_error = np.abs(estimate - exact_prob)\n        \n        results.append([estimate, abs_error])\n\n    # Final print statement in the exact required format '[[est1,err1],[est2,err2],...]'.\n    # Manual string construction is used to avoid spaces.\n    final_output_str = \"[\" + \",\".join([f\"[{est},{err}]\" for est, err in results]) + \"]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}