{
    "hands_on_practices": [
        {
            "introduction": "在离散分布的逆变换采样中，一个直接的方法是自 $k=0$ 开始累加概率质量函数（PMF）。然而，当分布的均值较大时，这种线性扫描的效率会很低。本练习  将利用二项分布的一个基本对称性，即若 $X \\sim \\mathrm{Bin}(n,p)$，则 $n-X \\sim \\mathrm{Bin}(n, 1-p)$，来分析并量化一种简单而强大的优化技巧。通过推导其对算法预期成本的改善，您将不仅学会一个实用技巧，更将锻炼算法分析的核心能力。",
            "id": "3292742",
            "problem": "考虑在随机模拟和蒙特卡洛方法中，使用逆变换法生成二项随机变量。令 $K \\sim \\mathrm{Binomial}(n,p)$ 表示一个二项随机变量，其概率质量函数 (PMF) 为 $f(k) = \\binom{n}{k} p^{k} (1-p)^{n-k}$（对于 $k \\in \\{0,1,\\dots,n\\}$），累积分布函数 (CDF) 为 $F(k) = \\sum_{j=0}^{k} f(j)$。经典的逆变换采样器抽取一个 $U \\sim \\mathrm{Uniform}(0,1)$，并返回满足 $F(k) \\ge U$ 的最小整数 $k$。为此，它从 $k=0$ 开始向上计算 PMF 的连续部分和，直到超过该阈值。将算法成本定义为为生成每个变量所执行的 PMF 求值次数。\n\n你需要分析，将参数重参数化为 $q=\\min(p,1-p)$ 并在 $p$ 被 $1-p$ 替换时使用对称性，将如何影响预期的算法成本。具体来说，考虑以下两种基于逆变换的采样器：\n\n- 基准采样器：直接通过从 $k=0$ 开始对 $f(k)$ 求和来对参数为 $p$ 的分布进行逆变换。\n- 重参数化采样器：定义 $q = \\min(p,1-p)$，通过从 $k=0$ 开始对 $f_{q}(k) = \\binom{n}{k} q^{k} (1-q)^{n-k}$ 求和来对参数为 $q$ 的二项分布进行逆变换，然后，如果 $p1/2$，则返回 $n - K'$，其中 $K' \\sim \\mathrm{Binomial}(n,q)$；如果 $p \\le 1/2$，则返回 $K'$ 不变。\n\n仅从以下基本依据出发：\n- 二项 PMF 和 CDF 的定义。\n- 逆变换采样返回一个具有目标分布的变量这一事实。\n- 期望的线性性。\n\n推导每个采样器的 PMF 期望求值次数，并将改进因子 $I(n,p)$ 定义为基准采样器的期望成本与重参数化采样器的期望成本之比。证明重参数化为 $q=\\min(p,1-p)$ 如何改善运行时间，并通过提供一个用 $n$ 和 $p$ 表示的 $I(n,p)$ 的单一封闭形式解析表达式来量化此改进。此外，讨论当 $p$ 接近 $0$ 和 $p$ 接近 $1$ 时 $I(n,p)$ 的行为。\n\n你的最终答案必须仅为 $I(n,p)$ 的封闭形式表达式。不需要数值近似或四舍五入。",
            "solution": "问题要求分析两种基于逆变换的采样器在生成二项随机变量 $K \\sim \\mathrm{Binomial}(n,p)$ 时的计算成本。成本定义为生成单个变量所需的概率质量函数 (PMF) 的求值次数。我们需要推导每种采样器的期望成本，并求出这些成本的比率，即改进因子 $I(n,p)$。\n\n首先，我们分析所述通用逆变换方法的成本。该采样器生成一个均匀随机数 $U \\sim \\mathrm{Uniform}(0,1)$，并返回使累积分布函数 (CDF) $F(k)$ 满足 $F(k) \\ge U$ 的最小整数 $k$。CDF 是通过对 PMF 值求和来计算的，$F(k) = \\sum_{j=0}^{k} f(j)$。如果采样器返回值 $k_{obs}$，它必定已经检查了 $k=0, 1, \\dots, k_{obs}$ 的条件。计算 $F(k_{obs})$ 需要对 $j=0, 1, \\dots, k_{obs}$ 的 PMF 进行求值。因此，为生成变量 $k_{obs}$ 而执行的 PMF 求值次数为 $k_{obs}+1$。成本 $C$ 本身是一个随机变量，其函数上依赖于生成的随机变量 $K$，即 $C = K+1$。\n\n预期的算法成本是这个随机变量的期望值，$E[C] = E[K+1]$。根据期望的线性性，这变为 $E[C] = E[K] + 1$。\n\n现在，我们将这个通用结果应用于两种采样器。\n\n**1. 基准采样器的期望成本**\n\n基准采样器直接从 $\\mathrm{Binomial}(n,p)$ 分布中生成一个变量 $K$。二项随机变量 $K \\sim \\mathrm{Binomial}(n,p)$ 的期望值是一个标准结果，由 $E[K] = np$ 给出。\n令 $E_{base}$ 为基准采样器的期望成本。使用我们推导的成本公式：\n$$E_{base} = E[K] + 1 = np + 1$$\n\n**2. 重参数化采样器的期望成本**\n\n重参数化采样器首先定义 $q = \\min(p, 1-p)$。然后它使用相同的逆变换方法生成一个中间随机变量 $K' \\sim \\mathrm{Binomial}(n,q)$。采样器的成本由 PMF 求值的次数定义，这仅发生在此生成步骤中。$K'$ 的后续转换（直接返回 $K'$ 或返回 $n - K'$）不涉及 PMF 求值，根据问题的成本定义，这是无成本的。\n\n因此，该采样器的成本就是生成 $K'$ 的成本。令 $E_{reparam}$ 为此期望成本。中间变量 $K' \\sim \\mathrm{Binomial}(n,q)$ 的期望值为 $E[K'] = nq$。\n遵循与基准采样器相同的逻辑，期望成本为：\n$$E_{reparam} = E[K'] + 1 = nq + 1$$\n代入 $q$ 的定义：\n$$E_{reparam} = n \\min(p, 1-p) + 1$$\n值得注意的是，这个采样器是有效的。如果 $p \\le 1/2$，则 $q=p$，采样器返回 $K' \\sim \\mathrm{Binomial}(n,p)$，这是正确的。如果 $p  1/2$，则 $q=1-p$，采样器返回 $n-K'$，其中 $K' \\sim \\mathrm{Binomial}(n,1-p)$。二项分布的一个已知性质是，如果 $X \\sim \\mathrm{Binomial}(n, \\theta)$，那么 $n-X \\sim \\mathrm{Binomial}(n, 1-\\theta)$。因此，$n-K' \\sim \\mathrm{Binomial}(n, 1-(1-p)) = \\mathrm{Binomial}(n,p)$，所以在这种情况下输出也是正确的。\n\n**3. 改进因子 $I(n,p)$**\n\n改进因子 $I(n,p)$ 定义为基准采样器的期望成本与重参数化采样器的期望成本之比。\n$$I(n,p) = \\frac{E_{base}}{E_{reparam}} = \\frac{np + 1}{n \\min(p, 1-p) + 1}$$\n此表达式即为所求的改进因子的封闭形式解析表达式。\n\n**4. 改进因子的讨论**\n\n我们可以通过考虑基于 $p$ 值的两种情况来分析 $I(n,p)$ 的行为。\n\n情况 1：$p \\le 1/2$。\n在这种情况下，$\\min(p, 1-p) = p$。改进因子变为：\n$$I(n,p) = \\frac{np + 1}{np + 1} = 1$$\n这表明对于 $p \\le 1/2$，重参数化采样器与基准采样器相同，在期望运行时间上没有改进。这是合乎逻辑的，因为重参数化 $q=\\min(p,1-p)$ 会得到 $q=p$。\n\n情况 2：$p  1/2$。\n在这种情况下，$\\min(p, 1-p) = 1-p$。改进因子变为：\n$$I(n,p) = \\frac{np + 1}{n(1-p) + 1}$$\n由于 $p > 1/2$，因此 $p > 1-p$。对于 $n0$，这意味着 $np > n(1-p)$，因此 $np+1 > n(1-p)+1$。因此，对于 $p  1/2$，$I(n,p)  1$，表示性能有严格的改进。重参数化利用二项分布的对称性，将一个高均值（$np > n/2$）问题转化为一个低均值（$n(1-p)  n/2$）问题，这对于一个简单的向上搜索逆变换采样器而言成本更低。\n\n最后，我们考察在参数 $p \\in [0,1]$ 的边界处 $I(n,p)$ 的行为。\n- 当 $p \\to 0^+$ 时，我们处于情况 1，并且 $I(n,p) = 1$。正如预期的那样，没有改进。\n- 当 $p \\to 1^-$ 时，我们处于情况 2。我们对 $I(n,p)$ 的表达式取极限：\n$$\\lim_{p \\to 1^-} I(n,p) = \\lim_{p \\to 1^-} \\frac{np + 1}{n(1-p) + 1} = \\frac{n(1) + 1}{n(1-1) + 1} = \\frac{n+1}{1} = n+1$$\n当 $p$ 接近 $1$ 时，改进因子接近 $n+1$。这代表了非常显著的加速。基准采样器的期望成本接近 $n+1$，因为它必须对几乎所有 $n+1$ 个概率质量进行求和。相比之下，重参数化采样器的期望成本接近 $n(1-1)+1=1$，因为它从一个均值接近 $0$ 的分布中生成变量。",
            "answer": "$$\\boxed{\\frac{np + 1}{n \\min(p, 1-p) + 1}}$$"
        },
        {
            "introduction": "延续高效逆变换采样的主题，我们探索一种更高级的算法。与从头开始的线性扫描不同，我们可以利用二项分布的单峰特性来优化采样过程。本练习  要求您首先推导出一个关键的概率质量函数递推关系，然后设计并实现一个从分布的众数（即最可能出现的值）开始向外扩展的采样器。这是一个实用且优雅的算法，它展示了对分布结构的深刻理解如何催生出更优越的算法设计。",
            "id": "3292698",
            "problem": "考虑一个服从二项分布的离散随机变量 $X$，其概率质量函数定义为 $P(X=k)$，其中 $k \\in \\{0,1,\\dots,n\\}$，参数为 $n \\in \\mathbb{N}$ 和 $p \\in [0,1]$。从二项分布的基本定义出发，即它是 $n$ 次独立同分布的伯努利试验的总和，每次试验的成功概率为 $p$；并结合经过充分检验的二项分布概率质量函数公式 $P(X=k)$，即 $n$ 选 $k$ 的组合乘以 $p^{k}$ 再乘以 $(1-p)^{n-k}$。请从第一性原理出发，推導出一个形式如下的连续概率递推关系\n$$\nP(X=k+1)=P(X=k)\\cdot\\frac{(n-k)}{(k+1)}\\cdot\\frac{p}{(1-p)}.\n$$\n在此递推关系建立之后，设计并实现一个针对 $X$ 的基于逆变换的采样算法，该算法通过从众数开始向外累积概率质量函数值来生成单个二项分布随机数。该算法必须：\n- 当 $p \\in (0,1)$ 时，计算众数 $m=\\lfloor (n+1)p \\rfloor$，并明确处理 $p=0$ 和 $p=1$ 的退化情况，此时所有概率质量分别集中在 $k=0$ 和 $k=n$。\n- 在 $k=m$ 处进行初始化，使用基于对数和伽马函数的数值稳定表达式计算 $P(X=m)$。\n- 使用推导出的递推关系从 $P(X=m)$ 计算出 $P(X=m+1)$ 和 $P(X=m-1)$，然后继续向外扩展，交替选择当前具有较大概率质量的一侧，从单个均匀分布随机数 $U \\sim \\mathrm{Uniform}(0,1)$ 中减去每个访问过的概率质量，直到 $U$ 的剩余值落入当前概率质量的范围内，此时返回相应的索引 $k$。\n- 确保每个概率质量 $P(X=k)$ 最多被访问一次，并且算法能以正确的样本值停止。讨论为何该算法的期望运行时间与采样索引和众数之间的期望绝对距离 $|k-m|$ 是同阶的，其依据是二项分布概率质量函数围绕众数的单峰性和单调递减性。\n\n您的程序必须实现此算法，并将其应用于以下测试套件，其中指定了参数 $(n,p,U)$：\n- 测试用例 1：$(n,p,U)=(50,\\,0.3,\\,0.73)$。\n- 测试用例 2：$(n,p,U)=(1000,\\,0.5,\\,0.5)$。\n- 测试用例 3：$(n,p,U)=(20,\\,0.8,\\,0.999999)$。\n- 测试用例 4：$(n,p,U)=(10,\\,1.0,\\,0.42)$。\n- 测试用例 5：$(n,p,U)=(7,\\,0.0,\\,0.999)$。\n- 测试用例 6：$(n,p,U)=(200,\\,0.05,\\,0.2)$。\n\n每个测试用例产生一个整数输出，代表采样值 $k$。您的程序应生成一行输出，其中包含用方括号括起来并以逗号分隔的结果列表，例如 $[k_1,k_2,k_3,k_4,k_5,k_6]$。不涉及物理单位，所有角度都与本问题无关。所有答案均为整数，不使用百分比。",
            "solution": "该问题被评估为有效的，因为它在科学上基于概率论的原理，特别是二项分布和逆变换采样方法。问题提法明确，为其执行提供了完整且一致的算法规范和所需数据。语言客观且正式。\n\n### 概率递推关系的推导\n\n设 $X$ 是一个参数为 $n \\in \\mathbb{N}$ 和 $p \\in [0,1]$ 的二项随机变量。其概率质量函数 (PMF) 由下式给出：\n$$\nP(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} = \\frac{n!}{k!(n-k)!} p^k (1-p)^{n-k}\n$$\n其中 $k \\in \\{0, 1, \\dots, n\\}$。\n\n我们希望找到连续概率 $P(X=k+1)$ 和 $P(X=k)$ 之间的递推关系。我们可以将 $k+1$ 的 PMF 表示为：\n$$\nP(X=k+1) = \\binom{n}{k+1} p^{k+1} (1-p)^{n-(k+1)} = \\frac{n!}{(k+1)!(n-k-1)!} p^{k+1} (1-p)^{n-k-1}\n$$\n为了找到递推关系，我们计算比率 $\\frac{P(X=k+1)}{P(X=k)}$：\n$$\n\\frac{P(X=k+1)}{P(X=k)} = \\frac{\\frac{n!}{(k+1)!(n-k-1)!} p^{k+1} (1-p)^{n-k-1}}{\\frac{n!}{k!(n-k)!} p^k (1-p)^{n-k}}\n$$\n这可以分为两部分：二项式系数的比率和概率项的比率。\n\n二项式系数的比率简化为：\n$$\n\\frac{\\binom{n}{k+1}}{\\binom{n}{k}} = \\frac{n!}{(k+1)!(n-k-1)!} \\cdot \\frac{k!(n-k)!}{n!} = \\frac{k!(n-k)(n-k-1)!}{(k+1)k!(n-k-1)!} = \\frac{n-k}{k+1}\n$$\n概率项的比率简化为：\n$$\n\\frac{p^{k+1}(1-p)^{n-k-1}}{p^k(1-p)^{n-k}} = \\frac{p}{1-p}\n$$\n结合这两个结果，得到连续概率的比率：\n$$\n\\frac{P(X=k+1)}{P(X=k)} = \\frac{n-k}{k+1} \\cdot \\frac{p}{1-p}\n$$\n重新整理此方程，得到所需的前向递推关系：\n$$\nP(X=k+1) = P(X=k) \\cdot \\frac{n-k}{k+1} \\cdot \\frac{p}{1-p}\n$$\n由此，我们也可以推导出用于计算小于众数值的概率的后向递推关系：\n$$\nP(X=k) = P(X=k+1) \\cdot \\frac{k+1}{n-k} \\cdot \\frac{1-p}{p}\n$$\n\n### 算法设计与分析\n\n该问题要求一种基于逆变换的采样算法，该算法从概率质量函数的众数向外进行探索。此方法是一种有效的采样技术，因为对于随机变量 $U \\sim \\mathrm{Uniform}(0,1)$，算法在值 $k$ 处终止的概率恰好是 $P(X=k)$。标准的逆变换采样是按 $k$ 的递增顺序对概率求和，而此算法重新排列了求和顺序，这不影响采样方法本身的有效性，尽管对于给定的 $U$，它可能产生与标准方法不同的样本。\n\n该算法按以下步骤进行：\n\n1.  **处理退化情况**：如果 $p=1$，分布是确定性的，所有概率质量都在 $k=n$ 处。算法必须返回 $n$。如果 $p=0$，所有概率质量都在 $k=0$ 处，因此必须返回 $0$。\n\n2.  **计算众数**：对于 $p \\in (0,1)$，二项分布的 PMF 是单峰的。众数 $m$ 是使 $P(X=k)$ 最大化的 $k$ 值，由 $m = \\lfloor(n+1)p\\rfloor$ 给出。这将是我们的起点。\n\n3.  **在众数处初始化**：必须计算众数处的概率 $P(X=m)$。为避免大 $n$ 导致的数值下溢或上溢，我们首先计算 PMF 的对数，然后取幂。对数 PMF 为：\n    $$\n    \\ln P(X=m) = \\ln\\left(\\binom{n}{m}\\right) + m\\ln(p) + (n-m)\\ln(1-p)\n    $$\n    二项式系数的对数 $\\ln(\\binom{n}{m}) = \\ln(n!) - \\ln(m!) - \\ln((n-m)!)$ 使用对数伽马函数 $\\ln(k!) = \\text{gammaln}(k+1)$ 计算，以保证数值稳定性：\n    $$\n    \\ln P(X=m) = \\text{gammaln}(n+1) - \\text{gammaln}(m+1) - \\text{gammaln}(n-m+1) + m\\ln p + (n-m)\\ln(1-p)\n    $$\n    由此，$P(X=m) = \\exp(\\ln(P(X=m)))$。\n\n4.  **通过向外扩展进行逆变换采样**：给定一个均匀分布随机数 $U \\sim \\mathrm{Uniform}(0,1)$，我们使用逆变换采样的“累积并检验”原则。\n    *   初始化累积概率和 $S = P(X=m)$。如果 $U \\le S$，则样本为 $m$。\n    *   否则，算法从众数向外扩展。我们维护两个指针 $k_{low} = m-1$ 和 $k_{high} = m+1$，以及使用推导出的递推关系计算出的相应概率 $P_{low} = P(X=k_{low})$ 和 $P_{high} = P(X=k_{high})$。\n    *   在一个循环中，我们比较 $P_{low}$ 和 $P_{high}$。问题陈述要求我们必须选择概率质量较大的一侧。\n    *   如果 $P_{high} > P_{low}$ (且 $k_{high} \\le n$)，我们将 $P_{high}$ 加到和 $S$ 中。如果 $U \\le S$，我们返回 $k_{high}$。否则，我们使用前向递推更新 $P_{high}$ 为 $k_{high}+1$ 处的概率，并递增 $k_{high}$。\n    *   如果 $P_{low} \\ge P_{high}$ (且 $k_{low} \\ge 0$)，我们将 $P_{low}$ 加到 $S$ 中。如果 $U \\le S$，我们返回 $k_{low}$。否则，我们使用后向递推更新 $P_{low}$ 为 $k_{low}-1$ 处的概率，并递减 $k_{low}$。\n    *   该过程一直持续到满足条件 $U \\le S$ 为止。循环保证会终止，因为 $S$ 严格递增并趋近于 1。\n\n5.  **运行时间分析**：循环中每一步的计算成本是恒定的。步数是在累积和超过 $U$ 之前我们必须求和的概率数量。由于我们从众数 $m$ (最可能的值) 开始，并向概率较小的值扩展，所以对于对应于接近众数的 $k$ 值的 $U$，算法将更快终止。因此，期望迭代次数与随机变量与其众数的期望绝对偏差 $E[|X-m|]$ 相关。对于二项分布，其离散程度由标准差 $\\sigma = \\sqrt{np(1-p)}$ 来表征。期望距离 $E[|X-m|]$ 与 $\\sigma$ 同阶。因此，该算法的期望运行时间为 $O(\\sqrt{np(1-p)})$，对于大的 $n$，这比标准逆变换方法的 $O(np)$ 更高效。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef generate_binomial_variate(n, p, u):\n    \"\"\"\n    Generates a single binomial variate using an inversion-based method\n    that expands outward from the mode.\n    \n    Args:\n        n (int): The number of trials.\n        p (float): The success probability of each trial.\n        u (float): A uniform random variate from (0, 1).\n    \n    Returns:\n        int: The generated binomial variate k.\n    \"\"\"\n    # Step 1: Handle degenerate cases\n    if p == 0.0:\n        return 0\n    if p == 1.0:\n        return n\n\n    # Step 2: Compute the mode\n    m = int(np.floor((n + 1) * p))\n\n    # Step 3: Initialize at the mode with a numerically stable PMF calculation\n    log_p = np.log(p)\n    log_1_minus_p = np.log(1.0 - p)\n    \n    log_pmf_m = (gammaln(n + 1) - gammaln(m + 1) - gammaln(n - m + 1)\n                 + m * log_p + (n - m) * log_1_minus_p)\n    pmf_m = np.exp(log_pmf_m)\n\n    # Step 4: Inversion sampling by outward expansion\n    cum_prob = pmf_m\n    if u = cum_prob:\n        return m\n\n    # Initialize pointers and probabilities for the expansion\n    k_low = m - 1\n    k_high = m + 1\n    \n    # Pre-calculate recursion ratio for efficiency\n    p_ratio = p / (1.0 - p)\n\n    # Calculate initial probabilities for the two sides\n    # Use -1.0 as a sentinel for out-of-bounds indices\n    pmf_low = pmf_m * (m / (n - m + 1.0)) / p_ratio if k_low >= 0 else -1.0\n    pmf_high = pmf_m * ((n - m) / (m + 1.0)) * p_ratio if k_high = n else -1.0\n\n    while True:\n        # Determine which side has the larger next probability mass\n        # If pmf_high = pmf_low, we default to the 'low' side (includes tie-breaking)\n        if pmf_high > pmf_low:\n            cum_prob += pmf_high\n            if u = cum_prob:\n                return k_high\n            \n            # Update for the next step on the high side\n            k_high_current = k_high\n            k_high += 1\n            if k_high = n:\n                pmf_high *= ((n - k_high_current) / (k_high_current + 1.0)) * p_ratio\n            else: # Boundary reached\n                pmf_high = -1.0\n        else:\n            if pmf_low  0: # Both sides exhausted, should not happen. Break to be safe.\n                break\n\n            cum_prob += pmf_low\n            if u = cum_prob:\n                return k_low\n\n            # Update for the next step on the low side\n            k_low_current = k_low\n            k_low -= 1\n            if k_low >= 0:\n                pmf_low *= (k_low_current / (n - k_low_current + 1.0)) / p_ratio\n            else: # Boundary reached\n                pmf_low = -1.0\n    \n    # This part should ideally not be reached if u in [0,1) and calculations are precise.\n    # It can be a safeguard for floating point issues.\n    return n if p > 0.5 else 0\n\n\ndef solve():\n    \"\"\"\n    Executes the binomial variate generation algorithm for the specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement: (n, p, U)\n    test_cases = [\n        (50, 0.3, 0.73),\n        (1000, 0.5, 0.5),\n        (20, 0.8, 0.999999),\n        (10, 1.0, 0.42),\n        (7, 0.0, 0.999),\n        (200, 0.05, 0.2),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, p, u = case\n        result = generate_binomial_variate(n, p, u)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现代模拟任务常常需要生成海量样本，这对并行计算提出了要求。幸运的是，二项分布的聚合性质为并行化提供了理论基础：具有相同成功概率 $p$ 的独立二项分布变量之和仍然服从二项分布。在本练习  中，您将从第一性原理出发证明该性质，并基于此设计和验证一个并行二项分布生成器。这个过程将涉及负载均衡和随机数流管理等实际工程问题，让您体验如何将理论性质转化为可扩展的计算方案。",
            "id": "3292711",
            "problem": "您需要从第一性原理出发，推断二项分布的聚合特性，然后设计并验证一个利用此特性的并行二项分布变量生成器。任务分为三个部分。\n\nA部分（理论推导）。仅从以下定义出发：一个参数为 $n$ 和 $p$ 的二项随机变量可以表示为 $n$ 个成功概率为 $p$ 的独立同分布伯努利随机变量之和，以及独立性的定义，推导出如果 $X_1$ 和 $X_2$ 独立，且 $X_1 \\sim \\mathrm{Bin}(n_1,p)$，$X_2 \\sim \\mathrm{Bin}(n_2,p)$，那么 $X_1 + X_2 \\sim \\mathrm{Bin}(n_1 + n_2, p)$。您的推导必须是自洽的，并且只能依赖于这些基本原理，例如指示和表示法、期望的基本性质、独立离散分布的卷积或概率生成函数。不得引用任何针对二项分布的专门预推导卷积恒等式。\n\nB部分（算法设计）。利用A部分的结果，设计一个并行的二项分布变量生成器。给定整数 $n \\ge 0$、概率 $p \\in [0,1]$ 以及整数个工作单元 $w \\ge 1$，请具体说明如何将 $n$ 划分为非负整数 $(n_1,\\dots,n_w)$ 使得 $\\sum_{j=1}^w n_j = n$，让每个工作单元 $j$ 独立地生成 $X^{(j)} \\sim \\mathrm{Bin}(n_j,p)$，并返回聚合结果 $X = \\sum_{j=1}^w X^{(j)}$。使用A部分的结论来证明其正确性。明确讨论：\n- 一种有原则的划分选择，例如 $n_j \\in \\{\\lfloor n/w \\rfloor,\\lceil n/w \\rceil\\}$，以平衡计算负载。\n- 工作单元之间的独立性要求，以及如何通过不同的伪随机数生成器（PRNG）子流来实现。\n- 就 $n$、$w$ 和请求的样本数量 $m$ 而言的计算复杂度，比较中心化生成器与并行聚合方法。\n\nC部分（实现与验证）。实现一个完整、可运行的程序，该程序：\n- 使用可复现的种子方案来构建独立的PRNG流。设基础种子为 $S=123456789$。对于测试用例索引 $t \\in \\{0,1,2,\\dots\\}$，将中心化生成器的种子定义为 $S_\\mathrm{cent}(t) = S + 10^6 t + 1$。对于测试用例 $t$ 中的并行生成器，定义并行基础种子为 $S_\\mathrm{par}(t) = S + 10^6 t + 2$，工作单元 $j$ 的种子为 $S_\\mathrm{par}(t) + j$，其中 $j \\in \\{0,1,\\dots,w-1\\}$。每个PRNG流必须由其指定的组件独占使用。\n- 对每个测试用例，使用中心化的二项分布生成器为 $(n,p)$ 生成 $m$ 个独立样本，并使用您设计的具有 $w$ 个工作单元的并行聚合生成器为相同的 $(n,p)$ 生成 $m$ 个独立样本。\n- 通过为每个测试用例计算以下内容来验证并行生成器：\n  1. 经验概率质量函数（pmf）的上确界范数差异 $D = \\max_{k \\in \\{0,\\dots,n\\}} \\left| \\hat{p}_\\mathrm{par}(k) - \\hat{p}_\\mathrm{cent}(k) \\right|$，其中 $\\hat{p}$ 表示频率除以 $m$。\n  2. 并行样本均值相对于理论均值的绝对误差 $| \\overline{X}_\\mathrm{par} - n p |$。\n  3. 并行样本方差相对于理论方差的绝对误差 $| s^2_\\mathrm{par} - n p (1-p) |$，其中 $s^2_\\mathrm{par}$ 是并行样本的总体方差（分母为 $m$）。\n  4. 一个布尔值的通过/失败标志，当且仅当以下所有条件同时成立时为 `True`：\n     - $D \\le 0.02$，\n     - $| \\overline{X}_\\mathrm{par} - n p | \\le 4 \\sqrt{ \\frac{n p (1-p)}{m} }$,\n     - $| s^2_\\mathrm{par} - n p (1-p) | \\le \\frac{6\\, n p (1-p)}{\\sqrt{m}}$。\n- 使用以下参数值测试套件，以确保覆盖不同情景：\n  1. 用例1： $(n,p,w,m) = (60,\\, 0.3,\\, 3,\\, 50000)$。\n  2. 用例2： $(n,p,w,m) = (1,\\, 0.5,\\, 7,\\, 40000)$。\n  3. 用例3： $(n,p,w,m) = (1000,\\, 0.01,\\, 8,\\, 50000)$。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。此列表的每个元素对应一个测试用例，并且其本身必须是包含四个量 $[D,\\,\\text{mean\\_error},\\,\\text{var\\_error},\\,\\text{pass}]$ 的列表，其中 $D$、$\\text{mean\\_error}$ 和 $\\text{var\\_error}$ 是精确到小数点后六位的浮点数，$\\text{pass}$ 是一个布尔值。例如，一个符合要求的输出行格式为 $[[0.012345,0.000678,0.001234,True],[\\dots],[\\dots]]$。\n\n注意：\n- 不涉及物理单位。\n- 所有概率必须表示为 $[0,1]$ 区间内的小数。\n- 您可以使用任何现代编程语言来实现算法逻辑，但最终答案必须是一个完整、可运行的程序。运行时必须是自包含的，并且不得需要用户输入、外部文件或网络访问。",
            "solution": "用户请求了一个涉及二项分布的三部分解决方案：理论推导、算法设计和用于验证的编程实现。\n\n### 问题验证\n\n**第1步：提取给定信息**\n\n*   **A部分（理论）：**\n    *   一个二项随机变量 $\\mathrm{Bin}(n,p)$ 是 $n$ 个成功概率为 $p$ 的独立同分布（i.i.d.）伯努利随机变量之和。\n    *   $X_1$ 和 $X_2$ 是独立随机变量，其中 $X_1 \\sim \\mathrm{Bin}(n_1,p)$ 且 $X_2 \\sim \\mathrm{Bin}(n_2,p)$。\n    *   推导出 $X_1 + X_2 \\sim \\mathrm{Bin}(n_1 + n_2, p)$。\n    *   推导必须是自洽的，且只能使用基本原理（例如，指示和表示法），不能使用预先推导的二项分布卷积恒等式。\n\n*   **B部分（算法）：**\n    *   使用 $w$ 个工作单元为参数 $(n, p)$ 设计一个并行二项分布变量生成器。\n    *   将 $n$ 划分为非负整数 $(n_1, \\dots, n_w)$，使得 $\\sum_{j=1}^w n_j = n$。\n    *   工作单元 $j$ 生成 $X^{(j)} \\sim \\mathrm{Bin}(n_j, p)$。\n    *   最终结果为 $X = \\sum_{j=1}^w X^{(j)}$。\n    *   讨论一种有原则的划分方法（例如，$n_j \\in \\{\\lfloor n/w \\rfloor, \\lceil n/w \\rceil\\}$）以实现负载均衡。\n    *   讨论独立性要求和PRNG子流。\n    *   为 $m$ 个样本分析中心化生成器与并行生成器的计算复杂度。\n\n*   **C部分（实现）：**\n    *   **种子方案：**\n        *   基础种子：$S = 123456789$。\n        *   测试用例索引 $t \\in \\{0, 1, 2, \\dots\\}$。\n        *   中心化生成器种子：$S_\\mathrm{cent}(t) = S + 10^6 t + 1$。\n        *   并行生成器基础种子：$S_\\mathrm{par}(t) = S + 10^6 t + 2$。\n        *   工作单元 $j$ 的种子：$S_\\mathrm{par}(t) + j$，其中 $j \\in \\{0, \\dots, w-1\\}$。\n    *   **生成任务：** 对每个测试用例，使用中心化生成器和并行生成器各生成 $m$ 个样本。\n    *   **验证指标：**\n        1.  $D = \\max_{k \\in \\{0,\\dots,n\\}} \\left| \\hat{p}_\\mathrm{par}(k) - \\hat{p}_\\mathrm{cent}(k) \\right|$（PMF上确界范数差异）。\n        2.  $| \\overline{X}_\\mathrm{par} - n p |$（并行样本均值的绝对误差）。\n        3.  $| s^2_\\mathrm{par} - n p (1-p) |$（并行样本方差的绝对误差，分母为 $m$）。\n    *   **通过/失败标准：**\n        *   布尔值 `pass` 为真，当且仅当以下所有条件成立：\n            *   $D \\le 0.02$。\n            *   $| \\overline{X}_\\mathrm{par} - n p | \\le 4 \\sqrt{ \\frac{n p (1-p)}{m} }$。\n            *   $| s^2_\\mathrm{par} - n p (1-p) | \\le \\frac{6\\, n p (1-p)}{\\sqrt{m}}$。\n    *   **测试套件：**\n        1.  $(n,p,w,m) = (60, 0.3, 3, 50000)$。\n        2.  $(n,p,w,m) = (1, 0.5, 7, 40000)$。\n        3.  $(n,p,w,m) = (1000, 0.01, 8, 50000)$。\n    *   **输出格式：** 单行 `[[D1,mean_error1,var_error1,pass1],[D2,...],...]`，浮点数保留6位小数。\n\n**第2步：使用提取的给定信息进行验证**\n\n问题陈述被评估为有效。\n*   **科学性（关键）：** 该问题植根于基础概率论，特别是二项分布的性质和随机模拟的原理。聚合特性是一个公认的定理。验证指标是比较分布和估计量的标准统计度量。不存在伪科学或无根据的主张。\n*   **定义明确：** 该问题提供了所有必要的参数、定义、公式和测试用例。每个部分的目标都清晰明确，理论、算法和实现任务都存在唯一且有意义的解。\n*   **客观性（关键）：** 语言精确且量化。包括种子方案和验证阈值在内的所有要求都是客观规定的，没有主观解释的余地。\n\n**第3步：结论与行动**\n\n问题是**有效的**。开始进行解答。\n\n### A部分：理论推导\n\n命题是：如果 $X_1 \\sim \\mathrm{Bin}(n_1, p)$ 和 $X_2 \\sim \\mathrm{Bin}(n_2, p)$ 是独立随机变量，那么它们的和 $Y = X_1 + X_2$ 服从二项分布 $\\mathrm{Bin}(n_1 + n_2, p)$。我们将从指定的第一性原理推导此命题。\n\n**1. 二项分布变量的指示和表示法：**\n根据定义，一个随机变量 $X$ 服从参数为 $n$ 和 $p$ 的二项分布（记作 $X \\sim \\mathrm{Bin}(n, p)$），如果它可以表示为 $n$ 个独立同分布（i.i.d.）的伯努利随机变量之和，每个伯努利变量的成功概率为 $p$。设这些伯努利变量为 $\\{B_i\\}_{i=1}^n$，其中 $B_i \\sim \\mathrm{Bernoulli}(p)$。那么，$X = \\sum_{i=1}^{n} B_i$。每个 $B_i$ 以概率 $p$ 取值 $1$（成功），以概率 $1-p$ 取值 $0$（失败）。\n\n**2. $X_1$ 和 $X_2$ 的表示：**\n给定 $X_1 \\sim \\mathrm{Bin}(n_1, p)$，我们可以将其表示为 $n_1$ 个独立同分布的伯努利变量之和：\n$$X_1 = \\sum_{i=1}^{n_1} U_i, \\quad \\text{其中 } U_i \\sim \\mathrm{Bernoulli}(p) \\text{ 是 i.i.d.}$$\n类似地，对于 $X_2 \\sim \\mathrm{Bin}(n_2, p)$，我们可以写成：\n$$X_2 = \\sum_{j=1}^{n_2} V_j, \\quad \\text{其中 } V_j \\sim \\mathrm{Bernoulli}(p) \\text{ 是 i.i.d.}$$\n\n**3. 组合表示：**\n和 $Y = X_1 + X_2$ 可以写成：\n$$Y = \\left(\\sum_{i=1}^{n_1} U_i\\right) + \\left(\\sum_{j=1}^{n_2} V_j\\right)$$\n问题陈述中指出 $X_1$ 和 $X_2$ 是独立的。由于 $X_1$ 是随机变量集合 $\\{U_i\\}_{i=1}^{n_1}$ 的函数，而 $X_2$ 是 $\\{V_j\\}_{j=1}^{n_2}$ 的函数，因此 $X_1$ 和 $X_2$ 的独立性意味着随机变量的整个集合 $\\{U_i\\}$ 独立于整个集合 $\\{V_j\\}$。\n\n因此，所有 $n_1 + n_2$ 个伯努利变量的集合 $\\{U_1, \\dots, U_{n_1}, V_1, \\dots, V_{n_2}\\}$ 是一组相互独立的随机变量。由于所有的 $U_i$ 和 $V_j$ 都来自同一个 $\\mathrm{Bernoulli}(p)$ 分布，这个集合构成了一组 $n_1 + n_2$ 个独立同分布的伯努利随机变量。\n\n**4. 结论：**\n和 $Y$ 是 $n_1 + n_2$ 个独立同分布的 $\\mathrm{Bernoulli}(p)$ 变量之和。根据二项随机变量的基本定义，$Y$ 必须服从一个二项分布，其参数对应于总试验次数和共同的成功概率。\n因此，$Y = X_1 + X_2 \\sim \\mathrm{Bin}(n_1 + n_2, p)$。推导完毕。\n\n### B部分：算法设计\n\nA部分的理论结果是并行二项分布变量生成器的基础。\n\n**1. 算法与正确性：**\n该算法将总试验次数 $n$ 分配给 $w$ 个并行工作单元。\n*   **划分：** 将整数 $n$ 分为 $w$ 个非负整数 $n_1, n_2, \\dots, n_w$，使得 $\\sum_{j=1}^w n_j = n$。\n*   **并行生成：** 每个工作单元 $j$ 负责独立生成一个随机变量 $X^{(j)} \\sim \\mathrm{Bin}(n_j, p)$。\n*   **聚合：** 最终结果是所有工作单元生成的变量之和：$X = \\sum_{j=1}^w X^{(j)}$。\n\n**正确性：** 此过程的正确性可以通过对工作单元数量 $w$ 进行归纳，并使用A部分的结果来证明。\n*   **基础情况 ($w=2$)：** 我们有 $X = X^{(1)} + X^{(2)}$。由于工作单元独立运行，$X^{(1)} \\sim \\mathrm{Bin}(n_1, p)$ 和 $X^{(2)} \\sim \\mathrm{Bin}(n_2, p)$ 是独立的。根据A部分，它们的和是 $X \\sim \\mathrm{Bin}(n_1 + n_2, p) = \\mathrm{Bin}(n, p)$。\n*   **归纳步骤：** 假设对于 $k-1$ 个工作单元，其和 $S_{k-1} = \\sum_{j=1}^{k-1} X^{(j)}$ 服从 $\\mathrm{Bin}(\\sum_{j=1}^{k-1} n_j, p)$ 分布。现在考虑 $k$ 个工作单元。其和为 $S_k = S_{k-1} + X^{(k)}$。根据工作单元的独立性，$S_{k-1}$ 独立于 $X^{(k)} \\sim \\mathrm{Bin}(n_k, p)$。再次应用A部分的结果，$S_k \\sim \\mathrm{Bin}((\\sum_{j=1}^{k-1} n_j) + n_k, p) = \\mathrm{Bin}(\\sum_{j=1}^{k} n_j, p)$。\n通过归纳法，对于 $w$ 个工作单元，最终的和 $X$ 服从 $\\mathrm{Bin}(\\sum_{j=1}^w n_j, p) = \\mathrm{Bin}(n, p)$ 分布。\n\n**2. 用于负载均衡的原则性划分：**\n为了最大化并行计算的效率，计算负载应尽可能均匀地分配给各个工作单元。生成一个二项分布变量的时间大致与试验次数 $n_j$ 成正比。因此，$n_j$ 的值应尽可能彼此接近。一种标准方法是使用带余除法。设 $n = q \\cdot w + r$，其中 $q = \\lfloor n/w \\rfloor$ 是商，$r = n \\pmod w$ 是余数（$0 \\le r  w$）。\n*   将 $n_j = q+1$ 次试验分配给 $r$ 个工作单元。\n*   将 $n_j = q$ 次试验分配给剩下的 $w-r$ 个工作单元。\n这确保了任意两个工作单元的试验次数之差最多为1。这与建议的选择 $n_j \\in \\{\\lfloor n/w \\rfloor, \\lceil n/w \\rceil\\}$ 相符。\n\n**3. 独立性与PRNG子流：**\n理论上对 $X^{(j)}$ 之间独立性的要求，在模拟中是通过确保每个工作单元使用独立的伪随机数流来实现的。如果所有工作单元都使用相同的伪随机数生成器（PRNG）和相同的种子，它们的输出将完全相同，从而违反了独立性。一种稳健的方法是为每个工作单元的PRNG初始化一个唯一的种子。问题中为此指定了一个确定性方案：给定一个测试用例的并行基础种子 $S_{\\mathrm{par}}(t)$，工作单元 $j$ 的PRNG使用 $S_{\\mathrm{par}}(t) + j$ 作为种子。对于高质量的PRNG（如`numpy.random.Generator`中使用的PCG64），此方法能产生统计上独立的随机流。\n\n**4. 计算复杂度：**\n让我们分析生成 $m$ 个样本的复杂度，假设生成一个 $\\mathrm{Bin}(k,p)$ 样本的成本为 $O(k)$。\n*   **中心化生成器：** 单个进程从 $\\mathrm{Bin}(n,p)$ 生成 $m$ 个样本。\n    *   总工作量（CPU时间）：$m \\times O(n) = O(mn)$。\n    *   挂钟时间（在单核上）：$O(mn)$。\n*   **并行聚合生成器（使用 $w$ 个核心）：** $w$ 个工作单元中的每一个都从 $\\mathrm{Bin}(n_j, p)$ 生成 $m$ 个样本，其中 $n_j \\approx n/w$。\n    *   总工作量（CPU时间）：总计算量为 $\\sum_{j=1}^{w} O(m n_j) = O(m \\sum_{j=1}^{w} n_j) = O(mn)$。总工作量保持不变。\n    *   挂钟时间：在一个有 $w$ 个核心的并行环境中，所有工作单元同时计算。时间由负载最重的工作单元决定，其工作量为 $O(m \\cdot \\lceil n/w \\rceil)$。生成后，需要将 $w$ 个包含 $m$ 个样本的数组相加。这个聚合步骤在单台机器上需要 $O(mw)$ 的时间。因此，总挂钟时间为 $T_{\\mathrm{par}} \\approx O(m \\cdot n/w + mw)$。如果生成时间的减少超过了聚合的开销，即 $mn \\gg m \\cdot n/w + mw$，简化为 $n \\gg n/w + w$，那么并行方法比中心化方法更有效。这通常在 $n$ 相对于 $w^2$ 很大时成立。\n\n### C部分：实现与验证\n\n实现将遵循指定的逻辑，从中心化生成器和并行聚合生成器生成样本。然后，它将计算指定的验证指标，以确认并行生成器产生的样本来自一个与目标二项分布在统计上无法区分的分布。\n\n对每个测试用例，算法按以下步骤进行：\n1.  **设置：** 定义参数 $(n, p, w, m)$ 和测试用例索引 $t$。\n2.  **中心化生成：** 使用种子 $S_\\mathrm{cent}(t) = S + 10^6 t + 1$ 初始化一个 `numpy.random.Generator`。从 $\\mathrm{Bin}(n, p)$ 生成一个包含 $m$ 个样本的数组。\n3.  **并行生成：**\n    a. 计算用于负载均衡的划分 $(n_1, \\dots, n_w)$。\n    b. 对于每个工作单元 $j \\in \\{0, \\dots, w-1\\}$，使用种子 $S_\\mathrm{par}(t) + j$ 初始化一个 `numpy.random.Generator`。\n    c. 每个工作单元从 $\\mathrm{Bin}(n_j, p)$ 生成一个包含 $m$ 个样本的数组。\n    d. 将这 $w$ 个数组按元素相加，以产生最终的 $m$ 个并行聚合样本的数组。\n4.  **验证：**\n    a. 使用 `numpy.bincount` 计算两组样本的经验PMF，并计算上确界范数差异 $D$。\n    b. 使用 `numpy.mean` 和 `numpy.var` 计算并行聚合样本的样本均值和方差。\n    c. 计算样本均值和方差相对于理论值 $np$ 和 $np(1-p)$ 的绝对误差。\n    d. 使用计算出的指标及其指定的容忍度界限，评估三个通过/失败条件。总的 `pass` 标志是这些条件的逻辑与。\n5.  **输出：** 将四个结果指标（$D$、均值误差、方差误差、通过标志）格式化为所需的字符串格式，并追加到结果列表中。在所有测试用例结束后，打印最终格式化的字符串。\n\n实现细节在最终答案的代码块中提供。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and validates a parallel binomial variate generator based on the\n    aggregation property of the binomial distribution.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, p, w, m)\n        (60, 0.3, 3, 50000),\n        (1, 0.5, 7, 40000),\n        (1000, 0.01, 8, 50000),\n    ]\n\n    # Global base seed\n    S = 123456789\n\n    results_data = []\n    for t, case in enumerate(test_cases):\n        n, p, w, m = case\n\n        # --- Part 1: Centralized Generator ---\n        seed_cent = S + 10**6 * t + 1\n        rng_cent = np.random.default_rng(seed_cent)\n        samples_cent = rng_cent.binomial(n, p, size=m)\n\n        # --- Part 2: Parallel-Aggregated Generator ---\n        seed_par_base = S + 10**6 * t + 2\n        \n        # Determine partition for load balancing\n        q = n // w\n        r = n % w\n        n_parts = [q + 1] * r + [q] * (w - r)\n\n        worker_samples_list = []\n        for j in range(w):\n            seed_worker = seed_par_base + j\n            rng_worker = np.random.default_rng(seed_worker)\n            n_j = n_parts[j]\n            if n_j > 0:\n                worker_samples = rng_worker.binomial(n_j, p, size=m)\n            else:\n                worker_samples = np.zeros(m, dtype=int)\n            worker_samples_list.append(worker_samples)\n        \n        # Aggregate results from workers\n        samples_par = np.sum(worker_samples_list, axis=0)\n\n        # --- Part 3: Validation ---\n        \n        # 1. PMF sup-norm difference D\n        max_val = n\n        pmf_cent = np.bincount(samples_cent, minlength=max_val + 1) / m\n        pmf_par = np.bincount(samples_par, minlength=max_val + 1) / m\n        D = np.max(np.abs(pmf_par - pmf_cent))\n\n        # Theoretical mean and variance\n        mean_theory = n * p\n        var_theory = n * p * (1 - p)\n\n        # 2. Absolute error in parallel sample mean\n        mean_par = np.mean(samples_par)\n        mean_error = np.abs(mean_par - mean_theory)\n\n        # 3. Absolute error in parallel sample variance\n        var_par = np.var(samples_par)  # ddof=0 is default (population variance)\n        var_error = np.abs(var_par - var_theory)\n\n        # 4. Pass/fail flag\n        # Condition 1\n        cond1 = D = 0.02\n\n        # Condition 2\n        if var_theory > 0:\n            mean_error_tolerance = 4 * np.sqrt(var_theory / m)\n        else:\n            mean_error_tolerance = 0.0\n        cond2 = mean_error = mean_error_tolerance\n\n        # Condition 3\n        var_error_tolerance = 6 * var_theory / np.sqrt(m) if var_theory > 0 else 0.0\n        cond3 = var_error = var_error_tolerance\n        \n        pass_flag = cond1 and cond2 and cond3\n\n        results_data.append([\n            D,\n            mean_error,\n            var_error,\n            pass_flag\n        ])\n    \n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res in results_data:\n        d_val, me_val, ve_val, pf_val = res\n        formatted_results.append(\n            f\"[{d_val:.6f},{me_val:.6f},{ve_val:.6f},{pf_val}]\"\n        )\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}