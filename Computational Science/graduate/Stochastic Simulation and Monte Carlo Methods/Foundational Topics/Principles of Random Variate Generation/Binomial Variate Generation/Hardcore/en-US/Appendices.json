{
    "hands_on_practices": [
        {
            "introduction": "Our exploration begins with a foundational technique for improving algorithmic efficiency. While a straightforward inversion sampler is correct, its performance can be suboptimal. This exercise  challenges you to analytically quantify the computational cost of this basic method and prove the performance gains achieved by exploiting the binomial distribution's inherent symmetry.",
            "id": "3292742",
            "problem": "Consider generating a binomial random variate using the inversion method within stochastic simulation and Monte Carlo methods. Let $K \\sim \\mathrm{Binomial}(n,p)$ denote a binomial random variate with probability mass function (PMF) $f(k) = \\binom{n}{k} p^{k} (1-p)^{n-k}$ for $k \\in \\{0,1,\\dots,n\\}$, and cumulative distribution function (CDF) $F(k) = \\sum_{j=0}^{k} f(j)$. The classical inversion sampler draws $U \\sim \\mathrm{Uniform}(0,1)$ and returns the smallest $k$ such that $F(k) \\ge U$, computing successive partial sums of the PMF from $k=0$ upward until the threshold is crossed. Define the algorithmic cost as the number of PMF evaluations performed per variate.\n\nYou are asked to analyze how reparameterizing to $q=\\min(p,1-p)$ and using the symmetry $K \\overset{d}{=} n - K$ when $p$ is replaced by $1-p$ affects the expected algorithmic cost. Specifically, consider the following two inversion-based samplers:\n\n- Baseline sampler: Invert the distribution with parameter $p$ directly, by summing $f(k)$ from $k=0$.\n- Reparameterized sampler: Define $q = \\min(p,1-p)$, invert the binomial distribution with parameter $q$ by summing $f_{q}(k) = \\binom{n}{k} q^{k} (1-q)^{n-k}$ from $k=0$, and then, if $p1/2$, return $n - K'$ where $K' \\sim \\mathrm{Binomial}(n,q)$; if $p \\le 1/2$, return $K'$ unchanged.\n\nStarting only from the following fundamental bases:\n- The definition of the binomial PMF and CDF.\n- The fact that inversion sampling returns a variate with the target distribution.\n- Linearity of expectation.\n\nDerive the expected number of PMF evaluations for each sampler and define the improvement factor $I(n,p)$ as the ratio of the baseline sampler’s expected cost to the reparameterized sampler’s expected cost. Demonstrate how reparameterizing to $q=\\min(p,1-p)$ improves runtime, and quantify this improvement by providing a single, closed-form analytic expression for $I(n,p)$ in terms of $n$ and $p$. Additionally, discuss the behavior of $I(n,p)$ for $p$ near $0$ and $p$ near $1$.\n\nYour final answer must be the closed-form expression for $I(n,p)$ only. No numerical approximation or rounding is required.",
            "solution": "The problem asks for an analysis of the computational cost of two inversion-based samplers for generating a binomial random variate, $K \\sim \\mathrm{Binomial}(n,p)$. The cost is defined as the number of evaluations of the probability mass function (PMF) required to generate a single variate. We are to derive the expected cost for each sampler and find the ratio of these costs, termed the improvement factor $I(n,p)$.\n\nFirst, we analyze the cost of the general inversion method as described. The sampler generates a uniform random number $U \\sim \\mathrm{Uniform}(0,1)$ and returns the smallest integer $k$ for which the cumulative distribution function (CDF) $F(k)$ satisfies $F(k) \\ge U$. The CDF is computed by summing the PMF values, $F(k) = \\sum_{j=0}^{k} f(j)$. If the sampler returns the value $k_{obs}$, it must have checked the condition for $k=0, 1, \\dots, k_{obs}$. The computation of $F(k_{obs})$ requires evaluating the PMF for $j=0, 1, \\dots, k_{obs}$. Therefore, the number of PMF evaluations performed to generate the variate $k_{obs}$ is $k_{obs}+1$. The cost, $C$, is itself a random variable, functionally dependent on the generated random variate $K$, such that $C = K+1$.\n\nThe expected algorithmic cost is the expectation of this random variable, $E[C] = E[K+1]$. Using the linearity of expectation, this becomes $E[C] = E[K] + 1$.\n\nNow, we apply this general result to the two samplers.\n\n**1. Expected Cost of the Baseline Sampler**\n\nThe baseline sampler generates a variate $K$ directly from the $\\mathrm{Binomial}(n,p)$ distribution. The expected value of a binomial random variate $K \\sim \\mathrm{Binomial}(n,p)$ is a standard result, given by $E[K] = np$.\nLet $E_{base}$ be the expected cost of the baseline sampler. Using our derived cost formula:\n$$E_{base} = E[K] + 1 = np + 1$$\n\n**2. Expected Cost of the Reparameterized Sampler**\n\nThe reparameterized sampler first defines $q = \\min(p, 1-p)$. It then generates an intermediate random variate $K' \\sim \\mathrm{Binomial}(n,q)$ using the same inversion method. The cost of the sampler is defined by the number of PMF evaluations, which occurs only during this generation step. The subsequent transformation of $K'$ (either returning $K'$ directly or returning $n - K'$) does not involve PMF evaluations and, according to the problem's definition of cost, is cost-free.\n\nThe cost of this sampler is therefore the cost of generating $K'$. Let $E_{reparam}$ be this expected cost. The expected value of the intermediate variate $K' \\sim \\mathrm{Binomial}(n,q)$ is $E[K'] = nq$.\nFollowing the same logic as for the baseline sampler, the expected cost is:\n$$E_{reparam} = E[K'] + 1 = nq + 1$$\nSubstituting the definition of $q$:\n$$E_{reparam} = n \\min(p, 1-p) + 1$$\nIt is worth noting that this sampler is valid. If $p \\le 1/2$, then $q=p$, and the sampler returns $K' \\sim \\mathrm{Binomial}(n,p)$, which is correct. If $p  1/2$, then $q=1-p$, and the sampler returns $n-K'$, where $K' \\sim \\mathrm{Binomial}(n,1-p)$. It is a known property of the binomial distribution that if $X \\sim \\mathrm{Binomial}(n, \\theta)$, then $n-X \\sim \\mathrm{Binomial}(n, 1-\\theta)$. Thus, $n-K' \\sim \\mathrm{Binomial}(n, 1-(1-p)) = \\mathrm{Binomial}(n,p)$, so the output is correct in this case as well.\n\n**3. The Improvement Factor $I(n,p)$**\n\nThe improvement factor $I(n,p)$ is defined as the ratio of the expected cost of the baseline sampler to that of the reparameterized sampler.\n$$I(n,p) = \\frac{E_{base}}{E_{reparam}} = \\frac{np + 1}{n \\min(p, 1-p) + 1}$$\nThis expression is the required closed-form analytic expression for the improvement factor.\n\n**4. Discussion of the Improvement Factor**\n\nWe can analyze the behavior of $I(n,p)$ by considering two cases based on the value of $p$.\n\nCase 1: $p \\le 1/2$.\nIn this case, $\\min(p, 1-p) = p$. The improvement factor becomes:\n$$I(n,p) = \\frac{np + 1}{np + 1} = 1$$\nThis indicates that for $p \\le 1/2$, the reparameterized sampler is identical to the baseline sampler, offering no improvement in expected runtime. This is logical, as the reparameterization $q=\\min(p,1-p)$ results in $q=p$.\n\nCase 2: $p  1/2$.\nIn this case, $\\min(p, 1-p) = 1-p$. The improvement factor becomes:\n$$I(n,p) = \\frac{np + 1}{n(1-p) + 1}$$\nSince $p  1/2$, it follows that $p  1-p$. For $n0$, this implies $np  n(1-p)$, and thus $np+1  n(1-p)+1$. Therefore, for $p  1/2$, $I(n,p)  1$, signifying a strict improvement in performance. The reparameterization leverages the symmetry of the binomial distribution to convert a problem with a high mean ($np  n/2$) into one with a low mean ($n(1-p)  n/2$), which is less costly for a simple upwards-searching inversion sampler.\n\nFinally, we examine the behavior of $I(n,p)$ at the boundaries of the parameter $p \\in [0,1]$.\n- As $p \\to 0^+$, we are in Case 1, and $I(n,p) = 1$. There is no improvement, as expected.\n- As $p \\to 1^-$, we are in Case 2. We take the limit of the expression for $I(n,p)$:\n$$\\lim_{p \\to 1^-} I(n,p) = \\lim_{p \\to 1^-} \\frac{np + 1}{n(1-p) + 1} = \\frac{n(1) + 1}{n(1-1) + 1} = \\frac{n+1}{1} = n+1$$\nThe improvement factor approaches $n+1$ as $p$ approaches $1$. This represents a very significant speedup. The baseline sampler's expected cost approaches $n+1$, as it must sum nearly all $n+1$ probability masses. In contrast, the reparameterized sampler's expected cost approaches $n(1-1)+1=1$, because it generates a variate from a distribution with a mean near $0$.",
            "answer": "$$\\boxed{\\frac{np + 1}{n \\min(p, 1-p) + 1}}$$"
        },
        {
            "introduction": "Having optimized for a single computational thread, we now turn to the challenge of scalability. This practice  explores the powerful aggregation property of the binomial distribution, demonstrating how it provides a theoretical foundation for designing a parallel generator. You will implement and validate an algorithm that distributes the computational load across multiple workers, a cornerstone of modern high-performance simulation.",
            "id": "3292711",
            "problem": "You are to reason from first principles about the aggregation property of the binomial distribution and then design and validate a parallel binomial variate generator that exploits this property. The tasks are in three parts.\n\nPart A (theoretical derivation). Starting only from the definition that a binomial random variable with parameters $n$ and $p$ can be represented as a sum of $n$ independent and identically distributed Bernoulli random variables with success probability $p$, and from the definition of independence, derive that if $X_1$ and $X_2$ are independent with $X_1 \\sim \\mathrm{Bin}(n_1,p)$ and $X_2 \\sim \\mathrm{Bin}(n_2,p)$, then $X_1 + X_2 \\sim \\mathrm{Bin}(n_1 + n_2, p)$. Your derivation must be self-contained and rely only on these fundamental bases, such as indicator-sum representations, basic properties of expectation, convolution of independent discrete distributions, or probability generating functions. Do not invoke any specialized pre-derived convolution identities for binomial distributions.\n\nPart B (algorithm design). Using the result of Part A, design a parallel binomial variate generator. Given integers $n \\ge 0$, a probability $p \\in [0,1]$, and an integer number of workers $w \\ge 1$, specify how to partition $n$ into nonnegative integers $(n_1,\\dots,n_w)$ with $\\sum_{j=1}^w n_j = n$, have each worker $j$ independently generate $X^{(j)} \\sim \\mathrm{Bin}(n_j,p)$, and return the aggregate $X = \\sum_{j=1}^w X^{(j)}$. Justify correctness using Part A. Explicitly discuss:\n- A principled choice of partition, for example $n_j \\in \\{\\lfloor n/w \\rfloor,\\lceil n/w \\rceil\\}$, to balance computational load.\n- Independence requirements across workers and how to achieve them with distinct pseudorandom number generator (PRNG) substreams.\n- Computational complexity in terms of $n$, $w$, and the number of requested samples $m$, comparing a centralized generator versus the parallel aggregation.\n\nPart C (implementation and validation). Implement a complete, runnable program that:\n- Uses a reproducible seeding scheme to construct independent PRNG streams. Let the base seed be $S=123456789$. For test case index $t \\in \\{0,1,2,\\dots\\}$, define the seed for the centralized generator as $S_\\mathrm{cent}(t) = S + 10^6 t + 1$. For the parallel generator in test case $t$, define a parallel base seed $S_\\mathrm{par}(t) = S + 10^6 t + 2$ and the worker-$j$ seed as $S_\\mathrm{par}(t) + j$ for $j \\in \\{0,1,\\dots,w-1\\}$. Each PRNG stream must be used exclusively by its designated component.\n- For each test case, generates $m$ independent samples from a centralized binomial generator for $(n,p)$ and $m$ independent samples from your parallel-aggregated generator with $w$ workers and the same $(n,p)$.\n- Validates the parallel generator by computing, for each test case:\n  1. The empirical probability mass function (pmf) sup-norm difference $D = \\max_{k \\in \\{0,\\dots,n\\}} \\left| \\hat{p}_\\mathrm{par}(k) - \\hat{p}_\\mathrm{cent}(k) \\right|$, where $\\hat{p}$ denotes the frequency divided by $m$.\n  2. The absolute error in the parallel sample mean with respect to the theoretical mean $| \\overline{X}_\\mathrm{par} - n p |$.\n  3. The absolute error in the parallel sample variance with respect to the theoretical variance $| s^2_\\mathrm{par} - n p (1-p) |$, where $s^2_\\mathrm{par}$ is the population (denominator $m$) variance of the parallel samples.\n  4. A boolean pass/fail flag equal to $\\mathrm{true}$ if and only if all of the following hold simultaneously:\n     - $D \\le 0.02$,\n     - $| \\overline{X}_\\mathrm{par} - n p | \\le 4 \\sqrt{ \\frac{n p (1-p)}{m} }$,\n     - $| s^2_\\mathrm{par} - n p (1-p) | \\le \\frac{6 n p (1-p)}{\\sqrt{m}}$.\n- Uses the following test suite of parameter values to ensure coverage of different regimes:\n  1. Case $1$: $(n,p,w,m) = (60,\\, 0.3,\\, 3,\\, 50000)$.\n  2. Case $2$: $(n,p,w,m) = (1,\\, 0.5,\\, 7,\\, 40000)$.\n  3. Case $3$: $(n,p,w,m) = (1000,\\, 0.01,\\, 8,\\, 50000)$.\n- Final Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list corresponds to one test case and must itself be a list of the four quantities $[D,\\,\\text{mean\\_error},\\,\\text{var\\_error},\\,\\text{pass}]$, where $D$, $\\text{mean\\_error}$, and $\\text{var\\_error}$ are floats rounded to exactly six digits after the decimal point, and $\\text{pass}$ is a boolean. For example, a conforming output line has the form $[[0.012345,0.000678,0.001234,\\mathrm{True}],[\\dots],[\\dots]]$.\n\nNotes:\n- There are no physical units involved.\n- All probabilities must be expressed as decimals in $[0,1]$.\n- You may use any modern programming language to implement the algorithmic logic, but the final answer must be a complete, runnable program. The runtime must be self-contained and must not require user input, external files, or network access.",
            "solution": "The solution is presented in two parts, corresponding to the theoretical derivation (Part A) and the algorithm design (Part B) requested in the problem.\n\n### Part A: Theoretical Derivation of the Aggregation Property\n\nThe proposition is that if $X_1 \\sim \\mathrm{Bin}(n_1, p)$ and $X_2 \\sim \\mathrm{Bin}(n_2, p)$ are independent random variables, then their sum $Y = X_1 + X_2$ follows a binomial distribution $\\mathrm{Bin}(n_1 + n_2, p)$. This can be derived from first principles as follows:\n\n**1. Indicator-Sum Representation:**\nBy definition, a random variable $X \\sim \\mathrm{Bin}(n, p)$ can be represented as the sum of $n$ independent and identically distributed (i.i.d.) Bernoulli random variables, $\\{B_i\\}_{i=1}^n$, where each $B_i \\sim \\mathrm{Bernoulli}(p)$. Thus, $X = \\sum_{i=1}^{n} B_i$.\n\n**2. Representation of $X_1$ and $X_2$:**\nGiven $X_1 \\sim \\mathrm{Bin}(n_1, p)$, we can represent it as the sum of $n_1$ i.i.d. Bernoulli variables:\n$X_1 = \\sum_{i=1}^{n_1} U_i$, where $U_i \\sim \\mathrm{Bernoulli}(p)$ are i.i.d.\nSimilarly, for $X_2 \\sim \\mathrm{Bin}(n_2, p)$:\n$X_2 = \\sum_{j=1}^{n_2} V_j$, where $V_j \\sim \\mathrm{Bernoulli}(p)$ are i.i.d.\n\n**3. Combining the Representations:**\nThe sum $Y = X_1 + X_2$ can be written as:\n$Y = \\left(\\sum_{i=1}^{n_1} U_i\\right) + \\left(\\sum_{j=1}^{n_2} V_j\\right)$\nSince $X_1$ and $X_2$ are independent, the set of random variables $\\{U_i\\}$ is independent of the set $\\{V_j\\}$. Therefore, the collection of all $n_1 + n_2$ Bernoulli variables, $\\{U_1, \\dots, U_{n_1}, V_1, \\dots, V_{n_2}\\}$, is a set of mutually independent random variables. As all $U_i$ and $V_j$ are drawn from the same $\\mathrm{Bernoulli}(p)$ distribution, this collection forms a set of $n_1 + n_2$ i.i.d. Bernoulli random variables.\n\n**4. Conclusion:**\nThe sum $Y$ is the sum of $n_1 + n_2$ i.i.d. $\\mathrm{Bernoulli}(p)$ variables. By the fundamental definition of a binomial random variable, $Y$ must therefore follow a binomial distribution with parameters $n_1 + n_2$ and $p$.\n$Y = X_1 + X_2 \\sim \\mathrm{Bin}(n_1 + n_2, p)$.\n\n### Part B: Algorithm Design for a Parallel Generator\n\nThe theoretical result from Part A provides the basis for a parallel binomial variate generator.\n\n**1. Algorithm and Correctness:**\nThe algorithm partitions the total number of trials, $n$, among $w$ parallel workers.\n- **Partition:** Divide $n$ into $w$ non-negative integers $n_1, n_2, \\dots, n_w$ such that $\\sum_{j=1}^w n_j = n$.\n- **Parallel Generation:** Each worker $j$ independently generates a random variate $X^{(j)} \\sim \\mathrm{Bin}(n_j, p)$.\n- **Aggregation:** The final result is the sum of the variates from all workers: $X = \\sum_{j=1}^w X^{(j)}$.\nThe correctness of this procedure follows by induction on the number of workers, using the aggregation property proven in Part A.\n\n**2. Principled Partitioning for Load Balancing:**\nTo maximize parallel efficiency, the computational load should be distributed as evenly as possible. Since the time to generate a binomial variate is roughly proportional to the number of trials, the partition sizes $n_j$ should be as close as possible. A standard approach is to use division with remainder. Let $n = q \\cdot w + r$, where $q = \\lfloor n/w \\rfloor$ and $r = n \\pmod w$.\n- Assign $n_j = q+1$ trials to $r$ of the workers.\n- Assign $n_j = q$ trials to the remaining $w-r$ workers.\nThis ensures that trial counts for any two workers differ by at most 1, corresponding to $n_j \\in \\{\\lfloor n/w \\rfloor, \\lceil n/w \\rceil\\}$.\n\n**3. Independence and PRNG Substreams:**\nThe theoretical requirement of independence between workers is achieved by ensuring each worker uses an independent stream of pseudorandom numbers. A robust method, as specified in the problem, is to initialize each worker's PRNG with a unique seed derived from a common base seed (e.g., worker $j$ uses seed `base_seed + j`). For high-quality PRNGs, this method produces statistically independent random streams.\n\n**4. Computational Complexity:**\nAssuming the cost of generating one $\\mathrm{Bin}(k,p)$ sample is $O(k)$, the complexity for generating $m$ samples is as follows:\n- **Centralized Generator:** Total work is $O(mn)$. On a single core, wall-clock time is $O(mn)$.\n- **Parallel Generator:** With $w$ cores, each worker performs $O(m \\cdot n_j)$ work where $n_j \\approx n/w$. The wall-clock time is determined by the most loaded worker, resulting in a generation time of $O(m \\cdot n/w)$. An additional aggregation step of summing the results takes $O(mw)$ time. The total wall-clock time is approximately $O(m \\cdot n/w + mw)$. Parallelism is efficient when the generation cost dominates the aggregation overhead, typically when $n$ is large relative to $w$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and validates a parallel binomial variate generator based on the\n    aggregation property of the binomial distribution.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, p, w, m)\n        (60, 0.3, 3, 50000),\n        (1, 0.5, 7, 40000),\n        (1000, 0.01, 8, 50000),\n    ]\n\n    # Global base seed\n    S = 123456789\n\n    results_data = []\n    for t, case in enumerate(test_cases):\n        n, p, w, m = case\n\n        # --- Part 1: Centralized Generator ---\n        seed_cent = S + 10**6 * t + 1\n        rng_cent = np.random.default_rng(seed_cent)\n        samples_cent = rng_cent.binomial(n, p, size=m)\n\n        # --- Part 2: Parallel-Aggregated Generator ---\n        seed_par_base = S + 10**6 * t + 2\n        \n        # Determine partition for load balancing\n        q = n // w\n        r = n % w\n        n_parts = [q + 1] * r + [q] * (w - r)\n\n        worker_samples_list = []\n        for j in range(w):\n            seed_worker = seed_par_base + j\n            rng_worker = np.random.default_rng(seed_worker)\n            n_j = n_parts[j]\n            worker_samples = rng_worker.binomial(n_j, p, size=m)\n            worker_samples_list.append(worker_samples)\n        \n        # Aggregate results from workers\n        samples_par = np.sum(worker_samples_list, axis=0)\n\n        # --- Part 3: Validation ---\n        \n        # 1. PMF sup-norm difference D\n        max_val = n\n        pmf_cent = np.bincount(samples_cent, minlength=max_val + 1) / m\n        pmf_par = np.bincount(samples_par, minlength=max_val + 1) / m\n        D = np.max(np.abs(pmf_par - pmf_cent))\n\n        # Theoretical mean and variance\n        mean_theory = n * p\n        var_theory = n * p * (1 - p)\n\n        # 2. Absolute error in parallel sample mean\n        mean_par = np.mean(samples_par)\n        mean_error = np.abs(mean_par - mean_theory)\n\n        # 3. Absolute error in parallel sample variance\n        var_par = np.var(samples_par)  # ddof=0 is default (population variance)\n        var_error = np.abs(var_par - var_theory)\n\n        # 4. Pass/fail flag\n        # Condition 1\n        cond1 = D = 0.02\n\n        # Condition 2\n        if var_theory  0:\n            mean_error_tolerance = 4 * np.sqrt(var_theory / m)\n        else:\n            mean_error_tolerance = 0.0\n        cond2 = mean_error = mean_error_tolerance\n\n        # Condition 3\n        var_error_tolerance = 6 * var_theory / np.sqrt(m)\n        cond3 = var_error = var_error_tolerance\n        \n        pass_flag = cond1 and cond2 and cond3\n\n        results_data.append([\n            D,\n            mean_error,\n            var_error,\n            pass_flag\n        ])\n    \n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res in results_data:\n        d_val, me_val, ve_val, pf_val = res\n        formatted_results.append(\n            f\"[{d_val:.6f},{me_val:.6f},{ve_val:.6f},{pf_val}]\"\n        )\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Finally, we apply our understanding of the binomial distribution's structure to a more advanced problem: the efficient estimation of rare event probabilities. Standard Monte Carlo methods are often infeasible for such tasks, but importance sampling offers a powerful solution. This exercise  guides you through the derivation and implementation of an estimator based on exponential tilting, revealing how the moment-generating function can be used to dramatically improve simulation efficiency.",
            "id": "3292775",
            "problem": "Design and implement an importance sampling estimator for the right-tail probability of a binomial distribution using exponential tilting. The target is the probability $\\mathbb{P}\\{X \\ge k\\}$ where $X \\sim \\mathrm{Bin}(n,p)$. You must derive the tilted sampling distribution from first principles and implement a program that estimates this probability efficiently, even when it is small.\n\nStart from the following fundamental base:\n- The binomial distribution $\\mathrm{Bin}(n,p)$ is the distribution of the sum of $n$ independent Bernoulli trials with success probability $p$, where $0 lt; p lt; 1$ and $n$ is a positive integer.\n- The moment generating function $M_{X}(\\theta)$ of a binomial random variable $X \\sim \\mathrm{Bin}(n,p)$ is $M_{X}(\\theta) = \\left( (1-p) + p e^{\\theta} \\right)^{n}$, and the cumulant generating function is $K(\\theta) = \\log M_{X}(\\theta)$.\n- Exponential tilting (also called the Esscher transform) with parameter $\\theta$ produces a new distribution with probability mass function proportional to $e^{\\theta x} \\mathbb{P}\\{X=x\\}$.\n\nYour tasks are:\n1. Using the cumulant generating function definition and exponential tilting, derive the tilted probability mass function $q_{\\theta}(x)$ in terms of the original mass function $p(x)$ for $X \\sim \\mathrm{Bin}(n,p)$, and show algebraically that $q_{\\theta}$ is itself binomial with success parameter $p_{\\theta}$ for some $p_{\\theta} \\in (0,1)$ that you must identify explicitly.\n2. Show that the derivative $K'(\\theta)$ equals the mean under the tilted measure and derive the equation that selects $\\theta$ such that $K'(\\theta) = k$. Solve this equation in closed form to express $\\theta$ (and thus $p_{\\theta}$) in terms of $n$, $p$, and $k$. Justify why this choice makes the event $\\{X \\ge k\\}$ typical under the tilted distribution.\n3. For a given $\\theta$, derive the likelihood ratio (Radon–Nikodym derivative) $w_{\\theta}(x)$ between the original and the tilted distributions and show how to build an unbiased importance sampling estimator for $\\mathbb{P}\\{X \\ge k\\}$ using samples from the tilted distribution.\n4. Implement a program that:\n   - For each test case $(n,p,k)$ below, computes the tilting parameter $\\theta$ and the associated $p_{\\theta}$.\n   - Simulates $M$ independent and identically distributed samples $X_{1},\\dots,X_{M}$ from the tilted binomial distribution $\\mathrm{Bin}(n,p_{\\theta})$.\n   - Computes the importance sampling estimate $\\widehat{\\alpha} = \\frac{1}{M} \\sum_{i=1}^{M} \\mathbf{1}\\{X_{i} \\ge k\\} w_{\\theta}(X_{i})$, where $w_{\\theta}(x)$ is the likelihood ratio you derived.\n   - For verification, also computes the exact probability $\\alpha^{\\star} = \\mathbb{P}\\{X \\ge k\\}$ using a numerically stable method.\n   - Reports, for each test case, a pair of floats: the estimate $\\widehat{\\alpha}$ and the absolute error $|\\widehat{\\alpha} - \\alpha^{\\star}|$.\n5. Use the following test suite with a fixed simulation budget and random seed:\n   - Test A: $(n,p,k) = (\\,100\\,,\\,0.30\\,,\\,45\\,)$.\n   - Test B: $(n,p,k) = (\\,1000\\,,\\,0.20\\,,\\,260\\,)$.\n   - Test C: $(n,p,k) = (\\,200\\,,\\,0.05\\,,\\,25\\,)$.\n   - Test D: $(n,p,k) = (\\,50\\,,\\,0.50\\,,\\,30\\,)$.\n   Use $M = 100000$ tilted samples per test case and set the pseudorandom seed to $123456789$ for reproducibility.\n\nImplementation details and constraints:\n- Your simulation of the tilted binomial variates must use the fact that the tilted distribution is itself binomial with parameter $p_{\\theta}$ derived from $\\theta$ and the original $p$.\n- All computations must be done in floating-point arithmetic. Angles are not involved. No physical units are involved. Any ratio must be expressed as a decimal number.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list of the form $[\\widehat{\\alpha},|\\widehat{\\alpha}-\\alpha^{\\star}|]$. For example, a valid output format is $[[0.123,0.0012],[0.045,0.00003],[\\dots],[\\dots]]$.\n\nYour derivations should avoid heuristics and follow from the stated fundamental base without invoking any unproven shortcuts. Ensure all numerical values needed for the test suite are computed by your program, not hard-coded.",
            "solution": "We seek to estimate $\\alpha = \\mathbb{P}\\{X \\ge k\\}$, where $X \\sim \\mathrm{Bin}(n,p)$. The probability mass function (PMF) is $p(x) = \\mathbb{P}\\{X=x\\} = \\binom{n}{x} p^x (1-p)^{n-x}$. The solution involves three main derivations.\n\n### 1. Derivation of the Tilted Distribution\n\nExponential tilting modifies the original PMF $p(x)$ to a new PMF $q_{\\theta}(x)$ defined as being proportional to $e^{\\theta x} p(x)$. To be a valid PMF, $q_{\\theta}(x)$ must be normalized:\n$$\nq_{\\theta}(x) = \\frac{e^{\\theta x} p(x)}{\\sum_{j=0}^{n} e^{\\theta j} p(j)} = \\frac{e^{\\theta x} p(x)}{M_X(\\theta)}\n$$\nwhere $M_X(\\theta) = \\mathbb{E}[e^{\\theta X}]$ is the moment generating function (MGF). For a binomial distribution, $M_X(\\theta) = ((1-p) + pe^{\\theta})^n$. Substituting the PMF and MGF gives:\n$$\nq_{\\theta}(x) = \\frac{e^{\\theta x} \\binom{n}{x} p^x (1-p)^{n-x}}{((1-p) + pe^{\\theta})^n}\n$$\nRearranging this expression reveals its structure:\n$$\nq_{\\theta}(x) = \\binom{n}{x} \\frac{(pe^{\\theta})^x (1-p)^{n-x}}{((1-p) + pe^{\\theta})^n} = \\binom{n}{x} \\left(\\frac{pe^{\\theta}}{(1-p) + pe^{\\theta}}\\right)^x \\left(\\frac{1-p}{(1-p) + pe^{\\theta}}\\right)^{n-x}\n$$\nThis is the PMF of a binomial distribution $\\mathrm{Bin}(n, p_{\\theta})$ with a new success probability parameter, which we can identify by inspection:\n$$\np_{\\theta} = \\frac{pe^{\\theta}}{1-p+pe^{\\theta}}\n$$\nThe term for failures is consistent, as $1-p_{\\theta} = \\frac{1-p}{1-p+pe^{\\theta}}$. Thus, the tilted distribution is indeed binomial.\n\n### 2. Optimal Choice of Tilting Parameter $\\theta$\n\nFor estimating $\\mathbb{P}\\{X \\ge k\\}$, an effective strategy is to choose $\\theta$ such that the mean of the tilted distribution equals the threshold $k$. The mean of the tilted distribution, $\\mathbb{E}_{\\theta}[X]$, is equal to the derivative of the cumulant generating function (CGF), $K'(\\theta)$, where $K(\\theta) = \\log M_X(\\theta)$.\n\nFor the binomial CGF, $K(\\theta) = n \\log(1-p+pe^{\\theta})$, the derivative is:\n$$\n\\mathbb{E}_{\\theta}[X] = K'(\\theta) = n \\frac{pe^{\\theta}}{1-p+pe^{\\theta}} = n p_{\\theta}\n$$\nSetting the tilted mean to $k$ gives $n p_{\\theta} = k$, which implies the optimal tilted success probability is simply:\n$$\np_{\\theta} = \\frac{k}{n}\n$$\nWe solve for $\\theta$ by substituting this back into its definition:\n$$\n\\frac{k}{n} = \\frac{pe^{\\theta}}{1-p+pe^{\\theta}} \\implies k(1-p+pe^{\\theta}) = npe^{\\theta} \\implies k(1-p) = (np - kp)e^{\\theta}\n$$\nThis yields the optimal tilting parameter:\n$$\n\\theta = \\log\\left(\\frac{k(1-p)}{p(n-k)}\\right)\n$$\nThis choice of $\\theta$ centers the new sampling distribution $\\mathrm{Bin}(n, k/n)$ at the rare event boundary, dramatically improving sampling efficiency.\n\n### 3. The Importance Sampling Estimator\n\nThe importance sampling estimator for $\\alpha = \\mathbb{E}_{p}[\\mathbf{1}\\{X \\ge k\\}]$ uses samples $X_i \\sim q_{\\theta}$ from the tilted distribution:\n$$\n\\widehat{\\alpha} = \\frac{1}{M} \\sum_{i=1}^{M} \\mathbf{1}\\{X_i \\ge k\\} w_{\\theta}(X_i)\n$$\nThe term $w_{\\theta}(x)$ is the likelihood ratio (or weight) needed to correct for sampling from $q_\\theta$ instead of $p$:\n$$\nw_{\\theta}(x) = \\frac{p(x)}{q_{\\theta}(x)} = \\frac{p(x)}{e^{\\theta x}p(x)/M_X(\\theta)} = M_X(\\theta)e^{-\\theta x} = e^{K(\\theta) - \\theta x}\n$$\nThis estimator is unbiased, as $\\mathbb{E}_{\\theta}[\\mathbf{1}\\{X \\ge k\\} w_{\\theta}(X)] = \\sum_{x=k}^{n} q_{\\theta}(x) \\frac{p(x)}{q_{\\theta}(x)} = \\sum_{x=k}^{n} p(x) = \\alpha$. The implementation in the answer block will use these derived formulas.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import binom\n\ndef solve():\n    \"\"\"\n    Designs and implements an importance sampling estimator for the right-tail\n    probability of a binomial distribution using exponential tilting.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (100, 0.30, 45),\n        (1000, 0.20, 260),\n        (200, 0.05, 25),\n        (50, 0.50, 30),\n    ]\n    \n    # Simulation parameters\n    M = 100000\n    seed = 123456789\n    \n    # Initialize the random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n    \n    results = []\n    for n, p, k in test_cases:\n        # Step 1: Derive tilting parameters\n        # The optimal tilted probability p_theta that centers the distribution at k is k/n.\n        p_theta = k / n\n        \n        # Calculate the tilting parameter theta.\n        # This is derived from solving n * p_theta = K'(theta) for theta.\n        # theta = log( (k*(1-p)) / (p*(n-k)) )\n        # This formula is valid for 0  k  n.\n        theta = np.log((k * (1 - p)) / (p * (n - k)))\n        \n        # Calculate the cumulant generating function K(theta). Using the simplified form:\n        # K(theta) = n * log((1-p)/(1-p_theta)) = n * log(n*(1-p) / (n-k))\n        K_theta = n * np.log((n * (1 - p)) / (n - k))\n        \n        # Step 2: Simulate M samples from the tilted distribution Bin(n, p_theta).\n        # We sample from Bin(n, k/n), which has a mean of k.\n        tilted_samples = rng.binomial(n, p_theta, size=M)\n        \n        # Step 3: Compute the importance sampling estimate.\n        # The estimate is the sample mean of I{X=k} * w(X), where X~q_theta.\n        \n        # We only need to sum weights for samples that fall in the event region {X = k}.\n        indicator = tilted_samples = k\n        relevant_samples = tilted_samples[indicator]\n        \n        # The likelihood ratio (weight) is w(x) = exp(K(theta) - theta * x).\n        weights = np.exp(K_theta - theta * relevant_samples)\n        \n        # The final estimate is the sum of weights divided by the total number of samples M.\n        estimate = np.sum(weights) / M\n        \n        # Step 4: Compute the exact probability for verification.\n        # P(X = k) is the survival function at k-1.\n        # scipy.stats.binom.sf(k, n, p) calculates P(X  k).\n        exact_prob = binom.sf(k - 1, n, p)\n        \n        # Step 5: Compute the absolute error.\n        abs_error = np.abs(estimate - exact_prob)\n        \n        results.append([estimate, abs_error])\n\n    # Final print statement in the exact required format '[[est1,err1],[est2,err2],...]'.\n    # Manual string construction is used to avoid spaces.\n    final_output_str = \"[\" + \",\".join([f\"[{est},{err}]\" for est, err in results]) + \"]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}