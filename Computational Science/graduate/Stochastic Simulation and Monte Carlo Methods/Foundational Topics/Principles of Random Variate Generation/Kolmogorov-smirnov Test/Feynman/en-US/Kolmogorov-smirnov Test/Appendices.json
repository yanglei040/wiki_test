{
    "hands_on_practices": [
        {
            "introduction": "Before using or analyzing the Kolmogorov-Smirnov (KS) test, it is essential to understand how to compute its statistic, $D_n$, from a given dataset. This exercise focuses on translating the mathematical definition, $D_n = \\sup_{x} |F_n(x) - F_0(x)|$, into an efficient $O(n \\log n)$ algorithm. By implementing the calculation from first principles, you will gain a concrete understanding of how the empirical and theoretical cumulative distribution functions are compared, moving beyond a black-box function call. ",
            "id": "3315959",
            "problem": "You are given an independent and identically distributed sample $x_1,\\dots,x_n$ from an unknown distribution with a known target cumulative distribution function $F_0$. The empirical cumulative distribution function $F_n$ is defined by $F_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{x_i \\le x\\}$. The one-sample Kolmogorov-Smirnov statistic is the quantity\n$$\nD_n = \\sup_{x \\in \\mathbb{R}} \\left| F_n(x) - F_0(x) \\right|.\n$$\nYour task is to design and implement an $O(n \\log n)$ algorithm that computes $D_n$ for given $x_1,\\dots,x_n$ and $F_0$ by relying only on the following foundations:\n- $F_n$ and $F_0$ are nondecreasing, right-continuous functions with left limits and take values in $[0,1]$.\n- The function $F_n$ is a step function with jumps of size $1/n$ at the observed sample points.\n\nYour algorithm must adhere to the following structural principles:\n- Use sorting to order the sample, which must dominate the asymptotic complexity.\n- Use a single left-to-right scan over the sorted sample to identify where the supremum of the absolute difference is attained.\n- Evaluate $F_0$ at the relevant points encountered during the scan.\n\nYou must implement the algorithm in a single program that applies it to a predefined test suite and produces the outputs in the exact format described below.\n\nTest Suite:\n- Case A (general case): $n = 10$, $x = [0.14, 0.91, 0.22, 0.33, 0.73, 0.59, 0.01, 0.48, 0.64, 0.27]$, $F_0$ is the uniform distribution on $[0,1]$ given by $F_0(x) = 0$ for $x < 0$, $F_0(x) = x$ for $0 \\le x < 1$, and $F_0(x) = 1$ for $x \\ge 1$.\n- Case B (boundary size): $n = 1$, $x = [0.3]$, $F_0$ uniform on $[0,1]$ as in Case A.\n- Case C (ties): $n = 6$, $x = [0.2, 0.2, 0.5, 0.5, 0.5, 0.9]$, $F_0$ uniform on $[0,1]$ as in Case A.\n- Case D (outside support): $n = 6$, $x = [-1.0, -0.2, 0.0, 0.1, 1.1, 1.3]$, $F_0$ uniform on $[0,1]$ as in Case A.\n- Case E (non-uniform target): $n = 6$, $x = [0.05, 0.15, 0.7, 0.3, 0.9, 1.2]$, $F_0$ is the exponential distribution with rate $\\lambda = 2$, i.e., $F_0(x) = 0$ for $x < 0$ and $F_0(x) = 1 - e^{-\\lambda x}$ for $x \\ge 0$.\n- Case F (unbounded support target): $n = 7$, $x = [-1.2, -0.5, 0.0, 0.2, 0.8, 1.5, 2.0]$, $F_0$ is the normal distribution with mean $\\mu = 0$ and standard deviation $\\sigma = 1$, i.e., $F_0(x) = \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)$ where $\\Phi$ is the standard normal cumulative distribution function.\n\nImplementation requirements:\n- The algorithm must use sorting followed by a single scan, and must evaluate the target $F_0$ only at points needed by the scan logic.\n- The program must handle ties in the sample correctly.\n- The program must compute $D_n$ for each case and produce the final result in the exact format described below.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of decimal numbers rounded to six digits after the decimal point and enclosed in square brackets, for example, $[0.123456,0.000000,0.500000]$.\n- There are no physical units involved in this problem. All outputs are dimensionless real numbers.",
            "solution": "The user wants me to act as a meticulous STEM professor and solve a problem related to the Kolmogorov-Smirnov test.\n\nHere's my plan:\n1.  **Role Play**: Adopt the persona of a meticulous and exacting STEM professor. This means I need to be precise, correct, and prioritize logical consistency.\n2.  **Problem Validation**: Critically examine the problem statement before attempting a solution.\n    *   **Step 1: Extract Givens**: List all the provided data and definitions verbatim. This includes the definition of the empirical CDF $F_n(x)$, the Kolmogorov-Smirnov statistic $D_n$, the task of designing an $O(n \\log n)$ algorithm, the structural principles for the algorithm, and the specific test cases.\n    *   **Step 2: Validate**: Check the problem against the criteria:\n        *   **Scientifically Grounded**: The problem is about computing the Kolmogorov-Smirnov (KS) statistic. This is a standard concept in non-parametric statistics. The definitions of $F_n(x)$ and $D_n$ are correct. The properties of CDFs mentioned (nondecreasing, right-continuous, values in $[0,1]$) are standard. The statement that $F_n$ is a step function is also correct. The problem is firmly grounded in mathematical statistics.\n        *   **Well-Posed**: The goal is to compute a specific, well-defined mathematical quantity, $D_n$. The constraints on the algorithm ($O(n \\log n)$, sort then scan) guide the implementation but do not make the problem ill-posed. The existence and uniqueness of $D_n$ for a given sample and $F_0$ are guaranteed by the properties of the supremum of a bounded function over $\\mathbb{R}$.\n        *   **Objective**: The problem is stated using precise mathematical language. There are no subjective or opinion-based statements.\n        *   **Flaw Checklist**:\n            1.  *Scientific/Factual Unsoundness*: No. The definitions and concepts are standard in statistics.\n            2.  *Non-Formalizable/Irrelevant*: No. It's a formal mathematical problem directly related to the specified topic.\n            3.  *Incomplete/Contradictory*: No. All necessary information (sample data, target CDFs) is provided for the test cases. The constraints on the algorithm are consistent.\n            4.  *Unrealistic/Infeasible*: No. The test cases involve standard distributions and small sample sizes. The computation is perfectly feasible.\n            5.  *Ill-Posed/Poorly Structured*: No. The quantity $D_n$ is well-defined.\n            6.  *Pseudo-Profound/Trivial*: No. While the concept might be familiar to statisticians, deriving the efficient algorithm requires careful thought about where the supremum can be achieved. It's a non-trivial algorithmic task.\n            7.  *Outside Scientific Verifiability*: No. The result can be verified by direct calculation or by using standard library functions (though the prompt requires me to implement it from scratch).\n    *   **Step 3: Verdict**: The problem is valid. I will proceed with the solution.\n\n3.  **Solution Phase**: Now I'll design the solution.\n    *   **Principle-Based Design**: I need to explain the theoretical underpinning of the algorithm before presenting the code.\n    *   **The Core Idea**: The problem asks for $D_n = \\sup_{x \\in \\mathbb{R}} |F_n(x) - F_0(x)|$.\n        *   $F_0(x)$ is a continuous (or at least non-decreasing and right-continuous) function.\n        *   $F_n(x)$ is a step function. It is constant between the sorted sample points. Let the sorted sample be $x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(n)}$.\n        *   The function $F_n(x)$ changes its value only at the sample points $x_i$.\n        *   For $x < x_{(1)}$, $F_n(x) = 0$.\n        *   For $x_{(i)} \\le x < x_{(i+1)}$, $F_n(x) = i/n$. (Assuming no ties for now).\n        *   For $x \\ge x_{(n)}$, $F_n(x) = 1$.\n        *   The difference function $g(x) = F_n(x) - F_0(x)$ is piecewise continuous. The supremum of $|g(x)|$ must occur at points where $g(x)$ has a local extremum or at the points of discontinuity.\n        *   The discontinuities of $g(x)$ are at the sample points $x_i$. The derivative of $g(x)$ is $g'(x) = -F_0'(x) = -f_0(x)$ between the sample points (if $F_0$ is differentiable). Since $f_0(x) \\ge 0$, $g(x)$ is non-increasing between the sample points.\n        *   This means the supremum of $g(x)$ in an interval $[x_{(i)}, x_{(i+1)})$ must be at the left endpoint, $x_{(i)}$. The infimum must be at the right endpoint, just before the jump, i.e., at $\\lim_{x \\to x_{(i+1)}^-} x$.\n        *   So, we only need to check the values of $|F_n(x) - F_0(x)|$ at the points $x_i$.\n        *   Let the sorted sample be $x_{(1)}, \\dots, x_{(n)}$.\n        *   The function $F_n(x)$ jumps at each $x_{(i)}$. So we need to evaluate the difference $|F_n(x) - F_0(x)|$ just before and just after the jump.\n        *   Let's consider a point $x_{(i)}$.\n            *   Just before the jump (at $x_{(i)} - \\epsilon$ for small $\\epsilon > 0$), the value of the empirical CDF is $F_n(x_{(i)}^-)$. If there are no ties and all $x_j$ are distinct, then $F_n(x_{(i)}^-) = (i-1)/n$.\n            *   At the jump point and just after (at $x_{(i)}$), the value is $F_n(x_{(i)}) = i/n$.\n        *   So, for each sorted sample point $x_{(i)}$, we need to check two values:\n            1.  $|F_n(x_{(i)}^-) - F_0(x_{(i)})| = |\\frac{i-1}{n} - F_0(x_{(i)})|$\n            2.  $|F_n(x_{(i)}) - F_0(x_{(i)})| = |\\frac{i}{n} - F_0(x_{(i)})|$\n        *   The supremum $D_n$ will be the maximum of these values over all $i=1, \\dots, n$.\n        *   The overall algorithm is:\n            1.  Sort the sample $x$ to get $x_{(1)}, \\dots, x_{(n)}$. This takes $O(n \\log n)$.\n            2.  Initialize a variable `max_d` to $0$.\n            3.  Iterate from $i=1$ to $n$:\n                a.  Get the sorted sample point $x_{(i)}$.\n                b.  Calculate $F_0(x_{(i)})$.\n                c.  Calculate the two differences:\n                    *   $d_1 = |\\frac{i-1}{n} - F_0(x_{(i)})|$\n                    *   $d_2 = |\\frac{i}{n} - F_0(x_{(i)})|$\n                d.  Update `max_d = max(max_d, d_1, d_2)`.\n            4.  Return `max_d`. This scan takes $O(n)$.\n        *   The total complexity is dominated by the sort, so it is $O(n \\log n)$. This matches the requirement.\n\n    *   **Handling Ties**: The problem statement explicitly mentions ties.\n        *   If there are ties, for example, $x_{(i-1)} < x_{(i)} = x_{(i+1)} = \\dots = x_{(j)} < x_{(j+1)}$, then the jump at this value is of size $(j - (i-1))/n$.\n        *   The definition of $F_n(x)$ is $F_n(x) = \\frac{1}{n} \\sum_{k=1}^n \\mathbf{1}\\{x_k \\le x\\}$.\n        *   If the sorted sample is $x_{(1)}, \\dots, x_{(n)}$, then:\n            *   $F_n(x_{(i)}) = \\frac{1}{n} \\times (\\text{number of samples } \\le x_{(i)})$.\n            *   If all values are distinct, then at $x_{(i)}$, there are $i$ samples less than or equal to it, so $F_n(x_{(i)}) = i/n$.\n            *   $F_n(x_{(i)}^-) = \\lim_{\\epsilon \\to 0^+} F_n(x_{(i)}-\\epsilon) = \\frac{1}{n} \\times (\\text{number of samples } < x_{(i)})$.\n            *   If all values are distinct, this is $(i-1)/n$.\n        *   So the general logic should be:\n            1.  Sort the sample $x$ to get $x_{(1)}, \\dots, x_{(n)}$.\n            2.  Initialize `max_d = 0`.\n            3.  Iterate $i$ from $1$ to $n$:\n                a.  Let $x_{val} = x_{(i)}$.\n                b.  $F_n(x_{val}) = i/n$. (This is correct because sorting puts all values $\\le x_{(i)}$ in the first $i$ positions.)\n                c.  $F_n(x_{val}^-) = (i-1)/n$. (This is correct because there are $i-1$ values strictly smaller than or equal to $x_{(i-1)}$, which is the largest value strictly less than $x_{(i)}$ *if there are no ties*).\n                d.  With ties, this logic is faulty. For a value $v$ in the sample data, let's say it appears $k$ times, and there are $m$ values strictly smaller than $v$. Then for any $x$ where $x=v$, we have $F_n(x) = (m+k)/n$, and $F_n(x^-) = m/n$.\n            4.  The correct algorithm with ties:\n                a. Sort the sample: $x_{(1)}, \\dots, x_{(n)}$.\n                b. Initialize `max_d = 0`.\n                c. Loop $i$ from $1$ to $n$:\n                    i. Let $val = x_{(i-1)}$ (using 0-based indexing for the array).\n                    ii. Calculate $F_0(val)$.\n                    iii. The value of $F_n$ *at* this point is $i/n$. So one candidate for the maximum difference is $|i/n - F_0(val)|$.\n                    iv. The value of $F_n$ just *before* this point (from the previous step) is $(i-1)/n$. So another candidate is $|(i-1)/n - F_0(val)|$.\n                    v. `max_d = max(max_d, |i/n - F_0(val)|, |(i-1)/n - F_0(val)|)`.\n            5. This handles ties automatically. Let's trace it.\n               Sample: $[0.2, 0.2, 0.5]$. $n=3$. Sorted: $[0.2, 0.2, 0.5]$.\n               - i=1: $x_{(1)}=0.2$. $F_0(0.2)=0.2$.\n                 - diff1 = $|1/3 - 0.2| = |0.333 - 0.2| = 0.133$.\n                 - diff2 = $|0/3 - 0.2| = 0.2$.\n                 - Current max_d = 0.2.\n               - i=2: $x_{(2)}=0.2$. $F_0(0.2)=0.2$.\n                 - diff1 = $|2/3 - 0.2| = |0.667 - 0.2| = 0.467$.\n                 - diff2 = $|1/3 - 0.2| = |0.333 - 0.2| = 0.133$.\n                 - Current max_d = max(0.2, 0.467) = 0.467.\n               - i=3: $x_{(3)}=0.5$. $F_0(0.5)=0.5$.\n                 - diff1 = $|3/3 - 0.5| = |1.0 - 0.5| = 0.5$.\n                 - diff2 = $|2/3 - 0.5| = |0.667 - 0.5| = 0.167$.\n                 - Current max_d = max(0.467, 0.5) = 0.5.\n               Final result: $D_3 = 0.5$.\n\n               Let's analyze this result more carefully.\n               The ECDF is:\n               - $F_3(x) = 0$ for $x < 0.2$.\n               - $F_3(x) = 2/3$ for $0.2 \\le x < 0.5$.\n               - $F_3(x) = 1$ for $x \\ge 0.5$.\n               The difference function is $|F_3(x) - F_0(x)|$:\n               - For $x < 0.2$: $|0 - x|$. Max at $x \\to 0.2^-$ is $0.2$. (Matches calculation for i=1, diff2).\n               - For $0.2 \\le x < 0.5$: $|2/3 - x|$. Max at $x=0.2$ is $|2/3-0.2| \\approx 0.467$. Min at $x \\to 0.5^-$ is $|2/3-0.5| \\approx 0.167$. (Matches calculations for i=2, diff1 and i=3, diff2).\n               - For $x \\ge 0.5$: $|1 - x|$. Max at $x=0.5$ is $|1-0.5| = 0.5$. (Matches calculation for i=3, diff1).\n               The maximum is indeed $0.5$.\n\n               It seems the simple formula $D_n = \\max_{i=1,\\dots,n} \\max( |i/n - F_0(x_{(i)})|, |(i-1)/n - F_0(x_{(i)})| )$ is correct even with ties, provided $F_0$ is continuous. The problem statement gives properties of $F_n$ and $F_0$ that are \"nondecreasing, right-continuous functions with left limits\". If $F_0$ also has jumps, the situation is more complex. But for all test cases, $F_0$ is continuous. So I will use this formula.\n\n    *   **Algorithm summary**:\n        1.  Input: sample data $x_1, \\dots, x_n$ and a function for computing $F_0$.\n        2.  Sort the sample data: $x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(n)}$.\n        3.  Initialize `max_d = 0.0`.\n        4.  Loop for $i$ from $1$ to $n$:\n            a. Let $val = x_{(i-1)}$ (using 0-based index).\n            b. Compute $f0\\_val = F_0(val)$.\n            c. Compute $d1 = |(i/n) - f0\\_val|$.\n            d. Compute $d2 = |((i-1)/n) - f0\\_val|$.\n            e. Update `max_d = max(max_d, d1, d2)`.\n        5.  The result is `max_d`.\n\n    This algorithm is $O(n \\log n)$ due to sorting and $O(n)$ for the scan. It fits all requirements.\n\n4.  **Implementation**: Now, I'll plan the implementation for each test case.\n    *   **Main function `ks_test_statistic`**: This will implement the algorithm described above. It will take the sample `x` and the target CDF function `f0` as arguments.\n    *   **Target CDF functions**: I need to implement a function for each $F_0$.\n        *   `f0_uniform_0_1(x)`: Implements the CDF of $U(0,1)$.\n        *   `f0_exp_lambda_2(x)`: Implements the CDF of $\\text{Exp}(\\lambda=2)$. $F_0(x) = 1 - e^{-2x}$ for $x \\ge 0$.\n        *   `f0_norm_0_1(x)`: Implements the CDF of $N(0,1)$. I can use `scipy.stats.norm.cdf` or `math.erf`. `scipy` is allowed. `scipy.special.erf` is also available. $\\Phi(z) = \\frac{1}{2}(1 + \\text{erf}(z/\\sqrt{2}))$. This is the way to go since `scipy.stats` is not explicitly listed, but `scipy` is. Let's check `scipy.special.erf`. Yes, it exists.\n\n    *   **Test Cases**:\n        *   **Case A**: `n=10`, `x = [0.14, ...], 'f0': f0_uniform_0_1`.\n        *   **Case B**: `n=1`, `x=[0.3]`, `F0` is `U(0,1)`.\n            - Sorted x: `[0.3]`. n=1.\n            - i=1, x_1=0.3, F0=0.3.\n            - d_plus = |1/1 - 0.3| = 0.7.\n            - d_minus = |0/1 - 0.3| = 0.3.\n            - max_d = 0.7.\n        *   **Case C**: `n=6`, `x=[0.2, 0.2, 0.5, 0.5, 0.5, 0.9]`, `F0` is `U(0,1)`.\n            - Result should be ~0.333333.\n        *   **Case D**: `n=6`, `x=[-1.0, -0.2, 0.0, 0.1, 1.1, 1.3]`, `F0` is `U(0,1)`.\n            - Result should be ~0.566667.\n        *   **Case E**: `n=6`, `x = [0.05, 0.15, 0.7, 0.3, 0.9, 1.2]`, `F0` is `Exp(lambda=2)`.\n        *   **Case F**: `n=7`, `x = [-1.2, -0.5, 0.0, 0.2, 0.8, 1.5, 2.0]`, `F0` is `N(0,1)`.\n\n5.  **Final Code Structure**: The structure I outlined in my thought process is good. I will implement it.\n    The final answer should be rounded to six decimal places, in a comma-separated list within brackets. The f-string formatting `f'{r:.6f}'` will achieve this.\n\nI'm confident in the validation, the derived algorithm, and the implementation plan. I will now generate the final output.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Solves the Kolmogorov-Smirnov test statistic problem for a suite of test cases.\n    \"\"\"\n\n    # --- Target Cumulative Distribution Functions (CDFs) ---\n\n    def f0_uniform_0_1(x):\n        \"\"\"CDF of the uniform distribution on [0,1].\"\"\"\n        if x < 0:\n            return 0.0\n        elif x < 1:\n            return x\n        else:\n            return 1.0\n\n    def f0_exp_lambda_2(x):\n        \"\"\"CDF of the exponential distribution with rate lambda=2.\"\"\"\n        lam = 2.0\n        if x < 0:\n            return 0.0\n        else:\n            return 1.0 - np.exp(-lam * x)\n\n    def f0_norm_0_1(x, mu=0.0, sigma=1.0):\n        \"\"\"CDF of the normal distribution with mean mu and std dev sigma.\"\"\"\n        z = (x - mu) / sigma\n        return 0.5 * (1.0 + erf(z / np.sqrt(2.0)))\n\n    # --- Core Algorithm ---\n\n    def compute_dn(x_sample, f0_func):\n        \"\"\"\n        Computes the one-sample Kolmogorov-Smirnov statistic D_n.\n\n        The algorithm follows from an analysis of the statistic's definition:\n        D_n = sup |F_n(x) - F_0(x)|.\n        Since F_n(x) is a step function and F_0(x) is continuous for the\n        given cases, the supremum of the absolute difference must be achieved\n        at the sample points x_i.\n        At each sorted sample point x_(i), the empirical CDF F_n(x) can be\n        viewed from the left (value before the jump) and from the right\n        (value at and after the jump).\n        - The ECDF value for x < x_(i) corresponds to (i-1)/n.\n        - The ECDF value for x >= x_(i) corresponds to i/n.\n        Thus, for each sorted point x_(i), we check two differences against\n        the theoretical CDF F_0(x_(i)):\n        1. | (i/n) - F_0(x_(i)) |\n        2. | ((i-1)/n) - F_0(x_(i)) |\n        The maximum over all such differences for i=1,...,n is the statistic D_n.\n        This holds even with ties in the sample, as the logic effectively\n        checks the difference at the beginning and end of each step in the ECDF.\n        The complexity is O(n log n) from sorting, plus O(n) for the scan.\n        \"\"\"\n        n = len(x_sample)\n        if n == 0:\n            return 0.0\n        \n        x_sorted = np.sort(x_sample)\n        \n        max_d = 0.0\n        for i in range(1, n + 1):\n            val = x_sorted[i - 1]\n            f0_val = f0_func(val)\n            \n            # Difference between the upper ECDF step and the theoretical CDF\n            d1 = abs(i / n - f0_val)\n            \n            # Difference between the lower ECDF step and the theoretical CDF\n            d2 = abs((i - 1) / n - f0_val)\n            \n            if d1 > max_d:\n                max_d = d1\n            if d2 > max_d:\n                max_d = d2\n                \n        return max_d\n\n    # --- Test Suite Definition ---\n\n    test_cases = [\n        # Case A: General case, Uniform(0,1)\n        (np.array([0.14, 0.91, 0.22, 0.33, 0.73, 0.59, 0.01, 0.48, 0.64, 0.27]), f0_uniform_0_1),\n        \n        # Case B: Boundary size, n=1\n        (np.array([0.3]), f0_uniform_0_1),\n        \n        # Case C: Ties in the sample\n        (np.array([0.2, 0.2, 0.5, 0.5, 0.5, 0.9]), f0_uniform_0_1),\n\n        # Case D: Sample values outside the target distribution's main support\n        (np.array([-1.0, -0.2, 0.0, 0.1, 1.1, 1.3]), f0_uniform_0_1),\n\n        # Case E: Non-uniform target (Exponential)\n        (np.array([0.05, 0.15, 0.7, 0.3, 0.9, 1.2]), f0_exp_lambda_2),\n        \n        # Case F: Unbounded support target (Normal)\n        (np.array([-1.2, -0.5, 0.0, 0.2, 0.8, 1.5, 2.0]), f0_norm_0_1),\n    ]\n\n    # --- Execution and Output ---\n    \n    results = []\n    for x_sample, f0_func in test_cases:\n        dn_value = compute_dn(x_sample, f0_func)\n        results.append(dn_value)\n\n    # Format output as a comma-separated list of numbers rounded to 6 decimal places\n    output_str = \",\".join([f\"{r:.6f}\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Choosing the right statistical test is critical, as different tests possess varying sensitivity to different types of model misspecification. A key application of Monte Carlo simulation is to compare the statistical power of competing tests under controlled scenarios. This practice guides you through a simulation to compare the power of the KS test against the Anderson-Darling and Cramér-von Mises tests, specifically for detecting deviations in the tails of a distribution.  Through this study, you will learn how to quantify a test's performance and appreciate why tail-weighted tests can be more powerful for certain important alternatives.",
            "id": "3315968",
            "problem": "You are tasked with designing and implementing a Monte Carlo simulation program that compares the empirical power of three one-sample goodness-of-fit tests—the Kolmogorov–Smirnov test, the Anderson–Darling test, and the Cramér–von Mises test—for detecting tail mis-specification that arises when a practitioner uses samples from a proposal distribution with a mis-specified variance without applying importance weights. The true target distribution is the standard normal cumulative distribution function, denoted by $F(t)$ for $t \\in \\mathbb{R}$. The practitioner simulates independent and identically distributed samples from a proposal distribution with cumulative distribution function $Q_s(t)$ corresponding to a normal distribution with mean $0$ and variance $s^2$, where $s$ may differ slightly from $1$. Under local alternatives, the misspecification is parameterized by $F_n(t) = F(t) + n^{-1/2} h(t)$ for a suitable function $h(t)$ that reflects tail distortion caused by small variance mis-specification $s(n) = 1 \\pm c n^{-1/2}$; the practitioner incorrectly treats the raw proposal samples as draws from $F$ and applies unweighted tests, thereby introducing tail mis-specification. Your program must compare the empirical power of the three tests at significance level $\\alpha = 0.05$ across a suite of sample sizes and mis-specification magnitudes.\n\nFundamental base for the derivation and algorithmic design shall use the following well-tested facts and definitions:\n- The Glivenko–Cantelli theorem ensures the uniform convergence of the empirical distribution function to $F(t)$ under correct specification, and convergence of the empirical process to a centered Gaussian process follows under additional regularity by functional central limit theorems.\n- Under local alternatives $F_n(t) = F(t) + n^{-1/2} h(t)$, the centered and scaled empirical process exhibits a deterministic drift determined by $h(t)$ that influences test statistics differently depending on their tail sensitivity.\n\nYour simulation will consist of repeated draws of $n$ independent and identically distributed samples $X_1, \\dots, X_n$ from $Q_s$, where $Q_s$ is the cumulative distribution function of a normal distribution with mean $0$ and variance $s^2$. For each replication and each parameter setting, you will compute and record whether each test rejects the null hypothesis $H_0: X_i \\sim F$ at significance level $\\alpha = 0.05$, where $F$ is the cumulative distribution function of the standard normal distribution. You must use the following testing procedures:\n- The Kolmogorov–Smirnov test at level $\\alpha = 0.05$ with two-sided alternative.\n- The Anderson–Darling test specialized to the normal distribution, using the test’s $5\\%$ critical value threshold for rejection (note that the Anderson–Darling implementation provides critical values at fixed levels and not exact $p$-values).\n- The Cramér–von Mises test with the standard normal cumulative distribution function and rejection based on $p$-value less than $\\alpha = 0.05$.\n\nYour program must implement the simulation with $m = 600$ Monte Carlo replications per test case, using a fixed pseudorandom seed for reproducibility. For each test case, the empirical power is defined as the fraction of replications in which the test rejects $H_0$. You must report the three empirical powers for the Kolmogorov–Smirnov test, the Anderson–Darling test, and the Cramér–von Mises test, in that order, for each test case.\n\nThe test suite is given by the following parameter settings, each encoded as a pair $(n, s)$:\n- Case $1$: $(n, s) = (200, 1.0)$, representing the exact null hypothesis.\n- Case $2$: $(n, s) = (200, 1 + 2/\\sqrt{200})$, representing a local heavy-tail alternative with variance inflation.\n- Case $3$: $(n, s) = (500, 1 + 2/\\sqrt{500})$, representing a local heavy-tail alternative with a larger sample size.\n- Case $4$: $(n, s) = (500, 1 - 2/\\sqrt{500})$, representing a local light-tail alternative with a larger sample size.\n- Case $5$: $(n, s) = (200, 1.3)$, representing a non-local heavy-tail alternative with substantial variance inflation.\n- Case $6$: $(n, s) = (50, 1 + 1/\\sqrt{50})$, representing a local heavy-tail alternative with a small sample size.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of three rounded empirical powers $[\\text{KS}, \\text{AD}, \\text{CvM}]$; round each power to $3$ decimal places. For example, the output format must be of the form $[[p_{11},p_{12},p_{13}],[p_{21},p_{22},p_{23}],\\dots,[p_{61},p_{62},p_{63}]]$ with each $p_{ij}$ a decimal number in $[0,1]$.",
            "solution": "The problem statement has been rigorously validated and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to proceed with a unique and verifiable solution. The problem describes a standard, well-defined computational experiment in statistics.\n\nThe objective is to conduct a Monte Carlo simulation to compare the empirical power of three goodness-of-fit tests: the Kolmogorov-Smirnov (KS) test, the Anderson-Darling (AD) test, and the Cramér–von Mises (CvM) test. The simulation is designed to assess the sensitivity of these tests to tail misspecification, which occurs when data generated from a normal distribution with variance $s^2 \\neq 1$ are incorrectly tested against the standard normal distribution, which has variance $1$.\n\nThe core of this problem lies in hypothesis testing. The null hypothesis, $H_0$, posits that the sample data $X_1, \\dots, X_n$ are drawn from the standard normal distribution, whose cumulative distribution function (CDF) is denoted by $F(t)$. The alternative hypothesis, $H_A$, is that the data are drawn from a normal distribution with mean $0$ and variance $s^2$, where $s \\neq 1$. The CDF for the alternative distribution is $Q_s(t) = F(t/s)$.\n\nAll three tests are based on the discrepancy between the empirical distribution function (EDF), $F_n(t) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}_{X_i \\le t}$, and the hypothesized CDF, $F(t)$. The test statistics are functionals of the empirical process, defined as $\\sqrt{n}(F_n(t) - F(t))$.\n\n$1$. The **Kolmogorov-Smirnov test** statistic is the supremum of the absolute difference between the EDF and the target CDF:\n$$D_n = \\sup_{t \\in \\mathbb{R}} |F_n(t) - F(t)|$$\nThis test is sensitive to the single largest vertical deviation between the two functions and is known for being equally sensitive across the entire domain of the distribution.\n\n$2$. The **Cramér–von Mises test** statistic is an integrated squared difference, weighted by the probability density of the target distribution:\n$$W_n^2 = n \\int_{-\\infty}^{\\infty} [F_n(t) - F(t)]^2 dF(t)$$\nThis provides a measure of the average discrepancy over the entire range of values.\n\n$3$. The **Anderson-Darling test** statistic is a variant of the Cramér–von Mises statistic that applies a weight function to emphasize discrepancies in the tails of the distribution:\n$$A_n^2 = n \\int_{-\\infty}^{\\infty} \\frac{[F_n(t) - F(t)]^2}{F(t)(1 - F(t))} dF(t)$$\nThe denominator, $F(t)(1 - F(t))$, becomes small as $t \\to \\pm\\infty$, thus amplifying the importance of deviations in the tails. This property makes the AD test particularly suitable for detecting the kind of variance misspecification described in the problem, which primarily manifests as differences in tail behavior (heavier tails for $s > 1$, lighter tails for $s < 1$).\n\nThe simulation algorithm is designed to estimate the power of each test, defined as the probability of correctly rejecting a false null hypothesis. For the null case ($s=1$), the simulation estimates the Type I error rate, which should be close to the specified significance level $\\alpha = 0.05$.\n\nThe algorithmic procedure is as follows:\n$1$. A master list of test cases, each defined by a pair $(n, s)$, is established as per the problem statement. The parameters specified are the number of Monte Carlo replications $m = 600$ and the significance level $\\alpha = 0.05$.\n$2$. For reproducibility, the pseudorandom number generator is initialized with a fixed seed, which will be set to $12345$.\n$3$. An outer loop iterates through each test case $(n, s)$. For each case, rejection counters for the three tests are initialized to $0$.\n$4$. An inner loop executes $m = 600$ times. In each replication:\n    a. A sample of size $n$ is generated from the proposal distribution, a normal distribution with mean $0$ and standard deviation $s$. This is achieved by generating $n$ standard normal variates and multiplying each by $s$.\n    b. Each of the three goodness-of-fit tests is applied to this sample, with the null hypothesis being that the data originate from the standard normal distribution $N(0, 1)$.\n    c. For the KS and CvM tests, the null hypothesis $H_0$ is rejected if the computed $p$-value is less than $\\alpha = 0.05$.\n    d. For the AD test, $H_0$ is rejected if the computed test statistic is greater than the critical value corresponding to the $5\\%$ significance level.\n    e. If a test rejects $H_0$, its corresponding rejection counter is incremented.\n$5$. After all $m$ replications for a given test case are complete, the empirical power for each test is calculated as the ratio of rejections to the total number of replications: $\\text{Power} = \\frac{\\text{Number of Rejections}}{m}$.\n$6$. The calculated powers are rounded to $3$ decimal places, and the resulting triplet $[\\text{Power}_{\\text{KS}}, \\text{Power}_{\\text{AD}}, \\text{Power}_{\\text{CvM}}]$ is stored.\n$7$. After iterating through all test cases, the final list of results is formatted into the specified string format. This systematic approach ensures a robust and reproducible comparison of the test powers under the specified conditions of tail misspecification.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Performs a Monte Carlo simulation to compare the empirical power of the\n    Kolmogorov-Smirnov, Anderson-Darling, and Cramér-von Mises tests for\n    detecting variance mis-specification in normally distributed data.\n    \"\"\"\n    # Simulation parameters\n    m_replications = 600\n    alpha = 0.05\n    seed = 12345\n    rng = np.random.default_rng(seed)\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (n, s) where n is sample size and s is the std. dev.\n    test_cases = [\n        (200, 1.0),\n        (200, 1.0 + 2.0 / np.sqrt(200)),\n        (500, 1.0 + 2.0 / np.sqrt(500)),\n        (500, 1.0 - 2.0 / np.sqrt(500)),\n        (200, 1.3),\n        (50, 1.0 + 1.0 / np.sqrt(50)),\n    ]\n\n    all_results = []\n\n    for n, s in test_cases:\n        # Counters for rejections for each test\n        ks_rejects = 0\n        ad_rejects = 0\n        cvm_rejects = 0\n\n        # The critical value for the Anderson-Darling test at 5% significance level.\n        # The `anderson` function returns critical values for levels [15, 10, 5, 2.5, 1] %.\n        # So, the 5% critical value is at index 2.\n        # We perform one test run outside the loop to get this value for 'norm' distribution.\n        # This is a slight optimization but mostly for clarity.\n        ad_test_info = stats.anderson(rng.standard_normal(size=10), dist='norm')\n        ad_critical_value_5_percent = ad_test_info.critical_values[2]\n\n        for _ in range(m_replications):\n            # Generate a sample from N(0, s^2)\n            # The scale parameter in numpy.random.normal is the standard deviation (s), not variance.\n            sample = rng.normal(loc=0.0, scale=s, size=n)\n\n            # 1. Kolmogorov-Smirnov test\n            # H0: sample is from standard normal ('norm')\n            ks_stat, ks_pvalue = stats.kstest(sample, 'norm', alternative='two-sided')\n            if ks_pvalue < alpha:\n                ks_rejects += 1\n\n            # 2. Anderson-Darling test\n            # H0: sample is from standard normal ('norm')\n            ad_statistic, _, _ = stats.anderson(sample, dist='norm')\n            if ad_statistic > ad_critical_value_5_percent:\n                ad_rejects += 1\n            \n            # 3. Cramér-von Mises test\n            # H0: sample is from standard normal ('norm')\n            cvm_result = stats.cramervonmises(sample, 'norm')\n            if cvm_result.pvalue < alpha:\n                cvm_rejects += 1\n\n        # Calculate empirical powers\n        power_ks = ks_rejects / m_replications\n        power_ad = ad_rejects / m_replications\n        power_cvm = cvm_rejects / m_replications\n\n        # Round to 3 decimal places\n        case_results = [\n            np.round(power_ks, 3),\n            np.round(power_ad, 3),\n            np.round(power_cvm, 3)\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string as specified in the problem.\n    # str() on a list of lists creates the correct format with brackets and commas.\n    # .replace(\" \", \"\") removes cosmetic whitespace.\n    output_string = str(all_results).replace(\" \", \"\")\n    \n    # Final print statement in the exact required format.\n    print(output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "In many real-world applications, the null hypothesis is composite—for instance, asserting that data is normally distributed without specifying the mean or variance. When parameters are estimated from the data, the standard critical values for the KS test become invalid. This advanced exercise introduces the parametric bootstrap, a powerful and general Monte Carlo technique for approximating the correct null distribution of the test statistic in such cases. Mastering this procedure enables the correct application of goodness-of-fit tests in a wide range of practical scenarios, a vital skill for robust statistical analysis. ",
            "id": "3316006",
            "problem": "You are given an independent and identically distributed sample $\\{X_1,\\dots,X_n\\}$ from an unknown distribution. Consider a parametric composite null hypothesis $H_0: X_i \\sim F(x;\\theta_0)$ for some parameter $\\theta_0$ in a parameter space $\\Theta$, where $F(x;\\theta)$ is a fully specified cumulative distribution function. Let $\\hat{\\theta}_n$ denote the Maximum Likelihood Estimator (MLE) computed from the observed sample. Define the empirical distribution function $F_n(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le x\\}$ and the one-sample Kolmogorov-Smirnov statistic under the composite null,\n$$\nD_n = \\sup_{x \\in \\mathbb{R}} \\left| F_n(x) - F(x;\\hat{\\theta}_n) \\right|.\n$$\nYour task is to design and implement a parametric bootstrap algorithm to approximate the finite-sample distribution of $D_n$ under $H_0$ when $\\theta$ is estimated, and to compute a bootstrap $p$-value. Specifically:\n- Propose a parametric bootstrap procedure that, for a chosen number of bootstrap replicates $B$, repeatedly resamples from $F(x;\\hat{\\theta}_n)$, refits the parameter on each bootstrap resample via MLE, recomputes the Kolmogorov-Smirnov statistic $D_n^{*}$ on the bootstrap resample, and outputs the Monte Carlo approximation of the $p$-value, defined as the fraction of bootstrap statistics at least as large as the observed $D_n$.\n- Provide a principled justification of the algorithm’s asymptotic validity, starting from fundamental definitions of the empirical process and standard regularity conditions for MLE, and without invoking shortcut formulas for the limiting distribution.\n\nImplementation details:\n- For computing $D_n$, use the definition above and the fact that, for continuous $F(\\cdot;\\theta)$, the supremum is attained at sample points. You must derive an implementable formula for $D_n$ as part of your solution.\n- Implement MLE for the following parametric families:\n  1. Normal family $F(x;\\theta)$ with $\\theta=(\\mu,\\sigma)$, where $\\mu \\in \\mathbb{R}$ and $\\sigma>0$.\n  2. Exponential family $F(x;\\theta)$ with $\\theta=(\\lambda)$, $\\lambda>0$, supported on $[0,\\infty)$.\n\nTesting protocol:\n- Your program must construct and analyze the following test suite. Each test case specifies a data-generating process (for the observed sample), the null model family used for fitting and testing, sample size $n$, number of bootstrap replicates $B$, and a random seed. In all cases, the output must be a float $p$-value in $[0,1]$.\n  1. Normal-null, correctly specified: observed data generated from a normal distribution with parameters $\\mu=0$, $\\sigma=1$; null model family is normal; $n=120$; $B=500$; seed $20231102$.\n  2. Normal-null, misspecified (heavy tails): observed data generated from a Student $t$ distribution with degrees of freedom $\\nu=3$ and scaled to unit variance; null model family is normal; $n=120$; $B=500$; seed $20231103$.\n  3. Exponential-null, correctly specified: observed data generated from an exponential distribution with rate $\\lambda=1.5$; null model family is exponential; $n=100$; $B=400$; seed $20231104$.\n  4. Normal-null, boundary-sized sample: observed data generated from a normal distribution with parameters $\\mu=0.5$, $\\sigma=0.2$; null model family is normal; $n=30$; $B=400$; seed $20231105$.\n\nOutput specification:\n- Your program must produce a single line of output containing the bootstrap $p$-values for the four test cases as a comma-separated list enclosed in square brackets, for example, $[p_1,p_2,p_3,p_4]$.\n- Each $p$-value must be printed as a float rounded to six decimal places.\n- No other text must be printed.\n\nNotes:\n- There are no physical units involved.\n- Angles are not involved.\n- Percentages must be expressed as decimals in $[0,1]$.\n\nYour algorithm must be entirely self-contained and must use only the allowed programming libraries. Your justification must begin from the empirical process and standard asymptotic properties of the MLE and proceed to conclude the asymptotic validity of the parametric bootstrap for the Kolmogorov-Smirnov statistic under the composite null.",
            "solution": "The problem is valid. It presents a well-defined task in computational statistics, specifically the application of the parametric bootstrap to approximate the null distribution of the Kolmogorov-Smirnov (KS) statistic when distribution parameters are estimated from the data. The underlying statistical theory is sound, and the implementation details and test cases are complete and consistent.\n\nThe core of the problem is that the standard Kolmogorov-Smirnov test assumes the null distribution is fully specified. When parameters of the null distribution are estimated from the same data being tested, the KS statistic $D_n = \\sup_{x \\in \\mathbb{R}} \\left| F_n(x) - F(x;\\hat{\\theta}_n) \\right|$ is no longer distribution-free. Its asymptotic distribution depends on the true (unknown) parameters of the null hypothesis, rendering standard tables inapplicable. This is famously known as the Lilliefors test for the normal family case. The parametric bootstrap provides a general and powerful method to approximate the correct null distribution.\n\n### Justification of the Parametric Bootstrap Algorithm\n\n**1. The Asymptotic Distribution of the KS Statistic with Estimated Parameters**\n\nLet the null hypothesis be $H_0: X_i \\sim F(x; \\theta_0)$ for some unknown $\\theta_0 \\in \\Theta$. The test statistic is $D_n = \\sup_x |F_n(x) - F(x;\\hat{\\theta}_n)|$, where $F_n(x)$ is the empirical distribution function (EDF) and $\\hat{\\theta}_n$ is the Maximum Likelihood Estimator (MLE) of $\\theta$. We analyze the centered and scaled statistic $\\sqrt{n} D_n$.\n\nWe can decompose the term inside the supremum as:\n$$\n\\sqrt{n}(F_n(x) - F(x;\\hat{\\theta}_n)) = \\sqrt{n}(F_n(x) - F(x;\\theta_0)) - \\sqrt{n}(F(x;\\hat{\\theta}_n) - F(x;\\theta_0))\n$$\nThe first term, $\\alpha_n(x) = \\sqrt{n}(F_n(x) - F(x;\\theta_0))$, is the empirical process. By Donsker's theorem, under $H_0$, $\\alpha_n(\\cdot)$ converges in distribution to a Gaussian process $G(\\cdot)$, which is a Brownian bridge $B(F(x; \\theta_0))$ tied to $0$ at both ends.\n\nFor the second term, under standard regularity conditions for MLE, we can perform a Taylor expansion of $F(x;\\hat{\\theta}_n)$ around the true parameter $\\theta_0$. For $\\theta \\in \\mathbb{R}^p$:\n$$\nF(x;\\hat{\\theta}_n) \\approx F(x;\\theta_0) + (\\hat{\\theta}_n - \\theta_0)^T \\nabla_{\\theta} F(x;\\theta_{0})\n$$\nwhere $\\nabla_{\\theta}$ is the gradient with respect to $\\theta$. This gives:\n$$\n\\sqrt{n}(F(x;\\hat{\\theta}_n) - F(x;\\theta_0)) \\approx (\\sqrt{n}(\\hat{\\theta}_n - \\theta_0))^T \\nabla_{\\theta} F(x;\\theta_0)\n$$\nAgain, under regularity conditions, the MLE is asymptotically efficient and normal:\n$$\n\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\xrightarrow{d} \\mathcal{N}(0, I(\\theta_0)^{-1})\n$$\nwhere $I(\\theta_0)$ is the Fisher information matrix at the true parameter $\\theta_0$.\n\nCombining these results, the process $\\sqrt{n}(F_n(x) - F(x;\\hat{\\theta}_n))$ converges in distribution to a limiting Gaussian process:\n$$\n\\mathbb{G}_n(x) = \\alpha_n(x) - (\\sqrt{n}(\\hat{\\theta}_n - \\theta_0))^T \\nabla_{\\theta} F(x;\\theta_0) + o_p(1) \\xrightarrow{d} \\mathbb{G}(x; \\theta_0)\n$$\nThe limiting process $\\mathbb{G}(x; \\theta_0)$ is a Gaussian process whose covariance structure depends on $\\theta_0$ through both the gradient term $\\nabla_{\\theta} F(x;\\theta_0)$ and the Fisher information $I(\\theta_0)$ that governs the variance of the MLE. The test statistic $\\sqrt{n}D_n$ converges in distribution to $\\sup_x|\\mathbb{G}(x;\\theta_0)|$. Since this limiting distribution depends on the unknown $\\theta_0$, the statistic is not pivotal, and we cannot pre-tabulate its critical values.\n\n**2. The Bootstrap Solution**\n\nThe parametric bootstrap circumvents this problem by approximating the unknown distribution of $D_n$ under $H_0$. The procedure rests on the \"bootstrap principle\": we treat the estimated model $F(x;\\hat{\\theta}_n)$ as if it were the true data-generating distribution.\n\nThe algorithm is as follows:\n1.  Compute the MLE $\\hat{\\theta}_n$ from the observed sample $\\{X_1, \\dots, X_n\\}$.\n2.  Calculate the observed test statistic $D_n^{\\text{obs}} = \\sup_{x} |F_n(x) - F(x;\\hat{\\theta}_n)|$.\n3.  For a large number of replicates $B$:\n    a. Generate a bootstrap sample $\\{X_1^*, \\dots, X_n^*\\}$ of size $n$ by drawing i.i.d. random variates from the distribution $F(x;\\hat{\\theta}_n)$.\n    b. Compute the MLE $\\hat{\\theta}_n^*$ from the bootstrap sample $\\{X_i^*\\}$.\n    c. Compute the bootstrap statistic $D_n^* = \\sup_{x} |F_n^*(x) - F(x;\\hat{\\theta}_n^*)|$, where $F_n^*(x)$ is the EDF of the bootstrap sample.\n4.  The collection of bootstrap statistics $\\{D_{n,1}^*, \\dots, D_{n,B}^*\\}$ forms an empirical distribution that approximates the true null distribution of $D_n$.\n5.  The $p$-value is estimated as the fraction of bootstrap statistics that are greater than or equal to the observed statistic:\n    $$\n    p\\text{-value} = \\frac{1}{B} \\sum_{b=1}^B \\mathbf{1}\\{D_{n,b}^* \\ge D_n^{\\text{obs}}\\}\n    $$\n\n**3. Asymptotic Validity**\n\nThe validity of this procedure relies on the fact that, conditional on the original data, the bootstrap statistic $D_n^*$ has the same limiting distribution as the original statistic $D_n$, with $\\theta_0$ replaced by $\\hat{\\theta}_n$. That is, $\\sqrt{n}D_n^*$ converges in distribution to $\\sup_x|\\mathbb{G}(x; \\hat{\\theta}_n)|$.\n\nBecause the MLE $\\hat{\\theta}_n$ is a consistent estimator of $\\theta_0$ (i.e., $\\hat{\\theta}_n \\xrightarrow{p} \\theta_0$ as $n \\to \\infty$), and assuming the covariance structure of the Gaussian process $\\mathbb{G}(x;\\theta)$ is continuous in $\\theta$, the bootstrap limiting distribution $\\sup_x|\\mathbb{G}(x; \\hat{\\theta}_n)|$ converges in probability to the true limiting distribution $\\sup_x|\\mathbb{G}(x; \\theta_0)|$. This implies that the distribution of $D_n^*$ (conditional on the data) consistently estimates the true null distribution of $D_n$. Therefore, quantiles from the bootstrap distribution are consistent estimates of the true quantiles, and the resulting $p$-value is asymptotically valid.\n\n### Algorithmic Implementation\n\n**MLE Formulas:**\n- For a Normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, the MLEs are the sample mean $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n X_i$ and the sample standard deviation (with divisor $n$) $\\hat{\\sigma} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(X_i-\\hat{\\mu})^2}$.\n- For an Exponential distribution with rate $\\lambda$, the PDF is $f(x;\\lambda) = \\lambda e^{-\\lambda x}$. The MLE for $\\lambda$ is the reciprocal of the sample mean: $\\hat{\\lambda} = 1/\\bar{X} = n/\\sum_{i=1}^n X_i$.\n\n**KS Statistic Formula:**\nFor a continuous CDF $F$, the supremum of $|F_n(x) - F(x;\\hat{\\theta}_n)|$ is achieved at one of the sample points $X_{(i)}$. Let $X_{(1)} \\le \\dots \\le X_{(n)}$ be the sorted sample. The statistic can be computed efficiently as:\n$$\nD_n = \\max_{i=1,\\dots,n} \\max\\left( \\frac{i}{n} - F(X_{(i)};\\hat{\\theta}_n), F(X_{(i)};\\hat{\\theta}_n) - \\frac{i-1}{n} \\right)\n$$\nThis formula accounts for the maximum discrepancies just after and just before the jumps in the EDF at each sample point.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, expon, t\n\ndef mle_normal(data: np.ndarray) -> tuple[float, float]:\n    \"\"\"Computes the Maximum Likelihood Estimators for a Normal distribution.\"\"\"\n    mu_hat = np.mean(data)\n    sigma_hat = np.std(data)  # MLE uses n in denominator, ddof=0 is default\n    return mu_hat, sigma_hat\n    \ndef mle_exponential(data: np.ndarray) -> tuple[float]:\n    \"\"\"Computes the Maximum Likelihood Estimator for an Exponential distribution.\"\"\"\n    # Note: lambda is the rate parameter.\n    if np.mean(data) == 0:\n        # Handle case with all zeros, though unlikely for continuous data.\n        # Return a large lambda to signify a distribution concentrated at 0.\n        return (np.inf,)\n    lambda_hat = 1.0 / np.mean(data)\n    return (lambda_hat,)\n\ndef calculate_ks_statistic(data: np.ndarray, model_name: str) -> tuple[float, tuple]:\n    \"\"\"\n    Calculates the Kolmogorov-Smirnov statistic for a given sample and a parametric family.\n    \n    Returns the statistic D_n and the estimated parameters theta_hat.\n    \"\"\"\n    n = len(data)\n    if n == 0:\n        return 0.0, ()\n        \n    data_sorted = np.sort(data)\n    \n    if model_name == 'normal':\n        params = mle_normal(data)\n        mu_hat, sigma_hat = params\n        if sigma_hat == 0: # All data points are the same\n            return 1.0, params\n        cdf_values = norm.cdf(data_sorted, loc=mu_hat, scale=sigma_hat)\n    elif model_name == 'exponential':\n        params = mle_exponential(data)\n        lambda_hat, = params\n        if lambda_hat == np.inf: # All data points are zero\n             return 1.0, params\n        # scipy.stats.expon uses scale parameter beta = 1/lambda\n        cdf_values = expon.cdf(data_sorted, scale=1.0/lambda_hat)\n    else:\n        raise ValueError(f\"Unknown model name: {model_name}\")\n\n    i_vals = np.arange(1, n + 1)\n    \n    d_plus = np.max((i_vals / n) - cdf_values)\n    d_minus = np.max(cdf_values - ((i_vals - 1) / n))\n    \n    d_n = np.max([d_plus, d_minus])\n    \n    return d_n, params\n\ndef run_bootstrap_test(observed_data: np.ndarray, null_model: str, B: int, rng: np.random.Generator) -> float:\n    \"\"\"\n    Performs a parametric bootstrap for the KS test with estimated parameters.\n    \"\"\"\n    # 1. Calculate observed statistic and parameters\n    d_observed, theta_hat_observed = calculate_ks_statistic(observed_data, null_model)\n    n = len(observed_data)\n    \n    bootstrap_stats = np.zeros(B)\n    for i in range(B):\n        # 2a. Generate a bootstrap sample from F(x; theta_hat_observed)\n        if null_model == 'normal':\n            mu_hat, sigma_hat = theta_hat_observed\n            if sigma_hat <= 0: # Cannot generate from degenerate distribution\n                bootstrap_sample = np.full(n, mu_hat)\n            else:\n                bootstrap_sample = rng.normal(loc=mu_hat, scale=sigma_hat, size=n)\n        elif null_model == 'exponential':\n            lambda_hat, = theta_hat_observed\n            if lambda_hat <= 0 or not np.isfinite(lambda_hat): # Cannot generate\n                bootstrap_sample = np.zeros(n)\n            else:\n                # numpy.random.exponential uses scale beta = 1/lambda\n                bootstrap_sample = rng.exponential(scale=1.0/lambda_hat, size=n)\n\n        # 2b. Calculate KS statistic for the bootstrap sample\n        d_bootstrap, _ = calculate_ks_statistic(bootstrap_sample, null_model)\n        bootstrap_stats[i] = d_bootstrap\n        \n    # 3. Calculate p-value\n    # Defined as the fraction of bootstrap statistics at least as large as the observed one.\n    count_exceeding = np.sum(bootstrap_stats >= d_observed)\n    p_value = count_exceeding / B\n    \n    return p_value\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            'description': 'Normal-null, correctly specified',\n            'data_gen': {'name': 'normal', 'params': {'loc': 0.0, 'scale': 1.0}},\n            'null_model': 'normal',\n            'n': 120, 'B': 500, 'seed': 20231102\n        },\n        {\n            'description': 'Normal-null, misspecified (heavy tails)',\n            'data_gen': {'name': 't_scaled', 'params': {'df': 3}},\n            'null_model': 'normal',\n            'n': 120, 'B': 500, 'seed': 20231103\n        },\n        {\n            'description': 'Exponential-null, correctly specified',\n            'data_gen': {'name': 'exponential', 'params': {'scale': 1.0 / 1.5}},\n            'null_model': 'exponential',\n            'n': 100, 'B': 400, 'seed': 20231104\n        },\n        {\n            'description': 'Normal-null, boundary-sized sample',\n            'data_gen': {'name': 'normal', 'params': {'loc': 0.5, 'scale': 0.2}},\n            'null_model': 'normal',\n            'n': 30, 'B': 400, 'seed': 20231105\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        n = case['n']\n        \n        # Generate the \"observed\" data for the test case\n        gen_info = case['data_gen']\n        if gen_info['name'] == 'normal':\n            observed_data = rng.normal(size=n, **gen_info['params'])\n        elif gen_info['name'] == 'exponential':\n            observed_data = rng.exponential(size=n, **gen_info['params'])\n        elif gen_info['name'] == 't_scaled':\n            df = gen_info['params']['df']\n            # Var(t(df)) = df / (df - 2). We scale to get unit variance.\n            # Scale factor = 1 / sqrt(Var(t(df))) = sqrt((df-2)/df)\n            scale_factor = np.sqrt((df - 2) / df)\n            observed_data = scale_factor * t.rvs(df=df, size=n, random_state=rng)\n        \n        p_value = run_bootstrap_test(observed_data, case['null_model'], case['B'], rng)\n        results.append(p_value)\n\n    # Format output as specified\n    formatted_results = [f\"{p:.6f}\" for p in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}