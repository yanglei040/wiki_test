{
    "hands_on_practices": [
        {
            "introduction": "Mastering any computational technique begins with a solid grasp of its fundamental mechanism. This first exercise provides a concrete, step-by-step application of the inverse transform method for a simple discrete distribution. By manually constructing the cumulative probability thresholds and mapping a given uniform variate to an outcome, you will solidify your understanding of how the unit interval is partitioned to generate samples with the correct probabilities .",
            "id": "3314769",
            "problem": "Consider a discrete random variable $X$ supported on the ordered set $\\{x_{1}, x_{2}, x_{3}, x_{4}\\}$ with probability vector $p=(0.05, 0.25, 0.4, 0.3)$, where $p_{i}=\\mathbb{P}(X=x_{i})$ for $i\\in\\{1,2,3,4\\}$. A single draw $U$ is observed from the continuous uniform distribution on $(0,1)$, with $U=0.7$. Using only the definition of the cumulative distribution function (CDF) for discrete variables and the first-principles construction that maps a uniform $(0,1)$ variate to an outcome via the inverse cumulative principle, do the following: construct the cumulative array $c_{k}=\\sum_{i=1}^{k}p_{i}$ for $k\\in\\{1,2,3,4\\}$, determine the sampled index $k$ selected by inverse transform sampling from this $U$, and identify the corresponding outcome $x_{k}$. Explicitly articulate how boundary cases where $U$ coincides with a jump of the CDF are handled based on the definitions you use. Report only the sampled index $k$ as your final answer. No rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of stochastic simulation, is well-posed with a complete and consistent setup, and is expressed in objective, formal language. The provided probability vector $p=(0.05, 0.25, 0.4, 0.3)$ is valid, as its elements are non-negative and sum to $1$: $0.05 + 0.25 + 0.4 + 0.3 = 1$. The problem asks for a direct application of the inverse transform sampling method for a discrete random variable, a fundamental technique in Monte Carlo methods.\n\nLet $X$ be the discrete random variable with support $\\{x_{1}, x_{2}, x_{3}, x_{4}\\}$ and probability mass function (PMF) given by $\\mathbb{P}(X=x_i) = p_i$. The given probability vector is $p = (p_1, p_2, p_3, p_4) = (0.05, 0.25, 0.4, 0.3)$.\n\nThe inverse transform sampling method relies on the cumulative distribution function (CDF) of $X$. For a discrete variable, the CDF, $F_X(x) = \\mathbb{P}(X \\le x)$, is a step function. The values of the CDF at the support points $x_k$ are given by the cumulative probabilities $c_k$:\n$$c_k = F_X(x_k) = \\sum_{i=1}^{k} p_i$$\nBy convention, we define $c_0 = 0$.\n\nFirst, we construct the cumulative array $c = (c_1, c_2, c_3, c_4)$ using the given probabilities:\n- $c_1 = p_1 = 0.05$\n- $c_2 = p_1 + p_2 = 0.05 + 0.25 = 0.30$\n- $c_3 = c_2 + p_3 = 0.30 + 0.40 = 0.70$\n- $c_4 = c_3 + p_4 = 0.70 + 0.30 = 1.00$\nThe cumulative array is thus $(0.05, 0.30, 0.70, 1.00)$.\n\nThe principle of inverse transform sampling is to generate a random variate $U$ from the standard uniform distribution $\\mathcal{U}(0,1)$ and then find the outcome $x_k$ corresponding to the unique index $k$ that satisfies the condition:\n$$c_{k-1}  U \\le c_k$$\nThis condition partitions the interval $(0,1)$ into disjoint sub-intervals $(c_{k-1}, c_k]$, each of length $c_k - c_{k-1} = p_k$. The probability that $U$ falls into a specific interval $(c_{k-1}, c_k]$ is exactly $p_k$, ensuring that the sampled outcome $x_k$ is chosen with the correct probability. An equivalent and often more practical way to state the rule is to find the smallest index $k$ such that $U \\le c_k$.\n\nThe problem explicitly asks how boundary cases are handled. The given draw is $U=0.7$. This value coincides exactly with the cumulative probability $c_3$. The choice of the inequality $c_{k-1}  U \\le c_k$ is crucial. The semi-open interval $(c_{k-1}, c_k]$ includes its right endpoint. Therefore, if $U$ is equal to some $c_k$, it falls into the interval corresponding to the index $k$.\n\nThis convention is a direct consequence of the definition of the generalized inverse CDF, or quantile function, which for a distribution function $F$ is given by $F^{-1}(u) = \\inf\\{x : F(x) \\ge u\\}$. For our discrete case, the sample is $x_k$ where $k$ is the smallest integer for which $c_k \\ge U$.\n\nApplying this rule to the given uniform variate $U=0.7$:\nWe seek the smallest integer $k \\in \\{1, 2, 3, 4\\}$ such that $c_k \\ge 0.7$.\n- For $k=1$: Is $c_1 \\ge 0.7$? $0.05 \\ge 0.7$ is false.\n- For $k=2$: Is $c_2 \\ge 0.7$? $0.30 \\ge 0.7$ is false.\n- For $k=3$: Is $c_3 \\ge 0.7$? $0.70 \\ge 0.7$ is true.\n\nSince we have found the smallest index $k$ for which the condition holds, the search terminates. The sampled index is $k=3$. The corresponding outcome would be $x_3$.\n\nTo verify using the interval notation:\n- For $k=1$: $c_0  U \\le c_1 \\implies 0  0.7 \\le 0.05$. False.\n- For $k=2$: $c_1  U \\le c_2 \\implies 0.05  0.7 \\le 0.30$. False.\n- For $k=3$: $c_2  U \\le c_3 \\implies 0.30  0.7 \\le 0.70$. True, because $U=0.7$ satisfies the $\\le$ condition.\n- For $k=4$: $c_3  U \\le c_4 \\implies 0.70  0.7 \\le 1.00$. False.\n\nBoth formulations confirm that the sampled index is $k=3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "Translating a theoretically perfect algorithm into functional code requires confronting the realities of finite-precision arithmetic. This practice explores a critical numerical pitfall: the loss of probability mass when computing cumulative sums for highly skewed distributions due to floating-point absorption. You will quantify this effect and implement a mitigation strategy, developing essential skills for writing robust simulation code .",
            "id": "3314825",
            "problem": "Consider inverse transform sampling for a discrete random variable with finite support $\\{1,2,\\dots,n\\}$. Let the probability mass function (pmf) be given by nonnegative probabilities $p_1,\\dots,p_n$ with cumulative distribution function (cdf) thresholds $c_k = \\sum_{i=1}^k p_i$ for $k \\in \\{1,\\dots,n\\}$. The inverse transform method draws a single uniform random variate $U$ on $[0,1]$ and returns the smallest index $k$ such that $U \\le c_k$. In practical implementations on the Institute of Electrical and Electronics Engineers (IEEE) $754$ double precision arithmetic, $c_k$ is computed via floating-point addition. Because floating-point addition rounds to the nearest representable number, it is possible that $c_k$ equals $c_{k-1}$ numerically even though $p_k > 0$, which collapses the interval $(c_{k-1},c_k]$ to have zero width. In that case, category $k$ is unreachable by the inverse transform selection, and its probability mass is effectively lost as far as the sampler is concerned. \n\nYour tasks are:\n- Starting from the core definition of inverse transform sampling and the behavior of floating-point round-to-nearest addition, develop a principled way to quantify the total probability mass that becomes unreachable when cumulative thresholds $c_k$ are formed by sequential floating-point addition $c_k = c_{k-1} + p_k$ (with $c_0 = 0$). The quantification must be expressible purely in terms of the pmf and IEEE $754$ arithmetic outcomes, without appeal to external data, and must be implementable algorithmically.\n- For the highly skewed pmf specified by $p_1 = 1 - 10^{-12}$ and $p_2 = \\cdots = p_{10} = 10^{-13}$ (all arithmetic in IEEE $754$ double precision), determine the risk that all small masses $p_2,\\dots,p_{10}$ numerically underflow in the cumulative thresholds $c_k$ (that is, $c_k = c_{k-1}$ numerically for each of $k \\in \\{2,\\dots,10\\}$), and compute the total lost mass under that risk model as described above.\n- Propose and implement at least one mitigation strategy grounded in first principles (for example, reordering the accumulation or using a numerically stable summation scheme) that reduces or eliminates the loss of probability mass when forming cumulative thresholds, and quantify its effect on the same pmf.\n\nDesign a program that computes the total lost probability mass under naive forward accumulation and under your chosen mitigation for each pmf in the following test suite. Each pmf is provided as a concrete list of $p_i$ values in IEEE $754$ double precision:\n\n- Test case $1$ (skewed, as in the scenario): $n = 10$, $p_1 = 1 - 10^{-12}$, $p_2 = \\cdots = p_{10} = 10^{-13}$.\n- Test case $2$ (individual small masses well below unit roundoff near $1$): $n = 10$, $p_1 = 1 - 9 \\cdot 10^{-17}$, $p_2 = \\cdots = p_{10} = 10^{-17}$.\n- Test case $3$ (exact unit in the last place near $1$): let $\\varepsilon = 2^{-52}$, $n = 10$, $p_1 = 1 - 3\\varepsilon$, $p_2 = p_3 = p_4 = \\varepsilon$, $p_5 = \\cdots = p_{10} = 0$.\n- Test case $4$ (subnormal tail well below unit roundoff near $1$): $n = 10$, $p_1 = 1 - 9 \\cdot 10^{-309}$, $p_2 = \\cdots = p_{10} = 10^{-309}$.\n- Test case $5$ (individual small masses slightly below unit roundoff near $1$): $n = 10$, $p_1 = 1 - 9 \\cdot 10^{-16}$, $p_2 = \\cdots = p_{10} = 10^{-16}$.\n\nYour program must, for each test case $j \\in \\{1,2,3,4,5\\}$:\n- Compute the total lost mass under naive forward accumulation, defined as the sum of $p_k$ for which the sequential floating-point update $c_k \\leftarrow c_{k-1} + p_k$ yields $c_k = c_{k-1}$ numerically.\n- Compute the total lost mass under your mitigation.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The line must be a list of two lists: the first list contains the five floating-point results for the naive lost masses for test cases $1$ through $5$, and the second list contains the five floating-point results for the mitigated lost masses for the same test cases, in order. For example, the output should have the exact form \"[[r1_naive,r2_naive,r3_naive,r4_naive,r5_naive],[r1_mitigated,r2_mitigated,r3_mitigated,r4_mitigated,r5_mitigated]]\", where each $r\\_{\\cdot}$ is a floating-point number.",
            "solution": "The problem requires an analysis of numerical precision loss in the inverse transform sampling method for discrete random variables, specifically concerning the computation of cumulative distribution function (CDF) thresholds using IEEE $754$ double precision arithmetic. A method to quantify this loss and a mitigation strategy must be developed and implemented.\n\nThe problem is scientifically well-posed and grounded in the established principles of numerical analysis and stochastic simulation. It describes a known phenomenon where floating-point addition can lead to a loss of information, specifically the absorption of small quantities when added to large ones. The objectives are clear, formalizable, and verifiable through computation. The problem is therefore deemed valid.\n\n### Principle of Numerical Loss in Cumulative Summation\n\nFor a discrete random variable with support $\\{1, 2, \\dots, n\\}$ and a probability mass function (pmf) $\\{p_1, p_2, \\dots, p_n\\}$, the CDF is defined as $C(k) = \\sum_{i=1}^k p_i$. The inverse transform method relies on the sequence of CDF values $c_k = C(k)$. A standard implementation computes these sequentially:\n$$c_k = c_{k-1} + p_k$$\nwith $c_0 = 0$.\n\nIn floating-point arithmetic, this operation is $c_k' = \\text{fl}(c_{k-1}' + p_k)$, where $\\text{fl}(\\cdot)$ denotes the operation of rounding to the nearest representable floating-point number. A loss of probability mass for category $k$ occurs if the computed cumulative probability does not increase after adding $p_k$, i.e., $c_k' = c_{k-1}'$. This happens when $p_k$ is substantially smaller in magnitude than $c_{k-1}'$.\n\nAccording to the IEEE $754$ standard for double precision arithmetic, a number $x$ is added to $y$ and the result is rounded. If we consider the addition $a+b$, the result will be rounded to $a$ if $|b|$ is less than half of the gap between $a$ and the next representable number. This gap is called the \"unit in the last place,\" or $\\text{ulp}(a)$. The condition for absorption is approximately:\n$\\text{fl}(a+b) = a \\quad \\text{if} \\quad |b|  \\frac{1}{2}\\text{ulp}(a)$\nFor numbers $a$ in the interval $[1, 2)$, $\\text{ulp}(a)$ is the machine epsilon, $\\varepsilon = 2^{-52} \\approx 2.22 \\times 10^{-16}$. In the context of the problem, if $p_1$ is close to $1$, then $c_1' = \\text{fl}(p_1)$ will also be close to $1$. Subsequent additions of small probabilities $p_k$ for $k \\ge 2$ will be to a cumulative sum $c_{k-1}' \\approx 1$. Therefore, if $p_k  \\frac{1}{2}\\text{ulp}(c_{k-1}') \\approx \\frac{1}{2}\\varepsilon \\approx 1.11 \\times 10^{-16}$, the mass $p_k$ is guaranteed to be absorbed, leading to $c_k' = c_{k-1}'$. The category $k$ becomes unreachable.\n\n### Quantification of Lost Mass\n\n**1. Naive Forward Accumulation**\nThe problem provides a direct algorithmic definition for quantifying the lost mass. The total lost mass is the sum of all probabilities $p_k$ for which the numerically computed cumulative sum does not change.\n\nLet $p = (p_1, p_2, \\dots, p_n)$ be the vector of probabilities represented as double precision numbers.\nThe algorithm is as follows:\n1. Initialize total lost mass $L_{naive} = 0$ and cumulative sum $c' = 0$.\n2. For $k=1, \\dots, n$:\n   a. Let $c_{prev}' = c'$.\n   b. Update the cumulative sum: $c' \\leftarrow \\text{fl}(c_{prev}' + p_k)$.\n   c. If $c' = c_{prev}'$ and $p_k > 0$, then update the lost mass: $L_{naive} \\leftarrow L_{naive} + p_k$.\n3. Return $L_{naive}$.\n\nThis procedure directly translates the problem's definition into a computational method.\n\n**2. Mitigation Strategy and Quantification**\nThe root cause of the numerical error is adding very small numbers to a large number (one close to $1$). A standard principle in numerical analysis is to reorder summation to add numbers of similar magnitude first.\n\nAn initial idea is to compute the CDF via its complement, $c_k = 1 - \\sum_{i=k+1}^n p_i$. The tail sum $T_k = \\sum_{i=k+1}^n p_i$ can be computed accurately by summing backwards from $p_n$ (adding small numbers first). However, this strategy is ultimately flawed because the final operation, $c_k' = \\text{fl}(1 - T_k)$, re-introduces the same fundamental problem. If the accurately computed tail sum $T_k$ is smaller than $\\frac{1}{2}\\varepsilon$, the subtraction from $1$ will be rounded back to $1.0$, collapsing the interval.\n\nA more robust mitigation strategy is to **sort the probabilities before accumulation**.\n1. Create a new vector $p_{sorted}$ by sorting the original probabilities $p_k$ in ascending order.\n2. Compute the cumulative sums for this sorted vector: $c'_{(j)} = \\text{fl}(c'_{(j-1)} + p_{(j)})$, where $p_{(j)}$ is the $j$-th element of $p_{sorted}$.\nThis approach ensures that small numbers are added together first, building up a sum that gradually increases in magnitude. This minimizes the loss of precision at each step. While the resulting CDF thresholds $c'_{(j)}$ correspond to a reordered set of outcomes, the process of their formation can be analyzed for numerical stability. We can quantify the \"lost mass\" within this improved computational scheme to demonstrate its effectiveness. The lost mass under this mitigated scheme is calculated by applying the same logic as the naive case but to the sorted probability vector.\n\nThe algorithm for the mitigated case is:\n1. Create a sorted vector $p_{sorted}$ from $p$.\n2. Initialize total lost mass $L_{mitigated} = 0$ and cumulative sum $c' = 0$.\n3. For each probability $p_{val}$ in $p_{sorted}$:\n   a. Let $c_{prev}' = c'$.\n   b. Update the cumulative sum: $c' \\leftarrow \\text{fl}(c_{prev}' + p_{val})$.\n   c. If $c' = c_{prev}'$ and $p_{val} > 0$, then update the lost mass: $L_{mitigated} \\leftarrow L_{mitigated} + p_{val}$.\n4. Return $L_{mitigated}$.\n\nThis strategy effectively eliminates the loss of mass for the given test cases, as the summation of small probabilities is performed first, preserving their contribution to the cumulative sum until the final large probability is added.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_lost_mass_naive(p_list: list[np.float64]) - np.float64:\n    \"\"\"\n    Computes the total lost probability mass under naive forward accumulation.\n\n    The lost mass is the sum of probabilities p_k for which the sequential\n    floating-point update c_k = c_{k-1} + p_k results in c_k == c_{k-1}.\n    \"\"\"\n    n = len(p_list)\n    lost_mass = np.float64(0.0)\n    c_k = np.float64(0.0)\n    \n    for k in range(n):\n        p_k = p_list[k]\n        c_prev = c_k\n        c_k = c_prev + p_k\n        \n        # Check if the cumulative sum failed to advance despite a positive probability\n        if c_k == c_prev and p_k > 0:\n            lost_mass += p_k\n            \n    return lost_mass\n\ndef calculate_lost_mass_mitigated(p_list: list[np.float64]) - np.float64:\n    \"\"\"\n    Computes the total lost probability mass under a mitigation strategy.\n\n    The mitigation strategy involves sorting the probabilities in ascending order\n    before performing the cumulative summation. This ensures that small numbers\n    are added to other small numbers, preserving precision.\n    \"\"\"\n    # Sort probabilities from smallest to largest\n    p_sorted = sorted(p_list)\n    \n    lost_mass = np.float64(0.0)\n    c_k = np.float64(0.0)\n    \n    for p_val in p_sorted:\n        c_prev = c_k\n        c_k += p_val\n        \n        # Check for lost mass in the same way, but on the sorted list\n        if c_k == c_prev and p_val > 0:\n            lost_mass += p_val\n            \n    return lost_mass\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on the specified test cases.\n    \"\"\"\n    # Define test cases using IEEE 754 double precision (np.float64)\n    eps = np.float64(2**-52)\n    \n    test_cases = [\n        # Test case 1 (skewed)\n        [np.float64(1.0 - 1e-12)] + [np.float64(1e-13)] * 9,\n        # Test case 2 (masses below unit roundoff near 1)\n        [np.float64(1.0 - 9e-17)] + [np.float64(1e-17)] * 9,\n        # Test case 3 (exact unit in the last place near 1)\n        [np.float64(1.0 - 3 * eps), eps, eps, eps] + [np.float64(0.0)] * 6,\n        # Test case 4 (subnormal tail)\n        [np.float64(1.0 - 9e-309)] + [np.float64(1e-309)] * 9,\n        # Test case 5 (masses slightly below unit roundoff near 1)\n        [np.float64(1.0 - 9e-16)] + [np.float64(1e-16)] * 9,\n    ]\n\n    naive_results = []\n    mitigated_results = []\n\n    for p_list in test_cases:\n        # Ensure probabilities are numpy float64\n        p_np = [np.float64(p) for p in p_list]\n        \n        # Calculate lost mass for both methods\n        naive_loss = calculate_lost_mass_naive(p_np)\n        mitigated_loss = calculate_lost_mass_mitigated(p_np)\n        \n        naive_results.append(naive_loss)\n        mitigated_results.append(mitigated_loss)\n\n    # Format the final output exactly as specified\n    naive_str = ','.join(map(str, naive_results))\n    mitigated_str = ','.join(map(str, mitigated_results))\n    \n    print(f\"[[{naive_str}],[{mitigated_str}]]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many real-world stochastic models are defined over enormous, yet sparse, state spaces where a naive implementation would be computationally infeasible. This hands-on problem challenges you to design and implement an efficient inverse transform sampler that scales with the number of non-zero probability outcomes, $s$, not the size of the support, $n$. You will use sorted data structures and binary search to build a sampler that is both memory- and time-efficient, a crucial skill for tackling large-scale simulations .",
            "id": "3314767",
            "problem": "You are given a discrete distribution on a very large finite support of size $n$, where only $s$ support points have nonzero probability mass. The nonzero masses are provided as a multiset of $(\\text{index}, \\text{weight})$ pairs. The indices lie in $\\{0,1,\\dots,n-1\\}$, and the weights are strictly positive real numbers. The task is to design and implement an Inverse Transform Sampling (ITS) procedure specialized for the sparse case by constructing a sorted sparse prefix sum, and then to quantify the behavior of your implementation through a test suite. The implementation must not materialize any dense arrays of length $n$.\n\nFundamental base assumptions and definitions to use:\n- A Probability Mass Function (PMF) is any mapping $p:\\{0,1,\\dots,n-1\\}\\to[0,1]$ such that $\\sum_{i=0}^{n-1}p(i)=1$.\n- The Cumulative Distribution Function (CDF) associated with a PMF is $F(k)=\\sum_{i=0}^{k}p(i)$ for $k\\in\\{0,1,\\dots,n-1\\}$.\n- A Uniform$(0,1)$ random variable has constant density on the unit interval and is a standard source of randomness in Monte Carlo methods.\n\nYour program must:\n- Accept as built-in data a list of $(\\text{index}, \\text{weight})$ pairs and a support size $n$ for each test case.\n- Aggregate equal indices by summing their weights so that each distinct index appears exactly once with its aggregated weight. Discard any pairs with invalid indices that are strictly less than $0$ or strictly greater than or equal to $n$; however, the provided test suite guarantees validity so your program may assert this property instead of silently discarding.\n- Normalize the aggregated weights to probabilities by dividing by their total sum, and construct a sorted sparse prefix sum of the probabilities ordered by increasing index. This prefix sum is the sparse CDF evaluated only at the distinct indices with nonzero mass.\n- Implement an inverse transform sampler that:\n  - Draws $u\\sim \\text{Uniform}(0,1)$.\n  - Uses a binary search on the sparse CDF to find the smallest sparse position whose CDF value is greater than or equal to $u$.\n  - Returns the corresponding original support index in $\\{0,1,\\dots,n-1\\}$.\n  - Supports vectorized sampling of $m$ independent draws in one call using an array of $m$ independent uniforms.\n- Do not materialize any dense length-$n$ arrays; all computation must be in terms of the $s$ nonzero entries and arrays of that scale.\n\nComplexity goals to analyze and reflect in your outputs:\n- Building the sampler by sorting $s$ pairs and constructing the sparse prefix sum should run in time $\\mathcal{O}(s\\log s)$ and memory $\\mathcal{O}(s)$.\n- Each independent sample should be found by a binary search in time $\\mathcal{O}(\\log s)$ on the sparse CDF. For vectorized sampling of $m$ independent draws, the overall time should be $\\mathcal{O}(m\\log s)$.\n\nTest suite specification:\n- Your program must implement and run the following three test cases internally, with the specified constants and random number generator seeds to ensure reproducibility. In all cases, the program must not read any external input.\n  1. Small, unsorted input with repeats:\n     - $n=20$.\n     - Pairs: $(7,2.0)$, $(3,1.0)$, $(7,3.0)$, $(0,4.0)$, $(19,10.0)$.\n     - Sampling size $m=50000$ draws.\n     - Random Number Generator (RNG) seed $r=20231101$ for sampling.\n  2. Degenerate single support point:\n     - $n=10^8$.\n     - Pairs: $(12345678,1.0)$.\n     - Sampling size $m=10000$ draws.\n     - RNG seed $r=20231102$ for sampling.\n  3. Large sparse case approximating the scale $n=10^8$, $s=10^5$:\n     - $n=10^8$.\n     - Construct $s=10^5$ pairs procedurally with a fixed RNG seed $r_{\\text{build}}=20231103$ as follows:\n       - Draw indices independently and identically distributed from the discrete uniform distribution on $\\{0,1,\\dots,n-1\\}$.\n       - Draw positive weights independently and identically distributed from the exponential distribution with rate $\\lambda=1$, i.e., density $w\\mapsto e^{-w}$ on $[0,\\infty)$.\n       - The resulting multiset of pairs may contain repeated indices; these must be aggregated by summation before normalization.\n     - Sampling size $m=20000$ draws using the same seed $r=20231103$ for sampling.\n- For each test case, compute the following outputs:\n  - Let $s^{\\star}$ denote the number of distinct indices after aggregating repeats. Report $s^{\\star}$ as an integer.\n  - Report $\\lceil\\log_2(s^{\\star})\\rceil$ as an integer, which reflects the depth needed for a balanced binary search.\n  - Let $\\mu=\\sum_{i}p(i)\\,i$ be the theoretical mean of the index under the distribution, and let $M_2=\\sum_{i}p(i)\\,i^2$ be the theoretical second moment. Using the $m$ samples, compute the empirical sample mean $\\widehat{\\mu}$ and the empirical sample second moment $\\widehat{M}_2$. Report the relative absolute errors $E_1=\\lvert\\widehat{\\mu}-\\mu\\rvert/\\lvert\\mu\\rvert$ and $E_2=\\lvert\\widehat{M}_2-M_2\\rvert/\\lvert M_2\\rvert$ as floating-point numbers. Use nonrandom algebra for $\\mu$ and $M_2$ based on the normalized probabilities, and randomness only for the samples.\n- Final output format:\n  - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The overall result must be a list of length $3$, one entry per test case, in order. Each entry must itself be a list of the form $[s^{\\star},\\lceil\\log_2(s^{\\star})\\rceil,E_1,E_2]$, where $E_1$ and $E_2$ are rounded to six digits after the decimal point. For example, the output must look like $[[s^{\\star}_1,\\lceil\\log_2(s^{\\star}_1)\\rceil,E_{1,1},E_{2,1}],[s^{\\star}_2,\\lceil\\log_2(s^{\\star}_2)\\rceil,E_{1,2},E_{2,2}],[s^{\\star}_3,\\lceil\\log_2(s^{\\star}_3)\\rceil,E_{1,3},E_{2,3}]]$ with the specified numeric rounding.\n\nScientific realism and constraints:\n- All arrays and computations must be constructed only from the $s$ nonzero entries and the sample size $m$, never from a dense array of length $n$.\n- Use only well-tested numerical operations such as sorting, cumulative sums, and binary searches on monotone arrays.\n- Angles are not involved in this problem, so no angle units are required. No physical units are involved.\n\nYour output must be fully determined by the built-in tests and RNG seeds, so it is reproducible across executions.",
            "solution": "The problem of generating random variates from a specified probability distribution is a cornerstone of stochastic simulation. The inverse transform sampling (ITS) method is a fundamental and widely applicable technique for this task. We are asked to design and implement an efficient ITS procedure for a discrete probability distribution defined on a large support $\\{0, 1, \\dots, n-1\\}$, but where the probability mass is concentrated on a small, sparse subset of $s$ points. The primary constraint is to avoid materializing any data structures of size $n$, which would be computationally infeasible for large $n$.\n\nThe solution consists of two main stages: a one-time setup of a sparse sampler data structure and the repeated use of this structure to generate samples.\n\n**1. The Principle of Inverse Transform Sampling for Discrete Variables**\n\nLet $X$ be a discrete random variable with support $\\{x_1, x_2, \\dots\\}$ and probability mass function (PMF) $p(x_i) = P(X=x_i)$. The cumulative distribution function (CDF) is $F(x) = P(X \\le x) = \\sum_{x_i \\le x} p(x_i)$. The ITS method relies on the following property: if $U$ is a random variable uniformly distributed on $(0,1)$, then the random variable $X = F^{-1}(U)$ has the CDF $F(x)$. For a discrete distribution, the inverse CDF is defined as $F^{-1}(u) = \\inf\\{x: F(x) \\ge u\\}$.\n\nAlgorithmically, this means to draw a sample from the distribution of $X$, we:\n1.  Draw a random number $u$ from a Uniform$(0,1)$ distribution.\n2.  Find the smallest index $k$ from the support such that its cumulative probability $F(k)$ is greater than or equal to $u$.\n3.  This index $k$ is our sample.\n\nA naive implementation would involve computing and storing the entire CDF as an array of length $n$. Searching this array for each sample would be efficient, but constructing and storing it would require $\\mathcal{O}(n)$ time and memory, which is prohibitive for the specified problem scale (e.g., $n=10^8$).\n\n**2. Algorithmic Design for Sparse Distributions**\n\nThe key insight is that the CDF is a step function that only changes its value at the $s$ points with non-zero probability mass. We can therefore represent the CDF sparsely, using only these $s$ points. This leads to an algorithm with time and memory complexities that depend on $s$, not $n$.\n\nThe procedure is as follows:\n\n**Step 2.1: Preprocessing and Aggregation**\n\nThe input is a multiset of $(\\text{index}, \\text{weight})$ pairs, which may contain duplicate indices. We must first aggregate these pairs to obtain a unique set of indices, each associated with the sum of its weights. A hash map (dictionary in Python) is the ideal data structure for this, allowing for aggregation in expected $\\mathcal{O}(s)$ time. This step produces $s^{\\star} \\le s$ unique pairs of $(\\text{index}, \\text{total\\_weight})$.\n\n**Step 2.2: Sorting by Index**\n\nTo construct a CDF, the probability masses must be ordered by their support indices. We sort the $s^{\\star}$ aggregated pairs in ascending order of their indices. This is the most computationally intensive part of the setup, requiring $\\mathcal{O}(s^{\\star} \\log s^{\\star})$ time.\n\n**Step 2.3: Normalization and Sparse CDF Construction**\n\nAfter sorting, we have two ordered sequences: the unique indices $j'_1  j'_2  \\dots  j'_{s^{\\star}}$ and their corresponding aggregated weights $W'_1, W'_2, \\dots, W'_{s^{\\star}}$.\n1.  **Normalization**: We compute the total weight $W_{\\text{total}} = \\sum_{l=1}^{s^{\\star}} W'_l$. The probability mass for each index $j'_l$ is then $p'_l = W'_l / W_{\\text{total}}$.\n2.  **Prefix Sum**: We construct the sparse CDF by computing the prefix sum (cumulative sum) of the sorted probabilities. Let this be the array $C$, where $C_l = \\sum_{k=1}^{l} p'_k$. The $l$-th element $C_l$ represents the value of the CDF at the $l$-th sparse support point, i.e., $C_l = F(j'_l)$.\n\nThe setup phase results in two arrays of length $s^{\\star}$: one holding the sorted unique indices (`sparse_indices`) and another holding the corresponding cumulative probabilities (`sparse_cdf`). The memory required is $\\mathcal{O}(s^{\\star})$. The total setup time is dominated by sorting, yielding a complexity of $\\mathcal{O}(s \\log s)$.\n\n**Step 2.4: Vectorized Sampling**\n\nWith the sparse data structures prepared, drawing $m$ samples is highly efficient:\n1.  Generate an array of $m$ independent random numbers $u_1, u_2, \\dots, u_m$ from Uniform$(0,1)$.\n2.  For each $u_k$, we need to find the index $l$ of the first element in `sparse_cdf` that is greater than or equal to $u_k$. Since `sparse_cdf` is monotonically increasing, this search can be performed efficiently using binary search in $\\mathcal{O}(\\log s^{\\star})$ time.\n3.  The desired sample is the original support index found at `sparse_indices[l]`.\n\nModern numerical libraries like NumPy offer vectorized implementations of binary search (e.g., `numpy.searchsorted`), which can find the insertion points for all $m$ uniform variates in the `sparse_cdf` array in a single, highly optimized call. The total time for sampling $m$ variates is therefore $\\mathcal{O}(m \\log s^{\\star})$.\n\n**3. Verification and Analysis**\n\nTo validate the implementation, we compare the empirical moments computed from the generated samples against their theoretical counterparts derived directly from the normalized PMF.\n-   The theoretical mean is $\\mu = \\sum_{l=1}^{s^{\\star}} p'_l \\cdot j'_l$.\n-   The theoretical second moment is $M_2 = \\sum_{l=1}^{s^{\\star}} p'_l \\cdot (j'_l)^2$.\n-   The sample mean is $\\widehat{\\mu} = \\frac{1}{m} \\sum_{k=1}^{m} \\text{sample}_k$.\n-   The sample second moment is $\\widehat{M}_2 = \\frac{1}{m} \\sum_{k=1}^{m} (\\text{sample}_k)^2$.\n\nBy the Law of Large Numbers, as the sample size $m$ increases, the empirical moments should converge to the theoretical moments. The relative errors, $E_1 = \\lvert\\widehat{\\mu}-\\mu\\rvert/\\lvert\\mu\\rvert$ and $E_2 = \\lvert\\widehat{M}_2-M_2\\rvert/\\lvert M_2\\rvert$, quantify this convergence and should be small for a sufficiently large $m$. Additionally, we report $s^{\\star}$ and $\\lceil\\log_2(s^{\\star})\\rceil$, which confirm the scale of the sparse problem and the logarithmic depth of the binary search, respectively. This comprehensive approach validates both the correctness and efficiency of the sparse inverse transform sampler.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\nclass SparseITSampler:\n    \"\"\"\n    An Inverse Transform Sampler for sparse discrete probability distributions.\n\n    It is initialized with (index, weight) pairs defining a PMF on a large\n    support {0, ..., n-1}. It avoids materializing dense arrays of size n.\n    \"\"\"\n    def __init__(self, pairs, n):\n        \"\"\"\n        Initializes the sampler by processing pairs, sorting, and building a sparse CDF.\n        \n        Complexity: O(s log s) time, O(s) space, where s is the number of pairs.\n        \"\"\"\n        # The problem statement guarantees all indices are in [0, n-1].\n        self.n = n\n\n        # Step 1: Aggregate weights for repeated indices. O(s)\n        agg_weights = {}\n        for index, weight in pairs:\n            agg_weights[index] = agg_weights.get(index, 0.0) + weight\n        \n        if not agg_weights:\n            self.s_star = 0\n            self.indices = np.array([], dtype=np.int64)\n            self.probs = np.array([], dtype=np.float64)\n            self.cdf = np.array([], dtype=np.float64)\n            return\n\n        # Step 2: Sort pairs by index. O(s* log s*)\n        sorted_pairs = sorted(agg_weights.items())\n        \n        self.s_star = len(sorted_pairs)\n        \n        indices_arr = np.array([p[0] for p in sorted_pairs], dtype=np.int64)\n        weights_arr = np.array([p[1] for p in sorted_pairs], dtype=np.float64)\n        \n        self.indices = indices_arr\n        \n        # Step 3: Normalize weights and build sparse CDF. O(s*)\n        total_weight = np.sum(weights_arr)\n        if total_weight > 0:\n            self.probs = weights_arr / total_weight\n        else:\n            # This case is avoided by problem spec (weights > 0)\n            self.probs = np.zeros_like(weights_arr)\n\n        self.cdf = np.cumsum(self.probs)\n        # Ensure the CDF ends precisely at 1.0 to handle u=1.0 correctly,\n        # though numpy's uniform is in [0, 1).\n        if self.s_star > 0:\n            self.cdf[-1] = 1.0\n\n    def sample(self, m, seed):\n        \"\"\"\n        Generates m samples from the distribution.\n        \n        Complexity: O(m log s*) time.\n        \"\"\"\n        if self.s_star == 0:\n            return np.array([], dtype=np.int64)\n        \n        rng = np.random.default_rng(seed)\n        u_samples = rng.uniform(size=m)\n        \n        # Use binary search (vectorized) to find the sample for each u.\n        # np.searchsorted(a, v, side='left') finds indices i such that a[i-1]  v = a[i]\n        # which is equivalent to finding the smallest index i where a[i] >= v.\n        sample_indices_in_cdf = np.searchsorted(self.cdf, u_samples, side='left')\n        \n        return self.indices[sample_indices_in_cdf]\n\n    def theoretical_moments(self):\n        \"\"\"\n        Calculates the theoretical mean and second moment of the distribution.\n        \"\"\"\n        if self.s_star == 0:\n            return 0.0, 0.0\n        \n        # Using float64 for indices avoids potential overflow in squaring,\n        # although int64 is sufficient for indices up to ~3e9.\n        indices_f64 = self.indices.astype(np.float64)\n        \n        mu = np.dot(self.probs, indices_f64)\n        m2 = np.dot(self.probs, indices_f64**2)\n        \n        return mu, m2\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases_spec = [\n        # 1. Small, unsorted input with repeats\n        {\n            \"n\": 20,\n            \"pairs\": [(7, 2.0), (3, 1.0), (7, 3.0), (0, 4.0), (19, 10.0)],\n            \"m\": 50000,\n            \"seed\": 20231101,\n        },\n        # 2. Degenerate single support point\n        {\n            \"n\": int(1e8),\n            \"pairs\": [(12345678, 1.0)],\n            \"m\": 10000,\n            \"seed\": 20231102,\n        },\n        # 3. Large sparse case\n        {\n            \"n\": int(1e8),\n            \"s_gen\": int(1e5), \n            \"build_seed\": 20231103,\n            \"m\": 20000,\n            \"seed\": 20231103,\n        }\n    ]\n\n    # Procedurally generate pairs for Test Case 3\n    case3_spec = test_cases_spec[2]\n    rng_build = np.random.default_rng(case3_spec[\"build_seed\"])\n    g_indices = rng_build.integers(0, case3_spec[\"n\"], size=case3_spec[\"s_gen\"])\n    g_weights = rng_build.exponential(scale=1.0, size=case3_spec[\"s_gen\"])\n    case3_spec[\"pairs\"] = list(zip(g_indices, g_weights))\n\n    results = []\n    for case in test_cases_spec:\n        # Initialize the sampler from the specification\n        sampler = SparseITSampler(case[\"pairs\"], case[\"n\"])\n        \n        # 1. s_star: number of distinct indices\n        s_star = sampler.s_star\n        \n        # 2. ceil(log2(s_star)): theoretical binary search depth\n        log2_s_star = 0\n        if s_star > 0:\n            # (s_star - 1).bit_length() is an efficient integer alternative\n            log2_s_star = math.ceil(math.log2(s_star))\n\n        # Calculate theoretical moments\n        mu, m2 = sampler.theoretical_moments()\n        \n        # Generate samples\n        samples = sampler.sample(case[\"m\"], case[\"seed\"])\n        \n        # Calculate empirical moments from samples\n        hat_mu = np.mean(samples)\n        # Cast to float64 before squaring to prevent potential overflow with large indices\n        hat_m2 = np.mean(samples.astype(np.float64)**2)\n\n        # 3. E1: relative error of the mean\n        e1 = abs(hat_mu - mu) / abs(mu) if mu != 0 else (0.0 if hat_mu == 0 else float('inf'))\n\n        # 4. E2: relative error of the second moment\n        e2 = abs(hat_m2 - m2) / abs(m2) if m2 != 0 else (0.0 if hat_m2 == 0 else float('inf'))\n\n        # Format results for the current test case\n        case_result = f\"[{s_star},{log2_s_star},{e1:.6f},{e2:.6f}]\"\n        results.append(case_result)\n    \n    # Print the final output in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}