{
    "hands_on_practices": [
        {
            "introduction": "To truly master inverse transform sampling, we begin with a foundational exercise that demystifies the core mechanism. This practice walks you through a direct, step-by-step application of the method for a simple discrete distribution with a handful of outcomes. By manually calculating the cumulative probabilities and mapping a given uniform variate to its corresponding outcome, you will gain a concrete understanding of how the unit interval is partitioned according to the probability mass function .",
            "id": "3314769",
            "problem": "Consider a discrete random variable $X$ supported on the ordered set $\\{x_{1}, x_{2}, x_{3}, x_{4}\\}$ with probability vector $p=(0.05, 0.25, 0.4, 0.3)$, where $p_{i}=\\mathbb{P}(X=x_{i})$ for $i\\in\\{1,2,3,4\\}$. A single draw $U$ is observed from the continuous uniform distribution on $(0,1)$, with $U=0.7$. Using only the definition of the cumulative distribution function (CDF) for discrete variables and the first-principles construction that maps a uniform $(0,1)$ variate to an outcome via the inverse cumulative principle, do the following: construct the cumulative array $c_{k}=\\sum_{i=1}^{k}p_{i}$ for $k\\in\\{1,2,3,4\\}$, determine the sampled index $k$ selected by inverse transform sampling from this $U$, and identify the corresponding outcome $x_{k}$. Explicitly articulate how boundary cases where $U$ coincides with a jump of the CDF are handled based on the definitions you use. Report only the sampled index $k$ as your final answer. No rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of stochastic simulation, is well-posed with a complete and consistent setup, and is expressed in objective, formal language. The provided probability vector $p=(0.05, 0.25, 0.4, 0.3)$ is valid, as its elements are non-negative and sum to $1$: $0.05 + 0.25 + 0.4 + 0.3 = 1$. The problem asks for a direct application of the inverse transform sampling method for a discrete random variable, a fundamental technique in Monte Carlo methods.\n\nLet $X$ be the discrete random variable with support $\\{x_{1}, x_{2}, x_{3}, x_{4}\\}$ and probability mass function (PMF) given by $\\mathbb{P}(X=x_i) = p_i$. The given probability vector is $p = (p_1, p_2, p_3, p_4) = (0.05, 0.25, 0.4, 0.3)$.\n\nThe inverse transform sampling method relies on the cumulative distribution function (CDF) of $X$. For a discrete variable, the CDF, $F_X(x) = \\mathbb{P}(X \\le x)$, is a step function. The values of the CDF at the support points $x_k$ are given by the cumulative probabilities $c_k$:\n$$c_k = F_X(x_k) = \\sum_{i=1}^{k} p_i$$\nBy convention, we define $c_0 = 0$.\n\nFirst, we construct the cumulative array $c = (c_1, c_2, c_3, c_4)$ using the given probabilities:\n- $c_1 = p_1 = 0.05$\n- $c_2 = p_1 + p_2 = 0.05 + 0.25 = 0.30$\n- $c_3 = c_2 + p_3 = 0.30 + 0.40 = 0.70$\n- $c_4 = c_3 + p_4 = 0.70 + 0.30 = 1.00$\nThe cumulative array is thus $(0.05, 0.30, 0.70, 1.00)$.\n\nThe principle of inverse transform sampling is to generate a random variate $U$ from the standard uniform distribution $\\mathcal{U}(0,1)$ and then find the outcome $x_k$ corresponding to the unique index $k$ that satisfies the condition:\n$$c_{k-1}  U \\le c_k$$\nThis condition partitions the interval $(0,1)$ into disjoint sub-intervals $(c_{k-1}, c_k]$, each of length $c_k - c_{k-1} = p_k$. The probability that $U$ falls into a specific interval $(c_{k-1}, c_k]$ is exactly $p_k$, ensuring that the sampled outcome $x_k$ is chosen with the correct probability. An equivalent and often more practical way to state the rule is to find the smallest index $k$ such that $U \\le c_k$.\n\nThe problem explicitly asks how boundary cases are handled. The given draw is $U=0.7$. This value coincides exactly with the cumulative probability $c_3$. The choice of the inequality $c_{k-1}  U \\le c_k$ is crucial. The semi-open interval $(c_{k-1}, c_k]$ includes its right endpoint. Therefore, if $U$ is equal to some $c_k$, it falls into the interval corresponding to the index $k$.\n\nThis convention is a direct consequence of the definition of the generalized inverse CDF, or quantile function, which for a distribution function $F$ is given by $F^{-1}(u) = \\inf\\{x : F(x) \\ge u\\}$. For our discrete case, the sample is $x_k$ where $k$ is the smallest integer for which $c_k \\ge U$.\n\nApplying this rule to the given uniform variate $U=0.7$:\nWe seek the smallest integer $k \\in \\{1, 2, 3, 4\\}$ such that $c_k \\ge 0.7$.\n- For $k=1$: Is $c_1 \\ge 0.7$? $0.05 \\ge 0.7$ is false.\n- For $k=2$: Is $c_2 \\ge 0.7$? $0.30 \\ge 0.7$ is false.\n- For $k=3$: Is $c_3 \\ge 0.7$? $0.70 \\ge 0.7$ is true.\n\nSince we have found the smallest index $k$ for which the condition holds, the search terminates. The sampled index is $k=3$. The corresponding outcome would be $x_3$.\n\nTo verify using the interval notation:\n- For $k=1$: $c_0  U \\le c_1 \\implies 0  0.7 \\le 0.05$. False.\n- For $k=2$: $c_1  U \\le c_2 \\implies 0.05  0.7 \\le 0.30$. False.\n- For $k=3$: $c_2  U \\le c_3 \\implies 0.30  0.7 \\le 0.70$. True, because $U=0.7$ satisfies the $\\le$ condition.\n- For $k=4$: $c_3  U \\le c_4 \\implies 0.70  0.7 \\le 1.00$. False.\n\nBoth formulations confirm that the sampled index is $k=3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "While direct summation works for any discrete distribution, for many well-known families like the Geometric distribution, we can derive an explicit, closed-form expression for the quantile function. This practice challenges you to derive this function, providing an elegant and efficient shortcut for sampling that avoids iterative summation . More importantly, it pushes you to consider the practical limitations of finite-precision arithmetic, analyzing the numerical stability of this direct formula versus naive methods, a crucial skill for any computational scientist.",
            "id": "3314823",
            "problem": "You are tasked with constructing an inverse transform sampler for a geometric distribution supported on the positive integers. Let $X$ be a geometric random variable with success probability $p \\in (0,1)$ supported on $\\{1,2,\\ldots\\}$, with probability mass function $\\mathbb{P}(X=k)=p(1-p)^{k-1}$ for $k \\in \\mathbb{N}$. Let $U$ be a standard uniform random variable on $(0,1)$, independent of $X$. Starting from the definitions of the cumulative distribution function and the inverse transform method for discrete distributions, derive the exact quantile function $Q(u)$ such that $X \\stackrel{d}{=} Q(U)$, expressed in closed form in terms of $u \\in (0,1)$ and $p \\in (0,1)$. Your derivation must justify each inequality manipulation from first principles and must address the discrete nature of the support.\n\nThen, compare the numerical stability of two inversion procedures for small $p$ in the standard floating-point model with rounding to nearest and unit roundoff $\\varepsilon$: \n- a prefix-sum inversion that accumulates $S_{k}=\\sum_{j=1}^{k} p(1-p)^{j-1}$ until $S_{k} \\geq u$, and \n- a direct inversion that computes $Q(u)$ using elementary functions. \n\nYour comparison must be based on first-order floating-point error propagation and cancellation analysis and should identify the asymptotic scaling, as $p \\to 0$, of the largest $k$ for which the prefix-sum method can still increment $S_{k}$ before stagnation near $1$ occurs. Explain why an implementation of the direct inversion using the compensated functions $\\mathrm{log1p}$ (the natural logarithm of $1+x$) and $\\mathrm{expm1}$ (the exponential of $x$ minus $1$) mitigates cancellation in the regime $u \\uparrow 1$ and $p \\downarrow 0$.\n\nExpress your final answer as the closed-form expression for the exact quantile function $Q(u)$ only. No numerical rounding is required for the final expression. Use the natural logarithm $\\ln$ in your expression.",
            "solution": "The problem as stated is valid. It is scientifically grounded in probability theory and numerical analysis, well-posed, objective, and internally consistent.\n\nFirst, we derive the closed-form expression for the quantile function $Q(u)$. Let $X$ be a geometric random variable on the support $\\{1, 2, 3, \\ldots\\}$ with probability of success $p \\in (0,1)$. The probability mass function (PMF) is given by $\\mathbb{P}(X=k) = p(1-p)^{k-1}$ for $k \\in \\{1, 2, 3, \\ldots\\}$.\n\nThe cumulative distribution function (CDF), $F(k)$, is the probability $\\mathbb{P}(X \\le k)$. For an integer $k \\ge 1$, this is the sum of the probabilities for all outcomes up to $k$:\n$$F(k) = \\mathbb{P}(X \\le k) = \\sum_{j=1}^{k} \\mathbb{P}(X=j) = \\sum_{j=1}^{k} p(1-p)^{j-1}$$\nThis is a finite geometric series with first term $a=p$ and common ratio $r=1-p$. The sum of the first $k$ terms is given by $a\\frac{1-r^k}{1-r}$.\n$$F(k) = p \\frac{1 - (1-p)^k}{1 - (1-p)} = p \\frac{1 - (1-p)^k}{p} = 1 - (1-p)^k$$\nThe inverse transform method for a discrete random variable defines the quantile function $Q(u)$ for $u \\in (0,1)$ as the smallest integer $k$ in the support of $X$ for which the CDF is greater than or equal to $u$.\n$$Q(u) = \\min \\{k \\in \\{1, 2, 3, \\ldots\\} : F(k) \\ge u\\}$$\nWe substitute the expression for the CDF and solve for $k$:\n$$1 - (1-p)^k \\ge u$$\nRearranging the terms of the inequality, we get:\n$$1 - u \\ge (1-p)^k$$\nSince both $1-u$ and $1-p$ are in the interval $(0,1)$, we can take the natural logarithm of both sides. As the natural logarithm is a strictly increasing function, the direction of the inequality is preserved:\n$$\\ln(1 - u) \\ge \\ln((1-p)^k)$$\nUsing the logarithm property $\\ln(a^b) = b\\ln(a)$, we have:\n$$\\ln(1 - u) \\ge k \\ln(1 - p)$$\nTo isolate $k$, we must divide by $\\ln(1-p)$. It is critical to note that since $p \\in (0,1)$, $1-p \\in (0,1)$, which implies that $\\ln(1-p)$ is a negative number. Dividing an inequality by a negative number reverses its direction:\n$$k \\ge \\frac{\\ln(1 - u)}{\\ln(1 - p)}$$\nThe quantile function $Q(u)$ is the smallest integer $k$ that satisfies this condition. The smallest integer greater than or equal to a real number is given by the ceiling function, $\\lceil \\cdot \\rceil$.\nTherefore, the exact quantile function is:\n$$Q(u) = \\left\\lceil \\frac{\\ln(1 - u)}{\\ln(1 - p)} \\right\\rceil$$\n\nNext, we compare the numerical stability of two inversion procedures for small $p$.\n\nThe first method is the prefix-sum inversion. This method computes the partial sums of the PMF, $S_{k} = \\sum_{j=1}^{k} p(1-p)^{j-1}$, iteratively until $S_k \\ge u$. The update step in floating-point arithmetic is $\\hat{S}_{k} = \\mathrm{fl}(\\hat{S}_{k-1} + p(1-p)^{k-1})$, where $\\hat{S}_{k-1}$ is the previously computed sum. For large $k$, the sum $S_{k-1} = 1-(1-p)^{k-1}$ approaches $1$. The term being added, $p(1-p)^{k-1}$, becomes very small. In a floating-point system with unit roundoff $\\varepsilon$, an addition $x+y$ results in $x$ if $|y|  \\varepsilon|x|$. This phenomenon is known as swamping or absorption. In our case, the summation stagnates when the new term is smaller than the precision of the running sum. As $\\hat{S}_{k-1}$ approaches $1$, this occurs when $p(1-p)^{k-1} \\lesssim \\varepsilon$. We can find the approximate value of $k$ where this happens by solving $p(1-p)^{k-1} = \\varepsilon$.\nTaking logarithms: $\\ln(p) + (k-1)\\ln(1-p) = \\ln(\\varepsilon)$.\nFor small $p$, we use the approximation $\\ln(1-p) \\approx -p$.\n$\\ln(p) - (k-1)p \\approx \\ln(\\varepsilon)$\n$(k-1)p \\approx \\ln(p) - \\ln(\\varepsilon)$\n$k \\approx 1 + \\frac{\\ln(p) - \\ln(\\varepsilon)}{p}$.\nThe largest representable value of $k$ scales asymptotically as $O(\\frac{\\ln p}{p})$ as $p \\to 0$. For a small success probability $p$, the mean of the distribution, $1/p$, is large. This numerical limitation means the prefix-sum method can fail to generate variates from the tail of the distribution, even for values comparable to the mean.\n\nThe second method is the direct inversion, which computes $Q(u)$ using the derived formula. In the regime where $p \\to 0$ and $u \\to 1$, this method may also face numerical challenges if implemented naively.\nThe expression to compute is $k = \\lceil \\frac{\\ln(1-u)}{\\ln(1-p)} \\rceil$.\nThe primary numerical issue arises from the denominator, $\\ln(1-p)$, for small $p$. If $p$ is smaller than the machine precision relative to $1$, the floating-point evaluation of $1-p$ may be exactly $1$, leading to $\\ln(1)=0$ and division by zero. Even if $1-p$ is not exactly $1$, its computation involves subtracting a small number from $1$, which can lead to a result with a large relative error that is then passed to the logarithm function. This problem is mitigated by using a compensated function like `log1p(x)`, which computes $\\ln(1+x)$ accurately even for very small $|x|$. By computing the denominator as `log1p(-p)`, we avoid this catastrophic cancellation and obtain an accurate value for $\\ln(1-p)$.\n\nThe role of the `expm1(x)` function, which accurately computes $\\exp(x)-1$ for small $|x|$, is less direct for the closed-form formula itself. However, the problem asks about \"an implementation of the direct inversion\". One such robust implementation, alternative to evaluating the potentially problematic ceiling of a ratio of logs, is to perform a search for the smallest integer $k$ satisfying $F(k) \\ge u$. To do this efficiently and accurately, we need a stable way to compute the CDF, $F(k) = 1 - (1-p)^k$.\nFor small $p$ and moderate $k$, the value of $(1-p)^k$ is close to $1$, so computing $F(k)$ via $1 - (1-p)^k$ leads to catastrophic cancellation. A stable computation can be achieved by rewriting the expression:\n$$F(k) = 1 - \\exp(k \\ln(1-p))$$\nLet $x = k \\ln(1-p)$. For small $p$, $x$ is a small negative number. The computation of $1-\\exp(x)$ is precisely the problem that `expm1` is designed to solve. We can write $F(k) = -\\text{expm1}(x)$. Combining this with the use of `log1p` for the logarithm, we get a numerically stable formula for the CDF:\n$$F(k) = -\\text{expm1}(k \\cdot \\text{log1p}(-p))$$\nAn inversion algorithm can then use this stable CDF calculation within a search procedure (like bisection) to find $k=Q(u)$. This explains how both `log1p` and `expm1` can be used together in a robust implementation to perform inverse transform sampling, particularly in the challenging regime of small $p$ and $u$ near $1$, where large values of $k$ are expected.\nThe direct formula, when implemented carefully as $k = \\lceil \\ln(1-u) / \\text{log1p}(-p) \\rceil$, is generally superior in performance to a search-based method and overcomes the stagnation limitations of the prefix-sum approach.",
            "answer": "$$\n\\boxed{\\left\\lceil \\frac{\\ln(1-u)}{\\ln(1-p)} \\right\\rceil}\n$$"
        },
        {
            "introduction": "Real-world applications often involve distributions over immense state spaces where most outcomes have zero probability. This exercise tackles the challenge of designing an efficient sampler for such sparse distributions, where a naive approach would fail due to prohibitive memory and time requirements . You will combine data structures and algorithmic principles—specifically, aggregation, sorting, and binary search—to build a scalable sampler whose performance depends only on the number of non-zero probability outcomes, not the size of the vast underlying support.",
            "id": "3314767",
            "problem": "You are given a discrete distribution on a very large finite support of size $n$, where only $s$ support points have nonzero probability mass. The nonzero masses are provided as a multiset of $(\\text{index}, \\text{weight})$ pairs. The indices lie in $\\{0,1,\\dots,n-1\\}$, and the weights are strictly positive real numbers. The task is to design and implement an Inverse Transform Sampling (ITS) procedure specialized for the sparse case by constructing a sorted sparse prefix sum, and then to quantify the behavior of your implementation through a test suite. The implementation must not materialize any dense arrays of length $n$.\n\nFundamental base assumptions and definitions to use:\n- A Probability Mass Function (PMF) is any mapping $p:\\{0,1,\\dots,n-1\\}\\to[0,1]$ such that $\\sum_{i=0}^{n-1}p(i)=1$.\n- The Cumulative Distribution Function (CDF) associated with a PMF is $F(k)=\\sum_{i=0}^{k}p(i)$ for $k\\in\\{0,1,\\dots,n-1\\}$.\n- A Uniform$(0,1)$ random variable has constant density on the unit interval and is a standard source of randomness in Monte Carlo methods.\n\nYour program must:\n- Accept as built-in data a list of $(\\text{index}, \\text{weight})$ pairs and a support size $n$ for each test case.\n- Aggregate equal indices by summing their weights so that each distinct index appears exactly once with its aggregated weight. Discard any pairs with invalid indices that are strictly less than $0$ or strictly greater than or equal to $n$; however, the provided test suite guarantees validity so your program may assert this property instead of silently discarding.\n- Normalize the aggregated weights to probabilities by dividing by their total sum, and construct a sorted sparse prefix sum of the probabilities ordered by increasing index. This prefix sum is the sparse CDF evaluated only at the distinct indices with nonzero mass.\n- Implement an inverse transform sampler that:\n  - Draws $u\\sim \\text{Uniform}(0,1)$.\n  - Uses a binary search on the sparse CDF to find the smallest sparse position whose CDF value is greater than or equal to $u$.\n  - Returns the corresponding original support index in $\\{0,1,\\dots,n-1\\}$.\n  - Supports vectorized sampling of $m$ independent draws in one call using an array of $m$ independent uniforms.\n- Do not materialize any dense length-$n$ arrays; all computation must be in terms of the $s$ nonzero entries and arrays of that scale.\n\nComplexity goals to analyze and reflect in your outputs:\n- Building the sampler by sorting $s$ pairs and constructing the sparse prefix sum should run in time $\\mathcal{O}(s\\log s)$ and memory $\\mathcal{O}(s)$.\n- Each independent sample should be found by a binary search in time $\\mathcal{O}(\\log s)$ on the sparse CDF. For vectorized sampling of $m$ independent draws, the overall time should be $\\mathcal{O}(m\\log s)$.\n\nTest suite specification:\n- Your program must implement and run the following three test cases internally, with the specified constants and random number generator seeds to ensure reproducibility. In all cases, the program must not read any external input.\n  1. Small, unsorted input with repeats:\n     - $n=20$.\n     - Pairs: $(7,2.0)$, $(3,1.0)$, $(7,3.0)$, $(0,4.0)$, $(19,10.0)$.\n     - Sampling size $m=50000$ draws.\n     - Random Number Generator (RNG) seed $r=20231101$ for sampling.\n  2. Degenerate single support point:\n     - $n=10^8$.\n     - Pairs: $(12345678,1.0)$.\n     - Sampling size $m=10000$ draws.\n     - RNG seed $r=20231102$ for sampling.\n  3. Large sparse case approximating the scale $n=10^8$, $s=10^5$:\n     - $n=10^8$.\n     - Construct $s=10^5$ pairs procedurally with a fixed RNG seed $r_{\\text{build}}=20231103$ as follows:\n       - Draw indices independently and identically distributed from the discrete uniform distribution on $\\{0,1,\\dots,n-1\\}$.\n       - Draw positive weights independently and identically distributed from the exponential distribution with rate $\\lambda=1$, i.e., density $w\\mapsto e^{-w}$ on $[0,\\infty)$.\n       - The resulting multiset of pairs may contain repeated indices; these must be aggregated by summation before normalization.\n     - Sampling size $m=20000$ draws using the same seed $r=20231103$ for sampling.\n- For each test case, compute the following outputs:\n  - Let $s^{\\star}$ denote the number of distinct indices after aggregating repeats. Report $s^{\\star}$ as an integer.\n  - Report $\\lceil\\log_2(s^{\\star})\\rceil$ as an integer, which reflects the depth needed for a balanced binary search.\n  - Let $\\mu=\\sum_{i}p(i)\\,i$ be the theoretical mean of the index under the distribution, and let $M_2=\\sum_{i}p(i)\\,i^2$ be the theoretical second moment. Using the $m$ samples, compute the empirical sample mean $\\widehat{\\mu}$ and the empirical sample second moment $\\widehat{M}_2$. Report the relative absolute errors $E_1=\\lvert\\widehat{\\mu}-\\mu\\rvert/\\lvert\\mu\\rvert$ and $E_2=\\lvert\\widehat{M}_2-M_2\\rvert/\\lvert M_2\\rvert$ as floating-point numbers. Use nonrandom algebra for $\\mu$ and $M_2$ based on the normalized probabilities, and randomness only for the samples.\n- Final output format:\n  - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The overall result must be a list of length $3$, one entry per test case, in order. Each entry must itself be a list of the form $[s^{\\star},\\lceil\\log_2(s^{\\star})\\rceil,E_1,E_2]$, where $E_1$ and $E_2$ are rounded to six digits after the decimal point. For example, the output must look like $[[s^{\\star}_1,\\lceil\\log_2(s^{\\star}_1)\\rceil,E_{1,1},E_{2,1}],[s^{\\star}_2,\\lceil\\log_2(s^{\\star}_2)\\rceil,E_{1,2},E_{2,2}],[s^{\\star}_3,\\lceil\\log_2(s^{\\star}_3)\\rceil,E_{1,3},E_{2,3}]]$ with the specified numeric rounding.\n\nScientific realism and constraints:\n- All arrays and computations must be constructed only from the $s$ nonzero entries and the sample size $m$, never from a dense array of length $n$.\n- Use only well-tested numerical operations such as sorting, cumulative sums, and binary searches on monotone arrays.\n- Angles are not involved in this problem, so no angle units are required. No physical units are involved.\n\nYour output must be fully determined by the built-in tests and RNG seeds, so it is reproducible across executions.",
            "solution": "The problem of generating random variates from a specified probability distribution is a cornerstone of stochastic simulation. The inverse transform sampling (ITS) method is a fundamental and widely applicable technique for this task. We are asked to design and implement an efficient ITS procedure for a discrete probability distribution defined on a large support $\\{0, 1, \\dots, n-1\\}$, but where the probability mass is concentrated on a small, sparse subset of $s$ points. The primary constraint is to avoid materializing any data structures of size $n$, which would be computationally infeasible for large $n$.\n\nThe solution consists of two main stages: a one-time setup of a sparse sampler data structure and the repeated use of this structure to generate samples.\n\n**1. The Principle of Inverse Transform Sampling for Discrete Variables**\n\nLet $X$ be a discrete random variable with support $\\{x_1, x_2, \\dots\\}$ and probability mass function (PMF) $p(x_i) = P(X=x_i)$. The cumulative distribution function (CDF) is $F(x) = P(X \\le x) = \\sum_{x_i \\le x} p(x_i)$. The ITS method relies on the following property: if $U$ is a random variable uniformly distributed on $(0,1)$, then the random variable $X = F^{-1}(U)$ has the CDF $F(x)$. For a discrete distribution, the inverse CDF is defined as $F^{-1}(u) = \\inf\\{x: F(x) \\ge u\\}$.\n\nAlgorithmically, this means to draw a sample from the distribution of $X$, we:\n1.  Draw a random number $u$ from a Uniform$(0,1)$ distribution.\n2.  Find the smallest index $k$ from the support such that its cumulative probability $F(k)$ is greater than or equal to $u$.\n3.  This index $k$ is our sample.\n\nA naive implementation would involve computing and storing the entire CDF as an array of length $n$. Searching this array for each sample would be efficient, but constructing and storing it would require $\\mathcal{O}(n)$ time and memory, which is prohibitive for the specified problem scale (e.g., $n=10^8$).\n\n**2. Algorithmic Design for Sparse Distributions**\n\nThe key insight is that the CDF is a step function that only changes its value at the $s$ points with non-zero probability mass. We can therefore represent the CDF sparsely, using only these $s$ points. This leads to an algorithm with time and memory complexities that depend on $s$, not $n$.\n\nThe procedure is as follows:\n\n**Step 2.1: Preprocessing and Aggregation**\n\nThe input is a multiset of $(\\text{index}, \\text{weight})$ pairs, which may contain duplicate indices. We must first aggregate these pairs to obtain a unique set of indices, each associated with the sum of its weights. A hash map (dictionary in Python) is the ideal data structure for this, allowing for aggregation in expected $\\mathcal{O}(s)$ time. This step produces $s^{\\star} \\le s$ unique pairs of $(\\text{index}, \\text{total\\_weight})$.\n\n**Step 2.2: Sorting by Index**\n\nTo construct a CDF, the probability masses must be ordered by their support indices. We sort the $s^{\\star}$ aggregated pairs in ascending order of their indices. This is the most computationally intensive part of the setup, requiring $\\mathcal{O}(s^{\\star} \\log s^{\\star})$ time.\n\n**Step 2.3: Normalization and Sparse CDF Construction**\n\nAfter sorting, we have two ordered sequences: the unique indices $j'_1  j'_2  \\dots  j'_{s^{\\star}}$ and their corresponding aggregated weights $W'_1, W'_2, \\dots, W'_{s^{\\star}}$.\n1.  **Normalization**: We compute the total weight $W_{\\text{total}} = \\sum_{l=1}^{s^{\\star}} W'_l$. The probability mass for each index $j'_l$ is then $p'_l = W'_l / W_{\\text{total}}$.\n2.  **Prefix Sum**: We construct the sparse CDF by computing the prefix sum (cumulative sum) of the sorted probabilities. Let this be the array $C$, where $C_l = \\sum_{k=1}^{l} p'_k$. The $l$-th element $C_l$ represents the value of the CDF at the $l$-th sparse support point, i.e., $C_l = F(j'_l)$.\n\nThe setup phase results in two arrays of length $s^{\\star}$: one holding the sorted unique indices (`sparse_indices`) and another holding the corresponding cumulative probabilities (`sparse_cdf`). The memory required is $\\mathcal{O}(s^{\\star})$. The total setup time is dominated by sorting, yielding a complexity of $\\mathcal{O}(s \\log s)$.\n\n**Step 2.4: Vectorized Sampling**\n\nWith the sparse data structures prepared, drawing $m$ samples is highly efficient:\n1.  Generate an array of $m$ independent random numbers $u_1, u_2, \\dots, u_m$ from Uniform$(0,1)$.\n2.  For each $u_k$, we need to find the index $l$ of the first element in `sparse_cdf` that is greater than or equal to $u_k$. Since `sparse_cdf` is monotonically increasing, this search can be performed efficiently using binary search in $\\mathcal{O}(\\log s^{\\star})$ time.\n3.  The desired sample is the original support index found at `sparse_indices[l]`.\n\nModern numerical libraries like NumPy offer vectorized implementations of binary search (e.g., `numpy.searchsorted`), which can find the insertion points for all $m$ uniform variates in the `sparse_cdf` array in a single, highly optimized call. The total time for sampling $m$ variates is therefore $\\mathcal{O}(m \\log s^{\\star})$.\n\n**3. Verification and Analysis**\n\nTo validate the implementation, we compare the empirical moments computed from the generated samples against their theoretical counterparts derived directly from the normalized PMF.\n-   The theoretical mean is $\\mu = \\sum_{l=1}^{s^{\\star}} p'_l \\cdot j'_l$.\n-   The theoretical second moment is $M_2 = \\sum_{l=1}^{s^{\\star}} p'_l \\cdot (j'_l)^2$.\n-   The sample mean is $\\widehat{\\mu} = \\frac{1}{m} \\sum_{k=1}^{m} \\text{sample}_k$.\n-   The sample second moment is $\\widehat{M}_2 = \\frac{1}{m} \\sum_{k=1}^{m} (\\text{sample}_k)^2$.\n\nBy the Law of Large Numbers, as the sample size $m$ increases, the empirical moments should converge to the theoretical moments. The relative errors, $E_1 = \\lvert\\widehat{\\mu}-\\mu\\rvert/\\lvert\\mu\\rvert$ and $E_2 = \\lvert\\widehat{M}_2-M_2\\rvert/\\lvert M_2\\rvert$, quantify this convergence and should be small for a sufficiently large $m$. Additionally, we report $s^{\\star}$ and $\\lceil\\log_2(s^{\\star})\\rceil$, which confirm the scale of the sparse problem and the logarithmic depth of the binary search, respectively. This comprehensive approach validates both the correctness and efficiency of the sparse inverse transform sampler.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\nclass SparseITSampler:\n    \"\"\"\n    An Inverse Transform Sampler for sparse discrete probability distributions.\n\n    It is initialized with (index, weight) pairs defining a PMF on a large\n    support {0, ..., n-1}. It avoids materializing dense arrays of size n.\n    \"\"\"\n    def __init__(self, pairs, n):\n        \"\"\"\n        Initializes the sampler by processing pairs, sorting, and building a sparse CDF.\n        \n        Complexity: O(s log s) time, O(s) space, where s is the number of pairs.\n        \"\"\"\n        # The problem statement guarantees all indices are in [0, n-1].\n        self.n = n\n\n        # Step 1: Aggregate weights for repeated indices. O(s)\n        agg_weights = {}\n        for index, weight in pairs:\n            agg_weights[index] = agg_weights.get(index, 0.0) + weight\n        \n        if not agg_weights:\n            self.s_star = 0\n            self.indices = np.array([], dtype=np.int64)\n            self.probs = np.array([], dtype=np.float64)\n            self.cdf = np.array([], dtype=np.float64)\n            return\n\n        # Step 2: Sort pairs by index. O(s* log s*)\n        sorted_pairs = sorted(agg_weights.items())\n        \n        self.s_star = len(sorted_pairs)\n        \n        indices_arr = np.array([p[0] for p in sorted_pairs], dtype=np.int64)\n        weights_arr = np.array([p[1] for p in sorted_pairs], dtype=np.float64)\n        \n        self.indices = indices_arr\n        \n        # Step 3: Normalize weights and build sparse CDF. O(s*)\n        total_weight = np.sum(weights_arr)\n        if total_weight > 0:\n            self.probs = weights_arr / total_weight\n        else:\n            # This case is avoided by problem spec (weights > 0)\n            self.probs = np.zeros_like(weights_arr)\n\n        self.cdf = np.cumsum(self.probs)\n        # Ensure the CDF ends precisely at 1.0 to handle u=1.0 correctly,\n        # though numpy's uniform is in [0, 1).\n        if self.s_star > 0:\n            self.cdf[-1] = 1.0\n\n    def sample(self, m, seed):\n        \"\"\"\n        Generates m samples from the distribution.\n        \n        Complexity: O(m log s*) time.\n        \"\"\"\n        if self.s_star == 0:\n            return np.array([], dtype=np.int64)\n        \n        rng = np.random.default_rng(seed)\n        u_samples = rng.uniform(size=m)\n        \n        # Use binary search (vectorized) to find the sample for each u.\n        # np.searchsorted(a, v, side='left') finds indices i such that a[i-1]  v = a[i]\n        # which is equivalent to finding the smallest index i where a[i] >= v.\n        sample_indices_in_cdf = np.searchsorted(self.cdf, u_samples, side='left')\n        \n        return self.indices[sample_indices_in_cdf]\n\n    def theoretical_moments(self):\n        \"\"\"\n        Calculates the theoretical mean and second moment of the distribution.\n        \"\"\"\n        if self.s_star == 0:\n            return 0.0, 0.0\n        \n        # Using float64 for indices avoids potential overflow in squaring,\n        # although int64 is sufficient for indices up to ~3e9.\n        indices_f64 = self.indices.astype(np.float64)\n        \n        mu = np.dot(self.probs, indices_f64)\n        m2 = np.dot(self.probs, indices_f64**2)\n        \n        return mu, m2\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases_spec = [\n        # 1. Small, unsorted input with repeats\n        {\n            \"n\": 20,\n            \"pairs\": [(7, 2.0), (3, 1.0), (7, 3.0), (0, 4.0), (19, 10.0)],\n            \"m\": 50000,\n            \"seed\": 20231101,\n        },\n        # 2. Degenerate single support point\n        {\n            \"n\": int(1e8),\n            \"pairs\": [(12345678, 1.0)],\n            \"m\": 10000,\n            \"seed\": 20231102,\n        },\n        # 3. Large sparse case\n        {\n            \"n\": int(1e8),\n            \"s_gen\": int(1e5), \n            \"build_seed\": 20231103,\n            \"m\": 20000,\n            \"seed\": 20231103,\n        }\n    ]\n\n    # Procedurally generate pairs for Test Case 3\n    case3_spec = test_cases_spec[2]\n    rng_build = np.random.default_rng(case3_spec[\"build_seed\"])\n    g_indices = rng_build.integers(0, case3_spec[\"n\"], size=case3_spec[\"s_gen\"])\n    g_weights = rng_build.exponential(scale=1.0, size=case3_spec[\"s_gen\"])\n    case3_spec[\"pairs\"] = list(zip(g_indices, g_weights))\n\n    results = []\n    for case in test_cases_spec:\n        # Initialize the sampler from the specification\n        sampler = SparseITSampler(case[\"pairs\"], case[\"n\"])\n        \n        # 1. s_star: number of distinct indices\n        s_star = sampler.s_star\n        \n        # 2. ceil(log2(s_star)): theoretical binary search depth\n        log2_s_star = 0\n        if s_star > 0:\n            # (s_star - 1).bit_length() is an efficient integer alternative\n            log2_s_star = math.ceil(math.log2(s_star))\n\n        # Calculate theoretical moments\n        mu, m2 = sampler.theoretical_moments()\n        \n        # Generate samples\n        samples = sampler.sample(case[\"m\"], case[\"seed\"])\n        \n        # Calculate empirical moments from samples\n        hat_mu = np.mean(samples)\n        # Cast to float64 before squaring to prevent potential overflow with large indices\n        hat_m2 = np.mean(samples.astype(np.float64)**2)\n\n        # 3. E1: relative error of the mean\n        e1 = abs(hat_mu - mu) / abs(mu) if mu != 0 else (0.0 if hat_mu == 0 else float('inf'))\n\n        # 4. E2: relative error of the second moment\n        e2 = abs(hat_m2 - m2) / abs(m2) if m2 != 0 else (0.0 if hat_m2 == 0 else float('inf'))\n\n        # Format results for the current test case\n        case_result = f\"[{s_star},{log2_s_star},{e1:.6f},{e2:.6f}]\"\n        results.append(case_result)\n    \n    # Print the final output in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}