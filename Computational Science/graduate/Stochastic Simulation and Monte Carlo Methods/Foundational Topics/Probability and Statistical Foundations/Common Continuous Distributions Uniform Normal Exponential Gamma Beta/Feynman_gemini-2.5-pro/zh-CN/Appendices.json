{
    "hands_on_practices": [
        {
            "introduction": "在应用一个概率分布之前，透彻理解其均值和方差等基本性质是至关重要的。本练习将通过第一性原理，利用 Gamma 函数和 Beta 函数的恒等式，来推导 Beta 分布的矩。这个过程不仅能巩固您对这些特殊函数运用的熟练度，还能加深您对 Beta 分布内在结构的理论认识 。",
            "id": "3296518",
            "problem": "设 $X$ 是一个定义在 $(0,1)$ 上的连续随机变量，其概率密度函数 (PDF) 为\n$$\nf_{X}(x) \\;=\\; \\frac{1}{B(a,b)} \\, x^{a-1} (1-x)^{b-1}, \\quad x \\in (0,1),\n$$\n其中 $a0$ 和 $b0$ 是形状参数，$B(a,b)$ 表示欧拉贝塔函数\n$$\nB(a,b) \\;=\\; \\int_{0}^{1} x^{a-1} (1-x)^{b-1} \\,\\mathrm{d}x,\n$$\n伽马函数 $\\Gamma(\\cdot)$ 通过恒等式 $B(a,b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$ 和函数方程 $\\Gamma(z+1) = z \\,\\Gamma(z)$ (对于 $z0$) 与 $B(\\cdot,\\cdot)$ 相关。严格从这些定义和恒等式出发，且不使用任何预先计算的矩，推导出 $X$ 的一阶原点矩 $E[X]$ 和方差 $\\operatorname{Var}(X)$ 关于 $a$ 和 $b$ 的闭式表达式。请仅用 $a$ 和 $b$ 的解析函数形式提供你的最终表达式。不需要四舍五入，你的最终答案必须是单个闭式表达式或一个包含结果的单行矩阵。",
            "solution": "该问题是有效的。它在科学上基于概率论和统计学理论，特别是关于贝塔分布。问题陈述清晰，提供了推导所需量（即期望值和方差）的所有必要定义和恒等式。问题陈述客观且无歧义。我们可以开始求解。\n\n任务是推导一个服从参数为 $a  0$ 和 $b  0$ 的贝塔分布的随机变量 $X$ 的一阶原点矩 $E[X]$ 和方差 $\\operatorname{Var}(X)$。\n\n首先，我们推导期望值 $E[X]$。根据定义，对于一个概率密度函数 (PDF) 为 $f_X(x)$ 且支撑集为 $(0, 1)$ 的连续随机变量，其期望值由下式给出：\n$$\nE[X] = \\int_{0}^{1} x \\, f_X(x) \\, \\mathrm{d}x\n$$\n代入给定的 PDF，$f_{X}(x) = \\frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1}$，我们有：\n$$\nE[X] = \\int_{0}^{1} x \\left( \\frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1} \\right) \\mathrm{d}x\n$$\n项 $\\frac{1}{B(a,b)}$ 是一个常数，可以从积分中提出：\n$$\nE[X] = \\frac{1}{B(a,b)} \\int_{0}^{1} x \\cdot x^{a-1} (1-x)^{b-1} \\, \\mathrm{d}x\n$$\n合并积分内的 $x$ 的幂次：\n$$\nE[X] = \\frac{1}{B(a,b)} \\int_{0}^{1} x^{a} (1-x)^{b-1} \\, \\mathrm{d}x\n$$\n我们可以将 $x$ 的指数重写为 $a = (a+1) - 1$ 以匹配欧拉贝塔函数定义 $B(p,q) = \\int_{0}^{1} u^{p-1}(1-u)^{q-1} \\mathrm{d}u$ 的形式。\n$$\nE[X] = \\frac{1}{B(a,b)} \\int_{0}^{1} x^{(a+1)-1} (1-x)^{b-1} \\, \\mathrm{d}x\n$$\n根据定义，该积分现在是贝塔函数 $B(a+1, b)$。\n$$\nE[X] = \\frac{B(a+1, b)}{B(a,b)}\n$$\n现在，我们使用给定的恒等式 $B(a,b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$ 将此比率用伽马函数表示。\n$$\nE[X] = \\frac{\\frac{\\Gamma(a+1)\\Gamma(b)}{\\Gamma(a+1+b)}}{\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}} = \\frac{\\Gamma(a+1)\\Gamma(b)}{\\Gamma(a+b+1)} \\cdot \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\n$$\n$\\Gamma(b)$ 项相互抵消，得到：\n$$\nE[X] = \\frac{\\Gamma(a+1)}{\\Gamma(a)} \\cdot \\frac{\\Gamma(a+b)}{\\Gamma(a+b+1)}\n$$\n使用函数方程 $\\Gamma(z+1) = z\\Gamma(z)$，我们可以简化伽马函数的比率：\n$\\Gamma(a+1) = a\\Gamma(a)$，所以 $\\frac{\\Gamma(a+1)}{\\Gamma(a)} = a$。\n$\\Gamma(a+b+1) = (a+b)\\Gamma(a+b)$，所以 $\\frac{\\Gamma(a+b)}{\\Gamma(a+b+1)} = \\frac{1}{a+b}$。\n将这些代回 $E[X]$ 的表达式中：\n$$\nE[X] = a \\cdot \\frac{1}{a+b} = \\frac{a}{a+b}\n$$\n\n接下来，我们推导方差 $\\operatorname{Var}(X)$。方差定义为 $\\operatorname{Var}(X) = E[X^2] - (E[X])^2$。我们已经求出了 $E[X]$，所以我们需要计算二阶原点矩 $E[X^2]$。\n$$\nE[X^2] = \\int_{0}^{1} x^2 f_X(x) \\, \\mathrm{d}x = \\int_{0}^{1} x^2 \\left( \\frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1} \\right) \\mathrm{d}x\n$$\n$$\nE[X^2] = \\frac{1}{B(a,b)} \\int_{0}^{1} x^{a+1} (1-x)^{b-1} \\, \\mathrm{d}x\n$$\n将 $x$ 的指数重写为 $a+1 = (a+2)-1$，我们认出该积分为 $B(a+2, b)$。\n$$\nE[X^2] = \\frac{B(a+2, b)}{B(a,b)}\n$$\n再次，我们将其转换为使用伽马函数的表达式：\n$$\nE[X^2] = \\frac{\\frac{\\Gamma(a+2)\\Gamma(b)}{\\Gamma(a+2+b)}}{\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}} = \\frac{\\Gamma(a+2)}{\\Gamma(a)} \\cdot \\frac{\\Gamma(a+b)}{\\Gamma(a+b+2)}\n$$\n重复使用函数方程 $\\Gamma(z+1)=z\\Gamma(z)$：\n$\\Gamma(a+2) = (a+1)\\Gamma(a+1) = (a+1)a\\Gamma(a)$，所以 $\\frac{\\Gamma(a+2)}{\\Gamma(a)} = (a+1)a$。\n$\\Gamma(a+b+2) = (a+b+1)\\Gamma(a+b+1) = (a+b+1)(a+b)\\Gamma(a+b)$，所以 $\\frac{\\Gamma(a+b)}{\\Gamma(a+b+2)} = \\frac{1}{(a+b+1)(a+b)}$。\n代入这些结果：\n$$\nE[X^2] = a(a+1) \\cdot \\frac{1}{(a+b)(a+b+1)} = \\frac{a(a+1)}{(a+b)(a+b+1)}\n$$\n现在我们可以计算方差：\n$$\n\\operatorname{Var}(X) = E[X^2] - (E[X])^2 = \\frac{a(a+1)}{(a+b)(a+b+1)} - \\left(\\frac{a}{a+b}\\right)^2\n$$\n$$\n\\operatorname{Var}(X) = \\frac{a(a+1)}{(a+b)(a+b+1)} - \\frac{a^2}{(a+b)^2}\n$$\n为了合并这些分数，我们找到公分母，即 $(a+b)^2(a+b+1)$：\n$$\n\\operatorname{Var}(X) = \\frac{a(a+1)(a+b)}{(a+b)^2(a+b+1)} - \\frac{a^2(a+b+1)}{(a+b)^2(a+b+1)}\n$$\n$$\n\\operatorname{Var}(X) = \\frac{a(a+1)(a+b) - a^2(a+b+1)}{(a+b)^2(a+b+1)}\n$$\n我们展开分子中的项：\n$$\na(a+1)(a+b) = a(a^2 + ab + a + b) = a^3 + a^2b + a^2 + ab\n$$\n$$\na^2(a+b+1) = a^3 + a^2b + a^2\n$$\n用第一个展开式减去第二个展开式：\n$$\n\\text{分子} = (a^3 + a^2b + a^2 + ab) - (a^3 + a^2b + a^2) = ab\n$$\n因此，方差为：\n$$\n\\operatorname{Var}(X) = \\frac{ab}{(a+b)^2(a+b+1)}\n$$\n推导出的 $E[X]$ 和 $\\operatorname{Var}(X)$ 的表达式仅含 $a$ 和 $b$，符合要求。\n结果是 $E[X] = \\frac{a}{a+b}$ 和 $\\operatorname{Var}(X) = \\frac{ab}{(a+b)^2(a+b+1)}$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{a}{a+b}  \\frac{ab}{(a+b)^{2}(a+b+1)} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "将理论知识付诸实践是掌握统计建模的关键。一个核心任务便是从观测数据中估计分布的参数。本练习将指导您为 Gamma 分布的形状参数 $k$ 推导并实现最大似然估计（MLE），这需要应用牛顿法进行数值求解。通过这个练习，您将体会到统计理论（如得分函数和 Hessian 矩阵）与数值优化算法之间的紧密联系 。",
            "id": "3296511",
            "problem": "考虑从一个伽马分布中抽取的独立同分布观测值 $x_1,\\dots,x_n$，该分布的形状参数为 $k$，尺度参数 $\\theta$ 已知，其概率密度函数为 $f(x\\mid k,\\theta) = \\dfrac{x^{k-1} e^{-x/\\theta}}{\\Gamma(k)\\,\\theta^{k}}$，其中 $x0$，$k0$ 且 $\\theta0$。你的任务是从基本定义和性质出发，为在 $\\theta$ 已知时估计形状参数 $k$ 构建完整的最大似然估计过程，然后将其实现为一个程序，用以计算若干测试用例的估计值。\n\n你必须仅从以下基础出发：伽马分布的定义、对数似然函数的定义，以及伽马函数的对数 $\\ln\\Gamma(\\cdot)$ 及其导数的公认性质。你不能假定任何用于得分、Hessian矩阵或更新规则的预先推导出的表达式。\n\n按顺序执行以下操作：\n\n- 推导已知 $\\theta$ 时数据 $x_1,\\dots,x_n$ 的对数似然 $\\ell(k)$。\n- 推导得分函数 $S(k) = \\dfrac{\\partial}{\\partial k}\\ell(k)$，并用digamma函数 $\\psi(k)$ 表示，其中 $\\psi(k) = \\dfrac{d}{dk}\\ln\\Gamma(k)$。\n- 推导二阶导数 $H(k) = \\dfrac{\\partial^2}{\\partial k^2}\\ell(k)$，并用trigamma函数 $\\psi_1(k)$ 表示，其中 $\\psi_1(k) = \\dfrac{d}{dk}\\psi(k)$。\n- 证明最大似然估计量 $\\widehat{k}$ 满足 $S(\\widehat{k})=0$，并概述一个形式为 $k_{\\text{new}} = k_{\\text{old}} - \\dfrac{S(k_{\\text{old}})}{H(k_{\\text{old}})}$ 的牛顿迭代法，明确地用 $\\psi(\\cdot)$ 和 $\\psi_1(\\cdot)$ 来表示 $S(\\cdot)$ 和 $H(\\cdot)$，并确保在整个迭代过程中 $k$ 保持在 $(0,\\infty)$ 区间内。\n\n实现要求：\n\n- 实现一个函数，该函数在给定正数据向量 $(x_1,\\dots,x_n)$ 和已知 $\\theta0$ 的情况下，通过牛顿法计算最大似然估计值 $\\widehat{k}$，并在必要时使用步长衰减来强制执行 $k$ 的正性约束。\n- 使用一个停止准则，该准则基于绝对牛顿步长或绝对得分值小于容差 $\\varepsilon$，其中 $\\varepsilon = 10^{-10}$。\n- 最多使用 $100$ 次迭代；如果方法在此预算内未能收敛，则返回最后一次的迭代值。\n- 为保证数值稳定性，如果一个建议的牛顿更新会导致非正的形状参数，则对步长重复应用 $1/2$ 的回溯因子，直到更新后的迭代值为正。\n- 初始猜测值应为 $k^{(0)} = \\max\\{ \\overline{x}/\\theta, 10^{-6} \\}$，其中 $\\overline{x}$ 是样本均值。\n\n测试套件：\n\n计算以下五个案例的 $\\widehat{k}$。在每个案例中，程序应在输出聚合前将最终估计值四舍五入到 $10$ 位小数。\n\n$\\,\\,$\n- 案例 A: $\\theta = 2.0$, $n=10$, 数据 $x^{(A)} = (3.2,4.7,5.1,1.8,6.4,2.9,7.2,3.5,4.0,5.6)$。\n$\\,\\,$\n- 案例 B: $\\theta = 1.0$, $n=10$, 数据 $x^{(B)} = (0.15,0.32,0.08,1.5,0.42,2.3,0.27,0.65,3.0,0.55)$。\n$\\,\\,$\n- 案例 C: $\\theta = 0.5$, $n=12$, 数据 $x^{(C)} = (18.2,19.7,21.3,20.5,22.1,19.9,20.2,18.8,21.0,20.7,19.5,20.1)$。\n$\\,\\,$\n- 案例 D: $\\theta = 3.0$, $n=1$, 数据 $x^{(D)} = (4.5)$。\n$\\,\\,$\n- 案例 E: $\\theta = 0.8$, $n=5$, 数据 $x^{(E)} = (0.05,0.12,0.03,0.20,0.18)$。\n\n最终输出格式：\n\n你的程序应产生单行输出，其中包含五个估计的形状参数，按A、B、C、D、E的顺序排列，形式为用方括号括起来的逗号分隔列表，如 $[r_A,r_B,r_C,r_D,r_E]$。每个 $r_\\cdot$ 都必须是四舍五入到 $10$ 位小数的浮点数。不允许有其他输出。",
            "solution": "该问题要求在尺度参数 $\\theta$ 已知的假设下，推导并实现伽马分布形状参数 $k$ 的最大似然估计（MLE）。我们将按照要求从第一性原理出发。\n\n设 $x_1, \\dots, x_n$ 是来自伽马分布 $\\text{Gamma}(k, \\theta)$ 的独立同分布（i.i.d.）观测值，其概率密度函数（PDF）由下式给出：\n$$f(x \\mid k, \\theta) = \\frac{x^{k-1} e^{-x/\\theta}}{\\Gamma(k) \\theta^k}$$\n其中 $x  0$，形状参数 $k  0$，且已知的尺度参数 $\\theta  0$。\n\n**1. 对数似然函数 $\\ell(k)$ 的推导**\n\n似然函数 $L(k)$ 是在给定参数 $k$ 的情况下观测到数据 $x_1, \\dots, x_n$ 的联合概率。根据独立同分布的假设，这是各个密度的乘积：\n$$L(k) = \\prod_{i=1}^n f(x_i \\mid k, \\theta) = \\prod_{i=1}^n \\frac{x_i^{k-1} e^{-x_i/\\theta}}{\\Gamma(k) \\theta^k}$$\n为了简化最大化问题，我们使用对数似然函数 $\\ell(k) = \\ln L(k)$。\n$$\\ell(k) = \\ln \\left( \\prod_{i=1}^n \\frac{x_i^{k-1} e^{-x_i/\\theta}}{\\Gamma(k) \\theta^k} \\right) = \\sum_{i=1}^n \\ln \\left( \\frac{x_i^{k-1} e^{-x_i/\\theta}}{\\Gamma(k) \\theta^k} \\right)$$\n利用对数的性质，我们展开单个观测值的贡献表达式：\n$$\\ln f(x_i \\mid k, \\theta) = \\ln(x_i^{k-1}) + \\ln(e^{-x_i/\\theta}) - \\ln(\\Gamma(k)) - \\ln(\\theta^k)$$\n$$\\ln f(x_i \\mid k, \\theta) = (k-1)\\ln x_i - \\frac{x_i}{\\theta} - \\ln\\Gamma(k) - k\\ln\\theta$$\n对所有 $n$ 个观测值求和，我们得到总的对数似然：\n$$\\ell(k) = \\sum_{i=1}^n \\left( (k-1)\\ln x_i - \\frac{x_i}{\\theta} - \\ln\\Gamma(k) - k\\ln\\theta \\right)$$\n我们可以将各项分组：\n$$\\ell(k) = (k-1)\\sum_{i=1}^n \\ln x_i - \\frac{1}{\\theta}\\sum_{i=1}^n x_i - n\\ln\\Gamma(k) - nk\\ln\\theta$$\n这就是在尺度参数 $\\theta$ 已知时，形状参数 $k$ 的对数似然函数。\n\n**2. 得分函数 $S(k)$ 的推导**\n\n得分函数是对数似然函数关于参数 $k$ 的一阶导数。它表示为 $S(k) = \\frac{\\partial \\ell(k)}{\\partial k}$。我们对 $\\ell(k)$ 逐项关于 $k$ 求导：\n$$S(k) = \\frac{\\partial}{\\partial k} \\left( (k-1)\\sum_{i=1}^n \\ln x_i - \\frac{1}{\\theta}\\sum_{i=1}^n x_i - n\\ln\\Gamma(k) - nk\\ln\\theta \\right)$$\n各项的导数是：\n- $\\frac{\\partial}{\\partial k} \\left( (k-1)\\sum_{i=1}^n \\ln x_i \\right) = \\sum_{i=1}^n \\ln x_i$\n- $\\frac{\\partial}{\\partial k} \\left( -\\frac{1}{\\theta}\\sum_{i=1}^n x_i \\right) = 0$ (因为此项不依赖于 $k$)\n- $\\frac{\\partial}{\\partial k} \\left( -n\\ln\\Gamma(k) \\right) = -n \\frac{d}{dk}\\ln\\Gamma(k) = -n\\psi(k)$，其中 $\\psi(k) = \\frac{\\Gamma'(k)}{\\Gamma(k)}$ 是digamma函数。\n- $\\frac{\\partial}{\\partial k} \\left( -nk\\ln\\theta \\right) = -n\\ln\\theta$\n\n结合这些结果，得分函数为：\n$$S(k) = \\sum_{i=1}^n \\ln x_i - n\\psi(k) - n\\ln\\theta$$\n\n**3. 二阶导数 $H(k)$ 的推导**\n\n对数似然函数的二阶导数（在多参数情况下有时称为Hessian矩阵）是 $H(k) = \\frac{\\partial^2 \\ell(k)}{\\partial k^2} = \\frac{\\partial S(k)}{\\partial k}$。对得分函数 $S(k)$ 关于 $k$ 求导：\n$$H(k) = \\frac{\\partial}{\\partial k} \\left( \\sum_{i=1}^n \\ln x_i - n\\psi(k) - n\\ln\\theta \\right)$$\n唯一依赖于 $k$ 的项是 $-n\\psi(k)$。其导数为：\n- $\\frac{\\partial}{\\partial k} \\left( -n\\psi(k) \\right) = -n \\frac{d\\psi(k)}{dk} = -n\\psi_1(k)$，其中 $\\psi_1(k)$ 是trigamma函数。\n\n因此，二阶导数为：\n$$H(k) = -n\\psi_1(k)$$\n\n**4. 最大似然估计量与牛顿法**\n\n$k$ 的最大似然估计量（MLE），记作 $\\widehat{k}$，是使 $\\ell(k)$ 最大化的 $k$ 值。这可以通过将得分函数设为零来找到：\n$$S(\\widehat{k}) = \\sum_{i=1}^n \\ln x_i - n\\psi(\\widehat{k}) - n\\ln\\theta = 0$$\n这个方程无法以闭合形式求解 $\\widehat{k}$，因此需要数值方法。牛顿法是一种高效的求根迭代过程。迭代公式如下：\n$k_{\\text{new}} = k_{\\text{old}} - \\frac{S(k_{\\text{old}})}{H(k_{\\text{old}})}$\ntrigamma函数 $\\psi_1(k)$ 对所有 $k > 0$ 均为正。因此，$H(k) = -n\\psi_1(k)  0$，这意味着对数似然函数 $\\ell(k)$ 是严格凹的。这是一个关键性质，因为它保证了通过求解 $S(k)=0$ 找到的任何驻点都是唯一的全局最大值点。\n\n代入我们推导出的 $S(k)$ 和 $H(k)$ 的表达式：\n$$k_{\\text{new}} = k_{\\text{old}} - \\frac{\\sum_{i=1}^n \\ln x_i - n\\psi(k_{\\text{old}}) - n\\ln\\theta}{-n\\psi_1(k_{\\text{old}})}$$\n$$k_{\\text{new}} = k_{\\text{old}} + \\frac{\\sum_{i=1}^n \\ln x_i - n\\psi(k_{\\text{old}}) - n\\ln\\theta}{n\\psi_1(k_{\\text{old}})}$$\n\n参数 $k$ 必须保持在其定义域 $(0, \\infty)$ 内。如果一个牛顿步长 $k_{\\text{new}} = k_{\\text{old}} - S(k_{\\text{old}})/H(k_{\\text{old}})$ 导致 $k_{\\text{new}} \\le 0$，则说明步长过大。我们引入步长衰减（一种回溯线搜索）来防止这种情况。设建议的步长为 $\\Delta k = -S(k_{\\text{old}})/H(k_{\\text{old}})$。我们寻找一个因子 $\\alpha \\in (0, 1]$ 使得 $k_{\\text{old}} + \\alpha \\Delta k > 0$。该过程从 $\\alpha = 1$ 开始，并重复将其减半（$\\alpha \\leftarrow \\alpha/2$），直到满足正性约束。更新就变为 $k_{\\text{new}} = k_{\\text{old}} + \\alpha \\Delta k$。\n\n迭代从一个初始猜测值 $k^{(0)}$ 开始。一个合理的选择是矩估计量 $k^{(0)} = \\overline{x}/\\theta$，其中 $\\overline{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$。为确保 $k^{(0)} > 0$，我们使用 $k^{(0)} = \\max\\{\\overline{x}/\\theta, 10^{-6}\\}$。迭代持续进行，直到所施加步长的大小 $|\\alpha\\Delta k|$ 小于容差 $\\varepsilon=10^{-10}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import digamma, polygamma\n\ndef estimate_k_mle(data: np.ndarray, theta: float) - float:\n    \"\"\"\n    Computes the Maximum Likelihood Estimate for the shape parameter k of a\n    Gamma distribution with known scale parameter theta.\n\n    Args:\n        data: A numpy array of observations.\n        theta: The known scale parameter.\n\n    Returns:\n        The MLE of the shape parameter k.\n    \"\"\"\n    # Define constants for the iterative solver\n    TOLERANCE = 1e-10\n    MAX_ITER = 100\n\n    # Pre-calculate sufficient statistics\n    n = float(len(data))\n    if n == 0:\n        raise ValueError(\"Data cannot be empty.\")\n    \n    sample_mean = np.mean(data)\n    sum_log_data = np.sum(np.log(data))\n\n    # Initial guess for k using method of moments, ensuring positivity\n    k = max(sample_mean / theta, 1e-6)\n    \n    for _ in range(MAX_ITER):\n        # Evaluate the score and Hessian at the current k\n        psi_k = digamma(k)\n        psi1_k = polygamma(1, k)\n        \n        # Score function: S(k) = sum(log(x_i)) - n*psi(k) - n*log(theta)\n        score = sum_log_data - n * psi_k - n * np.log(theta)\n        \n        # Second derivative (Hessian): H(k) = -n*psi_1(k)\n        hessian = -n * psi1_k\n        \n        # Newton-Raphson step calculation\n        if hessian == 0:  # Avoid division by zero\n            break\n        step = -score / hessian\n        \n        # Step damping (backtracking) to ensure k remains positive\n        alpha = 1.0\n        while k + alpha * step = 0:\n            alpha /= 2.0\n            if alpha  1e-8:  # Failsafe for backtracking\n                step = 0.\n                break\n        \n        # Update k with the damped step\n        actual_step = alpha * step\n        k += actual_step\n        \n        # Convergence check on the magnitude of the actual step\n        if np.abs(actual_step)  TOLERANCE:\n            break\n            \n    return k\n\ndef solve():\n    \"\"\"\n    Defines test cases, computes the MLE for each, and prints the results\n    in the specified format.\n    \"\"\"\n    test_cases = [\n        (2.0, np.array([3.2, 4.7, 5.1, 1.8, 6.4, 2.9, 7.2, 3.5, 4.0, 5.6])), # Case A\n        (1.0, np.array([0.15, 0.32, 0.08, 1.5, 0.42, 2.3, 0.27, 0.65, 3.0, 0.55])), # Case B\n        (0.5, np.array([18.2, 19.7, 21.3, 20.5, 22.1, 19.9, 20.2, 18.8, 21.0, 20.7, 19.5, 20.1])), # Case C\n        (3.0, np.array([4.5])), # Case D\n        (0.8, np.array([0.05, 0.12, 0.03, 0.20, 0.18])) # Case E\n    ]\n    \n    results = []\n    for theta, data in test_cases:\n        k_hat = estimate_k_mle(data, theta)\n        # Round the final estimate to 10 decimal places\n        results.append(round(k_hat, 10))\n        \n    # Format the output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了单个分布的性质和参数估计之后，我们可以探索它们在高级方法中的应用。本练习聚焦于将 Beta 分布作为核心工具，来设计一种用于蒙特卡洛积分的分层抽样方案。这个过程将展示如何利用对分布累积分布函数（CDF）的深刻理解，来构建强大的方差缩减技术，从而显著提升模拟算法的效率 。",
            "id": "3296497",
            "problem": "考虑一个由 Beta 核加权的函数积分，其定义为实数参数 $a0$ 和 $b0$ 以及一个可测函数 $g:[0,1]\\to\\mathbb{R}$，\n$$\nI(a,b;g)\\;=\\;\\int_0^1 x^{a-1}(1-x)^{b-1} \\, g(x) \\, dx.\n$$\n令 $X$ 是一个服从 $\\mathrm{Beta}(a,b)$ 分布的随机变量，其概率密度函数为\n$$\nf_{a,b}(x)\\;=\\;\\frac{x^{a-1}(1-x)^{b-1}}{B(a,b)} \\quad \\text{for } x\\in(0,1),\n$$\n其中 $B(a,b)$ 是 Beta 函数，定义为\n$$\nB(a,b)\\;=\\;\\int_0^1 x^{a-1}(1-x)^{b-1}\\,dx.\n$$\n观察可知\n$$\nI(a,b;g)\\;=\\;B(a,b)\\,\\mathbb{E}\\!\\left[g(X)\\right].\n$$\n您将为 $I(a,b;g)$ 设计一个在 $[0,1]$ 上的分层蒙特卡洛估计器，通过抽样 $X\\sim \\mathrm{Beta}(a,b)$ 并根据 Beta 概率密度函数，通过累积分布函数（CDF）导出的等概率分层来优化分层宽度。具体来说，使用 Beta-CDF 分位数将 $[0,1]$ 划分为 $L$ 个层：\n$$\nq_i \\;=\\; F_{a,b}^{-1}\\!\\left(\\frac{i}{L}\\right), \\quad i=0,1,\\dots,L,\n$$\n其中 $F_{a,b}$ 是 $\\mathrm{Beta}(a,b)$ 的累积分布函数。在每个层 $S_i=(q_{i-1},q_i]$ 内，根据条件分布 $X\\mid X\\in S_i$ 抽取 $n_i$ 个样本，这可以通过在受限的 CDF 区间上使用逆变换采样来实现。构建分层估计器\n$$\n\\widehat{I}_{\\mathrm{strat}} \\;=\\; B(a,b)\\,\\sum_{i=1}^L p_i \\,\\overline{g}_i,\n$$\n其中 $p_i=\\mathbb{P}(X\\in S_i)$ 且 $\\overline{g}_i$ 是在层 $S_i$ 内 $g(X)$ 的样本均值。对于等概率分层，对所有 $i$ 都有 $p_i = 1/L$。\n\n任务：\n- 从第一性原理出发，推导限制在 $[F_{a,b}(q_{i-1}), F_{a,b}(q_i)]$ 上的逆变换采样能从条件分布 $X\\mid X\\in S_i$ 生成样本，并证明 $\\widehat{I}_{\\mathrm{strat}}$ 的无偏性。\n- 使用全方差定律，获得 $\\widehat{I}_{\\mathrm{strat}}$ 的方差表达式，并论证对于相等的样本数量 $n_i=n/L$ 和局部平滑的 $g$，通过选择等 Beta 概率 $p_i=1/L$ 的分层，在层内方差可比的近似下，利用平方和约束 $\\sum_{i=1}^L p_i=1$ 的凸性，可以最小化方差的主导项。\n- 实现一个完整的程序，该程序：\n  1. 使用 Beta-CDF 分位数 $q_i$ 构建 $L$ 个等概率分层。\n  2. 在每个层 $S_i$ 中，通过逆变换采样抽取 $n/L$ 个样本：抽取 $U\\sim \\mathrm{Uniform}(i-1,i)/L$ 并设置 $X=F_{a,b}^{-1}(U)$。\n  3. 计算 $\\widehat{I}_{\\mathrm{strat}}$。\n\n数值规格：\n- 当使用三角函数时，角度必须以弧度为单位。\n- 结果必须表示为十进制数（浮点数）。\n\n测试套件：\n对每个案例使用 $L=20$ 个分层和 $n=200000$ 个总样本。对每个案例使用确定性的伪随机数生成器和不同的种子。计算并输出以下四个案例中每个案例的 $\\widehat{I}_{\\mathrm{strat}}$：\n1. 案例 1：$a=2.0$, $b=5.0$, $g(x)=\\exp(x)$。种子 $=12345$。\n2. 案例 2：$a=0.5$, $b=0.5$, $g(x)=\\sqrt{x}$。种子 $=12346$。\n3. 案例 3：$a=5.0$, $b=5.0$, $g(x)=\\sin(\\pi x)$，其中 $\\pi$ 以弧度为单位。种子 $=12347$。\n4. 案例 4：$a=2.0$, $b=3.0$, $g(x)=\\mathbf{1}\\{x\\ge 0.9\\}$，其中 $\\mathbf{1}\\{\\cdot\\}$ 表示指示函数。种子 $=12348$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如 $[r_1,r_2,r_3,r_4]$）。每个 $r_i$ 的格式必须精确到小数点后 $10$ 位。",
            "solution": "所述问题具有科学依据，提法恰当，并且是自洽的。它提出了随机模拟领域中一个关于蒙特卡洛积分方差缩减的标准而详细的任务。所有参数、方法、函数和测试案例都得到了清晰明确的定义。理论部分是蒙特卡洛方法领域的标准证明。因此，该问题被认为是有效的，下面提供了完整的解决方案。\n\n核心任务是使用分层蒙特卡洛方法估计积分\n$$\nI(a,b;g)\\;=\\;\\int_0^1 x^{a-1}(1-x)^{b-1} \\, g(x) \\, dx\n$$\n该积分可以表示为 $g(X)$ 的期望，其中 $X$ 是一个服从 Beta 分布的随机变量，$X \\sim \\mathrm{Beta}(a,b)$。$X$ 的概率密度函数（PDF）是 $f_{a,b}(x) = \\frac{x^{a-1}(1-x)^{b-1}}{B(a,b)}$，其中 $B(a,b)$ 是 Beta 函数。因此，该积分为 $I(a,b;g) = B(a,b)\\,\\mathbb{E}[g(X)]$。该方法采用基于 Beta 分布分位数的分层，为 $i=1, \\dots, L$ 创建 $L$ 个层 $S_i = (q_{i-1}, q_i]$，其中 $q_i = F_{a,b}^{-1}(i/L)$，$F_{a,b}$ 是 $\\mathrm{Beta}(a,b)$ 分布的累积分布函数（CDF）。\n\n**1. 抽样方法的推导和估计器的无偏性**\n\n首先，我们验证从条件分布 $X\\mid X \\in S_i$ 进行抽样的提议方法。该方法是从区间 $[F_{a,b}(q_{i-1}), F_{a,b}(q_i)]$ 上的均匀分布中抽取 $U$，然后计算样本为 $X_{\\text{sample}} = F_{a,b}^{-1}(U)$。根据问题定义，$q_{i-1} = F_{a,b}^{-1}((i-1)/L)$ 且 $q_i = F_{a,b}^{-1}(i/L)$，所以 $U$ 的区间是 $[(i-1)/L, i/L]$。\n\n令 $Y$ 是对于给定层 $S_i=(q_{i-1}, q_i]$ 通过此过程生成的随机变量。对于 $y \\in S_i$，其 CDF $F_Y(y)$ 为：\n$$\nF_Y(y) = \\mathbb{P}(Y \\le y) = \\mathbb{P}(F_{a,b}^{-1}(U) \\le y) = \\mathbb{P}(U \\le F_{a,b}(y))\n$$\n因为 $U \\sim \\mathrm{Uniform}((i-1)/L, i/L)$，其 CDF 为 $\\mathbb{P}(U \\le u) = \\frac{u-(i-1)/L}{i/L - (i-1)/L} = L(u - (i-1)/L)$，对于其支撑集中的 $u$。代入 $u=F_{a,b}(y)$ 并使用 $F_{a,b}(q_{i-1}) = (i-1)/L$，我们得到：\n$$\n\\mathbb{P}(U \\le F_{a,b}(y)) = \\frac{F_{a,b}(y) - F_{a,b}(q_{i-1})}{F_{a,b}(q_i) - F_{a,b}(q_{i-1})}\n$$\n现在，考虑在事件 $X \\in S_i$ 条件下，$X \\sim \\mathrm{Beta}(a,b)$ 的 CDF。对于任何 $y \\in S_i$，此条件 CDF 为：\n$$\nF_{X|S_i}(y) = \\mathbb{P}(X \\le y \\mid X \\in S_i) = \\frac{\\mathbb{P}(X \\le y \\text{ and } q_{i-1}  X \\le q_i)}{\\mathbb{P}(q_{i-1}  X \\le q_i)}\n$$\n由于 $y > q_{i-1}$，分子中的事件是 $\\{q_{i-1}  X \\le y\\}$。此概率为 $F_{a,b}(y) - F_{a,b}(q_{i-1})$。分母中的概率为 $F_{a,b}(q_i) - F_{a,b}(q_{i-1})$。因此，\n$$\nF_{X|S_i}(y) = \\frac{F_{a,b}(y) - F_{a,b}(q_{i-1})}{F_{a,b}(q_i) - F_{a,b}(q_{i-1})}\n$$\nCDF $F_Y(y)$ 和 $F_{X|S_i}(y)$ 是相同的。这证明了限制在适当 CDF 区间上的逆变换方法能正确地从条件分布 $X\\mid X \\in S_i$ 中生成样本。\n\n接下来，我们证明分层估计器 $\\widehat{I}_{\\mathrm{strat}} = B(a,b)\\,\\sum_{i=1}^L p_i \\,\\overline{g}_i$ 是无偏的。我们取其期望：\n$$\n\\mathbb{E}[\\widehat{I}_{\\mathrm{strat}}] = \\mathbb{E}\\left[B(a,b)\\,\\sum_{i=1}^L p_i \\,\\overline{g}_i\\right] = B(a,b)\\,\\sum_{i=1}^L p_i \\,\\mathbb{E}[\\overline{g}_i]\n$$\n项 $\\overline{g}_i$ 是从条件分布 $X \\mid X \\in S_i$ 中抽取的 $n_i$ 个独立同分布样本 $X_{ij}$ 的样本均值。样本均值的期望是单个样本的期望：\n$$\n\\mathbb{E}[\\overline{g}_i] = \\mathbb{E}\\left[\\frac{1}{n_i}\\sum_{j=1}^{n_i} g(X_{ij})\\right] = \\frac{1}{n_i}\\sum_{j=1}^{n_i} \\mathbb{E}[g(X_{ij})] = \\mathbb{E}[g(X) \\mid X \\in S_i]\n$$\n将其代回，并注意到 $p_i = \\mathbb{P}(X \\in S_i)$：\n$$\n\\mathbb{E}[\\widehat{I}_{\\mathrm{strat}}] = B(a,b)\\,\\sum_{i=1}^L \\mathbb{P}(X \\in S_i) \\mathbb{E}[g(X) \\mid X \\in S_i]\n$$\n根据全期望定律，右侧的和恰好是 $\\mathbb{E}[g(X)]$。因此，\n$$\n\\mathbb{E}[\\widehat{I}_{\\mathrm{strat}}] = B(a,b)\\,\\mathbb{E}[g(X)] = I(a,b;g)\n$$\n这证实了估计器是无偏的。\n\n**2. 方差分析与优化**\n\n分层估计器的方差由下式给出：\n$$\n\\mathrm{Var}(\\widehat{I}_{\\mathrm{strat}}) = \\mathrm{Var}\\left(B(a,b)\\,\\sum_{i=1}^L p_i \\,\\overline{g}_i\\right) = (B(a,b))^2 \\mathrm{Var}\\left(\\sum_{i=1}^L p_i \\,\\overline{g}_i\\right)\n$$\n由于来自不同层的样本是独立的，和的方差是方差的和：\n$$\n\\mathrm{Var}(\\widehat{I}_{\\mathrm{strat}}) = (B(a,b))^2 \\sum_{i=1}^L p_i^2 \\mathrm{Var}(\\overline{g}_i)\n$$\n令 $\\sigma_i^2 = \\mathrm{Var}(g(X) \\mid X \\in S_i)$ 为层 $S_i$ 内 $g(X)$ 的条件方差。由于 $\\overline{g}_i$ 是 $n_i$ 个样本的均值，$\\mathrm{Var}(\\overline{g}_i) = \\sigma_i^2/n_i$。估计器的方差为：\n$$\n\\mathrm{Var}(\\widehat{I}_{\\mathrm{strat}}) = (B(a,b))^2 \\sum_{i=1}^L \\frac{p_i^2 \\sigma_i^2}{n_i}\n$$\n问题指定了每层的样本数相等，$n_i = n/L$，以及等概率分层，$p_i = 1/L$。问题要求论证选择 $p_i=1/L$ 有助于最小化方差。为将其形式化，我们固定分配规则 $n_i=n/L$，并考虑层边界的选择，这决定了 $p_i$ 和 $\\sigma_i^2$ 的值。当 $n_i=n/L$ 时，方差表达式变为：\n$$\n\\mathrm{Var}(\\widehat{I}_{\\mathrm{strat}}) = (B(a,b))^2 \\frac{L}{n} \\sum_{i=1}^L p_i^2 \\sigma_i^2\n$$\n问题引入了近似，即对于局部平滑函数 $g$ 和窄层，层内方差是可比较的，即对于所有 $i=1,\\dots,L$，$\\sigma_i^2 \\approx \\sigma^2$。在此近似下，最小化方差等同于最小化项 $\\sum_{i=1}^L p_i^2$，受约束于 $\\sum_{i=1}^L p_i = 1$ 和 $p_i \\ge 0$。\n\n这是一个经典的优化问题。使用拉格朗日乘数法，我们寻求最小化 $\\mathcal{L}(p_1, \\dots, p_L, \\lambda) = \\sum_{i=1}^L p_i^2 - \\lambda(\\sum_{i=1}^L p_i - 1)$。对 $p_k$ 求偏导数：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_k} = 2p_k - \\lambda = 0 \\quad \\implies \\quad p_k = \\frac{\\lambda}{2}\n$$\n这意味着所有 $p_k$ 必须相等。将其代入约束 $\\sum p_i = 1$ 中：\n$$\n\\sum_{i=1}^L \\frac{\\lambda}{2} = 1 \\quad \\implies \\quad L \\frac{\\lambda}{2} = 1 \\quad \\implies \\quad \\lambda = \\frac{2}{L}\n$$\n因此，最优选择是 $p_k = 1/L$。\n或者，对凸函数 $\\phi(p)=p^2$ 使用 Jensen 不等式，我们有 $\\frac{1}{L} \\sum p_i^2 \\ge (\\frac{1}{L}\\sum p_i)^2 = (\\frac{1}{L})^2$，这意味着 $\\sum p_i^2 \\ge 1/L$。当且仅当所有 $p_i$ 相等时，即 $p_i=1/L$ 时，等号成立。\n\n因此，在层内方差近似相等的简化假设下，当从每个层抽取固定数量的样本（$n_i=n/L$）时，选择等概率分层（$p_i=1/L$）可以最小化分层估计器的方差。问题中使用 Beta-CDF 分位数来定义分层的设计，恰好实现了这种等概率分层。\n\n**3. 算法实现**\n算法流程如下：\n1.  设置总分层数 $L=20$ 和总样本数 $n=200000$。每层的样本数为 $n_i = n/L = 10000$。\n2.  对于每个由参数 $a, b$、函数 $g(x)$ 和一个种子定义的测试案例：\n    a. 使用指定的种子初始化伪随机数生成器。\n    b. 初始化一个空列表以存储层样本均值 $\\overline{g}_i$。\n    c. 对每个层 $i$（从 $1$到 $L$）进行循环。\n        i. 定义概率区间 $[(i-1)/L, i/L]$。\n        ii. 从此区间生成 $n_i$ 个均匀分布的随机变量 $U_j$。\n        iii. 应用 Beta 逆 CDF（百分点函数，PPF）将这些变量转换为 $n_i$ 个随机变量 $X_j \\sim X \\mid X \\in S_i$。即 $X_j = F_{a,b}^{-1}(U_j)$。\n        iv. 将函数 $g$ 应用于每个 $X_j$ 以获得 $g(X_j)$。\n        v. 计算这些 $g(X_j)$ 值的样本均值以得到 $\\overline{g}_i$。\n        vi. 存储 $\\overline{g}_i$。\n    d. 计算 Beta 函数值 $B(a,b)$。\n    e. 计算估计器。对于等概率分层，$p_i = 1/L$，因此估计器简化为：\n       $$\n       \\widehat{I}_{\\mathrm{strat}} = B(a,b)\\,\\sum_{i=1}^L \\frac{1}{L} \\overline{g}_i = B(a,b) \\times \\left(\\frac{1}{L}\\sum_{i=1}^L \\overline{g}_i\\right)\n       $$\n       这是 Beta 函数值乘以层样本均值的平均值。\n3.  收集所有测试案例的结果并按要求格式化。\n实现将使用 `scipy.stats.beta.ppf` 作为逆 CDF，并使用 `scipy.special.beta` 作为 Beta 函数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta as beta_dist\nfrom scipy.special import beta as beta_func\n\ndef solve():\n    \"\"\"\n    Computes a stratified Monte Carlo estimate of an integral weighted by a Beta kernel.\n    \"\"\"\n    \n    # Numerical specifications from the problem statement\n    L = 20  # Number of strata\n    n = 200000  # Total number of samples\n    n_per_stratum = n // L\n    \n    # Define the test cases with parameters (a, b), function g(x), and seed.\n    test_cases = [\n        {\n            \"a\": 2.0, \"b\": 5.0, \n            \"g\": lambda x: np.exp(x), \n            \"seed\": 12345\n        },\n        {\n            \"a\": 0.5, \"b\": 0.5, \n            \"g\": lambda x: np.sqrt(x), \n            \"seed\": 12346\n        },\n        {\n            \"a\": 5.0, \"b\": 5.0, \n            \"g\": lambda x: np.sin(np.pi * x), \n            \"seed\": 12347\n        },\n        {\n            \"a\": 2.0, \"b\": 3.0, \n            \"g\": lambda x: np.where(x = 0.9, 1.0, 0.0), \n            \"seed\": 12348\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        a = case[\"a\"]\n        b = case[\"b\"]\n        g_func = case[\"g\"]\n        seed = case[\"seed\"]\n        \n        # Use a deterministic pseudorandom generator with the specified seed.\n        rng = np.random.default_rng(seed)\n        \n        stratum_means = []\n        \n        # Loop over each of the L strata\n        for i in range(1, L + 1):\n            # Define the probability interval for the current stratum.\n            # This creates equal-probability strata.\n            prob_lower_bound = (i - 1) / L\n            prob_upper_bound = i / L\n            \n            # 1. Draw n_per_stratum samples from Uniform(prob_lower_bound, prob_upper_bound).\n            u_samples = rng.uniform(prob_lower_bound, prob_upper_bound, size=n_per_stratum)\n            \n            # 2. Use inverse transform sampling to get samples from the conditional Beta distribution.\n            # The inverse CDF is the percent-point function (ppf).\n            x_samples = beta_dist.ppf(u_samples, a, b)\n            \n            # 3. Apply the function g(x) to the samples.\n            g_values = g_func(x_samples)\n            \n            # 4. Compute the sample mean for the stratum.\n            g_bar_i = np.mean(g_values)\n            stratum_means.append(g_bar_i)\n\n        # The stratified estimator for E[g(X)] is the mean of the stratum means\n        # because all strata have equal probability (p_i = 1/L).\n        # E_strat[g(X)] = sum(p_i * g_bar_i) = sum((1/L) * g_bar_i) = mean(g_bar_i)\n        estimated_expectation = np.mean(stratum_means)\n        \n        # The integral I is B(a,b) * E[g(X)].\n        beta_val = beta_func(a, b)\n        \n        I_strat = beta_val * estimated_expectation\n        results.append(I_strat)\n\n    # Final print statement in the exact required format.\n    # Each result r_i must be formatted to exactly 10 digits after the decimal point.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n\n```"
        }
    ]
}