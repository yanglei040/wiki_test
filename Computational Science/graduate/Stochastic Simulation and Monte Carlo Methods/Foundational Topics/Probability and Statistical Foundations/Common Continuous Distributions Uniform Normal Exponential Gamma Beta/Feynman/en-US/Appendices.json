{
    "hands_on_practices": [
        {
            "introduction": "A deep understanding of a probability distribution goes beyond simply recognizing its probability density function (PDF). This practice focuses on the fundamental skill of deriving a distribution's key properties, such as its mean and variance, directly from its definition. By working through the Beta distribution and utilizing the integral identities of the Beta and Gamma functions, you will sharpen your analytical abilities and gain a more profound appreciation for the mathematical structure that underpins these essential statistical tools.",
            "id": "3296518",
            "problem": "Let $X$ be a continuous random variable supported on $(0,1)$ with probability density function (PDF) given by\n$$\nf_{X}(x) \\;=\\; \\frac{1}{B(a,b)} \\, x^{a-1} (1-x)^{b-1}, \\quad x \\in (0,1),\n$$\nwhere $a0$ and $b0$ are shape parameters, and $B(a,b)$ denotes the Euler Beta function\n$$\nB(a,b) \\;=\\; \\int_{0}^{1} x^{a-1} (1-x)^{b-1} \\,\\mathrm{d}x,\n$$\nwith the Gamma function $\\Gamma(\\cdot)$ related to $B(\\cdot,\\cdot)$ by the identity $B(a,b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$ and the functional equation $\\Gamma(z+1) = z \\,\\Gamma(z)$ for $z0$. Starting strictly from these definitions and identities, and without invoking pre-tabulated moments, derive closed-form expressions in terms of $a$ and $b$ for the first raw moment $E[X]$ and the variance $\\operatorname{Var}(X)$ of $X$. Provide your final expressions as analytic functions of $a$ and $b$ only. No rounding is required, and your final answer must be a single closed-form expression or a single row matrix collecting your results.",
            "solution": "The problem is valid. It is scientifically grounded in the theory of probability and statistics, specifically concerning the Beta distribution. It is well-posed, providing all necessary definitions and identities to derive the required quantities, namely the expected value and variance. The problem statement is objective and unambiguous. We may proceed with the solution.\n\nThe task is to derive the first raw moment, $E[X]$, and the variance, $\\operatorname{Var}(X)$, for a random variable $X$ following a Beta distribution with parameters $a  0$ and $b  0$.\n\nFirst, we derive the expected value, $E[X]$. By definition, for a continuous random variable with probability density function (PDF) $f_X(x)$ and support $(0, 1)$, the expected value is given by:\n$$\nE[X] = \\int_{0}^{1} x \\, f_X(x) \\, \\mathrm{d}x\n$$\nSubstituting the given PDF, $f_{X}(x) = \\frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1}$, we have:\n$$\nE[X] = \\int_{0}^{1} x \\left( \\frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1} \\right) \\mathrm{d}x\n$$\nThe term $\\frac{1}{B(a,b)}$ is a constant and can be factored out of the integral:\n$$\nE[X] = \\frac{1}{B(a,b)} \\int_{0}^{1} x \\cdot x^{a-1} (1-x)^{b-1} \\, \\mathrm{d}x\n$$\nCombining the powers of $x$ inside the integral:\n$$\nE[X] = \\frac{1}{B(a,b)} \\int_{0}^{1} x^{a} (1-x)^{b-1} \\, \\mathrm{d}x\n$$\nWe can rewrite the exponent of $x$ as $a = (a+1) - 1$ to match the form of the Euler Beta function definition, $B(p,q) = \\int_{0}^{1} u^{p-1}(1-u)^{q-1} \\mathrm{d}u$.\n$$\nE[X] = \\frac{1}{B(a,b)} \\int_{0}^{1} x^{(a+1)-1} (1-x)^{b-1} \\, \\mathrm{d}x\n$$\nThe integral is now, by definition, the Beta function $B(a+1, b)$.\n$$\nE[X] = \\frac{B(a+1, b)}{B(a,b)}\n$$\nNow, we use the provided identity $B(a,b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$ to express this ratio in terms of Gamma functions.\n$$\nE[X] = \\frac{\\frac{\\Gamma(a+1)\\Gamma(b)}{\\Gamma(a+1+b)}}{\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}} = \\frac{\\Gamma(a+1)\\Gamma(b)}{\\Gamma(a+b+1)} \\cdot \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\n$$\nThe $\\Gamma(b)$ terms cancel out, yielding:\n$$\nE[X] = \\frac{\\Gamma(a+1)}{\\Gamma(a)} \\cdot \\frac{\\Gamma(a+b)}{\\Gamma(a+b+1)}\n$$\nUsing the functional equation $\\Gamma(z+1) = z\\Gamma(z)$, we can simplify the ratios of Gamma functions:\n$\\Gamma(a+1) = a\\Gamma(a)$, so $\\frac{\\Gamma(a+1)}{\\Gamma(a)} = a$.\n$\\Gamma(a+b+1) = (a+b)\\Gamma(a+b)$, so $\\frac{\\Gamma(a+b)}{\\Gamma(a+b+1)} = \\frac{1}{a+b}$.\nSubstituting these back into the expression for $E[X]$:\n$$\nE[X] = a \\cdot \\frac{1}{a+b} = \\frac{a}{a+b}\n$$\n\nNext, we derive the variance, $\\operatorname{Var}(X)$. The variance is defined as $\\operatorname{Var}(X) = E[X^2] - (E[X])^2$. We have already found $E[X]$, so we need to compute the second raw moment, $E[X^2]$.\n$$\nE[X^2] = \\int_{0}^{1} x^2 f_X(x) \\, \\mathrm{d}x = \\int_{0}^{1} x^2 \\left( \\frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1} \\right) \\mathrm{d}x\n$$\n$$\nE[X^2] = \\frac{1}{B(a,b)} \\int_{0}^{1} x^{a+1} (1-x)^{b-1} \\, \\mathrm{d}x\n$$\nRewriting the exponent of $x$ as $a+1 = (a+2)-1$, we recognize the integral as $B(a+2, b)$.\n$$\nE[X^2] = \\frac{B(a+2, b)}{B(a,b)}\n$$\nAgain, we convert this to an expression with Gamma functions:\n$$\nE[X^2] = \\frac{\\frac{\\Gamma(a+2)\\Gamma(b)}{\\Gamma(a+2+b)}}{\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}} = \\frac{\\Gamma(a+2)}{\\Gamma(a)} \\cdot \\frac{\\Gamma(a+b)}{\\Gamma(a+b+2)}\n$$\nUsing the functional equation $\\Gamma(z+1)=z\\Gamma(z)$ repeatedly:\n$\\Gamma(a+2) = (a+1)\\Gamma(a+1) = (a+1)a\\Gamma(a)$, so $\\frac{\\Gamma(a+2)}{\\Gamma(a)} = (a+1)a$.\n$\\Gamma(a+b+2) = (a+b+1)\\Gamma(a+b+1) = (a+b+1)(a+b)\\Gamma(a+b)$, so $\\frac{\\Gamma(a+b)}{\\Gamma(a+b+2)} = \\frac{1}{(a+b+1)(a+b)}$.\nSubstituting these results:\n$$\nE[X^2] = a(a+1) \\cdot \\frac{1}{(a+b)(a+b+1)} = \\frac{a(a+1)}{(a+b)(a+b+1)}\n$$\nNow we can compute the variance:\n$$\n\\operatorname{Var}(X) = E[X^2] - (E[X])^2 = \\frac{a(a+1)}{(a+b)(a+b+1)} - \\left(\\frac{a}{a+b}\\right)^2\n$$\n$$\n\\operatorname{Var}(X) = \\frac{a(a+1)}{(a+b)(a+b+1)} - \\frac{a^2}{(a+b)^2}\n$$\nTo combine these fractions, we find a common denominator, which is $(a+b)^2(a+b+1)$:\n$$\n\\operatorname{Var}(X) = \\frac{a(a+1)(a+b)}{(a+b)^2(a+b+1)} - \\frac{a^2(a+b+1)}{(a+b)^2(a+b+1)}\n$$\n$$\n\\operatorname{Var}(X) = \\frac{a(a+1)(a+b) - a^2(a+b+1)}{(a+b)^2(a+b+1)}\n$$\nLet's expand the terms in the numerator:\n$$\na(a+1)(a+b) = a(a^2 + ab + a + b) = a^3 + a^2b + a^2 + ab\n$$\n$$\na^2(a+b+1) = a^3 + a^2b + a^2\n$$\nSubtracting the second expansion from the first:\n$$\n\\text{Numerator} = (a^3 + a^2b + a^2 + ab) - (a^3 + a^2b + a^2) = ab\n$$\nTherefore, the variance is:\n$$\n\\operatorname{Var}(X) = \\frac{ab}{(a+b)^2(a+b+1)}\n$$\nThe derived expressions for $E[X]$ and $\\operatorname{Var}(X)$ are in terms of $a$ and $b$ only, as required.\nThe results are $E[X] = \\frac{a}{a+b}$ and $\\operatorname{Var}(X) = \\frac{ab}{(a+b)^2(a+b+1)}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{a}{a+b}  \\frac{ab}{(a+b)^{2}(a+b+1)} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Theoretical knowledge of a distribution's properties finds its true power in practical applications, particularly in Monte Carlo methods. Building on the analytical understanding of the Beta distribution , this practice demonstrates how to leverage its cumulative distribution function (CDF) to design a sophisticated variance reduction technique. You will implement an equal-probability stratified sampling scheme to showcase how a theoretical construct—the inverse CDF—becomes a practical tool for improving the efficiency and accuracy of numerical integration.",
            "id": "3296497",
            "problem": "Consider the integral of a function weighted by a Beta kernel, defined for real parameters $a0$ and $b0$ and a measurable function $g:[0,1]\\to\\mathbb{R}$,\n$$\nI(a,b;g)\\;=\\;\\int_0^1 x^{a-1}(1-x)^{b-1} \\, g(x) \\, dx.\n$$\nLet $X$ be a random variable with the $\\mathrm{Beta}(a,b)$ distribution, whose probability density function is\n$$\nf_{a,b}(x)\\;=\\;\\frac{x^{a-1}(1-x)^{b-1}}{B(a,b)} \\quad \\text{for } x\\in(0,1),\n$$\nwhere $B(a,b)$ is the Beta function defined by\n$$\nB(a,b)\\;=\\;\\int_0^1 x^{a-1}(1-x)^{b-1}\\,dx.\n$$\nObserve that\n$$\nI(a,b;g)\\;=\\;B(a,b)\\,\\mathbb{E}\\!\\left[g(X)\\right].\n$$\nYou will design a stratified Monte Carlo estimator over $[0,1]$ for $I(a,b;g)$ by sampling $X\\sim \\mathrm{Beta}(a,b)$ and optimizing strata widths according to the Beta probability density function via equal-probability stratification induced by the cumulative distribution function (CDF). Specifically, partition $[0,1]$ into $L$ strata using Beta-CDF quantiles:\n$$\nq_i \\;=\\; F_{a,b}^{-1}\\!\\left(\\frac{i}{L}\\right), \\quad i=0,1,\\dots,L,\n$$\nwhere $F_{a,b}$ is the cumulative distribution function of $\\mathrm{Beta}(a,b)$. Within each stratum $S_i=(q_{i-1},q_i]$, draw $n_i$ samples according to the conditional distribution $X\\mid X\\in S_i$, which can be achieved by inverse transform sampling on the restricted CDF interval. Form the stratified estimator\n$$\n\\widehat{I}_{\\mathrm{strat}} \\;=\\; B(a,b)\\,\\sum_{i=1}^L p_i \\,\\overline{g}_i,\n$$\nwhere $p_i=\\mathbb{P}(X\\in S_i)$ and $\\overline{g}_i$ is the sample mean of $g(X)$ within stratum $S_i$. For equal-probability strata, $p_i = 1/L$ for all $i$. \n\nTasks:\n- Derive, from first principles, that inverse transform sampling restricted to $[F_{a,b}(q_{i-1}), F_{a,b}(q_i)]$ generates samples from the conditional distribution $X\\mid X\\in S_i$, and prove the unbiasedness of $\\widehat{I}_{\\mathrm{strat}}$.\n- Using the law of total variance, obtain a variance expression for $\\widehat{I}_{\\mathrm{strat}}$ and argue that, for equal sample counts $n_i=n/L$ and locally smooth $g$, choosing strata via equal Beta probabilities $p_i=1/L$ minimizes the leading term of the variance under the approximation that the within-stratum variances are comparable, by using the convexity of the sum of squares constraint $\\sum_{i=1}^L p_i=1$.\n- Implement a complete program that:\n  1. Constructs $L$ equal-probability strata using Beta-CDF quantiles $q_i$.\n  2. In each stratum $S_i$, draws $n/L$ samples via inverse transform sampling: draw $U\\sim \\mathrm{Uniform}((i-1)/L, i/L)$ and set $X=F_{a,b}^{-1}(U)$.\n  3. Computes $\\widehat{I}_{\\mathrm{strat}}$.\n\nNumerical specifications:\n- Angles must be in radians whenever trigonometric functions are used.\n- Results must be expressed as decimals (floating-point numbers).\n\nTest suite:\nUse $L=20$ strata and $n=200000$ total samples for each case. Use a deterministic pseudorandom generator with distinct seeds per case. Compute and output $\\widehat{I}_{\\mathrm{strat}}$ for each of the following four cases:\n1. Case $1$: $a=2.0$, $b=5.0$, $g(x)=\\exp(x)$. Seed $=12345$.\n2. Case $2$: $a=0.5$, $b=0.5$, $g(x)=\\sqrt{x}$. Seed $=12346$.\n3. Case $3$: $a=5.0$, $b=5.0$, $g(x)=\\sin(\\pi x)$ with $\\pi$ in radians. Seed $=12347$.\n4. Case $4$: $a=2.0$, $b=3.0$, $g(x)=\\mathbf{1}\\{x\\ge 0.9\\}$, where $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. Seed $=12348$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$). Each $r_i$ must be formatted to exactly $10$ digits after the decimal point.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and self-contained. It presents a standard, albeit detailed, task in the field of stochastic simulation concerning variance reduction for Monte Carlo integration. All parameters, methodologies, functions, and test cases are clearly and unambiguously defined. The theoretical components are standard proofs in the domain of Monte Carlo methods. Therefore, the problem is deemed valid and a full solution is provided below.\n\nThe core task is to estimate the integral\n$$\nI(a,b;g)\\;=\\;\\int_0^1 x^{a-1}(1-x)^{b-1} \\, g(x) \\, dx\n$$\nusing a stratified Monte Carlo approach. The integral can be expressed in terms of the expectation of $g(X)$ where $X$ is a random variable following the Beta distribution, $X \\sim \\mathrm{Beta}(a,b)$. The probability density function (PDF) of $X$ is $f_{a,b}(x) = \\frac{x^{a-1}(1-x)^{b-1}}{B(a,b)}$, where $B(a,b)$ is the Beta function. The integral is thus $I(a,b;g) = B(a,b)\\,\\mathbb{E}[g(X)]$. The method employs stratification based on quantiles of the Beta distribution, creating $L$ strata $S_i = (q_{i-1}, q_i]$ for $i=1, \\dots, L$, where $q_i = F_{a,b}^{-1}(i/L)$ and $F_{a,b}$ is the cumulative distribution function (CDF) of the $\\mathrm{Beta}(a,b)$ distribution.\n\n**1. Derivation of the Sampling Method and Unbiasedness of the Estimator**\n\nFirst, we validate the proposed method for sampling from the conditional distribution $X\\mid X \\in S_i$. The method is to draw $U$ from a uniform distribution on the interval $[F_{a,b}(q_{i-1}), F_{a,b}(q_i)]$ and then compute the sample as $X_{\\text{sample}} = F_{a,b}^{-1}(U)$. By the problem definition, $q_{i-1} = F_{a,b}^{-1}((i-1)/L)$ and $q_i = F_{a,b}^{-1}(i/L)$, so the interval for $U$ is $[(i-1)/L, i/L]$.\n\nLet $Y$ be a random variable generated by this procedure for a given stratum $S_i=(q_{i-1}, q_i]$. Its CDF, $F_Y(y)$, for $y \\in S_i$ is:\n$$\nF_Y(y) = \\mathbb{P}(Y \\le y) = \\mathbb{P}(F_{a,b}^{-1}(U) \\le y) = \\mathbb{P}(U \\le F_{a,b}(y))\n$$\nSince $U \\sim \\mathrm{Uniform}((i-1)/L, i/L)$, its CDF is $\\mathbb{P}(U \\le u) = \\frac{u-(i-1)/L}{i/L - (i-1)/L} = L(u - (i-1)/L)$ for $u$ in its support. Substituting $u=F_{a,b}(y)$ and using $F_{a,b}(q_{i-1}) = (i-1)/L$, we get:\n$$\n\\mathbb{P}(U \\le F_{a,b}(y)) = \\frac{F_{a,b}(y) - F_{a,b}(q_{i-1})}{F_{a,b}(q_i) - F_{a,b}(q_{i-1})}\n$$\nNow, consider the CDF of $X \\sim \\mathrm{Beta}(a,b)$ conditioned on the event $X \\in S_i$. For any $y \\in S_i$, this conditional CDF is:\n$$\nF_{X|S_i}(y) = \\mathbb{P}(X \\le y \\mid X \\in S_i) = \\frac{\\mathbb{P}(X \\le y \\text{ and } q_{i-1}  X \\le q_i)}{\\mathbb{P}(q_{i-1}  X \\le q_i)}\n$$\nSince $y  q_{i-1}$, the event in the numerator is $\\{q_{i-1}  X \\le y\\}$. This probability is $F_{a,b}(y) - F_{a,b}(q_{i-1})$. The probability in the denominator is $F_{a,b}(q_i) - F_{a,b}(q_{i-1})$. Thus,\n$$\nF_{X|S_i}(y) = \\frac{F_{a,b}(y) - F_{a,b}(q_{i-1})}{F_{a,b}(q_i) - F_{a,b}(q_{i-1})}\n$$\nThe CDFs $F_Y(y)$ and $F_{X|S_i}(y)$ are identical. This proves that the inverse transform method restricted to the appropriate CDF interval correctly generates samples from the conditional distribution $X\\mid X \\in S_i$.\n\nNext, we prove that the stratified estimator $\\widehat{I}_{\\mathrm{strat}} = B(a,b)\\,\\sum_{i=1}^L p_i \\,\\overline{g}_i$ is unbiased. We take its expectation:\n$$\n\\mathbb{E}[\\widehat{I}_{\\mathrm{strat}}] = \\mathbb{E}\\left[B(a,b)\\,\\sum_{i=1}^L p_i \\,\\overline{g}_i\\right] = B(a,b)\\,\\sum_{i=1}^L p_i \\,\\mathbb{E}[\\overline{g}_i]\n$$\nThe term $\\overline{g}_i$ is the sample mean of $n_i$ i.i.d. samples $X_{ij}$ drawn from the conditional distribution $X \\mid X \\in S_i$. The expectation of the sample mean is the expectation of a single sample:\n$$\n\\mathbb{E}[\\overline{g}_i] = \\mathbb{E}\\left[\\frac{1}{n_i}\\sum_{j=1}^{n_i} g(X_{ij})\\right] = \\frac{1}{n_i}\\sum_{j=1}^{n_i} \\mathbb{E}[g(X_{ij})] = \\mathbb{E}[g(X) \\mid X \\in S_i]\n$$\nSubstituting this back, and noting that $p_i = \\mathbb{P}(X \\in S_i)$:\n$$\n\\mathbb{E}[\\widehat{I}_{\\mathrm{strat}}] = B(a,b)\\,\\sum_{i=1}^L \\mathbb{P}(X \\in S_i) \\mathbb{E}[g(X) \\mid X \\in S_i]\n$$\nBy the law of total expectation, the sum on the right is precisely $\\mathbb{E}[g(X)]$. Therefore,\n$$\n\\mathbb{E}[\\widehat{I}_{\\mathrm{strat}}] = B(a,b)\\,\\mathbb{E}[g(X)] = I(a,b;g)\n$$\nThis confirms that the estimator is unbiased.\n\n**2. Variance Analysis and Optimization**\n\nThe variance of the stratified estimator is given by:\n$$\n\\mathrm{Var}(\\widehat{I}_{\\mathrm{strat}}) = \\mathrm{Var}\\left(B(a,b)\\,\\sum_{i=1}^L p_i \\,\\overline{g}_i\\right) = (B(a,b))^2 \\mathrm{Var}\\left(\\sum_{i=1}^L p_i \\,\\overline{g}_i\\right)\n$$\nSince samples from different strata are independent, the variance of the sum is the sum of the variances:\n$$\n\\mathrm{Var}(\\widehat{I}_{\\mathrm{strat}}) = (B(a,b))^2 \\sum_{i=1}^L p_i^2 \\mathrm{Var}(\\overline{g}_i)\n$$\nLet $\\sigma_i^2 = \\mathrm{Var}(g(X) \\mid X \\in S_i)$ be the conditional variance of $g(X)$ within stratum $S_i$. Since $\\overline{g}_i$ is a mean of $n_i$ samples, $\\mathrm{Var}(\\overline{g}_i) = \\sigma_i^2/n_i$. The variance of the estimator is:\n$$\n\\mathrm{Var}(\\widehat{I}_{\\mathrm{strat}}) = (B(a,b))^2 \\sum_{i=1}^L \\frac{p_i^2 \\sigma_i^2}{n_i}\n$$\nThe problem specifies equal sample counts per stratum, $n_i = n/L$, and equal-probability strata, $p_i = 1/L$. The question asks to argue that choosing $p_i=1/L$ helps minimize variance. To formalize this, we fix the allocation rule $n_i=n/L$ and consider the choice of strata boundaries, which determines the values of $p_i$ and $\\sigma_i^2$. With $n_i=n/L$, the variance expression becomes:\n$$\n\\mathrm{Var}(\\widehat{I}_{\\mathrm{strat}}) = (B(a,b))^2 \\frac{L}{n} \\sum_{i=1}^L p_i^2 \\sigma_i^2\n$$\nThe problem introduces the approximation that for a locally smooth function $g$ and narrow strata, the within-stratum variances are comparable, i.e., $\\sigma_i^2 \\approx \\sigma^2$ for all $i=1,\\dots,L$. Under this approximation, minimizing the variance is equivalent to minimizing the term $\\sum_{i=1}^L p_i^2$, subject to the constraint that $\\sum_{i=1}^L p_i = 1$ and $p_i \\ge 0$.\n\nThis is a classic optimization problem. Using the method of Lagrange multipliers, we seek to minimize $\\mathcal{L}(p_1, \\dots, p_L, \\lambda) = \\sum_{i=1}^L p_i^2 - \\lambda(\\sum_{i=1}^L p_i - 1)$. Taking partial derivatives with respect to $p_k$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_k} = 2p_k - \\lambda = 0 \\quad \\implies \\quad p_k = \\frac{\\lambda}{2}\n$$\nThis implies that all $p_k$ must be equal. Substituting this into the constraint $\\sum p_i = 1$:\n$$\n\\sum_{i=1}^L \\frac{\\lambda}{2} = 1 \\quad \\implies \\quad L \\frac{\\lambda}{2} = 1 \\quad \\implies \\quad \\lambda = \\frac{2}{L}\n$$\nThus, the optimal choice is $p_k = \\frac{1}{L}(\\frac{2}{L}) = 1/L$.\nAlternatively, using Jensen's inequality for the convex function $\\phi(p)=p^2$, we have $\\frac{1}{L} \\sum p_i^2 \\ge (\\frac{1}{L}\\sum p_i)^2 = (\\frac{1}{L})^2$, which implies $\\sum p_i^2 \\ge 1/L$. Equality holds if and only if all $p_i$ are equal, i.e., $p_i=1/L$.\n\nTherefore, under the simplifying assumption that within-stratum variances are approximately equal, choosing equal-probability strata ($p_i=1/L$) minimizes the variance of the stratified estimator when a fixed number of samples ($n_i=n/L$) is drawn from each stratum. The problem's design, which uses Beta-CDF quantiles to define strata, exactly implements this equal-probability stratification.\n\n**3. Algorithmic Implementation**\nThe algorithm proceeds as follows:\n1.  Set the total number of strata $L=20$ and total samples $n=200000$. The number of samples per stratum is $n_i = n/L = 10000$.\n2.  For each test case defined by parameters $a$, $b$, a function $g(x)$, and a seed:\n    a. Initialize a pseudorandom number generator with the specified seed.\n    b. Initialize an empty list to store the stratum sample means, $\\overline{g}_i$.\n    c. Loop through each stratum $i$ from $1$ to $L$.\n        i. Define the probability interval $[(i-1)/L, i/L]$.\n        ii. Generate $n_i$ uniform random variates $U_j$ from this interval.\n        iii. Apply the Beta inverse CDF (percent-point function, PPF) to transform these into $n_i$ random variates $X_j \\sim X \\mid X \\in S_i$. That is, $X_j = F_{a,b}^{-1}(U_j)$.\n        iv. Apply the function $g$ to each $X_j$ to obtain $g(X_j)$.\n        v. Compute the sample mean of these $g(X_j)$ values to get $\\overline{g}_i$.\n        vi. Store $\\overline{g}_i$.\n    d. Compute the Beta function value $B(a,b)$.\n    e. Compute the estimator. For equal probability strata, $p_i = 1/L$, so the estimator simplifies:\n       $$\n       \\widehat{I}_{\\mathrm{strat}} = B(a,b)\\,\\sum_{i=1}^L \\frac{1}{L} \\overline{g}_i = B(a,b) \\times \\left(\\frac{1}{L}\\sum_{i=1}^L \\overline{g}_i\\right)\n       $$\n       This is the Beta function value multiplied by the average of the stratum sample means.\n3.  Collect the results for all test cases and format them as required.\nThe implementation will use `scipy.stats.beta.ppf` for the inverse CDF and `scipy.special.beta` for the Beta function.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta as beta_dist\nfrom scipy.special import beta as beta_func\n\ndef solve():\n    \"\"\"\n    Computes a stratified Monte Carlo estimate of an integral weighted by a Beta kernel.\n    \"\"\"\n    \n    # Numerical specifications from the problem statement\n    L = 20  # Number of strata\n    n = 200000  # Total number of samples\n    n_per_stratum = n // L\n    \n    # Define the test cases with parameters (a, b), function g(x), and seed.\n    test_cases = [\n        {\n            \"a\": 2.0, \"b\": 5.0, \n            \"g\": lambda x: np.exp(x), \n            \"seed\": 12345\n        },\n        {\n            \"a\": 0.5, \"b\": 0.5, \n            \"g\": lambda x: np.sqrt(x), \n            \"seed\": 12346\n        },\n        {\n            \"a\": 5.0, \"b\": 5.0, \n            \"g\": lambda x: np.sin(np.pi * x), \n            \"seed\": 12347\n        },\n        {\n            \"a\": 2.0, \"b\": 3.0, \n            \"g\": lambda x: np.where(x >= 0.9, 1.0, 0.0), \n            \"seed\": 12348\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        a = case[\"a\"]\n        b = case[\"b\"]\n        g_func = case[\"g\"]\n        seed = case[\"seed\"]\n        \n        # Use a deterministic pseudorandom generator with the specified seed.\n        rng = np.random.default_rng(seed)\n        \n        stratum_means = []\n        \n        # Loop over each of the L strata\n        for i in range(1, L + 1):\n            # Define the probability interval for the current stratum.\n            # This creates equal-probability strata.\n            prob_lower_bound = (i - 1) / L\n            prob_upper_bound = i / L\n            \n            # 1. Draw n_per_stratum samples from Uniform(prob_lower_bound, prob_upper_bound).\n            u_samples = rng.uniform(prob_lower_bound, prob_upper_bound, size=n_per_stratum)\n            \n            # 2. Use inverse transform sampling to get samples from the conditional Beta distribution.\n            # The inverse CDF is the percent-point function (ppf).\n            x_samples = beta_dist.ppf(u_samples, a, b)\n            \n            # 3. Apply the function g(x) to the samples.\n            g_values = g_func(x_samples)\n            \n            # 4. Compute the sample mean for the stratum.\n            g_bar_i = np.mean(g_values)\n            stratum_means.append(g_bar_i)\n\n        # The stratified estimator for E[g(X)] is the mean of the stratum means\n        # because all strata have equal probability (p_i = 1/L).\n        # E_strat[g(X)] = sum(p_i * g_bar_i) = sum((1/L) * g_bar_i) = mean(g_bar_i)\n        estimated_expectation = np.mean(stratum_means)\n        \n        # The integral I is B(a,b) * E[g(X)].\n        beta_val = beta_func(a, b)\n        \n        I_strat = beta_val * estimated_expectation\n        results.append(I_strat)\n\n    # Final print statement in the exact required format.\n    # Each result r_i must be formatted to exactly 10 digits after the decimal point.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "In many scientific and engineering contexts, the task is not to simulate from a distribution with known parameters, but rather to infer those parameters from observed data. This practice addresses this inverse problem by guiding you through the Maximum Likelihood Estimation (MLE) for the Gamma distribution's shape parameter. You will derive the necessary components for a Newton-Raphson optimization algorithm and implement it, providing hands-on experience with a powerful numerical method that is indispensable when analytical solutions for parameter estimates are unavailable.",
            "id": "3296511",
            "problem": "Consider independent and identically distributed observations $x_1,\\dots,x_n$ drawn from a Gamma distribution with shape parameter $k$ and known scale parameter $\\theta$, whose probability density function is $f(x\\mid k,\\theta) = \\dfrac{x^{k-1} e^{-x/\\theta}}{\\Gamma(k)\\,\\theta^{k}}$ for $x0$, $k0$, and $\\theta0$. Your tasks are to start from fundamental definitions and properties and build the complete maximum likelihood estimation procedure for the shape parameter $k$ when $\\theta$ is known, and then implement it as a program that computes the estimate for several test cases.\n\nYou must proceed from the following bases only: the definition of the Gamma distribution, the definition of the log-likelihood function, and well-established properties of the logarithm of the Gamma function $\\ln\\Gamma(\\cdot)$ and its derivatives. You must not assume any pre-derived expression for the score, Hessian, or update rules.\n\nPerform the following, in order:\n\n- Derive the log-likelihood $\\ell(k)$ for the data $x_1,\\dots,x_n$ with known $\\theta$.\n- Derive the score function $S(k) = \\dfrac{\\partial}{\\partial k}\\ell(k)$ expressed using the digamma function $\\psi(k)$, where $\\psi(k) = \\dfrac{d}{dk}\\ln\\Gamma(k)$.\n- Derive the second derivative $H(k) = \\dfrac{\\partial^2}{\\partial k^2}\\ell(k)$ expressed using the trigamma function $\\psi_1(k) = \\dfrac{d}{dk}\\psi(k)$.\n- Show that the maximum likelihood estimator $\\widehat{k}$ solves $S(\\widehat{k})=0$, and outline a Newton iteration of the form $k_{\\text{new}} = k_{\\text{old}} - \\dfrac{S(k_{\\text{old}})}{H(k_{\\text{old}})}$, explicitly identifying $S(\\cdot)$ and $H(\\cdot)$ in terms of $\\psi(\\cdot)$ and $\\psi_1(\\cdot)$, and ensuring $k$ remains in $(0,\\infty)$ throughout the iteration.\n\nImplementation requirements:\n\n- Implement a function that, given a positive data vector $(x_1,\\dots,x_n)$ and a known $\\theta0$, computes the maximum likelihood estimate $\\widehat{k}$ by Newton’s method with step damping to enforce the positivity constraint on $k$ if necessary.\n- Use a stopping criterion based on either the absolute Newton step size or the absolute score magnitude falling below a tolerance $\\varepsilon$, with $\\varepsilon = 10^{-10}$.\n- Use at most $100$ iterations; if the method fails to converge within this budget, return the last iterate.\n- For numerical stability, if a proposed Newton update would yield a nonpositive shape, apply a backtracking factor of $1/2$ repeatedly to the step until the updated iterate is positive.\n- The initial guess should be $k^{(0)} = \\max\\{ \\overline{x}/\\theta, 10^{-6} \\}$, where $\\overline{x}$ is the sample mean.\n\nTest suite:\n\nCompute $\\widehat{k}$ for the following five cases. In each case, the program should round the final estimate to $10$ decimal places before output aggregation.\n\n- Case A: $\\theta = 2.0$, $n=10$, data $x^{(A)} = (3.2,4.7,5.1,1.8,6.4,2.9,7.2,3.5,4.0,5.6)$.\n\n- Case B: $\\theta = 1.0$, $n=10$, data $x^{(B)} = (0.15,0.32,0.08,1.5,0.42,2.3,0.27,0.65,3.0,0.55)$.\n\n- Case C: $\\theta = 0.5$, $n=12$, data $x^{(C)} = (18.2,19.7,21.3,20.5,22.1,19.9,20.2,18.8,21.0,20.7,19.5,20.1)$.\n\n- Case D: $\\theta = 3.0$, $n=1$, data $x^{(D)} = (4.5)$.\n\n- Case E: $\\theta = 0.8$, $n=5$, data $x^{(E)} = (0.05,0.12,0.03,0.20,0.18)$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the five estimated shape parameters, in the order A, B, C, D, E, as a comma-separated list enclosed in square brackets, like $[r_A,r_B,r_C,r_D,r_E]$. Each $r_\\cdot$ must be a floating-point number rounded to $10$ decimal places. No other output is permitted.",
            "solution": "The problem requires the derivation and implementation of the maximum likelihood estimation (MLE) for the shape parameter $k$ of a Gamma distribution, assuming the scale parameter $\\theta$ is known. We will proceed from first principles as dictated.\n\nLet $x_1, \\dots, x_n$ be independent and identically distributed (i.i.d.) observations from a Gamma distribution, $\\text{Gamma}(k, \\theta)$, with a probability density function (PDF) given by:\n$$f(x \\mid k, \\theta) = \\frac{x^{k-1} e^{-x/\\theta}}{\\Gamma(k) \\theta^k}$$\nfor $x  0$, shape parameter $k  0$, and known scale parameter $\\theta  0$.\n\n**1. Derivation of the Log-Likelihood Function $\\ell(k)$**\n\nThe likelihood function $L(k)$ is the joint probability of observing the data $x_1, \\dots, x_n$, given the parameter $k$. Due to the i.i.d. assumption, this is the product of the individual densities:\n$$L(k) = \\prod_{i=1}^n f(x_i \\mid k, \\theta) = \\prod_{i=1}^n \\frac{x_i^{k-1} e^{-x_i/\\theta}}{\\Gamma(k) \\theta^k}$$\nTo simplify the maximization problem, we work with the log-likelihood function, $\\ell(k) = \\ln L(k)$.\n$$\\ell(k) = \\ln \\left( \\prod_{i=1}^n \\frac{x_i^{k-1} e^{-x_i/\\theta}}{\\Gamma(k) \\theta^k} \\right) = \\sum_{i=1}^n \\ln \\left( \\frac{x_i^{k-1} e^{-x_i/\\theta}}{\\Gamma(k) \\theta^k} \\right)$$\nUsing the properties of logarithms, we expand the expression for a single observation's contribution:\n$$\\ln f(x_i \\mid k, \\theta) = \\ln(x_i^{k-1}) + \\ln(e^{-x_i/\\theta}) - \\ln(\\Gamma(k)) - \\ln(\\theta^k)$$\n$$\\ln f(x_i \\mid k, \\theta) = (k-1)\\ln x_i - \\frac{x_i}{\\theta} - \\ln\\Gamma(k) - k\\ln\\theta$$\nSumming over all $n$ observations, we obtain the total log-likelihood:\n$$\\ell(k) = \\sum_{i=1}^n \\left( (k-1)\\ln x_i - \\frac{x_i}{\\theta} - \\ln\\Gamma(k) - k\\ln\\theta \\right)$$\nWe can group the terms:\n$$\\ell(k) = (k-1)\\sum_{i=1}^n \\ln x_i - \\frac{1}{\\theta}\\sum_{i=1}^n x_i - n\\ln\\Gamma(k) - nk\\ln\\theta$$\nThis is the log-likelihood function for the shape parameter $k$ with a known scale $\\theta$.\n\n**2. Derivation of the Score Function $S(k)$**\n\nThe score function is the first derivative of the log-likelihood function with respect to the parameter $k$. It is denoted by $S(k) = \\frac{\\partial \\ell(k)}{\\partial k}$. We differentiate $\\ell(k)$ term by term with respect to $k$:\n$$S(k) = \\frac{\\partial}{\\partial k} \\left( (k-1)\\sum_{i=1}^n \\ln x_i - \\frac{1}{\\theta}\\sum_{i=1}^n x_i - n\\ln\\Gamma(k) - nk\\ln\\theta \\right)$$\nThe derivatives of the individual terms are:\n- $\\frac{\\partial}{\\partial k} \\left( (k-1)\\sum_{i=1}^n \\ln x_i \\right) = \\sum_{i=1}^n \\ln x_i$\n- $\\frac{\\partial}{\\partial k} \\left( -\\frac{1}{\\theta}\\sum_{i=1}^n x_i \\right) = 0$ (since this term does not depend on $k$)\n- $\\frac{\\partial}{\\partial k} \\left( -n\\ln\\Gamma(k) \\right) = -n \\frac{d}{dk}\\ln\\Gamma(k) = -n\\psi(k)$, where $\\psi(k) = \\frac{\\Gamma'(k)}{\\Gamma(k)}$ is the digamma function.\n- $\\frac{\\partial}{\\partial k} \\left( -nk\\ln\\theta \\right) = -n\\ln\\theta$\n\nCombining these results, the score function is:\n$$S(k) = \\sum_{i=1}^n \\ln x_i - n\\psi(k) - n\\ln\\theta$$\n\n**3. Derivation of the Second Derivative $H(k)$**\n\nThe second derivative of the log-likelihood function, sometimes called the Hessian in multiparameter cases, is $H(k) = \\frac{\\partial^2 \\ell(k)}{\\partial k^2} = \\frac{\\partial S(k)}{\\partial k}$. Differentiating the score function $S(k)$ with respect to $k$:\n$$H(k) = \\frac{\\partial}{\\partial k} \\left( \\sum_{i=1}^n \\ln x_i - n\\psi(k) - n\\ln\\theta \\right)$$\nThe only term that depends on $k$ is $-n\\psi(k)$. Its derivative is:\n- $\\frac{\\partial}{\\partial k} \\left( -n\\psi(k) \\right) = -n \\frac{d\\psi(k)}{dk} = -n\\psi_1(k)$, where $\\psi_1(k)$ is the trigamma function.\n\nThus, the second derivative is:\n$$H(k) = -n\\psi_1(k)$$\n\n**4. Maximum Likelihood Estimator and Newton's Method**\n\nThe maximum likelihood estimator (MLE) for $k$, denoted $\\widehat{k}$, is the value of $k$ that maximizes $\\ell(k)$. This is found by setting the score function to zero:\n$$S(\\widehat{k}) = \\sum_{i=1}^n \\ln x_i - n\\psi(\\widehat{k}) - n\\ln\\theta = 0$$\nThis equation cannot be solved for $\\widehat{k}$ in closed form, so a numerical method is required. Newton's method is an efficient iterative procedure for finding roots. The iteration is given by:\n$$k_{\\text{new}} = k_{\\text{old}} - \\frac{S(k_{\\text{old}})}{H(k_{\\text{old}})}$$\nThe trigamma function $\\psi_1(k)$ is positive for all $k  0$. Therefore, $H(k) = -n\\psi_1(k)  0$, which implies that the log-likelihood function $\\ell(k)$ is strictly concave. This is a crucial property, as it guarantees that any stationary point found by solving $S(k)=0$ is a unique global maximum.\n\nSubstituting our derived expressions for $S(k)$ and $H(k)$:\n$$k_{\\text{new}} = k_{\\text{old}} - \\frac{\\sum_{i=1}^n \\ln x_i - n\\psi(k_{\\text{old}}) - n\\ln\\theta}{-n\\psi_1(k_{\\text{old}})}$$\n$$k_{\\text{new}} = k_{\\text{old}} + \\frac{\\sum_{i=1}^n \\ln x_i - n\\psi(k_{\\text old}) - n\\ln\\theta}{n\\psi_1(k_{\\text old})}$$\n\nThe parameter $k$ must remain in its domain $(0, \\infty)$. If a Newton step $k_{\\text{new}} = k_{\\text{old}} - S(k_{\\text{old}})/H(k_{\\text{old}})$ results in $k_{\\text{new}} \\le 0$, the step is too large. We introduce step damping (a backtracking line search) to prevent this. Let the proposed step be $\\Delta k = -S(k_{\\text{old}})/H(k_{\\text{old}})$. We find a factor $\\alpha \\in (0, 1]$ such that $k_{\\text{old}} + \\alpha \\Delta k  0$. The procedure is to start with $\\alpha = 1$ and repeatedly halve it ($\\alpha \\leftarrow \\alpha/2$) until the positivity constraint is satisfied. The update then becomes $k_{\\text{new}} = k_{\\text{old}} + \\alpha \\Delta k$.\n\nThe iteration starts with an initial guess $k^{(0)}$. A reasonable choice is the method of moments estimator, $k^{(0)} = \\overline{x}/\\theta$, where $\\overline{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$. To ensure $k^{(0)}  0$, we use $k^{(0)} = \\max\\{\\overline{x}/\\theta, 10^{-6}\\}$. The iteration proceeds until the magnitude of the applied step, $|\\alpha\\Delta k|$, is smaller than a tolerance $\\varepsilon=10^{-10}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import digamma, polygamma\n\ndef estimate_k_mle(data: np.ndarray, theta: float) - float:\n    \"\"\"\n    Computes the Maximum Likelihood Estimate for the shape parameter k of a\n    Gamma distribution with known scale parameter theta.\n\n    Args:\n        data: A numpy array of observations.\n        theta: The known scale parameter.\n\n    Returns:\n        The MLE of the shape parameter k.\n    \"\"\"\n    # Define constants for the iterative solver\n    TOLERANCE = 1e-10\n    MAX_ITER = 100\n\n    # Pre-calculate sufficient statistics\n    n = float(len(data))\n    if n == 0:\n        raise ValueError(\"Data cannot be empty.\")\n    \n    sample_mean = np.mean(data)\n    sum_log_data = np.sum(np.log(data))\n\n    # Initial guess for k using method of moments, ensuring positivity\n    k = max(sample_mean / theta, 1e-6)\n    \n    for _ in range(MAX_ITER):\n        # Evaluate the score and Hessian at the current k\n        psi_k = digamma(k)\n        psi1_k = polygamma(1, k)\n        \n        # Score function: S(k) = sum(log(x_i)) - n*psi(k) - n*log(theta)\n        score = sum_log_data - n * psi_k - n * np.log(theta)\n        \n        # Second derivative (Hessian): H(k) = -n*psi_1(k)\n        hessian = -n * psi1_k\n        \n        # Newton-Raphson step calculation\n        if hessian == 0:  # Avoid division by zero\n            break\n        step = -score / hessian\n        \n        # Step damping (backtracking) to ensure k remains positive\n        alpha = 1.0\n        while k + alpha * step = 0:\n            alpha /= 2.0\n            if alpha  1e-8:  # Failsafe for backtracking\n                step = 0.\n                break\n        \n        # Update k with the damped step\n        actual_step = alpha * step\n        k += actual_step\n        \n        # Convergence check on the magnitude of the actual step\n        if np.abs(actual_step)  TOLERANCE:\n            break\n            \n    return k\n\ndef solve():\n    \"\"\"\n    Defines test cases, computes the MLE for each, and prints the results\n    in the specified format.\n    \"\"\"\n    test_cases = [\n        (2.0, np.array([3.2, 4.7, 5.1, 1.8, 6.4, 2.9, 7.2, 3.5, 4.0, 5.6])), # Case A\n        (1.0, np.array([0.15, 0.32, 0.08, 1.5, 0.42, 2.3, 0.27, 0.65, 3.0, 0.55])), # Case B\n        (0.5, np.array([18.2, 19.7, 21.3, 20.5, 22.1, 19.9, 20.2, 18.8, 21.0, 20.7, 19.5, 20.1])), # Case C\n        (3.0, np.array([4.5])), # Case D\n        (0.8, np.array([0.05, 0.12, 0.03, 0.20, 0.18])) # Case E\n    ]\n    \n    results = []\n    for theta, data in test_cases:\n        k_hat = estimate_k_mle(data, theta)\n        # Round the final estimate to 10 decimal places\n        results.append(round(k_hat, 10))\n        \n    # Format the output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}