{
    "hands_on_practices": [
        {
            "introduction": "Before tackling mixed random variables directly, we first warm up with a foundational concept in stochastic modeling: treating a model parameter as a random variable itself. This exercise  provides crucial practice in calculating a conditional expectation, showing how to update our knowledge of an unknown probability based on experimental evidence. Mastering this type of Bayesian update is a cornerstone for building more complex stochastic simulations.",
            "id": "1327117",
            "problem": "An aerospace engineering team is evaluating a new type of photovoltaic cell for use in long-duration satellite missions. The probability, $P$, that a randomly chosen cell of this type remains operational after a simulated 10-year mission is not known with certainty. Based on prior research on similar materials, the team models their uncertainty about $P$ by treating it as a random variable. The prior belief for $P$ is described by a Beta probability density function:\n$$f_P(p) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha, \\beta)} \\quad \\text{for } p \\in [0, 1]$$\nwhere $\\alpha  0$ and $\\beta  0$ are known parameters representing prior knowledge, and $B(\\alpha, \\beta)$ is the Beta function. You may use the standard integral representation of the Beta function, $B(x, y) = \\int_0^1 t^{x-1}(1-t)^{y-1} dt$.\n\nTo gather more data, the team conducts an accelerated life test on a batch of $n$ of these new cells. After the test, which simulates the harsh conditions of space, it is observed that exactly $k$ of the cells are still operational. The operational status of each cell is considered an independent Bernoulli trial with the same success probability $P$.\n\nCalculate the expected value of the probability $P$, conditioned on the observation that $k$ out of $n$ cells survived the test. Express your answer as a single closed-form analytic expression in terms of $n, k, \\alpha$, and $\\beta$.",
            "solution": "We model the per-cell survival as independent Bernoulli trials with success probability $P$. The prior for $P$ is $\\operatorname{Beta}(\\alpha,\\beta)$ with density $f_{P}(p)=B(\\alpha,\\beta)^{-1} p^{\\alpha-1}(1-p)^{\\beta-1}$ on $[0,1]$. Given $k$ operational cells out of $n$, the likelihood as a function of $p$ is\n$$\nL(p\\mid k,n)=\\binom{n}{k} p^{k}(1-p)^{n-k}.\n$$\nBy Bayes’ theorem, the posterior density of $P$ given the data is proportional to the product of likelihood and prior:\n$$\nf_{P\\mid k,n}(p)\\propto L(p\\mid k,n)\\,f_{P}(p)=\\binom{n}{k}\\frac{1}{B(\\alpha,\\beta)}\\,p^{\\alpha+k-1}(1-p)^{\\beta+n-k-1},\\quad p\\in[0,1].\n$$\nThe normalizing constant is the integral over $p\\in[0,1]$:\n$$\n\\int_{0}^{1}\\binom{n}{k}\\frac{1}{B(\\alpha,\\beta)}\\,p^{\\alpha+k-1}(1-p)^{\\beta+n-k-1}\\,dp=\\binom{n}{k}\\frac{B(\\alpha+k,\\beta+n-k)}{B(\\alpha,\\beta)}.\n$$\nTherefore the posterior density simplifies to the Beta density\n$$\nf_{P\\mid k,n}(p)=\\frac{p^{\\alpha+k-1}(1-p)^{\\beta+n-k-1}}{B(\\alpha+k,\\beta+n-k)},\\quad p\\in[0,1],\n$$\ni.e., $P\\mid(k,n)\\sim\\operatorname{Beta}(\\alpha+k,\\beta+n-k)$.\n\nThe conditional expectation of $P$ is the mean of this Beta distribution:\n$$\n\\mathbb{E}[P\\mid k,n]=\\int_{0}^{1}p\\,f_{P\\mid k,n}(p)\\,dp=\\frac{1}{B(\\alpha+k,\\beta+n-k)}\\int_{0}^{1}p^{\\alpha+k}(1-p)^{\\beta+n-k-1}\\,dp.\n$$\nRecognizing the integral as a Beta function,\n$$\n\\int_{0}^{1}p^{\\alpha+k}(1-p)^{\\beta+n-k-1}\\,dp=B(\\alpha+k+1,\\beta+n-k),\n$$\nwe obtain\n$$\n\\mathbb{E}[P\\mid k,n]=\\frac{B(\\alpha+k+1,\\beta+n-k)}{B(\\alpha+k,\\beta+n-k)}.\n$$\nUsing the Beta function identity $B(u+1,v)=\\frac{u}{u+v}B(u,v)$ (which follows from the integral representation by integration by parts), with $u=\\alpha+k$ and $v=\\beta+n-k$, we conclude\n$$\n\\mathbb{E}[P\\mid k,n]=\\frac{\\alpha+k}{\\alpha+\\beta+n}.\n$$\nThis is the desired closed-form expression in terms of $n$, $k$, $\\alpha$, and $\\beta$.",
            "answer": "$$\\boxed{\\frac{\\alpha+k}{\\alpha+\\beta+n}}$$"
        },
        {
            "introduction": "Having explored parameters as random variables, we now turn to the unique challenges posed by mixed discrete-continuous distributions. This practice  highlights a common but critical error in simulation—failing to account for the discrete \"atom\" of a mixed variable. By deriving the exact analytical bias of this naive approach, you will gain a quantitative understanding of why careful algorithm design is essential for obtaining accurate results.",
            "id": "3333837",
            "problem": "Consider a mixed random variable $X$ on $[0,\\infty)$ defined by the following decomposition of its probability measure into a discrete atom and an absolutely continuous part. The atom is located at $x=0$ with mass $\\alpha \\in (0,1)$, that is $\\mathbb{P}(X=0)=\\alpha$. Conditional on $X0$, the continuous component has an exponential distribution with rate $\\lambda0$, so that the density on $(0,\\infty)$ is $f(x)=(1-\\alpha)\\lambda \\exp(-\\lambda x)$. The cumulative distribution function $F$ therefore satisfies $F(0)=\\alpha$ and, for $x0$, $F(x)=\\alpha+(1-\\alpha)(1-\\exp(-\\lambda x))$.\n\nYou are tasked with estimating the quantity $\\theta=\\mathbb{E}[\\exp(-\\beta X)]$ for a fixed $\\beta0$ using Monte Carlo simulation. A practitioner, unaware of the need for explicit randomization at jumps, implements a naive inverse transform sampling scheme that ignores the atom: they construct a continuous proxy cumulative distribution function by normalizing the absolutely continuous component to total mass $1$, namely $F_{\\mathrm{cont}}(x)=(1-\\alpha)^{-1}(F(x)-\\alpha)=1-\\exp(-\\lambda x)$ for $x\\geq 0$, and then generate samples via $X_{\\mathrm{naive}}=F_{\\mathrm{cont}}^{-1}(U)$ with $U\\sim \\mathrm{Uniform}(0,1)$. In other words, the naive simulator samples $X_{\\mathrm{naive}}$ from a purely exponential distribution with rate $\\lambda$ and omits the randomization that would select the atom at $x=0$ with probability $\\alpha$.\n\nStarting only from the definitions of mixed random variables, their cumulative distribution functions and the definition of expectation as an integral with respect to the probability measure, derive the exact bias of the resulting Monte Carlo estimator as the number of samples tends to infinity. That is, derive the closed-form expression for\n$$\n\\mathrm{Bias}=\\lim_{n\\to\\infty}\\left(\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\exp(-\\beta X_{\\mathrm{naive},i})\\right]-\\theta\\right),\n$$\nwhere $X_{\\mathrm{naive},i}$ are independent and identically distributed draws from the naive inverse scheme described above. Express your final answer as a single closed-form analytic expression in terms of $\\alpha$, $\\lambda$, and $\\beta$. No rounding is required.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- A mixed random variable $X$ is defined on the support $[0, \\infty)$.\n- Its probability measure has a discrete atom at $x=0$ with mass $\\mathbb{P}(X=0)=\\alpha$, where $\\alpha \\in (0,1)$.\n- Its absolutely continuous component is defined on $(0, \\infty)$ with probability density $f(x)=(1-\\alpha)\\lambda \\exp(-\\lambda x)$ for $\\lambda0$. This corresponds to the density of the variable $X$ conditional on $X0$ being $\\lambda \\exp(-\\lambda x)$.\n- The cumulative distribution function (CDF) is $F(x) = \\alpha$ for $x=0$ and $F(x)=\\alpha+(1-\\alpha)(1-\\exp(-\\lambda x))$ for $x0$.\n- The quantity to be estimated is $\\theta=\\mathbb{E}[\\exp(-\\beta X)]$ for a fixed constant $\\beta0$.\n- A naive Monte Carlo sampling scheme generates samples $X_{\\mathrm{naive}}$ from a purely exponential distribution with rate $\\lambda$. The problem states this is done via $X_{\\mathrm{naive}}=F_{\\mathrm{cont}}^{-1}(U)$ with $U\\sim \\mathrm{Uniform}(0,1)$ and $F_{\\mathrm{cont}}(x)=1-\\exp(-\\lambda x)$.\n- The task is to derive the asymptotic bias of the Monte Carlo estimator:\n$$\n\\mathrm{Bias}=\\lim_{n\\to\\infty}\\left(\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\exp(-\\beta X_{\\mathrm{naive},i})\\right]-\\theta\\right)\n$$\nwhere $X_{\\mathrm{naive},i}$ are independent and identically distributed (i.i.d.) draws.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is grounded in the established principles of probability theory, specifically concerning mixed random variables, expectation, and Monte Carlo simulation. The concepts of CDF, probability density, atoms, and inverse transform sampling are standard.\n- **Well-Posedness**: The problem is well-posed. It asks for a definite, computable quantity (asymptotic bias) based on clearly defined distributions and parameters. The parameters $\\alpha$, $\\lambda$, and $\\beta$ are constrained, ensuring the requested expectations are finite and well-defined.\n- **Objectivity**: The language is precise, mathematical, and free of subjective content.\n- **Consistency and Completeness**: The provided information is self-consistent. The CDF $F(x)$ correctly combines the point mass at $x=0$ with the integral of the given density on $(0, \\infty)$: $\\mathbb{P}(X \\le x) = \\mathbb{P}(X=0) + \\int_0^x (1-\\alpha)\\lambda \\exp(-\\lambda t) dt = \\alpha + (1-\\alpha)[-\\exp(-\\lambda t)]_0^x = \\alpha + (1-\\alpha)(1-\\exp(-\\lambda x))$, which matches the provided $F(x)$. All necessary information is given.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution process will now commence.\n\nThe bias of an estimator $\\hat{\\theta}_n = \\frac{1}{n}\\sum_{i=1}^{n} g(X_i)$ for a parameter $\\theta = \\mathbb{E}[g(X)]$ is defined as $\\mathbb{E}[\\hat{\\theta}_n] - \\theta$. In this problem, the estimator is constructed using samples $X_{\\mathrm{naive},i}$ instead of the correct samples $X_i$. The quantity to be calculated is the asymptotic bias.\n\nFirst, let's simplify the expression for the bias. The estimator is $\\hat{\\theta}_{\\mathrm{naive}, n} = \\frac{1}{n}\\sum_{i=1}^{n}\\exp(-\\beta X_{\\mathrm{naive},i})$. By linearity of expectation and the fact that the samples $X_{\\mathrm{naive},i}$ are i.i.d., its expectation is:\n$$\n\\mathbb{E}\\left[\\hat{\\theta}_{\\mathrm{naive}, n}\\right] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\exp(-\\beta X_{\\mathrm{naive},i})\\right] = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive},i})] = \\frac{1}{n} \\cdot n \\cdot \\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] = \\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})]\n$$\nThis expectation does not depend on the sample size $n$. The bias is therefore constant with respect to $n$. The limit in the problem's definition of bias is thus simply the bias itself:\n$$\n\\mathrm{Bias} = \\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] - \\theta = \\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] - \\mathbb{E}[\\exp(-\\beta X)]\n$$\nThe problem reduces to calculating two expectations and finding their difference.\n\n**1. Calculation of the true expectation, $\\theta = \\mathbb{E}[\\exp(-\\beta X)]$**\n\nThe expectation of a function $g(X)$ of a mixed random variable $X$ is computed by integrating $g(x)$ with respect to the probability measure $dP_X(x)$. This measure consists of a discrete part (a point mass) and a continuous part.\n$$\n\\theta = \\mathbb{E}[\\exp(-\\beta X)] = \\int_{[0,\\infty)} \\exp(-\\beta x) \\, dP_X(x)\n$$\nThis integral can be decomposed into the sum of the contribution from the atom at $x=0$ and the integral over the continuous part on $(0,\\infty)$:\n$$\n\\theta = \\exp(-\\beta \\cdot 0) \\cdot \\mathbb{P}(X=0) + \\int_0^{\\infty} \\exp(-\\beta x) f(x) \\, dx\n$$\nUsing the given values, $\\mathbb{P}(X=0)=\\alpha$ and $f(x)=(1-\\alpha)\\lambda \\exp(-\\lambda x)$ for $x>0$:\n$$\n\\theta = \\exp(0) \\cdot \\alpha + \\int_0^{\\infty} \\exp(-\\beta x) (1-\\alpha)\\lambda \\exp(-\\lambda x) \\, dx\n$$\n$$\n\\theta = 1 \\cdot \\alpha + (1-\\alpha)\\lambda \\int_0^{\\infty} \\exp(-(\\lambda+\\beta)x) \\, dx\n$$\nThe integral is evaluated as:\n$$\n\\int_0^{\\infty} \\exp(-(\\lambda+\\beta)x) \\, dx = \\left[ \\frac{\\exp(-(\\lambda+\\beta)x)}{-(\\lambda+\\beta)} \\right]_0^{\\infty} = 0 - \\frac{1}{-(\\lambda+\\beta)} = \\frac{1}{\\lambda+\\beta}\n$$\nThe integral converges since $\\lambda>0$ and $\\beta>0$, which implies $\\lambda+\\beta>0$.\nSubstituting this result back into the expression for $\\theta$:\n$$\n\\theta = \\alpha + (1-\\alpha) \\lambda \\frac{1}{\\lambda+\\beta} = \\alpha + (1-\\alpha) \\frac{\\lambda}{\\lambda+\\beta}\n$$\n\n**2. Calculation of the naive expectation, $\\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})]$**\n\nThe problem states that $X_{\\mathrm{naive}}$ is sampled from a purely exponential distribution with rate $\\lambda$. This is consistent with the inverse transform method applied to $F_{\\mathrm{cont}}(x) = 1-\\exp(-\\lambda x)$. The probability density function of $X_{\\mathrm{naive}}$ is $f_{\\mathrm{naive}}(x) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$.\nThe expectation is calculated as:\n$$\n\\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] = \\int_0^{\\infty} \\exp(-\\beta x) f_{\\mathrm{naive}}(x) \\, dx = \\int_0^{\\infty} \\exp(-\\beta x) \\lambda \\exp(-\\lambda x) \\, dx\n$$\n$$\n\\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] = \\lambda \\int_0^{\\infty} \\exp(-(\\lambda+\\beta)x) \\, dx\n$$\nThis is the same integral form as encountered before, multiplied by $\\lambda$:\n$$\n\\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] = \\lambda \\left( \\frac{1}{\\lambda+\\beta} \\right) = \\frac{\\lambda}{\\lambda+\\beta}\n$$\nThis result is also recognizable as the moment-generating function of an exponential distribution with rate $\\lambda$, $M(t)=\\frac{\\lambda}{\\lambda-t}$, evaluated at $t=-\\beta$.\n\n**3. Calculation of the Bias**\n\nFinally, we compute the bias by subtracting the true expectation $\\theta$ from the naively computed expectation.\n$$\n\\mathrm{Bias} = \\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] - \\theta = \\frac{\\lambda}{\\lambda+\\beta} - \\left( \\alpha + (1-\\alpha) \\frac{\\lambda}{\\lambda+\\beta} \\right)\n$$\nDistributing the negative sign:\n$$\n\\mathrm{Bias} = \\frac{\\lambda}{\\lambda+\\beta} - \\alpha - (1-\\alpha) \\frac{\\lambda}{\\lambda+\\beta}\n$$\nGroup the terms containing the fraction:\n$$\n\\mathrm{Bias} = \\left(1 - (1-\\alpha)\\right) \\frac{\\lambda}{\\lambda+\\beta} - \\alpha\n$$\n$$\n\\mathrm{Bias} = (1 - 1 + \\alpha) \\frac{\\lambda}{\\lambda+\\beta} - \\alpha\n$$\n$$\n\\mathrm{Bias} = \\alpha \\frac{\\lambda}{\\lambda+\\beta} - \\alpha\n$$\nFactor out the common term $\\alpha$:\n$$\n\\mathrm{Bias} = \\alpha \\left( \\frac{\\lambda}{\\lambda+\\beta} - 1 \\right)\n$$\nCombine the terms inside the parenthesis over a common denominator:\n$$\n\\mathrm{Bias} = \\alpha \\left( \\frac{\\lambda - (\\lambda+\\beta)}{\\lambda+\\beta} \\right) = \\alpha \\left( \\frac{\\lambda - \\lambda - \\beta}{\\lambda+\\beta} \\right) = \\alpha \\left( \\frac{-\\beta}{\\lambda+\\beta} \\right)\n$$\nThus, the final expression for the bias is:\n$$\n\\mathrm{Bias} = -\\frac{\\alpha\\beta}{\\lambda+\\beta}\n$$\nThe bias is negative, which is expected. The naive sampler ignores the atom at $x=0$, where the function $g(x)=\\exp(-\\beta x)$ attains its maximum value of $g(0)=1$. By sampling only from the continuous part where $x>0$ and $g(x)1$, the naive estimator systematically underestimates the true mean.",
            "answer": "$$\n\\boxed{-\\frac{\\alpha\\beta}{\\lambda+\\beta}}\n$$"
        },
        {
            "introduction": "Building on our understanding of the pitfalls of naive simulation, this final practice guides you toward an efficient and theoretically sound solution. In this advanced exercise , you will design a stratified Monte Carlo estimator that intelligently separates the discrete and continuous components of a mixed random variable. The task involves deriving the optimal allocation of computational effort to minimize variance, culminating in an implemented algorithm that demonstrates a powerful variance reduction technique.",
            "id": "3333855",
            "problem": "Construct and analyze a stratified Monte Carlo estimator for a mixed random variable using only fundamental definitions and principles. Let $X$ be a mixed random variable with a discrete atom at $0$ and a continuous component on $(0,\\infty)$. Specifically, assume $\\mathbb{P}(X=0)=p$ and the conditional distribution of $X$ given $X0$ has a probability density function $g$ on $(0,\\infty)$; equivalently, $X$ has a mixed distribution with mass $p$ at $0$ and density $(1-p)g(x)$ on $(0,\\infty)$. Given a measurable function $h:(0,\\infty)\\to\\mathbb{R}$ with $h(0)$ defined, your task is to derive a stratified estimator for $\\mathbb{E}[h(X)]$ by stratifying on the sets $\\{X=0\\}$ and $\\{X0\\}$ and to optimize the sample allocation between strata under a cost constraint to minimize variance.\n\nBase your derivation on the following fundamental definitions and well-tested facts:\n- The law of total expectation: $\\mathbb{E}[h(X)] = \\mathbb{E}[h(X)\\mid X=0]\\mathbb{P}(X=0) + \\mathbb{E}[h(X)\\mid X0]\\mathbb{P}(X0)$.\n- For a stratified estimator using independent samples within strata with allocations $n_0$ samples from $\\{X=0\\}$ and $n_1$ samples from $\\{X0\\}$, with stratum weights $w_0=\\mathbb{P}(X=0)$ and $w_1=\\mathbb{P}(X0)$, the unbiased estimator is $\\widehat{\\mu} = w_0 \\widehat{\\mu}_0 + w_1 \\widehat{\\mu}_1$, where $\\widehat{\\mu}_i$ is the sample mean of $h(X)$ within stratum $i\\in\\{0,1\\}$.\n- The variance of the stratified estimator under independence is $\\mathrm{Var}(\\widehat{\\mu}) = \\sum_{i\\in\\{0,1\\}} \\frac{w_i^2 \\sigma_i^2}{n_i}$ where $\\sigma_i^2$ is the within-stratum variance of $h(X)$.\n- A fixed computational budget modeled by a linear cost constraint $\\sum_{i\\in\\{0,1\\}} c_i n_i \\le B$, with known positive per-sample costs $c_0$ and $c_1$, and a fixed budget $B$ (interpreted as total allowable cost).\n\nYour tasks:\n- Derive from first principles an optimal allocation rule $(n_0^\\star,n_1^\\star)$ that minimizes $\\mathrm{Var}(\\widehat{\\mu})$ subject to the budget constraint $\\sum_{i\\in\\{0,1\\}} c_i n_i \\le B$, for given $p$ and $g$ and measurable $h$.\n- Specialize your derivation to the given mixed structure where $\\{X=0\\}$ is an atom, and clearly state what happens to the optimal allocation in this case.\n- Construct an implementable stratified estimator that uses the derived optimal allocation. If any stratum does not require sampling under the optimal allocation, explain why and how your estimator simplifies in that case.\n- Implement the estimator and compute its value for each of the test cases below using a pseudorandom generator with a fixed seed $s$ per test case so that the output is deterministic. Use independent streams across test cases by seeding with $s+k$ for test case index $k$ starting at $0$.\n\nTest suite (each case specifies $p$, continuous part $g$, function $h$, budget $B$ with unitless costs, and per-sample costs $c_0,c_1$):\n- Case $1$: $p=\\frac{1}{3}$, $g$ is exponential with rate $\\lambda=2$ (density $g(x)=2 e^{-2x}$ on $(0,\\infty)$), $h(x)=x + \\mathbf{1}\\{x1\\}$, budget $B=1000$, costs $c_0=1$, $c_1=1$, seed $s=12345$.\n- Case $2$: $p=0.99$, $g$ is gamma with shape $k=3$ and scale $\\theta=1$ (density $g(x)=\\frac{x^{k-1}e^{-x/\\theta}}{\\Gamma(k)\\theta^k}$ on $(0,\\infty)$), $h(x)=x^2$, budget $B=200$, costs $c_0=1$, $c_1=1$, seed $s=12345$.\n- Case $3$: $p=0.05$, $g$ is lognormal with parameters $\\mu=0$ and $\\sigma=1$ (i.e., $\\log Y\\sim \\mathcal{N}(0,1)$ for $Y\\sim g$), $h(x)=\\mathbf{1}\\{x2\\}$, budget $B=500$, costs $c_0=1$, $c_1=1$, seed $s=12345$.\n- Case $4$: $p=1$, $g$ is exponential with rate $\\lambda=1$ (density $g(x)=e^{-x}$ on $(0,\\infty)$), $h(x)=\\sqrt{x+1}-1$, budget $B=50$, costs $c_0=1$, $c_1=1$, seed $s=12345$.\n\nFor each case, in addition to computing the stratified estimator with the optimal allocation, also compute the exact value of $\\mathbb{E}[h(X)]$ using analytic formulas derived from the given analytic formulas wherever they are available:\n- For exponential with rate $\\lambda$, $\\mathbb{E}[Y]=\\frac{1}{\\lambda}$ and $\\mathbb{P}(Yt)=e^{-\\lambda t}$ for $t0$.\n- For gamma with shape $k$ and scale $\\theta$, $\\mathbb{E}[Y^2]=\\theta^2 k(1+k)$.\n- For lognormal with parameters $\\mu$ and $\\sigma$, $\\mathbb{P}(Yt)=1-\\Phi\\left(\\frac{\\ln t - \\mu}{\\sigma}\\right)$ for $t0$, where $\\Phi$ is the cumulative distribution function of the standard normal distribution.\n\nYour program must:\n- Implement the optimal allocation computation and the stratified estimator based on your derivation.\n- For each test case, output the optimal stratum sample counts $(n_0^\\star,n_1^\\star)$ as integers, the stratified estimator value as a floating-point number, and the exact value as a floating-point number computed from the given analytic formulas.\n- Use the specified seeds to make the simulation results deterministic.\n- Ensure that the budget constraint $\\sum_{i\\in\\{0,1\\}} c_i n_i \\le B$ is satisfied.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case in the order $1,2,3,4$, append in order four entries: $n_0^\\star$, $n_1^\\star$, the stratified estimator value, and the exact value. For example, an output with two cases would look like $[n_{0,1}^\\star,n_{1,1}^\\star,\\widehat{\\mu}_1,\\mu_1,n_{0,2}^\\star,n_{1,2}^\\star,\\widehat{\\mu}_2,\\mu_2]$; extend this pattern for all four cases.",
            "solution": "The problem requires the derivation and implementation of a stratified Monte Carlo estimator for a mixed random variable. The solution proceeds in two stages: first, a formal derivation of the optimal sample allocation and the resulting estimator, and second, the application of this framework to the specific test cases provided.\n\n### Principle-Based Derivation and Design\n\nLet $X$ be a mixed random variable with distribution $\\mathbb{P}(X=0)=p$ and a conditional probability density function $g(x)$ on $(0, \\infty)$ for $X0$. We are tasked with estimating $\\mathbb{E}[h(X)]$ for a given function $h$.\n\nThe problem specifies stratifying the sample space into two strata: $S_0 = \\{x | x=0\\}$ and $S_1 = \\{x | x0\\}$. The corresponding stratum weights are the probabilities of these events:\n$w_0 = \\mathbb{P}(X \\in S_0) = \\mathbb{P}(X=0) = p$\n$w_1 = \\mathbb{P}(X \\in S_1) = \\mathbb{P}(X0) = 1-p$\n\nThe law of total expectation provides the exact value we wish to estimate:\n$$\n\\mathbb{E}[h(X)] = \\mathbb{E}[h(X) | X \\in S_0] w_0 + \\mathbb{E}[h(X) | X \\in S_1] w_1\n$$\nLet $\\mu_i = \\mathbb{E}[h(X) | X \\in S_i]$ be the conditional mean of $h(X)$ in stratum $i$. Then $\\mathbb{E}[h(X)] = \\mu_0 w_0 + \\mu_1 w_1$.\n\nA stratified Monte Carlo estimator for $\\mathbb{E}[h(X)]$ is given by $\\widehat{\\mu} = w_0 \\widehat{\\mu}_0 + w_1 \\widehat{\\mu}_1$, where $\\widehat{\\mu}_i$ is the sample mean of $h(X)$ based on $n_i$ samples drawn from stratum $i$. The variance of this estimator, assuming independent samples, is:\n$$\n\\mathrm{Var}(\\widehat{\\mu}) = \\frac{w_0^2 \\sigma_0^2}{n_0} + \\frac{w_1^2 \\sigma_1^2}{n_1}\n$$\nwhere $\\sigma_i^2 = \\mathrm{Var}(h(X) | X \\in S_i)$ is the variance of $h(X)$ within stratum $i$.\n\n**1. Optimal Sample Allocation**\n\nOur goal is to minimize this variance subject to a linear budget constraint $\\sum_{i=0,1} c_i n_i \\le B$, where $c_i  0$ is the cost per sample in stratum $i$ and $B$ is the total budget. To minimize variance, we should use the entire budget, so the constraint becomes an equality: $c_0 n_0 + c_1 n_1 = B$.\n\nThis is a constrained optimization problem that can be solved using the method of Lagrange multipliers. The Lagrangian is:\n$$\nL(n_0, n_1, \\lambda) = \\left( \\frac{w_0^2 \\sigma_0^2}{n_0} + \\frac{w_1^2 \\sigma_1^2}{n_1} \\right) + \\lambda(c_0 n_0 + c_1 n_1 - B)\n$$\nSetting the partial derivatives with respect to $n_0$ and $n_1$ to zero yields:\n$$\n\\frac{\\partial L}{\\partial n_i} = -\\frac{w_i^2 \\sigma_i^2}{n_i^2} + \\lambda c_i = 0 \\implies n_i^2 = \\frac{w_i^2 \\sigma_i^2}{\\lambda c_i} \\implies n_i = \\frac{w_i \\sigma_i}{\\sqrt{\\lambda c_i}}\n$$\nThis implies that the optimal allocation $n_i^\\star$ is proportional to $w_i \\sigma_i / \\sqrt{c_i}$. To find the constant of proportionality, we substitute this into the budget constraint:\n$$\n\\sum_{i=0,1} c_i \\left( \\frac{w_i \\sigma_i}{\\sqrt{\\lambda c_i}} \\right) = B \\implies \\frac{1}{\\sqrt{\\lambda}} \\sum_{i=0,1} w_i \\sigma_i \\sqrt{c_i} = B \\implies \\frac{1}{\\sqrt{\\lambda}} = \\frac{B}{\\sum_{j=0,1} w_j \\sigma_j \\sqrt{c_j}}\n$$\nSubstituting this back into the expression for $n_i$ gives the optimal allocation (for real-valued $n_i$):\n$$\nn_i^\\star = B \\frac{w_i \\sigma_i / \\sqrt{c_i}}{\\sum_{j=0,1} w_j \\sigma_j \\sqrt{c_j}}\n$$\n\n**2. Specialization to the Mixed Distribution**\n\nWe now apply this general result to our specific stratification.\n-   **Stratum $S_0 = \\{X=0\\}$:** A sample from this stratum is always $X=0$. Thus, the value $h(X)$ is always the constant $h(0)$. The variance of a constant is zero. Therefore, the within-stratum variance $\\sigma_0^2$ is:\n    $$\n    \\sigma_0^2 = \\mathrm{Var}(h(X) | X=0) = 0\n    $$\n-   **Stratum $S_1 = \\{X0\\}$:** A sample from this stratum follows the distribution with density $g(x)$. The variance $\\sigma_1^2$ is the variance of $h(Y)$ where $Y \\sim g$, i.e., $\\sigma_1^2 = \\mathrm{Var}(h(Y))$.\n\nThe fact that $\\sigma_0 = 0$ dramatically simplifies the optimal allocation. Substituting $\\sigma_0=0$ into the allocation formula:\n$$\nn_0^\\star = B \\frac{w_0 (0) / \\sqrt{c_0}}{w_0 (0) \\sqrt{c_0} + w_1 \\sigma_1 \\sqrt{c_1}} = 0\n$$\n$$\nn_1^\\star = B \\frac{w_1 \\sigma_1 / \\sqrt{c_1}}{w_0 (0) \\sqrt{c_0} + w_1 \\sigma_1 \\sqrt{c_1}} = B \\frac{w_1 \\sigma_1 / \\sqrt{c_1}}{w_1 \\sigma_1 \\sqrt{c_1}} = \\frac{B}{c_1}\n$$\nThis derivation holds if $w_1 \\sigma_1  0$, i.e., $p1$ and $h(X)$ is not constant on $(0, \\infty)$. Since sample sizes must be integers, we take the floor to ensure the budget constraint is satisfied:\n$$\nn_0^\\star = 0 \\quad \\text{and} \\quad n_1^\\star = \\left\\lfloor \\frac{B}{c_1} \\right\\rfloor\n$$\nThis result is intuitive: since the outcome in stratum $S_0$ is deterministic, there is no uncertainty to reduce. All of the simulation budget should be allocated to stratum $S_1$, which is the sole source of variance.\n\n**3. The Resulting Estimator**\n\nWith the optimal allocation $n_0^\\star=0$, we do not perform any sampling for stratum $S_0$. Instead of a sample mean $\\widehat{\\mu}_0$, we use the exact conditional expectation $\\mu_0 = \\mathbb{E}[h(X)|X=0] = h(0)$. For stratum $S_1$, we generate $n_1^\\star$ samples $Y_1, \\dots, Y_{n_1^\\star}$ from the density $g(x)$ and compute the sample mean $\\widehat{\\mu}_1 = \\frac{1}{n_1^\\star} \\sum_{j=1}^{n_1^\\star} h(Y_j)$.\n\nThe final stratified estimator is:\n$$\n\\widehat{\\mu} = w_0 \\mu_0 + w_1 \\widehat{\\mu}_1 = p \\cdot h(0) + (1-p) \\frac{1}{n_1^\\star} \\sum_{j=1}^{n_1^\\star} h(Y_j)\n$$\n\n**4. Edge Case: $p=1$**\n\nIf $p=1$, the random variable $X$ is deterministically $0$. Thus, $\\mathbb{E}[h(X)] = h(0)$ exactly. No estimation is required. In this case, $w_0=1$ and $w_1=0$. The variance of the stratified estimator is $\\mathrm{Var}(\\widehat{\\mu}) = \\frac{1^2 \\cdot 0^2}{n_0} + \\frac{0^2 \\cdot \\sigma_1^2}{n_1} = 0$ for any non-zero $n_0, n_1$. Since the variance is already zero without any sampling, the optimal allocation that minimizes variance (and cost) is $n_0^\\star = 0$ and $n_1^\\star = 0$. The estimator's value is simply the exact value, $h(0)$.\n\n**5. Analytic Calculation of Exact Values**\n\nFor each test case, the exact value is $\\mu = \\mathbb{E}[h(X)] = p \\cdot h(0) + (1-p) \\mathbb{E}[h(Y)]$, where $Y \\sim g$. The values of $h(0)$ and $\\mathbb{E}[h(Y)]$ are computed as follows:\n-   **Case 1:** $p=\\frac{1}{3}$, $Y \\sim \\text{Exp}(\\lambda=2)$, $h(x)=x + \\mathbf{1}\\{x1\\}$. $h(0)=0$. $\\mathbb{E}[h(Y)] = \\mathbb{E}[Y] + \\mathbb{P}(Y1) = \\frac{1}{\\lambda} + e^{-\\lambda} = \\frac{1}{2} + e^{-2}$.\n    $\\mu = (1-\\frac{1}{3})(\\frac{1}{2} + e^{-2}) = \\frac{1}{3} + \\frac{2}{3}e^{-2}$.\n-   **Case 2:** $p=0.99$, $Y \\sim \\text{Gamma}(k=3, \\theta=1)$, $h(x)=x^2$. $h(0)=0$. $\\mathbb{E}[h(Y)] = \\mathbb{E}[Y^2] = \\theta^2 k(k+1) = 1^2 \\cdot 3(4) = 12$.\n    $\\mu = (1-0.99)(12) = 0.12$.\n-   **Case 3:** $p=0.05$, $Y \\sim \\text{Lognormal}(\\mu_{ln}=0, \\sigma_{ln}=1)$, $h(x)=\\mathbf{1}\\{x2\\}$. $h(0)=0$. $\\mathbb{E}[h(Y)] = \\mathbb{P}(Y2) = 1-\\Phi(\\frac{\\ln(2)-\\mu_{ln}}{\\sigma_{ln}}) = 1-\\Phi(\\ln 2)$, where $\\Phi$ is the standard normal CDF.\n    $\\mu = (1-0.05)(1-\\Phi(\\ln 2))$.\n-   **Case 4:** $p=1$, $h(x)=\\sqrt{x+1}-1$. $h(0)=0$. The distribution is a point mass at $0$.\n    $\\mu = \\mathbb{E}[h(X)] = h(0) = 0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes stratified Monte Carlo estimates for a mixed random variable\n    based on optimal allocation derived from first principles.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'p': 1/3,\n            'g': {'type': 'exponential', 'lambda': 2},\n            'h': lambda x: x + (x  1).astype(float),\n            'B': 1000, 'c0': 1, 'c1': 1, 's': 12345\n        },\n        {\n            'p': 0.99,\n            'g': {'type': 'gamma', 'k': 3, 'theta': 1},\n            'h': lambda x: x**2,\n            'B': 200, 'c0': 1, 'c1': 1, 's': 12345\n        },\n        {\n            'p': 0.05,\n            'g': {'type': 'lognormal', 'mu': 0, 'sigma': 1},\n            'h': lambda x: (x  2).astype(float),\n            'B': 500, 'c0': 1, 'c1': 1, 's': 12345\n        },\n        {\n            'p': 1,\n            'g': {'type': 'exponential', 'lambda': 1},\n            'h': lambda x: np.sqrt(x + 1) - 1,\n            'B': 50, 'c0': 1, 'c1': 1, 's': 12345\n        }\n    ]\n\n    results = []\n    \n    for k, case in enumerate(test_cases):\n        p = case['p']\n        g_params = case['g']\n        h = case['h']\n        B, c0, c1 = case['B'], case['c0'], case['c1']\n        seed = case['s']\n\n        # h(0) is needed for both estimator and exact value.\n        h_at_0 = h(np.array([0.0]))[0]\n        \n        # --- Optimal Allocation and Estimation ---\n        n0_star, n1_star = 0, 0\n        estimator_val = h_at_0  # Default for p=1\n\n        # The optimal allocation is n0=0, n1=floor(B/c1) unless p=1.\n        if p == 1:\n            # If p=1, X=0 deterministically. No sampling needed.\n            n0_star = 0\n            n1_star = 0\n            estimator_val = h_at_0\n        else:\n            # Allocate entire budget to stratum 1 (the continuous part).\n            n0_star = 0\n            n1_star = int(np.floor(B / c1))\n\n            if n1_star  0:\n                rng = np.random.default_rng(seed + k)\n                samples_g = np.array([])\n                \n                if g_params['type'] == 'exponential':\n                    # scale = 1/lambda\n                    samples_g = rng.exponential(scale=1.0/g_params['lambda'], size=n1_star)\n                elif g_params['type'] == 'gamma':\n                    # shape=k, scale=theta\n                    samples_g = rng.gamma(shape=g_params['k'], scale=g_params['theta'], size=n1_star)\n                elif g_params['type'] == 'lognormal':\n                    # mean=mu, sigma=sigma\n                    samples_g = rng.lognormal(mean=g_params['mu'], sigma=g_params['sigma'], size=n1_star)\n                \n                h_samples = h(samples_g)\n                mu1_hat = np.mean(h_samples)\n                estimator_val = p * h_at_0 + (1 - p) * mu1_hat\n            else: # If budget is too small (n1_star = 0)\n                # No samples can be drawn, the estimator cannot be computed with sampling.\n                # In this problem setting, B/c1 = 50, so n1_star  0.\n                # If it were 0, the best one could do is use the known part.\n                estimator_val = p * h_at_0\n\n        # --- Analytic Calculation of Exact Value ---\n        exact_val = 0.0\n        \n        if g_params['type'] == 'exponential':\n            # E[h(Y)] for Y ~ Exp(lambda) and h(x) = x + 1_{x1}\n            # E[Y] + P(Y1) = 1/lambda + exp(-lambda)\n            lam = g_params['lambda']\n            E_hY = 1.0/lam + np.exp(-lam)\n        elif g_params['type'] == 'gamma':\n            # E[Y^2] for Y ~ Gamma(k, theta) = theta^2 * k * (k+1)\n            k_shape = g_params['k']\n            theta_scale = g_params['theta']\n            E_hY = (theta_scale**2) * k_shape * (k_shape + 1)\n        elif g_params['type'] == 'lognormal':\n            # E[1_{Y2}] for Y ~ Lognormal(mu, sigma) = P(Y2)\n            # P(Y2) = 1 - Phi((ln(2)-mu)/sigma)\n            mu_ln = g_params['mu']\n            sigma_ln = g_params['sigma']\n            E_hY = 1.0 - norm.cdf((np.log(2) - mu_ln) / sigma_ln)\n\n        if p == 1:\n            exact_val = h_at_0\n        else:\n            exact_val = p * h_at_0 + (1 - p) * E_hY\n        \n        results.extend([n0_star, n1_star, estimator_val, exact_val])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.7f}' if isinstance(x, float) else str(x) for x in results)}]\")\n\nsolve()\n```"
        }
    ]
}