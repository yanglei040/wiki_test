{
    "hands_on_practices": [
        {
            "introduction": "To truly master a statistical tool, one must look under the hood. This first exercise  moves beyond formulaic application, guiding you to derive the delta method's core results directly from first principles. By using a Taylor expansion, you will not only approximate the variance of a transformed estimator but also uncover how the function's curvature introduces statistical bias.",
            "id": "3352081",
            "problem": "Consider a Monte Carlo setting where a positive quantity is estimated by sample averaging. Let $\\{X_{i}\\}_{i=1}^{n}$ be independent and identically distributed random variables, and let $h(X)$ be a measurable, almost surely positive integrand with finite variance. Define the positive parameter $\\theta \\equiv \\mathbb{E}[h(X)]  0$ and its Monte Carlo estimator $\\hat{\\theta}_{n} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} h(X_{i})$. Denote $\\sigma_{h}^{2} \\equiv \\mathrm{Var}(h(X)) \\in (0,\\infty)$. Let $g(x) = \\ln x$ be the natural logarithm.\n\nStarting only from the Central Limit Theorem (CLT) and a second-order Taylor expansion, and without invoking any pre-stated delta-method formulas, do the following:\n\n- Derive a first-order approximation to $\\mathrm{Var}(g(\\hat{\\theta}_{n}))$ in closed form as $n \\to \\infty$, expressed solely in terms of $\\theta$, $\\sigma_{h}^{2}$, and $n$.\n- Explain how the curvature of the logarithm (its second derivative) determines the sign and leading order of the bias of $g(\\hat{\\theta}_{n})$ as an estimator of $g(\\theta)$.\n\nYour final answer must be a single closed-form analytic expression for the first-order approximation to $\\mathrm{Var}(g(\\hat{\\theta}_{n}))$ in terms of $\\theta$, $\\sigma_{h}^{2}$, and $n$. No numerical evaluation is required. Do not include any units.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective, situated within the standard framework of statistical inference and Monte Carlo methods. All necessary information is provided, and the problem is free of contradictions or ambiguities.\n\nThis problem requires two main tasks: first, to derive a first-order approximation for the variance of a transformed Monte Carlo estimator, and second, to explain the bias of this transformed estimator.\n\nLet the sequence of random variables $\\{h(X_{i})\\}_{i=1}^{n}$ be denoted by $\\{Y_{i}\\}_{i=1}^{n}$. By the problem statement, these are independent and identically distributed (i.i.d.) random variables with expectation $\\mathbb{E}[Y_{i}] = \\theta$ and variance $\\mathrm{Var}(Y_{i}) = \\sigma_{h}^{2}$. The Monte Carlo estimator is the sample mean $\\hat{\\theta}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i}$.\n\nThe expectation of the estimator is $\\mathbb{E}[\\hat{\\theta}_{n}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} Y_{i}\\right] = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}[Y_{i}] = \\frac{1}{n}(n\\theta) = \\theta$, which shows that $\\hat{\\theta}_{n}$ is an unbiased estimator of $\\theta$.\nThe variance of the estimator, due to the independence of the $Y_i$, is $\\mathrm{Var}(\\hat{\\theta}_{n}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} Y_{i}\\right) = \\frac{1}{n^{2}}\\sum_{i=1}^{n} \\mathrm{Var}(Y_{i}) = \\frac{1}{n^{2}}(n\\sigma_{h}^{2}) = \\frac{\\sigma_{h}^{2}}{n}$.\n\nAccording to the Central Limit Theorem (CLT), as $n \\to \\infty$, the distribution of the normalized sample mean converges to a standard normal distribution. A common statement of the CLT is:\n$$ \\sqrt{n}(\\hat{\\theta}_{n} - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma_{h}^{2}) $$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution. This implies that for large $n$, $\\hat{\\theta}_{n}$ is approximately normally distributed, with mean $\\theta$ and variance $\\frac{\\sigma_{h}^{2}}{n}$.\n\n**Part 1: Derivation of the Variance Approximation**\n\nWe are asked to find an approximation for $\\mathrm{Var}(g(\\hat{\\theta}_{n}))$, where $g(x) = \\ln x$. The derivation must start from first principles, namely the CLT and a Taylor expansion. We perform a first-order Taylor series expansion of the function $g(\\hat{\\theta}_{n})$ around the point $\\theta = \\mathbb{E}[\\hat{\\theta}_{n}]$.\nA first-order expansion is given by:\n$$ g(\\hat{\\theta}_{n}) = g(\\theta) + g'(\\theta)(\\hat{\\theta}_{n} - \\theta) + R_{1} $$\nwhere $R_{1}$ is the remainder term, which is of a smaller order than the linear term for $\\hat{\\theta}_{n}$ close to $\\theta$. For $g(x) = \\ln x$, the first derivative is $g'(x) = \\frac{1}{x}$. Evaluating this at $x = \\theta$ gives $g'(\\theta) = \\frac{1}{\\theta}$.\nThus, the first-order approximation is:\n$$ g(\\hat{\\theta}_{n}) \\approx g(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta) $$\nWe are interested in the variance of this expression. Using the properties of variance, where $\\mathrm{Var}(aZ+b) = a^{2}\\mathrm{Var}(Z)$ for a constant $a$ and a random variable $Z$:\n$$ \\mathrm{Var}(g(\\hat{\\theta}_{n})) \\approx \\mathrm{Var}\\left(g(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta)\\right) $$\nSince $g(\\theta)=\\ln(\\theta)$ and $\\theta$ are constants, they do not contribute to the variance. The expression simplifies to:\n$$ \\mathrm{Var}(g(\\hat{\\theta}_{n})) \\approx \\mathrm{Var}\\left(\\frac{1}{\\theta}\\hat{\\theta}_{n}\\right) = \\left(\\frac{1}{\\theta}\\right)^{2} \\mathrm{Var}(\\hat{\\theta}_{n}) $$\nSubstituting the known variance $\\mathrm{Var}(\\hat{\\theta}_{n}) = \\frac{\\sigma_{h}^{2}}{n}$:\n$$ \\mathrm{Var}(g(\\hat{\\theta}_{n})) \\approx \\frac{1}{\\theta^{2}} \\left( \\frac{\\sigma_{h}^{2}}{n} \\right) = \\frac{\\sigma_{h}^{2}}{n\\theta^{2}} $$\nThis is the required first-order approximation for the variance of $g(\\hat{\\theta}_{n})$ as $n \\to \\infty$.\n\n**Part 2: Bias and the Role of Curvature**\n\nTo analyze the bias of $g(\\hat{\\theta}_{n})$ as an estimator of $g(\\theta) = \\ln \\theta$, we must approximate its expectation, $\\mathbb{E}[g(\\hat{\\theta}_{n})]$. A first-order Taylor expansion is insufficient, as $\\mathbb{E}[\\hat{\\theta}_{n} - \\theta] = 0$. We must proceed to the second-order Taylor expansion:\n$$ g(\\hat{\\theta}_{n}) = g(\\theta) + g'(\\theta)(\\hat{\\theta}_{n} - \\theta) + \\frac{1}{2}g''(\\theta)(\\hat{\\theta}_{n} - \\theta)^{2} + R_{2} $$\nFor $g(x) = \\ln x$, the second derivative is $g''(x) = -\\frac{1}{x^{2}}$. Evaluating at $x = \\theta$ gives $g''(\\theta) = -\\frac{1}{\\theta^{2}}$. Substituting the derivatives into the expansion yields:\n$$ g(\\hat{\\theta}_{n}) \\approx \\ln(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta) - \\frac{1}{2\\theta^{2}}(\\hat{\\theta}_{n} - \\theta)^{2} $$\nTaking the expectation of both sides and using the linearity of expectation:\n$$ \\mathbb{E}[g(\\hat{\\theta}_{n})] \\approx \\mathbb{E}\\left[\\ln(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta) - \\frac{1}{2\\theta^{2}}(\\hat{\\theta}_{n} - \\theta)^{2}\\right] $$\n$$ \\mathbb{E}[g(\\hat{\\theta}_{n})] \\approx \\ln(\\theta) + \\frac{1}{\\theta}\\mathbb{E}[\\hat{\\theta}_{n} - \\theta] - \\frac{1}{2\\theta^{2}}\\mathbb{E}[(\\hat{\\theta}_{n} - \\theta)^{2}] $$\nWe evaluate the expectation terms:\n- The first term is a constant: $\\mathbb{E}[\\ln(\\theta)] = \\ln(\\theta)$.\n- The second term is zero because $\\hat{\\theta}_{n}$ is an unbiased estimator of $\\theta$: $\\mathbb{E}[\\hat{\\theta}_{n} - \\theta] = \\mathbb{E}[\\hat{\\theta}_{n}] - \\theta = \\theta - \\theta = 0$.\n- The third term's expectation is the definition of the variance of $\\hat{\\theta}_{n}$: $\\mathbb{E}[(\\hat{\\theta}_{n} - \\theta)^{2}] = \\mathrm{Var}(\\hat{\\theta}_{n}) = \\frac{\\sigma_{h}^{2}}{n}$.\n\nSubstituting these results back into the approximation for the expectation:\n$$ \\mathbb{E}[g(\\hat{\\theta}_{n})] \\approx \\ln(\\theta) - \\frac{1}{2\\theta^{2}}\\left(\\frac{\\sigma_{h}^{2}}{n}\\right) $$\nThe bias of an estimator is defined as $\\mathrm{Bias}(g(\\hat{\\theta}_{n})) = \\mathbb{E}[g(\\hat{\\theta}_{n})] - g(\\theta)$. Therefore, the leading-order term for the bias is:\n$$ \\mathrm{Bias}(g(\\hat{\\theta}_{n})) \\approx -\\frac{\\sigma_{h}^{2}}{2n\\theta^{2}} $$\nThe bias is proportional to the second derivative of the function $g$, which represents its curvature. For a general function $g$, the leading-order bias term is $\\frac{1}{2}g''(\\theta)\\mathrm{Var}(\\hat{\\theta}_{n})$. The sign of the bias is thus determined by the sign of the curvature $g''(\\theta)$. For the natural logarithm function, $g''(x) = -1/x^{2}$, which is strictly negative for all $x$ in its domain $(0, \\infty)$. Since $\\theta  0$, the curvature at $\\theta$, $g''(\\theta) = -1/\\theta^{2}$, is negative. Given that $\\sigma_{h}^{2}  0$ and $n \\ge 1$, the entire bias term is negative.\nThis demonstrates that the concavity of the logarithm function (negative curvature) causes the estimator $\\ln(\\hat{\\theta}_{n})$ to systematically underestimate the true value $\\ln(\\theta)$. This result is a quantitative refinement of Jensen's inequality, which for a concave function $g$ states that $\\mathbb{E}[g(Z)] \\le g(\\mathbb{E}[Z])$. Here, with $Z=\\hat{\\theta}_{n}$, we get $\\mathbb{E}[\\ln(\\hat{\\theta}_{n})] \\le \\ln(\\mathbb{E}[\\hat{\\theta}_{n}]) = \\ln(\\theta)$. The Taylor expansion quantifies this inequality to leading order in $1/n$.",
            "answer": "$$\\boxed{\\frac{\\sigma_{h}^{2}}{n\\theta^{2}}}$$"
        },
        {
            "introduction": "In many practical Monte Carlo simulations, the function of interest is an expectation that lacks a closed-form expression, making its derivatives analytically intractable. This exercise  simulates this common research scenario, tasking you with implementing a fully numerical delta method. You will estimate the required Jacobian using finite differences and common random numbers, and in doing so, gain direct experience with the sources of error in practical applications.",
            "id": "3352079",
            "problem": "Let $\\theta \\in \\mathbb{R}^2$ denote a parameter vector with coordinates $\\theta = (\\mu,\\sigma)$, where $\\mu \\in \\mathbb{R}$ and $\\sigma \\in (0,\\infty)$. Consider the mapping $g:\\mathbb{R}^2 \\to \\mathbb{R}^2$ defined by\n$$\ng(\\theta)\n=\n\\begin{bmatrix}\n\\mathbb{E}_{\\theta}[X^3] \\\\\n\\mathbb{E}_{\\theta}[\\sin(X)]\n\\end{bmatrix},\n\\quad\nX \\sim \\mathcal{N}(\\mu,\\sigma^2),\n$$\nwith the angle of the sine function in radians.\n\nSuppose $\\hat{\\theta}$ is an estimator of $\\theta$ such that, for large sample size, its distribution is approximately multivariate normal with mean $\\theta$ and covariance matrix $V_{\\theta}$ (a given, fixed $2 \\times 2$ positive semidefinite matrix).\n\nYou will implement a fully numerical Monte Carlo procedure to estimate the Jacobian $J_g(\\theta)$ (the $2 \\times 2$ matrix of first partial derivatives of $g$ at $\\theta$) using finite differences where the mapping $g$ is itself evaluated by Monte Carlo integration. You will then use this estimated Jacobian in a delta method variance calculation and quantify the effect of Jacobian approximation error on the resulting delta method covariance estimate.\n\nFundamental base to be used:\n- The mapping $g(\\theta)$ is defined by expectations under a normal law and admits Monte Carlo approximations as sample averages by the Law of Large Numbers.\n- The Jacobian $J_g(\\theta)$ can be approximated by symmetric finite differences with step size $h$.\n- The delta method arises from a first-order Taylor expansion of $g(\\hat{\\theta})$ around $\\theta$ and the asymptotic normality of $\\hat{\\theta}$.\n\nYour tasks are:\n1. Implement a Monte Carlo estimator of $g(\\theta)$ for any $\\theta = (\\mu,\\sigma)$ as follows. Draw $M$ independent standard normal samples $Z_1,\\dots,Z_M \\sim \\mathcal{N}(0,1)$, and define $X_i(\\mu,\\sigma) = \\mu + \\sigma Z_i$ for $i \\in \\{1,\\dots,M\\}$. Approximate $g(\\theta)$ by the sample averages\n$$\n\\widehat{g}_1(\\theta) = \\frac{1}{M}\\sum_{i=1}^M X_i(\\mu,\\sigma)^3, \n\\quad\n\\widehat{g}_2(\\theta) = \\frac{1}{M}\\sum_{i=1}^M \\sin\\bigl(X_i(\\mu,\\sigma)\\bigr).\n$$\n2. Implement a symmetric finite-difference approximation to each column of $J_g(\\theta)$ using the same underlying standard normal draws to reduce variance (common random numbers). For the $\\mu$-direction, use\n$$\n\\widehat{J}_{\\cdot,1}(\\theta;h,M)\n=\n\\frac{\\widehat{g}(\\mu+h,\\sigma) - \\widehat{g}(\\mu-h,\\sigma)}{2h},\n$$\nand for the $\\sigma$-direction, use\n$$\n\\widehat{J}_{\\cdot,2}(\\theta;h,M)\n=\n\\frac{\\widehat{g}(\\mu,\\sigma+h) - \\widehat{g}(\\mu,\\sigma-h)}{2h}.\n$$\nUse the same $Z_1,\\dots,Z_M$ to compute all four evaluations $\\widehat{g}(\\mu\\pm h,\\sigma)$ and $\\widehat{g}(\\mu,\\sigma\\pm h)$.\n3. Given $V_{\\theta}$, construct the delta method covariance estimate\n$$\n\\widehat{\\Sigma}_g(\\theta;h,M) = \\widehat{J}_g(\\theta;h,M)\\, V_{\\theta}\\, \\widehat{J}_g(\\theta;h,M)^{\\top}.\n$$\n4. For assessment, derive the analytic Jacobian $J_g(\\theta)$ and the corresponding analytic delta method covariance\n$$\n\\Sigma_g(\\theta) = J_g(\\theta)\\, V_{\\theta}\\, J_g(\\theta)^{\\top}.\n$$\n5. For each test case specified below, compute the relative Frobenius error\n$$\n\\mathrm{Err}(\\theta,h,M,V_{\\theta})\n=\n\\frac{\\left\\| \\widehat{\\Sigma}_g(\\theta;h,M) - \\Sigma_g(\\theta) \\right\\|_F}{\\left\\| \\Sigma_g(\\theta) \\right\\|_F}.\n$$\n\nAngle unit requirement: all trigonometric arguments are in radians.\n\nTest suite:\n- Case $1$: $\\mu = 0.5$, $\\sigma = 1.2$, \n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.04  0.01 \\\\\n0.01  0.09\n\\end{bmatrix},\n$$\n$h = 10^{-3}$, $M = 50000$, seed $= 12345$.\n- Case $2$: $\\mu = 0.5$, $\\sigma = 1.2$, \n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.04  0.01 \\\\\n0.01  0.09\n\\end{bmatrix},\n$$\n$h = 10^{-1}$, $M = 50000$, seed $= 67890$.\n- Case $3$: $\\mu = 0.5$, $\\sigma = 1.2$, \n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.04  0.01 \\\\\n0.01  0.09\n\\end{bmatrix},\n$$\n$h = 10^{-5}$, $M = 2000$, seed $= 13579$.\n- Case $4$: $\\mu = 1.5$, $\\sigma = 0.5$, \n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.01  -0.003 \\\\\n-0.003  0.02\n\\end{bmatrix},\n$$\n$h = 5 \\times 10^{-4}$, $M = 200000$, seed $= 24680$.\n\nRequired final output format: Your program should produce a single line of output containing the four relative errors for the four cases as a comma-separated list enclosed in square brackets, for example\n$$\n[\\mathrm{Err}_1,\\mathrm{Err}_2,\\mathrm{Err}_3,\\mathrm{Err}_4].\n$$\nEach entry must be a real number.",
            "solution": "The problem has been validated and is deemed sound. It is a well-posed, scientifically grounded problem in computational statistics that is free of contradictions, ambiguities, or pseudo-scientific claims. All necessary data and definitions for a unique, reproducible solution are provided.\n\nThe primary objective is to quantify the error in the delta method's covariance matrix approximation when the underlying Jacobian is estimated numerically. This involves a two-layer Monte Carlo procedure: the function $g(\\theta)$ is itself an expectation estimated by Monte Carlo, and its Jacobian $J_g(\\theta)$ is then approximated using finite differences on this Monte Carlo estimator. The error analysis compares this fully numerical result against the analytical ground truth.\n\nThe solution proceeds in the following steps:\n1.  Derivation of the analytic expressions for the mapping $g(\\theta)$ and its Jacobian $J_g(\\theta)$.\n2.  Implementation of the numerical estimators for $g(\\theta)$ and $J_g(\\theta)$ as specified.\n3.  Computation of both the analytic and estimated delta method covariance matrices.\n4.  Calculation of the specified relative Frobenius error for each test case.\n\n**1. Analytic Derivations**\n\nLet $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The parameter vector is $\\theta = (\\mu, \\sigma)$. The mapping is $g(\\theta) = [\\mathbb{E}_{\\theta}[X^3], \\mathbb{E}_{\\theta}[\\sin(X)]]^{\\top}$.\n\nFirst component, $g_1(\\theta) = \\mathbb{E}[X^3]$:\nThe third central moment of a normal distribution is $0$. The relationship between raw moments $m_k = \\mathbb{E}[X^k]$ and central moments is given by the centered variable $X-\\mu$. A more direct approach is to use the moment-generating function $M_X(t) = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$ or to expand $X=\\mu+\\sigma Z$ where $Z \\sim \\mathcal{N}(0,1)$.\nUsing the latter, $\\mathbb{E}[X^3] = \\mathbb{E}[(\\mu + \\sigma Z)^3] = \\mathbb{E}[\\mu^3 + 3\\mu^2\\sigma Z + 3\\mu\\sigma^2 Z^2 + \\sigma^3 Z^3]$.\nBy linearity of expectation and using the moments of the standard normal distribution $\\mathbb{E}[Z]=0$, $\\mathbb{E}[Z^2]=1$, and $\\mathbb{E}[Z^3]=0$, we get:\n$g_1(\\mu, \\sigma) = \\mu^3 + 3\\mu\\sigma^2 \\mathbb{E}[Z^2] = \\mu^3 + 3\\mu\\sigma^2$.\n\nSecond component, $g_2(\\theta) = \\mathbb{E}[\\sin(X)]$:\nWe compute this by considering the characteristic function of $X$, $\\phi_X(t) = \\mathbb{E}[e^{itX}] = \\exp(i\\mu t - \\frac{1}{2}\\sigma^2 t^2)$.\nUsing Euler's formula, $\\sin(X) = \\mathrm{Im}(e^{iX})$, and the linearity of expectation:\n$\\mathbb{E}[\\sin(X)] = \\mathbb{E}[\\mathrm{Im}(e^{iX})] = \\mathrm{Im}(\\mathbb{E}[e^{iX}])$.\n$\\mathbb{E}[e^{iX}]$ is $\\phi_X(1) = \\exp(i\\mu - \\frac{1}{2}\\sigma^2) = e^{-\\sigma^2/2}e^{i\\mu} = e^{-\\sigma^2/2}(\\cos(\\mu) + i\\sin(\\mu))$.\nThe imaginary part is:\n$g_2(\\mu, \\sigma) = \\sin(\\mu) e^{-\\sigma^2/2}$.\n\nThe analytic mapping is:\n$g(\\theta) = \\begin{bmatrix} \\mu^3 + 3\\mu\\sigma^2 \\\\ \\sin(\\mu) e^{-\\sigma^2/2} \\end{bmatrix}$.\n\nThe analytic Jacobian $J_g(\\theta)$ is the matrix of first partial derivatives of $g$ with respect to $\\mu$ and $\\sigma$:\n$J_g(\\theta) = \\begin{bmatrix} \\frac{\\partial g_1}{\\partial \\mu}  \\frac{\\partial g_1}{\\partial \\sigma} \\\\ \\frac{\\partial g_2}{\\partial \\mu}  \\frac{\\partial g_2}{\\partial \\sigma} \\end{bmatrix}$.\nThe partial derivatives are:\n$\\frac{\\partial g_1}{\\partial \\mu} = 3\\mu^2 + 3\\sigma^2$\n$\\frac{\\partial g_1}{\\partial \\sigma} = 6\\mu\\sigma$\n$\\frac{\\partial g_2}{\\partial \\mu} = \\cos(\\mu) e^{-\\sigma^2/2}$\n$\\frac{\\partial g_2}{\\partial \\sigma} = \\sin(\\mu) \\cdot e^{-\\sigma^2/2} \\cdot (-\\sigma) = -\\sigma \\sin(\\mu) e^{-\\sigma^2/2}$\n\nThus, the analytic Jacobian is:\n$$\nJ_g(\\theta) = \\begin{bmatrix} 3\\mu^2 + 3\\sigma^2  6\\mu\\sigma \\\\ \\cos(\\mu) e^{-\\sigma^2/2}  -\\sigma \\sin(\\mu) e^{-\\sigma^2/2} \\end{bmatrix}\n$$\n\n**2. Numerical Estimation Procedure**\n\nThe implementation will follow the specified numerical methods.\n\nA function, `g_hat(mu, sigma, Z)`, estimates $g(\\theta)$ for given parameters $\\mu, \\sigma$ and a pre-generated array of $M$ standard normal samples $Z = (Z_1, \\dots, Z_M)$. It computes $X_i = \\mu + \\sigma Z_i$ and then returns the sample means of $X_i^3$ and $\\sin(X_i)$.\n\nA second function, `J_hat(mu, sigma, h, Z)`, estimates the Jacobian $J_g(\\theta)$. It uses the symmetric finite difference formulas provided. Crucially, it employs the common random numbers (CRN) technique by passing the same array of samples `Z` to each call of `g_hat`:\n$$\n\\widehat{J}_{\\cdot,1} = \\frac{\\text{g\\_hat}(\\mu+h, \\sigma, Z) - \\text{g\\_hat}(\\mu-h, \\sigma, Z)}{2h}\n$$\n$$\n\\widehat{J}_{\\cdot,2} = \\frac{\\text{g\\_hat}(\\mu, \\sigma+h, Z) - \\text{g\\_hat}(\\mu, \\sigma-h, Z)}{2h}\n$$\nThese two column vectors are then assembled into the $2 \\times 2$ estimated Jacobian matrix $\\widehat{J}_g(\\theta; h, M)$.\n\n**3. Delta Method and Error Calculation**\n\nFor each test case, we compute two covariance matrices:\nThe analytic delta method covariance: $\\Sigma_g(\\theta) = J_g(\\theta)\\, V_{\\theta}\\, J_g(\\theta)^{\\top}$, using the analytically derived $J_g(\\theta)$.\nThe estimated delta method covariance: $\\widehat{\\Sigma}_g(\\theta;h,M) = \\widehat{J}_g(\\theta;h,M)\\, V_{\\theta}\\, \\widehat{J}_g(\\theta;h,M)^{\\top}$, using the numerically estimated $\\widehat{J}_g$.\n\nThe final error is calculated as the relative Frobenius norm of the difference between these two matrices:\n$$\n\\mathrm{Err} = \\frac{\\left\\| \\widehat{\\Sigma}_g(\\theta;h,M) - \\Sigma_g(\\theta) \\right\\|_F}{\\left\\| \\Sigma_g(\\theta) \\right\\|_F}\n$$\nwhere $\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}$.\n\n**4. Execution for Test Cases**\n\nA loop iterates through each of the four test cases. In each iteration:\n1.  The parameters $(\\mu, \\sigma, V_{\\theta}, h, M, \\text{seed})$ are extracted.\n2.  The random number generator is seeded.\n3.  $M$ standard normal samples are drawn.\n4.  $\\Sigma_g(\\theta)$ and $\\widehat{\\Sigma}_g(\\theta;h,M)$ are computed as described above.\n5.  The relative error $\\mathrm{Err}$ is calculated and stored.\n\nFinally, the list of four error values is formatted and printed as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef g_hat_mc(mu, sigma, z_samples):\n    \"\"\"\n    Computes the Monte Carlo estimate of the mapping g(theta).\n    \n    Args:\n        mu (float): The mean parameter.\n        sigma (float): The standard deviation parameter.\n        z_samples (np.ndarray): An array of standard normal random samples.\n    \n    Returns:\n        np.ndarray: A 2-element array containing the estimates of E[X^3] and E[sin(X)].\n    \"\"\"\n    x_samples = mu + sigma * z_samples\n    g1_hat = np.mean(x_samples**3)\n    g2_hat = np.mean(np.sin(x_samples))  # np.sin uses radians\n    return np.array([g1_hat, g2_hat])\n\ndef J_g_hat_mc(mu, sigma, h, z_samples):\n    \"\"\"\n    Computes the finite-difference estimate of the Jacobian of g(theta)\n    using common random numbers.\n    \n    Args:\n        mu (float): The mean parameter.\n        sigma (float): The standard deviation parameter.\n        h (float): The finite difference step size.\n        z_samples (np.ndarray): An array of standard normal random samples.\n    \n    Returns:\n        np.ndarray: A 2x2 matrix representing the estimated Jacobian.\n    \"\"\"\n    # First column: partial derivatives with respect to mu\n    g_plus_mu = g_hat_mc(mu + h, sigma, z_samples)\n    g_minus_mu = g_hat_mc(mu - h, sigma, z_samples)\n    J_col1 = (g_plus_mu - g_minus_mu) / (2 * h)\n    \n    # Second column: partial derivatives with respect to sigma\n    g_plus_sigma = g_hat_mc(mu, sigma + h, z_samples)\n    g_minus_sigma = g_hat_mc(mu, sigma - h, z_samples)\n    J_col2 = (g_plus_sigma - g_minus_sigma) / (2 * h)\n    \n    # Assemble the Jacobian matrix from its columns\n    return np.stack([J_col1, J_col2], axis=1)\n\ndef J_g_analytic(mu, sigma):\n    \"\"\"\n    Computes the analytic Jacobian of g(theta).\n    \n    Args:\n        mu (float): The mean parameter.\n        sigma (float): The standard deviation parameter.\n    \n    Returns:\n        np.ndarray: A 2x2 matrix representing the analytic Jacobian.\n    \"\"\"\n    J11 = 3 * mu**2 + 3 * sigma**2\n    J12 = 6 * mu * sigma\n    \n    exp_term = np.exp(-0.5 * sigma**2)\n    J21 = np.cos(mu) * exp_term\n    J22 = -sigma * np.sin(mu) * exp_term\n    \n    return np.array([[J11, J12], [J21, J22]])\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute the errors.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Base case\n        {'mu': 0.5, 'sigma': 1.2, 'V_theta': [[0.04, 0.01], [0.01, 0.09]], 'h': 1e-3, 'M': 50000, 'seed': 12345},\n        # Case 2: Large finite-difference step h\n        {'mu': 0.5, 'sigma': 1.2, 'V_theta': [[0.04, 0.01], [0.01, 0.09]], 'h': 1e-1, 'M': 50000, 'seed': 67890},\n        # Case 3: Small Monte Carlo sample size M\n        {'mu': 0.5, 'sigma': 1.2, 'V_theta': [[0.04, 0.01], [0.01, 0.09]], 'h': 1e-5, 'M': 2000, 'seed': 13579},\n        # Case 4: Different parameter set, large M\n        {'mu': 1.5, 'sigma': 0.5, 'V_theta': [[0.01, -0.003], [-0.003, 0.02]], 'h': 5e-4, 'M': 200000, 'seed': 24680},\n    ]\n\n    results = []\n    for case in test_cases:\n        mu = case['mu']\n        sigma = case['sigma']\n        V_theta = np.array(case['V_theta'])\n        h = case['h']\n        M = case['M']\n        seed = case['seed']\n\n        # --- Analytic \"Ground Truth\" Calculation ---\n        J_analytic = J_g_analytic(mu, sigma)\n        Sigma_g_analytic = J_analytic @ V_theta @ J_analytic.T\n        \n        # --- Numerical Estimation ---\n        # 1. Set up the random number generator and draw samples\n        rng = np.random.default_rng(seed)\n        z_samples = rng.standard_normal(M)\n        \n        # 2. Estimate the Jacobian using Monte Carlo and finite differences\n        J_hat = J_g_hat_mc(mu, sigma, h, z_samples)\n        \n        # 3. Compute the estimated delta method covariance matrix\n        Sigma_g_hat = J_hat @ V_theta @ J_hat.T\n\n        # --- Error Quantification ---\n        # Compute the relative Frobenius error\n        numerator = np.linalg.norm(Sigma_g_hat - Sigma_g_analytic, 'fro')\n        denominator = np.linalg.norm(Sigma_g_analytic, 'fro')\n        \n        # Denominator should not be zero for the given test cases\n        if denominator == 0:\n            # This case is unlikely here but is good practice to handle.\n            # If numerator is also 0, error is 0. Otherwise, error is infinite.\n            relative_error = 0.0 if numerator == 0.0 else np.inf\n        else:\n            relative_error = numerator / denominator\n        \n        results.append(relative_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The delta method, while powerful, is based on a linear approximation that is not always adequate. This final practice  focuses on building practical judgment by contrasting the delta method with the computationally intensive but often more robust bootstrap. You will analyze scenarios involving high nonlinearity and other challenging conditions to learn when to rely on the delta method and when to turn to alternatives.",
            "id": "3352093",
            "problem": "Consider independent and identically distributed (i.i.d.) observations $X_1,\\dots,X_n$ from a distribution indexed by a scalar parameter $\\theta_0 \\in \\Theta \\subset \\mathbb{R}$. Let $\\hat{\\theta}$ be a consistent estimator of $\\theta_0$ that satisfies the Central Limit Theorem (CLT): there exists $\\sigma^2  0$ such that $\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\Rightarrow \\mathcal{N}(0,\\sigma^2)$. You wish to estimate the variance $\\operatorname{Var}(g(\\hat{\\theta}))$ for a deterministic mapping $g:\\Theta \\to \\mathbb{R}$ that may be highly nonlinear near $\\theta_0$. Two general-purpose approaches are commonly used: the delta method and the bootstrap, where the bootstrap may be either nonparametric (resampling the data) or parametric (simulating from a parametric model).\n\nSelect all statements below that correctly articulate the contrast between the delta method and the bootstrap for estimating $\\operatorname{Var}(g(\\hat{\\theta}))$ when $g$ is highly nonlinear, and that describe conditions under which the bootstrap outperforms the delta method.\n\nA. If $g'(\\theta_0) = 0$ and $g''(\\theta_0) \\neq 0$, the first-order delta method yields a variance estimate equal to $0$, whereas a properly implemented bootstrap captures the nonzero variability of $g(\\hat{\\theta})$ at order $n^{-2}$; in this setting the bootstrap typically outperforms the first-order delta method.\n\nB. When $g$ is continuously differentiable in a neighborhood of $\\theta_0$ with $|g'(\\theta_0)|  \\infty$, and $\\sqrt{n}(\\hat{\\theta} - \\theta_0)$ is asymptotically normal, the delta method variance estimate is first-order correct, and the bootstrap cannot improve the asymptotic order, although it may reduce finite-sample error by implicitly incorporating higher-order curvature.\n\nC. If $g$ has a jump discontinuity at $\\theta_0$, then both the delta method and the usual nonparametric bootstrap provide consistent variance estimates for $g(\\hat{\\theta})$, because resampling mimics the empirical distribution exactly.\n\nD. If $\\theta_0$ lies on the boundary of the parameter space and $g$ accentuates boundary effects (for example $g(\\theta) = 1/\\theta$ with $\\theta_0$ close to $0$), the linearization inherent in the delta method can severely mischaracterize variance, whereas a parametric bootstrap that enforces the boundary constraint can provide more accurate variance estimates in finite samples.\n\nE. In models where $\\hat{\\theta}$ has infinite variance due to heavy tails, a smooth $g$ always yields finite $\\operatorname{Var}(g(\\hat{\\theta}))$, hence both the delta method and bootstrap remain valid variance estimators regardless of tail behavior.",
            "solution": "The problem statement has been validated and is deemed sound. It presents a standard, well-posed question in the field of statistical inference, resting on fundamental and correct principles of asymptotic theory and resampling methods. The premises are clear, consistent, and scientifically grounded.\n\nThe core of the problem is to compare the performance of the delta method and the bootstrap for estimating the variance of a function of an estimator, $\\operatorname{Var}(g(\\hat{\\theta}))$, particularly when the function $g$ is nonlinear.\n\nLet us first state the principles of each method. We are given that $\\hat{\\theta}$ is a consistent estimator for $\\theta_0$ and satisfies $\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\Rightarrow \\mathcal{N}(0, \\sigma^2)$, where $\\Rightarrow$ denotes convergence in distribution. This implies that for large $n$, the distribution of $\\hat{\\theta}$ is approximately $\\mathcal{N}(\\theta_0, \\sigma^2/n)$.\n\nThe **first-order delta method** is based on a first-order Taylor series expansion of $g(\\hat{\\theta})$ around the true parameter $\\theta_0$:\n$$g(\\hat{\\theta}) \\approx g(\\theta_0) + g'(\\theta_0)(\\hat{\\theta} - \\theta_0)$$\nAssuming $g$ is differentiable at $\\theta_0$ with $g'(\\theta_0) \\neq 0$, the variance of $g(\\hat{\\theta})$ is approximated as:\n$$\\operatorname{Var}(g(\\hat{\\theta})) \\approx \\operatorname{Var}(g(\\theta_0) + g'(\\theta_0)(\\hat{\\theta} - \\theta_0)) = [g'(\\theta_0)]^2 \\operatorname{Var}(\\hat{\\theta})$$\nUsing the asymptotic variance of $\\hat{\\theta}$, which is $\\sigma^2/n$, we get the asymptotic variance of $g(\\hat{\\theta})$:\n$$\\operatorname{Var}(g(\\hat{\\theta})) \\approx \\frac{[g'(\\theta_0)]^2 \\sigma^2}{n}$$\nA practical estimator for this variance is obtained by plugging in estimators for $\\theta_0$ and $\\sigma^2$:\n$$\\widehat{\\operatorname{Var}}_{\\Delta}(g(\\hat{\\theta})) = \\frac{[g'(\\hat{\\theta})]^2 \\hat{\\sigma}^2}{n}$$\nThis method relies on a linear approximation of $g$ and the asymptotic normality of $\\hat{\\theta}$.\n\nThe **bootstrap** is a computational, resampling-based method. In its nonparametric form, one draws $B$ resamples $\\{X_1^*, \\dots, X_n^*\\}$ with replacement from the original data $\\{X_1, \\dots, X_n\\}$. For each resample $b \\in \\{1,\\dots,B\\}$, the estimator $\\hat{\\theta}_b^*$ is computed. The bootstrap estimate of the variance of $g(\\hat{\\theta})$ is the sample variance of the transformed bootstrap replicates:\n$$\\widehat{\\operatorname{Var}}_{\\text{boot}}(g(\\hat{\\theta})) = \\frac{1}{B-1} \\sum_{b=1}^{B} \\left( g(\\hat{\\theta}_b^*) - \\frac{1}{B}\\sum_{j=1}^{B} g(\\hat{\\theta}_j^*) \\right)^2$$\nThe bootstrap's core principle is that the relationship between the bootstrap world (resamples from the empirical distribution) and the empirical world (the original sample) mimics the relationship between the empirical world and the true population. It tends to capture higher-order properties of the sampling distribution beyond just the variance, such as skewness, which are ignored by the first-order delta method.\n\nNow, we evaluate each statement.\n\n**A. If $g'(\\theta_0) = 0$ and $g''(\\theta_0) \\neq 0$, the first-order delta method yields a variance estimate equal to $0$, whereas a properly implemented bootstrap captures the nonzero variability of $g(\\hat{\\theta})$ at order $n^{-2}$; in this setting the bootstrap typically outperforms the first-order delta method.**\n\nThe first-order delta method variance estimator is $\\widehat{\\operatorname{Var}}_{\\Delta}(g(\\hat{\\theta})) = [g'(\\hat{\\theta})]^2 \\hat{\\sigma}^2/n$. Since $\\hat{\\theta}$ is a consistent estimator of $\\theta_0$ and $g'$ is presumably continuous, $g'(\\hat{\\theta}) \\to g'(\\theta_0) = 0$. Thus, the variance estimate converges to $0$. This is incorrect, as $g(\\hat{\\theta})$ is not constant.\n\nTo correctly analyze the variability, we must use a second-order Taylor expansion:\n$$g(\\hat{\\theta}) \\approx g(\\theta_0) + g'(\\theta_0)(\\hat{\\theta} - \\theta_0) + \\frac{1}{2}g''(\\theta_0)(\\hat{\\theta} - \\theta_0)^2 = g(\\theta_0) + \\frac{1}{2}g''(\\theta_0)(\\hat{\\theta} - \\theta_0)^2$$\nThis implies that $g(\\hat{\\theta}) - g(\\theta_0)$ is of order $(\\hat{\\theta} - \\theta_0)^2$, which is $O_p(n^{-1})$. The variance is of order $n^{-2}$, specifically $\\operatorname{Var}(g(\\hat{\\theta})) \\approx \\operatorname{Var}\\left(\\frac{1}{2}g''(\\theta_0)(\\hat{\\theta} - \\theta_0)^2\\right) = \\frac{[g''(\\theta_0)]^2}{4}\\operatorname{Var}((\\hat{\\theta} - \\theta_0)^2)$, which can be shown to be proportional to $n^{-2}$.\nA valid bootstrap procedure generates a distribution for $\\hat{\\theta}^*$ that mimics the distribution of $\\hat{\\theta}$. Consequently, the distribution of $g(\\hat{\\theta}^*)$ will have a non-zero variance that correctly reflects the true, non-zero variance of $g(\\hat{\\theta})$, which is of order $n^{-2}$. The bootstrap does not rely on the derivative being non-zero and captures the curvature of $g$. Therefore, the bootstrap dramatically outperforms the first-order delta method, which fails completely.\n\nVerdict: **Correct**\n\n**B. When $g$ is continuously differentiable in a neighborhood of $\\theta_0$ with $|g'(\\theta_0)|  \\infty$, and $\\sqrt{n}(\\hat{\\theta} - \\theta_0)$ is asymptotically normal, the delta method variance estimate is first-order correct, and the bootstrap cannot improve the asymptotic order, although it may reduce finite-sample error by implicitly incorporating higher-order curvature.**\n\nUnder these \"standard\" conditions, the first-order delta method works. The variance is of order $n^{-1}$, and the delta method provides a consistent estimator for this variance. This is what \"first-order correct\" means. The bootstrap also provides a consistent estimator for the same asymptotic variance. In this sense, it does not improve the \"asymptotic order\" of the quantity being estimated ($\\operatorname{Var}(g(\\hat{\\theta}))$ is still $O(n^{-1})$). However, bootstrap estimators for variance often have better higher-order asymptotic properties. For instance, the bias of the bootstrap variance estimator may be of a smaller order in $n$ than the bias of the delta method estimator. This theoretical improvement translates to better performance in finite samples. The bootstrap distribution of $\\hat{\\theta}^*$ can capture skewness and other non-normal features of the finite-sample distribution of $\\hat{\\theta}$, which are ignored by the simple normal approximation underlying the delta method. The statement is an accurate and nuanced description of the relative merits of the two methods in the standard case.\n\nVerdict: **Correct**\n\n**C. If $g$ has a jump discontinuity at $\\theta_0$, then both the delta method and the usual nonparametric bootstrap provide consistent variance estimates for $g(\\hat{\\theta})$, because resampling mimics the empirical distribution exactly.**\n\nThe delta method is predicated on the differentiability of $g$ at $\\theta_0$. If $g$ is discontinuous at $\\theta_0$, it cannot be differentiable, and the method is not applicable. Its theoretical foundation is violated.\nThe standard nonparametric bootstrap can also fail for non-smooth functionals. The consistency of the bootstrap requires certain smoothness conditions. If $g$ is discontinuous, the distribution of $g(\\hat{\\theta}^*)$ may not be a good approximation of the distribution of $g(\\hat{\\theta})$. For example, if $\\hat{\\theta}$ is the sample mean $\\bar{X}_n$ and $g$ is the indicator function $I(\\theta  \\theta_0)$, the bootstrap distribution is centered at $g(\\bar{X}_n)$ rather than providing a good estimate of the true sampling variability around $g(\\theta_0)$. The failure of the bootstrap for estimators like the maximum of a uniform distribution is a classic example of this issue. Thus, the statement that *both* methods provide consistent estimates is false.\n\nVerdict: **Incorrect**\n\n**D. If $\\theta_0$ lies on the boundary of the parameter space and $g$ accentuates boundary effects (for example $g(\\theta) = 1/\\theta$ with $\\theta_0$ close to $0$), the linearization inherent in the delta method can severely mischaracterize variance, whereas a parametric bootstrap that enforces the boundary constraint can provide more accurate variance estimates in finite samples.**\n\nWhen the true parameter $\\theta_0$ is on the boundary of the parameter space $\\Theta$, the asymptotic distribution of $\\sqrt{n}(\\hat{\\theta} - \\theta_0)$ is often non-normal (e.g., a mixture of a point mass and a truncated normal distribution). This violates the normality assumption of the standard delta method. Furthermore, if $g$ is highly nonlinear near the boundary, such as $g(\\theta) = 1/\\theta$ for $\\theta_0$ near $0$, the linear approximation central to the delta method is grossly inaccurate over the sampling distribution of $\\hat{\\theta}$, leading to a poor variance estimate.\nA parametric bootstrap, on the other hand, simulates new data from the fitted parametric model $F_{\\hat{\\theta}}$. If the parameter estimation procedure itself respects the boundary constraint (e.g., using a constrained MLE), then the bootstrap replicates $\\hat{\\theta}_b^*$ will also lie within the valid parameter space. The resulting empirical distribution of $g(\\hat{\\theta}_b^*)$ naturally and automatically accounts for both the non-normal sampling distribution of the estimator near the boundary and the severe nonlinearity of the function $g$. This makes it a far more reliable tool in such scenarios.\n\nVerdict: **Correct**\n\n**E. In models where $\\hat{\\theta}$ has infinite variance due to heavy tails, a smooth $g$ always yields finite $\\operatorname{Var}(g(\\hat{\\theta}))$, hence both the delta method and bootstrap remain valid variance estimators regardless of tail behavior.**\n\nThis statement contains multiple falsehoods. First, the premise that a smooth $g$ *always* yields a finite variance for $g(\\hat{\\theta})$ is incorrect. A simple counterexample is $g(\\theta) = \\theta$, where $\\operatorname{Var}(g(\\hat{\\theta})) = \\operatorname{Var}(\\hat{\\theta})$, which is infinite by hypothesis. While a bounded function $g$ (e.g., $g(\\theta) = \\arctan(\\theta)$) would indeed result in a finite variance, the claim is universal and therefore false.\nSecond, the validity of the methods is compromised. The delta method is derived from the Central Limit Theorem resulting in a normal limit with finite variance. If $\\hat{\\theta}$ has infinite variance, this premise is violated, and the delta method is inapplicable. The bootstrap also faces fundamental problems. The consistency of the bootstrap variance estimator for the sample mean, for instance, requires the existence of a finite second moment (i.e., finite variance) in the underlying population. If the statistic of interest has infinite variance, the bootstrap variance estimate will not converge to a meaningful quantity.\n\nVerdict: **Incorrect**",
            "answer": "$$\\boxed{ABD}$$"
        }
    ]
}