{
    "hands_on_practices": [
        {
            "introduction": "At the heart of every stochastic simulation lies a pseudo-random number generator (PRNG), the engine that produces the sequence of numbers driving the model. The quality of this engine is not a given; a poor generator can introduce subtle correlations that invalidate simulation results. This exercise takes you under the hood of the classic Linear Congruential Generator (LCG), challenging you to use fundamental principles from number theory to design a generator with the maximum possible period length, a primary measure of its quality. By engaging with this problem , you will see how abstract mathematical structures directly govern the practical performance of computational tools.",
            "id": "3308890",
            "problem": "Consider the linear congruential generator (LCG) defined by the recurrence\n$$X_{n+1} \\equiv a X_{n} + c \\pmod{m},$$\nwith modulus $m$, multiplier $a$, and increment $c$. The period of the state sequence $\\{X_{n}\\}$ is the smallest positive integer $T$ such that $X_{n+T} \\equiv X_{n} \\pmod{m}$ for all $n$. Assume a hardware-imposed constraint that $c = 0$ and that the modulus is fixed to\n$$m = 2^{11} \\cdot 3^{4} \\cdot 5^{2}.$$\nYou may choose the multiplier $a$ and the initial seed $X_{0}$, subject to the constraints $0  a  m$, $0  X_{0}  m$, and $\\gcd(X_{0}, m) = 1$.\n\nUsing only fundamental definitions and well-tested facts from number theory and modular arithmetic, construct a choice of $a$ and $X_{0}$ that maximizes the period of the LCG sequence $\\{X_{n}\\}$ under these constraints. Justify rigorously why your construction achieves the maximal possible period with this modulus by analyzing the multiplicative structure modulo $m$ and any necessary decompositions. Then compute the maximal achievable period length as a single integer.\n\nYour final answer must be the maximal period length. Do not provide intermediate values. Do not include units. No rounding is required.",
            "solution": "The problem as stated is valid. It is a well-posed problem in number theory, directly applicable to the analysis of pseudorandom number generators, specifically multiplicative congruential generators. The problem is scientifically grounded, objective, and contains all necessary information for a rigorous solution. We proceed with the solution.\n\nThe linear congruential generator (LCG) is defined by the recurrence relation\n$$X_{n+1} \\equiv a X_{n} + c \\pmod{m}.$$\nThe problem imposes the constraints $c=0$, yielding a Multiplicative Congruential Generator (MCG):\n$$X_{n+1} \\equiv a X_{n} \\pmod{m}.$$\nBy induction, the sequence is given by $X_{n} \\equiv a^{n} X_{0} \\pmod{m}$. The period of the sequence, $T$, is the smallest positive integer such that $X_{n+T} \\equiv X_{n} \\pmod{m}$ for all non-negative integers $n$. For $n=0$, this condition becomes $X_{T} \\equiv X_{0} \\pmod{m}$, which translates to\n$$a^{T} X_{0} \\equiv X_{0} \\pmod{m}.$$\nWe are given the constraint that $\\gcd(X_{0}, m) = 1$. This implies that $X_{0}$ has a multiplicative inverse modulo $m$. We can multiply both sides of the congruence by $X_{0}^{-1}$ to obtain\n$$a^{T} \\equiv 1 \\pmod{m}.$$\nThis equation reveals that the period $T$ is the multiplicative order of the multiplier $a$ in the group of units modulo $m$, denoted $(\\mathbb{Z}/m\\mathbb{Z})^{\\times}$. The problem of maximizing the period $T$ is therefore equivalent to finding the maximum possible order of an element in this group. This maximum order is known as the exponent of the group.\n\nThe exponent of the group $(\\mathbb{Z}/m\\mathbb{Z})^{\\times}$ is given by the Carmichael function, $\\lambda(m)$. To achieve the maximum period, we must select a multiplier $a$ such that its order modulo $m$ is $\\lambda(m)$. Such an element $a$ is often called a primitive element modulo $m$, although this term is more formally reserved for generators of cyclic groups. The existence of such an element is guaranteed. The choice of the seed $X_{0}$ is any integer satisfying $0  X_{0}  m$ and $\\gcd(X_{0}, m) = 1$; for example, $X_{0}=1$ is a valid choice. The specific value of $X_{0}$ does not affect the length of the period, only the sequence of states itself.\n\nOur task reduces to computing $\\lambda(m)$ for the given modulus $m = 2^{11} \\cdot 3^{4} \\cdot 5^{2}$. The Carmichael function has a key property related to the prime factorization of its argument. If $m = p_1^{k_1} p_2^{k_2} \\cdots p_r^{k_r}$ is the prime factorization of $m$, then\n$$\\lambda(m) = \\text{lcm}(\\lambda(p_1^{k_1}), \\lambda(p_2^{k_2}), \\dots, \\lambda(p_r^{k_r})).$$\nThe function $\\lambda(p^k)$ is defined as follows:\n\\begin{itemize}\n    \\item $\\lambda(1) = 1$\n    \\item $\\lambda(2) = 1$\n    \\item $\\lambda(4) = 2$\n    \\item $\\lambda(2^k) = 2^{k-2}$ for $k \\ge 3$\n    \\item $\\lambda(p^k) = \\phi(p^k) = p^{k-1}(p-1)$ for an odd prime $p$, where $\\phi$ is Euler's totient function.\n\\end{itemize}\nFor our given modulus $m = 2^{11} \\cdot 3^{4} \\cdot 5^{2}$, we apply these rules to each prime power factor:\n\\begin{enumerate}\n    \\item For the factor $2^{11}$: Since the exponent $11 \\ge 3$, we have\n    $$\\lambda(2^{11}) = 2^{11-2} = 2^{9} = 512.$$\n    \\item For the factor $3^{4}$: Since $3$ is an odd prime, we have\n    $$\\lambda(3^{4}) = \\phi(3^{4}) = 3^{4-1}(3-1) = 3^{3} \\cdot 2 = 27 \\cdot 2 = 54.$$\n    \\item For the factor $5^{2}$: Since $5$ is an odd prime, we have\n    $$\\lambda(5^{2}) = \\phi(5^{2}) = 5^{2-1}(5-1) = 5^{1} \\cdot 4 = 20.$$\n\\end{enumerate}\nThe maximal period is the least common multiple (lcm) of these values:\n$$T_{\\text{max}} = \\lambda(m) = \\text{lcm}(\\lambda(2^{11}), \\lambda(3^{4}), \\lambda(5^{2})) = \\text{lcm}(512, 54, 20).$$\nTo compute the lcm, we find the prime factorization of each number:\n\\begin{itemize}\n    \\item $512 = 2^9$\n    \\item $54 = 2 \\cdot 27 = 2^{1} \\cdot 3^{3}$\n    \\item $20 = 4 \\cdot 5 = 2^{2} \\cdot 5^{1}$\n\\end{itemize}\nThe lcm is the product of the highest powers of all prime factors present:\n$$T_{\\text{max}} = \\text{lcm}(2^9, 2^1 \\cdot 3^3, 2^2 \\cdot 5^1) = 2^{\\max(9,1,2)} \\cdot 3^{\\max(0,3,0)} \\cdot 5^{\\max(0,0,1)} = 2^9 \\cdot 3^3 \\cdot 5^1.$$\nNow we compute this value:\n$$T_{\\text{max}} = 512 \\cdot 27 \\cdot 5 = 13824 \\cdot 5 = 69120.$$\nThus, the maximal achievable period is $69120$.\n\nTo justify that this period is achievable, we must demonstrate that a multiplier $a$ exists with this order. By the Chinese Remainder Theorem, the group $(\\mathbb{Z}/m\\mathbb{Z})^{\\times}$ is isomorphic to the direct product of the component groups:\n$$(\\mathbb{Z}/m\\mathbb{Z})^{\\times} \\cong (\\mathbb{Z}/2^{11}\\mathbb{Z})^{\\times} \\times (\\mathbb{Z}/3^{4}\\mathbb{Z})^{\\times} \\times (\\mathbb{Z}/5^{2}\\mathbb{Z})^{\\times}.$$\nAn element $a \\in (\\mathbb{Z}/m\\mathbb{Z})^{\\times}$ corresponds to a tuple $(a_1, a_2, a_3)$ where $a_1 \\in (\\mathbb{Z}/2^{11}\\mathbb{Z})^{\\times}$, $a_2 \\in (\\mathbb{Z}/3^{4}\\mathbb{Z})^{\\times}$, and $a_3 \\in (\\mathbb{Z}/5^{2}\\mathbb{Z})^{\\times}$. The order of $a$ is the lcm of the orders of its components. To maximize this order, we must choose components $a_1, a_2, a_3$ that each have the maximal possible order in their respective groups.\n\\begin{itemize}\n    \\item In $(\\mathbb{Z}/2^{11}\\mathbb{Z})^{\\times}$, an element of maximal order $\\lambda(2^{11}) = 512$ exists (e.g., $a_1=3$ or $a_1=5$).\n    \\item In $(\\mathbb{Z}/3^{4}\\mathbb{Z})^{\\times}$, which is a cyclic group, an element of maximal order $\\lambda(3^4) = 54$ exists (a primitive root modulo $3^4$, e.g., $a_2=2$).\n    \\item In $(\\mathbb{Z}/5^{2}\\mathbb{Z})^{\\times}$, which is also cyclic, an element of maximal order $\\lambda(5^2) = 20$ exists (a primitive root modulo $5^2$, e.g., $a_3=2$).\n\\end{itemize}\nUsing the Chinese Remainder Theorem, we can solve the system of congruences:\n$$a \\equiv a_1 \\pmod{2^{11}}$$\n$$a \\equiv a_2 \\pmod{3^{4}}$$\n$$a \\equiv a_3 \\pmod{5^{2}}$$\nA unique solution for $a$ modulo $m$ exists. The order of this constructed $a$ will be $\\text{lcm}(\\text{ord}(a_1), \\text{ord}(a_2), \\text{ord}(a_3)) = \\text{lcm}(512, 54, 20) = 69120$.\nTherefore, a multiplier $a$ can be constructed to achieve the maximal period. As established, any seed $X_0$ with $\\gcd(X_0,m)=1$ completes the construction. The maximal period length is thus confirmed to be $69120$.",
            "answer": "$$\\boxed{69120}$$"
        },
        {
            "introduction": "Importance Sampling (IS) is a cornerstone of the Monte Carlo toolkit, allowing us to estimate properties of a complex target distribution $\\pi$ by sampling from a simpler proposal distribution $q$. However, its power comes with a critical vulnerability: if the proposal fails to adequately cover the target, the variance of the IS estimator can become infinite, rendering the estimate useless. This practice explores a powerful technique for stabilizing the estimator—weight truncation—which deliberately introduces a small bias to achieve a dramatic reduction in variance. This exercise  will guide you through a rigorous analysis of this bias-variance tradeoff, providing essential insights into the practical art of designing robust Monte Carlo estimators.",
            "id": "3308916",
            "problem": "Consider a target probability density $p$ on $\\mathbb{R}^{d}$ and a proposal probability density $q$ on $\\mathbb{R}^{d}$ such that $p$ is absolutely continuous with respect to $q$ and the importance weight $w(x) = \\frac{p(x)}{q(x)}$ is finite $q$-almost surely. Let $h:\\mathbb{R}^{d}\\to\\mathbb{R}$ be a measurable function with $|h(x)| \\le B$ for all $x\\in\\mathbb{R}^{d}$, where $B\\in(0,\\infty)$ is a known constant, and define the target expectation $\\mu = \\mathbb{E}_{p}[h(X)]$ with $X\\sim p$. By the change-of-measure identity, $\\mu = \\mathbb{E}_{q}[w(X)\\,h(X)]$ with $X\\sim q$. Consider the truncated importance weight $w_{T}(x) = \\min\\{w(x),\\,T\\}$ for a threshold $T \\ge 1$, and the corresponding truncated estimator mean $\\mu_{T} = \\mathbb{E}_{q}[w_{T}(X)\\,h(X)]$.\n\nYou are asked to analyze the effect of truncation on bias and variance in Monte Carlo (MC) importance sampling, starting from first principles and core definitions. In particular:\n\n1. Using only foundational definitions of expectation, bias, and variance, express the bias introduced by truncation and reduce it to an expectation involving the tail of $w$ under $q$.\n\n2. Assume the following polynomial tail condition on the weights: there exist constants $C0$ and $\\beta1$ such that for all $t \\ge T$,\n$$\n\\mathbb{P}_{q}\\!\\big(w(X)  t\\big) \\le C\\,t^{-\\beta}.\n$$\nDerive a closed-form upper bound for the absolute bias $|\\mu - \\mu_{T}|$ in terms of $B, C, \\beta,$ and $T$, using only well-tested facts about expectations and tail probabilities.\n\n3. Briefly, from first principles, bound the per-sample variance of the truncated estimator. That is, give a finite upper bound on $\\mathrm{Var}_{q}\\!\\big(w_{T}(X)\\,h(X)\\big)$ in terms of $B$ and $T$.\n\nExpress your final answer as a single analytic expression for the bias bound derived in part 2. No numerical rounding is required. Define any acronyms on first use; for example, Importance Sampling (IS) and Monte Carlo (MC).",
            "solution": "The problem is valid as it is scientifically grounded in the theory of Monte Carlo (MC) methods, well-posed with sufficient information for a unique solution, and stated objectively. We proceed with the solution.\n\nThe problem asks for an analysis of a truncated Importance Sampling (IS) estimator. We are given a target probability density $p$ and a proposal probability density $q$ on $\\mathbb{R}^{d}$. The importance weight is $w(x) = \\frac{p(x)}{q(x)}$. We wish to estimate the expectation $\\mu = \\mathbb{E}_{p}[h(X)] = \\int_{\\mathbb{R}^d} h(x)p(x)dx$ for a test function $h$ bounded such that $|h(x)| \\le B$. Using a change of measure, this is equivalent to $\\mu = \\mathbb{E}_{q}[w(X)h(X)]$. The truncated estimator introduces the truncated weight $w_{T}(x) = \\min\\{w(x), T\\}$ for some threshold $T \\ge 1$, leading to the biased estimator mean $\\mu_{T} = \\mathbb{E}_{q}[w_{T}(X)h(X)]$.\n\nPart 1: Analysis of the bias.\nThe bias of the truncated estimator is the difference between the true expectation $\\mu$ and the mean of the truncated estimator $\\mu_T$. Using the definitions provided:\n$$\n\\text{Bias} = \\mu - \\mu_{T} = \\mathbb{E}_{q}[w(X)h(X)] - \\mathbb{E}_{q}[w_{T}(X)h(X)]\n$$\nBy the linearity of expectation, we can combine the terms:\n$$\n\\text{Bias} = \\mathbb{E}_{q}\\big[ (w(X) - w_{T}(X)) h(X) \\big]\n$$\nThe term $w(x) - w_{T}(x)$ depends on whether $w(x)$ exceeds the threshold $T$. By definition, $w_{T}(x) = \\min\\{w(x), T\\}$.\nIf $w(x) \\le T$, then $w_{T}(x) = w(x)$, and $w(x) - w_{T}(x) = 0$.\nIf $w(x)  T$, then $w_{T}(x) = T$, and $w(x) - w_{T}(x) = w(x) - T$.\nThis allows us to express the difference using an indicator function, $\\mathbb{I}(A)$, which is $1$ if event $A$ is true and $0$ otherwise:\n$$\nw(x) - w_{T}(x) = (w(x) - T) \\mathbb{I}(w(x)  T)\n$$\nSubstituting this back into the expression for the bias, we obtain:\n$$\n\\text{Bias} = \\mathbb{E}_{q}\\big[ (w(X) - T) h(X) \\mathbb{I}(w(X)  T) \\big]\n$$\nThis expression represents the bias as an expectation under the proposal distribution $q$, specifically related to the \"tail\" of the weight distribution where $w(X)  T$.\n\nPart 2: Upper bound for the absolute bias.\nWe wish to find an upper bound for the absolute bias, $|\\mu - \\mu_{T}|$, using the polynomial tail condition $\\mathbb{P}_{q}(w(X)  t) \\le C t^{-\\beta}$ for $t \\ge T$, where $C  0$ and $\\beta  1$.\nStarting from the expression for the bias:\n$$\n|\\mu - \\mu_{T}| = \\left| \\mathbb{E}_{q}\\big[ (w(X) - T) h(X) \\mathbb{I}(w(X)  T) \\big] \\right|\n$$\nUsing the property that $|\\mathbb{E}[Y]| \\le \\mathbb{E}[|Y|]$ and the fact that $|ab| = |a||b|$:\n$$\n|\\mu - \\mu_{T}| \\le \\mathbb{E}_{q}\\big[ |(w(X) - T) h(X) \\mathbb{I}(w(X)  T)| \\big] = \\mathbb{E}_{q}\\big[ |w(X) - T| \\cdot |h(X)| \\cdot |\\mathbb{I}(w(X)  T)| \\big]\n$$\nIn the domain of the expectation, $w(X)  T$, which implies $|w(X) - T| = w(X) - T$. The indicator function is non-negative, so its absolute value is itself. We are given the uniform bound $|h(x)| \\le B$. Substituting these bounds:\n$$\n|\\mu - \\mu_{T}| \\le \\mathbb{E}_{q}\\big[ (w(X) - T) \\cdot B \\cdot \\mathbb{I}(w(X)  T) \\big]\n$$\nSince $B$ is a positive constant, it can be factored out of the expectation:\n$$\n|\\mu - \\mu_{T}| \\le B \\cdot \\mathbb{E}_{q}\\big[ (w(X) - T) \\mathbb{I}(w(X)  T) \\big]\n$$\nTo evaluate the remaining expectation, we use the identity for the expectation of a non-negative random variable $Y$, which states $\\mathbb{E}[Y] = \\int_{0}^{\\infty} \\mathbb{P}(Y  y) dy$. Let the random variable be $Y = (w(X) - T) \\mathbb{I}(w(X)  T)$. The event $\\{Y  y\\}$ for $y \\ge 0$ is equivalent to the event $\\{w(X) - T  y\\}$ and $\\{w(X)  T\\}$, which simplifies to $\\{w(X)  T + y\\}$.\nTherefore, the expectation is:\n$$\n\\mathbb{E}_{q}\\big[ (w(X) - T) \\mathbb{I}(w(X)  T) \\big] = \\int_{0}^{\\infty} \\mathbb{P}_{q}(w(X)  T + y) dy\n$$\nWe perform a change of variable by letting $t = T + y$, so $y = t - T$ and $dy = dt$. The limits of integration change from $y=0$ to $t=T$ and from $y \\to \\infty$ to $t \\to \\infty$.\n$$\n\\mathbb{E}_{q}\\big[ (w(X) - T) \\mathbb{I}(w(X)  T) \\big] = \\int_{T}^{\\infty} \\mathbb{P}_{q}(w(X)  t) dt\n$$\nNow we apply the given polynomial tail condition, $\\mathbb{P}_{q}(w(X)  t) \\le C t^{-\\beta}$, which holds for $t \\ge T$:\n$$\n\\int_{T}^{\\infty} \\mathbb{P}_{q}(w(X)  t) dt \\le \\int_{T}^{\\infty} C t^{-\\beta} dt\n$$\nWe evaluate the integral:\n$$\n\\int_{T}^{\\infty} C t^{-\\beta} dt = C \\left[ \\frac{t^{-\\beta+1}}{-\\beta+1} \\right]_{T}^{\\infty}\n$$\nSince $\\beta  1$, the exponent $1-\\beta$ is negative. Thus, as $t \\to \\infty$, $t^{1-\\beta} \\to 0$. The integral evaluates to:\n$$\nC \\left( 0 - \\frac{T^{1-\\beta}}{1-\\beta} \\right) = C \\frac{T^{1-\\beta}}{\\beta-1}\n$$\nCombining this result with the bound on the absolute bias:\n$$\n|\\mu - \\mu_{T}| \\le B \\cdot \\frac{C T^{1-\\beta}}{\\beta-1}\n$$\nThis is the desired closed-form upper bound.\n\nPart 3: Upper bound for the per-sample variance.\nWe need to give a finite upper bound on $\\mathrm{Var}_{q}(w_{T}(X)h(X))$. Let $Z(X) = w_{T}(X)h(X)$. The variance is defined as $\\mathrm{Var}_{q}(Z(X)) = \\mathbb{E}_{q}[Z(X)^2] - (\\mathbb{E}_{q}[Z(X)])^2$. A simple upper bound for the variance is the second moment, as variance is non-negative:\n$$\n\\mathrm{Var}_{q}(Z(X)) \\le \\mathbb{E}_{q}[Z(X)^2]\n$$\nLet's analyze the term $Z(x)^2$:\n$$\nZ(x)^2 = (w_{T}(x)h(x))^2 = w_{T}(x)^2 h(x)^2\n$$\nBy definition, $w_{T}(x) = \\min\\{w(x), T\\}$. Since $w(x) = p(x)/q(x) \\ge 0$, we have $0 \\le w_{T}(x) \\le T$. Squaring this gives $w_{T}(x)^2 \\le T^2$.\nWe are also given that $|h(x)| \\le B$, which implies $h(x)^2 \\le B^2$.\nCombining these bounds, we find a uniform bound on $Z(x)^2$:\n$$\nZ(x)^2 = w_{T}(x)^2 h(x)^2 \\le T^2 B^2\n$$\nThis inequality holds for all $x \\in \\mathbb{R}^d$. Therefore, the expectation is also bounded:\n$$\n\\mathbb{E}_{q}[Z(X)^2] = \\mathbb{E}_{q}[w_{T}(X)^2 h(X)^2] \\le \\mathbb{E}_{q}[T^2 B^2] = T^2 B^2\n$$\nThus, we have an upper bound for the variance:\n$$\n\\mathrm{Var}_{q}(w_{T}(X)h(X)) \\le B^2 T^2\n$$\nThis demonstrates that truncation of the importance weights makes the variance of the per-sample estimator finite and bounded, a key motivation for this technique.",
            "answer": "$$\\boxed{\\frac{B C T^{1-\\beta}}{\\beta - 1}}$$"
        },
        {
            "introduction": "When we cannot sample from a target distribution directly, Markov Chain Monte Carlo (MCMC) methods allow us to generate a dependent sequence of samples that converges to the desired distribution. A key theoretical and practical question is: how fast does this convergence happen? This exercise explores the concept of geometric ergodicity—an exponential rate of convergence—by examining a case where it famously fails: a Random-Walk Metropolis algorithm targeting a heavy-tailed distribution like the Student's $t$. By working through this problem , you will uncover how a mismatch in the tail behavior of the target and proposal distributions can have profound consequences for the validity of the Central Limit Theorem and the reliability of our uncertainty estimates.",
            "id": "3308869",
            "problem": "Consider a Random-Walk Metropolis (RWM) Markov chain on the real line with target distribution having density proportional to the standard Student-$t$ distribution with $\\nu0$ degrees of freedom, that is, target density $\\pi(x) \\propto \\left(1 + \\frac{x^{2}}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}$ on $\\mathbb{R}$. The proposal is Gaussian: given the current state $X_{n}=x$, the proposal $Y$ is drawn from $q(\\cdot \\mid x) = \\mathcal{N}(x, \\sigma^2)$ for a fixed $0\\sigma\\infty$, and the standard Metropolis acceptance probability is used. \n\nStarting from the definitions of geometric ergodicity for Markov chains, the form of the Random-Walk Metropolis kernel, and the asymptotic normality of ergodic averages under a central limit theorem (CLT), do the following:\n\n(i) Using the tail behavior of $\\pi$, argue from first principles why geometric ergodicity fails for this RWM chain for any $\\nu0$.\n\n(ii) Let $f(x)=x$. Consider the Monte Carlo estimator of $\\mathbb{E}_{\\pi}[f(X)]$ given by the sample mean of the Markov chain. Using only necessary moment conditions for a CLT of ergodic averages to be possible, determine the smallest degrees-of-freedom threshold $\\nu_{\\star}$ such that, for $\\nu\\nu_{\\star}$, a CLT for the sample mean of $f(X)$ can possibly hold. Provide $\\nu_{\\star}$ as a single real number.\n\n(iii) Briefly explain the impact of the failure of geometric ergodicity on (a) the validity of a CLT for the sample mean of $f(X)$ in the regime $\\nu \\le \\nu_{\\star}$ and $\\nu\\nu_{\\star}$, and (b) the consistency of variance estimation methods such as batch means for estimating the asymptotic variance when they rely on geometric ergodicity.\n\nYour final answer must be the exact value of $\\nu_{\\star}$, written as a single real number. No units are involved, and no numerical rounding is required.",
            "solution": "This problem concerns the long-term behavior of a Random-Walk Metropolis (RWM) algorithm targeting a Student-$t$ distribution. The key feature of this target is its \"heavy tails,\" i.e., polynomial decay, which contrasts with the \"light tails\" (exponential decay) of distributions like the Gaussian.\n\n**(i) Failure of Geometric Ergodicity**\n\nA Markov chain is geometrically ergodic if it converges to its stationary distribution $\\pi$ at an exponential rate. Formally, if $P^n(x, \\cdot)$ is the distribution of $X_n$ starting from $X_0=x$, then geometric ergodicity means there exists a function $M(x)$ and a rate $\\rho \\in (0,1)$ such that $\\| P^n(x, \\cdot) - \\pi(\\cdot) \\|_{\\text{TV}} \\le M(x) \\rho^n$ for all $n$, where $\\|\\cdot\\|_{\\text{TV}}$ is the total variation norm. For RWM algorithms, a crucial factor determining the rate of convergence is the behavior of the target density $\\pi(x)$ in the tails (i.e., for large $|x|$).\n\nThe intuition is that a chain starting far from the main mass of the distribution must be \"pulled back\" towards the center sufficiently strongly. This \"pull\" is induced by the proposal mechanism and the acceptance probability. In an RWM, proposals are local. Given $X_n=x$, a proposal $Y=x+Z\\sigma$ is made, where $Z \\sim \\mathcal{N}(0,1)$. The acceptance probability is $\\alpha(x, Y) = \\min\\left(1, \\frac{\\pi(Y)}{\\pi(x)}\\right)$.\n\nLet's analyze this for large $x  0$. The target density is $\\pi(x) \\propto \\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\frac{\\nu+1}{2})}$. For large $x$, this behaves as $\\pi(x) \\approx C x^{-(\\nu+1)}$ for some constant $C$.\nThe ratio of densities for a proposal $y=x+z$ is:\n$$ \\frac{\\pi(y)}{\\pi(x)} = \\left(\\frac{1+x^2/\\nu}{1+y^2/\\nu}\\right)^{(\\frac{\\nu+1}{2})} \\approx \\left(\\frac{x^2}{y^2}\\right)^{(\\frac{\\nu+1}{2})} = \\left(\\frac{x}{x+z}\\right)^{\\nu+1} = \\left(1+\\frac{z}{x}\\right)^{-(\\nu+1)} $$\nUsing a Taylor expansion for small $z/x$, this is approximately $1 - (\\nu+1)\\frac{z}{x}$.\n\nThe expected drift of the chain at a large state $x$ is approximately $\\mathbb{E}[X_{n+1}-X_n | X_n=x]$. A move is accepted with probability $\\alpha(x,Y)$. The expected change is $\\mathbb{E}[(Y-x)\\alpha(x,Y)|X_n=x]$. For large $x$, the target density is locally flat, meaning $\\pi(y)/\\pi(x) \\approx 1$ for typical proposals $Y$ where $|Y-x|$ is of order $\\sigma$. Thus, the acceptance probability $\\alpha(x,Y)$ is close to $1$. The chain's local behavior mimics that of a simple symmetric random walk. A simple random walk on $\\mathbb{R}$ is null recurrent and explores space slowly.\n\nTo achieve geometric ergodicity, the chain must have a strong drift towards the origin. The drift for this chain is of order $O(1/x)$. This can be seen by a more careful calculation of the expected one-step change in position. This drift is too weak to ensure an exponential return to the center. For comparison, if the target had exponential tails, e.g., $\\pi(x) \\propto \\exp(-|x|)$, the drift would be a constant, which is a much stronger restoring force.\n\nThis weak restoring force means that when the chain wanders out into the tails, it takes a polynomially long time in $x$ to return to the center, rather than a logarithmic time as would be required for geometric ergodicity. Theory confirms this: for RWM algorithms with Gaussian proposals, geometric ergodicity holds if and only if the target density has tails that are lighter than exponential (e.g. $\\pi(x) \\propto \\exp(-c|x|^\\beta)$ with $\\beta  1$). The Student-$t$ distribution has polynomial tails, which are heavier than exponential, precluding geometric ergodicity for any $\\nu0$ and $\\sigma0$.\n\n**(ii) Threshold for the Central Limit Theorem (CLT)**\n\nA Central Limit Theorem for ergodic averages of a function $f$ for a Markov chain typically states that $\\sqrt{N}(\\bar{f}_N - \\mathbb{E}_{\\pi}[f]) \\xrightarrow{d} \\mathcal{N}(0, \\sigma_f^2)$, where $\\bar{f}_N = \\frac{1}{N}\\sum_{n=1}^N f(X_n)$ and $\\sigma_f^2$ is the asymptotic variance.\nA fundamental, necessary condition for such a CLT to hold with a finite, non-zero asymptotic variance $\\sigma_f^2$ is that the variance of $f(X)$ under the stationary distribution $\\pi$ must be finite:\n$$ \\text{Var}_{\\pi}(f(X)) = \\mathbb{E}_{\\pi}[f(X)^2] - (\\mathbb{E}_{\\pi}[f(X)])^2  \\infty $$\nIf $\\text{Var}_{\\pi}(f(X))$ is infinite, the sample average may still converge under a different normalization (e.g., to a stable law), but the standard CLT with $\\sqrt{N}$ scaling and a Gaussian limit fails.\n\nIn this problem, the function is $f(x)=x$ and the stationary distribution $\\pi$ is the Student-$t$ distribution with $\\nu$ degrees of freedom. We must find the condition on $\\nu$ for which $\\text{Var}_{\\pi}(X)  \\infty$.\n\nThe Student-$t$ distribution is symmetric about $0$. For its mean to exist, we need $\\mathbb{E}_\\pi[|X|]  \\infty$. This is true if $\\nu1$. If so, $\\mathbb{E}_\\pi[X]=0$.\nThe variance is $\\text{Var}_{\\pi}(X) = \\mathbb{E}_{\\pi}[X^2] - (\\mathbb{E}_{\\pi}[X])^2 = \\mathbb{E}_{\\pi}[X^2]$.\nWe need to determine for which $\\nu$ the second moment, $\\mathbb{E}_{\\pi}[X^2]$, is finite.\n$$ \\mathbb{E}_{\\pi}[X^2] = C_\\nu \\int_{-\\infty}^{\\infty} x^2 \\left(1 + \\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}} dx $$\nwhere $C_\\nu$ is the normalization constant. For this integral to converge, the integrand must decay sufficiently fast as $|x| \\to \\infty$. For large $|x|$, the integrand behaves like:\n$$ x^2 \\left(\\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}} \\propto x^2 |x|^{-(\\nu+1)} = |x|^{2 - (\\nu+1)} = |x|^{1-\\nu} $$\nThe integral $\\int_a^\\infty x^{p} dx$ converges if and only if $p  -1$. In our case, the exponent is $p = 1-\\nu$. So we require:\n$$ 1 - \\nu  -1 \\implies 2  \\nu $$\nTherefore, the variance of the Student-$t$ distribution is finite if and only if $\\nu  2$. This establishes the minimal moment condition for a CLT for the sample mean of $f(x)=x$ to possibly hold. The RWM chain is reversible, and for reversible chains, the condition $\\text{Var}_\\pi(f(X))  \\infty$ is often sufficient for the CLT to hold, even if the chain is not geometrically ergodic.\n\nThus, the smallest degrees-of-freedom threshold is $\\nu_{\\star} = 2$.\n\n**(iii) Impact of Failure of Geometric Ergodicity**\n\n(a) **On the validity of a CLT for the sample mean of $f(X)$**:\n- In the regime $\\nu  \\nu_{\\star} = 2$, the condition $\\text{Var}_{\\pi}(X)  \\infty$ is met. For reversible Markov chains like this RWM, this condition is sufficient for a CLT to hold. So, despite the failure of geometric ergodicity, the CLT is still valid. The lack of geometric ergodicity implies that the autocovariances $\\text{Cov}(X_n, X_0)$ decay polynomially rather than exponentially, but they still decay fast enough for the sum in the asymptotic variance formula to converge.\n- In the regime $\\nu \\le \\nu_{\\star} = 2$, the variance $\\text{Var}_{\\pi}(X)$ is infinite. A standard CLT with $\\sqrt{N}$ scaling and a Gaussian limit cannot hold. The sample average, when properly scaled, will converge in distribution to a non-Gaussian stable law. The failure of geometric ergodicity is a secondary issue here; the primary obstacle is the infinite variance of the target distribution.\n\n(b) **On the consistency of variance estimation methods**:\nThe failure of geometric ergodicity has severe practical consequences for estimating the asymptotic variance $\\sigma_f^2$ that appears in the CLT. Standard methods for estimating $\\sigma_f^2$, such as the method of batch means or spectral variance estimators, rely on the assumption that correlations decay rapidly.\nThe theoretical proofs for the consistency of these estimators (i.e., that the estimate converges to the true $\\sigma_f^2$ as the sample size $N \\to \\infty$) almost always assume geometric ergodicity or a similar strong mixing condition. When a chain is only polynomially ergodic, as is the case here, the long-range correlations are much more persistent.\nThis slow decay of correlations means that to obtain approximately independent batches for the batch means method, the batch size must be much larger than in the geometrically ergodic case. Standard choices for batch size growth rates (e.g., $b_N \\propto N^{1/2}$) may be insufficient, leading to biased or inconsistent estimators of the variance. In essence, while a CLT may hold for $\\nu2$, the failure of geometric ergodicity makes it exceedingly difficult to reliably estimate the variance of the limiting normal distribution using standard MCMC output analysis tools.",
            "answer": "$$\n\\boxed{2}\n$$"
        }
    ]
}