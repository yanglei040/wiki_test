{
    "hands_on_practices": [
        {
            "introduction": "Before we can assign probabilities, we must first define the collection of events to which probabilities can be assigned. This collection, a $\\sigma$-algebra, is the structural backbone of any probability space, encoding the information we can resolve about a system's outcomes. This exercise  moves beyond abstract definitions, challenging you to construct and compare different $\\sigma$-algebras on the same set of outcomes. By building these structures from first principles, you will gain a tangible understanding of how they represent distinct states of information and form the foundation for more complex probabilistic reasoning.",
            "id": "3331638",
            "problem": "In the context of modeling information structures in stochastic simulation and Monte Carlo methods, distinct collections of measurable events on the same underlying outcome space can be incomparable by inclusion. Let $\\Omega$ be a finite outcome space with $|\\Omega|=4$. \n\nUsing only the definitions of a $\\sigma$-algebra (closed under complements and countable unions, and containing $\\emptyset$ and $\\Omega$) and the notion of atoms induced by a partition, do the following:\n\n- Explicitly construct $2$ distinct $\\sigma$-algebras $\\mathcal{F}$ and $\\mathcal{G}$ on the same $\\Omega$ such that neither $\\mathcal{F} \\subset \\mathcal{G}$ nor $\\mathcal{G} \\subset \\mathcal{F}$ holds. Justify from first principles why each is a $\\sigma$-algebra and why neither contains the other.\n\n- Then, without appealing to any unproven theorems beyond these definitions, determine the cardinality of the $\\sigma$-algebra $\\sigma(\\mathcal{F} \\cup \\mathcal{G})$ generated by the union $\\mathcal{F} \\cup \\mathcal{G}$.\n\nYour final response must be a single integer equal to the number of sets in $\\sigma(\\mathcal{F} \\cup \\mathcal{G})$. No rounding is required. Do not include units.",
            "solution": "Let the finite outcome space be $\\Omega = \\{1, 2, 3, 4\\}$, which satisfies the condition $|\\Omega|=4$.\n\nFirst, we construct two distinct $\\sigma$-algebras, $\\mathcal{F}$ and $\\mathcal{G}$, on $\\Omega$ that are incomparable under set inclusion. A $\\sigma$-algebra on a finite set is generated by a partition of that set. The elements of the $\\sigma$-algebra are all possible unions of the disjoint sets (atoms) in the partition.\n\nLet us define a partition of $\\Omega$ as $P_{\\mathcal{F}} = \\{\\{1, 2\\}, \\{3, 4\\}\\}$. The atoms of the corresponding $\\sigma$-algebra $\\mathcal{F}$ are $A_1 = \\{1, 2\\}$ and $A_2 = \\{3, 4\\}$. The $\\sigma$-algebra $\\mathcal{F}$ consists of all possible unions of these atoms, which includes the empty union (the empty set) and the union of all atoms ($\\Omega$).\nThus, $\\mathcal{F} = \\{\\emptyset, \\{1, 2\\}, \\{3, 4\\}, \\{1, 2, 3, 4\\}\\}$.\n\nTo verify from first principles that $\\mathcal{F}$ is a $\\sigma$-algebra:\n1.  $\\Omega \\in \\mathcal{F}$: By construction, $\\Omega = \\{1, 2, 3, 4\\} = \\{1, 2\\} \\cup \\{3, 4\\}$ is in $\\mathcal{F}$.\n2.  Closure under complementation:\n    - $\\emptyset^c = \\Omega \\in \\mathcal{F}$\n    - $\\Omega^c = \\emptyset \\in \\mathcal{F}$\n    - $(\\{1, 2\\})^c = \\{3, 4\\} \\in \\mathcal{F}$\n    - $(\\{3, 4\\})^c = \\{1, 2\\} \\in \\mathcal{F}$\n    The set is closed under complements.\n3.  Closure under countable (and thus finite) unions: All possible unions of elements of $\\mathcal{F}$ are already in $\\mathcal{F}$. For example, $\\{1, 2\\} \\cup \\{3, 4\\} = \\{1, 2, 3, 4\\} \\in \\mathcal{F}$. Other unions are trivial (e.g., $A \\cup \\emptyset = A$). The set is closed under unions.\nTherefore, $\\mathcal{F}$ is a valid $\\sigma$-algebra.\n\nNext, we construct a second $\\sigma$-algebra, $\\mathcal{G}$. To ensure it is incomparable with $\\mathcal{F}$, we must choose a partition whose atoms are not unions of the atoms of $\\mathcal{F}$, nor are the atoms of $\\mathcal{F}$ unions of atoms of the new partition.\nLet us define the partition $P_{\\mathcal{G}} = \\{\\{1, 3\\}, \\{2, 4\\}\\}$. The atoms are $B_1 = \\{1, 3\\}$ and $B_2 = \\{2, 4\\}$.\nThe resulting $\\sigma$-algebra is $\\mathcal{G} = \\{\\emptyset, \\{1, 3\\}, \\{2, 4\\}, \\{1, 2, 3, 4\\}\\}$.\nThe verification that $\\mathcal{G}$ is a $\\sigma$-algebra is analogous to the verification for $\\mathcal{F}$.\n\nNow, we justify why neither contains the other:\n- To show $\\mathcal{F} \\not\\subset \\mathcal{G}$, we must find an element in $\\mathcal{F}$ that is not in $\\mathcal{G}$. The set $\\{1, 2\\} \\in \\mathcal{F}$. However, the elements of $\\mathcal{G}$ are $\\emptyset, \\{1, 3\\}, \\{2, 4\\}, \\Omega$. Clearly, $\\{1, 2\\} \\notin \\mathcal{G}$. Thus, $\\mathcal{F} \\not\\subset \\mathcal{G}$.\n- To show $\\mathcal{G} \\not\\subset \\mathcal{F}$, we must find an element in $\\mathcal{G}$ that is not in $\\mathcal{F}$. The set $\\{1, 3\\} \\in \\mathcal{G}$. The elements of $\\mathcal{F}$ are $\\emptyset, \\{1, 2\\}, \\{3, 4\\}, \\Omega$. Clearly, $\\{1, 3\\} \\notin \\mathcal{F}$. Thus, $\\mathcal{G} \\not\\subset \\mathcal{F}$.\nThe two $\\sigma$-algebras are therefore incomparable by inclusion.\n\nThe second part of the problem is to determine the cardinality of $\\sigma(\\mathcal{F} \\cup \\mathcal{G})$, the smallest $\\sigma$-algebra containing both $\\mathcal{F}$ and $\\mathcal{G}$. Let us denote this a $\\sigma$-algebra by $\\mathcal{H}$.\n$\\mathcal{H}$ must contain all elements of $\\mathcal{F}$ and all elements of $\\mathcal{G}$. Since a $\\sigma$-algebra must be closed under intersections (as $A \\cap B = (A^c \\cup B^c)^c$), $\\mathcal{H}$ must contain all intersections of sets from $\\mathcal{F} \\cup \\mathcal{G}$.\n\nThe atoms of $\\mathcal{H}$ are formed by the non-empty intersections of the atoms of $\\mathcal{F}$ and $\\mathcal{G}$.\nThe atoms of $\\mathcal{F}$ are $P_{\\mathcal{F}} = \\{\\{1, 2\\}, \\{3, 4\\}\\}$.\nThe atoms of $\\mathcal{G}$ are $P_{\\mathcal{G}} = \\{\\{1, 3\\}, \\{2, 4\\}\\}$.\n\nLet's compute the intersections:\n- Intersection of the first atom of $\\mathcal{F}$ with the atoms of $\\mathcal{G}$:\n  - $\\{1, 2\\} \\cap \\{1, 3\\} = \\{1\\}$\n  - $\\{1, 2\\} \\cap \\{2, 4\\} = \\{2\\}$\n- Intersection of the second atom of $\\mathcal{F}$ with the atoms of $\\mathcal{G}$:\n  - $\\{3, 4\\} \\cap \\{1, 3\\} = \\{3\\}$\n  - $\\{3, 4\\} \\cap \\{2, 4\\} = \\{4\\}$\n\nThese intersection operations show that any $\\sigma$-algebra containing $\\mathcal{F}$ and $\\mathcal{G}$ must contain the singleton sets $\\{1\\}, \\{2\\}, \\{3\\}, \\{4\\}$. These singletons form a partition of $\\Omega$, $P_{\\mathcal{H}} = \\{\\{1\\}, \\{2\\}, \\{3\\}, \\{4\\}\\}$. These are the atoms of the generated $\\sigma$-algebra $\\mathcal{H} = \\sigma(\\mathcal{F} \\cup \\mathcal{G})$.\n\nSince $\\mathcal{H}$ is a $\\sigma$-algebra and it contains all the singleton sets $\\{i\\}$ for $i \\in \\Omega$, it must be closed under countable unions. Therefore, any subset of $\\Omega$ can be formed as a finite union of these singletons, and thus must belong to $\\mathcal{H}$. For any set $A \\subseteq \\Omega$, we have $A = \\bigcup_{i \\in A} \\{i\\}$. Since each $\\{i\\} \\in \\mathcal{H}$, their union $A$ must also be in $\\mathcal{H}$.\nThis means that every subset of $\\Omega$ must be in $\\mathcal{H}$. The collection of all subsets of $\\Omega$ is the power set of $\\Omega$, denoted $2^\\Omega$.\nTherefore, $\\mathcal{H} = \\sigma(\\mathcal{F} \\cup \\mathcal{G}) = 2^\\Omega$.\n\nThe problem asks for the cardinality of this generated $\\sigma$-algebra. The cardinality of the power set of a finite set with $n$ elements is $2^n$.\nIn our case, $|\\Omega| = 4$.\nSo, the cardinality of $\\mathcal{H}$ is $|2^\\Omega| = 2^{|\\Omega|} = 2^4$.\n\nCalculating the value:\n$2^4 = 2 \\times 2 \\times 2 \\times 2 = 16$.\n\nThe number of sets in $\\sigma(\\mathcal{F} \\cup \\mathcal{G})$ is $16$. This result relies only on the axioms of a $\\sigma$-algebra and the concept of it being generated by a partition of atoms.",
            "answer": "$$\n\\boxed{16}\n$$"
        },
        {
            "introduction": "Independence is a cornerstone assumption in stochastic simulation, often invoked when drawing sequences of random numbers. However, its subtleties can be a source of profound modeling errors. This practice explores the crucial distinction between pairwise and mutual independence using a classic construction . By demonstrating that a set of events can be independent in pairs but fail to be mutually independent, you will develop a more rigorous and cautious approach to building and verifying the statistical assumptions that underpin your simulations.",
            "id": "3331642",
            "problem": "Consider the following construction motivated by diagnostic counterexamples in stochastic simulation and Monte Carlo methods. Let the probability space be $(\\Omega,\\mathcal{F},\\mathbb{P})$ with $\\Omega=\\{(0,0),(0,1),(1,0),(1,1)\\}$, $\\mathcal{F}=2^{\\Omega}$, and $\\mathbb{P}$ the uniform probability measure on $\\Omega$ (each atom has probability $1/4$). Let $X:\\Omega\\to\\{0,1\\}$ and $Y:\\Omega\\to\\{0,1\\}$ be the coordinate projection random variables, and define the events\n$$A=\\{X=1\\},\\quad B=\\{Y=1\\},\\quad C=\\{X=Y\\}.$$\nThese events are used to test independence assumptions in algorithms that rely on sampling from Bernoulli variables.\n\nUsing only the axioms of probability and the definitions of independence, do the following:\n\n- Show that $A$, $B$, and $C$ are pairwise independent, but not mutually independent.\n- Compute the probabilities $\\mathbb{P}(A)$, $\\mathbb{P}(B)$, $\\mathbb{P}(C)$, $\\mathbb{P}(A\\cap B)$, $\\mathbb{P}(A\\cap C)$, $\\mathbb{P}(B\\cap C)$, and $\\mathbb{P}(A\\cap B\\cap C)$.\n- Let $I_A$, $I_B$, and $I_C$ denote the indicator random variables of $A$, $B$, and $C$, respectively. Determine the joint distribution of $(I_A,I_B,I_C)$ and the product measure $\\mathbb{P}_{I_A}\\otimes\\mathbb{P}_{I_B}\\otimes\\mathbb{P}_{I_C}$ on $\\{0,1\\}^3$.\n\nFinally, compute the Kullback–Leibler divergence (KLD) between the joint distribution of $(I_A,I_B,I_C)$ and the product of the marginals, that is\n$$D_{\\mathrm{KL}}\\!\\left(\\mathbb{P}_{(I_A,I_B,I_C)}\\,\\middle\\|\\,\\mathbb{P}_{I_A}\\otimes\\mathbb{P}_{I_B}\\otimes\\mathbb{P}_{I_C}\\right),$$\nand express your final answer as a closed-form analytic expression. No rounding is required.",
            "solution": "We begin by formally defining the events and computing their probabilities based on the provided probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$, where $\\Omega=\\{(0,0),(0,1),(1,0),(1,1)\\}$ and the measure $\\mathbb{P}$ is uniform, assigning a probability of $\\frac{1}{4}$ to each elementary outcome. The random variables $X$ and $Y$ are the coordinate projections, so for an outcome $\\omega=(\\omega_1, \\omega_2) \\in \\Omega$, we have $X(\\omega)=\\omega_1$ and $Y(\\omega)=\\omega_2$.\n\nThe events $A$, $B$, and $C$ are defined as subsets of $\\Omega$:\n$A = \\{ \\omega \\in \\Omega \\mid X(\\omega)=1 \\} = \\{(1,0), (1,1)\\}$\n$B = \\{ \\omega \\in \\Omega \\mid Y(\\omega)=1 \\} = \\{(0,1), (1,1)\\}$\n$C = \\{ \\omega \\in \\Omega \\mid X(\\omega)=Y(\\omega) \\} = \\{(0,0), (1,1)\\}$\n\nWe compute the probabilities of these events:\n$\\mathbb{P}(A) = \\frac{|A|}{|\\Omega|} = \\frac{2}{4} = \\frac{1}{2}$\n$\\mathbb{P}(B) = \\frac{|B|}{|\\Omega|} = \\frac{2}{4} = \\frac{1}{2}$\n$\\mathbb{P}(C) = \\frac{|C|}{|\\Omega|} = \\frac{2}{4} = \\frac{1}{2}$\n\nNext, we identify the pairwise and three-way intersections of these events:\n$A \\cap B = \\{(1,1)\\}$\n$A \\cap C = \\{(1,1)\\}$\n$B \\cap C = \\{(1,1)\\}$\n$A \\cap B \\cap C = (A \\cap B) \\cap C = \\{(1,1)\\} \\cap \\{(0,0), (1,1)\\} = \\{(1,1)\\}$\n\nWe compute the probabilities of these intersections:\n$\\mathbb{P}(A \\cap B) = \\frac{|A \\cap B|}{|\\Omega|} = \\frac{1}{4}$\n$\\mathbb{P}(A \\cap C) = \\frac{|A \\cap C|}{|\\Omega|} = \\frac{1}{4}$\n$\\mathbb{P}(B \\cap C) = \\frac{|B \\cap C|}{|\\Omega|} = \\frac{1}{4}$\n$\\mathbb{P}(A \\cap B \\cap C) = \\frac{|A \\cap B \\cap C|}{|\\Omega|} = \\frac{1}{4}$\n\nWith these probabilities, we can demonstrate the independence properties.\nFor pairwise independence, we check if $\\mathbb{P}(E_1 \\cap E_2) = \\mathbb{P}(E_1)\\mathbb{P}(E_2)$ for each pair of events.\n$\\mathbb{P}(A)\\mathbb{P}(B) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$. Since this equals $\\mathbb{P}(A \\cap B)$, events $A$ and $B$ are independent.\n$\\mathbb{P}(A)\\mathbb{P}(C) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$. Since this equals $\\mathbb{P}(A \\cap C)$, events $A$ and $C$ are independent.\n$\\mathbb{P}(B)\\mathbb{P}(C) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$. Since this equals $\\mathbb{P}(B \\cap C)$, events $B$ and $C$ are independent.\nThus, the events $A$, $B$, and $C$ are pairwise independent.\n\nFor mutual independence, the condition $\\mathbb{P}(A \\cap B \\cap C) = \\mathbb{P}(A)\\mathbb{P}(B)\\mathbb{P}(C)$ must also hold.\n$\\mathbb{P}(A)\\mathbb{P}(B)\\mathbb{P}(C) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{8}$.\nWe have $\\mathbb{P}(A \\cap B \\cap C) = \\frac{1}{4}$.\nSince $\\frac{1}{4} \\neq \\frac{1}{8}$, the events $A$, $B$, and $C$ are not mutually independent.\n\nNext, we determine the joint distribution of the indicator random variables $(I_A, I_B, I_C)$, which we denote by $P(i,j,k) = \\mathbb{P}(I_A=i, I_B=j, I_C=k)$. We evaluate the vector $(I_A(\\omega), I_B(\\omega), I_C(\\omega))$ for each $\\omega \\in \\Omega$:\nFor $\\omega=(0,0)$: $X=0, Y=0$. Events $A,B$ are false, $C$ is true. The outcome is $(0,0,1)$.\nFor $\\omega=(0,1)$: $X=0, Y=1$. Events $A,C$ are false, $B$ is true. The outcome is $(0,1,0)$.\nFor $\\omega=(1,0)$: $X=1, Y=0$. Events $B,C$ are false, $A$ is true. The outcome is $(1,0,0)$.\nFor $\\omega=(1,1)$: $X=1, Y=1$. Events $A,B,C$ are all true. The outcome is $(1,1,1)$.\nSince each $\\omega \\in \\Omega$ has probability $\\frac{1}{4}$, the joint distribution $\\mathbb{P}_{(I_A,I_B,I_C)}$ is given by the probability mass function $P(i,j,k)$:\n$P(0,0,1) = \\frac{1}{4}$, $P(0,1,0) = \\frac{1}{4}$, $P(1,0,0) = \\frac{1}{4}$, $P(1,1,1) = \\frac{1}{4}$.\nThe probability is $0$ for the other four outcomes in $\\{0,1\\}^3$: $(0,0,0)$, $(0,1,1)$, $(1,0,1)$, $(1,1,0)$.\n\nNow, we determine the product measure $\\mathbb{P}_{I_A}\\otimes\\mathbb{P}_{I_B}\\otimes\\mathbb{P}_{I_C}$. Let us denote its probability mass function by $Q(i,j,k)$. This measure corresponds to the distribution of three mutually independent Bernoulli variables.\nThe marginal distributions are:\n$\\mathbb{P}_{I_A}(1) = \\mathbb{P}(A) = \\frac{1}{2}$, so $\\mathbb{P}_{I_A}(0) = 1-\\frac{1}{2} = \\frac{1}{2}$.\n$\\mathbb{P}_{I_B}(1) = \\mathbb{P}(B) = \\frac{1}{2}$, so $\\mathbb{P}_{I_B}(0) = 1-\\frac{1}{2} = \\frac{1}{2}$.\n$\\mathbb{P}_{I_C}(1) = \\mathbb{P}(C) = \\frac{1}{2}$, so $\\mathbb{P}_{I_C}(0) = 1-\\frac{1}{2} = \\frac{1}{2}$.\nThe product measure is defined by $Q(i,j,k) = \\mathbb{P}_{I_A}(i) \\mathbb{P}_{I_B}(j) \\mathbb{P}_{I_C}(k)$.\nFor any $(i,j,k) \\in \\{0,1\\}^3$, $Q(i,j,k) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{8}$. This is the uniform distribution on the $8$ vertices of the cube $\\{0,1\\}^3$.\n\nFinally, we compute the Kullback–Leibler divergence from the product-of-marginals distribution $Q$ to the true joint distribution $P$. The problem asks for $D_{\\mathrm{KL}}(P \\| Q)$.\nThe formula for the KLD for discrete distributions is:\n$$D_{\\mathrm{KL}}(P \\| Q) = \\sum_{(i,j,k) \\in \\{0,1\\}^3} P(i,j,k) \\ln\\left(\\frac{P(i,j,k)}{Q(i,j,k)}\\right)$$\nThe summation is non-zero only over the support of $P$, which is the set $S = \\{(0,0,1), (0,1,0), (1,0,0), (1,1,1)\\}$.\nFor each $(i,j,k) \\in S$, we have $P(i,j,k) = \\frac{1}{4}$ and $Q(i,j,k) = \\frac{1}{8}$.\nThe ratio $\\frac{P(i,j,k)}{Q(i,j,k)}$ is constant for all points in the support of $P$:\n$$\\frac{P(i,j,k)}{Q(i,j,k)} = \\frac{1/4}{1/8} = 2$$\nSubstituting this into the KLD formula:\n$$D_{\\mathrm{KL}}(P \\| Q) = \\sum_{(i,j,k) \\in S} \\frac{1}{4} \\ln(2)$$\nSince the size of the support set $S$ is $|S|=4$, the sum becomes:\n$$D_{\\mathrm{KL}}(P \\| Q) = \\frac{1}{4}\\ln(2) + \\frac{1}{4}\\ln(2) + \\frac{1}{4}\\ln(2) + \\frac{1}{4}\\ln(2) = 4 \\left( \\frac{1}{4} \\ln(2) \\right) = \\ln(2)$$\nThe Kullback–Leibler divergence is $\\ln(2)$.",
            "answer": "$$\\boxed{\\ln(2)}$$"
        },
        {
            "introduction": "A frequent point of confusion when bridging theory and practice is the concept of an event with probability zero. In a theoretical model with continuous variables, the probability of any single value being realized is zero, yet in a computational simulation, specific values occur constantly. This exercise  directly addresses this apparent paradox by contrasting an ideal continuous model with its finite-precision discrete counterpart. Working through this reconciliation will clarify the powerful meaning of \"almost sure\" statements and equip you to correctly interpret the results of stochastic simulations in light of their underlying theoretical framework.",
            "id": "3331663",
            "problem": "Consider two models for a stream of independent and identically distributed random variates intended to be Uniform on the unit interval. Model C (the continuous, ideal model) is a probability space $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ on which $(U_i)_{i\\ge 1}$ are independent and identically distributed with the continuous uniform distribution on $(0,1)$, understood as the Borel $\\sigma$-algebra on $(0,1)$ endowed with Lebesgue measure. Model D (the finite-precision, implementable model) is a probability space $\\left(\\tilde{\\Omega},\\tilde{\\mathcal{F}},\\tilde{\\mathbb{P}}\\right)$ on which $(\\tilde{U}_i)_{i\\ge 1}$ are independent and identically distributed taking values in a finite set $S\\subset (0,1)$ of cardinality $M\\in\\mathbb{N}$, each point of $S$ having probability $1/M$ (for concreteness, think of $S$ as the set of representable floating-point grid points produced by a pseudorandom number generator complying with the Institute of Electrical and Electronics Engineers (IEEE) $754$ standard). For $i\\in\\mathbb{N}$ define the event $E_i^{\\mathrm{C}}=\\{U_{2i-1}=U_{2i}\\}$ under Model C and $E_i^{\\mathrm{D}}=\\{\\tilde{U}_{2i-1}=\\tilde{U}_{2i}\\}$ under Model D. For $n\\in\\mathbb{N}$ let $N_n^{\\mathrm{C}}=\\sum_{i=1}^n \\mathbf{1}_{E_i^{\\mathrm{C}}}$ and $F_n^{\\mathrm{C}}=N_n^{\\mathrm{C}}/n$ denote the count and empirical frequency of exact ties among $n$ successive disjoint pairs in Model C, and similarly $N_n^{\\mathrm{D}}=\\sum_{i=1}^n \\mathbf{1}_{E_i^{\\mathrm{D}}}$ and $F_n^{\\mathrm{D}}=N_n^{\\mathrm{D}}/n$ for Model D.\n\nSelect all options that correctly construct an event of probability zero that nevertheless occurs with positive frequency in finite samples when implemented, and correctly reconcile this phenomenon with the meaning of almost sure statements in measure-theoretic probability.\n\nA. In Model C, for each $i$, $\\mathbb{P}\\!\\left(E_i^{\\mathrm{C}}\\right)=0$, and for every $n$, $\\mathbb{P}\\!\\left(N_n^{\\mathrm{C}}=0\\right)=1$, so $F_n^{\\mathrm{C}}=0$ almost surely for all $n$. In Model D, for each $i$, $\\tilde{\\mathbb{P}}\\!\\left(E_i^{\\mathrm{D}}\\right)=1/M$, the indicators $\\mathbf{1}_{E_i^{\\mathrm{D}}}$ are independent and identically distributed with mean $1/M$, and by the Strong Law of Large Numbers (SLLN) $F_n^{\\mathrm{D}}\\to 1/M$ almost surely as $n\\to\\infty$. Thus the event “a pair ties exactly” has probability zero in the ideal model but occurs with positive empirical frequency in finite-precision samples.\n\nB. If a single finite sample from Model C exhibits at least one exact tie, then the almost sure statement “no exact ties occur” is falsified, because “almost sure” means the event must hold for every outcome.\n\nC. There is no contradiction between probability zero in Model C and positive frequency in Model D: “almost sure” permits a set of outcomes of probability zero under a given measure on which the prohibited behavior may occur; finite samples cannot falsify an almost sure statement. In Model C, $F_n^{\\mathrm{C}}=0$ for every $n$ almost surely; in Model D, $\\mathbb{E}[F_n^{\\mathrm{D}}]=1/M$ and, by the Strong Law of Large Numbers, $F_n^{\\mathrm{D}}$ concentrates near $1/M$ for large $n$.\n\nD. By countable additivity, $\\mathbb{P}\\!\\left(\\bigcup_{i=1}^n E_i^{\\mathrm{C}}\\right)=\\sum_{i=1}^n \\mathbb{P}\\!\\left(E_i^{\\mathrm{C}}\\right)=n\\cdot 0=0$, but because the union ranges over multiple indices, for sufficiently large $n$ one obtains a positive probability of at least one tie even in Model C, explaining positive frequency without any discretization.\n\nE. To reconcile probability zero with positive finite-sample frequency, one must abandon countable additivity on uncountable spaces; under merely finitely additive probabilities, zero-probability events may occur with positive empirical frequency even when the underlying model is continuous.",
            "solution": "This problem highlights the crucial difference between an idealized continuous probability model (Model C) and its discrete, finite-precision computational implementation (Model D). Let's analyze the behavior of ties in both models.\n\n**Analysis of Model C (Continuous)**\nIn Model C, the random variables $U_{2i-1}$ and $U_{2i}$ are i.i.d., each with a continuous uniform distribution on $(0,1)$. Their joint probability density is uniform on the unit square $(0,1) \\times (0,1)$. The probability of an exact tie, $E_i^{\\mathrm{C}} = \\{U_{2i-1} = U_{2i}\\}$, corresponds to the integral of this density over the diagonal line segment from $(0,0)$ to $(1,1)$, which has a two-dimensional area of zero. Thus, for any single pair $i$, the probability of a tie is zero: $\\mathbb{P}(E_i^{\\mathrm{C}}) = 0$.\n\nThe probability of at least one tie occurring in $n$ pairs is $\\mathbb{P}(\\bigcup_{i=1}^n E_i^{\\mathrm{C}})$. By the union bound (subadditivity), this probability is less than or equal to the sum of the individual probabilities:\n$$ \\mathbb{P}\\left(\\bigcup_{i=1}^n E_i^{\\mathrm{C}}\\right) \\le \\sum_{i=1}^n \\mathbb{P}(E_i^{\\mathrm{C}}) = \\sum_{i=1}^n 0 = 0 $$\nSince probability must be non-negative, the probability of at least one tie is exactly $0$. This means that the event of having zero ties, $\\{N_n^{\\mathrm{C}}=0\\}$, has probability $1$. Consequently, the empirical frequency of ties, $F_n^{\\mathrm{C}}$, is $0$ almost surely for any finite $n$.\n\n**Analysis of Model D (Discrete)**\nIn Model D, the variables $\\tilde{U}_{j}$ are i.i.d. and drawn from a finite set $S$ of size $M$, with each value having probability $1/M$. The probability of a tie, $E_i^{\\mathrm{D}} = \\{\\tilde{U}_{2i-1} = \\tilde{U}_{2i}\\}$, is calculated by summing over all possible matching values:\n$$ \\tilde{\\mathbb{P}}(E_i^{\\mathrm{D}}) = \\sum_{s \\in S} \\tilde{\\mathbb{P}}(\\tilde{U}_{2i-1} = s \\text{ and } \\tilde{U}_{2i} = s) $$\nDue to independence, this becomes:\n$$ \\tilde{\\mathbb{P}}(E_i^{\\mathrm{D}}) = \\sum_{s \\in S} \\left(\\frac{1}{M}\\right) \\cdot \\left(\\frac{1}{M}\\right) = M \\cdot \\frac{1}{M^2} = \\frac{1}{M} $$\nThe probability of a tie is positive. The indicator variables $\\mathbf{1}_{E_i^{\\mathrm{D}}}$ are i.i.d. Bernoulli random variables with success probability $p=1/M$. By the Strong Law of Large Numbers (SLLN), their average, the empirical frequency $F_n^{\\mathrm{D}}$, converges almost surely to the expected value:\n$$ F_n^{\\mathrm{D}} \\xrightarrow{\\text{a.s.}} \\frac{1}{M} \\quad \\text{as } n \\to \\infty $$\nThis means the observed frequency of ties in a simulation will be close to $1/M$ for large $n$.\n\nNow we evaluate the options based on this analysis.\n\n**A. In Model C, for each $i$, $\\mathbb{P}\\!\\left(E_i^{\\mathrm{C}}\\right)=0$, and for every $n$, $\\mathbb{P}\\!\\left(N_n^{\\mathrm{C}}=0\\right)=1$, so $F_n^{\\mathrm{C}}=0$ almost surely for all $n$. In Model D, for each $i$, $\\tilde{\\mathbb{P}}\\!\\left(E_i^{\\mathrm{D}}\\right)=1/M$, the indicators $\\mathbf{1}_{E_i^{\\mathrm{D}}}$ are independent and identically distributed with mean $1/M$, and by the Strong Law of Large Numbers (SLLN) $F_n^{\\mathrm{D}}\\to 1/M$ almost surely as $n\\to\\infty$. Thus the event “a pair ties exactly” has probability zero in the ideal model but occurs with positive empirical frequency in finite-precision samples.**\nThis statement is entirely correct. It accurately describes the behavior in both models based on our derivation and correctly summarizes the apparent paradox.\n\n**B. If a single finite sample from Model C exhibits at least one exact tie, then the almost sure statement “no exact ties occur” is falsified, because “almost sure” means the event must hold for every outcome.**\nThis is incorrect. The term \"almost sure\" (or \"with probability 1\") allows for a set of exceptional outcomes, as long as this exceptional set has a probability measure of zero. The existence of such an outcome does not falsify the probabilistic statement.\n\n**C. There is no contradiction between probability zero in Model C and positive frequency in Model D: “almost sure” permits a set of outcomes of probability zero under a given measure on which the prohibited behavior may occur; finite samples cannot falsify an almost sure statement. In Model C, $F_n^{\\mathrm{C}}=0$ for every $n$ almost surely; in Model D, $\\mathbb{E}[F_n^{\\mathrm{D}}]=1/M$ and, by the Strong Law of Large Numbers, $F_n^{\\mathrm{D}}$ concentrates near $1/M$ for large $n$.**\nThis option correctly explains the reconciliation. The discrepancy arises because Model C and Model D are different probability spaces. It correctly defines \"almost sure\" and correctly summarizes the key results for both models, including the expectation $\\mathbb{E}[F_n^{\\mathrm{D}}]=1/M$.\n\n**D. By countable additivity, $\\mathbb{P}\\!\\left(\\bigcup_{i=1}^n E_i^{\\mathrm{C}}\\right)=\\sum_{i=1}^n \\mathbb{P}\\!\\left(E_i^{\\mathrm{C}}\\right)=n\\cdot 0=0$, but because the union ranges over multiple indices, for sufficiently large $n$ one obtains a positive probability of at least one tie even in Model C, explaining positive frequency without any discretization.**\nThis is incorrect. The calculation shows the probability of a union of these zero-probability events is zero for any finite $n$. The claim that this probability becomes positive for large $n$ is false and contradicts the axioms of probability.\n\n**E. To reconcile probability zero with positive finite-sample frequency, one must abandon countable additivity on uncountable spaces; under merely finitely additive probabilities, zero-probability events may occur with positive empirical frequency even when the underlying model is continuous.**\nThis is incorrect. The standard axiomatic framework (with countable additivity) is perfectly capable of explaining the phenomenon. The reconciliation lies in understanding that the practical, finite-precision model (Model D) is fundamentally different from the idealized continuous model (Model C), not in changing the mathematical foundation of probability theory itself.\n\nTherefore, options A and C are the correct statements.",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}