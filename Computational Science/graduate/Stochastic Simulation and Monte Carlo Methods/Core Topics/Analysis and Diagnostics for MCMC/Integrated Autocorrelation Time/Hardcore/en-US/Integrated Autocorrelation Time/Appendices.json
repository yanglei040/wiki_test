{
    "hands_on_practices": [
        {
            "introduction": "The autoregressive process of order 1, or AR(1) process, is a cornerstone model for time-correlated data in fields ranging from econometrics to statistical physics. It describes a simple memory effect where the current state is a scaled version of the previous state plus a random shock. This exercise  is fundamental for building intuition about how the strength and sign of this memory, controlled by the parameter $\\phi$, directly map to the integrated autocorrelation time $\\tau_{\\mathrm{int}}$, our key measure of simulation inefficiency. By deriving this relationship from first principles, you will gain a deep understanding of why persistent chains ($\\phi  0$) require longer runs and how anti-persistent chains ($\\phi  0$) can surprisingly be more efficient than independent sampling.",
            "id": "3313056",
            "problem": "Consider a stationary autoregressive process of order $1$ (AR($1$)) defined by $X_{t}=\\phi X_{t-1}+\\varepsilon_{t}$ for $t\\in\\mathbb{Z}$, where $\\{ \\varepsilon_{t} \\}$ are independent and identically distributed with $\\varepsilon_{t}\\sim \\mathcal{N}(0,\\sigma^{2})$, and $|\\phi|1$. Let the autocovariance function be $\\gamma(k)=\\mathbb{E}\\!\\left[(X_{t}-\\mu)(X_{t+k}-\\mu)\\right]$ with $\\mu=\\mathbb{E}[X_{t}]$, the normalized autocorrelation function be $\\rho_{k}=\\gamma(k)/\\gamma(0)$, and the integrated autocorrelation time (as used in Markov chain Monte Carlo (MCMC)) be defined by\n$$\n\\tau_{\\mathrm{int}}=1+2\\sum_{k=1}^{\\infty}\\rho_{k}.\n$$\nStarting from these definitions and the AR($1$) recursion only, derive a closed-form expression for $\\tau_{\\mathrm{int}}$ as a function of $\\phi$, valid for $|\\phi|1$. Then, specialize to the case $\\phi0$ and establish that the alternating-sign correlations imply $\\tau_{\\mathrm{int}}1$. Finally, determine the minimum attainable value of $\\tau_{\\mathrm{int}}$ in the limit $\\phi\\to -1^{+}$.\n\nProvide your final answer as a pair consisting of the exact closed-form $\\tau_{\\mathrm{int}}(\\phi)$ and the limiting minimal value as $\\phi\\to -1^{+}$. No rounding is required, and no physical units are involved.",
            "solution": "The problem is to derive the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, for a stationary AR($1$) process and find its minimum value in the limit $\\phi\\to -1^{+}$.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n-   Process definition: Stationary autoregressive process of order $1$ (AR($1$)), $X_{t}=\\phi X_{t-1}+\\varepsilon_{t}$ for $t\\in\\mathbb{Z}$.\n-   Noise properties: $\\{ \\varepsilon_{t} \\}$ are independent and identically distributed (i.i.d.) with $\\varepsilon_{t}\\sim \\mathcal{N}(0,\\sigma^{2})$.\n-   Stationarity condition: $|\\phi|1$.\n-   Autocovariance function definition: $\\gamma(k)=\\mathbb{E}\\!\\left[(X_{t}-\\mu)(X_{t+k}-\\mu)\\right]$, where $\\mu=\\mathbb{E}[X_{t}]$.\n-   Normalized autocorrelation function definition: $\\rho_{k}=\\gamma(k)/\\gamma(0)$.\n-   Integrated autocorrelation time definition: $\\tau_{\\mathrm{int}}=1+2\\sum_{k=1}^{\\infty}\\rho_{k}$.\n-   Tasks:\n    1.  Derive a closed-form expression for $\\tau_{\\mathrm{int}}$ as a function of $\\phi$.\n    2.  For $\\phi0$, establish that $\\tau_{\\mathrm{int}}1$.\n    3.  Determine the minimum attainable value of $\\tau_{\\mathrm{int}}$ in the limit $\\phi\\to -1^{+}$.\n    4.  The final answer should be a pair: the expression for $\\tau_{\\mathrm{int}}(\\phi)$ and its limiting minimal value.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard derivation in the field of time series analysis and Monte Carlo methods. All terms are formally defined, the constraints are consistent and sufficient, and the tasks lead to a unique, verifiable mathematical result. The problem does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\n**Derivation**\n\nOur derivation proceeds in several steps:\n1.  Determine the mean $\\mu$ of the process.\n2.  Determine the autocovariance function $\\gamma(k)$.\n3.  Determine the normalized autocorrelation function $\\rho_{k}$.\n4.  Compute the sum to find $\\tau_{\\mathrm{int}}$.\n5.  Analyze the results for the specified conditions.\n\n**1. Mean of the Process**\nWe take the expectation of the AR($1$) equation:\n$$\n\\mathbb{E}[X_{t}] = \\mathbb{E}[\\phi X_{t-1}+\\varepsilon_{t}] = \\phi\\mathbb{E}[X_{t-1}] + \\mathbb{E}[\\varepsilon_{t}]\n$$\nSince the process is stationary, the mean is constant, i.e., $\\mathbb{E}[X_{t}] = \\mathbb{E}[X_{t-1}] = \\mu$. From the problem statement, we know $\\mathbb{E}[\\varepsilon_{t}]=0$. Substituting these into the equation gives:\n$$\n\\mu = \\phi\\mu + 0\n$$\n$$\n\\mu(1-\\phi) = 0\n$$\nGiven the condition $|\\phi|1$, we have $1-\\phi \\neq 0$. Therefore, the mean of the process must be $\\mu=0$.\n\n**2. Autocovariance Function $\\gamma(k)$**\nWith $\\mu=0$, the autocovariance function simplifies to $\\gamma(k) = \\mathbb{E}[X_{t}X_{t+k}]$.\n\nFirst, we find the variance, $\\gamma(0)$:\n$$\n\\gamma(0) = \\mathbb{E}[X_t^2] = \\mathbb{E}[(\\phi X_{t-1}+\\varepsilon_{t})^2] = \\mathbb{E}[\\phi^2 X_{t-1}^2 + 2\\phi X_{t-1}\\varepsilon_t + \\varepsilon_t^2]\n$$\n$$\n\\gamma(0) = \\phi^2 \\mathbb{E}[X_{t-1}^2] + 2\\phi \\mathbb{E}[X_{t-1}\\varepsilon_t] + \\mathbb{E}[\\varepsilon_t^2]\n$$\nBy stationarity, $\\mathbb{E}[X_{t-1}^2] = \\gamma(0)$. We are given $\\mathbb{E}[\\varepsilon_t^2] = \\sigma^2$. The process $X_{t-1}$ is a function of $\\varepsilon_{t-1}, \\varepsilon_{t-2}, \\dots$. Since the noise terms $\\{\\varepsilon_t\\}$ are i.i.d., $X_{t-1}$ is independent of $\\varepsilon_t$. Thus, $\\mathbb{E}[X_{t-1}\\varepsilon_t] = \\mathbb{E}[X_{t-1}]\\mathbb{E}[\\varepsilon_t] = \\mu \\cdot 0 = 0$.\nSubstituting these results:\n$$\n\\gamma(0) = \\phi^2 \\gamma(0) + \\sigma^2\n$$\n$$\n\\gamma(0)(1-\\phi^2) = \\sigma^2\n$$\n$$\n\\gamma(0) = \\frac{\\sigma^2}{1-\\phi^2}\n$$\nThis is well-defined as $|\\phi|1$ implies $1-\\phi^2  0$.\n\nNext, we find $\\gamma(k)$ for $k \\ge 1$. We start with the definition $\\gamma(k) = \\mathbb{E}[X_{t}X_{t+k}]$ and substitute the AR($1$) relation for $X_{t+k}$:\n$$\n\\gamma(k) = \\mathbb{E}[X_t(\\phi X_{t+k-1} + \\varepsilon_{t+k})] = \\phi\\mathbb{E}[X_t X_{t+k-1}] + \\mathbb{E}[X_t \\varepsilon_{t+k}]\n$$\nThe first term is $\\phi\\gamma(k-1)$. For the second term, $X_t$ is a function of $\\varepsilon_t, \\varepsilon_{t-1}, \\dots$. Since $k \\ge 1$, $t+k  t$, and $X_t$ is independent of $\\varepsilon_{t+k}$. Therefore, $\\mathbb{E}[X_t \\varepsilon_{t+k}] = \\mathbb{E}[X_t]\\mathbb{E}[\\varepsilon_{t+k}] = \\mu \\cdot 0 = 0$.\nThis leads to the Yule-Walker equations for an AR($1$) process:\n$$\n\\gamma(k) = \\phi\\gamma(k-1) \\quad \\text{for } k \\ge 1\n$$\nThis is a recursive relationship. Starting from $\\gamma(0)$, we find:\n$$\n\\gamma(1) = \\phi\\gamma(0)\n$$\n$$\n\\gamma(2) = \\phi\\gamma(1) = \\phi^2\\gamma(0)\n$$\nBy induction, for any integer $k \\ge 0$:\n$$\n\\gamma(k) = \\phi^k \\gamma(0)\n$$\n\n**3. Normalized Autocorrelation Function $\\rho_k$**\nThe normalized autocorrelation function is defined as $\\rho_k = \\gamma(k)/\\gamma(0)$. Using our result for $\\gamma(k)$:\n$$\n\\rho_k = \\frac{\\phi^k \\gamma(0)}{\\gamma(0)} = \\phi^k\n$$\n\n**4. Integrated Autocorrelation Time $\\tau_{\\mathrm{int}}$**\nNow we can compute $\\tau_{\\mathrm{int}}$ using its definition:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2\\sum_{k=1}^{\\infty}\\rho_{k} = 1 + 2\\sum_{k=1}^{\\infty}\\phi^k\n$$\nThe sum is a geometric series. Since $|\\phi|1$, the series converges to:\n$$\n\\sum_{k=1}^{\\infty}\\phi^k = \\frac{\\phi}{1-\\phi}\n$$\nSubstituting this into the expression for $\\tau_{\\mathrm{int}}$:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2\\left(\\frac{\\phi}{1-\\phi}\\right) = \\frac{1-\\phi}{1-\\phi} + \\frac{2\\phi}{1-\\phi} = \\frac{1-\\phi+2\\phi}{1-\\phi}\n$$\nThe closed-form expression for the integrated autocorrelation time is:\n$$\n\\tau_{\\mathrm{int}}(\\phi) = \\frac{1+\\phi}{1-\\phi}\n$$\nThis completes the first part of the problem.\n\n**5. Analysis of Special Cases**\n\n**Case $\\phi  0$**\nWhen $\\phi0$, the correlations $\\rho_k=\\phi^k$ alternate in sign: $\\rho_10$, $\\rho_20$, $\\rho_30$, and so on. We need to establish that $\\tau_{\\mathrm{int}}1$.\nUsing the derived expression, if we have $-1  \\phi  0$:\n- The numerator, $1+\\phi$, is positive, since $\\phi  -1$. Also, $1+\\phi  1$.\n- The denominator, $1-\\phi$, is positive and greater than $1$, since $\\phi  0$.\nSo we have $0  1+\\phi  1$ and $1  1-\\phi  2$.\nThe ratio $\\tau_{\\mathrm{int}} = \\frac{1+\\phi}{1-\\phi}$ is therefore a positive number less than $1$. To see this more formally, $\\tau_{\\mathrm{int}}  1$ is equivalent to $\\frac{1+\\phi}{1-\\phi}  1$. Since $1-\\phi0$, we can multiply both sides by it without changing the inequality direction:\n$$\n1+\\phi  1-\\phi\n$$\n$$\n2\\phi  0\n$$\n$$\n\\phi  0\n$$\nThis inequality holds by assumption for this case, so we have proven that $\\tau_{\\mathrm{int}}  1$ for $\\phi  0$. This implies that anticorrelations ($\\rho_k0$ for odd $k$) can lead to more efficient sampling than independent sampling ($\\tau_{\\mathrm{int}}1$), a key result in MCMC methods.\n\n**Minimum Value in the Limit $\\phi \\to -1^{+}$**\nFinally, we need to find the minimum value of $\\tau_{\\mathrm{int}}$ as $\\phi$ approaches $-1$ from above. This corresponds to the limit:\n$$\n\\lim_{\\phi\\to -1^{+}} \\tau_{\\mathrm{int}}(\\phi) = \\lim_{\\phi\\to -1^{+}} \\frac{1+\\phi}{1-\\phi}\n$$\nSince the function is continuous at $\\phi=-1$, we can evaluate the limit by direct substitution:\n$$\n\\lim_{\\phi\\to -1^{+}} \\frac{1+\\phi}{1-\\phi} = \\frac{1+(-1)}{1-(-1)} = \\frac{0}{2} = 0\n$$\nTo confirm this is the minimum attainable value on the interval $(-1,1)$, we can examine the derivative of $\\tau_{\\mathrm{int}}(\\phi)$ with respect to $\\phi$:\n$$\n\\frac{d\\tau_{\\mathrm{int}}}{d\\phi} = \\frac{d}{d\\phi}\\left(\\frac{1+\\phi}{1-\\phi}\\right) = \\frac{(1)(1-\\phi) - (1+\\phi)(-1)}{(1-\\phi)^2} = \\frac{1-\\phi+1+\\phi}{(1-\\phi)^2} = \\frac{2}{(1-\\phi)^2}\n$$\nSince the derivative is always positive for $\\phi \\in (-1,1)$, the function $\\tau_{\\mathrm{int}}(\\phi)$ is strictly increasing on its entire domain. Therefore, its minimum value is approached at the left boundary of the domain, i.e., as $\\phi \\to -1^{+}$. The minimum value is $0$.\n\nThe two parts of the final answer are the derived expression for $\\tau_{\\mathrm{int}}(\\phi)$ and its limiting value as $\\phi \\to -1^{+}$.\n1.  $\\tau_{\\mathrm{int}}(\\phi) = \\frac{1+\\phi}{1-\\phi}$\n2.  $\\lim_{\\phi\\to -1^{+}} \\tau_{\\mathrm{int}}(\\phi) = 0$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1+\\phi}{1-\\phi}  0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In contrast to the infinite memory of an AR(1) process, many physical and statistical systems exhibit correlations that vanish after a short time. The moving-average process of order 1, or MA(1) process, is the canonical model for such finite memory, where a state is influenced only by the most recent random shocks. This exercise  challenges you to derive the integrated autocorrelation time for this distinct correlation structure. The simplicity of the final result reveals how fundamentally the nature of the underlying process dictates the behavior of $\\tau_{\\mathrm{int}}$ and sharpens your ability to connect a stochastic model to its efficiency characteristics.",
            "id": "3312989",
            "problem": "Consider a zero-mean moving-average process of order one defined by $X_t=\\epsilon_t+\\theta\\,\\epsilon_{t-1}$, where $\\{\\epsilon_t\\}_{t\\in\\mathbb{Z}}$ is a white-noise sequence with $\\mathbb{E}[\\epsilon_t]=0$, $\\mathrm{Var}(\\epsilon_t)=\\sigma_{\\epsilon}^{2}\\in(0,\\infty)$, and $\\epsilon_t$ independent of $\\epsilon_s$ for $t\\neq s$. Let $\\gamma_k=\\mathrm{Cov}(X_t,X_{t+k})$ denote the autocovariance function at lag $k\\in\\mathbb{Z}$ and $\\rho_k=\\gamma_k/\\gamma_0$ the corresponding autocorrelation function. The integrated autocorrelation time (IAT) of the process, as used in the analysis of Monte Carlo estimators, is defined by $\\tau_{\\mathrm{int}}=1+2\\sum_{k=1}^{\\infty}\\rho_k$ provided the series converges absolutely.\n\nTasks:\n- Starting from the given definitions and properties of white noise, derive $\\gamma_k$ for all integer lags $k$, and hence obtain $\\rho_k$ for all integer lags $k$.\n- Use your result to compute the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ as a function of the parameter $\\theta$.\n- Based on your expression for $\\tau_{\\mathrm{int}}$, identify the range of $\\theta\\in\\mathbb{R}$ for which $\\tau_{\\mathrm{int}}1$, and provide a brief interpretation of this condition in terms of serial dependence.\n\nYour final reported answer must be a single closed-form analytic expression for $\\tau_{\\mathrm{int}}$ as a function of $\\theta$. No numerical rounding is required.",
            "solution": "The problem is well-posed and scientifically grounded in the theory of stochastic processes. We proceed with the derivation.\n\nThe moving-average process of order one is defined as $X_t = \\epsilon_t + \\theta \\epsilon_{t-1}$, where $\\{\\epsilon_t\\}$ is a white-noise sequence with $\\mathbb{E}[\\epsilon_t]=0$ and $\\mathrm{Var}(\\epsilon_t) = \\mathbb{E}[\\epsilon_t^2] = \\sigma_\\epsilon^2$. The noise terms are independent, which implies $\\mathbb{E}[\\epsilon_t \\epsilon_s] = 0$ for $t \\neq s$. We can summarize this property using the Kronecker delta $\\delta_{ts}$ as $\\mathbb{E}[\\epsilon_t \\epsilon_s] = \\delta_{ts} \\sigma_\\epsilon^2$.\n\nFirst, we confirm that the process $X_t$ has a zero mean:\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[\\epsilon_t + \\theta \\epsilon_{t-1}] = \\mathbb{E}[\\epsilon_t] + \\theta \\mathbb{E}[\\epsilon_{t-1}] = 0 + \\theta \\cdot 0 = 0\n$$\n\n**Task 1: Derivation of the autocovariance and autocorrelation functions**\n\nThe autocovariance function at lag $k$ is $\\gamma_k = \\mathrm{Cov}(X_t, X_{t+k})$. Since the process is zero-mean, this simplifies to:\n$$\n\\gamma_k = \\mathbb{E}[X_t X_{t+k}] = \\mathbb{E}[(\\epsilon_t + \\theta \\epsilon_{t-1})(\\epsilon_{t+k} + \\theta \\epsilon_{t+k-1})]\n$$\nExpanding the product and using the linearity of the expectation operator, we get:\n$$\n\\gamma_k = \\mathbb{E}[\\epsilon_t \\epsilon_{t+k}] + \\theta \\mathbb{E}[\\epsilon_t \\epsilon_{t+k-1}] + \\theta \\mathbb{E}[\\epsilon_{t-1} \\epsilon_{t+k}] + \\theta^2 \\mathbb{E}[\\epsilon_{t-1} \\epsilon_{t+k-1}]\n$$\nWe evaluate this expression for different integer values of $k$.\n\nCase 1: $k=0$ (Variance)\nFor $k=0$, the expression becomes:\n$$\n\\gamma_0 = \\mathbb{E}[\\epsilon_t \\epsilon_t] + \\theta \\mathbb{E}[\\epsilon_t \\epsilon_{t-1}] + \\theta \\mathbb{E}[\\epsilon_{t-1} \\epsilon_t] + \\theta^2 \\mathbb{E}[\\epsilon_{t-1} \\epsilon_{t-1}]\n$$\nUsing the property $\\mathbb{E}[\\epsilon_t \\epsilon_s] = \\delta_{ts} \\sigma_\\epsilon^2$:\n$$\n\\gamma_0 = \\sigma_\\epsilon^2 + \\theta \\cdot 0 + \\theta \\cdot 0 + \\theta^2 \\sigma_\\epsilon^2 = (1 + \\theta^2)\\sigma_\\epsilon^2\n$$\n\nCase 2: $k=1$ (Lag-1 autocovariance)\nFor $k=1$, the expression becomes:\n$$\n\\gamma_1 = \\mathbb{E}[\\epsilon_t \\epsilon_{t+1}] + \\theta \\mathbb{E}[\\epsilon_t \\epsilon_t] + \\theta \\mathbb{E}[\\epsilon_{t-1} \\epsilon_{t+1}] + \\theta^2 \\mathbb{E}[\\epsilon_{t-1} \\epsilon_t]\n$$\nThe only non-zero expectation term is $\\mathbb{E}[\\epsilon_t \\epsilon_t] = \\sigma_\\epsilon^2$:\n$$\n\\gamma_1 = 0 + \\theta \\sigma_\\epsilon^2 + \\theta \\cdot 0 + \\theta^2 \\cdot 0 = \\theta \\sigma_\\epsilon^2\n$$\n\nCase 3: $|k| \\ge 2$\nFor $k \\ge 2$, all time indices in the expectation terms are different:\n- $\\mathbb{E}[\\epsilon_t \\epsilon_{t+k}]$: indices differ by $k \\ge 2$.\n- $\\mathbb{E}[\\epsilon_t \\epsilon_{t+k-1}]$: indices differ by $k-1 \\ge 1$.\n- $\\mathbb{E}[\\epsilon_{t-1} \\epsilon_{t+k}]$: indices differ by $k+1 \\ge 3$.\n- $\\mathbb{E}[\\epsilon_{t-1} \\epsilon_{t+k-1}]$: indices differ by $k \\ge 2$.\nThus, all expectation terms are zero.\n$$\n\\gamma_k = 0 \\quad \\text{for } k \\ge 2\n$$\nThe autocovariance function is symmetric, i.e., $\\gamma_k = \\gamma_{-k}$. Therefore, $\\gamma_k = 0$ for all $|k| \\ge 2$, and $\\gamma_{-1} = \\gamma_1 = \\theta \\sigma_\\epsilon^2$.\n\nIn summary, the autocovariance function $\\gamma_k$ is:\n$$\n\\gamma_k =\n\\begin{cases}\n(1+\\theta^2)\\sigma_\\epsilon^2  \\text{if } k=0 \\\\\n\\theta\\sigma_\\epsilon^2  \\text{if } |k|=1 \\\\\n0  \\text{if } |k| \\ge 2\n\\end{cases}\n$$\nThe autocorrelation function is defined as $\\rho_k = \\gamma_k / \\gamma_0$.\nFor $k=0$, $\\rho_0 = \\gamma_0 / \\gamma_0 = 1$.\nFor $|k|=1$, $\\rho_1 = \\rho_{-1} = \\gamma_1 / \\gamma_0 = \\frac{\\theta \\sigma_\\epsilon^2}{(1+\\theta^2)\\sigma_\\epsilon^2} = \\frac{\\theta}{1+\\theta^2}$.\nFor $|k| \\ge 2$, $\\rho_k = 0 / \\gamma_0 = 0$.\n\nIn summary, the autocorrelation function $\\rho_k$ is:\n$$\n\\rho_k =\n\\begin{cases}\n1  \\text{if } k=0 \\\\\n\\frac{\\theta}{1+\\theta^2}  \\text{if } |k|=1 \\\\\n0  \\text{if } |k| \\ge 2\n\\end{cases}\n$$\n\n**Task 2: Computation of the integrated autocorrelation time**\n\nThe integrated autocorrelation time $\\tau_{\\mathrm{int}}$ is defined by the series $\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k$. Since $\\rho_k = 0$ for all $k \\ge 2$, the infinite sum truncates to a single term:\n$$\n\\sum_{k=1}^{\\infty} \\rho_k = \\rho_1 + \\rho_2 + \\rho_3 + \\dots = \\rho_1 + 0 + 0 + \\dots = \\rho_1\n$$\nSubstituting this into the definition of $\\tau_{\\mathrm{int}}$:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2 \\rho_1 = 1 + 2 \\left( \\frac{\\theta}{1+\\theta^2} \\right)\n$$\nWe can write this as a single fraction:\n$$\n\\tau_{\\mathrm{int}} = \\frac{1+\\theta^2}{1+\\theta^2} + \\frac{2\\theta}{1+\\theta^2} = \\frac{1+\\theta^2+2\\theta}{1+\\theta^2} = \\frac{(1+\\theta)^2}{1+\\theta^2}\n$$\n\n**Task 3: Analysis of the condition $\\tau_{\\mathrm{int}}  1$**\n\nWe seek the range of $\\theta \\in \\mathbb{R}$ for which $\\tau_{\\mathrm{int}}  1$.\n$$\n\\frac{(1+\\theta)^2}{1+\\theta^2}  1\n$$\nSince $1+\\theta^2  0$ for any real $\\theta$, we can multiply both sides by $1+\\theta^2$ without changing the direction of the inequality:\n$$\n(1+\\theta)^2  1+\\theta^2\n$$\nExpanding the left side:\n$$\n1 + 2\\theta + \\theta^2  1 + \\theta^2\n$$\nSubtracting $1+\\theta^2$ from both sides yields:\n$$\n2\\theta  0\n$$\n$$\n\\theta  0\n$$\nThe condition $\\tau_{\\mathrm{int}}  1$ holds for all $\\theta \\in (-\\infty, 0)$.\n\nInterpretation: The integrated autocorrelation time $\\tau_{\\mathrm{int}}$ measures the effective number of time steps over which the process values are correlated.\n-   If $\\tau_{\\mathrm{int}}  1$, the sum of autocorrelations is positive ($\\sum \\rho_k  0$), indicating overall positive serial dependence (persistence). A value above the mean tends to be followed by another value above the mean.\n-   If $\\tau_{\\mathrm{int}} = 1$, the sum is zero, characteristic of an uncorrelated process like white noise.\n-   If $\\tau_{\\mathrm{int}}  1$, the sum is negative ($\\sum \\rho_k  0$), indicating overall negative serial dependence (anti-persistence). A value above the mean tends to be followed by a value below the mean.\n\nFor the MA(1) process, the condition $\\tau_{\\mathrm{int}}  1$ is equivalent to $2\\rho_1  0$, or $\\rho_1  0$. Since $\\rho_1 = \\frac{\\theta}{1+\\theta^2}$, and $1+\\theta^2  0$, the sign of $\\rho_1$ is determined by the sign of $\\theta$. Thus, $\\tau_{\\mathrm{int}}  1$ precisely when $\\theta  0$.\nA negative value of $\\theta$ in $X_t = \\epsilon_t + \\theta \\epsilon_{t-1}$ means that a positive shock $\\epsilon_{t-1}$ at time $t-1$ contributes positively to $X_{t-1}$ but negatively to $X_t$. This induces an oscillatory behavior where successive values of the time series are negatively correlated, which is the meaning of anti-persistence.",
            "answer": "$$\\boxed{\\frac{(1+\\theta)^2}{1+\\theta^2}}$$"
        },
        {
            "introduction": "Moving from idealized models to practical data analysis, we confront the challenge that the true autocorrelation function is unknown and must be estimated. This final practice  serves as a crucial bridge between theory and application, highlighting a common and dangerous pitfall in estimating the integrated autocorrelation time from finite data. You will first derive the formal definition of $\\tau_{\\mathrm{int}}$ from the variance of the sample mean, cementing its theoretical importance. Then, through a combination of analytical work and coding, you will investigate a synthetic process with oscillatory correlations to demonstrate why naively discarding negative correlation terms can lead to a severe overestimation of the statistical error, providing a powerful lesson in the subtleties of practical MCMC output analysis.",
            "id": "3313017",
            "problem": "Consider a discrete-time, strictly stationary, ergodic, mean-zero process with finite variance, denoted by the sequence of random variables $\\{X_t\\}_{t \\in \\mathbb{Z}}$ with variance $\\mathbb{V}[X_t] = \\gamma_0$, autocovariance function $\\gamma_t = \\mathbb{E}[X_0 X_t]$, and normalized autocorrelation function $\\rho_t = \\gamma_t / \\gamma_0$. The variance of the arithmetic mean $\\,\\bar{X}_n = \\frac{1}{n}\\sum_{t=1}^n X_t\\,$ can be expressed in terms of $\\gamma_t$ via identities that hold for wide-sense stationary processes. Starting from these core definitions, derive the asymptotic form of $\\mathbb{V}[\\bar{X}_n]$ for large $n$ such that it is proportional to $\\gamma_0/n$ multiplied by a dimensionless factor aggregating lagged correlation contributions.\n\nIn Markov Chain Monte Carlo (MCMC), this dimensionless factor is called the integrated autocorrelation time (IACT). In practice, one often estimates IACT by truncating the sum of lagged autocorrelation contributions at a window $m$ and sometimes discards negative-lag contributions (i.e., negative values of $\\rho_t$ at positive lags $t \\ge 1$). For oscillatory autocorrelations (for example, those that alternate in sign), discarding negative contributions can introduce a substantial upward bias in the IACT estimate because it destroys cancellation that would otherwise occur between positive and negative terms.\n\nYou will analyze this bias using a synthetic, purely mathematical autocorrelation model and implement estimators that either drop or include negative-lag contributions.\n\nDefinitions and model:\n- Let the normalized autocorrelation function be specified by $\\rho_0 = 1$ and, for positive lags $t \\ge 1$, by\n$$\n\\rho_t = a^t (-1)^t,\n$$\nwhere $a \\in (0,1)$ is a geometric decay parameter. This produces an oscillatory autocorrelation that alternates in sign.\n- Let the large-$n$ IACT be the dimensionless factor multiplying $\\gamma_0/n$ in the large-$n$ limit of $\\mathbb{V}[\\bar{X}_n]$, obtained from the aggregation of lagged autocorrelation contributions.\n- Consider two finite-window estimators at window size $m \\in \\mathbb{N}$:\n  1. The \"signed\" windowed estimator, which aggregates $\\rho_t$ for $t=1,\\dots,m$ with signs intact.\n  2. The \"drop-negative\" windowed estimator, which aggregates only the positive values of $\\rho_t$ for $t=1,\\dots,m$ and discards negative ones.\n\nTasks:\n1. Starting from the definitions of $\\gamma_t$ and $\\rho_t$, and the identity that expresses $\\mathbb{V}[\\bar{X}_n]$ in terms of autocovariances for wide-sense stationary processes, derive the large-$n$ dimensionless factor (IACT) as an infinite series in $\\rho_t$. Do not assume any shortcut formulas; derive the aggregation structure that yields the large-$n$ factor from first principles.\n2. For the oscillatory model $\\rho_t = a^t (-1)^t$, determine the exact large-$n$ IACT by summing the relevant infinite series. Express this IACT in closed form as a function of $a$.\n3. For a finite window $m$, derive closed forms for both windowed estimators:\n   - The \"signed\" windowed estimator that aggregates $\\rho_t$ for $t=1,\\dots,m$ with signs,\n   - The \"drop-negative\" windowed estimator that aggregates only those $\\rho_t$ that are positive in this range.\n4. Show analytically why discarding negative-lag contributions inflates the estimator when $\\rho_t$ alternates in sign. Your explanation must be in terms of cancellation in alternating series.\n5. Implement a program that, for each test case $(a,m)$, computes:\n   - The exact large-$n$ IACT,\n   - The signed windowed estimate,\n   - The drop-negative windowed estimate,\n   - The ratio of absolute errors, defined as\n     $$\n     R(a,m) = \\frac{\\left|\\text{drop-negative}(a,m) - \\text{IACT}(a)\\right|}{\\left|\\text{signed}(a,m) - \\text{IACT}(a)\\right|},\n     $$\n     with the convention that division by zero is regularized by replacing a zero denominator with a small positive constant $10^{-12}$.\n   A ratio $R(a,m)  1$ indicates that including negative-lag contributions reduces estimator bias compared to dropping them.\n\nAngle units are not involved in this problem. No physical units appear; all quantities are dimensionless.\n\nTest Suite:\nUse the following parameter pairs $(a,m)$:\n- $(0.9,50)$: happy path with strong correlation and a reasonably large window,\n- $(0.7,20)$: moderately strong correlation, moderate window,\n- $(0.99,100)$: boundary case close to a unit root with a large window,\n- $(0.3,10)$: weak correlation, small window,\n- $(0.9,1)$: extreme truncation boundary (tiny window).\n\nProgram Output Specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- Each test caseâ€™s result must be a list of four floats:\n  $[\\text{IACT}(a),\\ \\text{signed}(a,m),\\ \\text{drop-negative}(a,m),\\ R(a,m)]$.\n- The final output must be a single list containing these five test-case lists, for example:\n  $[[v_{11},v_{12},v_{13},v_{14}],[v_{21},v_{22},v_{23},v_{24}],\\dots]$.",
            "solution": "The user-provided problem has been assessed and is determined to be valid. It is a well-posed, scientifically grounded, and objective problem in the domain of stochastic simulation methods. All definitions are standard, and the tasks constitute a rigorous mathematical and computational exercise based on first principles.\n\n### 1. Derivation of the Large-$n$ Integrated Autocorrelation Time (IACT)\n\nWe begin with a discrete-time, strictly stationary, ergodic, mean-zero process $\\{X_t\\}_{t \\in \\mathbb{Z}}$. The variance is $\\mathbb{V}[X_t] = \\gamma_0$ and the autocovariance is $\\gamma_t = \\mathbb{E}[X_0 X_t]$. The arithmetic mean of $n$ samples is $\\bar{X}_n = \\frac{1}{n}\\sum_{t=1}^n X_t$.\n\nThe variance of the arithmetic mean is given by:\n$$\n\\mathbb{V}[\\bar{X}_n] = \\mathbb{V}\\left[\\frac{1}{n}\\sum_{t=1}^n X_t\\right]\n$$\nUsing the linearity property of variance for a sum of random variables, we have:\n$$\n\\mathbb{V}[\\bar{X}_n] = \\frac{1}{n^2} \\sum_{t=1}^n \\sum_{s=1}^n \\text{Cov}(X_t, X_s)\n$$\nFor a stationary process, the covariance depends only on the time lag, $\\text{Cov}(X_t, X_s) = \\gamma_{|t-s|}$. Substituting this into the sum:\n$$\n\\mathbb{V}[\\bar{X}_n] = \\frac{1}{n^2} \\sum_{t=1}^n \\sum_{s=1}^n \\gamma_{|t-s|}\n$$\nThis double summation can be re-indexed by the lag $k = t-s$. The lag $k$ ranges from $-(n-1)$ to $n-1$. For a given lag $k$, there are $n-|k|$ pairs $(t,s)$ in the sum such that $t-s=k$. This allows us to rewrite the sum as:\n$$\n\\mathbb{V}[\\bar{X}_n] = \\frac{1}{n^2} \\sum_{k=-(n-1)}^{n-1} (n-|k|) \\gamma_k\n$$\nDistributing the $1/n^2$ term:\n$$\n\\mathbb{V}[\\bar{X}_n] = \\frac{1}{n} \\sum_{k=-(n-1)}^{n-1} \\left(1 - \\frac{|k|}{n}\\right) \\gamma_k\n$$\nSince the process is real-valued, its autocovariance function is even, $\\gamma_k = \\gamma_{-k}$. We can separate the $k=0$ term and combine the positive and negative lags:\n$$\n\\mathbb{V}[\\bar{X}_n] = \\frac{1}{n} \\left[ \\gamma_0 + 2 \\sum_{t=1}^{n-1} \\left(1 - \\frac{t}{n}\\right) \\gamma_t \\right]\n$$\nFactoring out the variance $\\gamma_0$ and using the normalized autocorrelation function $\\rho_t = \\gamma_t / \\gamma_0$:\n$$\n\\mathbb{V}[\\bar{X}_n] = \\frac{\\gamma_0}{n} \\left[ 1 + 2 \\sum_{t=1}^{n-1} \\left(1 - \\frac{t}{n}\\right) \\rho_t \\right]\n$$\nFor a process with summable correlations (i.e., $\\sum_{t=1}^\\infty |\\rho_t|  \\infty$), as $n \\to \\infty$, the factor $(1 - t/n) \\to 1$ for any fixed $t$. The sum can be approximated by its infinite counterpart:\n$$\n\\mathbb{V}[\\bar{X}_n] \\approx \\frac{\\gamma_0}{n} \\left( 1 + 2 \\sum_{t=1}^\\infty \\rho_t \\right) \\quad \\text{for large } n\n$$\nThe dimensionless factor, defined as the integrated autocorrelation time ($\\tau$), is therefore:\n$$\n\\tau = 1 + 2 \\sum_{t=1}^{\\infty} \\rho_t\n$$\n\n### 2. Exact IACT for the Oscillatory Model\n\nThe problem specifies the model $\\rho_t = a^t (-1)^t = (-a)^t$ for $t \\ge 1$, with $a \\in (0,1)$. We substitute this into the IACT formula:\n$$\n\\tau_{\\text{exact}} = 1 + 2 \\sum_{t=1}^{\\infty} (-a)^t\n$$\nThe summation is an infinite geometric series with first term $-a$ and common ratio $-a$. Since $a \\in (0,1)$, the magnitude of the ratio $|-a|$ is less than $1$, so the series converges. The sum is given by:\n$$\n\\sum_{t=1}^{\\infty} (-a)^t = \\frac{\\text{first term}}{1 - \\text{ratio}} = \\frac{-a}{1 - (-a)} = \\frac{-a}{1+a}\n$$\nSubstituting this back into the expression for $\\tau_{\\text{exact}}$:\n$$\n\\tau_{\\text{exact}}(a) = 1 + 2 \\left( \\frac{-a}{1+a} \\right) = \\frac{1+a - 2a}{1+a} = \\frac{1-a}{1+a}\n$$\n\n### 3. Finite-Window Estimators\n\nThe two estimators are defined by truncating the sum at a finite window $m$.\n\n**Signed Windowed Estimator**: This estimator includes all terms up to lag $m$:\n$$\n\\tau_{\\text{signed}}(a,m) = 1 + 2 \\sum_{t=1}^{m} (-a)^t\n$$\nThe sum is a finite geometric series: $\\sum_{k=1}^m r^k = r \\frac{1-r^m}{1-r}$. With $r=-a$:\n$$\n\\sum_{t=1}^{m} (-a)^t = (-a) \\frac{1 - (-a)^m}{1 - (-a)} = -a \\frac{1 - (-a)^m}{1+a}\n$$\nThus, the estimator is:\n$$\n\\tau_{\\text{signed}}(a,m) = 1 - \\frac{2a(1 - (-a)^m)}{1+a}\n$$\n\n**Drop-Negative Windowed Estimator**: This estimator includes only the positive terms of $\\rho_t$ in the sum up to lag $m$. For the model $\\rho_t = (-a)^t$ with $a0$, the terms are positive only when the lag $t$ is an even integer. The estimator is:\n$$\n\\tau_{\\text{drop-neg}}(a,m) = 1 + 2 \\sum_{\\substack{t=1,\\dots,m \\\\ t \\text{ is even}}} \\rho_t\n$$\nLet $t = 2k$. The sum becomes $\\sum_{k=1}^{\\lfloor m/2 \\rfloor} \\rho_{2k} = \\sum_{k=1}^{\\lfloor m/2 \\rfloor} (-a)^{2k} = \\sum_{k=1}^{\\lfloor m/2 \\rfloor} (a^2)^k$. This is a geometric series with ratio $a^2$. Let $M = \\lfloor m/2 \\rfloor$. If $M=0$, the sum is $0$. If $M  0$, the sum is:\n$$\n\\sum_{k=1}^{M} (a^2)^k = a^2 \\frac{1 - (a^2)^M}{1 - a^2}\n$$\nThe estimator is therefore:\n$$\n\\tau_{\\text{drop-neg}}(a,m) = 1 + 2 \\left( a^2 \\frac{1 - (a^2)^{\\lfloor m/2 \\rfloor}}{1 - a^2} \\right) \\quad \\text{for } \\lfloor m/2 \\rfloor  0\n$$\nAnd $\\tau_{\\text{drop-neg}}(a,m) = 1$ for $\\lfloor m/2 \\rfloor = 0$ (i.e., $m2$).\n\n### 4. Analytical Explanation of Estimator Bias\n\nThe bias of an estimator is its expected deviation from the true value. Here, we analyze the systematic error introduced by dropping negative correlation terms.\n\nThe exact IACT, $\\tau_{\\text{exact}} = \\frac{1-a}{1+a}$, is derived from the sum $S_{\\infty} = \\sum_{t=1}^\\infty \\rho_t = \\frac{-a}{1+a}$. Since $a \\in (0,1)$, this sum is negative. The negativity arises from the cancellation inherent in an alternating series where the first term ($\\rho_1 = -a$) is negative and has the largest magnitude. The sum of negative-lag terms (odd $t$) outweighs the sum of positive-lag terms (even $t$).\n\nThe **signed estimator** approximates this alternating sum with a finite partial sum, $S_m = \\sum_{t=1}^m \\rho_t$. The partial sums of a convergent alternating series oscillate around the limit. The error $|S_m - S_{\\infty}|$ decreases as $m$ increases. This estimator correctly captures the essential feature of the series: the cancellation between positive and negative terms.\n\nThe **drop-negative estimator** is constructed from the sum $S'_m = \\sum_{t=1}^m \\max(0, \\rho_t)$. For the model $\\rho_t = (-a)^t$, this sum includes only the positive terms $\\rho_t$ where $t$ is even.\n$$\nS'_m = a^2 + a^4 + \\dots + a^{2\\lfloor m/2 \\rfloor}\n$$\nThis sum is manifestly positive for $m \\ge 2$. The estimator $\\tau_{\\text{drop-neg}} = 1 + 2S'_m$ is therefore always greater than $1$. In contrast, the true value $\\tau_{\\text{exact}} = (1-a)/(1+a)$ is always less than $1$.\n\nThe fundamental flaw of the drop-negative procedure is that it attempts to estimate a quantity based on a negative sum ($S_{\\infty}$) using a strictly positive sum ($S'_m$). By discarding all negative contributions, it completely destroys the crucial cancellation that defines the behavior of the series. This systematically inflates the estimate, leading to a large positive bias. The signed estimator, by preserving the cancellation, provides a far more accurate approximation of the true underlying physics of the correlated system. The ratio of errors $R(a,m)$ is expected to be large, confirming that the signed estimator is substantially less biased.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes IACT estimates for a set of test cases based on an oscillatory\n    autocorrelation model.\n    \"\"\"\n    # Test cases defined in the problem statement.\n    test_cases = [\n        (0.9, 50),\n        (0.7, 20),\n        (0.99, 100),\n        (0.3, 10),\n        (0.9, 1),\n    ]\n\n    results = []\n    \n    # Small constant for regularizing division by zero.\n    epsilon = 1e-12\n\n    for a, m in test_cases:\n        # Task 2: Calculate the exact large-n IACT.\n        # tau_exact = (1 - a) / (1 + a)\n        iact_exact = (1.0 - a) / (1.0 + a)\n\n        # Task 3: Calculate the signed windowed estimator.\n        # tau_signed = 1 - (2*a*(1 - (-a)**m)) / (1 + a)\n        sum_part_signed = (2.0 * a * (1.0 - (-a)**m)) / (1.0 + a)\n        signed_estimate = 1.0 - sum_part_signed\n\n        # Task 3: Calculate the drop-negative windowed estimator.\n        # This sums only positive rho_t, which occur at even lags t.\n        # The sum is for t = 2, 4, ..., 2*M where M = m // 2.\n        # Sum = a^2 * (1 - (a^2)^M) / (1 - a^2)\n        M = m // 2\n        if M == 0:\n            sum_part_drop_neg = 0.0\n        else:\n            a_sq = a**2\n            # Geometric series sum: r * (1-r^n) / (1-r) with r=a^2, n=M\n            sum_drop_neg_terms = a_sq * (1.0 - a_sq**M) / (1.0 - a_sq)\n            sum_part_drop_neg = 2.0 * sum_drop_neg_terms\n        \n        drop_negative_estimate = 1.0 + sum_part_drop_neg\n\n        # Task 5: Calculate the ratio of absolute errors.\n        # R(a,m) = |drop-negative - IACT| / |signed - IACT|\n        abs_error_signed = abs(signed_estimate - iact_exact)\n        abs_error_drop_neg = abs(drop_negative_estimate - iact_exact)\n        \n        # Regularize the denominator to avoid division by zero.\n        denominator = abs_error_signed if abs_error_signed > epsilon else epsilon\n\n        error_ratio = abs_error_drop_neg / denominator\n\n        results.append([\n            iact_exact,\n            signed_estimate,\n            drop_negative_estimate,\n            error_ratio\n        ])\n\n    # Format the final output string according to the specification.\n    # [[v11,v12,v13,v14],[v21,v22,v23,v24],...]\n    case_results_str = [\n        f\"[{','.join(f'{val:.8f}' for val in res)}]\" for res in results\n    ]\n    final_output = f\"[{','.join(case_results_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}