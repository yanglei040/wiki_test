{
    "hands_on_practices": [
        {
            "introduction": "To move from subjective visual checks to objective assessment, we must quantify the properties of a \"good\" trace plot. This exercise guides you through deriving and implementing a core set of convergence diagnostics from fundamental statistical principles, including the widely used potential scale reduction factor, $\\hat{R}$ . By applying these tools to simulated chains with known properties, you will develop a practical understanding of how these metrics detect common convergence failures.",
            "id": "3289581",
            "problem": "You are tasked with formalizing a quantitative method to assess convergence from trace plots in stochastic simulation, starting from core definitions in probability and statistics. You will then implement and apply this method to a specified test suite of simulated Markov Chain Monte Carlo traces.\n\nFundamental base and scenario: Consider $m$ parallel simulated sequences (“chains”) $\\{x_{j,t}\\}_{t=1}^{n}$ for $j \\in \\{1,\\dots,m\\}$ of a scalar parameter, each generated by a first-order autoregressive process (AR($1$)) centered on a possibly time-varying mean:\n$$\nx_{j,t} \\;=\\; \\mu_{j,t} \\;+\\; \\phi\\left(x_{j,t-1}-\\mu_{j,t}\\right) \\;+\\; \\varepsilon_{j,t},\n$$\nwith $\\varepsilon_{j,t} \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}\\!\\left(0, \\sigma^2(1-\\phi^2)\\right)$, where $\\phi \\in (-1,1)$ controls correlation and $\\sigma^2$ is the stationary marginal variance when $\\mu_{j,t}$ is constant. For constant $\\mu_{j,t} \\equiv \\mu$, the stationary distribution is $\\mathcal{N}(\\mu,\\sigma^2)$.\n\nYour goal is to derive, implement, and apply the following quantitative “trace-extraction” diagnostics, each grounded in core definitions:\n- A between-versus-within variance comparison across split chains (to detect between-chain inconsistencies) derived from the law of total variance and sample variance definitions.\n- A lag-$1$ autocorrelation estimator (to detect poor mixing) derived from sample autocovariance and variance.\n- An early-versus-late window standardized drift magnitude (to detect nonstationary trends) derived from sample means and pooled variance.\n- An approximate total effective sample size under an AR($1$) approximation derived from the integrated autocorrelation time.\n\nUsing these diagnostics, a chain ensemble is declared “converged” if and only if all of the following are simultaneously satisfied:\n1. The split-chain potential scale reduction factor $\\hat{R}$ is at most $\\tau_R = 1.05$.\n2. The average lag-$1$ autocorrelation across chains is at most $\\tau_\\rho = 0.3$.\n3. The standardized drift magnitude is at most $\\tau_D = 0.15$.\n4. The total effective sample size is at least $E_{\\min} = 2000$.\n\nDerive computable formulas as follows, starting only from the definitions of sample mean, sample variance, autocovariance, the law of total variance, and the AR($1$) integrated autocorrelation time:\n\n- Split-chain variability comparison: Split each chain into two halves, yielding $m_s = 2m$ sequences of equal length $n_s = n/2$. Let $\\bar{x}_{.j}$ denote the sample mean of split sequence $j$ and $s_j^2$ its sample variance. Let $\\bar{x}_{..}$ be the mean across all split sequences. Define the within-sequence variance $W$ as the average of $s_j^2$ and the between-sequence variance $B$ proportional to the variance of $\\bar{x}_{.j}$, and from these construct an overdispersed variance estimator $V^+$ and the potential scale reduction factor $\\hat{R}$. The derivation must begin from the sample mean and variance definitions and the law of total variance, and must not assume any pre-packaged formula.\n\n- Lag-$1$ autocorrelation: For each chain $j$, estimate the lag-$1$ autocovariance and autocorrelation using the definitions of sample autocovariance and sample variance, then average the autocorrelations across chains.\n\n- Standardized drift magnitude: For each chain $j$, compute the mean of the first $k$ samples and the mean of the last $k$ samples, where $k = \\lfloor 0.4 n \\rfloor$. Define the drift for chain $j$ as the absolute difference of these two means, and standardize by the pooled standard deviation across all chains and all times. Average this standardized drift over $j=1,\\dots,m$.\n\n- Effective sample size under AR($1$): Using the AR($1$) approximation, the integrated autocorrelation time is obtained from the geometric series of autocorrelations, yielding a computable expression in terms of lag-$1$ autocorrelation $\\rho$. From this, derive the total effective sample size for $m$ chains each of length $n$.\n\nTest suite: You must simulate the data for four cases, each with $m=4$ chains and $n=2000$ samples per chain, Gaussian noise variance specified via $\\sigma^2=1$, and random seed fixed at $12345$ for reproducibility. Use the AR($1$) recursion above with initial values $x_{j,1}$ drawn from the stationary distribution corresponding to the initial mean in each chain. For each case, define $\\mu_{j,t}$ by constant or stepwise-constant means as specified. For all cases, set $\\sigma^2=1$.\n\n- Case A (well-mixed, stationary): $m=4$, $n=2000$, $\\phi=0.2$, and $\\mu_{j,t} \\equiv 0$ for all $j$ and $t$.\n- Case B (sticky, highly autocorrelated): $m=4$, $n=2000$, $\\phi=0.98$, and $\\mu_{j,t} \\equiv 0$ for all $j$ and $t$.\n- Case C (intra-chain mean shift): $m=4$, $n=2000$, $\\phi=0.5$, and for each chain $j$, $\\mu_{j,t} = 0$ for $t \\le n/2$ and $\\mu_{j,t} = 1$ for $t > n/2$.\n- Case D (between-chain location discrepancy): $m=4$, $n=2000$, $\\phi=0.5$, with two chains having $\\mu_{j,t} \\equiv 0$ and two chains having $\\mu_{j,t} \\equiv 1$ for all $t$.\n\nAngle units and physical units are not applicable; no unit conversions are required.\n\nYour program must:\n- Implement the simulation for each case exactly as specified, using the same autoregressive recursion with $\\sigma^2=1$ and the fixed seed $12345$.\n- Compute the four diagnostics as derived above.\n- For each case, output an integer $1$ if all four inequalities are satisfied, and $0$ otherwise.\n\nFinal output format: Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets (e.g., \"[1,0,1,1]\") with no spaces.",
            "solution": "The problem is well-posed and scientifically grounded, providing a complete and consistent set of requirements for deriving, implementing, and applying a suite of MCMC convergence diagnostics. The task is to formalize these diagnostics from fundamental statistical definitions, apply them to simulated data under specified conditions, and determine whether the simulations meet a set of convergence criteria. The problem is valid.\n\nThe solution proceeds in two parts: first, the derivation of the four required diagnostic formulas, and second, their implementation in a program to analyze the specified test cases.\n\n### Derivation of Convergence Diagnostics\n\nLet there be $m$ chains, each of length $n$. The data for chain $j$ is the sequence $\\{x_{j,t}\\}_{t=1}^{n}$.\n\n#### 1. Split-Chain Variability Comparison ($\\hat{R}$)\nThis diagnostic, known as the potential scale reduction factor or Gelman-Rubin statistic, compares the variance within individual chains to an estimate of the total variance across all chains. A significant discrepancy indicates that the chains have not all converged to the same distribution. The derivation begins by splitting each of the $m$ chains of length $n$ into two halves, yielding $m_s=2m$ split chains, each of length $n_s = n/2$.\n\nLet $x_{j,t}$ now denote the $t$-th sample of the $j$-th *split* chain, where $j \\in \\{1, \\dots, m_s\\}$ and $t \\in \\{1, \\dots, n_s\\}$.\n\nThe sample mean of split chain $j$ is defined as:\n$$\n\\bar{x}_{.j} = \\frac{1}{n_s} \\sum_{t=1}^{n_s} x_{j,t}\n$$\n\nThe sample variance of split chain $j$ is:\n$$\ns_j^2 = \\frac{1}{n_s-1} \\sum_{t=1}^{n_s} (x_{j,t} - \\bar{x}_{.j})^2\n$$\n\nThe **within-sequence variance**, $W$, is the average of these individual sample variances. It estimates the mean of the variances within each sequence, corresponding to $E[\\text{Var}(\\psi|\\text{chain})]$ from the law of total variance.\n$$\nW = \\frac{1}{m_s} \\sum_{j=1}^{m_s} s_j^2\n$$\n\nThe **between-sequence variance**, $B$, is proportional to the sample variance of the split-chain means. It captures the variance of the means between sequences, related to $\\text{Var}[E(\\psi|\\text{chain})]$. Let $\\bar{x}_{..}$ be the grand mean of all samples across all split chains, $\\bar{x}_{..} = \\frac{1}{m_s} \\sum_{j=1}^{m_s} \\bar{x}_{.j}$.\n$$\nB = \\frac{n_s}{m_s-1} \\sum_{j=1}^{m_s} (\\bar{x}_{.j} - \\bar{x}_{..})^2\n$$\nThe factor $n_s$ scales the variance of the means to be comparable to the variance of the individual data points.\n\nAn overdispersed estimator for the marginal variance of the parameter, denoted $V^+$, is constructed as a weighted average of the within-sequence and between-sequence variances:\n$$\nV^+ = \\frac{n_s-1}{n_s} W + \\frac{1}{n_s} B\n$$\nThis formula combines the two sources of variation to provide a more robust estimate of the overall parameter variance than $W$ alone, especially when chains have not fully mixed.\n\nFinally, the **potential scale reduction factor**, $\\hat{R}$, is the ratio of the total variance estimate to the within-chain variance estimate:\n$$\n\\hat{R} = \\sqrt{\\frac{V^+}{W}} = \\sqrt{\\frac{n_s-1}{n_s} + \\frac{B}{n_s W}}\n$$\nIf all chains have converged to the same stationary distribution, $B$ will be of similar magnitude to $W$, and $\\hat{R}$ will be close to $1$. A value of $\\hat{R} > 1$ suggests that the chains have not yet fully explored the target distribution and that the variance could be reduced by running the chains longer.\n\n#### 2. Lag-1 Autocorrelation ($\\bar{\\rho}_1$)\nHigh autocorrelation within a chain indicates poor mixing, as adjacent samples are highly dependent and provide redundant information. We estimate the lag-$1$ autocorrelation for each original chain ($j \\in \\{1, \\dots, m\\}$ of length $n$).\n\nFor each chain $j$, the sample mean is $\\bar{x}_j = \\frac{1}{n} \\sum_{t=1}^{n} x_{j,t}$.\nThe sample autocovariance at lag $k$ is defined as:\n$$\n\\hat{\\gamma}_j(k) = \\frac{1}{n} \\sum_{t=1}^{n-k} (x_{j,t} - \\bar{x}_j)(x_{j,t+k} - \\bar{x}_j)\n$$\nThe sample variance is the autocovariance at lag $0$ (using a divisor of $n$ for consistency):\n$$\n\\hat{\\gamma}_j(0) = \\frac{1}{n} \\sum_{t=1}^{n} (x_{j,t} - \\bar{x}_j)^2\n$$\nThe sample autocorrelation at lag $k=1$ for chain $j$ is the ratio of the autocovariance at lag $1$ to the variance:\n$$\n\\hat{\\rho}_j(1) = \\frac{\\hat{\\gamma}_j(1)}{\\hat{\\gamma}_j(0)}\n$$\nThe final diagnostic is the average lag-$1$ autocorrelation across all $m$ chains:\n$$\n\\bar{\\rho}_1 = \\frac{1}{m} \\sum_{j=1}^{m} \\hat{\\rho}_j(1)\n$$\n\n#### 3. Standardized Drift Magnitude ($\\bar{D}_{\\text{std}}$)\nThis diagnostic detects non-stationarity by comparing the mean of the early part of a chain to the mean of the late part. A significant difference suggests the chain is still trending and has not reached its stationary distribution.\n\nFor each chain $j$ of length $n$, we define two windows of size $k = \\lfloor 0.4n \\rfloor$.\nThe mean of the first $k$ samples is:\n$$\n\\bar{x}_{j, \\text{early}} = \\frac{1}{k} \\sum_{t=1}^{k} x_{j,t}\n$$\nThe mean of the last $k$ samples is:\n$$\n\\bar{x}_{j, \\text{late}} = \\frac{1}{k} \\sum_{t=n-k+1}^{n} x_{j,t}\n$$\nThe drift for chain $j$ is the absolute difference between these means:\n$$\nD_j = |\\bar{x}_{j, \\text{early}} - \\bar{x}_{j, \\text{late}}|\n$$\nTo make this quantity comparable across different problems, it is standardized by the pooled standard deviation, $S$, computed over all $m \\times n$ samples. Let $\\bar{x}_{\\text{grand}}$ be the mean of all samples. The pooled sample variance is:\n$$\nS^2 = \\frac{1}{mn-1} \\sum_{j=1}^{m} \\sum_{t=1}^{n} (x_{j,t} - \\bar{x}_{\\text{grand}})^2\n$$\nThe pooled standard deviation is $S = \\sqrt{S^2}$. The standardized drift for chain $j$ is $D_{j, \\text{std}} = D_j / S$.\nThe final diagnostic is the average standardized drift over all chains:\n$$\n\\bar{D}_{\\text{std}} = \\frac{1}{m} \\sum_{j=1}^{m} D_{j, \\text{std}}\n$$\n\n#### 4. Effective Sample Size ($ESS_{total}$)\nThe effective sample size adjusts the nominal sample size ($m \\times n$) downward to account for autocorrelation. Under an AR($1$) approximation, the autocorrelation at lag $k$ is $\\rho(k) = (\\rho_1)^k$ for $k \\ge 0$, where $\\rho_1$ is the lag-$1$ autocorrelation.\n\nThe integrated autocorrelation time (IACT), denoted by $\\tau$, is given by the sum of all autocorrelations:\n$$\n\\tau = 1 + 2\\sum_{k=1}^{\\infty} \\rho(k)\n$$\nSubstituting the AR($1$) form for $\\rho(k)$ and using the formula for a geometric series for $|\\rho_1|  1$:\n$$\n\\sum_{k=1}^{\\infty} (\\rho_1)^k = \\frac{\\rho_1}{1-\\rho_1}\n$$\nThis gives the IACT as a simple function of $\\rho_1$:\n$$\n\\tau = 1 + 2 \\frac{\\rho_1}{1-\\rho_1} = \\frac{1-\\rho_1+2\\rho_1}{1-\\rho_1} = \\frac{1+\\rho_1}{1-\\rho_1}\n$$\nThe effective sample size for a single chain of length $n$ is $ESS_{\\text{single}} = n/\\tau$. For $m$ independent chains, the total effective sample size is the sum of the individual effective sample sizes. Using the average lag-$1$ autocorrelation $\\bar{\\rho}_1$ (from part 2) as our estimate for $\\rho_1$:\n$$\nESS_{total} = m \\times \\frac{n}{\\tau} = m \\times n \\times \\frac{1-\\bar{\\rho}_1}{1+\\bar{\\rho}_1}\n$$\n---\nThe provided implementation will now compute these four diagnostics for each test case and apply the specified thresholding rules to determine convergence.",
            "answer": "```python\nimport numpy as np\n\ndef simulate_ar1_traces(m, n, phi, mu_func, sigma_sq, seed):\n    \"\"\"\n    Simulates m traces from an AR(1) process.\n    x_jt = mu_jt + phi*(x_j,t-1 - mu_jt) + eps_jt\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    traces = np.zeros((m, n))\n    mu = np.zeros((m, n))\n    for j in range(m):\n        for t in range(n):\n            mu[j, t] = mu_func(j, t, n)\n\n    # Initial values x_j,1 drawn from stationary dist N(mu_j,1, sigma^2)\n    traces[:, 0] = rng.normal(loc=mu[:, 0], scale=np.sqrt(sigma_sq), size=m)\n\n    # Noise standard deviation\n    noise_std = np.sqrt(sigma_sq * (1 - phi**2))\n\n    # Generate traces for t > 1\n    for t in range(1, n):\n        epsilon = rng.normal(loc=0, scale=noise_std, size=m)\n        mu_t = mu[:, t]\n        x_prev = traces[:, t - 1]\n        traces[:, t] = mu_t + phi * (x_prev - mu_t) + epsilon\n    \n    return traces\n\ndef calculate_r_hat(traces):\n    \"\"\"Computes the potential scale reduction factor R-hat.\"\"\"\n    m, n = traces.shape\n    if n % 2 != 0:\n        raise ValueError(\"Number of samples n must be even for splitting.\")\n    \n    n_s = n // 2\n    m_s = 2 * m\n    \n    # Split chains into two halves\n    split_traces = traces.reshape(m_s, n_s)\n    \n    # Calculate within-chain means and variances\n    chain_means = np.mean(split_traces, axis=1)\n    chain_vars = np.var(split_traces, axis=1, ddof=1)\n    \n    # Within-sequence variance W\n    W = np.mean(chain_vars)\n    if W == 0: return 1.0 # Avoid division by zero if variance is nil\n\n    # Between-sequence variance B\n    grand_mean = np.mean(chain_means)\n    B = (n_s / (m_s - 1)) * np.sum((chain_means - grand_mean)**2)\n    \n    # Overdispersed variance estimator V+\n    V_plus = ((n_s - 1) / n_s) * W + (1 / n_s) * B\n    \n    r_hat = np.sqrt(V_plus / W)\n    return r_hat\n\ndef calculate_avg_autocorr(traces):\n    \"\"\"Calculates the average lag-1 autocorrelation across chains.\"\"\"\n    m, n = traces.shape\n    autocorrs = []\n    \n    for j in range(m):\n        chain = traces[j, :]\n        mean = np.mean(chain)\n        centered_chain = chain - mean\n        \n        # Autocovariance at lag 1, using n as divisor\n        gamma_1 = np.dot(centered_chain[:-1], centered_chain[1:]) / n\n        \n        # Variance (autocovariance at lag 0)\n        gamma_0 = np.dot(centered_chain, centered_chain) / n\n        \n        if gamma_0 == 0:\n            rho_1 = 0.0\n        else:\n            rho_1 = gamma_1 / gamma_0\n        autocorrs.append(rho_1)\n        \n    return np.mean(autocorrs)\n\ndef calculate_drift(traces):\n    \"\"\"Calculates the average standardized drift magnitude.\"\"\"\n    m, n = traces.shape\n    k = int(0.4 * n)\n    \n    early_means = np.mean(traces[:, :k], axis=1)\n    late_means = np.mean(traces[:, -k:], axis=1)\n    \n    drifts = np.abs(early_means - late_means)\n    \n    # Pooled standard deviation across all samples\n    pooled_std = np.std(traces, ddof=1)\n    \n    if pooled_std == 0:\n        return 0.0 # No drift if there is no variation\n        \n    std_drifts = drifts / pooled_std\n    \n    return np.mean(std_drifts)\n\ndef calculate_ess(traces, avg_rho1):\n    \"\"\"Calculates the total effective sample size.\"\"\"\n    m, n = traces.shape\n    \n    # Ensure rho1 is in a valid range to prevent division by zero or negative ESS\n    if avg_rho1 >= 1.0:\n        return 0.0\n    \n    iact = (1.0 + avg_rho1) / (1.0 - avg_rho1)\n    \n    ess = m * n / iact\n    return ess\n\ndef solve():\n    # Define thresholds\n    tau_R = 1.05\n    tau_rho = 0.3\n    tau_D = 0.15\n    E_min = 2000\n\n    # Common parameters\n    m = 4\n    n = 2000\n    sigma_sq = 1.0\n    seed = 12345\n\n    # Test cases setup\n    test_cases = [\n        # Case A: Well-mixed, stationary\n        {'phi': 0.2, 'mu_func': lambda j, t, n: 0.0},\n        # Case B: Sticky, highly autocorrelated\n        {'phi': 0.98, 'mu_func': lambda j, t, n: 0.0},\n        # Case C: Intra-chain mean shift\n        {'phi': 0.5, 'mu_func': lambda j, t, n: 1.0 if t > n / 2 else 0.0},\n        # Case D: Between-chain location discrepancy\n        {'phi': 0.5, 'mu_func': lambda j, t, n: 1.0 if j >= m / 2 else 0.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        # Simulate data for the current case\n        traces = simulate_ar1_traces(m, n, case['phi'], case['mu_func'], sigma_sq, seed)\n        \n        # Calculate diagnostics\n        r_hat = calculate_r_hat(traces)\n        avg_rho1 = calculate_avg_autocorr(traces)\n        drift = calculate_drift(traces)\n        ess = calculate_ess(traces, avg_rho1)\n        \n        # Check convergence criteria\n        converged = (\n            r_hat = tau_R and\n            avg_rho1 = tau_rho and\n            drift = tau_D and\n            ess >= E_min\n        )\n        \n        results.append(1 if converged else 0)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Diagnostics based on raw parameter values can be sensitive to the shape of the posterior distribution, particularly in cases involving skewness or heavy tails. This practice introduces a powerful technique for building robust diagnostics by operating on the ranks of the samples rather than their raw values . You will implement the rank-normalized potential scale reduction factor, a robust analogue of $\\hat{R}$, and investigate its direct connection to the visual interpretation of trace plot stability.",
            "id": "3289568",
            "problem": "You are given multiple independent Monte Carlo Markov Chain (MCMC) chains for a scalar parameter $ \\theta $ and tasked with assessing convergence using rank-normalized trace plots and a rank-based potential scale reduction factor. The fundamental base for this assessment must begin from the definition of exchangeable draws approximating a stationary target distribution, the notion of empirical ranks mapping to distribution-free quantiles, and the decomposition of variability into within-chain and between-chain components when comparing multiple chains.\n\nStarting from this base, implement the following algorithmic procedures in a complete, runnable program:\n\n1. Construct rank-normalized traces: Pool all draws of $ \\theta $ across chains, compute their empirical ranks, map each pooled rank to an approximate standard normal score by applying the inverse cumulative distribution function of the standard normal distribution to the rescaled ranks, and then reshape back to the original chain-by-iteration layout.\n\n2. Compute a rank-normalized split potential scale reduction factor (often referred to as a rank-$ \\hat R $) by splitting each chain into two halves, computing the within-chain variance and the between-chain variance across the split chains, producing a variance estimate that blends these components, and finally forming the ratio that reflects potential scale reduction.\n\n3. Define a quantitative proxy for visual rank stability in the trace plots: Using the last half of iterations in the rank-normalized traces, compute the standard deviation of the per-chain means and normalize this by the pooled standard deviation across all chains and iterations in that last half. This ratio quantifies overlap of chain means relative to the typical spread in the traces.\n\n4. Declare that “small rank-$ \\hat R $” holds if the rank-normalized split potential scale reduction factor is strictly less than $ 1.05 $, and declare that “visual rank stability” holds if the last-half mean-overlap ratio is strictly less than $ 0.1 $. For each test case below, report whether these two declarations agree, i.e., whether both are simultaneously true or both are simultaneously false.\n\nThe program must generate synthetic MCMC outputs for $ \\theta $ according to the test suite below. Random number generation must be made reproducible using the specified seeds. All angles (if any arise) must be treated in radians. No physical units are involved. The final program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n\nTest suite:\n\n- Case A (well-mixed): $ 4 $ chains, $ 2000 $ iterations per chain, seed $ 123 $. Each chain’s draws are independent and identically distributed from a normal distribution with mean $ 0 $ and standard deviation $ 1 $.\n\n- Case B (location-shift in one chain): $ 4 $ chains, $ 2000 $ iterations per chain, seed $ 456 $. Chains $ 1 $, $ 2 $, and $ 3 $ are drawn from a normal distribution with mean $ 0 $ and standard deviation $ 1 $. Chain $ 4 $ is drawn from a normal distribution with mean $ 1.5 $ and standard deviation $ 1 $.\n\n- Case C (multi-modality across chains): $ 4 $ chains, $ 2000 $ iterations per chain, seed $ 789 $. Chain $ 1 $ is drawn from a normal distribution with mean $ -2 $ and standard deviation $ 1 $. Chain $ 2 $ is drawn from a normal distribution with mean $ 2 $ and standard deviation $ 1 $. Chain $ 3 $ takes its first $ 1000 $ iterations from a normal distribution with mean $ -2 $ and standard deviation $ 1 $ and its last $ 1000 $ iterations from a normal distribution with mean $ 2 $ and standard deviation $ 1 $. Chain $ 4 $ alternates between means $ -2 $ and $ 2 $ every $ 25 $ iterations with standard deviation $ 1 $.\n\n- Case D (heavy-tailed target with short chains): $ 4 $ chains, $ 50 $ iterations per chain, seed $ 321 $. Each chain’s draws are independently from a standard Cauchy distribution with location $ 0 $ and scale $ 1 $.\n\nOutput specification:\n\n- For each case A–D, compute the rank-normalized split potential scale reduction factor and the last-half mean-overlap ratio as described. Let $ r_{\\text{thresh}} = 1.05 $ and $ v_{\\text{thresh}} = 0.1 $. The result for a case is the boolean value of the statement $ \\big( \\text{rank-}\\hat R  r_{\\text{thresh}} \\big) $ equals $ \\big( \\text{ratio}  v_{\\text{thresh}} \\big) $.\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $ [\\text{Case A result},\\text{Case B result},\\text{Case C result},\\text{Case D result}] $.",
            "solution": "The problem requires the implementation and comparison of two MCMC convergence diagnostics for a scalar parameter $ \\theta $: the rank-normalized split potential scale reduction factor ($ \\hat{R} $) and a quantitative proxy for visual rank stability. The assessment is to be performed on four synthetic test cases representing different convergence scenarios.\n\nThe fundamental principle behind these diagnostics is to assess whether multiple MCMC chains, initiated from diverse starting points, have all converged to sampling from the same stationary target distribution. If they have, the variation between chains should be statistically indistinguishable from the variation within each chain. Using ranks rather than raw parameter values makes these diagnostics robust to the specific shape of the target distribution, particularly those with heavy tails or other challenging geometries.\n\nLet the MCMC output consist of $ M $ chains, each with $ N $ iterations. The draws are denoted by $ \\theta_{i,j} $ for chain $ i \\in \\{1, \\dots, M\\} $ and iteration $ j \\in \\{1, \\dots, N\\} $. The total number of draws is $ S = MN $.\n\n### 1. Constructing Rank-Normalized Traces\n\nThe first step is to transform the raw draws $ \\theta_{i,j} $ into a space where their distributional properties are standardized. This is achieved through rank normalization.\n\n1.  **Pooling and Ranking**: All $ S = MN $ draws are pooled into a single collection $\\{ \\theta_k \\}_{k=1}^S$. The empirical rank $ r_k $ is computed for each draw $ \\theta_k $, representing its position in the sorted collection (from $ 1 $ to $ S $).\n2.  **Rescaling**: Each rank $ r_k $ is rescaled to the interval $ (0, 1) $ to approximate its cumulative probability. A standard approach is to use a continuity correction, yielding $ p_k = \\frac{r_k - 0.5}{S} $.\n3.  **Normalization**: The rescaled ranks are transformed into standard normal scores by applying the inverse cumulative distribution function (CDF) of the standard normal distribution, $ \\Phi^{-1} $. The resulting rank-normalized draw is $ z_k = \\Phi^{-1}(p_k) $.\n4.  **Reshaping**: The flat vector of $ S $ normalized scores $\\{ z_k \\}_{k=1}^S$ is reshaped back into an $ M \\times N $ matrix, let's call it $ z_{i,j} $, preserving the original chain and iteration structure. All subsequent calculations are performed on these $ z_{i,j} $ values.\n\n### 2. Computing the Rank-Normalized Split Potential Scale Reduction Factor ($ \\hat{R} $)\n\nThe $ \\hat{R} $ statistic compares the variance between chains to the variance within chains. The \"split\" version enhances its ability to detect non-stationarity by treating the first and second halves of each chain as separate chains.\n\n1.  **Splitting Chains**: The $ M $ rank-normalized chains of length $ N $ are split into $ m = 2M $ chains of length $ n = N/2 $. Let these split chains be denoted by $ \\phi_{k,l} $ for $ k \\in \\{1, \\dots, m\\} $ and $ l \\in \\{1, \\dots, n\\} $.\n2.  **Within-Chain Variance ($ W $)**: For each split chain $ k $, we compute its sample variance:\n    $$ s_k^2 = \\frac{1}{n-1} \\sum_{l=1}^{n} (\\phi_{k,l} - \\bar{\\phi}_{k\\cdot})^2 $$\n    where $ \\bar{\\phi}_{k\\cdot} $ is the mean of chain $ k $. The average of these variances gives the within-chain variance:\n    $$ W = \\frac{1}{m} \\sum_{k=1}^{m} s_k^2 $$\n3.  **Between-Chain Variance ($ B $)**: We compute the variance of the means of the split chains:\n    $$ B = \\frac{n}{m-1} \\sum_{k=1}^{m} (\\bar{\\phi}_{k\\cdot} - \\bar{\\phi}_{\\cdot\\cdot})^2 $$\n    where $ \\bar{\\phi}_{\\cdot\\cdot} $ is the mean of all draws across all split chains. The factor $ n $ scales this variance to be comparable to $ W $.\n4.  **Pooled Variance ($ \\hat{V} $)**: An estimate of the marginal variance of the target distribution is formed by a weighted average of $ W $ and $ B $:\n    $$ \\hat{V} = \\frac{n-1}{n} W + \\frac{1}{n} B $$\n5.  **Scale Reduction Factor ($ \\hat{R} $)**: The final statistic is the ratio of the pooled variance estimate to the within-chain variance, which represents the potential reduction in scale if sampling were to continue:\n    $$ \\hat{R} = \\sqrt{\\frac{\\hat{V}}{W}} $$\n    If the chains have converged, $ W $ and $ \\hat{V} $ will be nearly equal, and $ \\hat{R} $ will be close to $ 1.0 $. The problem specifies a threshold $ r_{\\text{thresh}} = 1.05 $.\n\n### 3. Quantitative Proxy for Visual Rank Stability\n\nThis metric quantifies the visual impression one gets from inspecting trace plots. For well-mixed, converged chains, the traces should overlap, and their individual means should be close to each other relative to the overall spread.\n\n1.  **Isolate Last Half**: We consider only the second half of the rank-normalized traces, i.e., $ z_{i,j} $ for $ i \\in \\{1, \\dots, M\\} $ and $ j \\in \\{N/2+1, \\dots, N\\} $.\n2.  **Per-Chain Mean**: For each chain $ i $, compute its mean over this last half, $ \\bar{z}'_{i\\cdot} $.\n3.  **Standard Deviation of Means**: Compute the sample standard deviation of these $ M $ chain means, let's call this $ \\sigma_{\\text{means}} $. This measures the spread between the central tendencies of the chains.\n4.  **Pooled Standard Deviation**: Compute the sample standard deviation of all draws across all chains in the last half, let's call this $ \\sigma_{\\text{pooled}} $. This measures the typical spread of the draws within the traces.\n5.  **Ratio**: The final overlap ratio is:\n    $$ \\rho = \\frac{\\sigma_{\\text{means}}}{\\sigma_{\\text{pooled}}} $$\n    A small ratio indicates that the differences between chain means are small compared to the typical fluctuations within the chains, suggesting good mixing and overlap. The problem specifies a threshold $ v_{\\text{thresh}} = 0.1 $.\n\n### 4. Decision Logic\n\nFor each test case, we compute the rank-normalized split $ \\hat{R} $ and the last-half mean-overlap ratio $ \\rho $. We then evaluate two boolean conditions:\n- \"small rank-$ \\hat{R} $\" is true if $ \\hat{R}  1.05 $.\n- \"visual rank stability\" is true if $ \\rho  0.1 $.\n\nThe final result for the case is the boolean value of the logical test that these two conditions agree, i.e., whether they are both true or both false:\n$$ \\text{Result} = (\\hat{R}  1.05) == (\\rho  0.1) $$\nThis tests the consistency of the two diagnostic measures across different scenarios.\n- **Case A** represents an ideal, well-mixed scenario where both diagnostics should indicate convergence.\n- **Case B** introduces a location shift in one chain, a clear failure to converge that both diagnostics should detect.\n- **Case C** simulates complex non-stationarity and multimodality, another failure mode that diagnostics should identify.\n- **Case D** uses short chains from a heavy-tailed Cauchy distribution, testing the robustness of rank-based methods in a challenging, high-variance, low-information setting. In such a difficult case, both diagnostics are expected to flag non-convergence.\n\nThe goal is to verify if these two distinct but related measures provide a consistent signal about convergence status.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef generate_case_data(case_params):\n    \"\"\"Generates synthetic MCMC data based on case parameters.\"\"\"\n    M = case_params['M']\n    N = case_params['N']\n    seed = case_params['seed']\n    name = case_params['name']\n    \n    np.random.seed(seed)\n    \n    draws = np.zeros((M, N))\n    \n    if name == 'A':\n        # All chains are i.i.d. from a standard normal distribution.\n        draws = np.random.normal(loc=0.0, scale=1.0, size=(M, N))\n    elif name == 'B':\n        # 3 chains from N(0,1), 1 chain from N(1.5, 1).\n        draws[:3, :] = np.random.normal(loc=0.0, scale=1.0, size=(M - 1, N))\n        draws[3, :] = np.random.normal(loc=1.5, scale=1.0, size=N)\n    elif name == 'C':\n        # Complex multi-modality and non-stationarity.\n        N_half = N // 2\n        # Chain 1: N(-2, 1)\n        draws[0, :] = np.random.normal(loc=-2.0, scale=1.0, size=N)\n        # Chain 2: N(2, 1)\n        draws[1, :] = np.random.normal(loc=2.0, scale=1.0, size=N)\n        # Chain 3: Switches from N(-2, 1) to N(2, 1) halfway.\n        draws[2, :N_half] = np.random.normal(loc=-2.0, scale=1.0, size=N_half)\n        draws[2, N_half:] = np.random.normal(loc=2.0, scale=1.0, size=N_half)\n        # Chain 4: Alternates between modes every 25 iterations.\n        alt_size = 25\n        num_blocks = N // alt_size\n        for i in range(num_blocks):\n            start_idx = i * alt_size\n            end_idx = start_idx + alt_size\n            mean = -2.0 if i % 2 == 0 else 2.0\n            draws[3, start_idx:end_idx] = np.random.normal(loc=mean, scale=1.0, size=alt_size)\n    elif name == 'D':\n        # Heavy-tailed Cauchy distribution.\n        draws = np.random.standard_cauchy(size=(M, N))\n        \n    return draws\n\ndef compute_diagnostics(draws):\n    \"\"\"\n    Computes rank-normalized split R-hat and the last-half mean-overlap ratio.\n    \"\"\"\n    M, N = draws.shape\n    if N  2:\n        # Not enough data to split chains or calculate variance\n        return np.inf, np.inf\n\n    # 1. Rank-normalize the traces\n    S = M * N\n    flat_draws = draws.flatten()\n    # Compute ranks from 1 to S\n    ranks = flat_draws.argsort().argsort() + 1\n    # Rescale ranks to (0, 1)\n    rescaled_ranks = (ranks - 0.5) / S\n    # Apply inverse normal CDF (probit function)\n    z_scores_flat = norm.ppf(rescaled_ranks)\n    z_traces = z_scores_flat.reshape(M, N)\n\n    # 2. Compute rank-normalized split R-hat\n    m = 2 * M\n    n = N // 2\n    \n    # Reshape into split chains\n    split_chains = z_traces[:, :2*n].reshape(m, n)\n\n    # Calculate within-chain and between-chain variance\n    chain_means = np.mean(split_chains, axis=1)\n    # ddof=1 for sample variance\n    W = np.mean(np.var(split_chains, axis=1, ddof=1)) \n    \n    if W == 0:\n        # If all chains are constant and identical, W can be 0.\n        # This implies perfect convergence in a trivial case.\n        R_hat = 1.0\n    else:\n        # ddof=1 for sample variance of the means\n        B = n * np.var(chain_means, ddof=1)\n        V_hat = ((n - 1) / n) * W + (1 / n) * B\n        R_hat = np.sqrt(V_hat / W)\n\n    # 3. Compute last-half mean-overlap ratio\n    last_half_traces = z_traces[:, N//2:]\n    \n    # Per-chain means for the last half\n    last_half_means = np.mean(last_half_traces, axis=1)\n    \n    sigma_means = np.std(last_half_means, ddof=1) if M > 1 else 0.0\n    \n    # Pooled standard deviation for the last half\n    # ddof=1 for sample standard deviation\n    sigma_pooled = np.std(last_half_traces, ddof=1)\n    \n    if sigma_pooled == 0:\n        # Chains are constant in the last half.\n        # If means are also identical, ratio is 0. If not, it's undefined.\n        # We can treat this as 0 if sigma_means is also 0, else a large number.\n        ratio = 0.0 if sigma_means == 0.0 else np.inf\n    else:\n        ratio = sigma_means / sigma_pooled\n\n    return R_hat, ratio\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'name': 'A', 'M': 4, 'N': 2000, 'seed': 123},\n        {'name': 'B', 'M': 4, 'N': 2000, 'seed': 456},\n        {'name': 'C', 'M': 4, 'N': 2000, 'seed': 789},\n        {'name': 'D', 'M': 4, 'N': 50, 'seed': 321},\n    ]\n\n    r_thresh = 1.05\n    v_thresh = 0.1\n\n    results = []\n    for case in test_cases:\n        draws = generate_case_data(case)\n        rank_R_hat, mean_overlap_ratio = compute_diagnostics(draws)\n        \n        # Check if the declarations agree\n        r_hat_ok = rank_R_hat  r_thresh\n        ratio_ok = mean_overlap_ratio  v_thresh\n        \n        agreement = (r_hat_ok == ratio_ok)\n        results.append(agreement)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A particularly deceptive MCMC failure mode occurs when a chain appears stable but is actually trapped in a near-deterministic, periodic cycle. Standard diagnostics that assess variance and stationarity can easily miss this pathological behavior. This practice equips you with a specialized diagnostic based on spectral analysis, enabling you to distinguish genuine stochastic stationarity from hidden cycling and sharpening your ability to detect subtle yet critical convergence issues .",
            "id": "3289543",
            "problem": "You are given the task of designing and implementing a quantitative diagnostic to distinguish between deterministic cycling and genuine stationarity in Markov Chain Monte Carlo (MCMC) trace plots that exhibit minimal variability. Work in the setting of a scalar stochastic process $\\{X_t\\}_{t=1}^N$ produced by a Gibbs sampler. Start from the following foundational elements.\n\n- A Markov chain is a stochastic process $\\{X_t\\}$ with the property that $\\mathbb{P}(X_{t+1} \\in A \\mid X_1,\\dots,X_t) = \\mathbb{P}(X_{t+1} \\in A \\mid X_t)$ for any measurable set $A$. A stationary distribution $\\pi$ satisfies $\\pi T = \\pi$, where $T$ is the transition kernel of the chain.\n- The autocovariance function of a (weakly) stationary process is $\\gamma(k) = \\mathrm{Cov}(X_t, X_{t+k})$ for integer lag $k$, and the spectral density is the discrete-time Fourier transform of $\\gamma(k)$. The periodogram at frequency $\\omega_k = 2\\pi k/N$ is defined by $I(\\omega_k) = \\frac{1}{N}\\left|\\sum_{t=1}^N X_t e^{-i \\omega_k t}\\right|^2$.\n- In a nearly deterministic cycle, successive states almost deterministically map to the next with vanishing innovation variance, while a genuinely stationary chain has nonzero innovation variance that sustains variability over time, even if the trace appears visually flat.\n\nYour program must implement a diagnostic test grounded in these principles:\n\n1. Compute a spectral peak ratio to quantify sharp periodicity. Let $I(\\omega_k)$ be the periodogram computed via the discrete Fourier transform at the nonnegative Fourier frequencies $\\omega_k = 2\\pi k/N$ for $k \\in \\{0,1,\\dots,\\lfloor N/2 \\rfloor\\}$. Define the low-frequency baseline $S_{\\text{low}}$ as the arithmetic mean of the first $M$ positive frequency ordinates, i.e., $S_{\\text{low}} = \\frac{1}{M}\\sum_{k=1}^M I(\\omega_k)$, and define the nonzero-frequency peak $S_{\\text{peak}} = \\max_{k \\ge 1} I(\\omega_k)$. The spectral peak ratio is $R_{\\text{spec}} = \\frac{S_{\\text{peak}}}{S_{\\text{low}}}$.\n2. Compute an innovation ratio to quantify near-determinism over one-step transitions. Let $a^\\star$ be the least-squares coefficient minimizing $\\sum_{t=1}^{N-1} (X_{t+1} - a X_t)^2$, which equals $a^\\star = \\frac{\\mathrm{Cov}(X_t, X_{t+1})}{\\mathrm{Var}(X_t)}$ when $\\mathrm{Var}(X_t) \\neq 0$. Define residuals $E_t = X_{t+1} - a^\\star X_t$ and the innovation ratio $R_{\\text{var}} = \\frac{\\mathrm{Var}(E_t)}{\\mathrm{Var}(X_t)}$.\n3. Classify a chain as deterministically cycling if and only if both $R_{\\text{spec}}  T_{\\text{spec}}$ and $R_{\\text{var}}  T_{\\text{var}}$, where $T_{\\text{spec}}$ and $T_{\\text{var}}$ are fixed thresholds.\n\nImplement the above diagnostic with the following fixed constants suitable for advanced-graduate-level analysis: $N = 4096$, $M = 5$, $T_{\\text{spec}} = 20$, and $T_{\\text{var}} = 10^{-3}$. Use the discrete Fourier transform via the real-valued fast Fourier transform. Handle degenerate variances robustly by adding a small numerical stabilization constant where appropriate.\n\nTest suite. Apply your diagnostic to the following four synthetic scenarios, each producing a sequence of length $N$:\n\n- Case $1$ (near-deterministic period-$2$ cycle with tiny noise): $X_{t+1} = -X_t + \\varepsilon Z_t$ with $X_1 = 1$, $\\varepsilon = 10^{-4}$, and $\\{Z_t\\}$ independent and identically distributed standard normal random variables. This simulates a Gibbs sampler with nearly deterministic full conditionals that produce an odd-even alternation.\n- Case $2$ (genuinely stationary, highly persistent but not periodic): an autoregressive process of order one defined by $X_{t+1} = \\phi X_t + \\sigma Z_t$ with $\\phi = 0.95$, $\\sigma = 0.05$, and $\\{Z_t\\}$ independent and identically distributed standard normal, after a burn-in of $B = 512$ steps to approximate stationarity. Return the last $N$ values.\n- Case $3$ (genuinely stationary with negative autocorrelation but not deterministic): an autoregressive process of order one defined by $X_{t+1} = \\phi X_t + \\sigma Z_t$ with $\\phi = -0.8$, $\\sigma = 0.2$, and $\\{Z_t\\}$ independent and identically distributed standard normal, after a burn-in of $B = 512$ steps to approximate stationarity. Return the last $N$ values.\n- Case $4$ (near-deterministic period-$3$ cycle with tiny noise): the chain cycles through the deterministic states $\\{1, -\\tfrac{1}{2}, 0.2\\}$ in order, with $X_{t+1} = s_{(t \\bmod 3) + 1} + \\varepsilon Z_t$, where the state tuple is $(s_1, s_2, s_3) = (1, -\\tfrac{1}{2}, 0.2)$, $\\varepsilon = 10^{-4}$, and $\\{Z_t\\}$ independent and identically distributed standard normal, starting from $X_1 = s_1$.\n\nYour program must:\n\n- Use a fixed random seed so that outputs are exactly reproducible.\n- For each case, generate the sequence and apply the diagnostic to return a boolean indicating whether deterministic cycling is detected.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $4$, for example, $[b_1,b_2,b_3,b_4]$ where each $b_j$ is either True or False.\n\nNo physical units or angle units are involved in this problem. All random variables are real-valued. All numerical constants in this problem are exact and must be used as stated. The random seed choice is arbitrary but must be fixed consistently across all cases within a single program run. The final outputs must be booleans.",
            "solution": "The problem of distinguishing between a genuine stationary Markov Chain Monte Carlo (MCMC) process and one exhibiting pathological deterministic cycling is a critical aspect of convergence diagnostics. A trace plot that appears flat may misleadingly suggest convergence, while the chain could be trapped in a near-deterministic, periodic cycle. The provided problem statement outlines a valid, well-posed, and scientifically grounded task to implement a specific two-part diagnostic for this purpose. The problem is formally specified with all necessary constants and definitions, allowing for a unique and reproducible solution. We will proceed with a step-by-step implementation and analysis.\n\nThe diagnostic is composed of two metrics: a spectral peak ratio, $R_{\\text{spec}}$, to detect sharp periodicity, and an innovation ratio, $R_{\\text{var}}$, to quantify one-step determinism. A chain is classified as deterministically cycling only if it exhibits both strong periodicity and strong determinism.\n\nFirst, we analyze the spectral properties of the time series $\\{X_t\\}_{t=1}^N$. A periodic or cyclic process concentrates its variance at specific frequencies corresponding to the period of the cycle. This manifests as sharp peaks in its spectral density. The periodogram, $I(\\omega_k)$, serves as an estimator of the spectral density at a set of discrete Fourier frequencies $\\omega_k = 2\\pi k/N$. It is defined as\n$$\nI(\\omega_k) = \\frac{1}{N}\\left|\\sum_{t=1}^N X_t e^{-i \\omega_k t}\\right|^2\n$$\nwhere $N$ is the length of the series. To quantify the presence of a dominant periodic component, we define the spectral peak ratio, $R_{\\text{spec}}$. This metric compares the maximal power at any non-zero frequency, $S_{\\text{peak}}$, to a baseline power level, $S_{\\text{low}}$. The peak power is $S_{\\text{peak}} = \\max_{k \\ge 1} I(\\omega_k)$. The baseline is computed as the average power over the first $M$ positive frequencies: $S_{\\text{low}} = \\frac{1}{M}\\sum_{k=1}^M I(\\omega_k)$. The ratio is then $R_{\\text{spec}} = S_{\\text{peak}} / S_{\\text{low}}$. A large value of $R_{\\text{spec}}$ indicates that one frequency component is significantly more powerful than the general low-frequency background, a hallmark of periodicity. We use the specified threshold $T_{\\text{spec}} = 20$.\n\nSecond, we assess the determinism of the chain's transitions. A nearly deterministic process is one where the next state, $X_{t+1}$, is almost completely determined by the current state, $X_t$, with very little random innovation. We probe this relationship using a simple first-order autoregressive model, $X_{t+1} \\approx a X_t$. The problem specifies finding the coefficient $a^\\star$ that minimizes the sum of squared errors, $\\sum_{t=1}^{N-1} (X_{t+1} - a X_t)^2$. The solution to this standard least-squares problem (for a regression through the origin) is\n$$\na^\\star = \\frac{\\sum_{t=1}^{N-1} X_t X_{t+1}}{\\sum_{t=1}^{N-1} X_t^2}\n$$\nThe residuals of this fit, $E_t = X_{t+1} - a^\\star X_t$, represent the \"innovation\" or the part of $X_{t+1}$ not explained by a linear function of $X_t$. The innovation ratio, $R_{\\text{var}}$, is defined as the ratio of the variance of these residuals to the variance of the original series:\n$$\nR_{\\text{var}} = \\frac{\\mathrm{Var}(E_t)}{\\mathrm{Var}(X_t)}\n$$\nIf $R_{\\text{var}}$ is very small, it implies that the simple linear model accounts for almost all the variability in the process, suggesting a high degree of one-step determinism. The specified threshold is $T_{\\text{var}} = 10^{-3}$. It is crucial to note that this diagnostic specifically tests for *linear* one-step predictability.\n\nThe final classification rule is that a chain is flagged as deterministically cycling if and only if both conditions are met: $R_{\\text{spec}} > T_{\\text{spec}}$ and $R_{\\text{var}}  T_{\\text{var}}$. This ensures that we identify processes that are both periodic and highly predictable.\n\nWe apply this diagnostic to four test cases using the constants $N=4096$ and $M=5$.\nCase 1: $X_{t+1} = -X_t + \\varepsilon Z_t, \\varepsilon=10^{-4}$. This is a period-2 cycle. The model $X_{t+1} \\approx a X_t$ is an excellent fit with $a^\\star \\approx -1$. The residuals will be approximately $\\varepsilon Z_t$, leading to a very small $\\mathrm{Var}(E_t)$ and thus a very small $R_{\\text{var}}$. The periodogram will have a massive peak at the Nyquist frequency ($\\pi$), causing a very large $R_{\\text{spec}}$. Both conditions are expected to be met, resulting in a `True` classification.\nCase 2: A stationary AR(1) process with $\\phi=0.95$. This process is highly persistent but not periodic. Its spectral density is smooth and peaked at frequency zero. Thus, $R_{\\text{spec}}$ is expected to be small. The innovation ratio $R_{\\text{var}}$ will be approximately $1-\\phi^2 = 1 - 0.95^2 \\approx 0.0975$, which is well above $T_{\\text{var}}$. The classification should be `False`.\nCase 3: A stationary AR(1) process with $\\phi=-0.8$. This process alternates but is not a deterministic cycle. Its spectral density is smooth and peaked at the Nyquist frequency. However, the peak is broad, not sharp, so $R_{\\text{spec}}$ is unlikely to exceed the threshold. The innovation ratio $R_{\\text{var}}$ will be approximately $1-\\phi^2 = 1 - (-0.8)^2 = 0.36$, far above $T_{\\text{var}}$. The classification should be `False`.\nCase 4: A period-3 cycle, $X_{t+1} = s_{(t \\bmod 3)+1} + \\varepsilon Z_t$. This process is strongly periodic, so $R_{\\text{spec}}$ will be very large due to a sharp peak at a frequency corresponding to period 3. However, the sequence of pairs $(X_t, X_{t+1})$ follows a three-point pattern that is not well-approximated by a single line through the origin. Therefore, the AR(1) model is a poor fit, and the residual variance $\\mathrm{Var}(E_t)$ will be large. Consequently, $R_{\\text{var}}$ will not be smaller than $T_{\\text{var}}$. The diagnostic, as specified, is sensitive only to linear (or period-2) determinism and will fail to detect the deterministic nature of this higher-order cycle. The classification is expected to be `False`.\n\nThe implementation will generate these four time series, apply the specified diagnostic calculations, and return a boolean classification for each. Numerical stability is ensured by adding a small constant, $\\epsilon = 10^{-15}$, to denominators where division by zero could occur.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the MCMC diagnostic on four test cases.\n    \"\"\"\n    # Fixed parameters from the problem statement\n    N = 4096\n    M = 5\n    T_spec = 20.0\n    T_var = 1e-3\n    B = 512\n    RANDOM_SEED = 123\n    \n    # Numerical stabilization constant\n    STABILIZATION_EPS = 1e-15\n\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    def run_diagnostic(X: np.ndarray) -> bool:\n        \"\"\"\n        Applies the diagnostic test to a time series X.\n\n        Args:\n            X: A 1D numpy array representing the time series.\n\n        Returns:\n            A boolean indicating if deterministic cycling is detected.\n        \"\"\"\n        # Ensure N is the length of X for calculation\n        N_local = len(X)\n        if N_local  M + 2: # Need enough points for M and for AR(1)\n            return False\n\n        # 1. Compute the spectral peak ratio R_spec\n        # Compute real FFT and then the periodogram\n        # The problem defines I(w_k) = (1/N) |FFT(X)_k|^2\n        fft_coeffs = np.fft.rfft(X)\n        periodogram = (1 / N_local) * np.abs(fft_coeffs)**2\n\n        # Positive-frequency ordinates start from index 1 (k=0 is DC component)\n        pos_freq_periodogram = periodogram[1:]\n        \n        # Handle cases with no positive frequency components / flat spectrum\n        if len(pos_freq_periodogram) == 0:\n            return False\n\n        S_peak = np.max(pos_freq_periodogram)\n        \n        # S_low is the mean of the first M positive frequency ordinates\n        S_low = np.mean(pos_freq_periodogram[:M])\n        \n        R_spec = S_peak / (S_low + STABILIZATION_EPS)\n\n        # 2. Compute the innovation ratio R_var\n        var_X = np.var(X)\n        if var_X  STABILIZATION_EPS:\n            # Constant series is stationary, not cycling. R_var is effectively 0.\n            # But it's not 'cycling', so the periodicity test would fail anyway.\n            # A constant series has S_peak=0 so R_spec=0. Diagnostic fails.\n            return False\n\n        X_prefix = X[:-1]\n        X_suffix = X[1:]\n\n        # Calculate a_star from the least-squares minimization of sum((X_{t+1} - a*X_t)^2)\n        # a_star = sum(X_t * X_{t+1}) / sum(X_t^2)\n        sum_xt_xt1 = np.dot(X_prefix, X_suffix)\n        sum_xt_sq = np.dot(X_prefix, X_prefix)\n        a_star = sum_xt_xt1 / (sum_xt_sq + STABILIZATION_EPS)\n\n        # Calculate residuals and their variance\n        E = X_suffix - a_star * X_prefix\n        var_E = np.var(E)\n\n        R_var = var_E / (var_X + STABILIZATION_EPS)\n\n        # 3. Classify the chain\n        is_cycling = (R_spec > T_spec) and (R_var  T_var)\n        return is_cycling\n\n    def generate_case1() -> np.ndarray:\n        epsilon = 1e-4\n        X = np.zeros(N)\n        X[0] = 1.0\n        Z = rng.standard_normal(N - 1)\n        for t in range(N - 1):\n            X[t+1] = -X[t] + epsilon * Z[t]\n        return X\n\n    def generate_case2() -> np.ndarray:\n        phi, sigma = 0.95, 0.05\n        X = np.zeros(N + B)\n        X[0] = 0.0\n        Z = rng.standard_normal(N + B - 1)\n        for t in range(N + B - 1):\n            X[t+1] = phi * X[t] + sigma * Z[t]\n        return X[B:]\n\n    def generate_case3() -> np.ndarray:\n        phi, sigma = -0.8, 0.2\n        X = np.zeros(N + B)\n        X[0] = 0.0\n        Z = rng.standard_normal(N + B - 1)\n        for t in range(N + B - 1):\n            X[t+1] = phi * X[t] + sigma * Z[t]\n        return X[B:]\n\n    def generate_case4() -> np.ndarray:\n        s_tuple = (1.0, -0.5, 0.2)\n        epsilon = 1e-4\n        X = np.zeros(N)\n        X[0] = s_tuple[0]\n        Z = rng.standard_normal(N - 1)\n        for t in range(N-1):\n            s_index = (t+1) % 3\n            X[t+1] = s_tuple[s_index] + epsilon*Z[t]\n        return X\n\n    test_cases = [\n        generate_case1,\n        generate_case2,\n        generate_case3,\n        generate_case4\n    ]\n\n    results = []\n    for gen_func in test_cases:\n        X_series = gen_func()\n        result = run_diagnostic(X_series)\n        results.append(result)\n\n    # Format the final output exactly as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}