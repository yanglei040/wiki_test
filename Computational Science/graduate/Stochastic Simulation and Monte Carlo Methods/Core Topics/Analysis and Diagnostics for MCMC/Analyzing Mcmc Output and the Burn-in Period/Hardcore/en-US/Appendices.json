{
    "hands_on_practices": [
        {
            "introduction": "The primary goal of many MCMC simulations is to estimate an expectation, like the posterior mean of a parameter. Since MCMC generates autocorrelated samples, the standard error formulas for independent data are not applicable. This exercise demonstrates how to compute the Monte Carlo Standard Error (MCSE) and a confidence interval that properly accounts for this serial dependence, allowing you to quantify the precision of your estimate .",
            "id": "3287635",
            "problem": "A practitioner runs a Markov Chain Monte Carlo (MCMC) algorithm to estimate the expectation $\\mathbb{E}_{\\pi} f$ of a measurable function $f$ under a stationary target distribution $\\pi$ on $\\mathbb{R}$. The chain is constructed by a random-walk Metropolis transition and is known to be geometrically ergodic. The practitioner discards a burn-in period of $B$ iterations to mitigate transient bias, and retains $n$ post-burn-in samples $\\{X_{B+1},\\dots,X_{B+n}\\}$ assumed to be approximately stationary. Let $\\bar{f}_{n} = n^{-1}\\sum_{t=1}^{n} f(X_{B+t})$ denote the post-burn-in average.\n\nStarting from the Markov chain central limit theorem and the definition of the Monte Carlo standard error (MCSE) as the standard deviation of the sampling distribution of $\\bar{f}_{n}$ under $\\pi$ when the samples are generated by an ergodic Markov chain, derive how to construct a large-sample confidence interval for $\\mathbb{E}_{\\pi} f$ that accounts for autocorrelation. Explain why discarding burn-in does not alter the asymptotic distribution of $\\bar{f}_{n}$ but is used to reduce bias in finite samples.\n\nAssume that a consistent spectral (or batch-means) estimator of the asymptotic variance of $\\sqrt{n}(\\bar{f}_{n}-\\mathbb{E}_{\\pi} f)$ computed from the $n$ post-burn-in samples yields $\\hat{\\sigma}^{2}_{f} = 2.5$. Suppose further that $n = 10^{5}$ and the observed post-burn-in average is $\\bar{f}_{n} = 0.7321$.\n\nCompute:\n- The Monte Carlo standard error (MCSE).\n- A $95\\%$ confidence interval for $\\mathbb{E}_{\\pi} f$ based on the normal approximation.\n\nRound all numerical answers to four significant figures. Do not use a percentage sign; interpret $95\\%$ via the standard normal quantile.",
            "solution": "Let $\\mu_{f} = \\mathbb{E}_{\\pi} f$ be the target expectation. We are given the MCMC sample average based on $n$ post-burn-in samples, $\\bar{f}_{n} = \\frac{1}{n} \\sum_{t=1}^{n} f(X_{B+t})$.\n\nFirst, we address the theoretical construction of the confidence interval. The foundation is the Markov Chain Central Limit Theorem (CLT). For a geometrically ergodic Markov chain $\\{X_t\\}$ and a real-valued function $f$ such that $\\mathbb{E}_\\pi [f(X_1)^2]  \\infty$, the CLT states:\n$$\n\\sqrt{n} (\\bar{f}_n - \\mu_f) \\xrightarrow{d} N(0, \\sigma^2_f)\n$$\nas $n \\to \\infty$, where $\\xrightarrow{d}$ denotes convergence in distribution, and $N(0, \\sigma^2_f)$ is a normal distribution with mean $0$ and variance $\\sigma^2_f$. The asymptotic variance $\\sigma^2_f$ is given by:\n$$\n\\sigma^2_f = \\text{Var}_{\\pi}(f(X_1)) + 2 \\sum_{k=1}^{\\infty} \\text{Cov}_{\\pi}(f(X_1), f(X_{1+k}))\n$$\nThis expression shows that $\\sigma^2_f$ incorporates the variance of $f(X_1)$ under the stationary distribution $\\pi$ as well as all the autocovariances at different lags $k$. This is how the CLT for dependent samples accounts for autocorrelation. If the samples were independent, all covariance terms would be zero, and $\\sigma^2_f$ would reduce to $\\text{Var}_{\\pi}(f(X_1))$.\n\nFrom the CLT, for a large sample size $n$, the sampling distribution of the estimator $\\bar{f}_n$ can be approximated by a normal distribution:\n$$\n\\bar{f}_n \\approx N\\left(\\mu_f, \\frac{\\sigma^2_f}{n}\\right)\n$$\nThe Monte Carlo Standard Error (MCSE) is defined as the standard deviation of this sampling distribution, $\\text{MCSE}(\\bar{f}_n) = \\sqrt{\\frac{\\sigma^2_f}{n}}$. To make this operational, we must estimate the unknown $\\sigma^2_f$ from the sample. The problem provides that a consistent estimator, $\\hat{\\sigma}^2_f$, is available (computed using methods like spectral analysis at frequency zero or batch means). The estimated MCSE is then:\n$$\n\\widehat{\\text{MCSE}}(\\bar{f}_n) = \\sqrt{\\frac{\\hat{\\sigma}^2_f}{n}} = \\frac{\\hat{\\sigma}_f}{\\sqrt{n}}\n$$\nA large-sample $(1-\\alpha) \\times 100\\%$ confidence interval for $\\mu_f$ is constructed by standardizing $\\bar{f}_n$:\n$$\n\\frac{\\bar{f}_n - \\mu_f}{\\widehat{\\text{MCSE}}(\\bar{f}_n)} \\approx N(0, 1)\n$$\nThis leads to the confidence interval formula:\n$$\n\\bar{f}_n \\pm z_{1-\\alpha/2} \\cdot \\widehat{\\text{MCSE}}(\\bar{f}_n)\n$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution.\n\nSecond, we address the role of the burn-in period. Discarding the initial $B$ samples does not alter the asymptotic distribution of $\\bar{f}_n$. The CLT is a limiting result as the sample size $n \\to \\infty$. The influence of any finite number of initial samples, such as the $B$ burn-in samples, becomes negligible in this limit. The normalization by $\\sqrt{n}$ ensures that the contribution of a fixed number of terms vanishes. Therefore, the asymptotic variance $\\sigma^2_f$ and the limiting normal distribution are the same whether we start summing from $t=1$ or $t=B+1$. However, for a finite sample size $n$, the burn-in is crucial for reducing bias. The MCMC chain is started from some initial state $X_0$ which is typically not a draw from the stationary distribution $\\pi$. For small $t$, the distribution of $X_t$ has not yet converged to $\\pi$. Consequently, the expectation $\\mathbb{E}[f(X_t) \\mid X_0]$ is not equal to $\\mu_f = \\mathbb{E}_{\\pi} f$. By discarding the first $B$ iterations, where $B$ is chosen to be large enough for the chain to \"forget\" its starting point and for the distribution of $X_B$ to be close to $\\pi$, we ensure that the retained samples $\\{X_{B+1}, \\dots, X_{B+n}\\}$ are approximately distributed according to $\\pi$. This makes the estimator $\\bar{f}_n$ approximately unbiased for $\\mu_f$, i.e., $\\mathbb{E}[\\bar{f}_n] \\approx \\mu_f$. Without burn-in, the estimator would suffer from \"transient bias\" due to the influence of the initial non-stationary part of the chain.\n\nFinally, we compute the required quantities.\nThe given data are:\n- Sample size: $n = 10^5$\n- Post-burn-in average: $\\bar{f}_n = 0.7321$\n- Estimated asymptotic variance: $\\hat{\\sigma}^2_f = 2.5$\n\nThe Monte Carlo Standard Error (MCSE) is estimated as:\n$$\n\\widehat{\\text{MCSE}} = \\sqrt{\\frac{\\hat{\\sigma}^2_f}{n}} = \\sqrt{\\frac{2.5}{10^5}} = \\sqrt{2.5 \\times 10^{-5}} = \\sqrt{25 \\times 10^{-6}} = 5 \\times 10^{-3} = 0.005\n$$\nRounding to four significant figures, the MCSE is $0.005000$.\n\nFor a $95\\%$ confidence interval, the significance level is $\\alpha = 0.05$. The corresponding standard normal quantile is $z_{1-\\alpha/2} = z_{0.975}$. We use the standard value $z_{0.975} \\approx 1.960$.\nThe margin of error ($ME$) is:\n$$\nME = z_{0.975} \\times \\widehat{\\text{MCSE}} \\approx 1.960 \\times 0.005000 = 0.009800\n$$\nThe $95\\%$ confidence interval for $\\mathbb{E}_{\\pi} f$ is given by $\\bar{f}_n \\pm ME$:\n$$\n0.7321 \\pm 0.009800\n$$\nThe lower bound is:\n$$\nL = 0.7321 - 0.009800 = 0.7223\n$$\nThe upper bound is:\n$$\nU = 0.7321 + 0.009800 = 0.7419\n$$\nBoth bounds are already at four significant figures. Thus, the $95\\%$ confidence interval is $[0.7223, 0.7419]$.\n\nSummary of numerical answers:\n- MCSE: $0.005000$\n- $95\\%$ CI: $[0.7223, 0.7419]$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.005000  0.7223  0.7419 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Before we can trust our estimates, we must gather evidence that the Markov chain has converged to its stationary distribution. The potential scale reduction factor (PSRF), or $\\hat{R}$, is a cornerstone diagnostic that compares the variance within parallel chains to the variance between them. This practice challenges you to extend this concept to a multidimensional parameter space by computing the multivariate PSRF (MPSRF), a crucial step for diagnosing convergence in complex models .",
            "id": "3287654",
            "problem": "A practitioner runs $m$ independent Markov chain Monte Carlo (MCMC) chains for a $p$-dimensional parameter to assess convergence after a burn-in period. The first $b$ draws of each chain are discarded as burn-in, and the remaining $n$ draws per chain are used to estimate the within-chain and between-chain variability.\n\nLet $p=3$, $m=3$, the total iterations per chain be $N_{\\text{total}}=1500$, and the burn-in be $b=500$, so that $n=1000$ post-burn-in draws are retained per chain. Suppose that, from these post-burn-in draws, the pooled within-chain covariance estimator is\n$$\nW \\;=\\; \\begin{pmatrix}\n4  0  0 \\\\\n0  1  0 \\\\\n0  0  9\n\\end{pmatrix},\n$$\nand the scaled between-chain covariance estimator (defined as $B = n$ times the sample covariance of the $m$ chain means) is\n$$\nB \\;=\\; \\begin{pmatrix}\n2.4  0  0 \\\\\n0  0.4  0 \\\\\n0  0  4.5\n\\end{pmatrix}.\n$$\n\nUsing the foundational definitions of the within-chain covariance $W$ and the between-chain covariance $B$ for multiple chains and the interpretation of the multivariate estimated potential scale reduction factor (MPSRF) as the supremum of the univariate potential scale reduction factor (PSRF) over all unit-length linear projections of the parameter space, compute the MPSRF for the post-burn-in draws. Express your final numerical answer rounded to four significant figures.",
            "solution": "The problem asks for the computation of the Multivariate Estimated Potential Scale Reduction Factor (MPSRF). The problem statement provides a crucial interpretive key: the MPSRF is the supremum of the univariate Potential Scale Reduction Factor (PSRF) over all unit-length linear projections of the $p$-dimensional parameter space.\n\nLet $\\theta$ be the $p$-dimensional parameter vector. A linear projection is defined by $\\psi = u^T \\theta$, where $u$ is a $p$-dimensional column vector with unit length, i.e., $u^T u = 1$. The univariate PSRF for the scalar projection $\\psi$, denoted $\\hat{R}_{\\psi}$, is defined by its square:\n$$ \\hat{R}_{\\psi}^2 = \\frac{\\hat{V}_{\\psi}}{W_{\\psi}} $$\nwhere $W_{\\psi}$ is the estimated within-chain variance for $\\psi$, and $\\hat{V}_{\\psi}$ is the estimated mixture variance for $\\psi$.\n\nGiven the within-chain covariance matrix $W$ and the scaled between-chain covariance matrix $B$ for the vector $\\theta$, the corresponding scalar variances for the projection $\\psi$ are given by quadratic forms:\n$$ W_{\\psi} = u^T W u $$\n$$ B_{\\psi} = u^T B u $$\n\nThe modern, corrected estimator for the mixture variance $\\hat{V}_{\\psi}$, as established by Brooks and Gelman (1998) and commonly used in Bayesian analysis, is a weighted sum of the within-chain and between-chain variances:\n$$ \\hat{V}_{\\psi} = \\frac{n-1}{n} W_{\\psi} + \\frac{m+1}{mn} B_{\\psi} $$\nHere, $n$ is the number of post-burn-in draws per chain and $m$ is the number of chains. The definition of $B$ provided in the problem, as $n$ times the sample covariance of the chain means, is consistent with this formulation.\n\nSubstituting the expressions for $W_{\\psi}$, $B_{\\psi}$, and $\\hat{V}_{\\psi}$ into the formula for $\\hat{R}_{\\psi}^2$ yields:\n$$ \\hat{R}_{\\psi}^2 = \\frac{\\frac{n-1}{n} (u^T W u) + \\frac{m+1}{mn} (u^T B u)}{u^T W u} = \\frac{n-1}{n} + \\frac{m+1}{mn} \\frac{u^T B u}{u^T W u} $$\n\nThe MPSRF is the square root of the supremum of $\\hat{R}_{\\psi}^2$ over all possible unit vectors $u$.\n$$ (\\text{MPSRF})^2 = \\sup_{u : u^T u = 1} \\left( \\hat{R}_{\\psi}^2 \\right) = \\sup_{u : u^T u = 1} \\left( \\frac{n-1}{n} + \\frac{m+1}{mn} \\frac{u^T B u}{u^T W u} \\right) $$\nSince the term $\\frac{n-1}{n}$ and the coefficient $\\frac{m+1}{mn}$ are constant with respect to $u$, finding the supremum requires maximizing the generalized Rayleigh quotient $\\frac{u^T B u}{u^T W u}$.\n$$ (\\text{MPSRF})^2 = \\frac{n-1}{n} + \\frac{m+1}{mn} \\sup_{u : u^T u = 1} \\left( \\frac{u^T B u}{u^T W u} \\right) $$\n\nThe supremum of the generalized Rayleigh quotient is the largest eigenvalue, $\\lambda_{\\max}$, of the generalized eigenvalue problem $Bv = \\lambda Wv$. Since $W$ is a covariance matrix and its given form is diagonal with positive entries, it is positive definite and thus invertible. Therefore, this problem is equivalent to finding the largest eigenvalue of the matrix $W^{-1}B$.\n\nThe given data are:\n- Number of chains, $m=3$.\n- Number of post-burn-in draws, $n=1000$.\n- Within-chain covariance matrix, $W = \\begin{pmatrix} 4  0  0 \\\\ 0  1  0 \\\\ 0  0  9 \\end{pmatrix}$.\n- Scaled between-chain covariance matrix, $B = \\begin{pmatrix} 2.4  0  0 \\\\ 0  0.4  0 \\\\ 0  0  4.5 \\end{pmatrix}$.\n\nFirst, we compute the inverse of $W$. Since $W$ is a diagonal matrix, its inverse is the diagonal matrix of the reciprocals of its diagonal elements:\n$$ W^{-1} = \\begin{pmatrix} 4^{-1}  0  0 \\\\ 0  1^{-1}  0 \\\\ 0  0  9^{-1} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  1  0 \\\\ 0  0  \\frac{1}{9} \\end{pmatrix} $$\n\nNext, we compute the product $W^{-1}B$:\n$$ W^{-1}B = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  1  0 \\\\ 0  0  \\frac{1}{9} \\end{pmatrix} \\begin{pmatrix} 2.4  0  0 \\\\ 0  0.4  0 \\\\ 0  0  4.5 \\end{pmatrix} $$\n$$ W^{-1}B = \\begin{pmatrix} \\frac{2.4}{4}  0  0 \\\\ 0  0.4  0 \\\\ 0  0  \\frac{4.5}{9} \\end{pmatrix} = \\begin{pmatrix} 0.6  0  0 \\\\ 0  0.4  0 \\\\ 0  0  0.5 \\end{pmatrix} $$\nThe eigenvalues of a diagonal matrix are its diagonal entries. The eigenvalues are $\\lambda_1 = 0.6$, $\\lambda_2 = 0.4$, and $\\lambda_3 = 0.5$. The largest eigenvalue is $\\lambda_{\\max} = 0.6$.\n\nNow, we substitute the values of $n$, $m$, and $\\lambda_{\\max}$ into the expression for $(\\text{MPSRF})^2$:\n$$ (\\text{MPSRF})^2 = \\frac{1000-1}{1000} + \\frac{3+1}{3 \\times 1000} \\lambda_{\\max} $$\n$$ (\\text{MPSRF})^2 = \\frac{999}{1000} + \\frac{4}{3000} (0.6) $$\n$$ (\\text{MPSRF})^2 = 0.999 + \\frac{2.4}{3000} = 0.999 + 0.0008 $$\n$$ (\\text{MPSRF})^2 = 0.9998 $$\n\nFinally, the MPSRF is the square root of this value:\n$$ \\text{MPSRF} = \\sqrt{0.9998} \\approx 0.999899995 $$\nRounding to four significant figures, we obtain $0.9999$.",
            "answer": "$$\n\\boxed{0.9999}\n$$"
        },
        {
            "introduction": "A common heuristic is to declare convergence when the $\\hat{R}$ statistic is 'close to 1,' but this raises the question of how to account for sampling variability. A low $\\hat{R}$ value can occur by chance even for non-converged chains, risking a premature end to the burn-in period. This advanced exercise delves into the theoretical underpinnings of this diagnostic by having you derive its exact sampling distribution under stationarity, providing a rigorous basis for designing robust stopping rules .",
            "id": "3287678",
            "problem": "Consider $m \\geq 2$ parallel chains in a Markov Chain Monte Carlo (MCMC) run targeting a one-dimensional normal distribution with mean $\\mu$ and variance $\\sigma^{2}$. After an initial transient phase, suppose the chains are in stationarity. For chain $j \\in \\{1,\\dots,m\\}$, denote by $\\bar{X}_{j}$ the chain mean and by $s_{j}^{2}$ the unbiased within-chain sample variance, computed from $n \\geq 2$ consecutive post-transient draws that we initially treat as independent. Define the within-chain variance estimator $W$ and the between-chain variance estimator $B$ by\n$$\nW \\equiv \\frac{1}{m}\\sum_{j=1}^{m} s_{j}^{2}, \\qquad B \\equiv n \\cdot S_{\\bar{X}}^{2}, \\qquad S_{\\bar{X}}^{2} \\equiv \\frac{1}{m-1}\\sum_{j=1}^{m} \\left(\\bar{X}_{j} - \\bar{X}_{\\cdot}\\right)^{2}, \\quad \\bar{X}_{\\cdot} \\equiv \\frac{1}{m}\\sum_{j=1}^{m} \\bar{X}_{j}.\n$$\nLet the potential scale reduction factor (PSRF) be defined by the commonly used estimator\n$$\n\\hat{R} \\equiv \\sqrt{\\frac{\\hat{V}}{W}}, \\qquad \\hat{V} \\equiv \\frac{n-1}{n}\\,W + \\frac{1}{n}\\,B.\n$$\nStarting only from the core definitions above and well-tested facts about the normal model (namely, the independence of sample means and unbiased sample variances, the chi-square distribution of scaled unbiased sample variances, and the Fisher–Snedecor $F$ distribution for ratios of independent scaled chi-squares), derive the exact finite-sample sampling distribution of $\\hat{R}$ under stationarity.\n\nThen, to design a stopping rule for the burn-in period that is resistant to spurious early stopping caused by finite-sample variability, propose a data-dependent threshold function $t_{\\alpha}(m,\\widehat{n}_{\\mathrm{eff}},\\alpha)$ such that, under stationarity with per-chain effective sample size $\\widehat{n}_{\\mathrm{eff}}$ replacing $n$ to account for autocorrelation in the post-transient draws, the probability that the rule $\\hat{R} \\leq t_{\\alpha}(m,\\widehat{n}_{\\mathrm{eff}},\\alpha)$ fires is exactly $1-\\alpha$. Express your final threshold in closed form using the quantile function of the Fisher–Snedecor $F$ distribution, with degrees of freedom parameterized in terms of $m$ and $\\widehat{n}_{\\mathrm{eff}}$. Your final answer must be a single closed-form analytic expression for $t_{\\alpha}(m,\\widehat{n}_{\\mathrm{eff}},\\alpha)$.",
            "solution": "The problem asks for two results. First, the derivation of the exact finite-sample sampling distribution of the potential scale reduction factor, $\\hat{R}$, under the assumption that $m$ parallel MCMC chains are generating independent and identically distributed (i.i.d.) draws from a one-dimensional normal distribution $N(\\mu, \\sigma^2)$. Second, an expression for a threshold $t_{\\alpha}(m, \\widehat{n}_{\\mathrm{eff}}, \\alpha)$ for a stopping rule, which is derived by extending the first result to account for autocorrelation via an effective sample size, $\\widehat{n}_{\\mathrm{eff}}$.\n\nLet us begin with the first part. The definition of the potential scale reduction factor is given by $\\hat{R} \\equiv \\sqrt{\\hat{V}/W}$, where $\\hat{V} \\equiv \\frac{n-1}{n}W + \\frac{1}{n}B$. We are interested in the distribution of this quantity. It is more convenient to analyze the distribution of its square, $\\hat{R}^2$.\n$$\n\\hat{R}^2 = \\frac{\\hat{V}}{W} = \\frac{\\frac{n-1}{n}W + \\frac{1}{n}B}{W} = \\frac{n-1}{n} + \\frac{1}{n}\\frac{B}{W}\n$$\nThe sampling distribution of $\\hat{R}^2$ is thus a linear transformation of the sampling distribution of the ratio of the between-chain variance $B$ to the within-chain variance $W$. Our goal is to find the distribution of $B/W$.\n\nThe problem states that for each chain $j \\in \\{1, \\dots, m\\}$, we have $n \\geq 2$ i.i.d. draws from a $N(\\mu, \\sigma^2)$ distribution.\nThe within-chain variance estimator is $W = \\frac{1}{m}\\sum_{j=1}^{m} s_{j}^{2}$, where $s_j^2$ is the unbiased sample variance for chain $j$. According to standard results for sampling from a normal distribution, the quantity $\\frac{(n-1)s_j^2}{\\sigma^2}$ follows a chi-square distribution with $n-1$ degrees of freedom, i.e., $\\frac{(n-1)s_j^2}{\\sigma^2} \\sim \\chi^2_{n-1}$. Since the $m$ chains are independent, the $s_j^2$ are independent random variables.\nLet us consider the scaled version of $W$:\n$$\n\\frac{m(n-1)W}{\\sigma^2} = \\frac{m(n-1)}{\\sigma^2} \\left( \\frac{1}{m}\\sum_{j=1}^{m} s_{j}^{2} \\right) = \\sum_{j=1}^{m} \\frac{(n-1)s_j^2}{\\sigma^2}\n$$\nThis is a sum of $m$ independent $\\chi^2_{n-1}$ random variables. The sum of independent chi-square variables is a chi-square variable with degrees of freedom equal to the sum of the individual degrees of freedom. Therefore,\n$$\n\\frac{m(n-1)W}{\\sigma^2} \\sim \\chi^2_{m(n-1)}\n$$\n\nNext, we analyze the between-chain variance estimator, $B = n S_{\\bar{X}}^2$, where $S_{\\bar{X}}^2 = \\frac{1}{m-1}\\sum_{j=1}^{m} (\\bar{X}_{j} - \\bar{X}_{\\cdot})^2$. The sample mean for each chain, $\\bar{X}_j$, is the average of $n$ i.i.d. draws from $N(\\mu, \\sigma^2)$. Thus, the $\\bar{X}_j$ themselves are i.i.d. random variables from the distribution $N(\\mu, \\sigma^2/n)$.\n$S_{\\bar{X}}^2$ is the unbiased sample variance of these $m$ sample means. The population variance of the $\\bar{X}_j$ is $\\sigma^2/n$. For samples from a normal distribution, the scaled sample variance follows a chi-square distribution. Specifically,\n$$\n\\frac{(m-1)S_{\\bar{X}}^2}{\\sigma^2/n} \\sim \\chi^2_{m-1}\n$$\nNow, substitute $S_{\\bar{X}}^2 = B/n$:\n$$\n\\frac{(m-1)(B/n)}{\\sigma^2/n} = \\frac{(m-1)B}{\\sigma^2} \\sim \\chi^2_{m-1}\n$$\n\nThe ratio $B/W$ can be related to a Fisher-Snedecor $F$-distribution, which is defined as the ratio of two independent chi-square variables, each divided by its degrees of freedom. For this, we need to establish the independence of $B$ and $W$.\n$W$ is a function of the set of within-chain sample variances $\\{s_1^2, \\dots, s_m^2\\}$.\n$B$ is a function of the set of chain sample means $\\{\\bar{X}_1, \\dots, \\bar{X}_m\\}$.\nBy Cochran's theorem, for samples from a normal distribution, the sample mean $\\bar{X}_j$ and the sample variance $s_j^2$ are independent for any given chain $j$. Since the chains are themselves independent, the entire set of sample means $\\{\\bar{X}_j\\}_{j=1}^m$ is independent of the entire set of sample variances $\\{s_j^2\\}_{j=1}^m$. Consequently, $B$ and $W$ are independent random variables.\n\nWe can now form the $F$-statistic. Let $F_{d_1, d_2}$ denote a random variable following an $F$-distribution with $d_1$ and $d_2$ degrees of freedom. By definition,\n$$\nF_{d_1, d_2} = \\frac{U_1/d_1}{U_2/d_2}\n$$\nwhere $U_1 \\sim \\chi^2_{d_1}$ and $U_2 \\sim \\chi^2_{d_2}$ are independent.\nLet's choose our chi-square variables:\n$U_1 = \\frac{(m-1)B}{\\sigma^2}$ with $d_1 = m-1$.\n$U_2 = \\frac{m(n-1)W}{\\sigma^2}$ with $d_2 = m(n-1)$.\nThe ratio is:\n$$\n\\frac{U_1/d_1}{U_2/d_2} = \\frac{\\left(\\frac{(m-1)B}{\\sigma^2}\\right)/(m-1)}{\\left(\\frac{m(n-1)W}{\\sigma^2}\\right)/(m(n-1))} = \\frac{B/\\sigma^2}{W/\\sigma^2} = \\frac{B}{W}\n$$\nThus, we have derived the sampling distribution of the ratio $B/W$:\n$$\n\\frac{B}{W} \\sim F_{m-1, m(n-1)}\n$$\nThis implies that $\\hat{R}^2 = \\frac{n-1}{n} + \\frac{1}{n}F$ where $F \\sim F_{m-1, m(n-1)}$. This concludes the first part of the problem.\n\nFor the second part, we need to propose a threshold $t_{\\alpha}(m, \\widehat{n}_{\\mathrm{eff}}, \\alpha)$ such that the probability of the stopping rule $\\hat{R} \\leq t_{\\alpha}$ firing is $1-\\alpha$ under stationarity. The problem instructs us to account for autocorrelation in the MCMC draws by replacing the number of draws, $n$, with a suitable effective sample size, $\\widehat{n}_{\\mathrm{eff}}$.\nMaking this substitution in our derived distribution, we have that under these approximate conditions, $\\hat{R}^2$ is distributed as:\n$$\n\\hat{R}^2 \\sim \\frac{\\widehat{n}_{\\mathrm{eff}}-1}{\\widehat{n}_{\\mathrm{eff}}} + \\frac{1}{\\widehat{n}_{\\mathrm{eff}}} F_{m-1, m(\\widehat{n}_{\\mathrm{eff}}-1)}\n$$\nThe condition for the stopping rule is $P(\\hat{R} \\leq t_{\\alpha}) = 1-\\alpha$. Since $\\hat{R}$ and $t_{\\alpha}$ are necessarily non-negative, this is equivalent to $P(\\hat{R}^2 \\leq t_{\\alpha}^2) = 1-\\alpha$.\nLet $F$ be a random variable distributed as $F_{m-1, m(\\widehat{n}_{\\mathrm{eff}}-1)}$. The probability statement becomes:\n$$\nP\\left(\\frac{\\widehat{n}_{\\mathrm{eff}}-1}{\\widehat{n}_{\\mathrm{eff}}} + \\frac{1}{\\widehat{n}_{\\mathrm{eff}}} F \\leq t_{\\alpha}^2\\right) = 1-\\alpha\n$$\nWe now solve the inequality for $F$:\n$$\n\\frac{1}{\\widehat{n}_{\\mathrm{eff}}} F \\leq t_{\\alpha}^2 - \\frac{\\widehat{n}_{\\mathrm{eff}}-1}{\\widehat{n}_{\\mathrm{eff}}}\n$$\n$$\nF \\leq \\widehat{n}_{\\mathrm{eff}} t_{\\alpha}^2 - (\\widehat{n}_{\\mathrm{eff}}-1)\n$$\nSo the original probability statement is equivalent to:\n$$\nP\\left(F \\leq \\widehat{n}_{\\mathrm{eff}} t_{\\alpha}^2 - (\\widehat{n}_{\\mathrm{eff}}-1)\\right) = 1-\\alpha\n$$\nLet $F_{d_1, d_2, p}$ denote the $p$-th quantile of the $F$-distribution with $d_1$ and $d_2$ degrees of freedom. By definition, $P(F \\leq F_{d_1, d_2, p}) = p$.\nComparing this with our expression, we can identify $p = 1-\\alpha$ and equate the arguments:\n$$\n\\widehat{n}_{\\mathrm{eff}} t_{\\alpha}^2 - (\\widehat{n}_{\\mathrm{eff}}-1) = F_{m-1, m(\\widehat{n}_{\\mathrm{eff}}-1), 1-\\alpha}\n$$\nNow, we solve for $t_{\\alpha}$.\n$$\n\\widehat{n}_{\\mathrm{eff}} t_{\\alpha}^2 = (\\widehat{n}_{\\mathrm{eff}}-1) + F_{m-1, m(\\widehat{n}_{\\mathrm{eff}}-1), 1-\\alpha}\n$$\n$$\nt_{\\alpha}^2 = \\frac{\\widehat{n}_{\\mathrm{eff}}-1}{\\widehat{n}_{\\mathrm{eff}}} + \\frac{1}{\\widehat{n}_{\\mathrm{eff}}} F_{m-1, m(\\widehat{n}_{\\mathrm{eff}}-1), 1-\\alpha}\n$$\nTaking the square root gives the final expression for the threshold $t_{\\alpha}$, which we can write as $t_{\\alpha}(m, \\widehat{n}_{\\mathrm{eff}}, \\alpha)$.\n$$\nt_{\\alpha}(m, \\widehat{n}_{\\mathrm{eff}}, \\alpha) = \\sqrt{\\frac{\\widehat{n}_{\\mathrm{eff}}-1}{\\widehat{n}_{\\mathrm{eff}}} + \\frac{1}{\\widehat{n}_{\\mathrm{eff}}} F_{m-1, m(\\widehat{n}_{\\mathrm{eff}}-1), 1-\\alpha}}\n$$\nThis expression is the closed-form analytic solution for the data-dependent threshold as requested.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{\\widehat{n}_{\\mathrm{eff}}-1}{\\widehat{n}_{\\mathrm{eff}}} + \\frac{1}{\\widehat{n}_{\\mathrm{eff}}} F_{m-1, m(\\widehat{n}_{\\mathrm{eff}}-1), 1-\\alpha}}}\n$$"
        }
    ]
}