{
    "hands_on_practices": [
        {
            "introduction": "The first critical step in any MCMC analysis is to assess whether the chains have converged to the target stationary distribution. The multivariate potential scale reduction factor (MPSRF) provides a powerful diagnostic for this by comparing the within-chain and between-chain variability across multiple parallel simulations. This exercise guides you through computing the MPSRF, revealing how it distills complex, high-dimensional information into a single, interpretable number to diagnose convergence.",
            "id": "3287654",
            "problem": "A practitioner runs $m$ independent Markov chain Monte Carlo (MCMC) chains for a $p$-dimensional parameter to assess convergence after a burn-in period. The first $b$ draws of each chain are discarded as burn-in, and the remaining $n$ draws per chain are used to estimate the within-chain and between-chain variability.\n\nLet $p=3$, $m=3$, the total iterations per chain be $N_{\\text{total}}=1500$, and the burn-in be $b=500$, so that $n=1000$ post-burn-in draws are retained per chain. Suppose that, from these post-burn-in draws, the pooled within-chain covariance estimator is\n$$\nW \\;=\\; \\begin{pmatrix}\n4 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 9\n\\end{pmatrix},\n$$\nand the scaled between-chain covariance estimator (defined as $B = n$ times the sample covariance of the $m$ chain means) is\n$$\nB \\;=\\; \\begin{pmatrix}\n2.4 & 0 & 0 \\\\\n0 & 0.4 & 0 \\\\\n0 & 0 & 4.5\n\\end{pmatrix}.\n$$\n\nUsing the foundational definitions of the within-chain covariance $W$ and the between-chain covariance $B$ for multiple chains and the interpretation of the multivariate estimated potential scale reduction factor (MPSRF) as the supremum of the univariate potential scale reduction factor (PSRF) over all unit-length linear projections of the parameter space, compute the MPSRF for the post-burn-in draws. Express your final numerical answer rounded to four significant figures.",
            "solution": "The problem asks for the computation of the Multivariate Estimated Potential Scale Reduction Factor (MPSRF). The problem statement provides a crucial interpretive key: the MPSRF is the supremum of the univariate Potential Scale Reduction Factor (PSRF) over all unit-length linear projections of the $p$-dimensional parameter space.\n\nLet $\\theta$ be the $p$-dimensional parameter vector. A linear projection is defined by $\\psi = u^T \\theta$, where $u$ is a $p$-dimensional column vector with unit length, i.e., $u^T u = 1$. The univariate PSRF for the scalar projection $\\psi$, denoted $\\hat{R}_{\\psi}$, is defined by its square:\n$$ \\hat{R}_{\\psi}^2 = \\frac{\\hat{V}_{\\psi}}{W_{\\psi}} $$\nwhere $W_{\\psi}$ is the estimated within-chain variance for $\\psi$, and $\\hat{V}_{\\psi}$ is the estimated mixture variance for $\\psi$.\n\nGiven the within-chain covariance matrix $W$ and the scaled between-chain covariance matrix $B$ for the vector $\\theta$, the corresponding scalar variances for the projection $\\psi$ are given by quadratic forms:\n$$ W_{\\psi} = u^T W u $$\n$$ B_{\\psi} = u^T B u $$\n\nThe modern, corrected estimator for the mixture variance $\\hat{V}_{\\psi}$, as established by Brooks and Gelman (1998) and commonly used in Bayesian analysis, is a weighted sum of the within-chain and between-chain variances:\n$$ \\hat{V}_{\\psi} = \\frac{n-1}{n} W_{\\psi} + \\frac{m+1}{mn} B_{\\psi} $$\nHere, $n$ is the number of post-burn-in draws per chain and $m$ is the number of chains. The definition of $B$ provided in the problem, as $n$ times the sample covariance of the chain means, is consistent with this formulation.\n\nSubstituting the expressions for $W_{\\psi}$, $B_{\\psi}$, and $\\hat{V}_{\\psi}$ into the formula for $\\hat{R}_{\\psi}^2$ yields:\n$$ \\hat{R}_{\\psi}^2 = \\frac{\\frac{n-1}{n} (u^T W u) + \\frac{m+1}{mn} (u^T B u)}{u^T W u} = \\frac{n-1}{n} + \\frac{m+1}{mn} \\frac{u^T B u}{u^T W u} $$\n\nThe MPSRF is the square root of the supremum of $\\hat{R}_{\\psi}^2$ over all possible unit vectors $u$.\n$$ (\\text{MPSRF})^2 = \\sup_{u : u^T u = 1} \\left( \\hat{R}_{\\psi}^2 \\right) = \\sup_{u : u^T u = 1} \\left( \\frac{n-1}{n} + \\frac{m+1}{mn} \\frac{u^T B u}{u^T W u} \\right) $$\nSince the term $\\frac{n-1}{n}$ and the coefficient $\\frac{m+1}{mn}$ are constant with respect to $u$, finding the supremum requires maximizing the generalized Rayleigh quotient $\\frac{u^T B u}{u^T W u}$.\n$$ (\\text{MPSRF})^2 = \\frac{n-1}{n} + \\frac{m+1}{mn} \\sup_{u : u^T u = 1} \\left( \\frac{u^T B u}{u^T W u} \\right) $$\n\nThe supremum of the generalized Rayleigh quotient is the largest eigenvalue, $\\lambda_{\\max}$, of the generalized eigenvalue problem $Bv = \\lambda Wv$. Since $W$ is a covariance matrix and its given form is diagonal with positive entries, it is positive definite and thus invertible. Therefore, this problem is equivalent to finding the largest eigenvalue of the matrix $W^{-1}B$.\n\nThe given data are:\n- Number of chains, $m=3$.\n- Number of post-burn-in draws, $n=1000$.\n- Within-chain covariance matrix, $W = \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 9 \\end{pmatrix}$.\n- Scaled between-chain covariance matrix, $B = \\begin{pmatrix} 2.4 & 0 & 0 \\\\ 0 & 0.4 & 0 \\\\ 0 & 0 & 4.5 \\end{pmatrix}$.\n\nFirst, we compute the inverse of $W$. Since $W$ is a diagonal matrix, its inverse is the diagonal matrix of the reciprocals of its diagonal elements:\n$$ W^{-1} = \\begin{pmatrix} 4^{-1} & 0 & 0 \\\\ 0 & 1^{-1} & 0 \\\\ 0 & 0 & 9^{-1} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & \\frac{1}{9} \\end{pmatrix} $$\n\nNext, we compute the product $W^{-1}B$:\n$$ W^{-1}B = \\begin{pmatrix} \\frac{1}{4} & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & \\frac{1}{9} \\end{pmatrix} \\begin{pmatrix} 2.4 & 0 & 0 \\\\ 0 & 0.4 & 0 \\\\ 0 & 0 & 4.5 \\end{pmatrix} $$\n$$ W^{-1}B = \\begin{pmatrix} \\frac{2.4}{4} & 0 & 0 \\\\ 0 & 0.4 & 0 \\\\ 0 & 0 & \\frac{4.5}{9} \\end{pmatrix} = \\begin{pmatrix} 0.6 & 0 & 0 \\\\ 0 & 0.4 & 0 \\\\ 0 & 0 & 0.5 \\end{pmatrix} $$\nThe eigenvalues of a diagonal matrix are its diagonal entries. The eigenvalues are $\\lambda_1 = 0.6$, $\\lambda_2 = 0.4$, and $\\lambda_3 = 0.5$. The largest eigenvalue is $\\lambda_{\\max} = 0.6$.\n\nNow, we substitute the values of $n$, $m$, and $\\lambda_{\\max}$ into the expression for $(\\text{MPSRF})^2$:\n$$ (\\text{MPSRF})^2 = \\frac{1000-1}{1000} + \\frac{3+1}{3 \\times 1000} \\lambda_{\\max} $$\n$$ (\\text{MPSRF})^2 = \\frac{999}{1000} + \\frac{4}{3000} (0.6) $$\n$$ (\\text{MPSRF})^2 = 0.999 + \\frac{2.4}{3000} = 0.999 + 0.0008 $$\n$$ (\\text{MPSRF})^2 = 0.9998 $$\n\nFinally, the MPSRF is the square root of this value:\n$$ \\text{MPSRF} = \\sqrt{0.9998} \\approx 0.999899995 $$\nRounding to four significant figures, we obtain $0.9999$.",
            "answer": "$$\n\\boxed{0.9999}\n$$"
        },
        {
            "introduction": "After using a diagnostic like the MPSRF  to establish convergence and discarding a suitable burn-in period, the focus shifts to estimation. The Monte Carlo Standard Error (MCSE) is the gold standard for quantifying the precision of an MCMC estimator, as it properly accounts for the autocorrelation inherent in the samples. This practice problem demonstrates how to use the MCSE, derived from the Markov chain central limit theorem, to construct a valid confidence interval for your target estimate.",
            "id": "3287635",
            "problem": "A practitioner runs a Markov Chain Monte Carlo (MCMC) algorithm to estimate the expectation $\\mathbb{E}_{\\pi} f$ of a measurable function $f$ under a stationary target distribution $\\pi$ on $\\mathbb{R}$. The chain is constructed by a random-walk Metropolis transition and is known to be geometrically ergodic. The practitioner discards a burn-in period of $B$ iterations to mitigate transient bias, and retains $n$ post-burn-in samples $\\{X_{B+1},\\dots,X_{B+n}\\}$ assumed to be approximately stationary. Let $\\bar{f}_{n} = n^{-1}\\sum_{t=1}^{n} f(X_{B+t})$ denote the post-burn-in average.\n\nStarting from the Markov chain central limit theorem and the definition of the Monte Carlo standard error (MCSE) as the standard deviation of the sampling distribution of $\\bar{f}_{n}$ under $\\pi$ when the samples are generated by an ergodic Markov chain, derive how to construct a large-sample confidence interval for $\\mathbb{E}_{\\pi} f$ that accounts for autocorrelation. Explain why discarding burn-in does not alter the asymptotic distribution of $\\bar{f}_{n}$ but is used to reduce bias in finite samples.\n\nAssume that a consistent spectral (or batch-means) estimator of the asymptotic variance of $\\sqrt{n}(\\bar{f}_{n}-\\mathbb{E}_{\\pi} f)$ computed from the $n$ post-burn-in samples yields $\\hat{\\sigma}^{2}_{f} = 2.5$. Suppose further that $n = 10^{5}$ and the observed post-burn-in average is $\\bar{f}_{n} = 0.7321$.\n\nCompute:\n- The Monte Carlo standard error (MCSE).\n- A $95\\%$ confidence interval for $\\mathbb{E}_{\\pi} f$ based on the normal approximation.\n\nRound all numerical answers to four significant figures. Do not use a percentage sign; interpret $95\\%$ via the standard normal quantile.",
            "solution": "Let $\\mu_{f} = \\mathbb{E}_{\\pi} f$ be the target expectation. We are given the MCMC sample average based on $n$ post-burn-in samples, $\\bar{f}_{n} = \\frac{1}{n} \\sum_{t=1}^{n} f(X_{B+t})$.\n\nFirst, we address the theoretical construction of the confidence interval. The foundation is the Markov Chain Central Limit Theorem (CLT). For a geometrically ergodic Markov chain $\\{X_t\\}$ and a real-valued function $f$ such that $\\mathbb{E}_\\pi [f(X_1)^2] < \\infty$, the CLT states:\n$$\n\\sqrt{n} (\\bar{f}_n - \\mu_f) \\xrightarrow{d} N(0, \\sigma^2_f)\n$$\nas $n \\to \\infty$, where $\\xrightarrow{d}$ denotes convergence in distribution, and $N(0, \\sigma^2_f)$ is a normal distribution with mean $0$ and variance $\\sigma^2_f$. The asymptotic variance $\\sigma^2_f$ is given by:\n$$\n\\sigma^2_f = \\text{Var}_{\\pi}(f(X_1)) + 2 \\sum_{k=1}^{\\infty} \\text{Cov}_{\\pi}(f(X_1), f(X_{1+k}))\n$$\nThis expression shows that $\\sigma^2_f$ incorporates the variance of $f(X_1)$ under the stationary distribution $\\pi$ as well as all the autocovariances at different lags $k$. This is how the CLT for dependent samples accounts for autocorrelation. If the samples were independent, all covariance terms would be zero, and $\\sigma^2_f$ would reduce to $\\text{Var}_{\\pi}(f(X_1))$.\n\nFrom the CLT, for a large sample size $n$, the sampling distribution of the estimator $\\bar{f}_n$ can be approximated by a normal distribution:\n$$\n\\bar{f}_n \\approx N\\left(\\mu_f, \\frac{\\sigma^2_f}{n}\\right)\n$$\nThe Monte Carlo Standard Error (MCSE) is defined as the standard deviation of this sampling distribution, $\\text{MCSE}(\\bar{f}_n) = \\sqrt{\\frac{\\sigma^2_f}{n}}$. To make this operational, we must estimate the unknown $\\sigma^2_f$ from the sample. The problem provides that a consistent estimator, $\\hat{\\sigma}^2_f$, is available (computed using methods like spectral analysis at frequency zero or batch means). The estimated MCSE is then:\n$$\n\\widehat{\\text{MCSE}}(\\bar{f}_n) = \\sqrt{\\frac{\\hat{\\sigma}^2_f}{n}} = \\frac{\\hat{\\sigma}_f}{\\sqrt{n}}\n$$\nA large-sample $(1-\\alpha) \\times 100\\%$ confidence interval for $\\mu_f$ is constructed by standardizing $\\bar{f}_n$:\n$$\n\\frac{\\bar{f}_n - \\mu_f}{\\widehat{\\text{MCSE}}(\\bar{f}_n)} \\approx N(0, 1)\n$$\nThis leads to the confidence interval formula:\n$$\n\\bar{f}_n \\pm z_{1-\\alpha/2} \\cdot \\widehat{\\text{MCSE}}(\\bar{f}_n)\n$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution.\n\nSecond, we address the role of the burn-in period. Discarding the initial $B$ samples does not alter the asymptotic distribution of $\\bar{f}_n$. The CLT is a limiting result as the sample size $n \\to \\infty$. The influence of any finite number of initial samples, such as the $B$ burn-in samples, becomes negligible in this limit. The normalization by $\\sqrt{n}$ ensures that the contribution of a fixed number of terms vanishes. Therefore, the asymptotic variance $\\sigma^2_f$ and the limiting normal distribution are the same whether we start summing from $t=1$ or $t=B+1$. However, for a finite sample size $n$, the burn-in is crucial for reducing bias. The MCMC chain is started from some initial state $X_0$ which is typically not a draw from the stationary distribution $\\pi$. For small $t$, the distribution of $X_t$ has not yet converged to $\\pi$. Consequently, the expectation $\\mathbb{E}[f(X_t) \\mid X_0]$ is not equal to $\\mu_f = \\mathbb{E}_{\\pi} f$. By discarding the first $B$ iterations, where $B$ is chosen to be large enough for the chain to \"forget\" its starting point and for the distribution of $X_B$ to be close to $\\pi$, we ensure that the retained samples $\\{X_{B+1}, \\dots, X_{B+n}\\}$ are approximately distributed according to $\\pi$. This makes the estimator $\\bar{f}_n$ approximately unbiased for $\\mu_f$, i.e., $\\mathbb{E}[\\bar{f}_n] \\approx \\mu_f$. Without burn-in, the estimator would suffer from \"transient bias\" due to the influence of the initial non-stationary part of the chain.\n\nFinally, we compute the required quantities.\nThe given data are:\n- Sample size: $n = 10^5$\n- Post-burn-in average: $\\bar{f}_n = 0.7321$\n- Estimated asymptotic variance: $\\hat{\\sigma}^2_f = 2.5$\n\nThe Monte Carlo Standard Error (MCSE) is estimated as:\n$$\n\\widehat{\\text{MCSE}} = \\sqrt{\\frac{\\hat{\\sigma}^2_f}{n}} = \\sqrt{\\frac{2.5}{10^5}} = \\sqrt{2.5 \\times 10^{-5}} = \\sqrt{25 \\times 10^{-6}} = 5 \\times 10^{-3} = 0.005\n$$\nRounding to four significant figures, the MCSE is $0.005000$.\n\nFor a $95\\%$ confidence interval, the significance level is $\\alpha = 0.05$. The corresponding standard normal quantile is $z_{1-\\alpha/2} = z_{0.975}$. We use the standard value $z_{0.975} \\approx 1.960$.\nThe margin of error ($ME$) is:\n$$\nME = z_{0.975} \\times \\widehat{\\text{MCSE}} \\approx 1.960 \\times 0.005000 = 0.009800\n$$\nThe $95\\%$ confidence interval for $\\mathbb{E}_{\\pi} f$ is given by $\\bar{f}_n \\pm ME$:\n$$\n0.7321 \\pm 0.009800\n$$\nThe lower bound is:\n$$\nL = 0.7321 - 0.009800 = 0.7223\n$$\nThe upper bound is:\n$$\nU = 0.7321 + 0.009800 = 0.7419\n$$\nBoth bounds are already at four significant figures. Thus, the $95\\%$ confidence interval is $[0.7223, 0.7419]$.\n\nSummary of numerical answers:\n- MCSE: $0.005000$\n- $95\\%$ CI: $[0.7223, 0.7419]$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.005000 & 0.7223 & 0.7419 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "To obtain a precise estimate, we aim to minimize the MCSE discussed previously . This raises a practical question: how should we manage simulation output, especially with long chains and storage limitations? This exercise tackles the common practice of \"thinning\" by rigorously comparing it to an alternative, revealing which strategy is statistically optimal and deepening your understanding of how sample correlation impacts estimator efficiency.",
            "id": "3287676",
            "problem": "Consider a Markov chain Monte Carlo (MCMC) run of length $n$ producing a stationary time series $\\{f(X_t)\\}_{t=1}^{n}$ intended to estimate $\\mu_{\\pi}=\\mathbb{E}_{\\pi}[f(X)]$, where $\\pi$ is the target distribution. Suppose the chain is $\\pi$-stationary after an optional burn-in of $b$ iterations and that the stationary autocorrelation function of $\\{f(X_t)\\}$ is $\\rho_k=\\phi^{k}$ for some fixed $\\phi\\in(0,1)$ and the stationary marginal variance is $\\sigma_f^{2}=\\mathrm{Var}_{\\pi}(f(X))$. You have a storage budget that would permit saving only $n/5$ raw samples of $f(X_t)$, but a lossless on-the-fly compression scheme reduces the per-sample storage by a factor of $5$ (so that compressing every generated sample fits in the budget). You must choose between two strategies for estimating $\\mu_{\\pi}$ subject to this budget:\n\n- Thinning: discard the burn-in and then save every $m$th value of $f(X_t)$, with $m\\in\\mathbb{N}$, and compute the empirical average over the saved values.\n- Compression: discard the burn-in and compress every post-burn-in sample $f(X_t)$ on the fly, and compute the empirical average over all post-burn-in values.\n\nDefine the Monte Carlo standard error (MCSE) of an estimator as the square root of its asymptotic variance. Using fundamental definitions of autocovariance, spectral density at frequency zero, and the integrated autocorrelation time, derive the asymptotic MCSEs of both strategies. Then, under the given storage budget and autocorrelation structure, determine the strategy and burn-in choice that minimize the MCSE and report the minimal achievable MCSE as a single closed-form analytic expression in terms of $\\sigma_f^{2}$, $\\phi$, and $n$. Your final answer must be a single analytic expression. No units are required. Do not round the final expression.",
            "solution": "The problem asks us to determine the optimal strategy between thinning and on-the-fly compression for estimating the mean $\\mu_{\\pi} = \\mathbb{E}_{\\pi}[f(X)]$ from a stationary MCMC time series, subject to a storage budget. The goal is to minimize the Monte Carlo Standard Error (MCSE). The solution requires deriving the MCSE for each strategy, optimizing the parameters for each, and then comparing the minimal achievable MCSE.\n\nFirst, we establish the general formula for the asymptotic variance of a sample mean estimator from a stationary time series. Let $\\{Y_t\\}_{t=1}^{N}$ be a stationary time series with marginal variance $\\mathrm{Var}(Y_t) = \\sigma^2$ and autocorrelation function $\\rho_k = \\mathrm{Corr}(Y_t, Y_{t+k})$. The estimator for the mean is $\\hat{\\mu} = \\frac{1}{N} \\sum_{t=1}^{N} Y_t$. The asymptotic variance of this estimator is given by the Central Limit Theorem for correlated data:\n$$\n\\mathrm{Var}(\\hat{\\mu}) \\approx \\frac{\\sigma^2}{N} \\tau_{eff}\n$$\nwhere $\\tau_{eff}$ is the integrated autocorrelation time (IACT), defined as:\n$$\n\\tau_{eff} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k\n$$\nThe MCSE is the square root of this variance, $\\mathrm{MCSE}(\\hat{\\mu}) = \\sqrt{\\mathrm{Var}(\\hat{\\mu})}$.\n\nIn our problem, the time series is $\\{f(X_t)\\}$, so we identify $Y_t = f(X_t)$. The stationary marginal variance is given as $\\sigma_f^2$. The stationary autocorrelation function is given as $\\rho_k = \\phi^k$ for some $\\phi \\in (0,1)$. We can calculate the IACT for this AR($1$) correlation structure:\n$$\n\\tau_{eff} = 1 + 2 \\sum_{k=1}^{\\infty} \\phi^k\n$$\nThis is a geometric series, which sums to $\\phi/(1-\\phi)$.\n$$\n\\tau_{eff} = 1 + 2 \\frac{\\phi}{1-\\phi} = \\frac{1-\\phi+2\\phi}{1-\\phi} = \\frac{1+\\phi}{1-\\phi}\n$$\nSo, for a stationary series of length $N$ with this autocorrelation, the squared MCSE is:\n$$\n\\mathrm{MCSE}^2 \\approx \\frac{\\sigma_f^2}{N} \\left(\\frac{1+\\phi}{1-\\phi}\\right)\n$$\n\nNow we analyze the two strategies. Both strategies involve discarding an initial burn-in of $b$ samples from the total run of length $n$. The number of post-burn-in samples is $n-b$. The problem states the chain is stationary after this burn-in, and leaves $b$ as a parameter to be chosen.\n\n**Strategy 1: Compression**\nIn this strategy, we use all $N_{comp} = n-b$ post-burn-in samples. The estimator is the empirical average of these $n-b$ samples. The compression scheme allows us to store all these samples within the budget. The storage constraint is that the number of stored samples ($n-b$) times the cost per sample ($1/5$ of a raw sample) must be less than or equal to the budget ($n/5$ raw samples).\n$$\n(n-b) \\times \\frac{1}{5} \\le \\frac{n}{5} \\implies n-b \\le n \\implies b \\ge 0\n$$\nThis constraint is satisfied for any valid burn-in length $b \\in [0, n)$.\nThe squared MCSE for this strategy, as a function of the chosen burn-in $b$, is:\n$$\n\\mathrm{MCSE}_{comp}^2(b) = \\frac{\\sigma_f^2}{N_{comp}} \\tau_{eff} = \\frac{\\sigma_f^2}{n-b} \\left(\\frac{1+\\phi}{1-\\phi}\\right)\n$$\nTo minimize $\\mathrm{MCSE}_{comp}(b)$, we must minimize its square. This expression is minimized by maximizing the denominator $n-b$, which means choosing the smallest possible value for $b$. As the problem allows us to choose any $b \\ge 0$ (\"optional burn-in\"), the optimal choice is $b=0$. This choice yields the minimal squared MCSE for the compression strategy:\n$$\n\\mathrm{MCSE}_{comp,min}^2 = \\frac{\\sigma_f^2}{n} \\left(\\frac{1+\\phi}{1-\\phi}\\right)\n$$\n\n**Strategy 2: Thinning**\nIn this strategy, after a burn-in of $b$ samples, we keep every $m$-th sample, where $m \\in \\mathbb{N}$. The number of samples stored is $N_{thin} = \\lfloor (n-b)/m \\rfloor$, which we approximate as $(n-b)/m$ for large $n$. The storage budget requires $N_{thin} \\le n/5$.\n$$\n\\frac{n-b}{m} \\le \\frac{n}{5} \\implies m \\ge \\frac{5(n-b)}{n} = 5\\left(1-\\frac{b}{n}\\right)\n$$\nThe thinned sequence, $\\{f(X_{b+km})\\}_{k=1}^{N_{thin}}$, is also stationary with marginal variance $\\sigma_f^2$. Its autocorrelation function, $\\rho'_j$, corresponds to a lag of $jm$ in the original chain:\n$$\n\\rho'_j = \\rho_{jm} = \\phi^{jm} = (\\phi^m)^j\n$$\nThis is another AR($1$)-type correlation with parameter $\\phi' = \\phi^m$. The IACT for the thinned series, $\\tau'_{eff}$, is:\n$$\n\\tau'_{eff} = \\frac{1+\\phi^m}{1-\\phi^m}\n$$\nThe squared MCSE for the thinning strategy, as a function of $b$ and $m$, is:\n$$\n\\mathrm{MCSE}_{thin}^2(b,m) = \\frac{\\sigma_f^2}{N_{thin}} \\tau'_{eff} = \\frac{\\sigma_f^2}{(n-b)/m} \\left(\\frac{1+\\phi^m}{1-\\phi^m}\\right) = \\frac{\\sigma_f^2 m}{n-b} \\left(\\frac{1+\\phi^m}{1-\\phi^m}\\right)\n$$\nTo minimize this, we must choose $b \\in [0, n)$ and an integer $m \\ge 5(1-b/n)$. Let's analyze the function $g(m) = m \\left(\\frac{1+\\phi^m}{1-\\phi^m}\\right)$. The derivative of $g(m)$ with respect to $m$ (treating $m$ as a continuous variable) is $g'(m) = \\frac{1 - (\\phi^m)^2 + 2m \\phi^m \\ln(\\phi)}{(1-\\phi^m)^2}$. For $\\phi \\in (0,1)$ and $m \\ge 1$, the numerator is positive, proving that $g(m)$ is a strictly increasing function for $m \\ge 1$.\nTherefore, for any fixed $b$, to minimize $\\mathrm{MCSE}_{thin}^2$, we should choose the smallest integer $m$ that satisfies the constraint:\n$$\nm_{opt}(b) = \\left\\lceil 5\\left(1-\\frac{b}{n}\\right) \\right\\rceil\n$$\nNow we must find the value of $b \\in [0, n)$ that minimizes $\\mathrm{MCSE}_{thin}^2(b, m_{opt}(b))$. Let's analyze the term we want to minimize, apart from the constant $\\sigma_f^2$: $H(b) = \\frac{g(m_{opt}(b))}{n-b}$.\nLet's analyze $H(b)$ over intervals where $m_{opt}(b)$ is constant. For an integer $k \\in \\{1, 2, 3, 4, 5\\}$, $m_{opt}(b) = k$ when $k-1 < 5(1-b/n) \\le k$. This corresponds to the interval $b \\in [n(1-k/5), n(1-(k-1)/5))$. Within such an interval, $H(b) = \\frac{g(k)}{n-b}$, which is minimized at the smallest value of $b$ in the interval, i.e., at $b = n(1-k/5)$. At this point, $n-b = nk/5$. The minimum value of $H(b)$ in this interval is $\\frac{g(k)}{nk/5} = \\frac{5}{n} \\frac{g(k)}{k}$.\nThe minimization problem over $b$ thus reduces to minimizing $\\frac{g(k)}{k}$ over $k \\in \\{1, 2, 3, 4, 5\\}$.\n$$\n\\frac{g(k)}{k} = \\frac{1}{k} \\cdot k \\left(\\frac{1+\\phi^k}{1-\\phi^k}\\right) = \\frac{1+\\phi^k}{1-\\phi^k}\n$$\nLet $h(k) = \\frac{1+\\phi^k}{1-\\phi^k}$. The derivative is $h'(k) = \\frac{2\\phi^k \\ln(\\phi)}{(1-\\phi^k)^2} < 0$ since $\\phi \\in (0,1)$. Thus, $h(k)$ is a decreasing function of $k$. To minimize $\\frac{g(k)}{k}$, we must choose the largest possible value of $k$. The maximum value for $m_{opt}(b)=k$ is $k=5$, which occurs for $b \\in [0, n/5)$. The minimum is achieved at $b=0$.\nSo, the optimal thinning strategy is to set $b=0$ and $m=5$. The minimal squared MCSE for thinning is:\n$$\n\\mathrm{MCSE}_{thin,min}^2 = \\mathrm{MCSE}_{thin}^2(0,5) = \\frac{\\sigma_f^2 \\cdot 5}{n-0} \\left(\\frac{1+\\phi^5}{1-\\phi^5}\\right) = \\frac{5\\sigma_f^2}{n} \\left(\\frac{1+\\phi^5}{1-\\phi^5}\\right)\n$$\n\n**Comparison of Optimal Strategies**\nWe now compare the minimal squared MCSE of the two strategies:\n$$\n\\mathrm{MCSE}_{comp,min}^2 = \\frac{\\sigma_f^2}{n} \\left(\\frac{1+\\phi}{1-\\phi}\\right) \\quad \\text{vs.} \\quad \\mathrm{MCSE}_{thin,min}^2 = \\frac{\\sigma_f^2}{n} \\cdot 5 \\left(\\frac{1+\\phi^5}{1-\\phi^5}\\right)\n$$\nThe comparison reduces to comparing $\\left(\\frac{1+\\phi}{1-\\phi}\\right)$ and $5 \\left(\\frac{1+\\phi^5}{1-\\phi^5}\\right)$. This is equivalent to comparing $g(1)$ and $g(5)$. As established earlier, $g(m)$ is a strictly increasing function for $m \\ge 1$. Therefore, for $\\phi \\in (0,1)$:\n$$\ng(5) > g(1) \\implies 5 \\left(\\frac{1+\\phi^5}{1-\\phi^5}\\right) > 1 \\left(\\frac{1+\\phi}{1-\\phi}\\right)\n$$\nThis implies that $\\mathrm{MCSE}_{thin,min}^2 > \\mathrm{MCSE}_{comp,min}^2$.\n\nThe compression strategy with zero burn-in is strictly superior to the best possible thinning strategy. The minimal achievable MCSE is therefore determined by the compression strategy with $b=0$.\n\nThe minimal achievable MCSE is:\n$$\n\\mathrm{MCSE}_{min} = \\sqrt{\\mathrm{MCSE}_{comp,min}^2} = \\sqrt{\\frac{\\sigma_f^2}{n}\\left(\\frac{1+\\phi}{1-\\phi}\\right)}\n$$\nThis result confirms the general principle that, for a fixed number of MCMC iterations, thinning is suboptimal as it discards information. The storage constraint forces thinning to be aggressive (large $m$), which amplifies the variance, whereas compression allows all information from the post-burn-in chain to be used, leading to a smaller variance.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{\\sigma_f^2}{n} \\left(\\frac{1+\\phi}{1-\\phi}\\right)}}\n$$"
        }
    ]
}