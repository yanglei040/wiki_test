{
    "hands_on_practices": [
        {
            "introduction": "To effectively manage autocorrelated data, one must first master the fundamental relationship between the parameters of a stochastic process and its correlation structure. This exercise provides a rigorous, hands-on derivation of the integrated autocorrelation time and effective sample size for the widely-used ARMA(1,1) process. By working from first principles, you will build the essential analytical skills needed to dissect how a model's dynamics directly translate into sampling efficiency .",
            "id": "3304666",
            "problem": "Consider a strictly stationary autoregressive moving-average process of order one-one, denoted $X_t$, defined by the recursion $X_t = \\phi X_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1}$, where $|\\phi|  1$, $\\{\\epsilon_t\\}$ is a zero-mean independent and identically distributed white noise sequence with variance $\\sigma_{\\epsilon}^{2}$, and $\\phi,\\theta \\in \\mathbb{R}$. Let the mean of $X_t$ be zero. For a stationary process, the autocovariance function is $\\gamma_k = \\operatorname{Cov}(X_t,X_{t+k})$ and the autocorrelation function is $\\rho_k = \\gamma_k/\\gamma_0$. The integrated autocorrelation time (IAT) is defined by $\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k$. For the Monte Carlo estimator of the mean $\\bar{X}_n = \\frac{1}{n}\\sum_{t=1}^{n} X_t$, assume its variance can be written as $\\operatorname{Var}(\\bar{X}_n) = \\operatorname{Var}(X_t)\\,\\tau_{\\mathrm{int}}/n = \\operatorname{Var}(X_t)/n_{\\mathrm{eff}}$, thereby defining the effective sample size $n_{\\mathrm{eff}}$.\n\nStarting only from these definitions and the given data-generating equation, and without using any pre-tabulated formulas for autoregressive moving-average processes, derive the closed-form expression of the autocorrelation function $\\rho_k$ for all integers $k \\geq 1$. Then, use it to obtain closed-form expressions for $\\tau_{\\mathrm{int}}$ and $n_{\\mathrm{eff}}$ as functions of $\\phi$, $\\theta$, and $n$. Your final answer must be a closed-form analytic expression for both $\\tau_{\\mathrm{int}}$ and $n_{\\mathrm{eff}}$; do not substitute numerical values and do not include physical units. If any simplification is possible, present the simplified form. The final boxed answer should contain only the expressions for $\\tau_{\\mathrm{int}}$ and $n_{\\mathrm{eff}}$ (in that order), formatted as a row matrix. No rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. It presents a standard problem in time series analysis without any factual unsoundness, ambiguity, or contradiction. We can therefore proceed with the derivation.\n\nThe goal is to derive the autocorrelation function ($\\rho_k$), the integrated autocorrelation time ($\\tau_{\\mathrm{int}}$), and the effective sample size ($n_{\\mathrm{eff}}$) for the given strictly stationary ARMA($1,1$) process:\n$$\nX_t = \\phi X_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1}\n$$\nwhere $\\{\\epsilon_t\\}$ is a zero-mean white noise process with variance $\\sigma_{\\epsilon}^{2}$, and $|\\phi|  1$. The mean of the process is given as $\\operatorname{E}[X_t] = 0$.\n\nFirst, we derive the autocovariance function $\\gamma_k = \\operatorname{Cov}(X_t, X_{t+k}) = \\operatorname{E}[X_t X_{t+k}]$.\n\n**Step 1: Calculate the variance, $\\gamma_0$.**\nThe variance is $\\gamma_0 = \\operatorname{Var}(X_t) = \\operatorname{E}[X_t^2]$. Using the process definition:\n$$\n\\gamma_0 = \\operatorname{E}[(\\phi X_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1})^2]\n$$\nExpanding the square and taking the expectation of each term:\n$$\n\\gamma_0 = \\operatorname{E}[\\phi^2 X_{t-1}^2 + \\epsilon_t^2 + \\theta^2 \\epsilon_{t-1}^2 + 2\\phi X_{t-1}\\epsilon_t + 2\\phi\\theta X_{t-1}\\epsilon_{t-1} + 2\\theta \\epsilon_t\\epsilon_{t-1}]\n$$\nWe evaluate each term using the properties of the process:\n- By stationarity, $\\operatorname{E}[X_{t-1}^2] = \\gamma_0$.\n- By definition of white noise, $\\operatorname{E}[\\epsilon_t^2] = \\sigma_{\\epsilon}^2$ and $\\operatorname{E}[\\epsilon_{t-1}^2] = \\sigma_{\\epsilon}^2$.\n- $\\epsilon_t$ is independent of past values of the process, so $\\operatorname{E}[X_{t-1}\\epsilon_t] = \\operatorname{E}[X_{t-1}]\\operatorname{E}[\\epsilon_t] = 0$.\n- $\\epsilon_t$ and $\\epsilon_{t-1}$ are uncorrelated, so $\\operatorname{E}[\\epsilon_t\\epsilon_{t-1}] = 0$.\n- To evaluate $\\operatorname{E}[X_{t-1}\\epsilon_{t-1}]$, we substitute the definition of $X_{t-1}$:\n$$\n\\operatorname{E}[X_{t-1}\\epsilon_{t-1}] = \\operatorname{E}[(\\phi X_{t-2} + \\epsilon_{t-1} + \\theta \\epsilon_{t-2})\\epsilon_{t-1}] = \\phi\\operatorname{E}[X_{t-2}\\epsilon_{t-1}] + \\operatorname{E}[\\epsilon_{t-1}^2] + \\theta\\operatorname{E}[\\epsilon_{t-2}\\epsilon_{t-1}] = 0 + \\sigma_{\\epsilon}^2 + 0 = \\sigma_{\\epsilon}^2\n$$\nSubstituting these back into the expression for $\\gamma_0$:\n$$\n\\gamma_0 = \\phi^2\\gamma_0 + \\sigma_{\\epsilon}^2 + \\theta^2\\sigma_{\\epsilon}^2 + 0 + 2\\phi\\theta\\sigma_{\\epsilon}^2 + 0\n$$\nSolving for $\\gamma_0$:\n$$\n\\gamma_0(1-\\phi^2) = \\sigma_{\\epsilon}^2(1 + \\theta^2 + 2\\phi\\theta)\n$$\n$$\n\\gamma_0 = \\sigma_{\\epsilon}^2 \\frac{1 + 2\\phi\\theta + \\theta^2}{1 - \\phi^2}\n$$\n\n**Step 2: Calculate the autocovariance for lag $k=1$, $\\gamma_1$.**\n$$\n\\gamma_1 = \\operatorname{E}[X_t X_{t+1}] = \\operatorname{E}[X_t (\\phi X_t + \\epsilon_{t+1} + \\theta \\epsilon_t)]\n$$\n$$\n\\gamma_1 = \\phi\\operatorname{E}[X_t^2] + \\operatorname{E}[X_t\\epsilon_{t+1}] + \\theta\\operatorname{E}[X_t\\epsilon_t]\n$$\n- $\\operatorname{E}[X_t^2] = \\gamma_0$.\n- $\\epsilon_{t+1}$ is independent of $X_t$, so $\\operatorname{E}[X_t\\epsilon_{t+1}] = 0$.\n- Using the same logic as for $\\operatorname{E}[X_{t-1}\\epsilon_{t-1}]$, we find $\\operatorname{E}[X_t\\epsilon_t] = \\sigma_{\\epsilon}^2$.\nTherefore:\n$$\n\\gamma_1 = \\phi\\gamma_0 + \\theta\\sigma_{\\epsilon}^2\n$$\n\n**Step 3: Calculate the autocovariance for lags $k \\geq 2$, $\\gamma_k$.**\n$$\n\\gamma_k = \\operatorname{E}[X_t X_{t+k}] = \\operatorname{E}[X_t (\\phi X_{t+k-1} + \\epsilon_{t+k} + \\theta \\epsilon_{t+k-1})]\n$$\n$$\n\\gamma_k = \\phi\\operatorname{E}[X_t X_{t+k-1}] + \\operatorname{E}[X_t\\epsilon_{t+k}] + \\theta\\operatorname{E}[X_t\\epsilon_{t+k-1}]\n$$\n- $\\operatorname{E}[X_t X_{t+k-1}] = \\gamma_{k-1}$.\n- For $k \\geq 2$, both $t+k$ and $t+k-1$ are greater than $t$. Thus, $\\epsilon_{t+k}$ and $\\epsilon_{t+k-1}$ are independent of $X_t$. This implies $\\operatorname{E}[X_t\\epsilon_{t+k}] = 0$ and $\\operatorname{E}[X_t\\epsilon_{t+k-1}] = 0$.\nWe obtain the recurrence relation for $k \\geq 2$:\n$$\n\\gamma_k = \\phi\\gamma_{k-1}\n$$\n\n**Step 4: Derive the autocorrelation function, $\\rho_k$.**\nThe autocorrelation function is $\\rho_k = \\gamma_k/\\gamma_0$.\nFor $k \\geq 2$, we have $\\rho_k = \\phi\\rho_{k-1}$. By induction, this implies $\\rho_k = \\phi^{k-1}\\rho_1$ for all $k \\geq 1$.\nNow we find $\\rho_1$:\n$$\n\\rho_1 = \\frac{\\gamma_1}{\\gamma_0} = \\frac{\\phi\\gamma_0 + \\theta\\sigma_{\\epsilon}^2}{\\gamma_0} = \\phi + \\frac{\\theta\\sigma_{\\epsilon}^2}{\\gamma_0}\n$$\nSubstitute the expression for $\\gamma_0$:\n$$\n\\rho_1 = \\phi + \\frac{\\theta\\sigma_{\\epsilon}^2}{\\sigma_{\\epsilon}^2 \\frac{1 + 2\\phi\\theta + \\theta^2}{1 - \\phi^2}} = \\phi + \\frac{\\theta(1 - \\phi^2)}{1 + 2\\phi\\theta + \\theta^2}\n$$\nBringing to a common denominator:\n$$\n\\rho_1 = \\frac{\\phi(1 + 2\\phi\\theta + \\theta^2) + \\theta(1 - \\phi^2)}{1 + 2\\phi\\theta + \\theta^2} = \\frac{\\phi + 2\\phi^2\\theta + \\phi\\theta^2 + \\theta - \\theta\\phi^2}{1 + 2\\phi\\theta + \\theta^2}\n$$\n$$\n\\rho_1 = \\frac{\\phi + \\theta + \\phi^2\\theta + \\phi\\theta^2}{1 + 2\\phi\\theta + \\theta^2} = \\frac{(\\phi + \\theta)(1 + \\phi\\theta)}{1 + 2\\phi\\theta + \\theta^2}\n$$\nSo, the autocorrelation function for $k \\geq 1$ is:\n$$\n\\rho_k = \\phi^{k-1} \\left( \\frac{(\\phi + \\theta)(1 + \\phi\\theta)}{1 + 2\\phi\\theta + \\theta^2} \\right)\n$$\n\n**Step 5: Derive the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$.**\nThe definition is $\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k$.\nThe sum is a geometric series:\n$$\n\\sum_{k=1}^{\\infty} \\rho_k = \\sum_{k=1}^{\\infty} \\phi^{k-1}\\rho_1 = \\rho_1 \\sum_{j=0}^{\\infty} \\phi^j\n$$\nSince $|\\phi|1$, the series converges to $1/(1-\\phi)$.\n$$\n\\sum_{k=1}^{\\infty} \\rho_k = \\frac{\\rho_1}{1-\\phi}\n$$\nSubstituting this into the definition of $\\tau_{\\mathrm{int}}$:\n$$\n\\tau_{\\mathrm{int}} = 1 + \\frac{2\\rho_1}{1-\\phi} = 1 + \\frac{2}{1-\\phi} \\frac{(\\phi + \\theta)(1 + \\phi\\theta)}{1 + 2\\phi\\theta + \\theta^2}\n$$\nTo simplify, we combine the terms:\n$$\n\\tau_{\\mathrm{int}} = \\frac{(1-\\phi)(1 + 2\\phi\\theta + \\theta^2) + 2(\\phi + \\theta)(1 + \\phi\\theta)}{(1-\\phi)(1 + 2\\phi\\theta + \\theta^2)}\n$$\nLet's expand the numerator:\n$$\n(1 - \\phi + 2\\phi\\theta - 2\\phi^2\\theta + \\theta^2 - \\phi\\theta^2) + (2\\phi + 2\\phi^2\\theta + 2\\theta + 2\\phi\\theta^2)\n$$\n$$\n= 1 + (-\\phi+2\\phi) + (2\\phi\\theta) + (-2\\phi^2\\theta+2\\phi^2\\theta) + \\theta^2 + (-\\phi\\theta^2+2\\phi\\theta^2) + 2\\theta\n$$\n$$\n= 1 + \\phi + 2\\theta + 2\\phi\\theta + \\theta^2 + \\phi\\theta^2\n$$\nThis can be factored as:\n$$\n(1+\\phi) + 2\\theta(1+\\phi) + \\theta^2(1+\\phi) = (1+\\phi)(1+2\\theta+\\theta^2) = (1+\\phi)(1+\\theta)^2\n$$\nSo, the simplified expression for $\\tau_{\\mathrm{int}}$ is:\n$$\n\\tau_{\\mathrm{int}} = \\frac{(1+\\phi)(1+\\theta)^2}{(1-\\phi)(1 + 2\\phi\\theta + \\theta^2)}\n$$\n\n**Step 6: Derive the effective sample size, $n_{\\mathrm{eff}}$.**\nThe problem defines $n_{\\mathrm{eff}} = n / \\tau_{\\mathrm{int}}$. Substituting the expression for $\\tau_{\\mathrm{int}}$:\n$$\nn_{\\mathrm{eff}} = n \\frac{1}{\\tau_{\\mathrm{int}}} = n \\frac{(1-\\phi)(1 + 2\\phi\\theta + \\theta^2)}{(1+\\phi)(1+\\theta)^2}\n$$\nThese are the required closed-form expressions.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{(1+\\phi)(1+\\theta)^2}{(1-\\phi)(1 + 2\\phi\\theta + \\theta^2)}  n \\frac{(1-\\phi)(1+2\\phi\\theta+\\theta^2)}{(1+\\phi)(1+\\theta)^2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "After analyzing the origins of autocorrelation, the next step is to learn how to mitigate its effects. This practice demonstrates how a deep understanding of a process's structure can be used to actively improve simulation efficiency through variance reduction techniques. You will see how a cleverly designed control variate can transform a highly correlated AR(1) sequence into an uncorrelated one, providing a dramatic boost to the effective sample size and a clear illustration of theory put into practice .",
            "id": "3304651",
            "problem": "Consider a stationary Gaussian AutoRegressive of order one (AR(1)) process $\\{X_t\\}_{t \\geq 0}$ with mean zero, marginal variance $\\operatorname{Var}(X_t)=\\sigma^{2}$, and autocorrelation function $\\rho_{X}(k)=\\rho^{k}$ for lag $k \\geq 0$, where $|\\rho|1$. A Monte Carlo estimator at iteration $t$ is the scalar $X_t$. You now apply a control variate that uses the lagged value $X_{t-1}$ to construct a new estimator sequence\n$$\nY_t \\;=\\; s\\,\\big(X_t - \\lambda X_{t-1}\\big),\n$$\nwhere $s \\in \\mathbb{R}$ is chosen so that $\\operatorname{Var}(Y_t)=\\operatorname{Var}(X_t)=\\sigma^{2}$ for all choices of $\\lambda \\in \\mathbb{R}$. This control variate does not alter the marginal variance but changes the dependence structure in time by introducing correlation between successive estimates through $\\lambda$.\n\nStarting from the definitions of the autocovariance function and the integrated autocorrelation time (IAT), and using only stationarity and the AR(1) dependence structure, derive the closed-form factor by which the effective sample size (ESS) improves when $\\lambda$ is chosen equal to the AR(1) coefficient $\\rho$. Express your final answer as a single analytic function of $\\rho$, denoted $\\mathcal{F}(\\rho)=\\mathcal{E}_{\\text{new}}/\\mathcal{E}_{\\text{old}}$, where $\\mathcal{E}_{\\text{old}}$ is the ESS of the original sequence $\\{X_t\\}$ and $\\mathcal{E}_{\\text{new}}$ is the ESS of the transformed sequence $\\{Y_t\\}$ with $\\lambda=\\rho$.",
            "solution": "The problem requires the derivation of the improvement factor in the effective sample size (ESS) when a specific control variate is applied to a stationary AR(1) process. The improvement factor is defined as the ratio $\\mathcal{F}(\\rho) = \\mathcal{E}_{\\text{new}}/\\mathcal{E}_{\\text{old}}$.\n\nThe effective sample size, $\\mathcal{E}$, for a sequence of $N$ correlated samples is given by $\\mathcal{E} = N / \\tau$, where $\\tau$ is the integrated autocorrelation time (IAT). The IAT is defined in terms of the autocorrelation function (ACF), $\\rho(k)$, as:\n$$\n\\tau = 1 + 2 \\sum_{k=1}^{\\infty} \\rho(k)\n$$\nThe improvement factor can therefore be expressed as the ratio of the IATs of the original and transformed sequences:\n$$\n\\mathcal{F}(\\rho) = \\frac{\\mathcal{E}_{\\text{new}}}{\\mathcal{E}_{\\text{old}}} = \\frac{N / \\tau_{\\text{new}}}{N / \\tau_{\\text{old}}} = \\frac{\\tau_{\\text{old}}}{\\tau_{\\text{new}}}\n$$\nOur task reduces to calculating $\\tau_{\\text{old}}$ for the sequence $\\{X_t\\}$ and $\\tau_{\\text{new}}$ for the sequence $\\{Y_t\\}$.\n\nFirst, we calculate the IAT for the original AR(1) sequence, $\\{X_t\\}$. The problem states that its ACF is $\\rho_X(k) = \\rho^k$ for $k \\geq 0$. Using the definition of IAT, we have:\n$$\n\\tau_{\\text{old}} = \\tau_X = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_X(k) = 1 + 2 \\sum_{k=1}^{\\infty} \\rho^k\n$$\nThe sum is a geometric series. Since $|\\rho|  1$, the series converges:\n$$\n\\sum_{k=1}^{\\infty} \\rho^k = \\frac{\\rho}{1-\\rho}\n$$\nSubstituting this back into the expression for $\\tau_{\\text{old}}$ gives:\n$$\n\\tau_{\\text{old}} = 1 + 2 \\frac{\\rho}{1-\\rho} = \\frac{1-\\rho + 2\\rho}{1-\\rho} = \\frac{1+\\rho}{1-\\rho}\n$$\nNext, we analyze the transformed sequence $\\{Y_t\\}$ to find its IAT, $\\tau_{\\text{new}}$. The sequence is defined as $Y_t = s(X_t - \\lambda X_{t-1})$, with the specific choice $\\lambda = \\rho$.\n$$\nY_t = s(X_t - \\rho X_{t-1})\n$$\nA stationary AR(1) process $\\{X_t\\}$ with mean zero and ACF $\\rho_X(k)=\\rho^k$ is defined by the stochastic difference equation:\n$$\nX_t = \\rho X_{t-1} + \\epsilon_t\n$$\nwhere $\\{\\epsilon_t\\}$ is a sequence of uncorrelated (white noise) random variables with mean zero, $\\mathbb{E}[\\epsilon_t]=0$, and constant variance $\\operatorname{Var}(\\epsilon_t) = \\sigma_\\epsilon^2$. The variables $\\epsilon_t$ are also uncorrelated with past values of the process, i.e., $\\operatorname{Cov}(\\epsilon_t, X_{t-j})=0$ for $j0$.\n\nThe variance of $X_t$ is $\\sigma^2$. From the defining equation, we have:\n$$\n\\operatorname{Var}(X_t) = \\operatorname{Var}(\\rho X_{t-1} + \\epsilon_t) = \\rho^2 \\operatorname{Var}(X_{t-1}) + \\operatorname{Var}(\\epsilon_t)\n$$\nSince the process is stationary, $\\operatorname{Var}(X_t) = \\operatorname{Var}(X_{t-1}) = \\sigma^2$. This yields:\n$$\n\\sigma^2 = \\rho^2 \\sigma^2 + \\sigma_\\epsilon^2 \\implies \\sigma_\\epsilon^2 = \\sigma^2(1-\\rho^2)\n$$\nNow, we substitute the definition of the AR(1) process into the expression for $Y_t$:\n$$\nY_t = s(X_t - \\rho X_{t-1}) = s((\\rho X_{t-1} + \\epsilon_t) - \\rho X_{t-1}) = s \\epsilon_t\n$$\nThis reveals that the transformed sequence $\\{Y_t\\}$ is simply a scaled version of the white noise innovation process $\\{\\epsilon_t\\}$. The problem statement requires that $\\operatorname{Var}(Y_t) = \\sigma^2$. Let's verify the scaling factor $s$:\n$$\n\\operatorname{Var}(Y_t) = \\operatorname{Var}(s \\epsilon_t) = s^2 \\operatorname{Var}(\\epsilon_t) = s^2 \\sigma^2 (1-\\rho^2)\n$$\nSetting $\\operatorname{Var}(Y_t) = \\sigma^2$, we get $s^2 \\sigma^2 (1-\\rho^2) = \\sigma^2$, which implies $s^2 = \\frac{1}{1-\\rho^2}$. This confirms the existence of such a scaling factor $s$.\n\nTo compute $\\tau_{\\text{new}}$, we need the ACF of the $\\{Y_t\\}$ sequence, $\\rho_Y(k)$. The ACF is defined as $\\rho_Y(k) = \\frac{\\operatorname{Cov}(Y_t, Y_{t-k})}{\\operatorname{Var}(Y_t)}$. We know a priori that $\\operatorname{Var}(Y_t)=\\sigma^2$. The autocovariance for lag $k \\geq 1$ is:\n$$\n\\operatorname{Cov}(Y_t, Y_{t-k}) = \\operatorname{Cov}(s \\epsilon_t, s \\epsilon_{t-k}) = s^2 \\operatorname{Cov}(\\epsilon_t, \\epsilon_{t-k})\n$$\nSince $\\{\\epsilon_t\\}$ is a white noise process, its elements are uncorrelated in time. Thus, for any $k \\geq 1$:\n$$\n\\operatorname{Cov}(\\epsilon_t, \\epsilon_{t-k}) = 0\n$$\nThis implies that for all $k \\ge 1$, $\\operatorname{Cov}(Y_t, Y_{t-k})=0$. The ACF of the $\\{Y_t\\}$ sequence is therefore:\n$$\n\\rho_Y(k) = \\frac{0}{\\sigma^2} = 0 \\quad \\text{for all } k \\geq 1\n$$\nThe $\\{Y_t\\}$ sequence is uncorrelated. Now we can compute its IAT, $\\tau_{\\text{new}}$:\n$$\n\\tau_{\\text{new}} = \\tau_Y = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_Y(k) = 1 + 2 \\sum_{k=1}^{\\infty} 0 = 1\n$$\nThe IAT of the new sequence is $1$, which is the theoretical minimum, corresponding to a sequence of independent samples.\n\nFinally, we compute the improvement factor $\\mathcal{F}(\\rho)$:\n$$\n\\mathcal{F}(\\rho) = \\frac{\\tau_{\\text{old}}}{\\tau_{\\text{new}}} = \\frac{\\frac{1+\\rho}{1-\\rho}}{1} = \\frac{1+\\rho}{1-\\rho}\n$$\nThis is the closed-form factor by which the effective sample size improves. By transforming the correlated sequence $\\{X_t\\}$ into the uncorrelated sequence $\\{Y_t\\}$, the control variate technique with $\\lambda=\\rho$ has maximized the effective sample size for a fixed number of samples $N$.",
            "answer": "$$\n\\boxed{\\frac{1+\\rho}{1-\\rho}}\n$$"
        },
        {
            "introduction": "The ultimate goal for any practitioner is to achieve maximum efficiency not just statistically, but also computationally. This final exercise elevates the concept of ESS to a real-world optimization problem, where statistical gains are weighed against wall-clock time. By modeling both the decorrelation effects of algorithmic parameters and the costs of parallel hardware execution, you are challenged to find the optimal configuration that maximizes the most crucial metric: effective samples generated per second .",
            "id": "3304672",
            "problem": "Consider a class of Markov Chain Monte Carlo (MCMC) algorithms that produce autocorrelated samples. For a given algorithm configuration, define a parameter vector $\\theta$ consisting of an algorithmic step size $\\varepsilon$ and the number of parallel chains $p$ executed concurrently on computing hardware. Assume that each chain is stationary with mean $0$ and variance $\\sigma^2$, and that its lag-$k$ autocorrelation is denoted by $r(k)$. Further suppose that the per-chain dynamics can be well approximated by a first-order autoregressive process, so that there exists a correlation parameter $\\rho(\\varepsilon)$ satisfying $r(k) = \\rho(\\varepsilon)^k$ for all integer $k \\ge 0$, with $0  \\rho(\\varepsilon)  1$.\n\nDefine the total number of iterations per chain as $N$, and consider running $p$ independent chains in parallel so that the total number of samples produced per global iteration (one step per chain) is $p$. The hardware executes these $p$ chain updates concurrently with imperfect scaling. Model the per-chain computation time as an increasing function of step size\n$$\nt_{\\text{chain}}(\\varepsilon) = t_0 + t_1 \\varepsilon^2,\n$$\nwhere $t_0$ and $t_1$ are positive constants measured in seconds. Model the parallel speedup via an Amdahl-type expression\n$$\nS(p) = \\frac{p}{1 + \\kappa (p - 1)},\n$$\nwhere $\\kappa  0$ quantifies the fraction of non-parallelizable work. Model additional overhead as\n$$\n\\text{overhead}(p) = \\eta (p - 1)^2,\n$$\nwith $\\eta  0$ measured in seconds per global iteration. The total wall-time cost per global iteration (one step taken on every chain in parallel) is then\n$$\nc(\\varepsilon,p) = \\frac{t_{\\text{chain}}(\\varepsilon)\\, p}{S(p)} + \\text{overhead}(p),\n$$\nexpressed in seconds.\n\nStarting from first principles and core definitions of autocovariance and autocorrelation in time series, derive the effective sample size (ESS) achieved by one chain after $N$ iterations as a function of $\\rho(\\varepsilon)$, and extend it to $p$ independent chains. The derivation must begin from the definition of the variance of the sample mean in terms of the autocovariance function and the autocorrelation function, and proceed logically to identify the integrated autocorrelation time and the resulting effective sample size. Use that derivation to construct a program that, for each test case below, evaluates the objective\n$$\n\\max_{\\varepsilon \\in \\mathcal{E},\\, p \\in \\mathcal{P}} \\frac{\\text{ESS}(\\varepsilon,p)}{c(\\varepsilon,p)},\n$$\nwhere $\\mathcal{E}$ is a finite set of candidate step sizes and $\\mathcal{P}$ is a finite set of candidate thread counts. The parameterization of $\\rho(\\varepsilon)$ is given by\n$$\n\\rho(\\varepsilon) = \\exp(-\\alpha \\varepsilon),\n$$\nwith $\\alpha  0$ provided per test case.\n\nYour program must:\n- Implement the derivation-based formula for $\\text{ESS}(\\varepsilon,p)$ using the autocorrelation structure $r(k) = \\rho(\\varepsilon)^k$.\n- Evaluate $\\text{ESS}(\\varepsilon,p) / c(\\varepsilon,p)$ for every pair $(\\varepsilon,p)$ in the provided test suite.\n- Select the maximizer $(\\varepsilon^\\star, p^\\star)$ that attains the largest ratio. If multiple pairs attain the same maximum value (to within numerical tolerance of $10^{-12}$), select the one with the smallest $p$; if still tied, select the one with the smallest $\\varepsilon$.\n- Produce as output, for each test case, a list containing $[\\varepsilon^\\star, p^\\star, \\text{ESS-per-second}^\\star]$, where $\\text{ESS-per-second}^\\star = \\text{ESS}(\\varepsilon^\\star,p^\\star) / c(\\varepsilon^\\star,p^\\star)$ is expressed in samples per second. Report $\\varepsilon^\\star$ rounded to $3$ decimal places and $\\text{ESS-per-second}^\\star$ rounded to $6$ decimal places. The thread count $p^\\star$ is an integer.\n\nPhysical and numerical units:\n- Time quantities $t_0$, $t_1$, and $\\eta$ are in seconds.\n- The objective ratio $\\text{ESS}(\\varepsilon,p) / c(\\varepsilon,p)$ must be reported in samples per second (a float).\n\nAngle units do not apply. No percentages appear in the final answer.\n\nTest suite:\n- Case $1$ (happy path):\n    - $N = 10000$, $\\alpha = 3.0$, $t_0 = 1\\times 10^{-4}$, $t_1 = 5\\times 10^{-4}$, $\\kappa = 0.15$, $\\eta = 2\\times 10^{-5}$,\n    - $\\mathcal{E} = \\{0.02, 0.05, 0.10, 0.20, 0.40\\}$,\n    - $\\mathcal{P} = \\{1, 2, 4, 8, 16\\}$.\n- Case $2$ (limited scaling and high overhead):\n    - $N = 10000$, $\\alpha = 1.0$, $t_0 = 5\\times 10^{-4}$, $t_1 = 1\\times 10^{-4}$, $\\kappa = 0.5$, $\\eta = 1\\times 10^{-4}$,\n    - $\\mathcal{E} = \\{0.01, 0.03, 0.05, 0.10, 0.20, 0.30\\}$,\n    - $\\mathcal{P} = \\{1, 2, 3, 4, 6, 8\\}$.\n- Case $3$ (near-ideal scaling and fast decorrelation):\n    - $N = 20000$, $\\alpha = 5.0$, $t_0 = 1\\times 10^{-4}$, $t_1 = 1\\times 10^{-4}$, $\\kappa = 0.05$, $\\eta = 5\\times 10^{-6}$,\n    - $\\mathcal{E} = \\{0.05, 0.10, 0.20, 0.40, 0.80\\}$,\n    - $\\mathcal{P} = \\{1, 2, 4, 8, 16, 32\\}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no extra spaces. For example, the output should look like\n$$\n[\\,[\\varepsilon_1^\\star,p_1^\\star,\\text{ESSperSec}_1^\\star],\\,[\\varepsilon_2^\\star,p_2^\\star,\\text{ESSperSec}_2^\\star],\\,[\\varepsilon_3^\\star,p_3^\\star,\\text{ESSperSec}_3^\\star]\\,],\n$$\nwhere every inner list corresponds to one test case, ordered as Case $1$, Case $2$, Case $3$.",
            "solution": "**Problem Validation**\n\nThe problem statement has been evaluated and is determined to be **valid**. It is scientifically grounded in the principles of Markov Chain Monte Carlo (MCMC) methods, statistical time series analysis, and computational performance modeling. The problem is well-posed, with a clearly defined objective function to be maximized over a finite set of parameters, and includes a specific tie-breaking rule to ensure a unique solution. All necessary parameters and models are explicitly provided, and the problem is free from ambiguity, contradiction, or subjective content.\n\n**Derivation and Solution Methodology**\n\nThe objective is to maximize the ratio of effective sample size (ESS) to computational cost. We first derive the formula for ESS from first principles as required.\n\nLet $\\{X_i\\}_{i=1}^N$ be a sequence of $N$ samples from a single stationary Markov chain with mean $\\mathbb{E}[X_i] = 0$ and variance $\\text{Var}(X_i) = \\sigma^2$. The sample mean is $\\hat{\\mu}_N = \\frac{1}{N}\\sum_{i=1}^N X_i$.\n\nThe variance of the sample mean is given by:\n$$ \\text{Var}(\\hat{\\mu}_N) = \\text{Var}\\left(\\frac{1}{N}\\sum_{i=1}^N X_i\\right) = \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j=1}^N \\text{Cov}(X_i, X_j) $$\nFor a stationary process, the autocovariance $\\gamma(k) = \\text{Cov}(X_i, X_{i+k})$ depends only on the lag $k$. Also, $\\gamma(k) = \\gamma(-k)$ and $\\gamma(0) = \\sigma^2$. The double summation can be re-indexed by $k=j-i$:\n$$ \\text{Var}(\\hat{\\mu}_N) = \\frac{1}{N^2} \\sum_{k=-(N-1)}^{N-1} (N-|k|) \\gamma(k) $$\nUsing the symmetry of $\\gamma(k)$ and expressing it in terms of the autocorrelation function $r(k) = \\gamma(k)/\\gamma(0)$, we have:\n$$ \\text{Var}(\\hat{\\mu}_N) = \\frac{\\sigma^2}{N} \\left[ 1 + 2 \\sum_{k=1}^{N-1} \\left(1-\\frac{k}{N}\\right) r(k) \\right] $$\nFor a number of samples $N$ much larger than the correlation length of the chain, this expression is well approximated by replacing the finite sum with an infinite one and dropping the $(1-k/N)$ term:\n$$ \\text{Var}(\\hat{\\mu}_N) \\approx \\frac{\\sigma^2}{N} \\left[ 1 + 2 \\sum_{k=1}^{\\infty} r(k) \\right] $$\nThe effective sample size, $N_{\\text{eff}}$, is defined as the size of an i.i.d. sample that would yield the same variance in its sample mean, i.e., $\\text{Var}(\\hat{\\mu}_{N_{\\text{eff}}}^{\\text{iid}}) = \\sigma^2/N_{\\text{eff}}$. Equating this with the variance for the autocorrelated chain gives:\n$$ \\frac{\\sigma^2}{N_{\\text{eff}}} = \\frac{\\sigma^2}{N} \\left[ 1 + 2 \\sum_{k=1}^{\\infty} r(k) \\right] $$\nThis gives the definition of ESS for a single chain:\n$$ N_{\\text{eff}} = \\frac{N}{1 + 2 \\sum_{k=1}^{\\infty} r(k)} = \\frac{N}{\\tau} $$\nwhere $\\tau = 1 + 2 \\sum_{k=1}^{\\infty} r(k)$ is the integrated autocorrelation time (IACT).\n\nThe problem specifies an AR(1) process for the chain's autocorrelation structure, $r(k) = \\rho(\\varepsilon)^k$ for $k \\ge 0$, where $\\rho(\\varepsilon)$ is the lag-$1$ autocorrelation which depends on the step size $\\varepsilon$. Given $0  \\rho(\\varepsilon)  1$, the infinite sum is a convergent geometric series:\n$$ \\sum_{k=1}^{\\infty} \\rho(\\varepsilon)^k = \\frac{\\rho(\\varepsilon)}{1-\\rho(\\varepsilon)} $$\nSubstituting this into the expression for $\\tau$:\n$$ \\tau(\\varepsilon) = 1 + 2 \\frac{\\rho(\\varepsilon)}{1-\\rho(\\varepsilon)} = \\frac{1-\\rho(\\varepsilon) + 2\\rho(\\varepsilon)}{1-\\rho(\\varepsilon)} = \\frac{1+\\rho(\\varepsilon)}{1-\\rho(\\varepsilon)} $$\nThe ESS for a single chain of length $N$ is therefore:\n$$ \\text{ESS}_{\\text{single}}(N, \\varepsilon) = \\frac{N}{\\tau(\\varepsilon)} = N \\frac{1-\\rho(\\varepsilon)}{1+\\rho(\\varepsilon)} $$\nSince we are running $p$ independent chains, the total ESS is the sum of the ESS from each chain. As all chains are assumed to have identical statistical properties, the total ESS is:\n$$ \\text{ESS}(\\varepsilon, p) = p \\times \\text{ESS}_{\\text{single}}(N, \\varepsilon) = p N \\frac{1-\\rho(\\varepsilon)}{1+\\rho(\\varepsilon)} $$\n\nThe objective is to maximize the ratio of total ESS to the computational cost per global iteration, denoted $c(\\varepsilon,p)$. The problem states that maximizing $\\text{ESS}(\\varepsilon,p) / c(\\varepsilon,p)$ is the goal. Although $\\text{ESS}$ is the total over $N$ iterations and $c$ is the cost per iteration, maximizing this quantity is equivalent to maximizing the rate of ESS generation, as the factor $N$ is constant for a given test case.\n\nThe full objective function to be maximized is:\n$$ f(\\varepsilon, p) = \\frac{\\text{ESS}(\\varepsilon,p)}{c(\\varepsilon,p)} $$\nwhere the components are given by:\n1.  **Correlation parameter**: $\\rho(\\varepsilon) = \\exp(-\\alpha \\varepsilon)$\n2.  **Effective Sample Size**: $\\text{ESS}(\\varepsilon, p) = p N \\frac{1 - \\exp(-\\alpha \\varepsilon)}{1 + \\exp(-\\alpha \\varepsilon)}$\n3.  **Cost per global iteration**: $c(\\varepsilon,p) = \\frac{t_{\\text{chain}}(\\varepsilon)\\, p}{S(p)} + \\text{overhead}(p)$, which simplifies to:\n    $$ c(\\varepsilon, p) = (t_0 + t_1 \\varepsilon^2) (1 + \\kappa(p-1)) + \\eta(p-1)^2 $$\n\nThe program must find the pair $(\\varepsilon^\\star, p^\\star)$ that maximizes $f(\\varepsilon, p)$ for $\\varepsilon \\in \\mathcal{E}$ and $p \\in \\mathcal{P}$, where $\\mathcal{E}$ and $\\mathcal{P}$ are the finite sets of step sizes and thread counts provided for each test case.\n\nThe optimization is performed by a grid search over all pairs $(\\varepsilon, p)$ in the Cartesian product $\\mathcal{E} \\times \\mathcal{P}$. For each pair, we compute the objective function $f(\\varepsilon, p)$. The candidate sets $\\mathcal{E}$ and $\\mathcal{P}$ are sorted in ascending order before commencing the search. We maintain the current best-found parameters $(\\varepsilon^\\star, p^\\star)$ and the corresponding maximum value $f^\\star$. A new pair $(\\varepsilon,p)$ replaces the current best if its objective value is strictly greater than $f^\\star$ (by more than a numerical tolerance of $10^{-12}$). Due to the sorted iteration order, the first time a maximum value is found, it will be with the smallest possible $p$, and for that $p$, the smallest possible $\\varepsilon$. Any subsequent pair that achieves a tied value will have a larger $p$ or $\\varepsilon$ and will thus be correctly ignored, satisfying the tie-breaking rule.\n\nThe final output for each test case is a list containing the optimal step size $\\varepsilon^\\star$ (rounded to $3$ decimal places), the optimal number of chains $p^\\star$ (as an integer), and the maximal objective value $f(\\varepsilon^\\star, p^\\star)$ (rounded to $6$ decimal places).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the MCMC optimization problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"N\": 10000, \"alpha\": 3.0, \"t0\": 1e-4, \"t1\": 5e-4, \"kappa\": 0.15, \"eta\": 2e-5,\n            \"E_set\": [0.02, 0.05, 0.10, 0.20, 0.40],\n            \"P_set\": [1, 2, 4, 8, 16]\n        },\n        # Case 2 (limited scaling and high overhead)\n        {\n            \"N\": 10000, \"alpha\": 1.0, \"t0\": 5e-4, \"t1\": 1e-4, \"kappa\": 0.5, \"eta\": 1e-4,\n            \"E_set\": [0.01, 0.03, 0.05, 0.10, 0.20, 0.30],\n            \"P_set\": [1, 2, 3, 4, 6, 8]\n        },\n        # Case 3 (near-ideal scaling and fast decorrelation)\n        {\n            \"N\": 20000, \"alpha\": 5.0, \"t0\": 1e-4, \"t1\": 1e-4, \"kappa\": 0.05, \"eta\": 5e-6,\n            \"E_set\": [0.05, 0.10, 0.20, 0.40, 0.80],\n            \"P_set\": [1, 2, 4, 8, 16, 32]\n        }\n    ]\n\n    results = []\n    \n    TOL = 1e-12\n\n    for case in test_cases:\n        N = case[\"N\"]\n        alpha = case[\"alpha\"]\n        t0 = case[\"t0\"]\n        t1 = case[\"t1\"]\n        kappa = case[\"kappa\"]\n        eta = case[\"eta\"]\n        E_set = sorted(case[\"E_set\"])\n        P_set = sorted(case[\"P_set\"])\n\n        best_eps_star = -1.0\n        best_p_star = -1\n        max_ratio = -1.0\n\n        for p in P_set:\n            for eps in E_set:\n                # Calculate the correlation parameter rho\n                rho = np.exp(-alpha * eps)\n                \n                # Calculate the total Effective Sample Size (ESS)\n                # ESS is calculated for p independent chains, each of length N\n                if abs(1.0 + rho) > 1e-15: # Avoid division by zero\n                    ess_factor = (1.0 - rho) / (1.0 + rho)\n                    ess = p * N * ess_factor\n                else: \n                    ess = 0.0\n\n                # Calculate the total wall-time cost per global iteration, c(eps, p)\n                cost_per_iter = (t0 + t1 * eps**2) * (1.0 + kappa * (p - 1.0)) + eta * (p - 1.0)**2\n                \n                # The objective is ESS / cost per *total run*. Total cost is cost_per_iter * N.\n                # However, since N is constant for the optimization, we can maximize ESS/cost_per_iter.\n                # This gives ESS rate (ESS per second).\n                # But the problem asks for ESS(eps,p) / c(eps,p), which is ESS per iter / cost per iter.\n                # Let's clarify: The objective is ESS_total / T_total.\n                # T_total = c(eps, p) * N.\n                # Objective = (p * N * ess_factor) / (c(eps, p) * N) = (p * ess_factor) / c(eps, p).\n                # The problem statement has ESS(eps,p)/c(eps,p) where ESS(eps,p) is total ESS.\n                # Let's follow the problem as written. It asks to maximize ESS(eps,p)/c(eps,p).\n                # The units are samples / (seconds/iteration) = samples-iterations / second. This is odd.\n                # A more logical objective is ESS-per-second = ESS_total / Total_time = ESS / (c * N).\n                # Let's re-read the problem: \"evaluates the objective max ... ESS(eps,p)/c(eps,p)\".\n                # The output name is \"ESS-per-second\". This implies the objective *is* rate.\n                # Let's assume the intent is rate, which is ESS_total / Total_Time.\n                # Total_Time = c(eps, p) * N.\n                # Rate = ESS / (c * N) = (p * N * ess_factor) / (c * N) = p * ess_factor / c.\n                ess_rate_factor = p * ess_factor\n                \n                if cost_per_iter > 1e-15:\n                    current_ratio = ess_rate_factor / cost_per_iter\n                else:\n                    current_ratio = 0.0\n\n                # Apply maximization and tie-breaking rules.\n                if current_ratio > max_ratio + TOL:\n                    max_ratio = current_ratio\n                    best_eps_star = eps\n                    best_p_star = p\n        \n        # Format results as specified for the final output\n        formatted_eps = round(best_eps_star, 3)\n        formatted_ratio = round(max_ratio, 6)\n        \n        results.append([formatted_eps, best_p_star, formatted_ratio])\n\n    # Construct the final output string exactly as specified.\n    inner_lists = [f\"[{e},{p},{r}]\" for e, p, r in results]\n    print(f\"[{','.join(inner_lists)}]\")\n\nsolve()\n```"
        }
    ]
}