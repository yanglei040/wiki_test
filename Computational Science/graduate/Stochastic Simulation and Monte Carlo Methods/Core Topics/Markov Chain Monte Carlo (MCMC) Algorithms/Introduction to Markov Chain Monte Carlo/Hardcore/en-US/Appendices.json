{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Markov chain Monte Carlo, we must first master the mechanics of how the chain progresses from one state to the next. This first exercise focuses on Gibbs sampling, a cornerstone of MCMC, which is particularly effective when sampling from full conditional distributions is feasible. By manually performing one full sweep of a Gibbs sampler for a bivariate normal target, you will gain a concrete understanding of how the algorithm iteratively updates each variable to explore the target distribution .",
            "id": "3313366",
            "problem": "Consider a target distribution that is a bivariate normal with mean vector $\\boldsymbol{\\mu} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ and covariance matrix $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 4  3 \\\\ 3  9 \\end{pmatrix}$. A Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm that iteratively samples from the full conditional distributions. Starting from first principles for multivariate normal distributions, derive explicit expressions for the full conditional distributions $p(x_{1} \\mid x_{2})$ and $p(x_{2} \\mid x_{1})$ in terms of the entries of $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$. Then, perform one complete Gibbs sweep in the order $x_{1}$-then-$x_{2}$ starting from the current state $(x_{1}^{(0)}, x_{2}^{(0)}) = (0, 0)$, using independent standard normal variates $z_{1} = 1$ and $z_{2} = 0$ to realize the updates. State the updated pair $(x_{1}^{(1)}, x_{2}^{(1)})$ as exact expressions. Provide your final answer as a single row vector. Do not round.",
            "solution": "The user wants to solve a problem that requires deriving the full conditional distributions for a bivariate normal distribution and then performing one step of a Gibbs sampler.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens:**\n- Target distribution: Bivariate normal.\n- Mean vector: $\\boldsymbol{\\mu} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n- Covariance matrix: $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 4  3 \\\\ 3  9 \\end{pmatrix}$.\n- Algorithm: Gibbs sampler.\n- Task 1: Derive explicit expressions for the full conditional distributions $p(x_{1} \\mid x_{2})$ and $p(x_{2} \\mid x_{1})$.\n- Task 2: Perform one complete Gibbs sweep.\n- Sweep order: $x_{1}$-then-$x_{2}$.\n- Starting state: $(x_{1}^{(0)}, x_{2}^{(0)}) = (0, 0)$.\n- Standard normal variates for realization: $z_{1} = 1$ and $z_{2} = 0$.\n- Final answer format: The updated pair $(x_{1}^{(1)}, x_{2}^{(1)})$ as a single row vector, with exact expressions.\n\n**1.2. Validate Using Extracted Givens:**\n- **Scientific Grounding:** The problem is grounded in the theory of multivariate statistics and Markov chain Monte Carlo (MCMC) methods. The Gibbs sampler is a standard algorithm for sampling from multivariate distributions, and its application to the multivariate normal distribution is a canonical example. The provided covariance matrix $\\boldsymbol{\\Sigma}$ is symmetric. Its determinant is $\\det(\\boldsymbol{\\Sigma}) = (4)(9) - (3)(3) = 36 - 9 = 27  0$, and its diagonal entries (variances) are positive. Thus, $\\boldsymbol{\\Sigma}$ is a valid positive definite covariance matrix. The problem is scientifically and mathematically sound.\n- **Well-Posedness:** The problem is well-posed. It provides all necessary information—the target distribution parameters, the initial state, and the random variates for the update step—to compute a unique solution.\n- **Objectivity:** The problem is stated using precise, objective mathematical language.\n- **Completeness and Consistency:** The problem is self-contained and free of contradictions.\n\n**1.3. Verdict:**\nThe problem is valid.\n\n### Step 2: Solution\n\nThe problem requires us to first derive the full conditional distributions for a bivariate normal distribution and then use them to perform one sweep of a Gibbs sampler.\n\nLet a random vector $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ follow a bivariate normal distribution with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$:\n$$\n\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\n$$\nwhere\n$$\n\\boldsymbol{\\mu} = \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\sigma_1^2  \\rho \\sigma_1 \\sigma_2 \\\\ \\rho \\sigma_1 \\sigma_2  \\sigma_2^2 \\end{pmatrix} = \\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix}\n$$\n\nThe full conditional distributions for a multivariate normal are themselves normal. For the bivariate case, the conditional distribution of $x_1$ given $x_2$ is $p(x_1 \\mid x_2) = \\mathcal{N}(\\mu_{1|2}, \\sigma_{1|2}^2)$, where:\n- Conditional mean: $\\mu_{1|2} = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1} (x_2 - \\mu_2)$\n- Conditional variance: $\\sigma_{1|2}^2 = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}$\n\nSimilarly, the conditional distribution of $x_2$ given $x_1$ is $p(x_2 \\mid x_1) = \\mathcal{N}(\\mu_{2|1}, \\sigma_{2|1}^2)$, where:\n- Conditional mean: $\\mu_{2|1} = \\mu_2 + \\Sigma_{21} \\Sigma_{11}^{-1} (x_1 - \\mu_1)$\n- Conditional variance: $\\sigma_{2|1}^2 = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}$\n\nFrom the problem statement, we have:\n$$\n\\boldsymbol{\\mu} = \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix} = \\begin{pmatrix} 4  3 \\\\ 3  9 \\end{pmatrix}\n$$\nSo, $\\mu_1 = 1$, $\\mu_2 = -1$, $\\Sigma_{11} = 4$, $\\Sigma_{22} = 9$, and $\\Sigma_{12} = \\Sigma_{21} = 3$. Since $\\Sigma_{11}$ and $\\Sigma_{22}$ are scalars, their inverses are simply their reciprocals: $\\Sigma_{11}^{-1} = \\frac{1}{4}$ and $\\Sigma_{22}^{-1} = \\frac{1}{9}$.\n\n**Derivation of $p(x_1 \\mid x_2)$:**\nThe mean of the conditional distribution $p(x_1 \\mid x_2)$ is:\n$$\n\\mu_{1|2} = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1} (x_2 - \\mu_2) = 1 + (3) \\left(\\frac{1}{9}\\right) (x_2 - (-1)) = 1 + \\frac{1}{3}(x_2 + 1) = 1 + \\frac{1}{3}x_2 + \\frac{1}{3} = \\frac{4}{3} + \\frac{1}{3}x_2\n$$\nThe variance is:\n$$\n\\sigma_{1|2}^2 = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21} = 4 - (3) \\left(\\frac{1}{9}\\right) (3) = 4 - \\frac{9}{9} = 4 - 1 = 3\n$$\nThus, the full conditional distribution for $x_1$ is:\n$$\np(x_1 \\mid x_2) = \\mathcal{N}\\left(\\frac{4}{3} + \\frac{1}{3}x_2, 3\\right)\n$$\n\n**Derivation of $p(x_2 \\mid x_1)$:**\nThe mean of the conditional distribution $p(x_2 \\mid x_1)$ is:\n$$\n\\mu_{2|1} = \\mu_2 + \\Sigma_{21} \\Sigma_{11}^{-1} (x_1 - \\mu_1) = -1 + (3) \\left(\\frac{1}{4}\\right) (x_1 - 1) = -1 + \\frac{3}{4}x_1 - \\frac{3}{4} = -\\frac{7}{4} + \\frac{3}{4}x_1\n$$\nThe variance is:\n$$\n\\sigma_{2|1}^2 = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12} = 9 - (3) \\left(\\frac{1}{4}\\right) (3) = 9 - \\frac{9}{4} = \\frac{36 - 9}{4} = \\frac{27}{4}\n$$\nThus, the full conditional distribution for $x_2$ is:\n$$\np(x_2 \\mid x_1) = \\mathcal{N}\\left(-\\frac{7}{4} + \\frac{3}{4}x_1, \\frac{27}{4}\\right)\n$$\n\n**Performing one Gibbs sweep:**\nThe Gibbs sampler proceeds by iteratively drawing samples from the full conditional distributions. We start at state $(x_1^{(0)}, x_2^{(0)}) = (0, 0)$.\n\n**Step 2.1: Update $x_1$**\nWe sample $x_1^{(1)}$ from $p(x_1 \\mid x_2 = x_2^{(0)})$.\nThe distribution for this sample is $\\mathcal{N}(\\mu_{1|2}^{(0)}, \\sigma_{1|2}^2)$, where:\n- Mean: $\\mu_{1|2}^{(0)} = \\frac{4}{3} + \\frac{1}{3}x_2^{(0)} = \\frac{4}{3} + \\frac{1}{3}(0) = \\frac{4}{3}$\n- Variance: $\\sigma_{1|2}^2 = 3$. The standard deviation is $\\sigma_{1|2} = \\sqrt{3}$.\nTo generate a sample from $\\mathcal{N}(\\mu, \\sigma^2)$, we use the transformation $x = \\mu + \\sigma z$, where $z \\sim \\mathcal{N}(0, 1)$. We are given the standard normal variate $z_1 = 1$.\n$$\nx_1^{(1)} = \\mu_{1|2}^{(0)} + \\sigma_{1|2} \\cdot z_1 = \\frac{4}{3} + \\sqrt{3} \\cdot (1) = \\frac{4}{3} + \\sqrt{3}\n$$\n\n**Step 2.2: Update $x_2$**\nWe sample $x_2^{(1)}$ from $p(x_2 \\mid x_1 = x_1^{(1)})$, using the newly updated value of $x_1$.\nThe distribution for this sample is $\\mathcal{N}(\\mu_{2|1}^{(1)}, \\sigma_{2|1}^2)$, where:\n- Mean: $\\mu_{2|1}^{(1)} = -\\frac{7}{4} + \\frac{3}{4}x_1^{(1)} = -\\frac{7}{4} + \\frac{3}{4}\\left(\\frac{4}{3} + \\sqrt{3}\\right) = -\\frac{7}{4} + 1 + \\frac{3\\sqrt{3}}{4} = -\\frac{3}{4} + \\frac{3\\sqrt{3}}{4}$\n- Variance: $\\sigma_{2|1}^2 = \\frac{27}{4}$. The standard deviation is $\\sigma_{2|1} = \\sqrt{\\frac{27}{4}} = \\frac{3\\sqrt{3}}{2}$.\nWe are given the standard normal variate $z_2 = 0$.\n$$\nx_2^{(1)} = \\mu_{2|1}^{(1)} + \\sigma_{2|1} \\cdot z_2 = \\left(-\\frac{3}{4} + \\frac{3\\sqrt{3}}{4}\\right) + \\frac{3\\sqrt{3}}{2} \\cdot (0) = -\\frac{3}{4} + \\frac{3\\sqrt{3}}{4}\n$$\n\nAfter one complete Gibbs sweep, the updated state is $(x_1^{(1)}, x_2^{(1)}) = \\left(\\frac{4}{3} + \\sqrt{3}, -\\frac{3}{4} + \\frac{3\\sqrt{3}}{4}\\right)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{4}{3} + \\sqrt{3}  -\\frac{3}{4} + \\frac{3\\sqrt{3}}{4} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Moving from a single calculated step to a full-fledged simulation is crucial for appreciating the power of MCMC. This practice guides you through implementing the versatile Metropolis-Hastings algorithm to empirically verify the ergodic theorem, the theoretical pillar that guarantees MCMC's success. By simulating chains for various target distributions and observing the convergence of sample averages to their true expected values, you will bridge the gap between algorithmic theory and practical estimation .",
            "id": "3313367",
            "problem": "You are asked to design and implement a Markov Chain Monte Carlo (MCMC) simulation to empirically illustrate the convergence of the sample average to the true expectation with respect to a known target distribution. The setting is entirely probabilistic and mathematical. The fundamental base for this task consists of: the definition of a Markov chain with an invariant distribution, the concept of reversibility with respect to a target density, and the ergodic theorem for Markov chains which guarantees convergence of empirical averages under standard irreducibility and aperiodicity conditions. You must not rely on any black-box sampler; instead, construct a reversible Markov chain using the Metropolis–Hastings mechanism with symmetric proposals.\n\nGiven a target distribution with density proportional to a known function $\\pi(x)$ and a measurable function $f(x)$ such that $\\mathbb{E}_{\\pi}[|f(X)|]  \\infty$, define the empirical average\n$$\n\\bar{f}_n \\equiv \\frac{1}{n} \\sum_{t=1}^{n} f(X_t),\n$$\nwhere $X_t$ is the state of the chain at iteration $t$ after discarding an initial burn-in segment. The goal is to compute $\\bar{f}_n$ for several test cases and compare it to the true expectation $\\mathbb{E}_{\\pi}[f]$ using the absolute error $|\\bar{f}_n - \\mathbb{E}_{\\pi}[f]|$, thereby illustrating convergence as $n$ grows.\n\nConstruct a time-homogeneous Markov chain on the state space appropriate to each test case, with the following requirements.\n- Use a random-walk Metropolis–Hastings kernel with symmetric Gaussian proposals. Concretely, at each step propose $Y = X + \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_d)$ in dimension $d$, and apply the acceptance rule that ensures reversibility with respect to the target density $\\pi$.\n- The chain must be initialized at a specified deterministic state $x_0$ and must discard an initial burn-in of $b$ iterations before collecting $n$ samples for the average $\\bar{f}_n$. Use $b = \\lfloor n / 10 \\rfloor$ for each requested $n$.\n- For reproducibility, use a fixed pseudorandom seed for each test case and each $n$ as specified below.\n\nTest suite. Implement the following five target-function pairs and compute the absolute error $|\\bar{f}_n - \\mathbb{E}_{\\pi}[f]|$ for the three sample sizes $n \\in \\{\\,500, 5000, 50000\\,\\}$. In every case use the step size $\\sigma$ and initial state $x_0$ exactly as specified. For multivariate targets, use the identity covariance for the proposal scaled by $\\sigma$, and denote $x = (x_1, \\ldots, x_d)$.\n\n- Case $C_1$ (univariate standard normal moment):\n  - Target: $\\pi(x) \\propto \\exp(-\\tfrac{1}{2} x^2)$ on $\\mathbb{R}$, dimension $d = 1$.\n  - Function: $f(x) = x^2$.\n  - True expectation: $\\mathbb{E}_{\\pi}[f] = 1$.\n  - Parameters: $\\sigma = 1.0$, $x_0 = 0$.\n  - Seeds for $n \\in \\{\\,500, 5000, 50000\\,\\}$: $101$, $102$, $103$.\n\n- Case $C_2$ (univariate standard normal indicator):\n  - Target: $\\pi(x) \\propto \\exp(-\\tfrac{1}{2} x^2)$ on $\\mathbb{R}$, dimension $d = 1$.\n  - Function: $f(x) = \\mathbf{1}\\{|x| \\le 1\\}$.\n  - True expectation: $\\mathbb{E}_{\\pi}[f] = 2 \\Phi(1) - 1$, where $\\Phi$ is the standard normal cumulative distribution function.\n  - Parameters: $\\sigma = 1.0$, $x_0 = 0$.\n  - Seeds for $n \\in \\{\\,500, 5000, 50000\\,\\}$: $202$, $203$, $204$.\n\n- Case $C_3$ (bivariate standard normal cross-moment):\n  - Target: $\\pi(x) \\propto \\exp(-\\tfrac{1}{2} \\|x\\|^2)$ on $\\mathbb{R}^2$, dimension $d = 2$.\n  - Function: $f(x) = x_1 x_2$.\n  - True expectation: $\\mathbb{E}_{\\pi}[f] = 0$.\n  - Parameters: $\\sigma = 0.9$, $x_0 = (0, 0)$.\n  - Seeds for $n \\in \\{\\,500, 5000, 50000\\,\\}$: $303$, $304$, $305$.\n\n- Case $C_4$ (Beta target via transformation):\n  - Target: $\\pi(x) \\propto x^{a-1} (1-x)^{b-1}$ on $(0,1)$ with $a = 2.5$, $b = 5.5$. Implement the chain on $\\mathbb{R}$ using the logistic transform $x = \\mathrm{logit}^{-1}(u) = 1/(1+e^{-u})$ and a Gaussian random walk in $u$, accounting for the Jacobian to target the correct stationary distribution in $u$.\n  - Function: $f(x) = x$.\n  - True expectation: $\\mathbb{E}_{\\pi}[f] = \\dfrac{a}{a+b} = \\dfrac{2.5}{8.0}$.\n  - Parameters: $\\sigma = 1.25$, $u_0 = 0$ (which corresponds to $x_0 = 0.5$).\n  - Seeds for $n \\in \\{\\,500, 5000, 50000\\,\\}$: $404$, $405$, $406$.\n\n- Case $C_5$ (heavy-tailed Student distribution):\n  - Target: $\\pi(x) \\propto \\left(1 + \\dfrac{x^2}{\\nu}\\right)^{-(\\nu+1)/2}$ on $\\mathbb{R}$ with $\\nu = 3$, dimension $d = 1$.\n  - Function: $f(x) = x^2$.\n  - True expectation: $\\mathbb{E}_{\\pi}[f] = \\dfrac{\\nu}{\\nu - 2} = 3$.\n  - Parameters: $\\sigma = 2.5$, $x_0 = 0$.\n  - Seeds for $n \\in \\{\\,500, 5000, 50000\\,\\}$: $505$, $506$, $507$.\n\nFor each case $C_j$ and each $n \\in \\{\\,500, 5000, 50000\\,\\}$:\n- Run the Metropolis–Hastings chain with burn-in $b = \\lfloor n/10 \\rfloor$ and collect $n$ post-burn-in samples to compute $\\bar{f}_n$.\n- Compute the absolute error $|\\bar{f}_n - \\mathbb{E}_{\\pi}[f]|$.\n- Round the absolute error to six decimal places.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following fixed order:\n- Iterate $j$ in increasing order over the cases $C_1, C_2, C_3, C_4, C_5$,\n- For each case, iterate $n$ in increasing order over $\\{\\,500, 5000, 50000\\,\\}$,\n- Append the corresponding rounded absolute error to the list.\nFor example, the output must look like a single line of the form \"[e_{1,500},e_{1,5000},e_{1,50000},e_{2,500},\\ldots,e_{5,50000}]\", where $e_{j,n}$ denotes the required error for case $C_j$ at sample size $n$. There are no physical quantities or angles in this task, hence no units are required. All numerical answers must be pure real numbers as floats. The list should contain exactly $15$ floats. No other text should be printed.",
            "solution": "The problem requires the implementation of a Metropolis-Hastings Markov Chain Monte Carlo (MCMC) simulation to empirically verify the convergence of sample averages to known theoretical expectations for several target probability distributions. The solution involves constructing a suitable Markov chain for each case, running the simulation, and calculating the absolute error between the empirical estimate and the true value.\n\n### Principles of Metropolis-Hastings MCMC\n\nThe core of this problem is the Metropolis-Hastings (M-H) algorithm, a method for generating a sequence of random samples from a probability distribution for which direct sampling is difficult. The sequence of samples constitutes a Markov chain, $\\{X_t\\}_{t=0}^\\infty$. The M-H algorithm is designed such that the stationary (or invariant) distribution of this Markov chain is the desired target distribution, $\\pi(x)$.\n\nThe algorithm proceeds as follows, starting from an initial state $X_0=x_0$:\n1.  At iteration $t$, given the current state $X_t$, propose a new state $Y$ from a proposal distribution $q(Y|X_t)$.\n2.  Calculate the acceptance probability, $\\alpha(X_t, Y)$, given by:\n    $$\n    \\alpha(X_t, Y) = \\min\\left(1, \\frac{\\pi(Y)q(X_t|Y)}{\\pi(X_t)q(Y|X_t)}\\right)\n    $$\n    The term $\\frac{\\pi(Y)}{\\pi(X_t)}$ is the likelihood ratio, and $\\frac{q(X_t|Y)}{q(Y|X_t)}$ is the Hastings ratio. Since the target density $\\pi(x)$ often includes an unknown normalizing constant, we can use any function $\\tilde{\\pi}(x) \\propto \\pi(x)$ in the ratio, as the constants cancel.\n3.  Generate a random number $u$ from a uniform distribution on $[0,1]$.\n4.  The next state $X_{t+1}$ is set to the proposal $Y$ if $u  \\alpha(X_t, Y)$; otherwise, the chain remains at the current state, $X_{t+1} = X_t$.\n\nThis procedure guarantees that the resulting Markov chain satisfies the detailed balance condition with respect to $\\pi$, which is a sufficient condition for $\\pi$ to be the stationary distribution.\n\nThe problem specifies a random-walk Metropolis sampler with a symmetric Gaussian proposal distribution: $Y = X_t + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_d)$. For a symmetric proposal, $q(Y|X_t) = q(X_t|Y)$, so the Hastings ratio is $1$, and the acceptance probability simplifies to:\n$$\n\\alpha(X_t, Y) = \\min\\left(1, \\frac{\\pi(Y)}{\\pi(X_t)}\\right)\n$$\nTo improve numerical stability, computations are performed using log-probabilities. The acceptance step involves comparing $\\log(u)$ with the log-acceptance ratio $\\log(\\alpha) = \\min\\left(0, \\log\\tilde{\\pi}(Y) - \\log\\tilde{\\pi}(X_t)\\right)$.\n\n### Ergodic Theorem and Estimation\n\nUnder conditions of irreducibility and aperiodicity, which are met by our construction, the ergodic theorem for Markov chains ensures that the sample average of a function $f(x)$ converges to its true expectation under the stationary distribution:\n$$\n\\bar{f}_n = \\frac{1}{n} \\sum_{t=1}^{n} f(X_t) \\xrightarrow{a.s.} \\mathbb{E}_{\\pi}[f(X)] \\quad \\text{as } n \\to \\infty\n$$\nThe initial samples of the chain are typically discarded (a \"burn-in\" period) to reduce the influence of the arbitrary starting state $x_0$. The problem specifies a burn-in of $b = \\lfloor n/10 \\rfloor$ iterations. The empirical average is then computed using the subsequent $n$ samples.\n\n### Implementation for Test Cases\n\nA general MCMC function is implemented to handle the simulation for all specified cases. This function takes as input the log-target density, a function to evaluate on the samples, the initial state, proposal standard deviation, and simulation parameters.\n\n**Case $C_1$: Univariate Normal, $f(x) = x^2$**\n- Target: $\\pi(x) \\propto \\exp(-\\frac{1}{2}x^2)$ on $\\mathbb{R}$.\n- Log-target: $\\log\\tilde{\\pi}(x) = -\\frac{1}{2}x^2$.\n- Function: $f(x) = x^2$.\n- True Expectation: $\\mathbb{E}_{\\pi}[f(X)] = 1$.\n\n**Case $C_2$: Univariate Normal, $f(x) = \\mathbf{1}\\{|x| \\le 1\\}$**\n- Target: Same as $C_1$, $\\log\\tilde{\\pi}(x) = -\\frac{1}{2}x^2$.\n- Function: $f(x) = \\mathbf{1}\\{|x| \\le 1\\}$, which is $1$ if $|x| \\le 1$ and $0$ otherwise.\n- True Expectation: $\\mathbb{E}_{\\pi}[f(X)] = P(|X| \\le 1) = \\Phi(1) - \\Phi(-1) = 2\\Phi(1) - 1$, where $\\Phi$ is the standard normal CDF.\n\n**Case $C_3$: Bivariate Normal, $f(x) = x_1 x_2$**\n- Target: $\\pi(x) \\propto \\exp(-\\frac{1}{2}\\|x\\|^2)$ on $\\mathbb{R}^2$, where $x=(x_1, x_2)$.\n- Log-target: $\\log\\tilde{\\pi}(x) = -\\frac{1}{2}(x_1^2 + x_2^2)$.\n- Function: $f(x) = x_1 x_2$.\n- True Expectation: $\\mathbb{E}_{\\pi}[f(X)] = 0$.\n\n**Case $C_4$: Beta Distribution via Transformation**\n- The target for $x \\in (0,1)$ is the Beta distribution density, $\\pi_X(x) \\propto x^{a-1}(1-x)^{b-1}$ with $a=2.5, b=5.5$.\n- A direct random walk on $(0,1)$ is problematic. Instead, we reparameterize using the logistic function, $x = g(u) = 1/(1+e^{-u})$, which maps $u \\in \\mathbb{R}$ to $x \\in (0,1)$.\n- The MCMC is run on the transformed variable $u$. The target density for $u$, $\\pi_U(u)$, is found using the change of variables formula: $\\pi_U(u) = \\pi_X(g(u))|g'(u)|$.\n- The Jacobian determinant is $|g'(u)| = \\frac{e^{-u}}{(1+e^{-u})^2} = g(u)(1-g(u))$.\n- Thus, $\\pi_U(u) \\propto [g(u)]^{a-1}[1-g(u)]^{b-1} \\cdot g(u)(1-g(u)) = [g(u)]^a[1-g(u)]^b$.\n- Log-target for $u$: $\\log\\tilde{\\pi}_U(u) = a \\log(g(u)) + b \\log(1-g(u))$.\n- Function: The expectation is of $f(x)=x$. The simulation generates samples $u_t$, which are transformed back to $x_t=g(u_t)$ for averaging.\n- True Expectation: $\\mathbb{E}_{\\pi_X}[X] = \\frac{a}{a+b} = \\frac{2.5}{8.0}$.\n\n**Case $C_5$: Student's t-Distribution, $f(x) = x^2$**\n- Target: The density of a Student's t-distribution with $\\nu=3$ degrees of freedom, $\\pi(x) \\propto (1 + \\frac{x^2}{\\nu})^{-(\\nu+1)/2}$.\n- Log-target: $\\log\\tilde{\\pi}(x) = -\\frac{\\nu+1}{2}\\log(1 + \\frac{x^2}{\\nu})$.\n- Function: $f(x) = x^2$.\n- True Expectation: The variance of a standard t-distribution with $\\nu$ degrees of freedom is $\\frac{\\nu}{\\nu-2}$ for $\\nu2$. Since the mean is $0$, $\\mathbb{E}_{\\pi}[X^2] = \\text{Var}(X) = \\frac{3}{3-2} = 3$.\n\nFor each of the $5$ cases and $3$ sample sizes, the simulation is run with the specified parameters ($\\sigma$, $x_0$, seed), and the absolute error $|\\bar{f}_n - \\mathbb{E}_{\\pi}[f]|$ is computed and recorded. The final output is a list of these $15$ errors.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def run_mcmc(log_target_pdf, func_to_eval, x0, sigma, n_samples, seed):\n        \"\"\"\n        Runs a random-walk Metropolis-Hastings simulation.\n\n        Args:\n            log_target_pdf: A function that computes the log of the target density (unnormalized).\n            func_to_eval: A function of the chain's state to be averaged.\n            x0: Initial state of the chain (tuple or list).\n            sigma: Standard deviation of the Gaussian proposal distribution.\n            n_samples: Number of samples to collect after burn-in.\n            seed: Seed for the random number generator.\n\n        Returns:\n            The empirical average of func_to_eval over the samples.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        burn_in = n_samples // 10\n        total_iter = n_samples + burn_in\n\n        x = np.array(x0, dtype=float)\n        dim = x.size\n        \n        # Initial log probability\n        current_log_p = log_target_pdf(x)\n\n        f_values = []\n        for i in range(total_iter):\n            # Propose a new state\n            proposal = x + rng.normal(loc=0.0, scale=sigma, size=dim)\n\n            # Calculate acceptance probability in log-space\n            proposal_log_p = log_target_pdf(proposal)\n            log_alpha = proposal_log_p - current_log_p\n\n            if np.log(rng.uniform())  log_alpha:\n                x = proposal\n                current_log_p = proposal_log_p\n            \n            # Collect sample after burn-in\n            if i = burn_in:\n                f_values.append(func_to_eval(x))\n\n        return np.mean(f_values)\n\n    # --- Test Case Definitions ---\n\n    # Case C1: Univariate standard normal moment\n    c1_log_pdf = lambda x: -0.5 * x[0]**2\n    c1_f = lambda x: x[0]**2\n    \n    # Case C2: Univariate standard normal indicator\n    c2_log_pdf = c1_log_pdf\n    c2_f = lambda x: 1.0 if np.abs(x[0]) = 1.0 else 0.0\n    c2_true_E = 2 * norm.cdf(1) - 1\n\n    # Case C3: Bivariate standard normal cross-moment\n    c3_log_pdf = lambda x: -0.5 * (x[0]**2 + x[1]**2)\n    c3_f = lambda x: x[0] * x[1]\n\n    # Case C4: Beta target via transformation\n    c4_a, c4_b = 2.5, 5.5\n    g = lambda u_vec: 1.0 / (1.0 + np.exp(-u_vec[0]))\n    c4_log_pdf_u = lambda u: c4_a * np.log(g(u)) + c4_b * np.log(1.0 - g(u))\n    c4_f_on_u = lambda u: g(u)\n    c4_true_E = c4_a / (c4_a + c4_b)\n\n    # Case C5: Heavy-tailed Student distribution\n    c5_nu = 3.0\n    c5_log_pdf = lambda x: -((c5_nu + 1.0) / 2.0) * np.log(1.0 + x[0]**2 / c5_nu)\n    c5_f = lambda x: x[0]**2\n    c5_true_E = c5_nu / (c5_nu - 2.0)\n\n\n    test_cases = [\n        {\n            \"name\": \"C1\", \"log_pdf\": c1_log_pdf, \"func_f\": c1_f, \"true_E\": 1.0,\n            \"sigma\": 1.0, \"x0\": (0.0,), \"seeds\": {500: 101, 5000: 102, 50000: 103}\n        },\n        {\n            \"name\": \"C2\", \"log_pdf\": c2_log_pdf, \"func_f\": c2_f, \"true_E\": c2_true_E,\n            \"sigma\": 1.0, \"x0\": (0.0,), \"seeds\": {500: 202, 5000: 203, 50000: 204}\n        },\n        {\n            \"name\": \"C3\", \"log_pdf\": c3_log_pdf, \"func_f\": c3_f, \"true_E\": 0.0,\n            \"sigma\": 0.9, \"x0\": (0.0, 0.0), \"seeds\": {500: 303, 5000: 304, 50000: 305}\n        },\n        {\n            \"name\": \"C4\", \"log_pdf\": c4_log_pdf_u, \"func_f\": c4_f_on_u, \"true_E\": c4_true_E,\n            \"sigma\": 1.25, \"x0\": (0.0,), \"seeds\": {500: 404, 5000: 405, 50000: 406}\n        },\n        {\n            \"name\": \"C5\", \"log_pdf\": c5_log_pdf, \"func_f\": c5_f, \"true_E\": c5_true_E,\n            \"sigma\": 2.5, \"x0\": (0.0,), \"seeds\": {500: 505, 5000: 506, 50000: 507}\n        }\n    ]\n\n    sample_sizes = [500, 5000, 50000]\n    results = []\n\n    for case in test_cases:\n        for n in sample_sizes:\n            seed = case[\"seeds\"][n]\n            \n            # Run the MCMC simulation\n            empirical_mean = run_mcmc(\n                log_target_pdf=case[\"log_pdf\"],\n                func_to_eval=case[\"func_f\"],\n                x0=case[\"x0\"],\n                sigma=case[\"sigma\"],\n                n_samples=n,\n                seed=seed\n            )\n            \n            # Compute and store the absolute error\n            error = np.abs(empirical_mean - case[\"true_E\"])\n            rounded_error = round(error, 6)\n            results.append(rounded_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The true power of MCMC lies in its applicability to complex, high-dimensional, and even non-Euclidean state spaces where other methods fail. This advanced exercise challenges you to apply the Metropolis-Hastings framework to sample from a distribution over a combinatorial space of Directed Acyclic Graphs (DAGs). Successfully navigating this problem requires a deep understanding of the full acceptance ratio, including the often-overlooked Hastings correction term, demonstrating how MCMC can be adapted to solve sophisticated problems in areas like Bayesian machine learning .",
            "id": "3313416",
            "problem": "Consider the state space of Directed Acyclic Graphs (DAGs) on the node set $\\{1,2,3,4\\}$. Suppose we aim to sample from a posterior distribution over DAGs that is proportional to $\\exp(S(G))$, where $S(G)$ is a decomposable log-score that sums local scores over nodes based on their parent sets, and the prior over DAGs is uniform. We design a Markov chain Monte Carlo (MCMC) sampler using the Metropolis-Hastings (MH) algorithm with proposals restricted to local edge edits that preserve acyclicity. A local edit is one of: adding a single directed edge $i \\to j$ not currently present, deleting a single existing directed edge, or reversing a single existing edge $i \\to j$ to $j \\to i$ (only if the resulting graph remains acyclic). At each step, the proposal mechanism selects uniformly at random from the set of all valid local edits in the current DAG. All moves that would produce a cycle are disallowed.\n\nStarting from the current DAG $G$ with edges $\\{1 \\to 2,\\; 1 \\to 3,\\; 3 \\to 4\\}$, consider proposing the addition of the edge $4 \\to 2$, yielding the proposed DAG $G'$ with edges $\\{1 \\to 2,\\; 1 \\to 3,\\; 3 \\to 4,\\; 4 \\to 2\\}$. The decomposable log-score $S(G)$ changes only through the local score of node $2$, whose parent set changes from $\\{1\\}$ in $G$ to $\\{1,4\\}$ in $G'$. The relevant local scores are:\n- $\\ell_{2}(\\{1\\}) = -120.5$,\n- $\\ell_{2}(\\{1,4\\}) = -121.3$.\n\nWorking from first principles appropriate for Markov chain Monte Carlo, derive the general MH acceptance probability for this DAG proposal mechanism under the given posterior. Then, for the specific move from $G$ to $G'$, compute the acceptance probability $\\alpha(G \\to G')$ by explicitly enumerating the valid local edits from $G$ and from $G'$, and by evaluating the change in $S(G)$. Round your final numerical answer for $\\alpha(G \\to G')$ to four significant figures and express it as a unitless decimal.",
            "solution": "The user-provided problem has been analyzed according to the specified validation protocol.\n\n### Step 1: Extract Givens\n-   **State Space**: The set of all Directed Acyclic Graphs (DAGs) on the node set $V=\\{1,2,3,4\\}$.\n-   **Target Distribution**: A posterior distribution over DAGs $G$, denoted $\\pi(G)$, proportional to $\\exp(S(G))$. This implies a uniform prior over the space of DAGs.\n-   **Score Function**: $S(G)$ is a decomposable log-score, $S(G) = \\sum_{i \\in V} \\ell_i(\\text{Pa}_G(i))$, where $\\text{Pa}_G(i)$ is the parent set of node $i$ in graph $G$.\n-   **MCMC Algorithm**: Metropolis-Hastings (MH).\n-   **Proposal Mechanism**: A proposal $G'$ is generated from the current DAG $G$ by selecting uniformly at random from the set of all valid local edits.\n-   **Valid Local Edits**:\n    1.  Adding a single directed edge $i \\to j$ not currently in $G$, provided the resulting graph is acyclic.\n    2.  Deleting a single existing directed edge $i \\to j$.\n    3.  Reversing a single existing directed edge $i \\to j$ to $j \\to i$, provided the resulting graph is acyclic.\n-   **Current State**: The DAG $G$ with edge set $E_G = \\{1 \\to 2,\\; 1 \\to 3,\\; 3 \\to 4\\}$.\n-   **Proposed State**: The DAG $G'$ with edge set $E_{G'} = \\{1 \\to 2,\\; 1 \\to 3,\\; 3 \\to 4,\\; 4 \\to 2\\}$.\n-   **Local Scores**: $\\ell_{2}(\\{1\\}) = -120.5$ and $\\ell_{2}(\\{1,4\\}) = -121.3$.\n-   **Constraint**: The log-score $S(G)$ changes only through the local score of node $2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed to be **valid**.\n-   **Scientifically Grounded**: The problem describes a standard Bayesian structure learning task for graphical models using MCMC, a core topic in machine learning and statistics. The Metropolis-Hastings algorithm, decomposable scores, and the described neighborhood structure (local edge edits) are all well-established and fundamentally sound concepts.\n-   **Well-Posed**: The problem asks for a general formula and a specific numerical calculation. All information required for these tasks is provided. The instructions are clear, and a unique, meaningful solution can be determined.\n-   **Objective**: The problem is stated using precise, formal language and contains no subjective or ambiguous elements.\n\n### Step 3: Verdict and Action\nThe problem is valid. A detailed solution is provided below.\n\nThe Metropolis-Hastings (MH) acceptance probability for a transition from a state $G$ to a proposed state $G'$ is given by the general formula:\n$$\n\\alpha(G \\to G') = \\min\\left(1, \\frac{\\pi(G')}{\\pi(G)} \\frac{q(G' \\to G)}{q(G \\to G')}\\right)\n$$\nwhere $\\pi(G)$ is the target probability of state $G$, and $q(G \\to G')$ is the proposal probability of moving from $G$ to $G'$.\n\nFirst, we determine the target distribution ratio, $\\frac{\\pi(G')}{\\pi(G)}$. The problem states that the posterior is proportional to $\\exp(S(G))$ and the prior over DAGs is uniform. A uniform prior $P(G)$ means $P(G) = C$ for some constant $C$ for any valid DAG $G$. Thus, the target distribution is $\\pi(G) \\propto P(\\text{Data}|G)P(G) \\propto \\exp(S(G)) \\cdot C$. The ratio is:\n$$\n\\frac{\\pi(G')}{\\pi(G)} = \\frac{C \\cdot \\exp(S(G'))}{C \\cdot \\exp(S(G))} = \\exp(S(G') - S(G))\n$$\nNext, we determine the proposal probability ratio, $\\frac{q(G' \\to G)}{q(G \\to G')}$. The proposal mechanism selects uniformly from the set of all valid local edits. Let $N(G)$ be the set of all DAGs reachable from $G$ via one valid local edit. The size of this set is $|N(G)|$. The proposal probability is therefore $q(G \\to G') = \\frac{1}{|N(G)|}$ for any $G' \\in N(G)$, and $0$ otherwise. Similarly, the reverse proposal probability is $q(G' \\to G) = \\frac{1}{|N(G')|}$.\nThe proposal ratio is:\n$$\n\\frac{q(G' \\to G)}{q(G \\to G')} = \\frac{1/|N(G')|}{1/|N(G)|} = \\frac{|N(G)|}{|N(G')|}\n$$\nSubstituting these expressions into the MH formula gives the general acceptance probability for this specific sampler:\n$$\n\\alpha(G \\to G') = \\min\\left(1, \\exp(S(G') - S(G)) \\frac{|N(G)|}{|N(G')|}\\right)\n$$\nThis is the general expression for the acceptance probability.\n\nNow, we compute this value for the specific move from $G$ to $G'$.\nThe change in the log-score, $\\Delta S = S(G') - S(G)$, is determined by the change in local scores. The move from $G$ to $G'$ consists of adding the edge $4 \\to 2$. This only changes the parent set of node $2$. In $G$, $\\text{Pa}_G(2)=\\{1\\}$. In $G'$, $\\text{Pa}_{G'}(2)=\\{1,4\\}$. All other parent sets remain the same.\nGiven the local scores $\\ell_{2}(\\{1\\}) = -120.5$ and $\\ell_{2}(\\{1,4\\}) = -121.3$, the change in the total score is:\n$$\n\\Delta S = S(G') - S(G) = \\ell_{2}(\\text{Pa}_{G'}(2)) - \\ell_{2}(\\text{Pa}_G(2)) = -121.3 - (-120.5) = -0.8\n$$\nThe target ratio is $\\exp(-0.8)$.\n\nNext, we must compute $|N(G)|$ and $|N(G')|$ by enumerating all valid local edits from each graph.\n\n**Enumerating edits from $G$ (Edges: $\\{1 \\to 2, 1 \\to 3, 3 \\to 4\\}$):**\nThe paths in $G$ are $1 \\to 2$, $1 \\to 3$, $3 \\to 4$, and $1 \\to 3 \\to 4$.\n1.  **Deletions**: There are $3$ edges, so there are $3$ valid deletion moves.\n2.  **Additions**: We can add an edge $i \\to j$ if it does not already exist and there is no path from $j$ to $i$ in $G$. There are $4 \\times 3 - 3 = 9$ non-existent edges.\n    -   Invalid additions (create a cycle):\n        -   $2 \\to 1$ (cycle with $1 \\to 2$)\n        -   $3 \\to 1$ (cycle with $1 \\to 3$)\n        -   $4 \\to 1$ (cycle with $1 \\to 3 \\to 4$)\n        -   $4 \\to 3$ (cycle with $3 \\to 4$)\n    -   Valid additions: $9 - 4 = 5$ moves. They are $1 \\to 4$, $2 \\to 3$, $2 \\to 4$, $3 \\to 2$, $4 \\to 2$.\n3.  **Reversals**: We can reverse an edge $i \\to j$ if there is no other path from $i$ to $j$ in $G$.\n    -   Reverse $1 \\to 2$: No other path from $1$ to $2$. Valid.\n    -   Reverse $1 \\to 3$: No other path from $1$ to $3$. Valid.\n    -   Reverse $3 \\to 4$: No other path from $3$ to $4$. Valid.\n    -   There are $3$ valid reversal moves.\nThe total number of valid edits from $G$ is $|N(G)| = 3 (\\text{deletions}) + 5 (\\text{additions}) + 3 (\\text{reversals}) = 11$.\n\n**Enumerating edits from $G'$ (Edges: $\\{1 \\to 2, 1 \\to 3, 3 \\to 4, 4 \\to 2\\}$):**\nThe graph $G'$ is acyclic. Its paths include $1 \\to 2$, $1 \\to 3 \\to 4 \\to 2$, $1 \\to 3$, $3 \\to 4$, and $4 \\to 2$.\n1.  **Deletions**: There are $4$ edges, so there are $4$ valid deletion moves.\n2.  **Additions**: We check the $4 \\times 3 - 4 = 8$ non-existent edges.\n    -   Invalid additions (path $j \\to i$ exists in $G'$):\n        -   $2 \\to 1$ (path $1 \\to 2$ exists)\n        -   $3 \\to 1$ (path $1 \\to 3$ exists)\n        -   $4 \\to 1$ (path $1 \\to 3 \\to 4$ exists)\n        -   $2 \\to 3$ (path $3 \\to 4 \\to 2$ exists)\n        -   $2 \\to 4$ (path $4 \\to 2$ exists)\n        -   $4 \\to 3$ (path $3 \\to 4$ exists)\n    -   Valid additions: The remaining $2$ moves are $1 \\to 4$ and $3 \\to 2$.\n3.  **Reversals**: We check the $4$ existing edges $i \\to j$.\n    -   Reverse $1 \\to 2$: Invalid, as another path $1 \\to 3 \\to 4 \\to 2$ exists.\n    -   Reverse $1 \\to 3$: No other path from $1$ to $3$. Valid.\n    -   Reverse $3 \\to 4$: No other path from $3$ to $4$. Valid.\n    -   Reverse $4 \\to 2$: No other path from $4$ to $2$. Valid.\n    -   There are $3$ valid reversal moves.\nThe total number of valid edits from $G'$ is $|N(G')| = 4 (\\text{deletions}) + 2 (\\text{additions}) + 3 (\\text{reversals}) = 9$.\n\nFinally, we assemble the acceptance probability:\n$$\n\\alpha(G \\to G') = \\min\\left(1, \\exp(-0.8) \\frac{11}{9}\\right)\n$$\nCalculating the value:\n$$\n\\exp(-0.8) \\approx 0.449329\n$$\n$$\n\\frac{11}{9} \\approx 1.222222\n$$\n$$\n\\exp(-0.8) \\frac{11}{9} \\approx 0.449329 \\times 1.222222 \\approx 0.5491807\n$$\nThis value is less than $1$. Rounding to four significant figures, we get $0.5492$.",
            "answer": "$$\\boxed{0.5492}$$"
        }
    ]
}