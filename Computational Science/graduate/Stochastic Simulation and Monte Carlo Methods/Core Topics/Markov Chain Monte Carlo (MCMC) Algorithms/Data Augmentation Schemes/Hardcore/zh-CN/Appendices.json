{
    "hands_on_practices": [
        {
            "introduction": "数据增强的一个核心思想是通过引入潜变量来简化后验分布的计算。在隐马尔可夫模型（HMM）的贝叶斯推断中，一个经典的增强策略是将未观测到的状态序列视为“缺失数据”。通过以完整的状态序列为条件，模型参数（例如转移矩阵）的后验推断常常可以简化为共轭形式，从而实现高效的吉布斯采样。这项练习  让你亲手实践这一基本思想，通过推导和计算来加深对数据增强在动态模型中应用的理解。",
            "id": "3301965",
            "problem": "考虑一个具有 $K=3$ 个隐状态和转移矩阵 $P=\\{p_{i,j}\\}_{i,j=1}^{3}$ 的时间齐次有限状态马尔可夫链，其中每一行 $P_{i,\\cdot}$ 位于 $2$-单纯形上。假设对各行设置了独立的狄利克雷先验：对于每个 $i \\in \\{1,2,3\\}$，$P_{i,\\cdot} \\sim \\mathrm{Dir}(\\alpha_{i,1}, \\alpha_{i,2}, \\alpha_{i,3})$，超参数 $\\alpha_{i,j}  0$。为实现共轭更新，使用一种数据增强方案，将隐状态序列 $s_{1:T}$ 视为增强数据。假设给定一个长度为 $T=10$ 的完整示例隐状态序列为 $s_{1:10} = (1,2,3,1,3,3,2,1,2,3)$。\n\n给定超参数\n$$\n\\alpha_{1,\\cdot} = (0.7,\\,1.2,\\,0.9), \\quad \\alpha_{2,\\cdot} = (1.1,\\,0.6,\\,0.8), \\quad \\alpha_{3,\\cdot} = (0.5,\\,0.5,\\,0.5).\n$$\n\n从核心定义和经过检验的事实出发，包括具有完整状态路径的马尔可夫链转移似然和贝叶斯法则，按以下步骤进行：\n\n1. 在给定增强序列 $s_{1:T}$ 和狄利克雷先验的条件下，推导每一行 $P_{i,\\cdot}$ 的完全条件后验分布。您的推导必须从基于转移计数 $n_{i,j} = \\sum_{t=1}^{T-1} \\mathbf{1}\\{s_t=i,\\,s_{t+1}=j\\}$ 的似然分解开始，并且必须得出一个后验的闭式表达式，该后验为狄利克雷分布，其参数结合了先验超参数和观测到的转移计数。\n\n2. 通过刻画第一行 $P_{1,\\cdot}$ 的完全条件分布，对以给定 $s_{1:T}$ 为条件的转移矩阵执行一个吉布斯（Gibbs）更新步骤。然后，在此完全条件分布下，计算特定转移概率 $p_{1,3}$（从状态 $1$ 到状态 $3$）的后验均值。将您的最终数值答案四舍五入至四位有效数字。\n\n最终答案必须是单个实数。无需单位。",
            "solution": "该问题陈述经过仔细验证，并被确定为有效。它在贝叶斯统计和马尔可夫链理论方面具有科学依据，提法得当，提供了所有必要信息，并且表述客观。因此，我们可以进行完整解答。\n\n该问题要求完成两个主要任务：首先，推导马尔可夫链转移矩阵各行的完全条件后验分布的一般形式；其次，将此结果应用于一个具体案例以计算后验均值。\n\n**1. 完全条件后验的推导**\n\n我们给定一个具有 $K$ 个状态和转移矩阵 $P = \\{p_{i,j}\\}_{i,j=1}^{K}$ 的时间齐次有限状态马尔可夫链，其中 $p_{i,j} = P(s_{t+1}=j | s_t=i)$。每一行 $P_{i,\\cdot} = (p_{i,1}, \\dots, p_{i,K})$ 是一个在 $(K-1)$-单纯形上的概率向量。\n\n贝叶斯框架要求为未知参数 $P$ 指定一个先验分布，并基于数据指定一个似然函数。然后通过贝叶斯法则找到后验分布。\n\n**先验分布：**\n假设转移矩阵 $P$ 的各行是先验独立的。对于每一行 $i \\in \\{1,\\dots,K\\}$，先验是一个狄利克雷分布：\n$$\nP_{i,\\cdot} \\sim \\mathrm{Dir}(\\alpha_{i,1}, \\dots, \\alpha_{i,K})\n$$\n单行 $P_{i,\\cdot}$ 的概率密度函数 (PDF) 由下式给出：\n$$\np(P_{i,\\cdot} | \\alpha_{i,\\cdot}) = \\frac{\\Gamma\\left(\\sum_{j=1}^{K} \\alpha_{i,j}\\right)}{\\prod_{j=1}^{K} \\Gamma(\\alpha_{i,j})} \\prod_{j=1}^{K} p_{i,j}^{\\alpha_{i,j}-1}\n$$\n由于独立性，整个矩阵 $P$ 的联合先验是各行先验的乘积：\n$$\np(P | \\alpha) = \\prod_{i=1}^{K} p(P_{i,\\cdot} | \\alpha_{i,\\cdot})\n$$\n\n**似然函数：**\n数据是隐状态的完整序列 $s_{1:T} = (s_1, s_2, \\dots, s_T)$。在给定转移矩阵 $P$ 的条件下，该序列的似然由每次转移概率的乘积决定：\n$$\n\\mathcal{L}(P; s_{1:T}) = p(s_{1:T} | P) = p(s_1) \\prod_{t=1}^{T-1} P(s_{t+1} | s_t, P) = p(s_1) \\prod_{t=1}^{T-1} p_{s_t, s_{t+1}}\n$$\n在推断转移概率时，我们通常以第一个状态 $s_1$ 为条件。似然可以通过聚合转移来重新表达。令 $n_{i,j}$ 为序列中从状态 $i$ 到状态 $j$ 的观测转移次数：\n$$\nn_{i,j} = \\sum_{t=1}^{T-1} \\mathbf{1}\\{s_t=i, s_{t+1}=j\\}\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。似然函数随后变为：\n$$\n\\mathcal{L}(P; s_{1:T}) \\propto \\prod_{i=1}^{K} \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}}\n$$\n关键的是，这个似然可以按转移矩阵的行 $i$ 分解：\n$$\n\\mathcal{L}(P; s_{1:T}) \\propto \\prod_{i=1}^{K} \\left( \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}} \\right)\n$$\n项 $\\prod_{j=1}^{K} p_{i,j}^{n_{i,j}}$ 是针对从状态 $i$ 发出的转移的多项分布的核。\n\n**后验分布：**\n根据贝叶斯法则，$P$ 的后验分布与似然和先验的乘积成正比：\n$$\np(P | s_{1:T}, \\alpha) \\propto \\mathcal{L}(P; s_{1:T}) \\cdot p(P | \\alpha)\n$$\n由于先验和似然都按 $P$ 的行进行分解，后验也同样分解：\n$$\np(P | s_{1:T}, \\alpha) \\propto \\prod_{i=1}^{K} \\left[ \\left( \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}} \\right) \\left( \\frac{\\Gamma\\left(\\sum_{j=1}^{K} \\alpha_{i,j}\\right)}{\\prod_{j=1}^{K} \\Gamma(\\alpha_{i,j})} \\prod_{j=1}^{K} p_{i,j}^{\\alpha_{i,j}-1} \\right) \\right]\n$$\n这表明每一行 $P_{i,\\cdot}$ 的完全条件后验可以独立推导。对于单行 $i$：\n$$\np(P_{i,\\cdot} | s_{1:T}, \\alpha_{i,\\cdot}) \\propto \\left( \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}} \\right) \\cdot \\left( \\prod_{j=1}^{K} p_{i,j}^{\\alpha_{i,j}-1} \\right)\n$$\n合并各项，我们得到：\n$$\np(P_{i,\\cdot} | s_{1:T}, \\alpha_{i,\\cdot}) \\propto \\prod_{j=1}^{K} p_{i,j}^{n_{i,j} + \\alpha_{i,j} - 1}\n$$\n此表达式是具有更新参数的狄利克雷分布的核。因此，转移矩阵第 $i$ 行的完全条件后验分布为：\n$$\nP_{i,\\cdot} | s_{1:T}, \\alpha_{i,\\cdot} \\sim \\mathrm{Dir}(\\alpha_{i,1} + n_{i,1}, \\alpha_{i,2} + n_{i,2}, \\dots, \\alpha_{i,K} + n_{i,K})\n$$\n这证明了狄利克雷先验与转移计数的多项似然的共轭性。\n\n**2. Gibbs 更新与后验均值计算**\n\n我们现在将此结果应用于具有 $K=3$ 个状态的具体问题。\n\n**转移计数：**\n首先，我们必须从给定序列 $s_{1:10} = (1,2,3,1,3,3,2,1,2,3)$ 计算转移计数 $n_{i,j}$。共有 $T-1=9$ 次转移：\n$1 \\to 2$, $2 \\to 3$, $3 \\to 1$, $1 \\to 3$, $3 \\to 3$, $3 \\to 2$, $2 \\to 1$, $1 \\to 2$, $2 \\to 3$。\n\n将这些转移聚合到一个计数矩阵 $N = \\{n_{i,j}\\}$ 中：\n-   从状态 $1$ 出发的转移：$1 \\to 2$ (两次)，$1 \\to 3$ (一次)。所以，$n_{1,1}=0$, $n_{1,2}=2$, $n_{1,3}=1$。\n-   从状态 $2$ 出发的转移：$2 \\to 3$ (两次)，$2 \\to 1$ (一次)。所以，$n_{2,1}=1$, $n_{2,2}=0$, $n_{2,3}=2$。\n-   从状态 $3$ 出发的转移：$3 \\to 1$ (一次)，$3 \\to 2$ (一次)，$3 \\to 3$ (一次)。所以，$n_{3,1}=1$, $n_{3,2}=1$, $n_{3,3}=1$。\n\n计数矩阵为：\n$$ N = \\begin{pmatrix} 0  2  1 \\\\ 1  0  2 \\\\ 1  1  1 \\end{pmatrix} $$\n\n**$P_{1,\\cdot}$ 的完全条件分布：**\n问题要求刻画第一行 $P_{1,\\cdot} = (p_{1,1}, p_{1,2}, p_{1,3})$ 的完全条件分布。\n此行的先验超参数给定为 $\\alpha_{1,\\cdot} = (0.7, 1.2, 0.9)$。\n此行的转移计数为 $n_{1,\\cdot} = (n_{1,1}, n_{1,2}, n_{1,3}) = (0, 2, 1)$。\n\n使用推导出的公式，后验参数 $\\alpha'_{1,\\cdot}$ 为：\n$$\n\\alpha'_{1,j} = \\alpha_{1,j} + n_{1,j}\n$$\n$$\n\\alpha'_{1,1} = 0.7 + 0 = 0.7\n$$\n$$\n\\alpha'_{1,2} = 1.2 + 2 = 3.2\n$$\n$$\n\\alpha'_{1,3} = 0.9 + 1 = 1.9\n$$\n因此，转移矩阵第一行的完全条件后验分布为：\n$$\nP_{1,\\cdot} | s_{1:10}, \\alpha_{1,\\cdot} \\sim \\mathrm{Dir}(0.7, 3.2, 1.9)\n$$\n\n**$p_{1,3}$ 的后验均值：**\n我们需要计算转移概率 $p_{1,3}$ 的后验均值。狄利克雷分布的一个标准性质是，如果一个随机向量 $(X_1, \\dots, X_K) \\sim \\mathrm{Dir}(\\beta_1, \\dots, \\beta_K)$，其第 $j$ 个分量的期望值为：\n$$\n\\mathbb{E}[X_j] = \\frac{\\beta_j}{\\sum_{k=1}^{K} \\beta_k}\n$$\n将此应用于我们对 $P_{1,\\cdot}$ 的后验分布， $p_{1,3}$ 的后验均值为：\n$$\n\\mathbb{E}[p_{1,3} | s_{1:10}, \\alpha_{1,\\cdot}] = \\frac{\\alpha'_{1,3}}{\\alpha'_{1,1} + \\alpha'_{1,2} + \\alpha'_{1,3}}\n$$\n代入计算出的后验参数：\n$$\n\\mathbb{E}[p_{1,3} | s_{1:10}, \\alpha_{1,\\cdot}] = \\frac{1.9}{0.7 + 3.2 + 1.9} = \\frac{1.9}{5.8}\n$$\n现在，我们计算其数值：\n$$\n\\frac{1.9}{5.8} = \\frac{19}{58} \\approx 0.327586206...\n$$\n四舍五入到四位有效数字，我们得到 $0.3276$。",
            "answer": "$$\\boxed{0.3276}$$"
        },
        {
            "introduction": "数据增强的威力远不止于填补缺失数据，它更是一种强大的算法设计工具，能够改造复杂的似然函数。对于像逻辑斯蒂回归这样的非共轭模型，Pólya-Gamma (PG) 数据增强方案通过引入辅助潜变量，巧妙地将其似然函数表示为高斯分布的混合形式。这项练习  将指导你为一个具体的逻辑斯蒂回归模型实现一步基于PG增强的吉布斯采样，让你体验如何运用现代数据增强技术将一个原本棘手的计算问题转化为一个易于处理的条件高斯模型。",
            "id": "3302005",
            "problem": "考虑一个具有二元响应和高斯先验的贝叶斯逻辑回归，使用通过 Pólya–Gamma (PG) 数据增强构建的吉布斯采样器进行一步拟合。假设有 $n$ 个观测值，其预测变量为 $x_i \\in \\mathbb{R}^p$，响应为 $y_i \\in \\{0,1\\}$，回归系数为 $\\beta \\in \\mathbb{R}^p$。假设先验为 $\\beta \\sim \\mathcal{N}(0, \\sigma_\\beta^2 I_p)$。逻辑斯谛模型的似然函数为 $p(y_i \\mid x_i, \\beta) = \\operatorname{Bernoulli}(\\pi_i)$，其中 $\\pi_i = (1 + \\exp(-x_i^\\top \\beta))^{-1}$。在 Pólya–Gamma (PG) 增强下，单次完整的吉布斯迭代会采样潜变量 $\\omega_i$，然后从其完全条件分布中采样 $\\beta$，其中 Pólya–Gamma 分布表示为 $\\operatorname{PG}(b, c)$，且 $b \\in \\mathbb{R}_+$, $c \\in \\mathbb{R}$。\n\n您的任务是为每个指定的测试用例，实现恰好一次完整的吉布斯迭代，包括以下步骤：\n- 对每个 $i \\in \\{1,\\dots,n\\}$，使用指定的截断水平 $K$ 和截断无穷级数表示法，从 $\\operatorname{PG}(1, x_i^\\top \\beta^{(0)})$ 中采样 $\\omega_i$。\n- 在给定 $\\omega = (\\omega_1,\\dots,\\omega_n)$、先验 $\\beta \\sim \\mathcal{N}(0, \\sigma_\\beta^2 I_p)$ 以及带有 PG 增强的逻辑斯谛似然函数的情况下，从其完全条件分布中采样 $\\beta^{(1)}$。\n\n请使用以下合成测试套件。对于每个用例，取初始状态 $\\beta^{(0)} = 0_p$（$p$ 维零向量），并使用指定的截断水平 $K$、先验方差 $\\sigma_\\beta^2$ 以及给定的数据 $(X,y)$。下面所有的实数、整数和向量都是精确值，必须按原样使用。\n\n测试用例 1：\n- $n = 5$, $p = 2$。\n- $X^{(1)} = \\begin{bmatrix}\n1.0  -0.5 \\\\\n1.2  0.3 \\\\\n-0.7  1.4 \\\\\n0.0  -1.0 \\\\\n2.0  2.0\n\\end{bmatrix}$，\n$y^{(1)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$。\n- $\\sigma_\\beta^2 = 5.0$。\n- $K = 200$。\n\n测试用例 2：\n- $n = 8$, $p = 3$。\n- $X^{(2)} = \\begin{bmatrix}\n1.0  0.5  -1.0 \\\\\n-0.3  2.0  0.7 \\\\\n0.8  -0.6  1.5 \\\\\n1.2  1.1  0.0 \\\\\n-1.5  0.4  -0.2 \\\\\n0.0  -1.2  0.9 \\\\\n2.0  0.0  1.0 \\\\\n-0.7  -0.8  0.3\n\\end{bmatrix}$，\n$y^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$。\n- $\\sigma_\\beta^2 = 2.5$。\n- $K = 150$。\n\n测试用例 3 (秩亏设计，用于测试带先验的数值稳定性)：\n- $n = 5$, $p = 3$。\n- $X^{(3)} = \\begin{bmatrix}\n1.0  1.0  2.0 \\\\\n0.9  1.1  2.0 \\\\\n1.1  0.9  2.0 \\\\\n-0.5  -0.5  -1.0 \\\\\n2.0  2.0  4.0\n\\end{bmatrix}$，\n$y^{(3)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$。\n- $\\sigma_\\beta^2 = 10.0$。\n- $K = 50$。\n\n随机性与确定性要求：\n- 为伪随机数生成器使用单个全局基础种子 $s_0 = 2025$。对于测试用例索引 $j \\in \\{1,2,3\\}$，使用种子 $s_j = s_0 + j$ 创建一个独立的生成器。\n- 使用双精度浮点运算。\n\nPólya–Gamma 采样器要求：\n- 使用截断无穷级数表示法来采样 $\\omega \\sim \\operatorname{PG}(1, c)$：\n$$\n\\omega \\approx \\frac{1}{2 \\pi^2} \\sum_{k=1}^{K} \\frac{g_k}{\\left(k - \\tfrac{1}{2}\\right)^2 + \\frac{c^2}{4 \\pi^2}}, \\quad g_k \\overset{\\text{i.i.d.}}{\\sim} \\operatorname{Gamma}(1,1).\n$$\n- 使用给定的 $K$ 精确计算该级数。\n\n每次迭代的计算复杂度核算模型：\n- 每次加法、减法、乘法、除法或调用随机变量生成器计为一次原始操作。\n- 诸如 $\\pi$ 之类的常数成本为零操作；读取数组元素成本为零操作。\n- 对每个 $i$ 形成 $c_i = x_i^\\top \\beta^{(0)}$ 的成本是 $p$ 次乘法加上 $(p-1)$ 次加法。\n- 对每个 $\\omega_i$，令 $c = c_i$，并定义每个 $i$ 的开销为：$c^2$ 的 1 次乘法，$c^2/(4\\pi^2)$ 的 1 次除法，以及用 $1/(2\\pi^2)$ 缩放最终级数和的 1 次乘法或除法。对于每个 $k \\in \\{1,\\dots,K\\}$，每项的成本恰好是 6 次操作：一次减法形成 $k - \\tfrac{1}{2}$，一次乘法对其平方，一次加法加上常数偏移，一次伽马随机抽取，一次除法计算 $g_k$ 除以分母，以及一次加法累加级数。\n- 为采样 $\\beta$ 组装高斯线性代数：通过将 $X$ 的每一行乘以 $\\omega_i$ 来形成 $W X$（成本为 $n p$ 次乘法），然后使用朴素算法形成 $X^\\top (W X)$，成本为 $p^2 n$ 次乘法和 $p^2 (n-1)$ 次加法（总计 $p^2 (2n - 1)$ 次操作）。将岭项 $(1/\\sigma_\\beta^2) I_p$ 加到对角线上：1 次除法形成 $1/\\sigma_\\beta^2$ 再加上 $p$ 次对角线加法。\n- 计算 $\\kappa = y - \\tfrac{1}{2}\\mathbf{1}$：恰好 $n$ 次减法。形成 $b = X^\\top \\kappa$：成本为 $p n$ 次乘法和 $p(n-1)$ 次加法（总计 $p (2n - 1)$ 次操作）。\n- 使用数值稳定的方法计算精度矩阵 $A$ 的 Cholesky 分解 $A = L L^\\top$；将其计为 $\\left\\lfloor p^3/3 \\right\\rfloor$ 次原始操作。\n- 使用 $L$ 和 $L^\\top$ 通过前向和后向替换求解 $A \\mu = b$：两次三角求解合计计为 $2 p^2$ 次操作。\n- 抽取 $z \\sim \\mathcal{N}(0, I_p)$：$p$ 次独立标准正态抽取计为 $p$ 次操作。\n- 使用单次三角求解 $L^\\top x = z$ 形成一个协方差为 $A^{-1}$ 的高斯样本，并设置 $\\beta^{(1)} = \\mu + x$：三角求解计为 $p^2$ 次操作，最终求和计为 $p$ 次加法。\n\n根据此模型，给定 $n$、$p$、$K$ 的一次迭代的总操作数为\n$$\n\\begin{aligned}\n\\mathrm{Ops}(n,p,K) \\;=\\; n\\big( p + (p-1) + 1 + 1 + 1 + 6K \\big) \\\\\n+ n p \\\\\n+ p^2 (2n - 1) \\\\\n+ 1 + p \\\\\n+ n \\\\\n+ p (2n - 1) \\\\\n+ \\left\\lfloor \\frac{p^3}{3} \\right\\rfloor \\\\\n+ 3 p^2 \\\\\n+ p \\\\\n+ p.\n\\end{aligned}\n$$\n此公式必须严格按照所述实现。\n\n程序要求：\n- 对于每个测试用例，执行恰好一次如上所述的完整吉布斯迭代，并使用提供的核算模型计算总操作数。\n- 您的程序应生成单行输出，其中包含三个用例的结果，格式为一个由三个列表组成的逗号分隔列表，每个测试用例一个列表。对于每个测试用例 $j$，输出一个列表，其中包含按顺序采样的 $\\beta^{(1)}$ 条目，后跟该用例的整数总操作数。例如，最终格式应类似于 $[\\,[\\beta^{(1)}_{1},\\dots,\\beta^{(1)}_{p},\\mathrm{Ops}]\\, , \\, [\\dots] \\, , \\, [\\dots]\\,]$，所有浮点数均以标准十进制形式打印。",
            "solution": "该问题要求实现贝叶斯逻辑回归的吉布斯采样器的单次迭代。这是通过使用 Pólya–Gamma (PG) 数据增强方案来实现的，这是一种在具有类逻辑斯谛函数的模型中简化后验推断的强大技术。该过程涉及两个主要步骤：采样辅助的 Pólya–Gamma 潜变量，然后从其完全条件分布中采样回归系数，由于数据增强，该分布变为多元高斯分布。\n\n对于单个观测值 $y_i \\in \\{0, 1\\}$，预测变量向量为 $x_i \\in \\mathbb{R}^p$，系数为 $\\beta \\in \\mathbb{R}^p$ 的逻辑斯谛似然函数由下式给出\n$$ p(y_i \\mid x_i, \\beta) = \\frac{(e^{x_i^\\top \\beta})^{y_i}}{1 + e^{x_i^\\top \\beta}} $$\n通过引入一个服从 Pólya–Gamma 分布（表示为 $\\operatorname{PG}(b, c)$）的潜变量 $\\omega_i$，可以重写此表达式。关键恒等式为：\n$$ \\frac{(e^{\\psi})^a}{(1+e^{\\psi})^b} = 2^{-b} e^{(a-b/2)\\psi} \\int_0^\\infty e^{-\\omega \\psi^2/2} p(\\omega \\mid b, 0) \\, d\\omega $$\n此恒等式的一个变换（通过倾斜 PG 分布）使我们能够将逻辑斯谛似然函数表示为高斯混合形式：\n$$ p(y_i \\mid x_i, \\beta) \\propto e^{\\kappa_i x_i^\\top \\beta} \\int_0^\\infty e^{-\\frac{\\omega_i}{2}(x_i^\\top \\beta)^2} p(\\omega_i \\mid 1, x_i^\\top \\beta) \\, d\\omega_i $$\n其中 $\\kappa_i = y_i - \\frac{1}{2}$。这种表述是有利的，因为在给定 $\\omega_i$ 的条件下，似然项关于 $\\beta$ 与高斯密度成正比。\n\n吉布斯采样器从参数的完全条件分布中迭代抽样。对于此模型，两个步骤是：\n1.  在给定当前系数 $\\beta^{(t)}$ 的情况下，采样潜变量 $\\omega = (\\omega_1, \\dots, \\omega_n)$。\n2.  在给定数据和新采样的潜变量 $\\omega$ 的情况下，采样系数 $\\beta^{(t+1)}$。\n\n**步骤 1：采样潜变量 $\\omega_i$**\n每个 $\\omega_i$ 的完全条件分布是 $\\omega_i \\mid \\beta \\sim \\operatorname{PG}(1, x_i^\\top \\beta)$。问题指定从初始状态 $\\beta^{(0)} = 0_p$（$p$ 维零向量）开始。因此，对于所有 $i=1, \\dots, n$，PG 分布的参数 $c_i = x_i^\\top \\beta^{(0)}$ 均为 $0$。因此，我们需要从 $\\operatorname{PG}(1, 0)$ 中采样 $\\omega_i$。\n问题强制要求为此采样过程使用特定的截断无穷级数表示法：\n$$ \\omega_i \\approx \\frac{1}{2 \\pi^2} \\sum_{k=1}^{K} \\frac{g_k}{\\left(k - \\frac{1}{2}\\right)^2 + \\frac{c_i^2}{4 \\pi^2}} $$\n其中 $g_k \\overset{\\text{i.i.d.}}{\\sim} \\operatorname{Gamma}(1,1)$（等价于速率为 1 的指数分布），$K$ 是指定的截断水平。由于 $c_i=0$，公式简化为：\n$$ \\omega_i \\approx \\frac{1}{2 \\pi^2} \\sum_{k=1}^{K} \\frac{g_k}{\\left(k - \\frac{1}{2}\\right)^2} $$\n对于每个观测值 $i$，我们生成 $K$ 个伽马变量并计算此和。\n\n**步骤 2：采样系数 $\\beta^{(1)}$**\n在给定采样的潜变量 $\\omega_1, \\dots, \\omega_n$ 的情况下，我们从其完全条件分布 $p(\\beta \\mid y, X, \\omega)$ 中采样 $\\beta^{(1)}$。后验分布与先验和增强似然的乘积成正比：\n$$ p(\\beta \\mid y, X, \\omega) \\propto p(\\beta) \\prod_{i=1}^n p(y_i \\mid x_i, \\beta, \\omega_i) $$\n先验为 $\\beta \\sim \\mathcal{N}(0, \\sigma_\\beta^2 I_p)$，其密度为 $p(\\beta) \\propto \\exp\\left(-\\frac{1}{2\\sigma_\\beta^2} \\beta^\\top \\beta\\right)$。\n增强似然为 $\\prod_{i=1}^n \\exp\\left( \\kappa_i x_i^\\top \\beta - \\frac{\\omega_i}{2}(x_i^\\top\\beta)^2 \\right)$。\n结合这些项，$\\beta$ 的对数后验是一个二次函数：\n$$ \\log p(\\beta \\mid \\dots) = C - \\frac{1}{2\\sigma_\\beta^2} \\beta^\\top \\beta + \\sum_{i=1}^n \\left( \\kappa_i x_i^\\top \\beta - \\frac{\\omega_i}{2}(x_i^\\top\\beta)^2 \\right) \\\\ = C - \\frac{1}{2} \\left( \\beta^\\top \\left( \\sum_{i=1}^n \\omega_i x_i x_i^\\top + \\frac{1}{\\sigma_\\beta^2}I_p \\right) \\beta - 2 \\left( \\sum_{i=1}^n \\kappa_i x_i \\right)^\\top \\beta \\right) $$\n这是一个多元高斯分布 $\\mathcal{N}(\\mu_\\beta, \\Sigma_\\beta)$ 的核。精度矩阵 $\\Sigma_\\beta^{-1}$ 和均值 $\\mu_\\beta$ 为：\n$$ A = \\Sigma_\\beta^{-1} = X^\\top W X + \\frac{1}{\\sigma_\\beta^2}I_p $$\n$$ \\mu_\\beta = A^{-1} (X^\\top \\kappa) $$\n其中 $W = \\operatorname{diag}(\\omega_1, \\dots, \\omega_n)$ 且 $\\kappa = (y_1 - \\frac{1}{2}, \\dots, y_n - \\frac{1}{2})^\\top$。\n为了从 $\\mathcal{N}(\\mu_\\beta, A^{-1})$ 中采样 $\\beta^{(1)}$，我们遵循指定的方法：\n1.  计算精度矩阵 $A$ 和向量 $b = X^\\top \\kappa$。\n2.  通过求解线性系统 $A \\mu_\\beta = b$ 来计算均值 $\\mu_\\beta$。\n3.  对精度矩阵进行 Cholesky 分解，$A = LL^\\top$。\n4.  抽取一个独立标准正态变量的向量，$z \\sim \\mathcal{N}(0, I_p)$。\n5.  通过计算 $x = (L^\\top)^{-1} z$ 从 $\\mathcal{N}(0, A^{-1})$ 生成一个样本。这是通过求解三角系统 $L^\\top x = z$ 来完成的。\n6.  通过加上均值获得最终样本：$\\beta^{(1)} = \\mu_\\beta + x$。\n\n最后，使用明确提供的精确公式计算此迭代的计算成本，该公式根据详细的核算模型对每个算术运算和随机变量生成的成本求和。此实现将严格遵守所有指定的数值、用于随机数生成的种子以及每个测试用例的程序步骤。",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian logistic regression problem using one Gibbs iteration \n    with a Pólya–Gamma data augmentation scheme for three test cases.\n    \"\"\"\n    \n    # Store test case parameters\n    test_cases = [\n        {\n            \"n\": 5, \"p\": 2,\n            \"X\": np.array([\n                [1.0, -0.5], [1.2, 0.3], [-0.7, 1.4], [0.0, -1.0], [2.0, 2.0]\n            ]),\n            \"y\": np.array([1, 0, 1, 0, 1]),\n            \"sigma_beta_sq\": 5.0,\n            \"K\": 200,\n            \"case_index\": 1\n        },\n        {\n            \"n\": 8, \"p\": 3,\n            \"X\": np.array([\n                [1.0, 0.5, -1.0], [-0.3, 2.0, 0.7], [0.8, -0.6, 1.5], [1.2, 1.1, 0.0],\n                [-1.5, 0.4, -0.2], [0.0, -1.2, 0.9], [2.0, 0.0, 1.0], [-0.7, -0.8, 0.3]\n            ]),\n            \"y\": np.array([1, 0, 1, 1, 0, 0, 1, 0]),\n            \"sigma_beta_sq\": 2.5,\n            \"K\": 150,\n            \"case_index\": 2\n        },\n        {\n            \"n\": 5, \"p\": 3,\n            \"X\": np.array([\n                [1.0, 1.0, 2.0], [0.9, 1.1, 2.0], [1.1, 0.9, 2.0],\n                [-0.5, -0.5, -1.0], [2.0, 2.0, 4.0]\n            ]),\n            \"y\": np.array([1, 1, 0, 0, 1]),\n            \"sigma_beta_sq\": 10.0,\n            \"K\": 50,\n            \"case_index\": 3\n        }\n    ]\n\n    s0 = 2025\n    results = []\n\n    def sample_polya_gamma(rng, c, K):\n        \"\"\"Samples from PG(1, c) using a truncated series.\"\"\"\n        c2_term = c**2 / (4 * np.pi**2)\n        g_k = rng.standard_gamma(1, size=K)\n        k_vals = np.arange(1, K + 1)\n        denominators = (k_vals - 0.5)**2 + c2_term\n        omega = np.sum(g_k / denominators) / (2 * np.pi**2)\n        return omega\n\n    def calculate_ops(n, p, K):\n        \"\"\"Calculates total operations based on the provided accounting model.\"\"\"\n        term1 = n * (p + (p - 1) + 1 + 1 + 1 + 6 * K)\n        term2 = n * p\n        term3 = p**2 * (2 * n - 1)\n        term4 = 1 + p\n        term5 = n\n        term6 = p * (2 * n - 1)\n        term7 = p**3 // 3\n        term8 = 3 * p**2\n        term9 = p\n        term10 = p\n        return int(term1 + term2 + term3 + term4 + term5 + term6 + term7 + term8 + term9 + term10)\n\n    for case in test_cases:\n        n, p = case[\"n\"], case[\"p\"]\n        X, y = case[\"X\"], case[\"y\"]\n        sigma_beta_sq, K = case[\"sigma_beta_sq\"], case[\"K\"]\n        case_index = case[\"case_index\"]\n\n        # Set up the random number generator for the current case\n        seed = s0 + case_index\n        rng = np.random.default_rng(seed)\n\n        # Initial state beta_0 is a zero vector\n        beta_0 = np.zeros(p)\n\n        # --- Step 1: Sample latent variables omega ---\n        # Since beta_0 is zero, c_i = x_i^T beta_0 = 0 for all i\n        c_vals = X @ beta_0\n        \n        omegas = np.array([sample_polya_gamma(rng, c_vals[i], K) for i in range(n)])\n\n        # --- Step 2: Sample beta^(1) from its full conditional ---\n        W = np.diag(omegas)\n        \n        # Precision matrix of the Gaussian conditional for beta\n        A = X.T @ W @ X + (1 / sigma_beta_sq) * np.identity(p)\n        \n        # Mean component\n        kappa = y - 0.5\n        b = X.T @ kappa\n        \n        # Mean of the Gaussian conditional\n        mu = linalg.solve(A, b, assume_a='pos')\n        \n        # Sample from N(mu, A^-1) using Cholesky decomposition\n        L = linalg.cholesky(A, lower=True)\n        z = rng.standard_normal(size=p)\n        \n        # Solve L^T x = z for x\n        x = linalg.solve_triangular(L.T, z, lower=False)\n        \n        beta_1 = mu + x\n        \n        # --- Calculate total operations ---\n        ops = calculate_ops(n, p, K)\n\n        # Append results for this case\n        case_result = list(beta_1) + [ops]\n        results.append(case_result)\n\n    # Format and print the final output exactly as required\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "设计一个高效的MCMC采样器不仅需要知道如何应用数据增强，更需要理解不同增强方案对算法性能的影响。在层级模型中，中心化（centered）与非中心化（non-centered）参数化是两种常见但性能迥异的增强策略，它们深刻影响着参数在后验分布中的耦合程度。这项练习  要求你首先从理论上通过计算后验相关性来预测哪种参数化方案的混合速度更快，然后通过编写吉布斯采样器并分析样本的自相关性来验证你的预测，从而将抽象的理论分析与具体的算法性能紧密联系起来。",
            "id": "3301996",
            "problem": "考虑一个分层正态模型，该模型包含一个标量潜变量、一个标量位置参数和已知的观测噪声。该模型规定如下。参数$\\mu$的先验分布为$\\mu \\sim \\mathcal{N}(0, v_0)$。在给定$\\mu$和一个方差参数$\\sigma^2$（假设已知）的条件下，潜变量$z$的分布为$z \\mid \\mu, \\sigma^2 \\sim \\mathcal{N}(\\mu, \\sigma^2)$。在给定$z$和一个已知的观测方差$\\tau^2$的条件下，独立观测值$y_1, \\dots, y_n$的分布为$y_i \\mid z, \\tau^2 \\sim \\mathcal{N}(z, \\tau^2)$，其中$i = 1, \\dots, n$。这种生成性规定是潜变量的中心化参数化。\n\n通过引入一个辅助标准正态变量$\\eta$并定义$z = \\mu + \\sigma \\eta$（其中$\\eta \\sim \\mathcal{N}(0, 1)$），可以得到一种非中心化参数化。在这种参数化下，观测模型变为$y_i \\mid \\mu, \\eta, \\sigma^2, \\tau^2 \\sim \\mathcal{N}(\\mu + \\sigma \\eta, \\tau^2)$，对于$i = 1, \\dots, n$独立成立。\n\n任务是，在一个$\\sigma^2$相对于由$\\tau^2$和$n$决定的数据信息$y_1, \\dots, y_n$较大的情况下，使用一个基于依赖性的、有原则的准则（该准则基于高斯后验结构，例如，通过联合二次型所蕴含的后验协方差和相关性，或通过费雪信息考量），预测哪种参数化方案在双块吉布斯采样器中能产生更快的混合速度。这里的采样器分别针对中心化方案下的$(\\mu, z)$和非中心化方案下的$(\\mu, \\eta)$。您的预测应从第一性原理出发，通过分析块之间的后验耦合如何依赖于$\\sigma^2$、$\\tau^2$和$n$来得出。\n\n然后，通过实现两个吉布斯采样器（一个用于中心化参数化，更新$z \\mid \\mu, y$和$\\mu \\mid z$；另一个用于非中心化参数化，更新$\\eta \\mid \\mu, y$和$\\mu \\mid \\eta$）来验证您的预测。在经过适当的老化期后，使用生成的$\\mu$的马尔可夫链蒙特卡洛 (MCMC) 样本计算每个链的经验滞后1阶自相关，并确定在每个测试案例中哪种参数化方案产生更小的自相关（解释为更快的混合速度）。\n\n在您的推导中，请使用以下基本事实和定义作为基础：贝叶斯定理；多元正态分布及其条件分布的形式；对于高斯目标分布，吉布斯更新是条件变量的线性函数这一观察；由二次型所蕴含的后验协方差和相关矩阵的定义。不要直接假设或引用给出混合速率公式的任何结果；相反，您应通过将后验依赖性与线性吉布斯算子的收缩性质联系起来，来推导您的预测。\n\n实现您的程序以处理以下参数值的测试套件。在所有案例中，设置所有$i$的$y_i = 0$，以将结构性依赖效应与随机数据波动分离开来。测试套件是：\n- 案例1（先验方差相对于数据信息较大）：$n = 1$, $\\tau^2 = 10$, $\\sigma^2 = 1000$, $v_0 = 100$。\n- 案例2（数据信息相对于先验方差较强）：$n = 50$, $\\tau^2 = 1$, $\\sigma^2 = 0.01$, $v_0 = 100$。\n- 案例3（中等情况）：$n = 10$, $\\tau^2 = 5$, $\\sigma^2 = 1$, $v_0 = 10$。\n\n您的程序必须：\n- 通过计算中心化参数化（块$\\mu$和$z$）和非中心化参数化（块$\\mu$和$\\eta$）中两个吉布斯块之间的后验相关性，为每个案例推导出一个预测。该计算使用由高斯二次型所蕴含的后验精度矩阵的逆。绝对后验相关性较小的参数化应被预测为混合速度更快。\n- 为每个案例运行两个吉布斯采样器（中心化和非中心化），使用固定的随机种子，每个链至少有$5000$次迭代的老化期，并且总迭代次数至少为$20000$次。计算每个链老化期后采样得到的$\\mu$值的经验滞后1阶自相关，并宣布自相关较小的参数化为经验上混合速度更快的方案。\n- 对于每个案例，输出一个布尔值，指示您基于相关性的预测是否与经验比较结果一致。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表（例如，“[result1,result2,result3]”）。每个结果必须是布尔值“True”或“False”。答案中不涉及物理单位，也不出现角度或百分比。",
            "solution": "用户希望预测并验证分层正态模型的两种参数化（中心化CP和非中心化NCP）中，哪一种能在双块吉布斯采样器中产生更快的混合速度。预测将基于每种参数化中块之间的后验相关性。验证将通过比较参数 $\\mu$ 的生成样本的经验滞后1阶自相关来完成。\n\n### 问题验证\n问题陈述清晰、自成体系，并且在贝叶斯统计和蒙特卡洛方法方面有坚实的科学基础。模型是标准的，参数已提供，预测和验证的标准也定义明确。该问题是有效的，可以按所述方式解决。\n\n### 理论分析与预测准则\n\n双块吉布斯采样器在二元高斯目标分布上的效率由两个块之间的后验相关性决定。设块为 $(\\theta_1, \\theta_2)$。吉布斯采样器从条件分布 $p(\\theta_1|\\theta_2)$ 和 $p(\\theta_2|\\theta_1)$ 中迭代抽样。链的收敛速率由后验相关性的平方 $\\rho^2 = \\text{corr}(\\theta_1, \\theta_2 | \\text{data})^2$ 控制。更小的绝对相关性 $|\\rho|$ 意味着更小的 $\\rho^2$，从而导致更快的收敛和样本中更小的自相关（即更快的混合速度）。\n\n我们的预测将基于计算每种参数化的后验相关性。绝对后验相关性较小的参数化被预测为混合速度更快。\n\n联合后验分布与似然和先验的乘积成正比。对于高斯模型，对数后验是参数的二次函数，$\\log p(\\theta | y) \\propto -\\frac{1}{2} \\theta^T Q \\theta + \\text{线性项}$，其中 $\\theta$ 是参数向量，$Q$ 是后验精度矩阵。对于双变量向量 $\\theta = (\\theta_1, \\theta_2)^T$，后验相关性由 $\\rho = -Q_{12} / \\sqrt{Q_{11}Q_{22}}$ 给出。我们为所有 $i$ 设置数据 $y_i=0$，这意味着样本均值 $\\bar{y}=0$。\n\n**1. 中心化参数化 (CP)**\n\n采样块为 $(\\mu, z)$。对数后验密度为：\n$$ \\log p(\\mu, z | y) \\propto \\log p(y|z, \\tau^2) + \\log p(z|\\mu, \\sigma^2) + \\log p(\\mu | v_0) $$\n当 $y_i=0$ 时，对数似然项为 $\\log p(y|z, \\tau^2) \\propto -n z^2 / (2\\tau^2)$。\n$$ \\log p(\\mu, z | y) \\propto -\\frac{n z^2}{2\\tau^2} - \\frac{(z - \\mu)^2}{2\\sigma^2} - \\frac{\\mu^2}{2v_0} $$\n展开二次项揭示了 $(\\mu, z)$ 的后验精度矩阵：\n$$ \\log p(\\mu, z | y) \\propto -\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma^2} + \\frac{1}{v_0}\\right)\\mu^2 + \\left(\\frac{n}{\\tau^2} + \\frac{1}{\\sigma^2}\\right)z^2 - \\frac{2}{\\sigma^2} \\mu z \\right] $$\n这意味着后验精度矩阵为：\n$$ Q_{\\text{CP}} = \\begin{pmatrix} 1/v_0 + 1/\\sigma^2  -1/\\sigma^2 \\\\ -1/\\sigma^2  n/\\tau^2 + 1/\\sigma^2 \\end{pmatrix} $$\n$\\mu$ 和 $z$ 之间的后验相关性为：\n$$ \\rho_{\\text{CP}} = \\frac{-Q_{\\text{CP},12}}{\\sqrt{Q_{\\text{CP},11} Q_{\\text{CP},22}}} = \\frac{1/\\sigma^2}{\\sqrt{(1/v_0 + 1/\\sigma^2)(n/\\tau^2 + 1/\\sigma^2)}} $$\n\n**2. 非中心化参数化 (NCP)**\n\n采样块为 $(\\mu, \\eta)$，其中 $z = \\mu + \\sigma\\eta$。对数后验密度为：\n$$ \\log p(\\mu, \\eta | y) \\propto \\log p(y|\\mu, \\eta, \\sigma^2, \\tau^2) + \\log p(\\eta) + \\log p(\\mu | v_0) $$\n当 $y_i=0$ 时，数据项基于 $y_i \\sim \\mathcal{N}(\\mu + \\sigma\\eta, \\tau^2)$，所以 $\\log p(y|\\mu,\\eta,\\dots) \\propto -n(\\mu + \\sigma\\eta)^2/(2\\tau^2)$。\n$$ \\log p(\\mu, \\eta | y) \\propto -\\frac{n(\\mu+\\sigma\\eta)^2}{2\\tau^2} - \\frac{\\eta^2}{2} - \\frac{\\mu^2}{2v_0} $$\n展开二次项：\n$$ \\log p(\\mu, \\eta | y) \\propto -\\frac{1}{2} \\left[ \\left(\\frac{n}{\\tau^2} + \\frac{1}{v_0}\\right)\\mu^2 + \\left(\\frac{n\\sigma^2}{\\tau^2} + 1\\right)\\eta^2 + \\frac{2n\\sigma}{\\tau^2}\\mu\\eta \\right] $$\n$(\\mu, \\eta)$ 的后验精度矩阵为：\n$$ Q_{\\text{NCP}} = \\begin{pmatrix} n/\\tau^2 + 1/v_0  n\\sigma/\\tau^2 \\\\ n\\sigma/\\tau^2  n\\sigma^2/\\tau^2 + 1 \\end{pmatrix} $$\n$\\mu$ 和 $\\eta$ 之间的后验相关性为：\n$$ \\rho_{\\text{NCP}} = \\frac{-Q_{\\text{NCP},12}}{\\sqrt{Q_{\\text{NCP},11} Q_{\\text{NCP},22}}} = \\frac{-n\\sigma/\\tau^2}{\\sqrt{(n/\\tau^2 + 1/v_0)(n\\sigma^2/\\tau^2 + 1)}} $$\n其中 $\\sigma = \\sqrt{\\sigma^2}$。\n\n### 经验验证\n\n我们将为两种参数化实现双块吉布斯采样器。采样器所需的条件分布从联合后验推导得出。对于一个具有精度 $Q$ 的通用二元正态分布，条件分布 $p(\\theta_1|\\theta_2)$ 为 $\\mathcal{N}(-Q_{12}\\theta_2/Q_{11}, 1/Q_{11})$。\n\n**CP 采样器：**\n1. 采样 $z^{(t+1)} \\sim p(z | \\mu^{(t)}, y) = \\mathcal{N}\\left(\\frac{\\mu^{(t)}/\\sigma^2}{n/\\tau^2 + 1/\\sigma^2}, \\left(n/\\tau^2 + 1/\\sigma^2\\right)^{-1}\\right)$\n2. 采样 $\\mu^{(t+1)} \\sim p(\\mu | z^{(t+1)}) = \\mathcal{N}\\left(\\frac{z^{(t+1)}/\\sigma^2}{1/v_0 + 1/\\sigma^2}, \\left(1/v_0 + 1/\\sigma^2\\right)^{-1}\\right)$\n\n**NCP 采样器：**\n1. 采样 $\\eta^{(t+1)} \\sim p(\\eta | \\mu^{(t)}, y) = \\mathcal{N}\\left(\\frac{-n\\sigma\\mu^{(t)}/\\tau^2}{n\\sigma^2/\\tau^2 + 1}, \\left(n\\sigma^2/\\tau^2 + 1\\right)^{-1}\\right)$\n2. 采样 $\\mu^{(t+1)} \\sim p(\\mu | \\eta^{(t+1)}, y) = \\mathcal{N}\\left(\\frac{-n\\sigma\\eta^{(t+1)}/\\tau^2}{n/\\tau^2 + 1/v_0}, \\left(n/\\tau^2 + 1/v_0\\right)^{-1}\\right)$\n\n对于每个测试案例，我们运行两个采样器，丢弃老化期序列，然后计算 $\\mu$ 样本的经验滞后1阶自相关。产生较小自相关的参数化被经验地认为是更快的。然后将预测与此经验发现进行比较。\n\n### 测试案例分析\n\n- **案例1（大 $\\sigma^2=1000$，小数据信息 $n/\\tau^2=0.1$）：** 此处 $|\\rho_{\\text{CP}}|$ 很小（$\\approx 0.03$），而 $|\\rho_{\\text{NCP}}|$ 很大（$\\approx 0.95$）。我们预测 CP 会更快。\n- **案例2（小 $\\sigma^2=0.01$，大数据信息 $n/\\tau^2=50$）：** 此处 $|\\rho_{\\text{CP}}|$ 很大（$\\approx 0.82$），而 $|\\rho_{\\text{NCP}}|$ 较小（$\\approx 0.58$）。我们预测 NCP 会更快。\n- **案例3（中等情况）：** 此处 $|\\rho_{\\text{CP}}|$（$\\approx 0.55$）小于 $|\\rho_{\\text{NCP}}|$（$\\approx 0.80$）。我们预测 CP 会更快。\n\n实现将系统地执行这些计算和比较。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Predicts and verifies Gibbs sampler performance for centered vs. non-centered\n    parametrizations of a hierarchical normal model.\n    \"\"\"\n    np.random.seed(0)\n\n    test_cases = [\n        # Case 1 (large prior variance relative to data information)\n        (1, 10.0, 1000.0, 100.0),\n        # Case 2 (strong data information relative to prior variance)\n        (50, 1.0, 0.01, 100.0),\n        # Case 3 (moderate regime)\n        (10, 5.0, 1.0, 10.0),\n    ]\n\n    n_iter = 20000\n    burn_in = 5000\n\n    results = []\n    \n    for case in test_cases:\n        n, tau2, sigma2, v0 = case\n        \n        # --- Theoretical Prediction using Posterior Correlation ---\n        \n        # Centered Parametrization (CP) posterior correlation for (mu, z)\n        q11_cp = 1.0 / sigma2 + 1.0 / v0\n        q22_cp = float(n) / tau2 + 1.0 / sigma2\n        q12_cp = -1.0 / sigma2\n        rho_cp = -q12_cp / np.sqrt(q11_cp * q22_cp)\n\n        # Non-Centered Parametrization (NCP) posterior correlation for (mu, eta)\n        sigma = np.sqrt(sigma2)\n        q11_ncp = float(n) / tau2 + 1.0 / v0\n        q22_ncp = float(n) * sigma2 / tau2 + 1.0\n        q12_ncp = float(n) * sigma / tau2\n        rho_ncp = -q12_ncp / np.sqrt(q11_ncp * q22_ncp)\n        \n        # Prediction: smaller absolute correlation means faster mixing\n        predicted_faster_is_cp = abs(rho_cp)  abs(rho_ncp)\n\n        # --- Empirical Verification using Gibbs Samplers ---\n        \n        # CP Gibbs Sampler\n        mu_samples_cp = np.zeros(n_iter)\n        mu_cp = 0.0\n        # Pre-compute conditional variances\n        s2_z_cp = 1.0 / (n / tau2 + 1.0 / sigma2)\n        s2_mu_cp = 1.0 / (1.0 / v0 + 1.0 / sigma2)\n        for i in range(n_iter):\n            # Update z | mu, y\n            mean_z = s2_z_cp * (mu_cp / sigma2)\n            z = np.random.normal(mean_z, np.sqrt(s2_z_cp))\n            # Update mu | z\n            mean_mu = s2_mu_cp * (z / sigma2)\n            mu_cp = np.random.normal(mean_mu, np.sqrt(s2_mu_cp))\n            mu_samples_cp[i] = mu_cp\n        \n        # NCP Gibbs Sampler\n        mu_samples_ncp = np.zeros(n_iter)\n        mu_ncp = 0.0\n        # Pre-compute conditional variances\n        s2_eta_ncp = 1.0 / (1.0 + n * sigma2 / tau2)\n        s2_mu_ncp = 1.0 / (1.0 / v0 + n / tau2)\n        for i in range(n_iter):\n            # Update eta | mu, y\n            mean_eta = s2_eta_ncp * (-n * sigma * mu_ncp / tau2)\n            eta = np.random.normal(mean_eta, np.sqrt(s2_eta_ncp))\n            # Update mu | eta, y\n            mean_mu = s2_mu_ncp * (-n * sigma * eta / tau2)\n            mu_ncp = np.random.normal(mean_mu, np.sqrt(s2_mu_ncp))\n            mu_samples_ncp[i] = mu_ncp\n\n        # Post-processing: remove burn-in\n        mu_samples_cp_post_burn = mu_samples_cp[burn_in:]\n        mu_samples_ncp_post_burn = mu_samples_ncp[burn_in:]\n\n        # Compute empirical lag-1 autocorrelation for mu\n        # np.corrcoef calculates the correlation matrix; [0, 1] gives the lag-1 ACF\n        acf_cp = np.corrcoef(mu_samples_cp_post_burn[:-1], mu_samples_cp_post_burn[1:])[0, 1]\n        acf_ncp = np.corrcoef(mu_samples_ncp_post_burn[:-1], mu_samples_ncp_post_burn[1:])[0, 1]\n        \n        # Empirical result: smaller ACF means faster mixing\n        empirically_faster_is_cp = acf_cp  acf_ncp\n        \n        # --- Comparison ---\n        # Check if the theoretical prediction matches the empirical result\n        agreement = (predicted_faster_is_cp == empirically_faster_is_cp)\n        results.append(str(agreement))\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}