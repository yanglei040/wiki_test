{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of a Gibbs sampler is determined by how quickly the generated samples decorrelate and explore the full target distribution. This practice provides a foundational analysis of sampler performance by calculating the exact autocorrelation function for a bivariate Gaussian model. By working through this problem, you will see precisely how the correlation $\\rho$ in the target distribution governs the mixing speed of the sampler's components. ",
            "id": "3358507",
            "problem": "Consider a two-variable Gibbs sampler targeting a centered bivariate normal distribution. Let $(X,Y) \\sim \\mathcal{N}(\\boldsymbol{0}, \\Sigma)$ with covariance matrix\n$$\n\\Sigma \\;=\\; \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix},\n$$\nwhere $|\\rho| < 1$. The Gibbs sampler performs a deterministic scan at each iteration $t$: first draw $X$ from its conditional distribution given the current $Y$, then draw $Y$ from its conditional distribution given the newly updated $X$. Let $x_t$ denote the value of $X$ after completion of the full two-variable Gibbs sweep at iteration $t$, so that the sequence $\\{x_t\\}_{t \\geq 0}$ forms a univariate Markov chain embedded in the full Gibbs chain.\n\nUsing only fundamental properties of the multivariate normal distribution (in particular, the form of conditional distributions) and basic definitions from Markov chain Monte Carlo (MCMC), derive the autocorrelation function (ACF) of $\\{x_t\\}$ at lag $k$ under the stationary regime. Express the ACF, denoted $\\mathrm{acf}(k) = \\mathrm{Corr}(x_t, x_{t+k})$, as a closed-form analytic expression in terms of $\\rho$ and $k$. Your final answer must be a single analytic expression. No numerical approximation is required.",
            "solution": "The user has requested the derivation of the autocorrelation function for the sequence of one component generated by a two-variable Gibbs sampler targeting a centered bivariate normal distribution.\n\nThe problem is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It is a standard problem in the theory of Markov chain Monte Carlo methods. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe target distribution is a bivariate normal distribution for the random vector $(X, Y)$, given by $(X,Y) \\sim \\mathcal{N}(\\boldsymbol{0}, \\Sigma)$, where the mean vector is $\\boldsymbol{\\mu} = \\boldsymbol{0}$ and the covariance matrix is\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_X^2 & \\rho \\sigma_X \\sigma_Y \\\\ \\rho \\sigma_X \\sigma_Y & \\sigma_Y^2 \\end{pmatrix} = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\n$$\nThis implies that the marginal distributions are standard normal, $X \\sim \\mathcal{N}(0,1)$ and $Y \\sim \\mathcal{N}(0,1)$, with $\\sigma_X^2 = \\sigma_Y^2 = 1$. The parameter $\\rho$ is the correlation coefficient between $X$ and $Y$, with the constraint $|\\rho| < 1$ ensuring the covariance matrix is positive definite.\n\nThe primary step is to determine the conditional distributions required for the Gibbs sampler. For a general bivariate normal distribution as specified above, the conditional distribution of $X$ given $Y=y$ is also normal:\n$$\nX | Y=y \\sim \\mathcal{N}\\left(\\mu_X + \\frac{\\rho \\sigma_X}{\\sigma_Y}(y - \\mu_Y), \\sigma_X^2(1-\\rho^2)\\right)\n$$\nIn our specific case, with $\\mu_X = \\mu_Y = 0$ and $\\sigma_X = \\sigma_Y = 1$, the conditional distributions simplify significantly.\nThe conditional distribution of $X$ given $Y=y$ is:\n$$\np(x|y) \\sim \\mathcal{N}(\\rho y, 1-\\rho^2)\n$$\nBy symmetry, the conditional distribution of $Y$ given $X=x$ is:\n$$\np(y|x) \\sim \\mathcal{N}(\\rho x, 1-\\rho^2)\n$$\n\nThe Gibbs sampling algorithm proceeds in iterations. Let $(X_t, Y_t)$ be the state of the chain after iteration $t$. The problem statement defines a deterministic scan where, to get from state $t$ to $t+1$, we first update $X$ and then $Y$. Let the state at the beginning of iteration $t+1$ be $(X_t, Y_t)$. The updates are:\n$1$. Draw an intermediate value $X'_{t+1}$ from the conditional distribution of $X$ given $Y_t$: $X'_{t+1} \\sim p(X | Y=Y_t)$.\n$2$. Draw the new value $Y_{t+1}$ from the conditional distribution of $Y$ given this new $X'_{t+1}$: $Y_{t+1} \\sim p(Y | X=X'_{t+1})$.\nThe state after the full sweep is $(X'_{t+1}, Y_{t+1})$. The problem specifies that the sequence of interest is $\\{x_t\\}_{t \\geq 0}$, where $x_t$ is the value of the $X$ component after the sweep at iteration $t$. Adopting this notation, we set $X_t = X'_t$. So, the transition from iteration $t$ to $t+1$ involves the sequence of states $(X_t, Y_t) \\rightarrow (X_{t+1}, Y_{t+1})$, generated as follows:\n$$\nX_{t+1} \\sim p(X | Y=Y_t)\n$$\n$$\nY_{t+1} \\sim p(Y | X=X_{t+1})\n$$\nWe seek the autocorrelation function $\\mathrm{acf}(k) = \\mathrm{Corr}(X_t, X_{t+k})$ for the stationary sequence $\\{X_t\\}$. In the stationary regime, the moments of the distribution are time-invariant. In particular, from the target distribution, we have $E[X_t] = 0$ and $\\mathrm{Var}(X_t) = E[X_t^2] = 1$ for all $t$.\n\nThe autocorrelation at lag $k$ is defined as:\n$$\n\\mathrm{acf}(k) = \\mathrm{Corr}(X_t, X_{t+k}) = \\frac{\\mathrm{Cov}(X_t, X_{t+k})}{\\sqrt{\\mathrm{Var}(X_t) \\mathrm{Var}(X_{t+k})}}\n$$\nSince $\\mathrm{Var}(X_t) = \\mathrm{Var}(X_{t+k}) = 1$ in stationarity, the autocorrelation is equal to the autocovariance:\n$$\n\\mathrm{acf}(k) = \\mathrm{Cov}(X_t, X_{t+k}) = E[X_t X_{t+k}] - E[X_t]E[X_{t+k}] = E[X_t X_{t+k}]\n$$\nTo find $E[X_t X_{t+k}]$, we can use the law of iterated expectations. First, let us establish the relationship between $X_{t+1}$ and $X_t$. The update can be described by a generative model based on the conditional distributions:\n$$\nX_{t+1} = \\rho Y_t + \\varepsilon_{t+1}, \\quad \\text{where } \\varepsilon_{t+1} \\sim \\mathcal{N}(0, 1-\\rho^2)\n$$\nAnd from the previous step (or from the stationary distribution of $(X_t, Y_t)$), we know that $Y_t$ and $X_t$ are related via:\n$$\nY_t = \\rho X_t + \\delta_t, \\quad \\text{where } \\delta_t \\sim \\mathcal{N}(0, 1-\\rho^2)\n$$\nThe noise term $\\delta_t$ is independent of $X_t$, and $\\varepsilon_{t+1}$ is independent of $Y_t$ (and thus also of $X_t$ and $\\delta_t$).\nSubstituting the expression for $Y_t$ into the one for $X_{t+1}$, we get:\n$$\nX_{t+1} = \\rho(\\rho X_t + \\delta_t) + \\varepsilon_{t+1} = \\rho^2 X_t + \\rho \\delta_t + \\varepsilon_{t+1}\n$$\nThis reveals that the sequence $\\{X_t\\}$ follows a first-order autoregressive process, AR($1$). Let's use this structure to find the conditional expectation $E[X_{t+1} | X_t]$.\n$$\nE[X_{t+1} | X_t=x_t] = E[\\rho^2 x_t + \\rho \\delta_t + \\varepsilon_{t+1} | X_t=x_t] = \\rho^2 x_t + \\rho E[\\delta_t] + E[\\varepsilon_{t+1}] = \\rho^2 x_t\n$$\nSo, $E[X_{t+1} | X_t] = \\rho^2 X_t$.\nNow, we can find the autocovariance for lag $k=1$:\n$$\nE[X_t X_{t+1}] = E[E[X_t X_{t+1} | X_t]] = E[X_t E[X_{t+1} | X_t]] = E[X_t (\\rho^2 X_t)] = \\rho^2 E[X_t^2] = \\rho^2 \\mathrm{Var}(X_t) = \\rho^2\n$$\nThus, $\\mathrm{acf}(1) = \\rho^2$.\n\nTo find the ACF for an arbitrary lag $k$, we find the conditional expectation $E[X_{t+k} | X_t]$ by iterating the conditional expectation one step at a time:\n$$\nE[X_{t+k} | X_t] = E[E[X_{t+k} | X_{t+k-1}] | X_t] = E[\\rho^2 X_{t+k-1} | X_t] = \\rho^2 E[X_{t+k-1} | X_t]\n$$\nThis gives a recursive relation. Unfolding this recursion, we get:\n$$\nE[X_{t+k} | X_t] = \\rho^2 E[X_{t+k-1} | X_t] = (\\rho^2)^2 E[X_{t+k-2} | X_t] = \\dots = (\\rho^2)^k E[X_t | X_t] = (\\rho^2)^k X_t\n$$\nNow we can compute the autocovariance for lag $k$:\n$$\n\\mathrm{Cov}(X_t, X_{t+k}) = E[X_t X_{t+k}] = E[E[X_t X_{t+k} | X_t]] = E[X_t E[X_{t+k} | X_t]]\n$$\nSubstituting the result for the conditional expectation:\n$$\n\\mathrm{Cov}(X_t, X_{t+k}) = E[X_t ((\\rho^2)^k X_t)] = (\\rho^2)^k E[X_t^2] = (\\rho^2)^k \\mathrm{Var}(X_t) = (\\rho^2)^k \\cdot 1 = \\rho^{2k}\n$$\nSince the autocorrelation equals the autocovariance in this case, we have our final result. The autocorrelation function of the embedded Markov chain $\\{x_t\\}$ at lag $k$ is:\n$$\n\\mathrm{acf}(k) = \\rho^{2k}\n$$\nThis result shows that the correlation between samples decays geometrically. The rate of decay is determined by $\\rho^2$. A value of $\\rho$ close to $\\pm 1$ implies high correlation in the target distribution, which leads to slow decay ($\\rho^2$ close to $1$) and thus slow mixing of the Gibbs sampler for the $X$ component. Conversely, when $\\rho$ is close to $0$, the variables are nearly independent, and the samples generated by the Gibbs sampler are nearly independent ($\\rho^2$ close to $0$).",
            "answer": "$$\n\\boxed{\\rho^{2k}}\n$$"
        },
        {
            "introduction": "Moving from idealized models to practical applications, Gibbs samplers can exhibit pathological behavior when the target posterior is multimodal. This exercise explores the classic problem of \"label switching\" in Bayesian mixture models, a direct consequence of posterior symmetry. You will analyze why a standard Gibbs sampler fails to mix between modes and evaluate the validity and effectiveness of different strategies to address this common challenge. ",
            "id": "3358492",
            "problem": "Consider a Bayesian finite mixture model with $2$ Normal components for observed data $x_1,\\dots,x_n$, where each $x_i \\in \\mathbb{R}$. The sampling model is\n$$\nx_i \\mid z_i,\\mu_1,\\mu_2,\\sigma^2 \\sim \\mathcal{N}\\big(\\mu_{z_i},\\sigma^2\\big), \\quad z_i \\in \\{1,2\\},\n$$\nwith latent allocations $z_1,\\dots,z_n$, a known variance $\\sigma^2>0$, and component means $\\mu_1,\\mu_2 \\in \\mathbb{R}$. The mixture weight $\\pi \\in (0,1)$ satisfies\n$$\n\\mathbb{P}(z_i=1\\mid \\pi) = \\pi,\\quad \\mathbb{P}(z_i=2\\mid \\pi) = 1-\\pi,\n$$\nand the prior is symmetric:\n$$\n\\pi \\sim \\text{Beta}(\\alpha,\\alpha),\\quad \\mu_1 \\stackrel{\\text{ind}}{\\sim} \\mathcal{N}(0,\\tau^2),\\quad \\mu_2 \\stackrel{\\text{ind}}{\\sim} \\mathcal{N}(0,\\tau^2),\n$$\nwith fixed hyperparameters $\\alpha>0$ and $\\tau^2>0$. A data-augmented Gibbs sampler cyclically updates $(z_1,\\dots,z_n)$ given $(\\pi,\\mu_1,\\mu_2)$ and $x_{1:n}$, then $(\\mu_1,\\mu_2)$ given $(z_{1:n},x_{1:n})$, then $\\pi$ given $z_{1:n}$, and repeats. Assume the observed data exhibit two well-separated empirical clusters, in the sense that the likelihood under two distinct location parameters near $-a$ and $+a$ for some $a>0$ is strongly favored over single-cluster explanations, and the priors are as given above.\n\nFrom the foundational definitions of Bayesâ€™ theorem and Gibbs sampling (sampling from full conditional distributions to leave the target posterior invariant), analyze the posterior structure and the mixing behavior of this Gibbs sampler under the given symmetry. Select all statements that are correct.\n\nA. Because the prior is exchangeable in $(\\mu_1,\\mu_2)$ and the likelihood depends on labels only through the pair $(\\{\\mu_1,\\mu_2\\},\\{z_i\\})$, the joint posterior is invariant to the relabeling map $(\\mu_1,\\mu_2,\\pi,z_{1:n}) \\mapsto (\\mu_2,\\mu_1,1-\\pi,\\tilde z_{1:n})$ with $\\tilde z_i = 3-z_i$, and for well-separated clusters the marginal posterior of $(\\mu_1,\\mu_2)$ is bimodal with two symmetric modes related by label switching.\n\nB. In the regime of large $n$ and well-separated clusters, the single-site data-augmented Gibbs sampler mixes rapidly between the two labelings, because each sweep updates all $z_i$ and hence can effectively flip the allocation vector to the opposite labeling with probability bounded away from $0$.\n\nC. Enforcing the identifiability constraint $\\mu_1 \\le \\mu_2$ by rejecting any proposed update that violates it both removes the label-switching multimodality and still leaves the unconstrained posterior on $(\\mu_1,\\mu_2,\\pi,z_{1:n})$ invariant.\n\nD. A tempered transition or parallel tempering scheme that introduces an inverse-temperature parameter $\\beta \\in (0,1]$ to flatten the likelihood at $\\beta<1$, while maintaining detailed balance for transitions within and between temperatures, can substantially increase the frequency of label switching in the base chain at $\\beta=1$ without changing its invariant distribution.\n\nE. A posteriori relabeling of saved draws by swapping component labels to enforce the order $\\mu_1 \\le \\mu_2$ yields valid estimates for expectations of any exchangeable function $f(\\mu_1,\\mu_2)$ that satisfies $f(\\mu_1,\\mu_2)=f(\\mu_2,\\mu_1)$, but such post-processing does not alter the mixing properties of the underlying Markov chain.\n\nF. Collapsing out $(\\pi,\\mu_1,\\mu_2)$ analytically to obtain a Gibbs sampler on $z_{1:n}$ eliminates the label-switching symmetry and yields a unimodal posterior over $z_{1:n}$ under the symmetric priors given above.\n\nSelect all that apply.",
            "solution": "An analysis of the problem statement and the provided options is conducted below.\n\n### Validation of the Problem Statement\n\n**Step 1: Extract Givens**\n-   **Model:** Bayesian finite mixture model with $k=2$ Normal components.\n-   **Data:** $x_1, \\dots, x_n$ with $x_i \\in \\mathbb{R}$.\n-   **Sampling Model (Likelihood):** $x_i \\mid z_i, \\mu_1, \\mu_2, \\sigma^2 \\sim \\mathcal{N}(\\mu_{z_i}, \\sigma^2)$, where $z_i \\in \\{1, 2\\}$ is a latent allocation variable.\n-   **Known Parameter:** Variance $\\sigma^2 > 0$ is known.\n-   **Mixture Weights:** $\\mathbb{P}(z_i = 1 \\mid \\pi) = \\pi$ and $\\mathbb{P}(z_i = 2 \\mid \\pi) = 1 - \\pi$ for $\\pi \\in (0, 1)$.\n-   **Priors:**\n    -   $\\pi \\sim \\text{Beta}(\\alpha, \\alpha)$ for a fixed $\\alpha > 0$.\n    -   $\\mu_1 \\stackrel{\\text{ind}}{\\sim} \\mathcal{N}(0, \\tau^2)$ for a fixed $\\tau^2 > 0$.\n    -   $\\mu_2 \\stackrel{\\text{ind}}{\\sim} \\mathcal{N}(0, \\tau^2)$.\n    -   The priors on $\\pi, \\mu_1, \\mu_2$ are mutually independent.\n-   **Algorithm:** A data-augmented Gibbs sampler that cyclically updates:\n    1.  $(z_1, \\dots, z_n)$ given $(\\pi, \\mu_1, \\mu_2)$ and data.\n    2.  $(\\mu_1, \\mu_2)$ given $(z_1, \\dots, z_n)$ and data.\n    3.  $\\pi$ given $(z_1, \\dots, z_n)$.\n-   **Assumption:** The data $x_{1:n}$ exhibit two well-separated empirical clusters.\n-   **Task:** Analyze the posterior structure and the mixing behavior of the Gibbs sampler.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem describes a standard Bayesian Gaussian mixture model. The specified priors (Normal, Beta) and the data-augmented Gibbs sampling strategy are foundational and widely used in Bayesian statistics and machine learning. There are no violations of scientific or mathematical principles.\n-   **Well-Posed:** The problem is well-posed. The model is fully specified with proper priors, ensuring a proper posterior distribution. The task is to analyze known properties and behaviors of this standard model and sampler, which is a meaningful and solvable problem in statistical theory.\n-   **Objective:** The problem is stated in precise, objective, and standard terminology of Bayesian statistics. The assumption of \"well-separated clusters\" is a common and clear theoretical condition under which a mixture model's behavior is studied.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. There are no ambiguities, contradictions, or fallacies. I will now proceed to the solution.\n\n### Derivation and Option Analysis\n\nThe core of this problem is the \"label switching\" phenomenon, which arises from the non-identifiability of the mixture components. The model's likelihood is invariant to permutations of the component labels if the other parameters are permuted accordingly.\n\nLet $\\theta = (\\pi, \\mu_1, \\mu_2)$ be the parameter vector for the marginal likelihood (after integrating out $z_{1:n}$). The likelihood for a single observation is $p(x_i | \\theta) = \\pi \\mathcal{N}(x_i | \\mu_1, \\sigma^2) + (1-\\pi) \\mathcal{N}(x_i | \\mu_2, \\sigma^2)$.\nIf we define a permuted parameter vector $\\theta' = (1-\\pi, \\mu_2, \\mu_1)$, the likelihood is unchanged:\n$p(x_i | \\theta') = (1-\\pi) \\mathcal{N}(x_i | \\mu_2, \\sigma^2) + \\pi \\mathcal{N}(x_i | \\mu_1, \\sigma^2) = p(x_i | \\theta)$.\nSince the full likelihood is $L(\\theta | x_{1:n}) = \\prod_i p(x_i | \\theta)$, it is also invariant to this permutation.\n\nThe prior distribution is $p(\\theta) = p(\\pi) p(\\mu_1) p(\\mu_2)$. The prior on $\\mu_1, \\mu_2$ is $p(\\mu_1, \\mu_2) = \\mathcal{N}(\\mu_1 | 0, \\tau^2) \\mathcal{N}(\\mu_2 | 0, \\tau^2)$, which is symmetric: $p(\\mu_1, \\mu_2) = p(\\mu_2, \\mu_1)$. The prior on $\\pi$ is $p(\\pi) = \\text{Beta}(\\pi | \\alpha, \\alpha)$, which is symmetric about $0.5$, meaning $p(\\pi) = p(1-\\pi)$.\nThus, the prior density is also invariant under the permutation: $p(\\theta') = p(\\theta)$.\n\nSince the posterior is proportional to the product of the likelihood and the prior, $p(\\theta | x_{1:n}) \\propto L(\\theta | x_{1:n}) p(\\theta)$, the posterior is also invariant: $p(\\theta | x_{1:n}) = p(\\theta' | x_{1:n})$. This implies that if $(\\hat{\\pi}, \\hat{\\mu}_1, \\hat{\\mu}_2)$ is a mode of the posterior, then $(1-\\hat{\\pi}, \\hat{\\mu}_2, \\hat{\\mu}_1)$ must also be a mode.\n\n**A. Because the prior is exchangeable in $(\\mu_1,\\mu_2)$ and the likelihood depends on labels only through the pair $(\\{\\mu_1,\\mu_2\\},\\{z_i\\})$, the joint posterior is invariant to the relabeling map $(\\mu_1,\\mu_2,\\pi,z_{1:n}) \\mapsto (\\mu_2,\\mu_1,1-\\pi,\\tilde z_{1:n})$ with $\\tilde z_i = 3-z_i$, and for well-separated clusters the marginal posterior of $(\\mu_1,\\mu_2)$ is bimodal with two symmetric modes related by label switching.**\n\nLet's check the invariance of the full joint distribution of data and parameters, $p(x_{1:n}, z_{1:n}, \\mu_1, \\mu_2, \\pi)$. This can be factored as $p(x_{1:n}|z_{1:n},\\mu_1,\\mu_2)p(z_{1:n}|\\pi)p(\\pi)p(\\mu_1)p(\\mu_2)$.\nLet's apply the transformation: $(\\mu_1,\\mu_2,\\pi,z_{1:n}) \\mapsto (\\mu_1',\\mu_2',\\pi',\\tilde{z}_{1:n}) = (\\mu_2,\\mu_1,1-\\pi,\\tilde{z}_{1:n})$.\n1.  **Likelihood:** $p(x_{1:n}| \\tilde{z}_{1:n}, \\mu_1', \\mu_2') = \\prod_i \\mathcal{N}(x_i | \\mu'_{\\tilde{z}_i}, \\sigma^2)$. If $z_i=1$, then $\\tilde{z}_i=2$, so the mean is $\\mu'_2 = \\mu_1$. If $z_i=2$, then $\\tilde{z}_i=1$, so the mean is $\\mu'_1 = \\mu_2$. In both cases, the mean is $\\mu_{z_i}$. So $p(x_{1:n}|\\tilde{z}_{1:n},\\mu_2,\\mu_1) = p(x_{1:n}|z_{1:n},\\mu_1,\\mu_2)$.\n2.  **Allocation Prior:** $p(\\tilde{z}_{1:n}|\\pi') = \\prod_i (\\pi')^{\\mathbb{I}(\\tilde{z}_i=1)} (1-\\pi')^{\\mathbb{I}(\\tilde{z}_i=2)} = \\prod_i (1-\\pi)^{\\mathbb{I}(z_i=2)} (\\pi)^{\\mathbb{I}(z_i=1)} = p(z_{1:n}|\\pi)$.\n3.  **Parameter Priors:** $p(\\pi')p(\\mu_1')p(\\mu_2') = p(1-\\pi)p(\\mu_2)p(\\mu_1)$. Due to the symmetric Beta prior and i.i.d. Normal priors, this equals $p(\\pi)p(\\mu_1)p(\\mu_2)$.\n\nThe joint distribution is invariant, which implies the full posterior $p(\\mu_1, \\mu_2, \\pi, z_{1:n} | x_{1:n})$ is invariant under this map. Marginalizing out $\\pi$ and $z_{1:n}$ gives the marginal posterior $p(\\mu_1, \\mu_2 | x_{1:n})$, which must be symmetric, i.e., $p(\\mu_1, \\mu_2 | x_{1:n}) = p(\\mu_2, \\mu_1 | x_{1:n})$. For well-separated clusters, the data will strongly support distinct values for the means, say near locations $a$ and $b$ ($a \\neq b$). This creates a posterior mode near $(\\mu_1, \\mu_2) = (a, b)$. Due to symmetry, there must be a corresponding mode near $(b, a)$. Hence, the marginal posterior for $(\\mu_1, \\mu_2)$ is bimodal.\n**Verdict: Correct.**\n\n**B. In the regime of large $n$ and well-separated clusters, the single-site data-augmented Gibbs sampler mixes rapidly between the two labelings, because each sweep updates all $z_i$ and hence can effectively flip the allocation vector to the opposite labeling with probability bounded away from $0$.**\n\nThe Gibbs sampler updates variables from their full conditional distributions. Consider the state is near one posterior mode: $(\\mu_1, \\mu_2) \\approx (a,b)$ where $a$ and $b$ are the centers of the two well-separated clusters. When updating $z_i$ for a data point $x_i$ from the first cluster (i.e., $x_i \\approx a$), the conditional probabilities are $\\mathbb{P}(z_i=1|\\dots) \\propto \\pi \\mathcal{N}(x_i|a, \\sigma^2)$ and $\\mathbb{P}(z_i=2|\\dots) \\propto (1-\\pi) \\mathcal{N}(x_i|b, \\sigma^2)$. As the clusters are well-separated, $\\mathcal{N}(x_i|a, \\sigma^2) \\gg \\mathcal{N}(x_i|b, \\sigma^2)$, so $\\mathbb{P}(z_i=1|\\dots) \\approx 1$. To switch to the other labeled mode, all $z_i$ for the first cluster must flip to label $2$, and all $z_i$ for the second cluster must flip to label $1$. A single Gibbs sweep updates each $z_i$ independently (given the parameters), so the probability of all $z_i$ flipping to the \"wrong\" component is vanishingly small. The sampler gets trapped in one of the modes, and the probability of transitioning to the other mode across the low-probability barrier separating them is extremely low. Thus, the sampler mixes very slowly.\n**Verdict: Incorrect.**\n\n**C. Enforcing the identifiability constraint $\\mu_1 \\le \\mu_2$ by rejecting any proposed update that violates it both removes the label-switching multimodality and still leaves the unconstrained posterior on $(\\mu_1,\\mu_2,\\pi,z_{1:n})$ invariant.**\n\nImposing the constraint $\\mu_1 \\le \\mu_2$ by rejection sampling on Gibbs updates modifies the sampler's transition kernel. The original Gibbs sampler is designed to have the unconstrained posterior $p(\\theta|x_{1:n})$ as its stationary distribution. The modified sampler's state space is restricted to the subspace $\\mathcal{S} = \\{(\\mu_1,\\mu_2,\\pi,z_{1:n}) : \\mu_1 \\le \\mu_2 \\}$. The invariant distribution of this new Markov chain is the posterior conditional on this subspace, i.e., $p(\\theta|x_{1:n}, \\theta \\in \\mathcal{S})$, which is proportional to $p(\\theta|x_{1:n})\\mathbb{I}(\\mu_1 \\le \\mu_2)$. This is a different distribution from the unconstrained posterior. The claim that the procedure \"leaves the unconstrained posterior ... invariant\" is false. The procedure changes the invariant distribution of the sampler.\n**Verdict: Incorrect.**\n\n**D. A tempered transition or parallel tempering scheme that introduces an inverse-temperature parameter $\\beta \\in (0,1]$ to flatten the likelihood at $\\beta<1$, while maintaining detailed balance for transitions within and between temperatures, can substantially increase the frequency of label switching in the base chain at $\\beta=1$ without changing its invariant distribution.**\n\nParallel tempering (or Metropolis-Coupled MCMC) runs multiple Markov chains in parallel targeting powered posteriors $p_\\beta(\\theta|x_{1:n}) \\propto [L(\\theta|x_{1:n})]^\\beta p(\\theta)$, for a sequence of inverse temperatures $1 = \\beta_1 > \\beta_2 > \\dots > \\beta_M > 0$. For $\\beta < 1$, the posterior landscape is flattened, lowering the probability barriers between modes. Chains at high temperatures (low $\\beta$) can jump between the label-switched modes easily. The algorithm includes swap moves between chains at adjacent temperatures, which are accepted or rejected with a Metropolis-Hastings probability that maintains detailed balance for the joint system. This allows the cold chain ($\\beta=1$) to access states from the other modes discovered by the hot chains. This procedure is a valid MCMC algorithm, and by construction, the marginal stationary distribution of the chain at $\\beta=1$ is the original target posterior $p(\\theta|x_{1:n})$. The method is specifically designed to improve mixing in multimodal distributions, and it does so by increasing the frequency of jumps between modes (label switching).\n**Verdict: Correct.**\n\n**E. A posteriori relabeling of saved draws by swapping component labels to enforce the order $\\mu_1 \\le \\mu_2$ yields valid estimates for expectations of any exchangeable function $f(\\mu_1,\\mu_2)$ that satisfies $f(\\mu_1,\\mu_2)=f(\\mu_2,\\mu_1)$, but such post-processing does not alter the mixing properties of the underlying Markov chain.**\n\nThis statement has two parts.\n1. \"such post-processing does not alter the mixing properties of the underlying Markov chain\": This is true. Post-processing is applied to the saved output of the chain. It has no effect on the generation of the samples by the Markov chain itself. If the chain mixes poorly and gets stuck in one mode, post-processing does not fix this.\n2. \"yields valid estimates for expectations of any exchangeable function $f(\\mu_1,\\mu_2)$\": An exchangeable function is one like $f(\\mu_1, \\mu_2) = \\mu_1+\\mu_2$ or $f(\\mu_1, \\mu_2) = \\mu_1^2 + \\mu_2^2$, where $f(\\mu_1, \\mu_2) = f(\\mu_2, \\mu_1)$. The true expectation is $\\mathbb{E}[f] = \\int f(\\mu_1, \\mu_2) p(\\mu_1, \\mu_2|x_{1:n}) d\\mu_1 d\\mu_2$. Because both $f$ and $p(\\mu_1, \\mu_2|x_{1:n})$ are symmetric with respect to swapping $\\mu_1$ and $\\mu_2$, the expectation over the full space is equal to the expectation over one of the modes (e.g., the region where $\\mu_1 \\le \\mu_2$), up to a normalization constant. In fact, an MCMC average for $f$ will be correct even if the sampler stays in one mode, because the loss of samples from the other mode is exactly compensated by the symmetry of $f$. The post-processing is unnecessary for such functions, but does no harm to the estimate. For non-exchangeable functions (e.g., $f(\\mu_1, \\mu_2) = \\mu_1$), this would not hold. The statement is carefully restricted to exchangeable functions, for which it is true.\n**Verdict: Correct.**\n\n**F. Collapsing out $(\\pi,\\mu_1,\\mu_2)$ analytically to obtain a Gibbs sampler on $z_{1:n}$ eliminates the label-switching symmetry and yields a unimodal posterior over $z_{1:n}$ under the symmetric priors given above.**\n\nA collapsed Gibbs sampler works on the marginal posterior of the allocations, $p(z_{1:n}|x_{1:n})$. This is proportional to $p(x_{1:n}|z_{1:n}) p(z_{1:n})$. We need to check if the symmetry is eliminated.\nLet $\\tilde{z}_{1:n}$ be the label-swapped allocation vector, $\\tilde{z}_i = 3-z_i$.\nThe integrated prior for allocations is $p(z_{1:n}) = \\int p(z_{1:n}|\\pi)p(\\pi)d\\pi = \\frac{B(n_1+\\alpha, n_2+\\alpha)}{B(\\alpha,\\alpha)}$, where $n_k$ is the count of allocations to component $k$. Since $B(a,b)=B(b,a)$, and swapping labels means swapping $n_1$ and $n_2$, $p(\\tilde{z}_{1:n}) = p(z_{1:n})$. The prior on allocations is symmetric.\nThe marginal likelihood is $p(x_{1:n}|z_{1:n}) = \\int p(x_{1:n}|z_{1:n}, \\mu_1, \\mu_2)p(\\mu_1,\\mu_2)d\\mu_1 d\\mu_2$. Due to the i.i.d. priors on $\\mu_1, \\mu_2$, this decomposes into a product of two identical-form integrals, one over the data in component 1 and one over the data in component 2. Swapping the labels $z_{1:n} \\to \\tilde{z}_{1:n}$ is equivalent to swapping the sets of data points fed into these two integrals, which leaves their product unchanged. Thus, $p(x_{1:n}|\\tilde{z}_{1:n}) = p(x_{1:n}|z_{1:n})$.\nSince both terms are symmetric, the posterior $p(z_{1:n}|x_{1:n})$ is also symmetric. The label-switching symmetry is not eliminated; it is inherited by the marginal posterior on $z_{1:n}$. Consequently, if there is a modal allocation $z^*$, there is also a symmetric mode $\\tilde{z}^*$. The posterior is not unimodal.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ADE}$$"
        },
        {
            "introduction": "While the Gibbs framework is robust, creating customized or \"collapsed\" versions requires careful theoretical validation to ensure the correct stationary distribution is preserved. This practice presents a critical case study in a hierarchical model where the order of operations in a partially collapsed sampler determines its validity. By analyzing two seemingly similar schemes, you will learn why one fails and one succeeds, providing a vital lesson in the rigorous construction of MCMC algorithms. ",
            "id": "3358515",
            "problem": "Consider the following linear-Gaussian hierarchical model for three real-valued variables $x$, $y$, and $z$:\n- The prior for $z$ is $z \\sim \\mathcal{N}(0,\\tau^{2})$.\n- The conditional distribution for $y$ given $z$ is $y \\mid z \\sim \\mathcal{N}(\\alpha z,\\sigma_{y}^{2})$.\n- The conditional distribution for $x$ given $y$ and $z$ is $x \\mid y,z \\sim \\mathcal{N}(\\beta y + \\gamma z,\\sigma_{x}^{2})$.\n\nThe joint distribution $\\pi(x,y,z)$ is thus well-defined and strictly positive for all $(x,y,z) \\in \\mathbb{R}^{3}$.\n\nA partially collapsed Gibbs sampler is constructed by replacing one full-conditional update with a collapsed update that integrates out a nuisance variable. Specifically, consider the collapsed update for $z$ based on $p(z \\mid x)$, obtained by marginalizing over $y$ from the joint $\\pi(x,y,z)$.\n\nTwo different composition orders of the partially collapsed Gibbs sampler are proposed:\n\n- Scheme A (update $y$ first): starting from $(x^{(t)},y^{(t)},z^{(t)})$, perform\n  1. Sample $y^{(t+1)} \\sim p(y \\mid z^{(t)})$,\n  2. Sample $z^{(t+1)} \\sim p(z \\mid x^{(t)})$,\n  3. Sample $x^{(t+1)} \\sim p(x \\mid y^{(t+1)}, z^{(t+1)})$.\n\n- Scheme B (update $z$ first): starting from $(x^{(t)},y^{(t)},z^{(t)})$, perform\n  1. Sample $z^{(t+1)} \\sim p(z \\mid x^{(t)})$,\n  2. Sample $y^{(t+1)} \\sim p(y \\mid z^{(t+1)})$,\n  3. Sample $x^{(t+1)} \\sim p(x \\mid y^{(t+1)}, z^{(t+1)})$.\n\nUsing only the foundational definitions of Gibbs sampling, Markov chain stationarity, and Gaussian conditioning, do the following:\n\n1. Derive the collapsed conditional $p(z \\mid x)$ in closed form by marginalizing out $y$ from the joint $\\pi(x,y,z)$. Express its mean and variance explicitly in terms of $(\\alpha,\\beta,\\gamma,\\tau^{2},\\sigma_{y}^{2},\\sigma_{x}^{2})$.\n\n2. Analyze the stationarity of Scheme A and Scheme B by explicitly evaluating the invariance condition for the composed Markov kernel acting on $\\pi(x,y,z)$. Explain precisely why one of the orders fails to preserve $\\pi(x,y,z)$ as the stationary distribution while the other maintains stationarity.\n\n3. For the correctly ordered scheme that maintains stationarity, compute the marginal variance of $x$ under the stationary distribution $\\pi(x,y,z)$ for the parameter values\n$$\n\\alpha = 0.7,\\quad \\beta = 1.2,\\quad \\gamma = -0.5,\\quad \\tau^{2} = 2.5,\\quad \\sigma_{y}^{2} = 1.3,\\quad \\sigma_{x}^{2} = 0.9.\n$$\nRound your final numeric answer for the marginal variance of $x$ to four significant figures. No physical units are required.",
            "solution": "The problem requires a three-part analysis of a partially collapsed Gibbs sampler for a linear-Gaussian hierarchical model. First, we validate the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Hierarchical model structure:\n  - $p(z) = \\mathcal{N}(z \\mid 0, \\tau^{2})$\n  - $p(y \\mid z) = \\mathcal{N}(y \\mid \\alpha z, \\sigma_{y}^{2})$\n  - $p(x \\mid y, z) = \\mathcal{N}(x \\mid \\beta y + \\gamma z, \\sigma_{x}^{2})$\n- Joint distribution $\\pi(x,y,z) = p(x \\mid y,z) p(y \\mid z) p(z)$ is strictly positive.\n- Sampling Scheme A:\n  1. Sample $y^{(t+1)} \\sim p(y \\mid z^{(t)})$\n  2. Sample $z^{(t+1)} \\sim p(z \\mid x^{(t)})$\n  3. Sample $x^{(t+1)} \\sim p(x \\mid y^{(t+1)}, z^{(t+1)})$\n- Sampling Scheme B:\n  1. Sample $z^{(t+1)} \\sim p(z \\mid x^{(t)})$\n  2. Sample $y^{(t+1)} \\sim p(y \\mid z^{(t+1)})$\n  3. Sample $x^{(t+1)} \\sim p(x \\mid y^{(t+1)}, z^{(t+1)})$\n- Task 1: Derive the mean and variance of the collapsed conditional $p(z \\mid x)$.\n- Task 2: Analyze the stationarity of Schemes A and B with respect to $\\pi(x,y,z)$.\n- Task 3: For the stationary scheme, compute the marginal variance of $x$, $Var(x)$, under $\\pi(x,y,z)$ for the parameters: $\\alpha = 0.7$, $\\beta = 1.2$, $\\gamma = -0.5$, $\\tau^{2} = 2.5$, $\\sigma_{y}^{2} = 1.3$, $\\sigma_{x}^{2} = 0.9$, rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard problem in the field of computational statistics (MCMC methods), dealing with the properties of Gibbs samplers for a linear-Gaussian model. All necessary definitions and parameters are provided, and there are no internal contradictions. The tasks are specific and lead to unique, verifiable mathematical results. The problem does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n### Solution\n\n**1. Derivation of the Collapsed Conditional $p(z \\mid x)$**\n\nTo derive $p(z \\mid x)$, we first need the joint distribution of $(x, z)$, which is obtained by marginalizing $y$ out of the full joint distribution $\\pi(x, y, z)$. The model can be written as a system of linear equations with Gaussian noise:\n$z = \\epsilon_z$, where $\\epsilon_z \\sim \\mathcal{N}(0, \\tau^2)$.\n$y = \\alpha z + \\epsilon_y$, where $\\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2)$.\n$x = \\beta y + \\gamma z + \\epsilon_x$, where $\\epsilon_x \\sim \\mathcal{N}(0, \\sigma_x^2)$.\nThe noise terms $\\epsilon_z$, $\\epsilon_y$, $\\epsilon_x$ are mutually independent.\n\nBy substituting the expression for $y$ into the expression for $x$, we can express $x$ in terms of $z$ and the independent noise terms:\n$$ x = \\beta(\\alpha z + \\epsilon_y) + \\gamma z + \\epsilon_x = (\\alpha\\beta + \\gamma)z + \\beta\\epsilon_y + \\epsilon_x $$\nSince $x$ and $z$ are linear combinations of Gaussian random variables, they are jointly Gaussian. We can now find the parameters of this joint distribution, $\\pi(x,z)$. The means are $E[z]=0$ and $E[x] = E[(\\alpha\\beta + \\gamma)z + \\beta\\epsilon_y + \\epsilon_x] = 0$.\n\nThe variances and covariance are:\n$Var(z) = E[z^2] = \\tau^2$.\n$Var(x) = Var((\\alpha\\beta + \\gamma)z + \\beta\\epsilon_y + \\epsilon_x)$. Due to the independence of $z$ (which is $\\epsilon_z$), $\\epsilon_y$, and $\\epsilon_x$:\n$$ Var(x) = (\\alpha\\beta + \\gamma)^2 Var(z) + \\beta^2 Var(\\epsilon_y) + Var(\\epsilon_x) = (\\alpha\\beta + \\gamma)^2 \\tau^2 + \\beta^2 \\sigma_y^2 + \\sigma_x^2 $$\n$Cov(x, z) = E[xz] = E[ ((\\alpha\\beta + \\gamma)z + \\beta\\epsilon_y + \\epsilon_x) z ] = E[(\\alpha\\beta + \\gamma)z^2 + \\beta\\epsilon_y z + \\epsilon_x z]$.\nUsing linearity of expectation and independence ($E[\\epsilon_y z] = E[\\epsilon_y]E[z] = 0$, $E[\\epsilon_x z] = 0$):\n$$ Cov(x, z) = (\\alpha\\beta + \\gamma)E[z^2] = (\\alpha\\beta + \\gamma)\\tau^2 $$\nSo, the joint distribution of $(z, x)$ is:\n$$ \\begin{pmatrix} z \\\\ x \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\tau^2 & (\\alpha\\beta + \\gamma)\\tau^2 \\\\ (\\alpha\\beta + \\gamma)\\tau^2 & (\\alpha\\beta + \\gamma)^2 \\tau^2 + \\beta^2 \\sigma_y^2 + \\sigma_x^2 \\end{pmatrix} \\right) $$\nUsing the standard formula for conditional Gaussian distributions, the distribution of $z$ given $x$, denoted $p(z \\mid x)$, is a Gaussian $\\mathcal{N}(\\mu_{z|x}, \\sigma_{z|x}^2)$ with:\n$$ \\mu_{z|x} = E[z \\mid x] = E[z] + \\frac{Cov(z,x)}{Var(x)}(x - E[x]) = \\frac{(\\alpha\\beta + \\gamma)\\tau^2}{(\\alpha\\beta + \\gamma)^2 \\tau^2 + \\beta^2 \\sigma_y^2 + \\sigma_x^2} x $$\n$$ \\sigma_{z|x}^2 = Var(z \\mid x) = Var(z) - \\frac{Cov(z,x)^2}{Var(x)} = \\tau^2 - \\frac{\\left((\\alpha\\beta + \\gamma)\\tau^2\\right)^2}{(\\alpha\\beta + \\gamma)^2 \\tau^2 + \\beta^2 \\sigma_y^2 + \\sigma_x^2} $$\n\n**2. Analysis of Stationarity**\n\nA Markov chain with transition kernel $K(s' \\mid s)$ has a stationary distribution $\\pi(s)$ if the invariance condition holds: $\\int K(s' \\mid s) \\pi(s) ds = \\pi(s')$. For Gibbs sampling, the overall kernel is a composition of individual update kernels.\n\n**Scheme B Analysis:** The transition from state $s^{(t)}=(x^{(t)},y^{(t)},z^{(t)})$ to $s^{(t+1)}=(x^{(t+1)},y^{(t+1)},z^{(t+1)})$ is defined by a sequence of three draws. The combined transition probability (the kernel) is:\n$$ K_B(x',y',z' \\mid x,y,z) = p(x' \\mid y',z') p(y' \\mid z') p(z' \\mid x) $$\nwhere we have relabeled the next state variables with primes for clarity. The old state is $(x,y,z)$ and the new is $(x',y',z')$.\n\nTo check for invariance, we evaluate the integral:\n$$ \\int \\pi(x,y,z) K_B(x',y',z' \\mid x,y,z) \\,dx\\,dy\\,dz $$\nSubstituting the kernel and the joint distribution $\\pi(x,y,z) = p(x|y,z)p(y|z)p(z)$:\n$$ \\int p(x|y,z)p(y|z)p(z) \\cdot p(x' \\mid y',z') p(y' \\mid z') p(z' \\mid x) \\,dx\\,dy\\,dz $$\nThe terms $p(x' \\mid y',z')$ and $p(y' \\mid z')$ can be moved outside the integral:\n$$ p(x' \\mid y',z') p(y' \\mid z') \\int p(z' \\mid x) \\, [p(x|y,z)p(y|z)p(z)] \\,dx\\,dy\\,dz $$\nThe term in the square brackets is $\\pi(x,y,z)$. The integral becomes:\n$$ \\int p(z' \\mid x) \\pi(x,y,z) \\,dx\\,dy\\,dz = \\int_x p(z' \\mid x) \\left( \\int_y \\int_z \\pi(x,y,z) \\,dz\\,dy \\right) \\,dx = \\int_x p(z' \\mid x) \\pi(x) \\,dx $$\nwhere $\\pi(x)$ is the marginal distribution of $x$. By the definition of conditional probability, $p(z' \\mid x) = \\frac{\\pi(x,z')}{\\pi(x)}$.\nThe integral is therefore $\\int_x \\frac{\\pi(x,z')}{\\pi(x)} \\pi(x) \\,dx = \\int_x \\pi(x,z') \\,dx = \\pi(z')$, the marginal distribution of $z'$.\nSo, the full expression for the next-state distribution is:\n$$ p(x' \\mid y',z') p(y' \\mid z') \\pi(z') $$\nFrom the model definition, $\\pi(z')$ is identical to the prior $p(z')$. Therefore, this expression is exactly the definition of the joint distribution $\\pi(x',y',z') = p(x'|y',z')p(y'|z')p(z')$.\nThus, Scheme B preserves $\\pi(x,y,z)$ as its stationary distribution. It is a valid sampler. This structure is valid because it represents an ancestral sampling scheme for the joint $(x',y',z')$ where the root node $z'$ is drawn from a collapsed conditional distribution $p(z'|x)$ that itself preserves the marginal $\\pi(x,z)$.\n\n**Scheme A Analysis:** The transition kernel for this scheme is:\n$$ K_A(x',y',z' \\mid x,y,z) = p(x' \\mid y',z') p(z' \\mid x) p(y' \\mid z) $$\nThe critical difference is that the new draw $y'$ depends on the old state variable $z$, not the new one $z'$.\nLet's check the invariance condition:\n$$ \\int \\pi(x,y,z) K_A(x',y',z' \\mid x,y,z) \\,dx\\,dy\\,dz $$\n$$ = p(x' \\mid y',z') \\int p(z' \\mid x) p(y' \\mid z) \\pi(x,y,z) \\,dx\\,dy\\,dz $$\nThe integral term does not simplify to $\\pi(y', z')$ as needed. Let's analyze it:\n$$ \\int_x \\int_y \\int_z p(z' \\mid x) p(y' \\mid z) p(x|y,z) p(y|z) p(z) \\,dz\\,dy\\,dx $$\nRearranging the integration gives:\n$$ \\int_x p(z'|x) \\left( \\int_z p(y'|z) p(z) \\left( \\int_y p(x|y,z)p(y|z) \\,dy \\right) \\,dz \\right) \\,dx $$\nThe innermost integral is $\\int_y p(x,y|z) \\,dy = p(x|z)$, where $p(x|z) = \\int p(x|y,z)p(y|z) \\text{d}y$. So the expression becomes:\n$$ \\int_x p(z'|x) \\left( \\int_z p(y'|z) p(x|z) p(z) \\,dz \\right) \\,dx $$\nThe term $\\int_z p(y'|z) p(x,z) \\,dz$ does not simplify to a convenient form like $\\pi(x,y')$. It represents an expectation with respect to $\\pi(z)$ and depends on both $x$ and $y'$. Because this integral does not evaluate to $\\pi(y',z')$, the overall expression does not reduce to $\\pi(x',y',z')$.\nThe structural flaw in Scheme A is the broken chain of conditioning. The draw of $y'$ uses \"stale\" information ($z=z^{(t)}$) while the subsequent draw of $x'$ uses \"updated\" information ($z'=z^{(t+1)}$). This failure to propagate the most recent information at each step prevents the composed kernel from preserving the target stationary distribution. Therefore, Scheme A is not a valid Gibbs sampler for the target $\\pi(x,y,z)$.\n\n**3. Marginal Variance of $x$**\n\nThe correctly ordered scheme that maintains stationarity is Scheme B. The problem asks for the marginal variance of $x$ under the stationary distribution $\\pi(x,y,z)$. This quantity, $Var(x)$, is an attribute of the target distribution itself, not the sampler dynamics. We have already derived the expression for $Var(x)$ in Part 1.\n$$ Var(x) = (\\alpha\\beta + \\gamma)^2 \\tau^2 + \\beta^2 \\sigma_y^2 + \\sigma_x^2 $$\nWe substitute the given parameter values:\n$\\alpha = 0.7$\n$\\beta = 1.2$\n$\\gamma = -0.5$\n$\\tau^{2} = 2.5$\n$\\sigma_{y}^{2} = 1.3$\n$\\sigma_{x}^{2} = 0.9$\n\nFirst, calculate the term $\\alpha\\beta + \\gamma$:\n$$ \\alpha\\beta + \\gamma = (0.7)(1.2) + (-0.5) = 0.84 - 0.5 = 0.34 $$\nNow, substitute this and the other values into the variance formula:\n$$ Var(x) = (0.34)^2 (2.5) + (1.2)^2 (1.3) + 0.9 $$\n$$ Var(x) = (0.1156)(2.5) + (1.44)(1.3) + 0.9 $$\n$$ Var(x) = 0.289 + 1.872 + 0.9 $$\n$$ Var(x) = 3.061 $$\nThe question asks to round the result to four significant figures. The calculated value $3.061$ already has four significant figures.",
            "answer": "$$\\boxed{3.061}$$"
        }
    ]
}