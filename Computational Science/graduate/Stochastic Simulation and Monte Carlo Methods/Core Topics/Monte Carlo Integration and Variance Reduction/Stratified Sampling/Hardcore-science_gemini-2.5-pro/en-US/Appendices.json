{
    "hands_on_practices": [
        {
            "introduction": "Stratified sampling is fundamentally a technique for variance reduction, and its power is most evident when dealing with functions that have high variability across their domain. This practice  provides a quintessential example by focusing on the estimation of tail expectations for a heavy-tailed Pareto distribution. You will derive the optimal Neyman allocation from first principles and implement it to demonstrate quantitatively how stratifying the domain into a \"bulk\" and \"tail\" region dramatically improves estimation efficiency compared to crude Monte Carlo sampling.",
            "id": "3349482",
            "problem": "Consider a heavy-tailed random variable $X$ with Pareto distribution characterized by scale parameter $x_m>0$ and shape parameter $\\alpha>2$, with Probability Density Function (PDF) $f_X(x)=\\alpha x_m^{\\alpha} x^{-(\\alpha+1)}$ for $x\\ge x_m$. Define the indicator-weighted tail integrand $g(x)=x\\mathbf{1}\\{x>x_0\\}$ for a threshold $x_0>x_m$. The goal is to estimate the tail expectation $\\mu=E[g(X)]$ using Monte Carlo (MC) methods and to show that a two-stratum stratified sampling design with Neyman allocation reduces the coefficient of variation compared to crude MC.\n\nStarting only from fundamental definitions of expectation, variance, and stratified sampling, and without using pre-specified shortcut formulas, perform the following:\n\n1. Define the two strata: the bulk stratum $h=0$ as $[x_m,x_0]$ and the tail stratum $h=1$ as $(x_0,\\infty)$. Let the stratum weights be $w_h=P(X\\in\\text{stratum }h)$ and the within-stratum standard deviations of the integrand be $\\sigma_h=\\sqrt{\\mathrm{Var}(g(X)\\mid X\\in\\text{stratum }h)}$. Using first principles, derive symbolic expressions needed to compute $\\mu$, $\\mathrm{Var}(g(X))$, and $\\mathrm{Var}(g(X)\\mid X>x_0)$ for the Pareto model.\n\n2. Construct the stratified estimator $\\hat{\\mu}_{\\text{strat}}=\\sum_{h=0}^1 w_h \\bar{g}_h$, where $\\bar{g}_h$ is the sample average of $g(X)$ computed from $n_h$ independent samples drawn from the conditional distribution of $X$ restricted to stratum $h$. Starting from the variance of $\\hat{\\mu}_{\\text{strat}}$ under independence, derive the Neyman allocation rule $n_h\\propto w_h \\sigma_h$ that minimizes the variance subject to a fixed total sample size $N=\\sum_{h=0}^1 n_h$. Explain how this rule applies to the present $g(x)$, noting any zero-variance contributions.\n\n3. Define the Coefficient of Variation (CV) of an unbiased estimator $\\hat{\\mu}$ as $\\mathrm{CV}=\\sqrt{\\mathrm{Var}(\\hat{\\mu})}/\\mu$. For crude MC with $N$ independent draws from $X$, express $\\mathrm{CV}_{\\text{crude}}$ in terms of $\\mathrm{Var}(g(X))$, $\\mu$, and $N$. For the two-stratum design with Neyman allocation, express $\\mathrm{CV}_{\\text{strat}}$ in terms of the tail stratum quantities and $N$.\n\n4. Implement a program that computes $\\mathrm{CV}_{\\text{crude}}$, $\\mathrm{CV}_{\\text{strat}}$, and their ratio $r=\\mathrm{CV}_{\\text{strat}}/\\mathrm{CV}_{\\text{crude}}$ using the analytic expressions you derive. Use the following test suite of parameter values to evaluate and compare the coefficients of variation:\n   - Test case $1$: $(\\alpha,x_m,x_0,N)=(3.0,1.0,2.0,10000)$\n   - Test case $2$: $(\\alpha,x_m,x_0,N)=(2.1,1.0,5.0,10000)$\n   - Test case $3$: $(\\alpha,x_m,x_0,N)=(5.0,1.0,10.0,10000)$\n   - Test case $4$: $(\\alpha,x_m,x_0,N)=(3.0,1.0,1.1,10000)$\n\n5. For each test case, also compute the boolean $b$ indicating improvement, defined as $b=(\\mathrm{CV}_{\\text{strat}}<\\mathrm{CV}_{\\text{crude}})$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following flattened order over the test cases: $[\\mathrm{CV}_{\\text{crude}}^{(1)},\\mathrm{CV}_{\\text{strat}}^{(1)},r^{(1)},b^{(1)},\\mathrm{CV}_{\\text{crude}}^{(2)},\\mathrm{CV}_{\\text{strat}}^{(2)},r^{(2)},b^{(2)},\\mathrm{CV}_{\\text{crude}}^{(3)},\\mathrm{CV}_{\\text{strat}}^{(3)},r^{(3)},b^{(3)},\\mathrm{CV}_{\\text{crude}}^{(4)},\\mathrm{CV}_{\\text{strat}}^{(4)},r^{(4)},b^{(4)}]$. All numerical answers must be dimensionless real numbers (floats) or booleans. No physical units are involved in this problem.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. It is a standard application of variance reduction techniques in Monte Carlo integration and is free of contradictions or fallacies. Therefore, the problem is deemed valid. We proceed with a full solution derived from first principles.\n\nThe solution requires deriving symbolic expressions for several statistical quantities related to a Pareto-distributed random variable $X$. The Probability Density Function (PDF) is given by $f_X(x) = \\alpha x_m^{\\alpha} x^{-(\\alpha+1)}$ for $x \\ge x_m$, with parameters $\\alpha > 2$ and $x_m > 0$. The integrand of interest is $g(x) = x\\mathbf{1}\\{x>x_0\\}$, where $x_0 > x_m$.\n\n### Part 1: Derivation of Fundamental Quantities\n\nFirst, we derive the Cumulative Distribution Function (CDF) of $X$, denoted $F_X(x) = P(X \\le x)$, which is essential for calculating stratum weights. For $x \\ge x_m$:\n$$\nF_X(x) = \\int_{x_m}^{x} f_X(t) dt = \\int_{x_m}^{x} \\alpha x_m^{\\alpha} t^{-(\\alpha+1)} dt = \\alpha x_m^{\\alpha} \\left[ \\frac{t^{-\\alpha}}{-\\alpha} \\right]_{x_m}^{x} = -x_m^{\\alpha} (x^{-\\alpha} - x_m^{-\\alpha}) = 1 - \\left(\\frac{x_m}{x}\\right)^{\\alpha}\n$$\n\nThe two strata are the bulk stratum $h=0$ corresponding to the interval $[x_m, x_0]$ and the tail stratum $h=1$ corresponding to $(x_0, \\infty)$. The stratum weights $w_h = P(X \\in \\text{stratum } h)$ are:\n$$\nw_0 = P(x_m \\le X \\le x_0) = F_X(x_0) - F_X(x_m) = \\left(1 - \\left(\\frac{x_m}{x_0}\\right)^{\\alpha}\\right) - 0 = 1 - \\left(\\frac{x_m}{x_0}\\right)^{\\alpha}\n$$\n$$\nw_1 = P(X > x_0) = 1 - F_X(x_0) = 1 - \\left(1 - \\left(\\frac{x_m}{x_0}\\right)^{\\alpha}\\right) = \\left(\\frac{x_m}{x_0}\\right)^{\\alpha}\n$$\nIt is readily verified that $w_0 + w_1 = 1$.\n\nNow, we derive the moments of the integrand $g(X) = X\\mathbf{1}\\{X>x_0\\}$.\nThe expectation $\\mu = \\mathrm{E}[g(X)]$ is given by:\n$$\n\\mu = \\int_{x_m}^{\\infty} g(x) f_X(x) dx = \\int_{x_m}^{\\infty} x\\mathbf{1}\\{x>x_0\\} (\\alpha x_m^{\\alpha} x^{-(\\alpha+1)}) dx\n$$\nThe indicator function $\\mathbf{1}\\{x>x_0\\}$ restricts the integration domain to $(x_0, \\infty)$:\n$$\n\\mu = \\int_{x_0}^{\\infty} x (\\alpha x_m^{\\alpha} x^{-(\\alpha+1)}) dx = \\alpha x_m^{\\alpha} \\int_{x_0}^{\\infty} x^{-\\alpha} dx = \\alpha x_m^{\\alpha} \\left[ \\frac{x^{-\\alpha+1}}{1-\\alpha} \\right]_{x_0}^{\\infty}\n$$\nSince $\\alpha > 2$, the term at the upper limit is $0$.\n$$\n\\mu = \\alpha x_m^{\\alpha} \\left(0 - \\frac{x_0^{1-\\alpha}}{1-\\alpha}\\right) = \\frac{\\alpha}{\\alpha-1} x_m^{\\alpha} x_0^{1-\\alpha}\n$$\nTo find the variance $\\mathrm{Var}(g(X)) = \\mathrm{E}[g(X)^2] - \\mu^2$, we first compute the second moment $\\mathrm{E}[g(X)^2]$:\n$$\n\\mathrm{E}[g(X)^2] = \\int_{x_0}^{\\infty} x^2 (\\alpha x_m^{\\alpha} x^{-(\\alpha+1)}) dx = \\alpha x_m^{\\alpha} \\int_{x_0}^{\\infty} x^{1-\\alpha} dx = \\alpha x_m^{\\alpha} \\left[ \\frac{x^{2-\\alpha}}{2-\\alpha} \\right]_{x_0}^{\\infty}\n$$\nSince $\\alpha > 2$, the term at the upper limit is again $0$.\n$$\n\\mathrm{E}[g(X)^2] = \\alpha x_m^{\\alpha} \\left(0 - \\frac{x_0^{2-\\alpha}}{2-\\alpha}\\right) = \\frac{\\alpha}{\\alpha-2} x_m^{\\alpha} x_0^{2-\\alpha}\n$$\nThe variance is therefore:\n$$\n\\mathrm{Var}(g(X)) = \\frac{\\alpha}{\\alpha-2} x_m^{\\alpha} x_0^{2-\\alpha} - \\left(\\frac{\\alpha}{\\alpha-1} x_m^{\\alpha} x_0^{1-\\alpha}\\right)^2\n$$\nNext, we determine the within-stratum variances $\\sigma_h^2 = \\mathrm{Var}(g(X) \\mid X \\in \\text{stratum } h)$.\nFor stratum $h=0$ ($[x_m, x_0]$): The integrand $g(X) = X\\mathbf{1}\\{X>x_0\\}$ is identically $0$ for any $X$ in this stratum. The variance of a constant is $0$.\n$$\n\\sigma_0^2 = \\mathrm{Var}(g(X) \\mid X \\in [x_m, x_0]) = 0 \\implies \\sigma_0 = 0\n$$\nFor stratum $h=1$ ($(x_0, \\infty)$): Here, $g(X) = X$. We need to compute $\\sigma_1^2 = \\mathrm{Var}(X \\mid X > x_0)$. The conditional PDF is $f_{X|X>x_0}(x) = f_X(x)/P(X>x_0) = f_X(x)/w_1$ for $x > x_0$.\n$$\nf_{X|X>x_0}(x) = \\frac{\\alpha x_m^{\\alpha} x^{-(\\alpha+1)}}{(x_m/x_0)^{\\alpha}} = \\alpha x_0^{\\alpha} x^{-(\\alpha+1)}\n$$\nThis is the PDF of a Pareto distribution with shape $\\alpha$ and scale $x_0$. We require its variance. The conditional mean is:\n$$\n\\mathrm{E}[X \\mid X > x_0] = \\int_{x_0}^{\\infty} x (\\alpha x_0^{\\alpha} x^{-(\\alpha+1)}) dx = \\frac{\\alpha}{\\alpha-1} x_0\n$$\nThe conditional second moment is:\n$$\n\\mathrm{E}[X^2 \\mid X > x_0] = \\int_{x_0}^{\\infty} x^2 (\\alpha x_0^{\\alpha} x^{-(\\alpha+1)}) dx = \\frac{\\alpha}{\\alpha-2} x_0^2\n$$\nThe conditional variance is:\n$$\n\\sigma_1^2 = \\mathrm{Var}(X \\mid X > x_0) = \\mathrm{E}[X^2 \\mid X > x_0] - (\\mathrm{E}[X \\mid X > x_0])^2 = \\frac{\\alpha}{\\alpha-2} x_0^2 - \\left(\\frac{\\alpha}{\\alpha-1} x_0\\right)^2\n$$\n$$\n\\sigma_1^2 = x_0^2 \\alpha \\left(\\frac{1}{\\alpha-2} - \\frac{\\alpha}{(\\alpha-1)^2}\\right) = x_0^2 \\alpha \\left(\\frac{(\\alpha-1)^2 - \\alpha(\\alpha-2)}{(\\alpha-2)(\\alpha-1)^2}\\right) = x_0^2 \\alpha \\left(\\frac{\\alpha^2 - 2\\alpha + 1 - \\alpha^2 + 2\\alpha}{(\\alpha-2)(\\alpha-1)^2}\\right)\n$$\n$$\n\\sigma_1^2 = \\frac{\\alpha x_0^2}{(\\alpha-2)(\\alpha-1)^2}\n$$\n\n### Part 2: Neyman Allocation\n\nThe stratified estimator is $\\hat{\\mu}_{\\text{strat}} = \\sum_{h=0}^1 w_h \\bar{g}_h$, where $\\bar{g}_h$ is the mean of $n_h$ samples from stratum $h$. The variance of this estimator is:\n$$\n\\mathrm{Var}(\\hat{\\mu}_{\\text{strat}}) = \\sum_{h=0}^1 w_h^2 \\mathrm{Var}(\\bar{g}_h) = \\sum_{h=0}^1 w_h^2 \\frac{\\sigma_h^2}{n_h} = \\frac{w_0^2 \\sigma_0^2}{n_0} + \\frac{w_1^2 \\sigma_1^2}{n_1}\n$$\nNeyman allocation minimizes this variance subject to a fixed total sample size $N=n_0+n_1$. We use the method of Lagrange multipliers. Let $L(n_0, n_1, \\lambda) = \\frac{w_0^2 \\sigma_0^2}{n_0} + \\frac{w_1^2 \\sigma_1^2}{n_1} + \\lambda(n_0+n_1-N)$.\nSetting partial derivatives to zero, $\\frac{\\partial L}{\\partial n_h} = -\\frac{w_h^2 \\sigma_h^2}{n_h^2} + \\lambda = 0$, yields $n_h = \\frac{w_h \\sigma_h}{\\sqrt{\\lambda}}$. This shows $n_h \\propto w_h \\sigma_h$. The exact allocation is $n_h = N \\frac{w_h \\sigma_h}{\\sum_k w_k \\sigma_k}$.\n\nIn our specific problem, $\\sigma_0 = 0$. The allocation formula prescribes:\n$$\nn_0 = N \\frac{w_0 \\sigma_0}{w_0 \\sigma_0 + w_1 \\sigma_1} = 0\n$$\n$$\nn_1 = N \\frac{w_1 \\sigma_1}{w_0 \\sigma_0 + w_1 \\sigma_1} = N \\frac{w_1 \\sigma_1}{w_1 \\sigma_1} = N\n$$\nThe rule dictates that all sampling effort should be focused on stratum $1$, as stratum $0$ has zero variance and its contribution to the integral is known with certainty to be zero. The total variance under Neyman allocation can be generally expressed as $\\mathrm{Var}(\\hat{\\mu}_{\\text{strat}}) = \\frac{1}{N}(\\sum_h w_h \\sigma_h)^2$. For our case, this simplifies to:\n$$\n\\mathrm{Var}(\\hat{\\mu}_{\\text{strat}}) = \\frac{1}{N}(w_0 \\sigma_0 + w_1 \\sigma_1)^2 = \\frac{(w_1 \\sigma_1)^2}{N}\n$$\n\n### Part 3: Coefficient of Variation\n\nThe Coefficient of Variation ($\\mathrm{CV}$) for an unbiased estimator $\\hat{\\mu}$ is $\\mathrm{CV} = \\sqrt{\\mathrm{Var}(\\hat{\\mu})}/\\mu$.\nFor crude Monte Carlo with $N$ samples, $\\hat{\\mu}_{\\text{crude}}$ is unbiased with variance $\\mathrm{Var}(\\hat{\\mu}_{\\text{crude}}) = \\mathrm{Var}(g(X))/N$.\n$$\n\\mathrm{CV}_{\\text{crude}} = \\frac{\\sqrt{\\mathrm{Var}(g(X))/N}}{\\mu} = \\frac{\\sqrt{\\mathrm{Var}(g(X))}}{\\mu \\sqrt{N}}\n$$\nFor stratified sampling with Neyman allocation, $\\hat{\\mu}_{\\text{strat}}$ is also unbiased, with variance $\\mathrm{Var}(\\hat{\\mu}_{\\text{strat}}) = (w_1 \\sigma_1)^2/N$.\n$$\n\\mathrm{CV}_{\\text{strat}} = \\frac{\\sqrt{(w_1 \\sigma_1)^2/N}}{\\mu} = \\frac{w_1 \\sigma_1}{\\mu \\sqrt{N}}\n$$\n\n### Part 4 & 5: Implementation Formulas\n\nThe program will implement the following formulas derived above for each test case $(\\alpha, x_m, x_0, N)$:\n1.  Mean: $\\mu = \\frac{\\alpha}{\\alpha-1} x_m^{\\alpha} x_0^{1-\\alpha}$\n2.  Variance of $g(X)$: $\\mathrm{Var}(g(X)) = \\left(\\frac{\\alpha}{\\alpha-2} x_m^{\\alpha} x_0^{2-\\alpha}\\right) - \\mu^2$\n3.  $\\mathrm{CV}_{\\text{crude}} = \\frac{\\sqrt{\\mathrm{Var}(g(X))}}{\\mu \\sqrt{N}}$\n4.  Tail weight: $w_1 = (x_m/x_0)^{\\alpha}$\n5.  Tail stdev: $\\sigma_1 = \\sqrt{\\frac{\\alpha x_0^2}{(\\alpha-2)(\\alpha-1)^2}} = \\frac{x_0}{\\alpha-1}\\sqrt{\\frac{\\alpha}{\\alpha-2}}$\n6.  $\\mathrm{CV}_{\\text{strat}} = \\frac{w_1 \\sigma_1}{\\mu \\sqrt{N}}$\n7.  Ratio: $r = \\mathrm{CV}_{\\text{strat}} / \\mathrm{CV}_{\\text{crude}} = \\frac{w_1 \\sigma_1}{\\sqrt{\\mathrm{Var}(g(X))}}$\n8.  Improvement: $b = (\\mathrm{CV}_{\\text{strat}} < \\mathrm{CV}_{\\text{crude}})$. As shown by the law of total variance, $\\mathrm{Var}(g(X)) = w_1 \\sigma_1^2 + w_0 w_1 (\\mu/w_1)^2 = w_1\\sigma_1^2 + w_0/w_1 \\mu^2$. The ratio squared is $r^2 = \\frac{w_1^2 \\sigma_1^2}{\\mathrm{Var}(g(X))} = \\frac{w_1^2 \\sigma_1^2}{w_1 \\sigma_1^2 + w_0 w_1 (\\mu/w_1)^2} < 1$ as long as $w_0>0$, which is guaranteed by $x_0>x_m$. Thus, $b$ will always be true.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares coefficients of variation for crude Monte Carlo\n    and stratified sampling estimators of a tail expectation.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, x_m, x_0, N)\n        (3.0, 1.0, 2.0, 10000),\n        (2.1, 1.0, 5.0, 10000),\n        (5.0, 1.0, 10.0, 10000),\n        (3.0, 1.0, 1.1, 10000),\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, x_m, x_0, N = case\n\n        # 1. Calculate the true mean mu = E[g(X)]\n        mu = (alpha / (alpha - 1)) * (x_m**alpha) * (x_0**(1 - alpha))\n\n        # 2. Calculate the variance of g(X) for crude MC\n        # E[g(X)^2]\n        E_g_sq = (alpha / (alpha - 2)) * (x_m**alpha) * (x_0**(2 - alpha))\n        var_g = E_g_sq - mu**2\n\n        # 3. Calculate CV_crude\n        cv_crude = np.sqrt(var_g) / (mu * np.sqrt(N))\n\n        # 4. Calculate quantities for stratified sampling\n        # Stratum 1 (tail) weight w_1\n        w_1 = (x_m / x_0)**alpha\n        \n        # Stratum 1 (tail) standard deviation sigma_1\n        sigma_1_sq = (alpha * x_0**2) / ((alpha - 2) * (alpha - 1)**2)\n        sigma_1 = np.sqrt(sigma_1_sq)\n\n        # 5. Calculate CV_strat\n        cv_strat = (w_1 * sigma_1) / (mu * np.sqrt(N))\n\n        # 6. Calculate the ratio and improvement boolean\n        ratio = cv_strat / cv_crude\n        improvement = cv_strat  cv_crude\n\n        # Append results in the specified order\n        results.append(cv_crude)\n        results.append(cv_strat)\n        results.append(ratio)\n        results.append(improvement)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond its role in variance reduction, stratification is a powerful tool for controlling for confounding variables and ensuring valid statistical conclusions. This exercise  moves from pure efficiency to conceptual clarity by demonstrating how stratified analysis can resolve Simpson's paradox. By simulating a scenario where a naive, unstratified analysis leads to a conclusion that is the opposite of the truth, you will gain a hands-on appreciation for how conditioning on a confounding variable (the stratum) is essential for uncovering the true underlying causal effects.",
            "id": "3349495",
            "problem": "Construct a complete Monte Carlo (Monte Carlo is a numerical method based on repeated random sampling) experiment that exhibits Simpson’s paradox in the context of stratified sampling. Work in the following fully specified, purely probabilistic data-generating process with two or more strata. For each unit, a stratum label $S \\in \\{0,1,\\dots,K-1\\}$ is drawn independently from a categorical distribution with stratum probabilities $\\pi_s$, treatment $T \\in \\{0,1\\}$ is assigned independently across units but with stratum-dependent probability $\\tau_s = \\mathbb{P}(T=1 \\mid S=s)$, and the outcome is generated by\n$$\nY = \\mu_s + \\delta_s \\, T + \\varepsilon_s, \\quad \\varepsilon_s \\sim \\mathcal{N}(0,\\sigma_s^2) \\text{ independently across units, with } \\sigma_s \\ge 0.\n$$\nHere, $\\mu_s$ is a stratum-specific baseline mean, $\\delta_s$ is a stratum-specific treatment effect, and $\\sigma_s^2$ is a stratum-specific variance. The Average Treatment Effect (ATE) is defined as $\\sum_{s=0}^{K-1} \\pi_s \\, \\delta_s$. Consider two estimators for the treatment effect:\n- The naive difference-in-means estimator that ignores $S$, which is the sample mean of $Y$ among treated minus the sample mean of $Y$ among control, i.e., $\\bar{Y}_{T=1} - \\bar{Y}_{T=0}$.\n- The stratified difference-in-means estimator that first computes the within-stratum difference $\\bar{Y}_{1,s} - \\bar{Y}_{0,s}$ and then aggregates these differences using prespecified weights equal to the population stratum probabilities $\\pi_s$, i.e., $\\sum_{s=0}^{K-1} \\pi_s \\, (\\bar{Y}_{1,s} - \\bar{Y}_{0,s})$.\n\nYour program must, for each test case below, do all of the following:\n1) Simulate an independent and identically distributed sample of size $n$ from the model above using the provided parameters and random seed. Compute the naive and stratified estimators from this simulated sample. These two values are the Monte Carlo outputs.\n2) Derive from first principles, using only the definitions of conditional expectation and Bayes’ rule, the analytical expectations of the naive and stratified estimators under the stated data-generating process (do not use any pre-derived estimator formulas). Compute these analytical expectations numerically for the provided parameters.\n3) Determine whether Simpson’s paradox occurs in expectation, defined here as the event that the analytical expectation of the naive estimator and the ATE (which equals the analytical expectation of the stratified estimator) have opposite signs, i.e., whether their product is negative.\n4) For both the naive and stratified estimators, check whether the simulated Monte Carlo estimate is close to its analytical expectation within an absolute tolerance of 0.02.\n\nTest suite. For each test case, you are given the tuple $(K, \\boldsymbol{\\pi}, \\boldsymbol{\\tau}, \\boldsymbol{\\mu}, \\boldsymbol{\\delta}, \\boldsymbol{\\sigma}, n, \\text{seed})$ with the following semantics:\n- $K$: the number of strata.\n- $\\boldsymbol{\\pi} = (\\pi_0,\\dots,\\pi_{K-1})$, stratum probabilities with $\\sum_s \\pi_s = 1$.\n- $\\boldsymbol{\\tau} = (\\tau_0,\\dots,\\tau_{K-1})$, treatment assignment probabilities.\n- $\\boldsymbol{\\mu} = (\\mu_0,\\dots,\\mu_{K-1})$, baseline means.\n- $\\boldsymbol{\\delta} = (\\delta_0,\\dots,\\delta_{K-1})$, treatment effects.\n- $\\boldsymbol{\\sigma} = (\\sigma_0,\\dots,\\sigma_{K-1})$, standard deviations.\n- $n$: sample size.\n- $\\text{seed}$: integer seed for reproducibility.\n\nUse the following four test cases:\n- Case A (two strata; Simpson’s paradox with positive within-stratum effects): $K = 2$, $\\boldsymbol{\\pi} = (0.3, 0.7)$, $\\boldsymbol{\\tau} = (0.9, 0.1)$, $\\boldsymbol{\\mu} = (0.0, 2.0)$, $\\boldsymbol{\\delta} = (1.0, 1.0)$, $\\boldsymbol{\\sigma} = (1.0, 1.0)$, $n = 200000$, $\\text{seed} = 202701$.\n- Case B (two strata; no confounding so no paradox): $K = 2$, $\\boldsymbol{\\pi} = (0.3, 0.7)$, $\\boldsymbol{\\tau} = (0.5, 0.5)$, $\\boldsymbol{\\mu} = (0.0, 2.0)$, $\\boldsymbol{\\delta} = (1.0, 0.5)$, $\\boldsymbol{\\sigma} = (1.0, 1.0)$, $n = 200000$, $\\text{seed} = 202702$.\n- Case C (two strata; zero outcome noise and strong confounding): $K = 2$, $\\boldsymbol{\\pi} = (0.2, 0.8)$, $\\boldsymbol{\\tau} = (0.95, 0.05)$, $\\boldsymbol{\\mu} = (1.0, 4.0)$, $\\boldsymbol{\\delta} = (0.4, 0.4)$, $\\boldsymbol{\\sigma} = (0.0, 0.0)$, $n = 200000$, $\\text{seed} = 202703$.\n- Case D (three strata; constant treatment effect but confounded assignment): $K = 3$, $\\boldsymbol{\\pi} = (0.1, 0.2, 0.7)$, $\\boldsymbol{\\tau} = (0.9, 0.5, 0.1)$, $\\boldsymbol{\\mu} = (0.0, 1.0, 3.0)$, $\\boldsymbol{\\delta} = (0.5, 0.5, 0.5)$, $\\boldsymbol{\\sigma} = (1.0, 1.0, 1.0)$, $n = 300000$, $\\text{seed} = 202704$.\n\nFinal output format. Your program should produce a single line of output containing the results as a Python-style list of four inner lists, one per test case, in the same order as above. Each inner list must contain exactly seven entries in this order: \n[Monte Carlo naive estimator (rounded to six decimals), Monte Carlo stratified estimator (rounded to six decimals), analytical expectation of naive estimator (rounded to six decimals), analytical expectation of stratified estimator (rounded to six decimals), boolean closeness of Monte Carlo naive to its analytical expectation within 0.02, boolean closeness of Monte Carlo stratified to its analytical expectation within 0.02, boolean indicating whether Simpson’s paradox occurs in expectation].\nFor example, your program should print something of the form \n$[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$\non a single line, with no other characters. No percentages or physical units appear in this problem.",
            "solution": "## Problem Validation\nThe problem statement is a valid, well-posed, and scientifically grounded exercise in stochastic simulation and causal inference.\n\n### Step 1: Extract Givens\n- **Data-Generating Process**: For each unit, a stratum label $S \\in \\{0, 1, \\dots, K-1\\}$ is drawn from a categorical distribution with probabilities $\\boldsymbol{\\pi} = (\\pi_s)$. Treatment $T \\in \\{0, 1\\}$ is assigned with stratum-dependent probability $\\tau_s = \\mathbb{P}(T=1 \\mid S=s)$. The outcome $Y$ is generated by $Y = \\mu_s + \\delta_s T + \\varepsilon_s$, where $\\varepsilon_s \\sim \\mathcal{N}(0, \\sigma_s^2)$.\n- **Average Treatment Effect (ATE)**: The target parameter is $ATE = \\sum_{s=0}^{K-1} \\pi_s \\delta_s$.\n- **Naive Estimator**: $\\hat{\\delta}_{naive} = \\bar{Y}_{T=1} - \\bar{Y}_{T=0}$, the unadjusted difference-in-means.\n- **Stratified Estimator**: $\\hat{\\delta}_{strat} = \\sum_{s=0}^{K-1} \\pi_s (\\bar{Y}_{1,s} - \\bar{Y}_{0,s})$, where $\\bar{Y}_{t,s}$ is the sample mean of $Y$ for units with $T=t$ and $S=s$.\n- **Simulation Task**: For each test case, simulate a sample of size $n$, compute the Monte Carlo estimates for $\\hat{\\delta}_{naive}$ and $\\hat{\\delta}_{strat}$.\n- **Analytical Task**: Derive the analytical expectations of both estimators and compute their numerical values.\n- **Simpson's Paradox Definition**: The paradox occurs in expectation if $\\mathbb{E}[\\hat{\\delta}_{naive}] \\times ATE  0$.\n- **Closeness Check**: A Monte Carlo estimate is \"close\" to its analytical expectation if the absolute difference is less than 0.02.\n- **Test Cases**: Four specific parameter sets $(K, \\boldsymbol{\\pi}, \\boldsymbol{\\tau}, \\boldsymbol{\\mu}, \\boldsymbol{\\delta}, \\boldsymbol{\\sigma}, n, \\text{seed})$ are provided.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem uses a standard potential outcomes framework with a linear model for outcomes, which is a cornerstone of causal inference and econometrics. The concepts of confounding, stratification, and Simpson's paradox are fundamental in statistics. The entire setup is scientifically and mathematically sound.\n- **Well-Posed**: The problem is fully specified. All necessary parameters for simulation and analytical calculation are provided. The objectives are clear and unambiguous, leading to a unique and meaningful set of results for each test case.\n- **Objective**: The problem is stated in precise mathematical language, free of subjectivity or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed to the solution.\n\n---\n\n## Analytical Derivation of Estimator Expectations\n\nWe derive the analytical expectations for the stratified and naive estimators from first principles.\n\n### Expectation of the Stratified Estimator\nThe stratified estimator is defined as $\\hat{\\delta}_{strat} = \\sum_{s=0}^{K-1} \\pi_s (\\bar{Y}_{1,s} - \\bar{Y}_{0,s})$.\nBy the linearity of expectation, its expected value is:\n$$\n\\mathbb{E}[\\hat{\\delta}_{strat}] = \\mathbb{E}\\left[\\sum_{s=0}^{K-1} \\pi_s (\\bar{Y}_{1,s} - \\bar{Y}_{0,s})\\right] = \\sum_{s=0}^{K-1} \\pi_s \\mathbb{E}[\\bar{Y}_{1,s} - \\bar{Y}_{0,s}]\n$$\nTo evaluate the inner expectation, we consider the expected outcome conditional on stratum $s$ and treatment status $t$. For a unit in stratum $s$ with treatment $T=1$:\n$$\n\\mathbb{E}[Y | S=s, T=1] = \\mathbb{E}[\\mu_s + \\delta_s \\cdot 1 + \\varepsilon_s] = \\mu_s + \\delta_s\n$$\nsince $\\mathbb{E}[\\varepsilon_s] = 0$. As the sample mean $\\bar{Y}_{1,s}$ is an unbiased estimator of the conditional population mean, we have $\\mathbb{E}[\\bar{Y}_{1,s}] = \\mu_s + \\delta_s$.\nSimilarly, for a control unit ($T=0$) in stratum $s$:\n$$\n\\mathbb{E}[Y | S=s, T=0] = \\mathbb{E}[\\mu_s + \\delta_s \\cdot 0 + \\varepsilon_s] = \\mu_s\n$$\nThus, $\\mathbb{E}[\\bar{Y}_{0,s}] = \\mu_s$.\nThe expectation of the within-stratum difference-in-means is:\n$$\n\\mathbb{E}[\\bar{Y}_{1,s} - \\bar{Y}_{0,s}] = \\mathbb{E}[\\bar{Y}_{1,s}] - \\mathbb{E}[\\bar{Y}_{0,s}] = (\\mu_s + \\delta_s) - \\mu_s = \\delta_s\n$$\nSubstituting this back into the expression for $\\mathbb{E}[\\hat{\\delta}_{strat}]$:\n$$\n\\mathbb{E}[\\hat{\\delta}_{strat}] = \\sum_{s=0}^{K-1} \\pi_s \\delta_s\n$$\nThis is precisely the definition of the Average Treatment Effect (ATE). The stratified estimator is therefore an unbiased estimator of the ATE.\n\n### Expectation of the Naive Estimator\nThe naive estimator is $\\hat{\\delta}_{naive} = \\bar{Y}_{T=1} - \\bar{Y}_{T=0}$. Its expectation is $\\mathbb{E}[\\hat{\\delta}_{naive}] = \\mathbb{E}[\\bar{Y}_{T=1}] - \\mathbb{E}[\\bar{Y}_{T=0}]$. By the law of large numbers, $\\mathbb{E}[\\bar{Y}_{T=t}] = \\mathbb{E}[Y | T=t]$. We use the law of total expectation, conditioning on the stratum $S$.\nFor the treated group:\n$$\n\\mathbb{E}[Y | T=1] = \\sum_{s=0}^{K-1} \\mathbb{E}[Y | S=s, T=1] \\mathbb{P}(S=s | T=1)\n$$\nWe have $\\mathbb{E}[Y | S=s, T=1] = \\mu_s + \\delta_s$. We find the conditional probability $\\mathbb{P}(S=s | T=1)$ using Bayes' rule:\n$$\n\\mathbb{P}(S=s | T=1) = \\frac{\\mathbb{P}(T=1 | S=s) \\mathbb{P}(S=s)}{\\mathbb{P}(T=1)} = \\frac{\\tau_s \\pi_s}{\\sum_{s'=0}^{K-1} \\mathbb{P}(T=1 | S=s') \\mathbb{P}(S=s')} = \\frac{\\tau_s \\pi_s}{\\sum_{s'=0}^{K-1} \\tau_{s'} \\pi_{s'}}\n$$\nSo, the expected outcome for the treated is:\n$$\n\\mathbb{E}[Y | T=1] = \\sum_{s=0}^{K-1} (\\mu_s + \\delta_s) \\frac{\\tau_s \\pi_s}{\\sum_{s'} \\tau_{s'} \\pi_{s'}} = \\frac{\\sum_{s=0}^{K-1} \\pi_s \\tau_s (\\mu_s + \\delta_s)}{\\sum_{s'} \\pi_{s'} \\tau_{s'}}\n$$\nFor the control group, by analogous logic:\n$$\n\\mathbb{E}[Y | T=0] = \\sum_{s=0}^{K-1} \\mathbb{E}[Y | S=s, T=0] \\mathbb{P}(S=s | T=0)\n$$\nWith $\\mathbb{E}[Y | S=s, T=0] = \\mu_s$ and $\\mathbb{P}(S=s | T=0) = \\frac{(1-\\tau_s) \\pi_s}{\\sum_{s'} (1-\\tau_{s'}) \\pi_{s'}}$, we get:\n$$\n\\mathbb{E}[Y | T=0] = \\frac{\\sum_{s=0}^{K-1} \\pi_s (1-\\tau_s) \\mu_s}{\\sum_{s'} \\pi_{s'} (1-\\tau_{s'})}\n$$\nThe expectation of the naive estimator is the difference of these two quantities:\n$$\n\\mathbb{E}[\\hat{\\delta}_{naive}] = \\frac{\\sum_{s=0}^{K-1} \\pi_s \\tau_s (\\mu_s + \\delta_s)}{\\sum_{s'} \\pi_{s'} \\tau_{s'}} - \\frac{\\sum_{s=0}^{K-1} \\pi_s (1-\\tau_s) \\mu_s}{\\sum_{s'} \\pi_{s'} (1-\\tau_{s'})}\n$$\nThis expression reveals that $\\mathbb{E}[\\hat{\\delta}_{naive}]$ is composed of a weighted average of the true effects $\\delta_s$ and a bias term arising from confounding. The bias term is non-zero if the treatment probability $\\tau_s$ is correlated with the baseline mean $\\mu_s$ across strata. If $\\tau_s$ is constant for all $s$, there is no confounding, and the bias term vanishes, making $\\mathbb{E}[\\hat{\\delta}_{naive}] = ATE$. Simpson's paradox occurs when this bias is strong enough to change the sign of the estimator in expectation, i.e., $\\mathbb{E}[\\hat{\\delta}_{naive}] \\times ATE  0$.\n\n## Computational Strategy\nFor each test case, the program will execute the following steps:\n\n1.  **Analytical Computations**:\n    - Compute the ATE, which is the analytical expectation of the stratified estimator, using $ATE = \\sum_{s} \\pi_s \\delta_s$.\n    - Compute the analytical expectation of the naive estimator using the derived formula above.\n    - Determine if Simpson's paradox occurs by checking if $ATE \\times \\mathbb{E}[\\hat{\\delta}_{naive}]  0$.\n\n2.  **Monte Carlo Simulation**:\n    - A random number generator is initialized with the specified seed for reproducibility.\n    - A sample of $n$ stratum labels $S$ is drawn from the categorical distribution defined by $\\boldsymbol{\\pi}$.\n    - For each simulated unit $i$ in stratum $S_i=s$, treatment $T_i$ is assigned by drawing from a Bernoulli distribution with parameter $\\tau_s$.\n    - For each unit $i$, the outcome $Y_i$ is generated as $Y_i = \\mu_{S_i} + \\delta_{S_i} T_i + \\varepsilon_i$, where $\\varepsilon_i$ is drawn from $\\mathcal{N}(0, \\sigma_{S_i}^2)$.\n\n3.  **Monte Carlo Estimators**:\n    - The naive estimator a, $\\hat{\\delta}_{naive}$, is computed by finding the difference between the sample mean outcome of all treated units and the sample mean outcome of all control units.\n    - The stratified estimator, $\\hat{\\delta}_{strat}$, is computed by first calculating the difference-in-means within each stratum $s$, and then taking a weighted average of these differences, with weights given by $\\pi_s$.\n\n4.  **Final Evaluation**:\n    - The simulated estimators $\\hat{\\delta}_{naive}$ and $\\hat{\\delta}_{strat}$ are compared to their respective analytical expectations. The closeness is determined by checking if the absolute difference is within the tolerance of 0.02.\n    - The results, including the two Monte Carlo estimates, the two analytical expectations, the two boolean closeness checks, and the boolean paradox indicator, are collected and formatted as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Monte Carlo experiments for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case A: two strata; Simpson’s paradox with positive within-stratum effects\n        (2, np.array([0.3, 0.7]), np.array([0.9, 0.1]), np.array([0.0, 2.0]), np.array([1.0, 1.0]), np.array([1.0, 1.0]), 200000, 202701),\n        # Case B: two strata; no confounding so no paradox\n        (2, np.array([0.3, 0.7]), np.array([0.5, 0.5]), np.array([0.0, 2.0]), np.array([1.0, 0.5]), np.array([1.0, 1.0]), 200000, 202702),\n        # Case C: two strata; zero outcome noise and strong confounding\n        (2, np.array([0.2, 0.8]), np.array([0.95, 0.05]), np.array([1.0, 4.0]), np.array([0.4, 0.4]), np.array([0.0, 0.0]), 200000, 202703),\n        # Case D: three strata; constant treatment effect but confounded assignment\n        (3, np.array([0.1, 0.2, 0.7]), np.array([0.9, 0.5, 0.1]), np.array([0.0, 1.0, 3.0]), np.array([0.5, 0.5, 0.5]), np.array([1.0, 1.0, 1.0]), 300000, 202704),\n    ]\n\n    all_results = []\n    for case_params in test_cases:\n        result = process_case(*case_params)\n        all_results.append(result)\n\n    # Format the final output as a Python-style list of lists.\n    output_str = \"[\"\n    for i, res in enumerate(all_results):\n        # Unpack results and format them\n        mc_naive, mc_strat, an_naive, an_strat, close_naive, close_strat, paradox = res\n        \n        # Format list elements\n        res_str = f\"[{mc_naive:.6f},{mc_strat:.6f},{an_naive:.6f},{an_strat:.6f},{close_naive},{close_strat},{paradox}]\"\n        \n        output_str += res_str\n        if i  len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\n\ndef process_case(K, pi, tau, mu, delta, sigma, n, seed):\n    \"\"\"\n    Processes a single test case: performs analytical calculations, \n    runs the simulation, and computes the required outputs.\n    \"\"\"\n    \n    # --- 1. Analytical Computations ---\n    \n    # The ATE is the analytical expectation of the stratified estimator.\n    analytical_strat_exp = np.sum(pi * delta)\n\n    # Calculate the analytical expectation of the naive estimator.\n    tau_overall = np.sum(pi * tau)\n    \n    # Handle edge cases where tau_overall is 0 or 1.\n    if tau_overall > 0:\n        exp_Y_T1 = np.sum(pi * tau * (mu + delta)) / tau_overall\n    else:\n        exp_Y_T1 = 0\n    \n    if tau_overall  1:\n        exp_Y_T0 = np.sum(pi * (1 - tau) * mu) / (1 - tau_overall)\n    else:\n        exp_Y_T0 = 0\n\n    analytical_naive_exp = exp_Y_T1 - exp_Y_T0\n\n    # Determine if Simpson's paradox occurs in expectation.\n    simpson_paradox_occurs = (analytical_naive_exp * analytical_strat_exp)  0\n\n    # --- 2. Monte Carlo Simulation ---\n    rng = np.random.default_rng(seed)\n\n    # a. Generate strata S\n    S = rng.choice(K, size=n, p=pi)\n\n    # b. Generate treatment T\n    # Vectorized assignment: create an array of treatment probabilities based on stratum\n    tau_by_unit = tau[S]\n    T = (rng.random(size=n)  tau_by_unit).astype(int)\n\n    # c. Generate outcome Y\n    mu_by_unit = mu[S]\n    delta_by_unit = delta[S]\n    sigma_by_unit = sigma[S]\n    epsilon = rng.normal(loc=0.0, scale=sigma_by_unit, size=n)\n    Y = mu_by_unit + delta_by_unit * T + epsilon\n\n    # --- 3. Compute Monte Carlo Estimators ---\n    \n    # a. Naive difference-in-means estimator\n    mask_treated = T == 1\n    mask_control = T == 0\n    \n    # Ensure no division by zero if a group is empty (unlikely with large n)\n    mc_naive_est = 0\n    if np.any(mask_treated) and np.any(mask_control):\n        mc_naive_est = np.mean(Y[mask_treated]) - np.mean(Y[mask_control])\n\n    # b. Stratified difference-in-means estimator\n    strat_diffs = np.zeros(K)\n    for s in range(K):\n        mask_s = (S == s)\n        Y_s_treated = Y[mask_s  mask_treated]\n        Y_s_control = Y[mask_s  mask_control]\n\n        # Calculate within-stratum difference, handling potentially empty cells\n        if Y_s_treated.size > 0 and Y_s_control.size > 0:\n            strat_diffs[s] = np.mean(Y_s_treated) - np.mean(Y_s_control)\n        else:\n            # If a cell is empty, its contribution to the effect is undefined.\n            # We treat it as 0, which is reasonable if n is large and this event is rare.\n            # For this problem's parameters, cells will be populated.\n            strat_diffs[s] = 0.0\n\n    mc_strat_est = np.sum(pi * strat_diffs)\n    \n    # --- 4. Closeness Checks ---\n    tolerance = 0.02\n    naive_is_close = np.abs(mc_naive_est - analytical_naive_exp)  tolerance\n    strat_is_close = np.abs(mc_strat_est - analytical_strat_exp)  tolerance\n\n    return [\n        mc_naive_est,\n        mc_strat_est,\n        analytical_naive_exp,\n        analytical_strat_exp,\n        naive_is_close,\n        strat_is_close,\n        simpson_paradox_occurs,\n    ]\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Classical stratified estimation relies on sample frequencies, which can be unreliable or misleading in rare-event simulations where some strata may yield zero positive counts. This practice  addresses this advanced, practical challenge by introducing a Bayesian framework. By applying conjugate Beta priors to the binomial outcomes in each stratum, you will learn to derive a smoothed posterior estimate for the overall event probability, providing a more robust and principled way to handle the uncertainty associated with zero-count observations.",
            "id": "3349500",
            "problem": "Consider a stratified Monte Carlo (MC) simulation to estimate a rare-event probability across $H$ disjoint strata. Let the stratum-specific event probabilities be $p_{h} \\in (0,1)$ for $h \\in \\{1,\\dots,H\\}$, and suppose the overall event probability of interest is the population-weighted quantity $\\theta = \\sum_{h=1}^{H} w_{h} p_{h}$, where the known stratum weights satisfy $w_{h} \\geq 0$ and $\\sum_{h=1}^{H} w_{h} = 1$. Within each stratum $h$, you observe $n_{h}$ independent Bernoulli($p_{h}$) trials, with $y_{h}$ observed events. In rare-event regimes, some strata may have zero observed events ($y_{h} = 0$) even when $p_{h}  0$, which motivates Bayesian smoothing. Assume independent Beta priors $p_{h} \\sim \\mathrm{Beta}(a_{h}, b_{h})$ for each stratum.\n\nStarting from Bayes’ theorem and the definition of $\\theta$ as a weighted average of the $p_{h}$, derive the posterior distribution of each $p_{h}$ given the stratified binomial observations and the Beta priors. Then, using fundamental properties of expectation and variance, derive the posterior mean $\\mathbb{E}[\\theta \\mid \\text{data}]$ and posterior variance $\\mathrm{Var}(\\theta \\mid \\text{data})$ under the assumption that the strata are conditionally independent given the data.\n\nFinally, compute these two posterior summaries for the following scientifically realistic, small-sample, rare-event scenario with potential zero counts:\n- Number of strata: $H = 4$.\n- Stratum weights: $(w_{1}, w_{2}, w_{3}, w_{4}) = (0.4, 0.3, 0.2, 0.1)$.\n- Observed counts and trials: $(y_{1}, n_{1}) = (0, 25)$, $(y_{2}, n_{2}) = (1, 10)$, $(y_{3}, n_{3}) = (0, 15)$, $(y_{4}, n_{4}) = (0, 5)$.\n- Independent Beta priors: $(a_{h}, b_{h}) = (0.2, 3.8)$ for all $h$.\n\nReport the pair $\\big(\\mathbb{E}[\\theta \\mid \\text{data}], \\mathrm{Var}(\\theta \\mid \\text{data})\\big)$ as your final answer. Round each of the two numerical values to five significant figures. No units are required for this probability and variance.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in standard Bayesian statistical theory, specifically the use of conjugate Beta priors for Binomial likelihoods. The problem is well-posed, providing all necessary data and definitions to compute a unique solution. The language is objective and mathematically precise. There are no contradictions, ambiguities, or factual inaccuracies.\n\nWe are tasked with deriving the posterior mean and variance of a composite parameter $\\theta = \\sum_{h=1}^{H} w_{h} p_{h}$, where each $p_{h}$ is a stratum-specific probability. The derivation proceeds in two main steps: first, determining the posterior distribution for each $p_{h}$, and second, using these posterior distributions to find the moments of $\\theta$.\n\n**Step 1: Derivation of the Posterior Distribution for $p_{h}$**\n\nFor each stratum $h \\in \\{1,\\dots,H\\}$, we are given a prior distribution for the probability $p_{h}$ and a likelihood function based on the observed data.\n\nThe prior distribution for $p_{h}$ is given as a Beta distribution:\n$$p(p_{h}) \\sim \\mathrm{Beta}(a_{h}, b_{h})$$\nThe probability density function (PDF) is proportional to $p_{h}^{a_{h}-1} (1-p_{h})^{b_{h}-1}$.\n$$p(p_{h}) \\propto p_{h}^{a_{h}-1} (1-p_{h})^{b_{h}-1}$$\n\nThe data for stratum $h$ consist of observing $y_{h}$ events in $n_{h}$ independent Bernoulli trials. The likelihood of a specific value of $p_{h}$ given this data is described by the Binomial probability mass function:\n$$L(p_{h} \\mid y_{h}, n_{h}) = \\binom{n_{h}}{y_{h}} p_{h}^{y_{h}} (1-p_{h})^{n_{h}-y_{h}}$$\n\nAccording to Bayes' theorem, the posterior distribution of $p_{h}$ is proportional to the product of the likelihood and the prior:\n$$p(p_{h} \\mid y_{h}, n_{h}) \\propto L(p_{h} \\mid y_{h}, n_{h}) \\cdot p(p_{h})$$\nSubstituting the expressions and dropping the constant binomial coefficient $\\binom{n_{h}}{y_{h}}$:\n$$p(p_{h} \\mid y_{h}, n_{h}) \\propto \\left( p_{h}^{y_{h}} (1-p_{h})^{n_{h}-y_{h}} \\right) \\cdot \\left( p_{h}^{a_{h}-1} (1-p_{h})^{b_{h}-1} \\right)$$\nCombining the terms with the same base:\n$$p(p_{h} \\mid y_{h}, n_{h}) \\propto p_{h}^{y_{h} + a_{h} - 1} (1-p_{h})^{n_{h} - y_{h} + b_{h} - 1}$$\n\nThis functional form is the kernel of a Beta distribution. This demonstrates the conjugacy of the Beta prior with the Binomial likelihood. The posterior distribution of $p_{h}$ given the data is thus:\n$$p_{h} \\mid y_{h}, n_{h} \\sim \\mathrm{Beta}(a_{h} + y_{h}, b_{h} + n_{h} - y_{h})$$\nLet's denote the posterior parameters as $a'_{h} = a_{h} + y_{h}$ and $b'_{h} = b_{h} + n_{h} - y_{h}$.\n\n**Step 2: Derivation of Posterior Mean and Variance of $\\theta$**\n\nThe overall event probability $\\theta$ is a linear combination of the stratum-specific probabilities $p_{h}$:\n$$\\theta = \\sum_{h=1}^{H} w_{h} p_{h}$$\n\nTo find the posterior mean of $\\theta$, $\\mathbb{E}[\\theta \\mid \\text{data}]$, we use the linearity of expectation. The expectation is conditioned on the full dataset, which consists of the observations from all strata.\n$$\\mathbb{E}[\\theta \\mid \\text{data}] = \\mathbb{E}\\left[\\sum_{h=1}^{H} w_{h} p_{h} \\mid \\text{data}\\right] = \\sum_{h=1}^{H} w_{h} \\mathbb{E}[p_{h} \\mid \\text{data}]$$\nSince the observations $(y_h, n_h)$ in one stratum are independent of those in other strata, the posterior for $p_h$ depends only on its own data: $\\mathbb{E}[p_{h} \\mid \\text{data}] = \\mathbb{E}[p_{h} \\mid y_{h}, n_{h}]$.\n\nThe mean of a $\\mathrm{Beta}(\\alpha, \\beta)$ distribution is $\\frac{\\alpha}{\\alpha + \\beta}$. Using the posterior parameters $a'_{h}$ and $b'_{h}$:\n$$\\mathbb{E}[p_{h} \\mid y_{h}, n_{h}] = \\frac{a'_{h}}{a'_{h} + b'_{h}} = \\frac{a_{h} + y_{h}}{(a_{h} + y_{h}) + (b_{h} + n_{h} - y_{h})} = \\frac{a_{h} + y_{h}}{a_{h} + b_{h} + n_{h}}$$\nTherefore, the posterior mean of $\\theta$ is:\n$$\\mathbb{E}[\\theta \\mid \\text{data}] = \\sum_{h=1}^{H} w_{h} \\left( \\frac{a_{h} + y_{h}}{a_{h} + b_{h} + n_{h}} \\right)$$\n\nTo find the posterior variance of $\\theta$, $\\mathrm{Var}(\\theta \\mid \\text{data})$, we use the property that for a sum of independent random variables, the variance of the sum is the sum of the variances. The problem states to assume conditional independence of the strata given the data, which means the posterior distributions of the $p_h$ are independent.\n$$\\mathrm{Var}(\\theta \\mid \\text{data}) = \\mathrm{Var}\\left(\\sum_{h=1}^{H} w_{h} p_{h} \\mid \\text{data}\\right) = \\sum_{h=1}^{H} \\mathrm{Var}(w_{h} p_{h} \\mid \\text{data}) = \\sum_{h=1}^{H} w_{h}^{2} \\mathrm{Var}(p_{h} \\mid \\text{data})$$\n\nThe variance of a $\\mathrm{Beta}(\\alpha, \\beta)$ distribution is $\\frac{\\alpha \\beta}{(\\alpha+\\beta)^{2}(\\alpha+\\beta+1)}$. Using the posterior parameters:\n$$\\mathrm{Var}(p_{h} \\mid y_{h}, n_{h}) = \\frac{a'_{h} b'_{h}}{(a'_{h} + b'_{h})^{2}(a'_{h} + b'_{h} + 1)} = \\frac{(a_{h} + y_{h})(b_{h} + n_{h} - y_{h})}{(a_{h} + b_{h} + n_{h})^{2}(a_{h} + b_{h} + n_{h} + 1)}$$\nTherefore, the posterior variance of $\\theta$ is:\n$$\\mathrm{Var}(\\theta \\mid \\text{data}) = \\sum_{h=1}^{H} w_{h}^{2} \\left( \\frac{(a_{h} + y_{h})(b_{h} + n_{h} - y_{h})}{(a_{h} + b_{h} + n_{h})^{2}(a_{h} + b_{h} + n_{h} + 1)} \\right)$$\n\n**Step 3: Numerical Computation**\n\nWe are given the following values:\n- $H = 4$\n- $(w_{1}, w_{2}, w_{3}, w_{4}) = (0.4, 0.3, 0.2, 0.1)$\n- $(y_{1}, n_{1}) = (0, 25)$, $(y_{2}, n_{2}) = (1, 10)$, $(y_{3}, n_{3}) = (0, 15)$, $(y_{4}, n_{4}) = (0, 5)$\n- $(a_{h}, b_{h}) = (0.2, 3.8)$ for all $h$, so $a_{h} + b_{h} = 4.0$ for all $h$.\n\nWe compute the posterior mean and variance for each stratum:\n\n**Stratum 1:** $w_1 = 0.4$, $y_1 = 0$, $n_1 = 25$\n- $\\mathbb{E}[p_{1} \\mid \\text{data}] = \\frac{0.2+0}{4.0+25} = \\frac{0.2}{29}$\n- $\\mathrm{Var}(p_{1} \\mid \\text{data}) = \\frac{(0.2+0)(3.8+25-0)}{(4.0+25)^{2}(4.0+25+1)} = \\frac{0.2 \\times 28.8}{29^{2} \\times 30} = \\frac{5.76}{25230}$\n\n**Stratum 2:** $w_2 = 0.3$, $y_2 = 1$, $n_2 = 10$\n- $\\mathbb{E}[p_{2} \\mid \\text{data}] = \\frac{0.2+1}{4.0+10} = \\frac{1.2}{14}$\n- $\\mathrm{Var}(p_{2} \\mid \\text{data}) = \\frac{(0.2+1)(3.8+10-1)}{(4.0+10)^{2}(4.0+10+1)} = \\frac{1.2 \\times 12.8}{14^{2} \\times 15} = \\frac{15.36}{2940}$\n\n**Stratum 3:** $w_3 = 0.2$, $y_3 = 0$, $n_3 = 15$\n- $\\mathbb{E}[p_{3} \\mid \\text{data}] = \\frac{0.2+0}{4.0+15} = \\frac{0.2}{19}$\n- $\\mathrm{Var}(p_{3} \\mid \\text{data}) = \\frac{(0.2+0)(3.8+15-0)}{(4.0+15)^{2}(4.0+15+1)} = \\frac{0.2 \\times 18.8}{19^{2} \\times 20} = \\frac{3.76}{7220}$\n\n**Stratum 4:** $w_4 = 0.1$, $y_4 = 0$, $n_4 = 5$\n- $\\mathbb{E}[p_{4} \\mid \\text{data}] = \\frac{0.2+0}{4.0+5} = \\frac{0.2}{9}$\n- $\\mathrm{Var}(p_{4} \\mid \\text{data}) = \\frac{(0.2+0)(3.8+5-0)}{(4.0+5)^{2}(4.0+5+1)} = \\frac{0.2 \\times 8.8}{9^{2} \\times 10} = \\frac{1.76}{810}$\n\nNow, we compute the final summaries for $\\theta$:\n\n**Posterior Mean $\\mathbb{E}[\\theta \\mid \\text{data}]$:**\n$$\\mathbb{E}[\\theta \\mid \\text{data}] = (0.4)\\frac{0.2}{29} + (0.3)\\frac{1.2}{14} + (0.2)\\frac{0.2}{19} + (0.1)\\frac{0.2}{9}$$\n$$= \\frac{0.08}{29} + \\frac{0.36}{14} + \\frac{0.04}{19} + \\frac{0.02}{9}$$\n$$\\approx 0.00275862 + 0.02571429 + 0.00210526 + 0.00222222$$\n$$\\approx 0.03280039$$\nRounding to five significant figures, $\\mathbb{E}[\\theta \\mid \\text{data}] \\approx 0.032800$.\n\n**Posterior Variance $\\mathrm{Var}(\\theta \\mid \\text{data})$:**\n$$\\mathrm{Var}(\\theta \\mid \\text{data}) = (0.4)^{2} \\frac{5.76}{25230} + (0.3)^{2} \\frac{15.36}{2940} + (0.2)^{2} \\frac{3.76}{7220} + (0.1)^{2} \\frac{1.76}{810}$$\n$$= (0.16) \\frac{5.76}{25230} + (0.09) \\frac{15.36}{2940} + (0.04) \\frac{3.76}{7220} + (0.01) \\frac{1.76}{810}$$\n$$\\approx 0.16(0.00022830) + 0.09(0.00522449) + 0.04(0.00052078) + 0.01(0.00217284)$$\n$$\\approx 0.000036528 + 0.000470204 + 0.000020831 + 0.000021728$$\n$$\\approx 0.000549291$$\nRounding to five significant figures, $\\mathrm{Var}(\\theta \\mid \\text{data}) \\approx 0.00054929$.\n\nThe final pair of values is $(0.032800, 0.00054929)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.032800  0.00054929 \\end{pmatrix}}$$"
        }
    ]
}