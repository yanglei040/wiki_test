{
    "hands_on_practices": [
        {
            "introduction": "This first exercise goes to the heart of the antithetic variates technique. By analyzing simple odd and even functions with respect to a symmetric distribution, you will derive from first principles how negative correlation leads to variance reduction, and conversely, how positive correlation can be detrimental. This practice is designed to build fundamental intuition for when this method is likely to be effective. ",
            "id": "3288466",
            "problem": "Let $X$ be a real-valued random variable with a distribution that is symmetric about $0$, that is, its probability density function $f$ satisfies $f(x)=f(-x)$ for all $x \\in \\mathbb{R}$. You are tasked with estimating $\\mu = \\mathbb{E}[h(X)]$ using a fixed computational budget of $M=2n$ function evaluations of $h$. Consider two estimators:\n\n1. The standard Monte Carlo (MC) estimator, which uses $M$ independent and identically distributed draws $Y_{1},\\dots,Y_{M}$ with $Y_{j} \\overset{d}{=} X$:\n$$\n\\widehat{\\mu}_{\\mathrm{MC}} \\;=\\; \\frac{1}{M}\\sum_{j=1}^{M} h(Y_{j}) \\, .\n$$\n\n2. The antithetic variates (AV) estimator, which forms $n$ dependent pairs by sampling $X_{1},\\dots,X_{n}$ with $X_{i} \\overset{d}{=} X$ and pairing each with its antithetic partner $-X_{i}$:\n$$\n\\widehat{\\mu}_{\\mathrm{AV}} \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} \\frac{h(X_{i}) + h(-X_{i})}{2} \\, .\n$$\n\nStart from the definitions of variance and covariance, and from first principles derive an exact expression for the variance reduction factor\n$$\n\\mathrm{VRF} \\;=\\; \\frac{\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{MC}}\\right)}{\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{AV}}\\right)}\n$$\nunder a fixed total budget $M=2n$. Then, specialize your expression to the cases $h(x)=x$ and $h(x)=x^{2}$, and compute the corresponding values of $\\mathrm{VRF}$. Finally, justify qualitatively, using properties of symmetry and parity, why the result differs between odd and even functions.\n\nReport your final answer as a single row matrix containing the two values of $\\mathrm{VRF}$ in the order $h(x)=x$ and $h(x)=x^{2}$. Do not approximate your results. Do not include units in your final numeric values.",
            "solution": "The problem asks for the derivation of the variance reduction factor (VRF) for an antithetic variates estimator compared to a standard Monte Carlo estimator, and its evaluation for two specific functions. The problem is well-posed and scientifically sound, resting on fundamental principles of stochastic simulation. We proceed with the solution.\n\nFirst, we derive a general expression for the Variance Reduction Factor, $\\mathrm{VRF}$. The total computational budget is fixed at $M=2n$ evaluations of the function $h$.\n\nThe standard Monte Carlo (MC) estimator is given by:\n$$\n\\widehat{\\mu}_{\\mathrm{MC}} = \\frac{1}{M}\\sum_{j=1}^{M} h(Y_{j})\n$$\nwhere $Y_j$ are independent and identically distributed (i.i.d.) draws from the distribution of $X$. Since the samples $h(Y_j)$ are i.i.d., the variance of the estimator is:\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{MC}}) = \\mathrm{Var}\\left(\\frac{1}{M}\\sum_{j=1}^{M} h(Y_{j})\\right) = \\frac{1}{M^2} \\sum_{j=1}^{M} \\mathrm{Var}(h(Y_j)) = \\frac{M}{M^2} \\mathrm{Var}(h(X)) = \\frac{1}{M} \\mathrm{Var}(h(X))\n$$\n\nThe antithetic variates (AV) estimator is given by:\n$$\n\\widehat{\\mu}_{\\mathrm{AV}} = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{h(X_{i}) + h(-X_{i})}{2}\n$$\nLet us define $Z_i = \\frac{h(X_i) + h(-X_i)}{2}$. The samples $X_1, \\dots, X_n$ are i.i.d., which implies that the terms $Z_1, \\dots, Z_n$ are also i.i.d. The variance of the AV estimator is therefore:\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{AV}}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} Z_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}(Z_i) = \\frac{n}{n^2} \\mathrm{Var}(Z_1) = \\frac{1}{n} \\mathrm{Var}(Z_1)\n$$\nNow, we compute the variance of a single term $Z_1$:\n$$\n\\mathrm{Var}(Z_1) = \\mathrm{Var}\\left(\\frac{h(X_1) + h(-X_1)}{2}\\right) = \\frac{1}{4} \\mathrm{Var}(h(X_1) + h(-X_1))\n$$\nUsing the formula for the variance of a sum of two random variables, $\\mathrm{Var}(A+B) = \\mathrm{Var}(A) + \\mathrm{Var}(B) + 2\\mathrm{Cov}(A,B)$, we get:\n$$\n\\mathrm{Var(Z_1)} = \\frac{1}{4} \\left[ \\mathrm{Var}(h(X_1)) + \\mathrm{Var}(h(-X_1)) + 2\\mathrm{Cov}(h(X_1), h(-X_1)) \\right]\n$$\nThe problem states that the distribution of $X$ is symmetric about $0$, meaning $X$ and $-X$ have the same distribution ($X \\overset{d}{=} -X$). Consequently, any function of these variables, $h(X)$ and $h(-X)$, also have the same distribution. This implies that their variances are equal: $\\mathrm{Var}(h(X_1)) = \\mathrm{Var}(h(-X_1)) = \\mathrm{Var}(h(X))$. Substituting this into the expression for $\\mathrm{Var}(Z_1)$:\n$$\n\\mathrm{Var}(Z_1) = \\frac{1}{4} \\left[ 2\\mathrm{Var}(h(X)) + 2\\mathrm{Cov}(h(X), h(-X)) \\right] = \\frac{1}{2} \\left[ \\mathrm{Var}(h(X)) + \\mathrm{Cov}(h(X), h(-X)) \\right]\n$$\nThus, the variance of the AV estimator is:\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{AV}}) = \\frac{1}{n} \\mathrm{Var}(Z_1) = \\frac{1}{2n} \\left[ \\mathrm{Var}(h(X)) + \\mathrm{Cov}(h(X), h(-X)) \\right]\n$$\nThe Variance Reduction Factor is the ratio of the two variances, under the fixed budget constraint $M=2n$:\n$$\n\\mathrm{VRF} = \\frac{\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{MC}})}{\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{AV}})} = \\frac{\\frac{1}{M}\\mathrm{Var}(h(X))}{\\frac{1}{2n} \\left[ \\mathrm{Var}(h(X)) + \\mathrm{Cov}(h(X), h(-X)) \\right]} = \\frac{\\frac{1}{2n}\\mathrm{Var}(h(X))}{\\frac{1}{2n} \\left[ \\mathrm{Var}(h(X)) + \\mathrm{Cov}(h(X), h(-X)) \\right]}\n$$\nThis simplifies to the general expression for the VRF:\n$$\n\\mathrm{VRF} = \\frac{\\mathrm{Var}(h(X))}{\\mathrm{Var}(h(X)) + \\mathrm{Cov}(h(X), h(-X))}\n$$\nThis expression can also be written in terms of the correlation coefficient $\\rho = \\mathrm{Corr}(h(X), h(-X))$. Since $\\mathrm{Var}(h(X))=\\mathrm{Var}(h(-X))$, we have $\\rho = \\frac{\\mathrm{Cov}(h(X), h(-X))}{\\mathrm{Var}(h(X))}$, so $\\mathrm{Cov}(h(X), h(-X)) = \\rho \\mathrm{Var}(h(X))$. The VRF is then $\\mathrm{VRF} = \\frac{\\mathrm{Var}(h(X))}{\\mathrm{Var}(h(X)) + \\rho \\mathrm{Var}(h(X))} = \\frac{1}{1+\\rho}$, assuming $\\mathrm{Var}(h(X)) \\neq 0$.\n\nNow, we specialize this result for the two given cases.\n\nCase 1: $h(x)=x$\nThe function $h(x)=x$ is an odd function. We need to compute the terms in the VRF expression.\nThe variance is $\\mathrm{Var}(h(X)) = \\mathrm{Var}(X)$.\nThe covariance term is $\\mathrm{Cov}(h(X), h(-X)) = \\mathrm{Cov}(X, -X)$.\nUsing the property $\\mathrm{Cov}(aU, bV) = ab\\mathrm{Cov}(U,V)$, with $a=1, b=-1, U=V=X$, we have:\n$$\n\\mathrm{Cov}(X, -X) = -1 \\cdot \\mathrm{Cov}(X,X) = -\\mathrm{Var}(X)\n$$\nSubstituting these into the general VRF expression:\n$$\n\\mathrm{VRF} = \\frac{\\mathrm{Var}(X)}{\\mathrm{Var}(X) + (-\\mathrm{Var}(X))} = \\frac{\\mathrm{Var}(X)}{0}\n$$\nProvided that $X$ is not a constant (i.e., $\\mathrm{Var}(X)  0$), this ratio is infinite. This indicates a perfect variance reduction.\nLet's analyze the AV estimator directly for this case:\n$$\n\\widehat{\\mu}_{\\mathrm{AV}} = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{h(X_{i}) + h(-X_{i})}{2} = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{X_i + (-X_i)}{2} = \\frac{1}{n}\\sum_{i=1}^{n} 0 = 0\n$$\nThe estimator is always $0$, a constant. Therefore, its variance is $\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{AV}}) = 0$. Since $X$ is symmetric about $0$, the true mean is $\\mu=\\mathbb{E}[X]=0$. The AV estimator gives the exact answer with zero variance. As long as $\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{MC}}) = \\frac{1}{2n}\\mathrm{Var}(X)  0$, the ratio of variances is infinite.\n$$\n\\mathrm{VRF}_{h(x)=x} = \\infty\n$$\n\nCase 2: $h(x)=x^2$\nThe function $h(x)=x^2$ is an even function, meaning $h(-x)=(-x)^2=x^2=h(x)$.\nThe variance is $\\mathrm{Var}(h(X)) = \\mathrm{Var}(X^2)$.\nThe covariance term is:\n$$\n\\mathrm{Cov}(h(X), h(-X)) = \\mathrm{Cov}(X^2, (-X)^2) = \\mathrm{Cov}(X^2, X^2) = \\mathrm{Var}(X^2)\n$$\nSubstituting these into the VRF expression:\n$$\n\\mathrm{VRF} = \\frac{\\mathrm{Var}(X^2)}{\\mathrm{Var}(X^2) + \\mathrm{Var}(X^2)} = \\frac{\\mathrm{Var}(X^2)}{2\\mathrm{Var}(X^2)}\n$$\nAssuming $X^2$ is not a constant (which is true unless $X$ takes only values $\\pm c$ for some constant $c$), we have $\\mathrm{Var}(X^2)  0$ and the expression simplifies to:\n$$\n\\mathrm{VRF}_{h(x)=x^2} = \\frac{1}{2}\n$$\n\nFinally, we provide a qualitative justification for the difference.\nThe effectiveness of antithetic variates hinges on the covariance between $h(X)$ and $h(-X)$. Variance reduction is achieved when this covariance is negative.\nFor a function $h(x)$ and a random variable $X$ with a distribution symmetric about $0$, the properties of $h$ determine the sign of this covariance.\n\n- When $h(x)$ is an odd function, $h(-x) = -h(x)$. This implies $h(-X) = -h(X)$. The correlation between $h(X)$ and $h(-X)$ is $\\mathrm{Corr}(h(X), -h(X)) = -1$. This is a perfect negative correlation, which is the ideal scenario for variance reduction. The antithetic pair average, $\\frac{h(X) + h(-X)}{2} = \\frac{h(X) - h(X)}{2} = 0$, is a constant. Since the true mean $\\mathbb{E}[h(X)]$ is $0$ for a symmetric $X$ and odd $h$, the estimator gives the exact result with zero variance, leading to infinite VRF.\n\n- When $h(x)$ is an even function, $h(-x) = h(x)$. This implies $h(-X) = h(X)$. The correlation between $h(X)$ and $h(-X)$ is $\\mathrm{Corr}(h(X), h(X)) = +1$. This is a perfect positive correlation, which is the worst-case scenario for variance reduction as it maximizes the variance of the sum. For each pair $(X_i, -X_i)$, the two function evaluations $h(X_i)$ and $h(-X_i)$ yield the same value. The antithetic estimator $\\widehat{\\mu}_{\\mathrm{AV}} = \\frac{1}{n} \\sum \\frac{h(X_i) + h(X_i)}{2} = \\frac{1}{n} \\sum h(X_i)$ behaves like a standard MC estimator with only $n$ samples. For the same computational budget of $2n$ function evaluations, the standard MC estimator uses $2n$ independent samples. Therefore, the AV estimator is based on half the number of independent pieces of information, and as variance is inversely proportional to sample size, its variance is double that of the standard MC estimator. This results in a $\\mathrm{VRF}$ of $1/2$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\infty  \\frac{1}{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Moving from theory to practical application, this problem addresses a common task in designing Monte Carlo studies. Using statistical estimates gathered from a pilot simulation, you will calculate the number of antithetic pairs required to achieve a target precision for your final estimate. This exercise demonstrates how the variance reduction achieved through antithetic sampling directly translates into a quantifiable and more efficient computational budget. ",
            "id": "3288414",
            "problem": "A practitioner is estimating the mean of a simulation output using antithetic variates within the Monte Carlo framework. Let $U \\sim \\mathrm{Uniform}(0,1)$ and define $X = h(U)$ and $X' = h(1-U)$, where $h(u)$ is a measurable function with finite second moments. The antithetic variates estimator with $n$ independent and identically distributed (i.i.d.) pairs is constructed as the sample mean of pair averages, namely\n$$\nY_i = \\frac{X_i + X'_i}{2}, \\quad \\hat{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^{n} Y_i,\n$$\nwith $(X_i, X'_i)$ formed from $U_i$ and $1-U_i$, and pairs $\\{(X_i,X'_i)\\}_{i=1}^n$ independent across $i$. Assume asymptotic normality via the Central Limit Theorem (CLT) and construct a two-sided $(1-\\alpha)$ confidence interval (CI) for the mean $\\mu = \\mathbb{E}[X]$ with half-width $\\epsilon$, using the standard normal quantile.\n\nA pilot run with $n_0$ antithetic pairs produced empirical estimates for the variance of the single-run output and the within-pair covariance:\n$$\n\\hat{\\sigma}^2_X = 0.047, \\quad \\widehat{\\mathrm{Cov}}(X,X') = -0.025.\n$$\nYou aim to achieve a target CI half-width of $\\epsilon = 0.01$ at confidence level $(1-\\alpha) = 0.95$. Treat the variance entering the CLT for the pair-averaged $Y_i$ as unknown and plug in the pilot estimates to determine the required number of pairs $n$ so that the half-width is at most $\\epsilon$. Use the standard normal critical value and choose the smallest integer $n$ that satisfies the requirement.\n\nCompute the required number of pairs $n$. The final answer must be a single number. No units are involved.",
            "solution": "The problem requires the determination of the number of antithetic pairs, $n$, needed to estimate a mean $\\mu$ with a specified precision. We begin by analyzing the statistical properties of the antithetic variates estimator.\n\nThe estimator for the mean $\\mu = \\mathbb{E}[X]$ is given by $\\hat{\\mu}_n = \\frac{1}{n} \\sum_{i=1}^{n} Y_i$, where each $Y_i$ is the average of an antithetic pair, $Y_i = \\frac{X_i + X'_i}{2}$. The random variables $X_i$ and $X'_i$ are generated as $X_i = h(U_i)$ and $X'_i = h(1-U_i)$, where $\\{U_i\\}_{i=1}^n$ are independent and identically distributed (i.i.d.) random variables from a $\\mathrm{Uniform}(0,1)$ distribution.\n\nFirst, we establish that $\\hat{\\mu}_n$ is an unbiased estimator of $\\mu$. The expected value of $Y_i$ is:\n$$\n\\mathbb{E}[Y_i] = \\mathbb{E}\\left[\\frac{X_i + X'_i}{2}\\right] = \\frac{1}{2}(\\mathbb{E}[X_i] + \\mathbb{E}[X'_i])\n$$\nSince $U_i \\sim \\mathrm{Uniform}(0,1)$, the random variable $1-U_i$ is also distributed as $\\mathrm{Uniform}(0,1)$. This implies that $X_i = h(U_i)$ and $X'_i = h(1-U_i)$ are identically distributed. Consequently, they have the same expectation:\n$$\n\\mathbb{E}[X_i] = \\mathbb{E}[X'_i] = \\mu\n$$\nTherefore, the expectation of $Y_i$ is:\n$$\n\\mathbb{E}[Y_i] = \\frac{1}{2}(\\mu + \\mu) = \\mu\n$$\nThe expectation of the overall estimator $\\hat{\\mu}_n$ is then:\n$$\n\\mathbb{E}[\\hat{\\mu}_n] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} Y_i\\right] = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}[Y_i] = \\frac{1}{n}(n\\mu) = \\mu\n$$\nThus, the estimator is unbiased.\n\nNext, we determine the variance of the estimator. Since the pairs $(X_i, X'_i)$ are independent across $i$, the random variables $Y_i$ are i.i.d. The variance of $\\hat{\\mu}_n$ is:\n$$\n\\mathrm{Var}(\\hat{\\mu}_n) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} Y_i\\right) = \\frac{1}{n^2}\\sum_{i=1}^{n} \\mathrm{Var}(Y_i) = \\frac{1}{n}\\mathrm{Var}(Y_1)\n$$\nLet's denote $\\sigma_Y^2 = \\mathrm{Var}(Y_1)$. The variance of a single pair average $Y_1$ is:\n$$\n\\sigma_Y^2 = \\mathrm{Var}\\left(\\frac{X_1 + X'_1}{2}\\right) = \\frac{1}{4}\\mathrm{Var}(X_1 + X'_1)\n$$\nUsing the formula for the variance of a sum of correlated random variables:\n$$\n\\mathrm{Var}(X_1 + X'_1) = \\mathrm{Var}(X_1) + \\mathrm{Var}(X'_1) + 2\\mathrm{Cov}(X_1, X'_1)\n$$\nAs $X_1$ and $X'_1$ are identically distributed, they have the same variance, $\\mathrm{Var}(X_1) = \\mathrm{Var}(X'_1) = \\sigma_X^2$. Substituting this into the expression for $\\sigma_Y^2$:\n$$\n\\sigma_Y^2 = \\frac{1}{4}(\\sigma_X^2 + \\sigma_X^2 + 2\\mathrm{Cov}(X_1, X'_1)) = \\frac{1}{2}(\\sigma_X^2 + \\mathrm{Cov}(X_1, X'_1))\n$$\nThe Central Limit Theorem (CLT) states that for large $n$, the distribution of $\\hat{\\mu}_n$ is approximately normal:\n$$\n\\hat{\\mu}_n \\approx \\mathcal{N}\\left(\\mu, \\frac{\\sigma_Y^2}{n}\\right)\n$$\nA two-sided $(1-\\alpha)$ confidence interval for $\\mu$ is given by $\\hat{\\mu}_n \\pm \\epsilon$, where the half-width $\\epsilon$ is:\n$$\n\\epsilon = z_{1-\\alpha/2} \\frac{\\sigma_Y}{\\sqrt{n}}\n$$\nHere, $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution. We are given a target half-width $\\epsilon$ and must find the minimum required number of pairs $n$. Rearranging the equation for $n$:\n$$\n\\sqrt{n} = \\frac{z_{1-\\alpha/2} \\sigma_Y}{\\epsilon} \\implies n = \\left(\\frac{z_{1-\\alpha/2} \\sigma_Y}{\\epsilon}\\right)^2\n$$\nThe true variance $\\sigma_Y^2$ is unknown. The problem instructs us to use the estimates from a pilot run to determine $n$. We substitute the sample estimates for the true population parameters:\n$$\n\\hat{\\sigma}_Y^2 = \\frac{1}{2}(\\hat{\\sigma}_X^2 + \\widehat{\\mathrm{Cov}}(X,X'))\n$$\nThe given values from the pilot run are:\n- Estimated variance of a single output: $\\hat{\\sigma}_X^2 = 0.047$.\n- Estimated covariance of an antithetic pair: $\\widehat{\\mathrm{Cov}}(X,X') = -0.025$.\n\nUsing these values, we compute the estimated variance of the pair-averaged output:\n$$\n\\hat{\\sigma}_Y^2 = \\frac{1}{2}(0.047 + (-0.025)) = \\frac{1}{2}(0.022) = 0.011\n$$\nThe required confidence level is $(1-\\alpha) = 0.95$, so $\\alpha = 0.05$. The critical value is $z_{1-\\alpha/2} = z_{0.975}$, which is the standard normal quantile corresponding to a cumulative probability of $0.975$. The value is $z_{0.975} \\approx 1.96$. The target half-width is $\\epsilon = 0.01$.\n\nNow we substitute all values into the formula for $n$:\n$$\nn = \\left(\\frac{z_{0.975} \\hat{\\sigma}_Y}{\\epsilon}\\right)^2 = \\left(\\frac{1.96 \\times \\sqrt{0.011}}{0.01}\\right)^2\n$$\nLet's compute the value:\n$$\nn = \\frac{(1.96)^2 \\times 0.011}{(0.01)^2} = \\frac{3.8416 \\times 0.011}{0.0001} = \\frac{0.0422576}{0.0001} = 422.576\n$$\nSince the number of pairs $n$ must be an integer, we must choose the smallest integer that satisfies the condition $n \\ge 422.576$. This is achieved by taking the ceiling of the calculated value:\n$$\nn = \\lceil 422.576 \\rceil = 423\n$$\nTherefore, $423$ antithetic pairs are required to achieve the desired confidence interval half-width.",
            "answer": "$$\\boxed{423}$$"
        },
        {
            "introduction": "Efficient implementation is crucial for leveraging the power of large-scale Monte Carlo simulations. This final practice challenges you to think like a computational scientist, focusing on the correct data architecture for a vectorized implementation. You will determine how to structure data arrays to manage antithetic pairs for multiple estimators simultaneously, ensuring the integrity of the statistical method by preserving the pairing structure and preventing cross-contamination in calculations. ",
            "id": "3288419",
            "problem": "Consider estimating $K$ expectations of the form $\\mu_k = \\mathbb{E}[g_k(U)]$ for $k \\in \\{1,\\dots,K\\}$, where each $g_k:[0,1]^d \\to \\mathbb{R}$ is a measurable function and $U \\sim \\text{Uniform}([0,1]^d)$. The Monte Carlo method estimates each $\\mu_k$ by averaging $g_k$ evaluated at independent samples of $U$. The antithetic variates technique uses paired samples $\\left(U_i, 1 - U_i\\right)$ for $i \\in \\{1,\\dots,m\\}$ to induce negative correlation between evaluations, with the goal of reducing the variance of the averaged estimator. You intend to implement the antithetic pairing in a vectorized manner so that the pairs $\\left(U_i, 1 - U_i\\right)$ remain matched, and so that sample covariances are computed strictly within pairs without mixing across different pairs or across different estimators $k$.\n\nThe implementation must satisfy the following two requirements derived from the classical Monte Carlo setup and the definition of antithetic variates:\n\n1. The estimator for each $\\mu_k$ uses the pairwise average within each antithetic pair, namely for each $i$,\n$$A_{i,k} = \\frac{1}{2}\\left(g_k(U_i) + g_k(1-U_i)\\right),$$\nand the final estimator is\n$$\\hat{\\mu}_k = \\frac{1}{m}\\sum_{i=1}^m A_{i,k}.$$\n\n2. The sample covariance to diagnose antithetic correlation for each $k$ must be computed strictly within each pair $i$, using only the two antithetic replicates for that same $i$ and that same $k$, and then averaged over pairs. That is, letting $Y_{i,0,k} = g_k(U_i)$ and $Y_{i,1,k} = g_k(1-U_i)$, a valid within-pair sample covariance for estimator $k$ is of the form\n$$C_k = \\frac{1}{m}\\sum_{i=1}^{m} \\left(Y_{i,0,k} - A_{i,k}\\right)\\left(Y_{i,1,k} - A_{i,k}\\right),$$\nensuring no cross-contamination across different pairs or across different $k$.\n\nYou will implement this computation in vectorized form by organizing the data into arrays and taking averages along axes corresponding to the antithetic-pair dimension and the pair index dimension, without any operations that break the one-to-one matching between $U_i$ and $1-U_i$ or that aggregate covariances across different $k$.\n\nWhich option correctly describes a vectorized arrangement and aggregation scheme that guarantees antithetic pairing is preserved and covariances are computed within pairs without cross-contamination across estimators?\n\nA. Construct a three-dimensional tensor $X \\in [0,1]^{m \\times 2 \\times d}$ with entries $X_{i,0,:} = U_i$ and $X_{i,1,:} = 1 - U_i$. For each $k \\in \\{1,\\dots,K\\}$, evaluate $Y_{i,j,k} = g_k(X_{i,j,:})$ to obtain a tensor $Y \\in \\mathbb{R}^{m \\times 2 \\times K}$. Compute pairwise means $A_{i,k} = \\frac{1}{2}\\sum_{j=0}^{1} Y_{i,j,k}$ by averaging over the antithetic axis $j$, then $\\hat{\\mu}_k = \\frac{1}{m}\\sum_{i=1}^{m} A_{i,k}$ by averaging over the pair index $i$. Compute the within-pair covariance for each $k$ as $C_k = \\frac{1}{m}\\sum_{i=1}^{m} \\left(Y_{i,0,k} - A_{i,k}\\right)\\left(Y_{i,1,k} - A_{i,k}\\right)$. This uses a dedicated antithetic axis and never mixes across different $i$ or across different $k$.\n\nB. Form a matrix $Z \\in [0,1]^{2m \\times d}$ by stacking all $U_i$ followed by all $1-U_i$ along a single sample axis. For each $k$, evaluate $W_{t,k} = g_k(Z_{t,:})$ for $t \\in \\{1,\\dots,2m\\}$ and compute $\\hat{\\mu}_k = \\frac{1}{2m}\\sum_{t=1}^{2m} W_{t,k}$. Compute covariance by taking the sample covariance over all $2m$ values $W_{t,k}$ against $\\hat{\\mu}_k$, e.g., $C_k = \\frac{1}{2m}\\sum_{t=1}^{2m} \\left(W_{t,k} - \\hat{\\mu}_k\\right)^2$, treating all $2m$ as a single pool.\n\nC. Generate $U_1,\\dots,U_m$ and compute $1-U_1,\\dots,1-U_m$. Sort the $U_i$ to produce $U_{(1)},\\dots,U_{(m)}$ and sort the $1-U_i$ to produce $V_{(1)},\\dots,V_{(m)}$. Pair $U_{(i)}$ with $V_{(i)}$ for $i \\in \\{1,\\dots,m\\}$. Compute $\\hat{\\mu}_k$ and $C_k$ by averaging and covariance over the $2m$ sorted entries per $k$.\n\nD. Construct $X \\in [0,1]^{m \\times 2 \\times d}$ and $Y \\in \\mathbb{R}^{m \\times 2 \\times K}$ as in option A, compute $A_{i,k}$ and $\\hat{\\mu}_k$ as in option A, but compute a single pooled covariance $C_{\\text{pooled}} = \\frac{1}{m K}\\sum_{k=1}^{K}\\sum_{i=1}^{m} \\left(Y_{i,0,k} - \\hat{\\mu}_k\\right)\\left(Y_{i,1,k} - \\hat{\\mu}_k\\right)$ over all $k$, using the global means $\\hat{\\mu}_k$ rather than the pairwise means $A_{i,k}$, and then report $C_{\\text{pooled}}$ as the antithetic covariance for every $k$.",
            "solution": "The problem statement is valid. It describes the antithetic variates method for variance reduction in Monte Carlo estimation, a standard technique in stochastic simulation. The problem provides precise mathematical definitions for the estimator and a diagnostic sample covariance, along with explicit constraints on the implementation (vectorization, preservation of pairing, and no cross-contamination). The question is to identify which of the provided computational schemes correctly implements this well-defined algorithm. The problem is scientifically grounded, well-posed, and objective.\n\nThe central challenge is to arrange the data in a vectorized manner that respects the inherent structure of the antithetic variates method. This structure involves three key indices:\n1.  The pair index, $i \\in \\{1, \\dots, m\\}$.\n2.  The antithetic index, which distinguishes between the original sample and its antithetic counterpart within a pair (e.g., $j \\in \\{0, 1\\}$ for $U_i$ and $1-U_i$).\n3.  The estimator index, $k \\in \\{1, \\dots, K\\}$, for the different functions $g_k$.\n\nA correct vectorized implementation must use a data structure, such as a multi-dimensional array (tensor), that has distinct axes corresponding to these indices. This preserves the pairings and prevents \"cross-contamination\" as required. Let's formalize the required computations based on such a structure.\n\nLet $U_i$ be $m$ independent samples from $\\text{Uniform}([0,1]^d)$. The antithetic pairs are $(U_i, 1-U_i)$.\nThe evaluated function values can be organized into a three-dimensional tensor $Y \\in \\mathbb{R}^{m \\times 2 \\times K}$, where the axes correspond to the pair index $i$, the antithetic index $j$, and the estimator index $k$, respectively. The entries of this tensor are:\n$$Y_{i,0,k} = g_k(U_i)$$\n$$Y_{i,1,k} = g_k(1-U_i)$$\nThe problem defines two required computations:\n\n1.  **The Estimator $\\hat{\\mu}_k$**: This involves a two-step averaging process.\n    a. First, for each pair $i$ and estimator $k$, compute the pairwise average:\n    $$A_{i,k} = \\frac{1}{2}(Y_{i,0,k} + Y_{i,1,k})$$\n    In a vectorized scheme, this corresponds to taking the mean of the tensor $Y$ along the antithetic axis (axis $j$). This operation would produce an intermediate tensor $A \\in \\mathbb{R}^{m \\times K}$.\n    b. Second, average these pairwise means over all pairs:\n    $$\\hat{\\mu}_k = \\frac{1}{m}\\sum_{i=1}^m A_{i,k}$$\n    This corresponds to taking the mean of the intermediate tensor $A$ along the pair-index axis (axis $i$). This results in a vector of length $K$.\n\n2.  **The Diagnostic Covariance $C_k$**: This must be computed strictly within each pair and for each estimator.\n    $$C_k = \\frac{1}{m}\\sum_{i=1}^{m} (Y_{i,0,k} - A_{i,k})(Y_{i,1,k} - A_{i,k})$$\n    To compute this in a vectorized way:\n    a. Use the previously computed pairwise means $A_{i,k}$. Note that for the term corresponding to pair $i$, we must use the mean $A_{i,k}$ from that specific pair, not the overall mean $\\hat{\\mu}_k$.\n    b. For each $i$ and $k$, calculate the product of deviations: $(Y_{i,0,k} - A_{i,k})(Y_{i,1,k} - A_{i,k})$. This requires broadcasting the tensor $A$ (of shape $m \\times K$) to match the shape of $Y$ ($m \\times 2 \\times K$) for subtraction, then performing a product along the antithetic axis.\n    c. Finally, average these products over all pairs:\n    $$C_k = \\frac{1}{m}\\sum_{i=1}^m [\\text{product for pair } i]$$\n    This corresponds to an average along the pair-index axis (axis $i$). The structure ensures that calculations for a given $k$ use only data related to $g_k$, and calculations for a given pair $i$ use only data from that pair.\n\nNow we evaluate each option against this correct scheme.\n\nA. Construct a three-dimensional tensor $X \\in [0,1]^{m \\times 2 \\times d}$ with entries $X_{i,0,:} = U_i$ and $X_{i,1,:} = 1 - U_i$. For each $k \\in \\{1,\\dots,K\\}$, evaluate $Y_{i,j,k} = g_k(X_{i,j,:})$ to obtain a tensor $Y \\in \\mathbb{R}^{m \\times 2 \\times K}$. Compute pairwise means $A_{i,k} = \\frac{1}{2}\\sum_{j=0}^{1} Y_{i,j,k}$ by averaging over the antithetic axis $j$, then $\\hat{\\mu}_k = \\frac{1}{m}\\sum_{i=1}^{m} A_{i,k}$ by averaging over the pair index $i$. Compute the within-pair covariance for each $k$ as $C_k = \\frac{1}{m}\\sum_{i=1}^{m} \\left(Y_{i,0,k} - A_{i,k}\\right)\\left(Y_{i,1,k} - A_{i,k}\\right)$. This uses a dedicated antithetic axis and never mixes across different $i$ or across different $k$.\n\nThis option perfectly matches the derived correct procedure. The data is organized into a $m \\times 2 \\times K$ tensor, which correctly represents the three-level structure of the problem. The computation of $\\hat{\\mu}_k$ follows the two-step averaging process (first over the antithetic axis, then the pair axis). The computation of $C_k$ is stated exactly as required by the problem's definition, implying the use of the pair-specific means $A_{i,k}$ and separate calculations for each $k$. The concluding sentence accurately summarizes why this approach is correct.\nVerdict: **Correct**.\n\nB. Form a matrix $Z \\in [0,1]^{2m \\times d}$ by stacking all $U_i$ followed by all $1-U_i$ along a single sample axis. For each $k$, evaluate $W_{t,k} = g_k(Z_{t,:})$ for $t \\in \\{1,\\dots,2m\\}$ and compute $\\hat{\\mu}_k = \\frac{1}{2m}\\sum_{t=1}^{2m} W_{t,k}$. Compute covariance by taking the sample covariance over all $2m$ values $W_{t,k}$ against $\\hat{\\mu}_k$, e.g., $C_k = \\frac{1}{2m}\\sum_{t=1}^{2m} \\left(W_{t,k} - \\hat{\\mu}_k\\right)^2$, treating all $2m$ as a single pool.\n\nThis option fails to preserve the pairing structure required for the covariance calculation. While the calculation of $\\hat{\\mu}_k$ yields the correct numerical value, the method (\"treating all $2m$ as a single pool\") violates the spirit of the antithetic estimator which relies on pairwise averaging. More critically, the covariance calculation is completely wrong. It computes the *sample variance* of the $2m$ pooled outcomes, not the within-pair *sample covariance* between $g_k(U_i)$ and $g_k(1-U_i)$. This violates Requirement 2 by not computing a covariance, by using the global mean $\\hat{\\mu}_k$ instead of the pairwise mean $A_{i,k}$, and by breaking the \"strictly within each pair\" constraint.\nVerdict: **Incorrect**.\n\nC. Generate $U_1,\\dots,U_m$ and compute $1-U_1,\\dots,1-U_m$. Sort the $U_i$ to produce $U_{(1)},\\dots,U_{(m)}$ and sort the $1-U_i$ to produce $V_{(1)},\\dots,V_{(m)}$. Pair $U_{(i)}$ with $V_{(i)}$ for $i \\in \\{1,\\dots,m\\}$. Compute $\\hat{\\mu}_k$ and $C_k$ by averaging and covariance over the $2m$ sorted entries per $k$.\n\nThis option describes a fundamentally different procedure. By sorting the sets $\\{U_i\\}$ and $\\{1-U_i\\}$ independently and then re-pairing them based on rank, it completely destroys the original antithetic pairing of $(U_i, 1-U_i)$. The resulting pairs $(U_{(i)}, V_{(i)})$ are not antithetic pairs in the sense required by the method. Therefore, any subsequent calculation of estimators or covariances based on these new pairs does not implement the specified antithetic variates technique. This violates the problem-statement's core premise.\nVerdict: **Incorrect**.\n\nD. Construct $X \\in [0,1]^{m \\times 2 \\times d}$ and $Y \\in \\mathbb{R}^{m \\times 2 \\times K}$ as in option A, compute $A_{i,k}$ and $\\hat{\\mu}_k$ as in option A, but compute a single pooled covariance $C_{\\text{pooled}} = \\frac{1}{m K}\\sum_{k=1}^{K}\\sum_{i=1}^{m} \\left(Y_{i,0,k} - \\hat{\\mu}_k\\right)\\left(Y_{i,1,k} - \\hat{\\mu}_k\\right)$ over all $k$, using the global means $\\hat{\\mu}_k$ rather than the pairwise means $A_{i,k}$, and then report $C_{\\text{pooled}}$ as the antithetic covariance for every $k$.\n\nThis option starts correctly with the data arrangement and the calculation of $\\hat{\\mu}_k$, but it falters significantly in the covariance calculation, violating Requirement 2 in two distinct ways. First, it uses the global mean for estimator $k$, $\\hat{\\mu}_k$, in the deviation calculation, instead of the required pair-specific mean $A_{i,k}$. Requirement $2$ is explicit in its use of $A_{i,k}$. Second, it pools the results across all estimators $k=1, \\dots, K$ to compute a single $C_{\\text{pooled}}$, which violates the constraint that covariances must be computed \"without mixing across different estimators $k$\". The problem asks for a diagnosis $C_k$ for each estimator $k$, not a single pooled value.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}