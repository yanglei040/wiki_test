{
    "hands_on_practices": [
        {
            "introduction": "Mastering the selection of control variates begins with understanding the core mechanics of variance minimization. This first exercise provides a foundational workout, asking you to derive and apply the classic formula for optimal coefficients. By working through a concrete example where variables share underlying noise sources, you will gain a practical intuition for how the covariance structure dictates the best way to combine controls and quantify the resulting improvement in estimator precision.",
            "id": "3325570",
            "problem": "You are estimating the unknown mean $\\,\\mu = \\mathbb{E}[X]\\,$ of a random variable $\\,X\\,$ by Monte Carlo sampling with control variates. In each replicate, correlated sampling with common random numbers is used: the same latent noise sources drive both the target $\\,X\\,$ and the controls $\\,C_{j}\\,$. Let $\\,U_{0}, U_{1}, U_{2}\\,$ and $\\,V_{1}, V_{2}, V_{3}\\,$ be independent standard normal random variables. The target and three candidate controls are defined by\n$$\nX \\;=\\; 3\\,U_{0} \\;+\\; 2\\,U_{1} \\;+\\; 1\\,U_{2}, \n$$\n$$\nC_{1} \\;=\\; 2\\,U_{0} \\;+\\; 1\\,U_{1} \\;+\\; \\sqrt{0.5}\\,V_{1},\\qquad\nC_{2} \\;=\\; 1\\,U_{0} \\;+\\; 2\\,U_{2} \\;+\\; \\sqrt{0.25}\\,V_{2},\\qquad\nC_{3} \\;=\\; 1.5\\,U_{1} \\;+\\; 0.5\\,U_{2} \\;+\\; \\sqrt{0.75}\\,V_{3}.\n$$\nAssume $\\,\\mathbb{E}[C_{j}] = 0\\,$ is known for all $\\,j\\,$. Consider the single-replicate controlled estimator\n$$\nY(\\boldsymbol{a}) \\;=\\; X \\;-\\; \\sum_{j\\in S} a_{j}\\,C_{j},\n$$\nwhere $\\,S\\,$ is a chosen subset of controls and $\\,\\boldsymbol{a} = (a_{j})_{j\\in S}\\,$ are deterministic coefficients. The unbiasedness of $\\,Y(\\boldsymbol{a})\\,$ follows from the definition of control variates.\n\nStarting from first principles—namely the definition of variance, the bilinearity of covariance, and the calculus of unconstrained quadratic minimization—derive the coefficient vector $\\,\\boldsymbol{a}^{\\star}\\,$ that minimizes $\\,\\mathrm{Var}\\!\\left(Y(\\boldsymbol{a})\\right)\\,$ for a given subset $\\,S\\,$. Then use the given correlation structure induced by common random numbers to compute the following:\n\n1. The optimal coefficient vector $\\,\\boldsymbol{a}^{\\star}\\,$ and the resulting minimal single-replicate variance $\\,\\mathrm{Var}\\!\\left(Y(\\boldsymbol{a}^{\\star})\\right)\\,$ when $\\,S = \\{1,2\\}\\,$.\n2. The optimal coefficient vector $\\,\\boldsymbol{a}^{\\star}\\,$ and the resulting minimal single-replicate variance $\\,\\mathrm{Var}\\!\\left(Y(\\boldsymbol{a}^{\\star})\\right)\\,$ when $\\,S = \\{1,3\\}\\,$.\n\nFinally, determine which subset $\\,S\\,$ yields the smaller minimal variance and report, as your final answer, the two optimal coefficients for the better subset followed by that minimal variance.\n\nYour final answer must be a single row matrix containing three entries $\\,\\big(a_{j}^{\\star}\\big)_{j\\in S}\\,$ and $\\,\\mathrm{Var}\\!\\left(Y(\\boldsymbol{a}^{\\star})\\right)\\,$. No rounding is required. No units are involved.",
            "solution": "The user wants me to solve a problem about control variates in Monte Carlo simulation.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n\nThe problem provides the following data and definitions:\n-   Independent standard normal random variables: $U_{0}, U_{1}, U_{2}, V_{1}, V_{2}, V_{3}$.\n-   Target random variable: $X = 3\\,U_{0} + 2\\,U_{1} + 1\\,U_{2}$.\n-   Candidate control variates:\n    -   $C_{1} = 2\\,U_{0} + 1\\,U_{1} + \\sqrt{0.5}\\,V_{1}$\n    -   $C_{2} = 1\\,U_{0} + 2\\,U_{2} + \\sqrt{0.25}\\,V_{2}$\n    -   $C_{3} = 1.5\\,U_{1} + 0.5\\,U_{2} + \\sqrt{0.75}\\,V_{3}$\n-   Known expectations for controls: $\\mathbb{E}[C_{j}] = 0$ for all $j$.\n-   Controlled estimator for a subset of controls $S$ and coefficient vector $\\boldsymbol{a} = (a_{j})_{j\\in S}$: $Y(\\boldsymbol{a}) = X - \\sum_{j\\in S} a_{j}\\,C_{j}$.\n-   The objective is to find the optimal coefficient vector $\\boldsymbol{a}^{\\star}$ that minimizes $\\mathrm{Var}(Y(\\boldsymbol{a}))$. This must be done for two subsets, $S = \\{1,2\\}$ and $S = \\{1,3\\}$. Finally, compare the minimal variances and report the coefficients and variance for the better subset.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The problem lies within the well-established field of Monte Carlo methods, specifically variance reduction using control variates. The setup using linear combinations of Gaussian random variables is a standard, tractable model for demonstrating the principles.\n2.  **Well-Posed:** The objective is to minimize the variance of the controlled estimator, which is a quadratic function of the coefficients. This is an unconstrained quadratic programming problem that has a unique solution, provided the covariance matrix of the controls is positive definite (which it will be, as the controls are not linearly dependent).\n3.  **Objective:** The problem is stated using precise mathematical definitions and language. There is no subjectivity or ambiguity.\n4.  **Complete and Consistent:** All necessary information is provided. The definitions of $X$ and $C_j$ are given, and the statistical properties of the underlying noise terms ($U_i, V_j$) are specified as independent standard normal. The condition $\\mathbb{E}[C_{j}] = 0$ is consistent with their definitions, as they are linear combinations of zero-mean variables. The problem is self-contained. The tasks are clearly specified.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. It is a standard, well-posed problem in stochastic simulation. I will proceed with a full solution.\n\n**Solution Derivation**\n\nThe problem asks for a derivation from first principles.\n\n**1. General Derivation of Optimal Coefficients**\n\nLet $S$ be a chosen subset of control variates. The controlled estimator is $Y(\\boldsymbol{a}) = X - \\sum_{j \\in S} a_j C_j$. We can write this in vector form as $Y(\\boldsymbol{a}) = X - \\boldsymbol{a}^T \\boldsymbol{C}$, where $\\boldsymbol{C}$ is the column vector of controls $(C_j)_{j \\in S}$ and $\\boldsymbol{a}$ is the column vector of coefficients $(a_j)_{j \\in S}$.\n\nThe objective is to minimize the variance of $Y(\\boldsymbol{a})$:\n$$\n\\mathrm{Var}(Y(\\boldsymbol{a})) = \\mathrm{Var}(X - \\boldsymbol{a}^T \\boldsymbol{C})\n$$\nUsing the bilinearity of covariance, where $\\mathrm{Var}(Z) = \\mathrm{Cov}(Z,Z)$:\n$$\n\\mathrm{Var}(Y(\\boldsymbol{a})) = \\mathrm{Cov}(X - \\boldsymbol{a}^T \\boldsymbol{C}, X - \\boldsymbol{a}^T \\boldsymbol{C})\n$$\n$$\n\\mathrm{Var}(Y(\\boldsymbol{a})) = \\mathrm{Cov}(X,X) - \\mathrm{Cov}(X, \\boldsymbol{a}^T \\boldsymbol{C}) - \\mathrm{Cov}(\\boldsymbol{a}^T \\boldsymbol{C}, X) + \\mathrm{Cov}(\\boldsymbol{a}^T \\boldsymbol{C}, \\boldsymbol{a}^T \\boldsymbol{C})\n$$\nSince coefficients $\\boldsymbol{a}$ are deterministic:\n$$\n\\mathrm{Var}(Y(\\boldsymbol{a})) = \\mathrm{Var}(X) - \\boldsymbol{a}^T \\mathrm{Cov}(\\boldsymbol{C}, X) - \\mathrm{Cov}(X, \\boldsymbol{C}) \\boldsymbol{a} + \\boldsymbol{a}^T \\mathrm{Cov}(\\boldsymbol{C}, \\boldsymbol{C}) \\boldsymbol{a}\n$$\nLet $\\sigma_X^2 = \\mathrm{Var}(X)$, $\\boldsymbol{\\sigma}_{XC} = \\mathrm{Cov}(X, \\boldsymbol{C})$ (a column vector), and $\\boldsymbol{\\Sigma}_{CC} = \\mathrm{Cov}(\\boldsymbol{C}, \\boldsymbol{C})$ (the covariance matrix of the controls). Note that $\\mathrm{Cov}(\\boldsymbol{C}, X) = \\mathrm{Cov}(X, \\boldsymbol{C})^T = \\boldsymbol{\\sigma}_{XC}^T$. The variance to be minimized, as a function of $\\boldsymbol{a}$, is:\n$$\nf(\\boldsymbol{a}) = \\sigma_X^2 - 2 \\boldsymbol{a}^T \\boldsymbol{\\sigma}_{XC} + \\boldsymbol{a}^T \\boldsymbol{\\Sigma}_{CC} \\boldsymbol{a}\n$$\nThis is a quadratic function of $\\boldsymbol{a}$. To find the minimum, we take the gradient with respect to $\\boldsymbol{a}$ and set it to zero.\n$$\n\\nabla_{\\boldsymbol{a}} f(\\boldsymbol{a}) = -2 \\boldsymbol{\\sigma}_{XC} + 2 \\boldsymbol{\\Sigma}_{CC} \\boldsymbol{a}\n$$\nSetting the gradient to zero to find the optimal coefficient vector $\\boldsymbol{a}^{\\star}$:\n$$\n-2 \\boldsymbol{\\sigma}_{XC} + 2 \\boldsymbol{\\Sigma}_{CC} \\boldsymbol{a}^{\\star} = \\boldsymbol{0}\n$$\n$$\n\\boldsymbol{\\Sigma}_{CC} \\boldsymbol{a}^{\\star} = \\boldsymbol{\\sigma}_{XC}\n$$\nAssuming $\\boldsymbol{\\Sigma}_{CC}$ is invertible (which it is, as the controls are not perfectly collinear), the optimal coefficient vector is:\n$$\n\\boldsymbol{a}^{\\star} = \\boldsymbol{\\Sigma}_{CC}^{-1} \\boldsymbol{\\sigma}_{XC}\n$$\nThe resulting minimal variance is found by substituting $\\boldsymbol{a}^{\\star}$ back into $f(\\boldsymbol{a})$:\n$$\n\\mathrm{Var}(Y(\\boldsymbol{a}^{\\star})) = \\sigma_X^2 - 2 (\\boldsymbol{a}^{\\star})^T \\boldsymbol{\\sigma}_{XC} + (\\boldsymbol{a}^{\\star})^T \\boldsymbol{\\Sigma}_{CC} \\boldsymbol{a}^{\\star}\n$$\nSubstitute $\\boldsymbol{a}^{\\star} = \\boldsymbol{\\Sigma}_{CC}^{-1} \\boldsymbol{\\sigma}_{XC}$ and $(\\boldsymbol{a}^{\\star})^T = \\boldsymbol{\\sigma}_{XC}^T (\\boldsymbol{\\Sigma}_{CC}^{-1})^T$. Since $\\boldsymbol{\\Sigma}_{CC}$ is symmetric, its inverse is also symmetric, so $(\\boldsymbol{\\Sigma}_{CC}^{-1})^T = \\boldsymbol{\\Sigma}_{CC}^{-1}$.\n$$\n\\mathrm{Var}(Y(\\boldsymbol{a}^{\\star})) = \\sigma_X^2 - 2 \\boldsymbol{\\sigma}_{XC}^T \\boldsymbol{\\Sigma}_{CC}^{-1} \\boldsymbol{\\sigma}_{XC} + (\\boldsymbol{\\sigma}_{XC}^T \\boldsymbol{\\Sigma}_{CC}^{-1}) \\boldsymbol{\\Sigma}_{CC} (\\boldsymbol{\\Sigma}_{CC}^{-1} \\boldsymbol{\\sigma}_{XC})\n$$\n$$\n\\mathrm{Var}(Y(\\boldsymbol{a}^{\\star})) = \\sigma_X^2 - 2 \\boldsymbol{\\sigma}_{XC}^T \\boldsymbol{\\Sigma}_{CC}^{-1} \\boldsymbol{\\sigma}_{XC} + \\boldsymbol{\\sigma}_{XC}^T \\boldsymbol{\\Sigma}_{CC}^{-1} \\boldsymbol{\\sigma}_{XC}\n$$\n$$\n\\mathrm{Var}(Y(\\boldsymbol{a}^{\\star})) = \\sigma_X^2 - \\boldsymbol{\\sigma}_{XC}^T \\boldsymbol{\\Sigma}_{CC}^{-1} \\boldsymbol{\\sigma}_{XC} = \\sigma_X^2 - \\boldsymbol{\\sigma}_{XC}^T \\boldsymbol{a}^{\\star}\n$$\n\n**2. Calculation of Required Variances and Covariances**\n\nThe random variables $U_0, U_1, U_2, V_1, V_2, V_3$ are independent with mean $0$ and variance $1$. For any two variables $A = \\sum \\alpha_i W_i$ and $B = \\sum \\beta_i W_i$, where $W_i$ are these basis random variables, we have $\\mathrm{Var}(A) = \\sum \\alpha_i^2$ and $\\mathrm{Cov}(A,B) = \\sum \\alpha_i \\beta_i$.\n\n- $\\mathrm{Var}(X) = 3^2 + 2^2 + 1^2 = 9 + 4 + 1 = 14$\n- $\\mathrm{Var}(C_1) = 2^2 + 1^2 + (\\sqrt{0.5})^2 = 4 + 1 + 0.5 = 5.5 = \\frac{11}{2}$\n- $\\mathrm{Var}(C_2) = 1^2 + 2^2 + (\\sqrt{0.25})^2 = 1 + 4 + 0.25 = 5.25 = \\frac{21}{4}$\n- $\\mathrm{Var}(C_3) = (1.5)^2 + (0.5)^2 + (\\sqrt{0.75})^2 = 2.25 + 0.25 + 0.75 = 3.25 = \\frac{13}{4}$\n\n- $\\mathrm{Cov}(X, C_1) = (3)(2) + (2)(1) + (1)(0) = 8$\n- $\\mathrm{Cov}(X, C_2) = (3)(1) + (2)(0) + (1)(2) = 5$\n- $\\mathrm{Cov}(X, C_3) = (3)(0) + (2)(1.5) + (1)(0.5) = 3 + 0.5 = 3.5 = \\frac{7}{2}$\n\n- $\\mathrm{Cov}(C_1, C_2) = (2)(1) + (1)(0) + (0)(2) = 2$\n- $\\mathrm{Cov}(C_1, C_3) = (2)(0) + (1)(1.5) + (0)(0.5) = 1.5 = \\frac{3}{2}$\n- $\\mathrm{Cov}(C_2, C_3) = (1)(0) + (0)(1.5) + (2)(0.5) = 1$\n\n**3. Analysis for Subset $S = \\{1, 2\\}$**\n\nHere, $\\boldsymbol{C} = (C_1, C_2)^T$.\nThe required vectors and matrices are:\n$\\boldsymbol{\\sigma}_{XC} = \\begin{pmatrix} \\mathrm{Cov}(X, C_1) \\\\ \\mathrm{Cov}(X, C_2) \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 5 \\end{pmatrix}$\n$\\boldsymbol{\\Sigma}_{CC} = \\begin{pmatrix} \\mathrm{Var}(C_1) & \\mathrm{Cov}(C_1, C_2) \\\\ \\mathrm{Cov}(C_2, C_1) & \\mathrm{Var}(C_2) \\end{pmatrix} = \\begin{pmatrix} 5.5 & 2 \\\\ 2 & 5.25 \\end{pmatrix} = \\begin{pmatrix} \\frac{11}{2} & 2 \\\\ 2 & \\frac{21}{4} \\end{pmatrix}$\n\nFirst, we compute the inverse of $\\boldsymbol{\\Sigma}_{CC}$:\n$\\det(\\boldsymbol{\\Sigma}_{CC}) = (\\frac{11}{2})(\\frac{21}{4}) - (2)(2) = \\frac{231}{8} - 4 = \\frac{231 - 32}{8} = \\frac{199}{8}$\n$\\boldsymbol{\\Sigma}_{CC}^{-1} = \\frac{1}{\\det(\\boldsymbol{\\Sigma}_{CC})} \\begin{pmatrix} \\frac{21}{4} & -2 \\\\ -2 & \\frac{11}{2} \\end{pmatrix} = \\frac{8}{199} \\begin{pmatrix} \\frac{21}{4} & -2 \\\\ -2 & \\frac{11}{2} \\end{pmatrix} = \\frac{1}{199} \\begin{pmatrix} 42 & -16 \\\\ -16 & 44 \\end{pmatrix}$\n\nNow, we find the optimal coefficients $\\boldsymbol{a}^{\\star} = (a_1^{\\star}, a_2^{\\star})^T$:\n$\\boldsymbol{a}^{\\star} = \\boldsymbol{\\Sigma}_{CC}^{-1} \\boldsymbol{\\sigma}_{XC} = \\frac{1}{199} \\begin{pmatrix} 42 & -16 \\\\ -16 & 44 \\end{pmatrix} \\begin{pmatrix} 8 \\\\ 5 \\end{pmatrix} = \\frac{1}{199} \\begin{pmatrix} 42 \\cdot 8 - 16 \\cdot 5 \\\\ -16 \\cdot 8 + 44 \\cdot 5 \\end{pmatrix} = \\frac{1}{199} \\begin{pmatrix} 336 - 80 \\\\ -128 + 220 \\end{pmatrix} = \\frac{1}{199} \\begin{pmatrix} 256 \\\\ 92 \\end{pmatrix}$\nSo, $a_1^{\\star} = \\frac{256}{199}$ and $a_2^{\\star} = \\frac{92}{199}$.\n\nThe minimal variance is:\n$\\mathrm{Var}(Y(\\boldsymbol{a}^{\\star})) = \\mathrm{Var}(X) - \\boldsymbol{\\sigma}_{XC}^T \\boldsymbol{a}^{\\star} = 14 - \\begin{pmatrix} 8 & 5 \\end{pmatrix} \\begin{pmatrix} 256/199 \\\\ 92/199 \\end{pmatrix}$\n$\\mathrm{Var}(Y(\\boldsymbol{a}^{\\star})) = 14 - \\frac{8 \\cdot 256 + 5 \\cdot 92}{199} = 14 - \\frac{2048 + 460}{199} = 14 - \\frac{2508}{199}$\n$\\mathrm{Var}(Y(\\boldsymbol{a}^{\\star})) = \\frac{14 \\cdot 199 - 2508}{199} = \\frac{2786 - 2508}{199} = \\frac{278}{199}$\n\n**4. Analysis for Subset $S = \\{1, 3\\}$**\n\nHere, $\\boldsymbol{C} = (C_1, C_3)^T$.\nThe required vectors and matrices are:\n$\\boldsymbol{\\sigma}_{XC} = \\begin{pmatrix} \\mathrm{Cov}(X, C_1) \\\\ \\mathrm{Cov}(X, C_3) \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 3.5 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ \\frac{7}{2} \\end{pmatrix}$\n$\\boldsymbol{\\Sigma}_{CC} = \\begin{pmatrix} \\mathrm{Var}(C_1) & \\mathrm{Cov}(C_1, C_3) \\\\ \\mathrm{Cov(C_3, C_1)} & \\mathrm{Var}(C_3) \\end{pmatrix} = \\begin{pmatrix} 5.5 & 1.5 \\\\ 1.5 & 3.25 \\end{pmatrix} = \\begin{pmatrix} \\frac{11}{2} & \\frac{3}{2} \\\\ \\frac{3}{2} & \\frac{13}{4} \\end{pmatrix}$\n\nFirst, we compute the inverse of $\\boldsymbol{\\Sigma}_{CC}$:\n$\\det(\\boldsymbol{\\Sigma}_{CC}) = (\\frac{11}{2})(\\frac{13}{4}) - (\\frac{3}{2})^2 = \\frac{143}{8} - \\frac{9}{4} = \\frac{143 - 18}{8} = \\frac{125}{8}$\n$\\boldsymbol{\\Sigma}_{CC}^{-1} = \\frac{8}{125} \\begin{pmatrix} \\frac{13}{4} & -\\frac{3}{2} \\\\ -\\frac{3}{2} & \\frac{11}{2} \\end{pmatrix} = \\frac{1}{125} \\begin{pmatrix} 26 & -12 \\\\ -12 & 44 \\end{pmatrix}$\n\nNow, we find the optimal coefficients $\\boldsymbol{a}^{\\star} = (a_1^{\\star}, a_3^{\\star})^T$:\n$\\boldsymbol{a}^{\\star} = \\boldsymbol{\\Sigma}_{CC}^{-1} \\boldsymbol{\\sigma}_{XC} = \\frac{1}{125} \\begin{pmatrix} 26 & -12 \\\\ -12 & 44 \\end{pmatrix} \\begin{pmatrix} 8 \\\\ 7/2 \\end{pmatrix} = \\frac{1}{125} \\begin{pmatrix} 26 \\cdot 8 - 12 \\cdot \\frac{7}{2} \\\\ -12 \\cdot 8 + 44 \\cdot \\frac{7}{2} \\end{pmatrix} = \\frac{1}{125} \\begin{pmatrix} 208 - 42 \\\\ -96 + 154 \\end{pmatrix} = \\frac{1}{125} \\begin{pmatrix} 166 \\\\ 58 \\end{pmatrix}$\nSo, $a_1^{\\star} = \\frac{166}{125}$ and $a_3^{\\star} = \\frac{58}{125}$.\n\nThe minimal variance is:\n$\\mathrm{Var}(Y(\\boldsymbol{a}^{\\star})) = \\mathrm{Var}(X) - \\boldsymbol{\\sigma}_{XC}^T \\boldsymbol{a}^{\\star} = 14 - \\begin{pmatrix} 8 & \\frac{7}{2} \\end{pmatrix} \\begin{pmatrix} 166/125 \\\\ 58/125 \\end{pmatrix}$\n$\\mathrm{Var}(Y(\\boldsymbol{a}^{\\star})) = 14 - \\frac{8 \\cdot 166 + \\frac{7}{2} \\cdot 58}{125} = 14 - \\frac{1328 + 7 \\cdot 29}{125} = 14 - \\frac{1328 + 203}{125} = 14 - \\frac{1531}{125}$\n$\\mathrm{Var}(Y(\\boldsymbol{a}^{\\star})) = \\frac{14 \\cdot 125 - 1531}{125} = \\frac{1750 - 1531}{125} = \\frac{219}{125}$\n\n**5. Comparison of Subsets**\n\nWe must determine which subset, $S=\\{1,2\\}$ or $S=\\{1,3\\}$, yields a smaller minimal variance.\n- For $S=\\{1,2\\}$, the minimal variance is $\\frac{278}{199}$.\n- For $S=\\{1,3\\}$, the minimal variance is $\\frac{219}{125}$.\n\nTo compare the fractions, we can find a common denominator or convert to decimals.\n$\\frac{278}{199} \\approx 1.39698$\n$\\frac{219}{125} = \\frac{219 \\times 8}{125 \\times 8} = \\frac{1752}{1000} = 1.752$\n\nSince $1.39698 < 1.752$, we have $\\frac{278}{199} < \\frac{219}{125}$.\nThe better subset is $S=\\{1,2\\}$, as it results in a lower variance for the controlled estimator.\n\nThe question asks for the two optimal coefficients for the better subset followed by that minimal variance.\nThe better subset is $S=\\{1,2\\}$, for which:\n- Optimal coefficients are $a_1^{\\star} = \\frac{256}{199}$ and $a_2^{\\star} = \\frac{92}{199}$.\n- Minimal variance is $\\frac{278}{199}$.\n\nThe final answer consists of these three values in a row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{256}{199} & \\frac{92}{199} & \\frac{278}{199}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In practical applications, simulation is not free; every variable we compute has a cost. This exercise moves beyond pure statistical optimization to incorporate economic constraints, a crucial step toward real-world problem-solving. You will frame the selection of control variates as a portfolio optimization problem, aiming to find the subset of controls that provides the maximum variance reduction for a given computational budget. This practice bridges the gap between statistical theory and resource-constrained experimental design.",
            "id": "3325545",
            "problem": "You are given a scalar random quantity $X$ to be estimated by Monte Carlo (MC) sampling and a collection of $m$ control variates (CV), denoted by the random vector $C = (C_1,\\dots,C_m)^\\top$, where each $C_j$ has a known evaluation cost $c_j \\in \\mathbb{R}_{>0}$. You may choose a subset $S \\subseteq \\{1,\\dots,m\\}$ of controls to evaluate and a coefficient vector $\\beta \\in \\mathbb{R}^m$ to form the adjusted estimator $X - \\beta^\\top C$. The objective is to reduce variance subject to a total evaluation cost budget $B \\in \\mathbb{R}_{\\ge 0}$. For any admissible subset $S$, coefficients for indices outside $S$ are constrained to be zero.\n\nAssume that all random variables are centered (zero mean) and second moments exist. You are given the variance $\\mathrm{Var}(X)$, the covariance vector $\\Sigma_{CX} \\in \\mathbb{R}^m$ with entries $\\Sigma_{CX,j} = \\mathrm{Cov}(C_j,X)$, and the covariance matrix $\\Sigma_{CC} \\in \\mathbb{R}^{m \\times m}$ with entries $\\Sigma_{CC,ij} = \\mathrm{Cov}(C_i,C_j)$. The matrix $\\Sigma_{CC}$ is symmetric and positive semidefinite. Costs are additive, that is, the total cost of a subset $S$ is $\\sum_{j \\in S} c_j$.\n\nYour tasks are:\n- Starting only from core definitions of variance and covariance, derive how to optimally choose, for a fixed subset $S$, the coefficient vector $\\beta$ that minimizes $\\mathrm{Var}(X - \\beta^\\top C)$ subject to the constraint that $\\beta_j = 0$ for all $j \\notin S$.\n- Deduce the resulting objective value (the minimized variance for subset $S$) as a function of $\\Sigma_{CX}$ and $\\Sigma_{CC}$ associated with indices in $S$.\n- Show that selecting the subset is a combinatorial optimization problem over $S$ under the cost budget constraint $\\sum_{j \\in S} c_j \\le B$, and characterize precisely how this relates to a knapsack-like structure. Explain the case where $\\Sigma_{CC}$ is diagonal (controls uncorrelated) and contrast it with the correlated case.\n- Implement an exact optimizer that, given $(\\mathrm{Var}(X), \\Sigma_{CX}, \\Sigma_{CC}, c, B)$, computes:\n  1) the optimal subset $S^\\star$ that minimizes the variance under the budget constraint,\n  2) the corresponding minimized variance value.\n  If a relevant submatrix of $\\Sigma_{CC}$ is singular, use the Moore–Penrose pseudoinverse to evaluate the minimized variance without allowing any instability in your implementation.\n\nTie-breaking requirement: If multiple subsets $S$ achieve the same minimized variance (up to absolute tolerance $10^{-12}$), return the lexicographically smallest index set (indices are $1$-based and listed in increasing order) among the optimal subsets.\n\nNumerical output specification:\n- For each test case, output a pair $[S^\\star, v^\\star]$, where $S^\\star$ is the $1$-based index list of the chosen controls in strictly increasing order, and $v^\\star$ is the minimized variance rounded to six decimal places as a real number.\n- Aggregate all test case outputs into a single outer list in the order given below.\n- Your program should produce a single line of output containing this outer list without any spaces, with comma-separated entries and square brackets. For example, a valid output format is $[[[1,2],0.123456],[[],2.000000]]$.\n\nAngles are not involved. Physical units are not involved.\n\nTest suite:\nFor each case below, the data are dimensionless and internally consistent. Each test case consists of $(\\mathrm{Var}(X), \\Sigma_{CX}, \\Sigma_{CC}, c, B)$.\n\n- Test 1 (general correlated case; budget exactly fits a nontrivial pair):\n  - $\\mathrm{Var}(X) = 1.2$\n  - $\\Sigma_{CX} = [0.4,\\,0.3,\\,0.2]^\\top$\n  - $\\Sigma_{CC} = \\begin{bmatrix}1.0 & 0.2 & 0.1\\\\ 0.2 & 1.0 & 0.3\\\\ 0.1 & 0.3 & 1.0\\end{bmatrix}$\n  - $c = [2,\\,1,\\,3]^\\top$, $B = 3$\n\n- Test 2 (boundary case with zero budget):\n  - $\\mathrm{Var}(X) = 2.0$\n  - $\\Sigma_{CX} = [0.6,\\,0.4]^\\top$\n  - $\\Sigma_{CC} = \\begin{bmatrix}1.0 & 0.5\\\\ 0.5 & 1.0\\end{bmatrix}$\n  - $c = [1,\\,1]^\\top$, $B = 0$\n\n- Test 3 (tie requiring lexicographic selection under knapsack-like structure with independent controls):\n  - $\\mathrm{Var}(X) = 1.0$\n  - $\\Sigma_{CX} = [0.5,\\,0.5]^\\top$\n  - $\\Sigma_{CC} = \\begin{bmatrix}1.0 & 0.0\\\\ 0.0 & 1.0\\end{bmatrix}$\n  - $c = [1,\\,1]^\\top$, $B = 1$\n\n- Test 4 (four controls; correlations make some combinations strictly better):\n  - $\\mathrm{Var}(X) = 1.3$\n  - $\\Sigma_{CX} = [0.45,\\,0.35,\\,0.25,\\,0.15]^\\top$\n  - $\\Sigma_{CC} = \\begin{bmatrix}\n  1.0 & 0.3 & 0.2 & 0.1\\\\\n  0.3 & 1.0 & 0.25 & 0.15\\\\\n  0.2 & 0.25 & 1.0 & 0.35\\\\\n  0.1 & 0.15 & 0.35 & 1.0\n  \\end{bmatrix}$\n  - $c = [2,\\,2,\\,1,\\,1]^\\top$, $B = 4$\n\n- Test 5 (highly correlated controls; non-additivity of variance reductions):\n  - $\\mathrm{Var}(X) = 1.0$\n  - $\\Sigma_{CX} = [0.5,\\,0.5]^\\top$\n  - $\\Sigma_{CC} = \\begin{bmatrix}1.0 & 0.9\\\\ 0.9 & 1.0\\end{bmatrix}$\n  - $c = [1,\\,1]^\\top$, $B = 2$\n\nYour program must compute $[S^\\star, v^\\star]$ for each test case and print a single line containing the outer list of all five results, with no spaces, as specified above. The results must be ordered in the same order as the test suite cases. Round each $v^\\star$ to six decimal places.",
            "solution": "The problem is valid as it is scientifically grounded in probability theory and statistics, specifically the theory of variance reduction in Monte Carlo methods. It is well-posed, with a clear objective function, well-defined constraints, and a unique solution guaranteed by the tie-breaking rule. The language is objective and the setup is complete and consistent.\n\nThe task is to find an optimal subset of control variates $S^\\star \\subseteq \\{1, \\dots, m\\}$ and corresponding coefficients $\\beta^\\star$ that minimize the variance of the estimator $X_c = X - \\beta^\\top C$, subject to a budget constraint $B$ on the total evaluation cost of the selected controls. The coefficients $\\beta_j$ are constrained to be zero for any control variate $j$ not in the selected subset $S$.\n\nFirst, for a fixed non-empty subset of control variates $S$, we derive the optimal coefficients $\\beta_S^\\star$. The vector of all coefficients is $\\beta$, but we only need to determine the non-zero elements corresponding to indices in $S$, which we denote by the sub-vector $\\beta_S$. The estimator is $X_c = X - \\beta_S^\\top C_S$, where $C_S$ is the sub-vector of controls with indices in $S$. The variance of this estimator is given by:\n$$V_S(\\beta_S) = \\mathrm{Var}(X - \\beta_S^\\top C_S)$$\nUsing the bilinearity of the covariance operator, and given that all random variables are centered (zero mean), we can expand this expression:\n$$V_S(\\beta_S) = \\mathrm{Cov}(X - \\beta_S^\\top C_S, X - \\beta_S^\\top C_S)$$\n$$V_S(\\beta_S) = \\mathrm{Var}(X) - 2 \\mathrm{Cov}(X, \\beta_S^\\top C_S) + \\mathrm{Var}(\\beta_S^\\top C_S)$$\nThe covariance and variance terms involving vectors can be expressed using the provided covariance matrices. Let $\\Sigma_{C_S X}$ be the sub-vector of $\\Sigma_{CX}$ corresponding to indices in $S$, and $\\Sigma_{C_S C_S}$ be the principal submatrix of $\\Sigma_{CC}$ for indices in $S$.\n$$\\mathrm{Cov}(X, \\beta_S^\\top C_S) = \\mathbb{E}[X (C_S^\\top \\beta_S)] = \\mathbb{E}[X C_S^\\top] \\beta_S = \\Sigma_{X C_S} \\beta_S = \\Sigma_{C_S X}^\\top \\beta_S$$\n$$\\mathrm{Var}(\\beta_S^\\top C_S) = \\mathbb{E}[(\\beta_S^\\top C_S)(\\beta_S^\\top C_S)^\\top] = \\mathbb{E}[\\beta_S^\\top C_S C_S^\\top \\beta_S] = \\beta_S^\\top \\mathbb{E}[C_S C_S^\\top] \\beta_S = \\beta_S^\\top \\Sigma_{C_S C_S} \\beta_S$$\nSubstituting these into the variance expression, we obtain a quadratic function of $\\beta_S$:\n$$V_S(\\beta_S) = \\mathrm{Var}(X) - 2 \\Sigma_{C_S X}^\\top \\beta_S + \\beta_S^\\top \\Sigma_{C_S C_S} \\beta_S$$\nTo find the minimum, we compute the gradient with respect to $\\beta_S$ and set it to zero.\n$$\\nabla_{\\beta_S} V_S(\\beta_S) = -2 \\Sigma_{C_S X} + 2 \\Sigma_{C_S C_S} \\beta_S$$\nSetting the gradient to zero gives the normal equations for the optimal coefficients $\\beta_S^\\star$:\n$$\\Sigma_{C_S C_S} \\beta_S^\\star = \\Sigma_{C_S X}$$\nThe matrix $\\Sigma_{C_S C_S}$ is symmetric and positive semidefinite. If it is singular, a unique solution for $\\beta_S^\\star$ may not exist. However, the problem specifies the use of the Moore-Penrose pseudoinverse, denoted by the superscript $+$, which provides the minimum-norm solution. Thus, the optimal coefficient vector is:\n$$\\beta_S^\\star = \\Sigma_{C_S C_S}^{+} \\Sigma_{C_S X}$$\n\nNext, we deduce the resulting minimized variance, $V_S^\\star = V_S(\\beta_S^\\star)$, by substituting $\\beta_S^\\star$ back into the variance expression:\n$$V_S^\\star = \\mathrm{Var}(X) - 2 \\Sigma_{C_S X}^\\top \\beta_S^\\star + (\\beta_S^\\star)^\\top \\Sigma_{C_S C_S} \\beta_S^\\star$$\nFrom the normal equations, we can left-multiply by $(\\beta_S^\\star)^\\top$ to get $(\\beta_S^\\star)^\\top \\Sigma_{C_S C_S} \\beta_S^\\star = (\\beta_S^\\star)^\\top \\Sigma_{C_S X}$. Substituting this into the variance expression simplifies one term:\n$$V_S^\\star = \\mathrm{Var}(X) - 2 \\Sigma_{C_S X}^\\top \\beta_S^\\star + \\Sigma_{C_S X}^\\top \\beta_S^\\star = \\mathrm{Var}(X) - \\Sigma_{C_S X}^\\top \\beta_S^\\star$$\nFinally, substituting the expression for $\\beta_S^\\star$:\n$$V_S^\\star = \\mathrm{Var}(X) - \\Sigma_{C_S X}^\\top \\Sigma_{C_S C_S}^{+} \\Sigma_{C_S X}$$\nThe term $R(S) = \\Sigma_{C_S X}^\\top \\Sigma_{C_S C_S}^{+} \\Sigma_{C_S X}$ represents the total variance reduction achieved by using the control variates in subset $S$. For the empty set $S = \\emptyset$, the reduction is $R(\\emptyset)=0$, and the variance is simply $\\mathrm{Var}(X)$.\n\nThe overall problem is to select the subset $S$ that minimizes $V_S^\\star$ subject to the total cost constraint $\\sum_{j \\in S} c_j \\le B$. This is equivalent to maximizing the variance reduction $R(S)$:\n$$\\max_{S \\subseteq \\{1,\\dots,m\\}} R(S) \\quad \\text{subject to} \\quad \\sum_{j \\in S} c_j \\le B$$\nThis is a combinatorial optimization problem. Each control variate $j$ can be viewed as an \"item\" with a \"cost\" $c_j$. The goal is to choose a \"knapsack\" of items (the subset $S$) that maximizes a \"value\" function $R(S)$ without exceeding the budget $B$.\n\nThe structure of this problem depends on the properties of the covariance matrix $\\Sigma_{CC}$.\nIf $\\Sigma_{CC}$ is diagonal, the control variates are uncorrelated. The submatrix $\\Sigma_{C_S C_S}$ is also diagonal for any $S$, with diagonal entries $\\mathrm{Var}(C_j)$ for $j \\in S$. Its inverse (assuming all $\\mathrm{Var}(C_j) > 0$) is also diagonal with entries $1/\\mathrm{Var}(C_j)$. The variance reduction $R(S)$ becomes additive:\n$$R(S) = \\sum_{j \\in S} \\frac{\\mathrm{Cov}(C_j, X)^2}{\\mathrm{Var}(C_j)}$$\nIn this case, the problem becomes a classic 0-1 knapsack problem, where the value of item $j$ is $v_j = \\mathrm{Cov}(C_j, X)^2 / \\mathrm{Var}(C_j)$ and its weight is $c_j$.\n\nIn the general case, where $\\Sigma_{CC}$ is not diagonal, the controls are correlated. The value function $R(S)$ is no longer additive. The variance reduction from adding a new control variate depends on which controls are already in the set $S$. This is a non-linear or generalized knapsack problem. Although more complex, for a small number of control variates $m$, this problem can be solved exactly by enumerating all $2^m$ possible subsets $S$, checking the feasibility of each subset against the budget $B$, calculating its corresponding variance $V_S^\\star$, and selecting the subset that yields the minimum variance.\n\nThe algorithm to find the optimal subset $S^\\star$ and its minimized variance $v^\\star$ is as follows:\n1. Initialize a minimum variance $v_{min}$ to a very large number (or to $\\mathrm{Var}(X)$) and a list of optimal subsets, `optimal_subsets`.\n2. Iterate through all $2^m$ subsets of $\\{1, \\dots, m\\}$.\n3. For each subset $S$:\n    a. Calculate its total cost, $\\text{cost}_S = \\sum_{j \\in S} c_j$.\n    b. If $\\text{cost}_S > B$, the subset is infeasible; discard it.\n    c. Otherwise, calculate the minimized variance $V_S^\\star = \\mathrm{Var}(X) - R(S)$. If $S$ is empty, $V_\\emptyset^\\star = \\mathrm{Var}(X)$.\n    d. Compare $V_S^\\star$ with $v_{min}$.\n        i. If $V_S^\\star < v_{min} - \\epsilon$ (where $\\epsilon=10^{-12}$ is a small tolerance), a new best subset is found. Update $v_{min} = V_S^\\star$ and reset `optimal_subsets` to `[S]`.\n        ii. If $|V_S^\\star - v_{min}| \\le \\epsilon$, a tie is found. Add $S$ to `optimal_subsets`.\n4. After checking all subsets, the optimal subset $S^\\star$ is the lexicographically smallest list in `optimal_subsets`. The optimal variance is $v^\\star = v_{min}$.\nThis brute-force approach guarantees finding the exact global optimum, respecting the tie-breaking rule.",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main function to solve all test cases for optimal control variate selection.\n    \"\"\"\n    \n    # Test 1\n    test1 = {\n        \"var_x\": 1.2,\n        \"sigma_cx\": np.array([0.4, 0.3, 0.2]),\n        \"sigma_cc\": np.array([[1.0, 0.2, 0.1], [0.2, 1.0, 0.3], [0.1, 0.3, 1.0]]),\n        \"costs\": np.array([2.0, 1.0, 3.0]),\n        \"budget\": 3.0\n    }\n    \n    # Test 2\n    test2 = {\n        \"var_x\": 2.0,\n        \"sigma_cx\": np.array([0.6, 0.4]),\n        \"sigma_cc\": np.array([[1.0, 0.5], [0.5, 1.0]]),\n        \"costs\": np.array([1.0, 1.0]),\n        \"budget\": 0.0\n    }\n\n    # Test 3\n    test3 = {\n        \"var_x\": 1.0,\n        \"sigma_cx\": np.array([0.5, 0.5]),\n        \"sigma_cc\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n        \"costs\": np.array([1.0, 1.0]),\n        \"budget\": 1.0\n    }\n    \n    # Test 4\n    test4 = {\n        \"var_x\": 1.3,\n        \"sigma_cx\": np.array([0.45, 0.35, 0.25, 0.15]),\n        \"sigma_cc\": np.array([\n            [1.0, 0.3, 0.2, 0.1],\n            [0.3, 1.0, 0.25, 0.15],\n            [0.2, 0.25, 1.0, 0.35],\n            [0.1, 0.15, 0.35, 1.0]\n        ]),\n        \"costs\": np.array([2.0, 2.0, 1.0, 1.0]),\n        \"budget\": 4.0\n    }\n    \n    # Test 5\n    test5 = {\n        \"var_x\": 1.0,\n        \"sigma_cx\": np.array([0.5, 0.5]),\n        \"sigma_cc\": np.array([[1.0, 0.9], [0.9, 1.0]]),\n        \"costs\": np.array([1.0, 1.0]),\n        \"budget\": 2.0\n    }\n    \n    test_cases = [test1, test2, test3, test4, test5]\n    results = []\n\n    for case in test_cases:\n        s_star, v_star = _solve_one_case(**case)\n        results.append((s_star, v_star))\n\n    # Format the final output string according to the specification.\n    # e.g., [[[1,2],0.123456],[[],2.000000]]\n    formatted_results = []\n    for s, v in results:\n        s_str = f\"[{','.join(map(str, s))}]\"\n        v_str = f\"{v:.6f}\"\n        formatted_results.append(f\"[{s_str},{v_str}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _solve_one_case(var_x, sigma_cx, sigma_cc, costs, budget):\n    \"\"\"\n    Solves a single optimal control variate selection problem.\n    \"\"\"\n    m = len(costs)\n    indices = range(m)\n    \n    min_variance = float('inf')\n    optimal_subsets = []\n    tie_tol = 1e-12\n\n    for k in range(m + 1):\n        for s_indices_0based in combinations(indices, k):\n            subset_cost = np.sum(costs[list(s_indices_0based)])\n            \n            if subset_cost > budget:\n                continue\n\n            current_variance = var_x\n            if s_indices_0based: # If subset is not empty\n                # Convert tuple of indices to a list for slicing\n                idx_list = list(s_indices_0based)\n                \n                # Extract sub-vectors and sub-matrices\n                sigma_cs_x = sigma_cx[idx_list]\n                # Use np.ix_ for creating submatrices from index lists\n                sigma_cs_cs = sigma_cc[np.ix_(idx_list, idx_list)]\n                \n                # Calculate variance reduction using Moore-Penrose pseudoinverse\n                try:\n                    # Invertible case is faster - but pinv handles both\n                    pinv_sigma_cs_cs = np.linalg.pinv(sigma_cs_cs)\n                    variance_reduction = sigma_cs_x.T @ pinv_sigma_cs_cs @ sigma_cs_x\n                    current_variance -= variance_reduction\n                except np.linalg.LinAlgError:\n                    # This should not happen with pinv, but as a failsafe\n                    continue\n            \n            # Update minimum variance and list of optimal subsets\n            if abs(current_variance - min_variance)  tie_tol:\n                # Tie in variance, add subset to list\n                optimal_subsets.append(list(s_indices_0based))\n            elif current_variance  min_variance:\n                # Found a new minimum variance\n                min_variance = current_variance\n                optimal_subsets = [list(s_indices_0based)]\n    \n    # Apply tie-breaking rule: choose the lexicographically smallest subset\n    # An empty list is lexicographically smaller than any non-empty list.\n    # Python's `min` for lists of lists performs lexicographical comparison.\n    best_subset_0based = min(optimal_subsets)\n    \n    # Convert to 1-based indexing for the final output\n    best_subset_1based = [i + 1 for i in best_subset_0based]\n    \n    return best_subset_1based, min_variance\n\n# Run the solver\nsolve()\n```"
        },
        {
            "introduction": "The elegant theory of optimal control variates rests on key assumptions, such as the existence of finite variances. This final practice delves into what happens when these assumptions break down, a common occurrence when dealing with heavy-tailed distributions. You will investigate how to handle controls whose raw moments may be infinite, exploring preprocessing techniques like truncation and winsorization to create usable, finite-variance alternatives. This exercise is essential for developing robust simulation strategies and appreciating the interplay between statistical theory and practical data wrangling.",
            "id": "3325576",
            "problem": "Consider independent and identically distributed samples $\\{X_i\\}_{i=1}^n$ from a Laplace distribution with zero mean and scale parameter $b0$, so that $X\\sim\\mathrm{Laplace}(0,b)$ and $\\mathrm{Var}(X)=2b^2$. The target is to estimate the expectation $\\mu=\\mathbb{E}[X]$ using Monte Carlo with control variates. You have two candidate control variates $C^{(1)}$ and $C^{(2)}$ constructed from the following model: for $j\\in\\{1,2\\}$,\n$$\nC^{(j)} \\equiv \\lambda_j X + U^{(j)},\n$$\nwhere $U^{(j)}$ is independent of $X$ and has a Pareto distribution with scale $x_m0$ and shape $\\alpha_j0$, denoted $U^{(j)}\\sim\\mathrm{Pareto}(x_m,\\alpha_j)$. The Pareto density is $f(u)=\\alpha x_m^\\alpha u^{-(\\alpha+1)}$ for $u\\ge x_m$. The parameters $\\lambda_j$ capture correlation with $X$ through the linear term $\\lambda_j X$, while the additive term $U^{(j)}$ induces heavy-tail behavior in the control.\n\nThe canonical control variate estimator of $\\mu$ for a single control $C$ and coefficient $\\beta\\in\\mathbb{R}$ is\n$$\n\\hat{\\mu}_n(\\beta) \\equiv \\frac{1}{n}\\sum_{i=1}^n \\left( X_i - \\beta\\left(C_i - \\mathbb{E}[C]\\right)\\right).\n$$\nAssume you only use control variate transformations that preserve the known expectation of the control by replacing $U^{(j)}$ with a processed version whose expectation can be computed exactly from distributional information, and leave the correlated linear component $\\lambda_j X$ unchanged. Specifically, consider the following strategies applied to $U^{(j)}$:\n\n- One-sided winsorization at threshold $\\tau x_m$: $W_\\tau(U^{(j)}) \\equiv \\min\\{U^{(j)},\\tau\\}$.\n\n- One-sided truncation at threshold $\\tau x_m$: $T_\\tau(U^{(j)}) \\equiv U^{(j)}\\mathbf{1}\\{U^{(j)}\\le \\tau\\}$.\n\nThe processed controls are then defined as $C^{(j)}_{\\mathrm{win}} \\equiv \\lambda_j X + W_\\tau(U^{(j)})$ and $C^{(j)}_{\\mathrm{trunc}} \\equiv \\lambda_j X + T_\\tau(U^{(j)})$. You may also consider the raw controls $C^{(j)}$ without processing, but their second moments may be infinite depending on $\\alpha_j$.\n\nStarting only from the fundamental definitions of variance, covariance, independence, and the classical Central Limit Theorem (CLT) for independent and identically distributed sums with finite variance, perform the following:\n\n1. Derive the expression for the variance of the control variate estimator $\\hat{\\mu}_n(\\beta)$ as a function of $\\beta$, and then derive the value of $\\beta$ that minimizes this variance under the model $C=\\lambda X + V$, where $V$ is independent of $X$. Express the minimal asymptotic variance in terms of $\\mathrm{Var}(X)$, $\\lambda$, and $\\mathrm{Var}(V)$.\n\n2. For $U\\sim \\mathrm{Pareto}(x_m,\\alpha)$ and a threshold $\\tau x_m$, derive closed-form expressions for the first and second moments of the one-sided winsorized variable $W_\\tau(U)$ and the one-sided truncated variable $T_\\tau(U)$. Your derivation must handle the special cases $\\alpha=1$ and $\\alpha=2$ correctly. Express your formulas using integrals of the Pareto density and evaluate them into explicit functions of $x_m$, $\\alpha$, and $\\tau$.\n\n3. Using the results of parts $1$ and $2$, compute the variance reduction factor (VRF), defined as\n$$\n\\mathrm{VRF} \\equiv \\frac{\\mathrm{Var}(X)}{\\mathrm{Var}\\left(X - \\beta^\\star\\left(C - \\mathbb{E}[C]\\right)\\right)},\n$$\nfor each candidate control $j\\in\\{1,2\\}$ under each strategy: raw, winsorized, and truncated. Here $\\beta^\\star$ denotes the variance-minimizing coefficient found in part $1$ with the appropriate processed control.\n\n4. Quantify the impact on asymptotic normality by determining, for each strategy, whether the classical Central Limit Theorem applies to $\\hat{\\mu}_n(\\beta^\\star)$, based solely on finiteness of second moments of the terms involved. State clearly the condition on $\\alpha$ under which the raw control has finite variance. Explain why the processed strategies lead to finite second moments for any $\\alpha0$.\n\n5. Among all strategies and candidates, select the one that achieves the largest variance reduction factor. If none yields a finite-variance estimator, select the no-control option.\n\nYour program must compute these quantities for the following test suite of parameter values and produce a single line of output with the specified format. In all cases, the Laplace scale is $b$, the sample size is $n$, the Pareto scale is $x_m$, the shape parameters are $\\alpha_1$ and $\\alpha_2$, the correlation coefficients are $\\lambda_1$ and $\\lambda_2$, and the processing threshold is $\\tau$.\n\nTest Suite:\n- Case $1$: $b=1$, $n=50000$, $x_m=1$, $\\alpha_1=1.5$, $\\alpha_2=3.0$, $\\lambda_1=0.8$, $\\lambda_2=0.2$, $\\tau=10$.\n- Case $2$: $b=0.5$, $n=100000$, $x_m=1$, $\\alpha_1=2.0$, $\\alpha_2=2.5$, $\\lambda_1=0.3$, $\\lambda_2=0.9$, $\\tau=20$.\n- Case $3$: $b=2$, $n=20000$, $x_m=1$, $\\alpha_1=0.8$, $\\alpha_2=1.2$, $\\lambda_1=0.7$, $\\lambda_2=0.7$, $\\tau=5$.\n\nFinal Output Format:\n- For each case, your program should output a list $[s,v,c_r,c_s]$ where $s$ is an integer code for the selected strategy, $v$ is the variance reduction factor as a floating-point number, $c_r$ is a boolean encoded as $0$ or $1$ indicating whether any raw control among the candidates yields asymptotic normality, and $c_s$ is a boolean encoded as $0$ or $1$ indicating whether the selected strategy yields asymptotic normality.\n- Use the following strategy codes: $0$ for no control, $1$ for raw candidate $1$, $2$ for winsorized candidate $1$, $3$ for truncated candidate $1$, $4$ for raw candidate $2$, $5$ for winsorized candidate $2$, $6$ for truncated candidate $2$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each case’s list enclosed in square brackets. For example: \"[[s1,v1,c_r1,c_s1],[s2,v2,c_r2,c_s2],[s3,v3,c_r3,c_s3]]\". There are no physical units involved; all quantities are dimensionless real numbers or booleans.",
            "solution": "The problem requires a systematic evaluation of several control variate strategies for estimating the mean of a Laplace distribution. This involves deriving the optimal control variate coefficient, the resulting variance reduction, the moments of processed Pareto random variables, and assessing the applicability of the Central Limit Theorem for each strategy.\n\n### Step 1: Derivation of the Optimal Control Variate and Minimal Variance\n\nLet $X$ be the random variable of interest with mean $\\mu = \\mathbb{E}[X]$, and let $C$ be a control variate with known expectation $\\mathbb{E}[C]$. The control variate estimator for $\\mu$ based on $n$ samples is\n$$\n\\hat{\\mu}_n(\\beta) = \\frac{1}{n}\\sum_{i=1}^n Y_i, \\quad \\text{where} \\quad Y_i = X_i - \\beta(C_i - \\mathbb{E}[C]).\n$$\nSince the samples $\\{X_i, C_i\\}$ are independent and identically distributed, the variance of the estimator is\n$$\n\\mathrm{Var}(\\hat{\\mu}_n(\\beta)) = \\frac{1}{n} \\mathrm{Var}(Y_1) = \\frac{1}{n}\\mathrm{Var}(X - \\beta(C - \\mathbb{E}[C])).\n$$\nThe term $\\mathbb{E}[C]$ is a constant, so it does not affect the variance. We seek to minimize\n$$\n\\mathrm{Var}(X - \\beta C) = \\mathrm{Var}(X) + \\mathrm{Var}(\\beta C) - 2\\mathrm{Cov}(X, \\beta C) = \\mathrm{Var}(X) + \\beta^2 \\mathrm{Var}(C) - 2\\beta \\mathrm{Cov}(X, C).\n$$\nThis is a quadratic function of $\\beta$. Assuming $\\mathrm{Var}(C)$ is finite and non-zero, the minimum is found by setting the derivative with respect to $\\beta$ to zero:\n$$\n\\frac{d}{d\\beta} \\left( \\mathrm{Var}(X) + \\beta^2 \\mathrm{Var}(C) - 2\\beta \\mathrm{Cov}(X, C) \\right) = 2\\beta \\mathrm{Var}(C) - 2\\mathrm{Cov}(X, C) = 0.\n$$\nThis yields the optimal coefficient $\\beta^\\star$:\n$$\n\\beta^\\star = \\frac{\\mathrm{Cov}(X, C)}{\\mathrm{Var}(C)}.\n$$\nSubstituting $\\beta^\\star$ back into the variance expression gives the minimal variance of the summand:\n$$\n\\mathrm{Var}(X - \\beta^\\star C) = \\mathrm{Var}(X) + \\left(\\frac{\\mathrm{Cov}(X, C)}{\\mathrm{Var}(C)}\\right)^2 \\mathrm{Var}(C) - 2\\frac{\\mathrm{Cov}(X, C)}{\\mathrm{Var}(C)}\\mathrm{Cov}(X, C) = \\mathrm{Var}(X) - \\frac{\\mathrm{Cov}(X, C)^2}{\\mathrm{Var}(C)}.\n$$\nThis can be expressed using the correlation coefficient $\\rho_{XC} = \\mathrm{Cov}(X, C) / \\sqrt{\\mathrm{Var}(X)\\mathrm{Var}(C)}$ as $\\mathrm{Var}(X)(1 - \\rho_{XC}^2)$.\n\nThe problem specifies the control variate model $C = \\lambda X + V$, where $X$ and $V$ are independent. For this model:\n$$\n\\mathrm{Cov}(X, C) = \\mathrm{Cov}(X, \\lambda X + V) = \\lambda \\mathrm{Cov}(X, X) + \\mathrm{Cov}(X, V) = \\lambda \\mathrm{Var}(X) \\quad (\\text{since } X, V \\text{ are independent}).\n$$\n$$\n\\mathrm{Var}(C) = \\mathrm{Var}(\\lambda X + V) = \\lambda^2 \\mathrm{Var}(X) + \\mathrm{Var}(V) \\quad (\\text{since } X, V \\text{ are independent}).\n$$\nThe minimal variance of the term $Y_i$ is therefore:\n$$\n\\mathrm{Var}(Y_i) = \\mathrm{Var}(X) - \\frac{(\\lambda \\mathrm{Var}(X))^2}{\\lambda^2 \\mathrm{Var}(X) + \\mathrm{Var}(V)} = \\mathrm{Var}(X) \\left( 1 - \\frac{\\lambda^2 \\mathrm{Var}(X)}{\\lambda^2 \\mathrm{Var}(X) + \\mathrm{Var}(V)} \\right) = \\mathrm{Var}(X) \\frac{\\mathrm{Var(V)}}{\\lambda^2 \\mathrm{Var}(X) + \\mathrm{Var}(V)}.\n$$\nThis is the minimal asymptotic variance of the estimator, scaled by $n$.\n\n### Step 2: Moments of Processed Pareto Variables\n\nLet $U \\sim \\mathrm{Pareto}(x_m, \\alpha)$, with probability density function $f_U(u) = \\alpha x_m^\\alpha u^{-(\\alpha+1)}$ for $u \\ge x_m$. We derive the first and second moments of the winsorized variable $W_\\tau(U) = \\min(U, \\tau)$ and the truncated variable $T_\\tau(U) = U \\cdot \\mathbf{1}\\{U \\le \\tau\\}$ for a threshold $\\tau  x_m$.\n\nThe $k$-th moment of a processed variable $g(U)$ is $\\mathbb{E}[g(U)^k] = \\int_{x_m}^\\infty g(u)^k f_U(u) du$.\n\n**Winsorized Variable $W_\\tau(U)$:**\nThe first moment is:\n$$\n\\mathbb{E}[W_\\tau(U)] = \\int_{x_m}^{\\tau} u (\\alpha x_m^\\alpha u^{-\\alpha-1}) du + \\int_{\\tau}^{\\infty} \\tau (\\alpha x_m^\\alpha u^{-\\alpha-1}) du\n$$\nThe second integral is $\\tau \\alpha x_m^\\alpha \\int_{\\tau}^{\\infty} u^{-\\alpha-1} du = \\tau \\alpha x_m^\\alpha [-\\frac{1}{\\alpha} u^{-\\alpha}]_{\\tau}^{\\infty} = \\tau \\alpha x_m^\\alpha (0 - (-\\frac{1}{\\alpha}\\tau^{-\\alpha})) = x_m^\\alpha \\tau^{1-\\alpha}$.\nFor the first integral, we consider two cases for $\\int u^{-\\alpha} du$:\n- If $\\alpha \\neq 1$: $\\alpha x_m^\\alpha \\int_{x_m}^{\\tau} u^{-\\alpha} du = \\alpha x_m^\\alpha [\\frac{u^{1-\\alpha}}{1-\\alpha}]_{x_m}^{\\tau} = \\frac{\\alpha x_m^\\alpha}{1-\\alpha}(\\tau^{1-\\alpha} - x_m^{1-\\alpha})$.\n  Combining gives $\\mathbb{E}[W_\\tau(U)] = \\frac{\\alpha x_m^\\alpha}{1-\\alpha}(\\tau^{1-\\alpha} - x_m^{1-\\alpha}) + x_m^\\alpha \\tau^{1-\\alpha} = \\frac{x_m^\\alpha\\tau^{1-\\alpha}}{1-\\alpha} - \\frac{\\alpha x_m}{1-\\alpha}$.\n- If $\\alpha = 1$: $x_m \\int_{x_m}^{\\tau} u^{-1} du = x_m [\\ln u]_{x_m}^{\\tau} = x_m \\ln(\\tau/x_m)$.\n  Combining gives $\\mathbb{E}[W_\\tau(U)] = x_m \\ln(\\tau/x_m) + x_m$.\n\nThe second moment is:\n$$\n\\mathbb{E}[W_\\tau(U)^2] = \\int_{x_m}^{\\tau} u^2 (\\alpha x_m^\\alpha u^{-\\alpha-1}) du + \\int_{\\tau}^{\\infty} \\tau^2 (\\alpha x_m^\\alpha u^{-\\alpha-1}) du\n$$\nThe second integral is $\\tau^2 \\alpha x_m^\\alpha \\int_{\\tau}^{\\infty} u^{-\\alpha-1} du = \\tau^2 (x_m^\\alpha \\tau^{-\\alpha}) = x_m^\\alpha \\tau^{2-\\alpha}$.\nFor the first integral, $\\int u^{1-\\alpha} du$:\n- If $\\alpha \\neq 2$: $\\alpha x_m^\\alpha \\int_{x_m}^{\\tau} u^{1-\\alpha} du = \\alpha x_m^\\alpha [\\frac{u^{2-\\alpha}}{2-\\alpha}]_{x_m}^{\\tau} = \\frac{\\alpha x_m^\\alpha}{2-\\alpha}(\\tau^{2-\\alpha} - x_m^{2-\\alpha})$.\n  Combining gives $\\mathbb{E}[W_\\tau(U)^2] = \\frac{\\alpha x_m^\\alpha}{2-\\alpha}(\\tau^{2-\\alpha} - x_m^{2-\\alpha}) + x_m^\\alpha \\tau^{2-\\alpha} = \\frac{2x_m^\\alpha\\tau^{2-\\alpha}}{2-\\alpha} - \\frac{\\alpha x_m^2}{2-\\alpha}$.\n- If $\\alpha = 2$: $2x_m^2 \\int_{x_m}^{\\tau} u^{-1} du = 2x_m^2[\\ln u]_{x_m}^{\\tau} = 2x_m^2 \\ln(\\tau/x_m)$.\n  Combining gives $\\mathbb{E}[W_\\tau(U)^2] = 2x_m^2 \\ln(\\tau/x_m) + x_m^2$.\n\n**Truncated Variable $T_\\tau(U)$:**\nThe moments are simply the first integrals from the winsorized case, as the variable is zero for $u\\tau$.\nFirst moment:\n- If $\\alpha \\neq 1$: $\\mathbb{E}[T_\\tau(U)] = \\frac{\\alpha x_m^\\alpha}{1-\\alpha}(\\tau^{1-\\alpha} - x_m^{1-\\alpha})$.\n- If $\\alpha = 1$: $\\mathbb{E}[T_\\tau(U)] = x_m \\ln(\\tau/x_m)$.\nSecond moment:\n- If $\\alpha \\neq 2$: $\\mathbb{E}[T_\\tau(U)^2] = \\frac{\\alpha x_m^\\alpha}{2-\\alpha}(\\tau^{2-\\alpha} - x_m^{2-\\alpha})$.\n- If $\\alpha = 2$: $\\mathbb{E}[T_\\tau(U)^2] = 2x_m^2 \\ln(\\tau/x_m)$.\n\nIn all cases, the variance is $\\mathrm{Var}(V) = \\mathbb{E}[V^2] - (\\mathbb{E}[V])^2$ for $V \\in \\{W_\\tau(U), T_\\tau(U)\\}$.\n\n### Step 3: Variance Reduction Factor (VRF)\n\nThe VRF is defined as the ratio of the simple Monte Carlo variance to the optimally-controlled variance:\n$$\n\\mathrm{VRF} = \\frac{\\mathrm{Var}(X)}{\\mathrm{Var}(X - \\beta^\\star(C - \\mathbb{E}[C]))}.\n$$\nUsing the expression for the minimal variance from Step 1:\n$$\n\\mathrm{VRF} = \\frac{\\mathrm{Var}(X)}{\\mathrm{Var}(X) \\frac{\\mathrm{Var}(V)}{\\lambda^2 \\mathrm{Var}(X) + \\mathrm{Var}(V)}} = \\frac{\\lambda^2 \\mathrm{Var}(X) + \\mathrm{Var}(V)}{\\mathrm{Var}(V)} = 1 + \\frac{\\lambda^2 \\mathrm{Var}(X)}{\\mathrm{Var}(V)}.\n$$\nThis formula applies provided $\\mathrm{Var}(V)$ is finite and positive. If $\\mathrm{Var}(V)$ is infinite, $\\beta^\\star$ must be chosen as $0$ to ensure a finite variance for the estimator, which reduces to the simple Monte Carlo estimator. In this case, there is no variance reduction, so $\\mathrm{VRF}=1$.\nWe are given $\\mathrm{Var}(X) = 2b^2$. For each strategy, we compute $\\mathrm{Var}(V)$ where $V$ is the (possibly processed) noise term $U^{(j)}$.\n\n### Step 4: Asymptotic Normality and the Central Limit Theorem (CLT)\n\nThe estimator is $\\hat{\\mu}_n(\\beta^\\star) = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\beta^\\star(C_i - \\mathbb{E}[C]))$. By the classical CLT, $\\sqrt{n}(\\hat{\\mu}_n(\\beta^\\star) - \\mu)$ converges in distribution to a normal distribution if the terms in the sum have finite variance. The variance of each term is $\\mathrm{Var}(X - \\beta^\\star C)$, which we found to be $\\mathrm{Var}(X) \\frac{\\mathrm{Var}(V)}{\\lambda^2 \\mathrm{Var}(X) + \\mathrm{Var}(V)}$.\nThis variance is finite if and only if $\\mathrm{Var}(V)$ is finite, since $\\mathrm{Var}(X) = 2b^2$ is finite.\nThe applicability of the CLT thus hinges on the finiteness of $\\mathrm{Var}(V)$.\n\n- **Raw Control:** $V = U^{(j)}$. The variance of a Pareto($x_m, \\alpha_j$) distribution, $\\mathrm{Var}(U^{(j)})$, is finite if and only if its second moment exists, which requires $\\alpha_j  2$.\n- **Winsorized Control:** $V = W_\\tau(U^{(j)}) = \\min(U^{(j)}, \\tau)$. This variable is bounded above by $\\tau$ and below by $x_m  0$. Any bounded random variable has finite moments of all orders. Therefore, $\\mathrm{Var}(W_\\tau(U^{(j)}))$ is always finite for any $\\alpha_j  0$.\n- **Truncated Control:** $V = T_\\tau(U^{(j)}) = U^{(j)} \\mathbf{1}\\{U^{(j)} \\le \\tau\\}$. This variable is bounded between $0$ and $\\tau$. Consequently, $\\mathrm{Var}(T_\\tau(U^{(j)}))$ is always finite for any $\\alpha_j  0$.\n\nIn summary, the processed strategies (winsorization, truncation) always yield a control variate estimator to which the CLT applies. The raw strategy is only guaranteed to do so if the Pareto shape parameter $\\alpha2$.\n\n### Step 5: Strategy Selection\n\nThe optimal strategy is the one that maximizes the VRF. We compute the VRF for each of the $2 \\times 3 = 6$ possible strategies (two candidates, three processing types). The baseline is the \"no control\" option, with $\\mathrm{VRF}=1$. If a strategy results in an infinite variance for the control part (i.e., raw control with $\\alpha \\le 2$), its VRF is taken as $1$, as no variance reduction is achieved. We select the strategy code corresponding to the maximum VRF found.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the control variate selection problem for a given set of test cases.\n    \"\"\"\n    test_cases = [\n        # (b, n, x_m, alpha1, alpha2, lambda1, lambda2, tau)\n        (1.0, 50000, 1.0, 1.5, 3.0, 0.8, 0.2, 10.0),\n        (0.5, 100000, 1.0, 2.0, 2.5, 0.3, 0.9, 20.0),\n        (2.0, 20000, 1.0, 0.8, 1.2, 0.7, 0.7, 5.0),\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        b, _, x_m, alpha1, alpha2, lambda1, lambda2, tau = case\n        var_X = 2 * b**2\n        candidates = [\n            {'alpha': alpha1, 'lambda': lambda1},\n            {'alpha': alpha2, 'lambda': lambda2},\n        ]\n\n        best_vrf = 1.0\n        best_strategy_code = 0\n        clt_for_best_strategy = True  # Default 'no control' has finite variance\n\n        # --- Moment Calculation Functions ---\n        def get_winsorized_moments(alpha, xm, t):\n            if alpha == 1.0:\n                e1 = xm * (np.log(t / xm) + 1.0)\n            else:\n                e1 = (xm**alpha * t**(1.0 - alpha)) / (1.0 - alpha) - (alpha * xm) / (1.0 - alpha)\n            \n            if alpha == 2.0:\n                e2 = 2.0 * xm**2 * np.log(t / xm) + xm**2\n            else:\n                e2 = (2.0 * xm**alpha * t**(2.0 - alpha)) / (2.0 - alpha) - (alpha * xm**2) / (2.0 - alpha)\n            return e1, e2\n\n        def get_truncated_moments(alpha, xm, t):\n            if alpha == 1.0:\n                e1 = xm * np.log(t / xm)\n            else:\n                e1 = (alpha * xm**alpha) / (1.0 - alpha) * (t**(1.0 - alpha) - xm**(1.0 - alpha))\n            \n            if alpha == 2.0:\n                e2 = 2.0 * xm**2 * np.log(t / xm)\n            else:\n                e2 = (alpha * xm**alpha) / (2.0 - alpha) * (t**(2.0 - alpha) - xm**(2.0 - alpha))\n            return e1, e2\n\n        # --- Evaluate Strategies ---\n        for i, cand in enumerate(candidates):\n            alpha, lam = cand['alpha'], cand['lambda']\n            \n            # --- Strategy: Raw ---\n            var_V_raw = np.inf\n            clt_raw = False\n            if alpha > 2.0:\n                var_V_raw = (alpha * x_m**2) / ((alpha - 2.0) * (alpha - 1.0)**2)\n                clt_raw = True\n            \n            vrf_raw = 1.0\n            if np.isfinite(var_V_raw) and var_V_raw > 0:\n                vrf_raw = 1.0 + (lam**2 * var_X) / var_V_raw\n\n            strategy_code_raw = 1 + i * 3\n            if vrf_raw > best_vrf:\n                best_vrf = vrf_raw\n                best_strategy_code = strategy_code_raw\n                clt_for_best_strategy = clt_raw\n                \n            # --- Strategy: Winsorized ---\n            e1_win, e2_win = get_winsorized_moments(alpha, x_m, tau)\n            var_V_win = e2_win - e1_win**2\n            clt_win = True # Always finite variance\n            vrf_win = 1.0\n            if var_V_win > 0:\n              vrf_win = 1.0 + (lam**2 * var_X) / var_V_win\n\n            strategy_code_win = 2 + i * 3\n            if vrf_win > best_vrf:\n                best_vrf = vrf_win\n                best_strategy_code = strategy_code_win\n                clt_for_best_strategy = clt_win\n                \n            # --- Strategy: Truncated ---\n            e1_trunc, e2_trunc = get_truncated_moments(alpha, x_m, tau)\n            var_V_trunc = e2_trunc - e1_trunc**2\n            clt_trunc = True # Always finite variance\n            vrf_trunc = 1.0\n            if var_V_trunc > 0:\n              vrf_trunc = 1.0 + (lam**2 * var_X) / var_V_trunc\n\n            strategy_code_trunc = 3 + i * 3\n            if vrf_trunc > best_vrf:\n                best_vrf = vrf_trunc\n                best_strategy_code = strategy_code_trunc\n                clt_for_best_strategy = clt_trunc\n\n        # --- Determine boolean flags ---\n        # c_r: any raw control has finite variance (i.e. yields asymptotic normality)\n        c_r = 1 if (candidates[0]['alpha'] > 2.0 or candidates[1]['alpha'] > 2.0) else 0\n        \n        # c_s: selected strategy has finite variance\n        c_s = 1 if clt_for_best_strategy else 0\n        \n        all_results.append(f\"[{best_strategy_code},{best_vrf},{c_r},{c_s}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}