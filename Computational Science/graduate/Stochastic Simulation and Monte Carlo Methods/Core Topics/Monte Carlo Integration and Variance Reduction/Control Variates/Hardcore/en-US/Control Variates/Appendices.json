{
    "hands_on_practices": [
        {
            "introduction": "To effectively apply the control variates method, one must first grasp its theoretical foundation. This initial exercise  guides you through deriving the optimal coefficient $c^*$ for a single control variate from first principles. By minimizing the variance of the estimator $\\hat{\\mu}_c = \\bar{Y} - c(\\bar{X} - \\mu_X)$, you will build a solid understanding of how the correlation between your variable of interest $Y$ and a control $X$ can be leveraged to produce more precise Monte Carlo estimates.",
            "id": "3299185",
            "problem": "You are estimating the mean $\\,\\mu_Y = \\mathbb{E}[Y]\\,$ of a real-valued random variable $\\,Y\\,$ using Monte Carlo (MC) with a control variate. You observe $\\,n\\,$ independent and identically distributed (i.i.d.) draws of a bivariate pair $\\,(Y, X)\\,$ with known control mean $\\,\\mu_X = \\mathbb{E}[X]\\,$. Define the control variate estimator $\\,\\hat{\\mu}_c = \\bar{Y} - c \\left(\\bar{X} - \\mu_X\\right)\\,$, where $\\,\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\,$ and $\\,\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\,$. Assume $\\,\\operatorname{Var}(Y) = 2\\,$, $\\,\\operatorname{Var}(X) = 3\\,$, the correlation is $\\,\\rho = 0.8\\,$, and the sample size is $\\,n = 2000\\,$.\n\nStarting only from the linearity of expectation, the variance and covariance definitions for averages of i.i.d. samples, and the variance formula for a linear combination of random variables, derive the optimal coefficient $\\,c^*\\,$ that minimizes $\\,\\operatorname{Var}(\\hat{\\mu}_c)\\,$, the corresponding minimized variance $\\,\\operatorname{Var}(\\hat{\\mu}_{c^*})\\,$, and the variance reduction relative to the plain sample mean $\\,\\bar{Y}\\,$, defined as $\\,1 - \\frac{\\operatorname{Var}(\\hat{\\mu}_{c^*})}{\\operatorname{Var}(\\bar{Y})}\\,$.\n\nGiven $\\,\\rho = 0.8\\,$, $\\,\\operatorname{Var}(Y) = 2\\,$, $\\,\\operatorname{Var}(X) = 3\\,$, and $\\,n = 2000\\,$, compute numerically:\n- $\\,c^*\\,$,\n- $\\,\\operatorname{Var}(\\hat{\\mu}_{c^*})\\,$,\n- the variance reduction $\\,1 - \\frac{\\operatorname{Var}(\\hat{\\mu}_{c^*})}{\\operatorname{Var}(\\bar{Y})}\\,$,\n\nand report your final answer as a row vector $(c^*, \\operatorname{Var}(\\hat{\\mu}_{c^*}), \\text{reduction})$, rounded to four significant figures. Express the reduction as a decimal (not using a percent sign).",
            "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- Estimator for the mean $\\mu_Y = \\mathbb{E}[Y]$: $\\hat{\\mu}_c = \\bar{Y} - c \\left(\\bar{X} - \\mu_X\\right)$\n- Sample means: $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i$ and $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$\n- Draws: $n$ independent and identically distributed (i.i.d.) draws of a bivariate pair $(Y, X)$\n- Known control mean: $\\mu_X = \\mathbb{E}[X]$\n- Variance of $Y$: $\\operatorname{Var}(Y) = 2$\n- Variance of $X$: $\\operatorname{Var}(X) = 3$\n- Correlation of $Y$ and $X$: $\\rho = 0.8$\n- Sample size: $n = 2000$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly rooted in the theory of Monte Carlo methods, specifically variance reduction using control variates. All concepts—expectation, variance, covariance, correlation, and sample means—are standard in probability and statistics.\n- **Well-Posed:** The problem is well-posed. It asks for the derivation and computation of the optimal coefficient for a control variate estimator, the resulting variance, and the variance reduction. This is a classic, solvable problem with a unique and meaningful solution.\n- **Objective:** The problem is stated using precise, objective mathematical language.\n- **Complete and Consistent:** All necessary data and definitions are provided, and there are no contradictions.\n- **Realistic and Feasible:** The setup and values are realistic for a typical simulation context.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution follows.\n\nThe control variate estimator for $\\mu_Y = \\mathbb{E}[Y]$ is given by\n$$\n\\hat{\\mu}_c = \\bar{Y} - c(\\bar{X} - \\mu_X)\n$$\nwhere $c$ is a constant, and $\\bar{Y}$ and $\\bar{X}$ are the sample means from $n$ i.i.d. observations $(Y_i, X_i)$. The mean of the control variate, $\\mu_X = \\mathbb{E}[X]$, is known.\n\nFirst, we establish that $\\hat{\\mu}_c$ is an unbiased estimator of $\\mu_Y$ for any choice of $c$. Using the linearity of expectation:\n$$\n\\mathbb{E}[\\hat{\\mu}_c] = \\mathbb{E}[\\bar{Y} - c(\\bar{X} - \\mu_X)] = \\mathbb{E}[\\bar{Y}] - c(\\mathbb{E}[\\bar{X}] - \\mathbb{E}[\\mu_X])\n$$\nSince the samples are i.i.d., $\\mathbb{E}[\\bar{Y}] = \\mu_Y$ and $\\mathbb{E}[\\bar{X}] = \\mu_X$. As $\\mu_X$ is a known constant, $\\mathbb{E}[\\mu_X] = \\mu_X$.\n$$\n\\mathbb{E}[\\hat{\\mu}_c] = \\mu_Y - c(\\mu_X - \\mu_X) = \\mu_Y\n$$\nThe estimator is indeed unbiased. Our goal is to find the coefficient $c^*$ that minimizes the variance of this estimator.\n\nThe variance of $\\hat{\\mu}_c$ is:\n$$\n\\operatorname{Var}(\\hat{\\mu}_c) = \\operatorname{Var}(\\bar{Y} - c(\\bar{X} - \\mu_X))\n$$\nSince $\\mu_X$ is a constant, this is equivalent to:\n$$\n\\operatorname{Var}(\\hat{\\mu}_c) = \\operatorname{Var}(\\bar{Y} - c\\bar{X})\n$$\nUsing the formula for the variance of a linear combination of random variables, $\\operatorname{Var}(A - B) = \\operatorname{Var}(A) + \\operatorname{Var}(B) - 2\\operatorname{Cov}(A, B)$, we have:\n$$\n\\operatorname{Var}(\\hat{\\mu}_c) = \\operatorname{Var}(\\bar{Y}) + \\operatorname{Var}(c\\bar{X}) - 2\\operatorname{Cov}(\\bar{Y}, c\\bar{X})\n$$\nUsing the properties of variance and covariance, $\\operatorname{Var}(kZ) = k^2\\operatorname{Var}(Z)$ and $\\operatorname{Cov}(Z_1, kZ_2) = k\\operatorname{Cov}(Z_1, Z_2)$, this becomes:\n$$\n\\operatorname{Var}(\\hat{\\mu}_c) = \\operatorname{Var}(\\bar{Y}) + c^2\\operatorname{Var}(\\bar{X}) - 2c\\operatorname{Cov}(\\bar{Y}, \\bar{X})\n$$\nNext, we express $\\operatorname{Var}(\\bar{Y})$, $\\operatorname{Var}(\\bar{X})$, and $\\operatorname{Cov}(\\bar{Y}, \\bar{X})$ in terms of the population moments. Since the samples $(Y_i, X_i)$ are i.i.d., for the sample means $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i$ and $\\bar{X} = \\frac{1}{n}\\sum_{j=1}^n X_j$:\n$$\n\\operatorname{Var}(\\bar{Y}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\operatorname{Var}(Y_i) = \\frac{n\\operatorname{Var}(Y)}{n^2} = \\frac{\\operatorname{Var}(Y)}{n}\n$$\n$$\n\\operatorname{Var}(\\bar{X}) = \\frac{\\operatorname{Var}(X)}{n}\n$$\n$$\n\\operatorname{Cov}(\\bar{Y}, \\bar{X}) = \\operatorname{Cov}\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i, \\frac{1}{n}\\sum_{j=1}^n X_j\\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\operatorname{Cov}(Y_i, X_j)\n$$\nSince the pairs are i.i.d., $\\operatorname{Cov}(Y_i, X_j)=0$ for $i \\neq j$. Thus, only terms with $i=j$ survive:\n$$\n\\operatorname{Cov}(\\bar{Y}, \\bar{X}) = \\frac{1}{n^2} \\sum_{i=1}^n \\operatorname{Cov}(Y_i, X_i) = \\frac{n\\operatorname{Cov}(Y, X)}{n^2} = \\frac{\\operatorname{Cov}(Y, X)}{n}\n$$\nSubstituting these expressions back into the variance of $\\hat{\\mu}_c$:\n$$\n\\operatorname{Var}(\\hat{\\mu}_c) = \\frac{\\operatorname{Var}(Y)}{n} + c^2\\frac{\\operatorname{Var}(X)}{n} - 2c\\frac{\\operatorname{Cov}(Y, X)}{n} = \\frac{1}{n} \\left( \\operatorname{Var}(Y) - 2c\\operatorname{Cov}(Y, X) + c^2\\operatorname{Var}(X) \\right)\n$$\nThis expression is a quadratic function of $c$. To find the value $c^*$ that minimizes this variance, we take the derivative with respect to $c$ and set it to zero:\n$$\n\\frac{d}{dc}\\operatorname{Var}(\\hat{\\mu}_c) = \\frac{1}{n} \\left( -2\\operatorname{Cov}(Y, X) + 2c\\operatorname{Var}(X) \\right) = 0\n$$\n$$\n-2\\operatorname{Cov}(Y, X) + 2c^*\\operatorname{Var}(X) = 0\n$$\nSolving for the optimal coefficient $c^*$:\n$$\nc^* = \\frac{\\operatorname{Cov}(Y, X)}{\\operatorname{Var}(X)}\n$$\nNow we find the minimized variance $\\operatorname{Var}(\\hat{\\mu}_{c^*})$ by substituting $c^*$ back into the variance expression:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{c^*}) = \\frac{1}{n} \\left( \\operatorname{Var}(Y) - 2 \\frac{\\operatorname{Cov}(Y, X)}{\\operatorname{Var}(X)} \\operatorname{Cov}(Y, X) + \\left(\\frac{\\operatorname{Cov}(Y, X)}{\\operatorname{Var}(X)}\\right)^2 \\operatorname{Var}(X) \\right)\n$$\n$$\n\\operatorname{Var}(\\hat{\\mu}_{c^*}) = \\frac{1}{n} \\left( \\operatorname{Var}(Y) - \\frac{2\\operatorname{Cov}(Y, X)^2}{\\operatorname{Var}(X)} + \\frac{\\operatorname{Cov}(Y, X)^2}{\\operatorname{Var}(X)} \\right)\n$$\n$$\n\\operatorname{Var}(\\hat{\\mu}_{c^*}) = \\frac{1}{n} \\left( \\operatorname{Var}(Y) - \\frac{\\operatorname{Cov}(Y, X)^2}{\\operatorname{Var}(X)} \\right)\n$$\nUsing the definition of the correlation coefficient $\\rho = \\frac{\\operatorname{Cov}(Y, X)}{\\sqrt{\\operatorname{Var}(Y)}\\sqrt{\\operatorname{Var}(X)}}$, we have $\\operatorname{Cov}(Y, X)^2 = \\rho^2 \\operatorname{Var}(Y)\\operatorname{Var}(X)$. Substituting this gives:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{c^*}) = \\frac{1}{n} \\left( \\operatorname{Var}(Y) - \\frac{\\rho^2 \\operatorname{Var}(Y)\\operatorname{Var}(X)}{\\operatorname{Var}(X)} \\right) = \\frac{\\operatorname{Var}(Y)}{n} (1 - \\rho^2)\n$$\nThe variance of the plain sample mean estimator is $\\operatorname{Var}(\\bar{Y}) = \\frac{\\operatorname{Var}(Y)}{n}$. The variance reduction relative to $\\bar{Y}$ is:\n$$\n1 - \\frac{\\operatorname{Var}(\\hat{\\mu}_{c^*})}{\\operatorname{Var}(\\bar{Y})} = 1 - \\frac{\\frac{\\operatorname{Var}(Y)}{n} (1 - \\rho^2)}{\\frac{\\operatorname{Var}(Y)}{n}} = 1 - (1 - \\rho^2) = \\rho^2\n$$\nNow, we compute the numerical values given $\\rho = 0.8$, $\\operatorname{Var}(Y) = 2$, $\\operatorname{Var}(X) = 3$, and $n = 2000$.\n\n1.  **Optimal coefficient $c^*$**:\n    First, we need $\\operatorname{Cov}(Y, X)$.\n    $$\n    \\operatorname{Cov}(Y, X) = \\rho \\sqrt{\\operatorname{Var}(Y)} \\sqrt{\\operatorname{Var}(X)} = 0.8 \\times \\sqrt{2} \\times \\sqrt{3} = 0.8\\sqrt{6}\n    $$\n    $$\n    c^* = \\frac{\\operatorname{Cov}(Y, X)}{\\operatorname{Var}(X)} = \\frac{0.8\\sqrt{6}}{3} \\approx 0.65319659...\n    $$\n    Rounded to four significant figures, $c^* = 0.6532$.\n\n2.  **Minimized variance $\\operatorname{Var}(\\hat{\\mu}_{c^*})$**:\n    $$\n    \\operatorname{Var}(\\hat{\\mu}_{c^*}) = \\frac{\\operatorname{Var}(Y)}{n} (1 - \\rho^2) = \\frac{2}{2000} (1 - 0.8^2) = 0.001 \\times (1 - 0.64) = 0.001 \\times 0.36 = 0.00036\n    $$\n    In scientific notation and rounded to four significant figures, this is $3.600 \\times 10^{-4}$.\n\n3.  **Variance reduction**:\n    $$\n    \\text{reduction} = \\rho^2 = 0.8^2 = 0.64\n    $$\n    Expressed as a decimal with four significant figures, the reduction is $0.6400$.\n\nThe final answer is the row vector $(c^*, \\operatorname{Var}(\\hat{\\mu}_{c^*}), \\text{reduction})$ with values rounded to four significant figures.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.6532  3.600 \\times 10^{-4}  0.6400 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In most practical scenarios, the true population moments needed to calculate the optimal coefficient $c^*$ are unknown. This practice  bridges the gap between theory and application by demonstrating how to estimate the optimal coefficient using sample statistics derived from your simulation data. This exercise mirrors the standard workflow in stochastic simulation, where the generated data itself is used to refine and improve the final estimate.",
            "id": "3299180",
            "problem": "You are estimating the unknown mean $\\,\\theta = \\mathbb{E}[Y]\\,$ of a real-valued random variable $\\,Y\\,$ using Monte Carlo simulation with a control variate. You have independent and identically distributed samples $\\,(Y_{i},X_{i})\\,$ for $\\,i=1,\\dots,n\\,$, where $\\,X\\,$ is a real-valued control variate with known mean $\\,\\mathbb{E}[X]=0\\,$ and finite second moments. Consider the family of unbiased control variate estimators\n$$\n\\hat{\\theta}(c) \\equiv \\bar{Y} - c\\left(\\bar{X} - \\mu_{X}\\right),\n$$\nwhere $\\,\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}\\,$, $\\,\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\,$, and $\\,\\mu_{X}=\\mathbb{E}[X]=0\\,$. You are given the following sample summary statistics computed from the simulated dataset:\n$$\n\\bar{Y}=1.5,\\quad \\bar{X}=0.7,\\quad S_{XX}=2.4,\\quad S_{YX}=1.2,\n$$\nwhere $\\,S_{XX}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}\\,$ and $\\,S_{YX}=\\frac{1}{n-1}\\sum_{i=1}^{n}(Y_{i}-\\bar{Y})(X_{i}-\\bar{X})\\,$ denote the unbiased sample variance of $\\,X\\,$ and the unbiased sample covariance between $\\,Y\\,$ and $\\,X\\,$, respectively.\n\nStarting from first principles—namely, the definitions of unbiasedness, variance, covariance, and the variance of a linear combination—derive the value of the variance-minimizing coefficient $\\,c^{*}\\,$ and the corresponding adjusted estimator value $\\,\\hat{\\theta}(c^{*})\\,$ using only the provided summary statistics. Assume all required moments exist and are finite, but do not assume any specific distributional form for $\\,(Y,X)\\,$.\n\nState your final answer as two values in the order $c^{*}$ and $\\hat{\\theta}(c^{*})$. Give your answer in exact form with no rounding.",
            "solution": "The problem requires the derivation of the variance-minimizing coefficient for a control variate estimator and the calculation of the corresponding point estimate for an unknown mean $\\theta$. The process begins with validating the problem statement.\n\n### Step 1: Extract Givens\n- The parameter to be estimated is $\\theta = \\mathbb{E}[Y]$.\n- The data consist of independent and identically distributed (i.i.d.) pairs $(Y_{i},X_{i})$ for $i=1,\\dots,n$.\n- $X$ is a control variate with known mean $\\mu_{X} = \\mathbb{E}[X]=0$.\n- The family of control variate estimators is given by $\\hat{\\theta}(c) \\equiv \\bar{Y} - c\\left(\\bar{X} - \\mu_{X}\\right)$, where $\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}$ and $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. Given $\\mu_X = 0$, this simplifies to $\\hat{\\theta}(c) = \\bar{Y} - c\\bar{X}$.\n- The provided sample summary statistics are:\n  - $\\bar{Y}=1.5$\n  - $\\bar{X}=0.7$\n  - Unbiased sample variance of $X$: $S_{XX}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2} = 2.4$\n  - Unbiased sample covariance of $Y$ and $X$: $S_{YX}=\\frac{1}{n-1}\\sum_{i=1}^{n}(Y_{i}-\\bar{Y})(X_{i}-\\bar{X}) = 1.2$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It describes a standard and fundamental application of the control variate method, a widely used variance reduction technique in Monte Carlo simulation. The objective is to find an optimal parameter by minimizing a variance function, which is a well-posed optimization problem. The definitions and data provided are clear, consistent, and sufficient for computing an estimate of the optimal coefficient and the resulting adjusted mean. The use of sample statistics ($S_{XX}$, $S_{YX}$) as estimates for the corresponding population parameters ($\\text{Var}(X)$, $\\text{Cov}(Y,X)$) is a standard and necessary procedure when the population moments are unknown. The problem is free from ambiguity, subjective claims, and factual errors.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. We proceed with the solution.\n\n### Derivation from First Principles\n\nThe objective is to find the value of the coefficient $c$ that minimizes the variance of the estimator $\\hat{\\theta}(c)$. The estimator is given as $\\hat{\\theta}(c) = \\bar{Y} - c\\bar{X}$, since $\\mu_X = 0$.\n\nFirst, we confirm that the estimator is unbiased for any choice of $c$. The expectation of the estimator is:\n$$\n\\mathbb{E}[\\hat{\\theta}(c)] = \\mathbb{E}[\\bar{Y} - c\\bar{X}] = \\mathbb{E}[\\bar{Y}] - c\\mathbb{E}[\\bar{X}]\n$$\nSince the samples are i.i.d., the expectation of the sample mean is the population mean:\n$$\n\\mathbb{E}[\\bar{Y}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}\\right] = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}[Y_{i}] = \\frac{1}{n}(n\\theta) = \\theta\n$$\n$$\n\\mathbb{E}[\\bar{X}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right] = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}[X_{i}] = \\frac{1}{n}(n\\mu_{X}) = \\mu_{X} = 0\n$$\nSubstituting these into the expectation of the estimator yields:\n$$\n\\mathbb{E}[\\hat{\\theta}(c)] = \\theta - c(0) = \\theta\n$$\nThe estimator $\\hat{\\theta}(c)$ is indeed unbiased for $\\theta$ for any finite value of $c$.\n\nNext, we derive the variance of $\\hat{\\theta}(c)$. Using the property for the variance of a linear combination of random variables, $\\text{Var}(A - B) = \\text{Var}(A) + \\text{Var}(B) - 2\\text{Cov}(A,B)$, we have:\n$$\n\\text{Var}(\\hat{\\theta}(c)) = \\text{Var}(\\bar{Y} - c\\bar{X}) = \\text{Var}(\\bar{Y}) + \\text{Var}(c\\bar{X}) - 2\\text{Cov}(\\bar{Y}, c\\bar{X})\n$$\n$$\n\\text{Var}(\\hat{\\theta}(c)) = \\text{Var}(\\bar{Y}) + c^{2}\\text{Var}(\\bar{X}) - 2c\\text{Cov}(\\bar{Y}, \\bar{X})\n$$\nLet $\\sigma_{Y}^{2} = \\text{Var}(Y)$, $\\sigma_{X}^{2} = \\text{Var}(X)$, and $\\sigma_{YX} = \\text{Cov}(Y,X)$. For i.i.d. samples, the variances and covariance of the sample means are:\n$$\n\\text{Var}(\\bar{Y}) = \\frac{\\sigma_{Y}^{2}}{n}, \\quad \\text{Var}(\\bar{X}) = \\frac{\\sigma_{X}^{2}}{n}, \\quad \\text{Cov}(\\bar{Y}, \\bar{X}) = \\frac{\\sigma_{YX}}{n}\n$$\nSubstituting these into the variance expression for $\\hat{\\theta}(c)$:\n$$\n\\text{Var}(\\hat{\\theta}(c)) = \\frac{\\sigma_{Y}^{2}}{n} + c^{2}\\frac{\\sigma_{X}^{2}}{n} - 2c\\frac{\\sigma_{YX}}{n} = \\frac{1}{n} \\left(\\sigma_{Y}^{2} - 2c\\sigma_{YX} + c^{2}\\sigma_{X}^{2}\\right)\n$$\nTo find the value of $c$ that minimizes this variance, we differentiate $\\text{Var}(\\hat{\\theta}(c))$ with respect to $c$ and set the result to zero:\n$$\n\\frac{d}{dc}\\text{Var}(\\hat{\\theta}(c)) = \\frac{d}{dc}\\left[\\frac{1}{n} \\left(\\sigma_{Y}^{2} - 2c\\sigma_{YX} + c^{2}\\sigma_{X}^{2}\\right)\\right] = \\frac{1}{n} \\left(-2\\sigma_{YX} + 2c\\sigma_{X}^{2}\\right)\n$$\nSetting the derivative to zero:\n$$\n\\frac{1}{n} \\left(-2\\sigma_{YX} + 2c\\sigma_{X}^{2}\\right) = 0 \\implies -2\\sigma_{YX} + 2c\\sigma_{X}^{2} = 0 \\implies c = \\frac{\\sigma_{YX}}{\\sigma_{X}^{2}}\n$$\nThis optimal coefficient is denoted $c^{*}$. The second derivative, $\\frac{d^{2}}{dc^{2}}\\text{Var}(\\hat{\\theta}(c)) = \\frac{2\\sigma_{X}^{2}}{n}$, is positive (assuming $X$ is not a constant, so $\\sigma_{X}^{2}0$), which confirms that this value of $c$ corresponds to a minimum.\nThus, the theoretically optimal coefficient is $c^{*} = \\frac{\\text{Cov}(Y,X)}{\\text{Var}(X)}$.\n\nIn practice, the population covariance $\\sigma_{YX}$ and variance $\\sigma_{X}^{2}$ are unknown. We estimate them using their unbiased sample counterparts, $S_{YX}$ and $S_{XX}$, provided in the problem statement. The problem asks for the value of $c^{*}$ using only the given summary statistics, which implies we must use these sample estimates.\n$$\nc^{*} \\approx \\hat{c}^{*} = \\frac{S_{YX}}{S_{XX}}\n$$\nUsing the provided numerical values:\n$$\nS_{YX} = 1.2, \\quad S_{XX} = 2.4\n$$\n$$\nc^{*} = \\frac{1.2}{2.4} = 0.5\n$$\nNow, we compute the adjusted estimator value $\\hat{\\theta}(c^{*})$ using this optimal coefficient and the given sample means:\n$$\n\\hat{\\theta}(c^{*}) = \\bar{Y} - c^{*}\\bar{X}\n$$\nUsing the provided values $\\bar{Y} = 1.5$ and $\\bar{X} = 0.7$:\n$$\n\\hat{\\theta}(c^{*}) = 1.5 - (0.5)(0.7) = 1.5 - 0.35 = 1.15\n$$\nThe variance-minimizing coefficient is $0.5$, and the resulting adjusted estimate of $\\theta$ is $1.15$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5  1.15 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The utility of control variates is significantly enhanced when we can incorporate information from multiple correlated variables. This final exercise  generalizes the method to a multivariate setting, where the scalar coefficient $c$ is replaced by a coefficient vector $c$. You will employ matrix algebra to derive the optimal vector $c^{\\star}$ that minimizes the estimator's variance, equipping you with a more powerful and flexible tool for variance reduction in complex, high-dimensional problems.",
            "id": "3299208",
            "problem": "Consider a scalar random variable $Y$ and a two-dimensional control variate vector $X=(X_{1},X_{2})^{\\top}$. You wish to estimate the mean $\\mu_{Y}=\\mathbb{E}[Y]$ via the control variates method in Monte Carlo (MC), using the single-replication residual $Y-c^{\\top}(X-\\mu_{X})$, where $\\mu_{X}=\\mathbb{E}[X]$ is known. Assume that $(Y,X)$ has finite second moments and that the second-order structure is fully characterized by the following numerical quantities:\n$$\n\\Sigma_{XX}=\\begin{pmatrix}2  1 \\\\ 1  3\\end{pmatrix},\\qquad \\Sigma_{XY}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix},\\qquad \\operatorname{Var}(Y)=5,\n$$\nwhere $\\Sigma_{XX}=\\operatorname{Cov}(X,X)$ and $\\Sigma_{XY}=\\operatorname{Cov}(X,Y)$.\n\nStarting only from the definitions of variance and covariance and standard properties of linear combinations of random variables, determine the coefficient vector $c^{\\star}$ that minimizes the variance of the residual $Y-c^{\\top}(X-\\mu_{X})$, and compute the resulting minimized variance $\\operatorname{Var}\\big(Y-(c^{\\star})^{\\top}(X-\\mu_{X})\\big)$.\n\nExpress your final answer as a single row vector $\\big(c_{1}^{\\star},c_{2}^{\\star},V_{\\min}\\big)$ with exact values (no rounding or decimal approximations). No units are required.",
            "solution": "The problem is valid. It is a well-posed problem in the field of stochastic simulation, specifically concerning the control variates method for variance reduction. All necessary information is provided, and the problem is based on standard, sound principles of probability and statistics.\n\nThe objective is to find the coefficient vector $c^{\\star} \\in \\mathbb{R}^2$ that minimizes the variance of the control variate estimator, and to compute this minimum variance. The single-replication estimator is given by $Z_c = Y - c^{\\top}(X - \\mu_X)$, where $c = (c_1, c_2)^{\\top}$ is the vector of coefficients. The variance to be minimized is $V(c) = \\operatorname{Var}(Z_c) = \\operatorname{Var}(Y - c^{\\top}(X - \\mu_X))$.\n\nFirst, we expand the variance expression. Since the variance of a random variable is invariant to shifts by a constant, and $\\mu_X$ is a constant vector, we have:\n$$\nV(c) = \\operatorname{Var}(Y - c^{\\top}X + c^{\\top}\\mu_X) = \\operatorname{Var}(Y - c^{\\top}X)\n$$\nWe use the general property for the variance of a linear combination of random variables. If we define a new random vector $W = \\begin{pmatrix} Y \\\\ X \\end{pmatrix}$, the term $Y - c^{\\top}X$ can be written as $a^{\\top}W$ where $a^{\\top} = \\begin{pmatrix} 1  -c^{\\top} \\end{pmatrix} = \\begin{pmatrix} 1  -c_1  -c_2 \\end{pmatrix}$. The variance is then given by:\n$$\nV(c) = \\operatorname{Var}(a^{\\top}W) = a^{\\top}\\operatorname{Cov}(W,W)a\n$$\nThe covariance matrix of $W$ is a block matrix composed of the given second-order moments:\n$$\n\\operatorname{Cov}(W,W) = \\begin{pmatrix} \\operatorname{Var}(Y)  \\operatorname{Cov}(Y,X) \\\\ \\operatorname{Cov}(X,Y)  \\operatorname{Cov}(X,X) \\end{pmatrix} = \\begin{pmatrix} \\operatorname{Var}(Y)  \\Sigma_{XY}^{\\top} \\\\ \\Sigma_{XY}  \\Sigma_{XX} \\end{pmatrix}\n$$\nSubstituting $a$ and $\\operatorname{Cov}(W,W)$ into the variance expression:\n$$\nV(c) = \\begin{pmatrix} 1  -c^{\\top} \\end{pmatrix} \\begin{pmatrix} \\operatorname{Var}(Y)  \\Sigma_{XY}^{\\top} \\\\ \\Sigma_{XY}  \\Sigma_{XX} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -c \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\nV(c) = \\begin{pmatrix} \\operatorname{Var}(Y) - c^{\\top}\\Sigma_{XY}  \\Sigma_{XY}^{\\top} - c^{\\top}\\Sigma_{XX} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -c \\end{pmatrix}\n$$\n$$\nV(c) = (\\operatorname{Var}(Y) - c^{\\top}\\Sigma_{XY}) \\cdot 1 + (\\Sigma_{XY}^{\\top} - c^{\\top}\\Sigma_{XX}) \\cdot (-c)\n$$\n$$\nV(c) = \\operatorname{Var}(Y) - c^{\\top}\\Sigma_{XY} - \\Sigma_{XY}^{\\top}c + c^{\\top}\\Sigma_{XX}c\n$$\nSince $c^{\\top}\\Sigma_{XY}$ is a scalar, it is equal to its transpose, $(\\Sigma_{XY})^{\\top}c = \\Sigma_{XY}^{\\top}c$. Thus, the expression simplifies to a quadratic form in $c$:\n$$\nV(c) = \\operatorname{Var}(Y) - 2c^{\\top}\\Sigma_{XY} + c^{\\top}\\Sigma_{XX}c\n$$\nTo find the coefficient vector $c^{\\star}$ that minimizes this variance, we differentiate $V(c)$ with respect to the vector $c$ and set the gradient to the zero vector. Using standard matrix calculus rules ($\\nabla_x (b^{\\top}x) = b$ and $\\nabla_x (x^{\\top}Ax) = 2Ax$ for symmetric $A$):\n$$\n\\nabla_{c}V(c) = -2\\Sigma_{XY} + 2\\Sigma_{XX}c\n$$\nSetting the gradient to zero to find the optimal $c^{\\star}$:\n$$\n-2\\Sigma_{XY} + 2\\Sigma_{XX}c^{\\star} = 0\n$$\n$$\n\\Sigma_{XX}c^{\\star} = \\Sigma_{XY}\n$$\nThis is a system of linear equations for $c^{\\star}$. The solution is $c^{\\star} = \\Sigma_{XX}^{-1}\\Sigma_{XY}$, provided that $\\Sigma_{XX}$ is invertible.\n\nWe are given the numerical values:\n$$\n\\Sigma_{XX}=\\begin{pmatrix}2  1 \\\\ 1  3\\end{pmatrix}, \\qquad \\Sigma_{XY}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix}, \\qquad \\operatorname{Var}(Y)=5\n$$\nFirst, we compute the inverse of $\\Sigma_{XX}$. The determinant is $\\det(\\Sigma_{XX}) = (2)(3) - (1)(1) = 6 - 1 = 5$. Since the determinant is non-zero, the inverse exists.\n$$\n\\Sigma_{XX}^{-1} = \\frac{1}{\\det(\\Sigma_{XX})} \\begin{pmatrix} 3  -1 \\\\ -1  2 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 3  -1 \\\\ -1  2 \\end{pmatrix}\n$$\nNow, we can solve for $c^{\\star}$:\n$$\nc^{\\star} = \\Sigma_{XX}^{-1}\\Sigma_{XY} = \\frac{1}{5} \\begin{pmatrix} 3  -1 \\\\ -1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n$$\nc^{\\star} = \\frac{1}{5} \\begin{pmatrix} (3)(1) + (-1)(2) \\\\ (-1)(1) + (2)(2) \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 3 - 2 \\\\ -1 + 4 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1/5 \\\\ 3/5 \\end{pmatrix}\n$$\nSo, the optimal coefficients are $c_1^{\\star} = 1/5$ and $c_2^{\\star} = 3/5$.\n\nNext, we compute the minimized variance, $V_{\\min} = V(c^{\\star})$. We substitute $c^{\\star}$ back into the expression for $V(c)$:\n$$\nV_{\\min} = \\operatorname{Var}(Y) - 2(c^{\\star})^{\\top}\\Sigma_{XY} + (c^{\\star})^{\\top}\\Sigma_{XX}c^{\\star}\n$$\nUsing the condition $\\Sigma_{XX}c^{\\star} = \\Sigma_{XY}$, we can simplify this expression:\n$$\nV_{\\min} = \\operatorname{Var}(Y) - 2(c^{\\star})^{\\top}\\Sigma_{XY} + (c^{\\star})^{\\top}(\\Sigma_{XY})\n$$\n$$\nV_{\\min} = \\operatorname{Var}(Y) - (c^{\\star})^{\\top}\\Sigma_{XY}\n$$\nSubstituting the numerical values for $\\operatorname{Var}(Y)$, $c^{\\star}$, and $\\Sigma_{XY}$:\n$$\n(c^{\\star})^{\\top}\\Sigma_{XY} = \\begin{pmatrix} 1/5  3/5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = (1/5)(1) + (3/5)(2) = \\frac{1}{5} + \\frac{6}{5} = \\frac{7}{5}\n$$\n$$\nV_{\\min} = 5 - \\frac{7}{5} = \\frac{25}{5} - \\frac{7}{5} = \\frac{18}{5}\n$$\nThe optimal coefficient vector is $c^{\\star} = (1/5, 3/5)^{\\top}$ and the resulting minimum variance is $V_{\\min} = 18/5$. The final answer is requested as a single row vector $(c_1^{\\star}, c_2^{\\star}, V_{\\min})$.\n\nFinal vector: $\\left(\\frac{1}{5}, \\frac{3}{5}, \\frac{18}{5}\\right)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{5}  \\frac{3}{5}  \\frac{18}{5}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}