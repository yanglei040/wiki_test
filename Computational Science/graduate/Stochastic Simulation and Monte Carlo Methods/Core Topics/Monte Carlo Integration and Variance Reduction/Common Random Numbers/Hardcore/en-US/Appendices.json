{
    "hands_on_practices": [
        {
            "introduction": "Before applying any technique, it is crucial to understand its mathematical basis. This first exercise guides you through deriving the fundamental formula for the variance of the difference between two estimators when using Common Random Numbers (CRN). By working through this derivation , you will see precisely how a positive correlation $\\rho$ between simulation outputs reduces the variance of their difference and gain the skills to construct random variates with a specific, desired correlation.",
            "id": "3297387",
            "problem": "Consider two alternative simulation estimators, each targeting an unknown real-valued performance measure. For replication index $i \\in \\{1,\\dots,n\\}$, define per-replication outputs\n$$Y_{1,i} = f_1\\!\\left(U_{2i-1},U_{2i}\\right), \\quad Y_{2,i} = f_2\\!\\left(U_{2i-1},U_{2i}\\right),$$\nwhere $\\{U_k\\}_{k \\ge 1}$ is a single shared stream of independent and identically distributed $\\mathrm{Uniform}(0,1)$ random variables, and $f_1,f_2$ are measurable functions. The corresponding sample-mean estimators are\n$$\\hat{\\mu}_1 = \\frac{1}{n}\\sum_{i=1}^n Y_{1,i}, \\qquad \\hat{\\mu}_2 = \\frac{1}{n}\\sum_{i=1}^n Y_{2,i}.$$\nAssume that for each $i$, the pair $(Y_{1,i},Y_{2,i})$ has finite second moments, marginal standard deviations $\\sigma_10$ and $\\sigma_20$ that do not depend on $i$, and correlation\n$$\\rho \\equiv \\mathrm{corr}\\!\\left(Y_{1,i},Y_{2,i}\\right) = \\mathrm{corr}\\!\\left(f_1\\!\\left(U_{2i-1},U_{2i}\\right), f_2\\!\\left(U_{2i-1},U_{2i}\\right)\\right),$$\nwith $\\rho \\in (-1,1)$ fixed across replications. Replications are independent across $i$.\n\nTask A. Starting from the basic definitions of variance, covariance, and correlation, derive an exact expression for $\\mathrm{Var}\\!\\left(\\hat{\\mu}_1 - \\hat{\\mu}_2\\right)$ in terms of $n$, $\\sigma_1$, $\\sigma_2$, and $\\rho$ under common random numbers.\n\nTask B. Construct explicit functions $f_1$ and $f_2$ that, for a prescribed target correlation $\\rho \\in (-1,1)$ and prescribed marginal parameters $(m_1,\\sigma_1)$ and $(m_2,\\sigma_2)$ with $\\sigma_10$ and $\\sigma_20$, produce per-replication outputs $Y_{1,i} \\sim \\mathcal{N}(m_1,\\sigma_1^2)$ and $Y_{2,i} \\sim \\mathcal{N}(m_2,\\sigma_2^2)$ and achieve $\\mathrm{corr}(Y_{1,i},Y_{2,i})=\\rho$. Your construction must use only the shared stream $\\{U_k\\}_{k\\ge 1}$, and both $f_1$ and $f_2$ must use the same pair $\\left(U_{2i-1},U_{2i}\\right)$ at replication $i$.\n\nTask C. Briefly justify how $\\mathrm{Var}\\!\\left(\\hat{\\mu}_1 - \\hat{\\mu}_2\\right)$ depends on $\\rho$ and identify which values of $\\rho$ minimize or maximize this variance subject to $\\rho \\in (-1,1)$.\n\nProvide, as the final answer, the closed-form expression from Task A for $\\mathrm{Var}\\!\\left(\\hat{\\mu}_1 - \\hat{\\mu}_2\\right)$ as a function of $n$, $\\sigma_1$, $\\sigma_2$, and $\\rho$. No numerical rounding is required, and no physical units are involved.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of stochastic simulation and probability theory, specifically the method of common random numbers (CRN) for variance reduction. The problem is well-posed, with all necessary data and conditions provided, and it is expressed in objective, formal mathematical language. It contains no scientific flaws, ambiguities, or contradictions. We can therefore proceed with a solution.\n\nThe problem is divided into three tasks. We address each in turn.\n\nTask A: Derive an exact expression for $\\mathrm{Var}\\!\\left(\\hat{\\mu}_1 - \\hat{\\mu}_2\\right)$.\n\nWe are asked to find the variance of the difference between two sample-mean estimators, $\\hat{\\mu}_1 = \\frac{1}{n}\\sum_{i=1}^n Y_{1,i}$ and $\\hat{\\mu}_2 = \\frac{1}{n}\\sum_{i=1}^n Y_{2,i}$. We begin by expressing the difference of the estimators in terms of the per-replication outputs.\n$$ \\hat{\\mu}_1 - \\hat{\\mu}_2 = \\frac{1}{n}\\sum_{i=1}^n Y_{1,i} - \\frac{1}{n}\\sum_{i=1}^n Y_{2,i} = \\frac{1}{n}\\sum_{i=1}^n \\left(Y_{1,i} - Y_{2,i}\\right) $$\nWe now compute the variance of this difference. Using the property that $\\mathrm{Var}(cZ) = c^2\\mathrm{Var}(Z)$ for a constant $c$, we have:\n$$ \\mathrm{Var}\\!\\left(\\hat{\\mu}_1 - \\hat{\\mu}_2\\right) = \\mathrm{Var}\\!\\left(\\frac{1}{n}\\sum_{i=1}^n \\left(Y_{1,i} - Y_{2,i}\\right)\\right) = \\frac{1}{n^2}\\mathrm{Var}\\!\\left(\\sum_{i=1}^n \\left(Y_{1,i} - Y_{2,i}\\right)\\right) $$\nThe problem states that replications are independent across the index $i$. This means that the pairs $(Y_{1,i}, Y_{2,i})$ are independent of $(Y_{1,j}, Y_{2,j})$ for $i \\ne j$. Consequently, the random variables representing the difference within each replication, $D_i = Y_{1,i} - Y_{2,i}$, are independent across $i$. For independent random variables, the variance of the sum is the sum of the variances.\n$$ \\mathrm{Var}\\!\\left(\\sum_{i=1}^n \\left(Y_{1,i} - Y_{2,i}\\right)\\right) = \\sum_{i=1}^n \\mathrm{Var}\\!\\left(Y_{1,i} - Y_{2,i}\\right) $$\nSubstituting this into our expression for the variance of the estimator difference:\n$$ \\mathrm{Var}\\!\\left(\\hat{\\mu}_1 - \\hat{\\mu}_2\\right) = \\frac{1}{n^2}\\sum_{i=1}^n \\mathrm{Var}\\!\\left(Y_{1,i} - Y_{2,i}\\right) $$\nThe problem specifies that the marginal standard deviations $\\sigma_1$, $\\sigma_2$ and the correlation $\\rho$ are fixed across all replications $i$. This implies that $\\mathrm{Var}\\!\\left(Y_{1,i} - Y_{2,i}\\right)$ is constant for all $i$. Let's compute this common variance term. Using the formula for the variance of a difference of two random variables, $\\mathrm{Var}(A-B) = \\mathrm{Var}(A) + \\mathrm{Var}(B) - 2\\mathrm{Cov}(A,B)$, we get:\n$$ \\mathrm{Var}\\!\\left(Y_{1,i} - Y_{2,i}\\right) = \\mathrm{Var}(Y_{1,i}) + \\mathrm{Var}(Y_{2,i}) - 2\\mathrm{Cov}(Y_{1,i}, Y_{2,i}) $$\nWe are given that $\\mathrm{Var}(Y_{1,i}) = \\sigma_1^2$ and $\\mathrm{Var}(Y_{2,i}) = \\sigma_2^2$. The covariance can be expressed in terms of the correlation $\\rho$ and the standard deviations:\n$$ \\rho = \\mathrm{corr}(Y_{1,i}, Y_{2,i}) = \\frac{\\mathrm{Cov}(Y_{1,i}, Y_{2,i})}{\\sqrt{\\mathrm{Var}(Y_{1,i})}\\sqrt{\\mathrm{Var}(Y_{2,i})}} = \\frac{\\mathrm{Cov}(Y_{1,i}, Y_{2,i})}{\\sigma_1 \\sigma_2} $$\nTherefore, $\\mathrm{Cov}(Y_{1,i}, Y_{2,i}) = \\rho \\sigma_1 \\sigma_2$. Substituting these into the variance expression:\n$$ \\mathrm{Var}\\!\\left(Y_{1,i} - Y_{2,i}\\right) = \\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2 $$\nSince this term is the same for all $n$ replications, the sum becomes:\n$$ \\sum_{i=1}^n \\mathrm{Var}\\!\\left(Y_{1,i} - Y_{2,i}\\right) = n \\left(\\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2\\right) $$\nFinally, we substitute this back into the expression for $\\mathrm{Var}\\!\\left(\\hat{\\mu}_1 - \\hat{\\mu}_2\\right)$:\n$$ \\mathrm{Var}\\!\\left(\\hat{\\mu}_1 - \\hat{\\mu}_2\\right) = \\frac{1}{n^2} \\left[ n \\left(\\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2\\right) \\right] = \\frac{\\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2}{n} $$\nThis is the desired exact expression.\n\nTask B: Construct explicit functions $f_1$ and $f_2$.\n\nWe need to construct functions $f_1(u,v)$ and $f_2(u,v)$ such that for a given replication $i$, with $u = U_{2i-1}$ and $v = U_{2i}$, the outputs $Y_{1,i} = f_1(u,v)$ and $Y_{2,i} = f_2(u,v)$ follow a bivariate normal distribution with mean vector $(m_1, m_2)$, variances $(\\sigma_1^2, \\sigma_2^2)$, and correlation $\\rho$.\n\nThe standard procedure involves three steps:\n1.  Generate a pair of independent standard normal random variables, $Z_1, Z_2 \\sim \\mathcal{N}(0,1)$, from the pair of independent uniform random variables $U_{2i-1}, U_{2i}$. A common method is the Box-Muller transform:\n    $$ Z_1 = \\sqrt{-2\\ln(U_{2i-1})} \\cos(2\\pi U_{2i}) $$\n    $$ Z_2 = \\sqrt{-2\\ln(U_{2i-1})} \\sin(2\\pi U_{2i}) $$\n2.  Transform the independent standard normal variables $(Z_1, Z_2)$ into a pair of correlated standard normal variables $(W_1, W_2)$ with correlation $\\rho$. This can be achieved using a linear transformation that is equivalent to a Cholesky decomposition of the target correlation matrix:\n    $$ W_1 = Z_1 $$\n    $$ W_2 = \\rho Z_1 + \\sqrt{1-\\rho^2} Z_2 $$\n    By construction, $(W_1, W_2)$ has a bivariate normal distribution with mean $(0,0)$, variances $(1,1)$, and covariance $\\mathrm{Cov}(W_1,W_2) = \\rho$.\n3.  Apply a final affine transformation to scale and shift $(W_1, W_2)$ to match the desired marginal parameters $(m_1, \\sigma_1)$ and $(m_2, \\sigma_2)$:\n    $$ Y_{1,i} = m_1 + \\sigma_1 W_1 $$\n    $$ Y_{2,i} = m_2 + \\sigma_2 W_2 $$\nBy combining these steps, we can define the required functions $f_1$ and $f_2$. For any pair of inputs $(u,v)$ corresponding to $(U_{2i-1}, U_{2i})$:\n$$ f_1(u,v) = m_1 + \\sigma_1 \\left( \\sqrt{-2\\ln(u)} \\cos(2\\pi v) \\right) $$\n$$ f_2(u,v) = m_2 + \\sigma_2 \\left( \\rho \\left(\\sqrt{-2\\ln(u)} \\cos(2\\pi v)\\right) + \\sqrt{1-\\rho^2} \\left(\\sqrt{-2\\ln(u)} \\sin(2\\pi v)\\right) \\right) $$\nThis construction fulfills all requirements of the task.\n\nTask C: Justify the dependence of $\\mathrm{Var}\\!\\left(\\hat{\\mu}_1 - \\hat{\\mu}_2\\right)$ on $\\rho$ and identify extremal values.\n\nFrom Task A, the variance of the difference is given by:\n$$ V(\\rho) = \\mathrm{Var}\\!\\left(\\hat{\\mu}_1 - \\hat{\\mu}_2\\right) = \\frac{1}{n}\\left(\\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2\\right) $$\nThe parameters $n, \\sigma_1, \\sigma_2$ are all positive constants. Thus, $V(\\rho)$ is a linear function of the correlation $\\rho$. To analyze its dependence on $\\rho$, we can examine its derivative with respect to $\\rho$:\n$$ \\frac{d V}{d\\rho} = \\frac{d}{d\\rho}\\left[\\frac{\\sigma_1^2 + \\sigma_2^2}{n} - \\frac{2\\sigma_1\\sigma_2}{n}\\rho\\right] = -\\frac{2\\sigma_1\\sigma_2}{n} $$\nSince $\\sigma_1  0$, $\\sigma_2  0$, and $n \\ge 1$, the derivative $\\frac{dV}{d\\rho}$ is a negative constant. This proves that $V(\\rho)$ is a strictly decreasing function of $\\rho$ over its entire domain $\\rho \\in (-1,1)$.\n\nTo minimize the variance, one must choose the largest possible value of $\\rho$. As $\\rho$ approaches its upper bound of $1$, the variance is minimized. The infimum of the variance is:\n$$ \\lim_{\\rho \\to 1^{-}} V(\\rho) = \\frac{\\sigma_1^2 + \\sigma_2^2 - 2\\sigma_1\\sigma_2}{n} = \\frac{(\\sigma_1 - \\sigma_2)^2}{n} $$\nThis demonstrates the power of CRN: if the two simulated systems are strongly positively correlated, the variance of the difference of their performance estimators can be significantly reduced.\n\nConversely, to maximize the variance, one must choose the smallest possible value of $\\rho$. As $\\rho$ approaches its lower bound of $-1$, the variance is maximized. The supremum of the variance is:\n$$ \\lim_{\\rho \\to -1^{+}} V(\\rho) = \\frac{\\sigma_1^2 + \\sigma_2^2 + 2\\sigma_1\\sigma_2}{n} = \\frac{(\\sigma_1 + \\sigma_2)^2}{n} $$\nIn summary, the variance is minimized for values of $\\rho$ approaching $1$ and maximized for values of $\\rho$ approaching $-1$.",
            "answer": "$$\\boxed{\\frac{\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2}{n}}$$"
        },
        {
            "introduction": "Theory provides the \"why,\" but simulation demonstrates the tangible impact. This practice  bridges that gap by having you design and implement a Monte Carlo experiment to measure the effectiveness of CRN across several scenarios. You will compare theoretical predictions with empirical estimates from your code, developing a concrete intuition for when CRN is highly beneficial and when it can be detrimental.",
            "id": "3297426",
            "problem": "You are asked to design and analyze a Monte Carlo experiment to quantify the effectiveness of the Common Random Numbers technique in variance reduction. The analysis must be grounded in first principles of probability and statistics and implemented in a single runnable program. The objective is to estimate the variance reduction achieved by pairing outputs using a shared source of randomness and to compare it with the variance when outputs are generated independently. The experiment must also characterize the induced correlation between the paired outputs in the Common Random Numbers setting. All angles in trigonometric functions must be in radians. No physical units are involved.\n\nBegin from the following fundamental base.\n\nLet $W$ be a random input, $X = f_1(W)$ and $Y = f_2(W)$ be two outputs computed from $W$. The target quantity is the mean difference $\\Delta = \\mathbb{E}[X] - \\mathbb{E}[Y]$. Consider two single-sample estimators of $\\Delta$:\n- The independent-pairs estimator uses $X^{\\mathrm{ind}} = f_1(W_1)$ and $Y^{\\mathrm{ind}} = f_2(W_2)$ with $W_1$ and $W_2$ independent and identically distributed, producing the single-sample difference $D^{\\mathrm{ind}} = X^{\\mathrm{ind}} - Y^{\\mathrm{ind}}$.\n- The common-random-numbers estimator uses the same shared input $W$ to produce $X^{\\mathrm{crn}} = f_1(W)$ and $Y^{\\mathrm{crn}} = f_2(W)$, producing the single-sample difference $D^{\\mathrm{crn}} = X^{\\mathrm{crn}} - Y^{\\mathrm{crn}}$.\n\nUse the fact that for any random variables $U$ and $V$, one has $\\mathrm{Var}(U - V) = \\mathrm{Var}(U) + \\mathrm{Var}(V) - 2 \\mathrm{Cov}(U,V)$, and that if $U$ and $V$ are independent, then $\\mathrm{Cov}(U,V) = 0$. The Pearson correlation coefficient between $U$ and $V$ is $\\rho(U,V) = \\mathrm{Cov}(U,V) / \\sqrt{\\mathrm{Var}(U)\\mathrm{Var}(V)}$.\n\nDefine the variance reduction ratio $\\mathrm{VRR}$ as the ratio of the single-sample variance of $D^{\\mathrm{ind}}$ to the single-sample variance of $D^{\\mathrm{crn}}$. That is, $\\mathrm{VRR} = \\mathrm{Var}(D^{\\mathrm{ind}}) / \\mathrm{Var}(D^{\\mathrm{crn}})$, where the variance is interpreted per single paired observation. A value of $\\mathrm{VRR}  1$ indicates that Common Random Numbers reduce the variance of the difference estimator relative to independent sampling, $\\mathrm{VRR} = 1$ indicates no change, and $\\mathrm{VRR}  1$ indicates that Common Random Numbers increase the variance.\n\nYour tasks are:\n- Derive expressions for $\\mathrm{Var}(D^{\\mathrm{ind}})$ and $\\mathrm{Var}(D^{\\mathrm{crn}})$ in terms of $\\mathrm{Var}(X)$, $\\mathrm{Var}(Y)$, and $\\mathrm{Cov}(X,Y)$.\n- For each test case, derive closed-form expressions for $\\mathrm{Var}(X)$, $\\mathrm{Var}(Y)$, and $\\mathrm{Cov}(X,Y)$ from the distribution of $W$ and the functions $f_1$ and $f_2$, and thereby derive theoretical values of $\\mathrm{VRR}$ and $\\rho(X,Y)$.\n- Implement a Monte Carlo program to estimate empirical values of $\\mathrm{VRR}$ and $\\rho(X,Y)$ by simulation with Common Random Numbers and independent pairs. Use a single shared pseudo-random number generator seed so that the experiment is reproducible. Use $n = 200000$ independent samples for each test case.\n\nTest suite specification. Analyze the following four cases, which together cover a general case, a boundary case with no variance reduction, and two significant edge cases where Common Random Numbers are either beneficial or harmful:\n- Case $1$: $W \\sim \\mathcal{N}(0,1)$, $X = \\exp(a W)$ with $a = 0.8$, $Y = \\exp(b W + c)$ with $b = 0.6$ and $c = 0.2$.\n- Case $2$: $W \\sim \\mathcal{N}(0,1)$, $X = \\exp(a W)$ with $a = 0.8$, $Y = \\exp(-b W + c)$ with $b = 0.6$ and $c = 0.2$.\n- Case $3$: $W \\sim \\mathrm{Uniform}(0,1)$, $X = \\sin(2 \\pi W)$, $Y = \\cos(2 \\pi W)$.\n- Case $4$: $W \\sim \\mathcal{N}(0,1)$, $X = W + W^2$, $Y = W - W^2$.\n\nFor trigonometric functions, interpret $2 \\pi W$ in radians. For simulation, use a single pseudo-random seed of $42$ and $n = 200000$ samples per case.\n\nRequired final output format. Your program should produce a single line of output containing a list of four sublists, each sublist corresponding to a test case in the same order as above. Each sublist must contain four floating-point numbers in the order $[\\mathrm{VRR}_{\\mathrm{emp}}, \\mathrm{VRR}_{\\mathrm{theory}}, \\rho_{\\mathrm{emp}}, \\rho_{\\mathrm{theory}}]$. The entire output must be a comma-separated list enclosed in square brackets, for example $[[x_1,x_2,x_3,x_4],[y_1,y_2,y_3,y_4],[z_1,z_2,z_3,z_4],[w_1,w_2,w_3,w_4]]$ with no additional text.",
            "solution": "The problem requires a theoretical and empirical analysis of the Common Random Numbers (CRN) variance reduction technique. The analysis will be performed on four distinct cases to illustrate the conditions under which CRN is beneficial, neutral, or detrimental.\n\nFirst, we establish the general theoretical framework for the analysis. Let $X = f_1(W)$ and $Y = f_2(W)$ be two random variables that are functions of a common underlying random variable $W$. We are interested in estimating the difference in their means, $\\Delta = \\mathbb{E}[X] - \\mathbb{E}[Y]$.\n\nThe independent-pairs estimator for a single sample is $D^{\\mathrm{ind}} = X^{\\mathrm{ind}} - Y^{\\mathrm{ind}} = f_1(W_1) - f_2(W_2)$, where $W_1$ and $W_2$ are independent and identically distributed copies of $W$. Consequently, $X^{\\mathrm{ind}}$ and $Y^{\\mathrm{ind}}$ are independent. The variance of this estimator is given by:\n$$\n\\mathrm{Var}(D^{\\mathrm{ind}}) = \\mathrm{Var}(X^{\\mathrm{ind}} - Y^{\\mathrm{ind}})\n$$\nUsing the property $\\mathrm{Var}(U - V) = \\mathrm{Var}(U) + \\mathrm{Var}(V) - 2 \\mathrm{Cov}(U,V)$, and the fact that $\\mathrm{Cov}(X^{\\mathrm{ind}}, Y^{\\mathrm{ind}}) = 0$ due to independence, we have:\n$$\n\\mathrm{Var}(D^{\\mathrm{ind}}) = \\mathrm{Var}(X^{\\mathrm{ind}}) + \\mathrm{Var}(Y^{\\mathrm{ind}})\n$$\nSince $W_1$ and $W_2$ have the same distribution as $W$, we have $\\mathrm{Var}(X^{\\mathrm{ind}}) = \\mathrm{Var}(X)$ and $\\mathrm{Var(Y^{\\mathrm{ind}})} = \\mathrm{Var}(Y)$. Thus,\n$$\n\\mathrm{Var}(D^{\\mathrm{ind}}) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)\n$$\n\nThe common-random-numbers (CRN) estimator for a single sample is $D^{\\mathrm{crn}} = X^{\\mathrm{crn}} - Y^{\\mathrm{crn}} = f_1(W) - f_2(W)$. Here, $X^{\\mathrm{crn}}$ and $Y^{\\mathrm{crn}}$ are derived from the same random input $W$ and are therefore generally correlated. The variance of this estimator is:\n$$\n\\mathrm{Var}(D^{\\mathrm{crn}}) = \\mathrm{Var}(X^{\\mathrm{crn}} - Y^{\\mathrm{crn}}) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) - 2 \\mathrm{Cov}(X,Y)\n$$\nwhere $\\mathrm{Cov}(X,Y)$ is the covariance between $X=f_1(W)$ and $Y=f_2(W)$.\n\nThe Variance Reduction Ratio (VRR) is defined as the ratio of these two variances:\n$$\n\\mathrm{VRR} = \\frac{\\mathrm{Var}(D^{\\mathrm{ind}})}{\\mathrm{Var}(D^{\\mathrm{crn}})} = \\frac{\\mathrm{Var}(X) + \\mathrm{Var}(Y)}{\\mathrm{Var}(X) + \\mathrm{Var}(Y) - 2 \\mathrm{Cov}(X,Y)}\n$$\nA positive covariance, $\\mathrm{Cov}(X,Y)  0$, leads to $\\mathrm{VRR}  1$, indicating variance reduction. A negative covariance, $\\mathrm{Cov}(X,Y)  0$, leads to $\\mathrm{VRR}  1$, indicating variance inflation. If $\\mathrm{Cov}(X,Y) = 0$, then $\\mathrm{VRR} = 1$, and CRN has no effect. The strength of this effect is related to the Pearson correlation coefficient, $\\rho(X,Y) = \\mathrm{Cov}(X,Y) / \\sqrt{\\mathrm{Var}(X)\\mathrm{Var}(Y)}$.\n\nWe now proceed to derive the theoretical values for each of the four specified cases.\n\n**Case 1: $W \\sim \\mathcal{N}(0,1)$, $X = \\exp(a W)$ with $a = 0.8$, $Y = \\exp(b W + c)$ with $b = 0.6, c = 0.2$.**\nFor a random variable $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the moment generating function is $\\mathbb{E}[e^{tZ}] = e^{t\\mu + \\frac{1}{2}t^2\\sigma^2}$. For $W \\sim \\mathcal{N}(0,1)$, we have $\\mathbb{E}[e^{tW}] = e^{t^2/2}$.\nThe required moments are:\n$\\mathbb{E}[X] = \\mathbb{E}[e^{aW}] = e^{a^2/2}$\n$\\mathbb{E}[X^2] = \\mathbb{E}[e^{2aW}] = e^{(2a)^2/2} = e^{2a^2}$\n$\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = e^{2a^2} - e^{a^2} = e^{a^2}(e^{a^2}-1)$.\nSimilarly for $Y = e^c e^{bW}$:\n$\\mathbb{E}[Y] = e^c \\mathbb{E}[e^{bW}] = e^c e^{b^2/2}$\n$\\mathbb{E}[Y^2] = e^{2c} \\mathbb{E}[e^{2bW}] = e^{2c} e^{2b^2}$\n$\\mathrm{Var}(Y) = e^{2c}e^{2b^2} - (e^c e^{b^2/2})^2 = e^{2c}(e^{2b^2} - e^{b^2}) = e^{2c}e^{b^2}(e^{b^2}-1)$.\nFor the covariance:\n$\\mathbb{E}[XY] = \\mathbb{E}[e^{aW}e^{bW+c}] = e^c \\mathbb{E}[e^{(a+b)W}] = e^c e^{(a+b)^2/2}$.\n$\\mathrm{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = e^c e^{(a+b)^2/2} - (e^{a^2/2})(e^c e^{b^2/2}) = e^c e^{(a^2+b^2)/2}(e^{ab} - 1)$.\nThe correlation is $\\rho(X,Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\mathrm{Var}(Y)}} = \\frac{e^c e^{(a^2+b^2)/2}(e^{ab} - 1)}{\\sqrt{e^{a^2}(e^{a^2}-1) \\cdot e^{2c}e^{b^2}(e^{b^2}-1)}} = \\frac{e^{ab}-1}{\\sqrt{(e^{a^2}-1)(e^{b^2}-1)}}$.\nSubstituting $a=0.8$ and $b=0.6$:\n$\\rho_{\\mathrm{theory}} = \\frac{e^{0.48}-1}{\\sqrt{(e^{0.64}-1)(e^{0.36}-1)}} \\approx 0.98844$.\n$\\mathrm{VRR}_{\\mathrm{theory}}$ is calculated using the derived variances and covariance. Since $X$ and $Y$ are both increasing functions of $W$, we expect a strong positive correlation and significant variance reduction.\n\n**Case 2: $W \\sim \\mathcal{N}(0,1)$, $X = \\exp(a W)$ with $a = 0.8$, $Y = \\exp(-b W + c)$ with $b = 0.6, c = 0.2$.**\nThis case is similar to Case 1, but with $b$ replaced by $-b$. The variances $\\mathrm{Var}(X)$ and $\\mathrm{Var}(Y)$ remain unchanged, as they depend on $b^2$.\nThe covariance term changes:\n$\\mathrm{Cov}(X,Y) = e^c e^{(a^2+(-b)^2)/2}(e^{a(-b)}-1) = e^c e^{(a^2+b^2)/2}(e^{-ab}-1)$.\nThe correlation becomes:\n$\\rho(X,Y) = \\frac{e^{-ab}-1}{\\sqrt{(e^{a^2}-1)(e^{b^2}-1)}}$.\nSubstituting $a=0.8$ and $b=0.6$:\n$\\rho_{\\mathrm{theory}} = \\frac{e^{-0.48}-1}{\\sqrt{(e^{0.64}-1)(e^{0.36}-1)}} \\approx -0.61163$.\nHere, $X$ is an increasing function of $W$ while $Y$ is a decreasing function. This induces a negative correlation, which will make the denominator of the VRR formula larger than the numerator, resulting in $\\mathrm{VRR}  1$.\n\n**Case 3: $W \\sim \\mathrm{Uniform}(0,1)$, $X = \\sin(2 \\pi W)$, $Y = \\cos(2 \\pi W)$.**\nWe compute the moments by integration over $[0,1]$.\n$\\mathbb{E}[X] = \\int_0^1 \\sin(2\\pi w) \\,dw = 0$.\n$\\mathbb{E}[Y] = \\int_0^1 \\cos(2\\pi w) \\,dw = 0$.\n$\\mathbb{E}[X^2] = \\int_0^1 \\sin^2(2\\pi w) \\,dw = \\int_0^1 \\frac{1-\\cos(4\\pi w)}{2} \\,dw = \\frac{1}{2}$.\n$\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = 1/2$.\n$\\mathbb{E}[Y^2] = \\int_0^1 \\cos^2(2\\pi w) \\,dw = \\int_0^1 \\frac{1+\\cos(4\\pi w)}{2} \\,dw = \\frac{1}{2}$.\n$\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = 1/2$.\n$\\mathbb{E}[XY] = \\int_0^1 \\sin(2\\pi w)\\cos(2\\pi w) \\,dw = \\int_0^1 \\frac{1}{2}\\sin(4\\pi w) \\,dw = 0$.\n$\\mathrm{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = 0$.\nTherefore, $\\rho_{\\mathrm{theory}} = 0$ and $\\mathrm{VRR}_{\\mathrm{theory}} = \\frac{1/2 + 1/2}{1/2 + 1/2 - 0} = 1$. In this case, the outputs are uncorrelated, and CRN provides no variance reduction.\n\n**Case 4: $W \\sim \\mathcal{N}(0,1)$, $X = W + W^2$, $Y = W - W^2$.**\nWe use the moments of the standard normal distribution: $\\mathbb{E}[W]=0$, $\\mathbb{E}[W^2]=1$, $\\mathbb{E}[W^3]=0$, $\\mathbb{E}[W^4]=3$.\n$\\mathbb{E}[X] = \\mathbb{E}[W+W^2] = \\mathbb{E}[W]+\\mathbb{E}[W^2] = 0+1=1$.\n$\\mathbb{E}[Y] = \\mathbb{E}[W-W^2] = \\mathbb{E}[W]-\\mathbb{E}[W^2] = 0-1=-1$.\n$\\mathbb{E}[X^2] = \\mathbb{E}[(W+W^2)^2] = \\mathbb{E}[W^2+2W^3+W^4] = 1+0+3=4$.\n$\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = 4-1^2=3$.\n$\\mathbb{E}[Y^2] = \\mathbb{E}[(W-W^2)^2] = \\mathbb{E}[W^2-2W^3+W^4] = 1-0+3=4$.\n$\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = 4-(-1)^2=3$.\n$\\mathbb{E}[XY] = \\mathbb{E}[(W+W^2)(W-W^2)] = \\mathbb{E}[W^2 - W^4] = 1-3=-2$.\n$\\mathrm{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = -2 - (1)(-1) = -1$.\n$\\rho_{\\mathrm{theory}} = \\frac{-1}{\\sqrt{3 \\cdot 3}} = -1/3$.\n$\\mathrm{VRR}_{\\mathrm{theory}} = \\frac{3+3}{3+3-2(-1)} = \\frac{6}{8} = 0.75$.\nSimilar to Case 2, the negative correlation results in variance inflation, $\\mathrm{VRR}  1$.\n\nThe Monte Carlo simulation will estimate these quantities empirically. For each case, we will generate $n = 200000$ samples.\n1. To estimate $\\mathrm{Var}(D^{\\mathrm{crn}})$ and $\\rho(X,Y)$, we generate $n$ samples of $W$, denoted $\\{w_i\\}_{i=1}^n$. We compute paired outputs $\\{x_i = f_1(w_i), y_i = f_2(w_i)\\}_{i=1}^n$ and differences $\\{d_i^{\\mathrm{crn}} = x_i - y_i\\}_{i=1}^n$. The sample variance of $\\{d_i^{\\mathrm{crn}}\\}$ estimates $\\mathrm{Var}(D^{\\mathrm{crn}})$, and the sample correlation between $\\{x_i\\}$ and $\\{y_i\\}$ estimates $\\rho(X,Y)$.\n2. To estimate $\\mathrm{Var}(D^{\\mathrm{ind}})$, we generate two independent sets of $n$ samples, $\\{w_{1,i}\\}_{i=1}^n$ and $\\{w_{2,i}\\}_{i=1}^n$. We compute independent outputs $\\{x_i^{\\mathrm{ind}} = f_1(w_{1,i}), y_i^{\\mathrm{ind}} = f_2(w_{2,i})\\}_{i=1}^n$ and differences $\\{d_i^{\\mathrm{ind}} = x_i^{\\mathrm{ind}} - y_i^{\\mathrm{ind}}\\}_{i=1}^n$. The sample variance of $\\{d_i^{\\mathrm{ind}}\\}$ estimates $\\mathrm{Var}(D^{\\mathrm{ind}})$.\n3. The empirical VRR is the ratio of these estimated variances.\nThe use of a fixed pseudo-random number generator seed ensures the reproducibility of the experiment.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Performs theoretical and empirical analysis of the Common Random Numbers\n    (CRN) variance reduction technique for four test cases.\n    \"\"\"\n    seed = 42\n    n_samples = 200000\n    rng = np.random.default_rng(seed)\n\n    test_cases = [\n        {'id': 1, 'dist': 'normal', 'params': {'a': 0.8, 'b': 0.6, 'c': 0.2}},\n        {'id': 2, 'dist': 'normal', 'params': {'a': 0.8, 'b': -0.6, 'c': 0.2}}, # Effective b is -0.6\n        {'id': 3, 'dist': 'uniform', 'params': {}},\n        {'id': 4, 'dist': 'normal', 'params': {}}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # --- Theoretical Calculations ---\n        if case['id'] == 1 or case['id'] == 2:\n            a = case['params']['a']\n            b_val = case['params']['b'] # Use b_val to distinguish from variable b in formulas\n            c = case['params']['c']\n            \n            # Var(X)\n            var_x_theory = np.exp(a**2) * (np.exp(a**2) - 1)\n            \n            # Var(Y) - depends on b^2, so same for case 1 and 2\n            b_sq = b_val**2\n            var_y_theory = np.exp(2 * c) * np.exp(b_sq) * (np.exp(b_sq) - 1)\n            \n            # Cov(X,Y)\n            cov_xy_theory = np.exp(c) * np.exp((a**2 + b_sq) / 2) * (np.exp(a * b_val) - 1)\n            \n            # rho(X,Y)\n            rho_theory = (np.exp(a * b_val) - 1) / np.sqrt((np.exp(a**2) - 1) * (np.exp(b_sq) - 1))\n            \n            # VRR\n            vrr_theory = (var_x_theory + var_y_theory) / (var_x_theory + var_y_theory - 2 * cov_xy_theory)\n\n        elif case['id'] == 3:\n            var_x_theory = 0.5\n            var_y_theory = 0.5\n            cov_xy_theory = 0.0\n            rho_theory = 0.0\n            vrr_theory = 1.0\n\n        elif case['id'] == 4:\n            var_x_theory = 3.0\n            var_y_theory = 3.0\n            cov_xy_theory = -1.0\n            rho_theory = -1.0 / 3.0\n            vrr_theory = 6.0 / 8.0\n\n        # --- Empirical (Simulation) Calculations ---\n        # Generate random numbers based on distribution\n        if case['dist'] == 'normal':\n            w_crn = rng.normal(size=n_samples)\n            w1_ind = rng.normal(size=n_samples)\n            w2_ind = rng.normal(size=n_samples)\n        elif case['dist'] == 'uniform':\n            w_crn = rng.uniform(size=n_samples)\n            w1_ind = rng.uniform(size=n_samples)\n            w2_ind = rng.uniform(size=n_samples)\n\n        # Define functions f1(w) and f2(w) per case\n        if case['id'] == 1:\n            a, b, c = case['params']['a'], case['params']['b'], case['params']['c']\n            f1 = lambda w: np.exp(a * w)\n            f2 = lambda w: np.exp(b * w + c)\n        elif case['id'] == 2:\n            a, b, c = case['params']['a'], 0.6, case['params']['c'] # b is positive 0.6 here\n            f1 = lambda w: np.exp(a * w)\n            f2 = lambda w: np.exp(-b * w + c)\n        elif case['id'] == 3:\n            f1 = lambda w: np.sin(2 * np.pi * w)\n            f2 = lambda w: np.cos(2 * np.pi * w)\n        elif case['id'] == 4:\n            f1 = lambda w: w + w**2\n            f2 = lambda w: w - w**2\n            \n        # CRN part\n        x_crn = f1(w_crn)\n        y_crn = f2(w_crn)\n        d_crn = x_crn - y_crn\n        var_d_crn_emp = np.var(d_crn, ddof=1)\n        \n        # Independent sampling part\n        x_ind = f1(w1_ind)\n        y_ind = f2(w2_ind)\n        d_ind = x_ind - y_ind\n        var_d_ind_emp = np.var(d_ind, ddof=1)\n        \n        # Empirical results\n        vrr_emp = var_d_ind_emp / var_d_crn_emp\n        rho_emp = np.corrcoef(x_crn, y_crn)[0, 1]\n        \n        results.append([vrr_emp, vrr_theory, rho_emp, rho_theory])\n\n    # Final print statement in the exact required format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "A common pitfall is to assume that CRN always reduces variance. This advanced exercise  challenges that assumption with a carefully constructed scenario where the effectiveness of CRN depends critically on a system parameter. By analyzing a system with discontinuous outputs, you will discover how the underlying covariance can flip from positive to negative, causing CRN to inflate variance instead of reducing it, underscoring the importance of analyzing the correlation structure of the specific problem.",
            "id": "3297431",
            "problem": "Consider a parametrized simulator that transforms a single Uniform random variable into a discrete output via a deterministic rule. Let $U$ be a single draw from a Uniform distribution on $[0,1)$, denoted $U \\sim \\mathrm{Uniform}(0,1)$. For a parameter value $\\theta$, the simulator outputs $X(\\theta,U)$ taking values in a finite set. In this problem, you will analyze the effect of using Common Random Numbers (CRN) on the variance of a difference estimator for indicator responses. You will construct a concrete example that exhibits a variance increase under CRN due to a discontinuity at a mass point in the output distribution.\n\nDefine three numerical levels $L=0.0$, $m=0.5$, and $H=1.0$ with $L  m  H$. Consider two parameter values $\\theta_1$ and $\\theta_2$ that map $U$ to $X(\\theta,U)$ as follows:\n- For $\\theta_1$: \n  - If $0 \\le U  0.5$, then $X(\\theta_1,U) = H$.\n  - If $0.5 \\le U  0.75$, then $X(\\theta_1,U) = m$.\n  - If $0.75 \\le U  1$, then $X(\\theta_1,U) = L$.\n- For $\\theta_2$:\n  - If $0 \\le U  0.4$, then $X(\\theta_2,U) = m$.\n  - If $0.4 \\le U  0.9$, then $X(\\theta_2,U) = H$.\n  - If $0.9 \\le U  1$, then $X(\\theta_2,U) = L$.\n\nFor a real threshold $c \\in \\mathbb{R}$, define the indicator response\n$$\nY_k(c,U) \\equiv \\mathbf{1}\\{X(\\theta_k,U) \\ge c\\}, \\quad k \\in \\{1,2\\}.\n$$\nWe are interested in the difference in expectations $\\Delta(c) \\equiv \\mathbb{E}[Y_1(c,U)] - \\mathbb{E}[Y_2(c,U)]$ and the Monte Carlo estimator of $\\Delta(c)$ based on $n$ independent replicates. Let $D(c,U) \\equiv Y_1(c,U) - Y_2(c,U)$ denote the per-replicate contribution, so that the sample mean difference has variance $\\mathrm{Var}(\\bar{D}(c)) = \\mathrm{Var}(D(c,U))/n$.\n\nTwo coupling regimes are considered for generating the pair $(Y_1(c,U),Y_2(c,U))$ per replicate:\n- Common Random Numbers (CRN): Use the same underlying $U$ for both $\\theta_1$ and $\\theta_2$.\n- Independent streams (IND): Use independent $U_1$ and $U_2$ for $\\theta_1$ and $\\theta_2$ respectively.\n\nStarting from the basic probability definitions for expectation, variance, and covariance, and using only the mapping above and the properties of the Uniform distribution on $[0,1)$, derive exact, closed-form expressions (as functions of $c$) for:\n- The success probabilities $p_1(c) \\equiv \\mathbb{P}\\{Y_1(c,U)=1\\}$ and $p_2(c) \\equiv \\mathbb{P}\\{Y_2(c,U)=1\\}$.\n- The overlap probability $p_{12}(c) \\equiv \\mathbb{P}\\{Y_1(c,U)=1 \\text{ and } Y_2(c,U)=1\\}$ under CRN.\n- The covariance $\\mathrm{Cov}_{\\mathrm{CRN}}(c) \\equiv \\mathrm{Cov}(Y_1(c,U),Y_2(c,U)) = p_{12}(c) - p_1(c)p_2(c)$.\n- The per-replicate variances\n  $$\n  V_{\\mathrm{CRN}}(c) \\equiv \\mathrm{Var}(Y_1(c,U) - Y_2(c,U)) \\quad \\text{under CRN},\n  $$\n  $$\n  V_{\\mathrm{IND}}(c) \\equiv \\mathrm{Var}(Y_1(c,U) - Y_2(c,U)) \\quad \\text{under IND}.\n  $$\nExplain from first principles why $V_{\\mathrm{CRN}}(c) - V_{\\mathrm{IND}}(c) = -2\\,\\mathrm{Cov}_{\\mathrm{CRN}}(c)$, and analyze how the discontinuity at the mass point $m$ causes a sign flip of $\\mathrm{Cov}_{\\mathrm{CRN}}(c)$ as $c$ crosses $m$.\n\nYour program must compute the exact values of $V_{\\mathrm{CRN}}(c) - V_{\\mathrm{IND}}(c)$ using the above construction, for the following test suite of thresholds:\n- $c_1 = 0.3$,\n- $c_2 = 0.5$,\n- $c_3 = 0.7$,\n- $c_4 = 1.0$,\n- $c_5 = 1.1$,\n- $c_6 = -0.1$.\n\nDesign for coverage:\n- The cases $c_1$ and $c_2$ probe behavior below and at the mass point $m$.\n- The cases $c_3$ and $c_4$ probe behavior strictly above $m$, including at $H$.\n- The cases $c_5$ and $c_6$ are boundary/extreme conditions outside the support.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3,r4,r5,r6]\"), where each $r_i$ equals the exact float value of $V_{\\mathrm{CRN}}(c_i) - V_{\\mathrm{IND}}(c_i)$ in the order given. No physical units are involved. Angles are not used. Percentages must be expressed as decimals if they appear, but they are not needed here. The program must not perform Monte Carlo simulation; it must compute exact values from the definitions and the mapping provided.",
            "solution": "The problem is deemed valid as it is scientifically grounded, well-posed, objective, and internally consistent. It presents a solvable challenge in the field of stochastic simulation that can be addressed using fundamental principles of probability theory.\n\nThe objective is to analyze the variance of a difference estimator under two sampling strategies: Common Random Numbers (CRN) and Independent streams (IND). We are asked to compute the exact value of $V_{\\mathrm{CRN}}(c) - V_{\\mathrm{IND}}(c)$ for a specified set of thresholds $c$.\n\nLet $U \\sim \\mathrm{Uniform}(0,1)$. The outputs of the simulators are $X(\\theta_1, U)$ and $X(\\theta_2, U)$, which take values in the set $\\{L, m, H\\}$, where $L=0.0$, $m=0.5$, and $H=1.0$. The indicator responses are $Y_k(c,U) = \\mathbf{1}\\{X(\\theta_k, U) \\ge c\\}$ for $k \\in \\{1,2\\}$. These are Bernoulli random variables.\n\nFirst, we establish the relationship between the variance difference and the covariance under CRN. The variance of a difference of two random variables $A$ and $B$ is given by $\\mathrm{Var}(A - B) = \\mathrm{Var}(A) + \\mathrm{Var}(B) - 2\\mathrm{Cov}(A,B)$.\n\nUnder the CRN regime, the same random number $U$ is used to generate both $Y_1(c,U)$ and $Y_2(c,U)$. The variance of their difference is:\n$$V_{\\mathrm{CRN}}(c) = \\mathrm{Var}(Y_1(c,U) - Y_2(c,U)) = \\mathrm{Var}(Y_1(c,U)) + \\mathrm{Var}(Y_2(c,U)) - 2\\mathrm{Cov}(Y_1(c,U), Y_2(c,U))$$\n\nUnder the IND regime, independent random numbers $U_1$ and $U_2$ are used. Let $Y_1' = Y_1(c,U_1)$ and $Y_2' = Y_2(c,U_2)$. Since $U_1$ and $U_2$ are independent, $Y_1'$ and $Y_2'$ are also independent, which implies their covariance is zero: $\\mathrm{Cov}(Y_1', Y_2') = 0$. The variance of the difference is:\n$$V_{\\mathrm{IND}}(c) = \\mathrm{Var}(Y_1' - Y_2') = \\mathrm{Var}(Y_1') + \\mathrm{Var}(Y_2')$$\nThe marginal distributions of $Y_1$ and $Y_1'$ are identical, as are those of $Y_2$ and $Y_2'$. Thus, $\\mathrm{Var}(Y_1(c,U)) = \\mathrm{Var}(Y_1')$ and $\\mathrm{Var}(Y_2(c,U)) = \\mathrm{Var}(Y_2')$.\nSubtracting $V_{\\mathrm{IND}}(c)$ from $V_{\\mathrm{CRN}}(c)$ yields:\n$$V_{\\mathrm{CRN}}(c) - V_{\\mathrm{IND}}(c) = -2\\mathrm{Cov}(Y_1(c,U), Y_2(c,U)) = -2\\mathrm{Cov}_{\\mathrm{CRN}}(c)$$\nThis relationship demonstrates that the change in variance due to CRN is entirely determined by the covariance between the two outputs. A positive covariance leads to variance reduction, while a negative covariance leads to variance increase.\n\nTo compute $\\mathrm{Cov}_{\\mathrm{CRN}}(c)$, we use the formula $\\mathrm{Cov}_{\\mathrm{CRN}}(c) = \\mathbb{E}[Y_1 Y_2] - \\mathbb{E}[Y_1]\\mathbb{E}[Y_2]$. Since $Y_k$ are indicator variables, we have $\\mathbb{E}[Y_k] = \\mathbb{P}\\{Y_k=1\\} = p_k(c)$ and $\\mathbb{E}[Y_1 Y_2] = \\mathbb{P}\\{Y_1=1 \\text{ and } Y_2=1\\} = p_{12}(c)$.\nTherefore, $\\mathrm{Cov}_{\\mathrm{CRN}}(c) = p_{12}(c) - p_1(c)p_2(c)$.\n\nWe now derive the probabilities $p_1(c)$, $p_2(c)$, and $p_{12}(c)$ as piecewise functions of the threshold $c$.\n\nLet's denote the sets of $U$ values corresponding to each output level:\nFor $\\theta_1$: $S_{1,H} = [0, 0.5)$, $S_{1,m} = [0.5, 0.75)$, $S_{1,L} = [0.75, 1)$.\nFor $\\theta_2$: $S_{2,m} = [0, 0.4)$, $S_{2,H} = [0.4, 0.9)$, $S_{2,L} = [0.9, 1)$.\n\nThe success probability $p_1(c) = \\mathbb{P}\\{X(\\theta_1, U) \\ge c\\}$ is the measure of the set of $U$ where the condition holds.\n- If $c  H=1.0$: The condition is never met. $p_1(c)=0$.\n- If $m  c \\le H$ ($0.5  c \\le 1.0$): Condition met if $X(\\theta_1,U)=H$. This is for $U \\in S_{1,H}$. The measure is $0.5$. So $p_1(c)=0.5$.\n- If $L  c \\le m$ ($0  c \\le 0.5$): Condition met if $X(\\theta_1,U) \\in \\{m,H\\}$. This is for $U \\in S_{1,H} \\cup S_{1,m} = [0, 0.75)$. The measure is $0.75$. So $p_1(c)=0.75$.\n- If $c \\le L=0.0$: Condition always met. The measure is $1.0$. So $p_1(c)=1.0$.\n\nThe success probability $p_2(c) = \\mathbb{P}\\{X(\\theta_2, U) \\ge c\\}$ is calculated similarly.\n- If $c  H=1.0$: $p_2(c)=0$.\n- If $m  c \\le H$ ($0.5  c \\le 1.0$): Condition met if $X(\\theta_2,U)=H$. This is for $U \\in S_{2,H} = [0.4, 0.9)$. The measure is $0.5$. So $p_2(c)=0.5$.\n- If $L  c \\le m$ ($0  c \\le 0.5$): Condition met if $X(\\theta_2,U) \\in \\{m,H\\}$. This is for $U \\in S_{2,m} \\cup S_{2,H} = [0, 0.9)$. The measure is $0.9$. So $p_2(c)=0.9$.\n- If $c \\le L=0.0$: $p_2(c)=1.0$.\n\nThe overlap probability $p_{12}(c) = \\mathbb{P}\\{X(\\theta_1, U) \\ge c \\text{ and } X(\\theta_2, U) \\ge c\\}$ is the measure of the intersection of the corresponding sets of $U$.\n- If $c  H=1.0$: The intersection is empty. $p_{12}(c)=0$.\n- If $m  c \\le H$ ($0.5  c \\le 1.0$): The sets are $S_{1,H}$ and $S_{2,H}$. The intersection is $[0, 0.5) \\cap [0.4, 0.9) = [0.4, 0.5)$. The measure is $0.1$. So $p_{12}(c)=0.1$.\n- If $L  c \\le m$ ($0  c \\le 0.5$): The sets are $S_{1,H} \\cup S_{1,m}$ and $S_{2,m} \\cup S_{2,H}$. The intersection is $[0, 0.75) \\cap [0, 0.9) = [0, 0.75)$. The measure is $0.75$. So $p_{12}(c)=0.75$.\n- If $c \\le L=0.0$: The intersection is $[0,1)$. The measure is $1.0$. So $p_{12}(c)=1.0$.\n\nNow we compute $\\mathrm{Cov}_{\\mathrm{CRN}}(c) = p_{12}(c) - p_1(c)p_2(c)$.\n- If $c  1.0$: $\\mathrm{Cov}_{\\mathrm{CRN}}(c) = 0 - (0)(0) = 0$.\n- If $0.5  c \\le 1.0$: $\\mathrm{Cov}_{\\mathrm{CRN}}(c) = 0.1 - (0.5)(0.5) = 0.1 - 0.25 = -0.15$.\n- If $0  c \\le 0.5$: $\\mathrm{Cov}_{\\mathrm{CRN}}(c) = 0.75 - (0.75)(0.9) = 0.75 - 0.675 = 0.075$.\n- If $c \\le 0.0$: $\\mathrm{Cov}_{\\mathrm{CRN}}(c) = 1.0 - (1.0)(1.0) = 0$.\n\nThe covariance exhibits a sharp discontinuity and sign flip as the threshold $c$ crosses the mass point $m=0.5$.\nFor $c \\le m=0.5$ (but $cL=0.0$), the events $\\{X(\\theta_1,U)\\ge c\\}$ and $\\{X(\\theta_2,U)\\ge c\\}$ are positively correlated. The sets of $U$ that satisfy these conditions, $[0, 0.75)$ and $[0, 0.9)$, have a large overlap. This positive correlation, $\\mathrm{Cov}_{\\mathrm{CRN}}(c) = 0.075$, means CRN reduces variance.\nFor $c  m=0.5$, the events become $\\{X(\\theta_1,U)=H\\}$ and $\\{X(\\theta_2,U)=H\\}$. The corresponding sets of $U$, $[0, 0.5)$ and $[0.4, 0.9)$, are now substantially misaligned. This misalignment leads to an overlap probability $p_{12}(c) = 0.1$ that is smaller than the product of the marginal probabilities $p_1(c)p_2(c)=0.25$, resulting in a negative correlation, $\\mathrm{Cov}_{\\mathrm{CRN}}(c) = -0.15$. In this regime, CRN increases variance.\nThis sign flip is a direct consequence of the non-monotonic relationship between $X(\\theta_1,U)$ and $X(\\theta_2,U)$ over the domain of $U$, and how the indicator function threshold $c$ interacts with the discontinuous structure of the outputs.\n\nFinally, we calculate the variance difference, $V_{\\mathrm{CRN}}(c) - V_{\\mathrm{IND}}(c) = -2\\mathrm{Cov}_{\\mathrm{CRN}}(c)$.\n- For $c_1=0.3$: $(0  c \\le 0.5)$, diff = $-2(0.075) = -0.15$.\n- For $c_2=0.5$: $(0  c \\le 0.5)$, diff = $-2(0.075) = -0.15$.\n- For $c_3=0.7$: $(0.5  c \\le 1.0)$, diff = $-2(-0.15) = 0.3$.\n- For $c_4=1.0$: $(0.5  c \\le 1.0)$, diff = $-2(-0.15) = 0.3$.\n- For $c_5=1.1$: $(c  1.0)$, diff = $-2(0) = 0.0$.\n- For $c_6=-0.1$: $(c \\le 0.0)$, diff = $-2(0) = 0.0$.\nThese values will be computed by the program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are needed for this exact computation.\n\ndef solve():\n    \"\"\"\n    Computes the exact variance difference V_CRN(c) - V_IND(c) for a suite of thresholds c.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.3,),\n        (0.5,),\n        (0.7,),\n        (1.0,),\n        (1.1,),\n        (-0.1,),\n    ]\n\n    results = []\n    # Define the numerical levels from the problem statement.\n    L, m, H = 0.0, 0.5, 1.0\n\n    for case in test_cases:\n        c = case[0]\n        \n        # Calculate p1(c) = P(X(theta_1, U) = c)\n        # The mapping for theta_1 is:\n        # [0, 0.5) - H=1.0\n        # [0.5, 0.75) - m=0.5\n        # [0.75, 1) - L=0.0\n        if c  H:\n            p1_c = 0.0\n        elif c  m:  # c is in (0.5, 1.0]\n            # X = c only if X=H. P(X=H) = measure of [0, 0.5) = 0.5\n            p1_c = 0.5\n        elif c  L:  # c is in (0.0, 0.5]\n            # X = c if X=H or X=m. P(X=H or X=m) = measure of [0, 0.75) = 0.75\n            p1_c = 0.75\n        else: # c = 0.0\n            # X = c always true since all outputs are non-negative.\n            p1_c = 1.0\n\n        # Calculate p2(c) = P(X(theta_2, U) = c)\n        # The mapping for theta_2 is:\n        # [0, 0.4) - m=0.5\n        # [0.4, 0.9) - H=1.0\n        # [0.9, 1) - L=0.0\n        if c  H:\n            p2_c = 0.0\n        elif c  m:  # c is in (0.5, 1.0]\n            # X = c only if X=H. P(X=H) = measure of [0.4, 0.9) = 0.5\n            p2_c = 0.5\n        elif c  L:  # c is in (0.0, 0.5]\n            # X = c if X=H or X=m. P(X=H or X=m) = measure of [0, 0.9) = 0.9\n            p2_c = 0.9\n        else: # c = 0.0\n            p2_c = 1.0\n\n        # Calculate p12(c) = P(X1 = c and X2 = c)\n        if c  H:\n            p12_c = 0.0\n        elif c  m:  # c is in (0.5, 1.0]\n            # Need X1=H and X2=H. Intersection of [0, 0.5) and [0.4, 0.9) is [0.4, 0.5).\n            # Measure = 0.1\n            p12_c = 0.1\n        elif c  L:  # c is in (0.0, 0.5]\n            # Need (X1=H or X1=m) and (X2=H or X2=m).\n            # Intersection of [0, 0.75) and [0, 0.9) is [0, 0.75).\n            # Measure = 0.75\n            p12_c = 0.75\n        else: # c = 0.0\n            p12_c = 1.0\n        \n        # Calculate the covariance under CRN\n        cov_crn_c = p12_c - p1_c * p2_c\n        \n        # The variance difference is -2 * covariance\n        variance_diff = -2.0 * cov_crn_c\n        \n        results.append(variance_diff)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}