{
    "hands_on_practices": [
        {
            "introduction": "后层化估计量的方差是评估其统计精度的核心。本练习将理论推导与计算机模拟相结合，旨在加深您对后层化估计量方差的理解。您将首先在给定各层样本量的条件下，为有限总体中的后层化均值估计量推导出其精确方差的解析表达式，然后通过蒙特卡洛模拟来验证一个“代入式”方差估计量的准确性。这个过程将巩固您对抽样理论基本原理的掌握，并提升您通过模拟来评估统计方法性能的实践能力。",
            "id": "3330424",
            "problem": "考虑一个有限总体，该总体被划分为 $H$ 个事后层，索引为 $h \\in \\{1,\\dots,H\\}$。层 $h$ 包含 $N_h$ 个单元，其值为 $\\{y_{h,i}\\}_{i=1}^{N_h}$。从大小为 $N=\\sum_{h=1}^H N_h$ 的完整总体中，抽取一个大小为 $n$ 的单阶段无放回简单随机样本（SRSWOR）。抽样后，观测到的层样本量为 $\\{n_h\\}_{h=1}^H$，且 $\\sum_{h=1}^H n_h = n$。定义总体事后分层权重 $W_h = N_h/N$。总体均值的事后分层估计量为 $\\hat{\\mu}_{PS} = \\sum_{h=1}^H W_h \\,\\bar{Y}_{h}$，其中 $\\bar{Y}_{h}$ 是从层 $h$ 中观测到的单元的样本均值。在已实现的 $\\{n_h\\}$ 的条件下进行分析。\n\n任务 A（推导）：从有限总体中无放回简单随机抽样（SRSWOR）的基本定义和全方差定律出发，推导精确的条件方差 $\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\})$，用层大小 $\\{N_h\\}$、已实现的层样本量 $\\{n_h\\}$、有限总体事后分层权重 $\\{W_h\\}$ 和有限总体层内方差来表示。您必须从一个经过充分检验的事实开始：在大小为 $N_h$ 的单个有限层内进行无放回简单随机抽样时，对于从 $n_h$ 个单元计算出的样本均值，其方差等于有限总体校正与按 $1/n_h$ 缩放的层内方差的乘积。不要预先假设或陈述最终表达式；请从第一性原理出发进行推导。\n\n任务 B（通过随机模拟进行评估）：当实践中层内有限总体方差未知时，一种自然的插入式估计量是用从该层观测到的单元计算出的相应无偏样本方差来替换每个未知的层内方差。通过蒙特卡罗（随机）模拟，在已实现的 $\\{n_h\\}$ 条件下，通过将其平均值与任务 A 中推导出的精确条件方差进行比较，来评估此插入式估计量的准确性。\n\n对于下文测试套件中的每个测试用例，请精确实现以下内容。\n\n1. 确定性总体构建。对于每个层 $h \\in \\{1,\\dots,H\\}$ 和每个单元索引 $i \\in \\{1,\\dots,N_h\\}$，根据测试套件中给出的特定于用例的确定性公式生成有限总体值 $y_{h,i}$。这确保了总体是固定且已知的。\n\n2. 精确条件方差。令 $S_h^2$ 表示层 $h$ 内的有限总体方差，其分母为 $N_h-1$，即 $S_h^2 = \\frac{1}{N_h-1}\\sum_{i=1}^{N_h}\\left(y_{h,i}-\\mu_h\\right)^2$，其中 $\\mu_h = \\frac{1}{N_h}\\sum_{i=1}^{N_h} y_{h,i}$。使用您在任务 A 中的推导，计算在给定已实现的 $\\{n_h\\}$ 的情况下 $\\hat{\\mu}_{PS}$ 的精确条件方差 $V_{\\text{true}}$。\n\n3. 抽样分布的蒙特卡罗近似。在已实现的 $\\{n_h\\}$ 条件下，模拟以下实验的 $R$ 次独立重复：对于每个层 $h$，从该层的 $N_h$ 个单元中抽取一个大小为 $n_h$ 的无放回简单随机样本；计算层样本均值 $\\bar{Y}_h$ 和无偏层样本方差 $s_h^2$（当 $n_h \\ge 2$ 时，分母为 $n_h-1$）。当 $n_h = 1$ 时，按惯例设置 $s_h^2 = 0$。对于每次重复 $r \\in \\{1,\\dots,R\\}$，计算：\n   - 事后分层估计量 $\\hat{\\mu}_{PS}^{(r)} = \\sum_{h=1}^H W_h \\bar{Y}_h^{(r)}$，\n   - 插入式方差估计量 $\\hat{V}^{(r)} = \\sum_{h=1}^H W_h^2 \\left(1 - \\frac{n_h}{N_h}\\right)\\frac{s_h^{2\\,(r)}}{n_h}$。\n   最后，计算蒙特卡罗方差 $V_{\\text{mc}}$，即 $\\{\\hat{\\mu}_{PS}^{(r)}\\}_{r=1}^R$ 的经验方差（分母为 $R-1$），并计算蒙特卡罗平均插入式方差 $\\overline{V}_{\\text{plug}} = \\frac{1}{R}\\sum_{r=1}^R \\hat{V}^{(r)}$。\n\n4. 准确性指标。为每个测试用例报告以下浮点数：\n   - $V_{\\text{true}}$；\n   - 蒙特卡罗方差的绝对相对误差， $|V_{\\text{mc}} - V_{\\text{true}}|/V_{\\text{true}}$；\n   - 插入式估计量的相对偏差， $(\\overline{V}_{\\text{plug}} - V_{\\text{true}})/V_{\\text{true}}$。\n   所有比率必须以小数形式报告（而非百分比）。\n\n实现细节：\n- 随机性必须是可复现的。按照每个测试用例的指定，使用固定的伪随机种子。\n- 所有层内抽样必须是无放回的，并且在所有组合上是均匀的。\n- 所有计算都是无量纲的；不涉及物理单位。\n\n测试套件：\n为以下三个测试用例提供结果。对每个用例，使用指定的重复次数 $R$ 和种子。\n\n- 用例 1：\n  - $H = 3$，\n  - $(N_1,N_2,N_3) = (30,40,50)$，\n  - $(n_1,n_2,n_3) = (5,8,10)$，\n  - 总体生成器：对于每个 $h \\in \\{1,2,3\\}$ 和 $i \\in \\{1,\\dots,N_h\\}$，\n    $y_{h,i} = \\alpha_h + \\beta_h \\, i + \\gamma_h \\, (-1)^i$，其中\n    $(\\alpha_1,\\alpha_2,\\alpha_3) = (10,20,30)$，\n    $(\\beta_1,\\beta_2,\\beta_3) = (0.5,1.0,1.5)$，\n    $(\\gamma_1,\\gamma_2,\\gamma_3) = (3,2,1)$。\n  - 重复次数 $R = 20000$，\n  - 种子 $= 202401$。\n\n- 用例 2（边界情况，一个层的样本量 $n_h=1$）：\n  - $H = 3$，\n  - $(N_1,N_2,N_3) = (20,25,35)$，\n  - $(n_1,n_2,n_3) = (1,4,7)$，\n  - 总体生成器：对于每个 $h \\in \\{1,2,3\\}$ 和 $i \\in \\{1,\\dots,N_h\\}$，\n    $y_{h,i} = \\alpha_h + \\beta_h \\, \\sqrt{i} + \\gamma_h \\, \\left((i \\bmod 3) - 1\\right)$，其中\n    $(\\alpha_1,\\alpha_2,\\alpha_3) = (5,8,12)$，\n    $(\\beta_1,\\beta_2,\\beta_3) = (2.0,1.0,0.5)$，\n    $(\\gamma_1,\\gamma_2,\\gamma_3) = (1,3,2)$。\n  - 重复次数 $R = 20000$，\n  - 种子 $= 202402$。\n\n- 用例 3（边界情况，一个层进行完全枚举以强调有限总体校正）：\n  - $H = 2$，\n  - $(N_1,N_2) = (12,18)$，\n  - $(n_1,n_2) = (12,9)$，\n  - 总体生成器：对于每个 $h \\in \\{1,2\\}$ 和 $i \\in \\{1,\\dots,N_h\\}$，\n    $y_{h,i} = \\alpha_h + \\beta_h \\, i + \\gamma_h \\, \\frac{i^2}{N_h}$，其中\n    $(\\alpha_1,\\alpha_2) = (0,4)$，\n    $(\\beta_1,\\beta_2) = (1.0,0.5)$，\n    $(\\gamma_1,\\gamma_2) = (0.2,0.1)$。\n  - 重复次数 $R = 20000$，\n  - 种子 $= 202403$。\n\n最终输出格式：\n您的程序应生成单行输出，包含一个类 JSON 格式的列表表示，该列表包含三个元素（每个测试用例一个），其中每个元素是按上述第 $4$ 项指定顺序排列的三个浮点数的列表，即，\n$[ [V_{\\text{true}}, |V_{\\text{mc}}-V_{\\text{true}}|/V_{\\text{true}}, (\\overline{V}_{\\text{plug}}-V_{\\text{true}})/V_{\\text{true}}], \\dots ]$。\n例如，一个有效的形状是 $[[x_{11},x_{12},x_{13}],[x_{21},x_{22},x_{23}],[x_{31},x_{32},x_{33}]]$，不含任何额外文本。",
            "solution": "用户提供的问题经评估为有效。该问题在科学上基于抽样调查和蒙特卡罗模拟的原理，问题阐述清晰，目标明确，数据充分，并且没有问题验证协议中列出的任何使其无效的缺陷。因此，我们可以继续提供完整解决方案。\n\n该问题分为两部分：任务 A，对事后分层估计量的条件方差进行理论推导；任务 B，通过随机模拟对一个插入式方差估计量进行数值评估。\n\n### 任务 A：条件方差的推导\n\n我们被要求推导精确的条件方差 $\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\})$。总体均值的事后分层估计量由下式给出：\n$$\n\\hat{\\mu}_{PS} = \\sum_{h=1}^H W_h \\bar{Y}_{h}\n$$\n其中 $W_h = N_h/N$ 是已知的总体事后分层权重，而 $\\bar{Y}_h$ 是在层 $h$ 中观测到的 $n_h$ 个单元的样本均值。\n\n推导过程以已实现的层样本量 $\\{n_h\\}_{h=1}^H$ 为条件。这意味着在整个推导过程中，我们将 $n_h$ 值视为固定常数。随机性来自于在给定每层样本量的情况下，从该层中选择具体单元的过程。\n\n随机变量和的方差涉及协方差项。将方差算子应用于 $\\hat{\\mu}_{PS}$：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\}) = \\operatorname{Var}\\left(\\sum_{h=1}^H W_h \\bar{Y}_h \\mid \\{n_h\\}\\right)\n$$\n使用加权和的方差通用公式 $\\operatorname{Var}(\\sum_{i} a_i X_i) = \\sum_{i} a_i^2 \\operatorname{Var}(X_i) + \\sum_{i \\neq j} a_i a_j \\operatorname{Cov}(X_i, X_j)$，我们得到：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\}) = \\sum_{h=1}^H W_h^2 \\operatorname{Var}(\\bar{Y}_h \\mid n_h) + \\sum_{h \\neq k} W_h W_k \\operatorname{Cov}(\\bar{Y}_h, \\bar{Y}_k \\mid \\{n_h\\})\n$$\n问题陈述，从总总体中抽取一个大小为 $n$ 的单阶段无放回简单随机样本（SRSWOR）。然而，为了分析事后分层估计量，标准的理论方法（以及以 $\\{n_h\\}$ 为条件所隐含的方法）是将抽样过程在概念上视为等同于从每个层 $h$ 中独立地进行大小为 $n_h$ 的无放回简单随机抽样。在此条件框架下，一个层中单元的选择独立于另一层中的选择。因此，对于 $h \\ne k$，样本均值 $\\bar{Y}_h$ 和 $\\bar{Y}_k$ 是条件独立的。这意味着它们的条件协方差为零：\n$$\n\\operatorname{Cov}(\\bar{Y}_h, \\bar{Y}_k \\mid \\{n_h\\}) = 0 \\quad \\text{for } h \\neq k\n$$\n因此，方差表达式得以简化，因为协方差项之和为零：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\}) = \\sum_{h=1}^H W_h^2 \\operatorname{Var}(\\bar{Y}_h \\mid n_h)\n$$\n现在，我们必须找到 $\\operatorname{Var}(\\bar{Y}_h \\mid n_h)$ 的表达式。问题指示我们使用一个经过充分检验的事实，即在大小为 $N_h$ 的单个有限总体层内，从一个大小为 $n_h$ 的无放回简单随机样本中得到的样本均值的方差由下式给出：\n$$\n\\operatorname{Var}(\\bar{Y}_h \\mid n_h) = \\left(1 - \\frac{n_h}{N_h}\\right) \\frac{S_h^2}{n_h}\n$$\n其中：\n-   $\\left(1 - \\frac{n_h}{N_h}\\right)$ 是有限总体校正（FPC），它解释了抽样是从有限总体中无放回地进行的这一事实。当 $n_h$ 接近 $N_h$ 时，关于均值的不确定性减小，当 $n_h = N_h$ 时完全消失。\n-   $S_h^2$ 是层 $h$ 中值 $\\{y_{h,i}\\}_{i=1}^{N_h}$ 的有限总体方差，其定义中使用分母 $N_h-1$ 以与其在抽样理论中的作用保持一致：\n    $$\n    S_h^2 = \\frac{1}{N_h-1}\\sum_{i=1}^{N_h}\\left(y_{h,i}-\\mu_h\\right)^2\n    $$\n    其中 $\\mu_h$ 是层 $h$ 的真实均值。\n\n将 $\\operatorname{Var}(\\bar{Y}_h \\mid n_h)$ 的表达式代回到我们的求和式中，我们得到了事后分层估计量精确条件方差的最终推导表达式：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\}) = \\sum_{h=1}^H W_h^2 \\left(1 - \\frac{n_h}{N_h}\\right) \\frac{S_h^2}{n_h}\n$$\n这就是我们将表示为 $V_{\\text{true}}$ 的表达式。它是总体层大小 $\\{N_h\\}$、层权重 $\\{W_h\\}$、已实现的样本量 $\\{n_h\\}$ 以及（通常未知的）真实层内总体方差 $\\{S_h^2\\}$ 的函数。\n\n### 任务 B：模拟设计\n\n该模拟评估了“插入式”方差估计量 $\\hat{V}$ 的性质，该估计量是通过将未知总体方差 $S_h^2$ 的估计值代入 $V_{\\text{true}}$ 的公式中形成的。基于大小为 $n_h$ 的样本的 $S_h^2$ 的标准无偏估计量是样本方差 $s_h^2$：\n$$\ns_h^2 = \\frac{1}{n_h-1}\\sum_{j=1}^{n_h}(y_{h,j}^{\\text{sample}} - \\bar{Y}_h)^2 \\quad \\text{for } n_h \\ge 2\n$$\n问题规定，按惯例，如果 $n_h=1$，则 $s_h^2=0$。因此，方差的插入式估计量为：\n$$\n\\hat{V} = \\sum_{h=1}^H W_h^2 \\left(1 - \\frac{n_h}{N_h}\\right)\\frac{s_h^2}{n_h}\n$$\n对每个测试用例，模拟按以下步骤进行：\n\n1.  **总体构建**：根据测试用例中提供的确定性函数 $y_{h,i} = f(h,i)$，为每个层 $h$ 构建一个不可变的有限总体。这允许精确计算真实的总体参数。\n\n2.  **真实方差计算 ($V_{\\text{true}}$)**：对于每个层 $h$，从生成的总体值中计算出真实的有限总体方差 $S_h^2$。将这些值与给定的 $N_h$ 和 $n_h$ 一起，用于任务 A 中推导的公式，以计算精确的条件方差 $V_{\\text{true}}$。\n\n3.  **蒙特卡罗模拟**：生成大量的重复 $R$，以近似抽样分布。对于每次重复 $r \\in \\{1,\\dots,R\\}$：\n    a.  对于每个 $h \\in \\{1,\\dots,H\\}$，从层 $h$ 的总体中通过无放回简单随机抽样抽取一个大小为 $n_h$ 的新样本。\n    b.  从抽取的样本中计算每个层的样本均值 $\\bar{Y}_h^{(r)}$ 和样本方差 $s_h^{2,(r)}$。关键是，对于任何 $n_h=1$ 的层，都应用规则 $s_h^2=0$。\n    c.  计算并存储事后分层估计量 $\\hat{\\mu}_{PS}^{(r)} = \\sum_{h} W_h \\bar{Y}_h^{(r)}$ 及其对应的插入式方差估计量 $\\hat{V}^{(r)} = \\sum_{h} W_h^2 (1 - n_h/N_h) s_h^{2,(r)}/n_h$。\n\n4.  **指标计算**：在所有 $R$ 次重复完成后：\n    a.  蒙特卡罗方差 $V_{\\text{mc}}$ 计算为 $R$ 个存储的 $\\hat{\\mu}_{PS}^{(r)}$ 值的样本方差（分母为 $R-1$）。这可作为 $V_{\\text{true}}$ 的基于模拟的估计。\n    b.  平均插入式方差 $\\overline{V}_{\\text{plug}}$ 计算为 $R$ 个存储的 $\\hat{V}^{(r)}$ 值的算术平均值。这近似于插入式估计量的期望值 $E[\\hat{V} \\mid \\{n_h\\}]$。\n    c.  计算指定的准确性指标：\n        -   $V_{\\text{true}}$\n        -   $|V_{\\text{mc}} - V_{\\text{true}}|/V_{\\text{true}}$：这衡量了蒙特卡罗模拟本身收敛到真实方差的程度。当 $R$ 很大时，这个值应该很小。\n        -   $(\\overline{V}_{\\text{plug}} - V_{\\text{true}})/V_{\\text{true}}$：这衡量了插入式方差估计量的相对偏差。理论预测，当且仅当所有 $n_h \\ge 2$ 时，该估计量是无偏的（$E[\\hat{V}] = V_{\\text{true}}$）。如果任何 $n_h=1$，则该估计量预期是有偏的，因为 $E[s_h^2 | n_h=1] = 0 \\neq S_h^2$。\n\n该实现将使用固定的随机种子以保证可复现性，并将对提供的三个测试用例中的每一个执行这些步骤。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the post-stratification problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"H\": 3,\n            \"N_h\": (30, 40, 50),\n            \"n_h\": (5, 8, 10),\n            \"pop_gen_params\": {\n                \"alpha\": (10, 20, 30),\n                \"beta\": (0.5, 1.0, 1.5),\n                \"gamma\": (3, 2, 1)\n            },\n            \"pop_gen_func\": lambda h, i, p: p[\"alpha\"][h-1] + p[\"beta\"][h-1] * i + p[\"gamma\"][h-1] * (-1)**i,\n            \"R\": 20000,\n            \"seed\": 202401\n        },\n        {\n            \"H\": 3,\n            \"N_h\": (20, 25, 35),\n            \"n_h\": (1, 4, 7),\n            \"pop_gen_params\": {\n                \"alpha\": (5, 8, 12),\n                \"beta\": (2.0, 1.0, 0.5),\n                \"gamma\": (1, 3, 2)\n            },\n            \"pop_gen_func\": lambda h, i, p: p[\"alpha\"][h-1] + p[\"beta\"][h-1] * np.sqrt(i) + p[\"gamma\"][h-1] * ((i % 3) - 1),\n            \"R\": 20000,\n            \"seed\": 202402\n        },\n        {\n            \"H\": 2,\n            \"N_h\": (12, 18),\n            \"n_h\": (12, 9),\n            \"pop_gen_params\": {\n                \"alpha\": (0, 4),\n                \"beta\": (1.0, 0.5),\n                \"gamma\": (0.2, 0.1)\n            },\n            \"pop_gen_func\": lambda h, i, p, N_h: p[\"alpha\"][h-1] + p[\"beta\"][h-1] * i + p[\"gamma\"][h-1] * i**2 / N_h,\n            \"R\": 20000,\n            \"seed\": 202403\n        }\n    ]\n\n    results_all_cases = []\n\n    for case_idx, case in enumerate(test_cases):\n        H = case[\"H\"]\n        N_h_tuple = case[\"N_h\"]\n        n_h_tuple = case[\"n_h\"]\n        pop_gen_params = case[\"pop_gen_params\"]\n        pop_gen_func = case[\"pop_gen_func\"]\n        R = case[\"R\"]\n        seed = case[\"seed\"]\n\n        rng = np.random.default_rng(seed)\n\n        # 1. Deterministic population construction\n        populations = []\n        for h_idx in range(H):\n            h = h_idx + 1\n            N_h_val = N_h_tuple[h_idx]\n            indices = np.arange(1, N_h_val + 1)\n            if case_idx == 2: # Case 3 has a different generator signature\n                y_values = pop_gen_func(h, indices, pop_gen_params, N_h_val)\n            else:\n                y_values = pop_gen_func(h, indices, pop_gen_params)\n            populations.append(y_values)\n        \n        # 2. Exact conditional variance\n        N_h_arr = np.array(N_h_tuple, dtype=float)\n        n_h_arr = np.array(n_h_tuple, dtype=float)\n        \n        N_total = np.sum(N_h_arr)\n        W_h_arr = N_h_arr / N_total\n        \n        # Calculate true population variances S_h^2\n        S_h_sq_arr = np.array([np.var(pop, ddof=1) if len(pop) > 1 else 0 for pop in populations])\n        \n        # Finite Population Correction\n        fpc_arr = 1.0 - n_h_arr / N_h_arr\n        \n        # Calculate V_true\n        # Add a small epsilon to denominator to avoid division by zero warning if n_h=0, though not in test cases\n        \n        # If n_h_arr[h] is zero, the term is undefined. Here n_h >= 1.\n        # If S_h_sq_arr[h] is zero, the term is zero.\n        # If fpc_arr[h] is zero (n_h=N_h), the term is zero.\n        # The logic below handles these cases correctly.\n        V_true_terms = np.zeros_like(W_h_arr, dtype=float)\n        valid_n_h_mask = n_h_arr > 0\n        V_true_terms[valid_n_h_mask] = (W_h_arr[valid_n_h_mask]**2 * fpc_arr[valid_n_h_mask] * S_h_sq_arr[valid_n_h_mask]) / n_h_arr[valid_n_h_mask]\n        V_true = np.sum(V_true_terms)\n\n        # 3. Monte Carlo approximation\n        mu_ps_reps = np.zeros(R)\n        V_hat_reps = np.zeros(R)\n\n        for r in range(R):\n            mu_ps_r = 0.0\n            V_hat_r = 0.0\n            for h_idx in range(H):\n                # Draw SRSWOR sample for stratum h\n                sample = rng.choice(populations[h_idx], size=int(n_h_arr[h_idx]), replace=False)\n                \n                # Compute sample mean\n                y_bar_h = np.mean(sample)\n                \n                # Compute sample variance s_h^2, with rule for n_h = 1\n                s_h_sq_r = 0.0\n                if n_h_arr[h_idx] > 1:\n                    s_h_sq_r = np.var(sample, ddof=1)\n                \n                # Accumulate for mu_ps and V_hat for this replicate\n                mu_ps_r += W_h_arr[h_idx] * y_bar_h\n                \n                # The contribution to variance is zero if n_h = 0. Problem has n_h >= 1.\n                if n_h_arr[h_idx] > 0:\n                    V_hat_r += (W_h_arr[h_idx]**2 * fpc_arr[h_idx] * s_h_sq_r) / n_h_arr[h_idx]\n            \n            mu_ps_reps[r] = mu_ps_r\n            V_hat_reps[r] = V_hat_r\n\n        # Calculate Monte Carlo variance and average plug-in variance\n        V_mc = np.var(mu_ps_reps, ddof=1)\n        V_plug_bar = np.mean(V_hat_reps)\n\n        # 4. Accuracy metrics\n        # Avoid division by zero if V_true is zero\n        if V_true == 0:\n            rel_err_V_mc = 0.0 if V_mc == 0.0 else np.inf\n            rel_bias_V_plug = 0.0 if V_plug_bar == 0.0 else np.inf\n        else:\n            rel_err_V_mc = np.abs(V_mc - V_true) / V_true\n            rel_bias_V_plug = (V_plug_bar - V_true) / V_true\n\n        results_all_cases.append([V_true, rel_err_V_mc, rel_bias_V_plug])\n    \n    # Final print statement in the exact required format.\n    print(f\"{results_all_cases}\")\n\nsolve()\n```"
        },
        {
            "introduction": "对于复杂的估计量或抽样设计，推导方差的解析公式可能非常困难甚至不可行。本练习将介绍一种更为通用和强大的方差估计技术：重抽样方法。您将亲手实现两种主流的重复权重方法——刀切法（jackknife）和自助法（bootstrap），来估计后层化加权均值的方差。此练习的核心挑战在于构造重复权重，使其在每次重抽样中都能保持关键的后层化校准约束，这对于保证方差估计的准确性至关重要。通过这项实践，您将掌握在应用统计中处理复杂加权估计量方差的一项核心实用技能。",
            "id": "3330435",
            "problem": "考虑一个有限总体，它被划分为 $K$ 个事后层，索引为 $k \\in \\{1,\\dots,K\\}$。已知各事后层的总体规模为 $N_k$，样本 $s = \\bigcup_{k=1}^K s_k$ 由每个事后层 $k$ 中抽取的 $n_k$ 个抽样单元组成。将事后层 $k$ 中的抽样单元的索引记为 $i \\in s_k$。设初始抽样权重 $w_i$ 为非负，并对每个事后层 $k$ 满足事后分层校准约束 $\\sum_{i \\in s_k} w_i = N_k$。设 $y_i$ 为单元 $i$ 所关注的标量调查变量。定义事后分层加权均值估计量为\n$$\n\\hat{\\mu} \\equiv \\frac{\\sum_{k=1}^K \\sum_{i \\in s_k} w_i \\, y_i}{\\sum_{k=1}^K N_k}.\n$$\n您的任务是为两种重抽样方案——删除-1刀切法和事后层内自助法——构建重复权重。这些权重必须在每次重复中都保持事后分层约束。然后，使用这些重复样本来估计事后分层加权均值的方差。\n\n使用的基本原理和定义：\n- 样本的Horvitz–Thompson总量估计量为 $ \\hat{T} = \\sum_{i \\in s} w_i y_i $，事后分层加权均值为 $ \\hat{\\mu} = \\hat{T} / N $，其中 $ N = \\sum_{k=1}^K N_k $ 已知。\n- 事后分层校准要求对每个事后层 $k$ 满足约束 $ \\sum_{i \\in s_k} w_i = N_k $。\n- 在删除-1刀切法中，通过删除一个单元并调整剩余单元的权重来形成一个重复样本，以使约束条件保持满足。\n- 在自助法中，通过在每个事后层内进行有放回的重抽样来形成重复样本，然后校准临时的重复权重以满足约束条件。\n\n您必须实现以下重复权重的构建方法：\n\n1. 删除-1刀切法重复权重：\n   - 对于事后层 $k^\\ast$ 中的一个单元 $d \\in s_{k^\\ast}$，通过设置 $w_d^{(r)} = 0$ 并将同一事后层中剩余权重乘以一个因子 $ \\alpha_{k^\\ast}^{(r)} $ 来定义一个重复样本，使得 $ \\sum_{i \\in s_{k^\\ast}} w_i^{(r)} = N_{k^\\ast} $。对于所有其他事后层 $k \\neq k^\\ast$，设置 $w_i^{(r)} = w_i$。\n   - 您必须处理 $n_{k^\\ast} = 1$ 的边界情况。在这种情况下，删除唯一的单元会使校准未定义；因此，应跳过为此类事后层创建刀切法重复样本。\n   - 对于每个刀切法重复样本 $r$，计算事后分层加权均值 $ \\hat{\\mu}^{(r)} $。\n   - 使用适用于平滑统计量的删除-1刀切法公式来估计刀切方差：\n     $$\n     \\widehat{\\mathrm{Var}}_{\\mathrm{JK}}(\\hat{\\mu}) = \\frac{m-1}{m} \\sum_{r=1}^{m} \\left(\\hat{\\mu}^{(r)} - \\hat{\\mu}\\right)^2,\n     $$\n     其中 $m$ 是实际构建的刀切法重复样本总数（即，仅对 $n_k \\geq 2$ 的事后层求和）。\n\n2. 事后层内自助法重复权重：\n   - 对于每个重复样本 $b \\in \\{1,\\dots,B\\}$ 和每个事后层 $k$，从一个多项分布中为 $s_k$ 中的每个抽样单元 $i$ 抽取计数 $c_i^{(b)}$。该多项分布有 $n_k$ 次试验，且 $s_k$ 中每个抽样单元的概率相等，为 $1/n_k$。在每个事后层内使用相同的独立多项分布构造。\n   - 形成临时权重 $\\tilde{w}_i^{(b)} = w_i \\, c_i^{(b)}$，然后为每个事后层计算一个校准因子 $ \\alpha_k^{(b)} $，使得 $ \\sum_{i \\in s_k} \\alpha_k^{(b)} \\tilde{w}_i^{(b)} = N_k $，并设置 $ w_i^{(b)} = \\alpha_k^{(b)} \\tilde{w}_i^{(b)} $。\n   - 对于每个自助法重复样本 $b$，计算 $ \\hat{\\mu}^{(b)} $。\n   - 将自助法方差估计为重复估计值的无偏样本方差：\n     $$\n     \\widehat{\\mathrm{Var}}_{\\mathrm{BS}}(\\hat{\\mu}) = \\frac{1}{B-1} \\sum_{b=1}^{B} \\left(\\hat{\\mu}^{(b)} - \\bar{\\mu}_{\\mathrm{BS}}\\right)^2,\n     $$\n     其中 $ \\bar{\\mu}_{\\mathrm{BS}} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\mu}^{(b)} $。\n\n约束验证：\n- 对于每个构建的刀切法重复样本和每个事后层 $k$，验证 $ \\sum_{i \\in s_k} w_i^{(r)} = N_k $。\n- 对于每个自助法重复样本 $b$ 和每个事后层 $k$，验证 $ \\sum_{i \\in s_k} w_i^{(b)} = N_k $。\n\n针对以下测试套件，实现一个完整的程序，该程序构建重复权重，计算重复估计值，验证事后分层约束，并输出加权均值的刀切法和自助法方差估计值，同时附上布尔标志，以指示是否在所有重复样本中都满足了约束。所有数组均按事后层 $k = 1, 2, \\dots, K$ 的顺序列出。\n\n测试套件：\n- 案例1（一般情况，事后层内权重相等）：\n  - $K = 3$，$N_k = \\{\\,1200, 800, 1000\\,\\}$。\n  - 样本量：$n_k = \\{\\,4, 3, 5\\,\\}$。\n  - 各事后层的权重：事后层1中的 $w_i$ 为 $\\{\\,300, 300, 300, 300\\,\\}$；事后层2中为 $\\{\\,266.\\overline{6}, 266.\\overline{6}, 266.\\overline{6}\\,\\}$；事后层3中为 $\\{\\,200, 200, 200, 200, 200\\,\\}$。\n  - $y_i$ 值：事后层1为 $\\{\\,10.0, 12.5, 9.3, 11.1\\,\\}$；事后层2为 $\\{\\,7.0, 8.2, 6.5\\,\\}$；事后层3为 $\\{\\,15.0, 14.2, 16.8, 13.9, 15.5\\,\\}$。\n- 案例2（边界情况，包含一个 $n_k = 1$ 的事后层和不相等的权重）：\n  - $K = 3$，$N_k = \\{\\,500, 300, 200\\,\\}$。\n  - 样本量：$n_k = \\{\\,1, 3, 2\\,\\}$。\n  - 各事后层的权重：事后层1为 $\\{\\,500\\,\\}$；事后层2为 $\\{\\,120, 90, 90\\,\\}$；事后层3为 $\\{\\,120, 80\\,\\}$。\n  - $y_i$ 值：事后层1为 $\\{\\,20.0\\,\\}$；事后层2为 $\\{\\,5.5, 4.0, 6.5\\,\\}$；事后层3为 $\\{\\,8.0, 7.5\\,\\}$。\n- 案例3（边缘情况，权重高度不均且 $y_i$ 为重尾分布）：\n  - $K = 2$，$N_k = \\{\\,10000, 5000\\,\\}$。\n  - 样本量：$n_k = \\{\\,5, 4\\,\\}$。\n  - 各事后层的权重：事后层1为 $\\{\\,7000, 500, 500, 1000, 1000\\,\\}$；事后层2为 $\\{\\,1000, 1200, 1800, 1000\\,\\}$。\n  - $y_i$ 值：事后层1为 $\\{\\,1000, 50, 60, 55, 65\\,\\}$；事后层2为 $\\{\\,70, 80, 75, 85\\,\\}$。\n\n使用 $B = 500$ 个自助法重复样本和一个固定的伪随机数生成器种子 $271828$ 以确保可复现性。所有计算必须使用实数。程序必须以 $10^{-12}$ 的数值容差验证每个重复样本的校准约束。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个列表的列表，每个案例对应一个内层列表。每个内层列表的格式为 $[\\,\\widehat{\\mathrm{Var}}_{\\mathrm{JK}}(\\hat{\\mu}), \\widehat{\\mathrm{Var}}_{\\mathrm{BS}}(\\hat{\\mu}), \\text{constraints\\_ok\\_JK}, \\text{constraints\\_ok\\_BS}\\,]$，其中方差条目为浮点数，约束指标为布尔值。整个输出必须是严格符合该格式的单行文本，例如 $[\\,[0.0,0.0,\\text{True},\\text{True}]\\,]$，但需要填充三个指定案例的结果。",
            "solution": "该问题要求实现并比较两种用于事后分层加权均值估计量的、基于重抽样的方差估计方法：删除-1刀切法和事后层内自助法。这两种方法的一个关键组成部分是在每次重复中都保持事后分层校准约束。\n\n首先，我们建立初始设置。总体被划分为 $K$ 个事后层，每个事后层 $k \\in \\{1, \\dots, K\\}$ 的总体数量 $N_k$ 已知。总总体规模为 $N = \\sum_{k=1}^K N_k$。我们给定一个样本 $s$，它由从每个事后层 $k$ 抽取的 $n_k$ 个单元组成，记为 $s_k$。对于每个抽样单元 $i \\in s$，我们有一个调查变量 $y_i$ 和一个初始非负权重 $w_i$。这些初始权重经过校准，使得对每个事后层 $k$，约束 $\\sum_{i \\in s_k} w_i = N_k$ 都得到满足。\n\n事后分层加权均值估计量 $\\hat{\\mu}$ 定义为：\n$$\n\\hat{\\mu} = \\frac{\\sum_{k=1}^K \\sum_{i \\in s_k} w_i y_i}{N}\n$$\n我们的目标是使用两种指定的重抽样方案来估计该估计量的方差 $\\widehat{\\mathrm{Var}}(\\hat{\\mu})$。\n\n**1. 删除-1刀切法方差估计**\n\n删除-1刀切法通过每次系统地移除一个抽样单元，并调整剩余单元的权重以维持校准约束，来创建一系列“重复样本”。\n\n设一个重复样本由 $r$ 索引，对应于从其事后层 $k^\\ast$ 中删除单元 $d$。有效重复样本的总数 $m$ 是所有至少包含两个单元的事后层的样本量之和：$m = \\sum_{k: n_k \\ge 2} n_k$。$n_k = 1$ 的情况被明确排除，因为删除唯一的单元将使该层的重新校准变得不可能。\n\n对于通过删除单元 $d \\in s_{k^\\ast}$（其中 $n_{k^\\ast} \\ge 2$）而形成的重复样本 $r$，其重复权重 $w_i^{(r)}$ 定义如下：\n- 对于被删除的单元 $d$，其权重设为零：$w_d^{(r)} = 0$。\n- 对于位于不同事后层（$k \\neq k^\\ast$）的任何其他单元 $i$，其权重保持不变：对于 $i \\in s_k$，$w_i^{(r)} = w_i$。\n- 对于同一事后层 $k^\\ast$ 内的剩余单元 $i$（$i \\in s_{k^\\ast}, i \\neq d$），其原始权重被统一乘以一个因子 $\\alpha_{k^\\ast}^{(r)}$：$w_i^{(r)} = \\alpha_{k^\\ast}^{(r)} w_i$。\n\n缩放因子 $\\alpha_{k^\\ast}^{(r)}$ 是通过对事后层 $k^\\ast$ 强制施加校准约束而导出的：\n$$\n\\sum_{i \\in s_{k^\\ast}} w_i^{(r)} = N_{k^\\ast}\n$$\n代入 $w_i^{(r)}$ 的定义：\n$$\nw_d^{(r)} + \\sum_{i \\in s_{k^\\ast}, i \\neq d} w_i^{(r)} = 0 + \\sum_{i \\in s_{k^\\ast}, i \\neq d} \\alpha_{k^\\ast}^{(r)} w_i = \\alpha_{k^\\ast}^{(r)} \\sum_{i \\in s_{k^\\ast}, i \\neq d} w_i = N_{k^\\ast}\n$$\n剩余单元的原始权重之和为 $(\\sum_{i \\in s_{k^\\ast}} w_i) - w_d$。由于原始权重是经过校准的，这个和等于 $N_{k^\\ast} - w_d$。因此：\n$$\n\\alpha_{k^\\ast}^{(r)} (N_{k^\\ast} - w_d) = N_{k^\\ast} \\implies \\alpha_{k^\\ast}^{(r)} = \\frac{N_{k^\\ast}}{N_{k^\\ast} - w_d}\n$$\n只要 $N_{k^\\ast} - w_d \\neq 0$，该因子就是良定义的。如果 $n_{k^\\ast} \\ge 2$ 且权重不是病态的（例如，一个单元的权重为 $N_{k^\\ast}$ 而其他单元的权重为零，如果我们假设权重为正，这将与 $n_{k^\\ast} \\ge 2$ 相矛盾），那么这个条件就能得到保证。\n\n对于 $m$ 个重复样本中的每一个，我们计算其重复均值估计量：\n$$\n\\hat{\\mu}^{(r)} = \\frac{\\sum_{k=1}^K \\sum_{i \\in s_k} w_i^{(r)} y_i}{N}\n$$\n然后，$\\hat{\\mu}$ 的刀切法方差估计如下：\n$$\n\\widehat{\\mathrm{Var}}_{\\mathrm{JK}}(\\hat{\\mu}) = \\frac{m-1}{m} \\sum_{r=1}^{m} \\left(\\hat{\\mu}^{(r)} - \\hat{\\mu}\\right)^2\n$$\n\n**2. 事后层内自助法方差估计**\n\n自助法通过从原始样本中有放回地重抽样单元来生成重复样本。对于事后分层数据，此重抽样在每个事后层内独立进行。\n\n对于 $B$ 个自助法重复样本中的每一个（由 $b \\in \\{1, \\dots, B\\}$ 索引），以及对于每个事后层 $k$，我们执行以下步骤：\n1.  从一个具有 $n_k$ 次试验和 $n_k$ 个类别的多项分布中，抽取一组整数计数 $\\{c_i^{(b)}\\}_{i \\in s_k}$，每个类别的概率相等，为 $1/n_k$。计数 $c_i^{(b)}$ 表示单元 $i$ 在层 $k$ 的自助法重抽样样本中被选中的次数。注意 $\\sum_{i \\in s_k} c_i^{(b)} = n_k$。\n2.  通过将原始权重与这些计数相乘，形成临时重复权重：对于每个 $i \\in s_k$，$\\tilde{w}_i^{(b)} = w_i c_i^{(b)}$。\n3.  校准这些临时权重以满足事后分层约束。为每个事后层 $k$ 计算一个缩放因子 $\\alpha_k^{(b)}$：\n    $$\n    \\alpha_k^{(b)} = \\frac{N_k}{\\sum_{i \\in s_k} \\tilde{w}_i^{(b)}} = \\frac{N_k}{\\sum_{i \\in s_k} w_i c_i^{(b)}}\n    $$\n    只要分母不为零，这个因子就是良定义的。如果至少有一个权重 $w_i > 0$ 的单元在自助法重抽样中被选中（即 $c_i^{(b)} > 0$），则此条件成立。考虑到测试案例中的样本量和正权重，分母为零的概率可以忽略不计。\n4.  最终的自助法重复权重为 $w_i^{(b)} = \\alpha_k^{(b)} \\tilde{w}_i^{(b)}$，适用于所有单元 $i \\in s_k$。对所有事后层重复此过程，从而得到一整套重复权重 $\\{w_i^{(b)}\\}_{i \\in s}$。\n\n对于 $B$ 个重复样本中的每一个，我们计算其重复均值估计量：\n$$\n\\hat{\\mu}^{(b)} = \\frac{\\sum_{k=1}^K \\sum_{i \\in s_k} w_i^{(b)} y_i}{N}\n$$\n自助法方差是这些重复估计值的样本方差。首先，我们求出重复估计值的均值 $\\bar{\\mu}_{\\mathrm{BS}} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\mu}^{(b)}$。然后方差为：\n$$\n\\widehat{\\mathrm{Var}}_{\\mathrm{BS}}(\\hat{\\mu}) = \\frac{1}{B-1} \\sum_{b=1}^{B} \\left(\\hat{\\mu}^{(b)} - \\bar{\\mu}_{\\mathrm{BS}}\\right)^2\n$$\n\n**3. 实现与验证**\n\n实现过程将首先计算初始估计值 $\\hat{\\mu}$ 来处理每个测试案例。然后，它将执行刀切法程序，生成每个重复样本，在 $10^{-12}$ 的容差范围内验证校准约束 $\\sum_{i \\in s_k} w_i^{(r)} = N_k$ 对所有 $k$ 都成立，并存储重复估计值 $\\hat{\\mu}^{(r)}$。随后，它将计算 $\\widehat{\\mathrm{Var}}_{\\mathrm{JK}}(\\hat{\\mu})$。\n\n接下来，它将为 $B=500$ 个重复样本执行自助法程序，使用固定的随机数生成器种子 $271828$ 以保证可复现性。对于每个重复样本，它将生成权重 $w_i^{(b)}$ 并验证所有 $k$ 的校准约束 $\\sum_{i \\in s_k} w_i^{(b)} = N_k$。它将存储重复估计值 $\\hat{\\mu}^{(b)}$，然后计算 $\\widehat{\\mathrm{Var}}_{\\mathrm{BS}}(\\hat{\\mu})$。布尔标志将跟踪每种方法是否满足了所有约束。最终输出将是一个包含每个测试案例结果的列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    \n    # Global parameters from the problem statement.\n    B = 500\n    SEED = 271828\n    TOL = 1e-12\n\n    def process_case(Nk_list, nk_list, weights_per_stratum_list, y_per_stratum_list):\n        \"\"\"\n        Processes a single test case to compute jackknife and bootstrap variances.\n        \"\"\"\n        # --- Data Preprocessing ---\n        Nk = np.array(Nk_list, dtype=float)\n        nk = np.array(nk_list, dtype=int)\n        \n        K = len(Nk)\n        N_total = np.sum(Nk)\n        \n        # Flatten data into single arrays and create an index mapping each unit to its stratum.\n        w0 = np.concatenate([np.array(w, dtype=float) for w in weights_per_stratum_list])\n        y = np.concatenate([np.array(val, dtype=float) for val in y_per_stratum_list])\n        \n        stratum_indices = np.repeat(np.arange(K), nk)\n\n        # --- Initial Estimate Calculation ---\n        T_hat = np.sum(w0 * y)\n        mu_hat = T_hat / N_total\n\n        # --- Method 1: Delete-One Jackknife ---\n        constraints_ok_JK = True\n        mu_hat_jk_reps = []\n        \n        num_total_units = len(w0)\n        replicate_count_jk = 0\n\n        for i in range(num_total_units):\n            k_star = stratum_indices[i]  # Stratum of the unit to be dropped.\n            \n            if nk[k_star] < 2:\n                continue  # Skip strata with n_k  2 as per instruction.\n            \n            replicate_count_jk += 1\n            \n            # Create replicate weights.\n            w_rep = w0.copy()\n            \n            # Calculate the adjustment factor for the stratum of the dropped unit.\n            w_d = w0[i]\n            sum_w_minus_d = Nk[k_star] - w_d\n            \n            # This should always be safe due to the nk[k_star] >= 2 check.\n            alpha = Nk[k_star] / sum_w_minus_d if sum_w_minus_d != 0 else 0\n            \n            # Apply the adjustment.\n            stratum_mask = (stratum_indices == k_star)\n            w_rep[stratum_mask] *= alpha\n            w_rep[i] = 0.0  # Set weight of dropped unit to zero.\n            \n            # Verification of constraints for the current replicate.\n            for k in range(K):\n                sum_w_k = np.sum(w_rep[stratum_indices == k])\n                if not np.isclose(sum_w_k, Nk[k], atol=TOL, rtol=0):\n                    constraints_ok_JK = False\n            \n            # Calculate replicate estimate.\n            mu_hat_rep = np.sum(w_rep * y) / N_total\n            mu_hat_jk_reps.append(mu_hat_rep)\n        \n        # Calculate Jackknife variance.\n        var_jk = 0.0\n        if replicate_count_jk > 1:\n            m = replicate_count_jk\n            mu_hat_jk_reps = np.array(mu_hat_jk_reps)\n            var_jk = ((m - 1.0) / m) * np.sum((mu_hat_jk_reps - mu_hat)**2)\n        \n        # --- Method 2: Within-Post-Stratum Bootstrap ---\n        constraints_ok_BS = True\n        mu_hat_bs_reps = []\n        rng = np.random.default_rng(SEED)\n        \n        for _ in range(B):\n            w_rep = np.zeros_like(w0)\n            \n            for k in range(K):\n                stratum_mask = (stratum_indices == k)\n                if nk[k] == 0:\n                    continue\n\n                w0_k = w0[stratum_mask]\n                \n                # Draw multinomial counts for resampling with replacement.\n                probs = np.full(nk[k], 1.0 / nk[k])\n                counts = rng.multinomial(nk[k], probs, size=1)[0]\n                \n                # Calculate temporary weights and their sum.\n                w_tilde_k = w0_k * counts\n                sum_w_tilde_k = np.sum(w_tilde_k)\n                \n                # Calculate calibration factor.\n                alpha_k = Nk[k] / sum_w_tilde_k if sum_w_tilde_k  0 else 0.0\n                \n                # Final replicate weights for the stratum.\n                w_rep_k = w_tilde_k * alpha_k\n                w_rep[stratum_mask] = w_rep_k\n                \n                # Verification for this stratum in this replicate.\n                if not np.isclose(np.sum(w_rep_k), Nk[k], atol=TOL, rtol=0):\n                    constraints_ok_BS = False\n            \n            # Calculate total replicate estimate.\n            mu_hat_rep = np.sum(w_rep * y) / N_total\n            mu_hat_bs_reps.append(mu_hat_rep)\n        \n        # Calculate Bootstrap variance (using sample variance, ddof=1).\n        mu_hat_bs_reps = np.array(mu_hat_bs_reps)\n        var_bs = np.var(mu_hat_bs_reps, ddof=1)\n        \n        return [var_jk, var_bs, constraints_ok_JK, constraints_ok_BS]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"Nk\": [1200, 800, 1000], \"nk\": [4, 3, 5],\n            \"weights\": [[300.0] * 4, [800.0 / 3.0] * 3, [200.0] * 5],\n            \"y\": [[10.0, 12.5, 9.3, 11.1], [7.0, 8.2, 6.5], [15.0, 14.2, 16.8, 13.9, 15.5]]\n        },\n        {\n            \"Nk\": [500, 300, 200], \"nk\": [1, 3, 2],\n            \"weights\": [[500.0], [120.0, 90.0, 90.0], [120.0, 80.0]],\n            \"y\": [[20.0], [5.5, 4.0, 6.5], [8.0, 7.5]]\n        },\n        {\n            \"Nk\": [10000, 5000], \"nk\": [5, 4],\n            \"weights\": [[7000.0, 500.0, 500.0, 1000.0, 1000.0], [1000.0, 1200.0, 1800.0, 1000.0]],\n            \"y\": [[1000.0, 50.0, 60.0, 55.0, 65.0], [70.0, 80.0, 75.0, 85.0]]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        res = process_case(case[\"Nk\"], case[\"nk\"], case[\"weights\"], case[\"y\"])\n        results.append(res)\n\n    # Format the final output string exactly as required.\n    # Python's str(True) -> 'True', which matches the desired format.\n    result_strings = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个精确的方差估计固然重要，但同样关键的是要评估估计量本身的稳定性。在实践中，某些观测值或层可能对最终估计值及其方差产生不成比例的巨大影响，这预示着潜在的不稳定性。本练习将指导您构建一套诊断工具，用于量化和识别这些问题。您将学习如何从第一性原理出发，推导并计算“有效观测单元数”和“有效方差贡献单元数”，并利用基于影响力和杠杆值的指标来标记出可能导致方差膨胀的极端观测值和层。这项实践将使您具备批判性评估后层化模型质量的能力，这是负责任数据分析的关键一步。",
            "id": "3330477",
            "problem": "考虑一个被划分为 $H$ 个事后分层（单元）的有限总体。设各单元的真实总体构成由单元比例向量 $\\{p_h\\}_{h=1}^H$ 给出，满足 $\\sum_{h=1}^H p_h = 1$ 且 $p_h \\in (0,1)$。从一个样本量为 $n$ 的样本中，假设每个单元的实现样本数为 $\\{n_h\\}_{h=1}^H$，其中 $\\sum_{h=1}^H n_h = n$ 且 $n_h \\in \\mathbb{N}$, $n_h \\geq 1$。假设在每个单元 $h$ 内，样本为感兴趣的结果变量提供了一个单元均值 $\\bar{y}_h$ 和一个单元内样本方差估计值 $\\hat{\\sigma}_h^2$。考虑总体的标准事后分层估计量，该估计量通过按总体构成对单元均值进行加权聚合。\n\n你将构建一些诊断指标，用以量化有多少观测单元和多少单元对估计量有有效贡献，并使用杠杆率或影响度量来标记指示潜在方差膨胀的极端权重。这些诊断指标必须从以下基本依据出发，由第一性原理推导得出：\n\n- 加权均值估计的 Horvitz–Thompson 特征描述：均值估计量表示为加权和的比率，其权重经过校准，使得加权样本能够重现已知的总体构成。\n- 平滑估计量方差的一阶泰勒线性化：在单元间独立和单元内抽样假设下，事后分层均值的渐近方差可以表示为依赖于 $p_h$、$n_h$ 和 $\\hat{\\sigma}_h^2$ 的各单元特定贡献之和。\n\n仅根据给定的输入定义以下量，不引入任何外部参数：\n\n1. 由事后分层所隐含的观测层级权重，以及每个观测值的相关归一化影响度，该影响度定义为估计量对该观测值扰动的敏感度，并经过缩放以使所有观测值的总和为 $1$。使用此值来量化对估计量有贡献的有效观测单元数。\n\n2. 来自事后分层均值方差的一阶线性化的单元层级方差贡献，以及每个单元的相关归一化方差杠杆率。使用此值来量化贡献方差的有效单元数。\n\n3. 一种基于影响度的诊断指标，用于标记极端观测值：如果一个观测值的归一化影响度超过 $2/n$，则将其分类为极端值。\n\n4. 一种基于杠杆率的诊断指标，用于标记在方差方面表现极端的单元：如果一个单元的归一化方差杠杆率超过 $2/H$，则将其分类为极端单元。\n\n如果总线性化方差聚合值为零，则按照约定将方差杠杆率向量定义为全零，并将贡献方差的有效单元数定义为 $H$。\n\n你的任务是实现一个程序，对每个测试用例，计算一个包含以下六个量的列表，并严格按照此顺序排列：\n\n- 有效观测单元数，以实数表示。\n- 贡献方差的有效单元数，以实数表示。\n- 根据上述观测值影响度规则计数的极端观测值数量（一个整数）。\n- 根据上述方差杠杆率规则计数的极端单元数量（一个整数）。\n- 所有观测值中最大的观测层级归一化影响度值，以实数表示。\n- 所有单元中最大的单元层级归一化方差杠杆率值，以实数表示。\n\n你的程序必须生成单行输出，其中包含所有测试用例的结果。结果格式为一个用方括号括起来的逗号分隔列表，每个测试用例的六元组显示为一个用方括号括起来的、无空格的逗号分隔列表。例如，输出格式必须严格为 $[ [a_1,a_2,a_3,a_4,a_5,a_6],[b_1,b_2,b_3,b_4,b_5,b_6],\\ldots ]$ 的形式，且不含任何空格。\n\n使用以下参数值测试套件，它涵盖了一般情况、一个应导致极端观测权重的高度稀疏单元的情况，以及一个零方差单元的边界情况：\n\n- 测试用例 1:\n  - $H = 5$\n  - $p = [0.15, 0.25, 0.10, 0.30, 0.20]$\n  - $n_h = [60, 80, 40, 70, 50]$\n  - $n = 300$\n  - $\\hat{\\sigma}^2 = [1.0, 0.8, 1.2, 0.9, 1.1]$\n\n- 测试用例 2:\n  - $H = 4$\n  - $p = [0.40, 0.30, 0.20, 0.10]$\n  - $n_h = [10, 90, 80, 70]$\n  - $n = 250$\n  - $\\hat{\\sigma}^2 = [1.0, 0.8, 0.7, 0.6]$\n\n- 测试用例 3:\n  - $H = 3$\n  - $p = [0.50, 0.30, 0.20]$\n  - $n_h = [150, 1, 49]$\n  - $n = 200$\n  - $\\hat{\\sigma}^2 = [1.0, 0.0, 1.5]$\n\n你的程序不得读取任何输入；它必须嵌入上述测试套件，并以确切要求的格式打印单行输出。所有答案均为无单位的实数或整数。实现必须与上述从基本依据推导出的结论保持一致，并且必须完全依赖于给定的参数 $H$、$p$、$n_h$、$n$ 和 $\\hat{\\sigma}_h^2$。",
            "solution": "该问题要求推导和计算一个总体均值的事后分层估计量的若干诊断量。推导将如指定的那样基于第一性原理。设事后分层的数量为 $H$，总体单元比例为 $\\{p_h\\}_{h=1}^H$，实现的样本单元数量为 $\\{n_h\\}_{h=1}^H$，总样本量为 $n = \\sum_{h=1}^H n_h$，单元内样本方差估计值为 $\\{\\hat{\\sigma}_h^2\\}_{h=1}^H$。\n\n总体均值 $\\mu$ 的事后分层估计量 $\\hat{\\mu}_{ps}$ 由下式给出：\n$$ \\hat{\\mu}_{ps} = \\sum_{h=1}^H p_h \\bar{y}_h $$\n其中 $\\bar{y}_h$ 是单元 $h$ 中结果变量的样本均值。设样本中的单个观测值由 $i=1, \\dots, n$ 索引，并令 $c(i)$ 表示观测值 $i$ 所属的单元。单元均值为 $\\bar{y}_h = \\frac{1}{n_h} \\sum_{i: c(i)=h} y_i$。将此代入估计量可得：\n$$ \\hat{\\mu}_{ps} = \\sum_{h=1}^H p_h \\left( \\frac{1}{n_h} \\sum_{i: c(i)=h} y_i \\right) = \\sum_{h=1}^H \\sum_{i: c(i)=h} \\frac{p_h}{n_h} y_i $$\n这表明该估计量是单个观测值的加权和，$\\hat{\\mu}_{ps} = \\sum_{i=1}^n w_i y_i$，其中单元 $c(i)$ 中观测值 $i$ 的权重 $w_i$ 由下式给出：\n$$ w_i = w_{c(i)} = \\frac{p_{c(i)}}{n_{c(i)}} $$\n很容易验证这些权重经过校准，其总和为 $1$：\n$$ \\sum_{i=1}^n w_i = \\sum_{h=1}^H \\sum_{i: c(i)=h} w_i = \\sum_{h=1}^H n_h \\left( \\frac{p_h}{n_h} \\right) = \\sum_{h=1}^H p_h = 1 $$\n\n建立了这些权重之后，我们就可以推导所需的诊断量。\n\n**1. 观测层级的诊断指标与有效单元数**\n\n问题将观测值的归一化影响度定义为估计量对该观测值变化的敏感度，并经过缩放以使总和为 $1$。$\\hat{\\mu}_{ps}$ 对 $y_i$ 变化的敏感度是其偏导数：\n$$ \\frac{\\partial \\hat{\\mu}_{ps}}{\\partial y_i} = \\frac{\\partial}{\\partial y_i} \\sum_{j=1}^n w_j y_j = w_i $$\n由于权重 $w_i$ 的总和已经为 $1$，它们本身就是归一化的影响度值，我们将其表示为 $\\text{infl}_i = w_i$。\n\n有效观测单元数 $n_{eff, obs}$ 可以使用 Kish 的有效样本量公式来量化，该公式适用于总和为 $1$ 的一组权重：\n$$ n_{eff, obs} = \\frac{\\left(\\sum_{i=1}^n w_i\\right)^2}{\\sum_{i=1}^n w_i^2} = \\frac{1}{\\sum_{i=1}^n w_i^2} $$\n权重平方和可以通过按单元对观测值进行分组来计算：\n$$ \\sum_{i=1}^n w_i^2 = \\sum_{h=1}^H \\sum_{i: c(i)=h} \\left(\\frac{p_h}{n_h}\\right)^2 = \\sum_{h=1}^H n_h \\frac{p_h^2}{n_h^2} = \\sum_{h=1}^H \\frac{p_h^2}{n_h} $$\n因此，有效观测单元数为：\n$$ n_{eff, obs} = \\frac{1}{\\sum_{h=1}^H p_h^2 / n_h} $$\n如果一个观测值的影响度超过 $2/n$，则将其标记为极端。对于单元 $h$ 中的一个观测值，此条件为 $\\text{infl}_i = p_h/n_h  2/n$。如果此条件成立，则该单元中的所有 $n_h$ 个观测值都被归类为极端。极端观测值的总数是满足此标准的所有单元 $h$ 的 $n_h$ 之和。最大的观测层级影响度就是 $\\max_{h} (p_h/n_h)$。\n\n**2. 单元层级的诊断指标与有效单元数**\n\n在给定实现的样本量 $n_h$ 并假设跨单元抽样独立的条件下，事后分层估计量的方差为：\n$$ \\text{Var}(\\hat{\\mu}_{ps}) = \\text{Var}\\left(\\sum_{h=1}^H p_h \\bar{y}_h\\right) = \\sum_{h=1}^H p_h^2 \\text{Var}(\\bar{y}_h) $$\n使用提供的每个单元内总体方差 $\\sigma_h^2$ 的样本方差估计值 $\\hat{\\sigma}_h^2$，以及公式 $\\text{Var}(\\bar{y}_h) \\approx \\hat{\\sigma}_h^2/n_h$，$\\hat{\\mu}_{ps}$ 的估计方差为：\n$$ \\hat{V}(\\hat{\\mu}_{ps}) = \\sum_{h=1}^H \\frac{p_h^2 \\hat{\\sigma}_h^2}{n_h} $$\n该表达式将总方差表示为各单元特定贡献之和，即 $V_h = p_h^2 \\hat{\\sigma}_h^2 / n_h$。单元 $h$ 的“归一化方差杠杆率”（表示为 $\\lambda_h$）是其对总方差的贡献分数：\n$$ \\lambda_h = \\frac{V_h}{\\sum_{j=1}^H V_j} = \\frac{p_h^2 \\hat{\\sigma}_h^2 / n_h}{\\sum_{j=1}^H p_j^2 \\hat{\\sigma}_j^2 / n_j} $$\n这些杠杆率的总和为 $1$（前提是总方差不为零）。贡献方差的有效单元数 $n_{eff, cell}$ 使用应用于单元层级杠杆率的相同 Kish 公式计算：\n$$ n_{eff, cell} = \\frac{1}{\\sum_{h=1}^H \\lambda_h^2} $$\n根据问题陈述，如果总方差 $\\sum_j V_j=0$，我们定义所有 $h$ 的 $\\lambda_h=0$，并且 $n_{eff, cell}=H$。\n如果一个单元的杠杆率超过 $2/H$（即 $\\lambda_h  2/H$），则将其标记为极端。总数是满足此条件的单元数量。最大的单元层级杠杆率是 $\\max_h \\lambda_h$。\n\n**计算公式摘要：**\n1.  **有效观测单元数**: $n_{eff, obs} = 1 / (\\sum_{h=1}^H p_h^2 / n_h)$\n2.  **贡献方差的有效单元数**:\n    - 令 $V_h = p_h^2 \\hat{\\sigma}_h^2 / n_h$ 且 $V_{tot} = \\sum_h V_h$。\n    - 如果 $V_{tot} = 0$，则 $n_{eff, cell} = H$。\n    - 否则，$\\lambda_h = V_h / V_{tot}$ 且 $n_{eff, cell} = 1 / (\\sum_{h=1}^H \\lambda_h^2)$。\n3.  **极端观测值计数**: $\\sum_{h=1}^H n_h \\cdot \\mathbf{1}(p_h/n_h  2/n)$，其中 $\\mathbf{1}(\\cdot)$ 是指示函数。\n4.  **极端单元计数**: $\\sum_{h=1}^H \\mathbf{1}(\\lambda_h  2/H)$，其中如果 $V_{tot}=0$，$\\lambda_h$ 定义为 $0$。\n5.  **最大的观测层级归一化影响度**: $\\max_{h} (p_h / n_h)$。\n6.  **最大的单元层级归一化方差杠杆率**: $\\max_h \\lambda_h$，如果 $V_{tot}=0$，则定义为 $0$。\n下面针对给定的测试用例实现了这些公式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the post-stratification diagnostics problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        (5, [0.15, 0.25, 0.10, 0.30, 0.20], [60, 80, 40, 70, 50], 300, [1.0, 0.8, 1.2, 0.9, 1.1]),\n        # Test Case 2\n        (4, [0.40, 0.30, 0.20, 0.10], [10, 90, 80, 70], 250, [1.0, 0.8, 0.7, 0.6]),\n        # Test Case 3\n        (3, [0.50, 0.30, 0.20], [150, 1, 49], 200, [1.0, 0.0, 1.5]),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        H, p_list, n_h_list, n, sigma2_list = case\n        \n        p = np.array(p_list, dtype=float)\n        n_h = np.array(n_h_list, dtype=float)\n        sigma2 = np.array(sigma2_list, dtype=float)\n\n        # 1. Effective number of observational units\n        sum_sq_weights = np.sum(p**2 / n_h)\n        n_eff_obs = 1.0 / sum_sq_weights\n\n        # 3.  5. Observation-level diagnostics\n        obs_influences = p / n_h\n        # 5. Maximum observation-level normalized influence\n        max_obs_influence = np.max(obs_influences)\n        \n        # 3. Count of extreme observations\n        threshold_obs = 2.0 / n\n        extreme_obs_mask = obs_influences > threshold_obs\n        count_extreme_obs = int(np.sum(n_h[extreme_obs_mask]))\n\n        # 2., 4.,  6. Cell-level diagnostics\n        V_h = (p**2 * sigma2) / n_h\n        V_tot = np.sum(V_h)\n\n        if V_tot == 0:\n            # Special case as defined in the problem\n            # 2. Effective number of variance-contributing cells\n            n_eff_cell = float(H)\n            # 6. Maximum cell-level normalized variance-leverage\n            max_cell_leverage = 0.0\n            # 4. Count of extreme cells\n            count_extreme_cells = 0\n        else:\n            cell_leverages = V_h / V_tot\n            # 2. Effective number of variance-contributing cells\n            n_eff_cell = 1.0 / np.sum(cell_leverages**2)\n            # 6. Maximum cell-level normalized variance-leverage\n            max_cell_leverage = np.max(cell_leverages)\n            \n            # 4. Count of extreme cells\n            threshold_cell = 2.0 / H\n            count_extreme_cells = int(np.sum(cell_leverages > threshold_cell))\n            \n        result_tuple = [\n            n_eff_obs,\n            n_eff_cell,\n            count_extreme_obs,\n            count_extreme_cells,\n            max_obs_influence,\n            max_cell_leverage\n        ]\n        all_results.append(result_tuple)\n\n    # Final print statement in the exact required format.\n    # Format each list of results into \"[v1,v2,...]\" and join them.\n    formatted_results = []\n    for res_list in all_results:\n        # Using a general a number format that avoids scientific notation for\n        # some values and keeps precision, and converting ints to int strings.\n        s_list = []\n        for v in res_list:\n            if isinstance(v, int):\n                s_list.append(str(v))\n            else:\n                s_list.append(format(v, '.17g').rstrip('0').rstrip('.'))\n        \n        inner_str = \"[\" + \",\".join(s_list) + \"]\"\n        formatted_results.append(inner_str)\n    \n    final_output_string = \"[\" + \",\".join(formatted_results) + \"]\"\n    print(final_output_string.replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}