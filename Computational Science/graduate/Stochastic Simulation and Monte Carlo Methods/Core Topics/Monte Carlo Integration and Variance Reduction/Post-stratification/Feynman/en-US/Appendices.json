{
    "hands_on_practices": [
        {
            "introduction": "A solid understanding of post-stratification begins with its theoretical properties under idealized conditions. This practice solidifies the derivation of the estimator's conditional variance, a foundational result in survey sampling theory. You will then use Monte Carlo simulation to computationally verify key theoretical properties of the common \"plug-in\" variance estimator, bridging the gap between algebraic derivation and practical implementation .",
            "id": "3330424",
            "problem": "Consider a finite population partitioned into $H$ post-strata indexed by $h \\in \\{1,\\dots,H\\}$. Stratum $h$ contains $N_h$ units with values $\\{y_{h,i}\\}_{i=1}^{N_h}$. A single-stage Simple Random Sample Without Replacement (SRSWOR) of size $n$ is drawn from the full population of size $N=\\sum_{h=1}^H N_h$. After sampling, the observed stratum counts are $\\{n_h\\}_{h=1}^H$ with $\\sum_{h=1}^H n_h = n$. Define the population post-stratification weights $W_h = N_h/N$. The post-stratified estimator of the overall mean is $\\hat{\\mu}_{PS} = \\sum_{h=1}^H W_h \\,\\bar{Y}_{h}$ where $\\bar{Y}_{h}$ is the sample mean of the observed units from stratum $h$. Work conditionally on the realized $\\{n_h\\}$.\n\nTask A (derivation): Starting from fundamental definitions of SRSWOR in a finite population and the law of total variance, derive the exact conditional variance $\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\})$ in terms of the stratum sizes $\\{N_h\\}$, the realized stratum sample sizes $\\{n_h\\}$, the finite population post-stratum weights $\\{W_h\\}$, and the finite-population within-stratum variances. You must begin from the well-tested fact that under SRSWOR within a single finite stratum of size $N_h$, for a sample mean computed from $n_h$ units, the variance equals the product of the finite population correction and the within-stratum variance scaled by $1/n_h$. Do not assume or state the final expression in advance; derive it from first principles.\n\nTask B (assessment via stochastic simulation): When the within-stratum finite-population variances are unknown in practice, a natural plug-in estimator replaces each unknown within-stratum variance with the corresponding unbiased sample variance computed from the observed units in that stratum. Assess the accuracy of this plug-in estimator by Monte Carlo (stochastic) simulation conditional on the realized $\\{n_h\\}$ by comparing its average value to the exact conditional variance derived in Task A.\n\nPrecisely implement the following for each test case in the test suite below.\n\n1. Deterministic population construction. For each stratum $h \\in \\{1,\\dots,H\\}$ and each unit index $i \\in \\{1,\\dots,N_h\\}$, generate the finite population values $y_{h,i}$ by the case-specific deterministic formula given in the test suite. This ensures that the population is fixed and known.\n\n2. Exact conditional variance. Let $S_h^2$ denote the finite-population variance within stratum $h$ computed with denominator $N_h-1$, i.e., $S_h^2 = \\frac{1}{N_h-1}\\sum_{i=1}^{N_h}\\left(y_{h,i}-\\mu_h\\right)^2$ where $\\mu_h = \\frac{1}{N_h}\\sum_{i=1}^{N_h} y_{h,i}$. Using your derivation from Task A, compute the exact conditional variance $V_{\\text{true}}$ of $\\hat{\\mu}_{PS}$ given the realized $\\{n_h\\}$.\n\n3. Monte Carlo approximation of the sampling distribution. Conditional on the realized $\\{n_h\\}$, simulate $R$ independent replicates of the following experiment: for each stratum $h$, draw an SRSWOR sample of size $n_h$ from the $N_h$ units in that stratum; compute the stratum sample mean $\\bar{Y}_h$ and the unbiased stratum sample variance $s_h^2$ with denominator $n_h-1$ when $n_h \\ge 2$. When $n_h = 1$, set $s_h^2 = 0$ by convention. For each replicate $r \\in \\{1,\\dots,R\\}$, compute:\n   - the post-stratified estimator $\\hat{\\mu}_{PS}^{(r)} = \\sum_{h=1}^H W_h \\bar{Y}_h^{(r)}$,\n   - the plug-in variance estimator $\\hat{V}^{(r)} = \\sum_{h=1}^H W_h^2 \\left(1 - \\frac{n_h}{N_h}\\right)\\frac{s_h^{2\\,(r)}}{n_h}$.\n   At the end, compute the Monte Carlo variance $V_{\\text{mc}}$ as the empirical variance (with denominator $R-1$) of $\\{\\hat{\\mu}_{PS}^{(r)}\\}_{r=1}^R$, and compute the Monte Carlo average plug-in variance $\\overline{V}_{\\text{plug}} = \\frac{1}{R}\\sum_{r=1}^R \\hat{V}^{(r)}$.\n\n4. Accuracy metrics. Report, for each test case, the following floats:\n   - $V_{\\text{true}}$;\n   - the absolute relative error of the Monte Carlo variance, $|V_{\\text{mc}} - V_{\\text{true}}|/V_{\\text{true}}$;\n   - the relative bias of the plug-in estimator, $(\\overline{V}_{\\text{plug}} - V_{\\text{true}})/V_{\\text{true}}$.\n   All ratios must be reported as decimals (not percentages).\n\nImplementation details:\n- Randomness must be reproducible. Use a fixed pseudo-random seed as specified per test case.\n- All sampling within strata must be without replacement and uniformly over combinations.\n- All computations are dimensionless; no physical units are involved.\n\nTest Suite:\nProvide results for the following three test cases. For each case, use the stated replication count $R$ and seed.\n\n- Case $1$:\n  - $H = 3$,\n  - $(N_1,N_2,N_3) = (30,40,50)$,\n  - $(n_1,n_2,n_3) = (5,8,10)$,\n  - population generator: for each $h \\in \\{1,2,3\\}$ and $i \\in \\{1,\\dots,N_h\\}$,\n    $y_{h,i} = \\alpha_h + \\beta_h \\, i + \\gamma_h \\, (-1)^i$ with\n    $(\\alpha_1,\\alpha_2,\\alpha_3) = (10,20,30)$,\n    $(\\beta_1,\\beta_2,\\beta_3) = (0.5,1.0,1.5)$,\n    $(\\gamma_1,\\gamma_2,\\gamma_3) = (3,2,1)$.\n  - Replications $R = 20000$,\n  - Seed $= 202401$.\n\n- Case $2$ (edge case with a stratum of size $n_h=1$):\n  - $H = 3$,\n  - $(N_1,N_2,N_3) = (20,25,35)$,\n  - $(n_1,n_2,n_3) = (1,4,7)$,\n  - population generator: for each $h \\in \\{1,2,3\\}$ and $i \\in \\{1,\\dots,N_h\\}$,\n    $y_{h,i} = \\alpha_h + \\beta_h \\, \\sqrt{i} + \\gamma_h \\, \\left((i \\bmod 3) - 1\\right)$ with\n    $(\\alpha_1,\\alpha_2,\\alpha_3) = (5,8,12)$,\n    $(\\beta_1,\\beta_2,\\beta_3) = (2.0,1.0,0.5)$,\n    $(\\gamma_1,\\gamma_2,\\gamma_3) = (1,3,2)$.\n  - Replications $R = 20000$,\n  - Seed $= 202402$.\n\n- Case $3$ (boundary with full enumeration in one stratum to stress the finite population correction):\n  - $H = 2$,\n  - $(N_1,N_2) = (12,18)$,\n  - $(n_1,n_2) = (12,9)$,\n  - population generator: for each $h \\in \\{1,2\\}$ and $i \\in \\{1,\\dots,N_h\\}$,\n    $y_{h,i} = \\alpha_h + \\beta_h \\, i + \\gamma_h \\, \\frac{i^2}{N_h}$ with\n    $(\\alpha_1,\\alpha_2) = (0,4)$,\n    $(\\beta_1,\\beta_2) = (1.0,0.5)$,\n    $(\\gamma_1,\\gamma_2) = (0.2,0.1)$.\n  - Replications $R = 20000$,\n  - Seed $= 202403$.\n\nFinal output format:\nYour program should produce a single line of output containing a JSON-like representation of a list with three elements (one per test case), where each element is a list of the three floats in the order specified in item $4$ above, i.e.,\n$[ [V_{\\text{true}}, |V_{\\text{mc}}-V_{\\text{true}}|/V_{\\text{true}}, (\\overline{V}_{\\text{plug}}-V_{\\text{true}})/V_{\\text{true}}], \\dots ]$.\nFor example, a valid shape is $[[x_{11},x_{12},x_{13}],[x_{21},x_{22},x_{23}],[x_{31},x_{32},x_{33}]]$ with no additional text.",
            "solution": "The user-provided problem is assessed as valid. It is scientifically grounded in the principles of survey sampling and Monte Carlo simulation, well-posed with a clear objective and sufficient data, and free from any of the invalidating flaws listed in the problem validation protocol. We may therefore proceed with a full solution.\n\nThe problem is divided into two parts: Task A, a theoretical derivation of the conditional variance of the post-stratified estimator, and Task B, a numerical assessment of a plug-in variance estimator via stochastic simulation.\n\n### Task A: Derivation of the Conditional Variance\n\nWe are asked to derive the exact conditional variance $\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\})$. The post-stratified estimator for the population mean is given by:\n$$\n\\hat{\\mu}_{PS} = \\sum_{h=1}^H W_h \\bar{Y}_{h}\n$$\nwhere $W_h = N_h/N$ are the known population post-stratum weights and $\\bar{Y}_h$ is the sample mean of the $n_h$ units observed in stratum $h$.\n\nThe derivation proceeds conditionally on the realized stratum sample sizes $\\{n_h\\}_{h=1}^H$. This means we treat the $n_h$ values as fixed constants throughout the derivation. The randomness comes from the selection of specific units within each stratum, given the sample size for that stratum.\n\nThe variance of a sum of random variables involves covariance terms. Applying the variance operator to $\\hat{\\mu}_{PS}$:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\}) = \\operatorname{Var}\\left(\\sum_{h=1}^H W_h \\bar{Y}_h \\mid \\{n_h\\}\\right)\n$$\nUsing the general formula for the variance of a weighted sum, $\\operatorname{Var}(\\sum_{i} a_i X_i) = \\sum_{i} a_i^2 \\operatorname{Var}(X_i) + \\sum_{i \\neq j} a_i a_j \\operatorname{Cov}(X_i, X_j)$, we get:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\}) = \\sum_{h=1}^H W_h^2 \\operatorname{Var}(\\bar{Y}_h \\mid n_h) + \\sum_{h \\neq k} W_h W_k \\operatorname{Cov}(\\bar{Y}_h, \\bar{Y}_k \\mid \\{n_h\\})\n$$\nThe problem states that a single Simple Random Sample Without Replacement (SRSWOR) of size $n$ is drawn from the total population. However, for the purpose of analyzing the post-stratified estimator, the standard theoretical approach, and the one implied by conditioning on $\\{n_h\\}$, is to consider the sampling process as being conceptually equivalent to performing an independent SRSWOR of size $n_h$ from each stratum $h$. Under this conditional framework, the selection of units in one stratum is independent of the selection in another. Consequently, the sample means $\\bar{Y}_h$ and $\\bar{Y}_k$ for $h \\ne k$ are conditionally independent. This implies their conditional covariance is zero:\n$$\n\\operatorname{Cov}(\\bar{Y}_h, \\bar{Y}_k \\mid \\{n_h\\}) = 0 \\quad \\text{for } h \\neq k\n$$\nThe variance expression thus simplifies, as the sum of covariance terms vanishes:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\}) = \\sum_{h=1}^H W_h^2 \\operatorname{Var}(\\bar{Y}_h \\mid n_h)\n$$\nNow, we must find the expression for $\\operatorname{Var}(\\bar{Y}_h \\mid n_h)$. The problem instructs us to use the well-tested fact for the variance of a sample mean from an SRSWOR of size $n_h$ within a single finite population stratum of size $N_h$. This variance is given by:\n$$\n\\operatorname{Var}(\\bar{Y}_h \\mid n_h) = \\left(1 - \\frac{n_h}{N_h}\\right) \\frac{S_h^2}{n_h}\n$$\nwhere:\n-   $\\left(1 - \\frac{n_h}{N_h}\\right)$ is the Finite Population Correction (FPC), which accounts for the fact that sampling is without replacement from a finite population. As $n_h$ approaches $N_h$, the uncertainty about the mean decreases, vanishing completely when $n_h = N_h$.\n-   $S_h^2$ is the finite-population variance for the values $\\{y_{h,i}\\}_{i=1}^{N_h}$ in stratum $h$, defined with an $N_h-1$ denominator to be consistent with its role in sampling theory:\n    $$\n    S_h^2 = \\frac{1}{N_h-1}\\sum_{i=1}^{N_h}\\left(y_{h,i}-\\mu_h\\right)^2\n    $$\n    where $\\mu_h$ is the true mean of stratum $h$.\n\nSubstituting the expression for $\\operatorname{Var}(\\bar{Y}_h \\mid n_h)$ back into our summation, we arrive at the final derived expression for the exact conditional variance of the post-stratified estimator:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\}) = \\sum_{h=1}^H W_h^2 \\left(1 - \\frac{n_h}{N_h}\\right) \\frac{S_h^2}{n_h}\n$$\nThis is the expression we will denote as $V_{\\text{true}}$. It is a function of the population stratum sizes $\\{N_h\\}$, stratum weights $\\{W_h\\}$, realized sample sizes $\\{n_h\\}$, and the (usually unknown) true within-stratum population variances $\\{S_h^2\\}$.\n\n### Task B: Simulation Design\n\nThe simulation assesses the properties of a \"plug-in\" variance estimator, $\\hat{V}$, which is formed by substituting an estimate for the unknown population variance $S_h^2$ into the formula for $V_{\\text{true}}$. The standard unbiased estimator for $S_h^2$ based on a sample of size $n_h$ is the sample variance $s_h^2$:\n$$\ns_h^2 = \\frac{1}{n_h-1}\\sum_{j=1}^{n_h}(y_{h,j}^{\\text{sample}} - \\bar{Y}_h)^2 \\quad \\text{for } n_h \\ge 2\n$$\nThe problem specifies that by convention, $s_h^2=0$ if $n_h=1$. The plug-in estimator for the variance is therefore:\n$$\n\\hat{V} = \\sum_{h=1}^H W_h^2 \\left(1 - \\frac{n_h}{N_h}\\right)\\frac{s_h^2}{n_h}\n$$\nThe simulation proceeds as follows for each test case:\n\n1.  **Population Construction**: An immutable, finite population is constructed for each stratum $h$ according to the deterministic function $y_{h,i} = f(h,i)$ provided in the test case. This allows for the exact computation of true population parameters.\n\n2.  **True Variance Calculation ($V_{\\text{true}}$)**: For each stratum $h$, the true finite-population variance $S_h^2$ is computed from the generated population values. These, along with the given $N_h$ and $n_h$, are used in the derived formula from Task A to calculate the exact conditional variance $V_{\\text{true}}$.\n\n3.  **Monte Carlo Simulation**: A large number of replicates, $R$, are generated to approximate the sampling distribution. For each replicate $r \\in \\{1,\\dots,R\\}$:\n    a.  A new sample of size $n_h$ is drawn via SRSWOR from the population of stratum $h$, for each $h \\in \\{1,\\dots,H\\}$.\n    b.  The sample mean $\\bar{Y}_h^{(r)}$ and sample variance $s_h^{2,(r)}$ are computed for each stratum from the drawn samples. Crucially, the rule $s_h^2=0$ is applied for any stratum where $n_h=1$.\n    c.  The post-stratified estimate $\\hat{\\mu}_{PS}^{(r)} = \\sum_{h} W_h \\bar{Y}_h^{(r)}$ and its corresponding plug-in variance estimate $\\hat{V}^{(r)} = \\sum_{h} W_h^2 (1 - n_h/N_h) s_h^{2,(r)}/n_h$ are computed and stored.\n\n4.  **Metric Computation**: After all $R$ replicates are completed:\n    a.  The Monte Carlo variance, $V_{\\text{mc}}$, is calculated as the sample variance (with denominator $R-1$) of the $R$ stored values of $\\hat{\\mu}_{PS}^{(r)}$. This serves as a simulation-based estimate of $V_{\\text{true}}$.\n    b.  The average plug-in variance, $\\overline{V}_{\\text{plug}}$, is calculated as the arithmetic mean of the $R$ stored values of $\\hat{V}^{(r)}$. This approximates the expected value of the plug-in estimator, $E[\\hat{V} \\mid \\{n_h\\}]$.\n    c.  The specified accuracy metrics are computed:\n        -   $V_{\\text{true}}$\n        -   $|V_{\\text{mc}} - V_{\\text{true}}|/V_{\\text{true}}$: This measures how well the Monte Carlo simulation itself converged to the true variance. With large $R$, this should be small.\n        -   $(\\overline{V}_{\\text{plug}} - V_{\\text{true}})/V_{\\text{true}}$: This measures the relative bias of the plug-in variance estimator. Theory predicts this estimator is unbiased ($E[\\hat{V}] = V_{\\text{true}}$) if and only if all $n_h \\ge 2$. If any $n_h=1$, the estimator is expected to be biased, as $E[s_h^2 | n_h=1] = 0 \\neq S_h^2$.\n\nThe implementation will use a fixed random seed for reproducibility and will execute these steps for each of the three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the post-stratification problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"H\": 3,\n            \"N_h\": (30, 40, 50),\n            \"n_h\": (5, 8, 10),\n            \"pop_gen_params\": {\n                \"alpha\": (10, 20, 30),\n                \"beta\": (0.5, 1.0, 1.5),\n                \"gamma\": (3, 2, 1)\n            },\n            \"pop_gen_func\": lambda h, i, p: p[\"alpha\"][h-1] + p[\"beta\"][h-1] * i + p[\"gamma\"][h-1] * (-1)**i,\n            \"R\": 20000,\n            \"seed\": 202401\n        },\n        {\n            \"H\": 3,\n            \"N_h\": (20, 25, 35),\n            \"n_h\": (1, 4, 7),\n            \"pop_gen_params\": {\n                \"alpha\": (5, 8, 12),\n                \"beta\": (2.0, 1.0, 0.5),\n                \"gamma\": (1, 3, 2)\n            },\n            \"pop_gen_func\": lambda h, i, p: p[\"alpha\"][h-1] + p[\"beta\"][h-1] * np.sqrt(i) + p[\"gamma\"][h-1] * ((i % 3) - 1),\n            \"R\": 20000,\n            \"seed\": 202402\n        },\n        {\n            \"H\": 2,\n            \"N_h\": (12, 18),\n            \"n_h\": (12, 9),\n            \"pop_gen_params\": {\n                \"alpha\": (0, 4),\n                \"beta\": (1.0, 0.5),\n                \"gamma\": (0.2, 0.1)\n            },\n            \"pop_gen_func\": lambda h, i, p, N_h: p[\"alpha\"][h-1] + p[\"beta\"][h-1] * i + p[\"gamma\"][h-1] * i**2 / N_h,\n            \"R\": 20000,\n            \"seed\": 202403\n        }\n    ]\n\n    results_all_cases = []\n\n    for case_idx, case in enumerate(test_cases):\n        H = case[\"H\"]\n        N_h_tuple = case[\"N_h\"]\n        n_h_tuple = case[\"n_h\"]\n        pop_gen_params = case[\"pop_gen_params\"]\n        pop_gen_func = case[\"pop_gen_func\"]\n        R = case[\"R\"]\n        seed = case[\"seed\"]\n\n        rng = np.random.default_rng(seed)\n\n        # 1. Deterministic population construction\n        populations = []\n        for h_idx in range(H):\n            h = h_idx + 1\n            N_h_val = N_h_tuple[h_idx]\n            indices = np.arange(1, N_h_val + 1)\n            if case_idx == 2: # Case 3 has a different generator signature\n                y_values = pop_gen_func(h, indices, pop_gen_params, N_h_val)\n            else:\n                y_values = pop_gen_func(h, indices, pop_gen_params)\n            populations.append(y_values)\n        \n        # 2. Exact conditional variance\n        N_h_arr = np.array(N_h_tuple, dtype=float)\n        n_h_arr = np.array(n_h_tuple, dtype=float)\n        \n        N_total = np.sum(N_h_arr)\n        W_h_arr = N_h_arr / N_total\n        \n        # Calculate true population variances S_h^2\n        S_h_sq_arr = np.array([np.var(pop, ddof=1) if len(pop) > 1 else 0 for pop in populations])\n        \n        # Finite Population Correction\n        fpc_arr = 1.0 - n_h_arr / N_h_arr\n        \n        # Calculate V_true\n        # Add a small epsilon to denominator to avoid division by zero warning if n_h=0, though not in test cases\n        \n        # If n_h_arr[h] is zero, the term is undefined. Here n_h >= 1.\n        # If S_h_sq_arr[h] is zero, the term is zero.\n        # If fpc_arr[h] is zero (n_h=N_h), the term is zero.\n        # The logic below handles these cases correctly.\n        V_true_terms = np.zeros_like(W_h_arr, dtype=float)\n        valid_n_h_mask = n_h_arr > 0\n        V_true_terms[valid_n_h_mask] = (W_h_arr[valid_n_h_mask]**2 * fpc_arr[valid_n_h_mask] * S_h_sq_arr[valid_n_h_mask]) / n_h_arr[valid_n_h_mask]\n        V_true = np.sum(V_true_terms)\n\n        # 3. Monte Carlo approximation\n        mu_ps_reps = np.zeros(R)\n        V_hat_reps = np.zeros(R)\n\n        for r in range(R):\n            mu_ps_r = 0.0\n            V_hat_r = 0.0\n            for h_idx in range(H):\n                # Draw SRSWOR sample for stratum h\n                sample = rng.choice(populations[h_idx], size=int(n_h_arr[h_idx]), replace=False)\n                \n                # Compute sample mean\n                y_bar_h = np.mean(sample)\n                \n                # Compute sample variance s_h^2, with rule for n_h = 1\n                s_h_sq_r = 0.0\n                if n_h_arr[h_idx] > 1:\n                    s_h_sq_r = np.var(sample, ddof=1)\n                \n                # Accumulate for mu_ps and V_hat for this replicate\n                mu_ps_r += W_h_arr[h_idx] * y_bar_h\n                \n                # The contribution to variance is zero if n_h = 0. Problem has n_h >= 1.\n                if n_h_arr[h_idx] > 0:\n                    V_hat_r += (W_h_arr[h_idx]**2 * fpc_arr[h_idx] * s_h_sq_r) / n_h_arr[h_idx]\n            \n            mu_ps_reps[r] = mu_ps_r\n            V_hat_reps[r] = V_hat_r\n\n        # Calculate Monte Carlo variance and average plug-in variance\n        V_mc = np.var(mu_ps_reps, ddof=1)\n        V_plug_bar = np.mean(V_hat_reps)\n\n        # 4. Accuracy metrics\n        # Avoid division by zero if V_true is zero\n        if V_true == 0:\n            rel_err_V_mc = 0.0 if V_mc == 0.0 else np.inf\n            rel_bias_V_plug = 0.0 if V_plug_bar == 0.0 else np.inf\n        else:\n            rel_err_V_mc = np.abs(V_mc - V_true) / V_true\n            rel_bias_V_plug = (V_plug_bar - V_true) / V_true\n\n        results_all_cases.append([V_true, rel_err_V_mc, rel_bias_V_plug])\n    \n    # Final print statement in the exact required format.\n    print(f\"{results_all_cases}\")\n\nsolve()\n```"
        },
        {
            "introduction": "In many real-world applications, the true population margins required for post-stratification are not known perfectly and must themselves be estimated from an external data source, such as a high-quality census or survey. This exercise demonstrates how to quantify the additional variance introduced by the uncertainty in these estimated margins. By applying the delta method, you will learn to derive and calculate this crucial component of variance, leading to more honest and accurate statements of an estimator's precision .",
            "id": "3330465",
            "problem": "Consider a finite population partitioned into $K=3$ mutually exclusive post-strata indexed by $k \\in \\{1,2,3\\}$ according to an auxiliary categorical variable $Z$. Let $p_k = \\mathbb{P}(Z=k)$ denote the population margin for stratum $k$ and let $\\mu_k = \\mathbb{E}[Y \\mid Z=k]$ denote the stratum-specific mean of a scalar outcome $Y$. The population mean of $Y$ can be expressed as $\\theta = \\sum_{k=1}^{3} p_k \\mu_k$.\n\nA Monte Carlo sample of size $n$ produces independent stratum mean estimators $\\hat{\\mu}_k$ for $k=1,2,3$, with $\\mathbb{E}[\\hat{\\mu}_k] = \\mu_k$ and $\\operatorname{Var}(\\hat{\\mu}_k) = v_k$, and assume $\\operatorname{Cov}(\\hat{\\mu}_i,\\hat{\\mu}_j)=0$ for $i \\neq j$. Post-stratification uses the population margins to form the estimator $\\hat{\\theta}_{\\mathrm{PS}} = \\sum_{k=1}^{3} p_k \\hat{\\mu}_k$.\n\nIn practice, the margins $p_k$ are not known and are estimated from an independent external probability survey of size $m$ using simple random sampling from the same population, yielding $\\hat{p}_k = N_k/m$ where $(N_1,N_2,N_3) \\sim \\operatorname{Multinomial}(m; p_1,p_2,p_3)$. Assume the external survey is independent of the Monte Carlo sample. The implemented estimator is therefore $\\tilde{\\theta} = \\sum_{k=1}^{3} \\hat{p}_k \\hat{\\mu}_k$.\n\nUsing the first-order Taylor expansion and the delta method under the Central Limit Theorem (CLT) assumptions for $\\hat{\\mu}_k$ and $\\hat{p}_k$, do the following:\n\n1. Derive the first-order bias of $\\tilde{\\theta}$ relative to $\\theta$ and state whether margin estimation introduces first-order bias.\n2. Derive the first-order variance of $\\tilde{\\theta}$, clearly identifying the additional variance component attributable to margin uncertainty (i.e., the sampling error in $\\hat{p}_k$).\n3. Evaluate numerically the additional variance component due to margin uncertainty for the following values: $\\mathbf{p} = (0.52, 0.31, 0.17)$, $\\boldsymbol{\\mu} = (1.1, 0.7, 1.6)$, and $m = 1500$. Round your numerical answer to four significant figures.\n\nYour final reported value must be the single real number corresponding to the additional variance component due solely to margin uncertainty in $\\hat{p}_k$.",
            "solution": "The problem is valid. It is a well-posed, self-contained, and scientifically grounded problem in the field of statistical simulation and survey methodology. All necessary data and definitions are provided, and there are no contradictions or ambiguities.\n\nLet the estimator be $\\tilde{\\theta} = \\sum_{k=1}^{3} \\hat{p}_k \\hat{\\mu}_k$. This estimator is a function of two sets of random variables: the estimated population margins $\\{\\hat{p}_k\\}_{k=1}^3$ from an external survey of size $m$, and the estimated stratum means $\\{\\hat{\\mu}_k\\}_{k=1}^3$ from a Monte Carlo sample of size $n$. The true values are $\\{p_k\\}_{k=1}^3$ and $\\{\\mu_k\\}_{k=1}^3$. The target parameter is $\\theta = \\sum_{k=1}^{3} p_k \\mu_k$.\n\nWe are given:\n- The external survey yields $(N_1, N_2, N_3) \\sim \\operatorname{Multinomial}(m; p_1, p_2, p_3)$, with $\\hat{p}_k = N_k/m$. From the properties of the multinomial distribution, we have $\\mathbb{E}[\\hat{p}_k] = p_k$, $\\operatorname{Var}(\\hat{p}_k) = \\frac{p_k(1-p_k)}{m}$, and for $i \\neq j$, $\\operatorname{Cov}(\\hat{p}_i, \\hat{p}_j) = -\\frac{p_i p_j}{m}$.\n- The Monte Carlo sample yields unbiased estimators $\\hat{\\mu}_k$ such that $\\mathbb{E}[\\hat{\\mu}_k] = \\mu_k$ and $\\operatorname{Var}(\\hat{\\mu}_k) = v_k$. The estimators are uncorrelated, so $\\operatorname{Cov}(\\hat{\\mu}_i, \\hat{\\mu}_j) = 0$ for $i \\neq j$.\n- The two samples are independent, so $\\operatorname{Cov}(\\hat{p}_i, \\hat{\\mu}_j) = 0$ for all $i,j$.\n\n### 1. First-Order Bias of $\\tilde{\\theta}$\n\nThe bias of an estimator $\\tilde{\\theta}$ is defined as $\\operatorname{Bias}(\\tilde{\\theta}) = \\mathbb{E}[\\tilde{\\theta}] - \\theta$. We can compute the exact expectation of $\\tilde{\\theta}$:\n$$\n\\mathbb{E}[\\tilde{\\theta}] = \\mathbb{E}\\left[\\sum_{k=1}^{3} \\hat{p}_k \\hat{\\mu}_k\\right]\n$$\nUsing the linearity of expectation:\n$$\n\\mathbb{E}[\\tilde{\\theta}] = \\sum_{k=1}^{3} \\mathbb{E}[\\hat{p}_k \\hat{\\mu}_k]\n$$\nSince the external survey and the Monte Carlo sample are independent, the estimators $\\hat{p}_k$ and $\\hat{\\mu}_k$ are independent. Therefore, the expectation of their product is the product of their expectations:\n$$\n\\mathbb{E}[\\hat{p}_k \\hat{\\mu}_k] = \\mathbb{E}[\\hat{p}_k] \\mathbb{E}[\\hat{\\mu}_k]\n$$\nWe are given that $\\mathbb{E}[\\hat{\\mu}_k] = \\mu_k$ and we know that $\\mathbb{E}[\\hat{p}_k] = p_k$. Substituting these into the expression:\n$$\n\\mathbb{E}[\\tilde{\\theta}] = \\sum_{k=1}^{3} p_k \\mu_k = \\theta\n$$\nThe bias is therefore:\n$$\n\\operatorname{Bias}(\\tilde{\\theta}) = \\mathbb{E}[\\tilde{\\theta}] - \\theta = \\theta - \\theta = 0\n$$\nThe estimator $\\tilde{\\theta}$ is exactly unbiased. This implies that the first-order bias is also zero. A first-order Taylor expansion of $\\tilde{\\theta}$ around the true values $(\\mathbf{p}, \\boldsymbol{\\mu})$ is given by:\n$$\n\\tilde{\\theta} \\approx \\theta + \\sum_{k=1}^{3} \\frac{\\partial \\tilde{\\theta}}{\\partial p_k}\\bigg|_{(\\mathbf{p}, \\boldsymbol{\\mu})} (\\hat{p}_k-p_k) + \\sum_{k=1}^{3} \\frac{\\partial \\tilde{\\theta}}{\\partial \\mu_k}\\bigg|_{(\\mathbf{p}, \\boldsymbol{\\mu})} (\\hat{\\mu}_k-\\mu_k)\n$$\nwhere the derivatives are $\\mu_k$ and $p_k$ respectively.\n$$\n\\tilde{\\theta}_{\\text{1st-order}} = \\theta + \\sum_{k=1}^{3} \\mu_k (\\hat{p}_k-p_k) + \\sum_{k=1}^{3} p_k (\\hat{\\mu}_k-\\mu_k)\n$$\nThe expectation of this first-order approximation is:\n$$\n\\mathbb{E}[\\tilde{\\theta}_{\\text{1st-order}}] = \\mathbb{E}[\\theta] + \\sum_{k=1}^{3} \\mu_k \\mathbb{E}[\\hat{p}_k-p_k] + \\sum_{k=1}^{3} p_k \\mathbb{E}[\\hat{\\mu}_k-\\mu_k] = \\theta + 0 + 0 = \\theta\n$$\nThus, the bias of the first-order approximation is zero. Margin estimation does not introduce first-order bias.\n\n### 2. First-Order Variance of $\\tilde{\\theta}$\n\nWe use the delta method, which approximates the variance of a function of random variables using a first-order Taylor expansion. From the expansion above:\n$$\n\\tilde{\\theta} - \\theta \\approx \\sum_{k=1}^{3} \\mu_k (\\hat{p}_k-p_k) + \\sum_{k=1}^{3} p_k (\\hat{\\mu}_k-\\mu_k)\n$$\nThe variance of $\\tilde{\\theta}$ is approximated by the variance of this linear combination:\n$$\n\\operatorname{Var}(\\tilde{\\theta}) \\approx \\operatorname{Var}\\left( \\sum_{k=1}^{3} \\mu_k (\\hat{p}_k-p_k) + \\sum_{k=1}^{3} p_k (\\hat{\\mu}_k-\\mu_k) \\right)\n$$\nThe two sums correspond to the two independent sampling processes (for $\\hat{p}_k$ and $\\hat{\\mu}_k$). Therefore, the covariance between the two sum terms is zero, and the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}(\\tilde{\\theta}) \\approx \\operatorname{Var}\\left( \\sum_{k=1}^{3} \\mu_k \\hat{p}_k \\right) + \\operatorname{Var}\\left( \\sum_{k=1}^{3} p_k \\hat{\\mu}_k \\right)\n$$\nThe second term is the variance of the post-stratified estimator $\\hat{\\theta}_{\\mathrm{PS}}$ where margins are known:\n$$\n\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{PS}}) = \\operatorname{Var}\\left( \\sum_{k=1}^{3} p_k \\hat{\\mu}_k \\right) = \\sum_{k=1}^{3} p_k^2 \\operatorname{Var}(\\hat{\\mu}_k) = \\sum_{k=1}^{3} p_k^2 v_k\n$$\nThis is the variance component due to the estimation of the stratum means $\\mu_k$.\n\nThe first term is the additional variance component attributable to margin uncertainty (the sampling error in $\\hat{p}_k$). Let's denote this component by $V_{\\text{add}}$:\n$$\nV_{\\text{add}} = \\operatorname{Var}\\left( \\sum_{k=1}^{3} \\mu_k \\hat{p}_k \\right)\n$$\nUsing the formula for the variance of a linear combination of correlated variables:\n$$\nV_{\\text{add}} = \\sum_{i=1}^{3} \\sum_{j=1}^{3} \\mu_i \\mu_j \\operatorname{Cov}(\\hat{p}_i, \\hat{p}_j)\n$$\nSubstitute the covariance formula for the multinomial proportions, $\\operatorname{Cov}(\\hat{p}_i, \\hat{p}_j) = \\frac{1}{m}(\\delta_{ij} p_i - p_i p_j)$, where $\\delta_{ij}$ is the Kronecker delta:\n$$\nV_{\\text{add}} = \\sum_{i=1}^{3} \\sum_{j=1}^{3} \\mu_i \\mu_j \\frac{1}{m}(\\delta_{ij} p_i - p_i p_j) = \\frac{1}{m} \\left( \\sum_{i=1}^{3} \\mu_i^2 p_i - \\sum_{i=1}^{3}\\sum_{j=1}^{3} \\mu_i p_i \\mu_j p_j \\right)\n$$\nThe second sum is a product of two identical sums:\n$$\n\\sum_{i=1}^{3}\\sum_{j=1}^{3} (\\mu_i p_i) (\\mu_j p_j) = \\left( \\sum_{k=1}^{3} \\mu_k p_k \\right) \\left( \\sum_{k=1}^{3} \\mu_k p_k \\right) = \\left( \\sum_{k=1}^{3} p_k \\mu_k \\right)^2 = \\theta^2\n$$\nThe first sum is the population expectation of $\\mu^2$ over the strata: $\\sum_{k=1}^{3} p_k \\mu_k^2 = \\mathbb{E}_{Z}[\\mu^2]$.\nThus, the additional variance component is:\n$$\nV_{\\text{add}} = \\frac{1}{m} \\left( \\sum_{k=1}^{3} p_k \\mu_k^2 - \\theta^2 \\right) = \\frac{1}{m} \\sum_{k=1}^{3} p_k (\\mu_k - \\theta)^2\n$$\nThis term represents the population variance of the stratum means, scaled by the external survey size $m$.\n\nThe full first-order variance of $\\tilde{\\theta}$ is:\n$$\n\\operatorname{Var}(\\tilde{\\theta}) \\approx \\sum_{k=1}^{3} p_k^2 v_k + \\frac{1}{m} \\sum_{k=1}^{3} p_k (\\mu_k - \\theta)^2\n$$\n\n### 3. Numerical Evaluation of the Additional Variance Component\n\nThe problem requires a numerical evaluation of the additional variance component $V_{\\text{add}}$ for the given values:\n- $\\mathbf{p} = (0.52, 0.31, 0.17)$\n- $\\boldsymbol{\\mu} = (1.1, 0.7, 1.6)$\n- $m = 1500$\n\nFirst, we compute the population mean $\\theta = \\sum_{k=1}^{3} p_k \\mu_k$:\n$$\n\\theta = (0.52)(1.1) + (0.31)(0.7) + (0.17)(1.6) = 0.572 + 0.217 + 0.272 = 1.061\n$$\nNext, we compute the population variance of the stratum means, $\\sum_{k=1}^{3} p_k (\\mu_k-\\theta)^2$:\n- For $k=1$: $p_1(\\mu_1-\\theta)^2 = 0.52(1.1 - 1.061)^2 = 0.52(0.039)^2 = 0.52(0.001521) = 0.00079092$\n- For $k=2$: $p_2(\\mu_2-\\theta)^2 = 0.31(0.7 - 1.061)^2 = 0.31(-0.361)^2 = 0.31(0.130321) = 0.04039951$\n- For $k=3$: $p_3(\\mu_3-\\theta)^2 = 0.17(1.6 - 1.061)^2 = 0.17(0.539)^2 = 0.17(0.290521) = 0.04938857$\n\nThe sum is:\n$$\n\\sum_{k=1}^{3} p_k (\\mu_k - \\theta)^2 = 0.00079092 + 0.04039951 + 0.04938857 = 0.090579\n$$\nFinally, we calculate the additional variance component $V_{\\text{add}}$:\n$$\nV_{\\text{add}} = \\frac{1}{m} \\sum_{k=1}^{3} p_k (\\mu_k - \\theta)^2 = \\frac{0.090579}{1500} \\approx 0.000060386\n$$\nRounding to four significant figures, we get:\n$$\nV_{\\text{add}} \\approx 0.00006039\n$$\nThis is the single real number asked for in the final answer.",
            "answer": "$$\n\\boxed{0.00006039}\n$$"
        },
        {
            "introduction": "A robust statistical analysis goes beyond simply calculating an estimate and its standard error; it requires diagnosing the stability and reliability of the result. This practice introduces essential diagnostic tools to assess the health of a post-stratified estimator. You will learn to compute measures of influence and leverage that reveal how much individual observations or entire post-strata contribute to the final estimate and its variance, allowing you to identify potential sources of instability and build more trustworthy models .",
            "id": "3330477",
            "problem": "Consider a finite population partitioned into $H$ post-strata (cells). Let the true population composition across cells be given by the vector of cell proportions $\\{p_h\\}_{h=1}^H$ satisfying $\\sum_{h=1}^H p_h = 1$ and $p_h \\in (0,1)$. From a sample of size $n$, suppose that the realized sample counts per cell are $\\{n_h\\}_{h=1}^H$, with $\\sum_{h=1}^H n_h = n$ and $n_h \\in \\mathbb{N}$, $n_h \\geq 1$. Assume that within each cell $h$, the sample provides a cell mean $\\bar{y}_h$ and a within-cell sample variance estimate $\\hat{\\sigma}_h^2$ for the outcome of interest. Consider the standard post-stratified estimator of the population mean that aggregates cell means weighted by the population composition.\n\nYou will construct diagnostics that quantify how many observational units and how many cells effectively contribute to the estimator, and that flag extreme weights using leverage or influence measures indicative of potential variance inflation. These diagnostics must be derived from first principles starting from the following fundamental bases:\n\n- The Horvitz–Thompson characterization of weighted mean estimation: the estimator of the mean is expressed as a ratio of weighted sums, with weights calibrated so that the weighted sample reproduces the known population composition.\n- First-order Taylor linearization of the variance of a smooth estimator: the asymptotic variance of the post-stratified mean can be expressed as a sum of cell-specific contributions depending on $p_h$, $n_h$, and $\\hat{\\sigma}_h^2$ under independence across cells and within-cell sampling assumptions.\n\nDefine the following quantities solely from the given inputs, without introducing any external parameters:\n\n1. The observation-level weights implied by post-stratification and the associated normalized influence for each observation as the sensitivity of the estimator to a perturbation in that observation’s value, scaled to sum to $1$ across all observations. Use this to quantify the effective number of observational units contributing to the estimator.\n\n2. The cell-level variance contributions from a first-order linearization of the post-stratified mean’s variance and the associated normalized variance-leverage for each cell. Use this to quantify the effective number of variance-contributing cells.\n\n3. An influence-based diagnostic to flag extreme observations: classify an observation as extreme if its normalized influence exceeds $2/n$.\n\n4. A leverage-based diagnostic to flag extreme cells with respect to variance: classify a cell as extreme if its normalized variance-leverage exceeds $2/H$.\n\nIf the total linearized variance aggregation is zero, define the variance-leverage vector as all zeros and the effective number of variance-contributing cells as $H$ by convention.\n\nYour task is to implement a program that, for each test case, computes a list containing the following six quantities in this exact order:\n\n- The effective number of observational units, expressed as a real number.\n- The effective number of variance-contributing cells, expressed as a real number.\n- The count of extreme observations (an integer) under the observation influence rule described above.\n- The count of extreme cells (an integer) under the variance-leverage rule described above.\n- The maximum observation-level normalized influence value across all observations, expressed as a real number.\n- The maximum cell-level normalized variance-leverage value across all cells, expressed as a real number.\n\nYour program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case’s six-tuple appears as a bracketed, comma-separated list with no spaces. For example, the output format must be exactly of the form $[ [a_1,a_2,a_3,a_4,a_5,a_6],[b_1,b_2,b_3,b_4,b_5,b_6],\\ldots ]$ but without any spaces anywhere.\n\nUse the following test suite of parameter values, which covers a general case, a case with a highly sparse cell that should induce extreme observation weights, and a boundary case with a zero-variance cell:\n\n- Test Case 1:\n  - $H = 5$\n  - $p = [0.15, 0.25, 0.10, 0.30, 0.20]$\n  - $n_h = [60, 80, 40, 70, 50]$\n  - $n = 300$\n  - $\\hat{\\sigma}^2 = [1.0, 0.8, 1.2, 0.9, 1.1]$\n\n- Test Case 2:\n  - $H = 4$\n  - $p = [0.40, 0.30, 0.20, 0.10]$\n  - $n_h = [10, 90, 80, 70]$\n  - $n = 250$\n  - $\\hat{\\sigma}^2 = [1.0, 0.8, 0.7, 0.6]$\n\n- Test Case 3:\n  - $H = 3$\n  - $p = [0.50, 0.30, 0.20]$\n  - $n_h = [150, 1, 49]$\n  - $n = 200$\n  - $\\hat{\\sigma}^2 = [1.0, 0.0, 1.5]$\n\nYour program must not read any input; it must embed the above test suite and print the single-line output in the exact required format. All answers are unitless real numbers or integers. The implementation must be consistent with the derivations from the fundamental bases described above and must rely exclusively on the given parameters $H$, $p$, $n_h$, $n$, and $\\hat{\\sigma}_h^2$.",
            "solution": "The problem asks for the derivation and computation of several diagnostic quantities for a post-stratified estimator of a population mean. The derivations will be based on first principles as specified. Let the number of post-strata be $H$, the population cell proportions be $\\{p_h\\}_{h=1}^H$, the realized sample cell counts be $\\{n_h\\}_{h=1}^H$, the total sample size be $n = \\sum_{h=1}^H n_h$, and the within-cell sample variance estimates be $\\{\\hat{\\sigma}_h^2\\}_{h=1}^H$.\n\nThe post-stratified estimator of the population mean, $\\mu$, is given by:\n$$ \\hat{\\mu}_{ps} = \\sum_{h=1}^H p_h \\bar{y}_h $$\nwhere $\\bar{y}_h$ is the sample mean of the outcome variable in cell $h$. Let the individual observations in the sample be indexed by $i=1, \\dots, n$, and let $c(i)$ denote the cell to which observation $i$ belongs. The cell mean is $\\bar{y}_h = \\frac{1}{n_h} \\sum_{i: c(i)=h} y_i$. Substituting this into the estimator gives:\n$$ \\hat{\\mu}_{ps} = \\sum_{h=1}^H p_h \\left( \\frac{1}{n_h} \\sum_{i: c(i)=h} y_i \\right) = \\sum_{h=1}^H \\sum_{i: c(i)=h} \\frac{p_h}{n_h} y_i $$\nThis reveals that the estimator is a weighted sum of the individual observations, $\\hat{\\mu}_{ps} = \\sum_{i=1}^n w_i y_i$, where the weight $w_i$ for an observation $i$ in cell $c(i)$ is given by:\n$$ w_i = w_{c(i)} = \\frac{p_{c(i)}}{n_{c(i)}} $$\nIt is straightforward to verify that these weights are calibrated to sum to $1$:\n$$ \\sum_{i=1}^n w_i = \\sum_{h=1}^H \\sum_{i: c(i)=h} w_i = \\sum_{h=1}^H n_h \\left( \\frac{p_h}{n_h} \\right) = \\sum_{h=1}^H p_h = 1 $$\n\nWith these weights established, we can derive the required diagnostic quantities.\n\n**1. Observation-Level Diagnostics and Effective Number of Units**\n\nThe problem defines the normalized influence of an observation as the sensitivity of the estimator to that observation's value, scaled to sum to $1$. The sensitivity of $\\hat{\\mu}_{ps}$ to a change in $y_i$ is its partial derivative:\n$$ \\frac{\\partial \\hat{\\mu}_{ps}}{\\partial y_i} = \\frac{\\partial}{\\partial y_i} \\sum_{j=1}^n w_j y_j = w_i $$\nSince the weights $w_i$ already sum to $1$, they are themselves the normalized influence values, which we denote as $\\text{infl}_i = w_i$.\n\nThe effective number of observational units, $n_{eff, obs}$, can be quantified using Kish's effective sample size formula for a set of weights that sum to $1$:\n$$ n_{eff, obs} = \\frac{\\left(\\sum_{i=1}^n w_i\\right)^2}{\\sum_{i=1}^n w_i^2} = \\frac{1}{\\sum_{i=1}^n w_i^2} $$\nThe sum of squared weights can be calculated by grouping observations by cell:\n$$ \\sum_{i=1}^n w_i^2 = \\sum_{h=1}^H \\sum_{i: c(i)=h} \\left(\\frac{p_h}{n_h}\\right)^2 = \\sum_{h=1}^H n_h \\frac{p_h^2}{n_h^2} = \\sum_{h=1}^H \\frac{p_h^2}{n_h} $$\nTherefore, the effective number of observational units is:\n$$ n_{eff, obs} = \\frac{1}{\\sum_{h=1}^H p_h^2 / n_h} $$\nAn observation is flagged as extreme if its influence exceeds $2/n$. For an observation in cell $h$, this condition is $\\text{infl}_i = p_h/n_h > 2/n$. If this holds, all $n_h$ observations in that cell are classified as extreme. The total count of extreme observations is the sum of $n_h$ over all cells $h$ that satisfy this criterion. The maximum observation-level influence is simply $\\max_{h} (p_h/n_h)$.\n\n**2. Cell-Level Diagnostics and Effective Number of Cells**\n\nThe variance of the post-stratified estimator, conditional on the realized sample sizes $n_h$ and assuming independence of sampling across cells, is:\n$$ \\text{Var}(\\hat{\\mu}_{ps}) = \\text{Var}\\left(\\sum_{h=1}^H p_h \\bar{y}_h\\right) = \\sum_{h=1}^H p_h^2 \\text{Var}(\\bar{y}_h) $$\nUsing the provided sample variance estimates $\\hat{\\sigma}_h^2$ for the population variance $\\sigma_h^2$ within each cell, and the formula $\\text{Var}(\\bar{y}_h) \\approx \\hat{\\sigma}_h^2/n_h$, the estimated variance of $\\hat{\\mu}_{ps}$ is:\n$$ \\hat{V}(\\hat{\\mu}_{ps}) = \\sum_{h=1}^H \\frac{p_h^2 \\hat{\\sigma}_h^2}{n_h} $$\nThis expression represents the total variance as a sum of cell-specific contributions, $V_h = p_h^2 \\hat{\\sigma}_h^2 / n_h$. The \"normalized variance-leverage\" for cell $h$, denoted $\\lambda_h$, is its fractional contribution to the total variance:\n$$ \\lambda_h = \\frac{V_h}{\\sum_{j=1}^H V_j} = \\frac{p_h^2 \\hat{\\sigma}_h^2 / n_h}{\\sum_{j=1}^H p_j^2 \\hat{\\sigma}_j^2 / n_j} $$\nThese leverages sum to $1$ (provided the total variance is non-zero). The effective number of variance-contributing cells, $n_{eff, cell}$, is calculated using the same Kish formula applied to the cell-level leverages:\n$$ n_{eff, cell} = \\frac{1}{\\sum_{h=1}^H \\lambda_h^2} $$\nAs per the problem statement, if the total variance $\\sum_j V_j=0$, we define $\\lambda_h=0$ for all $h$, and $n_{eff, cell}=H$.\nA cell is flagged as extreme if its leverage exceeds $2/H$, i.e., $\\lambda_h > 2/H$. The total count is the number of cells satisfying this. The maximum cell-level leverage is $\\max_h \\lambda_h$.\n\n**Summary of Formulas for Computation:**\n1.  **Effective number of observational units**: $n_{eff, obs} = 1 / (\\sum_{h=1}^H p_h^2 / n_h)$\n2.  **Effective number of variance-contributing cells**:\n    - Let $V_h = p_h^2 \\hat{\\sigma}_h^2 / n_h$ and $V_{tot} = \\sum_h V_h$.\n    - If $V_{tot} = 0$, $n_{eff, cell} = H$.\n    - Otherwise, $\\lambda_h = V_h / V_{tot}$ and $n_{eff, cell} = 1 / (\\sum_{h=1}^H \\lambda_h^2)$.\n3.  **Count of extreme observations**: $\\sum_{h=1}^H n_h \\cdot \\mathbf{1}(p_h/n_h > 2/n)$, where $\\mathbf{1}(\\cdot)$ is the indicator function.\n4.  **Count of extreme cells**: $\\sum_{h=1}^H \\mathbf{1}(\\lambda_h > 2/H)$, where $\\lambda_h$ is defined as $0$ if $V_{tot}=0$.\n5.  **Maximum observation-level normalized influence**: $\\max_{h} (p_h / n_h)$.\n6.  **Maximum cell-level normalized variance-leverage**: $\\max_h \\lambda_h$, defined as $0$ if $V_{tot}=0$.\nThese formulas are implemented below for the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the post-stratification diagnostics problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        (5, [0.15, 0.25, 0.10, 0.30, 0.20], [60, 80, 40, 70, 50], 300, [1.0, 0.8, 1.2, 0.9, 1.1]),\n        # Test Case 2\n        (4, [0.40, 0.30, 0.20, 0.10], [10, 90, 80, 70], 250, [1.0, 0.8, 0.7, 0.6]),\n        # Test Case 3\n        (3, [0.50, 0.30, 0.20], [150, 1, 49], 200, [1.0, 0.0, 1.5]),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        H, p_list, n_h_list, n, sigma2_list = case\n        \n        p = np.array(p_list, dtype=float)\n        n_h = np.array(n_h_list, dtype=float)\n        sigma2 = np.array(sigma2_list, dtype=float)\n\n        # 1. Effective number of observational units\n        sum_sq_weights = np.sum(p**2 / n_h)\n        n_eff_obs = 1.0 / sum_sq_weights\n\n        # 3.  5. Observation-level diagnostics\n        obs_influences = p / n_h\n        # 5. Maximum observation-level normalized influence\n        max_obs_influence = np.max(obs_influences)\n        \n        # 3. Count of extreme observations\n        threshold_obs = 2.0 / n\n        extreme_obs_mask = obs_influences > threshold_obs\n        count_extreme_obs = int(np.sum(n_h[extreme_obs_mask]))\n\n        # 2., 4.,  6. Cell-level diagnostics\n        V_h = (p**2 * sigma2) / n_h\n        V_tot = np.sum(V_h)\n\n        if V_tot == 0:\n            # Special case as defined in the problem\n            # 2. Effective number of variance-contributing cells\n            n_eff_cell = float(H)\n            # 6. Maximum cell-level normalized variance-leverage\n            max_cell_leverage = 0.0\n            # 4. Count of extreme cells\n            count_extreme_cells = 0\n        else:\n            cell_leverages = V_h / V_tot\n            # 2. Effective number of variance-contributing cells\n            n_eff_cell = 1.0 / np.sum(cell_leverages**2)\n            # 6. Maximum cell-level normalized variance-leverage\n            max_cell_leverage = np.max(cell_leverages)\n            \n            # 4. Count of extreme cells\n            threshold_cell = 2.0 / H\n            count_extreme_cells = int(np.sum(cell_leverages > threshold_cell))\n            \n        result_tuple = [\n            n_eff_obs,\n            n_eff_cell,\n            count_extreme_obs,\n            count_extreme_cells,\n            max_obs_influence,\n            max_cell_leverage\n        ]\n        all_results.append(result_tuple)\n\n    # Final print statement in the exact required format.\n    # Format each list of results into \"[v1,v2,...]\" and join them.\n    formatted_results = []\n    for res_list in all_results:\n        # Using a general a number format that avoids scientific notation for\n        # some values and keeps precision, and converting ints to int strings.\n        s_list = []\n        for v in res_list:\n            if isinstance(v, int):\n                s_list.append(str(v))\n            else:\n                s_list.append(format(v, '.17g').rstrip('0').rstrip('.'))\n        \n        inner_str = \"[\" + \",\".join(s_list) + \"]\"\n        formatted_results.append(inner_str)\n    \n    final_output_string = \"[\" + \",\".join(formatted_results) + \"]\"\n    print(final_output_string.replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}