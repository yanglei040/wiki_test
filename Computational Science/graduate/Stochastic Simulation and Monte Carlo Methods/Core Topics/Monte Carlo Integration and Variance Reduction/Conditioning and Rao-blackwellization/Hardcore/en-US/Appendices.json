{
    "hands_on_practices": [
        {
            "introduction": "The Rao-Blackwell theorem provides more than just a theoretical guarantee; it offers a constructive method for improving estimators. This first exercise puts the theorem into practice in the familiar context of comparing two normal populations. By starting with a simple, intuitive, yet inefficient estimator, we will systematically apply the conditioning procedure to derive the optimal unbiased estimator, revealing how the formal mechanics of the theorem lead to an intuitively satisfying and statistically powerful result .",
            "id": "1922449",
            "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample from a normal distribution with an unknown mean $\\mu$ and an unknown variance $\\sigma^2 > 0$. Independently, let $Y_1, Y_2, \\dots, Y_m$ be another random sample from a normal distribution with an unknown mean $\\theta$ and the same unknown variance $\\sigma^2$. The primary parameter of interest is the difference between the means, $\\delta = \\mu - \\theta$.\n\nConsider the simple estimator for $\\delta$ given by $T = X_1 - Y_1$. While this estimator is unbiased for $\\delta$, its variance may be large, making it potentially inefficient for estimation.\n\nYour task is to find the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for $\\delta$. Express your answer as an analytic expression in terms of the sample values $X_i$ and $Y_j$, and the sample sizes $n$ and $m$.",
            "solution": "To find the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for $\\delta = \\mu - \\theta$, we will use the Lehmann-Scheffé theorem. This involves finding a complete sufficient statistic for the parameters $(\\mu, \\theta, \\sigma^2)$ and then finding a function of this statistic that is an unbiased estimator for $\\delta$. A practical way to do this is to start with a simple unbiased estimator and use the Rao-Blackwell theorem to improve it by conditioning on the complete sufficient statistic.\n\nFirst, let's confirm that the proposed estimator $T = X_1 - Y_1$ is unbiased for $\\delta$.\nThe expectation of $T$ is:\n$$E[T] = E[X_1 - Y_1] = E[X_1] - E[Y_1]$$\nSince $X_1$ is from a population with mean $\\mu$ and $Y_1$ is from a population with mean $\\theta$, we have:\n$$E[T] = \\mu - \\theta = \\delta$$\nSo, $T$ is indeed an unbiased estimator for $\\delta$.\n\nNext, we find a complete sufficient statistic for the parameter vector $(\\mu, \\theta, \\sigma^2)$. The joint probability density function (PDF) of the two samples $\\mathbf{X} = (X_1, \\dots, X_n)$ and $\\mathbf{Y} = (Y_1, \\dots, Y_m)$ is the product of their individual joint PDFs due to independence:\n$$f(\\mathbf{x}, \\mathbf{y} | \\mu, \\theta, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) \\prod_{j=1}^m \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_j - \\theta)^2}{2\\sigma^2}\\right)$$\n$$f(\\mathbf{x}, \\mathbf{y} | \\mu, \\theta, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{(n+m)/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\left[ \\sum_{i=1}^n (x_i - \\mu)^2 + \\sum_{j=1}^m (y_j - \\theta)^2 \\right]\\right)$$\nWe can expand the terms in the exponent:\n$$\\sum_{i=1}^n (x_i^2 - 2\\mu x_i + \\mu^2) + \\sum_{j=1}^m (y_j^2 - 2\\theta y_j + \\theta^2) = \\sum x_i^2 - 2\\mu \\sum x_i + n\\mu^2 + \\sum y_j^2 - 2\\theta \\sum y_j + m\\theta^2$$\nThe exponent can be rewritten as:\n$$-\\frac{1}{2\\sigma^2} \\left(\\sum x_i^2 + \\sum y_j^2\\right) + \\frac{\\mu}{\\sigma^2}\\sum x_i + \\frac{\\theta}{\\sigma^2}\\sum y_j - \\frac{n\\mu^2 + m\\theta^2}{2\\sigma^2}$$\nThis is in the form of a three-parameter exponential family. By the Fisher-Neyman Factorization Theorem, a set of sufficient statistics for $(\\mu, \\theta, \\sigma^2)$ is $S = \\left(\\sum_{i=1}^n X_i, \\sum_{j=1}^m Y_j, \\sum_{i=1}^n X_i^2 + \\sum_{j=1}^m Y_j^2\\right)$. Since the family of normal distributions is a full-rank exponential family, this sufficient statistic is also complete.\n\nAccording to the Rao-Blackwell and Lehmann-Scheffé theorems, the UMVUE for $\\delta$ is the conditional expectation of any unbiased estimator given the complete sufficient statistic. Let's call our UMVUE $T^*$.\n$$T^* = E[T | S] = E\\left[X_1 - Y_1 \\left| \\sum X_i, \\sum Y_j, \\sum X_i^2 + \\sum Y_j^2 \\right.\\right]$$\nBy linearity of conditional expectation:\n$$T^* = E\\left[X_1 \\left| S \\right.\\right] - E\\left[Y_1 \\left| S \\right.\\right]$$\n\nLet's compute $E[X_1 | S]$. Due to the i.i.d. nature of the $X_i$ sample, the random variables $X_1, X_2, \\dots, X_n$ are exchangeable. This implies that their conditional expectations given the symmetric function $S$ are all equal:\n$$E[X_1 | S] = E[X_2 | S] = \\dots = E[X_n | S]$$\nNow, consider the sum:\n$$\\sum_{i=1}^n E[X_i | S] = E\\left[\\sum_{i=1}^n X_i \\left| S \\right.\\right]$$\nThe sum $\\sum_{i=1}^n X_i$ is a component of the sufficient statistic $S$. Conditioning on $S$ means we are treating its components as known constants. Therefore:\n$$E\\left[\\sum_{i=1}^n X_i \\left| S \\right.\\right] = \\sum_{i=1}^n X_i$$\nCombining these facts, we have:\n$$n E[X_1 | S] = \\sum_{i=1}^n X_i$$\n$$E[X_1 | S] = \\frac{1}{n} \\sum_{i=1}^n X_i = \\bar{X}$$\nThis is the sample mean of the $X$ sample. Note that $\\bar{X}$ is a function of $\\sum X_i$, which is part of the sufficient statistic $S$.\n\nBy a completely analogous argument for the $Y$ sample:\n$$E[Y_1 | S] = E[Y_2 | S] = \\dots = E[Y_m | S]$$\nAnd\n$$m E[Y_1 | S] = \\sum_{j=1}^m E[Y_j | S] = E\\left[\\sum_{j=1}^m Y_j \\left| S \\right.\\right] = \\sum_{j=1}^m Y_j$$\n$$E[Y_1 | S] = \\frac{1}{m} \\sum_{j=1}^m Y_j = \\bar{Y}$$\n\nSubstituting these results back into the expression for $T^*$:\n$$T^* = E[X_1 | S] - E[Y_1 | S] = \\bar{X} - \\bar{Y}$$\nThe UMVUE is the difference of the sample means. Expressed in terms of the sample values and sizes as requested:\n$$T^* = \\frac{1}{n}\\sum_{i=1}^n X_i - \\frac{1}{m}\\sum_{j=1}^m Y_j$$\nThis estimator is a function of the complete sufficient statistic $S$ and is unbiased, hence it is the UMVUE for $\\delta = \\mu - \\theta$.",
            "answer": "$$\\boxed{\\frac{1}{n}\\sum_{i=1}^{n} X_i - \\frac{1}{m}\\sum_{j=1}^{m} Y_j}$$"
        },
        {
            "introduction": "Moving beyond standard fixed-sample-size experiments, this next practice applies the Rao-Blackwell theorem to a sequential sampling design. This scenario, where the sample size itself is a random variable, requires a careful application of conditioning on the appropriate sufficient statistic. Your task is not only to derive the improved estimator for the success probability $p$ but also to analytically compute the variance reduction factor, providing a concrete measure of the significant efficiency gains achieved through conditioning .",
            "id": "1922452",
            "problem": "A sequential clinical trial is designed to test a new drug. Patients are treated one by one, and the outcome for each is either a \"success\" or a \"failure\". The probability of success, $p$, is assumed to be constant for all patients, with $0 < p < 1$. The trial continues until exactly $k$ successes have been recorded. Let $N$ be the total number of patients treated, which is a random variable.\n\nAn initial, simple unbiased estimator for the success probability $p$ is proposed: $\\hat{p}_{crude} = X_1$, where $X_1=1$ if the first patient has a successful outcome, and $X_1=0$ otherwise.\n\nAccording to statistical theory, this estimator can be improved using the Rao-Blackwell theorem. The process involves conditioning $\\hat{p}_{crude}$ on a sufficient statistic for the parameter $p$. For this experimental design, the total number of trials, $N$, is a sufficient statistic. The resulting Rao-Blackwellized estimator is $\\hat{p}_{RB} = E[\\hat{p}_{crude} | N]$.\n\nYour task is to analyze the improvement in estimation quality for the specific case where the trial is set to stop after $k=2$ successes. Calculate the variance reduction factor, defined as the ratio $\\mathcal{R} = \\frac{\\text{Var}(\\hat{p}_{crude})}{\\text{Var}(\\hat{p}_{RB})}$. Express your final answer as an analytical function of $p$.",
            "solution": "Let $X_{i} \\sim \\text{Bernoulli}(p)$ independently with $0<p<1$. The stopping rule is to stop at the time $N$ when $k=2$ successes have been observed. The crude estimator is $\\hat{p}_{\\text{crude}}=X_{1}$, with\n$$\n\\mathbb{E}[X_{1}]=p,\\qquad \\text{Var}(X_{1})=p(1-p).\n$$\n\nBy the Rao-Blackwell theorem, the improved estimator is $\\hat{p}_{\\text{RB}}=\\mathbb{E}[X_{1}\\mid N]$. For $k=2$, conditioning on $N=n$ implies that among the first $n-1$ trials there is exactly one success and $X_{n}=1$. The conditional distribution over sequences with $N=n$ is uniform because each such sequence has probability $p^{2}(1-p)^{n-2}$. Hence, among the first $n-1$ positions, exactly one is a success, uniformly located; therefore\n$$\n\\mathbb{P}(X_{1}=1\\mid N=n)=\\frac{1}{n-1},\\qquad \\text{so}\\qquad \\hat{p}_{\\text{RB}}=\\frac{1}{N-1}.\n$$\n\nWe next compute $\\text{Var}(\\hat{p}_{\\text{RB}})$. For $k=2$, $N$ has the negative binomial distribution\n$$\n\\mathbb{P}(N=n)=(n-1)p^{2}(1-p)^{n-2},\\quad n=2,3,\\dots\n$$\nThus\n$$\n\\mathbb{E}\\!\\left[\\frac{1}{N-1}\\right]=\\sum_{n=2}^{\\infty}\\frac{1}{n-1}(n-1)p^{2}(1-p)^{n-2}\n=p^{2}\\sum_{m=0}^{\\infty}(1-p)^{m}=p,\n$$\nwhich confirms unbiasedness. Also,\n$$\n\\mathbb{E}\\!\\left[\\frac{1}{(N-1)^{2}}\\right]\n=\\sum_{n=2}^{\\infty}\\frac{1}{(n-1)^{2}}(n-1)p^{2}(1-p)^{n-2}\n=p^{2}\\sum_{m=0}^{\\infty}\\frac{(1-p)^{m}}{m+1}.\n$$\nUsing the series identity $\\sum_{m=0}^{\\infty}\\frac{x^{m}}{m+1}=\\frac{-\\ln(1-x)}{x}$ for $|x|<1$ with $x=1-p$, we get\n$$\n\\mathbb{E}\\!\\left[\\frac{1}{(N-1)^{2}}\\right]\n=p^{2}\\cdot \\frac{-\\ln(1-(1-p))}{1-p}\n=\\frac{p^{2}(-\\ln p)}{1-p}.\n$$\nTherefore,\n$$\n\\text{Var}\\!\\left(\\hat{p}_{\\text{RB}}\\right)\n=\\mathbb{E}\\!\\left[\\frac{1}{(N-1)^{2}}\\right]-\\left(\\mathbb{E}\\!\\left[\\frac{1}{N-1}\\right]\\right)^{2}\n=\\frac{p^{2}(-\\ln p)}{1-p}-p^{2}\n=p^{2}\\left(\\frac{-\\ln p}{1-p}-1\\right).\n$$\nThe variance reduction factor is\n$$\n\\mathcal{R}=\\frac{\\text{Var}(\\hat{p}_{\\text{crude}})}{\\text{Var}(\\hat{p}_{\\text{RB}})}\n=\\frac{p(1-p)}{p^{2}\\left(\\frac{-\\ln p}{1-p}-1\\right)}\n=\\frac{(1-p)^{2}}{p\\left(-\\ln p-(1-p)\\right)}\n=\\frac{(1-p)^{2}}{p\\left(p-1-\\ln p\\right)}.\n$$\nThis is an analytical function of $p$, valid for $0<p<1$.",
            "answer": "$$\\boxed{\\frac{(1-p)^{2}}{p\\left(p-1-\\ln p\\right)}}$$"
        },
        {
            "introduction": "The power of conditioning extends directly into the domain of modern computational statistics, where it serves as a crucial variance reduction technique. This final practice explores the \"Rao-Blackwellization\" of Monte Carlo estimators within a Bayesian hierarchical model. By comparing an estimator that relies purely on sampling with one that analytically integrates out a random parameter, you will empirically verify how replacing a random quantity with its conditional expectation leads to a more precise and efficient simulation .",
            "id": "3315544",
            "problem": "Consider the hierarchical model defined by a likelihood and a prior. Let $x_1,\\dots,x_n \\mid \\theta \\stackrel{\\text{ind}}{\\sim} \\operatorname{Poisson}(\\theta)$ and $\\theta \\sim \\operatorname{Gamma}(\\alpha,\\beta)$ where $\\operatorname{Gamma}(\\alpha,\\beta)$ denotes the gamma distribution with shape parameter $\\alpha$ and rate parameter $\\beta$ (so its probability density function is proportional to $\\theta^{\\alpha-1}\\exp(-\\beta \\theta)$). Define the joint distribution $f(x,\\theta)=f(x\\mid \\theta)\\,\\pi(\\theta)$, and the marginal (also called the evidence or marginal likelihood) $m(x)=\\int f(x\\mid \\theta)\\,\\pi(\\theta)\\,\\mathrm{d}\\theta$. After observing data $x=(x_1,\\dots,x_n)$, the posterior distribution for $\\theta$ is $\\pi(\\theta\\mid x)\\propto f(x\\mid \\theta)\\,\\pi(\\theta)$ and the posterior predictive distribution for a new count $Y$ is $m(y\\mid x)=\\int f(y\\mid \\theta)\\,\\pi(\\theta\\mid x)\\,\\mathrm{d}\\theta$.\n\nTasks:\n- Starting from the core definitions of joint, marginal, and conditional distributions, derive the posterior $\\pi(\\theta\\mid x)$ and the closed-form expression for the posterior predictive mass function $m(y\\mid x)$ for the conjugate pair above. Your derivation must begin from $f(x\\mid \\theta)=\\prod_{i=1}^n \\frac{\\exp(-\\theta)\\,\\theta^{x_i}}{x_i!}$ and $\\pi(\\theta)\\propto \\theta^{\\alpha-1}\\exp(-\\beta \\theta)$ and proceed via first principles to the integral defining $m(y\\mid x)$; do not skip steps by invoking a pre-remembered final expression.\n- Construct and compare two unbiased Monte Carlo (MC) estimators of $m(y\\mid x)$:\n  1. A numerical integration estimator that samples $\\theta_1,\\dots,\\theta_N \\stackrel{\\text{iid}}{\\sim} \\pi(\\theta\\mid x)$ and computes $\\widehat{m}_{\\text{num}}(y\\mid x)=\\frac{1}{N}\\sum_{j=1}^N f(y\\mid \\theta_j)$.\n  2. An estimator that first integrates out $\\theta$ analytically to obtain the closed-form posterior predictive distribution for $Y\\mid x$, then samples $Y_1,\\dots,Y_N \\stackrel{\\text{iid}}{\\sim} m(\\cdot\\mid x)$ and computes $\\widehat{m}_{\\text{an}}(y\\mid x)=\\frac{1}{N}\\sum_{j=1}^N \\mathbb{I}\\{Y_j=y\\}$.\nExplain conceptually, in terms of conditional expectation and the Rao–Blackwell theorem, why these two estimators differ in variance, and connect this difference to the structure of the hierarchy $Y\\mid \\theta$ and $\\theta\\mid x$.\n\nUse the following test suite of parameter values to evaluate and compare these estimators. In all cases, fix the pseudorandom generator seed to $12345$ so that the outputs are deterministic.\n\n- Test case $1$: $\\alpha=3$, $\\beta=1.2$, $x=(3,1,0,2)$, target $y=2$, MC sample size $N=100000$.\n- Test case $2$: $\\alpha=1$, $\\beta=0.5$, $x=()$ (empty, so $n=0$), target $y=0$, MC sample size $N=100000$.\n- Test case $3$: $\\alpha=10$, $\\beta=3$, $x=(10,9,12,8,11)$, target $y=15$, MC sample size $N=100000$.\n\nYour program must:\n- Compute the closed-form value $m(y\\mid x)$ for each test case.\n- Compute $\\widehat{m}_{\\text{num}}(y\\mid x)$ and $\\widehat{m}_{\\text{an}}(y\\mid x)$ as defined above for each test case.\n- Return, for each test case in order, the following five floats: the closed-form $m(y\\mid x)$, the numerical-integration MC estimate $\\widehat{m}_{\\text{num}}(y\\mid x)$, the analytically-integrated MC estimate $\\widehat{m}_{\\text{an}}(y\\mid x)$, the absolute error $\\left|\\widehat{m}_{\\text{num}}(y\\mid x)-m(y\\mid x)\\right|$, and the absolute error $\\left|\\widehat{m}_{\\text{an}}(y\\mid x)-m(y\\mid x)\\right|$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the $5$ floats for test case $1$ first, followed by the $5$ floats for test case $2$, and then the $5$ floats for test case $3$; for example, $[m_1,\\widehat{m}_{\\text{num},1},\\widehat{m}_{\\text{an},1},e_{\\text{num},1},e_{\\text{an},1},m_2,\\dots]$.\n- No physical units or angle units are involved in this problem. All outputs must be real numbers.",
            "solution": "### Analytical Derivations\n\n**1. Derivation of the Posterior Distribution $\\pi(\\theta\\mid x)$**\n\nThe posterior distribution for $\\theta$ given the observed data $x=(x_1, \\dots, x_n)$ is given by Bayes' theorem:\n$$\n\\pi(\\theta\\mid x) \\propto f(x\\mid\\theta)\\,\\pi(\\theta)\n$$\nThe likelihood function $f(x\\mid\\theta)$ for $n$ independent and identically distributed observations from a $\\operatorname{Poisson}(\\theta)$ distribution is the product of the individual probability mass functions:\n$$\nf(x\\mid\\theta) = \\prod_{i=1}^n f(x_i\\mid\\theta) = \\prod_{i=1}^n \\frac{\\exp(-\\theta)\\,\\theta^{x_i}}{x_i!} = \\frac{\\exp(-n\\theta)\\,\\theta^{\\sum_{i=1}^n x_i}}{\\prod_{i=1}^n x_i!}\n$$\nThe prior distribution for $\\theta$ is a Gamma distribution, $\\theta \\sim \\operatorname{Gamma}(\\alpha,\\beta)$, with a probability density function (PDF) proportional to:\n$$\n\\pi(\\theta) \\propto \\theta^{\\alpha-1}\\exp(-\\beta \\theta)\n$$\nCombining the likelihood and the prior, the unnormalized posterior is:\n$$\n\\pi(\\theta\\mid x) \\propto \\left( \\frac{\\exp(-n\\theta)\\,\\theta^{\\sum_{i=1}^n x_i}}{\\prod_{i=1}^n x_i!} \\right) \\left( \\theta^{\\alpha-1}\\exp(-\\beta \\theta) \\right)\n$$\nSince we are interested in the distribution of $\\theta$, we can drop any terms that do not depend on $\\theta$, such as $1/\\prod_{i=1}^n x_i!$:\n$$\n\\pi(\\theta\\mid x) \\propto \\exp(-n\\theta)\\,\\theta^{\\sum_{i=1}^n x_i} \\cdot \\theta^{\\alpha-1}\\exp(-\\beta \\theta)\n$$\nCombining the exponential and power terms of $\\theta$:\n$$\n\\pi(\\theta\\mid x) \\propto \\theta^{(\\alpha + \\sum_{i=1}^n x_i) - 1} \\exp(-(\\beta+n)\\theta)\n$$\nThis expression is the kernel of a Gamma distribution. If we define the posterior parameters $\\alpha' = \\alpha + \\sum_{i=1}^n x_i$ and $\\beta' = \\beta + n$, the posterior distribution is:\n$$\n\\pi(\\theta\\mid x) \\propto \\theta^{\\alpha' - 1} \\exp(-\\beta'\\theta)\n$$\nThus, the posterior distribution of $\\theta$ given $x$ is a Gamma distribution: $\\theta\\mid x \\sim \\operatorname{Gamma}(\\alpha', \\beta')$. The normalized posterior PDF is $\\pi(\\theta\\mid x) = \\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')} \\theta^{\\alpha'-1} \\exp(-\\beta'\\theta)$ for $\\theta > 0$.\n\n**2. Derivation of the Posterior Predictive Mass Function $m(y\\mid x)$**\n\nThe posterior predictive distribution for a new observation $Y$ is obtained by averaging the likelihood of $Y$ over the posterior distribution of $\\theta$:\n$$\nm(y\\mid x) = \\int_0^\\infty f(y\\mid\\theta)\\,\\pi(\\theta\\mid x)\\,\\mathrm{d}\\theta\n$$\nHere, $f(y\\mid\\theta)$ is the PMF of a $\\operatorname{Poisson}(\\theta)$ distribution, $f(y\\mid\\theta) = \\frac{\\exp(-\\theta)\\,\\theta^y}{y!}$, and $\\pi(\\theta\\mid x)$ is the PDF of a $\\operatorname{Gamma}(\\alpha', \\beta')$ distribution. Substituting these into the integral:\n$$\nm(y\\mid x) = \\int_0^\\infty \\left( \\frac{\\exp(-\\theta)\\,\\theta^y}{y!} \\right) \\left( \\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')} \\theta^{\\alpha'-1} \\exp(-\\beta'\\theta) \\right) \\mathrm{d}\\theta\n$$\nWe can move terms not depending on $\\theta$ outside the integral:\n$$\nm(y\\mid x) = \\frac{(\\beta')^{\\alpha'}}{y!\\,\\Gamma(\\alpha')} \\int_0^\\infty \\theta^y\\,\\theta^{\\alpha'-1} \\exp(-\\theta)\\,\\exp(-\\beta'\\theta)\\,\\mathrm{d}\\theta\n$$\nCombining terms inside the integral:\n$$\nm(y\\mid x) = \\frac{(\\beta')^{\\alpha'}}{y!\\,\\Gamma(\\alpha')} \\int_0^\\infty \\theta^{(y+\\alpha')-1} \\exp(-(\\beta'+1)\\theta)\\,\\mathrm{d}\\theta\n$$\nThe integral is the kernel of a Gamma PDF. The general form of a Gamma integral is $\\int_0^\\infty t^{k-1} e^{-rt} \\mathrm{d}t = \\frac{\\Gamma(k)}{r^k}$. In our case, the shape is $k=y+\\alpha'$ and the rate is $r=\\beta'+1$. Therefore, the integral evaluates to:\n$$\n\\int_0^\\infty \\theta^{(y+\\alpha')-1} \\exp(-(\\beta'+1)\\theta)\\,\\mathrm{d}\\theta = \\frac{\\Gamma(y+\\alpha')}{(\\beta'+1)^{y+\\alpha'}}\n$$\nSubstituting this result back into the expression for $m(y\\mid x)$:\n$$\nm(y\\mid x) = \\frac{(\\beta')^{\\alpha'}}{y!\\,\\Gamma(\\alpha')} \\frac{\\Gamma(y+\\alpha')}{(\\beta'+1)^{y+\\alpha'}}\n$$\nUsing the identity $y! = \\Gamma(y+1)$ and rearranging the terms:\n$$\nm(y\\mid x) = \\frac{\\Gamma(y+\\alpha')}{\\Gamma(y+1)\\Gamma(\\alpha')} \\left( \\frac{\\beta'}{\\beta'+1} \\right)^{\\alpha'} \\left( \\frac{1}{\\beta'+1} \\right)^y\n$$\nThis is the PMF of a Negative Binomial distribution. Using the combinatorial notation $\\binom{n}{k} = \\frac{\\Gamma(n+1)}{\\Gamma(k+1)\\Gamma(n-k+1)}$, we can write $\\frac{\\Gamma(y+\\alpha')}{\\Gamma(y+1)\\Gamma(\\alpha')} = \\binom{y+\\alpha'-1}{y}$. If we set the parameters $r = \\alpha'$ and $p = \\frac{\\beta'}{\\beta'+1}$, the PMF becomes:\n$$\nm(y\\mid x) = \\binom{y+r-1}{y} p^r (1-p)^y\n$$\nThis confirms that the posterior predictive distribution for $Y$ is a Negative Binomial distribution, $Y\\mid x \\sim \\operatorname{NB}(r=\\alpha', p=\\frac{\\beta'}{\\beta'+1})$.\n\n### Monte Carlo Estimators and Variance Comparison\n\nWe are asked to compare two unbiased MC estimators for $m(y\\mid x) = P(Y=y \\mid x)$.\n\n1.  **Numerical Integration Estimator $\\widehat{m}_{\\text{num}}(y\\mid x)$**: This estimator uses samples from the posterior distribution of $\\theta$ to approximate the integral defining $m(y\\mid x)$. It is calculated as $\\widehat{m}_{\\text{num}}(y\\mid x) = \\frac{1}{N}\\sum_{j=1}^N f(y\\mid\\theta_j)$, where $\\theta_j \\sim \\pi(\\theta\\mid x)$. This is an estimate of $\\mathbb{E}_{\\theta\\mid x}[f(y\\mid\\theta)]$.\n\n2.  **Analytically Integrated Estimator $\\widehat{m}_{\\text{an}}(y\\mid x)$**: This estimator leverages the analytically derived closed-form posterior predictive distribution $m(\\cdot\\mid x)$. It is calculated as $\\widehat{m}_{\\text{an}}(y\\mid x) = \\frac{1}{N}\\sum_{j=1}^N \\mathbb{I}\\{Y_j=y\\}$, where $Y_j \\sim m(\\cdot\\mid x)$ and $\\mathbb{I}{\\cdot}$ is the indicator function. This is an estimate of $\\mathbb{E}_{Y\\mid x}[\\mathbb{I}\\{Y=y\\}]$.\n\n**Conceptual Explanation of Variance Difference**:\n\nThe difference in variance between the two estimators is a classic illustration of Rao-Blackwellization. We are estimating the same quantity, $m(y\\mid x) = P(Y=y\\mid x)$. The two estimators correspond to two different ways of computing this expectation via Monte Carlo. The law of total expectation shows they are both unbiased: $\\mathbb{E}[\\widehat{m}_{\\text{num}}] = \\mathbb{E}_{\\theta|x}[f(y|\\theta)] = m(y|x)$ and $\\mathbb{E}[\\widehat{m}_{\\text{an}}] = \\mathbb{E}_{Y|x}[\\mathbb{I}\\{Y=y\\}] = m(y|x)$.\n\nTheir variances, however, are different. Let $W = \\mathbb{I}\\{Y=y\\}$ be the random variable associated with the analytically integrated estimator (for a single sample). The random variable for the numerical integration estimator is $Z = f(y\\mid\\theta)$. The connection between them is that $Z$ is the conditional expectation of $W$ given $\\theta$: $Z = f(y\\mid\\theta) = P(Y=y\\mid\\theta) = \\mathbb{E}[W\\mid\\theta]$.\n\nThe Rao-Blackwell theorem states that $\\operatorname{Var}(Z) \\le \\operatorname{Var}(W)$. More explicitly, the law of total variance applied to $W$ gives:\n$$\n\\operatorname{Var}(W) = \\mathbb{E}[\\operatorname{Var}(W\\mid\\theta)] + \\operatorname{Var}(\\mathbb{E}[W\\mid\\theta])\n$$\nSubstituting our variables:\n$$\n\\operatorname{Var}_{Y\\mid x}(\\mathbb{I}\\{Y=y\\}) = \\mathbb{E}_{\\theta\\mid x}[\\operatorname{Var}_{Y\\mid\\theta}(\\mathbb{I}\\{Y=y\\})] + \\operatorname{Var}_{\\theta\\mid x}(f(y\\mid\\theta))\n$$\nThe variance of the summand for $\\widehat{m}_{\\text{an}}$ is the term on the left. The variance of the summand for $\\widehat{m}_{\\text{num}}$ is the second term on the right. Since the first term on the right, $\\mathbb{E}_{\\theta\\mid x}[f(y\\mid\\theta)(1-f(y\\mid\\theta))]$, is an expectation of a non-negative quantity, it must be non-negative. Therefore:\n$$\n\\operatorname{Var}_{\\theta\\mid x}(f(y\\mid\\theta)) \\le \\operatorname{Var}_{Y\\mid x}(\\mathbb{I}\\{Y=y\\})\n$$\nThis implies that $\\operatorname{Var}(\\widehat{m}_{\\text{num}}) \\le \\operatorname{Var}(\\widehat{m}_{\\text{an}})$.\n\nConceptually, the estimator $\\widehat{m}_{\\text{an}}$ involves two layers of randomness: first, the uncertainty about $\\theta$ (described by its posterior $\\pi(\\theta|x)$), and second, the observational randomness of $Y$ for a given $\\theta$ (described by the Poisson distribution). The estimator $\\widehat{m}_{\\text{num}}$ analytically averages out, or \"integrates out\", the second layer of randomness (the Poisson sampling). By replacing the noisy binary indicator $\\mathbb{I}\\{Y=y\\}$ with its smoother conditional expectation $f(y\\mid\\theta)$, this Rao-Blackwellized procedure removes a source of variance, leading to a more efficient estimator. This process is known as Rao-Blackwellization.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.stats import poisson\n\ndef solve():\n    \"\"\"\n    Computes and compares two Monte Carlo estimators for the posterior predictive\n    probability mass function in a Poisson-Gamma conjugate model.\n    \"\"\"\n    \n    test_cases = [\n        # (alpha, beta, x, y, N)\n        (3.0, 1.2, (3, 1, 0, 2), 2, 100000),\n        (1.0, 0.5, (), 0, 100000),\n        (10.0, 3.0, (10, 9, 12, 8, 11), 15, 100000),\n    ]\n\n    # Fix the pseudorandom generator seed for deterministic output.\n    np.random.seed(12345)\n\n    results = []\n    for alpha, beta, x, y, N in test_cases:\n        # Step 1: Calculate posterior parameters\n        n = len(x)\n        sum_x = sum(x)\n        \n        alpha_prime = alpha + sum_x\n        beta_prime = beta + n\n\n        # Step 2: Compute the closed-form value of m(y|x)\n        # This is the PMF of a Negative Binomial distribution with parameters\n        # r = alpha_prime and p = beta_prime / (beta_prime + 1)\n        log_m_true = (gammaln(y + alpha_prime) \n                      - (gammaln(y + 1) + gammaln(alpha_prime))\n                      + alpha_prime * np.log(beta_prime) \n                      - (y + alpha_prime) * np.log(beta_prime + 1))\n        m_true = np.exp(log_m_true)\n\n        # Step 3: Compute the numerical integration estimator, hat_m_num\n        # Sample thetas from the posterior Gamma(alpha', beta')\n        # Note: numpy.random.gamma uses a scale parameter, which is 1/rate.\n        scale_param = 1.0 / beta_prime\n        thetas = np.random.gamma(shape=alpha_prime, scale=scale_param, size=N)\n        \n        # Calculate the Poisson PMF f(y|theta) for each sample\n        # This is E[I(Y=y)|theta]\n        f_values = poisson.pmf(y, thetas)\n        m_hat_num = np.mean(f_values)\n        \n        # Step 4: Compute the analytically-integrated estimator, hat_m_an\n        # Sample Y from the posterior predictive NegativeBinomial(r, p)\n        # and compute the proportion of samples equal to y.\n        p_nb = beta_prime / (beta_prime + 1)\n        Y_samples = np.random.negative_binomial(n=alpha_prime, p=p_nb, size=N)\n        m_hat_an = np.mean(Y_samples == y)\n\n        # Step 5: Calculate absolute errors\n        error_num = abs(m_hat_num - m_true)\n        error_an = abs(m_hat_an - m_true)\n        \n        results.extend([m_true, m_hat_num, m_hat_an, error_num, error_an])\n\n    # Format the final output as a single comma-separated list in brackets.\n    # Use general formatting to avoid scientific notation where possible, but allow it if needed.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}