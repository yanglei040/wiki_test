{
    "hands_on_practices": [
        {
            "introduction": "A core requirement of crude Monte Carlo integration is the ability to generate samples uniformly from the integration domain $D$. This is straightforward for simple shapes like hyperrectangles, but many real-world problems involve more complex geometries. This first exercise  guides you through a complete, practical workflow: you will first implement the rejection sampling algorithm to draw samples from a triangular domain, and then use these samples to estimate integrals, bridging the gap between sampling theory and practical integration.",
            "id": "3301576",
            "problem": "You are asked to design, analyze, and implement a rejection sampling algorithm to draw samples from the Uniform distribution (Unif) on the set $$D=\\{x\\in[0,1]^2:\\,x_1+x_2\\le 1\\}$$ and to use these samples for crude Monte Carlo (MC) integration. Begin from first principles: the definition of the Uniform distribution on a measurable set, the definition of rejection sampling, and the definition of crude Monte Carlo integration as the estimation of integrals by sampling from a distribution whose expectation is related to the integral of interest.\n\nConstruct a rejection sampling scheme that uses proposals from the Uniform distribution on the square $$S=[0,1]^2,$$ with acceptance determined by whether a proposed point lies in $$D.$$ Derive, from the foundational definitions, the acceptance probability of this scheme, and justify that the accepted draws are distributed as Uniform on $$D.$$\n\nImplement the algorithm to generate a specified number of accepted samples, denote this number by $$N_{\\text{acc}},$$ and record the total number of proposals $$N_{\\text{prop}}$$ used. Compute the empirical acceptance rate $$\\hat{\\alpha}=N_{\\text{acc}}/N_{\\text{prop}}.$$ Using the accepted samples $$\\{X^{(i)}\\}_{i=1}^{N_{\\text{acc}}}$$ with $$X^{(i)}\\in D,$$ estimate the integrals\n$$I_1=\\int_D x_1 x_2\\,\\mathrm{d}x \\quad\\text{and}\\quad I_2=\\int_D (x_1+x_2)\\,\\mathrm{d}x,$$\nvia crude Monte Carlo integration by relating these integrals to expectations under the Uniform distribution on $$D$$ and the area of $$D.$$\n\nYour program must implement the following deterministic test suite and produce results for each test case:\n\n- Test case $$1$$: Random number generator (RNG) seed $$12345$$ and accepted sample target $$N_{\\text{acc}}=1.$$\n- Test case $$2$$: RNG seed $$202310$$ and accepted sample target $$N_{\\text{acc}}=5000.$$\n- Test case $$3$$: RNG seed $$42$$ and accepted sample target $$N_{\\text{acc}}=20000.$$\n\nFor each test case, run the rejection sampler until exactly $$N_{\\text{acc}}$$ points have been accepted, then:\n- Compute and return the empirical acceptance rate $$\\hat{\\alpha}$$ as a floating-point number.\n- Compute and return the crude Monte Carlo estimates of $$I_1$$ and $$I_2$$ as floating-point numbers.\n- Return the total number of proposals $$N_{\\text{prop}}$$ as an integer.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is itself a list $$[\\hat{\\alpha},\\,\\widehat{I}_1,\\,\\widehat{I}_2,\\,N_{\\text{prop}}].$$ For example, the final output should look like $$[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]].$$ No physical units or angles are involved in this problem. All randomness must be controlled by the specified RNG seeds to ensure reproducibility. Use inclusive acceptance on the boundary, i.e., accept when $$x_1+x_2\\le 1.$$",
            "solution": "The problem is valid as it is scientifically grounded in probability theory and numerical methods, is well-posed, objective, self-contained, and computationally feasible. We proceed with a complete solution.\n\nThe solution will be developed from first principles, beginning with the foundational definitions of the relevant mathematical and algorithmic concepts.\n\n**1. Theoretical Framework**\n\n**a. The Target Distribution**\nThe objective is to draw samples from the Uniform distribution on the set $$D = \\{x \\in [0,1]^2 : x_1+x_2 \\le 1\\}$$. The set $$D$$ is a right triangle in the plane with vertices at $$(0,0)$$, $$(1,0)$$, and $$(0,1)$$. The area of this set, denoted as $$\\text{Area}(D)$$, is a standard geometric result:\n$$\n\\text{Area}(D) = \\frac{1}{2} \\times \\text{base} \\times \\text{height} = \\frac{1}{2} \\times 1 \\times 1 = \\frac{1}{2}\n$$\nThe probability density function (PDF) of a random variable $$X$$ that is uniformly distributed on a measurable set $$A \\subset \\mathbb{R}^d$$ is defined as:\n$$\nf_X(x) = \\begin{cases} 1/\\text{Area}(A)  \\text{if } x \\in A \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nTherefore, the target PDF, which we denote as $$p(x)$$, for the Uniform distribution on $$D$$ is:\n$$\np(x) = \\begin{cases} 1/(1/2) = 2  \\text{if } x \\in D \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThis can be written compactly using an indicator function, $$p(x) = 2 \\cdot \\mathbb{I}_{D}(x)$$.\n\n**b. Rejection Sampling**\nRejection sampling is a method for generating samples from a target distribution with PDF $$p(x)$$ when we can easily sample from another distribution, the proposal distribution, with PDF $$g(x)$$. A necessary condition is that the support of $$g(x)$$ must contain the support of $$p(x)$$. Furthermore, there must exist a constant $$M  \\infty$$ such that $$p(x) \\le M \\cdot g(x)$$ for all $$x$$. The algorithm proceeds as follows:\n1. Draw a sample $$Y$$ from the proposal distribution $$g(x)$$.\n2. Draw a sample $$U$$ from the Uniform distribution on $$[0, 1]$$.\n3. If $$U \\le \\frac{p(Y)}{M g(Y)}$$, accept the sample $$Y$$ (i.e., set $$X=Y$$). Otherwise, reject $$Y$$ and return to step 1.\n\n**c. Constructing the Sampler for $$D$$**\nThe problem specifies using a proposal distribution that is Uniform on the square $$S = [0,1]^2$$. The area of $$S$$ is $$\\text{Area}(S) = 1^2 = 1$$. The proposal PDF, $$g(x)$$, is therefore:\n$$\ng(x) = \\begin{cases} 1/\\text{Area}(S) = 1  \\text{if } x \\in S \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThe support of $$g(x)$$ is $$S$$, and since $$D \\subset S$$, the support of $$g(x)$$ contains the support of $$p(x)$$. We now find the constant $$M$$. We require $$p(x) \\le M \\cdot g(x)$$ for all $$x$$.\n- If $$x \\in D$$, then $$p(x)=2$$ and $$g(x)=1$$. The condition is $$2 \\le M \\cdot 1$$, so $$M \\ge 2$$.\n- If $$x \\in S \\setminus D$$, then $$p(x)=0$$ and $$g(x)=1$$. The condition is $$0 \\le M \\cdot 1$$, which is true for any non-negative $$M$$.\n- If $$x \\notin S$$, then $$p(x)=0$$ and $$g(x)=0$$. The condition holds.\nThe minimal value for $$M$$ that satisfies the inequality everywhere is $$M=2$$.\n\nNow, we analyze the acceptance condition $$U \\le \\frac{p(Y)}{M g(Y)}$$ with $$M=2$$:\n- If the proposed sample $$Y$$ is in $$D$$, then $$p(Y)=2$$ and $$g(Y)=1$$. The ratio is $$\\frac{p(Y)}{M g(Y)} = \\frac{2}{2 \\cdot 1} = 1$$. The condition is $$U \\le 1$$, which is always true for a sample $$U$$ from $$\\text{Unif}(0,1)$$.\n- If the proposed sample $$Y$$ is in $$S \\setminus D$$, then $$p(Y)=0$$ and $$g(Y)=1$$. The ratio is $$\\frac{p(Y)}{M g(Y)} = \\frac{0}{2 \\cdot 1} = 0$$. The condition is $$U \\le 0$$, which is almost surely false.\n\nThis simplifies the rejection sampling algorithm significantly:\n1. Propose a sample $$Y=(Y_1, Y_2)$$ from the Uniform distribution on $$S=[0,1]^2$$. This is achieved by drawing $$Y_1 \\sim \\text{Unif}(0,1)$$ and $$Y_2 \\sim \\text{Unif}(0,1)$$ independently.\n2. If $$Y \\in D$$ (i.e., if $$Y_1 + Y_2 \\le 1$$), accept $$Y$$.\n3. Otherwise, reject $$Y$$ and repeat.\n\n**d. Acceptance Probability and Distribution of Accepted Samples**\nThe probability of accepting a proposal $$Y$$ in a single trial, denoted by $$\\alpha$$, is the probability that $$Y$$ falls into the acceptance region, which is $$D$$.\n$$\n\\alpha = P(Y \\in D) = \\int_S \\mathbb{I}_D(y) g(y) \\, \\mathrm{d}y = \\int_D g(y) \\, \\mathrm{d}y\n$$\nSince $$g(y)=1$$ for all $$y \\in D \\subset S$$, we have:\n$$\n\\alpha = \\int_D 1 \\, \\mathrm{d}y = \\text{Area}(D) = \\frac{1}{2}\n$$\nThe theoretical acceptance probability is $$\\alpha = 0.5$$. The number of proposals $$N_{\\text{prop}}$$ required to obtain one accepted sample follows a Geometric distribution with success probability $$\\alpha$$. The expected number of proposals to get $$N_{\\text{acc}}$$ samples is $$N_{\\text{acc}} / \\alpha$$.\n\nTo confirm that the accepted samples $$X$$ are indeed uniformly distributed on $$D$$, we consider the probability that an accepted sample falls into an arbitrary measurable subset $$A \\subseteq D$$.\n$$\nP(X \\in A) = P(Y \\in A \\mid Y \\text{ is accepted}) = \\frac{P(Y \\in A \\text{ and } Y \\text{ is accepted})}{P(Y \\text{ is accepted})}\n$$\nSince $$A \\subseteq D$$, a sample $$Y \\in A$$ is always accepted. Thus, $$P(Y \\in A \\text{ and } Y \\text{ is accepted}) = P(Y \\in A)$$.\n$$\nP(Y \\in A) = \\int_A g(y) \\, \\mathrm{d}y = \\int_A 1 \\, \\mathrm{d}y = \\text{Area}(A)\n$$\nThe denominator is the overall acceptance probability, $$P(Y \\text{ is accepted}) = \\alpha = \\text{Area}(D)$$. Therefore,\n$$\nP(X \\in A) = \\frac{\\text{Area}(A)}{\\text{Area}(D)}\n$$\nThis is precisely the definition of the uniform probability measure on $$D$$. The resulting samples are correctly distributed.\n\n**2. Monte Carlo Integration**\n\nWe wish to estimate an integral of the form $$I = \\int_D h(x) \\, \\mathrm{d}x$$. This integral can be related to the expectation of $$h(X)$$ where $$X$$ is a random variable with PDF $$p(x) = \\text{Unif}(D)$$.\n$$\nE[h(X)] = \\int_D h(x) p(x) \\, \\mathrm{d}x = \\int_D h(x) \\frac{1}{\\text{Area}(D)} \\, \\mathrm{d}x\n$$\nRearranging this gives the identity:\n$$\nI = \\int_D h(x) \\, \\mathrm{d}x = \\text{Area}(D) \\cdot E[h(X)]\n$$\nBy the Law of Large Numbers, the expectation $$E[h(X)]$$ can be estimated by the sample mean of $$h$$ evaluated over a set of $$N_{\\text{acc}}$$ independent samples $$\\{X^{(i)}\\}_{i=1}^{N_{\\text{acc}}}$$ drawn from $$p(x)$$:\n$$\nE[h(X)] \\approx \\frac{1}{N_{\\text{acc}}} \\sum_{i=1}^{N_{\\text{acc}}} h(X^{(i)})\n$$\nSubstituting this into the identity for $$I$$ yields the crude Monte Carlo estimator, $$\\widehat{I}$$:\n$$\n\\widehat{I} = \\text{Area}(D) \\cdot \\frac{1}{N_{\\text{acc}}} \\sum_{i=1}^{N_{\\text{acc}}} h(X^{(i)})\n$$\nGiven $$\\text{Area}(D)=1/2$$, the specific estimators for $$I_1 = \\int_D x_1 x_2 \\, \\mathrm{d}x$$ and $$I_2 = \\int_D (x_1+x_2) \\, \\mathrm{d}x$$ are:\n- For $$I_1$$, let $$h_1(x) = x_1 x_2$$. The estimator is:\n$$\n\\widehat{I}_1 = \\frac{1}{2} \\cdot \\frac{1}{N_{\\text{acc}}} \\sum_{i=1}^{N_{\\text{acc}}} X^{(i)}_1 X^{(i)}_2\n$$\n- For $$I_2$$, let $$h_2(x) = x_1 + x_2$$. The estimator is:\n$$\n\\widehat{I}_2 = \\frac{1}{2} \\cdot \\frac{1}{N_{\\text{acc}}} \\sum_{i=1}^{N_{\\text{acc}}} (X^{(i)}_1 + X^{(i)}_2)\n$$\n\n**3. Implementation Plan**\nThe implementation will consist of a function that takes an RNG seed and a target number of accepted samples, $$N_{\\text{acc}}$$, as input.\n1. Initialize a random number generator with the specified seed for reproducibility.\n2. Initialize an empty list for accepted samples and set the proposal count, $$N_{\\text{prop}}$$, to $$0$$.\n3. Loop until the number of accepted samples reaches $$N_{\\text{acc}}$$:\n    a. Increment $$N_{\\text{prop}}$$.\n    b. Generate a $$2$$-dimensional proposal point $$x = (x_1, x_2)$$ by drawing two numbers from $$\\text{Unif}(0,1)$$.\n    c. If $$x_1 + x_2 \\le 1$$, add the point $$x$$ to the list of accepted samples.\n4. Once the loop terminates, calculate the required quantities:\n    a. Empirical acceptance rate: $$\\hat{\\alpha} = N_{\\text{acc}} / N_{\\text{prop}}$$.\n    b. Monte Carlo estimates $$\\widehat{I}_1$$ and $$\\widehat{I}_2$$ using the formulas derived above, applied to the collected samples.\n    c. The total number of proposals, $$N_{\\text{prop}}$$.\n5. Return these four values: $$[\\hat{\\alpha}, \\widehat{I}_1, \\widehat{I}_2, N_{\\text{prop}}]$$.\nThe main program will execute this logic for each of the specified test cases and format the results as a single string.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the rejection sampling and Monte Carlo integration as per the problem description.\n    Runs a deterministic test suite and prints the results in the specified format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (RNG seed, Number of accepted samples N_acc)\n        (12345, 1),\n        (202310, 5000),\n        (42, 20000),\n    ]\n\n    all_results = []\n    for seed, N_acc in test_cases:\n        # Initialize the random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        accepted_samples = []\n        N_prop = 0\n\n        # Run the rejection sampler until exactly N_acc samples are collected.\n        while len(accepted_samples)  N_acc:\n            N_prop += 1\n            # Propose a point from the Uniform distribution on the unit square S = [0,1]^2.\n            # This is done by drawing two independent samples from Unif(0,1).\n            proposal_point = rng.random(size=2)\n            \n            # Acceptance condition: check if the point lies in the target set D.\n            # D = {x in [0,1]^2: x_1 + x_2 = 1}.\n            # The boundary condition x_1 + x_2 = 1 is inclusive as specified.\n            if proposal_point[0] + proposal_point[1] = 1.0:\n                accepted_samples.append(proposal_point)\n\n        # 1. Compute the empirical acceptance rate.\n        # This is the ratio of accepted samples to total proposals.\n        # The theoretical rate is Area(D)/Area(S) = (1/2)/1 = 0.5.\n        alpha_hat = N_acc / N_prop\n\n        # 2. Compute the crude Monte Carlo estimates of the integrals.\n        # The general formula is: I_hat = Area(D) * (1/N_acc) * sum(h(X_i)).\n        # Here, Area(D) = 0.5.\n        area_d = 0.5\n        \n        # Convert the list of samples to a NumPy array for efficient, vectorized calculations.\n        if N_acc  0:\n            samples_np = np.array(accepted_samples)\n            \n            # For I_1 = integral(x_1 * x_2), the function is h_1(x) = x_1 * x_2.\n            h1_values = samples_np[:, 0] * samples_np[:, 1]\n            mean_h1 = np.mean(h1_values)\n            I1_hat = area_d * mean_h1\n            \n            # For I_2 = integral(x_1 + x_2), the function is h_2(x) = x_1 + x_2.\n            h2_values = samples_np[:, 0] + samples_np[:, 1]\n            mean_h2 = np.mean(h2_values)\n            I2_hat = area_d * mean_h2\n        else: # This branch is not hit by the given test cases but is robust.\n            I1_hat = 0.0\n            I2_hat = 0.0\n\n        # 3. N_prop is the total number of proposals.\n        # It's an integer.\n\n        # Package the results for this test case.\n        case_result = [alpha_hat, I1_hat, I2_hat, N_prop]\n        all_results.append(case_result)\n\n    # Format the final output according to the problem specification.\n    # e.g., [[val,val,val,val],[val,val,val,val]]\n    inner_lists_str = []\n    for res in all_results:\n        # Format each inner list as '[v1,v2,v3,v4]' without extra spaces.\n        inner_str = '[' + ','.join(map(str, res)) + ']'\n        inner_lists_str.append(inner_str)\n    \n    final_output_str = '[' + ','.join(inner_lists_str) + ']'\n\n    # Final print statement in the exact required format.\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The efficiency of a Monte Carlo estimator is fundamentally linked to the variance of the integrand over the sampling domain. This practice  allows you to analytically explore this connection by examining a discontinuous function, where variance can be high. Furthermore, it introduces the powerful variance reduction technique of integrand smoothing, demonstrating how a clever modification of the problem formulation can yield a more precise estimate for the same computational effort.",
            "id": "3301534",
            "problem": "Consider crude Monte Carlo integration on the interval $[0,1]$ with independent and identically distributed samples $U_{1},\\dots,U_{n} \\sim \\mathrm{Uniform}(0,1)$. For a threshold $a \\in (0,1)$, define the discontinuous integrand $f_{a}(x) = \\mathrm{sign}(x-a)$, where $\\mathrm{sign}(x-a)=1$ if $x \\ge a$ and $\\mathrm{sign}(x-a)=-1$ if $xa$. Let the target integral be $I(a)=\\int_{0}^{1} f_{a}(x)\\,dx$, and let the crude Monte Carlo estimator be $\\widehat{I}_{n}(a) = \\frac{1}{n}\\sum_{i=1}^{n} f_{a}(U_{i})$.\n\n1. Starting from the definitions of expectation and variance for independent and identically distributed random variables, derive $I(a)$ and $\\mathrm{Var}(\\widehat{I}_{n}(a))$ in closed form, and determine the value of $a \\in (0,1)$ that maximizes $\\mathrm{Var}(\\widehat{I}_{n}(a))$. Explain, in terms of symmetry of the sign function over $[0,1]$, why this maximizer occurs where it does.\n\n2. To reduce variance without changing the sampling distribution, consider the symmetric case $a=\\tfrac{1}{2}$ and replace the discontinuous integrand with a smoothed, continuous integrand $f_{\\epsilon}: [0,1] \\to \\mathbb{R}$ for a smoothing parameter $\\epsilon \\in \\left(0,\\tfrac{1}{2}\\right)$ defined by\n$$\nf_{\\epsilon}(x) =\n\\begin{cases}\n-1,  0 \\le x \\le \\tfrac{1}{2}-\\epsilon, \\\\[6pt]\n\\dfrac{x-\\tfrac{1}{2}}{\\epsilon},  \\tfrac{1}{2}-\\epsilon  x  \\tfrac{1}{2}+\\epsilon, \\\\[6pt]\n1,  \\tfrac{1}{2}+\\epsilon \\le x \\le 1.\n\\end{cases}\n$$\nUsing only first principles (the definitions of expectation and variance and basic calculus), show that the crude Monte Carlo estimator $\\widehat{I}_{n,\\epsilon}=\\frac{1}{n}\\sum_{i=1}^{n} f_{\\epsilon}(U_{i})$ remains unbiased for the same integral as with $f_{1/2}$, and derive an exact closed-form expression for $\\mathrm{Var}(\\widehat{I}_{n,\\epsilon})$ as a function of $\\epsilon$ and $n$.\n\nProvide, as your final answer, the exact closed-form expression you obtain for $\\mathrm{Var}(\\widehat{I}_{n,\\epsilon})$. No numerical rounding is required.",
            "solution": "This problem is analyzed in two parts. First, we analyze the crude Monte Carlo estimator for a discontinuous step function. Second, we analyze an estimator for a smoothed version of that function and compute its variance.\n\nPart 1: Analysis of the Discontinuous Integrand $f_a(x)$\n\nThe target integral $I(a)$ is defined as $I(a) = \\int_{0}^{1} f_{a}(x) \\, dx$. The integrand is $f_{a}(x) = \\mathrm{sign}(x-a)$, which is $-1$ for $x  a$ and $1$ for $x \\ge a$. We can compute the integral directly:\n$$\nI(a) = \\int_{0}^{a} (-1) \\, dx + \\int_{a}^{1} (1) \\, dx = [-x]_{0}^{a} + [x]_{a}^{1} = (-a - 0) + (1 - a) = 1 - 2a\n$$\nThe crude Monte Carlo estimator is $\\widehat{I}_{n}(a) = \\frac{1}{n}\\sum_{i=1}^{n} f_{a}(U_{i})$, where $U_i \\sim \\mathrm{Uniform}(0,1)$ are independent and identically distributed (i.i.d.). The expectation of this estimator is:\n$$\n\\mathrm{E}[\\widehat{I}_{n}(a)] = \\mathrm{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} f_{a}(U_{i})\\right] = \\frac{1}{n}\\sum_{i=1}^{n} \\mathrm{E}[f_{a}(U_{i})]\n$$\nSince the $U_i$ are i.i.d., $\\mathrm{E}[f_{a}(U_{i})]$ is the same for all $i$. Let $U$ be a generic random variable with $U \\sim \\mathrm{Uniform}(0,1)$. Its probability density function is $p(u)=1$ for $u \\in [0,1]$.\n$$\n\\mathrm{E}[f_{a}(U)] = \\int_{0}^{1} f_{a}(u) p(u) \\, du = \\int_{0}^{1} f_{a}(u) \\, du = I(a)\n$$\nThus, $\\mathrm{E}[\\widehat{I}_{n}(a)] = \\frac{1}{n} \\sum_{i=1}^{n} I(a) = I(a) = 1 - 2a$. The estimator is unbiased.\n\nNext, we derive the variance $\\mathrm{Var}(\\widehat{I}_{n}(a))$. Since the samples $U_i$ are i.i.d., the random variables $f_{a}(U_i)$ are also i.i.d. The variance of a sum of i.i.d. random variables is the sum of their variances.\n$$\n\\mathrm{Var}(\\widehat{I}_{n}(a)) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} f_{a}(U_{i})\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}(f_{a}(U_{i})) = \\frac{n}{n^2} \\mathrm{Var}(f_{a}(U)) = \\frac{1}{n} \\mathrm{Var}(f_{a}(U))\n$$\nTo find $\\mathrm{Var}(f_{a}(U))$, we use the formula $\\mathrm{Var}(X) = \\mathrm{E}[X^2] - (\\mathrm{E}[X])^2$. We already know $\\mathrm{E}[f_{a}(U)] = 1 - 2a$. We now compute $\\mathrm{E}[(f_{a}(U))^2]$.\nThe function $f_{a}(x)$ takes values $-1$ and $1$. In either case, $(f_{a}(x))^2 = 1$. Therefore, the random variable $(f_{a}(U))^2$ is always equal to $1$.\n$$\n\\mathrm{E}[(f_{a}(U))^2] = \\mathrm{E}[1] = 1\n$$\nNow we can compute the variance of a single sample:\n$$\n\\mathrm{Var}(f_{a}(U)) = \\mathrm{E}[(f_{a}(U))^2] - (\\mathrm{E}[f_{a}(U)])^2 = 1 - (1 - 2a)^2 = 1 - (1 - 4a + 4a^2) = 4a - 4a^2 = 4a(1-a)\n$$\nThe variance of the estimator is therefore:\n$$\n\\mathrm{Var}(\\widehat{I}_{n}(a)) = \\frac{4a(1-a)}{n}\n$$\nTo find the value of $a \\in (0,1)$ that maximizes this variance, we need to maximize the function $g(a) = a(1-a) = a - a^2$. We take the derivative with respect to $a$ and set it to zero:\n$$\n\\frac{dg}{da} = 1 - 2a = 0 \\implies a = \\frac{1}{2}\n$$\nThe second derivative is $\\frac{d^2g}{da^2} = -2$, which is negative, confirming that $a = \\frac{1}{2}$ is a local maximum. Since $a = \\frac{1}{2}$ is the only critical point in the interval $(0,1)$, it is the global maximizer.\n\nThe maximizer occurs at $a = \\frac{1}{2}$ due to the symmetry of the problem. The variance of the estimator is directly proportional to the variance of a single sample outcome, $\\mathrm{Var}(f_a(U))$. The random variable $Y = f_a(U)$ takes two values, $-1$ and $1$. The variance of such a binary random variable is maximized when the two outcomes are equally probable.\nThe probability of each outcome is determined by $a$:\n$P(Y=1) = P(U \\ge a) = \\int_a^1 1 \\, du = 1-a$.\n$P(Y=-1) = P(U  a) = \\int_0^a 1 \\, du = a$.\nFor these probabilities to be equal, we must have $a = 1-a$, which gives $2a=1$ or $a = \\frac{1}{2}$. At this value, the discontinuity of the sign function is located at the center of the integration interval $[0,1]$. A uniform random draw $U$ from $[0,1]$ is equally likely to fall above or below $\\frac{1}{2}$, leading to maximum uncertainty (variance) in the value of $f_{1/2}(U)$.\n\nPart 2: Analysis of the Smoothed Integrand $f_\\epsilon(x)$\n\nWe now consider the case $a = \\frac{1}{2}$ and the smoothed integrand $f_{\\epsilon}(x)$. The crude Monte Carlo estimator is $\\widehat{I}_{n,\\epsilon}=\\frac{1}{n}\\sum_{i=1}^{n} f_{\\epsilon}(U_{i})$.\nFirst, we show that this estimator is unbiased for the same integral as $f_{1/2}(x)$. From Part 1, the target integral is $I(\\frac{1}{2}) = 1 - 2(\\frac{1}{2}) = 0$.\nThe expectation of the new estimator is $\\mathrm{E}[\\widehat{I}_{n,\\epsilon}] = \\mathrm{E}[f_{\\epsilon}(U)] = \\int_0^1 f_{\\epsilon}(x) \\, dx$. We compute this integral:\n$$\n\\int_{0}^{1} f_{\\epsilon}(x) \\, dx = \\int_{0}^{\\frac{1}{2}-\\epsilon} (-1) \\, dx + \\int_{\\frac{1}{2}-\\epsilon}^{\\frac{1}{2}+\\epsilon} \\frac{x-\\frac{1}{2}}{\\epsilon} \\, dx + \\int_{\\frac{1}{2}+\\epsilon}^{1} (1) \\, dx\n$$\nThe first integral is $[-x]_{0}^{\\frac{1}{2}-\\epsilon} = -(\\frac{1}{2}-\\epsilon) = \\epsilon - \\frac{1}{2}$.\nThe third integral is $[x]_{\\frac{1}{2}+\\epsilon}^{1} = 1 - (\\frac{1}{2}+\\epsilon) = \\frac{1}{2} - \\epsilon$.\nFor the second integral, we make the substitution $u = x - \\frac{1}{2}$, so $du = dx$. The limits of integration become $-\\epsilon$ and $\\epsilon$.\n$$\n\\int_{\\frac{1}{2}-\\epsilon}^{\\frac{1}{2}+\\epsilon} \\frac{x-\\frac{1}{2}}{\\epsilon} \\, dx = \\frac{1}{\\epsilon} \\int_{-\\epsilon}^{\\epsilon} u \\, du = \\frac{1}{\\epsilon} \\left[\\frac{u^2}{2}\\right]_{-\\epsilon}^{\\epsilon} = \\frac{1}{2\\epsilon} (\\epsilon^2 - (-\\epsilon)^2) = 0\n$$\nSumming the three parts, $\\mathrm{E}[\\widehat{I}_{n,\\epsilon}] = (\\epsilon - \\frac{1}{2}) + 0 + (\\frac{1}{2} - \\epsilon) = 0$.\nSince $\\mathrm{E}[\\widehat{I}_{n,\\epsilon}] = 0 = I(\\frac{1}{2})$, the estimator $\\widehat{I}_{n,\\epsilon}$ is unbiased for the same integral as $\\widehat{I}_{n}(\\frac{1}{2})$.\n\nNext, we derive the variance $\\mathrm{Var}(\\widehat{I}_{n,\\epsilon})$. As before, $\\mathrm{Var}(\\widehat{I}_{n,\\epsilon}) = \\frac{1}{n} \\mathrm{Var}(f_{\\epsilon}(U))$.\n$$\n\\mathrm{Var}(f_{\\epsilon}(U)) = \\mathrm{E}[(f_{\\epsilon}(U))^2] - (\\mathrm{E}[f_{\\epsilon}(U)])^2\n$$\nSince we have shown that $\\mathrm{E}[f_{\\epsilon}(U)] = 0$, the variance simplifies to $\\mathrm{Var}(f_{\\epsilon}(U)) = \\mathrm{E}[(f_{\\epsilon}(U))^2]$.\nWe compute this expectation by integrating $(f_{\\epsilon}(x))^2$ over $[0,1]$:\n$$\n\\mathrm{E}[(f_{\\epsilon}(U))^2] = \\int_{0}^{1} (f_{\\epsilon}(x))^2 \\, dx = \\int_{0}^{\\frac{1}{2}-\\epsilon} (-1)^2 \\, dx + \\int_{\\frac{1}{2}-\\epsilon}^{\\frac{1}{2}+\\epsilon} \\left(\\frac{x-\\frac{1}{2}}{\\epsilon}\\right)^2 \\, dx + \\int_{\\frac{1}{2}+\\epsilon}^{1} (1)^2 \\, dx\n$$\nThe first integral is $\\int_{0}^{\\frac{1}{2}-\\epsilon} 1 \\, dx = [\\frac{1}{2}-\\epsilon] - 0 = \\frac{1}{2} - \\epsilon$.\nThe third integral is $\\int_{\\frac{1}{2}+\\epsilon}^{1} 1 \\, dx = 1 - [\\frac{1}{2}+\\epsilon] = \\frac{1}{2} - \\epsilon$.\nFor the second integral, we again use the substitution $u = x - \\frac{1}{2}$:\n$$\n\\int_{\\frac{1}{2}-\\epsilon}^{\\frac{1}{2}+\\epsilon} \\frac{(x-\\frac{1}{2})^2}{\\epsilon^2} \\, dx = \\frac{1}{\\epsilon^2} \\int_{-\\epsilon}^{\\epsilon} u^2 \\, du = \\frac{1}{\\epsilon^2} \\left[\\frac{u^3}{3}\\right]_{-\\epsilon}^{\\epsilon} = \\frac{1}{3\\epsilon^2} (\\epsilon^3 - (-\\epsilon)^3) = \\frac{2\\epsilon^3}{3\\epsilon^2} = \\frac{2\\epsilon}{3}\n$$\nSumming the three parts gives the variance of a single sample:\n$$\n\\mathrm{Var}(f_{\\epsilon}(U)) = \\mathrm{E}[(f_{\\epsilon}(U))^2] = \\left(\\frac{1}{2} - \\epsilon\\right) + \\frac{2\\epsilon}{3} + \\left(\\frac{1}{2} - \\epsilon\\right) = 1 - 2\\epsilon + \\frac{2\\epsilon}{3} = 1 - \\frac{6\\epsilon}{3} + \\frac{2\\epsilon}{3} = 1 - \\frac{4\\epsilon}{3}\n$$\nFinally, the variance of the estimator $\\widehat{I}_{n,\\epsilon}$ is:\n$$\n\\mathrm{Var}(\\widehat{I}_{n,\\epsilon}) = \\frac{1}{n} \\mathrm{Var}(f_{\\epsilon}(U)) = \\frac{1}{n} \\left(1 - \\frac{4\\epsilon}{3}\\right)\n$$\nThis is the required closed-form expression for the variance of the smoothed estimator. Note that for any $\\epsilon \\in (0, \\frac{1}{2})$, the term $1 - \\frac{4\\epsilon}{3}$ is less than $1$, so the variance is reduced compared to the discontinuous case where the variance was $\\frac{1}{n}$ (for $a=\\frac{1}{2}$).",
            "answer": "$$\\boxed{\\frac{1}{n} \\left( 1 - \\frac{4\\epsilon}{3} \\right)}$$"
        },
        {
            "introduction": "The celebrated $\\frac{1}{\\sqrt{n}}$ convergence rate of Monte Carlo methods is a direct consequence of the Central Limit Theorem, which assumes the integrand has finite variance. This final practice  explores a critical boundary case where this assumption is violated, using an integrand that belongs to $L^1([0,1])$ but not $L^2([0,1])$. By working through this example, you will discover the consequences of infinite variance: the Strong Law of Large Numbers may still hold, but the standard CLT breaks down, requiring a generalized limit theorem and leading to a much slower rate of convergence.",
            "id": "3301536",
            "problem": "Consider crude Monte Carlo integration of the integral $I=\\int_{0}^{1} h(x)\\,dx$ over the domain $D=[0,1]$. Let $(X_i)_{i\\ge 1}$ be an independent and identically distributed sequence with $X_i\\sim \\mathrm{Uniform}(0,1)$. The crude Monte Carlo estimator is $\\widehat I_n=\\frac{1}{n}\\sum_{i=1}^n h(X_i)$. You are asked to select the correct statements regarding a specific heavy-tailed integrand $h$ and its implications for variance estimation and limit theorems.\n\nTake the integrand $h:D\\to [1,\\infty)$ defined by $h(x)=x^{-3/4}$ for $x\\in(0,1]$ and $h(0)=+\\infty$ (interpreted in the Lebesgue sense so that $h\\in L^1([0,1])$ but the point singularity at $x=0$ is integrable). For the estimator $\\widehat I_n$, consider the following statements:\n\nA. $h\\in L^1([0,1])$ but $h\\notin L^2([0,1])$.\n\nB. $\\widehat I_n$ is unbiased and satisfies the Strong Law of Large Numbers (SLLN), and the usual Central Limit Theorem (CLT) with $\\sqrt{n}$-normalization and finite asymptotic variance applies.\n\nC. $\\mathrm{Var}(h(X_1))=+\\infty$, and no consistent estimator of $\\mathrm{Var}(h(X_1))$ exists; in particular, the sample variance $S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n\\big(h(X_i)-\\widehat I_n\\big)^2$ diverges to $+\\infty$ almost surely as $n\\to\\infty$.\n\nD. The fluctuations of $\\widehat I_n$ obey a heavy-tailed generalized limit theorem: $n^{1/4}\\big(\\widehat I_n - I\\big)$ converges in distribution to a non-degenerate $\\alpha$-stable law with tail index $\\alpha=\\frac{4}{3}$ (up to centering), whereas $\\sqrt{n}\\big(\\widehat I_n - I\\big)$ diverges in probability.\n\nSelect all correct statements.",
            "solution": "The user wants a meticulous validation of the problem statement, followed by a full solution deriving the correct answer and evaluating all options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   The integral to be estimated is $I=\\int_{0}^{1} h(x)\\,dx$.\n-   The domain of integration is $D=[0,1]$.\n-   The sampling is done using an independent and identically distributed (i.i.d.) sequence of random variables $(X_i)_{i\\ge 1}$, where $X_i\\sim \\mathrm{Uniform}(0,1)$.\n-   The crude Monte Carlo estimator is given by $\\widehat I_n=\\frac{1}{n}\\sum_{i=1}^n h(X_i)$.\n-   The integrand is the function $h:D\\to [1,\\infty)$ defined by $h(x)=x^{-3/4}$ for $x\\in(0,1]$ and $h(0)=+\\infty$.\n-   It is specified that $h$ is interpreted in the Lebesgue sense and $h\\in L^1([0,1])$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding:** The problem is firmly situated in the mathematical fields of numerical integration, probability theory, and statistics, specifically concerning Monte Carlo methods. It tests the understanding of $L^p$ spaces, the Law of Large Numbers (LLN), the Central Limit Theorem (CLT), and the Generalized Central Limit Theorem for heavy-tailed distributions (stable laws). All concepts are standard and rigorously defined.\n-   **Well-Posedness:** The problem defines a specific integrand, a sampling distribution, and an estimator. It then asks to verify several statements about the mathematical properties of this setup. These are well-posed questions with unique, verifiable answers.\n-   **Objectivity:** The problem is stated using precise, objective mathematical language. Terms like \"unbiased\", \"Strong Law of Large Numbers\", \"converges in distribution\", and \"$\\alpha$-stable law\" have unambiguous definitions.\n-   **Flaw Checklist:**\n    1.  **Scientific/Factual Unsoundness:** None. The setup is a canonical example used to illustrate the breakdown of the standard CLT and the emergence of stable laws.\n    2.  **Non-Formalizable or Irrelevant:** None. The problem is perfectly formalizable.\n    3.  **Incomplete or Contradictory Setup:** None. The problem provides all necessary information. The statement that $h\\in L^1([0,1])$ is a premise that can be independently verified, confirming the consistency of the setup.\n    4.  **Unrealistic or Infeasible:** Not applicable as it's a purely mathematical problem. The mathematical constructs are valid.\n    5.  **Ill-Posed or Poorly Structured:** None.\n    6.  **Pseudo-Profound, Trivial, or Tautological:** The problem is neither trivial nor tautological. It requires a solid understanding of a non-trivial interplay between integrability conditions and limit theorems in probability.\n    7.  **Outside Scientific Verifiability:** All claims are mathematically verifiable.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the solution.\n\n### Solution Derivation\n\nLet $Y_i = h(X_i)$ be the sampled values. Since the $X_i$ are i.i.d. $\\mathrm{Uniform}(0,1)$, the $Y_i$ are also i.i.d. random variables. The crude Monte Carlo estimator is the sample mean of these variables, $\\widehat I_n = \\frac{1}{n}\\sum_{i=1}^n Y_i$.\n\nThe core of the problem lies in analyzing the moments of the random variable $Y_1 = h(X_1)$. The $k$-th moment is given by $E[Y_1^k] = E[(h(X_1))^k]$. By the law of the unconscious statistician, this is:\n$$E[(h(X_1))^k] = \\int_0^1 (h(x))^k f_{X_1}(x) dx$$\nSince $X_1 \\sim \\mathrm{Uniform}(0,1)$, its probability density function is $f_{X_1}(x) = 1$ for $x \\in [0,1]$. Thus:\n$$E[(h(X_1))^k] = \\int_0^1 (h(x))^k dx = \\int_0^1 (x^{-3/4})^k dx = \\int_0^1 x^{-3k/4} dx$$\nThis integral is a standard $p$-integral of the form $\\int_0^1 x^{-p} dx$, which converges if and only if $p  1$. In our case, the condition for convergence is $\\frac{3k}{4}  1$, or $k  \\frac{4}{3}$.\n\n**1. First Moment (Expectation):**\nFor $k=1$, we have $p = \\frac{3(1)}{4} = \\frac{3}{4}  1$. The integral converges.\n$$I = E[Y_1] = \\int_0^1 x^{-3/4} dx = \\left[ \\frac{x^{-3/4+1}}{-3/4+1} \\right]_0^1 = \\left[ \\frac{x^{1/4}}{1/4} \\right]_0^1 = [4x^{1/4}]_0^1 = 4(1) - 4(0) = 4$$\nThe expectation of $h(X_1)$ is finite and equal to $I=4$. This confirms that $h \\in L^1([0,1])$.\n\n**2. Second Moment and Variance:**\nFor $k=2$, we have $p = \\frac{3(2)}{4} = \\frac{3}{2}  1$. The integral diverges.\n$$E[Y_1^2] = \\int_0^1 x^{-3/2} dx = \\left[ \\frac{x^{-1/2}}{-1/2} \\right]_0^1 = [-2x^{-1/2}]_0^1$$\nThis expression diverges to $+\\infty$ as $x \\to 0^+$.\nTherefore, $E[Y_1^2] = +\\infty$.\nThe variance of $Y_1$ is $\\mathrm{Var}(Y_1) = E[Y_1^2] - (E[Y_1])^2 = +\\infty - 4^2 = +\\infty$. The variance is infinite.\n\nWith these foundational results, we can evaluate each statement.\n\n### Option-by-Option Analysis\n\n**Statement A: $h\\in L^1([0,1])$ but $h\\notin L^2([0,1])$.**\n-   The space $L^p([0,1])$ consists of functions $f$ such that $\\int_0^1 |f(x)|^p dx  \\infty$.\n-   For $p=1$, we check $\\int_0^1 |h(x)| dx$. Since $h(x) = x^{-3/4} \\ge 1  0$, this is $\\int_0^1 x^{-3/4} dx$. As calculated above, this integral evaluates to $4$, which is finite. Thus, $h \\in L^1([0,1])$.\n-   For $p=2$, we check $\\int_0^1 |h(x)|^2 dx = \\int_0^1 (x^{-3/4})^2 dx = \\int_0^1 x^{-3/2} dx$. As calculated above, this integral diverges. Thus, $h \\notin L^2([0,1])$.\n-   The statement is a conjunction of two true facts.\n-   Verdict: **Correct**.\n\n**Statement B: $\\widehat I_n$ is unbiased and satisfies the Strong Law of Large Numbers (SLLN), and the usual Central Limit Theorem (CLT) with $\\sqrt{n}$-normalization and finite asymptotic variance applies.**\n-   **Unbiasedness:** The expected value of the estimator is $E[\\widehat I_n] = E[\\frac{1}{n}\\sum_{i=1}^n h(X_i)] = \\frac{1}{n}\\sum_{i=1}^n E[h(X_i)]$. Since $E[h(X_i)] = I = 4$ for all $i$, we have $E[\\widehat I_n] = \\frac{1}{n}(n \\cdot I) = I$. The estimator is unbiased. This part is true.\n-   **SLLN:** Kolmogorov's Strong Law of Large Numbers states that for a sequence of i.i.d. random variables $Y_i$, if $E[|Y_1|]  \\infty$, then the sample mean converges almost surely to the expectation. Here $Y_i = h(X_i)$. Since $h(x) \\ge 1$, $|h(X_1)| = h(X_1)$, and we found $E[h(X_1)] = 4  \\infty$. Therefore, the SLLN applies, and $\\widehat I_n \\to I$ almost surely. This part is true.\n-   **CLT:** The standard Lindeberg-Lévy Central Limit Theorem states that for a sequence of i.i.d. random variables $Y_i$ with finite mean $\\mu$ and finite variance $\\sigma^2  0$, the normalized sum $\\sqrt{n}(\\frac{1}{n}\\sum Y_i - \\mu)$ converges in distribution to a normal distribution $N(0, \\sigma^2)$. A necessary condition for this theorem is that the variance $\\sigma^2 = \\mathrm{Var}(Y_1)$ must be finite. We have established that $\\mathrm{Var}(h(X_1)) = +\\infty$. Therefore, the standard CLT does not apply.\n-   The statement incorrectly claims that the usual CLT applies.\n-   Verdict: **Incorrect**.\n\n**Statement C: $\\mathrm{Var}(h(X_1))=+\\infty$, and no consistent estimator of $\\mathrm{Var}(h(X_1))$ exists; in particular, the sample variance $S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n\\big(h(X_i)-\\widehat I_n\\big)^2$ diverges to $+\\infty$ almost surely as $n\\to\\infty$.**\n-   **Variance:** As shown, $\\mathrm{Var}(h(X_1)) = E[(h(X_1))^2] - (E[h(X_1)])^2 = +\\infty$. This part is true.\n-   **Sample Variance Behavior:** Let $Y_i = h(X_i)$. The sample variance is $S_n^2 = \\frac{1}{n-1}\\left(\\sum_{i=1}^n Y_i^2 - n(\\widehat I_n)^2\\right) = \\frac{n}{n-1}\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i^2 - (\\widehat I_n)^2\\right)$.\n    -   As $n\\to\\infty$, the pre-factor $\\frac{n}{n-1} \\to 1$.\n    -   By the SLLN (Statement B), $\\widehat I_n \\to I = 4$ almost surely. Thus $(\\widehat I_n)^2 \\to 16$ almost surely. This term is convergent and bounded.\n    -   Consider the term $\\frac{1}{n}\\sum_{i=1}^n Y_i^2$. The variables $Z_i = Y_i^2 = (h(X_i))^2$ are i.i.d. and non-negative. Their expectation is $E[Z_i] = E[Y_i^2] = +\\infty$. A known extension of the SLLN states that if $Z_i$ are i.i.d. non-negative random variables with $E[Z_1] = +\\infty$, then $\\frac{1}{n}\\sum_{i=1}^n Z_i \\to +\\infty$ almost surely.\n    -   Combining these results, $S_n^2$ behaves like $1 \\cdot (+\\infty - 16)$, which diverges to $+\\infty$ almost surely.\n-   The claim that $S_n^2$ diverges to $+\\infty$ a.s. is correct. This is the behavior one would expect from an estimator of an infinite quantity.\n-   Verdict: **Correct**.\n\n**Statement D: The fluctuations of $\\widehat I_n$ obey a heavy-tailed generalized limit theorem: $n^{1/4}\\big(\\widehat I_n - I\\big)$ converges in distribution to a non-degenerate $\\alpha$-stable law with tail index $\\alpha=\\frac{4}{3}$ (up to centering), whereas $\\sqrt{n}\\big(\\widehat I_n - I\\big)$ diverges in probability.**\n-   **Generalized CLT:** Since the variance of $Y_1=h(X_1)$ is infinite, we must investigate if it lies in the domain of attraction of a stable law. We analyze the tail probability $P(Y_1  y)$. For $y \\ge 1$:\n    $$P(Y_1  y) = P(h(X_1)  y) = P(X_1^{-3/4}  y) = P(X_1  y^{-4/3})$$\n    Since $X_1 \\sim \\mathrm{Uniform}(0,1)$, this probability is $P(Y_1  y) = y^{-4/3}$.\n    This shows that the tail of the distribution of $Y_1$ is regularly varying with index $\\alpha = 4/3$.\n-   Since $1  \\alpha  2$, the random variable $Y_1$ is in the domain of attraction of an $\\alpha$-stable law. The Generalized CLT applies. For a sum of i.i.d. variables from such a distribution, the proper normalization for convergence to a stable law is $n^{1/\\alpha}$.\n-   The convergence result is for the centered sum: $\\frac{1}{n^{1/\\alpha}} \\sum_{i=1}^n (Y_i - E[Y_i]) \\xrightarrow{d} S$, where $S$ is an $\\alpha$-stable random variable.\n-   With $\\alpha = 4/3$, the normalization is $n^{1/(4/3)} = n^{3/4}$. The limit theorem is: $\\frac{1}{n^{3/4}} \\sum_{i=1}^n (h(X_i) - I) \\xrightarrow{d} S$.\n-   Let's check the expression in the statement:\n    $n^{1/4}(\\widehat I_n - I) = n^{1/4}\\left(\\frac{1}{n}\\sum_{i=1}^n h(X_i) - I\\right) = \\frac{n^{1/4}}{n} \\sum_{i=1}^n (h(X_i) - I) = \\frac{1}{n^{3/4}} \\sum_{i=1}^n (h(X_i) - I)$.\n    This is precisely the correctly normalized quantity. The statement that it converges in distribution to a non-degenerate $\\alpha$-stable law with $\\alpha=4/3$ is correct.\n-   **Divergence of $\\sqrt{n}$-normalized term:** We can write $\\sqrt{n}(\\widehat I_n - I)$ as:\n    $$\\sqrt{n}(\\widehat I_n - I) = n^{1/2}(\\widehat I_n - I) = n^{1/2-1/4} \\cdot \\left(n^{1/4}(\\widehat I_n - I)\\right) = n^{1/4} \\cdot Z_n$$\n    where $Z_n = n^{1/4}(\\widehat I_n - I)$. We just showed that $Z_n$ converges in distribution to a non-degenerate stable law $S$. A sequence of random variables that converges in distribution is stochastically bounded (tight). Since $n^{1/4} \\to \\infty$ and $Z_n$ converges to a random variable that is not zero almost surely, the product $n^{1/4} Z_n$ must diverge in probability.\n-   Verdict: **Correct**.",
            "answer": "$$\\boxed{ACD}$$"
        }
    ]
}