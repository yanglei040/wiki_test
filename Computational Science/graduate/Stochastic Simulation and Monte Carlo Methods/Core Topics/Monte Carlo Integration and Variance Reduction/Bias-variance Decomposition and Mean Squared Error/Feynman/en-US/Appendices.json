{
    "hands_on_practices": [
        {
            "introduction": "We begin with a foundational scenario common in quality control and reliability engineering: estimating the mean lifetime of a product. This exercise  demonstrates how to calculate the Mean Squared Error ($MSE$) for one of the most common estimators—the sample mean. By working through this problem, you will reinforce the direct relationship between $MSE$ and variance for unbiased estimators and see quantitatively how increasing sample size improves estimation accuracy.",
            "id": "1934129",
            "problem": "In a factory that manufactures a new type of long-lasting Light-Emitting Diode (LED) bulb, the operational lifetime of a bulb is a critical quality metric. Extensive prior data suggests that the lifetime of these bulbs, measured in thousands of hours, can be accurately modeled by an exponential distribution with an unknown true mean lifetime, denoted by $\\theta$.\n\nA quality control engineer is tasked with estimating this mean lifetime $\\theta$. The engineer collects a random sample of $n$ bulbs from the production line and measures their lifetimes, which are designated as $X_1, X_2, \\dots, X_n$. The proposed estimator for the true mean lifetime $\\theta$ is the sample mean, $\\hat{\\theta} = \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$.\n\nTo evaluate the performance of this estimator, the engineer decides to compute its Mean Squared Error (MSE). The MSE of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as the expected value of the squared difference between the estimator and the parameter: $\\text{MSE}(\\hat{\\theta}) = \\mathbb{E}\\left[(\\hat{\\theta} - \\theta)^2\\right]$.\n\nYour task is to derive a general expression for the MSE of the sample mean estimator $\\hat{\\theta}$ in this context. Express your answer in terms of the true mean lifetime $\\theta$ and the sample size $n$.",
            "solution": "Let $X_{1},\\dots,X_{n}$ be independent and identically distributed exponential random variables with mean $\\theta$. For the exponential distribution parameterized by its mean, one has $\\mathbb{E}[X_{i}]=\\theta$ and $\\operatorname{Var}(X_{i})=\\theta^{2}$ for each $i$.\n\nThe estimator is the sample mean $\\hat{\\theta}=\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. By linearity of expectation,\n$$\n\\mathbb{E}[\\bar{X}]=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}[X_{i}]=\\frac{1}{n}\\cdot n\\theta=\\theta,\n$$\nso the estimator is unbiased and the bias is $0$.\n\nUsing independence and the variance-scaling property,\n$$\n\\operatorname{Var}(\\bar{X})=\\operatorname{Var}\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right)=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\operatorname{Var}(X_{i})=\\frac{1}{n^{2}}\\cdot n\\theta^{2}=\\frac{\\theta^{2}}{n}.\n$$\nThe Mean Squared Error satisfies the bias-variance decomposition\n$$\n\\operatorname{MSE}(\\hat{\\theta})=\\mathbb{E}\\big[(\\hat{\\theta}-\\theta)^{2}\\big]=\\operatorname{Var}(\\hat{\\theta})+\\big(\\mathbb{E}[\\hat{\\theta}]-\\theta\\big)^{2}.\n$$\nSince the bias is $0$, this reduces to\n$$\n\\operatorname{MSE}(\\bar{X})=\\operatorname{Var}(\\bar{X})=\\frac{\\theta^{2}}{n}.\n$$",
            "answer": "$$\\boxed{\\frac{\\theta^{2}}{n}}$$"
        },
        {
            "introduction": "Moving beyond the straightforward case of unbiased estimators, this next practice challenges the intuition that they are always preferable. By comparing a standard, data-driven estimator against a simple, fixed (and therefore biased) guess, we can directly observe the essence of the bias-variance tradeoff. This hypothetical scenario  is crucial for understanding why biased models are not just a theoretical curiosity but are often intentionally used in modern statistics and machine learning to achieve lower overall error.",
            "id": "1934147",
            "problem": "In statistical estimation theory, we often compare different estimators for a parameter based on their performance. Consider a random variable $X$ that follows a Bernoulli distribution with parameter $p$, denoted as $X \\sim \\text{Bernoulli}(p)$, where $p$ is the probability of success ($X=1$) and $1-p$ is the probability of failure ($X=0$).\n\nSuppose we have only a single observation of $X$ to estimate the unknown parameter $p$. Two different estimators are proposed:\n\n1.  The first estimator, $\\hat{p}_1$, is simply the observed outcome: $\\hat{p}_1 = X$.\n2.  The second estimator, $\\hat{p}_2$, is a fixed constant value: $\\hat{p}_2 = 0.8$.\n\nThe quality of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is often measured by its Mean Squared Error (MSE), which is defined as $\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$, where $E[\\cdot]$ denotes the expectation.\n\nAssuming the true value of the parameter is $p=0.9$, which of the two estimators has a lower MSE?\n\nA. The MSE of $\\hat{p}_1$ is lower.\n\nB. The MSE of $\\hat{p}_2$ is lower.\n\nC. The MSEs are equal.\n\nD. The comparison cannot be made with the given information.",
            "solution": "We are given a single observation $X \\sim \\text{Bernoulli}(p)$ and two estimators: $\\hat{p}_{1}=X$ and $\\hat{p}_{2}=0.8$. The Mean Squared Error is defined as $\\text{MSE}(\\hat{\\theta})=E[(\\hat{\\theta}-\\theta)^{2}]$. We compute the MSEs symbolically in terms of $p$ and then evaluate at $p=0.9$.\n\nFor $\\hat{p}_{1}=X$, using the bias-variance decomposition,\n$$\n\\text{MSE}(\\hat{p}_{1})= \\operatorname{Var}(X) + \\left(E[X]-p\\right)^{2}.\n$$\nFor $X \\sim \\text{Bernoulli}(p)$, $E[X]=p$ and $\\operatorname{Var}(X)=p(1-p)$. Thus,\n$$\n\\text{MSE}(\\hat{p}_{1})=p(1-p).\n$$\nAlternatively, directly:\n$$\n\\text{MSE}(\\hat{p}_{1})=E[(X-p)^{2}]=E[X^{2}] - 2pE[X] + p^{2} = E[X] - 2p^{2} + p^{2} = p - p^{2} = p(1-p).\n$$\n\nFor $\\hat{p}_{2}=0.8$, since it is a constant,\n$$\n\\text{MSE}(\\hat{p}_{2})=E[(0.8-p)^{2}]=(0.8-p)^{2}.\n$$\n\nNow evaluate at the true value $p=0.9$:\n$$\n\\text{MSE}(\\hat{p}_{1})=0.9(1-0.9)=0.09,\\qquad \\text{MSE}(\\hat{p}_{2})=(0.8-0.9)^{2}=0.01.\n$$\nSince $0.01  0.09$, the second estimator has the lower MSE. Therefore, the correct option is B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "This final practice synthesizes our understanding in an advanced context central to stochastic simulation and computational finance. You will tackle the optimization of a Monte Carlo estimator's parameters—balancing the number of simulations against the accuracy of each one—under a fixed computational budget. This exercise  demonstrates how the bias-variance decomposition provides the theoretical framework for making critical, real-world decisions to maximize the efficiency of complex computational experiments.",
            "id": "3292367",
            "problem": "Consider a single-level Monte Carlo estimator for an expectation $\\mu = \\mathbb{E}[G(X)]$ where $X$ is the terminal state of a continuous-time stochastic process and $G$ is a measurable functional. The estimator uses a time-discretization parameter $h \\in (0,1]$ and $n \\in \\mathbb{N}$ independent samples, yielding an approximation $\\widehat{\\mu}_{n,h} = \\frac{1}{n}\\sum_{i=1}^{n} G\\!\\left(X^{(i)}_{h}\\right)$, where $X^{(i)}_{h}$ denotes the $i$th discretized sample path at resolution $h$. Under standard regularity conditions for weak approximation of the underlying process, assume the following:\n- The bias satisfies $|\\mathbb{E}[\\widehat{\\mu}_{n,h}] - \\mu| = c_{b}\\, h^{p} + o(h^{p})$ as $h \\to 0$ for some constant $c_{b}  0$ and order $p  0$.\n- The per-sample variance is uniformly bounded in $h$, i.e., $\\operatorname{Var}(G(X_{h})) \\leq c_{v}$ for some constant $c_{v}  0$.\n- The computational cost scales as $C(n,h) = \\kappa\\, \\frac{n}{h}$ for some constant $\\kappa  0$, reflecting that each sample path requires $\\mathcal{O}(h^{-1})$ time steps.\n\nUsing the fundamental definitions of bias, variance, and mean squared error (MSE), and treating $n$ as a continuous positive variable for the purpose of optimization, formulate and solve the constrained optimization problem of choosing $(n,h)$ to minimize the MSE subject to a fixed computational budget $\\mathcal{B}  0$ (i.e., $C(n,h) \\leq \\mathcal{B}$, with the optimizer attaining the budget). Derive exact symbolic expressions for the optimal $h^{\\star}$, $n^{\\star}$, and the minimized mean squared error $\\mathrm{MSE}^{\\star}$ in terms of $p$, $c_{b}$, $c_{v}$, $\\kappa$, and $\\mathcal{B}$. Finally, interpret the optimal scaling of $h^{\\star}$, $n^{\\star}$, and $\\mathrm{MSE}^{\\star}$ with respect to $\\mathcal{B}$ and the order $p$. Express the optimal quantities as exact analytic expressions; no numerical rounding is required.",
            "solution": "The objective is to minimize the Mean Squared Error (MSE) of the estimator $\\widehat{\\mu}_{n,h}$ for a given computational budget $\\mathcal{B}$. The MSE is defined as $\\mathrm{MSE}(\\widehat{\\mu}_{n,h}) = \\mathbb{E}[(\\widehat{\\mu}_{n,h} - \\mu)^2]$. It can be decomposed into the sum of the squared bias and the variance of the estimator:\n$$\n\\mathrm{MSE}(\\widehat{\\mu}_{n,h}) = \\left(\\mathbb{E}[\\widehat{\\mu}_{n,h}] - \\mu\\right)^2 + \\operatorname{Var}(\\widehat{\\mu}_{n,h})\n$$\n\nWe are given the following relationships for the leading-order terms as $h \\to 0$:\n$1$. The squared bias is $(\\mathbb{E}[\\widehat{\\mu}_{n,h}] - \\mu)^2$. Since the estimator is a mean of $n$ independent samples, its expectation is $\\mathbb{E}[\\widehat{\\mu}_{n,h}] = \\mathbb{E}[G(X_h)]$. The problem states $|\\mathbb{E}[G(X_h)] - \\mu| = c_{b} h^{p} + o(h^p)$. We approximate the squared bias by the square of its leading term:\n$$\n\\left(\\mathbb{E}[\\widehat{\\mu}_{n,h}] - \\mu\\right)^2 \\approx (c_{b} h^{p})^2 = c_{b}^2 h^{2p}\n$$\n$2$. The variance of the estimator is $\\operatorname{Var}(\\widehat{\\mu}_{n,h}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} G(X^{(i)}_{h})\\right)$. Since the samples are independent and identically distributed, the variance of the mean is the variance of a single sample divided by $n$:\n$$\n\\operatorname{Var}(\\widehat{\\mu}_{n,h}) = \\frac{1}{n} \\operatorname{Var}(G(X_h))\n$$\nThe problem states that $\\operatorname{Var}(G(X_h)) \\leq c_{v}$. We assume this bound is representative of the variance for the purpose of optimization, so $\\operatorname{Var}(G(X_h)) \\approx c_{v}$. Thus,\n$$\n\\operatorname{Var}(\\widehat{\\mu}_{n,h}) \\approx \\frac{c_{v}}{n}\n$$\n\nCombining these terms, the MSE as a function of $n$ and $h$ is:\n$$\n\\mathrm{MSE}(n,h) = c_{b}^2 h^{2p} + \\frac{c_{v}}{n}\n$$\n\nThe optimization is subject to a fixed computational budget $\\mathcal{B}$. The cost function is $C(n,h) = \\kappa \\frac{n}{h}$. The constraint is that the cost equals the budget:\n$$\n\\kappa \\frac{n}{h} = \\mathcal{B}\n$$\n\nWe can use the constraint to express $n$ in terms of $h$:\n$$\nn = \\frac{\\mathcal{B} h}{\\kappa}\n$$\nSubstituting this into the MSE expression eliminates the variable $n$, yielding an objective function that depends only on $h$:\n$$\nf(h) \\equiv \\mathrm{MSE}(h) = c_{b}^2 h^{2p} + \\frac{c_{v}}{\\frac{\\mathcal{B} h}{\\kappa}} = c_{b}^2 h^{2p} + \\frac{c_{v} \\kappa}{\\mathcal{B}} h^{-1}\n$$\n\nTo find the optimal step size $h^{\\star}$ that minimizes this function, we compute the derivative of $f(h)$ with respect to $h$ and set it to zero:\n$$\n\\frac{df}{dh} = \\frac{d}{dh} \\left( c_{b}^2 h^{2p} + \\frac{c_{v} \\kappa}{\\mathcal{B}} h^{-1} \\right) = 2p c_{b}^2 h^{2p-1} - \\frac{c_{v} \\kappa}{\\mathcal{B}} h^{-2}\n$$\nSetting the derivative to zero gives:\n$$\n2p c_{b}^2 (h^{\\star})^{2p-1} = \\frac{c_{v} \\kappa}{\\mathcal{B}} (h^{\\star})^{-2}\n$$\nSolving for $h^{\\star}$:\n$$\n(h^{\\star})^{2p+1} = \\frac{c_{v} \\kappa}{2p c_{b}^2 \\mathcal{B}}\n$$\n$$\nh^{\\star} = \\left(\\frac{c_{v} \\kappa}{2p c_{b}^2 \\mathcal{B}}\\right)^{\\frac{1}{2p+1}}\n$$\nTo confirm this is a minimum, we check the second derivative:\n$$\n\\frac{d^2f}{dh^2} = 2p(2p-1) c_{b}^2 h^{2p-2} + 2 \\frac{c_{v} \\kappa}{\\mathcal{B}} h^{-3}\n$$\nSince $p0$, $c_b0$, $c_v0$, $\\kappa0$, $\\mathcal{B}0$, and $h0$, the second term is always positive. If $p \\ge 1/2$, the first term is non-negative. If $0  p  1/2$, the first term is negative, but the problem of balancing bias and variance is typically posed for methods where $p \\ge 1/2$ (e.g. Euler-Maruyama where $p=1$). However, even for $0  p  1/2$, the condition for a minimum from the first-order necessary condition $2p c_{b}^2 (h^{\\star})^{2p} = \\frac{c_{v} \\kappa}{\\mathcal{B} h^{\\star}}$ guarantees the second derivative is positive: $\\frac{d^2f}{dh^2} |_{h=h^*} = (2p-1)\\frac{c_v\\kappa}{\\mathcal{B}(h^*)^3} + 2\\frac{c_v\\kappa}{\\mathcal{B}(h^*)^3} = (2p+1)\\frac{c_v\\kappa}{\\mathcal{B}(h^*)^3}  0$. Thus, $h^{\\star}$ corresponds to a local minimum.\n\nNext, we find the optimal number of samples, $n^{\\star}$, by substituting $h^{\\star}$ into the constraint equation:\n$$\nn^{\\star} = \\frac{\\mathcal{B}}{\\kappa} h^{\\star} = \\frac{\\mathcal{B}}{\\kappa} \\left(\\frac{c_{v} \\kappa}{2p c_{b}^2 \\mathcal{B}}\\right)^{\\frac{1}{2p+1}}\n$$\n\nFinally, we find the minimized mean squared error, $\\mathrm{MSE}^{\\star}$, by substituting $h^{\\star}$ and $n^{\\star}$ into the MSE expression. A key insight comes from the first-order condition, which states that at the optimum, the variance term and the squared bias term are related. Let $B^2(h) = c_{b}^2 h^{2p}$ and $V(n) = c_{v}/n$. The condition $2p c_{b}^2 (h^{\\star})^{2p-1} = \\frac{c_{v} \\kappa}{\\mathcal{B}} (h^{\\star})^{-2}$ can be multiplied by $h^\\star$ to give $2p c_{b}^2 (h^{\\star})^{2p} = \\frac{c_{v} \\kappa}{\\mathcal{B} h^{\\star}}$. This is equivalent to $2p B^2(h^{\\star}) = V(n^{\\star})$.\nThe minimal MSE is thus:\n$$\n\\mathrm{MSE}^{\\star} = B^2(h^{\\star}) + V(n^{\\star}) = B^2(h^{\\star}) + 2p B^2(h^{\\star}) = (1+2p) B^2(h^{\\star})\n$$\nSubstituting the expression for $h^{\\star}$ into this:\n$$\n\\mathrm{MSE}^{\\star} = (1+2p) c_{b}^2 (h^{\\star})^{2p} = (1+2p) c_{b}^2 \\left[ \\left(\\frac{c_{v} \\kappa}{2p c_{b}^2 \\mathcal{B}}\\right)^{\\frac{1}{2p+1}} \\right]^{2p}\n$$\n$$\n\\mathrm{MSE}^{\\star} = (1+2p) c_{b}^2 \\left(\\frac{c_{v} \\kappa}{2p c_{b}^2 \\mathcal{B}}\\right)^{\\frac{2p}{2p+1}}\n$$\nThis can be simplified for presentation:\n$$\n\\mathrm{MSE}^{\\star} = (1+2p) (c_{b}^2)^{1 - \\frac{2p}{2p+1}} \\left(\\frac{c_{v} \\kappa}{2p \\mathcal{B}}\\right)^{\\frac{2p}{2p+1}} = (1+2p) (c_b^2)^{\\frac{1}{2p+1}} \\left(\\frac{c_v \\kappa}{2p \\mathcal{B}}\\right)^{\\frac{2p}{2p+1}}\n$$\nAn equivalent elegant form is:\n$$\n\\mathrm{MSE}^{\\star} = \\frac{1+2p}{2p} V(n^\\star) = \\frac{1+2p}{2p} \\left(\\frac{c_v \\kappa}{\\mathcal{B}}\\right)^{\\frac{2p}{2p+1}}(2p c_b^2)^{\\frac{1}{2p+1}}\n$$\n\nThe scaling of the optimal parameters with respect to the budget $\\mathcal{B}$ is:\n-   $h^{\\star} \\propto \\mathcal{B}^{-\\frac{1}{2p+1}}$. As the budget increases, the optimal time step decreases, allowing for a reduction in bias.\n-   $n^{\\star} \\propto \\mathcal{B} \\cdot h^{\\star} \\propto \\mathcal{B} \\cdot \\mathcal{B}^{-\\frac{1}{2p+1}} = \\mathcal{B}^{\\frac{2p}{2p+1}}$. As the budget increases, the optimal number of samples increases, allowing for a reduction in variance.\n-   $\\mathrm{MSE}^{\\star} \\propto \\mathcal{B}^{-\\frac{2p}{2p+1}}$. The minimum achievable error decreases as the budget increases. The rate of this decrease, determined by the exponent $\\frac{2p}{2p+1}$, improves as the order of the method $p$ increases. For large $p$, the error scales nearly as $\\mathcal{B}^{-1}$.\n\nThe final expressions for the optimal quantities are:\n$$\nh^{\\star} = \\left(\\frac{c_{v} \\kappa}{2p c_{b}^2 \\mathcal{B}}\\right)^{\\frac{1}{2p+1}}\n$$\n$$\nn^{\\star} = \\frac{\\mathcal{B}}{\\kappa} \\left(\\frac{c_{v} \\kappa}{2p c_{b}^2 \\mathcal{B}}\\right)^{\\frac{1}{2p+1}}\n$$\n$$\n\\mathrm{MSE}^{\\star} = (1+2p) c_{b}^2 \\left(\\frac{c_{v} \\kappa}{2p c_{b}^2 \\mathcal{B}}\\right)^{\\frac{2p}{2p+1}}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\left(\\frac{c_{v} \\kappa}{2p c_{b}^2 \\mathcal{B}}\\right)^{\\frac{1}{2p+1}}  \\frac{\\mathcal{B}}{\\kappa} \\left(\\frac{c_{v} \\kappa}{2p c_{b}^2 \\mathcal{B}}\\right)^{\\frac{1}{2p+1}}  (1+2p) c_{b}^2 \\left(\\frac{c_{v} \\kappa}{2p c_{b}^2 \\mathcal{B}}\\right)^{\\frac{2p}{2p+1}} \\end{pmatrix}}\n$$"
        }
    ]
}