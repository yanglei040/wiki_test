{
    "hands_on_practices": [
        {
            "introduction": "This first practice explores a fundamental property of stochastic processes: the probability of reaching a certain state before another. Using a birth-death process, a cornerstone model for phenomena like queuing theory and population dynamics, you will derive and solve the backward equations to find this \"hitting probability.\" This exercise is crucial for understanding how the structure of the $Q$-matrix directly translates into a system of equations governing the long-term fate of the process .",
            "id": "3298791",
            "problem": "Consider a birth–death continuous-time Markov chain (CTMC) on the finite state space $\\{0,1,2,\\dots,j\\}$ with generator (also called the $Q$-matrix) entries given by $q_{i,i+1}=\\lambda_{i}$, $q_{i,i-1}=\\mu_{i}$, and $q_{i,i}=-(\\lambda_{i}+\\mu_{i})$ for $1 \\leq i \\leq j-1$, while $q_{0,0}=0$ and $q_{j,j}=0$ so that $0$ and $j$ are absorbing. Assume $\\lambda_{i}  0$ and $\\mu_{i}  0$ for all $1 \\leq i \\leq j-1$, and that the chain is nonexplosive.\n\nLet $T_{A}$ denote the first hitting time of a set $A \\subseteq \\{0,1,\\dots,j\\}$. Define, for each $i \\in \\{0,1,\\dots,j\\}$,\n$$\nh_{i} \\equiv \\mathbb{P}_{i}\\big(T_{\\{j\\}}  T_{\\{0\\}}\\big),\n$$\nthe probability that starting from state $i$, the chain ever hits the target state $j$ before it hits state $0$.\n\nTasks:\n- Starting only from the definition of the generator ($Q$-matrix) of a CTMC and first-step analysis, derive the backward difference equations that $h_{i}$ must satisfy for $1 \\leq i \\leq j-1$, together with the boundary conditions at $i=0$ and $i=j$.\n- Solve these equations to obtain a closed-form solution for $h_{i}$ in terms of the ratios of birth to death rates. Your solution must be expressed explicitly using products and sums of these ratios.\n- Finally, specialize to the homogeneous case $\\lambda_{i} \\equiv \\lambda  0$ and $\\mu_{i} \\equiv \\mu  0$ with $\\mu \\neq \\lambda$. Define the death-to-birth rate ratio $\\rho \\equiv \\mu/\\lambda$ and express $h_{i}$ for $1 \\leq i \\leq j-1$ in terms of $i$, $j$, and $\\rho$.\n\nExpress your final answer as a single closed-form analytic expression for $h_{i}$ in the homogeneous case, as a function of $i$, $j$, and $\\rho$. No numerical rounding is required.",
            "solution": "The problem as stated is a standard, well-posed problem in the theory of continuous-time Markov chains and is mathematically and scientifically sound. We can therefore proceed with a full derivation.\n\nThe problem concerns a birth-death continuous-time Markov chain on the finite state space $S = \\{0, 1, 2, \\dots, j\\}$. The states $0$ and $j$ are absorbing, which is reflected in the generator matrix entries $q_{0,0}=0$ and $q_{j,j}=0$. For the transient states $i \\in \\{1, 2, \\dots, j-1\\}$, the transition rates are given by $q_{i, i+1} = \\lambda_i  0$ and $q_{i, i-1} = \\mu_i  0$. The diagonal entries are $q_{i,i} = -(\\lambda_i + \\mu_i)$, representing the total rate of leaving state $i$.\n\nWe are asked to find $h_i \\equiv \\mathbb{P}_{i}(T_{\\{j\\}}  T_{\\{0\\}})$, the probability that the process, starting in state $i$, is absorbed at state $j$ rather than at state $0$.\n\n**Part 1: Derivation of the Backward Equations**\n\nThe absorption probabilities $h_i$ are harmonic with respect to the generator $Q$ on the set of transient states, $\\{1, 2, \\dots, j-1\\}$. This means that for any $i$ in this set, $(Qh)(i) = 0$. The operator $(Qh)(i)$ is defined as $\\sum_{k \\in S} q_{i,k} h_k$. For a state $i \\in \\{1, 2, \\dots, j-1\\}$, the only non-zero off-diagonal rates are to states $i-1$ and $i+1$.\n\nThe condition $(Qh)(i) = 0$ translates to:\n$$ q_{i, i-1} h_{i-1} + q_{i, i} h_i + q_{i, i+1} h_{i+1} = 0 $$\nSubstituting the given generator entries:\n$$ \\mu_i h_{i-1} - (\\lambda_i + \\mu_i)h_i + \\lambda_i h_{i+1} = 0 $$\nThis equation holds for $i \\in \\{1, 2, \\dots, j-1\\}$. We can rearrange this to obtain a backward difference equation.\n$$ \\lambda_i (h_{i+1} - h_i) = \\mu_i (h_i - h_{i-1}) $$\n\nNext, we establish the boundary conditions for $i=0$ and $i=j$.\nIf the process starts in state $i=0$, since this state is absorbing, the process remains at $0$ for all time. Thus, the hitting time for state $0$ is $T_{\\{0\\}} = 0$. As $j \\neq 0$, the hitting time for state $j$ must be $T_{\\{j\\}}  0$ (or infinite). Therefore, the event $T_{\\{j\\}}  T_{\\{0\\}}$ is impossible. The probability is $0$.\n$$ h_0 = \\mathbb{P}_0(T_{\\{j\\}}  T_{\\{0\\}}) = 0 $$\nIf the process starts in state $i=j$, it is already in the target absorbing state. The hitting time is $T_{\\{j\\}} = 0$. As $j \\neq 0$, $T_{\\{0\\}}  0$. The event $T_{\\{j\\}}  T_{\\{0\\}}$ is certain. The probability is $1$.\n$$ h_j = \\mathbb{P}_j(T_{\\{j\\}}  T_{\\{0\\}}) = 1 $$\n\n**Part 2: Solution for General Rates**\n\nLet $\\Delta_i = h_i - h_{i-1}$ for $i \\in \\{1, 2, \\dots, j\\}$. The difference equation becomes:\n$$ \\lambda_i \\Delta_{i+1} = \\mu_i \\Delta_i $$\nThis gives a recurrence relation for the differences:\n$$ \\Delta_{i+1} = \\frac{\\mu_i}{\\lambda_i} \\Delta_i \\quad \\text{for } 1 \\le i \\le j-1 $$\nWe can solve this by iteration:\n$$ \\Delta_i = \\left( \\prod_{k=1}^{i-1} \\frac{\\mu_k}{\\lambda_k} \\right) \\Delta_1 \\quad \\text{for } i \\ge 2 $$\nLet's define $\\pi_0 = 1$ and for $k \\ge 1$, $\\pi_k = \\prod_{l=1}^{k} \\frac{\\mu_l}{\\lambda_l}$. Then the relation can be written as $\\Delta_i = \\pi_{i-1} \\Delta_1$ for $i \\ge 1$.\n\nNow, we express $h_i$ as a sum of these differences (a telescoping sum):\n$$ h_i = h_0 + \\sum_{k=1}^{i} (h_k - h_{k-1}) = h_0 + \\sum_{k=1}^{i} \\Delta_k $$\nUsing the boundary condition $h_0=0$:\n$$ h_i = \\sum_{k=1}^{i} \\Delta_k = \\sum_{k=1}^{i} \\pi_{k-1} \\Delta_1 = \\Delta_1 \\sum_{k=0}^{i-1} \\pi_k $$\nTo find the constant $\\Delta_1 = h_1 - h_0 = h_1$, we apply the second boundary condition, $h_j=1$:\n$$ h_j = 1 = \\Delta_1 \\sum_{k=0}^{j-1} \\pi_k $$\nThis implies:\n$$ \\Delta_1 = \\frac{1}{\\sum_{k=0}^{j-1} \\pi_k} $$\nSubstituting this back into the expression for $h_i$:\n$$ h_i = \\frac{\\sum_{k=0}^{i-1} \\pi_k}{\\sum_{k=0}^{j-1} \\pi_k} = \\frac{\\sum_{k=0}^{i-1} \\left( \\prod_{l=1}^{k} \\frac{\\mu_l}{\\lambda_l} \\right)}{\\sum_{k=0}^{j-1} \\left( \\prod_{l=1}^{k} \\frac{\\mu_l}{\\lambda_l} \\right)} $$\nwhere the product $\\prod_{l=1}^{0} (\\cdot)$ is understood to be $1$. This is the closed-form solution for the general case.\n\n**Part 3: Specialization to the Homogeneous Case**\n\nWe are given the homogeneous case where $\\lambda_i \\equiv \\lambda  0$ and $\\mu_i \\equiv \\mu  0$ for all valid $i$, and $\\mu \\neq \\lambda$. The ratio of death to birth rates is constant:\n$$ \\rho \\equiv \\frac{\\mu}{\\lambda} $$\nThe terms $\\pi_k$ become:\n$$ \\pi_k = \\prod_{l=1}^{k} \\frac{\\mu}{\\lambda} = \\rho^k $$\nThe expression for $h_i$ simplifies to a ratio of geometric series:\n$$ h_i = \\frac{\\sum_{k=0}^{i-1} \\rho^k}{\\sum_{k=0}^{j-1} \\rho^k} $$\nSince the problem states $\\mu \\neq \\lambda$, we have $\\rho \\neq 1$. We can use the formula for the sum of a finite geometric series, $\\sum_{k=0}^{n-1} r^k = \\frac{r^n - 1}{r-1}$.\nApplying this formula to the numerator and denominator:\n$$ \\sum_{k=0}^{i-1} \\rho^k = \\frac{\\rho^i - 1}{\\rho - 1} $$\n$$ \\sum_{k=0}^{j-1} \\rho^k = \\frac{\\rho^j - 1}{\\rho - 1} $$\nSubstituting these into the expression for $h_i$:\n$$ h_i = \\frac{\\frac{\\rho^i - 1}{\\rho - 1}}{\\frac{\\rho^j - 1}{\\rho - 1}} = \\frac{\\rho^i - 1}{\\rho^j - 1} $$\nThis expression is valid for $i \\in \\{1, 2, \\dots, j-1\\}$. This is the required closed-form solution for $h_i$ in the homogeneous case as a function of $i$, $j$, and $\\rho$.\nFor checking, we can verify the boundary conditions. For $i=0$, $h_0 = (\\rho^0 - 1)/(\\rho^j - 1) = (1-1)/(\\rho^j-1) = 0$. For $i=j$, $h_j = (\\rho^j-1)/(\\rho^j-1)=1$. The formula holds for the boundaries as well.",
            "answer": "$$\n\\boxed{\\frac{\\rho^{i} - 1}{\\rho^{j} - 1}}\n$$"
        },
        {
            "introduction": "While exact analytical solutions are valuable, most real-world continuous-time Markov chains are too complex and must be simulated. This exercise bridges theory and practice by analyzing the accuracy of a popular simulation algorithm, the explicit tau-leaping method. You will use the generator $Q$ and its associated semigroup $\\exp(\\tau Q)$ as the \"ground truth\" to derive the local weak error of the approximation, revealing a foundational property of this widely-used numerical scheme .",
            "id": "3298772",
            "problem": "Consider a Continuous-Time Markov Chain (CTMC) on a countable state space $S \\subset \\mathbb{Z}^{d}$ specified by a finite set of reaction channels indexed by $r \\in \\{1,\\dots,R\\}$ with state-dependent intensities $a_{r}(x) \\ge 0$ and stoichiometric increment vectors $\\nu_{r} \\in \\mathbb{Z}^{d}$. The infinitesimal generator $Q$ acts on a bounded test function $f : S \\to \\mathbb{R}$ by\n$$\n(Q f)(x) \\;=\\; \\sum_{r=1}^{R} a_{r}(x)\\,\\big(f(x+\\nu_{r}) - f(x)\\big).\n$$\nLet $X(t)$ denote the exact CTMC started at $X(0)=x \\in S$, and define its expectation semigroup by the standard representation $[\\,\\exp(\\tau Q)\\,f\\,](x) = \\mathbb{E}\\big[f(X(\\tau)) \\,\\big|\\, X(0)=x\\big]$.\n\nThe explicit tau-leaping method with step size $\\tau0$ approximates the one-step distribution by freezing the intensities at the current state $x$ and drawing independent Poisson random variables $P_{r} \\sim \\mathrm{Poisson}(a_{r}(x)\\,\\tau)$, yielding the leap $Y = x + \\sum_{r=1}^{R} \\nu_{r} P_{r}$. For any bounded test function $f$, the tau-leap one-step expectation is $\\mathbb{E}\\big[f(Y)\\big]$.\n\nDefine the one-step weak error as\n$$\n\\Delta(\\tau;x,f) \\;=\\; \\big[\\,\\exp(\\tau Q)\\,f\\,\\big](x) \\;-\\; \\mathbb{E}\\big[f(Y)\\big].\n$$\nStarting from the core definitions above and without introducing any shortcut formulas beyond standard semigroup and generator properties, derive the small-$\\tau$ asymptotic for $\\Delta(\\tau;x,f)$ and identify the coefficient of the $\\mathcal{O}(\\tau)$ term in this expansion. Your final answer must be the value of that coefficient as a single closed-form analytic expression. No rounding is required and no physical units arise in this problem.",
            "solution": "The objective is to find the coefficient of the $\\mathcal{O}(\\tau)$ term in the small-$\\tau$ asymptotic expansion of the one-step weak error, which is defined as:\n$$\n\\Delta(\\tau;x,f) \\;=\\; \\big[\\,\\exp(\\tau Q)\\,f\\,\\big](x) \\;-\\; \\mathbb{E}\\big[f(Y)\\big]\n$$\nTo achieve this, we will find the Taylor series expansions of both terms with respect to $\\tau$ up to the first order.\n\nFirst, we expand the exact expectation term, $[\\,\\exp(\\tau Q)\\,f\\,](x)$. The term involves the expectation semigroup $\\exp(\\tau Q)$. Its standard Taylor series expansion in $\\tau$ around $\\tau=0$ is given by:\n$$\n\\exp(\\tau Q) \\;=\\; I + \\tau Q + \\frac{\\tau^2}{2!} Q^2 + \\dots + \\frac{\\tau^k}{k!} Q^k + \\dots\n$$\nwhere $I$ is the identity operator. Applying this operator expansion to the test function $f$ and evaluating at the state $x$, we are interested in the terms up to order $\\tau$.\n$$\n[\\,\\exp(\\tau Q)\\,f\\,](x) \\;=\\; \\big[\\,(I + \\tau Q + \\mathcal{O}(\\tau^2))\\,f\\,\\big](x) \\;=\\; f(x) + \\tau (Qf)(x) + \\mathcal{O}(\\tau^2)\n$$\nThis provides the expansion for the first part of the error expression.\n\nNext, we analyze the tau-leaping expectation, $\\mathbb{E}[f(Y)]$. The state after one tau-leap, $Y$, is defined as $Y = x + \\sum_{r=1}^{R} \\nu_{r} P_{r}$, where the $P_{r}$ are independent Poisson random variables, $P_{r} \\sim \\mathrm{Poisson}(\\lambda_r)$ with $\\lambda_r = a_r(x)\\tau$.\nTo compute the expectation $\\mathbb{E}[f(Y)]$, it is convenient to use shift operators. Let $T_v$ be the operator that shifts the argument of a function by a vector $v$, i.e., $T_v f(x) = f(x+v)$. The state $Y$ can be used to write $f(Y)$ as:\n$$\nf(Y) = f\\left(x + \\sum_{r=1}^{R} \\nu_{r} P_{r}\\right) = \\left(\\prod_{r=1}^{R} T_{\\nu_r}^{P_r}\\right) f(x)\n$$\nWe now take the expectation. Since the random variables $P_r$ are independent, the expectation of the product of operators is the product of the expectations:\n$$\n\\mathbb{E}[f(Y)] = \\mathbb{E}\\left[\\left(\\prod_{r=1}^{R} T_{\\nu_r}^{P_r}\\right)f(x)\\right] = \\left(\\prod_{r=1}^{R} \\mathbb{E}[T_{\\nu_r}^{P_r}]\\right)f(x)\n$$\nFor a single reaction channel $r$, the expectation $\\mathbb{E}[T_{\\nu_r}^{P_r}]$ is calculated using the probability mass function of the Poisson distribution $P(P_r=k) = e^{-\\lambda_r} \\lambda_r^k / k!$:\n$$\n\\mathbb{E}[T_{\\nu_r}^{P_r}] = \\sum_{k=0}^{\\infty} (T_{\\nu_r})^k P(P_r=k) = \\sum_{k=0}^{\\infty} (T_{\\nu_r})^k \\frac{e^{-\\lambda_r} \\lambda_r^k}{k!} = e^{-\\lambda_r} \\sum_{k=0}^{\\infty} \\frac{(\\lambda_r T_{\\nu_r})^k}{k!} = e^{-\\lambda_r} \\exp(\\lambda_r T_{\\nu_r}) = \\exp(\\lambda_r(T_{\\nu_r}-I))\n$$\nThe total operator for the tau-leap expectation is the product of these individual operators. Since shift operators commute ($T_{v_1} T_{v_2} = T_{v_2} T_{v_1}$), the operators $(T_{\\nu_r}-I)$ also commute. Thus, the product of their exponentials is the exponential of their sum:\n$$\n\\prod_{r=1}^{R} \\mathbb{E}[T_{\\nu_r}^{P_r}] = \\prod_{r=1}^{R} \\exp(\\lambda_r(T_{\\nu_r}-I)) = \\exp\\left(\\sum_{r=1}^{R} \\lambda_r(T_{\\nu_r}-I)\\right)\n$$\nSubstituting $\\lambda_r = a_r(x)\\tau$, we obtain:\n$$\n\\mathbb{E}[f(Y)] = \\left[ \\exp\\left(\\tau \\sum_{r=1}^{R} a_r(x)(T_{\\nu_r}-I)\\right) \\right] f(x)\n$$\nThe operator within the exponential, $\\sum_{r=1}^{R} a_r(x)(T_{\\nu_r}-I)$, is equivalent to the generator $Q$ but with the propensities \"frozen\" at the initial state $x$. Let's denote this operator by $Q_x$. Note that the action of $Q$ on a function $g$ is $(Qg)(y) = \\sum_r a_r(y)(g(y+\\nu_r)-g(y))$, whereas for $Q_x$ it is $(Q_x g)(y) = \\sum_r a_r(x)(g(y+\\nu_r)-g(y))$.\nSo, we can write the tau-leap expectation as $\\mathbb{E}[f(Y)] = [\\exp(\\tau Q_x)f](x)$.\nWe now expand this expression for small $\\tau$:\n$$\n\\mathbb{E}[f(Y)] = \\big[\\,(I + \\tau Q_x + \\mathcal{O}(\\tau^2))\\,f\\,\\big](x) = f(x) + \\tau (Q_x f)(x) + \\mathcal{O}(\\tau^2)\n$$\nTo compare this with the exact expansion, we must evaluate $(Q_x f)(x)$:\n$$\n(Q_x f)(x) = \\left[\\left(\\sum_{r=1}^{R} a_r(x)(T_{\\nu_r}-I)\\right)f\\right](x) = \\sum_{r=1}^{R} a_r(x) (f(x+\\nu_r)-f(x))\n$$\nFrom the problem's definition, this is identical to $(Qf)(x)$. Thus, $(Q_x f)(x) = (Qf)(x)$.\nThe expansion for the tau-leap expectation is therefore:\n$$\n\\mathbb{E}[f(Y)] = f(x) + \\tau (Qf)(x) + \\mathcal{O}(\\tau^2)\n$$\nFinally, we compute the weak error $\\Delta(\\tau;x,f)$ by subtracting the expansion of $\\mathbb{E}[f(Y)]$ from the expansion of $[\\exp(\\tau Q)f](x)$:\n$$\n\\Delta(\\tau;x,f) = \\left(f(x) + \\tau (Qf)(x) + \\mathcal{O}(\\tau^2)\\right) - \\left(f(x) + \\tau (Qf)(x) + \\mathcal{O}(\\tau^2)\\right) = \\mathcal{O}(\\tau^2)\n$$\nThe asymptotic expansion of the error is $\\Delta(\\tau;x,f) = 0 \\cdot \\tau^0 + 0 \\cdot \\tau^1 + C_2 \\tau^2 + \\dots$. The coefficient of the $\\mathcal{O}(\\tau)$ term is the coefficient of $\\tau^1$, which our derivation shows to be zero. This is an expected result, as the explicit tau-leaping method is known to be first-order accurate in the weak sense, implying that its local weak error is of order $\\mathcal{O}(\\tau^2)$.\n\nThe question asks for the coefficient of the $\\mathcal{O}(\\tau)$ term. Based on this rigorous derivation, that coefficient is $0$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "Our models often contain parameters that are uncertain or need to be fitted to data. This final practice dives into the advanced topic of sensitivity analysis, demonstrating how to compute the derivative of an expected outcome with respect to a model parameter $\\theta$. You will derive the \"score function\" from the parametric generator $Q^{\\theta}$ and use it within a likelihood ratio estimator, a powerful Monte Carlo technique that is central to stochastic optimization and model calibration .",
            "id": "3298796",
            "problem": "Consider a parametric family of Continuous-Time Markov Chains (CTMC) on the finite state space $\\{0,1\\}$ with generator (also called the $Q$-matrix) given by\n$$\nQ^\\theta \\;=\\; \\begin{pmatrix}\n-\\theta  \\theta \\\\\n\\beta  -\\beta\n\\end{pmatrix},\n$$\nwhere $\\theta \\in (0,\\infty)$ is the parameter of interest and $\\beta \\in (0,\\infty)$ is a fixed constant. Let $\\{X_t^\\theta\\}_{t \\ge 0}$ denote the CTMC with initial state $X_0^\\theta = x_0 \\in \\{0,1\\}$, governed by the standard CTMC construction: conditional on the current state $i \\in \\{0,1\\}$, the holding time is exponentially distributed with rate $\\lambda_i^\\theta = -q_{ii}^\\theta$, and the next state $j \\ne i$ is chosen with transition probability $q_{ij}^\\theta / \\lambda_i^\\theta$.\n\nLet $T  0$ be a fixed horizon and define the observable $g(X_T^\\theta) = \\mathbf{1}\\{X_T^\\theta = 1\\}$. The objective is to estimate the pathwise derivative $\\partial_\\theta \\mathbb{E}[g(X_T^\\theta)]$ using the likelihood ratio (also known as score function) method based on Radon–Nikodym derivatives between path measures induced by $Q^\\theta$.\n\nFundamental base to use:\n- The CTMC path density on $[0,T]$ induced by a generator $Q^\\theta$ admits the usual multiplicative form given by the product over jumps of $q_{X_{t^-},X_t}^\\theta$ and the exponential of minus the time-integral of exit rates $\\lambda_{X_t}^\\theta = -q_{X_t,X_t}^\\theta$.\n- The Radon–Nikodym derivative of path measures yields the log-likelihood of a path under parameter $\\theta$, and its derivative with respect to $\\theta$ is the score.\n- The score function identity (under standard regularity conditions) states that for any integrable functional $H$, $\\partial_\\theta \\mathbb{E}_\\theta[H] = \\mathbb{E}_\\theta[H \\cdot S_T(\\theta)]$, where $S_T(\\theta)$ is the derivative with respect to $\\theta$ of the log-likelihood of the observed path over $[0,T]$.\n\nTasks:\n- Starting from the fundamental base above and without assuming any shortcut formulas, derive an explicit expression for the score $S_T(\\theta)$ specialized to the two-state model with $Q^\\theta$ given above. Your derivation must be expressed in terms of:\n  - The total time spent in state $0$ up to time $T$, denoted $\\int_0^T \\mathbf{1}\\{X_t^\\theta=0\\} \\, dt$.\n  - The number of jumps from state $0$ to state $1$ up to time $T$, denoted $N_{01}(T)$.\n- Use the score function identity to obtain a likelihood ratio estimator for $\\partial_\\theta \\mathbb{E}[g(X_T^\\theta)]$ of the form $\\widehat{\\Delta}_{\\mathrm{LR}} = \\frac{1}{M} \\sum_{m=1}^M g(X_T^{\\theta,(m)}) S_T^{(m)}(\\theta)$.\n- Consider the variance reduction by centering with a baseline $c \\in \\mathbb{R}$ to form the centered estimator $\\widehat{\\Delta}_{\\mathrm{LR}}^{(c)} = \\frac{1}{M} \\sum_{m=1}^M \\bigl(g(X_T^{\\theta,(m)}) - c\\bigr) S_T^{(m)}(\\theta)$. Explain why centering preserves unbiasedness, and implement a two-stage procedure where a pilot run is used to approximate the optimal constant baseline $c^\\star = \\mathrm{Cov}(g(X_T^\\theta), S_T(\\theta))/\\mathrm{Var}(S_T(\\theta))$.\n- Derive in closed form the exact sensitivity $\\partial_\\theta \\mathbb{E}[g(X_T^\\theta)]$ for the two-state model using only the forward equation for the marginal probability $p_\\theta(t) = \\mathbb{P}(X_t^\\theta=1)$ and elementary calculus. Express the result explicitly in terms of $\\theta$, $\\beta$, $T$, and $x_0$.\n\nImplementation requirements:\n- Implement an exact simulator for sample paths of the CTMC on $\\{0,1\\}$ with the given $Q^\\theta$ over horizon $[0,T]$, accumulating both $N_{01}(T)$ and $\\int_0^T \\mathbf{1}\\{X_t^\\theta=0\\}\\,dt$, and the final state $X_T^\\theta$, for each replication. Use independent and identically distributed samples across replications.\n- For each test case below, use a two-stage Monte Carlo computation:\n  - A pilot stage with $M_{\\text{pilot}}$ samples to estimate the baseline $\\widehat{c}$ as $\\widehat{c} = \\widehat{\\mathrm{Cov}}(g,S)/\\widehat{\\mathrm{Var}}(S)$.\n  - A main stage with $M_{\\text{main}}$ samples to estimate:\n    - The uncentered LR mean $\\widehat{\\Delta}_{\\mathrm{LR}}$ and sample variance $\\widehat{\\mathrm{Var}}(\\text{uncentered})$.\n    - The centered LR mean $\\widehat{\\Delta}_{\\mathrm{LR}}^{(\\widehat{c})}$ and sample variance $\\widehat{\\mathrm{Var}}(\\text{centered})$.\n- Independently compute the exact sensitivity $\\partial_\\theta \\mathbb{E}[g(X_T^\\theta)]$ using your closed-form formula, and compare it to the Monte Carlo mean. Use an absolute tolerance of $\\varepsilon = 0.02$ to decide agreement.\n\nTest suite:\n- Case $\\mathrm{A}$: $\\theta = 1.5$, $\\beta = 2.0$, $T = 1.0$, $x_0 = 0$.\n- Case $\\mathrm{B}$: $\\theta = 0.2$, $\\beta = 3.0$, $T = 2.0$, $x_0 = 0$.\n- Case $\\mathrm{C}$: $\\theta = 1.0$, $\\beta = 1.0$, $T = 0.0$, $x_0 = 1$.\n- Case $\\mathrm{D}$: $\\theta = 4.0$, $\\beta = 0.5$, $T = 5.0$, $x_0 = 0$.\n- Variance growth check (same parameters across two horizons):\n  - Case $\\mathrm{E1}$: $\\theta = 1.5$, $\\beta = 2.0$, $T = 0.5$, $x_0 = 0$.\n  - Case $\\mathrm{E2}$: $\\theta = 1.5$, $\\beta = 2.0$, $T = 3.0$, $x_0 = 0$.\n\nQuantifiable outputs:\n- For each of Cases $\\mathrm{A}$–$\\mathrm{D}$, output two booleans:\n  - $b_1$: whether the absolute error between the main-stage LR estimate (uncentered) and the exact sensitivity is at most $\\varepsilon$.\n  - $b_2$: whether the main-stage centered variance is less than or equal to the main-stage uncentered variance.\n- For the variance growth check using Cases $\\mathrm{E1}$ and $\\mathrm{E2}$, output one boolean:\n  - $b_3$: whether the uncentered LR variance for $T=3.0$ is greater than or equal to that for $T=0.5$.\n\nMonte Carlo sizes and reproducibility:\n- Use $M_{\\text{pilot}} = 20000$ and $M_{\\text{main}} = 120000$ for each case, and fix the random seed to ensure reproducibility.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be ordered as\n$$\n[b_{\\mathrm{A},1}, b_{\\mathrm{A},2}, b_{\\mathrm{B},1}, b_{\\mathrm{B},2}, b_{\\mathrm{C},1}, b_{\\mathrm{C},2}, b_{\\mathrm{D},1}, b_{\\mathrm{D},2}, b_{\\mathrm{E},3}],\n$$\nwhere each $b_{\\cdot,\\cdot}$ is a boolean as defined above. No other text should be printed.",
            "solution": "The user-provided problem has been assessed and validated as a well-posed and scientifically sound exercise in computational stochastic modeling. All necessary parameters and definitions are provided, the theoretical underpinnings are standard, and the objectives are clear and formalizable. We may therefore proceed with a complete solution.\n\n### Part 1: Derivation of the Score Function $S_T(\\theta)$\n\nThe score function is the derivative of the log-likelihood of a sample path with respect to the parameter of interest, $\\theta$. A path of a Continuous-Time Markov Chain (CTMC) on a finite state space, observed over the interval $[0, T]$, is characterized by the initial state $X_0$, the sequence of states visited, and the times of the jumps between them.\n\nThe likelihood of a specific path realization $\\omega = (X_s)_{s \\in [0,T]}$ is given by the product of the transition rates for each jump and the survival probabilities for the holding times in each state. This can be expressed as:\n$$\nL_T(\\theta; \\omega) = \\left( \\prod_{k=1}^{N(T)} q_{X_{t_k^-}, X_{t_k}}^\\theta \\right) \\exp\\left( -\\int_0^T \\lambda_{X_s}^\\theta \\, ds \\right)\n$$\nwhere $N(T)$ is the number of jumps in $[0, T]$ occurring at times $t_k$, $q_{ij}^\\theta$ is the transition rate from state $i$ to $j$, and $\\lambda_i^\\theta = -q_{ii}^\\theta$ is the total exit rate from state $i$.\n\nThe log-likelihood, $\\log L_T(\\theta)$, is:\n$$\n\\log L_T(\\theta; \\omega) = \\sum_{k=1}^{N(T)} \\log q_{X_{t_k^-}, X_{t_k}}^\\theta - \\int_0^T \\lambda_{X_s}^\\theta \\, ds\n$$\nFor the given two-state model with generator $Q^\\theta = \\begin{pmatrix} -\\theta  \\theta \\\\ \\beta  -\\beta \\end{pmatrix}$, we have:\n-   Transition rates: $q_{01}^\\theta = \\theta$ and $q_{10}^\\theta = \\beta$. Only $q_{01}^\\theta$ depends on $\\theta$.\n-   Exit rates: $\\lambda_0^\\theta = \\theta$ and $\\lambda_1^\\theta = \\beta$. Only $\\lambda_0^\\theta$ depends on $\\theta$.\n\nLet $N_{01}(T)$ be the number of jumps from state $0$ to $1$ in $[0, T]$, and $N_{10}(T)$ be the number of jumps from $1$ to $0$. Let $T_0 = \\int_0^T \\mathbf{1}\\{X_s=0\\} \\, ds$ and $T_1 = \\int_0^T \\mathbf{1}\\{X_s=1\\} \\, ds$ be the total time spent in states $0$ and $1$, respectively.\n\nThe log-likelihood for this specific model becomes:\n$$\n\\log L_T(\\theta) = N_{01}(T) \\log(q_{01}^\\theta) + N_{10}(T) \\log(q_{10}^\\theta) - (T_0 \\lambda_0^\\theta + T_1 \\lambda_1^\\theta)\n$$\nSubstituting the specific rates:\n$$\n\\log L_T(\\theta) = N_{01}(T) \\log(\\theta) + N_{10}(T) \\log(\\beta) - T_0 \\theta - T_1 \\beta\n$$\nThe score function $S_T(\\theta)$ is the derivative of $\\log L_T(\\theta)$ with respect to $\\theta$. For a fixed sample path, the quantities $N_{01}(T)$, $N_{10}(T)$, $T_0$, and $T_1$ are constants.\n$$\nS_T(\\theta) = \\frac{\\partial}{\\partial \\theta} \\left( N_{01}(T) \\log(\\theta) + N_{10}(T) \\log(\\beta) - T_0 \\theta - T_1 \\beta \\right)\n$$\nTaking the derivative, we find that only the terms involving $\\theta$ contribute:\n$$\nS_T(\\theta) = \\frac{N_{01}(T)}{\\theta} - T_0\n$$\nThis is the explicit expression for the score in terms of the required path statistics.\n\n### Part 2: Likelihood Ratio Estimation and Centering\n\nThe score function identity states that for a suitably regular observable $H$, $\\partial_\\theta \\mathbb{E}_\\theta[H] = \\mathbb{E}_\\theta[H \\cdot S_T(\\theta)]$. Applying this to our observable $g(X_T^\\theta) = \\mathbf{1}\\{X_T^\\theta = 1\\}$, the sensitivity is $\\partial_\\theta \\mathbb{E}[g(X_T^\\theta)] = \\mathbb{E}[g(X_T^\\theta) S_T(\\theta)]$. A Monte Carlo estimator based on $M$ independent paths is:\n$$\n\\widehat{\\Delta}_{\\mathrm{LR}} = \\frac{1}{M} \\sum_{m=1}^M g(X_T^{\\theta,(m)}) S_T^{(m)}(\\theta)\n$$\nTo reduce the variance of this estimator, a baseline $c \\in \\mathbb{R}$ can be introduced, forming the centered estimator:\n$$\n\\widehat{\\Delta}_{\\mathrm{LR}}^{(c)} = \\frac{1}{M} \\sum_{m=1}^M \\bigl(g(X_T^{\\theta,(m)}) - c\\bigr) S_T^{(m)}(\\theta)\n$$\nThis estimator remains unbiased for any constant $c$ because the expectation of the score function is zero, $\\mathbb{E}[S_T(\\theta)] = 0$. This property holds under regularity conditions that allow exchanging differentiation and integration, which are satisfied here.\nThe expectation of the centered estimator is:\n$$\n\\mathbb{E}[\\widehat{\\Delta}_{\\mathrm{LR}}^{(c)}] = \\mathbb{E}[(g(X_T^\\theta) - c) S_T(\\theta)] = \\mathbb{E}[g(X_T^\\theta) S_T(\\theta)] - c \\mathbb{E}[S_T(\\theta)] = \\mathbb{E}[g(X_T^\\theta) S_T(\\theta)] - c \\cdot 0 = \\partial_\\theta \\mathbb{E}[g(X_T^\\theta)]\n$$\nThe problem specifies using a two-stage method to estimate a baseline $c^\\star = \\mathrm{Cov}(g(X_T^\\theta), S_T(\\theta))/\\mathrm{Var}(S_T(\\theta))$. This is implemented by using a pilot run of $M_{\\text{pilot}}$ samples to compute sample estimates $\\widehat{\\mathrm{Cov}}(g,S)$ and $\\widehat{\\mathrm{Var}}(S)$ to obtain an estimate $\\widehat{c}$, which is then used in a main run of $M_{\\text{main}}$ samples.\n\n### Part 3: Exact Sensitivity via Forward Equation\n\nThe exact sensitivity can be derived analytically. Let $p_1(t) = \\mathbb{P}(X_t^\\theta=1)$. The state probabilities evolve according to the Kolmogorov forward equation $\\frac{d\\vec{p}}{dt} = \\vec{p} Q^\\theta$. For $p_1(t)$, this yields the ordinary differential equation:\n$$\n\\frac{dp_1(t)}{dt} = p_0(t)q_{01}^\\theta + p_1(t)q_{11}^\\theta = (1-p_1(t))\\theta + p_1(t)(-\\beta) = \\theta - (\\theta+\\beta)p_1(t)\n$$\nThis is a first-order linear ODE. With an initial condition $p_1(0) = \\mathbf{1}\\{x_0=1\\}$, the solution at time $T$ is:\n$$\np_1(T) = \\left(p_1(0) - \\frac{\\theta}{\\theta+\\beta}\\right) e^{-(\\theta+\\beta)T} + \\frac{\\theta}{\\theta+\\beta}\n$$\nWe seek the derivative of $p_1(T)$ with respect to $\\theta$:\n$$\n\\frac{\\partial p_1(T)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left[ \\left(p_1(0) - \\frac{\\theta}{\\theta+\\beta}\\right) e^{-(\\theta+\\beta)T} + \\frac{\\theta}{\\theta+\\beta} \\right]\n$$\nUsing the product and quotient rules for differentiation, we obtain:\n$$\n\\frac{\\partial p_1(T)}{\\partial \\theta} = \\left(-\\frac{\\beta}{(\\theta+\\beta)^2}\\right)e^{-(\\theta+\\beta)T} + \\left(p_1(0) - \\frac{\\theta}{\\theta+\\beta}\\right)\\left(-T e^{-(\\theta+\\beta)T}\\right) + \\frac{\\beta}{(\\theta+\\beta)^2}\n$$\nThis expression can be simplified to:\n$$\n\\frac{\\partial p_1(T)}{\\partial \\theta} = \\frac{\\beta}{(\\theta+\\beta)^2} \\left(1 - e^{-(\\theta+\\beta)T}\\right) - T \\left(p_1(0) - \\frac{\\theta}{\\theta+\\beta}\\right) e^{-(\\theta+\\beta)T}\n$$\nThis closed-form formula provides the exact value against which the Monte Carlo estimates will be benchmarked.\nThe implementation will follow from these derivations, simulating CTMC paths to collect statistics, computing the LR estimates (both uncentered and centered), comparing them to the exact value, and evaluating the required boolean conditions.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CTMC sensitivity analysis problem as specified.\n    \n    This function encapsulates all logic, including simulation, analytical calculations,\n    and result aggregation, to produce the final output in the required format.\n    It adheres to the specified Python version and library constraints.\n    \"\"\"\n\n    # Fixed random seed for reproducibility as per problem statement\n    SEED = 42\n    \n    # Monte Carlo simulation parameters\n    M_PILOT = 20000\n    M_MAIN = 120000\n    EPSILON = 0.02\n\n    # Global RNG instance for the entire run\n    RNG = np.random.default_rng(SEED)\n\n    def ctmc_simulator(theta, beta, T, x0):\n        \"\"\"Simulates one path of the 2-state CTMC.\"\"\"\n        current_time = 0.0\n        current_state = x0\n        \n        time_in_state_0 = 0.0\n        jumps_0_to_1 = 0\n\n        if T == 0.0:\n            if x0 == 0:\n                time_in_state_0 = 0.0 # Or T, but T is 0\n            return x0, time_in_state_0, 0\n        \n        while current_time  T:\n            is_state_0 = (current_state == 0)\n            \n            # This check is for robustness against theta=0 or beta=0, though problem statement implies > 0\n            rate = theta if is_state_0 else beta\n            if rate = 0:\n                time_spent_in_state = T - current_time\n                if is_state_0:\n                    time_in_state_0 += time_spent_in_state\n                current_time = T\n                continue\n\n            holding_time = RNG.exponential(1.0 / rate)\n            \n            time_spent_in_state = min(holding_time, T - current_time)\n            \n            if is_state_0:\n                time_in_state_0 += time_spent_in_state\n                \n            current_time += time_spent_in_state\n            \n            if current_time  T: # A jump occurred before T\n                if is_state_0:\n                    jumps_0_to_1 += 1\n                current_state = 1 - current_state\n            # else: process terminates at T, state doesn't change\n            \n        return current_state, time_in_state_0, jumps_0_to_1\n\n    def exact_sensitivity(theta, beta, T, x0):\n        \"\"\"Computes the exact sensitivity using the derived analytical formula.\"\"\"\n        p1_0 = 1.0 if x0 == 1 else 0.0\n        \n        if T == 0.0:\n            return 0.0\n\n        s = theta + beta\n        exp_term = np.exp(-s * T)\n        \n        deriv = (beta / s**2) * (1 - exp_term) - T * (p1_0 - theta / s) * exp_term\n        return deriv\n    \n    def run_simulation_for_case(params):\n        \"\"\"Executes the two-stage MC simulation for a given test case.\"\"\"\n        theta, beta, T, x0 = params\n\n        # Pilot Stage to estimate the baseline c\n        pilot_g = np.zeros(M_PILOT)\n        pilot_S = np.zeros(M_PILOT)\n        \n        for i in range(M_PILOT):\n            final_state, time_in_0, n_01 = ctmc_simulator(theta, beta, T, x0)\n            pilot_g[i] = 1.0 if final_state == 1 else 0.0\n            pilot_S[i] = (n_01 / theta) - time_in_0 if theta > 0 else -time_in_0\n\n        var_S = np.var(pilot_S, ddof=1)\n        if var_S > 1e-15:\n            # np.cov uses N-1 denominator by default, which is sample covariance\n            cov_matrix = np.cov(pilot_g, pilot_S)\n            cov_g_S = cov_matrix[0, 1]\n            c_hat = cov_g_S / var_S\n        else:\n            # If Score has no variance (e.g., T=0), any baseline works as S=0.\n            c_hat = 0.0\n\n        # Main Stage for estimation\n        uncentered_lr_samples = np.zeros(M_MAIN)\n        centered_lr_samples = np.zeros(M_MAIN)\n\n        for i in range(M_MAIN):\n            final_state, time_in_0, n_01 = ctmc_simulator(theta, beta, T, x0)\n            g = 1.0 if final_state == 1 else 0.0\n            S = (n_01 / theta) - time_in_0 if theta > 0 else -time_in_0\n\n            uncentered_lr_samples[i] = g * S\n            centered_lr_samples[i] = (g - c_hat) * S\n\n        uncentered_mean = np.mean(uncentered_lr_samples)\n        uncentered_var = np.var(uncentered_lr_samples, ddof=1)\n        \n        centered_mean = np.mean(centered_lr_samples) # for debug\n        centered_var = np.var(centered_lr_samples, ddof=1)\n\n        exact_val = exact_sensitivity(theta, beta, T, x0)\n\n        b1 = np.abs(uncentered_mean - exact_val) = EPSILON\n        b2 = centered_var = uncentered_var\n\n        return b1, b2, uncentered_var\n\n    test_cases = {\n        \"A\": (1.5, 2.0, 1.0, 0),\n        \"B\": (0.2, 3.0, 2.0, 0),\n        \"C\": (1.0, 1.0, 0.0, 1),\n        \"D\": (4.0, 0.5, 5.0, 0),\n        \"E1\": (1.5, 2.0, 0.5, 0),\n        \"E2\": (1.5, 2.0, 3.0, 0)\n    }\n\n    final_results = []\n    \n    # Cases A, B, C, D\n    for case_name in [\"A\", \"B\", \"C\", \"D\"]:\n        params = test_cases[case_name]\n        b1, b2, _ = run_simulation_for_case(params)\n        final_results.extend([b1, b2])\n\n    # Case E for variance growth check\n    _, _, var_e1 = run_simulation_for_case(test_cases[\"E1\"])\n    _, _, var_e2 = run_simulation_for_case(test_cases[\"E2\"])\n    bE3 = var_e2 >= var_e1\n    final_results.append(bE3)\n    \n    # Format the final output as a single line string\n    # Python's str(True) is 'True', which needs to be lowercased for JSON-like booleans.\n    # The prompt asks for `[True,True,...]` so `str()` is correct.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        }
    ]
}