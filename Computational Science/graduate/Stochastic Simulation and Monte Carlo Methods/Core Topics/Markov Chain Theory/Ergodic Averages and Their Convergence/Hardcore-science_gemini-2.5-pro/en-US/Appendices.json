{
    "hands_on_practices": [
        {
            "introduction": "The pointwise ergodic theorem guarantees the convergence of time averages to space averages, but it rests on the crucial assumption that the observable is integrable. This exercise provides a hands-on exploration of why this condition, $f \\in L^{1}(\\pi)$, is not just a technicality by constructing a stationary process where its failure leads to the dramatic divergence of the ergodic averages. By working through this example, you will gain a deeper appreciation for the foundational requirements of ergodic theory .",
            "id": "3305621",
            "problem": "Let $\\{X_{k}\\}_{k \\ge 1}$ be an independent and identically distributed (i.i.d.) sequence on $\\mathbb{N}$ with marginal distribution $\\pi$ given by\n$$\n\\pi(\\{m\\}) \\;=\\; \\frac{c}{m \\, (\\ln m)^{2}} \\quad \\text{for all } m \\in \\{3,4,5,\\dots\\},\n$$\nand $\\pi(\\{0\\})=\\pi(\\{1\\})=\\pi(\\{2\\})=0$, where the normalizing constant $c$ is defined by\n$$\nc^{-1} \\;=\\; \\sum_{m=3}^{\\infty} \\frac{1}{m \\, (\\ln m)^{2}} \\;\\; \\infty.\n$$\nDefine the observable $f:\\mathbb{N}\\to[0,\\infty)$ by $f(x)=x$, and consider the time averages\n$$\nA_{n} \\;=\\; \\frac{1}{n}\\sum_{k=1}^{n} f(X_{k}) \\, .\n$$\nAnswer the following, using only fundamental definitions and general principles of stationarity, independence, and summation/probability bounds (do not assume any convergence theorems that presuppose integrability of $f$):\n\n1. Verify that the process $\\{X_{k}\\}_{k\\ge 1}$ is strictly stationary with stationary distribution $\\pi$, and that $f \\notin L^{1}(\\pi)$.\n2. Prove that the events $\\{X_{n} \\geq n^{2}\\}$ occur infinitely often with probability one, and deduce the almost sure behavior of $\\limsup_{n\\to\\infty} A_{n}$.\n3. Let $\\mathcal{E}$ denote the event that the limit $\\lim_{n\\to\\infty}A_{n}$ exists in $\\mathbb{R}$ (that is, is finite). Compute the value of the probability $\\mathbb{P}(\\mathcal{E})$.\n\nIn your argument, clearly articulate the role of the integrability condition $f\\in L^{1}(\\pi)$ in ergodic theorems for stationary processes and why its failure here fundamentally alters the behavior of the time averages. Your final answer must be a single real number giving the value of $\\mathbb{P}(\\mathcal{E})$. No units are required, and no rounding is needed for the final answer.",
            "solution": "We begin by recalling the relevant foundational concepts. A stochastic process $\\{X_{k}\\}_{k\\ge 1}$ is strictly stationary if for every $n\\in\\mathbb{N}$ and every Borel sets $B_{1},\\dots,B_{n}$, the joint law of $(X_{1},\\dots,X_{n})$ equals the joint law of $(X_{1+\\ell},\\dots,X_{n+\\ell})$ for every shift $\\ell \\in \\mathbb{N}$. An observable $f$ is integrable with respect to the stationary distribution $\\pi$ if $\\int |f| \\,\\mathrm{d}\\pi  \\infty$. For independent and identically distributed (i.i.d.) sequences, stationarity is immediate because all finite-dimensional distributions are products of the same marginal. The time average is $A_{n} = \\frac{1}{n}\\sum_{k=1}^{n} f(X_{k})$.\n\nStep $1$: Stationarity and non-integrability. Because $\\{X_{k}\\}_{k\\ge 1}$ are i.i.d. with common marginal $\\pi$, the process is strictly stationary: for each $n\\in\\mathbb{N}$ and shift $\\ell \\in \\mathbb{N}$, the vectors $(X_{1},\\dots,X_{n})$ and $(X_{1+\\ell},\\dots,X_{n+\\ell})$ are i.i.d. with joint law $\\pi^{\\otimes n}$. Hence the stationary distribution is $\\pi$. Next, we verify that $f\\not\\in L^{1}(\\pi)$. Compute\n$$\n\\int |f| \\,\\mathrm{d}\\pi \\;=\\; \\sum_{m=3}^{\\infty} m \\,\\pi(\\{m\\}) \\;=\\; \\sum_{m=3}^{\\infty} m \\cdot \\frac{c}{m(\\ln m)^{2}} \\;=\\; c \\sum_{m=3}^{\\infty} \\frac{1}{(\\ln m)^{2}} \\, .\n$$\nTo determine the behavior of the series, compare with an integral. Consider the integral test with the monotone function $x \\mapsto (\\ln x)^{-2}$ on $[3,\\infty)$. Using the change of variables $u=\\ln x$ (so $\\mathrm{d}x = \\exp(u)\\,\\mathrm{d}u$),\n$$\n\\int_{3}^{N} \\frac{\\mathrm{d}x}{(\\ln x)^{2}} \\;=\\; \\int_{\\ln 3}^{\\ln N} \\frac{\\exp(u)}{u^{2}}\\,\\mathrm{d}u \\, .\n$$\nAs $N\\to\\infty$, the right-hand side diverges to $+\\infty$ because the integrand grows like $\\exp(u)/u^{2}$ and thus the antiderivative is asymptotic to $\\exp(u)/u^{2}$, which goes to infinity. Therefore,\n$$\n\\sum_{m=3}^{\\infty} \\frac{1}{(\\ln m)^{2}} \\;=\\; +\\infty \\, ,\n$$\nand we conclude $\\int |f| \\,\\mathrm{d}\\pi = +\\infty$, i.e., $f \\notin L^{1}(\\pi)$.\n\nStep $2$: Infinitely many large exceedances and the limsup of $A_{n}$. Define the events\n$$\nE_{n} \\;=\\; \\{ X_{n} \\,\\ge\\, n^{2} \\} \\, .\n$$\nBecause the $\\{X_{n}\\}$ are independent, the $\\{E_{n}\\}$ are independent as well. We bound\n$$\n\\mathbb{P}(E_{n}) \\;=\\; \\mathbb{P}(X_{n} \\ge n^{2}) \\;=\\; \\sum_{m=n^{2}}^{\\infty} \\pi(\\{m\\}) \\;=\\; c \\sum_{m=n^{2}}^{\\infty} \\frac{1}{m(\\ln m)^{2}} \\, .\n$$\nUsing a standard sum-integral comparison for a positive decreasing function $g(x)=\\frac{1}{x(\\ln x)^{2}}$ on $[3,\\infty)$,\n$$\n\\sum_{m=n^{2}}^{\\infty} \\frac{1}{m(\\ln m)^{2}} \\;\\ge\\; \\int_{n^{2}}^{\\infty} \\frac{\\mathrm{d}x}{x(\\ln x)^{2}} \\;=\\; \\left[ -\\frac{1}{\\ln x} \\right]_{x=n^{2}}^{x=\\infty} \\;=\\; \\frac{1}{\\ln(n^{2})} \\;=\\; \\frac{1}{2 \\ln n} \\, .\n$$\nHence, for all $n \\ge 3$,\n$$\n\\mathbb{P}(E_{n}) \\;\\ge\\; \\frac{c}{2 \\ln n} \\, .\n$$\nTherefore,\n$$\n\\sum_{n=3}^{\\infty} \\mathbb{P}(E_{n}) \\;\\ge\\; \\frac{c}{2} \\sum_{n=3}^{\\infty} \\frac{1}{\\ln n} \\;=\\; +\\infty \\, .\n$$\nBecause $\\{E_{n}\\}$ are independent and the sum of their probabilities diverges, the second Borelâ€“Cantelli lemma yields\n$$\n\\mathbb{P}(E_{n} \\text{ i.o.}) \\;=\\; 1 \\, ,\n$$\nthat is, with probability one, the event $E_{n}$ occurs infinitely often. On the event $E_{n}$, we have $X_{n} \\ge n^{2}$, and thus\n$$\n\\frac{1}{n}\\sum_{k=1}^{n} X_{k} \\;\\ge\\; \\frac{X_{n}}{n} \\;\\ge\\; n \\, .\n$$\nBecause $E_{n}$ occurs infinitely often almost surely, it follows that\n$$\n\\limsup_{n\\to\\infty} A_{n} \\;=\\; \\limsup_{n\\to\\infty} \\frac{1}{n}\\sum_{k=1}^{n} X_{k} \\;=\\; +\\infty \\quad \\text{almost surely.}\n$$\n\nStep $3$: The probability of a finite real limit for $A_{n}$. Let $\\mathcal{E} = \\left\\{ \\lim_{n\\to\\infty} A_{n} \\text{ exists in } \\mathbb{R} \\right\\}$. The almost sure statement $\\limsup_{n\\to\\infty} A_{n} = +\\infty$ implies that on a set of probability one, the sequence $\\{A_{n}\\}$ does not have a finite real limit. Therefore,\n$$\n\\mathbb{P}(\\mathcal{E}) \\;=\\; 0 \\, .\n$$\n\nExplanation of the role of integrability in ergodic theorems. In stationary settings, the pointwise ergodic theorem of George David Birkhoff guarantees almost sure convergence of time averages to the space average only under the integrability condition $f \\in L^{1}(\\pi)$. This hypothesis ensures that the contribution of rare, large values of $f(X_{k})$ is sufficiently controlled in aggregate to yield stabilization of $\\frac{1}{n}\\sum_{k=1}^{n} f(X_{k})$. In the present construction, $f \\notin L^{1}(\\pi)$ because $\\int |f|\\,\\mathrm{d}\\pi = +\\infty$, and indeed we have shown that extremely large values occur sufficiently often (infinitely often with probabilities bounded below by a divergent series) to force the time averages to blow up along a subsequence, making a finite real limit impossible. Thus, the failure of integrability explicitly manifests as the failure of convergence of ergodic averages, underscoring the necessity of the $L^{1}$ assumption in the classical ergodic theorem for stationary processes.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "After examining a case where ergodic averages fail to converge, we now turn to a model where they behave well: the first-order autoregressive, or AR(1), process. This practice bridges the gap between abstract theory and application by asking you to verify the conditions that guarantee a Central Limit Theorem for ergodic averages in this ubiquitous model. Successfully completing this problem will solidify your understanding of how strong mixing properties ensure the convergence of estimators from correlated data .",
            "id": "3305618",
            "problem": "Consider the first-order autoregressive process defined by $X_{k+1}=\\phi X_{k}+\\epsilon_{k+1}$, where $(\\epsilon_{k})_{k\\ge 1}$ are independent and identically distributed real-valued random variables with $\\mathbb{E}[\\epsilon_{1}]=0$ and $\\mathrm{Var}(\\epsilon_{1})=\\sigma_{\\epsilon}^{2}\\in(0,\\infty)$, and where $|\\phi|1$. Assume the initial state $X_{0}$ has the unique stationary distribution of the chain. Define the strong mixing (also called $\\alpha$-mixing) coefficients by $\\alpha(n)=\\sup_{k\\ge 0}\\sup_{A\\in\\mathcal{F}_{-\\infty}^{k},\\,B\\in\\mathcal{F}_{k+n}^{\\infty}}|\\mathbb{P}(A\\cap B)-\\mathbb{P}(A)\\mathbb{P}(B)|$, where $\\mathcal{F}_{a}^{b}$ denotes the $\\sigma$-algebra generated by $\\{X_{t}:a\\leq t\\leq b\\}$.\n\nYou may use as foundational starting points only the following:\n- Definitions of Markov chains, stationarity, strong mixing coefficients, total variation norm, and the Central Limit Theorem (CLT) for stationary strongly mixing sequences under summable mixing coefficients and finite second moments.\n- Standard facts about autoregressive processes of order $1$ with $|\\phi|1$, including existence and uniqueness of a stationary distribution, and that the stationary autocovariance function of a Gaussian autoregressive process of order $1$ has the form $\\gamma(h)=\\gamma(0)\\phi^{|h|}$.\n\nTasks:\n- First, starting from the Markov property and contractivity of the affine recursion, establish that the $\\alpha$-mixing coefficients $\\alpha(n)$ decay at an exponential rate in $n$.\n- Next, using only the exponential decay of $\\alpha$-mixing and boundedness of $f$, deduce a Central Limit Theorem for ergodic averages of the form $\\frac{1}{n}\\sum_{k=1}^{n}f(X_{k})$ with any bounded measurable $f$, and identify the form of the asymptotic variance as a covariance series.\n- Finally, specialize to the case $\\epsilon_{k}\\sim\\mathcal{N}(0,\\sigma_{\\epsilon}^{2})$ and $f(x)=x$, and compute the exact closed-form expression for the asymptotic variance in the CLT for $\\sqrt{n}\\big(\\frac{1}{n}\\sum_{k=1}^{n}X_{k}-\\mathbb{E}[X_{0}]\\big)$.\n\nYour final answer must be a single closed-form analytic expression for this asymptotic variance in terms of $\\phi$ and $\\sigma_{\\epsilon}^{2}$. No numerical approximation is required.",
            "solution": "The problem is analyzed in three parts as requested. First, we establish the exponential decay of the strong mixing coefficients. Second, we use this result to deduce a Central Limit Theorem (CLT) for ergodic averages. Third, we compute the explicit asymptotic variance for a specific case.\n\nPart 1: Exponential Decay of $\\alpha$-mixing Coefficients\n\nThe process is defined by the first-order autoregressive (AR($1$)) equation $X_{k+1}=\\phi X_{k}+\\epsilon_{k+1}$, where $|\\phi|1$. This is a time-homogeneous Markov process. The problem asks to establish the exponential decay of the $\\alpha$-mixing coefficients, $\\alpha(n)$, starting from the Markov property and the contractivity of the recursion.\n\nConsider two realizations of the process, $(X_k^x)$ and $(X_k^y)$, starting from two different initial states $X_0=x$ and $X_0=y$, but driven by the same sequence of innovations $(\\epsilon_k)_{k\\ge 1}$. By iterating the recursion, we can express the state at time $k$ as:\n$$X_k^x = \\phi^k x + \\sum_{j=1}^{k} \\phi^{k-j}\\epsilon_j$$\n$$X_k^y = \\phi^k y + \\sum_{j=1}^{k} \\phi^{k-j}\\epsilon_j$$\nThe difference between these two paths at time $k$ is:\n$$X_k^x - X_k^y = \\phi^k (x - y)$$\nTaking the absolute value, we observe the contractivity property:\n$$|X_k^x - X_k^y| = |\\phi|^k |x - y|$$\nSince $|\\phi|1$, the distance between the two paths contracts to zero at an exponential rate. This property is fundamental to proving geometric ergodicity.\n\nFor a stationary Markov process, the strong mixing coefficient $\\alpha(n)$ is defined as $\\alpha(n)=\\sup_{k\\ge 0}\\sup_{A\\in\\mathcal{F}_{-\\infty}^{k},\\,B\\in\\mathcal{F}_{k+n}^{\\infty}}|\\mathbb{P}(A\\cap B)-\\mathbb{P}(A)\\mathbb{P}(B)|$. Due to stationarity and the Markov property, the decay rate of $\\alpha(n)$ is governed by the rate at which the $n$-step transition probability distribution $P^n(x, \\cdot) = \\mathbb{P}(X_n \\in \\cdot | X_0=x)$ converges to the unique stationary distribution $\\pi(\\cdot)$.\n\nA standard theorem in the theory of Markov chains states that if a process is geometrically ergodic, meaning that the total variation distance between the $n$-step transition distribution and the stationary distribution decays exponentially, i.e., $\\sup_x \\|P^n(x, \\cdot) - \\pi(\\cdot) \\|_{TV} \\le M r^n$ for some $M0$ and $r \\in (0,1)$, then its $\\alpha$-mixing coefficients also decay exponentially. The contractivity shown above is a sufficient condition for geometric ergodicity for models of this type. This connection is formalized through coupling arguments. Therefore, we can conclude that there exist constants $C0$ and $\\rho \\in (0,1)$ such that for all $n \\ge 1$:\n$$\\alpha(n) \\le C \\rho^n$$\nThis establishes that the $\\alpha$-mixing coefficients decay at an exponential rate.\n\nPart 2: Central Limit Theorem for Ergodic Averages\n\nWe are asked to deduce a Central Limit Theorem for ergodic averages of the form $\\frac{1}{n}\\sum_{k=1}^{n}f(X_{k})$ for a bounded, measurable function $f$. Let $Y_k = f(X_k)$.\nThe foundational CLT for stationary, strongly-mixing sequences states that if $(Y_k)$ is a stationary sequence with mean $\\mu = \\mathbb{E}[Y_k]$, finite second moments $\\mathbb{E}[Y_k^2]  \\infty$, and strong mixing coefficients $\\alpha_Y(n)$ that are summable, i.e., $\\sum_{n=1}^\\infty \\alpha_Y(n)  \\infty$, then\n$$ \\sqrt{n}\\left(\\frac{1}{n}\\sum_{k=1}^{n}Y_k - \\mu\\right) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2) $$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution, and the asymptotic variance $\\sigma^2$ is given by\n$$ \\sigma^2 = \\sum_{h=-\\infty}^{\\infty} \\mathrm{Cov}(Y_0, Y_h) = \\mathrm{Var}(Y_0) + 2\\sum_{h=1}^{\\infty}\\mathrm{Cov}(Y_0, Y_h) $$\nWe must verify the conditions for the sequence $Y_k = f(X_k)$.\n1.  **Stationarity:** Since the process $(X_k)$ is assumed to be stationary, the transformed process $(Y_k = f(X_k))$ is also stationary.\n2.  **Finite Second Moment:** The function $f$ is bounded, so there exists a constant $M$ such that $|f(x)| \\le M$ for all $x$. Thus, $\\mathbb{E}[Y_k^2] = \\mathbb{E}[f(X_k)^2] \\le M^2  \\infty$.\n3.  **Summable Mixing Coefficients:** The mixing coefficients for the sequence $(Y_k)$, let's call them $\\alpha_Y(n)$, are bounded by the mixing coefficients of the original sequence $(X_k)$, i.e., $\\alpha_Y(n) \\le \\alpha_X(n)$. From Part 1, we established that $\\alpha_X(n) \\le C \\rho^n$ for $\\rho \\in (0,1)$. The geometric series $\\sum_{n=1}^\\infty C\\rho^n$ converges, so $\\sum_{n=1}^\\infty \\alpha_X(n)  \\infty$, which implies $\\sum_{n=1}^\\infty \\alpha_Y(n)  \\infty$.\n\nAll conditions are met. Thus, the CLT holds for $\\frac{1}{n}\\sum_{k=1}^{n}f(X_{k})$. The asymptotic variance, denoted $\\sigma_f^2$, takes the form of a covariance series:\n$$ \\sigma_f^2 = \\mathrm{Var}(f(X_0)) + 2\\sum_{k=1}^{\\infty}\\mathrm{Cov}(f(X_0), f(X_k)) $$\n\nPart 3: Computation of the Asymptotic Variance\n\nWe now specialize to the case where $\\epsilon_k \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ and the function is $f(x)=x$. The problem asks for the asymptotic variance in the CLT for $\\sqrt{n}\\big(\\frac{1}{n}\\sum_{k=1}^{n}X_{k}-\\mathbb{E}[X_{0}]\\big)$. The general formula for the asymptotic variance $\\sigma^2$ is:\n$$ \\sigma^2 = \\mathrm{Var}(X_0) + 2\\sum_{k=1}^{\\infty}\\mathrm{Cov}(X_0, X_k) $$\nLet $\\gamma(h) = \\mathrm{Cov}(X_0, X_h)$ be the autocovariance function. The formula is $\\sigma^2 = \\gamma(0) + 2\\sum_{k=1}^{\\infty}\\gamma(k)$.\n\nWe first find the mean and variance of the stationary process. Taking the expectation of the AR($1$) equation in stationarity yields $\\mathbb{E}[X_k] = \\phi \\mathbb{E}[X_{k-1}] + \\mathbb{E}[\\epsilon_k]$. With $\\mathbb{E}[\\epsilon_k]=0$ and $\\mathbb{E}[X_k]=\\mathbb{E}[X_{k-1}]=\\mu_X$, we get $\\mu_X = \\phi \\mu_X$, which implies $(1-\\phi)\\mu_X=0$. Since $|\\phi|1$, we must have $\\mu_X=0$. So, $\\mathbb{E}[X_k]=0$ for all $k$.\n\nNext, we find the variance, $\\gamma(0) = \\mathrm{Var}(X_k)$.\n$ \\mathrm{Var}(X_k) = \\mathrm{Var}(\\phi X_{k-1} + \\epsilon_k) $. Since $X_{k-1}$ is determined by innovations up to time $k-1$, it is independent of $\\epsilon_k$. Thus,\n$ \\mathrm{Var}(X_k) = \\phi^2 \\mathrm{Var}(X_{k-1}) + \\mathrm{Var}(\\epsilon_k) $. In stationarity, $\\mathrm{Var}(X_k)=\\mathrm{Var}(X_{k-1})=\\gamma(0)$ and $\\mathrm{Var}(\\epsilon_k)=\\sigma_\\epsilon^2$.\n$ \\gamma(0) = \\phi^2 \\gamma(0) + \\sigma_\\epsilon^2 \\implies \\gamma(0)(1-\\phi^2) = \\sigma_\\epsilon^2 \\implies \\gamma(0) = \\frac{\\sigma_\\epsilon^2}{1-\\phi^2} $.\n\nNow we find the autocovariance $\\gamma(k) = \\mathrm{Cov}(X_0, X_k) = \\mathbb{E}[X_0 X_k]$ for $k0$. We use the provided fact that for a stationary Gaussian AR($1$) process, $\\gamma(h) = \\gamma(0)\\phi^{|h|}$. So for $k \\ge 1$, $\\gamma(k) = \\gamma(0)\\phi^k$.\n\nWe substitute these into the formula for the asymptotic variance:\n$$ \\sigma^2 = \\gamma(0) + 2\\sum_{k=1}^{\\infty} \\gamma(0)\\phi^k = \\gamma(0) \\left(1 + 2\\sum_{k=1}^{\\infty}\\phi^k\\right) $$\nThe sum is a geometric series: $\\sum_{k=1}^{\\infty}\\phi^k = \\frac{\\phi}{1-\\phi}$, which converges because $|\\phi|1$.\nSubstituting this sum:\n$$ \\sigma^2 = \\gamma(0) \\left(1 + 2\\frac{\\phi}{1-\\phi}\\right) = \\gamma(0) \\left(\\frac{1-\\phi+2\\phi}{1-\\phi}\\right) = \\gamma(0) \\left(\\frac{1+\\phi}{1-\\phi}\\right) $$\nFinally, substitute the expression for $\\gamma(0)$:\n$$ \\sigma^2 = \\left(\\frac{\\sigma_\\epsilon^2}{1-\\phi^2}\\right) \\left(\\frac{1+\\phi}{1-\\phi}\\right) $$\nUsing the factorization $1-\\phi^2 = (1-\\phi)(1+\\phi)$:\n$$ \\sigma^2 = \\frac{\\sigma_\\epsilon^2}{(1-\\phi)(1+\\phi)} \\frac{1+\\phi}{1-\\phi} = \\frac{\\sigma_\\epsilon^2}{(1-\\phi)^2} $$\nThis is the closed-form expression for the asymptotic variance.",
            "answer": "$$\\boxed{\\frac{\\sigma_{\\epsilon}^{2}}{(1-\\phi)^{2}}}$$"
        },
        {
            "introduction": "Knowing that an ergodic average converges is the first step; quantifying the uncertainty in that estimate is the crucial next one for any practical application. This problem introduces the fundamental concept of the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, which measures the effective number of independent samples in a correlated sequence. By deriving the relationship between $\\tau_{\\mathrm{int}}$ and the asymptotic variance of an MCMC estimator, you will learn the primary tool for assessing the efficiency of stochastic simulations .",
            "id": "3305623",
            "problem": "Consider a time-homogeneous, irreducible, positive recurrent Markov chain $\\,\\{X_t\\}_{t \\ge 0}\\,$ on a measurable state space with unique invariant distribution $\\,\\pi\\,$. Assume the chain is initialized in stationarity, so that $\\,X_0 \\sim \\pi\\,$ and $\\,\\{X_t\\}\\,$ is strictly stationary. Let $\\,f:\\text{state space}\\to\\mathbb{R}\\,$ be integrable with finite variance under $\\,\\pi\\,$, denoted $\\,\\operatorname{Var}_{\\pi}(f) = \\sigma_f^2 \\in (0,\\infty)\\,$. Define the ergodic average\n$$\n\\hat{\\pi}_n(f) \\;=\\; \\frac{1}{n}\\sum_{t=1}^{n} f(X_t).\n$$\nLet the autocovariance function be $\\,\\gamma_k = \\operatorname{Cov}_{\\pi}(f(X_0), f(X_k))\\,$ and the autocorrelation function be $\\,\\rho_k = \\gamma_k/\\gamma_0\\,$, where $\\,\\gamma_0 = \\sigma_f^2\\,$. Assume $\\,\\sum_{k=1}^{\\infty} |\\rho_k|  \\infty\\,$.\n\n1) Starting from first principles and the definition of variance and covariance, express $\\,\\operatorname{Var}(\\hat{\\pi}_n(f))\\,$ exactly in terms of the finite-sample sum of autocovariances $\\,\\gamma_k\\,$. Then, under the given summability assumption, extract the leading $\\,1/n\\,$ term as $\\,n \\to \\infty\\,$ in terms of the infinite series in $\\,\\rho_k\\,$ and $\\,\\sigma_f^2\\,$.\n\n2) Introduce the integrated autocorrelation time $\\,\\tau_{\\mathrm{int}}\\,$ as a functional of the autocorrelation sequence, and rewrite your leading-order expression for $\\,\\operatorname{Var}(\\hat{\\pi}_n(f))\\,$ succinctly in terms of $\\,\\sigma_f^2\\,$, $\\,\\tau_{\\mathrm{int}}\\,$, and $\\,n\\,$.\n\n3) Now specialize to a first-order autoregressive correlation structure $\\,\\rho_k = \\alpha^k\\,$ for $\\,k \\ge 1\\,$ with $\\,|\\alpha|1\\,$. Let $\\,\\alpha = \\tfrac{1}{2}\\,$, $\\,\\sigma_f^2 = 2\\,$, and $\\,n = 10^{4}\\,$. Using your expression from part $\\,2)\\,$, compute the approximate value of $\\,\\operatorname{Var}(\\hat{\\pi}_n(f))\\,$. Round your final numerical answer to four significant figures.",
            "solution": "The problem is divided into three parts. We will address them sequentially.\n\nPart 1: Derivation of the exact and asymptotic variance of the ergodic average.\n\nThe ergodic average is defined as $\\hat{\\pi}_n(f) = \\frac{1}{n}\\sum_{t=1}^{n} f(X_t)$. We start from the definition of variance:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{t=1}^{n} f(X_t)\\right)\n$$\nUsing the property $\\operatorname{Var}(cY) = c^2\\operatorname{Var}(Y)$ for a constant $c$, we can factor out $\\frac{1}{n^2}$:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) = \\frac{1}{n^2} \\operatorname{Var}\\left(\\sum_{t=1}^{n} f(X_t)\\right)\n$$\nThe variance of a sum of random variables is the sum of all entries in their covariance matrix:\n$$\n\\operatorname{Var}\\left(\\sum_{t=1}^{n} f(X_t)\\right) = \\sum_{s=1}^{n} \\sum_{t=1}^{n} \\operatorname{Cov}(f(X_s), f(X_t))\n$$\nThe problem states that the Markov chain $\\{X_t\\}_{t \\ge 0}$ is strictly stationary. This implies that the covariance between $f(X_s)$ and $f(X_t)$ depends only on the time lag $|t-s|$. Let $\\gamma_k = \\operatorname{Cov}_{\\pi}(f(X_0), f(X_k))$ be the autocovariance at lag $k$. Then, by stationarity:\n$$\n\\operatorname{Cov}(f(X_s), f(X_t)) = \\operatorname{Cov}(f(X_0), f(X_{|t-s|})) = \\gamma_{|t-s|}\n$$\nSubstituting this into the double summation, we have:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) = \\frac{1}{n^2} \\sum_{s=1}^{n} \\sum_{t=1}^{n} \\gamma_{|t-s|}\n$$\nTo evaluate this sum, we re-index it by the lag $k = t-s$. The lag $k$ ranges from $-(n-1)$ to $n-1$. For a given lag $k$, there are $n-|k|$ pairs of indices $(s,t)$ in the sum (where $1 \\leq s,t \\leq n$) such that $t-s=k$. The sum can thus be written as:\n$$\n\\sum_{s=1}^{n} \\sum_{t=1}^{n} \\gamma_{|t-s|} = \\sum_{k=-(n-1)}^{n-1} (n-|k|) \\gamma_{|k|}\n$$\nThe autocovariance function is even, meaning $\\gamma_k = \\gamma_{-k}$, so $\\gamma_{|k|}$ is simply $\\gamma_k$ for any integer $k$. We can split the sum:\n$$\n\\sum_{k=-(n-1)}^{n-1} (n-|k|) \\gamma_k = (n-0)\\gamma_0 + \\sum_{k=1}^{n-1} (n-k)\\gamma_k + \\sum_{k=-(n-1)}^{-1} (n-|k|)\\gamma_k\n$$\nLetting $j = -k$ in the last term, it becomes $\\sum_{j=1}^{n-1} (n-j)\\gamma_{-j} = \\sum_{j=1}^{n-1} (n-j)\\gamma_j$. The total sum is therefore:\n$$\nn\\gamma_0 + 2\\sum_{k=1}^{n-1} (n-k)\\gamma_k\n$$\nSubstituting this back into the expression for the variance yields the exact expression:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) = \\frac{1}{n^2} \\left[ n\\gamma_0 + 2\\sum_{k=1}^{n-1} (n-k)\\gamma_k \\right] = \\frac{\\gamma_0}{n} + \\frac{2}{n}\\sum_{k=1}^{n-1}\\left(1 - \\frac{k}{n}\\right)\\gamma_k\n$$\nNow, we find the leading term as $n \\to \\infty$. Using the definitions $\\gamma_0 = \\sigma_f^2$ and $\\rho_k = \\gamma_k/\\gamma_0$, we rewrite the variance as:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) = \\frac{\\sigma_f^2}{n} \\left[ 1 + 2\\sum_{k=1}^{n-1}\\left(1 - \\frac{k}{n}\\right)\\rho_k \\right]\n$$\nFor large $n$, the term $(1 - k/n)$ approaches $1$ for any fixed $k$, and the upper limit of the sum goes to infinity. The problem states that $\\sum_{k=1}^{\\infty} |\\rho_k|  \\infty$, which means the series $\\sum \\rho_k$ converges absolutely. This allows us to interchange the limit and the sum (via the Dominated Convergence Theorem for series, since $|(1-k/n)\\rho_k| \\le |\\rho_k|$ and $\\sum|\\rho_k|$ is a convergent dominating series).\n$$\n\\lim_{n\\to\\infty} \\sum_{k=1}^{n-1}\\left(1 - \\frac{k}{n}\\right)\\rho_k = \\sum_{k=1}^{\\infty} \\rho_k\n$$\nTherefore, for large $n$, we have $n \\operatorname{Var}(\\hat{\\pi}_n(f)) \\to \\sigma_f^2 (1 + 2\\sum_{k=1}^{\\infty} \\rho_k)$. The leading order term in $1/n$ is:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) \\approx \\frac{\\sigma_f^2}{n} \\left( 1 + 2\\sum_{k=1}^{\\infty} \\rho_k \\right)\n$$\n\nPart 2: Introduction of the integrated autocorrelation time.\n\nThe integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, is defined as the sum of the autocorrelation function over all lags:\n$$\n\\tau_{\\mathrm{int}} = \\sum_{k=-\\infty}^{\\infty} \\rho_k\n$$\nBy definition, $\\rho_0 = \\gamma_0/\\gamma_0 = 1$. The autocorrelation function is also an even function, $\\rho_k = \\rho_{-k}$. Thus, we can write $\\tau_{\\mathrm{int}}$ as:\n$$\n\\tau_{\\mathrm{int}} = \\rho_0 + \\sum_{k=1}^{\\infty} \\rho_k + \\sum_{k=-\\infty}^{-1} \\rho_k = 1 + 2\\sum_{k=1}^{\\infty} \\rho_k\n$$\nThis is precisely the factor appearing in the asymptotic expression for the variance. The condition $\\sum_{k=1}^{\\infty} |\\rho_k|  \\infty$ ensures that $\\tau_{\\mathrm{int}}$ is finite. Substituting this definition into our result from Part 1, we obtain the succinct expression:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) \\approx \\frac{\\sigma_f^2 \\tau_{\\mathrm{int}}}{n}\n$$\n\nPart 3: Specialization to an AR(1) correlation structure and numerical calculation.\n\nWe are given a first-order autoregressive correlation structure, $\\rho_k = \\alpha^k$ for $k \\ge 1$, with $|\\alpha|  1$. We first compute $\\tau_{\\mathrm{int}}$ for this structure using its definition from Part 2:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2\\sum_{k=1}^{\\infty} \\rho_k = 1 + 2\\sum_{k=1}^{\\infty} \\alpha^k\n$$\nThe sum is a standard geometric series: $\\sum_{k=1}^{\\infty} \\alpha^k = \\frac{\\alpha}{1-\\alpha}$. Substituting this into the expression for $\\tau_{\\mathrm{int}}$ gives:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2\\left(\\frac{\\alpha}{1-\\alpha}\\right) = \\frac{1-\\alpha+2\\alpha}{1-\\alpha} = \\frac{1+\\alpha}{1-\\alpha}\n$$\nThe problem provides the values $\\alpha = \\frac{1}{2}$, $\\sigma_f^2 = 2$, and $n = 10^4$. We first calculate $\\tau_{\\mathrm{int}}$ with $\\alpha = \\frac{1}{2}$:\n$$\n\\tau_{\\mathrm{int}} = \\frac{1+\\frac{1}{2}}{1-\\frac{1}{2}} = \\frac{\\frac{3}{2}}{\\frac{1}{2}} = 3\n$$\nNow, we use the approximate formula for the variance with the given numerical values:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) \\approx \\frac{\\sigma_f^2 \\tau_{\\mathrm{int}}}{n} = \\frac{2 \\times 3}{10^4} = \\frac{6}{10000} = 0.0006\n$$\nThe problem requires the final answer to be rounded to four significant figures. We express $0.0006$ in scientific notation to explicitly show the significant figures: $6.000 \\times 10^{-4}$.",
            "answer": "$$ \\boxed{6.000 \\times 10^{-4}} $$"
        }
    ]
}