{
    "hands_on_practices": [
        {
            "introduction": "Understanding ergodicity begins with mastering the fundamental structure of a Markov chain's state space. This exercise provides a concrete platform to dissect a finite-state chain that is not irreducible, a common scenario in many applications. By identifying the communicating classes, transient states, and stationary distributions of the closed classes, you will apply the ergodic decomposition theorem to predict the chain's long-term behavior based on its starting position.",
            "id": "3305963",
            "problem": "Consider a time-homogeneous Markov chain with discrete time and finite state space $\\mathcal{X} = \\{1,2,3,4,5,6,7\\}$. The transition probability matrix $P$ is given by the following rows (each row sums to $1$):\n$$\nP(1,\\cdot) = \\left(\\tfrac{1}{2}, \\tfrac{1}{2}, 0, 0, 0, 0, 0\\right),\\quad\nP(2,\\cdot) = \\left(\\tfrac{1}{3}, \\tfrac{2}{3}, 0, 0, 0, 0, 0\\right),\n$$\n$$\nP(3,\\cdot) = \\left(0, 0, \\tfrac{1}{2}, \\tfrac{1}{4}, \\tfrac{1}{4}, 0, 0\\right),\\quad\nP(4,\\cdot) = \\left(0, 0, \\tfrac{1}{3}, \\tfrac{1}{3}, \\tfrac{1}{3}, 0, 0\\right),\n$$\n$$\nP(5,\\cdot) = \\left(0, 0, \\tfrac{1}{4}, \\tfrac{1}{2}, \\tfrac{1}{4}, 0, 0\\right),\\quad\nP(6,\\cdot) = \\left(\\tfrac{1}{4}, \\tfrac{1}{4}, \\tfrac{1}{2}, 0, 0, 0, 0\\right),\n$$\n$$\nP(7,\\cdot) = \\left(0, 0, 0, \\tfrac{3}{5}, \\tfrac{1}{5}, 0, \\tfrac{1}{5}\\right).\n$$\nYou may assume standard definitions: a Markov chain is a sequence of random variables $(X_{n})_{n \\geq 0}$ with the Markov property; two states $i,j \\in \\mathcal{X}$ communicate if $i$ can reach $j$ and $j$ can reach $i$ with positive probability in some finite number of steps; a communicating class is closed if the chain cannot leave it once entered; a chain is irreducible if it has a single communicating class; a stationary distribution $\\pi$ on a class satisfies $\\pi = \\pi P$ with $\\sum_{x} \\pi(x) = 1$; and the ergodic theorem for Markov chains ensures that for a positive recurrent, aperiodic, irreducible class, time averages converge almost surely to expectations under its stationary distribution.\n\nDefine the function $g:\\mathcal{X}\\to\\mathbb{R}$ by $g(1)=0$, $g(2)=2$, $g(3)=3$, $g(4)=4$, $g(5)=5$, $g(6)=6$, $g(7)=7$. The initial distribution is concentrated on states $6$ and $7$: $\\mathbb{P}(X_{0}=6)=\\tfrac{2}{3}$ and $\\mathbb{P}(X_{0}=7)=\\tfrac{1}{3}$.\n\nTasks:\n- Using only the core definitions stated above, determine the communicating classes of the chain and identify which are closed. Conclude whether the chain is irreducible or reducible.\n- For each closed class, justify positive recurrence and aperiodicity directly from the given $P$, and derive its stationary distribution from the defining equations $\\pi=\\pi P$ and $\\sum \\pi = 1$.\n- Leveraging these structural facts and foundational results, determine the almost-sure limit of the empirical time average\n$$\n\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{k=0}^{n-1} g\\!\\left(X_{k}\\right).\n$$\nExpress your final answer as a single exact real number. No rounding is required.",
            "solution": "The user wants me to solve a problem on a discrete-time Markov chain.\n\n### Step 1: Extract Givens\n- **State Space**: $\\mathcal{X} = \\{1, 2, 3, 4, 5, 6, 7\\}$.\n- **Transition Probabilities** $P(i, j)$:\n  - $P(1, \\cdot) = (\\frac{1}{2}, \\frac{1}{2}, 0, 0, 0, 0, 0)$\n  - $P(2, \\cdot) = (\\frac{1}{3}, \\frac{2}{3}, 0, 0, 0, 0, 0)$\n  - $P(3, \\cdot) = (0, 0, \\frac{1}{2}, \\frac{1}{4}, \\frac{1}{4}, 0, 0)$\n  - $P(4, \\cdot) = (0, 0, \\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}, 0, 0)$\n  - $P(5, \\cdot) = (0, 0, \\frac{1}{4}, \\frac{1}{2}, \\frac{1}{4}, 0, 0)$\n  - $P(6, \\cdot) = (\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}, 0, 0, 0, 0)$\n  - $P(7, \\cdot) = (0, 0, 0, \\frac{3}{5}, \\frac{1}{5}, 0, \\frac{1}{5})$\n- **Function** $g:\\mathcal{X} \\to \\mathbb{R}$:\n  - $g(1)=0$, $g(2)=2$, $g(3)=3$, $g(4)=4$, $g(5)=5$, $g(6)=6$, $g(7)=7$.\n- **Initial Distribution**:\n  - $\\mathbb{P}(X_0=6) = \\frac{2}{3}$\n  - $\\mathbb{P}(X_0=7) = \\frac{1}{3}$\n- **Definitions**:\n  - Time-homogeneous Markov chain, discrete time, finite state space.\n  - Communicating states.\n  - Communicating class.\n  - Closed class.\n  - Irreducible vs. reducible chain.\n  - Stationary distribution $\\pi = \\pi P$.\n  - Ergodic theorem for Markov chains.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It presents a standard exercise in the theory of Markov chains. The transition probabilities for each row sum to $1$. The state space is finite. All terms are well-defined within the context of stochastic processes. The tasks are specific and solvable using the provided information. The problem is self-contained, consistent, and well-posed. The requirement for a single real number as the final answer despite the reducible nature of the chain implies the expected value of the random limit is sought, which is a standard interpretation in this context.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full solution.\n\n### Solution\n\nThe solution proceeds in three parts as requested: determining the structure of the chain, analyzing its closed classes, and computing the required limit.\n\n**1. Communicating Classes and Reducibility**\nWe determine the communicating classes by analyzing the state transitions. Two states $i$ and $j$ communicate, written $i \\leftrightarrow j$, if there exist integers $n, m \\ge 0$ such that $P^n(i, j) > 0$ and $P^m(j, i) > 0$.\n\n- **States $1$ and $2$**: $P(1, 2) = \\frac{1}{2} > 0$ and $P(2, 1) = \\frac{1}{3} > 0$. Thus, $1 \\leftrightarrow 2$. This forms a communicating class $\\mathcal{C}_1 = \\{1, 2\\}$. For any $i \\in \\mathcal{C}_1$ and $j \\notin \\mathcal{C}_1$, $P(i, j) = 0$. Therefore, $\\mathcal{C}_1$ is a closed class.\n\n- **States $3, 4, 5$**:\n  - $P(3, 4) = \\frac{1}{4} > 0$ and $P(4, 3) = \\frac{1}{3} > 0$, so $3 \\leftrightarrow 4$.\n  - $P(3, 5) = \\frac{1}{4} > 0$ and $P(5, 3) = \\frac{1}{4} > 0$, so $3 \\leftrightarrow 5$.\n  - By transitivity, states $\\{3, 4, 5\\}$ form a single communicating class, $\\mathcal{C}_2 = \\{3, 4, 5\\}$. For any $i \\in \\mathcal{C}_2$ and $j \\notin \\mathcal{C}_2$, $P(i, j) = 0$. Therefore, $\\mathcal{C}_2$ is a closed class.\n\n- **State $6$**: State $6$ can transition to state $1 \\in \\mathcal{C}_1$ since $P(6, 1) = \\frac{1}{4} > 0$, and to state $3 \\in \\mathcal{C}_2$ since $P(6, 3) = \\frac{1}{2} > 0$. Once the chain enters a closed class, it cannot leave. Thus, neither $\\mathcal{C}_1$ nor $\\mathcal{C}_2$ can transition back to state $6$. State $6$ is therefore a transient state.\n\n- **State $7$**: State $7$ can transition to state $4 \\in \\mathcal{C}_2$ since $P(7, 4) = \\frac{3}{5} > 0$. Since $\\mathcal{C}_2$ is a closed class, the chain cannot return to state $7$ after entering $\\mathcal{C}_2$. State $7$ is therefore a transient state.\n\nThe state space is partitioned into two closed communicating classes, $\\mathcal{C}_1 = \\{1, 2\\}$ and $\\mathcal{C}_2 = \\{3, 4, 5\\}$, and a set of transient states $\\mathcal{T} = \\{6, 7\\}$. Since the chain possesses more than one communicating class, it is **reducible**.\n\n**2. Analysis of Closed Classes**\n\nFor each closed class, we analyze its properties and find its unique stationary distribution.\n\n**Analysis of $\\mathcal{C}_1 = \\{1, 2\\}$:**\nThe sub-stochastic matrix for this class is $P_1 = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}$.\n- **Positive Recurrence**: A Markov chain on a finite state space is positive recurrent if and only if it is irreducible. The sub-chain on $\\mathcal{C}_1$ is irreducible and has a finite number of states, so it is positive recurrent.\n- **Aperiodicity**: The period of a state $i$ is the greatest common divisor of all $n \\ge 1$ for which $P^n(i, i) > 0$. Since $P(1, 1) = \\frac{1}{2} > 0$, the period of state $1$ is $d(1) = \\text{gcd}(1, \\dots) = 1$. Since all states in a communicating class share the same period, the class $\\mathcal{C}_1$ is aperiodic.\n- **Stationary Distribution**: Let $\\pi_1 = (\\pi_1(1), \\pi_1(2))$ be the stationary distribution. It must satisfy $\\pi_1 = \\pi_1 P_1$ and $\\pi_1(1) + \\pi_1(2) = 1$.\n$$ (\\pi_1(1), \\pi_1(2)) = (\\pi_1(1), \\pi_1(2)) \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{3} & \\frac{2}{3} \\end{pmatrix} $$\nThis gives the equation $\\pi_1(1) = \\frac{1}{2}\\pi_1(1) + \\frac{1}{3}\\pi_1(2)$, which simplifies to $\\frac{1}{2}\\pi_1(1) = \\frac{1}{3}\\pi_1(2)$, or $3\\pi_1(1) = 2\\pi_1(2)$.\nUsing $\\pi_1(2) = 1 - \\pi_1(1)$, we have $3\\pi_1(1) = 2(1 - \\pi_1(1)) = 2 - 2\\pi_1(1)$.\nThis yields $5\\pi_1(1) = 2$, so $\\pi_1(1) = \\frac{2}{5}$. Consequently, $\\pi_1(2) = 1 - \\frac{2}{5} = \\frac{3}{5}$.\nThe stationary distribution for $\\mathcal{C}_1$ is $\\pi_1 = (\\frac{2}{5}, \\frac{3}{5})$.\n\n**Analysis of $\\mathcal{C}_2 = \\{3, 4, 5\\}$:**\nThe sub-stochastic matrix is $P_2 = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{4} & \\frac{1}{4} \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{4} & \\frac{1}{2} & \\frac{1}{4} \\end{pmatrix}$.\n- **Positive Recurrence**: Same as for $\\mathcal{C}_1$, this is a finite, irreducible sub-chain, hence positive recurrent.\n- **Aperiodicity**: Since $P(3, 3) = \\frac{1}{2} > 0$, the period of state $3$ is $1$, and the class $\\mathcal{C}_2$ is aperiodic.\n- **Stationary Distribution**: Let $\\pi_2 = (\\pi_2(3), \\pi_2(4), \\pi_2(5))$ be the stationary distribution. We solve $\\pi_2 = \\pi_2 P_2$ subject to $\\sum_{i=3}^5 \\pi_2(i) = 1$.\n\\begin{align*} \\pi_2(3) &= \\frac{1}{2}\\pi_2(3) + \\frac{1}{3}\\pi_2(4) + \\frac{1}{4}\\pi_2(5) \\\\ \\pi_2(4) &= \\frac{1}{4}\\pi_2(3) + \\frac{1}{3}\\pi_2(4) + \\frac{1}{2}\\pi_2(5) \\\\ \\pi_2(5) &= \\frac{1}{4}\\pi_2(3) + \\frac{1}{3}\\pi_2(4) + \\frac{1}{4}\\pi_2(5) \\end{align*}\nSimplifying these equations gives:\n\\begin{align*} \\frac{1}{2}\\pi_2(3) &= \\frac{1}{3}\\pi_2(4) + \\frac{1}{4}\\pi_2(5) \\implies 6\\pi_2(3) = 4\\pi_2(4) + 3\\pi_2(5) \\\\ \\frac{2}{3}\\pi_2(4) &= \\frac{1}{4}\\pi_2(3) + \\frac{1}{2}\\pi_2(5) \\implies 8\\pi_2(4) = 3\\pi_2(3) + 6\\pi_2(5) \\end{align*}\nUsing the normalization condition $\\pi_2(5) = 1 - \\pi_2(3) - \\pi_2(4)$, we substitute into the simplified equations:\n$6\\pi_2(3) = 4\\pi_2(4) + 3(1 - \\pi_2(3) - \\pi_2(4)) \\implies 9\\pi_2(3) = \\pi_2(4) + 3 \\implies \\pi_2(4) = 9\\pi_2(3) - 3$.\n$8\\pi_2(4) = 3\\pi_2(3) + 6(1 - \\pi_2(3) - \\pi_2(4)) \\implies 14\\pi_2(4) = 6 - 3\\pi_2(3)$.\nSubstituting the expression for $\\pi_2(4)$ into the second equation:\n$14(9\\pi_2(3) - 3) = 6 - 3\\pi_2(3) \\implies 126\\pi_2(3) - 42 = 6 - 3\\pi_2(3) \\implies 129\\pi_2(3) = 48$.\n$\\pi_2(3) = \\frac{48}{129} = \\frac{16}{43}$.\nThen $\\pi_2(4) = 9(\\frac{16}{43}) - 3 = \\frac{144 - 129}{43} = \\frac{15}{43}$.\nAnd $\\pi_2(5) = 1 - \\frac{16}{43} - \\frac{15}{43} = \\frac{43 - 31}{43} = \\frac{12}{43}$.\nThe stationary distribution for $\\mathcal{C}_2$ is $\\pi_2 = (\\frac{16}{43}, \\frac{15}{43}, \\frac{12}{43})$.\n\n**3. Long-term Time Average Limit**\nThe limit $L = \\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{k=0}^{n-1} g(X_k)$ depends on which closed class the chain is absorbed into. The contribution from the finite time spent in transient states vanishes in the limit. By the ergodic theorem for Markov chains, if the chain is absorbed into class $\\mathcal{C}_i$, the limit will be $\\mathbb{E}_{\\pi_i}[g(X)] = \\sum_{j \\in \\mathcal{C}_i} g(j)\\pi_i(j)$.\nThe problem asks for a single numerical answer, which implies we should calculate the expected value of this random limit, $\\mathbb{E}[L]$, given the initial distribution of $X_0$.\n$$ \\mathbb{E}[L] = \\mathbb{P}(\\text{absorption in } \\mathcal{C}_1) \\cdot \\mathbb{E}_{\\pi_1}[g] + \\mathbb{P}(\\text{absorption in } \\mathcal{C}_2) \\cdot \\mathbb{E}_{\\pi_2}[g] $$\nLet $h_i = \\mathbb{P}(\\text{absorption in } \\mathcal{C}_1 | X_0 = i)$. For $i \\in \\mathcal{C}_1$, $h_i=1$. For $i \\in \\mathcal{C}_2$, $h_i=0$. For transient states $i \\in \\mathcal{T}$, $h_i = \\sum_{j \\in \\mathcal{X}} P(i,j)h_j$.\n$h_6 = P(6,1)h_1 + P(6,2)h_2 + P(6,3)h_3 = \\frac{1}{4}(1) + \\frac{1}{4}(1) + \\frac{1}{2}(0) = \\frac{1}{2}$.\n$h_7 = P(7,4)h_4 + P(7,5)h_5 + P(7,7)h_7 = \\frac{3}{5}(0) + \\frac{1}{5}(0) + \\frac{1}{5}h_7 \\implies \\frac{4}{5}h_7 = 0 \\implies h_7 = 0$.\n\nThe total probability of absorption in $\\mathcal{C}_1$ is:\n$\\mathbb{P}(\\text{absorption in } \\mathcal{C}_1) = \\mathbb{P}(X_0=6)h_6 + \\mathbb{P}(X_0=7)h_7 = \\frac{2}{3} \\cdot \\frac{1}{2} + \\frac{1}{3} \\cdot 0 = \\frac{1}{3}$.\nThe probability of absorption in $\\mathcal{C}_2$ is $1 - \\frac{1}{3} = \\frac{2}{3}$.\n\nNow we compute the expected values of $g$ under each stationary distribution:\n$\\mathbb{E}_{\\pi_1}[g] = g(1)\\pi_1(1) + g(2)\\pi_1(2) = 0 \\cdot \\frac{2}{5} + 2 \\cdot \\frac{3}{5} = \\frac{6}{5}$.\n$\\mathbb{E}_{\\pi_2}[g] = g(3)\\pi_2(3) + g(4)\\pi_2(4) + g(5)\\pi_2(5) = 3 \\cdot \\frac{16}{43} + 4 \\cdot \\frac{15}{43} + 5 \\cdot \\frac{12}{43} = \\frac{48 + 60 + 60}{43} = \\frac{168}{43}$.\n\nFinally, we compute the expected value of the limit:\n$\\mathbb{E}[L] = \\left(\\frac{1}{3}\\right) \\cdot \\mathbb{E}_{\\pi_1}[g] + \\left(\\frac{2}{3}\\right) \\cdot \\mathbb{E}_{\\pi_2}[g]$\n$\\mathbb{E}[L] = \\left(\\frac{1}{3}\\right) \\cdot \\left(\\frac{6}{5}\\right) + \\left(\\frac{2}{3}\\right) \\cdot \\left(\\frac{168}{43}\\right)$\n$\\mathbb{E}[L] = \\frac{2}{5} + \\frac{2 \\cdot 168}{3 \\cdot 43} = \\frac{2}{5} + \\frac{336}{129}$\nSince $129 = 3 \\cdot 43$ and $336 = 3 \\cdot 112$, the second term simplifies:\n$\\mathbb{E}[L] = \\frac{2}{5} + \\frac{112}{43}$\nTo sum these fractions, we find a common denominator, which is $5 \\cdot 43 = 215$.\n$\\mathbb{E}[L] = \\frac{2 \\cdot 43}{5 \\cdot 43} + \\frac{112 \\cdot 5}{43 \\cdot 5} = \\frac{86}{215} + \\frac{560}{215} = \\frac{86 + 560}{215} = \\frac{646}{215}$.\nThis fraction is irreducible as $646 = 2 \\cdot 17 \\cdot 19$ and $215 = 5 \\cdot 43$.\nThe final result is the expected value of the almost-sure limit.",
            "answer": "$$\\boxed{\\frac{646}{215}}$$"
        },
        {
            "introduction": "Moving from finite to continuous state spaces, this practice explores the properties of an autoregressive process, a cornerstone of time-series analysis. Here, you will connect the abstract property of geometric ergodicity to a powerful analytical device: the Poisson equation. Solving this equation for a specific observable not only deepens the understanding of convergence rates but also reveals a practical application in constructing optimal control variates to reduce variance in Monte Carlo simulations.",
            "id": "3305949",
            "problem": "Consider a time-homogeneous Markov chain on $\\mathbb{R}$ defined by the autoregressive of order $1$ (AR(1)) update $X_{n+1}=\\rho\\,X_n+\\xi_{n+1}$ where $\\{\\xi_n\\}_{n\\geq 1}$ is an independent and identically distributed sequence with $\\xi_n\\sim\\mathcal{N}(0,\\tau^2)$, $\\rho\\in(-1,1)$, and $\\tau>0$. Let $\\pi$ denote the unique stationary distribution, and let $\\Pi$ denote the associated Markov kernel acting on bounded measurable functions $g$ by $\\Pi g(x)=\\mathbb{E}[g(\\rho x+\\xi)]$ where $\\xi\\sim\\mathcal{N}(0,\\tau^2)$ is independent of $x$. For the observable $f(x)=x^2$, consider the Poisson equation for the chain,\n$$\ng-\\Pi g=f-\\pi(f),\n$$\nwhere $\\pi(f)=\\int_{\\mathbb{R}} f(x)\\,\\pi(\\mathrm{d}x)$ denotes expectation under the stationary distribution.\n\nStarting from fundamental definitions of stationarity, Markov kernels, and conditional expectation, and without invoking any prepackaged formulas beyond those definitions, do the following:\n\n1. Derive $\\pi(f)$ by enforcing the distributional fixed-point property of stationarity and basic moment relations for affine transformations of a random variable.\n2. Solve the Poisson equation above for $g$ within the class of polynomial functions of degree at most two, and determine the canonical solution by imposing the normalization $\\pi(g)=0$.\n3. Using the solution’s regularity, define the control variate $h(x)=g(x)-\\Pi g(x)$ and analyze the adjusted estimator $\\widehat{\\mu}_n(\\beta)=\\frac{1}{n}\\sum_{k=1}^{n} f(X_k)-\\beta\\,\\frac{1}{n}\\sum_{k=1}^{n} h(X_k)$, identifying the value of $\\beta$ that yields zero variance in principle. Explain how the existence of the solution $g$ and its square integrability under $\\pi$ relies on the geometric ergodicity induced by $|\\rho|<1$, and why solvability fails when $|\\rho|\\geq 1$.\n\nYour final answer must be the explicit closed-form analytical expression for the canonical solution $g(x)$ found in step $2$. No rounding is required. Express your final answer as a single closed-form expression.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and contains sufficient information for a unique solution. We proceed with the derivation.\n\nThe problem asks for a three-part solution concerning a time-homogeneous Markov chain defined by the AR(1) process $X_{n+1}=\\rho\\,X_n+\\xi_{n+1}$ on the state space $\\mathbb{R}$. The parameters are constrained such that $\\rho\\in(-1,1)$ and $\\tau>0$, where $\\{\\xi_n\\}$ is an i.i.d. sequence of random variables with distribution $\\mathcal{N}(0,\\tau^2)$.\n\n**1. Derivation of the stationary expectation $\\pi(f)$**\n\nThe stationary distribution $\\pi$ is a probability distribution such that if $X_n \\sim \\pi$, then $X_{n+1} = \\rho X_n + \\xi_{n+1}$ also has the distribution $\\pi$. This is the distributional fixed-point property. Let $X \\sim \\pi$. We can find the first two moments of $\\pi$ by applying this property.\n\nThe mean of $X$ under $\\pi$ is $\\mu_\\pi = \\mathbb{E}_\\pi[X]$. Taking the expectation of the AR(1) equation, we have:\n$$\n\\mathbb{E}_\\pi[X_{n+1}] = \\mathbb{E}_\\pi[\\rho X_n + \\xi_{n+1}]\n$$\nSince $X_n$ and $X_{n+1}$ both follow the distribution $\\pi$, their expectations are equal, $\\mathbb{E}_\\pi[X_{n+1}] = \\mathbb{E}_\\pi[X_n] = \\mu_\\pi$. The noise term $\\xi_{n+1}$ is independent of $X_n$ and has mean $\\mathbb{E}[\\xi_{n+1}]=0$.\n$$\n\\mu_\\pi = \\rho\\,\\mathbb{E}_\\pi[X_n] + \\mathbb{E}[\\xi_{n+1}] = \\rho\\,\\mu_\\pi + 0\n$$\nThis gives the equation $\\mu_\\pi(1-\\rho) = 0$. Since $\\rho \\in (-1,1)$, we have $1-\\rho \\neq 0$, which implies $\\mu_\\pi = 0$.\n\nThe variance of $X$ under $\\pi$ is $\\sigma_\\pi^2 = \\text{Var}_\\pi(X) = \\mathbb{E}_\\pi[X^2] - (\\mathbb{E}_\\pi[X])^2$. Since $\\mu_\\pi=0$, the variance is simply the second moment: $\\sigma_\\pi^2 = \\mathbb{E}_\\pi[X^2]$. We compute the variance of $X_{n+1}$:\n$$\n\\text{Var}_\\pi(X_{n+1}) = \\text{Var}_\\pi(\\rho X_n + \\xi_{n+1})\n$$\nSince $X_n$ is independent of $\\xi_{n+1}$, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}_\\pi(X_{n+1}) = \\text{Var}_\\pi(\\rho X_n) + \\text{Var}(\\xi_{n+1}) = \\rho^2 \\text{Var}_\\pi(X_n) + \\tau^2\n$$\nBy stationarity, $\\text{Var}_\\pi(X_{n+1}) = \\text{Var}_\\pi(X_n) = \\sigma_\\pi^2$.\n$$\n\\sigma_\\pi^2 = \\rho^2 \\sigma_\\pi^2 + \\tau^2\n$$\nRearranging gives $\\sigma_\\pi^2(1-\\rho^2) = \\tau^2$. Since $|\\rho|<1$, $1-\\rho^2 > 0$, and we can solve for the variance:\n$$\n\\sigma_\\pi^2 = \\frac{\\tau^2}{1-\\rho^2}\n$$\nThe observable is given by $f(x)=x^2$. The quantity $\\pi(f)$ is the expectation of $f(X)$ under the stationary distribution $\\pi$:\n$$\n\\pi(f) = \\mathbb{E}_\\pi[f(X)] = \\mathbb{E}_\\pi[X^2]\n$$\nAs established, this is precisely the variance of the stationary distribution, since its mean is zero.\n$$\n\\pi(f) = \\sigma_\\pi^2 = \\frac{\\tau^2}{1-\\rho^2}\n$$\nIt is a known property that for an AR(1) process with Gaussian innovations, the unique stationary distribution is also Gaussian. Thus, $\\pi \\sim \\mathcal{N}(0, \\frac{\\tau^2}{1-\\rho^2})$.\n\n**2. Solution of the Poisson Equation**\n\nWe must solve the Poisson equation $g-\\Pi g=f-\\pi(f)$ for a function $g$ that is a polynomial of degree at most two. Let's write $g(x) = ax^2 + bx + c$, where $a, b, c$ are constants to be determined.\n\nFirst, we compute the action of the Markov kernel $\\Pi$ on $g$. The kernel is defined as $\\Pi g(x)=\\mathbb{E}[g(\\rho x+\\xi)]$ where $\\xi\\sim\\mathcal{N}(0,\\tau^2)$.\n$$\n\\Pi g(x) = \\mathbb{E}[a(\\rho x+\\xi)^2 + b(\\rho x+\\xi) + c]\n$$\nUsing the linearity of expectation and treating $x$ as a constant with respect to the expectation over $\\xi$:\n$$\n\\Pi g(x) = a\\,\\mathbb{E}[(\\rho x+\\xi)^2] + b\\,\\mathbb{E}[\\rho x+\\xi] + c\n$$\nWe evaluate the expectations involving $\\xi$. We know $\\mathbb{E}[\\xi]=0$ and $\\mathbb{E}[\\xi^2]=\\text{Var}(\\xi)=\\tau^2$.\n$$\n\\mathbb{E}[\\rho x+\\xi] = \\rho x + \\mathbb{E}[\\xi] = \\rho x\n$$\n$$\n\\mathbb{E}[(\\rho x+\\xi)^2] = \\mathbb{E}[\\rho^2 x^2 + 2\\rho x \\xi + \\xi^2] = \\rho^2 x^2 + 2\\rho x \\mathbb{E}[\\xi] + \\mathbb{E}[\\xi^2] = \\rho^2 x^2 + \\tau^2\n$$\nSubstituting these back into the expression for $\\Pi g(x)$:\n$$\n\\Pi g(x) = a(\\rho^2 x^2 + \\tau^2) + b(\\rho x) + c = a\\rho^2 x^2 + b\\rho x + (a\\tau^2 + c)\n$$\nNow we compute the left-hand side of the Poisson equation, $g(x) - \\Pi g(x)$:\n$$\ng(x) - \\Pi g(x) = (ax^2 + bx + c) - (a\\rho^2 x^2 + b\\rho x + a\\tau^2 + c)\n$$\n$$\ng(x) - \\Pi g(x) = (a-a\\rho^2)x^2 + (b-b\\rho)x - a\\tau^2 = a(1-\\rho^2)x^2 + b(1-\\rho)x - a\\tau^2\n$$\nThe right-hand side of the Poisson equation is $f(x) - \\pi(f) = x^2 - \\frac{\\tau^2}{1-\\rho^2}$.\nEquating the two polynomials, we match the coefficients of the powers of $x$:\n$$\na(1-\\rho^2)x^2 + b(1-\\rho)x - a\\tau^2 = 1 \\cdot x^2 + 0 \\cdot x - \\frac{\\tau^2}{1-\\rho^2}\n$$\n- Coefficient of $x^2$: $a(1-\\rho^2) = 1$. Since $|\\rho|<1$, $1-\\rho^2 \\neq 0$, so $a = \\frac{1}{1-\\rho^2}$.\n- Coefficient of $x$: $b(1-\\rho) = 0$. Since $|\\rho|<1$, $1-\\rho \\neq 0$, so $b = 0$.\n- Constant term: $-a\\tau^2 = -\\frac{\\tau^2}{1-\\rho^2}$. Substituting our value for $a$: $-\\frac{1}{1-\\rho^2} \\tau^2 = -\\frac{\\tau^2}{1-\\rho^2}$. This is consistent, so the polynomial form is valid.\n\nThe solution has the form $g(x) = \\frac{1}{1-\\rho^2}x^2 + c$. To find the canonical solution, we enforce the normalization condition $\\pi(g)=0$.\n$$\n\\pi(g) = \\mathbb{E}_\\pi[g(X)] = \\mathbb{E}_\\pi\\left[\\frac{1}{1-\\rho^2}X^2 + c\\right] = \\frac{1}{1-\\rho^2}\\mathbb{E}_\\pi[X^2] + c\n$$\nWe previously found $\\mathbb{E}_\\pi[X^2] = \\pi(f) = \\frac{\\tau^2}{1-\\rho^2}$.\n$$\n\\pi(g) = \\frac{1}{1-\\rho^2}\\left(\\frac{\\tau^2}{1-\\rho^2}\\right) + c = \\frac{\\tau^2}{(1-\\rho^2)^2} + c\n$$\nSetting $\\pi(g)=0$ yields $c = -\\frac{\\tau^2}{(1-\\rho^2)^2}$.\nTherefore, the canonical solution is:\n$$\ng(x) = \\frac{x^2}{1-\\rho^2} - \\frac{\\tau^2}{(1-\\rho^2)^2}\n$$\n\n**3. Control Variate Analysis and the Role of $|\\rho|<1$**\n\nThe control variate is defined as $h(x) = g(x) - \\Pi g(x)$. By the Poisson equation, this is exactly $h(x) = f(x) - \\pi(f)$. The adjusted estimator for $\\pi(f)$ is $\\widehat{\\mu}_n(\\beta)=\\frac{1}{n}\\sum_{k=1}^{n} f(X_k)-\\beta\\,\\frac{1}{n}\\sum_{k=1}^{n} h(X_k)$. Substituting the expression for $h(X_k)$:\n$$\n\\widehat{\\mu}_n(\\beta)=\\frac{1}{n}\\sum_{k=1}^{n} f(X_k)-\\beta\\,\\frac{1}{n}\\sum_{k=1}^{n} (f(X_k) - \\pi(f))\n$$\n$$\n\\widehat{\\mu}_n(\\beta) = (1-\\beta)\\left(\\frac{1}{n}\\sum_{k=1}^{n} f(X_k)\\right) + \\beta\\,\\pi(f)\n$$\nThe error of this estimator is $\\widehat{\\mu}_n(\\beta) - \\pi(f) = (1-\\beta)\\left(\\frac{1}{n}\\sum_{k=1}^{n} f(X_k) - \\pi(f)\\right)$. The variance of the error is $(1-\\beta)^2 \\text{Var}\\left(\\frac{1}{n}\\sum_{k=1}^{n} f(X_k)\\right)$. To achieve zero variance, we must choose $\\beta=1$. With $\\beta=1$, the estimator becomes $\\widehat{\\mu}_n(1) = \\pi(f)$, which is the exact value. This demonstrates that the solution to the Poisson equation provides a \"perfect\" control variate, which reduces the variance to zero in principle. This is theoretical because using it requires knowing $\\pi(f)$, the very quantity we wish to estimate.\n\nThe condition $|\\rho|<1$ is what ensures the Markov chain is geometrically ergodic. This property guarantees that for any centered observable $\\phi$ (i.e., $\\pi(\\phi)=0$) that is square-integrable under $\\pi$ (i.e., $\\phi \\in L^2(\\pi)$), the Poisson equation $\\ g-\\Pi g = \\phi$ has a unique solution $g \\in L^2(\\pi)$ (up to the normalization constant). In our case, $\\phi = f - \\pi(f) = x^2 - \\frac{\\tau^2}{1-\\rho^2}$. Since the stationary distribution $\\pi$ is Gaussian, all its moments are finite, which implies that $\\phi \\in L^2(\\pi)$ and our explicit solution $g(x)$ is also in $L^2(\\pi)$.\n\nIf $|\\rho| \\geq 1$, the chain is no longer geometrically ergodic (it is transient for $|\\rho|>1$ and null recurrent for $|\\rho|=1$ as long as $\\tau>0$). No invariant probability distribution $\\pi$ exists. Consequently, $\\pi(f)$ is undefined. Furthermore, looking at the algebraic derivation, the coefficient $a$ of the $x^2$ term in $g(x)$ is $a=\\frac{1}{1-\\rho^2}$. If $|\\rho|=1$, this expression is undefined, indicating that no polynomial solution of degree $2$ exists. More fundamentally, the operator $(I-\\Pi)$ is not invertible on the relevant function space, meaning the Poisson equation is ill-posed and generally has no solution for a given $\\phi$. The convergence of the formal series solution $g = \\sum_{k=0}^\\infty \\Pi^k \\phi$ depends on $\\Pi$ being a contraction, a property which is tied to the condition $|\\rho|<1$.",
            "answer": "$$\n\\boxed{\\frac{x^2}{1-\\rho^2} - \\frac{\\tau^2}{(1-\\rho^2)^2}}\n$$"
        },
        {
            "introduction": "Theoretical ergodicity guarantees convergence in the infinite-time limit, but what happens in a finite-time simulation? This practice confronts the critical challenge of metastability, where a chain becomes trapped in one region of the state space, exhibiting 'apparent non-ergodicity'. By analyzing a Metropolis-Adjusted Langevin Algorithm (MALA) sampler on a classic double-well potential, you will use tools from statistical physics to quantify the exponentially long waiting times between modes, providing a tangible explanation for why some MCMC simulations fail to converge on practical timescales.",
            "id": "3305937",
            "problem": "Consider a one-dimensional target distribution with density $\\pi(x) \\propto \\exp(-\\beta V(x))$ where the potential $V:\\mathbb{R}\\to\\mathbb{R}$ is the double-well\n$$\nV(x) = \\frac{(x^{2}-a^{2})^{2}}{4},\n$$\nwith fixed $a>0$ and inverse temperature $\\beta>0$. Let $\\{X_{n}\\}_{n\\geq 0}$ be the Metropolis Adjusted Langevin Algorithm (MALA) Markov chain with step size $h>0$ targeting $\\pi$, defined by the proposal\n$$\nY = X_{n} - h\\,\\nabla V(X_{n}) + \\sqrt{\\frac{2h}{\\beta}}\\,\\xi_{n}, \\quad \\xi_{n}\\sim \\mathcal{N}(0,1),\n$$\nand accepted with the standard Metropolis–Hastings acceptance probability that ensures $\\pi$ is invariant. Assume the regime of small step size and low noise in which $h\\to 0$ and $\\beta\\to\\infty$, with $h$ sufficiently small that acceptance is close to one in the wells and the chain is well-approximated by the overdamped Langevin diffusion\n$$\n\\mathrm{d}X_{t} = -\\nabla V(X_{t})\\,\\mathrm{d}t + \\sqrt{\\frac{2}{\\beta}}\\,\\mathrm{d}W_{t},\n$$\nwith the identification that one MALA iteration corresponds to an Euler–Maruyama time increment $\\Delta t \\approx h$ for the diffusion.\n\nLet $\\tau_{h}$ be the first hitting time (in number of MALA iterations) to the left well neighborhood from the right well, defined by starting at $X_{0}$ concentrated near $x=+a$ and\n$$\n\\tau_{h} = \\inf\\{n\\geq 1: X_{n}\\in (-\\infty,-a+\\delta)\\}\n$$\nfor some fixed small $\\delta\\in(0,a)$, with the understanding that barrier crossing occurs via the saddle at $x=0$. Using potential-theoretic large deviation ideas (Eyring–Kramers arguments) appropriate to the overdamped diffusion, derive the leading-order asymptotic expression, in closed form, for the mean tunneling time $\\mathbb{E}[\\tau_{h}]$ as $\\beta\\to\\infty$ and $h\\to 0$, expressed in terms of $a$, $\\beta$, and $h$. Then briefly explain how this asymptotic leads to metastability and apparent non-ergodicity in finite-time Markov Chain Monte Carlo (MCMC) sampling. Provide your final answer for $\\mathbb{E}[\\tau_{h}]$ as a single analytic expression. No rounding is required.",
            "solution": "The problem statement is a valid application of stochastic process theory to analyze the performance of a Markov Chain Monte Carlo (MCMC) algorithm. All provided information is self-contained, scientifically grounded in the fields of statistical mechanics and stochastic calculus, and mathematically well-posed. We proceed with the solution.\n\nThe problem asks for the leading-order asymptotic expression for the mean number of iterations, $\\mathbb{E}[\\tau_h]$, for a Metropolis Adjusted Langevin Algorithm (MALA) chain to transition between the two wells of a double-well potential. The analysis is to be performed in the low-noise ($\\beta \\to \\infty$) and small step-size ($h \\to 0$) regime, where the MALA dynamics are well-approximated by an overdamped Langevin diffusion.\n\nThe target distribution has a probability density $\\pi(x) \\propto \\exp(-\\beta V(x))$, where the potential is given by\n$$\nV(x) = \\frac{(x^{2}-a^{2})^{2}}{4}.\n$$\nThe corresponding overdamped Langevin diffusion is\n$$\n\\mathrm{d}X_{t} = -\\nabla V(X_{t})\\,\\mathrm{d}t + \\sqrt{\\frac{2}{\\beta}}\\,\\mathrm{d}W_{t},\n$$\nwhere $W_t$ is a standard Wiener process. The problem states that a single MALA iteration with step size $h$ corresponds to a time increment of $\\Delta t = h$ for this diffusion.\n\nThe mean time for such a process to escape from a potential well can be calculated using the Eyring-Kramers formula. First, we must identify the minima (stable states) and the saddle point (transition state) of the potential $V(x)$. The critical points are found by setting the first derivative of the potential to zero.\nThe gradient (in one dimension, the derivative) of the potential is\n$$\n\\nabla V(x) = V'(x) = \\frac{d}{dx}\\left[\\frac{(x^{2}-a^{2})^{2}}{4}\\right] = \\frac{1}{4} \\cdot 2(x^{2}-a^{2}) \\cdot (2x) = x(x^{2}-a^{2}).\n$$\nSetting $V'(x)=0$ yields the critical points at $x=0$, $x=a$, and $x=-a$.\n\nTo classify these points, we compute the second derivative:\n$$\nV''(x) = \\frac{d}{dx}[x^{3}-a^{2}x] = 3x^{2}-a^{2}.\n$$\nWe evaluate $V''(x)$ at each critical point:\n- At $x = \\pm a$: $V''(\\pm a) = 3(\\pm a)^{2} - a^{2} = 2a^{2}$. Since $a>0$, $V''(\\pm a) > 0$, indicating that $x=\\pm a$ are local minima, corresponding to the bottoms of the two wells.\n- At $x=0$: $V''(0) = 3(0)^{2} - a^{2} = -a^{2}$. Since $V''(0) < 0$, $x=0$ is a local maximum, which acts as the saddle point for transitions between the two wells.\n\nThe transition from the right well (around $x=a$) to the left well (around $x=-a$) requires the system to overcome the potential barrier at $x=0$. The height of this barrier, $\\Delta V$, is the difference between the potential at the saddle point and the potential at the minimum:\n$$\nV(a) = \\frac{(a^{2}-a^{2})^{2}}{4} = 0.\n$$\n$$\nV(0) = \\frac{(0^{2}-a^{2})^{2}}{4} = \\frac{(-a^{2})^{2}}{4} = \\frac{a^{4}}{4}.\n$$\n$$\n\\Delta V = V(0) - V(a) = \\frac{a^{4}}{4} - 0 = \\frac{a^{4}}{4}.\n$$\n\nThe Eyring-Kramers formula for the mean first passage time, $\\mathbb{E}[\\tau_{cont}]$, for a one-dimensional system described by the given Langevin equation to escape from a minimum at $x_{min}$ over a saddle at $x_{sad}$ is:\n$$\n\\mathbb{E}[\\tau_{cont}] \\approx \\frac{2\\pi}{\\sqrt{V''(x_{min})|V''(x_{sad})|}} \\exp(\\beta \\Delta V).\n$$\nIn our case, the starting minimum is $x_{min}=a$ and the saddle is $x_{sad}=0$. Substituting our previously calculated values:\n- $V''(x_{min}) = V''(a) = 2a^{2}$\n- $|V''(x_{sad})| = |V''(0)| = |-a^{2}| = a^{2}$\n- $\\Delta V = a^{4}/4$\n\nThe mean transition time in the continuous-time diffusion model is thus:\n$$\n\\mathbb{E}[\\tau_{cont}] \\approx \\frac{2\\pi}{\\sqrt{(2a^{2})(a^{2})}} \\exp\\left(\\beta \\frac{a^{4}}{4}\\right) = \\frac{2\\pi}{\\sqrt{2a^{4}}} \\exp\\left(\\frac{\\beta a^{4}}{4}\\right) = \\frac{2\\pi}{a^{2}\\sqrt{2}} \\exp\\left(\\frac{\\beta a^{4}}{4}\\right).\n$$\nSimplifying the pre-factor gives:\n$$\n\\mathbb{E}[\\tau_{cont}] \\approx \\frac{\\pi\\sqrt{2}}{a^{2}} \\exp\\left(\\frac{\\beta a^{4}}{4}\\right).\n$$\nThis expression represents the mean time in the units of the continuous Langevin process. The problem asks for the mean number of MALA iterations, $\\mathbb{E}[\\tau_{h}]$. We are given that one MALA iteration corresponds to a time step $\\Delta t = h$. Therefore, to find the number of iterations, we divide the total continuous time by the time per iteration:\n$$\n\\mathbb{E}[\\tau_{h}] = \\frac{\\mathbb{E}[\\tau_{cont}]}{\\Delta t} = \\frac{\\mathbb{E}[\\tau_{cont}]}{h}.\n$$\nSubstituting the expression for $\\mathbb{E}[\\tau_{cont}]$, we obtain the final asymptotic expression for the mean tunneling time in MALA iterations:\n$$\n\\mathbb{E}[\\tau_{h}] \\approx \\frac{\\pi\\sqrt{2}}{h a^{2}} \\exp\\left(\\frac{\\beta a^{4}}{4}\\right).\n$$\n\nThis asymptotic result explains the phenomena of metastability and apparent non-ergodicity in MCMC simulations.\n\nMetastability: The mean transition time $\\mathbb{E}[\\tau_{h}]$ grows exponentially with the term $\\beta a^{4}$. In the low-noise regime ($\\beta \\gg 1$), this time becomes exceedingly long. A state is called metastable if the system, once in that state, remains there for a very long time before transitioning to a more stable state or another state of similar stability. Here, the neighborhoods of the potential wells at $x=\\pm a$ are the metastable states.\n\nApparent Non-Ergodicity: An MCMC sampler is ergodic if, given enough time, it explores the entire state space according to the target distribution $\\pi(x)$. For this problem, $\\pi(x)$ is bimodal, with symmetric peaks at $x=\\pm a$. A truly ergodic sampler should visit both wells and distribute its samples between them according to the equal weights of the modes. However, the number of iterations required to see a single transition between wells, $\\mathbb{E}[\\tau_{h}]$, can easily exceed the total number of iterations in any practical MCMC run. For a simulation of length $N$ where $N \\ll \\mathbb{E}[\\tau_{h}]$, a chain initialized in one well (e.g., near $x=a$) will be trapped there for the entire simulation. It will fail to sample the other mode of the distribution near $x=-a$. The empirical average computed from such a run will not converge to the true expectation under $\\pi(x)$. This gives the appearance of non-ergodicity, as the chain fails to explore its state space globally on a practical timescale. The chain is theoretically ergodic, but its mixing time is exponentially large, rendering it ineffective for finite-time sampling of a multimodal distribution.",
            "answer": "$$\n\\boxed{\\frac{\\pi\\sqrt{2}}{h a^{2}} \\exp\\left(\\frac{\\beta a^{4}}{4}\\right)}\n$$"
        }
    ]
}