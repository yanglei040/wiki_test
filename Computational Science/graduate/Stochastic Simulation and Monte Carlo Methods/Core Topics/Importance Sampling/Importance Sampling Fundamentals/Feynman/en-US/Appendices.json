{
    "hands_on_practices": [
        {
            "introduction": "A primary application of importance sampling is the efficient estimation of rare-event probabilities, where naive simulation would require an astronomical number of trials. This practice guides you through the entire workflow, from deriving an optimal proposal distribution using the principle of exponential tilting to implementing it in code. By comparing your importance sampling estimator to a crude Monte Carlo approach , you will gain a tangible understanding of the dramatic variance reduction that a well-designed proposal can achieve.",
            "id": "3312679",
            "problem": "Consider independent and identically distributed random variables $X_1, X_2, \\dots, X_n$ each having a standard normal distribution. Define the rare-event probability\n$$\np(n,a) \\equiv \\mathbb{P}\\left\\{\\frac{1}{n}\\sum_{i=1}^n X_i \\ge a\\right\\} = \\mathbb{P}\\left\\{\\sum_{i=1}^n X_i \\ge n a\\right\\}.\n$$\nStarting from fundamental definitions of expectation, probability, and the change-of-measure principle via the Radon–Nikodým derivative, derive and implement an estimator for $p(n,a)$ that uses Importance Sampling (IS). The design must be guided only by the following well-tested facts:\n- The moment generating function of a standard normal random variable exists for every real argument $t$, and its cumulant generating function $\\psi(t)$ is finite for all real $t$.\n- The sum of independent normal random variables is normal, with the mean and variance given by the sums of the individual means and variances, respectively.\n- The Monte Carlo method estimates expectations by empirical averages of independent samples.\n\nYou must:\n- Construct a direct Monte Carlo estimator (sometimes called a “crude” estimator) for $p(n,a)$.\n- Construct an Importance Sampling estimator for $p(n,a)$ by an exponential tilting change of measure that makes the rare event typical under the sampling distribution. Your derivation must start from the Radon–Nikodým derivative definition and the cumulant generating function, and must justify the choice of tilting parameter using a first-principles optimality argument grounded in the convexity properties of the cumulant generating function.\n- Implement both estimators efficiently by sampling the sum $\\sum_{i=1}^n X_i$ directly, rather than the $n$-dimensional vector. Explicitly use the fact that if $Y_i$ are independent normal random variables with mean $\\mu$ and variance $1$, then $\\sum_{i=1}^n Y_i$ is normal with mean $n\\mu$ and variance $n$.\n\nNo physical units are involved. All angles, if any appear in your reasoning, must be in radians, but none are required here. The final outputs are floats.\n\nTest Suite:\n- Case $1$: $n=100$, $a=0.5$, Monte Carlo sample size $M=200000$, random seed $s=12345$.\n- Case $2$: $n=100$, $a=0.0$, Monte Carlo sample size $M=100000$, random seed $s=54321$.\n- Case $3$: $n=50$, $a=0.3$, Monte Carlo sample size $M=50000$, random seed $s=2025$.\n- Case $4$: $n=200$, $a=1.0$, Monte Carlo sample size $M=300000$, random seed $s=777$.\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output first the crude Monte Carlo estimate and then the Importance Sampling estimate, in order. That is, the single line must look like\n$$\n[\\widehat{p}_{\\text{crude},1},\\widehat{p}_{\\text{IS},1},\\widehat{p}_{\\text{crude},2},\\widehat{p}_{\\text{IS},2},\\widehat{p}_{\\text{crude},3},\\widehat{p}_{\\text{IS},3},\\widehat{p}_{\\text{crude},4},\\widehat{p}_{\\text{IS},4}],\n$$\nwhere each $\\widehat{p}$ is a float.",
            "solution": "The problem is subjected to validation and is determined to be a valid, well-posed problem in the domain of computational statistics and Monte Carlo methods. It is scientifically grounded, self-contained, and objective. We may therefore proceed with a full derivation and solution.\n\nThe quantity to be estimated is the rare-event probability\n$$\np(n,a) = \\mathbb{P}\\left\\{\\frac{1}{n}\\sum_{i=1}^n X_i \\ge a\\right\\}\n$$\nwhere $X_1, X_2, \\dots, X_n$ are independent and identically distributed (i.i.d.) standard normal random variables, $X_i \\sim \\mathcal{N}(0, 1)$. Let $S_n = \\sum_{i=1}^n X_i$. From the properties of the normal distribution, the sum of independent normal random variables is also normal. The mean of the sum is the sum of the means, $\\mathbb{E}[S_n] = \\sum_{i=1}^n \\mathbb{E}[X_i] = \\sum_{i=1}^n 0 = 0$. The variance of the sum is the sum of the variances, $\\text{Var}(S_n) = \\sum_{i=1}^n \\text{Var}(X_i) = \\sum_{i=1}^n 1 = n$. Thus, $S_n \\sim \\mathcal{N}(0, n)$.\n\nThe probability can be rewritten in terms of $S_n$ as $p(n,a) = \\mathbb{P}\\{S_n \\ge na\\}$. This probability can be expressed as an expectation of an indicator function, which forms the basis for Monte Carlo estimation:\n$$\np(n,a) = \\mathbb{E}\\left[\\mathbb{I}\\{S_n \\ge na\\}\\right]\n$$\nwhere $\\mathbb{I}\\{\\cdot\\}$ is the indicator function, which equals $1$ if the condition is true and $0$ otherwise. The expectation is taken with respect to the probability distribution of $S_n$, which has the probability density function (PDF) $f_{S_n}(s) = \\frac{1}{\\sqrt{2\\pi n}} \\exp\\left(-\\frac{s^2}{2n}\\right)$.\n\n**Crude Monte Carlo Estimator**\n\nThe crude Monte Carlo method directly applies the law of large numbers to estimate the expectation. We generate $M$ independent samples of the random variable $S_n$, denoted $S_n^{(1)}, S_n^{(2)}, \\dots, S_n^{(M)}$, from its distribution $\\mathcal{N}(0, n)$. The estimator for $p(n,a)$ is then the sample mean of the indicator function evaluated on these samples:\n$$\n\\widehat{p}_{\\text{crude}}(n,a) = \\frac{1}{M}\\sum_{j=1}^M \\mathbb{I}\\left\\{S_n^{(j)} \\ge na\\right\\}\n$$\nThis estimator is unbiased, i.e., $\\mathbb{E}[\\widehat{p}_{\\text{crude}}(n,a)] = p(n,a)$. However, if the event $\\{S_n \\ge na\\}$ is rare, most samples $S_n^{(j)}$ will result in $\\mathbb{I}\\{S_n^{(j)} \\ge na\\} = 0$, leading to a high variance and inefficient estimation.\n\n**Importance Sampling (IS) Estimator**\n\nThe core idea of Importance Sampling is to change the probability measure to one where the rare event becomes more frequent. We introduce a new sampling distribution with PDF $g(s)$ and rewrite the expectation:\n$$\np(n,a) = \\int_{-\\infty}^\\infty \\mathbb{I}\\{s \\ge na\\} f_{S_n}(s) ds = \\int_{-\\infty}^\\infty \\mathbb{I}\\{s \\ge na\\} \\frac{f_{S_n}(s)}{g(s)} g(s) ds = \\mathbb{E}_g\\left[\\mathbb{I}\\{Y \\ge na\\} L(Y)\\right]\n$$\nHere, $Y$ is a random variable with PDF $g(s)$, and $L(s) = \\frac{f_{S_n}(s)}{g(s)}$ is the likelihood ratio, which is the Radon–Nikodým derivative of the original measure with respect to the new measure.\n\nThe IS estimator is then formed by drawing $M$ samples $Y^{(1)}, \\dots, Y^{(M)}$ from the new distribution $g(s)$ and computing the sample mean:\n$$\n\\widehat{p}_{\\text{IS}}(n,a) = \\frac{1}{M}\\sum_{j=1}^M \\mathbb{I}\\left\\{Y^{(j)} \\ge na\\right\\} L(Y^{(j)})\n$$\nThis estimator is also unbiased for any $g(s)$ that has support covering the integration domain where the integrand is non-zero.\n\n**Exponential Tilting and Optimal Parameter Selection**\n\nA powerful choice for the new distribution $g(s)$ is an exponential tilting of the original distribution $f_{S_n}(s)$. This is defined using a tilting parameter $\\theta$:\n$$\ng_\\theta(s) = f_{S_n}(s) e^{\\theta s - \\Psi_{S_n}(\\theta)}\n$$\nwhere $\\Psi_{S_n}(\\theta) = \\ln \\mathbb{E}[e^{\\theta S_n}]$ is the cumulant generating function (CGF) of $S_n$. The term $e^{-\\Psi_{S_n}(\\theta)}$ is the normalization constant. As $X_i$ are i.i.d., $\\Psi_{S_n}(\\theta) = n \\psi(\\theta)$, where $\\psi(\\theta)$ is the CGF of a single $X_i \\sim \\mathcal{N}(0, 1)$. For a standard normal variable, the moment generating function is $M_X(t) = e^{t^2/2}$, so the CGF is $\\psi(t) = \\ln(M_X(t)) = t^2/2$. Consequently, the CGF for $S_n$ is $\\Psi_{S_n}(\\theta) = n\\theta^2/2$.\n\nSubstituting the PDFs and the CGF into the definition of $g_\\theta(s)$:\n$$\ng_\\theta(s) = \\left[\\frac{1}{\\sqrt{2\\pi n}} e^{-s^2/(2n)}\\right] e^{\\theta s - n\\theta^2/2} = \\frac{1}{\\sqrt{2\\pi n}} \\exp\\left(-\\frac{s^2 - 2n\\theta s + n^2\\theta^2}{2n}\\right) = \\frac{1}{\\sqrt{2\\pi n}} \\exp\\left(-\\frac{(s - n\\theta)^2}{2n}\\right)\n$$\nThis is the PDF of a normal distribution with mean $n\\theta$ and variance $n$, i.e., $Y \\sim \\mathcal{N}(n\\theta, n)$. The likelihood ratio is $L(s) = \\frac{f_{S_n}(s)}{g_\\theta(s)} = e^{-\\theta s + \\Psi_{S_n}(\\theta)} = e^{-\\theta s + n\\theta^2/2}$.\n\nThe optimal choice of $\\theta$ is one that minimizes the variance of the IS estimator. An effective heuristic, which is asymptotically optimal in the context of large deviations, is to choose $\\theta$ such that the mean of the new distribution is centered at the \"most important\" region of the event. For the one-sided event $\\{S_n \\ge na\\}$, this corresponds to centering the new distribution at the boundary of the event region.\n\nWe set the mean of the sampling distribution equal to the threshold $na$:\n$$\n\\mathbb{E}_\\theta[Y] = na\n$$\nA fundamental property of exponential families is that the mean of the tilted distribution is the derivative of the CGF: $\\mathbb{E}_\\theta[Y] = \\Psi'_{S_n}(\\theta)$. Thus, we solve for $\\theta$ in the equation:\n$$\n\\Psi'_{S_n}(\\theta) = na\n$$\nGiven $\\Psi_{S_n}(\\theta) = n\\theta^2/2$, its derivative is $\\Psi'_{S_n}(\\theta) = n\\theta$. The equation becomes:\n$$\nn\\theta = na \\implies \\theta = a\n$$\nThe CGF $\\Psi_{S_n}(\\theta)$ is strictly convex because its second derivative $\\Psi''_{S_n}(\\theta) = n > 0$. This strict convexity guarantees that the equation $\\Psi'_{S_n}(\\theta) = na$ has a unique solution for $\\theta$.\n\nWith the optimal parameter $\\theta = a$, our IS procedure is as follows:\n1.  Sample $Y^{(j)}$ from the distribution $\\mathcal{N}(na, n)$.\n2.  Calculate the likelihood ratio for each sample: $L(Y^{(j)}) = \\exp\\left(-a Y^{(j)} + na^2/2\\right)$.\n3.  Compute the estimate:\n    $$\n    \\widehat{p}_{\\text{IS}}(n,a) = \\frac{1}{M}\\sum_{j=1}^M \\mathbb{I}\\left\\{Y^{(j)} \\ge na\\right\\} e^{-a Y^{(j)} + na^2/2}\n    $$\nBy centering the sampling distribution at the event boundary, we ensure that a substantial fraction of samples (approximately $50\\%$) will fall into the event region $\\{Y \\ge na\\}$, which drastically reduces the variance of the estimator compared to the crude method, especially when $p(n,a)$ is very small.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements crude Monte Carlo and Importance Sampling estimators\n    for a rare-event probability involving a sum of standard normal variables.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, a, M, random_seed)\n        (100, 0.5, 200000, 12345),\n        (100, 0.0, 100000, 54321),\n        (50, 0.3, 50000, 2025),\n        (200, 1.0, 300000, 777),\n    ]\n\n    results = []\n    for n, a, M, s in test_cases:\n        # Initialize a random number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(s)\n\n        # The target event is sum(X_i) = n*a.\n        threshold = n * a\n\n        # The sum S_n = sum(X_i) is distributed as Normal(0, n).\n        mean_original = 0.0\n        # The standard deviation is sqrt(n).\n        std_dev = np.sqrt(n)\n\n        # --- Crude Monte Carlo Estimator ---\n        # Generate M samples from the original distribution N(0, n).\n        samples_crude = rng.normal(loc=mean_original, scale=std_dev, size=M)\n        \n        # The estimate is the fraction of samples that fall into the event region.\n        p_crude = np.mean(samples_crude = threshold)\n        results.append(p_crude)\n\n        # --- Importance Sampling (IS) Estimator ---\n        # The optimal tilting parameter is theta = a.\n        theta = a\n\n        # The mean of the IS sampling distribution N(n*theta, n).\n        mean_is = n * theta\n        # The standard deviation is the same, sqrt(n).\n        \n        # Generate M samples from the tilted (IS) distribution.\n        samples_is = rng.normal(loc=mean_is, scale=std_dev, size=M)\n        \n        # Calculate the likelihood ratio for each sample.\n        # L(s) = exp(-theta*s + n*psi(theta)), where psi(theta) = theta^2/2 for N(0,1).\n        # CGF of the sum is n*psi(theta).\n        log_likelihood_ratios = -theta * samples_is + n * theta**2 / 2.0\n        likelihood_ratios = np.exp(log_likelihood_ratios)\n\n        # The IS estimate is the average of the indicator function times the likelihood ratio.\n        # The indicator function is 1 if sample = threshold, and 0 otherwise.\n        indicators = (samples_is = threshold)\n        p_is = np.mean(indicators * likelihood_ratios)\n        results.append(p_is)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While variance reduction is the goal of importance sampling, how do we formally quantify the efficiency of our sampler? This exercise introduces the concept of the Effective Sample Size (ESS), a crucial diagnostic that measures how much \"information\" we are getting from our samples relative to drawing from the target directly. You will work through an analytical optimization to find the proposal distribution that maximizes the ESS, demonstrating a principled approach to proposal design  that goes beyond simple heuristics.",
            "id": "3312707",
            "problem": "Consider a target probability density $\\,\\pi(x)\\,$ on $\\,\\mathbb{R}^{d}\\,$ given by a multivariate normal distribution with mean $\\,\\mu \\in \\mathbb{R}^{d}\\,$ and positive-definite covariance matrix $\\,\\Sigma \\in \\mathbb{R}^{d \\times d}\\,$, i.e., $\\,\\pi(x) = \\mathcal{N}(x;\\mu,\\Sigma)\\,$. For Importance Sampling (IS), draw $\\,n\\,$ independent samples $\\,X_{1},\\dots,X_{n}\\,$ from a proposal family $\\,q_{\\theta}(x)\\,$ that is also multivariate normal with the same covariance but a tunable mean parameter $\\,\\theta \\in \\mathbb{R}^{d}\\,$, i.e., $\\,q_{\\theta}(x) = \\mathcal{N}(x;\\theta,\\Sigma)\\,$. Define the IS weights $\\,w(x) = \\pi(x)/q_{\\theta}(x)\\,$ and the Effective Sample Size (ESS) for these $\\,n\\,$ samples by\n$$\n\\mathrm{ESS} \\;=\\; \\frac{\\left(\\sum_{i=1}^{n} w(X_{i})\\right)^{2}}{\\sum_{i=1}^{n} w(X_{i})^{2}}.\n$$\nStarting from first principles—namely, the definitions of Importance Sampling, properties of the multivariate normal distribution, and the Law of Large Numbers—derive an analytically tractable approximation to $\\,\\mathbb{E}[\\mathrm{ESS}]\\,$ that only depends on the first and second moments of $\\,w(X)\\,$ under $\\,q_{\\theta}\\,$. Then, for the specified $\\,\\pi\\,$ and $\\,q_{\\theta}\\,$, maximize this tractable approximation with respect to $\\,\\theta\\,$. Provide a single closed-form analytic expression for the optimizer $\\,\\theta^{\\star}\\,$. No numerical evaluation is required, and no rounding is needed. Express your final answer without units.",
            "solution": "The problem asks us to find the parameter $\\theta$ of a proposal distribution $q_\\theta(x) = \\mathcal{N}(x;\\theta,\\Sigma)$ that maximizes the Effective Sample Size (ESS) when the target is $\\pi(x) = \\mathcal{N}(x;\\mu,\\Sigma)$. We begin by deriving a large-sample approximation for the expectation of the ESS.\n\n**1. Asymptotic Approximation of ESS**\n\nThe given formula for ESS is $\\mathrm{ESS} = \\frac{(\\sum_{i=1}^{n} w(X_{i}))^{2}}{\\sum_{i=1}^{n} w(X_{i})^{2}}$. We can rewrite this as:\n$$\n\\mathrm{ESS} = n \\frac{\\left(\\frac{1}{n}\\sum_{i=1}^{n} w(X_{i})\\right)^{2}}{\\frac{1}{n}\\sum_{i=1}^{n} w(X_{i})^{2}}\n$$\nFor a large number of samples $n$, the Law of Large Numbers states that the sample averages converge to their true expectations under the proposal distribution $q_\\theta$.\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} w(X_{i}) \\to \\mathbb{E}_{q_{\\theta}}[w(X)] \\quad \\text{and} \\quad \\frac{1}{n}\\sum_{i=1}^{n} w(X_{i})^{2} \\to \\mathbb{E}_{q_{\\theta}}[w(X)^2]\n$$\nThis allows us to approximate the expectation of the ratio by the ratio of expectations, yielding the asymptotic approximation:\n$$\n\\mathbb{E}[\\mathrm{ESS}] \\approx n \\frac{(\\mathbb{E}_{q_{\\theta}}[w(X)])^{2}}{\\mathbb{E}_{q_{\\theta}}[w(X)^2]}\n$$\n\n**2. Calculation of Moments**\n\nNext, we calculate the first two moments of the weight function $w(x) = \\pi(x)/q_{\\theta}(x)$.\n\nThe first moment is a fundamental identity of importance sampling:\n$$\n\\mathbb{E}_{q_{\\theta}}[w(X)] = \\int w(x) q_{\\theta}(x) \\, dx = \\int \\frac{\\pi(x)}{q_{\\theta}(x)} q_{\\theta}(x) \\, dx = \\int \\pi(x) \\, dx = 1\n$$\nThe second moment, $\\mathbb{E}_{q_{\\theta}}[w(X)^2] = \\int w(x)^2 q_{\\theta}(x) \\, dx$, requires simplifying the product of three Gaussian densities. The weight function is:\n$$\nw(x) = \\frac{\\mathcal{N}(x;\\mu,\\Sigma)}{\\mathcal{N}(x;\\theta,\\Sigma)} = \\exp\\left(-\\frac{1}{2}\\left[(x-\\mu)^T \\Sigma^{-1} (x-\\mu) - (x-\\theta)^T \\Sigma^{-1} (x-\\theta)\\right]\\right)\n$$\nThe exponent simplifies to $x^T\\Sigma^{-1}(\\theta-\\mu) - \\frac{1}{2}(\\mu^T\\Sigma^{-1}\\mu - \\theta^T\\Sigma^{-1}\\theta)$. Thus, $w(x)^2 = \\exp\\left(2x^T\\Sigma^{-1}(\\theta-\\mu) - (\\mu^T\\Sigma^{-1}\\mu - \\theta^T\\Sigma^{-1}\\theta)\\right)$.\n\nThe integral for the second moment can be recognized as being related to the moment-generating function (MGF) of a multivariate normal random variable $X \\sim \\mathcal{N}(\\theta,\\Sigma)$, which is $M_X(t) = \\mathbb{E}[\\exp(t^T X)] = \\exp(t^T\\theta + \\frac{1}{2}t^T\\Sigma t)$.\n$$\n\\mathbb{E}_{q_{\\theta}}[w(X)^2] = \\exp(-(\\mu^T\\Sigma^{-1}\\mu - \\theta^T\\Sigma^{-1}\\theta)) \\cdot \\mathbb{E}_{q_{\\theta}}[\\exp((2\\Sigma^{-1}(\\theta-\\mu))^T X)]\n$$\nSetting $t = 2\\Sigma^{-1}(\\theta-\\mu)$, the expectation term is the MGF $M_X(t)$. The exponent in the MGF is:\n$$\nt^T\\theta + \\frac{1}{2}t^T\\Sigma t = (2\\Sigma^{-1}(\\theta-\\mu))^T \\theta + \\frac{1}{2}(2\\Sigma^{-1}(\\theta-\\mu))^T \\Sigma (2\\Sigma^{-1}(\\theta-\\mu))\n$$\n$$\n= 2(\\theta-\\mu)^T\\Sigma^{-1}\\theta + 2(\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)\n$$\nCombining this with the constant term from $w(x)^2$ gives the exponent of the second moment:\n$$\n\\text{Exponent} = -(\\mu^T\\Sigma^{-1}\\mu - \\theta^T\\Sigma^{-1}\\theta) + 2(\\theta^T\\Sigma^{-1}\\theta - \\mu^T\\Sigma^{-1}\\theta) + 2(\\theta^T\\Sigma^{-1}\\theta - 2\\mu^T\\Sigma^{-1}\\theta + \\mu^T\\Sigma^{-1}\\mu)\n$$\nAfter simplification, this becomes $(\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)$. Thus, the second moment is:\n$$\n\\mathbb{E}_{q_{\\theta}}[w(X)^2] = \\exp\\left((\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\n\n**3. Maximization**\n\nSubstituting the moments into our ESS approximation gives:\n$$\n\\mathbb{E}[\\mathrm{ESS}] \\approx n \\frac{1^2}{\\exp\\left((\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)\\right)} = n \\exp\\left(-(\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\nTo maximize this expression with respect to $\\theta$, we can equivalently minimize the positive term in the exponent:\n$$\nQ(\\theta) = (\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)\n$$\nThis is the squared Mahalanobis distance between $\\theta$ and $\\mu$. Since the covariance matrix $\\Sigma$ is positive-definite, its inverse $\\Sigma^{-1}$ is also positive-definite. For any positive-definite matrix $A$, the quadratic form $v^T A v \\ge 0$, with equality if and only if $v=0$.\nTherefore, $Q(\\theta)$ is minimized when $\\theta-\\mu=0$, which implies $\\theta=\\mu$.\n\nThe optimal choice for the proposal mean $\\theta^{\\star}$ that maximizes the ESS is the target mean $\\mu$. This intuitively makes sense, as it makes the proposal distribution identical to the target, yielding weights $w(x)=1$ and the maximum possible ESS of $n$.",
            "answer": "$$ \\boxed{\\mu} $$"
        },
        {
            "introduction": "Standard techniques like exponential tilting are powerful, but they are not a silver bullet and can fail spectacularly if misapplied. This exercise presents a scenario where exponential tilting leads to unbounded weights and an infinite-variance estimator, a critical failure mode in practice. By contrasting this with an alternative proposal based on transport maps , you will learn to critically analyze the behavior of importance weights and appreciate the need for more advanced and robust proposal design strategies for challenging problems.",
            "id": "3312692",
            "problem": "Consider the following rare-event estimation problem in the context of Importance Sampling (IS). Let the target distribution be the Laplace distribution with location parameter $0$ and scale parameter $1$, with probability density function $p(x) = \\frac{1}{2}\\exp(-|x|)$ for all $x \\in \\mathbb{R}$. The rare event is the left-tail exceedance $\\{x \\le -a\\}$ for a fixed $a  0$. Two proposal construction strategies are considered:\n\n1. Exponential-family tilting: For a fixed tilt parameter $\\theta \\in (-1,0)$, define the proposal $q_{\\theta}(x)$ by $q_{\\theta}(x) \\propto p(x)\\exp(\\theta x)$, normalized to be a probability density on $\\mathbb{R}$.\n\n2. Transport map proposal: Take a base proposal $q_{0}$ to be the Exponential$(\\lambda)$ distribution with rate parameter $\\lambda  0$ on $[0,\\infty)$, with density $q_{0}(u) = \\lambda \\exp(-\\lambda u)$. Define the deterministic transport $T:[0,\\infty) \\to (-\\infty,-a]$ by $T(u) = -a - u$, and set $q_{T} = T_{\\#}q_{0}$ to be the pushforward density on $(-\\infty,-a]$ induced by $T$.\n\nYou are to compare these two proposal constructions for estimating the rare-event probability $\\mathbb{P}(X \\le -a)$ using Importance Sampling. Work from first principles and the following foundational definitions:\n\n- The Importance Sampling weight for a proposal $q$ is $w(x) = \\frac{p(x)}{q(x)}$ on the support of $q$.\n- The pushforward $q_{T} = T_{\\#}q_{0}$ satisfies $q_{T}(x) = q_{0}(u)\\left|\\frac{d u}{d x}\\right|$ when $x = T(u)$ and $T$ is differentiable and invertible on its range.\n- The moment generating function $M_{X}(\\theta) = \\mathbb{E}[\\exp(\\theta X)]$ of the Laplace$(0,1)$ distribution exists for $|\\theta|  1$ and equals $M_{X}(\\theta) = \\frac{1}{1 - \\theta^{2}}$.\n\nTasks:\n- Using these definitions, show that for $\\theta \\in (-1,0)$, the IS weights $w_{\\theta}(x)$ associated with the exponential-family tilting $q_{\\theta}$ are unbounded on the event $\\{x \\le -a\\}$.\n- For the transport proposal $q_{T}$, derive the exact IS weights $w_{T}(x)$ on $(-\\infty,-a]$ and determine the supremum over the event, $\\sup_{x \\le -a} w_{T}(x)$.\n\nReport the final answer as the exact closed-form expression for $\\sup_{x \\le -a} w_{T}(x)$ in terms of $a$ and $\\lambda$. No rounding is required.",
            "solution": "This problem requires analyzing two importance sampling (IS) proposals for estimating a rare-event probability for a Laplace distribution. We will analyze each proposal separately.\n\n**Part 1: Exponential-Family Tilting Proposal $q_{\\theta}(x)$**\n\nFirst, we derive the normalized density $q_{\\theta}(x)$. The unnormalized density is $p(x)\\exp(\\theta x)$. The normalization constant is the integral of this product over $\\mathbb{R}$, which is the definition of the moment-generating function (MGF) of the target distribution $p(x)$. Using the provided MGF for the Laplace(0,1) distribution:\n$$ C_{\\theta} = \\int_{-\\infty}^{\\infty} p(x)\\exp(\\theta x) dx = M_X(\\theta) = \\frac{1}{1 - \\theta^2} $$\nThis is valid for $|\\theta|  1$, which covers the specified range $\\theta \\in (-1, 0)$. The normalized proposal density is:\n$$ q_{\\theta}(x) = \\frac{p(x)\\exp(\\theta x)}{M_X(\\theta)} = (1 - \\theta^2) p(x) \\exp(\\theta x) $$\nNext, we derive the IS weight $w_{\\theta}(x) = p(x)/q_{\\theta}(x)$:\n$$ w_{\\theta}(x) = \\frac{p(x)}{(1 - \\theta^2) p(x) \\exp(\\theta x)} = \\frac{1}{1 - \\theta^2} \\exp(-\\theta x) $$\nWe are asked to show that these weights are unbounded on the event region $\\{x \\le -a\\}$. Since the parameter $\\theta$ is in the range $(-1,0)$, the term $-\\theta$ is positive, i.e., $-\\theta \\in (0,1)$. We analyze the behavior of the weight as $x$ approaches $-\\infty$:\n$$ \\lim_{x \\to -\\infty} w_{\\theta}(x) = \\lim_{x \\to -\\infty} \\frac{1}{1 - \\theta^2} \\exp(-\\theta x) $$\nBecause $-\\theta > 0$, the argument of the exponential, $-\\theta x$, approaches $+\\infty$ as $x \\to -\\infty$. Consequently, the weight function diverges to infinity. Since the function is unbounded on the event region, the IS estimator will have infinite variance, rendering this proposal strategy ineffective.\n\n**Part 2: Transport Map Proposal $q_T(x)$**\n\nFirst, we derive the proposal density $q_T(x)$, which is the pushforward of the Exponential($\\lambda$) distribution $q_0(u)$ under the map $T(u) = -a - u$. The support of $q_0$ is $[0,\\infty)$, so the support of $q_T$ is $(-\\infty, -a]$.\nTo apply the change-of-variables formula, we find the inverse map $u = T^{-1}(x)$ and its Jacobian. From $x = -a - u$, we have $u = -a - x$. The derivative is $\\frac{du}{dx} = -1$, so the absolute value of the Jacobian is $|\\frac{du}{dx}| = 1$.\nThe pushforward density is given by $q_T(x) = q_0(T^{-1}(x)) \\left|\\frac{du}{dx}\\right|$:\n$$ q_T(x) = q_0(-a-x) \\cdot 1 = \\lambda \\exp(-\\lambda(-a-x)) = \\lambda \\exp(\\lambda a + \\lambda x) $$\nThis density is defined for $x \\in (-\\infty, -a]$. Next, we derive the IS weight $w_T(x) = p(x)/q_T(x)$ on this support. For $x \\le -a  0$, the absolute value $|x|$ becomes $-x$.\n$$ p(x) = \\frac{1}{2}\\exp(-|x|) = \\frac{1}{2}\\exp(x) \\quad \\text{for } x \\le -a $$\nThe weight function is therefore:\n$$ w_T(x) = \\frac{\\frac{1}{2}\\exp(x)}{\\lambda \\exp(\\lambda a + \\lambda x)} = \\frac{1}{2\\lambda} \\exp(x - (\\lambda a + \\lambda x)) = \\frac{1}{2\\lambda} \\exp(-\\lambda a + (1-\\lambda)x) $$\nFinally, we find the supremum of this weight function over the domain $x \\in (-\\infty, -a]$. The behavior of $w_T(x)$ depends on the sign of the coefficient of $x$, which is $(1-\\lambda)$.\n- If $\\lambda > 1$, then $1-\\lambda  0$. The exponent $(1-\\lambda)x$ goes to $+\\infty$ as $x \\to -\\infty$, making the weights unbounded.\n- If $\\lambda \\le 1$, then $1-\\lambda \\ge 0$. In this case, $\\exp((1-\\lambda)x)$ is a non-decreasing function of $x$. Its maximum over the interval $(-\\infty, -a]$ occurs at the right endpoint, $x=-a$.\nAn effective IS proposal requires bounded weights, so we consider the case $\\lambda \\le 1$. The supremum is found by evaluating the weight at $x=-a$:\n$$ \\sup_{x \\le -a} w_T(x) = w_T(-a) = \\frac{1}{2\\lambda} \\exp(-\\lambda a + (1-\\lambda)(-a)) $$\n$$ = \\frac{1}{2\\lambda} \\exp(-\\lambda a - a + \\lambda a) = \\frac{1}{2\\lambda} \\exp(-a) $$\nThis is the closed-form expression for the supremum of the weights for any effective proposal in this family.",
            "answer": "$$\\boxed{\\frac{\\exp(-a)}{2\\lambda}}$$"
        }
    ]
}