{
    "hands_on_practices": [
        {
            "introduction": "A key to successful importance sampling is choosing a proposal distribution $q(x)$ that effectively approximates the target distribution, especially in regions that contribute most to the expectation. A primary metric for quantifying the efficiency of an importance sampler is the Effective Sample Size (ESS), which measures how much the variance of an estimator is inflated due to unequal importance weights. This exercise  provides a foundational theoretical workout, guiding you to derive the optimal proposal distribution within a given parametric family by maximizing a large-sample approximation of the ESS, thereby revealing the direct link between sampling efficiency and the similarity of the proposal and target distributions.",
            "id": "3312707",
            "problem": "Consider a target probability density $\\,\\pi(x)\\,$ on $\\,\\mathbb{R}^{d}\\,$ given by a multivariate normal distribution with mean $\\,\\mu \\in \\mathbb{R}^{d}\\,$ and positive-definite covariance matrix $\\,\\Sigma \\in \\mathbb{R}^{d \\times d}\\,$, i.e., $\\,\\pi(x) = \\mathcal{N}(x;\\mu,\\Sigma)\\,$. For Importance Sampling (IS), draw $\\,n\\,$ independent samples $\\,X_{1},\\dots,X_{n}\\,$ from a proposal family $\\,q_{\\theta}(x)\\,$ that is also multivariate normal with the same covariance but a tunable mean parameter $\\,\\theta \\in \\mathbb{R}^{d}\\,$, i.e., $\\,q_{\\theta}(x) = \\mathcal{N}(x;\\theta,\\Sigma)\\,$. Define the IS weights $\\,w(x) = \\pi(x)/q_{\\theta}(x)\\,$ and the Effective Sample Size (ESS) for these $\\,n\\,$ samples by\n$$\n\\mathrm{ESS} \\;=\\; \\frac{\\left(\\sum_{i=1}^{n} w(X_{i})\\right)^{2}}{\\sum_{i=1}^{n} w(X_{i})^{2}}.\n$$\nStarting from first principles—namely, the definitions of Importance Sampling, properties of the multivariate normal distribution, and the Law of Large Numbers—derive an analytically tractable approximation to $\\,\\mathbb{E}[\\mathrm{ESS}]\\,$ that only depends on the first and second moments of $\\,w(X)\\,$ under $\\,q_{\\theta}\\,$. Then, for the specified $\\,\\pi\\,$ and $\\,q_{\\theta}\\,$, maximize this tractable approximation with respect to $\\,\\theta\\,$. Provide a single closed-form analytic expression for the optimizer $\\,\\theta^{\\star}\\,$. No numerical evaluation is required, and no rounding is needed. Express your final answer without units.",
            "solution": "The user wants me to solve the following problem.\n- **Problem Statement**:\nConsider a target probability density $\\,\\pi(x)\\,$ on $\\,\\mathbb{R}^{d}\\,$ given by a multivariate normal distribution with mean $\\,\\mu \\in \\mathbb{R}^{d}\\,$ and positive-definite covariance matrix $\\,\\Sigma \\in \\mathbb{R}^{d \\times d}\\,$, i.e., $\\,\\pi(x) = \\mathcal{N}(x;\\mu,\\Sigma)\\,$. For Importance Sampling (IS), draw $\\,n\\,$ independent samples $\\,X_{1},\\dots,X_{n}\\,$ from a proposal family $\\,q_{\\theta}(x)\\,$ that is also multivariate normal with the same covariance but a tunable mean parameter $\\,\\theta \\in \\mathbb{R}^{d}\\,$, i.e., $\\,q_{\\theta}(x) = \\mathcal{N}(x;\\theta,\\Sigma)\\,$. Define the IS weights $\\,w(x) = \\pi(x)/q_{\\theta}(x)\\,$ and the Effective Sample Size (ESS) for these $\\,n\\,$ samples by\n$$\n\\mathrm{ESS} \\;=\\; \\frac{\\left(\\sum_{i=1}^{n} w(X_{i})\\right)^{2}}{\\sum_{i=1}^{n} w(X_{i})^{2}}.\n$$\nStarting from first principles—namely, the definitions of Importance Sampling, properties of the multivariate normal distribution, and the Law of Large Numbers—derive an analytically tractable approximation to $\\,\\mathbb{E}[\\mathrm{ESS}]\\,$ that only depends on the first and second moments of $\\,w(X)\\,$ under $\\,q_{\\theta}\\,$. Then, for the specified $\\,\\pi\\,$ and $\\,q_{\\theta}\\,$, maximize this tractable approximation with respect to $\\,\\theta\\,$. Provide a single closed-form analytic expression for the optimizer $\\,\\theta^{\\star}\\,$. No numerical evaluation is required, and no rounding is needed. Express your final answer without units.\n\n### Step 1: Extract Givens\n- Target distribution: $\\pi(x) = \\mathcal{N}(x; \\mu, \\Sigma)$, where $\\mu \\in \\mathbb{R}^d$ and $\\Sigma \\in \\mathbb{R}^{d \\times d}$ is positive-definite.\n- Proposal distribution: $q_{\\theta}(x) = \\mathcal{N}(x; \\theta, \\Sigma)$, where $\\theta \\in \\mathbb{R}^d$ is a tunable parameter.\n- Samples: $X_1, \\dots, X_n$ are independent and identically distributed (i.i.d.) samples from $q_{\\theta}(x)$.\n- Importance weight function: $w(x) = \\pi(x) / q_{\\theta}(x)$.\n- Effective Sample Size (ESS) definition: $\\mathrm{ESS} = \\frac{(\\sum_{i=1}^{n} w(X_{i}))^{2}}{\\sum_{i=1}^{n} w(X_{i})^{2}}$.\n- Task 1: Derive an analytically tractable approximation to $\\mathbb{E}[\\mathrm{ESS}]$ using the first and second moments of $w(X)$ under $q_{\\theta}$.\n- Task 2: Maximize this approximation with respect to $\\theta$ to find the optimal parameter $\\theta^{\\star}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-established within the field of stochastic simulation and Monte Carlo methods. The concepts of Importance Sampling, multivariate normal distributions, and Effective Sample Size are standard and mathematically sound.\n- **Well-Posed**: The problem is clearly stated with all necessary definitions. The objective is explicit: derive an approximation and find its maximum. The existence of a unique solution is plausible given the structure of the problem.\n- **Objective**: The language is formal, precise, and devoid of any subjectivity.\n\nThe problem does not exhibit any of the invalidity flaws:\n1.  It is scientifically and mathematically sound.\n2.  It is a formal problem, directly relevant to the topic.\n3.  It is self-contained and free of contradictions. The condition that $\\Sigma$ is positive-definite is correctly stated.\n4.  The setup is a standard, idealized scenario used to study the properties of Importance Sampling; it is not physically unrealistic.\n5.  It is well-structured and leads to a unique, meaningful solution.\n6.  The problem is not trivial; it requires a multi-step derivation involving statistical theory and calculus.\n7.  The result is mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\n###\nThe problem requires us to first derive a tractable approximation for the expectation of the Effective Sample Size, $\\mathbb{E}[\\mathrm{ESS}]$, and then maximize this approximation with respect to the proposal parameter $\\theta$.\n\nFirst, let's derive the approximation for $\\mathbb{E}[\\mathrm{ESS}]$. The given expression for ESS is a ratio of sums of random variables:\n$$\n\\mathrm{ESS} = \\frac{\\left(\\sum_{i=1}^{n} w(X_{i})\\right)^{2}}{\\sum_{i=1}^{n} w(X_{i})^{2}}\n$$\nwhere $X_i \\sim q_{\\theta}(x)$ are i.i.d. samples.\nWe can rewrite this expression by dividing the numerator and denominator by $n^2$ and $n$ respectively:\n$$\n\\mathrm{ESS} = n \\frac{\\left(\\frac{1}{n}\\sum_{i=1}^{n} w(X_{i})\\right)^{2}}{\\frac{1}{n}\\sum_{i=1}^{n} w(X_{i})^{2}}\n$$\nBy the Law of Large Numbers, for a large number of samples $n$, the sample averages converge in probability to their respective expectations:\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} w(X_{i}) \\xrightarrow{p} \\mathbb{E}_{q_{\\theta}}[w(X)]\n$$\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} w(X_{i})^{2} \\xrightarrow{p} \\mathbb{E}_{q_{\\theta}}[w(X)^2]\n$$\nwhere the expectations are taken with respect to the proposal distribution $q_{\\theta}$. Using this convergence, we can form an approximation for $\\mathrm{ESS}$ which becomes increasingly accurate as $n \\to \\infty$. The expectation of the ratio is approximated by the ratio of expectations. Thus, the tractable approximation for $\\mathbb{E}[\\mathrm{ESS}]$ is given by:\n$$\n\\mathbb{E}[\\mathrm{ESS}] \\approx n \\frac{\\left(\\mathbb{E}_{q_{\\theta}}[w(X)]\\right)^{2}}{\\mathbb{E}_{q_{\\theta}}[w(X)^2]}\n$$\nThis expression depends only on the first two moments of the weight $w(X)$ under $q_{\\theta}$, as requested.\n\nNext, we must calculate these two moments for the specified distributions.\nThe first moment is a fundamental property of importance sampling:\n$$\n\\mathbb{E}_{q_{\\theta}}[w(X)] = \\int w(x) q_{\\theta}(x) \\, dx = \\int \\frac{\\pi(x)}{q_{\\theta}(x)} q_{\\theta}(x) \\, dx = \\int \\pi(x) \\, dx = 1\n$$\nsince $\\pi(x)$ is a probability density function.\n\nNow, we calculate the second moment, $\\mathbb{E}_{q_{\\theta}}[w(X)^2]$. The weight function is a ratio of two multivariate normal densities:\n$$\nw(x) = \\frac{\\mathcal{N}(x;\\mu,\\Sigma)}{\\mathcal{N}(x;\\theta,\\Sigma)} = \\frac{\\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}\\exp\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right)}{\\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}\\exp\\left(-\\frac{1}{2}(x-\\theta)^T \\Sigma^{-1} (x-\\theta)\\right)}\n$$\n$$\nw(x) = \\exp\\left(-\\frac{1}{2}\\left[(x-\\mu)^T \\Sigma^{-1} (x-\\mu) - (x-\\theta)^T \\Sigma^{-1} (x-\\theta)\\right]\\right)\n$$\nLet's simplify the term in the exponent:\n$$\n(x-\\mu)^T \\Sigma^{-1} (x-\\mu) - (x-\\theta)^T \\Sigma^{-1} (x-\\theta) = (x^T\\Sigma^{-1}x - 2x^T\\Sigma^{-1}\\mu + \\mu^T\\Sigma^{-1}\\mu) - (x^T\\Sigma^{-1}x - 2x^T\\Sigma^{-1}\\theta + \\theta^T\\Sigma^{-1}\\theta)\n$$\n$$\n= 2x^T\\Sigma^{-1}(\\theta-\\mu) + \\mu^T\\Sigma^{-1}\\mu - \\theta^T\\Sigma^{-1}\\theta\n$$\nSo, the weight is $w(x) = \\exp\\left(x^T\\Sigma^{-1}(\\theta-\\mu) + \\frac{1}{2}(\\mu^T\\Sigma^{-1}\\mu - \\theta^T\\Sigma^{-1}\\theta)\\right)$, where I have absorbed the factor of $-1/2$. A mistake was made in sign above. The argument of the exponential is $-\\frac{1}{2} (2x^T\\Sigma^{-1}(\\theta-\\mu) + \\mu^T\\Sigma^{-1}\\mu - \\theta^T\\Sigma^{-1}\\theta) = -x^T\\Sigma^{-1}(\\theta-\\mu) - \\frac{1}{2}(\\mu^T\\Sigma^{-1}\\mu - \\theta^T\\Sigma^{-1}\\theta)$.\nThus, $w(x)^2$ is:\n$$\nw(x)^2 = \\exp\\left(-2x^T\\Sigma^{-1}(\\theta-\\mu) - (\\mu^T\\Sigma^{-1}\\mu - \\theta^T\\Sigma^{-1}\\theta)\\right)\n$$\nThe second moment is the expectation of this quantity under $q_{\\theta}(x) = \\mathcal{N}(x;\\theta,\\Sigma)$:\n$$\n\\mathbb{E}_{q_{\\theta}}[w(X)^2] = \\int w(x)^2 q_{\\theta}(x) \\, dx\n$$\n$$\n= \\int \\exp\\left(-2x^T\\Sigma^{-1}(\\theta-\\mu) - (\\mu^T\\Sigma^{-1}\\mu - \\theta^T\\Sigma^{-1}\\theta)\\right) \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}\\exp\\left(-\\frac{1}{2}(x-\\theta)^T \\Sigma^{-1} (x-\\theta)\\right) dx\n$$\nLet's combine the terms in the exponents. The overall exponent is:\n$$\n-2x^T\\Sigma^{-1}(\\theta-\\mu) - (\\mu^T\\Sigma^{-1}\\mu - \\theta^T\\Sigma^{-1}\\theta) - \\frac{1}{2}(x^T\\Sigma^{-1}x - 2x^T\\Sigma^{-1}\\theta + \\theta^T\\Sigma^{-1}\\theta)\n$$\n$$\n= -\\frac{1}{2}x^T\\Sigma^{-1}x + (2x^T\\Sigma^{-1}\\mu - 2x^T\\Sigma^{-1}\\theta + x^T\\Sigma^{-1}\\theta) - \\mu^T\\Sigma^{-1}\\mu + \\theta^T\\Sigma^{-1}\\theta - \\frac{1}{2}\\theta^T\\Sigma^{-1}\\theta\n$$\n$$\n= -\\frac{1}{2}x^T\\Sigma^{-1}x + x^T\\Sigma^{-1}(2\\mu - \\theta) - \\mu^T\\Sigma^{-1}\\mu + \\frac{1}{2}\\theta^T\\Sigma^{-1}\\theta\n$$\nWe complete the square for the terms involving $x$:\n$$\n-\\frac{1}{2}x^T\\Sigma^{-1}x + x^T\\Sigma^{-1}(2\\mu - \\theta) = -\\frac{1}{2}\\left(x - (2\\mu-\\theta)\\right)^T\\Sigma^{-1}\\left(x-(2\\mu-\\theta)\\right) + \\frac{1}{2}(2\\mu-\\theta)^T\\Sigma^{-1}(2\\mu-\\theta)\n$$\nSubstituting this back into the full exponent expression:\n$$\n\\text{Exponent} = -\\frac{1}{2}\\left(x - (2\\mu-\\theta)\\right)^T\\Sigma^{-1}\\left(x-(2\\mu-\\theta)\\right) + \\frac{1}{2}(2\\mu-\\theta)^T\\Sigma^{-1}(2\\mu-\\theta) - \\mu^T\\Sigma^{-1}\\mu + \\frac{1}{2}\\theta^T\\Sigma^{-1}\\theta\n$$\nThe constant part of the exponent simplifies to:\n$$\n\\frac{1}{2}(4\\mu^T\\Sigma^{-1}\\mu - 4\\mu^T\\Sigma^{-1}\\theta + \\theta^T\\Sigma^{-1}\\theta) - \\mu^T\\Sigma^{-1}\\mu + \\frac{1}{2}\\theta^T\\Sigma^{-1}\\theta\n$$\n$$\n= 2\\mu^T\\Sigma^{-1}\\mu - 2\\mu^T\\Sigma^{-1}\\theta + \\frac{1}{2}\\theta^T\\Sigma^{-1}\\theta - \\mu^T\\Sigma^{-1}\\mu + \\frac{1}{2}\\theta^T\\Sigma^{-1}\\theta\n$$\n$$\n= \\mu^T\\Sigma^{-1}\\mu - 2\\mu^T\\Sigma^{-1}\\theta + \\theta^T\\Sigma^{-1}\\theta = (\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)\n$$\nThe integral for the second moment becomes:\n$$\n\\mathbb{E}_{q_{\\theta}}[w(X)^2] = \\int \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}\\exp\\left(-\\frac{1}{2}\\left(x - (2\\mu-\\theta)\\right)^T\\Sigma^{-1}\\left(x-(2\\mu-\\theta)\\right) \\right) \\exp\\left((\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)\\right) dx\n$$\nThe term $\\exp\\left((\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)\\right)$ is a constant with respect to $x$. The remaining integral is the integral of a normal density $\\mathcal{N}(x; 2\\mu-\\theta, \\Sigma)$ over its domain, which is $1$.\nTherefore, the second moment is:\n$$\n\\mathbb{E}_{q_{\\theta}}[w(X)^2] = \\exp\\left((\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\nNow, substitute the moments back into the approximation for $\\mathbb{E}[\\mathrm{ESS}]$:\n$$\n\\mathbb{E}[\\mathrm{ESS}] \\approx n \\frac{1^2}{\\exp\\left((\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)\\right)} = n \\exp\\left(-(\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\nOur final task is to maximize this expression with respect to $\\theta$. Let the objective function be $J(\\theta)$:\n$$\nJ(\\theta) = n \\exp\\left(-(\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\nSince $n$ is a positive constant and the exponential function $\\exp(z)$ is strictly monotonically increasing, maximizing $J(\\theta)$ is equivalent to maximizing its argument, which in turn is equivalent to minimizing the quadratic form:\n$$\nQ(\\theta) = (\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)\n$$\nThe term $Q(\\theta)$ is the squared Mahalanobis distance between the vectors $\\theta$ and $\\mu$. The matrix $\\Sigma$ is given as a positive-definite covariance matrix, which implies its inverse $\\Sigma^{-1}$ is also positive-definite.\nFor any positive-definite matrix $A$ and any non-zero vector $v$, the quadratic form $v^T A v$ is strictly positive. The minimum value of this form is $0$, which is achieved if and only if $v = 0$.\nIn our case, let $v = \\theta - \\mu$. The minimum of $Q(\\theta)$ is $0$, and this occurs if and only if:\n$$\n\\theta - \\mu = 0\n$$\nThis implies that the optimal choice for the parameter $\\theta$ is:\n$$\n\\theta^{\\star} = \\mu\n$$\nThis result is intuitively correct. The effective sample size is a measure of sampling efficiency. The efficiency is maximized when the proposal distribution $q_{\\theta}(x)$ is identical to the target distribution $\\pi(x)$. In this problem, both distributions are in the same family $\\mathcal{N}(\\cdot, \\Sigma)$, so making them identical requires setting their means to be equal, i.e., $\\theta = \\mu$. In this case, $w(x) = 1$ for all $x$, and the true ESS becomes exactly $n$, which is its maximum possible value. Our approximation also yields this maximum value: $J(\\mu) = n \\exp(0) = n$.",
            "answer": "$$ \\boxed{\\mu} $$"
        },
        {
            "introduction": "While matching the target is ideal, it is often infeasible in practice, particularly for complex problems like rare-event probability estimation. In these scenarios, the design of the proposal distribution is critical and requires more sophisticated strategies than simple parameter tuning. This practice  contrasts two powerful techniques for constructing proposals in such challenging settings: exponential tilting and transport maps. By analyzing the behavior of the importance weights, this exercise reveals a crucial failure mode—unbounded weights leading to infinite variance—and demonstrates how a thoughtful choice of proposal structure is essential for developing a stable and effective estimator.",
            "id": "3312692",
            "problem": "Consider the following rare-event estimation problem in the context of Importance Sampling (IS). Let the target distribution be the Laplace distribution with location parameter $0$ and scale parameter $1$, with probability density function $p(x) = \\frac{1}{2}\\exp(-|x|)$ for all $x \\in \\mathbb{R}$. The rare event is the left-tail exceedance $\\{x \\le -a\\}$ for a fixed $a  0$. Two proposal construction strategies are considered:\n\n1. Exponential-family tilting: For a fixed tilt parameter $\\theta \\in (-1,0)$, define the proposal $q_{\\theta}(x)$ by $q_{\\theta}(x) \\propto p(x)\\exp(\\theta x)$, normalized to be a probability density on $\\mathbb{R}$.\n\n2. Transport map proposal: Take a base proposal $q_{0}$ to be the Exponential$(\\lambda)$ distribution with rate parameter $\\lambda  0$ on $[0,\\infty)$, with density $q_{0}(u) = \\lambda \\exp(-\\lambda u)$. Define the deterministic transport $T:[0,\\infty) \\to (-\\infty,-a]$ by $T(u) = -a - u$, and set $q_{T} = T_{\\#}q_{0}$ to be the pushforward density on $(-\\infty,-a]$ induced by $T$.\n\nYou are to compare these two proposal constructions for estimating the rare-event probability $\\mathbb{P}(X \\le -a)$ using Importance Sampling. Work from first principles and the following foundational definitions:\n\n- The Importance Sampling weight for a proposal $q$ is $w(x) = \\frac{p(x)}{q(x)}$ on the support of $q$.\n- The pushforward $q_{T} = T_{\\#}q_{0}$ satisfies $q_{T}(x) = q_{0}(u)\\left|\\frac{d u}{d x}\\right|$ when $x = T(u)$ and $T$ is differentiable and invertible on its range.\n- The moment generating function $M_{X}(\\theta) = \\mathbb{E}[\\exp(\\theta X)]$ of the Laplace$(0,1)$ distribution exists for $|\\theta|  1$ and equals $M_{X}(\\theta) = \\frac{1}{1 - \\theta^{2}}$.\n\nTasks:\n- Using these definitions, show that for $\\theta \\in (-1,0)$, the IS weights $w_{\\theta}(x)$ associated with the exponential-family tilting $q_{\\theta}$ are unbounded on the event $\\{x \\le -a\\}$.\n- For the transport proposal $q_{T}$, derive the exact IS weights $w_{T}(x)$ on $(-\\infty,-a]$ and determine the supremum over the event, $\\sup_{x \\le -a} w_{T}(x)$.\n\nReport the final answer as the exact closed-form expression for $\\sup_{x \\le -a} w_{T}(x)$ in terms of $a$ and $\\lambda$. No rounding is required.",
            "solution": "The user wants to analyze and compare two different importance sampling proposal strategies for estimating the rare-event probability $\\mathbb{P}(X \\le -a)$ where $X$ follows a Laplace distribution.\n\n### Step 1: Problem Validation\n\n**Extracted Givens:**\n- Target distribution density: $p(x) = \\frac{1}{2}\\exp(-|x|)$ for $x \\in \\mathbb{R}$.\n- Rare event: $\\{x \\le -a\\}$ for a fixed $a  0$.\n- Proposal 1 (Exponential Tilting): $q_{\\theta}(x) \\propto p(x)\\exp(\\theta x)$ for a fixed $\\theta \\in (-1,0)$.\n- Proposal 2 (Transport Map):\n    - Base distribution density: $q_{0}(u) = \\lambda \\exp(-\\lambda u)$ for $u \\in [0,\\infty)$, with $\\lambda  0$.\n    - Transport map: $T:[0,\\infty) \\to (-\\infty,-a]$ defined by $T(u) = -a - u$.\n    - Proposal density: $q_{T} = T_{\\#}q_{0}$.\n- IS weight definition: $w(x) = \\frac{p(x)}{q(x)}$.\n- Pushforward density formula: $q_{T}(x) = q_{0}(u)\\left|\\frac{d u}{d x}\\right|$ for $x = T(u)$.\n- MGF of Laplace$(0,1)$: $M_{X}(\\theta) = \\mathbb{E}[\\exp(\\theta X)] = \\frac{1}{1 - \\theta^{2}}$ for $|\\theta|  1$.\n\n**Validation:**\n1.  **Scientific Grounding:** The problem is based on established principles of probability theory and stochastic simulation, specifically importance sampling, Laplace distributions, exponential tilting, and transport maps. The concepts and definitions are standard and scientifically sound.\n2.  **Well-Posedness:** The tasks are clearly defined: show unboundedness of weights for one proposal and derive the weights and their supremum for the other. The given information is sufficient to perform these tasks.\n3.  **Objectivity:** The problem is stated in precise, objective mathematical language.\n4.  **Consistency and Completeness:** The problem is self-contained. The normalization constant for $q_{\\theta}(x)$ can be found using the provided MGF. The details for the transport map proposal are all specified. There are no contradictions.\n\n**Verdict:** The problem is valid. We proceed to the solution.\n\n### Step 2: Solution Derivation\n\nThe solution is divided into two parts, one for each proposal construction strategy.\n\n**Part 1: Exponential-Family Tilting Proposal $q_{\\theta}(x)$**\n\nFirst, we must normalize the proposal density $q_{\\theta}(x)$. The definition is $q_{\\theta}(x) \\propto p(x)\\exp(\\theta x)$. The normalization constant $C_{\\theta}$ is the integral of the unnormalized density over $\\mathbb{R}$:\n$$C_{\\theta} = \\int_{-\\infty}^{\\infty} p(x)\\exp(\\theta x) \\,dx$$\nThis integral is, by definition, the moment generating function (MGF) of the random variable $X$ with density $p(x)$, evaluated at $\\theta$. The problem provides this MGF:\n$$C_{\\theta} = M_{X}(\\theta) = \\frac{1}{1 - \\theta^{2}}$$\nThis is valid for $|\\theta|  1$, which includes the specified range $\\theta \\in (-1,0)$.\nThus, the normalized proposal density is:\n$$q_{\\theta}(x) = \\frac{p(x)\\exp(\\theta x)}{M_{X}(\\theta)} = (1 - \\theta^{2})p(x)\\exp(\\theta x)$$\nNow, we can compute the importance sampling weight $w_{\\theta}(x)$:\n$$w_{\\theta}(x) = \\frac{p(x)}{q_{\\theta}(x)} = \\frac{p(x)}{(1 - \\theta^{2})p(x)\\exp(\\theta x)} = \\frac{1}{1 - \\theta^{2}}\\exp(-\\theta x)$$\nWe are asked to show that these weights are unbounded on the event $\\{x \\le -a\\}$. This event corresponds to the interval $(-\\infty, -a]$. We must analyze the behavior of $w_{\\theta}(x)$ as $x \\to -\\infty$.\nThe problem specifies that $\\theta \\in (-1,0)$, which implies that $-\\theta$ is strictly positive, i.e., $-\\theta \\in (0,1)$.\nConsider the limit of $w_{\\theta}(x)$ as $x$ approaches $-\\infty$:\n$$\\lim_{x \\to -\\infty} w_{\\theta}(x) = \\lim_{x \\to -\\infty} \\frac{1}{1 - \\theta^{2}}\\exp(-\\theta x)$$\nSince $-\\theta  0$, the argument of the exponential, $-\\theta x$, approaches $+\\infty$ as $x \\to -\\infty$. Therefore, the exponential term diverges:\n$$\\lim_{x \\to -\\infty} \\exp(-\\theta x) = +\\infty$$\nThis implies that the weight function also diverges:\n$$\\lim_{x \\to -\\infty} w_{\\theta}(x) = +\\infty$$\nSince the function $w_{\\theta}(x)$ is continuous and its limit is infinite at one end of the interval $(-\\infty, -a]$, it is unbounded on this interval. This demonstrates that for any choice of $\\theta \\in (-1,0)$, the exponential tilting proposal leads to an importance sampling estimator with infinite variance for the specified rare event.\n\n**Part 2: Transport Map Proposal $q_{T}(x)$**\n\nFirst, we derive the proposal density $q_T(x) = (T_{\\#}q_0)(x)$. The transport map is $x = T(u) = -a - u$. The base distribution $q_0$ has support $u \\in [0,\\infty)$, so the support of $q_T$ is the image of this set under $T$, which is $(-\\infty, -a]$.\nTo use the pushforward formula, we must find the inverse map $u = T^{-1}(x)$ and the Jacobian of this transformation.\nFrom $x = -a - u$, we get the inverse map:\n$$u = -a - x$$\nThe derivative is $\\frac{du}{dx} = -1$. The absolute value of the Jacobian determinant is $\\left|\\frac{du}{dx}\\right| = |-1| = 1$.\nUsing the pushforward formula, for $x \\in (-\\infty, -a]$:\n$$q_{T}(x) = q_{0}(T^{-1}(x))\\left|\\frac{du}{dx}\\right| = q_{0}(-a-x) \\cdot 1$$\nSubstituting into the density for $q_0(u) = \\lambda \\exp(-\\lambda u)$:\n$$q_{T}(x) = \\lambda \\exp(-\\lambda(-a-x)) = \\lambda \\exp(\\lambda a + \\lambda x)$$\nThis is the proposal density for $x \\le -a$. For $x  -a$, $q_T(x) = 0$.\n\nNext, we derive the IS weight $w_T(x) = \\frac{p(x)}{q_T(x)}$ on the support of $q_T$, which is $(-\\infty, -a]$. On this interval, $x$ is negative, so the absolute value $|x|$ in the target density $p(x)$ is equal to $-x$.\n$$p(x) = \\frac{1}{2}\\exp(-(-x)) = \\frac{1}{2}\\exp(x) \\quad \\text{for } x \\le -a$$\nNow we can form the weight function:\n$$w_T(x) = \\frac{p(x)}{q_T(x)} = \\frac{\\frac{1}{2}\\exp(x)}{\\lambda \\exp(\\lambda a + \\lambda x)} = \\frac{1}{2\\lambda} \\exp(x - (\\lambda a + \\lambda x)) = \\frac{1}{2\\lambda} \\exp(-\\lambda a + (1-\\lambda)x)$$\n\nFinally, we determine the supremum of $w_T(x)$ over the event domain $x \\in (-\\infty, -a]$. The behavior of $w_T(x)$ depends on the sign of the coefficient of $x$ in the exponent, which is $(1-\\lambda)$.\n- **Case 1: $\\lambda  1$.** In this case, $1-\\lambda  0$. The function $w_T(x)$ is proportional to $\\exp((1-\\lambda)x)$. As $x \\to -\\infty$, the argument $(1-\\lambda)x \\to +\\infty$. Thus, $\\lim_{x \\to -\\infty} w_T(x) = +\\infty$. The weights are unbounded, and the supremum is infinite. Such a proposal is not useful for variance reduction.\n- **Case 2: $\\lambda \\le 1$.** In this case, $1-\\lambda \\ge 0$. The function $\\exp((1-\\lambda)x)$ is a non-decreasing function of $x$. Therefore, its supremum over the interval $(-\\infty, -a]$ must occur at the right boundary, $x = -a$.\n\nThe problem asks for the expression for the supremum. An importance sampling proposal is only considered effective if the resulting weights are bounded, which ensures a finite variance for the IS estimator. This requires that the supremum be finite, which confines our interest to the case where $\\lambda \\le 1$. We therefore compute the supremum for this range of $\\lambda$.\nFor any $\\lambda \\in (0,1]$, the supremum is achieved at $x=-a$:\n$$\\sup_{x \\le -a} w_{T}(x) = w_{T}(-a) = \\frac{1}{2\\lambda} \\exp(-\\lambda a + (1-\\lambda)(-a))$$\n$$= \\frac{1}{2\\lambda} \\exp(-\\lambda a - a + \\lambda a)$$\n$$= \\frac{1}{2\\lambda} \\exp(-a)$$\nThis expression gives the finite supremum of the importance weights for any effective proposal within this transport map family, characterized by $\\lambda \\in (0,1]$. Note that this supremum is minimized when $\\lambda$ is maximized, i.e., at $\\lambda=1$, which yields the optimal constant weight $w_T(x) = \\frac{1}{2}\\exp(-a)$.\nThe question asks for the supremum in terms of $a$ and $\\lambda$. Providing the result for the case where the supremum is finite is the only way to provide a meaningful closed-form expression as requested.",
            "answer": "$$\\boxed{\\frac{\\exp(-a)}{2\\lambda}}$$"
        },
        {
            "introduction": "This practice bridges the gap between the theoretical principles of importance sampling and their practical implementation. You will develop an efficient estimator for a classic rare-event probability problem, a task where naive simulation methods typically fail due to the high number of samples required. By deriving the optimal parameters for an exponential tilting proposal and then implementing both the naive Monte Carlo and the advanced importance sampling estimators in code , you will gain a concrete, computational understanding of the dramatic variance reduction that makes importance sampling an indispensable tool in science and engineering.",
            "id": "3312679",
            "problem": "Consider independent and identically distributed random variables $X_1, X_2, \\dots, X_n$ each having a standard normal distribution. Define the rare-event probability\n$$\np(n,a) \\equiv \\mathbb{P}\\left\\{\\frac{1}{n}\\sum_{i=1}^n X_i \\ge a\\right\\} = \\mathbb{P}\\left\\{\\sum_{i=1}^n X_i \\ge n a\\right\\}.\n$$\nStarting from fundamental definitions of expectation, probability, and the change-of-measure principle via the Radon–Nikodým derivative, derive and implement an estimator for $p(n,a)$ that uses Importance Sampling (IS). The design must be guided only by the following well-tested facts:\n- The moment generating function of a standard normal random variable exists for every real argument $t$, and its cumulant generating function $\\psi(t)$ is finite for all real $t$.\n- The sum of independent normal random variables is normal, with the mean and variance given by the sums of the individual means and variances, respectively.\n- The Monte Carlo method estimates expectations by empirical averages of independent samples.\n\nYou must:\n- Construct a direct Monte Carlo estimator (sometimes called a “crude” estimator) for $p(n,a)$.\n- Construct an Importance Sampling estimator for $p(n,a)$ by an exponential tilting change of measure that makes the rare event typical under the sampling distribution. Your derivation must start from the Radon–Nikodým derivative definition and the cumulant generating function, and must justify the choice of tilting parameter using a first-principles optimality argument grounded in the convexity properties of the cumulant generating function.\n- Implement both estimators efficiently by sampling the sum $\\sum_{i=1}^n X_i$ directly, rather than the $n$-dimensional vector. Explicitly use the fact that if $Y_i$ are independent normal random variables with mean $\\mu$ and variance $1$, then $\\sum_{i=1}^n Y_i$ is normal with mean $n\\mu$ and variance $n$.\n\nNo physical units are involved. All angles, if any appear in your reasoning, must be in radians, but none are required here. The final outputs are floats.\n\nTest Suite:\n- Case $1$: $n=100$, $a=0.5$, Monte Carlo sample size $M=200000$, random seed $s=12345$.\n- Case $2$: $n=100$, $a=0.0$, Monte Carlo sample size $M=100000$, random seed $s=54321$.\n- Case $3$: $n=50$, $a=0.3$, Monte Carlo sample size $M=50000$, random seed $s=2025$.\n- Case $4$: $n=200$, $a=1.0$, Monte Carlo sample size $M=300000$, random seed $s=777$.\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output first the crude Monte Carlo estimate and then the Importance Sampling estimate, in order. That is, the single line must look like\n$$\n[\\widehat{p}_{\\text{crude},1},\\widehat{p}_{\\text{IS},1},\\widehat{p}_{\\text{crude},2},\\widehat{p}_{\\text{IS},2},\\widehat{p}_{\\text{crude},3},\\widehat{p}_{\\text{IS},3},\\widehat{p}_{\\text{crude},4},\\widehat{p}_{\\text{IS},4}],\n$$\nwhere each $\\widehat{p}$ is a float.",
            "solution": "The problem is subjected to validation and is determined to be a valid, well-posed problem in the domain of computational statistics and Monte Carlo methods. It is scientifically grounded, self-contained, and objective. We may therefore proceed with a full derivation and solution.\n\nThe quantity to be estimated is the rare-event probability\n$$\np(n,a) = \\mathbb{P}\\left\\{\\frac{1}{n}\\sum_{i=1}^n X_i \\ge a\\right\\}\n$$\nwhere $X_1, X_2, \\dots, X_n$ are independent and identically distributed (i.i.d.) standard normal random variables, $X_i \\sim \\mathcal{N}(0, 1)$. Let $S_n = \\sum_{i=1}^n X_i$. From the properties of the normal distribution, the sum of independent normal random variables is also normal. The mean of the sum is the sum of the means, $\\mathbb{E}[S_n] = \\sum_{i=1}^n \\mathbb{E}[X_i] = \\sum_{i=1}^n 0 = 0$. The variance of the sum is the sum of the variances, $\\text{Var}(S_n) = \\sum_{i=1}^n \\text{Var}(X_i) = \\sum_{i=1}^n 1 = n$. Thus, $S_n \\sim \\mathcal{N}(0, n)$.\n\nThe probability can be rewritten in terms of $S_n$ as $p(n,a) = \\mathbb{P}\\{S_n \\ge na\\}$. This probability can be expressed as an expectation of an indicator function, which forms the basis for Monte Carlo estimation:\n$$\np(n,a) = \\mathbb{E}\\left[\\mathbb{I}\\{S_n \\ge na\\}\\right]\n$$\nwhere $\\mathbb{I}\\{\\cdot\\}$ is the indicator function, which equals $1$ if the condition is true and $0$ otherwise. The expectation is taken with respect to the probability distribution of $S_n$, which has the probability density function (PDF) $f_{S_n}(s) = \\frac{1}{\\sqrt{2\\pi n}} \\exp\\left(-\\frac{s^2}{2n}\\right)$.\n\n**Crude Monte Carlo Estimator**\n\nThe crude Monte Carlo method directly applies the law of large numbers to estimate the expectation. We generate $M$ independent samples of the random variable $S_n$, denoted $S_n^{(1)}, S_n^{(2)}, \\dots, S_n^{(M)}$, from its distribution $\\mathcal{N}(0, n)$. The estimator for $p(n,a)$ is then the sample mean of the indicator function evaluated on these samples:\n$$\n\\widehat{p}_{\\text{crude}}(n,a) = \\frac{1}{M}\\sum_{j=1}^M \\mathbb{I}\\left\\{S_n^{(j)} \\ge na\\right\\}\n$$\nThis estimator is unbiased, i.e., $\\mathbb{E}[\\widehat{p}_{\\text{crude}}(n,a)] = p(n,a)$. However, if the event $\\{S_n \\ge na\\}$ is rare, most samples $S_n^{(j)}$ will result in $\\mathbb{I}\\{S_n^{(j)} \\ge na\\} = 0$, leading to a high variance and inefficient estimation.\n\n**Importance Sampling (IS) Estimator**\n\nThe core idea of Importance Sampling is to change the probability measure to one where the rare event becomes more frequent. We introduce a new sampling distribution with PDF $g(s)$ and rewrite the expectation:\n$$\np(n,a) = \\int_{-\\infty}^\\infty \\mathbb{I}\\{s \\ge na\\} f_{S_n}(s) ds = \\int_{-\\infty}^\\infty \\mathbb{I}\\{s \\ge na\\} \\frac{f_{S_n}(s)}{g(s)} g(s) ds = \\mathbb{E}_g\\left[\\mathbb{I}\\{Y \\ge na\\} L(Y)\\right]\n$$\nHere, $Y$ is a random variable with PDF $g(s)$, and $L(s) = \\frac{f_{S_n}(s)}{g(s)}$ is the likelihood ratio, which is the Radon–Nikodým derivative of the original measure with respect to the new measure.\n\nThe IS estimator is then formed by drawing $M$ samples $Y^{(1)}, \\dots, Y^{(M)}$ from the new distribution $g(s)$ and computing the sample mean:\n$$\n\\widehat{p}_{\\text{IS}}(n,a) = \\frac{1}{M}\\sum_{j=1}^M \\mathbb{I}\\left\\{Y^{(j)} \\ge na\\right\\} L(Y^{(j)})\n$$\nThis estimator is also unbiased for any $g(s)$ that has support covering the integration domain where the integrand is non-zero.\n\n**Exponential Tilting and Optimal Parameter Selection**\n\nA powerful choice for the new distribution $g(s)$ is an exponential tilting of the original distribution $f_{S_n}(s)$. This is defined using a tilting parameter $\\theta$:\n$$\ng_\\theta(s) = f_{S_n}(s) e^{\\theta s - \\Psi_{S_n}(\\theta)}\n$$\nwhere $\\Psi_{S_n}(\\theta) = \\ln \\mathbb{E}[e^{\\theta S_n}]$ is the cumulant generating function (CGF) of $S_n$. The term $e^{-\\Psi_{S_n}(\\theta)}$ is the normalization constant. As $X_i$ are i.i.d., $\\Psi_{S_n}(\\theta) = n \\psi(\\theta)$, where $\\psi(\\theta)$ is the CGF of a single $X_i \\sim \\mathcal{N}(0, 1)$. For a standard normal variable, the moment generating function is $M_X(t) = e^{t^2/2}$, so the CGF is $\\psi(t) = \\ln(M_X(t)) = t^2/2$. Consequently, the CGF for $S_n$ is $\\Psi_{S_n}(\\theta) = n\\theta^2/2$.\n\nSubstituting the PDFs and the CGF into the definition of $g_\\theta(s)$:\n$$\ng_\\theta(s) = \\left[\\frac{1}{\\sqrt{2\\pi n}} e^{-s^2/(2n)}\\right] e^{\\theta s - n\\theta^2/2} = \\frac{1}{\\sqrt{2\\pi n}} \\exp\\left(-\\frac{s^2 - 2n\\theta s + n^2\\theta^2}{2n}\\right) = \\frac{1}{\\sqrt{2\\pi n}} \\exp\\left(-\\frac{(s - n\\theta)^2}{2n}\\right)\n$$\nThis is the PDF of a normal distribution with mean $n\\theta$ and variance $n$, i.e., $Y \\sim \\mathcal{N}(n\\theta, n)$. The likelihood ratio is $L(s) = \\frac{f_{S_n}(s)}{g_\\theta(s)} = e^{-\\theta s + \\Psi_{S_n}(\\theta)} = e^{-\\theta s + n\\theta^2/2}$.\n\nThe optimal choice of $\\theta$ is one that minimizes the variance of the IS estimator. The variance of the estimator is determined by the second moment of the random variable $Z = \\mathbb{I}\\{Y \\ge na\\}L(Y)$. An effective heuristic, which is asymptotically optimal in the context of large deviations, is to choose $\\theta$ such that the mean of the new distribution is centered at the \"most important\" region of the event. For the one-sided event $\\{S_n \\ge na\\}$, this corresponds to centering the new distribution at the boundary of the event region.\n\nWe set the mean of the sampling distribution equal to the threshold $na$:\n$$\n\\mathbb{E}_\\theta[Y] = na\n$$\nA fundamental property of exponential families is that the mean of the tilted distribution is the derivative of the CGF: $\\mathbb{E}_\\theta[Y] = \\Psi'_{S_n}(\\theta)$. Thus, we solve for $\\theta$ in the equation:\n$$\n\\Psi'_{S_n}(\\theta) = na\n$$\nGiven $\\Psi_{S_n}(\\theta) = n\\theta^2/2$, its derivative is $\\Psi'_{S_n}(\\theta) = n\\theta$. The equation becomes:\n$$\nn\\theta = na \\implies \\theta = a\n$$\nThe CGF $\\Psi_{S_n}(\\theta)$ is strictly convex because its second derivative $\\Psi''_{S_n}(\\theta) = n  0$. This strict convexity guarantees that the equation $\\Psi'_{S_n}(\\theta) = na$ has a unique solution for $\\theta$.\n\nWith the optimal parameter $\\theta = a$, our IS procedure is as follows:\n1.  Sample $Y^{(j)}$ from the distribution $\\mathcal{N}(na, n)$.\n2.  Calculate the likelihood ratio for each sample: $L(Y^{(j)}) = \\exp\\left(-a Y^{(j)} + na^2/2\\right)$.\n3.  Compute the estimate:\n    $$\n    \\widehat{p}_{\\text{IS}}(n,a) = \\frac{1}{M}\\sum_{j=1}^M \\mathbb{I}\\left\\{Y^{(j)} \\ge na\\right\\} e^{-a Y^{(j)} + na^2/2}\n    $$\nBy centering the sampling distribution at the event boundary, we ensure that a substantial fraction of samples (approximately $50\\%$) will fall into the event region $\\{Y \\ge na\\}$, which drastically reduces the variance of the estimator compared to the crude method, especially when $p(n,a)$ is very small.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements crude Monte Carlo and Importance Sampling estimators\n    for a rare-event probability involving a sum of standard normal variables.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, a, M, random_seed)\n        (100, 0.5, 200000, 12345),\n        (100, 0.0, 100000, 54321),\n        (50, 0.3, 50000, 2025),\n        (200, 1.0, 300000, 777),\n    ]\n\n    results = []\n    for n, a, M, s in test_cases:\n        # Initialize a random number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(s)\n\n        # The target event is sum(X_i) = n*a.\n        threshold = n * a\n\n        # The sum S_n = sum(X_i) is distributed as Normal(0, n).\n        mean_original = 0.0\n        # The standard deviation is sqrt(n).\n        std_dev = np.sqrt(n)\n\n        # --- Crude Monte Carlo Estimator ---\n        # Generate M samples from the original distribution N(0, n).\n        samples_crude = rng.normal(loc=mean_original, scale=std_dev, size=M)\n        \n        # The estimate is the fraction of samples that fall into the event region.\n        p_crude = np.mean(samples_crude = threshold)\n        results.append(p_crude)\n\n        # --- Importance Sampling (IS) Estimator ---\n        # The optimal tilting parameter is theta = a.\n        theta = a\n\n        # The mean of the IS sampling distribution N(n*theta, n).\n        mean_is = n * theta\n        # The standard deviation is the same, sqrt(n).\n        \n        # Generate M samples from the tilted (IS) distribution.\n        samples_is = rng.normal(loc=mean_is, scale=std_dev, size=M)\n        \n        # Calculate the likelihood ratio for each sample.\n        # L(s) = exp(-theta*s + n*psi(theta)), where psi(theta) = theta^2/2 for N(0,1).\n        # CGF of the sum is n*psi(theta).\n        log_likelihood_ratios = -theta * samples_is + n * theta**2 / 2.0\n        likelihood_ratios = np.exp(log_likelihood_ratios)\n\n        # The IS estimate is the average of the indicator function times the likelihood ratio.\n        # The indicator function is 1 if sample = threshold, and 0 otherwise.\n        indicators = (samples_is = threshold)\n        p_is = np.mean(indicators * likelihood_ratios)\n        results.append(p_is)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}