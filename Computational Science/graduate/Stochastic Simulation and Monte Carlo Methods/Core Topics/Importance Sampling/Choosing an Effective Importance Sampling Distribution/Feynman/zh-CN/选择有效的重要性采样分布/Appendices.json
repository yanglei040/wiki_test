{
    "hands_on_practices": [
        {
            "introduction": "重要性采样中最灾难性的失败是得到一个方差无穷的估计量。这种情况通常悄无声息地发生，导致结果完全不可靠。这个问题通常源于“尾部不匹配”（tail mismatch），即提议分布的尾部相对于目标分布而言过轻。本练习将指导你诊断这一关键问题。通过分析重要性权重方差的渐近行为，你将学到选择提议分布以保证方差有限的基本法则，这是设计有效提议分布不可妥协的第一步。",
            "id": "3295486",
            "problem": "考虑使用重要性采样来估计在目标密度 $p$（定义于 $\\mathbb{R}$ 上）下有界可测函数 $g$ 的期望 $I = \\mathbb{E}_{p}[g(X)]$。假设存在 $x_{0} > 0$ 和一个常数 $c_{p} > 0$，使得 $p$ 具有对称的帕累托型尾部：\n$$\np(x) \\sim c_{p} \\lvert x \\rvert^{-(\\alpha+1)} \\quad \\text{当 } \\lvert x \\rvert \\to \\infty, \\quad \\alpha > 0.\n$$\n假设 $g$ 是有界的，即存在 $M < \\infty$ 使得对所有 $x$ 都有 $\\lvert g(x) \\rvert \\le M$。您的任务是选择一个有效的重要性采样提议分布 $q$，以避免基于重要性权重 $w(x) = p(x)/q(x)$ 的重要性采样估计量出现无穷方差。\n\n正在考虑两种提议分布族：\n\n1. 高斯提议分布 $q_{G}$，其密度为 $q_{G}(x) \\propto \\exp(-x^{2}/2)$。\n\n2. 自由度为 $\\nu > 0$ 的学生 $t_{\\nu}$ 提议分布 $q_{T,\\nu}$，其连续密度 $q_{T,\\nu}(x)$ 是对称的并且满足\n$$\nq_{T,\\nu}(x) \\sim c_{q} \\lvert x \\rvert^{-(\\nu+1)} \\quad \\text{当 } \\lvert x \\rvert \\to \\infty,\n$$\n对于某个常数 $c_{q} > 0$（即通常的学生t分布的尾部行为；您可以假设存在一个固定的尺度参数，其值不影响尾部指数）。\n\n您还可以使用从外部尾部数据构建的尾部指数 $\\alpha$ 的一个一致估计量 $\\widehat{\\alpha}_{n}$（例如，通过对一个独立的重尾样本使用Hill估计量），并且您可以将 $\\nu$ 选择为 $\\widehat{\\alpha}_{n}$ 和一个小的安全边际 $\\epsilon \\in (0,1)$ 的函数。\n\n请仅使用重要性采样和方差的基本定义，以及标准的渐近尾部比较，判断以下哪个陈述对尾部不匹配问题给出了正确的诊断，并为选择 $\\nu$ 以确保重要性采样估计量的方差有限提供了正确的规则：\n\nA. 高斯提议分布 $q_{G}$ 就足够了，因为当 $g$ 有界时，中心极限定理能确保方差有限。\n\nB. 使用学生 $t_{\\nu}$ 提议分布，并对一个固定的很小的 $\\epsilon > 0$ 选择任意 $\\nu \\in (0, 2\\widehat{\\alpha}_{n} - \\epsilon)$；这使得 $q$ 的尾部比所要求的更重，并保证 $\\mathbb{E}_{q}[(p(X)/q(X))^{2}] < \\infty$，因此当 $g$ 有界时，估计量的方差有限。\n\nC. 使用学生 $t_{\\nu}$ 提议分布，并取 $\\nu = \\widehat{\\alpha}_{n}$ 来匹配目标分布的尾部指数；这个选择是确保 $\\mathbb{E}_{q}[(p(X)/q(X))^{2}] < \\infty$ 的充分必要条件。\n\nD. 使用学生 $t_{\\nu}$ 提议分布，并取 $\\nu > 2\\widehat{\\alpha}_{n}$；使 $q$ 的尾部比 $p$ 更轻可以避免过大的权重并保证方差有限。\n\nE. 使用学生 $t_{\\nu}$ 提议分布，并取 $\\nu \\le 2\\widehat{\\alpha}_{n}$；等式 $\\nu = 2\\widehat{\\alpha}_{n}$ 是可接受的，并且仍然能得到重要性采样估计量的有限方差。\n\n选择正确的选项，并精确说明基于尾部指数 $\\alpha$ 和 $\\nu$ 的潜在方差有限性条件。",
            "solution": "我们从重要性采样估计量的定义开始。如果 $X \\sim q$，则 $I = \\mathbb{E}_{p}[g(X)]$ 的基本估计量为 $\\widehat{I} = \\frac{1}{m} \\sum_{i=1}^{m} w(X_{i}) g(X_{i})$，其中 $w(x) = p(x)/q(x)$。其在 $q$ 分布下的方差为\n$$\n\\operatorname{Var}_{q}(\\widehat{I}) = \\frac{1}{m} \\left\\{ \\mathbb{E}_{q}\\big[(w(X) g(X))^{2}\\big] - I^{2} \\right\\}.\n$$\n由于 $g$ 受 $M$ 所界定，我们有\n$$\n\\mathbb{E}_{q}\\big[(w(X) g(X))^{2}\\big] \\le M^{2} \\, \\mathbb{E}_{q}\\big[ w(X)^{2} \\big].\n$$\n因此，方差有限的一个充分条件是\n$$\n\\mathbb{E}_{q}\\big[ w(X)^{2} \\big] = \\int_{\\mathbb{R}} \\frac{p(x)^{2}}{q(x)} \\, \\mathrm{d}x < \\infty.\n$$\n因此，核心的可积性问题是 $\\int p(x)^{2}/q(x) \\, \\mathrm{d}x$ 是否有限。\n\n我们现在通过渐近比较来诊断尾部不匹配问题。在尾部，我们有 $p(x) \\sim c_{p} \\lvert x \\rvert^{-(\\alpha+1)}$。对于学生 $t_{\\nu}$ 提议分布，$q_{T,\\nu}(x) \\sim c_{q} \\lvert x \\rvert^{-(\\nu+1)}$。在远尾部，被积函数的行为如下\n$$\n\\frac{p(x)^{2}}{q_{T,\\nu}(x)} \\sim C \\, \\lvert x \\rvert^{-\\left(2(\\alpha+1)\\right)} \\, \\lvert x \\rvert^{\\nu+1} = C \\, \\lvert x \\rvert^{-\\left(2\\alpha + 2 - \\nu - 1\\right)} = C \\, \\lvert x \\rvert^{-\\left(2\\alpha + 1 - \\nu\\right)},\n$$\n对于某个常数 $C > 0$。在一维情况下，积分 $\\int_{R}^{\\infty} x^{-k} \\, \\mathrm{d}x$ 收敛当且仅当 $k > 1$。因此，我们要求尾部指数满足\n$$\n2\\alpha + 1 - \\nu > 1 \\quad \\Longleftrightarrow \\quad 2\\alpha - \\nu > 0 \\quad \\Longleftrightarrow \\quad \\nu < 2\\alpha.\n$$\n因此，为了使 $\\mathbb{E}_{q}[w^{2}]$ 有限（从而在 $g$ 有界时使估计量的方差有限），学生t分布的自由度必须满足严格不等式 $\\nu < 2\\alpha$。边界情况 $\\nu = 2\\alpha$ 产生的尾部指数等于 $1$，这会导致一个对数发散的积分；因此它不能得到有限方差。\n\n接下来，考虑密度为 $q_{G}(x) \\propto \\exp(-x^{2}/2)$ 的高斯提议分布。在尾部，\n$$\n\\frac{p(x)^{2}}{q_{G}(x)} \\asymp \\lvert x \\rvert^{-2(\\alpha+1)} \\, \\exp\\!\\left(\\frac{x^{2}}{2}\\right),\n$$\n其积分显然是发散的，因为高斯分布的分母呈指数衰减，使得 $1/q_{G}(x)$ 呈超多项式增长，而 $p(x)^{2}$ 的多项式衰减无法抵消这种指数增长。因此，$\\mathbb{E}_{q_{G}}[w^{2}] = \\infty$，并且在高斯提议分布 $q_{G}$ 下，重要性采样的方差是无穷大的。\n\n我们现在评估这些选项：\n\nA. 这个选项声称由于中心极限定理，高斯提议分布就足够了。这是不正确的。中心极限定理并不保证重要性采样估计量的方差有限；事实上，我们明确计算出 $\\int p^{2}/q_{G}$ 是发散的，因此即使 $g$ 有界，$\\mathbb{E}_{q_{G}}[w^{2}] = \\infty$。结论：不正确。\n\nB. 这个选项建议使用学生 $t_{\\nu}$ 分布，并对一个小的 $\\epsilon > 0$ 选择任意 $\\nu \\in (0, 2\\widehat{\\alpha}_{n} - \\epsilon)$。由于 $\\widehat{\\alpha}_{n}$ 是 $\\alpha$ 的一致估计量且 $\\epsilon > 0$ 是固定的，对于大的 $n$，这能以高概率确保 $\\nu < 2\\alpha$，这正是使 $\\mathbb{E}_{q}[w^{2}] < \\infty$ 所需的严格条件。此外，选择比阈值更小的 $\\nu$（即更重的尾部）是一种保守的做法，有助于避免因估计误差而导致的无穷方差。结论：正确。\n\nC. 这个选项设定 $\\nu = \\widehat{\\alpha}_{n}$ 并断言这是充分且必要的。条件 $\\nu < 2\\alpha$ 并不要求 $\\nu = \\alpha$；许多值都满足它。虽然 $\\nu = \\alpha$（如果确实等于真实的 $\\alpha$）是充分的，但它不是必要的。因此，必要性的声称是错误的。结论：不正确。\n\nD. 这个选项建议 $\\nu > 2\\widehat{\\alpha}_{n}$，这使得提议分布的尾部比阈值更轻，并且渐近地看，比所要求的更轻。这违反了严格条件 $\\nu < 2\\alpha$，并导致 $\\mathbb{E}_{q}[w^{2}] = \\infty$。结论：不正确。\n\nE. 这个选项允许 $\\nu \\le 2\\widehat{\\alpha}_{n}$ 并声称等式是可接受的。边界情况 $\\nu = 2\\alpha$ 产生的尾部指数等于 $1$，这会导致 $\\int p^{2}/q$ 的对数发散。因此，需要严格不等式；等式是不可接受的。结论：不正确。\n\n因此，正确的选择是使用学生 $t_{\\nu}$ 提议分布，其 $\\nu$ 严格小于 $2\\alpha$。当使用估计量 $\\widehat{\\alpha}_{n}$ 时，一个实用且有原则的规则是选择比 $2\\widehat{\\alpha}_{n}$ 小一个固定安全边际 $\\epsilon > 0$ 的 $\\nu$ 值，以高概率地保持严格不等式，从而避免重要性采样估计量的方差为无穷大。",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "在确保方差有限之后，我们更希望选择一个能最小化方差的提议分布。现有几种原则性的标准，但它们可能导出截然不同的提议分布，各有优劣。本练习对比了两种最重要的设计哲学：最小化Kullback-Leibler (KL)散度与最小化$\\chi^2$散度。你将探索KL散度最小化如何体现为一种“质量覆盖”（mass-covering）策略，而$\\chi^2$散度最小化则是一种“模式寻求”（mode-seeking）策略。通过分析重尾和多峰目标这两个典型场景，你将为在实际提议设计中何时使用何种标准以及如何权衡关键利弊建立更深刻的直觉。",
            "id": "3295509",
            "problem": "考虑通过重要性采样估计标量 $\\mu = \\mathbb{E}_p[f(X)]$，其中 $X \\in \\mathbb{R}$，$p$ 是一个已知的采样分布，$f$ 是一个可测可积函数。使用提议分布 $q$ 的重要性采样估计器的权重为 $w(X) = \\frac{f(X) p(X)}{q(X)}$。定义非负函数 $h(x) = |f(x)| p(x)$ 及其归一化常数 $Z = \\int_{\\mathbb{R}} h(x) \\, dx$，并令 $g(x) = \\frac{h(x)}{Z}$ 是与 $h$ 成正比的概率密度。选择提议分布 $q$ 的两个常见设计原则是：\n- 最小化前向Kullback–Leibler散度 (KL)，$\\mathrm{KL}(g \\| q) = \\int_{\\mathbb{R}} g(x) \\log\\left( \\frac{g(x)}{q(x)} \\right) dx$；\n- 最小化Pearson卡方散度，$\\chi^2(g \\| q) = \\int_{\\mathbb{R}} \\frac{(g(x) - q(x))^2}{q(x)} dx = \\int_{\\mathbb{R}} \\frac{g(x)^2}{q(x)} dx - 1$。\n\n以下事实将作为基本依据：\n- 在 $q$ 分布下，重要性权重的方差与卡方散度相关：$\\operatorname{Var}_q[w(X)] = Z^2 \\chi^2(g \\| q)$，前提是 $\\int_{\\mathbb{R}} \\frac{g(x)^2}{q(x)} dx < \\infty$。\n\n您将在两个在尾部行为和模态上有所不同的科学现实场景中比较这些设计原则，然后评估给出的陈述。\n\n场景 I（在轻尾族内，重尾导致前向KL失效）：\n- 令 $p$ 为自由度为 $\\nu_g = 3$、位置为 $0$、尺度为 $1$ 的学生t分布，记为 $t_3(0,1)$。令 $f(x) \\equiv 1$，因此 $h(x) = p(x)$ 且 $g(x) = p(x)$ 是 $t_3$ 密度。\n- 考虑通过在 $\\mathcal{Q}_\\mathrm{G}$ 上最小化 $\\mathrm{KL}(g \\| q)$，从高斯族 $\\mathcal{Q}_\\mathrm{G} = \\{ \\mathcal{N}(0, s^2) : s > 0 \\}$ 中选择 $q$。\n- 同时考虑通过在 $\\mathcal{Q}_\\mathrm{T}$ 上最小化 $\\chi^2(g \\| q)$，从学生t分布族 $\\mathcal{Q}_\\mathrm{T} = \\{ t_\\nu(0,1) : \\nu > 2 \\}$ 中选择 $q$。\n\n场景 II（罕见的次要模式，其中在单峰族上的卡方散度最小化会集中在主导模式上，而对次要尾部的覆盖不足）：\n- 令 $p$ 为高斯混合 $p(x) = (1-\\epsilon) \\, \\phi(x; 0, 1) + \\epsilon \\, \\phi(x; M, 1)$，其中 $0 < \\epsilon \\ll 1$，$M \\gg 1$，且 $\\phi(x; m, s^2)$ 表示 $\\mathcal{N}(m, s^2)$ 密度。令 $f(x) \\equiv 1$，因此 $g(x) = p(x)$。\n- 考虑通过最小化 $\\mathrm{KL}(g \\| q)$ 或最小化 $\\chi^2(g \\| q)$，从单高斯族 $\\mathcal{Q}_\\mathrm{G}^\\mathrm{one} = \\{ \\mathcal{N}(m, s^2) : m \\in \\mathbb{R}, s^2 > 0 \\}$ 中选择 $q$。\n\n以下哪个陈述是正确的？选择所有适用的选项。\n\nA. 在场景 I 中，在 $\\mathcal{Q}_\\mathrm{G}$ 上最小化 $\\mathrm{KL}(g \\| q)$ 得到 $q = \\mathcal{N}(0, s^2)$ 且 $s^2 = \\mathbb{E}_g[X^2] = 3$，但 $\\int_{\\mathbb{R}} \\frac{g(x)^2}{q(x)} dx = \\infty$。相反，在 $\\mathcal{Q}_\\mathrm{T}$ 上最小化 $\\chi^2(g \\| q)$ 可以选择一个重尾的 $q$（例如，$q = t_3(0,1)$），此时 $\\int_{\\mathbb{R}} \\frac{g(x)^2}{q(x)} dx < \\infty$，从而控制了权重爆炸。\n\nB. 在场景 II 中，在 $\\mathcal{Q}_\\mathrm{G}^\\mathrm{one}$ 上最小化 $\\chi^2(g \\| q)$ 会将 $q$ 的中心置于 $x = M$ 附近，并具有较大的方差，以便 $q$ 覆盖两个模式，通常比通过最小化 $\\mathrm{KL}(g \\| q)$ 得到的 $q$ 产生更小的重要性权重方差。\n\nC. 因为 $\\operatorname{Var}_q[w(X)] = Z^2 \\chi^2(g \\| q)$，最小化 $\\mathrm{KL}(g \\| q)$ 总是能在所有 $q$ 中得到最小的重要性权重方差。\n\nD. 在场景 II 中，在 $\\mathcal{Q}_\\mathrm{G}^\\mathrm{one}$ 上最小化 $\\mathrm{KL}(g \\| q)$ 得到 $m^\\star = \\mathbb{E}_g[X] = \\epsilon M$ 和 $s^{2\\star} = \\operatorname{Var}_g(X) = 1 + \\epsilon(1-\\epsilon) M^2$，这会将质量分配到两个模式上，并且相对于集中在主导模式附近并严重覆盖不足罕见模式的 $\\chi^2$ 最小化器，它极大地减少了由尾部引起的 $\\int_{\\mathbb{R}} \\frac{g(x)^2}{q(x)} dx$ 的爆炸。\n\nE. 对于任何具有多项式尾部衰减的密度 $g$ 和任何高斯提议分布 $q$，积分 $\\int_{\\mathbb{R}} \\frac{g(x)^2}{q(x)} dx$ 都是有限的，因此当 $q$ 是高斯分布时不会发生权重爆炸。\n\n你的答案必须是 $\\{ \\text{A}, \\text{B}, \\text{C}, \\text{D}, \\text{E} \\}$ 的子集，且至少选择一个选项。",
            "solution": "问题要求评估关于为重要性采样选择提议分布 $q$ 的几个陈述，这些陈述基于两个设计原则：最小化前向Kullback-Leibler (KL) 散度 $\\mathrm{KL}(g \\| q)$ 和最小化Pearson卡方散度 $\\chi^2(g \\| q)$。此处，$g(x) = \\frac{|f(x)|p(x)}{Z}$ 是用于选择高效提议分布的归一化目标密度，与重要性权重的方差有关。在所给的场景中，$f(x) \\equiv 1$，因此 $g(x)=p(x)$ 且 $Z=1$。提供的关键关系是 $\\operatorname{Var}_q[w(X)] = Z^2 \\chi^2(g \\| q)$，对于 $f(x)=1$ 的情况，该关系简化为 $\\operatorname{Var}_q[w(X)] = \\chi^2(p \\| q) = \\int_{\\mathbb{R}} \\frac{p(x)^2}{q(x)} dx - 1$。因此，最小化重要性权重方差等价于最小化积分 $\\int_{\\mathbb{R}} \\frac{p(x)^2}{q(x)} dx$。\n\n首先，我们验证问题陈述。\n### 第一步：提取已知条件\n- 目标：估计 $\\mu = \\mathbb{E}_p[f(X)]$。\n- 估计器：重要性采样，使用提议分布 $q$ 和权重 $w(X) = \\frac{f(X) p(X)}{q(X)}$。\n- 定义：$h(x) = |f(x)| p(x)$，$Z = \\int_{\\mathbb{R}} h(x) \\, dx$，$g(x) = \\frac{h(x)}{Z}$。\n- 散度：$\\mathrm{KL}(g \\| q) = \\int_{\\mathbb{R}} g(x) \\log\\left( \\frac{g(x)}{q(x)} \\right) dx$ 和 $\\chi^2(g \\| q) = \\int_{\\mathbb{R}} \\frac{g(x)^2}{q(x)} dx - 1$。\n- 基本事实：$\\operatorname{Var}_q[w(X)] = Z^2 \\chi^2(g \\| q)$，前提是积分有限。\n- **场景 I：** $p(x) = t_3(0,1)$（自由度为 $\\nu_g=3$ 的学生t分布），$f(x)=1$。因此 $g(x)=p(x)$。\n  - 提议分布族：$\\mathcal{Q}_\\mathrm{G} = \\{ \\mathcal{N}(0, s^2) : s > 0 \\}$ 用于最小化 $\\mathrm{KL}(g \\| q)$。\n  - 提议分布族：$\\mathcal{Q}_\\mathrm{T} = \\{ t_\\nu(0,1) : \\nu > 2 \\}$ 用于最小化 $\\chi^2(g \\| q)$。\n- **场景 II：** $p(x) = (1-\\epsilon) \\, \\phi(x; 0, 1) + \\epsilon \\, \\phi(x; M, 1)$，其中 $0 < \\epsilon \\ll 1$，$M \\gg 1$，且 $f(x)=1$。因此 $g(x)=p(x)$。\n  - 提议分布族：$\\mathcal{Q}_\\mathrm{G}^\\mathrm{one} = \\{ \\mathcal{N}(m, s^2) : m \\in \\mathbb{R}, s^2 > 0 \\}$ 用于最小化 $\\mathrm{KL}(g \\| q)$ 或 $\\chi^2(g \\| q)$。\n\n### 第二步：使用提取的已知条件进行验证\n该问题在科学上植根于蒙特卡洛方法的理论。KL散度和卡方散度的概念是标准的，它们在优化重要性采样提议分布中的应用是一个被深入研究的课题。所提出的场景是用于说明这些散度不同行为的典型例子。方差和卡方散度之间的关系在给定场景下是正确的。问题陈述清晰、客观，并包含进行严谨分析所需的足够信息。未检测到任何缺陷。\n\n### 第三步：结论与行动\n问题是有效的。我们继续进行解答和逐项分析。\n\n### 散度最小化原则\n- **最小化 $\\mathrm{KL}(g \\| q)$：** 当 $q$ 属于指数族（如高斯族）时，最小化 $\\mathrm{KL}(g \\| q)$ 等价于匹配期望充分统计量。对于高斯提议分布 $q(x) = \\mathcal{N}(m, s^2)$，这意味着将 $q$ 的均值和方差设置为等于 $g$ 的均值和方差。这是一种“矩匹配”属性。该散度惩罚 $q$ 在 $g$ 为零处非零的情况，但对 $q$ 在 $g$ 非零处值很小的情况不太敏感，这通常导致宽泛的、“质量覆盖”的提议分布，它会对目标的多个模式进行平均。\n- **最小化 $\\chi^2(g \\| q)$：** 这等价于最小化 $\\int \\frac{g(x)^2}{q(x)} dx$。该目标会严重惩罚 $q(x)$ 很小但 $g(x)$ 不小的情况。这迫使 $q(x)$ 在 $g(x)$ 大的任何地方都必须大，从而导致“模式寻求”行为。它还鼓励 $q$ 的尾部至少与 $g$ 一样重，因为在重尾的 $g$ 下使用轻尾的 $q$ 会导致积分发散。这与控制重要性权重的方差直接相关。\n\n### 逐项分析\n\n**A. 在场景 I 中，在 $\\mathcal{Q}_\\mathrm{G}$ 上最小化 $\\mathrm{KL}(g \\| q)$ 得到 $q = \\mathcal{N}(0, s^2)$ 且 $s^2 = \\mathbb{E}_g[X^2] = 3$，但 $\\int_{\\mathbb{R}} \\frac{g(x)^2}{q(x)} dx = \\infty$。相反，在 $\\mathcal{Q}_\\mathrm{T}$ 上最小化 $\\chi^2(g \\| q)$ 可以选择一个重尾的 $q$（例如，$q = t_3(0,1)$），此时 $\\int_{\\mathbb{R}} \\frac{g(x)^2}{q(x)} dx < \\infty$，从而控制了权重爆炸。**\n\n- **KL最小化分析：** 在场景 I 中，$g(x)$ 是自由度为 $\\nu=3$ 的学生t分布的密度，经缩放后方差为 $\\frac{\\nu}{\\nu-2} = \\frac{3}{3-2} = 3$。均值为 $0$。在 高斯族 $\\mathcal{Q}_\\mathrm{G}$ 上最小化 $\\mathrm{KL}(g \\| q)$ 涉及矩匹配。$\\mathcal{Q}_\\mathrm{G}$ 中提议分布的均值已经为 $0$。我们必须匹配二阶矩（方差）。因此，最优的 $q$ 是 $\\mathcal{N}(0, s^2)$，其中 $s^2 = \\mathbb{E}_g[X^2] = 3$。\n- 现在我们来分析积分 $\\int_{\\mathbb{R}} \\frac{g(x)^2}{q(x)} dx$。目标密度 $g(x)=t_3(0,1)$ 具有多项式尾部，对于大的 $|x|$，其行为类似于 $|x|^{-(\\nu+1)} = |x|^{-4}$。因此，$g(x)^2$ 的行为类似于 $|x|^{-8}$。提议分布 $q(x)=\\mathcal{N}(0,3)$ 具有高斯（指数）尾部，$q(x) \\propto e^{-x^2/6}$。被积函数 $\\frac{g(x)^2}{q(x)}$ 在 $|x|$ 很大时表现得像 $|x|^{-8}e^{x^2/6}$。指数增长占主导地位，因此被积函数趋于无穷大，积分发散。这意味着重要性权重的方差为无穷大。\n- **$\\chi^2$最小化分析：** 我们希望对于 $q \\in \\mathcal{Q}_\\mathrm{T} = \\{t_\\nu(0,1) : \\nu > 2\\}$ 最小化 $\\int \\frac{g(x)^2}{q(x)} dx$。目标本身，$g(x)=t_3(0,1)$，是这个族的一个成员（因为 $3>2$）。柯西-施瓦茨不等式意味着 $\\int \\frac{g^2}{q} dx \\ge (\\int g dx)^2 = 1$。最小值 $1$ 当且仅当 $q(x) \\propto g(x)$ 时达到，即 $q(x)=g(x)$。因此，最优选择是 $q(x) = t_3(0,1)$。对于这个选择，$\\int \\frac{g(x)^2}{q(x)} dx = \\int \\frac{g(x)^2}{g(x)} dx = \\int g(x) dx = 1$，这是有限的。这正确地控制了权重方差。\n- **结论：** 该陈述完全正确。**正确**。\n\n**B. 在场景 II 中，在 $\\mathcal{Q}_\\mathrm{G}^\\mathrm{one}$ 上最小化 $\\chi^2(g \\| q)$ 会将 $q$ 的中心置于 $x = M$ 附近，并具有较大的方差，以便 $q$ 覆盖两个模式，通常比通过最小化 $\\mathrm{KL}(g \\| q)$ 得到的 $q$ 产生更小的重要性权重方差。**\n\n- **$\\chi^2$最小化分析：** 在场景 II 中，$g(x) = (1-\\epsilon) \\phi(x; 0, 1) + \\epsilon \\phi(x; M, 1)$，其中 $\\epsilon \\ll 1, M \\gg 1$。$g(x)^2 \\approx (1-\\epsilon)^2 \\phi(x;0,1)^2 + \\epsilon^2 \\phi(x;M,1)^2$ 项，因为交叉项可以忽略不计。由于 $\\epsilon \\ll 1$，第一项 $(1-\\epsilon)^2 \\phi(x;0,1)^2$ 以压倒性优势主导第二项。为了最小化 $\\int g(x)^2/q(x) dx$，提议分布 $q(x)$ 必须在 $g(x)^2$ 大的地方也很大。这意味着 $q(x)$ 将专注于近似位于 $x=0$ 处的主导模式。它将是一个中心在 $x=0$ 附近、方差很小的单峰高斯分布，实际上忽略了位于 $x=M$ 处的罕见模式。该陈述声称 $q$ 的中心在 $x=M$ 附近且方差很大，这与这种“模式寻求”行为相反。这种策略会导致非常大的权重方差，因为 $q(x)$ 在罕见模式处会非常小，导致比率 $g(x)^2/q(x)$ 在那里爆炸。\n- **与KL的比较：** 如在D的分析中所示，KL最小化器将是一个非常宽的高斯分布，覆盖两个模式，从而得到更小的方差。\n- **结论：** 该陈述错误地描述了 $\\chi^2$ 最小化器的行为，并错误地将其性能与KL最小化器进行了比较。**不正确**。\n\n**C. 因为 $\\operatorname{Var}_q[w(X)] = Z^2 \\chi^2(g \\| q)$，最小化 $\\mathrm{KL}(g \\| q)$ 总是能在所有 $q$ 中得到最小的重要性权重方差。**\n\n- **分析：** 方差与 $\\chi^2(g \\| q)$ 直接成正比。因此，为了最小化方差，必须最小化 $\\chi^2$ 散度，而不是KL散度。$\\mathrm{KL}(g \\| q)$ 和 $\\chi^2(g \\| q)$ 的最小化器通常是不同的。场景 I 提供了一个直接的反例：最小化KL散度导致一个方差无穷大的提议分布，而最小化 $\\chi^2$ 散度（在一个合适的族内）则得到一个方差有限的提议分布。前提是正确的，但结论是不合逻辑的推论。\n- **结论：** 该陈述包含一个基本的逻辑错误。**不正确**。\n\n**D. 在场景 II 中，在 $\\mathcal{Q}_\\mathrm{G}^\\mathrm{one}$ 上最小化 $\\mathrm{KL}(g \\| q)$ 得到 $m^\\star = \\mathbb{E}_g[X] = \\epsilon M$ 和 $s^{2\\star} = \\operatorname{Var}_g(X) = 1 + \\epsilon(1-\\epsilon) M^2$，这会将质量分配到两个模式上，并且相对于集中在主导模式附近并严重覆盖不足罕见模式的 $\\chi^2$ 最小化器，它极大地减少了由尾部引起的 $\\int_{\\mathbb{R}} \\frac{g(x)^2}{q(x)} dx$ 的爆炸。**\n\n- **KL最小化分析：** 如前所述，在一个高斯族上最小化 $\\mathrm{KL}(g \\| q)$ 对应于匹配前两个矩。\n  - 均值：$\\mathbb{E}_g[X] = (1-\\epsilon)\\mathbb{E}[\\mathcal{N}(0,1)] + \\epsilon\\mathbb{E}[\\mathcal{N}(M,1)] = (1-\\epsilon) \\cdot 0 + \\epsilon \\cdot M = \\epsilon M$。所以 $m^\\star = \\epsilon M$。\n  - 二阶矩：$\\mathbb{E}_g[X^2] = (1-\\epsilon)\\mathbb{E}[X^2]_{\\mathcal{N}(0,1)} + \\epsilon\\mathbb{E}[X^2]_{\\mathcal{N}(M,1)} = (1-\\epsilon)(1^2+0^2) + \\epsilon(1^2+M^2) = 1-\\epsilon + \\epsilon + \\epsilon M^2 = 1 + \\epsilon M^2$。\n  - 方差：$\\operatorname{Var}_g(X) = \\mathbb{E}_g[X^2] - (\\mathbb{E}_g[X])^2 = (1+\\epsilon M^2) - (\\epsilon M)^2 = 1+\\epsilon M^2 - \\epsilon^2 M^2 = 1 + \\epsilon(1-\\epsilon)M^2$。所以 $s^{2\\star} = 1 + \\epsilon(1-\\epsilon)M^2$。\n- 由此产生的提议分布 $q(x) = \\mathcal{N}(\\epsilon M, 1+\\epsilon(1-\\epsilon)M^2)$ 具有非常大的方差（因为 $M \\gg 1$），使其成为一个覆盖了 $0$ 处模式和 $M$ 处模式的宽分布。这是前向KL的“质量覆盖”属性。\n- **与$\\chi^2$最小化器的比较：** 如选项B的分析所述，$\\chi^2$ 最小化器将是一个窄的高斯分布，集中在 $x=0$ 处的主模式上，严重欠采样 $x=M$ 处的罕见模式。这导致了 $\\int g(x)^2/q(x) dx$ 项的爆炸，从而产生非常大的方差。通过KL推导出的提议分布，由于其分布较宽，使得比率 $g(x)/q(x)$ 在 $g(x)$ 的支撑域上更加稳定，从而导致方差小得多。\n- **结论：** 该陈述准确地描述了KL最小化器，并正确地将其在此场景下相对于 $\\chi^2$ 最小化器的优越性能进行了对比。**正确**。\n\n**E. 对于任何具有多项式尾部衰减的密度 $g$ 和任何高斯提议分布 $q$，积分 $\\int_{\\mathbb{R}} \\frac{g(x)^2}{q(x)} dx$ 都是有限的，因此当 $q$ 是高斯分布时不会发生权重爆炸。**\n\n- **分析：** 这是一个很强的一般性论断。一个具有多项式尾部衰减的密度 $g$ 满足 $g(x) \\approx C|x|^{-k}$，对于某个常数 $C$ 和幂 $k>1$（当 $|x|$ 很大时）。其平方的衰减为 $g(x)^2 \\approx C^2|x|^{-2k}$。一个高斯提议分布 $q$ 具有指数尾部衰减，$q(x) \\approx D e^{-ax^2}$，对于某些常数 $D, a > 0$。被积函数中的比率是 $\\frac{g(x)^2}{q(x)} \\approx \\frac{C^2}{D}|x|^{-2k}e^{ax^2}$。当 $|x| \\to \\infty$ 时，指数项 $e^{ax^2}$ 的增长速度远快于任何多项式项的衰减速度。因此被积函数发散，积分是无穷大的。场景 I 是一个具体的反例：$g$ 是一个t分布（多项式尾部），$q$ 是高斯分布，我们已经证明该积分是无穷大的。\n- **结论：** 该陈述是错误的。使用轻尾提议分布（如高斯分布）从重尾目标（如具有多项式尾部的分布）中采样，是导致无限方差（权重爆炸）的典型情况。**不正确**。",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "对于复杂的问题，找到一个单一的、全局都表现良好的提议分布可能非常困难。一个强大的替代方案是采用“分而治之”的策略。本练习介绍了分层重要性采样，它将状态空间分割成不同区域（层），并为每个区域设计专门的提议分布。你将推导每个层内的理论最优提议分布，以及在各层之间分配计算预算的最優策略。这个动手实践问题连接了理论与实践。你将实现从推导最优采样规则到数值计算分配策略的完整设计流程，为结构化、高效地解决困难的积分问题提供一个范本。",
            "id": "3295480",
            "problem": "给定一个已知目标密度和一个可测被积函数下的目标期望。设 $X \\in \\mathcal{X}$ 的密度函数为 $p(x)$（关于勒贝格测度），目标是估计积分 $\\mu = \\mathbb{E}_p[f(X)] = \\int_{\\mathcal{X}} f(x) p(x) \\, dx$。假设状态空间被划分为不相交的可测层 $\\{S_k\\}_{k=1}^K$，使得 $\\bigcup_{k=1}^K S_k = \\mathcal{X}$。考虑一个分层重要性抽样估计量，它对每个层 $S_k$，从一个支撑在 $S_k$ 上的提议密度 $q_k(x)$ 中抽取 $n_k$ 个独立样本，并构成无偏估计量\n$$\\hat{\\mu} = \\sum_{k=1}^K \\frac{1}{n_k} \\sum_{i=1}^{n_k} f(X_{k,i}) \\frac{p(X_{k,i})}{q_k(X_{k,i})} \\mathbf{1}_{S_k}(X_{k,i}), \\quad X_{k,i} \\sim q_k(\\cdot).$$\n假设所有积分都是有限的，并且提议密度 $q_k$ 在 $S_k$ 上关于勒贝格测度是绝对连续的。\n\n任务 A (推导)：仅从重要性抽样、无偏性、方差和 Cauchy–Schwarz 不等式的定义出发，推导在固定总样本预算 $N = \\sum_{k=1}^K n_k$ 下的以下设计原则：\n- 对于每个固定的层 $S_k$，描述能够最小化单位样本方差贡献 $\\mathrm{Var}_{q_k}\\big(f(X) \\tfrac{p(X)}{q_k(X)} \\mathbf{1}_{S_k}(X)\\big)$ 的提议密度 $q_k$（支撑在 $S_k$ 上）的选择。将答案表示为一个归一化常数的倍数，并证明它达到了所述的最小值。\n- 证明在这些每层最优提议密度下，第 $k$ 层的最小单位样本方差可以表示为 $f$ 和 $|f|$ 对 $p$ 的层特定积分，并证明其为非负。\n- 使用拉格朗日乘子法，推导在约束 $\\sum_{k=1}^K n_k = N$ 下最小化总方差 $\\sum_{k=1}^K \\mathrm{Var}_{q_k}\\big(f(X)\\tfrac{p}{q_k}\\mathbf{1}_{S_k}\\big)/n_k$ 的 $\\{n_k\\}$ 分配规则，并说明该规则如何使各层的边际方差贡献相等。给出最终的最小总方差表达式，其为 $N$ 和每层难度度量的函数。\n\n任务 B (计算)：实现一个程序，对于以下每个测试用例，计算任务 A 中隐含的每层难度度量、在固定总预算 $N$ 下最小化方差的最优分配比例 $r_k = n_k/N$ 以及相应的最小总方差。由于所需积分通常没有封闭形式解，你的程序必须通过一维数值积分（quadrature）来计算它们。所有角度必须解释为弧度。\n\n对于每个测试用例，程序必须：\n- 如下文规定，定义 $p(x)$ 和 $f(x)$。\n- 将层 $\\{S_k\\}_{k=1}^K$ 定义为区间；对于无界端点，使用 $(-\\infty)$ 和 $(+\\infty)$。\n- 对每个 $k$，计算层积分\n  $$\\mu_k = \\int_{S_k} f(x) p(x) \\, dx, \\qquad A_k = \\int_{S_k} |f(x)| p(x) \\, dx.$$\n- 计算第 $k$ 层的最小单位样本方差为\n  $$V_k^\\star = \\max\\{A_k^2 - \\mu_k^2, 0\\},$$\n  其中最大值操作是为了防止由于数值积分误差而产生的微小负值。\n- 计算分配比例\n  $$r_k = \\begin{cases}\n    \\dfrac{\\sqrt{V_k^\\star}}{\\sum_{j=1}^K \\sqrt{V_j^\\star}},  & \\text{若 } \\sum_{j=1}^K \\sqrt{V_j^\\star} > 0,\\\\[1em]\n    \\dfrac{1}{K},  & \\text{否则,}\n  \\end{cases}$$\n  以及最小总方差\n  $$\\mathrm{Var}_{\\min} = \\frac{\\left(\\sum_{k=1}^K \\sqrt{V_k^\\star}\\right)^2}{N}.$$\n\n测试套件:\n- 案例 1:\n  - 目标密度 $p(x)$ 是在 $[0,1]$ 上的 $\\mathrm{Beta}(a,b)$ 分布，参数为 $a=2.0, b=5.0$；即，当 $x \\in [0,1]$ 时，$p(x) = \\dfrac{x^{a-1}(1-x)^{b-1}}{B(a,b)}$，否则 $p(x)=0$，其中 $B(a,b)$ 是贝塔函数。\n  - 被积函数 $f(x) = \\sin(6 \\pi x)$。\n  - 分层: $S_1 = [0,0.5]$, $S_2 = (0.5,1]$。\n  - 总预算 $N = 1000$。\n- 案例 2:\n  - 目标密度 $p(x)$ 是在 $[0,1]$ 上的 $\\mathrm{Beta}(a,b)$ 分布，参数为 $a=0.7, b=3.2$。\n  - 被积函数 $f(x) = \\sin(10 \\pi x) + 0.5 \\cos(4 \\pi x)$。\n  - 分层: $S_1 = [0,0.25]$, $S_2 = (0.25,0.75]$, $S_3 = (0.75,1]$。\n  - 总预算 $N = 1500$。\n- 案例 3:\n  - 目标密度 $p(x)$ 是在 $\\mathbb{R}$ 上的 $\\mathcal{N}(0,1)$ 分布，即对所有 $x \\in \\mathbb{R}$，$p(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\dfrac{x^2}{2}\\right)$。\n  - 被积函数 $f(x) = x^2 \\sin(3 x)$。\n  - 分层: $S_1 = (-\\infty,-1]$, $S_2 = (-1,1]$, $S_3 = (1,\\infty)$。\n  - 总预算 $N = 5000$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个以逗号分隔的列表的列表形式的结果。\n- 对于每个测试用例，按顺序输出一个包含 $K+1$ 个浮点数的列表：$K$ 个分配比例 $[r_1,\\dots,r_K]$，后跟最小总方差 $\\mathrm{Var}_{\\min}$。\n- 所有浮点数必须四舍五入到六位小数。\n- 最终输出必须是严格格式的单行文本：例如，对于两个各含两个层的测试用例，它应看起来像 $[[r_{1,1},r_{1,2},V_{\\min,1}],[r_{2,1},r_{2,2},V_{\\min,2}]]$，不含空格。\n- 你的程序必须是自包含的，不得读取任何输入，不得访问外部文件或网络，并且必须确定性地运行。",
            "solution": "该问题要求完成两项任务。任务 A 是推导分层重要性抽样估计量的最优设计。任务 B 是针对三个特定测试用例计算此最优设计。\n\n### 任务 A：设计原则的推导\n\n积分 $\\mu = \\mathbb{E}_p[f(X)]$ 的估计量由下式给出\n$$ \\hat{\\mu} = \\sum_{k=1}^K \\hat{\\mu}_k = \\sum_{k=1}^K \\frac{1}{n_k} \\sum_{i=1}^{n_k} f(X_{k,i}) \\frac{p(X_{k,i})}{q_k(X_{k,i})} \\mathbf{1}_{S_k}(X_{k,i}), \\quad X_{k,i} \\sim q_k(\\cdot) $$\n其中层 $\\{S_k\\}_{k=1}^K$ 划分了状态空间 $\\mathcal{X}$。由于样本在每个层内以及各层之间是独立抽取的，总估计量的方差是各层估计量方差的总和：\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\sum_{k=1}^K \\mathrm{Var}(\\hat{\\mu}_k) $$\n单个层 $S_k$ 的估计量是 $n_k$ 个独立同分布随机变量的平均值。设 $Y_{k,i} = f(X_{k,i}) \\frac{p(X_{k,i})}{q_k(X_{k,i})}$，其中 $X_{k,i} \\sim q_k$。第 $k$ 层的估计量为 $\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i=1}^{n_k} Y_{k,i}$。其方差为\n$$ \\mathrm{Var}(\\hat{\\mu}_k) = \\mathrm{Var}\\left(\\frac{1}{n_k} \\sum_{i=1}^{n_k} Y_{k,i}\\right) = \\frac{1}{n_k} \\mathrm{Var}_{q_k}(Y_k) $$\n其中 $Y_k$ 是一个与任何 $Y_{k,i}$ 具有相同分布的通用随机变量。因此，总方差为\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\sum_{k=1}^K \\frac{\\mathrm{Var}_{q_k}(Y_k)}{n_k} $$\n最小化这个总方差包括两个步骤：首先，对每个层 $S_k$，选择提议密度 $q_k$ 以最小化单位样本方差 $\\mathrm{Var}_{q_k}(Y_k)$；其次，在各层之间分配总样本预算 $N$（即选择 $\\{n_k\\}$）以最小化最终的总方差。\n\n**1. 最优提议密度 $q_k(x)$**\n\n对于一个固定的层 $S_k$，我们希望最小化单位样本方差：\n$$ \\mathrm{Var}_{q_k}(Y_k) = \\mathbb{E}_{q_k}[Y_k^2] - (\\mathbb{E}_{q_k}[Y_k])^2 $$\n首先，我们分析期望项。根据重要性抽样的定义，该估计量对于层积分 $\\mu_k = \\int_{S_k} f(x) p(x) \\, dx$ 是无偏的。\n$$ \\mathbb{E}_{q_k}[Y_k] = \\int_{S_k} f(x) \\frac{p(x)}{q_k(x)} q_k(x) \\, dx = \\int_{S_k} f(x) p(x) \\, dx =: \\mu_k $$\n这个期望 $\\mu_k$ 是一个常数，仅依赖于 $f$、$p$ 和 $S_k$，而与提议密度 $q_k(x)$ 的选择无关。因此，最小化 $\\mathrm{Var}_{q_k}(Y_k)$ 等价于最小化二阶矩 $\\mathbb{E}_{q_k}[Y_k^2]$。\n\n二阶矩为：\n$$ \\mathbb{E}_{q_k}[Y_k^2] = \\int_{S_k} \\left(f(x) \\frac{p(x)}{q_k(x)}\\right)^2 q_k(x) \\, dx = \\int_{S_k} \\frac{f(x)^2 p(x)^2}{q_k(x)} \\, dx $$\n我们寻求关于函数 $q_k(x)$ 最小化该积分，约束条件是 $q_k(x)$ 为 $S_k$ 上的概率密度函数，即 $\\int_{S_k} q_k(x) \\, dx = 1$ 且 $q_k(x) \\ge 0$。\n\n我们可以使用积分形式的 Cauchy-Schwarz 不等式来找到该积分的一个下界。该不等式表明，对于实值函数 $g$ 和 $h$，有 $(\\int g(x)h(x) dx)^2 \\le (\\int g(x)^2 dx)(\\int h(x)^2 dx)$。我们定义 $g(x) = \\sqrt{q_k(x)}$ 和 $h(x) = \\frac{|f(x)|p(x)}{\\sqrt{q_k(x)}}$。在定义域 $S_k$ 上应用该不等式：\n$$ \\left( \\int_{S_k} \\sqrt{q_k(x)} \\cdot \\frac{|f(x)|p(x)}{\\sqrt{q_k(x)}} \\, dx \\right)^2 \\le \\left( \\int_{S_k} (\\sqrt{q_k(x)})^2 \\, dx \\right) \\left( \\int_{S_k} \\left(\\frac{|f(x)|p(x)}{\\sqrt{q_k(x)}}\\right)^2 \\, dx \\right) $$\n化简为：\n$$ \\left( \\int_{S_k} |f(x)|p(x) \\, dx \\right)^2 \\le \\left( \\int_{S_k} q_k(x) \\, dx \\right) \\left( \\int_{S_k} \\frac{f(x)^2 p(x)^2}{q_k(x)} \\, dx \\right) $$\n由于 $\\int_{S_k} q_k(x) \\, dx = 1$，我们有：\n$$ \\mathbb{E}_{q_k}[Y_k^2] = \\int_{S_k} \\frac{f(x)^2 p(x)^2}{q_k(x)} \\, dx \\ge \\left( \\int_{S_k} |f(x)|p(x) \\, dx \\right)^2 $$\n当 Cauchy-Schwarz 不等式中的等号成立时，$\\mathbb{E}_{q_k}[Y_k^2]$ 达到最小值。这当且仅当 $g(x)$ 与 $h(x)$ 成比例时发生，即 $\\sqrt{q_k(x)} \\propto \\frac{|f(x)|p(x)}{\\sqrt{q_k(x)}}$。这意味着 $q_k(x) \\propto |f(x)|p(x)$。\n\n因此，最小化单位样本方差的最优提议密度 $q_k^*(x)$ 在层 $S_k$ 上与 $|f(x)|p(x)$ 成正比：\n$$ q_k^*(x) = \\frac{|f(x)|p(x)}{\\int_{S_k} |f(y)|p(y) \\, dy} $$\n\n**2. 最小单位样本方差**\n\n我们使用最优提议密度 $q_k^*(x)$ 来计算最小方差。设 $A_k = \\int_{S_k} |f(x)|p(x) \\, dx$。\n二阶矩的最小值是我们找到的下界：\n$$ \\min_{q_k} \\mathbb{E}_{q_k}[Y_k^2] = \\left( \\int_{S_k} |f(x)|p(x) \\, dx \\right)^2 = A_k^2 $$\n于是，第 $k$ 层的最小单位样本方差（记为 $V_k^\\star$）为：\n$$ V_k^\\star = \\min_{q_k} \\mathrm{Var}_{q_k}(Y_k) = A_k^2 - \\mu_k^2 = \\left(\\int_{S_k} |f(x)|p(x) \\, dx\\right)^2 - \\left(\\int_{S_k} f(x)p(x) \\, dx\\right)^2 $$\n为证明其非负，我们使用积分的性质 $|\\int g(x) dx| \\le \\int |g(x)| dx$。设 $g(x) = f(x)p(x)$。由于 $p(x) \\ge 0$，有 $|g(x)| = |f(x)|p(x)$。\n$$ |\\mu_k| = \\left|\\int_{S_k} f(x)p(x) \\, dx\\right| \\le \\int_{S_k} |f(x)p(x)| \\, dx = \\int_{S_k} |f(x)|p(x) \\, dx = A_k $$\n两边平方得到 $\\mu_k^2 \\le A_k^2$，这意味着 $V_k^\\star = A_k^2 - \\mu_k^2 \\ge 0$。\n\n**3. 最优样本分配 $\\{n_k\\}$**\n\n对于每个层使用最优提议密度 $q_k^*(x)$，总方差变为：\n$$ V_{tot}(\\{n_k\\}) = \\sum_{k=1}^K \\frac{V_k^\\star}{n_k} $$\n我们需要在总样本量约束 $\\sum_{k=1}^K n_k = N$ 下最小化此量。我们使用拉格朗日乘子法。拉格朗日函数为：\n$$ \\mathcal{L}(\\{n_k\\}, \\lambda) = \\sum_{k=1}^K \\frac{V_k^\\star}{n_k} + \\lambda \\left( \\left(\\sum_{k=1}^K n_k\\right) - N \\right) $$\n对每个 $n_k$ 求偏导数并令其为零，得到最优性条件：\n$$ \\frac{\\partial\\mathcal{L}}{\\partial n_k} = -\\frac{V_k^\\star}{n_k^2} + \\lambda = 0 \\implies n_k^2 = \\frac{V_k^\\star}{\\lambda} \\implies n_k = \\frac{\\sqrt{V_k^\\star}}{\\sqrt{\\lambda}} $$\n此结果表明，最优样本量 $n_k$ 与 $\\sqrt{V_k^\\star}$ 成正比，后者可以解释为在最优提议密度下，第 $k$ 层中重要性加权样本的标准差。条件 $\\lambda = V_k^\\star / n_k^2$ 对所有 $k$ 恒定，意味着在最优状态下，分配一个额外样本点所带来的边际方差减少量在所有层中是相等的。\n\n为了求出乘子的值，我们使用预算约束：\n$$ \\sum_{k=1}^K n_k = \\sum_{k=1}^K \\frac{\\sqrt{V_k^\\star}}{\\sqrt{\\lambda}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{k=1}^K \\sqrt{V_k^\\star} = N $$\n求解 $\\sqrt{\\lambda}$：\n$$ \\sqrt{\\lambda} = \\frac{\\sum_{j=1}^K \\sqrt{V_j^\\star}}{N} $$\n将其代回 $n_k$ 的表达式：\n$$ n_k = N \\frac{\\sqrt{V_k^\\star}}{\\sum_{j=1}^K \\sqrt{V_j^\\star}} $$\n这就是最优分配规则，也称为 Neyman 分配。\n\n最后，我们将最优的 $n_k$ 值代回到 $V_{tot}$ 的表达式中，求得最终的最小总方差 $\\mathrm{Var}_{\\min}$：\n$$ \\mathrm{Var}_{\\min} = \\sum_{k=1}^K \\frac{V_k^\\star}{n_k} = \\sum_{k=1}^K V_k^\\star \\left( N \\frac{\\sqrt{V_k^\\star}}{\\sum_{j=1}^K \\sqrt{V_j^\\star}} \\right)^{-1} = \\sum_{k=1}^K \\frac{\\sqrt{V_k^\\star} \\left(\\sum_{j=1}^K \\sqrt{V_j^\\star}\\right)}{N} $$\n$$ \\mathrm{Var}_{\\min} = \\frac{1}{N} \\left(\\sum_{j=1}^K \\sqrt{V_j^\\star}\\right) \\left(\\sum_{k=1}^K \\sqrt{V_k^\\star}\\right) = \\frac{\\left(\\sum_{k=1}^K \\sqrt{V_k^\\star}\\right)^2}{N} $$\n项 $\\sqrt{V_k^\\star}$ 就是问题中提到的每层难度度量。",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.stats import beta, norm\n\ndef solve():\n    \"\"\"\n    Computes optimal allocation and minimal variance for stratified importance sampling\n    for three specified test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"p_dist\": (\"beta\", 2.0, 5.0),\n            \"f\": lambda x: np.sin(6 * np.pi * x),\n            \"strata\": [[0, 0.5], [0.5, 1.0]],\n            \"N\": 1000,\n        },\n        {\n            \"p_dist\": (\"beta\", 0.7, 3.2),\n            \"f\": lambda x: np.sin(10 * np.pi * x) + 0.5 * np.cos(4 * np.pi * x),\n            \"strata\": [[0, 0.25], [0.25, 0.75], [0.75, 1.0]],\n            \"N\": 1500,\n        },\n        {\n            \"p_dist\": (\"norm\", 0.0, 1.0),\n            \"f\": lambda x: x**2 * np.sin(3 * x),\n            \"strata\": [[-np.inf, -1.0], [-1.0, 1.0], [1.0, np.inf]],\n            \"N\": 5000,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        p_dist_info = case[\"p_dist\"]\n        f = case[\"f\"]\n        strata = case[\"strata\"]\n        N = case[\"N\"]\n        K = len(strata)\n\n        if p_dist_info[0] == \"beta\":\n            a, b = p_dist_info[1], p_dist_info[2]\n            p = lambda x: beta.pdf(x, a, b)\n        elif p_dist_info[0] == \"norm\":\n            mu, sigma = p_dist_info[1], p_dist_info[2]\n            p = lambda x: norm.pdf(x, mu, sigma)\n        \n        V_star_list = []\n        for k in range(K):\n            lower, upper = strata[k]\n            \n            # Integrand for mu_k: f(x) * p(x)\n            integrand_mu = lambda x: f(x) * p(x)\n            mu_k, _ = quad(integrand_mu, lower, upper)\n            \n            # Integrand for A_k: |f(x)| * p(x)\n            integrand_A = lambda x: np.abs(f(x)) * p(x)\n            A_k, _ = quad(integrand_A, lower, upper)\n            \n            # Minimal per-sample variance for stratum k\n            V_star_k = max(A_k**2 - mu_k**2, 0)\n            V_star_list.append(V_star_k)\n\n        # Compute allocation fractions and minimal total variance\n        sqrt_V_star = np.sqrt(V_star_list)\n        sum_sqrt_V_star = np.sum(sqrt_V_star)\n\n        if sum_sqrt_V_star > 0:\n            ratios = sqrt_V_star / sum_sqrt_V_star\n        else:\n            ratios = np.full(K, 1.0 / K)\n            \n        var_min = (sum_sqrt_V_star**2) / N\n\n        case_result = list(ratios)\n        case_result.append(var_min)\n        results.append(case_result)\n\n    # Format output string\n    case_strings = []\n    for res_list in results:\n        formatted_nums = [f\"{x:.6f}\" for x in res_list]\n        case_strings.append(f\"[{','.join(formatted_nums)}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```"
        }
    ]
}