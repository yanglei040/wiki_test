{
    "hands_on_practices": [
        {
            "introduction": "理解一个公式的最好方法是追溯其源头。本练习将引导您从第一性原理出发，推导有效样本量（ESS）最常用的计算公式。通过将自归一化重要性抽样估计量的方差与理想的、未加权估计量的方差相等同，您将揭示 ESS 的数学基础，并巩固对其为何能作为估计量质量度量的理解。",
            "id": "3304977",
            "problem": "考虑一个目标分布，其在 $\\mathbb{R}^{d}$ 上的密度为 $\\pi(x)$，以及一个被积函数 $h:\\mathbb{R}^{d}\\to\\mathbb{R}$，在 $\\pi$ 下具有有限方差。设 $x_{1},\\dots,x_{N}$ 是从一个提议密度 $q(x)$ 中抽取的独立样本，并定义未归一化的重要性权重 $w_{i}=\\pi(x_{i})/q(x_{i})0$。$I=\\mathbb{E}_{\\pi}[h(X)]$ 的自归一化重要性采样（SNIS）估计量为 $\\hat{I}_{\\mathrm{SN}}=\\sum_{i=1}^{N}\\tilde{w}_{i}h(x_{i})$，其中归一化权重为 $\\tilde{w}_{i}=w_{i}/\\sum_{j=1}^{N}w_{j}$。定义有效样本量（ESS）为这样一个数 $N_{\\mathrm{eff}}$，使得 $\\hat{I}_{\\mathrm{SN}}$ 的方差与 $N_{\\mathrm{eff}}$ 个独立目标抽样的等权重平均值的方差相匹配，具体含义如下：利用独立同分布随机变量的加权平均的方差性质，用 $\\tilde{w}_{i}$ 来表示 $\\operatorname{Var}(\\hat{I}_{\\mathrm{SN}})$，并将其与在 $\\pi$ 分布下 $N_{\\mathrm{eff}}$ 个独立的 $h(X)$ 副本的等权重平均的方差相等。推导 $N_{\\mathrm{eff}}$ 关于未归一化权重 $\\{w_{i}\\}_{i=1}^{N}$ 的显式表达式。\n\n接下来，评估 $N_{\\mathrm{eff}}$ 对未归一化权重缩放的敏感性：对于一个常数 $c0$，考虑缩放后的权重 $w_{i}\\mapsto c\\,w_{i}$，并计算在任意 $c0$ 处导数 $\\frac{\\mathrm{d}}{\\mathrm{d}c}N_{\\mathrm{eff}}(c\\,w_{1},\\dots,c\\,w_{N})$ 的值。\n\n将您的最终答案表示为一个双元素行向量，按顺序包含：用 $\\{w_{i}\\}$ 表示的 $N_{\\mathrm{eff}}$ 的推导表达式，以及 $\\frac{\\mathrm{d}}{\\mathrm{d}c}N_{\\mathrm{eff}}(c\\,w_{1},\\dots,c\\,w_{N})$ 的简化解析表达式。无需四舍五入。",
            "solution": "该问题提出了两个任务：第一，根据指定的方差匹配条件，为自归一化重要性采样（SNIS）估计量推导有效样本量（$N_{\\mathrm{eff}}$）的表达式；第二，评估此 $N_{\\mathrm{eff}}$ 对未归一化权重的全局缩放的敏感性。\n\n我们从第一个任务开始：推导 $N_{\\mathrm{eff}}$。\n\n问题将有效样本量 $N_{\\mathrm{eff}}$ 定义为从目标分布 $\\pi(x)$ 中抽取的独立样本数量，这些样本产生的估计量与 SNIS 估计量 $\\hat{I}_{\\mathrm{SN}}$ 具有相同的方差。\n\n首先，我们确定基准估计量的方差。对于 $I = \\mathbb{E}_{\\pi}[h(X)]$，一个理想的蒙特卡洛估计量是基于直接从目标分布 $\\pi(x)$ 中抽取的 $N_{\\mathrm{eff}}$ 个独立同分布（i.i.d.）样本 $\\{X'_j\\}_{j=1}^{N_{\\mathrm{eff}}}$，其形式为简单均值 $\\hat{I}_{\\mathrm{MC}} = \\frac{1}{N_{\\mathrm{eff}}} \\sum_{j=1}^{N_{\\mathrm{eff}}} h(X'_j)$。该估计量的方差由下式给出：\n$$\n\\operatorname{Var}(\\hat{I}_{\\mathrm{MC}}) = \\operatorname{Var}\\left(\\frac{1}{N_{\\mathrm{eff}}} \\sum_{j=1}^{N_{\\mathrm{eff}}} h(X'_j)\\right) = \\frac{1}{N_{\\mathrm{eff}}^2} \\sum_{j=1}^{N_{\\mathrm{eff}}} \\operatorname{Var}_{\\pi}(h(X)) = \\frac{N_{\\mathrm{eff}}}{N_{\\mathrm{eff}}^2} \\operatorname{Var}_{\\pi}(h(X)) = \\frac{\\sigma_{\\pi}^2}{N_{\\mathrm{eff}}}\n$$\n其中 $\\sigma_{\\pi}^2 = \\operatorname{Var}_{\\pi}(h(X))$ 是被积函数 $h(X)$ 在目标分布 $\\pi$ 下的方差。\n\n接下来，我们考虑 SNIS 估计量 $\\hat{I}_{\\mathrm{SN}} = \\sum_{i=1}^{N}\\tilde{w}_{i}h(x_{i})$，其中 $x_i$ 是从提议分布 $q(x)$ 中抽取的独立同分布样本，$\\tilde{w}_i$ 是归一化的重要性权重。对 $\\operatorname{Var}(\\hat{I}_{\\mathrm{SN}})$ 进行严格推导是复杂的，因为权重 $\\tilde{w}_i$ 本身是随机变量并且是相关的。然而，问题陈述给出了一个具体指令：“利用独立同分布随机变量的加权平均的方差性质，用 $\\tilde{w}_{i}$ 来表示 $\\operatorname{Var}(\\hat{I}_{\\mathrm{SN}})$”。\n\n这引导我们采用一个常见的启发式模型，在该模型中，估计量的方差通过将归一化权重 $\\tilde{w}_i$ 视为固定的、预先计算的常数，并假装被平均的随机变量（我们称之为 $Z_i$）是从*目标*分布 $\\pi$ 中抽取的独立同分布样本来近似。在这个简化模型下，加权和 $\\sum_{i=1}^{N} \\tilde{w}_i Z_i$ 的方差为（其中 $Z_i$ 是独立同分布的，方差为 $\\sigma_{\\pi}^2$）：\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{N} \\tilde{w}_i Z_i\\right) = \\sum_{i=1}^{N} \\operatorname{Var}(\\tilde{w}_i Z_i) = \\sum_{i=1}^{N} \\tilde{w}_i^2 \\operatorname{Var}(Z_i) = \\left(\\sum_{i=1}^{N} \\tilde{w}_i^2\\right) \\sigma_{\\pi}^2\n$$\n这就是我们被指示使用的 $\\hat{I}_{\\mathrm{SN}}$ 的方差表达式。\n\n根据问题对 $N_{\\mathrm{eff}}$ 的定义，我们必须使两个方差表达式相等：\n$$\n\\frac{\\sigma_{\\pi}^2}{N_{\\mathrm{eff}}} = \\left(\\sum_{i=1}^{N} \\tilde{w}_i^2\\right) \\sigma_{\\pi}^2\n$$\n假设在 $\\pi$ 分布下 $h(x)$ 不是几乎处处为常数，则有 $\\sigma_{\\pi}^2  0$，我们可以用 $\\sigma_{\\pi}^2$ 除以两边来求解 $N_{\\mathrm{eff}}$：\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\tilde{w}_i^2}\n$$\n问题要求用未归一化的权重 $w_i = \\pi(x_i)/q(x_i)$ 来表示此表达式。归一化权重为 $\\tilde{w}_i = w_i / \\sum_{j=1}^{N} w_j$。将此代入我们关于 $N_{\\mathrm{eff}}$ 的表达式中：\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\left(\\frac{w_i}{\\sum_{j=1}^{N} w_j}\\right)^2} = \\frac{1}{\\frac{\\sum_{i=1}^{N} w_i^2}{\\left(\\sum_{j=1}^{N} w_j\\right)^2}}\n$$\n简化后得到我们答案的第一部分：\n$$\nN_{\\mathrm{eff}} = \\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2}\n$$\n\n现在进行第二个任务，我们必须评估 $N_{\\mathrm{eff}}$ 对未归一化权重缩放的敏感性。让我们考虑一组缩放后的权重 $w'_i = c w_i$，其中常数 $c  0$。我们将 $N_{\\mathrm{eff}}(c)$ 定义为使用这些缩放权重计算的有效样本量：\n$$\nN_{\\mathrm{eff}}(c) = N_{\\mathrm{eff}}(c w_1, \\dots, c w_N) = \\frac{\\left(\\sum_{i=1}^{N} (c w_i)\\right)^2}{\\sum_{i=1}^{N} (c w_i)^2}\n$$\n我们可以从求和中提出常数 $c$：\n$$\nN_{\\mathrm{eff}}(c) = \\frac{\\left(c \\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} c^2 w_i^2} = \\frac{c^2 \\left(\\sum_{i=1}^{N} w_i\\right)^2}{c^2 \\sum_{i=1}^{N} w_i^2}\n$$\n由于 $c  0$，我们有 $c^2 \\neq 0$，因此可以消去分子和分母中的 $c^2$ 项：\n$$\nN_{\\mathrm{eff}}(c) = \\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2}\n$$\n这个表达式与原始的 $N_{\\mathrm{eff}}$ 表达式相同，并且与缩放因子 $c$ 无关。这是预料之中的，因为权重归一化的过程 $\\tilde{w}_i = w_i / \\sum_j w_j$ 内在地消除了任何全局缩放因子。缩放后的归一化权重为 $\\tilde{w}'_i = \\frac{c w_i}{\\sum_j (c w_j)} = \\frac{c w_i}{c \\sum_j w_j} = \\tilde{w}_i$，因此量 $N_{\\mathrm{eff}} = 1/\\sum_i \\tilde{w}_i^2$ 显然对缩放是不变的。\n\n由于 $N_{\\mathrm{eff}}(c)$ 是关于 $c$ 的常数函数，其关于 $c$ 的导数对于任何 $c  0$ 都必须为零。\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}c}N_{\\mathrm{eff}}(c\\,w_{1},\\dots,c\\,w_{N}) = \\frac{\\mathrm{d}}{\\mathrm{d}c} \\left( \\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2} \\right) = 0\n$$\n这就完成了问题的第二部分。我们按照要求将两个结果组合成一个行向量。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2}  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在掌握了理论之后，我们将转向实际应用中的一个关键挑战：数值稳定性。在本练习中，您将设计一种算法，直接利用对数权重来计算有效样本量（ESS），这种情况在复杂模型中非常普遍。您将运用“log-sum-exp”技巧来避免朴素实现中可能出现的上溢和下溢错误，这对于构建稳健可靠的科学计算软件至关重要。",
            "id": "3304971",
            "problem": "给定一组加权样本，其未归一化权重仅通过其自然对数形式给出。现有 $n$ 个样本，由 $i \\in \\{1,\\dots,n\\}$ 索引，其未归一化的正权重为 $W_i \\in (0,\\infty)$，并定义对数权重 $\\ell_i = \\log W_i \\in \\mathbb{R} \\cup \\{-\\infty\\}$。加权估计量由归一化权重 $w_i = W_i / \\sum_{j=1}^{n} W_j$ 构成，而有效样本量（Effective Sample Size, ESS）是这样一个量，它使得该加权估计量的方差等于一个使用相同底层样本但权重均等的无权估计量的方差。您的任务是，从独立随机变量的方差性质出发，推导出以权重表示的 ESS，然后设计一个仅在 $\\ell_i$ 上操作的数值稳定算法来计算 ESS，此算法应避免显式生成 $W_i$，且在 $\\ell_i$ 值非常大（正或负）或等于 $-\\infty$ 时不损失数值精度。\n\n在您的推导中，请使用以下基本原理：\n- 对于一组独立的、均值为零的随机变量 $Z_i$，若它们具有共同方差 $\\sigma^2$，则其线性组合的方差为 $\\mathrm{Var}\\left(\\sum_{i=1}^{n} a_i Z_i\\right) = \\sigma^2 \\sum_{i=1}^{n} a_i^2$。\n- 对于包含 $n$ 个樣本的无权平均，其权重均等，为 $a_i = 1/n$，产生的方差为 $\\sigma^2 / n$。\n\n基于这些原理，完成以下所有任务：\n1. 从基本原理出发，推导出一个用权重 $W_i$ 及其归一化版本 $w_i$ 表示的有效样本量（ESS）的闭式表达式。不要假设任何快捷公式；请展示 ESS 是如何通过将加权估计量的方差与无权估计量的方差相等而得出的。\n2. 从推导出的表达式出发，展示如何通过在对数域中处理和与积，仅使用对数权重 $\\ell_i$ 来計算 ESS。您的推导必须明确展示一个避免对极大或极小的数进行直接指数运算的变换。\n3. 解释如果简单地对 $\\ell_i$ 进行指数运算以生成 $W_i$ 会出现的数值不稳定性，包括当 $\\ell_i$ 很大时发生上溢，以及当 $\\ell_i$ 是很大的负数时发生下溢的情况。提供精确的数值稳定变换，该变换利用对数域来缓解这些问题。\n4. 为所有 $\\ell_i = -\\infty$ 的退化情况（即所有 $W_i = 0$）指定一个稳健的约定。根据此约定，定义您的程序必须返回的 ESS 值。\n\n然后，实现一个完整且可运行的程序，该程序：\n- 不消耗任何输入，仅使用下面指定的测试套件。\n- 对于每个测试用例，使用您的数值稳定方法从给定的对数权重列表中计算 ESS，该方法不显式生成 $W_i$，也不需要在浮点运算中对 $w_i$ 进行归一化。\n- 生成单行输出，其中包含一个逗号分隔的列表，用方括号括起，其中包含所有测试用例的 ESS 值，四舍五入到 $10$ 位小数。输出格式必须严格为 $[r_1,r_2,\\dots,r_m]$，其中每个 $r_k$ 是一个小数点后有 $10$ 位数字的十进制字符串。\n\n不涉及物理单位；不涉及角度。所有答案必须表示为上述指定的浮点十进制数。\n\n测试套件（每个案例都是一个对数权重向量）：\n- 案例 1（均匀权重）：$\\ell = [0.0, 0.0, 0.0, 0.0, 0.0]$。\n- 案例 2（具有层级结构的极大正对数权重）：$\\ell = [1000.0, 999.0, 990.0, 900.0, 800.0]$。\n- 案例 3（极大负但相等的对数权重）：$\\ell = [-1000.0, -1000.0, -1000.0]$。\n- 案例 4（一个权重占主导，其他可忽略）：$\\ell = [0.0, -1000.0, -1000.0, -1000.0]$。\n- 案例 5（有限值与负无穷的混合）：$\\ell = [0.0, -\\infty, -\\infty, -10.0]$。\n- 案例 6（全为负无穷；退化情况）：$\\ell = [-\\infty, -\\infty, -\\infty]$。\n- 案例 7（单个样本）：$\\ell = [123.456]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的结果列表，用方括号括起（例如 $[r_1,r_2,\\dots,r_7]$），其中每个 $r_k$ 是对应案例的 ESS，四舍五入到 $10$ 位小数。",
            "solution": "根据一组加权样本的对数权重计算其有效样本量（ESS）是计算统计学中的一项标准任务，尤其是在重要性采样和序贯蒙特卡洛方法的背景下。问题陈述具有科学依据、定义明确、客观且完整。我们首先从基本原理推导所需表达式，然后设计一个数值稳定的算法来解决该问题。\n\n### 步骤 1：有效样本量 (ESS) 的推导\n\n我们从问题陈述中给出的基本原理开始。考虑一组 $n$ 个独立的、同分布的、均值为零的随机变量 $Z_i$，每个变量具有共同的方差 $\\mathrm{Var}(Z_i) = \\sigma^2$。\n\n$Z_i$ 所属的底层分布的均值的加权估计量由 $\\hat{\\mu} = \\sum_{i=1}^{n} w_i Z_i$ 给出，其中 $w_i$ 是归一化权重，满足 $\\sum_{i=1}^{n} w_i = 1$。该估计量的方差可以使用给定的独立随机变量线性组合的方差公式 $\\mathrm{Var}\\left(\\sum_{i=1}^{n} a_i Z_i\\right) = \\sigma^2 \\sum_{i=1}^{n} a_i^2$ 推导。令系数 $a_i = w_i$，我们得到：\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\sigma^2 \\sum_{i=1}^{n} w_i^2 $$\n\n对于一个包含 $N$ 个样本的标准无权估计量，其权重是均匀的，$a_i = 1/N$，其方差为 $\\mathrm{Var}(\\hat{\\mu}_{\\text{unweighted}}) = \\sigma^2 \\sum_{i=1}^{N} (1/N)^2 = \\sigma^2 \\cdot N \\cdot (1/N^2) = \\sigma^2 / N$。\n\n有效样本量（ESS）定义为无权估计量需要多少个样本 $n_{eff}$ 才能使其方差与我们的加权估计量的方差相同。我们记 $\\mathrm{ESS} = n_{eff}$。因此，我们令两个方差相等：\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{\\mathrm{ESS}} $$\n$$ \\sigma^2 \\sum_{i=1}^{n} w_i^2 = \\frac{\\sigma^2}{\\mathrm{ESS}} $$\n解出 ESS，我们得到第一个关于归一化权重 $w_i$ 的所需表达式：\n$$ \\mathrm{ESS} = \\frac{1}{\\sum_{i=1}^{n} w_i^2} $$\n\n归一化权重 $w_i$ 通过关系 $w_i = W_i / S_W$ 与未归一化的正权重 $W_i$ 相关，其中 $S_W = \\sum_{j=1}^{n} W_j$ 是未归一化权重的总和。将此代入 ESS 表达式的分母中得到：\n$$ \\sum_{i=1}^{n} w_i^2 = \\sum_{i=1}^{n} \\left( \\frac{W_i}{\\sum_{j=1}^{n} W_j} \\right)^2 = \\frac{\\sum_{i=1}^{n} W_i^2}{\\left(\\sum_{j=1}^{n} W_j\\right)^2} $$\n将此代回 ESS 的表达式中，得到关于未归一化权重 $W_i$ 的闭式表达式：\n$$ \\mathrm{ESS} = \\frac{\\left(\\sum_{j=1}^{n} W_j\\right)^2}{\\sum_{i=1}^{n} W_i^2} $$\n这完成了推导的第一部分。\n\n### 步骤 2 和 3：基于对数权重的数值稳定计算\n\n给定的是未归一化权重的自然对数 $\\ell_i = \\log W_i$，这意味着 $W_i = e^{\\ell_i}$。将此代入推导出的 ESS 公式中：\n$$ \\mathrm{ESS} = \\frac{\\left(\\sum_{i=1}^{n} e^{\\ell_i}\\right)^2}{\\sum_{i=1}^{n} e^{2\\ell_i}} $$\n\n**数值不稳定性：**\n一个简单的实现会首先计算权重 $W_i = e^{\\ell_i}$，然后使用该公式。这种方法是数值不稳定的。\n1.  **上溢 (Overflow)：** 如果任何 $\\ell_i$ 是大的正数（例如，对于 IEEE 754 双精度浮点数，$\\ell_i > 709.78$），计算 $e^{\\ell_i}$ 将导致上溢（结果为无穷大）。这将污染求和结果，导致一个不正确或未定义的结果，如 $\\infty/\\infty$。\n2.  **下溢 (Underflow)：** 如果任何 $\\ell_i$ 是大的负数（例如 $\\ell_i  -745$），$e^{\\ell_i}$ 将下溢为 $0$。虽然这对于单个权重可能是可接受的，但如果所有权重都很小，它们的和也可能下溢为 $0$，导致除以零的错误。此外，如果权重的动态范围很大，这还会导致灾难性的相对精度损失。\n\n**数值稳定的变换：**\n为了缓解这些问题，我们采用一种通常称为“log-sum-exp”技巧的标准技术。令 $\\ell_{\\max} = \\max_{i \\in \\{1,\\dots,n\\}} \\ell_i$。我们可以从分子和中提出因子 $e^{\\ell_{\\max}}$，从分母和中提出因子 $e^{2\\ell_{\\max}}$。\n\n令 $S_1 = \\sum_{i=1}^{n} W_i = \\sum_{i=1}^{n} e^{\\ell_i}$ 和 $S_2 = \\sum_{i=1}^{n} W_i^2 = \\sum_{i=1}^{n} e^{2\\ell_i}$。\n我们将 $S_1$重写为：\n$$ S_1 = \\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}} e^{\\ell_{\\max}} = e^{\\ell_{\\max}} \\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}} $$\n我们将 $S_2$重写为：\n$$ S_2 = \\sum_{i=1}^{n} e^{2\\ell_i - 2\\ell_{\\max}} e^{2\\ell_{\\max}} = e^{2\\ell_{\\max}} \\sum_{i=1}^{n} e^{2(\\ell_i - \\ell_{\\max})} $$\n\n现在，将这些分解后的形式代回 ESS 表达式 $\\mathrm{ESS} = S_1^2 / S_2$ 中：\n$$ \\mathrm{ESS} = \\frac{\\left(e^{\\ell_{\\max}} \\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}}\\right)^2}{e^{2\\ell_{\\max}} \\sum_{i=1}^{n} e^{2(\\ell_i - \\ell_{\\max})}} = \\frac{e^{2\\ell_{\\max}} \\left(\\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}}\\right)^2}{e^{2\\ell_{\\max}} \\sum_{i=1}^{n} e^{2(\\ell_i - \\ell_{\\max})}} $$\n$e^{2\\ell_{\\max}}$ 项在代数上消掉了，从而得到最终的、数值稳定的公式：\n$$ \\mathrm{ESS} = \\frac{\\left(\\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}}\\right)^2}{\\sum_{i=1}^{n} e^{2(\\ell_i - \\ell_{\\max})}} $$\n这个表达式是稳定的，因为指数运算的参数 $\\ell_i - \\ell_{\\max}$ 总是小于或等于 $0$。这可以防止上溢。和中的最大项是 $e^0 = 1$，这为其他项保留了数值精度，否则这些项相对于一个非常大的数可能会下溢。这种变换使我们能够仅使用对数权重上的运算来计算 ESS，而无需直接生成可能存在问题的 $W_i$ 或归一化的 $w_i$ 值。\n\n### 步骤 4：退化情况的约定\n\n退化情况发生在所有对数权重均为负无穷时，即对于所有 $i \\in \\{1,\\dots,n\\}$ 都有 $\\ell_i = -\\infty$。这对应于所有未归一化权重都为零，$W_i = 0$。\n在这种情况下，权重之和 $\\sum W_i = 0$，权重平方和 $\\sum W_i^2 = 0$。ESS 公式变为 $\\mathrm{ESS} = 0^2 / 0$，这是一个不定式。\n\n从统计学的角度来看，一组全零的权重不提供任何信息，因为没有任何样本对估计量有贡献。这等效于拥有零个有效样本。因此，最合理和最稳健的约定是在这种情况下将 ESS 定义为 $0$。\n\n在我们的数值稳定算法中，这种情况对应于 $\\ell_{\\max} = -\\infty$。我们必须在进行主要计算之前明确检查此条件，以避免不定式 $\\ell_i - \\ell_{\\max} = -\\infty - (-\\infty)$。如果 $\\ell_{\\max} = -\\infty$，算法应返回 $0.0$。对于只有部分（而非全部）$\\ell_i$ 为 $-\\infty$ 的情况，稳定公式能正确处理它们：如果 $\\ell_{\\max}$ 是有限的，那么对于一个 $\\ell_i = -\\infty$，项 $\\ell_i - \\ell_{\\max} = -\\infty$，而 $e^{-\\infty}$ 的值为 $0$，从而正确地将该样本排除在计算之外。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the effective sample size problem for a predefined test suite.\n    \"\"\"\n\n    def compute_ess(log_weights: np.ndarray) - float:\n        \"\"\"\n        Computes the Effective Sample Size (ESS) from a vector of log-weights\n        using a numerically stable algorithm.\n\n        Args:\n            log_weights: A 1D numpy array of unnormalized log-weights.\n\n        Returns:\n            The calculated ESS as a float.\n        \"\"\"\n        # Ensure input is a numpy array for vectorized operations\n        log_weights = np.asarray(log_weights, dtype=np.float64)\n\n        # Handle the trivial case of a single sample\n        if log_weights.size == 1:\n            return 1.0\n        \n        # Handle the case of an empty set of weights\n        if log_weights.size == 0:\n            return 0.0\n\n        # Find the maximum log-weight\n        max_log_weight = np.max(log_weights)\n\n        # Handle the degenerate case where all log-weights are -inf\n        # This implies all weights are 0, so ESS is 0.\n        if max_log_weight == -np.inf:\n            return 0.0\n\n        # Shift log-weights to prevent overflow/underflow\n        # This is the core of the log-sum-exp trick\n        shifted_log_weights = log_weights - max_log_weight\n        \n        # Calculate the terms for the sums in the stable ESS formula.\n        # Numerator sum's terms: exp(ell_i - ell_max)\n        # Denominator sum's terms: exp(2 * (ell_i - ell_max))\n        # This avoids direct calculation of W_i = exp(ell_i)\n        \n        # Sum of exp(l_i - l_max)\n        s1 = np.sum(np.exp(shifted_log_weights))\n        \n        # Sum of exp(2 * (l_i - l_max)) = sum of (exp(l_i - l_max))^2\n        s2 = np.sum(np.exp(2 * shifted_log_weights))\n        \n        # The terms exp(2 * l_max) from numerator and denominator cancel out\n        # ESS = (sum(W_i))^2 / sum(W_i^2)\n        #     = (exp(l_max) * sum(exp(l_i-l_max)))^2 / (exp(2*l_max) * sum(exp(2*(l_i-l_max))))\n        #     = (sum(exp(l_i-l_max)))^2 / sum(exp(2*(l_i-l_max)))\n        \n        # The denominator s2 cannot be zero if there is at least one finite log-weight,\n        # because the maximum shifted log-weight is 0, making its term exp(0)=1.\n        ess = s1**2 / s2\n        \n        return ess\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n        np.array([1000.0, 999.0, 990.0, 900.0, 800.0]),\n        np.array([-1000.0, -1000.0, -1000.0]),\n        np.array([0.0, -1000.0, -1000.0, -1000.0]),\n        np.array([0.0, -np.inf, -np.inf, -10.0]),\n        np.array([-np.inf, -np.inf, -np.inf]),\n        np.array([123.456])\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_ess(case)\n        # Format to 10 decimal places as specified\n        results.append(f\"{result:.10f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后，我们将探讨有效样本量（ESS）如何从一个诊断工具转变为设计高效高级算法的关键要素。本问题以退火重要性抽样（AIS）为背景，这是一种从复杂分布中采样的强大技术。通过分析增量权重所对应的 ESS，您将学习如何策略性地设计“退火阶梯”，以在计算成本和抽样精度之间取得平衡，这正是现代蒙特卡洛方法中的核心权衡之一。",
            "id": "3304985",
            "problem": "考虑两个$d$维高斯分布：一个基础分布$p_0(x) = \\mathcal{N}(0, I_d)$和一个目标分布$p_1(x) = \\mathcal{N}(\\mu, I_d)$，其中$I_d$是$d \\times d$的单位矩阵，$\\mu \\in \\mathbb{R}^d$是一个固定的均值向量。通过$\\pi_\\beta(x) \\propto p_0(x)^{1-\\beta} p_1(x)^\\beta$定义中间分布的几何路径$\\{\\pi_\\beta\\}_{\\beta \\in [0,1]}$。在退火重要性采样（AIS）中，我们构建一个退火阶梯$0 = \\beta_0  \\beta_1  \\cdots  \\beta_K = 1$，并在每一步$k$为粒子$x_i \\sim \\pi_{\\beta_k}$计算增量重要性权重，其公式为\n$$\nw_i^{(k)} = \\exp\\big((\\beta_{k+1} - \\beta_k) \\, \\Delta U(x_i)\\big),\n$$\n其中$\\Delta U(x) = \\log p_1(x) - \\log p_0(x)$。对于具有未归一化权重$\\{w_i\\}_{i=1}^N$的加权样本，定义归一化权重$\\tilde{w}_i = w_i / \\sum_{j=1}^N w_j$和有效样本量（ESS）为\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^N \\tilde{w}_i^2}.\n$$\n在独立同分布权重的大样本情况下，每步有效样本量分数的一个基本代理是比率\n$$\nR = \\frac{\\big(\\mathbb{E}[w]\\big)^2}{\\mathbb{E}[w^2]},\n$$\n对于固定的权重分布，该比率是当$N \\to \\infty$时$N_{\\mathrm{eff}}/N$的渐近极限。\n\n您的任务如下。\n\n1) 从上述定义出发，并且不假设任何直接给出答案的结果，推导在指定的 高斯对 $(p_0, p_1)$ 下，$\\Delta U(x)$ 在 $\\pi_\\beta$ 分布下的分布，并由此推导从 $\\beta$ 到 $\\beta'$ 的单步增量重要性权重 $w = \\exp((\\beta' - \\beta)\\Delta U(x))$ 的分布。然后，仅使用第一性原理以及高斯和对数正态随机变量的性质，推导渐近每步有效样本量分数 $R(\\Delta \\beta)$ 作为 $\\Delta \\beta = \\beta' - \\beta$ 和 $\\|\\mu\\|_2$ 的函数的精确表达式，其中 $\\|\\mu\\|_2$ 表示 $\\mu$ 的欧几里得范数。\n\n2) 固定一个阈值 $\\theta \\in (0,1)$ 和一个阶梯 $0 = \\beta_0  \\beta_1  \\cdots  \\beta_K = 1$。施加约束，使得对于每一相邻对 $(\\beta_k, \\beta_{k+1})$，渐近每步有效样本量分数满足 $R(\\beta_{k+1}-\\beta_k) \\ge \\theta$。证明在这些约束下，对于指定的模型，选择等间距的阶梯是最优的，并根据 $\\|\\mu\\|_2$ 和 $\\theta$ 确定满足该约束的最小整数增量数 $K_{\\min}$。从第一性原理出发，提供清晰的论证，包括所使用的任何凸性或不等式论证。\n\n3) 将使用 $N$ 个粒子的 AIS 过程的计算成本定义为 $C = N K$，其中 $K$ 是增量步数。对于下面测试套件中的每个参数集，使用您推导的公式计算对 $(K_{\\min}, C_{\\min})$。所有数值答案必须是整数。\n\n测试套件：\n- 案例 1：$d = 10$, $\\|\\mu\\|_2 = 3.0$, $\\theta = 0.5$, $N = 1000$。\n- 案例 2：$d = 5$, $\\|\\mu\\|_2 = 0.0$, $\\theta = 0.9$, $N = 500$。\n- 案例 3：$d = 50$, $\\|\\mu\\|_2 = 5.0$, $\\theta = 0.95$, $N = 100$。\n- 案例 4：$d = 100$, $\\|\\mu\\|_2 = 20.0$, $\\theta = 0.2$, $N = 200$。\n- 案例 5：$d = 3$, $\\|\\mu\\|_2 = 1.0$, $\\theta = 0.999$, $N = 1000$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，每个元素是一个双元素列表 $[K_{\\min}, C_{\\min}]$，按上述顺序对应一个测试案例。例如，两个假设案例的输出将类似于 $[[1,100],[4,400]]$。此问题不涉及物理单位，所有数值输出必须是整数。",
            "solution": "该问题要求对特定高斯分布对的退火重要性采样（AIS）进行多步推导，然后分析最优退火步数并应用于一个测试套件。解答相应地分为三部分。\n\n第 1 部分：渐近每步有效样本量分数 $R(\\Delta \\beta)$ 的推导。\n\n推导首先从刻画中间分布 $\\pi_\\beta(x) \\propto p_0(x)^{1-\\beta} p_1(x)^\\beta$ 开始。基础分布为 $p_0(x) = \\mathcal{N}(x | 0, I_d)$，目标分布为 $p_1(x) = \\mathcal{N}(x | \\mu, I_d)$。它们的对数概率密度，在相差一个归一化常数的情况下，为\n$$\n\\log p_0(x) = -\\frac{1}{2} x^T x + \\text{const.}\n$$\n$$\n\\log p_1(x) = -\\frac{1}{2} (x-\\mu)^T (x-\\mu) + \\text{const.}\n$$\n因此，中间分布 $\\pi_\\beta(x)$ 的对数密度为：\n$$\n\\log \\pi_\\beta(x) = (1-\\beta) \\log p_0(x) + \\beta \\log p_1(x) + \\text{const.}\n$$\n$$\n= -\\frac{1-\\beta}{2} x^T x - \\frac{\\beta}{2} (x-\\mu)^T (x-\\mu) + \\text{const.}\n$$\n$$\n= -\\frac{1-\\beta}{2} x^T x - \\frac{\\beta}{2} (x^T x - 2x^T\\mu + \\mu^T\\mu) + \\text{const.}\n$$\n$$\n= -\\frac{1}{2} x^T x + \\beta x^T\\mu - \\frac{\\beta}{2} \\mu^T\\mu + \\text{const.}\n$$\n为了将其识别为高斯分布，我们对包含 $x$ 的项进行配方：\n$$\n-\\frac{1}{2} (x^T x - 2\\beta x^T\\mu) - \\frac{\\beta}{2} \\mu^T\\mu = -\\frac{1}{2} (x - \\beta\\mu)^T (x - \\beta\\mu) + \\frac{1}{2} (\\beta\\mu)^T(\\beta\\mu) - \\frac{\\beta}{2} \\mu^T\\mu\n$$\n不含 $x$ 的项被吸收到归一化常数中。指数中的二次型 $-\\frac{1}{2} (x - \\beta\\mu)^T (x - \\beta\\mu)$ 表明，$\\pi_\\beta(x)$ 是一个均值为 $\\beta\\mu$、协方差矩阵为 $I_d$ 的高斯分布。因此，$\\pi_\\beta(x) = \\mathcal{N}(x | \\beta\\mu, I_d)$。\n\n接下来，我们确定对于样本 $x \\sim \\pi_\\beta(x)$，对数似然差 $\\Delta U(x) = \\log p_1(x) - \\log p_0(x)$ 的分布。\n$$\n\\Delta U(x) = \\left(-\\frac{1}{2} (x-\\mu)^T (x-\\mu)\\right) - \\left(-\\frac{1}{2} x^T x\\right) = -\\frac{1}{2} (x^T x - 2x^T\\mu + \\mu^T\\mu) + \\frac{1}{2} x^T x = x^T\\mu - \\frac{1}{2} \\mu^T\\mu\n$$\n由于 $x \\sim \\mathcal{N}(\\beta\\mu, I_d)$，$\\Delta U(x)$ 是高斯随机向量的仿射变换，这意味着它是一个标量高斯随机变量。其均值为：\n$$\n\\mathbb{E}_{\\pi_\\beta}[\\Delta U(x)] = \\mathbb{E}[x^T\\mu] - \\frac{1}{2} \\|\\mu\\|_2^2 = (\\mathbb{E}[x])^T\\mu - \\frac{1}{2} \\|\\mu\\|_2^2 = (\\beta\\mu)^T\\mu - \\frac{1}{2} \\|\\mu\\|_2^2 = \\left(\\beta - \\frac{1}{2}\\right) \\|\\mu\\|_2^2\n$$\n其方差为：\n$$\n\\text{Var}_{\\pi_\\beta}(\\Delta U(x)) = \\text{Var}(x^T\\mu) = \\mu^T \\text{Cov}(x) \\mu = \\mu^T I_d \\mu = \\|\\mu\\|_2^2\n$$\n所以，对于 $x \\sim \\pi_\\beta(x)$，我们已确定 $\\Delta U(x) \\sim \\mathcal{N}\\left( (\\beta - \\frac{1}{2})\\|\\mu\\|_2^2, \\|\\mu\\|_2^2 \\right)$。\n\n增量权重为 $w = \\exp(\\Delta\\beta \\cdot \\Delta U(x))$，其中 $\\Delta\\beta = \\beta_{k+1} - \\beta_k$。权重的对数 $\\log w = \\Delta\\beta \\cdot \\Delta U(x)$ 也服从正态分布：\n$$\n\\log w \\sim \\mathcal{N}\\left( \\Delta\\beta\\left(\\beta - \\frac{1}{2}\\right)\\|\\mu\\|_2^2, (\\Delta\\beta)^2\\|\\mu\\|_2^2 \\right)\n$$\n因此，$w$ 服从对数正态分布。如果一个随机变量 $Y$ 服从对数正态分布，使得 $\\log Y \\sim \\mathcal{N}(m, s^2)$，那么它的矩由 $\\mathbb{E}[Y^k] = \\exp(km + \\frac{1}{2}k^2s^2)$ 给出。在我们的例子中，$m = \\Delta\\beta(\\beta - \\frac{1}{2})\\|\\mu\\|_2^2$ 且 $s^2 = (\\Delta\\beta)^2\\|\\mu\\|_2^2$。\n\n最后，我们计算渐近 ESS 分数 $R(\\Delta\\beta) = (\\mathbb{E}[w])^2 / \\mathbb{E}[w^2]$。\n一阶原点矩 ($k=1$) 为 $\\mathbb{E}[w] = \\exp(m + \\frac{1}{2}s^2)$。\n二阶原点矩 ($k=2$) 为 $\\mathbb{E}[w^2] = \\exp(2m + \\frac{1}{2}(2^2)s^2) = \\exp(2m + 2s^2)$。\n因此，比率 $R$ 为：\n$$\nR(\\Delta\\beta) = \\frac{\\left(\\exp(m + \\frac{1}{2}s^2)\\right)^2}{\\exp(2m + 2s^2)} = \\frac{\\exp(2m + s^2)}{\\exp(2m + 2s^2)} = \\exp(s^2 - 2s^2) = \\exp(-s^2)\n$$\n代入 $s^2$ 的表达式，我们得到精确表达式：\n$$\nR(\\Delta\\beta) = \\exp\\left(-(\\Delta\\beta)^2 \\|\\mu\\|_2^2\\right)\n$$\n值得注意的是，这个表达式仅取决于步长 $\\Delta\\beta$ 和均值偏移向量 $\\mu$ 的L2范数平方，而不取决于采样温度 $\\beta$ 或维度 $d$。\n\n第 2 部分：最小增量数 $K_{\\min}$ 的确定。\n\n问题对阶梯中的所有相邻步 $(\\beta_k, \\beta_{k+1})$ 施加了约束 $R(\\beta_{k+1}-\\beta_k) \\ge \\theta$，其中 $\\theta \\in (0,1)$。令 $\\Delta\\beta_k = \\beta_{k+1}-\\beta_k$。该约束变为：\n$$\n\\exp(-(\\Delta\\beta_k)^2 \\|\\mu\\|_2^2) \\ge \\theta\n$$\n对两边取自然对数，这是一个严格递增函数：\n$$\n-(\\Delta\\beta_k)^2 \\|\\mu\\|_2^2 \\ge \\log \\theta \\implies (\\Delta\\beta_k)^2 \\le -\\frac{\\log \\theta}{\\|\\mu\\|_2^2}\n$$\n由于 $\\theta \\in (0,1)$，$\\log\\theta  0$，所以右侧为正。这意味着步长存在一个上界：\n$$\n\\Delta\\beta_k \\le \\frac{\\sqrt{-\\log \\theta}}{\\|\\mu\\|_2} \\equiv \\Delta\\beta_{\\max}\n$$\n此推导假设 $\\|\\mu\\|_2 > 0$。我们寻求最小化遍历区间 $[0,1]$ 所需的总步数 $K$，其约束条件为 $\\sum_{k=0}^{K-1} \\Delta\\beta_k = 1$ 且对于所有 $k$ 都有 $\\Delta\\beta_k \\le \\Delta\\beta_{\\max}$。为了最小化 $K$，必须最大化平均步长 $1/K$，这意味着每个单独的步长 $\\Delta\\beta_k$ 都应尽可能大。条件 $\\sum_{k=0}^{K-1} \\Delta\\beta_k = 1$ 与一致上界 $\\Delta\\beta_k \\le \\Delta\\beta_{\\max}$ 相结合，得到：\n$$\n1 = \\sum_{k=0}^{K-1} \\Delta\\beta_k \\le \\sum_{k=0}^{K-1} \\Delta\\beta_{\\max} = K \\cdot \\Delta\\beta_{\\max}\n$$\n这得出了 $K$ 的一个下界：$K \\ge 1 / \\Delta\\beta_{\\max}$。由于 $K$ 必须是整数，最小步数是 $K_{\\min} = \\lceil 1 / \\Delta\\beta_{\\max} \\rceil$。这个最小值可以通过一个等间距的阶梯实现，其中对所有 $k$ 都有 $\\Delta\\beta_k = 1/K_{\\min}$。这个选择满足约束条件，因为 $1/K_{\\min} \\le \\Delta\\beta_{\\max}$。因此，等间距阶梯是最优的。\n$K_{\\min}$ 的显式公式为：\n$$\nK_{\\min} = \\left\\lceil \\frac{\\|\\mu\\|_2}{\\sqrt{-\\log \\theta}} \\right\\rceil\n$$\n在 $\\|\\mu\\|_2 = 0$ 的特殊情况下，$p_0$ 和 $p_1$ 是相同的。因此 $\\Delta U(x) = 0$，权重 $w=1$，且 $R=1$。约束 $1 \\ge \\theta$ 总是满足的，所以从 $\\beta=0$ 到 $\\beta=1$ 的单步就足够了，得出 $K_{\\min} = 1$。上述公式会得出 $\\lceil 0 \\rceil = 0$，这不是一个有效的步数。因此，我们必须将其作为特殊情况处理。\n\n第 3 部分：测试套件的计算。\n\n使用推导出的公式，如果 $\\|\\mu\\|_2=0$，最小步数为 $K_{\\min} = 1$；如果 $\\|\\mu\\|_20$，最小步数为 $K_{\\min} = \\lceil \\|\\mu\\|_2 / \\sqrt{-\\log \\theta} \\rceil$。相应的最小计算成本为 $C_{\\min} = N \\cdot K_{\\min}$。提供的 Python 代码为给定的测试用例实现了这些计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Calculates the minimal number of AIS steps and the corresponding computational cost\n    for a series of test cases based on a derived formula.\n    \"\"\"\n    # Test suite: each element is a tuple of (d, ||mu||_2, theta, N)\n    # The dimension d is not used in the final formula but is kept for consistency.\n    test_cases = [\n        # Case 1: d = 10, ||mu||_2 = 3.0, theta = 0.5, N = 1000\n        (10, 3.0, 0.5, 1000),\n        # Case 2: d = 5, ||mu||_2 = 0.0, theta = 0.9, N = 500\n        (5, 0.0, 0.9, 500),\n        # Case 3: d = 50, ||mu||_2 = 5.0, theta = 0.95, N = 100\n        (50, 5.0, 0.95, 100),\n        # Case 4: d = 100, ||mu||_2 = 20.0, theta = 0.2, N = 200\n        (100, 20.0, 0.2, 200),\n        # Case 5: d = 3, ||mu||_2 = 1.0, theta = 0.999, N = 1000\n        (3, 1.0, 0.999, 1000),\n    ]\n\n    results = []\n    for case in test_cases:\n        _d, mu_norm, theta, N = case\n\n        if mu_norm == 0.0:\n            # If ||mu||_2 is 0, p0 and p1 are identical. The ESS fraction R is always 1.\n            # Thus, a single step is sufficient.\n            k_min = 1\n        else:\n            # For ||mu||_2  0, the minimal number of steps K_min is given by\n            # ceil(||mu||_2 / sqrt(-log(theta))).\n            # We use numpy for log and sqrt, and math.ceil to get an integer result.\n            argument = mu_norm / np.sqrt(-np.log(theta))\n            k_min = math.ceil(argument)\n\n        # The minimal cost C_min is N * K_min.\n        c_min = N * k_min\n        \n        results.append([k_min, c_min])\n\n    # Format the output as a string representation of a list of lists.\n    # e.g., [[K1, C1], [K2, C2]]\n    # Each inner list is converted to a string, then they are joined by commas,\n    # and finally enclosed in square brackets.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Python's default string representation of a list includes spaces.\n    # The example output [[1,100],[4,400]] has no spaces.\n    # We remove spaces to match the dense format.\n    print(output_str.replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}