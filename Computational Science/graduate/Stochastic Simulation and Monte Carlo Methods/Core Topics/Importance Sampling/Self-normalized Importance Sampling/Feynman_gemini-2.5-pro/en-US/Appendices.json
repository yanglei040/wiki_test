{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with a \"sanity check\" exercise. Here, we analyze the special case where the proposal distribution $q$ is identical to the target distribution $p$. This scenario allows us to apply the general formula for the asymptotic variance of the Self-Normalized Importance Sampling (SNIS) estimator, derived from the Delta method, and see how it correctly simplifies to the familiar variance of a standard Monte Carlo estimator. This practice () reinforces the underlying theory by connecting it to a known baseline, ensuring you understand the mechanics before moving to more complex cases.",
            "id": "3338585",
            "problem": "Consider Self-Normalized Importance Sampling (SNIS) for estimating the expectation of a function under a target distribution. Let the target distribution be $p=\\mathcal{N}(0,1)$, the proposal distribution be $q=\\mathcal{N}(0,1)$, and the integrand be $h(x)=x^{4}$. Define the unnormalized importance weight $w(x)$ by $w(x)=\\frac{p(x)}{q(x)}$, and define the SNIS estimator based on independent and identically distributed samples $X_{1},\\dots,X_{n}\\sim q$ by\n$$\n\\hat{\\mu}_{n}=\\sum_{i=1}^{n}\\tilde{w}_{i}h(X_{i}),\\quad\\text{where}\\quad \\tilde{w}_{i}=\\frac{w(X_{i})}{\\sum_{j=1}^{n}w(X_{j})}.\n$$\nStarting from foundational definitions and the Central Limit Theorem (CLT), use the ratio-of-means representation of SNIS and the Delta method to derive the asymptotic variance constant $\\sigma^{2}$ defined by\n$$\n\\sqrt{n}\\left(\\hat{\\mu}_{n}-\\mu\\right)\\;\\Rightarrow\\;\\mathcal{N}\\!\\left(0,\\sigma^{2}\\right),\n$$\nwhere $\\mu=\\mathbb{E}_{p}[h(X)]$. Compute $\\mu$, determine $w(x)$ explicitly, and then derive a closed-form expression for $\\sigma^{2}$ that reduces to a concrete real number for the given $p$, $q$, and $h$. Express your final answer as a row vector $\\big(\\mu,\\;w(x),\\;\\sigma^{2}\\big)$ using the LaTeX $\\mathrm{pmatrix}$ environment. No rounding is required, and no units are involved.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded within the established theory of stochastic simulation and Monte Carlo methods, specifically self-normalized importance sampling. The problem is well-posed, objective, and self-contained, providing all necessary information to derive the requested quantities. The setup, while representing a simplified case where the target and proposal distributions are identical, is a valid scenario to test the application of the specified theoretical tools (the Delta method for ratio estimators). No scientific or factual unsoundness, contradictions, or ambiguities are present.\n\nThe solution proceeds as follows. First, the target expectation $\\mu$ and the unnormalized importance weight function $w(x)$ are determined. Second, the Self-Normalized Importance Sampling (SNIS) estimator is represented as a ratio of two sample means. Third, the Delta method is applied to this ratio to derive the general form of the asymptotic variance constant $\\sigma^2$. Finally, this general expression is evaluated for the specific distributions and integrand provided in the problem to obtain a numerical result.\n\nThe target expectation $\\mu$ is defined as the expectation of the function $h(x)=x^4$ with respect to the target distribution $p=\\mathcal{N}(0,1)$. The variable $X$ is a standard normal random variable.\n$$\n\\mu = \\mathbb{E}_{p}[h(X)] = \\mathbb{E}_{p}[X^4]\n$$\nThis is the fourth moment of a standard normal distribution. For a random variable $Z \\sim \\mathcal{N}(0, 1)$, the $k$-th moment $\\mathbb{E}[Z^k]$ is given by $(k-1)!! = (k-1)(k-3)\\cdots 1$ for even $k$, and $0$ for odd $k$. For $k=4$, the fourth moment is:\n$$\n\\mu = (4-1)!! = 3 \\times 1 = 3\n$$\n\nNext, the unnormalized importance weight function $w(x)$ is determined. It is the ratio of the target probability density function (PDF) $p(x)$ to the proposal PDF $q(x)$.\nGiven $p(x) = \\mathcal{N}(0,1)$ and $q(x) = \\mathcal{N}(0,1)$, their PDFs are identical:\n$$\np(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)\n$$\n$$\nq(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)\n$$\nTherefore, the weight function is:\n$$\nw(x) = \\frac{p(x)}{q(x)} = \\frac{\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)}{\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)} = 1\n$$\nThe weight is constant and equal to $1$ for all $x$.\n\nThe SNIS estimator $\\hat{\\mu}_n$ can be expressed as a ratio of sample means. Let $Y_i = w(X_i)h(X_i)$ and $Z_i = w(X_i)$, where $X_i \\sim q$. The estimator is:\n$$\n\\hat{\\mu}_{n} = \\frac{\\sum_{i=1}^{n} Y_i}{\\sum_{j=1}^{n} Z_j} = \\frac{\\frac{1}{n}\\sum_{i=1}^{n} Y_i}{\\frac{1}{n}\\sum_{j=1}^{n} Z_j} = \\frac{\\bar{Y}_n}{\\bar{Z}_n}\n$$\nTo find the asymptotic variance, we use the Delta method. We first consider the joint asymptotic distribution of the vector $(\\bar{Y}_n, \\bar{Z}_n)$. Let $V_i = \\begin{pmatrix} Y_i \\\\ Z_i \\end{pmatrix}$. The expectation of $V_i$ with respect to the proposal distribution $q$ is:\n$$\n\\mathbb{E}_q[V] = \\begin{pmatrix} \\mathbb{E}_q[Y] \\\\ \\mathbb{E}_q[Z] \\end{pmatrix} = \\begin{pmatrix} \\mathbb{E}_q[w(X)h(X)] \\\\ \\mathbb{E}_q[w(X)] \\end{pmatrix}\n$$\nCalculating these expectations:\n$$\n\\mathbb{E}_q[w(X)] = \\int w(x)q(x) \\,dx = \\int \\frac{p(x)}{q(x)}q(x) \\,dx = \\int p(x) \\,dx = 1\n$$\n$$\n\\mathbb{E}_q[w(X)h(X)] = \\int w(x)h(x)q(x) \\,dx = \\int \\frac{p(x)}{q(x)}h(x)q(x) \\,dx = \\int h(x)p(x) \\,dx = \\mathbb{E}_p[h(X)] = \\mu\n$$\nSo, $\\mathbb{E}_q[V] = \\begin{pmatrix} \\mu \\\\ 1 \\end{pmatrix}$. By the multivariate Central Limit Theorem,\n$$\n\\sqrt{n}\\left(\\begin{pmatrix} \\bar{Y}_n \\\\ \\bar{Z}_n \\end{pmatrix} - \\begin{pmatrix} \\mu \\\\ 1 \\end{pmatrix}\\right) \\Rightarrow \\mathcal{N}(0, \\Sigma)\n$$\nwhere $\\Sigma$ is the covariance matrix of $V_i$ under $q$:\n$$\n\\Sigma = \\text{Cov}_q(V) = \\begin{pmatrix} \\text{Var}_q(Y)  \\text{Cov}_q(Y,Z) \\\\ \\text{Cov}_q(Y,Z)  \\text{Var}_q(Z) \\end{pmatrix}\n$$\nThe estimator $\\hat{\\mu}_n$ is a function $g(\\bar{Y}_n, \\bar{Z}_n)$ where $g(y,z) = y/z$. By the Delta method, the asymptotic distribution of $\\sqrt{n}(\\hat{\\mu}_n - \\mu)$ is $\\mathcal{N}(0, \\sigma^2)$, with $\\sigma^2 = (\\nabla g)^T \\Sigma (\\nabla g)$, where the gradient $\\nabla g$ is evaluated at $(\\mathbb{E}_q[Y], \\mathbb{E}_q[Z]) = (\\mu, 1)$.\nThe gradient of $g(y,z)$ is $\\nabla g(y,z) = \\begin{pmatrix} 1/z \\\\ -y/z^2 \\end{pmatrix}$. Evaluated at $(\\mu, 1)$, it is $\\nabla g(\\mu,1) = \\begin{pmatrix} 1 \\\\ -\\mu \\end{pmatrix}$.\nThe asymptotic variance is:\n$$\n\\sigma^2 = \\begin{pmatrix} 1  -\\mu \\end{pmatrix} \\Sigma \\begin{pmatrix} 1 \\\\ -\\mu \\end{pmatrix}\n= \\begin{pmatrix} 1  -\\mu \\end{pmatrix} \\begin{pmatrix} \\text{Var}_q(Y)  \\text{Cov}_q(Y,Z) \\\\ \\text{Cov}_q(Y,Z)  \\text{Var}_q(Z) \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -\\mu \\end{pmatrix}\n$$\n$$\n\\sigma^2 = \\text{Var}_q(Y) - 2\\mu\\text{Cov}_q(Y,Z) + \\mu^2\\text{Var}_q(Z)\n$$\nThis is the variance of the random variable $Y-\\mu Z$ under $q$.\n$$\n\\sigma^2 = \\text{Var}_q(Y - \\mu Z) = \\mathbb{E}_q\\left[ \\left( (Y - \\mu Z) - \\mathbb{E}_q[Y-\\mu Z] \\right)^2 \\right]\n$$\nSince $\\mathbb{E}_q[Y-\\mu Z] = \\mathbb{E}_q[Y] - \\mu \\mathbb{E}_q[Z] = \\mu - \\mu(1) = 0$, this simplifies to:\n$$\n\\sigma^2 = \\mathbb{E}_q[(Y - \\mu Z)^2] = \\mathbb{E}_q[(w(X)h(X) - \\mu w(X))^2] = \\mathbb{E}_q[w(X)^2 (h(X) - \\mu)^2]\n$$\nThis is the general expression for the asymptotic variance constant of the SNIS estimator.\n\nNow, we substitute the specific values for this problem: $w(x)=1$, $h(x)=x^4$, and $\\mu=3$. The expectation is taken with respect to $q = \\mathcal{N}(0,1)$.\n$$\n\\sigma^2 = \\mathbb{E}_q[1^2 (X^4 - 3)^2] = \\mathbb{E}_q[(X^4 - 3)^2]\n$$\nExpanding the square:\n$$\n\\sigma^2 = \\mathbb{E}_q[X^8 - 6X^4 + 9]\n$$\nBy linearity of expectation:\n$$\n\\sigma^2 = \\mathbb{E}_q[X^8] - 6\\mathbb{E}_q[X^4] + 9\n$$\nWe need the eighth moment of a standard normal distribution. Using the formula $(k-1)!!$ for $k=8$:\n$$\n\\mathbb{E}_q[X^8] = (8-1)!! = 7 \\times 5 \\times 3 \\times 1 = 105\n$$\nWe have already computed $\\mathbb{E}_q[X^4] = 3$ (since $p=q$). Substituting these values:\n$$\n\\sigma^2 = 105 - 6(3) + 9 = 105 - 18 + 9 = 87 + 9 = 96\n$$\nAs a verification, in the special case where $p=q$, $w(x)=1$ and the SNIS estimator reduces to the standard Monte Carlo estimator $\\hat{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n h(X_i)$. By the standard Central Limit Theorem, the asymptotic variance is simply the variance of the integrand with respect to the sampling distribution, $\\text{Var}_q(h(X))$.\n$$\n\\sigma^2 = \\text{Var}_q(h(X)) = \\text{Var}_q(X^4) = \\mathbb{E}_q[(X^4)^2] - (\\mathbb{E}_q[X^4])^2 = \\mathbb{E}_q[X^8] - (\\mathbb{E}_q[X^4])^2\n$$\n$$\n\\sigma^2 = 105 - 3^2 = 105 - 9 = 96\n$$\nThe result is consistent.\n\nThe final answer is the row vector $(\\mu, w(x), \\sigma^2)$. Substituting the computed values:\n$$\n(\\mu, w(x), \\sigma^2) = (3, 1, 96)\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 3  1  96 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having established the baseline, we now move to the more realistic and common situation where the target and proposal distributions differ. This practice () has you analyze the core components of the importance sampling estimator when using an over-dispersed proposal distribution—a common and often effective strategy. By explicitly calculating the unnormalized weights and the moments of the weighted integrand, you will develop a concrete understanding of how the choice of $q$ directly influences the statistical properties of the estimate and the overall efficiency of the sampling procedure.",
            "id": "3338601",
            "problem": "Consider Self-Normalized Importance Sampling (SNIS) within the broader context of Monte Carlo (MC) methods. Let the target distribution be the standard normal density $p=\\mathcal{N}(0,1)$ and the proposal distribution be the zero-mean normal with variance $4$, $q=\\mathcal{N}(0,4)$. Define the integrand $h(x)=x^{2}$. Starting from the foundational definitions of importance sampling and the relationship between expectations under $p$ and $q$ via the Radon–Nikodym derivative, determine the following:\n\n1. The unnormalized importance weight $w(x)$ associated with sampling from $q$ to estimate expectations under $p$ for the integrand $h(x)$.\n2. The expectation $\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right]$.\n3. The variance $\\mathrm{Var}_{q}\\!\\left(h(X)\\,w(X)\\right)$ expressed in closed form.\n\nUse only well-established facts about Gaussian probability density functions and Gaussian integral identities in your derivations. Provide exact expressions; do not approximate or round. Express your final answer as a single row matrix containing the three requested expressions in the order given and do not include any explanatory text in the final answer.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of Monte Carlo methods, is well-posed with all necessary information provided, and is stated using objective, formal language. We can proceed with the solution.\n\nThe problem asks for three quantities related to an importance sampling setup. The target distribution is the standard normal, $p=\\mathcal{N}(0,1)$, with probability density function (PDF) $p(x)$. The proposal distribution is a zero-mean normal with variance $4$, $q=\\mathcal{N}(0,4)$, with PDF $q(x)$. The integrand is $h(x)=x^2$.\n\nThe PDF for the target distribution $p(x)$ is:\n$$p(x) = \\frac{1}{\\sqrt{2\\pi \\cdot 1}} \\exp\\left(-\\frac{x^2}{2 \\cdot 1}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$$\nThe PDF for the proposal distribution $q(x)$ is:\n$$q(x) = \\frac{1}{\\sqrt{2\\pi \\cdot 4}} \\exp\\left(-\\frac{x^2}{2 \\cdot 4}\\right) = \\frac{1}{2\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{8}\\right)$$\n\n**1. The unnormalized importance weight $w(x)$**\n\nThe unnormalized importance weight, $w(x)$, is defined as the Radon-Nikodym derivative of the target measure with respect to the proposal measure, which for absolutely continuous distributions is the ratio of their PDFs:\n$$w(x) = \\frac{p(x)}{q(x)}$$\nSubstituting the expressions for the PDFs:\n$$w(x) = \\frac{\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)}{\\frac{1}{2\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{8}\\right)}$$\nThe constant terms simplify:\n$$w(x) = 2 \\frac{\\exp\\left(-\\frac{x^2}{2}\\right)}{\\exp\\left(-\\frac{x^2}{8}\\right)} = 2 \\exp\\left(-\\frac{x^2}{2} - \\left(-\\frac{x^2}{8}\\right)\\right)$$\n$$w(x) = 2 \\exp\\left(-\\frac{4x^2}{8} + \\frac{x^2}{8}\\right) = 2 \\exp\\left(-\\frac{3x^2}{8}\\right)$$\nThis is the first requested expression.\n\n**2. The expectation $\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right]$**\n\nThe expectation of the product $h(X)w(X)$ is computed with respect to the proposal distribution $q$. By definition of expectation:\n$$\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right] = \\int_{-\\infty}^{\\infty} h(x) w(x) q(x) \\, dx$$\nBy substituting $w(x) = p(x)/q(x)$, we see a fundamental identity of importance sampling:\n$$\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right] = \\int_{-\\infty}^{\\infty} h(x) \\frac{p(x)}{q(x)} q(x) \\, dx = \\int_{-\\infty}^{\\infty} h(x) p(x) \\, dx$$\nThis integral is, by definition, the expectation of $h(X)$ with respect to the target distribution $p$:\n$$\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right] = \\mathbb{E}_{p}\\!\\left[h(X)\\right]$$\nGiven $h(x) = x^2$ and $p = \\mathcal{N}(0,1)$, we are calculating the second moment of a standard normal random variable. For a random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the second moment is $\\mathbb{E}[X^2] = \\mathrm{Var}(X) + (\\mathbb{E}[X])^2 = \\sigma^2 + \\mu^2$. For the standard normal distribution, $\\mu = 0$ and $\\sigma^2 = 1$.\nTherefore:\n$$\\mathbb{E}_{p}\\!\\left[X^2\\right] = 1 + 0^2 = 1$$\nSo, the second requested value is $\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right] = 1$.\n\n**3. The variance $\\mathrm{Var}_{q}\\!\\left(h(X)\\,w(X)\\right)$**\n\nThe variance of a random variable $Y$ is given by $\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$. Let $Y = h(X)w(X)$. We need to calculate:\n$$\\mathrm{Var}_{q}\\!\\left(h(X)\\,w(X)\\right) = \\mathbb{E}_{q}\\!\\left[\\left(h(X)\\,w(X)\\right)^2\\right] - \\left(\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right]\\right)^2$$\nWe already found that $\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right] = 1$, so the second term is $1^2=1$. We now compute the first term, $\\mathbb{E}_{q}\\!\\left[\\left(h(X)\\,w(X)\\right)^2\\right]$:\n$$\\mathbb{E}_{q}\\!\\left[\\left(h(X)\\,w(X)\\right)^2\\right] = \\int_{-\\infty}^{\\infty} (h(x)w(x))^2 q(x) \\, dx$$\nLet's substitute the expressions for $h(x)$, $w(x)$, and $q(x)$:\n$$(h(x)w(x))^2 = \\left(x^2 \\cdot 2 \\exp\\left(-\\frac{3x^2}{8}\\right)\\right)^2 = 4x^4 \\exp\\left(-2 \\cdot \\frac{3x^2}{8}\\right) = 4x^4 \\exp\\left(-\\frac{3x^2}{4}\\right)$$\nNow, we place this into the integral with $q(x)$:\n$$\\mathbb{E}_{q}\\!\\left[\\left(h(X)\\,w(X)\\right)^2\\right] = \\int_{-\\infty}^{\\infty} \\left(4x^4 \\exp\\left(-\\frac{3x^2}{4}\\right)\\right) \\left(\\frac{1}{2\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{8}\\right)\\right) \\, dx$$\nCombine the terms:\n$$= \\frac{4}{2\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} x^4 \\exp\\left(-\\frac{3x^2}{4} - \\frac{x^2}{8}\\right) \\, dx$$\n$$= \\frac{2}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} x^4 \\exp\\left(-\\frac{6x^2+x^2}{8}\\right) \\, dx = \\frac{2}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} x^4 \\exp\\left(-\\frac{7x^2}{8}\\right) \\, dx$$\nThis integral can be solved using the general formula for moments of a centered Gaussian distribution. Let's consider an unnormalized Gaussian density proportional to $\\exp(-ax^2)$, with $a = 7/8$. The fourth moment of a $\\mathcal{N}(0, \\sigma^2)$ distribution is $3\\sigma^4$. The variance of this related Gaussian is found by setting $a = 1/(2\\sigma^2)$, which gives $\\sigma^2 = 1/(2a) = 1/(2 \\cdot 7/8) = 4/7$.\nThe integral $\\int_{-\\infty}^{\\infty} x^4 \\exp(-ax^2)dx$ can be solved using the identity $\\int_{-\\infty}^{\\infty} x^{2n} \\exp(-ax^2)dx = \\frac{(2n-1)!!}{2^n a^n}\\sqrt{\\frac{\\pi}{a}}$.\nHere, $2n=4 \\implies n=2$ and $a=7/8$.\n$$\\int_{-\\infty}^{\\infty} x^4 \\exp\\left(-\\frac{7x^2}{8}\\right) \\, dx = \\frac{(2(2)-1)!!}{2^2 (7/8)^2}\\sqrt{\\frac{\\pi}{7/8}} = \\frac{3!!}{4(49/64)}\\sqrt{\\frac{8\\pi}{7}} = \\frac{3}{49/16}\\sqrt{\\frac{8\\pi}{7}} = \\frac{48}{49}\\sqrt{\\frac{8\\pi}{7}}$$\nMultiplying by the constant $\\frac{2}{\\sqrt{2\\pi}}$ from before:\n$$\\mathbb{E}_{q}\\!\\left[\\left(h(X)\\,w(X)\\right)^2\\right] = \\frac{2}{\\sqrt{2\\pi}} \\cdot \\frac{48}{49}\\sqrt{\\frac{8\\pi}{7}} = \\frac{96}{49} \\sqrt{\\frac{8\\pi}{2\\pi \\cdot 7}} = \\frac{96}{49} \\sqrt{\\frac{4}{7}} = \\frac{96}{49} \\cdot \\frac{2}{\\sqrt{7}} = \\frac{192}{49\\sqrt{7}}$$\nFinally, we compute the variance:\n$$\\mathrm{Var}_{q}\\!\\left(h(X)\\,w(X)\\right) = \\frac{192}{49\\sqrt{7}} - 1^2 = \\frac{192}{49\\sqrt{7}} - 1$$\nThis is the third requested expression.\n\nThe three requested expressions are:\n1. $w(x) = 2 \\exp\\left(-\\frac{3x^2}{8}\\right)$\n2. $\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right] = 1$\n3. $\\mathrm{Var}_{q}\\!\\left(h(X)\\,w(X)\\right) = \\frac{192}{49\\sqrt{7}} - 1$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 \\exp\\left(-\\frac{3x^2}{8}\\right)  1  \\frac{192}{49\\sqrt{7}} - 1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "We conclude our practices by tackling one of the most significant challenges in modern computational statistics: the curse of dimensionality. This advanced exercise () guides you through an analysis of how SNIS performance behaves in high-dimensional settings, using the Effective Sample Size (ESS) as a critical diagnostic tool. By deriving the direct relationship between the ESS, the problem's dimension $d$, and the spectral properties of the target covariance matrix, you will gain a deep, quantitative insight into why importance sampling can fail and how to diagnose this potential collapse in efficiency.",
            "id": "3338598",
            "problem": "Consider Self-Normalized Importance Sampling (SNIS) for estimating expectations under a $d$-dimensional Gaussian target with density $\\pi(x) = \\mathcal{N}(0,\\Sigma)$ using a Gaussian proposal $q(x) = \\mathcal{N}(0,I)$, where $\\Sigma \\in \\mathbb{R}^{d \\times d}$ is symmetric positive definite, $I$ is the $d \\times d$ identity matrix, and $x \\in \\mathbb{R}^{d}$. Let the unnormalized importance weight be $w(x) = \\pi(x)/q(x)$ and the normalized weight be $\\tilde{w}_{i} = w(x_{i}) / \\sum_{j=1}^{n} w(x_{j})$ for independent and identically distributed samples $x_{1},\\dots,x_{n} \\sim q$. The Effective Sample Size (ESS) of the SNIS weights is defined by $\\mathrm{ESS} = \\left(\\sum_{i=1}^{n} w(x_{i})\\right)^{2} \\big/ \\sum_{i=1}^{n} w(x_{i})^{2}$. Your task is to analyze the high-dimensional behavior of the normalized ESS fraction $\\mathrm{ESS}/n$ as a function of the spectrum of $\\Sigma$ and the dimension $d$.\n\nYou must proceed from first principles and well-tested facts only. In particular, you may rely on the Law of Large Numbers (LLN) and Gaussian integral identities for quadratic forms, but you must not assume any pre-derived formula for $\\mathrm{ESS}/n$.\n\nTasks:\n- Derive, starting from the definition of SNIS and the LLN, a closed-form expression for the almost sure limit of $\\mathrm{ESS}/n$ as $n \\to \\infty$ in terms of $\\mathbb{E}_{q}[w(X)]$ and $\\mathbb{E}_{q}[w(X)^{2}]$, provided these expectations are finite.\n- Specialize to the Gaussian case $\\pi = \\mathcal{N}(0,\\Sigma)$ and $q=\\mathcal{N}(0,I)$, and evaluate $\\mathbb{E}_{q}[w(X)]$ and $\\mathbb{E}_{q}[w(X)^{2}]$ using only fundamental Gaussian integral identities for quadratic forms. Express the limit of $\\mathrm{ESS}/n$ as a function of the eigenvalues $\\{\\lambda_{i}\\}_{i=1}^{d}$ of $\\Sigma$.\n- Identify the spectral condition under which $\\mathbb{E}_{q}[w(X)^{2}]$ is finite, and characterize the corresponding phase transition of the ESS fraction as $d$ grows. In particular, determine how the product over the spectrum drives the limit to zero or keeps it bounded, and justify any boundary behavior.\n- Implement a program that, for a given finite set of spectra and dimensions, computes the theoretical limit of $\\mathrm{ESS}/n$ derived above. You must not simulate; compute the limit in closed form using the eigenvalues.\n\nAngle units are not relevant. There are no physical units.\n\nTest suite and output specification:\n- Use the following five test cases, each specified by a dimension $d$ and a multiset of eigenvalues of $\\Sigma$ given in block form $(\\lambda,\\text{count})$:\n  $1.$ $d = 10$, spectrum $\\{(1,10)\\}$.\n  $2.$ $d = 10$, spectrum $\\{(1/2,10)\\}$.\n  $3.$ $d = 20$, spectrum $\\{(4/5,10),(6/5,10)\\}$.\n  $4.$ $d = 10$, spectrum $\\{(2,1),(1,9)\\}$.\n  $5.$ $d = 5$, spectrum $\\{(5/2,1),(1,4)\\}$.\n- For each test case, compute the theoretical limit of $\\mathrm{ESS}/n$ as $n \\to \\infty$ derived from your analysis. If the limit is zero due to divergence of $\\mathbb{E}_{q}[w(X)^{2}]$ or a zero factor, return $0$.\n- Your program should produce a single line of output containing the five results as a comma-separated list of floating-point numbers rounded to six decimal places and enclosed in square brackets (e.g., \"[$r_{1},r_{2},r_{3},r_{4},r_{5}$]\").",
            "solution": "The problem requires an analysis of the asymptotic behavior of the Effective Sample Size (ESS) for Self-Normalized Importance Sampling (SNIS) in a high-dimensional Gaussian setting. We must first derive a closed-form expression for the limit of the normalized ESS fraction, $\\mathrm{ESS}/n$, as the number of samples $n$ approaches infinity. Then, this expression must be evaluated for the specific target and proposal distributions, and finally used to compute results for a given set of test cases.\n\nLet us begin by establishing the theoretical foundation.\n\n**Step 1: Asymptotic Limit of the ESS Fraction**\n\nThe Effective Sample Size is defined as:\n$$ \\mathrm{ESS} = \\frac{\\left(\\sum_{i=1}^{n} w(x_{i})\\right)^{2}}{\\sum_{i=1}^{n} w(x_{i})^{2}} $$\nwhere $w(x_i)$ are the unnormalized importance weights for a set of i.i.d. samples $x_1, \\dots, x_n$ drawn from the proposal distribution $q(x)$. We are interested in the almost sure limit of the normalized ESS fraction, $\\mathrm{ESS}/n$, as $n \\to \\infty$. We can rewrite the expression as:\n$$ \\frac{\\mathrm{ESS}}{n} = \\frac{1}{n} \\frac{\\left(\\sum_{i=1}^{n} w(x_{i})\\right)^{2}}{\\sum_{i=1}^{n} w(x_{i})^{2}} = \\frac{\\left(\\frac{1}{n}\\sum_{i=1}^{n} w(x_{i})\\right)^{2}}{\\frac{1}{n}\\sum_{i=1}^{n} w(x_{i})^{2}} $$\nThe samples $X_1, \\dots, X_n$ are drawn i.i.d. from $q$, so the weights $W_i = w(X_i)$ also form a sequence of i.i.d. random variables. According to the strong Law of Large Numbers (LLN), the sample means of $W$ and $W^2$ converge almost surely to their respective expectations, provided these expectations are finite:\n$$ \\frac{1}{n}\\sum_{i=1}^{n} w(X_{i}) \\xrightarrow{\\text{a.s.}} \\mathbb{E}_{q}[w(X)] \\quad \\text{as } n \\to \\infty $$\n$$ \\frac{1}{n}\\sum_{i=1}^{n} w(X_{i})^{2} \\xrightarrow{\\text{a.s.}} \\mathbb{E}_{q}[w(X)^{2}] \\quad \\text{as } n \\to \\infty $$\nBy the continuous mapping theorem, the ratio of these quantities converges to the ratio of their limits, assuming the limit of the denominator is non-zero. Therefore, the almost sure limit of the ESS fraction is:\n$$ \\lim_{n \\to \\infty} \\frac{\\mathrm{ESS}}{n} \\xrightarrow{\\text{a.s.}} \\frac{\\left(\\mathbb{E}_{q}[w(X)]\\right)^{2}}{\\mathbb{E}_{q}[w(X)^{2}]} $$\nThis result holds if $\\mathbb{E}_{q}[w(X)^2]$ is finite. If $\\mathbb{E}_{q}[w(X)^2]$ is infinite, the variance of the weights is infinite, which leads to a collapse of the estimator's efficiency, and the limit of $\\mathrm{ESS}/n$ is $0$.\n\n**Step 2: Evaluation of Expectations for the Gaussian Case**\n\nThe target and proposal distributions are given by:\n-   Target $\\pi(x) = \\mathcal{N}(x | 0, \\Sigma) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} x^T \\Sigma^{-1} x\\right)$\n-   Proposal $q(x) = \\mathcal{N}(x | 0, I) = \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} x^T x\\right)$\nwhere $|\\Sigma| = \\det(\\Sigma)$. The unnormalized weight function $w(x) = \\pi(x)/q(x)$ is:\n$$ w(x) = \\frac{1}{|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} x^T \\Sigma^{-1} x + \\frac{1}{2} x^T x\\right) = \\frac{1}{|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} x^T (\\Sigma^{-1} - I) x\\right) $$\nFirst, we compute the expectation of the weight, $\\mathbb{E}_{q}[w(X)]$:\n$$ \\mathbb{E}_{q}[w(X)] = \\int_{\\mathbb{R}^d} w(x) q(x) dx = \\int_{\\mathbb{R}^d} \\frac{\\pi(x)}{q(x)} q(x) dx = \\int_{\\mathbb{R}^d} \\pi(x) dx = 1 $$\nThis is a general property, as $\\pi(x)$ is a probability density function. Thus, $(\\mathbb{E}_{q}[w(X)])^2 = 1^2 = 1$. The asymptotic ESS fraction simplifies to:\n$$ \\lim_{n \\to \\infty} \\frac{\\mathrm{ESS}}{n} = \\frac{1}{\\mathbb{E}_{q}[w(X)^{2}]} $$\nNext, we compute the second moment of the weight, $\\mathbb{E}_{q}[w(X)^2]$:\n$$ \\mathbb{E}_{q}[w(X)^{2}] = \\int_{\\mathbb{R}^d} w(x)^2 q(x) dx $$\nThe term $w(x)^2 q(x)$ is:\n$$ w(x)^2 q(x) = \\left(\\frac{1}{|\\Sigma|^{1/2}} \\exp\\left[-\\frac{1}{2} x^T (\\Sigma^{-1} - I) x\\right]\\right)^2 \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} x^T x\\right) $$\n$$ = \\frac{1}{|\\Sigma|} \\frac{1}{(2\\pi)^{d/2}} \\exp\\left[-x^T (\\Sigma^{-1} - I) x - \\frac{1}{2} x^T x\\right] $$\n$$ = \\frac{1}{|\\Sigma|(2\\pi)^{d/2}} \\exp\\left[-\\frac{1}{2} x^T (2(\\Sigma^{-1} - I) + I) x\\right] $$\n$$ = \\frac{1}{|\\Sigma|(2\\pi)^{d/2}} \\exp\\left[-\\frac{1}{2} x^T (2\\Sigma^{-1} - I) x\\right] $$\nLet the matrix $A = 2\\Sigma^{-1} - I$. The expectation is then:\n$$ \\mathbb{E}_{q}[w(X)^{2}] = \\frac{1}{|\\Sigma|} \\int_{\\mathbb{R}^d} \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} x^T A x\\right) dx $$\nThe integral is related to the normalization constant of a Gaussian distribution. Using the identity $\\int \\exp(-\\frac{1}{2}x^T A x) dx = (2\\pi)^{d/2} |A|^{-1/2}$ for a positive definite matrix $A$, we get:\n$$ \\mathbb{E}_{q}[w(X)^{2}] = \\frac{1}{|\\Sigma|} |A|^{-1/2} = \\frac{1}{|\\Sigma| |2\\Sigma^{-1} - I|^{1/2}} $$\nLet $\\{\\lambda_i\\}_{i=1}^d$ be the eigenvalues of $\\Sigma$. The determinant is the product of eigenvalues, so $|\\Sigma| = \\prod_{i=1}^d \\lambda_i$. The eigenvalues of $2\\Sigma^{-1} - I$ are $2/\\lambda_i - 1$. Thus, its determinant is $|2\\Sigma^{-1} - I| = \\prod_{i=1}^d (2/\\lambda_i - 1)$.\nSubstituting this into the expression for the expectation:\n$$ |2\\Sigma^{-1} - I|^{1/2} = \\left(\\prod_{i=1}^d \\frac{2-\\lambda_i}{\\lambda_i}\\right)^{1/2} = \\frac{\\sqrt{\\prod(2-\\lambda_i)}}{\\sqrt{\\prod\\lambda_i}} = \\frac{|\\sqrt{2I - \\Sigma}|}{|\\Sigma|^{1/2}} $$\nSo,\n$$ \\mathbb{E}_{q}[w(X)^{2}] = \\frac{1}{|\\Sigma| \\frac{|\\sqrt{2I - \\Sigma}|}{|\\Sigma|^{1/2}}} = \\frac{1}{|\\Sigma|^{1/2} |\\sqrt{2I - \\Sigma}|} = \\frac{1}{\\sqrt{|\\Sigma||2I - \\Sigma|}} $$\n$$ = \\frac{1}{\\sqrt{\\prod \\lambda_i \\prod(2-\\lambda_i)}} = \\left(\\prod_{i=1}^d \\lambda_i(2-\\lambda_i)\\right)^{-1/2} $$\nFinally, the asymptotic ESS fraction is:\n$$ \\lim_{n \\to \\infty} \\frac{\\mathrm{ESS}}{n} = \\frac{1}{\\mathbb{E}_{q}[w(X)^{2}]} = \\sqrt{\\prod_{i=1}^d \\lambda_i(2-\\lambda_i)} $$\n\n**Step 3: Condition for Finiteness and Phase Transition**\n\nThe integral for $\\mathbb{E}_{q}[w(X)^2]$ converges if and only if the matrix $A = 2\\Sigma^{-1} - I$ is symmetric positive definite. Since $\\Sigma$ is symmetric, $A$ is symmetric. For it to be positive definite, all its eigenvalues must be positive. The eigenvalues of $A$ are $2/\\lambda_i - 1$.\n$$ \\frac{2}{\\lambda_i} - 1  0 \\implies \\frac{2}{\\lambda_i}  1 $$\nSince $\\Sigma$ is positive definite, its eigenvalues $\\lambda_i$ are all positive. Thus, we can multiply by $\\lambda_i$ without changing the inequality:\n$$ 2  \\lambda_i $$\nThe condition for a finite second moment of the weights, and thus a non-zero asymptotic ESS fraction, is that all eigenvalues of $\\Sigma$ must be strictly less than $2$.\n$$ 0  \\lambda_i  2 \\quad \\text{for all } i \\in \\{1, \\dots, d\\} $$\nIf any eigenvalue $\\lambda_k \\ge 2$, then $\\mathbb{E}_{q}[w(X)^2]$ diverges, and the asymptotic ESS fraction collapses to $0$. This marks a phase transition. The function $f(\\lambda) = \\lambda(2-\\lambda)$ is maximized at $\\lambda=1$ with $f(1) = 1$, and is less than $1$ for any other $\\lambda \\in (0, 2)$. The asymptotic ESS fraction, being a product of $d$ such terms (under a square root), will decay exponentially to $0$ as $d \\to \\infty$ unless all $\\lambda_i$ are exactly $1$. This illustrates the curse of dimensionality in importance sampling.\n\n**Computational Formula**\n\nFor a given spectrum $\\{\\lambda_i\\}_{i=1}^d$ of $\\Sigma$:\n1.  If there exists any eigenvalue $\\lambda_k \\ge 2$, the limit of $\\mathrm{ESS}/n$ is $0$.\n2.  Otherwise, the limit is $\\sqrt{\\prod_{i=1}^d (\\lambda_i(2-\\lambda_i))}$.\nThis formula will be used to compute the results for the test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the theoretical limit of ESS/n for the given test cases.\n\n    The problem asks for the analysis of the Self-Normalized Importance Sampling (SNIS)\n    Effective Sample Size (ESS) for a Gaussian target and proposal. The asymptotic limit\n    of the normalized ESS fraction ESS/n as n - infinity is given by:\n\n        lim (ESS/n) = (E_q[w(X)])^2 / E_q[w(X)^2]\n\n    For the specified Gaussian distributions pi = N(0, Sigma) and q = N(0, I),\n    this limit can be derived in closed form. E_q[w(X)] is always 1. The main\n    challenge is E_q[w(X)^2], which is finite only if all eigenvalues of Sigma,\n    lambda_i, satisfy 0  lambda_i  2.\n\n    The final formula for the limit in terms of the eigenvalues {lambda_i} is:\n\n        lim (ESS/n) = sqrt(product_{i=1 to d} [lambda_i * (2 - lambda_i)])\n\n    If any lambda_i = 2, the second moment of the weights diverges, and the\n    asymptotic ESS/n is 0.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (d, spectrum as list of (eigenvalue, count))\n        (10, [(1.0, 10)]),\n        (10, [(1.0/2.0, 10)]),\n        (20, [(4.0/5.0, 10), (6.0/5.0, 10)]),\n        (10, [(2.0, 1), (1.0, 9)]),\n        (5, [(5.0/2.0, 1), (1.0, 4)]),\n    ]\n\n    results = []\n    for d, spectrum in test_cases:\n        \n        product_val = 1.0\n        is_divergent = False\n\n        for val, count in spectrum:\n            if val = 2.0:\n                is_divergent = True\n                break\n            # Since Sigma is positive definite, val  0 is implied.\n\n            # We calculate the term lambda * (2 - lambda) for each eigenvalue\n            term = val * (2.0 - val)\n            \n            # The total product is the product of these terms, raised to the\n            # power of their multiplicity.\n            product_val *= (term ** count)\n\n        if is_divergent:\n            result = 0.0\n        else:\n            result = np.sqrt(product_val)\n\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}