{
    "hands_on_practices": [
        {
            "introduction": "Our first practice lays the groundwork for all that follows. Here, we move beyond abstract definitions to compute the essential components of an importance sampling estimator for a concrete, non-trivial case where the target and proposal distributions differ . By explicitly deriving the weight function and key statistical moments, you will solidify your understanding of how importance sampling re-weights expectations from one distribution to another.",
            "id": "3338601",
            "problem": "Consider Self-Normalized Importance Sampling (SNIS) within the broader context of Monte Carlo (MC) methods. Let the target distribution be the standard normal density $p=\\mathcal{N}(0,1)$ and the proposal distribution be the zero-mean normal with variance $4$, $q=\\mathcal{N}(0,4)$. Define the integrand $h(x)=x^{2}$. Starting from the foundational definitions of importance sampling and the relationship between expectations under $p$ and $q$ via the Radonâ€“Nikodym derivative, determine the following:\n\n1. The unnormalized importance weight $w(x)$ associated with sampling from $q$ to estimate expectations under $p$ for the integrand $h(x)$.\n2. The expectation $\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right]$.\n3. The variance $\\mathrm{Var}_{q}\\!\\left(h(X)\\,w(X)\\right)$ expressed in closed form.\n\nUse only well-established facts about Gaussian probability density functions and Gaussian integral identities in your derivations. Provide exact expressions; do not approximate or round. Express your final answer as a single row matrix containing the three requested expressions in the order given and do not include any explanatory text in the final answer.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of Monte Carlo methods, is well-posed with all necessary information provided, and is stated using objective, formal language. We can proceed with the solution.\n\nThe problem asks for three quantities related to an importance sampling setup. The target distribution is the standard normal, $p=\\mathcal{N}(0,1)$, with probability density function (PDF) $p(x)$. The proposal distribution is a zero-mean normal with variance $4$, $q=\\mathcal{N}(0,4)$, with PDF $q(x)$. The integrand is $h(x)=x^2$.\n\nThe PDF for the target distribution $p(x)$ is:\n$$p(x) = \\frac{1}{\\sqrt{2\\pi \\cdot 1}} \\exp\\left(-\\frac{x^2}{2 \\cdot 1}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$$\nThe PDF for the proposal distribution $q(x)$ is:\n$$q(x) = \\frac{1}{\\sqrt{2\\pi \\cdot 4}} \\exp\\left(-\\frac{x^2}{2 \\cdot 4}\\right) = \\frac{1}{2\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{8}\\right)$$\n\n**1. The unnormalized importance weight $w(x)$**\n\nThe unnormalized importance weight, $w(x)$, is defined as the Radon-Nikodym derivative of the target measure with respect to the proposal measure, which for absolutely continuous distributions is the ratio of their PDFs:\n$$w(x) = \\frac{p(x)}{q(x)}$$\nSubstituting the expressions for the PDFs:\n$$w(x) = \\frac{\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)}{\\frac{1}{2\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{8}\\right)}$$\nThe constant terms simplify:\n$$w(x) = 2 \\frac{\\exp\\left(-\\frac{x^2}{2}\\right)}{\\exp\\left(-\\frac{x^2}{8}\\right)} = 2 \\exp\\left(-\\frac{x^2}{2} - \\left(-\\frac{x^2}{8}\\right)\\right)$$\n$$w(x) = 2 \\exp\\left(-\\frac{4x^2}{8} + \\frac{x^2}{8}\\right) = 2 \\exp\\left(-\\frac{3x^2}{8}\\right)$$\nThis is the first requested expression.\n\n**2. The expectation $\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right]$**\n\nThe expectation of the product $h(X)w(X)$ is computed with respect to the proposal distribution $q$. By definition of expectation:\n$$\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right] = \\int_{-\\infty}^{\\infty} h(x) w(x) q(x) \\, dx$$\nBy substituting $w(x) = p(x)/q(x)$, we see a fundamental identity of importance sampling:\n$$\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right] = \\int_{-\\infty}^{\\infty} h(x) \\frac{p(x)}{q(x)} q(x) \\, dx = \\int_{-\\infty}^{\\infty} h(x) p(x) \\, dx$$\nThis integral is, by definition, the expectation of $h(X)$ with respect to the target distribution $p$:\n$$\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right] = \\mathbb{E}_{p}\\!\\left[h(X)\\right]$$\nGiven $h(x) = x^2$ and $p = \\mathcal{N}(0,1)$, we are calculating the second moment of a standard normal random variable. For a random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the second moment is $\\mathbb{E}[X^2] = \\mathrm{Var}(X) + (\\mathbb{E}[X])^2 = \\sigma^2 + \\mu^2$. For the standard normal distribution, $\\mu = 0$ and $\\sigma^2 = 1$.\nTherefore:\n$$\\mathbb{E}_{p}\\!\\left[X^2\\right] = 1 + 0^2 = 1$$\nSo, the second requested value is $\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right] = 1$.\n\n**3. The variance $\\mathrm{Var}_{q}\\!\\left(h(X)\\,w(X)\\right)$**\n\nThe variance of a random variable $Y$ is given by $\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$. Let $Y = h(X)w(X)$. We need to calculate:\n$$\\mathrm{Var}_{q}\\!\\left(h(X)\\,w(X)\\right) = \\mathbb{E}_{q}\\!\\left[\\left(h(X)\\,w(X)\\right)^2\\right] - \\left(\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right]\\right)^2$$\nWe already found that $\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right] = 1$, so the second term is $1^2=1$. We now compute the first term, $\\mathbb{E}_{q}\\!\\left[\\left(h(X)\\,w(X)\\right)^2\\right]$:\n$$\\mathbb{E}_{q}\\!\\left[\\left(h(X)\\,w(X)\\right)^2\\right] = \\int_{-\\infty}^{\\infty} (h(x)w(x))^2 q(x) \\, dx$$\nLet's substitute the expressions for $h(x)$, $w(x)$, and $q(x)$:\n$$(h(x)w(x))^2 = \\left(x^2 \\cdot 2 \\exp\\left(-\\frac{3x^2}{8}\\right)\\right)^2 = 4x^4 \\exp\\left(-2 \\cdot \\frac{3x^2}{8}\\right) = 4x^4 \\exp\\left(-\\frac{3x^2}{4}\\right)$$\nNow, we place this into the integral with $q(x)$:\n$$\\mathbb{E}_{q}\\!\\left[\\left(h(X)\\,w(X)\\right)^2\\right] = \\int_{-\\infty}^{\\infty} \\left(4x^4 \\exp\\left(-\\frac{3x^2}{4}\\right)\\right) \\left(\\frac{1}{2\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{8}\\right)\\right) \\, dx$$\nCombine the terms:\n$$= \\frac{4}{2\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} x^4 \\exp\\left(-\\frac{3x^2}{4} - \\frac{x^2}{8}\\right) \\, dx$$\n$$= \\frac{2}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} x^4 \\exp\\left(-\\frac{6x^2+x^2}{8}\\right) \\, dx = \\frac{2}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} x^4 \\exp\\left(-\\frac{7x^2}{8}\\right) \\, dx$$\nThis integral can be solved using the general formula for moments of a centered Gaussian distribution. Let's consider an unnormalized Gaussian density proportional to $\\exp(-ax^2)$, with $a = 7/8$. The fourth moment of a $\\mathcal{N}(0, \\sigma^2)$ distribution is $3\\sigma^4$. The variance of this related Gaussian is found by setting $a = 1/(2\\sigma^2)$, which gives $\\sigma^2 = 1/(2a) = 1/(2 \\cdot 7/8) = 4/7$.\nThe integral $\\int_{-\\infty}^{\\infty} x^4 \\exp(-ax^2)dx$ can be solved using the identity $\\int_{-\\infty}^{\\infty} x^{2n} \\exp(-ax^2)dx = \\frac{(2n-1)!!}{2^n a^n}\\sqrt{\\frac{\\pi}{a}}$.\nHere, $2n=4 \\implies n=2$ and $a=7/8$.\n$$\\int_{-\\infty}^{\\infty} x^4 \\exp\\left(-\\frac{7x^2}{8}\\right) \\, dx = \\frac{(2(2)-1)!!}{2^2 (7/8)^2}\\sqrt{\\frac{\\pi}{7/8}} = \\frac{3!!}{4(49/64)}\\sqrt{\\frac{8\\pi}{7}} = \\frac{3}{49/16}\\sqrt{\\frac{8\\pi}{7}} = \\frac{48}{49}\\sqrt{\\frac{8\\pi}{7}}$$\nMultiplying by the constant $\\frac{2}{\\sqrt{2\\pi}}$ from before:\n$$\\mathbb{E}_{q}\\!\\left[\\left(h(X)\\,w(X)\\right)^2\\right] = \\frac{2}{\\sqrt{2\\pi}} \\cdot \\frac{48}{49}\\sqrt{\\frac{8\\pi}{7}} = \\frac{96}{49} \\sqrt{\\frac{8\\pi}{2\\pi \\cdot 7}} = \\frac{96}{49} \\sqrt{\\frac{4}{7}} = \\frac{96}{49} \\cdot \\frac{2}{\\sqrt{7}} = \\frac{192}{49\\sqrt{7}}$$\nFinally, we compute the variance:\n$$\\mathrm{Var}_{q}\\!\\left(h(X)\\,w(X)\\right) = \\frac{192}{49\\sqrt{7}} - 1^2 = \\frac{192}{49\\sqrt{7}} - 1$$\nThis is the third requested expression.\n\nThe three requested expressions are:\n1. $w(x) = 2 \\exp\\left(-\\frac{3x^2}{8}\\right)$\n2. $\\mathbb{E}_{q}\\!\\left[h(X)\\,w(X)\\right] = 1$\n3. $\\mathrm{Var}_{q}\\!\\left(h(X)\\,w(X)\\right) = \\frac{192}{49\\sqrt{7}} - 1$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 \\exp\\left(-\\frac{3x^2}{8}\\right)  1  \\frac{192}{49\\sqrt{7}} - 1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A successful importance sampling scheme depends critically on the choice of the proposal distribution. This exercise explores what happens when that choice goes wrong, specifically when the proposal has lighter tails than the target . You will investigate the moments of the weight distribution to understand the mathematical origins of estimator instability and the phenomenon of 'weight collapse', a crucial lesson in designing robust Monte Carlo methods.",
            "id": "3338565",
            "problem": "Consider a one-dimensional target probability density $p(x)$ on $\\mathbb{R}$ that is strictly positive and satisfies a stretched sub-exponential tail condition: there exist constants $a0$ and $\\beta \\in (0,1)$ such that\n$$\np(x) \\propto \\exp\\!\\big(-a\\,|x|^{\\beta}\\big) \\quad \\text{as } |x|\\to\\infty,\n$$\nwith a finite normalization constant. Let the proposal density $q(x)$ be the Gaussian density with zero mean and variance $\\sigma^{2}  0$, i.e.,\n$$\nq(x) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\Big(-\\frac{x^{2}}{2\\sigma^{2}}\\Big).\n$$\nDefine the importance weight $w(x) = \\frac{p(x)}{q(x)}$ and the self-normalized weight for a sample $\\{X_{i}\\}_{i=1}^{n}$ drawn independently from $q$ as $\\tilde{w}_{i} = \\frac{w(X_{i})}{\\sum_{j=1}^{n} w(X_{j})}$. For $\\lambda \\in \\mathbb{R}$, define the function\n$$\n\\psi(\\lambda) = \\mathbb{E}_{q}\\!\\left[\\exp\\!\\big(\\lambda \\log w(X)\\big)\\right] = \\mathbb{E}_{q}\\!\\left[w(X)^{\\lambda}\\right].\n$$\n\nStarting from first principles in Monte Carlo importance sampling, and using only basic properties of integrability and asymptotic comparison of functions, determine the exact value of the supremum\n$$\n\\lambda^{\\star} = \\sup\\Big\\{\\lambda \\in \\mathbb{R} : \\psi(\\lambda)  \\infty\\Big\\}.\n$$\n\nExplain the implications of the finiteness range of $\\psi(\\lambda)$ for the right-tail behavior of the self-normalized weights $\\tilde{w}_{i}$, focusing on whether the heavy right-tail of the unnormalized weights $w(X)$ can cause a single $\\tilde{w}_{i}$ to concentrate near $1$ with non-negligible probability as $n$ increases, and the consequences for stability of self-normalized importance sampling estimators. Your final answer must be the single real value of $\\lambda^{\\star}$; no rounding is required.",
            "solution": "The problem is evaluated as valid. It is scientifically grounded in the theory of Monte Carlo methods, is well-posed, objective, and contains all necessary information to derive a unique solution.\n\nThe primary task is to determine the value of $\\lambda^{\\star} = \\sup\\Big\\{\\lambda \\in \\mathbb{R} : \\psi(\\lambda)  \\infty\\Big\\}$, where $\\psi(\\lambda) = \\mathbb{E}_{q}\\!\\left[w(X)^{\\lambda}\\right]$.\n\nBy definition of expectation, $\\psi(\\lambda)$ is given by the integral:\n$$\n\\psi(\\lambda) = \\mathbb{E}_{q}\\!\\left[w(X)^{\\lambda}\\right] = \\int_{-\\infty}^{\\infty} w(x)^{\\lambda} q(x) dx.\n$$\nSubstituting the definition of the importance weight, $w(x) = \\frac{p(x)}{q(x)}$, we get:\n$$\n\\psi(\\lambda) = \\int_{-\\infty}^{\\infty} \\left(\\frac{p(x)}{q(x)}\\right)^{\\lambda} q(x) dx = \\int_{-\\infty}^{\\infty} [p(x)]^{\\lambda} [q(x)]^{1-\\lambda} dx.\n$$\nThe convergence of this integral depends on the asymptotic behavior of the integrand, $[p(x)]^{\\lambda} [q(x)]^{1-\\lambda}$, as $|x| \\to \\infty$.\n\nWe are given the asymptotic behavior of the target density $p(x)$:\n$$\np(x) \\propto \\exp\\big(-a\\,|x|^{\\beta}\\big) \\quad \\text{as } |x|\\to\\infty,\n$$\nfor constants $a0$ and $\\beta \\in (0,1)$. Let's denote this as $p(x) \\sim C_p \\exp(-a|x|^{\\beta})$ for some constant $C_p  0$.\nThe proposal density $q(x)$ is a Gaussian:\n$$\nq(x) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\Big(-\\frac{x^{2}}{2\\sigma^{2}}\\Big).\n$$\nLet's analyze the asymptotic behavior of the integrand for large $|x|$:\n$$\n[p(x)]^{\\lambda} [q(x)]^{1-\\lambda} \\sim \\left(C_p \\exp(-a|x|^{\\beta})\\right)^{\\lambda} \\left(\\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\!\\Big(-\\frac{x^{2}}{2\\sigma^{2}}\\Big)\\right)^{1-\\lambda}.\n$$\nCollecting the exponential terms, the integrand is asymptotically proportional to:\n$$\n\\exp\\big(-a\\lambda|x|^{\\beta}\\big) \\exp\\left(-(1-\\lambda)\\frac{x^{2}}{2\\sigma^{2}}\\right) = \\exp\\left(-a\\lambda|x|^{\\beta} - \\frac{1-\\lambda}{2\\sigma^{2}}x^{2}\\right).\n$$\nThe convergence of the integral $\\int_{-\\infty}^{\\infty} \\exp\\left(-a\\lambda|x|^{\\beta} - \\frac{1-\\lambda}{2\\sigma^{2}}x^{2}\\right) dx$ is determined by the sign of the coefficient of the dominant term in the exponent as $|x| \\to \\infty$. The two terms in the exponent are of order $|x|^{\\beta}$ and $x^2$. Since it is given that $\\beta \\in (0,1)$, we have $2  \\beta$, which means the $x^2$ term is dominant.\n\nFor the integral to converge, the integrand must decay to zero sufficiently fast as $|x| \\to \\infty$. This requires the dominant term in the exponent to be negative and tend to $-\\infty$. The dominant term is $-\\frac{1-\\lambda}{2\\sigma^2}x^2$. Its coefficient must be negative:\n$$\n-\\frac{1-\\lambda}{2\\sigma^2}  0.\n$$\nSince $\\sigma^2  0$, this inequality is equivalent to:\n$$\n-(1-\\lambda)  0 \\implies 1-\\lambda  0 \\implies \\lambda  1.\n$$\nFor any $\\lambda  1$, the exponent is dominated by a negative quadratic term, ensuring the integral converges. Thus, $\\psi(\\lambda)  \\infty$ for all $\\lambda  1$.\n\nNow, we must analyze the boundary case $\\lambda = 1$. The integrand becomes:\n$$\n[p(x)]^{1} [q(x)]^{1-1} = p(x).\n$$\nSo, $\\psi(1) = \\int_{-\\infty}^{\\infty} p(x) dx$. The problem statement specifies that $p(x)$ is a \"probability density\" with a \"finite normalization constant.\" This implies that $\\int_{-\\infty}^{\\infty} p(x) dx$ is finite (and equals $1$ if $p(x)$ is already normalized). Therefore, $\\psi(1)  \\infty$.\n\nFinally, we consider the case $\\lambda  1$. In this case, $1-\\lambda  0$. The coefficient of the dominant $x^2$ term in the exponent, $-\\frac{1-\\lambda}{2\\sigma^2}$, becomes positive. The exponent tends to $+\\infty$ as $|x| \\to \\infty$, causing the integrand to diverge. The integral is therefore infinite. Thus, $\\psi(\\lambda) = \\infty$ for all $\\lambda  1$.\n\nIn summary, the set of $\\lambda$ for which $\\psi(\\lambda)$ is finite is $(-\\infty, 1]$. The supremum of this set is:\n$$\n\\lambda^{\\star} = \\sup\\big((-\\infty, 1]\\big) = 1.\n$$\n\nThe second part of the problem asks for the implications of this result. The value $\\lambda^{\\star}$ characterizes the right-tail behavior of the distribution of the unnormalized weights $w(X)$ when $X \\sim q$. A standard result from an advanced probability theory states that if $\\mathbb{E}[Y^{\\lambda}]  \\infty$ for $\\lambda  \\alpha$ and $\\mathbb{E}[Y^{\\lambda}] = \\infty$ for $\\lambda  \\alpha$, then the tail probability behaves as $P(Y  y) \\asymp y^{-\\alpha}$ for large $y$. In our case, with $Y=w(X)$ and $\\lambda^\\star = 1$, the distribution of weights has a very heavy tail, satisfying $P_q(w(X)  t) \\asymp t^{-1}$ as $t \\to \\infty$.\n\nThis has profound consequences for self-normalized importance sampling (SNIS):\n1.  **Infinite Variance of Weights**: The variance of the weights is given by $\\text{Var}_q(w(X)) = \\mathbb{E}_q[w(X)^2] - (\\mathbb{E}_q[w(X)])^2 = \\psi(2) - (\\psi(1))^2$. Since $\\lambda^\\star = 1$, $\\psi(\\lambda)$ is infinite for any $\\lambda  1$, including $\\lambda=2$. The variance of the weights is infinite.\n2.  **Estimator Instability**: For a random sample $\\{X_{i}\\}_{i=1}^{n}$ from $q(x)$, the sum of unnormalized weights, $S_n = \\sum_{j=1}^{n} w(X_j)$, is used to compute the self-normalized weights $\\tilde{w}_i = w(X_i)/S_n$. For distributions with infinite variance, and particularly for those with a tail index $\\lambda^\\star \\le 1$, the sum $S_n$ is asymptotically dominated by its single largest term, $w_{\\max} = \\max_{i} w(X_i)$.\n3.  **Weight Collapse**: The consequence is that as $n$ increases, there is a non-negligible probability that one of the self-normalized weights, say $\\tilde{w}_k = w(X_k)/S_n$, will be close to $1$, while all other weights $\\tilde{w}_{i}$ for $i \\neq k$ will be close to $0$. This phenomenon is known as \"weight collapse\" or \"degeneracy\". The entire sample is effectively represented by a single point.\n4.  **Unreliable Estimates**: The SNIS estimator for an expectation $\\mathbb{E}_p[f(X)]$ is $\\hat{\\mu}_n = \\sum_{i=1}^n \\tilde{w}_i f(X_i)$. When weight collapse occurs, the estimate becomes $\\hat{\\mu}_n \\approx f(X_k)$, where $X_k$ is the single point with the dominant weight. The estimate is therefore extremely unstable and its precision is abysmal; its value depends almost entirely on one random draw, $X_k$. Repeating the simulation would likely lead to a completely different dominating point and a wildly different estimate. This renders a SNIS estimator practically useless, even if it is technically consistent (i.e., it converges to the true value as $n \\to \\infty$, albeit extremely slowly).\n\nThis catastrophic failure is a hallmark of choosing a proposal density $q(x)$ with significantly lighter tails (here, Gaussian, $\\propto \\exp(-c x^2)$) than the target density $p(x)$ (here, stretched sub-exponential, $\\propto \\exp(-a |x|^\\beta)$ with $\\beta  2$). The proposal density fails to generate samples in the tail regions of the target, and when by chance it does, that single sample is assigned an enormous weight to compensate, which in turn destabilizes the entire estimation. A stable, low-variance SNIS estimator generally requires $\\lambda^\\star  2$. The result $\\lambda^\\star = 1$ signals a severe mismatch and a failed importance sampling strategy.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Many modern scientific problems are high-dimensional, a domain where Monte Carlo methods can struggle. This final practice confronts the 'curse of dimensionality' head-on by introducing the Effective Sample Size ($ESS$) as a key diagnostic for estimator efficiency . You will derive how the $ESS$ behaves in a high-dimensional Gaussian setting, revealing a sharp phase transition that can render an otherwise reasonable sampler useless as dimensionality grows.",
            "id": "3338598",
            "problem": "Consider Self-Normalized Importance Sampling (SNIS) for estimating expectations under a $d$-dimensional Gaussian target with density $\\pi(x) = \\mathcal{N}(0,\\Sigma)$ using a Gaussian proposal $q(x) = \\mathcal{N}(0,I)$, where $\\Sigma \\in \\mathbb{R}^{d \\times d}$ is symmetric positive definite, $I$ is the $d \\times d$ identity matrix, and $x \\in \\mathbb{R}^{d}$. Let the unnormalized importance weight be $w(x) = \\pi(x)/q(x)$ and the normalized weight be $\\tilde{w}_{i} = w(x_{i}) / \\sum_{j=1}^{n} w(x_{j})$ for independent and identically distributed samples $x_{1},\\dots,x_{n} \\sim q$. The Effective Sample Size (ESS) of the SNIS weights is defined by $\\mathrm{ESS} = \\left(\\sum_{i=1}^{n} w(x_{i})\\right)^{2} \\big/ \\sum_{i=1}^{n} w(x_{i})^{2}$. Your task is to analyze the high-dimensional behavior of the normalized ESS fraction $\\mathrm{ESS}/n$ as a function of the spectrum of $\\Sigma$ and the dimension $d$.\n\nYou must proceed from first principles and well-tested facts only. In particular, you may rely on the Law of Large Numbers (LLN) and Gaussian integral identities for quadratic forms, but you must not assume any pre-derived formula for $\\mathrm{ESS}/n$.\n\nTasks:\n- Derive, starting from the definition of SNIS and the LLN, a closed-form expression for the almost sure limit of $\\mathrm{ESS}/n$ as $n \\to \\infty$ in terms of $\\mathbb{E}_{q}[w(X)]$ and $\\mathbb{E}_{q}[w(X)^{2}]$, provided these expectations are finite.\n- Specialize to the Gaussian case $\\pi = \\mathcal{N}(0,\\Sigma)$ and $q=\\mathcal{N}(0,I)$, and evaluate $\\mathbb{E}_{q}[w(X)]$ and $\\mathbb{E}_{q}[w(X)^{2}]$ using only fundamental Gaussian integral identities for quadratic forms. Express the limit of $\\mathrm{ESS}/n$ as a function of the eigenvalues $\\{\\lambda_{i}\\}_{i=1}^{d}$ of $\\Sigma$.\n- Identify the spectral condition under which $\\mathbb{E}_{q}[w(X)^{2}]$ is finite, and characterize the corresponding phase transition of the ESS fraction as $d$ grows. In particular, determine how the product over the spectrum drives the limit to zero or keeps it bounded, and justify any boundary behavior.\n- Implement a program that, for a given finite set of spectra and dimensions, computes the theoretical limit of $\\mathrm{ESS}/n$ derived above. You must not simulate; compute the limit in closed form using the eigenvalues.\n\nAngle units are not relevant. There are no physical units.\n\nTest suite and output specification:\n- Use the following five test cases, each specified by a dimension $d$ and a multiset of eigenvalues of $\\Sigma$ given in block form $(\\lambda,\\text{count})$:\n  $1.$ $d = 10$, spectrum $\\{(1,10)\\}$.\n  $2.$ $d = 10$, spectrum $\\{(1/2,10)\\}$.\n  $3.$ $d = 20$, spectrum $\\{(4/5,10),(6/5,10)\\}$.\n  $4.$ $d = 10$, spectrum $\\{(2,1),(1,9)\\}$.\n  $5.$ $d = 5$, spectrum $\\{(5/2,1),(1,4)\\}$.\n- For each test case, compute the theoretical limit of $\\mathrm{ESS}/n$ as $n \\to \\infty$ derived from your analysis. If the limit is zero due to divergence of $\\mathbb{E}_{q}[w(X)^{2}]$ or a zero factor, return $0$.\n- Your program should produce a single line of output containing the five results as a comma-separated list of floating-point numbers rounded to six decimal places and enclosed in square brackets (e.g., \"[$r_{1},r_{2},r_{3},r_{4},r_{5}$]\").",
            "solution": "The problem requires an analysis of the asymptotic behavior of the Effective Sample Size (ESS) for Self-Normalized Importance Sampling (SNIS) in a high-dimensional Gaussian setting. We must first derive a closed-form expression for the limit of the normalized ESS fraction, $\\mathrm{ESS}/n$, as the number of samples $n$ approaches infinity. Then, this expression must be evaluated for the specific target and proposal distributions, and finally used to compute results for a given set of test cases.\n\nLet us begin by establishing the theoretical foundation.\n\n**Step 1: Asymptotic Limit of the ESS Fraction**\n\nThe Effective Sample Size is defined as:\n$$ \\mathrm{ESS} = \\frac{\\left(\\sum_{i=1}^{n} w(x_{i})\\right)^{2}}{\\sum_{i=1}^{n} w(x_{i})^{2}} $$\nwhere $w(x_i)$ are the unnormalized importance weights for a set of i.i.d. samples $x_1, \\dots, x_n$ drawn from the proposal distribution $q(x)$. We are interested in the almost sure limit of the normalized ESS fraction, $\\mathrm{ESS}/n$, as $n \\to \\infty$. We can rewrite the expression as:\n$$ \\frac{\\mathrm{ESS}}{n} = \\frac{1}{n} \\frac{\\left(\\sum_{i=1}^{n} w(x_{i})\\right)^{2}}{\\sum_{i=1}^{n} w(x_{i})^{2}} = \\frac{\\left(\\frac{1}{n}\\sum_{i=1}^{n} w(x_{i})\\right)^{2}}{\\frac{1}{n}\\sum_{i=1}^{n} w(x_{i})^{2}} $$\nThe samples $X_1, \\dots, X_n$ are drawn i.i.d. from $q$, so the weights $W_i = w(X_i)$ also form a sequence of i.i.d. random variables. According to the strong Law of Large Numbers (LLN), the sample means of $W$ and $W^2$ converge almost surely to their respective expectations, provided these expectations are finite:\n$$ \\frac{1}{n}\\sum_{i=1}^{n} w(X_{i}) \\xrightarrow{\\text{a.s.}} \\mathbb{E}_{q}[w(X)] \\quad \\text{as } n \\to \\infty $$\n$$ \\frac{1}{n}\\sum_{i=1}^{n} w(X_{i})^{2} \\xrightarrow{\\text{a.s.}} \\mathbb{E}_{q}[w(X)^{2}] \\quad \\text{as } n \\to \\infty $$\nBy the continuous mapping theorem, the ratio of these quantities converges to the ratio of their limits, assuming the limit of the denominator is non-zero. Therefore, the almost sure limit of the ESS fraction is:\n$$ \\lim_{n \\to \\infty} \\frac{\\mathrm{ESS}}{n} \\xrightarrow{\\text{a.s.}} \\frac{\\left(\\mathbb{E}_{q}[w(X)]\\right)^{2}}{\\mathbb{E}_{q}[w(X)^{2}]} $$\nThis result holds if $\\mathbb{E}_{q}[w(X)^2]$ is finite. If $\\mathbb{E}_{q}[w(X)^2]$ is infinite, the variance of the weights is infinite, which leads to a collapse of the estimator's efficiency, and the limit of $\\mathrm{ESS}/n$ is $0$.\n\n**Step 2: Evaluation of Expectations for the Gaussian Case**\n\nThe target and proposal distributions are given by:\n-   Target $\\pi(x) = \\mathcal{N}(x | 0, \\Sigma) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} x^T \\Sigma^{-1} x\\right)$\n-   Proposal $q(x) = \\mathcal{N}(x | 0, I) = \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} x^T x\\right)$\nwhere $|\\Sigma| = \\det(\\Sigma)$. The unnormalized weight function $w(x) = \\pi(x)/q(x)$ is:\n$$ w(x) = \\frac{1}{|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} x^T \\Sigma^{-1} x + \\frac{1}{2} x^T x\\right) = \\frac{1}{|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} x^T (\\Sigma^{-1} - I) x\\right) $$\nFirst, we compute the expectation of the weight, $\\mathbb{E}_{q}[w(X)]$:\n$$ \\mathbb{E}_{q}[w(X)] = \\int_{\\mathbb{R}^d} w(x) q(x) dx = \\int_{\\mathbb{R}^d} \\frac{\\pi(x)}{q(x)} q(x) dx = \\int_{\\mathbb{R}^d} \\pi(x) dx = 1 $$\nThis is a general property, as $\\pi(x)$ is a probability density function. Thus, $(\\mathbb{E}_{q}[w(X)])^2 = 1^2 = 1$. The asymptotic ESS fraction simplifies to:\n$$ \\lim_{n \\to \\infty} \\frac{\\mathrm{ESS}}{n} = \\frac{1}{\\mathbb{E}_{q}[w(X)^{2}]} $$\nNext, we compute the second moment of the weight, $\\mathbb{E}_{q}[w(X)^2]$:\n$$ \\mathbb{E}_{q}[w(X)^{2}] = \\int_{\\mathbb{R}^d} w(x)^2 q(x) dx $$\nThe term $w(x)^2 q(x)$ is:\n$$ w(x)^2 q(x) = \\left(\\frac{1}{|\\Sigma|^{1/2}} \\exp\\left[-\\frac{1}{2} x^T (\\Sigma^{-1} - I) x\\right]\\right)^2 \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} x^T x\\right) $$\n$$ = \\frac{1}{|\\Sigma|} \\frac{1}{(2\\pi)^{d/2}} \\exp\\left[-x^T (\\Sigma^{-1} - I) x - \\frac{1}{2} x^T x\\right] $$\n$$ = \\frac{1}{|\\Sigma|(2\\pi)^{d/2}} \\exp\\left[-\\frac{1}{2} x^T (2(\\Sigma^{-1} - I) + I) x\\right] $$\n$$ = \\frac{1}{|\\Sigma|(2\\pi)^{d/2}} \\exp\\left[-\\frac{1}{2} x^T (2\\Sigma^{-1} - I) x\\right] $$\nLet the matrix $A = 2\\Sigma^{-1} - I$. The expectation is then:\n$$ \\mathbb{E}_{q}[w(X)^{2}] = \\frac{1}{|\\Sigma|} \\int_{\\mathbb{R}^d} \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} x^T A x\\right) dx $$\nThe integral is related to the normalization constant of a Gaussian distribution. Using the identity $\\int \\exp(-\\frac{1}{2}x^T A x) dx = (2\\pi)^{d/2} |A|^{-1/2}$ for a positive definite matrix $A$, we get:\n$$ \\mathbb{E}_{q}[w(X)^{2}] = \\frac{1}{|\\Sigma|} |A|^{-1/2} = \\frac{1}{|\\Sigma| |2\\Sigma^{-1} - I|^{1/2}} $$\nLet $\\{\\lambda_i\\}_{i=1}^d$ be the eigenvalues of $\\Sigma$. The determinant is the product of eigenvalues, so $|\\Sigma| = \\prod_{i=1}^d \\lambda_i$. The eigenvalues of $2\\Sigma^{-1} - I$ are $2/\\lambda_i - 1$. Thus, its determinant is $|2\\Sigma^{-1} - I| = \\prod_{i=1}^d (2/\\lambda_i - 1)$.\nSubstituting this into the expression for the expectation:\n$$ |2\\Sigma^{-1} - I|^{1/2} = \\left(\\prod_{i=1}^d \\frac{2-\\lambda_i}{\\lambda_i}\\right)^{1/2} = \\frac{\\sqrt{\\prod(2-\\lambda_i)}}{\\sqrt{\\prod\\lambda_i}} = \\frac{|\\sqrt{2I - \\Sigma}|}{|\\Sigma|^{1/2}} $$\nSo,\n$$ \\mathbb{E}_{q}[w(X)^{2}] = \\frac{1}{|\\Sigma| \\frac{|\\sqrt{2I - \\Sigma}|}{|\\Sigma|^{1/2}}} = \\frac{1}{|\\Sigma|^{1/2} |\\sqrt{2I - \\Sigma}|} = \\frac{1}{\\sqrt{|\\Sigma||2I - \\Sigma|}} $$\n$$ = \\frac{1}{\\sqrt{\\prod \\lambda_i \\prod(2-\\lambda_i)}} = \\left(\\prod_{i=1}^d \\lambda_i(2-\\lambda_i)\\right)^{-1/2} $$\nFinally, the asymptotic ESS fraction is:\n$$ \\lim_{n \\to \\infty} \\frac{\\mathrm{ESS}}{n} = \\frac{1}{\\mathbb{E}_{q}[w(X)^{2}]} = \\sqrt{\\prod_{i=1}^d \\lambda_i(2-\\lambda_i)} $$\n\n**Step 3: Condition for Finiteness and Phase Transition**\n\nThe integral for $\\mathbb{E}_{q}[w(X)^2]$ converges if and only if the matrix $A = 2\\Sigma^{-1} - I$ is symmetric positive definite. Since $\\Sigma$ is symmetric, $A$ is symmetric. For it to be positive definite, all its eigenvalues must be positive. The eigenvalues of $A$ are $2/\\lambda_i - 1$.\n$$ \\frac{2}{\\lambda_i} - 1  0 \\implies \\frac{2}{\\lambda_i}  1 $$\nSince $\\Sigma$ is positive definite, its eigenvalues $\\lambda_i$ are all positive. Thus, we can multiply by $\\lambda_i$ without changing the inequality:\n$$ 2  \\lambda_i $$\nThe condition for a finite second moment of the weights, and thus a non-zero asymptotic ESS fraction, is that all eigenvalues of $\\Sigma$ must be strictly less than $2$.\n$$ 0  \\lambda_i  2 \\quad \\text{for all } i \\in \\{1, \\dots, d\\} $$\nIf any eigenvalue $\\lambda_k \\ge 2$, then $\\mathbb{E}_{q}[w(X)^2]$ diverges, and the asymptotic ESS fraction collapses to $0$. This marks a phase transition. The function $f(\\lambda) = \\lambda(2-\\lambda)$ is maximized at $\\lambda=1$ with $f(1) = 1$, and is less than $1$ for any other $\\lambda \\in (0, 2)$. The asymptotic ESS fraction, being a product of $d$ such terms (under a square root), will decay exponentially to $0$ as $d \\to \\infty$ unless all $\\lambda_i$ are exactly $1$. This illustrates the curse of dimensionality in importance sampling.\n\n**Computational Formula**\n\nFor a given spectrum $\\{\\lambda_i\\}_{i=1}^d$ of $\\Sigma$:\n1.  If there exists any eigenvalue $\\lambda_k \\ge 2$, the limit of $\\mathrm{ESS}/n$ is $0$.\n2.  Otherwise, the limit is $\\sqrt{\\prod_{i=1}^d (\\lambda_i(2-\\lambda_i))}$.\nThis formula will be used to compute the results for the test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the theoretical limit of ESS/n for the given test cases.\n\n    The problem asks for the analysis of the Self-Normalized Importance Sampling (SNIS)\n    Effective Sample Size (ESS) for a Gaussian target and proposal. The asymptotic limit\n    of the normalized ESS fraction ESS/n as n - infinity is given by:\n\n        lim (ESS/n) = (E_q[w(X)])^2 / E_q[w(X)^2]\n\n    For the specified Gaussian distributions pi = N(0, Sigma) and q = N(0, I),\n    this limit can be derived in closed form. E_q[w(X)] is always 1. The main\n    challenge is E_q[w(X)^2], which is finite only if all eigenvalues of Sigma,\n    lambda_i, satisfy 0  lambda_i  2.\n\n    The final formula for the limit in terms of the eigenvalues {lambda_i} is:\n\n        lim (ESS/n) = sqrt(product_{i=1 to d} [lambda_i * (2 - lambda_i)])\n\n    If any lambda_i = 2, the second moment of the weights diverges, and the\n    asymptotic ESS/n is 0.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (d, spectrum as list of (eigenvalue, count))\n        (10, [(1.0, 10)]),\n        (10, [(1.0/2.0, 10)]),\n        (20, [(4.0/5.0, 10), (6.0/5.0, 10)]),\n        (10, [(2.0, 1), (1.0, 9)]),\n        (5, [(5.0/2.0, 1), (1.0, 4)]),\n    ]\n\n    results = []\n    for d, spectrum in test_cases:\n        \n        product_val = 1.0\n        is_divergent = False\n\n        for val, count in spectrum:\n            if val = 2.0:\n                is_divergent = True\n                break\n            # Since Sigma is positive definite, val  0 is implied.\n\n            # We calculate the term lambda * (2 - lambda) for each eigenvalue\n            term = val * (2.0 - val)\n            \n            # The total product is the product of these terms, raised to the\n            # power of their multiplicity.\n            product_val *= (term ** count)\n\n        if is_divergent:\n            result = 0.0\n        else:\n            result = np.sqrt(product_val)\n\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}