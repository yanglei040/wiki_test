{
    "hands_on_practices": [
        {
            "introduction": "Well-Tempered Metadynamics (WTMetaD) is a powerful enhanced sampling technique that accelerates the exploration of complex free energy landscapes. It works by building a history-dependent bias potential that discourages the system from revisiting already explored states.\n\nThis exercise  takes you through a foundational derivation to understand how WTMetaD achieves this. By deriving the relationship between the converged bias potential and the system's underlying free energy, you will grasp the mechanism that allows the method to flatten energy barriers and quantitatively determine the resulting acceleration in sampling rare events.",
            "id": "3305295",
            "problem": "Consider a classical system with microscopic coordinates $\\mathbf{x}$ evolving at temperature $T$ in the canonical ensemble, with potential energy $U(\\mathbf{x})$. Let $\\xi(\\mathbf{x})$ be a differentiable collective variable. The unbiased equilibrium marginal along $\\xi$ is $P_{0}(\\xi) \\propto \\int \\mathrm{d}\\mathbf{x}\\,\\delta\\!\\left(\\xi-\\xi(\\mathbf{x})\\right)\\exp\\!\\left[-\\beta U(\\mathbf{x})\\right]$, where $\\beta = 1/(k_{B}T)$ and $k_{B}$ is the Boltzmann constant. Define the free energy along $\\xi$ by $F(\\xi) = -\\beta^{-1}\\ln P_{0}(\\xi) + C$, where $C$ is an arbitrary constant.\n\nNow apply well-tempered metadynamics (WTMetaD), that is, a history-dependent bias potential $V(\\xi,t)$ that is updated by depositing a kernel centered at the instantaneous $\\xi(t)$ with an amplitude tempered by $\\exp\\!\\left[-V(\\xi(t),t)/(k_{B}\\Delta T)\\right]$, where $\\Delta T0$ is a tunable parameter and the bias factor is $\\gamma = (T+\\Delta T)/T$. Assume a narrow, normalized kernel so that in the long-time limit the mean deposition rate at any fixed $\\xi$ is proportional to the stationary expectation of $\\exp\\!\\left[-V(\\xi(\\mathbf{x}),t)/(k_{B}\\Delta T)\\right]\\delta\\!\\left(\\xi-\\xi(\\mathbf{x})\\right)$ under the biased dynamics. In the asymptotic long-time regime, suppose the bias converges to a time-independent limit $V_{\\infty}(\\xi)$ and the $\\xi$-marginal under the biased dynamics is $P_{\\infty}(\\xi) \\propto \\exp\\!\\left[-\\beta\\left(F(\\xi)+V_{\\infty}(\\xi)\\right)\\right]$. Impose the stationarity condition that the mean deposition rate with respect to $\\xi$ becomes independent of $\\xi$.\n\n(a) Starting only from these definitions and conditions, derive the functional relation between the asymptotic bias $V_{\\infty}(\\xi)$ and the free energy $F(\\xi)$, up to an additive constant.\n\n(b) Let $\\Delta F$ denote the unbiased free-energy barrier between two metastable basins along $\\xi$. Using the Arrhenius-type barrier dependence of the Kramers barrier-crossing rate in the overdamped regime, and assuming that WTMetaD modifies the barrier height via the effective free energy $F(\\xi)+V_{\\infty}(\\xi)$ while leaving dynamical prefactors unchanged, compute the acceleration factor $\\alpha$ (the ratio of the biased to unbiased rate) in terms of $\\beta$, $\\Delta F$, and $\\gamma$.\n\nYour final answer must be given as a single row matrix with two entries using the LaTeX $\\mathrm{pmatrix}$ environment: the first entry is your closed-form expression for $V_{\\infty}(\\xi)$ (set the irrelevant additive constant to zero by convention), and the second entry is your closed-form expression for the acceleration factor $\\alpha$. No numerical evaluation is required, and no units are to be reported in the final answer.",
            "solution": "The problem is found to be valid as it is scientifically grounded in the principles of statistical mechanics and enhanced sampling methods, is well-posed, objective, and internally consistent.\n\nPart (a): Derivation of the asymptotic bias potential $V_{\\infty}(\\xi)$.\n\nThe problem states that in the well-tempered metadynamics (WTMetaD) scheme, the history-dependent bias potential $V(\\xi,t)$ is updated with a deposition rate that is tempered. It is given that in the long-time limit ($t \\to \\infty$), the mean deposition rate at a fixed value of the collective variable $\\xi$ is proportional to the stationary expectation of the tempering factor multiplied by a delta function specifying the location. Let's denote the asymptotic mean deposition rate as $R_{\\infty}(\\xi)$. The bias potential converges to a time-independent limit $V_{\\infty}(\\xi)$.\n\nThe stationary expectation is taken with respect to the biased canonical ensemble, characterized by the potential energy $U(\\mathbf{x}) + V_{\\infty}(\\xi(\\mathbf{x}))$. The expectation of an observable $A(\\mathbf{x})$ is $\\langle A(\\mathbf{x}) \\rangle_{V_{\\infty}}$.\nAccording to the problem statement, the deposition rate is proportional to:\n$$R_{\\infty}(\\xi) \\propto \\left\\langle \\exp\\left[-\\frac{V(\\xi(\\mathbf{x}),t)}{k_{B}\\Delta T}\\right] \\delta\\left(\\xi-\\xi(\\mathbf{x})\\right) \\right\\rangle_{V_{\\infty}}$$\nIn the asymptotic limit, $V(\\xi(\\mathbf{x}),t) \\to V_{\\infty}(\\xi(\\mathbf{x}))$. The expression becomes:\n$$R_{\\infty}(\\xi) \\propto \\left\\langle \\exp\\left[-\\frac{V_{\\infty}(\\xi(\\mathbf{x}))}{k_{B}\\Delta T}\\right] \\delta\\left(\\xi-\\xi(\\mathbf{x})\\right) \\right\\rangle_{V_{\\infty}}$$\nThe delta function $\\delta\\left(\\xi-\\xi(\\mathbf{x})\\right)$ forces $\\xi(\\mathbf{x})$ to be equal to $\\xi$. Therefore, inside the expectation integral, the term $\\exp\\left[-V_{\\infty}(\\xi(\\mathbf{x}))/(k_{B}\\Delta T)\\right]$ can be replaced by $\\exp\\left[-V_{\\infty}(\\xi)/(k_{B}\\Delta T)\\right]$. Since this term no longer depends on the integration variable $\\mathbf{x}$, it can be pulled out of the expectation:\n$$R_{\\infty}(\\xi) \\propto \\exp\\left[-\\frac{V_{\\infty}(\\xi)}{k_{B}\\Delta T}\\right] \\left\\langle \\delta\\left(\\xi-\\xi(\\mathbf{x})\\right) \\right\\rangle_{V_{\\infty}}$$\nThe term $\\left\\langle \\delta\\left(\\xi-\\xi(\\mathbf{x})\\right) \\right\\rangle_{V_{\\infty}}$ is, by definition, the marginal probability density of the collective variable $\\xi$ under the biased dynamics, which we denote as $P_{\\infty}(\\xi)$.\n$$P_{\\infty}(\\xi) = \\left\\langle \\delta\\left(\\xi-\\xi(\\mathbf{x})\\right) \\right\\rangle_{V_{\\infty}} = \\frac{\\int \\mathrm{d}\\mathbf{x}\\,\\delta\\!\\left(\\xi-\\xi(\\mathbf{x})\\right)\\exp\\!\\left[-\\beta \\left(U(\\mathbf{x})+V_{\\infty}(\\xi(\\mathbf{x}))\\right)\\right]}{\\int \\mathrm{d}\\mathbf{x}\\,\\exp\\!\\left[-\\beta \\left(U(\\mathbf{x})+V_{\\infty}(\\xi(\\mathbf{x}))\\right)\\right]}$$\nThus, the asymptotic deposition rate is proportional to the product of the tempering factor and the biased probability density:\n$$R_{\\infty}(\\xi) \\propto \\exp\\left[-\\frac{V_{\\infty}(\\xi)}{k_{B}\\Delta T}\\right] P_{\\infty}(\\xi)$$\nThe problem imposes a stationarity condition: in the long-time limit, the deposition rate $R_{\\infty}(\\xi)$ becomes independent of $\\xi$. Let us set it to a constant, $K$.\n$$K = \\text{const} \\cdot \\exp\\left[-\\frac{V_{\\infty}(\\xi)}{k_{B}\\Delta T}\\right] P_{\\infty}(\\xi)$$\nThis implies that the product itself must be constant:\n$$\\exp\\left[-\\frac{V_{\\infty}(\\xi)}{k_{B}\\Delta T}\\right] P_{\\infty}(\\xi) = \\text{constant}$$\nThe problem also provides the relationship between the biased marginal $P_{\\infty}(\\xi)$ and the free energy $F(\\xi)$:\n$$P_{\\infty}(\\xi) \\propto \\exp\\left[-\\beta\\left(F(\\xi)+V_{\\infty}(\\xi)\\right)\\right]$$\nSubstituting this into the stationarity equation:\n$$\\exp\\left[-\\frac{V_{\\infty}(\\xi)}{k_{B}\\Delta T}\\right] \\exp\\left[-\\beta\\left(F(\\xi)+V_{\\infty}(\\xi)\\right)\\right] = \\text{constant}$$\nCombining the exponents yields:\n$$\\exp\\left[-\\frac{V_{\\infty}(\\xi)}{k_{B}\\Delta T} - \\beta F(\\xi) - \\beta V_{\\infty}(\\xi)\\right] = \\text{constant}$$\nTaking the natural logarithm of both sides:\n$$-\\frac{V_{\\infty}(\\xi)}{k_{B}\\Delta T} - \\beta F(\\xi) - \\beta V_{\\infty}(\\xi) = \\text{constant}$$\nWe solve for $V_{\\infty}(\\xi)$ in terms of $F(\\xi)$. Let $\\beta = 1/(k_B T)$.\n$$-V_{\\infty}(\\xi) \\left(\\frac{1}{k_{B}\\Delta T} + \\frac{1}{k_{B}T}\\right) = \\frac{1}{k_{B}T} F(\\xi) + \\text{constant}$$\n$$-V_{\\infty}(\\xi) \\left(\\frac{T+\\Delta T}{k_{B}T\\Delta T}\\right) = \\frac{1}{k_{B}T} F(\\xi) + \\text{constant}$$\n$$V_{\\infty}(\\xi) = -\\left(\\frac{k_{B}T\\Delta T}{T+\\Delta T}\\right)\\left(\\frac{1}{k_{B}T} F(\\xi) + \\text{constant}'\\right)$$\n$$V_{\\infty}(\\xi) = -\\frac{\\Delta T}{T+\\Delta T} F(\\xi) + \\text{constant}''$$\nThe bias factor is given as $\\gamma = (T+\\Delta T)/T$. From this, we can express the prefactor in terms of $\\gamma$:\n$$\\gamma - 1 = \\frac{T+\\Delta T}{T} - 1 = \\frac{\\Delta T}{T}$$\nThe prefactor is $\\frac{\\Delta T}{T+\\Delta T} = \\frac{\\Delta T/T}{(T+\\Delta T)/T} = \\frac{\\gamma-1}{\\gamma}$.\nSo, the relationship is:\n$$V_{\\infty}(\\xi) = -\\frac{\\gamma - 1}{\\gamma} F(\\xi) + \\text{constant}''$$\nSetting the arbitrary additive constant to zero as per convention, we obtain the final expression for part (a):\n$$V_{\\infty}(\\xi) = -\\frac{\\gamma-1}{\\gamma} F(\\xi)$$\n\nPart (b): Calculation of the acceleration factor $\\alpha$.\n\nThe problem states that the barrier-crossing rate follows an Arrhenius-type dependence on the free-energy barrier $\\Delta F$. For the unbiased system, the rate $k_{\\text{unbiased}}$ is:\n$$k_{\\text{unbiased}} = A \\exp(-\\beta \\Delta F)$$\nwhere $A$ is a dynamical prefactor.\n\nIn WTMetaD, the system evolves under an effective free energy $F_{\\text{eff}}(\\xi) = F(\\xi) + V_{\\infty}(\\xi)$. Using the result from part (a):\n$$F_{\\text{eff}}(\\xi) = F(\\xi) - \\frac{\\gamma-1}{\\gamma}F(\\xi) = \\left(1 - \\frac{\\gamma-1}{\\gamma}\\right)F(\\xi) = \\frac{1}{\\gamma}F(\\xi)$$\nThis shows that the effective free-energy landscape is a scaled version of the original one. If the original free-energy barrier between two basins is $\\Delta F = F_{\\text{max}} - F_{\\text{min}}$, the new effective barrier $\\Delta F_{\\text{eff}}$ is:\n$$\\Delta F_{\\text{eff}} = F_{\\text{eff,max}} - F_{\\text{eff,min}} = \\frac{1}{\\gamma}F_{\\text{max}} - \\frac{1}{\\gamma}F_{\\text{min}} = \\frac{1}{\\gamma}\\Delta F$$\nThe biased rate $k_{\\text{biased}}$ is determined by this effective barrier. The problem assumes the prefactor $A$ remains unchanged.\n$$k_{\\text{biased}} = A \\exp(-\\beta \\Delta F_{\\text{eff}}) = A \\exp\\left(-\\frac{\\beta \\Delta F}{\\gamma}\\right)$$\nThe acceleration factor $\\alpha$ is the ratio of the biased rate to the unbiased rate:\n$$\\alpha = \\frac{k_{\\text{biased}}}{k_{\\text{unbiased}}} = \\frac{A \\exp\\left(-\\frac{\\beta \\Delta F}{\\gamma}\\right)}{A \\exp(-\\beta \\Delta F)}$$\n$$\\alpha = \\exp\\left(-\\frac{\\beta \\Delta F}{\\gamma} + \\beta \\Delta F\\right) = \\exp\\left[\\beta \\Delta F \\left(1 - \\frac{1}{\\gamma}\\right)\\right]$$\nThis can be written as:\n$$\\alpha = \\exp\\left[\\beta \\Delta F \\left(\\frac{\\gamma-1}{\\gamma}\\right)\\right]$$\nThis is the final expression for part (b).",
            "answer": "$$\\boxed{\\begin{pmatrix} -\\frac{\\gamma-1}{\\gamma} F(\\xi)  \\exp\\left[\\beta \\Delta F \\left(\\frac{\\gamma - 1}{\\gamma}\\right)\\right] \\end{pmatrix}}$$"
        },
        {
            "introduction": "Choosing the right enhanced sampling method is critical for computational efficiency and accuracy. Different methods, such as Umbrella Sampling (US) and Wang-Landau (WL) sampling, have distinct strengths and weaknesses rooted in their underlying algorithms and sources of statistical error.\n\nThis practice  challenges you to move beyond simply using a method to critically analyzing its performance. You will model and compare the mean-squared error of free energy estimates from US and a hybrid WL-reweighting scheme, deriving a condition that dictates which method is superior based on sampling statistics and inherent bias errors.",
            "id": "3305331",
            "problem": "Consider a one-dimensional collective variable (CV) $s$ for a complex system at inverse temperature $\\beta = 1/(k_B T)$, where $k_B$ is Boltzmann’s constant and $T$ is the absolute temperature. The canonical probability density of $s$ is $P_{\\beta}(s)$, and the associated free energy (also called the potential of mean force) is $F(s) = -k_B T \\ln P_{\\beta}(s) + C$, where $C$ is an irrelevant constant.\n\nTwo enhanced sampling strategies are considered to estimate $F(s)$ at a target bin $s^{\\star}$:\n- Umbrella sampling (US) based on a set of harmonic bias windows, combined through a statistically optimal estimator (such as the Weighted Histogram Analysis Method) to yield an effective, approximately unbiased count $H_{\\mathrm{US}}(s^{\\star})$ with mean $n_{\\mathrm{US}}$ and negligible bias at $s^{\\star}$, under the assumption of sufficient overlap and correct global normalization.\n- A hybrid scheme that first performs Wang-Landau (WL) sampling directly in CV space to flatten the histogram of $s$ by adaptively updating a bias $V_{\\mathrm{WL}}(s)$ until histogram flatness, then reconstructs the canonical free energy via reweighting of the WL trajectory, i.e., using samples with weight $\\exp(\\beta V_{\\mathrm{WL}}(s))$ to obtain an estimator $\\widehat{P}_{\\beta}(s)$ and hence $\\widehat{F}(s)$.\n\nAssume the following scientifically standard conditions:\n1. The CV range is discretized into $B$ bins, and histogram counts per bin can be modeled as independent Poisson random variables for large sampling duration. For a bin with mean count $n$, the variance of the log count satisfies $\\mathrm{Var}[\\ln H] \\approx 1/n$ by the delta method applied to a Poisson random variable with mean $n$.\n2. In the US case, at $s^{\\star}$ the effective count is $H_{\\mathrm{US}}(s^{\\star})$ with mean $n_{\\mathrm{US}}$ and negligible estimator bias in $\\ln P_{\\beta}(s^{\\star})$ due to overlap and self-consistent normalization. Correlations are negligible at the level of the derived variance.\n3. In the WL-reweighting case, after WL convergence the histogram of $s$ is approximately flat, so $H_{\\mathrm{WL}}(s)$ has mean $n_{\\mathrm{WL}}$ for every bin. The WL bias $V_{\\mathrm{WL}}(s)$ used for reweighting is imperfect: write $V_{\\mathrm{WL}}(s) = V^{\\star}(s) + \\epsilon(s)$, where $V^{\\star}(s)$ is the unknown true bias that would exactly flatten the canonical $P_{\\beta}(s)$, and $\\epsilon(s)$ is a small, zero-mean error. Model $\\epsilon(s)$ as independent of histogram counts, with variance $\\mathrm{Var}[\\epsilon(s^{\\star})] = \\sigma_{V}^{2}$.\n4. Ignoring global normalization fluctuations that affect all bins equally at first order, the WL-reweighted estimator of the free energy at $s^{\\star}$ is dominated by the local contributions from $H_{\\mathrm{WL}}(s^{\\star})$ and $V_{\\mathrm{WL}}(s^{\\star})$.\n\nStarting only from the above definitions and assumptions, and without invoking any pre-packaged variance formulae for these methods, derive the threshold condition under which the WL-reweighting hybrid scheme has strictly smaller mean-squared error in the free energy estimate at $s^{\\star}$ than umbrella sampling. Express your final result as a single, closed-form analytic expression for the maximum tolerable WL bias error variance $\\sigma_{V,\\max}^{2}$ as a function of $n_{\\mathrm{WL}}$, $n_{\\mathrm{US}}$, $k_B$, and $T$, such that the WL-reweighting estimator’s variance at $s^{\\star}$ does not exceed that of umbrella sampling.\n\nYour answer must be a single expression. No numerical evaluation is required. Do not include any units in your final answer. If intermediate approximations are needed, justify them from first principles within your derivation.",
            "solution": "The user wants to find the threshold condition under which a hybrid Wang-Landau (WL) reweighting scheme provides a more accurate estimate of the free energy $F(s)$ at a specific collective variable value $s^{\\star}$ than an Umbrella Sampling (US) scheme. The accuracy is to be measured by the mean-squared error (MSE). The problem asks for the maximum tolerable variance of the WL bias potential, $\\sigma_{V,\\max}^{2}$, for the WL method's variance to not exceed that of the US method.\n\nLet us begin by establishing the expressions for the Mean-Squared Error (MSE) of the free energy estimate for each method, based on the provided definitions and assumptions. The free energy $F(s)$ is related to the canonical probability density $P_{\\beta}(s)$ by $F(s) = -k_B T \\ln P_{\\beta}(s) + C$, where $\\beta = 1/(k_B T)$. The MSE of an estimator $\\widehat{F}(s^{\\star})$ for the true value $F(s^{\\star})$ is defined as $\\mathrm{MSE} = E[(\\widehat{F}(s^{\\star}) - F(s^{\\star}))^2] = \\mathrm{Var}[\\widehat{F}(s^{\\star})] + (\\mathrm{Bias}[\\widehat{F}(s^{\\star})])^2$.\n\nFirst, we analyze the Umbrella Sampling (US) method.\nThe estimator for the free energy, $\\widehat{F}_{\\mathrm{US}}(s^{\\star})$, is derived from the effective histogram count $H_{\\mathrm{US}}(s^{\\star})$. Since $F(s) \\propto -\\ln P_{\\beta}(s)$ and $P_{\\beta}(s)$ is estimated from the counts $H(s)$, the estimator for the free energy takes the form $\\widehat{F}_{\\mathrm{US}}(s^{\\star}) \\approx -k_B T \\ln H_{\\mathrm{US}}(s^{\\star}) + C'$, where $C'$ is a constant.\nThe problem states that for the US method, there is \"negligible estimator bias in $\\ln P_{\\beta}(s^{\\star})$\". This implies that the bias of $\\widehat{F}_{\\mathrm{US}}(s^{\\star})$ is also negligible. Consequently, the MSE is dominated by the variance:\n$\\mathrm{MSE}_{\\mathrm{US}}(F(s^{\\star})) \\approx \\mathrm{Var}[\\widehat{F}_{\\mathrm{US}}(s^{\\star})]$.\nWe compute the variance as follows:\n$\\mathrm{Var}[\\widehat{F}_{\\mathrm{US}}(s^{\\star})] = \\mathrm{Var}[ -k_B T \\ln H_{\\mathrm{US}}(s^{\\star}) + C' ] = (-k_B T)^2 \\mathrm{Var}[\\ln H_{\\mathrm{US}}(s^{\\star})]$.\nAssumption 2 provides that for a bin with mean count $n$, $\\mathrm{Var}[\\ln H] \\approx 1/n$. For the US case at $s^{\\star}$, the mean count is given as $n_{\\mathrm{US}}$.\nTherefore, the MSE for the US method is:\n$$ \\mathrm{MSE}_{\\mathrm{US}}(F(s^{\\star})) \\approx (k_B T)^2 \\frac{1}{n_{\\mathrm{US}}} $$\n\nNext, we analyze the hybrid WL-reweighting scheme.\nThe problem states that the canonical free energy is reconstructed via reweighting. The estimator for the canonical probability is $\\widehat{P}_{\\beta}(s) \\propto H_{\\mathrm{WL}}(s) \\exp(\\beta V_{\\mathrm{WL}}(s))$, where $H_{\\mathrm{WL}}(s)$ is the count in the bin $s$ from the WL simulation and $V_{\\mathrm{WL}}(s)$ is the final WL bias potential.\nThe corresponding free energy estimator is:\n$\\widehat{F}_{\\mathrm{WL}}(s) = -k_B T \\ln \\widehat{P}_{\\beta}(s) + C'' = -k_B T \\ln(H_{\\mathrm{WL}}(s)) - k_B T \\ln(\\exp(\\beta V_{\\mathrm{WL}}(s))) + \\text{const}$.\nUsing $\\beta = 1/(k_B T)$, this simplifies to:\n$\\widehat{F}_{\\mathrm{WL}}(s) = -V_{\\mathrm{WL}}(s) - k_B T \\ln H_{\\mathrm{WL}}(s) + \\text{const}$.\nThe error in this estimator arises from two independent sources as per the problem statement:\n1. The residual error in the WL bias potential, $\\epsilon(s) = V_{\\mathrm{WL}}(s) - V^{\\star}(s)$, where $V^{\\star}(s) = -F(s) + \\text{const}$ is the true potential that flattens the landscape. This error is a random variable with mean $E[\\epsilon(s^{\\star})] = 0$ and variance $\\mathrm{Var}[\\epsilon(s^{\\star})] = \\sigma_{V}^{2}$.\n2. The statistical fluctuation in the histogram counts, $H_{\\mathrm{WL}}(s^{\\star})$, which is a Poisson random variable with mean $n_{\\mathrm{WL}}$.\n\nThe total variance of the estimator $\\widehat{F}_{\\mathrm{WL}}(s^{\\star})$ is the sum of the variances from these two independent sources. We also assume that any bias in this estimator is of higher order in $1/n_{\\mathrm{WL}}$ and can be neglected for large sampling, similar to the US case. Thus, we approximate the MSE with the variance.\n$\\mathrm{MSE}_{\\mathrm{WL}}(F(s^{\\star})) \\approx \\mathrm{Var}[\\widehat{F}_{\\mathrm{WL}}(s^{\\star})] = \\mathrm{Var}[-V_{\\mathrm{WL}}(s^{\\star}) - k_B T \\ln H_{\\mathrm{WL}}(s^{\\star})]$.\nGiven the independence of $\\epsilon(s^{\\star})$ (and thus $V_{\\mathrm{WL}}(s^{\\star})$) and $H_{\\mathrm{WL}}(s^{\\star})$:\n$\\mathrm{Var}[\\widehat{F}_{\\mathrm{WL}}(s^{\\star})] = \\mathrm{Var}[-V_{\\mathrm{WL}}(s^{\\star})] + \\mathrm{Var}[-k_B T \\ln H_{\\mathrm{WL}}(s^{\\star})]$.\nThe first term is $\\mathrm{Var}[-V_{\\mathrm{WL}}(s^{\\star})] = \\mathrm{Var}[-(V^{\\star}(s^{\\star}) + \\epsilon(s^{\\star}))] = \\mathrm{Var}[-\\epsilon(s^{\\star})] = \\mathrm{Var}[\\epsilon(s^{\\star})] = \\sigma_{V}^{2}$, since $V^{\\star}(s^{\\star})$ is a constant.\nThe second term is $(k_B T)^2 \\mathrm{Var}[\\ln H_{\\mathrm{WL}}(s^{\\star})]$. Using Assumption 2 again with the mean count $n_{\\mathrm{WL}}$ for the WL histogram, this variance is $(k_B T)^2 (1/n_{\\mathrm{WL}})$.\nCombining these terms, the MSE for the WL-reweighting method is:\n$$ \\mathrm{MSE}_{\\mathrm{WL}}(F(s^{\\star})) \\approx \\sigma_{V}^{2} + (k_B T)^2 \\frac{1}{n_{\\mathrm{WL}}} $$\n\nThe problem asks for the condition under which the WL scheme is strictly better, i.e., $\\mathrm{MSE}_{\\mathrm{WL}}(F(s^{\\star}))  \\mathrm{MSE}_{\\mathrm{US}}(F(s^{\\star}))$. The threshold for this condition is found by setting the two MSEs equal. The maximum tolerable variance, $\\sigma_{V,\\max}^{2}$, is the value of $\\sigma_{V}^{2}$ at this threshold.\n$ \\mathrm{MSE}_{\\mathrm{WL}}(F(s^{\\star})) = \\mathrm{MSE}_{\\mathrm{US}}(F(s^{\\star})) $\n$$ \\sigma_{V,\\max}^{2} + (k_B T)^2 \\frac{1}{n_{\\mathrm{WL}}} = (k_B T)^2 \\frac{1}{n_{\\mathrm{US}}} $$\nSolving for $\\sigma_{V,\\max}^{2}$:\n$$ \\sigma_{V,\\max}^{2} = (k_B T)^2 \\frac{1}{n_{\\mathrm{US}}} - (k_B T)^2 \\frac{1}{n_{\\mathrm{WL}}} $$\nThis expression can be factored to yield the final result.\n$$ \\sigma_{V,\\max}^{2} = (k_B T)^2 \\left( \\frac{1}{n_{\\mathrm{US}}} - \\frac{1}{n_{\\mathrm{WL}}} \\right) $$\nThis result represents the maximum allowable error variance in the converged Wang-Landau potential for the hybrid method to match the precision of the umbrella sampling method. For the WL method to be superior, $\\sigma_V^2$ must be less than this value. A positive solution for $\\sigma_{V,\\max}^{2}$ exists only if $n_{\\mathrm{WL}}  n_{\\mathrm{US}}$, signifying that the WL simulation must achieve higher effective sampling in the bin of interest to compensate for the additional error source from the imperfect bias potential.",
            "answer": "$$\\boxed{(k_B T)^2 \\left(\\frac{1}{n_{\\mathrm{US}}} - \\frac{1}{n_{\\mathrm{WL}}}\\right)}$$"
        },
        {
            "introduction": "In Wang-Landau sampling, the algorithm's convergence is monitored by checking for the \"flatness\" of the visited energy histogram. While this is a practical heuristic, a deeper understanding requires connecting this empirical observation to the rigorous theory of Markov chains.\n\nThis advanced exercise  bridges the gap between practice and theory by asking you to derive a quantitative diagnostic. You will relate the spectral gap $\\gamma$ of the system's transition matrix—a formal measure of convergence speed—to the expected flatness of the energy histogram, providing a powerful, data-free bound on sampling quality.",
            "id": "3305336",
            "problem": "Consider Wang–Landau sampling (WLS) of a system with a discrete energy grid of $m$ bins, indexed by $i \\in \\{1,\\dots,m\\}$. The current estimate of the density of states is $g(i)$, while the (unknown) true density of states is $g_{\\mathrm{true}}(i)$. The induced target distribution on energy under the current $g$ is $\\pi_{g}(i) \\propto g_{\\mathrm{true}}(i)/g(i)$, normalized so that $\\sum_{i=1}^{m} \\pi_{g}(i) = 1$. Assume a symmetric nearest-neighbor proposal on energy bins with Metropolis–Hastings acceptance, and that we have made the resulting Markov chain lazy by adding a holding probability of $1/2$ at each step to eliminate periodicity. Let $P_{g}$ denote this lazy transition matrix, which is reversible with respect to $\\pi_{g}$ and has eigenvalues $1=\\lambda_{1}  \\lambda_{2} \\ge \\dots \\ge \\lambda_{m} \\ge 0$. Define the spectral gap $\\gamma = 1 - \\lambda_{2}$.\n\nSuppose the chain is started in stationarity under $\\pi_{g}$ and run for $n$ steps, producing the energy histogram counts $N_{i}$, $i=1,\\dots,m$. Define the root-mean-square (RMS) flatness metric\n$$\nR \\;=\\; \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{N_{i}}{n} - \\frac{1}{m}\\right)^{2}}.\n$$\nLet $S_{2} \\equiv \\sum_{i=1}^{m} \\pi_{g}(i)^{2}$ denote the second moment of $\\pi_{g}$.\n\nStarting only from the fundamental definitions of reversibility, the spectral gap, and stationarity of a Markov chain, and from well-tested spectral properties (orthogonal eigen-decomposition in $L^{2}(\\pi_{g})$ and nonnegativity of all eigenvalues for a lazy chain), derive a nonasymptotic, expectation-level, diagnostic that quantitatively relates the spectral gap $\\gamma$ to histogram flatness. Specifically, show that there is a data-free upper bound of the form\n$$\n\\mathbb{E}[R] \\;\\le\\; \\varepsilon_{\\star}(\\gamma, m, n, S_{2})\n$$\nwith an explicit closed-form expression for $\\varepsilon_{\\star}(\\gamma, m, n, S_{2})$ depending only on $\\gamma$, $m$, $n$, and $S_{2}$.\n\nYour task is to derive and report the exact analytic expression for $\\varepsilon_{\\star}(\\gamma, m, n, S_{2})$. The final answer must be a single closed-form expression. If you choose to take a square root when passing from a bound on $\\mathbb{E}[R^{2}]$ to a bound on $\\mathbb{E}[R]$, justify your step using a general inequality valid for nonnegative random variables.\n\nProvide your final answer as the explicit formula for $\\varepsilon_{\\star}(\\gamma, m, n, S_{2})$ in a single closed form. No numerical evaluation is required and no units are involved. Do not provide an inequality; provide only the expression on the right-hand side.",
            "solution": "The objective is to derive a non-asymptotic upper bound $\\varepsilon_{\\star}(\\gamma, m, n, S_{2})$ on the expected root-mean-square (RMS) flatness metric, $\\mathbb{E}[R]$, where\n$$\nR = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{N_{i}}{n} - \\frac{1}{m}\\right)^{2}}\n$$\nThe number of samples is $n$, the number of bins is $m$, and $N_i$ is the histogram count for bin $i$.\n\nThe derivation proceeds by first finding a bound on the expectation of the squared metric, $\\mathbb{E}[R^2]$, and then relating it to $\\mathbb{E}[R]$. For any non-negative random variable $X$, Jensen's inequality for the concave function $f(x)=\\sqrt{x}$ states that $\\mathbb{E}[\\sqrt{X}] \\le \\sqrt{\\mathbb{E}[X]}$. Thus, $\\mathbb{E}[R] \\le \\sqrt{\\mathbb{E}[R^2]}$. Our task reduces to finding an upper bound on $\\mathbb{E}[R^2]$.\n\nLet's expand the expression for $\\mathbb{E}[R^2]$:\n$$\n\\mathbb{E}[R^2] = \\mathbb{E}\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{N_{i}}{n} - \\frac{1}{m}\\right)^{2}\\right] = \\frac{1}{m}\\sum_{i=1}^{m} \\mathbb{E}\\left[\\left(\\frac{N_{i}}{n} - \\frac{1}{m}\\right)^{2}\\right]\n$$\nWe use the standard decomposition of the mean squared error into variance and squared bias. For each term in the sum, let $\\hat{\\pi}_i = N_i/n$ be the empirical frequency for bin $i$.\n$$\n\\mathbb{E}\\left[\\left(\\hat{\\pi}_i - \\frac{1}{m}\\right)^{2}\\right] = \\mathbb{E}\\left[\\left(\\hat{\\pi}_i - \\mathbb{E}[\\hat{\\pi}_i] + \\mathbb{E}[\\hat{\\pi}_i] - \\frac{1}{m}\\right)^{2}\\right] = \\text{Var}(\\hat{\\pi}_i) + \\left(\\mathbb{E}[\\hat{\\pi}_i] - \\frac{1}{m}\\right)^2\n$$\nThe cross-term vanishes because $\\mathbb{E}[\\hat{\\pi}_i - \\mathbb{E}[\\hat{\\pi}_i]] = 0$.\n\nThe Markov chain is started in its stationary distribution $\\pi_g$. Therefore, for any time step $t$, the probability of being in state $i$ is $P(X_t=i) = \\pi_g(i)$. The expected histogram count $\\mathbb{E}[N_i]$ is:\n$$\n\\mathbb{E}[N_i] = \\mathbb{E}\\left[\\sum_{t=0}^{n-1} \\mathbb{I}(X_t=i)\\right] = \\sum_{t=0}^{n-1} \\mathbb{E}[\\mathbb{I}(X_t=i)] = \\sum_{t=0}^{n-1} \\pi_g(i) = n\\pi_g(i)\n$$\nThus, $\\mathbb{E}[\\hat{\\pi}_i] = \\mathbb{E}[N_i/n] = \\pi_g(i)$.\n\nSubstituting this into the expression for $\\mathbb{E}[R^2]$:\n$$\n\\mathbb{E}[R^2] = \\frac{1}{m}\\sum_{i=1}^{m} \\left( \\text{Var}\\left(\\frac{N_i}{n}\\right) + \\left(\\pi_g(i) - \\frac{1}{m}\\right)^2 \\right)\n$$\nThis separates the bound into two parts: a variance term, related to the efficiency of sampling, and a bias term, related to the deviation of the target distribution $\\pi_g$ from the uniform distribution $1/m$.\n\nLet's analyze the bias term first:\n$$\n\\frac{1}{m}\\sum_{i=1}^{m} \\left(\\pi_g(i) - \\frac{1}{m}\\right)^2 = \\frac{1}{m} \\left( \\sum_{i=1}^{m}\\pi_g(i)^2 - \\frac{2}{m}\\sum_{i=1}^{m}\\pi_g(i) + \\sum_{i=1}^{m}\\frac{1}{m^2} \\right)\n$$\nUsing the definitions $S_2 = \\sum_{i=1}^{m} \\pi_g(i)^2$ and the normalization $\\sum_{i=1}^{m} \\pi_g(i) = 1$, this becomes:\n$$\n\\frac{1}{m} \\left( S_2 - \\frac{2}{m} \\cdot 1 + m \\cdot \\frac{1}{m^2} \\right) = \\frac{S_2}{m} - \\frac{1}{m^2}\n$$\nNow, let's analyze the variance term, $\\frac{1}{m}\\sum_i \\text{Var}(\\frac{N_i}{n}) = \\frac{1}{mn^2}\\sum_i \\text{Var}(N_i)$. The variance of $N_i = \\sum_{t=0}^{n-1} \\mathbb{I}(X_t=i)$ for a stationary Markov chain is given by:\n$$\n\\text{Var}(N_i) = n \\text{Var}_{\\pi_g}(\\mathbb{I}_i) + 2 \\sum_{k=1}^{n-1} (n-k) \\text{Cov}_{\\pi_g}(\\mathbb{I}_i(X_0), \\mathbb{I}_i(X_k))\n$$\nwhere $\\mathbb{I}_i$ is the indicator function for state $i$. The variance of the indicator is $\\text{Var}_{\\pi_g}(\\mathbb{I}_i) = \\pi_g(i)(1-\\pi_g(i))$.\nThe autocovariance term can be bounded using the spectral properties of the transition operator $P_g$. For a reversible Markov chain and any observable $f$, the autocovariance at lag $k$ is bounded by:\n$$\n\\text{Cov}_{\\pi_g}(f(X_0), f(X_k)) \\le \\lambda_2^k \\text{Var}_{\\pi_g}(f)\n$$\nwhere $\\lambda_2$ is the second largest eigenvalue of $P_g$. Since the chain is lazy, all eigenvalues are non-negative, validating this inequality.\nApplying this to $f=\\mathbb{I}_i$:\n$$\n\\text{Var}(N_i) \\le n \\text{Var}_{\\pi_g}(\\mathbb{I}_i) + 2 \\sum_{k=1}^{n-1} (n-k) \\lambda_2^k \\text{Var}_{\\pi_g}(\\mathbb{I}_i) = \\text{Var}_{\\pi_g}(\\mathbb{I}_i) \\left( n + 2 \\sum_{k=1}^{n-1} (n-k) \\lambda_2^k \\right)\n$$\nTo obtain a simple closed-form bound, we bound the sum term:\n$$\n\\sum_{k=1}^{n-1} (n-k) \\lambda_2^k \\le \\sum_{k=1}^{n-1} n \\lambda_2^k = n \\sum_{k=1}^{n-1} \\lambda_2^k \\le n \\sum_{k=1}^{\\infty} \\lambda_2^k = n \\frac{\\lambda_2}{1-\\lambda_2}\n$$\nSubstituting this back into the bound for $\\text{Var}(N_i)$:\n$$\n\\text{Var}(N_i) \\le \\text{Var}_{\\pi_g}(\\mathbb{I}_i) \\left( n + 2n \\frac{\\lambda_2}{1-\\lambda_2} \\right) = n \\text{Var}_{\\pi_g}(\\mathbb{I}_i) \\left( 1 + \\frac{2\\lambda_2}{1-\\lambda_2} \\right) = n \\text{Var}_{\\pi_g}(\\mathbb{I}_i) \\frac{1+\\lambda_2}{1-\\lambda_2}\n$$\nNow, sum over all bins $i$:\n$$\n\\sum_{i=1}^m \\text{Var}(N_i) \\le n \\left( \\sum_{i=1}^m \\text{Var}_{\\pi_g}(\\mathbb{I}_i) \\right) \\frac{1+\\lambda_2}{1-\\lambda_2}\n$$\nThe sum of variances is $\\sum_i \\pi_g(i)(1-\\pi_g(i)) = \\sum_i \\pi_g(i) - \\sum_i \\pi_g(i)^2 = 1-S_2$.\nThe spectral gap is $\\gamma = 1-\\lambda_2$, so $\\lambda_2 = 1-\\gamma$. The fraction becomes:\n$$\n\\frac{1+\\lambda_2}{1-\\lambda_2} = \\frac{1 + (1-\\gamma)}{1 - (1-\\gamma)} = \\frac{2-\\gamma}{\\gamma}\n$$\nThus, the sum of variances is bounded by:\n$$\n\\sum_{i=1}^m \\text{Var}(N_i) \\le n(1-S_2)\\frac{2-\\gamma}{\\gamma}\n$$\nNow we assemble the final bound for $\\mathbb{E}[R^2]$:\n$$\n\\mathbb{E}[R^2] \\le \\frac{1}{mn^2} \\left( n(1-S_2)\\frac{2-\\gamma}{\\gamma} \\right) + \\left( \\frac{S_2}{m} - \\frac{1}{m^2} \\right)\n$$\n$$\n\\mathbb{E}[R^2] \\le \\frac{(1-S_2)(2-\\gamma)}{mn\\gamma} + \\frac{S_2}{m} - \\frac{1}{m^2}\n$$\nFinally, we take the square root to get the bound on $\\mathbb{E}[R]$. Let $\\varepsilon_{\\star}^2$ be the right-hand side of the above inequality. Then $\\mathbb{E}[R] \\le \\sqrt{\\varepsilon_{\\star}^2} = \\varepsilon_{\\star}$. The desired expression is:\n$$\n\\varepsilon_{\\star}(\\gamma, m, n, S_{2}) = \\sqrt{\\frac{(1-S_2)(2-\\gamma)}{mn\\gamma} + \\frac{S_2}{m} - \\frac{1}{m^2}}\n$$\nThis expression depends only on the declared variables $\\gamma$, $m$, $n$, and $S_2$, and represents the required non-asymptotic, expectation-level diagnostic.",
            "answer": "$$\\boxed{\\sqrt{\\frac{(1-S_{2})(2-\\gamma)}{mn\\gamma} + \\frac{S_{2}}{m} - \\frac{1}{m^{2}}}}$$"
        }
    ]
}