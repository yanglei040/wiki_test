{
    "hands_on_practices": [
        {
            "introduction": "We begin by connecting the abstract definition of a copula to the familiar concept of statistical independence. This exercise uses Sklar's theorem to demonstrate why the joint distribution of independent variables is uniquely described by the product copula, providing a crucial baseline for understanding more complex dependence structures. Mastering this fundamental link is the first step toward appreciating how copulas separate marginal behavior from the dependence structure. ",
            "id": "1387890",
            "problem": "In reliability engineering, accurately modeling the joint failure behavior of system components is crucial. Consider two critical components in a newly designed data center, Component 1 and Component 2. Let their respective lifetimes be represented by the random variables $T_1$ and $T_2$. Through extensive testing, it has been determined that the failures of these two components are statistically independent.\n\nThe marginal Cumulative Distribution Functions (CDFs) for the lifetimes, denoted by $F_1(t_1) = P(T_1 \\le t_1)$ and $F_2(t_2) = P(T_2 \\le t_2)$, are known to be continuous. The joint behavior of these lifetimes is described by the joint CDF, $H(t_1, t_2) = P(T_1 \\le t_1, T_2 \\le t_2)$.\n\nAccording to Sklar's theorem, for any multivariate distribution with continuous marginals, there exists a unique function $C: [0, 1]^2 \\to [0, 1]$, called a copula, that links the marginal CDFs to their joint CDF. For our two-component system, this relationship is given by:\n$$H(t_1, t_2) = C(F_1(t_1), F_2(t_2))$$\n\nGiven that the lifetimes $T_1$ and $T_2$ are independent, which of the following expressions correctly represents the functional form of the unique copula $C(u, v)$ that describes their joint distribution?\n\nA. $C(u, v) = uv$\n\nB. $C(u, v) = \\min(u, v)$\n\nC. $C(u, v) = \\max(u + v - 1, 0)$\n\nD. $C(u, v) = (u^{-\\theta} + v^{-\\theta} - 1)^{-1/\\theta}$, for some parameter $\\theta  0$.\n\nE. $C(u, v) = u+v-1$",
            "solution": "The problem asks us to identify the specific form of the copula function, $C(u, v)$, that describes the joint distribution of two independent random variables with continuous marginals. We are given the relationship from Sklar's theorem and the property of independence.\n\nStep 1: State the definition of independence for random variables.\nTwo random variables, $T_1$ and $T_2$, are statistically independent if and only if their joint Cumulative Distribution Function (CDF), $H(t_1, t_2)$, is the product of their marginal CDFs, $F_1(t_1)$ and $F_2(t_2)$. Mathematically, this is expressed as:\n$$H(t_1, t_2) = F_1(t_1) F_2(t_2)$$\n\nStep 2: Use Sklar's theorem.\nThe problem provides the expression for the joint CDF based on Sklar's theorem:\n$$H(t_1, t_2) = C(F_1(t_1), F_2(t_2))$$\nHere, $C(u, v)$ is the unique copula function that couples the marginals to form the joint distribution. The arguments of the copula are the values of the marginal CDFs.\n\nStep 3: Equate the two expressions for the joint CDF.\nSince both expressions represent the same joint CDF $H(t_1, t_2)$, we can set them equal to each other:\n$$C(F_1(t_1), F_2(t_2)) = F_1(t_1) F_2(t_2)$$\n\nStep 4: Determine the general form of the copula $C(u, v)$.\nLet us define new variables $u$ and $v$ as the outputs of the marginal CDFs:\n$$u = F_1(t_1)$$\n$$v = F_2(t_2)$$\nSince $F_1$ and $F_2$ are continuous CDFs, their ranges are the entire interval $[0, 1]$. This means that for any pair $(u, v)$ in the domain of the copula, $[0, 1] \\times [0, 1]$, we can find corresponding values $t_1$ and $t_2$ such that $u = F_1(t_1)$ and $v = F_2(t_2)$. Therefore, we can generalize the relationship from the specific values $F_1(t_1)$ and $F_2(t_2)$ to the general variables $u$ and $v$.\n\nSubstituting $u$ and $v$ into the equation from Step 3, we get:\n$$C(u, v) = u \\cdot v$$\nThis specific functional form is known as the product copula or the independence copula.\n\nStep 5: Compare the result with the given options.\nThe derived form of the copula is $C(u, v) = uv$. This matches option A.\n\nThe other options represent different dependence structures:\n-   Option B, $C(u, v) = \\min(u, v)$, is the Fréchet-Hoeffding upper bound copula, representing perfect positive dependence (comonotonicity).\n-   Option C, $C(u, v) = \\max(u+v-1, 0)$, is the Fréchet-Hoeffding lower bound copula, representing perfect negative dependence (countermonotonicity).\n-   Option D, $C(u, v) = (u^{-\\theta} + v^{-\\theta} - 1)^{-1/\\theta}$, is the Clayton copula, which models a particular type of dependence (stronger in the lower tail).\n-   Option E, $C(u, v) = u+v-1$, is not a valid copula. For a function to be a copula, it must satisfy boundary conditions, including $C(u, 1) = u$. For option E, $C(u, 1) = u+1-1=u$, which works. However, another condition is $C(u, v) \\ge 0$. If $u=0.1$ and $v=0.1$, $C(0.1, 0.1) = 0.1+0.1-1 = -0.8$, which violates the non-negativity property of probability distributions.\n\nThus, the only correct choice for independent random variables is the product copula.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Building a valid copula requires satisfying specific mathematical properties. This practice challenges you to work from first principles with the Farlie-Gumbel-Morgenstern (FGM) family, starting from a candidate density to derive the copula function and determine the conditions for its validity. You will then derive its Kendall's tau, linking the copula's structure directly to a fundamental measure of dependence and deepening your understanding of the interplay between a copula's functional form and the dependence it models. ",
            "id": "3300458",
            "problem": "Consider the bivariate function with density on the unit square given by the Farlie–Gumbel–Morgenstern (FGM) copula candidate\n$$\nc(u,v) \\;=\\; 1 + \\theta\\,(1 - 2u)(1 - 2v), \\quad (u,v) \\in [0,1]^2, \\quad \\theta \\in \\mathbb{R}.\n$$\nYou are told only that a bivariate copula is a distribution function on $[0,1]^2$ with uniform marginals, and that a function $C:[0,1]^2\\to[0,1]$ is a copula if and only if it is grounded, has uniform marginals, and is $2$-increasing, where $2$-increasingness means that for every axis-aligned rectangle $R=[u_1,u_2]\\times[v_1,v_2]\\subset[0,1]^2$ with $u_1\\le u_2$ and $v_1\\le v_2$, the volume\n$$\nV_C(R)\\;=\\; C(u_2,v_2)-C(u_2,v_1)-C(u_1,v_2)+C(u_1,v_1)\n$$\nis nonnegative.\n\n(a) Starting from these definitions, derive the distribution function $C(u,v)$ corresponding to the given $c(u,v)$ and determine for which real values of $\\theta$ the function $C$ is a valid copula. Your verification must explicitly address groundedness, uniform marginals, and $2$-increasingness.\n\n(b) Using only the definition of Kendall’s tau for continuous bivariate distributions as a concordance measure,\n$$\n\\tau \\;=\\; \\mathbb{P}\\big((X-X')(Y-Y')0\\big)\\;-\\;\\mathbb{P}\\big((X-X')(Y-Y')0\\big),\n$$\nwhere $(X,Y)$ and $(X',Y')$ are independent and identically distributed, derive a closed-form expression for Kendall’s tau as a function of $\\theta$ for the copula you obtained in part (a). Do not assume any special-purpose formula for Kendall’s tau; derive it from first principles by transforming to uniform marginals and expressing $\\tau$ as an integral that you evaluate.\n\nProvide your final answer as a single closed-form analytic expression in terms of $\\theta$. No numerical approximation is required. Do not include any units. If you find parameter restrictions for validity, you may assume $\\theta$ satisfies them when reporting Kendall’s tau.",
            "solution": "The problem is divided into two parts. Part (a) requires the derivation of the distribution function $C(u,v)$ from the given density candidate $c(u,v)$ and the determination of the parameter range for $\\theta$ that makes $C(u,v)$ a valid copula. Part (b) requires a first-principles derivation of Kendall's tau, $\\tau$, for this copula family.\n\nPart (a): Validation of the FGM Copula\n\nFirst, we derive the bivariate distribution function $C(u,v)$ by integrating the density $c(u,v)$ over the rectangle $[0,u]\\times[0,v]$.\nThe density is given as $c(s,t) = 1 + \\theta(1 - 2s)(1 - 2t)$ for $(s,t) \\in [0,1]^2$.\n$$\nC(u,v) = \\int_0^u \\int_0^v c(s,t) \\, dt \\, ds = \\int_0^u \\int_0^v \\left[1 + \\theta(1 - 2s)(1 - 2t)\\right] \\, dt \\, ds\n$$\nWe integrate with respect to $t$ first:\n$$\n\\int_0^v \\left[1 + \\theta(1 - 2s)(1 - 2t)\\right] \\, dt = \\left[t + \\theta(1 - 2s)(t - t^2)\\right]_{t=0}^{t=v} = v + \\theta(1 - 2s)(v - v^2)\n$$\nNow, we integrate this result with respect to $s$:\n$$\nC(u,v) = \\int_0^u \\left[v + \\theta(1 - 2s)(v - v^2)\\right] \\, ds = \\left[vs + \\theta(s - s^2)(v - v^2)\\right]_{s=0}^{s=u}\n$$\n$$\nC(u,v) = uv + \\theta(u - u^2)(v - v^2) = uv + \\theta u(1-u)v(1-v)\n$$\nThis is the candidate for the copula distribution function. Now, we must verify the three conditions for it to be a valid copula for some range of $\\theta \\in \\mathbb{R}$.\n\n1.  Groundedness: A function $C$ is grounded if $C(u,0) = 0$ for all $u \\in [0,1]$ and $C(0,v) = 0$ for all $v \\in [0,1]$.\n    $$\n    C(u,0) = u(0) + \\theta u(1-u)(0)(1-0) = 0\n    $$\n    $$\n    C(0,v) = (0)v + \\theta (0)(1-0)v(1-v) = 0\n    $$\n    The function is grounded for any value of $\\theta$.\n\n2.  Uniform Marginals: The function must satisfy $C(u,1) = u$ for all $u \\in [0,1]$ and $C(1,v) = v$ for all $v \\in [0,1]$.\n    $$\n    C(u,1) = u(1) + \\theta u(1-u)(1)(1-1) = u + \\theta u(1-u)(0) = u\n    $$\n    $$\n    C(1,v) = (1)v + \\theta (1)(1-1)v(1-v) = v + \\theta (0)v(1-v) = v\n    $$\n    The function has uniform marginals for any value of $\\theta$.\n\n3.  $2$-increasingness: The function $C$ must be $2$-increasing, which means the volume $V_C(R)$ of any rectangle $R = [u_1, u_2] \\times [v_1, v_2] \\subseteq [0,1]^2$ must be non-negative.\n    $$\n    V_C(R) = C(u_2,v_2)-C(u_2,v_1)-C(u_1,v_2)+C(u_1,v_1) \\ge 0\n    $$\n    Since the density $c(u,v) = \\frac{\\partial^2 C}{\\partial u \\partial v}$ exists, this condition is equivalent to requiring the density to be non-negative everywhere on its domain, i.e., $c(u,v) \\ge 0$ for all $(u,v) \\in [0,1]^2$.\n    We must find the values of $\\theta$ for which:\n    $$\n    1 + \\theta\\,(1 - 2u)(1 - 2v) \\ge 0 \\quad \\forall (u,v) \\in [0,1]^2\n    $$\n    Let $x = 1 - 2u$ and $y = 1 - 2v$. As $u$ and $v$ vary in $[0,1]$, $x$ and $y$ vary in $[-1,1]$. The term $(1-2u)(1-2v)$ takes values in the interval $[-1,1]$. The extreme values are achieved at the corners of the unit square:\n    -   Maximum value: $(1-2(0))(1-2(0)) = 1$ at $(u,v)=(0,0)$.\n    -   Maximum value: $(1-2(1))(1-2(1)) = 1$ at $(u,v)=(1,1)$.\n    -   Minimum value: $(1-2(0))(1-2(1)) = -1$ at $(u,v)=(0,1)$.\n    -   Minimum value: $(1-2(1))(1-2(0)) = -1$ at $(u,v)=(1,0)$.\n\n    To ensure $c(u,v) \\ge 0$, we must check the inequality $1 + \\theta(1-2u)(1-2v) \\ge 0$ under the most restrictive conditions.\n    -   If $\\theta  0$, the minimum value of $\\theta(1-2u)(1-2v)$ is $\\theta \\times (-1) = -\\theta$. The condition becomes $1 - \\theta \\ge 0$, which implies $\\theta \\le 1$.\n    -   If $\\theta  0$, the minimum value of $\\theta(1-2u)(1-2v)$ is $\\theta \\times (1) = \\theta$. The condition becomes $1 + \\theta \\ge 0$, which implies $\\theta \\ge -1$.\n    -   If $\\theta = 0$, $c(u,v) = 1 \\ge 0$, which is valid.\n    Combining these cases, the function $C(u,v)$ is a valid copula if and only if $\\theta \\in [-1, 1]$.\n\nPart (b): Derivation of Kendall's Tau\n\nKendall's tau, $\\tau$, is defined as the difference between the probability of concordance and the probability of discordance for two independent, identically distributed pairs of random variables $(X,Y)$ and $(X',Y')$.\n$$\n\\tau = \\mathbb{P}\\big((X-X')(Y-Y')0\\big) - \\mathbb{P}\\big((X-X')(Y-Y')0\\big)\n$$\nLet $P_c$ be the probability of concordance and $P_d$ be the probability of discordance. For continuous random variables, the probability of a tie is $0$, so $P_c + P_d = 1$. This implies $\\tau = P_c - (1-P_c) = 2P_c - 1$.\n\nThe concordance event $(X-X')(Y-Y')0$ is equivalent to $(XX' \\text{ and } YY')$ or $(XX' \\text{ and } YY')$. By transforming to uniform marginals $U=F_X(X)$ and $V=F_Y(Y)$, this is equivalent to $(UU' \\text{ and } VV')$ or $(UU' \\text{ and } VV')$. The pairs $(U,V)$ and $(U',V')$ are i.i.d. with distribution function $C(u,v)$ and density $c(u,v)$.\n\nThe probability of concordance is $P_c = \\mathbb{P}(UU', VV') + \\mathbb{P}(UU', VV')$.\nWe can calculate $P_c$ by conditioning on the value of one pair, say $(U,V)=(u,v)$, and then taking the expectation. The conditional probability of concordance is:\n\\begin{align*}\n\\mathbb{P}\\left((u-U')(v-V')0\\right) = \\mathbb{P}(U'u, V'v) + \\mathbb{P}(U'u, V'v) \\\\\n= C(u,v) + \\int_u^1 \\int_v^1 c(s,t) \\, ds \\, dt \\\\\n= C(u,v) + \\left[ C(1,1) - C(1,v) - C(u,1) + C(u,v) \\right] \\\\\n= C(u,v) + \\left[ 1 - v - u + C(u,v) \\right] \\\\\n= 1 - u - v + 2C(u,v)\n\\end{align*}\nTaking the expectation with respect to $(U,V)$, which follow the distribution $C$:\n$$\nP_c = \\mathbb{E}\\left[1 - U - V + 2C(U,V)\\right] = 1 - \\mathbb{E}[U] - \\mathbb{E}[V] + 2\\mathbb{E}[C(U,V)]\n$$\nSince $U$ and $V$ are standard uniform random variables, $\\mathbb{E}[U] = 1/2$ and $\\mathbb{E}[V] = 1/2$.\n$$\nP_c = 1 - \\frac{1}{2} - \\frac{1}{2} + 2\\mathbb{E}[C(U,V)] = 2\\mathbb{E}[C(U,V)]\n$$\nSubstituting this into the expression for $\\tau$:\n$$\n\\tau = 2P_c - 1 = 2(2\\mathbb{E}[C(U,V)]) - 1 = 4\\mathbb{E}[C(U,V)] - 1\n$$\nThe expectation $\\mathbb{E}[C(U,V)]$ is given by the integral:\n$$\n\\mathbb{E}[C(U,V)] = \\int_0^1 \\int_0^1 C(u,v) c(u,v) \\, du \\, dv\n$$\nWe substitute the expressions for $C(u,v)$ and $c(u,v)$:\n$$\nC(u,v)c(u,v) = \\left[uv + \\theta u(1-u)v(1-v)\\right] \\left[1 + \\theta (1-2u)(1-2v)\\right]\n$$\n$$\n= uv + \\theta uv(1-2u)(1-2v) + \\theta u(1-u)v(1-v) + \\theta^2 u(1-u)(1-2u)v(1-v)(1-2v)\n$$\nThe integral is separable for each term. Let's evaluate the necessary one-dimensional integrals:\n$$\n\\int_0^1 u \\, du = \\left[\\frac{u^2}{2}\\right]_0^1 = \\frac{1}{2}\n$$\n$$\n\\int_0^1 u(1-u) \\, du = \\int_0^1 (u-u^2) \\, du = \\left[\\frac{u^2}{2} - \\frac{u^3}{3}\\right]_0^1 = \\frac{1}{2} - \\frac{1}{3} = \\frac{1}{6}\n$$\n$$\n\\int_0^1 u(1-2u) \\, du = \\int_0^1 (u-2u^2) \\, du = \\left[\\frac{u^2}{2} - \\frac{2u^3}{3}\\right]_0^1 = \\frac{1}{2} - \\frac{2}{3} = -\\frac{1}{6}\n$$\n$$\n\\int_0^1 u(1-u)(1-2u) \\, du = \\int_0^1 (u-3u^2+2u^3) \\, du = \\left[\\frac{u^2}{2} - u^3 + \\frac{u^4}{2}\\right]_0^1 = \\frac{1}{2} - 1 + \\frac{1}{2} = 0\n$$\nNow we compute $\\mathbb{E}[C(U,V)]$ by integrating the four terms:\n\\begin{align*}\n\\mathbb{E}[C(U,V)] = \\left(\\int_0^1 u \\, du\\right)^2 + \\theta \\left(\\int_0^1 u(1-2u) \\, du\\right)^2 + \\theta \\left(\\int_0^1 u(1-u) \\, du\\right)^2 \\\\\n\\quad + \\theta^2 \\left(\\int_0^1 u(1-u)(1-2u) \\, du\\right)^2 \\\\\n= \\left(\\frac{1}{2}\\right)^2 + \\theta \\left(-\\frac{1}{6}\\right)^2 + \\theta \\left(\\frac{1}{6}\\right)^2 + \\theta^2(0)^2 \\\\\n= \\frac{1}{4} + \\frac{\\theta}{36} + \\frac{\\theta}{36} + 0 = \\frac{1}{4} + \\frac{2\\theta}{36} = \\frac{1}{4} + \\frac{\\theta}{18}\n\\end{align*}\nFinally, we substitute this result into the expression for $\\tau$:\n$$\n\\tau = 4\\mathbb{E}[C(U,V)] - 1 = 4\\left(\\frac{1}{4} + \\frac{\\theta}{18}\\right) - 1 = 1 + \\frac{4\\theta}{18} - 1 = \\frac{2\\theta}{9}\n$$\nThis is the closed-form expression for Kendall's tau for the FGM copula, valid for $\\theta \\in [-1,1]$.",
            "answer": "$$\\boxed{\\frac{2\\theta}{9}}$$"
        },
        {
            "introduction": "A key advantage of using copulas is the ability to model tail dependence—the tendency for variables to experience extreme events together—which is often absent in simpler models. This advanced exercise guides you through deriving the tail dependence coefficients for the versatile Student-t copula, revealing how its parameters control joint extreme events. By analyzing its limiting behavior, you will also see how it relates to the well-known Gaussian copula, which lacks tail dependence. ",
            "id": "3300430",
            "problem": "Consider a bivariate Student-$t$ copula constructed from an elliptical scale-mixture representation of a bivariate Student-$t$ distribution with degrees of freedom $\\nu \\in (2,\\infty)$ and correlation parameter $\\rho \\in (-1,1)$. Let $(X,Y)$ be a bivariate Student-$t$ random vector with correlation $\\rho$ and common degrees of freedom $\\nu$, and define the copula variables $U = t_{\\nu}(X)$ and $V = t_{\\nu}(Y)$, where $t_{\\nu}$ denotes the cumulative distribution function (CDF) of the standard univariate Student-$t$ distribution with $\\nu$ degrees of freedom. The upper and lower tail dependence coefficients of the copula are defined by\n$$\n\\lambda_{U} \\equiv \\lim_{u \\to 1^{-}} \\mathbb{P}\\!\\left(U  u \\mid V  u\\right), \n\\qquad\n\\lambda_{L} \\equiv \\lim_{u \\to 0^{+}} \\mathbb{P}\\!\\left(U \\leq u \\mid V \\leq u\\right).\n$$\nStarting only from core definitions of copula tail dependence, the scale-mixture construction of the bivariate Student-$t$ law, and the exact conditional distribution of one Student-$t$ component given the other, derive a closed-form expression for $\\lambda_U$ and $\\lambda_L$ in terms of the CDF $t_{\\nu+1}$ of the standard univariate Student-$t$ distribution with $\\nu+1$ degrees of freedom. Then, analyze the limiting value of these coefficients as $\\nu \\to \\infty$ for fixed $\\rho \\in (-1,1)$. Your final answer must be a single closed-form analytic expression for $\\lambda_U=\\lambda_L$ together with its limit as $\\nu \\to \\infty$, presented as a row matrix. No numerical approximation or rounding is required.",
            "solution": "First, we establish that the upper and lower tail dependence coefficients are equal. The bivariate Student-$t$ distribution is an elliptical distribution, which is centrally symmetric. This implies that its copula is radially symmetric, meaning the distribution of $(1-U, 1-V)$ is the same as that of $(U,V)$. A direct consequence of this property is that $\\lambda_L = \\lambda_U$. We will derive this common value, denoted by $\\lambda$.\n\nThe upper tail dependence coefficient can be conveniently expressed for derivation purposes as $\\lambda_U = 2 \\lim_{u \\to 1^-} \\mathbb{P}(V  u \\mid U=u)$. We can translate this back to the original Student-$t$ variables, $X$ and $Y$. Let $x_u = t_\\nu^{-1}(u)$. As $u \\to 1^-$, we have $x_u \\to \\infty$. The expression becomes:\n$$\n\\lambda = 2 \\lim_{x \\to \\infty} \\mathbb{P}(Y  x \\mid X=x)\n$$\nWe now use the given property for the conditional distribution. The random variable $T = \\frac{Y - \\rho x}{\\sqrt{\\frac{\\nu+x^2}{\\nu+1}(1-\\rho^2)}}$ is distributed as a standard Student-$t$ with $\\nu+1$ degrees of freedom, which we denote by $T_{\\nu+1}$. We can express the conditional probability as:\n$$\n\\mathbb{P}(Y  x \\mid X=x) = \\mathbb{P}\\left(\\rho x + T_{\\nu+1}\\sqrt{\\frac{\\nu+x^2}{\\nu+1}(1-\\rho^2)}  x \\right) = \\mathbb{P}\\left(T_{\\nu+1}  \\frac{x(1 - \\rho)}{\\sqrt{\\frac{\\nu+x^2}{\\nu+1}(1-\\rho^2)}} \\right)\n$$\nTo evaluate the limit as $x \\to \\infty$, we analyze the argument of the probability function:\n$$\n\\lim_{x \\to \\infty} \\frac{x(1 - \\rho)}{\\sqrt{\\frac{x^2(1+\\nu/x^2)}{\\nu+1}(1-\\rho^2)}} = \\lim_{x \\to \\infty} \\frac{x(1 - \\rho)}{\\frac{x\\sqrt{1-\\rho^2}}{\\sqrt{\\nu+1}}\\sqrt{1+\\nu/x^2}} = \\frac{(1 - \\rho)\\sqrt{\\nu+1}}{\\sqrt{(1-\\rho)(1+\\rho)}} = \\sqrt{\\frac{(\\nu+1)(1-\\rho)}{1+\\rho}}\n$$\nThe limiting conditional probability is therefore $\\mathbb{P}\\left(T_{\\nu+1} > \\sqrt{\\frac{(\\nu+1)(1-\\rho)}{1+\\rho}}\\right)$. Using the CDF $t_{\\nu+1}$ of the $T_{\\nu+1}$ distribution and the symmetry property $1-t_{\\nu+1}(z) = t_{\\nu+1}(-z)$, we can write this as $t_{\\nu+1}\\left(-\\sqrt{\\frac{(\\nu+1)(1-\\rho)}{1+\\rho}}\\right)$.\nSubstituting this back into the expression for $\\lambda$, we obtain the closed-form expression for the tail dependence coefficients:\n$$\n\\lambda = \\lambda_U = \\lambda_L = 2 t_{\\nu+1}\\left(-\\sqrt{\\frac{(\\nu+1)(1-\\rho)}{1+\\rho}}\\right)\n$$\nFinally, we analyze the limit of $\\lambda$ as $\\nu \\to \\infty$. As the degrees of freedom $\\nu$ approach infinity, the Student-$t$ distribution $t_{\\nu+1}$ converges to the standard Normal distribution, with CDF $\\Phi$. The argument of the CDF, $-\\sqrt{\\frac{(\\nu+1)(1-\\rho)}{1+\\rho}}$, approaches $-\\infty$ since $\\rho \\in (-1,1)$.\nTherefore, the limit is:\n$$\n\\lim_{\\nu \\to \\infty} \\lambda = 2 \\lim_{A \\to -\\infty} \\Phi(A) = 2 \\times 0 = 0\n$$\nThis result is consistent with the fact that the Student-$t$ copula converges to the Gaussian copula, which is known to have zero tail dependence. The final answer comprises the general expression for $\\lambda$ and its limit.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 t_{\\nu+1}\\left(-\\sqrt{\\frac{(\\nu+1)(1-\\rho)}{1+\\rho}}\\right)  0\n\\end{pmatrix}\n}\n$$"
        }
    ]
}