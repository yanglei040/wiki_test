{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of any multilevel or multi-index method hinges on a fundamental principle: allocating computational effort wisely across different components. This first practice drills down into the core optimization problem, where you will use the method of Lagrange multipliers to determine the optimal way to distribute samples to either minimize variance for a fixed budget or minimize cost for a target precision . Mastering this foundational technique is the first step toward understanding the remarkable efficiency of advanced Monte Carlo methods.",
            "id": "3321940",
            "problem": "Consider a finite multi-index set $\\mathcal{I}$, where each multi-index $\\alpha \\in \\mathcal{I}$ labels an independent incremental random variable $\\Delta_{\\alpha}$ used in Multi-Index Monte Carlo (MIMC) and unbiased multilevel estimators. For each $\\alpha \\in \\mathcal{I}$, you are given two positive parameters: the per-sample computational cost $\\kappa_{\\alpha}  0$ and the variance $\\mathrm{Var}[\\Delta_{\\alpha}] = v_{\\alpha} \\ge 0$. Suppose you build an unbiased estimator of the global quantity of interest as a weighted sum of independent sample averages across indices, using $n_{\\alpha}$ independent samples for index $\\alpha$, and assume independence across all $\\alpha \\in \\mathcal{I}$. The total expected computational cost equals $\\sum_{\\alpha \\in \\mathcal{I}} \\kappa_{\\alpha} n_{\\alpha}$, and under independence, the variance of the estimator equals $\\sum_{\\alpha \\in \\mathcal{I}} \\frac{v_{\\alpha}}{n_{\\alpha}}$.\n\nStarting only from the definitions of variance of independent sums and expected cost additivity, design a method to choose real-valued allocations $n_{\\alpha} \\ge 0$ across indices to optimize one of two objectives:\n- Objective A: Minimize estimator variance subject to a fixed total expected computational budget.\n- Objective B: Minimize total expected computational cost subject to a fixed target variance.\n\nYour program must implement the derived optimal allocation and compute the optimal objective value (not the allocation itself) for each test case. Treat the allocations $n_{\\alpha}$ as nonnegative real variables (do not round to integers). If any $v_{\\alpha} = 0$, your method must handle this case correctly.\n\nUse the following test suite. Each test case is specified by the pair of arrays of parameters and one scalar constraint. For Objective A, the constraint is the budget $B$. For Objective B, the constraint is the target variance $V_{\\text{target}}$.\n\n- Test case $1$ (Objective A): $v = \\left[0.04, 0.01, 0.0025\\right]$, $c = \\left[1, 2, 4\\right]$, $B = 100$.\n- Test case $2$ (Objective A): $v = \\left[0.0, 0.09, 0.01\\right]$, $c = \\left[1, 1, 10\\right]$, $B = 50$.\n- Test case $3$ (Objective B): $v = \\left[0.5, 0.2, 0.05, 0.025\\right]$, $c = \\left[4, 2, 1, 0.5\\right]$, $V_{\\text{target}} = 0.01$.\n- Test case $4$ (Objective A): $v = \\left[10^{-6}, 1.0, 0.1\\right]$, $c = \\left[1000, 1, 5\\right]$, $B = 200$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The entry for each test case must be the optimal objective value as a floating-point number, ordered in the same sequence as the test suite, i.e., $\\left[\\text{result}_1, \\text{result}_2, \\text{result}_3, \\text{result}_4\\right]$. No physical units are involved in this problem. Angles do not appear. Percentages must not be used in the output.",
            "solution": "The problem asks for an optimal allocation of computational effort in a Multi-Index Monte Carlo (MIMC) or multilevel setting. This is a classic constrained optimization problem solvable using the method of Lagrange multipliers. The variables to be optimized are the number of samples $n_{\\alpha} \\ge 0$ for each multi-index $\\alpha$ in a finite set $\\mathcal{I}$. The two objective functions are the total variance $V$ and the total computational cost $C$, defined as:\n$$V(\\mathbf{n}) = \\sum_{\\alpha \\in \\mathcal{I}} \\frac{v_{\\alpha}}{n_{\\alpha}}$$\n$$C(\\mathbf{n}) = \\sum_{\\alpha \\in \\mathcal{I}} \\kappa_{\\alpha} n_{\\alpha}$$\nwhere $v_{\\alpha} \\ge 0$ is the variance and $\\kappa_{\\alpha}  0$ is the cost per sample for index $\\alpha$. The vector $\\mathbf{n}$ contains the allocations $\\{n_{\\alpha}\\}_{\\alpha \\in \\mathcal{I}}$.\n\nFirst, we address the case where all $v_{\\alpha}  0$. In this scenario, any optimal allocation must have $n_{\\alpha}  0$ for all $\\alpha$, otherwise the variance $V(\\mathbf{n})$ would be infinite. The problem can be treated as an optimization over positive real numbers. We will later show how this framework correctly handles the case where some $v_{\\alpha} = 0$.\n\n### Objective A: Minimize Variance for a Fixed Budget\n\nThe first optimization problem is to minimize the total variance $V(\\mathbf{n})$ subject to a fixed total computational cost, or budget, $B  0$.\n$$ \\text{Minimize } V(\\mathbf{n}) = \\sum_{\\alpha \\in \\mathcal{I}} \\frac{v_{\\alpha}}{n_{\\alpha}} \\quad \\text{subject to} \\quad \\sum_{\\alpha \\in \\mathcal{I}} \\kappa_{\\alpha} n_{\\alpha} = B $$\nThis is a convex optimization problem, as the objective function is a sum of convex functions and the constraint is linear. We introduce a Lagrange multiplier $\\lambda$ and form the Lagrangian function $\\mathcal{L}(\\mathbf{n}, \\lambda)$:\n$$ \\mathcal{L}(\\mathbf{n}, \\lambda) = \\sum_{\\alpha \\in \\mathcal{I}} \\frac{v_{\\alpha}}{n_{\\alpha}} + \\lambda \\left( \\left( \\sum_{\\alpha \\in \\mathcal{I}} \\kappa_{\\alpha} n_{\\alpha} \\right) - B \\right) $$\nTo find the stationary points, we compute the partial derivative of $\\mathcal{L}$ with respect to each $n_{\\alpha}$ and set it to zero:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial n_{\\alpha}} = -\\frac{v_{\\alpha}}{n_{\\alpha}^2} + \\lambda \\kappa_{\\alpha} = 0 $$\nSolving for $n_{\\alpha}$ yields:\n$$ n_{\\alpha}^2 = \\frac{v_{\\alpha}}{\\lambda \\kappa_{\\alpha}} \\implies n_{\\alpha} = \\frac{1}{\\sqrt{\\lambda}} \\sqrt{\\frac{v_{\\alpha}}{\\kappa_{\\alpha}}} $$\nSince $n_{\\alpha}$, $\\kappa_{\\alpha}$, and $v_{\\alpha}$ are positive, the Lagrange multiplier $\\lambda$ must also be positive. We determine $\\lambda$ by substituting the expression for $n_{\\alpha}$ into the budget constraint:\n$$ \\sum_{\\alpha \\in \\mathcal{I}} \\kappa_{\\alpha} \\left( \\frac{1}{\\sqrt{\\lambda}} \\sqrt{\\frac{v_{\\alpha}}{\\kappa_{\\alpha}}} \\right) = B $$\n$$ \\frac{1}{\\sqrt{\\lambda}} \\sum_{\\alpha \\in \\mathcal{I}} \\sqrt{\\kappa_{\\alpha} v_{\\alpha}} = B $$\nSolving for $\\sqrt{\\lambda}$:\n$$ \\sqrt{\\lambda} = \\frac{1}{B} \\sum_{\\beta \\in \\mathcal{I}} \\sqrt{\\kappa_{\\beta} v_{\\beta}} $$\nThe optimal allocation $n_{\\alpha}^*$ is therefore:\n$$ n_{\\alpha}^* = \\frac{B}{\\sum_{\\beta \\in \\mathcal{I}} \\sqrt{\\kappa_{\\beta} v_{\\beta}}} \\sqrt{\\frac{v_{\\alpha}}{\\kappa_{\\alpha}}} $$\nThe problem asks for the minimum variance $V_{min}$, which is obtained by substituting $n_{\\alpha}^*$ back into the variance expression:\n$$ V_{min} = \\sum_{\\alpha \\in \\mathcal{I}} \\frac{v_{\\alpha}}{n_{\\alpha}^*} = \\sum_{\\alpha \\in \\mathcal{I}} v_{\\alpha} \\left( \\frac{\\sum_{\\beta \\in \\mathcal{I}} \\sqrt{\\kappa_{\\beta} v_{\\beta}}}{B} \\sqrt{\\frac{\\kappa_{\\alpha}}{v_{\\alpha}}} \\right) = \\frac{1}{B} \\left( \\sum_{\\beta \\in \\mathcal{I}} \\sqrt{\\kappa_{\\beta} v_{\\beta}} \\right) \\left( \\sum_{\\alpha \\in \\mathcal{I}} \\sqrt{v_{\\alpha} \\kappa_{\\alpha}} \\right) $$\nThis simplifies to the final expression for the minimum variance:\n$$ V_{min} = \\frac{1}{B} \\left( \\sum_{\\alpha \\in \\mathcal{I}} \\sqrt{v_{\\alpha} \\kappa_{\\alpha}} \\right)^2 $$\n\n### Objective B: Minimize Cost for a Fixed Variance\n\nThe second optimization problem is to minimize the total cost $C(\\mathbf{n})$ subject to a fixed target variance $V_{\\text{target}}  0$.\n$$ \\text{Minimize } C(\\mathbf{n}) = \\sum_{\\alpha \\in \\mathcal{I}} \\kappa_{\\alpha} n_{\\alpha} \\quad \\text{subject to} \\quad \\sum_{\\alpha \\in \\mathcal{I}} \\frac{v_{\\alpha}}{n_{\\alpha}} = V_{\\text{target}} $$\nThis is also a convex optimization problem (linear objective function, convex constraint set). We form the Lagrangian with multiplier $\\mu$:\n$$ \\mathcal{L}(\\mathbf{n}, \\mu) = \\sum_{\\alpha \\in \\mathcal{I}} \\kappa_{\\alpha} n_{\\alpha} + \\mu \\left( \\left( \\sum_{\\alpha \\in \\mathcal{I}} \\frac{v_{\\alpha}}{n_{\\alpha}} \\right) - V_{\\text{target}} \\right) $$\nSetting the partial derivatives with respect to $n_{\\alpha}$ to zero:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial n_{\\alpha}} = \\kappa_{\\alpha} - \\mu \\frac{v_{\\alpha}}{n_{\\alpha}^2} = 0 $$\nSolving for $n_{\\alpha}$ gives a similar relationship:\n$$ n_{\\alpha}^2 = \\mu \\frac{v_{\\alpha}}{\\kappa_{\\alpha}} \\implies n_{\\alpha} = \\sqrt{\\mu} \\sqrt{\\frac{v_{\\alpha}}{\\kappa_{\\alpha}}} $$\nHere, $\\mu$ must be positive. We find $\\mu$ using the variance constraint:\n$$ \\sum_{\\alpha \\in \\mathcal{I}} \\frac{v_{\\alpha}}{\\sqrt{\\mu} \\sqrt{v_{\\alpha}/\\kappa_{\\alpha}}} = V_{\\text{target}} $$\n$$ \\frac{1}{\\sqrt{\\mu}} \\sum_{\\alpha \\in \\mathcal{I}} \\sqrt{v_{\\alpha} \\kappa_{\\alpha}} = V_{\\text{target}} $$\nSolving for $\\sqrt{\\mu}$:\n$$ \\sqrt{\\mu} = \\frac{1}{V_{\\text{target}}} \\sum_{\\beta \\in \\mathcal{I}} \\sqrt{v_{\\beta} \\kappa_{\\beta}} $$\nFinally, we compute the minimum cost $C_{min}$ by substituting the optimal allocation $n_{\\alpha}^* = \\sqrt{\\mu}\\sqrt{v_{\\alpha}/\\kappa_{\\alpha}}$ into the cost function:\n$$ C_{min} = \\sum_{\\alpha \\in \\mathcal{I}} \\kappa_{\\alpha} n_{\\alpha}^* = \\sum_{\\alpha \\in \\mathcal{I}} \\kappa_{\\alpha} \\left( \\sqrt{\\mu} \\sqrt{\\frac{v_{\\alpha}}{\\kappa_{\\alpha}}} \\right) = \\sqrt{\\mu} \\sum_{\\alpha \\in \\mathcal{I}} \\sqrt{\\kappa_{\\alpha} v_{\\alpha}} $$\nSubstituting the expression for $\\sqrt{\\mu}$:\n$$ C_{min} = \\left( \\frac{1}{V_{\\text{target}}} \\sum_{\\beta \\in \\mathcal{I}} \\sqrt{v_{\\beta} \\kappa_{\\beta}} \\right) \\left( \\sum_{\\alpha \\in \\mathcal{I}} \\sqrt{v_{\\alpha} \\kappa_{\\alpha}} \\right) $$\nThis simplifies to the final expression for the minimum cost:\n$$ C_{min} = \\frac{1}{V_{\\text{target}}} \\left( \\sum_{\\alpha \\in \\mathcal{I}} \\sqrt{v_{\\alpha} \\kappa_{\\alpha}} \\right)^2 $$\n\n### Synthesis and Handling of Zero Variances\n\nThe results for both objectives exhibit a clear duality. Let $S = \\sum_{\\alpha \\in \\mathcal{I}} \\sqrt{v_{\\alpha} \\kappa_{\\alpha}}$. Then:\n- Objective A: $V_{min} = S^2 / B$\n- Objective B: $C_{min} = S^2 / V_{\\text{target}}$\n\nThis implies the relationship $V_{min} B = C_{min} V_{\\text{target}} = S^2$, which encapsulates the fundamental trade-off between variance and cost.\n\nThe problem requires correct handling of cases where $v_{\\alpha} = 0$ for some $\\alpha$. Our derived formulas accommodate this case without any modification. If $v_{\\alpha_0} = 0$ for some index $\\alpha_0$, the corresponding term in the sum $S$ is $\\sqrt{v_{\\alpha_0} \\kappa_{\\alpha_0}} = \\sqrt{0 \\cdot \\kappa_{\\alpha_0}} = 0$. This term simply vanishes from the sum. This is consistent with the optimization logic: if a component $\\Delta_{\\alpha_0}$ contributes zero variance, the optimal strategy is to allocate zero samples ($n_{\\alpha_0}=0$) to it to minimize cost, reserving the entire budget for components with non-zero variance. Our formula for the optimal allocation $n_{\\alpha}^* \\propto \\sqrt{v_{\\alpha}/\\kappa_{\\alpha}}$ naturally yields $n_{\\alpha_0}^* = 0$ when $v_{\\alpha_0} = 0$. Therefore, the derived formulas are general and robust. The implementation will compute the sum $S$ over all indices, and the zero-variance terms will correctly contribute zero.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimal resource allocation problem for Multi-Index Monte Carlo\n    and computes the optimal objective value for a series of test cases.\n    \"\"\"\n\n    # The test suite is defined as a list of tuples.\n    # Each tuple contains: (objective_type, v_array, c_array, constraint_value)\n    # objective_type 'A' corresponds to minimizing variance for a fixed budget.\n    # objective_type 'B' corresponds to minimizing cost for a fixed variance.\n    test_cases = [\n        ('A', np.array([0.04, 0.01, 0.0025]), np.array([1, 2, 4]), 100.0),\n        ('A', np.array([0.0, 0.09, 0.01]), np.array([1, 1, 10]), 50.0),\n        ('B', np.array([0.5, 0.2, 0.05, 0.025]), np.array([4, 2, 1, 0.5]), 0.01),\n        ('A', np.array([1e-6, 1.0, 0.1]), np.array([1000, 1, 5]), 200.0),\n    ]\n\n    results = []\n    for objective_type, v, c, constraint in test_cases:\n        # The core of the solution is based on the quantity S, which is the sum\n        # of the square roots of the product of variance and cost for each index.\n        # S = sum(sqrt(v_alpha * kappa_alpha))\n        # numpy's vectorized operations make this calculation concise.\n        \n        # Taking the square root of the element-wise product of v and c.\n        # This correctly handles cases where v_alpha is 0, as sqrt(0 * k) = 0.\n        sqrt_vk = np.sqrt(v * c)\n        \n        # Summing the results to get the total S.\n        S = np.sum(sqrt_vk)\n        \n        # The optimal objective value is S^2 divided by the constraint value.\n        # For Objective A (minimize variance), the constraint is the budget B.\n        #   V_min = S^2 / B\n        # For Objective B (minimize cost), the constraint is the target variance V_target.\n        #   C_min = S^2 / V_target\n        optimal_value = (S ** 2) / constraint\n        \n        results.append(optimal_value)\n\n    # The final output must be a single line containing a comma-separated list\n    # of results enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solver.\nsolve()\n\n```"
        },
        {
            "introduction": "Now we apply the principle of optimal allocation to the classic Multilevel Monte Carlo (MLMC) estimator. This practice goes a step further by incorporating the trade-off between statistical error (variance) and discretization error (bias), which is controlled by choosing the finest resolution level, $L$. You will derive the optimal number of samples per level and the total computational work required to achieve a prescribed accuracy $\\varepsilon$ , revealing the celebrated $\\mathcal{O}(\\varepsilon^{-2})$ complexity of MLMC for favorable problems.",
            "id": "3321906",
            "problem": "Consider a sequence of discretized approximations $\\{Y_{l}\\}_{l \\in \\mathbb{N}_{0}}$ to a target random variable $X$ with finite mean, constructed by refining a mesh with refinement factor $2$ per level $l$. Assume the following standard rate conditions hold with positive constants $c_{w}$, $c_{s}$, and $c_{c}$ and exponents $\\alpha$, $\\beta$, and $\\gamma$:\n- Weak error (bias) rate: $|\\mathbb{E}[Y_{l} - X]| \\leq c_{w} \\, 2^{-\\alpha l}$ for all $l$.\n- Strong error (level-difference variance) rate under a faithful coupling of successive levels: $\\mathrm{Var}(Y_{l} - Y_{l-1}) \\leq c_{s} \\, 2^{-\\beta l}$ for all $l \\geq 1$, and $\\mathrm{Var}(Y_{0}) \\leq c_{s}$.\n- Per-sample work (cost) at level $l$: $C_{l} = c_{c} \\, 2^{\\gamma l}$ for all $l$.\n\nDefine the Multilevel Monte Carlo (MLMC) estimator $\\widehat{Q}$ at maximum level $L$ by the telescoping sum with independent sampling across levels,\n$$\n\\widehat{Q} \\;=\\; \\sum_{l=0}^{L} \\frac{1}{n_{l}} \\sum_{i=1}^{n_{l}} \\Delta_{l}^{(i)}, \\quad \\text{where} \\quad \\Delta_{0} := Y_{0}, \\;\\; \\Delta_{l} := Y_{l} - Y_{l-1} \\;\\; \\text{for} \\;\\; l \\geq 1,\n$$\nand where $n_{l} \\in \\mathbb{N}$ denotes the number of Monte Carlo samples at level $l$. Let the mean squared error (MSE) be $\\mathbb{E}\\big[(\\widehat{Q} - \\mathbb{E}[X])^{2}\\big] = \\mathrm{bias}^{2} + \\mathrm{variance}$, with the bias given by $\\mathbb{E}[Y_{L} - X]$ and the variance given by $\\sum_{l=0}^{L} \\mathrm{Var}(\\Delta_{l})/n_{l}$ under independence across levels.\n\nYou are given the specific parameter values $\\alpha = 2$, $\\beta = 3$, $\\gamma = 1$, $c_{w} = 3$, $c_{s} = 2$, and $c_{c} = 5$. For a prescribed accuracy target $\\varepsilon \\in (0,1)$, design the asymptotically optimal MLMC allocation $(L, \\{n_{l}\\}_{l=0}^{L})$ as $\\varepsilon \\to 0$ using only the bias-variance decomposition, the rate assumptions above, and first principles of constrained optimization. Choose $L$ and $\\{n_{l}\\}$ so that the MSE is at most $\\varepsilon^{2}$ by allocating equal error budgets to squared bias and variance. Then determine the leading-order asymptotic expression for the minimal expected computational work $\\mathbb{E}[\\mathcal{W}]$ as a function of $\\varepsilon$ in the form $K \\, \\varepsilon^{-2}$, where $K$ is a constant depending only on $c_{w}$, $c_{s}$, $c_{c}$, $\\alpha$, $\\beta$, and $\\gamma$. Provide the simplified analytic expression for $\\mathbb{E}[\\mathcal{W}]$ to leading order as $\\varepsilon \\to 0$. The final answer must be a single analytic expression; do not include units.",
            "solution": "We start from the mean squared error (MSE) decomposition for the Multilevel Monte Carlo (MLMC) estimator $\\widehat{Q}$:\n$$\n\\mathbb{E}\\big[(\\widehat{Q} - \\mathbb{E}[X])^{2}\\big] \\;=\\; \\big(\\mathbb{E}[Y_{L} - X]\\big)^{2} \\;+\\; \\sum_{l=0}^{L} \\frac{\\mathrm{Var}(\\Delta_{l})}{n_{l}},\n$$\nwhere $\\Delta_{0} := Y_{0}$ and $\\Delta_{l} := Y_{l} - Y_{l-1}$ for $l \\geq 1$, with independent sampling across levels. The expected work is\n$$\n\\mathbb{E}[\\mathcal{W}] \\;=\\; \\sum_{l=0}^{L} n_{l} \\, C_{l}.\n$$\nWe are given the rate assumptions\n$$\n|\\mathbb{E}[Y_{l} - X]| \\leq c_{w} \\, 2^{-\\alpha l}, \\quad \\mathrm{Var}(\\Delta_{l}) \\leq c_{s} \\, 2^{-\\beta l} \\;\\; (l \\geq 1), \\;\\; \\mathrm{Var}(\\Delta_{0}) \\leq c_{s}, \\quad C_{l} = c_{c} \\, 2^{\\gamma l}.\n$$\nWith the specific values $\\alpha = 2$, $\\beta = 3$, $\\gamma = 1$, $c_{w} = 3$, $c_{s} = 2$, $c_{c} = 5$, we have for all $l \\geq 1$,\n$$\n\\mathrm{Var}(\\Delta_{l}) \\leq 2 \\cdot 2^{-3l}, \\quad C_{l} = 5 \\cdot 2^{l}.\n$$\nWe allocate equal error budgets to squared bias and variance:\n$$\n\\big(\\mathbb{E}[Y_{L} - X]\\big)^{2} \\leq \\frac{\\varepsilon^{2}}{2}, \\qquad \\sum_{l=0}^{L} \\frac{\\mathrm{Var}(\\Delta_{l})}{n_{l}} \\leq \\frac{\\varepsilon^{2}}{2}.\n$$\nFirst, we choose the maximal level $L$ to satisfy the bias constraint based on the weak error rate:\n$$\n|\\mathbb{E}[Y_{L} - X]| \\leq c_{w} \\, 2^{-\\alpha L} \\leq \\frac{\\varepsilon}{\\sqrt{2}}.\n$$\nThis yields\n$$\n2^{-\\alpha L} \\leq \\frac{\\varepsilon}{\\sqrt{2}\\, c_{w}} \\;\\;\\Longleftrightarrow\\;\\; L \\geq \\frac{1}{\\alpha} \\log_{2}\\!\\left(\\frac{\\sqrt{2}\\, c_{w}}{\\varepsilon}\\right).\n$$\nWith $\\alpha = 2$ and $c_{w} = 3$, one convenient asymptotically optimal choice is\n$$\nL \\;=\\; \\left\\lceil \\frac{1}{2} \\log_{2}\\!\\left(\\frac{3\\sqrt{2}}{\\varepsilon}\\right) \\right\\rceil,\n$$\nwhich ensures the squared bias is at most $\\varepsilon^{2}/2$.\n\nSecond, we optimize the allocation $\\{n_{l}\\}_{l=0}^{L}$ to minimize work subject to the variance constraint. Let $V_{l} := \\mathrm{Var}(\\Delta_{l})$ and recall $C_{l}$ is the per-sample cost. We solve\n$$\n\\min_{\\{n_{l} \\geq 1\\}} \\sum_{l=0}^{L} n_{l} C_{l} \\quad \\text{subject to} \\quad \\sum_{l=0}^{L} \\frac{V_{l}}{n_{l}} \\leq \\frac{\\varepsilon^{2}}{2}.\n$$\nRelaxing $n_{l}$ to positive reals and using a Lagrange multiplier $\\lambda  0$, the Lagrangian is\n$$\n\\mathcal{L}(\\{n_{l}\\},\\lambda) \\;=\\; \\sum_{l=0}^{L} n_{l} C_{l} \\;+\\; \\lambda\\!\\left(\\sum_{l=0}^{L} \\frac{V_{l}}{n_{l}} - \\frac{\\varepsilon^{2}}{2}\\right).\n$$\nStationarity with respect to $n_{l}$ gives\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial n_{l}} \\;=\\; C_{l} \\;-\\; \\lambda \\frac{V_{l}}{n_{l}^{2}} \\;=\\; 0 \\;\\;\\Longrightarrow\\;\\; n_{l} \\;=\\; \\sqrt{\\lambda} \\, \\sqrt{\\frac{V_{l}}{C_{l}}}.\n$$\nImposing the variance constraint at equality yields\n$$\n\\sum_{l=0}^{L} \\frac{V_{l}}{n_{l}} \\;=\\; \\sum_{l=0}^{L} \\frac{V_{l}}{\\sqrt{\\lambda} \\, \\sqrt{V_{l}/C_{l}}} \\;=\\; \\frac{1}{\\sqrt{\\lambda}} \\sum_{l=0}^{L} \\sqrt{V_{l} C_{l}} \\;=\\; \\frac{\\varepsilon^{2}}{2}.\n$$\nThus\n$$\n\\sqrt{\\lambda} \\;=\\; \\frac{2}{\\varepsilon^{2}} \\sum_{l=0}^{L} \\sqrt{V_{l} C_{l}}.\n$$\nThe corresponding minimal expected work is\n$$\n\\mathbb{E}[\\mathcal{W}]_{\\min} \\;=\\; \\sum_{l=0}^{L} n_{l} C_{l} \\;=\\; \\sum_{l=0}^{L} \\left(\\sqrt{\\lambda} \\, \\sqrt{\\frac{V_{l}}{C_{l}}}\\right) C_{l} \\;=\\; \\sqrt{\\lambda} \\sum_{l=0}^{L} \\sqrt{V_{l} C_{l}} \\;=\\; \\frac{2}{\\varepsilon^{2}} \\left(\\sum_{l=0}^{L} \\sqrt{V_{l} C_{l}}\\right)^{2}.\n$$\nWe now evaluate the sum using the given rates. For $l \\geq 1$, $V_{l} \\leq 2 \\cdot 2^{-3l}$ and $C_{l} = 5 \\cdot 2^{l}$, so\n$$\n\\sqrt{V_{l} C_{l}} \\;\\leq\\; \\sqrt{(2 \\cdot 2^{-3l}) (5 \\cdot 2^{l})} \\;=\\; \\sqrt{10} \\, 2^{-(3-1)l/2} \\;=\\; \\sqrt{10} \\, 2^{-l}.\n$$\nFor $l = 0$, $V_{0} \\leq 2$ and $C_{0} = 5$, hence $\\sqrt{V_{0} C_{0}} \\leq \\sqrt{10}$. Therefore,\n$$\n\\sum_{l=0}^{L} \\sqrt{V_{l} C_{l}} \\;\\leq\\; \\sqrt{10} \\sum_{l=0}^{L} 2^{-l} \\;=\\; \\sqrt{10} \\, \\frac{1 - 2^{-(L+1)}}{1 - 2^{-1}} \\;=\\; 2 \\sqrt{10} \\, \\big(1 - 2^{-(L+1)}\\big).\n$$\nAs $\\varepsilon \\to 0$, we have $L \\to \\infty$, and the geometric tail $2^{-(L+1)} \\to 0$. Thus, to leading order,\n$$\n\\sum_{l=0}^{L} \\sqrt{V_{l} C_{l}} \\;\\sim\\; 2 \\sqrt{10}.\n$$\nSubstituting into the minimal work expression, we obtain the leading-order asymptotics\n$$\n\\mathbb{E}[\\mathcal{W}]_{\\min} \\;\\sim\\; \\frac{2}{\\varepsilon^{2}} \\left(2 \\sqrt{10}\\right)^{2} \\;=\\; \\frac{2}{\\varepsilon^{2}} \\cdot 4 \\cdot 10 \\;=\\; \\frac{80}{\\varepsilon^{2}}, \\quad \\text{as } \\varepsilon \\to 0.\n$$\nWe verify the regime consistency: since $\\beta = 3  \\gamma = 1$, the geometric ratio $2^{-(\\beta - \\gamma)/2} = 2^{-1}$ is strictly less than $1$, implying the sum $\\sum_{l \\geq 0} \\sqrt{V_{l} C_{l}}$ converges, and hence the optimal MLMC complexity scales as $\\varepsilon^{-2}$ without logarithmic penalty, as derived.\n\nFinally, the contribution of the bias choice $L = \\lceil \\frac{1}{2} \\log_{2}(\\frac{3\\sqrt{2}}{\\varepsilon}) \\rceil$ only affects lower-order terms via the negligible geometric tail, leaving the leading constant unchanged. Therefore, the leading-order minimal expected computational work is\n$$\n\\mathbb{E}[\\mathcal{W}]_{\\min} \\;\\sim\\; \\frac{80}{\\varepsilon^{2}} \\quad \\text{as } \\varepsilon \\to 0.\n$$\nThis completes the derivation from the bias-variance decomposition, discretization rates, and constrained optimization.",
            "answer": "$$\\boxed{\\frac{80}{\\varepsilon^{2}}}$$"
        },
        {
            "introduction": "Having mastered optimization for the one-dimensional hierarchy of MLMC, we now turn to the more general Multi-Index Monte Carlo (MIMC) framework. Before an estimator can be optimized, it must be well-defined; this final practice challenges you to establish the theoretical bedrock upon which MIMC estimators are built . You will derive the precise, component-wise conditions on a problem's decay and growth rates that ensure the resulting estimator is simultaneously unbiased, has finite variance, and incurs a finite expected computational cost.",
            "id": "3321945",
            "problem": "Consider a multi-index Monte Carlo construction for approximating the expectation of a target quantity. Let the dimension be $d \\in \\mathbb{N}$ and let the multi-index be $\\boldsymbol{n} = (n_1,\\ldots,n_d) \\in \\mathbb{Z}_+^d$. Assume there is a collection of random increments $\\Delta_{\\boldsymbol{n}}$ such that the exact expectation of interest can be represented as a convergent telescoping/multi-index sum of expectations $\\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} \\mathbb{E}[\\Delta_{\\boldsymbol{n}}]$, where the convergence is absolute. An unbiased multilevel estimator is constructed by independent Bernoulli selectors $B_{\\boldsymbol{n}} \\sim \\mathrm{Bernoulli}(q_{\\boldsymbol{n}})$, independent across $\\boldsymbol{n}$ and independent of the increments $\\Delta_{\\boldsymbol{n}}$, and the estimator is defined by\n$$\nY = \\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} \\frac{B_{\\boldsymbol{n}}}{q_{\\boldsymbol{n}}} \\Delta_{\\boldsymbol{n}},\n$$\nwhere the sampling probabilities $q_{\\boldsymbol{n}}$ are of product form\n$$\nq_{\\boldsymbol{n}} = c_q \\prod_{i=1}^d 2^{-r_i n_i},\n$$\nwith $0  c_q \\le 1$ chosen to satisfy $0 \\le q_{\\boldsymbol{n}} \\le 1$ for all $\\boldsymbol{n}$, and component-wise decay exponents $\\boldsymbol{r} = (r_1,\\ldots,r_d)$.\n\nAssume the following common asymptotic product models hold for the increment moments and the computational work, uniformly in $\\boldsymbol{n}$:\n- The first moment decays as $\\mathbb{E}[\\Delta_{\\boldsymbol{n}}] \\asymp c_\\mu \\prod_{i=1}^d 2^{-\\alpha_i n_i}$ with $c_\\mu  0$ and decay exponents $\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots,\\alpha_d)$.\n- The second moment decays as $\\mathbb{E}[\\Delta_{\\boldsymbol{n}}^2] \\asymp c_2 \\prod_{i=1}^d 2^{-\\beta_i n_i}$ with $c_2  0$ and decay exponents $\\boldsymbol{\\beta} = (\\beta_1,\\ldots,\\beta_d)$.\n- The computational work for level $\\boldsymbol{n}$ grows as $w(\\boldsymbol{n}) \\asymp c_w \\prod_{i=1}^d 2^{\\gamma_i n_i}$ with $c_w  0$ and growth exponents $\\boldsymbol{\\gamma} = (\\gamma_1,\\ldots,\\gamma_d)$.\n\nFrom first principles of expectation, variance, and independence in probability theory, the following properties are defined:\n- Unbiasedness: $\\mathbb{E}[Y] = \\sum_{\\boldsymbol{n}} \\mathbb{E}[\\Delta_{\\boldsymbol{n}}]$ has finite value equal to the target expectation.\n- Finite variance: $\\mathrm{Var}(Y)  \\infty$.\n- Finite expected cost: $\\mathbb{E}\\!\\left[\\sum_{\\boldsymbol{n}} B_{\\boldsymbol{n}} w(\\boldsymbol{n})\\right]  \\infty$.\n\nDerive, starting only from the definitions of expectation $\\mathbb{E}[\\cdot]$, variance $\\mathrm{Var}(\\cdot)$, independence of random variables, and absolute convergence of product-separable sums, explicit component-wise inequalities on $\\boldsymbol{\\alpha}$, $\\boldsymbol{\\beta}$, $\\boldsymbol{\\gamma}$, and $\\boldsymbol{r}$ that guarantee:\n1) Unbiasedness of $Y$,\n2) Finite variance of $Y$,\n3) Finite expected computational cost.\n\nThen, implement these conditions algorithmically: given $d$ and the vectors $\\boldsymbol{\\alpha}$, $\\boldsymbol{\\beta}$, $\\boldsymbol{\\gamma}$, and $\\boldsymbol{r}$, return a triplet of booleans indicating whether each of the three properties holds.\n\nYour program must evaluate the following test suite of parameter sets:\n- Test case $1$: $d = 2$, $\\boldsymbol{\\alpha} = (1.0, 0.8)$, $\\boldsymbol{\\beta} = (2.0, 1.5)$, $\\boldsymbol{\\gamma} = (0.3, 0.4)$, $\\boldsymbol{r} = (1.2, 1.0)$.\n- Test case $2$: $d = 3$, $\\boldsymbol{\\alpha} = (0.5, 1.0, 0.7)$, $\\boldsymbol{\\beta} = (1.0, 0.9, 1.1)$, $\\boldsymbol{\\gamma} = (0.6, 0.4, 0.3)$, $\\boldsymbol{r} = (0.8, 0.6, 0.7)$.\n- Test case $3$: $d = 2$, $\\boldsymbol{\\alpha} = (0.7, 0.9)$, $\\boldsymbol{\\beta} = (1.2, 1.0)$, $\\boldsymbol{\\gamma} = (0.1, 0.2)$, $\\boldsymbol{r} = (1.2, 0.5)$.\n- Test case $4$: $d = 2$, $\\boldsymbol{\\alpha} = (1.0, 1.0)$, $\\boldsymbol{\\beta} = (0.4, 0.7)$, $\\boldsymbol{\\gamma} = (0.5, 0.6)$, $\\boldsymbol{r} = (0.55, 0.65)$.\n- Test case $5$: $d = 3$, $\\boldsymbol{\\alpha} = (0.0, -0.1, 0.2)$, $\\boldsymbol{\\beta} = (1.5, 1.5, 1.5)$, $\\boldsymbol{\\gamma} = (0.5, 0.5, 0.5)$, $\\boldsymbol{r} = (1.0, 1.0, 1.0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a three-entry list of booleans $[\\text{unbiased}, \\text{finite\\_variance}, \\text{finite\\_cost}]$ for the corresponding test case, with no spaces. For example, a valid format is $[[\\text{True},\\text{False},\\text{True}],[\\text{False},\\text{False},\\text{True}]]$.",
            "solution": "The problem requires the derivation of conditions for an unbiased multi-index Monte Carlo estimator to be well-defined, possess finite variance, and have a finite expected computational cost. These conditions will be derived from first principles and the provided asymptotic models for the moments of the increments and the computational work.\n\nLet the multi-index be $\\boldsymbol{n} = (n_1, \\ldots, n_d) \\in \\mathbb{Z}_+^d$. The estimator is given by\n$$ Y = \\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} \\frac{B_{\\boldsymbol{n}}}{q_{\\boldsymbol{n}}} \\Delta_{\\boldsymbol{n}} $$\nwhere $B_{\\boldsymbol{n}} \\sim \\mathrm{Bernoulli}(q_{\\boldsymbol{n}})$ are independent Bernoulli random variables, also independent of the random increments $\\Delta_{\\boldsymbol{n}}$. The sampling probabilities are $q_{\\boldsymbol{n}} = c_q \\prod_{i=1}^d 2^{-r_i n_i}$. The asymptotic behaviors of the increment moments and work are:\n$\\mathbb{E}[\\Delta_{\\boldsymbol{n}}] \\asymp c_\\mu \\prod_{i=1}^d 2^{-\\alpha_i n_i}$,\n$\\mathbb{E}[\\Delta_{\\boldsymbol{n}}^2] \\asymp c_2 \\prod_{i=1}^d 2^{-\\beta_i n_i}$, and\n$w(\\boldsymbol{n}) \\asymp c_w \\prod_{i=1}^d 2^{\\gamma_i n_i}$.\n\nA crucial tool for this analysis is the convergence criterion for a multi-dimensional sum of product-form terms. A sum $\\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} \\prod_{i=1}^d c_i 2^{-k_i n_i}$ is absolutely convergent if and only if each of the corresponding one-dimensional geometric series converges. This occurs when the base of each geometric series has a magnitude less than $1$, which for $2^{-k_i}$ implies $2^{-k_i}  1$, or $k_i  0$. Thus, for the multi-dimensional sum to converge, we require $k_i  0$ for all $i = 1, \\ldots, d$.\n\n1. Unbiasedness and Finiteness of Expectation\nThe problem defines the unbiasedness property as $\\mathbb{E}[Y] = \\sum_{\\boldsymbol{n}} \\mathbb{E}[\\Delta_{\\boldsymbol{n}}]$ having a finite value. To establish this, we first compute the expectation of $Y$. By linearity of expectation, which can be applied if the sum of the absolute expectations is finite (Fubini-Tonelli theorem), we have:\n$$ \\mathbb{E}[Y] = \\mathbb{E}\\left[\\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} \\frac{B_{\\boldsymbol{n}}}{q_{\\boldsymbol{n}}} \\Delta_{\\boldsymbol{n}}\\right] = \\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} \\mathbb{E}\\left[\\frac{B_{\\boldsymbol{n}}}{q_{\\boldsymbol{n}}} \\Delta_{\\boldsymbol{n}}\\right] $$\nFor each term, we use the independence of $B_{\\boldsymbol{n}}$ and $\\Delta_{\\boldsymbol{n}}$:\n$$ \\mathbb{E}\\left[\\frac{B_{\\boldsymbol{n}}}{q_{\\boldsymbol{n}}} \\Delta_{\\boldsymbol{n}}\\right] = \\frac{1}{q_{\\boldsymbol{n}}} \\mathbb{E}[B_{\\boldsymbol{n}}] \\mathbb{E}[\\Delta_{\\boldsymbol{n}}] $$\nSince $B_{\\boldsymbol{n}}$ is a Bernoulli random variable with parameter $q_{\\boldsymbol{n}}$, its expectation is $\\mathbb{E}[B_{\\boldsymbol{n}}] = q_{\\boldsymbol{n}}$. Substituting this into the expression gives:\n$$ \\frac{1}{q_{\\boldsymbol{n}}} (q_{\\boldsymbol{n}}) \\mathbb{E}[\\Delta_{\\boldsymbol{n}}] = \\mathbb{E}[\\Delta_{\\boldsymbol{n}}] $$\nThus, the estimator $Y$ is formally unbiased, i.e., $\\mathbb{E}[Y] = \\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} \\mathbb{E}[\\Delta_{\\boldsymbol{n}}]$. The condition for this property to hold meaningfully is that the expectation must be finite. This requires the sum $\\sum_{\\boldsymbol{n}} \\mathbb{E}[\\Delta_{\\boldsymbol{n}}]$ to be absolutely convergent, a condition explicitly stated in the problem setup. We analyze the convergence of $\\sum_{\\boldsymbol{n}} |\\mathbb{E}[\\Delta_{\\boldsymbol{n}}]|$. Using the asymptotic model:\n$$ \\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} |\\mathbb{E}[\\Delta_{\\boldsymbol{n}}]| \\asymp \\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} c_\\mu \\prod_{i=1}^d 2^{-\\alpha_i n_i} $$\nFor this sum to converge, we require the exponent of the decay term to be positive for each component $i$:\n$$ \\alpha_i  0 \\quad \\text{for all } i = 1, \\ldots, d $$\nThis set of inequalities is the condition for the unbiasedness property to hold with a finite expectation.\n\n2. Finite Variance\nThe variance of $Y$ must be finite, $\\mathrm{Var}(Y)  \\infty$. The terms $\\frac{B_{\\boldsymbol{n}}}{q_{\\boldsymbol{n}}} \\Delta_{\\boldsymbol{n}}$ are mutually independent across $\\boldsymbol{n}$, assuming the increments $\\Delta_{\\boldsymbol{n}}$ are also independent for different $\\boldsymbol{n}$ (a standard assumption in this context). Therefore, the variance of the sum is the sum of the variances:\n$$ \\mathrm{Var}(Y) = \\mathrm{Var}\\left(\\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} \\frac{B_{\\boldsymbol{n}}}{q_{\\boldsymbol{n}}} \\Delta_{\\boldsymbol{n}}\\right) = \\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} \\mathrm{Var}\\left(\\frac{B_{\\boldsymbol{n}}}{q_{\\boldsymbol{n}}} \\Delta_{\\boldsymbol{n}}\\right) $$\nThe variance of a single term is $\\mathrm{Var}(Z_{\\boldsymbol{n}}) = \\mathbb{E}[Z_{\\boldsymbol{n}}^2] - (\\mathbb{E}[Z_{\\boldsymbol{n}}])^2$, where $Z_{\\boldsymbol{n}} = \\frac{B_{\\boldsymbol{n}}}{q_{\\boldsymbol{n}}} \\Delta_{\\boldsymbol{n}}$. We have already shown $\\mathbb{E}[Z_{\\boldsymbol{n}}] = \\mathbb{E}[\\Delta_{\\boldsymbol{n}}]$. We now compute the second moment:\n$$ \\mathbb{E}[Z_{\\boldsymbol{n}}^2] = \\mathbb{E}\\left[\\left(\\frac{B_{\\boldsymbol{n}}}{q_{\\boldsymbol{n}}} \\Delta_{\\boldsymbol{n}}\\right)^2\\right] = \\frac{1}{q_{\\boldsymbol{n}}^2} \\mathbb{E}[B_{\\boldsymbol{n}}^2 \\Delta_{\\boldsymbol{n}}^2] $$\nUsing the independence of $B_{\\boldsymbol{n}}$ and $\\Delta_{\\boldsymbol{n}}$, and the property that for a Bernoulli variable $B_{\\boldsymbol{n}}^2 = B_{\\boldsymbol{n}}$, we get:\n$$ \\mathbb{E}[Z_{\\boldsymbol{n}}^2] = \\frac{1}{q_{\\boldsymbol{n}}^2} \\mathbb{E}[B_{\\boldsymbol{n}}^2] \\mathbb{E}[\\Delta_{\\boldsymbol{n}}^2] = \\frac{1}{q_{\\boldsymbol{n}}^2} \\mathbb{E}[B_{\\boldsymbol{n}}] \\mathbb{E}[\\Delta_{\\boldsymbol{n}}^2] = \\frac{q_{\\boldsymbol{n}}}{q_{\\boldsymbol{n}}^2} \\mathbb{E}[\\Delta_{\\boldsymbol{n}}^2] = \\frac{1}{q_{\\boldsymbol{n}}} \\mathbb{E}[\\Delta_{\\boldsymbol{n}}^2] $$\nSo, the variance of $Y$ is:\n$$ \\mathrm{Var}(Y) = \\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} \\left( \\frac{1}{q_{\\boldsymbol{n}}} \\mathbb{E}[\\Delta_{\\boldsymbol{n}}^2] - \\left(\\mathbb{E}[\\Delta_{\\boldsymbol{n}}]\\right)^2 \\right) $$\nFor this sum of non-negative terms to converge, it is necessary and sufficient that the sums of the positive and negative parts converge separately.\na) Convergence of $\\sum_{\\boldsymbol{n}} \\frac{1}{q_{\\boldsymbol{n}}} \\mathbb{E}[\\Delta_{\\boldsymbol{n}}^2]$:\nThe term behaves as $\\frac{1}{q_{\\boldsymbol{n}}} \\mathbb{E}[\\Delta_{\\boldsymbol{n}}^2] \\asymp \\left(\\frac{1}{c_q} \\prod_{i=1}^d 2^{r_i n_i}\\right) \\left(c_2 \\prod_{i=1}^d 2^{-\\beta_i n_i}\\right) = \\frac{c_2}{c_q} \\prod_{i=1}^d 2^{-(\\beta_i - r_i) n_i}$.\nConvergence requires $\\beta_i - r_i  0$, or $\\beta_i  r_i$ for all $i = 1, \\ldots, d$.\nb) Convergence of $\\sum_{\\boldsymbol{n}} (\\mathbb{E}[\\Delta_{\\boldsymbol{n}}])^2$:\nThe term behaves as $(\\mathbb{E}[\\Delta_{\\boldsymbol{n}}])^2 \\asymp \\left(c_\\mu \\prod_{i=1}^d 2^{-\\alpha_i n_i}\\right)^2 = c_\\mu^2 \\prod_{i=1}^d 2^{-2\\alpha_i n_i}$.\nConvergence requires $2\\alpha_i  0$, or $\\alpha_i  0$ for all $i = 1, \\ldots, d$. This is the same condition as for the finiteness of the expectation.\nFor the total variance to be finite, both conditions must hold. Therefore, the complete condition for finite variance is:\n$$ (\\beta_i  r_i \\text{ for all } i = 1, \\ldots, d) \\quad \\text{AND} \\quad (\\alpha_i  0 \\text{ for all } i = 1, \\ldots, d) $$\n\n3. Finite Expected Cost\nThe total computational cost is $C = \\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} B_{\\boldsymbol{n}} w(\\boldsymbol{n})$. We need its expectation to be finite. Since cost is a non-negative quantity, we can interchange expectation and summation (Tonelli's theorem):\n$$ \\mathbb{E}[C] = \\mathbb{E}\\left[\\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} B_{\\boldsymbol{n}} w(\\boldsymbol{n})\\right] = \\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} \\mathbb{E}[B_{\\boldsymbol{n}} w(\\boldsymbol{n})] $$\nThe work $w(\\boldsymbol{n})$ for a given level $\\boldsymbol{n}$ is deterministic. So, $\\mathbb{E}[B_{\\boldsymbol{n}} w(\\boldsymbol{n})] = w(\\boldsymbol{n}) \\mathbb{E}[B_{\\boldsymbol{n}}] = w(\\boldsymbol{n}) q_{\\boldsymbol{n}}$.\nThe expected cost is therefore $\\mathbb{E}[C] = \\sum_{\\boldsymbol{n} \\in \\mathbb{Z}_+^d} q_{\\boldsymbol{n}} w(\\boldsymbol{n})$. We check the convergence of this sum using the asymptotic models:\n$$ q_{\\boldsymbol{n}} w(\\boldsymbol{n}) \\asymp \\left(c_q \\prod_{i=1}^d 2^{-r_i n_i}\\right) \\left(c_w \\prod_{i=1}^d 2^{\\gamma_i n_i}\\right) = c_q c_w \\prod_{i=1}^d 2^{-(r_i - \\gamma_i)n_i} $$\nFor this sum to converge, the exponent must be positive for each component:\n$$ r_i - \\gamma_i  0 \\quad \\text{or} \\quad r_i  \\gamma_i \\quad \\text{for all } i = 1, \\ldots, d $$\n\nSummary of Conditions:\n- Unbiasedness (finite expectation): $\\alpha_i  0$ for $i=1, \\ldots, d$.\n- Finite Variance: $\\beta_i  r_i$ for $i=1, \\ldots, d$ AND $\\alpha_i  0$ for $i=1, \\ldots, d$.\n- Finite Expected Cost: $r_i  \\gamma_i$ for $i=1, \\ldots, d$.\nThe algorithm will check these component-wise strict inequalities.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies conditions for unbiasedness, finite variance, and finite\n    expected cost for a multi-index Monte Carlo estimator.\n    \"\"\"\n\n    test_cases = [\n        # (d, alpha, beta, gamma, r)\n        (2, (1.0, 0.8), (2.0, 1.5), (0.3, 0.4), (1.2, 1.0)),\n        (3, (0.5, 1.0, 0.7), (1.0, 0.9, 1.1), (0.6, 0.4, 0.3), (0.8, 0.6, 0.7)),\n        (2, (0.7, 0.9), (1.2, 1.0), (0.1, 0.2), (1.2, 0.5)),\n        (2, (1.0, 1.0), (0.4, 0.7), (0.5, 0.6), (0.55, 0.65)),\n        (3, (0.0, -0.1, 0.2), (1.5, 1.5, 1.5), (0.5, 0.5, 0.5), (1.0, 1.0, 1.0)),\n    ]\n\n    results = []\n    for case in test_cases:\n        d, alpha, beta, gamma, r = case\n        \n        # 1. Unbiasedness (Finite Expectation) Condition:\n        # The sum of expectations sum(E[Delta_n]) must converge.\n        # This occurs if the decay rate for each component is positive.\n        # Condition: alpha_i  0 for all i.\n        unbiased_check = all(a  0 for a in alpha)\n\n        # 2. Finite Variance Condition:\n        # The sum of variances sum(Var(Y_n)) must converge.\n        # Var(Y_n) = (1/q_n)E[Delta_n^2] - (E[Delta_n])^2\n        # Convergence requires convergence of both sum( (1/q_n)E[Delta_n^2] )\n        # and sum( (E[Delta_n])^2 ).\n        # Condition 1: sum( (1/q_n)E[Delta_n^2] ) converges if beta_i  r_i for all i.\n        # Condition 2: sum( (E[Delta_n])^2 ) converges if 2*alpha_i  0, i.e., alpha_i  0 for all i.\n        # This second condition is the same as for unbiasedness.\n        finite_var_check_1 = all(b  rr for b, rr in zip(beta, r))\n        finite_var_check = finite_var_check_1 and unbiased_check\n        \n        # 3. Finite Expected Cost Condition:\n        # The sum of expected costs sum(q_n * w_n) must converge.\n        # This occurs if the decay rate of the product is positive.\n        # Condition: r_i  gamma_i for all i.\n        finite_cost_check = all(rr  g for rr, g in zip(r, gamma))\n        \n        results.append([unbiased_check, finite_var_check, finite_cost_check])\n\n    # Format the final output string exactly as specified, with no spaces.\n    final_output = str(results).replace(\" \", \"\")\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}