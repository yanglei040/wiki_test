{
    "hands_on_practices": [
        {
            "introduction": "Normalizing flows construct complex densities by applying a sequence of invertible transformations to a simple base distribution. This first exercise provides a concrete look at the mechanics of a Real NVP (Non-Volume Preserving) coupling layer, a foundational building block for many flow-based models. By performing the forward pass, the inverse pass, and the Jacobian determinant calculation, you will see firsthand how these transformations are designed for both expressivity and computational efficiency . Mastering these fundamental operations is essential for understanding how normalizing flows can model intricate data distributions while maintaining a tractable likelihood.",
            "id": "3318886",
            "problem": "Consider a two-dimensional Real Non-Volume Preserving (RNVP) coupling transformation from normalizing flows, defined on $\\mathbb{R}^{2}$ as follows. Let the input be $z = (z_{1}, z_{2})$ and let the output be $x = (x_{1}, x_{2})$ under the coupling rule\n$$\nx_{1} = z_{1}, \\quad x_{2} = z_{2} \\cdot \\exp\\big(s(z_{1})\\big) + t(z_{1}),\n$$\nwhere the scale function $s(\\cdot)$ and the shift function $t(\\cdot)$ are specified by\n$$\ns(u) = \\ln\\big(1 + u^{2}\\big), \\qquad t(u) = \\sin(u) + u^{3}.\n$$\nAssume the change-of-variables principle for probability densities and the standard Jacobian determinant rule under differentiable bijections. For the specific input\n$$\nz = \\left(1, \\frac{1}{2}\\right),\n$$\nperform the following tasks:\n- Compute the forward transformation $x = f(z)$.\n- Compute the inverse transformation $z = f^{-1}(x)$ evaluated at the forward image $x$ obtained above.\n- Compute the log absolute determinant of the Jacobian matrix of the forward transformation evaluated at $z$, which quantifies the local log-density change under the mapping.\n\nExpress your final results as a single row matrix containing, in order, the five quantities $(x_{1}, x_{2}, z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}}, \\Delta)$, where $\\Delta$ denotes the log absolute determinant of the Jacobian of the forward transformation at $z$. No rounding is required, and your final answer must be exact.",
            "solution": "The user has provided a problem statement regarding a Real Non-Volume Preserving (RNVP) coupling transformation from the field of normalizing flows. I will first validate the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Transformation**: A two-dimensional Real Non-Volume Preserving (RNVP) coupling transformation $f: \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$, mapping $z = (z_{1}, z_{2})$ to $x = (x_{1}, x_{2})$.\n-   **Transformation Rule**:\n    $$\n    x_{1} = z_{1}\n    $$\n    $$\n    x_{2} = z_{2} \\cdot \\exp\\big(s(z_{1})\\big) + t(z_{1})\n    $$\n-   **Scale Function**: $s(u) = \\ln\\big(1 + u^{2}\\big)$.\n-   **Shift Function**: $t(u) = \\sin(u) + u^{3}$.\n-   **Input Point**: $z = \\left(1, \\frac{1}{2}\\right)$.\n-   **Required Computations**:\n    1.  The forward transformation $x = f(z)$ at the given input point.\n    2.  The inverse transformation $z^{\\mathrm{inv}} = f^{-1}(x)$ evaluated at the computed image $x$.\n    3.  The log absolute determinant of the Jacobian of the forward transformation, $\\Delta = \\ln|\\det(J_{f}(z))|$, evaluated at the given input point.\n-   **Output Format**: A single row matrix containing the five quantities $(x_{1}, x_{2}, z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}}, \\Delta)$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding**: The problem is based on established mathematical principles of normalizing flows, specifically the RNVP architecture. The transformation, functions, and Jacobian calculation are standard components in this area of machine learning and are mathematically sound.\n-   **Well-Posedness**: The transformation is a diffeomorphism. The scale function $s(u)$ and shift function $t(u)$ are infinitely differentiable for all $u \\in \\mathbb{R}$. The transformation is invertible since the term $\\exp(s(z_{1})) = \\exp(\\ln(1+z_1^2)) = 1+z_1^2$ is strictly positive, allowing for a unique solution for $z_{2}$. The problem is clearly defined and has a unique, stable solution.\n-   **Objectivity**: The problem statement is purely mathematical and objective, with no subjective or ambiguous language.\n\n**Step 3: Verdict and Action**\nThe problem is mathematically consistent, well-posed, and scientifically grounded. It is therefore **valid**. I will proceed with the solution.\n\n### Solution\n\nThe solution requires performing three distinct computations based on the provided RNVP transformation.\n\n**1. Forward Transformation $x = f(z)$**\n\nWe are given the input point $z = (z_{1}, z_{2}) = (1, \\frac{1}{2})$.\nThe transformation is defined as:\n$$\nx_{1} = z_{1}\n$$\n$$\nx_{2} = z_{2} \\cdot \\exp\\big(s(z_{1})\\big) + t(z_{1})\n$$\nFirst, we evaluate the scale function $s(z_{1})$ and the shift function $t(z_{1})$ at $z_{1} = 1$.\n$$\ns(1) = \\ln(1 + 1^{2}) = \\ln(2)\n$$\n$$\nt(1) = \\sin(1) + 1^{3} = \\sin(1) + 1\n$$\nNow, we can compute $x_{1}$ and $x_{2}$.\n$$\nx_{1} = z_{1} = 1\n$$\n$$\nx_{2} = z_{2} \\cdot \\exp(s(1)) + t(1) = \\frac{1}{2} \\cdot \\exp(\\ln(2)) + (\\sin(1) + 1)\n$$\nSince $\\exp(\\ln(2)) = 2$, this simplifies to:\n$$\nx_{2} = \\frac{1}{2} \\cdot 2 + \\sin(1) + 1 = 1 + \\sin(1) + 1 = 2 + \\sin(1)\n$$\nThus, the forward transformation yields the point $x = (x_{1}, x_{2}) = (1, 2 + \\sin(1))$.\n\n**2. Inverse Transformation $z^{\\mathrm{inv}} = f^{-1}(x)$**\n\nTo find the inverse transformation $f^{-1}$, we must solve the forward transformation equations for $z_{1}$ and $z_{2}$ in terms of $x_{1}$ and $x_{2}$.\nFrom $x_{1} = z_{1}$, we immediately have:\n$$\nz_{1} = x_{1}\n$$\nWe substitute this into the second equation:\n$$\nx_{2} = z_{2} \\cdot \\exp(s(x_{1})) + t(x_{1})\n$$\nNow, we solve for $z_{2}$:\n$$\nx_{2} - t(x_{1}) = z_{2} \\cdot \\exp(s(x_{1}))\n$$\n$$\nz_{2} = \\frac{x_{2} - t(x_{1})}{\\exp(s(x_{1}))} = \\big(x_{2} - t(x_{1})\\big) \\exp(-s(x_{1}))\n$$\nSo the inverse transformation $f^{-1}$ is given by:\n$$\nz_{1} = x_{1}\n$$\n$$\nz_{2} = \\big(x_{2} - t(x_{1})\\big) \\exp(-s(x_{1}))\n$$\nWe evaluate this inverse transformation at the point $x = (1, 2 + \\sin(1))$ computed in the previous step. Let the result be $z^{\\mathrm{inv}} = (z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}})$.\n$$\nz_{1}^{\\mathrm{inv}} = x_{1} = 1\n$$\nFor $z_{2}^{\\mathrm{inv}}$, we first evaluate $s(x_{1})$ and $t(x_{1})$ at $x_{1} = 1$:\n$$\ns(1) = \\ln(1+1^{2}) = \\ln(2)\n$$\n$$\nt(1) = \\sin(1) + 1^{3} = \\sin(1) + 1\n$$\nNow substitute these into the expression for $z_{2}$:\n$$\nz_{2}^{\\mathrm{inv}} = \\big((2 + \\sin(1)) - (\\sin(1) + 1)\\big) \\exp(-\\ln(2))\n$$\n$$\nz_{2}^{\\mathrm{inv}} = (2 + \\sin(1) - \\sin(1) - 1) \\cdot \\frac{1}{\\exp(\\ln(2))}\n$$\n$$\nz_{2}^{\\mathrm{inv}} = (1) \\cdot \\frac{1}{2} = \\frac{1}{2}\n$$\nAs expected, the inverse transformation applied to the image of $z$ returns the original point. So, $z^{\\mathrm{inv}} = (z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}}) = (1, \\frac{1}{2})$.\n\n**3. Log Absolute Determinant of the Jacobian, $\\Delta$**\n\nThe forward transformation is $x = f(z)$, where $x_{1} = z_{1}$ and $x_{2} = z_{2} \\exp(s(z_{1})) + t(z_{1})$. The Jacobian matrix $J_{f}(z)$ is given by:\n$$\nJ_{f}(z) = \\begin{pmatrix} \\frac{\\partial x_{1}}{\\partial z_{1}} & \\frac{\\partial x_{1}}{\\partial z_{2}} \\\\ \\frac{\\partial x_{2}}{\\partial z_{1}} & \\frac{\\partial x_{2}}{\\partial z_{2}} \\end{pmatrix}\n$$\nWe calculate the partial derivatives:\n$$\n\\frac{\\partial x_{1}}{\\partial z_{1}} = 1\n$$\n$$\n\\frac{\\partial x_{1}}{\\partial z_{2}} = 0\n$$\n$$\n\\frac{\\partial x_{2}}{\\partial z_{1}} = \\frac{\\partial}{\\partial z_{1}} \\left( z_{2} \\exp(s(z_{1})) + t(z_{1}) \\right) = z_{2} \\exp(s(z_{1})) s'(z_{1}) + t'(z_{1})\n$$\n$$\n\\frac{\\partial x_{2}}{\\partial z_{2}} = \\frac{\\partial}{\\partial z_{2}} \\left( z_{2} \\exp(s(z_{1})) + t(z_{1}) \\right) = \\exp(s(z_{1}))\n$$\nThe Jacobian matrix is:\n$$\nJ_{f}(z) = \\begin{pmatrix} 1 & 0 \\\\ z_{2} \\exp(s(z_{1})) s'(z_{1}) + t'(z_{1}) & \\exp(s(z_{1})) \\end{pmatrix}\n$$\nThis is a lower triangular matrix. The determinant of a triangular matrix is the product of its diagonal elements.\n$$\n\\det(J_{f}(z)) = 1 \\cdot \\exp(s(z_{1})) = \\exp(s(z_{1}))\n$$\nThe log absolute determinant $\\Delta$ is:\n$$\n\\Delta = \\ln |\\det(J_{f}(z))| = \\ln |\\exp(s(z_{1}))|\n$$\nSince $\\exp(\\cdot)$ is always positive, the absolute value is redundant.\n$$\n\\Delta = \\ln(\\exp(s(z_{1}))) = s(z_{1})\n$$\nThis is a characteristic property of RNVP coupling layers which makes the Jacobian determinant computationally efficient. We evaluate $\\Delta$ at the input point $z = (1, \\frac{1}{2})$, which only depends on $z_{1}=1$.\n$$\n\\Delta = s(1) = \\ln(1 + 1^{2}) = \\ln(2)\n$$\n\n**Final Assembly**\n\nWe now assemble the five computed quantities $(x_{1}, x_{2}, z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}}, \\Delta)$ into a single row matrix:\n-   $x_{1} = 1$\n-   $x_{2} = 2 + \\sin(1)$\n-   $z_{1}^{\\mathrm{inv}} = 1$\n-   $z_{2}^{\\mathrm{inv}} = \\frac{1}{2}$\n-   $\\Delta = \\ln(2)$\n\nThe resulting vector is $(1, 2 + \\sin(1), 1, \\frac{1}{2}, \\ln(2))$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 2 + \\sin(1) & 1 & \\frac{1}{2} & \\ln(2)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While Variational Autoencoders (VAEs) are powerful generative models, their training objective can sometimes lead to undesirable outcomes. This problem presents a theoretical exercise to investigate \"posterior collapse,\" a common failure mode where the decoder learns to ignore the information encoded in the latent variable $z$. By analytically deriving the optimal encoder parameters for a specialized VAE, you will quantitatively demonstrate how the Evidence Lower Bound (ELBO) can be maximized by sacrificing the mutual information between the data $x$ and the latent code $z$ . This analysis provides deep insight into the delicate balance within the VAE objective and is crucial for diagnosing and mitigating training issues.",
            "id": "3318904",
            "problem": "Consider a one-dimensional Variational Autoencoder (VAE) with a scalar latent variable $z \\in \\mathbb{R}$ and a scalar observation $x \\in \\mathbb{R}$. Let the data-generating distribution be $p_{\\mathrm{data}}(x)$, which is Gaussian with zero mean and variance $\\sigma_{d}^{2}$, that is, $p_{\\mathrm{data}}(x)$ is $\\mathcal{N}(0,\\sigma_{d}^{2})$. The prior over the latent variable is $p(z)$, which is standard normal $\\mathcal{N}(0,1)$. The decoder is specified to ignore $z$ and output $p_{\\theta}(x \\mid z) = \\mathcal{N}(0,\\sigma_{x}^{2})$, independent of $z$. The encoder is restricted to the Gaussian family $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\alpha x,\\sigma_{q}^{2})$ with parameters $\\phi=(\\alpha,\\sigma_{q}^{2})$.\n\nStarting from first principles, use the definition of the Evidence Lower Bound (ELBO) as a lower bound to the marginal log-likelihood and the definition of Kullback–Leibler divergence to derive the expected ELBO under $p_{\\mathrm{data}}(x)$ as a function of $\\alpha$ and $\\sigma_{q}^{2}$, and then determine the encoder parameters $(\\alpha^{\\star},\\sigma_{q}^{2\\star})$ that maximize this expected ELBO. Next, starting from the definition of mutual information, compute the mutual information $\\mathbb{I}_{q}(x;z)$ under the joint $q(x,z) = p_{\\mathrm{data}}(x)\\, q_{\\phi}(z \\mid x)$, evaluated at the optimizing parameters $(\\alpha^{\\star},\\sigma_{q}^{2\\star})$. Report the pair consisting of the maximized expected ELBO and the mutual information at the optimizer as a row matrix. Express your final answer in closed form in terms of $\\sigma_{x}^{2}$ and $\\sigma_{d}^{2}$. No rounding is required.",
            "solution": "The user wants to solve a problem about a one-dimensional Variational Autoencoder (VAE).\n\n### Step 1: Extract Givens\nThe givens are:\n- Data-generating distribution: $p_{\\mathrm{data}}(x) = \\mathcal{N}(x | 0, \\sigma_{d}^{2})$.\n- Latent prior distribution: $p(z) = \\mathcal{N}(z | 0, 1)$.\n- Decoder (likelihood) distribution: $p_{\\theta}(x \\mid z) = \\mathcal{N}(x \\mid 0, \\sigma_{x}^{2})$.\n- Encoder (variational posterior) distribution: $q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\alpha x, \\sigma_{q}^{2})$.\n- The parameters to be optimized are $\\phi = (\\alpha, \\sigma_{q}^{2})$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, using standard definitions and principles from machine learning, probability theory, and information theory. It is a well-posed theoretical exercise with a clear objective. All terms are precisely defined, and the premises are self-contained and consistent. The setup, while simplified (e.g., a decoder that ignores the latent variable), is mathematically valid and serves to test fundamental concepts. The problem is not trivial, requiring multi-step derivation.\n\n### Step 3: Verdict and Action\nThe problem is valid. We will now proceed with the solution.\n\n### Derivation of the Expected Evidence Lower Bound (ELBO)\n\nThe Evidence Lower Bound (ELBO) for a single data point $x$ is defined as:\n$$\n\\mathrm{ELBO}(x) = \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)}[\\ln p_{\\theta}(x \\mid z)] - \\mathrm{KL}(q_{\\phi}(z \\mid x) \\| p(z))\n$$\nWe will compute each of the two terms separately.\n\nThe first term is the expected log-likelihood, or the reconstruction term. The decoder is $p_{\\theta}(x \\mid z) = \\mathcal{N}(x \\mid 0, \\sigma_{x}^{2})$. The probability density function is:\n$$\np_{\\theta}(x \\mid z) = \\frac{1}{\\sqrt{2\\pi\\sigma_{x}^{2}}} \\exp\\left(-\\frac{x^2}{2\\sigma_{x}^{2}}\\right)\n$$\nThe natural logarithm of this is:\n$$\n\\ln p_{\\theta}(x \\mid z) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}}\n$$\nSince this expression does not depend on the latent variable $z$, the expectation with respect to $q_{\\phi}(z \\mid x)$ is simply the expression itself:\n$$\n\\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)}[\\ln p_{\\theta}(x \\mid z)] = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}}\n$$\n\nThe second term is the Kullback–Leibler (KL) divergence between the encoder distribution $q_{\\phi}(z \\mid x)$ and the prior $p(z)$. We are given $q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\alpha x, \\sigma_{q}^{2})$ and $p(z) = \\mathcal{N}(z \\mid 0, 1)$. The KL divergence between two univariate normal distributions $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $\\mathcal{N}(\\mu_2, \\sigma_2^2)$ is:\n$$\n\\mathrm{KL}(\\mathcal{N}(\\mu_1, \\sigma_1^2) \\| \\mathcal{N}(\\mu_2, \\sigma_2^2)) = \\ln\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n$$\nIn our case, $\\mu_1 = \\alpha x$, $\\sigma_1^2 = \\sigma_q^2$, $\\mu_2 = 0$, and $\\sigma_2^2 = 1$. Substituting these into the formula:\n$$\n\\mathrm{KL}(q_{\\phi}(z \\mid x) \\| p(z)) = \\ln\\frac{1}{\\sigma_q} + \\frac{\\sigma_q^2 + (\\alpha x - 0)^2}{2 \\cdot 1^2} - \\frac{1}{2}\n$$\n$$\n= -\\frac{1}{2}\\ln(\\sigma_q^2) + \\frac{\\sigma_q^2 + \\alpha^2 x^2}{2} - \\frac{1}{2}\n$$\n\nCombining the two terms, the ELBO for a single point $x$ is:\n$$\n\\mathrm{ELBO}(x) = \\left(-\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}}\\right) - \\left(-\\frac{1}{2}\\ln(\\sigma_q^2) + \\frac{\\sigma_q^2 + \\alpha^2 x^2}{2} - \\frac{1}{2}\\right)\n$$\n$$\n\\mathrm{ELBO}(x) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}} + \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} - \\frac{\\alpha^2 x^2}{2} + \\frac{1}{2}\n$$\n\nThe problem asks for the expected ELBO under the data distribution $p_{\\mathrm{data}}(x) = \\mathcal{N}(x \\mid 0, \\sigma_d^2)$. Let's denote this as $\\mathcal{L}(\\alpha, \\sigma_q^2) = \\mathbb{E}_{x \\sim p_{\\mathrm{data}}(x)}[\\mathrm{ELBO}(x)]$. We take the expectation of the terms that depend on $x$:\n$$\n\\mathcal{L}(\\alpha, \\sigma_q^2) = \\mathbb{E}_{x \\sim \\mathcal{N}(0,\\sigma_d^2)}\\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}} + \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} - \\frac{\\alpha^2 x^2}{2} + \\frac{1}{2} \\right]\n$$\nUsing the linearity of expectation, and noting that for $x \\sim \\mathcal{N}(0, \\sigma_d^2)$, we have $\\mathbb{E}[x^2] = \\mathrm{Var}(x) + (\\mathbb{E}[x])^2 = \\sigma_d^2 + 0^2 = \\sigma_d^2$:\n$$\n\\mathcal{L}(\\alpha, \\sigma_q^2) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\mathbb{E}[x^2]}{2\\sigma_{x}^{2}} + \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} - \\frac{\\alpha^2 \\mathbb{E}[x^2]}{2} + \\frac{1}{2}\n$$\n$$\n\\mathcal{L}(\\alpha, \\sigma_q^2) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_d^2}{2\\sigma_{x}^{2}} - \\frac{\\alpha^2 \\sigma_d^2}{2} + \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} + \\frac{1}{2}\n$$\n\n### Maximization of the Expected ELBO\n\nTo find the optimal parameters $(\\alpha^{\\star}, \\sigma_q^{2\\star})$, we maximize $\\mathcal{L}(\\alpha, \\sigma_q^2)$ by taking partial derivatives with respect to $\\alpha$ and $\\sigma_q^2$ and setting them to zero. The terms not involving $\\alpha$ or $\\sigma_q^2$ are constant with respect to the optimization.\n\nThe function can be separated for $\\alpha$ and $\\sigma_q^2$:\n$$\n\\mathcal{L}(\\alpha, \\sigma_q^2) = \\left( -\\frac{\\alpha^2 \\sigma_d^2}{2} \\right) + \\left( \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} \\right) + \\text{Constant}\n$$\nFor $\\alpha$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = - \\alpha \\sigma_d^2\n$$\nSetting this to zero, $- \\alpha \\sigma_d^2 = 0$. Since $\\sigma_d^2 > 0$, this implies $\\alpha^{\\star} = 0$. The second derivative $\\frac{\\partial^2 \\mathcal{L}}{\\partial \\alpha^2} = -\\sigma_d^2 < 0$, confirming this is a maximum.\n\nFor $\\sigma_q^2$: Let $v = \\sigma_q^2$. We optimize the term $f(v) = \\frac{1}{2}\\ln(v) - \\frac{v}{2}$.\n$$\n\\frac{\\partial f}{\\partial v} = \\frac{1}{2v} - \\frac{1}{2}\n$$\nSetting this to zero, $\\frac{1}{2v} - \\frac{1}{2} = 0 \\implies v = 1$. So, $\\sigma_q^{2\\star} = 1$. The second derivative $\\frac{\\partial^2 f}{\\partial v^2} = -\\frac{1}{2v^2} < 0$ for $v>0$, confirming a maximum.\n\nThus, the optimal encoder parameters are $(\\alpha^{\\star}, \\sigma_q^{2\\star}) = (0, 1)$.\n\n### Maximized Expected ELBO Value\n\nWe substitute these optimal parameters back into the expression for $\\mathcal{L}(\\alpha, \\sigma_q^2)$:\n$$\n\\mathcal{L}^{\\star} = \\mathcal{L}(0, 1) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_d^2}{2\\sigma_{x}^{2}} - \\frac{0^2 \\sigma_d^2}{2} + \\frac{1}{2}\\ln(1) - \\frac{1}{2} + \\frac{1}{2}\n$$\n$$\n\\mathcal{L}^{\\star} = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_d^2}{2\\sigma_{x}^{2}} - 0 + 0 - \\frac{1}{2} + \\frac{1}{2}\n$$\n$$\n\\mathcal{L}^{\\star} = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_d^2}{2\\sigma_{x}^{2}}\n$$\n\n### Mutual Information\n\nThe mutual information between $x$ and $z$ under the joint distribution $q(x,z) = p_{\\mathrm{data}}(x)q_{\\phi}(z \\mid x)$ is given by:\n$$\n\\mathbb{I}_q(x;z) = \\mathbb{E}_{x \\sim p_{\\mathrm{data}}(x)}[\\mathrm{KL}(q_\\phi(z \\mid x) \\| p(z))]\n$$\nThis is precisely the expectation of the KL-divergence term that we calculated for the ELBO. We need to evaluate this at the optimizing parameters $(\\alpha^{\\star}, \\sigma_q^{2\\star}) = (0, 1)$.\n\nAt these parameters, the encoder becomes:\n$$\nq_{\\phi^{\\star}}(z \\mid x) = \\mathcal{N}(z \\mid 0 \\cdot x, 1) = \\mathcal{N}(z \\mid 0, 1)\n$$\nThis is identical to the prior distribution $p(z)$. Therefore, for any value of $x$, the KL divergence is:\n$$\n\\mathrm{KL}(q_{\\phi^{\\star}}(z \\mid x) \\| p(z)) = \\mathrm{KL}(\\mathcal{N}(z \\mid 0, 1) \\| \\mathcal{N}(z \\mid 0, 1)) = 0\n$$\nSince the KL divergence is $0$ for all $x$, its expectation over $p_{\\mathrm{data}}(x)$ is also $0$:\n$$\n\\mathbb{I}_q(x;z)|_{\\phi=\\phi^{\\star}} = \\mathbb{E}_{x \\sim p_{\\mathrm{data}}(x)}[0] = 0\n$$\nThis result is expected. The optimal encoder $q_{\\phi^{\\star}}(z \\mid x)$ ignores the input $x$ entirely and outputs a distribution identical to the prior. This means that under the model $q(x,z)$, the latent variable $z$ is independent of the observation $x$, and thus their mutual information is zero. This phenomenon is known as posterior collapse.\n\n### Final Result\n\nThe pair consists of the maximized expected ELBO and the mutual information at the optimizer.\n- Maximized expected ELBO: $\\mathcal{L}^{\\star} = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_{d}^{2}}{2\\sigma_{x}^{2}}$\n- Mutual information: $\\mathbb{I}_q(x;z)|_{\\phi=\\phi^{\\star}} = 0$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_{d}^{2}}{2\\sigma_{x}^{2}} & 0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Developing a sophisticated generative model is not complete without a rigorous evaluation of its performance. This final practice moves from model-building to model assessment, equipping you with a standard toolkit for quantifying a model's effectiveness. Using a set of hypothetical model outputs, you will calculate three critical metrics: bits-per-dimension (bpd) to measure density estimation quality, the Effective Sample Size (ESS) to assess importance sampling efficiency, and a chi-square statistic based on the Probability Integral Transform (PIT) to check for model calibration . These hands-on calculations will solidify your understanding of how to benchmark and compare different learned sampling and density models in a principled way.",
            "id": "3318928",
            "problem": "You are evaluating a learned proposal distribution produced by a Normalizing Flow (NF) model for a one-dimensional synthetic target and a flow-based density model used to score held-out data. You are provided with the following for a held-out set of size $N = 6$ with data dimensionality $D = 1$:\n\n1. Natural-log-likelihoods under the learned density model, $\\{\\ln p_{\\theta}(x_i)\\}_{i=1}^{N}$:\n$\\{-1.20,\\,-0.80,\\,-1.50,\\,-1.10,\\,-0.95,\\,-1.30\\}$.\n\n2. Unnormalized log-importance weights from self-normalized importance sampling with proposal $q$ and unnormalized target $\\tilde{p}$, given as $a_i = \\ln \\tilde{p}(x_i) - \\ln q(x_i)$ for $i=1,\\dots,N$:\n$\\{-0.2,\\,0.0,\\,0.1,\\,-0.5,\\,0.3,\\,-0.1\\}$.\n\n3. Probability Integral Transform (PIT) values under the learned density model for the same $N$ test points, defined as $u_i = F_{\\theta}(x_i)$ where $F_{\\theta}$ is the cumulative distribution function of $p_{\\theta}$:\n$\\{0.12,\\,0.47,\\,0.68,\\,0.83,\\,0.24,\\,0.59\\}$.\n\nStarting only from core definitions appropriate to stochastic simulation and machine learning for sampling:\n\n- Bits-per-dimension (bpd) is the average information content per dimension measured in bits across the dataset, where information content for a data point $x$ under a model $p_{\\theta}$ is the negative base-$2$ logarithm of its likelihood.\n- Effective Sample Size (ESS) for self-normalized importance sampling is defined in terms of the normalized importance weights $\\{w_i\\}$, $w_i = \\tilde{w}_i / \\sum_{j=1}^{N} \\tilde{w}_j$ with $\\tilde{w}_i \\propto \\tilde{p}(x_i)/q(x_i)$.\n- A calibration discrepancy based on the Probability Integral Transform (PIT) can be constructed by comparing the empirical distribution of $\\{u_i\\}$ to the uniform distribution on $[0,1]$. Use a Pearson chi-square discrepancy with $B=4$ equal-width bins on $[0,1]$, comparing observed counts to the uniform expectation.\n\nCompute the following three quantities:\n\n1. The bits-per-dimension across the $N$ held-out points.\n2. The Effective Sample Size computed from the given unnormalized log-importance weights.\n3. The PIT-based Pearson chi-square discrepancy using $B=4$ equal-width histogram bins.\n\nReport your answer as a row vector $\\big[\\text{bpd},\\,\\text{ESS},\\,\\chi^2\\big]$ with each entry rounded to four significant figures. No units are required.",
            "solution": "The problem requires the computation of three distinct metrics to evaluate a learned model: the bits-per-dimension (bpd), the Effective Sample Size (ESS), and a Pearson chi-square statistic based on the Probability Integral Transform (PIT). The problem is well-posed, scientifically grounded, and provides all necessary data and definitions for a unique solution.\n\nThe provided data are:\n- Sample size: $N=6$.\n- Data dimensionality: $D=1$.\n- Natural-log-likelihoods: $\\{\\ln p_{\\theta}(x_i)\\}_{i=1}^{6} = \\{-1.20,\\,-0.80,\\,-1.50,\\,-1.10,\\,-0.95,\\,-1.30\\}$.\n- Unnormalized log-importance weights: $\\{a_i\\}_{i=1}^{6} = \\{-0.2,\\,0.0,\\,0.1,\\,-0.5,\\,0.3,\\,-0.1\\}$.\n- Probability Integral Transform (PIT) values: $\\{u_i\\}_{i=1}^{6} = \\{0.12,\\,0.47,\\,0.68,\\,0.83,\\,0.24,\\,0.59\\}$.\n\nWe will compute each of the three quantities in turn.\n\n1.  **Bits-per-dimension (bpd)**\n\nThe bits-per-dimension (bpd) is defined as the average negative log-likelihood in base $2$, normalized by the number of dimensions. The formula is:\n$$ \n\\text{bpd} = \\frac{1}{D} \\left\\langle -\\log_2 p_{\\theta}(x) \\right\\rangle = -\\frac{1}{DN} \\sum_{i=1}^{N} \\log_2 p_{\\theta}(x_i) \n$$\nWe are given the natural-log-likelihoods, $\\ln p_{\\theta}(x_i)$. We use the change of base formula for logarithms, $\\log_2(y) = \\frac{\\ln(y)}{\\ln(2)}$, to express the bpd in terms of the given values:\n$$ \n\\text{bpd} = -\\frac{1}{DN \\ln(2)} \\sum_{i=1}^{N} \\ln p_{\\theta}(x_i) \n$$\nFirst, we compute the sum of the given log-likelihoods:\n$$ \n\\sum_{i=1}^{6} \\ln p_{\\theta}(x_i) = (-1.20) + (-0.80) + (-1.50) + (-1.10) + (-0.95) + (-1.30) = -6.85 \n$$\nNow, we substitute the values for $D=1$, $N=6$, and the sum into the bpd formula:\n$$ \n\\text{bpd} = -\\frac{1}{(1)(6) \\ln(2)} (-6.85) = \\frac{6.85}{6 \\ln(2)} \n$$\nUsing the approximation $\\ln(2) \\approx 0.693147$:\n$$ \n\\text{bpd} \\approx \\frac{6.85}{6 \\times 0.693147} \\approx \\frac{6.85}{4.158882} \\approx 1.64708 \n$$\nRounding to four significant figures, we get $\\text{bpd} \\approx 1.647$.\n\n2.  **Effective Sample Size (ESS)**\n\nThe Effective Sample Size (ESS) for self-normalized importance sampling is given by:\n$$ \n\\text{ESS} = \\frac{1}{\\sum_{i=1}^{N} w_i^2} \n$$\nwhere $w_i$ are the normalized importance weights. The normalized weights are calculated from the unnormalized weights $\\tilde{w}_i$ as $w_i = \\frac{\\tilde{w}_i}{\\sum_{j=1}^{N} \\tilde{w}_j}$.\nThe problem provides the unnormalized log-importance weights, $a_i = \\ln \\tilde{w}_i$ (up to a constant). Thus, we can take $\\tilde{w}_i = \\exp(a_i)$. The set of log-weights is $\\{a_i\\} = \\{-0.2,\\,0.0,\\,0.1,\\,-0.5,\\,0.3,\\,-0.1\\}$.\nThe unnormalized weights are:\n$$ \n\\{\\tilde{w}_i\\}_{i=1}^6 = \\{\\exp(-0.2),\\, \\exp(0.0),\\, \\exp(0.1),\\, \\exp(-0.5),\\, \\exp(0.3),\\, \\exp(-0.1)\\} \n$$\nA numerically stable way to compute ESS is using the formula $\\text{ESS} = \\frac{(\\sum \\tilde{w}_i)^2}{\\sum \\tilde{w}_i^2}$.\nFirst, we compute the sum of the unnormalized weights:\n$$ \n\\sum_{i=1}^6 \\tilde{w}_i = \\exp(-0.2) + 1 + \\exp(0.1) + \\exp(-0.5) + \\exp(0.3) + \\exp(-0.1) \n$$\n$$ \n\\sum_{i=1}^6 \\tilde{w}_i \\approx 0.81873 + 1.0 + 1.10517 + 0.60653 + 1.34986 + 0.90484 = 5.78513 \n$$\nNext, we compute the sum of the squares of the unnormalized weights, which is $\\sum \\exp(2a_i)$:\n$$ \n\\sum_{i=1}^6 \\tilde{w}_i^2 = \\exp(-0.4) + \\exp(0.0) + \\exp(0.2) + \\exp(-1.0) + \\exp(0.6) + \\exp(-0.2) \n$$\n$$ \n\\sum_{i=1}^6 \\tilde{w}_i^2 \\approx 0.67032 + 1.0 + 1.22140 + 0.36788 + 1.82212 + 0.81873 = 5.90045 \n$$\nNow we can compute the ESS:\n$$ \n\\text{ESS} = \\frac{(\\sum \\tilde{w}_i)^2}{\\sum \\tilde{w}_i^2} \\approx \\frac{(5.78513)^2}{5.90045} \\approx \\frac{33.4677}{5.90045} \\approx 5.67202 \n$$\nRounding to four significant figures, we get $\\text{ESS} \\approx 5.672$.\n\n3.  **PIT-based Pearson Chi-Square Discrepancy ($\\chi^2$)**\n\nThe PIT values $\\{u_i\\}$ should follow a uniform distribution on the interval $[0,1]$ if the model is perfectly calibrated. We test this hypothesis using a Pearson chi-square goodness-of-fit test. We use $B=4$ bins of equal width on $[0,1]$.\nThe bins are: $[0, 0.25)$, $[0.25, 0.50)$, $[0.50, 0.75)$, and $[0.75, 1.0]$.\nThe total number of samples is $N=6$. The expected count $E_j$ in each bin $j$ is $E_j = N/B = 6/4 = 1.5$.\nThe given PIT values are $\\{u_i\\} = \\{0.12,\\,0.47,\\,0.68,\\,0.83,\\,0.24,\\,0.59\\}$.\nWe find the observed counts $O_j$ by sorting the values into the bins:\n- Bin 1: $[0, 0.25) \\rightarrow$ values $0.12, 0.24$. So, $O_1 = 2$.\n- Bin 2: $[0.25, 0.50) \\rightarrow$ value $0.47$. So, $O_2 = 1$.\n- Bin 3: $[0.50, 0.75) \\rightarrow$ values $0.68, 0.59$. So, $O_3 = 2$.\n- Bin 4: $[0.75, 1.0] \\rightarrow$ value $0.83$. So, $O_4 = 1$.\nThe observed counts are $\\{O_j\\} = \\{2, 1, 2, 1\\}$. The sum is $2+1+2+1=6=N$.\nThe Pearson chi-square statistic is calculated as:\n$$ \n\\chi^2 = \\sum_{j=1}^{B} \\frac{(O_j - E_j)^2}{E_j} \n$$\nSubstituting the values:\n$$ \n\\chi^2 = \\frac{(2 - 1.5)^2}{1.5} + \\frac{(1 - 1.5)^2}{1.5} + \\frac{(2 - 1.5)^2}{1.5} + \\frac{(1 - 1.5)^2}{1.5} \n$$\n$$ \n\\chi^2 = \\frac{(0.5)^2}{1.5} + \\frac{(-0.5)^2}{1.5} + \\frac{(0.5)^2}{1.5} + \\frac{(-0.5)^2}{1.5} \n$$\n$$ \n\\chi^2 = \\frac{0.25}{1.5} + \\frac{0.25}{1.5} + \\frac{0.25}{1.5} + \\frac{0.25}{1.5} = 4 \\times \\frac{0.25}{1.5} = \\frac{1}{1.5} = \\frac{2}{3} \n$$\n$$ \n\\chi^2 = \\frac{2}{3} \\approx 0.66666\\dots \n$$\nRounding to four significant figures, we get $\\chi^2 \\approx 0.6667$.\n\nThe three computed quantities, rounded to four significant figures, are:\n- $\\text{bpd} = 1.647$\n- $\\text{ESS} = 5.672$\n- $\\chi^2 = 0.6667$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.647 & 5.672 & 0.6667\n\\end{pmatrix}\n}\n$$"
        }
    ]
}