## 引言
[贝叶斯神经网络](@entry_id:746725)（BNN）通过将参数视为[概率分布](@entry_id:146404)而非固定值，为现代深度学习引入了原则性的[不确定性量化](@entry_id:138597)框架，这在安全攸关的应用中至关重要。然而，BNN的核心挑战在于其参数的[后验分布](@entry_id:145605) $p(\theta|D)$ 几乎总是高维、非凸且难以解析处理的，这使得精确推断成为一个巨大的难题。因此，我们必须依赖各种[近似推断](@entry_id:746496)方法来从这个复杂的后验分布中提取有价值的信息。

本文旨在深入剖析解决这一难题的核心技术：[后验采样](@entry_id:753636)。我们将系统性地引导您穿越贝叶斯推断的理论与实践景观。在“原理与机制”一章中，您将学习到两类主要的[后验近似](@entry_id:753628)方法——基于[马尔可夫链蒙特卡洛](@entry_id:138779)（MCMC）的采样技术和[变分推断](@entry_id:634275)（VI）——的内部工作原理。接着，在“应用与跨学科连接”一章中，我们将展示如何利用这些方法进行不确定性量化、[模型诊断](@entry_id:136895)与选择，并探讨其与[统计学习理论](@entry_id:274291)等领域的深刻联系。最后，“动手实践”部分将提供具体的编程练习，帮助您将理论知识转化为解决实际问题的能力。通过本次学习，您将能够掌握在BNN中进行有效后验推断的基石。

## 原理与机制

在[贝叶斯神经网络](@entry_id:746725)（BNN）的框架中，核心任务是从后验分布 $p(\theta | D) \propto p(D | \theta) p(\theta)$ 中进行推断。其中，$\theta$ 代表网络的所有参数（权重和偏置），$D$ 是观测数据。然而，由于神经[网络模型](@entry_id:136956)的高度[非线性](@entry_id:637147)以及参数维度通常极大，这个后验分布几乎总是难以解析处理的。因此，我们必须依赖[近似推断](@entry_id:746496)方法。本章将深入探讨两类主流的[后验采样](@entry_id:753636)方法——马尔可夫链蒙特卡洛（MCMC）和[变分推断](@entry_id:634275)（VI）——的核心原理与机制。我们还将讨论先验设计如何塑造后验分布的几何形态，以及一些深刻的理论视角，这些共同构成了在BNN中进行有效后验推断的基石。

### 基于马尔可夫链蒙特卡洛的[后验采样](@entry_id:753636)

[MCMC方法](@entry_id:137183)通过构建一个马尔可夫链，使其[平稳分布](@entry_id:194199)恰好是我们想要采样的目标[后验分布](@entry_id:145605) $p(\theta | D)$，从而生成一系列来自该后验的样本。对于高维连续空间，特别是像BNN这样的复杂模型，基于梯度的[MCMC方法](@entry_id:137183)至关重要。

#### 随机梯度[Langevin动力学](@entry_id:142305) (SGLD)

[Langevin动力学](@entry_id:142305)是一种物理系统模型，描述了粒子在[势场](@entry_id:143025)中受随机力驱动的运动。通过将其应用于统计采样，我们可以构建一个探索后验概率[分布](@entry_id:182848)的算法。考虑一个由负对数后验定义的势能函数 $U(\theta) = -\log p(\theta | D)$。一个参数 $\theta$ 在此势能景观中的[过阻尼](@entry_id:167953)Langevin扩散过程由以下随机微分方程（SDE）描述：
$$
d\theta_t = \frac{1}{2} \nabla_{\theta} \log p(\theta_t | D) dt + dW_t
$$
其中 $W_t$ 是一个标准的维纳过程（布朗运动）。该SDE的[平稳分布](@entry_id:194199)恰好是目标后验分布 $p(\theta | D)$。

为了在计算机上实现这一过程，我们可以使用欧拉-丸山（Euler-Maruyama）法对该SDE进行离散化，步长为 $\eta_t$：
$$
\theta_{t+1} = \theta_t + \frac{\eta_t}{2} \nabla_{\theta} \log p(\theta_t | D) + \sqrt{\eta_t} \xi_t
$$
其中 $\xi_t \sim \mathcal{N}(0, I)$ 是一个标准[高斯噪声](@entry_id:260752)向量。这便是[Langevin动力学](@entry_id:142305)（LD）采样算法。然而，在处理大型数据集时，计算整个数据集上的对数后验梯度 $\nabla_{\theta} \log p(\theta | D)$ 的成本是极其高昂的，因为它需要对所有数据点求和。

为了解决这个问题，**随机梯度[Langevin动力学](@entry_id:142305)（SGLD）**被提出。其核心思想是用一个基于小批量（mini-batch）数据的随机梯度来替代全批量梯度。对数后验梯度可以分解为先验梯度和似然梯度之和：
$$
\nabla_{\theta} \log p(\theta | D) = \nabla_{\theta} \log p(\theta) + \sum_{i=1}^{N} \nabla_{\theta} \log p(y_i | x_i, \theta)
$$
我们可以通过从数据集中随机抽取一个大小为 $m$ 的小批量 $B_t$，并适当地缩放似然梯度项，来构造一个无偏估计：
$$
\widehat{\nabla_{\theta} \log p(\theta | D)} = \nabla_{\theta} \log p(\theta) + \frac{N}{m} \sum_{i \in B_t} \nabla_{\theta} \log p(y_i | x_i, \theta)
$$
将这个随机[梯度估计](@entry_id:164549)代入LD更新规则，我们便得到了SGLD的[更新方程](@entry_id:264802)。

例如，考虑一个具有[高斯先验](@entry_id:749752) $p(\theta) = \mathcal{N}(0, \sigma_p^2 I)$ 和高斯似然 $p(y_i | x_i, \theta) = \mathcal{N}(f_{\theta}(x_i), \sigma_y^2)$ 的BNN，其中 $f_{\theta}(x)$ 是网络输出。SGLD的更新规则具体形式为 ：
$$
\theta_{t+1} = \theta_t + \frac{\eta_t}{2} \left[ -\frac{1}{\sigma_p^2} \theta_t + \frac{N}{m\sigma_y^2} \sum_{i \in B_t} (y_i - f_{\theta_t}(x_i)) \nabla_{\theta} f_{\theta_t}(x_i) \right] + \sqrt{\eta_t} \xi_t
$$
SGLD的巧妙之处在于它引入了两种随机性：来自小批量梯度的噪声和我们主动注入的高斯噪声。为了使SGLD生成的样本链能够渐近地收敛到真正的后验分布，步长序列 $\{\eta_t\}$ 必须满足特定的条件（类似于[随机近似](@entry_id:270652)中的[Robbins-Monro条件](@entry_id:634006)）：
1.  $\sum_{t=1}^{\infty} \eta_t = \infty$：步长衰减得不能太快，以保证采样器有足够的时间探索整个[参数空间](@entry_id:178581)。
2.  $\sum_{t=1}^{\infty} \eta_t^2  \infty$：步长衰减得必须足够快，以使注入噪声的总[方差](@entry_id:200758)有限，并控制[离散化误差](@entry_id:748522)。

满足这些条件的[步长方案](@entry_id:636095)（如 $\eta_t \propto t^{-a}$，其中 $a \in (0.5, 1]$）理论上保证了SGLD的收敛性。

#### 通过预处理加速采样

SGLD的一个实际挑战是，BNN的[后验分布](@entry_id:145605)通常具有高度病态（ill-conditioned）的几何特征，即在某些方向上非常陡峭，而在另一些方向上非常平坦。这会导致标准SGLD[采样效率](@entry_id:754496)低下。**[预处理](@entry_id:141204)（Preconditioning）**是一种通过调整采样过程来适应后验几何形态、从而加速混合的技术。

一个常用的方法是引入一个[对称正定](@entry_id:145886)的**预处理矩阵** $G$，它近似了后验协[方差](@entry_id:200758)的逆。这相当于在参数空间中进行了一次线性变换。我们可以将预处理的SGLD理解为在一个被“拉直”的变换空间中进行标准的、各向同性的SGLD采样 。

考虑一个线性变换 $\phi = S^{-1}\theta$，其中 $G = SS^T$ 是[预处理](@entry_id:141204)矩阵的分解（例如，$S = G^{1/2}$）。在$\phi$空间中，一个标准[Langevin动力学](@entry_id:142305)过程的SDE为 $d\phi_t = \frac{1}{2} \nabla_{\phi} \log \tilde{\pi}(\phi_t) dt + dW_t$，其中 $\tilde{\pi}(\phi)$ 是变换后的密度。通过Itô引理和[链式法则](@entry_id:190743)，可以证明这等价于在原始$\theta$空间中遵循以下SDE：
$$
d\theta_t = \frac{1}{2} G \nabla_{\theta} \log \pi(\theta_t) dt + S dW_t
$$
这里的关键在于漂移项（drift term）被矩阵 $G$ 左乘，而[扩散](@entry_id:141445)项（diffusion term）则被 $S=G^{1/2}$ 左乘。为了让[平稳分布](@entry_id:194199)为 $\pi(\theta)$，漂移项和[扩散张量](@entry_id:748421) $D = \frac{1}{2}SS^T = \frac{1}{2}G$ 必须满足[福克-普朗克](@entry_id:635508)（[Fokker-Planck](@entry_id:635508)）方程的定常条件。

对这个[预处理](@entry_id:141204)后的SDE进行欧拉-丸山离散化，噪声项的正确形式就变得显而易见。离散的噪声增量应为 $S (\sqrt{\eta}\xi) = \sqrt{\eta} S \xi$，其中 $\xi \sim \mathcal{N}(0, I)$。因此，预处理SGLD的更新规则中的噪声项为：
$$
\text{噪声项} = \sqrt{\eta} G^{1/2} \xi
$$
这个噪声向量服从均值为0、协[方差](@entry_id:200758)为 $\eta G$ 的高斯分布。通过这种方式，采样器在几何上更平坦的方向上采取更大的步伐，在更陡峭的方向上采取更小的步伐，从而显著提高了[采样效率](@entry_id:754496)。

#### [哈密顿蒙特卡洛](@entry_id:144208)与NUTS算法

虽然SGLD很有效，但其[随机游走](@entry_id:142620)的特性限制了它在高维空间中的探索效率。**[哈密顿蒙特卡洛](@entry_id:144208)（HMC）**通过引入物理学中的[哈密顿动力学](@entry_id:156273)来提出更有效的移动策略。HMC将参数 $\theta$ 视为“位置”变量 $q$，并引入一个辅助的“动量”变量 $p$。系统的总能量，即[哈密顿量](@entry_id:172864)，定义为势能和动能之和：$H(q, p) = U(q) + K(p)$。其中[势能](@entry_id:748988) $U(q) = -\log p(q|D)$，动能通常取为 $K(p) = \frac{1}{2} p^T M^{-1} p$，其中 $M$ 是[质量矩阵](@entry_id:177093)。

HMC通过以下两步交替进行：
1.  **动量更新**：从其[条件分布](@entry_id:138367) $p(p) \propto \exp(-K(p))$ 中随机重采样动量 $p$（通常是[高斯分布](@entry_id:154414)）。
2.  **[哈密顿动力学](@entry_id:156273)模拟**：从当前状态 $(q, p)$ 开始，沿着[哈密顿方程](@entry_id:156213)定义的轨迹演化一段时间。由于数值积分（如[蛙跳法](@entry_id:751210)）的误差，终点状态 $(q', p')$ 的[哈密顿量](@entry_id:172864)会略有变化，因此需要通过一个Metropolis-Hastings接受步骤来修正，以确保采样的正确性。

HMC的挑战在于需要手动设置两个超参数：积分步长 $\epsilon$ 和轨迹长度 $L$。**No-U-Turn Sampler（NUTS）**是一种自适应的HMC变体，它优雅地解决了这两个问题 。

首先，NUTS通过构建一个平衡二叉树来自动确定轨迹长度。它从初始点开始，向“前”和向“后”两个方向同时扩展轨迹，每次将轨迹长度加倍。扩展过程会一直持续，直到轨迹开始“掉头”返回。这个**U型转向（U-turn）**的判断标准是NUTS的核心。设当前轨迹的左右端点分别为 $(q^-, p^-)$ 和 $(q^+, p^+)$。如果继续扩展会导致动量指向轨迹内部，即满足以下任一条件，则认为发生了U型转向，应停止扩展：
$$
\langle q^+ - q^-, p^- \rangle  0 \quad \text{或} \quad \langle q^+ - q^-, p^+ \rangle  0
$$
其中 $\langle \cdot, \cdot \rangle$ 是欧几里得[内积](@entry_id:158127)。这个标准确保了采样器不会浪费计算在已经开始折返的无效路径上。

其次，NUTS在[预热](@entry_id:159073)（warm-up）阶段使用**对偶平均（dual-averaging）**算法来自适应地调整步长 $\epsilon$。其目标是使Metropolis-Hastings的平均接受概率 $\alpha_m$ 达到一个预设值 $\delta$（通常为0.8左右）。对偶平均法通过一个更稳定的[随机优化](@entry_id:178938)方案来更新对数步长 $\log \epsilon_m$，它使用了一个追踪累积误差的统计量 $H_m$，并朝着一个目标点 $\mu$ 收缩，从而比简单的随机梯度方法更鲁棒。

### [变分推断](@entry_id:634275)

[变分推断](@entry_id:634275)（VI）将后验推断问题转化为一个[优化问题](@entry_id:266749)。其核心思想是，寻找一个来自某个简单参数化[分布](@entry_id:182848)族 $q_{\phi}(\theta)$（称为变分族）中的成员，使其与真实的、复杂的后验分布 $p(\theta|D)$“最接近”。

#### [变分推断](@entry_id:634275)的目标：[证据下界](@entry_id:634110)

“接近程度”通常用Kullback-Leibler（KL）散度来衡量。具体来说，VI旨在最小化**前向[KL散度](@entry_id:140001)** $\mathrm{KL}(q_{\phi}(\theta) \,||\, p(\theta|D))$。经过简单的数学变换可以发现，最小化这个[KL散度](@entry_id:140001)等价于最大化所谓的**[证据下界](@entry_id:634110)（Evidence Lower Bound, ELBO）**：
$$
\mathcal{L}(\phi) = \mathbb{E}_{q_{\phi}(\theta)}[\log p(D|\theta)] - \mathrm{KL}(q_{\phi}(\theta) \,||\, p(\theta))
$$
ELBO由两项组成：第一项是期望[对数似然](@entry_id:273783)，鼓励变分[分布](@entry_id:182848) $q_{\phi}(\theta)$ 能够很好地解释数据；第二项是 $q_{\phi}(\theta)$ 与[先验分布](@entry_id:141376) $p(\theta)$ 之间的KL散度，起到了正则化的作用，防止 $q_{\phi}(\theta)$ 过分偏离先验。

#### 模式寻求与质量覆盖：[KL散度](@entry_id:140001)的不对称性

[KL散度](@entry_id:140001)的一个关键特性是其不对称性，即 $\mathrm{KL}(q || p) \neq \mathrm{KL}(p || q)$。选择哪种[KL散度](@entry_id:140001)作为目标，对近似结果的性质有深远的影响 。

VI使用的前向KL散度 $\mathrm{KL}(q || p) = \int q(\theta) \log \frac{q(\theta)}{p(\theta)} d\theta$ 具有所谓的**模式寻求（mode-seeking）**行为。我们可以从其定义中直观地理解这一点：为了使[KL散度](@entry_id:140001)为有限值，只要 $p(\theta)$ 趋于零， $q(\theta)$ 就必须趋于零。这意味着 $q(\theta)$ 会极力避免在后验概率密度低的地方分配概率质量。如果真实的后验分布 $p(\theta|D)$ 是多峰的（例如，由于BNN中的权重对称性），而我们选择的变分族 $q_{\phi}(\theta)$ 是单峰的（如高斯分布），那么为了最小化[KL散度](@entry_id:140001)，$q_{\phi}(\theta)$ 倾向于选择并拟合其中一个峰，而忽略其他峰。这种行为会导致对后验不确定性的严重低估。

与此相反，**反向[KL散度](@entry_id:140001)** $\mathrm{KL}(p || q) = \int p(\theta) \log \frac{p(\theta)}{q(\theta)} d\theta$ 则表现出**质量覆盖（mass-covering）**的行为。从其定义来看，如果 $p(\theta)$ 有概率质量的地方而 $q(\theta)$ 趋于零，$\log(p/q)$ 项会趋于无穷大，导致[KL散度](@entry_id:140001)爆炸。因此，为了最小化反向KL散度，$q(\theta)$ 必须在所有 $p(\theta)$ 不为零的地方都分配概率质量。当用一个单峰的 $q(\theta)$ 来近似一个多峰的 $p(\theta)$ 时，这会迫使 $q(\theta)$ 变得非常宽（[方差](@entry_id:200758)过大），以覆盖所有的模式，同时也不可避免地在模式之间的低概率区域分配了不小的概率质量。

#### 超越平均场：使用[归一化流](@entry_id:272573)构建表达性强的变分族

传统VI常使用**平均场（mean-field）**假设，即 $q(\theta) = \prod_i q_i(\theta_i)$，它假定所有参数在后验中是独立的。这极大地简化了计算，但牺牲了捕捉参数间相关性的能力，导致近似效果不佳。为了克服这一限制，我们需要更具表达力的变分族。

**[归一化流](@entry_id:272573)（Normalizing Flows）**提供了一个强大而灵活的框架来构建复杂的[分布](@entry_id:182848) 。其核心思想是通过一系列可逆的、可微的变换 $f_{\phi}$，将一个简单的基础[分布](@entry_id:182848)（如标准[高斯分布](@entry_id:154414)）$p_z(z)$ 变换为一个复杂的目标分布 $q_{\phi}(\theta)$。如果 $\theta = f_{\phi}(z)$，根据变量代换定理，$\theta$ 的对数概率密度为：
$$
\log q_{\phi}(\theta) = \log p_z(f_{\phi}^{-1}(\theta)) + \log |\det J_{f_{\phi}^{-1}}(\theta)|
$$
其中 $J_{f_{\phi}^{-1}}(\theta)$ 是逆变换的雅可比矩阵。[归一化流](@entry_id:272573)的关键在于设计出既具有强大表达能力，又易于计算逆变换及其雅可比行列式的变换函数 $f_{\phi}$。

**仿射[耦合层](@entry_id:637015)（Affine Coupling Layers）**是实现这一目标的一种流行架构。它将输入向量 $x$ 分成两部分 $(x_A, x_B)$。一部分 $x_A$ 保持不变，而另一部分 $x_B$ 则经过一个[仿射变换](@entry_id:144885)，其尺度 $s$ 和位移 $t$ 是由 $x_A$ 计算得出的：
$$
y_A = x_A, \quad y_B = x_B \odot \exp(s(x_A)) + t(x_A)
$$
这种设计的巧妙之处在于其雅可比矩阵是块三角矩阵，其[行列式](@entry_id:142978)就是对角线上[尺度因子](@entry_id:266678)之和的指数，非常容易计算。同时，其逆变换也具有解析形式且易于计算。通过堆叠多个这样的[耦合层](@entry_id:637015)（并交替变换的部分），我们可以构建出非常灵活和富有[表现力](@entry_id:149863)的变分后验分布，它能够捕捉参数之间的复杂依赖关系，远胜于平均场近似。

### 先验设计与后验几何

[后验分布](@entry_id:145605) $p(\theta|D)$ 同时由似然和先验决定。先验 $p(\theta)$ 不仅是正则化器，更编码了我们关于模型参数的先验知识，并深刻地影响着[后验分布](@entry_id:145605)的几何形态。

#### 促进稀疏性的层次化先验

在BNN中，一个关键的设计选择是权重的[先验分布](@entry_id:141376)。一个简单的选择是独立同分布的[高斯先验](@entry_id:749752)，但这可能无法有效地进行[模型选择](@entry_id:155601)或稀疏化。**层次化先验（Hierarchical Priors）**提供了一种更强大的建模工具。

一个著名的例子是**马蹄铁先验（Horseshoe Prior）**，它旨在实现“收缩大噪声，保留真信号”的效果  。其基本形式是为每个权重 $w_j$ 设置一个[高斯先验](@entry_id:749752)，但其[方差](@entry_id:200758)由一个超参数 $\tau_j$ 控制，而这个超参数本身又被赋予一个先验。一个典型的结构是：
$$
w_j | \tau_j \sim \mathcal{N}(0, \tau_j^2), \quad \tau_j \sim \text{Half-Cauchy}(\beta)
$$
其中半[柯西分布](@entry_id:266469)具有重尾特性。通[过积分](@entry_id:753033)掉 $\tau_j$，我们可以得到 $w_j$ 的边缘[先验分布](@entry_id:141376) $p(w_j)$。这个边缘先验有两个显著特征：
1.  **在零点处有一个无限高的尖峰**：其密度在 $|w_j| \to 0$ 时以 $\log(1/|w_j|)$ 的形式发散。这个尖峰强烈地将那些接近于零的权重（可能是噪声）向零收缩，从而产生[稀疏性](@entry_id:136793)。
2.  **[重尾](@entry_id:274276)特性**：其密度在 $|w_j| \to \infty$ 时以 $1/|w_j|^2$ 的多项式形式衰减。与[高斯先验](@entry_id:749752)的指数衰减相比，这种重尾允许少数真正大的权重（对应重要特征）存在而不过度惩罚它们。

这种“尖峰-重尾”的结合使得马蹄铁先验成为稀疏[贝叶斯建模](@entry_id:178666)中的有力工具。此外，通过为半柯西分布引入辅助变量，可以将其表示为逆伽马[分布](@entry_id:182848)的尺度混合，从而在某些模型下（如[贝叶斯线性回归](@entry_id:634286)）实现全共轭性，允许使用高效的[吉布斯采样器](@entry_id:265671)进行推断 。

#### [卷积神经网络](@entry_id:178973)中的结构化先验

对于具有特定结构的模型，如[卷积神经网络](@entry_id:178973)（CNN），[独立同分布](@entry_id:169067)的先验可能不是最优的。CNN中的滤波器（filter）具有空间结构，我们通常期望滤波器本身是平滑的。我们可以通过设计**结构化先验**来将这种[归纳偏置](@entry_id:137419)编码到模型中 。

一种优雅的方法是将滤波器权重函数 $f(\mathbf{p})$ 视为一个定义在滤波器空间坐标 $\mathbf{p}$ 上的**[高斯过程](@entry_id:182192)（Gaussian Process, GP）**。GP先验由一个[均值函数](@entry_id:264860)（通常为零）和一个[协方差核](@entry_id:266561)函数 $K_{\text{conv}}(\mathbf{p}, \mathbf{p}')$ 完全定义。[核函数](@entry_id:145324)可以被设计为使得空间上邻近的两个点 $\mathbf{p}$ 和 $\mathbf{p}'$ 的权重值高度相关，例如使用[径向基函数](@entry_id:754004)（RBF）核。

当我们在一个离散的 $k \times k$ 栅格上评估这个GP时，它就退化为一个多元高斯分布先验 $p(\mathbf{w}) = \mathcal{N}(\mathbf{0}, \mathbf{\Sigma})$，其中 $\mathbf{w}$ 是滤波器权重的向量化形式，[协方差矩阵](@entry_id:139155)的元素为 $\mathbf{\Sigma}_{ab} = K_{\text{conv}}(\mathbf{p}_a, \mathbf{p}_b)$。这个非对角的协方差矩阵精确地编码了[空间平滑](@entry_id:202768)性的先验知识。

在CNN中，**[权重共享](@entry_id:633885)**意味着同一个滤波器 $\mathbf{w}$ 被应用于输入图像的所有位置。在贝叶斯框架下，这意味着来自所有位置的数据都为推断这一个共享的参数 $\mathbf{w}$ 提供了信息。后验分布 $p(\mathbf{w}|D)$ 会整合来自所有图像块的证据，其后验[精度矩阵](@entry_id:264481)是先验精度与所有数据点贡献的似然精度之和。

#### [网络结构](@entry_id:265673)引起的对称性与平坦方向

[神经网](@entry_id:276355)络的结构本身可以引入非可识别性（non-identifiability）和对称性，这导致[后验分布](@entry_id:145605)具有复杂的几何特征，如多峰性和平坦方向。一个典型的例子是**[批量归一化](@entry_id:634986)（Batch Normalization, BN）**层引入的[尺度不变性](@entry_id:180291) 。

考虑一个BN层，它对输入到某个隐藏单元的预激活值进行归一化。可以证明，如果我们将该BN层之前线性变换的权重 $t_i$ 乘以一个正常数 $c_i  0$，即 $t_i \mapsto c_i t_i$，那么由于BN的归一化操作（减去均值，除以[标准差](@entry_id:153618)），这个尺度变换的效果被完美抵消了。只要后续的BN[仿射参数](@entry_id:260625) $(\gamma_i, \beta_i)$ 保持不变，最终的网络输出将完全不受影响。

这种[不变性](@entry_id:140168)对[后验分布](@entry_id:145605)有重大影响。如果先验分布本身也对这种尺度变换不敏感（例如，在对数尺度上是均匀的），那么整个[后验分布](@entry_id:145605)将在与这些尺度变换对应的方向上是完全平坦的。这意味着后验景观中存在一些“山谷”，沿着这些山谷移动，后验概率密度保持不变。

对于[MCMC采样](@entry_id:751801)器来说，这些平坦方向是巨大的挑战，因为标准的梯度驱动方法在平坦区域会迷失方向。然而，如果我们能识别出这些对称性，就可以设计出**对称性感知的采样器**。例如，我们可以提出一种只在这些平坦方向上移动的Metropolis-Hastings提议。由于后验在该方向上是平坦的，并且如果提议本身是对称的，那么接受概率将恒为1。这使得我们能够高效地探索这些非可识别的参数空间维度，而不会降低[采样效率](@entry_id:754496)。

### [无限宽度网络](@entry_id:635735)的理论视角

最后，从理论层面审视BNN，特别是当网络宽度趋于无穷大时，可以为我们提供更深层次的理解。

#### [神经网](@entry_id:276355)络高斯过程 (NNGP) 与[神经正切核](@entry_id:634487) (NTK)

在无限宽度极限下，BNN的行为与[高斯过程](@entry_id:182192)和[核方法](@entry_id:276706)紧密相连 。

首先，对于一个使用标准i.i.d.[高斯先验](@entry_id:749752)初始化的BNN，当其隐藏层宽度趋于无穷大时，根据中心极限定理，网络输出函数 $f_{\theta}(x)$ 的[分布](@entry_id:182848)会收敛到一个[高斯过程](@entry_id:182192)。这个GP的[协方差核](@entry_id:266561)被称为**[神经网](@entry_id:276355)络高斯过程（NNGP）核**，$K_{\text{NNGP}}(x, x')$。这个核完全由网络架构和初始化方案决定。它描述了在训练开始之前，BNN所代表的函数先验。$K_{\text{NNGP}}$ 是一个静态的对象，它定义了[函数空间](@entry_id:143478)的先验结构，并且在训练过程中保持不变（根据其定义）。

另一方面，如果我们考虑一个标准的（非贝叶斯的）[神经网](@entry_id:276355)络，并使用[梯度下降](@entry_id:145942)进行训练，其行为则由**[神经正切核](@entry_id:634487)（Neural Tangent Kernel, NTK）** $\Theta(x, x')$ 决定。NTK定义为网络输出梯度[内积](@entry_id:158127)的无限宽度极限：$\Theta(x, x') = \lim_{n \to \infty} \nabla_{\theta} f_{\theta}(x) \cdot \nabla_{\theta} f_{\theta}(x')$。

一个惊人的理论结果是：在无限宽度和特定的“NTK参数化”（权重[方差](@entry_id:200758)与[扇入](@entry_id:165329)（fan-in）成反比）下，NTK在整个梯度下降训练过程中保持“冻结”在其初始值。这意味着尽管网络参数在变化，但函数输出的演化遵循一个[线性动力学](@entry_id:177848)系统，该系统完全由这个固定的NTK所驱动。这表明，在这种极限情况下，训练一个无限宽的[神经网](@entry_id:276355)络等价于进行一次核回归，其[核函数](@entry_id:145324)就是NTK。

NNGP和NTK共同揭示了深度学习的“双重宇宙”：BNN在无限宽度下对应于一个固定的GP先验（由NNGP核定义），而标准NN的训练动态则由一个固定的线性模型（由NTK定义）所主导。这两者为我们理解BNN的先验结构和标准NN的[优化景观](@entry_id:634681)提供了统一而深刻的数学框架。