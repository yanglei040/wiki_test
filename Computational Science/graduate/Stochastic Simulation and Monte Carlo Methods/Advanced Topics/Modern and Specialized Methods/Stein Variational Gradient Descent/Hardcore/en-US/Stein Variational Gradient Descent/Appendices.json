{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Stein Variational Gradient Descent, it is essential to work through the mechanics of a single particle update. This first practice exercise  grounds the abstract theory in a concrete calculation, starting from the Stein operator and the definition of a reproducing kernel Hilbert space. By manually computing one iteration for a simple two-particle system, you will gain a firsthand understanding of how the kernel and the target score function interact to generate the velocity field that drives the particles.",
            "id": "3348310",
            "problem": "Consider Stein Variational Gradient Descent (SVGD), which performs functional gradient descent on the Kullback–Leibler divergence by transporting a particle distribution via a velocity field in a Reproducing Kernel Hilbert Space (RKHS). In one dimension, the Stein operator for a differentiable target density $p(x)$ acting on a scalar test function $f$ is $T_{p} f(x) = f^{\\prime}(x) + f(x)\\,\\nabla_{x} \\ln p(x)$. Let the target be the standard normal density $p(x) = \\mathcal{N}(0,1)$, and let the RKHS be induced by the Radial Basis Function (RBF) kernel with bandwidth parameter $h > 0$ given by $k_{h}(x,x^{\\prime}) = \\exp\\!\\big(-\\frac{(x-x^{\\prime})^{2}}{2 h^{2}}\\big)$. You are given two initial particles at locations $x_{1}^{(0)} = -1$ and $x_{2}^{(0)} = 2$. Using the SVGD framework starting from the Stein operator definition and the RKHS representer property (without assuming any pre-stated closed-form update), derive the empirical SVGD particle update for a single iteration with step size $\\epsilon > 0$, and then compute the updated particle locations $x_{1}^{(1)}$ and $x_{2}^{(1)}$ explicitly as analytic functions of $h$ and $\\epsilon$. Express your final result in exact closed form (no rounding). The final answer must be a single row vector containing $x_{1}^{(1)}$ and $x_{2}^{(1)}$ as functions of $h$ and $\\epsilon$.",
            "solution": "The user has asked for a derivation of the Stein Variational Gradient Descent (SVGD) update rule and its application to a specific two-particle system.\n\n### Step 1: Problem Validation\nThe problem statement is validated against the specified criteria.\n\n- **Givens**:\n    -   Method: Stein Variational Gradient Descent (SVGD).\n    -   Stein operator ($1D$): $T_{p} f(x) = f^{\\prime}(x) + f(x)\\,\\nabla_{x} \\ln p(x)$.\n    -   Target density: $p(x) = \\mathcal{N}(0,1)$, the standard normal distribution.\n    -   Reproducing Kernel Hilbert Space (RKHS): Induced by the Radial Basis Function (RBF) kernel $k_{h}(x,x^{\\prime}) = \\exp(-\\frac{(x-x^{\\prime})^{2}}{2 h^{2}})$ with bandwidth $h > 0$.\n    -   Initial particle locations: $x_{1}^{(0)} = -1$ and $x_{2}^{(0)} = 2$.\n    -   Number of particles: $N=2$.\n    -   Iteration step size: $\\epsilon > 0$.\n-   **Validation**:\n    -   **Scientifically Grounded:** The problem uses standard, well-established concepts from Bayesian inference, machine learning, and functional analysis (SVGD, KL divergence, RKHS, Stein's method). It is scientifically sound.\n    -   **Well-Posed:** The problem provides all necessary information (target density, kernel, initial conditions, step size) to compute a unique, single-step update for the particle locations.\n    -   **Objective:** The problem is phrased in precise, technical language with no subjective or ambiguous elements.\n    -   The problem is self-contained, consistent, and feasible.\n\n-   **Verdict**: The problem is valid.\n\n### Step 2: Derivation of the SVGD Update Rule\n\nThe objective of SVGD is to transport an initial particle distribution $q_0$ to a target distribution $p$ by applying a velocity field $\\phi(x)$ to the particles. The particles are updated as $x' = x + \\epsilon \\phi(x)$. The optimal velocity field $\\phi(x)$ is the one that maximally decreases the Kullback–Leibler (KL) divergence, $KL(q_k || p)$, at each step. This corresponds to performing functional gradient descent on the KL divergence in an RKHS $\\mathcal{H}$.\n\nThe directional derivative of the KL divergence with respect to a perturbation in the direction of $\\phi$ is given by:\n$$ \\nabla_{\\phi} KL(q || p) = - \\mathbb{E}_{x \\sim q}[T_p \\phi(x)] $$\nwhere $T_p$ is the Stein operator. To achieve the steepest descent, we must choose $\\phi$ to maximize $\\mathbb{E}_{x \\sim q}[T_p \\phi(x)]$ subject to the constraint that $\\phi$ lies within a unit ball in the RKHS, i.e., $\\|\\phi\\|_{\\mathcal{H}} \\le 1$.\n\nThe expression $F[\\phi] = \\mathbb{E}_{x \\sim q}[T_p \\phi(x)]$ is a linear functional of $\\phi$. By the Riesz representation theorem, for any such linear functional on an RKHS, there exists a unique element $\\psi_q \\in \\mathcal{H}$ such that $F[\\phi] = \\langle \\phi, \\psi_q \\rangle_{\\mathcal{H}}$ for all $\\phi \\in \\mathcal{H}$. The function $\\phi$ that maximizes this inner product under the norm constraint is $\\phi = \\psi_q / \\|\\psi_q\\|_{\\mathcal{H}}$. The direction of steepest ascent is therefore given by $\\psi_q$.\n\nWe can find $\\psi_q$ using the reproducing property of the kernel $k(x, x')$. For any function $f \\in \\mathcal{H}$ and point $y$, we have $f(y) = \\langle f(\\cdot), k(y, \\cdot) \\rangle_{\\mathcal{H}}$. Applying this to $\\psi_q$:\n$$ \\psi_q(y) = \\langle \\psi_q(\\cdot), k(y, \\cdot) \\rangle_{\\mathcal{H}} $$\nSince the inner product is symmetric, $\\langle f, g \\rangle_{\\mathcal{H}} = \\langle g, f \\rangle_{\\mathcal{H}}$, and from the definition of $\\psi_q$, we have $\\langle k(y, \\cdot), \\psi_q(\\cdot) \\rangle_{\\mathcal{H}} = F[k(y, \\cdot)]$. Therefore:\n$$ \\psi_q(y) = F[k(y, \\cdot)] = \\mathbb{E}_{x \\sim q}[T_{p,x} k(y, x)] $$\nHere, $T_{p,x}$ denotes that the Stein operator acts on $k(y,x)$ as a function of its second argument, $x$.\nFor a symmetric kernel, $k(y,x) = k(x,y)$, the optimal velocity field at a point $y$ is:\n$$ \\phi^*(y) = \\psi_q(y) = \\mathbb{E}_{x \\sim q}[T_{p,x} k(x,y)] = \\mathbb{E}_{x \\sim q}[\\nabla_x k(x,y) + k(x,y) \\nabla_x \\ln p(x)] $$\nIn practice, the expectation over $q$ is approximated by an empirical average over the current set of $N$ particles $\\{x_i\\}_{i=1}^N$:\n$$ \\phi(y) \\approx \\frac{1}{N} \\sum_{i=1}^N [\\nabla_{x_i} k(x_i, y) + k(x_i, y) \\nabla_{x_i} \\ln p(x_i)] $$\nThe SVGD update for a particle $x_j$ is then $x_j^{(t+1)} = x_j^{(t)} + \\epsilon \\phi(x_j^{(t)})$, where the velocity field is evaluated at the particle's current location $x_j^{(t)}$:\n$$ \\phi(x_j^{(t)}) = \\frac{1}{N} \\sum_{i=1}^N [\\nabla_{x_i} k(x_i^{(t)}, x_j^{(t)}) + k(x_i^{(t)}, x_j^{(t)}) \\nabla_{x_i} \\ln p(x_i^{(t)})] $$\n\n### Step 3: Application to the Given Problem\n\nWe apply the derived update rule to the specifics of the problem.\n1.  **Target Distribution**: $p(x) = \\mathcal{N}(0,1) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{x^2}{2})$.\n    The log-density is $\\ln p(x) = -\\frac{x^2}{2} - \\frac{1}{2}\\ln(2\\pi)$.\n    The score function (gradient of the log-density) is $\\nabla_x \\ln p(x) = -x$.\n\n2.  **Kernel**: The RBF kernel is $k(x, x') = \\exp(-\\frac{(x-x')^2}{2h^2})$.\n    Its gradient with respect to the first argument is:\n    $$ \\nabla_x k(x, x') = \\frac{\\partial}{\\partial x} \\exp\\left(-\\frac{(x-x')^2}{2h^2}\\right) = \\exp\\left(-\\frac{(x-x')^2}{2h^2}\\right) \\cdot \\left(-\\frac{2(x-x')}{2h^2}\\right) = -k(x,x') \\frac{x-x'}{h^2} $$\n\n3.  **Particle Update Equation**: Let the particle locations at iteration $t=0$ be $\\{x_i^{(0)}\\}_{i=1}^N$. The velocity field for particle $x_j^{(0)}$ is:\n    $$ \\phi(x_j^{(0)}) = \\frac{1}{N} \\sum_{i=1}^N \\left[ -k(x_i^{(0)}, x_j^{(0)}) \\frac{x_i^{(0)} - x_j^{(0)}}{h^2} + k(x_i^{(0)}, x_j^{(0)}) (-x_i^{(0)}) \\right] $$\n    $$ \\phi(x_j^{(0)}) = \\frac{1}{N} \\sum_{i=1}^N k(x_i^{(0)}, x_j^{(0)}) \\left( -\\frac{x_i^{(0)} - x_j^{(0)}}{h^2} - x_i^{(0)} \\right) $$\n\n4.  **Initial Conditions**: We have $N=2$ particles at $x_1^{(0)} = -1$ and $x_2^{(0)} = 2$.\n\n5.  **Compute update for $x_1^{(1)}$**: Let $j=1$.\n    $$ \\phi(x_1^{(0)}) = \\frac{1}{2} \\left[ k(x_1^{(0)}, x_1^{(0)}) \\left( -\\frac{x_1^{(0)} - x_1^{(0)}}{h^2} - x_1^{(0)} \\right) + k(x_2^{(0)}, x_1^{(0)}) \\left( -\\frac{x_2^{(0)} - x_1^{(0)}}{h^2} - x_2^{(0)} \\right) \\right] $$\n    We compute the terms:\n    -   $x_1^{(0)} = -1$, $x_2^{(0)} = 2$.\n    -   $k(x_1^{(0)}, x_1^{(0)}) = \\exp(0) = 1$.\n    -   $k(x_2^{(0)}, x_1^{(0)}) = \\exp\\left(-\\frac{(2 - (-1))^2}{2h^2}\\right) = \\exp\\left(-\\frac{9}{2h^2}\\right)$.\n    -   Term for $i=1$: $1 \\cdot (0 - (-1)) = 1$.\n    -   Term for $i=2$: $\\exp\\left(-\\frac{9}{2h^2}\\right) \\left( -\\frac{2 - (-1)}{h^2} - 2 \\right) = \\exp\\left(-\\frac{9}{2h^2}\\right) \\left( -\\frac{3}{h^2} - 2 \\right)$.\n    The velocity is:\n    $$ \\phi(x_1^{(0)}) = \\frac{1}{2} \\left[ 1 - \\left( \\frac{3}{h^2} + 2 \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) \\right] $$\n    The updated particle location is:\n    $$ x_1^{(1)} = x_1^{(0)} + \\epsilon \\phi(x_1^{(0)}) = -1 + \\frac{\\epsilon}{2} \\left[ 1 - \\left( 2 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) \\right] $$\n\n6.  **Compute update for $x_2^{(1)}$**: Let $j=2$.\n    $$ \\phi(x_2^{(0)}) = \\frac{1}{2} \\left[ k(x_1^{(0)}, x_2^{(0)}) \\left( -\\frac{x_1^{(0)} - x_2^{(0)}}{h^2} - x_1^{(0)} \\right) + k(x_2^{(0)}, x_2^{(0)}) \\left( -\\frac{x_2^{(0)} - x_2^{(0)}}{h^2} - x_2^{(0)} \\right) \\right] $$\n    We compute the terms:\n    -   $k(x_1^{(0)}, x_2^{(0)}) = \\exp\\left(-\\frac{(-1 - 2)^2}{2h^2}\\right) = \\exp\\left(-\\frac{9}{2h^2}\\right)$.\n    -   $k(x_2^{(0)}, x_2^{(0)}) = 1$.\n    -   Term for $i=1$: $\\exp\\left(-\\frac{9}{2h^2}\\right) \\left( -\\frac{-1 - 2}{h^2} - (-1) \\right) = \\exp\\left(-\\frac{9}{2h^2}\\right) \\left( \\frac{3}{h^2} + 1 \\right)$.\n    -   Term for $i=2$: $1 \\cdot (0 - 2) = -2$.\n    The velocity is:\n    $$ \\phi(x_2^{(0)}) = \\frac{1}{2} \\left[ \\left( \\frac{3}{h^2} + 1 \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) - 2 \\right] $$\n    The updated particle location is:\n    $$ x_2^{(1)} = x_2^{(0)} + \\epsilon \\phi(x_2^{(0)}) = 2 + \\frac{\\epsilon}{2} \\left[ \\left( 1 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) - 2 \\right] $$\n    This simplifies to:\n    $$ x_2^{(1)} = 2 - \\epsilon + \\frac{\\epsilon}{2} \\left( 1 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) $$\nThe final expressions for the updated particle locations are:\n$$ x_1^{(1)} = -1 + \\frac{\\epsilon}{2} \\left[ 1 - \\left( 2 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) \\right] $$\n$$ x_2^{(1)} = 2 - \\epsilon + \\frac{\\epsilon}{2} \\left( 1 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) $$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-1 + \\frac{\\epsilon}{2} \\left( 1 - \\left(2 + \\frac{3}{h^2}\\right) \\exp\\left(-\\frac{9}{2h^2}\\right) \\right) & 2 - \\epsilon + \\frac{\\epsilon}{2} \\left( 1 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A key strength of SVGD is its ability to maintain particle diversity, preventing the ensemble from collapsing to a single mode. This property arises from a delicate balance between an attraction force, driven by the target distribution's gradient, and a repulsive force, mediated by the kernel. This exercise  dissects these two components, allowing you to quantitatively analyze the equilibrium point where attraction and repulsion cancel, providing crucial insight into how SVGD helps particles explore and cover the entire target distribution.",
            "id": "3348300",
            "problem": "Consider Stein Variational Gradient Descent (SVGD), defined by the functional gradient flow that transports a set of particles to approximate a target density $p(x)$. For a finite set of $n$ particles $\\{x_{i}\\}_{i=1}^{n}$ in $\\mathbb{R}^{d}$, the empirical SVGD velocity at particle $x_{i}$ is\n$$\n\\phi(x_{i}) \\;=\\; \\frac{1}{n} \\sum_{j=1}^{n} \\Big( k(x_{j},x_{i}) \\,\\nabla \\ln p(x_{j}) \\;+\\; \\nabla_{x_{j}} k(x_{j},x_{i}) \\Big),\n$$\nwhere $k(x,y)$ is a positive definite kernel and $\\nabla$ denotes gradient with respect to the argument. In SVGD, the first term is the attraction toward high-density regions and the second term is a repulsion that spreads particles. Let the kernel be the Gaussian radial basis function (RBF) kernel $k(x,y) = \\exp\\!\\big(-\\|x-y\\|^{2}/(2 h^{2})\\big)$ with bandwidth $h>0$.\n\nTask A (scaling of repulsion): In one dimension ($d=1$), for $n=2$ particles at locations $x_{1}$ and $x_{2}$ with inter-particle distance $d = |x_{1}-x_{2}|$, derive the magnitude of the pairwise repulsion contribution from $x_{2}$ to the velocity of $x_{1}$ as a function of $d$ and $h$. You must express it in closed form using only $d$ and $h$.\n\nTask B (quantitative avoidance of mode collapse): Consider the one-dimensional Gaussian target $p(x) = \\mathcal{N}(0,\\sigma^{2})$ with variance $\\sigma^{2} > 0$. Place $n=2$ particles symmetrically at $x_{1}=a$ and $x_{2}=-a$ with $a>0$. Using only the SVGD definition above and the RBF kernel given, compute the exact condition under which the radial component of the SVGD velocity at $x_{1}$ points outward (i.e., increases $|a|$). Then, solve for the unique critical separation $a_{c}(h,\\sigma)$ for which the radial SVGD velocity at $x_{1}$ is exactly zero. Your final answer must be a single closed-form analytic expression for $a_{c}(h,\\sigma)$ with no unevaluated limits or implicit definitions.\n\nAnswer format requirement: Provide only the analytic expression for $a_{c}(h,\\sigma)$ as your final answer. Do not include units. No rounding is required.",
            "solution": "The user has provided a valid problem statement concerning the properties of Stein Variational Gradient Descent (SVGD). The problem is scientifically grounded, well-posed, and contains all necessary information for a rigorous mathematical derivation. I will proceed with a complete solution.\n\nThe problem is divided into two tasks. I will solve them sequentially to arrive at the final required expression for the critical separation $a_{c}(h, \\sigma)$.\n\nFirst, let's establish the necessary mathematical components.\nThe SVGD velocity at particle $x_i$ is given by:\n$$ \\phi(x_{i}) = \\frac{1}{n} \\sum_{j=1}^{n} \\left( k(x_{j},x_{i}) \\nabla \\ln p(x_{j}) + \\nabla_{x_{j}} k(x_{j},x_{i}) \\right) $$\nThe kernel is the one-dimensional ($d=1$) Gaussian RBF kernel:\n$$ k(x,y) = \\exp\\left(-\\frac{(x-y)^2}{2h^2}\\right) $$\nThe gradient of the kernel with respect to its first argument, $x_j$, is:\n$$ \\nabla_{x_{j}} k(x_{j},x_{i}) = \\frac{\\partial}{\\partial x_j} \\exp\\left(-\\frac{(x_j-x_i)^2}{2h^2}\\right) = \\exp\\left(-\\frac{(x_j-x_i)^2}{2h^2}\\right) \\cdot \\left(-\\frac{2(x_j-x_i)}{2h^2}\\right) = -\\frac{x_j-x_i}{h^2} k(x_j,x_i) $$\n\n**Task A: Scaling of Repulsion**\n\nThis task requires the magnitude of the pairwise repulsion contribution from particle $x_2$ to the velocity of particle $x_1$. The number of particles is $n=2$. The repulsion contribution is the part of the velocity update involving the kernel gradient. For the velocity $\\phi(x_1)$, the term corresponding to $j=2$ contains the repulsion from $x_2$. This contribution is given by $\\frac{1}{n} \\nabla_{x_2} k(x_2, x_1)$.\n\nUsing $n=2$ and the derivative calculated above:\n$$ \\text{Repulsion from } x_2 \\text{ on } x_1 = \\frac{1}{2} \\nabla_{x_2} k(x_2, x_1) = \\frac{1}{2} \\left( -\\frac{x_2-x_1}{h^2} k(x_2, x_1) \\right) = \\frac{x_1-x_2}{2h^2} k(x_1, x_2) $$\nThe problem asks for the magnitude of this term. Let $d = |x_1 - x_2|$. The kernel $k(x_1, x_2)$ is always positive.\n$$ k(x_1, x_2) = \\exp\\left(-\\frac{(x_1-x_2)^2}{2h^2}\\right) = \\exp\\left(-\\frac{d^2}{2h^2}\\right) $$\nThe magnitude is therefore:\n$$ \\left| \\frac{x_1-x_2}{2h^2} k(x_1, x_2) \\right| = \\frac{|x_1-x_2|}{2h^2} k(x_1, x_2) = \\frac{d}{2h^2} \\exp\\left(-\\frac{d^2}{2h^2}\\right) $$\nThis expression represents the magnitude of the repulsion force exerted by one particle on another as a function of their separation $d$ and the kernel bandwidth $h$.\n\n**Task B: Quantitative Avoidance of Mode Collapse**\n\nThis task requires us to find the critical separation $a_c(h, \\sigma)$ for a specific setup.\nThe given setup is:\n- Dimension: $d=1$.\n- Number of particles: $n=2$.\n- Particle locations: $x_1 = a$ and $x_2 = -a$, with $a > 0$.\n- Target density: $p(x) = \\mathcal{N}(0, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$.\n\nWe first compute the gradient of the log-target density:\n$$ \\ln p(x) = -\\ln(\\sqrt{2\\pi}\\sigma) - \\frac{x^2}{2\\sigma^2} $$\n$$ \\nabla \\ln p(x) = \\frac{d}{dx} \\ln p(x) = -\\frac{x}{\\sigma^2} $$\nAt the particle locations, we have:\n$$ \\nabla \\ln p(x_1) = \\nabla \\ln p(a) = -\\frac{a}{\\sigma^2} $$\n$$ \\nabla \\ln p(x_2) = \\nabla \\ln p(-a) = -\\frac{-a}{\\sigma^2} = \\frac{a}{\\sigma^2} $$\n\nNext, we evaluate the SVGD velocity at $x_1 = a$. The velocity expression is:\n$$ \\phi(x_1) = \\frac{1}{2} \\left[ \\left( k(x_1, x_1) \\nabla\\ln p(x_1) + \\nabla_{x_1} k(x_1, x_1) \\right) + \\left( k(x_2, x_1) \\nabla\\ln p(x_2) + \\nabla_{x_2} k(x_2, x_1) \\right) \\right] $$\nWe compute each term in the sum:\n\nTerm 1 (interaction with self, $j=1$):\n- $k(x_1, x_1) = k(a,a) = \\exp(0) = 1$.\n- $\\nabla\\ln p(x_1) = -a/\\sigma^2$.\n- $\\nabla_{x_1} k(x_1, x_1) = -\\frac{x_1-x_1}{h^2} k(x_1,x_1) = 0$.\nSo, the first term in brackets is $(1 \\cdot (-a/\\sigma^2) + 0) = -a/\\sigma^2$.\n\nTerm 2 (interaction with $x_2$, $j=2$):\n- $x_2 - x_1 = -a - a = -2a$.\n- $k(x_2, x_1) = k(-a, a) = \\exp\\left(-\\frac{(-2a)^2}{2h^2}\\right) = \\exp\\left(-\\frac{2a^2}{h^2}\\right)$.\n- $\\nabla\\ln p(x_2) = a/\\sigma^2$.\n- $\\nabla_{x_2} k(x_2, x_1) = -\\frac{x_2-x_1}{h^2} k(x_2, x_1) = -\\frac{-2a}{h^2} \\exp\\left(-\\frac{2a^2}{h^2}\\right) = \\frac{2a}{h^2} \\exp\\left(-\\frac{2a^2}{h^2}\\right)$.\nSo, the second term in brackets is:\n$$ \\exp\\left(-\\frac{2a^2}{h^2}\\right) \\cdot \\left(\\frac{a}{\\sigma^2}\\right) + \\frac{2a}{h^2} \\exp\\left(-\\frac{2a^2}{h^2}\\right) = \\exp\\left(-\\frac{2a^2}{h^2}\\right) \\left( \\frac{a}{\\sigma^2} + \\frac{2a}{h^2} \\right) $$\n\nNow, we assemble the full expression for $\\phi(x_1)$:\n$$ \\phi(x_1) = \\frac{1}{2} \\left[ -\\frac{a}{\\sigma^2} + \\exp\\left(-\\frac{2a^2}{h^2}\\right) \\left( \\frac{a}{\\sigma^2} + \\frac{2a}{h^2} \\right) \\right] $$\nFactoring out $a/2$:\n$$ \\phi(x_1) = \\frac{a}{2} \\left[ -\\frac{1}{\\sigma^2} + \\exp\\left(-\\frac{2a^2}{h^2}\\right) \\left( \\frac{1}{\\sigma^2} + \\frac{2}{h^2} \\right) \\right] $$\nThe critical separation $a_c$ is defined as the value of $a$ for which the velocity is zero, i.e., $\\phi(x_1) = 0$. Since we are given $a>0$, we need the term in the square brackets to be zero. Let $a = a_c$:\n$$ -\\frac{1}{\\sigma^2} + \\exp\\left(-\\frac{2a_c^2}{h^2}\\right) \\left( \\frac{1}{\\sigma^2} + \\frac{2}{h^2} \\right) = 0 $$\n$$ \\exp\\left(-\\frac{2a_c^2}{h^2}\\right) \\left( \\frac{1}{\\sigma^2} + \\frac{2}{h^2} \\right) = \\frac{1}{\\sigma^2} $$\n$$ \\exp\\left(-\\frac{2a_c^2}{h^2}\\right) = \\frac{1/\\sigma^2}{1/\\sigma^2 + 2/h^2} $$\nTo simplify the right-hand side, we find a common denominator:\n$$ \\frac{1/\\sigma^2}{(h^2 + 2\\sigma^2)/(\\sigma^2 h^2)} = \\frac{1}{\\sigma^2} \\cdot \\frac{\\sigma^2 h^2}{h^2 + 2\\sigma^2} = \\frac{h^2}{h^2 + 2\\sigma^2} $$\nThe equation for $a_c$ is:\n$$ \\exp\\left(-\\frac{2a_c^2}{h^2}\\right) = \\frac{h^2}{h^2 + 2\\sigma^2} $$\nTo solve for $a_c$, we take the natural logarithm of both sides:\n$$ -\\frac{2a_c^2}{h^2} = \\ln\\left(\\frac{h^2}{h^2 + 2\\sigma^2}\\right) $$\nUsing the property $\\ln(x/y) = -\\ln(y/x)$:\n$$ \\frac{2a_c^2}{h^2} = \\ln\\left(\\frac{h^2 + 2\\sigma^2}{h^2}\\right) = \\ln\\left(1 + \\frac{2\\sigma^2}{h^2}\\right) $$\nNow, we solve for $a_c^2$:\n$$ a_c^2 = \\frac{h^2}{2} \\ln\\left(1 + \\frac{2\\sigma^2}{h^2}\\right) $$\nSince $a > 0$, we take the positive square root:\n$$ a_c(h, \\sigma) = \\sqrt{\\frac{h^2}{2} \\ln\\left(1 + \\frac{2\\sigma^2}{h^2}\\right)} $$\nThis can be written as:\n$$ a_c(h, \\sigma) = h \\sqrt{\\frac{1}{2} \\ln\\left(1 + \\frac{2\\sigma^2}{h^2}\\right)} $$\nThis is the unique critical separation for which the SVGD velocity at $x_1=a$ is zero. For $a < a_c$, the term $\\exp(-2a^2/h^2)$ is larger, making the velocity positive (outward repulsion dominates). For $a > a_c$, the velocity is negative (inward attraction to the mode at $x=0$ dominates).",
            "answer": "$$ \\boxed{h \\sqrt{\\frac{1}{2} \\ln\\left(1 + \\frac{2\\sigma^2}{h^2}\\right)}} $$"
        },
        {
            "introduction": "In practice, the performance of SVGD is highly sensitive to the choice of the kernel bandwidth, $h$, which controls the length scale of particle interactions. This problem  explores the widely-used median heuristic for automatically setting this parameter. By analyzing its behavior in different scenarios, including high-dimensional spaces, you will build critical thinking about the practical challenges of over-smoothing and under-smoothing and how heuristic choices can impact algorithmic convergence.",
            "id": "3348268",
            "problem": "Consider Stein Variational Gradient Descent (SVGD) applied to a multivariate Gaussian target $p(x) = \\mathcal{N}(\\mu, \\Sigma)$ using a Radial Basis Function (RBF) kernel $k(x, y) = \\exp\\!\\left(-\\|x - y\\|^{2}/(2 h^{2})\\right)$ with bandwidth $h$ chosen at each iteration by the median heuristic $h^{2} = \\operatorname{median}\\{\\|x_{i} - x_{j}\\|^{2} : i \\neq j\\}$ computed from the current particle set $\\{x_{i}\\}_{i=1}^{n}$. The SVGD update direction for a test point $z$ is defined as $\\phi^{\\star}(z) = \\mathbb{E}_{x \\sim q}\\!\\left[k(x, z)\\,\\nabla_{x}\\log p(x) + \\nabla_{x} k(x, z)\\right]$, where $q$ denotes the current particle empirical distribution. For the Gaussian target, $\\nabla_{x}\\log p(x) = -\\Sigma^{-1}(x - \\mu)$, and for the RBF kernel, $\\nabla_{x}k(x, z) = -\\frac{1}{h^{2}}(x - z)\\,k(x, z)$.\n\nStarting from these definitions, use properties of Gaussian random vectors and concentration of measure to analyze how the median heuristic scales with the particle geometry and problem dimension. In particular, for $q$ approximately Gaussian with covariance close to $\\sigma_{q}^{2} I_{d}$, characterize the typical pairwise squared distances and their median as $d$ grows, and deduce the implications for the magnitude and locality of the two SVGD terms, $k(x, z)\\,\\nabla_{x}\\log p(x)$ and $\\nabla_{x}k(x, z)$, as functions of $h$. Interpret “over-smoothing” as choosing $h$ so large that $k$ becomes nearly non-local and the update excessively averages scores across distant particles, and “under-smoothing” as choosing $h$ so small that $k$ becomes highly localized with negligible coupling except for very near neighbors, leading to high-variance, overly local updates.\n\nWhich of the following statements correctly identifies scenarios in which the median heuristic leads to under- or over-smoothing for Gaussian targets?\n\nA. For an isotropic Gaussian target $p(x) = \\mathcal{N}(\\mu, \\sigma^{2} I_{d})$ and a current particle cloud $q$ that is also approximately isotropic with covariance $\\sigma_{q}^{2} I_{d}$, the median heuristic yields $h^{2} \\approx 2 d \\sigma_{q}^{2}$ (by concentration of pairwise distances). When either $d$ is large or $\\sigma_{q}^{2} \\gg \\sigma^{2}$, this choice causes over-smoothing: typical kernel weights $k(x, z)$ are near constant across the cloud and the repulsive term $\\nabla_{x}k(x, z)$ is attenuated by the factor $1/h^{2} \\propto 1/d$, biasing updates toward global score averaging and slowing contraction toward the target.\n\nB. For the same isotropic Gaussian target but with a current particle cloud $q$ that is highly concentrated ($\\sigma_{q}^{2} \\ll \\sigma^{2}$), the median heuristic sets $h^{2} \\approx 2 d \\sigma_{q}^{2}$, which is too small relative to the target scale. This produces under-smoothing: $k(x, z)$ becomes sharply peaked so that interactions vanish beyond very short ranges, the repulsive term only acts among very close neighbors, and the score term becomes effectively unsmoothed, resulting in high-variance, locally myopic updates that can impede spreading to match $\\sigma^{2}$.\n\nC. For a highly anisotropic Gaussian target with covariance $\\Sigma$ having a large condition number, and a current particle cloud aligned with the principal axes, the median heuristic based on Euclidean distances simultaneously over-smooths directions with large variance and under-smooths directions with small variance, because a single scalar $h$ cannot adapt to anisotropy.\n\nD. The median heuristic is asymptotically optimal for Gaussian targets as the number of particles $n \\to \\infty$ in any dimension $d$, in the sense that it universally minimizes the bias–variance tradeoff in the kernelized score approximation used by SVGD; therefore it cannot lead to under- or over-smoothing.\n\nSelect all that apply.",
            "solution": "The user wants a critical analysis of a problem concerning Stein Variational Gradient Descent (SVGD), followed by a solution.\n\n### Step 1: Problem Validation\n\nI will first extract the given information verbatim and then assess the validity of the problem statement.\n\n**Extracted Givens:**\n*   **Target distribution**: $p(x) = \\mathcal{N}(\\mu, \\Sigma)$, a multivariate Gaussian.\n*   **Kernel**: Radial Basis Function (RBF), $k(x, y) = \\exp\\!\\left(-\\|x - y\\|^{2}/(2 h^{2})\\right)$.\n*   **Bandwidth selection**: Median heuristic, $h^{2} = \\operatorname{median}\\{\\|x_{i} - x_{j}\\|^{2} : i \\neq j\\}$.\n*   **SVGD update direction**: $\\phi^{\\star}(z) = \\mathbb{E}_{x \\sim q}\\!\\left[k(x, z)\\,\\nabla_{x}\\log p(x) + \\nabla_{x} k(x, z)\\right]$, where $q$ is the particle empirical distribution.\n*   **Gaussian score function**: $\\nabla_{x}\\log p(x) = -\\Sigma^{-1}(x - \\mu)$.\n*   **RBF kernel gradient**: $\\nabla_{x}k(x, z) = -\\frac{1}{h^{2}}(x - z)\\,k(x, z)$.\n*   **Analysis setup**: Assume $q$ is approximately Gaussian with covariance $\\sigma_{q}^{2} I_{d}$.\n*   **Definition of over-smoothing**: \"$h$ so large that $k$ becomes nearly non-local and the update excessively averages scores across distant particles\".\n*   **Definition of under-smoothing**: \"$h$ so small that $k$ becomes highly localized... leading to high-variance, overly local updates\".\n\n**Validation Analysis:**\n1.  **Scientific Groundedness**: The problem is set within the established framework of SVGD, a standard algorithm in Bayesian computation and machine learning. All mathematical definitions (Gaussian PDF, RBF kernel, gradients, SVGD update) are correct and standard. The use of concentration of measure for high-dimensional Gaussians is a standard and appropriate tool for this analysis. The problem is scientifically sound.\n2.  **Well-Posedness**: The problem asks to evaluate several statements based on a theoretical analysis. The premises are clearly laid out, and the definitions of \"over-smoothing\" and \"under-smoothing\" are provided, allowing for a rigorous, objective analysis of the options. A unique set of correct/incorrect verdicts for the options can be derived.\n3.  **Objectivity**: The language is precise and mathematical. The concepts of over- and under-smoothing are defined explicitly for the purpose of the problem, removing subjectivity.\n4.  **Completeness and Consistency**: The problem provides all necessary components to perform the analysis. There are no internal contradictions.\n5.  **Unrealistic/Infeasible Conditions**: The scenarios described (isotropic/anisotropic targets, concentrated/dispersed particles) are common and realistic in the practical application of SVGD.\n\n**Verdict:**\nThe problem statement is **valid**. It is a well-posed, scientifically sound, and clear question that requires a solid conceptual understanding of SVGD and high-dimensional statistics. I will now proceed to the solution.\n\n### Step 2: Derivation and Option Analysis\n\nThe SVGD update for a point $z$ is driven by two terms:\n$$\n\\phi^{\\star}(z) = \\underbrace{-\\mathbb{E}_{x \\sim q}\\left[ k(x, z) \\Sigma^{-1}(x-\\mu) \\right]}_{\\text{Term 1: Weighted Score (Attraction)}} + \\underbrace{\\mathbb{E}_{x \\sim q}\\left[ \\frac{1}{h^2}(z-x)k(x,z) \\right]}_{\\text{Term 2: Repulsion}}\n$$\nTerm 1 pulls particles towards the mode of the target distribution $p(x)$, with the pull at $z$ being a kernel-weighted average of the scores $\\nabla_x \\log p(x)$ at other particle locations $x$. Term 2 is a repulsive force that pushes particles apart, preventing them from collapsing to a single point. The balance is controlled by the kernel $k$ and its bandwidth $h$.\n\n**Analysis of the Median Heuristic in High Dimensions**\nThe problem asks to analyze the median heuristic $h^{2} = \\operatorname{median}\\{\\|x_{i} - x_{j}\\|^{2} : i \\neq j\\}$ when the particle distribution $q$ is approximately $\\mathcal{N}(\\mu_q, \\sigma_q^2 I_d)$.\n\nLet $x_i, x_j$ be two independent samples from $q$. Their difference, $v = x_i - x_j$, follows the distribution $\\mathcal{N}(0, 2\\sigma_q^2 I_d)$. The squared distance is $\\|v\\|^2 = \\sum_{k=1}^d v_k^2$. Each component $v_k \\sim \\mathcal{N}(0, 2\\sigma_q^2)$. We can write this as $\\|v\\|^2 = 2\\sigma_q^2 \\sum_{k=1}^d Z_k^2$, where $Z_k \\sim \\mathcal{N}(0,1)$ are independent standard normal variables. The sum $\\sum_{k=1}^d Z_k^2$ follows a chi-squared distribution with $d$ degrees of freedom, denoted $\\chi^2(d)$.\n\nFor large $d$, the $\\chi^2(d)$ distribution is sharply concentrated around its mean, $\\mathbb{E}[\\chi^2(d)] = d$. Its median is also approximately $d$. Therefore, the distribution of pairwise squared distances $\\|x_i - x_j\\|^2$ is tightly concentrated around its mean value, $2\\sigma_q^2 d$. The median heuristic will thus select a bandwidth $h^2$ such that:\n$$\nh^2 \\approx 2d\\sigma_q^2\n$$\n\nWith this scaling, we can analyze the behavior of the kernel and the update terms. The argument of the RBF kernel for a typical pair of particles $(x,z)$ with $\\|x-z\\|^2 \\approx 2d\\sigma_q^2$ becomes:\n$$\n\\frac{-\\|x-z\\|^2}{2h^2} \\approx \\frac{-2d\\sigma_q^2}{2(2d\\sigma_q^2)} = -\\frac{1}{2}\n$$\nThis means that for large $d$, the kernel value $k(x,z)$ will be approximately $e^{-1/2}$ for most pairs of particles, rather than decaying to zero for distant pairs.\n\nNow, let's evaluate each option.\n\n**A. For an isotropic Gaussian target... when either $d$ is large or $\\sigma_{q}^{2} \\gg \\sigma^{2}$, this choice causes over-smoothing...**\n\n*   **Analysis**: The statement claims $h^2 \\approx 2d\\sigma_q^2$, which agrees with our derivation.\n*   **Case 1: Large $d$**. As shown above, for large $d$, the kernel $k(x,z)$ becomes nearly constant ($e^{-1/2}$) across most pairs of particles. This matches the problem's definition of over-smoothing, as the kernel is effectively non-local. The repulsive term's magnitude is scaled by $1/h^2 \\approx 1/(2d\\sigma_q^2)$, which diminishes as $d^{-1}$. This attenuation weakens the particle repulsion. The attraction term becomes a globally-averaged score, as $k(x,z)$ is nearly constant. This behavior slows down convergence dynamics, especially contraction. Thus, for large $d$, the median heuristic leads to over-smoothing.\n*   **Case 2: $\\sigma_q^2 \\gg \\sigma^2$**. The particle cloud is too dispersed compared to the target. The goal is to contract. The median heuristic picks $h^2 \\approx 2d\\sigma_q^2$. A large $\\sigma_q^2$ leads to a large $h$, which corresponds to over-smoothing by definition. A large kernel bandwidth causes excessive averaging of the score function, which can blunt the sharp gradient information needed for efficient contraction towards the target mode.\n*   **Conclusion**: The reasoning in this statement is sound and consistent with our analysis.\n\n*   **Verdict on A**: **Correct**.\n\n**B. For the same isotropic Gaussian target but with... $\\sigma_{q}^{2} \\ll \\sigma^{2}$, the median heuristic sets $h^{2} \\approx 2 d \\sigma_{q}^{2}$, which is too small... This produces under-smoothing...**\n\n*   **Analysis**: The scenario is a highly concentrated particle cloud that needs to expand to match the target.\n*   The median heuristic sets $h^2 \\approx 2d\\sigma_q^2$. Since $\\sigma_q^2$ is small, the bandwidth $h$ will be small relative to the scale of the target, $\\sigma$. This corresponds to the definition of under-smoothing.\n*   A small $h$ makes the kernel $k(x,z)$ sharply peaked. Interactions become negligible for any pair of particles $(x,z)$ where $\\|x-z\\|$ is significantly larger than $h$. This means interactions are very local.\n*   The attraction term $\\mathbb{E}_{x \\sim q}[k(x,z)\\nabla_x \\log p(x)]$ becomes a very local average of the score around $z$, approaching the un-smoothed, single-point score $\\nabla_z \\log p(z)$. Such an update is \"myopic\" and has high variance as it depends heavily on the noisy position of a single particle.\n*   This can impede spreading: The score term $\\nabla\\log p(x)=-\\frac{1}{\\sigma^2}(x-\\mu)$ pulls all particles toward the mean, resisting expansion. The repulsive term $\\nabla k$ is responsible for pushing them apart. With under-smoothing, the repulsion is strong but very short-ranged, and the local score updates are noisy. This can lead to inefficient, jittery particle motion rather than a coordinated expansion to fill the target's volume.\n*   **Conclusion**: The statement accurately describes the mechanism and consequences of under-smoothing in this scenario.\n\n*   **Verdict on B**: **Correct**.\n\n**C. For a highly anisotropic Gaussian target... the median heuristic based on Euclidean distances simultaneously over-smooths directions with large variance and under-smooths directions with small variance...**\n\n*   **Analysis**: Let the particle cloud have an anisotropic covariance $\\Sigma_q$ with principal variances $\\sigma_{q,k}^2$. The median heuristic, based on the scalar Euclidean distance $\\|x-y\\|^2 = \\sum_k (x_k-y_k)^2$, sets a single isotropic bandwidth $h$. This $h^2$ will be on the order of the average of the pairwise squared distances, $h^2 \\approx \\mathbb{E}[\\|x_i-x_j\\|^2] = 2\\,\\operatorname{Tr}(\\Sigma_q) = 2\\sum_k \\sigma_{q,k}^2$.\n*   The bandwidth $h$ is thus determined by the sum of variances across all directions.\n*   **Directions with small variance**: Let $\\sigma_{q,S}^2$ be a small variance component. The particle spread in this direction is $\\sim\\sigma_{q,S}$. The kernel bandwidth is $h \\approx \\sqrt{2\\operatorname{Tr}(\\Sigma_q)}$. Since $\\sigma_{q,S}^2$ is just one small part of the trace, it's clear that $h \\gg \\sigma_{q,S}$. Using a bandwidth much larger than the data scale is the definition of **over-smoothing**.\n*   **Directions with large variance**: Let $\\sigma_{q,L}^2$ be a large variance component. The statement claims this direction is over-smoothed. This would require $h \\gg \\sigma_{q,L}$. But $h^2 \\approx 2(\\sigma_{q,L}^2 + \\sum_{k \\neq L} \\sigma_{q,k}^2)$, so $h$ is of the same order as $\\sigma_{q,L}$. It's possible to get mild over-smoothing, but not necessarily.\n*   The statement also claims that directions with small variance are **under-smoothed**. This requires $h \\ll \\sigma_{q,S}$. As we established, the opposite is true: $h \\gg \\sigma_{q,S}$, leading to over-smoothing.\n*   The statement has the effects incorrectly assigned. A single, large, isotropic bandwidth $h$ (driven by the large variance directions) will be too large for the small variance directions, causing **over-smoothing** there. It might be too small for the large variance directions (causing **under-smoothing**) only in contrived cases where the sum of many small variances dominates one large variance, but as shown in the thought process, this is hard to achieve. The most straightforward and certain consequence is the over-smoothing of low-variance directions. The statement \"under-smooths directions with small variance\" is factually incorrect.\n*   **Conclusion**: While the core idea that a scalar $h$ is unsuited for anisotropy is correct, the specific claims about which directions are over/under-smoothed are incorrect.\n\n*   **Verdict on C**: **Incorrect**.\n\n**D. The median heuristic is asymptotically optimal for Gaussian targets... therefore it cannot lead to under- or over-smoothing.**\n\n*   **Analysis**: This statement makes a very strong claim about optimality. The median heuristic is a rule of thumb, prized for its simplicity and robustness, not for any proven optimality. The choice of bandwidth for kernel methods always involves a bias-variance tradeoff, and the optimal choice depends on the specific properties of the distribution and the desired error metric. There is no universally optimal bandwidth, and the median heuristic is certainly not one.\n*   The conclusion \"therefore it cannot lead to under- or over-smoothing\" is a direct contradiction of the analyses in A, B, and C, which show clear scenarios where it does. The premise is false, and the conclusion is incorrect.\n*   **Conclusion**: The statement is fundamentally wrong. Heuristics, by their nature, are approximations that can fail or be suboptimal in certain regimes.\n\n*   **Verdict on D**: **Incorrect**.",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}