{
    "hands_on_practices": [
        {
            "introduction": "A central claim of MC dropout is its interpretation as an approximation to Bayesian inference. This exercise provides a hands-on opportunity to verify this connection in an analytically tractable setting, the Bayesian linear regression model . By deriving and comparing the predictive variance from both MC dropout and the exact Bayesian posterior, you will gain a concrete understanding of how dropout-induced randomness mirrors true parameter uncertainty.",
            "id": "3321131",
            "problem": "Consider a linear regression model with additive Gaussian noise, where the observed response vector is modeled as $y \\in \\mathbb{R}^{n}$, features are collected in a design matrix $X \\in \\mathbb{R}^{n \\times d}$, and the regression coefficients are $\\beta \\in \\mathbb{R}^{d}$. The data-generating process is $y = X \\beta + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2} I_{n})$, where $\\sigma_{\\epsilon}^{2} > 0$ is known and $I_{n}$ is the $n \\times n$ identity matrix. Assume a Gaussian prior on the regression coefficients $\\beta \\sim \\mathcal{N}(0, \\sigma_{\\beta}^{2} I_{d})$ with $\\sigma_{\\beta}^{2} > 0$ and $I_{d}$ the $d \\times d$ identity matrix.\n\nAt test time, for a single new feature vector $x^{\\ast} \\in \\mathbb{R}^{d}$, consider feature-level Monte Carlo (MC) dropout using independent Bernoulli masks. Specifically, define a random mask vector $m \\in \\{0,1\\}^{d}$ with independent components $m_{i} \\sim \\mathrm{Bernoulli}(q)$ for $i \\in \\{1,\\dots,d\\}$, where $q \\in (0,1)$ is the retention probability. The masked input is $x^{\\ast}_{m} = m \\odot x^{\\ast}$, where $\\odot$ denotes elementwise multiplication. A single MC dropout predictive draw is generated as $y^{\\ast} = (x^{\\ast}_{m})^{\\top} \\hat{\\beta} + \\epsilon^{\\ast}$, where $\\epsilon^{\\ast} \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})$ is independent of $m$ and $\\hat{\\beta} \\in \\mathbb{R}^{d}$ is a fixed estimator of $\\beta$. In this setup, adopt the Maximum A Posteriori (MAP) estimator under the stated Gaussian prior and likelihood for $\\hat{\\beta}$.\n\nTasks:\n- Derive the MC dropout predictive mean $\\mathbb{E}[y^{\\ast} \\mid x^{\\ast}, \\hat{\\beta}]$ and variance $\\mathrm{Var}(y^{\\ast} \\mid x^{\\ast}, \\hat{\\beta})$ by averaging over the randomness in the mask $m$ and the noise $\\epsilon^{\\ast}$.\n- Derive the exact Bayesian linear regression posterior predictive mean and variance for $y^{\\ast}$ under the Gaussian prior $\\beta \\sim \\mathcal{N}(0, \\sigma_{\\beta}^{2} I_{d})$ and the Gaussian likelihood $y \\mid \\beta \\sim \\mathcal{N}(X \\beta, \\sigma_{\\epsilon}^{2} I_{n})$.\n- Using the MAP estimator $\\hat{\\beta}$ equal to the exact posterior mean for $\\beta$, provide a single closed-form analytic expression for the difference\n$$\\Delta(x^{\\ast}) = \\mathrm{Var}(y^{\\ast} \\mid x^{\\ast}, \\text{MC dropout}) - \\mathrm{Var}(y^{\\ast} \\mid x^{\\ast}, \\text{Bayesian posterior predictive}).$$\n\nExpress your final answer as a single symbolic expression for $\\Delta(x^{\\ast})$. No rounding is required, and no physical units apply. Define and use all symbols clearly.",
            "solution": "The problem asks for the derivation of predictive moments for Monte Carlo (MC) dropout and Bayesian linear regression, and finally for the difference between their predictive variances. We will address each part systematically.\n\n### **Part 1: MC Dropout Predictive Mean and Variance**\n\nThe model for a single MC dropout predictive draw is given by $y^{\\ast} = (x^{\\ast}_{m})^{\\top} \\hat{\\beta} + \\epsilon^{\\ast}$, where $x^{\\ast}_{m} = m \\odot x^{\\ast}$. The random variables in this expression are the mask vector $m$ and the noise term $\\epsilon^{\\ast}$. The components of the mask, $m_i$, are independent and identically distributed as $\\mathrm{Bernoulli}(q)$, and $\\epsilon^{\\ast} \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})$. The estimator $\\hat{\\beta}$ is treated as a fixed quantity for this calculation.\n\nThe predictive mean is the expectation of $y^{\\ast}$ over the distributions of $m$ and $\\epsilon^{\\ast}$. By the linearity of expectation:\n$$\n\\mathbb{E}[y^{\\ast} \\mid x^{\\ast}, \\hat{\\beta}] = \\mathbb{E}[(m \\odot x^{\\ast})^{\\top} \\hat{\\beta} + \\epsilon^{\\ast}] = \\mathbb{E}[(m \\odot x^{\\ast})^{\\top} \\hat{\\beta}] + \\mathbb{E}[\\epsilon^{\\ast}]\n$$\nGiven that $\\epsilon^{\\ast} \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})$, we have $\\mathbb{E}[\\epsilon^{\\ast}] = 0$. The first term can be written as:\n$$\n\\mathbb{E}[(m \\odot x^{\\ast})^{\\top} \\hat{\\beta}] = \\mathbb{E}\\left[\\sum_{i=1}^{d} m_i x_i^{\\ast} \\hat{\\beta}_i\\right] = \\sum_{i=1}^{d} \\mathbb{E}[m_i] x_i^{\\ast} \\hat{\\beta}_i\n$$\nFor a Bernoulli random variable $m_i \\sim \\mathrm{Bernoulli}(q)$, the expectation is $\\mathbb{E}[m_i] = q$. Substituting this back gives:\n$$\n\\mathbb{E}[y^{\\ast} \\mid x^{\\ast}, \\hat{\\beta}] = \\sum_{i=1}^{d} q x_i^{\\ast} \\hat{\\beta}_i = q (x^{\\ast})^{\\top} \\hat{\\beta}\n$$\n\nThe predictive variance is calculated using the law of total variance. Since $m$ and $\\epsilon^{\\ast}$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}(y^{\\ast} \\mid x^{\\ast}, \\hat{\\beta}) = \\mathrm{Var}((m \\odot x^{\\ast})^{\\top} \\hat{\\beta} + \\epsilon^{\\ast}) = \\mathrm{Var}((m \\odot x^{\\ast})^{\\top} \\hat{\\beta}) + \\mathrm{Var}(\\epsilon^{\\ast})\n$$\nWe are given $\\mathrm{Var}(\\epsilon^{\\ast}) = \\sigma_{\\epsilon}^{2}$. For the first term, we use the independence of the mask components $m_i$:\n$$\n\\mathrm{Var}((m \\odot x^{\\ast})^{\\top} \\hat{\\beta}) = \\mathrm{Var}\\left(\\sum_{i=1}^{d} m_i x_i^{\\ast} \\hat{\\beta}_i\\right) = \\sum_{i=1}^{d} \\mathrm{Var}(m_i x_i^{\\ast} \\hat{\\beta}_i)\n$$\nSince $x_i^{\\ast}$ and $\\hat{\\beta}_i$ are constants in this context, we have:\n$$\n\\mathrm{Var}(m_i x_i^{\\ast} \\hat{\\beta}_i) = (x_i^{\\ast} \\hat{\\beta}_i)^2 \\mathrm{Var}(m_i)\n$$\nFor a Bernoulli random variable $m_i \\sim \\mathrm{Bernoulli}(q)$, the variance is $\\mathrm{Var}(m_i) = q(1-q)$. Therefore,\n$$\n\\mathrm{Var}((m \\odot x^{\\ast})^{\\top} \\hat{\\beta}) = \\sum_{i=1}^{d} (x_i^{\\ast} \\hat{\\beta}_i)^2 q(1-q) = q(1-q) \\sum_{i=1}^{d} (x_i^{\\ast} \\hat{\\beta}_i)^2\n$$\nCombining the terms, the MC dropout predictive variance is:\n$$\n\\mathrm{Var}(y^{\\ast} \\mid x^{\\ast}, \\text{MC dropout}) = q(1-q) \\sum_{i=1}^{d} (\\hat{\\beta}_i x_i^{\\ast})^2 + \\sigma_{\\epsilon}^{2}\n$$\n\n### **Part 2: Bayesian Linear Regression Posterior Predictive**\n\nIn the Bayesian framework, we first determine the posterior distribution of the coefficients $\\beta$ given the data $(X, y)$. The posterior is proportional to the product of the likelihood and the prior: $p(\\beta \\mid X, y) \\propto p(y \\mid X, \\beta) p(\\beta)$.\nThe likelihood is $p(y \\mid X, \\beta) \\sim \\mathcal{N}(X\\beta, \\sigma_{\\epsilon}^{2} I_n)$, and the prior is $p(\\beta) \\sim \\mathcal{N}(0, \\sigma_{\\beta}^{2} I_d)$.\nThe posterior for $\\beta$ is a Gaussian distribution, $p(\\beta \\mid X, y) = \\mathcal{N}(\\mu_{\\beta}, \\Sigma_{\\beta})$, with covariance $\\Sigma_{\\beta}$ and mean $\\mu_{\\beta}$ given by:\n$$\n\\Sigma_{\\beta}^{-1} = \\frac{1}{\\sigma_{\\epsilon}^{2}} X^{\\top}X + \\frac{1}{\\sigma_{\\beta}^{2}} I_d \\implies \\Sigma_{\\beta} = \\left(\\frac{1}{\\sigma_{\\epsilon}^{2}} X^{\\top}X + \\frac{1}{\\sigma_{\\beta}^{2}} I_d\\right)^{-1}\n$$\n$$\n\\mu_{\\beta} = \\Sigma_{\\beta} \\left(\\frac{1}{\\sigma_{\\epsilon}^{2}} X^{\\top}y\\right) = \\left(\\frac{1}{\\sigma_{\\epsilon}^{2}} X^{\\top}X + \\frac{1}{\\sigma_{\\beta}^{2}} I_d\\right)^{-1} \\frac{1}{\\sigma_{\\epsilon}^{2}} X^{\\top}y\n$$\nLet's define the regularization parameter $\\lambda = \\frac{\\sigma_{\\epsilon}^2}{\\sigma_{\\beta}^2}$. Then we can write:\n$$\n\\Sigma_{\\beta} = \\sigma_{\\epsilon}^2 (X^{\\top}X + \\lambda I_d)^{-1}\n$$\n$$\n\\mu_{\\beta} = (X^{\\top}X + \\lambda I_d)^{-1} X^{\\top}y\n$$\nThis posterior mean $\\mu_{\\beta}$ is the MAP estimator for $\\beta$. The problem states that $\\hat{\\beta}$ used in the MC dropout part is this MAP estimator, so we have $\\hat{\\beta} = \\mu_{\\beta}$.\n\nThe posterior predictive distribution for a new observation $y^{\\ast}$ at input $x^{\\ast}$ is $p(y^{\\ast} \\mid x^{\\ast}, X, y) = \\int p(y^{\\ast} \\mid x^{\\ast}, \\beta) p(\\beta \\mid X, y) d\\beta$.\nGiven $y^{\\ast} = (x^{\\ast})^{\\top}\\beta + \\epsilon^{\\ast}$ with $\\epsilon^{\\ast} \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^2)$, the predictive distribution is also Gaussian. Its mean is:\n$$\n\\mathbb{E}[y^{\\ast} \\mid x^{\\ast}, X, y] = \\mathbb{E}_{\\beta \\mid X,y}[\\mathbb{E}[y^{\\ast} \\mid \\beta]] = \\mathbb{E}_{\\beta \\mid X,y}[(x^{\\ast})^{\\top}\\beta] = (x^{\\ast})^{\\top} \\mu_{\\beta} = (x^{\\ast})^{\\top} \\hat{\\beta}\n$$\nIts variance is found using the law of total variance:\n$$\n\\mathrm{Var}(y^{\\ast} \\mid x^{\\ast}, X, y) = \\mathbb{E}_{\\beta \\mid X,y}[\\mathrm{Var}(y^{\\ast} \\mid \\beta)] + \\mathrm{Var}_{\\beta \\mid X,y}(\\mathbb{E}[y^{\\ast} \\mid \\beta])\n$$\nThe first term is the expected data variance (aleatoric uncertainty): $\\mathbb{E}_{\\beta \\mid X,y}[\\sigma_{\\epsilon}^2] = \\sigma_{\\epsilon}^2$.\nThe second term is the variance in the mean prediction due to uncertainty in $\\beta$ (epistemic uncertainty):\n$$\n\\mathrm{Var}_{\\beta \\mid X,y}((x^{\\ast})^{\\top}\\beta) = (x^{\\ast})^{\\top} \\mathrm{Var}_{\\beta \\mid X,y}(\\beta) x^{\\ast} = (x^{\\ast})^{\\top} \\Sigma_{\\beta} x^{\\ast}\n$$\nThus, the Bayesian posterior predictive variance is:\n$$\n\\mathrm{Var}(y^{\\ast} \\mid x^{\\ast}, \\text{Bayesian posterior predictive}) = \\sigma_{\\epsilon}^{2} + (x^{\\ast})^{\\top} \\Sigma_{\\beta} x^{\\ast}\n$$\n\n### **Part 3: Difference in Variances**\n\nWe are asked to compute $\\Delta(x^{\\ast})$, the difference between the MC dropout variance and the Bayesian posterior predictive variance.\n$$\n\\Delta(x^{\\ast}) = \\mathrm{Var}(y^{\\ast} \\mid x^{\\ast}, \\text{MC dropout}) - \\mathrm{Var}(y^{\\ast} \\mid x^{\\ast}, \\text{Bayesian posterior predictive})\n$$\nSubstituting the expressions derived in the previous parts:\n$$\n\\Delta(x^{\\ast}) = \\left( q(1-q) \\sum_{i=1}^{d} (\\hat{\\beta}_i x_i^{\\ast})^2 + \\sigma_{\\epsilon}^{2} \\right) - \\left( \\sigma_{\\epsilon}^{2} + (x^{\\ast})^{\\top} \\Sigma_{\\beta} x^{\\ast} \\right)\n$$\nThe aleatoric uncertainty term $\\sigma_{\\epsilon}^{2}$ cancels out:\n$$\n\\Delta(x^{\\ast}) = q(1-q) \\sum_{i=1}^{d} (\\hat{\\beta}_i x_i^{\\ast})^2 - (x^{\\ast})^{\\top} \\Sigma_{\\beta} x^{\\ast}\n$$\nThe first term is a quadratic form in $x^{\\ast}$. Let $\\hat{\\beta} \\odot \\hat{\\beta}$ be the vector of element-wise squares of $\\hat{\\beta}$, i.e., $(\\hat{\\beta}_1^2, \\dots, \\hat{\\beta}_d^2)^{\\top}$, and let $\\mathrm{diag}(v)$ denote a diagonal matrix with the elements of vector $v$ on its diagonal. Then the sum can be written as $(x^{\\ast})^{\\top} \\mathrm{diag}(\\hat{\\beta} \\odot \\hat{\\beta}) x^{\\ast}$.\nSo, we have:\n$$\n\\Delta(x^{\\ast}) = q(1-q) (x^{\\ast})^{\\top} \\mathrm{diag}(\\hat{\\beta} \\odot \\hat{\\beta}) x^{\\ast} - (x^{\\ast})^{\\top} \\Sigma_{\\beta} x^{\\ast}\n$$\nThis can be expressed compactly as a single quadratic form:\n$$\n\\Delta(x^{\\ast}) = (x^{\\ast})^{\\top} \\left( q(1-q) \\mathrm{diag}(\\hat{\\beta} \\odot \\hat{\\beta}) - \\Sigma_{\\beta} \\right) x^{\\ast}\n$$\nTo obtain the final expression, we substitute the definition of $\\Sigma_{\\beta}$:\n$$\n\\Sigma_{\\beta} = \\sigma_{\\epsilon}^2(X^{\\top}X + \\frac{\\sigma_{\\epsilon}^2}{\\sigma_{\\beta}^2} I_d)^{-1}\n$$\nThe MAP estimator $\\hat{\\beta}$ is defined as $\\hat{\\beta} = (X^{\\top}X + \\frac{\\sigma_{\\epsilon}^2}{\\sigma_{\\beta}^2} I_d)^{-1}X^{\\top}y$. The final expression for $\\Delta(x^{\\ast})$ is therefore a function of the input $x^{\\ast}$, the data $(X, y)$, and the model hyperparameters $q, \\sigma_{\\epsilon}^2, \\sigma_{\\beta}^2$. The sum notation is explicit and clear for the final answer.\n\nFinal expression for the difference:\n$$\n\\Delta(x^{\\ast}) = q(1-q) \\sum_{i=1}^{d} (\\hat{\\beta}_i x_i^{\\ast})^2 - \\sigma_{\\epsilon}^2 (x^{\\ast})^{\\top} \\left(X^{\\top}X + \\frac{\\sigma_{\\epsilon}^2}{\\sigma_{\\beta}^2} I_d\\right)^{-1} x^{\\ast}\n$$\nwhere $\\hat{\\beta} = (X^{\\top}X + \\frac{\\sigma_{\\epsilon}^2}{\\sigma_{\\beta}^2} I_d)^{-1}X^{\\top}y$.",
            "answer": "$$\n\\boxed{q(1-q) \\sum_{i=1}^{d} (\\hat{\\beta}_i x_i^{\\ast})^2 - \\sigma_{\\epsilon}^2 (x^{\\ast})^{\\top} \\left(X^{\\top}X + \\frac{\\sigma_{\\epsilon}^2}{\\sigma_{\\beta}^2} I_d\\right)^{-1} x^{\\ast}}\n$$"
        },
        {
            "introduction": "The term 'dropout' can refer to several distinct schemes, most commonly dropping out activations or weights. This practice investigates the mathematical consequences of this choice, revealing that they are not equivalent . By calculating the bias in the predictive variance for both activation and weight dropout in a simple linear model, you will appreciate how implementation details can fundamentally alter the nature of the uncertainty approximation.",
            "id": "3321189",
            "problem": "Consider a linear-Gaussian regression model with two features. Let the output be generated by $y = \\mathbf{w}^{\\top}\\mathbf{x} + \\varepsilon$, where $\\mathbf{x} \\in \\mathbb{R}^{2}$, $\\mathbf{w} \\in \\mathbb{R}^{2}$, and the observation noise satisfies $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ with known variance $\\sigma^{2} > 0$. Suppose the posterior over the weights given data $D$ is Gaussian, $\\mathbf{w}\\mid D \\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$, with mean $\\mathbf{m} \\in \\mathbb{R}^{2}$ and positive definite covariance $\\mathbf{S} \\in \\mathbb{R}^{2\\times 2}$. The true posterior predictive distribution at covariate $\\mathbf{x}$ is therefore Gaussian with mean $\\mu_{\\mathrm{true}} = \\mathbf{m}^{\\top}\\mathbf{x}$ and variance $\\sigma_{\\mathrm{true}}^{2} = \\sigma^{2} + \\mathbf{x}^{\\top}\\mathbf{S}\\,\\mathbf{x}$.\n\nTo approximate $p(y\\mid \\mathbf{x}, D)$ using Monte Carlo (MC) dropout, consider two schemes that produce an approximate Gaussian predictive distribution by matching the first two moments of the random forward pass:\n\n1. Activation dropout (feature dropout) with dropout probability $p_{a} \\in (0,1)$, using inverted scaling: sample independent masks $z_{i} \\sim \\mathrm{Bernoulli}(1-p_{a})$ for $i \\in \\{1,2\\}$ and compute the stochastic prediction $y^{(a)} = \\mathbf{m}^{\\top}\\big((\\mathbf{z}/(1-p_{a})) \\odot \\mathbf{x}\\big) + \\varepsilon$, where $\\odot$ denotes elementwise multiplication and division by $(1-p_{a})$ is elementwise.\n\n2. Weight dropout with dropout probability $p_{w} \\in (0,1)$, using inverted scaling: sample independent masks $s_{i} \\sim \\mathrm{Bernoulli}(1-p_{w})$ for $i \\in \\{1,2\\}$ and compute the stochastic prediction $y^{(w)} = \\big((\\mathbf{s}/(1-p_{w})) \\odot \\mathbf{m}\\big)^{\\top}\\mathbf{x} + \\varepsilon$.\n\nFor each scheme, define its variance-based bias at $\\mathbf{x}$ for estimating $p(y\\mid \\mathbf{x}, D)$ as the difference between its approximate predictive variance (computed from the randomness in masks and noise) and the true posterior predictive variance, that is,\n$$\n\\mathrm{bias}_{a}(\\mathbf{x}) \\equiv \\mathrm{Var}\\!\\big(y^{(a)}\\mid \\mathbf{x}\\big) - \\big(\\sigma^{2} + \\mathbf{x}^{\\top}\\mathbf{S}\\,\\mathbf{x}\\big), \\quad \\mathrm{bias}_{w}(\\mathbf{x}) \\equiv \\mathrm{Var}\\!\\big(y^{(w)}\\mid \\mathbf{x}\\big) - \\big(\\sigma^{2} + \\mathbf{x}^{\\top}\\mathbf{S}\\,\\mathbf{x}\\big).\n$$\n\n(a) Derive closed-form expressions for $\\mathrm{bias}_{a}(\\mathbf{x})$ and $\\mathrm{bias}_{w}(\\mathbf{x})$ in terms of $\\mathbf{m}$, $\\mathbf{x}$, $p_{a}$, $p_{w}$, and $\\mathbf{S}$.\n\n(b) Evaluate these expressions at\n$$\n\\mathbf{x} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, \\quad \\mathbf{m} = \\begin{pmatrix} 1.5 \\\\ -0.5 \\end{pmatrix}, \\quad \\mathbf{S} = \\begin{pmatrix} 0.2 & 0.05 \\\\ 0.05 & 0.1 \\end{pmatrix}, \\quad \\sigma^{2} = 0.5, \\quad p_{a} = 0.2, \\quad p_{w} = 0.5.\n$$\n\nReport the single scalar quantity $\\Delta \\mathrm{bias} \\equiv \\mathrm{bias}_{a}(\\mathbf{x}) - \\mathrm{bias}_{w}(\\mathbf{x})$ as your final answer. Provide the exact value; no rounding is required. No units are needed.",
            "solution": "The problem requires the derivation and evaluation of the variance-based bias for two Monte Carlo dropout schemes in a linear-Gaussian regression model. We will first derive the general expressions for the bias in part (a), and then evaluate their difference for the specific numerical values given in part (b).\n\n### Part (a): Derivation of Bias Expressions\n\nThe true posterior predictive variance is given as $\\sigma_{\\mathrm{true}}^{2} = \\sigma^{2} + \\mathbf{x}^{\\top}\\mathbf{S}\\,\\mathbf{x}$. The bias for each scheme is the difference between its approximate predictive variance and this true variance. For any stochastic prediction of the form $y = Y + \\varepsilon$, where $Y$ is the stochastic part depending on the dropout masks and $\\varepsilon$ is the independent observation noise, the total variance is $\\mathrm{Var}(y) = \\mathrm{Var}(Y) + \\mathrm{Var}(\\varepsilon) = \\mathrm{Var}(Y) + \\sigma^2$.\n\nLet's analyze each scheme separately. We will use the law of total variance, which for a random variable $X$ can be written as $\\mathrm{Var}(X) = E[\\mathrm{Var}(X|Z)] + \\mathrm{Var}(E[X|Z])$. A more direct approach here is to use the definition $\\mathrm{Var}(X) = E[X^2] - (E[X])^2$.\n\n**1. Activation Dropout (Scheme 1)**\n\nThe stochastic prediction is $y^{(a)} = \\mathbf{m}^{\\top}\\big((\\mathbf{z}/(1-p_{a})) \\odot \\mathbf{x}\\big) + \\varepsilon$. Let $c_a = 1/(1-p_a)$. The stochastic term is $Y^{(a)} = \\mathbf{m}^{\\top}(c_a \\mathbf{z} \\odot \\mathbf{x}) = c_a \\sum_{i=1}^{2} m_i x_i z_i$. The random variables are $z_i \\sim \\mathrm{Bernoulli}(1-p_a)$, which are independent.\n\nFirst, we find the expectation of $Y^{(a)}$:\n$$\nE[Y^{(a)}] = E\\left[c_a \\sum_{i=1}^{2} m_i x_i z_i\\right] = c_a \\sum_{i=1}^{2} m_i x_i E[z_i]\n$$\nSince $E[z_i] = 1-p_a$, we have:\n$$\nE[Y^{(a)}] = \\frac{1}{1-p_a} \\sum_{i=1}^{2} m_i x_i (1-p_a) = \\sum_{i=1}^{2} m_i x_i = \\mathbf{m}^{\\top}\\mathbf{x}\n$$\nThe mean of the approximate predictive distribution matches the true posterior predictive mean, $\\mu_{\\mathrm{true}}$.\n\nNext, we find the variance of $Y^{(a)}$. We first compute the second moment $E[(Y^{(a)})^2]$:\n$$\nE[(Y^{(a)})^2] = E\\left[\\left(c_a \\sum_{i=1}^{2} m_i x_i z_i\\right)^2\\right] = c_a^2 E\\left[\\sum_{i=1}^{2}(m_i x_i z_i)^2 + \\sum_{i\\neq j} (m_i x_i z_i)(m_j x_j z_j)\\right]\n$$\nUsing linearity of expectation and the independence of $z_i$:\n$$\nE[(Y^{(a)})^2] = c_a^2 \\left(\\sum_{i=1}^{2} (m_i x_i)^2 E[z_i^2] + \\sum_{i\\neq j} (m_i x_i)(m_j x_j) E[z_i]E[z_j]\\right)\n$$\nFor a Bernoulli variable $z \\sim \\mathrm{Bernoulli}(q)$, $z^2 = z$, so $E[z^2]=E[z]=q$. Here $q=1-p_a$.\n$$\nE[(Y^{(a)})^2] = c_a^2 \\left((1-p_a)\\sum_{i=1}^{2} (m_i x_i)^2 + (1-p_a)^2\\sum_{i\\neq j} m_i x_i m_j x_j \\right)\n$$\nSubstituting $c_a = 1/(1-p_a)$:\n$$\nE[(Y^{(a)})^2] = \\frac{1}{1-p_a} \\sum_{i=1}^{2} (m_i x_i)^2 + \\sum_{i\\neq j} m_i x_i m_j x_j\n$$\nThe variance is $\\mathrm{Var}(Y^{(a)}) = E[(Y^{(a)})^2] - (E[Y^{(a)}])^2$:\n$$\n\\mathrm{Var}(Y^{(a)}) = \\left(\\frac{1}{1-p_a} \\sum_{i=1}^{2} (m_i x_i)^2 + \\sum_{i\\neq j} m_i x_i m_j x_j\\right) - \\left(\\sum_{i=1}^{2} m_i x_i\\right)^2\n$$\nSince $(\\sum_{i=1}^{2} m_i x_i)^2 = \\sum_{i=1}^{2} (m_i x_i)^2 + \\sum_{i\\neq j} m_i x_i m_j x_j$:\n$$\n\\mathrm{Var}(Y^{(a)}) = \\left(\\frac{1}{1-p_a} - 1\\right)\\sum_{i=1}^{2} (m_i x_i)^2 = \\frac{p_a}{1-p_a}\\sum_{i=1}^{2} (m_i x_i)^2\n$$\nThe total variance of the prediction $y^{(a)}$ is:\n$$\n\\mathrm{Var}(y^{(a)}) = \\mathrm{Var}(Y^{(a)}) + \\mathrm{Var}(\\varepsilon) = \\frac{p_a}{1-p_a}\\sum_{i=1}^{2} (m_i x_i)^2 + \\sigma^2\n$$\nThe bias for activation dropout is therefore:\n$$\n\\mathrm{bias}_{a}(\\mathbf{x}) = \\mathrm{Var}(y^{(a)}) - \\sigma^2_{\\mathrm{true}} = \\left(\\frac{p_a}{1-p_a}\\sum_{i=1}^{2} (m_i x_i)^2 + \\sigma^2\\right) - (\\sigma^2 + \\mathbf{x}^{\\top}\\mathbf{S}\\mathbf{x})\n$$\n$$\n\\mathrm{bias}_{a}(\\mathbf{x}) = \\frac{p_a}{1-p_a}\\sum_{i=1}^{2} (m_i x_i)^2 - \\mathbf{x}^{\\top}\\mathbf{S}\\mathbf{x}\n$$\n\n**2. Weight Dropout (Scheme 2)**\n\nThe stochastic prediction is $y^{(w)} = \\big((\\mathbf{s}/(1-p_{w})) \\odot \\mathbf{m}\\big)^{\\top}\\mathbf{x} + \\varepsilon$. Let $c_w = 1/(1-p_w)$. The stochastic term is $Y^{(w)} = (c_w \\mathbf{s} \\odot \\mathbf{m})^{\\top}\\mathbf{x} = c_w \\sum_{i=1}^{2} s_i m_i x_i$. The random variables are $s_i \\sim \\mathrm{Bernoulli}(1-p_w)$, which are independent.\nThis expression is structurally identical to the one for activation dropout, with $s_i$ replacing $z_i$ and $p_w$ replacing $p_a$. The derivation of the mean and variance is therefore analogous.\nThe mean is $E[Y^{(w)}] = \\mathbf{m}^{\\top}\\mathbf{x}$.\nThe variance is:\n$$\n\\mathrm{Var}(Y^{(w)}) = \\frac{p_w}{1-p_w}\\sum_{i=1}^{2} (m_i x_i)^2\n$$\nThe total variance of the prediction $y^{(w)}$ is:\n$$\n\\mathrm{Var}(y^{(w)}) = \\mathrm{Var(Y^{(w)})} + \\mathrm{Var}(\\varepsilon) = \\frac{p_w}{1-p_w}\\sum_{i=1}^{2} (m_i x_i)^2 + \\sigma^2\n$$\nThe bias for weight dropout is therefore:\n$$\n\\mathrm{bias}_{w}(\\mathbf{x}) = \\mathrm{Var}(y^{(w)}) - \\sigma^2_{\\mathrm{true}} = \\left(\\frac{p_w}{1-p_w}\\sum_{i=1}^{2} (m_i x_i)^2 + \\sigma^2\\right) - (\\sigma^2 + \\mathbf{x}^{\\top}\\mathbf{S}\\mathbf{x})\n$$\n$$\n\\mathrm{bias}_{w}(\\mathbf{x}) = \\frac{p_w}{1-p_w}\\sum_{i=1}^{2} (m_i x_i)^2 - \\mathbf{x}^{\\top}\\mathbf{S}\\mathbf{x}\n$$\n\n### Part (b): Evaluation\n\nWe are asked to compute $\\Delta \\mathrm{bias} = \\mathrm{bias}_{a}(\\mathbf{x}) - \\mathrm{bias}_{w}(\\mathbfx)$. Using the expressions derived in part (a):\n$$\n\\Delta \\mathrm{bias} = \\left(\\frac{p_a}{1-p_a}\\sum_{i=1}^{2} (m_i x_i)^2 - \\mathbf{x}^{\\top}\\mathbf{S}\\mathbf{x}\\right) - \\left(\\frac{p_w}{1-p_w}\\sum_{i=1}^{2} (m_i x_i)^2 - \\mathbf{x}^{\\top}\\mathbf{S}\\mathbf{x}\\right)\n$$\nThe term $\\mathbf{x}^{\\top}\\mathbf{S}\\mathbf{x}$ cancels out, simplifying the expression to:\n$$\n\\Delta \\mathrm{bias} = \\left(\\frac{p_a}{1-p_a} - \\frac{p_w}{1-p_w}\\right) \\sum_{i=1}^{2} (m_i x_i)^2\n$$\nNow, we substitute the given numerical values:\n$\\mathbf{x} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$, $\\mathbf{m} = \\begin{pmatrix} 1.5 \\\\ -0.5 \\end{pmatrix}$, $p_{a} = 0.2$, $p_{w} = 0.5$.\n\nFirst, we calculate the sum of squares term:\nThe vector $\\mathbf{m} \\odot \\mathbf{x}$ is:\n$$\n\\mathbf{m} \\odot \\mathbf{x} = \\begin{pmatrix} 1.5 \\times 2 \\\\ -0.5 \\times (-1) \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0.5 \\end{pmatrix}\n$$\nThe sum of squares is:\n$$\n\\sum_{i=1}^{2} (m_i x_i)^2 = 3^2 + (0.5)^2 = 9 + 0.25 = 9.25\n$$\n\nNext, we calculate the coefficient involving the dropout probabilities:\n$$\n\\frac{p_a}{1-p_a} = \\frac{0.2}{1-0.2} = \\frac{0.2}{0.8} = \\frac{1}{4} = 0.25\n$$\n$$\n\\frac{p_w}{1-p_w} = \\frac{0.5}{1-0.5} = \\frac{0.5}{0.5} = 1\n$$\nThe difference is:\n$$\n\\frac{p_a}{1-p_a} - \\frac{p_w}{1-p_w} = 0.25 - 1 = -0.75\n$$\n\nFinally, we compute $\\Delta \\mathrm{bias}$:\n$$\n\\Delta \\mathrm{bias} = (-0.75) \\times 9.25 = -\\frac{3}{4} \\times \\frac{37}{4} = -\\frac{111}{16}\n$$\nConverting the final fraction to a decimal gives:\n$$\n\\Delta \\mathrm{bias} = -6.9375\n$$\nThis is the exact value as required.",
            "answer": "$$\n\\boxed{-6.9375}\n$$"
        },
        {
            "introduction": "Applying Monte Carlo methods in practice always involves a trade-off between accuracy and computational cost. This exercise formalizes this trade-off for MC dropout by analyzing how the number of forward passes, $T$, affects the precision of our uncertainty estimate under a fixed computational budget . You will derive the optimal number of samples to minimize the error in the variance estimate, a crucial step in deploying these models efficiently.",
            "id": "3321171",
            "problem": "Consider a regression neural network that employs dropout at inference to perform Monte Carlo (MC) dropout for uncertainty quantification. For a fixed input $x$, each MC forward pass with dropout probability $p \\in (0,1)$ produces one predictive draw $Y_{t}(x)$, for $t=1,2,\\dots,T$. Assume the predictive draws are independent and identically distributed, approximately Gaussian with mean $\\mu(p)$ and variance $\\sigma^{2}(p)$, i.e., $Y_{t}(x) \\sim \\mathcal{N}(\\mu(p),\\sigma^{2}(p))$. The predictive variance is modeled as\n$$\n\\sigma^{2}(p) \\;=\\; \\sigma_{0}^{2} \\;+\\; \\kappa \\,\\frac{p}{1-p},\n$$\nwhere $\\sigma_{0}^{2} > 0$ represents the irreducible (aleatoric) noise and $\\kappa > 0$ captures the epistemic contribution induced by dropout. Let $\\bar{Y}(x) = \\frac{1}{T}\\sum_{t=1}^{T} Y_{t}(x)$ denote the sample mean, and consider the unbiased sample variance estimator\n$$\n\\hat{\\sigma}^{2}(x) \\;=\\; \\frac{1}{T-1} \\sum_{t=1}^{T} \\bigl(Y_{t}(x) - \\bar{Y}(x)\\bigr)^{2}.\n$$\nThe computational cost per forward pass is modeled as $c(p) = c_{f} + c_{a}\\,(1-p)$, with $c_{f} > 0$ accounting for fixed per-pass overhead and $c_{a} > 0$ scaling linearly with the fraction of active units $(1-p)$. You have a fixed computational budget $\\mathcal{B} > 0$ and must choose the number of MC passes $T$ subject to the budget constraint $T\\,c(p) \\le \\mathcal{B}$, with the simplifying assumption that $T$ can be treated as a continuous decision variable for optimization and that $\\mathcal{B} > c(p)$.\n\nStarting from the basic properties of the Gaussian distribution and the definition of the unbiased sample variance, derive an explicit expression for the mean squared error $\\mathrm{MSE}\\bigl(\\hat{\\sigma}^{2}(x)\\bigr)$ as a function of $T$ and $p$. Then, determine the optimal number of MC samples $T$ that minimizes this mean squared error under the budget constraint.\n\nYour final answer must be a single closed-form analytic expression for the optimal $T$ in terms of $\\mathcal{B}$, $c_{f}$, $c_{a}$, and $p$. No rounding is required.",
            "solution": "The problem asks for two main results: first, an explicit expression for the mean squared error $\\mathrm{MSE}\\bigl(\\hat{\\sigma}^{2}(x)\\bigr)$ of the sample variance estimator, and second, the optimal number of Monte Carlo samples $T$ that minimizes this MSE subject to a fixed computational budget.\n\nThe problem statement provides the following givens:\n- The Monte Carlo predictive draws $Y_{t}(x)$ for $t=1, 2, \\dots, T$ are independent and identically distributed (IID) from a Gaussian distribution, $Y_{t}(x) \\sim \\mathcal{N}(\\mu(p),\\sigma^{2}(p))$.\n- The sample variance estimator is $\\hat{\\sigma}^{2}(x) = \\frac{1}{T-1} \\sum_{t=1}^{T} \\bigl(Y_{t}(x) - \\bar{Y}(x)\\bigr)^{2}$, and it is stated to be an unbiased estimator of the true variance $\\sigma^2(p)$.\n- The computational cost per pass is $c(p) = c_{f} + c_{a}(1-p)$.\n- The total computational budget is $\\mathcal{B}$, leading to the constraint $T c(p) \\le \\mathcal{B}$.\n- The number of samples $T$ is treated as a continuous variable for optimization.\n\nFirst, we derive the expression for the Mean Squared Error (MSE) of the estimator $\\hat{\\sigma}^{2}(x)$. The MSE of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\mathrm{MSE}(\\hat{\\theta}) = \\mathrm{E}\\bigl[(\\hat{\\theta} - \\theta)^{2}\\bigr]$. This can be decomposed into the variance of the estimator and its squared bias:\n$$\n\\mathrm{MSE}(\\hat{\\theta}) = \\mathrm{Var}(\\hat{\\theta}) + \\bigl(\\mathrm{E}[\\hat{\\theta}] - \\theta\\bigr)^{2}\n$$\nIn our case, the estimator is $\\hat{\\sigma}^{2}(x)$ and the true parameter is $\\sigma^{2}(p)$. The problem states that $\\hat{\\sigma}^{2}(x)$ is an unbiased estimator. This implies that its expected value is equal to the true variance:\n$$\n\\mathrm{E}\\bigl[\\hat{\\sigma}^{2}(x)\\bigr] = \\sigma^{2}(p)\n$$\nTherefore, the bias term is zero, and the MSE is equal to the variance of the estimator:\n$$\n\\mathrm{MSE}\\bigl(\\hat{\\sigma}^{2}(x)\\bigr) = \\mathrm{Var}\\bigl(\\hat{\\sigma}^{2}(x)\\bigr)\n$$\nOur next task is to find the variance of the sample variance estimator, $\\mathrm{Var}\\bigl(\\hat{\\sigma}^{2}(x)\\bigr)$.\nFor a set of $T$ IID random variables $Y_1, \\dots, Y_T$ drawn from a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, a fundamental result from mathematical statistics (Cochran's theorem) states that the quantity $\\frac{(T-1)\\hat{\\sigma}^2}{\\sigma^2}$ follows a chi-squared ($\\chi^2$) distribution with $T-1$ degrees of freedom.\n$$\n\\frac{(T-1)\\hat{\\sigma}^{2}(x)}{\\sigma^{2}(p)} \\sim \\chi^{2}_{T-1}\n$$\nThe variance of a chi-squared distribution with $k$ degrees of freedom is $2k$. In our case, $k = T-1$, so the variance is:\n$$\n\\mathrm{Var}\\left(\\frac{(T-1)\\hat{\\sigma}^{2}(x)}{\\sigma^{2}(p)}\\right) = 2(T-1)\n$$\nWe can use the property of variance, $\\mathrm{Var}(aX) = a^2\\mathrm{Var}(X)$ for a constant $a$, to relate this to $\\mathrm{Var}\\bigl(\\hat{\\sigma}^{2}(x)\\bigr)$. Here, the constant scaling factor is $a = \\frac{T-1}{\\sigma^{2}(p)}$.\n$$\n\\mathrm{Var}\\left(\\frac{(T-1)\\hat{\\sigma}^{2}(x)}{\\sigma^{2}(p)}\\right) = \\left(\\frac{T-1}{\\sigma^{2}(p)}\\right)^{2} \\mathrm{Var}\\bigl(\\hat{\\sigma}^{2}(x)\\bigr)\n$$\nEquating the two expressions for the variance, we get:\n$$\n\\left(\\frac{T-1}{\\sigma^{2}(p)}\\right)^{2} \\mathrm{Var}\\bigl(\\hat{\\sigma}^{2}(x)\\bigr) = 2(T-1)\n$$\nSolving for $\\mathrm{Var}\\bigl(\\hat{\\sigma}^{2}(x)\\bigr)$:\n$$\n\\mathrm{Var}\\bigl(\\hat{\\sigma}^{2}(x)\\bigr) = \\frac{2(T-1)}{\\left(\\frac{(T-1)^2}{(\\sigma^{2}(p))^2}\\right)} = \\frac{2(T-1)(\\sigma^{2}(p))^2}{(T-1)^2} = \\frac{2\\bigl(\\sigma^{2}(p)\\bigr)^{2}}{T-1}\n$$\nThis gives the explicit expression for the MSE as a function of $T$ and $p$:\n$$\n\\mathrm{MSE}\\bigl(\\hat{\\sigma}^{2}(x)\\bigr) = \\frac{2\\bigl(\\sigma^{2}(p)\\bigr)^{2}}{T-1}\n$$\nNote that $\\sigma^2(p)$ is itself a function of $p$, as given by $\\sigma^{2}(p) = \\sigma_{0}^{2} + \\kappa \\frac{p}{1-p}$.\n\nNext, we need to find the optimal number of MC samples, $T$, that minimizes this MSE, subject to the computational budget constraint $T c(p) \\le \\mathcal{B}$. The optimization problem is:\n$$\n\\text{minimize} \\quad \\frac{2\\bigl(\\sigma^{2}(p)\\bigr)^{2}}{T-1} \\quad \\text{with respect to } T\n$$\n$$\n\\text{subject to} \\quad T c(p) \\le \\mathcal{B}\n$$\nFor a fixed dropout probability $p$, the term $2\\bigl(\\sigma^{2}(p)\\bigr)^{2}$ is a positive constant. Therefore, minimizing the MSE expression is equivalent to minimizing the term $\\frac{1}{T-1}$. Minimizing $\\frac{1}{T-1}$ is equivalent to maximizing its denominator, $T-1$, which in turn is equivalent to maximizing $T$.\n\nFrom the budget constraint, we can find the upper bound for $T$:\n$$\nT \\le \\frac{\\mathcal{B}}{c(p)}\n$$\nTo maximize $T$, we must choose its largest possible value, which is the upper bound defined by the budget constraint. Since $T$ is treated as a continuous variable, the optimal value $T_{opt}$ is achieved when the inequality becomes an equality:\n$$\nT_{opt} = \\frac{\\mathcal{B}}{c(p)}\n$$\nFinally, we substitute the given expression for the cost per pass, $c(p) = c_{f} + c_{a}(1-p)$, into the expression for $T_{opt}$:\n$$\nT_{opt} = \\frac{\\mathcal{B}}{c_{f} + c_{a}(1-p)}\n$$\nThis is the closed-form analytic expression for the optimal number of MC samples $T$ in terms of the given parameters.",
            "answer": "$$\n\\boxed{\\frac{\\mathcal{B}}{c_{f} + c_{a}(1-p)}}\n$$"
        }
    ]
}