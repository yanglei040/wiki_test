{
    "hands_on_practices": [
        {
            "introduction": "高斯过程回归虽然理论上很优雅，但在实践中常常会遇到数值不稳定的问题，尤其是在使用某些核函数或处理聚集的数据点时。本练习将引导你诊断和解决这些常见的数值病态问题，这是高斯过程从业者必须掌握的一项关键技能。通过分析格拉姆矩阵的条件数，你将学会如何识别不稳定性，并探索如增加噪声抖动或使用低秩近似等有效的补救措施。",
            "id": "3309568",
            "problem": "考虑一个具有零均值先验和在输入 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$ 上的正定协方差函数 $k(x,x')$ 的高斯过程回归。观测值被建模为 $y_i = f(x_i) + \\epsilon_i$，其中 $f \\sim \\mathcal{GP}(0,k)$ 且噪声 $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$ 是独立的高斯噪声。格拉姆矩阵是 $K \\in \\mathbb{R}^{n \\times n}$，其元素为 $K_{ij} = k(x_i,x_j)$，数据格拉姆矩阵是 $K_y = K + \\sigma_n^2 I_n$。对于对称正定矩阵，2-范数下的谱条件数定义为 $\\kappa_2(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$，其中 $\\lambda_{\\max}(A)$ 和 $\\lambda_{\\min}(A)$ 分别表示 $A$ 的最大和最小特征值。假设双精度机器精度满足 $\\varepsilon_{\\mathrm{mach}} \\approx 2.22 \\times 10^{-16}$。\n\n假设 $n = 300$ 个输入是高度聚集的（例如，几个紧密的簇被中等大小的间隙分开），并且协方差函数 $k$ 是平方指数核 $k(x,x') = \\sigma_f^2 \\exp\\!\\big(-\\|x-x'\\|^2/(2\\ell^2)\\big)$，其中 $\\sigma_f^2 = 1$ 且长度尺度 $\\ell$ 很短。一位从业者计算了 $K$ 的极端特征值，发现 $\\lambda_{\\max}(K) \\approx 1.2$ 和 $\\lambda_{\\min}(K) \\approx 10^{-12}$，而当前的噪声水平是 $\\sigma_n^2 \\approx 10^{-14}$。该从业者希望诊断并纠正影响下游随机模拟的数值病态问题，例如通过 Cholesky 分解从后验 $\\mathcal{N}(m, K_y)$ 中采样，或通过马尔可夫链蒙特卡洛（MCMC）探索超参数。\n\n下列哪些陈述是正确的？选择所有适用的选项。\n\nA. 给定所述的特征值和 $\\sigma_n^2 \\approx 10^{-14}$，数据格拉姆矩阵 $K_y$ 的谱条件数 $\\kappa_2(K_y)$ 约为 $10^{12}$ 的量级，这在双精度下足以导致随机模拟中使用的分解算法出现精度损失和不稳定性。\n\nB. 通过添加抖动 $\\delta I_n$ 来增加噪声水平（等效于将 $\\sigma_n^2$ 替换为 $\\sigma_n^2 + \\delta$），会使 $\\kappa_2(K_y)$ 作为 $\\delta$ 的函数严格递减。在给定的特征值下，实现 $\\kappa_2(K_y) \\leq 10^8$ 的最小 $\\delta$ 约等于 $1.2 \\times 10^{-8}$。\n\nC. 通过将 $k(x,x')$ 与一个紧支集函数相乘来进行核函数锥削（该函数在 $x=x'$ 时等于 $1$，但在其他情况下小于1），总是会减小 $\\kappa_2(K_y)$，并保持训练输入处的预测均值不变。\n\nD. 将 $K$ 替换为低秩近似 $U U^\\top$（其中 $U \\in \\mathbb{R}^{n \\times m}$ 且 $m \\ll n$，例如通过 Nyström 方法或主元 Cholesky 分解），然后使用 $U U^\\top + \\sigma_n^2 I_n$，可以改善条件数；一个有原则的截断规则是监控被舍弃的谱，并选择 $m$ 使得被忽略的特征值之和低于与 $\\varepsilon_{\\mathrm{mach}}$ 乘以 $\\operatorname{trace}(K)$ 成正比的容差。\n\nE. 对于平方指数核，在保持 $\\ell$ 固定的情况下，通过 $x_i \\mapsto a x_i$ 来重缩放输入，对于任何 $a > 0$ 都会保持 $K_y$ 的谱条件数不变。\n\nF. 在用于超参数推断的 MCMC 中，一个病态的 $K_y$ 通常会改善混合效果，因为对数边缘似然在近奇异配置附近变得更平坦，从而减少了链中的自相关。",
            "solution": "首先验证问题陈述以确保其在科学上是合理的、适定的和完整的。\n\n### 步骤1：提取已知条件\n- **模型**：高斯过程回归。\n- **先验**：零均值，$f \\sim \\mathcal{GP}(0,k)$。\n- **协方差函数**：$k(x,x')$，正定。\n- **输入**：$\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$，其中 $n=300$。输入是“高度聚集的”。\n- **观测模型**：$y_i = f(x_i) + \\epsilon_i$。\n- **噪声**：$\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$，独立的。\n- **格拉姆矩阵**：$K \\in \\mathbb{R}^{n \\times n}$，其中 $K_{ij} = k(x_i,x_j)$。\n- **数据格拉姆矩阵**：$K_y = K + \\sigma_n^2 I_n$。\n- **条件数**：对于对称正定矩阵 $A$，$\\kappa_2(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$。\n- **机器精度**：$\\varepsilon_{\\mathrm{mach}} \\approx 2.22 \\times 10^{-16}$。\n- **特定核函数**：平方指数核，$k(x,x') = \\sigma_f^2 \\exp\\!\\big(-\\|x-x'\\|^2/(2\\ell^2)\\big)$。\n- **超参数**：$\\sigma_f^2 = 1$，短长度尺度 $\\ell$。\n- **$K$ 的特征值**：$\\lambda_{\\max}(K) \\approx 1.2$，$\\lambda_{\\min}(K) \\approx 10^{-12}$。\n- **噪声方差**：$\\sigma_n^2 \\approx 10^{-14}$。\n\n### 步骤2：使用提取的已知条件进行验证\n问题陈述根植于高斯过程回归和数值线性代数的标准数学框架。所描述的场景——由聚集的数据点和平方指数核产生的近奇异格拉姆矩阵——是高斯过程实际应用中一个常见且现实的问题。提供的关于特征值和噪声的数值是合理的，并与此场景一致。问题陈述没有科学或事实上的不合理之处，没有矛盾，并且是适定的。术语是标准且明确的。问题是有效的。\n\n### 步骤3：结论与行动\n问题有效。我们开始解答。\n\n---\n\n### 对有效问题陈述的分析\n\n问题的核心在于数据格拉姆矩阵 $K_y = K + \\sigma_n^2 I_n$ 的性质。由于加上单位矩阵的倍数只是将谱进行平移，因此 $K_y$ 的特征值是 $\\lambda_i(K_y) = \\lambda_i(K) + \\sigma_n^2$。因此，谱条件数为：\n$$ \\kappa_2(K_y) = \\frac{\\lambda_{\\max}(K_y)}{\\lambda_{\\min}(K_y)} = \\frac{\\lambda_{\\max}(K) + \\sigma_n^2}{\\lambda_{\\min}(K) + \\sigma_n^2} $$\n我们已知 $\\lambda_{\\max}(K) \\approx 1.2$，$\\lambda_{\\min}(K) \\approx 10^{-12}$，以及 $\\sigma_n^2 \\approx 10^{-14}$。\n\n### 逐项分析\n\n**A. 给定所述的特征值和 $\\sigma_n^2 \\approx 10^{-14}$，数据格拉姆矩阵 $K_y$ 的谱条件数 $\\kappa_2(K_y)$ 约为 $10^{12}$ 的量级，这在双精度下足以导致随机模拟中使用的分解算法出现精度损失和不稳定性。**\n\n让我们用给定的值计算条件数：\n$$ \\lambda_{\\max}(K_y) = \\lambda_{\\max}(K) + \\sigma_n^2 \\approx 1.2 + 10^{-14} \\approx 1.2 $$\n$$ \\lambda_{\\min}(K_y) = \\lambda_{\\min}(K) + \\sigma_n^2 \\approx 10^{-12} + 10^{-14} = 10^{-12} + 0.01 \\times 10^{-12} = 1.01 \\times 10^{-12} $$\n条件数为：\n$$ \\kappa_2(K_y) = \\frac{\\lambda_{\\max}(K_y)}{\\lambda_{\\min}(K_y)} \\approx \\frac{1.2}{1.01 \\times 10^{-12}} \\approx 1.19 \\times 10^{12} $$\n这个值的量级是 $10^{12}$。在双精度算术中，$\\varepsilon_{\\mathrm{mach}} \\approx 10^{-16}$，一个通用的经验法则是，在求解线性系统时，预期会损失大约 $\\log_{10}(\\kappa_2)$ 个有效数字的精度。这里，$\\log_{10}(1.19 \\times 10^{12}) \\approx 12.08$。在约有 16 个可用精度数字的情况下，这只剩下约 $16 - 12 = 4$ 个有效数字，构成了严重的精度损失。这种程度的病态使得像 Cholesky 分解这样的标准算法在数值上变得不稳定。该陈述是准确的。\n\n**结论：正确**\n\n**B. 通过添加抖动 $\\delta I_n$ 来增加噪声水平（等效于将 $\\sigma_n^2$ 替换为 $\\sigma_n^2 + \\delta$），会使 $\\kappa_2(K_y)$ 作为 $\\delta$ 的函数严格递减。在给定的特征值下，实现 $\\kappa_2(K_y) \\leq 10^8$ 的最小 $\\delta$ 约等于 $1.2 \\times 10^{-8}$。**\n\n让新的条件数成为所加抖动 $\\delta > 0$ 的函数：\n$$ \\kappa(\\delta) = \\frac{\\lambda_{\\max}(K) + \\sigma_n^2 + \\delta}{\\lambda_{\\min}(K) + \\sigma_n^2 + \\delta} $$\n为了检查该函数是否严格递减，我们考察它关于 $\\delta$ 的导数。令 $a = \\lambda_{\\max}(K) + \\sigma_n^2$ 且 $b = \\lambda_{\\min}(K) + \\sigma_n^2$。\n$$ \\frac{d\\kappa}{d\\delta} = \\frac{d}{d\\delta}\\left(\\frac{a + \\delta}{b + \\delta}\\right) = \\frac{1(b+\\delta) - 1(a+\\delta)}{(b+\\delta)^2} = \\frac{b-a}{(b+\\delta)^2} $$\n由于 $K$ 是正定矩阵且不是单位矩阵的标量倍，$\\lambda_{\\max}(K) > \\lambda_{\\min}(K)$，这意味着 $a > b$。因此，$b-a < 0$，对于所有 $\\delta \\ge 0$，导数严格为负。所以，$\\kappa(\\delta)$ 是 $\\delta$ 的严格递减函数。\n\n现在，我们寻找实现 $\\kappa_2(K_y) \\le 10^8$ 的最小 $\\delta$。由于函数是递减的，这发生在边界处：\n$$ \\frac{\\lambda_{\\max}(K) + \\sigma_n^2 + \\delta}{\\lambda_{\\min}(K) + \\sigma_n^2 + \\delta} = 10^8 $$\n使用给定的值：\n$$ \\frac{1.2 + 10^{-14} + \\delta}{10^{-12} + 10^{-14} + \\delta} = 10^8 $$\n$$ 1.2 + 10^{-14} + \\delta = 10^8 (1.01 \\times 10^{-12} + \\delta) = 1.01 \\times 10^{-4} + 10^8 \\delta $$\n$$ 1.2 - 1.01 \\times 10^{-4} \\approx (10^8 - 1) \\delta $$\n$$ \\delta \\approx \\frac{1.2}{10^8} = 1.2 \\times 10^{-8} $$\n计算证实了这个近似值。该陈述的两部分都是准确的。\n\n**结论：正确**\n\n**C. 通过将 $k(x,x')$ 与一个紧支集函数相乘来进行核函数锥削（该函数在 $x=x'$ 时等于 $1$，但在其他情况下小于1），总是会减小 $\\kappa_2(K_y)$，并保持训练输入处的预测均值不变。**\n\n这个陈述提出了两个主张。让我们先分析第二个：锥削“保持训练输入处的预测均值不变”。在训练输入 $\\{x_i\\}_{i=1}^n$ 处的预测均值由向量 $\\mathbf{m}_{\\text{train}} = K (K_y)^{-1} \\mathbf{y} = K (K + \\sigma_n^2 I_n)^{-1} \\mathbf{y}$ 给出。这可以重写为 $\\mathbf{m}_{\\text{train}} = (I_n - \\sigma_n^2 (K + \\sigma_n^2 I_n)^{-1})\\mathbf{y}$。锥削将核矩阵 $K$ 替换为一个新矩阵 $K'$，其中 $K'_{ij} = K_{ij} T_{ij}$，对于一个锥削矩阵 $T$，$T_{ii}=1$ 且对于 $i \\ne j$ 有 $|T_{ij}| \\le 1$。由于 $T$ 不是全 1 矩阵，$K' \\neq K$。锥削后的预测均值为 $\\mathbf{m}'_{\\text{train}} = K' (K' + \\sigma_n^2 I_n)^{-1} \\mathbf{y}$。由于均值的整个表达式依赖于完整的矩阵 $K$，将其更改为 $K'$ 通常会改变预测均值。预测均值不会保持不变。由于陈述的一部分是错误的，整个陈述都是不正确的。\n\n**结论：不正确**\n\n**D. 将 $K$ 替换为低秩近似 $U U^\\top$（其中 $U \\in \\mathbb{R}^{n \\times m}$ 且 $m \\ll n$，例如通过 Nyström 方法或主元 Cholesky 分解），然后使用 $U U^\\top + \\sigma_n^2 I_n$，可以改善条件数；一个有原则的截断规则是监控被舍弃的谱，并选择 $m$ 使得被忽略的特征值之和低于与 $\\varepsilon_{\\mathrm{mach}}$ 乘以 $\\operatorname{trace}(K)$ 成正比的容差。**\n\n基于低秩方法的高斯过程近似是提高可扩展性和数值稳定性的标准技术。$n \\times n$ 矩阵 $K_y = UU^\\top + \\sigma_n^2 I_n$ 的求逆可以利用 Woodbury 矩阵恒等式高效进行：\n$$ (UU^\\top + \\sigma_n^2 I_n)^{-1} = \\frac{1}{\\sigma_n^2}I_n - \\frac{1}{\\sigma_n^2} U \\left( \\sigma_n^2 I_m + U^\\top U \\right)^{-1} U^\\top $$\n这需要对一个 $m \\times m$ 矩阵 $\\sigma_n^2 I_m + U^\\top U$ 求逆。在像 Nyström 这样的方法中，$U^\\top U$ 的特征值对应于 $K$ 的最大特征值。参与 $m \\times m$ 求逆的最小特征值将远大于 $\\lambda_{\\min}(K)$，从而导致一个条件好得多的矩阵求逆问题。因此，这些方法通过用一个条件良好的 $m \\times m$ 问题替换病态的 $n \\times n$ 问题来改善条件数。\n\n所提出的截断规则是选择秩 $m$，使得被舍弃的特征值之和 $\\sum_{i=m+1}^n \\lambda_i(K)$相对于总特征值之和 $\\operatorname{trace}(K) = \\sum_{i=1}^n \\lambda_i(K)$ 较小。将此相对误差的容差设置为机器精度的量级，即误差 $\\le C \\cdot \\varepsilon_{\\mathrm{mach}} \\operatorname{trace}(K)$，是数值线性代数中用于创建精确到机器精度的近似的一个标准且有原则的启发式方法。该陈述是准确的。\n\n**结论：正确**\n\n**E. 对于平方指数核，在保持 $\\ell$ 固定的情况下，通过 $x_i \\mapsto a x_i$ 来重缩放输入，对于任何 $a > 0$ 都会保持 $K_y$ 的谱条件数不变。**\n\n设原始核函数为 $k(x,x') = \\sigma_f^2 \\exp(-\\|x-x'\\|^2 / (2\\ell^2))$。将输入重缩放为 $z_i = a x_i$ 后，新的核矩阵 $K'$ 的元素为：\n$$ K'_{ij} = k(z_i, z_j) = \\sigma_f^2 \\exp(-\\|z_i-z_j\\|^2 / (2\\ell^2)) = \\sigma_f^2 \\exp(-\\|a(x_i-x_j)\\|^2 / (2\\ell^2)) = \\sigma_f^2 \\exp(-a^2\\|x_i-x_j\\|^2 / (2\\ell^2)) $$\n这等价于对原始输入使用一个新的长度尺度 $\\ell' = \\ell/a$。格拉姆矩阵 $K$ 对长度尺度参数高度敏感。例如，当 $\\ell \\to \\infty$ 时，$K \\to \\sigma_f^2 \\mathbf{1}\\mathbf{1}^\\top$（一个秩为 1 的矩阵），其条件数非常差。当 $\\ell \\to 0$ 时，$K \\to \\sigma_f^2 I_n$，其条件数是完美的（$\\kappa_2(K)=1$）。由于重缩放输入等价于改变长度尺度，而 $K$ 的谱（以及 $\\kappa_2(K)$ 和 $\\kappa_2(K_y)$）强烈依赖于长度尺度，所以条件数不会保持不变。\n\n**结论：不正确**\n\n**F. 在用于超参数推断的 MCMC 中，一个病态的 $K_y$ 通常会改善混合效果，因为对数边缘似然在近奇异配置附近变得更平坦，从而减少了链中的自相关。**\n\nGP 回归的对数边缘似然（LML）由 $\\log p(\\mathbf{y}|\\theta) = -\\frac{1}{2}\\mathbf{y}^\\top K_y^{-1} \\mathbf{y} - \\frac{1}{2}\\log|K_y| - \\frac{n}{2}\\log(2\\pi)$ 给出。当 $K_y$ 变得病态时，其最小特征值趋近于零，导致其行列式 $|K_y| = \\prod_i \\lambda_i(K_y)$ 趋近于零。因此，$\\log|K_y|$ 趋近于 $-\\infty$。这会在超参数 $\\theta$ 的空间中，于 LML 曲面上产生极陡的梯度和尖锐、狭窄的谷或脊。这种拓扑结构对于 MCMC 采样器来说是出了名的难以探索。采样器倾向于卡在这些区域，导致混合非常缓慢和链中的高自相关。LML 曲面变得更加崎岖和“尖锐”，而不是更平坦。因此，病态严重阻碍了 MCMC 的性能，而不是改善它。\n\n**结论：不正确**",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "标准高斯过程回归的 $O(n^3)$ 计算复杂度是其应用于大规模数据集的主要障碍。然而，当数据具有特定结构时（例如一维均匀网格上的数据），我们可以利用这种结构来大幅提升计算效率。本练习将探讨平稳核函数与规则网格的结合如何产生具有托普利茨（Toeplitz）结构的协方差矩阵，从而能够利用快速傅里叶变换（FFT）实现 $O(n \\log n)$ 级别的快速计算。",
            "id": "3309545",
            "problem": "考虑一个一维输入位置网格 $x_i = i\\Delta$，其中 $i = 0, 1, \\ldots, n-1$ 且 $\\Delta > 0$ 是固定的。令 $k(x, x') = \\kappa(|x - x'|)$ 为一个平稳协方差函数，并通过 $K_{ij} = k(x_i, x_j)$ 定义协方差矩阵 $K \\in \\mathbb{R}^{n \\times n}$。假设希望计算涉及线性系统 $(K + \\sigma^2 I)\\alpha = y$ 的高斯过程回归量（对于给定的向量 $y \\in \\mathbb{R}^n$ 和噪声水平 $\\sigma^2 > 0$），并近似计算 $\\log\\det(K + \\sigma^2 I)$，而不进行稠密矩阵分解。\n\n根据基本原理，例如平稳核的平移不变性定义、托普利兹(Toeplitz)矩阵和循环(circulant)矩阵的定义、离散卷积的性质，以及离散傅里叶变换(DFT)可以对角化循环矩阵这一事实，推断下列哪些陈述是正确的。\n\nA. 因为 $k(x, x')$ 仅取决于 $|x - x'|$ 并且输入位于一个均匀间隔的网格上，所以矩阵 $K$ 具有托普利兹(Toeplitz)结构，这意味着 $K_{ij}$ 仅取决于 $|i-j|$，因此同一对角线上的所有元素都相等。\n\nB. 对于任何 $n$，离散傅里叶变换(DFT)矩阵 $F \\in \\mathbb{C}^{n \\times n}$ 都能精确地对角化托普利兹(Toeplitz)矩阵 $K$，即 $F^* K F$ 是对角矩阵，因此 $K$ 可以通过一次正向和一次反向的快速傅里叶变换(FFT)来求逆。\n\nC. 矩阵向量积 $Kv$ 可以通过快速傅里叶变换(FFT)在 $O(n\\log n)$ 时间内实现，方法是将 $K$ 嵌入到一个更大的循环矩阵 $C \\in \\mathbb{R}^{m \\times m}$ 中，将 $v$ 零填充至长度 $m$，并计算相关的循环卷积；截取前 $n$ 个元素即可得到与 $K$ 的期望乘积。\n\nD. 线性系统 $(K + \\sigma^2 I)\\alpha = y$ 可以通过使用基于FFT的 $K$ 的对角化方法，在 $O(n\\log n)$ 时间内被精确求解，且这与 $n$ 和 $\\kappa$ 的选择无关。\n\nE. 使用共轭梯度法(CG)并结合经FFT加速的与 $K$ 的矩阵向量积，求解 $(K + \\sigma^2 I)\\alpha = y$ 的每次迭代成本为 $O(n\\log n)$，并且使用一个有效的预条件子，总成本可以近似按 $O(n\\log n)$ 缩放，最多相差一个取决于条件数的因子。\n\nF. 如果 $C$ 是由协方差的周期化版本构造的 $K$ 的循环嵌入，那么 $C$ 的特征值由 $C$ 的第一列的DFT给出，并且 $\\log\\det(K + \\sigma^2 I)$ 可由 $\\sum_{j=0}^{m-1} \\log\\big(\\lambda_j(C) + \\sigma^2\\big)$ 近似；当嵌入反映了一个有效的周期性协方差且嵌入大小 $m$ 足够大时，这个近似是准确的，但对于非周期性协方差，它通常是有偏的。\n\nG. 像Hutchinson方法这样的随机迹估计器，使用随机Rademacher向量 $z$ 时满足 $\\mathbb{E}\\left[z^{\\top}\\log(K + \\sigma^2 I)z\\right] = \\operatorname{trace}\\big(\\log(K + \\sigma^2 I)\\big)$，并且与Lanczos求积以及基于FFT的与 $K$ 的矩阵向量积相结合，它为近似计算 $\\log\\det(K + \\sigma^2 I)$ 提供了一种每次迭代成本为 $O(n\\log n)$ 的蒙特卡洛方法（最多相差探测向量数量和Lanczos步数的因子）。",
            "solution": "该问题要求评估关于在一维均匀间隔网格上进行高斯过程回归的计算方法的几个陈述。问题的核心在于由平稳核导出的协方差矩阵 $K$ 的结构，以及如何利用这种结构进行高效计算。\n\n给定的输入位置是 $x_i = i\\Delta$，其中 $i = 0, 1, \\ldots, n-1$。协方差函数是平稳的，$k(x, x') = \\kappa(|x - x'|)$。协方差矩阵的元素为 $K_{ij} = k(x_i, x_j)$。我们必须分析 $(K + \\sigma^2 I)\\alpha = y$ 的解以及 $\\log\\det(K + \\sigma^2 I)$ 的近似。\n\n让我们从基本原理出发评估每个陈述。\n\n**A. 因为 $k(x, x')$ 仅取决于 $|x - x'|$ 并且输入位于一个均匀间隔的网格上，所以矩阵 $K$ 具有托普利兹(Toeplitz)结构，这意味着 $K_{ij}$ 仅取决于 $|i-j|$，因此同一对角线上的所有元素都相等。**\n\n矩阵 $K$ 的元素由 $K_{ij} = k(x_i, x_j)$ 给出。代入网格定义，我们有：\n$$K_{ij} = \\kappa(|x_i - x_j|) = \\kappa(|i\\Delta - j\\Delta|) = \\kappa(|i-j|\\Delta)$$\n如果一个矩阵 $T$ 的元素满足 $T_{ij} = t_{i-j}$（对于某个序列 $\\{t_k\\}$），那么它就是一个托普利兹矩阵。这意味着沿任一对角线的元素都是恒定的。对角线由恒定的索引差 $j-i = d$ 定义。\n对于我们的矩阵 $K$，考虑由 $j-i = d$ 定义的对角线。该对角线上的一个元素是 $K_{i, i+d}$。其值为：\n$$K_{i, i+d} = \\kappa(|i - (i+d)|\\Delta) = \\kappa(|-d|\\Delta) = \\kappa(|d|\\Delta)$$\n由于这个值仅取决于对角线索引 $d$ 而不取决于具体的行索引 $i$，因此同一对角线上的所有元素确实相等。所以，$K$ 是一个托普利兹矩阵。此外，由于 $K_{ij} = \\kappa(|i-j|\\Delta)$ 且 $K_{ji} = \\kappa(|j-i|\\Delta) = \\kappa(|i-j|\\Delta)$，我们有 $K_{ij} = K_{ji}$，所以 $K$ 是一个对称托普利兹矩阵。根据我们的推导，$K_{ij}$ 仅取决于 $|i-j|$ 的陈述是正确的，这直接意味着所述的托普利兹结构。\n\n**结论：** 正确。\n\n**B. 对于任何 $n$，离散傅里叶变换(DFT)矩阵 $F \\in \\mathbb{C}^{n \\times n}$ 都能精确地对角化托普利兹(Toeplitz)矩阵 $K$，即 $F^* K F$ 是对角矩阵，因此 $K$ 可以通过一次正向和一次反向的快速傅里叶变换(FFT)来求逆。**\n\n众所周知，离散傅里叶变换(DFT)矩阵 $F$ 可以对角化所有循环矩阵。如果一个矩阵 $C \\in \\mathbb{R}^{n \\times n}$ 的行是第一行的循环移位，即 $C_{ij} = c_{(j-i) \\pmod n}$，那么它就是循环矩阵。通常情况下，托普利兹矩阵不是循环矩阵。\n要使我们的托普利兹矩阵 $K$ 是循环矩阵，我们需要 $K_{ij}$ 是 $(j-i) \\pmod n$ 的函数。我们来检验一下。我们有 $K_{ij} = \\kappa(|i-j|\\Delta)$。考虑元素 $K_{0, n-1}$。对于一个循环矩阵，这个元素应该等于 $K_{1,0}$。\n$$K_{0, n-1} = \\kappa(|0 - (n-1)|\\Delta) = \\kappa((n-1)\\Delta)$$\n$$K_{1,0} = \\kappa(|1-0|\\Delta) = \\kappa(\\Delta)$$\n要使 $K$ 成为循环矩阵，我们需要 $\\kappa((n-1)\\Delta) = \\kappa(\\Delta)$。这对于一般的平稳协方差函数（例如，平方指数核 $\\kappa(r) = \\exp(-r^2)$）是不成立的。\n由于 $K$ 是一个通用的托普利兹矩阵，而不一定是循环矩阵，所以DFT矩阵 $F$ 不能对角化它。声称 $F^* K F$ 是对角矩阵的说法是错误的。\n\n**结论：** 不正确。\n\n**C. 矩阵向量积 $Kv$ 可以通过快速傅里叶变换(FFT)在 $O(n\\log n)$ 时间内实现，方法是将 $K$ 嵌入到一个更大的循环矩阵 $C \\in \\mathbb{R}^{m \\times m}$ 中，将 $v$ 零填充至长度 $m$，并计算相关的循环卷积；截取前 $n$ 个元素即可得到与 $K$ 的期望乘积。**\n\n矩阵向量积 $(Kv)_i = \\sum_{j=0}^{n-1} K_{ij} v_j$。由于 $K$ 是托普利兹矩阵，我们可以写成 $K_{ij} = t_{i-j}$，其中 $t_k = \\kappa(|k|\\Delta)$。因此，$(Kv)_i = \\sum_{j=0}^{n-1} t_{i-j} v_j$。这是线性卷积的数学形式。\n卷积定理指出，两个序列的卷积可以通过在傅里叶域中进行逐元素乘法来计算。具体来说，*循环*卷积 $a *_{circ} b$ 可以计算为 $\\text{IDFT}(\\text{DFT}(a) \\odot \\text{DFT}(b))$，其中 $\\odot$ 是逐元素乘法。\n要使用该定理计算*线性*卷积，必须防止循环卷积的“环绕”效应。这可以通过对输入向量进行零填充来实现。如果我们想将一个长度为 $n$ 的序列与一个长度为 $n$ 的序列进行卷积，结果的长度为 $2n-1$。因此，我们必须将两个向量都填充到长度 $m \\ge 2n-1$。\n陈述中描述的过程是标准算法：\n1.  从托普利兹矩阵 $K$ 的第一列定义一个大小为 $m \\times m$（其中 $m \\ge 2n-1$）的循环矩阵 $C$。令 $c$ 为 $C$ 的第一列。\n2.  将向量 $v$ 零填充至长度 $m$，得到 $v_{pad}$。\n3.  通过FFT计算循环卷积：$w = \\text{iFFT}(\\text{FFT}(c) \\odot \\text{FFT}(v_{pad}))$。\n4.  $w$ 的前 $n$ 个元素等于所期望的乘积 $Kv$。\n快速傅里叶变换(FFT)算法在 $O(m \\log m)$ 时间内计算DFT。由于 $m$ 的选择与 $n$ 同阶（例如，$m \\approx 2n$），总复杂度为 $O(n \\log n)$。\n\n**结论：** 正确。\n\n**D. 线性系统 $(K + \\sigma^2 I)\\alpha = y$ 可以通过使用基于FFT的 $K$ 的对角化方法，在 $O(n\\log n)$ 时间内被精确求解，且这与 $n$ 和 $\\kappa$ 的选择无关。**\n\n这个陈述声称有一个精确解。使用基于FFT的对角化方法的精确解将要求矩阵 $A = K + \\sigma^2 I$ 能被DFT矩阵 $F$ 对角化。\n矩阵 $K$ 是托普利兹矩阵，而 $I$ 也是托普利兹矩阵（并且是循环矩阵）。两个托普利兹矩阵的和是一个托普利兹矩阵。因此，$A = K + \\sigma^2 I$ 是一个托普利兹矩阵。\n正如在对陈述B的分析中确立的，一个通用的托普利兹矩阵不能被DFT对角化。因此，所提出的求解方法的前提是错误的。\n人们可以通过用一个“接近”的循环矩阵 $C$ 替换托普利兹矩阵 $K + \\sigma^2 I$ 来*近似*求解，在 $O(n \\log n)$ 时间内求解 $(C + \\sigma^2 I)\\tilde{\\alpha} = y$。然而，$\\tilde{\\alpha}$ 不会是*精确*解 $\\alpha$，并且这个近似的质量取决于核 $\\kappa$ 和大小 $n$。声称有精确解是不正确的。\n\n**结论：** 不正确。\n\n**E. 使用共轭梯度法(CG)并结合经FFT加速的与 $K$ 的矩阵向量积，求解 $(K + \\sigma^2 I)\\alpha = y$ 的每次迭代成本为 $O(n\\log n)$，并且使用一个有效的预条件子，总成本可以近似按 $O(n\\log n)$ 缩放，最多相差一个取决于条件数的因子。**\n\n共轭梯度(CG)算法是求解线性系统 $A\\alpha = y$ 的一种迭代方法，其中 $A$ 是对称正定的。矩阵 $K$ 是一个协方差矩阵，因此是对称半正定的。当 $\\sigma^2 > 0$ 时，矩阵 $A = K + \\sigma^2 I$ 是对称正定的，所以CG适用。\nCG每次迭代中的主要计算工作是形式为 $Ap$ 的单个矩阵向量积。在我们的情况下，这是 $(K + \\sigma^2 I)p = Kp + \\sigma^2 p$。\n正如在对陈述C的分析中确立的，与托普利兹矩阵 $K$ 的矩阵向量积 $Kp$ 可以使用FFT在 $O(n \\log n)$ 时间内计算。$\\sigma^2 p$ 项是一个向量缩放操作，成本为 $O(n)$。因此，每次迭代的总成本主要由FFT决定，确实是 $O(n \\log n)$。\nCG的总成本是每次迭代的成本乘以收敛所需的迭代次数。迭代次数取决于矩阵 $A$ 的条件数。预处理是一种将系统转换为具有更好条件数的等效系统的技术，从而减少迭代次数。对于托普利兹系统，循环预条件子是一种常见且有效的选择。应用这样的预条件子也需要 $O(n \\log n)$ 的成本。如果预条件子是有效的，迭代次数可以变得很小且几乎与 $n$ 无关，从而导致总成本接近 $O(n \\log n)$。该陈述是对这种先进方法的精确和正确的描述。\n\n**结论：** 正确。\n\n**F. 如果 $C$ 是由协方差的周期化版本构造的 $K$ 的循环嵌入，那么 $C$ 的特征值由 $C$ 的第一列的DFT给出，并且 $\\log\\det(K + \\sigma^2 I)$ 可由 $\\sum_{j=0}^{m-1} \\log\\big(\\lambda_j(C) + \\sigma^2\\big)$ 近似；当嵌入反映了一个有效的周期性协方差且嵌入大小 $m$ 足够大时，这个近似是准确的，但对于非周期性协方差，它通常是有偏的。**\n\n该陈述描述了一种近似对数行列式的方法。该方法用一个更大尺寸 $m \\times m$ 的循环矩阵 $C$ 来近似托普利兹矩阵 $K$。\n近似值为 $\\log\\det(K + \\sigma^2 I) \\approx \\log\\det(C_{nn} + \\sigma^2 I_n)$，其中 $C_{nn}$ 是 $C$ 的 $n \\times n$ 左上角块。一个更简单但更常见的近似是 $\\log\\det(K+\\sigma^2 I) \\approx \\frac{n}{m} \\log\\det(C+\\sigma^2 I)$。该陈述提出的公式看起来像 $\\log\\det(C+\\sigma^2 I)$，这是对嵌入矩阵行列式的近似，而不是原始矩阵。然而，让我们分析其组成部分。\n1.  $C$ 的特征值：循环矩阵 $C$ 的特征值是其第一列的DFT的分量，这是一个基本性质。这是正确的。\n2.  循环近似的对数行列式：对于矩阵 $C + \\sigma^2 I$，特征值是 $\\lambda_j(C) + \\sigma^2$。行列式是特征值的乘积，对数行列式是对数特征值的和。所以，$\\log\\det(C + \\sigma^2 I) = \\sum_{j=0}^{m-1} \\log(\\lambda_j(C) + \\sigma^2)$。这个公式对于循环矩阵 $C+\\sigma^2 I$ 是正确的。\n3.  近似质量：表达式 $\\log\\det(C+\\sigma^2 I)$ 作为 $\\log\\det(K+\\sigma^2 I)$ 的近似（可能带有一个像 $n/m$ 这样的缩放因子）。这种近似源于用周期性结构（循环）替换非周期性协方差结构（托普利兹）。除非原始核已经以匹配循环结构的方式具有周期性，否则这种替换会引入系统误差或偏差。随着嵌入大小 $m$ 相对于 $n$ 的增加，近似的质量会提高，从而减少“环绕”效应。该陈述准确地描述了这些方面。\n\n**结论：** 正确。\n\n**G. 像Hutchinson方法这样的随机迹估计器，使用随机Rademacher向量 $z$ 时满足 $\\mathbb{E}\\left[z^{\\top}\\log(K + \\sigma^2 I)z\\right] = \\operatorname{trace}\\big(\\log(K + \\sigma^2 I)\\big)$，并且与Lanczos求积以及基于FFT的与 $K$ 的矩阵向量积相结合，它为近似计算 $\\log\\det(K + \\sigma^2 I)$ 提供了一种每次迭代成本为 $O(n\\log n)$ 的蒙特卡洛方法（最多相差探测向量数量和Lanczos步数的因子）。**\n\n该陈述描述了用于近似对数行列式的随机Lanczos求积(SLQ)方法。\n首先，任何正定矩阵 $A$ 的对数行列式可以表示为其矩阵对数的迹：$\\log\\det(A) = \\operatorname{tr}(\\log A)$。\nHutchinson方法通过期望 $\\mathbb{E}[z^T B z]$ 来估计矩阵 $B$ 的迹，其中 $z$ 是一个随机向量，其分量是独立同分布的，均值为0，方差为1。这是因为 $\\mathbb{E}[z^T B z] = \\mathbb{E}[\\sum_{i,j} z_i B_{ij} z_j] = \\sum_{i,j} B_{ij} \\mathbb{E}[z_i z_j] = \\sum_i B_{ii} = \\operatorname{tr}(B)$，因为 $\\mathbb{E}[z_i z_j] = \\delta_{ij}$。该陈述将此正确地应用于 $B = \\log(K + \\sigma^2 I)$。\n主要困难在于计算二次型 $z^T \\log(A) z$，其中 $A = K+\\sigma^2 I$。这不是一个简单的计算。Lanczos求积是一种近似形式为 $v^T f(A) v$ 的量的方法。通过以起始向量 $z$ 对 $A$ 运行 $k_{L}$ 步Lanczos算法，可以得到一个三对角矩阵 $T_{k_L}$。近似值为 $z^T f(A) z \\approx \\|z\\|^2 [f(T_{k_L})]_{11}$。\nLanczos算法的主要操作是与矩阵 $A$ 的矩阵向量乘法。根据陈述E，这样一个乘积的成本为 $O(n \\log n)$ 时间。\n因此，蒙特卡洛方法的一次“迭代”（为一个随机探测向量 $z$ 计算估计值）需要 $k_{L}$ 次矩阵向量积，成本为 $O(k_L n \\log n)$。如果Lanczos步数 $k_L$ 被认为是一个小常数，则每个探测向量的成本为 $O(n \\log n)$。该陈述是对这种先进数值方法的正确而准确的描述。\n\n**结论：** 正确。",
            "answer": "$$\\boxed{ACEFG}$$"
        },
        {
            "introduction": "对于大规模、非结构化的数据集，前述基于结构化网格的加速方法不再适用。本练习将介绍现代可扩展高斯过程的核心技术之一：使用引导点的变分推断方法。你将深入分析这种方法的训练和预测计算复杂度，并理解其如何通过小批量随机优化，在处理海量数据时维持计算的可行性。",
            "id": "3309550",
            "problem": "考虑一个零均值高斯过程回归模型，其先验为 $f \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot))$，输入为 $X \\in \\mathbb{R}^{n \\times d}$，输出为 $\\mathbf{y} \\in \\mathbb{R}^{n}$，并具有高斯似然 $p(\\mathbf{y} \\mid \\mathbf{f}) = \\prod_{i=1}^{n} \\mathcal{N}(y_{i} \\mid f(x_{i}), \\sigma^{2})$。引入 $m \\ll n$ 个诱导位置 $Z \\in \\mathbb{R}^{m \\times d}$，并记 $\\mathbf{u} = f(Z)$，其先验为 $p(\\mathbf{u}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{K}_{ZZ})$，其中 $\\mathbf{K}_{ZZ} \\in \\mathbb{R}^{m \\times m}$ 是 $Z$ 上的核矩阵。考虑使用高斯变分分布 $q(\\mathbf{u}) = \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$ 的变分诱导点方法，以及通过联合高斯的条件分布对 $q(\\mathbf{u})$ 进行边缘化得到的变分后验 $q(\\mathbf{f})$。训练目标是证据下界 (ELBO)，其定义为在 $q(\\mathbf{f})$ 下的对数似然期望减去 $q(\\mathbf{u})$ 和 $p(\\mathbf{u})$ 之间的 Kullback–Leibler 散度。假设协方差 $\\mathbf{S}$ 是稠密的，并且每对 $(x, z)$ 的核函数求值成本为常数时间，与 $m$ 和 $n$ 无关。\n\n您将在此设置下分析训练和预测的计算复杂度。使用以下基本依据和事实：\n\n1. 先验下 $(\\mathbf{u}, f(x))$ 的联合高斯结构决定了条件分布 $p(f(x) \\mid \\mathbf{u})$ 是高斯的，并且线性高斯变换保持高斯性，允许通过矩阵乘法和三角系统求解得到均值和方差的闭式解。\n2. 对于稠密的 $m \\times m$ 矩阵，Cholesky 分解的成本为 $O(m^{3})$，求解三角系统的成本为每个右侧项 $O(m^{2})$。一个 $m \\times m$ 的稠密矩阵与一个 m 维向量相乘的成本为 $O(m^{2})$，一个 m 维向量乘以一个已通过 Cholesky 分解求逆的 $m \\times m$ 矩阵的成本为每次求解 $O(m^{2})$。\n3. ELBO 分解为数据项上的对数似然期望之和，外加一个与数据无关的 Kullback–Leibler 项。小批量随机优化使用数据的随机子集，并重新缩放其贡献以形成和的无偏估计量。\n\n基于这些依据推导：\n\na) 当使用稠密 $\\mathbf{S}$ 和基于 $\\mathbf{K}_{ZZ}$ 的预计算时，对数据进行一次遍历以评估 ELBO 及其关于变分参数和核超参数的梯度时，全批量训练的时间复杂度（作为 $n$ 和 $m$ 的函数）。\n\nb) 在 $q(\\mathbf{f})$ 下计算预测均值和预测方差的每个测试点的计算成本。\n\nc) 在均匀随机地对数据索引进行子采样并重新缩放时，基于小批量的随机优化如何产生 ELBO 的无偏梯度估计量的原则性论证。假设对变分期望使用重参数化技巧，并且每个对数似然期望项的单样本蒙特卡洛估计量是无偏的。\n\n您的推导必须从所列依据出发，推理所需的线性代数运算序列，并仔细计算 $n$ 和 $m$ 的主导项，不使用任何快捷公式。除了稠密矩阵外，不假设任何特殊结构，并且不假设 $\\mathbf{S}$ 是对角或低秩的。\n\n将您的最终答案指定为包含三个条目的单行矩阵，顺序为：训练复杂度、每个测试点的预测均值成本、每个测试点的预测方差成本。每个条目必须是一个大O表达式。无需数值取整。",
            "solution": "该问题要求分析变分诱导点高斯过程模型的训练和预测的计算复杂度，并论证基于小批量的随机梯度估计量的无偏性。分析将分三部分进行，对应三个任务(a)、(b)和(c)。\n\na) 一次遍历的全批量训练时间复杂度\n\n训练目标是证据下界 (ELBO)，我们记为 $L$。其定义为：\n$$\nL = \\mathbb{E}_{q(\\mathbf{f})}[\\ln p(\\mathbf{y} \\mid \\mathbf{f})] - \\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})]\n$$\n给定分解的似然 $p(\\mathbf{y} \\mid \\mathbf{f}) = \\prod_{i=1}^{n} p(y_{i} \\mid f(x_{i}))$，对数似然期望项可以写成对 $n$ 个数据点的求和：\n$$\nL = \\sum_{i=1}^{n} \\mathbb{E}_{q(f_i)}[\\ln p(y_i \\mid f_i)] - \\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})]\n$$\n我们分析对 $n$ 个数据点进行一次完整遍历时，评估两个主要部分（求和项和KL散度）及其梯度的计算成本。\n\n首先，我们分析 Kullback–Leibler 散度项 $\\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})]$。变分分布为 $q(\\mathbf{u}) = \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$，先验分布为 $p(\\mathbf{u}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{K}_{ZZ})$。这两个多元高斯分布之间的 KL 散度为：\n$$\n\\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})] = \\frac{1}{2} \\left[ \\text{Tr}(\\mathbf{K}_{ZZ}^{-1}\\mathbf{S}) + \\mathbf{m}^T \\mathbf{K}_{ZZ}^{-1}\\mathbf{m} - m + \\ln\\det(\\mathbf{K}_{ZZ}) - \\ln\\det(\\mathbf{S}) \\right]\n$$\n该表达式各部分的计算成本如下：\n1.  对 $\\mathbf{K}_{ZZ} \\in \\mathbb{R}^{m \\times m}$ 进行 Cholesky 分解：这是高效求解和行列式计算所必需的预计算。成本为 $O(m^3)$。设 $\\mathbf{K}_{ZZ} = \\mathbf{L}_{ZZ}\\mathbf{L}_{ZZ}^T$。\n2.  $\\ln\\det(\\mathbf{K}_{ZZ})$：在 Cholesky 分解后，这等于 $2 \\sum_{j=1}^{m} \\ln((\\mathbf{L}_{ZZ})_{jj})$，成本为 $O(m)$。\n3.  $\\ln\\det(\\mathbf{S})$：由于 $\\mathbf{S}$ 是稠密的，我们计算其 Cholesky 分解 $\\mathbf{S} = \\mathbf{L}_{S}\\mathbf{L}_{S}^T$，成本为 $O(m^3)$，然后以 $O(m)$ 的成本计算对数行列式。\n4.  $\\mathbf{m}^T \\mathbf{K}_{ZZ}^{-1}\\mathbf{m}$：首先求解系统 $\\mathbf{K}_{ZZ}\\boldsymbol{\\alpha} = \\mathbf{m}$ 得到 $\\boldsymbol{\\alpha}$，然后计算点积 $\\mathbf{m}^T\\boldsymbol{\\alpha}$。使用 Cholesky 因子 $\\mathbf{L}_{ZZ}$，求解该系统需要两次三角求解，成本为 $O(m^2)$。随后的点积成本为 $O(m)$。总成本为 $O(m^2)$。\n5.  $\\text{Tr}(\\mathbf{K}_{ZZ}^{-1}\\mathbf{S})$：这是最昂贵的操作。一种直接的方法是计算 $\\mathbf{V} = \\mathbf{K}_{ZZ}^{-1}\\mathbf{S}$，然后取其迹。计算 $\\mathbf{V}$ 需要对 $\\mathbf{S}$ 的每一列 $\\mathbf{s}_j$ 求解 $m$ 个形式为 $\\mathbf{K}_{ZZ}\\mathbf{v}_j = \\mathbf{s}_j$ 的线性系统。每次求解的成本为 $O(m^2)$，导致总成本为 $m \\times O(m^2) = O(m^3)$。取迹的额外成本为 $O(m)$。\n\n将这些成本相加，KL 项的计算主要由 Cholesky 分解和迹项主导，复杂度为 $O(m^3)$。\n\n接下来，我们分析对数似然期望项 $\\sum_{i=1}^{n} \\mathbb{E}_{q(f_i)}[\\ln p(y_i \\mid f_i)]$。对于高斯似然 $p(y_i \\mid f_i) = \\mathcal{N}(y_i \\mid f_i, \\sigma^2)$，该期望有一个闭式解，它依赖于边缘变分后验 $q(f_i)$ 的均值 $\\mu_{q,i}$ 和方差 $\\sigma^2_{q,i}$。\n分布 $q(f_i)$ 是通过对联合分布 $p(f_i \\mid \\mathbf{u})q(\\mathbf{u})$ 进行边缘化得到的。根据基本依据1，$p(f_i \\mid \\mathbf{u})$ 是高斯的。设 $\\mathbf{k}_{i} = k(Z, x_i)$。那么 $p(f_i \\mid \\mathbf{u}) = \\mathcal{N}(f_i \\mid \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{u}, k(x_i, x_i) - \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_i)$。由于 $q(\\mathbf{u})$ 是高斯的，得到的边缘分布 $q(f_i)$ 也是高斯的。其均值和方差为：\n$$\n\\mu_{q,i} = \\mathbb{E}_{q(f_i)}[f_i] = \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{m}\n$$\n$$\n\\sigma^2_{q,i} = \\text{Var}_{q(f_i)}[f_i] = k(x_i, x_i) - \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_i + \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{S} \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_i\n$$\n为了评估对 $n$ 个数据点的求和，我们必须为每个 $i \\in \\{1, \\dots, n\\}$ 计算这些量。对于单个数据点 $i$：\n1.  计算核向量 $\\mathbf{k}_i \\in \\mathbb{R}^m$：$m$ 次核函数求值，成本为 $O(m)$。\n2.  定义 $\\boldsymbol{\\beta}_i = \\mathbf{K}_{ZZ}^{-1}\\mathbf{k}_i$。这通过使用预计算的 $\\mathbf{K}_{ZZ}$ 的 Cholesky 因子求解一个线性系统来计算。成本为 $O(m^2)$。\n3.  计算均值 $\\mu_{q,i} = \\boldsymbol{\\beta}_i^T \\mathbf{m}$。这是一个点积，成本为 $O(m)$。\n4.  计算方差分量：\n    *   $k(x_i, x_i)$: $O(1)$。\n    *   $\\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_i = \\mathbf{k}_i^T \\boldsymbol{\\beta}_i$：另一个点积，成本 $O(m)$。\n    *   $\\boldsymbol{\\beta}_i^T \\mathbf{S} \\boldsymbol{\\beta}_i$：由于 $\\mathbf{S}$ 是稠密的，这被计算为 $(\\boldsymbol{\\beta}_i^T \\mathbf{S}) \\boldsymbol{\\beta}_i$。矩阵向量乘积 $\\mathbf{S}^T\\boldsymbol{\\beta}_i$（或 $\\mathbf{S}\\boldsymbol{\\beta}_i$）成本为 $O(m^2)$，随后是一个成本为 $O(m)$ 的点积。总成本为 $O(m^2)$。\n\n计算一个数据点 $i$ 的均值和方差的总成本由求解 $\\boldsymbol{\\beta}_i$ 和涉及 $\\mathbf{S}$ 的二次型主导，为 $O(m^2)$。为了计算完整的对数似然期望项，我们必须对所有 $n$ 个数据点重复此过程，导致总成本为 $O(nm^2)$。\n\n结合一次全批量遍历的成本：整个 ELBO 评估需要一次性的 $O(m^3)$ 成本用于 $\\mathbf{K}_{ZZ}$ 的 Cholesky 分解，以及另一个 $O(m^3)$ 成本用于 KL 项的计算，再加上数据依赖项的 $O(nm^2)$ 成本。总成本为 $O(nm^2 + m^3)$。使用现代自动微分框架计算关于变分参数和核超参数的梯度，其反向传播的计算复杂度与前向传播渐进相同。因此，一个训练步骤（ELBO及其梯度）的复杂度为 $O(nm^2 + m^3)$。\n\nb) 每个测试点的预测成本\n\n对于一个新的测试点 $x_*$，我们计算预测分布 $q(f_*) = \\int p(f_* \\mid \\mathbf{u})q(\\mathbf{u})d\\mathbf{u}$。这是一个高斯分布，其均值和方差可以以闭式形式计算。我们假设模型已经训练好，所以变分参数 $\\mathbf{m}$、$\\mathbf{S}$ 和核超参数是固定的。训练后常见的预计算包括 $\\mathbf{K}_{ZZ}$ 的 Cholesky 因子。\n\n预测均值为：\n$$\n\\mu_{q,*} = \\mathbb{E}_{q(f_*)}[f_*] = \\mathbf{k}_*^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{m}\n$$\n其中 $\\mathbf{k}_* = k(Z, x_*)$。为了高效计算，我们可以在训练后一次性预计算向量 $\\boldsymbol{\\alpha} = \\mathbf{K}_{ZZ}^{-1}\\mathbf{m}$，成本为 $O(m^2)$。然后，对于每个测试点 $x_*$：\n1.  计算核向量 $\\mathbf{k}_*$: $O(m)$。\n2.  计算点积 $\\mu_{q,*} = \\mathbf{k}_*^T \\boldsymbol{\\alpha}$: $O(m)$。\n计算每个测试点预测均值的总计算成本为 $O(m)$。\n\n预测方差为：\n$$\n\\sigma^2_{q,*} = \\text{Var}_{q(f_*)}[f_*] = (k_{**} - \\mathbf{k}_*^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_*) + \\mathbf{k}_*^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{S} \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_*\n$$\n其中 $k_{**} = k(x_*, x_*)$。计算单个测试点的此值的成本为：\n1.  分别计算 $k_{**}$ 和 $\\mathbf{k}_*$: $O(1)$ 和 $O(m)$。\n2.  定义 $\\boldsymbol{\\beta}_* = \\mathbf{K}_{ZZ}^{-1}\\mathbf{k}_*$. 使用 $\\mathbf{K}_{ZZ}$ 的 Cholesky 因子求解此系统：$O(m^2)$。\n3.  计算第一项方差 $k_{**} - \\mathbf{k}_*^T \\boldsymbol{\\beta}_*$: $O(m)$。\n4.  计算第二项方差 $\\boldsymbol{\\beta}_*^T \\mathbf{S} \\boldsymbol{\\beta}_*$: 由于 $\\mathbf{S}$ 是一个稠密的 $m \\times m$ 矩阵，此成本为 $O(m^2)$。\n计算每个测试点预测方差的总计算成本由系统求解和与 $\\mathbf{S}$ 的二次型主导，复杂度为 $O(m^2)$。\n\nc) 小批量梯度估计量无偏性的论证\n\nELBO 可以写成每个数据贡献的总和加上一个与数据无关的项：\n$$\nL(\\boldsymbol{\\phi}) = \\sum_{i=1}^{n} L_i(\\boldsymbol{\\phi}) - \\text{KL}(\\boldsymbol{\\phi})\n$$\n其中 $L_i(\\boldsymbol{\\phi}) = \\mathbb{E}_{q(f_i)}[\\ln p(y_i \\mid f_i)]$ 且 $\\text{KL}(\\boldsymbol{\\phi}) = \\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})]$。模型的参数（变分参数和核参数）由 $\\boldsymbol{\\phi}$ 表示。\n\n根据梯度算子的线性性质，ELBO 关于这些参数的梯度为：\n$$\n\\nabla_{\\boldsymbol{\\phi}} L(\\boldsymbol{\\phi}) = \\nabla_{\\boldsymbol{\\phi}} \\left(\\sum_{i=1}^{n} L_i(\\boldsymbol{\\phi})\\right) - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi}) = \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi})\n$$\n在小批量随机优化中，我们不计算所有 $n$ 个数据点的完整和。相反，我们从索引集 $\\{1, \\dots, n\\}$ 中均匀随机地抽取一个大小为 $S \\ll n$ 的小批量 $B$。随机梯度估计量 $\\hat{\\mathbf{g}}(\\boldsymbol{\\phi})$ 的构造如下：\n$$\n\\hat{\\mathbf{g}}(\\boldsymbol{\\phi}) = \\frac{n}{S} \\sum_{j \\in B} \\nabla_{\\boldsymbol{\\phi}} L_j(\\boldsymbol{\\phi}) - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi})\n$$\n关键是重缩放因子 $\\frac{n}{S}$。为了证明该估计量是无偏的，我们对其关于小批量 $B$ 的随机选择取期望：\n$$\n\\mathbb{E}_{B}[\\hat{\\mathbf{g}}(\\boldsymbol{\\phi})] = \\mathbb{E}_{B} \\left[ \\frac{n}{S} \\sum_{j \\in B} \\nabla_{\\boldsymbol{\\phi}} L_j(\\boldsymbol{\\phi}) \\right] - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi})\n$$\nKL 项的梯度与数据无关，因此其期望就是其自身。我们关注缩放和的期望。设 $I_j$ 是一个指示随机变量，如果索引 $j$ 在小批量 $B$ 中，则为 $1$，否则为 $0$。该和可以重写为 $\\sum_{i=1}^{n} I_i \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi})$。利用期望的线性性质：\n$$\n\\mathbb{E}_{B} \\left[ \\frac{n}{S} \\sum_{i=1}^{n} I_i \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) \\right] = \\frac{n}{S} \\sum_{i=1}^{n} \\mathbb{E}_{B}[I_i] \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi})\n$$\n期望 $\\mathbb{E}_{B}[I_i]$ 是数据点 $i$ 被包含在从 $n$ 个项目中均匀抽样的大小为 $S$ 的小批量 $B$ 中的概率。这个概率是 $P(i \\in B) = \\frac{S}{n}$。将其代回：\n$$\n\\frac{n}{S} \\sum_{i=1}^{n} \\left(\\frac{S}{n}\\right) \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) = \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi})\n$$\n这正是完整梯度中的精确和。因此，我们已经证明：\n$$\n\\mathbb{E}_{B}[\\hat{\\mathbf{g}}(\\boldsymbol{\\phi})] = \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi}) = \\nabla_{\\boldsymbol{\\phi}} L(\\boldsymbol{\\phi})\n$$\n该估计量是无偏的。使用重参数化技巧允许通过将梯度移入期望内部来计算 $\\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) = \\nabla_{\\boldsymbol{\\phi}} \\mathbb{E}_{q(f_i)}[\\ln p(y_i|f_i)]$。如果对此项使用单样本蒙特卡洛估计，其无偏性确保了关于蒙特卡洛噪声的期望与关于小批量采样的期望相结合，仍然能得到总梯度 $\\nabla_{\\boldsymbol{\\phi}} L(\\boldsymbol{\\phi})$ 的无偏估计量。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nO(nm^2 + m^3) & O(m) & O(m^2)\n\\end{pmatrix}\n}\n$$"
        }
    ]
}