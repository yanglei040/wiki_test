{
    "hands_on_practices": [
        {
            "introduction": "为了真正掌握高斯过程，从基本构件出发亲手构造它是极有帮助的。Karhunen–Loève (KL) 展开提供了一种将高斯过程表示为一组基函数加权和的方法，这与确定性函数的傅里叶级数展开有异曲同工之妙。这个练习 () 将引导你为一个典型例子——维纳过程——进行具体的推导，从而加深对协方差核与过程样本路径之间内在联系的理解。",
            "id": "3309595",
            "problem": "考虑紧域 $[0,1]$ 上的一个零均值高斯过程 (GP) $f$，该域配备了勒贝格测度，其协方差核 $k(x,x')$ 是连续、对称、半正定的。定义相关的 Hilbert–Schmidt 积分算子 $\\mathcal{T}:L^{2}([0,1])\\to L^{2}([0,1])$ 为\n$$\n(\\mathcal{T}g)(x) \\equiv \\int_{0}^{1} k(x,t)\\, g(t)\\, dt,\n$$\n该算子是自伴、正的且紧的。设 $\\{(\\lambda_{j},\\phi_{j})\\}_{j\\geq 1}$ 是 $\\mathcal{T}$ 的特征对，其中特征函数 $\\{\\phi_{j}\\}_{j\\geq 1}$ 构成 $L^{2}([0,1])$ 的一个标准正交基，且特征值满足 $\\lambda_{j}\\geq 0$ 和 $\\sum_{j\\geq 1}\\lambda_{j}  \\infty$。根据 Mercer 分解，$k(x,x')$ 可表示为一致收敛的级数 $k(x,x')=\\sum_{j\\geq 1}\\lambda_{j}\\phi_{j}(x)\\phi_{j}(x')$。\n\n将 $f$ 的截断 Karhunen–Loève (KL) 展开 (Karhunen–Loève (KL)) 定义为\n$$\nf_{M}(x) \\equiv \\sum_{j=1}^{M} \\sqrt{\\lambda_{j}}\\, \\xi_{j}\\, \\phi_{j}(x),\n$$\n其中 $\\{\\xi_{j}\\}_{j\\geq 1}$ 是独立同分布的标准正态随机变量，$\\xi_{j}\\sim \\mathcal{N}(0,1)$。令截断误差为均方 $L^{2}([0,1])$ 误差 $\\mathbb{E}\\big[\\|f-f_{M}\\|_{L^{2}}^{2}\\big]$，其中 $\\|g\\|_{L^{2}}^{2}=\\int_{0}^{1} g(x)^{2}\\, dx$。\n\n对于 $[0,1]$ 上的特定核 $k(x,x')=\\min(x,x')$。从上述谱定义出发，且不假设任何快捷公式，推导特征对 $(\\lambda_{j},\\phi_{j})$，构造 $f_{M}(x)$，然后计算截断误差 $\\mathbb{E}\\big[\\|f-f_{M}\\|_{L^{2}}^{2}\\big]$ 作为 $M$ 的函数的精确解析表达式。您可以使用经典的特殊函数来表示最终结果，前提是您在推导过程中明确定义了它们。\n\n不需要进行数值近似；请以单个闭式解析表达式的形式提供最终答案。如果您引入任何缩写词（例如，蒙特卡洛（MC）），请在首次使用时写出其全称。",
            "solution": "用户希望我解决一个关于高斯过程的问题。\n\n### 问题验证\n\n首先，我将验证问题陈述。\n\n#### 步骤 1：提取已知条件\n\n- **过程与域**：紧域 $[0,1]$ 上的一个零均值高斯过程 (GP) $f$，该域配备了勒贝格测度。\n- **核**：一个连续、对称、半正定的协方差核 $k(x,x')$。\n- **积分算子**：$\\mathcal{T}:L^{2}([0,1])\\to L^{2}([0,1])$ 定义为 $(\\mathcal{T}g)(x) = \\int_{0}^{1} k(x,t)\\, g(t)\\, dt$。\n- **谱性质**：$\\mathcal{T}$ 是自伴、正的且紧的。其特征对为 $\\{(\\lambda_{j},\\phi_{j})\\}_{j\\geq 1}$，其中 $\\{\\phi_{j}\\}_{j\\geq 1}$ 是 $L^{2}([0,1])$ 的一个标准正交基，$\\lambda_{j}\\geq 0$，且 $\\sum_{j\\geq 1}\\lambda_{j}  \\infty$。\n- **Mercer 定理**：$k(x,x')=\\sum_{j\\geq 1}\\lambda_{j}\\phi_{j}(x)\\phi_{j}(x')$。\n- **截断展开**：截断 Karhunen–Loève (KL) 展开为 $f_{M}(x) = \\sum_{j=1}^{M} \\sqrt{\\lambda_{j}}\\, \\xi_{j}\\, \\phi_{j}(x)$，其中 $\\xi_{j}\\sim \\mathcal{N}(0,1)$ 是独立同分布 (i.i.d.) 的标准正态随机变量。\n- **截断误差**：定义为均方 $L^{2}([0,1])$ 误差，$\\mathbb{E}\\big[\\|f-f_{M}\\|_{L^{2}}^{2}\\big]$，其中 $\\|g\\|_{L^{2}}^{2}=\\int_{0}^{1} g(x)^{2}\\, dx$。\n- **特定核**：$[0,1]$ 上的 $k(x,x')=\\min(x,x')$。\n- **任务**：\n  1. 对于指定的核，推导其特征对 $(\\lambda_{j},\\phi_{j})$。\n  2. 构造 $f_{M}(x)$。\n  3. 计算截断误差 $\\mathbb{E}\\big[\\|f-f_{M}\\|_{L^{2}}^{2}\\big]$ 作为 $M$ 的函数的精确解析表达式。\n\n#### 步骤 2：使用提取的已知条件进行验证\n\n- **科学性**：该问题是随机过程和函数数据分析理论中的一个标准、基础课题。Karhunen–Loève 展开是该领域的基石，而核 $k(x,x')=\\min(x,x')$ 是标准维纳过程（布朗运动）的协方差函数，这是一个典型的例子。所有数学概念（Hilbert–Schmidt 算子、Mercer 定理、$L^2$ 空间）都是公认的。\n- **适定性**：问题陈述清晰，提供了所有必要的定义。它要求为一个明确定义的数学对象推导一个特定的量，并且期望得到一个唯一、有意义的解。\n- **客观性**：问题以精确、客观的数学语言表述，没有任何主观或模棱两可的术语。\n\n该问题没有表现出任何违反科学原理、不完整、矛盾或其他缺陷。这是一个定义明确但并非无足轻重的数学练习。\n\n#### 步骤 3：结论与行动\n\n问题是 **有效的**。我现在将开始解答。\n\n### 解题推导\n\n主要目标是计算截断误差 $\\mathbb{E}\\big[\\|f-f_{M}\\|_{L^{2}}^{2}\\big]$。这个量取决于积分算子 $\\mathcal{T}$ 的特征值。因此，第一步是解决指定核的特征值问题。\n\n**第 1 部分：推导特征对 $(\\lambda_j, \\phi_j)$**\n\n积分算子 $\\mathcal{T}$ 的特征值问题由第二类 Fredholm 积分方程给出：\n$$ (\\mathcal{T}\\phi)(x) = \\lambda \\phi(x) $$\n代入核 $k(x,t) = \\min(x,t)$，我们得到：\n$$ \\int_{0}^{1} \\min(x,t) \\phi(t) dt = \\lambda \\phi(x) $$\n我们可以根据最小值函数的定义来拆分积分：\n$$ \\int_{0}^{x} t \\phi(t) dt + \\int_{x}^{1} x \\phi(t) dt = \\lambda \\phi(x) $$\n假设 $\\phi$ 足够光滑，我们可以使用莱布尼茨积分法则对该方程关于 $x$ 求导。第一次求导得到：\n$$ \\frac{d}{dx}\\left(\\int_{0}^{x} t \\phi(t) dt\\right) + \\frac{d}{dx}\\left(x \\int_{x}^{1} \\phi(t) dt\\right) = \\lambda \\phi'(x) $$\n$$ x\\phi(x) + \\left(1 \\cdot \\int_{x}^{1} \\phi(t) dt + x \\cdot (-\\phi(x))\\right) = \\lambda \\phi'(x) $$\n$$ x\\phi(x) + \\int_{x}^{1} \\phi(t) dt - x\\phi(x) = \\lambda \\phi'(x) $$\n$$ \\int_{x}^{1} \\phi(t) dt = \\lambda \\phi'(x) $$\n第二次对 $x$ 求导：\n$$ -\\phi(x) = \\lambda \\phi''(x) $$\n这给出了二阶常微分方程 (ODE)：\n$$ \\phi''(x) + \\frac{1}{\\lambda} \\phi(x) = 0 $$\n该 ODE 的通解是 $\\phi(x) = A \\sin(\\frac{x}{\\sqrt{\\lambda}}) + B \\cos(\\frac{x}{\\sqrt{\\lambda}})$。\n\n为了找到常数 $A$ 和 $B$ 以及特征值 $\\lambda$，我们必须建立边界条件。\n1.  从原始积分方程中，令 $x=0$：\n    $$ \\int_{0}^{1} \\min(0,t)\\phi(t)dt = \\lambda \\phi(0) \\implies \\int_{0}^{1} 0 \\cdot \\phi(t) dt = 0 = \\lambda \\phi(0) $$\n    对于非平凡的特征系统，我们寻求 $\\lambda \\neq 0$，这意味着 $\\phi(0) = 0$。\n2.  从一次求导后的方程中，令 $x=1$：\n    $$ \\int_{1}^{1} \\phi(t) dt = \\lambda \\phi'(1) \\implies 0 = \\lambda \\phi'(1) $$\n    同样，对于 $\\lambda \\neq 0$，这意味着 $\\phi'(1) = 0$。\n\n现在，我们将这些边界条件应用于通解：\n-   在 $x=0$ 处，$\\phi(0) = A \\sin(0) + B \\cos(0) = B$。由于 $\\phi(0)=0$，我们有 $B=0$。解简化为 $\\phi(x) = A \\sin(\\frac{x}{\\sqrt{\\lambda}})$。\n-   导数为 $\\phi'(x) = \\frac{A}{\\sqrt{\\lambda}} \\cos(\\frac{x}{\\sqrt{\\lambda}})$。在 $x=1$ 处，必须有 $\\phi'(1) = 0$：\n    $$ \\frac{A}{\\sqrt{\\lambda}} \\cos\\left(\\frac{1}{\\sqrt{\\lambda}}\\right) = 0 $$\n    对于非平凡的特征函数，$A \\neq 0$。因此，我们要求 $\\cos(\\frac{1}{\\sqrt{\\lambda}}) = 0$。当参数是 $\\frac{\\pi}{2}$ 的奇数倍时，此条件成立：\n    $$ \\frac{1}{\\sqrt{\\lambda_j}} = \\frac{(2j-1)\\pi}{2}, \\quad \\text{for } j = 1, 2, 3, \\dots $$\n    解出特征值 $\\lambda_j$：\n    $$ \\lambda_j = \\frac{4}{(2j-1)^2 \\pi^2} $$\n相应的特征函数形式为 $\\phi_j(x) = A_j \\sin\\left(\\frac{(2j-1)\\pi}{2} x\\right)$。我们通过归一化条件 $\\|\\phi_j\\|_{L^2}^2 = 1$ 来确定常数 $A_j$：\n$$ \\int_{0}^{1} \\phi_j(x)^2 dx = A_j^2 \\int_{0}^{1} \\sin^2\\left(\\frac{(2j-1)\\pi}{2} x\\right) dx = 1 $$\n使用恒等式 $\\sin^2(\\theta) = \\frac{1 - \\cos(2\\theta)}{2}$：\n$$ \\int_{0}^{1} \\frac{1}{2}\\left(1 - \\cos((2j-1)\\pi x)\\right) dx = \\frac{1}{2} \\left[ x - \\frac{\\sin((2j-1)\\pi x)}{(2j-1)\\pi} \\right]_{0}^{1} $$\n$$ = \\frac{1}{2} \\left( (1 - \\frac{\\sin((2j-1)\\pi)}{(2j-1)\\pi}) - (0-0) \\right) = \\frac{1}{2}(1-0) = \\frac{1}{2} $$\n所以，$A_j^2 \\cdot \\frac{1}{2} = 1$，得出 $A_j = \\sqrt{2}$。标准正交特征函数为：\n$$ \\phi_j(x) = \\sqrt{2} \\sin\\left(\\frac{(2j-1)\\pi}{2} x\\right) $$\n\n**第 2 部分：计算截断误差**\n\n过程 $f(x)$ 的 Karhunen-Loève (KL) 展开由 $f(x) = \\sum_{j=1}^{\\infty} \\sqrt{\\lambda_j} \\xi_j \\phi_j(x)$ 给出。截断展开为 $f_M(x) = \\sum_{j=1}^{M} \\sqrt{\\lambda_j} \\xi_j \\phi_j(x)$。\n差值为残差过程：\n$$ f(x) - f_M(x) = \\sum_{j=M+1}^{\\infty} \\sqrt{\\lambda_j} \\xi_j \\phi_j(x) $$\n该差值的 $L^2$-范数的平方为：\n$$ \\|f - f_M\\|_{L^2}^2 = \\int_0^1 \\left( \\sum_{j=M+1}^{\\infty} \\sqrt{\\lambda_j} \\xi_j \\phi_j(x) \\right) \\left( \\sum_{k=M+1}^{\\infty} \\sqrt{\\lambda_k} \\xi_k \\phi_k(x) \\right) dx $$\n通过交换求和与积分（由展开式的一致收敛性证明其合理性）：\n$$ \\|f - f_M\\|_{L^2}^2 = \\sum_{j=M+1}^{\\infty} \\sum_{k=M+1}^{\\infty} \\sqrt{\\lambda_j \\lambda_k} \\xi_j \\xi_k \\int_0^1 \\phi_j(x) \\phi_k(x) dx $$\n由于特征函数 $\\{\\phi_j\\}$ 是标准正交的，我们有 $\\int_0^1 \\phi_j(x) \\phi_k(x) dx = \\delta_{jk}$，其中 $\\delta_{jk}$ 是克罗内克 δ。双重求和坍缩为单个求和：\n$$ \\|f - f_M\\|_{L^2}^2 = \\sum_{j=M+1}^{\\infty} \\lambda_j \\xi_j^2 $$\n截断误差是该量的期望值。利用期望的线性性质：\n$$ \\mathbb{E}\\big[\\|f - f_M\\|_{L^2}^2\\big] = \\mathbb{E}\\left[\\sum_{j=M+1}^{\\infty} \\lambda_j \\xi_j^2\\right] = \\sum_{j=M+1}^{\\infty} \\lambda_j \\mathbb{E}[\\xi_j^2] $$\n变量 $\\xi_j$ 是独立同分布的标准正态随机变量，$\\xi_j \\sim \\mathcal{N}(0,1)$。对于这样的变量，$\\mathbb{E}[\\xi_j] = 0$ 且 $\\text{Var}(\\xi_j) = 1$。方差为 $\\text{Var}(\\xi_j) = \\mathbb{E}[\\xi_j^2] - (\\mathbb{E}[\\xi_j])^2$，因此 $1 = \\mathbb{E}[\\xi_j^2] - 0^2$，这得出 $\\mathbb{E}[\\xi_j^2] = 1$。\n因此，截断误差就是被截断模态的特征值之和：\n$$ \\mathbb{E}\\big[\\|f - f_M\\|_{L^2}^2\\big] = \\sum_{j=M+1}^{\\infty} \\lambda_j $$\n\n**第 3 部分：评估特征值之和**\n\n现在我们将推导出的特征值 $\\lambda_j = \\frac{4}{(2j-1)^2 \\pi^2}$ 代入求和中：\n$$ \\mathbb{E}\\big[\\|f - f_M\\|_{L^2}^2\\big] = \\sum_{j=M+1}^{\\infty} \\frac{4}{(2j-1)^2 \\pi^2} = \\frac{4}{\\pi^2} \\sum_{j=M+1}^{\\infty} \\frac{1}{(2j-1)^2} $$\n为了计算这个和，我们使用 trigamma 函数，它是一个特殊函数，定义为伽玛函数对数的二阶导数，$\\psi_1(z) = \\frac{d^2}{dz^2} \\ln \\Gamma(z)$。它有一个级数表示：\n$$ \\psi_1(z) = \\sum_{n=0}^{\\infty} \\frac{1}{(z+n)^2} $$\n让我们重写我们的求和以匹配此形式。\n$$ \\sum_{j=M+1}^{\\infty} \\frac{1}{(2j-1)^2} = \\frac{1}{4} \\sum_{j=M+1}^{\\infty} \\frac{1}{(j - 1/2)^2} $$\n设求和指数为 $n=j-(M+1)$，因此 $j = n+M+1$。当 $j=M+1$ 时，$n=0$。求和变为：\n$$ \\frac{1}{4} \\sum_{n=0}^{\\infty} \\frac{1}{((n+M+1) - 1/2)^2} = \\frac{1}{4} \\sum_{n=0}^{\\infty} \\frac{1}{(n + (M+1/2))^2} $$\n这与参数为 $z = M+\\frac{1}{2}$ 的 trigamma 函数的级数定义完全匹配。因此：\n$$ \\sum_{j=M+1}^{\\infty} \\frac{1}{(2j-1)^2} = \\frac{1}{4} \\psi_1\\left(M+\\frac{1}{2}\\right) $$\n将此代回截断误差的表达式中：\n$$ \\mathbb{E}\\big[\\|f - f_M\\|_{L^2}^2\\big] = \\frac{4}{\\pi^2} \\left( \\frac{1}{4} \\psi_1\\left(M+\\frac{1}{2}\\right) \\right) = \\frac{1}{\\pi^2} \\psi_1\\left(M+\\frac{1}{2}\\right) $$\n这就是截断误差的最终解析表达式。",
            "answer": "$$ \\boxed{ \\frac{1}{\\pi^2} \\psi_1\\left(M+\\frac{1}{2}\\right) } $$"
        },
        {
            "introduction": "从理论构建转向一个主要的实践挑战。尽管高斯过程的公式非常优美，但其直接实现需要对协方差矩阵 $K$ 求逆，这在数值上可能非常不稳定，尤其是在使用特定核函数或数据配置时。这个练习 () 模拟了一个常见的矩阵病态情景，要求你诊断问题并评估标准的补救措施，从而培养构建稳健高斯过程模型的关键实践技能。",
            "id": "3309568",
            "problem": "考虑一个高斯过程回归，其先验为零均值，并在输入 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$ 上具有一个正定协方差函数 $k(x,x')$。观测值建模为 $y_i = f(x_i) + \\epsilon_i$，其中 $f \\sim \\mathcal{GP}(0,k)$，$\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$ 是独立高斯噪声。Gram 矩阵是 $K \\in \\mathbb{R}^{n \\times n}$，其元素为 $K_{ij} = k(x_i,x_j)$，数据 Gram 矩阵是 $K_y = K + \\sigma_n^2 I_n$。对于对称正定矩阵，其 $2$-范数下的谱条件数定义为 $\\kappa_2(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$，其中 $\\lambda_{\\max}(A)$ 和 $\\lambda_{\\min}(A)$ 分别表示 $A$ 的最大和最小特征值。假设双精度机器精度满足 $\\varepsilon_{\\mathrm{mach}} \\approx 2.22 \\times 10^{-16}$。\n\n假设 $n = 300$ 个输入是高度聚类的（例如，几个紧密的簇被中等大小的间隙分开），协方差函数 $k$ 是平方指数核 $k(x,x') = \\sigma_f^2 \\exp\\!\\big(-\\|x-x'\\|^2/(2\\ell^2)\\big)$，其中 $\\sigma_f^2 = 1$ 且长度尺度 $\\ell$ 很短。一位从业者计算了 $K$ 的极端特征值，发现 $\\lambda_{\\max}(K) \\approx 1.2$ 和 $\\lambda_{\\min}(K) \\approx 10^{-12}$，而当前的噪声水平为 $\\sigma_n^2 \\approx 10^{-14}$。该从业者希望诊断并纠正影响下游随机模拟的数值病态问题，例如通过 Cholesky 分解从后验分布 $\\mathcal{N}(m, K_y)$ 中采样，或通过马尔可夫链蒙特卡洛 (MCMC) 方法探索超参数。\n\n下列哪个陈述是正确的？选择所有适用项。\n\nA. 给定所述的特征值和 $\\sigma_n^2 \\approx 10^{-14}$，数据 Gram 矩阵 $K_y$ 的谱条件数 $\\kappa_2(K_y)$ 约为 $10^{12}$ 量级，这个值在双精度计算中足够大，会导致随机模拟所使用的分解方法出现精度损失和不稳定性。\n\nB. 通过添加一个抖动项 $\\delta I_n$（等价于将 $\\sigma_n^2$ 替换为 $\\sigma_n^2 + \\delta$）来增加噪声水平，会使 $\\kappa_2(K_y)$ 作为 $\\delta$ 的函数严格递减。在给定的特征值下，实现 $\\kappa_2(K_y) \\leq 10^8$ 的最小 $\\delta$ 约等于 $1.2 \\times 10^{-8}$。\n\nC. 通过将核函数 $k(x,x')$ 与一个在 $x=x'$ 时等于 $1$ 但在其他情况下小于 $1$ 的紧支集函数相乘来进行锥削，总会降低 $\\kappa_2(K_y)$，并保持训练输入点上的预测均值不变。\n\nD. 将 $K$ 替换为一个低秩近似 $U U^\\top$，其中 $U \\in \\mathbb{R}^{n \\times m}$ 且 $m \\ll n$（例如，通过 Nyström 方法或主元 Cholesky 分解），然后使用 $U U^\\top + \\sigma_n^2 I_n$，可以改善条件数；一个有原则的截断规则是监控被丢弃的谱，并选择 $m$，使得被忽略的特征值之和低于一个与 $\\varepsilon_{\\mathrm{mach}}$ 乘以 $\\operatorname{trace}(K)$ 成比例的容差。\n\nE. 对于平方指数核，当保持 $\\ell$ 固定时，对输入进行重缩放 $x_i \\mapsto a x_i$ 会对任何 $a > 0$ 保持 $K_y$ 的谱条件数不变。\n\nF. 在用于超参数推断的 MCMC 中，一个病态的 $K_y$ 通常会改善混合效果，因为对数边缘似然在接近奇异的配置附近变得更平坦，从而减少了链中的自相关。",
            "solution": "首先验证问题陈述，以确保其在科学上是合理的、适定的和完整的。\n\n### 第一步：提取给定条件\n- **模型**：高斯过程回归。\n- **先验**：零均值，$f \\sim \\mathcal{GP}(0,k)$。\n- **协方差函数**：$k(x,x')$，正定。\n- **输入**：$\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$，$n=300$。输入是“高度聚类的”。\n- **观测模型**：$y_i = f(x_i) + \\epsilon_i$。\n- **噪声**：$\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$，独立。\n- **Gram 矩阵**：$K \\in \\mathbb{R}^{n \\times n}$，其元素为 $K_{ij} = k(x_i,x_j)$。\n- **数据 Gram 矩阵**：$K_y = K + \\sigma_n^2 I_n$。\n- **条件数**：$\\kappa_2(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$，对于对称正定矩阵 A。\n- **机器精度**：$\\varepsilon_{\\mathrm{mach}} \\approx 2.22 \\times 10^{-16}$。\n- **特定核函数**：平方指数核，$k(x,x') = \\sigma_f^2 \\exp\\!\\big(-\\|x-x'\\|^2/(2\\ell^2)\\big)$。\n- **超参数**：$\\sigma_f^2 = 1$，短的长度尺度 $\\ell$。\n- **$K$ 的特征值**：$\\lambda_{\\max}(K) \\approx 1.2$，$\\lambda_{\\min}(K) \\approx 10^{-12}$。\n- **噪声方差**：$\\sigma_n^2 \\approx 10^{-14}$。\n\n### 第二步：使用提取的给定条件进行验证\n问题陈述植根于高斯过程回归和数值线性代数的标准数学框架。所描述的场景——由聚类数据点和平方指数核产生的近奇异 Gram 矩阵——是高斯过程实际应用中一个常见且现实的问题。提供的特征值和噪声的数值是合理的，并且与此场景一致。没有科学或事实上的不合理之处，没有矛盾，并且问题是适定的。术语是标准的且无歧义。问题是有效的。\n\n### 第三步：结论与行动\n问题有效。我们开始进行解答。\n\n---\n\n### 对有效问题陈述的分析\n\n问题的核心在于数据 Gram 矩阵 $K_y = K + \\sigma_n^2 I_n$ 的性质。$K_y$ 的特征值是 $\\lambda_i(K_y) = \\lambda_i(K) + \\sigma_n^2$，因为加上一个单位矩阵的倍数只会平移谱。因此，谱条件数为：\n$$ \\kappa_2(K_y) = \\frac{\\lambda_{\\max}(K_y)}{\\lambda_{\\min}(K_y)} = \\frac{\\lambda_{\\max}(K) + \\sigma_n^2}{\\lambda_{\\min}(K) + \\sigma_n^2} $$\n我们已知 $\\lambda_{\\max}(K) \\approx 1.2$, $\\lambda_{\\min}(K) \\approx 10^{-12}$，以及 $\\sigma_n^2 \\approx 10^{-14}$。\n\n### 逐项分析\n\n**A. 给定所述的特征值和 $\\sigma_n^2 \\approx 10^{-14}$，数据 Gram 矩阵 $K_y$ 的谱条件数 $\\kappa_2(K_y)$ 约为 $10^{12}$ 量级，这个值在双精度计算中足够大，会导致随机模拟所使用的分解方法出现精度损失和不稳定性。**\n\n让我们用给定的值计算条件数：\n$$ \\lambda_{\\max}(K_y) = \\lambda_{\\max}(K) + \\sigma_n^2 \\approx 1.2 + 10^{-14} \\approx 1.2 $$\n$$ \\lambda_{\\min}(K_y) = \\lambda_{\\min}(K) + \\sigma_n^2 \\approx 10^{-12} + 10^{-14} = 10^{-12} + 0.01 \\times 10^{-12} = 1.01 \\times 10^{-12} $$\n条件数为：\n$$ \\kappa_2(K_y) = \\frac{\\lambda_{\\max}(K_y)}{\\lambda_{\\min}(K_y)} \\approx \\frac{1.2}{1.01 \\times 10^{-12}} \\approx 1.19 \\times 10^{12} $$\n这个值在 $10^{12}$ 量级。在双精度算术中，$\\varepsilon_{\\mathrm{mach}} \\approx 10^{-16}$，一个普遍的经验法则是，在求解线性系统时，预计会损失大约 $\\log_{10}(\\kappa_2)$ 位的精度。这里，$\\log_{10}(1.19 \\times 10^{12}) \\approx 12.08$。考虑到大约有 16 位可用精度数字，这只剩下大约 $16 - 12 = 4$ 个有效数字，构成了严重的精度损失。这种程度的病态使得像 Cholesky 分解这样的标准算法在数值上不稳定。该陈述是准确的。\n\n**结论：正确**\n\n**B. 通过添加一个抖动项 $\\delta I_n$（等价于将 $\\sigma_n^2$ 替换为 $\\sigma_n^2 + \\delta$）来增加噪声水平，会使 $\\kappa_2(K_y)$ 作为 $\\delta$ 的函数严格递减。在给定的特征值下，实现 $\\kappa_2(K_y) \\leq 10^8$ 的最小 $\\delta$ 约等于 $1.2 \\times 10^{-8}$。**\n\n设新的条件数是所加抖动项 $\\delta > 0$ 的函数：\n$$ \\kappa(\\delta) = \\frac{\\lambda_{\\max}(K) + \\sigma_n^2 + \\delta}{\\lambda_{\\min}(K) + \\sigma_n^2 + \\delta} $$\n为了检查这个函数是否严格递减，我们考察它关于 $\\delta$ 的导数。设 $a = \\lambda_{\\max}(K) + \\sigma_n^2$ 且 $b = \\lambda_{\\min}(K) + \\sigma_n^2$。\n$$ \\frac{d\\kappa}{d\\delta} = \\frac{d}{d\\delta}\\left(\\frac{a + \\delta}{b + \\delta}\\right) = \\frac{1(b+\\delta) - 1(a+\\delta)}{(b+\\delta)^2} = \\frac{b-a}{(b+\\delta)^2} $$\n因为 $K$ 是正定的，并且不是单位矩阵的标量倍，所以 $\\lambda_{\\max}(K) > \\lambda_{\\min}(K)$，这意味着 $a > b$。因此，$b-a  0$，导数对所有 $\\delta \\ge 0$ 都是严格为负的。所以，$\\kappa(\\delta)$ 是 $\\delta$ 的一个严格递减函数。\n\n现在，我们找到实现 $\\kappa_2(K_y) \\le 10^8$ 的最小 $\\delta$。由于函数是递减的，这发生在边界上：\n$$ \\frac{\\lambda_{\\max}(K) + \\sigma_n^2 + \\delta}{\\lambda_{\\min}(K) + \\sigma_n^2 + \\delta} = 10^8 $$\n使用给定的值：\n$$ \\frac{1.2 + 10^{-14} + \\delta}{10^{-12} + 10^{-14} + \\delta} = 10^8 $$\n$$ 1.2 + 10^{-14} + \\delta = 10^8 (1.01 \\times 10^{-12} + \\delta) = 1.01 \\times 10^{-4} + 10^8 \\delta $$\n$$ 1.2 - 1.01 \\times 10^{-4} \\approx (10^8 - 1) \\delta $$\n$$ \\delta \\approx \\frac{1.2}{10^8} = 1.2 \\times 10^{-8} $$\n计算证实了近似值。陈述的两个部分都是准确的。\n\n**结论：正确**\n\n**C. 通过将核函数 $k(x,x')$ 与一个在 $x=x'$ 时等于 $1$ 但在其他情况下小于 $1$ 的紧支集函数相乘来进行锥削，总会降低 $\\kappa_2(K_y)$，并保持训练输入点上的预测均值不变。**\n\n这个陈述有两个论断。我们先分析第二个：锥削“保持训练输入点上的预测均值不变”。在训练输入点 $\\{x_i\\}_{i=1}^n$ 上的预测均值由向量 $\\mathbf{m}_{\\text{train}} = K (K_y)^{-1} \\mathbf{y} = K (K + \\sigma_n^2 I_n)^{-1} \\mathbf{y}$ 给出。这可以重写为 $\\mathbf{m}_{\\text{train}} = (I_n - \\sigma_n^2 (K + \\sigma_n^2 I_n)^{-1})\\mathbf{y}$。锥削将核矩阵 $K$ 替换为一个新矩阵 $K'$，其中 $K'_{ij} = K_{ij} T_{ij}$，$T$ 是一个锥削矩阵，满足 $T_{ii}=1$ 且对于 $i \\ne j$ 有 $|T_{ij}| \\le 1$。由于 $T$ 不是全 1 矩阵，所以 $K' \\neq K$。锥削后的预测均值为 $\\mathbf{m}'_{\\text{train}} = K' (K' + \\sigma_n^2 I_n)^{-1} \\mathbf{y}$。由于均值的整个表达式都依赖于完整的矩阵 $K$，将其更改为 $K'$ 通常会改变预测均值。预测均值没有保持不变。由于陈述的一部分是错误的，整个陈述都是不正确的。\n\n**结论：不正确**\n\n**D. 将 $K$ 替换为一个低秩近似 $U U^\\top$，其中 $U \\in \\mathbb{R}^{n \\times m}$ 且 $m \\ll n$（例如，通过 Nyström 方法或主元 Cholesky 分解），然后使用 $U U^\\top + \\sigma_n^2 I_n$，可以改善条件数；一个有原则的截断规则是监控被丢弃的谱，并选择 $m$，使得被忽略的特征值之和低于一个与 $\\varepsilon_{\\mathrm{mach}}$ 乘以 $\\operatorname{trace}(K)$ 成比例的容差。**\n\n基于低秩方法的高斯过程近似是提高可扩展性和数值稳定性的标准技术。$n \\times n$ 矩阵 $K_y = UU^\\top + \\sigma_n^2 I_n$ 的逆可以使用 Woodbury 矩阵恒等式高效地计算：\n$$ (UU^\\top + \\sigma_n^2 I_n)^{-1} = \\frac{1}{\\sigma_n^2}I_n - \\frac{1}{\\sigma_n^2} U \\left( \\sigma_n^2 I_m + U^\\top U \\right)^{-1} U^\\top $$\n这需要求一个 $m \\times m$ 矩阵 $\\sigma_n^2 I_m + U^\\top U$ 的逆。在像 Nyström 这样的方法中，$U^\\top U$ 的特征值对应于 $K$ 的最大的一些特征值。在 $m \\times m$ 求逆中涉及的最小特征值将远大于 $\\lambda_{\\min}(K)$，从而导致一个条件更好的矩阵求逆问题。因此，这些方法通过用一个良态的 $m \\times m$ 问题替换病态的 $n \\times n$ 问题来改善条件数。\n\n所提出的截断规则是，选择秩 $m$，使得被丢弃的特征值之和 $\\sum_{i=m+1}^n \\lambda_i(K)$ 相对于总特征值之和 $\\operatorname{trace}(K) = \\sum_{i=1}^n \\lambda_i(K)$ 较小。将此相对误差的容差设置为机器精度的量级，即误差 $\\le C \\cdot \\varepsilon_{\\mathrm{mach}} \\operatorname{trace}(K)$，是数值线性代数中创建精确到机器精度的近似值时一种标准且有原则的启发式方法。该陈述是准确的。\n\n**结论：正确**\n\n**E. 对于平方指数核，当保持 $\\ell$ 固定时，对输入进行重缩放 $x_i \\mapsto a x_i$ 会对任何 $a > 0$ 保持 $K_y$ 的谱条件数不变。**\n\n设原始核函数为 $k(x,x') = \\sigma_f^2 \\exp(-\\|x-x'\\|^2 / (2\\ell^2))$。将输入重缩放为 $z_i = a x_i$ 后，新的核矩阵 $K'$ 的元素为：\n$$ K'_{ij} = k(z_i, z_j) = \\sigma_f^2 \\exp(-\\|z_i-z_j\\|^2 / (2\\ell^2)) = \\sigma_f^2 \\exp(-\\|a(x_i-x_j)\\|^2 / (2\\ell^2)) = \\sigma_f^2 \\exp(-a^2\\|x_i-x_j\\|^2 / (2\\ell^2)) $$\n这等效于使用原始输入和一个新的长度尺度 $\\ell' = \\ell/a$。Gram 矩阵 $K$ 对长度尺度参数高度敏感。例如，当 $\\ell \\to \\infty$ 时，$K \\to \\sigma_f^2 \\mathbf{1}\\mathbf{1}^\\top$（一个秩为 1 的矩阵），这是非常病态的。当 $\\ell \\to 0$ 时，$K \\to \\sigma_f^2 I_n$，这是完美条件的（$\\kappa_2(K)=1$）。由于重缩放输入等效于改变长度尺度，而 $K$ 的谱（以及 $\\kappa_2(K)$ 和 $\\kappa_2(K_y)$）强烈依赖于长度尺度，因此条件数不会被保持。\n\n**结论：不正确**\n\n**F. 在用于超参数推断的 MCMC 中，一个病态的 $K_y$ 通常会改善混合效果，因为对数边缘似然在接近奇异的配置附近变得更平坦，从而减少了链中的自相关。**\n\nGP 回归的对数边缘似然 (LML) 由 $\\log p(\\mathbf{y}|\\theta) = -\\frac{1}{2}\\mathbf{y}^\\top K_y^{-1} \\mathbf{y} - \\frac{1}{2}\\log|K_y| - \\frac{n}{2}\\log(2\\pi)$ 给出。当 $K_y$ 变得病态时，其最小特征值接近于零，导致其行列式 $|K_y| = \\prod_i \\lambda_i(K_y)$ 也接近于零。因此，$\\log|K_y|$ 趋近于 $-\\infty$。这会在超参数 $\\theta$ 空间中的对数边缘似然曲面上产生极其陡峭的梯度和尖锐、狭窄的谷或脊。这样的拓扑结构对于 MCMC 采样器来说是出了名的难以探索。采样器往往会卡在这些区域，导致混合非常缓慢和链中的高自相关。LML 曲面变得更加崎岖和“尖锐”，而不是更平坦。因此，病态严重阻碍了 MCMC 的性能，而不是改善它。\n\n**结论：不正确**",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "直接对 $K$ 求逆的计算成本为 $\\mathcal{O}(n^3)$，这对于大型数据集是不可接受的，因此近似方法对于提升可扩展性至关重要。变分推断中的“引入点”(inducing points)方法是一种强大且流行的方法，它通过使用一小组“引入”点来降低计算复杂度。这个练习 () 专注于分析该方法的计算复杂度，让你深入了解它为何高效，以及现代高斯过程软件是如何处理大规模问题的。",
            "id": "3309550",
            "problem": "考虑一个零均值高斯过程回归模型，其先验为 $f \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot))$，输入为 $X \\in \\mathbb{R}^{n \\times d}$，输出为 $\\mathbf{y} \\in \\mathbb{R}^{n}$，并具有高斯似然 $p(\\mathbf{y} \\mid \\mathbf{f}) = \\prod_{i=1}^{n} \\mathcal{N}(y_{i} \\mid f(x_{i}), \\sigma^{2})$。引入 $m \\ll n$ 个诱导点位置 $Z \\in \\mathbb{R}^{m \\times d}$，并记 $\\mathbf{u} = f(Z)$，其先验为 $p(\\mathbf{u}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{K}_{ZZ})$，其中 $\\mathbf{K}_{ZZ} \\in \\mathbb{R}^{m \\times m}$ 是在 $Z$ 上的核矩阵。考虑使用高斯变分分布 $q(\\mathbf{u}) = \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$ 的变分诱导点方法，以及通过联合高斯分布的条件分布对 $q(\\mathbf{u})$ 进行边缘化得到的变分后验 $q(\\mathbf{f})$。训练目标是证据下界（ELBO），定义为在 $q(\\mathbf{f})$下的期望对数似然减去 $q(\\mathbf{u})$ 和 $p(\\mathbf{u})$ 之间的Kullback-Leibler散度。假设协方差 $\\mathbf{S}$ 是稠密的，并且每对 $(x, z)$ 的核函数求值花费与 $m$ 和 $n$ 无关的常数时间。\n\n您将在此设置下分析训练和预测的计算复杂度。请使用以下基本依据和事实：\n\n1.  在先验下，$(\\mathbf{u}, f(x))$ 的联合高斯结构决定了条件分布 $p(f(x) \\mid \\mathbf{u})$ 是高斯的，并且线性高斯变换保持高斯性，允许通过矩阵乘法和三角系统求解得到均值和方差的闭式解。\n2.  对于稠密的 $m \\times m$ 矩阵，Cholesky分解的成本为 $\\mathcal{O}(m^{3})$，求解三角系统的成本为每个右侧向量 $\\mathcal{O}(m^{2})$。一个 $m \\times m$ 的稠密矩阵与一个 $m$ 维向量相乘的成本为 $\\mathcal{O}(m^{2})$，一个 $m$ 维向量与一个已通过Cholesky分解的 $m \\times m$ 逆矩阵相乘的成本为每次求解 $\\mathcal{O}(m^{2})$。\n3.  ELBO分解为对数据求和的期望对数似然项，加上一个与数据无关的Kullback-Leibler项。小批量随机优化使用数据的随机子集，并重缩放其贡献以形成对总和的无偏估计量。\n\n基于这些依据推导：\n\na) 当使用稠密 $\\mathbf{S}$ 和基于 $\\mathbf{K}_{ZZ}$ 的预计算时，在对数据进行一次遍历以评估ELBO及其关于变分参数和核超参数的梯度时，全批量训练时间复杂度作为 $n$ 和 $m$ 的函数。\n\nb) 在 $q(\\mathbf{f})$ 下计算预测均值和预测方差的每个测试点的计算成本。\n\nc) 在使用重参数化技巧进行变分期望计算，并且每个期望对数似然项的单样本蒙特卡洛估计量是无偏的假设下，就小批量随机优化如何通过对数据索引进行均匀随机子采样和重缩放来产生ELBO的无偏梯度估计量，给出一个有理有据的证明。\n\n您的推导必须从所列出的依据出发，推断所需的线性代数运算序列，并仔细考虑 $n$ 和 $m$ 中的主导项，而不使用任何快捷公式。假设除了稠密矩阵外没有其他特殊结构，并且不假设 $\\mathbf{S}$ 是对角或低秩的。\n\n请将您的最终答案指定为一个单行矩阵，其中包含三个条目，顺序为：训练复杂度、每个测试点的预测均值成本、每个测试点的预测方差成本。每个条目必须是一个大O表达式。无需进行数值四舍五入。",
            "solution": "该问题要求分析变分诱导点高斯过程模型的训练和预测的计算复杂度，并证明基于小批量的随机梯度估计量的无偏性。分析将分三部分进行，对应于三个任务(a)、(b)和(c)。\n\na) 一次遍历的全批量训练时间复杂度\n\n训练目标是证据下界（ELBO），我们用 $L$ 表示。其定义为：\n$$\nL = \\mathbb{E}_{q(\\mathbf{f})}[\\ln p(\\mathbf{y} \\mid \\mathbf{f})] - \\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})]\n$$\n给定分解的似然 $p(\\mathbf{y} \\mid \\mathbf{f}) = \\prod_{i=1}^{n} p(y_{i} \\mid f(x_{i}))$，期望对数似然项可以写成对 $n$ 个数据点的求和：\n$$\nL = \\sum_{i=1}^{n} \\mathbb{E}_{q(f_i)}[\\ln p(y_i \\mid f_i)] - \\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})]\n$$\n我们分析对 $n$ 个数据点进行一次完整遍历时，评估两个主要部分（求和项和KL散度）及其梯度的计算成本。\n\n首先，我们分析Kullback-Leibler散度项，$\\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})]$。变分分布是 $q(\\mathbf{u}) = \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$，先验是 $p(\\mathbf{u}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{K}_{ZZ})$。这两个多元高斯分布之间的KL散度为：\n$$\n\\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})] = \\frac{1}{2} \\left[ \\text{Tr}(\\mathbf{K}_{ZZ}^{-1}\\mathbf{S}) + \\mathbf{m}^T \\mathbf{K}_{ZZ}^{-1}\\mathbf{m} - m + \\ln\\det(\\mathbf{K}_{ZZ}) - \\ln\\det(\\mathbf{S}) \\right]\n$$\n此表达式各部分的计算成本如下：\n1.  $\\mathbf{K}_{ZZ} \\in \\mathbb{R}^{m \\times m}$ 的Cholesky分解：这是为了高效求解和行列式计算所必需的预计算。成本为 $\\mathcal{O}(m^3)$。设 $\\mathbf{K}_{ZZ} = \\mathbf{L}_{ZZ}\\mathbf{L}_{ZZ}^T$。\n2.  $\\ln\\det(\\mathbf{K}_{ZZ})$：在Cholesky分解后，其值为 $2 \\sum_{j=1}^{m} \\ln((\\mathbf{L}_{ZZ})_{jj})$，成本为 $\\mathcal{O}(m)$。\n3.  $\\ln\\det(\\mathbf{S})$：由于 $\\mathbf{S}$ 是稠密的，我们计算其Cholesky分解 $\\mathbf{S} = \\mathbf{L}_{S}\\mathbf{L}_{S}^T$，成本为 $\\mathcal{O}(m^3)$，然后以 $\\mathcal{O}(m)$ 的成本计算对数行列式。\n4.  $\\mathbf{m}^T \\mathbf{K}_{ZZ}^{-1}\\mathbf{m}$：这通过首先求解系统 $\\mathbf{K}_{ZZ}\\boldsymbol{\\alpha} = \\mathbf{m}$ 以得到 $\\boldsymbol{\\alpha}$，然后计算点积 $\\mathbf{m}^T\\boldsymbol{\\alpha}$ 来计算。使用Cholesky因子 $\\mathbf{L}_{ZZ}$，求解该系统需要两次三角求解，成本为 $\\mathcal{O}(m^2)$。随后的点积成本为 $\\mathcal{O}(m)$。总成本为 $\\mathcal{O}(m^2)$。\n5.  $\\text{Tr}(\\mathbf{K}_{ZZ}^{-1}\\mathbf{S})$：这是最昂贵的操作。一种直接的方法是计算 $\\mathbf{V} = \\mathbf{K}_{ZZ}^{-1}\\mathbf{S}$，然后取其迹。计算 $\\mathbf{V}$ 需要对 $\\mathbf{S}$ 的每一列 $\\mathbf{s}_j$ 求解 $m$ 个形如 $\\mathbf{K}_{ZZ}\\mathbf{v}_j = \\mathbf{s}_j$ 的线性系统。每次求解的成本为 $\\mathcal{O}(m^2)$，导致总成本为 $m \\times \\mathcal{O}(m^2) = \\mathcal{O}(m^3)$。取迹的额外成本为 $\\mathcal{O}(m)$。\n\n总结这些成本，KL项的计算主要由Cholesky分解和迹项主导，其复杂度为 $\\mathcal{O}(m^3)$。\n\n接下来，我们分析期望对数似然项 $\\sum_{i=1}^{n} \\mathbb{E}_{q(f_i)}[\\ln p(y_i \\mid f_i)]$。对于高斯似然 $p(y_i \\mid f_i) = \\mathcal{N}(y_i \\mid f_i, \\sigma^2)$，该期望有一个闭式解，它依赖于边缘变分后验 $q(f_i)$ 的均值 $\\mu_{q,i}$ 和方差 $\\sigma^2_{q,i}$。\n分布 $q(f_i)$ 是通过对联合分布 $p(f_i \\mid \\mathbf{u})q(\\mathbf{u})$ 进行边缘化得到的。根据基本性质1， $p(f_i \\mid \\mathbf{u})$ 是高斯的。设 $\\mathbf{k}_{i} = k(Z, x_i)$。那么 $p(f_i \\mid \\mathbf{u}) = \\mathcal{N}(f_i \\mid \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{u}, k(x_i, x_i) - \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_i)$。由于 $q(\\mathbf{u})$ 是高斯的，所得到的边缘分布 $q(f_i)$ 也是高斯的。其均值和方差为：\n$$\n\\mu_{q,i} = \\mathbb{E}_{q(f_i)}[f_i] = \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{m}\n$$\n$$\n\\sigma^2_{q,i} = \\text{Var}_{q(f_i)}[f_i] = k(x_i, x_i) - \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_i + \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{S} \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_i\n$$\n为计算对 $n$ 个数据点的求和，我们必须为每个 $i \\in \\{1, \\dots, n\\}$ 计算这些量。对于单个数据点 $i$：\n1.  计算核向量 $\\mathbf{k}_i \\in \\mathbb{R}^m$：$m$ 次核函数求值，成本为 $\\mathcal{O}(m)$。\n2.  定义 $\\boldsymbol{\\beta}_i = \\mathbf{K}_{ZZ}^{-1}\\mathbf{k}_i$。这通过使用 $\\mathbf{K}_{ZZ}$ 的预计算Cholesky因子求解一个线性系统来计算。成本为 $\\mathcal{O}(m^2)$。\n3.  计算均值 $\\mu_{q,i} = \\boldsymbol{\\beta}_i^T \\mathbf{m}$。这是一个点积，成本为 $\\mathcal{O}(m)$。\n4.  计算方差分量：\n    *   $k(x_i, x_i)$: $\\mathcal{O}(1)$。\n    *   $\\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_i = \\mathbf{k}_i^T \\boldsymbol{\\beta}_i$：另一个点积，成本 $\\mathcal{O}(m)$。\n    *   $\\boldsymbol{\\beta}_i^T \\mathbf{S} \\boldsymbol{\\beta}_i$：由于 $\\mathbf{S}$ 是稠密的，这被计算为 $(\\boldsymbol{\\beta}_i^T \\mathbf{S}) \\boldsymbol{\\beta}_i$。矩阵-向量积 $\\mathbf{S}^T\\boldsymbol{\\beta}_i$（或 $\\mathbf{S}\\boldsymbol{\\beta}_i$）的成本为 $\\mathcal{O}(m^2)$，随后是一个点积，成本为 $\\mathcal{O}(m)$。总计为 $\\mathcal{O}(m^2)$。\n\n计算一个数据点 $i$ 的均值和方差的总成本主要由求解 $\\boldsymbol{\\beta}_i$ 和涉及 $\\mathbf{S}$ 的二次型主导，为 $\\mathcal{O}(m^2)$。要计算完整的期望对数似然项，我们必须对所有 $n$ 个数据点重复此操作，导致总成本为 $\\mathcal{O}(nm^2)$。\n\n结合单次全批量遍历的成本：整个ELBO的计算需要一次性的 $\\mathcal{O}(m^3)$ 成本用于 $\\mathbf{K}_{ZZ}$ 的Cholesky分解，以及另一次 $\\mathcal{O}(m^3)$ 成本用于KL项的计算，再加上数据相关项的 $\\mathcal{O}(nm^2)$ 成本。总成本为 $\\mathcal{O}(nm^2 + m^3)$。使用现代自动微分框架计算关于变分参数和核超参数的梯度涉及一个反向传播过程，其计算复杂度与前向传播过程渐近相同。因此，一个训练步骤（ELBO及其梯度）的复杂度是 $\\mathcal{O}(nm^2 + m^3)$。\n\nb) 每个测试点的预测成本\n\n对于一个新的测试点 $x_*$，我们计算预测分布 $q(f_*) = \\int p(f_* \\mid \\mathbf{u})q(\\mathbf{u})d\\mathbf{u}$。这是一个高斯分布，其均值和方差可以以闭式形式计算。我们假设模型已经训练好，所以变分参数 $\\mathbf{m}$、$\\mathbf{S}$ 和核超参数是固定的。训练后常见的预计算包括 $\\mathbf{K}_{ZZ}$ 的Cholesky因子。\n\n预测均值为：\n$$\n\\mu_{q,*} = \\mathbb{E}_{q(f_*)}[f_*] = \\mathbf{k}_*^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{m}\n$$\n其中 $\\mathbf{k}_* = k(Z, x_*)$。为了高效地计算它，我们可以在训练后一次性预计算向量 $\\boldsymbol{\\alpha} = \\mathbf{K}_{ZZ}^{-1}\\mathbf{m}$，成本为 $\\mathcal{O}(m^2)$。然后，对于每个测试点 $x_*$：\n1.  计算核向量 $\\mathbf{k}_*$: $\\mathcal{O}(m)$。\n2.  计算点积 $\\mu_{q,*} = \\mathbf{k}_*^T \\boldsymbol{\\alpha}$: $\\mathcal{O}(m)$。\n计算每个测试点的预测均值的总计算成本是 $\\mathcal{O}(m)$。\n\n预测方差为：\n$$\n\\sigma^2_{q,*} = \\text{Var}_{q(f_*)}[f_*] = (k_{**} - \\mathbf{k}_*^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_*) + \\mathbf{k}_*^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{S} \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_*\n$$\n其中 $k_{**} = k(x_*, x_*)$。为单个测试点计算它的成本是：\n1.  分别计算 $k_{**}$ 和 $\\mathbf{k}_*$: 成本分别为 $\\mathcal{O}(1)$ 和 $\\mathcalO(m)$。\n2.  定义 $\\boldsymbol{\\beta}_* = \\mathbf{K}_{ZZ}^{-1}\\mathbf{k}_*$. 使用 $\\mathbf{K}_{ZZ}$ 的Cholesky因子求解该系统：$\\mathcal{O}(m^2)$。\n3.  计算第一个方差项 $k_{**} - \\mathbf{k}_*^T \\boldsymbol{\\beta}_*$: $\\mathcal{O}(m)$。\n4.  计算第二个方差项 $\\boldsymbol{\\beta}_*^T \\mathbf{S} \\boldsymbol{\\beta}_*$：由于 $\\mathbf{S}$ 是一个稠密的 $m \\times m$ 矩阵，这需要成本 $\\mathcal{O}(m^2)$。\n计算每个测试点的预测方差的总计算成本主要由系统求解和与 $\\mathbf{S}$ 的二次型主导，其复杂度为 $\\mathcal{O}(m^2)$。\n\nc) 小批量梯度估计量无偏性的证明\n\nELBO可以写成每数据点贡献的总和加上一个与数据无关的项：\n$$\nL(\\boldsymbol{\\phi}) = \\sum_{i=1}^{n} L_i(\\boldsymbol{\\phi}) - \\text{KL}(\\boldsymbol{\\phi})\n$$\n其中 $L_i(\\boldsymbol{\\phi}) = \\mathbb{E}_{q(f_i)}[\\ln p(y_i \\mid f_i)]$ 且 $\\text{KL}(\\boldsymbol{\\phi}) = \\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})]$。模型的参数（变分参数和核参数）用 $\\boldsymbol{\\phi}$ 表示。\n\n根据梯度算子的线性性质，ELBO关于这些参数的梯度是：\n$$\n\\nabla_{\\boldsymbol{\\phi}} L(\\boldsymbol{\\phi}) = \\nabla_{\\boldsymbol{\\phi}} \\left(\\sum_{i=1}^{n} L_i(\\boldsymbol{\\phi})\\right) - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi}) = \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi})\n$$\n在小批量随机优化中，我们不计算对所有 $n$ 个数据点的完整总和。相反，我们从索引集 $\\{1, \\dots, n\\}$ 中均匀随机地抽取一个大小为 $S \\ll n$ 的小批量 $B$。随机梯度估计量 $\\hat{\\mathbf{g}}(\\boldsymbol{\\phi})$ 被构造为：\n$$\n\\hat{\\mathbf{g}}(\\boldsymbol{\\phi}) = \\frac{n}{S} \\sum_{j \\in B} \\nabla_{\\boldsymbol{\\phi}} L_j(\\boldsymbol{\\phi}) - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi})\n$$\n关键是重缩放因子 $\\frac{n}{S}$。为了证明该估计量是无偏的，我们对其关于小批量 $B$ 的随机选择取期望：\n$$\n\\mathbb{E}_{B}[\\hat{\\mathbf{g}}(\\boldsymbol{\\phi})] = \\mathbb{E}_{B} \\left[ \\frac{n}{S} \\sum_{j \\in B} \\nabla_{\\boldsymbol{\\phi}} L_j(\\boldsymbol{\\phi}) \\right] - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi})\n$$\nKL项的梯度与数据无关，所以其期望就是它本身。我们关注缩放后总和的期望。设 $I_j$ 为一个指示随机变量，如果索引 $j$ 在小批量 $B$ 中，则 $I_j=1$，否则 $I_j=0$。该总和可以重写为 $\\sum_{i=1}^{n} I_i \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi})$。利用期望的线性性质：\n$$\n\\mathbb{E}_{B} \\left[ \\frac{n}{S} \\sum_{i=1}^{n} I_i \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) \\right] = \\frac{n}{S} \\sum_{i=1}^{n} \\mathbb{E}_{B}[I_i] \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi})\n$$\n期望 $\\mathbb{E}_{B}[I_i]$ 是数据点 $i$ 被包含在从 $n$ 个项目中均匀采样的 $S$ 大小的小批量 $B$ 中的概率。这个概率是 $P(i \\in B) = \\frac{S}{n}$。将其代回：\n$$\n\\frac{n}{S} \\sum_{i=1}^{n} \\left(\\frac{S}{n}\\right) \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) = \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi})\n$$\n这恰好是完整梯度的精确总和。因此，我们已经证明：\n$$\n\\mathbb{E}_{B}[\\hat{\\mathbf{g}}(\\boldsymbol{\\phi})] = \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi}) = \\nabla_{\\boldsymbol{\\phi}} L(\\boldsymbol{\\phi})\n$$\n该估计量是无偏的。使用重参数化技巧允许通过将梯度移入期望内部来计算 $\\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) = \\nabla_{\\boldsymbol{\\phi}} \\mathbb{E}_{q(f_i)}[\\ln p(y_i|f_i)]$。如果对此项使用单样本蒙特卡洛估计，其无偏性保证了对蒙特卡洛噪声的期望与对小批量采样的期望相结合，仍然能得到对总梯度 $\\nabla_{\\boldsymbol{\\phi}} L(\\boldsymbol{\\phi})$ 的无偏估计。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\mathcal{O}(nm^2 + m^3)  \\mathcal{O}(m)  \\mathcal{O}(m^2)\n\\end{pmatrix}\n}\n$$"
        }
    ]
}