{
    "hands_on_practices": [
        {
            "introduction": "The Karhunen–Loève (KL) expansion offers a foundational perspective on Gaussian processes, representing them as a weighted sum of orthogonal basis functions derived from the covariance kernel itself. This exercise provides a hands-on opportunity to build a Gaussian process from its spectral components, deepening your understanding of the link between a kernel and the function space it defines. By deriving the KL expansion for the canonical Wiener process, you will see how its characteristic random walk behavior emerges directly from the kernel's spectral properties .",
            "id": "3309595",
            "problem": "Consider a zero-mean Gaussian process (GP) $f$ on the compact domain $[0,1]$ equipped with the Lebesgue measure, with a continuous, symmetric, positive semi-definite covariance kernel $k(x,x')$. Define the associated Hilbert–Schmidt integral operator $\\mathcal{T}:L^{2}([0,1])\\to L^{2}([0,1])$ by\n$$\n(\\mathcal{T}g)(x) \\equiv \\int_{0}^{1} k(x,t)\\, g(t)\\, dt,\n$$\nwhich is self-adjoint, positive, and compact. Let $\\{(\\lambda_{j},\\phi_{j})\\}_{j\\geq 1}$ be the eigenpairs of $\\mathcal{T}$, where the eigenfunctions $\\{\\phi_{j}\\}_{j\\geq 1}$ form an orthonormal basis of $L^{2}([0,1])$ and the eigenvalues satisfy $\\lambda_{j}\\geq 0$ with $\\sum_{j\\geq 1}\\lambda_{j}<\\infty$. The Mercer decomposition then states that $k(x,x')$ admits the uniformly convergent series $k(x,x')=\\sum_{j\\geq 1}\\lambda_{j}\\phi_{j}(x)\\phi_{j}(x')$.\n\nDefine the truncated Karhunen–Loève (KL) expansion for $f$ by\n$$\nf_{M}(x) \\equiv \\sum_{j=1}^{M} \\sqrt{\\lambda_{j}}\\, \\xi_{j}\\, \\phi_{j}(x),\n$$\nwhere $\\{\\xi_{j}\\}_{j\\geq 1}$ are independent and identically distributed standard normal random variables, $\\xi_{j}\\sim \\mathcal{N}(0,1)$. Let the truncation error be the mean-square $L^{2}([0,1])$ error $\\mathbb{E}\\big[\\|f-f_{M}\\|_{L^{2}}^{2}\\big]$, where $\\|g\\|_{L^{2}}^{2}=\\int_{0}^{1} g(x)^{2}\\, dx$.\n\nSpecialize to the kernel $k(x,x')=\\min(x,x')$ on $[0,1]$. Starting from the spectral definition above and without assuming any shortcut formulas, derive the eigenpairs $(\\lambda_{j},\\phi_{j})$, construct $f_{M}(x)$, and then compute the exact analytic expression for the truncation error $\\mathbb{E}\\big[\\|f-f_{M}\\|_{L^{2}}^{2}\\big]$ as a function of $M$. You may express your final result using classical special functions, provided they are explicitly defined in your derivation.\n\nNo numerical approximation is required; provide the final answer as a single closed-form analytic expression. If you introduce any acronyms (for example, Monte Carlo (MC)), spell out their full names on first use.",
            "solution": "The primary goal is to compute the truncation error $\\mathbb{E}\\big[\\|f-f_{M}\\|_{L^{2}}^{2}\\big]$. This quantity depends on the eigenvalues of the integral operator $\\mathcal{T}$. Thus, the first step is to solve the eigenvalue problem for the specified kernel.\n\n**Part 1: Derivation of the Eigenpairs $(\\lambda_j, \\phi_j)$**\n\nThe eigenvalue problem for the integral operator $\\mathcal{T}$ is given by the Fredholm integral equation of the second kind:\n$$ (\\mathcal{T}\\phi)(x) = \\lambda \\phi(x) $$\nSubstituting the kernel $k(x,t) = \\min(x,t)$, we have:\n$$ \\int_{0}^{1} \\min(x,t) \\phi(t) dt = \\lambda \\phi(x) $$\nWe can split the integral based on the definition of the minimum function:\n$$ \\int_{0}^{x} t \\phi(t) dt + \\int_{x}^{1} x \\phi(t) dt = \\lambda \\phi(x) $$\nAssuming $\\phi$ is sufficiently smooth, we can differentiate this equation with respect to $x$ using the Leibniz integral rule. The first differentiation yields:\n$$ \\frac{d}{dx}\\left(\\int_{0}^{x} t \\phi(t) dt\\right) + \\frac{d}{dx}\\left(x \\int_{x}^{1} \\phi(t) dt\\right) = \\lambda \\phi'(x) $$\n$$ x\\phi(x) + \\left(1 \\cdot \\int_{x}^{1} \\phi(t) dt + x \\cdot (-\\phi(x))\\right) = \\lambda \\phi'(x) $$\n$$ x\\phi(x) + \\int_{x}^{1} \\phi(t) dt - x\\phi(x) = \\lambda \\phi'(x) $$\n$$ \\int_{x}^{1} \\phi(t) dt = \\lambda \\phi'(x) $$\nDifferentiating a second time with respect to $x$:\n$$ -\\phi(x) = \\lambda \\phi''(x) $$\nThis gives the second-order ordinary differential equation (ODE):\n$$ \\phi''(x) + \\frac{1}{\\lambda} \\phi(x) = 0 $$\nThe general solution to this ODE is $\\phi(x) = A \\sin(\\frac{x}{\\sqrt{\\lambda}}) + B \\cos(\\frac{x}{\\sqrt{\\lambda}})$.\n\nTo find the constants $A$ and $B$ and the eigenvalues $\\lambda$, we must establish boundary conditions.\n1.  From the original integral equation, setting $x=0$:\n    $$ \\int_{0}^{1} \\min(0,t)\\phi(t)dt = \\lambda \\phi(0) \\implies \\int_{0}^{1} 0 \\cdot \\phi(t) dt = 0 = \\lambda \\phi(0) $$\n    For a non-trivial eigensystem, we seek $\\lambda \\neq 0$, which implies $\\phi(0) = 0$.\n2.  From the once-differentiated equation, setting $x=1$:\n    $$ \\int_{1}^{1} \\phi(t) dt = \\lambda \\phi'(1) \\implies 0 = \\lambda \\phi'(1) $$\n    Again, for $\\lambda \\neq 0$, this implies $\\phi'(1) = 0$.\n\nNow, we apply these boundary conditions to the general solution:\n-   At $x=0$, $\\phi(0) = A \\sin(0) + B \\cos(0) = B$. Since $\\phi(0)=0$, we have $B=0$. The solution simplifies to $\\phi(x) = A \\sin(\\frac{x}{\\sqrt{\\lambda}})$.\n-   The derivative is $\\phi'(x) = \\frac{A}{\\sqrt{\\lambda}} \\cos(\\frac{x}{\\sqrt{\\lambda}})$. At $x=1$, we must have $\\phi'(1) = 0$:\n    $$ \\frac{A}{\\sqrt{\\lambda}} \\cos\\left(\\frac{1}{\\sqrt{\\lambda}}\\right) = 0 $$\n    For a non-trivial eigenfunction, $A \\neq 0$. Thus, we require $\\cos(\\frac{1}{\\sqrt{\\lambda}}) = 0$. This condition is met when the argument is an odd multiple of $\\frac{\\pi}{2}$:\n    $$ \\frac{1}{\\sqrt{\\lambda_j}} = \\frac{(2j-1)\\pi}{2}, \\quad \\text{for } j = 1, 2, 3, \\dots $$\n    Solving for the eigenvalues $\\lambda_j$:\n    $$ \\lambda_j = \\frac{4}{(2j-1)^2 \\pi^2} $$\nThe corresponding eigenfunctions are of the form $\\phi_j(x) = A_j \\sin\\left(\\frac{(2j-1)\\pi}{2} x\\right)$. We determine the constant $A_j$ by the normalization condition $\\|\\phi_j\\|_{L^2}^2 = 1$:\n$$ \\int_{0}^{1} \\phi_j(x)^2 dx = A_j^2 \\int_{0}^{1} \\sin^2\\left(\\frac{(2j-1)\\pi}{2} x\\right) dx = 1 $$\nUsing the identity $\\sin^2(\\theta) = \\frac{1 - \\cos(2\\theta)}{2}$:\n$$ \\int_{0}^{1} \\frac{1}{2}\\left(1 - \\cos((2j-1)\\pi x)\\right) dx = \\frac{1}{2} \\left[ x - \\frac{\\sin((2j-1)\\pi x)}{(2j-1)\\pi} \\right]_{0}^{1} $$\n$$ = \\frac{1}{2} \\left( (1 - \\frac{\\sin((2j-1)\\pi)}{(2j-1)\\pi}) - (0-0) \\right) = \\frac{1}{2}(1-0) = \\frac{1}{2} $$\nSo, $A_j^2 \\cdot \\frac{1}{2} = 1$, which gives $A_j = \\sqrt{2}$. The orthonormal eigenfunctions are:\n$$ \\phi_j(x) = \\sqrt{2} \\sin\\left(\\frac{(2j-1)\\pi}{2} x\\right) $$\n\n**Part 2: Calculation of the Truncation Error**\n\nThe Karhunen-Loève (KL) expansion of the process $f(x)$ is given by $f(x) = \\sum_{j=1}^{\\infty} \\sqrt{\\lambda_j} \\xi_j \\phi_j(x)$. The truncated expansion is $f_M(x) = \\sum_{j=1}^{M} \\sqrt{\\lambda_j} \\xi_j \\phi_j(x)$.\nThe difference is the residual process:\n$$ f(x) - f_M(x) = \\sum_{j=M+1}^{\\infty} \\sqrt{\\lambda_j} \\xi_j \\phi_j(x) $$\nThe squared $L^2$-norm of this difference is:\n$$ \\|f - f_M\\|_{L^2}^2 = \\int_0^1 \\left( \\sum_{j=M+1}^{\\infty} \\sqrt{\\lambda_j} \\xi_j \\phi_j(x) \\right) \\left( \\sum_{k=M+1}^{\\infty} \\sqrt{\\lambda_k} \\xi_k \\phi_k(x) \\right) dx $$\nBy swapping the summation and integration (justified by the uniform convergence of the expansion):\n$$ \\|f - f_M\\|_{L^2}^2 = \\sum_{j=M+1}^{\\infty} \\sum_{k=M+1}^{\\infty} \\sqrt{\\lambda_j \\lambda_k} \\xi_j \\xi_k \\int_0^1 \\phi_j(x) \\phi_k(x) dx $$\nSince the eigenfunctions $\\{\\phi_j\\}$ are orthonormal, we have $\\int_0^1 \\phi_j(x) \\phi_k(x) dx = \\delta_{jk}$, where $\\delta_{jk}$ is the Kronecker delta. The double sum collapses to a single sum:\n$$ \\|f - f_M\\|_{L^2}^2 = \\sum_{j=M+1}^{\\infty} \\lambda_j \\xi_j^2 $$\nThe truncation error is the expectation of this quantity. Using the linearity of expectation:\n$$ \\mathbb{E}\\big[\\|f - f_M\\|_{L^2}^2\\big] = \\mathbb{E}\\left[\\sum_{j=M+1}^{\\infty} \\lambda_j \\xi_j^2\\right] = \\sum_{j=M+1}^{\\infty} \\lambda_j \\mathbb{E}[\\xi_j^2] $$\nThe variables $\\xi_j$ are i.i.d. standard normal, $\\xi_j \\sim \\mathcal{N}(0,1)$. For such a variable, $\\mathbb{E}[\\xi_j] = 0$ and $\\text{Var}(\\xi_j) = 1$. The variance is $\\text{Var}(\\xi_j) = \\mathbb{E}[\\xi_j^2] - (\\mathbb{E}[\\xi_j])^2$, so $1 = \\mathbb{E}[\\xi_j^2] - 0^2$, which gives $\\mathbb{E}[\\xi_j^2] = 1$.\nTherefore, the truncation error is simply the sum of the eigenvalues of the truncated modes:\n$$ \\mathbb{E}\\big[\\|f - f_M\\|_{L^2}^2\\big] = \\sum_{j=M+1}^{\\infty} \\lambda_j $$\n\n**Part 3: Evaluation of the Eigenvalue Sum**\n\nWe now substitute the derived eigenvalues $\\lambda_j = \\frac{4}{(2j-1)^2 \\pi^2}$ into the sum:\n$$ \\mathbb{E}\\big[\\|f - f_M\\|_{L^2}^2\\big] = \\sum_{j=M+1}^{\\infty} \\frac{4}{(2j-1)^2 \\pi^2} = \\frac{4}{\\pi^2} \\sum_{j=M+1}^{\\infty} \\frac{1}{(2j-1)^2} $$\nTo evaluate this sum, we use the trigamma function, which is a special function defined as the second derivative of the logarithm of the gamma function, $\\psi_1(z) = \\frac{d^2}{dz^2} \\ln \\Gamma(z)$. It has a series representation:\n$$ \\psi_1(z) = \\sum_{n=0}^{\\infty} \\frac{1}{(z+n)^2} $$\nLet's rewrite our summation to match this form.\n$$ \\sum_{j=M+1}^{\\infty} \\frac{1}{(2j-1)^2} = \\frac{1}{4} \\sum_{j=M+1}^{\\infty} \\frac{1}{(j - 1/2)^2} $$\nLet the index of summation be $n=j-(M+1)$, so $j = n+M+1$. When $j=M+1$, $n=0$. The sum becomes:\n$$ \\frac{1}{4} \\sum_{n=0}^{\\infty} \\frac{1}{((n+M+1) - 1/2)^2} = \\frac{1}{4} \\sum_{n=0}^{\\infty} \\frac{1}{(n + (M+1/2))^2} $$\nThis perfectly matches the series definition of the trigamma function with argument $z = M+\\frac{1}{2}$. Thus:\n$$ \\sum_{j=M+1}^{\\infty} \\frac{1}{(2j-1)^2} = \\frac{1}{4} \\psi_1\\left(M+\\frac{1}{2}\\right) $$\nSubstituting this back into the expression for the truncation error:\n$$ \\mathbb{E}\\big[\\|f - f_M\\|_{L^2}^2\\big] = \\frac{4}{\\pi^2} \\left( \\frac{1}{4} \\psi_1\\left(M+\\frac{1}{2}\\right) \\right) = \\frac{1}{\\pi^2} \\psi_1\\left(M+\\frac{1}{2}\\right) $$\nThis is the final analytic expression for the truncation error.",
            "answer": "$$ \\boxed{ \\frac{1}{\\pi^2} \\psi_1\\left(M+\\frac{1}{2}\\right) } $$"
        },
        {
            "introduction": "While elegant in theory, the practical application of Gaussian process regression often confronts the challenge of numerical instability. This exercise tackles the common problem of an ill-conditioned covariance matrix, which frequently arises when data points are clustered, making the standard GPR equations difficult or impossible to solve accurately. You will practice diagnosing the severity of this issue using the spectral condition number, $\\kappa_2(K_y)$, and learn to evaluate the effectiveness of standard remedies, such as regularization and low-rank approximations .",
            "id": "3309568",
            "problem": "Consider Gaussian process regression with a zero-mean prior and a positive definite covariance function $k(x,x')$ on inputs $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$. Observations are modeled as $y_i = f(x_i) + \\epsilon_i$ with $f \\sim \\mathcal{GP}(0,k)$ and independent Gaussian noise $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$. The Gram matrix is $K \\in \\mathbb{R}^{n \\times n}$ with entries $K_{ij} = k(x_i,x_j)$, and the data Gram matrix is $K_y = K + \\sigma_n^2 I_n$. For symmetric positive definite matrices, the spectral condition number in the $2$-norm is defined by $\\kappa_2(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$, where $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ denote the largest and smallest eigenvalues of $A$, respectively. Assume double-precision machine epsilon satisfies $\\varepsilon_{\\mathrm{mach}} \\approx 2.22 \\times 10^{-16}$.\n\nSuppose $n = 300$ inputs are heavily clustered (e.g., several tight clusters separated by moderate gaps), and the covariance function $k$ is the squared exponential $k(x,x') = \\sigma_f^2 \\exp\\!\\big(-\\|x-x'\\|^2/(2\\ell^2)\\big)$ with $\\sigma_f^2 = 1$ and a short length-scale $\\ell$. A practitioner computes the extremal eigenvalues of $K$ and finds $\\lambda_{\\max}(K) \\approx 1.2$ and $\\lambda_{\\min}(K) \\approx 10^{-12}$, while the current noise level is $\\sigma_n^2 \\approx 10^{-14}$. The practitioner wishes to diagnose and remedy numerical ill-conditioning issues that affect downstream stochastic simulation, such as sampling from the posterior $\\mathcal{N}(m, K_y)$ via Cholesky factorization or Markov chain Monte Carlo (MCMC) exploration of hyperparameters.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. Given the stated eigenvalues and $\\sigma_n^2 \\approx 10^{-14}$, the data Gram matrix $K_y$ has a spectral condition number $\\kappa_2(K_y)$ of order $10^{12}$, which is large enough in double precision to cause loss of accuracy and instability in factorizations used by stochastic simulation.\n\nB. Increasing the noise level by adding a jitter $\\delta I_n$ (equivalently replacing $\\sigma_n^2$ by $\\sigma_n^2 + \\delta$) strictly decreases $\\kappa_2(K_y)$ as a function of $\\delta$. Under the given eigenvalues, the smallest $\\delta$ that achieves $\\kappa_2(K_y) \\leq 10^8$ is approximately $1.2 \\times 10^{-8}$.\n\nC. Tapering the kernel by multiplying $k(x,x')$ with a compactly supported function that equals $1$ at $x=x'$ but is $ 1$ otherwise always decreases $\\kappa_2(K_y)$ and leaves the predictive mean at training inputs unchanged.\n\nD. Replacing $K$ by a low-rank approximation $U U^\\top$ with $U \\in \\mathbb{R}^{n \\times m}$ and $m \\ll n$ (e.g., via the Nyström method or pivoted Cholesky), and then using $U U^\\top + \\sigma_n^2 I_n$, can improve conditioning; a principled truncation rule is to monitor the discarded spectrum and choose $m$ so that the sum of omitted eigenvalues is below a tolerance proportional to $\\varepsilon_{\\mathrm{mach}}$ times $\\operatorname{trace}(K)$.\n\nE. For the squared exponential kernel, rescaling inputs by $x_i \\mapsto a x_i$ while keeping $\\ell$ fixed preserves the spectral condition number of $K_y$ for any $a > 0$.\n\nF. In MCMC for hyperparameter inference, an ill-conditioned $K_y$ typically improves mixing because the log marginal likelihood becomes flatter near near-singular configurations, reducing autocorrelation in the chain.",
            "solution": "### Analysis of a Valid Problem Statement\n\nThe core of the problem lies in the properties of the data Gram matrix $K_y = K + \\sigma_n^2 I_n$. The eigenvalues of $K_y$ are $\\lambda_i(K_y) = \\lambda_i(K) + \\sigma_n^2$, since adding a multiple of the identity matrix simply shifts the spectrum. The spectral condition number is therefore:\n$$ \\kappa_2(K_y) = \\frac{\\lambda_{\\max}(K_y)}{\\lambda_{\\min}(K_y)} = \\frac{\\lambda_{\\max}(K) + \\sigma_n^2}{\\lambda_{\\min}(K) + \\sigma_n^2} $$\nWe are given $\\lambda_{\\max}(K) \\approx 1.2$, $\\lambda_{\\min}(K) \\approx 10^{-12}$, and $\\sigma_n^2 \\approx 10^{-14}$.\n\n### Option-by-Option Analysis\n\n**A. Given the stated eigenvalues and $\\sigma_n^2 \\approx 10^{-14}$, the data Gram matrix $K_y$ has a spectral condition number $\\kappa_2(K_y)$ of order $10^{12}$, which is large enough in double precision to cause loss of accuracy and instability in factorizations used by stochastic simulation.**\n\nLet's compute the condition number with the given values:\n$$ \\lambda_{\\max}(K_y) = \\lambda_{\\max}(K) + \\sigma_n^2 \\approx 1.2 + 10^{-14} \\approx 1.2 $$\n$$ \\lambda_{\\min}(K_y) = \\lambda_{\\min}(K) + \\sigma_n^2 \\approx 10^{-12} + 10^{-14} = 10^{-12} + 0.01 \\times 10^{-12} = 1.01 \\times 10^{-12} $$\nThe condition number is:\n$$ \\kappa_2(K_y) = \\frac{\\lambda_{\\max}(K_y)}{\\lambda_{\\min}(K_y)} \\approx \\frac{1.2}{1.01 \\times 10^{-12}} \\approx 1.19 \\times 10^{12} $$\nThis value is of the order $10^{12}$. In double-precision arithmetic, where $\\varepsilon_{\\mathrm{mach}} \\approx 10^{-16}$, a general rule of thumb is that one can expect to lose about $\\log_{10}(\\kappa_2)$ digits of accuracy when solving a linear system. Here, $\\log_{10}(1.19 \\times 10^{12}) \\approx 12.08$. Given about $16$ available digits of precision, this leaves only about $16 - 12 = 4$ significant digits, which constitutes a severe loss of accuracy. This level of ill-conditioning makes standard algorithms like Cholesky factorization numerically unstable. The statement is accurate.\n\n**Verdict: Correct**\n\n**B. Increasing the noise level by adding a jitter $\\delta I_n$ (equivalently replacing $\\sigma_n^2$ by $\\sigma_n^2 + \\delta$) strictly decreases $\\kappa_2(K_y)$ as a function of $\\delta$. Under the given eigenvalues, the smallest $\\delta$ that achieves $\\kappa_2(K_y) \\leq 10^8$ is approximately $1.2 \\times 10^{-8}$.**\n\nLet the new condition number be a function of the added jitter $\\delta  0$:\n$$ \\kappa(\\delta) = \\frac{\\lambda_{\\max}(K) + \\sigma_n^2 + \\delta}{\\lambda_{\\min}(K) + \\sigma_n^2 + \\delta} $$\nTo check if this function is strictly decreasing, we examine its derivative with respect to $\\delta$. Let $a = \\lambda_{\\max}(K) + \\sigma_n^2$ and $b = \\lambda_{\\min}(K) + \\sigma_n^2$.\n$$ \\frac{d\\kappa}{d\\delta} = \\frac{d}{d\\delta}\\left(\\frac{a + \\delta}{b + \\delta}\\right) = \\frac{1(b+\\delta) - 1(a+\\delta)}{(b+\\delta)^2} = \\frac{b-a}{(b+\\delta)^2} $$\nSince $K$ is positive definite and not a scalar multiple of the identity, $\\lambda_{\\max}(K)  \\lambda_{\\min}(K)$, which implies $a  b$. Thus, $b-a  0$, and the derivative is strictly negative for all $\\delta \\ge 0$. So, $\\kappa(\\delta)$ is a strictly decreasing function of $\\delta$.\n\nNow, we find the smallest $\\delta$ that achieves $\\kappa_2(K_y) \\le 10^8$. Since the function is decreasing, this occurs at the boundary:\n$$ \\frac{\\lambda_{\\max}(K) + \\sigma_n^2 + \\delta}{\\lambda_{\\min}(K) + \\sigma_n^2 + \\delta} = 10^8 $$\nUsing the given values:\n$$ \\frac{1.2 + 10^{-14} + \\delta}{10^{-12} + 10^{-14} + \\delta} = 10^8 $$\n$$ 1.2 + 10^{-14} + \\delta = 10^8 (1.01 \\times 10^{-12} + \\delta) = 1.01 \\times 10^{-4} + 10^8 \\delta $$\n$$ 1.2 - 1.01 \\times 10^{-4} \\approx (10^8 - 1) \\delta $$\n$$ \\delta \\approx \\frac{1.2}{10^8} = 1.2 \\times 10^{-8} $$\nThe calculation confirms the approximate value. Both parts of the statement are accurate.\n\n**Verdict: Correct**\n\n**C. Tapering the kernel by multiplying $k(x,x')$ with a compactly supported function that equals $1$ at $x=x'$ but is $ 1$ otherwise always decreases $\\kappa_2(K_y)$ and leaves the predictive mean at training inputs unchanged.**\n\nThis statement makes two claims. Let's analyze the second one first: that tapering \"leaves the predictive mean at training inputs unchanged\". The predictive mean at the training inputs $\\{x_i\\}_{i=1}^n$ is given by the vector $\\mathbf{m}_{\\text{train}} = K (K_y)^{-1} \\mathbf{y} = K (K + \\sigma_n^2 I_n)^{-1} \\mathbf{y}$. This can be rewritten as $\\mathbf{m}_{\\text{train}} = (I_n - \\sigma_n^2 (K + \\sigma_n^2 I_n)^{-1})\\mathbf{y}$. Tapering replaces the kernel matrix $K$ with a new matrix $K'$, where $K'_{ij} = K_{ij} T_{ij}$ for a tapering matrix $T$ with $T_{ii}=1$ and $|T_{ij}| \\le 1$ for $i \\ne j$. Since $T$ is not the matrix of all ones, $K' \\neq K$. The predictive mean after tapering is $\\mathbf{m}'_{\\text{train}} = K' (K' + \\sigma_n^2 I_n)^{-1} \\mathbf{y}$. As the entire expression for the mean depends on the full matrix $K$, changing it to $K'$ will, in general, change the predictive mean. The predictive mean is not unchanged. Since a part of the statement is false, the entire statement is incorrect.\n\n**Verdict: Incorrect**\n\n**D. Replacing $K$ by a low-rank approximation $U U^\\top$ with $U \\in \\mathbb{R}^{n \\times m}$ and $m \\ll n$ (e.g., via the Nyström method or pivoted Cholesky), and then using $U U^\\top + \\sigma_n^2 I_n$, can improve conditioning; a principled truncation rule is to monitor the discarded spectrum and choose $m$ so that the sum of omitted eigenvalues is below a tolerance proportional to $\\varepsilon_{\\mathrm{mach}}$ times $\\operatorname{trace}(K)$.**\n\nGaussian process approximations based on low-rank methods are a standard technique to improve scalability and numerical stability. The inversion of the $n \\times n$ matrix $K_y = UU^\\top + \\sigma_n^2 I_n$ is performed efficiently using the Woodbury matrix identity:\n$$ (UU^\\top + \\sigma_n^2 I_n)^{-1} = \\frac{1}{\\sigma_n^2}I_n - \\frac{1}{\\sigma_n^2} U \\left( \\sigma_n^2 I_m + U^\\top U \\right)^{-1} U^\\top $$\nThis requires the inversion of an $m \\times m$ matrix, $\\sigma_n^2 I_m + U^\\top U$. In methods like Nyström, the eigenvalues of $U^\\top U$ correspond to the largest eigenvalues of $K$. The smallest eigenvalue involved in the $m \\times m$ inversion will be much larger than $\\lambda_{\\min}(K)$, leading to a much better-conditioned matrix inversion problem. Therefore, these methods improve conditioning by replacing the ill-conditioned $n \\times n$ problem with a well-conditioned $m \\times m$ one.\n\nThe proposed truncation rule is to choose the rank $m$ such that the sum of the discarded eigenvalues, $\\sum_{i=m+1}^n \\lambda_i(K)$, is small relative to the total sum of eigenvalues, $\\operatorname{trace}(K) = \\sum_{i=1}^n \\lambda_i(K)$. Setting the tolerance for this relative error to be on the order of machine epsilon, i.e., error $\\le C \\cdot \\varepsilon_{\\mathrm{mach}} \\operatorname{trace}(K)$, is a standard and principled heuristic in numerical linear algebra for creating approximations that are accurate up to machine precision. The statement is accurate.\n\n**Verdict: Correct**\n\n**E. For the squared exponential kernel, rescaling inputs by $x_i \\mapsto a x_i$ while keeping $\\ell$ fixed preserves the spectral condition number of $K_y$ for any $a > 0$.**\n\nLet the original kernel be $k(x,x') = \\sigma_f^2 \\exp(-\\|x-x'\\|^2 / (2\\ell^2))$. After rescaling the inputs to $z_i = a x_i$, the new kernel matrix $K'$ has entries:\n$$ K'_{ij} = k(z_i, z_j) = \\sigma_f^2 \\exp(-\\|z_i-z_j\\|^2 / (2\\ell^2)) = \\sigma_f^2 \\exp(-\\|a(x_i-x_j)\\|^2 / (2\\ell^2)) = \\sigma_f^2 \\exp(-a^2\\|x_i-x_j\\|^2 / (2\\ell^2)) $$\nThis is equivalent to using the original inputs with a new length-scale $\\ell' = \\ell/a$. The Gram matrix $K$ is highly sensitive to the length-scale parameter. For example, as $\\ell \\to \\infty$, $K \\to \\sigma_f^2 \\mathbf{1}\\mathbf{1}^\\top$ (a rank-1 matrix), which is very ill-conditioned. As $\\ell \\to 0$, $K \\to \\sigma_f^2 I_n$, which is perfectly conditioned ($\\kappa_2(K)=1$). Since rescaling the inputs is equivalent to changing the length-scale, and the spectrum of $K$ (and thus $\\kappa_2(K)$ and $\\kappa_2(K_y)$) depends strongly on the length-scale, the condition number is not preserved.\n\n**Verdict: Incorrect**\n\n**F. In MCMC for hyperparameter inference, an ill-conditioned $K_y$ typically improves mixing because the log marginal likelihood becomes flatter near near-singular configurations, reducing autocorrelation in the chain.**\n\nThe log marginal likelihood (LML) for GP regression is given by $\\log p(\\mathbf{y}|\\theta) = -\\frac{1}{2}\\mathbf{y}^\\top K_y^{-1} \\mathbf{y} - \\frac{1}{2}\\log|K_y| - \\frac{n}{2}\\log(2\\pi)$. When $K_y$ becomes ill-conditioned, its smallest eigenvalue approaches zero, causing its determinant $|K_y| = \\prod_i \\lambda_i(K_y)$ to approach zero. Consequently, $\\log|K_y|$ approaches $-\\infty$. This creates extremely steep gradients and sharp, narrow valleys or ridges in the LML surface in the space of hyperparameters $\\theta$. Such topologies are notoriously difficult for MCMC samplers to explore. Samplers tend to get stuck in these regions, leading to very slow mixing and high autocorrelation in the chain. The LML surface becomes more rugged and \"spiky,\" not flatter. Therefore, ill-conditioning severely hinders MCMC performance, it does not improve it.\n\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "To move beyond the computational and memory bottlenecks of exact GPR, which scales as $O(n^3)$, modern methods rely on principled approximations. This practice explores the variational inducing point framework, a cornerstone of scalable Gaussian processes, which uses a small set of $m$ 'inducing' points to create a low-rank approximation to the full process . By deriving the computational complexity and analyzing the role of minibatch optimization, you will gain insight into how these models achieve remarkable efficiency without sacrificing the core principles of Bayesian inference.",
            "id": "3309550",
            "problem": "Consider a zero-mean Gaussian process regression model with prior $f \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot))$ on inputs $X \\in \\mathbb{R}^{n \\times d}$ and outputs $\\mathbf{y} \\in \\mathbb{R}^{n}$ with Gaussian likelihood $p(\\mathbf{y} \\mid \\mathbf{f}) = \\prod_{i=1}^{n} \\mathcal{N}(y_{i} \\mid f(x_{i}), \\sigma^{2})$. Introduce $m \\ll n$ inducing locations $Z \\in \\mathbb{R}^{m \\times d}$ and denote $\\mathbf{u} = f(Z)$ with prior $p(\\mathbf{u}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{K}_{ZZ})$, where $\\mathbf{K}_{ZZ} \\in \\mathbb{R}^{m \\times m}$ is the kernel matrix on $Z$. Consider the variational inducing point method with a Gaussian variational distribution $q(\\mathbf{u}) = \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$ and the variational posterior $q(\\mathbf{f})$ obtained by marginalizing $q(\\mathbf{u})$ through the conditional of the joint Gaussian. The training objective is the Evidence Lower Bound (ELBO), defined as the expected log-likelihood under $q(\\mathbf{f})$ minus the Kullback–Leibler divergence between $q(\\mathbf{u})$ and $p(\\mathbf{u})$. Assume a dense covariance $\\mathbf{S}$ and that the kernel evaluations for each pair $(x, z)$ cost constant time independent of $m$ and $n$.\n\nYou will analyze the computational complexity of training and prediction in this setting. Use the following fundamental bases and facts:\n\n1. The joint Gaussian structure of $(\\mathbf{u}, f(x))$ under the prior determines that the conditional $p(f(x) \\mid \\mathbf{u})$ is Gaussian, and linear-Gaussian transformations preserve Gaussianity and allow closed-form means and variances via matrix multiplications and triangular solves.\n2. For dense $m \\times m$ matrices, Cholesky factorization costs $O(m^{3})$ and solving triangular systems costs $O(m^{2})$ per right-hand side. Multiplying an $m \\times m$ dense matrix by an $m$-vector costs $O(m^{2})$, and multiplying an $m$-vector by an $m \\times m$ inverse already factored via Cholesky costs $O(m^{2})$ per solve.\n3. The ELBO decomposes into a sum over data of expected log-likelihood terms plus a data-independent Kullback–Leibler term. Minibatch stochastic optimization uses random subsets of the data and rescales their contribution to form unbiased estimators of sums.\n\nDerive, from these bases:\n\na) The full-batch training time complexity as a function of $n$ and $m$ for one pass over the data when evaluating the ELBO and its gradients with respect to the variational parameters and kernel hyperparameters, when using dense $\\mathbf{S}$ and precomputations based on $\\mathbf{K}_{ZZ}$.\n\nb) The per-test-point computational cost to compute the predictive mean and predictive variance under $q(\\mathbf{f})$.\n\nc) A principled justification for how minibatch-based stochastic optimization yields unbiased gradient estimators of the ELBO when subsampling data indices uniformly at random and rescaling, assuming the reparameterization trick is used for the variational expectations and the single-sample Monte Carlo estimator of each expected log-likelihood term is unbiased.\n\nYour derivation must begin from the listed bases, reason about the sequence of linear algebra operations required, and carefully account for the dominant terms in $n$ and $m$ without invoking any shortcut formulas. Assume no special structure beyond dense matrices, and do not assume diagonal or low-rank $\\mathbf{S}$.\n\nSpecify your final answer as a single row matrix with three entries in the order: training complexity, predictive mean cost per test point, predictive variance cost per test point. Each entry must be a big-$O$ expression. No numerical rounding is required.",
            "solution": "The problem requires an analysis of the computational complexity of training and prediction for a variational inducing point Gaussian process model, and a justification for the unbiasedness of minibatch-based stochastic gradient estimators. The analysis will be performed in three parts, corresponding to the three tasks (a), (b), and (c).\n\na) Full-batch training time complexity for one pass\n\nThe training objective is the Evidence Lower Bound (ELBO), which we denote by $L$. It is defined as:\n$$\nL = \\mathbb{E}_{q(\\mathbf{f})}[\\ln p(\\mathbf{y} \\mid \\mathbf{f})] - \\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})]\n$$\nGiven the factorized likelihood $p(\\mathbf{y} \\mid \\mathbf{f}) = \\prod_{i=1}^{n} p(y_{i} \\mid f(x_{i}))$, the expected log-likelihood term can be written as a sum over the $n$ data points:\n$$\nL = \\sum_{i=1}^{n} \\mathbb{E}_{q(f_i)}[\\ln p(y_i \\mid f_i)] - \\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})]\n$$\nWe analyze the computational cost of evaluating each of the two main components (the sum and the KL-divergence) and their gradients for one full pass over the $n$ data points.\n\nFirst, we analyze the Kullback–Leibler divergence term, $\\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})]$. The variational distribution is $q(\\mathbf{u}) = \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$ and the prior is $p(\\mathbf{u}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{K}_{ZZ})$. The KL divergence between these two multivariate Gaussian distributions is:\n$$\n\\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})] = \\frac{1}{2} \\left[ \\text{Tr}(\\mathbf{K}_{ZZ}^{-1}\\mathbf{S}) + \\mathbf{m}^T \\mathbf{K}_{ZZ}^{-1}\\mathbf{m} - m + \\ln\\det(\\mathbf{K}_{ZZ}) - \\ln\\det(\\mathbf{S}) \\right]\n$$\nThe computational cost for each part of this expression is as follows:\n1.  Cholesky factorization of $\\mathbf{K}_{ZZ} \\in \\mathbb{R}^{m \\times m}$: A required precomputation for efficient solves and determinant calculation. This costs $O(m^3)$. Let $\\mathbf{K}_{ZZ} = \\mathbf{L}_{ZZ}\\mathbf{L}_{ZZ}^T$.\n2.  $\\ln\\det(\\mathbf{K}_{ZZ})$: This is $2 \\sum_{j=1}^{m} \\ln((\\mathbf{L}_{ZZ})_{jj})$, costing $O(m)$ after the Cholesky factorization.\n3.  $\\ln\\det(\\mathbf{S})$: Since $\\mathbf{S}$ is dense, we compute its Cholesky factorization $\\mathbf{S} = \\mathbf{L}_{S}\\mathbf{L}_{S}^T$, which costs $O(m^3)$, and then compute the log determinant in $O(m)$.\n4.  $\\mathbf{m}^T \\mathbf{K}_{ZZ}^{-1}\\mathbf{m}$: This is computed by first solving the system $\\mathbf{K}_{ZZ}\\boldsymbol{\\alpha} = \\mathbf{m}$ for $\\boldsymbol{\\alpha}$, and then calculating the dot product $\\mathbf{m}^T\\boldsymbol{\\alpha}$. Using the Cholesky factor $\\mathbf{L}_{ZZ}$, solving the system requires two triangular solves and costs $O(m^2)$. The subsequent dot product costs $O(m)$. Total cost is $O(m^2)$.\n5.  $\\text{Tr}(\\mathbf{K}_{ZZ}^{-1}\\mathbf{S})$: This is the most expensive operation. A direct approach is to compute $\\mathbf{V} = \\mathbf{K}_{ZZ}^{-1}\\mathbf{S}$ and then take its trace. Computing $\\mathbf{V}$ requires solving $m$ linear systems of the form $\\mathbf{K}_{ZZ}\\mathbf{v}_j = \\mathbf{s}_j$ for each column $\\mathbf{s}_j$ of $\\mathbf{S}$. Each solve costs $O(m^2)$, leading to a total cost of $m \\times O(m^2) = O(m^3)$. Taking the trace costs an additional $O(m)$.\n\nSumming these costs, the computation of the KL term is dominated by the Cholesky factorizations and the trace term, resulting in a complexity of $O(m^3)$.\n\nNext, we analyze the expected log-likelihood term, $\\sum_{i=1}^{n} \\mathbb{E}_{q(f_i)}[\\ln p(y_i \\mid f_i)]$. For the Gaussian likelihood $p(y_i \\mid f_i) = \\mathcal{N}(y_i \\mid f_i, \\sigma^2)$, this expectation has a closed form that depends on the mean $\\mu_{q,i}$ and variance $\\sigma^2_{q,i}$ of the marginal variational posterior $q(f_i)$.\nThe distribution $q(f_i)$ is obtained by marginalizing the joint $p(f_i \\mid \\mathbf{u})q(\\mathbf{u})$. Based on fundamental property 1, $p(f_i \\mid \\mathbf{u})$ is Gaussian. Let $\\mathbf{k}_{i} = k(Z, x_i)$. Then $p(f_i \\mid \\mathbf{u}) = \\mathcal{N}(f_i \\mid \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{u}, k(x_i, x_i) - \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_i)$. As $q(\\mathbf{u})$ is Gaussian, the resulting marginal $q(f_i)$ is also Gaussian. Its mean and variance are:\n$$\n\\mu_{q,i} = \\mathbb{E}_{q(f_i)}[f_i] = \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{m}\n$$\n$$\n\\sigma^2_{q,i} = \\text{Var}_{q(f_i)}[f_i] = k(x_i, x_i) - \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_i + \\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{S} \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_i\n$$\nTo evaluate the sum over $n$ data points, we must compute these quantities for each $i \\in \\{1, \\dots, n\\}$. For a single data point $i$:\n1.  Compute the kernel vector $\\mathbf{k}_i \\in \\mathbb{R}^m$: $m$ kernel evaluations, cost $O(m)$.\n2.  Define $\\boldsymbol{\\beta}_i = \\mathbf{K}_{ZZ}^{-1}\\mathbf{k}_i$. This is computed by solving a linear system using the precomputed Cholesky factor of $\\mathbf{K}_{ZZ}$. This costs $O(m^2)$.\n3.  Compute the mean $\\mu_{q,i} = \\boldsymbol{\\beta}_i^T \\mathbf{m}$. This is a dot product, costing $O(m)$.\n4.  Compute the variance components:\n    *   $k(x_i, x_i)$: $O(1)$.\n    *   $\\mathbf{k}_i^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_i = \\mathbf{k}_i^T \\boldsymbol{\\beta}_i$: Another dot product, $O(m)$.\n    *   $\\boldsymbol{\\beta}_i^T \\mathbf{S} \\boldsymbol{\\beta}_i$: Since $\\mathbf{S}$ is dense, this is computed as $(\\boldsymbol{\\beta}_i^T \\mathbf{S}) \\boldsymbol{\\beta}_i$. The matrix-vector product $\\mathbf{S}^T\\boldsymbol{\\beta}_i$ (or $\\mathbf{S}\\boldsymbol{\\beta}_i$) costs $O(m^2)$, followed by a dot product costing $O(m)$. Total is $O(m^2)$.\n\nThe total cost to compute the mean and variance for one data point $i$ is dominated by solving for $\\boldsymbol{\\beta}_i$ and the quadratic form involving $\\mathbf{S}$, which is $O(m^2)$. To compute the full expected log-likelihood term, we must repeat this for all $n$ data points, leading to a total cost of $O(nm^2)$.\n\nCombining the costs for a single full-batch pass: The overall ELBO evaluation requires a one-time cost of $O(m^3)$ for the Cholesky of $\\mathbf{K}_{ZZ}$ and another $O(m^3)$ for the KL term computations, plus the $O(nm^2)$ cost for the data-dependent term. The total cost is $O(nm^2 + m^3)$. The computation of gradients with respect to variational parameters and kernel hyperparameters using modern auto-differentiation frameworks involves a backward pass with computational complexity that is asymptotically the same as the forward pass. Thus, the complexity for one training step (ELBO and gradients) is $O(nm^2 + m^3)$.\n\nb) Per-test-point prediction cost\n\nFor a new test point $x_*$, we compute the predictive distribution $q(f_*) = \\int p(f_* \\mid \\mathbf{u})q(\\mathbf{u})d\\mathbf{u}$. This is a Gaussian distribution whose mean and variance can be computed in closed form. We assume the model has been trained, so variational parameters $\\mathbf{m}$, $\\mathbf{S}$ and kernel hyperparameters are fixed. Common precomputations after training include the Cholesky factor of $\\mathbf{K}_{ZZ}$.\n\nThe predictive mean is:\n$$\n\\mu_{q,*} = \\mathbb{E}_{q(f_*)}[f_*] = \\mathbf{k}_*^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{m}\n$$\nwhere $\\mathbf{k}_* = k(Z, x_*)$. To compute this efficiently, we can pre-calculate the vector $\\boldsymbol{\\alpha} = \\mathbf{K}_{ZZ}^{-1}\\mathbf{m}$ once after training, at a cost of $O(m^2)$. Then, for each test point $x_*$:\n1.  Compute the kernel vector $\\mathbf{k}_*$: $O(m)$.\n2.  Compute the dot product $\\mu_{q,*} = \\mathbf{k}_*^T \\boldsymbol{\\alpha}$: $O(m)$.\nThe total computational cost to compute the predictive mean per test point is $O(m)$.\n\nThe predictive variance is:\n$$\n\\sigma^2_{q,*} = \\text{Var}_{q(f_*)}[f_*] = (k_{**} - \\mathbf{k}_*^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_*) + \\mathbf{k}_*^T \\mathbf{K}_{ZZ}^{-1} \\mathbf{S} \\mathbf{K}_{ZZ}^{-1} \\mathbf{k}_*\n$$\nwhere $k_{**} = k(x_*, x_*)$. The cost to compute this for a single test point is:\n1.  Compute $k_{**}$ and $\\mathbf{k}_*$: $O(1)$ and $O(m)$, respectively.\n2.  Define $\\boldsymbol{\\beta}_* = \\mathbf{K}_{ZZ}^{-1}\\mathbf{k}_*$. Solve this system using the Cholesky factor of $\\mathbf{K}_{ZZ}$: $O(m^2)$.\n3.  Compute the first variance term $k_{**} - \\mathbf{k}_*^T \\boldsymbol{\\beta}_*$: $O(m)$.\n4.  Compute the second variance term $\\boldsymbol{\\beta}_*^T \\mathbf{S} \\boldsymbol{\\beta}_*$: As $\\mathbf{S}$ is a dense $m \\times m$ matrix, this costs $O(m^2)$.\nThe total computational cost to compute the predictive variance per test point is dominated by the system solve and the quadratic form with $\\mathbf{S}$, resulting in a complexity of $O(m^2)$.\n\nc) Justification for unbiased minibatch gradient estimators\n\nThe ELBO can be written as the sum of per-datum contributions plus a data-independent term:\n$$\nL(\\boldsymbol{\\phi}) = \\sum_{i=1}^{n} L_i(\\boldsymbol{\\phi}) - \\text{KL}(\\boldsymbol{\\phi})\n$$\nwhere $L_i(\\boldsymbol{\\phi}) = \\mathbb{E}_{q(f_i)}[\\ln p(y_i \\mid f_i)]$ and $\\text{KL}(\\boldsymbol{\\phi}) = \\text{KL}[q(\\mathbf{u}) || p(\\mathbf{u})]$. The parameters of the model (variational and kernel) are denoted by $\\boldsymbol{\\phi}$.\n\nThe gradient of the ELBO with respect to these parameters is, by linearity of the gradient operator:\n$$\n\\nabla_{\\boldsymbol{\\phi}} L(\\boldsymbol{\\phi}) = \\nabla_{\\boldsymbol{\\phi}} \\left(\\sum_{i=1}^{n} L_i(\\boldsymbol{\\phi})\\right) - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi}) = \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi})\n$$\nIn minibatch stochastic optimization, we do not compute the full sum over all $n$ data points. Instead, we draw a minibatch $B$ of size $S \\ll n$ uniformly at random from the set of indices $\\{1, \\dots, n\\}$. The stochastic gradient estimator $\\hat{\\mathbf{g}}(\\boldsymbol{\\phi})$ is constructed as:\n$$\n\\hat{\\mathbf{g}}(\\boldsymbol{\\phi}) = \\frac{n}{S} \\sum_{j \\in B} \\nabla_{\\boldsymbol{\\phi}} L_j(\\boldsymbol{\\phi}) - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi})\n$$\nThe key is the rescaling factor $\\frac{n}{S}$. To demonstrate that this estimator is unbiased, we take its expectation with respect to the random selection of the minibatch $B$:\n$$\n\\mathbb{E}_{B}[\\hat{\\mathbf{g}}(\\boldsymbol{\\phi})] = \\mathbb{E}_{B} \\left[ \\frac{n}{S} \\sum_{j \\in B} \\nabla_{\\boldsymbol{\\phi}} L_j(\\boldsymbol{\\phi}) \\right] - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi})\n$$\nThe KL term gradient is data-independent, so its expectation is itself. We focus on the expectation of the scaled sum. Let $I_j$ be an indicator random variable such that $I_j=1$ if index $j$ is in the minibatch $B$ and $I_j=0$ otherwise. The sum can be rewritten as $\\sum_{i=1}^{n} I_i \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi})$. Using the linearity of expectation:\n$$\n\\mathbb{E}_{B} \\left[ \\frac{n}{S} \\sum_{i=1}^{n} I_i \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) \\right] = \\frac{n}{S} \\sum_{i=1}^{n} \\mathbb{E}_{B}[I_i] \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi})\n$$\nThe expectation $\\mathbb{E}_{B}[I_i]$ is the probability that data point $i$ is included in a minibatch $B$ of size $S$ sampled uniformly from $n$ items. This probability is $P(i \\in B) = \\frac{S}{n}$. Substituting this back:\n$$\n\\frac{n}{S} \\sum_{i=1}^{n} \\left(\\frac{S}{n}\\right) \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) = \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi})\n$$\nThis is precisely the exact sum from the full gradient. Therefore, we have shown that:\n$$\n\\mathbb{E}_{B}[\\hat{\\mathbf{g}}(\\boldsymbol{\\phi})] = \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi}) - \\nabla_{\\boldsymbol{\\phi}} \\text{KL}(\\boldsymbol{\\phi}) = \\nabla_{\\boldsymbol{\\phi}} L(\\boldsymbol{\\phi})\n$$\nThe estimator is unbiased. The use of the reparameterization trick allows for the computation of $\\nabla_{\\boldsymbol{\\phi}} L_i(\\boldsymbol{\\phi})_ = \\nabla_{\\boldsymbol{\\phi}} \\mathbb{E}_{q(f_i)}[\\ln p(y_i|f_i)]$ by moving the gradient inside the expectation. If a single-sample Monte Carlo estimate is used for this term, its unbiasedness property ensures that the expectation over the Monte Carlo noise, combined with the expectation over the minibatch sampling, still results in an unbiased estimator for the total gradient $\\nabla_{\\boldsymbol{\\phi}} L(\\boldsymbol{\\phi})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nO(nm^2 + m^3)  O(m)  O(m^2)\n\\end{pmatrix}\n}\n$$"
        }
    ]
}