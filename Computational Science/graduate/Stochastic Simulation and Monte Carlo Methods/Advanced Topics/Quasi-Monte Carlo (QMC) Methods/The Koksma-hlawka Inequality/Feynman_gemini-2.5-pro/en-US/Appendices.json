{
    "hands_on_practices": [
        {
            "introduction": "The Koksma-Hlawka inequality provides a deterministic error bound for quasi-Monte Carlo (QMC) integration by connecting a function's \"roughness\" to a point set's \"uniformity.\" To truly grasp this pivotal relationship, it is essential to compute its two main components: the Hardy-Krause variation of the function, $V_{\\mathrm{HK}}(f)$, and the star discrepancy of the point set, $D_N^*(P)$. This first exercise  offers a direct, hands-on calculation for a simple two-dimensional case, allowing you to solidify your understanding of how these theoretical quantities are calculated and combined to produce a concrete error bound.",
            "id": "3354410",
            "problem": "Consider the following quasi-Monte Carlo (QMC) setting on the unit square $[0,1]^2$. Let the point set be\n$$\nP=\\{(x_1,y_1),(x_2,y_2),(x_3,y_3)\\}=\\left\\{\\left(\\tfrac{1}{4},\\tfrac{1}{4}\\right),\\left(\\tfrac{1}{2},\\tfrac{1}{2}\\right),\\left(\\tfrac{3}{4},\\tfrac{3}{4}\\right)\\right\\}.\n$$\nThe star-discrepancy $D_N^*(P)$ of a finite set of $N$ points in $[0,1]^2$ is defined as\n$$\nD_N^*(P)=\\sup_{t\\in[0,1]^2}\\left|\\frac{1}{N}\\sum_{i=1}^N \\mathbf{1}\\{(x_i,y_i)\\in [0,t)\\}-t_1t_2\\right|,\n$$\nwhere $t=(t_1,t_2)$, $[0,t)=[0,t_1)\\times[0,t_2)$, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nLet $f:[0,1]^2\\to\\mathbb{R}$ be given by $f(x,y)=xy$. Compute the Hardy–Krause variation $V_{\\mathrm{HK}}(f)$ from first principles. Then, using the Koksma–Hlawka inequality, determine the exact value of the resulting upper bound on the worst-case absolute integration error of the QMC estimator based on the point set $P$ for the integral $\\int_{[0,1]^2} f(x,y)\\,dx\\,dy$.\n\nProvide your final answer as a single exact rational number. No rounding is required. Do not include units.",
            "solution": "The problem asks for the upper bound on the absolute integration error for the function $f(x,y) = xy$ using the given point set $P$, as determined by the Koksma-Hlawka inequality. The Koksma-Hlawka inequality states that for a function $f$ with finite Hardy-Krause variation $V_{\\mathrm{HK}}(f)$ and a point set $P$ with star discrepancy $D_N^*(P)$, the absolute error of the quasi-Monte Carlo estimator is bounded as follows:\n$$ \\left| \\int_{[0,1]^2} f(\\mathbf{x}) d\\mathbf{x} - \\frac{1}{N} \\sum_{i=1}^N f(\\mathbf{x}_i) \\right| \\le V_{\\mathrm{HK}}(f) D_N^*(P) $$\nThus, we must compute two quantities: the Hardy-Krause variation of the function $f$, denoted $V_{\\mathrm{HK}}(f)$, and the star discrepancy of the point set $P$, denoted $D_N^*(P)$. The desired upper bound is the product of these two values.\n\nFirst, we compute the Hardy–Krause variation $V_{\\mathrm{HK}}(f)$ for the function $f(x,y) = xy$ on the domain $[0,1]^2$. For a function of two variables with continuous mixed partial derivatives, the variation in the sense of Hardy and Krause is given by the formula:\n$$ V_{\\mathrm{HK}}(f) = \\int_0^1 \\left| \\frac{\\partial f}{\\partial x}(x,1) \\right| dx + \\int_0^1 \\left| \\frac{\\partial f}{\\partial y}(1,y) \\right| dy + \\int_0^1 \\int_0^1 \\left| \\frac{\\partial^2 f}{\\partial x \\partial y}(x,y) \\right| dx dy $$\nFor the given function $f(x,y) = xy$, the partial derivatives are:\n$$ \\frac{\\partial f}{\\partial x}(x,y) = y $$\n$$ \\frac{\\partial f}{\\partial y}(x,y) = x $$\n$$ \\frac{\\partial^2 f}{\\partial x \\partial y}(x,y) = 1 $$\nWe evaluate the first-order partial derivatives at the boundary of the domain as required by the formula:\n$$ \\frac{\\partial f}{\\partial x}(x,1) = 1 $$\n$$ \\frac{\\partial f}{\\partial y}(1,y) = 1 $$\nSubstituting these expressions into the formula for $V_{\\mathrm{HK}}(f)$:\n$$ V_{\\mathrm{HK}}(f) = \\int_0^1 |1| dx + \\int_0^1 |1| dy + \\int_0^1 \\int_0^1 |1| dx dy $$\nEvaluating the integrals, we find:\n$$ V_{\\mathrm{HK}}(f) = 1 + 1 + 1 = 3 $$\n\nNext, we compute the star discrepancy $D_N^*(P)$ for the given point set $P$. The number of points is $N=3$, and the set is\n$$ P = \\left\\{\\left(\\frac{1}{4},\\frac{1}{4}\\right),\\left(\\frac{1}{2},\\frac{1}{2}\\right),\\left(\\frac{3}{4},\\frac{3}{4}\\right)\\right\\} $$\nWe can rewrite the points as $\\left(\\frac{1}{3+1}, \\frac{1}{3+1}\\right)$, $\\left(\\frac{2}{3+1}, \\frac{2}{3+1}\\right)$, $\\left(\\frac{3}{3+1}, \\frac{3}{3+1}\\right)$. This identifies $P$ as a specific instance of the \"diagonal\" point set $P_N = \\{(\\frac{i}{N+1}, \\dots, \\frac{i}{N+1})\\}_{i=1}^N$ in $s=2$ dimensions, with $N=3$. The star discrepancy of this particular set has a known closed-form expression:\n$$ D_N^*(P_N) = 1 - \\left(1 - \\frac{1}{N+1}\\right)^s $$\nWhile it is possible to compute the discrepancy from its fundamental definition by examining all possible anchor boxes, using this established formula is more direct. For our specific case, with $N=3$ and $s=2$:\n$$ D_3^*(P) = 1 - \\left(1 - \\frac{1}{3+1}\\right)^2 = 1 - \\left(1 - \\frac{1}{4}\\right)^2 $$\n$$ D_3^*(P) = 1 - \\left(\\frac{3}{4}\\right)^2 = 1 - \\frac{9}{16} = \\frac{16-9}{16} = \\frac{7}{16} $$\n\nFinally, we calculate the upper bound on the integration error given by the Koksma-Hlawka inequality by multiplying the two computed values.\n$$ \\text{Upper Bound} = V_{\\mathrm{HK}}(f) D_N^*(P) $$\nSubstituting the values we found:\n$$ \\text{Upper Bound} = 3 \\times \\frac{7}{16} = \\frac{21}{16} $$\nThis is the exact value of the upper bound on the absolute integration error for the function $f(x,y)=xy$ with the point set $P$, as provided by the Koksma-Hlawka inequality.",
            "answer": "$$\\boxed{\\frac{21}{16}}$$"
        },
        {
            "introduction": "Building on the foundational calculation in the previous exercise, we now explore how the Koksma-Hlawka inequality illuminates the power of QMC methods in higher dimensions and for large numbers of points. This practice  involves generalizing the variation calculation to an $s$-dimensional function and pairing it with a powerful, structured point set. By connecting the function's variation to the known discrepancy bounds for Faure sequences, you will derive an explicit error bound that reveals the superior convergence rate of QMC over standard Monte Carlo methods for certain classes of functions.",
            "id": "3308108",
            "problem": "Consider the function $f:[0,1]^s \\to \\mathbb{R}$ given by $f(\\boldsymbol{x})=\\prod_{j=1}^{s} x_j$, where $s \\geq 1$ is an integer. In quasi-Monte Carlo (QMC) integration with $N$ points $\\{\\boldsymbol{x}_n\\}_{n=0}^{N-1}\\subset[0,1]^s$, the equal-weight estimator of the integral $I(f)=\\int_{[0,1]^s} f(\\boldsymbol{x})\\,\\mathrm{d}\\boldsymbol{x}$ is $Q_N(f)=\\frac{1}{N}\\sum_{n=0}^{N-1} f(\\boldsymbol{x}_n)$. The Koksma–Hlawka inequality states that\n$$\n|Q_N(f)-I(f)| \\leq V_{\\mathrm{HK}}(f)\\, D_N^*(\\{\\boldsymbol{x}_n\\}),\n$$\nwhere $V_{\\mathrm{HK}}(f)$ is the variation of $f$ in the sense of Hardy–Krause (anchored at $1$) and $D_N^*$ is the star discrepancy.\n\nUsing only fundamental definitions and standard results, complete the following:\n\n1. Compute $V_{\\mathrm{HK}}(f)$ explicitly from the definition of Hardy–Krause variation anchored at $1$, namely\n$$\nV_{\\mathrm{HK}}(f) \\;=\\; \\sum_{\\emptyset \\neq u \\subseteq \\{1,\\dots,s\\}} \\int_{[0,1]^{|u|}} \\left| \\frac{\\partial^{|u|}}{\\partial \\boldsymbol{x}_u}\\, f\\bigl(\\boldsymbol{x}_u; \\boldsymbol{1}_{-u}\\bigr) \\right| \\,\\mathrm{d}\\boldsymbol{x}_u,\n$$\nwhere $\\boldsymbol{x}_u$ denotes the subvector $(x_j)_{j\\in u}$, $\\boldsymbol{1}_{-u}$ is the vector in the complementary coordinates with each component equal to $1$, and $\\frac{\\partial^{|u|}}{\\partial \\boldsymbol{x}_u}$ is the mixed partial derivative with respect to the variables in $u$.\n\n2. Let $b$ be a prime with $b \\geq s$, and let $\\{\\boldsymbol{x}_n\\}$ be the first $N=b^m$ points of a Faure sequence in base $b$. Use the facts that Faure sequences are digital $(0,s)$-sequences in base $b$ and that the first $b^m$ points of a $(0,s)$-sequence form a $(0,m,s)$-net in base $b$. Invoke the standard star discrepancy bound for $(0,m,s)$-nets in base $b$, of the form\n$$\nD_N^* \\;\\leq\\; C_{s,b}\\, \\frac{m^{\\,s-1}}{b^{m}},\n$$\nwhere $C_{s,b}>0$ is an explicit constant depending only on $s$ and $b$ (and independent of $m$ and $N$). Combine this with the Koksma–Hlawka inequality to obtain an explicit bound on $|Q_N(f)-I(f)|$ in terms of $s$, $b$, and $m$.\n\nYour final answer must be a single row vector whose first entry is $V_{\\mathrm{HK}}(f)$ and whose second entry is the bound you derive for $|Q_N(f)-I(f)|$ when $N=b^m$ Faure points are used. No asymptotic symbols are permitted in the final answer. If you simplify your expressions, do so exactly; do not introduce decimal approximations.",
            "solution": "The first part of the problem requires the computation of the Hardy-Krause variation (anchored at $1$) for the function $f:[0,1]^s \\to \\mathbb{R}$ defined by $f(\\boldsymbol{x})=\\prod_{j=1}^{s} x_j$. The Hardy-Krause variation is given by the formula:\n$$\nV_{\\mathrm{HK}}(f) \\;=\\; \\sum_{\\emptyset \\neq u \\subseteq \\{1,\\dots,s\\}} \\int_{[0,1]^{|u|}} \\left| \\frac{\\partial^{|u|}}{\\partial \\boldsymbol{x}_u}\\, f\\bigl(\\boldsymbol{x}_u; \\boldsymbol{1}_{-u}\\bigr) \\right| \\,\\mathrm{d}\\boldsymbol{x}_u.\n$$\nLet's analyze the term inside the integral for an arbitrary non-empty subset $u \\subseteq \\{1,\\dots,s\\}$. The vector $(\\boldsymbol{x}_u; \\boldsymbol{1}_{-u})$ is a point in $[0,1]^s$ whose components $x_j$ are variables if $j \\in u$ and are fixed to $1$ if $j \\notin u$. Applying this to the function $f$, we get:\n$$\nf\\bigl(\\boldsymbol{x}_u; \\boldsymbol{1}_{-u}\\bigr) = \\left( \\prod_{j \\in u} x_j \\right) \\left( \\prod_{k \\in \\{1,\\dots,s\\} \\setminus u} 1 \\right) = \\prod_{j \\in u} x_j.\n$$\nThis expression is a function of the variables $(x_j)_{j \\in u}$ only. Next, we must compute the mixed partial derivative of this function with respect to all variables $x_j$ where $j \\in u$.\n$$\n\\frac{\\partial^{|u|}}{\\partial \\boldsymbol{x}_u}\\, f\\bigl(\\boldsymbol{x}_u; \\boldsymbol{1}_{-u}\\bigr) = \\frac{\\partial^{|u|}}{\\partial \\boldsymbol{x}_u} \\left( \\prod_{j \\in u} x_j \\right).\n$$\nSince the function $\\prod_{j \\in u} x_j$ is a product of $|u|$ distinct variables, and we are taking the $|u|$-th order partial derivative with respect to each of these variables exactly once, the result is a constant. For any $j' \\in u$, $\\frac{\\partial}{\\partial x_{j'}} (\\prod_{j \\in u} x_j) = \\prod_{j \\in u, j \\neq j'} x_j$. Repeating this for all variables in $u$ yields:\n$$\n\\frac{\\partial^{|u|}}{\\partial \\boldsymbol{x}_u} \\left( \\prod_{j \\in u} x_j \\right) = 1.\n$$\nThe absolute value of this derivative is $|1| = 1$. Now we can evaluate the integral for a given non-empty set $u$:\n$$\n\\int_{[0,1]^{|u|}} \\left| \\frac{\\partial^{|u|}}{\\partial \\boldsymbol{x}_u}\\, f\\bigl(\\boldsymbol{x}_u; \\boldsymbol{1}_{-u}\\bigr) \\right| \\,\\mathrm{d}\\boldsymbol{x}_u = \\int_{[0,1]^{|u|}} 1 \\,\\mathrm{d}\\boldsymbol{x}_u.\n$$\nThis integral represents the volume of the $|u|$-dimensional unit hypercube, which is $1^{|u|} = 1$.\nNow we substitute this result back into the formula for the variation:\n$$\nV_{\\mathrm{HK}}(f) = \\sum_{\\emptyset \\neq u \\subseteq \\{1,\\dots,s\\}} 1.\n$$\nThis sum counts the number of non-empty subsets of the set $\\{1, \\dots, s\\}$. A set with $s$ elements has a total of $2^s$ subsets. Since we exclude the empty set $\\emptyset$, the number of non-empty subsets is $2^s - 1$.\nTherefore, the Hardy-Krause variation of $f$ is:\n$$\nV_{\\mathrm{HK}}(f) = 2^s - 1.\n$$\n\nThe second part of the problem is to derive an explicit bound on the quasi-Monte Carlo integration error $|Q_N(f)-I(f)|$ using the Koksma–Hlawka inequality, the result for $V_{\\mathrm{HK}}(f)$, and the given discrepancy bound for Faure sequences. The Koksma–Hlawka inequality is:\n$$\n|Q_N(f)-I(f)| \\leq V_{\\mathrm{HK}}(f)\\, D_N^*(\\{\\boldsymbol{x}_n\\}).\n$$\nWe have just found that $V_{\\mathrm{HK}}(f) = 2^s - 1$. The problem specifies that the points $\\{\\boldsymbol{x}_n\\}$ are the first $N=b^m$ points of a Faure sequence in base $b \\geq s$. These points form a $(0,m,s)$-net in base $b$. The star discrepancy $D_N^*$ for such a point set is bounded as given:\n$$\nD_N^* \\leq C_{s,b}\\, \\frac{m^{\\,s-1}}{b^{m}},\n$$\nwhere $N = b^m$ and $C_{s,b}$ is a constant depending only on $s$ and $b$.\nSubstituting our result for $V_{\\mathrm{HK}}(f)$ and the given bound for $D_N^*$ into the Koksma–Hlawka inequality, we obtain the error bound for the QMC estimator:\n$$\n|Q_N(f)-I(f)| \\leq (2^s - 1) \\, C_{s,b}\\, \\frac{m^{s-1}}{b^{m}}.\n$$\nThis expression provides the required bound in terms of $s$, $b$, and $m$.\n\nCombining the results for both parts, the first entry of the final answer is $V_{\\mathrm{HK}}(f)$ and the second entry is the error bound.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2^s - 1 & (2^s - 1) C_{s,b} \\frac{m^{s-1}}{b^m}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A crucial question for any mathematical inequality is its \"tightness\": can the bound ever be achieved, or is there always a gap between the error and the bound? This final practice  addresses this fundamental question for the Koksma-Hlawka inequality by guiding you through the construction of a \"worst-case\" function. By engaging with this problem, you will discover that the inequality is indeed sharp, meaning the bound cannot be universally improved, which establishes star discrepancy as the correct measure of uniformity for integration error among functions of bounded variation.",
            "id": "3354400",
            "problem": "Consider the unit hypercube $[0,1)^d$ and a finite point set $P_n = \\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\} \\subset [0,1)^d$. For a measurable function $f:[0,1)^d \\to \\mathbb{R}$, define the empirical cubature (Quasi-Monte Carlo average) $Q_n(f) = \\frac{1}{n}\\sum_{j=1}^n f(\\mathbf{x}_j)$ and the integral $I(f) = \\int_{[0,1)^d} f(\\mathbf{x})\\,d\\mathbf{x}$. The star discrepancy of $P_n$ is the quantity $D^*(P_n) = \\sup_{\\mathbf{t}\\in[0,1]^d}\\left|\\frac{1}{n}\\sum_{j=1}^n \\mathbf{1}\\{\\mathbf{x}_j < \\mathbf{t}\\} - \\prod_{i=1}^d t_i\\right|$, where the inequality $\\mathbf{x}_j < \\mathbf{t}$ is understood componentwise. The Hardy–Krause variation $V_{HK}(f)$ of $f$ is the standard multivariate generalization of bounded variation that controls the Quasi-Monte Carlo error via the Koksma–Hlawka inequality (KHI).\n\nYour task is to examine the tightness of the Koksma–Hlawka inequality by constructing, from first principles, a function $f$ and point sets $P_n$ for which the bound is nearly achieved, and to quantify the gap between the left-hand side and the right-hand side quantities. You must proceed without relying on any pre-derived shortcut formulas other than the core definitions above.\n\nFundamental base to use:\n- The definition of $Q_n(f)$ and $I(f)$.\n- The definition of star discrepancy $D^*(P_n)$ as a supremum over anchored axis-aligned boxes.\n- The definition of Hardy–Krause variation $V_{HK}(f)$ and its evaluation for indicator functions of anchored boxes.\n\nConstruction and computational plan:\n1. For each specified test case, compute an approximate star discrepancy by restricting the supremum to a finite tensor-product grid in $[0,1]^d$. For a given grid resolution $M$, let the candidate set be $\\mathcal{T}_M = \\{(k_1/M,\\ldots,k_d/M) : k_i \\in \\{1,2,\\ldots,M\\}\\}$. Define\n   $$D^*_M(P_n) = \\max_{\\mathbf{t}\\in \\mathcal{T}_M}\\left|\\frac{1}{n}\\sum_{j=1}^n \\mathbf{1}\\{\\mathbf{x}_j < \\mathbf{t}\\} - \\prod_{i=1}^d t_i\\right|.$$\n   This $D^*_M(P_n)$ is a computable lower bound of $D^*(P_n)$ that approaches it as $M$ increases.\n2. Let $\\mathbf{t}^{(c)}$ be a maximizer of the coarse-grid discrepancy $D^*_{M_c}(P_n)$ for a given coarse resolution $M_c$. Define the function\n   $$f_{\\mathbf{t}^{(c)}}(\\mathbf{x}) = \\prod_{i=1}^d \\mathbf{1}\\{x_i < t^{(c)}_i\\}.$$\n   Evaluate $Q_n(f_{\\mathbf{t}^{(c)}})$ and $I(f_{\\mathbf{t}^{(c)}})$. The absolute error is $E = \\left|Q_n(f_{\\mathbf{t}^{(c)}}) - I(f_{\\mathbf{t}^{(c)}})\\right|$.\n3. Using the fact that the Hardy–Krause variation for the anchored box indicator $f_{\\mathbf{t}}$ equals $V_{HK}(f_{\\mathbf{t}}) = 1$, form the computable upper bound $B = V_{HK}(f_{\\mathbf{t}^{(c)}})\\, D^*_{M_f}(P_n)$ using a finer resolution $M_f$. Define the gap $\\Delta = B - E$ and the tightness ratio $r = E/B$ (with the convention that $r=0$ if $B=0$).\n\nTest suite and specification:\n- All computations are to be performed in purely mathematical terms with no physical units.\n- Dimension and grids:\n  - Use $d=2$ for all test cases.\n  - Use a coarse grid resolution $M_c = 32$ and a fine grid resolution $M_f = 128$.\n- Point sets $P_n$:\n  1. Happy path low-discrepancy set: Halton sequence in dimension $d=2$ with bases $(2,3)$, first $n=64$ points.\n  2. Typical Monte Carlo set: Pseudorandom uniform points in $[0,1)^2$, $n=64$, generated deterministically with seed $12345$.\n  3. Adversarial clustered set: $n=16$ points in $[0,1)^2$ with $12$ points clustered near the origin and $4$ points near the opposite corner to induce large discrepancy. Specifically, $12$ points equal to $(0.05,0.05)$ and $4$ points equal to $(0.95,0.95)$.\n- For each test case, produce the four floats $[E,B,\\Delta,r]$ defined above.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of the three case results, each itself being a list of four floats, enclosed in square brackets. For example, the output must look exactly like\n$$[ [E_1,B_1,\\Delta_1,r_1], [E_2,B_2,\\Delta_2,r_2], [E_3,B_3,\\Delta_3,r_3] ].$$",
            "solution": "### 1. Theoretical Framework and Problem Interpretation\n\nThe Koksma-Hlawka inequality provides an upper bound on the error of a Quasi-Monte Carlo (QMC) integration:\n$$ \\left| I(f) - Q_n(f) \\right| \\le V_{HK}(f) \\cdot D^*(P_n) $$\nwhere $I(f) = \\int_{[0,1)^d} f(\\mathbf{x})\\,d\\mathbf{x}$ is the true integral of a function $f:[0,1)^d \\to \\mathbb{R}$, $Q_n(f) = \\frac{1}{n}\\sum_{j=1}^n f(\\mathbf{x}_j)$ is the QMC approximation using a point set $P_n = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}$, $V_{HK}(f)$ is the Hardy–Krause variation of $f$, and $D^*(P_n)$ is the star discrepancy of $P_n$.\n\nThe objective is to examine the tightness of this inequality. This is achieved by constructing a specific function $f$ for which the integration error is large, and comparing this error to the corresponding bound.\n\nThe computational plan specifies the following quantities:\n- $E$: The absolute integration error for a specially chosen function.\n- $B$: A computable approximation of the Koksma-Hlawka bound for that function.\n- $\\Delta = B - E$: The gap between the bound and the error.\n- $r = E/B$: The tightness ratio.\n\n### 2. Formalization of Computational Quantities\n\nThe procedure starts by finding a vector $\\mathbf{t}^{(c)}$ that maximizes the local discrepancy on a coarse grid $\\mathcal{T}_{M_c}$ of resolution $M_c$. This gives the coarse-grid approximate star discrepancy, $D^*_{M_c}(P_n)$:\n$$ D^*_{M_c}(P_n) = \\max_{\\mathbf{t} \\in \\mathcal{T}_{M_c}}\\left|\\frac{1}{n}\\sum_{j=1}^n \\mathbf{1}\\{\\mathbf{x}_j  \\mathbf{t}\\} - \\prod_{i=1}^d t_i\\right| = \\left|\\frac{1}{n}\\sum_{j=1}^n \\mathbf{1}\\{\\mathbf{x}_j  \\mathbf{t}^{(c)}\\} - Vo_d(\\mathbf{t}^{(c)})\\right| $$\nwhere $Vo_d(\\mathbf{t}) = \\prod_{i=1}^d t_i$ is the volume of the anchored box $[\\mathbf{0}, \\mathbf{t})$.\n\nThe chosen test function is the indicator function of the box anchored at this maximizer $\\mathbf{t}^{(c)}$:\n$$ f_{\\mathbf{t}^{(c)}}(\\mathbf{x}) = \\mathbf{1}\\{\\mathbf{x}  \\mathbf{t}^{(c)}\\} $$\n\nThe absolute integration error $E$ for this function is defined as $E = |Q_n(f_{\\mathbf{t}^{(c)}}) - I(f_{\\mathbf{t}^{(c)}})|$. By substituting the definitions:\n$$ Q_n(f_{\\mathbf{t}^{(c)}}) = \\frac{1}{n}\\sum_{j=1}^n \\mathbf{1}\\{\\mathbf{x}_j  \\mathbf{t}^{(c)}\\} $$\n$$ I(f_{\\mathbf{t}^{(c)}}) = \\int_{[0,1)^d} \\mathbf{1}\\{\\mathbf{x}  \\mathbf{t}^{(c)}\\} \\,d\\mathbf{x} = \\prod_{i=1}^d t^{(c)}_i = Vo_d(\\mathbf{t}^{(c)}) $$\nThus, the error $E$ is precisely the local discrepancy at $\\mathbf{t}^{(c)}$, which by construction is the maximum over the coarse grid:\n$$ E = \\left|\\frac{1}{n}\\sum_{j=1}^n \\mathbf{1}\\{\\mathbf{x}_j  \\mathbf{t}^{(c)}\\} - Vo_d(\\mathbf{t}^{(c)})\\right| = D^*_{M_c}(P_n) $$\n\nThe problem defines the bound $B$ as $B = V_{HK}(f_{\\mathbf{t}^{(c)}}) \\cdot D^*_{M_f}(P_n)$, where $D^*_{M_f}(P_n)$ is the discrepancy computed on a finer grid $\\mathcal{T}_{M_f}$. It is a standard result, provided in the problem, that the Hardy-Krause variation of an anchored box indicator function is $1$. Therefore, $V_{HK}(f_{\\mathbf{t}^{(c)}}) = 1$. The bound simplifies to:\n$$ B = 1 \\cdot D^*_{M_f}(P_n) = D^*_{M_f}(P_n) $$\n\nThe task thus reduces to computing the approximate star discrepancy on a coarse grid ($M_c=32$) to find $E$, and on a fine grid ($M_f=128$) to find $B$. The gap and ratio are then $\\Delta = B - E$ and $r = E / B$.\n\n### 3. Computational Methodology\n\nThe computation proceeds as follows for each test case:\n\n1.  **Point Set Generation**: The specified point set $P_n \\subset [0,1)^2$ is generated.\n    -   **Halton sequence**: Generated using the van der Corput sequence for bases $(2,3)$.\n    -   **Pseudorandom set**: Generated using a seeded random number generator for reproducibility.\n    -   **Adversarial set**: Constructed by placing points in specific clusters to maximize discrepancy.\n\n2.  **Approximate Star Discrepancy Calculation**: A function is implemented to compute $D^*_M(P_n)$ for a given resolution $M$.\n    -   A $d=2$ dimensional grid of test points $\\mathcal{T}_M = \\{(k_1/M, k_2/M) \\mid k_1, k_2 \\in \\{1, \\dots, M\\}\\}$ is created.\n    -   For each test point $\\mathbf{t} \\in \\mathcal{T}_M$, the local discrepancy is calculated: $d(\\mathbf{t}) = |\\frac{1}{n}\\sum_{j=1}^n \\mathbf{1}\\{\\mathbf{x}_j  \\mathbf{t}\\} - t_1 t_2|$.\n    -   The maximum of these values, $\\max_{\\mathbf{t} \\in \\mathcal{T}_M} d(\\mathbf{t})$, is returned as $D^*_M(P_n)$. This process is vectorized for efficiency, computing the local discrepancies for all grid points simultaneously.\n\n3.  **Calculation of Final Quantities**:\n    -   Calculate $E = D^*_{32}(P_n)$.\n    -   Calculate $B = D^*_{128}(P_n)$.\n    -   Calculate $\\Delta = B - E$.\n    -   Calculate $r = E/B$, with $r=0$ if $B=0$.\n\nThis procedure is applied to each of the three specified test cases, and the results $[E, B, \\Delta, r]$ are collected.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef van_der_corput(n, base):\n    \"\"\"Generates the first n terms of the van der Corput sequence for a given base.\"\"\"\n    sequence = np.zeros(n, dtype=float)\n    for i in range(n):\n        num = i + 1  # Use 1-based index for sequence generation\n        denom = float(base)\n        val = 0.0\n        while num > 0:\n            val += (num % base) / denom\n            num //= base\n            denom *= base\n        sequence[i] = val\n    return sequence\n\ndef halton_sequence(n, d, bases):\n    \"\"\"Generates an n-point Halton sequence in d dimensions.\"\"\"\n    if len(bases)  d:\n        raise ValueError(\"Number of bases must be at least d.\")\n    points = np.zeros((n, d), dtype=float)\n    for i in range(d):\n        points[:, i] = van_der_corput(n, bases[i])\n    return points\n\ndef compute_discrepancy(points, M):\n    \"\"\"Computes the approximate star discrepancy D*_M(P_n).\"\"\"\n    n, d = points.shape\n    if d != 2:\n        raise ValueError(f\"This implementation only supports d=2, but got d={d}.\")\n\n    t_coords = np.arange(1, M + 1, dtype=float) / M\n    t_grid_x, t_grid_y = np.meshgrid(t_coords, t_coords, indexing='ij')\n    \n    # Reshape grid for vectorized computation\n    # flat_t_vectors has shape (M*M, 2)\n    flat_t_vectors = np.stack((t_grid_x.ravel(), t_grid_y.ravel()), axis=-1)\n\n    # volumes has shape (M*M,)\n    volumes = flat_t_vectors[:, 0] * flat_t_vectors[:, 1]\n    \n    # Use broadcasting to compare all points to all test vectors\n    # points: (n, 1, 2)\n    # flat_t_vectors: (1, M*M, 2)\n    # in_box: (n, M*M)\n    in_box = np.all(points[:, np.newaxis, :]  flat_t_vectors[np.newaxis, :, :], axis=2)\n    \n    # counts has shape (M*M,)\n    counts = np.sum(in_box, axis=0)\n    \n    empirical_measures = counts / float(n)\n    \n    discrepancies = np.abs(empirical_measures - volumes)\n    \n    max_discrepancy = np.max(discrepancies)\n    \n    return max_discrepancy\n\ndef solve():\n    \"\"\"Main function to run the test cases and print results.\"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases_spec = [\n        {\n            \"name\": \"Halton\",\n            \"n\": 64,\n            \"generator\": lambda: halton_sequence(64, 2, [2, 3])\n        },\n        {\n            \"name\": \"Pseudo-random\",\n            \"n\": 64,\n            \"generator\": lambda: np.random.default_rng(12345).random((64, 2))\n        },\n        {\n            \"name\": \"Adversarial\",\n            \"n\": 16,\n            \"generator\": lambda: np.vstack([np.full((12, 2), 0.05), np.full((4, 2), 0.95)])\n        }\n    ]\n\n    M_c = 32\n    M_f = 128\n    \n    results = []\n    \n    for case_spec in test_cases_spec:\n        points = case_spec[\"generator\"]()\n        \n        # As derived in the solution, E is the discrepancy on the coarse grid.\n        # This is because the test function f is constructed from the maximizer\n        # on the coarse grid.\n        E = compute_discrepancy(points, M_c)\n        \n        # B is the KHI bound, approximated using the fine grid discrepancy.\n        # The variation V_HK(f) for the chosen function is 1.\n        B = compute_discrepancy(points, M_f)\n        \n        Delta = B - E\n        \n        # Ratio r = E/B, with r=0 if B=0.\n        r = E / B if B > 0 else 0.0\n        \n        results.append([E, B, Delta, r])\n\n    # Final print statement in the exact required format.\n    # The format [ [a, b], [c, d] ] is achieved by the default string\n    # representation of a list of lists in Python.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}