{
    "hands_on_practices": [
        {
            "introduction": "Simulating a Lévy process boils down to simulating the arrivals of its jumps, which are governed by a Poisson random measure. The thinning method, a form of acceptance-rejection sampling, is a fundamental technique for this task, allowing us to generate points from a complex intensity measure by using a simpler, dominating \"envelope\" measure. This exercise  provides direct practice with this core algorithm by asking you to analyze its efficiency, measured by the expected number of proposals needed for a given simulation horizon.",
            "id": "3342818",
            "problem": "Consider a Poisson random measure (PRM) on the product space $[0,t] \\times \\mathbb{R} \\setminus \\{0\\}$ with intensity measure $\\mathrm{d}s \\, \\nu(\\mathrm{d}x)$, where the Lévy density is given by\n$$\n\\nu(\\mathrm{d}x) = \\left( C |x|^{-1-\\alpha} \\mathbf{1}_{\\{|x|<1\\}} + D \\, e^{-\\lambda |x|} \\mathbf{1}_{\\{|x|\\ge 1\\}} \\right) \\mathrm{d}x,\n$$\nfor parameters $C>0$, $\\alpha>0$, $D \\ge 0$, and $\\lambda>0$. The PRM generates jumps of a pure-jump Lévy process with infinite activity near the origin when $C>0$ and $\\alpha>0$. To simulate, one typically introduces a truncation threshold $\\varepsilon \\in (0,1)$ and simulates only jumps with magnitude at least $\\varepsilon$, while handling the contribution from small jumps $\\{|x|<\\varepsilon\\}$ by approximation outside the scope of this task.\n\nConstruct an adaptive thinning algorithm to simulate the jump sizes for $|x|\\ge \\varepsilon$ over the time window $[0,t]$ based on the following principles:\n\n- Fundamental definition: A Poisson random measure on $[0,t]\\times E$ with intensity $\\mathrm{d}s\\,\\lambda(\\mathrm{d}x)$ can be simulated by thinning from a dominating measure $\\mathrm{d}s\\,\\mu(\\mathrm{d}x)$ with $\\mu(\\mathrm{d}x)\\ge \\lambda(\\mathrm{d}x)$, proposing a Poisson number of points with mean $t\\,\\mu(E)$ and accepting each proposed point $(s,x)$ with probability $\\lambda(\\mathrm{d}x)/\\mu(\\mathrm{d}x)$ evaluated at $x$.\n\n- Adaptive envelope design in the state variable:\n  1. On the tail region $\\{|x|\\ge 1\\}$, use the exact envelope equal to the Lévy density, $\\mu_{\\text{tail}}(\\mathrm{d}x) = D e^{-\\lambda |x|} \\mathbf{1}_{\\{|x|\\ge 1\\}} \\mathrm{d}x$.\n  2. On the truncated small-jump region $\\{\\varepsilon \\le |x| < 1\\}$, use a piecewise constant envelope adapted to the truncation level, $\\mu_{\\text{small}}(\\mathrm{d}x) = C \\varepsilon^{-1-\\alpha} \\mathbf{1}_{\\{\\varepsilon \\le |x| < 1\\}} \\mathrm{d}x$.\n\nThe combined envelope on $\\{|x|\\ge \\varepsilon\\}$ is\n$$\n\\mu_\\varepsilon(\\mathrm{d}x) = C \\varepsilon^{-1-\\alpha} \\mathbf{1}_{\\{\\varepsilon \\le |x| < 1\\}} \\mathrm{d}x \\;+\\; D e^{-\\lambda |x|} \\mathbf{1}_{\\{|x|\\ge 1\\}} \\mathrm{d}x,\n$$\nwhich satisfies $\\mu_\\varepsilon(\\mathrm{d}x) \\ge \\nu(\\mathrm{d}x)$ on $\\{|x|\\ge \\varepsilon\\}$. Proposals are generated from the PRM with intensity $\\mathrm{d}s\\,\\mu_\\varepsilon(\\mathrm{d}x)$ and thinned with acceptance probability $\\nu(x)/\\mu_\\varepsilon(x)$.\n\nYour task is to analyze from first principles the expected number of proposals as a function of the time horizon $t$ under this algorithm and implement a program that, for each specified parameter set, returns the expected number of proposals over $[0,t]$. The analysis must start from the definition of a Poisson random measure and the thinning principle. Use that the expected number of proposals equals the total mass of the envelope measure over $[0,t]\\times \\{|x|\\ge \\varepsilon\\}$.\n\nFor the adaptive envelope given above, derive a closed-form expression for the expected number of proposals over $[0,t]$ as a function of $t$, $C$, $\\alpha$, $D$, $\\lambda$, and $\\varepsilon$. Then, implement a program that computes this value for each of the following test cases:\n\n- Test case $1$: $(C,\\alpha,D,\\lambda,t,\\varepsilon) = (\\,0.5,\\,0.7,\\,0.8,\\,1.2,\\,3.5,\\,0.01\\,)$.\n- Test case $2$: $(C,\\alpha,D,\\lambda,t,\\varepsilon) = (\\,1.2,\\,1.5,\\,0.3,\\,0.9,\\,1.0,\\,0.001\\,)$.\n- Test case $3$: $(C,\\alpha,D,\\lambda,t,\\varepsilon) = (\\,0.0,\\,1.0,\\,1.0,\\,2.0,\\,10.0,\\,0.1\\,)$.\n- Test case $4$: $(C,\\alpha,D,\\lambda,t,\\varepsilon) = (\\,1.0,\\,0.3,\\,0.0,\\,1.0,\\,2.0,\\,0.2\\,)$.\n- Test case $5$: $(C,\\alpha,D,\\lambda,t,\\varepsilon) = (\\,2.0,\\,0.9,\\,0.5,\\,1.5,\\,0.75,\\,0.99\\,)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where $r_i$ is the expected number of proposals for test case $i$ as a floating-point number. No physical units are involved. Angles are not involved. Do not print any additional text.",
            "solution": "The problem statement has been validated and is deemed scientifically sound, well-posed, and objective. It is rooted in the established mathematical theory of Lévy processes and their simulation via Poisson random measures, presenting a standard academic problem in stochastic simulation. All parameters and conditions are clearly defined, and the premises are internally consistent. We may therefore proceed with the derivation and solution.\n\nThe objective is to find the expected number of proposals generated by an adaptive thinning algorithm for simulating the jumps of a Lévy process over the time interval $[0,t]$. The algorithm simulates jumps with magnitudes $|x| \\ge \\varepsilon$, where $\\varepsilon \\in (0,1)$ is a truncation threshold.\n\nAccording to the fundamental principles of Poisson processes, the expected number of points generated by a Poisson random measure (PRM) on a space $S$ with intensity measure $\\Lambda$ is equal to the total mass of the measure, given by the integral $\\int_S \\mathrm{d}\\Lambda$.\n\nIn this problem, the proposals are generated from a PRM on the space $[0,t] \\times \\{x \\in \\mathbb{R} : |x| \\ge \\varepsilon\\}$. The intensity measure for these proposals is specified as $\\mathrm{d}s\\,\\mu_\\varepsilon(\\mathrm{d}x)$, where $\\mu_\\varepsilon(\\mathrm{d}x)$ is the measure corresponding to a dominating envelope density. The expected number of proposals, denoted $\\mathbb{E}[N_{\\text{prop}}]$, is the integral of this intensity measure over the entire simulation domain:\n$$\n\\mathbb{E}[N_{\\text{prop}}] = \\int_{0}^{t} \\int_{\\{x : |x| \\ge \\varepsilon\\}} \\mu_\\varepsilon(\\mathrm{d}x) \\, \\mathrm{d}s\n$$\nThe envelope density, $\\mu_\\varepsilon(x)$, is given and does not depend on the time variable $s$. Therefore, the integral over the time interval $[0,t]$ can be separated and evaluates to $t$:\n$$\n\\mathbb{E}[N_{\\text{prop}}] = t \\int_{\\{x : |x| \\ge \\varepsilon\\}} \\mu_\\varepsilon(\\mathrm{d}x)\n$$\nThe task now reduces to computing the total mass of the spatial envelope measure $\\mu_\\varepsilon(\\mathrm{d}x)$ over the domain $\\{x : |x| \\ge \\varepsilon\\}$. Let us denote this total mass, or total proposal rate, by $\\Lambda_\\varepsilon$:\n$$\n\\Lambda_\\varepsilon = \\int_{\\{x : |x| \\ge \\varepsilon\\}} \\mu_\\varepsilon(\\mathrm{d}x)\n$$\nThe governing envelope measure is defined piecewise from its density:\n$$\n\\mu_\\varepsilon(\\mathrm{d}x) = \\left( C \\varepsilon^{-1-\\alpha} \\mathbf{1}_{\\{\\varepsilon \\le |x| < 1\\}} + D e^{-\\lambda |x|} \\mathbf{1}_{\\{|x|\\ge 1\\}} \\right) \\mathrm{d}x\n$$\nwhere $\\mathbf{1}_{A}$ is the indicator function for a set $A$. The integration domain $\\{x : |x| \\ge \\varepsilon\\}$ is the union of two disjoint sets: $\\{\\varepsilon \\le |x| < 1\\}$ and $\\{|x| \\ge 1\\}$. We can thus split the integral for $\\Lambda_\\varepsilon$ into two parts:\n$$\n\\Lambda_\\varepsilon = \\int_{\\{\\varepsilon \\le |x| < 1\\}} C \\varepsilon^{-1-\\alpha} \\mathrm{d}x + \\int_{\\{|x| \\ge 1\\}} D e^{-\\lambda |x|} \\mathrm{d}x\n$$\nWe evaluate each integral separately.\n\nFor the first integral, corresponding to the truncated small-jump region, the integrand $C \\varepsilon^{-1-\\alpha}$ is a constant with respect to the integration variable $x$. The integration domain $\\{\\varepsilon \\le |x| < 1\\}$ is $(-1, -\\varepsilon] \\cup [\\varepsilon, 1)$. The total length (Lebesgue measure) of this domain is $(1 - \\varepsilon) + (1 - \\varepsilon) = 2(1-\\varepsilon)$.\nAlternatively, we can express the integral explicitly, exploiting the symmetry of the integrand:\n$$\n\\int_{\\{\\varepsilon \\le |x| < 1\\}} C \\varepsilon^{-1-\\alpha} \\mathrm{d}x = 2 \\int_{\\varepsilon}^{1} C \\varepsilon^{-1-\\alpha} \\mathrm{d}x = 2 C \\varepsilon^{-1-\\alpha} \\int_{\\varepsilon}^{1} 1 \\, \\mathrm{d}x = 2 C \\varepsilon^{-1-\\alpha} [x]_{\\varepsilon}^{1} = 2 C \\varepsilon^{-1-\\alpha} (1-\\varepsilon)\n$$\n\nFor the second integral, corresponding to the large-jump tail region, we again use the symmetry of the integrand $e^{-\\lambda |x|}$:\n$$\n\\int_{\\{|x| \\ge 1\\}} D e^{-\\lambda |x|} \\mathrm{d}x = 2 \\int_{1}^{\\infty} D e^{-\\lambda x} \\mathrm{d}x\n$$\nThis is a standard exponential integral:\n$$\n2 D \\int_{1}^{\\infty} e^{-\\lambda x} \\mathrm{d}x = 2 D \\left[ -\\frac{1}{\\lambda} e^{-\\lambda x} \\right]_{1}^{\\infty} = 2 D \\left( \\lim_{x\\to\\infty} \\left(-\\frac{e^{-\\lambda x}}{\\lambda}\\right) - \\left(-\\frac{e^{-\\lambda \\cdot 1}}{\\lambda}\\right) \\right)\n$$\nSince $\\lambda > 0$, the limit is $0$. The integral evaluates to:\n$$\n= 2 D \\left( 0 + \\frac{e^{-\\lambda}}{\\lambda} \\right) = \\frac{2D}{\\lambda}e^{-\\lambda}\n$$\n\nCombining the two results, the total spatial proposal rate $\\Lambda_\\varepsilon$ is:\n$$\n\\Lambda_\\varepsilon = 2 C (1-\\varepsilon) \\varepsilon^{-1-\\alpha} + \\frac{2D}{\\lambda}e^{-\\lambda}\n$$\n\nFinally, the expected number of proposals over the time interval $[0,t]$ is obtained by multiplying the total spatial rate $\\Lambda_\\varepsilon$ by the time duration $t$:\n$$\n\\mathbb{E}[N_{\\text{prop}}] = t \\, \\Lambda_\\varepsilon = t \\left( 2 C (1-\\varepsilon) \\varepsilon^{-1-\\alpha} + \\frac{2D}{\\lambda}e^{-\\lambda} \\right)\n$$\nThis is the required closed-form expression. The provided parameters are $C>0$ (or $C=0$), $\\alpha>0$, $D \\ge 0$, $\\lambda>0$, $t>0$, and $\\varepsilon \\in (0,1)$. The formula is well-defined for all these parameter values. If $C=0$, the first term vanishes. If $D=0$, the second term vanishes. Both cases are handled correctly by the derived expression. We will now implement this formula to compute the result for the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the expected number of proposals for a Lévy process simulation\n    using an adaptive thinning algorithm based on a derived closed-form expression.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (C, alpha, D, lambda, t, epsilon)\n    test_cases = [\n        (0.5, 0.7, 0.8, 1.2, 3.5, 0.01),\n        (1.2, 1.5, 0.3, 0.9, 1.0, 0.001),\n        (0.0, 1.0, 1.0, 2.0, 10.0, 0.1),\n        (1.0, 0.3, 0.0, 1.0, 2.0, 0.2),\n        (2.0, 0.9, 0.5, 1.5, 0.75, 0.99),\n    ]\n\n    results = []\n    for case in test_cases:\n        C, alpha, D, lambda_param, t, epsilon = case\n\n        # The derived closed-form formula for the expected number of proposals is:\n        # E[N_prop] = t * (Lambda_small + Lambda_large)\n        # where Lambda_small = 2 * C * (1 - epsilon) * epsilon**(-1 - alpha)\n        # and Lambda_large = (2 * D / lambda) * exp(-lambda)\n\n        # Term for the small-jump region {epsilon <= |x| < 1}\n        # This term is zero if C is zero.\n        if C > 0:\n            term_small_jumps = 2 * C * (1 - epsilon) * (epsilon ** (-1 - alpha))\n        else:\n            term_small_jumps = 0.0\n        \n        # Term for the large-jump region {|x| >= 1}\n        # This term is zero if D is zero.\n        if D > 0:\n            term_large_jumps = (2 * D / lambda_param) * np.exp(-lambda_param)\n        else:\n            term_large_jumps = 0.0\n\n        # Total spatial proposal rate\n        total_spatial_rate = term_small_jumps + term_large_jumps\n        \n        # Expected number of proposals over time interval [0, t]\n        expected_proposals = t * total_spatial_rate\n        \n        results.append(expected_proposals)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In practice, simulating Lévy processes with infinite activity requires us to truncate small jumps below a certain threshold, $\\varepsilon$, which introduces an approximation error, or bias. At the same time, simulating the remaining jumps incurs a computational cost, which limits the number of Monte Carlo paths we can run under a fixed budget, affecting statistical variance. This exercise  places you in the role of a computational scientist, tasking you with navigating this classic bias-variance trade-off to find the optimal truncation level that minimizes the total error of your simulation.",
            "id": "3342762",
            "problem": "Consider a symmetric pure-jump Lévy process $X = \\{X_t\\}_{t \\in [0,T]}$ on a fixed horizon $[0,T]$ with Lévy measure given by $\\,\\nu(dx) = c\\,|x|^{-1-\\alpha}\\,\\mathbf{1}_{\\{|x|<1\\}}\\,dx\\,$ for some constants $c>0$ and $0<\\alpha<2$. The Lévy-Itô decomposition represents $X$ as a stochastic integral with respect to a Poisson random measure associated with $\\nu$, and the jump contribution over $[0,T]$ can be described via the Poisson random measure with intensity $T\\,\\nu(dx)$.\n\nYou are tasked with estimating the quadratic jump variation functional over $[0,T]$, defined by $J_T = \\sum_{0<s\\leq T}(\\Delta X_s)^2$, via Monte Carlo simulation. The exact simulation of $J_T$ is infeasible due to the infinite activity of small jumps near zero. Instead, consider an $\\varepsilon$-truncation strategy that simulates only jumps with $|x|>\\varepsilon$ exactly and discards jumps with $|x|\\leq\\varepsilon$. For a single path under truncation level $\\varepsilon$, denote by $Y^{(\\varepsilon)}$ the sum of squared jump magnitudes for $|x|>\\varepsilon$ over $[0,T]$.\n\nAssume the following cost model for simulating one path with truncation threshold $\\varepsilon$: each simulated jump costs $c_1>0$ units of work, and there is a per-path overhead cost $c_0>0$. The expected number of simulated jumps above threshold over $[0,T]$ is $T\\,\\lambda(\\varepsilon)$, where $\\lambda(\\varepsilon)=\\int_{\\varepsilon<|x|<1}\\nu(dx)$. Thus the expected computational cost per path is $C(\\varepsilon)=c_0 + c_1\\,T\\,\\lambda(\\varepsilon)$. Under a fixed total work budget $B>0$, the number of independent Monte Carlo paths is $N(\\varepsilon) = B / C(\\varepsilon)$.\n\nUsing only the core definitions of Lévy processes and well-tested properties of Poisson random measures (mean and variance of integrals against Poisson random measures), derive, as $\\varepsilon \\downarrow 0$ in the regime where the per-path overhead $c_0$ is negligible compared to the jump simulation cost $c_1\\,T\\,\\lambda(\\varepsilon)$, the asymptotic truncation level $\\varepsilon^{\\star}$ that minimizes the total mean squared error for estimating $\\mathbb{E}[J_T]$ by the sample mean of $Y^{(\\varepsilon)}$ over $N(\\varepsilon)$ paths. Your decision framework must combine the squared bias due to discarding small jumps and the Monte Carlo variance under the budget constraint to produce a single closed-form asymptotic expression for $\\varepsilon^{\\star}$ in terms of $B$, $c_1$, and $\\alpha$. Provide the final expression for $\\varepsilon^{\\star}$ and do not round; an exact analytical expression is required. No physical units are involved in this problem.",
            "solution": "The user-provided problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Process:** A symmetric pure-jump Lévy process $X = \\{X_t\\}_{t \\in [0,T]}$ on a time horizon $[0,T]$.\n- **Lévy Measure:** $\\nu(dx) = c\\,|x|^{-1-\\alpha}\\,\\mathbf{1}_{\\{|x|<1\\}}\\,dx$.\n- **Constants:** $c>0$, $T>0$, and $0<\\alpha<2$.\n- **Functional of Interest:** The quadratic jump variation $J_T = \\sum_{0<s\\leq T}(\\Delta X_s)^2$.\n- **Estimator:** An $\\varepsilon$-truncated sum of squared jumps, $Y^{(\\varepsilon)} = \\sum_{0<s\\leq T, |\\Delta X_s| > \\varepsilon}(\\Delta X_s)^2$.\n- **Cost Model:** Per-path cost is $C(\\varepsilon)=c_0 + c_1\\,T\\,\\lambda(\\varepsilon)$, with $c_0>0$ and $c_1>0$.\n- **Jump Intensity:** The expected number of jumps with magnitude greater than $\\varepsilon$ per unit time is $\\lambda(\\varepsilon)=\\int_{\\varepsilon<|x|<1}\\nu(dx)$.\n- **Budget Constraint:** Total work budget is $B>0$, yielding $N(\\varepsilon) = B / C(\\varepsilon)$ independent paths.\n- **Asymptotic Regime:** The overhead $c_0$ is negligible compared to the jump simulation cost, i.e., $C(\\varepsilon) \\approx c_1\\,T\\,\\lambda(\\varepsilon)$ as $\\varepsilon \\downarrow 0$.\n- **Objective:** Find the asymptotic optimal truncation level $\\varepsilon^{\\star}$ that minimizes the total Mean Squared Error (MSE) for the sample mean of $Y^{(\\varepsilon)}$, as a function of $B$, $c_1$, and $\\alpha$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the field of stochastic simulation and computational statistics.\n- **Scientific Grounding:** The problem uses standard concepts from the theory of Lévy processes (Lévy measure, Lévy-Itô decomposition) and Monte Carlo methods (MSE, bias-variance tradeoff, budget constraints). The form of the Lévy measure is characteristic of symmetric stable-like processes, and the condition $0 < \\alpha < 2$ is the standard parameter range for such processes to be well-defined. All components are scientifically sound.\n- **Well-Posedness:** The objective is to minimize a well-defined function (MSE) with respect to a single parameter ($\\varepsilon$) subject to a clear constraint (budget $B$). The target is an asymptotic expression, which is a standard and solvable task in analysis.\n- **Objectivity:** The problem is stated in precise mathematical language, free from ambiguity or subjective elements.\n- **Completeness and Consistency:** All necessary information—the process, the estimator, the cost model, and the optimization criterion—is provided. The assumptions, such as the negligible overhead cost in the small $\\varepsilon$ limit, are explicitly stated. There are no internal contradictions.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a standard, non-trivial problem in the analysis of Monte Carlo methods for jump-diffusion processes. We proceed to the solution.\n\n### Derivation of the Optimal Truncation Level\nThe goal is to minimize the Mean Squared Error (MSE) of the estimator $\\hat{J}_{T,\\varepsilon} = \\frac{1}{N(\\varepsilon)}\\sum_{i=1}^{N(\\varepsilon)} Y_i^{(\\varepsilon)}$, where $Y_i^{(\\varepsilon)}$ are i.i.d. copies of the truncated functional $Y^{(\\varepsilon)}$. The MSE is decomposed into squared bias and variance:\n$$ \\text{MSE}(\\varepsilon) = \\left( \\mathbb{E}\\left[\\hat{J}_{T,\\varepsilon}\\right] - \\mathbb{E}[J_T] \\right)^2 + \\text{Var}\\left(\\hat{J}_{T,\\varepsilon}\\right) = \\left(\\text{Bias}(\\varepsilon)\\right)^2 + \\frac{\\text{Var}(Y^{(\\varepsilon)})}{N(\\varepsilon)} $$\n\n**1. Bias Analysis**\nThe bias of the estimator arises from neglecting jumps with magnitude $|x| \\leq \\varepsilon$. Using the property that the expectation of an integral against a Poisson random measure is the integral against its intensity measure ($T\\,\\nu(dx)$), we have:\n$$ \\mathbb{E}[Y^{(\\varepsilon)}] = T \\int_{\\varepsilon < |x| < 1} x^2 \\nu(dx) $$\n$$ \\mathbb{E}[J_T] = T \\int_{|x|<1} x^2 \\nu(dx) $$\nThe bias is the expectation of the discarded part:\n$$ \\text{Bias}(\\varepsilon) = \\mathbb{E}[Y^{(\\varepsilon)}] - \\mathbb{E}[J_T] = -T \\int_{|x| \\leq \\varepsilon} x^2 \\nu(dx) $$\nSubstituting the Lévy measure $\\nu(dx) = c|x|^{-1-\\alpha}dx$:\n$$ \\text{Bias}(\\varepsilon) = -T \\int_{-\\varepsilon}^{\\varepsilon} x^2 c|x|^{-1-\\alpha} dx = -2cT \\int_{0}^{\\varepsilon} x^{1-\\alpha} dx $$\n$$ \\text{Bias}(\\varepsilon) = -2cT \\left[ \\frac{x^{2-\\alpha}}{2-\\alpha} \\right]_0^\\varepsilon = -\\frac{2cT}{2-\\alpha} \\varepsilon^{2-\\alpha} $$\nThe squared bias is therefore:\n$$ \\left(\\text{Bias}(\\varepsilon)\\right)^2 = \\left( \\frac{2cT}{2-\\alpha} \\right)^2 \\varepsilon^{2(2-\\alpha)} = \\frac{4c^2T^2}{(2-\\alpha)^2} \\varepsilon^{4-2\\alpha} $$\n\n**2. Variance and Cost Analysis**\nThe variance of the Monte Carlo estimator depends on $\\text{Var}(Y^{(\\varepsilon)})$ and the number of samples $N(\\varepsilon)$.\nFirst, we compute the variance of the single-path estimator $Y^{(\\varepsilon)}$. Using the property that the variance of an integral against a Poisson random measure is the integral of the squared function against its intensity measure:\n$$ \\text{Var}(Y^{(\\varepsilon)}) = T \\int_{\\varepsilon < |x| < 1} (x^2)^2 \\nu(dx) = T \\int_{\\varepsilon < |x| < 1} x^4 c|x|^{-1-\\alpha} dx $$\n$$ \\text{Var}(Y^{(\\varepsilon)}) = 2cT \\int_{\\varepsilon}^{1} x^{3-\\alpha} dx = 2cT \\left[ \\frac{x^{4-\\alpha}}{4-\\alpha} \\right]_\\varepsilon^1 = \\frac{2cT}{4-\\alpha} (1 - \\varepsilon^{4-\\alpha}) $$\nAs $\\varepsilon \\downarrow 0$, this variance converges to a constant:\n$$ \\text{Var}(Y^{(\\varepsilon)}) \\approx \\frac{2cT}{4-\\alpha} $$\nNext, we determine the number of paths $N(\\varepsilon)$. The cost per path depends on the jump intensity $\\lambda(\\varepsilon)$:\n$$ \\lambda(\\varepsilon) = \\int_{\\varepsilon < |x| < 1} \\nu(dx) = 2c \\int_{\\varepsilon}^{1} x^{-1-\\alpha} dx = 2c \\left[ \\frac{x^{-\\alpha}}{-\\alpha} \\right]_\\varepsilon^1 = \\frac{2c}{\\alpha}(\\varepsilon^{-\\alpha} - 1) $$\nIn the asymptotic regime $\\varepsilon \\downarrow 0$, $\\varepsilon^{-\\alpha}$ dominates, so $\\lambda(\\varepsilon) \\approx \\frac{2c}{\\alpha}\\varepsilon^{-\\alpha}$. The cost per path is $C(\\varepsilon) \\approx c_1 T \\lambda(\\varepsilon) \\approx \\frac{2cc_1T}{\\alpha}\\varepsilon^{-\\alpha}$. The number of paths under budget $B$ is:\n$$ N(\\varepsilon) = \\frac{B}{C(\\varepsilon)} \\approx \\frac{B}{\\frac{2cc_1T}{\\alpha}\\varepsilon^{-\\alpha}} = \\frac{B\\alpha}{2cc_1T}\\varepsilon^{\\alpha} $$\nThe variance of the final estimator is:\n$$ \\text{Var}\\left(\\hat{J}_{T,\\varepsilon}\\right) = \\frac{\\text{Var}(Y^{(\\varepsilon)})}{N(\\varepsilon)} \\approx \\frac{\\frac{2cT}{4-\\alpha}}{\\frac{B\\alpha}{2cc_1T}\\varepsilon^{\\alpha}} = \\frac{4c^2T^2c_1}{B\\alpha(4-\\alpha)} \\varepsilon^{-\\alpha} $$\n\n**3. MSE Minimization**\nWe now combine the squared bias and variance terms to form the asymptotic MSE:\n$$ \\text{MSE}(\\varepsilon) \\approx \\frac{4c^2T^2}{(2-\\alpha)^2} \\varepsilon^{4-2\\alpha} + \\frac{4c^2T^2c_1}{B\\alpha(4-\\alpha)} \\varepsilon^{-\\alpha} $$\nTo find the optimal $\\varepsilon^{\\star}$, we can minimize the function $f(\\varepsilon) = A \\varepsilon^{4-2\\alpha} + C \\varepsilon^{-\\alpha}$, where the constant factor $4c^2T^2$ can be ignored for optimization. Let:\n$$ g(\\varepsilon) = \\frac{1}{(2-\\alpha)^2} \\varepsilon^{4-2\\alpha} + \\frac{c_1}{B\\alpha(4-\\alpha)} \\varepsilon^{-\\alpha} $$\nDifferentiating with respect to $\\varepsilon$ and setting to zero:\n$$ \\frac{dg}{d\\varepsilon} = \\frac{4-2\\alpha}{(2-\\alpha)^2} \\varepsilon^{3-2\\alpha} - \\frac{\\alpha c_1}{B\\alpha(4-\\alpha)} \\varepsilon^{-\\alpha-1} = 0 $$\n$$ \\frac{2(2-\\alpha)}{(2-\\alpha)^2} \\varepsilon^{3-2\\alpha} = \\frac{c_1}{B(4-\\alpha)} \\varepsilon^{-\\alpha-1} $$\n$$ \\frac{2}{2-\\alpha} \\varepsilon^{3-2\\alpha} = \\frac{c_1}{B(4-\\alpha)} \\varepsilon^{-\\alpha-1} $$\nAssuming $\\varepsilon \\neq 0$, we multiply by $\\varepsilon^{\\alpha+1}$:\n$$ \\frac{2}{2-\\alpha} \\varepsilon^{(3-2\\alpha)+(\\alpha+1)} = \\frac{c_1}{B(4-\\alpha)} $$\n$$ \\frac{2}{2-\\alpha} \\varepsilon^{4-\\alpha} = \\frac{c_1}{B(4-\\alpha)} $$\nFinally, we solve for $\\varepsilon^{\\star}$:\n$$ \\varepsilon^{4-\\alpha} = \\frac{c_1(2-\\alpha)}{2B(4-\\alpha)} $$\n$$ \\varepsilon^{\\star} = \\left( \\frac{c_1(2-\\alpha)}{2B(4-\\alpha)} \\right)^{\\frac{1}{4-\\alpha}} $$\nThis expression gives the optimal truncation level in the specified asymptotic regime, balancing the systematic error from truncation (bias) against the statistical error from the Monte Carlo simulation (variance) under a fixed computational budget. The problem constants $c$ and $T$ cancel out, indicating that the optimal balance point is independent of the overall jump activity rate and time horizon, a common result in such optimization problems.",
            "answer": "$$\\boxed{\\left( \\frac{c_1(2-\\alpha)}{2B(4-\\alpha)} \\right)^{\\frac{1}{4-\\alpha}}}$$"
        },
        {
            "introduction": "After choosing an approximation scheme, such as simplifying the distribution of small jumps, it is crucial to formally quantify the resulting error. This practice problem  delves into the rigorous analysis of simulation accuracy by using the Wasserstein distance, a powerful metric for comparing the laws of two stochastic processes. By deriving a concrete upper bound for this distance, you will gain insight into how the error depends on the parameters of the approximation, connecting theoretical error bounds to practical simulation design.",
            "id": "3342746",
            "problem": "You are given a symmetric pure-jump Lévy process with Lévy measure defined by the density\n$$\n\\nu(dx) \\;=\\; c_s\\,|x|^{-p}\\,\\mathbf{1}_{\\{|x|\\le 1\\}}\\,dx \\;+\\; c_\\ell\\,|x|^{-1-\\alpha}\\,\\mathbf{1}_{\\{|x|>1\\}}\\,dx,\n$$\nwhere $c_s>0$, $c_\\ell>0$, $0<p<1$, and $1<\\alpha<2$. Consider a fixed time horizon $T>0$ and a truncation level $0<\\varepsilon\\le 1$. The Lévy-Itô decomposition implies that the small-jump component on $\\{|x|\\le \\varepsilon\\}$ is given by the compensated Poisson integral\n$$\nS_\\varepsilon(T) \\;=\\; \\int_0^T \\int_{|x|\\le \\varepsilon} x \\,\\tilde{N}(dt,dx),\n$$\nwhere $\\tilde{N}$ denotes the compensated Poisson random measure (PRM) associated with the Lévy measure $ \\nu(dx)$.\n\nWe define an approximation of the small-jump component by replacing $S_\\varepsilon(T)$ with a compensated compound Poisson process $\\widetilde{S}_\\varepsilon(T)$ that has the same small-jump intensity but a simplified truncated jump distribution:\n- The approximation uses intensity\n$$\n\\lambda_\\varepsilon \\;=\\; \\int_{|x|\\le \\varepsilon} \\nu(dx) \\;=\\; \\frac{2\\,c_s}{1-p}\\,\\varepsilon^{1-p},\n$$\nwhich is finite because $0<p<1$.\n- The approximate jump distribution is symmetric uniform on $[-\\varepsilon,\\varepsilon]$, that is, with density\n$$\nf_{\\text{approx}}(x) \\;=\\; \\frac{1}{2\\varepsilon}\\,\\mathbf{1}_{\\{|x|\\le\\varepsilon\\}}.\n$$\n- The approximation is compensated (mean zero), so $\\widetilde{S}_\\varepsilon(T)$ is the centered sum of compound Poisson jumps from $f_{\\text{approx}}$ with rate $\\lambda_\\varepsilon$ over time interval $[0,T]$.\n\nLet $X_T$ denote the true Lévy process at time $T$ and $Y_T$ denote the approximation at time $T$ obtained by coupling the large-jump part (i.e., $\\{|x|>\\varepsilon\\}$) identically and replacing the small-jump part $S_\\varepsilon(T)$ by $\\widetilde{S}_\\varepsilon(T)$ as above. Consider the Wasserstein-$2$ distance (also called quadratic Kantorovich distance) between the laws of $X_T$ and $Y_T$, defined by\n$$\nW_2(\\mathcal{L}(X_T),\\mathcal{L}(Y_T)) \\;=\\; \\inf_{\\pi\\in\\Pi(\\mathcal{L}(X_T),\\mathcal{L}(Y_T))} \\left( \\int |x-y|^2\\,d\\pi(x,y) \\right)^{1/2},\n$$\nwhere $\\Pi(\\mathcal{L}(X_T),\\mathcal{L}(Y_T))$ is the set of all couplings of the two laws. Using a canonical coupling that shares the large-jump paths and couples each small jump with an independent uniform approximate jump, derive an explicit, closed-form upper bound of the form\n$$\nW_2(\\mathcal{L}(X_T),\\mathcal{L}(Y_T)) \\;\\le\\; \\sqrt{ T\\,\\lambda_\\varepsilon \\,\\Big( \\operatorname{Var}(J_\\varepsilon) + \\operatorname{Var}(U_\\varepsilon) \\Big) },\n$$\nwhere $J_\\varepsilon$ is a random variable with density proportional to $|x|^{-p}$ on $[-\\varepsilon,\\varepsilon]$ (the true small-jump law conditioned on the truncation), and $U_\\varepsilon$ is uniform on $[-\\varepsilon,\\varepsilon]$. Your task is to:\n- Derive the explicit formula for $\\operatorname{Var}(J_\\varepsilon)$ and $\\operatorname{Var}(U_\\varepsilon)$ in terms of $p$ and $\\varepsilon$;\n- Simplify the bound algebraically to a single closed-form expression depending only on $c_s$, $p$, $\\varepsilon$, and $T$;\n- Implement a program that computes this bound for a given set of $(c_s,p,\\varepsilon,T)$.\n\nUse the following test suite (each test case is a tuple $(c_s,p,\\varepsilon,T)$):\n- Test $1$ (happy path): $(1.0,0.5,0.1,1.0)$.\n- Test $2$ (smaller truncation): $(1.0,0.5,0.05,1.0)$.\n- Test $3$ (near-boundary small-jump integrability): $(1.0,0.9,0.1,1.0)$.\n- Test $4$ (larger truncation): $(1.0,0.5,0.2,1.0)$.\n- Test $5$ (zero horizon boundary): $(1.0,0.5,0.1,0.0)$.\n- Test $6$ (stronger small-jump intensity): $(5.0,0.5,0.1,1.0)$.\n\nYour program should produce a single line of output containing the upper bounds for the test suite as a comma-separated list enclosed in square brackets (e.g., $[b_1,b_2,\\dots,b_6]$), where each $b_i$ is a floating-point number. No physical units are involved, and angles are not applicable. All outputs must be real numbers in standard decimal notation.",
            "solution": "The user has provided a valid, well-posed problem in the field of stochastic simulation. The problem is scientifically grounded in the theory of Lévy processes and asks for the derivation and implementation of an error bound for a specific approximation scheme. All required parameters and definitions are present, and the problem is free of contradictions or ambiguities. I will now proceed with the solution.\n\nThe objective is to derive a closed-form expression for the upper bound on the Wasserstein-$2$ distance between the true Lévy process $X_T$ and an approximate process $Y_T$. The provided form of the bound is\n$$\nW_2(\\mathcal{L}(X_T),\\mathcal{L}(Y_T)) \\le \\sqrt{ T\\,\\lambda_\\varepsilon \\,\\Big( \\operatorname{Var}(J_\\varepsilon) + \\operatorname{Var}(U_\\varepsilon) \\Big) },\n$$\nwhere $T$ is the time horizon, $\\lambda_\\varepsilon$ is the small-jump intensity, $J_\\varepsilon$ is a random variable representing the true small-jump size, and $U_\\varepsilon$ is a random variable representing the approximate small-jump size. The task requires calculating the variances of $J_\\varepsilon$ and $U_\\varepsilon$ and substituting them into the formula to obtain a final expression in terms of the parameters $c_s$, $p$, $\\varepsilon$, and $T$.\n\nThe given small-jump intensity is\n$$\n\\lambda_\\varepsilon = \\frac{2\\,c_s}{1-p}\\,\\varepsilon^{1-p}.\n$$\nOur derivation proceeds in three steps: first, we compute $\\operatorname{Var}(U_\\varepsilon)$; second, we compute $\\operatorname{Var}(J_\\varepsilon)$; third, we combine these results to obtain the final simplified bound.\n\n**Step 1: Variance of the Approximate Jump $U_\\varepsilon$**\n\nThe random variable $U_\\varepsilon$ represents a jump from the approximate distribution, which is symmetric uniform on the interval $[-\\varepsilon, \\varepsilon]$. The probability density function (PDF) of $U_\\varepsilon$ is\n$$\nf_U(u) = \\frac{1}{2\\varepsilon}\\,\\mathbf{1}_{\\{|u|\\le\\varepsilon\\}}.\n$$\nBy symmetry, the expected value of $U_\\varepsilon$ is zero:\n$$\n\\operatorname{E}[U_\\varepsilon] = \\int_{-\\varepsilon}^{\\varepsilon} u \\cdot \\frac{1}{2\\varepsilon} \\,du = \\frac{1}{2\\varepsilon} \\left[ \\frac{u^2}{2} \\right]_{-\\varepsilon}^{\\varepsilon} = \\frac{1}{4\\varepsilon} (\\varepsilon^2 - (-\\varepsilon)^2) = 0.\n$$\nThe variance is therefore equal to the second moment, $\\operatorname{E}[U_\\varepsilon^2]$:\n$$\n\\operatorname{Var}(U_\\varepsilon) = \\operatorname{E}[U_\\varepsilon^2] = \\int_{-\\varepsilon}^{\\varepsilon} u^2 \\cdot \\frac{1}{2\\varepsilon} \\,du = \\frac{1}{2\\varepsilon} \\left[ \\frac{u^3}{3} \\right]_{-\\varepsilon}^{\\varepsilon}.\n$$\nEvaluating the integral gives\n$$\n\\operatorname{Var}(U_\\varepsilon) = \\frac{1}{2\\varepsilon} \\left( \\frac{\\varepsilon^3}{3} - \\frac{(-\\varepsilon)^3}{3} \\right) = \\frac{1}{2\\varepsilon} \\left( \\frac{2\\varepsilon^3}{3} \\right) = \\frac{\\varepsilon^2}{3}.\n$$\n\n**Step 2: Variance of the True Small Jump $J_\\varepsilon$**\n\nThe random variable $J_\\varepsilon$ has a distribution corresponding to the true small-jump component of the Lévy measure, restricted to $\\{|x|\\le \\varepsilon\\}$. Its density is proportional to $|x|^{-p}$ on $[-\\varepsilon, \\varepsilon]$. Let the PDF of $J_\\varepsilon$ be $f_J(x) = C |x|^{-p}$ for $x \\in [-\\varepsilon, \\varepsilon]$, where $C$ is a normalization constant. We find $C$ by ensuring the integral of the PDF is $1$:\n$$\n\\int_{-\\varepsilon}^{\\varepsilon} C |x|^{-p} \\,dx = 1.\n$$\n$$\nC \\cdot 2 \\int_{0}^{\\varepsilon} x^{-p} \\,dx = C \\cdot 2 \\left[ \\frac{x^{1-p}}{1-p} \\right]_{0}^{\\varepsilon} = C \\cdot \\frac{2\\varepsilon^{1-p}}{1-p} = 1.\n$$\nThe condition $0 < p < 1$ ensures that $1-p > 0$, so the integral converges. The normalization constant is\n$$\nC = \\frac{1-p}{2\\varepsilon^{1-p}}.\n$$\nThus, the PDF is $f_J(x) = \\frac{1-p}{2\\varepsilon^{1-p}} |x|^{-p}$ for $x \\in [-\\varepsilon, \\varepsilon]$.\nBy symmetry, the expected value $\\operatorname{E}[J_\\varepsilon] = 0$. The variance is equal to the second moment, $\\operatorname{E}[J_\\varepsilon^2]$:\n$$\n\\operatorname{Var}(J_\\varepsilon) = \\operatorname{E}[J_\\varepsilon^2] = \\int_{-\\varepsilon}^{\\varepsilon} x^2 \\cdot f_J(x) \\,dx = \\int_{-\\varepsilon}^{\\varepsilon} x^2 \\cdot \\frac{1-p}{2\\varepsilon^{1-p}} |x|^{-p} \\,dx.\n$$\n$$\n\\operatorname{Var}(J_\\varepsilon) = \\frac{1-p}{2\\varepsilon^{1-p}} \\int_{-\\varepsilon}^{\\varepsilon} |x|^{2-p} \\,dx = \\frac{1-p}{2\\varepsilon^{1-p}} \\cdot 2 \\int_{0}^{\\varepsilon} x^{2-p} \\,dx.\n$$\nThe condition $0 < p < 1$ implies $2-p > 1$, so the integral converges.\n$$\n\\operatorname{Var}(J_\\varepsilon) = \\frac{1-p}{\\varepsilon^{1-p}} \\left[ \\frac{x^{3-p}}{3-p} \\right]_{0}^{\\varepsilon} = \\frac{1-p}{\\varepsilon^{1-p}} \\cdot \\frac{\\varepsilon^{3-p}}{3-p}.\n$$\nSimplifying the expression for the variance gives\n$$\n\\operatorname{Var}(J_\\varepsilon) = \\frac{1-p}{3-p} \\cdot \\varepsilon^{(3-p)-(1-p)} = \\frac{1-p}{3-p} \\varepsilon^2.\n$$\n\n**Step 3: Assembling the Final Bound**\n\nNow we substitute the expressions for $\\lambda_\\varepsilon$, $\\operatorname{Var}(U_\\varepsilon)$, and $\\operatorname{Var}(J_\\varepsilon)$ into the given inequality. Let $B$ be the upper bound.\n$$\nB^2 = T\\,\\lambda_\\varepsilon \\,\\Big( \\operatorname{Var}(J_\\varepsilon) + \\operatorname{Var}(U_\\varepsilon) \\Big).\n$$\nFirst, we sum the variances:\n$$\n\\operatorname{Var}(J_\\varepsilon) + \\operatorname{Var}(U_\\varepsilon) = \\frac{1-p}{3-p} \\varepsilon^2 + \\frac{1}{3} \\varepsilon^2 = \\varepsilon^2 \\left( \\frac{1-p}{3-p} + \\frac{1}{3} \\right).\n$$\n$$\n= \\varepsilon^2 \\left( \\frac{3(1-p) + (3-p)}{3(3-p)} \\right) = \\varepsilon^2 \\left( \\frac{3 - 3p + 3 - p}{3(3-p)} \\right) = \\varepsilon^2 \\left( \\frac{6 - 4p}{3(3-p)} \\right).\n$$\nNext, we multiply by $T \\lambda_\\varepsilon$:\n$$\nB^2 = T \\left( \\frac{2c_s}{1-p}\\varepsilon^{1-p} \\right) \\left( \\varepsilon^2 \\frac{2(3-2p)}{3(3-p)} \\right).\n$$\n$$\nB^2 = T \\cdot c_s \\cdot \\frac{2}{1-p} \\cdot \\frac{2(3-2p)}{3(3-p)} \\cdot \\varepsilon^{1-p} \\cdot \\varepsilon^2.\n$$\nCombining terms, we get\n$$\nB^2 = T \\cdot c_s \\cdot \\frac{4(3-2p)}{3(1-p)(3-p)} \\cdot \\varepsilon^{3-p}.\n$$\nThe final closed-form expression for the upper bound is the square root of this quantity:\n$$\nB(c_s, p, \\varepsilon, T) = \\sqrt{ T \\cdot c_s \\cdot \\frac{4(3-2p)}{3(1-p)(3-p)} \\cdot \\varepsilon^{3-p} }.\n$$\nThis formula depends only on the required parameters $c_s$, $p$, $\\varepsilon$, and $T$, and holds for $0<p<1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the Wasserstein-2 distance upper bound for a set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (c_s, p, epsilon, T).\n    test_cases = [\n        (1.0, 0.5, 0.1, 1.0),   # Test 1\n        (1.0, 0.5, 0.05, 1.0),  # Test 2\n        (1.0, 0.9, 0.1, 1.0),   # Test 3\n        (1.0, 0.5, 0.2, 1.0),   # Test 4\n        (1.0, 0.5, 0.1, 0.0),   # Test 5\n        (5.0, 0.5, 0.1, 1.0),   # Test 6\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack the parameters for the current test case.\n        c_s, p, epsilon, T = case\n\n        # The derived formula for the square of the bound is:\n        # B^2 = T * c_s * (4 * (3 - 2*p)) / (3 * (1-p) * (3-p)) * epsilon**(3-p)\n        \n        # Handle the edge case where T=0. The bound is 0.\n        if T == 0.0:\n            bound = 0.0\n        else:\n            # The parameter p is strictly between 0 and 1, so denominators are non-zero.\n            \n            # Calculate the coefficient term involving p\n            p_coefficient = (4 * (3 - 2 * p)) / (3 * (1 - p) * (3 - p))\n            \n            # Calculate the term involving epsilon\n            epsilon_term = epsilon**(3 - p)\n            \n            # Combine all parts to find the value inside the square root\n            inside_sqrt = T * c_s * p_coefficient * epsilon_term\n            \n            # The bound is the square root of the result\n            bound = np.sqrt(inside_sqrt)\n            \n        results.append(bound)\n\n    # Final print statement in the exact required format.\n    # The output is a comma-separated list of float values enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}