{
    "hands_on_practices": [
        {
            "introduction": "The thinning algorithm is an elegant and powerful construction, but how can we be certain that it generates a process with the correct statistical properties? This foundational exercise guides you through a rigorous confirmation of the method's validity . By deriving the distribution of the total event count from first principles, you will prove that the number of accepted points indeed follows a Poisson distribution whose mean is the integrated intensity $\\Lambda(T)$, the defining characteristic of a non-homogeneous Poisson process.",
            "id": "3343295",
            "problem": "Consider a non-homogeneous Poisson process (NHPP) on the interval $[0,T]$ with locally integrable intensity function $t \\mapsto \\lambda(t)$ satisfying $0 \\leq \\lambda(t) \\leq M$ for all $t \\in [0,T]$, where $M$ is a finite constant. Suppose you simulate this NHPP via the classical thinning method: first generate a homogeneous Poisson process with rate $M$ on $[0,T]$, producing proposed arrival times $S_{1}, S_{2}, \\dots, S_{N_{M}(T)}$, where $N_{M}(T)$ denotes the number of proposed points in $[0,T]$. Then, conditionally on the proposed times, accept each point at time $t$ independently with probability $p(t) = \\lambda(t)/M$. Let $N(T)$ denote the number of accepted points in $[0,T]$.\n\nStarting only from the following foundational facts:\n- For a homogeneous Poisson process with rate $M$ on $[0,T]$, the total count $N_{M}(T)$ has the Poisson distribution with parameter $MT$, and conditional on $N_{M}(T) = n$, the unordered proposed times are independent and identically distributed with the uniform distribution on $[0,T]$.\n- Independently accepting each proposed point at time $t$ with probability $p(t)$ multiplies the contribution of that point to the probability generating function (PGF) by $(1 - p(t)) + z\\,p(t)$, where the PGF of a nonnegative integer-valued random variable $X$ is defined as $G_{X}(z) = \\mathbb{E}[z^{X}]$.\n\nDerive the PGF $G_{N}(z) = \\mathbb{E}[z^{N(T)}]$ of the thinned count $N(T)$, and from it compute the expected value $\\mathbb{E}[N(T)]$ and the variance $\\mathrm{Var}(N(T))$. Express your final answer in terms of the cumulative rate\n$$\n\\Lambda(T) = \\int_{0}^{T} \\lambda(s)\\, ds.\n$$\nYour final answer must be a single closed-form analytic expression for both $\\mathbb{E}[N(T)]$ and $\\mathrm{Var}(N(T))$ written as a row matrix. No numerical rounding is required.",
            "solution": "The objective is to derive the probability generating function (PGF) of the count $N(T)$ of accepted points from a thinning process, and subsequently to compute its expectation $\\mathbb{E}[N(T)]$ and variance $\\mathrm{Var}(N(T))$. The derivation will proceed by conditioning, using the law of total expectation, and leveraging the foundational facts provided in the problem statement.\n\nLet $N(T)$ be the number of accepted points in the interval $[0, T]$. Let $N_M(T)$ be the number of proposed points generated by the homogeneous Poisson process (HPP) with rate $M$. The PGF of $N(T)$ is defined as $G_N(z) = \\mathbb{E}[z^{N(T)}]$. We can compute this using the law of total expectation, by first conditioning on the number of proposed points, $N_M(T)$, and their locations, $S_1, S_2, \\dots, S_{N_M(T)}$.\n\nLet's begin by conditioning on the event that there are $n$ proposed points, $N_M(T)=n$, located at specific times $s_1, \\dots, s_n$. For each proposed point at time $s_i$, a thinning operation is performed, which is an independent Bernoulli trial. Let $I_i$ be the indicator random variable that the $i$-th point is accepted. The probability of acceptance is given as $p(s_i) = \\lambda(s_i)/M$. Thus, $I_i \\sim \\mathrm{Bernoulli}(p(s_i))$. The total number of accepted points, conditional on the proposed points, is $N(T) = \\sum_{i=1}^n I_i$.\n\nSince the thinning trials are independent, the PGF of their sum is the product of their individual PGFs. The PGF of a Bernoulli variable $I_i$ with success probability $p(s_i)$ is $G_{I_i}(z) = \\mathbb{E}[z^{I_i}] = (1 - p(s_i)) \\cdot z^0 + p(s_i) \\cdot z^1 = 1 - p(s_i) + z p(s_i)$. This aligns with the structure given in \"Foundational Fact 2\".\n\nThe conditional PGF of $N(T)$ given $N_M(T)=n$ and the locations $S_1=s_1, \\dots, S_n=s_n$ is:\n$$\n\\mathbb{E}[z^{N(T)} | N_M(T)=n, S_1=s_1, \\dots, S_n=s_n] = \\mathbb{E}\\left[z^{\\sum_{i=1}^n I_i}\\right] = \\prod_{i=1}^n \\mathbb{E}[z^{I_i}] = \\prod_{i=1}^n (1 - p(s_i) + z p(s_i))\n$$\n\nNext, we must average this expression over the random locations of the proposed points. According to \"Foundational Fact 1\", conditional on $N_M(T)=n$, the unordered point locations $S_1, \\dots, S_n$ are independent and identically distributed (i.i.d.) random variables from the uniform distribution on $[0, T]$. Let's denote this by $S_i \\sim \\mathrm{U}[0, T]$. Due to the independence of the $S_i$, the expectation of the product is the product of the expectations:\n$$\n\\mathbb{E}[z^{N(T)} | N_M(T)=n] = \\mathbb{E}_{S_1,\\dots,S_n} \\left[ \\prod_{i=1}^n (1 - p(S_i) + z p(S_i)) \\right] = \\prod_{i=1}^n \\mathbb{E}_{S_i} [1 - p(S_i) + z p(S_i)]\n$$\nSince the $S_i$ are identically distributed, each expectation in the product is the same. Let $S$ be a generic random variable with $S \\sim \\mathrm{U}[0, T]$. The expectation is:\n$$\n\\psi(z) = \\mathbb{E}_S [1 - p(S) + z p(S)] = \\int_{0}^{T} (1 - p(s) + z p(s)) f_S(s) \\, ds\n$$\nwhere $f_S(s) = 1/T$ for $s \\in [0, T]$. Substituting $p(s) = \\lambda(s)/M$:\n$$\n\\psi(z) = \\frac{1}{T} \\int_{0}^{T} \\left(1 - \\frac{\\lambda(s)}{M} + z \\frac{\\lambda(s)}{M}\\right) ds = \\frac{1}{T} \\int_{0}^{T} \\left(1 + (z-1)\\frac{\\lambda(s)}{M}\\right) ds\n$$\n$$\n\\psi(z) = \\frac{1}{T} \\left( \\int_{0}^{T} 1 \\, ds + \\frac{z-1}{M} \\int_{0}^{T} \\lambda(s) \\, ds \\right) = \\frac{1}{T} \\left( T + \\frac{z-1}{M} \\Lambda(T) \\right) = 1 + \\frac{\\Lambda(T)}{MT}(z-1)\n$$\nwhere $\\Lambda(T) = \\int_0^T \\lambda(s) \\, ds$.\nThus, the PGF of $N(T)$ conditional on $N_M(T)=n$ is $(\\psi(z))^n$.\n\nFinally, we average over the number of proposed points, $N_M(T)$. From \"Foundational Fact 1\", $N_M(T)$ follows a Poisson distribution with parameter $MT$, i.e., $N_M(T) \\sim \\mathrm{Poisson}(MT)$. The unconditional PGF of $N(T)$ is:\n$$\nG_N(z) = \\mathbb{E}[z^{N(T)}] = \\mathbb{E}_{N_M(T)}[\\mathbb{E}[z^{N(T)} | N_M(T)]] = \\mathbb{E}_{N_M(T)} [(\\psi(z))^{N_M(T)}]\n$$\nThis is the PGF of $N_M(T)$ evaluated at the point $w = \\psi(z)$, i.e., $G_N(z) = G_{N_M(T)}(\\psi(z))$. The PGF of a Poisson distribution with parameter $\\mu$ is $G(w) = \\exp(\\mu(w-1))$. Here, $\\mu=MT$.\n$$\nG_N(z) = \\exp(MT(\\psi(z) - 1))\n$$\nSubstituting the expression for $\\psi(z)$:\n$$\nG_N(z) = \\exp\\left(MT\\left(\\left(1 + \\frac{\\Lambda(T)}{MT}(z-1)\\right) - 1\\right)\\right) = \\exp\\left(MT \\cdot \\frac{\\Lambda(T)}{MT}(z-1)\\right)\n$$\n$$\nG_N(z) = \\exp(\\Lambda(T)(z-1))\n$$\nThis is the PGF of a Poisson distribution with parameter $\\Lambda(T)$. Therefore, the number of accepted points $N(T)$ follows a Poisson distribution with mean and variance equal to $\\Lambda(T)$.\n\nWe now compute the expected value and variance of $N(T)$ from its PGF.\nThe expected value is given by the first derivative of the PGF evaluated at $z=1$:\n$$\n\\mathbb{E}[N(T)] = G'_N(z) \\Big|_{z=1}\n$$\n$$\nG'_N(z) = \\frac{d}{dz} \\exp(\\Lambda(T)(z-1)) = \\Lambda(T) \\exp(\\Lambda(T)(z-1))\n$$\n$$\n\\mathbb{E}[N(T)] = \\Lambda(T) \\exp(\\Lambda(T)(1-1)) = \\Lambda(T) \\exp(0) = \\Lambda(T)\n$$\nTo find the variance, we first compute the second factorial moment, $\\mathbb{E}[N(T)(N(T)-1)]$, which is given by the second derivative of the PGF at $z=1$:\n$$\n\\mathbb{E}[N(T)(N(T)-1)] = G''_N(z) \\Big|_{z=1}\n$$\n$$\nG''_N(z) = \\frac{d}{dz} \\left( \\Lambda(T) \\exp(\\Lambda(T)(z-1)) \\right) = (\\Lambda(T))^2 \\exp(\\Lambda(T)(z-1))\n$$\n$$\n\\mathbb{E}[N(T)(N(T)-1)] = (\\Lambda(T))^2 \\exp(\\Lambda(T)(1-1)) = (\\Lambda(T))^2\n$$\nThe variance is given by $\\mathrm{Var}(N(T)) = \\mathbb{E}[N(T)(N(T)-1)] + \\mathbb{E}[N(T)] - (\\mathbb{E}[N(T)])^2$:\n$$\n\\mathrm{Var}(N(T)) = (\\Lambda(T))^2 + \\Lambda(T) - (\\Lambda(T))^2 = \\Lambda(T)\n$$\nBoth the expected value and the variance of the thinned count $N(T)$ are equal to the cumulative rate $\\Lambda(T)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\Lambda(T)  \\Lambda(T)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In many applications, we need to simulate a process not for a fixed calendar duration, but until a certain amount of 'activity'—measured by the integrated intensity $\\Lambda(t)$—has occurred. This practice challenges you to think critically about the relationship between calendar time and this 'operational time' to design a correct stopping rule . Devising an implementable algorithm that guarantees all events are captured up to a random time defined by a threshold on $\\Lambda(t)$ is a subtle but essential skill for advanced simulation design.",
            "id": "3343293",
            "problem": "Consider a nonhomogeneous Poisson process (NHPP) with locally integrable intensity function $\\lambda(t)$ on $[0,\\infty)$, and let the integrated intensity be $\\Lambda(t) = \\int_{0}^{t} \\lambda(s)\\,ds$. Assume that $\\lambda(t)$ is bounded by a known constant $M > 0$, so that $\\lambda(t) \\le M$ for all $t \\ge 0$. You use thinning with a dominating homogeneous Poisson process of rate $M$: propose event times $T_1  T_2  \\cdots$ from the homogeneous process of rate $M$, and at each proposal time $T_i$, accept it as a target-event with probability $\\lambda(T_i)/M$, independently across $i$.\n\nFix a deterministic threshold $L  0$, and define the random calendar time $\\tau_L := \\inf\\{t \\ge 0 : \\Lambda(t) \\ge L\\}$. You want an implementable stopping rule, stated in terms of the simulated proposal times and readily computable quantities, that guarantees that upon stopping you have generated all and only the target-events occurring no later than the random calendar time $\\tau_L$.\n\nWhich option correctly specifies such a stopping rule and provides a valid justification?\n\nA. Stop at the first proposal index $i_\\star$ such that $\\Lambda(T_{i_\\star})  L$, and output exactly the accepted proposals among $\\{T_1,\\dots,T_{i_\\star-1}\\}$. Justification: Since $\\Lambda$ is nondecreasing and continuous, $\\Lambda(T_{i_\\star-1}) \\le L  \\Lambda(T_{i_\\star})$ implies $T_{i_\\star-1} \\le \\tau_L  T_{i_\\star}$; by thinning, all target-events up to $\\tau_L$ must be accepted proposals among the proposals strictly before $T_{i_\\star}$.\n\nB. Stop when the running count of proposals first reaches $\\lfloor L \\rfloor$, and output all accepted proposals up to that point. Justification: The proposals approximate the unit-rate Poisson process in integrated intensity time, so $\\lfloor L \\rfloor$ proposals suffice to cross the threshold $L$.\n\nC. At the start, sample $K \\sim \\mathrm{Poisson}(L)$; then run thinning and stop when the number of accepted proposals first equals $K$. Output those $K$ accepted times. Justification: The total number of target-events up to $\\tau_L$ is $\\mathrm{Poisson}(L)$, so stopping when the accepted count reaches $K$ reproduces the correct number and thus the correct set of events up to $\\tau_L$.\n\nD. Continue thinning until the integrated intensity at the last accepted time first exceeds $L$, i.e., until the first accepted index $j_\\star$ such that $\\Lambda(T_{j_\\star})  L$, and then output all accepted proposals up to and including $T_{j_\\star}$. Justification: The accepted time $T_{j_\\star}$ is the last target-event not later than $\\tau_L$ because its integrated intensity crosses $L$.\n\nOnly one option is correct. Choose it and be prepared to defend the validity of both the stopping rule and the justification based on first principles of the NHPP and the thinning construction, without assuming any special analytic form for $\\Lambda$ or its inverse.",
            "solution": "The problem requires finding a stopping rule for a thinning simulation that correctly generates all NHPP events up to the random time $\\tau_L$, where $\\tau_L$ is defined as the first time $t$ at which the integrated intensity $\\Lambda(t)$ reaches a threshold $L$. By continuity of $\\Lambda(t)$, we have $\\Lambda(\\tau_L) = L$.\n\nLet's analyze the options:\n\n**Option A:** This rule proposes stopping the generation of proposals at the first proposal time $T_{i_\\star}$ for which $\\Lambda(T_{i_\\star}) > L$.\n-   **Coverage (\"all\"):** Because $\\Lambda(t)$ is non-decreasing and $\\Lambda(T_{i_\\star}) > L = \\Lambda(\\tau_L)$, it must be that $T_{i_\\star} \\ge \\tau_L$. In fact, $T_{i_\\star} > \\tau_L$ almost surely since the proposal times are from a continuous-time process. This means that the entire target interval $[0, \\tau_L]$ is covered by the proposal-generating process up to time $T_{i_\\star}$. Any true NHPP event occurring in $[0, \\tau_L]$ must be an accepted proposal from the set $\\{T_1, T_2, \\dots, T_{i_\\star-1}\\}$ (or, with probability zero, $T_{i_\\star}$ itself). By checking all proposals up to $T_{i_\\star-1}$, we ensure no events in the target interval are missed.\n-   **Correctness (\"only\"):** The rule outputs accepted proposals from the set $\\{T_1, \\dots, T_{i_\\star-1}\\}$. For any such proposal time $T_j$ where $j  i_\\star$, the stopping condition implies that $\\Lambda(T_j) \\le L$. Since $\\Lambda(t)$ is non-decreasing and $\\Lambda(\\tau_L) = L$, it follows that $T_j \\le \\tau_L$. Thus, any accepted event from this set is guaranteed to fall within the correct time interval $[0, \\tau_L]$.\n-   **Conclusion:** This rule is both implementable (it depends only on the proposal times and the function $\\Lambda$) and correct.\n\n**Option B:** This rule is flawed because there is no direct relationship between the number of proposals (which depends on the dominating rate $M$) and the integrated intensity $L$ of the target process. For a large $M$, many more than $\\lfloor L \\rfloor$ proposals might occur before $\\tau_L$ is reached.\n\n**Option C:** This method generates a correct realization from the *distribution* of an NHPP over an operational time of $L$, but it does not correctly simulate the process up to the random time $\\tau_L$ for a *single path*. The number of events in $[0, \\tau_L]$ for a given path is a random but fixed outcome of that path; pre-sampling a number $K$ from a Poisson distribution does not guarantee matching this outcome. If the pre-sampled $K$ is less than the actual number of events in $[0, \\tau_L]$, the simulation stops too early.\n\n**Option D:** This rule is incorrect because it makes the stopping decision based on the times of *accepted* events. It is possible for a long sequence of proposals to be rejected. During this interval, the time $t$ could cross $\\tau_L$ without any accepted events occurring. The algorithm would fail to check for events in this gap and would miss them. The stopping rule must depend on the underlying proposal process to ensure the entire time axis is covered.\n\nTherefore, Option A provides the only correct and reliable stopping rule. It guarantees that the simulation continues long enough to cover the target interval $[0, \\tau_L]$ and correctly identifies the set of proposals that can contribute events to this interval.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Thinning is a widely applicable method, but its efficiency is not always optimal, especially when a tight dominating function is hard to find. This final exercise puts thinning into a practical context by comparing its computational cost against the inversion method, another cornerstone of NHPP simulation . By analyzing the expected number of proposals needed to generate a fixed number of events, you will quantify how the choice of the dominating rate directly impacts performance and gain insight into when one algorithm might be preferred over another.",
            "id": "3343303",
            "problem": "Consider a Non-Homogeneous Poisson Process (NHPP) with intensity function $\\lambda(t)=\\alpha t$ for $t\\geq 0$, where $\\alpha0$. You wish to simulate event times until a fixed number of events $n$ have been accepted.\n\nOne approach uses thinning with a dominating NHPP whose intensity function $m(t)=\\beta t$, where $\\beta\\geq \\alpha$ is a known constant. In thinning, a proposed event at time $t$ from the dominating process is accepted with probability $\\lambda(t)/m(t)$. Another approach is inversion that directly targets the $n$-th arrival time $T_n$ of the NHPP, based on time change and cumulative rate.\n\nStarting from the definitions of a NHPP, the thinning construction, and the time-change characterization via the cumulative rate function, derive an explicit analytical expression for the expected total number of proposed events required by the thinning method to obtain $n$ accepted events. Then, define an efficiency ratio $R(\\alpha,\\beta,n)$ as the expected number of proposals under thinning divided by the number of exponential random draws needed to perform inversion that directly targets $T_n$ by sampling the transformed interarrival times and mapping to clock time. Assume the inversion method uses a sum of $n$ independent exponential random variables of rate $1$ to construct the transformed $n$-th arrival and one application of the inverse cumulative rate mapping, and ignore the cost of evaluating $\\lambda(t)$ or $m(t)$ and of the inverse mapping itself in the ratio definition.\n\nProvide the final answer as a single closed-form analytical expression for $R(\\alpha,\\beta,n)$ in terms of $\\alpha$, $\\beta$, and $n$. No numerical approximation is required, and no units are involved. The final answer must be a single expression.",
            "solution": "This problem requires us to compare the computational cost, measured in specific units, of two methods for simulating an NHPP: thinning and inversion.\n\n**1. Cost of the Thinning Method**\n\nThe thinning method uses a dominating process with intensity $m(t) = \\beta t$ to generate proposals for a target process with intensity $\\lambda(t) = \\alpha t$. The condition $\\beta \\ge \\alpha$ ensures that $m(t) \\ge \\lambda(t)$ for all $t \\ge 0$.\n\nFor each proposal generated at time $t$, it is accepted with a probability $p(t)$ given by:\n$$\np(t) = \\frac{\\lambda(t)}{m(t)} = \\frac{\\alpha t}{\\beta t} = \\frac{\\alpha}{\\beta}\n$$\nThis acceptance probability, $p = \\alpha/\\beta$, is a constant, independent of time. The simulation of accepted events can therefore be viewed as a sequence of independent Bernoulli trials, where each trial corresponds to a proposal, and \"success\" corresponds to accepting the proposal.\n\nWe need to find the expected number of proposals required to obtain exactly $n$ accepted events. This is a classic scenario described by the negative binomial distribution. If $K$ is the number of trials (proposals) required to get $n$ successes, its expected value is:\n$$\n\\mathbb{E}[K] = \\frac{n}{p}\n$$\nSubstituting our success probability $p = \\alpha/\\beta$:\n$$\n\\mathbb{E}[K] = \\frac{n}{\\alpha/\\beta} = \\frac{n\\beta}{\\alpha}\n$$\nThis is the expected number of proposals required by the thinning method.\n\n**2. Cost of the Inversion Method**\n\nThe problem defines the cost of the inversion method as \"the number of exponential random draws needed\". The inversion method works by transforming the NHPP into a standard homogeneous Poisson process (HPP) with rate 1. The inter-arrival times of this standard HPP are independent and identically distributed exponential random variables with rate 1 (i.e., $\\mathrm{Exp}(1)$).\n\nTo find the $n$-th arrival time of the target NHPP, one first finds the $n$-th arrival time of the standard HPP, $\\tau_n$, by summing $n$ independent $\\mathrm{Exp}(1)$ draws:\n$$\n\\tau_n = E_1 + E_2 + \\dots + E_n, \\quad \\text{where } E_i \\sim \\mathrm{Exp}(1) \\text{ i.i.d.}\n$$\nThis step requires generating $n$ random numbers from an exponential distribution. The problem states to ignore other costs. Therefore, the cost of the inversion method is exactly $n$.\n\n**3. The Efficiency Ratio $R(\\alpha, \\beta, n)$**\n\nThe efficiency ratio is defined as the ratio of the two costs:\n$$\nR(\\alpha, \\beta, n) = \\frac{\\text{Expected proposals for thinning}}{\\text{Draws for inversion}}\n$$\nSubstituting the derived costs:\n$$\nR(\\alpha, \\beta, n) = \\frac{\\frac{n\\beta}{\\alpha}}{n}\n$$\nThe factor $n$ cancels out, leaving:\n$$\nR(\\alpha, \\beta, n) = \\frac{\\beta}{\\alpha}\n$$\nThis result shows that the relative efficiency of thinning versus inversion (under these cost definitions) depends only on the ratio of the dominating rate parameter to the target rate parameter. A \"tighter\" envelope (i.e., $\\beta$ closer to $\\alpha$) results in a more efficient thinning simulation, with the ratio approaching 1.",
            "answer": "$$\\boxed{\\frac{\\beta}{\\alpha}}$$"
        }
    ]
}