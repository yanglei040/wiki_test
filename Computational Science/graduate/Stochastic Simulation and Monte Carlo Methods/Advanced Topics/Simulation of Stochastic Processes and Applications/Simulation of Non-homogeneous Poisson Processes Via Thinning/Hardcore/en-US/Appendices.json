{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is fundamental, asking you to prove from first principles that the thinning algorithm is theoretically sound. By deriving the probability generating function of the thinned process, you will rigorously show that the number of generated events in an interval $[0, T]$ correctly follows a Poisson distribution with mean equal to the integrated intensity $\\Lambda(T) = \\int_{0}^{T} \\lambda(t) dt$. This practice solidifies the theoretical connection between the simulation method and the mathematical definition of a non-homogeneous Poisson process .",
            "id": "3343295",
            "problem": "Consider a non-homogeneous Poisson process (NHPP) on the interval $[0,T]$ with locally integrable intensity function $t \\mapsto \\lambda(t)$ satisfying $0 \\leq \\lambda(t) \\leq M$ for all $t \\in [0,T]$, where $M$ is a finite constant. Suppose you simulate this NHPP via the classical thinning method: first generate a homogeneous Poisson process with rate $M$ on $[0,T]$, producing proposed arrival times $S_{1}, S_{2}, \\dots, S_{N_{M}(T)}$, where $N_{M}(T)$ denotes the number of proposed points in $[0,T]$. Then, conditionally on the proposed times, accept each point at time $t$ independently with probability $p(t) = \\lambda(t)/M$. Let $N(T)$ denote the number of accepted points in $[0,T]$.\n\nStarting only from the following foundational facts:\n- For a homogeneous Poisson process with rate $M$ on $[0,T]$, the total count $N_{M}(T)$ has the Poisson distribution with parameter $MT$, and conditional on $N_{M}(T) = n$, the unordered proposed times are independent and identically distributed with the uniform distribution on $[0,T]$.\n- Independently accepting each proposed point at time $t$ with probability $p(t)$ multiplies the contribution of that point to the probability generating function (PGF) by $(1 - p(t)) + z\\,p(t)$, where the PGF of a nonnegative integer-valued random variable $X$ is defined as $G_{X}(z) = \\mathbb{E}[z^{X}]$.\n\nDerive the PGF $G_{N}(z) = \\mathbb{E}[z^{N(T)}]$ of the thinned count $N(T)$, and from it compute the expected value $\\mathbb{E}[N(T)]$ and the variance $\\mathrm{Var}(N(T))$. Express your final answer in terms of the cumulative rate\n$$\n\\Lambda(T) = \\int_{0}^{T} \\lambda(s)\\, ds.\n$$\nYour final answer must be a single closed-form analytic expression for both $\\mathbb{E}[N(T)]$ and $\\mathrm{Var}(N(T))$ written as a row matrix. No numerical rounding is required.",
            "solution": "The objective is to derive the probability generating function (PGF) of the count $N(T)$ of accepted points from a thinning process, and subsequently to compute its expectation $\\mathbb{E}[N(T)]$ and variance $\\mathrm{Var}(N(T))$. The derivation will proceed by conditioning, using the law of total expectation, and leveraging the foundational facts provided in the problem statement.\n\nLet $N(T)$ be the number of accepted points in the interval $[0, T]$. Let $N_M(T)$ be the number of proposed points generated by the homogeneous Poisson process (HPP) with rate $M$. The PGF of $N(T)$ is defined as $G_N(z) = \\mathbb{E}[z^{N(T)}]$. We can compute this using the law of total expectation, by first conditioning on the number of proposed points, $N_M(T)$, and their locations, $S_1, S_2, \\dots, S_{N_M(T)}$.\n\nLet's begin by conditioning on the event that there are $n$ proposed points, $N_M(T)=n$, located at specific times $s_1, \\dots, s_n$. For each proposed point at time $s_i$, a thinning operation is performed, which is an independent Bernoulli trial. Let $I_i$ be the indicator random variable that the $i$-th point is accepted. The probability of acceptance is given as $p(s_i) = \\lambda(s_i)/M$. Thus, $I_i \\sim \\text{Bernoulli}(p(s_i))$. The total number of accepted points, conditional on the proposed points, is $N(T) = \\sum_{i=1}^n I_i$.\n\nSince the thinning trials are independent, the PGF of their sum is the product of their individual PGFs. The PGF of a Bernoulli variable $I_i$ with success probability $p(s_i)$ is $G_{I_i}(z) = \\mathbb{E}[z^{I_i}] = (1 - p(s_i)) \\cdot z^0 + p(s_i) \\cdot z^1 = 1 - p(s_i) + z p(s_i)$. This aligns with the structure given in \"Foundational Fact 2\".\n\nThe conditional PGF of $N(T)$ given $N_M(T)=n$ and the locations $S_1=s_1, \\dots, S_n=s_n$ is:\n$$\n\\mathbb{E}[z^{N(T)} | N_M(T)=n, S_1=s_1, \\dots, S_n=s_n] = \\mathbb{E}\\left[z^{\\sum_{i=1}^n I_i}\\right] = \\prod_{i=1}^n \\mathbb{E}[z^{I_i}] = \\prod_{i=1}^n (1 - p(s_i) + z p(s_i))\n$$\n\nNext, we must average this expression over the random locations of the proposed points. According to \"Foundational Fact 1\", conditional on $N_M(T)=n$, the unordered point locations $S_1, \\dots, S_n$ are independent and identically distributed (i.i.d.) random variables from the uniform distribution on $[0, T]$. Let's denote this by $S_i \\sim U[0, T]$. Due to the independence of the $S_i$, the expectation of the product is the product of the expectations:\n$$\n\\mathbb{E}[z^{N(T)} | N_M(T)=n] = \\mathbb{E}_{S_1,\\dots,S_n} \\left[ \\prod_{i=1}^n (1 - p(S_i) + z p(S_i)) \\right] = \\prod_{i=1}^n \\mathbb{E}_{S_i} [1 - p(S_i) + z p(S_i)]\n$$\nSince the $S_i$ are identically distributed, each expectation in the product is the same. Let $S$ be a generic random variable with $S \\sim U[0, T]$. The expectation is:\n$$\n\\psi(z) = \\mathbb{E}_S [1 - p(S) + z p(S)] = \\int_{0}^{T} (1 - p(s) + z p(s)) f_S(s) \\, ds\n$$\nwhere $f_S(s) = 1/T$ for $s \\in [0, T]$. Substituting $p(s) = \\lambda(s)/M$:\n$$\n\\psi(z) = \\frac{1}{T} \\int_{0}^{T} \\left(1 - \\frac{\\lambda(s)}{M} + z \\frac{\\lambda(s)}{M}\\right) ds = \\frac{1}{T} \\int_{0}^{T} \\left(1 + (z-1)\\frac{\\lambda(s)}{M}\\right) ds\n$$\n$$\n\\psi(z) = \\frac{1}{T} \\left( \\int_{0}^{T} 1 \\, ds + \\frac{z-1}{M} \\int_{0}^{T} \\lambda(s) \\, ds \\right) = \\frac{1}{T} \\left( T + \\frac{z-1}{M} \\Lambda(T) \\right) = 1 + \\frac{\\Lambda(T)}{MT}(z-1)\n$$\nwhere $\\Lambda(T) = \\int_0^T \\lambda(s) \\, ds$.\nThus, the PGF of $N(T)$ conditional on $N_M(T)=n$ is $(\\psi(z))^n$.\n\nFinally, we average over the number of proposed points, $N_M(T)$. From \"Foundational Fact 1\", $N_M(T)$ follows a Poisson distribution with parameter $MT$, i.e., $N_M(T) \\sim \\text{Poisson}(MT)$. The unconditional PGF of $N(T)$ is:\n$$\nG_N(z) = \\mathbb{E}[z^{N(T)}] = \\mathbb{E}_{N_M(T)}[\\mathbb{E}[z^{N(T)} | N_M(T)]] = \\mathbb{E}_{N_M(T)} [(\\psi(z))^{N_M(T)}]\n$$\nThis is the PGF of $N_M(T)$ evaluated at the point $w = \\psi(z)$, i.e., $G_N(z) = G_{N_M(T)}(\\psi(z))$. The PGF of a Poisson distribution with parameter $\\mu$ is $G(w) = \\exp(\\mu(w-1))$. Here, $\\mu=MT$.\n$$\nG_N(z) = \\exp(MT(\\psi(z) - 1))\n$$\nSubstituting the expression for $\\psi(z)$:\n$$\nG_N(z) = \\exp\\left(MT\\left(\\left(1 + \\frac{\\Lambda(T)}{MT}(z-1)\\right) - 1\\right)\\right) = \\exp\\left(MT \\cdot \\frac{\\Lambda(T)}{MT}(z-1)\\right)\n$$\n$$\nG_N(z) = \\exp(\\Lambda(T)(z-1))\n$$\nThis is the PGF of a Poisson distribution with parameter $\\Lambda(T)$. Therefore, the number of accepted points $N(T)$ follows a Poisson distribution with mean and variance equal to $\\Lambda(T)$.\n\nWe now compute the expected value and variance of $N(T)$ from its PGF.\nThe expected value is given by the first derivative of the PGF evaluated at $z=1$:\n$$\n\\mathbb{E}[N(T)] = G'_N(z) \\Big|_{z=1}\n$$\n$$\nG'_N(z) = \\frac{d}{dz} \\exp(\\Lambda(T)(z-1)) = \\Lambda(T) \\exp(\\Lambda(T)(z-1))\n$$\n$$\n\\mathbb{E}[N(T)] = \\Lambda(T) \\exp(\\Lambda(T)(1-1)) = \\Lambda(T) \\exp(0) = \\Lambda(T)\n$$\nTo find the variance, we first compute the second factorial moment, $\\mathbb{E}[N(T)(N(T)-1)]$, which is given by the second derivative of the PGF at $z=1$:\n$$\n\\mathbb{E}[N(T)(N(T)-1)] = G''_N(z) \\Big|_{z=1}\n$$\n$$\nG''_N(z) = \\frac{d}{dz} \\left( \\Lambda(T) \\exp(\\Lambda(T)(z-1)) \\right) = (\\Lambda(T))^2 \\exp(\\Lambda(T)(z-1))\n$$\n$$\n\\mathbb{E}[N(T)(N(T)-1)] = (\\Lambda(T))^2 \\exp(\\Lambda(T)(1-1)) = (\\Lambda(T))^2\n$$\nThe variance is given by $\\mathrm{Var}(N(T)) = \\mathbb{E}[N(T)(N(T)-1)] + \\mathbb{E}[N(T)] - (\\mathbb{E}[N(T)])^2$:\n$$\n\\mathrm{Var}(N(T)) = (\\Lambda(T))^2 + \\Lambda(T) - (\\Lambda(T))^2 = \\Lambda(T)\n$$\nBoth the expected value and the variance of the thinned count $N(T)$ are equal to the cumulative rate $\\Lambda(T)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\Lambda(T)  \\Lambda(T)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A correct algorithm is not always an efficient one, and this practice explores the crucial concept of computational performance. You will compare the thinning method to the alternative inversion (or time-change) method by deriving a simple, elegant expression for their relative efficiency. This exercise highlights how the choice of the dominating intensity function is not arbitrary but is a key factor that directly governs the simulation's cost .",
            "id": "3343303",
            "problem": "Consider a Non-Homogeneous Poisson Process (NHPP) with intensity function $\\lambda(t)=\\alpha t$ for $t\\geq 0$, where $\\alpha>0$. You wish to simulate event times until a fixed number of events $n$ have been accepted.\n\nOne approach uses thinning with a dominating NHPP whose intensity function $m(t)=\\beta t$, where $\\beta\\geq \\alpha$ is a known constant. In thinning, a proposed event at time $t$ from the dominating process is accepted with probability $\\lambda(t)/m(t)$. Another approach is inversion that directly targets the $n$-th arrival time $T_n$ of the NHPP, based on time change and cumulative rate.\n\nStarting from the definitions of a NHPP, the thinning construction, and the time-change characterization via the cumulative rate function, derive an explicit analytical expression for the expected total number of proposed events required by the thinning method to obtain $n$ accepted events. Then, define an efficiency ratio $R(\\alpha,\\beta,n)$ as the expected number of proposals under thinning divided by the number of exponential random draws needed to perform inversion that directly targets $T_n$ by sampling the transformed interarrival times and mapping to clock time. Assume the inversion method uses a sum of $n$ independent exponential random variables of rate $1$ to construct the transformed $n$-th arrival and one application of the inverse cumulative rate mapping, and ignore the cost of evaluating $\\lambda(t)$ or $m(t)$ and of the inverse mapping itself in the ratio definition.\n\nProvide the final answer as a single closed-form analytical expression for $R(\\alpha,\\beta,n)$ in terms of $\\alpha$, $\\beta$, and $n$. No numerical approximation is required, and no units are involved. The final answer must be a single expression.",
            "solution": "The user wants to solve the problem by following these steps.\n1.  **Validate the problem statement**: The user wants to check if the problem is scientifically grounded, well-posed, objective, and complete. All conditions appear to be met. The problem is a standard exercise in stochastic simulation, specifically comparing the efficiency of the thinning method and the inversion method for Non-Homogeneous Poisson Processes (NHPPs). The given intensity functions $\\lambda(t) = \\alpha t$ and $m(t) = \\beta t$ with $\\alpha, \\beta > 0$ and $\\beta \\geq \\alpha$ are valid. The definitions of the two simulation methods and the efficiency ratio are precise and unambiguous. The problem is deemed valid.\n\n2.  **Derive the expected number of proposals for the thinning method**:\n    The target NHPP has an intensity function $\\lambda(t) = \\alpha t$ for $t \\geq 0$, with $\\alpha > 0$. The thinning method uses a dominating NHPP with intensity function $m(t) = \\beta t$, where $\\beta \\geq \\alpha$. The condition $\\beta \\geq \\alpha$ ensures that $m(t) \\geq \\lambda(t)$ for all $t \\geq 0$, which is a requirement for the thinning algorithm.\n\n    In the thinning method, events are first proposed according to the dominating process with rate $m(t)$. A proposed event at time $t$ is then \"thinned\" or accepted with a probability $p(t)$ given by:\n    $$p(t) = \\frac{\\lambda(t)}{m(t)}$$\n    For the given intensity functions, this probability is:\n    $$p(t) = \\frac{\\alpha t}{\\beta t} = \\frac{\\alpha}{\\beta}$$\n    Crucially, the acceptance probability $p(t)$ is a constant, independent of the time $t$. Let us denote this constant probability by $p = \\frac{\\alpha}{\\beta}$.\n\n    The simulation proceeds by generating proposals. Each proposal constitutes an independent Bernoulli trial with a probability of success (acceptance) equal to $p$. We need to continue this process until exactly $n$ events have been accepted.\n\n    Let $K$ be the random variable representing the total number of proposals required to obtain $n$ accepted events. This is a classic problem structure that corresponds to a negative binomial distribution. Specifically, $K$ is the number of trials needed to achieve the $n$-th success in a sequence of independent Bernoulli trials with success probability $p$.\n\n    The expected value of a random variable following a negative binomial distribution with parameters $n$ (number of successes) and $p$ (probability of success) is given by:\n    $$E[K] = \\frac{n}{p}$$\n    Substituting the value of $p = \\frac{\\alpha}{\\beta}$, we find the expected total number of proposed events:\n    $$E[K] = \\frac{n}{\\frac{\\alpha}{\\beta}} = \\frac{n\\beta}{\\alpha}$$\n\n3.  **Determine the cost of the inversion method**:\n    The inversion method, also known as the time-change method, relies on the cumulative intensity function $\\Lambda(t) = \\int_0^t \\lambda(s) ds$. The time-change theorem states that if $\\{T_i\\}$ are the event times of an NHPP with rate $\\lambda(t)$, then the transformed times $\\{\\Lambda(T_i)\\}$ are the event times of a standard homogeneous Poisson process with rate $1$.\n\n    The inter-arrival times of a standard homogeneous Poisson process are independent and identically distributed exponential random variables with rate $1$, i.e., $\\text{Exp}(1)$. To find the $n$-th event time $T_n$ of the NHPP, one can first find the $n$-th event time of the corresponding standard process, which we denote $\\tau_n = \\Lambda(T_n)$. This is done by summing $n$ independent $\\text{Exp}(1)$ random variables:\n    $$\\tau_n = E_1 + E_2 + \\dots + E_n, \\quad \\text{where } E_i \\sim \\text{Exp}(1) \\text{ i.i.d.}$$\n    Once $\\tau_n$ is obtained, the actual event time $T_n$ is found by inverting the cumulative intensity function: $T_n = \\Lambda^{-1}(\\tau_n)$.\n\n    The problem explicitly defines the measure of effort for this method as \"the number of exponential random draws needed to perform inversion that directly targets $T_n$\". Based on the procedure described, generating $\\tau_n$ requires drawing $n$ random numbers from the $\\text{Exp}(1)$ distribution. Therefore, the cost of curbing the inversion method, as defined in the problem, is exactly $n$.\n\n4.  **Calculate the efficiency ratio $R(\\alpha, \\beta, n)$**:\n    The efficiency ratio $R(\\alpha, \\beta, n)$ is defined as the expected number of proposals under thinning divided by the number of exponential random draws needed for inversion.\n\n    Using the results from the previous steps:\n    -   Numerator (Expected proposals for thinning): $\\frac{n\\beta}{\\alpha}$\n    -   Denominator (Random draws for inversion): $n$\n\n    The ratio is therefore:\n    $$R(\\alpha, \\beta, n) = \\frac{\\left( \\frac{n\\beta}{\\alpha} \\right)}{n}$$\n    Simplifying the expression, we get:\n    $$R(\\alpha, \\beta, n) = \\frac{\\beta}{\\alpha}$$\n    This result is independent of $n$. It shows that the relative efficiency of thinning compared to inversion (under the problem's specific cost definitions) depends only on how \"tight\" the dominating rate function is to the target rate function, as captured by the ratio $\\beta/\\alpha$. Since $\\beta \\geq \\alpha$, the ratio $R \\geq 1$, which confirms that the thinning method requires at least as many \"steps\" (proposals) on average as the inversion method (draws).",
            "answer": "$$\\boxed{\\frac{\\beta}{\\alpha}}$$"
        },
        {
            "introduction": "Advanced simulation practice involves not just running algorithms, but also diagnosing their performance and validating the setup. This problem guides you in developing a statistical estimator for the peak intensity, $\\sup_{t} \\lambda(t)$, using the simulation's own output, including both accepted and rejected proposals. This task provides insight into the statistical properties of the simulation process itself and demonstrates that even \"wasted\" proposals contain valuable information for analysis .",
            "id": "3343281",
            "problem": "You are given a non-homogeneous Poisson process (NHPP) with time-varying intensity function $\\lambda(t)$ on a finite horizon $[0,T]$. The standard thinning method simulates this NHPP using a dominating piecewise-constant envelope that is constant on each subinterval $I$ of a partition of $[0,T]$. For a subinterval $I \\subset [0,T]$ with length $|I|$, choose a constant envelope $M_I$ such that $M_I \\ge \\sup_{t \\in I} \\lambda(t)$. Proposal events are generated as a homogeneous Poisson process on $I$ with rate $M_I$, and each proposal at time $U \\in I$ is accepted independently with probability $\\lambda(U)/M_I$. The accepted events form the target NHPP on $I$.\n\nDefine a diagnostic estimator that uses both accepted and rejected proposals to estimate the essential supremum of the intensity over $I$. Partition each subinterval $I$ into $B$ equal-width microbins of width $w = |I|/B$. For a microbin $b$ with proposal count $N_b$ and accepted count $A_b$, define the empirical acceptance rate\n$$\nR_b \\equiv \n\\begin{cases}\nA_b/N_b,  \\text{if } N_b  0,\\\\\n0,  \\text{if } N_b = 0.\n\\end{cases}\n$$\nThen define the subinterval supremum estimator\n$$\n\\widehat{s}_I \\equiv M_I \\cdot \\max_{b \\in \\{1,\\dots,B\\}} R_b.\n$$\n\nStarting from first principles and standard facts about Poisson processes and independent thinning, do the following.\n\n1) Derive the unconditional expectation and variance of the single-bin ratio $R_b$ under the following idealization: within a microbin, the acceptance probability is constant $p_b \\equiv \\lambda(t_b)/M_I$ at some representative point $t_b \\in b$, and the expected proposals in the microbin are $\\mu_b \\equiv M_I w$. Use only the definitions of the Poisson distribution and the Binomial distribution and their basic properties; do not assume any specialized formulas for ratios of random variables. Your derivation must express $\\mathbb{E}[R_b]$ and $\\mathrm{Var}(R_b)$ explicitly in terms of $p_b$ and $\\mu_b$.\n\n2) Explain qualitatively, using your derived expressions, why $\\widehat{s}_I$ exhibits two competing effects: a downward bias from microbins with $N_b=0$, and an upward selection bias from taking a maximum over noisy estimates $R_b$ across microbins.\n\n3) Design a Monte Carlo procedure, consistent with the thinning construction, to quantify the bias $\\mathbb{E}[\\widehat{s}_I] - s_I$ and variance $\\mathrm{Var}(\\widehat{s}_I)$, where $s_I \\equiv \\sup_{t \\in I}\\lambda(t)$, for a specified $\\lambda(t)$, partition, envelope, and microbin count $B$. Your simulation must not sample individual proposal times; instead, use only independent Poisson counts of accepted and rejected proposals per microbin implied by thinning, namely\n$$\nA_b \\sim \\mathrm{Poisson}\\!\\left(\\int_b \\lambda(t)\\,dt\\right), \\qquad\nR_b^{\\mathrm{rej}} \\sim \\mathrm{Poisson}\\!\\left(\\int_b (M_I - \\lambda(t))\\,dt\\right),\n$$\nindependently across microbins and between $A_b$ and $R_b^{\\mathrm{rej}}$ within each microbin, and set $N_b = A_b + R_b^{\\mathrm{rej}}$. Compute $R_b$ and then $\\widehat{s}_I$ from these counts.\n\n4) Implement your procedure as a complete program that runs the following test suite. For each test case, compute across all subintervals $I$ in the partition: the absolute bias $\\left|\\mathbb{E}[\\widehat{s}_I] - s_I\\right|$ (approximated by the sample mean minus the true supremum) and the variance $\\mathrm{Var}(\\widehat{s}_I)$ (approximated by the sample variance). Summarize each test case by three floats: the mean absolute bias over subintervals, the maximum absolute bias over subintervals, and the mean variance over subintervals. Use the specified random seed for reproducibility and integrate $\\int_b \\lambda(t)\\,dt$ numerically to high accuracy. The true supremum $s_I$ should be computed by a dense grid search over each subinterval.\n\nTest Suite:\n\n- Test Case 1 (smooth, moderate intensity):\n  - Horizon $[0,4]$, partitioned into subintervals $[0,1], [1,2], [2,3], [3,4]$.\n  - Intensity $\\lambda_1(t) = 6 + 4\\sin(\\pi t) + 3\\exp\\!\\big(-(t-2)^2\\big)$.\n  - Microbins per subinterval $B = 10$.\n  - Envelope on $I$: $M_I = \\sup_{t \\in I} \\lambda_1(t) + \\delta$ with $\\delta = 1.0$.\n  - Replicates: $R = 30000$.\n  - Random seed: $12345$.\n\n- Test Case 2 (low counts, many empty microbins):\n  - Horizon $[0,2]$, partitioned into subintervals $[0,1], [1,2]$.\n  - Intensity $\\lambda_2(t) = 0.6 + 0.3\\sin(2\\pi t) + 0.1\\cos(4\\pi t)$.\n  - Microbins per subinterval $B = 20$.\n  - Envelope on $I$: $M_I = \\sup_{t \\in I} \\lambda_2(t) + \\delta$ with $\\delta = 0.2$.\n  - Replicates: $R = 60000$.\n  - Random seed: $24680$.\n\n- Test Case 3 (single-bin boundary case):\n  - Horizon $[0,1]$, single subinterval $[0,1]$.\n  - Intensity $\\lambda_3(t) = 2 + 2t$.\n  - Microbins per subinterval $B = 1$.\n  - Envelope on $I$: $M_I = \\sup_{t \\in I} \\lambda_3(t) + \\delta$ with $\\delta = 1.0$.\n  - Replicates: $R = 200000$.\n  - Random seed: $13579$.\n\nNumerics:\n\n- For each microbin integral $\\int_b \\lambda(t)\\,dt$ use a reliable numerical integrator to at least double-precision accuracy.\n- For $s_I = \\sup_{t \\in I}\\lambda(t)$ perform a dense grid search with at least $20001$ equally spaced points in $I$.\n- For each test case, output three floats in this order: mean absolute bias, maximum absolute bias, mean variance, each computed over the subintervals in the partition.\n\nFinal Output Format:\n\n- Your program should produce a single line of output containing the results of all test cases as a comma-separated list enclosed in square brackets, ordered as:\n  $[\\text{TC1-mean-abs-bias}, \\text{TC1-max-abs-bias}, \\text{TC1-mean-var}, \\text{TC2-mean-abs-bias}, \\text{TC2-max-abs-bias}, \\text{TC2-mean-var}, \\text{TC3-mean-abs-bias}, \\text{TC3-max-abs-bias}, \\text{TC3-mean-var}]$.\n- Express all numbers as decimal floats rounded to six digits after the decimal point.",
            "solution": "We begin with the thinning construction on a subinterval $I$ with constant envelope $M_I$. Let $b$ denote a microbin of width $w = |I|/B$, and adopt the idealization that the acceptance probability within $b$ is constant at $p_b \\equiv \\lambda(t_b)/M_I$ for some representative $t_b \\in b$. Let the expected number of proposals in $b$ be $\\mu_b \\equiv M_I w$. Under thinning, conditional on the number of proposals $N_b$, the accepted count $A_b$ is distributed as $\\mathrm{Binomial}(N_b, p_b)$, and unconditionally the total proposals $N_b$ are $\\mathrm{Poisson}(\\mu_b)$.\n\n1) Expectation and variance of $R_b$. Define\n$$\nR_b \\equiv \\begin{cases}\n\\frac{A_b}{N_b},  N_b  0,\\\\\n0,  N_b = 0.\n\\end{cases}\n$$\nCondition on $N_b=k$. For $k=0$, $R_b=0$ deterministically; for $k\\ge 1$, $A_b \\sim \\mathrm{Binomial}(k, p_b)$, so\n$$\n\\mathbb{E}[R_b \\mid N_b=k] = \\mathbb{E}\\!\\left[\\frac{A_b}{k}\\,\\middle|\\,N_b=k\\right] = \\frac{1}{k}\\cdot k p_b = p_b,\n$$\n$$\n\\mathrm{Var}(R_b \\mid N_b=k) = \\mathrm{Var}\\!\\left(\\frac{A_b}{k}\\,\\middle|\\,N_b=k\\right) = \\frac{1}{k^2} \\cdot k p_b(1-p_b) = \\frac{p_b(1-p_b)}{k}.\n$$\nUnconditionally, using the law of iterated expectation with $N_b \\sim \\mathrm{Poisson}(\\mu_b)$,\n$$\n\\mathbb{E}[R_b] = \\sum_{k=0}^{\\infty} \\mathbb{E}[R_b \\mid N_b=k] \\,\\mathbb{P}(N_b=k) \n= 0 \\cdot \\mathbb{P}(N_b=0) + \\sum_{k=1}^{\\infty} p_b \\,\\mathbb{P}(N_b=k)\n= p_b \\cdot \\mathbb{P}(N_b \\ge 1) = p_b \\big(1 - e^{-\\mu_b}\\big).\n$$\nHence, $\\mathbb{E}[R_b]$ is attenuated relative to $p_b$ by the factor $1 - e^{-\\mu_b}$ because of the convention $R_b=0$ when $N_b=0$.\n\nFor the variance, use the law of total variance,\n$$\n\\mathrm{Var}(R_b) = \\mathbb{E}\\!\\left[\\mathrm{Var}(R_b \\mid N_b)\\right] + \\mathrm{Var}\\!\\left(\\mathbb{E}[R_b \\mid N_b]\\right).\n$$\nWe have\n$$\n\\mathbb{E}\\!\\left[\\mathrm{Var}(R_b \\mid N_b)\\right] = \\sum_{k=1}^{\\infty} \\frac{p_b(1-p_b)}{k} \\,\\mathbb{P}(N_b=k)\n= p_b(1-p_b) \\, e^{-\\mu_b} \\sum_{k=1}^{\\infty} \\frac{\\mu_b^k}{k!\\,k}.\n$$\nDefine the series\n$$\nH(\\mu_b) \\equiv \\sum_{k=1}^{\\infty} \\frac{\\mu_b^k}{k!\\,k},\n$$\nwhich converges for all $\\mu_b \\in \\mathbb{R}$ and is positive for $\\mu_b0$. Next,\n$$\n\\mathrm{Var}\\!\\left(\\mathbb{E}[R_b \\mid N_b]\\right) = \\mathrm{Var}\\!\\left(p_b \\cdot \\mathbb{I}\\{N_b \\ge 1\\}\\right) \n= p_b^2 \\,\\mathrm{Var}\\!\\left(\\mathbb{I}\\{N_b \\ge 1\\}\\right)\n= p_b^2 \\left[\\mathbb{P}(N_b \\ge 1)\\mathbb{P}(N_b=0)\\right]\n= p_b^2 \\, e^{-\\mu_b}\\big(1 - e^{-\\mu_b}\\big).\n$$\nCombining terms,\n$$\n\\mathrm{Var}(R_b) = p_b(1-p_b) \\, e^{-\\mu_b} H(\\mu_b) + p_b^2 \\, e^{-\\mu_b}\\big(1 - e^{-\\mu_b}\\big).\n$$\nThis exact expression depends only on $p_b$ and $\\mu_b$ through $H(\\mu_b)$.\n\n2) Bias mechanisms for $\\widehat{s}_I$. The estimator $\\widehat{s}_I = M_I \\max_b R_b$ involves a maximum over noisy microbin estimators $R_b$. Two effects are at play:\n\n- Downward bias from empty microbins. As shown, each $R_b$ has $\\mathbb{E}[R_b] = p_b(1 - e^{-\\mu_b}) \\le p_b$, with strict inequality unless $\\mu_b \\to \\infty$. This is due to the convention $R_b=0$ when $N_b=0$, which occurs with probability $e^{-\\mu_b}$. Hence, microbins with small expected counts $\\mu_b$ systematically attenuate the mean away from the true $p_b$.\n\n- Upward selection bias from maximization. The operation $\\max_b R_b$ over many microbins favors upward fluctuations. Even if $R_b$ were unbiased for $p_b$, the maximum over $B$ noisy estimates would typically exceed the largest $p_b$ in expectation, especially when $B$ is large and the variances $\\mathrm{Var}(R_b)$ are non-negligible. Thus $\\mathbb{E}[\\max_b R_b]$ is pulled upward by extremal noise. The net bias $\\mathbb{E}[\\widehat{s}_I] - s_I$ depends on the balance between downward bias from zeros (controlled by $\\mu_b$) and upward selection bias (increasing with $B$ and variances).\n\n3) Monte Carlo procedure. To quantify the bias and variance of $\\widehat{s}_I$ without simulating individual proposal times, we use the independent Poisson decomposition for counts in each microbin $b \\subset I$:\n$$\nA_b \\sim \\mathrm{Poisson}\\!\\left(\\alpha_b\\right), \\quad \\alpha_b \\equiv \\int_b \\lambda(t)\\,dt, \\qquad\nR_b^{\\mathrm{rej}} \\sim \\mathrm{Poisson}\\!\\left(\\rho_b\\right), \\quad \\rho_b \\equiv \\int_b (M_I - \\lambda(t))\\,dt,\n$$\nindependently across $b$ and between $A_b$ and $R_b^{\\mathrm{rej}}$. Then the total proposals are $N_b = A_b + R_b^{\\mathrm{rej}}$. The empirical acceptance ratio is\n$$\nR_b = \\begin{cases}\n\\frac{A_b}{N_b},  N_b0,\\\\\n0,  N_b=0,\n\\end{cases}\n$$\nand the supremum estimator on $I$ is $\\widehat{s}_I = M_I \\cdot \\max_b R_b$. For each subinterval $I$, we repeat this sampling for a specified number of replicates $R$, compute the sample mean and sample variance of $\\widehat{s}_I$, and compare the mean to the numerical approximation of $s_I = \\sup_{t \\in I}\\lambda(t)$ obtained by a dense grid search.\n\n4) Implementation and test suite. For each test case, we:\n\n- Partition the horizon and, for each subinterval $I$, compute $M_I = \\sup_{t \\in I}\\lambda(t) + \\delta$ with the specified $\\delta$.\n- Partition $I$ into $B$ microbins and compute $\\alpha_b$ and $\\rho_b$ via numerical integration to double-precision accuracy.\n- Generate independent Poisson samples for $(A_b, R_b^{\\mathrm{rej}})$ across microbins and replicates to form $\\widehat{s}_I$ for each replicate.\n- Aggregate across subintervals in the test case to report: (i) mean absolute bias $\\frac{1}{|\\mathcal{I}|}\\sum_{I} \\left|\\overline{\\widehat{s}_I} - s_I\\right|$, (ii) maximum absolute bias $\\max_I \\left|\\overline{\\widehat{s}_I} - s_I\\right|$, and (iii) mean variance $\\frac{1}{|\\mathcal{I}|}\\sum_{I} \\widehat{\\mathrm{Var}}(\\widehat{s}_I)$, where $\\overline{\\widehat{s}_I}$ and $\\widehat{\\mathrm{Var}}(\\widehat{s}_I)$ are the sample mean and sample variance over replicates.\n\nWe ensure reproducibility by fixing the random seed per test case, compute $\\alpha_b$ and $s_I$ with reliable numerical methods, and output the required summary metrics. All outputs are rounded to six decimal places as required. This procedure directly quantifies the bias and variance of the supremum estimator $\\widehat{s}_I$ from proposal counts under realistic thinning.",
            "answer": "```python\nimport numpy as np\nfrom scipy import integrate\n\ndef sup_on_interval(lambda_fn, a, b, ngrid=20001):\n    xs = np.linspace(a, b, ngrid)\n    vals = lambda_fn(xs)\n    return float(np.max(vals))\n\ndef integrate_on_bin(lambda_fn, a, b):\n    # High-accuracy numerical integration of lambda over [a,b]\n    val, _ = integrate.quad(lambda t: float(lambda_fn(np.array([t]))[0]), a, b, epsabs=1e-10, epsrel=1e-10, limit=200)\n    return float(val)\n\ndef simulate_case(lambda_fn, intervals, B, delta, R, seed):\n    rng = np.random.default_rng(seed)\n    abs_biases = []\n    variances = []\n\n    for (a, b) in intervals:\n        # Compute true sup on interval and envelope M_I\n        s_true = sup_on_interval(lambda_fn, a, b, ngrid=40001)\n        M_I = s_true + delta\n\n        # Microbin partition\n        w = (b - a) / B\n        bin_edges = np.linspace(a, b, B + 1)\n\n        # Compute Poisson means for accepted and rejected counts per bin\n        alpha = np.empty(B, dtype=float)\n        for i in range(B):\n            left = bin_edges[i]\n            right = bin_edges[i + 1]\n            alpha[i] = integrate_on_bin(lambda_fn, left, right)\n        rho = M_I * w - alpha\n        # Numerical safety: clip to nonnegative\n        rho = np.clip(rho, 0.0, None)\n\n        # Draw Poisson counts: shape (R, B)\n        # Broadcasting alpha and rho to (R, B)\n        A = rng.poisson(alpha, size=(R, B))\n        Rj = rng.poisson(rho, size=(R, B))\n        N = A + Rj\n\n        # Compute ratios with safe handling of zeros\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratios = np.divide(A, N, out=np.zeros_like(A, dtype=float), where=(N  0))\n\n        # Supremum estimator per replicate\n        s_hat = M_I * np.max(ratios, axis=1)\n\n        # Sample statistics\n        mean_s_hat = float(np.mean(s_hat))\n        var_s_hat = float(np.var(s_hat, ddof=1))\n        bias = mean_s_hat - s_true\n\n        abs_biases.append(abs(bias))\n        variances.append(var_s_hat)\n\n    # Aggregate over subintervals\n    mean_abs_bias = float(np.mean(abs_biases))\n    max_abs_bias = float(np.max(abs_biases))\n    mean_variance = float(np.mean(variances))\n    return mean_abs_bias, max_abs_bias, mean_variance\n\ndef lambda1(t):\n    # t can be numpy array\n    return 6.0 + 4.0 * np.sin(np.pi * t) + 3.0 * np.exp(-((t - 2.0) ** 2))\n\ndef lambda2(t):\n    return 0.6 + 0.3 * np.sin(2.0 * np.pi * t) + 0.1 * np.cos(4.0 * np.pi * t)\n\ndef lambda3(t):\n    return 2.0 + 2.0 * t\n\ndef solve():\n    results = []\n\n    # Test Case 1\n    intervals1 = [(0.0, 1.0), (1.0, 2.0), (2.0, 3.0), (3.0, 4.0)]\n    B1 = 10\n    delta1 = 1.0\n    R1 = 30000\n    seed1 = 12345\n    mab1, xab1, mvar1 = simulate_case(lambda1, intervals1, B1, delta1, R1, seed1)\n    results.extend([mab1, xab1, mvar1])\n\n    # Test Case 2\n    intervals2 = [(0.0, 1.0), (1.0, 2.0)]\n    B2 = 20\n    delta2 = 0.2\n    R2 = 60000\n    seed2 = 24680\n    mab2, xab2, mvar2 = simulate_case(lambda2, intervals2, B2, delta2, R2, seed2)\n    results.extend([mab2, xab2, mvar2])\n\n    # Test Case 3\n    intervals3 = [(0.0, 1.0)]\n    B3 = 1\n    delta3 = 1.0\n    R3 = 200000\n    seed3 = 13579\n    mab3, xab3, mvar3 = simulate_case(lambda3, intervals3, B3, delta3, R3, seed3)\n    results.extend([mab3, xab3, mvar3])\n\n    # Format to six decimal places\n    formatted = \"[\" + \",\".join(f\"{x:.6f}\" for x in results) + \"]\"\n    print(formatted)\n\nsolve()\n```"
        }
    ]
}