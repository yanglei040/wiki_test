{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of mastering numerical methods is to bridge the gap between abstract theory and concrete implementation. This practice focuses on empirically verifying the strong convergence properties of two fundamental schemes for simulating stochastic differential equations: the Euler-Maruyama method and the more accurate Milstein method. By implementing these schemes and applying them to well-chosen test cases, you will use Monte Carlo simulation and log-log regression to measure their convergence orders, providing a tangible confirmation of the theoretical results and building a foundational understanding of how these methods perform in practice .",
            "id": "3339961",
            "problem": "Consider a one-dimensional Itô Stochastic Differential Equation (SDE) of the form $dX_t = a(X_t)\\,dt + b(X_t)\\,dW_t$ with initial condition $X_0 = x_0$, where $W_t$ is a standard Brownian motion and $a(\\cdot)$, $b(\\cdot)$ are sufficiently smooth functions. Strong convergence order quantifies how the pathwise approximation error decreases with the time step size. In this task, you will implement Monte Carlo estimators to empirically measure the strong convergence order at terminal time $T$ for two widely used numerical schemes: the Euler-Maruyama (EM) method and the Milstein method. You must use nested Brownian increments to couple different time step sizes, ensuring consistent pathwise comparisons.\n\nStart from the following fundamental base:\n- The definition of a one-dimensional Itô SDE $dX_t = a(X_t)\\,dt + b(X_t)\\,dW_t$ with $X_0 = x_0$, where $W_t$ is a standard Brownian motion.\n- The concept of strong error at terminal time $T$, namely the quantity $\\mathbb{E}\\left[\\,|\\,X_T - X_T^{(h)}\\,|\\,\\right]$, where $X_T$ is the exact solution and $X_T^{(h)}$ is the numerical approximation with time step size $h$.\n- The understanding that numerical schemes for SDEs can be derived from the Itô-Taylor expansion, and that their truncation yields discrete-time update rules involving $a(\\cdot)$, $b(\\cdot)$, and Brownian increments.\n\nYour program must:\n- Use Monte Carlo sampling with a fixed random seed to ensure reproducibility.\n- Construct a finest grid with $N_{\\text{fine}}$ steps on $[0,T]$ and generate i.i.d. normal increments with variance $T/N_{\\text{fine}}$. For each coarser grid with $N$ steps, obtain its Brownian increments by summing contiguous blocks of the fine-grid increments. This guarantees the same underlying Brownian path is used across discretizations.\n- For each scheme (Euler-Maruyama and Milstein) and each test case, compute the mean absolute strong error at terminal time for several step sizes $h = T/N$, then estimate the empirical strong convergence order $p$ by the slope of the linear regression of $\\log\\big(\\text{error}(h)\\big)$ versus $\\log(h)$.\n\nUse the following three test cases, each scientifically consistent and covering distinct regimes:\n- Test case $1$ (Geometric Brownian Motion): $dX_t = \\mu X_t\\,dt + \\sigma X_t\\,dW_t$, with $x_0 = 1.0$, $T = 1.0$, $\\mu = 0.3$, $\\sigma = 0.6$. The exact terminal value is known: $X_T = x_0 \\exp\\big((\\mu - \\tfrac{1}{2}\\sigma^2)T + \\sigma W_T\\big)$, where $W_T$ is the terminal Brownian value.\n- Test case $2$ (Ornstein-Uhlenbeck-type with additive noise): $dX_t = \\alpha X_t\\,dt + \\beta\\,dW_t$, with $x_0 = 1.0$, $T = 1.0$, $\\alpha = -1.5$, $\\beta = 0.5$. The exact terminal value is not to be used; instead, for each scheme, use the same scheme at a very fine time step as the reference path to compute strong errors.\n- Test case $3$ (Deterministic limit): $dX_t = \\alpha X_t\\,dt$, with $x_0 = 1.2$, $T = 1.0$, $\\alpha = 0.8$. Here $b(\\cdot)\\equiv 0$, and the exact terminal value is $X_T = x_0 e^{\\alpha T}$.\n\nMonte Carlo and discretization parameters:\n- Number of Monte Carlo paths $M = 10000$.\n- Finest grid size $N_{\\text{fine}} = 1024$.\n- Coarser grids $N \\in \\{16, 32, 64, 128\\}$, so that $h = T/N$ spans four scales.\n- Use nested increments from the finest grid for all coarser grids, per test case.\n- Use a fixed random seed $42$.\n\nComputation requirements:\n- For test case $1$, compute strong errors for each $N$ by comparing each scheme’s terminal approximation to the exact terminal value using the shared $W_T$ implied by the finest grid.\n- For test case $2$, compute strong errors by comparing each scheme’s terminal approximation at each $N$ to the same scheme’s terminal approximation on the finest grid $N_{\\text{fine}}$.\n- For test case $3$, compute strong errors for each $N$ by comparing each scheme’s terminal approximation to the exact terminal value.\n\nThen, for each test case and scheme, estimate the empirical strong convergence order $p$ as the slope of the best-fit line of $\\log\\big(\\text{error}(h)\\big)$ versus $\\log(h)$ over the four $h$ values.\n\nFinal output format:\n- Your program should produce a single line of output containing the six estimated orders as a comma-separated list enclosed in square brackets, ordered as $[\\text{p\\_EM\\_1}, \\text{p\\_Milstein\\_1}, \\text{p\\_EM\\_2}, \\text{p\\_Milstein\\_2}, \\text{p\\_EM\\_3}, \\text{p\\_Milstein\\_3}]$, where the subscripts indicate the test case index.\n- Each entry must be a float.\n\nNo physical units are involved. Angles do not appear. Percentages must not be used.\n\nTest suite coverage rationale:\n- Test case $1$ exercises multiplicative noise with a known exact solution, probing the canonical strong orders expected for the Euler-Maruyama and Milstein methods under smooth coefficients.\n- Test case $2$ exercises additive noise, for which diffusion is constant and derivative terms vanish, exposing how the schemes behave when the stochastic integral is exact at the discrete level for constant diffusion.\n- Test case $3$ is a boundary condition with no noise, reducing SDE discretization to ordinary differential equation discretization, testing whether the strong order aligns with deterministic numerical integration behavior.\n\nYour program must strictly adhere to the specified final output format.",
            "solution": "The problem requires the empirical estimation of the strong convergence order for the Euler-Maruyama and Milstein numerical schemes applied to one-dimensional Itô Stochastic Differential Equations (SDEs). The strong convergence order, denoted $p$, characterizes the rate at which the pathwise approximation error at a terminal time $T$ diminishes as a function of the time step size $h$.\n\nA one-dimensional Itô SDE is described by the formal differential equation:\n$$ dX_t = a(X_t) \\, dt + b(X_t) \\, dW_t, \\quad X_0 = x_0 $$\nwhere $X_t$ is the stochastic process, $a(\\cdot)$ is the drift coefficient, $b(\\cdot)$ is the diffusion coefficient, and $W_t$ is a standard one-dimensional Brownian motion (Wiener process).\n\nThe strong error at terminal time $T$ for a numerical approximation $X_T^{(h)}$ with step size $h$ is defined as the expected absolute difference between the numerical and true solutions:\n$$ \\epsilon(h) = \\mathbb{E}\\left[ \\,|\\, X_T - X_T^{(h)} \\,|\\, \\right] $$\nA scheme has a strong convergence order of $p$ if $\\epsilon(h) = \\mathcal{O}(h^p)$ for small $h$. Taking the logarithm, we have $\\log(\\epsilon(h)) \\approx C + p \\log(h)$, which implies that $p$ can be estimated as the slope of a linear regression of $\\log(\\epsilon(h))$ against $\\log(h)$. We will use a Monte Carlo method to estimate the expectation.\n\n### Numerical Schemes\n\nNumerical schemes for SDEs are typically derived from the Itô-Taylor expansion of the process $X_t$.\n\nThe **Euler-Maruyama (EM)** scheme is the simplest method, obtained by truncating the Itô-Taylor expansion. For a discrete time grid $0 = t_0  t_1  \\dots  t_N = T$ with a constant step size $h = t_{n+1} - t_n$, the update rule is:\n$$ X_{n+1} = X_n + a(X_n) h + b(X_n) \\Delta W_n $$\nwhere $X_n$ is the approximation of $X_{t_n}$ and $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$ is a Brownian increment, distributed as a normal random variable $\\mathcal{N}(0, h)$. Under suitable smoothness conditions on $a$ and $b$, the EM scheme has a strong convergence order of $p=0.5$.\n\nThe **Milstein** scheme includes an additional term from the Itô-Taylor expansion, which accounts for the interaction between the diffusion term and itself. Its update rule is:\n$$ X_{n+1} = X_n + a(X_n) h + b(X_n) \\Delta W_n + \\frac{1}{2} b(X_n) b'(X_n) \\left( (\\Delta W_n)^2 - h \\right) $$\nwhere $b'(x) = \\frac{db}{dx}$. This correction term generally improves the accuracy, and the Milstein scheme has a strong convergence order of $p=1.0$ for SDEs with smooth coefficients.\n\n### Simulation and Estimation Methodology\n\nWe will estimate the strong error $\\epsilon(h)$ for a set of step sizes $h_k = T/N_k$ where $N_k \\in \\{16, 32, 64, 128\\}$. The expectation is approximated by a Monte Carlo average over $M=10000$ simulated paths:\n$$ \\epsilon(h) \\approx \\frac{1}{M} \\sum_{i=1}^{M} \\left| \\, X_T^{(i)} - X_{T,h}^{(i)} \\, \\right| $$\nwhere $X_T^{(i)}$ is the reference solution for the $i$-th path and $X_{T,h}^{(i)}$ is the numerical approximation.\n\nTo ensure consistent pathwise comparison, a **nested increment** approach is employed. A single Brownian path is generated on a fine grid with $N_{\\text{fine}}=1024$ steps. The increments for this fine path are i.i.d. random variables $\\Delta W_k^{\\text{fine}} \\sim \\mathcal{N}(0, T/N_{\\text{fine}})$. The increments for any coarser grid with $N$ steps (where $N$ is a divisor of $N_{\\text{fine}}$) are constructed by summing contiguous blocks of $R = N_{\\text{fine}}/N$ fine increments. This ensures that the underlying Brownian motion is identical for all discretizations, reducing the variance of the error estimate.\n\nThe convergence order $p$ is then estimated for each scheme and test case by performing a linear regression on the pairs $(\\log(h_k), \\log(\\epsilon(h_k)))$. The slope of this regression line provides the empirical estimate for $p$.\n\n### Analysis of Test Cases\n\nThe three test cases are designed to probe the behavior of the schemes under different conditions.\n\n**Test Case 1: Geometric Brownian Motion**\n- SDE: $dX_t = \\mu X_t \\, dt + \\sigma X_t \\, dW_t$, with $X_0 = 1.0$, $T=1.0$, $\\mu=0.3$, $\\sigma=0.6$.\n- Coefficients: $a(x) = \\mu x$, $b(x) = \\sigma x$, and $b'(x) = \\sigma$.\n- EM rule: $X_{n+1} = X_n(1 + \\mu h + \\sigma \\Delta W_n)$.\n- Milstein rule: $X_{n+1} = X_n(1 + \\mu h + \\sigma \\Delta W_n) + \\frac{1}{2} \\sigma^2 X_n ((\\Delta W_n)^2 - h)$.\n- Reference: The exact solution at time $T$ is $X_T = X_0 \\exp\\left((\\mu - \\frac{1}{2}\\sigma^2)T + \\sigma W_T\\right)$, where $W_T$ is the sum of all fine-grid Brownian increments.\n- Expected Orders: This is a standard case with multiplicative noise. The theoretical orders are $p=0.5$ for EM and $p=1.0$ for Milstein.\n\n**Test Case 2: Ornstein-Uhlenbeck Type with Additive Noise**\n- SDE: $dX_t = \\alpha X_t \\, dt + \\beta \\, dW_t$, with $X_0 = 1.0$, $T=1.0$, $\\alpha=-1.5$, $\\beta=0.5$.\n- Coefficients: $a(x) = \\alpha x$, $b(x) = \\beta$ (constant), and $b'(x) = 0$.\n- With $b'(x) = 0$, the correction term in the Milstein scheme vanishes. Thus, the Milstein scheme becomes identical to the Euler-Maruyama scheme: $X_{n+1} = X_n + \\alpha X_n h + \\beta \\Delta W_n$.\n- Reference: Since an exact solution is not used, the reference solution is a highly accurate numerical simulation performed using the same scheme on the fine grid with $N_{\\text{fine}}=1024$ steps.\n- Expected Orders: Because the diffusion coefficient $b(x)$ is constant, the stochastic integral component is discretized exactly by both schemes. The approximation error arises solely from the discretization of the drift term, which is a first-order Euler method. Therefore, both schemes are expected to exhibit a strong convergence order of $p=1.0$.\n\n**Test Case 3: Deterministic Limit**\n- SDE: $dX_t = \\alpha X_t \\, dt$, with $X_0 = 1.2$, $T=1.0$, $\\alpha=0.8$.\n- This is an Ordinary Differential Equation (ODE) as the diffusion term is zero, $b(x) \\equiv 0$, which also implies $b'(x) \\equiv 0$.\n- Both EM and Milstein schemes reduce to the explicit Euler method for ODEs: $X_{n+1} = X_n + \\alpha X_n h = X_n(1 + \\alpha h)$.\n- Reference: The exact solution is $X_T = X_0 e^{\\alpha T}$.\n- Expected Orders: The explicit Euler method for ODEs has an order of convergence of $1.0$. Therefore, both schemes are expected to yield $p \\approx 1.0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Estimates the strong convergence order for the Euler-Maruyama and Milstein\n    schemes across three different SDE test cases.\n    \"\"\"\n    # Define global simulation parameters.\n    M = 10000\n    N_fine = 1024\n    Ns = np.array([16, 32, 64, 128])\n    SEED = 42\n    rng = np.random.default_rng(SEED)\n    \n    results = []\n\n    # === Test Case 1: Geometric Brownian Motion ===\n    x0_1, T_1, mu_1, sigma_1 = 1.0, 1.0, 0.3, 0.6\n    h_values_1 = T_1 / Ns\n    \n    # Generate one set of fine Brownian increments for all paths\n    fine_dWs_1 = rng.normal(loc=0.0, scale=np.sqrt(T_1 / N_fine), size=(M, N_fine))\n    \n    # Calculate exact terminal value as reference\n    W_T_1 = np.sum(fine_dWs_1, axis=1)\n    exact_XT_1 = x0_1 * np.exp((mu_1 - 0.5 * sigma_1**2) * T_1 + sigma_1 * W_T_1)\n\n    em_errors_1, milstein_errors_1 = [], []\n    for N in Ns:\n        h = T_1 / N\n        R = N_fine // N\n        coarse_dWs = fine_dWs_1.reshape(M, N, R).sum(axis=2)\n        \n        # Euler-Maruyama simulation\n        X_em = np.full(M, x0_1)\n        for i in range(N):\n            dW = coarse_dWs[:, i]\n            X_em += mu_1 * X_em * h + sigma_1 * X_em * dW\n        em_errors_1.append(np.mean(np.abs(X_em - exact_XT_1)))\n\n        # Milstein simulation\n        X_milstein = np.full(M, x0_1)\n        for i in range(N):\n            dW = coarse_dWs[:, i]\n            X_milstein += mu_1 * X_milstein * h + sigma_1 * X_milstein * dW + \\\n                          0.5 * sigma_1 * sigma_1 * X_milstein * (dW**2 - h)\n        milstein_errors_1.append(np.mean(np.abs(X_milstein - exact_XT_1)))\n\n    p_em_1 = np.polyfit(np.log(h_values_1), np.log(em_errors_1), 1)[0]\n    p_milstein_1 = np.polyfit(np.log(h_values_1), np.log(milstein_errors_1), 1)[0]\n    results.extend([p_em_1, p_milstein_1])\n\n    # === Test Case 2: Ornstein-Uhlenbeck-type (Additive Noise) ===\n    x0_2, T_2, alpha_2, beta_2 = 1.0, 1.0, -1.5, 0.5\n    h_values_2 = T_2 / Ns\n\n    fine_dWs_2 = rng.normal(loc=0.0, scale=np.sqrt(T_2 / N_fine), size=(M, N_fine))\n    \n    # Reference solution is the fine-grid simulation.\n    # For additive noise, EM and Milstein are identical (b'(x)=0).\n    X_ref_2 = np.full(M, x0_2)\n    h_fine = T_2 / N_fine\n    for i in range(N_fine):\n        dW = fine_dWs_2[:, i]\n        X_ref_2 += alpha_2 * X_ref_2 * h_fine + beta_2 * dW\n        \n    errors_2 = []\n    for N in Ns:\n        h = T_2 / N\n        R = N_fine // N\n        coarse_dWs = fine_dWs_2.reshape(M, N, R).sum(axis=2)\n\n        X_num = np.full(M, x0_2)\n        for i in range(N):\n            dW = coarse_dWs[:, i]\n            X_num += alpha_2 * X_num * h + beta_2 * dW\n        \n        errors_2.append(np.mean(np.abs(X_num - X_ref_2)))\n    \n    p_2 = np.polyfit(np.log(h_values_2), np.log(errors_2), 1)[0]\n    # EM and Milstein orders are the same.\n    p_em_2 = p_2\n    p_milstein_2 = p_2\n    results.extend([p_em_2, p_milstein_2])\n\n    # === Test Case 3: Deterministic Limit (ODE) ===\n    x0_3, T_3, alpha_3 = 1.2, 1.0, 0.8\n    h_values_3 = T_3 / Ns\n\n    # Exact solution for the ODE\n    exact_XT_3 = x0_3 * np.exp(alpha_3 * T_3)\n\n    # For b=0, EM and Milstein are identical (Euler method for ODEs).\n    # The simulation is deterministic, so no Monte Carlo paths are needed.\n    errors_3 = []\n    for N in Ns:\n        h = T_3 / N\n        X_num = float(x0_3) # Use a scalar\n        for _ in range(N):\n            X_num += alpha_3 * X_num * h\n            \n        errors_3.append(np.abs(X_num - exact_XT_3))\n\n    p_3 = np.polyfit(np.log(h_values_3), np.log(errors_3), 1)[0]\n    # EM and Milstein orders are the same.\n    p_em_3 = p_3\n    p_milstein_3 = p_3\n    results.extend([p_em_3, p_milstein_3])\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While higher-order methods like the Milstein scheme offer improved accuracy, this advantage often comes at an increased computational price. This practice shifts our focus from empirical implementation to analytical evaluation, asking a crucial question for any practitioner: what is the cost of accuracy? You will derive the computational complexity of the full Milstein scheme relative to a simplified version, paying close attention to how the cost scales with the dimension $m$ of the system. This exercise highlights the critical trade-off between numerical precision and computational efficiency that governs the choice of a solver in real-world applications .",
            "id": "3002635",
            "problem": "Consider an $m$-dimensional Itô stochastic differential equation (SDE) of the form\n$$\n\\mathrm{d}X_{t} = \\mu(X_{t})\\,\\mathrm{d}t + \\sigma(X_{t})\\,\\mathrm{d}W_{t},\n$$\nwhere $X_{t}\\in\\mathbb{R}^{m}$, $\\mu:\\mathbb{R}^{m}\\to\\mathbb{R}^{m}$, $\\sigma:\\mathbb{R}^{m}\\to\\mathbb{R}^{m\\times m}$, and $W_{t}$ is an $m$-dimensional standard Brownian motion. The Milstein family of strong order-$1$ schemes is based on a first-order Itô–Taylor expansion, which includes drift terms, diffusion terms, and additional first-order terms built from Lie derivatives of the diffusion columns along diffusion directions, together with off-diagonal iterated stochastic integrals (Lévy areas) when the noise is noncommutative. The so-called simplified Milstein scheme uses only the $i=j$ Lie-derivative contributions and replaces off-diagonal iterated integrals by algebraic surrogates, while the full Milstein scheme retains all $i,j$ Lie-derivative contributions and simulates the off-diagonal iterated integrals.\n\nAdopt the following operation-count model for one time step applied at a state $x\\in\\mathbb{R}^{m}$:\n\n- Evaluating the drift vector $\\mu(x)$ costs $c_{\\mu} m$.\n- Evaluating the diffusion matrix $\\sigma(x)$ costs $c_{\\sigma} m^{2}$.\n- For each diffusion column $\\sigma^{(j)}:\\mathbb{R}^{m}\\to\\mathbb{R}^{m}$, forming its Jacobian matrix $D\\sigma^{(j)}(x)\\in\\mathbb{R}^{m\\times m}$ costs $c_{D} m^{2}$.\n- Multiplying an $m\\times m$ matrix by an $m$-vector costs $c_{M} m$.\n- Generating a single scalar Gaussian increment costs $c_{W}$.\n- Simulating a single off-diagonal iterated integral (a Lévy area between two distinct Brownian components) costs $c_{A}$.\n\nAssume that both schemes evaluate $\\mu(x)$ and $\\sigma(x)$ once per step and generate $m$ Gaussian increments. Based on the Itô–Taylor expansion structure:\n\n- The simplified Milstein scheme requires, for each $j\\in\\{1,\\dots,m\\}$, only the diagonal Lie-derivative contribution, which can be assembled by forming $D\\sigma^{(j)}(x)$ once and multiplying it by $\\sigma^{(j)}(x)$ once; it does not simulate any off-diagonal iterated integrals.\n- The full Milstein scheme requires, for each $j\\in\\{1,\\dots,m\\}$, all $m$ directional actions $D\\sigma^{(j)}(x)\\,\\sigma^{(i)}(x)$ for $i\\in\\{1,\\dots,m\\}$, and it simulates all $\\frac{m(m-1)}{2}$ off-diagonal iterated integrals.\n\nIgnoring any costs not explicitly listed above, and expressing everything in terms of $m$ and the positive constants $c_{\\mu}$, $c_{\\sigma}$, $c_{D}$, $c_{M}$, $c_{W}$, and $c_{A}$, derive from first principles the exact symbolic expression for the ratio of the per-step computational cost of the full Milstein scheme to that of the simplified Milstein scheme, as a function of $m$.\n\nYour final answer must be a single closed-form analytic expression for this ratio in terms of $m$, $c_{\\mu}$, $c_{\\sigma}$, $c_{D}$, $c_{M}$, $c_{W}$, and $c_{A}$.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of numerical methods for stochastic differential equations, is well-posed with a clear objective and a complete set of definitions, and is expressed in objective, formal language. We proceed to derive the computational cost for each scheme based on the provided operation-count model.\n\nLet $C_{\\text{simp}}$ be the per-step computational cost of the simplified Milstein scheme and $C_{\\text{full}}$ be the per-step computational cost of the full Milstein scheme for an $m$-dimensional system.\n\nFirst, we calculate $C_{\\text{simp}}$. The total cost is the sum of the costs of its constituent operations for one time step.\n1.  Evaluation of the drift vector $\\mu(x)$: The cost is given as $c_{\\mu} m$.\n2.  Evaluation of the diffusion matrix $\\sigma(x)$: The cost is given as $c_{\\sigma} m^{2}$.\n3.  Generation of Wiener increments: The scheme requires $m$ independent scalar Gaussian increments. The cost is $m \\times c_{W} = c_{W} m$.\n4.  Lie derivative contributions: The simplified scheme requires only the diagonal contributions. For each $j \\in \\{1, \\dots, m\\}$, we must assemble the term involving $D\\sigma^{(j)}(x)\\sigma^{(j)}(x)$. This involves two sub-steps for each $j$:\n    a. Forming the Jacobian matrix $D\\sigma^{(j)}(x)$, which costs $c_{D} m^{2}$. Since this is done for each of the $m$ columns, the total cost for forming all necessary Jacobians is $m \\times (c_{D} m^{2}) = c_{D} m^{3}$.\n    b. Multiplying the $m \\times m$ matrix $D\\sigma^{(j)}(x)$ by the $m$-vector $\\sigma^{(j)}(x)$, which costs $c_{M} m$. This is performed for each of the $m$ values of $j$, so the total cost for these matrix-vector products is $m \\times (c_{M} m) = c_{M} m^{2}$.\n5.  Off-diagonal iterated integrals: The simplified scheme does not simulate these, so the cost is $0$.\n\nSumming these costs gives the total cost for the simplified scheme:\n$$ C_{\\text{simp}} = c_{\\mu} m + c_{\\sigma} m^{2} + c_{W} m + c_{D} m^{3} + c_{M} m^{2} $$\nGrouping terms by powers of $m$:\n$$ C_{\\text{simp}} = c_{D} m^{3} + (c_{\\sigma} + c_{M}) m^{2} + (c_{\\mu} + c_{W}) m $$\n\nNext, we calculate $C_{\\text{full}}$.\n1.  Evaluation of $\\mu(x)$: Cost is $c_{\\mu} m$.\n2.  Evaluation of $\\sigma(x)$: Cost is $c_{\\sigma} m^{2}$.\n3.  Generation of Wiener increments: Cost is $c_{W} m$.\n4.  Lie derivative contributions: The full scheme requires all $m^{2}$ actions $D\\sigma^{(j)}(x)\\sigma^{(i)}(x)$ for $i,j \\in \\{1, \\dots, m\\}$.\n    a. Forming Jacobians: As in the simplified scheme, we must form the Jacobian $D\\sigma^{(j)}(x)$ for each $j \\in \\{1, \\dots, m\\}$. The total cost remains $m \\times (c_{D} m^{2}) = c_{D} m^{3}$.\n    b. Matrix-vector products: For each of the $m$ Jacobians $D\\sigma^{(j)}(x)$, we must multiply it by all $m$ diffusion columns $\\sigma^{(i)}(x)$. This results in a total of $m \\times m = m^{2}$ matrix-vector products. The total cost for these products is $m^{2} \\times (c_{M} m) = c_{M} m^{3}$.\n5.  Off-diagonal iterated integrals: The full scheme simulates all $\\frac{m(m-1)}{2}$ distinct off-diagonal iterated integrals. The cost of simulating a single such integral is $c_{A}$. The total cost is $\\frac{m(m-1)}{2} c_{A}$.\n\nSumming these costs gives the total cost for the full scheme:\n$$ C_{\\text{full}} = c_{\\mu} m + c_{\\sigma} m^{2} + c_{W} m + c_{D} m^{3} + c_{M} m^{3} + \\frac{m(m-1)}{2} c_{A} $$\nExpanding and grouping terms by powers of $m$:\n$$ C_{\\text{full}} = (c_{D} + c_{M}) m^{3} + c_{\\sigma} m^{2} + (c_{\\mu} + c_{W}) m + \\frac{c_{A}}{2} m^{2} - \\frac{c_{A}}{2} m $$\n$$ C_{\\text{full}} = (c_{D} + c_{M}) m^{3} + \\left(c_{\\sigma} + \\frac{c_{A}}{2}\\right) m^{2} + \\left(c_{\\mu} + c_{W} - \\frac{c_{A}}{2}\\right) m $$\n\nFinally, we compute the ratio of the per-step computational cost of the full Milstein scheme to that of the simplified Milstein scheme:\n$$ \\frac{C_{\\text{full}}}{C_{\\text{simp}}} = \\frac{(c_{D} + c_{M}) m^{3} + (c_{\\sigma} + \\frac{c_{A}}{2}) m^{2} + (c_{\\mu} + c_{W} - \\frac{c_{A}}{2}) m}{c_{D} m^{3} + (c_{\\sigma} + c_{M}) m^{2} + (c_{\\mu} + c_{W}) m} $$\nSince $m$ is the dimension of the system, we can assume $m \\geq 1$. Thus, we can divide the numerator and the denominator by $m$:\n$$ \\frac{C_{\\text{full}}}{C_{\\text{simp}}} = \\frac{(c_{D} + c_{M}) m^{2} + (c_{\\sigma} + \\frac{c_{A}}{2}) m + (c_{\\mu} + c_{W} - \\frac{c_{A}}{2})}{c_{D} m^{2} + (c_{\\sigma} + c_{M}) m + (c_{\\mu} + c_{W})} $$\nThis is the final symbolic expression for the ratio.",
            "answer": "$$\\boxed{\\frac{(c_{D} + c_{M}) m^{2} + (c_{\\sigma} + \\frac{c_{A}}{2}) m + (c_{\\mu} + c_{W} - \\frac{c_{A}}{2})}{c_{D} m^{2} + (c_{\\sigma} + c_{M}) m + (c_{\\mu} + c_{W})}}$$"
        },
        {
            "introduction": "Fixed-step integration schemes are simple to implement but can be highly inefficient, taking unnecessarily small steps when the solution is smooth or dangerously large ones when it is volatile. This advanced practice introduces a more sophisticated and powerful approach: adaptive time-stepping. You are tasked with designing and implementing an adaptive controller for the Euler-Maruyama method that dynamically adjusts the step size $h$ based on local error estimates. The goal is to maintain a global error below a specified tolerance, a technique that is essential for efficiently simulating complex SDEs with varying dynamics .",
            "id": "3339942",
            "problem": "You are asked to design, justify, and implement an adaptive time-stepping scheme for simulating a scalar stochastic differential equation (SDE) using Euler–Maruyama integration, with a local step control derived from the sensitivity of the drift and diffusion. The SDE has the form\n$$\ndX_t = a(X_t)\\,dt + b(X_t)\\,dW_t,\n$$\nwhere $W_t$ is a standard Wiener process. Your design must be grounded in fundamental facts from Itō calculus and the strong error properties of Euler–Maruyama. The goal is to ensure that the mean-square strong error at a fixed final time $T$,\n$$\n\\mathbb{E}\\bigl[\\,|X_T - \\hat{X}_T|^2\\,\\bigr],\n$$\nmeets or is below a specified target tolerance.\n\nStart from the following accepted base:\n- Itō stochastic calculus for $dX_t = a(X_t)\\,dt + b(X_t)\\,dW_t$.\n- The Euler–Maruyama scheme as a first-order weak, half-order strong method, whose global mean-square strong error scales with the step size as a small-step asymptotic.\n- The fact that local mean-square one-step error depends on local variations of $a(\\cdot)$ and $b(\\cdot)$, which are captured by their derivatives.\n\nDesign requirements:\n1. Devise an adaptive time step selection based on instantaneous estimates of the magnitudes $|a'(X_t)|$ and $|b'(X_t)|$ evaluated at the current numerical state. Your design should use these derivatives to regulate the step size so that the accumulated mean-square strong error over $[0,T]$ is controlled to meet a specified target tolerance $\\varepsilon^2$ for $\\mathbb{E}\\bigl[\\,|X_T - \\hat{X}_T|^2\\,\\bigr]$.\n2. Your scheme must be implementable using the Euler–Maruyama update\n$$\n\\hat{X}_{t+h} = \\hat{X}_t + a(\\hat{X}_t)\\,h + b(\\hat{X}_t)\\,\\Delta W,\n$$\nwith $\\Delta W \\sim \\mathcal{N}(0,h)$, and must construct an adapted sequence of steps $h$ that tile the interval $[0,T]$ exactly (adjusting the last step if necessary).\n3. Your step selection must be explicitly expressed in terms of quantities computable at runtime from $a(\\cdot)$, $b(\\cdot)$, $a'(\\cdot)$, $b'(\\cdot)$, and problem parameters (including $\\varepsilon^2$ and $T$). You may include minimal regularization constants and safety factors, provided you justify them in your solution.\n\nMonte Carlo evaluation protocol:\n- Use the same realized Wiener path to couple the exact solution $X_T$ and the numerical solution $\\hat{X}_T$ to estimate the strong error. For each simulated path, draw Gaussian increments $\\Delta W \\sim \\mathcal{N}(0,h)$ for your adaptive step sequence, and define $W_T := \\sum \\Delta W$ for that path. For SDEs with closed-form solutions that depend only on $W_T$, compute $X_T$ exactly using $W_T$. Use the sample mean of squared errors over many independent paths as the Monte Carlo estimate of $\\mathbb{E}\\bigl[\\,|X_T - \\hat{X}_T|^2\\,\\bigr]$.\n- Fix a deterministic seed for the random number generator for reproducibility.\n\nTest suite:\nImplement your algorithm and evaluate it on the following three test cases. There are no physical units; treat all quantities as dimensionless. Angles do not appear. Output should be unitless real numbers or booleans as specified.\n\n1. Geometric Brownian Motion (GBM) with parameters\n   - $a(x) = \\mu x$, $b(x) = \\sigma x$, with derivatives $a'(x) = \\mu$, $b'(x) = \\sigma$,\n   - $X_0 = 1.0$, $T = 1.0$, $\\mu = 0.5$, $\\sigma = 0.7$, target $\\varepsilon^2 = 5\\times 10^{-3}$,\n   - Exact solution at time $T$ given Wiener terminal value $W_T$:\n     $$\n     X_T = X_0 \\exp\\bigl((\\mu - \\tfrac{1}{2}\\sigma^2)T + \\sigma W_T\\bigr).\n     $$\n2. Geometric Brownian Motion (GBM) with parameters\n   - $a(x) = \\mu x$, $b(x) = \\sigma x$, derivatives $a'(x) = \\mu$, $b'(x) = \\sigma$,\n   - $X_0 = 0.5$, $T = 1.5$, $\\mu = 1.2$, $\\sigma = 1.5$, target $\\varepsilon^2 = 2\\times 10^{-2}$,\n   - Exact solution as above with the given parameters.\n3. Constant-coefficient additive-noise SDE\n   - $a(x) = \\alpha$, $b(x) = \\beta$, derivatives $a'(x) = 0$, $b'(x) = 0$,\n   - $X_0 = -1.0$, $T = 2.0$, $\\alpha = 0.3$, $\\beta = 0.8$, target $\\varepsilon^2 = 10^{-8}$,\n   - Exact solution:\n     $$\n     X_T = X_0 + \\alpha T + \\beta W_T.\n     $$\n\nMonte Carlo configuration:\n- Use exactly $N = 4000$ independent paths for each test case.\n- Use a single fixed seed for the pseudorandom number generator to ensure reproducible results across runs.\n\nFinal program output:\n- For each test case, compute the Monte Carlo estimate of $\\mathbb{E}\\bigl[\\,|X_T - \\hat{X}_T|^2\\,\\bigr]$ and compare it to the target tolerance $\\varepsilon^2$.\n- Your program should produce a single line of output containing the three boolean results in a Python-style list, where each boolean indicates whether the estimated mean-square strong error does not exceed the target tolerance for the corresponding test case. For example, the output format must be exactly like:\n\"[True,False,True]\".",
            "solution": "The task is to design, justify, and implement an adaptive time-stepping scheme for the numerical simulation of a scalar stochastic differential equation (SDE) of the form $dX_t = a(X_t)\\,dt + b(X_t)\\,dW_t$. The scheme must use the Euler-Maruyama method and adapt the step size $h$ to control the global mean-square strong error, $\\mathbb{E}\\bigl[\\,|X_T - \\hat{X}_T|^2\\,\\bigr]$, to be below a specified tolerance $\\varepsilon^2$. The step-size control must be based on the derivatives of the drift and diffusion coefficients, $a'(x)$ and $b'(x)$.\n\n### Principle-Based Design of the Adaptive Scheme\n\nThe design of the adaptive step-size controller is based on a heuristic but standard model of error accumulation for strong solutions of SDEs.\n\n**1. Strong Error of the Euler-Maruyama Method**\n\nFor a constant step size $h$, the Euler-Maruyama method is known to have a strong order of convergence of $0.5$. This implies that the global mean-square strong error at a fixed time $T$ scales linearly with the step size for small $h$:\n$$\n\\mathbb{E}\\bigl[\\,|X_T - \\hat{X}_T|^2\\,\\bigr] \\approx C \\cdot h\n$$\nThe constant $C$ depends on the time interval $T$ and the properties of the coefficients $a(x)$ and $b(x)$ along the solution path. For a process where the \"stiffness\" or sensitivity varies, captured by the derivatives $a'$ and $b'$, it is reasonable to assume that $C$ can be expressed as an integral of some local error indicator function, $G(x)$, over the path. This gives:\n$$\n\\mathbb{E}\\bigl[\\,|X_T - \\hat{X}_T|^2\\,\\bigr] \\approx h \\int_0^T \\mathbb{E}[G(X_s)] ds\n$$\n\n**2. Generalization to an Adaptive Step Size**\n\nFor an adaptive scheme, the step size $h$ is no longer constant but becomes a function of the current state, $h_t = h(X_t)$. We generalize the error model by replacing the constant $h$ inside the integral with the state-dependent step size $h_s$:\n$$\n\\mathbb{E}\\bigl[\\,|X_T - \\hat{X}_T|^2\\,\\bigr] \\approx \\int_0^T \\mathbb{E}[G(X_s) h_s] ds\n$$\nThis model posits that the total error is the sum of local error contributions, where the contribution \"rate\" at time $s$ is $G(X_s) h_s$. While a simplification of the complex error propagation dynamics, this model provides a practical basis for an I-controller (Integral controller) design.\n\n**3. Step-Size Control Law via Error Equidistribution**\n\nTo control the total error to meet the target tolerance $\\varepsilon^2$, we employ the principle of error equidistribution. We seek to keep the instantaneous error rate constant throughout the simulation. That is, we impose the condition:\n$$\nG(X_t) h_t = K\n$$\nwhere $K$ is a constant. Substituting this into our global error model:\n$$\n\\int_0^T \\mathbb{E}[K] ds = K \\int_0^T ds = KT\n$$\nTo meet the tolerance, we set $KT = \\varepsilon^2$, which determines the constant $K = \\varepsilon^2 / T$. This yields the adaptive step-size control law:\n$$\nh_t = \\frac{\\varepsilon^2}{T \\cdot G(X_t)}\n$$\n\n**4. The Error Indicator Function $G(x)$**\n\nThe problem requires the controller to be based on $|a'(x)|$ and $|b'(x)|$. The local error of the Euler-Maruyama scheme arises from approximating the coefficients $a(s)$ and $b(s)$ as constant over a step $[t, t+h]$. The magnitude of this approximation error is related to the derivatives of the coefficients and the variation of the process itself, which is driven by the diffusion term $b(x)$. The leading terms in Itō-Taylor expansions for the local error involve products like $a'(x)b(x)$ and $b'(x)b(x)$. This motivates an error indicator function that combines these effects. We propose the following form for $G(x)$:\n$$\nG(x) = |b(x)| \\left( |a'(x)| + |b'(x)| \\right)\n$$\nThis form captures the required dependencies: the step size will decrease when the derivatives are large (high sensitivity) or when the diffusion term is large (high volatility). The proportionality constant is taken as $1$ for simplicity, as its effect can be absorbed into a safety factor.\n\n**5. Final Control Algorithm with Regularization**\n\nIn practice, we introduce a safety factor $\\gamma  1$ (e.g., $\\gamma=0.9$) to be more conservative and increase the probability of meeting the tolerance. Furthermore, we add a small regularization parameter $\\delta  0$ to the denominator to prevent division by zero or excessively large steps when $G(x)$ is zero or near-zero. This is critical for SDEs like the constant-coefficient case where the derivatives are zero, and for which the Euler-Maruyama scheme is exact and a very large step size is appropriate.\n\nThe final, implementable step-size rule at a state $\\hat{X}_t$ is:\n$$\nh = \\frac{\\gamma \\cdot \\varepsilon^2}{T \\cdot (G(\\hat{X}_t) + \\delta)}\n$$\nwhere $G(\\hat{X}_t) = |b(\\hat{X}_t)| \\left( |a'(\\hat{X}_t)| + |b'(\\hat{X}_t)| \\right)$.\n\nThe complete algorithm for a single path simulation is:\n1. Initialize $t=0$, $\\hat{X}=\\hat{X}_0$, and the cumulative Wiener increment $W_T=0$.\n2. While $t  T$:\n   a. Calculate the step size $h$ using the control law above.\n   b. To ensure the simulation terminates exactly at $T$, clip the step size: $h = \\min(h, T-t)$.\n   c. Perform the Euler-Maruyama update:\n      i. Generate a standard normal random variable $Z \\sim \\mathcal{N}(0,1)$.\n      ii. Compute the Wiener increment $\\Delta W = Z \\sqrt{h}$.\n      iii. Update the state: $\\hat{X} \\leftarrow \\hat{X} + a(\\hat{X})h + b(\\hat{X})\\Delta W$.\n   d. Update the total Wiener increment: $W_T \\leftarrow W_T + \\Delta W$.\n   e. Advance time: $t \\leftarrow t+h$.\n3. The simulation yields the numerical approximation $\\hat{X}_T$ and the corresponding total Wiener increment $W_T$.\n\nBy running this process for many independent paths and averaging the squared differences between the numerical solution $\\hat{X}_T$ and the exact solution $X_T(W_T)$, we obtain a Monte Carlo estimate of the mean-square strong error $\\mathbb{E}\\bigl[\\,|X_T - \\hat{X}_T|^2\\,\\bigr]$.",
            "answer": "```python\nimport numpy as np\nfrom typing import Callable, List, Tuple\n\ndef solve():\n    \"\"\"\n    Designs, implements, and evaluates an adaptive Euler-Maruyama scheme\n    for three SDE test cases.\n    \"\"\"\n\n    # Set a fixed seed for the random number generator for reproducibility.\n    np.random.seed(42)\n\n    def adaptive_sde_solver(\n        a: Callable[[float], float],\n        b: Callable[[float], float],\n        a_prime: Callable[[float], float],\n        b_prime: Callable[[float], float],\n        exact_sol: Callable[[float, float, float, float], float],\n        x0: float,\n        T: float,\n        target_error_sq: float,\n        num_paths: int\n    ) - bool:\n        \"\"\"\n        Solves an SDE using an adaptive Euler-Maruyama scheme and evaluates if the\n        mean-square strong error meets the target tolerance.\n\n        Args:\n            a: Drift coefficient function a(x).\n            b: Diffusion coefficient function b(x).\n            a_prime: Derivative of the drift function a'(x).\n            b_prime: Derivative of the diffusion function b'(x).\n            exact_sol: Function to compute the exact solution at time T given W_T.\n            x0: Initial condition.\n            T: Final time.\n            target_error_sq: The target mean-square strong error (epsilon^2).\n            num_paths: The number of Monte Carlo paths to simulate.\n\n        Returns:\n            A boolean indicating whether the estimated mean-square error is\n            less than or equal to the target tolerance.\n        \"\"\"\n        \n        total_squared_error = 0.0\n        \n        # Controller parameters\n        gamma = 0.9  # Safety factor\n        delta = 1e-12 # Regularization parameter\n\n        for _ in range(num_paths):\n            t = 0.0\n            x_hat = x0\n            w_T = 0.0\n\n            while t  T:\n                # Calculate coefficients and their derivatives at the current state\n                a_val = a(x_hat)\n                b_val = b(x_hat)\n                ap_val = a_prime(x_hat)\n                bp_val = b_prime(x_hat)\n\n                # Calculate the error indicator G(x)\n                g_val = abs(b_val) * (abs(ap_val) + abs(bp_val))\n                \n                # Calculate the adaptive step size h\n                h = (gamma * target_error_sq) / (T * (g_val + delta))\n                \n                # Adjust the last step to land exactly on T\n                if t + h  T:\n                    h = T - t\n                \n                # Generate Wiener increment\n                dw = np.random.normal(0, np.sqrt(h))\n                \n                # Euler-Maruyama step\n                x_hat += a_val * h + b_val * dw\n                \n                # Accumulate Wiener increment for the exact solution\n                w_T += dw\n                \n                # Advance time\n                t += h\n            \n            x_hat_T = x_hat\n            x_exact_T = exact_sol(x0, T, w_T, 0) # Placeholder for mu/alpha\n            \n            total_squared_error += (x_hat_T - x_exact_T)**2\n            \n        mean_squared_error = total_squared_error / num_paths\n        \n        return mean_squared_error = target_error_sq\n\n    # --- Test Cases Definition ---\n    \n    # Common parameters\n    N_PATHS = 4000\n\n    # Case 1: Geometric Brownian Motion\n    test_case_1_params = {\n        'mu': 0.5, 'sigma': 0.7, 'x0': 1.0, 'T': 1.0, \n        'eps2': 5e-3\n    }\n    a1 = lambda x: test_case_1_params['mu'] * x\n    b1 = lambda x: test_case_1_params['sigma'] * x\n    ap1 = lambda x: test_case_1_params['mu']\n    bp1 = lambda x: test_case_1_params['sigma']\n    def exact_gbm(x0, T, Wt, mu, sigma):\n        return x0 * np.exp((mu - 0.5 * sigma**2) * T + sigma * Wt)\n    \n    # Case 2: Geometric Brownian Motion\n    test_case_2_params = {\n        'mu': 1.2, 'sigma': 1.5, 'x0': 0.5, 'T': 1.5,\n        'eps2': 2e-2\n    }\n    a2 = lambda x: test_case_2_params['mu'] * x\n    b2 = lambda x: test_case_2_params['sigma'] * x\n    ap2 = lambda x: test_case_2_params['mu']\n    bp2 = lambda x: test_case_2_params['sigma']\n\n    # Case 3: Constant-coefficient SDE\n    test_case_3_params = {\n        'alpha': 0.3, 'beta': 0.8, 'x0': -1.0, 'T': 2.0,\n        'eps2': 1e-8\n    }\n    a3 = lambda x: test_case_3_params['alpha']\n    b3 = lambda x: test_case_3_params['beta']\n    ap3 = lambda x: 0.0\n    bp3 = lambda x: 0.0\n    def exact_const_coeff(x0, T, Wt, alpha, beta):\n        return x0 + alpha * T + beta * Wt\n        \n    # --- Execution of Test Suite ---\n\n    results = []\n\n    # Run Case 1\n    p1 = test_case_1_params\n    result1 = adaptive_sde_solver(\n        a1, b1, ap1, bp1, \n        lambda x0, T, Wt, _: exact_gbm(x0, T, Wt, p1['mu'], p1['sigma']),\n        p1['x0'], p1['T'], p1['eps2'], N_PATHS\n    )\n    results.append(result1)\n\n    # Run Case 2\n    p2 = test_case_2_params\n    result2 = adaptive_sde_solver(\n        a2, b2, ap2, bp2, \n        lambda x0, T, Wt, _: exact_gbm(x0, T, Wt, p2['mu'], p2['sigma']),\n        p2['x0'], p2['T'], p2['eps2'], N_PATHS\n    )\n    results.append(result2)\n\n    # Run Case 3\n    p3 = test_case_3_params\n    result3 = adaptive_sde_solver(\n        a3, b3, ap3, bp3,\n        lambda x0, T, Wt, _: exact_const_coeff(x0, T, Wt, p3['alpha'], p3['beta']),\n        p3['x0'], p3['T'], p3['eps2'], N_PATHS\n    )\n    results.append(result3)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}