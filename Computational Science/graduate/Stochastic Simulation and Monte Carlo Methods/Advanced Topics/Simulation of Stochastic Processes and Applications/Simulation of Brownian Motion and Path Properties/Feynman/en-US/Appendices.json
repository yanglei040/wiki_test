{
    "hands_on_practices": [
        {
            "introduction": "A defining feature of a Brownian path is its remarkable roughness. While continuous, it is nowhere differentiable, zig-zagging so erratically that its length over any interval is infinite. This exercise allows you to quantitatively explore this roughness by computing the path's $p$-variation . By observing how the sum of the magnitudes of its increments scales with the fineness of the time partition, you will empirically verify a cornerstone of stochastic calculus: that Brownian motion has a finite and non-zero quadratic variation (for $p=2$), but infinite total variation (for $p=1$).",
            "id": "3341139",
            "problem": "Consider a standard Brownian motion $B_t$ on the interval $[0,T]$ with $B_0=0$, defined as a continuous-time stochastic process with independent, stationary Gaussian increments, so that $B_{t+h}-B_t \\sim \\mathcal{N}(0,h)$ for any $t \\ge 0$ and $h0$, and with continuous paths. For a uniform partition of $[0,T]$ into $N$ subintervals, define $t_k = kT/N$ for $k=0,1,\\dots,N$, and the discrete $p$-variation sum\n$$\nS_p(N) \\;=\\; \\sum_{k=0}^{N-1} \\left|B_{t_{k+1}} - B_{t_k}\\right|^p.\n$$\nYou will empirically analyze how $S_p(N)$ behaves as $N$ increases for different values of $p$, by simulating independent sample paths of $B_t$ and aggregating results across paths. Your simulation must use the fundamental construction that each increment $B_{t_{k+1}}-B_{t_k}$ is generated as a Gaussian random variable with mean $0$ and variance $t_{k+1}-t_k = T/N$.\n\nYour program must:\n- Simulate $M$ independent sample paths of $B_t$ on $[0,T]$ for multiple partition sizes $N$, without invoking any shortcut formulas for the behavior of $S_p(N)$.\n- For each given $p$ in the test suite, compute the average $p$-variation $\\overline{S}_p(N)$ across the $M$ paths for each partition size $N$.\n- Estimate the scaling behavior of $\\overline{S}_p(N)$ with respect to $N$ by fitting a simple linear regression of $\\log \\overline{S}_p(N)$ against $\\log N$ and extracting the slope, denoted $b_p$.\n- Use $b_p$ to decide whether the sequence $\\overline{S}_p(N)$ appears to diverge (increases with $N$), stay approximately stable (does not systematically increase or decrease with $N$), or converge (decreases with $N$).\n- For $p=2$, also assess whether $\\overline{S}_2(N)$ at the finest partition is close to the time horizon $T$ to corroborate the stabilization of the quadratic variation.\n\nAll quantities are dimensionless in this task, and angles are not involved.\n\nTest suite specification:\n- Time horizon $T=1$.\n- Number of independent paths $M=600$.\n- Partition sizes $N \\in \\{256, 512, 1024, 2048, 4096\\}$.\n- Three values of $p$: $p_1=1.5$, $p_2=2.0$, $p_3=3.0$.\n\nDecision rules to produce quantifiable outputs:\n- For $p_1=1.5$, declare “divergence” if $b_{p_1}  0.1$; otherwise “no divergence”.\n- For $p_2=2.0$, declare “stable scaling” if $\\lvert b_{p_2} \\rvert  0.1$; additionally, declare “quadratic variation close to $T$” if $\\lvert \\overline{S}_{2}(4096) - T \\rvert  0.15$.\n- For $p_3=3.0$, declare “convergence” if $b_{p_3}  -0.1$; otherwise “no convergence”.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in this order:\n$$\n[\\text{diverge}_{p_1}, \\text{stable}_{p_2}, \\text{qv\\_close}_{p_2}, \\text{converge}_{p_3}],\n$$\nwhere each entry is a boolean that follows the decision rules above, with $\\text{diverge}_{p_1}$ corresponding to $p_1=1.5$, $\\text{stable}_{p_2}$ and $\\text{qv\\_close}_{p_2}$ corresponding to $p_2=2.0$, and $\\text{converge}_{p_3}$ corresponding to $p_3=3.0$. For example, the output might look like:\n$$\n[\\text{True},\\text{True},\\text{True},\\text{True}].\n$$",
            "solution": "The problem statement is valid. It concerns the empirical verification of the path properties of a standard Brownian motion, a well-defined and fundamental stochastic process. All parameters, methodologies, and decision criteria are specified, constituting a self-contained and scientifically sound computational problem in the field of stochastic simulation.\n\nThe core of the problem is to analyze the behavior of the discrete $p$-variation sum,\n$$\nS_p(N) = \\sum_{k=0}^{N-1} \\left|B_{t_{k+1}} - B_{t_k}\\right|^p,\n$$\nfor a standard Brownian motion $B_t$ on the interval $[0, T]$. The time interval is partitioned uniformly into $N$ subintervals of length $\\Delta t = T/N$, with partition points $t_k = k \\Delta t$.\n\n### 1. Simulation Principle\nA standard Brownian motion $B_t$ commencing at $B_0=0$ is characterized by its increments. For any $t \\ge 0$ and $h  0$, the increment $B_{t+h} - B_t$ is a Gaussian random variable with mean $0$ and variance $h$, denoted as $B_{t+h} - B_t \\sim \\mathcal{N}(0, h)$. Furthermore, increments over non-overlapping time intervals are independent.\n\nOur simulation strategy is built directly on this definition. To construct a sample path on the discrete grid $\\{t_k\\}_{k=0}^N$, we generate the sequence of increments $\\Delta B_k = B_{t_{k+1}} - B_{t_k}$ for $k=0, \\dots, N-1$. Since the time step is uniform, $\\Delta t = t_{k+1} - t_k = T/N$, each increment is an independent draw from the same distribution, $\\Delta B_k \\sim \\mathcal{N}(0, T/N)$. The path itself can be reconstructed as $B_{t_k} = \\sum_{i=0}^{k-1} \\Delta B_i$, but for the calculation of $S_p(N)$, only the increments $\\Delta B_k$ are needed.\n\nThe algorithm generates $M$ independent paths, meaning we repeat the process of generating a full set of $N$ increments $M$ times. For each path and each specified value of $p$, we compute $S_p(N)$. The results are then averaged across the $M$ paths to yield a stable estimate, $\\overline{S}_p(N)$, of the expectation $\\mathbb{E}[S_p(N)]$.\n\n### 2. Scaling Analysis\nThe theoretical behavior of $S_p(N)$ as $N \\to \\infty$ is a cornerstone of stochastic calculus. We can predict the scaling of its expectation with $N$. Let $Z_k = \\Delta B_k / \\sqrt{T/N}$ be a standard normal variable, $Z_k \\sim \\mathcal{N}(0,1)$.\nThe expected value of the $p$-variation sum is:\n$$\n\\mathbb{E}[S_p(N)] = \\mathbb{E}\\left[\\sum_{k=0}^{N-1} |\\Delta B_k|^p\\right] = \\sum_{k=0}^{N-1} \\mathbb{E}\\left[\\left|\\sqrt{T/N} Z_k\\right|^p\\right]\n$$\nBy linearity of expectation and since the increments are identically distributed,\n$$\n\\mathbb{E}[S_p(N)] = N \\cdot \\mathbb{E}\\left[| \\sqrt{T/N} Z |^p\\right] = N \\cdot (T/N)^{p/2} \\cdot \\mathbb{E}[|Z|^p] = N^{1-p/2} \\cdot T^{p/2} \\mathbb{E}[|Z|^p]\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$. The term $T^{p/2} \\mathbb{E}[|Z|^p]$ is a constant for a given $p$ and $T$. Therefore, the expected value scales with $N$ as $\\mathbb{E}[S_p(N)] \\propto N^{1-p/2}$.\n\nTaking the natural logarithm of this relationship gives:\n$$\n\\log(\\mathbb{E}[S_p(N)]) \\approx C_p + (1-p/2) \\log N\n$$\nwhere $C_p$ is a constant. This indicates a linear relationship between $\\log \\overline{S}_p(N)$ (our empirical estimate of $\\log \\mathbb{E}[S_p(N)]$) and $\\log N$. The slope of this line is the scaling exponent $b_p = 1 - p/2$.\n\nThe problem requires us to empirically estimate this slope $b_p$ by performing a simple linear regression on the simulated data pairs $(\\log N, \\log \\overline{S}_p(N))$ for the given set of partition sizes $N \\in \\{256, 512, 1024, 2048, 4096\\}$.\n\n### 3. Case-Specific Analysis\nWe apply this framework to the specified values of $p$:\n\n- **Case $p_1 = 1.5$**: The theoretical slope is $b_{1.5} = 1 - 1.5/2 = 1 - 0.75 = 0.25$. Since $b_{1.5}  0$, the sum $S_{1.5}(N)$ is expected to increase with $N$, indicating divergence. The empirical test for divergence is $b_{1.5}  0.1$.\n\n- **Case $p_2 = 2.0$ (Quadratic Variation)**: The theoretical slope is $b_2 = 1 - 2.0/2 = 0$. This implies that $S_2(N)$ should not systematically scale with $N$, but rather stabilize. The empirical test for stability is $|b_2|  0.1$.\nFurthermore, the theoretical value of the quadratic variation is exactly $T$. Let's check the expectation:\n$$\n\\mathbb{E}[S_2(N)] = N^{1-2/2} \\cdot T^{2/2} \\mathbb{E}[|Z|^2] = N^0 \\cdot T^1 \\cdot \\mathbb{E}[Z^2]\n$$\nSince $Z \\sim \\mathcal{N}(0,1)$, its variance is $\\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2 = 1$. With $\\mathbb{E}[Z]=0$, we get $\\mathbb{E}[Z^2]=1$. Thus, $\\mathbb{E}[S_2(N)] = T$. Our simulation average $\\overline{S}_2(N)$ should approximate $T$ for all $N$, and especially for large $N$. The problem asks to verify if $|\\overline{S}_2(4096) - T|  0.15$.\n\n- **Case $p_3 = 3.0$**: The theoretical slope is $b_3 = 1 - 3.0/2 = -0.5$. Since $b_3  0$, the sum $S_3(N)$ is expected to decrease with $N$, indicating convergence to $0$. The empirical test for convergence is $b_3  -0.1$.\n\n### 4. Implementation\nThe program implements this logic. For each $p \\in \\{1.5, 2.0, 3.0\\}$, it iterates through the specified values of $N$. For each $(p, N)$ pair, it generates an $M \\times N$ matrix of random numbers drawn from $\\mathcal{N}(0, T/N)$, representing the increments for $M$ paths. It then computes the $p$-variation for each path and averages the results to get $\\overline{S}_p(N)$. After collecting the averages for all $N$, it calculates the logs and performs a linear regression to find the slope $b_p$. Finally, it applies the decision rules to generate the required boolean outputs.",
            "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Empirically analyzes the p-variation of a simulated Brownian motion.\n    \"\"\"\n    # Test suite specification\n    T = 1.0\n    M = 600\n    N_values = np.array([256, 512, 1024, 2048, 4096])\n    p_values = [1.5, 2.0, 3.0]\n    \n    # Decision thresholds\n    b_diverge_threshold = 0.1\n    b_stable_threshold = 0.1\n    s2_close_threshold = 0.15\n    b_converge_threshold = -0.1\n\n    final_results = []\n\n    # Process p = 1.5\n    p1 = p_values[0]\n    avg_S_p1 = []\n    for N in N_values:\n        dt = T / N\n        std_dev = np.sqrt(dt)\n        # Generate increments for M paths simultaneously\n        increments = np.random.normal(loc=0.0, scale=std_dev, size=(M, N))\n        # Compute p-variation for each path\n        S_p_paths = np.sum(np.abs(increments)**p1, axis=1)\n        # Average across paths\n        avg_S_p1.append(np.mean(S_p_paths))\n\n    log_N = np.log(N_values)\n    log_S_p1 = np.log(avg_S_p1)\n    \n    # Perform linear regression to find the scaling exponent\n    lin_reg_p1 = stats.linregress(log_N, log_S_p1)\n    b_p1 = lin_reg_p1.slope\n    \n    # Apply decision rule for p=1.5\n    diverge_p1 = b_p1  b_diverge_threshold\n    final_results.append(diverge_p1)\n\n    # Process p = 2.0\n    p2 = p_values[1]\n    avg_S_p2 = []\n    for N in N_values:\n        dt = T / N\n        std_dev = np.sqrt(dt)\n        increments = np.random.normal(loc=0.0, scale=std_dev, size=(M, N))\n        # For p=2, abs() is unnecessary but harmless\n        S_p_paths = np.sum(increments**p2, axis=1) \n        avg_S_p2.append(np.mean(S_p_paths))\n\n    log_S_p2 = np.log(avg_S_p2)\n    \n    lin_reg_p2 = stats.linregress(log_N, log_S_p2)\n    b_p2 = lin_reg_p2.slope\n    \n    # Apply decision rules for p=2.0\n    stable_p2 = np.abs(b_p2)  b_stable_threshold\n    final_results.append(stable_p2)\n    \n    S2_at_finest_N = avg_S_p2[-1]\n    qv_close_p2 = np.abs(S2_at_finest_N - T)  s2_close_threshold\n    final_results.append(qv_close_p2)\n    \n    # Process p = 3.0\n    p3 = p_values[2]\n    avg_S_p3 = []\n    for N in N_values:\n        dt = T / N\n        std_dev = np.sqrt(dt)\n        increments = np.random.normal(loc=0.0, scale=std_dev, size=(M, N))\n        S_p_paths = np.sum(np.abs(increments)**p3, axis=1)\n        avg_S_p3.append(np.mean(S_p_paths))\n\n    log_S_p3 = np.log(avg_S_p3)\n    \n    lin_reg_p3 = stats.linregress(log_N, log_S_p3)\n    b_p3 = lin_reg_p3.slope\n    \n    # Apply decision rule for p=3.0\n    converge_p3 = b_p3  b_converge_threshold\n    final_results.append(converge_p3)\n\n    # Print final output in the required format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having explored the intrinsic nature of Brownian paths, we now turn to the practical matter of their numerical simulation. Since we can only approximate a continuous path on a discrete time grid, understanding the resulting error is paramount. This practice delves into two fundamental concepts of numerical accuracy: strong convergence, which measures the pathwise error, and weak convergence, which measures the error in expectation . By simulating and comparing errors for different types of path functionals, you will gain insight into how the Euler-Maruyama scheme performs and why weak convergence is often faster than strong convergence.",
            "id": "3341137",
            "problem": "Consider a standard Brownian motion $B_t$ on the time interval $[0,T]$ with $B_0 = 0$. The Euler–Maruyama (EM) method, here equivalent to simulating Brownian motion with independent Gaussian increments, constructs an approximate path $\\hat{B}_t$ on a coarse time grid with step size $\\Delta t = T/N$ by setting $\\hat{B}_{t_{k+1}} = \\hat{B}_{t_k} + \\Delta B_k$ with $\\Delta B_k \\sim \\mathcal{N}(0,\\Delta t)$ independently, and $t_k = k\\Delta t$ for $k=0,1,\\dots,N$. For path-dependent functionals $F(B_{[0,T]})$, define the strong error as $\\mathbb{E}\\left[\\,|F(\\hat{B}) - F(B)|\\,\\right]$ and the weak error as $\\left|\\,\\mathbb{E}[F(\\hat{B})] - \\mathbb{E}[F(B)]\\,\\right|$, where $B$ denotes the true path and $\\hat{B}$ the EM-approximated path on the coarse grid.\n\nStarting from the fundamental properties of Brownian motion, namely that it has independent increments with $\\Delta B_k \\sim \\mathcal{N}(0,\\Delta t)$, that $\\mathbb{E}[B_t] = 0$ and $\\mathrm{Var}(B_t) = t$, and leveraging the reflection principle and symmetry, consider the following three path functionals:\n- $F_1(B) = \\int_0^T B_t^2\\,dt$,\n- $F_2(B) = \\sup_{0 \\le t \\le T} B_t$,\n- $F_3(B) = \\int_0^T \\mathbf{1}_{\\{B_t  0\\}}\\,dt$.\n\nThe exact expectations for these are given by well-tested facts: $\\mathbb{E}[F_1(B)] = \\int_0^T \\mathbb{E}[B_t^2]\\,dt = \\int_0^T t\\,dt = T^2/2$, $\\mathbb{E}[F_2(B)] = \\sqrt{2T/\\pi}$ (by the reflection principle, $F_2(B)$ has the same distribution as $|B_T|$), and $\\mathbb{E}[F_3(B)] = T/2$ (by symmetry of $B_t$ about zero).\n\nTo quantify and compare strong versus weak convergence of the EM method for these functionals, proceed as follows. Use a high-resolution reference grid of size $N_{\\mathrm{ref}}$ (with step size $\\Delta t_{\\mathrm{ref}} = T/N_{\\mathrm{ref}}$) to define a surrogate of the true path $B$ and compute the reference values $F_i(B)$ by left Riemann sums for $F_1$ and $F_3$, and by the maximum over the fine grid for $F_2$. For each coarse grid size $N$ in a test suite, compute the EM approximation $\\hat{B}$ by subsampling the fine path and evaluate $F_i(\\hat{B})$ analogously on the coarse grid. Estimate the strong errors by Monte Carlo averages of $|F_i(\\hat{B}) - F_i(B)|$ and the weak errors by $|\\mathbb{E}[F_i(\\hat{B})] - \\mathbb{E}[F_i(B)]|$, where the latter uses the exact expectations specified above. For each functional, estimate the empirical convergence rates by fitting a line to the log–log plot of the strong and weak errors versus $\\Delta t$ and reporting the slopes.\n\nYour program must implement the following test suite and specifications:\n- Use $T = 1$, $N_{\\mathrm{ref}} = 4096$, and a Monte Carlo sample size of $M = 1500$ independent paths. Use a fixed random seed of $12345$ for reproducibility.\n- Use the coarse grid sizes $N \\in \\{4,8,16,32,64\\}$, i.e., $\\Delta t \\in \\{T/4, T/8, T/16, T/32, T/64\\}$.\n- Define the reference quantities using the fine grid as:\n  - $F_1(B) \\approx \\sum_{k=0}^{N_{\\mathrm{ref}}-1} B_{t_k}^2 \\,\\Delta t_{\\mathrm{ref}}$,\n  - $F_2(B) \\approx \\max_{0 \\le k \\le N_{\\mathrm{ref}}} B_{t_k}$,\n  - $F_3(B) \\approx \\sum_{k=0}^{N_{\\mathrm{ref}}-1} \\mathbf{1}_{\\{B_{t_k}  0\\}} \\,\\Delta t_{\\mathrm{ref}}$.\n- Define the coarse-grid approximations analogously on $\\{t_k\\}_{k=0}^N$ using left Riemann sums for $F_1$ and $F_3$, and the grid maximum for $F_2$.\n- For each functional $F_i$, compute the arrays of strong errors $E^{\\mathrm{str}}_i(N)$ and weak errors $E^{\\mathrm{weak}}_i(N)$ across the coarse $N$ values. Then compute the empirical strong and weak convergence rates $\\alpha_i^{\\mathrm{str}}$ and $\\alpha_i^{\\mathrm{weak}}$ as the slopes of the least-squares linear fits of $\\log(E^{\\mathrm{str}}_i)$ versus $\\log(\\Delta t)$ and $\\log(E^{\\mathrm{weak}}_i)$ versus $\\log(\\Delta t)$, respectively.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order $[\\alpha_1^{\\mathrm{str}},\\alpha_1^{\\mathrm{weak}},\\alpha_2^{\\mathrm{str}},\\alpha_2^{\\mathrm{weak}},\\alpha_3^{\\mathrm{str}},\\alpha_3^{\\mathrm{weak}}]$, with each entry a float.\n\nDesign for coverage:\n- The functional $F_1$ is a smooth integral functional and represents the general case where weak order is expected to be higher than strong order.\n- The functional $F_2$ tests sensitivity to path extrema between grid points.\n- The functional $F_3$ is discontinuous in the path and tests boundary crossing behavior.\n- The coarse grid sizes include both small and moderately large step sizes to probe convergence trends and boundary behavior.",
            "solution": "The problem requires an empirical investigation of the convergence properties of the Euler-Maruyama (EM) method for simulating functionals of a standard Brownian motion $B_t$. We are asked to estimate the strong and weak convergence rates for three specific path-dependent functionals. The validation confirms that the problem is scientifically sound, well-posed, and all parameters and procedures are clearly defined.\n\nThe core of the methodology is a Monte Carlo simulation. For a robust estimation of errors, particularly the strong error, we require a \"true\" path and a corresponding numerical approximation that are coupled. The problem specifies a standard and computationally efficient way to achieve this: we first simulate a high-resolution path which serves as a sufficiently accurate proxy for the true continuous-time path, and then obtain the coarser approximations by subsampling.\n\n**1. Path Generation and Coupling**\n\nWe begin by generating a set of $M$ independent, high-resolution paths. Each path is a discrete-time approximation of a standard Brownian motion $B_t$ on the interval $[0, T]$ with $B_0 = 0$. We use a fine time grid with $N_{\\mathrm{ref}}$ steps of size $\\Delta t_{\\mathrm{ref}} = T/N_{\\mathrm{ref}}$. A path is constructed by the recurrence relation:\n$$\nB_{t_{k+1}} = B_{t_k} + \\sqrt{\\Delta t_{\\mathrm{ref}}} \\cdot Z_k, \\quad k = 0, 1, \\dots, N_{\\mathrm{ref}}-1\n$$\nwhere $t_k = k \\Delta t_{\\mathrm{ref}}$ and the $Z_k$ are independent random variables drawn from the standard normal distribution $\\mathcal{N}(0, 1)$. These fine-grid paths, denoted $\\{B^{(j)}\\}_{j=1}^M$, will serve as our reference (\"true\") paths.\n\nFor each coarse grid size $N$ (where $N$ is a divisor of $N_{\\mathrm{ref}}$), the corresponding approximate path $\\hat{B}$ is generated by subsampling the fine path $B$. Specifically, if the decimation factor is $D = N_{\\mathrm{ref}}/N$, the coarse path points are taken as $\\hat{B}_{t_k} = B_{t_{k \\cdot D}}$ for $k = 0, 1, \\dots, N$. This procedure is not only computationally efficient but also theoretically sound. The increment of the coarse path, $\\hat{B}_{t_{k+1}} - \\hat{B}_{t_k}$, is the sum of $D$ consecutive increments from the fine path. Due to the properties of the normal distribution, this sum is also a normal random variable:\n$$\n\\sum_{i=kD}^{(k+1)D-1} \\sqrt{\\Delta t_{\\mathrm{ref}}} Z_i \\sim \\mathcal{N}\\left(0, \\sum_{i=kD}^{(k+1)D-1} (\\sqrt{\\Delta t_{\\mathrm{ref}}})^2\\right) = \\mathcal{N}(0, D \\cdot \\Delta t_{\\mathrm{ref}})\n$$\nSince $D \\cdot \\Delta t_{\\mathrm{ref}} = (N_{\\mathrm{ref}}/N) \\cdot (T/N_{\\mathrm{ref}}) = T/N = \\Delta t$, the coarse path increments $\\Delta \\hat{B}_k$ are distributed as $\\mathcal{N}(0, \\Delta t)$, exactly as prescribed by the Euler-Maruyama method on the coarse grid. This coupling ensures that the difference $|F(\\hat{B}) - F(B)|$ measures the error due to the coarser time discretization for the same underlying noise realization.\n\n**2. Functional and Error Estimation**\n\nFor each of the $M$ generated reference paths $B^{(j)}$ and their corresponding coarse-grid subsamples $\\hat{B}^{(j)}$, we compute the values of the three functionals:\n- $F_1(B) = \\int_0^T B_t^2\\,dt$\n- $F_2(B) = \\sup_{0 \\le t \\le T} B_t$\n- $F_3(B) = \\int_0^T \\mathbf{1}_{\\{B_t  0\\}}\\,dt$\n\nThe continuous operations (integral, supremum) are approximated on the discrete grids. As specified, the integrals for $F_1$ and $F_3$ are approximated using left Riemann sums, and the supremum for $F_2$ is approximated by the maximum value on the grid points. For a generic path $X$ on a grid with step size $\\delta t$ and $n$ intervals, the approximations are:\n- $F_1(X) \\approx \\sum_{k=0}^{n-1} X_{t_k}^2 \\cdot \\delta t$\n- $F_2(X) \\approx \\max_{0 \\le k \\le n} X_{t_k}$\n- $F_3(X) \\approx \\sum_{k=0}^{n-1} \\mathbf{1}_{\\{X_{t_k}  0\\}} \\cdot \\delta t$\n\nWith the functional values computed for both the reference path $B$ and the coarse approximation $\\hat{B}$ for each of the $M$ samples, we estimate the strong and weak errors.\n\nThe **strong error** for a given coarse grid size $N$ is estimated by the Monte Carlo average of the absolute differences:\n$$\nE_i^{\\mathrm{str}}(N) \\approx \\frac{1}{M} \\sum_{j=1}^{M} | F_i(\\hat{B}^{(j)}) - F_i(B^{(j)}) |\n$$\n\nThe **weak error** is estimated by comparing the Monte Carlo average of the functional on the coarse grid with its exact theoretical expectation:\n$$\nE_i^{\\mathrm{weak}}(N) = \\left| \\left( \\frac{1}{M} \\sum_{j=1}^{M} F_i(\\hat{B}^{(j)}) \\right) - \\mathbb{E}[F_i(B)] \\right|\n$$\nThe problem provides the exact expectations: $\\mathbb{E}[F_1(B)] = T^2/2$, $\\mathbb{E}[F_2(B)] = \\sqrt{2T/\\pi}$, and $\\mathbb{E}[F_3(B)] = T/2$.\n\n**3. Convergence Rate Calculation**\n\nThe rate of convergence $\\alpha$ describes how the error $E$ scales with the time step $\\Delta t$, typically following a power law $E(\\Delta t) \\approx C (\\Delta t)^\\alpha$ for some constant $C$ as $\\Delta t \\to 0$. To estimate $\\alpha$, we can transform this relationship into a linear one by taking the logarithm:\n$$\n\\log(E(\\Delta t)) \\approx \\log(C) + \\alpha \\log(\\Delta t)\n$$\nThis equation is of the form $y = m x + c$, where $y = \\log(E)$, $x = \\log(\\Delta t)$, and the slope $m$ is the convergence rate $\\alpha$.\n\nWe compute the arrays of strong and weak errors for each functional across the specified suite of coarse step sizes $\\Delta t = T/N$. Then, for each of the six error arrays, we perform an ordinary least-squares linear regression on the $(\\log(\\Delta t), \\log(E))$ data points. The slope of the resulting fit provides the empirical estimate for the convergence rate $\\alpha$. The implementation uses `numpy.polyfit` for this purpose. The final output will consist of the six estimated slopes: $\\{\\alpha_1^{\\mathrm{str}}, \\alpha_1^{\\mathrm{weak}}, \\alpha_2^{\\mathrm{str}}, \\alpha_2^{\\mathrm{weak}}, \\alpha_3^{\\mathrm{str}}, \\alpha_3^{\\mathrm{weak}}\\}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Estimates the strong and weak convergence rates of the Euler-Maruyama method\n    for three different Brownian path functionals.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    T = 1.0\n    N_ref = 4096\n    M = 1500\n    seed = 12345\n    N_coarse_list = [4, 8, 16, 32, 64]\n\n    # Initialize the random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # --- Step 1: Generate high-resolution reference paths ---\n    dt_ref = T / N_ref\n    # Generate Gaussian increments for M paths with N_ref steps.\n    increments = rng.normal(0.0, np.sqrt(dt_ref), size=(M, N_ref))\n    # Create paths by cumulative summation of increments.\n    # Paths have N_ref + 1 points, from t=0 to t=T.\n    paths_ref = np.zeros((M, N_ref + 1))\n    paths_ref[:, 1:] = np.cumsum(increments, axis=1)\n\n    # --- Step 2: Compute functional values on reference paths ---\n    # These serve as the \"true\" values for error calculation.\n    # F1(B) = integral of B_t^2 dt\n    F1_ref = np.sum(paths_ref[:, :-1]**2, axis=1) * dt_ref\n    # F2(B) = sup_{0=t=T} B_t\n    F2_ref = np.max(paths_ref, axis=1)\n    # F3(B) = integral of 1_{B_t  0} dt\n    F3_ref = np.sum(paths_ref[:, :-1]  0, axis=1) * dt_ref\n\n    # --- Step 3: Define exact expectations for weak error calculation ---\n    E_F1_exact = T**2 / 2.0\n    E_F2_exact = np.sqrt(2 * T / np.pi)\n    E_F3_exact = T / 2.0\n\n    # Dictionaries to store errors for each functional\n    strong_errors = {1: [], 2: [], 3: []}\n    weak_errors = {1: [], 2: [], 3: []}\n    delta_t_vals = []\n\n    # --- Step 4: Loop over coarse grid sizes to compute errors ---\n    for N_coarse in N_coarse_list:\n        dt_coarse = T / N_coarse\n        delta_t_vals.append(dt_coarse)\n\n        # Generate coarse paths by subsampling the fine paths.\n        decimation_factor = N_ref // N_coarse\n        paths_coarse = paths_ref[:, ::decimation_factor]\n        \n        # Compute functional values on the coarse paths.\n        F1_coarse = np.sum(paths_coarse[:, :-1]**2, axis=1) * dt_coarse\n        F2_coarse = np.max(paths_coarse, axis=1)\n        F3_coarse = np.sum(paths_coarse[:, :-1]  0, axis=1) * dt_coarse\n\n        # Estimate strong error (mean absolute difference).\n        strong_errors[1].append(np.mean(np.abs(F1_coarse - F1_ref)))\n        strong_errors[2].append(np.mean(np.abs(F2_coarse - F2_ref)))\n        strong_errors[3].append(np.mean(np.abs(F3_coarse - F3_ref)))\n\n        # Estimate weak error (absolute difference of means).\n        weak_errors[1].append(np.abs(np.mean(F1_coarse) - E_F1_exact))\n        weak_errors[2].append(np.abs(np.mean(F2_coarse) - E_F2_exact))\n        weak_errors[3].append(np.abs(np.mean(F3_coarse) - E_F3_exact))\n\n    # --- Step 5: Compute convergence rates via log-log linear regression ---\n    results = []\n    log_delta_t = np.log(np.array(delta_t_vals))\n\n    for i in range(1, 4):\n        # Strong rate\n        log_E_strong = np.log(np.array(strong_errors[i]))\n        # np.polyfit returns [slope, intercept] for degree 1.\n        slope_strong = np.polyfit(log_delta_t, log_E_strong, 1)[0]\n        results.append(slope_strong)\n\n        # Weak rate\n        log_E_weak = np.log(np.array(weak_errors[i]))\n        slope_weak = np.polyfit(log_delta_t, log_E_weak, 1)[0]\n        results.append(slope_weak)\n\n    # --- Step 6: Print the final results in the specified format ---\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The final practice elevates our simulation toolkit by introducing a powerful method for improving the efficiency of Monte Carlo estimation: variance reduction. A naive Monte Carlo simulation may require a vast number of samples to achieve a precise estimate. This exercise guides you through the process of constructing a control variate, a sophisticated technique that uses an auxiliary variable with a known expectation to reduce the variance of the estimator . You will apply this to estimate the expected maximum of a Brownian path, leveraging the analytical properties of the Brownian bridge to achieve a more precise result with less computational effort.",
            "id": "3341093",
            "problem": "Consider a one-dimensional Brownian motion path with diffusion coefficient $\\sigma  0$ starting at $0$, defined by $X_t = \\sigma W_t$ for $t \\in [0,T]$, where $W_t$ is a standard Brownian motion. Let the path maximum be $S_T = \\sup_{0 \\le t \\le T} X_t$. Your task is to construct a control variate for estimating the expectation $\\mathbb{E}[S_T]$ that is based on the Brownian bridge conditional structure, and to compute the optimal control coefficient by regression.\n\nFundamental base and definitions:\n- A standard Brownian motion $W_t$ has independent, stationary Gaussian increments with $W_0 = 0$ and $W_t - W_s \\sim \\mathcal{N}(0, t-s)$ for $0 \\le s  t$.\n- A Brownian bridge on $[0,T]$ conditioned on endpoints $X_0 = 0$ and $X_T = x$ is the Gaussian process $(X_t \\,|\\, X_T = x)$ whose trajectories are continuous and whose distribution can be derived from the reflection principle and conditioning on $X_T$.\n- The reflection principle provides the distribution of the running maximum of Brownian motion and underlies the conditioning formula for the maximum given the endpoint.\n- The unconditional distribution of $S_T$ for driftless Brownian motion satisfies $S_T \\stackrel{d}{=} |X_T|$; consequently $\\mathbb{E}[S_T] = \\sigma \\sqrt{\\frac{2T}{\\pi}}$.\n- Ordinary Least Squares (OLS) regression is used to estimate the optimal control coefficient by minimizing sample mean-squared error.\n\nProblem requirements:\n1. Simulate $N$ independent discrete-time Brownian motion paths on $[0,T]$ using an equidistant grid with $n$ steps, $\\Delta t = T/n$, and Gaussian increments $\\Delta X_k \\sim \\mathcal{N}(0, \\sigma^2 \\Delta t)$, $k = 1,\\dots,n$. Form the discretized path maximum $S_T^{(d)} = \\max\\{0, X_{\\Delta t}, X_{2\\Delta t}, \\dots, X_T\\}$ as an approximation to $S_T$.\n2. Using the conditional Brownian bridge law over $[0,T]$ given $X_T = x$, derive from first principles (reflection principle and conditioning) an analytic expression for the conditional expectation $\\mathbb{E}[S_T \\mid X_T = x]$ as a function of $x$, $\\sigma$, and $T$. This quantity is the control variate $C(x)$ to be used per path by plugging in the realized endpoint $x = X_T$.\n3. Prove that $\\mathbb{E}[C(X_T)] = \\mathbb{E}[S_T]$, and use the known formula $\\mathbb{E}[S_T] = \\sigma \\sqrt{\\frac{2T}{\\pi}}$ for centering the control variate.\n4. Compute the optimal control coefficient $\\beta^\\star$ via OLS regression (population-optimal slope) as $\\beta^\\star = \\frac{\\operatorname{Cov}(S_T^{(d)}, C(X_T))}{\\operatorname{Var}(C(X_T))}$ using the same Monte Carlo sample for both $S_T^{(d)}$ and $C(X_T)$.\n5. Form the variance-reduced estimator of $\\mathbb{E}[S_T]$ using the control variate:\n$$\n\\widehat{\\mu}_{\\mathrm{cv}} = \\overline{S_T^{(d)}} - \\widehat{\\beta}\\,\\big(\\overline{C(X_T)} - \\sigma \\sqrt{\\tfrac{2T}{\\pi}}\\big),\n$$\nwhere the overline denotes the sample mean and $\\widehat{\\beta}$ is the OLS estimate of $\\beta^\\star$ from the sample.\n6. Implement the above in a complete program. For each test case, return the triple of floats $[\\overline{S_T^{(d)}}, \\widehat{\\mu}_{\\mathrm{cv}}, \\widehat{\\beta}]$.\n\nTest suite:\n- Use the following parameter sets $(T, \\sigma, N, n, \\text{seed})$:\n  - Case A (general): $(T, \\sigma, N, n, \\text{seed}) = (1.0, 1.0, 8000, 1024, 12345)$.\n  - Case B (small horizon): $(T, \\sigma, N, n, \\text{seed}) = (0.1, 1.0, 12000, 512, 23456)$.\n  - Case C (high volatility): $(T, \\sigma, N, n, \\text{seed}) = (1.5, 1.8, 6000, 1024, 34567)$.\n  - Case D (coarse grid boundary): $(T, \\sigma, N, n, \\text{seed}) = (1.0, 1.0, 10000, 1, 45678)$.\n  - Case E (low volatility): $(T, \\sigma, N, n, \\text{seed}) = (1.0, 0.05, 8000, 1024, 56789)$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of three floats in the order $[\\overline{S_T^{(d)}}, \\widehat{\\mu}_{\\mathrm{cv}}, \\widehat{\\beta}]$, and the overall output must be a list of these lists, for example:\n$[[\\text{A}_1,\\text{A}_2,\\text{A}_3],[\\text{B}_1,\\text{B}_2,\\text{B}_3],\\dots]$.\n- No physical units are involved; all quantities are dimensionless real numbers.",
            "solution": "The problem is valid and requires the construction of a control variate for the Monte Carlo estimation of the expected maximum of a one-dimensional Brownian motion, $\\mathbb{E}[S_T]$. The solution involves deriving the control variate from first principles, implementing a Monte Carlo simulation to compute the optimal control coefficient, and applying the variance reduction technique.\n\n### Step 1: Derivation of the Control Variate\n\nThe problem asks for an analytic expression for the conditional expectation $C(x) = \\mathbb{E}[S_T \\mid X_T = x]$. The process is $X_t = \\sigma W_t$, where $W_t$ is a standard Brownian motion. The path maximum is $S_T = \\sup_{0 \\le t \\le T} X_t = \\sigma \\sup_{0 \\le t \\le T} W_t = \\sigma S'_T$, where $S'_T = \\sup_{0 \\le t \\le T} W_t$.\n\nThe conditional expectation can be scaled as follows:\n$$\n\\mathbb{E}[S_T \\mid X_T = x] = \\mathbb{E}[\\sigma S'_T \\mid \\sigma W_T = x] = \\sigma \\mathbb{E}[S'_T \\mid W_T = x/\\sigma]\n$$\nLet $y = x/\\sigma$. Our task reduces to finding $\\mathbb{E}[S'_T \\mid W_T = y]$.\n\nAccording to the reflection principle for Brownian motion, the conditional probability distribution of the maximum $S'_T$ given the endpoint $W_T=y$ is known. The conditional survival function is given by:\n$$\nP(S'_T \\ge a \\mid W_T = y) = e^{-2a(a-y)/T} \\quad \\text{for } a \\ge \\max(0, y)\n$$\nFor a non-negative random variable $Z$, its expectation can be computed by integrating its survival function: $\\mathbb{E}[Z] = \\int_0^\\infty P(Z  z) dz$. The conditional distribution of $S'_T$ is supported on $[\\max(0,y), \\infty)$.\n\nWe analyze two cases based on the sign of $y$.\n\nCase 1: $y  0$.\nThe support of $S'_T$ is $[y, \\infty)$. The expectation is given by:\n$$\n\\mathbb{E}[S'_T \\mid W_T = y] = y + \\int_y^\\infty P(S'_T  a \\mid W_T = y) da = y + \\int_y^\\infty e^{-2a(a-y)/T} da\n$$\nTo evaluate the integral, we complete the square in the exponent:\n$$-2a(a-y)/T = -\\frac{2}{T}(a^2 - ay) = -\\frac{2}{T}\\left(\\left(a - \\frac{y}{2}\\right)^2 - \\frac{y^2}{4}\\right) = -\\frac{2}{T}\\left(a - \\frac{y}{2}\\right)^2 + \\frac{y^2}{2T}$$\nThe integral becomes:\n$$\n\\int_y^\\infty e^{-2a(a-y)/T} da = e^{y^2/(2T)} \\int_y^\\infty e^{-\\frac{2}{T}(a - y/2)^2} da\n$$\nLet $u = (a - y/2)\\sqrt{2/T}$. Then $da = \\sqrt{T/2}du$. The lower limit of integration becomes $u_0 = (y - y/2)\\sqrt{2/T} = y/\\sqrt{2T}$. The integral transforms to:\n$$\ne^{y^2/(2T)} \\sqrt{T/2} \\int_{y/\\sqrt{2T}}^\\infty e^{-u^2} du = e^{y^2/(2T)} \\sqrt{T/2} \\left(\\frac{\\sqrt{\\pi}}{2} \\operatorname{erfc}\\left(\\frac{y}{\\sqrt{2T}}\\right)\\right) = \\sqrt{\\frac{T\\pi}{8}} e^{y^2/(2T)} \\operatorname{erfc}\\left(\\frac{y}{\\sqrt{2T}}\\right)\n$$\nwhere $\\operatorname{erfc}(z) = \\frac{2}{\\sqrt{\\pi}}\\int_z^\\infty e^{-u^2}du$ is the complementary error function.\nSo, for $y0$:\n$$\n\\mathbb{E}[S'_T \\mid W_T = y] = y + \\sqrt{\\frac{T\\pi}{8}} e^{y^2/(2T)} \\operatorname{erfc}\\left(\\frac{y}{\\sqrt{2T}}\\right)\n$$\n\nCase 2: $y \\le 0$.\nThe support of $S'_T$ is $[0, \\infty)$. The expectation is:\n$$\n\\mathbb{E}[S'_T \\mid W_T = y] = \\int_0^\\infty P(S'_T  a \\mid W_T=y) da = \\int_0^\\infty e^{-2a(a-y)/T} da\n$$\nUsing the same change of variable, the lower limit becomes $u_0 = (0 - y/2)\\sqrt{2/T} = -y/\\sqrt{2T}$.\n$$\n\\mathbb{E}[S'_T \\mid W_T = y] = \\sqrt{\\frac{T\\pi}{8}} e^{y^2/(2T)} \\operatorname{erfc}\\left(\\frac{-y}{\\sqrt{2T}}\\right)\n$$\nCombining both cases, and substituting back $y=x/\\sigma$, we obtain the control variate $C(x)$:\n$$\nC(x) = \\sigma \\left( \\max(0, \\frac{x}{\\sigma}) + \\sqrt{\\frac{T\\pi}{8}} e^{\\frac{(x/\\sigma)^2}{2T}} \\operatorname{erfc}\\left(\\frac{|x/\\sigma|}{\\sqrt{2T}}\\right) \\right)\n$$\n\n### Step 2: Centering the Control Variate\n\nThe problem asks to prove that $\\mathbb{E}[C(X_T)] = \\mathbb{E}[S_T]$. The control variate $C(x)$ was constructed to be $\\mathbb{E}[S_T \\mid X_T=x]$. By the law of total expectation (tower property), the expectation of the conditional expectation is the unconditional expectation:\n$$\n\\mathbb{E}[C(X_T)] = \\mathbb{E}[\\mathbb{E}[S_T \\mid X_T]] = \\mathbb{E}[S_T]\n$$\nThis proves the required property. The known analytical value for the unconditional mean is $\\mathbb{E}[S_T] = \\sigma \\sqrt{2T/\\pi}$, which we use for centering the control variate.\n\n### Step 3: Monte Carlo Simulation and Estimation\n\nThe procedure is as follows:\n1.  Simulate $N$ independent paths of the Brownian motion $X_t$ on a discrete time grid $t_k = k\\Delta t$ for $k=0, \\dots, n$, where $\\Delta t=T/n$. Each path is constructed by summing up $n$ independent Gaussian increments $\\Delta X_k \\sim \\mathcal{N}(0, \\sigma^2 \\Delta t)$.\n$$\nX_{t_k} = \\sum_{j=1}^k \\Delta X_j\n$$\n2.  For each simulated path $i \\in \\{1, \\dots, N\\}$, we compute the discretized maximum $S_T^{(d, i)} = \\max\\{0, X_{t_1}^{(i)}, \\dots, X_{t_n}^{(i)}\\}$ and the terminal value $X_T^{(i)} = X_{t_n}^{(i)}$.\n3.  Using the terminal values, we compute the control variate values $C_i = C(X_T^{(i)})$ for each path using the formula derived in Step 1.\n4.  The optimal control coefficient $\\beta^\\star$ is the population OLS slope, estimated from the sample data:\n$$\n\\widehat{\\beta} = \\frac{\\widehat{\\operatorname{Cov}}(S_T^{(d)}, C)}{\\widehat{\\operatorname{Var}}(C)} = \\frac{\\sum_{i=1}^N (S_T^{(d,i)} - \\overline{S_T^{(d)}})(C_i - \\bar{C})}{\\sum_{i=1}^N (C_i - \\bar{C})^2}\n$$\nwhere overlines denote sample means.\n5.  The control variate a-reduced estimator for $\\mathbb{E}[S_T]$ is:\n$$\n\\widehat{\\mu}_{\\mathrm{cv}} = \\overline{S_T^{(d)}} - \\widehat{\\beta}\\,\\left(\\overline{C} - \\sigma \\sqrt{\\frac{2T}{\\pi}}\\right)\n$$\nThis estimator corrects the crude Monte Carlo estimate $\\overline{S_T^{(d)}}$ based on the difference between the sample mean of the control variate and its known true mean.\n\n### Step 4: Algorithmic Implementation\n\nThe implementation translates the above steps into code. For each test case defined by parameters $(T, \\sigma, N, n, \\text{seed})$:\n1.  Set the random seed for reproducibility.\n2.  Generate an $N \\times n$ matrix of random increments from $\\mathcal{N}(0, \\sigma^2 T/n)$.\n3.  Compute the cumulative sum along the time axis (axis 1) to obtain $N$ paths.\n4.  Compute $S_T^{(d)}$ by taking the maximum over each path. A column of zeros is concatenated to the paths matrix before taking the maximum to account for the starting point $X_0=0$.\n5.  Extract the terminal values $X_T$.\n6.  Vectorize the computation of the control variate $C(X_T)$ using the derived formula, leveraging NumPy for array operations and `scipy.special.erfc` for the complementary error function.\n7.  Calculate the sample means, covariance, and variance to compute $\\widehat{\\beta}$.\n8.  Finally, compute the crude MC estimate $\\overline{S_T^{(d)}}$ and the control variate estimate $\\widehat{\\mu}_{\\mathrm{cv}}$.\n9.  The results $[\\overline{S_T^{(d)}}, \\widehat{\\mu}_{\\mathrm{cv}}, \\widehat{\\beta}]$ are collected for all test cases and printed in the specified format. The special case of $n=1$ is handled naturally by this procedure.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import erfc\n\ndef solve():\n    \"\"\"\n    Computes crude and control variate Monte Carlo estimates for the expected maximum of a Brownian motion.\n    \"\"\"\n    test_cases = [\n        # (T, sigma, N, n, seed)\n        (1.0, 1.0, 8000, 1024, 12345),   # Case A\n        (0.1, 1.0, 12000, 512, 23456),   # Case B\n        (1.5, 1.8, 6000, 1024, 34567),   # Case C\n        (1.0, 1.0, 10000, 1, 45678),     # Case D\n        (1.0, 0.05, 8000, 1024, 56789),  # Case E\n    ]\n\n    all_results = []\n\n    for T, sigma, N, n, seed in test_cases:\n        # Set the seed for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # 1. Simulate N independent discrete-time Brownian motion paths\n        delta_t = T / n\n        # Increments are N(0, sigma^2 * delta_t)\n        increments = rng.normal(loc=0.0, scale=sigma * np.sqrt(delta_t), size=(N, n))\n        \n        # Cumsum to get the paths. Paths start at 0.\n        paths = np.cumsum(increments, axis=1)\n        \n        # S_T^(d) is the maximum of the path including the starting point 0\n        paths_with_zero = np.concatenate((np.zeros((N, 1)), paths), axis=1)\n        s_T_d = np.max(paths_with_zero, axis=1)\n\n        # The endpoint of the path is X_T\n        x_T = paths[:, -1]\n\n        # 2. Derive and compute the control variate C(x) = E[S_T | X_T = x]\n        def control_variate_func(x, t_param, sigma_param):\n            y = x / sigma_param\n            \n            common_term = np.sqrt(t_param * np.pi / 8.0) * \\\n                          np.exp(y**2 / (2.0 * t_param)) * \\\n                          erfc(np.abs(y) / np.sqrt(2.0 * t_param))\n            \n            # Use np.where for vectorized conditional logic\n            cv_unscaled = np.where(y  0, y + common_term, common_term)\n            \n            return sigma_param * cv_unscaled\n\n        C = control_variate_func(x_T, T, sigma)\n\n        # 3. Use known formula for centering\n        mu_C_analytic = sigma * np.sqrt(2.0 * T / np.pi)\n\n        # 4. Compute optimal control coefficient beta_star via OLS\n        # We use sample statistics (ddof=1)\n        # beta_hat = Cov(S, C) / Var(C)\n        cov_matrix = np.cov(s_T_d, C, ddof=1)\n        beta_hat = cov_matrix[0, 1] / cov_matrix[1, 1]\n\n        # 5. Form the variance-reduced estimator\n        s_T_d_mean = np.mean(s_T_d)\n        C_mean = np.mean(C)\n        \n        mu_cv_hat = s_T_d_mean - beta_hat * (C_mean - mu_C_analytic)\n        \n        # 6. Store the triple of floats\n        all_results.append([s_T_d_mean, mu_cv_hat, beta_hat])\n\n    # Format the output as specified\n    # e.g., [[A1,A2,A3],[B1,B2,B3],...]\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]}]\" for r in all_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}