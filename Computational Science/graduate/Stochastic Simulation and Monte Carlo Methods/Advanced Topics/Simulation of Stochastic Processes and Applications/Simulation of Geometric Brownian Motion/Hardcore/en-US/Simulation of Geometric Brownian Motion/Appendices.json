{
    "hands_on_practices": [
        {
            "introduction": "Before applying simulation to complex problems, it's crucial to ensure our basic simulation engine is correct. A powerful way to do this is to verify that the simulated process respects the fundamental theoretical properties of the mathematical model. This practice focuses on the scaling invariance of Brownian motion, where the process $W_{ct}$ is distributionally equivalent to $\\sqrt{c} W_t$, and explores how this property translates to the Geometric Brownian Motion it drives. By implementing and testing this principle, you will solidify your understanding of the exact simulation formula for GBM and learn to use statistical tests to validate a simulation against analytical benchmarks.",
            "id": "3342027",
            "problem": "Consider a geometric Brownian motion defined by the stochastic differential equation (SDE) $dS_t = \\mu S_t \\, dt + \\sigma S_t \\, dW_t$ where $W_t$ is a Wiener process (Brownian motion), $S_t$ is the asset price at time $t$, $\\mu$ is the drift, and $\\sigma$ is the volatility. A fundamental property of Brownian motion is the scaling invariance in distribution: for any $c  0$, $W_{ct} \\stackrel{d}{=} \\sqrt{c}\\, W_t$. Starting from these fundamental definitions, you will verify the scaling invariance of the induced geometric Brownian motion at rescaled time $ct$ and assess the consistency of Monte Carlo (MC) estimators for expectations of functionals of the form $E[g(S_{ct})]$, where $g(x) = x^p$ for a given real exponent $p$. You must not use any shortcut formulas provided in this problem statement; instead, rely on these fundamental bases and derive anything needed in your solution.\n\nYour program must:\n- Implement two simulation routes to estimate $E[g(S_{ct})]$:\n  1. A direct route using samples of $W_{ct}$ as Gaussian random variables with variance $ct$ to simulate $S_{ct}$.\n  2. A scaling route using samples of $W_t$ as Gaussian random variables with variance $t$ and then forming $\\sqrt{c}\\, W_t$ to simulate $S_{ct}$.\n- Use $g(x) = x^p$ with a specified $p$ for each test case.\n- For each test case, compute:\n  - A boolean indicating whether the two MC estimators (from the direct route and the scaling route) agree within a prescribed tolerance defined by a three-standard-error criterion, i.e., whether $|\\hat{m}_1 - \\hat{m}_2| \\leq 3 \\sqrt{\\mathrm{SE}_1^2 + \\mathrm{SE}_2^2}$, where $\\hat{m}_i$ is the sample mean and $\\mathrm{SE}_i$ is the estimated standard error of route $i$.\n  - A boolean indicating whether the scaling-route MC estimator is consistent with the analytically derived benchmark $E[g(S_{ct})]$ (to be derived in your solution) under the same three-standard-error criterion. In the boundary case $c=0$, note that $S_{0} = S_0$ and $W_0 = 0$, so both routes are deterministic and should trivially pass both tests.\n\nYour program must use a fixed random seed $123456$ and a common sample size $N = 100000$ for all test cases. No physical units or angle units are involved. All outputs must be numerical booleans.\n\nTest suite:\n- Case A (happy path): $S_0 = 100$, $\\mu = 0.05$, $\\sigma = 0.2$, $t = 1.0$, $c = 1.0$, $p = 1.0$.\n- Case B (moderate scaling, negative drift, higher volatility): $S_0 = 50$, $\\mu = -0.02$, $\\sigma = 0.5$, $t = 1.0$, $c = 0.5$, $p = 2.0$.\n- Case C (large scaling to unit horizon with zero drift): $S_0 = 120$, $\\mu = 0.0$, $\\sigma = 1.0$, $t = 0.5$, $c = 2.0$, $p = 1.0$.\n- Case D (boundary case): $S_0 = 80$, $\\mu = 0.03$, $\\sigma = 0.4$, $t = 1.0$, $c = 0.0$, $p = 1.5$.\n- Case E (near-zero horizon): $S_0 = 100$, $\\mu = 0.01$, $\\sigma = 0.6$, $t = 1.0$, $c = 10^{-6}$, $p = 1.0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to a test case and is itself a two-element list of booleans in the order described above. For example: $[[b_{A,1}, b_{A,2}], [b_{B,1}, b_{B,2}], \\ldots]$ where $b_{X,1}$ is the scaling-invariance agreement boolean for case $X$ and $b_{X,2}$ is the consistency boolean for case $X$.",
            "solution": "The problem requires a validation of the scaling invariance property of geometric Brownian motion (GBM) through Monte Carlo simulation and a comparison with an analytically derived expectation. The solution proceeds in three stages: first, the derivation of the exact solution to the GBM stochastic differential equation (SDE); second, the derivation of the analytical expectation for the specified functional; and third, the design of the numerical simulation and statistical tests.\n\n**1. The Solution to the Geometric Brownian Motion SDE**\n\nThe process $S_t$ is defined by the SDE:\n$$dS_t = \\mu S_t \\, dt + \\sigma S_t \\, dW_t$$\nwhere $\\mu$ is the drift, $\\sigma$ is the volatility, and $W_t$ is a standard Wiener process. To find the explicit solution for $S_t$, we apply Itô's lemma to the function $f(S_t) = \\ln(S_t)$. The partial derivatives of $f$ are:\n$$ \\frac{\\partial f}{\\partial S} = \\frac{1}{S_t}, \\quad \\frac{\\partial^2 f}{\\partial S^2} = -\\frac{1}{S_t^2}, \\quad \\frac{\\partial f}{\\partial t} = 0 $$\nAccording to Itô's lemma, the differential of $f(S_t)$ is given by:\n$$ d(\\ln S_t) = \\left( \\frac{\\partial f}{\\partial t} + \\mu S_t \\frac{\\partial f}{\\partial S} + \\frac{1}{2} \\sigma^2 S_t^2 \\frac{\\partial^2 f}{\\partial S^2} \\right) dt + \\sigma S_t \\frac{\\partial f}{\\partial S} dW_t $$\nSubstituting the derivatives, we obtain:\n$$ d(\\ln S_t) = \\left( 0 + \\mu S_t \\cdot \\frac{1}{S_t} + \\frac{1}{2} \\sigma^2 S_t^2 \\cdot \\left(-\\frac{1}{S_t^2}\\right) \\right) dt + \\sigma S_t \\cdot \\frac{1}{S_t} dW_t $$\n$$ d(\\ln S_t) = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) dt + \\sigma dW_t $$\nIntegrating both sides from time $0$ to $t$:\n$$ \\int_0^t d(\\ln S_u) = \\int_0^t \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) du + \\int_0^t \\sigma dW_u $$\n$$ \\ln S_t - \\ln S_0 = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma (W_t - W_0) $$\nGiven that $W_0 = 0$, we can rearrange to solve for $S_t$:\n$$ S_t = S_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right) $$\nThis is the well-known solution for the asset price under GBM. To find the price at a rescaled time $ct$, we simply substitute $t$ with $ct$:\n$$ S_{ct} = S_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)ct + \\sigma W_{ct} \\right) $$\n\n**2. Justification of Simulation Routes and Scaling Invariance**\n\nThe problem requires simulating $S_{ct}$ via two routes, leveraging the scaling property of the Wiener process, $W_{ct} \\stackrel{d}{=} \\sqrt{c}\\, W_t$ for $c0$, where $\\stackrel{d}{=}$ denotes equality in distribution.\n\nA Wiener process increment $W_\\tau$ over a time interval $\\tau$ is a normally distributed random variable with mean $0$ and variance $\\tau$. That is, $W_\\tau \\sim N(0, \\tau)$. This can be expressed as $W_\\tau = \\sqrt{\\tau} Z$, where $Z \\sim N(0, 1)$ is a standard normal random variable.\n\n*   **Route 1 (Direct)**: This route simulates $S_{ct}$ using samples of $W_{ct}$. Following the property above, $W_{ct} \\sim N(0, ct)$. We generate $N$ samples from a standard normal distribution, $\\{Z_i\\}_{i=1}^N$, and construct the stochastic term as $\\sigma W_{ct}^{(i)} = \\sigma \\sqrt{ct} Z_i$.\n\n*   **Route 2 (Scaling)**: This route first considers $W_t$, where $W_t \\sim N(0, t)$, and then applies the scaling property. A sample of $W_t$ is constructed as $W_t^{(i)} = \\sqrt{t} Z_i$. The scaled process is then $\\sqrt{c} W_t^{(i)} = \\sqrt{c} (\\sqrt{t} Z_i) = \\sqrt{ct} Z_i$. The stochastic term for the simulation of $S_{ct}$ is $\\sigma \\left( \\sqrt{c} W_t^{(i)} \\right) = \\sigma \\sqrt{ct} Z_i$.\n\nBoth routes lead to an identical expression for the random variable $S_{ct}$:\n$$ S_{ct}^{(i)} = S_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)ct + \\sigma \\sqrt{ct} Z_i \\right) $$\nThis confirms that the scaling property of the Wiener process induces a corresponding and consistent scaling property in the GBM process. The numerical part of the problem tests this statistical equivalence by comparing estimates from two independent sets of random numbers.\n\n**3. Derivation of the Analytical Expectation**\n\nWe are tasked with computing the analytical benchmark for $E[g(S_{ct})]$, where $g(x) = x^p$.\n$$ E[g(S_{ct})] = E[(S_{ct})^p] = E\\left[ \\left( S_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)ct + \\sigma W_{ct} \\right) \\right)^p \\right] $$\n$$ E[(S_{ct})^p] = S_0^p E\\left[ \\exp\\left( p\\left(\\mu - \\frac{1}{2}\\sigma^2\\right)ct + p\\sigma W_{ct} \\right) \\right] $$\nThe non-stochastic part can be moved outside the expectation:\n$$ E[(S_{ct})^p] = S_0^p \\exp\\left( p\\left(\\mu - \\frac{1}{2}\\sigma^2\\right)ct \\right) E\\left[ \\exp(p\\sigma W_{ct}) \\right] $$\nThe remaining expectation is the moment-generating function (MGF) of the normal random variable $W_{ct}$. Let $X = W_{ct}$. We know $X \\sim N(0, ct)$. The MGF of a general normal random variable $Y \\sim N(\\nu, \\tau^2)$ is $M_Y(k) = E[e^{kY}] = \\exp(k\\nu + \\frac{1}{2}k^2\\tau^2)$. For our case, $Y=X$, $\\nu=0$, $\\tau^2 = ct$, and the MGF is evaluated at $k = p\\sigma$.\n$$ E\\left[ \\exp(p\\sigma W_{ct}) \\right] = \\exp\\left( (p\\sigma) \\cdot 0 + \\frac{1}{2}(p\\sigma)^2 (ct) \\right) = \\exp\\left( \\frac{1}{2}p^2\\sigma^2 ct \\right) $$\nSubstituting this back into the expression for the expectation:\n$$ E[(S_{ct})^p] = S_0^p \\exp\\left( p\\mu ct - \\frac{1}{2}p\\sigma^2 ct \\right) \\exp\\left( \\frac{1}{2}p^2\\sigma^2 ct \\right) $$\nCombining the exponents gives the final analytical formula:\n$$ E[(S_{ct})^p] = S_0^p \\exp\\left( p\\mu ct + \\frac{1}{2}(p^2 - p)\\sigma^2 ct \\right) = S_0^p \\exp\\left( ct \\left[ p\\mu + \\frac{1}{2}p(p-1)\\sigma^2 \\right] \\right) $$\n\n**4. Monte Carlo Simulation and Statistical Testing**\n\nThe simulation will be performed for each test case with a sample size of $N=100000$. A fixed random seed ensures reproducibility. For each case, we compute two Monte Carlo estimates and compare them.\n\nLet $\\{g_i\\}_{i=1}^N$ be the $N$ sample values of $g(S_{ct})$. The sample mean is $\\hat{m} = \\frac{1}{N} \\sum_{i=1}^N g_i$, and the sample standard deviation is $\\hat{s} = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (g_i - \\hat{m})^2}$. The standard error of the mean is $\\mathrm{SE} = \\hat{s} / \\sqrt{N}$.\n\n*   **Agreement Test**: We compute means $\\hat{m}_1, \\hat{m}_2$ and standard errors $\\mathrm{SE}_1, \\mathrm{SE}_2$ for the direct and scaling routes, respectively, using independent sets of random numbers. The test for agreement is whether the difference in means is statistically insignificant, defined by the criterion:\n$$ |\\hat{m}_1 - \\hat{m}_2| \\leq 3 \\sqrt{\\mathrm{SE}_1^2 + \\mathrm{SE}_2^2} $$\nThe term on the right is $3$ times the standard error of the difference between the two independent sample means.\n\n*   **Consistency Test**: We compare the estimate from the scaling route, $\\hat{m}_2$, with the analytically derived value, $E_{an} = E[(S_{ct})^p]$. The estimator is consistent if its value lies within a $3$-standard-error confidence interval of the true value:\n$$ |\\hat{m}_2 - E_{an}| \\leq 3 \\cdot \\mathrm{SE}_2 $$\n\nFor the boundary case where $c=0$, the time horizon $ct$ is $0$. Thus, $S_{ct} = S_0$ deterministically. Both simulation routes will yield the constant value $S_0$, so the samples will be $g(S_0)=(S_0)^p$. Consequently, $\\hat{m}_1 = \\hat{m}_2 = E_{an} = (S_0)^p$, and the sample variances (and standard errors) are $0$. Both test inequalities become $0 \\le 0$, which is true.\n\nThe implementation will follow this logic for each test case provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of verifying scaling invariance of geometric Brownian motion\n    and assessing the consistency of Monte Carlo estimators.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (happy path)\n        (100.0, 0.05, 0.2, 1.0, 1.0, 1.0),\n        # Case B (moderate scaling, negative drift, higher volatility)\n        (50.0, -0.02, 0.5, 1.0, 0.5, 2.0),\n        # Case C (large scaling to unit horizon with zero drift)\n        (120.0, 0.0, 1.0, 0.5, 2.0, 1.0),\n        # Case D (boundary case)\n        (80.0, 0.03, 0.4, 1.0, 0.0, 1.5),\n        # Case E (near-zero horizon)\n        (100.0, 0.01, 0.6, 1.0, 1.0e-6, 1.0),\n    ]\n\n    # Constants from the problem statement\n    N = 100000\n    SEED = 123456\n    rng = np.random.default_rng(SEED)\n\n    results = []\n    for case in test_cases:\n        S0, mu, sigma, t, c, p = case\n        \n        T = c * t\n\n        # Special handling for the deterministic boundary case c=0\n        if c == 0.0:\n            m1 = S0**p\n            se1 = 0.0\n            m2 = S0**p\n            se2 = 0.0\n            E_analytical = S0**p\n        else:\n            # Route 1: Direct simulation using W_ct ~ N(0, ct)\n            Z1 = rng.standard_normal(N)\n            S_ct1 = S0 * np.exp((mu - 0.5 * sigma**2) * T + sigma * np.sqrt(T) * Z1)\n            g_S1 = S_ct1**p\n            m1 = np.mean(g_S1)\n            se1 = np.std(g_S1, ddof=1) / np.sqrt(N)\n\n            # Route 2: Scaling simulation using sqrt(c) * W_t, where W_t ~ N(0, t)\n            # A new set of random numbers is drawn to ensure statistical independence\n            Z2 = rng.standard_normal(N)\n            scaled_W_t = np.sqrt(c) * (np.sqrt(t) * Z2)\n            S_ct2 = S0 * np.exp((mu - 0.5 * sigma**2) * T + sigma * scaled_W_t)\n            g_S2 = S_ct2**p\n            m2 = np.mean(g_S2)\n            se2 = np.std(g_S2, ddof=1) / np.sqrt(N)\n\n            # Analytical benchmark\n            exponent_term = T * (p * mu + 0.5 * p * (p - 1) * sigma**2)\n            E_analytical = S0**p * np.exp(exponent_term)\n\n        # Perform the two statistical tests\n\n        # 1. Agreement between the two MC estimators\n        # Test: |m1 - m2| = 3 * sqrt(SE1^2 + SE2^2)\n        agreement_threshold = 3.0 * np.sqrt(se1**2 + se2**2)\n        agreement_bool = np.abs(m1 - m2) = agreement_threshold\n\n        # 2. Consistency of the scaling-route estimator with the analytical benchmark\n        # Test: |m2 - E_analytical| = 3 * SE2\n        consistency_threshold = 3.0 * se2\n        # Handle case where standard error is zero (deterministic case)\n        if se2 == 0.0:\n             consistency_bool = np.isclose(m2, E_analytical)\n        else:\n             consistency_bool = np.abs(m2 - E_analytical) = consistency_threshold\n             \n        results.append([bool(agreement_bool), bool(consistency_bool)])\n\n    # Format and print the final results to match the required single-line format.\n    # str(results) produces a string like \"[[True, True], [True, False]]\".\n    # Removing spaces results in a compact representation \"[[True,True],[True,False]]\".\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "With a validated simulation method in hand, we can now apply it to practical problems like pricing financial derivatives. However, basic Monte Carlo simulation can be computationally expensive, making efficiency improvements a vital skill for any practitioner. This exercise introduces Common Random Numbers (CRN), a powerful variance reduction technique that is especially effective when estimating the difference between two correlated quantities, such as the value of an option spread. By implementing CRN and quantifying its effectiveness via the variance reduction factor, you will gain hands-on experience with a technique that can dramatically accelerate the convergence of Monte Carlo simulations in real-world applications.",
            "id": "3341965",
            "problem": "You are asked to design, analyze, and implement a Monte Carlo estimator for the price difference of two European call options under a Geometric Brownian Motion (GBM) model, using Common Random Numbers (CRN) to reduce variance. The underlying asset price process is modeled as a Geometric Brownian Motion (GBM) with stochastic differential equation (SDE) given by\n$$\ndS_t = \\mu S_t \\, dt + \\sigma S_t \\, dB_t,\n$$\nwhere $S_t$ is the asset price at time $t$, $\\mu$ is the drift, $\\sigma$ is the volatility, and $B_t$ is a standard Wiener process. The target quantity is the difference of expected payoffs at a fixed horizon $T$, namely\n$$\n\\Delta P = \\mathbb{E}\\big[(S_T - K_1)^+\\big] - \\mathbb{E}\\big[(S_T - K_2)^+\\big],\n$$\nfor two strikes $K_1$ and $K_2$, where $(x)^+ = \\max\\{x,0\\}$.\n\nStarting from the SDE definition and the properties of the Wiener process, derive a correct exact-time simulation scheme for $S_T$ that does not rely on time discretization. Then, design a Monte Carlo (MC) estimator for $\\Delta P$ that uses Common Random Numbers (CRN), meaning that for each simulated terminal draw you must evaluate both payoffs $(S_T - K_1)^+$ and $(S_T - K_2)^+$ using the same underlying randomness. Derive, from first principles and properties of variance and covariance, the form of the estimator variance under CRN and under a baseline \"independent streams\" approach in which the two option payoffs are simulated independently with the same total number of payoff evaluations. Use this to obtain an empirical estimate of a variance reduction factor.\n\nYour program must:\n- Implement exact simulation of $S_T$ under GBM.\n- For each test case, construct:\n  - A CRN estimator of $\\Delta P$ using $n$ independent draws of the terminal shock and evaluating both payoffs on each draw.\n  - An empirical estimate of the per-path variance under CRN, $\\widehat{\\mathrm{Var}}_{\\mathrm{CRN}} = \\widehat{\\mathrm{Var}}\\big((S_T - K_1)^+ - (S_T - K_2)^+\\big)$.\n  - An empirical estimate of the per-path variance under independent streams, $\\widehat{\\mathrm{Var}}_{\\mathrm{ind}} = \\widehat{\\mathrm{Var}}\\big((S_T - K_1)^+\\big) + \\widehat{\\mathrm{Var}}\\big((S_T - K_2)^+\\big)$, estimated from the same simulated terminal values so that total payoff evaluations are matched.\n  - The empirical variance reduction factor $R = \\widehat{\\mathrm{Var}}_{\\mathrm{ind}} / \\widehat{\\mathrm{Var}}_{\\mathrm{CRN}}$.\n  - The empirical correlation coefficient between the two payoffs, $\\widehat{\\rho} \\in [-1,1]$.\n- Use a fixed seed for the pseudorandom number generator in each test case to ensure reproducibility.\n\nTest suite:\n- Case $1$: $S_0 = 100$, $\\mu = 0.03$, $\\sigma = 0.2$, $T = 1$, $K_1 = 90$, $K_2 = 110$, $n = 200000$, seed $= 1729$.\n- Case $2$: $S_0 = 100$, $\\mu = 0.03$, $\\sigma = 0.2$, $T = 1$, $K_1 = 100$, $K_2 = 100.001$, $n = 300000$, seed $= 2029$.\n- Case $3$: $S_0 = 100$, $\\mu = 0.0$, $\\sigma = 0.8$, $T = 1$, $K_1 = 100$, $K_2 = 120$, $n = 250000$, seed $= 2222$.\n- Case $4$: $S_0 = 100$, $\\mu = 0.03$, $\\sigma = 0.4$, $T = 0.05$, $K_1 = 95$, $K_2 = 105$, $n = 300000$, seed $= 3333$.\n\nAll parameters are dimensionless scalars. There are no physical units and no angles. All reported numerical results must be real numbers. For each test case, your program must output a list of three floats:\n- the CRN estimate of $\\Delta P$,\n- the variance reduction factor $R$,\n- the payoff correlation estimate $\\widehat{\\rho}$,\n\neach rounded to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing a list with one entry per test case; each entry should itself be a three-element list in the order specified above.\n- For example, the overall printed object should look like\n$$\n[\\,[\\delta_1, R_1, \\rho_1], [\\delta_2, R_2, \\rho_2], [\\delta_3, R_3, \\rho_3], [\\delta_4, R_4, \\rho_4]\\,],\n$$\nwith each $\\delta_i$, $R_i$, and $\\rho_i$ printed to $6$ decimal places.",
            "solution": "The user has provided a well-posed problem in computational finance. I will first validate the problem statement and then provide a complete solution.\n\n### Step 1: Extract Givens\n- **Model**: The underlying asset price $S_t$ follows a Geometric Brownian Motion (GBM) with the stochastic differential equation (SDE):\n$$\ndS_t = \\mu S_t \\, dt + \\sigma S_t \\, dB_t\n$$\nwhere $\\mu$ is the drift, $\\sigma$ is the volatility, and $B_t$ is a standard Wiener process.\n- **Target Quantity**: The price difference of two European call options, $\\Delta P = \\mathbb{E}\\big[(S_T - K_1)^+\\big] - \\mathbb{E}\\big[(S_T - K_2)^+\\big]$, where $(x)^+ = \\max\\{x,0\\}$, $T$ is the expiry time, and $K_1$, $K_2$ are the strike prices.\n- **Methodology**: A Monte Carlo estimator using Common Random Numbers (CRN) for variance reduction.\n- **Required Derivations**:\n    1. An exact-time simulation scheme for $S_T$.\n    2. The form of the estimator variance under CRN and independent streams.\n- **Required Outputs for each test case**:\n    1. The CRN estimate of $\\Delta P$.\n    2. The empirical variance reduction factor $R = \\widehat{\\mathrm{Var}}_{\\mathrm{ind}} / \\widehat{\\mathrm{Var}}_{\\mathrm{CRN}}$.\n    3. The empirical correlation coefficient $\\widehat{\\rho}$ between the payoffs.\n- **Definitions for Empirical Estimates**:\n    - $\\widehat{\\mathrm{Var}}_{\\mathrm{CRN}} = \\widehat{\\mathrm{Var}}\\big((S_T - K_1)^+ - (S_T - K_2)^+\\big)$.\n    - $\\widehat{\\mathrm{Var}}_{\\mathrm{ind}} = \\widehat{\\mathrm{Var}}\\big((S_T - K_1)^+\\big) + \\widehat{\\mathrm{Var}}\\big((S_T - K_2)^+\\big)$.\n- **Test Suite**:\n    - Case $1$: $S_0 = 100$, $\\mu = 0.03$, $\\sigma = 0.2$, $T = 1$, $K_1 = 90$, $K_2 = 110$, $n = 200000$, seed $= 1729$.\n    - Case $2$: $S_0 = 100$, $\\mu = 0.03$, $\\sigma = 0.2$, $T = 1$, $K_1 = 100$, $K_2 = 100.001$, $n = 300000$, seed $= 2029$.\n    - Case $3$: $S_0 = 100$, $\\mu = 0.0$, $\\sigma = 0.8$, $T = 1$, $K_1 = 100$, $K_2 = 120$, $n = 250000$, seed $= 2222$.\n    - Case $4$: $S_0 = 100$, $\\mu = 0.03$, $\\sigma = 0.4$, $T = 0.05$, $K_1 = 95$, $K_2 = 105$, $n = 300000$, seed $= 3333$.\n- **Output Formatting**: A single-line list of lists, `[[delta_1, R_1, rho_1], [delta_2, R_2, rho_2], ...]`, with each number printed to $6$ decimal places.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on the standard GBM model from financial mathematics and employs standard Monte Carlo simulation techniques (CRN). All concepts are firmly rooted in stochastic calculus and statistics.\n- **Well-Posed**: All necessary parameters ($S_0, \\mu, \\sigma, T, K_1, K_2, n$, seed) are provided for each test case. The objective function and the quantities to be calculated are defined precisely. A unique numerical solution can be computed.\n- **Objective**: The problem is stated using precise, objective mathematical language.\n- **Conclusion**: The problem is valid. It is scientifically sound, well-posed, objective, complete, and non-trivial.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\n### Principle-Based Design and Derivations\n\n#### 1. Exact Simulation Scheme for Geometric Brownian Motion\nThe price process $S_t$ is governed by the SDE:\n$$\ndS_t = \\mu S_t \\, dt + \\sigma S_t \\, dB_t\n$$\nTo find an exact solution for $S_T$, we consider the process for $\\ln S_t$. Let $f(x) = \\ln x$. The first two derivatives are $f'(x) = 1/x$ and $f''(x) = -1/x^2$. By Itô's lemma, the dynamics of $f(S_t) = \\ln S_t$ are given by:\n$$\nd(\\ln S_t) = f'(S_t) dS_t + \\frac{1}{2} f''(S_t) (dS_t)^2\n$$\nAccording to Itô calculus rules, $(dS_t)^2 = (\\mu S_t dt + \\sigma S_t dB_t)^2 = \\sigma^2 S_t^2 dt$, as terms with $dt^2$ and $dt \\, dB_t$ are zero. Substituting the derivatives and $(dS_t)^2$:\n$$\nd(\\ln S_t) = \\frac{1}{S_t} (\\mu S_t dt + \\sigma S_t dB_t) + \\frac{1}{2} \\left(-\\frac{1}{S_t^2}\\right) (\\sigma^2 S_t^2 dt)\n$$\n$$\nd(\\ln S_t) = (\\mu dt + \\sigma dB_t) - \\frac{1}{2} \\sigma^2 dt = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) dt + \\sigma dB_t\n$$\nThis is an SDE with constant coefficients. We can integrate it from $t=0$ to $t=T$:\n$$\n\\int_0^T d(\\ln S_t) = \\int_0^T \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) dt + \\int_0^T \\sigma dB_t\n$$\n$$\n\\ln S_T - \\ln S_0 = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) T + \\sigma (B_T - B_0)\n$$\nGiven that $B_0=0$, the increment $B_T$ is a random variable distributed as a normal distribution with mean $0$ and variance $T$, i.e., $B_T \\sim \\mathcal{N}(0, T)$. We can write $B_T = \\sqrt{T} Z$, where $Z \\sim \\mathcal{N}(0, 1)$ is a standard normal random variable.\nExponentiating both sides gives the exact solution for $S_T$:\n$$\nS_T = S_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) T + \\sigma \\sqrt{T} Z \\right)\n$$\nThis formula provides an \"exact-time\" simulation scheme because it allows us to draw samples of the terminal price $S_T$ directly from its known distribution without discretizing the time interval $[0, T]$.\n\n#### 2. Monte Carlo Estimation and Variance Reduction\nLet $Y_1 = (S_T - K_1)^+$ and $Y_2 = (S_T - K_2)^+$ be the payoffs for the two call options. The quantity of interest is $\\Delta P = \\mathbb{E}[Y_1] - \\mathbb{E}[Y_2] = \\mathbb{E}[Y_1 - Y_2]$.\n\n**Common Random Numbers (CRN) Estimator**\nThe CRN technique involves using the same stream of random numbers to simulate both quantities being compared. For each of $n$ Monte Carlo trials, we generate a an independent standard normal random variate $Z_i$ for $i=1, \\dots, n$. We then compute a single terminal price $S_{T,i}$ using the scheme derived above. From this single $S_{T,i}$, we evaluate both payoffs, $Y_{1,i} = (S_{T,i} - K_1)^+$ and $Y_{2,i} = (S_{T,i} - K_2)^+$. The CRN estimator for $\\Delta P$ is the sample mean of the differences:\n$$\n\\widehat{\\Delta P}_{\\mathrm{CRN}} = \\frac{1}{n} \\sum_{i=1}^n (Y_{1,i} - Y_{2,i})\n$$\nThe variance of this estimator is given by:\n$$\n\\mathrm{Var}(\\widehat{\\Delta P}_{\\mathrm{CRN}}) = \\frac{1}{n} \\mathrm{Var}(Y_1 - Y_2)\n$$\nThe term $\\mathrm{Var}_{\\mathrm{CRN}} \\equiv \\mathrm{Var}(Y_1 - Y_2)$ is the per-path variance under CRN.\n\n**Baseline Independent Streams Estimator**\nFor comparison, consider an estimator where $\\mathbb{E}[Y_1]$ and $\\mathbb{E}[Y_2]$ are estimated independently. To match the total number of $2n$ payoff evaluations used in the CRN approach, one would use $n$ independent paths for $Y_1$ and another $n$ independent paths for $Y_2$. The estimator would be $\\widehat{\\Delta P}_{\\mathrm{ind}} = \\frac{1}{n}\\sum_{i=1}^n Y_{1,i} - \\frac{1}{n}\\sum_{j=1}^n Y'_{2,j}$, where $Y'$ is based on an independent simulation. The variance of this estimator is:\n$$\n\\mathrm{Var}(\\widehat{\\Delta P}_{\\mathrm{ind}}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n Y_{1,i}\\right) + \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{j=1}^n Y'_{2,j}\\right) = \\frac{1}{n}\\mathrm{Var}(Y_1) + \\frac{1}{n}\\mathrm{Var}(Y_2)\n$$\nThe corresponding per-path variance is $\\mathrm{Var}_{\\mathrm{ind}} \\equiv \\mathrm{Var}(Y_1) + \\mathrm{Var}(Y_2)$.\n\n**Variance Analysis and Reduction Factor**\nUsing the property $\\mathrm{Var}(X-Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) - 2\\mathrm{Cov}(X,Y)$, we can relate the two variances:\n$$\n\\mathrm{Var}_{\\mathrm{CRN}} = \\mathrm{Var}_{\\mathrm{ind}} - 2\\mathrm{Cov}(Y_1, Y_2)\n$$\nThe payoff functions $Y_1 = \\max(S_T - K_1, 0)$ and $Y_2 = \\max(S_T - K_2, 0)$ are both non-decreasing functions of the same underlying random variable $S_T$. Therefore, $Y_1$ and $Y_2$ are positively correlated, meaning $\\mathrm{Cov}(Y_1, Y_2)  0$. This implies that $\\mathrm{Var}_{\\mathrm{CRN}}  \\mathrm{Var}_{\\mathrm{ind}}$, so CRN achieves variance reduction.\n\nThe variance reduction factor is the ratio of the per-path variances:\n$$\nR = \\frac{\\mathrm{Var}_{\\mathrm{ind}}}{\\mathrm{Var}_{\\mathrm{CRN}}} = \\frac{\\mathrm{Var}(Y_1) + \\mathrm{Var}(Y_2)}{\\mathrm{Var}(Y_1 - Y_2)}\n$$\nA value of $R  1$ indicates that CRN is effective.\n\n**Empirical Quantities**\nIn practice, we compute empirical estimates from the simulated samples. Given the $n$ pairs of payoffs $\\{(Y_{1,i}, Y_{2,i})\\}_{i=1}^n$ generated using CRN:\n- The estimate of $\\Delta P$ is $\\widehat{\\Delta P} = \\bar{Y}_1 - \\bar{Y}_2$, where $\\bar{Y}_k = \\frac{1}{n}\\sum_{i=1}^n Y_{k,i}$.\n- The empirical per-path CRN variance is the sample variance of the differences:\n$\\widehat{\\mathrm{Var}}_{\\mathrm{CRN}} = \\widehat{\\mathrm{Var}}(Y_1 - Y_2) = \\frac{1}{n-1}\\sum_{i=1}^n \\left( (Y_{1,i}-Y_{2,i}) - (\\bar{Y}_1 - \\bar{Y}_2) \\right)^2$.\n- The empirical per-path independent variance is the sum of the individual sample variances:\n$\\widehat{\\mathrm{Var}}_{\\mathrm{ind}} = \\widehat{\\mathrm{Var}}(Y_1) + \\widehat{\\mathrm{Var}}(Y_2)$, where $\\widehat{\\mathrm{Var}}(Y_k) = \\frac{1}{n-1}\\sum_{i=1}^n (Y_{k,i} - \\bar{Y}_k)^2$.\n- The empirical variance reduction factor is $R = \\widehat{\\mathrm{Var}}_{\\mathrm{ind}} / \\widehat{\\mathrm{Var}}_{\\mathrm{CRN}}$.\n- The empirical correlation coefficient is calculated as:\n$$\n\\widehat{\\rho} = \\frac{\\widehat{\\mathrm{Cov}}(Y_1, Y_2)}{\\sqrt{\\widehat{\\mathrm{Var}}(Y_1) \\widehat{\\mathrm{Var}}(Y_2)}}\n$$\nwhere $\\widehat{\\mathrm{Cov}}(Y_1, Y_2) = \\frac{1}{n-1}\\sum_{i=1}^n (Y_{1,i} - \\bar{Y}_1)(Y_{2,i} - \\bar{Y}_2)$.\n\n#### 3. Algorithmic Implementation\nFor each test case with parameters $S_0, \\mu, \\sigma, T, K_1, K_2, n, \\text{seed}$:\n1. Initialize a pseudorandom number generator with the specified seed.\n2. Generate a vector of $n$ independent standard normal random variates $Z_i \\sim \\mathcal{N}(0, 1)$.\n3. Compute a vector of $n$ terminal stock prices $S_{T,i}$ using the exact formula: $S_{T,i} = S_0 \\exp\\left( (\\mu - 0.5\\sigma^2)T + \\sigma\\sqrt{T}Z_i \\right)$.\n4. Compute two vectors of payoffs, $Y_{1,i} = \\max(S_{T,i} - K_1, 0)$ and $Y_{2,i} = \\max(S_{T,i} - K_2, 0)$.\n5. Calculate the vector of payoff differences, $\\Delta_i = Y_{1,i} - Y_{2,i}$.\n6. Calculate the CRN estimate of the price difference: $\\widehat{\\Delta P} = \\mathrm{mean}(\\Delta_i)$.\n7. Calculate the empirical variances:\n    - $\\widehat{\\mathrm{Var}}_{\\mathrm{CRN}} = \\mathrm{var}(\\Delta_i, \\text{ddof}=1)$.\n    - $\\widehat{\\mathrm{Var}}(Y_1) = \\mathrm{var}(Y_{1,i}, \\text{ddof}=1)$.\n    - $\\widehat{\\mathrm{Var}}(Y_2) = \\mathrm{var}(Y_{2,i}, \\text{ddof}=1)$.\n    - $\\widehat{\\mathrm{Var}}_{\\mathrm{ind}} = \\widehat{\\mathrm{Var}}(Y_1) + \\widehat{\\mathrm{Var}}(Y_2)$.\n8. Calculate the empirical variance reduction factor $R = \\widehat{\\mathrm{Var}}_{\\mathrm{ind}} / \\widehat{\\mathrm{Var}}_{\\mathrm{CRN}}$.\n9. Calculate the empirical correlation coefficient $\\widehat{\\rho} = \\mathrm{corrcoef}(Y_{1,i}, Y_{2,i})$.\n10. Store the results $[\\widehat{\\Delta P}, R, \\widehat{\\rho}]$ for the current test case.\n11. After processing all cases, format and print the results as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs, analyzes, and implements a Monte Carlo estimator for the price\n    difference of two European call options under a Geometric Brownian Motion\n    (GBM) model, using Common Random Numbers (CRN) to reduce variance.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (S0, mu, sigma, T, K1, K2, n, seed)\n        (100, 0.03, 0.2, 1, 90, 110, 200000, 1729),\n        (100, 0.03, 0.2, 1, 100, 100.001, 300000, 2029),\n        (100, 0.0, 0.8, 1, 100, 120, 250000, 2222),\n        (100, 0.03, 0.4, 0.05, 95, 105, 300000, 3333),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        S0, mu, sigma, T, K1, K2, n, seed = case\n\n        # 1. Initialize a pseudorandom number generator with the specified seed.\n        # Use default_rng for modern, best-practice random number generation.\n        rng = np.random.default_rng(seed)\n\n        # 2. Generate a vector of n independent standard normal random variates.\n        Z = rng.standard_normal(n)\n\n        # 3. Compute a vector of n terminal stock prices using the exact formula.\n        drift = (mu - 0.5 * sigma**2) * T\n        diffusion = sigma * np.sqrt(T) * Z\n        S_T = S0 * np.exp(drift + diffusion)\n\n        # 4. Compute two vectors of payoffs.\n        Y1 = np.maximum(S_T - K1, 0)\n        Y2 = np.maximum(S_T - K2, 0)\n\n        # 5. Calculate the vector of payoff differences.\n        delta_payoffs = Y1 - Y2\n\n        # 6. Calculate the CRN estimate of the price difference.\n        delta_p_crn = np.mean(delta_payoffs)\n\n        # 7. Calculate the empirical variances. ddof=1 for sample variance.\n        # Per-path variance under CRN\n        var_crn = np.var(delta_payoffs, ddof=1)\n        \n        # Individual payoff variances\n        var_y1 = np.var(Y1, ddof=1)\n        var_y2 = np.var(Y2, ddof=1)\n        \n        # Per-path variance under hypothetical independent streams\n        var_ind = var_y1 + var_y2\n\n        # 8. Calculate the empirical variance reduction factor R.\n        # Check for division by zero, though highly unlikely here.\n        if var_crn > 0:\n            R = var_ind / var_crn\n        else:\n            # This case happens if all payoff differences are identical,\n            # indicating perfect correlation and infinite reduction.\n            # R can be set to infinity or a large number.\n            # For this problem, it's safe to assume var_crn will be positive.\n            R = np.inf\n\n\n        # 9. Calculate the empirical correlation coefficient.\n        # np.corrcoef returns a 2x2 matrix. The off-diagonal element is the correlation.\n        # A check is needed for cases where one variance is zero.\n        if var_y1 > 0 and var_y2 > 0:\n            rho = np.corrcoef(Y1, Y2)[0, 1]\n        else:\n            # If one variance is zero, the payoff is constant (likely 0),\n            # and correlation is undefined. Set to 1 as it's the limit.\n            rho = 1.0\n\n        # 10. Store the results for the current test case.\n        all_results.append([delta_p_crn, R, rho])\n\n    # 11. After processing all cases, format and print the results as specified.\n    # The output format requires printing each number to 6 decimal places.\n    # This must be done via string formatting.\n    output_parts = []\n    for result_group in all_results:\n        # result_group is like [delta, R, rho]\n        formatted_group = [f'{val:.6f}' for val in result_group]\n        output_parts.append(f\"[{', '.join(formatted_group)}]\")\n\n    final_output = f\"[{', '.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "So far, we have simulated Geometric Brownian Motion assuming its parameters, like the volatility $\\sigma$, are known quantities. In practice, these parameters are unknown and must be estimated from historical data, which presents an inverse problem. This practice shifts our focus from forward simulation to statistical inference, guiding you to derive a parameter estimator using the principle of Maximum Likelihood Estimation (MLE). You will then use Monte Carlo simulation itself as a powerful laboratory to study the statistical properties, such as bias and variance, of your derived estimator, bridging the gap between stochastic modeling and practical data analysis.",
            "id": "3341959",
            "problem": "You are asked to design and implement a complete, runnable program that uses Monte Carlo simulation to study Maximum Likelihood Estimation (MLE) of the volatility parameter under the geometric Brownian motion (GBM) model from discrete-time observations, and to compare the finite-sample variance of the estimator to the inverse of the Fisher information computed from the corresponding log-price process. The problem must be solved from first principles of stochastic calculus and statistical inference, without using closed-form estimators as a starting point. The fundamental base you may assume includes the stochastic differential equation definition of GBM, properties of Brownian motion, Itô’s lemma, the distribution of normally distributed increments, and the general form of the log-likelihood for a multivariate normal sample.\n\nStart from the following base. A geometric Brownian motion for a strictly positive process $S_t$ satisfies the stochastic differential equation\n$$\n\\mathrm{d}S_t = \\mu S_t \\,\\mathrm{d}t + \\sigma S_t \\,\\mathrm{d}W_t,\n$$\nwhere $W_t$ is a standard Brownian motion, $\\mu$ is the drift, and $\\sigma$ is the volatility. Define the log-price $X_t = \\log S_t$. Based on Itô’s lemma and standard properties of Brownian motion, the discrete increments $R_i = X_{t_i} - X_{t_{i-1}}$ over equally spaced times $t_i = i \\Delta$ for $i \\in \\{1,\\dots,n\\}$ are independent and normally distributed:\n$$\nR_i \\sim \\mathcal{N}\\!\\big(\\Delta(\\mu - \\tfrac{1}{2}\\sigma^2),\\, \\sigma^2 \\Delta\\big).\n$$\nUse only this base to derive the MLE for $\\sigma$ when the drift parameter is unknown. Then implement a Monte Carlo (MC) study to assess the finite-sample bias and variance of the resulting estimator, and compare the empirical variance to the asymptotic variance predicted by the Fisher information computed for the log-price model.\n\nYour program must do the following for each test case below:\n1. Simulate $M$ independent samples of length $n$ of the log-returns $R_1,\\dots,R_n$ from the exact normal transition law described above, under the given parameters $(\\mu,\\sigma,\\Delta)$.\n2. For each sample, compute the MLE of $\\sigma$ from the discrete observations of $S_{t_i}$, which are equivalently summarized by $R_i = \\log S_{t_i} - \\log S_{t_{i-1}}$.\n3. Compute the Monte Carlo estimate of the finite-sample bias of the estimator as the sample average of $\\widehat{\\sigma}$ minus the true $\\sigma$.\n4. Compute the Monte Carlo estimate of the finite-sample variance of the estimator as the empirical variance of $\\widehat{\\sigma}$ across the $M$ replications.\n5. Compute the asymptotic variance predicted by the Fisher information for $\\sigma$ based on the log-price model.\n6. Aggregate the results for all test cases into a single line of output.\n\nAll computations must be expressed in purely mathematical terms without physical units. Angles do not appear in this problem. Percentages, if any, must be expressed as decimals.\n\nTest suite. Use the following three test cases, each specified by a tuple $(n,\\Delta,\\mu,\\sigma,M,\\text{seed})$:\n- Case A (general, one trading year at daily spacing): $(n,\\Delta,\\mu,\\sigma,M,\\text{seed}) = (\\,252,\\, 1/252,\\, 0.08,\\, 0.25,\\, 20000,\\, 12345\\,)$.\n- Case B (small sample, coarse spacing): $(n,\\Delta,\\mu,\\sigma,M,\\text{seed}) = (\\,5,\\, 1,\\, 0.00,\\, 0.50,\\, 200000,\\, 67890\\,)$.\n- Case C (longer sample, fine spacing): $(n,\\Delta,\\mu,\\sigma,M,\\text{seed}) = (\\,1000,\\, 1/252,\\, 0.02,\\, 0.20,\\, 5000,\\, 54321\\,)$.\n\nFor each case, your program must output a triple $[\\text{bias}, \\text{variance}, \\text{asymptotic\\_variance}]$ in this order, where:\n- $\\text{bias}$ is the Monte Carlo estimate of $E[\\widehat{\\sigma}] - \\sigma$,\n- $\\text{variance}$ is the Monte Carlo estimate of $\\mathrm{Var}(\\widehat{\\sigma})$ using the empirical variance with divisor $M$,\n- $\\text{asymptotic\\_variance}$ is the inverse of the Fisher information for $\\sigma$ derived from the log-price model for a single sample of size $n$, expressed as a function of $\\sigma$ and $n$.\n\nFinal output format. Your program should produce a single line of output containing a list of the three triples for Cases A, B, and C in this order, as a comma-separated list enclosed in square brackets of lists, for example:\n\"[ [bias_A,variance_A,asymptotic_variance_A] , [bias_B,variance_B,asymptotic_variance_B] , [bias_C,variance_C,asymptotic_variance_C] ]\"\nYour program must be self-contained and must not require any input. It must use the exact parameter values above and the random seeds provided for reproducibility. The final output must be printed exactly once and must contain only the described list, with no additional text.",
            "solution": "The problem requires the derivation of the Maximum Likelihood Estimator (MLE) for the volatility parameter $\\sigma$ of a geometric Brownian motion (GBM) process, the derivation of its asymptotic variance via the Fisher information, and the implementation of a Monte Carlo simulation to assess the estimator's finite-sample properties.\n\nLet the stock price process $S_t$ follow a GBM:\n$$\n\\mathrm{d}S_t = \\mu S_t \\,\\mathrm{d}t + \\sigma S_t \\,\\mathrm{d}W_t\n$$\nwhere $\\mu$ is the drift, $\\sigma  0$ is the volatility, and $W_t$ is a standard Brownian motion. The log-price process is $X_t = \\log S_t$. By applying Itô's lemma to $f(S_t) = \\log S_t$, we find that $X_t$ follows an arithmetic Brownian motion:\n$$\n\\mathrm{d}X_t = (\\mu - \\tfrac{1}{2}\\sigma^2) \\,\\mathrm{d}t + \\sigma \\,\\mathrm{d}W_t\n$$\nFor discrete, equally spaced time intervals of length $\\Delta$, the log-returns $R_i = X_{t_i} - X_{t_{i-1}}$ for $t_i = i\\Delta$ are independent and identically distributed (i.i.d.) normal random variables:\n$$\nR_i \\sim \\mathcal{N}\\!\\left(\\nu, \\tau^2\\right)\n$$\nwhere the mean is $\\nu = \\Delta(\\mu - \\tfrac{1}{2}\\sigma^2)$ and the variance is $\\tau^2 = \\sigma^2 \\Delta$.\n\n### 1. Derivation of the Maximum Likelihood Estimator (MLE)\n\nWe aim to find the MLE for $\\sigma$ when $\\mu$ is unknown, based on a sample of $n$ log-returns $R_1, \\dots, R_n$. The parameters of the model are $(\\mu, \\sigma)$. The log-likelihood function for the sample is:\n$$\n\\ell(\\mu, \\sigma; R_1, \\dots, R_n) = \\sum_{i=1}^n \\log f(R_i; \\mu, \\sigma)\n$$\nwhere $f(R_i; \\mu, \\sigma)$ is the probability density function of the normal distribution with mean $\\nu = \\Delta(\\mu - \\frac{1}{2}\\sigma^2)$ and variance $\\sigma^2\\Delta$. The log-likelihood can be written as:\n$$\n\\ell(\\mu, \\sigma) = \\sum_{i=1}^n \\left( -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\sigma^2 \\Delta) - \\frac{(R_i - \\Delta(\\mu - \\frac{1}{2}\\sigma^2))^2}{2\\sigma^2 \\Delta} \\right)\n$$\n$$\n\\ell(\\mu, \\sigma) = -\\frac{n}{2}\\log(2\\pi\\Delta) - n\\log(\\sigma) - \\frac{1}{2\\sigma^2 \\Delta} \\sum_{i=1}^n \\left(R_i - \\Delta\\mu + \\frac{1}{2}\\Delta\\sigma^2\\right)^2\n$$\nTo find the MLEs $(\\widehat{\\mu}, \\widehat{\\sigma})$, we take the partial derivatives of $\\ell$ with respect to $\\mu$ and $\\sigma$ and set them to zero.\n\nThe partial derivative with respect to $\\mu$ is:\n$$\n\\frac{\\partial\\ell}{\\partial\\mu} = -\\frac{1}{2\\sigma^2 \\Delta} \\sum_{i=1}^n 2\\left(R_i - \\Delta\\mu + \\frac{1}{2}\\Delta\\sigma^2\\right)(-\\Delta) = \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\left(R_i - \\Delta\\mu + \\frac{1}{2}\\Delta\\sigma^2\\right)\n$$\nSetting $\\frac{\\partial\\ell}{\\partial\\mu} = 0$ implies $\\sum (R_i - \\widehat{\\nu}) = 0$, where $\\widehat{\\nu}$ is the MLE of the mean of the returns. This yields $\\widehat{\\nu} = \\frac{1}{n} \\sum R_i = \\bar{R}$. So, at the maximum, the parameters must satisfy:\n$$\n\\Delta(\\widehat{\\mu} - \\tfrac{1}{2}\\widehat{\\sigma}^2) = \\bar{R}\n$$\nThis establishes a relationship between $\\widehat{\\mu}$ and $\\widehat{\\sigma}$. We can now profile the likelihood by substituting $\\bar{R}$ for the mean parameter $\\nu$. It's more direct to work with the parameter $\\sigma^2$. The partial derivative with respect to $\\sigma^2$ is:\n$$\n\\frac{\\partial\\ell}{\\partial(\\sigma^2)} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2 \\Delta} \\sum_{i=1}^n \\left(R_i - \\nu\\right)^2\n$$\nSetting this to zero and substituting $\\widehat{\\nu} = \\bar{R}$, we get:\n$$\n\\frac{n}{2\\widehat{\\sigma}^2} = \\frac{1}{2(\\widehat{\\sigma}^2)^2 \\Delta} \\sum_{i=1}^n (R_i - \\bar{R})^2 \\implies \\widehat{\\sigma}^2 = \\frac{1}{n\\Delta} \\sum_{i=1}^n (R_i - \\bar{R})^2\n$$\nBy the invariance property of MLEs, the MLE for $\\sigma$ is the square root of the MLE for $\\sigma^2$:\n$$\n\\widehat{\\sigma} = \\sqrt{\\frac{1}{n\\Delta} \\sum_{i=1}^n (R_i - \\bar{R})^2}\n$$\nThis estimator will be used in the simulation. Note that $E[\\widehat{\\sigma}^2] = \\frac{n-1}{n}\\sigma^2$, so $\\widehat{\\sigma}^2$ is a biased estimator for $\\sigma^2$. Consequently, $\\widehat{\\sigma}$ is also a biased estimator for $\\sigma$.\n\n### 2. Derivation of the Asymptotic Variance\nThe asymptotic variance of $\\widehat{\\sigma}$ is given by the corresponding diagonal element of the inverse of the total Fisher information matrix. The total Fisher information for $n$ i.i.d. observations is $I_n(\\mu, \\sigma) = n I(\\mu, \\sigma)$, where $I(\\mu, \\sigma)$ is the Fisher information for a single observation.\n\nTo simplify the calculation, we use a reparameterization. Let $\\theta_1 = \\nu = \\Delta(\\mu - \\frac{1}{2}\\sigma^2)$ and $\\theta_2 = \\tau^2 = \\sigma^2 \\Delta$. The log-likelihood for one observation $R_i$ is $\\ell_i(\\theta_1, \\theta_2) = C - \\frac{1}{2}\\log(\\theta_2) - \\frac{(R_i - \\theta_1)^2}{2\\theta_2}$. The Fisher information matrix for $(\\theta_1, \\theta_2)$ is diagonal:\n$$\nI(\\theta_1, \\theta_2) = \\begin{pmatrix} -E[\\frac{\\partial^2 \\ell_i}{\\partial \\theta_1^2}]  0 \\\\ 0  -E[\\frac{\\partial^2 \\ell_i}{\\partial \\theta_2^2}] \\end{pmatrix} = \\begin{pmatrix} 1/\\theta_2  0 \\\\ 0  1/(2\\theta_2^2) \\end{pmatrix} = \\begin{pmatrix} 1/(\\sigma^2\\Delta)  0 \\\\ 0  1/(2\\sigma^4\\Delta^2) \\end{pmatrix}\n$$\nThe transformation from $(\\mu, \\sigma)$ to $(\\theta_1, \\sigma^2 \\Delta)$ has the Jacobian matrix $J$:\n$$\nJ = \\frac{\\partial(\\theta_1, \\theta_2)}{\\partial(\\mu, \\sigma)} = \\begin{pmatrix} \\partial\\theta_1/\\partial\\mu  \\partial\\theta_1/\\partial\\sigma \\\\ \\partial\\theta_2/\\partial\\mu  \\partial\\theta_2/\\partial\\sigma \\end{pmatrix} = \\begin{pmatrix} \\Delta  -\\Delta\\sigma \\\\ 0  2\\sigma\\Delta \\end{pmatrix}\n$$\nThe Fisher information matrix for $(\\mu, \\sigma)$ is $I(\\mu, \\sigma) = J^T I(\\theta_1, \\theta_2) J$:\n$$\nI(\\mu, \\sigma) = \\begin{pmatrix} \\Delta  0 \\\\ -\\Delta\\sigma  2\\sigma\\Delta \\end{pmatrix} \\begin{pmatrix} 1/(\\sigma^2\\Delta)  0 \\\\ 0  1/(2\\sigma^4\\Delta^2) \\end{pmatrix} \\begin{pmatrix} \\Delta  -\\Delta\\sigma \\\\ 0  2\\sigma\\Delta \\end{pmatrix} = \\begin{pmatrix} \\Delta/\\sigma^2  -\\Delta/\\sigma \\\\ -\\Delta/\\sigma  \\Delta + 2/\\sigma^2 \\end{pmatrix}\n$$\nThe determinant of this matrix is $\\det(I) = (\\Delta/\\sigma^2)(\\Delta + 2/\\sigma^2) - (-\\Delta/\\sigma)^2 = 2\\Delta/\\sigma^4$. The inverse is:\n$$\n[I(\\mu, \\sigma)]^{-1} = \\frac{\\sigma^4}{2\\Delta} \\begin{pmatrix} \\Delta + 2/\\sigma^2  \\Delta/\\sigma \\\\ \\Delta/\\sigma  \\Delta/\\sigma^2 \\end{pmatrix} = \\begin{pmatrix} \\sigma^4/2 + \\sigma^2/\\Delta  \\sigma^3/2 \\\\ \\sigma^3/2  \\sigma^2/2 \\end{pmatrix}\n$$\nThe asymptotic variance-covariance matrix for $(\\widehat{\\mu}, \\widehat{\\sigma})$ is $[n I(\\mu, \\sigma)]^{-1}$. The asymptotic variance of $\\widehat{\\sigma}$ is the $(2,2)$ element of this matrix:\n$$\n\\text{Asymptotic Var}(\\widehat{\\sigma}) = \\frac{\\sigma^2}{2n}\n$$\nThis result is independent of $\\mu$ and $\\Delta$, and depends only on the true volatility $\\sigma$ and the sample size $n$.\n\n### 3. Monte Carlo Simulation Design\nFor each test case $(n, \\Delta, \\mu, \\sigma, M, \\text{seed})$, the simulation proceeds as follows:\n1.  Initialize a random number generator with the given `seed`.\n2.  Repeat $M$ times:\n    a. Generate a sample of $n$ log-returns $\\{R_i\\}_{i=1}^n$ from the distribution $\\mathcal{N}(\\Delta(\\mu - \\frac{1}{2}\\sigma^2), \\sigma^2 \\Delta)$.\n    b. Calculate the MLE $\\widehat{\\sigma}$ using the formula $\\widehat{\\sigma} = \\sqrt{\\frac{1}{n\\Delta} \\sum_{i=1}^n (R_i - \\bar{R})^2}$.\n    c. Store the computed value of $\\widehat{\\sigma}$.\n3.  After $M$ replications, we have a sample of estimates $\\{\\widehat{\\sigma}_j\\}_{j=1}^M$.\n4.  Estimate the bias as the sample mean of the estimation error: $\\text{bias} = \\left(\\frac{1}{M} \\sum_{j=1}^M \\widehat{\\sigma}_j\\right) - \\sigma$.\n5.  Estimate the variance as the empirical variance of the estimates, using a divisor of $M$ as specified: $\\text{variance} = \\frac{1}{M} \\sum_{j=1}^M (\\widehat{\\sigma}_j - \\overline{\\widehat{\\sigma}})^2$, where $\\overline{\\widehat{\\sigma}} = \\frac{1}{M}\\sum_j \\widehat{\\sigma}_j$.\n6.  Calculate the theoretical asymptotic variance as $\\text{asymptotic\\_variance} = \\sigma^2 / (2n)$.\n7.  The final output for each case is the triplet $[\\text{bias}, \\text{variance}, \\text{asymptotic\\_variance}]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(n, delta, mu, sigma, M, seed):\n    \"\"\"\n    Runs the Monte Carlo simulation for a single test case to assess the\n    properties of the MLE for volatility in a GBM model.\n\n    Args:\n        n (int): Number of observations in each sample path.\n        delta (float): Time step between observations.\n        mu (float): Drift parameter of the GBM.\n        sigma (float): Volatility parameter of the GBM.\n        M (int): Number of Monte Carlo replications.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        list: A list containing [bias, variance, asymptotic_variance].\n    \"\"\"\n    # Use the modern recommended way to manage random state\n    rng = np.random.default_rng(seed)\n\n    # Pre-calculate true parameters of the normal distribution for log-returns\n    # R_i ~ N(nu, sd_R^2)\n    nu = delta * (mu - 0.5 * sigma**2)\n    sd_R = sigma * np.sqrt(delta)\n\n    # Array to store the M estimates of sigma\n    sigma_hats = np.zeros(M)\n    \n    # Generate all random numbers at once for efficiency\n    all_R = rng.normal(loc=nu, scale=sd_R, size=(M, n))\n\n    # Perform calculations for each of the M samples\n    for i in range(M):\n        R = all_R[i, :]\n        \n        # The MLE for the variance of returns, var(R) = (1/n) * sum((R_i - R_bar)^2)\n        # using ddof=0 for the MLE (divisor n)\n        var_R_mle = np.var(R, ddof=0)\n        \n        # The MLE for sigma^2 is derived from var_R_mle = sigma_hat^2 * delta\n        sigma_sq_hat = var_R_mle / delta\n        \n        # By invariance property of MLEs, hat(sigma) = sqrt(hat(sigma^2))\n        sigma_hats[i] = np.sqrt(sigma_sq_hat)\n\n    # 3. Compute the Monte Carlo estimate of the bias\n    bias = np.mean(sigma_hats) - sigma\n\n    # 4. Compute the Monte Carlo estimate of the variance\n    # The problem specifies using divisor M, which corresponds to ddof=0\n    variance = np.var(sigma_hats, ddof=0)\n\n    # 5. Compute the theoretical asymptotic variance from Fisher Information\n    # Asymptotic Var(hat(sigma)) = sigma^2 / (2*n)\n    asymptotic_variance = sigma**2 / (2 * n)\n    \n    return [bias, variance, asymptotic_variance]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, Delta, mu, sigma, M, seed)\n        (252, 1/252.0, 0.08, 0.25, 20000, 12345),  # Case A\n        (5, 1.0, 0.00, 0.50, 200000, 67890),    # Case B\n        (1000, 1/252.0, 0.02, 0.20, 5000, 54321), # Case C\n    ]\n\n    results = []\n    for case in test_cases:\n        n, delta, mu, sigma, M, seed = case\n        result = run_simulation(n, delta, mu, sigma, M, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The str() representation of a list is '[item1, item2, ...]', which matches the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}