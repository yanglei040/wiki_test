{
    "hands_on_practices": [
        {
            "introduction": "Before diving into the statistical properties of a simulation, it is crucial to consider its computational efficiency. This practice explores the trade-off between two exact methods for simulating a homogeneous Poisson process: generating interarrival times sequentially versus first determining the total count and then placing events uniformly. By modeling the computational cost based on the expected number of events, $\\mu = \\lambda T$, you will learn to devise a hybrid rule that selects the optimal algorithm, a fundamental skill in designing scalable simulations .",
            "id": "3342349",
            "problem": "You are to study two exact simulation algorithms for the homogeneous Poisson process on the interval $\\left(0,T\\right]$ with rate $\\lambda$, and design a hybrid selection rule that chooses the faster algorithm based on a principled computational cost model. Then, you must validate that the hybrid rule is empirically optimal under a Monte Carlo proxy for computational cost. Your program must implement the algorithms and the validation and output the results for a specified test suite in the required format.\n\nThe two methods to simulate the event times of a homogeneous Poisson process are as follows, stated in purely mathematical terms.\n\n- Sequential Exponential Generation (SEG): Generate independent inter-arrival times $E_1,E_2,\\dots$ where each $E_i \\sim \\text{Exponential}(\\lambda)$, form cumulative sums $S_k = \\sum_{i=1}^{k} E_i$, and stop at the first index $M$ such that $S_M > T$. The event times are $\\{S_k : S_k \\le T\\}$. By the definition of a homogeneous Poisson process, the count $N_T$ of events in $\\left(0,T\\right]$ has distribution $N_T \\sim \\text{Poisson}(\\lambda T)$ and the number of exponential variates generated equals $M = N_T + 1$ almost surely.\n\n- Count-Then-Sort (CTS): Generate $N \\sim \\text{Poisson}(\\lambda T)$, then generate $N$ independent Uniform variates $U_1,\\dots,U_N \\overset{\\text{iid}}{\\sim} \\text{Uniform}(0,T)$ and sort them to obtain the ordered event times.\n\nTo compare computational performance in a machine-independent manner, use a proxy cost model that counts fundamental operations with tunable constants representing hardware performance and implementation efficiency.\n\n- SEG proxy cost: Model the cost for one run as $C_{\\text{SEG}} = c_s \\cdot M = c_s \\cdot (N_T + 1)$, where $c_s > 0$ is an abstract constant that aggregates the per-step cost of sampling one exponential variate and updating the partial sum and comparison.\n\n- CTS proxy cost: Model the cost for one run as $C_{\\text{CTS}} = c_P + c_U \\cdot N + a \\cdot N \\log_2(\\max\\{N,1\\}) + b \\cdot N$, where $c_P \\ge 0$ models a fixed overhead to draw one Poisson count, $c_U \\ge 0$ models a per-uniform sampling cost, $a \\ge 0$ models the dominant $O(N \\log N)$ cost of sorting (with logarithm base $2$), and $b \\ge 0$ captures linear-time sorting overhead such as element moves. When $N \\in \\{0,1\\}$, define the $N \\log_2(\\max\\{N,1\\})$ term to be $0$.\n\nStarting from the characterization $N_T \\sim \\text{Poisson}(\\lambda T)$ and the exponential inter-arrival construction, derive an expected-cost approximation that depends only on the product $\\mu = \\lambda T$ and the constants $c_s, c_P, c_U, a, b$. Use the approximation $\\mathbb{E}[N_T] \\approx \\mu$ and $\\mathbb{E}[N_T \\log_2(N_T)] \\approx \\mu \\log_2(\\max\\{\\mu,1\\})$ to obtain a first-order rule-of-thumb for the expected costs:\n$$\n\\bar{C}_{\\text{SEG}}(\\mu) \\approx c_s \\cdot (\\mu + 1), \\quad\n\\bar{C}_{\\text{CTS}}(\\mu) \\approx c_P + c_U \\cdot \\mu + a \\cdot \\mu \\log_2(\\max\\{\\mu,1\\}) + b \\cdot \\mu.\n$$\nPropose a hybrid rule that selects the algorithm with the smaller approximated expected cost. This defines a phase transition in $\\mu$ and the hardware constants: for small $\\mu$ and expensive sorts, SEG may be preferred; for larger $\\mu$ and fast sorting with cheap uniform generation, CTS may be preferred, especially when $c_s$ is large relative to $a$.\n\nEmpirical validation protocol. For each parameter set in the test suite, perform $R$ Monte Carlo replications using a fixed seed to eliminate randomness in output formatting. In each replication, draw $N \\sim \\text{Poisson}(\\mu)$ and evaluate the proxy costs:\n$$\nC_{\\text{SEG}} = c_s \\cdot (N + 1), \\quad\nC_{\\text{CTS}} = c_P + c_U \\cdot N + a \\cdot N \\log_2(\\max\\{N,1\\}) + b \\cdot N.\n$$\nAverage over the $R$ replications to obtain empirical means $\\hat{C}_{\\text{SEG}}$ and $\\hat{C}_{\\text{CTS}}$. Define the empirically optimal method as the one with the smaller empirical mean cost (break ties in favor of SEG). Validate the hybrid rule by returning, for each test case, a boolean that is true if and only if the hybrid choice matches the empirically optimal method.\n\nTest suite. Use the five parameter sets below to explore the phase transition by varying $\\mu = \\lambda T$ and the hardware constants. For every case, use $R = 200000$ replications and the same fixed random seed.\n\n- Case $1$: $\\lambda = 0.001$, $T = 50.0$, $c_s = 1.0$, $c_P = 5.0$, $c_U = 1.0$, $a = 1.0$, $b = 0.0$.\n\n- Case $2$: $\\lambda = 1.0$, $T = 1.0$, $c_s = 1.0$, $c_P = 5.0$, $c_U = 1.0$, $a = 0.5$, $b = 0.0$.\n\n- Case $3$: $\\lambda = 2.0$, $T = 10.0$, $c_s = 1.0$, $c_P = 5.0$, $c_U = 1.0$, $a = 2.0$, $b = 0.0$.\n\n- Case $4$: $\\lambda = 2.0$, $T = 10.0$, $c_s = 1.0$, $c_P = 2.0$, $c_U = 0.2$, $a = 0.05$, $b = 0.0$.\n\n- Case $5$: $\\lambda = 100.0$, $T = 10.0$, $c_s = 10.0$, $c_P = 2.0$, $c_U = 0.2$, $a = 0.02$, $b = 0.0$.\n\nYour task. Implement a program that:\n\n- Computes the hybrid selection based on the approximated expected costs for each case.\n\n- Performs the empirical validation with $R = 200000$ replications per case, using a fixed seed for the pseudo-random number generator so that the output is deterministic.\n\n- For each case, returns a boolean indicating whether the hybrid choice equals the empirically optimal choice.\n\nFinal output format. Your program should produce a single line of output containing the five booleans as a comma-separated list enclosed in square brackets, for example, $[\\text{True},\\text{False},\\text{True},\\text{True},\\text{True}]$. The order of results must follow Cases $1$ through $5$ as listed above. No additional text is allowed in the output.",
            "solution": "The problem requires an analysis of two algorithms for simulating a homogeneous Poisson process on the interval $(0, T]$ with a constant rate $\\lambda > 0$: Sequential Exponential Generation (SEG) and Count-Then-Sort (CTS). The objective is to develop a hybrid selection rule based on a theoretical cost model and to validate this rule against an empirical cost evaluation obtained via Monte Carlo simulation.\n\nThe number of events $N_T$ in the interval $(0, T]$ follows a Poisson distribution with mean $\\mu = \\lambda T$, i.e., $N_T \\sim \\text{Poisson}(\\mu)$. This random variable is central to the computational cost of both algorithms.\n\nFirst, we formalize the theoretical cost models and the hybrid selection rule. The problem provides proxy cost functions for a single simulation run:\n- For SEG, the cost is $C_{\\text{SEG}} = c_s \\cdot (N_T + 1)$, where $N_T$ is the number of events realized in that run. The cost is proportional to the number of exponential variates generated, which is $N_T + 1$.\n- For CTS, the cost is $C_{\\text{CTS}} = c_P + c_U \\cdot N_T + a \\cdot N_T \\log_2(\\max\\{N_T,1\\}) + b \\cdot N_T$. This cost comprises a fixed component $c_P$ for sampling the Poisson count, a linear component for sampling $N_T$ uniform variates, and a quasi-linear component for sorting them, which is dominated by the $N \\log N$ term. The constants $c_s, c_P, c_U, a, b$ are non-negative and depend on the specific computational environment.\n\nTo derive a selection rule that is independent of a single random outcome, we compare the expected costs of the two algorithms.\nThe expected cost of SEG is calculated exactly:\n$$\n\\mathbb{E}[C_{\\text{SEG}}] = \\mathbb{E}[c_s \\cdot (N_T + 1)] = c_s \\cdot (\\mathbb{E}[N_T] + 1) = c_s(\\mu + 1)\n$$\nWe define this as the theoretical cost estimate $\\bar{C}_{\\text{SEG}}(\\mu) = c_s(\\mu + 1)$.\n\nThe expected cost of CTS is:\n$$\n\\mathbb{E}[C_{\\text{CTS}}] = \\mathbb{E}[c_P + c_U N_T + a N_T \\log_2(\\max\\{N_T,1\\}) + b N_T] \\\\\n= c_P + (c_U + b)\\mathbb{E}[N_T] + a\\mathbb{E}[N_T \\log_2(\\max\\{N_T,1\\})] \\\\\n= c_P + (c_U + b)\\mu + a\\mathbb{E}[N_T \\log_2(\\max\\{N_T,1\\})]\n$$\nThe term $\\mathbb{E}[N_T \\log_2(\\max\\{N_T,1\\})]$ does not have a simple closed-form expression. The problem suggests a first-order approximation based on Jensen's inequality, $\\mathbb{E}[f(X)] \\approx f(\\mathbb{E}[X])$. Applying this, we get:\n$$\n\\mathbb{E}[N_T \\log_2(\\max\\{N_T,1\\})] \\approx \\mu \\log_2(\\max\\{\\mu,1\\})\n$$\nThis yields the approximated expected cost for CTS:\n$$\n\\bar{C}_{\\text{CTS}}(\\mu) \\approx c_P + (c_U + b)\\mu + a \\mu \\log_2(\\max\\{\\mu,1\\})\n$$\nThe hybrid selection rule is to choose the algorithm with the lower approximated expected cost. We select SEG if $\\bar{C}_{\\text{SEG}}(\\mu) \\le \\bar{C}_{\\text{CTS}}(\\mu)$, and CTS otherwise. This establishes a \"hybrid choice\" based purely on the parameters $\\mu, c_s, c_P, c_U, a, b$.\n\nNext, we design the empirical validation protocol. The purpose is to determine the \"empirically optimal\" method by estimating the true expected costs, $\\mathbb{E}[C_{\\text{SEG}}]$ and $\\mathbb{E}[C_{\\text{CTS}}]$, with high accuracy using a Monte Carlo method. For each test case, we perform $R=200000$ replications. A fixed pseudo-random number generator seed is used to ensure the reproducibility of the results.\nThe procedure for each test case is as follows:\n1. Generate a vector of $R$ independent random variates, $\\{N_i\\}_{i=1}^R$, where each $N_i \\sim \\text{Poisson}(\\mu)$.\n2. For each $N_i$, calculate the cost that would have been incurred by each algorithm:\n   - $C_{\\text{SEG}, i} = c_s(N_i + 1)$\n   - $C_{\\text{CTS}, i} = c_P + c_U N_i + a N_i \\log_2(\\max\\{N_i,1\\}) + b N_i$\n3. Compute the sample means of these costs, which are the Monte Carlo estimates of the true expected costs:\n   - $\\hat{C}_{\\text{SEG}} = \\frac{1}{R} \\sum_{i=1}^R C_{\\text{SEG}, i}$\n   - $\\hat{C}_{\\text{CTS}} = \\frac{1}{R} \\sum_{i=1}^R C_{\\text{CTS}, i}$\nBy the Law of Large Numbers, for large $R$, $\\hat{C}_{\\text{SEG}} \\approx \\mathbb{E}[C_{\\text{SEG}}]$ and $\\hat{C}_{\\text{CTS}} \\approx \\mathbb{E}[C_{\\text{CTS}}]$.\nThe empirically optimal method is determined by comparing these sample means. Consistent with the problem's tie-breaking rule, we choose SEG if $\\hat{C}_{\\text{SEG}} \\le \\hat{C}_{\\text{CTS}}$, and CTS otherwise.\n\nFinally, for each test case, we compare the hybrid choice (pre-computation) with the empirically optimal choice. A boolean result of `True` signifies that the simple first-order approximation was sufficient to make the correct decision, while `False` would indicate that higher-order effects, neglected by the approximation, are significant enough to alter the outcome. The implementation will be performed using `numpy` for efficient, vectorized computation over the $R$ replications.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Validates a hybrid algorithm selection rule for simulating a homogeneous Poisson process.\n\n    The function iterates through a suite of test cases. For each case, it:\n    1.  Calculates a theoretical choice of algorithm (SEG vs. CTS) based on an\n        approximated expected-cost model.\n    2.  Determines the empirically optimal algorithm by running a Monte Carlo\n        simulation to estimate the average costs of each method.\n    3.  Compares the theoretical choice to the empirical choice and records\n        whether they match.\n\n    The final output is a list of booleans representing the result of this\n    comparison for each test case.\n    \"\"\"\n    \n    # Test suite as defined in the problem statement\n    test_cases = [\n        # Case 1: (lambda, T, c_s, c_P, c_U, a, b)\n        (0.001, 50.0, 1.0, 5.0, 1.0, 1.0, 0.0),\n        # Case 2\n        (1.0, 1.0, 1.0, 5.0, 1.0, 0.5, 0.0),\n        # Case 3\n        (2.0, 10.0, 1.0, 5.0, 1.0, 2.0, 0.0),\n        # Case 4\n        (2.0, 10.0, 1.0, 2.0, 0.2, 0.05, 0.0),\n        # Case 5\n        (100.0, 10.0, 10.0, 2.0, 0.2, 0.02, 0.0),\n    ]\n\n    R = 200000  # Number of Monte Carlo replications\n    SEED = 0 # Fixed seed for deterministic output\n\n    results = []\n\n    for case in test_cases:\n        lambda_rate, T, c_s, c_P, c_U, a, b = case\n        \n        mu = lambda_rate * T\n\n        # Step 1: Hybrid Rule Prediction (Theoretical Choice)\n        c_seg_approx = c_s * (mu + 1)\n        \n        # The term a*mu*log2(max{mu,1}) is zero if mu = 1\n        log_term_approx = 0.0\n        if mu > 1:\n            log_term_approx = a * mu * np.log2(mu)\n        \n        c_cts_approx = c_P + (c_U + b) * mu + log_term_approx\n        \n        hybrid_choice_is_seg = (c_seg_approx = c_cts_approx)\n\n        # Step 2: Empirical Validation\n        # Use a new random number generator with a fixed seed for each independent case\n        rng = np.random.default_rng(SEED)\n        \n        # Generate R Poisson-distributed samples for the event count N\n        n_samples = rng.poisson(mu, R)\n        \n        # Calculate empirical costs for SEG (vectorized)\n        cost_seg_samples = c_s * (n_samples + 1)\n        \n        # Calculate empirical costs for CTS (vectorized)\n        # The N*log2(max{N,1}) term is 0 for N in {0, 1}.\n        log_term_samples = np.zeros_like(n_samples, dtype=float)\n        mask = n_samples > 1\n        log_term_samples[mask] = n_samples[mask] * np.log2(n_samples[mask])\n        \n        cost_cts_samples = c_P + (c_U + b) * n_samples + a * log_term_samples\n        \n        # Compute the mean costs over R replications\n        c_seg_empirical = np.mean(cost_seg_samples)\n        c_cts_empirical = np.mean(cost_cts_samples)\n        \n        # Step 3: Determine Empirically Optimal Choice\n        # Tie-breaking rule: choose SEG if costs are equal\n        empirical_choice_is_seg = (c_seg_empirical = c_cts_empirical)\n        \n        # Step 4: Validate Hybrid Rule\n        # The result is True if the theoretical choice matches the empirical one\n        validation_result = (hybrid_choice_is_seg == empirical_choice_is_seg)\n        results.append(validation_result)\n\n    # Print the final list of boolean results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A critical step in any simulation study is validating that the implemented generator behaves according to its theoretical specification. For a homogeneous Poisson process, the interarrival times must follow an exponential distribution. This exercise guides you through constructing a principled goodness-of-fit test using the Kolmogorov-Smirnov statistic, addressing the subtle but important issue that arises when the rate parameter $\\lambda$ is estimated from the data itself by using a parametric bootstrap to obtain a valid $p$-value .",
            "id": "3342341",
            "problem": "You are asked to design and implement a principled goodness-of-fit procedure for assessing whether simulated interarrival times arise from a homogeneous Poisson process by testing their exponentiality. The context is a homogeneous Poisson process with constant rate $\\lambda  0$. The fundamental base for this problem is the definition that a homogeneous Poisson process has independent and stationary increments and that its interarrival times are independent and identically distributed exponential random variables with rate $\\lambda$. The test uses the Kolmogorov–Smirnov statistic after scaling by an estimated rate, and calibration of the rate must be performed from the simulated interarrival data themselves.\n\nPrinciples to use:\n- A homogeneous Poisson process with rate $\\lambda$ has interarrival times $\\{X_i\\}_{i=1}^n$ that are independent and identically distributed with the exponential distribution of rate $\\lambda$, meaning the cumulative distribution function is $F_{\\lambda}(x) = 1 - e^{-\\lambda x}$ for $x \\ge 0$.\n- The Kolmogorov–Smirnov (KS) statistic compares an empirical cumulative distribution function to a hypothesized cumulative distribution function via a uniform supremum of their absolute difference. The KS statistic is distribution-free when the hypothesized cumulative distribution function is fully specified and no parameters are estimated from the data.\n- Estimating $\\lambda$ from the same data introduces dependence between the empirical cumulative distribution function and the hypothesized cumulative distribution function under the null, invalidating the naive use of tabulated KS critical values. To address this, use a parametric bootstrap under the null hypothesis where the rate is calibrated from the data.\n\nYour program must implement the following steps for each test case:\n1. Simulate $n$ interarrival times according to the specified data-generating mechanism, using the provided rate and distribution specification. All times are to be treated as dimensionless durations; no physical unit conversions are required.\n2. Calibrate the rate $\\lambda$ from the simulated interarrival times using a statistically sound method. Estimation must be based on the interarrival sample itself. No external information or hints may be used. The calibrated rate is to be denoted $\\widehat{\\lambda}$ and must be computed via a method consistent with the properties of the exponential family based on first principles.\n3. Construct the Kolmogorov–Smirnov statistic for testing exponentiality after scaling by $\\widehat{\\lambda}$ as follows:\n   - Scale the interarrival times by $\\widehat{\\lambda}$, i.e., form $Y_i = \\widehat{\\lambda} X_i$ for $i = 1, \\ldots, n$.\n   - Define the empirical cumulative distribution function $\\widehat{F}_n(y)$ of $\\{Y_i\\}_{i=1}^n$.\n   - Define the hypothesized cumulative distribution function $F_0(y) = 1 - e^{-y}$, which corresponds to the exponential distribution with rate $1$.\n   - Compute the KS statistic $D_n = \\sup_{y \\ge 0} \\left| \\widehat{F}_n(y) - F_0(y) \\right|$.\n4. To obtain a valid $p$-value accounting for the rate estimated from the data, implement a parametric bootstrap under the null hypothesis of exponential interarrivals:\n   - Generate $B$ bootstrap samples of size $n$ from an exponential distribution with rate $\\widehat{\\lambda}$.\n   - For each bootstrap sample, re-estimate the rate $\\widehat{\\lambda}^{(b)}$ from that sample, scale by $\\widehat{\\lambda}^{(b)}$, and compute its KS statistic $D_n^{(b)}$ against $F_0$.\n   - The bootstrap $p$-value is the fraction of bootstrap statistics greater than or equal to the observed statistic, i.e., $p = \\frac{1}{B} \\sum_{b=1}^B \\mathbf{1}\\{D_n^{(b)} \\ge D_n\\}$.\n5. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order of the test suite, with each entry being the bootstrap $p$-value as a floating-point number rounded to six decimals.\n\nTest suite:\n- Case $1$ (happy path): Exponential interarrivals with true rate $\\lambda = 2$ and sample size $n = 500$, using $B = 400$ bootstrap replicates.\n- Case $2$ (model misspecification): Gamma interarrivals with shape $k = 2$ and rate $\\beta = 2 \\lambda$ so that the mean equals $1/\\lambda$, with true $\\lambda = 2$, sample size $n = 500$, using $B = 400$ bootstrap replicates.\n- Case $3$ (small-sample boundary): Exponential interarrivals with true rate $\\lambda = 1$ and sample size $n = 20$, using $B = 400$ bootstrap replicates.\n- Case $4$ (contamination): Mixture of exponentials where with probability $p = 0.3$ the rate is $3 \\lambda$ and with probability $1 - p$ the rate is $\\lambda$, with true $\\lambda = 3$, sample size $n = 500$, using $B = 400$ bootstrap replicates.\n- Case $5$ (large sample): Exponential interarrivals with true rate $\\lambda = 5$ and sample size $n = 2000$, using $B = 300$ bootstrap replicates.\n\nRandomness and reproducibility:\n- Use fixed seeds per test case to ensure reproducibility of the simulated data and the bootstrap procedure. The seed values are $1$, $2$, $3$, $4$, and $5$ for the five cases, respectively.\n\nFinal output format:\n- Your program should produce a single line of output containing the five bootstrap $p$-values for the above test suite, in order, rounded to six decimals, as a comma-separated list enclosed in square brackets (e.g., \"[0.532000,0.041000,0.872000,0.010000,0.619000]\").",
            "solution": "The problem of validating whether a sequence of event times originates from a homogeneous Poisson process is fundamentally a goodness-of-fit problem. The guiding principle is a core property of such processes: the interarrival times, $\\{X_i\\}_{i=1}^n$, are independent and identically distributed (i.i.d.) random variables following an exponential distribution. A homogeneous Poisson process is characterized by a constant rate parameter, $\\lambda  0$, and the cumulative distribution function (CDF) of its interarrival times is given by $F_{\\lambda}(x) = 1 - e^{-\\lambda x}$ for any time $x \\ge 0$. The task is to design a procedure that tests the null hypothesis, $H_0$, that a given sample of $n$ interarrival times $\\{X_i\\}_{i=1}^n$ is drawn from an exponential distribution with some unknown rate $\\lambda$.\n\nThe Kolmogorov-Smirnov (KS) test is a standard method for testing goodness-of-fit. It compares the empirical cumulative distribution function (ECDF) of the data, denoted $\\widehat{F}_n(x)$, to the hypothesized CDF, $F(x)$. The ECDF is defined as the proportion of the sample less than or equal to $x$: $\\widehat{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le x\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The KS test statistic, $D_n$, is the maximum absolute difference between these two functions over all possible values of $x$: $D_n = \\sup_{x} |\\widehat{F}_n(x) - F(x)|$.\n\nA critical complication arises because the rate parameter $\\lambda$ is unknown and must be estimated from the same data being tested. This procedure violates the assumptions of the standard KS test, whose null distribution is only distribution-free if the hypothesized CDF is fully specified without reference to the data. When parameters are estimated, the ECDF tends to be closer to the estimated CDF than it would be to the true one, leading to an artificially smaller test statistic. This effect invalidates the tabulated critical values for the standard KS test.\n\nTo construct a valid test, we first transform the problem to a parameter-free space. If the $X_i$ are indeed exponential with rate $\\lambda$, then the scaled variables $Y_i = \\lambda X_i$ are exponential with rate $1$ (a standard exponential distribution). Since $\\lambda$ is unknown, we use a principled estimate, $\\widehat{\\lambda}$, derived from the data. The maximum likelihood estimator (MLE) for the rate parameter of an exponential distribution is the reciprocal of the sample mean, $\\widehat{\\lambda} = 1 / \\overline{X} = n / (\\sum_{i=1}^n X_i)$. We then form the scaled sample $\\{Y_i = \\widehat{\\lambda} X_i\\}_{i=1}^n$ and test whether it follows a standard exponential distribution with CDF $F_0(y) = 1-e^{-y}$. The test statistic becomes $D_n = \\sup_{y \\ge 0} |\\widehat{F}_n(y) - F_0(y)|$, where $\\widehat{F}_n(y)$ is the ECDF of the scaled data $\\{Y_i\\}$. This is a form of the Lilliefors test for the exponential distribution.\n\nThe null distribution of this new statistic, $D_n$, is still not the standard KS distribution because of the use of the estimated parameter $\\widehat{\\lambda}$ in the scaling. To correctly assess the statistical significance of an observed value of $D_n$, we must determine its distribution under the null hypothesis. The parametric bootstrap provides a robust, computationally-driven method for this. The procedure simulates the null distribution by treating our estimated model as the true data-generating process.\n\nThe algorithm is as follows:\n1.  **Data Generation and Initial Analysis**: For a given test case, generate the primary data sample $\\{X_i\\}_{i=1}^n$ of size $n$ according to the specified mechanism.\n2.  **Rate Estimation**: Compute the MLE of the rate from this sample: $\\widehat{\\lambda} = n / \\sum_{i=1}^n X_i$.\n3.  **Observed Statistic**: Scale the data to obtain $\\{Y_i = \\widehat{\\lambda} X_i\\}_{i=1}^n$ and compute the observed KS statistic $D_n = \\sup_{y \\ge 0} |\\widehat{F}_n(y) - F_0(y)|$ against the standard exponential CDF.\n4.  **Parametric Bootstrap**: To generate the null distribution of the test statistic, perform the following steps for $b = 1, \\dots, B$, where $B$ is the number of bootstrap replicates:\n    a. Generate a bootstrap sample $\\{X_i^{(b)}\\}_{i=1}^n$ by drawing $n$ i.i.d. values from an exponential distribution with rate $\\widehat{\\lambda}$. This simulates a new dataset under the assumption that $H_0$ is true and our best estimate of the rate is correct.\n    b. For this bootstrap sample, repeat the entire analysis: compute its own rate estimate, $\\widehat{\\lambda}^{(b)} = n / \\sum_{i=1}^n X_i^{(b)}$; scale its data to get $\\{Y_i^{(b)} = \\widehat{\\lambda}^{(b)} X_i^{(b)}\\}_{i=1}^n$; and compute the corresponding KS statistic, $D_n^{(b)}$.\n5.  **p-value Calculation**: The collection of bootstrap statistics $\\{D_n^{(b)}\\}_{b=1}^B$ serves as an empirical approximation of the null distribution of our test statistic. The bootstrap $p$-value is the proportion of these simulated statistics that are greater than or equal to the originally observed statistic: $p = \\frac{1}{B} \\sum_{b=1}^B \\mathbf{1}\\{D_n^{(b)} \\ge D_n\\}$. A small $p$-value (e.g., $ 0.05$) indicates that the observed data are unlikely to have arisen from an exponential distribution, thus providing evidence against the hypothesis of a homogeneous Poisson process.\n\nThis procedure correctly accounts for the effect of parameter estimation and provides a valid goodness-of-fit test. The implementation will use `scipy.stats.kstest` to compute the KS statistic against the standard exponential distribution ('expon'), which has a rate of $1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    def estimate_rate(X):\n        \"\"\"\n        Computes the Maximum Likelihood Estimator (MLE) for the rate parameter\n        of an exponential distribution.\n        \"\"\"\n        # A small epsilon is added to avoid division by zero if mean is zero,\n        # which is practically impossible for non-empty positive-valued data.\n        return 1.0 / (np.mean(X) + np.finfo(float).eps)\n\n    def compute_ks_statistic(X, lambda_hat):\n        \"\"\"\n        Computes the Kolmogorov-Smirnov statistic for exponentiality after\n        scaling by the estimated rate.\n        \"\"\"\n        # Scale the data by the estimated rate.\n        Y = lambda_hat * X\n        # Perform KS test against the standard exponential distribution (rate=1),\n        # which is 'expon' in SciPy. We only need the statistic, not the p-value.\n        statistic, _ = kstest(Y, 'expon')\n        return statistic\n\n    def run_goodness_of_fit_test(case_params):\n        \"\"\"\n        Executes the entire goodness-of-fit procedure for a single test case.\n        \"\"\"\n        case_id = case_params['id']\n        dist_type = case_params['dist_type']\n        params = case_params['params']\n        n = case_params['n']\n        B = case_params['B']\n        seed = case_params['seed']\n\n        # Initialize Random Number Generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # 1. Simulate initial data sample\n        if dist_type == 'exponential':\n            true_lambda = params['lambda']\n            X = rng.exponential(scale=1.0/true_lambda, size=n)\n        elif dist_type == 'gamma':\n            k, beta = params['k'], params['beta']\n            X = rng.gamma(shape=k, scale=1.0/beta, size=n)\n        elif dist_type == 'mixture':\n            p, lambda1, lambda2 = params['p'], params['lambda1'], params['lambda2']\n            rates = np.where(rng.random(size=n)  p, lambda1, lambda2)\n            X = rng.exponential(scale=1.0/rates, size=n)\n        else:\n            raise ValueError(f\"Unknown distribution type: {dist_type}\")\n\n        # 2. Calibrate rate from the data\n        lambda_hat = estimate_rate(X)\n\n        # 3. Compute observed KS statistic\n        D_n_observed = compute_ks_statistic(X, lambda_hat)\n\n        # 4. Perform parametric bootstrap to find the p-value\n        bootstrap_stats = np.zeros(B)\n        for b in range(B):\n            # Generate a bootstrap sample under the null hypothesis (Exp(lambda_hat))\n            X_b = rng.exponential(scale=1.0/lambda_hat, size=n)\n            \n            # Re-estimate rate for the bootstrap sample\n            lambda_hat_b = estimate_rate(X_b)\n            \n            # Compute KS statistic for the bootstrap sample\n            D_n_b = compute_ks_statistic(X_b, lambda_hat_b)\n            bootstrap_stats[b] = D_n_b\n\n        # 5. Calculate bootstrap p-value\n        p_value = np.sum(bootstrap_stats = D_n_observed) / B\n        \n        return p_value\n\n    # Define the test suite from the problem statement.\n    test_cases = [\n        {'id': 1, 'dist_type': 'exponential', 'params': {'lambda': 2}, 'n': 500, 'B': 400, 'seed': 1},\n        {'id': 2, 'dist_type': 'gamma', 'params': {'k': 2, 'beta': 4}, 'n': 500, 'B': 400, 'seed': 2},\n        {'id': 3, 'dist_type': 'exponential', 'params': {'lambda': 1}, 'n': 20, 'B': 400, 'seed': 3},\n        {'id': 4, 'dist_type': 'mixture', 'params': {'p': 0.3, 'lambda1': 9, 'lambda2': 3}, 'n': 500, 'B': 400, 'seed': 4},\n        {'id': 5, 'dist_type': 'exponential', 'params': {'lambda': 5}, 'n': 2000, 'B': 300, 'seed': 5},\n    ]\n\n    results = []\n    for case in test_cases:\n        p_val = run_goodness_of_fit_test(case)\n        results.append(p_val)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond conforming to the correct marginal distribution, the interarrival times of a homogeneous Poisson process must also be statistically independent. This practice complements the previous goodness-of-fit test by focusing on this second crucial property. You will implement a portmanteau test, such as the Ljung-Box test, to detect any unintended serial correlation in your simulated sequence, thus completing a robust validation of the i.i.d. assumption underlying your simulator .",
            "id": "3342434",
            "problem": "A homogeneous Poisson process is defined by events occurring in time such that the number of events in any interval of length $t$ is Poisson distributed with mean $\\lambda t$, and the increments over disjoint intervals are independent. Equivalently, the interarrival times form an independent and identically distributed sequence of exponential random variables with rate $\\lambda$. Suppose you have implemented a simulator intended to produce interarrival sequences for a homogeneous Poisson process, but you wish to detect unintended dependence (correlation) between successive interarrival times. Your task is to design and implement a statistical test that uses sample autocorrelations of the interarrival sequence to detect such inadvertent correlation and to justify the expected null distribution under independence.\n\nStarting from fundamental definitions and well-tested facts, propose a test that aggregates a finite number of sample autocorrelations of the interarrival sequence and obtains a decision using its asymptotic null distribution when the interarrivals are independent and identically distributed with finite variance. Then implement the test as a complete, runnable program that:\n- Simulates interarrival sequences according to specified parameter sets.\n- Computes the sample autocorrelations up to a given maximum lag.\n- Aggregates these autocorrelations into a single test statistic.\n- Computes a $p$-value using the asymptotic null distribution justified under independence.\n- Outputs, for each test case, a boolean that is true if the null hypothesis of independence is rejected at a specified significance level and false otherwise.\n\nThe interarrival times must be generated in units of time, but no physical unit conversion is required in this problem. Angles are not involved. Significance levels must be expressed as decimals (for example, $0.05$), not with percentage signs.\n\nTest suite and parameter specification:\n- Each test case is a tuple of parameters $(\\lambda, n, m, \\alpha, \\text{mode}, c, \\text{seed})$, where $\\lambda$ is the rate parameter for the exponential interarrivals, $n$ is the sequence length, $m$ is the maximum lag for sample autocorrelations, $\\alpha$ is the significance level as a decimal, $\\text{mode}$ specifies how the sequence is generated, $c$ is a correlation-mixing coefficient used in certain modes, and $\\text{seed}$ is the pseudorandom number generator seed for reproducibility.\n- The generation modes are:\n    - $\\text{mode} = \\text{\"independent\"}$: draw $n$ independent exponential random variables with rate $\\lambda$.\n    - $\\text{mode} = \\text{\"mix\\_prev\"}$: draw $n$ independent exponential random variables $(Z_t)$ with rate $\\lambda$, and set $X_0 = Z_0$, $X_t = (1-c) Z_t + c Z_{t-1}$ for $t \\ge 1$, where $0 \\le c  1$; this introduces positive lag-$1$ correlation while keeping the same mean scale.\n- Use the following four test cases to exercise different facets of the solution:\n    1. $(\\lambda = 1.0, n = 4000, m = 10, \\alpha = 0.05, \\text{mode} = \\text{\"independent\"}, c = 0.0, \\text{seed} = 20231101)$: a large-sample independent case (happy path).\n    2. $(\\lambda = 1.0, n = 3000, m = 10, \\alpha = 0.05, \\text{mode} = \\text{\"mix\\_prev\"}, c = 0.7, \\text{seed} = 20231102)$: a moderate correlation case that should be detectable.\n    3. $(\\lambda = 2.0, n = 60, m = 8, \\alpha = 0.05, \\text{mode} = \\text{\"independent\"}, c = 0.0, \\text{seed} = 20231103)$: a small-sample independent case (boundary condition for the asymptotic approximation).\n    4. $(\\lambda = 1.0, n = 2000, m = 15, \\alpha = 0.01, \\text{mode} = \\text{\"mix\\_prev\"}, c = 0.99, \\text{seed} = 20231104)$: a strong correlation case with a stringent significance level (edge case).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$.\n- Each item $\\text{result}_i$ must be a boolean indicating whether the null hypothesis of independence is rejected at the specified level $\\alpha$ for the $i$-th test case.",
            "solution": "The problem requires the design and implementation of a statistical test to detect unintended serial correlation in a sequence of simulated interarrival times from a homogeneous Poisson process. The fundamental property of such a process is that the interarrival times form an independent and identically distributed (i.i.d.) sequence of exponential random variables. The proposed test will evaluate whether a given sequence of observations, $\\{X_t\\}_{t=1}^n$, deviates from this i.i.d. assumption by examining its sample autocorrelations.\n\nThe null hypothesis, $H_0$, is that the sequence $\\{X_t\\}_{t=1}^n$ is i.i.d. The alternative hypothesis, $H_a$, is that the sequence is not i.i.d., specifically that there exists some serial correlation, meaning the theoretical autocorrelation $\\rho(k) = \\text{Corr}(X_t, X_{t+k})$ is non-zero for at least one lag $k  0$.\n\nThe test chosen for this task is the Ljung-Box portmanteau test, a widely used and well-regarded method for detecting serial correlation in a time series. The design of this test proceeds from the definition of sample autocorrelations and their asymptotic properties under the null hypothesis.\n\nFirst, we define the necessary sample statistics for a given time series $X_1, X_2, \\dots, X_n$:\nThe sample mean is $\\bar{X} = \\frac{1}{n} \\sum_{t=1}^{n} X_t$.\nThe sample autocovariance at lag $k \\ge 0$ is defined as $\\hat{\\gamma}(k) = \\frac{1}{n} \\sum_{t=1}^{n-k} (X_t - \\bar{X})(X_{t+k} - \\bar{X})$.\nThe sample autocorrelation at lag $k \\ge 1$ is the normalized autocovariance: $\\hat{\\rho}(k) = \\frac{\\hat{\\gamma}(k)}{\\hat{\\gamma}(0)}$.\n\nThe justification for the test's null distribution is a cornerstone of time series analysis. A fundamental theorem, based on Bartlett's approximation, describes the large-sample behavior of sample autocorrelations of an i.i.d. sequence with finite variance. Under the null hypothesis that $\\{X_t\\}$ is an i.i.d. sequence (often referred to as white noise), for a sufficiently large sample size $n$, the sample autocorrelations $\\hat{\\rho}(k)$ for lags $k \\ge 1$ are approximately independent and normally distributed with a mean of $0$ and a variance of $1/n$. Symbolically,\n$$\n\\sqrt{n} \\hat{\\rho}(k) \\xrightarrow{d} \\mathcal{N}(0, 1) \\quad \\text{for } k \\ge 1\n$$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution and $\\mathcal{N}(0, 1)$ is the standard normal distribution.\n\nThis result allows for the construction of a test statistic that aggregates information across multiple lags. If each $\\sqrt{n} \\hat{\\rho}(k)$ is approximately a standard normal variate, then its square, $(\\sqrt{n} \\hat{\\rho}(k))^2 = n \\hat{\\rho}(k)^2$, is approximately a chi-squared variate with $1$ degree of freedom, $\\chi^2(1)$. Because the $\\hat{\\rho}(k)$ values for different lags are approximately independent under $H_0$, the sum of $m$ such squared terms will be approximately distributed as a chi-squared variate with $m$ degrees of freedom.\n\nThis principle leads to the Box-Pierce statistic, $Q_{BP} = n \\sum_{k=1}^{m} \\hat{\\rho}(k)^2$. A finite-sample refinement, which is more accurate for smaller $n$, is the Ljung-Box statistic, $Q_{LB}$. This is the statistic we will use. It is defined as:\n$$\nQ_{LB} = n(n+2) \\sum_{k=1}^{m} \\frac{\\hat{\\rho}(k)^2}{n-k}\n$$\nUnder the null hypothesis $H_0$, $Q_{LB}$ is asymptotically distributed as a chi-squared distribution with $m$ degrees of freedom, denoted $\\chi^2(m)$. The degrees of freedom, $m$, corresponds to the number of autocorrelations being tested.\n\nThe testing procedure is as follows:\n1. For a given time series of length $n$ and a chosen maximum lag $m$, compute the sample autocorrelations $\\hat{\\rho}(1), \\hat{\\rho}(2), \\dots, \\hat{\\rho}(m)$.\n2. Calculate the Ljung-Box statistic, $Q_{LB}$, using the formula above.\n3. Compute the $p$-value, which is the probability of observing a test statistic value as large as or larger than the one computed, assuming $H_0$ is true. This is given by $p = P(\\chi^2_m \\ge Q_{LB})$.\n4. Compare the $p$-value to a pre-specified significance level $\\alpha$. If $p  \\alpha$, we reject the null hypothesis of independence and conclude that there is significant evidence of serial correlation in the data. Otherwise, we fail to reject $H_0$.\n\nThe implementation will simulate time series data according to the two modes specified:\n- **`independent` mode**: The sequence $X_t$ is generated by drawing $n$ i.i.d. samples from an exponential distribution with rate parameter $\\lambda$. The probability density function is $f(x; \\lambda) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$. The scale parameter used in numerical libraries is the reciprocal of the rate, $\\beta = 1/\\lambda$. This mode should, by construction, satisfy the null hypothesis.\n- **`mix_prev` mode**: An underlying i.i.d. exponential sequence $\\{Z_t\\}$ with rate $\\lambda$ is first generated. The observed sequence $\\{X_t\\}$ is then constructed as $X_0 = Z_0$ and $X_t = (1-c)Z_t + cZ_{t-1}$ for $t \\ge 1$. This construction, a simple moving average process of order $1$ (MA(1)) applied to the i.i.d. innovations, intentionally introduces dependence between successive terms. For $c  0$, the lag-$1$ autocorrelation will be non-zero, violating the null hypothesis.\n\nFor each test case, the program will generate the sequence, compute the $Q_{LB}$ statistic, find the corresponding $p$-value from the $\\chi^2(m)$ distribution, and return a boolean indicating whether $H_0$ is rejected at the given significance level $\\alpha$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Designs and implements a statistical test to detect serial correlation\n    in simulated interarrival sequences of a homogeneous Poisson process.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda, n, m, alpha, mode, c, seed)\n        (1.0, 4000, 10, 0.05, \"independent\", 0.0, 20231101),\n        (1.0, 3000, 10, 0.05, \"mix_prev\", 0.7, 20231102),\n        (2.0, 60, 8, 0.05, \"independent\", 0.0, 20231103),\n        (1.0, 2000, 15, 0.01, \"mix_prev\", 0.99, 20231104),\n    ]\n\n    results = []\n    for case in test_cases:\n        lambda_val, n, m, alpha, mode, c, seed = case\n\n        # Set up the random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Generate the interarrival sequence\n        if mode == \"independent\":\n            # Generate n i.i.d. exponential random variables\n            x_seq = rng.exponential(scale=1.0 / lambda_val, size=n)\n        elif mode == \"mix_prev\":\n            # Generate underlying i.i.d. sequence Z\n            z_seq = rng.exponential(scale=1.0 / lambda_val, size=n)\n            # Create the correlated sequence X\n            x_seq = np.zeros_like(z_seq)\n            x_seq[0] = z_seq[0]\n            if n > 1:\n                x_seq[1:] = (1 - c) * z_seq[1:] + c * z_seq[:-1]\n        else:\n            raise ValueError(f\"Unknown mode: {mode}\")\n\n        # Step 2: Compute sample autocorrelations up to lag m\n        n_obs = len(x_seq)\n        x_mean = np.mean(x_seq)\n        x_centered = x_seq - x_mean\n        \n        # Autocovariance at lag 0\n        gamma0 = np.sum(x_centered**2) / n_obs\n        \n        # Avoid division by zero if variance is zero\n        if gamma0 == 0:\n            # If variance is zero, all autocorrelations are undefined or 0.\n            # No correlation to detect. Do not reject H0.\n            results.append(False)\n            continue\n\n        rhos = []\n        for k in range(1, m + 1):\n            # Autocovariance at lag k\n            gammak = np.sum(x_centered[:n_obs - k] * x_centered[k:]) / n_obs\n            # Autocorrelation at lag k\n            rhok = gammak / gamma0\n            rhos.append(rhok)\n        \n        rhos_sq = np.array(rhos)**2\n        \n        # Step 3: Compute the Ljung-Box test statistic\n        k_indices = np.arange(1, m + 1)\n        lb_stat = n_obs * (n_obs + 2) * np.sum(rhos_sq / (n_obs - k_indices))\n\n        # Step 4: Compute the p-value using the asymptotic chi-squared distribution\n        # The degrees of freedom is m, the number of lags tested.\n        p_value = chi2.sf(lb_stat, df=m)\n\n        # Step 5: Reject H0 if p-value is less than the significance level\n        reject_h0 = p_value  alpha\n        results.append(reject_h0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}