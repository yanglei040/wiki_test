{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, our first practice deconstructs the core of any SGHMC simulation: a single integration step. By analyzing the simple case of a one-dimensional quadratic potential, we can analytically determine how the system's state evolves over a discrete time step. This exercise provides a concrete understanding of how the deterministic forces and stochastic noise terms are combined by a common integrator, revealing the underlying mechanics of how SGHMC explores the state space .",
            "id": "3349051",
            "problem": "Consider a one-dimensional system with Hamiltonian dynamics at unit temperature for the quadratic potential energy function $U(q) = \\frac{A}{2} q^{2}$ with $A > 0$. Let $(q_{k}, p_{k}) \\in \\mathbb{R}^{2}$ denote the position and momentum at discrete time index $k$. The stochastic gradient Hamiltonian Monte Carlo (SGHMC) method with unit mass and fluctuation-dissipation at unit temperature is modeled in continuous time by the underdamped Langevin stochastic differential equations $dq = p \\, dt$ and $dp = - \\nabla U(q) \\, dt - c \\, p \\, dt + \\sqrt{2 c} \\, dW_{t}$, where $c > 0$ is a friction coefficient and $W_{t}$ is a standard Wiener process. Consider the explicit Euler–Maruyama discretization with step size $h > 0$ applied to this system:\n$$\np_{k+1} = p_{k} - h \\, \\nabla U(q_{k}) - c \\, h \\, p_{k} + \\sqrt{2 c h} \\, \\xi_{k}, \\quad \\xi_{k} \\sim \\mathcal{N}(0, 1),\n$$\n$$\nq_{k+1} = q_{k} + h \\, p_{k+1},\n$$\nwith $\\xi_{k}$ independent across steps and independent of $(q_{k}, p_{k})$. Using only the definitions above and standard properties of affine transformations of Gaussian random variables, derive the exact conditional distribution of $(q_{k+1}, p_{k+1})$ given $(q_{k}, p_{k})$ for the quadratic potential $U(q) = \\frac{A}{2} q^{2}$.\n\nProvide your final answer as the six scalar entries of the conditional mean vector and covariance matrix in the following order:\n- the two components of the mean vector $(\\mu_{q}, \\mu_{p})$ of $(q_{k+1}, p_{k+1})$ given $(q_{k}, p_{k})$,\n- followed by the four entries $(\\sigma_{qq}, \\sigma_{qp}, \\sigma_{pq}, \\sigma_{pp})$ of the conditional covariance matrix of $(q_{k+1}, p_{k+1})$ given $(q_{k}, p_{k})$,\n\nlisted as a single row using the $\\text{pmatrix}$ format. Your answer must be a single closed-form analytic expression in terms of $A$, $c$, $h$, $q_{k}$, and $p_{k}$.",
            "solution": "The user wants me to solve a problem about Stochastic Gradient Hamiltonian Monte Carlo.\n\n### Step 1: Problem Validation\n\nI will first perform the mandatory validation of the problem statement.\n\n#### Step 1.1: Extract Givens\n\n-   **System**: A one-dimensional system.\n-   **Potential Energy**: $U(q) = \\frac{A}{2} q^{2}$, with the constant $A > 0$.\n-   **State Variables**: Position $q_{k}$ and momentum $p_{k}$ at discrete time index $k$.\n-   **Continuous-Time Dynamics**: Underdamped Langevin SDEs for SGHMC at unit mass ($m=1$) and unit temperature ($k_B T = 1$).\n    $dq = p \\, dt$\n    $dp = - \\nabla U(q) \\, dt - c \\, p \\, dt + \\sqrt{2 c} \\, dW_{t}$, where $c > 0$ is a friction coefficient and $W_{t}$ is a standard Wiener process.\n-   **Discretization**: Explicit Euler–Maruyama scheme with step size $h > 0$.\n    $p_{k+1} = p_{k} - h \\, \\nabla U(q_{k}) - c \\, h \\, p_{k} + \\sqrt{2 c h} \\, \\xi_{k}$\n    $q_{k+1} = q_{k} + h \\, p_{k+1}$\n-   **Noise Term**: $\\xi_{k} \\sim \\mathcal{N}(0, 1)$, independent and identically distributed for each step $k$, and independent of the state $(q_{k}, p_{k})$.\n-   **Objective**: Derive the exact conditional probability distribution of $(q_{k+1}, p_{k+1})$ given $(q_{k}, p_{k})$.\n-   **Output Specification**: The final answer must be the six scalar entries of the conditional mean vector and covariance matrix, in the order $(\\mu_{q}, \\mu_{p}, \\sigma_{qq}, \\sigma_{qp}, \\sigma_{pq}, \\sigma_{pp})$.\n\n#### Step 1.2: Validate Using Extracted Givens\n\n-   **Scientific Grounding**: The problem is well-grounded in the fields of statistical mechanics and computational statistics. The underdamped Langevin equation is a fundamental model for stochastic dynamics, and the SGHMC algorithm with its discretization is a standard and widely used method in modern machine learning and Bayesian inference. The mathematical framework is entirely valid.\n-   **Well-Posedness**: The problem is well-posed. The state update equations are given explicitly. The only source of randomness is a Gaussian variable $\\xi_k$. The updates for $p_{k+1}$ and $q_{k+1}$ are affine transformations of $\\xi_k$. An affine transformation of a Gaussian random variable results in another Gaussian random variable. Therefore, the conditional distribution of $(q_{k+1}, p_{k+1})$ given $(q_{k}, p_{k})$ will be a bivariate Gaussian, which is uniquely determined by its mean vector and covariance matrix. All necessary parameters ($A, c, h$) and initial state variables ($q_k, p_k$) are provided.\n-   **Objectivity**: The problem is stated using precise and unambiguous mathematical language. There are no subjective or non-scientific elements.\n-   **Conclusion**: The problem does not violate any of the invalidity criteria. It is scientifically sound, well-posed, and objective.\n\n#### Step 1.3: Verdict and Action\n\nThe problem statement is **valid**. I will proceed with formulating the solution.\n\n### Step 2: Solution Derivation\n\nThe goal is to determine the conditional distribution of the state vector $X_{k+1} = \\begin{pmatrix} q_{k+1} \\\\ p_{k+1} \\end{pmatrix}$ given the state vector $X_{k} = \\begin{pmatrix} q_{k} \\\\ p_{k} \\end{pmatrix}$.\n\nFirst, we specify the gradient of the potential energy function $U(q)$.\nFor the quadratic potential $U(q) = \\frac{A}{2} q^{2}$, the gradient is $\\nabla U(q) = \\frac{dU}{dq} = A q$.\n\nSubstituting this into the given discretization scheme, we obtain the explicit update rules:\nThe update for momentum $p_{k+1}$ is:\n$$p_{k+1} = p_{k} - h (A q_{k}) - c h p_{k} + \\sqrt{2 c h} \\, \\xi_{k}$$\n$$p_{k+1} = (1 - c h) p_{k} - A h q_{k} + \\sqrt{2 c h} \\, \\xi_{k}$$\nThe update for position $q_{k+1}$ is:\n$$q_{k+1} = q_{k} + h p_{k+1}$$\n\nThese equations describe an affine transformation of the standard normal random variable $\\xi_k$ to the vector $(q_{k+1}, p_{k+1})$. The values of $q_k$ and $p_k$ are treated as fixed constants in the context of the conditional distribution. Since an affine transformation of a Gaussian variable is Gaussian, the conditional distribution of $(q_{k+1}, p_{k+1})$ is a bivariate normal distribution. This distribution is fully characterized by its mean vector and its covariance matrix.\n\n#### Calculation of the Conditional Mean Vector\n\nThe conditional mean vector is $\\mu = E[X_{k+1} | X_{k}] = \\begin{pmatrix} E[q_{k+1} | q_{k}, p_{k}] \\\\ E[p_{k+1} | q_{k}, p_{k}] \\end{pmatrix} = \\begin{pmatrix} \\mu_q \\\\ \\mu_p \\end{pmatrix}$.\nWe use the fact that $E[\\xi_k] = 0$.\n\nFirst, let's compute the conditional mean of the momentum, $\\mu_p$:\n$$\\mu_p = E[p_{k+1} | q_{k}, p_{k}] = E[(1 - c h) p_{k} - A h q_{k} + \\sqrt{2 c h} \\, \\xi_{k} | q_{k}, p_{k}]$$\nBy linearity of expectation, and treating $q_k$ and $p_k$ as constants:\n$$\\mu_p = (1 - c h) p_{k} - A h q_{k} + \\sqrt{2 c h} \\, E[\\xi_{k}]$$\n$$\\mu_p = (1 - c h) p_{k} - A h q_{k}$$\n\nNext, we compute the conditional mean of the position, $\\mu_q$:\n$$\\mu_q = E[q_{k+1} | q_{k}, p_{k}] = E[q_{k} + h p_{k+1} | q_{k}, p_{k}]$$\nUsing the linearity of expectation and the previously found result for $E[p_{k+1} | q_k, p_k]$:\n$$\\mu_q = q_{k} + h E[p_{k+1} | q_{k}, p_{k}] = q_{k} + h \\mu_p$$\n$$\\mu_q = q_{k} + h ((1 - c h) p_{k} - A h q_{k})$$\n$$\\mu_q = (1 - A h^{2}) q_{k} + h(1 - c h) p_{k}$$\n\nThus, the conditional mean vector is:\n$$\\mu = \\begin{pmatrix} (1 - A h^{2}) q_{k} + h(1 - c h) p_{k} \\\\ -A h q_{k} + (1 - c h) p_{k} \\end{pmatrix}$$\n\n#### Calculation of the Conditional Covariance Matrix\n\nThe conditional covariance matrix is $\\Sigma = \\text{Cov}(X_{k+1} | X_{k}) = \\begin{pmatrix} \\sigma_{qq} & \\sigma_{qp} \\\\ \\sigma_{pq} & \\sigma_{pp} \\end{pmatrix}$.\nThe entries are calculated from the deviations of $q_{k+1}$ and $p_{k+1}$ from their conditional means. Let's find these deviations:\n$$p_{k+1} - \\mu_p = \\left( (1 - c h) p_{k} - A h q_{k} + \\sqrt{2 c h} \\, \\xi_{k} \\right) - \\left( (1 - c h) p_{k} - A h q_{k} \\right) = \\sqrt{2 c h} \\, \\xi_{k}$$\n$$q_{k+1} - \\mu_q = (q_{k} + h p_{k+1}) - (q_{k} + h \\mu_p) = h (p_{k+1} - \\mu_p) = h \\sqrt{2 c h} \\, \\xi_{k}$$\n\nNow we can compute the elements of the covariance matrix using the fact that $\\text{Var}(\\xi_k) = E[\\xi_k^2] - (E[\\xi_k])^2 = E[\\xi_k^2] = 1$.\n\nThe variance of a random variable $Z$ is $\\text{Var}(Z) = E[(Z - E[Z])^2]$. For our conditional setting:\n$\\sigma_{pp} = \\text{Var}(p_{k+1} | q_{k}, p_{k}) = E[(p_{k+1} - \\mu_p)^2 | q_k, p_k]$\n$$\\sigma_{pp} = E[(\\sqrt{2 c h} \\, \\xi_{k})^2] = (2 c h) \\, E[\\xi_k^2] = 2 c h$$\n\n$\\sigma_{qq} = \\text{Var}(q_{k+1} | q_{k}, p_{k}) = E[(q_{k+1} - \\mu_q)^2 | q_k, p_k]$\n$$\\sigma_{qq} = E[(h \\sqrt{2 c h} \\, \\xi_{k})^2] = h^2 (2 c h) \\, E[\\xi_k^2] = 2 c h^{3}$$\n\n$\\sigma_{pq} = \\text{Cov}(p_{k+1}, q_{k+1} | q_{k}, p_{k}) = E[(p_{k+1} - \\mu_p)(q_{k+1} - \\mu_q) | q_k, p_k]$\n$$\\sigma_{pq} = E[(\\sqrt{2 c h} \\, \\xi_{k}) (h \\sqrt{2 c h} \\, \\xi_{k})] = h (2 c h) \\, E[\\xi_k^2] = 2 c h^{2}$$\n\nThe covariance matrix is symmetric, so $\\sigma_{qp} = \\sigma_{pq} = 2 c h^{2}$.\n\nThe conditional covariance matrix is:\n$$\\Sigma = \\begin{pmatrix} 2 c h^{3} & 2 c h^{2} \\\\ 2 c h^{2} & 2 c h \\end{pmatrix}$$\n\n#### Summary of Results\n\nThe conditional distribution of $(q_{k+1}, p_{k+1})$ given $(q_{k}, p_{k})$ is a bivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$.\n\nThe components of the mean vector are:\n- $\\mu_{q} = (1 - A h^{2}) q_{k} + h(1 - c h) p_{k}$\n- $\\mu_{p} = -A h q_{k} + (1 - c h) p_{k}$\n\nThe entries of the covariance matrix are:\n- $\\sigma_{qq} = 2 c h^{3}$\n- $\\sigma_{qp} = 2 c h^{2}$\n- $\\sigma_{pq} = 2 c h^{2}$\n- $\\sigma_{pp} = 2 c h$\n\nThese six scalar quantities constitute the final answer, ordered as requested.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n(1 - A h^{2}) q_{k} + h(1 - c h) p_{k} & -A h q_{k} + (1 - c h) p_{k} & 2 c h^{3} & 2 c h^{2} & 2 c h^{2} & 2 c h\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "After dissecting a single step, we now zoom out to analyze the system's long-term, collective behavior. For the important case of a quadratic potential, the continuous-time SGHMC dynamics simplify to a linear stochastic differential equation, a type of Ornstein-Uhlenbeck process. This exercise asks you to prove that the system converges to the correct target distribution by deriving the exact stationary covariance matrix and showing that it corresponds to the desired Gibbs-Boltzmann distribution, thus verifying the theoretical correctness of the sampler in the continuous-time limit .",
            "id": "3349094",
            "problem": "Consider Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) in $d$ dimensions with identity mass matrix, a quadratic potential energy $U(q)=\\tfrac{1}{2} q^{\\top} A q$ where $A \\in \\mathbb{R}^{d \\times d}$ is symmetric positive definite, and a symmetric positive semidefinite friction matrix $C \\in \\mathbb{R}^{d \\times d}$. Let $(q_t,p_t)$ satisfy the Itô Stochastic Differential Equation (SDE)\n$$\n\\mathrm{d} q_t = p_t \\,\\mathrm{d} t,\\quad\n\\mathrm{d} p_t = -A q_t \\,\\mathrm{d} t - C p_t \\,\\mathrm{d} t + \\sqrt{2 C}\\,\\mathrm{d} W_t,\n$$\nwhere $W_t$ is a standard $d$-dimensional Wiener process and $\\sqrt{2 C}$ denotes any matrix square root satisfying $(\\sqrt{2 C})(\\sqrt{2 C})^{\\top} = 2 C$. Define the $2d$-dimensional state $x_t = \\begin{pmatrix} q_t \\\\ p_t \\end{pmatrix}$.\n\nUsing only fundamental facts about linear Itô SDEs and Ornstein–Uhlenbeck processes, perform the following:\n\n1. Derive the exact solution for $x_t$ in terms of the drift matrix exponential and the stochastic convolution, starting from the linear SDE form of SGHMC.\n2. Identify a candidate stationary Gaussian distribution for $x_t$ that corresponds to the Hamiltonian target density implied by the quadratic potential and identity mass, and compute its covariance matrix $\\Sigma_{\\star}$ in block form.\n3. Verify that $\\Sigma_{\\star}$ is invariant under the SGHMC dynamics by checking the continuous-time Lyapunov equation associated with the linear SDE.\n4. Conclude with the explicit block-matrix expression for $\\Sigma_{\\star}$.\n\nYour final answer must be the single closed-form analytic expression for the stationary covariance matrix $\\Sigma_{\\star}$ in block form. No numerical rounding is required and no units are involved in this problem. Express your final answer as a single matrix expression.",
            "solution": "The problem asks for the derivation of the stationary covariance matrix for a specific Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) process. The process is described by a linear Itô Stochastic Differential Equation (SDE) system for the state vector $x_t = \\begin{pmatrix} q_t \\\\ p_t \\end{pmatrix} \\in \\mathbb{R}^{2d}$. We will follow the four steps outlined in the problem statement.\n\nFirst, we formulate the given system of SDEs as a single linear vector SDE. The state is $x_t = \\begin{pmatrix} q_t \\\\ p_t \\end{pmatrix}$. The system is given by:\n$$\n\\mathrm{d} q_t = p_t \\,\\mathrm{d} t \\\\\n\\mathrm{d} p_t = -A q_t \\,\\mathrm{d} t - C p_t \\,\\mathrm{d} t + \\sqrt{2 C}\\,\\mathrm{d} W_t\n$$\nThis can be written in matrix-vector form as:\n$$\n\\mathrm{d} \\begin{pmatrix} q_t \\\\ p_t \\end{pmatrix} = \\begin{pmatrix} 0 & I \\\\ -A & -C \\end{pmatrix} \\begin{pmatrix} q_t \\\\ p_t \\end{pmatrix} \\mathrm{d} t + \\begin{pmatrix} 0 \\\\ \\sqrt{2C} \\end{pmatrix} \\mathrm{d} W_t\n$$\nThis is a linear SDE of the Ornstein–Uhlenbeck type, $\\mathrm{d}x_t = B x_t \\mathrm{d}t + S \\mathrm{d}W_t$, with the drift matrix $B$ and the diffusion matrix $S$ defined as:\n$$\nB = \\begin{pmatrix} 0 & I \\\\ -A & -C \\end{pmatrix}, \\quad S = \\begin{pmatrix} 0 \\\\ \\sqrt{2C} \\end{pmatrix}\n$$\nHere, $I$ is the $d \\times d$ identity matrix, and $0$ represents the $d \\times d$ zero matrix where appropriate.\n\n1. Exact Solution for $x_t$:\nThe formal solution to a linear SDE of the form $\\mathrm{d}x_t = B x_t \\mathrm{d}t + S \\mathrm{d}W_t$ with initial condition $x_0$ is given by the variation of constants formula. The solution is:\n$$\nx_t = \\exp(Bt) x_0 + \\int_0^t \\exp(B(t-s)) S \\,\\mathrm{d}W_s\n$$\nThis expression gives the state $x_t$ at time $t$ in terms of the matrix exponential of the drift matrix, $\\exp(Bt)$, and the stochastic convolution integral, which captures the accumulated effect of the Wiener process $W_s$ over the interval $[0, t]$.\n\n2. Candidate Stationary Gaussian Distribution:\nThe SGHMC dynamics are designed to sample from a target distribution related to the Hamiltonian of the system. The potential energy is $U(q) = \\frac{1}{2} q^{\\top} A q$ and the kinetic energy (with identity mass matrix $M=I$) is $K(p) = \\frac{1}{2} p^{\\top} p$. The total Hamiltonian is $H(q,p) = U(q) + K(p)$.\n$$\nH(q,p) = \\frac{1}{2} q^{\\top} A q + \\frac{1}{2} p^{\\top} p\n$$\nThe fluctuation-dissipation theorem implies that the stationary distribution for these dynamics is the Gibbs-Boltzmann distribution $\\pi(q,p) \\propto \\exp(-H(q,p))$. Expressed in terms of the state vector $x$, this is:\n$$\n\\pi(x) \\propto \\exp\\left(-\\frac{1}{2}\\left(q^{\\top} A q + p^{\\top} p\\right)\\right) = \\exp\\left(-\\frac{1}{2} \\begin{pmatrix} q \\\\ p \\end{pmatrix}^{\\top} \\begin{pmatrix} A & 0 \\\\ 0 & I \\end{pmatrix} \\begin{pmatrix} q \\\\ p \\end{pmatrix}\\right)\n$$\nThis corresponds to a zero-mean multivariate Gaussian distribution with a density function $p(x) \\propto \\exp(-\\frac{1}{2} x^{\\top} \\Sigma_{\\star}^{-1} x)$. By comparing the exponents, we can identify the inverse of the stationary covariance matrix $\\Sigma_{\\star}$:\n$$\n\\Sigma_{\\star}^{-1} = \\begin{pmatrix} A & 0 \\\\ 0 & I \\end{pmatrix}\n$$\nSince $A$ is symmetric positive definite, it is invertible. The covariance matrix $\\Sigma_{\\star}$ is the inverse of $\\Sigma_{\\star}^{-1}$:\n$$\n\\Sigma_{\\star} = \\left(\\begin{pmatrix} A & 0 \\\\ 0 & I \\end{pmatrix}\\right)^{-1} = \\begin{pmatrix} A^{-1} & 0 \\\\ 0 & I^{-1} \\end{pmatrix} = \\begin{pmatrix} A^{-1} & 0 \\\\ 0 & I \\end{pmatrix}\n$$\nThis is our candidate for the stationary covariance matrix. It describes a distribution where the position $q$ and momentum $p$ are uncorrelated, with $\\mathbb{E}[qq^\\top] = A^{-1}$, $\\mathbb{E}[pp^\\top] = I$, and $\\mathbb{E}[qp^\\top] = 0$.\n\n3. Verification of Invariance via Lyapunov Equation:\nFor a linear SDE system, a covariance matrix $\\Sigma$ is stationary if and only if it is a positive semidefinite solution to the continuous-time algebraic Lyapunov equation:\n$$\nB \\Sigma + \\Sigma B^{\\top} + Q = 0\n$$\nwhere $Q = S S^\\top$ is the covariance of the noise term. Let's compute $Q$:\n$$\nQ = S S^{\\top} = \\begin{pmatrix} 0 \\\\ \\sqrt{2C} \\end{pmatrix} \\begin{pmatrix} 0^{\\top} & (\\sqrt{2C})^{\\top} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & (\\sqrt{2C})(\\sqrt{2C})^{\\top} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 2C \\end{pmatrix}\n$$\nWe must now verify that our candidate $\\Sigma_{\\star}$ satisfies $B \\Sigma_{\\star} + \\Sigma_{\\star} B^{\\top} + Q = 0$.\nThe transpose of the drift matrix $B$ is:\n$$\nB^{\\top} = \\begin{pmatrix} 0 & I \\\\ -A & -C \\end{pmatrix}^{\\top} = \\begin{pmatrix} 0^{\\top} & -A^{\\top} \\\\ I^{\\top} & -C^{\\top} \\end{pmatrix} = \\begin{pmatrix} 0 & -A \\\\ I & -C \\end{pmatrix}\n$$\nsince $A$ and $C$ are symmetric.\n\nNow, we compute the term $B \\Sigma_{\\star}$:\n$$\nB \\Sigma_{\\star} = \\begin{pmatrix} 0 & I \\\\ -A & -C \\end{pmatrix} \\begin{pmatrix} A^{-1} & 0 \\\\ 0 & I \\end{pmatrix} = \\begin{pmatrix} 0 \\cdot A^{-1} + I \\cdot 0 & 0 \\cdot 0 + I \\cdot I \\\\ -A \\cdot A^{-1} - C \\cdot 0 & -A \\cdot 0 - C \\cdot I \\end{pmatrix} = \\begin{pmatrix} 0 & I \\\\ -I & -C \\end{pmatrix}\n$$\nNext, we compute the term $\\Sigma_{\\star} B^{\\top}$:\n$$\n\\Sigma_{\\star} B^{\\top} = \\begin{pmatrix} A^{-1} & 0 \\\\ 0 & I \\end{pmatrix} \\begin{pmatrix} 0 & -A \\\\ I & -C \\end{pmatrix} = \\begin{pmatrix} A^{-1} \\cdot 0 + 0 \\cdot I & A^{-1}(-A) + 0 \\cdot (-C) \\\\ 0 \\cdot 0 + I \\cdot I & 0 \\cdot (-A) + I \\cdot (-C) \\end{pmatrix} = \\begin{pmatrix} 0 & -I \\\\ I & -C \\end{pmatrix}\n$$\nFinally, we sum the three terms $B \\Sigma_{\\star}$, $\\Sigma_{\\star} B^{\\top}$, and $Q$:\n$$\nB \\Sigma_{\\star} + \\Sigma_{\\star} B^{\\top} + Q = \\begin{pmatrix} 0 & I \\\\ -I & -C \\end{pmatrix} + \\begin{pmatrix} 0 & -I \\\\ I & -C \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 2C \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 0+0+0 & I-I+0 \\\\ -I+I+0 & -C-C+2C \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe Lyapunov equation is satisfied. This confirms that $\\Sigma_{\\star} = \\begin{pmatrix} A^{-1} & 0 \\\\ 0 & I \\end{pmatrix}$ is indeed the stationary covariance matrix for the given SGHMC process, provided a unique stationary distribution exists.\n\n4. Conclusion:\nThe derivation and subsequent verification through the Lyapunov equation confirm that the stationary state of the given SGHMC process is a zero-mean Gaussian distribution. The covariance matrix $\\Sigma_{\\star}$ of this stationary distribution is given by the block matrix derived above. This matrix is symmetric and positive definite, as required for a non-degenerate Gaussian distribution, because $A^{-1}$ is positive definite and $I$ is positive definite.\n\nThe explicit block-matrix expression for the stationary covariance matrix $\\Sigma_{\\star}$ is:\n$$\n\\Sigma_{\\star} = \\begin{pmatrix} \\mathbb{E}[q_t q_t^{\\top}] & \\mathbb{E}[q_t p_t^{\\top}] \\\\ \\mathbb{E}[p_t q_t^{\\top}] & \\mathbb{E}[p_t p_t^{\\top}] \\end{pmatrix} = \\begin{pmatrix} A^{-1} & 0 \\\\ 0 & I \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} A^{-1} & 0 \\\\ 0 & I \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In practical applications, the variance of the stochastic gradient noise is often unknown and can disrupt the sampler's properties. This final practice moves from idealized theory to a crucial diagnostic technique used to ensure simulation health. You will develop a tool based on the \"kinetic temperature,\" a concept borrowed from statistical mechanics, to monitor whether the injected noise correctly balances the gradient noise. Deriving and interpreting this diagnostic provides an essential skill for tuning SGHMC parameters and validating the correctness of the sampling process in realistic scenarios .",
            "id": "3349082",
            "problem": "Consider Stochastic Gradient Hamiltonian Monte Carlo (SGHMC), which augments Hamiltonian Monte Carlo (HMC) with friction and stochastic forcing to counteract stochastic gradient noise. Let the target density over position $q \\in \\mathbb{R}^d$ be proportional to $\\exp(-U(q))$, and let the Hamiltonian be $H(q,p) = U(q) + \\tfrac{1}{2} p^{\\top} M^{-1} p$ with a symmetric positive definite mass matrix $M \\in \\mathbb{R}^{d \\times d}$. In continuous time, the momentum $p$ evolves according to a Stochastic Differential Equation (SDE) that combines Hamilton’s equations, linear friction, and stochastic driving from both artificially injected noise and stochastic gradient error. Assume the following isotropic setting for analytical tractability: $M = m I_d$ with $m > 0$, friction $C = c I_d$ with $c > 0$, and the stochastic gradient noise can be modeled as approximately white with mean zero and covariance $2 b I_d$ per unit time for some $b \\ge 0$. The algorithm uses an estimate $\\hat b \\ge 0$ of $b$ to set the injected diffusion so that the net diffusion aims to reproduce the desired canonical marginal of $p$.\n\nMotivated by equilibrium statistical mechanics, define the marginal kinetic temperature as the scalar functional $\\mathbb{E}\\!\\left[p^{\\top} M^{-1} p\\right]/d$, where $d$ is the dimension. You are to construct a practical diagnostic based on a time average of this quantity along an SGHMC trajectory and reason, from first principles, how its deviation from $1$ reflects calibration error in the gradient noise estimate $\\hat b$.\n\nStarting from the canonical form of the momentum marginal under correct calibration, and using only fundamental properties of linear Ornstein–Uhlenbeck dynamics for the momentum under the isotropic assumptions above, decide which of the following statements correctly specifies a consistent diagnostic and its interpretation with respect to underestimation versus overestimation of $b$ by $\\hat b$.\n\nA. Define the diagnostic as the running average $\\hat T = \\tfrac{1}{K} \\sum_{k=1}^{K} \\tfrac{1}{d} p_k^{\\top} M^{-1} p_k$ collected along the chain. In the isotropic setting and small-step limit, $\\mathbb{E}[\\hat T]$ equals an effective temperature that increases linearly with $b - \\hat b$ relative to $c$, so that $\\hat T > 1$ indicates the injected diffusion is too small compared to the true gradient noise (i.e., $\\hat b < b$), while $\\hat T < 1$ indicates $\\hat b > b$. One can tune $\\hat b$ (or increase $c$) to drive $\\hat T$ back to $1$.\n\nB. Define the diagnostic as $\\hat T = \\tfrac{1}{K} \\sum_{k=1}^{K} \\tfrac{1}{d} p_k^{\\top} p_k$, ignoring the mass matrix. Under correct calibration it always equals $1$, and any deviation from $1$ is due only to Monte Carlo error and is independent of $\\hat b$.\n\nC. Use the running average of the Hamiltonian $\\tfrac{1}{K} \\sum_{k=1}^{K} H(q_k,p_k)$ as the diagnostic; if it is greater than $1$, then the gradient noise estimate is too large (i.e., $\\hat b > b$), and if it is less than $1$, then $\\hat b < b$.\n\nD. The marginal kinetic temperature equals $1$ in HMC-type samplers regardless of friction $c$ and noise calibration, so it cannot diagnose any mismatch between $b$ and $\\hat b$.",
            "solution": "We begin from the canonical momentum marginal under correct calibration. In a correctly calibrated SGHMC, the stationary joint is proportional to $\\exp(-H(q,p))$, which factorizes between $q$ and $p$ as $\\exp(-U(q)) \\exp\\!\\left(-\\tfrac{1}{2} p^{\\top} M^{-1} p\\right)$. Hence the momentum marginal is Gaussian with mean zero and covariance $M$. Therefore the marginal kinetic temperature equals\n$$\n\\frac{\\mathbb{E}\\!\\left[p^{\\top} M^{-1} p\\right]}{d} \\;=\\; \\frac{\\operatorname{tr}\\!\\left(M^{-1} \\operatorname{Cov}(p)\\right)}{d} \\;=\\; \\frac{\\operatorname{tr}(I_d)}{d} \\;=\\; 1.\n$$\nThis provides the baseline: under correct calibration, the kinetic temperature equals $1$.\n\nNext we consider how miscalibration modifies the momentum dynamics. Under the isotropic assumptions $M = m I_d$, $C = c I_d$, with stochastic gradient noise modeled as mean-zero white noise of covariance $2 b I_d$ per unit time, and with an estimate $\\hat b$ used to set the injected diffusion, the momentum SDE can be written as\n$$\n\\mathrm{d} p \\;=\\; - \\nabla U(q)\\, \\mathrm{d} t \\;-\\; c m^{-1} p\\, \\mathrm{d} t \\;+\\; \\sqrt{2\\,(c - \\hat b)}\\, \\mathrm{d} W_0 \\;+\\; \\sqrt{2\\, b}\\, \\mathrm{d} W_1,\n$$\nwhere $W_0$ and $W_1$ are independent standard $d$-dimensional Wiener processes. Aggregating independent Gaussian perturbations yields an equivalent representation\n$$\n\\mathrm{d} p \\;=\\; - \\nabla U(q)\\, \\mathrm{d} t \\;-\\; c m^{-1} p\\, \\mathrm{d} t \\;+\\; \\sqrt{2\\,\\big(c + (b - \\hat b)\\big)}\\, \\mathrm{d} \\widetilde{W},\n$$\nwith $\\widetilde{W}$ a standard $d$-dimensional Wiener process, provided that $c - \\hat b \\ge 0$ for well-posedness. The term $- \\nabla U(q)\\, \\mathrm{d} t$ couples to $q$, but the linear part in $p$ is an Ornstein–Uhlenbeck (OU) component. To analyze the stationary momentum marginal, we exploit the usual separation in the canonical target: the desired momentum marginal is independent of $q$, and the OU component controls the covariance of $p$.\n\nFormally, in the isotropic setting the stationary covariance $\\Sigma = \\operatorname{Cov}(p)$ for the OU part solves the continuous-time Lyapunov equation\n$$\n\\left(c m^{-1}\\right) \\Sigma \\;+\\; \\Sigma \\left(c m^{-1}\\right) \\;=\\; 2\\,\\big(c + (b - \\hat b)\\big)\\, I_d,\n$$\nwhich simplifies to\n$$\n\\frac{2 c}{m} \\,\\Sigma \\;=\\; 2\\,\\big(c + (b - \\hat b)\\big)\\, I_d \\quad \\Longrightarrow \\quad \\Sigma \\;=\\; m \\left(1 + \\frac{b - \\hat b}{c}\\right) I_d.\n$$\nHence the effective momentum marginal is Gaussian with covariance $\\Sigma = T_{\\mathrm{eff}}\\, M$, where the scalar effective temperature is\n$$\nT_{\\mathrm{eff}} \\;=\\; 1 + \\frac{b - \\hat b}{c}.\n$$\nThe marginal kinetic temperature then satisfies\n$$\n\\frac{\\mathbb{E}\\!\\left[p^{\\top} M^{-1} p\\right]}{d} \\;=\\; \\frac{\\operatorname{tr}\\!\\left(M^{-1} \\Sigma\\right)}{d} \\;=\\; \\frac{\\operatorname{tr}\\!\\left(M^{-1} \\cdot T_{\\mathrm{eff}}\\, M\\right)}{d} \\;=\\; T_{\\mathrm{eff}} \\;=\\; 1 + \\frac{b - \\hat b}{c}.\n$$\nTherefore:\n- If $\\hat b = b$, then $\\mathbb{E}\\!\\left[p^{\\top} M^{-1} p\\right]/d = 1$.\n- If $\\hat b < b$, then $b - \\hat b > 0$ and $\\mathbb{E}\\!\\left[p^{\\top} M^{-1} p\\right]/d > 1$ (the chain is too “hot”).\n- If $\\hat b > b$, then $b - \\hat b < 0$ and $\\mathbb{E}\\!\\left[p^{\\top} M^{-1} p\\right]/d < 1$ (the chain is too “cold”).\n\nA consistent empirical diagnostic is therefore the time average\n$$\n\\hat T \\;=\\; \\frac{1}{K} \\sum_{k=1}^{K} \\frac{1}{d}\\, p_k^{\\top} M^{-1} p_k,\n$$\ncomputed along the trajectory. In the small-step limit and for large $K$, $\\hat T$ concentrates near $1 + \\tfrac{b - \\hat b}{c}$. Its deviation from $1$ indicates the sign and magnitude of the calibration error relative to the friction $c$: values above $1$ imply underestimation of $b$, while values below $1$ imply overestimation.\n\nWe now evaluate each option.\n\nOption A: This option defines $\\hat T$ as $\\tfrac{1}{K} \\sum_{k=1}^{K} \\tfrac{1}{d} p_k^{\\top} M^{-1} p_k$ and states that, under the isotropic assumptions and small-step limit, its expectation equals an effective temperature that increases linearly with $b - \\hat b$ over $c$, with the correct directional interpretation: $\\hat T > 1$ when $\\hat b < b$, and $\\hat T < 1$ when $\\hat b > b$. It also notes that one can adjust $\\hat b$ (or increase $c$) to bring $\\hat T$ back to $1$. This matches the derivation $T_{\\mathrm{eff}} = 1 + \\tfrac{b - \\hat b}{c}$. Verdict: Correct.\n\nOption B: This option uses $\\hat T = \\tfrac{1}{K} \\sum \\tfrac{1}{d} p_k^{\\top} p_k$, ignoring $M$, and claims it equals $1$ under correct calibration and is independent of $\\hat b$. Under correct calibration, $\\operatorname{Cov}(p) = M = m I_d$, so $\\mathbb{E}\\!\\left[\\tfrac{1}{d} p^{\\top} p\\right] = \\tfrac{1}{d} \\operatorname{tr}(M) = m$, not $1$, unless $m = 1$. Moreover, under miscalibration the expectation scales with $1 + \\tfrac{b - \\hat b}{c}$, so it is not independent of $\\hat b$. Verdict: Incorrect.\n\nOption C: This option suggests using the running average of the Hamiltonian and interprets deviations relative to $1$ with a sign that is opposite to the derivation. The Hamiltonian is not dimensionless nor normalized to $1$, and its average at stationarity is not a universal constant; moreover, the proposed sign mapping contradicts $T_{\\mathrm{eff}} = 1 + \\tfrac{b - \\hat b}{c}$. Verdict: Incorrect.\n\nOption D: This asserts that the marginal kinetic temperature equals $1$ regardless of friction and calibration. As shown, miscalibration yields $T_{\\mathrm{eff}} \\ne 1$ unless $\\hat b = b$. Verdict: Incorrect.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}