## 引言
[随机游走Metropolis](@entry_id:754036)（RWM）算法是马尔可夫链蒙特卡洛（MCMC）方法中应用最广泛的基石之一。然而，其简单性的背后隐藏着一个棘手的挑战，尤其是在处理现代科学与工程中常见的高维问题时：算法的性能对其提议步长（proposal step size）的选择极为敏感。一个不当的步长选择可能导致[采样效率](@entry_id:754496)急剧下降，甚至使算法完全失效。如何系统地、有理论依据地选择这个关键参数，便构成了[计算统计学](@entry_id:144702)中的一个核心问题，即“最优缩放”问题。

本文旨在系统性地阐述[随机游走Metropolis](@entry_id:754036)算法在高维空间中的最优缩放理论。我们将从根本上回答“为什么存在一个[最优步长](@entry_id:143372)？”以及“这个[最优步长](@entry_id:143372)应该是什么？”等问题。通过本文的学习，您将掌握这一强大理论的精髓，并理解其在实际应用中的指导意义和局限性。

为实现这一目标，本文将分为三个核心部分。在“原理与机制”一章中，我们将深入探讨该理论的数学基础，从定义算法效率的准则出发，通过渐进分析，揭示著名的“0.234接受率法则”是如何从第一性原理中推导出来的。随后，在“应用与跨学科联系”一章中，我们将展示这一理论如何从抽象的数学走向具体的实践，包括如何进行算法的自适应调优，如何指导更复杂[MCMC算法](@entry_id:751788)变体的设计，并探索其与计算物理、[生物信息学](@entry_id:146759)等前沿领域的深刻联系。最后，在“动手实践”部分，您将通过解决一系列精心设计的问题，将理论知识内化为可以指导您研究和开发的实用技能。

## 原理与机制

[随机游走Metropolis](@entry_id:754036)（RWM）算法是[马尔可夫链蒙特卡洛](@entry_id:138779)（MCMC）方法家族中的基石。然而，当应用于高维空间时，其性能对[提议分布](@entry_id:144814)的尺度（即步长）表现出极大的敏感性。选择一个过小的步长会导致链在[状态空间](@entry_id:177074)中缓慢移动，产生高度自相关的样本，从而降低了探索效率。相反，一个过大的步长会频繁地将提议点抛向目标分布的低概率区域，导致绝大多数提议被拒绝，使得链几乎停滞不前。因此，在高维背景下，为RWM算法确定一个“恰到好处”的步长尺度，是确保有效探索的关键。本章将深入探讨指导这一选择的数学原理和核心机制，揭示高维RWM算法的最优缩放理论。

### 效率准则：期望平方跳跃距离

为了系统地研究[最优步长](@entry_id:143372)问题，我们首先需要一个量化算法效率的准则。一个直观且在理论上极具洞察力的度量是**期望平方跳跃距离 (Expected Squared Jumping Distance, ESJD)**。假设马尔可夫链在第 $k$ 步的状态为 $X_k$，通过[提议分布](@entry_id:144814)生成 $Y_k$，并以概率 $\alpha(X_k, Y_k)$ 接受该提议，则下一步的状态为 $X_{k+1}$。ESJD 定义为链在[稳态](@entry_id:182458)时一步之内状态变化的平方欧氏距离的[期望值](@entry_id:153208)：

$ \mathrm{ESJD} = \mathbb{E}[\|X_{k+1} - X_k\|^2] $

在[稳态](@entry_id:182458)下，$X_k$ 服从目标分布 $\pi$。由于链要么移动到提议点 $Y_k$，要么保持在原地 $X_k$，这个期望可以被分解为：

$ \mathrm{ESJD} = \mathbb{E}[\|Y_k - X_k\|^2 \alpha(X_k, Y_k) + \|X_k - X_k\|^2 (1 - \alpha(X_k, Y_k))] = \mathbb{E}[\|Y_k - X_k\|^2 \alpha(X_k, Y_k)] $

这个定义清晰地揭示了效率的两个核心要素之间的权衡：**提议跳跃的幅度**（由 $\|Y_k - X_k\|^2$ 体现）和**提议被接受的可能性**（由 $\alpha(X_k, Y_k)$ 体现）。一个高效的算法应该能够提出足够大的跳跃并且这些跳跃有合理的概率被接受。只优化其中一个方面是徒劳的：极小的步长会使接受率接近1，但 ESJD 趋近于0；极大的步长虽然 $\|Y_k - X_k\|^2$ 很大，但接受率会趋近于0，同样导致 ESJD 趋近于0。因此，最大化 ESJD 自然地成为了寻找[最优步长](@entry_id:143372)的核心目标。

更深层次的理论表明，最大化 ESJD 不仅仅是一个启发式选择。在高维极限下，当时间被恰当加速后，离散的马尔可夫链的单个坐标的[演化过程](@entry_id:175749)会收敛到一个连续时间的[随机微分方程](@entry_id:146618)——即一个[扩散过程](@entry_id:170696)。这个极限扩散过程的**[扩散](@entry_id:141445)系数 (diffusivity constant)** 直接决定了它探索[状态空间](@entry_id:177074)的速度。理论证明，这个[扩散](@entry_id:141445)系数正比于 ESJD。因此，通过调节步长来最大化 ESJD，我们实际上是在最大化极限过程的有效探索速度，从而最小化样本的自相关性，这为我们将 ESJD 作为混合速度的代理提供了坚实的理论基础 。

### 渐进机制：高维问题的形式化设定

为了对 ESJD 进行精确的[数学分析](@entry_id:139664)，我们需要在一个明确的框架下进行。最优缩放理论通常考虑一个典型的、具有挑战性但又易于分析的场景：

1.  **高维[状态空间](@entry_id:177074)**：[目标分布](@entry_id:634522)定义在 $\mathbb{R}^d$ 上，其中维度 $d \to \infty$。

2.  **积形式[目标分布](@entry_id:634522)**：目标概率密度函数 $\pi_d(x)$ 具有分量独立的积形式，即 $\pi_d(x) = \prod_{i=1}^d f(x_i)$，其中 $f$ 是一个定义在 $\mathbb{R}$ 上的单变量概率密度函数。这意味着各个坐标是[独立同分布](@entry_id:169067)的。

3.  **高斯[随机游走](@entry_id:142620)提议**：从当前状态 $x$，提议一个新的状态 $y = x + \sigma_d Z$，其中 $Z$ 是一个 $d$ 维标准正态随机向量（$Z \sim \mathcal{N}(0, I_d)$），$I_d$ 是 $d \times d$ [单位矩阵](@entry_id:156724)。

4.  **维度依赖的步长缩放**：提议分布的[尺度参数](@entry_id:268705)（步长）$\sigma_d$ 随着维度 $d$ 的增加而缩减，其形式为 $\sigma_d = \ell / \sqrt{d}$。这里，$\ell > 0$ 是一个与维度无关的调节常数。

这个 $\sigma_d = \ell / \sqrt{d}$ 的缩放至关重要。我们可以直观地理解它：一次提议跳跃的总平方大小为 $\|y-x\|^2 = \|\sigma_d Z\|^2 = (\ell^2/d) \|Z\|^2$。由于 $Z$ 的分量是独立的标准正态变量，根据[大数定律](@entry_id:140915)，$\|Z\|^2 = \sum_{i=1}^d Z_i^2$ 的[期望值](@entry_id:153208)为 $d$。因此，提议跳跃的期望平方大小为 $\mathbb{E}[\|y-x\|^2] = (\ell^2/d) \cdot d = \ell^2$，这是一个与维度 $d$ 无关的 $O(1)$ 量。这个缩放确保了在高维情况下，提议的步子既不会因为维度爆炸而变得过大，也不会因为维度诅咒而变得微不足道，从而使得一个非平凡的（既不为0也不为1）接受率成为可能 。

### 核心机制：接受率的渐进分析

在上述形式化设定下，我们可以分析当 $d \to \infty$ 时接受率 $\alpha(x,y)$ 的行为。对于[对称提议](@entry_id:755726)，接受率为 $\alpha(x,y) = \min\{1, \exp(\Delta)\}$, 其中 $\Delta = \log \pi_d(y) - \log \pi_d(x)$ 是对数密度比。

对于积形式的[目标分布](@entry_id:634522)，$\log \pi_d(x) = \sum_{i=1}^d \log f(x_i)$。令 $g(u) = \log f(u)$，则对[数密度](@entry_id:268986)比可以写为：
$ \Delta = \sum_{i=1}^d [g(y_i) - g(x_i)] = \sum_{i=1}^d \left[g\left(x_i + \frac{\ell}{\sqrt{d}}Z_i\right) - g(x_i)\right] $
由于当 $d \to \infty$ 时，步长分量 $\frac{\ell}{\sqrt{d}}Z_i$ 很小，我们可以对每个 $g$ 函数进行二阶[泰勒展开](@entry_id:145057) ：
$ g\left(x_i + \frac{\ell}{\sqrt{d}}Z_i\right) - g(x_i) \approx g'(x_i)\left(\frac{\ell}{\sqrt{d}}Z_i\right) + \frac{1}{2}g''(x_i)\left(\frac{\ell}{\sqrt{d}}Z_i\right)^2 $
将所有分量的贡献加起来，我们得到 $\Delta$ 的近似：
$ \Delta \approx \frac{\ell}{\sqrt{d}} \sum_{i=1}^d g'(x_i)Z_i + \frac{\ell^2}{2d} \sum_{i=1}^d g''(x_i)Z_i^2 $

现在，我们分析这两个和式的极限行为。在[稳态](@entry_id:182458)下，$x_i$ 是从 $f$ 中抽取的独立同分布样本，而 $Z_i$ 是独立的标准正态变量。

1.  **第二项（均值项）**：对于第二项 $\frac{\ell^2}{2d} \sum_{i=1}^d g''(x_i)Z_i^2$，其中的和式是[独立同分布随机变量](@entry_id:270381) $g''(x_i)Z_i^2$ 的样本均值（乘以 $d$）。根据大数定律，当 $d \to \infty$ 时，这个样本均值会收敛到其[期望值](@entry_id:153208)：
    $ \frac{1}{d} \sum_{i=1}^d g''(x_i)Z_i^2 \xrightarrow{P} \mathbb{E}[g''(X)Z^2] = \mathbb{E}[g''(X)]\mathbb{E}[Z^2] = \mathbb{E}[g''(X)] $
    在标准的[正则性条件](@entry_id:166962)下，通过分部积分可以证明一个关键的恒等式（巴特莱特第二恒等式）：$\mathbb{E}[g''(X)] = -\mathbb{E}[(g'(X))^2]$。我们定义**费雪信息 (Fisher Information)** 为 $I = \mathbb{E}[(g'(X))^2]$。因此，第二项在概率上收敛到一个常数：
    $ \frac{\ell^2}{2d} \sum_{i=1}^d g''(x_i)Z_i^2 \xrightarrow{P} -\frac{\ell^2 I}{2} $

2.  **第一项（[方差](@entry_id:200758)项）**：第一项 $\frac{\ell}{\sqrt{d}} \sum_{i=1}^d g'(x_i)Z_i$ 是一个由[独立同分布随机变量](@entry_id:270381) $W_i = g'(x_i)Z_i$ 构成的和式，并经过了中心极限定理的标准缩放。这些变量的均值为 $\mathbb{E}[W_i] = \mathbb{E}[g'(x_i)]\mathbb{E}[Z_i] = 0$（假设 $\mathbb{E}[g'(X)]=0$），[方差](@entry_id:200758)为 $\text{Var}(W_i) = \mathbb{E}[W_i^2] = \mathbb{E}[(g'(x_i))^2]\mathbb{E}[Z_i^2] = I \cdot 1 = I$。根据中心极限定理，当 $d \to \infty$ 时：
    $ \frac{1}{\sqrt{d}} \sum_{i=1}^d g'(x_i)Z_i \xrightarrow{\mathcal{D}} \mathcal{N}(0, I) $
    因此，第一项[依分布收敛](@entry_id:275544)到一个正态[随机变量](@entry_id:195330)：
    $ \frac{\ell}{\sqrt{d}} \sum_{i=1}^d g'(x_i)Z_i \xrightarrow{\mathcal{D}} \mathcal{N}(0, \ell^2 I) $

综合以上两点，并应用[斯卢茨基定理](@entry_id:181685) (Slutsky's Theorem)，对[数密度](@entry_id:268986)比 $\Delta$ [依分布收敛](@entry_id:275544)到一个正态[随机变量](@entry_id:195330) $W$，其均值是第一项和第二项极限均值之和，[方差](@entry_id:200758)是第一项的极限[方差](@entry_id:200758)。因此，我们得到了一个核心的结论 ：
$ \Delta \xrightarrow{\mathcal{D}} W \sim \mathcal{N}\left(-\frac{\ell^2 I}{2}, \ell^2 I\right) $

有了 $\Delta$ 的[极限分布](@entry_id:174797)，我们就可以[计算极限](@entry_id:138209)平均接受率 $a(\ell) = \lim_{d\to\infty} \mathbb{E}[\alpha(x,y)] = \mathbb{E}[\min\{1, \exp(W)\}]$。对于一个服从 $\mathcal{N}(\mu, \sigma^2)$ [分布](@entry_id:182848)的[随机变量](@entry_id:195330) $W$，这个期望有一个优美的解析解。特别地，对于我们导出的[极限分布](@entry_id:174797)，其均值 $\mu = -\ell^2 I / 2$ 和[方差](@entry_id:200758) $\sigma^2 = \ell^2 I$ 之间存在一个特殊关系：$\mu = -\sigma^2/2$。利用这个关系，可以证明 ：
$ a(\ell) = \mathbb{E}[\min\{1, \exp(W)\}] = 2\Phi\left(-\frac{\sigma}{2}\right) = 2\Phi\left(-\frac{\ell\sqrt{I}}{2}\right) $
其中 $\Phi(\cdot)$ 是标准正态分布的[累积分布函数 (CDF)](@entry_id:264700)。

### 最优缩放的确定

现在，我们将所有部分组合起来以确定最优的缩放参数 $\ell$。我们的目标是最大化渐进的ESJD。在 $d \to \infty$ 的极限下，提议的平方跳跃距离 $\|y-x\|^2 = \ell^2 \|Z\|^2/d$ 收敛到 $\ell^2$，而接受率收敛到 $a(\ell)$。因此，渐进的 ESJD 函数为：
$ \mathcal{E}(\ell) = \ell^2 a(\ell) = 2\ell^2 \Phi\left(-\frac{\ell\sqrt{I}}{2}\right) $
为了找到使 $\mathcal{E}(\ell)$ 最大化的 $\ell$，我们对其求导并令其为0。令 $u = \ell\sqrt{I}/2$，这个[优化问题](@entry_id:266749)最终归结为求解一个关于 $u$ 的[超越方程](@entry_id:276279) $2\Phi(-u) = u\phi(-u)$，其中 $\phi$ 是[标准正态分布](@entry_id:184509)的[概率密度函数](@entry_id:140610)。这个方程的数值解为 $u_\star \approx 1.19$。

由此，我们得到最优的缩放参数 $\ell_\star$：
$ \frac{\ell_\star\sqrt{I}}{2} = u_\star \implies \ell_\star \approx \frac{2 \times 1.19}{\sqrt{I}} = \frac{2.38}{\sqrt{I}} $
将这个最优值代回接受率公式 $a(\ell)$，我们得到了一个著名的普适性结论：
$ a(\ell_\star) = 2\Phi\left(-\frac{\ell_\star\sqrt{I}}{2}\right) = 2\Phi(-u_\star) \approx 2\Phi(-1.19) \approx 0.234 $
这个结果令人瞩目：对于一大类满足[正则性条件](@entry_id:166962)的积形式[目标分布](@entry_id:634522)，最优的RWM算法应该被调节到使其平均接受率约为 **0.234**。这个“0.234法则”为实践者提供了一个极其有用的指导原则 。值得注意的是，[最优步长](@entry_id:143372) $\ell_\star$ 取决于目标分布的（一维边际）费雪信息 $I$，这反映了[目标分布](@entry_id:634522)的“陡峭”程度。

### [扩散极限](@entry_id:168181)：更深层的理论视角

ESJD 提供了一个物理上直观的优化目标，但最优缩放理论的更深层基础在于RWM算法与一个[连续时间随机过程](@entry_id:188424)之间的联系。理论表明，如果我们不仅缩放空间（通过 $\sigma_d = \ell/\sqrt{d}$），还加速时间（将时间步 $k$ 视为连续时间 $t=k/d$），那么当 $d \to \infty$ 时，[马尔可夫链](@entry_id:150828)的任何一个坐标的演化过程 $U^{(d)}(t) = X_{\lfloor td \rfloor, 1}^{(d)}$ 会[依分布收敛](@entry_id:275544)到一个一维的**过阻尼朗之万[扩散](@entry_id:141445) (overdamped Langevin diffusion)** 过程 。

这个极限[扩散过程](@entry_id:170696) $U_t$ 遵循一个[随机微分方程](@entry_id:146618) (SDE)：
$ dU_t = \sqrt{h(\ell)}\, dB_t + \frac{1}{2} h(\ell)\, (\log f)'(U_t)\, dt $
其中 $B_t$ 是标准布朗运动，$(\log f)'$ 是对数密度的梯度（即[得分函数](@entry_id:164520)）。关键在于，这个SDE的[扩散](@entry_id:141445)和[漂移系数](@entry_id:199354)都由一个共同的“速度”函数 $h(\ell)$ 决定。这个函数 $h(\ell)$ 正是我们在前面遇到的渐进ESJD ：
$ h(\ell) = \ell^2 a(\ell) = 2\ell^2 \Phi\left(-\frac{\ell\sqrt{I}}{2}\right) $
这个SDE的[无穷小生成元](@entry_id:270424) $G$ 作用在光滑测试函数 $g$ 上，其形式为：
$ G g(u) = \frac{1}{2} h(\ell)\, g''(u) + \frac{1}{2} h(\ell)\, (\log f)'(u)\, g'(u) $
其中 $g''$ 的系数 $\frac{1}{2}h(\ell)$ 直接对应于SDE中的[扩散](@entry_id:141445)项 $\frac{1}{2}(\sqrt{h(\ell)})^2$，而 $g'$ 的系数则对应于漂移项。这个极限过程的平稳分布恰好是 $f(u)$，这表明该极限过程正确地保持了原始的[目标分布](@entry_id:634522)。

这个[扩散极限](@entry_id:168181)的视角为“0.234法则”提供了最终的理论依据。它表明，最大化 ESJD (即 $h(\ell)$) 等价于最大化描述链[长期行为](@entry_id:192358)的极限[朗之万动力学](@entry_id:142305)的[扩散](@entry_id:141445)速度。更快的[扩散](@entry_id:141445)意味着更快地探索[状态空间](@entry_id:177074)和更低的[自相关](@entry_id:138991)，这正是我们追求的[MCMC算法](@entry_id:751788)的效率。

### 理论的[适用范围](@entry_id:636189)、假设与局限性

任何强大的理论都有其边界。对于RWM的最优缩放理论，理解其假设和局限性对于避免在实践中误用至关重要。

#### [正则性条件](@entry_id:166962)

上述推导并非无条件成立。它依赖于目标密度 $f$ 的一系列**[正则性条件](@entry_id:166962)**。这些条件确保了[泰勒展开](@entry_id:145057)的有效性、[中心极限定理](@entry_id:143108)和强大数定律的适用性，以及各种极限运算的可交换性。一个典型且充分的条件集包括 ：
1.  **光滑性**: 对数密度 $\log f$ 至少是三阶连续可微的（$C^3$），并且其三阶导数有界。这保证了泰勒展开的[余项](@entry_id:159839)在 $d \to \infty$ 时可以被忽略。
2.  **[矩条件](@entry_id:136365)**: 相关的矩是有限的。最关键的是[费雪信息](@entry_id:144784) $0  I = \mathbb{E}[((\log f)')^2]  \infty$。$I0$ 确保了问题不是退化的，$I  \infty$ 是[中心极限定理](@entry_id:143108)成立的必要条件。此外，还需要对更高阶导数的矩做出假设，例如 $\mathbb{E}[|(\log f)''|^2]  \infty$，以确保[大数定律](@entry_id:140915)的应用。

这些条件对于积形式[目标分布](@entry_id:634522)是核心。对于更一般的、各向异性或坐标间存在弱依赖的目标 $\pi_d$，理论可以被推广，但需要直接假设一些关键的浓度现象，例如，要求 $\frac{1}{d}\|\nabla \log \pi_d(X)\|^2$ 和相关的二次型在 $d \to \infty$ 时[依概率收敛](@entry_id:145927)到确定的常数 。

#### 多峰[分布](@entry_id:182848)的挑战

最优缩放理论本质上描述的是算法在[目标分布](@entry_id:634522)典型区域内的**局部、[扩散](@entry_id:141445)性探索**行为。当[目标分布](@entry_id:634522)是多峰的，并且峰之间被低的概率区域（高[能量势](@entry_id:748988)垒）隔开时，该理论的预测可能会产生误导。

考虑一个由两个[高斯分布](@entry_id:154414)构成的平衡[混合模型](@entry_id:266571)，其两个均值（模式）之间的距离为 $O(\sqrt{d})$ 。在这种情况下：
*   **峰内混合 (Intra-modal mixing)**：在任何一个模式内部，目标分布局部看起来像一个单峰高斯分布。因此，采用 $\sigma_d = \ell_\star/\sqrt{d}$ 的步长（对应于0.234的接受率）将非常有效地探索该模式内部的区域。
*   **峰间混合 (Inter-modal mixing)**：然而，这种尺度的步长对于在两个模式之间跳跃是灾难性的。一次提议跳跃的典型大小是 $O(1)$，而模式间的距离是 $O(\sqrt{d})$。从一个模式的中心直接跳到另一个模式的中心的概率会随着 $d$ 的增加以 $\exp(-\Theta(d^2))$ 的速度衰减。更一般地，由于两个模式之间的“势垒”高度正比于 $d$，任何基于局部移动的算法（如RWM）从一个模式逃逸到另一个模式所需的期望时间将随维度 $d$ 呈指数增长，即 $\exp(\Theta(d))$。

这个例子清晰地表明，为局部[扩散](@entry_id:141445)优化的“0.234法则”并不能解决全局探索的挑战。在面对多峰性时，仅靠调节RWM的步长是远远不够的，需要采用更先进的、能够进行大尺度跳跃或跨越能量势垒的算法（如并行[回火](@entry_id:182408)、[哈密顿蒙特卡洛](@entry_id:144208)等）。

#### ESJD 作为优化目标的局限性

虽然 ESJD 是一个非常有效的全局性能代理，但在某些情况下，它可能无法完全捕捉到对于特定推断任务至关重要的混合行为。效率可能是**函数依赖的 (functional-dependent)**。

考虑一个具有“平坦”区域和“尖峰”区域的复合[目标分布](@entry_id:634522)。例如，一个在原点附近有极高曲率（由参数 $\epsilon \ll 1$ 控制），而在外部区域曲率平缓的[分布](@entry_id:182848) 。在这种情况下，ESJD 主要由链在大部分时间所处的平坦区域的行为所决定。我们可以找到两个不同的步长 $s_1$ 和 $s_2$，它们在平坦区域产生相似的移动效率，因此产生非常接近的全局 ESJD 值。

然而，如果我们关心的统计量是一个[非线性](@entry_id:637147)函数，例如指示函数 $g(x) = \mathbf{1}\{|x|\delta\}$，它度量了链在尖峰区域内的概率质量，那么算法的效率将取决于链在平坦区域和尖峰区域之间转换的频率。较大的步长虽然在平坦区域可能仍然有效，但它“命中”狭窄尖峰区域的概率会降低（通常与 $1/s$ 成反比）。这会导致两个区域间的混合变慢，从而增大了估计 $\mathbb{E}[g(x)]$ 的渐进[方差](@entry_id:200758)。因此，尽管两个步长 $s_1$ 和 $s_2$ 的 ESJD 相近，但具有较小步长的 $s_1$ 可能在估计尖峰区域的性质方面远比 $s_2$ 高效。这揭示了单一的、全局的效率度量（如ESJD）与特定于任务的效率（由特定函数的渐进[方差](@entry_id:200758)衡量）之间的潜在差异。

#### 边界效应

最优缩放理论的标准形式是在无约束的[欧氏空间](@entry_id:138052) $\mathbb{R}^d$ 中推导的。当[目标分布](@entry_id:634522)被限制在一个有界域（如超立方体 $[0,1]^d$）上时，一个自然的问题是：边界的存在是否会改变最优缩放的结果？

答案取决于目标密度在边界附近的行为。考虑一个在 $[0,1]^d$ 上的积形式目标，其一维边际是参数为 $\alpha$ 的对称Beta[分布](@entry_id:182848) $f_\alpha(x) \propto x^{\alpha-1}(1-x)^{\alpha-1}$ 。
*   如果 $\alpha  2$，那么不仅密度函数 $f_\alpha(x)$ 在[边界点](@entry_id:176493) $x=0$ 和 $x=1$ 处为0，其导数 $f'_\alpha(x)$ 也在边界处为0。这意味着[目标分布](@entry_id:634522)本身强烈地将[马尔可夫链](@entry_id:150828)推离边界。
*   在这种情况下，尽管提议过程可能需要边界反射来保持在域内，但一个坐标处于需要反射的[边界层](@entry_id:139416)（宽度为 $O(\sigma_d) = O(d^{-1/2})$）的概率会随着 $d$ 的增加而迅速衰减（速度快于 $d^{-1}$）。
*   因此，在 $d \to \infty$ 的极限中，边界反射事件发生的总概率趋于0。边界的存在对算法的渐进行为没有影响。极限[扩散过程](@entry_id:170696)、渐进接受率和最优缩放常数 $\ell_\star$ 都与无界情况下的结果完全相同。由边界引起的对最优缩放常数的修正项的极限为0。

这个结论再次强调了高维极限的“局部”性质：只要[目标分布](@entry_id:634522)在边界附近足够“温和”（即密度和其梯度消失），算法的行为就由[分布](@entry_id:182848)的“内部”特性主导，全局的几何约束变得无关紧要。