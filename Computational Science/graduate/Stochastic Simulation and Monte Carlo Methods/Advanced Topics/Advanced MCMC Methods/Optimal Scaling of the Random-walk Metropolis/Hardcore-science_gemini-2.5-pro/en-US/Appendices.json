{
    "hands_on_practices": [
        {
            "introduction": "This foundational exercise tackles two key questions for efficient high-dimensional sampling: what is the optimal *shape* for the proposal distribution, and what is the optimal *size* of the steps? Through an analysis of a Gaussian target, you will use symmetry arguments to determine the ideal proposal structure and derive the asymptotic relationship between step size and the canonical acceptance rate of $0.234$. This practice builds essential theoretical skills for understanding how and why MCMC algorithms are tuned .",
            "id": "3325167",
            "problem": "Consider the Random-Walk Metropolis (RWM) algorithm within Markov Chain Monte Carlo (MCMC) targeting a $d$-dimensional Gaussian distribution with density $\\pi(x) \\propto \\exp\\!\\big(-\\tfrac{1}{2} x^{\\top} \\Sigma^{-1} x\\big)$, i.e., $\\mathcal{N}(0,\\Sigma)$, where $\\Sigma$ is a symmetric positive-definite covariance matrix. Proposals are of the form $Y = X + \\xi$ with $\\xi \\sim \\mathcal{N}(0,C)$ and acceptance probability $\\alpha(X,Y) = \\min\\!\\big\\{1, \\exp\\!\\big(\\log \\pi(Y) - \\log \\pi(X)\\big)\\big\\}$. Define the Expected Squared Jump Distance (ESJD) as\n$$\n\\mathrm{ESJD}(C) = \\mathbb{E}\\Big[ \\|Y - X\\|^{2} \\, \\mathbf{1}\\{\\text{move accepted}\\} \\Big],\n$$\nwhere the expectation is with respect to the stationary distribution $\\pi$ and the proposal mechanism, and $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function.\n\n(a) Using first principles (linear transformations and rotational invariance), show that, up to a scalar scaling of the proposal covariance, the ESJD is maximized by choosing the proposal covariance proportional to the target covariance, i.e., $C$ proportional to $\\Sigma$.\n\n(b) Now restrict attention to the high-dimensional small-step regime in which proposals are of the form\n$$\nY = X + \\frac{l}{\\sqrt{d}} \\, \\Sigma^{1/2} Z, \\quad Z \\sim \\mathcal{N}(0,I_{d}),\n$$\nfor a dimension-free step-size parameter $l > 0$. Derive the asymptotic acceptance rate $\\alpha(l)$ in terms of $l$ for the Gaussian target in this regime and determine the value of $l$ that achieves the canonical acceptance rate target $0.234$. Round your final numerical value of $l$ to four significant figures. Your final answer must be the single real number $l$.",
            "solution": "The problem consists of two parts. Part (a) asks for a proof regarding the optimal proposal covariance for a Random-Walk Metropolis (RWM) sampler targeting a multivariate Gaussian distribution. Part (b) requires the derivation of the asymptotic acceptance rate in a high-dimensional regime and the calculation of the step-size parameter $l$ that yields a specific acceptance rate.\n\n### Part (a): Optimal Proposal Covariance\n\nThe goal is to maximize the Expected Squared Jump Distance (ESJD), defined as\n$$\n\\mathrm{ESJD}(C) = \\mathbb{E}\\Big[ \\|Y - X\\|^{2} \\, \\mathbf{1}\\{\\text{move accepted}\\} \\Big] = \\mathbb{E}\\Big[ \\|Y - X\\|^{2} \\, \\alpha(X,Y) \\Big]\n$$\nby choosing the proposal covariance matrix $C$. The expectation is over the stationary distribution $X \\sim \\pi = \\mathcal{N}(0,\\Sigma)$ and the proposal $Y = X + \\xi$ with $\\xi \\sim \\mathcal{N}(0,C)$. The acceptance probability is $\\alpha(X,Y) = \\min\\{1, \\pi(Y)/\\pi(X)\\}$.\n\nWe approach this using the principles of linear transformation and rotational invariance. The core idea is to transform the problem into a space where the target distribution is spherically symmetric, which simplifies the analysis.\n\n1.  **Linear Transformation (Whitening):**\n    Let $\\Sigma^{1/2}$ be the symmetric positive-definite square root of $\\Sigma$. We define a new set of coordinates via the linear transformation $x' = \\Sigma^{-1/2}x$. If $X \\sim \\mathcal{N}(0, \\Sigma)$, the transformed variable $X' = \\Sigma^{-1/2}X$ follows a standard multivariate normal distribution, $X' \\sim \\mathcal{N}(0, I_d)$, where $I_d$ is the $d \\times d$ identity matrix. The probability density of $X'$ is $\\pi'(x') \\propto \\exp(-\\frac{1}{2} x'^{\\top}x')$, which is spherically symmetric (i.e., invariant under rotations).\n\n2.  **Transformed Proposal:**\n    The proposal mechanism $Y = X + \\xi$ is transformed into the new coordinate system. Let $Y' = \\Sigma^{-1/2}Y$ and $\\xi' = \\Sigma^{-1/2}\\xi$.\n    $$\n    Y' = \\Sigma^{-1/2}(X+\\xi) = \\Sigma^{-1/2}X + \\Sigma^{-1/2}\\xi = X' + \\xi'\n    $$\n    Since $\\xi \\sim \\mathcal{N}(0, C)$, the transformed innovation term $\\xi'$ is also normally distributed with mean $\\mathbb{E}[\\xi'] = \\Sigma^{-1/2}\\mathbb{E}[\\xi] = 0$ and covariance\n    $$\n    C' = \\mathrm{Cov}(\\xi') = \\mathbb{E}[\\xi' \\xi'^{\\top}] = \\mathbb{E}[\\Sigma^{-1/2}\\xi (\\Sigma^{-1/2}\\xi)^{\\top}] = \\Sigma^{-1/2} \\mathbb{E}[\\xi\\xi^{\\top}] \\Sigma^{-1/2} = \\Sigma^{-1/2}C\\Sigma^{-1/2}\n    $$\n    So, in the whitened space, the proposal is $Y' = X' + \\xi'$ where $\\xi' \\sim \\mathcal{N}(0, C')$.\n\n3.  **Invariance of Acceptance Probability:**\n    The acceptance probability $\\alpha(X,Y)$ depends on the ratio of densities $\\pi(Y)/\\pi(X)$. Let's express this in terms of the transformed variables:\n    $$\n    \\log\\left(\\frac{\\pi(Y)}{\\pi(X)}\\right) = -\\frac{1}{2}Y^{\\top}\\Sigma^{-1}Y - \\left(-\\frac{1}{2}X^{\\top}\\Sigma^{-1}X\\right) = -\\frac{1}{2}(Y'^{\\top}Y' - X'^{\\top}X') = -\\frac{1}{2}(\\|Y'\\|^2 - \\|X'\\|^2)\n    $$\n    Thus, the acceptance probability $\\alpha$ depends only on the squared Euclidean norms of the state and proposal in the whitened space.\n\n4.  **Rotational Symmetry Argument:**\n    The problem of efficiently sampling from the standard normal target $\\mathcal{N}(0, I_d)$ using an RWM algorithm is now a function of the transformed proposal covariance $C'$. The target distribution $\\mathcal{N}(0, I_d)$ is invariant under any orthogonal transformation (rotation) $O$. That is, if $X' \\sim \\mathcal{N}(0,I_d)$, then $OX' \\sim \\mathcal{N}(0,I_d)$.\n\n    The ESJD is a measure of the sampler's efficiency. Let us consider the ESJD in the whitened space, $\\mathrm{ESJD}'(C') = \\mathbb{E}\\left[ \\|Y' - X'\\|^2 \\alpha(X',Y') \\right]$, as a proxy for efficiency. The entire sampling problem in this space is rotationally symmetric. If a proposal covariance $C'_{\\text{opt}}$ maximizes this efficiency metric (for a fixed scale, e.g., $\\det(C')=\\text{constant}$), then any rotated version of it, $O C'_{\\text{opt}} O^{\\top}$, must also be optimal. This is because a rotation of the proposal cannot affect the efficiency of sampling from a spherically symmetric target.\n\n    If we assume there is a unique optimal shape for the covariance matrix, it must be invariant under all rotations. That is, $C'_{\\text{opt}} = O C'_{\\text{opt}} O^{\\top}$ for all orthogonal matrices $O$. By Schur's Lemma, a matrix that commutes with all orthogonal matrices must be a scalar multiple of the identity matrix. Therefore, the optimal proposal covariance in the whitened space must be of the form $C'_{\\text{opt}} = c I_d$ for some scalar $c > 0$. An isotropic proposal is optimal for an isotropic target.\n\n5.  **Transforming Back to Original Space:**\n    Having established that the optimal choice for $C'$ is $c I_d$, we can find the corresponding optimal $C$ in the original space:\n    $$\n    C' = \\Sigma^{-1/2} C \\Sigma^{-1/2} = c I_d\n    $$\n    Pre-multiplying by $\\Sigma^{1/2}$ and post-multiplying by $\\Sigma^{1/2}$, we obtain:\n    $$\n    \\Sigma^{1/2}(\\Sigma^{-1/2} C \\Sigma^{-1/2})\\Sigma^{1/2} = \\Sigma^{1/2}(c I_d)\\Sigma^{1/2}\n    $$\n    $$\n    I_d C I_d = c \\Sigma^{1/2}\\Sigma^{1/2}\n    $$\n    $$\n    C = c \\Sigma\n    $$\n    This shows that the proposal covariance $C$ must be proportional to the target covariance $\\Sigma$ to maximize the sampler's efficiency, as measured by the ESJD.\n\n### Part (b): Asymptotic Acceptance Rate and Optimal Step Size\n\nWe are given the proposal form for the high-dimensional ($d \\to \\infty$) small-step regime:\n$$\nY = X + \\frac{l}{\\sqrt{d}} \\Sigma^{1/2} Z, \\quad Z \\sim \\mathcal{N}(0,I_{d})\n$$\nThe acceptance probability is $\\alpha(X,Y) = \\min\\{1, \\exp(\\Delta E)\\}$, where $\\Delta E = \\log \\pi(Y) - \\log \\pi(X)$.\n\n1.  **Derivation of $\\Delta E$:**\n    As shown in part (a), $\\Delta E = -\\frac{1}{2}(\\|Y'\\|^2 - \\|X'\\|^2)$, where $X' = \\Sigma^{-1/2}X$ and $Y' = \\Sigma^{-1/2}Y$.\n    The proposal in the whitened space is $Y' = X' + \\frac{l}{\\sqrt{d}} Z$.\n    Substituting this into the expression for $\\Delta E$:\n    $$\n    \\Delta E = -\\frac{1}{2}\\left(\\left\\|X' + \\frac{l}{\\sqrt{d}} Z\\right\\|^2 - \\|X'\\|^2\\right)\n    $$\n    $$\n    = -\\frac{1}{2}\\left(\\|X'\\|^2 + 2\\frac{l}{\\sqrt{d}} X'^{\\top}Z + \\frac{l^2}{d}\\|Z\\|^2 - \\|X'\\|^2\\right)\n    $$\n    $$\n    = -\\frac{l}{\\sqrt{d}} X'^{\\top}Z - \\frac{l^2}{2d}\\|Z\\|^2\n    $$\n\n2.  **Asymptotic Behavior ($d \\to \\infty$):**\n    We analyze the limiting distribution of $\\Delta E$.\n    - At stationarity, $X \\sim \\mathcal{N}(0, \\Sigma)$, so $X' = \\Sigma^{-1/2}X \\sim \\mathcal{N}(0,I_d)$. The components $X'_i$ are i.i.d. $\\mathcal{N}(0,1)$.\n    - The proposal innovation is $Z \\sim \\mathcal{N}(0,I_d)$. By the Law of Large Numbers (LLN), $\\frac{1}{d}\\|Z\\|^2 = \\frac{1}{d}\\sum_{i=1}^d Z_i^2 \\xrightarrow{p} \\mathbb{E}[Z_1^2] = 1$.\n    - The term $W_d = \\frac{1}{\\sqrt{d}}X'^{\\top}Z = \\frac{1}{\\sqrt{d}}\\sum_{i=1}^d X'_i Z_i$ is a sum of i.i.d. random variables with mean 0 and variance 1. By the Central Limit Theorem (CLT), $W_d$ converges in distribution to a standard normal variable, which we denote by $\\mathcal{G} \\sim \\mathcal{N}(0,1)$.\n\n    Substituting these asymptotic results into the expression for $\\Delta E$, we find that $\\Delta E$ converges in distribution:\n    $$\n    \\Delta E \\xrightarrow{\\mathcal{D}} -l\\mathcal{G} - \\frac{l^2}{2}\n    $$\n    The limiting distribution of $\\Delta E$ is a normal distribution with mean $-l^2/2$ and variance $l^2$.\n\n3.  **Asymptotic Acceptance Rate $\\alpha(l)$:**\n    The asymptotic acceptance rate is the expectation of the acceptance probability with respect to the limiting distribution of $\\Delta E$. Let $\\mathcal{W} \\sim \\mathcal{N}(-l^2/2, l^2)$ be the limiting variable.\n    $$\n    \\alpha(l) = \\mathbb{E}[\\min\\{1, \\exp(\\mathcal{W})\\}]\n    $$\n    This is a standard result. The expectation evaluates to:\n    $$\n    \\alpha(l) = 2\\Phi\\left(-\\frac{\\sqrt{\\text{Var}(\\mathcal{W})}}{2}\\right) = 2\\Phi\\left(-\\frac{\\sqrt{l^2}}{2}\\right) = 2\\Phi(-l/2)\n    $$\n    where $\\Phi(\\cdot)$ is the standard normal CDF.\n\n4.  **Solving for $l$:**\n    We are asked to find the value of $l$ that results in the canonical acceptance rate of $0.234$.\n    $$\n    \\alpha(l) = 2\\Phi(-l/2) = 0.234\n    $$\n    $$\n    \\Phi(-l/2) = \\frac{0.234}{2} = 0.117\n    $$\n    To solve for $l$, we use the inverse standard normal CDF, $\\Phi^{-1}$:\n    $$\n    -l/2 = \\Phi^{-1}(0.117)\n    $$\n    $$\n    l = -2 \\Phi^{-1}(0.117)\n    $$\n    Using a statistical calculator or software, we find the value of the quantile function $\\Phi^{-1}(0.117) \\approx -1.19015$.\n    Therefore,\n    $$\n    l \\approx -2 \\times (-1.19015) = 2.3803\n    $$\n    Rounding to four significant figures, we get $l \\approx 2.380$.",
            "answer": "$$\\boxed{2.380}$$"
        },
        {
            "introduction": "Theoretical results are most powerful when they can be verified and applied in practice, and this exercise bridges that gap between analytic formulas and numerical implementation. You will codify the asymptotic acceptance rate and sampler speed functions derived from theory. Subsequently, you will perform a numerical search to find the scaling parameter that maximizes sampler efficiency, thereby confirming the celebrated $0.234$ optimal acceptance rate through direct computation .",
            "id": "3325139",
            "problem": "Consider the Random-Walk Metropolis (RWM) algorithm within Markov chain Monte Carlo (MCMC) targeting the $d$-dimensional standard normal distribution with independent and identically distributed coordinates, denoted by the density $\\pi_d(\\mathbf{x}) \\propto \\exp\\!\\left(-\\frac{1}{2}\\lVert \\mathbf{x} \\rVert^2\\right)$ for $\\mathbf{x} \\in \\mathbb{R}^d$. Proposals are generated as $\\mathbf{Y} = \\mathbf{X} + s \\mathbf{Z}$ with $\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$ independent of $\\mathbf{X}$, where $s = \\frac{l}{\\sqrt{d}}$ and $l > 0$ is a dimensionless scaling parameter. The Metropolis acceptance probability is $\\alpha(\\mathbf{X}, \\mathbf{Y}) = \\min\\!\\left\\{1, \\frac{\\pi_d(\\mathbf{Y})}{\\pi_d(\\mathbf{X})}\\right\\}$, and the algorithm is run at stationarity where $\\mathbf{X} \\sim \\pi_d$.\n\nDefine the asymptotic acceptance rate function $a(l)$ as the limit of the expected acceptance probability under stationarity as $d \\to \\infty$, and define the asymptotic speed function $h(l)$ as the limiting expected squared jump distance (suitably rescaled to capture the diffusion limit) as $d \\to \\infty$. Both $a(l)$ and $h(l)$ are functions of the dimensionless scaling parameter $l$.\n\nStarting from the fundamental definitions above (target distribution, proposal mechanism, and acceptance probability) and without using pre-stated optimization formulas, derive expressions for $a(l)$ and $h(l)$ that arise in the high-dimensional limit $d \\to \\infty$. Then, implement a program that numerically evaluates these functions across $l$ and identifies the value of $l$ that maximizes $h(l)$. Finally, confirm numerically that the acceptance rate at this maximizing $l$ is close to the constant $0.234$.\n\nYour program must:\n- Compute $a(l)$ and $h(l)$ on specified grids of $l$.\n- Find the maximizing $l$ for $h(l)$ via a grid search.\n- Report the acceptance rate at the maximizing $l$.\n- Provide boolean confirmations for specified closeness or inequality conditions.\n\nTest Suite:\nFor each case below, perform a grid search over the interval and number of points specified, compute the acceptance rate $a(l^\\star)$ at the $l^\\star$ that maximizes $h(l)$ on that grid, and return the result specified.\n\n1. Case A (wide search): $l \\in [0.01, 10.0]$ using $5001$ evenly spaced points. Return the acceptance $a(l^\\star)$ as a float, and a boolean whether $\\lvert a(l^\\star) - 0.234 \\rvert \\leq 0.01$.\n2. Case B (small-step regime): $l \\in [0.001, 0.3]$ using $1000$ points. Return the acceptance $a(l^\\star)$ as a float, and a boolean whether $a(l^\\star) \\geq 0.5$.\n3. Case C (large-step regime): $l \\in [5.0, 15.0]$ using $1000$ points. Return the acceptance $a(l^\\star)$ as a float, and a boolean whether $a(l^\\star) \\leq 0.05$.\n4. Case D (focused search): $l \\in [0.05, 5.0]$ using $2000$ points. Return the acceptance $a(l^\\star)$ as a float, and a boolean whether $\\lvert a(l^\\star) - 0.234 \\rvert \\leq 0.005$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as follows for Cases Aâ€“D: $[a_A,\\text{close}_A,a_B,\\text{cond}_B,a_C,\\text{cond}_C,a_D,\\text{close}_D]$, where each $a_\\cdot$ is a float and each $\\text{close}_\\cdot$ or $\\text{cond}_\\cdot$ is a boolean. No physical units or angles are involved in this problem.",
            "solution": "The problem is to derive the asymptotic forms of the acceptance rate, $a(l)$, and the speed function, $h(l)$, for a Random-Walk Metropolis (RWM) algorithm targeting a $d$-dimensional standard normal distribution, and then to numerically optimize the speed.\n\nLet the target distribution in $\\mathbb{R}^d$ be $\\pi_d(\\mathbf{x}) \\propto \\exp(-\\frac{1}{2}\\lVert \\mathbf{x} \\rVert^2)$. The proposal is $\\mathbf{Y} = \\mathbf{X} + s \\mathbf{Z}$, where $\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$ and the step size is $s = l/\\sqrt{d}$. At stationarity, $\\mathbf{X} \\sim \\pi_d$.\n\nThe logarithm of the Metropolis-Hastings ratio is:\n$$\n\\Delta_d = \\log\\left(\\frac{\\pi_d(\\mathbf{Y})}{\\pi_d(\\mathbf{X})}\\right) = -\\frac{1}{2}(\\lVert \\mathbf{Y} \\rVert^2 - \\lVert \\mathbf{X} \\rVert^2)\n$$\nSubstituting the proposal expression:\n$$\n\\lVert \\mathbf{Y} \\rVert^2 - \\lVert \\mathbf{X} \\rVert^2 = \\left\\lVert \\mathbf{X} + \\frac{l}{\\sqrt{d}}\\mathbf{Z} \\right\\rVert^2 - \\lVert \\mathbf{X} \\rVert^2 = \\frac{2l}{\\sqrt{d}}\\mathbf{X}^T\\mathbf{Z} + \\frac{l^2}{d}\\lVert \\mathbf{Z} \\rVert^2\n$$\nSo, $\\Delta_d = -l \\frac{\\mathbf{X}^T\\mathbf{Z}}{\\sqrt{d}} - \\frac{l^2}{2} \\frac{\\lVert \\mathbf{Z} \\rVert^2}{d}$.\n\nIn the limit as $d \\to \\infty$, we analyze the two terms. Since $X_i, Z_i \\sim \\mathcal{N}(0,1)$ are i.i.d., the Central Limit Theorem implies $\\frac{1}{\\sqrt{d}}\\mathbf{X}^T\\mathbf{Z} \\xrightarrow{\\mathcal{D}} W \\sim \\mathcal{N}(0, 1)$. The Law of Large Numbers implies $\\frac{1}{d}\\lVert \\mathbf{Z} \\rVert^2 \\xrightarrow{p} \\mathbb{E}[Z_1^2] = 1$.\nBy Slutsky's theorem, $\\Delta_d$ converges in distribution to $\\Delta_\\infty = -lW - \\frac{l^2}{2}$. This limiting variable follows a normal distribution $\\mathcal{N}(-l^2/2, l^2)$.\n\nThe asymptotic acceptance rate $a(l)$ is the expectation of $\\min\\{1, \\exp(\\Delta_\\infty)\\}$.\n$$\na(l) = \\mathbb{E}_W\\left[\\min\\left\\{1, \\exp\\left(-lW - \\frac{l^2}{2}\\right)\\right\\}\\right]\n$$\nThe condition $\\exp(-lW - l^2/2) \\le 1$ holds when $-lW - l^2/2 \\le 0$, which for $l>0$ is equivalent to $W \\ge -l/2$. We partition the expectation:\n$$\na(l) = \\int_{-\\infty}^{-l/2} 1 \\cdot \\phi(w) \\,dw + \\int_{-l/2}^{\\infty} \\exp\\left(-lw - \\frac{l^2}{2}\\right) \\phi(w) \\,dw\n$$\nwhere $\\phi(w)$ is the standard normal PDF. The first integral is the standard normal CDF, $\\Phi(-l/2)$. The second integral can be shown to equal $\\Phi(-l/2)$ by completing the square in the exponent. This yields the asymptotic acceptance rate:\n$$\na(l) = 2\\Phi(-l/2)\n$$\n\nThe asymptotic speed function $h(l)$ is the limiting expected squared jump distance (ESJD), rescaled by an appropriate factor.\n$$\n\\text{ESJD}_d = \\mathbb{E}[\\alpha(\\mathbf{X}, \\mathbf{Y}) \\lVert \\mathbf{Y} - \\mathbf{X} \\rVert^2] = \\mathbb{E}\\left[\\alpha(\\mathbf{X}, \\mathbf{Y}) \\left\\lVert \\frac{l}{\\sqrt{d}}\\mathbf{Z} \\right\\rVert^2\\right] = \\frac{l^2}{d} \\mathbb{E}[\\alpha(\\mathbf{X}, \\mathbf{Y}) \\lVert\\mathbf{Z}\\rVert^2]\n$$\nIn the high-dimensional limit, the acceptance probability $\\alpha$ becomes independent of any single component of $\\mathbf{Z}$ and also of the norm $\\lVert\\mathbf{Z}\\rVert^2/d$, which concentrates at 1. Thus, the expectation decouples:\n$$\n\\lim_{d\\to\\infty} \\text{ESJD}_d = \\lim_{d\\to\\infty} \\frac{l^2}{d} \\mathbb{E}[\\alpha(\\mathbf{X}, \\mathbf{Y})] \\mathbb{E}[\\lVert\\mathbf{Z}\\rVert^2] = \\lim_{d\\to\\infty} \\frac{l^2}{d} a(l) \\cdot d = l^2 a(l)\n$$\nThe speed function is therefore:\n$$\nh(l) = l^2 a(l) = 2l^2\\Phi(-l/2)\n$$\nTo optimize the RWM algorithm, we seek the value $l^\\star$ that maximizes $h(l)$. This optimization is performed numerically by the provided code.\nThe code implements these functions and performs a grid search over specified intervals of $l$ to find the approximate maximizer $l^\\star$ of $h(l)$, and reports the acceptance rate $a(l^\\star)$ along with other specified checks.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Analyzes the optimal scaling of the Random-Walk Metropolis algorithm.\n\n    This function implements the derived asymptotic acceptance rate a(l) and\n    speed function h(l) for the RWM algorithm in high dimensions. It performs\n    a grid search to find the scaling parameter l that maximizes the speed\n    for different regimes, and reports the corresponding acceptance rates\n    and boolean checks as per the problem specification.\n    \"\"\"\n    \n    # Test cases defined in the problem statement\n    # Each case is a tuple: (l_min, l_max, num_points, check_type, check_value, tolerance)\n    # check_type: 1 for |a - val| = tol, 2 for a >= val, 3 for a = val\n    test_cases = [\n        # Case A: Wide search\n        (0.01, 10.0, 5001, 1, 0.234, 0.01),\n        # Case B: Small-step regime\n        (0.001, 0.3, 1000, 2, 0.5, None),\n        # Case C: Large-step regime\n        (5.0, 15.0, 1000, 3, 0.05, None),\n        # Case D: Focused search\n        (0.05, 5.0, 2000, 1, 0.234, 0.005),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        l_min, l_max, num_points, check_type, check_val, check_tol = case\n\n        # Create the grid of l values\n        l_grid = np.linspace(l_min, l_max, num_points, dtype=np.float64)\n\n        # Calculate the asymptotic acceptance rate a(l)\n        # a(l) = 2 * Phi(-l/2), where Phi is the standard normal CDF.\n        a_values = 2.0 * norm.cdf(-l_grid / 2.0)\n\n        # Calculate the asymptotic speed function h(l)\n        # h(l) = l^2 * a(l)\n        h_values = l_grid**2 * a_values\n\n        # Find the index of the maximum value of h(l) on the grid\n        idx_max = np.argmax(h_values)\n\n        # Get the acceptance rate at the l that maximizes h(l)\n        a_star = a_values[idx_max]\n        results.append(a_star)\n\n        # Perform the boolean check for the current case\n        if check_type == 1:\n            # Check if |a_star - check_val| = check_tol\n            condition_met = np.abs(a_star - check_val) = check_tol\n        elif check_type == 2:\n            # Check if a_star >= check_val\n            condition_met = a_star >= check_val\n        elif check_type == 3:\n            # Check if a_star = check_val\n            condition_met = a_star = check_val\n        \n        results.append(condition_met)\n\n    # Format the results into the required string format\n    # [a_A, close_A, a_B, cond_B, a_C, cond_C, a_D, close_D]\n    final_output_str = f\"[{','.join(str(x) for x in results)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The concept of 'optimal scaling' is often linked to maximizing the Expected Squared Jump Distance (ESJD), but what does this mean for statistical estimation? This exercise reveals the practical importance of this criterion by connecting it directly to the goal of minimizing estimator variance. You will discover that the scaling that maximizes the sampler's global speed is precisely the one that minimizes the variance of an estimate for a single coordinate, revealing a deep and powerful connection between geometry and statistical efficiency .",
            "id": "3325173",
            "problem": "Consider the Random-Walk Metropolis (RWM) algorithm for a $d$-dimensional target distribution $\\pi_{d}$ that is the product of $d$ independent standard normal components, so $\\pi_{d}(x) \\propto \\exp\\!\\left(-\\frac{1}{2} \\sum_{i=1}^{d} x_{i}^{2}\\right)$ for $x \\in \\mathbb{R}^{d}$. The proposal from current state $x$ is $y = x + \\frac{\\ell}{\\sqrt{d}} z$, where $z \\sim \\mathcal{N}(0, I_{d})$, and the Metropolis-Hastings acceptance probability is $\\alpha(x,y) = 1 \\wedge \\frac{\\pi_{d}(y)}{\\pi_{d}(x)}$. Let $h(x) = x_{1}$ be a low-dimensional functional of interest. Define the Expected Squared Jump Distance (ESJD) as the expected squared Euclidean norm of the accepted jump, i.e., $\\mathbb{E}\\!\\left[\\|y-x\\|^{2} \\cdot \\mathbf{1}\\{\\text{accept}\\}\\right]$, and the global ESJD refers to this quantity in the $d$-dimensional space. Assume the chain is started in stationarity.\n\nUsing only fundamental properties of the standard normal distribution, asymptotics for sums of independent and identically distributed random variables, and the definition of the Metropolis-Hastings algorithm, derive in the high-dimensional limit $d \\to \\infty$:\n- An explicit expression (in terms of $\\ell$) for the limiting average acceptance probability.\n- The leading-order dependence of the asymptotic variance of the ergodic average $n^{-1} \\sum_{k=1}^{n} h(X^{(k)})$ on $\\ell$, for large $d$, up to a positive multiplicative constant independent of $\\ell$.\n\nThen, determine the value of the proposal scaling $\\ell$ that minimizes this asymptotic variance for $h(x)=x_{1}$. Separately, determine the value of $\\ell$ that maximizes the global Expected Squared Jump Distance (ESJD). Provide both optimal $\\ell$ values. Round your answer to $4$ significant figures. Express the final answer as a pair in a single row matrix, where the first entry is the $\\ell$ that minimizes the asymptotic variance of $h(x)=x_{1}$, and the second entry is the $\\ell$ that maximizes the global ESJD.",
            "solution": "This problem requires deriving the optimal scaling parameter $\\ell$ under two different criteria: minimizing the asymptotic variance for a single coordinate and maximizing the global Expected Squared Jump Distance (ESJD). We begin by deriving the limiting average acceptance probability.\n\n### Limiting Average Acceptance Probability\n\nThe acceptance probability is $\\alpha(x,y) = 1 \\wedge \\frac{\\pi_{d}(y)}{\\pi_{d}(x)}$. The log of the density ratio is:\n$$\n\\Delta = \\log\\left(\\frac{\\pi_{d}(y)}{\\pi_{d}(x)}\\right) = -\\frac{1}{2}(\\|y\\|^2 - \\|x\\|^2)\n$$\nSubstituting the proposal $y = x + \\frac{\\ell}{\\sqrt{d}} z$:\n$$\n\\Delta = -\\frac{1}{2}\\left(2\\frac{\\ell}{\\sqrt{d}} x \\cdot z + \\frac{\\ell^2}{d}\\|z\\|^2\\right) = -\\ell \\frac{x \\cdot z}{\\sqrt{d}} - \\frac{\\ell^2}{2} \\frac{\\|z\\|^2}{d}\n$$\nIn the stationary limit ($x \\sim \\mathcal{N}(0, I_d)$) and as $d \\to \\infty$, the Central Limit Theorem shows $\\frac{x \\cdot z}{\\sqrt{d}} \\to W \\sim \\mathcal{N}(0,1)$, and the Law of Large Numbers shows $\\frac{\\|z\\|^2}{d} \\to 1$. Thus, $\\Delta$ converges in distribution to $\\Delta_\\infty = -\\ell W - \\frac{\\ell^2}{2}$, which follows a normal distribution $\\mathcal{N}(-\\ell^2/2, \\ell^2)$.\n\nThe limiting average acceptance probability, $\\bar{\\alpha}(\\ell)$, is $\\mathbb{E}[1 \\wedge \\exp(\\Delta_\\infty)]$. The expectation is over $W \\sim \\mathcal{N}(0,1)$. The term $\\exp(-\\ell W - \\ell^2/2)$ is less than or equal to 1 if and only if $-\\ell W - \\ell^2/2 \\le 0$, which for $\\ell > 0$ means $W \\ge -\\ell/2$. The expectation integral is:\n$$\n\\bar{\\alpha}(\\ell) = \\int_{-\\infty}^{-\\ell/2} 1 \\cdot \\phi(w) dw + \\int_{-\\ell/2}^{\\infty} \\exp(-\\ell w - \\ell^2/2) \\phi(w) dw\n$$\nwhere $\\phi(\\cdot)$ is the standard normal PDF. The first term is $\\Phi(-\\ell/2)$, the standard normal CDF. The second integral, after completing the square in the exponent, can be shown to also equal $\\Phi(-\\ell/2)$. Therefore, the well-known result is:\n$$\n\\bar{\\alpha}(\\ell) = 2\\Phi(-\\ell/2)\n$$\n\n### Optimal Scaling for $h(x)=x_1$\n\nThe asymptotic variance of the ergodic average of a functional $h(X)$ is inversely proportional to the sampler's efficiency with respect to that functional. In the diffusive limit of RWM, this efficiency is given by the Expected Squared Jump Distance (ESJD) for the component of interest, here $h(x)=x_1$. To minimize the asymptotic variance, we must maximize this component-wise ESJD.\n\nThe ESJD for the first component is:\n$$\n\\text{ESJD}_1 = \\mathbb{E}\\left[ (y_1 - x_1)^2 \\cdot \\mathbf{1}\\{\\text{accept}\\} \\right] = \\mathbb{E}\\left[ \\left(\\frac{\\ell}{\\sqrt{d}} z_1\\right)^2 \\alpha(x,y) \\right] = \\frac{\\ell^2}{d} \\mathbb{E}[z_1^2 \\alpha(x,y)]\n$$\nIn the limit as $d \\to \\infty$, the acceptance probability $\\alpha(x,y)$ depends on sums over all $d$ dimensions and becomes asymptotically independent of any single component $z_1$. Thus, the expectation decouples. The limiting, rescaled efficiency for the first coordinate is:\n$$\n\\lim_{d\\to\\infty} d \\cdot \\text{ESJD}_1 = \\ell^2 \\mathbb{E}[z_1^2] \\mathbb{E}[\\lim_{d\\to\\infty} \\alpha(x,y)] = \\ell^2 \\cdot 1 \\cdot \\bar{\\alpha}(\\ell)\n$$\nTo minimize the asymptotic variance, we must maximize the function $f(\\ell) = \\ell^2 \\bar{\\alpha}(\\ell) = 2\\ell^2 \\Phi(-\\ell/2)$.\n\n### Optimal Scaling for Global ESJD\n\nThe global ESJD is $\\mathbb{E}[\\|y-x\\|^2 \\cdot \\mathbf{1}\\{\\text{accept}\\}]$. In the high-dimensional limit, the expectation again decouples for the same reasons:\n$$\n\\text{ESJD}_{\\text{global}} = \\mathbb{E}\\left[ \\left\\|\\frac{\\ell}{\\sqrt{d}} z\\right\\|^2 \\alpha(x,y) \\right] = \\frac{\\ell^2}{d} \\mathbb{E}[\\|z\\|^2 \\alpha(x,y)] \\approx \\frac{\\ell^2}{d} \\mathbb{E}[\\|z\\|^2] \\mathbb{E}[\\alpha(x,y)] = \\frac{\\ell^2}{d} \\cdot d \\cdot \\bar{\\alpha}(\\ell) = \\ell^2 \\bar{\\alpha}(\\ell)\n$$\nMaximizing the global ESJD is therefore equivalent to maximizing the same function $f(\\ell) = \\ell^2 \\bar{\\alpha}(\\ell)$.\n\n### Optimization\nBoth criteria lead to maximizing $f(\\ell) = 2\\ell^2 \\Phi(-\\ell/2)$ for $\\ell > 0$. We find the maximum by setting the derivative to zero:\n$$\nf'(\\ell) = 4\\ell \\Phi(-\\ell/2) + 2\\ell^2 \\phi(-\\ell/2) \\cdot \\left(-\\frac{1}{2}\\right) = \\ell \\left( 4\\Phi(-\\ell/2) - \\ell \\phi(-\\ell/2) \\right) = 0\n$$\nSince $\\ell \\ne 0$, we solve the transcendental equation $4\\Phi(-\\ell/2) = \\ell \\phi(-\\ell/2)$. A numerical solution yields $\\ell/2 \\approx 1.19064$, which gives the optimal scaling parameter:\n$$\n\\ell \\approx 2 \\times 1.19064 = 2.38128\n$$\nRounding to 4 significant figures, $\\ell \\approx 2.381$. Since both optimization problems are equivalent, the optimal $\\ell$ is the same for both.\n\nThe optimal value for minimizing the asymptotic variance of $h(x)=x_1$ is $\\ell \\approx 2.381$.\nThe optimal value for maximizing the global ESJD is $\\ell \\approx 2.381$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.381  2.381\n\\end{pmatrix}\n}\n$$"
        }
    ]
}