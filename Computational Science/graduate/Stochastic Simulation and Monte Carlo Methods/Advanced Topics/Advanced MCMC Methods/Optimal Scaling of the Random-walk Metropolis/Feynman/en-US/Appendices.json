{
    "hands_on_practices": [
        {
            "introduction": "To effectively sample from a high-dimensional distribution, the proposal mechanism must be well-designed. This practice lays the theoretical groundwork for optimal scaling by first tackling a correlated Gaussian target, guiding you to prove that the most efficient proposal shape mirrors the target's own covariance structure . You will then derive the specific scaling parameter that achieves the canonical target acceptance rate of $0.234$, a cornerstone result in MCMC theory.",
            "id": "3325167",
            "problem": "Consider the Random-Walk Metropolis (RWM) algorithm within Markov Chain Monte Carlo (MCMC) targeting a $d$-dimensional Gaussian distribution with density $\\pi(x) \\propto \\exp\\!\\big(-\\tfrac{1}{2} x^{\\top} \\Sigma^{-1} x\\big)$, i.e., $\\mathcal{N}(0,\\Sigma)$, where $\\Sigma$ is a symmetric positive-definite covariance matrix. Proposals are of the form $Y = X + \\xi$ with $\\xi \\sim \\mathcal{N}(0,C)$ and acceptance probability $\\alpha(X,Y) = \\min\\!\\big\\{1, \\exp\\!\\big(\\log \\pi(Y) - \\log \\pi(X)\\big)\\big\\}$. Define the Expected Squared Jump Distance (ESJD) as\n$$\n\\mathrm{ESJD}(C) = \\mathbb{E}\\Big[ \\|Y - X\\|^{2} \\, \\mathbf{1}\\{\\text{move accepted}\\} \\Big],\n$$\nwhere the expectation is with respect to the stationary distribution $\\pi$ and the proposal mechanism, and $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function.\n\n(a) Using first principles (linear transformations and rotational invariance), show that, up to a scalar scaling of the proposal covariance, the ESJD is maximized by choosing the proposal covariance proportional to the target covariance, i.e., $C$ proportional to $\\Sigma$.\n\n(b) Now restrict attention to the high-dimensional small-step regime in which proposals are of the form\n$$\nY = X + \\frac{l}{\\sqrt{d}} \\, \\Sigma^{1/2} Z, \\quad Z \\sim \\mathcal{N}(0,I_{d}),\n$$\nfor a dimension-free step-size parameter $l > 0$. Derive the asymptotic acceptance rate $\\alpha(l)$ in terms of $l$ for the Gaussian target in this regime and determine the value of $l$ that achieves the canonical acceptance rate target $0.234$. Round your final numerical value of $l$ to four significant figures. Your final answer must be the single real number $l$.",
            "solution": "The problem consists of two parts. Part (a) asks for a proof regarding the optimal proposal covariance for a Random-Walk Metropolis (RWM) sampler targeting a multivariate Gaussian distribution. Part (b) requires the derivation of the asymptotic acceptance rate in a high-dimensional regime and the calculation of the step-size parameter $l$ that yields a specific acceptance rate.\n\n### Part (a): Optimal Proposal Covariance\n\nThe goal is to maximize the Expected Squared Jump Distance (ESJD), defined as\n$$\n\\mathrm{ESJD}(C) = \\mathbb{E}\\Big[ \\|Y - X\\|^{2} \\, \\mathbf{1}\\{\\text{move accepted}\\} \\Big] = \\mathbb{E}\\Big[ \\|Y - X\\|^{2} \\, \\alpha(X,Y) \\Big]\n$$\nby choosing the proposal covariance matrix $C$. The expectation is over the stationary distribution $X \\sim \\pi = \\mathcal{N}(0,\\Sigma)$ and the proposal $Y = X + \\xi$ with $\\xi \\sim \\mathcal{N}(0,C)$. The acceptance probability is $\\alpha(X,Y) = \\min\\{1, \\pi(Y)/\\pi(X)\\}$.\n\nWe approach this using the principles of linear transformation and rotational invariance. The core idea is to transform the problem into a space where the target distribution is spherically symmetric, which simplifies the analysis.\n\n1.  **Linear Transformation (Whitening):**\n    Let $\\Sigma^{1/2}$ be the symmetric positive-definite square root of $\\Sigma$. We define a new set of coordinates via the linear transformation $x' = \\Sigma^{-1/2}x$. If $X \\sim \\mathcal{N}(0, \\Sigma)$, the transformed variable $X' = \\Sigma^{-1/2}X$ follows a standard multivariate normal distribution, $X' \\sim \\mathcal{N}(0, I_d)$, where $I_d$ is the $d \\times d$ identity matrix. The probability density of $X'$ is $\\pi'(x') \\propto \\exp(-\\frac{1}{2} x'^{\\top}x')$, which is spherically symmetric (i.e., invariant under rotations).\n\n2.  **Transformed Proposal:**\n    The proposal mechanism $Y = X + \\xi$ is transformed into the new coordinate system. Let $Y' = \\Sigma^{-1/2}Y$ and $\\xi' = \\Sigma^{-1/2}\\xi$.\n    $$\n    Y' = \\Sigma^{-1/2}(X+\\xi) = \\Sigma^{-1/2}X + \\Sigma^{-1/2}\\xi = X' + \\xi'\n    $$\n    Since $\\xi \\sim \\mathcal{N}(0, C)$, the transformed innovation term $\\xi'$ is also normally distributed with mean $\\mathbb{E}[\\xi'] = \\Sigma^{-1/2}\\mathbb{E}[\\xi] = 0$ and covariance\n    $$\n    C' = \\mathrm{Cov}(\\xi') = \\mathbb{E}[\\xi' \\xi'^{\\top}] = \\mathbb{E}[\\Sigma^{-1/2}\\xi (\\Sigma^{-1/2}\\xi)^{\\top}] = \\Sigma^{-1/2} \\mathbb{E}[\\xi\\xi^{\\top}] \\Sigma^{-1/2} = \\Sigma^{-1/2}C\\Sigma^{-1/2}\n    $$\n    So, in the whitened space, the proposal is $Y' = X' + \\xi'$ where $\\xi' \\sim \\mathcal{N}(0, C')$.\n\n3.  **Invariance of Acceptance Probability:**\n    The acceptance probability $\\alpha(X,Y)$ depends on the ratio of densities $\\pi(Y)/\\pi(X)$. Let's express this in terms of the transformed variables:\n    $$\n    \\log\\left(\\frac{\\pi(Y)}{\\pi(X)}\\right) = -\\frac{1}{2}Y^{\\top}\\Sigma^{-1}Y - (-\\frac{1}{2}X^{\\top}\\Sigma^{-1}X) = -\\frac{1}{2}(Y'^{\\top}Y' - X'^{\\top}X') = -\\frac{1}{2}(\\|Y'\\|^2 - \\|X'\\|^2)\n    $$\n    Thus, the acceptance probability $\\alpha$ depends only on the squared Euclidean norms of the state and proposal in the whitened space.\n\n4.  **Rotational Symmetry Argument:**\n    The problem of efficiently sampling from the standard normal target $\\mathcal{N}(0, I_d)$ using an RWM algorithm is now a function of the transformed proposal covariance $C'$. The target distribution $\\mathcal{N}(0, I_d)$ is invariant under any orthogonal transformation (rotation) $O$. That is, if $X' \\sim \\mathcal{N}(0,I_d)$, then $OX' \\sim \\mathcal{N}(0,I_d)$.\n\n    The ESJD is a measure of the sampler's efficiency. Let us consider the ESJD in the whitened space, $\\mathrm{ESJD}'(C') = \\mathbb{E}\\left[ \\|Y' - X'\\|^2 \\alpha(X',Y') \\right]$, as a proxy for efficiency. The entire sampling problem in this space is rotationally symmetric. If a proposal covariance $C'_{\\text{opt}}$ maximizes this efficiency metric (for a fixed scale, e.g., $\\det(C')=\\text{constant}$), then any rotated version of it, $O C'_{\\text{opt}} O^{\\top}$, must also be optimal. This is because a rotation of the proposal cannot affect the efficiency of sampling from a spherically symmetric target.\n\n    If we assume there is a unique optimal shape for the covariance matrix, it must be invariant under all rotations. That is, $C'_{\\text{opt}} = O C'_{\\text{opt}} O^{\\top}$ for all orthogonal matrices $O$. By Schur's Lemma, a matrix that commutes with all orthogonal matrices must be a scalar multiple of the identity matrix. Therefore, the optimal proposal covariance in the whitened space must be of the form $C'_{\\text{opt}} = c I_d$ for some scalar $c > 0$. An isotropic proposal is optimal for an isotropic target.\n\n5.  **Transforming Back to Original Space:**\n    Having established that the optimal choice for $C'$ is $c I_d$, we can find the corresponding optimal $C$ in the original space:\n    $$\n    C' = \\Sigma^{-1/2} C \\Sigma^{-1/2} = c I_d\n    $$\n    Pre-multiplying by $\\Sigma^{1/2}$ and post-multiplying by $\\Sigma^{1/2}$, we obtain:\n    $$\n    \\Sigma^{1/2}(\\Sigma^{-1/2} C \\Sigma^{-1/2})\\Sigma^{1/2} = \\Sigma^{1/2}(c I_d)\\Sigma^{1/2}\n    $$\n    $$\n    I_d C I_d = c \\Sigma^{1/2}\\Sigma^{1/2}\n    $$\n    $$\n    C = c \\Sigma\n    $$\n    This shows that the proposal covariance $C$ must be proportional to the target covariance $\\Sigma$ to maximize the sampler's efficiency, as measured by the ESJD.\n\n### Part (b): Asymptotic Acceptance Rate and Optimal Step Size\n\nWe are given the proposal form for the high-dimensional ($d \\to \\infty$) small-step regime:\n$$\nY = X + \\frac{l}{\\sqrt{d}} \\Sigma^{1/2} Z, \\quad Z \\sim \\mathcal{N}(0,I_{d})\n$$\nThe acceptance probability is $\\alpha(X,Y) = \\min\\{1, \\exp(\\Delta E)\\}$, where $\\Delta E = \\log \\pi(Y) - \\log \\pi(X)$.\n\n1.  **Derivation of $\\Delta E$:**\n    As shown in part (a), $\\Delta E = -\\frac{1}{2}(\\|Y'\\|^2 - \\|X'\\|^2)$, where $X' = \\Sigma^{-1/2}X$ and $Y' = \\Sigma^{-1/2}Y$.\n    The proposal in the whitened space is $Y' = X' + \\frac{l}{\\sqrt{d}} Z$.\n    Substituting this into the expression for $\\Delta E$:\n    $$\n    \\Delta E = -\\frac{1}{2}\\left(\\left\\|X' + \\frac{l}{\\sqrt{d}} Z\\right\\|^2 - \\|X'\\|^2\\right)\n    $$\n    $$\n    = -\\frac{1}{2}\\left(\\|X'\\|^2 + 2\\frac{l}{\\sqrt{d}} X'^{\\top}Z + \\frac{l^2}{d}\\|Z\\|^2 - \\|X'\\|^2\\right)\n    $$\n    $$\n    = -\\frac{l}{\\sqrt{d}} X'^{\\top}Z - \\frac{l^2}{2d}\\|Z\\|^2\n    $$\n\n2.  **Asymptotic Behavior ($d \\to \\infty$):**\n    We analyze the limiting distribution of $\\Delta E$.\n    - The variable $X'$ follows a $\\mathcal{N}(0,I_d)$ distribution since we are in the stationary regime. The components $X'_i$ are i.i.d. $\\mathcal{N}(0,1)$. By the Law of Large Numbers (LLN), $\\frac{1}{d}\\|X'\\|^2 = \\frac{1}{d}\\sum_{i=1}^d (X'_i)^2 \\xrightarrow{p} \\mathbb{E}[(X'_1)^2] = 1$.\n    - Similarly, $Z \\sim \\mathcal{N}(0,I_d)$, so $\\frac{1}{d}\\|Z\\|^2 \\xrightarrow{p} 1$ by the LLN.\n    - Consider the term $W_d = \\frac{1}{\\sqrt{d}}X'^{\\top}Z = \\frac{1}{\\sqrt{d}}\\sum_{i=1}^d X'_i Z_i$. Conditioned on $X'$, $W_d$ is a normal random variable with mean $0$ and variance $\\frac{1}{d} \\sum_{i=1}^d (X'_i)^2 \\mathrm{Var}(Z_i) = \\frac{1}{d}\\|X'\\|^2$. Since $\\frac{1}{d}\\|X'\\|^2 \\xrightarrow{p} 1$ as $d \\to \\infty$, the conditional distribution of $W_d$ converges to a standard normal distribution $\\mathcal{N}(0,1)$. This implies that the unconditional distribution of $W_d$ also converges to $\\mathcal{N}(0,1)$. Let's denote this limiting standard normal variable by $\\mathcal{G}$.\n\n    Substituting these asymptotic results into the expression for $\\Delta E$:\n    $$\n    \\Delta E \\xrightarrow{d} -l\\mathcal{G} - \\frac{l^2}{2}\n    $$\n    The limiting distribution of $\\Delta E$ is a normal distribution with mean $-l^2/2$ and variance $l^2$.\n\n3.  **Asymptotic Acceptance Rate $\\alpha(l)$:**\n    The asymptotic acceptance rate is the expectation of the acceptance probability with respect to the limiting distribution of $\\Delta E$:\n    $$\n    \\alpha(l) = \\mathbb{E}_{\\mathcal{G}}\\left[\\min\\left\\{1, \\exp\\left(-l\\mathcal{G} - \\frac{l^2}{2}\\right)\\right\\}\\right]\n    $$\n    Let $\\phi(g)$ be the probability density function of $\\mathcal{G} \\sim \\mathcal{N}(0,1)$.\n    The expression $\\exp(-l\\mathcal{G} - l^2/2)$ is less than $1$ when its exponent is negative, i.e., $-l\\mathcal{G} - l^2/2  0$, which simplifies to $\\mathcal{G} > -l/2$ (since $l>0$).\n    We can split the expectation integral:\n    $$\n    \\alpha(l) = \\int_{-\\infty}^{-l/2} 1 \\cdot \\phi(g) \\,dg + \\int_{-l/2}^{\\infty} \\exp\\left(-lg - \\frac{l^2}{2}\\right) \\phi(g) \\,dg\n    $$\n    The first integral is the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(-l/2)$.\n    For the second integral, we substitute $\\phi(g) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-g^2/2)$:\n    $$\n    \\int_{-l/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-lg - \\frac{l^2}{2} - \\frac{g^2}{2}\\right) \\,dg\n    $$\n    We complete the square in the exponent: $-(\\frac{g^2}{2} + lg + \\frac{l^2}{2}) = -\\frac{1}{2}(g^2 + 2lg + l^2) = -\\frac{1}{2}(g+l)^2$.\n    The integral becomes:\n    $$\n    \\int_{-l/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(g+l)^2}{2}\\right) \\,dg\n    $$\n    Let $u = g+l$, so $du=dg$. The lower integration limit becomes $-l/2 + l = l/2$.\n    $$\n    \\int_{l/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) \\,du = 1 - \\Phi(l/2)\n    $$\n    Using the symmetry of the normal distribution, $1 - \\Phi(z) = \\Phi(-z)$, this integral is equal to $\\Phi(-l/2)$.\n    Combining the two parts, the asymptotic acceptance rate is:\n    $$\n    \\alpha(l) = \\Phi(-l/2) + \\Phi(-l/2) = 2\\Phi(-l/2)\n    $$\n\n4.  **Solving for $l$:**\n    We are asked to find the value of $l$ that results in the canonical acceptance rate of $0.234$.\n    $$\n    \\alpha(l) = 2\\Phi(-l/2) = 0.234\n    $$\n    $$\n    \\Phi(-l/2) = \\frac{0.234}{2} = 0.117\n    $$\n    To solve for $l$, we use the inverse standard normal CDF, $\\Phi^{-1}$:\n    $$\n    -l/2 = \\Phi^{-1}(0.117)\n    $$\n    $$\n    l = -2 \\Phi^{-1}(0.117)\n    $$\n    Using a statistical calculator or software, we find the value of the quantile function $\\Phi^{-1}(0.117) \\approx -1.19015$.\n    Therefore,\n    $$\n    l \\approx -2 \\times (-1.19015) = 2.3803\n    $$\n    Rounding to four significant figures, we get $l \\approx 2.380$.",
            "answer": "$$\\boxed{2.380}$$"
        },
        {
            "introduction": "Theoretical results come to life through implementation. Building upon the analytical foundations, this hands-on coding exercise asks you to numerically explore the trade-off between proposal size and acceptance rate . By implementing the asymptotic speed function $h(l)$, you will perform a grid search to find the optimal scaling parameter $l^{\\star}$ that maximizes sampling efficiency and verify that it corresponds to the celebrated $0.234$ acceptance rate.",
            "id": "3325139",
            "problem": "Consider the Random-Walk Metropolis (RWM) algorithm within Markov chain Monte Carlo (MCMC) targeting the $d$-dimensional standard normal distribution with independent and identically distributed coordinates, denoted by the density $\\pi_d(\\mathbf{x}) \\propto \\exp\\!\\left(-\\frac{1}{2}\\lVert \\mathbf{x} \\rVert^2\\right)$ for $\\mathbf{x} \\in \\mathbb{R}^d$. Proposals are generated as $\\mathbf{Y} = \\mathbf{X} + s \\mathbf{Z}$ with $\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$ independent of $\\mathbf{X}$, where $s = \\frac{l}{\\sqrt{d}}$ and $l > 0$ is a dimensionless scaling parameter. The Metropolis acceptance probability is $\\alpha(\\mathbf{X}, \\mathbf{Y}) = \\min\\!\\left\\{1, \\frac{\\pi_d(\\mathbf{Y})}{\\pi_d(\\mathbf{X})}\\right\\}$, and the algorithm is run at stationarity where $\\mathbf{X} \\sim \\pi_d$.\n\nDefine the asymptotic acceptance rate function $a(l)$ as the limit of the expected acceptance probability under stationarity as $d \\to \\infty$, and define the asymptotic speed function $h(l)$ as the limiting expected squared jump distance (suitably rescaled to capture the diffusion limit) as $d \\to \\infty$. Both $a(l)$ and $h(l)$ are functions of the dimensionless scaling parameter $l$.\n\nStarting from the fundamental definitions above (target distribution, proposal mechanism, and acceptance probability) and without using pre-stated optimization formulas, derive expressions for $a(l)$ and $h(l)$ that arise in the high-dimensional limit $d \\to \\infty$. Then, implement a program that numerically evaluates these functions across $l$ and identifies the value of $l$ that maximizes $h(l)$. Finally, confirm numerically that the acceptance rate at this maximizing $l$ is close to the constant $0.234$.\n\nYour program must:\n- Compute $a(l)$ and $h(l)$ on specified grids of $l$.\n- Find the maximizing $l$ for $h(l)$ via a grid search.\n- Report the acceptance rate at the maximizing $l$.\n- Provide boolean confirmations for specified closeness or inequality conditions.\n\nTest Suite:\nFor each case below, perform a grid search over the interval and number of points specified, compute the acceptance rate $a(l^\\star)$ at the $l^\\star$ that maximizes $h(l)$ on that grid, and return the result specified.\n\n1. Case A (wide search): $l \\in [0.01, 10.0]$ using $5001$ evenly spaced points. Return the acceptance $a(l^\\star)$ as a float, and a boolean whether $\\lvert a(l^\\star) - 0.234 \\rvert \\leq 0.01$.\n2. Case B (small-step regime): $l \\in [0.001, 0.3]$ using $1000$ points. Return the acceptance $a(l^\\star)$ as a float, and a boolean whether $a(l^\\star) \\geq 0.5$.\n3. Case C (large-step regime): $l \\in [5.0, 15.0]$ using $1000$ points. Return the acceptance $a(l^\\star)$ as a float, and a boolean whether $a(l^\\star) \\leq 0.05$.\n4. Case D (focused search): $l \\in [0.05, 5.0]$ using $2000$ points. Return the acceptance $a(l^\\star)$ as a float, and a boolean whether $\\lvert a(l^\\star) - 0.234 \\rvert \\leq 0.005$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as follows for Cases A–D: $[a_A,\\text{close}_A,a_B,\\text{cond}_B,a_C,\\text{cond}_C,a_D,\\text{close}_D]$, where each $a_\\cdot$ is a float and each $\\text{close}_\\cdot$ or $\\text{cond}_\\cdot$ is a boolean. No physical units or angles are involved in this problem.",
            "solution": "The user-provided problem is assessed to be valid. It is a well-posed, scientifically-grounded problem from the field of Markov chain Monte Carlo methods, specifically concerning the optimal scaling of Random-Walk Metropolis algorithms in high dimensions. The problem is self-contained and free of contradictions or ambiguities. I will now proceed with a formal derivation and solution.\n\nThe objective is to derive the asymptotic forms of the acceptance rate, $a(l)$, and the scaled speed function, $h(l)$, for a Random-Walk Metropolis (RWM) algorithm targeting a $d$-dimensional standard normal distribution, and then to numerically optimize the speed.\n\nLet the target distribution in $\\mathbb{R}^d$ be $\\pi_d(\\mathbf{x}) = (2\\pi)^{-d/2} \\exp(-\\frac{1}{2}\\lVert \\mathbf{x} \\rVert^2)$. The proposal is generated from the current state $\\mathbf{X}$ as $\\mathbf{Y} = \\mathbf{X} + s \\mathbf{Z}$, where $\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$ and the step size is $s = l/\\sqrt{d}$ for a scaling parameter $l > 0$. The algorithm is assumed to be in stationarity, so the current state $\\mathbf{X}$ is a draw from the target, $\\mathbf{X} \\sim \\pi_d$.\n\nThe Metropolis acceptance probability is given by $\\alpha(\\mathbf{X}, \\mathbf{Y}) = \\min\\left\\{1, \\frac{\\pi_d(\\mathbf{Y})}{\\pi_d(\\mathbf{X})}\\right\\}$.\nThe ratio of densities is:\n$$\n\\frac{\\pi_d(\\mathbf{Y})}{\\pi_d(\\mathbf{X})} = \\frac{\\exp(-\\frac{1}{2}\\lVert \\mathbf{Y} \\rVert^2)}{\\exp(-\\frac{1}{2}\\lVert \\mathbf{X} \\rVert^2)} = \\exp\\left(-\\frac{1}{2}(\\lVert \\mathbf{Y} \\rVert^2 - \\lVert \\mathbf{X} \\rVert^2)\\right)\n$$\nWe analyze the term in the exponent, $\\lVert \\mathbf{Y} \\rVert^2 - \\lVert \\mathbf{X} \\rVert^2$. Substituting the proposal expression:\n$$\n\\lVert \\mathbf{Y} \\rVert^2 = \\left\\lVert \\mathbf{X} + \\frac{l}{\\sqrt{d}}\\mathbf{Z} \\right\\rVert^2 = \\left(\\mathbf{X} + \\frac{l}{\\sqrt{d}}\\mathbf{Z}\\right)^T\\left(\\mathbf{X} + \\frac{l}{\\sqrt{d}}\\mathbf{Z}\\right) = \\lVert \\mathbf{X} \\rVert^2 + \\frac{2l}{\\sqrt{d}}\\mathbf{X}^T\\mathbf{Z} + \\frac{l^2}{d}\\lVert \\mathbf{Z} \\rVert^2\n$$\nThe difference is therefore:\n$$\n\\lVert \\mathbf{Y} \\rVert^2 - \\lVert \\mathbf{X} \\rVert^2 = \\frac{2l}{\\sqrt{d}}\\mathbf{X}^T\\mathbf{Z} + \\frac{l^2}{d}\\lVert \\mathbf{Z} \\rVert^2\n$$\nTo find the asymptotic behavior as $d \\to \\infty$, we examine each term.\nThe first term involves $\\mathbf{X}^T\\mathbf{Z} = \\sum_{i=1}^d X_i Z_i$. Since $\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$ and $\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$ are independent, their components $X_i$ and $Z_i$ are independent and identically distributed standard normal random variables. The expectation of the product is $\\mathbb{E}[X_i Z_i] = \\mathbb{E}[X_i]\\mathbb{E}[Z_i] = 0 \\cdot 0 = 0$. The variance is $\\text{Var}(X_i Z_i) = \\mathbb{E}[(X_i Z_i)^2] - (\\mathbb{E}[X_i Z_i])^2 = \\mathbb{E}[X_i^2]\\mathbb{E}[Z_i^2] = 1 \\cdot 1 = 1$.\nBy the Central Limit Theorem, the normalized sum converges in distribution to a standard normal random variable:\n$$\n\\frac{1}{\\sqrt{d}}\\mathbf{X}^T\\mathbf{Z} = \\frac{1}{\\sqrt{d}}\\sum_{i=1}^d X_i Z_i \\xrightarrow{d} W \\sim \\mathcal{N}(0, 1) \\quad \\text{as } d \\to \\infty\n$$\n\nThe second term involves $\\lVert \\mathbf{Z} \\rVert^2 = \\sum_{i=1}^d Z_i^2$. This is a sum of $d$ i.i.d. random variables $Z_i^2$, where $Z_i \\sim \\mathcal{N}(0,1)$, so $\\mathbb{E}[Z_i^2] = 1$. By the Law of Large Numbers:\n$$\n\\frac{1}{d}\\lVert \\mathbf{Z} \\rVert^2 = \\frac{1}{d}\\sum_{i=1}^d Z_i^2 \\xrightarrow{p} \\mathbb{E}[Z_1^2] = 1 \\quad \\text{as } d \\to \\infty\n$$\nwhere $\\xrightarrow{p}$ denotes convergence in probability.\n\nLet $\\Delta_d = -\\frac{1}{2}(\\lVert \\mathbf{Y} \\rVert^2 - \\lVert \\mathbf{X} \\rVert^2)$. Substituting the limits of the components:\n$$\n\\Delta_d = -\\frac{1}{2}\\left(\\frac{2l}{\\sqrt{d}}\\mathbf{X}^T\\mathbf{Z} + \\frac{l^2}{d}\\lVert \\mathbf{Z} \\rVert^2\\right) \\xrightarrow{d} -lW - \\frac{l^2}{2} \\quad \\text{as } d \\to \\infty\n$$\nThe acceptance probability $\\alpha_d = \\min\\{1, \\exp(\\Delta_d)\\}$ is a continuous function of $\\Delta_d$. By the continuous mapping theorem, its limiting distribution is:\n$$\n\\alpha_d \\xrightarrow{d} \\alpha(l, W) = \\min\\left\\{1, \\exp\\left(-lW - \\frac{l^2}{2}\\right)\\right\\}\n$$\nThe asymptotic acceptance rate $a(l)$ is the expectation of this limiting random variable with respect to $W \\sim \\mathcal{N}(0,1)$:\n$$\na(l) = \\mathbb{E}_W\\left[\\min\\left\\{1, \\exp\\left(-lW - \\frac{l^2}{2}\\right)\\right\\}\\right]\n$$\nThe expression inside the minimum is less than $1$ when its exponent is negative, i.e., $-lW - l^2/2  0$, which for $l>0$ is equivalent to $W > -l/2$. We partition the expectation over the real line:\n$$\na(l) = \\int_{-\\infty}^{-l/2} 1 \\cdot \\phi(w) \\,dw + \\int_{-l/2}^{\\infty} \\exp\\left(-lw - \\frac{l^2}{2}\\right) \\phi(w) \\,dw\n$$\nwhere $\\phi(w) = (2\\pi)^{-1/2}\\exp(-w^2/2)$ is the standard normal PDF.\nThe first integral is the definition of the standard normal cumulative distribution function (CDF), $\\Phi(-l/2)$.\nFor the second integral, we complete the square in the exponent:\n$$\n-lw - \\frac{l^2}{2} - \\frac{w^2}{2} = -\\frac{1}{2}(w^2 + 2lw + l^2) = -\\frac{1}{2}(w+l)^2\n$$\nThe integral becomes:\n$$\n\\int_{-l/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(w+l)^2}{2}\\right) \\,dw\n$$\nLet $u = w+l$, so $du = dw$. The lower integration limit becomes $-l/2 + l = l/2$. The integral is $\\int_{l/2}^{\\infty} \\phi(u) \\,du = 1 - \\Phi(l/2) = \\Phi(-l/2)$ by symmetry of the normal distribution.\nCombining the two parts, we obtain the asymptotic acceptance rate:\n$$\na(l) = \\Phi(-l/2) + \\Phi(-l/2) = 2\\Phi(-l/2)\n$$\n\nNext, we define the asymptotic speed function $h(l)$. This is a measure of the algorithm's efficiency, defined as the (rescaled) expected squared jump distance (ESJD). The jump is $\\mathbf{X}_{k+1} - \\mathbf{X}_k$. This is $\\mathbf{Y} - \\mathbf{X} = s\\mathbf{Z}$ if the proposal is accepted, and $\\mathbf{0}$ if rejected.\n$$\n\\text{ESJD}_d = \\mathbb{E}[\\lVert \\mathbf{X}_{k+1} - \\mathbf{X}_k \\rVert^2] = \\mathbb{E}[\\alpha(\\mathbf{X}, \\mathbf{Y}) \\lVert s\\mathbf{Z} \\rVert^2]\n$$\nHere, $\\mathbf{X}$ is the state at stationarity. Since $\\mathbf{Z}$ is independent of $\\mathbf{X}$, we can separate expectations:\n$$\n\\text{ESJD}_d = \\mathbb{E}[\\alpha(\\mathbf{X}, \\mathbf{Y})] \\cdot \\mathbb{E}[s^2 \\lVert \\mathbf{Z} \\rVert^2] = \\mathbb{E}[\\alpha(\\mathbf{X}, \\mathbf{Y})] \\cdot \\frac{l^2}{d} \\cdot d = l^2 \\mathbb{E}[\\alpha(\\mathbf{X}, \\mathbf{Y})]\n$$\nThe quantity $\\mathbb{E}[\\alpha(\\mathbf{X}, \\mathbf{Y})]$ is the expected acceptance rate for a finite dimension $d$. In the limit $d \\to \\infty$, this converges to $a(l)$. The limiting ESJD, which we call the speed function $h(l)$, is therefore:\n$$\nh(l) = l^2 a(l) = 2l^2\\Phi(-l/2)\n$$\nThis function $h(l)$ represents the trade-off between making large-variance proposals (large $l^2$) and having them accepted (large $a(l)$). To optimize the RWM algorithm's performance in high dimensions, one must find the value of $l$, denoted $l^\\star$, that maximizes $h(l)$. This optimization problem is typically solved numerically. The corresponding acceptance rate $a(l^\\star)$ is a universal constant for this class of problems, known to be approximately $0.234$.\n\nThe computational task is to implement the functions $a(l)$ and $h(l)$, perform a grid search over specified intervals of $l$ to find the approximate maximizer $l^\\star$ of $h(l)$, and report the acceptance rate $a(l^\\star)$ and other specified logical conditions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Analyzes the optimal scaling of the Random-Walk Metropolis algorithm.\n\n    This function implements the derived asymptotic acceptance rate a(l) and\n    speed function h(l) for the RWM algorithm in high dimensions. It performs\n    a grid search to find the scaling parameter l that maximizes the speed\n    for different regimes, and reports the corresponding acceptance rates\n    and boolean checks as per the problem specification.\n    \"\"\"\n    \n    # Test cases defined in the problem statement\n    # Each case is a tuple: (l_min, l_max, num_points, check_type, check_value)\n    # check_type: 1 for |a - val| = tol, 2 for a = val, 3 for a = val\n    test_cases = [\n        # Case A: Wide search\n        (0.01, 10.0, 5001, 1, 0.234, 0.01),\n        # Case B: Small-step regime\n        (0.001, 0.3, 1000, 2, 0.5, None),\n        # Case C: Large-step regime\n        (5.0, 15.0, 1000, 3, 0.05, None),\n        # Case D: Focused search\n        (0.05, 5.0, 2000, 1, 0.234, 0.005),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        l_min, l_max, num_points, check_type, check_val, check_tol = case\n\n        # Create the grid of l values\n        l_grid = np.linspace(l_min, l_max, num_points, dtype=np.float64)\n\n        # Calculate the asymptotic acceptance rate a(l)\n        # a(l) = 2 * Phi(-l/2), where Phi is the standard normal CDF.\n        a_values = 2.0 * norm.cdf(-l_grid / 2.0)\n\n        # Calculate the asymptotic speed function h(l)\n        # h(l) = l^2 * a(l)\n        h_values = l_grid**2 * a_values\n\n        # Find the index of the maximum value of h(l) on the grid\n        idx_max = np.argmax(h_values)\n\n        # Get the acceptance rate at the l that maximizes h(l)\n        a_star = a_values[idx_max]\n        results.append(a_star)\n\n        # Perform the boolean check for the current case\n        if check_type == 1:\n            # Check if |a_star - check_val| = check_tol\n            condition_met = np.abs(a_star - check_val) = check_tol\n        elif check_type == 2:\n            # Check if a_star = check_val\n            condition_met = a_star = check_val\n        elif check_type == 3:\n            # Check if a_star = check_val\n            condition_met = a_star = check_val\n        \n        results.append(condition_met)\n\n    # Format the results into the required string format\n    # [a_A, close_A, a_B, cond_B, a_C, cond_C, a_D, close_D]\n    final_output_str = f\"[{','.join(map(str, results))}]\"\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "When running an MCMC sampler, we might care about the overall efficiency of the chain or the precision of a single parameter estimate. This exercise explores the profound connection between these two goals, asking you to demonstrate a surprising equivalence in the high-dimensional limit . You will show that the scaling parameter $\\ell$ that maximizes the global Expected Squared Jump Distance (ESJD) is the very same one that minimizes the asymptotic variance for a single coordinate, justifying why ESJD is such a powerful and widely used metric for tuning.",
            "id": "3325173",
            "problem": "Consider the Random-Walk Metropolis (RWM) algorithm for a $d$-dimensional target distribution $\\pi_{d}$ that is the product of $d$ independent standard normal components, so $\\pi_{d}(x) \\propto \\exp\\!\\left(-\\frac{1}{2} \\sum_{i=1}^{d} x_{i}^{2}\\right)$ for $x \\in \\mathbb{R}^{d}$. The proposal from current state $x$ is $y = x + \\frac{\\ell}{\\sqrt{d}} z$, where $z \\sim \\mathcal{N}(0, I_{d})$, and the Metropolis-Hastings acceptance probability is $\\alpha(x,y) = 1 \\wedge \\frac{\\pi_{d}(y)}{\\pi_{d}(x)}$. Let $h(x) = x_{1}$ be a low-dimensional functional of interest. Define the Expected Squared Jump Distance (ESJD) as the expected squared Euclidean norm of the accepted jump, i.e., $\\mathbb{E}\\!\\left[\\|y-x\\|^{2} \\cdot \\mathbf{1}\\{\\text{accept}\\}\\right]$, and the global ESJD refers to this quantity in the $d$-dimensional space. Assume the chain is started in stationarity.\n\nUsing only fundamental properties of the standard normal distribution, asymptotics for sums of independent and identically distributed random variables, and the definition of the Metropolis-Hastings algorithm, derive in the high-dimensional limit $d \\to \\infty$:\n- An explicit expression (in terms of $\\ell$) for the limiting average acceptance probability.\n- The leading-order dependence of the asymptotic variance of the ergodic average $n^{-1} \\sum_{k=1}^{n} h(X^{(k)})$ on $\\ell$, for large $d$, up to a positive multiplicative constant independent of $\\ell$.\n\nThen, determine the value of the proposal scaling $\\ell$ that minimizes this asymptotic variance for $h(x)=x_{1}$. Separately, determine the value of $\\ell$ that maximizes the global Expected Squared Jump Distance (ESJD). Provide both optimal $\\ell$ values. Round your answer to $4$ significant figures. Express the final answer as a pair in a single row matrix, where the first entry is the $\\ell$ that minimizes the asymptotic variance of $h(x)=x_{1}$, and the second entry is the $\\ell$ that maximizes the global ESJD.",
            "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and self-contained.\n\n### Step 1: Extract Givens\n- Target distribution: $\\pi_{d}(x) \\propto \\exp(-\\frac{1}{2} \\sum_{i=1}^{d} x_{i}^{2})$ for $x \\in \\mathbb{R}^{d}$. This is a $d$-dimensional standard normal distribution, $\\mathcal{N}(0, I_{d})$.\n- Algorithm: Random-Walk Metropolis (RWM).\n- Proposal mechanism: From a current state $x$, a proposal $y$ is generated as $y = x + \\frac{\\ell}{\\sqrt{d}} z$, where $z \\sim \\mathcal{N}(0, I_{d})$.\n- Acceptance probability: $\\alpha(x,y) = 1 \\wedge \\frac{\\pi_{d}(y)}{\\pi_{d}(x)}$.\n- Functional of interest: $h(x) = x_{1}$.\n- Global Expected Squared Jump Distance (ESJD): $\\mathbb{E}[\\|y-x\\|^{2} \\cdot \\mathbf{1}\\{\\text{accept}\\}]$.\n- Assumption: The Markov chain is in its stationary distribution, i.e., $x \\sim \\pi_{d}$.\n- Task: In the limit as $d \\to \\infty$:\n  1. Derive the limiting average acceptance probability as a function of $\\ell$.\n  2. Derive the leading-order dependence of the asymptotic variance of the ergodic average of $h(x)=x_1$ on $\\ell$.\n  3. Determine the value of $\\ell$ that minimizes this asymptotic variance.\n  4. Determine the value of $\\ell$ that maximizes the global ESJD.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is a standard, canonical problem in the field of Markov chain Monte Carlo (MCMC) methods, specifically concerning the optimal scaling of high-dimensional samplers.\n- **Scientifically Grounded**: The problem is firmly rooted in probability theory, statistics, and the analysis of algorithms. The framework is that of the well-established Metropolis-Hastings algorithm. All premises are factually sound.\n- **Well-Posed**: The problem is clearly defined. The target, proposal, and quantities to be optimized are specified mathematically. The high-dimensional limit provides a clear asymptotic regime in which to work, leading to well-defined, unique solutions for the optimal scaling parameter.\n- **Objective**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Derivation of the Limiting Average Acceptance Probability\n\nThe acceptance probability is given by $\\alpha(x,y) = 1 \\wedge \\frac{\\pi_{d}(y)}{\\pi_{d}(x)}$. The ratio of densities is:\n$$\n\\frac{\\pi_{d}(y)}{\\pi_{d}(x)} = \\frac{\\exp(-\\frac{1}{2}\\|y\\|^2)}{\\exp(-\\frac{1}{2}\\|x\\|^2)} = \\exp\\left(-\\frac{1}{2}(\\|y\\|^2 - \\|x\\|^2)\\right)\n$$\nSubstituting $y = x + \\frac{\\ell}{\\sqrt{d}} z$:\n$$\n\\|y\\|^2 - \\|x\\|^2 = \\left\\|x + \\frac{\\ell}{\\sqrt{d}} z\\right\\|^2 - \\|x\\|^2 = \\|x\\|^2 + 2\\frac{\\ell}{\\sqrt{d}} x \\cdot z + \\frac{\\ell^2}{d}\\|z\\|^2 - \\|x\\|^2 = 2\\frac{\\ell}{\\sqrt{d}} x \\cdot z + \\frac{\\ell^2}{d}\\|z\\|^2\n$$\nThe logarithm of the density ratio, which we denote as $\\Delta$, is:\n$$\n\\Delta(x,z) = -\\frac{1}{2}\\left(2\\frac{\\ell}{\\sqrt{d}} x \\cdot z + \\frac{\\ell^2}{d}\\|z\\|^2\\right) = -\\ell \\frac{x \\cdot z}{\\sqrt{d}} - \\frac{\\ell^2}{2} \\frac{\\|z\\|^2}{d}\n$$\nWe analyze the behavior of the two terms in the limit $d \\to \\infty$. The chain is in stationarity, so $x \\sim \\mathcal{N}(0, I_{d})$, and the proposal innovation is $z \\sim \\mathcal{N}(0, I_d)$. The components $x_i$ and $z_i$ are all independent and identically distributed (i.i.d.) standard normal variables.\n\n1.  Consider the term $V_d = \\frac{\\|z\\|^2}{d} = \\frac{1}{d} \\sum_{i=1}^{d} z_i^2$. This is the sample mean of $d$ i.i.d. chi-squared variables with one degree of freedom, $\\chi^2(1)$. Since $\\mathbb{E}[z_i^2]=1$, by the Law of Large Numbers (LLN), $V_d \\to 1$ in probability as $d \\to \\infty$.\n\n2.  Consider the term $W_d = \\frac{x \\cdot z}{\\sqrt{d}} = \\frac{1}{\\sqrt{d}} \\sum_{i=1}^{d} x_i z_i$. The variables $U_i = x_i z_i$ are i.i.d. with mean $\\mathbb{E}[U_i] = \\mathbb{E}[x_i]\\mathbb{E}[z_i] = 0 \\cdot 0 = 0$ and variance $\\text{Var}(U_i) = \\mathbb{E}[U_i^2] - (\\mathbb{E}[U_i])^2 = \\mathbb{E}[x_i^2]\\mathbb{E}[z_i^2] - 0 = 1 \\cdot 1 = 1$. By the Central Limit Theorem (CLT), $W_d$ converges in distribution to a standard normal random variable, $W \\sim \\mathcal{N}(0,1)$.\n\nCombining these results, the log-ratio $\\Delta$ converges in distribution to a random variable $\\Delta_\\infty$:\n$$\n\\Delta(x,z) \\to \\Delta_\\infty = -\\ell W - \\frac{\\ell^2}{2}\n$$\nThe limiting random variable $\\Delta_\\infty$ follows a normal distribution: $\\mathbb{E}[\\Delta_\\infty] = -\\frac{\\ell^2}{2}$ and $\\text{Var}(\\Delta_\\infty) = (-\\ell)^2 \\text{Var}(W) = \\ell^2$. Thus, $\\Delta_\\infty \\sim \\mathcal{N}(-\\ell^2/2, \\ell^2)$.\n\nThe limiting average acceptance probability, $\\bar{\\alpha}(\\ell)$, is the expectation of $1 \\wedge \\exp(\\Delta_\\infty)$ over $W \\sim \\mathcal{N}(0,1)$. The term $\\exp(\\Delta_\\infty) = \\exp(-\\ell W - \\ell^2/2)$ is less than or equal to 1 if and only if its exponent is non-positive. For $\\ell > 0$, this condition is:\n$$-\\ell W - \\ell^2/2 \\le 0 \\iff \\ell W \\ge -\\ell^2/2 \\iff W \\ge -\\ell/2$$\nTherefore, we take the value 1 if $W  -\\ell/2$ and the value $\\exp(-\\ell W - \\ell^2/2)$ if $W \\ge -\\ell/2$. Let $\\phi(w)$ be the PDF of the standard normal distribution. The expectation can be written as the sum of two integrals:\n$$ \\bar{\\alpha}(\\ell) = \\int_{-\\infty}^{-\\ell/2} 1 \\cdot \\phi(w) dw + \\int_{-\\ell/2}^{\\infty} \\exp(-\\ell w - \\ell^2/2) \\phi(w) dw $$\nThe first integral is simply the definition of the standard normal CDF, $\\Phi(-\\ell/2)$. For the second integral, we substitute the definition of $\\phi(w)$ and complete the square in the exponent:\n$$ \\int_{-\\ell/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\ell w - \\frac{\\ell^2}{2} - \\frac{w^2}{2}\\right) dw = \\int_{-\\ell/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(w+\\ell)^2}{2}\\right) dw $$\nWe perform a change of variables $u = w+\\ell$. The integration limits change from $[-\\ell/2, \\infty)$ to $[\\ell/2, \\infty)$. The integral becomes:\n$$ \\int_{\\ell/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) du = \\int_{\\ell/2}^{\\infty} \\phi(u) du = 1 - \\Phi(\\ell/2) $$\nUsing the symmetry of the normal distribution, $1 - \\Phi(z) = \\Phi(-z)$, this is equal to $\\Phi(-\\ell/2)$.\nCombining the two parts, the final result for the limiting average acceptance probability is:\n$$ \\bar{\\alpha}(\\ell) = \\Phi(-\\ell/2) + \\Phi(-\\ell/2) = 2\\Phi(-\\ell/2) $$\n\n### Optimal Scaling for $h(x)=x_1$\nThe asymptotic variance of the ergodic average $\\frac{1}{n} \\sum_{k=1}^n h(X^{(k)})$ is inversely proportional to the algorithm's efficiency. For RWM in the high-dimensional, diffusive limit, the efficiency is proportional to the Expected Squared Jump Distance (ESJD) for the component of interest, $h(x)=x_1$. To minimize the asymptotic variance, we must maximize this ESJD.\nThe ESJD for the first component is:\n$$\n\\text{ESJD}_1 = \\mathbb{E}\\left[ (y_1 - x_1)^2 \\cdot \\mathbf{1}\\{\\text{accept}\\} \\right] = \\mathbb{E}\\left[ \\left(\\frac{\\ell}{\\sqrt{d}} z_1\\right)^2 \\alpha(x,y) \\right] = \\frac{\\ell^2}{d} \\mathbb{E}[z_1^2 \\alpha(x,y)]\n$$\nThe acceptance probability $\\alpha(x,y)$ depends on $x$ and $z=(z_1, \\dots, z_d)$. However, in the limit $d \\to \\infty$, its dependence on any single component $z_1$ becomes negligible because $\\alpha$ depends on sums over all $d$ components ($x \\cdot z$ and $\\|z\\|^2$). Therefore, $z_1^2$ and $\\alpha$ are asymptotically independent.\n$$\n\\lim_{d\\to\\infty} d \\cdot \\text{ESJD}_1 = \\ell^2 \\lim_{d\\to\\infty} \\mathbb{E}[z_1^2 \\alpha(x,y)] = \\ell^2 \\mathbb{E}[z_1^2] \\mathbb{E}[\\lim_{d\\to\\infty} \\alpha(x,y)] = \\ell^2 \\cdot 1 \\cdot \\bar{\\alpha}(\\ell)\n$$\nWe need to maximize the function $f(\\ell) = \\ell^2 \\bar{\\alpha}(\\ell) = 2\\ell^2 \\Phi(-\\ell/2)$ for $\\ell > 0$. We find the maximum by setting the derivative with respect to $\\ell$ to zero:\n$$\nf'(\\ell) = 4\\ell \\Phi(-\\ell/2) + 2\\ell^2 \\phi(-\\ell/2) \\cdot (-\\frac{1}{2}) = 4\\ell \\Phi(-\\ell/2) - \\ell^2 \\phi(-\\ell/2) = 0\n$$\nSince we seek $\\ell \\neq 0$, we can divide by $\\ell$:\n$$\n4\\Phi(-\\ell/2) = \\ell \\phi(-\\ell/2)\n$$\nThis is a transcendental equation for $\\ell$. Let $x=\\ell/2$. The equation becomes $4\\Phi(-x) = 2x \\phi(-x) = 2x \\phi(x)$.\n$$\n2\\Phi(-x) = x \\phi(x) = x \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)\n$$\nSolving this equation numerically gives $x \\approx 1.19064$.\nTherefore, the optimal scaling is $\\ell = 2x \\approx 2 \\times 1.19064 = 2.38128$.\nRounding to $4$ significant figures, $\\ell \\approx 2.381$.\n\n### Optimal Scaling for Global ESJD\nThe global ESJD is defined as $\\mathbb{E}[\\|y-x\\|^2 \\cdot \\mathbf{1}\\{\\text{accept}\\}]$.\n$$\n\\text{ESJD}_{\\text{global}} = \\mathbb{E}\\left[ \\left\\|\\frac{\\ell}{\\sqrt{d}} z\\right\\|^2 \\alpha(x,y) \\right] = \\frac{\\ell^2}{d} \\mathbb{E}[\\|z\\|^2 \\alpha(x,y)]\n$$\nWe can express $\\|z\\|^2$ as $\\sum_{i=1}^d z_i^2$. By linearity of expectation:\n$$\n\\mathbb{E}[\\|z\\|^2 \\alpha(x,y)] = \\sum_{i=1}^d \\mathbb{E}[z_i^2 \\alpha(x,y)]\n$$\nDue to the symmetry of the problem (i.i.d. target components and isotropic proposal), the term $\\mathbb{E}[z_i^2 \\alpha(x,y)]$ is identical for all $i=1, \\dots, d$. Thus:\n$$\n\\mathbb{E}[\\|z\\|^2 \\alpha(x,y)] = d \\cdot \\mathbb{E}[z_1^2 \\alpha(x,y)]\n$$\nSubstituting this back into the expression for global ESJD:\n$$\n\\text{ESJD}_{\\text{global}} = \\frac{\\ell^2}{d} \\left(d \\cdot \\mathbb{E}[z_1^2 \\alpha(x,y)]\\right) = \\ell^2 \\mathbb{E}[z_1^2 \\alpha(x,y)]\n$$\nIn the limit $d\\to\\infty$, this becomes:\n$$\n\\lim_{d\\to\\infty} \\text{ESJD}_{\\text{global}} = \\ell^2 \\mathbb{E}[z_1^2] \\mathbb{E}[\\lim \\alpha] = \\ell^2 \\bar{\\alpha}(\\ell)\n$$\nMaximizing the limiting global ESJD thus requires maximizing the same function $f(\\ell) = \\ell^2 \\bar{\\alpha}(\\ell)$ as before. The optimization problem is identical.\nTherefore, the value of $\\ell$ that maximizes the global ESJD is also $\\ell \\approx 2.381$.\n\n### Conclusion\nBoth optimization problems—minimizing the asymptotic variance for a single component and maximizing the global ESJD—lead to maximizing the same function $f(\\ell) = 2\\ell^2 \\Phi(-\\ell/2)$ in the high-dimensional limit. The optimal value for $\\ell$ is the same in both cases.\n\nValue for minimizing variance of $h(x)=x_1$: $\\ell \\approx 2.38128 \\to 2.381$.\nValue for maximizing global ESJD: $\\ell \\approx 2.38128 \\to 2.381$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.381  2.381\n\\end{pmatrix}\n}\n$$"
        }
    ]
}