{
    "hands_on_practices": [
        {
            "introduction": "The Reversible Jump MCMC acceptance ratio includes a Jacobian determinant to account for the change in volume induced by the trans-dimensional mapping. Mastering this calculation is the first step toward building a correct sampler. This foundational exercise  isolates the Jacobian calculation for a simple, linear split move, allowing you to focus on the essential mechanics of setting up the matrix of partial derivatives and computing its determinant.",
            "id": "3336823",
            "problem": "Consider a split move in Reversible Jump Markov Chain Monte Carlo (RJMCMC) for a trans-dimensional model. Let the original parameter vector be $(\\mu,\\sigma,u)$, where $\\mu \\in \\mathbb{R}$ is a location parameter, $\\sigma \\in \\mathbb{R}_{+}$ is a scale parameter, and $u \\in \\mathbb{R}$ is an auxiliary variable introduced to balance dimensions. The split move maps $(\\mu,\\sigma,u)$ to $(\\mu_{1},\\mu_{2},\\sigma')$ via the transformation\n$$\n\\mu_{1} = \\mu - \\delta u, \\qquad \\mu_{2} = \\mu + (1-\\delta)u, \\qquad \\sigma' = \\sigma,\n$$\nwhere $\\delta \\in (0,1)$ is a fixed tuning constant. In the Metropolisâ€“Hastings acceptance ratio for RJMCMC, the change-of-variables formula requires the Jacobian determinant of this transformation from $(\\mu,\\sigma,u)$ to $(\\mu_{1},\\mu_{2},\\sigma')$.\n\nUsing the foundational definition that the Jacobian determinant for a differentiable transformation $T: \\mathbb{R}^{k} \\to \\mathbb{R}^{k}$ is the determinant of its Jacobian matrix of partial derivatives, compute the Jacobian determinant of the mapping $(\\mu,\\sigma,u) \\mapsto (\\mu_{1},\\mu_{2},\\sigma')$ defined above. Express your final answer as a closed-form expression in terms of $\\delta$. No rounding is required.",
            "solution": "The user requests the computation of the Jacobian determinant for a specific transformation used in a Reversible Jump Markov Chain Monte Carlo (RJMCMC) split move.\n\nThe problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- **Context**: A split move in RJMCMC for a trans-dimensional model.\n- **Original parameter space**: The input vector is $(\\mu, \\sigma, u)$, with domains $\\mu \\in \\mathbb{R}$, $\\sigma \\in \\mathbb{R}_{+}$, and $u \\in \\mathbb{R}$.\n- **Target parameter space**: The output vector is $(\\mu_{1}, \\mu_{2}, \\sigma')$, where $\\sigma'=\\sigma$.\n- **Transformation equations**:\n  $$ \\mu_{1} = \\mu - \\delta u $$\n  $$ \\mu_{2} = \\mu + (1-\\delta)u $$\n  $$ \\sigma' = \\sigma $$\n- **Constant**: $\\delta$ is a fixed constant where $\\delta \\in (0,1)$.\n- **Objective**: Compute the Jacobian determinant of the mapping $(\\mu, \\sigma, u) \\mapsto (\\mu_{1}, \\mu_{2}, \\sigma')$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically sound, as it describes a standard dimension-matching transformation used in the well-established RJMCMC algorithm. It is well-posed, as the transformation is a differentiable function for which a Jacobian determinant can be uniquely computed. The problem is stated objectively using precise mathematical language. There are no contradictions, ambiguities, or missing information that would prevent a solution.\n\n**Verdict**: The problem is valid.\n\n**Solution Derivation**\nLet the transformation be denoted by $T$. The input variables are $(\\mu, \\sigma, u)$ and the output variables are $(\\mu_{1}, \\mu_{2}, \\sigma')$. The transformation is given by:\n$$ \\mu_{1}(\\mu, \\sigma, u) = \\mu - \\delta u $$\n$$ \\mu_{2}(\\mu, \\sigma, u) = \\mu + (1-\\delta)u $$\n$$ \\sigma'(\\mu, \\sigma, u) = \\sigma $$\n\nThe Jacobian determinant of this transformation is the determinant of the Jacobian matrix, $J$, whose elements are the partial derivatives of the output variables with respect to the input variables. The Jacobian matrix is defined as:\n$$\nJ = \\frac{\\partial(\\mu_{1}, \\mu_{2}, \\sigma')}{\\partial(\\mu, \\sigma, u)} =\n\\begin{pmatrix}\n\\frac{\\partial \\mu_{1}}{\\partial \\mu}  \\frac{\\partial \\mu_{1}}{\\partial \\sigma}  \\frac{\\partial \\mu_{1}}{\\partial u} \\\\\n\\frac{\\partial \\mu_{2}}{\\partial \\mu}  \\frac{\\partial \\mu_{2}}{\\partial \\sigma}  \\frac{\\partial \\mu_{2}}{\\partial u} \\\\\n\\frac{\\partial \\sigma'}{\\partial \\mu}  \\frac{\\partial \\sigma'}{\\partial \\sigma}  \\frac{\\partial \\sigma'}{\\partial u}\n\\end{pmatrix}\n$$\nWe now compute each partial derivative:\n\nFor the first row, corresponding to $\\mu_1$:\n$$ \\frac{\\partial \\mu_{1}}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu}(\\mu - \\delta u) = 1 $$\n$$ \\frac{\\partial \\mu_{1}}{\\partial \\sigma} = \\frac{\\partial}{\\partial \\sigma}(\\mu - \\delta u) = 0 $$\n$$ \\frac{\\partial \\mu_{1}}{\\partial u} = \\frac{\\partial}{\\partial u}(\\mu - \\delta u) = -\\delta $$\n\nFor the second row, corresponding to $\\mu_2$:\n$$ \\frac{\\partial \\mu_{2}}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu}(\\mu + (1-\\delta)u) = 1 $$\n$$ \\frac{\\partial \\mu_{2}}{\\partial \\sigma} = \\frac{\\partial}{\\partial \\sigma}(\\mu + (1-\\delta)u) = 0 $$\n$$ \\frac{\\partial \\mu_{2}}{\\partial u} = \\frac{\\partial}{\\partial u}(\\mu + (1-\\delta)u) = 1 - \\delta $$\n\nFor the third row, corresponding to $\\sigma'$:\n$$ \\frac{\\partial \\sigma'}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu}(\\sigma) = 0 $$\n$$ \\frac{\\partial \\sigma'}{\\partial \\sigma} = \\frac{\\partial}{\\partial \\sigma}(\\sigma) = 1 $$\n$$ \\frac{\\partial \\sigma'}{\\partial u} = \\frac{\\partial}{\\partial u}(\\sigma) = 0 $$\n\nAssembling these partial derivatives into the Jacobian matrix $J$:\n$$\nJ =\n\\begin{pmatrix}\n1  0  -\\delta \\\\\n1  0  1-\\delta \\\\\n0  1  0\n\\end{pmatrix}\n$$\nThe Jacobian determinant, denoted as $\\det(J)$, can be computed using cofactor expansion. Expanding along the second column, which contains two zeros, simplifies the calculation:\n$$\n\\det(J) = (0) \\cdot C_{12} + (0) \\cdot C_{22} + (1) \\cdot C_{32}\n$$\nwhere $C_{ij}$ is the cofactor of the element in the $i$-th row and $j$-th column. This simplifies to:\n$$\n\\det(J) = C_{32} = (-1)^{3+2} M_{32} = -M_{32}\n$$\nwhere $M_{32}$ is the minor corresponding to the entry $a_{32}$, obtained by taking the determinant of the submatrix formed by removing the $3$-rd row and $2$-nd column:\n$$\nM_{32} = \\det \\begin{pmatrix} 1  -\\delta \\\\ 1  1-\\delta \\end{pmatrix}\n$$\nCalculating this $2 \\times 2$ determinant:\n$$\nM_{32} = (1)(1-\\delta) - (-\\delta)(1) = 1 - \\delta + \\delta = 1\n$$\nSubstituting this result back into the expression for $\\det(J)$:\n$$\n\\det(J) = -M_{32} = -1\n$$\nThe Jacobian determinant is therefore $-1$. It is a constant value, independent of the tuning parameter $\\delta$. The requirement in the Metropolis-Hastings acceptance ratio is for the absolute value of the Jacobian determinant, which would be $|-1| = 1$. However, the question asks for the determinant itself.",
            "answer": "$$\n\\boxed{-1}\n$$"
        },
        {
            "introduction": "A primary application of RJMCMC is determining the appropriate number of components in a mixture model, a classic problem in model selection. This requires carefully designed \"split\" and \"merge\" moves that create or destroy components while maintaining detailed balance. This practice  guides you through the design of a full split transformation for a Gaussian mixture, involving weights, means, and scales, and illustrates how to incorporate an ordering constraint to handle the inherent label-switching problem.",
            "id": "3336794",
            "problem": "Consider a univariate $K$-component Gaussian mixture with parameters $\\Theta_{K}=\\{(w_{k},\\mu_{k},\\sigma_{k}): k=1,\\dots,K\\}$, where $w_{k}\\in(0,1)$, $\\sum_{k=1}^{K} w_{k}=1$, $\\mu_{k}\\in\\mathbb{R}$, and $\\sigma_{k}0$. You design a split move in Reversible Jump Markov Chain Monte Carlo (RJMCMC) to increase the model dimension from $K$ to $K+1$ by splitting a single component $(w,\\mu,\\sigma)$ (suppressing the index to reduce notational clutter) into two new components $(w_{1},\\mu_{1},\\sigma_{1})$ and $(w_{2},\\mu_{2},\\sigma_{2})$, with a symmetry-breaking ordering constraint on the means to avoid label-switching.\n\nTo ensure dimension matching and a bijection, you introduce auxiliary variables $(\\lambda,u,\\psi)$ with $\\lambda\\in(0,1)$, $u\\in(0,1)$, and $\\psi\\in\\mathbb{R}$, and a fixed tuning constant $\\alpha0$. The split transformation is defined by the following mapping:\n- Weights:\n$$\nw_{1}=w\\,\\lambda,\\qquad w_{2}=w\\,(1-\\lambda).\n$$\n- Ordered means via a positive separation:\n$$\n\\mu_{1}=\\mu-\\delta,\\qquad \\mu_{2}=\\mu+\\delta,\\qquad \\delta=\\sigma\\,\\alpha\\,u,\n$$\nso that $\\mu_{1}\\mu_{2}$ is enforced by construction.\n- Scales coupled multiplicatively:\n$$\n\\sigma_{1}=\\sigma\\,\\exp\\!\\left(\\frac{\\psi}{2}\\right),\\qquad \\sigma_{2}=\\sigma\\,\\exp\\!\\left(-\\frac{\\psi}{2}\\right).\n$$\n\nStarting from the fundamental change-of-variables principle for transformations in Monte Carlo methods and the dimension-matching requirement in Reversible Jump Markov Chain Monte Carlo (RJMCMC), derive the absolute value of the determinant of the Jacobian for the transformation\n$$\n(w,\\mu,\\sigma,\\lambda,u,\\psi)\\ \\mapsto\\ (w_{1},w_{2},\\mu_{1},\\mu_{2},\\sigma_{1},\\sigma_{2}),\n$$\nand simplify it to a single closed-form expression in terms of $w$, $\\sigma$, and $\\alpha$. Your final answer must be a single analytic expression. No numerical approximation is required.",
            "solution": "The problem requires the derivation of the absolute value of the determinant of the Jacobian for a specified transformation used in a Reversible Jump Markov Chain Monte Carlo (RJMCMC) split move.\n\nThe transformation maps the current component parameters and a set of auxiliary variables, $(w,\\mu,\\sigma,\\lambda,u,\\psi)$, to a new set of parameters for two components, $(w_{1},w_{2},\\mu_{1},\\mu_{2},\\sigma_{1},\\sigma_{2})$. The domain of the input variables is a subset of $\\mathbb{R}^6$, and the codomain is also a subset of $\\mathbb{R}^6$. The transformation is defined by the following set of equations:\n$w_{1}=w\\lambda$\n$w_{2}=w(1-\\lambda)$\n$\\mu_{1}=\\mu-\\sigma\\alpha u$\n$\\mu_{2}=\\mu+\\sigma\\alpha u$\n$\\sigma_{1}=\\sigma\\exp(\\frac{\\psi}{2})$\n$\\sigma_{2}=\\sigma\\exp(-\\frac{\\psi}{2})$\n\nThe Jacobian of this transformation is a $6 \\times 6$ matrix, $J$, where the entry in the $i$-th row and $j$-th column is the partial derivative of the $i$-th output variable with respect to the $j$-th input variable. We are asked to compute $|\\det(J)|$ for the mapping from the vector of input variables $\\mathbf{x} = (w,\\mu,\\sigma,\\lambda,u,\\psi)$ to the vector of output variables $\\mathbf{y} = (w_{1},w_{2},\\mu_{1},\\mu_{2},\\sigma_{1},\\sigma_{2})$.\n\nThe Jacobian matrix is given by:\n$$\nJ = \\frac{\\partial(w_{1},w_{2},\\mu_{1},\\mu_{2},\\sigma_{1},\\sigma_{2})}{\\partial(w,\\mu,\\sigma,\\lambda,u,\\psi)}\n$$\n\nTo simplify the calculation of the determinant, we can reorder the input variables to group them according to their roles in the transformation equations. This reveals the underlying structure of the mapping. A suitable ordering is $(w, \\lambda, \\mu, u, \\sigma, \\psi)$. Let us denote the Jacobian matrix corresponding to this new ordering of input variables as $J'$. A reordering of columns in a matrix only affects the sign of its determinant. Since we are interested in the absolute value of the determinant, $|\\det(J)|=|\\det(J')|$.\n\nThe reordered Jacobian matrix $J'$ is:\n$$\nJ' = \\frac{\\partial(w_{1},w_{2},\\mu_{1},\\mu_{2},\\sigma_{1},\\sigma_{2})}{\\partial(w,\\lambda,\\mu,u,\\sigma,\\psi)}\n$$\nLet us compute the partial derivatives for each entry of $J'$.\n\nFor the weights $(w_1, w_2)$:\n$\\frac{\\partial w_{1}}{\\partial w} = \\lambda$, $\\frac{\\partial w_{1}}{\\partial \\lambda} = w$. Other derivatives are zero.\n$\\frac{\\partial w_{2}}{\\partial w} = 1-\\lambda$, $\\frac{\\partial w_{2}}{\\partial \\lambda} = -w$. Other derivatives are zero.\n\nFor the means $(\\mu_1, \\mu_2)$:\n$\\frac{\\partial \\mu_{1}}{\\partial \\mu} = 1$, $\\frac{\\partial \\mu_{1}}{\\partial u} = -\\sigma\\alpha$, $\\frac{\\partial \\mu_{1}}{\\partial \\sigma} = -\\alpha u$. Other derivatives are zero.\n$\\frac{\\partial \\mu_{2}}{\\partial \\mu} = 1$, $\\frac{\\partial \\mu_{2}}{\\partial u} = \\sigma\\alpha$, $\\frac{\\partial \\mu_{2}}{\\partial \\sigma} = \\alpha u$. Other derivatives are zero.\n\nFor the scales $(\\sigma_1, \\sigma_2)$:\n$\\frac{\\partial \\sigma_{1}}{\\partial \\sigma} = \\exp(\\frac{\\psi}{2})$, $\\frac{\\partial \\sigma_{1}}{\\partial \\psi} = \\frac{\\sigma}{2}\\exp(\\frac{\\psi}{2})$. Other derivatives are zero.\n$\\frac{\\partial \\sigma_{2}}{\\partial \\sigma} = \\exp(-\\frac{\\psi}{2})$, $\\frac{\\partial \\sigma_{2}}{\\partial \\psi} = -\\frac{\\sigma}{2}\\exp(-\\frac{\\psi}{2})$. Other derivatives are zero.\n\nAssembling these derivatives into the matrix $J'$:\n$$\nJ' =\n\\begin{pmatrix}\n  \\frac{\\partial w_1}{\\partial w}  \\frac{\\partial w_1}{\\partial \\lambda}  \\frac{\\partial w_1}{\\partial \\mu}  \\frac{\\partial w_1}{\\partial u}  \\frac{\\partial w_1}{\\partial \\sigma}  \\frac{\\partial w_1}{\\partial \\psi} \\\\\n  \\frac{\\partial w_2}{\\partial w}  \\frac{\\partial w_2}{\\partial \\lambda}  \\frac{\\partial w_2}{\\partial \\mu}  \\frac{\\partial w_2}{\\partial u}  \\frac{\\partial w_2}{\\partial \\sigma}  \\frac{\\partial w_2}{\\partial \\psi} \\\\\n  \\frac{\\partial \\mu_1}{\\partial w}  \\frac{\\partial \\mu_1}{\\partial \\lambda}  \\frac{\\partial \\mu_1}{\\partial \\mu}  \\frac{\\partial \\mu_1}{\\partial u}  \\frac{\\partial \\mu_1}{\\partial \\sigma}  \\frac{\\partial \\mu_1}{\\partial \\psi} \\\\\n  \\frac{\\partial \\mu_2}{\\partial w}  \\frac{\\partial \\mu_2}{\\partial \\lambda}  \\frac{\\partial \\mu_2}{\\partial \\mu}  \\frac{\\partial \\mu_2}{\\partial u}  \\frac{\\partial \\mu_2}{\\partial \\sigma}  \\frac{\\partial \\mu_2}{\\partial \\psi} \\\\\n  \\frac{\\partial \\sigma_1}{\\partial w}  \\frac{\\partial \\sigma_1}{\\partial \\lambda}  \\frac{\\partial \\sigma_1}{\\partial \\mu}  \\frac{\\partial \\sigma_1}{\\partial u}  \\frac{\\partial \\sigma_1}{\\partial \\sigma}  \\frac{\\partial \\sigma_1}{\\partial \\psi} \\\\\n  \\frac{\\partial \\sigma_2}{\\partial w}  \\frac{\\partial \\sigma_2}{\\partial \\lambda}  \\frac{\\partial \\sigma_2}{\\partial \\mu}  \\frac{\\partial \\sigma_2}{\\partial u}  \\frac{\\partial \\sigma_2}{\\partial \\sigma}  \\frac{\\partial \\sigma_2}{\\partial \\psi}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\lambda  w  0  0  0  0 \\\\\n  1-\\lambda  -w  0  0  0  0 \\\\\n  0  0  1  -\\sigma\\alpha  -\\alpha u  0 \\\\\n  0  0  1  \\sigma\\alpha  \\alpha u  0 \\\\\n  0  0  0  0  \\exp(\\frac{\\psi}{2})  \\frac{\\sigma}{2}\\exp(\\frac{\\psi}{2}) \\\\\n  0  0  0  0  \\exp(-\\frac{\\psi}{2})  -\\frac{\\sigma}{2}\\exp(-\\frac{\\psi}{2})\n\\end{pmatrix}\n$$\nThis matrix is block upper-triangular:\n$$\nJ' = \\begin{pmatrix} A  \\mathbf{0}  \\mathbf{0} \\\\ \\mathbf{0}  B  C \\\\ \\mathbf{0}  \\mathbf{0}  D \\end{pmatrix}\n$$\nwhere $A$, $B$, $C$, and $D$ are $2 \\times 2$ matrices. The determinant of a block upper-triangular matrix is the product of the determinants of its diagonal blocks.\n$$\n\\det(J') = \\det(A) \\det(B) \\det(D)\n$$\nThe diagonal blocks are:\n$$\nA = \\frac{\\partial(w_1,w_2)}{\\partial(w,\\lambda)} = \\begin{pmatrix} \\lambda  w \\\\ 1-\\lambda  -w \\end{pmatrix}\n$$\n$$\nB = \\frac{\\partial(\\mu_1,\\mu_2)}{\\partial(\\mu,u)} = \\begin{pmatrix} 1  -\\sigma\\alpha \\\\ 1  \\sigma\\alpha \\end{pmatrix}\n$$\n$$\nD = \\frac{\\partial(\\sigma_1,\\sigma_2)}{\\partial(\\sigma,\\psi)} = \\begin{pmatrix} \\exp(\\frac{\\psi}{2})  \\frac{\\sigma}{2}\\exp(\\frac{\\psi}{2}) \\\\ \\exp(-\\frac{\\psi}{2})  -\\frac{\\sigma}{2}\\exp(-\\frac{\\psi}{2}) \\end{pmatrix}\n$$\nWe compute the determinant of each block:\n$$\n\\det(A) = (\\lambda)(-w) - (w)(1-\\lambda) = -w\\lambda - w + w\\lambda = -w\n$$\n$$\n\\det(B) = (1)(\\sigma\\alpha) - (-\\sigma\\alpha)(1) = \\sigma\\alpha + \\sigma\\alpha = 2\\sigma\\alpha\n$$\n$$\n\\det(D) = \\left(\\exp\\left(\\frac{\\psi}{2}\\right)\\right)\\left(-\\frac{\\sigma}{2}\\exp\\left(-\\frac{\\psi}{2}\\right)\\right) - \\left(\\frac{\\sigma}{2}\\exp\\left(\\frac{\\psi}{2}\\right)\\right)\\left(\\exp\\left(-\\frac{\\psi}{2}\\right)\\right) = -\\frac{\\sigma}{2} - \\frac{\\sigma}{2} = -\\sigma\n$$\nThe determinant of $J'$ is the product of these determinants:\n$$\n\\det(J') = \\det(A) \\cdot \\det(B) \\cdot \\det(D) = (-w)(2\\sigma\\alpha)(-\\sigma) = 2w\\sigma^2\\alpha\n$$\nThe problem asks for the absolute value of the determinant. According to the problem statement, the parameters satisfy $w \\in (0,1)$, $\\sigma  0$, and the tuning constant $\\alpha  0$. Therefore, the expression $2w\\sigma^2\\alpha$ is strictly positive.\n$$\n|\\det(J)| = |\\det(J')| = |2w\\sigma^2\\alpha| = 2w\\sigma^2\\alpha\n$$\nThe expression simplifies to a function of $w$, $\\sigma$, and $\\alpha$ only, as required. The auxiliary random variables $\\lambda$, $u$, and $\\psi$ do not appear in the final expression for the Jacobian determinant.",
            "answer": "$$\n\\boxed{2w\\sigma^2\\alpha}\n$$"
        },
        {
            "introduction": "The efficiency of an RJMCMC algorithm is not guaranteed; it depends critically on the interaction between the proposal distributions and the target likelihood. This advanced practice  explores the challenges that arise when using heavy-tailed likelihoods, such as the Student-$t$ distribution, which can cause poor mixing if proposals are not designed carefully. You will analyze the tail behavior of the acceptance probability and investigate how a sophisticated reparameterization, based on a latent scale-mixture representation, can be used to design more robust and stable proposals.",
            "id": "3336830",
            "problem": "Consider a trans-dimensional linear model used inside a Reversible Jump Markov Chain Monte Carlo (RJMCMC) scheme. At model index $k$, the response residuals are $r_{i} = y_{i} - x_{i}^{\\top}\\beta_{k}$ for $i = 1, \\dots, n$, and the observation noise is modeled with a Student-$t_{\\nu}$ distribution with location $0$ and scale $\\sigma^{2}$, that is the likelihood for residual $r$ is proportional to $\\left(1 + \\frac{r^{2}}{\\nu\\sigma^{2}}\\right)^{-\\frac{\\nu+1}{2}}$. Consider a birth move from $k$ to $k+1$ that adds one scalar coefficient $\\phi$ multiplying a known regressors vector $w = (w_{1}, \\dots, w_{n})^{\\top}$, so the proposed new residuals become $r_{i}' = r_{i} - w_{i}\\phi$. The birth move uses a dimension-matching mapping $\\phi = \\tau u$, where $u$ is an auxiliary variable drawn from a proposal distribution that is a standard normal with density $q(u) \\propto \\exp\\!\\left(-\\frac{u^{2}}{2}\\right)$, and the prior on the new coefficient is $\\phi \\sim \\mathcal{N}\\!\\left(0, \\tau^{2}\\right)$, so that the Jacobian determinant is $J = |\\partial \\phi/\\partial u| = \\tau$. The proposal for the reverse (death) move is chosen to satisfy detailed balance and symmetry of move probabilities, so that the acceptance probability reduces to a likelihood ratio.\n\nStarting from these definitions and no further shortcuts, derive the RJMCMC birth-move acceptance probability $\\alpha_{k \\to k+1}$ in terms of $(r_{i}, w_{i}, \\sigma, \\nu, \\tau, u)$, and obtain a non-asymptotic upper bound valid for all $u$ that depends only on these quantities. Then, identify the tail behavior of this bound as $\\nu \\to 1^{+}$ (the Cauchy limit) and $|u| \\to \\infty$, and simplify the bound to its limiting sharp form in closed algebraic terms.\n\nSeparately, use the normal-inverse-gamma scale-mixture representation of the Student-$t_{\\nu}$ distribution, namely $r_{i}\\mid \\lambda_{i} \\sim \\mathcal{N}\\!\\left(0, \\sigma^{2}\\lambda_{i}\\right)$ and $\\lambda_{i} \\sim \\mathrm{Inverse\\text{-}Gamma}\\!\\left(\\frac{\\nu}{2}, \\frac{\\nu}{2}\\right)$, to propose a reparameterization for an auxiliary variable $u$ based on the latent scale $\\lambda$ that yields an auxiliary $u$ with variance exactly $1$ for every $\\nu  0$. Your reparameterization must be expressed explicitly in terms of $\\nu$, the digamma function $\\psi(\\cdot)$, the trigamma function $\\psi_{1}(\\cdot)$, and $\\lambda$, and must be derived from first principles starting from the stated scale-mixture base.\n\nYour final answer must be a single closed-form analytical expression consisting of a two-entry row matrix. The first entry must be the limiting tail upper bound for $\\alpha_{k \\to k+1}$ as $\\nu \\to 1^{+}$, expressed in terms of $\\{r_{i}\\}$, $\\{w_{i}\\}$, $\\sigma$, $\\tau$, and $u$. The second entry must be your proposed reparameterization $u = g_{\\nu}(\\lambda)$ standardized to have variance $1$ for all $\\nu  0$. No numerical approximation is required and no units are involved. If any indices $i$ satisfy $w_{i} = 0$, ensure your expression correctly excludes their contribution to the product in the bound.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the theory of Reversible Jump Markov Chain Monte Carlo (RJMCMC), is well-posed with clear objectives, and is formulated using precise, objective language. The premises and conditions are internally consistent and relevant to advanced statistical computation.\n\nThe problem consists of two independent parts. I shall address them in sequence.\n\n### Part 1: RJMCMC Acceptance Probability and Limiting Bound\n\nThe first part of the problem requires the derivation of the birth-move acceptance probability, a corresponding non-asymptotic upper bound, and the limiting form of this bound.\n\nThe move from model $k$ to model $k+1$ is a birth move introducing a new coefficient $\\phi$. The problem states that the prior on the new coefficient $\\phi$ is $\\mathcal{N}(0, \\tau^2)$ and the proposal for the auxiliary variable $u$ is a standard normal distribution, $q(u)$, with the mapping $\\phi = \\tau u$. The Jacobian of this transformation is $J = |\\partial\\phi/\\partial u| = \\tau$. The ratio of the prior to the proposal density multiplied by the Jacobian is:\n$$\n\\frac{p(\\phi)}{q(u)} J = \\frac{\\frac{1}{\\sqrt{2\\pi}\\tau}\\exp(-\\frac{\\phi^2}{2\\tau^2})}{\\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{u^2}{2})} \\tau = \\frac{\\frac{1}{\\sqrt{2\\pi}\\tau}\\exp(-\\frac{(\\tau u)^2}{2\\tau^2})}{\\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{u^2}{2})} \\tau = \\frac{\\frac{1}{\\tau}\\exp(-\\frac{u^2}{2})}{\\exp(-\\frac{u^2}{2})} \\tau = 1\n$$\nWith the additional assumption of symmetric move probabilities $(p(k \\to k+1) = p(k+1 \\to k))$, the acceptance probability for the birth move, $\\alpha_{k \\to k+1}$, simplifies to the minimum of $1$ and the likelihood ratio.\n\nLet $L(y|k)$ be the likelihood under model $k$. The likelihood for a single observation's residual $r$ is given as proportional to $\\left(1 + \\frac{r^{2}}{\\nu\\sigma^{2}}\\right)^{-\\frac{\\nu+1}{2}}$. Assuming independence of observations, the total likelihood is the product over all data points.\nUnder model $k$, the residuals are $r_i$. The likelihood is:\n$$\nL(y|k) \\propto \\prod_{i=1}^{n} \\left(1 + \\frac{r_i^2}{\\nu\\sigma^2}\\right)^{-\\frac{\\nu+1}{2}}\n$$\nUnder model $k+1$, the residuals are $r_i' = r_i - w_i\\phi = r_i - w_i\\tau u$. The likelihood is:\n$$\nL(y|k+1) \\propto \\prod_{i=1}^{n} \\left(1 + \\frac{(r_i - w_i\\tau u)^2}{\\nu\\sigma^2}\\right)^{-\\frac{\\nu+1}{2}}\n$$\nThe likelihood ratio is therefore:\n$$\n\\frac{L(y|k+1)}{L(y|k)} = \\frac{\\prod_{i=1}^{n} \\left(1 + \\frac{(r_i - w_i\\tau u)^2}{\\nu\\sigma^2}\\right)^{-\\frac{\\nu+1}{2}}}{\\prod_{i=1}^{n} \\left(1 + \\frac{r_i^2}{\\nu\\sigma^2}\\right)^{-\\frac{\\nu+1}{2}}} = \\prod_{i=1}^{n} \\left(\\frac{1 + \\frac{r_i^2}{\\nu\\sigma^2}}{1 + \\frac{(r_i - w_i\\tau u)^2}{\\nu\\sigma^2}}\\right)^{\\frac{\\nu+1}{2}}\n$$\nThe acceptance probability is $\\alpha_{k \\to k+1} = \\min\\left(1, \\frac{L(y|k+1)}{L(y|k)}\\right)$. If for some index $i$, $w_i=0$, the corresponding term in the product is $1$, so we only need to consider indices $i$ for which $w_i \\neq 0$. Let the set of such indices be $I_w = \\{i \\mid w_i \\neq 0\\}$ and its size be $n_w = |I_w|$.\n\nThe problem asks for a non-asymptotic upper bound on this probability, valid for all $u$, and then for its limiting tail behavior. A bound that captures the tail behavior for large $|u|$ is required. Such a bound can be found by analyzing the asymptotic form of the likelihood ratio as $|u| \\to \\infty$.\nFor large $|u|$, the term $(r_i - w_i\\tau u)^2$ is dominated by $(w_i\\tau u)^2$.\n$$\n(r_i - w_i\\tau u)^2 = (w_i\\tau u)^2 - 2 r_i w_i \\tau u + r_i^2 \\sim (w_i\\tau u)^2 = w_i^2 \\tau^2 u^2 \\quad \\text{as } |u| \\to \\infty\n$$\nSubstituting this into the likelihood ratio expression gives the asymptotic behavior:\n$$\n\\frac{L(y|k+1)}{L(y|k)} \\sim \\prod_{i \\in I_w} \\left(\\frac{1 + \\frac{r_i^2}{\\nu\\sigma^2}}{1 + \\frac{w_i^2\\tau^2 u^2}{\\nu\\sigma^2}}\\right)^{\\frac{\\nu+1}{2}} \\sim \\prod_{i \\in I_w} \\left(\\frac{\\nu\\sigma^2 + r_i^2}{w_i^2\\tau^2 u^2}\\right)^{\\frac{\\nu+1}{2}} \\quad \\text{as } |u| \\to \\infty\n$$\nThis expression provides the tail behavior of the acceptance ratio. We can interpret this asymptotic form as the requested \"bound\" in the sense that it is a sharp asymptotic bound. Let's express this form, which we denote $B(u, \\nu)$, more explicitly:\n$$\nB(u, \\nu) = \\left( \\prod_{i \\in I_w} \\left(\\frac{\\nu\\sigma^2+r_i^2}{w_i^2\\tau^2}\\right)^{\\frac{\\nu+1}{2}} \\right) |u|^{-n_w(\\nu+1)}\n$$\nThe problem then asks for the limiting form of this bound as $\\nu \\to 1^+$ (the Cauchy limit) and $|u| \\to \\infty$. The asymptotic form already accounts for $|u| \\to \\infty$. We now take the limit of $B(u, \\nu)$ as $\\nu \\to 1^+$.\n\nThe exponent becomes:\n$$\n\\lim_{\\nu \\to 1^+} -n_w(\\nu+1) = -2n_w\n$$\nThe coefficient term becomes:\n$$\n\\lim_{\\nu \\to 1^+} \\prod_{i \\in I_w} \\left(\\frac{\\nu\\sigma^2+r_i^2}{w_i^2\\tau^2}\\right)^{\\frac{\\nu+1}{2}} = \\prod_{i \\in I_w} \\left(\\frac{\\sigma^2+r_i^2}{w_i^2\\tau^2}\\right)^{\\frac{1+1}{2}} = \\prod_{i \\in I_w} \\frac{\\sigma^2+r_i^2}{w_i^2\\tau^2}\n$$\nCombining these, the limiting sharp form of the bound is:\n$$\n\\left(\\prod_{i \\in I_w} \\frac{\\sigma^2+r_i^2}{w_i^2\\tau^2}\\right) |u|^{-2n_w} = \\frac{\\prod_{i \\in I_w}(\\sigma^2+r_i^2)}{(\\tau^2)^{n_w} (\\prod_{i \\in I_w} w_i^2) u^{2n_w}} = (\\tau u)^{-2n_w} \\prod_{i \\in I_w} \\frac{\\sigma^2+r_i^2}{w_i^2}\n$$\n\n### Part 2: Auxiliary Variable Reparameterization\n\nThe second part of the problem asks for a reparameterization $u = g_\\nu(\\lambda)$ for an auxiliary variable, based on a latent scale variable $\\lambda$ from the scale-mixture representation of the Student-$t$ distribution, such that the resulting variable $u$ has a variance of $1$ for all $\\nu  0$. The distribution of $\\lambda$ is given as $\\lambda \\sim \\mathrm{Inverse\\text{-}Gamma}(\\frac{\\nu}{2}, \\frac{\\nu}{2})$.\n\nA standard method to construct a variable with zero mean and unit variance is to standardize a suitable transformation of $\\lambda$. Let's choose the function $f(\\lambda) = \\ln\\lambda$. The standardized variable $u$ is then given by:\n$$\nu = \\frac{\\ln\\lambda - \\mathbb{E}[\\ln\\lambda]}{\\sqrt{\\mathbb{V}[\\ln\\lambda]}}\n$$\nWe need to find the mean and variance of $\\ln\\lambda$. Let $\\lambda \\sim \\mathrm{IG}(\\alpha, \\beta)$. Then the variable $Y = 1/\\lambda$ follows a Gamma distribution, $Y \\sim \\mathrm{Gamma}(\\alpha, \\beta)$.\nThe moments of $\\ln Y$ for a Gamma-distributed variable are well-known. The mean is given by:\n$$\n\\mathbb{E}[\\ln Y] = \\psi(\\alpha) - \\ln\\beta\n$$\nwhere $\\psi(\\cdot)$ is the digamma function. The variance is:\n$$\n\\mathbb{V}[\\ln Y] = \\psi_1(\\alpha)\n$$\nwhere $\\psi_1(\\cdot)$ is the trigamma function.\n\nSince $\\ln\\lambda = -\\ln Y$, the mean and variance of $\\ln\\lambda$ are:\n$$\n\\mathbb{E}[\\ln\\lambda] = \\mathbb{E}[-\\ln Y] = -(\\psi(\\alpha) - \\ln\\beta) = \\ln\\beta - \\psi(\\alpha)\n$$\n$$\n\\mathbb{V}[\\ln\\lambda] = \\mathbb{V}[-\\ln Y] = (-1)^2 \\mathbb{V}[\\ln Y] = \\psi_1(\\alpha)\n$$\nFor the given problem, the parameters of the Inverse-Gamma distribution are $\\alpha = \\frac{\\nu}{2}$ and $\\beta = \\frac{\\nu}{2}$. Substituting these values:\n$$\n\\mathbb{E}[\\ln\\lambda] = \\ln\\left(\\frac{\\nu}{2}\\right) - \\psi\\left(\\frac{\\nu}{2}\\right)\n$$\n$$\n\\mathbb{V}[\\ln\\lambda] = \\psi_1\\left(\\frac{\\nu}{2}\\right)\n$$\nTherefore, the reparameterization $u = g_\\nu(\\lambda)$ that produces a variable with mean $0$ and variance $1$ is:\n$$\nu = \\frac{\\ln\\lambda - \\left(\\ln\\left(\\frac{\\nu}{2}\\right) - \\psi\\left(\\frac{\\nu}{2}\\right)\\right)}{\\sqrt{\\psi_1\\left(\\frac{\\nu}{2}\\right)}}\n$$\nThis expression depends on $\\nu$, $\\lambda$, and the digamma and trigamma functions as required.\n\n### Summary of Final Answers\n\nThe two parts of the problem are distinct. The final answer requires a two-entry matrix containing the results.\n1. The limiting sharp tail upper bound for $\\alpha_{k \\to k+1}$ as $\\nu \\to 1^+$.\n2. The reparameterization $u = g_\\nu(\\lambda)$.\n\nLetting $n_w = |\\{i \\mid w_i \\neq 0\\}|$ be the number of non-zero entries in the vector $w$, the first entry is:\n$$\n(\\tau u)^{-2n_w} \\prod_{i: w_i \\neq 0} \\frac{\\sigma^2+r_i^2}{w_i^2}\n$$\nThe second entry is:\n$$\n\\frac{\\ln\\lambda - \\ln(\\frac{\\nu}{2}) + \\psi(\\frac{\\nu}{2})}{\\sqrt{\\psi_1(\\frac{\\nu}{2})}}\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n(\\tau u)^{-2|\\{i \\mid w_i \\neq 0\\}|} \\prod_{i: w_i \\neq 0} \\frac{\\sigma^2+r_i^2}{w_i^2}  \\frac{\\ln\\lambda - \\ln(\\frac{\\nu}{2}) + \\psi(\\frac{\\nu}{2})}{\\sqrt{\\psi_1(\\frac{\\nu}{2})}}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}