{
    "hands_on_practices": [
        {
            "introduction": "Before we can build and test numerical samplers, we must first understand the theoretical properties of the target distribution they aim to reproduce. This first practice grounds our understanding in the continuous-time underdamped Langevin equation and its stationary state for a simple but powerful model system—the multivariate harmonic oscillator. By deriving the exact covariances for both position and velocity, you will see how the potential energy landscape and temperature directly determine the system's equilibrium statistics, providing a crucial benchmark for any simulation .",
            "id": "3359276",
            "problem": "Consider the $d$-dimensional underdamped Langevin Stochastic Differential Equation (SDE) for position $x \\in \\mathbb{R}^{d}$ and velocity $v \\in \\mathbb{R}^{d}$,\n$$\n\\mathrm{d}x_{t} = v_{t}\\,\\mathrm{d}t, \n\\qquad\n\\mathrm{d}v_{t} = -\\gamma\\, v_{t}\\,\\mathrm{d}t - \\nabla U(x_{t})\\,\\mathrm{d}t + \\sqrt{\\frac{2\\gamma}{\\beta}}\\,\\mathrm{d}W_{t},\n$$\nwhere $W_{t}$ is a $d$-dimensional standard Wiener process, $\\gamma0$ is the friction coefficient, and $\\beta0$ is the inverse temperature. Assume the potential energy is quadratic,\n$$\nU(x) = \\frac{1}{2}\\, x^{\\top} A x,\n$$\nwhere $A \\in \\mathbb{R}^{d \\times d}$ is symmetric positive definite. Starting from the stationary Fokker–Planck equation and the fluctuation–dissipation balance encoded in the SDE above, determine the stationary density $\\pi(x,v)$ and use it to calculate the stationary covariances $\\mathrm{Cov}(x)$ and $\\mathrm{Cov}(v)$ implied by $\\pi$. Express your final answer as a single row matrix whose two entries are the closed-form expressions for $\\mathrm{Cov}(x)$ and $\\mathrm{Cov}(v)$, respectively. No numerical rounding is required.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in statistical mechanics and stochastic calculus, with all necessary information provided for a unique solution.\n\nThe underdamped Langevin dynamics for the state vector $(x_t, v_t)$ are given by the stochastic differential equations (SDEs):\n$$\n\\mathrm{d}x_{t} = v_{t}\\,\\mathrm{d}t\n$$\n$$\n\\mathrm{d}v_{t} = -\\gamma\\, v_{t}\\,\\mathrm{d}t - \\nabla U(x_{t})\\,\\mathrm{d}t + \\sqrt{\\frac{2\\gamma}{\\beta}}\\,\\mathrm{d}W_{t}\n$$\nThis system describes the motion of a particle with unit mass ($m=1$) in a potential $U(x)$, subject to friction and random thermal fluctuations. The form of the noise term is consistent with the fluctuation-dissipation theorem, which guarantees that the system will thermalize to a stationary state described by the Gibbs-Boltzmann distribution at inverse temperature $\\beta$.\n\nThe total energy of the system is given by the Hamiltonian $H(x, v)$, which is the sum of the potential energy $U(x)$ and the kinetic energy $K(v)$.\nThe potential energy is given as a quadratic form:\n$$\nU(x) = \\frac{1}{2}\\, x^{\\top} A x\n$$\nwhere $A$ is a symmetric positive definite matrix.\nThe kinetic energy for a particle with unit mass is:\n$$\nK(v) = \\frac{1}{2}\\, v^{\\top} v\n$$\nThus, the Hamiltonian is:\n$$\nH(x,v) = U(x) + K(v) = \\frac{1}{2}\\, x^{\\top} A x + \\frac{1}{2}\\, v^{\\top} v\n$$\nThe stationary distribution, or invariant measure, $\\pi(x, v)$ of the Langevin SDE is the Gibbs-Boltzmann distribution associated with this Hamiltonian:\n$$\n\\pi(x, v) = Z^{-1} \\exp(-\\beta H(x, v))\n$$\nwhere $Z$ is the normalization constant, also known as the partition function. Substituting the expression for $H(x,v)$:\n$$\n\\pi(x, v) = Z^{-1} \\exp\\left(-\\beta \\left(\\frac{1}{2}\\, x^{\\top} A x + \\frac{1}{2}\\, v^{\\top} v\\right)\\right)\n$$\nWe can separate the terms involving $x$ and $v$:\n$$\n\\pi(x, v) = Z^{-1} \\exp\\left(-\\frac{\\beta}{2}\\, x^{\\top} A x\\right) \\exp\\left(-\\frac{\\beta}{2}\\, v^{\\top} v\\right)\n$$\nThis demonstrates that the stationary distribution is a product of two functions, one depending only on $x$ and the other only on $v$. This implies that at stationarity, the position $x$ and velocity $v$ are statistically independent random variables. We can write $\\pi(x, v) = \\pi(x) \\pi(v)$, where:\n$$\n\\pi(x) \\propto \\exp\\left(-\\frac{1}{2}\\, x^{\\top} (\\beta A) x\\right)\n$$\n$$\n\\pi(v) \\propto \\exp\\left(-\\frac{1}{2}\\, v^{\\top} (\\beta I) v\\right)\n$$\nwhere $I$ is the $d \\times d$ identity matrix.\n\nBoth $\\pi(x)$ and $\\pi(v)$ are probability densities of multivariate normal distributions. A general $d$-dimensional multivariate normal distribution with mean $\\mu$ and covariance matrix $\\Sigma$ has a probability density function proportional to $\\exp\\left(-\\frac{1}{2} (z - \\mu)^{\\top} \\Sigma^{-1} (z - \\mu)\\right)$.\nFor both distributions, the quadratic form is centered, which means the mean vectors are zero:\n$$\n\\mathbb{E}[x] = \\int_{\\mathbb{R}^d} x \\, \\pi(x) \\, \\mathrm{d}x = 0\n$$\n$$\n\\mathbb{E}[v] = \\int_{\\mathbb{R}^d} v \\, \\pi(v) \\, \\mathrm{d}v = 0\n$$\nThe covariance matrices are defined as $\\mathrm{Cov}(x) = \\mathbb{E}[(x - \\mathbb{E}[x])(x - \\mathbb{E}[x])^{\\top}] = \\mathbb{E}[xx^{\\top}]$ and $\\mathrm{Cov}(v) = \\mathbb{E}[vv^{\\top}]$.\n\nTo find $\\mathrm{Cov}(x)$, we compare the exponent in $\\pi(x)$ with the standard form. We identify the inverse covariance matrix of $x$, denoted $\\Sigma_{x}^{-1}$, by the relation:\n$$\n-\\frac{1}{2}\\, x^{\\top} \\Sigma_{x}^{-1} x = -\\frac{1}{2}\\, x^{\\top} (\\beta A) x\n$$\nThis gives $\\Sigma_{x}^{-1} = \\beta A$. Since $A$ is symmetric positive definite and $\\beta  0$, $\\beta A$ is invertible. The covariance matrix for $x$ is therefore:\n$$\n\\mathrm{Cov}(x) = \\Sigma_x = (\\beta A)^{-1} = \\frac{1}{\\beta} A^{-1}\n$$\n\nSimilarly, for $\\mathrm{Cov}(v)$, we compare the exponent in $\\pi(v)$ with the standard form. We identify the inverse covariance matrix of $v$, denoted $\\Sigma_{v}^{-1}$, by the relation:\n$$\n-\\frac{1}{2}\\, v^{\\top} \\Sigma_{v}^{-1} v = -\\frac{1}{2}\\, v^{\\top} (\\beta I) v\n$$\nThis gives $\\Sigma_{v}^{-1} = \\beta I$. The covariance matrix for $v$ is therefore:\n$$\n\\mathrm{Cov}(v) = \\Sigma_v = (\\beta I)^{-1} = \\frac{1}{\\beta} I\n$$\nThe friction coefficient $\\gamma$ influences the dynamics (i.e., the rate of convergence to the stationary distribution), but it does not affect the stationary covariances themselves, as they are solely determined by the potential energy $U(x)$ and the temperature $\\beta^{-1}$.\n\nThe stationary covariances are $\\mathrm{Cov}(x) = \\frac{1}{\\beta} A^{-1}$ and $\\mathrm{Cov}(v) = \\frac{1}{\\beta} I$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\beta} A^{-1}  \\frac{1}{\\beta} I \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A cornerstone of simulating Langevin dynamics is correctly discretizing the interplay between frictional damping and thermal noise, a relationship known as the fluctuation-dissipation theorem. This exercise focuses on the velocity update, an exact solution to the Ornstein-Uhlenbeck process, which models this balance. By verifying that this discrete-time update preserves the correct equilibrium variance, you will gain insight into how stable numerical integrators are constructed to prevent unwanted heating or cooling and maintain the target temperature .",
            "id": "3359245",
            "problem": "Consider the scalar velocity sub-dynamics in underdamped Langevin dynamics, modeled as an Ornstein–Uhlenbeck (OU) step. The continuous-time stochastic differential equation is\n$$\n\\mathrm{d}v(t) \\;=\\; -\\,\\gamma\\,v(t)\\,\\mathrm{d}t \\;+\\; \\sqrt{\\frac{2\\,\\gamma}{\\beta}}\\,\\mathrm{d}W(t),\n$$\nwhere $v(t)$ is the velocity, $\\gamma \\!\\! 0$ is the friction coefficient, $\\beta \\!\\! 0$ is the inverse thermal energy, and $W(t)$ is a standard Wiener process. Over a finite time-step $\\Delta t \\!\\! 0$, the exact OU update can be written as\n$$\nv_{+} \\;=\\; \\exp(-\\gamma\\,\\Delta t)\\,v_{-} \\;+\\; \\sqrt{\\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}}\\,\\xi,\n$$\nwhere $\\xi \\sim \\mathcal{N}(0,1)$ is independent of $v_{-}$. Assume $\\mathbb{E}[v_{-}] = 0$ and $\\mathrm{Var}(v_{-}) = \\sigma_{0}^{2}$.\n\nUsing only fundamental properties of linear stochastic differential equations and the independence of Gaussian increments, derive the variance of $v_{+}$ in closed form in terms of $\\sigma_{0}^{2}$, $\\gamma$, $\\Delta t$, and $\\beta$. Then, by specializing your expression to the equilibrium value $\\sigma_{0}^{2} = 1/\\beta$, verify the discrete-time fluctuation–dissipation consistency (invariance of the equilibrium variance under the OU update). \n\nProvide your final answer as the single closed-form analytic expression for $\\mathrm{Var}(v_{+})$. No rounding is required and no units are involved.",
            "solution": "The problem statement is critically validated and found to be valid. It is scientifically grounded, well-posed, objective, and self-contained. The model presented is the standard Ornstein–Uhlenbeck process, which accurately describes the velocity component in underdamped Langevin dynamics. All parameters and conditions are clearly defined and consistent with established principles of statistical physics and stochastic calculus.\n\nThe primary objective is to derive the variance of the updated velocity, $\\mathrm{Var}(v_{+})$, given the discrete-time update rule:\n$$\nv_{+} \\;=\\; \\exp(-\\gamma\\,\\Delta t)\\,v_{-} \\;+\\; \\sqrt{\\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}}\\,\\xi\n$$\nWe are given that $\\mathbb{E}[v_{-}] = 0$ and $\\mathrm{Var}(v_{-}) = \\sigma_{0}^{2}$. The random variable $\\xi$ is drawn from a standard normal distribution, $\\xi \\sim \\mathcal{N}(0,1)$, which implies $\\mathbb{E}[\\xi] = 0$ and $\\mathrm{Var}(\\xi) = 1$. A crucial piece of information is that $\\xi$ is independent of $v_{-}$.\n\nThe expression for $v_{+}$ is a linear combination of the two random variables $v_{-}$ and $\\xi$. Let us define two new random variables for clarity:\n$$\nA = \\exp(-\\gamma\\,\\Delta t)\\,v_{-}\n$$\n$$\nB = \\sqrt{\\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}}\\,\\xi\n$$\nSo, $v_{+} = A + B$.\n\nThe variance of a sum of two random variables is given by $\\mathrm{Var}(A+B) = \\mathrm{Var}(A) + \\mathrm{Var}(B) + 2\\,\\mathrm{Cov}(A,B)$. Since $v_{-}$ and $\\xi$ are independent, any deterministic functions of them, such as $A$ and $B$, are also independent. For independent variables, their covariance is zero, i.e., $\\mathrm{Cov}(A,B) = 0$. Therefore, the variance of the sum simplifies to:\n$$\n\\mathrm{Var}(v_{+}) = \\mathrm{Var}(A) + \\mathrm{Var}(B)\n$$\nNow, we compute the variance of $A$ and $B$ separately. For any random variable $X$ and constant $c$, the variance property is $\\mathrm{Var}(cX) = c^{2}\\mathrm{Var}(X)$.\n\nFor variable $A$:\n$$\n\\mathrm{Var}(A) = \\mathrm{Var}\\left(\\exp(-\\gamma\\,\\Delta t)\\,v_{-}\\right) = \\left(\\exp(-\\gamma\\,\\Delta t)\\right)^{2} \\mathrm{Var}(v_{-})\n$$\nSubstituting the given $\\mathrm{Var}(v_{-}) = \\sigma_{0}^{2}$, we get:\n$$\n\\mathrm{Var}(A) = \\exp(-2\\,\\gamma\\,\\Delta t)\\,\\sigma_{0}^{2}\n$$\n\nFor variable $B$:\n$$\n\\mathrm{Var}(B) = \\mathrm{Var}\\left(\\sqrt{\\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}}\\,\\xi\\right) = \\left(\\sqrt{\\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}}\\right)^{2} \\mathrm{Var}(\\xi)\n$$\nSubstituting $\\mathrm{Var}(\\xi) = 1$, we get:\n$$\n\\mathrm{Var}(B) = \\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}\n$$\n\nCombining these two results, we obtain the expression for the variance of $v_{+}$:\n$$\n\\mathrm{Var}(v_{+}) = \\mathrm{Var}(A) + \\mathrm{Var}(B) = \\sigma_{0}^{2}\\,\\exp(-2\\,\\gamma\\,\\Delta t) + \\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}\n$$\nThis is the closed-form expression for $\\mathrm{Var}(v_{+})$ in terms of $\\sigma_{0}^{2}$, $\\gamma$, $\\Delta t$, and $\\beta$.\n\nThe second part of the problem asks to verify the discrete-time fluctuation–dissipation consistency. This involves checking if the equilibrium variance is invariant under the time evolution step. The equilibrium variance for the velocity in the Langevin system is known to be $\\frac{1}{\\beta}$, corresponding to the equipartition theorem where the average kinetic energy $\\frac{1}{2}m\\langle v^2 \\rangle = \\frac{1}{2}k_B T$. In our notation with unit mass ($m=1$) and $\\beta = (k_B T)^{-1}$, this is $\\frac{1}{2}\\mathrm{Var}(v) = \\frac{1}{2\\beta}$, so $\\mathrm{Var}(v) = \\frac{1}{\\beta}$.\n\nLet's specialize our derived expression by setting the initial variance to this equilibrium value, $\\sigma_{0}^{2} = \\frac{1}{\\beta}$:\n$$\n\\mathrm{Var}(v_{+}) = \\left(\\frac{1}{\\beta}\\right)\\,\\exp(-2\\,\\gamma\\,\\Delta t) + \\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}\n$$\nWe can factor out the common term $\\frac{1}{\\beta}$:\n$$\n\\mathrm{Var}(v_{+}) = \\frac{1}{\\beta} \\left( \\exp(-2\\,\\gamma\\,\\Delta t) + 1 - \\exp(-2\\,\\gamma\\,\\Delta t) \\right)\n$$\nThe exponential terms cancel each other out:\n$$\n\\mathrm{Var}(v_{+}) = \\frac{1}{\\beta} \\left( 1 \\right) = \\frac{1}{\\beta}\n$$\nThis result confirms that if the velocity distribution has the equilibrium variance $\\frac{1}{\\beta}$ at the beginning of the time step, it will retain this variance after the Ornstein–Uhlenbeck update. This demonstrates the consistency of the discrete update rule with the stationary (equilibrium) distribution, which is a key aspect of the fluctuation-dissipation theorem. The final answer required is the general expression for $\\mathrm{Var}(v_{+})$ derived initially.",
            "answer": "$$\\boxed{\\sigma_{0}^{2} \\exp(-2 \\gamma \\Delta t) + \\frac{1 - \\exp(-2 \\gamma \\Delta t)}{\\beta}}$$"
        },
        {
            "introduction": "The efficiency of a Monte Carlo sampler is not just about correctness, but about how quickly it generates independent samples from the target distribution. This advanced practice delves into the art of optimizing underdamped Langevin samplers by balancing deterministic exploration via Hamiltonian dynamics with stochastic momentum refreshments. You will derive the Effective Sample Size (ESS) per unit of computational cost, a key performance metric, and explore how the frequency of refreshments affects the sampler's ability to decorrelate, a vital concept in practical high-dimensional sampling .",
            "id": "3359269",
            "problem": "Consider one-dimensional underdamped Langevin dynamics at unit inverse temperature $ \\beta = 1 $ targeting the Gaussian density proportional to $ \\exp\\left(-U(x)\\right) $ with quadratic potential $ U(x) = \\tfrac{1}{2} x^2 $. Let the mass be $ m = 1 $ and the canonical Hamiltonian be $ H(x,v) = \\tfrac{1}{2} x^2 + \\tfrac{1}{2} v^2 $. Between momentum refreshments, the system evolves under the exact Hamiltonian flow for $ K $ micro-steps, each of size $ h  0 $, so that the combined $ K $-step flow is a rotation in phase space by angle $ K h $ (angles are in radians). After each block of $ K $ micro-steps, perform a partial momentum refresh $v \\leftarrow \\rho v + \\sqrt{1 - \\rho^2} \\, \\eta$, where $ \\eta \\sim \\mathcal{N}(0,1) $ is independent standard Gaussian noise and $\\rho \\in [0,1)$ is a fixed refreshment coefficient. Positions are recorded only immediately after each momentum refresh (i.e., at refresh instants). Assume refresh cost is negligible relative to force-gradient evaluations, and that each micro-step consumes exactly $ 1 $ gradient evaluation.\n\nStarting only from the following fundamental bases:\n- Hamilton’s equations for the Gaussian target under $ H(x,v) = \\tfrac{1}{2} x^2 + \\tfrac{1}{2} v^2 $, which imply the exact flow over time $ t $ is a rotation by angle $ t $ in the $ (x,v) $ plane and preserves the canonical measure with independent standard normal marginals for $ x $ and $ v $.\n- The definition of autoregressive correlation, integrated autocorrelation time, and Effective Sample Size (ESS), where the integrated autocorrelation time for a stationary scalar process with lag autocorrelation function $ \\rho_\\ell $ is $ \\tau_{\\mathrm{int}} = 1 + 2 \\sum_{\\ell=1}^{\\infty} \\rho_\\ell $, and the Effective Sample Size for $ N $ samples is $ \\mathrm{ESS} = \\dfrac{N}{\\tau_{\\mathrm{int}}} $.\n\nTasks:\n1) Derive, from first principles, the one-step lag autocorrelation of the recorded position process $ \\{x_n\\} $ (positions sampled at refresh instants) as a function of $ K $ and $ h $. Explain whether and why it depends on the refreshment coefficient $ \\rho $.\n2) Using the result of Task $ 1 $, derive the integrated autocorrelation time $ \\tau_{\\mathrm{int}}(K,h) $ of $ \\{x_n\\} $, and then the Effective Sample Size per recorded sample, $ \\mathrm{ESS}_{\\mathrm{per\\mbox{-}sample}}(K,h) $.\n3) Translate computational cost into an ESS per gradient evaluation, $ \\mathrm{ESS}_{\\mathrm{per\\mbox{-}grad}}(K,h) $, by accounting for the $ K $ micro-steps between recorded samples. Define the improvement factor $ I(K,h) $ as the ratio $ \\dfrac{\\mathrm{ESS}_{\\mathrm{per\\mbox{-}grad}}(K,h)}{\\mathrm{ESS}_{\\mathrm{per\\mbox{-}grad}}(1,h)} $.\n4) Implement a program that computes the theoretical improvement factor $ I(K,h) $ for a given test suite. Angles must be treated in radians. The program must output the improvement factors as floating-point numbers rounded to six decimal places. The final output formatting must be a single line containing a comma-separated list enclosed in square brackets.\n\nTest suite to evaluate and report $ I(K,h) $ in this exact order:\n- Case $ 1 $: $ h = 0.2 $, $ K = 1 $.\n- Case $ 2 $: $ h = 0.2 $, $ K = 7 $.\n- Case $ 3 $: $ h = 0.2 $, $ K = 8 $.\n- Case $ 4 $: $ h = 0.2 $, $ K = 16 $.\n- Case $ 5 $: $ h = 0.3 $, $ K = 5 $.\n- Case $ 6 $: $ h = 0.2 $, $ K = 31 $.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $ [r_1,r_2,\\dots,r_6] $), where each $ r_i $ is the theoretical improvement factor $ I(K,h) $ for the corresponding case, rounded to six decimal places. No other output is permitted.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, and objective. It provides a complete and consistent setup for a standard problem in stochastic simulation methods. We may therefore proceed with a formal derivation.\n\nThe overall task is to determine the sampling efficiency of an underdamped Langevin dynamics integrator for a simple harmonic oscillator potential, as a function of the number of Hamiltonian steps $K$ taken between momentum refreshments. The efficiency is quantified by the Effective Sample Size (ESS) per unit of computational cost, where cost is measured in gradient evaluations.\n\nLet the state of the one-dimensional system at the $n$-th step, immediately after the momentum refresh, be denoted by the phase space vector $(x_n, v_n)$. The stationary distribution for these dynamics is the canonical distribution, which for the given Hamiltonian $H(x,v) = \\tfrac{1}{2} x^2 + \\tfrac{1}{2} v^2$ at unit mass $m=1$ and unit inverse temperature $\\beta=1$, is a product of two independent standard normal distributions. Thus, in the stationary regime, we have:\n$\\mathbb{E}[x_n] = 0$, $\\mathbb{E}[v_n] = 0$\n$\\mathrm{Var}(x_n) = \\mathbb{E}[x_n^2] = 1$\n$\\mathrm{Var}(v_n) = \\mathbb{E}[v_n^2] = 1$\n$\\mathrm{Cov}(x_n, v_n) = \\mathbb{E}[x_n v_n] = 0$\n\nThe evolution from $(x_n, v_n)$ to $(x_{n+1}, v_{n+1})$ consists of two parts:\n1.  **Hamiltonian Flow:** The state $(x_n, v_n)$ evolves for a time $t = K h$ under Hamilton's equations. For the harmonic potential $U(x) = \\tfrac{1}{2}x^2$, the equations of motion are $\\dot{x} = v$ and $\\dot{v} = -x$. The solution is a rotation in the $(x,v)$ phase plane. Let the state just before the next refresh be $(x'_{n+1}, v'_{n+1})$. With angle $\\theta = K h$, the transformation is:\n$$\n\\begin{pmatrix} x'_{n+1} \\\\ v'_{n+1} \\end{pmatrix} =\n\\begin{pmatrix} \\cos(K h)  \\sin(K h) \\\\ -\\sin(K h)  \\cos(K h) \\end{pmatrix}\n\\begin{pmatrix} x_n \\\\ v_n \\end{pmatrix}\n$$\n2.  **Momentum Refresh:** A partial momentum refresh is applied. The position is unchanged, $x_{n+1} = x'_{n+1}$, while the velocity is updated:\n$$\nv_{n+1} = \\rho v'_{n+1} + \\sqrt{1 - \\rho^2} \\, \\eta_n\n$$\nwhere $\\eta_n \\sim \\mathcal{N}(0,1)$ is a standard Gaussian random variable independent of $(x_n, v_n)$.\n\nThe recorded samples are the positions $x_n$ taken immediately after each refresh.\n\n**Task 1: One-Step Lag Autocorrelation**\n\nWe need to compute the lag-$1$ autocorrelation of the position process $\\{x_n\\}$, which is defined as $\\rho_1 = \\mathrm{Corr}(x_{n+1}, x_n)$. Since the process is stationary with zero mean and unit variance, this simplifies to $\\rho_1 = \\mathbb{E}[x_{n+1} x_n]$.\n\nThe position at step $n+1$ is given by the Hamiltonian flow part:\n$$\nx_{n+1} = x'_{n+1} = x_n \\cos(K h) + v_n \\sin(K h)\n$$\nWe compute the expectation of the product $x_{n+1} x_n$:\n$$\n\\mathbb{E}[x_{n+1} x_n] = \\mathbb{E}[(x_n \\cos(K h) + v_n \\sin(K h)) x_n]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[x_{n+1} x_n] = \\mathbb{E}[x_n^2] \\cos(K h) + \\mathbb{E}[x_n v_n] \\sin(K h)\n$$\nUsing the stationary properties $\\mathbb{E}[x_n^2] = 1$ and $\\mathbb{E}[x_n v_n] = 0$:\n$$\n\\rho_1 = (1) \\cos(K h) + (0) \\sin(K h) = \\cos(K h)\n$$\nThe lag-$1$ autocorrelation is $\\rho_1 = \\cos(K h)$. This result depends on $K$ and $h$, but it does not depend on the refreshment coefficient $\\rho$. This is because the position $x_{n+1}$ is determined fully by the state $(x_n, v_n)$ and the Hamiltonian evolution. The momentum refresh, which involves $\\rho$, only affects the subsequent velocity $v_{n+1}$ and thus correlations at lag $2$ and higher, but not the correlation between $x_n$ and $x_{n+1}$.\n\n**Task 2: Integrated Autocorrelation Time and ESS per Sample**\n\nTo derive the integrated autocorrelation time, $\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{\\ell=1}^{\\infty} \\rho_\\ell$, we need the full autocorrelation function $\\{\\rho_\\ell\\}_{\\ell \\ge 1}$. This requires establishing the time-series structure of the process $\\{x_n\\}$. Let's write the full update equations:\n$$\nx_{n+1} = x_n \\cos(K h) + v_n \\sin(K h)\n$$\n$$\nv_{n+1} = \\rho (-x_n \\sin(K h) + v_n \\cos(K h)) + \\sqrt{1 - \\rho^2} \\eta_n\n$$\nThese equations describe a vector autoregressive process of order $1$. By eliminating the velocity variables, we can find the recurrence for the position process $\\{x_n\\}$ alone. From the first equation, we can express $v_n$ in terms of positions: $v_n \\sin(K h) = x_{n+1} - x_n \\cos(K h)$. Advancing the index by $-1$, we have $v_{n-1} \\sin(K h) = x_n - x_{n-1} \\cos(K h)$. The velocity update for $v_n$ is $v_n = \\rho (-x_{n-1} \\sin(K h) + v_{n-1} \\cos(K h)) + \\sqrt{1 - \\rho^2} \\eta_{n-1}$. Substituting the expressions for $v_n$ and $v_{n-1}$ yields a recurrence solely in terms of positions. This algebraic manipulation confirms that $\\{x_n\\}$ follows an AR(2) process:\n$$\nx_{n+1} = \\phi_1 x_n + \\phi_2 x_{n-1} + \\text{noise}_n\n$$\nwith coefficients $\\phi_1 = (1+\\rho)\\cos(K h)$ and $\\phi_2 = -\\rho$. The autocorrelations $\\rho_\\ell$ of this process satisfy the Yule-Walker equations: $\\rho_\\ell = \\phi_1 \\rho_{\\ell-1} + \\phi_2 \\rho_{\\ell-2}$ for $\\ell \\ge 2$. Summing this recurrence from $\\ell=2$ to $\\infty$ allows us to find the sum $S = \\sum_{\\ell=1}^\\infty \\rho_\\ell$:\n$$\nS = \\frac{\\rho_1 + \\phi_2}{1 - \\phi_1 - \\phi_2} = \\frac{\\cos(K h) - \\rho}{1 - (1+\\rho)\\cos(K h) + \\rho} = \\frac{\\cos(K h) - \\rho}{(1+\\rho)(1-\\cos(K h))}\n$$\nThe integrated autocorrelation time is then:\n$$\n\\tau_{\\mathrm{int}}(K, h, \\rho) = 1 + 2 S = 1 + 2 \\frac{\\cos(K h) - \\rho}{(1+\\rho)(1-\\cos(K h))} = \\frac{(1-\\rho)(1+\\cos(K h))}{(1+\\rho)(1-\\cos(K h))}\n$$\nThe Effective Sample Size per recorded sample is the reciprocal of the integrated autocorrelation time:\n$$\n\\mathrm{ESS}_{\\mathrm{per\\mbox{-}sample}}(K, h, \\rho) = \\frac{1}{\\tau_{\\mathrm{int}}} = \\frac{(1+\\rho)(1-\\cos(K h))}{(1-\\rho)(1+\\cos(K h))}\n$$\n\n**Task 3: ESS per Gradient and Improvement Factor**\n\nThe computational cost to generate one sample $x_n$ is dominated by the $K$ gradient evaluations performed during the Hamiltonian integration. We are given this cost is exactly $K$ units. Therefore, the ESS per gradient evaluation is:\n$$\n\\mathrm{ESS}_{\\mathrm{per\\mbox{-}grad}}(K, h, \\rho) = \\frac{\\mathrm{ESS}_{\\mathrm{per\\mbox{-}sample}}}{K} = \\frac{1}{K \\tau_{\\mathrm{int}}} = \\frac{1}{K} \\frac{(1+\\rho)(1-\\cos(K h))}{(1-\\rho)(1+\\cos(K h))}\n$$\nThe improvement factor $I(K,h)$ is the ratio of this efficiency measure for a given $(K,h)$ relative to the baseline case of $K=1$ with the same step size $h$:\n$$\nI(K, h) = \\frac{\\mathrm{ESS}_{\\mathrm{per\\mbox{-}grad}}(K, h, \\rho)}{\\mathrm{ESS}_{\\mathrm{per\\mbox{-}grad}}(1, h, \\rho)} = \\frac{\\frac{1}{K} \\frac{(1+\\rho)(1-\\cos(K h))}{(1-\\rho)(1+\\cos(K h))}}{\\frac{1}{1} \\frac{(1+\\rho)(1-\\cos(h))}{(1-\\rho)(1+\\cos(h))}}\n$$\nCrucially, the factor $\\frac{1+\\rho}{1-\\rho}$ cancels out, making the improvement factor independent of the refreshment coefficient $\\rho$:\n$$\nI(K, h) = \\frac{1}{K} \\frac{1-\\cos(K h)}{1+\\cos(K h)} \\frac{1+\\cos(h)}{1-\\cos(h)}\n$$\nUsing the trigonometric half-angle identities $1-\\cos(\\theta) = 2\\sin^2(\\theta/2)$ and $1+\\cos(\\theta) = 2\\cos^2(\\theta/2)$, we can simplify the expression $\\frac{1-\\cos(\\theta)}{1+\\cos(\\theta)} = \\tan^2(\\theta/2)$:\n$$\nI(K, h) = \\frac{1}{K} \\frac{\\tan^2(K h/2)}{\\tan^2(h/2)} = \\frac{1}{K} \\left( \\frac{\\tan(K h/2)}{\\tan(h/2)} \\right)^2\n$$\nThis is the final formula for the theoretical improvement factor.\n\n**Task 4: Implementation**\n\nThe derived formula for $I(K,h)$ is implemented in a Python program. The program computes this value for each pair of $(K,h)$ in the provided test suite. Angles are treated in radians as required by the trigonometric functions in standard numerical libraries.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the theoretical improvement factor I(K,h) for a series of test cases\n    based on the derived formula for underdamped Langevin dynamics on a quadratic potential.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (h, K)\n        (0.2, 1),\n        (0.2, 7),\n        (0.2, 8),\n        (0.2, 16),\n        (0.3, 5),\n        (0.2, 31),\n    ]\n\n    results = []\n    for h, K in test_cases:\n        # The derived formula for the improvement factor is:\n        # I(K, h) = (1/K) * (tan(K*h/2) / tan(h/2))^2\n        # The problem states angles are in radians, and numpy's trigonometric functions\n        # expect radians by default.\n        # The problem states h  0, so tan(h/2) is not zero.\n        \n        # Calculate arguments for the tan function\n        kh_div_2 = K * h / 2.0\n        h_div_2 = h / 2.0\n        \n        # Compute the ratio of tangents\n        tan_ratio = np.tan(kh_div_2) / np.tan(h_div_2)\n        \n        # Compute the improvement factor\n        improvement_factor = (1.0 / K) * (tan_ratio ** 2)\n        \n        # Round the result to six decimal places as required.\n        results.append(round(improvement_factor, 6))\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) conversion is sufficient as per standard float-to-string conversion.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}