{
    "hands_on_practices": [
        {
            "introduction": "A core challenge in designing an efficient Metropolis sampler is choosing the proposal distribution. The Adaptive Metropolis (AM) algorithm provides a powerful solution by learning the proposal covariance from the chain's history. This first exercise  delves into the theoretical justification for this approach, using the principle of affine invariance to argue why the optimal proposal covariance should mirror the shape of the target distribution. It then employs a diffusion-limit heuristic to derive the famous optimal scaling rule, showing how the proposal step size must shrink with increasing dimension to maintain efficiency.",
            "id": "3353620",
            "problem": "Consider the Haario–Saksman–Tamminen Adaptive Metropolis (AM) algorithm targeting a $d$-dimensional zero-mean Gaussian distribution with covariance matrix $\\Sigma_{\\star}$, that is, a target density $\\pi(x)$ proportional to $\\exp(-\\frac{1}{2} x^{\\top}\\Sigma_{\\star}^{-1}x)$ for $x\\in\\mathbb{R}^{d}$. At each iteration, the AM algorithm uses a symmetric Gaussian random-walk proposal of the form $q(x,\\cdot)=\\mathcal{N}(x, S)$, where $S$ is a positive-definite proposal covariance matrix that is adapted over time.\n\nStarting from the foundational properties of the Metropolis–Hastings algorithm and the affine invariance of Gaussian targets under linear transformations, argue—without invoking any specific optimal-scaling formula—that the asymptotically optimal structure of $S$ is a scalar multiple of $\\Sigma_{\\star}$, namely $S=s_{d}^{2}\\Sigma_{\\star}$ for some scalar $s_{d}>0$ depending on the dimension $d$. Then, using the diffusion-limit heuristic for high-dimensional Random Walk Metropolis (RWM), derive the limiting form of the acceptance probability as a function of the rescaled step size parameter and, by maximizing the expected squared jump distance, determine the optimal scalar $s_{d}$ as an explicit function of $d$.\n\nYour final answer must be a single closed-form analytic expression for $s_{d}$ in terms of $d$. Round the numerical prefactor to three significant figures. No physical units are involved in this problem.",
            "solution": "The problem asks for two main results regarding the Adaptive Metropolis (AM) algorithm for a Gaussian target. First, to deduce the optimal structure of the proposal covariance matrix $S$ using an affine invariance argument. Second, to derive the optimal scaling factor for this covariance matrix in the high-dimensional limit.\n\n### Part 1: Optimal Structure of the Proposal Covariance\n\nThe target density is $\\pi(x) \\propto \\exp(-\\frac{1}{2} x^{\\top}\\Sigma_{\\star}^{-1}x)$ for $x \\in \\mathbb{R}^d$. The proposal is a Gaussian random walk, $x' = x + \\xi$, where $\\xi \\sim \\mathcal{N}(0, S)$. The Metropolis-Hastings acceptance probability is $\\alpha(x, x') = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x)}\\right)$.\n\nWe invoke the principle of affine invariance. The efficiency of the sampling algorithm should not depend on the choice of basis for the state space $\\mathbb{R}^d$. Consider an arbitrary invertible linear transformation $y = Ax$, where $A$ is a $d \\times d$ matrix.\nIf $x \\sim \\mathcal{N}(0, \\Sigma_{\\star})$, then the transformed variable $y$ follows a Gaussian distribution $y \\sim \\mathcal{N}(0, A\\Sigma_{\\star}A^{\\top})$. The target density for $y$ is $\\pi_y(y) \\propto \\exp(-\\frac{1}{2} y^{\\top}(A\\Sigma_{\\star}A^{\\top})^{-1}y)$.\n\nThe proposal mechanism in the $y$-space corresponding to the one in the $x$-space is $y' = Ax' = A(x+\\xi) = y + A\\xi$. The proposal increment in the $y$-space is $\\xi_y = A\\xi$. Since $\\xi \\sim \\mathcal{N}(0, S)$, the transformed increment follows $\\xi_y \\sim \\mathcal{N}(0, ASA^{\\top})$. So, the proposal covariance in the $y$-space is $S_y = ASA^{\\top}$.\n\nThe acceptance probability for the original chain is a function of the log-ratio of target densities:\n$$ \\ln\\left(\\frac{\\pi(x')}{\\pi(x)}\\right) = -\\frac{1}{2}\\left( (x+\\xi)^{\\top}\\Sigma_{\\star}^{-1}(x+\\xi) - x^{\\top}\\Sigma_{\\star}^{-1}x \\right) = -x^{\\top}\\Sigma_{\\star}^{-1}\\xi - \\frac{1}{2}\\xi^{\\top}\\Sigma_{\\star}^{-1}\\xi $$\nThe acceptance probability for the transformed chain depends on the analogous quantity:\n$$ \\ln\\left(\\frac{\\pi_y(y')}{\\pi_y(y)}\\right) = -y^{\\top}(A\\Sigma_{\\star}A^{\\top})^{-1}\\xi_y - \\frac{1}{2}\\xi_y^{\\top}(A\\Sigma_{\\star}A^{\\top})^{-1}\\xi_y $$\nSubstituting $y = Ax$, $\\xi_y = A\\xi$, and $(A\\Sigma_{\\star}A^{\\top})^{-1} = (A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}$:\n$$ \\ln\\left(\\frac{\\pi_y(y')}{\\pi_y(y)}\\right) = -(Ax)^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}(A\\xi) - \\frac{1}{2}(A\\xi)^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}(A\\xi) $$\n$$ = -x^{\\top}A^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}A\\xi - \\frac{1}{2}\\xi^{\\top}A^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}A\\xi $$\n$$ = -x^{\\top}\\Sigma_{\\star}^{-1}\\xi - \\frac{1}{2}\\xi^{\\top}\\Sigma_{\\star}^{-1}\\xi = \\ln\\left(\\frac{\\pi(x')}{\\pi(x)}\\right) $$\nThe acceptance probabilities are identical. This means that algorithm performance metrics (like acceptance rate and autocorrelation time) are invariant under affine transformations, provided the proposal covariance is transformed accordingly as $S \\to ASA^{\\top}$.\n\nFor an adaptive algorithm that learns the proposal covariance $S$, it is natural to require that the optimal learned covariance respects this invariance. The problem becomes simplest in the coordinate system where the target distribution is uncorrelated and has unit variance in all directions, i.e., it is isotropic. We can achieve this by choosing a transformation matrix $A$ such that $A\\Sigma_{\\star}A^{\\top} = I_d$, the $d \\times d$ identity matrix. For instance, we can use a whitening transformation $A = \\Sigma_{\\star}^{-1/2}$. In this whitened space, the target is $\\mathcal{N}(0, I_d)$.\n\nFor an isotropic target, there is no reason to prefer one direction over another. Any efficient exploration of the state space should also be isotropic. Therefore, the optimal proposal covariance in the whitened space must be proportional to the identity matrix: $S_y = c I_d$ for some scalar $c > 0$.\n\nTransforming this optimal proposal back to the original $x$-space gives the structure of the optimal $S$:\n$S_y = ASA^{\\top} \\implies S = A^{-1} S_y (A^{\\top})^{-1}$\nUsing $A = \\Sigma_{\\star}^{-1/2}$ (so $A^{-1} = \\Sigma_{\\star}^{1/2}$) and $S_y = c I_d$:\n$$ S = \\Sigma_{\\star}^{1/2} (c I_d) (\\Sigma_{\\star}^{-1/2})^{\\top} = c \\Sigma_{\\star}^{1/2} (\\Sigma_{\\star}^{1/2})^{\\top} = c \\Sigma_{\\star} $$\nThus, the asymptotically optimal proposal covariance matrix $S$ must be a scalar multiple of the target covariance matrix $\\Sigma_{\\star}$. We denote the positive scalar constant of proportionality by $s_d^2$, so $S = s_d^2 \\Sigma_{\\star}$.\n\n### Part 2: Optimal Scaling Factor $s_d$\n\nBased on the argument above, we can analyze the algorithm's performance in the standardized space where the target is $\\pi(y) \\propto \\exp(-\\frac{1}{2}y^{\\top}y)$ and the proposal is $\\mathcal{N}(y, s_d^2 I_d)$, and the results will apply to the general case. We use $x$ instead of $y$ for simplicity in this section. The proposal is $x' = x + \\xi$ with $\\xi \\sim \\mathcal{N}(0, s_d^2 I_d)$.\n\nWe use the high-dimensional ($d \\to \\infty$) diffusion-limit heuristic. For the acceptance rate to remain non-zero, the step size must vanish. We use the scaling ansatz $s_d = \\lambda / \\sqrt{d}$ for some constant $\\lambda > 0$. The proposal is thus $\\xi \\sim \\mathcal{N}(0, (\\lambda^2/d)I_d)$.\n\nThe change in the log-target is $\\Delta E(x, \\xi) = \\frac{1}{2}\\|x\\|^2 - \\frac{1}{2}\\|x'\\|^2 = -x^{\\top}\\xi - \\frac{1}{2}\\|\\xi\\|^2$. The acceptance probability is $\\alpha(x, x') = \\min(1, \\exp(\\Delta E))$. Let's analyze the two terms in $\\Delta E$:\n1.  By the Law of Large Numbers, as $d \\to \\infty$, $\\|\\xi\\|^2 = \\sum_{i=1}^d \\xi_i^2$ converges to its expectation:\n    $$ E[\\|\\xi\\|^2] = E\\left[\\sum_{i=1}^d \\xi_i^2\\right] = d \\cdot E[\\xi_1^2] = d \\cdot \\text{Var}(\\xi_1) = d \\cdot \\frac{\\lambda^2}{d} = \\lambda^2 $$\n    So, $\\|\\xi\\|^2 \\to \\lambda^2$.\n2.  The term $x^{\\top}\\xi = \\sum_{i=1}^d x_i \\xi_i$ is a sum of i.i.d. terms, where $x_i \\sim \\mathcal{N}(0, 1)$ (since $x$ is from the stationary distribution) and $\\xi_i \\sim \\mathcal{N}(0, \\lambda^2/d)$. Each term has mean $E[x_i\\xi_i] = E[x_i]E[\\xi_i] = 0$ and variance $\\text{Var}(x_i\\xi_i) = E[x_i^2]E[\\xi_i^2] = 1 \\cdot (\\lambda^2/d) = \\lambda^2/d$.\n    The variance of the sum is $\\text{Var}(x^{\\top}\\xi) = d \\cdot (\\lambda^2/d) = \\lambda^2$. By the Central Limit Theorem, $x^{\\top}\\xi$ converges in distribution to a normal random variable $Z \\sim \\mathcal{N}(0, \\lambda^2)$.\n\nIn the limit $d\\to\\infty$, the acceptance probability becomes a deterministic function of $\\lambda$, obtained by averaging over the limiting distributions of the random components:\n$$ \\alpha(\\lambda) = E_Z[\\min(1, \\exp(-Z - \\frac{1}{2}\\lambda^2))] $$\nLet $Y = -Z - \\frac{1}{2}\\lambda^2$. Since $Z \\sim \\mathcal{N}(0, \\lambda^2)$, $Y \\sim \\mathcal{N}(-\\frac{1}{2}\\lambda^2, \\lambda^2)$. The expectation is\n$$ \\alpha(\\lambda) = \\int_{-\\infty}^{\\infty} \\min(1, e^y) p(y) dy = \\int_{-\\infty}^{0} e^y p(y) dy + \\int_{0}^{\\infty} p(y) dy $$\nwhere $p(y)$ is the density of $Y$. Let $\\Phi$ be the standard normal CDF. The second term is $P(Y > 0) = P\\left(\\mathcal{N}(-\\frac{1}{2}\\lambda^2, \\lambda^2) > 0\\right) = P\\left(\\mathcal{N}(0, 1) > \\frac{\\lambda}{2}\\right) = 1 - \\Phi(\\lambda/2) = \\Phi(-\\lambda/2)$.\nIt can be shown that the first integral also evaluates to $\\Phi(-\\lambda/2)$. Thus, the limiting acceptance rate is\n$$ \\alpha(\\lambda) = 2\\Phi(-\\lambda/2) $$\n\nTo find the optimal $\\lambda$, we maximize the sampler's efficiency, which is proportional to the Expected Squared Jump Distance (ESJD) per iteration. The jump is $x_{n+1}-x_n$, which is $\\xi$ on acceptance and $0$ on rejection.\n$$ \\text{ESJD} = E[\\|x_{n+1}-x_n\\|^2] = E[\\alpha(x_n, x_n') \\|\\xi\\|^2] $$\nIn the high-dimensional limit, the acceptance probability and the norm of the proposal step become asymptotically independent.\n$$ \\text{ESJD}(\\lambda) \\approx E[\\alpha(x_n, x_n')] E[\\|\\xi\\|^2] = \\alpha(\\lambda) E[\\|\\xi\\|^2] $$\nAs shown earlier, $E[\\|\\xi\\|^2] = \\lambda^2$. So we want to maximize the function $f(\\lambda) = \\lambda^2 \\alpha(\\lambda) = 2\\lambda^2 \\Phi(-\\lambda/2)$. We find the maximum by setting the derivative with respect to $\\lambda$ to zero:\n$$ \\frac{df}{d\\lambda} = \\frac{d}{d\\lambda} \\left[2\\lambda^2 \\Phi(-\\lambda/2)\\right] = 4\\lambda\\Phi(-\\lambda/2) + 2\\lambda^2 \\frac{d}{d\\lambda}\\Phi(-\\lambda/2) = 0 $$\nUsing the chain rule, and letting $\\phi$ be the standard normal PDF, $\\frac{d}{d\\lambda}\\Phi(-\\lambda/2) = \\phi(-\\lambda/2) \\cdot (-\\frac{1}{2}) = -\\frac{1}{2}\\phi(\\lambda/2)$ since $\\phi$ is an even function.\n$$ 4\\lambda\\Phi(-\\lambda/2) - \\lambda^2\\phi(\\lambda/2) = 0 $$\nFor $\\lambda > 0$, we can divide by $\\lambda$:\n$$ 4\\Phi(-\\lambda/2) = \\lambda\\phi(\\lambda/2) $$\nThis is a transcendental equation for the optimal $\\lambda$, which we label $\\lambda_{\\text{opt}}$. The equation cannot be solved in terms of elementary functions. Numerical solution yields $\\lambda_{\\text{opt}} \\approx 2.379$. The question requires rounding to three significant figures, so $\\lambda_{\\text{opt}} \\approx 2.38$.\n\nThe optimal scalar $s_d$ is then given by our scaling ansatz:\n$$ s_d = \\frac{\\lambda_{\\text{opt}}}{\\sqrt{d}} \\approx \\frac{2.38}{\\sqrt{d}} $$\nThis provides the explicit functional dependence of $s_d$ on the dimension $d$.",
            "answer": "$$ \\boxed{\\frac{2.38}{\\sqrt{d}}} $$"
        },
        {
            "introduction": "While the previous exercise established the optimal scaling law for the proposal variance, a related practical question is what acceptance rate we should aim for during a run. The acceptance rate is a crucial diagnostic for MCMC convergence and efficiency. This practice problem  directly computes the stationary acceptance rate in the high-dimensional limit, expressing it as a simple function of the scaling parameter. This classic result provides the theoretical foundation for the widely cited optimal acceptance rate of approximately $0.234$ for a broad class of problems.",
            "id": "3353654",
            "problem": "Consider the $d$-dimensional Random-Walk Metropolis algorithm targeting a multivariate normal distribution $\\pi = \\mathcal{N}(0, \\Sigma_{\\star})$ with symmetric proposal $q(\\cdot \\mid x) = \\mathcal{N}(x, s^{2}\\Sigma_{\\star})$ for a scalar scaling parameter $s > 0$. At stationarity, let $x \\sim \\pi$ and let $y \\sim q(\\cdot \\mid x)$; the acceptance probability is $\\alpha(x,y) = \\min\\{1, \\pi(y)/\\pi(x)\\}$. Using only the definition of the Metropolis-Hastings acceptance probability and properties of the multivariate normal distribution, derive from first principles an exact one-dimensional conditional representation of the stationary mean acceptance rate\n$$\n\\alpha_{d}(s) \\equiv \\mathbb{E}[\\alpha(x,y)]\n$$\nin terms of an expectation with respect to the Euclidean norm of a standard normal vector. Then, under the high-dimensional scaling $s = \\ell/\\sqrt{d}$ with fixed $\\ell \\in (0,\\infty)$, use a law of large numbers argument to compute the limiting stationary acceptance rate\n$$\n\\lim_{d \\to \\infty} \\alpha_{d}\\!\\left(\\frac{\\ell}{\\sqrt{d}}\\right)\n$$\nas a closed-form analytic expression in $\\ell$. Finally, briefly explain how this limiting expression justifies the scaling choice $s^{2} \\approx c^{2}/d$ in the Haario–Saksman–Tamminen Adaptive Metropolis algorithm, where the empirical covariance $\\widehat{\\Sigma}$ of the chain replaces $\\Sigma_{\\star}$.\n\nProvide your final answer as a single analytic expression in terms of $\\ell$. No rounding is required.",
            "solution": "The target distribution is a $d$-dimensional multivariate normal distribution $\\pi = \\mathcal{N}(0, \\Sigma_{\\star})$. Its probability density function (PDF) at a point $z \\in \\mathbb{R}^d$ is given by\n$$\n\\pi(z) = \\frac{1}{(2\\pi)^{d/2} \\det(\\Sigma_{\\star})^{1/2}} \\exp\\left(-\\frac{1}{2} z^\\top \\Sigma_{\\star}^{-1} z\\right)\n$$\nThe proposal distribution is a symmetric random walk, $q(y \\mid x) = \\mathcal{N}(x, s^2 \\Sigma_{\\star})$. This means a proposed state $y$ is generated as $y = x + \\epsilon$, where the perturbation $\\epsilon$ is drawn from $\\mathcal{N}(0, s^2 \\Sigma_{\\star})$.\n\nThe Metropolis-Hastings acceptance probability is $\\alpha(x,y) = \\min\\{1, R(x,y)\\}$, where the ratio $R(x,y)$ for a symmetric proposal is given by\n$$\nR(x,y) = \\frac{\\pi(y)}{\\pi(x)}\n$$\nSubstituting the PDF of the target distribution, we get\n$$\nR(x,y) = \\frac{\\exp\\left(-\\frac{1}{2} y^\\top \\Sigma_{\\star}^{-1} y\\right)}{\\exp\\left(-\\frac{1}{2} x^\\top \\Sigma_{\\star}^{-1} x\\right)} = \\exp\\left(-\\frac{1}{2} \\left( y^\\top \\Sigma_{\\star}^{-1} y - x^\\top \\Sigma_{\\star}^{-1} x \\right) \\right)\n$$\nWe substitute $y = x + \\epsilon$ into the quadratic form in the exponent:\n\\begin{align*}\ny^\\top \\Sigma_{\\star}^{-1} y - x^\\top \\Sigma_{\\star}^{-1} x &= (x+\\epsilon)^\\top \\Sigma_{\\star}^{-1} (x+\\epsilon) - x^\\top \\Sigma_{\\star}^{-1} x \\\\\n&= (x^\\top + \\epsilon^\\top) \\Sigma_{\\star}^{-1} (x+\\epsilon) - x^\\top \\Sigma_{\\star}^{-1} x \\\\\n&= x^\\top \\Sigma_{\\star}^{-1} x + 2 x^\\top \\Sigma_{\\star}^{-1} \\epsilon + \\epsilon^\\top \\Sigma_{\\star}^{-1} \\epsilon - x^\\top \\Sigma_{\\star}^{-1} x \\\\\n&= 2 x^\\top \\Sigma_{\\star}^{-1} \\epsilon + \\epsilon^\\top \\Sigma_{\\star}^{-1} \\epsilon\n\\end{align*}\nwhere we have used the symmetry of $\\Sigma_{\\star}^{-1}$. The ratio becomes\n$$\nR(x,y) = \\exp\\left(-x^\\top \\Sigma_{\\star}^{-1} \\epsilon - \\frac{1}{2} \\epsilon^\\top \\Sigma_{\\star}^{-1} \\epsilon\\right)\n$$\nThe mean stationary acceptance rate is $\\alpha_d(s) = \\mathbb{E}[\\alpha(x,y)]$, where the expectation is over the stationary distribution of $x$ and the proposal distribution of $y$ given $x$. This means $x \\sim \\mathcal{N}(0, \\Sigma_{\\star})$ and, independently, $\\epsilon \\sim \\mathcal{N}(0, s^2 \\Sigma_{\\star})$.\n\nTo simplify the expression, we perform a change of variables. Let $A = \\Sigma_{\\star}^{-1/2}$ be the symmetric square root of the inverse covariance matrix. Define the transformed variables:\n$$\n\\tilde{x} = A x \\quad \\text{and} \\quad \\tilde{\\epsilon} = A \\epsilon\n$$\nIf $x \\sim \\mathcal{N}(0, \\Sigma_{\\star})$, then $\\tilde{x} \\sim \\mathcal{N}(0, A \\Sigma_{\\star} A^\\top) = \\mathcal{N}(0, I_d)$, a standard $d$-dimensional normal distribution.\nIf $\\epsilon \\sim \\mathcal{N}(0, s^2 \\Sigma_{\\star})$, then $\\tilde{\\epsilon} \\sim \\mathcal{N}(0, A (s^2 \\Sigma_{\\star}) A^\\top) = \\mathcal{N}(0, s^2 I_d)$. This means we can write $\\tilde{\\epsilon} = s Z$, where $Z \\sim \\mathcal{N}(0, I_d)$ is a standard $d$-dimensional normal vector, independent of $\\tilde{x}$.\n\nThe terms in the exponent can now be rewritten as:\n$$\nx^\\top \\Sigma_{\\star}^{-1} \\epsilon = (A x)^\\top (A \\epsilon) = \\tilde{x}^\\top \\tilde{\\epsilon} = s \\tilde{x}^\\top Z\n$$\n$$\n\\epsilon^\\top \\Sigma_{\\star}^{-1} \\epsilon = (A \\epsilon)^\\top (A \\epsilon) = \\tilde{\\epsilon}^\\top \\tilde{\\epsilon} = s^2 Z^\\top Z = s^2 \\|Z\\|^2\n$$\nThe ratio $R$ now depends on two independent standard normal vectors $\\tilde{x}$ and $Z$:\n$$\nR = \\exp\\left(-s \\tilde{x}^\\top Z - \\frac{1}{2} s^2 \\|Z\\|^2\\right)\n$$\nThe expectation is now over $\\tilde{x}, Z \\sim \\mathcal{N}(0, I_d)$. Let's analyze the term $\\tilde{x}^\\top Z$. Conditional on $Z$, this is a linear combination of independent standard normal random variables, $\\sum_{i=1}^d Z_i \\tilde{x}_i$. It is therefore a normal random variable with mean $\\mathbb{E}[\\tilde{x}^\\top Z \\mid Z] = 0$ and variance $\\text{Var}(\\tilde{x}^\\top Z \\mid Z) = Z^\\top \\text{Var}(\\tilde{x}) Z = Z^\\top I_d Z = \\|Z\\|^2$.\nThus, we can write $\\tilde{x}^\\top Z = \\|Z\\| W$, where $W \\sim \\mathcal{N}(0,1)$ is a standard normal variable, independent of $Z$.\n\nLet $V = s \\|Z\\|$. The ratio becomes a function of $V$ and $W$:\n$$\nR = \\exp(-V W - V^2/2)\n$$\nThe acceptance probability, conditioned on $Z$ (and thus on $V$), is $\\mathbb{E}_W[\\min\\{1, R\\}]$. Let's compute this inner expectation:\n\\begin{align*}\n\\mathbb{E}_W[\\min\\{1, \\exp(-V W - V^2/2)\\}] &= \\int_{-\\infty}^{\\infty} \\min\\{1, \\exp(-V w - V^2/2)\\} \\frac{1}{\\sqrt{2\\pi}} \\exp(-w^2/2) \\, dw \\\\\n&= \\int_{-V/2}^{\\infty} \\exp(-V w - V^2/2) \\frac{1}{\\sqrt{2\\pi}} \\exp(-w^2/2) \\, dw + \\int_{-\\infty}^{-V/2} 1 \\cdot \\frac{1}{\\sqrt{2\\pi}} \\exp(-w^2/2) \\, dw\n\\end{align*}\nThe second integral is simply the probability $\\mathbb{P}(W < -V/2) = \\Phi(-V/2)$, where $\\Phi$ is the CDF of the standard normal distribution.\nFor the first integral, we combine the exponents:\n$$\n-Vw - \\frac{V^2}{2} - \\frac{w^2}{2} = -\\frac{1}{2}(w^2 + 2Vw + V^2) = -\\frac{1}{2}(w+V)^2\n$$\nThe integral is $\\int_{-V/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2}(w+V)^2) \\, dw$. Let $u = w+V$, so $du = dw$. The lower limit of integration becomes $u = -V/2 + V = V/2$. The integral is:\n$$\n\\int_{V/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp(-u^2/2) \\, du = \\mathbb{P}(U > V/2) = 1 - \\Phi(V/2) = \\Phi(-V/2)\n$$\nwhere $U \\sim \\mathcal{N}(0,1)$.\nTherefore, the acceptance probability conditional on $Z$ is $\\Phi(-V/2) + \\Phi(-V/2) = 2 \\Phi(-V/2)$. Substituting back $V = s\\|Z\\|$, we get $2\\Phi(-s\\|Z\\|/2)$.\n\nTo find the unconditional mean acceptance rate $\\alpha_d(s)$, we take the expectation over $Z \\sim \\mathcal{N}(0, I_d)$. This gives the requested one-dimensional conditional representation:\n$$\n\\alpha_d(s) = \\mathbb{E}_{Z \\sim \\mathcal{N}(0, I_d)} [2 \\Phi(-s\\|Z\\|/2)]\n$$\n\nNext, we compute the high-dimensional limit with the scaling $s = \\ell/\\sqrt{d}$ for a fixed constant $\\ell \\in (0, \\infty)$.\n$$\n\\lim_{d \\to \\infty} \\alpha_d\\left(\\frac{\\ell}{\\sqrt{d}}\\right) = \\lim_{d \\to \\infty} \\mathbb{E} \\left[ 2 \\Phi\\left(-\\frac{\\ell}{\\sqrt{d}} \\frac{\\|Z\\|}{2}\\right) \\right] = 2 \\lim_{d \\to \\infty} \\mathbb{E} \\left[ \\Phi\\left(-\\frac{\\ell}{2} \\frac{\\|Z\\|}{\\sqrt{d}}\\right) \\right]\n$$\nLet's analyze the term $\\|Z\\|/\\sqrt{d}$. We have $\\|Z\\|^2 = \\sum_{i=1}^d Z_i^2$, where $Z_i$ are i.i.d. $\\mathcal{N}(0,1)$ random variables. By the Law of Large Numbers, the sample mean converges in probability to the true mean:\n$$\n\\frac{1}{d} \\|Z\\|^2 = \\frac{1}{d} \\sum_{i=1}^d Z_i^2 \\xrightarrow{p} \\mathbb{E}[Z_1^2] \\quad \\text{as } d \\to \\infty\n$$\nThe expectation $\\mathbb{E}[Z_1^2] = \\text{Var}(Z_1) + (\\mathbb{E}[Z_1])^2 = 1 + 0^2 = 1$.\nThus, $\\|Z\\|^2/d \\xrightarrow{p} 1$. Since the square root function $g(x) = \\sqrt{x}$ is continuous for $x > 0$, the Continuous Mapping Theorem implies that\n$$\n\\frac{\\|Z\\|}{\\sqrt{d}} = \\sqrt{\\frac{\\|Z\\|^2}{d}} \\xrightarrow{p} \\sqrt{1} = 1 \\quad \\text{as } d \\to \\infty\n$$\nThe argument of the $\\Phi$ function, $-\\frac{\\ell}{2} \\frac{\\|Z\\|}{\\sqrt{d}}$, therefore converges in probability to $-\\ell/2$.\nThe function $\\Phi(x)$ is continuous and bounded (by $0$ and $1$). The integrand $\\Phi(\\cdot)$ is thus also bounded. By the Bounded Convergence Theorem (a special case of the Dominated Convergence Theorem), we can interchange the limit and the expectation:\n$$\n\\lim_{d \\to \\infty} \\mathbb{E} \\left[ \\Phi\\left(-\\frac{\\ell}{2} \\frac{\\|Z\\|}{\\sqrt{d}}\\right) \\right] = \\mathbb{E} \\left[ \\lim_{d \\to \\infty, p} \\Phi\\left(-\\frac{\\ell}{2} \\frac{\\|Z\\|}{\\sqrt{d}}\\right) \\right] = \\mathbb{E} \\left[ \\Phi\\left(-\\frac{\\ell}{2}\\right) \\right]\n$$\nSince $-\\ell/2$ is a constant, the expectation is simply the value itself, $\\Phi(-\\ell/2)$.\nThe limiting stationary acceptance rate is therefore:\n$$\n\\lim_{d \\to \\infty} \\alpha_d\\left(\\frac{\\ell}{\\sqrt{d}}\\right) = 2 \\Phi(-\\ell/2)\n$$\n\nFinally, we explain the justification for the scaling in the Adaptive Metropolis (AM) algorithm. The derived limit, $\\alpha(\\ell) = 2 \\Phi(-\\ell/2)$, shows that in order to maintain a constant, non-trivial acceptance rate (i.e., not $0$ or $1$) as the dimension $d \\to \\infty$, the proposal scaling parameter $s$ must be of order $d^{-1/2}$. Specifically, the proposal variance $s^2 \\Sigma_{\\star}$ must scale as $s^2 \\propto 1/d$. If $s$ were constant, the acceptance rate would converge to $0$; if $s$ scaled faster than $d^{-1/2}$ (e.g., $1/d$), the rate would converge to $1$.\n\nFor a specific class of problems, including the one considered, theory suggests an optimal limiting acceptance rate of approximately $0.234$ that maximizes the efficiency of the sampler. By setting our limiting expression equal to this value, we can find the optimal $\\ell$:\n$$\n2 \\Phi(-\\ell/2) = 0.234 \\implies \\Phi(-\\ell/2) = 0.117\n$$\nUsing the inverse CDF of the standard normal, $\\Phi^{-1}(0.117) \\approx -1.19$. This gives $-\\ell/2 \\approx -1.19$, or $\\ell \\approx 2.38$.\nThe corresponding optimal proposal variance scaling is $s^2 = \\ell^2/d \\approx (2.38)^2/d$.\n\nThe Haario-Saksman-Tamminen AM algorithm uses a proposal $q(y \\mid x) = \\mathcal{N}(x, s_d^2 \\widehat{\\Sigma})$, where $\\widehat{\\Sigma}$ is the empirical covariance of the chain's history, an adaptive substitute for the unknown true covariance $\\Sigma_{\\star}$. The scaling factor $s_d^2$ is set to $c^2/d$, where the constant $c$ is chosen based on this theoretical result. The standard choice is $c = 2.38$ (or often rounded to $2.4$). This scaling ensures that as the algorithm explores a high-dimensional space, it maintains a stable and efficient acceptance rate, preventing the chain from getting stuck (rate too low) or mixing too slowly (rate too high). The theoretical derivation provides the fundamental justification for this crucial algorithmic design choice.",
            "answer": "$$\n\\boxed{2 \\Phi\\left(-\\frac{\\ell}{2}\\right)}\n$$"
        },
        {
            "introduction": "The theoretical power of the AM algorithm rests on adapting a full $d \\times d$ covariance matrix, a task that becomes computationally prohibitive as the dimension $d$ grows large. A successful implementation requires balancing statistical efficiency with computational feasibility. This final exercise  moves from theory to practice by analyzing the per-iteration computational cost of the AM algorithm and exploring powerful strategies, such as low-rank approximations and periodic updates, to make it practical for high-dimensional applications.",
            "id": "3353639",
            "problem": "Consider the Adaptive Metropolis (AM) algorithm of Haario, Saksman, and Tamminen for a Markov Chain Monte Carlo (MCMC) method targeting a distribution on $\\mathbb{R}^{d}$ with state $X_{n} \\in \\mathbb{R}^{d}$ at iteration $n$. The AM proposal has the form $Y_{n} = X_{n} + s L_{n} Z_{n}$, where $s > 0$ is a scalar, $Z_{n} \\sim \\mathcal{N}(0, I_{d})$, and $L_{n}$ is a Cholesky factor so that $C_{n} = L_{n} L_{n}^{\\top}$ approximates the target covariance. The covariance update is based on the empirical covariance of the chain and a Robbins-Monro step-size $\\eta_{n} \\in (0,1)$ with a small regularization on the diagonal. Let the mean update be $ \\mu_{n+1} = \\mu_{n} + \\eta_{n} (X_{n} - \\mu_{n})$ and the covariance update be\n$$\nC_{n+1} = (1 - \\eta_{n}) C_{n} + \\eta_{n} \\left( (X_{n} - \\mu_{n})(X_{n} - \\mu_{n})^{\\top} \\right) + \\eta_{n} \\epsilon I_{d},\n$$\nwith $\\epsilon > 0$ a small constant. Assume a standard floating-point cost model in which a matrix-vector multiplication by a dense $d \\times d$ matrix costs $2 d^{2}$ flops, a multiplication by a lower-triangular $d \\times d$ matrix costs exactly $d^{2}$ flops, a dense outer product of two $d$-vectors costs $d^{2}$ flops, addition or scaling of a $d \\times d$ matrix costs $d^{2}$ flops, addition or scaling of a $d$-vector costs $d$ flops, and a Cholesky factorization of a dense symmetric positive definite $d \\times d$ matrix costs $\\frac{1}{3} d^{3}$ flops. Ignore random number generation costs and target density evaluation costs; focus only on the linear-algebraic work of proposal generation and adaptation.\n\nTwo cost-reduction strategies are proposed:\n\n- Periodic covariance factorization: Instead of recomputing $L_{n}$ at every iteration, recompute the full Cholesky factorization of $C_{n}$ once every $m \\in \\mathbb{N}$ iterations and reuse the most recent $L_{n}$ in between. Treat the Cholesky cost amortized over $m$ iterations as $\\frac{1}{3} d^{3} / m$ flops per iteration.\n\n- Low-rank covariance approximation: Maintain an approximation $C_{n} \\approx D_{n} + U_{n} U_{n}^{\\top}$ where $D_{n} \\in \\mathbb{R}^{d \\times d}$ is diagonal and $U_{n} \\in \\mathbb{R}^{d \\times r}$ has rank $r$ with $1 \\le r \\ll d$. For proposal generation, draw $Z_{r} \\sim \\mathcal{N}(0, I_{r})$ and $Z_{d} \\sim \\mathcal{N}(0, I_{d})$, then compute $Y_{n} = X_{n} + s \\left( U_{n} Z_{r} + \\sqrt{D_{n}} \\odot Z_{d} \\right)$, where $\\odot$ denotes elementwise multiplication and $\\sqrt{D_{n}}$ denotes elementwise square roots of the diagonal entries. Use the following operation counts for this low-rank proposal: computing $U_{n} Z_{r}$ costs $2 d r$ flops, computing $\\sqrt{D_{n}} \\odot Z_{d}$ costs $d$ flops, vector additions and scalings add $3 d$ flops, so the total proposal generation cost is $2 d r + 4 d$ flops. For the low-rank covariance update, assume the following steps and costs per iteration: compute $v_{n} = X_{n} - \\mu_{n}$ in $d$ flops, update $\\mu_{n+1} = \\mu_{n} + \\eta_{n} v_{n}$ in $2 d$ flops, update $D_{n+1}$ from the diagonal of the rank-one term in $2 d$ flops, and update $U_{n+1}$ via a rank-$r$ incremental projection and augmentation costing $2 d r + r^{2}$ flops. The total low-rank adaptation cost per iteration is therefore $2 d r + r^{2} + 5 d$ flops. Continue to perform full Cholesky factorizations of $C_{n}$ periodically every $m$ iterations to prevent long-term drift, so the amortized periodic cost $\\frac{1}{3} d^{3} / m$ applies as above.\n\nStarting from these fundamental definitions and cost models, derive, in closed form, the amortized per-iteration flop count, denoted $C_{\\mathrm{lr}}(d, m, r)$, when both strategies are used jointly: low-rank proposal and covariance updates every iteration, and full Cholesky recomputation every $m$ iterations. Express your final answer as a single analytic expression in terms of $d$, $m$, and $r$. Do not simplify to big-$\\mathcal{O}$ notation; give the exact expression under the stated flop model. No numerical rounding is required, and no units are to be reported in the answer.",
            "solution": "The Adaptive Metropolis algorithm constructs proposals by drawing from a Gaussian distribution with covariance proportional to the current estimate $C_{n}$. The computational burden per iteration comes from three sources: proposal generation using a factor of the covariance, mean and covariance adaptation updates, and occasional refactorization of the covariance to obtain a new Cholesky factor. To derive a closed-form expression for the amortized per-iteration flop count using both periodic factorization and low-rank covariance approximation, we sum the contributions from each component.\n\nThe fundamental AM proposal in a full-rank setting is $Y_{n} = X_{n} + s L_{n} Z_{n}$ with $C_{n} = L_{n} L_{n}^{\\top}$ and $Z_{n} \\sim \\mathcal{N}(0, I_{d})$. The cost of multiplying a lower-triangular $d \\times d$ matrix by a $d$-vector is exactly $d^{2}$ flops (there are $d(d+1)/2$ multiplications and $d(d-1)/2$ additions), and scaling and adding $d$-vectors add $2 d$ flops, so the full-rank proposal sampling cost is $d^{2} + 2 d$ flops. The mean update $\\mu_{n+1} = \\mu_{n} + \\eta_{n} (X_{n} - \\mu_{n})$ is a vector operation requiring computation of $v_{n} = X_{n} - \\mu_{n}$ in $d$ flops and scaling plus addition in $2 d$ flops, totaling $3 d$ flops. The covariance update\n$$\nC_{n+1} = (1 - \\eta_{n}) C_{n} + \\eta_{n} \\left( v_{n} v_{n}^{\\top} \\right) + \\eta_{n} \\epsilon I_{d}\n$$\nrequires the following operations: compute the outer product $v_{n} v_{n}^{\\top}$ in $d^{2}$ flops, scale $C_{n}$ by $(1 - \\eta_{n})$ in $d^{2}$ flops, add the two $d \\times d$ matrices in $d^{2}$ flops, and add the diagonal regularization in $d$ flops. Together with the $3 d$ flops for the mean update, the total adaptation cost in the full-rank case is $3 d^{2} + 3 d$ flops per iteration. If one recomputes the Cholesky factorization $L_{n}$ every iteration, the cost per iteration would also include $\\frac{1}{3} d^{3}$ flops for the Cholesky factorization; however, with periodic factorization every $m$ iterations, the amortized Cholesky cost per iteration is $\\frac{1}{3} d^{3} / m$ flops.\n\nWe now analyze the low-rank strategy. The covariance is approximated by $C_{n} \\approx D_{n} + U_{n} U_{n}^{\\top}$ where $D_{n}$ is diagonal and $U_{n}$ has $r$ columns. The proposal uses two independent standard normals, $Z_{r} \\sim \\mathcal{N}(0, I_{r})$ and $Z_{d} \\sim \\mathcal{N}(0, I_{d})$, and computes\n$$\nY_{n} = X_{n} + s \\left( U_{n} Z_{r} + \\sqrt{D_{n}} \\odot Z_{d} \\right).\n$$\nThe cost for $U_{n} Z_{r}$, a dense $d \\times r$ matrix times an $r$-vector, is $2 d r$ flops; $\\sqrt{D_{n}} \\odot Z_{d}$ is a diagonal scaling costing $d$ flops; the three $d$-vector operations (addition of the two components, scaling by $s$, and addition to $X_{n}$) cost $3 d$ flops. Therefore, proposal generation under the low-rank approximation costs $2 d r + 4 d$ flops per iteration.\n\nFor the low-rank covariance update, we retain the mean update cost $3 d$ flops as before, with $v_{n} = X_{n} - \\mu_{n}$ computed in $d$ flops and scaling plus addition in $2 d$ flops. The diagonal $D_{n}$ captures the variance not represented in the low-rank part; updating $D_{n}$ with the diagonal of the rank-one term $\\eta_{n} v_{n} v_{n}^{\\top}$ requires elementwise operations on $d$ entries, modeled as $2 d$ flops (one for computing $v_{n} \\odot v_{n}$ and one for scaling and addition). Updating the factor $U_{n}$ to reflect the off-diagonal structure of the covariance can be done with an incremental rank-$r$ procedure (for example, a projection of $v_{n}$ onto the current span of $U_{n}$ followed by a correction and truncation), whose dominant costs are a $d \\times r$ by $r$-vector multiplication and associated small $r \\times r$ operations. We model this as $2 d r + r^{2}$ flops per iteration. Summing these pieces, the low-rank adaptation cost per iteration is\n$$\n\\text{adapt}_{\\mathrm{lr}}(d, r) = d + 2 d + 2 d + (2 d r + r^{2}) = 2 d r + r^{2} + 5 d.\n$$\n\nTo prevent drift from accumulating in the low-rank approximation, we still perform periodic full Cholesky factorizations of the current $C_{n}$ every $m$ iterations. The amortized cost of this periodic factorization is $\\frac{d^{3}}{3 m}$ flops per iteration.\n\nCombining the three components for the joint strategy—low-rank proposal generation, low-rank adaptation, and amortized periodic full Cholesky factorization—the total amortized per-iteration flop count is\n$$\nC_{\\mathrm{lr}}(d, m, r) = \\underbrace{(2 d r + 4 d)}_{\\text{proposal}} + \\underbrace{(2 d r + r^{2} + 5 d)}_{\\text{adaptation}} + \\underbrace{\\frac{d^{3}}{3 m}}_{\\text{amortized Cholesky}}.\n$$\nSimplifying,\n$$\nC_{\\mathrm{lr}}(d, m, r) = 4 d r + r^{2} + 9 d + \\frac{d^{3}}{3 m}.\n$$\n\nThis is the closed-form amortized per-iteration flop count under the stated flop model when using a rank-$r$ low-rank proposal and covariance update, together with periodic full Cholesky recomputation every $m$ iterations. It makes explicit the trade-offs: lowering $r$ reduces the $4 d r + r^{2}$ term, and increasing $m$ reduces the $\\frac{d^{3}}{3 m}$ term, while the linear term $9 d$ reflects unavoidable $d$-vector operations per iteration.",
            "answer": "$$\\boxed{4 d r + r^{2} + 9 d + \\frac{d^{3}}{3 m}}$$"
        }
    ]
}