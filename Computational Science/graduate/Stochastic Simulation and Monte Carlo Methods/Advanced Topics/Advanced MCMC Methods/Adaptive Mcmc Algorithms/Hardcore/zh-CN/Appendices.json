{
    "hands_on_practices": [
        {
            "introduction": "本练习深入探讨了许多自适应MCMC算法的理论核心，这些算法通常被构建为随机近似过程。通过分析一个简单的一维自适应Metropolis-Hastings算法，你将从第一性原理出发推导其行为。这项实践对于理解算法的更新规则如何与其长期收敛性质相关联至关重要，并展示了如何使用随机近似中心极限定理等工具来刻画采样器的性能。",
            "id": "3287318",
            "problem": "考虑一个一维自适应Metropolis–Hastings (AMH) 算法，其目标分布为标准正态密度$\\pi(x) \\propto \\exp(-x^{2}/2)$。提议分布是高斯随机游走，其尺度为$s_{t} = \\exp(\\theta_{t})$，即$X_{t+1} \\sim \\mathcal{N}(X_{t}, s_{t}^{2})$。令$A_{t+1} \\in \\{0,1\\}$表示在第$t+1$次迭代时的Metropolis–Hastings接受指示符。对数尺度参数的自适应由以下随机近似递归控制：\n$$\n\\theta_{t+1} \\;=\\; \\theta_{t} \\;+\\; \\gamma_{t}\\,\\big(A_{t+1} - \\alpha^{\\star}\\big), \n\\qquad \\gamma_{t} \\;=\\; \\frac{c}{t},\n$$\n其中$c \\in (0,\\infty)$为固定值，目标接受率为$\\alpha^{\\star} \\in (0,1)$。定义运行平均值$\\bar{\\theta}_{t} \\;=\\; t^{-1} \\sum_{k=1}^{t} \\theta_{k}$。假设在自适应马尔可夫链蒙特卡洛 (MCMC) 中使用了标准的时间尺度分离：自适应每$b_{t}$个MCMC步骤应用一次，其中$b_{t} \\to \\infty$足够快，并且链混合得足够快，因此当提议尺度等于$s$时，随机近似中使用的接受指示符是均值为$\\alpha(s)$的渐近独立伯努利随机变量。\n\n从第一性原理（Metropolis–Hastings接受规则、标准正态分布的性质以及随机近似常微分方程方法）出发，完成以下任务：\n\n1) 证明随机近似的平均场漂移为\n$$\nh(\\theta) \\;=\\; \\mathbb{E}\\big[A_{t+1} \\mid \\theta_{t}=\\theta\\big] \\;-\\; \\alpha^{\\star} \\;=\\; \\alpha\\big(\\exp(\\theta)\\big) \\;-\\; \\alpha^{\\star},\n$$\n其中$\\alpha(s)$是提议尺度为$s$的随机游走Metropolis算法的平稳期望接受概率。从第一性原理推导，对于标准正态目标和高斯随机游走提议，\n$$\n\\alpha(s) \\;=\\; \\frac{2}{\\pi}\\,\\arctan\\!\\Big(\\frac{2}{s}\\Big), \\qquad s \\in (0,\\infty).\n$$\n\n2) 令$\\theta^{\\star}$表示$h(\\theta)$的唯一根，并记$s^{\\star} = \\exp(\\theta^{\\star})$。证明$h'(\\theta^{\\star})  0$，并用$s^{\\star}$明确表示$h'(\\theta^{\\star})$。\n\n3) 使用Polyak–Ruppert平均中心极限定理进行随机近似，建立平均迭代值的渐近正态性，\n$$\n\\sqrt{t}\\,\\big(\\bar{\\theta}_{t} - \\theta^{\\star}\\big) \\;\\Rightarrow\\; \\mathcal{N}\\big(0,\\, V\\big),\n$$\n并将渐近方差$V$推导为仅与$\\alpha^{\\star}$相关的闭式解。\n\n4) 在典型的一维目标接受率$\\alpha^{\\star} = 0.44$下，计算得到的$V$值。将答案四舍五入至四位有效数字。最终答案必须是一个没有单位的实数。",
            "solution": "该问题要求对一维自适应Metropolis-Hastings (AMH) 算法进行多部分分析。我们将按顺序处理每个部分，并按要求从第一性原理出发。\n\n### 第1部分：平均场漂移和接受概率\n\n首先，我们确定平均场漂移函数$h(\\theta)$的形式。对数尺度参数$\\theta_t$的随机近似递归由下式给出：\n$$\n\\theta_{t+1} = \\theta_{t} + \\gamma_{t}(A_{t+1} - \\alpha^{\\star})。\n$$\n平均场漂移，记为$h(\\theta_t)$，是$\\theta_t$在给定当前状态$\\theta_t$下，每单位步长$\\gamma_t$的期望变化量。形式上，\n$$\nh(\\theta_t) = \\mathbb{E}\\left[\\frac{\\theta_{t+1} - \\theta_t}{\\gamma_t} \\bigg| \\theta_t\\right] = \\mathbb{E}[A_{t+1} - \\alpha^{\\star} \\mid \\theta_t]。\n$$\n使用重期望定律和链混合足够快的假设，第$t+1$步的接受概率$A_{t+1}$仅取决于用于生成提议的提议尺度$s_t = \\exp(\\theta_t)$。令$\\theta_t = \\theta$。则$s_t=s=\\exp(\\theta)$。我们有\n$$\n\\mathbb{E}[A_{t+1} \\mid \\theta_t=\\theta] = \\alpha(s) = \\alpha(\\exp(\\theta))，\n$$\n其中$\\alpha(s)$是提议尺度为$s$的Metropolis-Hastings算法的平稳期望接受概率。因此，平均场漂移为\n$$\nh(\\theta) = \\alpha(\\exp(\\theta)) - \\alpha^{\\star}。\n$$\n\n接下来，我们推导$\\alpha(s)$的表达式，其目标密度为标准正态$\\pi(x) \\propto \\exp(-x^2/2)$，提议为高斯随机游走$Y \\sim \\mathcal{N}(x, s^2)$。提议密度$q(y|x)$是对称的，即$q(y|x) = q(x|y)$。从$x$移动到$y$的Metropolis-Hastings接受概率为\n$$\na(x,y) = \\min\\left(1, \\frac{\\pi(y)}{\\pi(x)}\\right) = \\min\\left(1, \\frac{\\exp(-y^2/2)}{\\exp(-x^2/2)}\\right) = \\min\\left(1, \\exp\\left(-\\frac{y^2-x^2}{2}\\right)\\right)。\n$$\n平稳期望接受概率$\\alpha(s)$是$a(X,Y)$的期望，其中$X \\sim \\pi$且$Y|X=x \\sim q(\\cdot|x)$。所以$X \\sim \\mathcal{N}(0,1)$，我们可以写成$Y = X + \\epsilon$，其中$\\epsilon \\sim \\mathcal{N}(0, s^2)$且与$X$独立。\n接受概率成为$X$和$\\epsilon$的函数：\n$$\na(X, \\epsilon) = \\min\\left(1, \\exp\\left(-\\frac{(X+\\epsilon)^2-X^2}{2}\\right)\\right) = \\min\\left(1, \\exp\\left(-\\frac{2X\\epsilon+\\epsilon^2}{2}\\right)\\right)。\n$$\n我们通过先对$\\epsilon$取条件来计算$\\alpha(s) = \\mathbb{E}_{X,\\epsilon}[a(X, \\epsilon)]$：\n$$\n\\alpha(s) = \\mathbb{E}_{\\epsilon}\\left[ \\mathbb{E}_{X| \\epsilon} \\left[ \\min\\left(1, \\exp\\left(-X\\epsilon - \\frac{\\epsilon^2}{2}\\right)\\right) \\right] \\right]。\n$$\n我们来分析内部的期望。当$-X\\epsilon - \\epsilon^2/2 > 0$时，比值大于1，这等价于$X\\epsilon  -\\epsilon^2/2$。\n情况1：$\\epsilon > 0$。条件是$X  -\\epsilon/2$。\n情况2：$\\epsilon  0$。条件是$X > -\\epsilon/2$。\n令$\\Phi(\\cdot)$为标准正态分布的累积分布函数(CDF)。我们来计算条件期望$f(\\epsilon) = \\mathbb{E}_{X}[a(X,\\epsilon)]$。\n对于$\\epsilon > 0$：\n$$\nf(\\epsilon) = \\int_{-\\infty}^{-\\epsilon/2} 1 \\cdot \\phi(x) \\, dx + \\int_{-\\epsilon/2}^{\\infty} \\exp\\left(-x\\epsilon - \\frac{\\epsilon^2}{2}\\right) \\phi(x) \\, dx,\n$$\n其中$\\phi(x) = (2\\pi)^{-1/2}\\exp(-x^2/2)$。第一个积分是$\\Phi(-\\epsilon/2)$。第二个积分是\n$$\n\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\epsilon/2}^{\\infty} \\exp\\left(-x\\epsilon - \\frac{\\epsilon^2}{2} - \\frac{x^2}{2}\\right) \\, dx = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\epsilon/2}^{\\infty} \\exp\\left(-\\frac{(x+\\epsilon)^2}{2}\\right) \\, dx。\n$$\n通过换元$u = x+\\epsilon$，积分下限变为$-\\epsilon/2+\\epsilon = \\epsilon/2$。该积分为$\\int_{\\epsilon/2}^{\\infty} \\phi(u) \\, du = 1-\\Phi(\\epsilon/2) = \\Phi(-\\epsilon/2)$。\n所以，对于$\\epsilon > 0$，$f(\\epsilon) = \\Phi(-\\epsilon/2) + \\Phi(-\\epsilon/2) = 2\\Phi(-\\epsilon/2) = 2\\Phi(-|\\epsilon|/2)$。\n对$\\epsilon  0$进行类似的计算，同样得到$f(\\epsilon) = 2\\Phi(\\epsilon/2) = 2\\Phi(-|\\epsilon|/2)$。\n因此，对于任何$\\epsilon \\neq 0$，我们有$\\mathbb{E}_{X}[a(X,\\epsilon)] = 2\\Phi(-|\\epsilon|/2)$。\n现在我们对$\\epsilon \\sim \\mathcal{N}(0,s^2)$取期望。令$\\epsilon=sZ$，其中$Z \\sim \\mathcal{N}(0,1)$。\n$$\n\\alpha(s) = \\mathbb{E}_{Z}\\left[ 2\\Phi\\left(-\\frac{s|Z|}{2}\\right) \\right] = 2 \\int_{-\\infty}^{\\infty} \\Phi\\left(-\\frac{s|z|}{2}\\right)\\phi(z) \\, dz = 4 \\int_{0}^{\\infty} \\Phi\\left(-\\frac{sz}{2}\\right)\\phi(z) \\, dz。\n$$\n该积分可以如下计算：\n$$\nI = \\int_{0}^{\\infty} \\Phi\\left(-\\frac{sz}{2}\\right)\\phi(z) \\, dz = \\int_0^\\infty \\left( \\int_{-\\infty}^{-sz/2} \\phi(u) \\, du \\right) \\phi(z) \\, dz = \\frac{1}{2\\pi} \\iint_D \\exp\\left(-\\frac{u^2+z^2}{2}\\right) \\, du \\, dz,\n$$\n其中积分区域为$D = \\{(u,z) : z > 0, u  -sz/2\\}$。这是$(u,z)$平面上的一个扇形区域。我们转换为极坐标$(r, \\theta)$，其中$z=r\\sin\\theta, u=r\\cos\\theta$。\n条件$z > 0$意味着$\\theta \\in (0, \\pi)$。条件$u  -sz/2$变为$r\\cos\\theta  -s/2 \\cdot r\\sin\\theta$，简化为$\\cot\\theta  -s/2$（因为在此范围内$\\sin\\theta>0$）。arccotangent函数将$(-\\infty,0)$映射到$(\\pi/2, \\pi)$，所以对$\\theta$的条件是$\\text{arccot}(-s/2)  \\theta  \\pi$。\n使用恒等式$\\text{arccot}(-x) = \\pi - \\text{arccot}(x) = \\pi - (\\pi/2 - \\arctan(x)) = \\pi/2 + \\arctan(x)$（对于$x > 0$），$\\theta$的范围是$(\\pi/2 + \\arctan(s/2), \\pi)$。\n极坐标下的积分为：\n$$\nI = \\frac{1}{2\\pi} \\int_{\\pi/2 + \\arctan(s/2)}^{\\pi} \\int_0^\\infty r\\exp(-r^2/2) \\, dr \\, d\\theta。\n$$\n内部积分是$\\int_0^\\infty r e^{-r^2/2} \\, dr = [-e^{-r^2/2}]_0^\\infty = 1$。\n$$\nI = \\frac{1}{2\\pi} \\left[ \\pi - \\left(\\frac{\\pi}{2} + \\arctan\\left(\\frac{s}{2}\\right)\\right) \\right] = \\frac{1}{2\\pi} \\left( \\frac{\\pi}{2} - \\arctan\\left(\\frac{s}{2}\\right) \\right)。\n$$\n使用恒等式$\\arctan(x) + \\arctan(1/x) = \\pi/2$（对于$x>0$），这简化为$I = \\frac{1}{2\\pi} \\arctan(2/s)$。\n最后，$\\alpha(s) = 4I = 4 \\left( \\frac{1}{2\\pi} \\arctan(2/s) \\right) = \\frac{2}{\\pi} \\arctan(2/s)$。\n\n### 第2部分：稳定性分析\n\n$h(\\theta)$的根$\\theta^\\star$满足$h(\\theta^\\star) = \\alpha(\\exp(\\theta^\\star)) - \\alpha^\\star = 0$。令$s^\\star = \\exp(\\theta^\\star)$。则$\\alpha(s^\\star) = \\alpha^\\star$。\n我们需要确定$h'(\\theta^\\star)$的符号。使用链式法则：\n$$\nh'(\\theta) = \\frac{d}{d\\theta} \\alpha(\\exp(\\theta)) = \\alpha'(\\exp(\\theta)) \\cdot \\exp(\\theta)。\n$$\n在根$\\theta^\\star$处，我们有$h'(\\theta^\\star) = \\alpha'(s^\\star) \\cdot s^\\star$。\n我们首先计算$\\alpha(s)$的导数：\n$$\n\\alpha'(s) = \\frac{d}{ds} \\left( \\frac{2}{\\pi} \\arctan\\left(\\frac{2}{s}\\right) \\right) = \\frac{2}{\\pi} \\cdot \\frac{1}{1+(2/s)^2} \\cdot \\left(-\\frac{2}{s^2}\\right) = \\frac{2}{\\pi} \\cdot \\frac{s^2}{s^2+4} \\cdot \\left(-\\frac{2}{s^2}\\right) = -\\frac{4}{\\pi(s^2+4)}。\n$$\n由于$s \\in (0, \\infty)$，我们有$s^2+4>0$，所以对所有$s$，$\\alpha'(s)  0$。\n这意味着$h'(\\theta^\\star) = \\alpha'(s^\\star) s^\\star  0$，这证实了该平衡点是稳定的。\n$h'(\\theta^\\star)$用$s^\\star$表示的显式表达式为：\n$$\nh'(\\theta^\\star) = s^\\star \\cdot \\left( -\\frac{4}{\\pi((s^\\star)^2+4)} \\right) = -\\frac{4s^\\star}{\\pi((s^\\star)^2+4)}。\n$$\n\n### 第3部分：渐近方差\n\nPolyak-Ruppert平均中心极限定理指出，在某些条件下（包括第2部分中证明的稳定性以及对步长常数$c$的条件），平均迭代值$\\bar{\\theta}_t = t^{-1}\\sum_{k=1}^t \\theta_k$是渐近正态的：\n$$\n\\sqrt{t}(\\bar{\\theta}_t - \\theta^\\star) \\Rightarrow \\mathcal{N}(0, V)。\n$$\n渐近方差$V$由公式$V = \\frac{\\Sigma}{(h'(\\theta^\\star))^2}$给出，其中$\\Sigma$是在平衡状态下驱动随机近似的噪声过程的方差。\n递归可以写成$\\theta_{t+1} = \\theta_t + \\gamma_t(h(\\theta_t) + M_{t+1})$，其中$M_{t+1} = A_{t+1} - \\mathbb{E}[A_{t+1}|\\theta_t]$是噪声项。在平衡状态（$\\theta_t = \\theta^\\star$），接受指示符$A_{t+1}$服从成功概率为$\\mathbb{E}[A_{t+1}|\\theta^\\star] = \\alpha(s^\\star) = \\alpha^\\star$的伯努利分布。\n伯努利($p$)随机变量的方差是$p(1-p)$。因此，平衡状态下的噪声方差是\n$$\n\\Sigma = \\text{Var}(A_{t+1}|\\theta_t=\\theta^\\star) = \\alpha^\\star(1-\\alpha^\\star)。\n$$\n为了将$V$表示为仅关于$\\alpha^\\star$的函数，我们必须用$\\alpha^\\star$来表示$h'(\\theta^\\star)$。从$\\alpha^\\star = \\frac{2}{\\pi}\\arctan(2/s^\\star)$，我们有$\\frac{\\pi\\alpha^\\star}{2} = \\arctan(2/s^\\star)$。对两边取正切，得到$\\tan(\\frac{\\pi\\alpha^\\star}{2}) = \\frac{2}{s^\\star}$，这意味着$s^\\star = \\frac{2}{\\tan(\\frac{\\pi\\alpha^\\star}{2})}$。\n将此代入$h'(\\theta^\\star)$的表达式：\n$$\nh'(\\theta^\\star) = -\\frac{4s^\\star}{\\pi((s^\\star)^2+4)} = -\\frac{4 \\left(\\frac{2}{\\tan(\\frac{\\pi\\alpha^\\star}{2})}\\right)}{\\pi \\left( \\left(\\frac{2}{\\tan(\\frac{\\pi\\alpha^\\star}{2})}\\right)^2 + 4 \\right)} = -\\frac{\\frac{8}{\\tan(\\frac{\\pi\\alpha^\\star}{2})}}{\\pi \\left( \\frac{4}{\\tan^2(\\frac{\\pi\\alpha^\\star}{2})} + 4 \\right)} = -\\frac{8}{\\pi \\tan(\\frac{\\pi\\alpha^\\star}{2})} \\frac{\\tan^2(\\frac{\\pi\\alpha^\\star}{2})}{4(1+\\tan^2(\\frac{\\pi\\alpha^\\star}{2}))}。\n$$\n使用恒等式$1+\\tan^2(x) = \\sec^2(x)$，这可以简化为：\n$$\nh'(\\theta^\\star) = -\\frac{2\\tan(\\frac{\\pi\\alpha^\\star}{2})}{\\pi \\sec^2(\\frac{\\pi\\alpha^\\star}{2})} = -\\frac{2}{\\pi} \\frac{\\sin(\\frac{\\pi\\alpha^\\star}{2})/\\cos(\\frac{\\pi\\alpha^\\star}{2})}{1/\\cos^2(\\frac{\\pi\\alpha^\\star}{2})} = -\\frac{2}{\\pi}\\sin\\left(\\frac{\\pi\\alpha^\\star}{2}\\right)\\cos\\left(\\frac{\\pi\\alpha^\\star}{2}\\right)。\n$$\n使用二倍角恒等式$2\\sin(x)\\cos(x) = \\sin(2x)$，我们得到了一个非常简洁的表达式：\n$$\nh'(\\theta^\\star) = -\\frac{1}{\\pi}\\sin\\left(2 \\cdot \\frac{\\pi\\alpha^\\star}{2}\\right) = -\\frac{\\sin(\\pi\\alpha^\\star)}{\\pi}。\n$$\n现在我们组合得到渐近方差$V$：\n$$\nV = \\frac{\\Sigma}{(h'(\\theta^\\star))^2} = \\frac{\\alpha^\\star(1-\\alpha^\\star)}{\\left(-\\frac{\\sin(\\pi\\alpha^\\star)}{\\pi}\\right)^2} = \\frac{\\pi^2 \\alpha^\\star(1-\\alpha^\\star)}{\\sin^2(\\pi\\alpha^\\star)}。\n$$\n\n### 第4部分：数值计算\n\n我们被要求在目标接受率$\\alpha^\\star = 0.44$下计算$V$。\n将这个值代入我们推导的$V$的公式中：\n$$\nV = \\frac{\\pi^2 (0.44)(1-0.44)}{\\sin^2(0.44\\pi)} = \\frac{\\pi^2 (0.44)(0.56)}{\\sin^2(0.44\\pi)} = \\frac{0.2464 \\pi^2}{\\sin^2(0.44\\pi)}。\n$$\n我们进行数值计算：\n正弦函数的参数是$0.44\\pi$弧度。\n$0.44\\pi \\approx 1.382300767$\n$\\sin(0.44\\pi) \\approx 0.98223604$\n$\\sin^2(0.44\\pi) \\approx 0.96478761$\n分子是$0.2464 \\pi^2 \\approx 0.2464 \\times 9.8696044 \\approx 2.4318705$。\n因此，方差为\n$$\nV \\approx \\frac{2.4318705}{0.96478761} \\approx 2.520653。\n$$\n四舍五入到四位有效数字，我们得到$2.521$。",
            "answer": "$$\\boxed{2.521}$$"
        },
        {
            "introduction": "从纯理论转向实际应用，本问题探讨了多维自适应MCMC中的一个关键挑战。理论模型假设使用精确算术，但现实世界中的算法必须处理浮点误差，这可能导致自适应协方差矩阵失去其关键的正定性。这项练习要求你像一名科学软件开发者一样思考，评估和设计稳健的数值检查与修正策略，以确保算法的稳定性和可靠性。",
            "id": "3353669",
            "problem": "考虑 Haario–Saksman–Tamminen (HST) 的自适应 Metropolis 算法，该算法在马尔可夫链的迭代 $n$ 过程中更新一个提议协方差矩阵 $\\Sigma_n$，以适应目标分布。在精确算术中，我们希望 $\\Sigma_n$ 是对称正定的，这样协方差为 $\\Sigma_n$ 的高斯提议才是良定义的，并且 Cholesky 分解也存在。在浮点运算中，舍入误差、小更新的累积和病态问题可能导致正定性的丧失或在形成提议时的数值失败。设计一个数值稳健的测试来检测 $\\Sigma_n$ 正定性的丧失，并指定使 $\\Sigma_n$ 变得适用的纠正措施，例如增加对角正则化项 $\\epsilon$ 或在需要时重新初始化分解。\n\n使用以下基本事实作为出发点，并用它们来证明你的选择：\n- 一个实对称矩阵 $A$ 是正定的，当且仅当其所有特征值都严格为正，这等价于存在一个对角项为正的 Cholesky 分解 $A = LL^\\top$。\n- 加上一个单位矩阵的倍数，即用 $A + \\epsilon I$（其中 $\\epsilon  0$）替换 $A$，会将其所有特征值向上平移 $\\epsilon$ 并保持对称性。\n- 在高维情况下，仅凭行列式不是一个可靠的正定性数值检验方法，并且对于一般矩阵，不带主元的高斯消元法是数值不稳定的。\n- Gershgorin 圆盘定理根据行和提供了特征值的界，但这个界可能比较保守，并且必须小心地应用于对称矩阵。\n- 在自适应算法中，反复的数值检查失败应该触发对提议的安全重新初始化，例如重置为一个缩放后的单位协方差，以确保稳定性。\n\n在 HST 自适应过程中，以下哪个程序是检测和纠正 $\\Sigma_n$ 正定性丧失的数值稳健方法？\n\nA. 在每次更新时，构造对称部分 $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$，并尝试对 $S_n$ 进行 Cholesky 分解。如果分解因非正主元而失败或返回非数值，则通过将 $S_n$ 替换为 $S_n + \\epsilon I$ 来增加对角正则化并重试，使用自适应策略 $\\epsilon \\leftarrow \\max\\{\\epsilon_{\\min}, \\gamma \\epsilon\\}$（其中 $\\gamma  1$），直到分解成功。如果连续 $k$ 次尝试均失败，则将协方差重新初始化为 $c I$（其中 $c  0$ 为某个尺度），并重建分解。使用得到的因子来定义提议。\n\nB. 通过不带主元的 LU 分解计算行列式 $\\det(\\Sigma_n)$。如果 $\\det(\\Sigma_n)  0$，则断定 $\\Sigma_n$ 是正定的并继续。如果 $\\det(\\Sigma_n) \\le 0$，则将 $\\epsilon$ 减小至趋于 $0$ 并重试 LU 分解，直到 $\\det(\\Sigma_n)  0$，然后不经重新初始化继续。\n\nC. 使用对称特征求解器（例如，Householder 三角化后接 QR 算法或 Lanczos 迭代）估计对称部分 $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$ 的最小特征值 $\\lambda_{\\min}$。如果对于一个小的阈值 $\\tau  0$，有 $\\lambda_{\\min}  \\tau$，则通过以下任一方式进行纠正：(i) 设置 $\\Sigma_n \\leftarrow S_n + \\epsilon I$，其中 $\\epsilon \\ge \\tau - \\lambda_{\\min}$；或 (ii) 通过将负的或过小的特征值替换为 $\\tau$ 并从其特征分解重构 $S_n$ 来投影谱，然后计算 Cholesky 因子来定义提议。如果重复的纠正超出预算，则重新初始化为 $c I$。\n\nD. 对 $\\Sigma_n$ 的行应用 Gershgorin 圆盘定理。如果任何 Gershgorin 圆盘与负实轴相交，则宣布正定性丧失，并通过减去一个对角项进行纠正，即设置 $\\Sigma_n \\leftarrow \\Sigma_n - \\epsilon I$（其中 $\\epsilon  0$），以使圆盘远离负轴。不经重新初始化继续。\n\nE. 当怀疑正定性丧失时，向 $\\Sigma_n$ 添加一个随机的秩一矩阵 $uu^\\top$（其中 $u$ 从一个超立方体中均匀抽取），并尝试对现有因子进行 Cholesky 降阶（downdate）。用新的 $u$ 重复此过程，直到降阶成功，然后使用更新后的分解作为提议，无需进一步检查。\n\n选择所有正确的选项。",
            "solution": "问题陈述要求我们为 Haario-Saksman-Tamminen (HST) Metropolis 算法中使用的自适应协方差矩阵 $\\Sigma_n$ 识别出用于检测和纠正正定性丧失的数值稳健程序。\n\n**问题验证**\n\n首先，我将验证问题陈述本身。\n\n**步骤 1：提取已知条件**\n-   **算法**：Haario–Saksman–Tamminen (HST) 自适应 Metropolis 算法。\n-   **矩阵**：在迭代 $n$ 时的提议协方差矩阵 $\\Sigma_n$。\n-   **期望性质**：$\\Sigma_n$ 应该是对称正定 (SPD)。\n-   **数值问题**：浮点运算可能导致正定性的丧失。\n-   **任务**：识别一个数值稳健的程序来测试和纠正这种丧失。\n-   **提供的基本事实**：\n    1.  一个实对称矩阵 $A$ 是对称正定 $\\iff$ 所有特征值都严格为正 $\\iff$ 存在一个对角项为正的 Cholesky 分解 $A = LL^\\top$。\n    2.  对于一个对称矩阵 $A$，矩阵 $A + \\epsilon I$（其中 $\\epsilon  0$）的特征值向上平移了 $\\epsilon$ 并且保持对称。\n    3.  行列式不是一个可靠的正定性数值检验。不带主元的高斯消元法是数值不稳定的。\n    4.  Gershgorin 圆盘定理给出了可能比较保守的特征值界。\n    5.  自适应算法中持续的数值失败需要对提议机制进行安全重新初始化。\n\n**步骤 2：使用提取的已知条件进行验证**\n-   **科学依据**：该问题在计算统计学和数值线性代数领域有充分的依据。HST 算法是自适应 MCMC 中的一个经典方法，而数值更新的协方差矩阵丧失正定性是一个真实且实际的问题。所提供的基本事实都是标准的、正确的定理和原则。\n-   **良构性**：该问题是良构的。它要求基于一组给定的基本原则，根据数值稳健性的标准来评估一系列提议的计算程序。通过逻辑推导可以得出一个确定的答案。\n-   **客观性**：该问题以客观、技术性的语言陈述，没有歧义或主观看法。\n\n**步骤 3：结论和行动**\n问题陈述是有效的。它科学合理、良构且客观。我现在将开始分析每个选项。\n\n**推导和逐项分析**\n\n核心任务是维持协方差矩阵 $\\Sigma_n$ 的对称正定性。一个稳健的程序必须包含三个部分：一个可靠的正定性检验，一个有效的纠正机制，以及一个用于稳定性的故障安全措施。由于浮点误差，$\\Sigma_n$ 可能会变得轻微不对称，因此通过取 $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$ 来强制对称化是一个标准且合理的预备步骤。\n\n**选项 A 分析**\n该选项提出了一个包含以下步骤的程序：\n1.  **对称化**：构造 $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$。这正确地处理了由浮点更新引起的任何微小的不对称性。\n2.  **检测**：尝试对 $S_n$ 进行 Cholesky 分解。如事实 1 所述，Cholesky 分解的存在性等价于矩阵是对称正定的。在计算上，尝试进行分解是测试此性质的标准、最有效且数值可靠的方法。如果 Cholesky 分解算法（例如，涉及计算 $\\sqrt{a_{kk} - \\sum_{j=1}^{k-1} L_{kj}^2}$）在平方根内遇到非正项，则该矩阵不是正定的。这是一个直接且决定性的检验。\n3.  **纠正**：如果分解失败，用 $S_n + \\epsilon I$ 替换 $S_n$，其中 $\\epsilon  0$。事实 2 证实了此操作会将 $S_n$ 的所有特征值向上平移 $\\epsilon$。对于足够大的 $\\epsilon$，所有特征值都将变为正值，从而使矩阵成为正定矩阵。这是一种称为对角加载或 Tikhonov 正则化的标准技术。\n4.  **自适应与稳定性**：该程序建议对 $\\epsilon$ 采用自适应策略（$\\epsilon \\leftarrow \\max\\{\\epsilon_{\\min}, \\gamma \\epsilon\\}$，其中 $\\gamma  1$），并在纠正重复失败时重新初始化为 $cI$。这确保了纠正是最小侵入性的，但在需要时会变得更强，并且回退机制保证了整个算法的稳定性，完全符合事实 5。\n\n该程序结合了可靠的检验、标准的纠正和稳定性机制，所有这些都与所提供的基本事实和数值线性代数的最佳实践相一致。\n**结论：正确**\n\n**选项 B 分析**\n该选项建议：\n1.  **检测**：计算 $\\det(\\Sigma_n)$ 并检查其是否为正。事实 3 明确且正确地指出，行列式不是一个可靠的检验。一个对称矩阵可以有正的行列式但不是正定的（例如，一个具有偶数个负项的对角矩阵）。例如，如果 $\\Sigma_n = \\text{diag}(-1, -1, 4)$，其行列式为 $4  0$，但它不是正定的。\n2.  **实现细节**：它建议使用不带主元的 LU 分解。事实 3 警告说，这对于一般矩阵是数值不稳定的。由于我们怀疑 $\\Sigma_n$ 可能不是对称正定矩阵，我们不能假设它属于无主元 LU 分解稳定的那类矩阵。\n3.  **纠正**：它建议如果 $\\det(\\Sigma_n) \\le 0$，则将 $\\epsilon$ 减小至趋于 $0$。这与正确的操作相反。要通过对角加载使矩阵正定，必须向对角线*添加*一个正值，这对应于在表达式 $S_n + \\epsilon I$ 中*增加* $\\epsilon$。\n4.  **稳定性**：它建议不经重新初始化继续，这与事实 5 中概述的确保稳定性的原则相矛盾。\n\n该程序在每个方面都存在缺陷：检验方法错误，纠正措施适得其反，并且缺乏稳定性保证。\n**结论：不正确**\n\n**选项 C 分析**\n该选项建议：\n1.  **对称化**：构造 $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$。正确。\n2.  **检测**：估计 $S_n$ 的最小特征值 $\\lambda_{\\min}$。根据事实 1，矩阵是正定的当且仅当其所有特征值都为正。因此，检查 $\\lambda_{\\min}$ 是否为正（或为数值安全起见大于一个小的容差 $\\tau  0$）是一个直接且理论上完美的检验。虽然计算上比 Cholesky 分解更昂贵，但它是一种完全有效且稳健的检测方法。\n3.  **纠正**：它提供了两种有效的纠正策略。\n    (i) 设置 $\\Sigma_n \\leftarrow S_n + \\epsilon I$，其中 $\\epsilon \\ge \\tau - \\lambda_{\\min}$。这是事实 2 的一个有针对性的应用。新矩阵的最小特征值将是 $\\lambda_{\\min} + \\epsilon$。选择 $\\epsilon \\ge \\tau - \\lambda_{\\min}$ 保证了新的最小特征值至少为 $\\tau$，从而使矩阵稳健地成为正定矩阵。\n    (ii) 通过将小于 $\\tau$ 的特征值替换为 $\\tau$ 并重构矩阵来投影谱。这是寻找一个邻近的正半定（或正定）矩阵的著名方法，是一种有原则的纠正方式。\n4.  **稳定性**：如果纠正需要过于频繁，它包括一个重新初始化步骤（$cI$），这遵循了事实 5。\n\n该程序在理论上是合理的，在数值上是稳健的。相对于选项 A，其较高的计算成本是一个实践上的权衡，而不是其稳健性的缺陷。\n**结论：正确**\n\n**选项 D 分析**\n该选项建议：\n1.  **检测**：使用 Gershgorin 圆盘定理。事实 4 正确地指出该定理提供的界可能比较保守。一个 Gershgorin 圆盘与负实轴相交并不证明某个特征值为负；它只表明存在这种可能性。这可能导致不必要的纠正（假阳性），使得该检验不是最优的。\n2.  **纠正**：它建议设置 $\\Sigma_n \\leftarrow \\Sigma_n - \\epsilon I$。此操作*减去*一个单位矩阵的正倍数，这将所有特征值*向下*平移 $\\epsilon$。这会使一个非正定的矩阵更加非正定，甚至可能使一个正定矩阵变为非正定。这在根本上是错误的。\n3.  **稳定性**：它缺乏重新初始化机制，违反了事实 5。\n\n提议的纠正措施是灾难性错误的。\n**结论：不正确**\n\n**选项 E 分析**\n该选项建议：\n1.  **纠正**：添加一个随机的秩一矩阵 $uu^\\top$。虽然添加一个像 $uu^\\top$ 这样的正半定矩阵不会减少任何特征值，但这并不是一种有针对性或有保证的修复负特征值的方法。这是一种启发式方法，而不是一个稳健、可控的程序。不清楚需要多少次随机尝试，或者对于一个有多个或大幅度负特征值的矩阵是否会成功。\n2.  **机制**：它提到了“Cholesky 降阶（downdate）”。降阶对应于减去一个秩一矩阵，即从 $A$ 的因子求出 $A - uu^\\top$ 的 Cholesky 因子。而该程序描述的是*添加* $uu^\\top$，对此应该使用 Cholesky *更新*。Cholesky 降阶在数值上已知不如更新稳定。这表明要么是一个术语错误，要么是对数值方法的误解。\n3.  **稳定性**：该程序依赖于重复的随机试验，没有最终的故障安全措施，如重新初始化到一个已知的良好状态，这违反了事实 5。\n\n这个程序不稳健；它基于一种随机启发式方法，并且在数值实现方面使用了有问题的术语。\n**结论：不正确**\n\n**结论**\n选项 A 和 C 都描述了确保提议协方差矩阵保持正定的有效且数值稳健的程序。选项 A 基于计算效率高的 Cholesky 分解检验。选项 C 基于计算成本更高但诊断能力更强的特征值分析。两者都采用了正确且标准的纠正和稳定性原则。因此，它们都是正确答案。",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "这最后一项实践是一个综合性练习，将理论概念与动手编程及实证分析相结合。你将实现一个自适应随机游走Metropolis采样器，并系统地研究适应速率（由参数 $\\alpha$ 控制）如何影响采样器效率（通过有效样本量ESS来衡量）。通过在具有不同特性的目标分布上测试你的实现，你将获得关于在满足“递减适应性”等理论条件与在有限时间模拟中实现最佳性能之间进行权衡的宝贵实践经验。",
            "id": "3287303",
            "problem": "考虑马尔可夫链蒙特卡洛（MCMC）中的随机游走Metropolis（RWM）算法。设实数线上的目标密度为一个对数可积的概率密度函数 $p(x)$，并考虑形式为 $x^{\\prime} = x + \\eta$ 的提议，其中 $\\eta \\sim \\mathcal{N}(0,\\sigma_n^2)$，其尺度 $\\sigma_n$ 随时间变化。在第 $n$ 步的 RWM 接受概率由 Metropolis–Hastings 准则给出：$a(x,x^{\\prime}) = \\min\\{1, p(x^{\\prime})/p(x)\\}$。我们研究形式为 $\\sigma_n = c \\, n^{-\\alpha}$ 的自适应方案，其中 $c \\gt 0$ 且 $\\alpha \\gt 0$。自适应 MCMC 中的适应性递减条件要求连续转移核之间的差异收敛于零，对于 RWM 而言，这可以通过分析 $|\\sigma_{n+1} - \\sigma_n|$ 是否收敛于零来进行。\n\n在有限的计算预算下，将近似最优效率定义为最大化估计器的有效样本量（ESS），其中 ESS 与积分自相关时间（IACT）成反比。对于状态的标量函数 $f(x)$，给定样本 $\\{f(X_i)\\}_{i=1}^N$，IACT 定义为 $\\tau = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k$，其中 $\\rho_k$ 是平稳序列 $\\{f(X_i)\\}$ 的滞后k阶自相关。ESS 为 $N / \\tau$。在固定的迭代预算下，更高的 ESS 表示更好的近似最优效率。\n\n您的任务是，在一个给定的有限集合中，确定哪个 $\\alpha$ 能够近似最大化效率，同时满足适应性递减的数值检验，并测试其对目标平滑度的敏感性。请基于以下基础进行工作：\n- 用于构建具有平稳密度 $p(x)$ 的可逆马尔可夫链的 Metropolis–Hastings 框架。\n- 自适应 MCMC 的适应性递减定义，即适应幅度趋于零，此处近似为 $|\\sigma_{n+1} - \\sigma_n| \\to 0$。\n- IACT 与 ESS 之间的关系，ESS $= N/\\tau$，其中 $\\tau$ 是根据样本自相关估计的。\n- 在适当的遍历性条件下，MCMC 平均的标准大数定律，这为在固定有限范围内基于 ESS 的比较提供了依据。\n\n您必须完全通过代码实现以下实验：\n\n1) 捕捉不同平滑度机制的目标：\n- 平滑、强对数凹：标准正态目标，密度为 $p_{\\mathrm{G}}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\{-x^2/2\\}$。\n- 众数处对数密度不平滑：拉普拉斯目标，密度为 $p_{\\mathrm{L}}(x) = \\frac{1}{2}\\exp\\{-|x|\\}$。\n- 平滑但重尾：自由度为 $\\nu = 3$ 的学生t分布，$p_{\\mathrm{T}}(x) = \\frac{\\Gamma((\\nu+1)/2)}{\\sqrt{\\nu\\pi}\\,\\Gamma(\\nu/2)}\\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\nu+1)/2}$，其中 $\\nu = 3$。\n\n2) 所有目标共享的算法设置：\n- 提议尺度方案 $\\sigma_n = c \\, n^{-\\alpha}$，其中 $c = 1.5$，$\\alpha$ 从有限集合 $\\{0.2, 0.5, 0.8\\}$ 中选择。\n- 总迭代次数 $T = 20000$，老化期 $B = 2000$。\n- 初始状态 $x_0 = 0$。\n- 使用给定的带有对称高斯提议的 RWM 算法。\n- 为评估效率，使用函数 $f(x) = x$。\n- 使用基于经验自相关序列 $\\{\\rho_k\\}_{k \\ge 1}$ 的初始正序列规则的一致性滞后窗方法来估计积分自相关时间 $\\tau$。\n- 计算 ESS 为 $N/\\tau$，其中 $N = T - B$。\n\n3) 适应性递减的数值验证：\n- 对于给定的 $\\alpha$，计算终端适应幅度 $\\Delta_T = |\\sigma_{T+1} - \\sigma_T|$，并要求 $\\Delta_T \\lt \\epsilon$，其中 $\\epsilon = 10^{-5}$。对于该目标，只有满足此检查的 $\\alpha$ 才有资格被选择。\n\n4) 选择标准：\n- 对于每个目标，在符合条件的 $\\alpha$ 中，选择使 ESS 最大化的 $\\alpha$。如果在 ESS 上出现数值上的平局（相对容差在 $10^{-3}$ 以内），则在并列的值中选择最小的 $\\alpha$。\n\n5) 测试套件：\n- 目标集：$\\{p_{\\mathrm{G}}, p_{\\mathrm{L}}, p_{\\mathrm{T}}\\}$，按上述顺序。\n- 候选 $\\alpha$ 集：$\\{0.2, 0.5, 0.8\\}$。\n- 超参数：$c = 1.5$, $T = 20000$, $B = 2000$, $\\epsilon = 10^{-5}$, $x_0 = 0$。\n\n6) 最终输出格式：\n- 您的程序应生成单行输出，其中包含为每个目标选择的 $\\alpha$ 值，按 $\\{p_{\\mathrm{G}}, p_{\\mathrm{L}}, p_{\\mathrm{T}}\\}$ 的顺序排列，以逗号分隔并用方括号括起来，每个 $\\alpha$ 值四舍五入到一位小数，例如 $[0.2,0.5,0.2]$。\n- 此问题不涉及物理单位，也不使用角度。\n\n您的实现必须通过设置您选择的显式随机数生成器种子来保证完全确定性。解决方案应有原则且可复现。\n\n您的交付成果是：\n- 一个完整的、可运行的程序，该程序执行所述的 RWM 模拟，为每个候选 $\\alpha$ 计算 ESS，验证适应性递减检查，根据标准为每个目标选择 $\\alpha$，并以确切要求的格式打印最终列表。\n- 按照规定格式的单行程序输出。",
            "solution": "该问题要求进行一项数值实验，以确定随机游走Metropolis（RWM）算法的最优自适应率参数 $\\alpha$。优化是在有限的迭代次数下进行的，并且针对三种不同的目标概率分布进行评估，每种分布都对采样器构成了独特的挑战。\n\n### 原理与方法\n\n问题的核心在于计算统计学中三个关键概念的交集：Metropolis-Hastings 算法、自适应马尔可夫链蒙特卡洛（MCMC）以及采样器效率的评估。\n\n**1. 随机游走Metropolis（RWM）算法**\n\nRWM 是 Metropolis-Hastings 算法的一个特例，用于从密度为 $p(x)$ 的目标概率分布中生成一系列样本 $\\{X_0, X_1, \\dots, X_T\\}$。在每一步 $n$，从以当前状态 $x_n$ 为中心的对称分布中提议一个候选状态 $x^{\\prime}$。问题指定了高斯提议：\n$$\nx^{\\prime} = x_n + \\eta, \\quad \\text{where } \\eta \\sim \\mathcal{N}(0, \\sigma_n^2)\n$$\n候选状态以概率 $a(x_n, x^{\\prime})$ 被接受，该概率为：\n$$\na(x_n, x^{\\prime}) = \\min\\left\\{1, \\frac{p(x^{\\prime})}{p(x_n)}\\right\\} = \\min\\left\\{1, \\exp(\\log p(x^{\\prime}) - \\log p(x_n))\\right\\}\n$$\n如果提议被接受，则 $X_{n+1} = x^{\\prime}$；否则，它被拒绝，并且 $X_{n+1} = x_n$。RWM 算法的性能严重依赖于提议尺度 $\\sigma_n$。小的 $\\sigma_n$ 会导致高接受率但状态空间探索缓慢，而大的 $\\sigma_n$ 则导致频繁拒绝，链条很少移动。\n\n**2. 自适应 MCMC 与适应性递减**\n\n找到一个最优的固定 $\\sigma$ 可能很困难。自适应 MCMC 通过允许 $\\sigma_n$ 在模拟过程中改变来解决这个问题，在运行时学习一个合适的值。问题指定了提议尺度的幂律衰减方案：\n$$\n\\sigma_n = c \\, n^{-\\alpha}\n$$\n其中 $c > 0$ 是初始尺度因子，$\\alpha > 0$ 控制衰减速率。为了使自适应 MCMC 算法在理论上有效（即，其经验平均值能收敛到正确的期望值），自适应性必须随时间递减。这被称为“适应性递减”条件。形式上，它要求连续转移核之间的全变差距离收敛到零。对于给定的 RWM 方案，一个必要（尽管不总是充分）的条件是自适应参数的变化幅度趋于零，即 $|\\sigma_{n+1} - \\sigma_n| \\to 0$。\n\n对于方案 $\\sigma_n = c n^{-\\alpha}$，我们有：\n$$\n|\\sigma_{n+1} - \\sigma_n| = |c(n+1)^{-\\alpha} - c n^{-\\alpha}| = c n^{-\\alpha} |(1+1/n)^{-\\alpha} - 1|\n$$\n对于大的 $n$ 使用泰勒展开，$(1+1/n)^{-\\alpha} \\approx 1 - \\alpha/n$，因此我们有：\n$$\n|\\sigma_{n+1} - \\sigma_n| \\approx c n^{-\\alpha} \\left|-\\frac{\\alpha}{n}\\right| = c\\alpha n^{-(\\alpha+1)}\n$$\n对于任何 $\\alpha > 0$，此量都收敛到零。问题对此条件在有限范围 $T$ 内施加了数值检验，要求终端适应幅度 $\\Delta_T = |\\sigma_{T+1} - \\sigma_T|$ 小于阈值 $\\epsilon = 10^{-5}$。只有满足此检验的 $\\alpha$ 值才被认为是合格的。较大的 $\\alpha$ 会使适应性递减得更快，从而更强烈地满足该条件，但也可能过早地将尺度 $\\sigma_n$ “冻结”在一个次优值上。\n\n**3. 采样器效率：IACT 与 ESS**\n\nMCMC 采样器的效率通过它产生来自目标分布的独立样本的速度来衡量。由于 MCMC 样本是相关的，我们使用有效样本量（ESS）作为度量。给定 $N$ 个老化期后的样本，ESS 定义为：\n$$\n\\text{ESS} = \\frac{N}{\\tau}\n$$\n其中 $\\tau$ 是积分自相关时间（IACT），定义为：\n$$\n\\tau = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k\n$$\n这里，$\\rho_k$ 是状态函数序列 $\\{f(X_i)\\}$ 的滞后k阶自相关。较小的 IACT 意味着样本之间的相关性较低，因此 ESS 较高，效率更好。任务是最大化函数 $f(x)=x$ 的 ESS。IACT 必须从有限的样本链中估计。我们使用“初始正序列”方法，该方法通过对经验自相关进行求和，只要它们保持正值，来估计 $\\tau$，这是一种避免高阶滞后估计噪声的常用启发式方法。\n\n### 实验设计与实现\n\n实验按以下步骤对三种目标分布（正态、拉普拉斯、学生t）中的每一种进行：\n\n1.  **资格检验**：对于每个候选 $\\alpha \\in \\{0.2, 0.5, 0.8\\}$，使用 $c=1.5$ 和 $T=20000$ 计算终端适应幅度 $\\Delta_T = |\\sigma_{T+1} - \\sigma_T|$。如果 $\\Delta_T \\ge 10^{-5}$，则该 $\\alpha$ 被舍弃。\n\n2.  **模拟**：对于每个合格的 $\\alpha$，从 $x_0=0$ 开始运行 RWM 模拟，共进行 $T=20000$ 次迭代。在第 $n$ 步的提议尺度为 $\\sigma_n = 1.5 \\times n^{-\\alpha}$。前 $B=2000$ 个样本作为老化期被丢弃。为确保公平比较，每次模拟都重新设定随机数生成器的种子。\n\n3.  **效率计算**：使用老化期后的 $N = T-B = 18000$ 个样本链来估计 $f(x)=x$ 的 IACT。这涉及计算经验自相关函数（ACF）并对初始的正项求和。然后将 ESS 计算为 $N/\\text{IACT}$。\n\n4.  **选择**：在为所有合格的 $\\alpha$ 值计算出 ESS 后，选择最优的 $\\alpha$。主要标准是选择产生最大 ESS 的 $\\alpha$。指定了一个决胜规则：如果多个 $\\alpha$ 值产生的 ESS 值在最大 ESS 的 $10^{-3}$ 相对容差范围内，则选择其中最小的 $\\alpha$。\n\n对三种目标密度重复此过程，这些目标密度被选择用来测试算法在具有不同平滑度和尾部重量特性的目标上的性能：\n-   **高斯分布 ($p_G$)**：平滑且强对数凹，是许多采样器的理想情况。\n-   **拉普拉斯分布 ($p_L$)**：在众数处有一个不可微的峰值，对依赖梯度的算法构成挑战。\n-   **学生t分布 ($p_T$ with $\\nu=3$)**：平滑但尾部比高斯分布更重，这可能使探索变得困难。\n\n最终输出是针对高斯、拉普拉斯和学生t目标的选定 $\\alpha$ 值的列表，按此顺序排列。\n\n以下是实现该实验的完整Python代码：\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ... # No other libraries needed\n\ndef solve():\n    \"\"\"\n    Performs an MCMC experiment to find the optimal adaptation rate alpha\n    for a Random-Walk Metropolis sampler on different target distributions.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = {\n        # Hyperparameters\n        \"SEED\": 42,\n        \"C\": 1.5,\n        \"ALPHA_CANDIDATES\": [0.2, 0.5, 0.8],\n        \"T\": 20000,\n        \"B\": 2000,\n        \"X0\": 0.0,\n        \"EPSILON\": 1e-5,\n        \"REL_TOL\": 1e-3,\n        \"NU_T\": 3.0\n    }\n\n    # Unnormalized log-probability density functions for the targets.\n    # Constant terms are omitted as they cancel in the M-H ratio.\n    def logp_normal(x):\n        return -0.5 * x**2\n\n    def logp_laplace(x):\n        return -np.abs(x)\n\n    def logp_student_t(x, nu=test_cases[\"NU_T\"]):\n        return -((nu + 1) / 2.0) * np.log(1.0 + x**2 / nu)\n\n    targets = [\n        {'name': 'Gaussian', 'log_p': logp_normal},\n        {'name': 'Laplace', 'log_p': logp_laplace},\n        {'name': 'Student-t', 'log_p': logp_student_t}\n    ]\n\n    def calculate_ess(samples):\n        \"\"\"\n        Calculates the Effective Sample Size (ESS) from a sequence of samples.\n        IACT is estimated using the initial positive sequence rule.\n        \"\"\"\n        n_samples = len(samples)\n        if n_samples  2:\n            return 0.0\n\n        demeaned_samples = samples - np.mean(samples)\n        \n        # Calculate autocovariance using FFT for efficiency\n        n_fft = 1  (2 * n_samples - 1).bit_length() # Next power of 2\n        fft_samples = np.fft.fft(demeaned_samples, n_fft)\n        autocov = np.fft.ifft(fft_samples * np.conj(fft_samples)).real\n        autocov = autocov[:n_samples] / n_samples\n        \n        if autocov[0] == 0:\n            return 0.0\n\n        acf = autocov / autocov[0]\n        \n        # Estimate IACT using the initial positive sequence rule\n        iact = 1.0\n        # Sum positive correlations up to a max lag for robustness\n        max_lag = n_samples // 5 \n        for k in range(1, min(len(acf), max_lag)):\n            if acf[k] > 0:\n                iact += 2.0 * acf[k]\n            else:\n                break\n        \n        return n_samples / iact if iact > 0 else 0.0\n\n    def rwm_sampler(log_p, x0, t_total, t_burn, c_adapt, alpha_adapt, rng):\n        \"\"\"\n        Implements the adaptive Random-Walk Metropolis sampler.\n        \"\"\"\n        chain = np.zeros(t_total)\n        x_current = x0\n        log_p_current = log_p(x_current)\n        \n        for n in range(1, t_total + 1):\n            sigma_n = c_adapt * n**(-alpha_adapt)\n            proposal = x_current + rng.normal(loc=0.0, scale=sigma_n)\n            \n            log_p_proposal = log_p(proposal)\n            log_acceptance_ratio = log_p_proposal - log_p_current\n            \n            if np.log(rng.uniform(0.0, 1.0))  log_acceptance_ratio:\n                x_current = proposal\n                log_p_current = log_p_proposal\n            \n            chain[n-1] = x_current\n            \n        return chain[t_burn:]\n\n    results = []\n    \n    C = test_cases[\"C\"]\n    T = test_cases[\"T\"]\n    B = test_cases[\"B\"]\n    X0 = test_cases[\"X0\"]\n    EPSILON = test_cases[\"EPSILON\"]\n    REL_TOL = test_cases[\"REL_TOL\"]\n    ALPHA_CANDIDATES = test_cases[\"ALPHA_CANDIDATES\"]\n    SEED = test_cases[\"SEED\"]\n\n    for target_spec in targets:\n        log_p_func = target_spec['log_p']\n        ess_results = []\n        \n        for alpha in ALPHA_CANDIDATES:\n            # 1. Diminishing Adaptation numerical verification\n            sigma_T = C * T**(-alpha)\n            sigma_T_plus_1 = C * (T + 1)**(-alpha)\n            delta_T = np.abs(sigma_T_plus_1 - sigma_T)\n            \n            if delta_T >= EPSILON:\n                continue\n\n            # 2. Run MCMC simulation. Reset seed for each run for fair comparison.\n            rng = np.random.default_rng(seed=SEED)\n            samples = rwm_sampler(log_p_func, X0, T, B, C, alpha, rng)\n            \n            # 3. Calculate ESS\n            ess = calculate_ess(samples)\n            ess_results.append((alpha, ess))\n            \n        # 4. Selection criterion: find best alpha\n        if not ess_results:\n            # This should not happen since all alphas pass the DA check.\n            # Handle defensively by appending a placeholder.\n            results.append(np.nan) \n            continue\n\n        max_ess = -1.0\n        for _, ess in ess_results:\n            if ess > max_ess:\n                max_ess = ess\n        \n        # Identify alphas within the tolerance of the max ESS\n        tied_alphas = []\n        for alpha, ess in ess_results:\n            if ess >= max_ess * (1.0 - REL_TOL):\n                tied_alphas.append(alpha)\n        \n        # Choose the smallest alpha among the tied values\n        best_alpha = min(tied_alphas)\n        results.append(best_alpha)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.1f}' for r in results)}]\")\n\nsolve()\n```",
            "answer": "$$\\boxed{[0.2,0.5,0.2]}$$"
        }
    ]
}