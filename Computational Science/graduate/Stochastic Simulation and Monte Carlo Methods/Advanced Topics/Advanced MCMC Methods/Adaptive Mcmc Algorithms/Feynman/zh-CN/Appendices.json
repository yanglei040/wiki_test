{
    "hands_on_practices": [
        {
            "introduction": "本练习将带你深入探讨许多自适应MCMC算法的理论核心：随机近似。通过分析一个简单的一维自适应Metropolis-Hastings算法 ()，你将推导出控制其行为的关键量，并证明其收敛性。这项实践对于理解自适应算法为何有效，以及如何用数学方法刻画其长期行为至关重要。",
            "id": "3287318",
            "problem": "考虑一个一维自适应 Metropolis–Hastings (AMH) 算法，其目标为标准正态密度 $\\pi(x) \\propto \\exp(-x^2/2)$。提议是高斯随机游走，其尺度为 $s_t = \\exp(\\theta_t)$，即 $X_{t+1} \\sim \\mathcal{N}(X_t, s_t^2)$。令 $A_{t+1} \\in \\{0,1\\}$ 表示在第 $t+1$ 次迭代时的 Metropolis–Hastings 接受指示符。对数尺度参数的自适应由以下随机逼近递推关系决定\n$$\n\\theta_{t+1} \\;=\\; \\theta_{t} \\;+\\; \\gamma_{t}\\,\\big(A_{t+1} - \\alpha^{\\star}\\big), \n\\qquad \\gamma_{t} \\;=\\; \\frac{c}{t},\n$$\n其中 $c \\in (0,\\infty)$ 为固定值，目标接受率为 $\\alpha^{\\star} \\in (0,1)$。定义移动平均 $\\bar{\\theta}_t \\;=\\; t^{-1} \\sum_{k=1}^{t} \\theta_k$。假设使用自适应马尔可夫链蒙特卡洛 (MCMC) 中标准的时标分离：自适应每 $b_t$ MCMC 步应用一次，且 $b_t \\to \\infty$ 增长得足够快，链混合得也足够快，因此在随机逼近中使用的接受指示符是渐近独立的伯努利随机变量，当提议尺度等于 $s$ 时，其均值为 $\\alpha(s)$。\n\n从第一性原理（Metropolis–Hastings 接受准则、标准正态分布的性质以及随机逼近常微分方程方法）出发，完成以下任务：\n\n1) 证明随机逼近的平均场漂移为\n$$\nh(\\theta) \\;=\\; \\mathbb{E}\\big[A_{t+1} \\,\\big|\\, \\theta_{t}=\\theta\\big] \\;-\\; \\alpha^{\\star} \\;=\\; \\alpha\\big(\\exp(\\theta)\\big) \\;-\\; \\alpha^{\\star},\n$$\n其中 $\\alpha(s)$ 是提议尺度为 $s$ 的随机游走 Metropolis 算法的平稳期望接受概率。从第一性原理推导，对于标准正态目标和高斯随机游走提议，\n$$\n\\alpha(s) \\;=\\; \\frac{2}{\\pi}\\,\\arctan\\!\\Big(\\frac{2}{s}\\Big), \\qquad s \\in (0,\\infty).\n$$\n\n2) 令 $\\theta^{\\star}$ 表示 $h(\\theta)$ 的唯一根，并记 $s^{\\star} = \\exp(\\theta^{\\star})$。证明 $h'(\\theta^{\\star})  0$，并将 $h'(\\theta^{\\star})$ 用 $s^{\\star}$ 明确表示。\n\n3) 使用随机逼近的 Polyak–Ruppert 平均中心极限定理，建立平均迭代的渐近正态性，\n$$\n\\sqrt{t}\\,\\big(\\bar{\\theta}_{t} - \\theta^{\\star}\\big) \\;\\Rightarrow\\; \\mathcal{N}\\big(0,\\, V\\big),\n$$\n并推导出渐近方差 $V$，其形式为仅包含 $\\alpha^{\\star}$ 的闭式解。\n\n4) 在经典的一维目标接受率 $\\alpha^{\\star} = 0.44$ 下，计算得到的 $V$ 值。将答案四舍五入到四位有效数字。最终答案必须是一个没有单位的实数。",
            "solution": "该问题要求对一维自适应 Metropolis-Hastings (AMH) 算法进行多部分分析。我们将按照要求，从第一性原理出发，依次解决每个部分。\n\n### 第1部分：平均场漂移和接受概率\n\n首先，我们确定平均场漂移函数 $h(\\theta)$ 的形式。对数尺度参数 $\\theta_t$ 的随机逼近递推关系由下式给出\n$$\n\\theta_{t+1} = \\theta_{t} + \\gamma_{t}(A_{t+1} - \\alpha^{\\star}).\n$$\n平均场漂移（记为 $h(\\theta_t)$）是在给定当前状态 $\\theta_t$ 的条件下，$\\theta_t$ 在单位步长 $\\gamma_t$ 上的期望变化。形式上，\n$$\nh(\\theta_t) = \\mathbb{E}\\left[\\frac{\\theta_{t+1} - \\theta_t}{\\gamma_t} \\bigg| \\theta_t\\right] = \\mathbb{E}[A_{t+1} - \\alpha^{\\star} | \\theta_t].\n$$\n根据重期望定律以及链混合足够快的假设，在第 $t+1$ 步的接受概率 $A_{t+1}$ 仅取决于用于生成提议的提议尺度 $s_t = \\exp(\\theta_t)$。令 $\\theta_t = \\theta$。则 $s_t=s=\\exp(\\theta)$。我们有\n$$\n\\mathbb{E}[A_{t+1} | \\theta_t=\\theta] = \\alpha(s) = \\alpha(\\exp(\\theta)),\n$$\n其中 $\\alpha(s)$ 是提议尺度为 $s$ 的 Metropolis-Hastings 算法的平稳期望接受概率。因此，平均场漂移为\n$$\nh(\\theta) = \\alpha(\\exp(\\theta)) - \\alpha^{\\star}.\n$$\n\n接下来，我们推导 $\\alpha(s)$ 的表达式，对于标准正态目标密度 $\\pi(x) \\propto \\exp(-x^2/2)$ 和高斯随机游走提议 $Y \\sim \\mathcal{N}(x, s^2)$。提议密度 $q(y|x)$ 是对称的，即 $q(y|x) = q(x|y)$。从 $x$ 移动到 $y$ 的 Metropolis-Hastings 接受概率为\n$$\na(x,y) = \\min\\left(1, \\frac{\\pi(y)}{\\pi(x)}\\right) = \\min\\left(1, \\frac{\\exp(-y^2/2)}{\\exp(-x^2/2)}\\right) = \\min\\left(1, \\exp\\left(-\\frac{y^2-x^2}{2}\\right)\\right).\n$$\n平稳期望接受概率 $\\alpha(s)$ 是 $a(X,Y)$ 的期望，其中 $X \\sim \\pi$ 且 $Y|X=x \\sim q(\\cdot|x)$。所以 $X \\sim \\mathcal{N}(0,1)$，我们可以写成 $Y = X + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, s^2)$ 与 $X$ 独立。\n接受概率成为 $X$ 和 $\\epsilon$ 的函数：\n$$\na(X, \\epsilon) = \\min\\left(1, \\exp\\left(-\\frac{(X+\\epsilon)^2-X^2}{2}\\right)\\right) = \\min\\left(1, \\exp\\left(-\\frac{2X\\epsilon+\\epsilon^2}{2}\\right)\\right).\n$$\n我们通过先对 $\\epsilon$ 取条件来计算 $\\alpha(s) = \\mathbb{E}_{X,\\epsilon}[a(X, \\epsilon)]$：\n$$\n\\alpha(s) = \\mathbb{E}_{\\epsilon}\\left[ \\mathbb{E}_{X| \\epsilon} \\left[ \\min\\left(1, \\exp\\left(-X\\epsilon - \\frac{\\epsilon^2}{2}\\right)\\right) \\right] \\right].\n$$\n我们来分析内部的期望。当 $-X\\epsilon - \\epsilon^2/2 > 0$ 时，比率大于1，这等价于 $X\\epsilon  -\\epsilon^2/2$。\n情况1：$\\epsilon > 0$。条件是 $X  -\\epsilon/2$。\n情况2：$\\epsilon  0$。条件是 $X > -\\epsilon/2$。\n令 $\\Phi(\\cdot)$ 为标准正态分布的累积分布函数(CDF)。我们来计算条件期望 $f(\\epsilon) = \\mathbb{E}_{X}[a(X,\\epsilon)]$。\n对于 $\\epsilon > 0$：\n$$\nf(\\epsilon) = \\int_{-\\infty}^{-\\epsilon/2} 1 \\cdot \\phi(x) \\, dx + \\int_{-\\epsilon/2}^{\\infty} \\exp\\left(-x\\epsilon - \\frac{\\epsilon^2}{2}\\right) \\phi(x) \\, dx,\n$$\n其中 $\\phi(x) = (2\\pi)^{-1/2}\\exp(-x^2/2)$。第一个积分是 $\\Phi(-\\epsilon/2)$。第二个积分是\n$$\n\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\epsilon/2}^{\\infty} \\exp\\left(-x\\epsilon - \\frac{\\epsilon^2}{2} - \\frac{x^2}{2}\\right) \\, dx = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\epsilon/2}^{\\infty} \\exp\\left(-\\frac{(x+\\epsilon)^2}{2}\\right) \\, dx.\n$$\n通过换元 $u = x+\\epsilon$，下限变为 $-\\epsilon/2+\\epsilon = \\epsilon/2$。该积分为 $\\int_{\\epsilon/2}^{\\infty} \\phi(u) \\, du = 1-\\Phi(\\epsilon/2) = \\Phi(-\\epsilon/2)$。\n因此，对于 $\\epsilon > 0$，$f(\\epsilon) = \\Phi(-\\epsilon/2) + \\Phi(-\\epsilon/2) = 2\\Phi(-\\epsilon/2) = 2\\Phi(-|\\epsilon|/2)$。\n对 $\\epsilon  0$ 进行类似的计算表明 $f(\\epsilon) = 2\\Phi(\\epsilon/2) = 2\\Phi(-|\\epsilon|/2)$ 也成立。\n因此，对于任意 $\\epsilon \\neq 0$，我们有 $\\mathbb{E}_{X}[a(X,\\epsilon)] = 2\\Phi(-|\\epsilon|/2)$。\n现在我们对 $\\epsilon \\sim \\mathcal{N}(0,s^2)$ 取期望。令 $\\epsilon=sZ$，其中 $Z \\sim \\mathcal{N}(0,1)$。\n$$\n\\alpha(s) = \\mathbb{E}_{Z}\\left[ 2\\Phi\\left(-\\frac{s|Z|}{2}\\right) \\right] = 2 \\int_{-\\infty}^{\\infty} \\Phi\\left(-\\frac{s|z|}{2}\\right)\\phi(z) \\, dz = 4 \\int_{0}^{\\infty} \\Phi\\left(-\\frac{sz}{2}\\right)\\phi(z) \\, dz.\n$$\n该积分可按如下方式计算：\n$$\nI = \\int_{0}^{\\infty} \\Phi\\left(-\\frac{sz}{2}\\right)\\phi(z) \\, dz = \\int_0^\\infty \\left( \\int_{-\\infty}^{-sz/2} \\phi(u) \\, du \\right) \\phi(z) \\, dz = \\frac{1}{2\\pi} \\iint_D \\exp\\left(-\\frac{u^2+z^2}{2}\\right) \\, du \\, dz,\n$$\n其中积分区域为 $D = \\{(u,z) : z>0, u  -sz/2\\}$。这是 $(u,z)$ 平面中的一个扇形区域。我们转换到极坐标 $(r, \\theta)$，其中 $z=r\\sin\\theta, u=r\\cos\\theta$。\n条件 $z>0$ 意味着 $\\theta \\in (0, \\pi)$。条件 $u0$ 意味着 $\\theta \\in (\\pi/2, 3\\pi/2)$。合在一起，$\\theta \\in (\\pi/2, \\pi)$。条件 $u  -sz/2$ 变为 $r\\cos\\theta  -s/2 \\cdot r\\sin\\theta$，简化为 $\\cot\\theta  -s/2$（因为在此范围内 $\\sin\\theta>0$）。反余切函数将 $(-\\infty,0)$ 映射到 $(\\pi/2, \\pi)$，所以对 $\\theta$ 的条件是 $\\text{arccot}(-s/2)  \\theta  \\pi$。\n使用恒等式 $\\text{arccot}(-x) = \\pi - \\text{arccot}(x) = \\pi - (\\pi/2 - \\arctan(x)) = \\pi/2 + \\arctan(x)$（对于 $x0$），$\\theta$ 的范围是 $(\\pi/2 + \\arctan(s/2), \\pi)$。\n在极坐标下的积分为：\n$$\nI = \\frac{1}{2\\pi} \\int_{\\pi/2 + \\arctan(s/2)}^{\\pi} \\int_0^\\infty r\\exp(-r^2/2) \\, dr \\, d\\theta.\n$$\n内部积分为 $\\int_0^\\infty r e^{-r^2/2} \\, dr = [-e^{-r^2/2}]_0^\\infty = 1$。\n$$\nI = \\frac{1}{2\\pi} \\left[ \\pi - \\left(\\frac{\\pi}{2} + \\arctan\\left(\\frac{s}{2}\\right)\\right) \\right] = \\frac{1}{2\\pi} \\left( \\frac{\\pi}{2} - \\arctan\\left(\\frac{s}{2}\\right) \\right).\n$$\n使用恒等式 $\\arctan(x) + \\arctan(1/x) = \\pi/2$（对于 $x0$），这可以简化为 $I = \\frac{1}{2\\pi} \\arctan(2/s)$。\n最后，$\\alpha(s) = 4I = 4 \\left( \\frac{1}{2\\pi} \\arctan(2/s) \\right) = \\frac{2}{\\pi} \\arctan(2/s)$。\n\n### 第2部分：稳定性分析\n\n$h(\\theta)$ 的根 $\\theta^\\star$ 满足 $h(\\theta^\\star) = \\alpha(\\exp(\\theta^\\star)) - \\alpha^\\star = 0$。令 $s^\\star = \\exp(\\theta^\\star)$。则 $\\alpha(s^\\star) = \\alpha^\\star$。\n我们需要确定 $h'(\\theta^\\star)$ 的符号。使用链式法则：\n$$\nh'(\\theta) = \\frac{d}{d\\theta} \\alpha(\\exp(\\theta)) = \\alpha'(\\exp(\\theta)) \\cdot \\exp(\\theta).\n$$\n在根 $\\theta^\\star$ 处，我们有 $h'(\\theta^\\star) = \\alpha'(s^\\star) \\cdot s^\\star$。\n我们首先计算 $\\alpha(s)$ 的导数：\n$$\n\\alpha'(s) = \\frac{d}{ds} \\left( \\frac{2}{\\pi} \\arctan\\left(\\frac{2}{s}\\right) \\right) = \\frac{2}{\\pi} \\cdot \\frac{1}{1+(2/s)^2} \\cdot \\left(-\\frac{2}{s^2}\\right) = \\frac{2}{\\pi} \\cdot \\frac{s^2}{s^2+4} \\cdot \\left(-\\frac{2}{s^2}\\right) = -\\frac{4}{\\pi(s^2+4)}.\n$$\n因为 $s \\in (0, \\infty)$，我们有 $s^2+4>0$，所以对于所有的 $s$，$\\alpha'(s)  0$。\n这意味着 $h'(\\theta^\\star) = \\alpha'(s^\\star) s^\\star  0$，这证实了该平衡点是稳定的。\n$h'(\\theta^\\star)$ 用 $s^\\star$ 表示的显式表达式为：\n$$\nh'(\\theta^\\star) = s^\\star \\cdot \\left( -\\frac{4}{\\pi((s^\\star)^2+4)} \\right) = -\\frac{4s^\\star}{\\pi((s^\\star)^2+4)}.\n$$\n\n### 第3部分：渐近方差\n\nPolyak-Ruppert 平均中心极限定理指出，在某些条件下（包括第2部分中证明的稳定性以及对步长常数 $c$ 的条件），平均迭代 $\\bar{\\theta}_t = t^{-1}\\sum_{k=1}^t \\theta_k$ 是渐近正态的：\n$$\n\\sqrt{t}(\\bar{\\theta}_t - \\theta^\\star) \\Rightarrow \\mathcal{N}(0, V).\n$$\n渐近方差 $V$ 由公式 $V = \\frac{\\Sigma}{(h'(\\theta^\\star))^2}$ 给出，其中 $\\Sigma$ 是驱动随机逼近过程在平衡状态下的噪声过程的方差。\n该递推关系可以写为 $\\theta_{t+1} = \\theta_t + \\gamma_t(h(\\theta_t) + M_{t+1})$，其中 $M_{t+1} = A_{t+1} - \\mathbb{E}[A_{t+1}|\\theta_t]$ 是噪声项。在平衡状态（$\\theta_t = \\theta^\\star$），接受指示符 $A_{t+1}$ 服从伯努利分布，其成功概率为 $\\mathbb{E}[A_{t+1}|\\theta^\\star] = \\alpha(s^\\star) = \\alpha^\\star$。\n一个伯努利($p$)随机变量的方差是 $p(1-p)$。因此，在平衡状态下的噪声方差为\n$$\n\\Sigma = \\text{Var}(A_{t+1}|\\theta_t=\\theta^\\star) = \\alpha^\\star(1-\\alpha^\\star).\n$$\n为了将 $V$ 表示为仅包含 $\\alpha^\\star$ 的函数，我们必须将 $h'(\\theta^\\star)$ 用 $\\alpha^\\star$ 表示。由 $\\alpha^\\star = \\frac{2}{\\pi}\\arctan(2/s^\\star)$，我们有 $\\frac{\\pi\\alpha^\\star}{2} = \\arctan(2/s^\\star)$。对两边取正切，得到 $\\tan(\\frac{\\pi\\alpha^\\star}{2}) = \\frac{2}{s^\\star}$，这意味着 $s^\\star = \\frac{2}{\\tan(\\frac{\\pi\\alpha^\\star}{2})}$。\n将此代入 $h'(\\theta^\\star)$ 的表达式中：\n$$\nh'(\\theta^\\star) = -\\frac{4s^\\star}{\\pi((s^\\star)^2+4)} = -\\frac{4 \\left(\\frac{2}{\\tan(\\frac{\\pi\\alpha^\\star}{2})}\\right)}{\\pi \\left( \\left(\\frac{2}{\\tan(\\frac{\\pi\\alpha^\\star}{2})}\\right)^2 + 4 \\right)} = -\\frac{\\frac{8}{\\tan(\\frac{\\pi\\alpha^\\star}{2})}}{\\pi \\left( \\frac{4}{\\tan^2(\\frac{\\pi\\alpha^\\star}{2})} + 4 \\right)} = -\\frac{8}{\\pi \\tan(\\frac{\\pi\\alpha^\\star}{2})} \\frac{\\tan^2(\\frac{\\pi\\alpha^\\star}{2})}{4(1+\\tan^2(\\frac{\\pi\\alpha^\\star}{2}))}.\n$$\n使用恒等式 $1+\\tan^2(x) = \\sec^2(x)$，这可以简化为：\n$$\nh'(\\theta^\\star) = -\\frac{2\\tan(\\frac{\\pi\\alpha^\\star}{2})}{\\pi \\sec^2(\\frac{\\pi\\alpha^\\star}{2})} = -\\frac{2}{\\pi} \\frac{\\sin(\\frac{\\pi\\alpha^\\star}{2})/\\cos(\\frac{\\pi\\alpha^\\star}{2})}{1/\\cos^2(\\frac{\\pi\\alpha^\\star}{2})} = -\\frac{2}{\\pi}\\sin\\left(\\frac{\\pi\\alpha^\\star}{2}\\right)\\cos\\left(\\frac{\\pi\\alpha^\\star}{2}\\right).\n$$\n使用二倍角恒等式 $2\\sin(x)\\cos(x) = \\sin(2x)$，我们得到一个非常简洁的表达式：\n$$\nh'(\\theta^\\star) = -\\frac{1}{\\pi}\\sin\\left(2 \\cdot \\frac{\\pi\\alpha^\\star}{2}\\right) = -\\frac{\\sin(\\pi\\alpha^\\star)}{\\pi}.\n$$\n现在我们组合出渐近方差 $V$：\n$$\nV = \\frac{\\Sigma}{(h'(\\theta^\\star))^2} = \\frac{\\alpha^\\star(1-\\alpha^\\star)}{\\left(-\\frac{\\sin(\\pi\\alpha^\\star)}{\\pi}\\right)^2} = \\frac{\\pi^2 \\alpha^\\star(1-\\alpha^\\star)}{\\sin^2(\\pi\\alpha^\\star)}.\n$$\n\n### 第4部分：数值计算\n\n题目要求我们计算当目标接受率为 $\\alpha^\\star = 0.44$ 时的 $V$ 值。\n将这个值代入我们推导出的 $V$ 公式中：\n$$\nV = \\frac{\\pi^2 (0.44)(1-0.44)}{\\sin^2(0.44\\pi)} = \\frac{\\pi^2 (0.44)(0.56)}{\\sin^2(0.44\\pi)} = \\frac{0.2464 \\pi^2}{\\sin^2(0.44\\pi)}.\n$$\n我们进行数值计算：\n正弦函数的参数是 $0.44\\pi$ 弧度。\n$0.44\\pi \\approx 1.382300767$\n$\\sin(0.44\\pi) \\approx 0.98223604$\n$\\sin^2(0.44\\pi) \\approx 0.96478761$\n分子是 $0.2464 \\pi^2 \\approx 0.2464 \\times 9.8696044 \\approx 2.4318705$。\n因此，方差为\n$$\nV \\approx \\frac{2.4318705}{0.96478761} \\approx 2.520653.\n$$\n四舍五入到四位有效数字，我们得到 $2.521$。",
            "answer": "$$\\boxed{2.521}$$"
        },
        {
            "introduction": "从一维理论转向高维应用，我们会遇到关键的实践挑战。本练习  旨在解决著名的Haario-Saksman-Tamminen (HST) 算法中的一个核心问题：确保自适应协方差矩阵的数值稳定性。你将评估不同的策略，以防止矩阵失去正定性——这是浮点运算中常见且可能导致模拟失败的问题。",
            "id": "3353669",
            "problem": "考虑 Haario–Saksman–Tamminen (HST) 的自适应 Metropolis 算法，该算法在马尔可夫链的迭代过程 $n$ 中更新提议协方差矩阵 $\\Sigma_n$，以适应目标分布。在精确算术中，我们希望 $\\Sigma_n$ 是对称正定的，这样协方差为 $\\Sigma_n$ 的高斯提议才是良定义的，并且 Cholesky 分解存在。在浮点运算中，舍入误差、小更新的累积和病态问题可能导致正定性的丧失或在形成提议时出现数值失败。设计一个数值稳健的测试来检测 $\\Sigma_n$ 的正定性丧失，并指定纠正措施以使 $\\Sigma_n$ 变得适用，例如增加对角正则化项 $\\epsilon$ 或在需要时重新初始化分解。\n\n使用以下基本事实作为出发点，并用它们来证明您的选择：\n- 一个实对称矩阵 $A$ 是正定的，当且仅当其所有特征值都严格为正，这等价于存在一个对角线元素为正的 Cholesky 分解 $A = LL^\\top$。\n- 加上单位矩阵的倍数，即用 $A + \\epsilon I$（其中 $\\epsilon  0$）替换 $A$，会将其所有特征值向上平移 $\\epsilon$ 并保持对称性。\n- 仅凭行列式在高维情况下不是一个可靠的正定性数值检验方法，且对于一般矩阵，不带主元的高斯消元法在数值上是不稳定的。\n- Gershgorin 圆盘定理根据行和提供了特征值的界，但可能比较保守，并且必须小心地应用于对称矩阵。\n- 在自适应算法中，反复的数值检查失败应该触发提议的安全重新初始化，例如重置为缩放后的单位协方差，以确保稳定性。\n\n在 HST 自适应过程中，以下哪个程序是检测和纠正 $\\Sigma_n$ 正定性丧失的数值稳健方法？\n\nA. 每次更新时，形成对称部分 $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$ 并尝试对 $S_n$ 进行 Cholesky 分解。如果分解因非正主元或返回非数值而失败，则通过将 $S_n$ 替换为 $S_n + \\epsilon I$ 来增加对角正则化并重试，使用自适应策略 $\\epsilon \\leftarrow \\max\\{\\epsilon_{\\min}, \\gamma \\epsilon\\}$ (其中 $\\gamma  1$) 直到分解成功。如果连续 $k$ 次尝试均失败，则将协方差重新初始化为 $c I$ (对于某个尺度 $c  0$) 并重建分解。使用得到的因子来定义提议。\n\nB. 通过不带主元的 LU 分解计算行列式 $\\det(\\Sigma_n)$。如果 $\\det(\\Sigma_n)  0$，则断定 $\\Sigma_n$ 是正定的并继续。如果 $\\det(\\Sigma_n) \\le 0$，则将 $\\epsilon$ 减小至 $0$ 并重试 LU 分解，直到 $\\det(\\Sigma_n)  0$，然后继续而不重新初始化。\n\nC. 使用对称特征求解器（例如，Householder 三角化后接 QR 算法或 Lanczos 迭代）估计对称部分 $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$ 的最小特征值 $\\lambda_{\\min}$。如果对于一个小阈值 $\\tau  0$ 有 $\\lambda_{\\min}  \\tau$，则通过以下任一方式进行纠正：(i) 设置 $\\Sigma_n \\leftarrow S_n + \\epsilon I$ (其中 $\\epsilon \\ge \\tau - \\lambda_{\\min}$)；或 (ii) 通过将负的或过小的特征值替换为 $\\tau$ 来投影谱，并从其特征分解中重建 $S_n$，然后计算 Cholesky 因子来定义提议。如果重复的纠正超出预算，则重新初始化为 $c I$。\n\nD. 将 Gershgorin 圆盘定理应用于 $\\Sigma_n$ 的行。如果任何 Gershgorin 圆盘与负实轴相交，则宣布失去正定性，并通过减去一个对角项来进行纠正，即设置 $\\Sigma_n \\leftarrow \\Sigma_n - \\epsilon I$ (其中 $\\epsilon  0$)，以使圆盘远离负轴。继续而不重新初始化。\n\nE. 当怀疑失去正定性时，向 $\\Sigma_n$ 添加一个随机的秩一顶 $uu^\\top$（其中 $u$ 从超立方体中均匀抽取），并尝试对现有因子进行 Cholesky 降阶（downdate）。用新的 $u$ 重复此过程，直到降阶成功，然后使用更新后的分解作为提议，无需进一步检查。\n\n选择所有正确选项。",
            "solution": "问题陈述要求找出数值稳健的程序，用于检测和纠正 Haario-Saksman-Tamminen (HST) Metropolis 算法中使用的自适应协方差矩阵 $\\Sigma_n$ 的正定性丧失。\n\n**问题验证**\n\n首先，我将验证问题陈述本身。\n\n**第1步：提取已知条件**\n-   **算法**: Haario–Saksman–Tamminen (HST) 自适应 Metropolis。\n-   **矩阵**: 第 $n$ 次迭代时的提议协方差矩阵 $\\Sigma_n$。\n-   **期望性质**: $\\Sigma_n$ 应为对称正定 (SPD)。\n-   **数值问题**: 浮点运算可能导致正定性丧失。\n-   **任务**: 找出一个数值稳健的程序来测试并纠正这种丧失。\n-   **已知事实**:\n    1.  一个实对称矩阵 $A$ 是 SPD $\\iff$ 所有特征值都严格为正 $\\iff$ 存在一个对角线元素为正的 Cholesky 分解 $A = LL^\\top$。\n    2.  对于一个对称矩阵 $A$，矩阵 $A + \\epsilon I$ (其中 $\\epsilon  0$) 的特征值向上平移 $\\epsilon$ 并且保持对称。\n    3.  行列式不是一个可靠的正定性数值检验方法。不带主元的高斯消元法在数值上是不稳定的。\n    4.  Gershgorin 圆盘定理给出了可能比较保守的特征值界。\n    5.  自适应算法中持续的数值失败需要对提议机制进行安全地重新初始化。\n\n**第2步：使用提取的已知条件进行验证**\n-   **科学依据**: 这个问题在计算统计学和数值线性代数领域有充分的依据。HST 算法是自适应 MCMC 中的一个经典方法，数值更新的协方差矩阵失去正定性是一个真实且实际的问题。所提供的事实都是标准的、正确的定理和原则。\n-   **良态性**: 这个问题是良态的。它要求根据一组基本原则，对照数值稳健性的标准来评估一组提议的计算程序。通过逻辑推导可以得出明确的答案。\n-   **客观性**: 问题以客观、技术性的语言陈述，没有歧义或主观论断。\n\n**第3步：结论与行动**\n问题陈述是有效的。它在科学上是合理的、良态的且客观的。我现在将开始分析每个选项。\n\n**推导与逐项分析**\n\n核心任务是维持协方差矩阵 $\\Sigma_n$ 的对称正定性。一个稳健的程序必须包含三个部分：一个可靠的正定性测试，一个有效的纠正机制，以及一个保障稳定性的故障安全措施。由于浮点误差，$\\Sigma_n$ 可能会变得轻微不对称，因此通过取 $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$ 来强制对称是一个标准且合理的初步步骤。\n\n**选项 A 分析**\n此选项提出的程序包含以下步骤：\n1.  **对称化**: 形成 $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$。这能正确处理由浮点更新引起的任何微小不对称性。\n2.  **检测**: 尝试对 $S_n$ 进行 Cholesky 分解。如事实1所述，Cholesky 分解的存在性等价于矩阵是对称正定的。在计算上，尝试进行分解是检验此性质的标准、最高效且数值可靠的方法。如果 Cholesky 分解算法（例如，涉及计算 $\\sqrt{a_{kk} - \\sum_{j=1}^{k-1} L_{kj}^2}$）在平方根内遇到非正项，则该矩阵不是正定的。这是一个直接且结论性的测试。\n3.  **纠正**: 如果分解失败，用 $S_n + \\epsilon I$ 替换 $S_n$（其中 $\\epsilon  0$）。事实2证实此操作会将 $S_n$ 的所有特征值向上平移 $\\epsilon$。对于足够大的 $\\epsilon$，所有特征值都将变为正数，从而使矩阵成为正定矩阵。这是一种称为对角加载或吉洪诺夫正则化的标准技术。\n4.  **自适应与稳定性**: 该程序建议为 $\\epsilon$ 设置一个自适应策略 ($\\epsilon \\leftarrow \\max\\{\\epsilon_{\\min}, \\gamma \\epsilon\\}$，其中 $\\gamma  1$)，并在纠正重复失败时重新初始化为 $cI$。这确保了纠正是最小侵入性的，但在需要时会变得更强，并且通过备用机制保证了算法的整体稳定性，完全符合事实5。\n\n此程序结合了可靠的测试、标准的纠正和稳定性机制，所有这些都与所提供的事实和数值线性代数的最佳实践相一致。\n**结论：正确**\n\n**选项 B 分析**\n此选项提出：\n1.  **检测**: 计算 $\\det(\\Sigma_n)$ 并检查其是否为正。事实3明确且正确地指出，行列式不是一个可靠的测试。一个对称矩阵可以有正的行列式但不是正定的（例如，具有偶数个负项的对角矩阵）。例如，如果 $\\Sigma_n = \\text{diag}(-1, -1, 4)$，其行列式为 $4  0$，但它不是正定的。\n2.  **实现细节**: 它建议使用不带主元的 LU 分解。事实3警告说，这对于一般矩阵在数值上是不稳定的。由于我们怀疑 $\\Sigma_n$ 可能不是 SPD，我们不能假设它属于无主元 LU 分解稳定的那类矩阵。\n3.  **纠正**: 它建议如果 $\\det(\\Sigma_n) \\le 0$ 则将 $\\epsilon$ 减小到 $0$。这与正确的操作相反。为了通过对角加载使矩阵正定，必须向对角线*添加*一个正值，这对应于在表达式 $S_n + \\epsilon I$ 中*增加* $\\epsilon$。\n4.  **稳定性**: 它提议不重新初始化而继续，这与事实5中概述的确保稳定性的原则相矛盾。\n\n这个程序在每个方面都有缺陷：测试是错误的，纠正是适得其反的，并且它缺乏稳定性保证。\n**结论：不正确**\n\n**选项 C 分析**\n此选项提出：\n1.  **对称化**: 形成 $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$。正确。\n2.  **检测**: 估计 $S_n$ 的最小特征值 $\\lambda_{\\min}$。根据事实1，矩阵是正定的当且仅当其所有特征值都为正。因此，检查 $\\lambda_{\\min}$ 是否为正（或为数值安全起见大于一个小的容差 $\\tau  0$）是一个直接且理论上完美的测试。虽然比 Cholesky 分解计算成本更高，但它是一个完全有效且稳健的检测方法。\n3.  **纠正**: 它提供了两种有效的纠正策略。\n    (i) 设置 $\\Sigma_n \\leftarrow S_n + \\epsilon I$（其中 $\\epsilon \\ge \\tau - \\lambda_{\\min}$）。这是事实2的一个有针对性的应用。新矩阵的最小特征值将是 $\\lambda_{\\min} + \\epsilon$。选择 $\\epsilon \\ge \\tau - \\lambda_{\\min}$ 保证了新的最小特征值至少为 $\\tau$，从而使矩阵稳健地正定。\n    (ii) 通过将小于 $\\tau$ 的特征值替换为 $\\tau$ 并重建矩阵来投影谱。这是找到一个邻近的半正定（或正定）矩阵的著名方法，并且是一种有原则的执行纠正的方式。\n4.  **稳定性**: 如果纠正过于频繁，它包含一个重新初始化步骤 ($cI$)，这遵循了事实5。\n\n这个程序在理论上是合理的，在数值上是稳健的。它相对于选项A更高的计算成本是一个实际的权衡，而不是其稳健性的缺陷。\n**结论：正确**\n\n**选项 D 分析**\n此选项提出：\n1.  **检测**: 使用 Gershgorin 圆盘定理。事实4正确地指出，这个定理提供的界可能是保守的。一个 Gershgorin 圆盘与负实轴相交并不证明某个特征值是负的；它只表明存在这种可能性。这可能导致不必要的纠正（假阳性），使得该测试并非最优。\n2.  **纠正**: 它建议设置 $\\Sigma_n \\leftarrow \\Sigma_n - \\epsilon I$。此操作*减去*单位矩阵的正倍数，这将所有特征值*向下*平移 $\\epsilon$。这会使一个非正定的矩阵更加非正定，甚至可能使一个正定矩阵变为非正定。这在根本上是错误的。\n3.  **稳定性**: 它缺少重新初始化机制，违反了事实5。\n\n提议的纠正方法是灾难性错误的。\n**结论：不正确**\n\n**选项 E 分析**\n此选项提出：\n1.  **纠正**: 添加一个随机的秩一矩阵 $uu^\\top$。虽然添加像 $uu^\\top$ 这样的半正定矩阵不会减小任何特征值，但这并不是一种有针对性或有保证的修复负特征值的方法。这是一种启发式方法，而不是一个稳健、受控的程序。不清楚需要多少次随机尝试，或者对于具有多个或大幅度负特征值的矩阵它是否会成功。\n2.  **机制**: 它提到了 \"Cholesky downdate\"（降阶）。降阶对应于减去一个秩一矩阵，即从 $A$ 的因子中找到 $A - uu^\\top$ 的 Cholesky 因子。该程序描述的是*添加* $uu^\\top$，对此应该使用 Cholesky *update*（更新）。已知 Cholesky 降阶在数值上不如更新稳定。这表明要么是术语错误，要么是对数值方法的误解。\n3.  **稳定性**: 该程序依赖于重复的随机试验，没有像重新初始化到已知良好状态那样的最终故障安全措施，这违反了事实5。\n\n这个程序不稳健；它基于一个随机的启发式方法，并且在数值实现方面使用了有问题的术语。\n**结论：不正确**\n\n**结论**\n选项A和C都描述了确保提议协方差矩阵保持正定的有效、数值稳健的程序。选项A基于计算上高效的 Cholesky 分解测试。选项C基于计算成本更高但诊断能力更强的特征值分析。两者都采用了正确且标准的纠正和稳定性原则。因此，它们都是正确答案。",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "理解的最终检验是付诸实践。这项编程练习  要求你构建一个随机游走Metropolis采样器，其提议尺度根据预设的规则进行自适应调整。你将在具有不同特征的目标分布上测试其性能，并使用有效样本量 ($ESS$) 来量化其效率，从而将“递减自适应”等理论概念与具体的性能指标联系起来。",
            "id": "3287303",
            "problem": "考虑马尔可夫链蒙特卡洛 (MCMC) 中的随机游走 Metropolis (RWM) 算法。设实线上的目标密度为一个对数可积的概率密度函数 $p(x)$，并考虑形式为 $x^{\\prime} = x + \\eta$ 的提议，其中 $\\eta \\sim \\mathcal{N}(0,\\sigma_n^2)$，其尺度 $\\sigma_n$ 随时间变化。在第 $n$ 步，RWM 的接受概率由 Metropolis–Hastings 法则给出，即 $a(x,x^{\\prime}) = \\min\\{1, p(x^{\\prime})/p(x)\\}$。我们研究形式为 $\\sigma_n = c \\, n^{-\\alpha}$ 的自适应方案，其中 $c \\gt 0$ 且 $\\alpha \\gt 0$。自适应 MCMC 中的递减自适应条件要求连续转移核之间的差异收敛于零，对于 RWM，这可以通过分析 $|\\sigma_{n+1} - \\sigma_n|$ 是否收敛于零来进行。\n\n我们将有限计算预算下的近似最优效率定义为估计器有效样本量 (ESS) 的最大化，其中 ESS 与积分自相关时间 (IACT) 成反比。对于状态的标量函数 $f(x)$，其样本为 $\\{f(X_i)\\}_{i=1}^N$，IACT 定义为 $\\tau = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k$，其中 $\\rho_k$ 是平稳序列 $\\{f(X_i)\\}$ 的滞后 k 阶自相关。ESS 为 $N / \\tau$。在固定的迭代预算下，更高的 ESS 表示更好的近似最优效率。\n\n您的任务是确定在给定的有限集合中，哪个 $\\alpha$ 在满足递减自适应的数值检验的同时，能够近似地最大化效率，并测试其对目标平滑度的敏感性。请基于以下基础进行操作：\n- 用于构建具有平稳密度 $p(x)$ 的可逆马尔可夫链的 Metropolis–Hastings 框架。\n- 自适应 MCMC 的递减自适应定义，即自适应幅度趋于消失，此处近似为 $|\\sigma_{n+1} - \\sigma_n| \\to 0$。\n- IACT 与 ESS 之间的关系，即 $ESS = N/\\tau$，其中 $\\tau$ 是根据样本自相关估计的。\n- 在适当的遍历性条件下，适用于 MCMC 均值的标准大数定律，这为在固定的有限范围内基于 ESS 进行比较提供了依据。\n\n您必须完全在代码中实现以下实验：\n\n1) 捕获不同平滑度机制的目标：\n- 平滑、强对数凹：标准正态目标，密度为 $p_{\\mathrm{G}}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\{-x^2/2\\}$。\n- 众数处对数密度不平滑：拉普拉斯目标，密度为 $p_{\\mathrm{L}}(x) = \\frac{1}{2}\\exp\\{-|x|\\}$。\n- 平滑但重尾：自由度为 $\\nu = 3$ 的学生 t 分布，其密度为 $p_{\\mathrm{T}}(x) = \\frac{\\Gamma((\\nu+1)/2)}{\\sqrt{\\nu\\pi}\\,\\Gamma(\\nu/2)}\\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\nu+1)/2}$，其中 $\\nu = 3$。\n\n2) 所有目标共享的算法设置：\n- 提议尺度方案 $\\sigma_n = c \\, n^{-\\alpha}$，其中 $c = 1.5$，$\\alpha$ 从有限集合 $\\{0.2, 0.5, 0.8\\}$ 中选择。\n- 总迭代次数 $T = 20000$，预烧期 $B = 2000$。\n- 初始状态 $x_0 = 0$。\n- 使用给定的带有对称高斯提议的 RWM 算法。\n- 为评估效率，使用函数 $f(x) = x$。\n- 使用基于经验自相关序列 $\\{\\rho_k\\}_{k \\ge 1}$ 的初始正序列规则的一致性滞后窗方法来估计积分自相关时间 $\\tau$。\n- 计算 ESS 为 $N/\\tau$，其中 $N = T - B$。\n\n3) 递减自适应数值验证：\n- 对于给定的 $\\alpha$，计算终端自适应幅度 $\\Delta_T = |\\sigma_{T+1} - \\sigma_T|$，并要求 $\\Delta_T  \\epsilon$，其中 $\\epsilon = 10^{-5}$。只有满足此检验的 $\\alpha$才有资格被该目标选中。\n\n4) 选择标准：\n- 对于每个目标，从符合条件的 $\\alpha$ 中，选择使 ESS 最大化的 $\\alpha$。如果在 ESS 上出现数值上的平局（在 $10^{-3}$ 的相对容差内），则在平局的值中选择最小的 $\\alpha$。\n\n5) 测试套件：\n- 目标集：按顺序为上面指定的 $\\{p_{\\mathrm{G}}, p_{\\mathrm{L}}, p_{\\mathrm{T}}\\}$。\n- 候选 $\\alpha$ 集：$\\{0.2, 0.5, 0.8\\}$。\n- 超参数：$c = 1.5$, $T = 20000$, $B = 2000$, $\\epsilon = 10^{-5}$, $x_0 = 0$。\n\n6) 最终输出格式：\n- 您的程序应生成单行输出，其中包含所选的 $\\alpha$ 值，每个目标一个，按 $\\{p_{\\mathrm{G}}, p_{\\mathrm{L}}, p_{\\mathrm{T}}\\}$ 的顺序排列，形式为用方括号括起来的逗号分隔列表，每个 $\\alpha$ 四舍五入到一位小数，例如 $[0.2,0.5,0.2]$。\n- 此问题不涉及物理单位，也未使用角度。\n\n您的实现必须通过设置您选择的显式随机数生成器种子来确保完全确定性。解决方案应有原则且可复现。\n\n您的交付物是：\n- 一个完整、可运行的程序，该程序执行所述的 RWM 模拟，为每个候选 $\\alpha$ 计算 ESS，验证递减自适应检查，根据标准为每个目标选择 $\\alpha$，并以确切的所需格式打印最终列表。\n- 按规定格式输出的单行程序结果。",
            "solution": "该问题要求进行一项数值实验，以确定随机游走 Metropolis (RWM) 算法的最优自适应率参数 $\\alpha$。优化在有限的迭代次数内执行，并针对三种不同的目标概率分布进行评估，每种分布都对采样器构成了独特的挑战。\n\n### 原理与方法\n\n问题的核心在于计算统计学中三个关键概念的交叉点：Metropolis-Hastings 算法、自适应马尔可夫链蒙特卡洛 (MCMC) 以及采样器效率的评估。\n\n**1. 随机游走 Metropolis (RWM) 算法**\n\nRWM 是 Metropolis-Hastings 算法的一个特例，用于从密度为 $p(x)$ 的目标概率分布中生成一系列样本 $\\{X_0, X_1, \\dots, X_T\\}$。在每一步 $n$，从以当前状态 $x_n$ 为中心的对称分布中提议一个候选状态 $x^{\\prime}$。问题指定了一个高斯提议：\n$$\nx^{\\prime} = x_n + \\eta, \\quad \\text{其中 } \\eta \\sim \\mathcal{N}(0, \\sigma_n^2)\n$$\n该候选状态以概率 $a(x_n, x^{\\prime})$ 被接受，该概率由以下公式给出：\n$$\na(x_n, x^{\\prime}) = \\min\\left\\{1, \\frac{p(x^{\\prime})}{p(x_n)}\\right\\} = \\min\\left\\{1, \\exp(\\log p(x^{\\prime}) - \\log p(x_n))\\right\\}\n$$\n如果提议被接受，则 $X_{n+1} = x^{\\prime}$；否则，提议被拒绝，且 $X_{n+1} = x_n$。RWM 算法的性能严重依赖于提议尺度 $\\sigma_n$。一个小的 $\\sigma_n$ 会导致高接受率但状态空间探索缓慢，而一个大的 $\\sigma_n$ 会导致频繁拒绝，使得马尔可夫链几乎不移动。\n\n**2. 自适应 MCMC 与递减自适应**\n\n找到一个最优的固定 $\\sigma$ 可能很困难。自适应 MCMC 通过允许 $\\sigma_n$ 在模拟过程中改变来解决这个问题，从而在运行时学习一个合适的值。问题为提议尺度指定了一个幂律衰减方案：\n$$\n\\sigma_n = c \\, n^{-\\alpha}\n$$\n其中 $c  0$ 是一个初始尺度因子，$\\alpha  0$ 控制衰减速率。为了使自适应 MCMC 算法在理论上有效（即，其经验均值能收敛到正确的期望值），自适应过程必须随时间递减。这被称为“递减自适应”条件。形式上，它要求连续转移核之间的全变差距离收敛到零。对于给定的 RWM 方案，一个必要（尽管不总是充分）的条件是自适应参数的变化幅度趋于消失，即 $|\\sigma_{n+1} - \\sigma_n| \\to 0$。\n\n对于方案 $\\sigma_n = c n^{-\\alpha}$，我们有：\n$$\n|\\sigma_{n+1} - \\sigma_n| = |c(n+1)^{-\\alpha} - c n^{-\\alpha}| = c n^{-\\alpha} |(1+1/n)^{-\\alpha} - 1|\n$$\n对于大的 $n$ 使用泰勒展开，$(1+1/n)^{-\\alpha} \\approx 1 - \\alpha/n$，因此我们有：\n$$\n|\\sigma_{n+1} - \\sigma_n| \\approx c n^{-\\alpha} \\left|-\\frac{\\alpha}{n}\\right| = c\\alpha n^{-(\\alpha+1)}\n$$\n对于任何 $\\alpha  0$，这个量都收敛于零。问题要求在有限的范围 $T$ 内对此条件进行数值检验，即要求终端自适应幅度 $\\Delta_T = |\\sigma_{T+1} - \\sigma_T|$ 小于阈值 $\\epsilon = 10^{-5}$。只有满足此检验的 $\\alpha$ 值才被视为合格。较大的 $\\alpha$ 会使自适应过程更快地递减，从而更强地满足该条件，但可能在运行早期就将尺度 $\\sigma_n$ “冻结”在一个次优值上。\n\n**3. 采样器效率：IACT 与 ESS**\n\nMCMC 采样器的效率通过它从目标分布中产生独立样本的速度来衡量。由于 MCMC 样本是相关的，我们使用有效样本量 (ESS) 作为度量标准。给定 $N$ 个预烧期后的样本，ESS 定义为：\n$$\n\\text{ESS} = \\frac{N}{\\tau}\n$$\n其中 $\\tau$ 是积分自相关时间 (IACT)，定义为：\n$$\n\\tau = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k\n$$\n这里，$\\rho_k$ 是状态函数平稳序列 $\\{f(X_i)\\}$ 的滞后 k 阶自相关。较小的 IACT 意味着样本之间的相关性较低，从而有更高的 ESS 和更好的效率。任务是最大化函数 $f(x) = x$ 的 ESS。IACT 必须从有限的样本链中估计。我们使用“初始正序列”方法，该方法通过对经验自相关进行求和来估计 $\\tau$，只要它们保持为正值。这是一种常见的启发式方法，用于避免来自高阶滞后估计的噪声。\n\n### 实验设计与实现\n\n对于三种目标分布（正态、拉普拉斯、学生 t）中的每一种，实验按以下步骤进行：\n\n1.  **资格检验**：对于每个候选 $\\alpha \\in \\{0.2, 0.5, 0.8\\}$，使用 $c=1.5$ 和 $T=20000$ 计算终端自适应幅度 $\\Delta_T = |\\sigma_{T+1} - \\sigma_T|$。如果 $\\Delta_T \\ge 10^{-5}$，则丢弃该 $\\alpha$。\n\n2.  **模拟**：对于每个合格的 $\\alpha$，从 $x_0=0$ 开始运行 RWM 模拟，共进行 $T=20000$ 次迭代。第 $n$ 步的提议尺度为 $\\sigma_n = 1.5 \\times n^{-\\alpha}$。前 $B=2000$ 个样本作为预烧期被丢弃。为确保公平比较，每次模拟都重新设定随机数生成器的种子。\n\n3.  **效率计算**：使用 $N = T-B = 18000$ 个预烧期后的样本链来估计 $f(x)=x$ 的 IACT。这涉及计算经验自相关函数 (ACF) 并对初始正项求和。然后将 ESS 计算为 $N/\\text{IACT}$。\n\n4.  **选择**：在计算完所有合格 $\\alpha$ 值的 ESS 后，选择最优的 $\\alpha$。主要标准是选择产生最大 ESS 的 $\\alpha$。指定了一条平局决胜规则：如果多个 $\\alpha$ 值产生的 ESS 值在最大 ESS 的 $10^{-3}$ 相对容差范围内，则选择其中最小的 $\\alpha$。\n\n对三种目标密度中的每一种重复此过程，选择这些密度是为了测试算法在具有不同平滑度和尾部权重特性的目标上的性能：\n-   **高斯分布 ($p_G$)**：平滑且强对数凹，是许多采样器的理想情况。\n-   **拉普拉斯分布 ($p_L$)**：在众数处有一个不可微的峰值，对依赖梯度的算法构成挑战。\n-   **学生 t 分布 ($p_T$，$\\nu=3$)**：平滑但比高斯分布具有更重的尾部，这可能使探索变得困难。\n\n最终输出是为高斯、拉普拉斯和学生 t 目标按此顺序选择的 $\\alpha$ 值的列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ... # No other libraries needed\n\ndef solve():\n    \"\"\"\n    Performs an MCMC experiment to find the optimal adaptation rate alpha\n    for a Random-Walk Metropolis sampler on different target distributions.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = {\n        # Hyperparameters\n        \"SEED\": 42,\n        \"C\": 1.5,\n        \"ALPHA_CANDIDATES\": [0.2, 0.5, 0.8],\n        \"T\": 20000,\n        \"B\": 2000,\n        \"X0\": 0.0,\n        \"EPSILON\": 1e-5,\n        \"REL_TOL\": 1e-3,\n        \"NU_T\": 3.0\n    }\n\n    # Unnormalized log-probability density functions for the targets.\n    # Constant terms are omitted as they cancel in the M-H ratio.\n    def logp_normal(x):\n        return -0.5 * x**2\n\n    def logp_laplace(x):\n        return -np.abs(x)\n\n    def logp_student_t(x, nu=test_cases[\"NU_T\"]):\n        return -((nu + 1) / 2.0) * np.log(1.0 + x**2 / nu)\n\n    targets = [\n        {'name': 'Gaussian', 'log_p': logp_normal},\n        {'name': 'Laplace', 'log_p': logp_laplace},\n        {'name': 'Student-t', 'log_p': logp_student_t}\n    ]\n\n    def calculate_ess(samples):\n        \"\"\"\n        Calculates the Effective Sample Size (ESS) from a sequence of samples.\n        IACT is estimated using the initial positive sequence rule.\n        \"\"\"\n        n_samples = len(samples)\n        if n_samples  2:\n            return 0.0\n\n        demeaned_samples = samples - np.mean(samples)\n        \n        # Calculate autocovariance using FFT for efficiency\n        n_fft = 1  (2 * n_samples - 1).bit_length() # Next power of 2\n        fft_samples = np.fft.fft(demeaned_samples, n_fft)\n        autocov = np.fft.ifft(fft_samples * np.conj(fft_samples)).real\n        autocov = autocov[:n_samples] / n_samples\n        \n        if autocov[0] == 0:\n            return 0.0\n\n        acf = autocov / autocov[0]\n        \n        # Estimate IACT using the initial positive sequence rule\n        iact = 1.0\n        # Sum positive correlations up to a max lag for robustness\n        max_lag = n_samples // 5 \n        for k in range(1, min(len(acf), max_lag)):\n            if acf[k]  0:\n                iact += 2.0 * acf[k]\n            else:\n                break\n        \n        return n_samples / iact if iact  0 else 0.0\n\n    def rwm_sampler(log_p, x0, t_total, t_burn, c_adapt, alpha_adapt, rng):\n        \"\"\"\n        Implements the adaptive Random-Walk Metropolis sampler.\n        \"\"\"\n        chain = np.zeros(t_total)\n        x_current = x0\n        log_p_current = log_p(x_current)\n        \n        for n in range(1, t_total + 1):\n            sigma_n = c_adapt * n**(-alpha_adapt)\n            proposal = x_current + rng.normal(loc=0.0, scale=sigma_n)\n            \n            log_p_proposal = log_p(proposal)\n            log_acceptance_ratio = log_p_proposal - log_p_current\n            \n            if np.log(rng.uniform(0.0, 1.0))  log_acceptance_ratio:\n                x_current = proposal\n                log_p_current = log_p_proposal\n            \n            chain[n-1] = x_current\n            \n        return chain[t_burn:]\n\n    results = []\n    \n    C = test_cases[\"C\"]\n    T = test_cases[\"T\"]\n    B = test_cases[\"B\"]\n    X0 = test_cases[\"X0\"]\n    EPSILON = test_cases[\"EPSILON\"]\n    REL_TOL = test_cases[\"REL_TOL\"]\n    ALPHA_CANDIDATES = test_cases[\"ALPHA_CANDIDATES\"]\n    SEED = test_cases[\"SEED\"]\n\n    for target_spec in targets:\n        log_p_func = target_spec['log_p']\n        ess_results = []\n        \n        for alpha in ALPHA_CANDIDATES:\n            # 1. Diminishing Adaptation numerical verification\n            sigma_T = C * T**(-alpha)\n            sigma_T_plus_1 = C * (T + 1)**(-alpha)\n            delta_T = np.abs(sigma_T_plus_1 - sigma_T)\n            \n            if delta_T = EPSILON:\n                continue\n\n            # 2. Run MCMC simulation. Reset seed for each run for fair comparison.\n            rng = np.random.default_rng(seed=SEED)\n            samples = rwm_sampler(log_p_func, X0, T, B, C, alpha, rng)\n            \n            # 3. Calculate ESS\n            ess = calculate_ess(samples)\n            ess_results.append((alpha, ess))\n            \n        # 4. Selection criterion: find best alpha\n        if not ess_results:\n            # This should not happen since all alphas pass the DA check.\n            # Handle defensively by appending a placeholder.\n            results.append(np.nan) \n            continue\n\n        max_ess = -1.0\n        for _, ess in ess_results:\n            if ess  max_ess:\n                max_ess = ess\n        \n        # Identify alphas within the tolerance of the max ESS\n        tied_alphas = []\n        for alpha, ess in ess_results:\n            if ess = max_ess * (1.0 - REL_TOL):\n                tied_alphas.append(alpha)\n        \n        # Choose the smallest alpha among the tied values\n        best_alpha = min(tied_alphas)\n        results.append(best_alpha)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.1f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}