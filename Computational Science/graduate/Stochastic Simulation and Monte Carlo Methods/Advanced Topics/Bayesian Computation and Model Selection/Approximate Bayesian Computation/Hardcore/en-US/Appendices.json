{
    "hands_on_practices": [
        {
            "introduction": "While Approximate Bayesian Computation (ABC) is often termed \"likelihood-free,\" it implicitly constructs an approximation of the likelihood function. This exercise  delves into this relationship by demonstrating how, under a Gaussian assumption for the summary statistics, the general ABC framework converges to a specific parametric form known as the synthetic likelihood. You will also derive the essential Monte Carlo estimators for the mean and covariance, which are the building blocks for implementing this powerful ABC variant.",
            "id": "3288741",
            "problem": "Let $X$ denote data generated under a parametric model indexed by parameter $\\theta \\in \\Theta \\subset \\mathbb{R}^{p}$, and let $s(X) \\in \\mathbb{R}^{d}$ be a fixed, low-dimensional summary statistic. Suppose that for each $\\theta$, the distribution of $s(X)\\mid \\theta$ is well-approximated by a multivariate normal distribution with mean $\\mu_{\\theta} \\in \\mathbb{R}^{d}$ and positive-definite covariance matrix $\\Sigma_{\\theta} \\in \\mathbb{R}^{d \\times d}$. You observe a single dataset $X_{\\mathrm{obs}}$ with observed summary $s_{\\mathrm{obs}} = s(X_{\\mathrm{obs}})$. Consider Approximate Bayesian Computation (ABC) based on summaries with a smoothing kernel $K_{\\epsilon}(\\cdot)$, tolerance $\\epsilon > 0$, and prior density $\\pi(\\theta)$. The ABC summary-based likelihood is the kernel convolution\n$$\nL_{\\mathrm{ABC},\\epsilon}(\\theta) \\propto \\int_{\\mathbb{R}^{d}} K_{\\epsilon}\\!\\left(s - s_{\\mathrm{obs}}\\right)\\, p\\!\\left(s \\mid \\theta\\right)\\, ds,\n$$\nwhere $p\\!\\left(s \\mid \\theta\\right)$ is the sampling density of $s(X)\\mid \\theta$. Let $\\phi(\\cdot;\\mu,\\Sigma)$ denote the $d$-variate Gaussian density with mean $\\mu$ and covariance $\\Sigma$.\n\nTasks:\n1) Starting from the definition of $L_{\\mathrm{ABC},\\epsilon}(\\theta)$ and the property that Gaussian kernels form an approximate identity as $\\epsilon \\to 0$, show that if $s(X)\\mid \\theta$ is approximately Gaussian with mean $\\mu_{\\theta}$ and covariance $\\Sigma_{\\theta}$, then the synthetic likelihood\n$$\nL_{\\mathrm{SL}}(\\theta) \\equiv \\phi\\!\\left(s_{\\mathrm{obs}};\\mu_{\\theta},\\Sigma_{\\theta}\\right)\n$$\nserves as a parametric Approximate Bayesian Computation (ABC) likelihood in the limit $\\epsilon \\to 0$.\n2) Now suppose that for a fixed $\\theta$, you can perform independent simulations to obtain $m \\in \\mathbb{N}$ independent replicates $s^{(1)},\\dots,s^{(m)}$ from the distribution of $s(X)\\mid \\theta$. Using only the definitions of expectation and covariance, derive Monte Carlo estimators $\\widehat{\\mu}_{\\theta}$ and $\\widehat{\\Sigma}_{\\theta}$ based on $\\{s^{(i)}\\}_{i=1}^{m}$ that are unbiased for $\\mu_{\\theta}$ and $\\Sigma_{\\theta}$ under exact Gaussianity of $s(X)\\mid \\theta$; express your answer in closed form using $\\{s^{(i)}\\}_{i=1}^{m}$ and $m$. You may treat $d$ as fixed and finite, and you may assume the usual regularity conditions ensuring the interchange of integration and limits for approximate identities.\n\nYour final answer must consist of a single row matrix containing, in order: the explicit closed-form expression for $L_{\\mathrm{SL}}(\\theta)$ as a function of $s_{\\mathrm{obs}},\\mu_{\\theta},\\Sigma_{\\theta}$, followed by $\\widehat{\\mu}_{\\theta}$, followed by $\\widehat{\\Sigma}_{\\theta}$. No numerical evaluation is required, and no rounding is needed. All symbols must be defined in your derivation. Do not include any units in your final answer.",
            "solution": "The problem consists of two parts. The first part requires demonstrating that the synthetic likelihood, $L_{\\mathrm{SL}}(\\theta)$, can be viewed as a limiting case of the Approximate Bayesian Computation (ABC) likelihood, $L_{\\mathrm{ABC},\\epsilon}(\\theta)$. The second part asks for the derivation of unbiased estimators for the mean and covariance of a Gaussian-distributed summary statistic, based on a set of simulated replicates.\n\n**Part 1: The Synthetic Likelihood as a Limiting ABC Likelihood**\n\nThe problem provides the definition of the ABC summary-based likelihood as a kernel convolution:\n$$\nL_{\\mathrm{ABC},\\epsilon}(\\theta) \\propto \\int_{\\mathbb{R}^{d}} K_{\\epsilon}\\!\\left(s - s_{\\mathrm{obs}}\\right)\\, p\\!\\left(s \\mid \\theta\\right)\\, ds\n$$\nHere, $p(s \\mid \\theta)$ is the sampling density of the summary statistic $s(X)$ given the parameter $\\theta$, $s_{\\mathrm{obs}}$ is the observed summary statistic, and $K_{\\epsilon}$ is a smoothing kernel with tolerance $\\epsilon > 0$.\n\nThe problem states that we should use a Gaussian kernel that forms an approximate identity as $\\epsilon \\to 0$. An approximate identity is a sequence of functions $\\{K_{\\epsilon}\\}_{\\epsilon > 0}$ that, in the limit $\\epsilon \\to 0$, converges to a Dirac delta function $\\delta$. For any well-behaved function $f$, the convolution with an approximate identity has the property:\n$$\n\\lim_{\\epsilon \\to 0} \\int_{\\mathbb{R}^{d}} K_{\\epsilon}(x-y) f(y) dy = f(x)\n$$\nIn our case, the function is the likelihood of the summary statistics, $f(s) = p(s \\mid \\theta)$, and we are evaluating the convolution at $s_{\\mathrm{obs}}$. Assuming the kernel is symmetric, $K_{\\epsilon}(u) = K_{\\epsilon}(-u)$, the integral can be written as:\n$$\n\\int_{\\mathbb{R}^{d}} K_{\\epsilon}\\!\\left(s - s_{\\mathrm{obs}}\\right)\\, p\\!\\left(s \\mid \\theta\\right)\\, ds = \\int_{\\mathbb{R}^{d}} K_{\\epsilon}\\!\\left(s_{\\mathrm{obs}} - s\\right)\\, p\\!\\left(s \\mid \\theta\\right)\\, ds\n$$\nThis is the convolution $(K_{\\epsilon} * p(\\cdot \\mid \\theta))(s_{\\mathrm{obs}})$. As $\\epsilon \\to 0$, the properties of the approximate identity imply that this convolution converges to the function $p(\\cdot \\mid \\theta)$ evaluated at $s_{\\mathrm{obs}}$.\n$$\n\\lim_{\\epsilon \\to 0} L_{\\mathrm{ABC},\\epsilon}(\\theta) \\propto \\lim_{\\epsilon \\to 0} \\int_{\\mathbb{R}^{d}} K_{\\epsilon}\\!\\left(s - s_{\\mathrm{obs}}\\right)\\, p\\!\\left(s \\mid \\theta\\right)\\, ds = p(s_{\\mathrm{obs}} \\mid \\theta)\n$$\nThis shows that in the limit of zero tolerance, the ABC likelihood becomes proportional to the true likelihood of the summary statistic.\n\nThe problem then introduces the assumption that the distribution of $s(X) \\mid \\theta$ is well-approximated by a $d$-variate normal distribution with mean $\\mu_{\\theta}$ and covariance $\\Sigma_{\\theta}$. This means we can approximate the sampling density $p(s \\mid \\theta)$ with a Gaussian probability density function $\\phi(s; \\mu_{\\theta}, \\Sigma_{\\theta})$:\n$$\np(s \\mid \\theta) \\approx \\phi(s; \\mu_{\\theta}, \\Sigma_{\\theta})\n$$\nSubstituting this approximation into our limiting result yields:\n$$\n\\lim_{\\epsilon \\to 0} L_{\\mathrm{ABC},\\epsilon}(\\theta) \\propto \\phi(s_{\\mathrm{obs}}; \\mu_{\\theta}, \\Sigma_{\\theta})\n$$\nThe expression on the right is precisely the definition of the synthetic likelihood, $L_{\\mathrm{SL}}(\\theta)$.\n$$\nL_{\\mathrm{SL}}(\\theta) \\equiv \\phi(s_{\\mathrm{obs}}; \\mu_{\\theta}, \\Sigma_{\\theta})\n$$\nThus, the synthetic likelihood serves as a parametric approximation to the ABC likelihood in the limit as $\\epsilon \\to 0$.\n\nThe explicit closed-form expression for the $d$-variate Gaussian density $\\phi(x; \\mu, \\Sigma)$ is:\n$$\n\\phi(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2} \\det(\\Sigma)^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right)\n$$\nTherefore, the synthetic likelihood is:\n$$\nL_{\\mathrm{SL}}(\\theta) = \\frac{1}{(2\\pi)^{d/2} \\det(\\Sigma_{\\theta})^{1/2}} \\exp\\left(-\\frac{1}{2}(s_{\\mathrm{obs}}-\\mu_{\\theta})^T \\Sigma_{\\theta}^{-1} (s_{\\mathrm{obs}}-\\mu_{\\theta})\\right)\n$$\n\n**Part 2: Derivation of Unbiased Estimators**\n\nWe are given $m$ independent and identically distributed (i.i.d.) replicates $s^{(1)}, \\dots, s^{(m)}$ drawn from the distribution of $s(X)\\mid \\theta$. For this part, we assume this distribution is exactly Gaussian, $s^{(i)} \\sim \\mathcal{N}(\\mu_{\\theta}, \\Sigma_{\\theta})$. We need to derive unbiased estimators for $\\mu_{\\theta}$ and $\\Sigma_{\\theta}$.\n\n**Unbiased Estimator for the Mean $\\mu_{\\theta}$**\nThe standard estimator for the population mean is the sample mean. We define the estimator $\\widehat{\\mu}_{\\theta}$ as:\n$$\n\\widehat{\\mu}_{\\theta} = \\frac{1}{m} \\sum_{i=1}^{m} s^{(i)}\n$$\nTo show it is unbiased, we must show that its expected value is $\\mu_{\\theta}$. Using the linearity of expectation:\n$$\nE[\\widehat{\\mu}_{\\theta}] = E\\left[\\frac{1}{m} \\sum_{i=1}^{m} s^{(i)}\\right] = \\frac{1}{m} \\sum_{i=1}^{m} E[s^{(i)}]\n$$\nSince each $s^{(i)}$ is drawn from a distribution with mean $\\mu_{\\theta}$, we have $E[s^{(i)}] = \\mu_{\\theta}$ for all $i \\in \\{1, \\dots, m\\}$.\n$$\nE[\\widehat{\\mu}_{\\theta}] = \\frac{1}{m} \\sum_{i=1}^{m} \\mu_{\\theta} = \\frac{1}{m} (m \\mu_{\\theta}) = \\mu_{\\theta}\n$$\nThis confirms that $\\widehat{\\mu}_{\\theta}$ is an unbiased estimator of $\\mu_{\\theta}$.\n\n**Unbiased Estimator for the Covariance $\\Sigma_{\\theta}$**\nThe standard unbiased estimator for the population covariance is the sample covariance matrix, which includes Bessel's correction. We define the estimator $\\widehat{\\Sigma}_{\\theta}$ as:\n$$\n\\widehat{\\Sigma}_{\\theta} = \\frac{1}{m-1} \\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T\n$$\nTo prove its unbiasedness, we compute its expectation. Let's first analyze the sum $\\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T$. We can rewrite the term inside the summation by adding and subtracting the true mean $\\mu_{\\theta}$:\n$$\ns^{(i)} - \\widehat{\\mu}_{\\theta} = (s^{(i)} - \\mu_{\\theta}) - (\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})\n$$\nThe sum becomes:\n\\begin{align*}\n\\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T &= \\sum_{i=1}^{m} \\left[(s^{(i)} - \\mu_{\\theta}) - (\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})\\right]\\left[(s^{(i)} - \\mu_{\\theta}) - (\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})\\right]^T \\\\\n&= \\sum_{i=1}^{m} (s^{(i)} - \\mu_{\\theta})(s^{(i)} - \\mu_{\\theta})^T - \\sum_{i=1}^m (s^{(i)} - \\mu_{\\theta})(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})^T \\\\\n& \\quad - \\sum_{i=1}^m (\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})(s^{(i)} - \\mu_{\\theta})^T + \\sum_{i=1}^m (\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})^T\n\\end{align*}\nWe note that $\\sum_{i=1}^{m} (s^{(i)} - \\mu_{\\theta}) = m(\\frac{1}{m}\\sum s^{(i)}) - m\\mu_{\\theta} = m(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})$. Using this, the expression simplifies to:\n$$\n\\sum_{i=1}^{m} (s^{(i)} - \\mu_{\\theta})(s^{(i)} - \\mu_{\\theta})^T - m(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})^T\n$$\nNow, we take the expectation of this expression:\n$$\nE\\left[ \\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T \\right] = E\\left[\\sum_{i=1}^{m} (s^{(i)} - \\mu_{\\theta})(s^{(i)} - \\mu_{\\theta})^T\\right] - m E\\left[(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})^T\\right]\n$$\nThe first term on the right-hand side is:\n$$\nE\\left[\\sum_{i=1}^{m} (s^{(i)} - \\mu_{\\theta})(s^{(i)} - \\mu_{\\theta})^T\\right] = \\sum_{i=1}^{m} E\\left[(s^{(i)} - \\mu_{\\theta})(s^{(i)} - \\mu_{\\theta})^T\\right] = \\sum_{i=1}^{m} \\text{Cov}(s^{(i)}) = \\sum_{i=1}^{m} \\Sigma_{\\theta} = m\\Sigma_{\\theta}\n$$\nThe second term involves the covariance matrix of the sample mean, $\\text{Cov}(\\widehat{\\mu}_{\\theta}) = E\\left[(\\widehat{\\mu}_{\\theta} - E[\\widehat{\\mu}_{\\theta}])(\\widehat{\\mu}_{\\theta} - E[\\widehat{\\mu}_{\\theta}])^T\\right] = E\\left[(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})^T\\right]$.\n$$\n\\text{Cov}(\\widehat{\\mu}_{\\theta}) = \\text{Cov}\\left(\\frac{1}{m}\\sum_{i=1}^{m} s^{(i)}\\right) = \\frac{1}{m^2}}\\text{Cov}\\left(\\sum_{i=1}^{m} s^{(i)}\\right)\n$$\nSince the samples $s^{(i)}$ are independent, the covariance of the sum is the sum of covariances:\n$$\n\\text{Cov}(\\widehat{\\mu}_{\\theta}) = \\frac{1}{m^2} \\sum_{i=1}^{m} \\text{Cov}(s^{(i)}) = \\frac{1}{m^2} \\sum_{i=1}^{m} \\Sigma_{\\theta} = \\frac{1}{m^2}(m\\Sigma_{\\theta}) = \\frac{1}{m}\\Sigma_{\\theta}\n$$\nSubstituting these results back, we get:\n$$\nE\\left[ \\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T \\right] = m\\Sigma_{\\theta} - m\\left(\\frac{1}{m}\\Sigma_{\\theta}\\right) = (m-1)\\Sigma_{\\theta}\n$$\nTherefore, the expectation of our proposed estimator $\\widehat{\\Sigma}_{\\theta}$ is:\n$$\nE[\\widehat{\\Sigma}_{\\theta}] = E\\left[ \\frac{1}{m-1} \\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T \\right] = \\frac{1}{m-1} E\\left[ \\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T \\right] = \\frac{1}{m-1} (m-1)\\Sigma_{\\theta} = \\Sigma_{\\theta}\n$$\nThis confirms that $\\widehat{\\Sigma}_{\\theta}$ is an unbiased estimator of $\\Sigma_{\\theta}$.\n\nThe final answer consists of the closed-form expression for $L_{\\mathrm{SL}}(\\theta)$, the estimator $\\widehat{\\mu}_{\\theta}$, and the estimator $\\widehat{\\Sigma}_{\\theta}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{(2\\pi)^{d/2} \\det(\\Sigma_{\\theta})^{1/2}} \\exp\\left(-\\frac{1}{2}(s_{\\mathrm{obs}}-\\mu_{\\theta})^T \\Sigma_{\\theta}^{-1} (s_{\\mathrm{obs}}-\\mu_{\\theta})\\right) & \\frac{1}{m}\\sum_{i=1}^{m} s^{(i)} & \\frac{1}{m-1}\\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The performance of Approximate Bayesian Computation hinges critically on the choice of summary statistics, as they must capture sufficient information about the parameters of interest. This problem  provides a hands-on comparison between moment-based and quantile-based summaries within a formal decision-theoretic framework. By deriving the Bayes risk for each, you will develop a rigorous understanding of how the statistical efficiency of a summary statistic directly influences the quality and uncertainty of the resulting posterior approximation.",
            "id": "3288801",
            "problem": "Consider a data-generating process where $Y_{1},\\dots,Y_{n}$ are independent and identically distributed random variables from a lognormal distribution with parameters $\\mu$ and $\\sigma^{2}$, meaning $\\ln(Y_{i}) \\equiv Z_{i} \\sim \\mathcal{N}(\\mu,\\sigma^{2})$, where $\\sigma^{2}$ is known and $\\mu$ is unknown. Suppose the prior for $\\mu$ is Gaussian, $\\mu \\sim \\mathcal{N}(m_{0},v_{0})$, with known $m_{0}$ and $v_{0}>0$. Approximate Bayesian computation (ABC) is performed with a symmetric kernel and tolerance parameter that is taken to the limit of zero, so that accepted samples from the simulator satisfy an exact match on a chosen low-dimensional summary statistic. Consider two ABC schemes that adopt different summaries computed on the log-transformed data:\n- A moment-based summary $s_{\\mathrm{mom}} = \\bar{Z} = \\frac{1}{n}\\sum_{i=1}^{n} Z_{i}$.\n- A quantile-based summary $s_{\\mathrm{quant}} = \\tilde{Z}$, the sample median of $\\{Z_{1},\\dots,Z_{n}\\}$.\n\nUnder quadratic loss for estimating $\\mu$, $L(\\hat{\\mu},\\mu) = (\\hat{\\mu} - \\mu)^{2}$, the Bayes estimator is the posterior mean, and the Bayes risk equals the posterior variance averaged over the prior predictive distribution. Assume the large-sample sampling distributions for the summaries are used in the ABC conditioning step: the moment-based summary satisfies $\\bar{Z} \\stackrel{\\mathrm{approx}}{\\sim} \\mathcal{N}(\\mu,\\sigma^{2}/n)$, and the quantile-based summary satisfies $\\tilde{Z} \\stackrel{\\mathrm{approx}}{\\sim} \\mathcal{N}\\!\\left(\\mu,\\frac{\\pi \\sigma^{2}}{2n}\\right)$.\n\nWithin this setup, derive the analytic expression for the minimum Bayes risk, under quadratic loss, achieved by the ABC posterior mean when choosing between these two summaries. Your final answer must be a single closed-form expression in terms of $n$, $v_{0}$, and $\\sigma^{2}$, and should not include any inequalities or equations. No numerical rounding is required.",
            "solution": "The problem asks for the minimum Bayes risk for estimating the parameter $\\mu$ under a quadratic loss function, $L(\\hat{\\mu}, \\mu) = (\\hat{\\mu} - \\mu)^2$, by choosing between two different summary statistics within an Approximate Bayesian Computation (ABC) framework. The ABC tolerance is taken to zero, which means the ABC posterior distribution for $\\mu$ converges to the exact posterior distribution conditional on the chosen summary statistic, $p(\\mu|s)$.\n\nUnder quadratic loss, the Bayes estimator is the posterior mean, $\\hat{\\mu}_{\\text{Bayes}} = \\mathbb{E}[\\mu|s]$. The risk associated with a specific observation of the summary statistic $s$ is the posterior variance, $\\text{Var}(\\mu|s)$. The overall Bayes risk, $R$, is the expected value of this posterior variance, taken over the marginal (prior predictive) distribution of the summary statistic, $p(s)$:\n$$ R = \\mathbb{E}_{s}[\\text{Var}(\\mu|s)] = \\int \\text{Var}(\\mu|s) p(s) \\,ds $$\n\nLet's first derive a general expression for the Bayes risk for a generic summary statistic $s$ whose sampling distribution is Gaussian. The problem sets up a conjugate Gaussian-Gaussian model.\nThe prior distribution for $\\mu$ is given as:\n$$ \\mu \\sim \\mathcal{N}(m_{0}, v_{0}) $$\nThis corresponds to a probability density function $p(\\mu) \\propto \\exp\\left(-\\frac{(\\mu-m_0)^2}{2v_0}\\right)$.\n\nThe likelihood of $\\mu$ given a summary statistic $s$ is based on the large-sample approximation of the sampling distribution of $s$. Let's assume a general form:\n$$ s | \\mu \\sim \\mathcal{N}(\\mu, \\tau^{2}) $$\nThis corresponds to a likelihood function $p(s|\\mu) \\propto \\exp\\left(-\\frac{(s-\\mu)^2}{2\\tau^2}\\right)$.\n\nThe posterior distribution $p(\\mu|s)$ is proportional to the product of the likelihood and the prior, $p(\\mu|s) \\propto p(s|\\mu)p(\\mu)$.\n$$ p(\\mu|s) \\propto \\exp\\left(-\\frac{(s-\\mu)^2}{2\\tau^2}\\right) \\exp\\left(-\\frac{(\\mu-m_0)^2}{2v_0}\\right) $$\nThe exponent is a quadratic in $\\mu$, which indicates the posterior is also Gaussian. We can find the parameters of the posterior distribution by completing the square for $\\mu$ in the exponent. The posterior precision (inverse variance) is the sum of the prior precision and the data precision:\n$$ \\frac{1}{\\text{Var}(\\mu|s)} = \\frac{1}{v_0} + \\frac{1}{\\tau^2} $$\nThus, the posterior variance is:\n$$ \\text{Var}(\\mu|s) = \\left(\\frac{1}{v_0} + \\frac{1}{\\tau^2}\\right)^{-1} = \\frac{v_0 \\tau^2}{v_0 + \\tau^2} $$\nCritically, the posterior variance does not depend on the specific observed value of the summary statistic $s$. Therefore, the expectation over $p(s)$ is trivial. The Bayes risk $R$ is simply equal to the posterior variance:\n$$ R(\\tau^2) = \\mathbb{E}_{s}\\left[\\frac{v_0 \\tau^2}{v_0 + \\tau^2}\\right] = \\frac{v_0 \\tau^2}{v_0 + \\tau^2} $$\nTo find the minimum Bayes risk, we must choose the summary statistic that minimizes this expression. Let's analyze the behavior of $R(\\tau^2)$ as a function of the sampling variance $\\tau^2$. We take the derivative with respect to $\\tau^2$:\n$$ \\frac{dR}{d(\\tau^2)} = \\frac{v_0(v_0 + \\tau^2) - v_0 \\tau^2(1)}{(v_0 + \\tau^2)^2} = \\frac{v_0^2}{(v_0 + \\tau^2)^2} $$\nSince $v_0 > 0$ is given, $v_0^2 > 0$, and the denominator is always positive. Thus, $\\frac{dR}{d(\\tau^2)} > 0$, which means the Bayes risk $R$ is a strictly monotonically increasing function of the summary statistic's sampling variance $\\tau^2$. To minimize the Bayes risk, we must choose the summary statistic with the smaller sampling variance.\n\nNow we evaluate the sampling variances for the two proposed summaries.\n1.  Moment-based summary: $s_{\\mathrm{mom}} = \\bar{Z}$.\n    The sampling distribution is given as $\\bar{Z} \\stackrel{\\mathrm{approx}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2/n)$. The sampling variance is:\n    $$ \\tau_{\\mathrm{mom}}^2 = \\frac{\\sigma^2}{n} $$\n2.  Quantile-based summary: $s_{\\mathrm{quant}} = \\tilde{Z}$.\n    The sampling distribution is given as $\\tilde{Z} \\stackrel{\\mathrm{approx}}{\\sim} \\mathcal{N}(\\mu, \\frac{\\pi \\sigma^2}{2n})$. The sampling variance is:\n    $$ \\tau_{\\mathrm{quant}}^2 = \\frac{\\pi \\sigma^2}{2n} $$\n\nWe now compare these two variances:\n$$ \\tau_{\\mathrm{mom}}^2 = \\frac{\\sigma^2}{n} \\quad \\text{and} \\quad \\tau_{\\mathrm{quant}}^2 = \\frac{\\pi}{2} \\cdot \\frac{\\sigma^2}{n} $$\nThe comparison hinges on the value of the constant $\\frac{\\pi}{2}$. Using the value $\\pi \\approx 3.14159$, we have $\\frac{\\pi}{2} \\approx 1.5708$.\nSince $\\frac{\\pi}{2} > 1$, it follows directly that:\n$$ \\tau_{\\mathrm{quant}}^2 > \\tau_{\\mathrm{mom}}^2 $$\nThe moment-based summary statistic, $\\bar{Z}$, has a smaller sampling variance than the quantile-based summary statistic, $\\tilde{Z}$. This is consistent with the fact that for a Gaussian sample, the sample mean is the minimum variance unbiased estimator of the population mean and is more efficient than the sample median.\n\nBecause the Bayes risk $R(\\tau^2)$ is an increasing function of $\\tau^2$, the minimum risk is achieved by using the summary with the minimum variance. Therefore, the optimal choice between the two is the moment-based summary, $s_{\\mathrm{mom}}$.\n\nThe minimum Bayes risk is obtained by substituting $\\tau^2 = \\tau_{\\mathrm{mom}}^2 = \\frac{\\sigma^2}{n}$ into our general expression for the risk:\n$$ R_{\\min} = R(\\tau_{\\mathrm{mom}}^2) = \\frac{v_0 (\\frac{\\sigma^2}{n})}{v_0 + \\frac{\\sigma^2}{n}} $$\nSimplifying this expression gives the final result:\n$$ R_{\\min} = \\frac{\\frac{v_0\\sigma^2}{n}}{\\frac{n v_0 + \\sigma^2}{n}} = \\frac{v_0 \\sigma^2}{n v_0 + \\sigma^2} $$\nThis expression represents the minimum Bayes risk under quadratic loss achievable by choosing between the two specified summary statistics.",
            "answer": "$$\\boxed{\\frac{v_{0}\\sigma^{2}}{nv_{0} + \\sigma^{2}}}$$"
        },
        {
            "introduction": "An essential, yet often overlooked, step in any ABC analysis is the validation of the approximation itself. This practical coding exercise  guides you through the process of building a formal calibration test to assess the quality of an ABC posterior predictive distribution. By comparing the ABC-generated distribution to the known exact distribution using the Cramer-von Mises statistic, you will gain direct experience in quantifying approximation error, a crucial skill for ensuring the reliability of your computational inferences.",
            "id": "3288776",
            "problem": "You are to construct a calibration test for Approximate Bayesian Computation (ABC) posterior predictive checks. The objective is to compare the empirical distribution of replicated data drawn from the ABC posterior predictive process to the empirical distribution of replicated data drawn from the exact posterior predictive process, and to quantify deviations using the Cramer–von Mises statistic.\n\nThe fundamental base of this task comprises the following well-tested formulas and definitions:\n\n- Bayes' rule and conjugate prior-posterior relationships for a Poisson model: Let the data be $y_1, y_2, \\dots, y_n$ with $y_i \\sim \\mathrm{Poisson}(\\theta)$, and a prior $\\theta \\sim \\mathrm{Gamma}(\\alpha, \\beta)$ in the shape–rate parameterization. The posterior is $\\theta \\mid y \\sim \\mathrm{Gamma}(\\alpha', \\beta')$ with $\\alpha' = \\alpha + \\sum_{i=1}^n y_i$ and $\\beta' = \\beta + n$.\n- The posterior predictive distribution for a single future observation $y^{\\mathrm{rep}}$ is the Poisson–Gamma mixture: equivalently, sample $\\lambda \\sim \\mathrm{Gamma}(\\alpha', \\beta')$ and then let $y^{\\mathrm{rep}} \\sim \\mathrm{Poisson}(\\lambda)$ to obtain exact posterior predictive samples.\n- Approximate Bayesian Computation (ABC) with rejection: propose $\\theta^{(j)} \\sim \\mathrm{Gamma}(\\alpha, \\beta)$, simulate a dataset $y^{(j)} = (y_1^{(j)}, \\dots, y_n^{(j)})$ from the model, compute a summary $s(y^{(j)})$, and accept $\\theta^{(j)}$ if a discrepancy $d(s(y^{(j)}), s(y))$ is less than a threshold $\\varepsilon$. Using accepted $\\theta$ draws, simulate $y^{\\mathrm{rep}} \\sim \\mathrm{Poisson}(\\theta)$ to approximate the posterior predictive distribution.\n- The Cramer–von Mises (CvM) two-sample statistic: given two samples $x_1, \\dots, x_m$ and $z_1, \\dots, z_n$, let $N = m + n$, and let $\\{w_{(i)}\\}_{i=1}^N$ be the pooled ordered values of $\\{x_j\\}$ and $\\{z_k\\}$. Define the empirical cumulative distribution functions $F_m$ and $G_n$. The two-sample Cramer–von Mises statistic is\n$$\nT_{\\mathrm{CvM}} = \\frac{mn}{N^2} \\sum_{i=1}^N \\left( F_m(w_{(i)}) - G_n(w_{(i)}) \\right)^2,\n$$\nwhere $F_m(w)$ is the proportion of $\\{x_j\\}$ less than or equal to $w$, and $G_n(w)$ is the proportion of $\\{z_k\\}$ less than or equal to $w$.\n\nModel and algorithm specification to implement:\n\n1. Data generating process: For each test case, generate observed data $y_1, \\dots, y_n$ as independent $\\mathrm{Poisson}(\\theta_{\\mathrm{true}})$ counts using a fixed random seed $123$ to ensure reproducibility.\n2. Prior: Use $\\theta \\sim \\mathrm{Gamma}(\\alpha, \\beta)$ with shape–rate parameters $(\\alpha, \\beta)$.\n3. ABC posterior via rejection:\n   - Summary: $s(y) = \\frac{1}{n} \\sum_{i=1}^n y_i$.\n   - Discrepancy: $d(s_{\\mathrm{sim}}, s_{\\mathrm{obs}}) = \\left| s_{\\mathrm{sim}} - s_{\\mathrm{obs}} \\right|$.\n   - Threshold: accept if $d(s_{\\mathrm{sim}}, s_{\\mathrm{obs}}) \\le \\varepsilon$.\n   - Proposal and simulation: draw $N_{\\mathrm{prior}}$ independent proposals $\\theta^{(j)} \\sim \\mathrm{Gamma}(\\alpha, \\beta)$; for each, simulate $n$ independent $\\mathrm{Poisson}(\\theta^{(j)})$ observations and compute $s_{\\mathrm{sim}}$.\n   - Accepted parameter set: $\\{\\theta^{(j)} : d \\le \\varepsilon\\}$.\n4. ABC posterior predictive sample: For each accepted $\\theta^{(j)}$, draw one $y^{\\mathrm{rep}}_j \\sim \\mathrm{Poisson}(\\theta^{(j)})$; this yields $m$ ABC posterior predictive replicates where $m$ is the number of accepted proposals.\n5. Exact posterior predictive sample: Compute $(\\alpha', \\beta')$ from the observed data, draw $m$ independent $\\lambda_j \\sim \\mathrm{Gamma}(\\alpha', \\beta')$, and then draw $z_j \\sim \\mathrm{Poisson}(\\lambda_j)$ to obtain $m$ exact posterior predictive replicates. Use the same number $m$ of exact replicates as the number of accepted ABC replicates to facilitate the two-sample comparison.\n6. Compute the two-sample Cramer–von Mises statistic $T_{\\mathrm{CvM}}$ between the ABC predictive sample and the exact predictive sample for each test case.\n\nTest suite:\n\nUse the following three test cases to assess calibration under different regimes. For all test cases, use the fixed random seed $123$.\n\n- Case $1$ (happy path): $(n, \\alpha, \\beta, \\theta_{\\mathrm{true}}, \\varepsilon, N_{\\mathrm{prior}}) = (50, 2.0, 1.0, 3.0, 0.12, 20000)$.\n- Case $2$ (more diffuse data): $(n, \\alpha, \\beta, \\theta_{\\mathrm{true}}, \\varepsilon, N_{\\mathrm{prior}}) = (30, 2.0, 1.0, 1.5, 0.20, 20000)$.\n- Case $3$ (larger sample size, tighter tolerance): $(n, \\alpha, \\beta, \\theta_{\\mathrm{true}}, \\varepsilon, N_{\\mathrm{prior}}) = (100, 2.0, 1.0, 4.0, 0.10, 25000)$.\n\nAnswer specification and required output format:\n\n- For each test case, compute a single real number equal to the Cramer–von Mises two-sample statistic $T_{\\mathrm{CvM}}$ comparing the ABC posterior predictive sample to the exact posterior predictive sample.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[t_1,t_2,t_3]$). No other text should be printed.\n\nAll quantities are counts or dimensionless; no physical units are involved. Angles are not used. Express all numerical outputs as decimal floating-point numbers.\n\nImplementation constraints:\n\n- Use a fixed random seed equal to $123$ for all random number generation to ensure reproducibility.\n- The program must be self-contained, require no input, and run to completion under the specified environment.",
            "solution": "The problem requires the construction and execution of a calibration test for an Approximate Bayesian Computation (ABC) procedure. The core of the task is to quantify the difference between a posterior predictive distribution approximated by ABC and the corresponding exact posterior predictive distribution. This is achieved by computing the Cramer-von Mises (CvM) two-sample statistic. The model is a Poisson likelihood with a conjugate Gamma prior, which allows for the analytical derivation of the exact posterior and, consequently, the exact posterior predictive distribution, providing a gold standard for comparison.\n\nFirst, let us formally define the statistical model and the distributions involved.\nThe observed data are a set of $n$ independent and identically distributed counts $y = \\{y_1, y_2, \\dots, y_n\\}$, where each observation is drawn from a Poisson distribution with an unknown rate parameter $\\theta$:\n$$y_i \\mid \\theta \\sim \\mathrm{Poisson}(\\theta)$$\nThe parameter $\\theta$ itself is assumed to follow a Gamma distribution, which serves as the prior. The Gamma prior, in its shape-rate parameterization, is given by:\n$$\\theta \\sim \\mathrm{Gamma}(\\alpha, \\beta)$$\nwhere $\\alpha$ is the shape parameter and $\\beta$ is the rate parameter. The probability density function is $f(\\theta; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\theta^{\\alpha-1} e^{-\\beta\\theta}$.\n\nDue to the conjugacy of the Gamma prior to the Poisson likelihood, the posterior distribution of $\\theta$ given the data $y$ is also a Gamma distribution. The posterior parameters, denoted $\\alpha'$ and $\\beta'$, are updated as follows:\n$$\\alpha' = \\alpha + \\sum_{i=1}^n y_i$$\n$$\\beta' = \\beta + n$$\nThus, the exact posterior distribution is $\\theta \\mid y \\sim \\mathrm{Gamma}(\\alpha', \\beta')$.\n\nThe posterior predictive distribution is the distribution of a new, unobserved data point, $y^{\\mathrm{rep}}$, after integrating over the uncertainty in the parameter $\\theta$ as described by its posterior distribution. The exact posterior predictive distribution is formally defined as:\n$$p(y^{\\mathrm{rep}} \\mid y) = \\int p(y^{\\mathrm{rep}} \\mid \\theta) p(\\theta \\mid y) \\, d\\theta$$\nIn this case, this corresponds to a Poisson-Gamma mixture model, also known as the Negative Binomial distribution. A sample from this distribution can be generated via a two-step hierarchical process:\n$1$. Draw a parameter value $\\lambda$ from the posterior distribution: $\\lambda \\sim \\mathrm{Gamma}(\\alpha', \\beta')$.\n$2$. Draw a replicated data point $y^{\\mathrm{rep}}$ from a Poisson distribution with that parameter: $y^{\\mathrm{rep}} \\sim \\mathrm{Poisson}(\\lambda)$.\nRepeating this process yields a sample from the exact posterior predictive distribution.\n\nThe problem then specifies an ABC procedure to approximate the posterior distribution. This is a simulation-based method that bypasses the explicit calculation of the likelihood function. The ABC rejection algorithm proceeds as follows:\n$1$. Propose a parameter $\\theta^{(j)}$ from the prior distribution: $\\theta^{(j)} \\sim \\mathrm{Gamma}(\\alpha, \\beta)$.\n$2$. Simulate a dataset $y^{(j)} = \\{y_1^{(j)}, \\dots, y_n^{(j)}\\}$ from the likelihood with the proposed parameter: $y_i^{(j)} \\sim \\mathrm{Poisson}(\\theta^{(j)})$.\n$3$. Compute a summary statistic for both the observed data, $s_{\\mathrm{obs}} = s(y)$, and the simulated data, $s_{\\mathrm{sim}} = s(y^{(j)})$. The specified summary statistic is the sample mean, $s(y) = \\frac{1}{n} \\sum_{i=1}^n y_i$.\n$4$. Calculate a discrepancy measure between the summary statistics, $d(s_{\\mathrm{sim}}, s_{\\mathrm{obs}}) = |s_{\\mathrm{sim}} - s_{\\mathrm{obs}}|$.\n$5$. If the discrepancy $d$ is less than or equal to a tolerance threshold $\\varepsilon$, i.e., $d \\le \\varepsilon$, the proposed parameter $\\theta^{(j)}$ is accepted as an approximate draw from the posterior distribution. Otherwise, it is rejected.\nThis process is repeated for a large number of proposals, $N_{\\mathrm{prior}}$, to form a set of accepted parameters, $\\{\\theta_k^{\\mathrm{acc}}\\}_{k=1}^m$, where $m$ is the total number of accepted samples. This set constitutes an empirical approximation of the posterior distribution $\\theta \\mid y$.\n\nThe ABC posterior predictive sample is then generated by taking each accepted parameter $\\theta_k^{\\mathrm{acc}}$ and drawing one new data point $y_k^{\\mathrm{rep}} \\sim \\mathrm{Poisson}(\\theta_k^{\\mathrm{acc}})$. This produces the sample $\\{y_k^{\\mathrm{rep}}\\}_{k=1}^m$.\n\nThe final step is to compare the ABC posterior predictive sample with the exact posterior predictive sample. Let the ABC predictive sample of size $m$ be $X = \\{x_1, \\dots, x_m\\}$ and the exact predictive sample of the same size $m$ be $Z = \\{z_1, \\dots, z_m\\}$. The problem mandates the use of the two-sample Cramer-von Mises (CvM) statistic to quantify the difference between their empirical distributions.\nLet $F_m$ and $G_m$ be the empirical cumulative distribution functions (ECDFs) of samples $X$ and $Z$, respectively. Let $\\{w_{(i)}\\}_{i=1}^N$ be the pooled and sorted values of the combined samples, where $N = 2m$. The CvM statistic is given by the formula:\n$$\nT_{\\mathrm{CvM}} = \\frac{m \\cdot m}{(m+m)^2} \\sum_{i=1}^{N} \\left( F_m(w_{(i)}) - G_m(w_{(i)}) \\right)^2 = \\frac{1}{4} \\sum_{i=1}^{2m} \\left( F_m(w_{(i)}) - G_m(w_{(i)}) \\right)^2\n$$\nHere, $F_m(w)$ is the proportion of elements in $X$ that are less than or equal to $w$, and $G_m(w)$ is the proportion of elements in $Z$ that are less than or equal to $w$. A smaller value of $T_{\\mathrm{CvM}}$ indicates a better match between the ABC approximation and the exact distribution.\n\nThe computational procedure proceeds as follows for each test case, using a fixed random seed of $123$ for full reproducibility:\n$1$. Set the parameters $(n, \\alpha, \\beta, \\theta_{\\mathrm{true}}, \\varepsilon, N_{\\mathrm{prior}})$ for the specific case.\n$2$. Generate the observed dataset $y_{\\mathrm{obs}}$ of size $n$ by drawing from $\\mathrm{Poisson}(\\theta_{\\mathrm{true}})$.\n$3$. Execute the ABC rejection algorithm for $N_{\\mathrm{prior}}$ proposals to obtain a set of $m$ accepted parameters $\\{\\theta_k^{\\mathrm{acc}}\\}$.\n$4$. Generate the ABC posterior predictive sample of size $m$ by drawing one $\\mathrm{Poisson}(\\theta_k^{\\mathrm{acc}})$ for each accepted parameter.\n$5$. Calculate the parameters of the exact posterior, $\\alpha' = \\alpha + \\sum y_i$ and $\\beta' = \\beta + n$.\n$6$. Generate the exact posterior predictive sample of size $m$ by drawing $\\lambda_k \\sim \\mathrm{Gamma}(\\alpha', \\beta')$ and then $z_k \\sim \\mathrm{Poisson}(\\lambda_k)$ for $k=1, \\dots, m$.\n$7$. Compute the $T_{\\mathrm{CvM}}$ statistic between the two predictive samples using the formula specified.\nThis entire process is repeated for the three test cases provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs the ABC calibration test for the three specified cases and prints the results.\n    \"\"\"\n    # Use a fixed random seed for reproducibility, as required.\n    seed = 123\n    rng = np.random.default_rng(seed)\n\n    # Test suite parameters:\n    # (n, alpha, beta, theta_true, epsilon, N_prior)\n    test_cases = [\n        (50, 2.0, 1.0, 3.0, 0.12, 20000),\n        (30, 2.0, 1.0, 1.5, 0.20, 20000),\n        (100, 2.0, 1.0, 4.0, 0.10, 25000),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        # The rng object is passed to ensure the random number stream is\n        # consumed sequentially across test cases, maintaining reproducibility.\n        cvm_statistic = run_simulation_case(case_params, rng)\n        results.append(cvm_statistic)\n\n    # Print the final results in the specified format: [t_1,t_2,t_3]\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation_case(case_params, rng):\n    \"\"\"\n    Executes the full simulation and comparison for a single test case.\n\n    Args:\n        case_params (tuple): A tuple containing the parameters\n                             (n, alpha, beta, theta_true, epsilon, N_prior).\n        rng (numpy.random.Generator): The random number generator instance.\n\n    Returns:\n        float: The calculated Cramer-von Mises statistic.\n    \"\"\"\n    n, alpha, beta, theta_true, epsilon, N_prior = case_params\n\n    # Step 1: Generate observed data\n    # Generate n i.i.d. Poisson samples for the \"observed\" data.\n    y_obs = rng.poisson(theta_true, size=n)\n    s_obs = np.mean(y_obs)\n\n    # Step 2: ABC posterior via rejection\n    # Propose N_prior parameters from the prior distribution.\n    theta_proposals = rng.gamma(shape=alpha, scale=1.0/beta, size=N_prior)\n\n    # Vectorized simulation for efficiency:\n    # For each proposed theta, simulate a dataset of size n.\n    # The shape of y_sim will be (N_prior, n).\n    y_sim = rng.poisson(theta_proposals[:, np.newaxis], size=(N_prior, n))\n    \n    # Compute the summary statistic (mean) for each simulated dataset.\n    s_sim = np.mean(y_sim, axis=1)\n\n    # Apply the ABC acceptance criterion.\n    discrepancy = np.abs(s_sim - s_obs)\n    accepted_mask = discrepancy <= epsilon\n    accepted_thetas = theta_proposals[accepted_mask]\n\n    m_acc = len(accepted_thetas)\n    # If no samples are accepted, the discrepancy cannot be measured.\n    # According to the problem's well-posed nature, this is not expected.\n    # We return 0.0, indicating no deviation between empty sets.\n    if m_acc == 0:\n        return 0.0\n\n    # Step 3: ABC posterior predictive sample\n    # For each accepted theta, draw one posterior predictive replicate.\n    abc_pred_sample = rng.poisson(accepted_thetas)\n\n    # Step 4: Exact posterior predictive sample\n    # Compute the parameters of the exact Gamma posterior.\n    alpha_prime = alpha + np.sum(y_obs)\n    beta_prime = beta + n\n\n    # Draw m_acc parameters from the exact posterior.\n    exact_posterior_thetas = rng.gamma(shape=alpha_prime, scale=1.0/beta_prime, size=m_acc)\n    # For each, draw one exact posterior predictive replicate.\n    exact_pred_sample = rng.poisson(exact_posterior_thetas)\n\n    # Step 5: Compute the Cramer-von Mises statistic\n    sample_1 = abc_pred_sample\n    sample_2 = exact_pred_sample\n    n_1 = len(sample_1)\n    n_2 = len(sample_2)\n    N = n_1 + n_2\n\n    # Pool and sort the observations from both samples.\n    w_pooled_sorted = np.sort(np.concatenate((sample_1, sample_2)))\n\n    # Sort each sample individually to compute ECDFs efficiently.\n    sample_1_sorted = np.sort(sample_1)\n    sample_2_sorted = np.sort(sample_2)\n\n    # Compute the ECDF values for each sample at each point in the pooled data.\n    # np.searchsorted gives the number of elements <= value, which is what we need for the ECDF.\n    ecdf_1 = np.searchsorted(sample_1_sorted, w_pooled_sorted, side='right') / n_1\n    ecdf_2 = np.searchsorted(sample_2_sorted, w_pooled_sorted, side='right') / n_2\n\n    # Calculate the sum of squared differences of the ECDFs.\n    sum_sq_diff = np.sum((ecdf_1 - ecdf_2)**2)\n\n    # Apply the full formula for the CvM statistic.\n    T_cvm = (n_1 * n_2 / N**2) * sum_sq_diff\n    \n    return T_cvm\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}