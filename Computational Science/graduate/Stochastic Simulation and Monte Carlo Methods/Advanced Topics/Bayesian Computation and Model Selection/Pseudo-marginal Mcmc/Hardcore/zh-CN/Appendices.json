{
    "hands_on_practices": [
        {
            "introduction": "伪边际方法的核心在于将随机性引入了Metropolis-Hastings接受步骤。本练习将对此随机性进行基础分析。我们将探讨似然估计量的方差（此处通过对数正态噪声建模）如何直接影响接受概率的分布。通过从第一性原理推导期望接受率，您将对“为何噪声过大的估计量会有损采样器效率”这一伪边际算法调优中的关键概念，建立起坚实的数学理解。",
            "id": "3332960",
            "problem": "考虑一个伪边际马尔可夫链蒙特卡洛 (MCMC) 算法，该算法使用一个通过乘性对数正态噪声实现的似然函数的无偏正估计量。设 Metropolis–Hastings (MH) 提议产生一个确定性的精确对数比 $r \\in \\mathbb{R}$，该对数比为目标分布与提议分布的反向/正向比率乘积的对数。并假设当前和提议的似然估计量使用独立的对数噪声变量 $\\epsilon$ 和 $\\epsilon'$，它们同分布于 $\\mathcal{N}(-\\sigma^{2}/2,\\sigma^{2})$，且 $\\sigma^{2}>0$。伪边际接受概率满足\n$$\n\\alpha \\;=\\; \\min\\{1, \\exp(r + \\epsilon' - \\epsilon)\\}.\n$$\n从 MH 算法和伪边际构造（即似然估计量在自然尺度上的无偏性）的第一性原理出发，显式地推导随机接受概率 $\\alpha$ 的分布，包括其在1处的任何点质量以及在 $(0,1)$ 上的密度函数。然后，计算期望 $\\mathbb{E}[\\alpha]$ 的精确解析表达式，并检验其对 $\\sigma^{2}$ 的依赖性，刻画其在 $\\sigma^{2} \\to 0$ 和 $\\sigma^{2} \\to \\infty$ 时的极限行为。在适当的情况下，你可以使用标准正态累积分布函数 (CDF) $\\Phi(\\cdot)$ 和概率密度函数 (PDF) $\\varphi(\\cdot)$，并且必须以闭合形式表示所有结果。\n\n请提供用 $r$ 和 $\\sigma$ 表示的 $\\mathbb{E}[\\alpha]$ 的精确闭合形式解析表达式作为最终答案。无需进行数值舍入。",
            "solution": "伪边际构造用一个正的无偏估计量 $\\widehat{L}(\\theta)$ 来替代精确的似然函数 $L(\\theta)$，使得 $\\mathbb{E}[\\widehat{L}(\\theta)] = L(\\theta)$。一种常见的做法是写成 $\\widehat{L}(\\theta) = L(\\theta)\\exp(\\epsilon)$ 的形式，其中 $\\epsilon$ 是均值为 $-\\sigma^{2}/2$、方差为 $\\sigma^{2}$ 的高斯变量，这确保了 $\\mathbb{E}[\\exp(\\epsilon)] = \\exp(-\\sigma^{2}/2 + \\sigma^{2}/2) = 1$，从而使得在自然尺度上的无偏性成立。\n\n给定一个提议，精确的 Metropolis–Hastings (MH) 接受率为 $\\exp(r)$，其中 $r$ 是精确的对数比。在伪边际 MH 算法中，接受率受到独立似然估计量之比的扰动，因此对数比变为\n$$\nR_{\\log} \\;=\\; r + \\epsilon' - \\epsilon.\n$$\n由于 $\\epsilon$ 和 $\\epsilon'$ 独立同分布于 $\\mathcal{N}(-\\sigma^{2}/2,\\sigma^{2})$，它们的差\n$$\nZ \\;=\\; \\epsilon' - \\epsilon\n$$\n是一个均值为 $0$、方差为 $2\\sigma^{2}$ 的高斯变量，即 $Z \\sim \\mathcal{N}(0,2\\sigma^{2})$。因此接受概率为\n$$\n\\alpha \\;=\\; \\min\\{1,\\exp(r+Z)\\}.\n$$\n\n为了推导 $\\alpha$ 的分布，我们观察到当 $r+Z \\geq 0$ 时，即当 $Z \\geq -r$ 时，有 $\\alpha=1$。否则，当 $r+Z  0$ 时，有 $\\alpha = \\exp(r+Z) \\in (0,1)$。为简洁起见，令 $v = 2\\sigma^{2}$。则 $Z \\sim \\mathcal{N}(0,v)$，并且在 1 处的点质量的概率为\n$$\n\\mathbb{P}(\\alpha = 1) \\;=\\; \\mathbb{P}(Z \\geq -r) \\;=\\; 1 - \\Phi\\!\\left(\\frac{-r}{\\sqrt{v}}\\right) \\;=\\; \\Phi\\!\\left(\\frac{r}{\\sqrt{v}}\\right) \\;=\\; \\Phi\\!\\left(\\frac{r}{\\sqrt{2}\\,\\sigma}\\right),\n$$\n其中 $\\Phi$ 是标准正态累积分布函数。在连续部分 $(0,1)$ 上，当 $r+Z0$ 时，$Z$ 和 $\\alpha$ 之间的映射关系为 $\\alpha = \\exp(r+Z) \\in (0,1)$，因此 $Z = \\ln(\\alpha) - r$。$Z$ 的密度函数为\n$$\nf_{Z}(z) \\;=\\; \\frac{1}{\\sqrt{2\\pi v}}\\exp\\!\\left(-\\frac{z^{2}}{2v}\\right) \\;=\\; \\frac{1}{\\sqrt{4\\pi \\sigma^{2}}}\\exp\\!\\left(-\\frac{z^{2}}{4\\sigma^{2}}\\right).\n$$\n通过变量替换，对于 $\\alpha \\in (0,1)$，\n$$\nf_{\\alpha}(\\alpha) \\;=\\; f_{Z}\\!\\left(\\ln(\\alpha)-r\\right)\\left|\\frac{d}{d\\alpha}\\big(\\ln(\\alpha)-r\\big)\\right|\n\\;=\\; \\frac{1}{\\sqrt{4\\pi \\sigma^{2}}}\\exp\\!\\left(-\\frac{(\\ln(\\alpha)-r)^{2}}{4\\sigma^{2}}\\right)\\cdot \\frac{1}{\\alpha}.\n$$\n因此，接受概率 $\\alpha$ 具有一个混合分布，它由一个在 1 处、概率为 $\\Phi(r/(\\sqrt{2}\\,\\sigma))$ 的点质量，以及一个在 $(0,1)$ 上、由下式给出的连续密度函数组成：\n$$\nf_{\\alpha}(\\alpha) \\;=\\; \\frac{1}{\\alpha\\,\\sqrt{4\\pi \\sigma^{2}}}\\exp\\!\\left(-\\frac{(\\ln(\\alpha)-r)^{2}}{4\\sigma^{2}}\\right), \\qquad 0  \\alpha  1.\n$$\n\n我们现在计算期望 $\\mathbb{E}[\\alpha]$。可以写出：\n$$\n\\mathbb{E}[\\alpha] \\;=\\; \\mathbb{E}\\big[\\min\\{1,\\exp(r+Z)\\}\\big] \\;=\\; \\mathbb{P}(r+Z \\geq 0) \\;+\\; \\mathbb{E}\\big[\\exp(r+Z)\\,\\mathbf{1}\\{r+Z0\\}\\big].\n$$\n第一项是 $\\Phi\\!\\left(r/\\sqrt{v}\\right)$。对于第二项，注意\n$$\n\\mathbb{E}\\big[\\exp(r+Z)\\,\\mathbf{1}\\{Z  -r\\}\\big] \\;=\\; \\exp(r)\\,\\mathbb{E}\\big[\\exp(Z)\\,\\mathbf{1}\\{Z  -r\\}\\big].\n$$\n被积函数 $\\mathbb{E}\\big[\\exp(Z)\\,\\mathbf{1}\\{Z  -r\\}\\big]$ 是一个关于截断正态分布的标准结果。对于 $Z \\sim \\mathcal{N}(0,v)$，该期望等于 $\\exp(v/2) \\Phi(\\frac{-r-v}{\\sqrt{v}})$。\n将此结果代回，第二项变为 $\\exp(r)\\exp(v/2)\\,\\Phi\\!\\left(\\frac{-r-v}{\\sqrt{v}}\\right) = \\exp(r+v/2)\\,\\Phi\\!\\left(\\frac{-r-v}{\\sqrt{v}}\\right)$。\n因此，总期望为：\n$$\n\\mathbb{E}[\\alpha] \\;=\\; \\Phi\\!\\left(\\frac{r}{\\sqrt{v}}\\right) + \\exp(r+v/2)\\,\\Phi\\!\\left(\\frac{-r-v}{\\sqrt{v}}\\right).\n$$\n将 $v=2\\sigma^2$ 代入，我们得到最终的表达式。在 $\\sigma^2 \\to 0$ 时，$v \\to 0$，噪声消失，$\\mathbb{E}[\\alpha] \\to \\min\\{1, e^r\\}$。在 $\\sigma^2 \\to \\infty$ 时，$v \\to \\infty$，$\\mathbb{E}[\\alpha] \\to 0$（对于固定的 $r$）。",
            "answer": "$$\\boxed{\\Phi\\!\\left(\\frac{r}{\\sqrt{2}\\,\\sigma}\\right) + \\exp\\!\\left(r+\\sigma^{2}\\right)\\,\\Phi\\!\\left(\\frac{-r-2\\sigma^{2}}{\\sqrt{2}\\,\\sigma}\\right)}$$"
        },
        {
            "introduction": "在理解了估计量方差至关重要之后，一个自然的问题随之而来：为什么不直接用更多的蒙特卡洛样本，让估计量尽可能精确呢？本练习旨在解决统计效率与计算成本之间的关键权衡。我们将分析一个通过平均多个估计量来降低方差，但同时线性增加计算成本的场景。这项实践揭示了伪边际MCMC的一个关键且时而反直觉的原则：简单地降低估计量方差并非总是最高效的策略。您将学习如何优化单位计算努力下的性能，这是实际应用中一项至关重要的技能。",
            "id": "3333004",
            "problem": "考虑一种伪边际马尔可夫链蒙特卡洛（MCMC）方法，该方法针对一个具有未知似然的后验密度，并在一个提议参数 $\\theta'$ 处使用似然的无偏估计量。假设单个Metropolis-Hastings（MH）步骤使用对称提议，因此Hastings修正被抵消。我们关注这样一个步骤，其中提议状态和当前状态之间的真实对数后验差是一个固定值 $\\Delta = -1$。在当前点和提议点，似然估计量都是通过对 $m$ 个独立同分布的无偏估计量进行平均来构建的。假设以下科学上标准的近似成立：根据中心极限定理和Delta方法，在任何固定参数值处，平均似然估计量的对数误差近似服从均值为 $0$、方差为 $\\sigma^{2}/m$ 的高斯分布。进一步假设在当前点和提议点的误差是独立的，并且 $\\sigma^{2} = 1$。\n\n在这些假设下，提议移动的伪边际接受概率可以写成 $\\alpha = \\min\\{1, \\exp(\\Delta + Z)\\}$，其中 $Z$ 是来自提议点和当前点的两个独立高斯对数估计误差之差。在此设定下：\n\n- 推导期望接受概率 $A(m) = \\mathbb{E}[\\alpha]$ 作为整数 $m \\geq 1$ 的函数的精确闭式表达式。\n- 定义使用 $m$ 个平均值的计算成本与 $m$ 成正比，并将单位成本效率定义为 $\\mathrm{Eff}(m) = A(m)/m$。\n- 确定使 $\\mathrm{Eff}(m)$ 最大化的整数 $m$。\n\n你的最终答案必须是使 $\\mathrm{Eff}(m)$ 最大化的单个整数。无需四舍五入。将最终答案表示为一个不带单位的整数。",
            "solution": "问题要求找到估计量的整数个数 $m$，以最大化一个伪边际MCMC步骤的效率。效率被定义为每单位计算成本的期望接受概率。\n\n首先，我们将问题的各个组成部分形式化。\n单个Metropolis-Hastings步骤的接受概率由 $\\alpha = \\min\\{1, \\exp(\\Delta + Z)\\}$ 给出，其中 $\\Delta = -1$ 是真实的对数后验差。\n项 $Z$ 代表对数似然比估计中的噪声。它是两个独立同分布（i.i.d.）随机变量之差，每个随机变量代表平均似然估计量的对数误差。\n设提议点的误差为 $\\epsilon_{\\text{prop}}$，当前点的误差为 $\\epsilon_{\\text{curr}}$。问题指出，根据中心极限定理和Delta方法，这些误差近似服从均值为 $0$、方差为 $\\frac{\\sigma^2}{m}$ 的高斯分布。\n给定 $\\sigma^2 = 1$，我们有 $\\epsilon_{\\text{prop}} \\sim \\mathcal{N}(0, 1/m)$ 和 $\\epsilon_{\\text{curr}} \\sim \\mathcal{N}(0, 1/m)$。\n变量 $Z$ 是它们的差，$Z = \\epsilon_{\\text{prop}} - \\epsilon_{\\text{curr}}$。由于它们是独立的，$Z$ 也是一个高斯随机变量。\n$Z$ 的均值是 $\\mathbb{E}[Z] = \\mathbb{E}[\\epsilon_{\\text{prop}}] - \\mathbb{E}[\\epsilon_{\\text{curr}}] = 0 - 0 = 0$。\n$Z$ 的方差是 $\\mathrm{Var}(Z) = \\mathrm{Var}(\\epsilon_{\\text{prop}}) + \\mathrm{Var}(\\epsilon_{\\text{curr}}) = \\frac{1}{m} + \\frac{1}{m} = \\frac{2}{m}$。\n所以，$Z \\sim \\mathcal{N}(0, 2/m)$。我们用 $v_m = 2/m$ 表示 $Z$ 的方差。$Z$ 的概率密度函数 (PDF) 是 $f_Z(z) = \\frac{1}{\\sqrt{2\\pi v_m}} \\exp\\left(-\\frac{z^2}{2v_m}\\right)$。\n\n第一个任务是推导期望接受概率 $A(m) = \\mathbb{E}[\\alpha]$。\n$$A(m) = \\mathbb{E}[\\min\\{1, \\exp(\\Delta + Z)\\}] = \\int_{-\\infty}^{\\infty} \\min\\{1, \\exp(\\Delta + z)\\} f_Z(z) dz$$\n项 $\\min\\{1, \\exp(\\Delta + z)\\}$ 取决于 $\\Delta + z$ 的符号。\n当 $\\Delta + z \\leq 0$，即 $z \\leq -\\Delta$ 时，$\\exp(\\Delta + z) \\leq 1$。\n当 $\\Delta + z > 0$，即 $z > -\\Delta$ 时，$\\exp(\\Delta + z) > 1$。\n所以我们在 $z = -\\Delta$ 处分割积分：\n$$A(m) = \\int_{-\\infty}^{-\\Delta} \\exp(\\Delta + z) f_Z(z) dz + \\int_{-\\Delta}^{\\infty} 1 \\cdot f_Z(z) dz$$\n第二个积分是概率 $P(Z \\geq -\\Delta)$。设 $\\Phi(\\cdot)$ 是标准正态分布 $\\mathcal{N}(0, 1)$ 的累积分布函数 (CDF)。\n$$\\int_{-\\Delta}^{\\infty} f_Z(z) dz = P(Z \\geq -\\Delta) = 1 - P(Z  -\\Delta) = 1 - \\Phi\\left(\\frac{-\\Delta}{\\sqrt{v_m}}\\right) = \\Phi\\left(\\frac{\\Delta}{\\sqrt{v_m}}\\right)$$\n第一个积分是：\n$$I_1 = \\int_{-\\infty}^{-\\Delta} \\exp(\\Delta + z) \\frac{1}{\\sqrt{2\\pi v_m}} \\exp\\left(-\\frac{z^2}{2v_m}\\right) dz$$\n$$I_1 = \\frac{\\exp(\\Delta)}{\\sqrt{2\\pi v_m}} \\int_{-\\infty}^{-\\Delta} \\exp\\left(z - \\frac{z^2}{2v_m}\\right) dz$$\n我们对指数部分进行配方：\n$$z - \\frac{z^2}{2v_m} = -\\frac{1}{2v_m}(z^2 - 2v_m z) = -\\frac{1}{2v_m}((z-v_m)^2 - v_m^2) = -\\frac{(z-v_m)^2}{2v_m} + \\frac{v_m}{2}$$\n将此代回积分中：\n$$I_1 = \\frac{\\exp(\\Delta)}{\\sqrt{2\\pi v_m}} \\int_{-\\infty}^{-\\Delta} \\exp\\left(-\\frac{(z-v_m)^2}{2v_m} + \\frac{v_m}{2}\\right) dz$$\n$$I_1 = \\exp(\\Delta + v_m/2) \\int_{-\\infty}^{-\\Delta} \\frac{1}{\\sqrt{2\\pi v_m}} \\exp\\left(-\\frac{(z-v_m)^2}{2v_m}\\right) dz$$\n被积函数是均值为 $v_m$、方差为 $v_m$ 的正态分布（即 $\\mathcal{N}(v_m, v_m)$）的PDF。该积分是一个随机变量 $W \\sim \\mathcal{N}(v_m, v_m)$ 小于或等于 $-\\Delta$ 的概率。\n$$I_1 = \\exp(\\Delta + v_m/2) P(W \\leq -\\Delta) = \\exp(\\Delta + v_m/2) \\Phi\\left(\\frac{-\\Delta - v_m}{\\sqrt{v_m}}\\right)$$\n结合这两个部分，我们得到 $A(m)$ 的表达式：\n$$A(m) = \\Phi\\left(\\frac{\\Delta}{\\sqrt{v_m}}\\right) + \\exp(\\Delta + v_m/2) \\Phi\\left(\\frac{-\\Delta - v_m}{\\sqrt{v_m}}\\right)$$\n代入给定值 $\\Delta = -1$ 和 $v_m = 2/m$：\n$$A(m) = \\Phi\\left(\\frac{-1}{\\sqrt{2/m}}\\right) + \\exp\\left(-1 + \\frac{1}{m}\\right) \\Phi\\left(\\frac{1 - 2/m}{\\sqrt{2/m}}\\right)$$\n$$A(m) = \\Phi\\left(-\\sqrt{\\frac{m}{2}}\\right) + \\exp\\left(\\frac{1}{m} - 1\\right) \\Phi\\left(\\frac{m-2}{\\sqrt{2m}}\\right)$$\n\n计算成本与 $m$ 成正比，所以我们可以将其写为 $C(m) = k \\cdot m$，其中某个常数 $k>0$。单位成本效率定义为 $\\mathrm{Eff}(m) = A(m)/C(m)$。由于我们要最大化这个函数，常数 $k$ 不影响 $m$ 的最优值，所以我们可以设 $k=1$ 并使用给定的定义：\n$$\\mathrm{Eff}(m) = \\frac{A(m)}{m}$$\n我们需要找到使该函数最大化的整数 $m \\geq 1$。我们可以分析其组成部分的行为。\n\n让我们分析函数 $A(v) = \\Phi(\\Delta/\\sqrt{v}) + \\exp(\\Delta + v/2) \\Phi((-\\Delta-v)/\\sqrt{v})$，其中我们将方差 $v$ 视为变量。对于 $\\Delta  0$ 的情况，在伪边际方法的研究中，这是一个已知的结果，即期望接受概率 $A(v)$ 是方差 $v$ 的一个严格递增函数。这是因为，尽管噪声会降低精确度，但当真实接受率很低时，噪声的方差越大，产生一个大的正向噪声从而使提议被接受的机会就越大。\n\n在我们的问题中，$v_m = 2/m$。随着 $m$ 增加，$v_m$ 减小。由于 $A(v)$ 是 $v$ 的增函数，而 $v_m$ 是 $m$ 的减函数，因此复合函数 $A(m) = A(v_m(m))$ 必须是 $m \\geq 1$ 的严格递减函数。\n\n我们要最大化 $\\mathrm{Eff}(m) = \\frac{A(m)}{m}$。\n对于 $m \\in \\{1, 2, 3, \\ldots\\}$，这个函数是两个正的、严格递减的函数的乘积：\n1. $A(m)$：如前所述，这是 $m$ 的一个严格递减函数。\n2. $1/m$：这也是 $m$ 的一个严格递减函数。\n\n两个正的、严格递减的函数的乘积也是一个严格递减的函数。设 $f(m) = A(m)$ 和 $g(m)=1/m$。对于 $m \\ge 1$，两者都是正的且递减的。那么 $f(m+1)  f(m)$ 且 $g(m+1)  g(m)$。因此，它们的乘积 $\\mathrm{Eff}(m+1) = f(m+1)g(m+1)  f(m)g(m) = \\mathrm{Eff}(m)$。\n因此，对于 $m \\geq 1$，$\\mathrm{Eff}(m)$ 是 $m$ 的一个严格递减函数。\n\n对于一个在正整数集合上的严格递减函数，其最大值必定出现在其定义域中最小的整数处。问题说明我们考虑的是整数 $m \\geq 1$。\n因此，在 $m=1$ 时达到最大效率。\n\n为了验证我们的推理，我们可以计算 $m$ 的前几个整数值的效率：\n对于 $m=1$：$v_1=2$。\n$A(1) = \\Phi(-1/\\sqrt{2}) + \\exp(0) \\Phi((1-2)/\\sqrt{2}) = 2\\Phi(-1/\\sqrt{2})$。\n使用 $\\Phi(-x) = 1-\\Phi(x)$，$A(1) = 2(1-\\Phi(1/\\sqrt{2}))$。数值上，$1/\\sqrt{2} \\approx 0.707$，且 $\\Phi(0.707) \\approx 0.760$。所以 $A(1) \\approx 2(1 - 0.760) = 0.48$。$\\mathrm{Eff}(1) \\approx 0.48$。\n\n对于 $m=2$：$v_2=1$。\n$A(2) = \\Phi(-1) + \\exp(-1/2)\\Phi(0) = \\Phi(-1) + 0.5\\exp(-1/2)$。\n数值上，$\\Phi(-1) \\approx 0.159$ 且 $\\exp(-1/2) \\approx 0.607$。所以 $A(2) \\approx 0.159 + 0.5(0.607) = 0.159 + 0.3035 = 0.4625$。\n$\\mathrm{Eff}(2) = A(2)/2 \\approx 0.4625/2 = 0.23125$。\n\n数值计算表明 $\\mathrm{Eff}(1) > \\mathrm{Eff}(2)$，这与我们关于 $\\mathrm{Eff}(m)$ 是 $m$ 的减函数的结论是一致的。因此，使 $\\mathrm{Eff}(m)$ 最大化的整数 $m \\geq 1$ 是 $m=1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "现在，我们从解析练习转向全面的编程实现。这项实践要求您为一个具有重尾分布的统计趣味模型构建一个伪边际采样器，这在稳健统计中是常见情景。您将实现一个PMMH算法，其似然估计量基于一个从学生t分布的潜变量表示中推导出的重要性采样估计量。本练习将让您直接体验PMMH的实际障碍，例如由于估计量方差过高，采样器可能陷入“卡滞”状态，尤其是在远离后验质量的区域进行初始化时。您还将实施一种收敛诊断方法，以经验性地分析算法的行为，从而将理论与实际性能联系起来。",
            "id": "3332969",
            "problem": "给定一个具有刻意设置的重尾的单维贝叶斯后验分布。未知参数表示为 $\\theta \\in \\mathbb{R}$。$\\theta$ 的先验是尺度为 $s_0$ 的 Cauchy 分布，其密度由 $p(\\theta) \\propto \\left(1 + (\\theta/s_0)^2\\right)^{-1}$ 给出。似然函数来自一个自由度为 $\\nu$、已知尺度为 $\\sigma$ 的 Student-$t$ 观测模型，使得在给定 $\\theta$ 的条件下，观测值 $y_1,\\dots,y_n$ 独立同分布，服从 $y_i \\sim \\text{Student-}t_\\nu(\\text{location}=\\theta,\\text{scale}=\\sigma)$。对于本问题中的测试套件，使用 $n=20$ 个观测值，\n$$\ny = \\left(0.5,-1.2,0.3,2.1,-0.7,1.5,-2.4,0.8,-0.1,3.0,-3.5,0.2,1.1,-0.4,0.0,2.6,-1.8,0.9,-0.9,0.4\\right),\n$$\n以及超参数 $\\nu=3$, $\\sigma=1$, $s_0=5$。\n\n您必须实现一个伪边际 Metropolis–Hastings (MH) 马尔可夫链蒙特卡洛算法，使用一个基于潜变量表示构建的无偏似然估计量来对 $\\theta$ 进行采样。通过高斯-逆伽马混合表示来对 Student-$t$ 似然进行建模：\n- 独立地引入潜精度变量 $\\lambda_i \\sim \\text{Gamma}(\\nu/2,\\nu/2)$，其中参数化为形状-率（shape-rate）。\n- 给定 $\\lambda_i$ 和 $\\theta$，条件似然为 $y_i \\mid \\lambda_i,\\theta \\sim \\mathcal{N}\\!\\left(\\theta, \\sigma^2/\\lambda_i\\right)$，对于不同的 $i$ 相互独立。\n\n由此，对于一个固定的 $\\theta$，定义边缘似然 $L(\\theta) = \\prod_{i=1}^n \\int \\phi\\!\\left(y_i;\\theta,\\sigma^2/\\lambda_i\\right)\\,\\text{Gamma}(\\lambda_i;\\nu/2,\\nu/2)\\,d\\lambda_i$ 的一个无偏估计量，其中 $\\phi(\\cdot;\\mu,\\tau^2)$ 表示均值为 $\\mu$、方差为 $\\tau^2$ 的高斯密度。对每个因子使用独立的重点抽样，其中提议分布为 $\\lambda_i \\sim \\text{Gamma}(\\nu/2,\\nu/2)$，以获得无偏因子估计量\n$$\n\\widehat{\\ell}_i(\\theta) \\;=\\; \\frac{1}{K} \\sum_{k=1}^K \\phi\\!\\left(y_i;\\theta,\\sigma^2/\\lambda_{i,k}\\right),\n$$\n然后取其乘积 $\\widehat{L}(\\theta) = \\prod_{i=1}^n \\widehat{\\ell}_i(\\theta)$。伪边际 MH 算法在接受率计算中使用 $\\widehat{L}(\\theta)$ 代替 $L(\\theta)$。使用对称高斯随机游走提议 $\\theta' \\sim \\mathcal{N}(\\theta,s_{\\text{prop}}^2)$。\n\n对于固定的随机种子，分析不同的初始化 $\\theta_0$ 如何影响经验平稳时间。对于给定的链，定义经验目标泛函 $f(\\theta) = \\theta$ 和一个参考后验均值 $\\mu_{\\text{ref}}$，该均值通过在有限网格上进行确定性的数值积分计算得出。具体来说，定义一个未归一化的后验分布\n$$\n\\widetilde{\\pi}(\\theta) \\;=\\; \\left(1 + (\\theta/s_0)^2\\right)^{-1} \\prod_{i=1}^n \\left(1 + \\frac{(y_i - \\theta)^2}{\\nu \\sigma^2}\\right)^{-(\\nu+1)/2},\n$$\n并使用一个从 $\\theta_{\\min}$ 到 $\\theta_{\\max}$、具有恒定间距 $\\Delta \\theta$ 的均匀网格 $\\{\\theta_j\\}_{j=1}^M$ 来近似\n$$\n\\mu_{\\text{ref}} \\;\\approx\\; \\frac{\\sum_{j=1}^M \\theta_j \\,\\widetilde{\\pi}(\\theta_j)}{\\sum_{j=1}^M \\widetilde{\\pi}(\\theta_j)},\n$$\n您的实现必须使用 $\\theta_{\\min}=-50$、$\\theta_{\\max}=50$ 和一个能产生 $M$ 个等间距点的恒定网格间距。在计算 $\\widetilde{\\pi}(\\theta)$ 时，可以忽略似然函数和先验中任何不依赖于 $\\theta$ 的乘法常数，因为它们在比率中会被抵消。\n\n给定一个伪边际 MH 链 $\\{\\theta_t\\}_{t=1}^T$，其提议标准差为 $s_{\\text{prop}}$，定义运行平均值 $m_t = t^{-1}\\sum_{s=1}^t \\theta_s$。对于容差 $\\delta$ 和稳定窗口长度 $w$，将经验平稳时间定义为满足以下条件的最小索引 $t^\\star$：\n$$\n\\max_{s \\in \\{t^\\star, t^\\star+1,\\dots, t^\\star+w-1\\}} \\left| m_s - \\mu_{\\text{ref}} \\right| \\le \\delta.\n$$\n如果在分配的模拟长度内不存在这样的 $t^\\star$，则报告 $T$。\n\n您的推导和实现应基于以下基本概念：\n- 伪边际方法通过一个非负无偏估计量替代难以处理的似然函数，并保持精确的后验分布作为马尔可夫链的不变分布。\n- Student-$t$ 分布的高斯-逆伽马表示是有效的，并被广泛用于构建无偏估计量。\n- 对于对称提议核 $q(\\theta' \\mid \\theta)$，Metropolis–Hastings 接受概率等于 $\\alpha(\\theta,\\theta') = \\min\\{1, p(\\theta') \\widehat{L}(\\theta') / (p(\\theta)\\widehat{L}(\\theta))\\}$，其中 $p(\\theta)$ 表示先验，$\\widehat{L}(\\theta)$ 是无偏似然估计量。\n\n使用以下固定的常数和测试套件：\n- 数据和超参数：\n  - $y$ 如上所给。\n  - $\\nu = 3$, $\\sigma = 1$, $s_0 = 5$。\n- 提议标准差：$s_{\\text{prop}} = 0.6$。\n- 随机种子：$12345$。\n- 积分网格：$\\theta_{\\min} = -50$，$\\theta_{\\max} = 50$，包含 $M = 10001$ 个等间距点。\n- 伪边际 MH 运行长度：$T = 20000$ 次迭代。\n- 容差和稳定窗口：$\\delta = 0.05$, $w = 500$。\n- 对于每个测试用例，无偏似然估计量如上定义，对每个观测值使用 $K$ 次抽样。\n- 测试用例（初始值 $\\theta_0$ 和 $K$）：\n  1. $\\theta_0 = 0.0$, $K = 5$。\n  2. $\\theta_0 = 20.0$, $K = 5$。\n  3. $\\theta_0 = -20.0$, $K = 5$。\n  4. $\\theta_0 = 20.0$, $K = 1$。\n\n您的程序必须：\n- 使用上面定义的 $\\widetilde{\\pi}(\\theta)$，通过指定的网格积分计算 $\\mu_{\\text{ref}}$。\n- 对于每个测试用例，使用指定的 $s_{\\text{prop}}$、$\\theta_0$ 和 $K$ 运行伪边际 MH 链 $T$ 次迭代，并使用上述规则计算 $t^\\star$。如果规则从未被满足，则对该测试用例返回 $T$。\n- 仅对所有蒙特卡洛随机过程使用指定的随机种子。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。结果必须是四个测试用例的四个整数 $t^\\star$，按上面列出的顺序排列。例如，输出行必须类似于 $[a,b,c,d]$，其中 $a$, $b$, $c$, 和 $d$ 是整数。",
            "solution": "用户要求实现并分析一个用于解决具有重尾后验分布的贝叶斯推断问题的伪边际 Metropolis-Hastings (PMMH) 算法。该解决方案涉及几个独立的、有原则的步骤：计算参考后验均值，使用自定义的无偏似然估计量实现 PMMH 采样器，并评估其收敛性。\n\n### 1. 贝叶斯模型规范\n\n问题是推断一个一维参数 $\\theta \\in \\mathbb{R}$。后验分布与先验和似然的乘积成正比，即 $p(\\theta|y) \\propto p(\\theta) L(\\theta; y)$。\n\n*   **先验：** $\\theta$ 上的先验是位置为 $0$、尺度为 $s_0=5$ 的 Cauchy 分布。其概率密度函数 (PDF) 为 $p(\\theta) \\propto \\left(1 + (\\theta/s_0)^2\\right)^{-1}$。这是一个重尾分布，表达了关于 $\\theta$ 的弱先验知识。\n\n*   **似然：** 数据 $y = \\{y_1, \\dots, y_n\\}$ 包含 $n=20$ 个观测值。似然模型假设每个观测值 $y_i$ 独立地从一个自由度为 $\\nu=3$、位置为 $\\theta$、尺度为 $\\sigma=1$ 的 Student-$t$ 分布中抽取。似然函数是 $L(\\theta; y) = \\prod_{i=1}^n p(y_i|\\theta)$，其中 Student-$t$ 分布的 PDF 为\n    $$\n    p(y_i|\\theta) = C_{\\nu,\\sigma} \\left(1 + \\frac{(y_i - \\theta)^2}{\\nu \\sigma^2}\\right)^{-\\frac{\\nu+1}{2}}\n    $$\n    其中 $C_{\\nu,\\sigma}$ 是一个与 $\\theta$ 无关的归一化常数。\n\n### 2. 伪边际 Metropolis-Hastings (PMMH)\n\nStudent-$t$ 似然函数没有一个对于某些 MCMC 方法来说很方便的简单解析形式。PMMH 算法避免了直接计算似然函数 $L(\\theta)$ 的需要。取而代之的是，它使用一个非负的无偏估计量 $\\widehat{L}(\\theta)$，使得 $\\mathbb{E}[\\widehat{L}(\\theta)] = L(\\theta)$。\n\n算法流程如下：\n1.  在第 $t$ 次迭代，给定当前状态 $(\\theta_{t-1}, \\widehat{L}_{t-1})$，其中 $\\widehat{L}_{t-1}$ 是 $\\theta_{t-1}$ 的似然估计值。\n2.  从 $q(\\cdot|\\theta_{t-1})$ 中提议一个新状态 $\\theta'$。问题指定了一个对称高斯随机游走提议，$\\theta' \\sim \\mathcal{N}(\\theta_{t-1}, s_{\\text{prop}}^2)$。\n3.  使用指定的估计程序，为提议的状态 $\\theta'$ 生成一个新的、独立的似然估计值 $\\widehat{L}'$。\n4.  以概率\n    $$\n    \\alpha(\\theta_{t-1}, \\theta') = \\min\\left\\{1, \\frac{p(\\theta') \\widehat{L}'}{p(\\theta_{t-1}) \\widehat{L}_{t-1}}\\right\\}\n    $$\n    接受该提议，其中 $p(\\cdot)$ 是先验密度。如果接受，新状态为 $(\\theta_t, \\widehat{L}_t) = (\\theta', \\widehat{L}')$。否则，链保持在当前状态，即 $(\\theta_t, \\widehat{L}_t) = (\\theta_{t-1}, \\widehat{L}_{t-1})$。\n\n关键在于，用于生成 $\\widehat{L}'$ 的随机数必须在每次提议时重新抽取，但如果提议被拒绝，则会重用旧的估计值 $\\widehat{L}_{t-1}$。这确保了马尔可夫链的不变分布是真实后验 $p(\\theta|y)$。为避免数值下溢，所有计算都在对数尺度上进行。\n\n### 3. 无偏似然估计量\n\n估计量 $\\widehat{L}(\\theta)$ 是基于 Student-$t$ 分布的高斯-逆伽马混合表示构建的。每个 $y_i \\sim \\text{Student-}t_\\nu(\\theta, \\sigma)$ 都可以进行分层建模：\n-   一个未观测到的精度变量 $\\lambda_i$ 从 Gamma 分布中抽取：$\\lambda_i \\sim \\text{Gamma}(\\nu/2, \\nu/2)$，其中参数化为（形状，率）。\n-   在给定 $\\lambda_i$ 和 $\\theta$ 的条件下，观测值是高斯分布的：$y_i \\mid \\lambda_i, \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2/\\lambda_i)$。\n\n边缘似然 $p(y_i|\\theta)$ 是对潜变量 $\\lambda_i$ 的积分：\n$p(y_i|\\theta) = \\int_0^\\infty \\mathcal{N}(y_i; \\theta, \\sigma^2/\\lambda_i) \\, \\text{Gamma}(\\lambda_i; \\nu/2, \\nu/2) \\, d\\lambda_i$。\n\n该积分使用蒙特卡洛积分进行估计。问题指定使用先验分布 $\\text{Gamma}(\\nu/2, \\nu/2)$ 作为重点抽样的提议分布。对于每个观测值 $y_i$，从这个 Gamma 分布中抽取 $K$ 个样本 $\\lambda_{i,k}$。第 $i$ 个似然因子的估计量为：\n$$\n\\widehat{\\ell}_i(\\theta) = \\frac{1}{K} \\sum_{k=1}^K \\mathcal{N}(y_i; \\theta, \\sigma^2/\\lambda_{i,k})\n$$\n该估计量是 $p(y_i|\\theta)$ 的无偏估计。总似然估计量是这些独立估计量的乘积，$\\widehat{L}(\\theta) = \\prod_{i=1}^n \\widehat{\\ell}_i(\\theta)$，它是真实似然 $L(\\theta)$ 的无偏估计。为保证数值稳定性，我们使用 log-sum-exp 技巧计算该估计量的对数：\n$\\log \\widehat{\\ell}_i(\\theta) = -\\log K + \\text{logsumexp}_{k=1}^K(\\log \\mathcal{N}(y_i; \\theta, \\sigma^2/\\lambda_{i,k}))$。\n\n### 4. 参考均值和收敛诊断\n\n为了评估 MCMC 链的收敛性，需要一个高精度的 $\\theta$ 后验均值的参考值。\n\n*   **参考均值 ($\\mu_{\\text{ref}}$):** 这是通过在从 $\\theta_{\\min}=-50$ 到 $\\theta_{\\max}=50$、包含 $M=10001$ 个点的精细 $\\theta$ 值网格上进行数值积分计算得出的。在每个网格点上评估未归一化的后验密度 $\\widetilde{\\pi}(\\theta) = p(\\theta) L(\\theta)$。然后通过加权平均来近似均值：\n    $$\n    \\mu_{\\text{ref}} \\approx \\frac{\\sum_{j=1}^M \\theta_j \\,\\widetilde{\\pi}(\\theta_j)}{\\sum_{j=1}^M \\widetilde{\\pi}(\\theta_j)}\n    $$\n    计算在对数空间中完成以防止数值错误，并使用真实的 Student-$t$ 似然。\n\n*   **平稳时间 ($t^\\star$):** 对于给定的 MCMC 链 $\\{\\theta_t\\}_{t=1}^T$，我们计算运行平均值 $m_t = \\frac{1}{t}\\sum_{s=1}^t \\theta_s$。平稳时间 $t^\\star$ 定义为第一个迭代索引，使得在一个长度为 $w=500$ 的窗口内，运行平均值保持在参考均值的容差 $\\delta=0.05$ 范围内：\n    $$\n    t^\\star = \\min \\left\\{ t \\in [1, T-w+1] \\;\\middle|\\; \\max_{s \\in \\{t, \\dots, t+w-1\\}} |m_s - \\mu_{\\text{ref}}| \\le \\delta \\right\\}\n    $$\n如果到 $T-w+1$ 时仍未满足此条件，则 $t^\\star$ 报告为总运行长度 $T=20000$。根据此标准，该诊断衡量了链的遍历均值收敛到真实后验均值的速度。\n\n对不同初始值 $\\theta_0$ 和粒子数 $K$ 的分析表明，PMMH 的性能对初始化（预烧期）和估计量方差是敏感的。较小的 $K$ 会导致 $\\log\\widehat{L}(\\theta)$ 的方差更高，这可能导致采样器“卡住”，从而产生差的混合效果和缓慢的收敛速度。",
            "answer": "```python\nimport numpy as np\nfrom scipy import stats, special\n\ndef solve():\n    \"\"\"\n    Implements and analyzes the pseudo-marginal Metropolis-Hastings algorithm.\n    \"\"\"\n    \n    # ------------------- GIVENS -------------------\n    # Data and hyperparameters\n    y = np.array([\n        0.5, -1.2, 0.3, 2.1, -0.7, 1.5, -2.4, 0.8, -0.1, 3.0,\n        -3.5, 0.2, 1.1, -0.4, 0.0, 2.6, -1.8, 0.9, -0.9, 0.4\n    ])\n    n = len(y)\n    nu = 3.0\n    sigma = 1.0\n    s0 = 5.0\n    \n    # MCMC and proposal settings\n    s_prop = 0.6\n    T = 20000\n    \n    # Quadrature grid for reference mean\n    theta_min = -50.0\n    theta_max = 50.0\n    M = 10001\n    \n    # Stationarity diagnostic settings\n    delta = 0.05\n    w = 500\n    \n    # Random seed\n    RANDOM_SEED = 12345\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # Test cases\n    test_cases = [\n        (0.0, 5),    # theta_0, K\n        (20.0, 5),\n        (-20.0, 5),\n        (20.0, 1),\n    ]\n    \n    # ------------------- HELPER FUNCTIONS -------------------\n\n    def log_cauchy_prior_unnorm(theta, s0_val):\n        \"\"\"Computes the unnormalized log-density of a Cauchy prior.\"\"\"\n        return -np.log(1 + (theta / s0_val)**2)\n\n    def compute_mu_ref():\n        \"\"\"Computes the reference posterior mean via grid quadrature.\"\"\"\n        theta_grid = np.linspace(theta_min, theta_max, M)\n        \n        # Log-prior on the grid\n        log_prior_vals = log_cauchy_prior_unnorm(theta_grid, s0)\n        \n        # Log-likelihood on the grid using broadcasting\n        # y reshaped to (n, 1) and theta_grid to (1, M) broadcasts to (n, M)\n        log_like_matrix = stats.t.logpdf(y[:, None], df=nu, loc=theta_grid, scale=sigma)\n        log_like_vals = np.sum(log_like_matrix, axis=0) # Sum over n\n        \n        # Unnormalized log-posterior\n        log_post_vals = log_prior_vals + log_like_vals\n        \n        # Stabilize by subtracting max before exponentiating\n        log_post_vals -= np.max(log_post_vals)\n        post_vals = np.exp(log_post_vals)\n        \n        # Compute mean via sum approximation\n        numerator = np.sum(theta_grid * post_vals)\n        denominator = np.sum(post_vals)\n        \n        return numerator / denominator\n\n    def estimate_log_likelihood(theta, K_val):\n        \"\"\"Computes the unbiased estimator of the log-likelihood.\"\"\"\n        shape_gamma = nu / 2.0\n        rate_gamma = nu / 2.0\n        scale_gamma = 1.0 / rate_gamma\n\n        # Draw latent variables lambda_ik ~ Gamma(nu/2, nu/2)\n        lambda_samples = rng.gamma(shape_gamma, scale_gamma, size=(n, K_val))\n        \n        # Variances of conditional Gaussians: sigma^2 / lambda_ik\n        variances = sigma**2 / lambda_samples\n        \n        # Log of Gaussian PDF: log(phi(y_i; theta, sigma^2/lambda_ik))\n        # Broadcasting y[:, None] (n,1) with variances (n, K) -> (n, K)\n        log_phi_vals = -0.5 * np.log(2 * np.pi * variances) - (y[:, None] - theta)**2 / (2 * variances)\n        \n        # Use log-sum-exp trick for numerical stability\n        # axis=1 sums over the K samples for each observation i\n        log_sum_exp_vals = special.logsumexp(log_phi_vals, axis=1)\n        log_ell_hat_vals = log_sum_exp_vals - np.log(K_val)\n        \n        # Total log-likelihood estimator is the sum of the logs of factor estimators\n        return np.sum(log_ell_hat_vals)\n\n    def run_pmmh(theta0, K_val):\n        \"\"\"Runs the PMMH sampler.\"\"\"\n        chain = np.zeros(T)\n        \n        # Initial state\n        theta_current = theta0\n        log_L_current = estimate_log_likelihood(theta_current, K_val)\n        \n        for t in range(T):\n            # Propose a new theta\n            theta_proposal = rng.normal(loc=theta_current, scale=s_prop)\n            \n            # Generate a new likelihood estimate for the proposal\n            log_L_proposal = estimate_log_likelihood(theta_proposal, K_val)\n            \n            # Log-priors for current and-proposed theta\n            log_prior_current = log_cauchy_prior_unnorm(theta_current, s0)\n            log_prior_proposal = log_cauchy_prior_unnorm(theta_proposal, s0)\n            \n            # Log of the Metropolis-Hastings acceptance ratio\n            log_alpha = (log_prior_proposal + log_L_proposal) - \\\n                        (log_prior_current + log_L_current)\n            \n            # Accept/reject step\n            if np.log(rng.uniform())  log_alpha:\n                theta_current = theta_proposal\n                log_L_current = log_L_proposal\n            \n            chain[t] = theta_current\n            \n        return chain\n\n    def compute_t_star(chain, mu_ref_val):\n        \"\"\"Computes the empirical time to stationarity.\"\"\"\n        running_sum = np.cumsum(chain)\n        t_values = np.arange(1, T + 1)\n        running_averages = running_sum / t_values\n        \n        t_star_val = T  # Default if condition is never met\n        \n        # Search for the smallest t_star (1-based index)\n        max_t_candidate = T - w + 1\n        for t_candidate in range(1, max_t_candidate + 1):\n            # Window indices are 0-based\n            start_idx = t_candidate - 1\n            end_idx = t_candidate + w - 1\n            window_slice = running_averages[start_idx:end_idx]\n            \n            max_abs_diff = np.max(np.abs(window_slice - mu_ref_val))\n            \n            if max_abs_diff = delta:\n                t_star_val = t_candidate\n                break\n                \n        return int(t_star_val)\n\n    # ------------------- MAIN EXECUTION -------------------\n    \n    # 1. Compute reference posterior mean\n    mu_ref = compute_mu_ref()\n    \n    results = []\n    \n    # 2. Run simulation for each test case\n    for theta_0, K in test_cases:\n        # Run the PMMH sampler\n        chain = run_pmmh(theta0=theta_0, K_val=K)\n        \n        # Compute the time to stationarity\n        t_star = compute_t_star(chain, mu_ref)\n        \n        results.append(t_star)\n\n    # 3. Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}