{
    "hands_on_practices": [
        {
            "introduction": "The simple ABC rejection algorithm often requires an impractically small tolerance $\\epsilon$ to yield an accurate posterior approximation. Regression-adjusted ABC provides a powerful correction, using local-linear regression to adjust sampled parameters and reduce the approximation error. This exercise  explores the theoretical underpinnings of this method, challenging you to derive the conditions for its convergence and to quantify the residual bias when model assumptions are nonlinear.",
            "id": "3286932",
            "problem": "Consider Approximate Bayesian Computation (ABC) with a scalar parameter $\\theta$, a scalar summary statistic $S$, and an observed summary $s_{0}$. Let the prior be $\\pi(\\theta)$ and let the data-generating mechanism induce a conditional density $f_{S\\mid\\theta}(s\\mid\\theta)$. Define the ABC posterior with kernel weighting as\n$$\np_{\\epsilon}(\\theta\\mid s_{0}) \\propto \\pi(\\theta) \\int K_{\\epsilon}(s - s_{0}) f_{S\\mid\\theta}(s\\mid\\theta)\\,\\mathrm{d}s,\\qquad K_{\\epsilon}(u) = \\frac{1}{\\epsilon} K\\!\\left(\\frac{u}{\\epsilon}\\right),\n$$\nwhere $K$ is a symmetric kernel with $\\int_{\\mathbb{R}} K(u)\\,\\mathrm{d}u = 1$, $\\int_{\\mathbb{R}} u K(u)\\,\\mathrm{d}u = 0$, and finite second moment $\\mu_{2}(K) \\equiv \\int_{\\mathbb{R}} u^{2} K(u)\\,\\mathrm{d}u < \\infty$. Consider the regression-adjusted ABC of Beaumont et al., which simulates independent pairs $(\\theta_{i}, S_{i})$ from the joint $\\pi(\\theta) f_{S\\mid\\theta}(s\\mid\\theta)$, assigns weights $w_{i} \\propto K_{\\epsilon}(S_{i}-s_{0})$, fits the local linear weighted least squares regression of $\\theta$ on $S$ around $s_{0}$,\n$$\n(\\hat{\\alpha}, \\hat{\\beta}) \\in \\arg\\min_{\\alpha,\\beta} \\sum_{i=1}^{n} w_{i}\\,\\big(\\theta_{i} - \\alpha - \\beta\\,(S_{i}-s_{0})\\big)^{2},\n$$\nand forms adjusted values $\\theta_{i}^{\\star} = \\theta_{i} - \\hat{\\beta}\\,(S_{i}-s_{0})$. The regression-adjusted ABC posterior is then the weighted empirical distribution of $\\{\\theta_{i}^{\\star}\\}$.\n\nAssume the following fundamental base conditions:\n\n- The prior $\\pi(\\theta)$ is continuous and positive on a neighborhood of the true posterior support.\n- The marginal density $g(s) = \\int \\pi(\\theta) f_{S\\mid\\theta}(s\\mid\\theta)\\,\\mathrm{d}\\theta$ is twice continuously differentiable and strictly positive at $s_{0}$.\n- The conditional mean function $m(s) \\equiv \\mathbb{E}[\\theta\\mid S=s]$ is twice continuously differentiable in a neighborhood of $s_{0}$, and the conditional variance $\\nu(s) \\equiv \\operatorname{Var}(\\theta\\mid S=s)$ is continuous at $s_{0}$.\n- The simulation size $n$ grows with $\\epsilon$ such that $n\\epsilon \\to \\infty$ as $\\epsilon \\to 0$, ensuring negligible stochastic error relative to the smoothing bias.\n\nPart A (convergence under linearity): Suppose that for $s$ in a neighborhood of $s_{0}$, the conditional distribution $\\theta\\mid S=s$ has mean $m(s) = \\alpha_{0} + \\beta_{0}s$ with constants $\\alpha_{0},\\beta_{0}\\in\\mathbb{R}$ and variance $\\nu(s) \\equiv \\nu_{0}$ constant, and that $\\theta\\mid S=s$ is continuous in $s$ in the sense of weak convergence. Starting from the definitions above, derive the conditions under which the regression-adjusted ABC posterior $p_{\\epsilon}^{\\mathrm{adj}}(\\theta\\mid s_{0})$ converges in total variation to the exact posterior $p(\\theta\\mid S=s_{0})$ as $\\epsilon \\to 0$.\n\nPart B (leading bias under nonlinearity): Now drop exact linearity and assume instead that $m(s)$ is twice continuously differentiable at $s_{0}$ with $m''(s_{0})$ possibly nonzero and that $\\nu(s)$ is continuous at $s_{0}$. Show that the regression-adjusted ABC posterior mean $\\mathbb{E}_{\\epsilon}^{\\mathrm{adj}}[\\theta\\mid s_{0}]$ admits the expansion\n$$\n\\mathbb{E}_{\\epsilon}^{\\mathrm{adj}}[\\theta\\mid s_{0}] \\;=\\; m(s_{0}) \\;+\\; B\\,\\epsilon^{2} \\;+\\; o(\\epsilon^{2}) \\qquad \\text{as } \\epsilon \\to 0,\n$$\nand express the leading bias coefficient $B$ in closed form in terms of $m''(s_{0})$ and the kernel second moment $\\mu_{2}(K)$ defined above. Your final answer should be the analytic expression for $B$ only, written in simplest exact form. Do not provide intermediate steps in the final answer.",
            "solution": "The problem investigates the properties of the regression-adjusted Approximate Bayesian Computation (ABC) posterior distribution. We analyze its convergence to the true posterior under linearity assumptions (Part A) and derive the leading-order bias of its mean under nonlinearity (Part B). The analysis relies on the asymptotic regime where the number of simulations $n \\to \\infty$ and the tolerance $\\epsilon \\to 0$ such that $n\\epsilon \\to \\infty$. This condition allows us to substitute Monte Carlo averages with their population expectations, thereby isolating the bias introduced by the kernel smoothing from the simulation-induced variance.\n\nIn this population limit, weighted averages of a function $H(\\theta, S)$ are given by expectations with respect to a kernel-weighted pseudo-posterior density.\n$$\n\\mathbb{E}_{w}[H(\\theta, S)] = \\frac{\\iint H(\\theta, s) K_{\\epsilon}(s-s_0) p(\\theta, s) \\,\\mathrm{d}\\theta\\,\\mathrm{d}s}{\\iint K_{\\epsilon}(s-s_0) p(\\theta, s) \\,\\mathrm{d}\\theta\\,\\mathrm{d}s}\n$$\nwhere $p(\\theta, s) = \\pi(\\theta)f_{S\\mid\\theta}(s\\mid\\theta) = p(\\theta|s)g(s)$ is the joint density of $(\\theta, S)$. The expectation can be simplified using iterated expectation:\n$$\n\\mathbb{E}_{w}[H(\\theta, S)] = \\frac{\\int \\mathbb{E}[H(\\theta, S) \\mid S=s] K_{\\epsilon}(s-s_0) g(s) \\,\\mathrm{d}s}{\\int K_{\\epsilon}(s-s_0) g(s) \\,\\mathrm{d}s}\n$$\nThis shows that the ABC procedure computes a kernel-weighted average of conditional expectations around the observed summary $s_0$.\n\n**Part A: Convergence under linearity**\n\nThe goal is to derive the conditions under which the regression-adjusted ABC posterior, $p_{\\epsilon}^{\\mathrm{adj}}(\\theta\\mid s_{0})$, converges in total variation to the exact posterior, $p(\\theta\\mid S=s_{0})$, as $\\epsilon \\to 0$.\n\nThe given assumptions for this part are:\n1.  The conditional mean is linear: $m(s) \\equiv \\mathbb{E}[\\theta\\mid S=s] = \\alpha_0 + \\beta_0 s$ for some constants $\\alpha_0, \\beta_0$. This can be rewritten as $m(s) = m(s_0) + \\beta_0(s-s_0)$ where $m(s_0) = \\alpha_0 + \\beta_0 s_0$.\n2.  The conditional variance is constant: $\\nu(s) \\equiv \\operatorname{Var}(\\theta\\mid S=s) = \\nu_0$.\n3.  The conditional distribution of $\\theta\\mid S=s$ is continuous in $s$ in the sense of weak convergence.\n\nThe regression adjustment involves shifting the sampled $\\theta_i$ values by $\\hat{\\beta}(S_i-s_0)$. In the population limit, $\\hat{\\beta}$ converges to\n$$\n\\beta^* = \\frac{\\operatorname{Cov}_w(\\theta, S)}{\\operatorname{Var}_w(S)} = \\frac{\\mathbb{E}_w[\\theta(S-s_0)] - \\mathbb{E}_w[\\theta]\\mathbb{E}_w[S-s_0]}{\\mathbb{E}_w[(S-s_0)^2] - (\\mathbb{E}_w[S-s_0])^2}\n$$\nUnder the linearity assumption for $m(s)$, we can analyze $\\beta^*$. The numerator's leading term is\n$\\mathbb{E}_w[\\theta(S-s_0)] = \\mathbb{E}_w[\\mathbb{E}[\\theta \\mid S](S-s_0)] = \\mathbb{E}_w[m(S)(S-s_0)] = \\mathbb{E}_w[(m(s_0) + \\beta_0(S-s_0))(S-s_0)] = m(s_0)\\mathbb{E}_w[S-s_0] + \\beta_0\\mathbb{E}_w[(S-s_0)^2]$.\nThus, the covariance is\n$\\operatorname{Cov}_w(\\theta, S) = m(s_0)\\mathbb{E}_w[S-s_0] + \\beta_0\\mathbb{E}_w[(S-s_0)^2] - \\mathbb{E}_w[\\theta]\\mathbb{E}_w[S-s_0]$.\nAs $\\epsilon \\to 0$, standard kernel smoothing results show $\\mathbb{E}_w[\\theta] \\to m(s_0)$. A more detailed calculation (as in Part B) shows that $\\mathbb{E}_w[\\theta] = m(s_0) + O(\\epsilon^2)$ and $\\mathbb{E}_w[S-s_0] = O(\\epsilon^2)$.\nThe covariance becomes $\\operatorname{Cov}_w(\\theta, S) \\approx \\beta_0 \\mathbb{E}_w[(S-s_0)^2]$.\nThe variance is $\\operatorname{Var}_w(S) \\approx \\mathbb{E}_w[(S-s_0)^2]$.\nTherefore, as $\\epsilon \\to 0$, $\\beta^* \\to \\beta_0$. The local linear regression consistently estimates the true slope $\\beta_0$.\n\nThe adjusted samples are $\\theta_i^\\star = \\theta_i - \\hat{\\beta}(S_i-s_0)$. In the population limit, we consider the distribution of the adjusted variable $\\theta^\\star = \\theta - \\beta_0(S-s_0)$. Let's analyze its properties conditional on $S=s$, for $s$ in a neighborhood of $s_0$.\nThe conditional mean of the adjusted variable is:\n$$\n\\mathbb{E}[\\theta^\\star \\mid S=s] = \\mathbb{E}[\\theta - \\beta_0(S-s_0) \\mid S=s] = m(s) - \\beta_0(s-s_0)\n$$\nUsing the linearity assumption $m(s) = m(s_0) + \\beta_0(s-s_0)$, this simplifies to:\n$$\n\\mathbb{E}[\\theta^\\star \\mid S=s] = (m(s_0) + \\beta_0(s-s_0)) - \\beta_0(s-s_0) = m(s_0)\n$$\nThe conditional variance of the adjusted variable is:\n$$\n\\operatorname{Var}(\\theta^\\star \\mid S=s) = \\operatorname{Var}(\\theta - \\beta_0(S-s_0) \\mid S=s) = \\operatorname{Var}(\\theta \\mid S=s) = \\nu(s)\n$$\nUsing the constant variance assumption $\\nu(s) = \\nu_0$, we get:\n$$\n\\operatorname{Var}(\\theta^\\star \\mid S=s) = \\nu_0\n$$\nSo, the conditional mean and variance of the adjusted variable $\\theta^\\star$ are constant for all $s$ in the neighborhood of $s_0$. They are equal to the mean and variance of the true posterior, $m(s_0)$ and $\\nu(s_0) = \\nu_0$.\n\nThe regression-adjusted ABC posterior $p_\\epsilon^{\\mathrm{adj}}(\\theta^\\star \\mid s_0)$ is the kernel-weighted average of the conditional distributions of $\\theta^\\star \\mid S=s$. Let $f_{\\theta^\\star\\mid S}(\\cdot|s)$ be the conditional density of $\\theta^\\star$ given $S=s$. Then,\n$$\np_\\epsilon^{\\mathrm{adj}}(t|s_0) = \\frac{\\int f_{\\theta^\\star\\mid S}(t\\mid s) K_\\epsilon(s-s_0)g(s)\\,\\mathrm{d}s}{\\int K_\\epsilon(s-s_0)g(s)\\,\\mathrm{d}s}\n$$\nAs $\\epsilon \\to 0$, the kernel $K_\\epsilon(s-s_0)$ concentrates its mass at $s=s_0$. Given the continuity of $g(s)$ and the assumed weak continuity of the conditional distribution of $\\theta \\mid S=s$ (which implies continuity of $f_{\\theta^\\star\\mid S}(t \\mid s)$ as a function of $s$ for fixed $t$ under mild conditions), the integral converges to the value of the integrand at $s_0$.\n$$\n\\lim_{\\epsilon \\to 0} p_\\epsilon^{\\mathrm{adj}}(t|s_0) = f_{\\theta^\\star\\mid S}(t\\mid s_0)\n$$\nThe distribution of $\\theta^\\star \\mid S=s_0$ is the distribution of $\\theta - \\beta_0(s_0-s_0) = \\theta$ given $S=s_0$. This is precisely the true posterior distribution $p(\\theta \\mid S=s_0)$.\n\nThus, the density of the regression-adjusted posterior converges pointwise to the density of the true posterior. With suitable regularity (e.g., uniform integrability, implied by the base conditions), pointwise convergence of densities implies convergence in $L_1$, which is equivalent to convergence in total variation.\n\nThe conditions under which this convergence holds are therefore:\n1.  Linearity of the conditional mean: $m(s) = \\alpha_0 + \\beta_0 s$.\n2.  Homoscedasticity (constant conditional variance): $\\nu(s) = \\nu_0$.\n3.  Continuity of the conditional distribution $\\theta \\mid S=s$ in $s$ (e.g., in weak convergence).\nThese conditions ensure that the regression adjustment creates a quantity $\\theta^\\star$ whose distribution is essentially \"pivotal\" (independent of $s$) near $s_0$, so that the final kernel averaging step correctly recovers the distribution at $s_0$.\n\n**Part B: Leading bias under nonlinearity**\n\nWe now drop the linearity and constant variance assumptions and derive the leading bias term of the mean of the regression-adjusted posterior, $\\mathbb{E}_{\\epsilon}^{\\mathrm{adj}}[\\theta\\mid s_{0}]$.\nThe adjusted posterior mean is (in the population limit):\n$$\n\\mathbb{E}_{\\epsilon}^{\\mathrm{adj}}[\\theta\\mid s_{0}] = \\mathbb{E}_w[\\theta] - \\beta^* \\mathbb{E}_w[S-s_0]\n$$\nWe need to find the expansion of each term up to order $\\epsilon^2$. We use standard asymptotic results for kernel smoothing. For a smooth function $f(s)$, the kernel-weighted average around $s_0$ has the expansion:\n$$\n\\mathbb{E}_w[f(S)] = \\frac{\\int f(s) K_\\epsilon(s-s_0)g(s)\\,\\mathrm{d}s}{\\int K_\\epsilon(s-s_0)g(s)\\,\\mathrm{d}s} = f(s_0) + \\frac{\\epsilon^2}{2}\\mu_2(K)\\left(f''(s_0)+2f'(s_0)\\frac{g'(s_0)}{g(s_0)}\\right) + o(\\epsilon^2)\n$$\nThis is the bias of the Nadaraya-Watson estimator of $f(s)$ at $s_0$.\n\nFirst, we find the expansion for $\\mathbb{E}_w[\\theta]$ by setting $f(s) = m(s)$:\n$$\n\\mathbb{E}_w[\\theta] = m(s_0) + \\frac{\\epsilon^2}{2}\\mu_2(K)\\left(m''(s_0)+2m'(s_0)\\frac{g'(s_0)}{g(s_0)}\\right) + o(\\epsilon^2)\n$$\nThis is the mean of the unadjusted ABC posterior.\n\nNext, we find the expansion for $\\mathbb{E}_w[S-s_0]$ by setting $f(s) = s-s_0$. Here, $f(s_0)=0$, $f'(s)=1$, and $f''(s)=0$.\n$$\n\\mathbb{E}_w[S-s_0] = 0 + \\frac{\\epsilon^2}{2}\\mu_2(K)\\left(0+2(1)\\frac{g'(s_0)}{g(s_0)}\\right) + o(\\epsilon^2) = \\mu_2(K)\\frac{g'(s_0)}{g(s_0)}\\epsilon^2 + o(\\epsilon^2)\n$$\nNow we analyze $\\beta^* = \\operatorname{Cov}_w(\\theta, S) / \\operatorname{Var}_w(S)$. As calculated in Part A, $\\beta^*$ converges to $m'(s_0)$. A more detailed calculation shows that corrections are of order $\\epsilon^2$, so $\\beta^* = m'(s_0) + O(\\epsilon^2)$. This is sufficient for our purpose.\n\nWe can now assemble the expression for the adjusted posterior mean:\n$$\n\\mathbb{E}_{\\epsilon}^{\\mathrm{adj}}[\\theta\\mid s_{0}] = \\underbrace{\\left( m(s_0) + \\frac{\\epsilon^2}{2}\\mu_2(K)\\left(m''(s_0)+2m'(s_0)\\frac{g'(s_0)}{g(s_0)}\\right) + o(\\epsilon^2) \\right)}_{\\mathbb{E}_w[\\theta]} \\\\ - \\underbrace{\\left( m'(s_0) + O(\\epsilon^2) \\right)}_{\\beta^*} \\underbrace{\\left( \\mu_2(K)\\frac{g'(s_0)}{g(s_0)}\\epsilon^2 + o(\\epsilon^2) \\right)}_{\\mathbb{E}_w[S-s_0]}\n$$\nNow, we expand and collect terms of order $\\epsilon^2$:\n$$\n\\mathbb{E}_{\\epsilon}^{\\mathrm{adj}}[\\theta\\mid s_{0}] = m(s_0) + \\frac{\\epsilon^2}{2}\\mu_2(K)m''(s_0) + \\epsilon^2\\mu_2(K)m'(s_0)\\frac{g'(s_0)}{g(s_0)} - m'(s_0)\\mu_2(K)\\frac{g'(s_0)}{g(s_0)}\\epsilon^2 + o(\\epsilon^2)\n$$\nThe terms involving $g'(s_0)$ cancel out:\n$$\n\\epsilon^2\\mu_2(K)m'(s_0)\\frac{g'(s_0)}{g(s_0)} - m'(s_0)\\mu_2(K)\\frac{g'(s_0)}{g(s_0)}\\epsilon^2 = 0\n$$\nThis leaves us with the final expression for the mean:\n$$\n\\mathbb{E}_{\\epsilon}^{\\mathrm{adj}}[\\theta\\mid s_{0}] = m(s_0) + \\frac{1}{2} m''(s_0) \\mu_2(K) \\epsilon^2 + o(\\epsilon^2)\n$$\nThis demonstrates a key advantage of local linear regression adjustment: the leading bias term does not depend on the derivatives of the marginal density $g(s)$, making the estimate more stable with respect to the distribution of the summary statistics.\n\nFrom this expansion, we can identify the leading bias coefficient $B$.\n$$\nB = \\frac{1}{2}m''(s_0)\\mu_2(K)\n$$",
            "answer": "$$ \\boxed{\\frac{1}{2}m''(s_{0})\\mu_{2}(K)} $$"
        },
        {
            "introduction": "To improve sampling efficiency over basic rejection, ABC-MCMC constructs a Markov chain that explores the approximate posterior distribution. The performance of this sampler critically depends on the design of its transition kernel. In this problem , you will first derive the general form of the Metropolis-Hastings acceptance probability that ensures the chain targets the correct ABC posterior, and then analyze the optimal tuning of the proposal kernel in a high-dimensional setting to maximize sampler efficiency.",
            "id": "3286942",
            "problem": "Consider a Bayesian model with parameter vector $\\theta \\in \\mathbb{R}^{d}$, prior density $\\pi(\\theta)$, and an intractable likelihood for data $y$ that can be simulated from a model $f(x \\mid \\theta)$ producing pseudo-data $x$. Let $S(\\cdot)$ be a measurable summary statistic and $\\rho(\\cdot,\\cdot)$ a nonnegative discrepancy. Define the Approximate Bayesian Computation (ABC) approximate likelihood at tolerance $\\epsilon > 0$ via the kernel $K_{\\epsilon}(r) \\propto K(r/\\epsilon)$ for a bounded, integrable $K$, by\n$$\nL_{\\epsilon}(y \\mid \\theta) \\;=\\; \\int K_{\\epsilon}\\!\\big(\\rho(S(x), S(y))\\big)\\, f(x \\mid \\theta)\\, \\mathrm{d}x.\n$$\nThe ABC posterior is $p_{\\epsilon}(\\theta \\mid y) \\propto \\pi(\\theta)\\, L_{\\epsilon}(y \\mid \\theta)$.\n\nAn ABC-Markov chain Monte Carlo (ABC-MCMC) method considers the augmented target on $(\\theta, x)$:\n$$\n\\tilde{\\pi}_{\\epsilon}(\\theta,x \\mid y) \\;\\propto\\; \\pi(\\theta)\\, f(x \\mid \\theta)\\, K_{\\epsilon}\\!\\big(\\rho(S(x), S(y))\\big),\n$$\nand uses a Metropolisâ€“Hastings (MH) proposal of the form: propose $\\theta' \\sim q(\\theta' \\mid \\theta)$ and then $x' \\sim f(x' \\mid \\theta')$, with acceptance probability $\\alpha\\big((\\theta,x),(\\theta',x')\\big)$.\n\nPart I (Reversibility of ABC-MCMC): Starting from the fundamental definition of detailed balance for MH on an augmented space and the definition of $p_{\\epsilon}(\\theta \\mid y)$ above, derive conditions on the proposal density $q(\\theta' \\mid \\theta)$ and write down the acceptance probability $\\alpha\\big((\\theta,x),(\\theta',x')\\big)$ that ensure the Markov chain on $\\theta$ obtained by marginalizing $(\\theta,x)$ is reversible with respect to $p_{\\epsilon}(\\theta \\mid y)$.\n\nPart II (Mixing under Hessian-matched proposals): Suppose that near a mode $\\theta^{\\star}$ of $p_{\\epsilon}(\\theta \\mid y)$, a second-order Taylor approximation gives\n$$\n\\log p_{\\epsilon}(\\theta \\mid y) \\;\\approx\\; C \\;-\\; \\tfrac{1}{2}(\\theta - \\theta^{\\star})^{\\top} H\\, (\\theta - \\theta^{\\star}),\n$$\nwhere $H$ is a symmetric positive definite matrix (the negative Hessian of $\\log p_{\\epsilon}(\\theta \\mid y)$ at $\\theta^{\\star}$) and $C$ is a constant. Consider the random-walk MH proposal tuned to the local curvature,\n$$\nq(\\theta' \\mid \\theta) \\;=\\; \\mathcal{N}\\!\\big(\\theta,\\, (l^{2}/d)\\, H^{-1}\\big),\n$$\nwith scale $l > 0$ and dimension $d$. In the transformed coordinates $z = H^{1/2}(\\theta - \\theta^{\\star})$, the target is approximately standard normal. Using first principles for the MH acceptance ratio under this quadratic approximation and a high-dimensional limit in which $d \\to \\infty$ with $l$ fixed, derive the limiting average acceptance probability $a(l)$ as a function of $l$, and then maximize the expected squared jump distance with respect to $l$ to identify the asymptotically optimal acceptance probability $a^{\\star}$.\n\nReport only the numeric value of the optimal acceptance probability $a^{\\star}$, rounded to three significant figures.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the theory of computational statistics, specifically Approximate Bayesian Computation (ABC) and Markov chain Monte Carlo (MCMC) methods. The problem is well-posed, objective, and contains all necessary information for a rigorous derivation.\n\nPart I: Reversibility of ABC-MCMC\n\nThe goal is to establish conditions for the reversibility of the Markov chain on $\\theta$ with respect to the ABC posterior $p_{\\epsilon}(\\theta \\mid y)$. We analyze the Metropolis-Hastings (MH) algorithm defined on the augmented space of states $(\\theta, x)$ with the target density $\\tilde{\\pi}_{\\epsilon}(\\theta,x \\mid y)$. The marginal distribution of this target with respect to $x$ is the desired posterior $p_{\\epsilon}(\\theta \\mid y)$:\n$$\n\\int \\tilde{\\pi}_{\\epsilon}(\\theta,x \\mid y) \\, \\mathrm{d}x \\propto \\int \\pi(\\theta)\\, f(x \\mid \\theta)\\, K_{\\epsilon}\\!\\big(\\rho(S(x), S(y))\\big) \\, \\mathrm{d}x = \\pi(\\theta) L_{\\epsilon}(y \\mid \\theta) \\propto p_{\\epsilon}(\\theta \\mid y).\n$$\nIf the chain on the augmented space $(\\theta, x)$ is reversible with respect to $\\tilde{\\pi}_{\\epsilon}(\\theta,x \\mid y)$, then the chain on the component $\\theta$ will be reversible with respect to its marginal distribution, $p_{\\epsilon}(\\theta \\mid y)$.\n\nThe Metropolis-Hastings (MH) algorithm requires that the transition kernel satisfies the detailed balance condition. Let the current state be $(\\theta, x)$ and the proposed state be $(\\theta', x')$. The forward proposal density is $Q((\\theta', x') \\mid (\\theta, x)) = q(\\theta' \\mid \\theta) f(x' \\mid \\theta')$. The reverse proposal density is $Q((\\theta, x) \\mid (\\theta', x')) = q(\\theta \\mid \\theta') f(x \\mid \\theta)$.\n\nThe standard MH acceptance probability is defined to satisfy detailed balance:\n$$\n\\alpha\\big((\\theta,x),(\\theta',x')\\big) = \\min \\left( 1, \\frac{\\tilde{\\pi}_{\\epsilon}(\\theta',x' \\mid y)\\, Q((\\theta,x) \\mid (\\theta',x'))}{\\tilde{\\pi}_{\\epsilon}(\\theta,x \\mid y)\\, Q((\\theta',x') \\mid (\\theta,x))} \\right).\n$$\nSubstituting the expressions for the target and proposal densities, the ratio becomes:\n$$\nR = \\frac{\\Big(\\pi(\\theta')\\, f(x' \\mid \\theta')\\, K_{\\epsilon}\\!\\big(\\rho(S(x'), S(y))\\big)\\Big) \\Big(q(\\theta \\mid \\theta')\\, f(x \\mid \\theta)\\Big)}{\\Big(\\pi(\\theta)\\, f(x \\mid \\theta)\\, K_{\\epsilon}\\!\\big(\\rho(S(x), S(y))\\big)\\Big) \\Big(q(\\theta' \\mid \\theta)\\, f(x' \\mid \\theta')\\Big)}.\n$$\nThe term $f(x \\mid \\theta)$ from the target and the reverse proposal cancel, and the term $f(x' \\mid \\theta')$ from the target and the forward proposal cancel. The ratio simplifies significantly:\n$$\nR = \\frac{\\pi(\\theta')\\, K_{\\epsilon}\\!\\big(\\rho(S(x'), S(y))\\big)\\, q(\\theta \\mid \\theta')}{\\pi(\\theta)\\, K_{\\epsilon}\\!\\big(\\rho(S(x), S(y))\\big)\\, q(\\theta' \\mid \\theta)}.\n$$\nThe acceptance probability is therefore:\n$$\n\\alpha\\big((\\theta,x),(\\theta',x')\\big) = \\min \\left( 1, \\frac{\\pi(\\theta')\\, K_{\\epsilon}\\!\\big(\\rho(S(x'), S(y))\\big)\\, q(\\theta \\mid \\theta')}{\\pi(\\theta)\\, K_{\\epsilon}\\!\\big(\\rho(S(x), S(y))\\big)\\, q(\\theta' \\mid \\theta)} \\right).\n$$\nThe condition on the proposal density $q(\\theta' \\mid \\theta)$ is that it must define an irreducible and aperiodic Markov chain on the parameter space of $\\theta$. No further specific constraints, such as symmetry, are required in general for the validity of the MH algorithm, although they can simplify the acceptance probability. If $q(\\theta' \\mid \\theta)$ is symmetric, i.e., $q(\\theta' \\mid \\theta) = q(\\theta \\mid \\theta')$, the acceptance probability further simplifies to:\n$$\n\\alpha\\big((\\theta,x),(\\theta',x')\\big) = \\min \\left( 1, \\frac{\\pi(\\theta')\\, K_{\\epsilon}\\!\\big(\\rho(S(x'), S(y))\\big)}{\\pi(\\theta)\\, K_{\\epsilon}\\!\\big(\\rho(S(x), S(y))\\big)} \\right).\n$$\nThis choice of $\\alpha$ ensures detailed balance for the augmented chain, which in turn guarantees that the marginal chain for $\\theta$ is reversible with respect to $p_{\\epsilon}(\\theta \\mid y)$.\n\nPart II: Mixing under Hessian-matched proposals\n\nThis part concerns the optimal scaling of a random-walk Metropolis-Hastings algorithm. The analysis is performed for an idealized MH sampler that targets the approximate posterior distribution $p_{\\epsilon}(\\theta \\mid y)$ directly. This is a standard approach for theoretical analysis. The target density is approximated by a Gaussian:\n$$\np(\\theta) \\propto \\exp\\left(-\\frac{1}{2}(\\theta - \\theta^{\\star})^{\\top} H (\\theta - \\theta^{\\star})\\right),\n$$\nwhere we have dropped the subscript $\\epsilon$ and constant $C$ for notational simplicity. The proposal density $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, (l^2/d)H^{-1})$ is symmetric, so the acceptance probability is $\\alpha(\\theta, \\theta') = \\min(1, p(\\theta')/p(\\theta))$.\n\nLet $\\log R = \\log(p(\\theta')/p(\\theta))$.\n$$\n\\log R = -\\frac{1}{2}(\\theta' - \\theta^{\\star})^{\\top} H (\\theta' - \\theta^{\\star}) + \\frac{1}{2}(\\theta - \\theta^{\\star})^{\\top} H (\\theta - \\theta^{\\star}).\n$$\nWe perform a change of variables to $z = H^{1/2}(\\theta - \\theta^{\\star})$. The target density for $z$ becomes the standard multivariate normal $\\mathcal{N}(0, I_d)$, so $p(z) \\propto \\exp(-\\frac{1}{2}z^{\\top}z)$. The proposal is $\\theta' = \\theta + \\delta$, where $\\delta \\sim \\mathcal{N}(0, (l^2/d)H^{-1})$. The proposed move in the $z$-space is $z' = H^{1/2}(\\theta' - \\theta^{\\star}) = z + H^{1/2}\\delta$. The increment $\\Delta_z = H^{1/2}\\delta$ has covariance $\\text{Cov}(\\Delta_z) = H^{1/2} (l^2/d)H^{-1} H^{1/2} = (l^2/d)I_d$. So, $\\Delta_z \\sim \\mathcal{N}(0, (l^2/d)I_d)$.\n\nThe log-ratio in terms of $z$ is:\n$$\n\\log R = -\\frac{1}{2}(z')^{\\top}z' + \\frac{1}{2}z^{\\top}z = -\\frac{1}{2}(z + \\Delta_z)^{\\top}(z + \\Delta_z) + \\frac{1}{2}z^{\\top}z = -z^{\\top}\\Delta_z - \\frac{1}{2}\\Delta_z^{\\top}\\Delta_z.\n$$\nWe analyze this in the high-dimensional limit $d \\to \\infty$. The current state $\\theta$ is assumed to be a typical draw from the target, which means $z \\sim \\mathcal{N}(0, I_d)$. By the Law of Large Numbers, $z^{\\top}z = \\sum_{i=1}^d z_i^2 \\approx d$ for large $d$.\nLet $\\Delta_z = (l/\\sqrt{d})U$, where $U \\sim \\mathcal{N}(0, I_d)$.\n$$\n\\log R = -z^{\\top}\\left(\\frac{l}{\\sqrt{d}}U\\right) - \\frac{1}{2}\\left(\\frac{l}{\\sqrt{d}}U\\right)^{\\top}\\left(\\frac{l}{\\sqrt{d}}U\\right) = -\\frac{l}{\\sqrt{d}}z^{\\top}U - \\frac{l^2}{2d}U^{\\top}U.\n$$\nIn the limit $d \\to \\infty$, $U^{\\top}U/d \\to 1$ and $z^{\\top}U$ is a sum of $d$ i.i.d. terms $z_i U_i$, so by the Central Limit Theorem, it behaves like a normal random variable. The mean of $\\log R$ over proposals is:\n$$\nE_U[\\log R] = -\\frac{l}{\\sqrt{d}}z^{\\top}E[U] - \\frac{l^2}{2d}E[U^{\\top}U] = 0 - \\frac{l^2}{2d}(d) = -\\frac{l^2}{2}.\n$$\nThe variance of $\\log R$ over proposals is:\n$$\n\\text{Var}_U(\\log R) = \\text{Var}_U\\left(-\\frac{l}{\\sqrt{d}}z^{\\top}U - \\frac{l^2}{2d}U^{\\top}U\\right).\n$$\nAs $d \\to \\infty$, the term $\\frac{l^2}{2d}U^{\\top}U$ converges to its mean $-l^2/2$, so its variance goes to $0$. The variance is dominated by the first term:\n$$\n\\text{Var}_U(\\log R) \\approx \\frac{l^2}{d}\\text{Var}_U(z^{\\top}U) = \\frac{l^2}{d} \\sum_{i=1}^d z_i^2 \\text{Var}(U_i) = \\frac{l^2}{d}z^{\\top}z.\n$$\nSince for a typical $z$, $z^{\\top}z \\approx d$, we have $\\text{Var}_U(\\log R) \\approx l^2$.\nThus, in the high-dimensional limit, $\\log R$ converges in distribution to a normal random variable:\n$$\n\\log R \\sim \\mathcal{N}(-l^2/2, l^2).\n$$\nThe average acceptance probability $a(l)$ is $E[\\min(1, \\exp(\\log R))]$, where the expectation is over this limiting distribution. This is a standard result, yielding:\n$$\na(l) = 2\\Phi(-l/2),\n$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal distribution.\n\nTo find the optimal scaling, we maximize the expected squared jump distance (ESJD), which is a measure of sampler efficiency. In the high-dimensional limit, the acceptance probability becomes independent of the current state, so ESJD can be written as the product of the average acceptance rate and the expected squared proposal distance:\n$$\n\\text{ESJD}(l) = E[\\alpha] E[\\|\\theta'-\\theta\\|^2] = a(l) E[\\|\\delta\\|^2].\n$$\n$E[\\|\\delta\\|^2] = E[\\delta^{\\top}\\delta] = \\text{Tr}(\\text{Cov}(\\delta)) = \\text{Tr}((l^2/d)H^{-1}) = (l^2/d)\\text{Tr}(H^{-1})$.\nThe quantity to maximize with respect to $l$ is $J(l) \\propto l^2 a(l) = 2l^2\\Phi(-l/2)$.\nWe find the maximum by setting the derivative with respect to $l$ to zero:\n$$\n\\frac{dJ}{dl} = 4l\\Phi(-l/2) + 2l^2 \\phi(-l/2) \\cdot (-\\frac{1}{2}) = 4l\\Phi(-l/2) - l^2\\phi(l/2) = 0,\n$$\nwhere $\\phi(\\cdot)$ is the standard normal probability density function, and we used $\\phi(-x)=\\phi(x)$. For $l>0$, we can divide by $l$:\n$$\n4\\Phi(-l/2) - l\\phi(l/2) = 0.\n$$\nLet $u = l/2$. The equation for the optimal $u^{\\star}$ is $4\\Phi(-u^{\\star}) = 2u^{\\star}\\phi(u^{\\star})$, or:\n$$\n2\\Phi(-u^{\\star}) = u^{\\star}\\phi(u^{\\star}).\n$$\nThis transcendental equation must be solved numerically. The solution is $u^{\\star} \\approx 1.1906$, which corresponds to an optimal scaling factor $l^{\\star} = 2u^{\\star} \\approx 2.38$.\n\nThe question asks for the asymptotically optimal acceptance probability $a^{\\star}$. This is given by $a(l^{\\star})$:\n$$\na^{\\star} = a(l^{\\star}) = 2\\Phi(-l^{\\star}/2) = 2\\Phi(-u^{\\star}).\n$$\nUsing the optimality condition we derived, $2\\Phi(-u^{\\star}) = u^{\\star}\\phi(u^{\\star})$, we have:\n$$\na^{\\star} = u^{\\star}\\phi(u^{\\star}).\n$$\nSubstituting the numerical value $u^{\\star} \\approx 1.1906$:\n$$\na^{\\star} \\approx 1.1906 \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{(1.1906)^2}{2}\\right) \\approx 1.1906 \\cdot \\phi(1.1906) \\approx 0.2338.\n$$\nThis is a classic result in MCMC theory. Rounded to three significant figures, the optimal acceptance probability is $0.234$.",
            "answer": "$$\n\\boxed{0.234}\n$$"
        },
        {
            "introduction": "Real-world modeling often involves a trade-off between simulator accuracy and computational cost. This exercise  places you in the role of a method designer, tasked with creating an efficient multi-fidelity ABC algorithm that uses a cheap, low-fidelity model to rapidly filter parameter proposals before engaging an expensive high-fidelity model. By implementing a delayed acceptance scheme and optimizing its tolerance thresholds, you will gain practical experience in balancing computational budgets with statistical accuracy.",
            "id": "3286933",
            "problem": "You are tasked with designing a multi-fidelity Approximate Bayesian Computation (ABC) algorithm using delayed acceptance for a scalar parameter estimation problem with a uniform prior. The design must start from first principles in stochastic simulation and Monte Carlo methods and adhere to the following specifications.\n\nFundamental setup:\n- The unknown parameter is $\\theta \\in \\mathbb{R}$ with prior density $\\pi(\\theta)$ given by a uniform distribution on $[a,b]$, namely $\\pi(\\theta) = \\mathcal{U}[a,b]$.\n- There are two simulators:\n  1. A low-fidelity simulator $p_L(x \\mid \\theta)$ producing summary statistic $S(x_L) = \\bar{x}_L$.\n  2. A high-fidelity simulator $p_H(x \\mid \\theta)$ producing summary statistic $S(x_H) = \\bar{x}_H$.\n- The observed dataset $y$ is summarized by $S(y) = \\bar{y}$.\n- The summary statistic for both simulators is the sample mean of $m$ independent draws.\n- The low-fidelity simulator has a bias and higher variance: each draw satisfies $X_{L,i} \\sim \\mathcal{N}(\\theta + b_{\\text{bias}}, \\sigma_L^2)$, so $S(x_L) = \\bar{x}_L \\sim \\mathcal{N}(\\theta + b_{\\text{bias}}, \\sigma_L^2 / m)$.\n- The high-fidelity simulator is unbiased with lower variance: each draw satisfies $X_{H,i} \\sim \\mathcal{N}(\\theta, \\sigma_H^2)$, so $S(x_H) = \\bar{x}_H \\sim \\mathcal{N}(\\theta, \\sigma_H^2 / m)$.\n- The discrepancy is the absolute difference $\\rho(u,v) = |u-v|$.\n\nDelayed acceptance ABC rejection scheme:\n- For a proposed $\\theta$ drawn from $\\pi(\\theta)$, simulate $x_L$ and compute $S(x_L)$.\n- Stage $1$ acceptance test: accept the proposal to stage $2$ if $\\rho(S(x_L), S(y)) \\le \\epsilon_L$.\n- If accepted to stage $2$, simulate $x_H$ and compute $S(x_H)$.\n- Final acceptance test: accept the proposal if $\\rho(S(x_H), S(y)) \\le \\epsilon_H$.\n- Each low-fidelity simulation incurs cost $c_L$, while each high-fidelity simulation (only executed when the stage $1$ test passes) incurs cost $c_H$.\n\nFoundational definitions:\n- Let $\\mu_L(\\theta) = \\theta + b_{\\text{bias}} - \\bar{y}$ and $\\sigma_L' = \\sigma_L / \\sqrt{m}$. Let $\\mu_H(\\theta) = \\theta - \\bar{y}$ and $\\sigma_H' = \\sigma_H / \\sqrt{m}$.\n- Define the acceptance probability at stage $1$ for a given $\\theta$:\n  $$A_L(\\theta; \\epsilon_L) = \\mathbb{P}\\left(|D_L| \\le \\epsilon_L \\mid \\theta \\right), \\quad D_L \\sim \\mathcal{N}\\left(\\mu_L(\\theta), (\\sigma_L')^2\\right).$$\n- Define the acceptance probability at stage $2$ for a given $\\theta$:\n  $$A_H(\\theta; \\epsilon_H) = \\mathbb{P}\\left(|D_H| \\le \\epsilon_H \\mid \\theta \\right), \\quad D_H \\sim \\mathcal{N}\\left(\\mu_H(\\theta), (\\sigma_H')^2\\right).$$\n- Under independence of simulator noise conditional on $\\theta$, the probability of final acceptance for a given $\\theta$ is $A_L(\\theta; \\epsilon_L) \\cdot A_H(\\theta; \\epsilon_H)$.\n\nPopulation-level quantities under the prior:\n- The stage $1$ acceptance rate is\n  $$\\alpha_L(\\epsilon_L) = \\int_{a}^{b} A_L(\\theta; \\epsilon_L) \\, \\frac{d\\theta}{b-a}.$$\n- The final acceptance rate is\n  $$\\alpha_F(\\epsilon_L, \\epsilon_H) = \\int_{a}^{b} A_L(\\theta; \\epsilon_L) \\cdot A_H(\\theta; \\epsilon_H) \\, \\frac{d\\theta}{b-a}.$$\n- The expected total cost per proposed parameter is\n  $$C_{\\text{per-proposal}}(\\epsilon_L) = c_L + c_H \\cdot \\alpha_L(\\epsilon_L).$$\n- The expected total cost per accepted sample for the delayed acceptance ABC rejection scheme is\n  $$J_{\\text{cost}}(\\epsilon_L, \\epsilon_H) = \\frac{c_L + c_H \\cdot \\alpha_L(\\epsilon_L)}{\\alpha_F(\\epsilon_L, \\epsilon_H)}.$$\n- Quality penalty based on the high-fidelity discrepancy:\n  - For a given $\\theta$, define the truncated second moment of the high-fidelity discrepancy:\n    $$M_2^H(\\theta; \\epsilon_H) = \\mathbb{E}\\left[D_H^2 \\cdot \\mathbf{1}\\{|D_H| \\le \\epsilon_H\\} \\mid \\theta\\right], \\quad D_H \\sim \\mathcal{N}\\left(\\mu_H(\\theta), (\\sigma_H')^2\\right).$$\n  - Define the prior-averaged truncated second moment weighted by stage $1$ acceptance:\n    $$N_2(\\epsilon_L, \\epsilon_H) = \\int_{a}^{b} M_2^H(\\theta; \\epsilon_H) \\cdot A_L(\\theta; \\epsilon_L) \\, \\frac{d\\theta}{b-a}.$$\n  - The conditional expected squared discrepancy given final acceptance is\n    $$Q(\\epsilon_L, \\epsilon_H) = \\frac{N_2(\\epsilon_L, \\epsilon_H)}{\\alpha_F(\\epsilon_L, \\epsilon_H)}.$$\n- Composite objective that trades off cost and fidelity:\n  $$J(\\epsilon_L, \\epsilon_H) = J_{\\text{cost}}(\\epsilon_L, \\epsilon_H) + \\lambda \\cdot Q(\\epsilon_L, \\epsilon_H),$$\n  where $\\lambda$ is a nonnegative weight parameter controlling the penalty on discrepancy.\n\nYour task:\n- Given test case parameters, compute $\\alpha_L(\\epsilon_L)$, $\\alpha_F(\\epsilon_L,\\epsilon_H)$, $Q(\\epsilon_L,\\epsilon_H)$, and minimize $J(\\epsilon_L, \\epsilon_H)$ over grid-constrained thresholds $\\epsilon_L \\in [\\epsilon_L^{\\min}, \\epsilon_L^{\\max}]$ and $\\epsilon_H \\in [\\epsilon_H^{\\min}, \\epsilon_H^{\\max}]$ with a prescribed step size. Use numerical quadrature over $\\theta \\in [a,b]$ by uniform grid averaging to approximate the integrals and closed-form expressions for Gaussian cumulative distribution functions and truncated moments.\n\nAnalytical forms to be used in your implementation:\n- For $D \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and any threshold $\\epsilon > 0$, the acceptance probability is\n  $$\\mathbb{P}(|D| \\le \\epsilon) = \\Phi\\left(\\frac{\\epsilon - \\mu}{\\sigma}\\right) - \\Phi\\left(\\frac{-\\epsilon - \\mu}{\\sigma}\\right),$$\n  where $\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal distribution.\n- The truncated second moment over $[-\\epsilon,\\epsilon]$ is\n  $$\\int_{-\\epsilon}^{\\epsilon} d^2 f_{D}(d) \\, \\mathrm{d}d = \\mu^2 P + 2 \\mu \\sigma \\left[\\phi(a) - \\phi(b)\\right] + \\sigma^2 \\left[P - b \\phi(b) + a \\phi(a)\\right],$$\n  where $a = \\frac{-\\epsilon - \\mu}{\\sigma}$, $b = \\frac{\\epsilon - \\mu}{\\sigma}$, $P = \\Phi(b) - \\Phi(a)$, and $\\phi(\\cdot)$ is the standard normal probability density function.\n\nTest suite specification:\n- For each test case $i$, a tuple of parameters is given as\n  $$\\left(a,b,\\bar{y},m,b_{\\text{bias}},\\sigma_L,\\sigma_H,c_L,c_H,\\lambda,\\epsilon_L^{\\min},\\epsilon_L^{\\max},\\epsilon_H^{\\min},\\epsilon_H^{\\max},\\Delta\\epsilon,\\;T\\right),$$\n  where $T$ is the number of uniform grid points used to approximate the prior integrals.\n- The program must evaluate $J(\\epsilon_L, \\epsilon_H)$ on the specified grid and return the minimizer $(\\epsilon_L^{\\star}, \\epsilon_H^{\\star})$ for each test case.\n\nUse the following test suite:\n1. Case $1$ (general case): $(a,b,\\bar{y},m,b_{\\text{bias}},\\sigma_L,\\sigma_H,c_L,c_H,\\lambda,\\epsilon_L^{\\min},\\epsilon_L^{\\max},\\epsilon_H^{\\min},\\epsilon_H^{\\max},\\Delta\\epsilon,T) = (0.0, 1.0, 0.6, 20, 0.2, 1.5, 1.0, 1.0, 5.0, 5.0, 0.01, 0.50, 0.005, 0.30, 0.01, 400)$.\n2. Case $2$ (boundary with zero fidelity penalty): $(0.0, 1.0, 0.6, 20, 0.25, 1.8, 1.0, 1.0, 5.0, 0.0, 0.01, 0.50, 0.005, 0.30, 0.01, 400)$.\n3. Case $3$ (expensive high-fidelity, cheap low-fidelity): $(0.0, 1.0, 0.6, 20, 0.3, 1.5, 1.0, 1.0, 10.0, 5.0, 0.01, 0.50, 0.005, 0.30, 0.01, 400)$.\n4. Case $4$ (nearly unbiased low-fidelity): $(0.0, 1.0, 0.6, 20, 0.0, 1.2, 1.0, 1.5, 8.0, 5.0, 0.01, 0.50, 0.005, 0.30, 0.01, 400)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a two-element list $[\\epsilon_L^{\\star}, \\epsilon_H^{\\star}]$ with floating-point values. For example, an output with two results would be formatted as $[[0.12,0.08],[0.20,0.10]]$.\n- No other text may be printed.",
            "solution": "The user has provided a problem that is scientifically grounded, well-posed, and objective. It is based on established principles of Approximate Bayesian Computation (ABC), specifically using a multi-fidelity, delayed acceptance rejection sampling scheme. The problem is formalizable, containing all necessary definitions, constants, and analytical forms to construct a solution. The objective is to find the optimal tolerance parameters $(\\epsilon_L, \\epsilon_H)$ that minimize a composite function representing the trade-off between computational cost and statistical accuracy. The task involves numerical quadrature and grid-based optimization, which are standard computational methods. The problem is self-contained and free of contradictions or ambiguities. Therefore, the problem is deemed valid and a full solution will be provided.\n\nThe problem requires us to minimize a composite objective function $J(\\epsilon_L, \\epsilon_H)$ for a delayed acceptance ABC algorithm. This function captures the trade-off between the computational cost per accepted sample and the quality of these samples, where quality is inversely related to the expected squared discrepancy. The minimization is performed over a discrete grid of the tolerance parameters $\\epsilon_L$ and $\\epsilon_H$.\n\nThe core of the solution involves the numerical evaluation of several integrals over the prior distribution of the parameter $\\theta$, where $\\pi(\\theta) = \\mathcal{U}[a,b]$. These integrals are approximated using numerical quadrature, specifically by averaging the integrand over a uniform grid of $T$ points in the interval $[a,b]$.\n\nLet the uniform grid of parameter values be $\\{\\theta_i\\}_{i=1}^T$, where $\\theta_i = a + (i-0.5) \\frac{b-a}{T}$. An integral of a function $f(\\theta)$ over the prior support is then approximated as:\n$$\n\\int_{a}^{b} f(\\theta) \\, \\frac{d\\theta}{b-a} \\approx \\frac{1}{b-a} \\sum_{i=1}^{T} f(\\theta_i) \\frac{b-a}{T} = \\frac{1}{T} \\sum_{i=1}^{T} f(\\theta_i)\n$$\n\nWe must first implement functions for the key analytical components provided in the problem statement.\n\nFor a normally distributed random variable $D \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the probability of its absolute value being within a threshold $\\epsilon > 0$ is given by:\n$$\n\\mathbb{P}(|D| \\le \\epsilon) = \\mathbb{P}(-\\epsilon \\le D \\le \\epsilon) = \\Phi\\left(\\frac{\\epsilon - \\mu}{\\sigma}\\right) - \\Phi\\left(\\frac{-\\epsilon - \\mu}{\\sigma}\\right)\n$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution. This formula will be used to compute the acceptance probabilities $A_L(\\theta; \\epsilon_L)$ and $A_H(\\theta; \\epsilon_H)$. For $A_L(\\theta; \\epsilon_L)$, we use $\\mu = \\mu_L(\\theta) = \\theta + b_{\\text{bias}} - \\bar{y}$ and $\\sigma = \\sigma'_L = \\sigma_L / \\sqrt{m}$. For $A_H(\\theta; \\epsilon_H)$, we use $\\mu = \\mu_H(\\theta) = \\theta - \\bar{y}$ and $\\sigma = \\sigma'_H = \\sigma_H / \\sqrt{m}$.\n\nThe truncated second moment of $D$ is given by:\n$$\nM_2(\\mu, \\sigma, \\epsilon) = \\mathbb{E}\\left[D^2 \\cdot \\mathbf{1}\\{|D| \\le \\epsilon\\}\\right] = \\int_{-\\epsilon}^{\\epsilon} d^2 \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(d-\\mu)^2}{2\\sigma^2}} \\, \\mathrm{d}d\n$$\nThe problem provides a closed-form solution for this integral:\n$$\nM_2(\\mu, \\sigma, \\epsilon) = \\mu^2 P + 2 \\mu \\sigma \\left[\\phi(\\alpha) - \\phi(\\beta)\\right] + \\sigma^2 \\left[P - \\beta \\phi(\\beta) + \\alpha \\phi(\\alpha)\\right]\n$$\nwith $\\alpha = \\frac{-\\epsilon - \\mu}{\\sigma}$, $\\beta = \\frac{\\epsilon - \\mu}{\\sigma}$, $P = \\Phi(\\beta) - \\Phi(\\alpha)$, and $\\phi(\\cdot)$ being the probability density function (PDF) of the standard normal distribution. This formula is used to compute $M_2^H(\\theta; \\epsilon_H)$ with $\\mu = \\mu_H(\\theta)$ and $\\sigma = \\sigma'_H$.\n\nWith these building blocks, we can compute the population-level quantities for each pair $(\\epsilon_L, \\epsilon_H)$ on the specified grid.\n\n1.  **Stage $1$ Acceptance Rate, $\\alpha_L(\\epsilon_L)$**:\n    $$\n    \\alpha_L(\\epsilon_L) = \\int_{a}^{b} A_L(\\theta; \\epsilon_L) \\, \\frac{d\\theta}{b-a} \\approx \\frac{1}{T} \\sum_{i=1}^{T} A_L(\\theta_i; \\epsilon_L)\n    $$\n\n2.  **Final Acceptance Rate, $\\alpha_F(\\epsilon_L, \\epsilon_H)$**:\n    $$\n    \\alpha_F(\\epsilon_L, \\epsilon_H) = \\int_{a}^{b} A_L(\\theta; \\epsilon_L) \\cdot A_H(\\theta; \\epsilon_H) \\, \\frac{d\\theta}{b-a} \\approx \\frac{1}{T} \\sum_{i=1}^{T} A_L(\\theta_i; \\epsilon_L) \\cdot A_H(\\theta_i; \\epsilon_H)\n    $$\n\n3.  **Prior-Averaged Truncated Second Moment, $N_2(\\epsilon_L, \\epsilon_H)$**:\n    $$\n    N_2(\\epsilon_L, \\epsilon_H) = \\int_{a}^{b} M_2^H(\\theta; \\epsilon_H) \\cdot A_L(\\theta; \\epsilon_L) \\, \\frac{d\\theta}{b-a} \\approx \\frac{1}{T} \\sum_{i=1}^{T} M_2^H(\\theta_i; \\epsilon_H) \\cdot A_L(\\theta_i; \\epsilon_L)\n    $$\n\nFrom these, we can compute the terms of the objective function:\n\n-   **Expected Cost per Accepted Sample, $J_{\\text{cost}}(\\epsilon_L, \\epsilon_H)$**:\n    $$\n    J_{\\text{cost}}(\\epsilon_L, \\epsilon_H) = \\frac{c_L + c_H \\cdot \\alpha_L(\\epsilon_L)}{\\alpha_F(\\epsilon_L, \\epsilon_H)}\n    $$\n\n-   **Conditional Expected Squared Discrepancy, $Q(\\epsilon_L, \\epsilon_H)$**:\n    $$\n    Q(\\epsilon_L, \\epsilon_H) = \\frac{N_2(\\epsilon_L, \\epsilon_H)}{\\alpha_F(\\epsilon_L, \\epsilon_H)}\n    $$\n\nThe composite objective function is then:\n$$\nJ(\\epsilon_L, \\epsilon_H) = J_{\\text{cost}}(\\epsilon_L, \\epsilon_H) + \\lambda \\cdot Q(\\epsilon_L, \\epsilon_H)\n$$\nIf $\\alpha_F(\\epsilon_L, \\epsilon_H) \\approx 0$, it signifies an extremely low probability of obtaining a sample, leading to a near-infinite cost. In the implementation, this case must be handled to avoid division by zero.\n\nThe overall algorithm performs a grid search. For each test case, we define the grid of values for $\\epsilon_L$ and $\\epsilon_H$ based on the provided minimum, maximum, and step size. We then iterate through every pair $(\\epsilon_L, \\epsilon_H)$ on this grid, compute the value of $J(\\epsilon_L, \\epsilon_H)$, and keep track of the pair that yields the minimum value of $J$. This pair, $(\\epsilon_L^{\\star}, \\epsilon_H^{\\star})$, is the solution for that test case.\n\nThe implementation will be structured to first calculate arrays of $A_L(\\theta_i)$, $A_H(\\theta_i)$, and $M_2^H(\\theta_i)$ values for each $(\\epsilon_L, \\epsilon_H)$ and grid of $\\theta_i$, and then use vectorized operations in `numpy` to efficiently compute the averages and final objective function value. This avoids recomputing values unnecessarily within the loops.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to solve the multi-fidelity ABC optimization problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # (a, b, y_bar, m, b_bias, sigma_L, sigma_H, c_L, c_H, lambda_val, eL_min, eL_max, eH_min, eH_max, d_eps, T)\n        (0.0, 1.0, 0.6, 20, 0.2, 1.5, 1.0, 1.0, 5.0, 5.0, 0.01, 0.50, 0.005, 0.30, 0.01, 400),\n        (0.0, 1.0, 0.6, 20, 0.25, 1.8, 1.0, 1.0, 5.0, 0.0, 0.01, 0.50, 0.005, 0.30, 0.01, 400),\n        (0.0, 1.0, 0.6, 20, 0.3, 1.5, 1.0, 1.0, 10.0, 5.0, 0.01, 0.50, 0.005, 0.30, 0.01, 400),\n        (0.0, 1.0, 0.6, 20, 0.0, 1.2, 1.0, 1.5, 8.0, 5.0, 0.01, 0.50, 0.005, 0.30, 0.01, 400),\n    ]\n\n    results = []\n    for case in test_cases:\n        optimal_eps = find_optimal_epsilons(case)\n        results.append(list(optimal_eps))\n    \n    # Format the final output as specified\n    output_str = \"[\" + \",\".join(f\"[{e[0]:.4f},{e[1]:.4f}]\" for e in results) + \"]\"\n    print(output_str.replace(\" \", \"\"))\n\ndef acceptance_prob(mu, sigma, epsilon):\n    \"\"\"\n    Calculates P(|D| <= epsilon) for D ~ N(mu, sigma^2).\n    \"\"\"\n    if sigma <= 0:\n        return 0.0\n    z_upper = (epsilon - mu) / sigma\n    z_lower = (-epsilon - mu) / sigma\n    return norm.cdf(z_upper) - norm.cdf(z_lower)\n\ndef truncated_second_moment(mu, sigma, epsilon):\n    \"\"\"\n    Calculates E[D^2 * 1{|D| <= epsilon}] for D ~ N(mu, sigma^2).\n    \"\"\"\n    if sigma <= 0:\n        return 0.0\n    \n    alpha_std = (-epsilon - mu) / sigma\n    beta_std = (epsilon - mu) / sigma\n    \n    P = norm.cdf(beta_std) - norm.cdf(alpha_std)\n    \n    phi_alpha = norm.pdf(alpha_std)\n    phi_beta = norm.pdf(beta_std)\n    \n    term1 = mu**2 * P\n    term2 = 2 * mu * sigma * (phi_alpha - phi_beta)\n    term3 = sigma**2 * (P - beta_std * phi_beta + alpha_std * phi_alpha)\n    \n    return term1 + term2 + term3\n\ndef find_optimal_epsilons(params):\n    \"\"\"\n    Performs grid search to find the optimal (epsilon_L, epsilon_H)\n    that minimize the objective function J.\n    \"\"\"\n    a, b, y_bar, m, b_bias, sigma_L, sigma_H, c_L, c_H, lambda_val, \\\n    eL_min, eL_max, eH_min, eH_max, d_eps, T = params\n    \n    sigma_L_prime = sigma_L / np.sqrt(m)\n    sigma_H_prime = sigma_H / np.sqrt(m)\n    \n    # Discretize the prior distribution of theta\n    theta_grid = np.linspace(a, b, T, endpoint=False) + (b - a) / (2 * T)\n    \n    # Create grids for epsilon values\n    eps_L_grid = np.arange(eL_min, eL_max + 0.5 * d_eps, d_eps)\n    eps_H_grid = np.arange(eH_min, eH_max + 0.5 * d_eps, d_eps)\n\n    min_J = np.inf\n    optimal_eps = (None, None)\n\n    # Pre-calculate mu values which are constant across epsilon loops\n    mu_L_theta = theta_grid + b_bias - y_bar\n    mu_H_theta = theta_grid - y_bar\n\n    for eps_L in eps_L_grid:\n        # Calculate quantities dependent on eps_L\n        A_L_vals = acceptance_prob(mu_L_theta, sigma_L_prime, eps_L)\n        alpha_L = np.mean(A_L_vals)\n\n        for eps_H in eps_H_grid:\n            # Calculate quantities dependent on eps_H\n            A_H_vals = acceptance_prob(mu_H_theta, sigma_H_prime, eps_H)\n            M2_H_vals = truncated_second_moment(mu_H_theta, sigma_H_prime, eps_H)\n\n            # Compute population-level integrals\n            alpha_F = np.mean(A_L_vals * A_H_vals)\n            N2 = np.mean(M2_H_vals * A_L_vals)\n\n            # Handle case where final acceptance rate is near zero\n            if alpha_F < 1e-12:\n                J = np.inf\n            else:\n                J_cost = (c_L + c_H * alpha_L) / alpha_F\n                Q = N2 / alpha_F\n                J = J_cost + lambda_val * Q\n            \n            if J < min_J:\n                min_J = J\n                optimal_eps = (eps_L, eps_H)\n                \n    return optimal_eps\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}