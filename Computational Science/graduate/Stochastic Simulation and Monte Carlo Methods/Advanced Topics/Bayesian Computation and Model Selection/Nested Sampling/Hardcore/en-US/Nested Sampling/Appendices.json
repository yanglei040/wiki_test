{
    "hands_on_practices": [
        {
            "introduction": "Deciding when to terminate a nested sampling run is a critical practical challenge, as running for too long is wasteful while stopping too early introduces bias. This exercise  explores a common termination heuristic based on monitoring the evidence contribution from the most recently sampled prior volume shells. By analyzing the stabilization of the running evidence diagnostic, you will develop a deeper intuition for how the algorithm traverses the posterior mass and when the remaining tail integral becomes negligible.",
            "id": "3323429",
            "problem": "Consider Bayesian model comparison with the Bayesian evidence (marginal likelihood) defined by $Z=\\int L(\\theta)\\,\\pi(\\theta)\\,d\\theta$, where $L(\\theta)$ is the likelihood and $\\pi(\\theta)$ is the prior density over parameter $\\theta$. Nested sampling reparameterizes this integral via the prior mass $X(\\lambda)=\\int \\mathbb{I}\\{L(\\theta)\\lambda\\}\\,\\pi(\\theta)\\,d\\theta$, yielding the identity $Z=\\int_{0}^{1} L(X)\\,dX$, with $L(X)$ the inverse likelihood contour function. A standard nested sampling run maintains $n_{\\mathrm{live}}$ live points and produces a decreasing sequence of prior volumes $\\{X_k\\}_{k\\ge 0}$ with $X_0=1$ and $X_kX_{k-1}$, and associated likelihood levels $\\{L_k\\}_{k\\ge 1}$, where at iteration $k$ the lowest-likelihood live point is removed at level $L_k$ and replaced while the prior volume shrinks from $X_{k-1}$ to $X_k$. The usual quadrature estimator of partial evidence after $k$ iterations is $\\hat Z_k=\\sum_{i=1}^{k} L_i\\,w_i$, where $w_i=X_{i-1}-X_i$ are the quadrature weights.\n\nDefine the running-window evidence diagnostic of width $m\\in\\mathbb{N}$ by $\\Delta \\hat Z_k=\\hat Z_{k}-\\hat Z_{k-m}$ for $k\\ge m$. Suppose that stabilization of the diagnostic is observed in the sense that, for some tolerances $\\varepsilon0$ and $\\eta0$, one has $|\\Delta \\hat Z_k-\\Delta \\hat Z_{k-1}|\\varepsilon$ and $\\Delta \\hat Z_k/\\hat Z_k\\eta$ across several successive $k$. From first principles of nested sampling, and using only well-tested facts such as the evidence identity $Z=\\int_{0}^{1}L(X)\\,dX$ and the expected geometric shrinkage $E[X_k]\\approx e^{-k/n_{\\mathrm{live}}}$, which implies that a fixed window width $m$ corresponds to an expected multiplicative contraction factor $\\rho\\approx e^{-m/n_{\\mathrm{live}}}$ in $X$, choose the statement that best justifies why stabilization of $\\Delta \\hat Z_k$ indicates that $X_k$ has shrunk past the posterior bulk and validates termination of the run.\n\nA. Starting from $Z=\\int_{0}^{1}L(X)\\,dX$, the evidence accumulated in the last window is $\\Delta \\hat Z_k\\approx \\int_{X_k}^{X_{k-m}} L(X)\\,dX$ up to quadrature error. Because $E[X_{k}]\\approx e^{-k/n_{\\mathrm{live}}}$, a fixed $m$ yields an expected constant-factor contraction $X_{k-m}\\approx \\rho^{-1}X_k$ with $\\rho\\approx e^{-m/n_{\\mathrm{live}}}$. If $\\Delta \\hat Z_k$ stabilizes at a small relative scale, then the integrals over successive multiplicative shells $\\left[X_{k},X_{k-m}\\right],\\left[X_{k+m},X_{k}\\right],\\dots$ are approximately stationary and small, implying that the measure $L(X)\\,dX$ is negligible on $\\left[0,X_k\\right]$. Equivalently, the remaining tail $Z-\\hat Z_k=\\int_{0}^{X_k}L(X)\\,dX$ is small compared to $\\hat Z_k$, so the bulk of the posterior mass with respect to $L(X)\\,dX$ has already been captured (i.e., $X_k$ lies below the posterior bulk), and terminating at iteration $k$ is valid.\n\nB. Stabilization of $\\Delta \\hat Z_k$ occurs when $X_k=1-e^{-k/n_{\\mathrm{live}}}$, because $\\Delta \\hat Z_k$ is determined solely by the shrinkage dynamics of $X_k$ and does not depend on the likelihood $L$. Therefore termination is always valid purely from prior-volume contraction, regardless of $L(X)$.\n\nC. $\\Delta \\hat Z_k$ stabilizes only when the maximum likelihood $L_{\\max}$ has been reached, so termination must be delayed until $L_k=L_{\\max}$. Once $L_k=L_{\\max}$, the evidence no longer changes because the integrand $L(X)$ stops increasing.\n\nD. If the variance of the likelihood values among live points becomes small, then $\\Delta \\hat Z_k$ stabilizes, which implies that $X_k$ has shrunk past the posterior bulk. Therefore termination can be based on the dispersion of $\\{L_i\\}$ alone without reference to the quadrature weights $\\{w_i\\}$ or the tail integral $\\int_{0}^{X_k}L(X)\\,dX$.",
            "solution": "### Step 1: Extract Givens\nThe problem statement provides the following definitions and conditions for a nested sampling algorithm:\n-   Bayesian evidence: $Z=\\int L(\\theta)\\,\\pi(\\theta)\\,d\\theta$, where $L(\\theta)$ is the likelihood and $\\pi(\\theta)$ is the prior density.\n-   Reparameterization via prior mass: $X(\\lambda)=\\int \\mathbb{I}\\{L(\\theta)\\lambda\\}\\,\\pi(\\theta)\\,d\\theta$.\n-   Evidence identity: $Z=\\int_{0}^{1} L(X)\\,dX$, where $L(X)$ is the inverse of $X(\\lambda)$.\n-   Algorithm parameters: $n_{\\mathrm{live}}$ live points.\n-   Algorithm outputs: A sequence of prior volumes $\\{X_k\\}_{k\\ge 0}$ with $X_0=1$ and $X_k  X_{k-1}$, and a sequence of likelihoods $\\{L_k\\}_{k\\ge 1}$. At iteration $k$, the prior volume shrinks from $X_{k-1}$ to $X_k$, and $L_k$ is the likelihood of the point removed.\n-   Evidence estimator: $\\hat Z_k=\\sum_{i=1}^{k} L_i\\,w_i$, with quadrature weights $w_i=X_{i-1}-X_i$.\n-   Running-window diagnostic: $\\Delta \\hat Z_k=\\hat Z_{k}-\\hat Z_{k-m}$ for $k\\ge m$ and window width $m\\in\\mathbb{N}$.\n-   Observed stabilization condition: $|\\Delta \\hat Z_k-\\Delta \\hat Z_{k-1}|\\varepsilon$ and $\\Delta \\hat Z_k/\\hat Z_k\\eta$ for small tolerances $\\varepsilon0$ and $\\eta0$.\n-   Expected prior volume shrinkage: $E[X_k]\\approx e^{-k/n_{\\mathrm{live}}}$.\n-   Expected multiplicative contraction for window $m$: $\\rho\\approx e^{-m/n_{\\mathrm{live}}}$.\n\nThe task is to identify the best justification for why the stabilization of $\\Delta \\hat Z_k$ validates terminating the run, based on the principle that $X_k$ has shrunk past the \"posterior bulk\".\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is a technically accurate and self-contained description of the nested sampling algorithm and a plausible termination heuristic.\n-   **Scientifically Grounded:** All definitions and relationships, such as the evidence identity $Z = \\int_0^1 L(X)dX$ and the numerical estimator $\\hat{Z}_k$, are standard in the literature on nested sampling. The expected shrinkage $E[X_k] \\approx e^{-k/n_{\\mathrm{live}}}$ is a well-known property of the algorithm. The proposed diagnostic is a logical construction based on these principles.\n-   **Well-Posed:** The problem asks for a conceptual justification, grounded in the provided first principles. It is well-posed, as it seeks the correct logical chain of reasoning connecting an observation (stabilization of $\\Delta \\hat{Z}_k$) to a conclusion (termination is valid).\n-   **Objective:** The language is formal and devoid of subjective claims. The conditions are specified mathematically.\n-   **Completeness and Consistency:** The problem provides all necessary information to reason about the relationship between the diagnostic, the evidence integral, and the termination condition. There are no internal contradictions.\n\nThe problem is deemed **valid**.\n\n### Step 3: Derivation and Option Analysis\n\nThe core of the problem is to understand what the stabilization of the diagnostic $\\Delta \\hat{Z}_k$ implies about the remaining, uncomputed portion of the evidence integral.\n\nThe total evidence is $Z=\\int_{0}^{1}L(X)\\,dX$. The numerical estimator after $k$ iterations, $\\hat Z_k = \\sum_{i=1}^{k} L_i(X_{i-1}-X_i)$, is a left-hand Riemann sum approximation of the partial integral $\\int_{X_k}^{1}L(X)\\,dX$. The error in the total evidence estimate is dominated by the un-sampled tail, $Z_{\\mathrm{rem}} = \\int_{0}^{X_k} L(X)\\,dX$. A valid termination criterion must ensure that this remaining evidence, $Z_{\\mathrm{rem}}$, is negligibly small compared to the accumulated evidence, $\\hat{Z}_k$.\n\nThe running-window diagnostic is defined as $\\Delta \\hat Z_k = \\hat Z_k - \\hat Z_{k-m}$. Let's expand this:\n$$ \\Delta \\hat Z_k = \\left(\\sum_{i=1}^{k} L_i w_i\\right) - \\left(\\sum_{i=1}^{k-m} L_i w_i\\right) = \\sum_{i=k-m+1}^{k} L_i w_i = \\sum_{i=k-m+1}^{k} L_i (X_{i-1} - X_i) $$\nThis sum represents the evidence accumulated in the last $m$ iterations of the algorithm. It is a numerical approximation of the integral of $L(X)$ over the prior volume interval $[X_k, X_{k-m}]$:\n$$ \\Delta \\hat Z_k \\approx \\int_{X_k}^{X_{k-m}} L(X)\\,dX $$\nThe problem states that this diagnostic stabilizes, meaning two conditions are met:\n1.  $|\\Delta \\hat Z_k - \\Delta \\hat Z_{k-1}|  \\varepsilon$: The amount of evidence contributed by successive windows of $m$ steps becomes nearly constant.\n2.  $\\Delta \\hat Z_k / \\hat Z_k  \\eta$: The amount of evidence contributed by the most recent window is a small fraction of the total evidence accumulated so far.\n\nThe \"posterior bulk\" corresponds to the region of prior volume $X$ where the integrand $L(X)$ is large and contributes most to the total integral $Z$. As the algorithm proceeds, $k$ increases, $X_k$ decreases, and $L_k$ increases. Once the algorithm has passed this bulk region, $L(X)$ begins to flatten out as it approaches its maximum value, $L_{\\max}$. At this stage, the remaining prior volume $X_k$ is very small.\n\nThe condition $\\Delta \\hat Z_k / \\hat Z_k  \\eta$ is the most critical part. It states that the contribution from the most recent \"shell\" of prior volume, $[X_k, X_{k-m}]$, is small relative to the total contribution from all previous shells, $[X_k, 1]$. If the contribution from this recent shell is already negligible, it is reasonable to infer that the contribution from the remaining, even smaller tail, $[0, X_k]$, will also be negligible. This is because $L(X)$ is a non-decreasing function for decreasing $X$, so it is bounded above by $L_{\\max}$ in the tail. Thus, $Z_{\\mathrm{rem}} = \\int_0^{X_k} L(X)dX \\le L_{\\max} X_k$. The geometric shrinkage property $E[X_k] \\approx e^{-k/n_{\\mathrm{live}}}$ ensures that $X_k$ becomes exponentially small, making the bound on the tail evidence $L_{\\max}X_k$ also shrink exponentially. The stabilization of $\\Delta \\hat{Z}_k$ at a small relative value is a strong indicator that we have entered this tail region where further sampling yields diminishing returns.\n\nNow, we evaluate the options based on this reasoning.\n\n**A. Starting from $Z=\\int_{0}^{1}L(X)\\,dX$, the evidence accumulated in the last window is $\\Delta \\hat Z_k\\approx \\int_{X_k}^{X_{k-m}} L(X)\\,dX$ up to quadrature error. Because $E[X_{k}]\\approx e^{-k/n_{\\mathrm{live}}}$, a fixed $m$ yields an expected constant-factor contraction $X_{k-m}\\approx \\rho^{-1}X_k$ with $\\rho\\approx e^{-m/n_{\\mathrm{live}}}$. If $\\Delta \\hat Z_k$ stabilizes at a small relative scale, then the integrals over successive multiplicative shells $\\left[X_{k},X_{k-m}\\right],\\left[X_{k+m},X_{k}\\right],\\dots$ are approximately stationary and small, implying that the measure $L(X)\\,dX$ is negligible on $\\left[0,X_k\\right]$. Equivalently, the remaining tail $Z-\\hat Z_k=\\int_{0}^{X_k}L(X)\\,dX$ is small compared to $\\hat Z_k$, so the bulk of the posterior mass with respect to $L(X)\\,dX$ has already been captured (i.e., $X_k$ lies below the posterior bulk), and terminating at iteration $k$ is valid.**\n-   **Analysis:** This statement correctly identifies $\\Delta \\hat Z_k$ as an approximation to $\\int_{X_k}^{X_{k-m}} L(X)\\,dX$. It correctly uses the property of geometric shrinkage to frame the problem in terms of multiplicative shells of prior volume. Crucially, it makes the correct logical leap: if the contribution from the current shell, $\\Delta \\hat Z_k$, is small relative to the total accumulated evidence $\\hat Z_k$, then the contribution from the remaining tail $\\int_{0}^{X_k}L(X)\\,dX$ is also expected to be small. This justifies termination. The reasoning is sound and built directly from the first principles provided.\n-   **Verdict:** Correct.\n\n**B. Stabilization of $\\Delta \\hat Z_k$ occurs when $X_k=1-e^{-k/n_{\\mathrm{live}}}$, because $\\Delta \\hat Z_k$ is determined solely by the shrinkage dynamics of $X_k$ and does not depend on the likelihood $L$. Therefore termination is always valid purely from prior-volume contraction, regardless of $L(X)$.**\n-   **Analysis:** This statement is incorrect on multiple counts. Firstly, the formula for prior volume shrinkage is given as $E[X_k] \\approx e^{-k/n_{\\mathrm{live}}}$, not $1 - e^{-k/n_{\\mathrm{live}}}$. The latter describes a quantity growing from $0$ to $1$. Secondly, the claim that $\\Delta \\hat Z_k$ does not depend on the likelihood $L$ is fundamentally false. The definition is $\\Delta \\hat Z_k = \\sum_{i=k-m+1}^{k} L_i w_i$, which explicitly involves the likelihood values $L_i$. The shape of the likelihood function $L(\\theta)$ determines where the \"posterior bulk\" is located, and termination cannot be decided without considering it.\n-   **Verdict:** Incorrect.\n\n**C. $\\Delta \\hat Z_k$ stabilizes only when the maximum likelihood $L_{\\max}$ has been reached, so termination must be delayed until $L_k=L_{\\max}$. Once $L_k=L_{\\max}$, the evidence no longer changes because the integrand $L(X)$ stops increasing.**\n-   **Analysis:** This statement is flawed. Stabilization of $\\Delta \\hat Z_k$ can happen before reaching the absolute maximum likelihood, for instance, on a broad plateau of high likelihood. More importantly, the claim that \"the evidence no longer changes\" is false. The evidence estimator $\\hat Z_k$ is a cumulative sum. Even if $L_i$ becomes constant at $L_{\\max}$ for all subsequent steps, $\\hat Z_k$ will continue to increase by $L_{\\max} w_i$ at each step $i$. The goal of termination is not to wait for the evidence to stop changing, but to determine when the remaining uncounted evidence is negligible.\n-   **Verdict:** Incorrect.\n\n**D. If the variance of the likelihood values among live points becomes small, then $\\Delta \\hat Z_k$ stabilizes, which implies that $X_k$ has shrunk past the posterior bulk. Therefore termination can be based on the dispersion of $\\{L_i\\}$ alone without reference to the quadrature weights $\\{w_i\\}$ or the tail integral $\\int_{0}^{X_k}L(X)\\,dX$.**\n-   **Analysis:** While a low variance of likelihoods among live points often correlates with being in a likelihood plateau region and can lead to a stable $\\Delta \\hat Z_k$, this is not a sufficient condition for termination. A key part of the termination criterion is the relative magnitude condition $\\Delta \\hat Z_k / \\hat Z_k  \\eta$, which this option ignores. Furthermore, the claim that termination can be decided \"without reference to the quadrature weights $\\{w_i\\}$\" is a grievous error. The evidence $Z=\\int L(X)dX$ is fundamentally an integral where $dX$ (approximated by $w_i$) is the measure. Ignoring the weights is equivalent to ignoring the prior volume, which would invalidate the entire method.\n-   **Verdict:** Incorrect.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Due to its stochastic nature, a single nested sampling run provides only one estimate, $\\hat{Z}$, of the true evidence. To build confidence in this result, it is essential to assess its statistical stability. This practice  guides you through a fundamental validation procedure: performing a paired-run consistency test. You will use the log-evidence estimates and their internal variance calculations from two independent runs to determine if they are statistically consistent with sampling the same true posterior.",
            "id": "3323388",
            "problem": "Consider Bayesian evidence estimation by nested sampling, where the evidence is defined as $Z = \\int L(\\theta)\\,\\pi(\\theta)\\,d\\theta$, with $L(\\theta)$ denoting the likelihood and $\\pi(\\theta)$ denoting the prior density. Each independent nested sampling run produces an estimator $\\hat{Z}$ along with an internal variance estimate for the logarithm of the evidence, $\\ln \\hat{Z}$. Assume that for sufficiently large numbers of live points and well-behaved likelihoods, the Central Limit Theorem (CLT) ensures that the estimator of the log-evidence from a single run, $\\ln \\hat{Z}^{(i)}$, is approximately normally distributed with mean $\\ln Z$ and variance $V^{(i)}$, and that two runs are independent.\n\nYou are given two independent runs on the same posterior target, with the following outputs:\n- Run $1$: $\\ln \\hat{Z}^{(1)} = -123.45$ and variance estimate $V^{(1)} = 0.04$.\n- Run $2$: $\\ln \\hat{Z}^{(2)} = -123.90$ and variance estimate $V^{(2)} = 0.0625$.\n\nDesign a paired-run consistency test that, under the hypothesis $H_0:\\ln Z^{(1)}=\\ln Z^{(2)}$ (agreement of runs), derives a $z$-score using only independence and the variance estimates. Then, using a two-sided significance level $\\alpha = 0.05$, evaluate the test statistic for the data above. For decision-making, you may use the two-sided standard normal critical value $c_{0.05} = 1.959964$. Compute the $z$-score and state whether the two runs agree according to the test rule, but report only the numerical value of the $z$-score as your final answer. Round your final $z$-score to four significant figures.",
            "solution": "The problem requires the design and application of a statistical test to check the consistency between two independent nested sampling runs. The test is based on the estimates of the logarithm of the Bayesian evidence, $\\ln \\hat{Z}$, and their associated variances.\n\nLet $X_1 = \\ln \\hat{Z}^{(1)}$ and $X_2 = \\ln \\hat{Z}^{(2)}$ be the random variables representing the log-evidence estimators from run $1$ and run $2$, respectively. The problem states that due to the Central Limit Theorem, these estimators are approximately normally distributed. Let their true means be $\\mu_1$ and $\\mu_2$, and their true variances be $\\sigma_1^2$ and $\\sigma_2^2$.\nThe problem provides the following assumptions and data:\n$1$. The estimators are normally distributed: $X_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $X_2 \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)$.\n$2$. The runs are independent.\n$3$. The observed values are $x_1 = -123.45$ and $x_2 = -123.90$.\n$4$. The variances are estimated as $\\sigma_1^2 \\approx V^{(1)} = 0.04$ and $\\sigma_2^2 \\approx V^{(2)} = 0.0625$.\n\nThe null hypothesis, $H_0$, for a consistency test is that both runs are sampling from posteriors with the same true evidence $Z$. This implies that the true means of the log-evidence estimators are equal:\n$$H_0: \\mu_1 = \\mu_2$$\nTo test this hypothesis, we consider the distribution of the difference between the two estimators, $\\Delta X = X_1 - X_2$.\n\nUnder the null hypothesis $H_0$, the expected value of the difference is:\n$$E[\\Delta X] = E[X_1 - X_2] = E[X_1] - E[X_2] = \\mu_1 - \\mu_2 = 0$$\nBecause the two runs are independent, the variance of the difference is the sum of their individual variances:\n$$\\text{Var}(\\Delta X) = \\text{Var}(X_1 - X_2) = \\text{Var}(X_1) + \\text{Var}(X_2) = \\sigma_1^2 + \\sigma_2^2$$\nSince $X_1$ and $X_2$ are normal random variables, their difference $\\Delta X$ is also a normal random variable. Therefore, under $H_0$, the distribution of the difference is:\n$$\\Delta X \\sim \\mathcal{N}(0, \\sigma_1^2 + \\sigma_2^2)$$\nThe test statistic is the standard score ($z$-score) of the observed difference, $\\Delta x = x_1 - x_2$. The $z$-score is defined as the observed value minus the mean, divided by the standard deviation.\n$$z = \\frac{\\Delta x - E[\\Delta X]}{\\sqrt{\\text{Var}(\\Delta X)}} = \\frac{(x_1 - x_2) - 0}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}}$$\nWe substitute the observed values and the given variance estimates into this formula:\n$$x_1 = -123.45$$\n$$x_2 = -123.90$$\n$$\\sigma_1^2 = V^{(1)} = 0.04$$\n$$\\sigma_2^2 = V^{(2)} = 0.0625$$\nThe numerator is the difference in the observed log-evidence values:\n$$\\Delta x = -123.45 - (-123.90) = -123.45 + 123.90 = 0.45$$\nThe total variance is the sum of the individual variances:\n$$\\sigma_1^2 + \\sigma_2^2 = 0.04 + 0.0625 = 0.1025$$\nThe standard deviation of the difference is the square root of the total variance:\n$$\\sqrt{\\sigma_1^2 + \\sigma_2^2} = \\sqrt{0.1025}$$\nNow, we compute the $z$-score:\n$$z = \\frac{0.45}{\\sqrt{0.1025}} \\approx \\frac{0.45}{0.3201562} \\approx 1.405565$$\nThe problem asks for the result to be rounded to four significant figures.\n$$z \\approx 1.406$$\nTo make a decision, we compare the absolute value of the computed $z$-score to the critical value $c_{0.05} = 1.959964$ for a two-sided test at a significance level of $\\alpha = 0.05$.\n$$|z| \\approx 1.406$$\nSince $|z|  c_{0.05}$ (i.e., $1.406  1.959964$), we do not have sufficient evidence to reject the null hypothesis $H_0$. This means the two runs are considered consistent with each other at the specified significance level. The problem, however, only asks for the numerical value of the $z$-score.",
            "answer": "$$\\boxed{1.406}$$"
        },
        {
            "introduction": "The output of nested sampling is far more than just a single evidence value; the full sequence of discarded \"dead\" points $\\{(\\theta_i, L_i)\\}$ and their associated prior volume weights $\\{w_i\\}$ offers a rich representation of the posterior landscape. This advanced exercise  shows how to repurpose this information to perform powerful sensitivity analyses. You will derive an estimator for the Gâteaux derivative of the evidence with respect to perturbations in the prior, a key tool for assessing model robustness.",
            "id": "3323423",
            "problem": "Let $y$ be observed data, $\\Theta \\subset \\mathbb{R}^{d}$ a parameter space, $\\pi(\\theta)$ a prior density on $\\Theta$, and $L(\\theta) = p(y \\mid \\theta)$ a strictly positive likelihood function. The Bayesian evidence is $Z = \\int_{\\Theta} L(\\theta) \\,\\pi(\\theta)\\,\\mathrm{d}\\theta$. Consider a prior perturbation along an exponential-tilting score direction defined by\n$$\n\\pi_{\\epsilon}(\\theta) = \\pi(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta) - \\kappa(\\epsilon)\\big),\n$$\nwhere $h:\\Theta \\to \\mathbb{R}$ is integrable under $\\pi$, and $\\kappa(\\epsilon) = \\ln \\int_{\\Theta} \\pi(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta)\\big)\\,\\mathrm{d}\\theta$ ensures normalization of $\\pi_{\\epsilon}$ for all sufficiently small $\\epsilon$. Define $Z(\\epsilon) = \\int_{\\Theta} L(\\theta)\\,\\pi_{\\epsilon}(\\theta)\\,\\mathrm{d}\\theta$. Assume that $h$ is centered under the prior at $\\epsilon=0$, i.e., $\\int_{\\Theta} h(\\theta)\\,\\pi(\\theta)\\,\\mathrm{d}\\theta = 0$, so that $\\kappa'(0) = 0$.\n\nA nested sampling run with $n_{\\mathrm{live}} \\in \\mathbb{N}$ live points produces a sequence of $m \\in \\mathbb{N}$ dead points $\\{(\\theta_{i}, L_{i})\\}_{i=1}^{m}$ with $L_{1} \\leq L_{2} \\leq \\cdots \\leq L_{m}$, and associated quadrature weights $\\{w_{i}\\}_{i=1}^{m}$ given by $w_{i} = X_{i-1} - X_{i}$, where $X_{0} = 1$, $X_{i} = \\prod_{j=1}^{i} T_{j}$, and $T_{j} \\sim \\mathrm{Beta}(n_{\\mathrm{live}}, 1)$ are independent shrinkage factors. The standard nested sampling estimator of the evidence is $\\widehat{Z} = \\sum_{i=1}^{m} w_{i}\\,L_{i}$.\n\nStarting only from the definitions above and basic properties of Gâteaux derivatives and expectations, derive the first-order sensitivity (Gâteaux derivative) of $Z(\\epsilon)$ at $\\epsilon=0$ in the direction $h$, and then express a nested sampling Monte Carlo estimator of this derivative using only the dead points $\\{(\\theta_{i}, L_{i})\\}_{i=1}^{m}$ and their weights $\\{w_{i}\\}_{i=1}^{m}$. Your final answer must be a single, closed-form analytic expression in terms of $\\{(\\theta_{i}, L_{i}, w_{i})\\}_{i=1}^{m}$ and $h(\\cdot)$ only. Do not introduce any additional quantities. No rounding is required and no units are involved. Provide the final expression as requested.",
            "solution": "The problem requires a two-part solution: first, to derive the Gâteaux derivative of the Bayesian evidence $Z(\\epsilon)$ with respect to a prior perturbation at $\\epsilon=0$, and second, to construct a Monte Carlo estimator for this derivative using the outputs of a nested sampling algorithm.\n\n### Part 1: Gâteaux Derivative of the Evidence\n\nThe Bayesian evidence $Z$ is defined as the marginal likelihood of the data $y$:\n$$\nZ = \\int_{\\Theta} L(\\theta) \\,\\pi(\\theta)\\,\\mathrm{d}\\theta\n$$\nwhere $L(\\theta)$ is the likelihood and $\\pi(\\theta)$ is the prior probability density over the parameter space $\\Theta$.\n\nThe problem introduces a perturbed prior density $\\pi_{\\epsilon}(\\theta)$ defined by an exponential tilting of the baseline prior $\\pi(\\theta)$:\n$$\n\\pi_{\\epsilon}(\\theta) = \\pi(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta) - \\kappa(\\epsilon)\\big)\n$$\nHere, $h(\\theta)$ is a score function, $\\epsilon$ is a small perturbation parameter, and $\\kappa(\\epsilon)$ is the cumulant-generating function for $h(\\theta)$ under $\\pi(\\theta)$, which serves as a normalization constant:\n$$\n\\kappa(\\epsilon) = \\ln \\int_{\\Theta} \\pi(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta)\\big)\\,\\mathrm{d}\\theta\n$$\nThe perturbed evidence, $Z(\\epsilon)$, is the evidence calculated with the perturbed prior:\n$$\nZ(\\epsilon) = \\int_{\\Theta} L(\\theta)\\,\\pi_{\\epsilon}(\\theta)\\,\\mathrm{d}\\theta = \\int_{\\Theta} L(\\theta) \\pi(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta) - \\kappa(\\epsilon)\\big)\\,\\mathrm{d}\\theta\n$$\nWe can rewrite this by factoring out the term that does not depend on the integration variable $\\theta$:\n$$\nZ(\\epsilon) = \\exp\\!\\big(-\\kappa(\\epsilon)\\big) \\int_{\\Theta} L(\\theta) \\pi(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta)\\big)\\,\\mathrm{d}\\theta\n$$\nThe first-order sensitivity of $Z(\\epsilon)$ with respect to $\\epsilon$ is its Gâteaux derivative, which we find by differentiating $Z(\\epsilon)$ with respect to $\\epsilon$. Assuming sufficient regularity to differentiate under the integral sign (Leibniz integral rule), we apply the product rule:\n$$\n\\frac{\\mathrm{d}Z}{\\mathrm{d}\\epsilon} = \\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\Big( \\exp\\!\\big(-\\kappa(\\epsilon)\\big) \\Big) \\left( \\int_{\\Theta} L(\\theta) \\pi(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta)\\big)\\,\\mathrm{d}\\theta \\right) + \\exp\\!\\big(-\\kappa(\\epsilon)\\big) \\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\left( \\int_{\\Theta} L(\\theta) \\pi(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta)\\big)\\,\\mathrm{d}\\theta \\right)\n$$\nLet's compute the derivatives of the individual parts.\nThe derivative of the normalization factor is:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\exp\\!\\big(-\\kappa(\\epsilon)\\big) = -\\kappa'(\\epsilon)\\,\\exp\\!\\big(-\\kappa(\\epsilon)\\big)\n$$\nThe derivative of the integral term is:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\int_{\\Theta} L(\\theta) \\pi(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta)\\big)\\,\\mathrm{d}\\theta = \\int_{\\Theta} L(\\theta) \\pi(\\theta) h(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta)\\big)\\,\\mathrm{d}\\theta\n$$\nSubstituting these back into the expression for $\\frac{\\mathrm{d}Z}{\\mathrm{d}\\epsilon}$:\n$$\n\\frac{\\mathrm{d}Z}{\\mathrm{d}\\epsilon} = -\\kappa'(\\epsilon)\\,\\exp\\!\\big(-\\kappa(\\epsilon)\\big) \\int_{\\Theta} L(\\theta) \\pi(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta)\\big)\\,\\mathrm{d}\\theta + \\exp\\!\\big(-\\kappa(\\epsilon)\\big) \\int_{\\Theta} L(\\theta) \\pi(\\theta) h(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta)\\big)\\,\\mathrm{d}\\theta\n$$\nWe can recognize that the first integral is related to $Z(\\epsilon)$:\n$$\n\\int_{\\Theta} L(\\theta) \\pi(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta)\\big)\\,\\mathrm{d}\\theta = Z(\\epsilon) / \\exp\\!\\big(-\\kappa(\\epsilon)\\big)\n$$\nSubstituting this simplifies the first term:\n$$\n\\frac{\\mathrm{d}Z}{\\mathrm{d}\\epsilon} = -\\kappa'(\\epsilon) Z(\\epsilon) + \\int_{\\Theta} L(\\theta) h(\\theta) \\left( \\pi(\\theta)\\,\\exp\\!\\big(\\epsilon\\,h(\\theta) - \\kappa(\\epsilon)\\big) \\right) \\mathrm{d}\\theta\n$$\nThe term in the parentheses is simply the perturbed prior $\\pi_{\\epsilon}(\\theta)$. So,\n$$\n\\frac{\\mathrm{d}Z}{\\mathrm{d}\\epsilon} = -\\kappa'(\\epsilon) Z(\\epsilon) + \\int_{\\Theta} L(\\theta) h(\\theta) \\pi_{\\epsilon}(\\theta) \\mathrm{d}\\theta\n$$\nWe are asked for the derivative at $\\epsilon=0$. We evaluate the expression at $\\epsilon=0$. At this point:\n1.  $\\pi_{0}(\\theta) = \\pi(\\theta)\\,\\exp(0 - \\kappa(0))$. Since $\\kappa(0) = \\ln \\int \\pi(\\theta)\\,\\exp(0)\\,\\mathrm{d}\\theta = \\ln(1) = 0$, we have $\\pi_{0}(\\theta) = \\pi(\\theta)$.\n2.  $Z(0) = \\int L(\\theta)\\pi(\\theta)\\mathrm{d}\\theta = Z$.\n3.  The problem states that $h$ is centered under the prior, i.e., $\\int_{\\Theta} h(\\theta)\\,\\pi(\\theta)\\,\\mathrm{d}\\theta = 0$. This implies that $\\kappa'(0)=0$, which is also given. To see this: $\\kappa'(\\epsilon) = \\frac{\\int \\pi(\\theta)h(\\theta)\\exp(\\epsilon h(\\theta)) d\\theta}{\\int \\pi(\\theta)\\exp(\\epsilon h(\\theta)) d\\theta} = \\mathbb{E}_{\\pi_\\epsilon}[h(\\theta)]$. At $\\epsilon=0$, $\\kappa'(0) = \\mathbb{E}_{\\pi_0}[h(\\theta)] = \\mathbb{E}_{\\pi}[h(\\theta)] = 0$.\n\nSubstituting these conditions into the expression for the derivative:\n$$\n\\left. \\frac{\\mathrm{d}Z}{\\mathrm{d}\\epsilon} \\right|_{\\epsilon=0} = -\\kappa'(0) Z(0) + \\int_{\\Theta} L(\\theta) h(\\theta) \\pi_{0}(\\theta) \\mathrm{d}\\theta\n$$\n$$\n\\left. \\frac{\\mathrm{d}Z}{\\mathrm{d}\\epsilon} \\right|_{\\epsilon=0} = -0 \\cdot Z + \\int_{\\Theta} L(\\theta) h(\\theta) \\pi(\\theta) \\mathrm{d}\\theta\n$$\nThus, the first-order sensitivity of the evidence in the direction $h$ is given by the integral:\n$$\n\\left. \\frac{\\mathrm{d}Z}{\\mathrm{d}\\epsilon} \\right|_{\\epsilon=0} = \\int_{\\Theta} L(\\theta) h(\\theta) \\pi(\\theta) \\mathrm{d}\\theta\n$$\n\n### Part 2: Nested Sampling Estimator\n\nThe nested sampling algorithm provides a framework for estimating integrals of the form $\\int_{\\Theta} g(\\theta)\\,\\pi(\\theta)\\,\\mathrm{d}\\theta$. The algorithm generates a sequence of $m$ dead points $\\{\\theta_i\\}_{i=1}^m$ with associated likelihoods $\\{L_i\\}_{i=1}^m$ ordered such that $L_1 \\leq L_2 \\leq \\cdots \\leq L_m$. These points define a sequence of nested prior volumes $X_i = \\int_{L(\\theta)L_i} \\pi(\\theta)\\,\\mathrm{d}\\theta$, with $X_0=1  X_1  \\cdots  X_m$.\n\nThe integral is re-written as a sum over disjoint shells in the parameter space. The parameter space $\\Theta$ is partitioned into shells $\\mathcal{C}_i = \\{\\theta \\in \\Theta \\mid L_{i-1}  L(\\theta) \\ge L_i\\}$ for $i=1,\\ldots,m$ (with $L_0 \\equiv \\infty$), plus a remaining volume for the live points. The integral of a function $g(\\theta)$ over the prior can be written as:\n$$\n\\int_{\\Theta} g(\\theta) \\pi(\\theta) \\mathrm{d}\\theta = \\sum_{i=1}^m \\int_{\\mathcal{C}_i} g(\\theta) \\pi(\\theta) \\mathrm{d}\\theta + \\int_{L(\\theta) \\ge L_m} g(\\theta) \\pi(\\theta) \\mathrm{d}\\theta\n$$\nThe nested sampling estimator approximates the integral over each shell $\\mathcal{C}_i$. The prior mass of the shell $\\mathcal{C}_i$ is $\\int_{\\mathcal{C}_i} \\pi(\\theta) \\mathrm{d}\\theta = X_{i-1} - X_i$, and this is precisely the definition of the quadrature weight $w_i$. The point $\\theta_i$ (with likelihood $L_i$) is treated as a representative sample for the shell $\\mathcal{C}_i$. Therefore, the integral of $g(\\theta)$ over this shell is approximated as:\n$$\n\\int_{\\mathcal{C}_i} g(\\theta) \\pi(\\theta) \\mathrm{d}\\theta \\approx g(\\theta_i) \\int_{\\mathcal{C}_i} \\pi(\\theta) \\mathrm{d}\\theta = g(\\theta_i) w_i\n$$\nSumming over all the shells corresponding to the dead points (and typically ignoring the small contribution from the final set of live points), we obtain the general nested sampling estimator for the integral of $g(\\theta)$:\n$$\n\\widehat{I}_g = \\sum_{i=1}^m w_i g(\\theta_i)\n$$\nIn our case, we need to estimate the sensitivity derivative, which is the integral of the function $g(\\theta) = L(\\theta)h(\\theta)$.\n$$\nS = \\left. \\frac{\\mathrm{d}Z}{\\mathrm{d}\\epsilon} \\right|_{\\epsilon=0} = \\int_{\\Theta} L(\\theta) h(\\theta) \\pi(\\theta) \\mathrm{d}\\theta\n$$\nApplying the nested sampling estimation formula with $g(\\theta_i) = L(\\theta_i)h(\\theta_i) = L_i h(\\theta_i)$, we get the estimator for $S$:\n$$\n\\widehat{S} = \\sum_{i=1}^{m} w_i L_i h(\\theta_i)\n$$\nThis expression uses only the quantities available from the nested sampling run: the dead points $\\{\\theta_i\\}_{i=1}^m$, their likelihoods $\\{L_i\\}_{i=1}^m$, the quadrature weights $\\{w_i\\}_{i=1}^m$, and the score function $h(\\cdot)$. This is the required Monte Carlo estimator.",
            "answer": "$$\n\\boxed{\\sum_{i=1}^{m} w_{i} L_{i} h(\\theta_{i})}\n$$"
        }
    ]
}