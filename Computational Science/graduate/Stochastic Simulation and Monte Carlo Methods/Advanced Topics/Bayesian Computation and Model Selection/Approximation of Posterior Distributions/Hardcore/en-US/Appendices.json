{
    "hands_on_practices": [
        {
            "introduction": "This first exercise grounds us in the fundamentals of Monte Carlo approximation. By tackling a classic Bayesian problem with a known analytical solution, you will implement self-normalized importance sampling (SNIS) to approximate posterior expectations . This practice is essential for building a concrete understanding of the core mechanics of importance sampling and quantifying its inherent finite-sample bias.",
            "id": "3289041",
            "problem": "You are given a single observation modelled by a Poisson likelihood with a Gamma prior on the unknown rate parameter. Your task is to derive the exact posterior distribution from first principles, compute exact posterior expectations for specified functionals, and then quantify the finite-sample bias of a Self-Normalized Importance Sampling (SNIS) approximation using a Lognormal proposal distribution. Your program must implement the full pipeline and produce a single-line output aggregating the bias estimates for a provided test suite.\n\nThe fundamental base you may use includes:\n- Bayes' theorem: for prior density $p(\\lambda)$, likelihood $p(y \\mid \\lambda)$, and posterior $p(\\lambda \\mid y)$, the identity $p(\\lambda \\mid y) \\propto p(y \\mid \\lambda) p(\\lambda)$.\n- The Poisson probability mass function: $p(y \\mid \\lambda) = \\exp(-\\lambda) \\lambda^{y} / y!$ for $y \\in \\{0,1,2,\\dots\\}$ and $\\lambda  0$.\n- The Gamma probability density function with shape-rate parameterization: $p(\\lambda \\mid \\alpha, \\beta) = \\beta^{\\alpha} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda) / \\Gamma(\\alpha)$ for $\\alpha  0$, $\\beta  0$, and $\\lambda  0$.\n- The Lognormal probability density function with parameters $(\\mu, \\sigma)$: $q(\\lambda \\mid \\mu, \\sigma) = \\left[ \\lambda \\sigma \\sqrt{2 \\pi} \\right]^{-1} \\exp\\left( - \\frac{(\\log \\lambda - \\mu)^2}{2 \\sigma^2} \\right)$ for $\\sigma  0$ and $\\lambda  0$.\n\nTasks to complete:\n1) Starting only from Bayes' theorem and the definitions of the Poisson and Gamma distributions, derive the exact posterior distribution $p(\\lambda \\mid y, \\alpha, \\beta)$ for the model with a single observation $y \\in \\{0,1,2,\\dots\\}$, prior $\\lambda \\sim \\mathrm{Gamma}(\\alpha, \\beta)$ with shape $\\alpha$ and rate $\\beta$, and likelihood $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$. Implement a function that returns the posterior parameters in the same shape-rate form.\n2) For the posterior from step $1$, consider the functionals $h_1(\\lambda) = \\lambda$ and $h_2(\\lambda) = \\log \\lambda$. Using properties of the Gamma distribution that qualify as well-tested facts, compute the exact expectations $\\mathbb{E}[h_1(\\lambda) \\mid y, \\alpha, \\beta]$ and $\\mathbb{E}[h_2(\\lambda) \\mid y, \\alpha, \\beta]$ under the exact posterior.\n3) Implement Self-Normalized Importance Sampling (SNIS) using a Lognormal proposal $q(\\lambda \\mid \\mu, \\sigma)$ to approximate $\\mathbb{E}[h(\\lambda)]$ for $h \\in \\{h_1, h_2\\}$. Given $N$ independent and identically distributed (IID) draws $\\{\\lambda_i\\}_{i=1}^N$ from $q$, form unnormalized importance weights $w_i \\propto \\frac{p(\\lambda_i \\mid y, \\alpha, \\beta)}{q(\\lambda_i \\mid \\mu, \\sigma)}$ using the posterior density up to a multiplicative constant, normalize the weights $\\tilde{w}_i = \\frac{w_i}{\\sum_{j=1}^N w_j}$, and compute the SNIS estimate $\\hat{I}_N(h) = \\sum_{i=1}^N \\tilde{w}_i h(\\lambda_i)$. Use numerically stable computations in the log domain for weights.\n4) Quantify the finite-sample bias of the SNIS estimator for both $h_1$ and $h_2$ by Monte Carlo replication. Specifically, with $R$ IID replicates of the SNIS procedure, compute the empirical bias estimate $\\widehat{\\mathrm{Bias}}_R(h) = \\left( \\frac{1}{R} \\sum_{r=1}^R \\hat{I}_{N,r}(h) \\right) - \\mathbb{E}[h(\\lambda) \\mid y, \\alpha, \\beta]$, where $\\hat{I}_{N,r}(h)$ is the SNIS estimate in replicate $r$. Use the base random seed $s_0$ and, for test case index $k \\in \\{0,1,2\\}$, use seed $s_0 + k$ to initialize the random number generator for that test case. All randomness must be generated from a Lognormal distribution parameterized by $(\\mu, \\sigma)$ for the proposal $q$.\n5) Implement the following test suite. For each test case, compute the two empirical biases in the order $[ \\widehat{\\mathrm{Bias}}_R(h_1), \\widehat{\\mathrm{Bias}}_R(h_2) ]$, and aggregate across test cases in the order case $1$, case $2$, case $3$ into a single flattened list.\n- Use $N = 2000$ and $R = 400$.\n- Use base seed $s_0 = 20251010$.\n- Test Case $1$: $y = 12$, $\\alpha = 2.5$, $\\beta = 1.3$, $\\mu = \\log\\left( \\frac{\\alpha + y}{\\beta + 1} \\right)$, $\\sigma = 0.7$.\n- Test Case $2$: $y = 1$, $\\alpha = 0.6$, $\\beta = 0.2$, $\\mu = -0.5$, $\\sigma = 1.1$.\n- Test Case $3$: $y = 100$, $\\alpha = 10.0$, $\\beta = 5.0$, $\\mu = \\log\\left( \\frac{\\alpha + y}{\\beta + 1} \\right) - 0.5$, $\\sigma = 0.5$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain $6$ floating-point numbers in the order $[\\widehat{\\mathrm{Bias}}_R(h_1)\\_\\mathrm{case1}, \\widehat{\\mathrm{Bias}}_R(h_2)\\_\\mathrm{case1}, \\widehat{\\mathrm{Bias}}_R(h_1)\\_\\mathrm{case2}, \\widehat{\\mathrm{Bias}}_R(h_2)\\_\\mathrm{case2}, \\widehat{\\mathrm{Bias}}_R(h_1)\\_\\mathrm{case3}, \\widehat{\\mathrm{Bias}}_R(h_2)\\_\\mathrm{case3}]$.",
            "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded, well-posed, objective, self-contained, and consistent. All provided data and definitions are clear and sufficient for deriving a unique and meaningful solution. The problem represents a standard, albeit comprehensive, exercise in Bayesian inference and Monte Carlo simulation. We now proceed with the solution.\n\n### Step 1: Derivation of the Posterior Distribution\n\nThe model is defined by a Poisson likelihood for a single observation $y$, and a Gamma prior for the rate parameter $\\lambda$.\nThe prior distribution is $\\lambda \\sim \\mathrm{Gamma}(\\alpha, \\beta)$, with probability density function (PDF):\n$$\np(\\lambda \\mid \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda}\n$$\nThe likelihood for an observation $y$ is $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$, with probability mass function (PMF):\n$$\np(y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{y}}{y!}\n$$\nAccording to Bayes' theorem, the posterior distribution $p(\\lambda \\mid y, \\alpha, \\beta)$ is proportional to the product of the likelihood and the prior:\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto p(y \\mid \\lambda) p(\\lambda \\mid \\alpha, \\beta)\n$$\nSubstituting the functional forms of the likelihood and prior, we have:\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\left( \\frac{e^{-\\lambda} \\lambda^{y}}{y!} \\right) \\left( \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda} \\right)\n$$\nWe can collect all terms that do not depend on $\\lambda$ into the proportionality constant. These terms are $\\frac{1}{y!}$, $\\beta^{\\alpha}$, and $\\frac{1}{\\Gamma(\\alpha)}$. Combining the terms that are functions of $\\lambda$:\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\lambda^{y} e^{-\\lambda} \\cdot \\lambda^{\\alpha - 1} e^{-\\beta \\lambda}\n$$\nUsing the properties of exponents, we combine the powers of $\\lambda$ and the arguments of the exponential function:\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\lambda^{y + \\alpha - 1} e^{-(\\lambda + \\beta\\lambda)}\n$$\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\lambda^{(\\alpha + y) - 1} e^{-(\\beta + 1)\\lambda}\n$$\nThis resulting expression is the kernel of a Gamma distribution. By inspection, we can identify the new parameters of this posterior Gamma distribution. Let the posterior parameters be $\\alpha'$ and $\\beta'$. We can see that:\n$$\n\\alpha' = \\alpha + y\n$$\n$$\n\\beta' = \\beta + 1\n$$\nThus, the posterior distribution for $\\lambda$ given the observation $y$ and prior parameters $\\alpha$ and $\\beta$ is a Gamma distribution with updated parameters:\n$$\n\\lambda \\mid y, \\alpha, \\beta \\sim \\mathrm{Gamma}(\\alpha' = \\alpha + y, \\beta' = \\beta + 1)\n$$\nThis demonstrates the conjugacy of the Gamma prior with the Poisson likelihood.\n\n### Step 2: Computation of Exact Posterior Expectations\n\nWith the posterior distribution identified as $\\lambda \\mid y \\sim \\mathrm{Gamma}(\\alpha', \\beta')$, we can compute the exact expectations of the specified functionals $h_1(\\lambda) = \\lambda$ and $h_2(\\lambda) = \\log\\lambda$.\n\nFor a random variable $X \\sim \\mathrm{Gamma}(k, \\theta)$ with shape $k$ and scale $\\theta$, the mean is $\\mathbb{E}[X] = k\\theta$. In our shape-rate parameterization where the rate is $\\beta_p = 1/\\theta$, the mean is $\\mathbb{E}[X] = k/\\beta_p$.\nFor our posterior distribution $\\mathrm{Gamma}(\\alpha', \\beta')$, the expectation of $h_1(\\lambda) = \\lambda$ is:\n$$\n\\mathbb{E}[h_1(\\lambda) \\mid y, \\alpha, \\beta] = \\mathbb{E}[\\lambda \\mid y] = \\frac{\\alpha'}{\\beta'} = \\frac{\\alpha + y}{\\beta + 1}\n$$\nFor the functional $h_2(\\lambda) = \\log\\lambda$, the expectation for a Gamma-distributed variable $X \\sim \\mathrm{Gamma}(\\alpha_p, \\beta_p)$ is given by $\\mathbb{E}[\\log X] = \\psi(\\alpha_p) - \\log(\\beta_p)$, where $\\psi(\\cdot)$ is the digamma function, defined as the logarithmic derivative of the gamma function, $\\psi(z) = \\frac{d}{dz}\\log\\Gamma(z)$.\nApplying this to our posterior distribution:\n$$\n\\mathbb{E}[h_2(\\lambda) \\mid y, \\alpha, \\beta] = \\mathbb{E}[\\log \\lambda \\mid y] = \\psi(\\alpha') - \\log(\\beta') = \\psi(\\alpha + y) - \\log(\\beta + 1)\n$$\n\n### Step 3: Self-Normalized Importance Sampling (SNIS)\n\nThe goal is to estimate the posterior expectation $\\mathbb{E}[h(\\lambda) \\mid y] = \\frac{\\int h(\\lambda) p(y|\\lambda)p(\\lambda) d\\lambda}{\\int p(y|\\lambda)p(\\lambda) d\\lambda}$ using a proposal distribution $q(\\lambda)$. Let $\\tilde{p}(\\lambda) = p(y|\\lambda)p(\\lambda)$ be the unnormalized posterior.\nThe SNIS estimator for a functional $h(\\lambda)$ based on $N$ samples $\\{\\lambda_i\\}_{i=1}^N$ from a proposal distribution $q(\\lambda)$ is:\n$$\n\\hat{I}_N(h) = \\sum_{i=1}^{N} \\tilde{w}_i h(\\lambda_i)\n$$\nwhere the normalized weights $\\tilde{w}_i$ are given by:\n$$\n\\tilde{w}_i = \\frac{w_i}{\\sum_{j=1}^{N} w_j} \\quad \\text{with raw weights} \\quad w_i = \\frac{\\tilde{p}(\\lambda_i)}{q(\\lambda_i)}\n$$\nOur target distribution (unnormalized posterior) is $\\tilde{p}(\\lambda) \\propto \\lambda^{\\alpha' - 1} e^{-\\beta'\\lambda}$, and our proposal is the Lognormal distribution $q(\\lambda \\mid \\mu, \\sigma)$.\nTo prevent numerical underflow or overflow, computations are performed in the log domain. The log of the raw weight is:\n$$\n\\log w_i = \\log \\tilde{p}(\\lambda_i) - \\log q(\\lambda_i)\n$$\nWe only need the kernel of the target density, so we use $\\log \\tilde{p}_{\\text{kernel}}(\\lambda) = (\\alpha' - 1)\\log\\lambda - \\beta'\\lambda$. The log of the proposal PDF is $\\log q(\\lambda \\mid \\mu, \\sigma) = -\\log\\lambda - \\log\\sigma - \\frac{1}{2}\\log(2\\pi) - \\frac{(\\log\\lambda - \\mu)^2}{2\\sigma^2}$. So,\n$$\n\\log w_i = \\left( (\\alpha' - 1)\\log\\lambda_i - \\beta'\\lambda_i \\right) - \\left( -\\log\\lambda_i - \\log\\sigma - \\frac{1}{2}\\log(2\\pi) - \\frac{(\\log\\lambda_i - \\mu)^2}{2\\sigma^2} \\right)\n$$\nFor normalization, we use the log-sum-exp trick. Let $L_i = \\log w_i$ and $L_{\\max} = \\max_i \\{L_i\\}$. The normalized weights are:\n$$\n\\tilde{w}_i = \\frac{e^{L_i}}{\\sum_{j=1}^N e^{L_j}} = \\frac{e^{L_i - L_{\\max}}}{\\sum_{j=1}^N e^{L_j - L_{\\max}}}\n$$\nThis stabilized computation avoids floating-point errors.\n\n### Step 4: Quantifying Finite-Sample Bias\n\nThe SNIS estimator is generally biased for finite $N$. The bias is defined as $\\mathrm{Bias}(\\hat{I}_N(h)) = \\mathbb{E}[\\hat{I}_N(h)] - \\mathbb{E}[h(\\lambda) \\mid y]$. We estimate this bias using Monte Carlo simulation. By generating $R$ independent replicates of the SNIS estimate, $\\{\\hat{I}_{N,r}(h)\\}_{r=1}^R$, we can approximate the expectation $\\mathbb{E}[\\hat{I}_N(h)]$ with the sample mean $\\frac{1}{R}\\sum_{r=1}^R \\hat{I}_{N,r}(h)$. The empirical bias is then:\n$$\n\\widehat{\\mathrm{Bias}}_R(h) = \\left( \\frac{1}{R} \\sum_{r=1}^R \\hat{I}_{N,r}(h) \\right) - \\mathbb{E}[h(\\lambda) \\mid y]\n$$\nThis procedure will be implemented for the functionals $h_1(\\lambda)$ and $h_2(\\lambda)$ for each test case specified in the problem. The random number generator is seeded with $s_0 + k$ for test case $k$ to ensure reproducibility.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import digamma\n\ndef solve():\n    \"\"\"\n    Implements the full pipeline to derive posterior, compute exact expectations,\n    and quantify the finite-sample bias of a Self-Normalized Importance Sampling\n    (SNIS) approximation for a Poisson-Gamma model.\n    \"\"\"\n\n    # Global parameters from the problem statement\n    N = 2000\n    R = 400\n    s0 = 20251010\n\n    def calculate_biases(y, alpha, beta, mu, sigma, seed):\n        \"\"\"\n        Calculates the SNIS bias for a single test case.\n\n        Args:\n            y (int): The observed Poisson count.\n            alpha (float): The shape parameter of the Gamma prior.\n            beta (float): The rate parameter of the Gamma prior.\n            mu (float): The mean parameter of the Lognormal proposal (on the log scale).\n            sigma (float): The standard deviation of the Lognormal proposal (on the log scale).\n            seed (int): The random seed for this test case.\n\n        Returns:\n            A tuple (bias_h1, bias_h2) containing the empirical biases for\n            h1(lambda) = lambda and h2(lambda) = log(lambda).\n        \"\"\"\n        # Set the seed for this specific test case to ensure reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Derive posterior parameters\n        # Posterior is Gamma(alpha', beta')\n        alpha_post = alpha + y\n        beta_post = beta + 1\n\n        # Step 2: Compute exact posterior expectations\n        exact_exp_h1 = alpha_post / beta_post\n        exact_exp_h2 = digamma(alpha_post) - np.log(beta_post)\n\n        # Accumulators for the mean of the SNIS estimates over R replicates\n        total_snis_h1 = 0.0\n        total_snis_h2 = 0.0\n\n        # Step 4: Quantify bias via Monte Carlo replication\n        for _ in range(R):\n            # Step 3: Implement one replicate of the SNIS estimator\n\n            # Draw N samples from the Lognormal proposal distribution q(lambda | mu, sigma)\n            # numpy.random.lognormal uses mu and sigma of the underlying normal distribution.\n            lambda_samples = rng.lognormal(mean=mu, sigma=sigma, size=N)\n            log_lambda_samples = np.log(lambda_samples)\n\n            # Calculate log of the unnormalized posterior (target) density kernel\n            # log p(lambda|y) \\propto (alpha_post - 1) * log(lambda) - beta_post * lambda\n            log_target_unnorm = (alpha_post - 1) * log_lambda_samples - beta_post * lambda_samples\n\n            # Calculate log of the Lognormal proposal density\n            # log q(lambda) = -log(lambda) - log(sigma) - 0.5*log(2*pi) - (log(lambda)-mu)^2 / (2*sigma^2)\n            log_proposal = -log_lambda_samples - np.log(sigma) - 0.5 * np.log(2 * np.pi) - \\\n                           (log_lambda_samples - mu)**2 / (2 * sigma**2)\n\n            # Calculate log of the unnormalized importance weights\n            log_weights = log_target_unnorm - log_proposal\n\n            # Normalize weights in a numerically stable way (log-sum-exp trick)\n            # This prevents underflow/overflow when exponentiating.\n            log_weights_max = np.max(log_weights)\n            weights = np.exp(log_weights - log_weights_max)\n            normalized_weights = weights / np.sum(weights)\n\n            # Compute the SNIS estimates for this replicate for h1 and h2\n            snis_h1 = np.sum(normalized_weights * lambda_samples)\n            snis_h2 = np.sum(normalized_weights * log_lambda_samples)\n\n            # Accumulate the estimates\n            total_snis_h1 += snis_h1\n            total_snis_h2 += snis_h2\n\n        # Calculate the average SNIS estimates over all R replicates\n        avg_snis_h1 = total_snis_h1 / R\n        avg_snis_h2 = total_snis_h2 / R\n\n        # Compute the final empirical bias estimates\n        bias_h1 = avg_snis_h1 - exact_exp_h1\n        bias_h2 = avg_snis_h2 - exact_exp_h2\n\n        return bias_h1, bias_h2\n\n    # Step 5: Implement the test suite\n    test_cases_params = [\n        # Test Case 1\n        {'y': 12, 'alpha': 2.5, 'beta': 1.3, 'mu_func': lambda a, y, b: np.log((a + y) / (b + 1)), 'sigma': 0.7},\n        # Test Case 2\n        {'y': 1, 'alpha': 0.6, 'beta': 0.2, 'mu_func': lambda a, y, b: -0.5, 'sigma': 1.1},\n        # Test Case 3\n        {'y': 100, 'alpha': 10.0, 'beta': 5.0, 'mu_func': lambda a, y, b: np.log((a + y) / (b + 1)) - 0.5, 'sigma': 0.5},\n    ]\n\n    all_biases = []\n    \n    for i, params in enumerate(test_cases_params):\n        y_val = params['y']\n        alpha_val = params['alpha']\n        beta_val = params['beta']\n        mu_val = params['mu_func'](alpha_val, y_val, beta_val)\n        sigma_val = params['sigma']\n        seed_val = s0 + i\n\n        bias_h1, bias_h2 = calculate_biases(y_val, alpha_val, beta_val, mu_val, sigma_val, seed_val)\n        all_biases.extend([bias_h1, bias_h2])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_biases))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on basic importance sampling, this practice explores a more sophisticated scenario where we aim to reduce estimator variance. You will implement a control variate technique guided by a local quadratic (Laplace) approximation of a multimodal posterior . This exercise is designed to reveal a crucial trade-off: while control variates can dramatically improve efficiency, using an imperfect model to inform them can introduce a systematic bias, which you will analyze both theoretically and empirically.",
            "id": "3289121",
            "problem": "Consider a one-dimensional Bayesian posterior distribution $\\,\\pi(x)\\,$ that is multimodal. You will approximate $\\,\\pi(x)\\,$ using local quadratic (Laplace) expansions around multiple maximum a posteriori points (modes), thereby forming a Gaussian mixture approximation $\\,\\tilde{\\pi}(x)\\,$, and then use self-normalized importance sampling with a control variate constructed from these local quadratic approximations. You must evaluate the quality of the posterior approximation and quantify the bias introduced when modes are missed or underweighted in $\\,\\tilde{\\pi}(x)\\,$.\n\nThe foundational base for the derivation and algorithm must begin from the following well-tested definitions:\n\n- The posterior distribution $\\,\\pi(x)\\,$ is defined up to proportionality by an unnormalized density $\\,\\pi^\\star(x)\\,$ and, for computational purposes, may be represented by a normalized mixture of Gaussian components for which moments can be computed exactly.\n- The self-normalized importance sampling estimator for $\\,\\mathbb{E}_\\pi[f(X)]\\,$ using proposal distribution $\\,q(x)\\,$ with weights $\\,w(x) = \\pi(x)/q(x)\\,$ is\n$$\n\\widehat{\\mu} \\equiv \\frac{\\sum_{i=1}^n w(X_i)\\,f(X_i)}{\\sum_{i=1}^n w(X_i)} \\quad \\text{with } X_i \\stackrel{\\text{i.i.d.}}{\\sim} q.\n$$\n- Control variates reduce variance by incorporating functions with known expectations. In self-normalized importance sampling, if $\\,h(x)\\,$ has a known target expectation $\\,\\mathbb{E}_\\pi[h(X)]\\,$, then for any constant $\\,c\\,$,\n$$\n\\widehat{\\mu}_{\\text{cv}}(c) \\equiv \\frac{\\sum_{i=1}^n w(X_i)\\,\\big(f(X_i)-c\\big(h(X_i)-\\mathbb{E}_\\pi[h(X)]\\big)\\big)}{\\sum_{i=1}^n w(X_i)}\n$$\nretains the same large-sample limit as $\\,\\widehat{\\mu}\\,$. If, instead, the expectation $\\,\\mathbb{E}_\\pi[h(X)]\\,$ is replaced by an approximation $\\,\\mathbb{E}_{\\tilde{\\pi}}[h(X)]\\,$ derived from local quadratic approximations around multiple modes, then a bias is introduced that can be quantified from first principles.\n\nYou are to implement and evaluate this methodology on a scientifically sound and fully specified target posterior and proposal approximations as follows.\n\nTarget posterior $\\,\\pi(x)\\,$:\n- Let $\\,\\pi(x)\\,$ be the normalized mixture of two Gaussian components:\n$$\n\\pi(x) = \\omega_1\\,\\mathcal{N}(x;\\mu_1,\\sigma_1^2) + \\omega_2\\,\\mathcal{N}(x;\\mu_2,\\sigma_2^2),\n$$\nwith parameters $\\,\\mu_1=-2\\,$, $\\,\\sigma_1=0.5\\,$, $\\,\\mu_2=2\\,$, $\\,\\sigma_2=0.7\\,$, and weights $\\,\\omega_1=0.6\\,$, $\\,\\omega_2=0.4\\,$. Here $\\,\\mathcal{N}(x;\\mu,\\sigma^2)\\,$ denotes the Gaussian probability density function with mean $\\,\\mu\\,$ and variance $\\,\\sigma^2\\,$.\n\nControl variate and estimand:\n- Let the function of interest be $\\,f(x)=x^3\\,$. The control variate is chosen as the quadratic $\\,h(x)=x^2\\,$, motivated by local quadratic approximations of the log-posterior around each mode; its expectation under any Gaussian mixture is available in closed form.\n\nProposal distributions $\\,q(x)\\,$ and approximate posterior $\\,\\tilde{\\pi}(x)\\,$:\n- Construct $\\,q(x) \\equiv \\tilde{\\pi}(x)\\,$ as a Gaussian mixture formed from local quadratic Laplace approximations around mode locations $\\,\\mu_k\\,$. For each test case below, define $\\,\\tilde{\\pi}(x)\\,$ via component means $\\,\\tilde{\\mu}_k\\,$, variances $\\,\\tilde{\\sigma}_k^2\\,$, and weights $\\,\\tilde{\\omega}_k\\,$. Sampling from $\\,q(x)\\,$ is done by first choosing a component index according to weights $\\,\\tilde{\\omega}_k\\,$ and then sampling a Gaussian with its corresponding $\\,\\tilde{\\mu}_k\\,$ and $\\,\\tilde{\\sigma}_k^2\\,$. The importance weights are $\\,w(x)=\\pi(x)/q(x)\\,$.\n\nBias quantification:\n- Using the control variate estimator with $\\,c=1\\,$ but replacing $\\,\\mathbb{E}_\\pi[h]\\,$ by $\\,\\mathbb{E}_{\\tilde{\\pi}}[h]\\,$, its large-sample limit equals\n$$\n\\lim_{n\\to\\infty}\\widehat{\\mu}_{\\text{cv}}(1) = \\mathbb{E}_\\pi[f] - \\big(\\mathbb{E}_\\pi[h] - \\mathbb{E}_{\\tilde{\\pi}}[h]\\big),\n$$\nso the asymptotic bias is\n$$\n\\text{Bias}_{\\infty} = -\\big(\\mathbb{E}_\\pi[h] - \\mathbb{E}_{\\tilde{\\pi}}[h]\\big).\n$$\nYou must compute both the empirical Monte Carlo bias for finite $\\,n\\,$ and the analytical asymptotic bias for each test case.\n\nAnalytical expectations:\n- For a Gaussian $\\,X\\sim\\mathcal{N}(\\mu,\\sigma^2)\\,$, the moments $\\,\\mathbb{E}[X^2]=\\sigma^2+\\mu^2\\,$ and $\\,\\mathbb{E}[X^3]=\\mu^3+3\\mu\\sigma^2\\,$ are well-tested facts. For a mixture $\\,\\sum_k \\omega_k \\mathcal{N}(\\mu_k,\\sigma_k^2)\\,$, $\\,\\mathbb{E}[X^2]=\\sum_k \\omega_k(\\sigma_k^2+\\mu_k^2)\\,$ and $\\,\\mathbb{E}[X^3]=\\sum_k \\omega_k(\\mu_k^3+3\\mu_k\\sigma^2)\\,$.\n\nTest suite:\n- Use sample size $\\,n=200000\\,$ and a deterministic random seed $\\,12345\\,$.\n- The four proposal cases $\\,\\tilde{\\pi}(x)\\,$ are:\n    1. Case A (happy path): $\\,\\tilde{\\omega}_1=0.6\\,$, $\\,\\tilde{\\sigma}_1=0.5\\,$, $\\,\\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.4\\,$, $\\,\\tilde{\\sigma}_2=0.7\\,$, $\\,\\tilde{\\mu}_2=2\\,$.\n    2. Case B (missed mode): $\\,\\tilde{\\omega}_1=1.0\\,$, $\\,\\tilde{\\sigma}_1=0.5\\,$, $\\,\\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.0\\,$, $\\,\\tilde{\\sigma}_2=0.7\\,$, $\\,\\tilde{\\mu}_2=2\\,$.\n    3. Case C (underweighted minor mode): $\\,\\tilde{\\omega}_1=0.85\\,$, $\\,\\tilde{\\sigma}_1=0.5\\,$, $\\,\\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.15\\,$, $\\,\\tilde{\\sigma}_2=0.7\\,$, $\\,\\tilde{\\mu}_2=2\\,$.\n    4. Case D (variance mis-specified in minor mode): $\\,\\tilde{\\omega}_1=0.6\\,$, $\\,\\tilde{\\sigma}_1=0.5\\,$, $\\,\\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.4\\,$, $\\,\\tilde{\\sigma}_2=0.35\\,$, $\\,\\tilde{\\mu}_2=2\\,$.\n- For every case, set $\\,c=1\\,$ in the control variate estimator and $\\,h(x)=x^2\\,$.\n\nRequired outputs:\n- For each test case, compute three quantities:\n    1. The empirical Monte Carlo bias of the control variate estimator: $\\,\\widehat{\\mu}_{\\text{cv}}(1) - \\mathbb{E}_\\pi[f]\\,$.\n    2. The empirical Monte Carlo bias of the plain self-normalized importance sampling estimator: $\\,\\widehat{\\mu} - \\mathbb{E}_\\pi[f]\\,$.\n    3. The analytical asymptotic bias of the control variate estimator: $\\, -\\big(\\mathbb{E}_\\pi[h]-\\mathbb{E}_{\\tilde{\\pi}}[h]\\big)\\,$.\n- Your program should produce a single line of output containing these $\\,12\\,$ results, ordered case-by-case and flattened, as a comma-separated list enclosed in square brackets, for example $\\,\\big[\\text{A1},\\text{A2},\\text{A3},\\text{B1},\\dots,\\text{D3}\\big]\\,$. There are no physical units involved in this problem.",
            "solution": "The problem requires an evaluation of self-normalized importance sampling with control variates, specifically focusing on the bias introduced by an imperfect approximation of the target posterior distribution. The target posterior $\\,\\pi(x)\\,$ is a bimodal Gaussian mixture. The proposal distribution $\\,q(x)\\,$ is also a Gaussian mixture, denoted $\\,\\tilde{\\pi}(x)\\,$, which serves as a Laplace-approximation-based model of $\\,\\pi(x)\\,$. We will compute the analytical and empirical biases of estimators for $\\,\\mathbb{E}_\\pi[f(X)]\\,$ where $\\,f(x)=x^3\\,$ for four different proposal distributions.\n\nFirst, we establish the ground truth by calculating the exact expectations of the estimand $\\,f(x)=x^3\\,$ and the control variate $\\,h(x)=x^2\\,$ with respect to the target posterior distribution $\\,\\pi(x)\\,$.\n\nThe target posterior is $\\,\\pi(x) = \\omega_1\\,\\mathcal{N}(x;\\mu_1,\\sigma_1^2) + \\omega_2\\,\\mathcal{N}(x;\\mu_2,\\sigma_2^2)\\,$ with parameters:\n- Component 1: $\\,\\mu_1=-2\\,$, $\\,\\sigma_1=0.5\\,$, $\\,\\omega_1=0.6\\,$\n- Component 2: $\\,\\mu_2=2\\,$, $\\,\\sigma_2=0.7\\,$, $\\,\\omega_2=0.4\\,$\n\nFor a Gaussian distribution $\\,X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\,$, the second and third moments are $\\,\\mathbb{E}[X^2]=\\sigma^2+\\mu^2\\,$ and $\\,\\mathbb{E}[X^3]=\\mu^3+3\\mu\\sigma^2\\,$. For a mixture, the moments are the weighted sum of the component moments.\n\nFor component 1:\n$\\,\\mathbb{E}_1[X^2] = \\sigma_1^2 + \\mu_1^2 = (0.5)^2 + (-2)^2 = 0.25 + 4 = 4.25\\,$\n$\\,\\mathbb{E}_1[X^3] = \\mu_1^3 + 3\\mu_1\\sigma_1^2 = (-2)^3 + 3(-2)(0.5)^2 = -8 - 1.5 = -9.5\\,$\n\nFor component 2:\n$\\,\\mathbb{E}_2[X^2] = \\sigma_2^2 + \\mu_2^2 = (0.7)^2 + (2)^2 = 0.49 + 4 = 4.49\\,$\n$\\,\\mathbb{E}_2[X^3] = \\mu_2^3 + 3\\mu_2\\sigma_2^2 = (2)^3 + 3(2)(0.7)^2 = 8 + 2.94 = 10.94\\,$\n\nThe moments of the target posterior $\\,\\pi(x)\\,$ are:\n$\\,\\mathbb{E}_\\pi[h(X)] = \\mathbb{E}_\\pi[X^2] = \\omega_1\\mathbb{E}_1[X^2] + \\omega_2\\mathbb{E}_2[X^2] = (0.6)(4.25) + (0.4)(4.49) = 2.55 + 1.796 = 4.346\\,$\n$\\,\\mathbb{E}_\\pi[f(X)] = \\mathbb{E}_\\pi[X^3] = \\omega_1\\mathbb{E}_1[X^3] + \\omega_2\\mathbb{E}_2[X^3] = (0.6)(-9.5) + (0.4)(10.94) = -5.7 + 4.376 = -1.324\\,$\n\nThese are the true values used for calculating biases. The true value of the estimand is $\\,\\mathbb{E}_\\pi[f] = -1.324\\,$.\n\nThe procedure for each test case is as follows:\n1.  Define the proposal distribution $\\,q(x) = \\tilde{\\pi}(x)\\,$ from the case parameters.\n2.  Calculate the expectation of the control variate under the proposal, $\\,\\mathbb{E}_{\\tilde{\\pi}}[h(X)]\\,$.\n3.  Calculate the analytical asymptotic bias of the control variate estimator: $\\,\\text{Bias}_{\\infty} = -\\big(\\mathbb{E}_\\pi[h] - \\mathbb{E}_{\\tilde{\\pi}}[h]\\big)\\,$. This is the third required output for the case.\n4.  Perform a Monte Carlo simulation with $\\,n=200000\\,$ samples $\\,X_i \\sim q(x)\\,$ using a random seed of $\\,12345\\,$.\n5.  For each sample $\\,X_i\\,$, compute the importance weight $\\,w(X_i) = \\pi(X_i)/q(X_i)\\,$.\n6.  Compute the plain self-normalized importance sampling estimate: $\\,\\widehat{\\mu} = \\frac{\\sum_i w(X_i) f(X_i)}{\\sum_i w(X_i)}\\,$.\n7.  Compute the control variate estimate (with $\\,c=1\\,$ and approximate expectation for $\\,h\\,$): $\\,\\widehat{\\mu}_{\\text{cv}}(1) = \\widehat{\\mu} - \\left(\\frac{\\sum_i w(X_i) h(X_i)}{\\sum_i w(X_i)} - \\mathbb{E}_{\\tilde{\\pi}}[h]\\right)\\,$.\n8.  Calculate the empirical bias for the plain estimator: $\\,\\widehat{\\mu} - \\mathbb{E}_\\pi[f]\\,$. This is the second required output.\n9.  Calculate the empirical bias for the control variate estimator: $\\,\\widehat{\\mu}_{\\text{cv}}(1) - \\mathbb{E}_\\pi[f]\\,$. This is the first required output.\n\n### Case A: Happy Path\nThe proposal $\\,\\tilde{\\pi}(x)\\,$ is identical to the target $\\,\\pi(x)\\,$.\n- Parameters: $\\,\\tilde{\\omega}_1=0.6, \\tilde{\\sigma}_1=0.5, \\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.4, \\tilde{\\sigma}_2=0.7, \\tilde{\\mu}_2=2\\,$.\n- $\\,\\mathbb{E}_{\\tilde{\\pi}}[h] = \\mathbb{E}_{\\pi}[h] = 4.346\\,$.\n- Analytical asymptotic bias: $\\,\\text{Bias}_{\\infty} = -(4.346 - 4.346) = 0\\,$.\n- In simulation, $\\,q(x) = \\pi(x)\\,$, so all importance weights $\\,w(x_i)=1\\,$. The estimators become standard Monte Carlo estimators. The empirical biases for both $\\,\\widehat{\\mu}\\,$ and $\\,\\widehat{\\mu}_{\\text{cv}}(1)\\,$ should be small, arising solely from sampling variability.\n\n### Case B: Missed Mode\nThe proposal $\\,\\tilde{\\pi}(x)\\,$ consists only of the first mode.\n- Parameters: $\\,\\tilde{\\omega}_1=1.0, \\tilde{\\sigma}_1=0.5, \\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.0\\,$.\n- The proposal is $\\,\\tilde{\\pi}(x) = \\mathcal{N}(x; -2, 0.5^2)\\,$.\n- $\\,\\mathbb{E}_{\\tilde{\\pi}}[h] = \\tilde{\\sigma}_1^2 + \\tilde{\\mu}_1^2 = (0.5)^2 + (-2)^2 = 4.25\\,$.\n- Analytical asymptotic bias: $\\,\\text{Bias}_{\\infty} = -(4.346 - 4.25) = -0.096\\,$.\n- The simulation samples exclusively from the first mode. The importance weights $\\,w(X_i) = \\pi(X_i)/\\mathcal{N}(X_i; -2, 0.5^2)\\,$ will have very high variance, as no samples explore the region of the second mode of $\\,\\pi(x)\\,$. We expect large empirical biases for both estimators.\n\n### Case C: Underweighted Minor Mode\nThe proposal $\\,\\tilde{\\pi}(x)\\,$ correctly identifies both modes but misjudges their relative weights.\n- Parameters: $\\,\\tilde{\\omega}_1=0.85, \\tilde{\\sigma}_1=0.5, \\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.15, \\tilde{\\sigma}_2=0.7, \\tilde{\\mu}_2=2\\,$.\n- The component expectations are $\\,\\mathbb{E}_1[X^2] = 4.25\\,$ and $\\,\\mathbb{E}_2[X^2] = 4.49\\,$.\n- $\\,\\mathbb{E}_{\\tilde{\\pi}}[h] = (0.85)(4.25) + (0.15)(4.49) = 3.6125 + 0.6735 = 4.286\\,$.\n- Analytical asymptotic bias: $\\,\\text{Bias}_{\\infty} = -(4.346 - 4.286) = -0.06\\,$.\n- Sampling undersamples the second mode. Importance weights for samples from the second mode will be larger on average ($\\,\\approx\\omega_2/\\tilde{\\omega}_2 = 0.4/0.15 \\approx 2.67\\,$), increasing the variance of the estimators compared to Case A. We expect moderate empirical biases.\n\n### Case D: Variance Mis-specified in Minor Mode\nThe proposal $\\,\\tilde{\\pi}(x)\\,$ has correct weights and means, but the variance of the second component is too small.\n- Parameters: $\\,\\tilde{\\omega}_1=0.6, \\tilde{\\sigma}_1=0.5, \\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.4, \\tilde{\\sigma}_2=0.35, \\tilde{\\mu}_2=2\\,$.\n- Component 1 expectation is $\\,\\mathbb{E}_1[X^2] = 4.25\\,$. For component 2 of the proposal: $\\,\\mathbb{E}_{\\tilde{\\pi},2}[X^2] = (0.35)^2 + 2^2 = 0.1225 + 4 = 4.1225\\,$.\n- $\\,\\mathbb{E}_{\\tilde{\\pi}}[h] = (0.6)(4.25) + (0.4)(4.1225) = 2.55 + 1.649 = 4.199\\,$.\n- Analytical asymptotic bias: $\\,\\text{Bias}_{\\infty} = -(4.346 - 4.199) = -0.147\\,$.\n- The proposal for the second mode is narrower than the target. Samples drawn from this component that fall in the tails of the true component distribution will receive extremely high weights, leading to high estimator variance. We expect significant empirical biases.\n\nThe empirical results from the simulation will reflect not only the asymptotic bias but also the finite-sample bias and variance of the estimators. The asymptotic bias provides a theoretical baseline for the bias of the control variate estimator, which is a deterministic consequence of the mismatch between the approximate and true posteriors.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Implements and evaluates self-normalized importance sampling with a control variate\n    for a bimodal target distribution, quantifying biases from imperfect proposals.\n    \"\"\"\n    # Problem-wide parameters\n    N_SAMPLES = 200000\n    SEED = 12345\n    ESTIMAND_FUNC = lambda x: x**3\n    CV_FUNC = lambda x: x**2\n    C_VAL = 1.0\n\n    # Target posterior distribution pi(x) parameters\n    pi_params = {\n        'mus': np.array([-2.0, 2.0]),\n        'sigmas': np.array([0.5, 0.7]),\n        'weights': np.array([0.6, 0.4])\n    }\n\n    # Test cases for the proposal distribution q(x) = pi_tilde(x)\n    test_cases = [\n        # Case A: Happy path (q = pi)\n        {\n            'mus': np.array([-2.0, 2.0]),\n            'sigmas': np.array([0.5, 0.7]),\n            'weights': np.array([0.6, 0.4])\n        },\n        # Case B: Missed mode\n        {\n            'mus': np.array([-2.0, 2.0]),\n            'sigmas': np.array([0.5, 0.7]),\n            'weights': np.array([1.0, 0.0])\n        },\n        # Case C: Underweighted minor mode\n        {\n            'mus': np.array([-2.0, 2.0]),\n            'sigmas': np.array([0.5, 0.7]),\n            'weights': np.array([0.85, 0.15])\n        },\n        # Case D: Variance mis-specified in minor mode\n        {\n            'mus': np.array([-2.0, 2.0]),\n            'sigmas': np.array([0.5, 0.35]),\n            'weights': np.array([0.6, 0.4])\n        }\n    ]\n\n    def gaussian_mixture_pdf(x, params):\n        \"\"\"Calculates the PDF of a Gaussian mixture.\"\"\"\n        pdf_val = 0.0\n        for mu, sigma, weight in zip(params['mus'], params['sigmas'], params['weights']):\n            if weight > 0:\n                pdf_val += weight * norm.pdf(x, loc=mu, scale=sigma)\n        return pdf_val\n\n    def gaussian_mixture_moment(k, params):\n        \"\"\"Calculates the k-th moment of a Gaussian mixture.\"\"\"\n        total_moment = 0.0\n        for mu, sigma, weight in zip(params['mus'], params['sigmas'], params['weights']):\n            if weight > 0:\n                if k == 2:\n                    moment = sigma**2 + mu**2\n                elif k == 3:\n                    moment = mu**3 + 3 * mu * sigma**2\n                else:\n                    raise ValueError(\"Only k=2 and k=3 are supported.\")\n                total_moment += weight * moment\n        return total_moment\n    \n    def sample_from_mixture(n, params, rng):\n        \"\"\"Generates n samples from a Gaussian mixture.\"\"\"\n        n_components = len(params['mus'])\n        # Handle cases where some weights are 0\n        active_indices = np.where(params['weights'] > 0)[0]\n        active_weights = params['weights'][active_indices]\n        \n        # Normalize weights in case they don't sum to 1 after filtering\n        active_weights /= np.sum(active_weights)\n\n        component_choices = rng.choice(active_indices, size=n, p=active_weights)\n        samples = rng.normal(\n            loc=params['mus'][component_choices],\n            scale=params['sigmas'][component_choices]\n        )\n        return samples\n\n    # Calculate true expectations for the target distribution pi\n    true_f_exp = gaussian_mixture_moment(3, pi_params)  # E_pi[f] = E_pi[x^3]\n    true_h_exp = gaussian_mixture_moment(2, pi_params)  # E_pi[h] = E_pi[x^2]\n\n    # Initialize random number generator\n    rng = np.random.default_rng(SEED)\n    \n    results = []\n    \n    for q_params in test_cases:\n        # 1. Analytical calculations for the proposal q\n        exp_h_q = gaussian_mixture_moment(2, q_params)\n        \n        # 2. Analytical asymptotic bias for the CV estimator\n        analytical_bias_cv = -(true_h_exp - exp_h_q)\n        \n        # 3. Monte Carlo Simulation\n        samples = sample_from_mixture(N_SAMPLES, q_params, rng)\n        \n        # 4. Calculate importance weights\n        pi_pdf_vals = gaussian_mixture_pdf(samples, pi_params)\n        q_pdf_vals = gaussian_mixture_pdf(samples, q_params)\n        \n        # Avoid division by zero if q_pdf is zero (should not happen with this setup)\n        # However, it's good practice. A zero q_pdf where pi_pdf is non-zero implies\n        # infinite variance, which the results will reflect via large weights.\n        weights = np.divide(pi_pdf_vals, q_pdf_vals, out=np.zeros_like(pi_pdf_vals), where=q_pdf_vals!=0)\n\n        # 5. Calculate estimators\n        f_vals = ESTIMAND_FUNC(samples)\n        h_vals = CV_FUNC(samples)\n        \n        sum_weights = np.sum(weights)\n        \n        # Plain self-normalized IS estimator for E[f]\n        mu_hat = np.sum(weights * f_vals) / sum_weights\n        \n        # Self-normalized IS estimator for E[h]\n        mu_hat_h = np.sum(weights * h_vals) / sum_weights\n        \n        # Control variate estimator\n        mu_hat_cv = mu_hat - C_VAL * (mu_hat_h - exp_h_q)\n\n        # 6. Calculate empirical biases\n        empirical_bias_cv = mu_hat_cv - true_f_exp\n        empirical_bias_plain = mu_hat - true_f_exp\n        \n        # Store results in the required order\n        results.extend([empirical_bias_cv, empirical_bias_plain, analytical_bias_cv])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice moves beyond re-weighting static samples to the modern paradigm of actively transforming a simple distribution into the target posterior. You will implement a foundational version of Stein Variational Gradient Descent (SVGD), an algorithm that iteratively pushes a set of particles toward the target distribution by following a functional gradient in a kernel-defined space . This exercise provides hands-on experience with a powerful, research-level algorithm, developing intuition for how geometric concepts are revolutionizing posterior approximation.",
            "id": "3289066",
            "problem": "You will implement a program that constructs an approximate transport map to push forward a known sampling distribution into a target posterior by optimizing within a Reproducing Kernel Hilbert Space (RKHS) to reduce a Stein discrepancy. The derivation and algorithm must begin from first principles and core definitions of the Stein operator and kernelized Stein discrepancy. You will then examine robustness of the optimized transport to misspecified kernel bandwidth choices.\n\nStart from the following base:\n- Let the target posterior density be $ p(\\mathbf{x}) $ on $ \\mathbb{R}^d $. You may evaluate $ \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}) $, known as the score function.\n- Let the sampling distribution be $ q(\\mathbf{x}) $, from which you draw independent samples $ \\{\\mathbf{z}_i\\}_{i=1}^n $. Define a transport map $ T(\\mathbf{x}) = \\mathbf{x} + f(\\mathbf{x}) $, with $ f $ taking values in a vector-valued RKHS induced by a positive definite kernel $ k(\\mathbf{x}, \\mathbf{y}) $.\n- The pushforward measure $ T_{\\#} q $ is the distribution of $ T(\\mathbf{Z}) $ where $ \\mathbf{Z} \\sim q $. You will approximate $ p $ by $ T_{\\#} q $ via functional optimization of $ f $.\n\nFundamental definitions to use:\n- For a differentiable test function $ \\phi: \\mathbb{R}^d \\to \\mathbb{R}^d $, the Stein operator for $ p $ is $ \\mathcal{T}_p \\phi(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})^{\\top} \\phi(\\mathbf{x}) + \\nabla_{\\mathbf{x}} \\cdot \\phi(\\mathbf{x}) $.\n- Given a scalar-valued positive definite kernel $ k(\\mathbf{x}, \\mathbf{y}) $ with associated scalar RKHS $ \\mathcal{H} $, the kernelized Stein discrepancy (KSD) between an empirical measure supported at points $ \\{\\mathbf{x}_i\\}_{i=1}^n $ and $ p $ is defined by taking the supremum of the expectation of the Stein operator over the unit ball of a vector-valued RKHS $ \\mathcal{H}^d $ built from $ k $. The squared KSD admits a closed-form empirical estimator when $ k $ is smooth and translation-invariant.\n\nYou will:\n- Derive, from the definitions above, a principled functional gradient descent step in $ \\mathcal{H}^d $ for minimizing a discrepancy between $ T_{\\#} q $ and $ p $ by updating particles $ \\{\\mathbf{x}_i\\}_{i=1}^n $ according to a direction in the RKHS that depends on the kernel and the score of $ p $. You must start from the Stein operator definition and the reproducing property, and you must not assume any pre-given “shortcut” formula.\n- Specialize to a radial basis function (RBF) kernel $ k(\\mathbf{x}, \\mathbf{y}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{y}\\|^2}{2 h^2}\\right) $. You will need gradients and mixed derivatives of $ k $ with respect to $ \\mathbf{x} $ and $ \\mathbf{y} $.\n\nImplementation requirements:\n- Implement an algorithm that, starting from $ n $ samples $ \\{\\mathbf{z}_i\\}_{i=1}^n \\sim q $, constructs particles $ \\{\\mathbf{x}_i^{(t)}\\} $ iteratively using a functional gradient in $ \\mathcal{H}^d $ that aims to reduce a Stein discrepancy with respect to $ p $. Use a fixed step size per test case.\n- At the end of the iterations, compute the empirical kernelized Stein discrepancy between the final particle set and $ p $ using the RBF kernel and its required derivatives. Use a bandwidth $ h_{\\mathrm{eval}} $ chosen by the median pairwise distance heuristic on the final particle set.\n- Examine robustness to misspecified kernels by varying the kernel bandwidth used within the optimization updates while keeping all other settings fixed. To do this, set the optimization bandwidth at each iteration to be $ h_t = f \\cdot \\mathrm{median\\_pairwise\\_distance}(\\{\\mathbf{x}_i^{(t)}\\}) $, where $ f $ is a fixed factor specified by each test case. The evaluation bandwidth $ h_{\\mathrm{eval}} $ for computing the final kernelized Stein discrepancy must always use $ f = 1 $ on the final particle set.\n\nTarget posterior and score:\n- Use a $ d = 2 $ dimensional Gaussian posterior $ p(\\mathbf{x}) = \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) $ with $ \\boldsymbol{\\mu} = (2, -1) $ and $ \\boldsymbol{\\Sigma} = \\begin{bmatrix} 1  0.8 \\\\ 0.8  2 \\end{bmatrix} $. The score is $ \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}) = - \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu}) $.\n- Use $ q(\\mathbf{x}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_2) $ as the initial sampling distribution for the particles.\n\nNumerical and algorithmic settings:\n- Number of particles $ n = 100 $.\n- Fixed step size $ \\epsilon = 0.02 $ per iteration.\n- Use the RBF kernel $ k $ throughout.\n\nTest suite:\nYou must run the following four test cases and report their final kernelized Stein discrepancy values as floats:\n- Case $ 1 $ (happy path): iterations $ T = 100 $, optimization bandwidth factor $ f = 1.0 $.\n- Case $ 2 $ (over-smoothing): iterations $ T = 100 $, optimization bandwidth factor $ f = 10.0 $.\n- Case $ 3 $ (under-smoothing): iterations $ T = 100 $, optimization bandwidth factor $ f = 0.1 $.\n- Case $ 4 $ (boundary: no transport): iterations $ T = 0 $, optimization bandwidth factor $ f = 1.0 $.\n\nRandomness:\n- Use a fixed random seed $ 12345 $ when drawing $ \\{\\mathbf{z}_i\\}_{i=1}^n $ so that the results are reproducible.\n\nOutput:\n- Your program must output a single line containing a list of the four final kernelized Stein discrepancy values corresponding to the four cases above, as a comma-separated list enclosed in square brackets, for example, $[v_1,v_2,v_3,v_4]$. Each $ v_i $ must be printed as a floating-point number.\n\nNo physical units or angle units are involved. Express any fractional quantities in decimal form when printed.",
            "solution": "We derive and implement an optimization of a transport map in a Reproducing Kernel Hilbert Space (RKHS) to approximate a target posterior by minimizing a Stein discrepancy, then study robustness to kernel misspecification.\n\nFoundations and definitions. Let $ p(\\mathbf{x}) $ be a target posterior density on $ \\mathbb{R}^d $ with a continuously differentiable log-density. Define the score $ \\mathbf{s}_p(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}) $. For a differentiable test function $ \\phi: \\mathbb{R}^d \\to \\mathbb{R}^d $, the Stein operator for $ p $ is\n\n$$\n\\mathcal{T}_p \\phi(\\mathbf{x}) \\triangleq \\mathbf{s}_p(\\mathbf{x})^{\\top} \\phi(\\mathbf{x}) + \\nabla_{\\mathbf{x}} \\cdot \\phi(\\mathbf{x}).\n$$\n\nIf $ \\mathbf{X} \\sim p $ and $ \\phi $ and $ p $ satisfy appropriate boundary conditions, then $ \\mathbb{E}_p[\\mathcal{T}_p \\phi(\\mathbf{X})] = 0 $. When $ \\mathbf{X} \\sim r $ for some distribution $ r $ not equal to $ p $, generally $ \\mathbb{E}_r[\\mathcal{T}_p \\phi(\\mathbf{X})] \\neq 0 $.\n\nReproducing Kernel Hilbert Space structure. Let $ k(\\mathbf{x}, \\mathbf{y}) $ be a positive definite kernel on $ \\mathbb{R}^d $, and $ \\mathcal{H} $ its scalar RKHS. Consider the vector-valued RKHS $ \\mathcal{H}^d $ of vector functions $ \\phi = (\\phi_1, \\ldots, \\phi_d) $ with each $ \\phi_j \\in \\mathcal{H} $. The reproducing property implies, for any $ f \\in \\mathcal{H} $, $ f(\\mathbf{x}) = \\langle f(\\cdot), k(\\cdot, \\mathbf{x}) \\rangle_{\\mathcal{H}} $, and similar coordinate-wise for $ \\phi \\in \\mathcal{H}^d $.\n\nKernelized Stein discrepancy (KSD). The kernelized Stein discrepancy between an empirical distribution supported on $ \\{\\mathbf{x}_i\\}_{i=1}^n $ and $ p $ is\n\n$$\n\\mathrm{KSD}(\\{\\mathbf{x}_i\\}, p; k) = \\sup_{\\phi \\in \\mathcal{H}^d: \\|\\phi\\|_{\\mathcal{H}^d} \\leq 1} \\left[ \\frac{1}{n} \\sum_{i=1}^n \\mathcal{T}_p \\phi(\\mathbf{x}_i) \\right].\n$$\n\nUnder smoothness conditions and with a translation-invariant kernel, the squared empirical KSD admits a closed-form expression:\n\n$$\n\\mathrm{KSD}^2 = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n u(\\mathbf{x}_i, \\mathbf{x}_j),\n$$\n\nwhere\n\n$$\nu(\\mathbf{x}, \\mathbf{y}) = \\mathbf{s}_p(\\mathbf{x})^{\\top} k(\\mathbf{x}, \\mathbf{y}) \\mathbf{s}_p(\\mathbf{y}) + \\mathbf{s}_p(\\mathbf{x})^{\\top} \\nabla_{\\mathbf{y}} k(\\mathbf{x}, \\mathbf{y}) + \\mathbf{s}_p(\\mathbf{y})^{\\top} \\nabla_{\\mathbf{x}} k(\\mathbf{x}, \\mathbf{y}) + \\mathrm{tr}\\left( \\nabla_{\\mathbf{x}} \\nabla_{\\mathbf{y}} k(\\mathbf{x}, \\mathbf{y}) \\right).\n$$\n\nFor the radial basis function (RBF) kernel $ k(\\mathbf{x}, \\mathbf{y}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{y}\\|^2}{2 h^2}\\right) $, we require the derivatives:\n\n$$\n\\nabla_{\\mathbf{x}} k(\\mathbf{x}, \\mathbf{y}) = - \\frac{1}{h^2} k(\\mathbf{x}, \\mathbf{y}) (\\mathbf{x} - \\mathbf{y}), \\quad \\nabla_{\\mathbf{y}} k(\\mathbf{x}, \\mathbf{y}) = \\frac{1}{h^2} k(\\mathbf{x}, \\mathbf{y}) (\\mathbf{x} - \\mathbf{y}),\n$$\n\n\n$$\n\\mathrm{tr}\\left( \\nabla_{\\mathbf{x}} \\nabla_{\\mathbf{y}} k(\\mathbf{x}, \\mathbf{y}) \\right) = k(\\mathbf{x}, \\mathbf{y}) \\left( \\frac{d}{h^2} - \\frac{\\|\\mathbf{x}-\\mathbf{y}\\|^2}{h^4} \\right).\n$$\n\n\nTransport map and functional gradient flow. Let $ q(\\mathbf{x}) $ be the initial distribution, and consider $ T(\\mathbf{x}) = \\mathbf{x} + f(\\mathbf{x}) $, where $ f \\in \\mathcal{H}^d $. One approach to approximating $ p $ is to minimize a divergence between $ T_{\\#} q $ and $ p $, such as the Kullback–Leibler divergence, using a functional gradient restricted to $ \\mathcal{H}^d $. Under this setting, a steepest descent direction in $ \\mathcal{H}^d $ for reducing the divergence induces the update\n\n$$\n\\mathbf{x}_i \\leftarrow \\mathbf{x}_i + \\epsilon \\, \\phi^{\\star}(\\mathbf{x}_i),\n$$\n\nwhere\n\n$$\n\\phi^{\\star}(\\cdot) \\in \\mathrm{argmax}_{\\|\\phi\\|_{\\mathcal{H}^d} \\leq 1} \\left\\langle \\phi, \\, \\frac{1}{n} \\sum_{j=1}^n \\left[ k(\\cdot, \\mathbf{x}_j) \\, \\mathbf{s}_p(\\mathbf{x}_j) + \\nabla_{\\mathbf{x}_j} k(\\mathbf{x}_j, \\cdot) \\right] \\right\\rangle_{\\mathcal{H}^d}.\n$$\n\nBy the Riesz representation theorem in $ \\mathcal{H}^d $, the optimizer aligns with the representer\n\n$$\n\\phi^{\\star}(\\cdot) \\propto \\frac{1}{n} \\sum_{j=1}^n \\left[ k(\\cdot, \\mathbf{x}_j) \\, \\mathbf{s}_p(\\mathbf{x}_j) + \\nabla_{\\mathbf{x}_j} k(\\mathbf{x}_j, \\cdot) \\right].\n$$\n\nChoosing a step size $ \\epsilon > 0 $ yields the practical particle update\n\n$$\n\\mathbf{x}_i \\leftarrow \\mathbf{x}_i + \\epsilon \\, \\frac{1}{n} \\sum_{j=1}^n \\left[ k(\\mathbf{x}_i, \\mathbf{x}_j) \\, \\mathbf{s}_p(\\mathbf{x}_j) + \\nabla_{\\mathbf{x}_j} k(\\mathbf{x}_j, \\mathbf{x}_i) \\right].\n$$\n\nThis functional gradient step is the canonical gradient flow in the RKHS that decreases a Stein discrepancy and is known to reduce the Kullback–Leibler divergence under suitable conditions.\n\nSpecialization to the RBF kernel. For the RBF kernel $ k(\\mathbf{x}, \\mathbf{y}) $, we have\n\n$$\n\\nabla_{\\mathbf{x}_j} k(\\mathbf{x}_j, \\mathbf{x}_i) = - \\frac{1}{h^2} k(\\mathbf{x}_j, \\mathbf{x}_i) \\, (\\mathbf{x}_j - \\mathbf{x}_i).\n$$\n\nThus, the per-iteration direction at $ \\mathbf{x}_i $ is\n\n$$\n\\phi(\\mathbf{x}_i) = \\frac{1}{n} \\sum_{j=1}^n \\left[ k(\\mathbf{x}_i, \\mathbf{x}_j) \\, \\mathbf{s}_p(\\mathbf{x}_j) - \\frac{1}{h^2} k(\\mathbf{x}_i, \\mathbf{x}_j) \\, (\\mathbf{x}_i - \\mathbf{x}_j) \\right].\n$$\n\n\nBandwidth selection and misspecification study. We set the optimization bandwidth at iteration $ t $ to be\n\n$$\nh_t = f \\cdot \\mathrm{median}\\left( \\{ \\|\\mathbf{x}_i^{(t)} - \\mathbf{x}_j^{(t)}\\| : i  j \\} \\right),\n$$\n\nwhere $ f $ is a fixed factor. We assess robustness by choosing $ f \\in \\{ 1.0, 10.0, 0.1 \\} $. For final evaluation of the kernelized Stein discrepancy, we define an evaluation bandwidth $ h_{\\mathrm{eval}} $ using the same median heuristic on the final particle set with $ f = 1.0 $. This separates the optimization kernel choice from the evaluation kernel.\n\nTarget posterior and score. We choose $ d = 2 $, $ p(\\mathbf{x}) = \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) $ with $ \\boldsymbol{\\mu} = (2, -1) $ and\n\n$$\n\\boldsymbol{\\Sigma} = \\begin{bmatrix} 1  0.8 \\\\ 0.8  2 \\end{bmatrix}, \\quad \\mathbf{s}_p(\\mathbf{x}) = - \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}).\n$$\n\nWe initialize $ n = 100 $ particles from $ q(\\mathbf{x}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_2) $ with a fixed seed $ 12345 $. We apply a fixed step size $ \\epsilon = 0.02 $ and run the specified number of iterations for each test case.\n\nComputation of the KSD. Given final particles $ \\{\\mathbf{x}_i\\}_{i=1}^n $, the squared KSD under the RBF kernel bandwidth $ h_{\\mathrm{eval}} $ is\n\n$$\n\\widehat{\\mathrm{KSD}}^2 = \\frac{1}{n^2} \\sum_{i,j=1}^n \\left[ \\mathbf{s}_p(\\mathbf{x}_i)^{\\top} \\mathbf{s}_p(\\mathbf{x}_j) \\, k_{ij} + \\mathbf{s}_p(\\mathbf{x}_i)^{\\top} \\nabla_{\\mathbf{y}} k(\\mathbf{x}_i, \\mathbf{x}_j) + \\mathbf{s}_p(\\mathbf{x}_j)^{\\top} \\nabla_{\\mathbf{x}} k(\\mathbf{x}_i, \\mathbf{x}_j) + \\mathrm{tr}\\left( \\nabla_{\\mathbf{x}} \\nabla_{\\mathbf{y}} k(\\mathbf{x}_i, \\mathbf{x}_j) \\right) \\right],\n$$\n\nwith $ k_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j) $. For the RBF kernel, substituting the derivatives yields a fully vectorizable computation.\n\nTest suite and expected behavior. We consider four cases:\n- Case $ 1 $: $ T = 100 $, $ f = 1.0 $. This “happy path” should reduce the KSD substantially compared to no transport.\n- Case $ 2 $: $ T = 100 $, $ f = 10.0 $. Over-smoothing tends to induce near-rigid motions and can slow or stall convergence, yielding a larger KSD than Case $ 1 $.\n- Case $ 3 $: $ T = 100 $, $ f = 0.1 $. Under-smoothing yields highly localized interactions, potentially causing particle clustering and poorer global transport, typically larger KSD than Case $ 1 $.\n- Case $ 4 $: $ T = 0 $, $ f = 1.0 $. No transport baseline; KSD should be the largest among the four or at least not smaller than well-tuned transport.\n\nThe program implements the update rule, computes the final KSD under the evaluation bandwidth, and prints a single line with the four KSD values in the order of the cases. The output is formatted as a comma-separated list enclosed in square brackets.\n\nAlgorithmic summary:\n- Initialize $ \\{\\mathbf{x}_i^{(0)}\\} \\sim q $ with seed $ 12345 $.\n- For iteration $ t = 0, \\ldots, T-1 $:\n  - Compute $ h_t = f \\cdot \\mathrm{median\\_pairwise\\_distance}(\\{\\mathbf{x}_i^{(t)}\\}) $.\n  - Compute $ \\phi(\\mathbf{x}_i^{(t)}) = \\frac{1}{n} \\sum_j \\left[ k(\\mathbf{x}_i^{(t)}, \\mathbf{x}_j^{(t)}) \\, \\mathbf{s}_p(\\mathbf{x}_j^{(t)}) - \\frac{1}{h_t^2} k(\\mathbf{x}_i^{(t)}, \\mathbf{x}_j^{(t)}) \\, (\\mathbf{x}_i^{(t)} - \\mathbf{x}_j^{(t)}) \\right] $.\n  - Update $ \\mathbf{x}_i^{(t+1)} = \\mathbf{x}_i^{(t)} + \\epsilon \\, \\phi(\\mathbf{x}_i^{(t)}) $.\n- After $ T $ iterations, set $ h_{\\mathrm{eval}} $ by the median heuristic with $ f = 1.0 $ on $ \\{\\mathbf{x}_i^{(T)}\\} $.\n- Compute $ \\widehat{\\mathrm{KSD}}(\\{\\mathbf{x}_i^{(T)}\\}, p; h_{\\mathrm{eval}}) $.\n- Repeat for each case and print the four KSD values as a single list.\n\nThis procedure instantiates an RKHS-based optimal transport map optimized to minimize a Stein discrepancy. The robustness study is conducted by varying the optimization kernel bandwidth and observing the final KSD values. We expect the well-tuned bandwidth to yield the smallest KSD, while misspecified bandwidths degrade performance, and the no-transport baseline to be worst or near worst.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef gaussian_score(x, mu, Sigma_inv):\n    # x: (n, d)\n    # score: grad log N(mu, Sigma) = - Sigma^{-1} (x - mu)\n    return - (x - mu) @ Sigma_inv.T\n\ndef pairwise_sq_dists(X, Y=None):\n    if Y is None:\n        Y = X\n    # Returns squared distances matrix between rows of X and Y\n    # Using (x - y)^2 = x^2 + y^2 - 2 x.y\n    X_norm = np.sum(X**2, axis=1)[:, None]\n    Y_norm = np.sum(Y**2, axis=1)[None, :]\n    K = X @ Y.T\n    D2 = X_norm + Y_norm - 2.0 * K\n    # Numerical stability: clip small negatives to zero\n    return np.maximum(D2, 0.0)\n\ndef median_bandwidth(X):\n    n = X.shape[0]\n    D2 = pairwise_sq_dists(X)\n    # Take upper triangle excluding diagonal\n    iu = np.triu_indices(n, k=1)\n    dists = np.sqrt(D2[iu])\n    # If all points are identical, fallback to 1.0\n    if dists.size == 0:\n        return 1.0\n    med = np.median(dists)\n    if not np.isfinite(med) or med = 0.0:\n        return 1.0\n    return med\n\ndef rbf_kernel(X, Y, h):\n    # Returns kernel matrix K and pairwise differences (X_i - Y_j)\n    D2 = pairwise_sq_dists(X, Y)\n    K = np.exp(-D2 / (2.0 * h * h))\n    return K, D2\n\ndef svgd_update(X, score_func, h, step_size):\n    # X: (n, d)\n    n, d = X.shape\n    S = score_func(X)  # (n, d)\n    K, D2 = rbf_kernel(X, X, h)  # (n, n)\n    # Compute phi(X) = (1/n) sum_j [ k(x_i, x_j) s(x_j) + grad_{x_j} k(x_j, x_i) ]\n    # grad_{x_j} k(x_j, x_i) = -(x_j - x_i)/h^2 * k(x_j, x_i) = (x_i - x_j)/h^2 * k(x_j, x_i)\n    # We'll compute first term: (K @ S) shape (n, d)\n    term1 = K @ S  # (n, d)\n    # For term2: sum_j (x_i - x_j) * k(x_i, x_j) / h^2\n    # Build weighted differences sum: For each i, sum_j k_ij * (x_i - x_j)\n    # Compute sum_j k_ij * x_j\n    K_sum = np.sum(K, axis=1)[:, None]  # (n,1)\n    KX = K @ X  # (n, d)\n    term2 = (X * K_sum - KX) / (h * h)  # (n, d)\n    phi = (term1 + term2) / n\n    X_new = X + step_size * phi\n    return X_new\n\ndef ksd_rbf(X, score_func, h):\n    # Compute empirical KSD with RBF kernel and given bandwidth h\n    n, d = X.shape\n    S = score_func(X)  # (n, d)\n    K, D2 = rbf_kernel(X, X, h)  # (n, n)\n    # term1: s_i^T s_j * k_ij\n    SDot = S @ S.T  # (n, n)\n    term1 = K * SDot\n    # term2: s_i^T grad_y k(x_i, x_j) = s_i^T [ (x_i - x_j)/h^2 * k_ij ]\n    # Build matrix of s_i^T (x_i - x_j): we can compute via broadcasting\n    # Compute A_ij = s_i^T x_i, B_ij = s_i^T x_j\n    s_xi = np.sum(S * X, axis=1)[:, None]  # (n,1), s_i^T x_i\n    s_xj = S @ X.T  # (n, n), s_i^T x_j\n    s_dot_xi_minus_xj = s_xi - s_xj  # (n, n)\n    term2 = K * (s_dot_xi_minus_xj / (h * h))\n    # term3: s_j^T grad_x k(x_i, x_j) = s_j^T [ -(x_i - x_j)/h^2 * k_ij ] = k_ij/h^2 * s_j^T (x_j - x_i)\n    # s_j^T x_j: (n,1) then transpose for broadcasting\n    sj_xj = np.sum(S * X, axis=1)[None, :]  # (1,n), s_j^T x_j\n    sj_xi = X @ S.T  # (n, n), x_i^T s_j = s_j^T x_i\n    s_j_dot_xj_minus_xi = sj_xj - sj_xi  # (n, n)\n    term3 = K * (s_j_dot_xj_minus_xi / (h * h))\n    # term4: trace of mixed Hessian: k_ij * (d/h^2 - ||x_i - x_j||^2 / h^4)\n    term4 = K * (d / (h * h) - D2 / (h**4))\n    U = term1 + term2 + term3 + term4\n    ksd2 = np.mean(U)\n    # Numerical robustness: KSD^2 can be slightly negative due to finite-sample estimation; clip to zero\n    return float(np.sqrt(max(ksd2, 0.0)))\n\ndef run_case(n_particles, d, mu, Sigma, steps, opt_bandwidth_factor, step_size, seed):\n    rng = np.random.default_rng(seed)\n    X0 = rng.normal(size=(n_particles, d))\n    # Precompute Sigma^{-1}\n    Sigma_inv = np.linalg.inv(Sigma)\n    mu_vec = np.array(mu).reshape(1, d)\n\n    # Define score function closure\n    def score_func(x):\n        return gaussian_score(x, mu_vec, Sigma_inv)\n\n    X = X0.copy()\n    for t in range(steps):\n        h_t = median_bandwidth(X) * opt_bandwidth_factor\n        # Avoid too small or zero bandwidth\n        if not np.isfinite(h_t) or h_t = 1e-8:\n            h_t = 1.0\n        X = svgd_update(X, score_func, h_t, step_size)\n\n    # Evaluation KSD bandwidth uses factor 1.0 on final particles\n    h_eval = median_bandwidth(X) * 1.0\n    if not np.isfinite(h_eval) or h_eval = 1e-8:\n        h_eval = 1.0\n    ksd_val = ksd_rbf(X, score_func, h_eval)\n    return ksd_val\n\ndef solve():\n    # Problem constants\n    d = 2\n    mu = [2.0, -1.0]\n    Sigma = np.array([[1.0, 0.8],\n                      [0.8, 2.0]])\n    n_particles = 100\n    step_size = 0.02\n    seed = 12345\n\n    # Test cases: (steps, opt_bandwidth_factor)\n    test_cases = [\n        (100, 1.0),   # Case 1: happy path\n        (100, 10.0),  # Case 2: over-smoothing\n        (100, 0.1),   # Case 3: under-smoothing\n        (0, 1.0),     # Case 4: no transport\n    ]\n\n    results = []\n    for steps, f in test_cases:\n        val = run_case(n_particles, d, mu, Sigma, steps, f, step_size, seed)\n        # Format to a reasonable number of decimals for stable output\n        results.append(f\"{val:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}