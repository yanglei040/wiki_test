{
    "hands_on_practices": [
        {
            "introduction": "To master the harmonic mean estimator (HME), we must first grasp the statistical origin of its instability. This instability is not a universal flaw but arises under specific, common conditions related to the behavior of the reciprocal likelihood, $1/p(y|\\theta)$, under the posterior distribution. This exercise  brilliantly isolates this mechanism by starting with a simplified model where the likelihood has bounded support, resulting in a perfectly stable estimator, and then contrasting it with a standard unbounded model to reveal how heavy posterior tails cause the estimator's variance to explode.",
            "id": "3311537",
            "problem": "Consider a Bayesian model with parameter space $\\Theta \\subset \\mathbb{R}^{d}$ equipped with Lebesgue measure and a fixed observed data value $y$. The marginal likelihood (also called the evidence) is defined by the integral $Z = \\int_{\\Theta} p(y \\mid \\theta)\\,\\pi(\\theta)\\,d\\theta$, and the posterior density is defined by $p(\\theta \\mid y) \\propto p(y \\mid \\theta)\\,\\pi(\\theta)$, where $\\pi(\\theta)$ is a proper prior density. The harmonic mean (HM) estimator is obtained by drawing independent samples $\\theta^{(1)},\\dots,\\theta^{(M)}$ from the posterior $p(\\theta \\mid y)$ and setting $\\widehat{Z}_{\\mathrm{HM}} = \\left\\{\\frac{1}{M}\\sum_{m=1}^{M} U(\\theta^{(m)})\\right\\}^{-1}$ with $U(\\theta) = \\frac{1}{p(y \\mid \\theta)}$. A sufficient condition for the stability of the HM estimator is that $U(\\theta)$ be almost surely bounded under the posterior distribution. In this exercise, you will use a toy likelihood with bounded support to demonstrate such stability and then contrast it with an unbounded-support case to isolate the mechanism of instability, working only from core definitions and standard integration facts.\n\nAssume first that there exists a measurable set $A \\subset \\Theta$ with finite, strictly positive Lebesgue measure $|A| \\in (0,\\infty)$ such that the likelihood is $p(y \\mid \\theta) = \\mathbf{1}\\{\\theta \\in A\\}/|A|$ and the prior is the uniform density $\\pi(\\theta) = \\mathbf{1}\\{\\theta \\in A\\}/|A|$ on the same set $A$.\n\n1. Using only the definitions of $Z$ and $p(\\theta \\mid y)$, compute $Z$ and $p(\\theta \\mid y)$ in this bounded-support model. Then define $U(\\theta) = \\frac{1}{p(y \\mid \\theta)}$ and determine its distribution under $p(\\theta \\mid y)$, including its expectation and variance.\n\n2. Using the definition of $\\widehat{Z}_{\\mathrm{HM}}$ and the distributional properties of $U(\\theta)$ from part 1, compute the exact variance $\\mathrm{Var}\\!\\left(\\widehat{Z}_{\\mathrm{HM}}\\right)$ for any sample size $M \\geq 1$. Your answer must be a single real number or a single closed-form analytic expression. No rounding is required.\n\n3. Now replace the bounded-support setup with an unbounded-support example to isolate the instability mechanism. Let $\\pi(\\theta)$ be the Gaussian prior $\\pi(\\theta) = \\varphi(\\theta; 0, \\tau_{0}^{2})$ on $\\mathbb{R}$, where $\\varphi(\\,\\cdot\\,; \\mu, \\sigma^{2})$ denotes the normal density with mean $\\mu$ and variance $\\sigma^{2}$, and let the likelihood be $p(y \\mid \\theta) = \\varphi(y; \\theta, \\sigma^{2})$ as a function of $\\theta \\in \\mathbb{R}$. Derive a necessary and sufficient condition on a constant $c  0$ such that $\\mathbb{E}_{p(\\theta \\mid y)}\\!\\left[\\exp\\!\\big(c\\,(\\theta - y)^{2}\\big)\\right]$ is finite, expressing the condition in terms of the posterior variance. Then evaluate this condition at $c = \\frac{1}{2\\sigma^{2}}$ and $c = \\frac{1}{\\sigma^{2}}$ to conclude about the finiteness of $\\mathbb{E}_{p(\\theta \\mid y)}[U(\\theta)]$ and $\\mathrm{Var}_{p(\\theta \\mid y)}(U(\\theta))$, respectively, and explain briefly how this contrast identifies the mechanism by which the HM estimator becomes unstable in unbounded-support settings.\n\nReport only the value of $\\mathrm{Var}\\!\\left(\\widehat{Z}_{\\mathrm{HM}}\\right)$ from part 2 as your final answer.",
            "solution": "The problem asks for an analysis of the harmonic mean (HM) estimator for the marginal likelihood $Z$ in two different settings: a stable bounded-support case and a potentially unstable unbounded-support case. The solution is divided into three parts.\n\n**Part 1: Bounded-Support Model Analysis**\n\nFirst, we compute the marginal likelihood $Z$ and the posterior density $p(\\theta \\mid y)$ for the bounded-support model.\nThe marginal likelihood, or evidence, is defined as $Z = \\int_{\\Theta} p(y \\mid \\theta)\\,\\pi(\\theta)\\,d\\theta$.\nThe provided likelihood is $p(y \\mid \\theta) = \\frac{\\mathbf{1}\\{\\theta \\in A\\}}{|A|}$ and the prior is $\\pi(\\theta) = \\frac{\\mathbf{1}\\{\\theta \\in A\\}}{|A|}$, where $|A|$ is the finite, non-zero Lebesgue measure of the set $A$.\nSubstituting these into the definition of $Z$:\n$$ Z = \\int_{\\Theta} \\left(\\frac{\\mathbf{1}\\{\\theta \\in A\\}}{|A|}\\right) \\left(\\frac{\\mathbf{1}\\{\\theta \\in A\\}}{|A|}\\right) \\,d\\theta = \\int_{\\Theta} \\frac{\\mathbf{1}\\{\\theta \\in A\\}}{|A|^2} \\,d\\theta $$\nSince the integrand is zero outside the set $A$, we can change the domain of integration to $A$:\n$$ Z = \\frac{1}{|A|^2} \\int_{A} 1 \\,d\\theta = \\frac{1}{|A|^2} |A| = \\frac{1}{|A|} $$\nThe posterior density is given by Bayes' theorem, $p(\\theta \\mid y) = \\frac{p(y \\mid \\theta)\\pi(\\theta)}{Z}$. Using the expressions for the numerator and the value of $Z$ we just calculated:\n$$ p(\\theta \\mid y) = \\frac{\\frac{\\mathbf{1}\\{\\theta \\in A\\}}{|A|^2}}{\\frac{1}{|A|}} = \\frac{\\mathbf{1}\\{\\theta \\in A\\}}{|A|} $$\nThis result shows that the posterior distribution of $\\theta$ is a uniform distribution over the set $A$.\n\nNext, we analyze the function $U(\\theta) = \\frac{1}{p(y \\mid \\theta)}$. The harmonic mean estimator is constructed using samples $\\theta^{(m)}$ drawn from the posterior distribution $p(\\theta \\mid y)$. Since the support of $p(\\theta \\mid y)$ is the set $A$, any sample $\\theta$ drawn from this distribution will be in $A$ almost surely. For any $\\theta \\in A$, the likelihood is a constant: $p(y \\mid \\theta) = \\frac{1}{|A|}$.\nTherefore, for any posterior sample $\\theta$, the value of $U(\\theta)$ is:\n$$ U(\\theta) = \\frac{1}{p(y \\mid \\theta)} = \\frac{1}{1/|A|} = |A| $$\nThis implies that the random variable $U(\\theta)$, where $\\theta \\sim p(\\theta \\mid y)$, is a constant with value $|A|$. Its probability distribution is a point mass at $|A|$.\nThe expectation and variance of this constant random variable are straightforward to determine:\n$$ \\mathbb{E}_{p(\\theta \\mid y)}[U(\\theta)] = \\mathbb{E}[|A|] = |A| $$\n$$ \\mathrm{Var}_{p(\\theta \\mid y)}(U(\\theta)) = \\mathrm{Var}(|A|) = 0 $$\nThis completes the analysis required by Part 1. The function $U(\\theta)$ is almost surely bounded (in fact, constant) under the posterior, which satisfies the sufficient condition for the stability of the HM estimator.\n\n**Part 2: Variance of the Harmonic Mean Estimator**\n\nThe harmonic mean estimator $\\widehat{Z}_{\\mathrm{HM}}$ is defined as:\n$$ \\widehat{Z}_{\\mathrm{HM}} = \\left\\{\\frac{1}{M}\\sum_{m=1}^{M} U(\\theta^{(m)})\\right\\}^{-1} $$\nwhere $\\theta^{(1)}, \\dots, \\theta^{(M)}$ are independent samples from the posterior $p(\\theta \\mid y)$.\nFrom Part 1, we know that for any such sample $\\theta^{(m)}$, the value of $U(\\theta^{(m)})$ is deterministically equal to $|A|$.\nSubstituting this into the estimator's formula:\n$$ \\widehat{Z}_{\\mathrm{HM}} = \\left\\{\\frac{1}{M}\\sum_{m=1}^{M} |A|\\right\\}^{-1} = \\left\\{\\frac{1}{M} (M \\cdot |A|)\\right\\}^{-1} = \\left\\{|A|\\right\\}^{-1} = \\frac{1}{|A|} $$\nThe estimator $\\widehat{Z}_{\\mathrm{HM}}$ is a random variable, as it depends on the random samples $\\{\\theta^{(m)}\\}$. However, in this specific model, its value is constant and equal to $\\frac{1}{|A|}$ for any set of posterior samples and any sample size $M \\geq 1$. It is always exactly equal to the true marginal likelihood $Z$.\nThe variance of any constant random variable is zero. Therefore, the variance of the HM estimator in this model is:\n$$ \\mathrm{Var}\\!\\left(\\widehat{Z}_{\\mathrm{HM}}\\right) = \\mathrm{Var}\\!\\left(\\frac{1}{|A|}\\right) = 0 $$\nThis is the final answer requested by the problem.\n\n**Part 3: Unbounded-Support Case and Instability Mechanism**\n\nFor completeness, we analyze the unbounded-support example. The posterior $p(\\theta \\mid y)$ resulting from a Gaussian prior $\\pi(\\theta) = \\varphi(\\theta; 0, \\tau_{0}^{2})$ and Gaussian likelihood $p(y \\mid \\theta) = \\varphi(y; \\theta, \\sigma^{2})$ is also a Gaussian, $\\varphi(\\theta; \\mu_p, \\sigma_p^2)$, with posterior variance $\\sigma_p^2 = \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right)^{-1}$.\nWe are asked for the condition on $c > 0$ under which $\\mathbb{E}_{p(\\theta \\mid y)}\\!\\left[\\exp\\!\\big(c\\,(\\theta - y)^{2}\\big)\\right]$ is finite. This expectation corresponds to an integral of the form $\\int \\exp(Q(\\theta))d\\theta$, where $Q(\\theta)$ is a quadratic in $\\theta$. The integral converges if and only if the coefficient of the $\\theta^2$ term in the exponent is negative. The exponent is $c(\\theta - y)^2 - \\frac{(\\theta - \\mu_p)^2}{2\\sigma_p^2}$. The coefficient of $\\theta^2$ is $c - \\frac{1}{2\\sigma_p^2}$. Thus, the expectation is finite if and only if $c  \\frac{1}{2\\sigma_p^2}$.\n\nNow we apply this to the moments of $U(\\theta) = \\frac{1}{p(y \\mid \\theta)} = \\sqrt{2\\pi\\sigma^2}\\exp\\left(\\frac{(\\theta-y)^2}{2\\sigma^2}\\right)$.\n1.  The expectation $\\mathbb{E}_{p(\\theta \\mid y)}[U(\\theta)]$ requires finiteness for $c = \\frac{1}{2\\sigma^2}$. The condition is $\\frac{1}{2\\sigma^2}  \\frac{1}{2\\sigma_p^2}$, or $\\sigma_p^2  \\sigma^2$. This is always true since $\\sigma_p^2 = \\frac{\\sigma^2\\tau_0^2}{\\sigma^2+\\tau_0^2}$ and $\\frac{\\tau_0^2}{\\sigma^2+\\tau_0^2}  1$. So the expectation of $U(\\theta)$ is finite.\n2.  The variance $\\mathrm{Var}_{p(\\theta \\mid y)}(U(\\theta))$ exists if $\\mathbb{E}_{p(\\theta \\mid y)}[U(\\theta)^2]$ is finite. Since $U(\\theta)^2 \\propto \\exp\\left(\\frac{(\\theta-y)^2}{\\sigma^2}\\right)$, this corresponds to $c = \\frac{1}{\\sigma^2}$. The condition becomes $\\frac{1}{\\sigma^2}  \\frac{1}{2\\sigma_p^2}$, or $2\\sigma_p^2  \\sigma^2$. Substituting for $\\sigma_p^2$, we get $2\\frac{\\sigma^2\\tau_0^2}{\\sigma^2+\\tau_0^2}  \\sigma^2$, which simplifies to $\\tau_0^2  \\sigma^2$.\n\nThis reveals the instability mechanism. The variance of $U(\\theta)$ is infinite unless the prior is sufficiently informative relative to the likelihood (i.e., $\\tau_0^2  \\sigma^2$). If the prior is diffuse, the posterior will have tails that are not thin enough to suppress the explosive growth of $U(\\theta)^2 = 1/p(y \\mid \\theta)^2$ for values of $\\theta$ far from $y$. This leads to an infinite variance for $U(\\theta)$, which in turn causes the HM estimator to be highly unstable. This contrasts sharply with the bounded-support case where $U(\\theta)$ was bounded, resulting in zero variance for the estimator.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "Beyond its statistical weaknesses, the harmonic mean estimator poses significant numerical challenges in practice. The direct computation of the sum of reciprocal likelihoods, $\\sum_{i=1}^n 1/L(\\theta_i)$, is highly susceptible to floating-point overflow, especially with large datasets where likelihood values can become astronomically small. This exercise  serves as a practical guide to implementing a numerically robust HME computation by correctly applying the \"log-sum-exp trick,\" forcing a crucial distinction between achieving computational stability and correcting the underlying statistical instability of the estimator itself.",
            "id": "3311572",
            "problem": "Consider a Bayesian model with data $y$, prior density $p(\\theta)$, likelihood $L(\\theta) = p(y \\mid \\theta)$, and posterior density $p(\\theta \\mid y) \\propto L(\\theta)p(\\theta)$. The marginal likelihood (model evidence) is $Z = p(y)$. The harmonic mean estimator $\\hat Z_{\\mathrm{HM}}$ uses draws $\\{\\theta_i\\}_{i=1}^n$ from $p(\\theta \\mid y)$ and the definition of the harmonic mean to form\n$$\n\\hat Z_{\\mathrm{HM}} \\;=\\; \\left( \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{L(\\theta_i)} \\right)^{-1}.\n$$\nSuppose you only have access to the log-likelihoods $\\ell_i = \\log L(\\theta_i)$, and you must compute $\\hat Z_{\\mathrm{HM}}$ in double precision floating-point arithmetic without incurring numerical overflow or underflow beyond what is unavoidable from the estimator’s intrinsic instability. Your goal is to propose a numerically stable computation based on the log-likelihoods, making use of a constant-shift accumulation strategy commonly known as the “log-sum-exp trick,” and to analyze the resulting overflow/underflow risks.\n\nSelect the option that correctly specifies a stable algorithm in terms of $\\{\\ell_i\\}_{i=1}^n$ and correctly characterizes the key overflow/underflow risks in double precision arithmetic (where $\\exp(x)$ overflows for $x \\gtrsim 709$ and underflows to zero for $x \\lesssim -745$).\n\nA. Let $x_i = -\\ell_i$ and $m = \\max_{1 \\le i \\le n} x_i$. Compute $s = \\sum_{i=1}^n \\exp(x_i - m)$ in floating point. Then compute $\\log \\hat Z_{\\mathrm{HM}} = \\log n - \\big(m + \\log s\\big)$ and, if needed, set $\\hat Z_{\\mathrm{HM}} = \\exp(\\log \\hat Z_{\\mathrm{HM}})$ only when $|\\log \\hat Z_{\\mathrm{HM}}| \\lesssim 709$; otherwise report $\\log \\hat Z_{\\mathrm{HM}}$ to avoid overflow/underflow. This prevents overflow in the accumulation of $\\sum_{i=1}^n \\exp(-\\ell_i)$ because all exponents $x_i - m \\le 0$, and any underflowed exponentials correspond to negligible contributions. The estimator remains statistically unstable because the largest $x_i$ (smallest likelihood) can dominate, but the computation itself is numerically well-conditioned.\n\nB. Let $x_i = -\\ell_i$ and $m = \\min_{1 \\le i \\le n} x_i$. Compute $s = \\sum_{i=1}^n \\exp(x_i - m)$, then set $\\hat Z_{\\mathrm{HM}} = \\frac{n}{\\exp(m)\\, s}$. Because the shift uses the minimum, the largest terms are downweighted, so overflow is avoided; underflow is not a concern because at least one term is $1$.\n\nC. Approximate $\\hat Z_{\\mathrm{HM}}$ by $\\hat Z_{\\mathrm{HM}} \\approx \\exp\\!\\left(\\log n - \\frac{1}{n}\\sum_{i=1}^n \\ell_i\\right)$, which is numerically stable since it uses only averages of $\\ell_i$. This is justified because the harmonic mean is close to the geometric mean for large $n$.\n\nD. Compute $t_i = \\exp(-\\ell_i)$ and evaluate $\\sum_{i=1}^n t_i$ using pairwise summation or Kahan compensated summation to avoid catastrophic cancellation, then set $\\hat Z_{\\mathrm{HM}} = n/\\sum_{i=1}^n t_i$. Since compensated summation reduces rounding error, overflow and underflow are avoided in practice and the result is stable.\n\nE. Clip the log-likelihoods by replacing each $\\ell_i$ with $\\tilde \\ell_i = \\max(\\ell_i, c)$ for some fixed constant $c$, then compute $\\hat Z_{\\mathrm{HM}}$ from $\\{\\tilde \\ell_i\\}$ as in the original definition. This prevents the smallest likelihood values from dominating, thereby improving both numerical and statistical stability without introducing meaningful bias.\n\nChoose all that apply.",
            "solution": "The problem requires finding a numerically stable method to compute the harmonic mean estimator $\\hat Z_{\\mathrm{HM}} = \\left( \\frac{1}{n} \\sum_{i=1}^n L(\\theta_i)^{-1} \\right)^{-1}$ from log-likelihoods $\\ell_i = \\log L(\\theta_i)$. The core task is to compute the sum $S = \\sum_{i=1}^n \\exp(-\\ell_i)$ without numerical overflow. This is a classic application of the log-sum-exp trick.\n\nLet $x_i = -\\ell_i$. The sum is $S = \\sum_{i=1}^n \\exp(x_i)$. A direct computation risks overflow if any $x_i$ is large (i.e., if any likelihood $L(\\theta_i)$ is very small). The standard stabilization technique is to shift the exponents by their maximum value. Let $m = \\max_{i} x_i$. The sum can be rewritten as:\n$$ S = \\sum_{i=1}^n \\exp(x_i - m + m) = \\exp(m) \\sum_{i=1}^n \\exp(x_i - m) $$\nThe terms in the new sum, $\\exp(x_i - m)$, are guaranteed not to overflow because their exponents $x_i - m$ are all less than or equal to 0. Any term where the exponent is a large negative number will safely underflow to zero, which is acceptable as its contribution to the sum is negligible compared to the term where the exponent is 0.\n\nThe full computation is best performed in the log domain to manage the full range of possible values:\n$$ \\log \\hat Z_{\\mathrm{HM}} = \\log(n) - \\log(S) = \\log(n) - \\left( m + \\log\\left(\\sum_{i=1}^n \\exp(x_i - m)\\right) \\right) $$\n\nLet's analyze the options based on this principle:\n- **A:** This option correctly implements the log-sum-exp trick using $m = \\max_{i} x_i$. It provides the correct formula for $\\log \\hat Z_{\\mathrm{HM}}$ and accurately describes how this method prevents overflow while correctly noting that it does not fix the underlying statistical instability of the estimator. This is the correct approach.\n- **B:** This option incorrectly uses $m = \\min_{i} x_i$ as the shift. This would cause the exponent for the largest $x_i$ to be $x_i - \\min_i x_i$, which is large and positive, exacerbating the risk of overflow.\n- **C:** This option proposes a different estimator (related to the geometric mean of likelihoods), not a stable computation of the harmonic mean estimator. The justification provided is also mathematically incorrect.\n- **D:** This option suggests using Kahan summation. While this method reduces rounding errors in summation, it does not prevent the initial overflow that occurs when computing the terms $\\exp(-\\ell_i)$ themselves.\n- **E:** This option modifies the original data by clipping log-likelihoods. This changes the estimator and introduces bias. The goal is to compute the original estimator stably, not a modified one.\n\nTherefore, option A is the only one that correctly describes a numerically stable algorithm for computing the specified harmonic mean estimator.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The theoretical and numerical instabilities of the harmonic mean estimator have severe practical consequences, often leading to erroneous scientific conclusions in Bayesian model selection. An unstable estimator can yield highly variable evidence estimates, causing it to incorrectly favor one model over another. This hands-on coding practice  allows you to witness this failure firsthand by simulating a model comparison scenario, where you will implement the HME and contrast its performance against the ground truth and a more robust alternative, the Widely Applicable Bayesian Information Criterion (WBIC).",
            "id": "3311539",
            "problem": "You are tasked with a principled, simulation-based comparison of the harmonic mean estimator and the Widely Applicable Bayesian Information Criterion (WBIC) for Bayesian model evidence in a nested model setting with weak identifiability. The focus is to show how the instability of the harmonic mean estimator can distort model selection. The context is stochastic simulation and Monte Carlo methods, using a normal likelihood with conjugate normal prior. The simulation must be fully self-contained and reproducible.\n\nThe fundamental base to use is Bayes' theorem and the definition of the marginal likelihood (model evidence): for data $y = (y_1,\\dots,y_n)$, likelihood $p(y \\mid \\theta)$, and prior $p(\\theta)$, the marginal likelihood is $Z = \\int p(y \\mid \\theta)\\, p(\\theta)\\, d\\theta$. The posterior is $p(\\theta \\mid y) \\propto p(y \\mid \\theta)\\, p(\\theta)$. The harmonic mean estimator relies on posterior samples and forms an estimate of $Z$ via averaging functions of $p(y \\mid \\theta)$ under $p(\\theta \\mid y)$. The Widely Applicable Bayesian Information Criterion (WBIC) approximates the negative log marginal likelihood via an expectation of the negative log-likelihood under a tempered posterior $p_t(\\theta \\mid y) \\propto p(y \\mid \\theta)^t p(\\theta)$ at inverse temperature $t = 1/\\log n$.\n\nSet up the following nested models for scalar parameter $\\mu$ with known variance $\\sigma^2$ and data $y_i \\sim \\mathcal{N}(\\mu_{\\text{true}}, \\sigma^2)$, independent for $i = 1,\\dots,n$:\n\n- Model $\\mathcal{M}_0$: the mean is fixed at $\\mu = 0$ (no free parameter).\n- Model $\\mathcal{M}_1$: the mean is unknown with prior $\\mu \\sim \\mathcal{N}(0, \\tau^2)$.\n\nUnder $\\mathcal{M}_1$, the posterior is conjugate normal. Under WBIC, the tempered posterior at inverse temperature $t$ is also normal. The harmonic mean estimator for $Z$ under $\\mathcal{M}_1$ uses samples from the posterior of $\\mu$. Because the harmonic mean estimator can have infinite variance in common settings, it is known to be unstable and potentially misleading for model selection. Your program must demonstrate this effect in carefully chosen test cases.\n\nImplement the following steps for each test case:\n\n1. Generate the data $y_1, \\dots, y_n$ from $\\mathcal{N}(\\mu_{\\text{true}}, \\sigma^2)$ using the fixed pseudorandom seed $314159$ to ensure reproducibility.\n2. Compute the exact log marginal likelihood $\\log Z_0$ for $\\mathcal{M}_0$ using the definition of $Z$ when $\\mu = 0$.\n3. Compute the exact log marginal likelihood $\\log Z_1$ for $\\mathcal{M}_1$ by analytically integrating out $\\mu$ using the normal-normal conjugacy. Do not assume any shortcut formula; derive from first principles in your solution and implement the derived expression in your program.\n4. Estimate $\\log Z_1$ via the harmonic mean estimator using $M$ independent samples of $\\mu$ from the exact posterior $p(\\mu \\mid y)$ under $\\mathcal{M}_1$, where $M = 30000$. Use a numerically stable implementation for the harmonic mean of $1/p(y \\mid \\mu)$ to avoid overflow or underflow when evaluating likelihoods for extreme $\\mu$.\n5. Approximate $-\\log Z_1$ via WBIC using $M$ samples from the tempered posterior $p_t(\\mu \\mid y)$ at inverse temperature $t = 1/\\log n$; compute the Monte Carlo expectation of $-\\log p(y \\mid \\mu)$ under $p_t(\\mu \\mid y)$. For $\\mathcal{M}_0$, note that there is no parameter and therefore $-\\log Z_0$ is exactly $-\\log p(y \\mid \\mu=0)$.\n6. Decide the selected model by three criteria:\n   - Harmonic mean selection: choose $\\mathcal{M}_0$ if $\\log Z_0  \\widehat{\\log Z_1}^{\\text{HM}}$, otherwise choose $\\mathcal{M}_1$.\n   - WBIC selection: choose $\\mathcal{M}_0$ if $-\\log Z_0  \\widehat{-\\log Z_1}^{\\text{WBIC}}$, otherwise choose $\\mathcal{M}_1$.\n   - Ground-truth selection: choose $\\mathcal{M}_0$ if $\\log Z_0  \\log Z_1$, otherwise choose $\\mathcal{M}_1$.\n7. Additionally, for each test case compute the instability indicator for the harmonic mean estimator under normal-normal conjugacy: the variance of the harmonic mean estimator is infinite if and only if $\\tau^2  \\sigma^2 / n$. Output this indicator as $1$ if $\\tau^2  \\sigma^2 / n$ and $0$ otherwise.\n\nTest Suite:\n\n- Case A (unstable, weak identifiability): $n = 20$, $\\sigma^2 = 1$, $\\tau^2 = 100$, $\\mu_{\\text{true}} = 0$.\n- Case B (more unstable, weaker identifiability): $n = 5$, $\\sigma^2 = 1$, $\\tau^2 = 1$, $\\mu_{\\text{true}} = 0$.\n- Case C (stable regime, stronger identifiability): $n = 100$, $\\sigma^2 = 1$, $\\tau^2 = 0.005$, $\\mu_{\\text{true}} = 0.3$.\n\nYour program should produce, for each case, the list $[\\text{HM\\_sel}, \\text{WBIC\\_sel}, \\text{Truth\\_sel}, \\text{HM\\_unstable}]$, where each element is an integer in $\\{0,1\\}$ and model selections are encoded as $0$ for $\\mathcal{M}_0$ and $1$ for $\\mathcal{M}_1$. The final output should aggregate the results for all three test cases into a single line as a comma-separated list enclosed in square brackets, for example, $[[0,1,0,1],[0,0,0,1],[1,1,1,0]]$.\n\nFinal Output Format:\n\n- Your program must print exactly one line containing a Python list of three lists, corresponding to Case A, Case B, and Case C, respectively. Each inner list must be of the form $[\\text{HM\\_sel}, \\text{WBIC\\_sel}, \\text{Truth\\_sel}, \\text{HM\\_unstable}]$.",
            "solution": "This problem requires implementing a simulation to compare the harmonic mean estimator (HME) and the Widely Applicable Bayesian Information Criterion (WBIC) for Bayesian model evidence, demonstrating the HME's instability. The simulation uses two nested models for data $y = (y_1, \\dots, y_n)$ drawn from a normal distribution $y_i \\sim \\mathcal{N}(\\mu_{\\text{true}}, \\sigma^2)$:\n-   **Model $\\mathcal{M}_0$**: The mean is fixed at $\\mu=0$.\n-   **Model $\\mathcal{M}_1$**: The mean is a random variable with a conjugate normal prior, $\\mu \\sim \\mathcal{N}(0, \\tau^2)$.\n\nTo establish a ground truth for model selection, we first derive the analytical marginal likelihoods for both models. The likelihood of the data $y$ given $\\mu$ and known variance $\\sigma^2$ is:\n$$ p(y \\mid \\mu, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - \\mu)^2\\right) $$\nUsing the sample mean $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$, the sum of squares is $\\sum_{i=1}^n (y_i - \\mu)^2 = \\sum_{i=1}^n (y_i - \\bar{y})^2 + n(\\bar{y} - \\mu)^2$.\n\nFor **Model $\\mathcal{M}_0$**, with $\\mu=0$, the marginal likelihood $Z_0$ is:\n$$ \\log Z_0 = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n y_i^2 $$\n\nFor **Model $\\mathcal{M}_1$**, we integrate the product of the likelihood and the prior $p(\\mu) = \\mathcal{N}(\\mu \\mid 0, \\tau^2)$ over $\\mu$. This is a standard normal-normal conjugacy calculation. Completing the square for $\\mu$ in the exponent and integrating yields the exact log marginal likelihood:\n$$ \\log Z_1 = -\\frac{n}{2}\\log(2\\pi) -\\frac{n-1}{2}\\log(\\sigma^2) - \\frac{1}{2}\\log(n\\tau^2+\\sigma^2) - \\frac{\\sum(y_i-\\bar{y})^2}{2\\sigma^2} - \\frac{n\\bar{y}^2}{2(n\\tau^2+\\sigma^2)} $$\nThese analytical forms for $\\log Z_0$ and $\\log Z_1$ constitute our ground truth.\n\nThe **Harmonic Mean Estimator (HME)** for $Z_1$ is calculated from $M$ samples $\\mu^{(j)}$ drawn from the posterior distribution $p(\\mu \\mid y) = \\mathcal{N}(\\mu \\mid \\mu_p, \\sigma_p^2)$, where $\\sigma_p^2 = \\left(\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1}$ and $\\mu_p = \\sigma_p^2 \\left(\\frac{n\\bar{y}}{\\sigma^2}\\right)$. The estimator is:\n$$ \\widehat{Z_1}^{\\text{HM}} = \\left( \\frac{1}{M}\\sum_{j=1}^M \\frac{1}{p(y \\mid \\mu^{(j)})} \\right)^{-1} $$\nThis is implemented using the log-sum-exp trick for numerical stability. For the normal-normal model, the HME variance is infinite if $\\tau^2 > \\sigma^2/n$, which indicates instability.\n\nThe **Widely Applicable Bayesian Information Criterion (WBIC)** approximates $-\\log Z_1$ as:\n$$ \\widehat{-\\log Z_1}^{\\text{WBIC}} = E_{p_t(\\mu \\mid y)}[-\\log p(y \\mid \\mu)] $$\nThe expectation is over a tempered posterior $p_t(\\mu \\mid y)$ with inverse temperature $t=1/\\log n$. This tempered posterior is also normal, $p_t(\\mu \\mid y) = \\mathcal{N}(\\mu \\mid \\mu_{p,t}, \\sigma_{p,t}^2)$, with parameters $\\sigma_{p,t}^2 = \\left(\\frac{tn}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1}$ and $\\mu_{p,t} = \\sigma_{p,t}^2 \\left(\\frac{tn\\bar{y}}{\\sigma^2}\\right)$. The expectation is estimated via Monte Carlo averaging.\n\n**Model selection** is performed by comparing the evidence for $\\mathcal{M}_0$ and $\\mathcal{M}_1$. The model with higher marginal likelihood is preferred. The simulation will compare the selections made by HME and WBIC against the ground truth derived from the exact analytical likelihoods.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a simulation to compare the Harmonic Mean Estimator (HME) and\n    the Widely Applicable Bayesian Information Criterion (WBIC) for Bayesian\n    model selection.\n    \"\"\"\n\n    # Define the test cases as per the problem statement.\n    test_cases = [\n        # (n, sigma^2, tau^2, mu_true)\n        (20, 1.0, 100.0, 0.0),  # Case A: Unstable HME, weak identifiability\n        (5, 1.0, 1.0, 0.0),      # Case B: Unstable HME, weaker identifiability\n        (100, 1.0, 0.005, 0.3),  # Case C: Stable HME, strong identifiability\n    ]\n\n    M = 30000  # Number of Monte Carlo samples\n    seed = 314159\n    rng = np.random.default_rng(seed)\n\n    results = []\n    \n    for n, sigma2, tau2, mu_true in test_cases:\n        # Step 1: Generate data\n        sigma = np.sqrt(sigma2)\n        y = rng.normal(loc=mu_true, scale=sigma, size=n)\n        \n        # Pre-compute summary statistics\n        y_bar = np.mean(y)\n        sum_y_sq = np.sum(y**2)\n        sum_sq_dev = np.sum((y - y_bar)**2)\n        \n        # Step 2: Compute exact log marginal likelihood for M0\n        log_Z0 = -0.5 * n * np.log(2 * np.pi * sigma2) - sum_y_sq / (2 * sigma2)\n        neg_log_Z0 = -log_Z0\n\n        # Step 3: Compute exact log marginal likelihood for M1\n        log_Z1 = (-0.5 * n * np.log(2 * np.pi) \n                  - 0.5 * (n - 1) * np.log(sigma2) \n                  - 0.5 * np.log(n * tau2 + sigma2) \n                  - sum_sq_dev / (2 * sigma2) \n                  - n * y_bar**2 / (2 * (n * tau2 + sigma2)))\n\n        # Step 4: Estimate log Z1 via Harmonic Mean Estimator (HME)\n        post_var = 1.0 / (n / sigma2 + 1.0 / tau2)\n        post_mean = post_var * (n * y_bar / sigma2)\n        mu_samples_post = rng.normal(loc=post_mean, scale=np.sqrt(post_var), size=M)\n        \n        # Calculate -log p(y | mu) for HME samples, needed for stable calculation\n        # -log p(y|mu) = 0.5*n*log(2*pi*sigma2) + sum(y_i - mu)^2/(2*sigma2)\n        # sum(y_i - mu)^2 = sum_sq_dev + n*(y_bar-mu)^2\n        neg_log_likelihoods_for_hme = (0.5 * n * np.log(2 * np.pi * sigma2)\n                                       + (sum_sq_dev + n * (y_bar - mu_samples_post)**2) / (2 * sigma2))\n        \n        # Numerically stable calculation of log HME\n        c_hme = np.max(neg_log_likelihoods_for_hme)\n        log_mean_inv_likelihood = c_hme + np.log(np.sum(np.exp(neg_log_likelihoods_for_hme - c_hme))) - np.log(M)\n        log_Z1_hme = -log_mean_inv_likelihood\n\n        # Step 5: Approximate -log Z1 via WBIC\n        t = 1.0 / np.log(n)\n        temp_post_var = 1.0 / (t * n / sigma2 + 1.0 / tau2)\n        temp_post_mean = temp_post_var * (t * n * y_bar / sigma2)\n        mu_samples_temp = rng.normal(loc=temp_post_mean, scale=np.sqrt(temp_post_var), size=M)\n\n        # Calculate -log p(y | mu) for WBIC samples\n        neg_log_likelihoods_for_wbic = (0.5 * n * np.log(2 * np.pi * sigma2)\n                                        + (sum_sq_dev + n * (y_bar - mu_samples_temp)**2) / (2 * sigma2))\n\n        neg_log_Z1_wbic = np.mean(neg_log_likelihoods_for_wbic)\n\n        # Step 6: Decide model selections\n        hm_sel = 1 if log_Z1_hme > log_Z0 else 0\n        wbic_sel = 1 if neg_log_Z1_wbic  neg_log_Z0 else 0\n        truth_sel = 1 if log_Z1 > log_Z0 else 0\n\n        # Step 7: Compute HME instability indicator\n        hm_unstable = 1 if tau2 > sigma2 / n else 0\n\n        results.append([hm_sel, wbic_sel, truth_sel, hm_unstable])\n    \n    # Format the final output string exactly as required\n    inner_strs = [f\"[{','.join(map(str, sublist))}]\" for sublist in results]\n    final_output = f\"[{','.join(inner_strs)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}