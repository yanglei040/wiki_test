{
    "hands_on_practices": [
        {
            "introduction": "在深入研究复杂的模拟技术之前，在一个可以精确计算边际似然的简单场景中验证方法的核心思想是很有启发性的。本练习使用一个共轭高斯模型，通过解析方式推导出 $p(y)$，并展示它如何与Chib恒等式的各个组成部分相关联，从而为理解该方法奠定坚实的理论基础。",
            "id": "3294504",
            "problem": "考虑一个方差已知的单观测高斯位置模型：似然为 $y \\mid \\theta \\sim \\mathcal{N}(\\theta,\\sigma^{2})$，先验为 $\\theta \\sim \\mathcal{N}(\\mu_{0},\\tau_{0}^{2})$，其中 $y \\in \\mathbb{R}$ 是观测值，而 $\\mu_{0} \\in \\mathbb{R}$、$\\tau_{0}^{2} \\in (0,\\infty)$ 和 $\\sigma^{2} \\in (0,\\infty)$ 是已知常数。\n\n仅从第一性原理出发，通过对参数 $\\theta$ 积分，以闭合形式推导边缘数据密度 $p(y)$。你的推导必须从边缘数据密度作为先验预测分布的基本定义出发，并得出一个简化的解析密度。然后，从原理层面简要解释，该共轭计算如何阐明 Chib 方法在非共轭模型中用于边缘似然估计所利用的关键恒等式，特别是参数空间中固定评估点和后验纵坐标的作用。\n\n将你的最终答案表示为关于 $y$、$\\mu_{0}$、$\\tau_{0}^{2}$ 和 $\\sigma^{2}$ 的单个闭合形式解析表达式。无需进行数值计算，也无需四舍五入。",
            "solution": "该问题要求推导具有共轭高斯先验的高斯位置模型的边缘数据密度 $p(y)$，并解释该推导如何阐明 Chib 的边缘似然估计算法。\n\n**第一部分：边缘数据密度 $p(y)$ 的推导**\n\n给定以下信息：\n- 似然：$p(y \\mid \\theta) = \\mathcal{N}(\\theta, \\sigma^2)$，这是给定参数 $\\theta$ 时数据 $y$ 的正态分布，方差 $\\sigma^2$ 已知。\n- 先验：$p(\\theta) = \\mathcal{N}(\\mu_0, \\tau_0^2)$，这是参数 $\\theta$ 的正态分布，均值 $\\mu_0$ 和方差 $\\tau_0^2$ 已知。\n\n概率密度函数 (PDF) 如下：\n$$p(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2}\\right)$$\n$$p(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right)$$\n\n边缘数据密度 $p(y)$，也称为先验预测分布或证据，是通过对联合分布 $p(y, \\theta) = p(y \\mid \\theta) p(\\theta)$ 在参数 $\\theta$ 的所有可能值上积分得到的：\n$$p(y) = \\int_{-\\infty}^{\\infty} p(y \\mid \\theta) p(\\theta) \\,d\\theta$$\n\n将 PDF 代入积分：\n$$p(y) = \\int_{-\\infty}^{\\infty} \\left[\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2}\\right)\\right] \\left[\\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right)\\right] \\,d\\theta$$\n\n我们可以合并常数项和指数函数：\n$$p(y) = \\frac{1}{2\\pi\\sigma\\tau_0} \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{1}{2}\\left[ \\frac{(y - \\theta)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2} \\right] \\right) \\,d\\theta$$\n\n为了求解积分，我们关注指数的参数，它是一个关于 $\\theta$ 的二次函数。我们将对 $\\theta$ 进行配方。令方括号中的项为 $Q(\\theta)$：\n$$Q(\\theta) = \\frac{y^2 - 2y\\theta + \\theta^2}{\\sigma^2} + \\frac{\\theta^2 - 2\\mu_0\\theta + \\mu_0^2}{\\tau_0^2}$$\n按 $\\theta$ 的幂次分组项：\n$$Q(\\theta) = \\theta^2 \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right) - 2\\theta \\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right) + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2}\\right)$$\n\n让我们定义两个新量，它们对应于 $\\theta$ 的后验方差 $\\tau_1^2$ 和后验均值 $\\mu_1$：\n后验精度是数据精度和先验精度的和：\n$$\\frac{1}{\\tau_1^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2} = \\frac{\\sigma^2 + \\tau_0^2}{\\sigma^2\\tau_0^2} \\implies \\tau_1^2 = \\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + \\tau_0^2}$$\n后验均值是数据和先验均值的精度加权平均：\n$$\\mu_1 = \\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right) \\bigg/ \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right) = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2}$$\n\n使用这些定义，我们可以将 $Q(\\theta)$ 重写为：\n$$Q(\\theta) = \\frac{1}{\\tau_1^2}\\theta^2 - \\frac{2\\mu_1}{\\tau_1^2}\\theta + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2}\\right)$$\n对包含 $\\theta$ 的项进行配方：\n$$Q(\\theta) = \\frac{1}{\\tau_1^2}(\\theta^2 - 2\\mu_1\\theta) + \\dots = \\frac{1}{\\tau_1^2}(\\theta - \\mu_1)^2 - \\frac{\\mu_1^2}{\\tau_1^2} + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2}\\right)$$\n$$Q(\\theta) = \\frac{(\\theta - \\mu_1)^2}{\\tau_1^2} + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\frac{\\mu_1^2}{\\tau_1^2}\\right)$$\n$p(y)$ 的积分变为：\n$$p(y) = \\frac{1}{2\\pi\\sigma\\tau_0} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{1}{2}\\left[\\frac{(\\theta - \\mu_1)^2}{\\tau_1^2} + C\\right]\\right) \\,d\\theta$$\n其中 $C = \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\frac{\\mu_1^2}{\\tau_1^2}$ 是一个关于 $\\theta$ 的常数。\n$$p(y) = \\frac{1}{2\\pi\\sigma\\tau_0} \\exp\\left(-\\frac{C}{2}\\right) \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(\\theta - \\mu_1)^2}{2\\tau_1^2}\\right) \\,d\\theta$$\n该积分是高斯 PDF $\\mathcal{N}(\\mu_1, \\tau_1^2)$ 核的积分。其值为 $\\sqrt{2\\pi\\tau_1^2}$。\n$$p(y) = \\frac{\\sqrt{2\\pi\\tau_1^2}}{2\\pi\\sigma\\tau_0} \\exp\\left(-\\frac{C}{2}\\right) = \\frac{\\tau_1}{\\sqrt{2\\pi}\\sigma\\tau_0} \\exp\\left(-\\frac{C}{2}\\right)$$\n\n现在，我们简化常数因子和指数项 $C$。\n常数因子是：\n$$\\frac{\\tau_1}{\\sqrt{2\\pi}\\sigma\\tau_0} = \\frac{1}{\\sqrt{2\\pi}\\sigma\\tau_0} \\sqrt{\\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + \\tau_0^2}} = \\frac{1}{\\sqrt{2\\pi}\\sigma\\tau_0} \\frac{\\sigma\\tau_0}{\\sqrt{\\sigma^2 + \\tau_0^2}} = \\frac{1}{\\sqrt{2\\pi(\\sigma^2 + \\tau_0^2)}}$$\n指数项 $C$ 简化为：\n$$C = \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\frac{1}{\\tau_1^2} \\mu_1^2 = \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\left(\\frac{\\sigma^2+\\tau_0^2}{\\sigma^2\\tau_0^2}\\right) \\left(\\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2}\\right)^2$$\n$$C = \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\frac{(y\\tau_0^2 + \\mu_0\\sigma^2)^2}{\\sigma^2\\tau_0^2(\\sigma^2 + \\tau_0^2)}$$\n通分，使用公分母 $\\sigma^2\\tau_0^2(\\sigma^2 + \\tau_0^2)$：\n$$C = \\frac{y^2\\tau_0^2(\\sigma^2 + \\tau_0^2) + \\mu_0^2\\sigma^2(\\sigma^2 + \\tau_0^2) - (y^2\\tau_0^4 + 2y\\mu_0\\sigma^2\\tau_0^2 + \\mu_0^2\\sigma^4)}{\\sigma^2\\tau_0^2(\\sigma^2 + \\tau_0^2)}$$\n展开并简化分子：\n$$(y^2\\sigma^2\\tau_0^2 + y^2\\tau_0^4) + (\\mu_0^2\\sigma^4 + \\mu_0^2\\sigma^2\\tau_0^2) - y^2\\tau_0^4 - 2y\\mu_0\\sigma^2\\tau_0^2 - \\mu_0^2\\sigma^4$$\n$$= y^2\\sigma^2\\tau_0^2 - 2y\\mu_0\\sigma^2\\tau_0^2 + \\mu_0^2\\sigma^2\\tau_0^2 = \\sigma^2\\tau_0^2(y^2 - 2y\\mu_0 + \\mu_0^2) = \\sigma^2\\tau_0^2(y - \\mu_0)^2$$\n因此，\n$$C = \\frac{\\sigma^2\\tau_0^2(y - \\mu_0)^2}{\\sigma^2\\tau_0^2(\\sigma^2 + \\tau_0^2)} = \\frac{(y - \\mu_0)^2}{\\sigma^2 + \\tau_0^2}$$\n将简化的常数因子和指数项代回 $p(y)$ 的表达式：\n$$p(y) = \\frac{1}{\\sqrt{2\\pi(\\sigma^2 + \\tau_0^2)}} \\exp\\left(-\\frac{(y - \\mu_0)^2}{2(\\sigma^2 + \\tau_0^2)}\\right)$$\n这是正态分布 $\\mathcal{N}(\\mu_0, \\sigma^2 + \\tau_0^2)$ 的 PDF。\n\n**第二部分：与 Chib 方法的联系**\n\n上述推导阐明了 Chib 方法进行边缘似然估计背后的原理。贝叶斯推断的基本恒等式关联了后验、先验、似然和边缘似然（证据）：\n$$p(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}$$\n这个方程可以重排以表示边缘似然：\n$$p(y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(\\theta \\mid y)}$$\n至关重要的是，这个恒等式必须对其支撑集中的*任何*参数值 $\\theta$ 都成立。Chib 的方法利用了这一事实，通过在单个固定点 $\\theta^*$ 处评估该恒等式：\n$$p(y) = \\frac{p(y \\mid \\theta^*) p(\\theta^*)}{p(\\theta^* \\mid y)}$$\n在对数空间中，这表示为 $\\ln p(y) = \\ln p(y \\mid \\theta^*) + \\ln p(\\theta^*) - \\ln p(\\theta^* \\mid y)$。\n\n我们的共轭计算从几个方面阐明了这一原理：\n1.  **解析验证**：对于共轭高斯模型，我们有右侧所有三项的闭合形式表达式。\n    - $p(y \\mid \\theta^*)$ 是在 $\\theta^*$ 处评估的给定似然 PDF。\n    - $p(\\theta^*)$ 是在 $\\theta^*$ 处评估的给定先验 PDF。\n    - $p(\\theta^* \\mid y)$ 是在 $\\theta^*$ 处评估的后验 PDF。我们的推导揭示了后验是 $p(\\theta \\mid y) = \\mathcal{N}(\\mu_1, \\tau_1^2)$，这是完全确定的。\n    通过积分对 $p(y)$ 进行的解析计算等同于计算右侧的比率。其结果与 $\\theta^*$ 的选择无关这一事实证明了该恒等式的有效性。分子中依赖于 $\\theta^*$ 的项（来自联合密度）和分母中依赖于 $\\theta^*$ 的项（来自后验纵坐标）必须完全抵消。\n\n2.  **固定点 $\\theta^*$ 的作用**：固定点 $\\theta^*$ 是为计算方便而选择的任意点。在一个 $p(y)$ 积分难解的非共轭模型中，通常有 MCMC 样本来近似后验分布 $p(\\theta \\mid y)$，但没有其归一化常数 $p(y)$。Chib 的方法使用这些样本来构建分母 $p(\\theta^* \\mid y)$（后验纵坐标）的估计。分子项 $p(y \\mid \\theta^*)$ 和 $p(\\theta^*)$ 通常可以从其函数形式直接简单地计算出来。\n\n3.  **后验纵坐标的作用**：项 $p(\\theta^* \\mid y)$ 是需要估计的关键量。在我们的共轭例子中，其值是解析已知的。在非共轭 MCMC 设置中，从一组样本中估计此密度纵坐标是 Chib 方法及其变体所解决的主要挑战，通常使用核密度估计或 Rao-Blackwellization 等技术。\n\n总之，共轭情况提供了一个原理证明。它表明边缘似然 $p(y)$ 正是关联联合密度 $p(y, \\theta)$ 与后验密度 $p(\\theta \\mid y)$ 的归一化常数。Chib 的方法是一种计算该常数的数值策略，它通过重排后验的定义并通过模拟来估计唯一的未知项，即后验纵坐标。",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi(\\sigma^{2} + \\tau_{0}^{2})}} \\exp\\left(-\\frac{(y - \\mu_{0})^{2}}{2(\\sigma^{2} + \\tau_{0}^{2})}\\right)}$$"
        },
        {
            "introduction": "从理论转向实践，本练习将指导您如何在一个多参数的贝叶斯线性回归模型中实现Chib方法，该模型是应用统计学中的主力。您将使用Gibbs采样器，并学习如何通过分解后验纵坐标并对MCMC输出进行平均来估计它，这是应用该方法的关键技巧。",
            "id": "3294515",
            "problem": "给定一个贝叶斯线性回归模型，该模型具有条件共轭先验，并允许使用一个双区块吉布斯采样器，其全条件密度具有闭式解。您的任务是使用 Chib 方法推导、实现并计算观测数据的对数边际似然，具体方法是通过平均化全条件概率的乘积来评估选定点 $\\theta^{\\star}$ 处的后验纵坐标。您的最终程序必须是完全自包含的，能够为指定的测试套件生成所要求的输出，并遵循最终的输出格式。\n\n模型与先验：\n- 似然：对于观测数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $y \\in \\mathbb{R}^{n}$，假设\n$$\ny \\mid \\beta, \\sigma^2 \\sim \\mathcal{N}\\!\\left(X \\beta, \\sigma^2 I_n\\right).\n$$\n- 先验：设先验为正态-逆伽马形式的条件共轭先验，\n$$\n\\beta \\mid \\sigma^2 \\sim \\mathcal{N}\\!\\left(m_0, \\sigma^2 S_0\\right), \\quad \\sigma^2 \\sim \\text{Inverse-Gamma}\\!\\left(a_0, b_0\\right),\n$$\n其中 $S_0 \\in \\mathbb{R}^{p \\times p}$ 为正定矩阵，超参数 $a_0, b_0  0$ 为正数。使用由下式给出的逆伽马密度：\n$$\nf_{\\text{IG}}(x \\mid a, b) = \\frac{b^a}{\\Gamma(a)} x^{-(a+1)} \\exp\\!\\left(-\\frac{b}{x}\\right), \\quad x  0.\n$$\n\n理论基础：\n- 使用贝叶斯定理和联合密度的因子分解原理。\n- 使用从共轭性中得到的全条件密度来构建吉布斯采样器。\n- 使用 Chib 恒等式计算边际似然，并将后验纵坐标分解为条件后验纵坐标的乘积。\n- 使用大数定律，通过对吉布斯样本求平均来近似期望值。\n- 使用标准多元正态和逆伽马密度公式。\n\n必须推导和实现的内容：\n- 推导 $\\beta \\mid \\sigma^2, y$ 和 $\\sigma^2 \\mid \\beta, y$ 的全条件密度。\n- 基于这些全条件密度构建一个双区块吉布斯采样器。\n- 指定一个一致的程序，从吉布斯输出中选择一个高后验密度点 $\\theta^{\\star} = (\\beta^{\\star}, \\sigma^{2\\star})$，例如后验均值。\n- 从贝叶斯定理和后验分布的因子分解出发，推导所需的分解，以便将后验纵坐标 $p(\\theta^{\\star} \\mid y)$ 评估为在 $\\theta^{\\star}$ 处的平均化全条件概率的乘积。\n- 将对数边际似然表示为一些项的和/差，这些项可以根据模型、先验和估计的后验纵坐标进行计算。避免使用任何未从上述核心原理推导出的简化公式。\n\n需要实现的计算方案：\n- 为该模型实现一个吉布斯采样器。\n- 运行吉布斯采样器，舍弃老化期（burn-in）样本，并将 $\\theta^{\\star}$ 计算为 $(\\beta, \\sigma^2)$ 的后验均值。\n- 使用高斯似然计算对数似然 $\\log p(y \\mid \\theta^{\\star})$。\n- 使用正态-逆伽马先验计算对数先验 $\\log p(\\theta^{\\star})$。\n- 使用在 $\\theta^{\\star}$ 处的平均化全条件概率的乘积来计算后验纵坐标 $\\log p(\\theta^{\\star} \\mid y)$。其中，第一个因子 $p(\\beta^{\\star} \\mid y)$ 通过对来自吉布斯采样器的后验抽取样本 $\\{\\sigma^{2(m)}\\}$ 上的全条件密度 $p(\\beta^{\\star} \\mid \\sigma^{2(m)}, y)$进行平均来近似；第二个因子 $p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y)$ 则通过在 $(\\beta^{\\star}, \\sigma^{2\\star})$ 处对应的全条件密度以闭式形式计算。\n- 通过 Chib 恒等式组合这些项以获得 $\\log p(y)$。\n\n测试套件和要求输出：\n实现您的程序，为以下三个测试用例计算对数边际似然估计值。对于所有测试，您必须使用相同的超参数：\n- 先验超参数：$m_0 = \\mathbf{0}_p$，$S_0 = c_0 I_p$，其中 $c_0 = 100$，$a_0 = 2$，$b_0 = 1$。\n- 每个测试的吉布斯采样设置：总迭代次数 $N_{\\text{iter}} = 9000$，老化期 $N_{\\text{burn}} = 4000$。为数据生成和吉布斯采样器使用指定的随机种子以确保可复现性。\n\n测试用例 1 (仅截距模型)：\n- 数据：$n = 6$, $p = 1$；$X = \\mathbf{1}_n$ 且 $y = [0.8, 1.2, 1.1, 0.7, 1.3, 0.9]^{\\top}$。\n- 吉布斯采样器种子：$202$。\n\n测试用例 2 (中等维度且预测变量相关)：\n- 数据生成种子：$123$。\n- 数据：$n = 30$, $p = 3$；对于 $i = 1,\\dots,n$，构造 $x_1$ 为 $x_{1,i} = -2 + 4 (i-1)/(n-1)$，抽取 $\\epsilon^{(x)}_i \\sim \\mathcal{N}(0, 0.1^2)$ 并设置 $x_{2,i} = 0.8 x_{1,i} + \\epsilon^{(x)}_i$。令 $X = [\\mathbf{1}_n, x_1, x_2]$。抽取 $\\epsilon^{(y)}_i \\sim \\mathcal{N}(0, 0.5^2)$ 并设置 $y_i = 1 + 2 x_{1,i} - 1 x_{2,i} + \\epsilon^{(y)}_i$。\n- 吉布斯采样器种子：$203$。\n\n测试用例 3 (近共线性)：\n- 数据生成种子：$456$。\n- 数据：$n = 20$, $p = 3$；对于 $i = 1,\\dots,n$，构造 $x_1$ 为 $x_{1,i} = -1 + 2 (i-1)/(n-1)$，抽取 $\\delta_i \\sim \\mathcal{N}(0, 1)$，设置 $x_{2,i} = x_{1,i} + 10^{-4} \\delta_i$。令 $X = [\\mathbf{1}_n, x_1, x_2]$。抽取 $\\epsilon^{(y)}_i \\sim \\mathcal{N}(0, 0.1^2)$ 并设置 $y_i = 0.5 + 1.0 x_{1,i} + 1.0 x_{2,i} + \\epsilon^{(y)}_i$。\n- 吉布斯采样器种子：$204$。\n\n角度和物理单位：不适用。此问题中未出现物理单位和角度。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个测试用例的估计对数边际似然值，四舍五入到六位小数，格式为方括号内以逗号分隔的列表，例如，“[x1,x2,x3]”。\n\n您的实现必须是一个单一、完整、可运行的程序，该程序生成数据（针对测试2和3），运行吉布斯采样器，使用在 $\\theta^{\\star}$ 处的平均化全条件概率的乘积计算 Chib 估计量，并以确切的所需格式打印结果。不允许用户输入。",
            "solution": "任务是使用 Chib 方法计算贝叶斯线性回归模型的对数边际似然 $\\log p(y)$。这需要为吉布斯采样器推导全条件后验分布，然后使用吉布斯采样器的输出来估计在特定高密度点 $\\theta^{\\star} = (\\beta^{\\star}, \\sigma^{2\\star})$ 处的后验纵坐标。\n\n模型定义如下：\n- 似然：$y \\mid \\beta, \\sigma^2 \\sim \\mathcal{N}(X \\beta, \\sigma^2 I_n)$\n- 先验：$\\beta \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2 S_0)$ 且 $\\sigma^2 \\sim \\text{Inverse-Gamma}(a_0, b_0)$\n\n逆伽马分布的密度为 $f_{\\text{IG}}(x \\mid a, b) = \\frac{b^a}{\\Gamma(a)} x^{-(a+1)} \\exp(-b/x)$，其中 $x  0$。\n\n**1. 全条件后验密度的推导**\n\n联合后验分布正比于似然和先验的乘积：\n$$p(\\beta, \\sigma^2 \\mid y) \\propto p(y \\mid \\beta, \\sigma^2) p(\\beta \\mid \\sigma^2) p(\\sigma^2)$$\n\n**$\\beta$ 的全条件分布：**\n为了找到全条件分布 $p(\\beta \\mid \\sigma^2, y)$，我们从联合后验中分离出包含 $\\beta$ 的项：\n$$p(\\beta \\mid \\sigma^2, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}(y - X\\beta)^T(y - X\\beta)\\right) \\exp\\left(-\\frac{1}{2\\sigma^2}(\\beta - m_0)^T S_0^{-1}(\\beta - m_0)\\right)$$\n展开指数中的二次型：\n$$-\\frac{1}{2\\sigma^2} \\left[ (y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta) + (\\beta^T S_0^{-1} \\beta - 2\\beta^T S_0^{-1} m_0 + m_0^T S_0^{-1} m_0) \\right]$$\n合并包含 $\\beta$ 的项：\n$$-\\frac{1}{2\\sigma^2} \\left[ \\beta^T(X^T X + S_0^{-1})\\beta - 2\\beta^T(X^T y + S_0^{-1} m_0) \\right] + \\text{const.}$$\n这是 $\\beta$ 的一个多元正态密度的核。通过配方法，我们确定后验精度矩阵 $S_n^{-1} = (X^T X + S_0^{-1})$ 和后验均值 $m_n = S_n(X^T y + S_0^{-1} m_0)$。因此，$\\beta$ 的全条件分布是：\n$$\\beta \\mid \\sigma^2, y \\sim \\mathcal{N}(m_n, \\sigma^2 S_n)$$\n其中 $S_n = (X^T X + S_0^{-1})^{-1}$ 且 $m_n = S_n(X^T y + S_0^{-1} m_0)$。\n\n**$\\sigma^2$ 的全条件分布：**\n为了找到全条件分布 $p(\\sigma^2 \\mid \\beta, y)$，我们分离出包含 $\\sigma^2$ 的项：\n$$p(\\sigma^2 \\mid \\beta, y) \\propto p(y \\mid \\beta, \\sigma^2) p(\\beta \\mid \\sigma^2) p(\\sigma^2)$$\n$$p(\\sigma^2 \\mid \\beta, y) \\propto \\left((\\sigma^2)^{-n/2} \\exp\\left(-\\frac{(y-X\\beta)^T(y-X\\beta)}{2\\sigma^2}\\right)\\right) \\times \\left((\\sigma^2)^{-p/2} \\exp\\left(-\\frac{(\\beta-m_0)^T S_0^{-1}(\\beta-m_0)}{2\\sigma^2}\\right)\\right) \\times \\left((\\sigma^2)^{-(a_0+1)} \\exp\\left(-\\frac{b_0}{\\sigma^2}\\right)\\right)$$\n合并各项：\n$$p(\\sigma^2 \\mid \\beta, y) \\propto (\\sigma^2)^{-(a_0 + \\frac{n+p}{2} + 1)} \\exp\\left(-\\frac{1}{\\sigma^2}\\left[b_0 + \\frac{1}{2}(y-X\\beta)^T(y-X\\beta) + \\frac{1}{2}(\\beta-m_0)^T S_0^{-1}(\\beta-m_0)\\right]\\right)$$\n这是逆伽马密度的核。后验的形状和尺度参数为：\n$$a_n = a_0 + \\frac{n+p}{2}$$\n$$b_n = b_0 + \\frac{1}{2}(y-X\\beta)^T(y-X\\beta) + \\frac{1}{2}(\\beta-m_0)^T S_0^{-1}(\\beta-m_0)$$\n因此，$\\sigma^2$ 的全条件分布是：\n$$\\sigma^2 \\mid \\beta, y \\sim \\text{Inverse-Gamma}(a_n, b_n)$$\n\n**2. Chib 边际似然方法的推导**\n\n边际似然 $p(y)$ 可以使用从贝叶斯定理推导出的恒等式来表示，该恒等式对任何参数值 $\\theta = (\\beta, \\sigma^2)$ 都成立：\n$$p(y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(\\theta \\mid y)}$$\n在对数空间中，该式为：\n$$\\log p(y) = \\log p(y \\mid \\theta) + \\log p(\\theta) - \\log p(\\theta \\mid y)$$\n为了数值稳定性，我们在一个高密度点 $\\theta^{\\star} = (\\beta^{\\star}, \\sigma^{2\\star})$ 处评估此恒等式，我们选择该点为根据吉布斯采样器输出计算出的后验均值。这三项分别是：\n1.  $\\log p(y \\mid \\theta^{\\star})$：在 $\\theta^{\\star}$ 处评估的对数似然。这是 $\\mathcal{N}(X\\beta^{\\star}, \\sigma^{2\\star}I_n)$ 在数据 $y$ 处的对数概率密度函数（log-pdf）。\n2.  $\\log p(\\theta^{\\star})$：在 $\\theta^{\\star}$ 处评估的对数先验。由于先验结构为 $p(\\theta) = p(\\beta \\mid \\sigma^2) p(\\sigma^2)$，此项为 $\\log p(\\beta^{\\star} \\mid \\sigma^{2\\star}) + \\log p(\\sigma^{2\\star})$，其中密度分别是先验分布 $\\mathcal{N}(m_0, \\sigma^{2\\star}S_0)$ 和 $\\text{IG}(a_0, b_0)$ 的密度。\n3.  $\\log p(\\theta^{\\star} \\mid y)$：在 $\\theta^{\\star}$ 处评估的对数后验纵坐标。此项需要仔细估计。\n\n**估计后验纵坐标 $\\log p(\\theta^{\\star} \\mid y)$**\n\n按照要求，我们使用链式法则分解后验纵坐标：\n$$p(\\theta^{\\star} \\mid y) = p(\\beta^{\\star}, \\sigma^{2\\star} \\mid y) = p(\\beta^{\\star} \\mid y) p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y)$$\n这两个因子按如下方式处理：\n\n- **第二个因子 $p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y)$**：这是在给定 $\\beta = \\beta^{\\star}$ 的条件下，$\\sigma^2$ 的全条件密度在 $\\sigma^{2\\star}$ 处的值。我们已经推导出该密度为 $\\text{IG}(a_n, b_n)$。我们可以通过将 $\\beta^{\\star}$ 和 $\\sigma^{2\\star}$ 代入参数为 $a_n^{\\star} = a_0 + \\frac{n+p}{2}$ 和 $b_n^{\\star} = b_0 + \\frac{1}{2}(y-X\\beta^{\\star})^T(y-X\\beta^{\\star}) + \\frac{1}{2}(\\beta^{\\star}-m_0)^T S_0^{-1}(\\beta^{\\star}-m_0)$ 的逆伽马概率密度函数（IG pdf）来直接计算其值。\n\n- **第一个因子 $p(\\beta^{\\star} \\mid y)$**：这是 $\\beta$ 的边际后验密度在 $\\beta^{\\star}$ 处的值。它可以表示为对 $\\sigma^2$ 的积分：\n$$p(\\beta^{\\star} \\mid y) = \\int p(\\beta^{\\star}, \\sigma^2 \\mid y) d\\sigma^2 = \\int p(\\beta^{\\star} \\mid \\sigma^2, y) p(\\sigma^2 \\mid y) d\\sigma^2 = E_{\\sigma^2 \\mid y}[p(\\beta^{\\star} \\mid \\sigma^2, y)]$$\n我们可以通过对我们的吉布斯采样器中老化期之后的 $\\sigma^2$ 样本进行平均来估计此期望。设 $\\{\\sigma^{2(m)}\\}_{m=1}^M$ 为 $M$ 个后验样本。蒙特卡洛估计为：\n$$\\hat{p}(\\beta^{\\star} \\mid y) = \\frac{1}{M} \\sum_{m=1}^{M} p(\\beta^{\\star} \\mid \\sigma^{2(m)}, y)$$\n每一项 $p(\\beta^{\\star} \\mid \\sigma^{2(m)}, y)$ 是 $\\beta$ 的全条件分布 $\\mathcal{N}(m_n, \\sigma^{2(m)} S_n)$ 在 $\\beta^{\\star}$ 处的概率密度函数（PDF）值。\n\n综合这些，对数后验纵坐标的估计值为：\n$$\\log p(\\theta^{\\star} \\mid y) \\approx \\log \\left(\\frac{1}{M} \\sum_{m=1}^M p(\\beta^{\\star} \\mid \\sigma^{2(m)}, y)\\right) + \\log p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y)$$\n\n**3. 计算算法**\n\n完整的算法如下：\n1.  **吉布斯采样**：\n    a. 初始化 $\\beta^{(0)}$ 和 $\\sigma^{2(0)}$。\n    b. 预先计算常数量：$S_0^{-1}$、$S_n=(X^TX+S_0^{-1})^{-1}$、$m_n=S_n(X^Ty+S_0^{-1}m_0)$ 和 $a_n=a_0+(n+p)/2$。\n    c. 对于 $t=1, \\dots, N_{\\text{iter}}$：\n        i.  从 $\\mathcal{N}(m_n, \\sigma^{2(t-1)}S_n)$ 中抽取 $\\beta^{(t)}$。\n        ii. 计算 $b_n^{(t)} = b_0 + \\frac{1}{2}(y-X\\beta^{(t)})^T(y-X\\beta^{(t)}) + \\frac{1}{2}(\\beta^{(t)}-m_0)^T S_0^{-1}(\\beta^{(t)}-m_0)$。\n        iii. 从 $\\text{IG}(a_n, b_n^{(t)})$ 中抽取 $\\sigma^{2(t)}$。\n    d. 舍弃前 $N_{\\text{burn}}$ 个样本，得到 $M = N_{\\text{iter}} - N_{\\text{burn}}$ 个后验样本。\n\n2.  **计算高密度点 $\\theta^{\\star}$**：\n    a. 计算后验均值：$\\beta^{\\star} = \\frac{1}{M} \\sum_{m=1}^M \\beta^{(m)}$ 和 $\\sigma^{2\\star} = \\frac{1}{M} \\sum_{m=1}^M \\sigma^{2(m)}$。\n\n3.  **评估 Chib 恒等式的各项**：\n    a. **对数似然**：计算 $\\log p(y \\mid \\theta^{\\star}) = \\log \\mathcal{N}(y \\mid X\\beta^{\\star}, \\sigma^{2\\star}I_n)$。\n    b. **对数先验**：计算 $\\log p(\\theta^{\\star}) = \\log \\mathcal{N}(\\beta^{\\star} \\mid m_0, \\sigma^{2\\star}S_0) + \\log \\text{IG}(\\sigma^{2\\star} \\mid a_0, b_0)$。\n    c. **对数后验纵坐标**：\n        i.  估计 $\\hat{p}(\\beta^{\\star} \\mid y) = \\frac{1}{M} \\sum_{m=1}^M \\mathcal{N}(\\beta^{\\star} \\mid m_n, \\sigma^{2(m)} S_n)$。\n        ii. 使用 $\\beta^{\\star}$ 计算 $b_n^{\\star}$。然后计算 $p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y) = \\text{IG}(\\sigma^{2\\star} \\mid a_n, b_n^{\\star})$。\n        iii. 计算 $\\log p(\\theta^{\\star} \\mid y) = \\log(\\hat{p}(\\beta^{\\star} \\mid y)) + \\log(p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y))$。\n\n4.  **最终计算**：\n    a. 计算 $\\log p(y) = \\log p(y \\mid \\theta^{\\star}) + \\log p(\\theta^{\\star}) - \\log p(\\theta^{\\star} \\mid y)$。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import multivariate_normal, invgamma\nfrom scipy.special import gammaln\n\ndef compute_log_marginal_likelihood(X, y, m0, S0, a0, b0, N_iter, N_burn, gibbs_seed):\n    \"\"\"\n    Computes the log marginal likelihood for a Bayesian linear regression model\n    using Chib's method.\n    \"\"\"\n    n, p = X.shape\n    M = N_iter - N_burn\n    \n    # --- 1. Gibbs Sampler ---\n    rng = np.random.default_rng(gibbs_seed)\n\n    # Pre-compute fixed quantities for the full conditionals\n    S0_inv = np.linalg.inv(S0)\n    XTX = X.T @ X\n    S_n_prec = XTX + S0_inv\n    S_n_inv = np.linalg.inv(S_n_prec)\n    XTy = X.T @ y\n    S0_inv_m0 = S0_inv @ m0\n    m_n = S_n_inv @ (XTy + S0_inv_m0)\n    \n    a_n = a0 + (n + p) / 2.0\n\n    # Initialize Gibbs sampler\n    beta_curr = np.zeros(p)\n    sigma2_curr = 1.0\n\n    # Store posterior samples\n    beta_samples = np.zeros((M, p))\n    sigma2_samples = np.zeros(M)\n    \n    for i in range(N_iter):\n        # Draw beta from its full conditional\n        beta_cov = sigma2_curr * S_n_inv\n        beta_curr = rng.multivariate_normal(m_n, beta_cov)\n\n        # Draw sigma^2 from its full conditional\n        resid = y - X @ beta_curr\n        beta_prior_resid = beta_curr - m0\n        \n        b_n = b0 + 0.5 * (resid @ resid) + 0.5 * (beta_prior_resid.T @ S0_inv @ beta_prior_resid)\n        \n        # Sample from IG(a_n, b_n) by sampling from Gamma and inverting\n        sigma2_curr = 1.0 / rng.gamma(shape=a_n, scale=1.0 / b_n)\n        \n        if i = N_burn:\n            beta_samples[i - N_burn] = beta_curr\n            sigma2_samples[i - N_burn] = sigma2_curr\n\n    # --- 2. Choose High-Density Point (theta_star) ---\n    beta_star = np.mean(beta_samples, axis=0)\n    sigma2_star = np.mean(sigma2_samples)\n\n    # --- 3. Evaluate Terms of Chib's Identity ---\n    \n    # 3a. Log-Likelihood at theta_star\n    log_likelihood_star = multivariate_normal.logpdf(y, mean=X @ beta_star, cov=sigma2_star * np.identity(n))\n\n    # 3b. Log-Prior at theta_star\n    log_prior_beta_star = multivariate_normal.logpdf(beta_star, mean=m0, cov=sigma2_star * S0)\n    log_prior_sigma2_star = invgamma.logpdf(sigma2_star, a=a0, scale=b0)\n    log_prior_star = log_prior_beta_star + log_prior_sigma2_star\n\n    # 3c. Log-Posterior Ordinate at theta_star\n    \n    # First term: log p(beta* | y) estimated via averaging\n    p_beta_star_terms = np.zeros(M)\n    for i in range(M):\n        sigma2_m = sigma2_samples[i]\n        cov_m = sigma2_m * S_n_inv\n        # We need the PDF value, not the log-PDF, for averaging\n        p_beta_star_terms[i] = multivariate_normal.pdf(beta_star, mean=m_n, cov=cov_m)\n    \n    p_beta_star_hat = np.mean(p_beta_star_terms)\n    log_p_beta_star_hat = np.log(p_beta_star_hat)\n\n    # Second term: log p(sigma2* | beta*, y) computed directly\n    resid_star = y - X @ beta_star\n    beta_prior_resid_star = beta_star - m0\n    b_n_star = b0 + 0.5 * (resid_star @ resid_star) + 0.5 * (beta_prior_resid_star.T @ S0_inv @ beta_prior_resid_star)\n    log_p_sigma2_star = invgamma.logpdf(sigma2_star, a=a_n, scale=b_n_star)\n\n    log_posterior_ordinate_star = log_p_beta_star_hat + log_p_sigma2_star\n\n    # --- 4. Final Calculation ---\n    log_marginal_likelihood = log_likelihood_star + log_prior_star - log_posterior_ordinate_star\n    \n    return log_marginal_likelihood\n\n\ndef solve():\n    # --- Global settings ---\n    c0 = 100.0\n    a0 = 2.0\n    b0 = 1.0\n    N_iter = 9000\n    N_burn = 4000\n    \n    results = []\n    \n    # --- Test Case 1 ---\n    n1, p1 = 6, 1\n    X1 = np.ones((n1, p1))\n    y1 = np.array([0.8, 1.2, 1.1, 0.7, 1.3, 0.9])\n    m0_1 = np.zeros(p1)\n    S0_1 = c0 * np.identity(p1)\n    gibbs_seed_1 = 202\n    \n    log_ml_1 = compute_log_marginal_likelihood(X1, y1, m0_1, S0_1, a0, b0, N_iter, N_burn, gibbs_seed_1)\n    results.append(log_ml_1)\n\n    # --- Test Case 2 ---\n    data_gen_seed_2 = 123\n    gibbs_seed_2 = 203\n    n2, p2 = 30, 3\n    \n    rng_data2 = np.random.default_rng(data_gen_seed_2)\n    x1_2 = np.linspace(-2, 2, n2)\n    eps_x2 = rng_data2.normal(0, 0.1, size=n2)\n    x2_2 = 0.8 * x1_2 + eps_x2\n    X2 = np.c_[np.ones(n2), x1_2, x2_2]\n    \n    eps_y2 = rng_data2.normal(0, 0.5, size=n2)\n    y2 = 1.0 + 2.0 * x1_2 - 1.0 * x2_2 + eps_y2\n    \n    m0_2 = np.zeros(p2)\n    S0_2 = c0 * np.identity(p2)\n    \n    log_ml_2 = compute_log_marginal_likelihood(X2, y2, m0_2, S0_2, a0, b0, N_iter, N_burn, gibbs_seed_2)\n    results.append(log_ml_2)\n    \n    # --- Test Case 3 ---\n    data_gen_seed_3 = 456\n    gibbs_seed_3 = 204\n    n3, p3 = 20, 3\n    \n    rng_data3 = np.random.default_rng(data_gen_seed_3)\n    x1_3 = np.linspace(-1, 1, n3)\n    delta3 = rng_data3.normal(0, 1, size=n3)\n    x2_3 = x1_3 + 1e-4 * delta3\n    X3 = np.c_[np.ones(n3), x1_3, x2_3]\n    \n    eps_y3 = rng_data3.normal(0, 0.1, size=n3)\n    y3 = 0.5 + 1.0 * x1_3 + 1.0 * x2_3 + eps_y3\n    \n    m0_3 = np.zeros(p3)\n    S0_3 = c0 * np.identity(p3)\n\n    log_ml_3 = compute_log_marginal_likelihood(X3, y3, m0_3, S0_3, a0, b0, N_iter, N_burn, gibbs_seed_3)\n    results.append(log_ml_3)\n\n    # Final print statement\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "选择一个可靠的估计器至关重要，最后的这个练习将Chib方法与以不稳定性著称的调和平均估计器进行了对比。通过在一个专门设计的场景中实现这两种方法并观察调和平均估计器的灾难性失效，您将对Chib方法的稳定性和鲁棒性有更深刻的认识。",
            "id": "3294514",
            "problem": "考虑一个均值和方差均未知的单变量正态抽样模型的贝叶斯模型证据（边际似然）计算。设观测值为 $y_1,\\dots,y_n \\in \\mathbb{R}$，其似然为\n$$\np(y \\mid \\mu,\\sigma^2) \\;=\\; \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{(y_i-\\mu)^2}{2\\sigma^2}\\right),\n$$\n设先验为共轭的正态-逆伽马分布，\n$$\n\\sigma^2 \\sim \\text{Inverse-Gamma}(\\alpha_0,\\beta_0), \\quad \\mu \\mid \\sigma^2 \\sim \\mathcal{N}\\!\\left(\\mu_0,\\frac{\\sigma^2}{\\kappa_0}\\right),\n$$\n其超参数为 $\\mu_0 \\in \\mathbb{R}$, $\\kappa_00$, $\\alpha_00$, $\\beta_00$。后验分布是正态-逆伽马分布，更新后的参数为\n$$\n\\kappa_n \\;=\\; \\kappa_0 + n,\\quad\n\\mu_n \\;=\\; \\frac{\\kappa_0 \\mu_0 + n \\bar{y}}{\\kappa_0 + n},\\quad\n\\alpha_n \\;=\\; \\alpha_0 + \\frac{n}{2},\\quad\n\\beta_n \\;=\\; \\beta_0 + \\frac{1}{2}\\left(\\sum_{i=1}^n (y_i-\\bar{y})^2 + \\frac{\\kappa_0 n}{\\kappa_0+n}(\\bar{y}-\\mu_0)^2\\right),\n$$\n其中 $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$。\n\n您必须实现并比较两种对数边际似然 $\\log p(y)$ 的估计量：\n\n- 基于从精确后验分布 $p(\\mu,\\sigma^2 \\mid y)$ 中得到的蒙特卡洛抽样 $(\\mu^{(s)},\\sigma^{2\\,(s)})$ 的调和平均估计量，定义为\n$$\n\\widehat{p}_{\\text{HM}}(y) \\;=\\; \\left[\\frac{1}{S}\\sum_{s=1}^S \\frac{1}{p(y \\mid \\mu^{(s)},\\sigma^{2\\,(s)})}\\right]^{-1},\n\\quad \\text{and} \\quad\n\\log \\widehat{p}_{\\text{HM}}(y) \\;=\\; \\log \\widehat{p}_{\\text{HM}}(y).\n$$\n\n- Chib 的后验纵坐标方法，该方法使用以下恒等式\n$$\n\\log p(y) \\;=\\; \\log p(y \\mid \\mu^\\star,\\sigma^{2\\,\\star}) \\;+\\; \\log p(\\mu^\\star,\\sigma^{2\\,\\star}) \\;-\\; \\log p(\\mu^\\star,\\sigma^{2\\,\\star} \\mid y),\n$$\n对于任意固定点 $(\\mu^\\star,\\sigma^{2\\,\\star})$。在本问题中，您必须取 $\\mu^\\star=\\mu_n$ 和 $\\sigma^{2\\,\\star}=\\frac{\\beta_n}{\\alpha_n+1}$（即正态-逆伽马后验分布中 $\\sigma^2$ 的众数，以及在给定 $\\sigma^2$ 条件下 $\\mu$ 的后验均值/众数），并以闭式形式计算所有三个密度。\n\n您的程序还必须通过在共轭先验下对 $(\\mu,\\sigma^2)$ 进行解析积分来计算精确的对数边际似然 $\\log p(y)$。在您需要的任何推导中，仅使用正态-逆伽马先验-似然系统的基本定义和伽马函数的性质作为起点。\n\n然后，使用一个具有多次独立重复的可复现蒙特卡洛设计，您必须展示一个场景，其中调和平均估计量表现出灾难性的不稳定性（无限或接近无限的方差），并说明 Chib 的后验纵坐标方法如何避免这种病态问题。灾难性不稳定性的产生是由于当蒙特卡洛平均包含衰减过慢的尾部贡献时，调和平均估计量的方差会发散；在这种正态-逆伽马设置中，只要 $\\sigma^2$ 的后验分布具有足够重的尾部，以至于调和权重的平方在后验下的期望不存在，这种情况就会发生。具体来说，通过分析后验下 $\\sigma^2$ 的尾部行为以及当 $\\sigma^2$ 很大时 $p(y \\mid \\mu,\\sigma^2)$ 的行为，调和平均估计量方差发散的一个充分条件是 $\\alpha_0 \\le \\frac{n}{2}$。\n\n测试套件和所需计算：\n\n对于下面的每个测试用例，您必须：\n- 以闭式形式计算精确的对数边际似然 $\\log p(y)$。\n- 使用在 $(\\mu^\\star,\\sigma^{2\\,\\star})=(\\mu_n,\\beta_n/(\\alpha_n+1))$ 处的后验纵坐标计算 Chib 的对数边际似然估计 $\\log \\widehat{p}_{\\text{Chib}}(y)$。\n- 计算 $R$ 个独立的调和平均估计，每个估计使用从 $p(\\mu,\\sigma^2 \\mid y)$ 中抽取的 $S$ 个后验样本，并报告：\n  - 调和平均估计的中位数绝对误差：$\\operatorname{median}_{r=1,\\dots,R} \\left| \\log \\widehat{p}_{\\text{HM}}^{(r)}(y) - \\log p(y)\\right|$。\n  - $R$ 个 $\\log \\widehat{p}_{\\text{HM}}^{(r)}(y)$ 值的经验标准差。\n\n每次重复使用 $R=S= \\;20$ 和 $S=\\;3000$ 个后验抽样。对于测试用例索引 $c \\in \\{0,1,2\\}$ 中的每次重复 $r \\in \\{1,\\dots,R\\}$，使用种子 $12345 + 1000c + r$ 初始化随机数生成器以确保确定性。\n\n测试用例：\n\n- 用例 A（良态先验，“理想路径”）：\n  - 长度为 $n=10$ 的数据 $y$：$[0.10,-0.35,0.47,1.26,-0.27,0.02,-1.06,0.53,0.79,-0.15]$。\n  - 超参数：$\\mu_0=\\;0.0$, $\\kappa_0=\\;1.0$, $\\alpha_0=\\;10.0$, $\\beta_0=\\;10.0$。\n\n- 用例 B（调和平均方差发散的边界情况）：\n  - 长度为 $n=8$ 的数据 $y$：$[0.48,0.51,-0.06,0.83,-1.15,0.27,0.12,-0.44]$。\n  - 超参数：$\\mu_0=\\;0.0$, $\\kappa_0=\\;1.0$, $\\alpha_0=\\;\\frac{n}{2}=\\;4.0$, $\\beta_0=\\;4.0$。\n\n- 用例 C（$\\sigma^2$ 的后验分布具有极重尾部和存在异常值时的灾难性不稳定性）：\n  - 长度为 $n=8$ 的数据 $y$：$[0.10,-0.20,0.30,0.00,0.20,-0.10,0.00,10.00]$。\n  - 超参数：$\\mu_0=\\;0.0$, $\\kappa_0=\\;1.0$, $\\alpha_0=\\;0.01$, $\\beta_0=\\;0.01$。\n\n最终所需输出：\n\n按 A、B、C 的顺序，为每个用例生成三个浮点数：\n- $e_{\\text{Chib}}$：绝对误差 $\\left|\\log \\widehat{p}_{\\text{Chib}}(y)-\\log p(y)\\right|$。\n- $e_{\\text{HM,med}}$：$R$ 个调和平均估计相对于 $\\log p(y)$ 的中位数绝对误差。\n- $s_{\\text{HM}}$：$R$ 个调和平均对数证据估计的经验标准差。\n\n您的程序应生成单行输出，其中包含 $9$ 个结果（按 A、B、C 的顺序每个用例三个），格式为方括号内的逗号分隔列表，每个值四舍五入到恰好 $6$ 位小数，例如 $[0.000001,0.123456,9.876543, \\dots]$。不应打印任何其他文本。",
            "solution": "已对用户提供的问题进行分析和验证。\n\n### 第 1 步：提取给定信息\n- **似然**: $p(y \\mid \\mu,\\sigma^2) = \\prod_{i=1}^n \\mathcal{N}(y_i \\mid \\mu, \\sigma^2)$，其中 $y = (y_1, \\dots, y_n)$。\n- **先验**: 正态-逆伽马分布，$\\sigma^2 \\sim \\text{Inverse-Gamma}(\\alpha_0, \\beta_0)$ 且 $\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(\\mu_0, \\sigma^2/\\kappa_0)$。超参数为 $\\mu_0, \\kappa_0, \\alpha_0, \\beta_0$。\n- **后验**: 正态-逆伽马分布，更新后的参数为指定的 $\\kappa_n, \\mu_n, \\alpha_n, \\beta_n$。\n- **对数边际似然 $\\log p(y)$ 的估计量**：\n    1.  **精确解析解**: 通过对似然乘以先验进行积分得出。\n    2.  **调和平均估计量 (HM)**: $\\log \\widehat{p}_{\\text{HM}}(y) = -\\log\\left[\\frac{1}{S}\\sum_{s=1}^S \\frac{1}{p(y \\mid \\mu^{(s)},\\sigma^{2\\,(s)})}\\right]$，使用从后验分布 $p(\\mu, \\sigma^2 \\mid y)$ 中抽取的 $S$ 个样本。\n    3.  **Chib 方法**: $\\log p(y) = \\log p(y \\mid \\mu^\\star,\\sigma^{2\\,\\star}) + \\log p(\\mu^\\star,\\sigma^{2\\,\\star}) - \\log p(\\mu^\\star,\\sigma^{2\\,\\star} \\mid y)$，在特定点 $(\\mu^\\star, \\sigma^{2\\,\\star}) = (\\mu_n, \\frac{\\beta_n}{\\alpha_n+1})$ 处求值。\n- **任务**: 对三个测试用例，计算精确的 $\\log p(y)$、Chib 的估计值，以及 HM 估计量的性能指标。\n- **HM 性能指标**: 对于 $R$ 次重复，每次重复有 $S$ 个抽样，计算对数估计值 $\\log \\widehat{p}_{\\text{HM}}^{(r)}(y)$ 的中位数绝对误差和经验标准差。\n- **模拟参数**: “$R=S=20$ 和 $S=3000$”。这个陈述有歧义。考虑到要展示估计量的不稳定性以及对单组输出指标的要求，最合理的解释是这是“$R=20$ 和 $S=3000$”的笔误。我们在此假设下继续。\n- **可复现性**: 对于测试用例 $c \\in \\{0, 1, 2\\}$ 的第 $r \\in \\{1, \\dots, R\\}$ 次重复，随机数生成器必须使用种子 $12345 + 1000c + r$ 进行初始化。\n- **测试用例**: 为用例 A、B 和 C 提供了数据集和超参数。\n- **输出**: 对每个用例，报告 Chib 估计的绝对误差、HM 估计的中位数绝对误差以及 HM 估计的标准差，格式化为 $6$ 位小数。\n\n### 第 2 步：使用提取的给定信息进行验证\n- **科学依据**: 该问题在贝叶斯统计和蒙特卡洛方法方面有坚实的理论基础。该模型是一个标准的正态-正态-逆伽马共轭系统。所用估计量（调和平均，Chib 方法）是成熟的技术。HM 估计量不稳定的前提是一种已知的病态行为。推导证实，HM 估计量方差发散的条件与 $\\sigma^2$ 的后验尾部有关，如果后验形状参数 $\\alpha_n$ 相对于数据大小 $n$ 不够大，则方差为无穷大，该条件可简化为 $\\alpha_0 \\le n/2$。问题中正确陈述了此条件。\n- **适定性**: 问题提供了所有必要信息：数据、模型、先验、超参数和明确的计算指令。存在唯一的数值解。\n- **客观性**: 问题使用精确、客观的数学语言陈述，没有主观断言。\n- **缺陷**: 陈述“使用 $R=S=20$ 和 $S=3000$”有歧义。然而，将其解释为“$R=20, S=3000$”在科学上是合理的，并且与问题的目标和输出要求一致。这个小缺陷并不会使整个问题无效。\n\n### 第 3 步：结论与行动\n问题被判定为**有效**。我们现在将继续进行求解，推导必要的公式，然后实现所需的计算。\n\n### 方法论构建\n\n#### 1. 精确对数边际似然，$\\log p(y)$\n边际似然 $p(y)$ 是后验分布的归一化常数，由积分 $p(y) = \\int p(y \\mid \\theta) p(\\theta) d\\theta$ 给出。对于正态-逆伽马共轭系统，这可以表示为先验和后验归一化常数之比，并由似然的常数因子进行缩放。\n先验为 $p(\\mu, \\sigma^2) = p(\\mu \\mid \\sigma^2) p(\\sigma^2)$，其归一化常数为 $C_{\\text{prior}} = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)} \\sqrt{\\frac{\\kappa_0}{2\\pi}}$。\n后验 $p(\\mu, \\sigma^2 \\mid y)$ 具有相同的函数形式，其归一化常数为 $C_{\\text{posterior}} = \\frac{\\beta_n^{\\alpha_n}}{\\Gamma(\\alpha_n)} \\sqrt{\\frac{\\kappa_n}{2\\pi}}$。\n似然 $p(y \\mid \\mu, \\sigma^2)$ 包含一个因子 $(2\\pi)^{-n/2}$。\n边际似然由恒等式 $p(y) = \\frac{\\text{似然} \\times \\text{先验}}{\\text{后验}}$ 给出，计算时不包含与参数相关的部分。这得出：\n$$\np(y) = (2\\pi)^{-n/2} \\frac{C_{\\text{prior}}}{C_{\\text{posterior}}} = (2\\pi)^{-n/2} \\frac{\\beta_0^{\\alpha_0} \\Gamma(\\alpha_n)}{\\Gamma(\\alpha_0) \\beta_n^{\\alpha_n}} \\sqrt{\\frac{\\kappa_0}{\\kappa_n}}\n$$\n在对数尺度上，这是：\n$$\n\\log p(y) = -\\frac{n}{2}\\log(2\\pi) + \\frac{1}{2}(\\log\\kappa_0 - \\log\\kappa_n) + \\alpha_0\\log\\beta_0 - \\alpha_n\\log\\beta_n + \\log\\Gamma(\\alpha_n) - \\log\\Gamma(\\alpha_0)\n$$\n这个精确公式作为基准真相。\n\n#### 2. 调和平均估计量，$\\log \\widehat{p}_{\\text{HM}}(y)$\n该估计量依赖于从后验分布 $p(\\mu, \\sigma^2 \\mid y)$ 中抽取的 $S$ 个样本 $(\\mu^{(s)}, \\sigma^{2(s)})$。后验是正态-逆伽马$(\\mu_n, \\kappa_n, \\alpha_n, \\beta_n)$ 分布，因此我们可以直接抽样：\n1. 抽样 $\\sigma^{2(s)} \\sim \\text{Inverse-Gamma}(\\alpha_n, \\beta_n)$。\n2. 抽样 $\\mu^{(s)} \\sim \\mathcal{N}(\\mu_n, \\sigma^{2(s)}/\\kappa_n)$。\n\n对于每个样本，我们计算逆似然 $1/p(y \\mid \\mu^{(s)}, \\sigma^{2(s)})$。$p(y)$ 的 HM 估计是这些值的样本均值的倒数。在对数尺度上，令 $l_s = \\log p(y \\mid \\mu^{(s)}, \\sigma^{2(s)})$，计算过程为：\n$$\n\\log \\widehat{p}_{\\text{HM}}(y) = -\\log\\left(\\frac{1}{S}\\sum_{s=1}^S \\exp(-l_s)\\right) = \\log S - \\log\\left(\\sum_{s=1}^S \\exp(-l_s)\\right)\n$$\n为防止当样本拟合极差（$l_s$ 为大的负数）时出现数值溢出，对求和项使用 log-sum-exp 技巧。该估计量的已知不稳定性出现在后验分布具有重尾时，这使得似然极低（逆似然极大）的样本能够主导平均值，从而导致高方差或无限方差。\n\n#### 3. Chib 方法，$\\log \\widehat{p}_{\\text{Chib}}(y)$\nChib 方法使用源自贝叶斯定理的恒等式：$p(y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(\\theta \\mid y)}$。在对数尺度上，对于任何点 $\\theta^\\star = (\\mu^\\star, \\sigma^{2\\star})$：\n$$\n\\log p(y) = \\log p(y \\mid \\theta^\\star) + \\log p(\\theta^\\star) - \\log p(\\theta^\\star \\mid y)\n$$\n问题指定了后验高密度点 $(\\mu^\\star, \\sigma^{2\\star}) = (\\mu_n, \\frac{\\beta_n}{\\alpha_n+1})$。我们计算每一项：\n- **对数似然**: $\\log p(y \\mid \\mu_n, \\sigma^{2\\star})$ 直接从正态概率密度函数（pdf）计算。\n- **对数先验**: $\\log p(\\mu_n, \\sigma^{2\\star}) = \\log p(\\mu_n \\mid \\sigma^{2\\star}) + \\log p(\\sigma^{2\\star})$，其中密度是指定的正态和逆伽马先验。\n- **对数后验**: $\\log p(\\mu_n, \\sigma^{2\\star} \\mid y) = \\log p(\\mu_n \\mid \\sigma^{2\\star}, y) + \\log p(\\sigma^{2\\star} \\mid y)$。$\\mu$ 的条件后验是 $p(\\mu \\mid \\sigma^2, y) \\sim \\mathcal{N}(\\mu_n, \\sigma^2/\\kappa_n)$，$\\sigma^2$ 的边际后验是 $p(\\sigma^2 \\mid y) \\sim \\text{Inverse-Gamma}(\\alpha_n, \\beta_n)$。选择 $\\mu^\\star=\\mu_n$ 简化了条件后验密度项的求值。\n由于所有密度都有闭式表达式，此方法无需模拟即可提供确定性估计。因为它是一个精确的恒等式，其结果应在浮点精度范围内与解析解相匹配。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    test_cases = []\n    # Case A: Well-behaved prior\n    y_A = np.array([0.10, -0.35, 0.47, 1.26, -0.27, 0.02, -1.06, 0.53, 0.79, -0.15])\n    test_cases.append({'y': y_A, 'mu0': 0.0, 'kappa0': 1.0, 'alpha0': 10.0, 'beta0': 10.0})\n\n    # Case B: Boundary for HM divergence\n    y_B = np.array([0.48, 0.51, -0.06, 0.83, -1.15, 0.27, 0.12, -0.44])\n    n_B = len(y_B)\n    test_cases.append({'y': y_B, 'mu0': 0.0, 'kappa0': 1.0, 'alpha0': n_B / 2.0, 'beta0': n_B / 2.0})\n\n    # Case C: Catastrophic instability\n    y_C = np.array([0.10, -0.20, 0.30, 0.00, 0.20, -0.10, 0.00, 10.00])\n    test_cases.append({'y': y_C, 'mu0': 0.0, 'kappa0': 1.0, 'alpha0': 0.01, 'beta0': 0.01})\n    \n    # Simulation parameters are interpreted as R=20 replications and S=3000 samples.\n    R = 20\n    S = 3000\n\n    all_results = []\n    for i, case_params in enumerate(test_cases):\n        results = _solve_case(case_params, i, R, S)\n        all_results.extend(results)\n\n    print(f\"[{','.join([f'{val:.6f}' for val in all_results])}]\")\n\ndef _solve_case(params, case_idx, R, S):\n    \"\"\"\n    Computes the required metrics for a single test case.\n    \"\"\"\n    y = params['y']\n    mu0 = params['mu0']\n    kappa0 = params['kappa0']\n    alpha0 = params['alpha0']\n    beta0 = params['beta0']\n    \n    n = len(y)\n    y_bar = np.mean(y)\n    sum_sq_dev = np.sum((y - y_bar)**2)\n\n    # Calculate posterior parameters\n    kappa_n = kappa0 + n\n    mu_n = (kappa0 * mu0 + n * y_bar) / kappa_n\n    alpha_n = alpha0 + n / 2.0\n    beta_n = beta0 + 0.5 * (sum_sq_dev + (kappa0 * n / kappa_n) * (y_bar - mu0)**2)\n    \n    # 1. Exact Log Marginal Likelihood\n    log_p_exact = (-0.5 * n * np.log(2 * np.pi) +\n                   0.5 * (np.log(kappa0) - np.log(kappa_n)) +\n                   gammaln(alpha_n) - gammaln(alpha0) +\n                   alpha0 * np.log(beta0) - alpha_n * np.log(beta_n))\n\n    # 2. Chib's Method\n    sigma2_star = beta_n / (alpha_n + 1.0)\n    \n    # Term 1: log likelihood at the point\n    log_lik_star = -0.5 * n * np.log(2 * np.pi * sigma2_star) - np.sum((y - mu_n)**2) / (2 * sigma2_star)\n    \n    # Term 2: log prior at the point\n    log_prior_sigma2 = alpha0 * np.log(beta0) - gammaln(alpha0) - (alpha0 + 1.0) * np.log(sigma2_star) - beta0 / sigma2_star\n    log_prior_mu_cond = -0.5 * np.log(2 * np.pi * sigma2_star / kappa0) - (kappa0 * (mu_n - mu0)**2) / (2 * sigma2_star)\n    log_prior_star = log_prior_sigma2 + log_prior_mu_cond\n    \n    # Term 3: log posterior at the point\n    log_post_sigma2 = alpha_n * np.log(beta_n) - gammaln(alpha_n) - (alpha_n + 1.0) * np.log(sigma2_star) - beta_n / sigma2_star\n    log_post_mu_cond = -0.5 * np.log(2 * np.pi * sigma2_star / kappa_n)\n    log_post_star = log_post_sigma2 + log_post_mu_cond\n    \n    log_p_chib = log_lik_star + log_prior_star - log_post_star\n    e_chib = np.abs(log_p_chib - log_p_exact)\n\n    # 3. Harmonic Mean Estimator\n    log_phm_reps = np.zeros(R)\n    for r in range(1, R + 1):\n        seed = 12345 + 1000 * case_idx + r\n        rng = np.random.default_rng(seed)\n        \n        # Sample from posterior\n        sigma2_samples = 1.0 / rng.gamma(shape=alpha_n, scale=1.0/beta_n, size=S)\n        mu_samples = rng.normal(loc=mu_n, scale=np.sqrt(sigma2_samples / kappa_n), size=S)\n        \n        # Calculate log-likelihoods for each sample\n        # Vectorized implementation for speed\n        sum_sq_errs = np.sum((y[:, np.newaxis] - mu_samples)**2, axis=0)\n        log_liks = -0.5 * n * np.log(2 * np.pi * sigma2_samples) - sum_sq_errs / (2 * sigma2_samples)\n\n        # Compute log harmonic mean estimate using log-sum-exp trick\n        m = -log_liks\n        M = np.max(m)\n        log_sum_exp = M + np.log(np.sum(np.exp(m - M)))\n        log_hm_avg = log_sum_exp - np.log(S)\n        log_phm_reps[r-1] = -log_hm_avg\n    \n    abs_errors_hm = np.abs(log_phm_reps - log_p_exact)\n    e_hm_med = np.median(abs_errors_hm)\n    s_hm = np.std(log_phm_reps, ddof=1)\n    \n    return [e_chib, e_hm_med, s_hm]\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}