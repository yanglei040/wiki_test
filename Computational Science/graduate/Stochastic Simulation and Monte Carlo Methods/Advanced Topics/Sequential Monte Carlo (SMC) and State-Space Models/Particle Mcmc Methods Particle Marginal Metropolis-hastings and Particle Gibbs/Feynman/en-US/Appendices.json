{
    "hands_on_practices": [
        {
            "introduction": "A Particle MCMC method's performance is deeply connected to the particle filter used to estimate the marginal likelihood. The filter's own efficiency hinges on the variance of its importance weights, as high variance leads to particle degeneracy and poor estimates. This exercise provides a foundational look into this mechanism by asking you to analytically compute and compare the weight variance for two different proposal distributions—the simple bootstrap proposal and the theoretically optimal one—within a standard linear Gaussian model . This comparison offers a concrete illustration of what \"optimality\" means in this context and why more sophisticated proposals can be beneficial.",
            "id": "3327380",
            "problem": "Consider a scalar Hidden Markov Model (HMM) with latent state $x_t \\in \\mathbb{R}$ and observation $y_t \\in \\mathbb{R}$, specified by the transition density $p_{\\theta}(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t ; \\phi x_{t-1}, \\sigma_x^2)$ and the emission density $p_{\\theta}(y_t \\mid x_t) = \\mathcal{N}(y_t ; x_t, \\sigma_y^2)$, where $\\phi \\in \\mathbb{R}$, $\\sigma_x^2 > 0$, and $\\sigma_y^2 > 0$ are fixed parameters. In the context of Particle Markov chain Monte Carlo (PMCMC), and specifically for a single time step within a particle filter used inside Particle Marginal Metropolis–Hastings (PMMH) or Particle Gibbs, consider the incremental importance weight at time $t$, defined for a general proposal $q(x_t \\mid x_{t-1}, y_t)$ by\n$$\nw_t(x_t; x_{t-1}, y_t) \\propto \\frac{p_{\\theta}(y_t \\mid x_t)\\, p_{\\theta}(x_t \\mid x_{t-1})}{q(x_t \\mid x_{t-1}, y_t)}.\n$$\nFocus on a single time step $t$ with $x_{t-1} = a \\in \\mathbb{R}$ and observed $y_t = y \\in \\mathbb{R}$. Compare the variance (with respect to the proposal draw of $x_t$) of the incremental importance weights under:\n- the bootstrap proposal $q_{\\text{boot}}(x_t \\mid x_{t-1}, y_t) = p_{\\theta}(x_t \\mid x_{t-1})$, and\n- the optimal proposal $q_{\\text{opt}}(x_t \\mid x_{t-1}, y_t) \\propto p_{\\theta}(y_t \\mid x_t)\\, p_{\\theta}(x_t \\mid x_{t-1})$.\n\nAssume the variance is computed conditional on the fixed values $x_{t-1} = a$ and $y_t = y$, and use the unnormalized incremental weights. Derive from first principles the exact variance under each proposal. Then, report as your final answer the closed-form analytic expression for the variance of the incremental importance weight under the bootstrap proposal as a function of $\\phi$, $a$, $y$, $\\sigma_x^2$, and $\\sigma_y^2$.\n\nAnswer in a single closed-form analytic expression. Do not include any units.",
            "solution": "The problem asks for the variance of the incremental importance weight for a particle filter at a single time step, under two different proposal distributions: the bootstrap proposal and the optimal proposal. The model is a linear Gaussian state-space model.\n\nFirst, we establish the notation for a single time step $t$, dropping the subscript for clarity.\nThe prior state is fixed at $x_{t-1} = a$.\nThe current observation is fixed at $y_t = y$.\nThe state transition model is $p(x|a) = p_{\\theta}(x_t=x | x_{t-1}=a) = \\mathcal{N}(x; \\phi a, \\sigma_x^2)$.\nThe observation model is $p(y|x) = p_{\\theta}(y_t=y | x_t=x) = \\mathcal{N}(y; x, \\sigma_y^2)$.\nThe unnormalized incremental importance weight is given by\n$$\nw(x) = \\frac{p(y|x) p(x|a)}{q(x|a, y)}\n$$\nThe variance is to be computed with respect to the random variable $x \\sim q(x|a, y)$, conditional on the fixed values $a$ and $y$.\n\nFirst, we analyze the optimal proposal, as requested for comparison.\nThe optimal proposal, in the sense that it minimizes the variance of the weights, is the posterior distribution of the state given the observation and the previous state:\n$$\nq_{\\text{opt}}(x|a, y) = p(x|a, y) = \\frac{p(y|x) p(x|a)}{p(y|a)}\n$$\nwhere $p(y|a) = \\int p(y|x) p(x|a) dx$ is the marginal likelihood of the observation.\nFor this proposal, the incremental weight is:\n$$\nw_{\\text{opt}}(x) = \\frac{p(y|x) p(x|a)}{q_{\\text{opt}}(x|a, y)} = \\frac{p(y|x) p(x|a)}{ \\frac{p(y|x) p(x|a)}{p(y|a)} } = p(y|a)\n$$\nThe weight $w_{\\text{opt}}(x)$ is equal to the marginal likelihood $p(y|a)$, which is a constant with respect to $x$. The variance of a constant is zero.\n$$\n\\text{Var}_{x \\sim q_{\\text{opt}}}[w_{\\text{opt}}(x)] = \\text{Var}[p(y|a)] = 0\n$$\nThis demonstrates the \"optimality\" of this proposal in terms of weight variance.\n\nNext, we analyze the bootstrap proposal, which is the main focus of the problem.\nThe bootstrap proposal uses the state transition model as the proposal distribution:\n$$\nq_{\\text{boot}}(x|a, y) = p(x|a) = \\mathcal{N}(x; \\phi a, \\sigma_x^2)\n$$\nFor this proposal, the incremental weight simplifies to the observation likelihood:\n$$\nw_{\\text{boot}}(x) = \\frac{p(y|x) p(x|a)}{q_{\\text{boot}}(x|a, y)} = \\frac{p(y|x) p(x|a)}{p(x|a)} = p(y|x)\n$$\nSpecifically, $w_{\\text{boot}}(x) = \\mathcal{N}(y; x, \\sigma_y^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left(-\\frac{(y-x)^2}{2\\sigma_y^2}\\right)$.\nWe need to find the variance of this weight, $\\text{Var}_{x \\sim q_{\\text{boot}}}[w_{\\text{boot}}(x)]$, where the randomness is from $x \\sim q_{\\text{boot}}(x|a,y) = \\mathcal{N}(x; \\phi a, \\sigma_x^2)$.\nThe variance is given by $\\text{Var}(w) = E[w^2] - (E[w])^2$.\n\nFirst, we compute the expected weight, $E[w_{\\text{boot}}(x)]$.\n$$\nE[w_{\\text{boot}}(x)] = \\int_{-\\infty}^{\\infty} w_{\\text{boot}}(x) q_{\\text{boot}}(x) dx = \\int_{-\\infty}^{\\infty} p(y|x) p(x|a) dx\n$$\nThis integral is the marginal likelihood $p(y|a)$. For this linear Gaussian system, $p(y|a)$ is also a Gaussian distribution. Since $x|a \\sim \\mathcal{N}(\\phi a, \\sigma_x^2)$ and $y|x \\sim \\mathcal{N}(x, \\sigma_y^2)$, we can view $y$ as $y=x+\\epsilon_y$ with $\\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2)$. Given $a$, $x$ is a random variable, so the random variable for the observation $Y$ is the sum of two independent random variables (one being $X$ and the other $\\epsilon_y$), and thus is also Gaussian. Its mean is $E[Y|a] = E[X|a] = \\phi a$ and its variance is $\\text{Var}(Y|a) = \\text{Var}(X|a) + \\text{Var}(\\epsilon_y) = \\sigma_x^2 + \\sigma_y^2$.\nTherefore, $p(y|a) = \\mathcal{N}(y; \\phi a, \\sigma_x^2+\\sigma_y^2)$.\nThe expected weight is the value of this PDF at $y$:\n$$\nE[w_{\\text{boot}}(x)] = \\frac{1}{\\sqrt{2\\pi(\\sigma_x^2+\\sigma_y^2)}} \\exp\\left(-\\frac{(y-\\phi a)^2}{2(\\sigma_x^2+\\sigma_y^2)}\\right)\n$$\nThe square of the expected weight is:\n$$\n(E[w_{\\text{boot}}(x)])^2 = \\frac{1}{2\\pi(\\sigma_x^2+\\sigma_y^2)} \\exp\\left(-\\frac{(y-\\phi a)^2}{\\sigma_x^2+\\sigma_y^2}\\right)\n$$\n\nNext, we compute the second moment, $E[w_{\\text{boot}}(x)^2]$.\n$$\nE[w_{\\text{boot}}(x)^2] = \\int_{-\\infty}^{\\infty} [w_{\\text{boot}}(x)]^2 q_{\\text{boot}}(x) dx = \\int_{-\\infty}^{\\infty} [p(y|x)]^2 p(x|a) dx\n$$\nWe have $[p(y|x)]^2 = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left(-\\frac{(y-x)^2}{2\\sigma_y^2}\\right)\\right)^2 = \\frac{1}{2\\pi\\sigma_y^2} \\exp\\left(-\\frac{(y-x)^2}{\\sigma_y^2}\\right)$.\nThe proposal is $p(x|a) = \\frac{1}{\\sqrt{2\\pi\\sigma_x^2}} \\exp\\left(-\\frac{(x-\\phi a)^2}{2\\sigma_x^2}\\right)$.\nThe integral for the second moment is:\n$$\nE[w_{\\text{boot}}(x)^2] = \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi\\sigma_y^2} \\exp\\left(-\\frac{(x-y)^2}{\\sigma_y^2}\\right) \\frac{1}{\\sqrt{2\\pi\\sigma_x^2}} \\exp\\left(-\\frac{(x-\\phi a)^2}{2\\sigma_x^2}\\right) dx\n$$\nThe integrand is a product of two Gaussian-like functions of $x$. The exponent is a quadratic in $x$:\n$$\n-\\left( \\frac{(x-y)^2}{\\sigma_y^2} + \\frac{(x-\\phi a)^2}{2\\sigma_x^2} \\right) = - \\left( x^2\\left(\\frac{1}{\\sigma_y^2}+\\frac{1}{2\\sigma_x^2}\\right) - 2x\\left(\\frac{y}{\\sigma_y^2}+\\frac{\\phi a}{2\\sigma_x^2}\\right) + \\left(\\frac{y^2}{\\sigma_y^2}+\\frac{(\\phi a)^2}{2\\sigma_x^2}\\right) \\right)\n$$\nThis is an unnormalized Gaussian form. Completing the square for $x$ in the exponent, we find that the integral of $\\exp(-\\text{quadratic})$ is a standard result. The calculation yields:\n$$\n\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(x-y)^2}{\\sigma_y^2}\\right) \\exp\\left(-\\frac{(x-\\phi a)^2}{2\\sigma_x^2}\\right) dx = \\sqrt{\\frac{2\\pi\\sigma_x^2\\sigma_y^2}{2\\sigma_x^2+\\sigma_y^2}} \\exp\\left(-\\frac{(y-\\phi a)^2}{2\\sigma_x^2+\\sigma_y^2}\\right)\n$$\nSubstituting this back into the expression for $E[w_{\\text{boot}}(x)^2]$:\n$$\nE[w_{\\text{boot}}(x)^2] = \\frac{1}{2\\pi\\sigma_y^2 \\sqrt{2\\pi\\sigma_x^2}} \\left( \\sqrt{\\frac{2\\pi\\sigma_x^2\\sigma_y^2}{2\\sigma_x^2+\\sigma_y^2}} \\exp\\left(-\\frac{(y-\\phi a)^2}{2\\sigma_x^2+\\sigma_y^2}\\right) \\right)\n$$\nSimplifying the pre-factors:\n$$\n\\frac{1}{2\\pi\\sigma_y^2 \\sqrt{2\\pi}\\sigma_x} \\left( \\sqrt{2\\pi} \\frac{\\sigma_x\\sigma_y}{\\sqrt{2\\sigma_x^2+\\sigma_y^2}} \\right) = \\frac{\\sigma_x\\sigma_y}{2\\pi\\sigma_y^2\\sigma_x\\sqrt{2\\sigma_x^2+\\sigma_y^2}} = \\frac{1}{2\\pi\\sigma_y\\sqrt{2\\sigma_x^2+\\sigma_y^2}}\n$$\nSo, the second moment is:\n$$\nE[w_{\\text{boot}}(x)^2] = \\frac{1}{2\\pi\\sigma_y\\sqrt{2\\sigma_x^2+\\sigma_y^2}} \\exp\\left(-\\frac{(y-\\phi a)^2}{2\\sigma_x^2+\\sigma_y^2}\\right)\n$$\nFinally, the variance is found by subtracting the squared mean from the second moment:\n$$\n\\text{Var}(w_{\\text{boot}}) = E[w_{\\text{boot}}^2] - (E[w_{\\text{boot}}])^2\n$$\n$$\n\\text{Var}(w_{\\text{boot}}) = \\frac{1}{2\\pi\\sigma_y\\sqrt{2\\sigma_x^2+\\sigma_y^2}} \\exp\\left(-\\frac{(y-\\phi a)^2}{2\\sigma_x^2+\\sigma_y^2}\\right) - \\frac{1}{2\\pi(\\sigma_x^2+\\sigma_y^2)} \\exp\\left(-\\frac{(y-\\phi a)^2}{\\sigma_x^2+\\sigma_y^2}\\right)\n$$\nThis is the closed-form analytic expression for the variance of the incremental importance weight under the bootstrap proposal.",
            "answer": "$$ \\boxed{ \\frac{1}{2\\pi\\sigma_y\\sqrt{2\\sigma_x^2+\\sigma_y^2}} \\exp\\left(-\\frac{(y-\\phi a)^2}{2\\sigma_x^2+\\sigma_y^2}\\right) - \\frac{1}{2\\pi(\\sigma_x^2+\\sigma_y^2)} \\exp\\left(-\\frac{(y-\\phi a)^2}{\\sigma_x^2+\\sigma_y^2}\\right) } $$"
        },
        {
            "introduction": "While the particle filter provides an unbiased estimate of the likelihood, $p_{\\theta}(y)$, the PMMH algorithm operates on the logarithm of this estimate, $\\log \\hat{p}(y \\mid \\theta, U)$. The randomness in the estimate introduces noise into the Metropolis-Hastings acceptance ratio, directly impacting sampler efficiency. This practice explores that crucial link by having you derive the celebrated formula for the average acceptance rate under a Gaussian noise approximation . Completing this derivation will provide a clear, quantitative understanding of how the variance of the log-likelihood estimator, $\\sigma^2$, governs the performance of the entire pseudo-marginal chain.",
            "id": "3327363",
            "problem": "Consider a Particle Markov chain Monte Carlo (PMCMC) algorithm using the particle marginal Metropolis–Hastings (PMMH) method to sample a parameter $\\theta$ from a posterior distribution with unnormalized density $\\pi(\\theta) p_{\\theta}(y)$, where $p_{\\theta}(y)$ is an intractable likelihood estimated unbiasedly by a particle filter. Let $\\hat{p}(y \\mid \\theta, U)$ denote the unbiased likelihood estimator obtained from the particle filter using random seeds $U$. Define the log-likelihood noise by $Z(\\theta, U) = \\log \\hat{p}(y \\mid \\theta, U) - \\log p_{\\theta}(y)$. Assume the following:\n- The estimator is unbiased on the likelihood scale, that is, $\\mathbb{E}[\\hat{p}(y \\mid \\theta, U) \\mid \\theta] = p_{\\theta}(y)$, which implies $\\mathbb{E}[\\exp(Z(\\theta, U)) \\mid \\theta] = 1$.\n- For each fixed $\\theta$, the distribution of $Z(\\theta, U)$ is approximately Gaussian with variance $\\sigma^{2}$, and (by the unbiasedness constraint) mean $-\\sigma^{2}/2$.\n- The proposal for $\\theta$ in the Metropolis–Hastings step is such that, conditionally on $\\theta$ and the proposed $\\theta'$, the exact Metropolis–Hastings ratio based on $\\pi(\\theta) p_{\\theta}(y)$ and the proposal kernel equals $1$. This isolates the effect of likelihood estimation noise on acceptance.\n- The particle filter is run with independent randomness at the current and proposed states, so that the corresponding log-noises are independent given $\\theta$ and $\\theta'$.\n- The Markov chain is at stationarity on the extended PMMH target over $(\\theta, U)$.\n\nUnder these conditions, the PMMH acceptance probability for a proposed move simplifies to\n$$\n\\alpha = \\min\\left\\{ 1, \\exp\\left(Z' - Z \\right) \\right\\}\n$$\nwhere $Z' = Z(\\theta', U')$ is the proposed log-noise and $Z = Z(\\theta, U)$ is the current log-noise.\n\nStarting from the above fundamental definitions, derive a closed-form analytic expression for the average acceptance probability $\\mathbb{E}[\\alpha]$ as a function of the noise variance $\\sigma^{2}$, where the expectation is taken jointly over the stationary distribution of the current log-noise and the proposal distribution of the new log-noise. You may express your answer in terms of the standard normal cumulative distribution function $\\Phi(\\cdot)$. Your final answer must be a single closed-form analytic expression depending only on $\\sigma^{2}$. No numerical approximation is required, and no units are involved. Express the final answer in exact form (no rounding). Also briefly indicate, in words or symbols, how the average acceptance depends on $\\sigma^{2}$.",
            "solution": "The objective is to derive a closed-form analytic expression for the average acceptance probability, $\\mathbb{E}[\\alpha]$, of a particle marginal Metropolis-Hastings (PMMH) move under a specific set of simplifying assumptions. The acceptance probability is given by $\\alpha = \\min\\left\\{ 1, \\exp\\left(Z' - Z \\right) \\right\\}$, where $Z$ and $Z'$ are the log-likelihood noises at the current and proposed states, respectively.\n\nThe core of the derivation lies in correctly identifying the probability distributions for the current noise $Z$ and the proposed noise $Z'$. The problem states that for any given parameter $\\theta$, the log-likelihood noise $Z(\\theta, U) = \\log \\hat{p}(y \\mid \\theta, U) - \\log p_{\\theta}(y)$ is approximately Gaussian. The condition that the likelihood estimator is unbiased, $\\mathbb{E}[\\exp(Z(\\theta, U)) \\mid \\theta] = 1$, implies that if $Z$ follows a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, its mean must be $\\mu = -\\sigma^2/2$. Thus, the distribution of log-noise generated by the particle filter for a given $\\theta$ is $q(z) = \\mathcal{N}(-\\frac{\\sigma^2}{2}, \\sigma^2)$.\n\nThe proposed log-noise, $Z' = Z(\\theta', U')$, is generated by running the particle filter with a new set of random numbers $U'$ for the proposed parameter $\\theta'$. Therefore, the distribution of $Z'$ is the one specified above:\n$$\nZ' \\sim \\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\n$$\n\nThe current log-noise, $Z$, is a component of the PMMH chain which is assumed to be at stationarity. The stationary distribution of the PMMH algorithm, on the extended space of parameters and auxiliary random variables $(\\theta, U)$, has a density proportional to the target $\\pi(\\theta) p_{\\theta}(y)$ times the proposal for the auxiliary variables. In our case, this corresponds to $\\pi(\\theta) \\hat{p}(y|\\theta, U)$. We can express this in terms of the log-noise $Z$:\n$$\n\\pi_{\\text{stationary}}(\\theta, Z) \\propto \\pi(\\theta) p_{\\theta}(y) \\exp(Z) q(Z \\mid \\theta)\n$$\nwhere $q(Z \\mid \\theta)$ is the density of $\\mathcal{N}(-\\sigma^2/2, \\sigma^2)$, which is assumed to be independent of $\\theta$. The marginal stationary distribution for $Z$, let's call its density $\\pi(z)$, is obtained by integrating over $\\theta$. Crucially, it is proportional to the original noise distribution $q(z)$ \"tilted\" by the factor $\\exp(z)$:\n$$\n\\pi(z) \\propto q(z) \\exp(z)\n$$\nLet's find the form of this distribution. Substituting the density for $q(z)$:\n$$\n\\pi(z) \\propto \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z - (-\\sigma^2/2))^2}{2\\sigma^2}\\right) \\exp(z)\n$$\n$$\n\\pi(z) \\propto \\exp\\left(-\\frac{(z + \\sigma^2/2)^2}{2\\sigma^2} + z\\right)\n$$\nWe complete the square for the term in the exponent:\n$$\n-\\frac{z^2 + z\\sigma^2 + \\sigma^4/4}{2\\sigma^2} + z = \\frac{-z^2 - z\\sigma^2 - \\sigma^4/4 + 2z\\sigma^2}{2\\sigma^2} = \\frac{-z^2 + z\\sigma^2 - \\sigma^4/4}{2\\sigma^2}\n$$\n$$\n= -\\frac{(z^2 - z\\sigma^2 + \\sigma^4/4)}{2\\sigma^2} = -\\frac{(z - \\sigma^2/2)^2}{2\\sigma^2}\n$$\nThis is the kernel of a Gaussian distribution with mean $\\sigma^2/2$ and variance $\\sigma^2$. Therefore, the stationary distribution of the current log-noise $Z$ is:\n$$\nZ \\sim \\mathcal{N}\\left(\\frac{\\sigma^2}{2}, \\sigma^2\\right)\n$$\n\nWe now define the difference $\\Delta Z = Z' - Z$. Since $Z'$ and $Z$ are independent, $\\Delta Z$ is also normally distributed. Its mean and variance are:\n$$\n\\mathbb{E}[\\Delta Z] = \\mathbb{E}[Z'] - \\mathbb{E}[Z] = \\left(-\\frac{\\sigma^2}{2}\\right) - \\left(\\frac{\\sigma^2}{2}\\right) = -\\sigma^2\n$$\n$$\n\\text{Var}(\\Delta Z) = \\text{Var}(Z') + \\text{Var}(Z) = \\sigma^2 + \\sigma^2 = 2\\sigma^2\n$$\nSo, $\\Delta Z \\sim \\mathcal{N}(-\\sigma^2, 2\\sigma^2)$.\n\nThe average acceptance probability is the expectation of $\\alpha = \\min\\{1, \\exp(\\Delta Z)\\}$. Let $f_{\\Delta Z}(x)$ be the probability density function of $\\Delta Z$. The expectation is:\n$$\n\\mathbb{E}[\\alpha] = \\int_{-\\infty}^{\\infty} \\min\\{1, \\exp(x)\\} f_{\\Delta Z}(x) \\, dx\n$$\nWe split the integral into two parts based on the value of $x$:\n$$\n\\mathbb{E}[\\alpha] = \\int_{-\\infty}^{0} \\exp(x) f_{\\Delta Z}(x) \\, dx + \\int_{0}^{\\infty} 1 \\cdot f_{\\Delta Z}(x) \\, dx\n$$\nThe second integral is simply the probability $P(\\Delta Z > 0)$. Let $S$ be a standard normal random variable, $S \\sim \\mathcal{N}(0, 1)$.\n$$\nP(\\Delta Z > 0) = P\\left(\\frac{\\Delta Z - (-\\sigma^2)}{\\sqrt{2\\sigma^2}} > \\frac{0 - (-\\sigma^2)}{\\sqrt{2\\sigma^2}}\\right) = P\\left(S > \\frac{\\sigma^2}{\\sqrt{2\\sigma^2}}\\right) = P\\left(S > \\frac{\\sigma}{\\sqrt{2}}\\right)\n$$\nUsing the standard normal cumulative distribution function $\\Phi(\\cdot)$, this probability is $1 - \\Phi(\\sigma/\\sqrt{2}) = \\Phi(-\\sigma/\\sqrt{2})$.\n\nThe first integral is $\\int_{-\\infty}^{0} \\exp(x) f_{\\Delta Z}(x) \\, dx$.\nLet $X = \\Delta Z \\sim \\mathcal{N}(\\mu, v)$ with $\\mu = -\\sigma^2$ and $v = 2\\sigma^2$. The integral is $\\mathbb{E}[\\exp(X) I(X \\le 0)]$.\nWe know that for a normally distributed variable $X$, $\\exp(x) f_X(x) = \\exp(\\mu + v/2) f_Y(x)$, where $Y \\sim \\mathcal{N}(\\mu+v, v)$.\nLet's compute the pre-factor $\\exp(\\mu + v/2)$:\n$$\n\\exp\\left(-\\sigma^2 + \\frac{2\\sigma^2}{2}\\right) = \\exp(-\\sigma^2 + \\sigma^2) = \\exp(0) = 1\n$$\nThe new mean is $\\mu+v = -\\sigma^2 + 2\\sigma^2 = \\sigma^2$. So $Y \\sim \\mathcal{N}(\\sigma^2, 2\\sigma^2)$.\nThe first integral becomes:\n$$\n\\int_{-\\infty}^{0} 1 \\cdot f_Y(x) \\, dx = P(Y \\le 0) = P\\left(\\frac{Y - \\sigma^2}{\\sqrt{2\\sigma^2}} \\le \\frac{0 - \\sigma^2}{\\sqrt{2\\sigma^2}}\\right)\n$$\n$$\n= P\\left(S \\le -\\frac{\\sigma^2}{\\sqrt{2\\sigma^2}}\\right) = P\\left(S \\le -\\frac{\\sigma}{\\sqrt{2}}\\right) = \\Phi\\left(-\\frac{\\sigma}{\\sqrt{2}}\\right)\n$$\n\nCombining the two results:\n$$\n\\mathbb{E}[\\alpha] = \\Phi\\left(-\\frac{\\sigma}{\\sqrt{2}}\\right) + \\Phi\\left(-\\frac{\\sigma}{\\sqrt{2}}\\right) = 2\\Phi\\left(-\\frac{\\sigma}{\\sqrt{2}}\\right)\n$$\nTo express this as a function of $\\sigma^2$ as requested:\n$$\n\\mathbb{E}[\\alpha] = 2\\Phi\\left(-\\sqrt{\\frac{\\sigma^2}{2}}\\right)\n$$\n\nThe average acceptance probability $\\mathbb{E}[\\alpha]$ is a monotonically decreasing function of the noise variance $\\sigma^2$. As $\\sigma^2 \\to 0$, the argument of $\\Phi$ goes to $0$, so $\\mathbb{E}[\\alpha] \\to 2\\Phi(0) = 2(1/2) = 1$, which corresponds to the case with no estimation noise. As $\\sigma^2 \\to \\infty$, the argument goes to $-\\infty$, so $\\mathbb{E}[\\alpha] \\to 2\\Phi(-\\infty) = 0$, meaning the acceptance rate collapses for very noisy likelihood estimates.",
            "answer": "$$\n\\boxed{2\\Phi\\left(-\\sqrt{\\frac{\\sigma^2}{2}}\\right)}\n$$"
        },
        {
            "introduction": "The previous exercises established that the variance of the log-likelihood estimator, which is controlled by the number of particles $N$, is a critical parameter for PMMH efficiency. A variance that is too high causes the acceptance rate to collapse, while one that is too low (using an excessively large $N$) wastes computational resources. This final practice moves from theory to application, asking you to evaluate and select statistically sound procedures for tuning $N$ to achieve a target variance . This is an essential skill for any practitioner aiming to implement PMMH effectively.",
            "id": "3327322",
            "problem": "Consider a latent state-space model with latent path $\\{x_{t}\\}_{t=1}^{T}$, observations $\\{y_{t}\\}_{t=1}^{T}$, and parameter $\\theta \\in \\Theta$. Let $Z(\\theta) = p(y_{1:T} \\mid \\theta)$ denote the marginal likelihood. In particle marginal Metropolis-Hastings (PMMH), a variant of Markov chain Monte Carlo (MCMC) that leverages a particle filter (PF), one replaces $Z(\\theta)$ by an unbiased estimator $\\hat{Z}_{N}(\\theta)$ produced by a sequential Monte Carlo (SMC) method with $N$ particles. It is well established in pseudo-marginal theory that the efficiency of PMMH depends on the variance of the log-likelihood estimator noise, and a common calibration objective is to choose $N$ such that $\\operatorname{Var}[\\log \\hat{Z}_{N}(\\theta)] \\approx \\sigma^{2}_{\\star}$ for a user-specified target $\\sigma^{2}_{\\star} > 0$. \n\nFrom the fundamental definitions of unbiased estimation and the Central Limit Theorem (CLT) for particle approximations, it is known that for fixed $\\theta$ and $T$, as $N \\to \\infty$, the distribution of $\\log \\hat{Z}_{N}(\\theta)$ concentrates and its variance typically decays on the order of $1/N$. Your task is to identify scientifically sound procedures to (i) select $N$ by targeting $\\operatorname{Var}[\\log \\hat{Z}_{N}(\\theta)] \\approx \\sigma^{2}_{\\star}$ and (ii) estimate $\\operatorname{Var}[\\log \\hat{Z}_{N}(\\theta)]$ using pilot computations or independent PF replicates, while respecting the requirement that the PMMH chain ultimately runs with a fixed $N$.\n\nWhich of the following procedures are valid and justified ways to meet the target variance objective and to estimate the variance from pilot runs or independent PF replicates?\n\nA. Fix a representative parameter value $\\theta_{\\mathrm{ref}}$ (for example, a posterior mode approximation or a maximum likelihood estimate from a preliminary optimization). For a grid of particle numbers $\\{N_{j}\\}_{j=1}^{J}$, and for each $N_{j}$, run $R$ independent PF replications at $\\theta_{\\mathrm{ref}}$ to obtain independent draws $\\{\\log \\hat{Z}_{N_{j}}^{(r)}(\\theta_{\\mathrm{ref}})\\}_{r=1}^{R}$. Compute the sample variance $s^{2}_{N_{j}}$ of these $R$ values for each $N_{j}$. Fit the relationship $s^{2}_{N_{j}} \\approx c(\\theta_{\\mathrm{ref}})/N_{j}$ by regressing $s^{2}_{N_{j}}$ on $1/N_{j}$ to estimate $c(\\theta_{\\mathrm{ref}})$. Choose $N_{\\star} = \\lceil c(\\theta_{\\mathrm{ref}})/\\sigma^{2}_{\\star} \\rceil$ and use this fixed $N_{\\star}$ for the main PMMH run.\n\nB. Run a short pilot PMMH at a provisional $N_{0}$ with a fixed proposal. At each visited $\\theta_{t}$ in the pilot, generate $K \\ge 2$ independent PF replications to obtain $\\{\\log \\hat{Z}_{N_{0}}^{(k)}(\\theta_{t})\\}_{k=1}^{K}$ and compute the per-iteration sample variance $\\hat{v}_{t}$. Average these variances over the pilot path to estimate the typical variance across the posterior region, $\\bar{v} = \\frac{1}{T_{\\mathrm{pilot}}}\\sum_{t=1}^{T_{\\mathrm{pilot}}} \\hat{v}_{t}$. Set $N_{\\star} = \\lceil N_{0} \\, \\bar{v} / \\sigma^{2}_{\\star} \\rceil$ and then fix $N_{\\star}$ for the production PMMH.\n\nC. For a single PF run at a chosen $N$, record the incremental log weights and compute the within-filter sample variance of these increments across particles and time to form a single-run proxy for $\\operatorname{Var}[\\log \\hat{Z}_{N}(\\theta)]$. Tune $N$ so that this single-run proxy numerically matches $\\sigma^{2}_{\\star}$, without using any independent PF replicates.\n\nD. Start with a small $N$ and run PMMH while adaptively increasing or decreasing $N$ during the chain to target an average acceptance rate of approximately $0.234$. Continue adapting $N$ on-the-fly throughout the main PMMH run to stabilize the acceptance rate near $0.234$.\n\nE. For a given $N$, compute the effective sample size (ESS) of the PF at each time step from a single run and choose $N$ large enough that the minimum ESS across time exceeds a fixed fraction of $N$ (for example, at least $0.5 N$). Conclude that this guarantees $\\operatorname{Var}[\\log \\hat{Z}_{N}(\\theta)] \\approx \\sigma^{2}_{\\star}$ and proceed with the resulting $N$.\n\nSelect all that apply.",
            "solution": "The problem statement is scientifically sound, well-posed, and objective. It accurately describes the setup of the Particle Marginal Metropolis-Hastings (PMMH) algorithm and the challenge of calibrating the number of particles $N$. The core concepts—unbiased likelihood estimation via a particle filter (PF), the resulting PMMH algorithm, and the relationship between the variance of the log-likelihood estimator and MCMC efficiency—are all standard and correct within the field of computational statistics. The asymptotic decay rate of the variance, $\\operatorname{Var}[\\log \\hat{Z}_{N}(\\theta)] \\propto 1/N$, is a known theoretical result. The problem is valid.\n\nThe central task is to find a principled way to choose a fixed number of particles, $N$, such that the variance of the log-likelihood estimator, $\\operatorname{Var}[\\log \\hat{Z}_{N}(\\theta)]$, is approximately equal to a target value $\\sigma^2_{\\star}$. The log-likelihood estimate $\\log \\hat{Z}_{N}(\\theta)$ is a random variable, where the randomness comes from the Monte Carlo procedure of the particle filter. To estimate the variance of any random variable, it is a fundamental statistical requirement to obtain multiple independent samples of that variable. Therefore, any valid procedure must involve running the particle filter multiple independent times for a given parameter value $\\theta$. Let these independent estimates be denoted by $\\{\\hat{Z}_{N}^{(r)}(\\theta)\\}_{r=1}^R$. The variance can then be estimated using the sample variance of the log-transformed values, $\\{\\log \\hat{Z}_{N}^{(r)}(\\theta)\\}_{r=1}^R$.\n\nAnother key principle is the asymptotic relationship $\\operatorname{Var}[\\log \\hat{Z}_{N}(\\theta)] \\approx c(\\theta)/N$ for large $N$, where $c(\\theta)$ is a constant that depends on the parameter $\\theta$ and the model structure, but not on $N$. This scaling law allows one to extrapolate from a pilot variance estimate obtained with $N_0$ particles to the required number of particles $N_{\\star}$ to achieve a target variance $\\sigma^2_{\\star}$:\n$$ \\sigma^2_{\\star} \\approx \\operatorname{Var}[\\log \\hat{Z}_{N_{\\star}}(\\theta)] \\approx \\frac{c(\\theta)}{N_{\\star}} $$\nIf we have an estimate of the variance $v_{N_0} \\approx \\operatorname{Var}[\\log \\hat{Z}_{N_0}(\\theta)]$ from a pilot run with $N_0$ particles, we can write $v_{N_0} \\approx c(\\theta)/N_0$. Combining these gives a rule for choosing $N_{\\star}$:\n$$ \\frac{\\sigma^2_{\\star}}{v_{N_0}} \\approx \\frac{N_0}{N_{\\star}} \\implies N_{\\star} \\approx N_0 \\frac{v_{N_0}}{\\sigma^2_{\\star}} $$\nFinally, for the PMMH algorithm to target the correct posterior distribution $p(\\theta \\mid y_{1:T})$, the number of particles $N$ must be fixed throughout the main MCMC run. Any adaptation of $N$ must be confined to a preliminary pilot or tuning phase.\n\nWe now evaluate each option against these principles.\n\nA. This procedure is statistically sound and follows established best practices.\n1. It correctly recognizes that to estimate $\\operatorname{Var}[\\log \\hat{Z}_{N_{j}}(\\theta_{\\mathrm{ref}})]$, one must generate multiple independent realizations of the estimator, which is achieved by running $R$ independent PF replications.\n2. It uses these replicates to compute a sample variance $s^{2}_{N_{j}}$, a valid estimator for the true variance.\n3. It robustly estimates the scaling constant $c(\\theta_{\\mathrm{ref}})$ by using a grid of particle numbers $\\{N_j\\}$ and fitting the theoretical relationship $s^{2}_{N_j} \\approx c(\\theta_{\\mathrm{ref}})/N_{j}$. This is more stable than relying on a single pair $(N_j, s^2_{N_j})$.\n4. It uses this estimated relationship to solve for the number of particles $N_{\\star}$ that should achieve the target variance $\\sigma^2_{\\star}$, namely $N_{\\star} = \\lceil c(\\theta_{\\mathrm{ref}})/\\sigma^{2}_{\\star} \\rceil$.\n5. It correctly specifies that this value $N_{\\star}$ is then fixed for the main PMMH run, ensuring the validity of the resulting MCMC chain.\nThe use of a representative parameter $\\theta_{\\mathrm{ref}}$ is a practical choice to perform this calibration, assuming the variance does not change drastically across the high-posterior density region.\nVerdict: **Correct**.\n\nB. This procedure is also statistically sound and represents a more sophisticated version of the method in A.\n1. Like A, it correctly uses $K \\ge 2$ independent PF replications at each parameter value $\\theta_t$ to obtain a valid estimate of the variance, $\\hat{v}_t = \\operatorname{Var}[\\log \\hat{Z}_{N_0}(\\theta_t)]$.\n2. Instead of using a single point estimate $\\theta_{\\mathrm{ref}}$, it runs a pilot PMMH chain to explore the region of high posterior probability. This is advantageous because the variance of the estimator can be parameter-dependent. By averaging the variance estimates $\\hat{v}_t$ over the states $\\{\\theta_t\\}$ visited by the pilot chain, it computes an estimate $\\bar{v}$ that is representative of the variance encountered during a typical PMMH run.\n3. It then uses the scaling law to determine the target number of particles: $N_{\\star} = \\lceil N_{0} \\, \\bar{v} / \\sigma^{2}_{\\star} \\rceil$. This is a direct application of the extrapolation formula derived above.\n4. Critically, it specifies that this calibration is done in a pilot phase, and the resulting $N_{\\star}$ is then fixed for the production run, preserving the validity of the PMMH algorithm.\nVerdict: **Correct**.\n\nC. This procedure is fundamentally flawed. It proposes to estimate $\\operatorname{Var}[\\log \\hat{Z}_{N}(\\theta)]$ from a single run of the particle filter. The quantity of interest, $\\log \\hat{Z}_{N}(\\theta)$, is a single scalar output from one complete run of the PF. One cannot estimate the variance of a random variable from a single sample. The \"within-filter sample variance of incremental log weights\" is a measure of particle diversity at various time steps *inside* the filter. While related to filter performance, it is a different quantity from the variance of the final log-likelihood estimate *across independent runs*. This procedure conflates these two distinct concepts and thus violates the basic statistical principle of how variance is estimated.\nVerdict: **Incorrect**.\n\nD. This procedure is invalid for two critical reasons. First, the target acceptance rate of $0.234$ is derived for Random-Walk Metropolis-Hastings under specific, idealized conditions (high-dimensional Gaussian target) and does not generally apply to PMMH. The optimal acceptance rate for PMMH is a function of the log-likelihood estimator variance and is typically lower. Second, and more importantly, it proposes to \"continue adapting $N$ on-the-fly throughout the main PMMH run\". This violates the conditions for the PMMH algorithm to be a valid pseudo-marginal scheme. An adaptive $N$ that depends on the history of the chain means the transition kernel is not stationary and does not satisfy detailed balance with respect to the target posterior distribution. The resulting chain would not sample from the correct posterior. This directly contradicts the problem's constraint and the theoretical requirements of PMMH.\nVerdict: **Incorrect**.\n\nE. This procedure uses a heuristic that is insufficient for the stated goal. The Effective Sample Size (ESS) is a useful diagnostic for particle degeneracy within a single PF run. A low ESS is indeed correlated with a high variance of the likelihood estimator. Therefore, ensuring ESS does not collapse is a necessary sanity check. However, there is no direct, universal mapping that guarantees a specific ESS threshold (e.g., $0.5N$) corresponds to a specific target variance $\\sigma^2_{\\star}$. The relationship is model-dependent and complex. This method provides at best a rough rule of thumb, not a principled procedure for calibrating $N$ to a specific numerical variance target. The conclusion that maintaining a certain ESS \"guarantees\" $\\operatorname{Var}[\\log \\hat{Z}_{N}(\\theta)] \\approx \\sigma^{2}_{\\star}$ is an unsubstantiated overstatement. Like C, it improperly relies on a single-run diagnostic to make a quantitative claim about a property (variance) that can only be estimated across multiple independent runs.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}