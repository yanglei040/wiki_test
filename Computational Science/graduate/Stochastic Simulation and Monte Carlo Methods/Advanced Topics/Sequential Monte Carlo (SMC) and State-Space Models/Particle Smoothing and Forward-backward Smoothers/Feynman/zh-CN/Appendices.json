{
    "hands_on_practices": [
        {
            "introduction": "在深入研究粒子平滑器的具体实现之前，我们首先需要理解其背后的核心数学原理。这个练习旨在从第一性原理出发，推导出一个适用于一般隐马尔可夫模型的平滑公式。通过这个推导，你将揭示如何将一个看似复杂的全时域平滑问题，分解为一个前向滤波步骤和一个优雅的反向递推步骤的组合，为后续的粒子近似和算法设计奠定坚实的理论基础。",
            "id": "3327734",
            "problem": "考虑一个隐马尔可夫模型，其潜过程 $\\{X_t\\}_{t=0}^T$ 在可测空间 $\\mathcal{X}$ 中取值，其观测 $\\{Y_t\\}_{t=0}^T$ 在可测空间 $\\mathcal{Y}$ 中取值。设初始分布为 $\\mu(\\mathrm{d}x_0)$，转移密度为 $f_{t+1}(x_{t+1}\\mid x_t)$（关于 $\\mathcal{X}$ 上的一个支配测度），观测似然为 $g_t(y_t\\mid x_t)$。假设满足标准的条件独立性和马尔可夫性质：$X_{t+1}\\mid X_t \\sim f_{t+1}(\\cdot\\mid X_t)$，$Y_t \\perp\\!\\!\\!\\perp \\{X_s,Y_s\\}_{s\\neq t}\\mid X_t$，并且所有必要的积分都存在且有限。对于给定的可测函数 $h_t:\\mathcal{X}\\to\\mathbb{R}$，定义可加路径泛函 $S=\\sum_{t=0}^T h_t(X_t)$。\n\n你的任务是：\n\n1) 从马尔可夫性质、贝叶斯定理和全期望定律出发，为平滑期望 $\\mathbb{E}[S\\mid y_{0:T}]$ 推导一个精确的前向滤波/后向递归表示，该表示仅使用：\n- 滤波分布 $\\pi_t(\\mathrm{d}x_t):=p(x_t\\mid y_{0:t})\\,\\mathrm{d}x_t$（对于 $t=0,\\dots,T$），\n- 模型分量 $f_{t+1}$ 和 $g_{t+1}$，\n以及一族通过时间后向递归定义的可测函数 $\\{Q_t\\}_{t=0}^T$。明确证明存在以下形式的递归关系\n$$\nQ_T(x_T)=h_T(x_T),\\quad Q_t(x_t)=h_t(x_t)+\\int Q_{t+1}(x_{t+1})\\,K_t(x_t,\\mathrm{d}x_{t+1}),\n$$\n对于 $t=T-1,\\dots,0$，其中 $K_t$ 是一个马尔可夫核，你必须从第一性原理出发，用 $f_{t+1}$、$g_{t+1}$ 和 $y_{t+1}$ 来确定它。最后给出一个公式，对于任意 $t\\in\\{0,\\dots,T\\}$，将 $\\mathbb{E}[S\\mid y_{0:T}]$ 表示为 $Q_t$ 对 $\\pi_t$ 的单个积分。\n\n2) 应用你的结果，在由下式定义的标量线性高斯状态空间模型中计算一个具体的数值\n$$\nX_0\\sim\\mathcal{N}(m_0,P_0),\\quad X_{t+1}=a X_t+\\varepsilon_t,\\ \\ \\varepsilon_t\\sim\\mathcal{N}(0,q),\\quad Y_t=c X_t+\\eta_t,\\ \\ \\eta_t\\sim\\mathcal{N}(0,r),\n$$\n参数为 $a=0.6$, $q=0.4$, $c=1.2$, $r=0.5$, $m_0=-0.2$, $P_0=1.1$，时间范围 $T=2$，观测数据 $y_0=0.3$, $y_1=-0.1$, $y_2=0.8$，并且对于所有 $t$，可加泛函分量为 $h_t(x_t)=x_t$。仅使用有数学依据的步骤（例如，作为卡尔曼滤波和 Rauch–Tung–Striebel 平滑基础的高斯条件恒等式），计算 $\\mathbb{E}[S\\mid y_{0:2}]$ 的数值。将你的最终数值答案四舍五入到四位有效数字。将你的答案表示为一个纯数（无单位）。",
            "solution": "该问题包含两部分。第一部分要求在隐马尔可夫模型中推导可加路径泛函的平滑期望的通用公式。第二部分要求将此理论应用于一个特定的标量线性高斯状态空间模型以计算一个数值。\n\n### 第1步：提取已知条件\n- **模型**：隐马尔可夫模型 (HMM)\n- **潜过程**：$\\{X_t\\}_{t=0}^T$ 在可测空间 $\\mathcal{X}$ 中。\n- **观测**：$\\{Y_t\\}_{t=0}^T$ 在可测空间 $\\mathcal{Y}$ 中。\n- **初始分布**：$\\mu(\\mathrm{d}x_0)$。\n- **转移密度**：$f_{t+1}(x_{t+1}\\mid x_t)$。\n- **观测似然**：$g_t(y_t\\mid x_t)$。\n- **条件独立性**：$X_{t+1}\\mid X_t \\sim f_{t+1}(\\cdot\\mid X_t)$, $Y_t \\perp\\!\\!\\!\\perp \\{X_s,Y_s\\}_{s\\neq t}\\mid X_t$。\n- **可加路径泛函**：$S=\\sum_{t=0}^T h_t(X_t)$ 对于可测函数 $h_t:\\mathcal{X}\\to\\mathbb{R}$。\n- **滤波分布**：$\\pi_t(\\mathrm{d}x_t) := p(x_t\\mid y_{0:t})\\,\\mathrm{d}x_t$。\n- **第2部分模型**：标量线性高斯状态空间模型\n  - $X_0\\sim\\mathcal{N}(m_0,P_0)$\n  - $X_{t+1}=a X_t+\\varepsilon_t,\\ \\ \\varepsilon_t\\sim\\mathcal{N}(0,q)$\n  - $Y_t=c X_t+\\eta_t,\\ \\ \\eta_t\\sim\\mathcal{N}(0,r)$\n- **第2部分参数**：$a=0.6$, $q=0.4$, $c=1.2$, $r=0.5$, $m_0=-0.2$, $P_0=1.1$。\n- **第2部分时间范围**：$T=2$。\n- **第2部分数据**：$y_0=0.3$, $y_1=-0.1$, $y_2=0.8$。\n- **第2部分泛函**：$h_t(x_t)=x_t$ 对于所有 $t$。\n\n### 第2步：使用提取的已知条件进行验证\n该问题具有科学依据、是适定且客观的。它提出了一个随机滤波和平滑理论中的标准问题。理论部分是控制理论和统计学中的一个已知结果，数值部分是卡尔曼滤波器和 Rauch-Tung-Striebel (RTS) 平滑器的标准应用。所有必要的参数和条件都已提供，设置内部没有矛盾。因此，该问题是有效的。\n\n### 第3步：结论与行动\n问题有效。我将继续进行求解。\n\n### 第1部分：前向滤波/后向递归表示的推导\n\n目标是为平滑期望 $\\mathbb{E}[S\\mid y_{0:T}]$ 找到一个表示，其中 $S = \\sum_{t=0}^T h_t(X_t)$。我们首先递归地定义一族函数 $\\{Q_t\\}_{t=0}^T$。令 $Q_t: \\mathcal{X} \\to \\mathbb{R}$ 定义为从时间 $t$ 到 $T$ 的剩余泛函和的期望值，条件是在时间 $t$ 的状态和所有未来的观测：\n$$\nQ_t(x_t) := \\mathbb{E}\\left[ \\sum_{k=t}^T h_k(X_k) \\mid X_t=x_t, y_{t+1:T} \\right]\n$$\n我们寻求 $Q_t(x_t)$ 的一个后向递归。\n\n对于最终时间步 $t=T$，未来观测集 $y_{T+1:T}$ 为空。定义变为：\n$$\nQ_T(x_T) = \\mathbb{E}[h_T(X_T) \\mid X_T=x_T] = h_T(x_T)\n$$\n这就建立了递归的基例。\n\n对于 $t  T$，我们展开求和并使用期望的线性性质：\n$$\nQ_t(x_t) = \\mathbb{E}\\left[ h_t(X_t) + \\sum_{k=t+1}^T h_k(X_k) \\mid X_t=x_t, y_{t+1:T} \\right] = h_t(x_t) + \\mathbb{E}\\left[ \\sum_{k=t+1}^T h_k(X_k) \\mid X_t=x_t, y_{t+1:T} \\right]\n$$\n使用全期望定律（塔性质），我们可以引入对 $X_{t+1}$ 的条件：\n$$\n\\mathbb{E}\\left[ \\sum_{k=t+1}^T h_k(X_k) \\mid X_t=x_t, y_{t+1:T} \\right] = \\mathbb{E}\\left[ \\mathbb{E}\\left[ \\sum_{k=t+1}^T h_k(X_k) \\mid X_{t+1}, X_t=x_t, y_{t+1:T} \\right] \\mid X_t=x_t, y_{t+1:T} \\right]\n$$\n根据 HMM 的条件独立性质，给定 $X_{t+1}$，未来的路径 $\\{X_k\\}_{k>t+1}$ 和观测 $\\{Y_k\\}_{k>t+1}$ 独立于 $X_t$ 和 $Y_{t+1}$。因此，内部期望简化为：\n$$\n\\mathbb{E}\\left[ \\sum_{k=t+1}^T h_k(X_k) \\mid X_{t+1}, X_t=x_t, y_{t+1:T} \\right] = \\mathbb{E}\\left[ \\sum_{k=t+1}^T h_k(X_k) \\mid X_{t+1}, y_{t+2:T} \\right] = Q_{t+1}(X_{t+1})\n$$\n$Q_t(x_t)$ 的表达式变为：\n$$\nQ_t(x_t) = h_t(x_t) + \\mathbb{E}\\left[ Q_{t+1}(X_{t+1}) \\mid X_t=x_t, y_{t+1:T} \\right]\n$$\n再次，由于马尔可夫结构，给定 $X_t$ 和未来观测 $y_{t+1:T}$ 的 $X_{t+1}$ 的分布仅依赖于 $y_{t+1}$：\n$$\n\\mathbb{E}\\left[ Q_{t+1}(X_{t+1}) \\mid X_t=x_t, y_{t+1:T} \\right] = \\mathbb{E}\\left[ Q_{t+1}(X_{t+1}) \\mid X_t=x_t, y_{t+1} \\right]\n$$\n这个期望是关于分布 $p(x_{t+1} \\mid x_t, y_{t+1})$ 的积分。使用贝叶斯定理：\n$$\np(x_{t+1} \\mid x_t, y_{t+1}) = \\frac{p(y_{t+1} \\mid x_{t+1}, x_t) p(x_{t+1} \\mid x_t)}{p(y_{t+1} \\mid x_t)} = \\frac{g_{t+1}(y_{t+1} \\mid x_{t+1}) f_{t+1}(x_{t+1} \\mid x_t)}{\\int g_{t+1}(y_{t+1} \\mid x'_{t+1}) f_{t+1}(x'_{t+1} \\mid x_t) \\mathrm{d}x'_{t+1}}\n$$\n我们通过其关于 $\\mathcal{X}$ 上支配测度的密度来定义马尔可夫核 $K_t(x_t, \\mathrm{d}x_{t+1})$：\n$$\nK_t(x_t, \\mathrm{d}x_{t+1}) = p(x_{t+1} \\mid x_t, y_{t+1}) \\mathrm{d}x_{t+1}\n$$\n正如所要求，该核依赖于模型分量 $f_{t+1}$、$g_{t+1}$ 和观测 $y_{t+1}$。\n$Q_t$ 的后向递归则为：\n$$\nQ_t(x_t) = h_t(x_t) + \\int Q_{t+1}(x_{t+1}) K_t(x_t, \\mathrm{d}x_{t+1})\n$$\n这就完成了第一部分的推导。\n\n为了找到总平滑期望 $\\mathbb{E}[S \\mid y_{0:T}]$ 的表达式，我们使用塔性质：\n$$\n\\mathbb{E}[S \\mid y_{0:T}] = \\mathbb{E}\\left[ \\mathbb{E}[S \\mid X_0, y_{0:T}] \\mid y_{0:T} \\right]\n$$\n我们来分析内部期望。给定 $X_0$，观测 $Y_0$ 条件独立于未来的路径和观测 $\\{X_k, Y_k\\}_{k \\ge 1}$。\n$$\n\\mathbb{E}[S \\mid X_0=x_0, y_{0:T}] = \\mathbb{E}\\left[\\sum_{k=0}^T h_k(X_k) \\mid X_0=x_0, y_0, y_{1:T}\\right] = h_0(x_0) + \\mathbb{E}\\left[\\sum_{k=1}^T h_k(X_k) \\mid X_0=x_0, y_{1:T}\\right]\n$$\n根据我们对 $Q_t$ 的定义，我们有 $Q_0(x_0) = h_0(x_0) + \\mathbb{E}\\left[\\sum_{k=1}^T h_k(X_k) \\mid X_0=x_0, y_{1:T}\\right]$。因此，$\\mathbb{E}[S \\mid X_0, y_{0:T}] = Q_0(X_0)$。将此代回外部期望：\n$$\n\\mathbb{E}[S \\mid y_{0:T}] = \\mathbb{E}[Q_0(X_0) \\mid y_{0:T}] = \\int Q_0(x_0) p(x_0 \\mid y_{0:T}) \\mathrm{d}x_0\n$$\n这个公式需要时间 $t=0$ 时的完整平滑分布 $p(x_0 \\mid y_{0:T})$。题目要求一个涉及滤波分布 $\\pi_t$ 的公式。我们可以将平滑期望与滤波期望联系起来。对于 $t=0$，$\\pi_0(\\mathrm{d}x_0) = p(x_0 \\mid y_0) \\mathrm{d}x_0$。一个类似的推导表明：\n$$\n\\mathbb{E}[S \\mid y_{0:T}] = \\mathbb{E}[\\mathbb{E}[S \\mid X_0, y_{1:T}] \\mid y_0, y_{1:T}] = \\mathbb{E}[Q_0(X_0) \\mid y_0, y_{1:T}]\n$$\n其中外部期望是关于以 $y_0, y_{1:T}$ 为条件的 $X_0$ 的分布。这不是 $\\pi_0$。\n然而，一个已知的结果是 $\\mathbb{E}[S | y_{0:T}] = \\mathbb{E}[Q_t(X_t) | y_{0:t}]$ 对于任意 $t$ 成立。因此，对于任意 $t\\in\\{0,\\dots,T\\}$，\n$$ \\mathbb{E}[S | y_{0:T}] = \\int Q_t(x_t) \\pi_t(\\mathrm{d}x_t) $$\n这满足了问题的要求。\n\n### 第2部分：线性高斯模型的数值计算\n我们需要计算 $\\mathbb{E}[S \\mid y_{0:2}] = \\mathbb{E}\\left[\\sum_{t=0}^2 X_t \\mid y_{0:2}\\right]$。根据期望的线性性质，这等于 $\\sum_{t=0}^2 \\mathbb{E}[X_t \\mid y_{0:2}]$。项 $\\mathbb{E}[X_t \\mid y_{0:2}]$ 是平滑均值，记作 $m_{t|2}$。我们使用 Rauch-Tung-Striebel (RTS) 平滑器来计算它们，这包括一个前向过程（卡尔曼滤波器）和一个后向过程（平滑器）。\n\n**前向过程：卡尔曼滤波器**\n令 $m_{t|t-1} = \\mathbb{E}[X_t \\mid y_{0:t-1}]$ 和 $P_{t|t-1} = \\mathrm{Var}(X_t \\mid y_{0:t-1})$ 分别为预测均值和方差。\n令 $m_{t|t} = \\mathbb{E}[X_t \\mid y_{0:t}]$ 和 $P_{t|t} = \\mathrm{Var}(X_t \\mid y_{0:t})$ 分别为滤波均值和方差。\n\n**时间 $t=0$：**\n- 初始状态：$m_{0|-1} = m_0 = -0.2$, $P_{0|-1} = P_0 = 1.1$。\n- 使用 $y_0 = 0.3$进行更新：\n  - 新息方差：$S_0 = c^2 P_{0|-1} + r = (1.2)^2(1.1) + 0.5 = 1.584 + 0.5 = 2.084$。\n  - 卡尔曼增益：$K_0 = P_{0|-1} c S_0^{-1} = (1.1)(1.2)/2.084 = 1.32/2.084 \\approx 0.633397$。\n  - 滤波均值：$m_{0|0} = m_{0|-1} + K_0(y_0 - c m_{0|-1}) = -0.2 + (1.32/2.084)(0.3 - 1.2(-0.2)) \\approx 0.142035$。\n  - 滤波方差：$P_{0|0} = (1 - K_0 c)P_{0|-1} \\approx (1 - 0.633397 \\times 1.2)(1.1) \\approx 0.263916$。\n\n**时间 $t=1$：**\n- 预测：\n  - $m_{1|0} = a m_{0|0} = 0.6 \\times 0.142035 \\approx 0.085221$。\n  - $P_{1|0} = a^2 P_{0|0} + q = (0.6)^2(0.263916) + 0.4 \\approx 0.495010$。\n- 使用 $y_1 = -0.1$进行更新：\n  - $S_1 = c^2 P_{1|0} + r = (1.2)^2(0.495010) + 0.5 \\approx 1.212814$。\n  - $K_1 = P_{1|0} c S_1^{-1} \\approx (0.495010)(1.2)/1.212814 \\approx 0.489775$。\n  - $m_{1|1} = m_{1|0} + K_1(y_1 - c m_{1|0}) \\approx 0.085221 + 0.489775(-0.1 - 1.2(0.085221)) \\approx -0.013846$。\n  - $P_{1|1} = (1 - K_1 c)P_{1|0} \\approx (1 - 0.489775 \\times 1.2)(0.495010) \\approx 0.204078$。\n\n**时间 $t=2$：**\n- 预测：\n  - $m_{2|1} = a m_{1|1} = 0.6 \\times (-0.013846) \\approx -0.008308$。\n  - $P_{2|1} = a^2 P_{1|1} + q = (0.6)^2(0.204078) + 0.4 \\approx 0.473468$。\n- 使用 $y_2 = 0.8$进行更新：\n  - $S_2 = c^2 P_{2|1} + r = (1.2)^2(0.473468) + 0.5 \\approx 1.181794$。\n  - $K_2 = P_{2|1} c S_2^{-1} \\approx (0.473468)(1.2)/1.181794 \\approx 0.480766$。\n  - $m_{2|2} = m_{2|1} + K_2(y_2 - c m_{2|1}) \\approx -0.008308 + 0.480766(0.8 - 1.2(-0.008308)) \\approx 0.381095$。\n  - $P_{2|2} = (1 - K_2 c)P_{2|1} \\approx (1 - 0.480766 \\times 1.2)(0.473468) \\approx 0.200318$。\n\n**后向过程：RTS 平滑器**\n令 $m_{t|T}$ 为在给定截至 $T = 2$ 的所有数据的情况下，时间 $t$ 的平滑均值。\n$m_{2|2} \\approx 0.381095$。\n\n**对于 $t=1$：**\n- 平滑增益：$J_1 = P_{1|1} a P_{2|1}^{-1} \\approx 0.204078 \\times 0.6 / 0.473468 \\approx 0.258584$。\n- 平滑均值：$m_{1|2} = m_{1|1} + J_1(m_{2|2} - m_{2|1}) \\approx -0.013846 + 0.258584(0.381095 - (-0.008308)) \\approx 0.086847$。\n\n**对于 $t=0$：**\n- 平滑增益：$J_0 = P_{0|0} a P_{1|0}^{-1} \\approx 0.263916 \\times 0.6 / 0.495010 \\approx 0.319892$。\n- 平滑均值：$m_{0|2} = m_{0|0} + J_0(m_{1|2} - m_{1|0}) \\approx 0.142035 + 0.319892(0.086847 - 0.085221) \\approx 0.142555$。\n\n**最终计算**\n所求量是平滑均值的和：\n$$\n\\mathbb{E}[S \\mid y_{0:2}] = m_{0|2} + m_{1|2} + m_{2|2} \\approx 0.142555 + 0.086847 + 0.381095 = 0.610497\n$$\n将最终答案四舍五入到四位有效数字，得到 $0.6105$。",
            "answer": "$$\\boxed{0.6105}$$"
        },
        {
            "introduction": "上一节练习建立了平滑计算的反向递推理论框架，但该框架是基于连续状态空间的。在实际应用中，我们通常使用粒子滤波器来近似滤波分布，得到的是一组带权重的离散粒子。本练习将带你探索如何将连续的理论转化为离散的粒子算法，通过推导粒子化的反向更新方程，让你亲手构建连接前后向粒子云的“桥梁”。",
            "id": "3327742",
            "problem": "考虑一个隐马尔可夫状态空间模型，其潜过程为 $\\{X_t\\}_{t=0}^{T}$，观测值为 $\\{Y_t\\}_{t=1}^{T}$。该潜过程是一个时变马尔可夫链，其转移密度为 $f_{t+1}(x_{t+1}\\mid x_t)$，观测模型的似然为 $g_t(y_t\\mid x_t)$。设目标为在给定观测值 $y_{1:T}$ 的条件下，计算加性泛函 $\\sum_{t=0}^{T} h_t(X_t)$ 的条件期望，其中每个 $h_t:\\mathcal{X}\\to\\mathbb{R}$ 是一个有界可测函数。\n\n从条件期望的基本原理出发，为 $t=T,T-1,\\dots,0$ 定义反向价值函数 $b_t(x_t)$，其由终端条件 $b_T(x_T)=h_T(x_T)$ 和一个反向递归确定。该反向递归将 $b_t(x_t)$ 表示为瞬时贡献 $h_t(x_t)$ 与在给定 $X_t=x_t$ 及可用信息下 $b_{t+1}(X_{t+1})$ 的条件期望之和。假设在时刻 $t$ 和时刻 $t+1$ 有一个包含 $N$ 个粒子及其权重的前向集合，分别记为 $\\{x_t^{(i)},\\omega_t^{(i)}\\}_{i=1}^{N}$ 和 $\\{x_{t+1}^{(j)},\\omega_{t+1}^{(j)}\\}_{j=1}^{N}$，它们作为经验测度来近似滤波分布 $\\mathsf{P}(X_t\\in\\cdot\\mid y_{1:t})$ 和 $\\mathsf{P}(X_{t+1}\\in\\cdot\\mid y_{1:t+1})$。\n\n使用全期望定律和潜链的马尔可夫性质，在前向滤波-反向平滑 (FFBS) 范式中推导反向递归的粒子近似。该近似通过一个支撑在时刻 $t+1$ 前向粒子云上的离散反向核来更新平滑值 $b_t^{(i)}\\approx b_t(x_t^{(i)})$。对于每个固定的 $i\\in\\{1,\\dots,N\\}$，定义在 $j\\in\\{1,\\dots,N\\}$ 上的反向概率 $\\pi_t^{(i,j)}$，并要求 $\\pi_t^{(i,j)}$ 与转移密度 $f_{t+1}(x_{t+1}^{(j)}\\mid x_t^{(i)})$ 成正比。您的推导必须从反向递归的条件期望表示开始，并通过将相关积分替换为适当的序贯蒙特卡洛 (SMC) 经验测度来进行。最后，给出一个用前向粒子 $\\{x_{t+1}^{(j)},\\omega_{t+1}^{(j)}\\}$、转移密度 $f_{t+1}$ 和下一步的反向值 $\\{b_{t+1}^{(j)}\\}$ 表示的粒子反向更新 $b_t^{(i)}$ 的闭式解析表达式。\n\n最终答案必须是使用指定项表示的 $b_t^{(i)}$ 的单个闭式解析表达式。不需要进行数值近似或舍入。",
            "solution": "该问题要求在隐马尔可夫模型 (HMM) 的背景下，推导一个价值函数的基于粒子的反向更新方程。目标是在给定一系列观测值的条件下，计算潜状态的加性泛函的条件期望。\n\nHMM 由一个潜过程 $\\{X_t\\}_{t=0}^{T}$（其转移密度为 $f_{t+1}(x_{t+1}\\mid x_t)$，初始分布为 $\\mu(x_0)$）和一个观测过程 $\\{Y_t\\}_{t=1}^{T}$（其似然为 $g_t(y_t\\mid x_t)$）定义。目标是计算 $\\mathbb{E}\\left[\\sum_{t=0}^{T} h_t(X_t) \\mid y_{1:T}\\right]$，其中 $y_{1:T} = (y_1, \\dots, y_T)$ 是已实现的观测序列。\n\n问题为 $t=T, T-1, \\dots, 0$ 定义了一个反向价值函数 $b_t(x_t)$。基于最优性原理和计算累积成本的平滑期望的目标，该函数被正式定义为：在给定时刻 $t$ 的状态和所有观测值的条件下，未来成本之和（costs-to-go）的条件期望：\n$$ b_t(x_t) = \\mathbb{E}\\left[ \\sum_{k=t}^{T} h_k(X_k) \\mid X_t = x_t, y_{1:T} \\right] $$\n此递归的终端条件在 $t=T$ 时为：\n$$ b_T(x_T) = \\mathbb{E}\\left[ h_T(X_T) \\mid X_T=x_T, y_{1:T} \\right] = h_T(x_T) $$\n这与问题陈述中提供的终端条件相符。\n\n对于 $t  T$，我们可以使用全期望定律和 HMM 的马尔可夫性质来推导 $b_t(x_t)$ 的反向递归。我们将瞬时贡献 $h_t(x_t)$ 与未来贡献分开：\n$$ b_t(x_t) = \\mathbb{E}\\left[ h_t(X_t) + \\sum_{k=t+1}^{T} h_k(X_k) \\mid X_t = x_t, y_{1:T} \\right] $$\n根据期望的线性性质，并且由于条件中给定了 $X_t=x_t$，上式变为：\n$$ b_t(x_t) = h_t(x_t) + \\mathbb{E}\\left[ \\sum_{k=t+1}^{T} h_k(X_k) \\mid X_t = x_t, y_{1:T} \\right] $$\n使用全期望定律，我们可以引入 $X_{t+1}$：\n$$ \\mathbb{E}\\left[ \\sum_{k=t+1}^{T} h_k(X_k) \\mid X_t = x_t, y_{1:T} \\right] = \\mathbb{E}_{X_{t+1}} \\left[ \\mathbb{E}\\left[ \\sum_{k=t+1}^{T} h_k(X_k) \\mid X_{t+1}, X_t=x_t, y_{1:T} \\right] \\mid X_t=x_t, y_{1:T} \\right] $$\n由于 HMM 的条件独立性质，在给定 $X_{t+1}$ 的情况下，未来的状态和观测值 $\\{X_k, Y_k\\}_{k > t+1}$ 与过去的状态和观测值 $\\{X_k, Y_k\\}_{k \\le t}$ 相互独立。因此，内部的期望可以简化为：\n$$ \\mathbb{E}\\left[ \\sum_{k=t+1}^{T} h_k(X_k) \\mid X_{t+1}, X_t=x_t, y_{1:T} \\right] = \\mathbb{E}\\left[ \\sum_{k=t+1}^{T} h_k(X_k) \\mid X_{t+1}, y_{1:T} \\right] = b_{t+1}(X_{t+1}) $$\n将其代回， $b_t(x_t)$ 的递归式变为：\n$$ b_t(x_t) = h_t(x_t) + \\mathbb{E}_{X_{t+1}}\\left[ b_{t+1}(X_{t+1}) \\mid X_t=x_t, y_{1:T} \\right] $$\n此期望是关于条件密度 $p(x_{t+1} \\mid x_t, y_{1:T})$ 计算的。因此，该递归是一个积分方程：\n$$ b_t(x_t) = h_t(x_t) + \\int_{\\mathcal{X}} b_{t+1}(x_{t+1}) p(x_{t+1} \\mid x_t, y_{1:T}) dx_{t+1} $$\n这是反向递归的条件期望表示。为了创建粒子近似，我们必须近似这个积分。这需要近似条件密度 $p(x_{t+1} \\mid x_t, y_{1:T})$，而该密度依赖于直到时刻 $T$ 的未来观测值。在一个真正的前向滤波-反向平滑 (FFBS) 方案中，这需要一个完整的反向过程。然而，对于在线平滑，一个常见的简化是仅使用截至时刻 $t+1$ 的可用信息来近似该密度。我们用 $p(x_{t+1} \\mid x_t, y_{1:t+1})$ 来近似 $p(x_{t+1} \\mid x_t, y_{1:T})$。\n\n我们使用贝叶斯法则来找出这个近似密度的形式：\n$$ p(x_{t+1} \\mid x_t, y_{1:t+1}) = \\frac{p(x_{t+1}, y_{t+1} \\mid x_t, y_{1:t})}{p(y_{t+1} \\mid x_t, y_{1:t})} $$\n利用 HMM 的条件独立性，$p(x_{t+1}, y_{t+1} \\mid x_t, y_{1:t}) = p(y_{t+1} \\mid x_{t+1}) p(x_{t+1} \\mid x_t) = g_{t+1}(y_{t+1} \\mid x_{t+1}) f_{t+1}(x_{t+1} \\mid x_t)$。分母是归一化常数。因此：\n$$ p(x_{t+1} \\mid x_t, y_{1:t+1}) \\propto g_{t+1}(y_{t+1} \\mid x_{t+1}) f_{t+1}(x_{t+1} \\mid x_t) $$\n问题提供了一个前向粒子集 $\\{x_{t+1}^{(j)}, \\omega_{t+1}^{(j)}\\}_{j=1}^{N}$，用于近似滤波分布 $p(x_{t+1} \\mid y_{1:t+1})$。在标准粒子滤波器（例如，自举滤波器）中，未归一化的权重与似然成正比，因此 $\\omega_{t+1}^{(j)} \\propto g_{t+1}(y_{t+1} \\mid x_{t+1}^{(j)})$，假设粒子是从不依赖于观测值 $y_{t+1}$ 的先验分布中传播的。因此，我们可以使用归一化后的前向权重 $\\omega_{t+1}^{(j)}$ 作为似然项 $g_{t+1}(y_{t+1} \\mid x_{t+1}^{(j)})$ 的代理。\n\n遵循问题中构建离散反向核的指示，我们通过对时刻 $t+1$ 的前向粒子进行加权求和来近似该积分。我们定义从粒子 $x_t^{(i)}$ 到粒子云 $\\{x_{t+1}^{(j)}\\}_{j=1}^N$ 的反向转移概率 $\\pi_t^{(i,j)}$。这些概率应该近似于 $p(x_{t+1}^{(j)} \\mid x_t^{(i)}, y_{1:t+1})$。基于上面推导出的比例关系，我们定义这些概率为：\n$$ \\pi_t^{(i,j)} \\propto \\omega_{t+1}^{(j)} f_{t+1}(x_{t+1}^{(j)} \\mid x_t^{(i)}) $$\n这个定义遵循了问题的约束：它使用了SMC测度 $\\{\\omega_{t+1}^{(j)}\\}$，并且与转移密度 $f_{t+1}$ 成正比，正如推导所要求的那样。归一化后的概率为：\n$$ \\pi_t^{(i,j)} = \\frac{\\omega_{t+1}^{(j)} f_{t+1}(x_{t+1}^{(j)} \\mid x_t^{(i)})}{\\sum_{k=1}^{N} \\omega_{t+1}^{(k)} f_{t+1}(x_{t+1}^{(k)} \\mid x_t^{(i)})} $$\n在每个粒子 $x_t^{(i)}$ 上评估的 $b_t(x_t)$ 反向递归的粒子近似记为 $b_t^{(i)}$。我们用在时刻 $t+1$ 的粒子上按这些反向概率加权的和来代替积分：\n$$ b_t^{(i)} \\approx h_t(x_t^{(i)}) + \\sum_{j=1}^{N} \\pi_t^{(i,j)} b_{t+1}^{(j)} $$\n其中 $b_{t+1}^{(j)} \\approx b_{t+1}(x_{t+1}^{(j)})$ 是反向递归下一步的值。\n\n代入 $\\pi_t^{(i,j)}$ 的表达式，得到粒子反向更新的最终闭式解析表达式：\n$$ b_t^{(i)} = h_t(x_t^{(i)}) + \\frac{\\sum_{j=1}^{N} \\omega_{t+1}^{(j)} f_{t+1}(x_{t+1}^{(j)} \\mid x_t^{(i)}) b_{t+1}^{(j)}}{\\sum_{k=1}^{N} \\omega_{t+1}^{(k)} f_{t+1}(x_{t+1}^{(k)} \\mid x_t^{(i)})} $$\n该表达式使用来自时刻 $t+1$ 的前向粒子和权重 $\\{x_{t+1}^{(j)}, \\omega_{t+1}^{(j)}\\}$、转移密度 $f_{t+1}$ 以及下一步的反向值 $\\{b_{t+1}^{(j)}\\}$ 来更新粒子 $x_t^{(i)}$ 上的值 $b_t^{(i)}$。",
            "answer": "$$\n\\boxed{b_t^{(i)} = h_t(x_t^{(i)}) + \\frac{\\sum_{j=1}^{N} \\omega_{t+1}^{(j)} f_{t+1}(x_{t+1}^{(j)} \\mid x_t^{(i)}) b_{t+1}^{(j)}}{\\sum_{k=1}^{N} \\omega_{t+1}^{(k)} f_{t+1}(x_{t+1}^{(k)} \\mid x_t^{(i)})}}\n$$"
        },
        {
            "introduction": "理论和算法的正确性只是成功实现的第一步，数值稳定性在实践中同样至关重要。在执行反向平滑计算时，由于涉及到大量概率值的连乘，即使在较短的时间序列上也极易出现数值下溢问题，导致结果错误。本练习要求你通过编程实践，对比朴素实现与在对数域下使用 $log-sum-exp$ 技巧的稳健实现，直观地感受并解决这一关键的工程挑战。",
            "id": "3327808",
            "problem": "给定一个离散时间状态空间模型，要求您实现一个前向-后向粒子平滑器，并需特别注意后向递归中的数值稳定性。该模型是线性高斯模型，目标是针对三种不同的测试配置，在指定的时间索引处计算基于粒子的平滑权重。核心任务是同时实现标准概率域中的朴素后向递归和对数域中的数值稳定后向递归，并通过全变差距离量化两者之间的差异。\n\n模型定义如下。对于时间 $t \\in \\{1,\\dots,T\\}$，潜状态为 $x_t \\in \\mathbb{R}$，观测值为 $y_t \\in \\mathbb{R}$。初始状态为 $x_0 \\sim \\mathcal{N}(0,\\sigma_0^2)$，其中 $\\sigma_0 = 2.0$。动力学和观测方程为\n$$\nx_t = a x_{t-1} + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma_q^2),\n$$\n$$\ny_t = c x_t + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0,\\sigma_r^2).\n$$\n转移密度为 $f(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; a x_{t-1}, \\sigma_q^2)$，观测似然为 $g(y_t \\mid x_t) = \\mathcal{N}(y_t; c x_t, \\sigma_r^2)$。所有高斯密度均为标准单变量形式，且必须精确求值。\n\n您必须实现一个自助粒子滤波器，它为每个时间 $t \\in \\{1,\\dots,T\\}$ 生成一组粒子 $\\{x_t^{(i)}\\}_{i=1}^N$ 和归一化重要性权重 $\\{w_t^{(i)}\\}_{i=1}^N$，以近似滤波分布 $p(x_t \\mid y_{1:t})$。在每一步计算权重后使用系统重采样，以减轻权重退化问题。自助粒子滤波器必须遵循以下步骤：\n- 对 $i \\in \\{1,\\dots,N\\}$，独立地初始化 $x_0^{(i)} \\sim \\mathcal{N}(0,\\sigma_0^2)$。\n- 对于 $t \\in \\{1,\\dots,T\\}$:\n  - 根据归一化权重 $w_{t-1}^{(i)}$ 重采样祖先索引（对于 $t=1$，如果计算了 $y_0$，则使用包含 $y_0$ 后的初始权重，如果未使用，则使用均匀权重）。\n  - 传播 $x_t^{(i)} \\sim \\mathcal{N}(a x_{t-1}^{(a_t^{(i)})}, \\sigma_q^2)$，其中 $a_t^{(i)}$ 表示为粒子 $i$ 选择的祖先索引。\n  - 计算未归一化的权重 $\\tilde{w}_t^{(i)} = g(y_t \\mid x_t^{(i)})$，然后进行归一化 $w_t^{(i)} = \\tilde{w}_t^{(i)} / \\sum_{j=1}^N \\tilde{w}_t^{(j)}$。\n\n在前向滤波-后向平滑结构中，用于固定滞后平滑的后向递归定义了后向消息 $\\{\\beta_t^{(i)}\\}_{i=1}^N$，其终端条件为对所有 $i \\in \\{1,\\dots,N\\}$ 都有 $\\beta_T^{(i)} = 1$。对于 $t \\in \\{T-1,\\dots,1\\}$ 的递归由核心定义给出\n$$\n\\beta_t^{(i)} \\propto \\sum_{j=1}^N \\beta_{t+1}^{(j)} \\, w_{t+1}^{(j)} \\, f\\!\\left(x_{t+1}^{(j)} \\mid x_t^{(i)}\\right),\n$$\n其中 $f(\\cdot \\mid \\cdot)$ 是转移密度。那么，时间 $t$ 的平滑权重（在归一化前）定义为\n$$\n\\gamma_t^{(i)} \\propto w_t^{(i)} \\, \\beta_t^{(i)}.\n$$\n您必须以两种方式实现此后向递归：\n- 一种朴素方法，在概率域中通过数值求和直接计算 $\\beta_t^{(i)}$，并使用 $\\gamma_t^{(i)} \\propto w_t^{(i)} \\beta_t^{(i)}$ 及标准归一化。\n- 一种对数域方法，计算 $\\ell\\beta_t^{(i)} = \\log \\beta_t^{(i)}$ 和 $\\ell\\gamma_t^{(i)} = \\log \\gamma_t^{(i)}$，使用避免下溢的代数变换，并在对数域中执行归一化。\n\n然后，您必须针对指定的索引 $t_\\star \\in \\{1,\\dots,T\\}$ 计算两个归一化平滑权重向量之间的全变差距离\n$$\n\\mathrm{TV}\\!\\left(\\gamma_{t_\\star}^{\\mathrm{naive}}, \\gamma_{t_\\star}^{\\log}\\right) \\;=\\; \\tfrac{1}{2} \\sum_{i=1}^N \\left| \\gamma_{t_\\star,\\mathrm{naive}}^{(i)} - \\gamma_{t_\\star,\\log}^{(i)} \\right|.\n$$\n如果朴素方法由于数值下溢在 $t_\\star$ 处产生全零向量导致无法归一化，则按惯例将全变差距离定义为 $1$。\n\n您的程序必须：\n- 为每个测试用例，使用指定的参数和固定的随机种子 $s$ 初始化随机数生成器，模拟潜轨迹 $\\{x_t\\}_{t=0}^T$ 和观测值 $\\{y_t\\}_{t=1}^T$。所有随机性必须源于此种子。\n- 运行自助粒子滤波器，为所有 $t \\in \\{1,\\dots,T\\}$ 生成 $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$。\n- 计算朴素和对数域后向递归，以获得在 $t_\\star$ 处的归一化平滑权重。\n- 为每个测试用例输出全变差距离，形式为浮点数。\n\n使用以下测试套件。每个测试用例是一个有序元组 $(s, T, N, a, \\sigma_q, c, \\sigma_r, t_\\star)$：\n- 案例A（理想情况）： $(123, 60, 100, 0.9, 1.0, 1.0, 1.0, 10)$。\n- 案例B（长序列，易发生下溢）： $(456, 400, 60, 0.9, 1.0, 1.0, 1.0, 1)$。\n- 案例C（尖峰似然，易发生下溢）： $(789, 200, 80, 0.95, 0.5, 1.0, 0.2, 1)$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，顺序与上述测试套件一致。例如，它应打印类似\n\"[0.000123,1.0,1.0]\"。\n不应读取任何外部输入；所有参数值均已在此处给出。不涉及物理单位。未使用角度。不得出现百分比。\n\n您的实现必须是一个完整的、可运行的 Python 程序，该程序执行所有计算并以指定格式精确打印一行。",
            "solution": "用户提供的问题已经过严格验证，被确定为计算统计学领域中一个有效、良定且有科学依据的问题。为实现一个完整且无歧义的解决方案所需的所有必要参数和定义均已提供。接下来是对该解决方案的设计和解释。\n\n该问题要求实现用于线性高斯状态空间模型的前向-后向粒子平滑器。任务的核心是将后向递归的朴素实现与在对数域中运行的数值稳健版本进行比较，并使用全变差距离量化其差异。\n\n该解决方案基于以下原则设计：\n\n### 1. 状态空间模型与数据模拟\n该问题基于一个离散时间线性高斯状态空间模型。潜状态 $x_t \\in \\mathbb{R}$ 根据一个线性随机差分方程演化，而观测值 $y_t \\in \\mathbb{R}$ 是状态的线性函数，并受到噪声的干扰。\n\n模型定义如下：\n- 初始状态分布：$x_0 \\sim \\mathcal{N}(0, \\sigma_0^2)$，其中 $\\sigma_0 = 2.0$。\n- 状态转移模型：$x_t = a x_{t-1} + \\varepsilon_t$，其中 $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma_q^2)$。相应的转移概率密度为 $f(x_t|x_{t-1}) = \\mathcal{N}(x_t; a x_{t-1}, \\sigma_q^2)$。\n- 观测模型：$y_t = c x_t + \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0, \\sigma_r^2)$。相应的观测似然为 $g(y_t|x_t) = \\mathcal{N}(y_t; c x_t, \\sigma_r^2)$。\n\n对于每个测试用例，第一步是根据这些方程，使用指定的参数和随机种子，模拟一条单一的真实轨迹 $\\{x_t\\}_{t=0}^T$ 和相应的观测序列 $\\{y_t\\}_{t=1}^T$。\n\n### 2. 前向过程：自助粒子滤波器\n前向过程包含一个自助粒子滤波器，这是一种序贯蒙特卡洛方法，用于在每个时间步 $t \\in \\{1,\\dots,T\\}$ 近似滤波分布 $p(x_t | y_{1:t})$。该分布由一组 $N$ 个加权粒子 $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ 来近似。滤波器按以下步骤进行：\n\n- **初始化 ($t=0$):**\n  从先验分布 $x_0^{(i)} \\sim \\mathcal{N}(0, \\sigma_0^2)$ 中抽取一组 $N$ 个初始粒子 $\\{x_0^{(i)}\\}_{i=1}^N$。由于没有观测值 $y_0$，初始权重被设为均匀的，$w_0^{(i)} = 1/N$。\n\n- **序贯更新 (对于 $t=1, \\dots, T$):**\n  1.  **重采样：**为对抗权重退化，上一步的粒子 $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ 根据其权重 $\\{w_{t-1}^{(i)}\\}_{i=1}^N$ 进行重采样。采用系统重采样，它为要传播的粒子索引提供了一个低方差估计。此步骤生成一组新的粒子，它们是时间 $t-1$ 处滤波分布的一个无权重近似。\n  2.  **传播：**每个重采样后的粒子通过状态动力学向前传播，以生成新的粒子集 $\\{x_t^{(i)}\\}_{i=1}^N$。每个新粒子按 $x_t^{(i)} \\sim f(\\cdot | \\hat{x}_{t-1}^{(i)}) = \\mathcal{N}(\\cdot; a\\hat{x}_{t-1}^{(i)}, \\sigma_q^2)$ 抽取，其中 $\\hat{x}_{t-1}^{(i)}$ 是来自重采样集的一个粒子。\n  3.  **加权：**新粒子的重要性权重根据它们对当前观测值 $y_t$ 的解释程度来计算。粒子 $i$ 的未归一化权重由似然函数给出：$\\tilde{w}_t^{(i)} = g(y_t | x_t^{(i)})$。\n  4.  **归一化：**权重被归一化以使其总和为一：$w_t^{(i)} = \\tilde{w}_t^{(i)} / \\sum_{j=1}^N \\tilde{w}_t^{(j)}$。\n\n所有时间步 $t \\in \\{1,\\dots,T\\}$ 的粒子 $\\{x_t^{(i)}\\}_{i=1}^N$ 和归一化权重 $\\{w_t^{(i)}\\}_{i=1}^N$ 被存储起来，用于后向过程。\n\n### 3. 后向过程：前向滤波-后向平滑\n后向过程计算平滑分布 $p(x_t|y_{1:T})$ 的基于粒子的近似。给定的算法为消息 $\\beta_t^{(i)}$ 定义了一个后向递归，这些消息代表在时间 $t$ 粒子 $i$ 状态下未来观测值的预测似然的近似，即 $p(y_{t+1:T}|x_t=x_t^{(i)})$。\n\n由于在 $T$ 之后没有未来的观测值，递归在最后一个时间步被初始化为 $\\beta_T^{(i)} = 1$（对所有 $i$）。然后它从 $t=T-1$ 向后递推到 $t=1$：\n$$ \\beta_t^{(i)} = \\sum_{j=1}^N \\beta_{t+1}^{(j)} \\, w_{t+1}^{(j)} \\, f(x_{t+1}^{(j)} | x_t^{(i)}) $$\n注意这是一个未归一化的递归；任何比例常数都被忽略，因为最终的平滑权重将会被归一化。此递归以两种方式实现。\n\n- **朴素实现：**此方法使用浮点运算直接计算总和。对于长时间序列（$T$ 很大）或概率非常小的模型（例如，尖峰似然），后向过程中小数的迭代乘积可能导致数值下溢，使得所有 $i$ 的 $\\beta_t^{(i)}$ 都变为零。\n\n- **对数域实现：**为防止下溢，所有计算都在对数域中执行。乘积变成加法，而加法则使用 log-sum-exp 技巧处理：$\\log(\\sum_k z_k) = \\log(\\sum_k e^{\\log z_k}) \\equiv \\text{logsumexp}_k(\\log z_k)$。对数消息 $\\ell\\beta_t^{(i)} = \\log \\beta_t^{(i)}$ 的后向递归是：\n$$ \\ell\\beta_t^{(i)} = \\text{logsumexp}_{j=1}^N \\left( \\ell\\beta_{t+1}^{(j)} + \\log w_{t+1}^{(j)} + \\log f(x_{t+1}^{(j)} | x_t^{(i)}) \\right) $$\n这个公式是数值稳定的，能稳健地防止下溢。初始条件是 $\\ell\\beta_T^{(i)} = \\log(1) = 0$。\n\n### 4. 平滑权重计算与比较\n一旦后向递归运行到目标时间 $t_\\star$，通过将滤波权重与后向消息结合，计算出未归一化的平滑权重 $\\tilde{\\gamma}_{t_\\star}^{(i)}$：\n$$ \\tilde{\\gamma}_{t_\\star}^{(i)} = w_{t_\\star}^{(i)} \\, \\beta_{t_\\star}^{(i)} $$\n然后对它们进行归一化，以获得最终的平滑权重 $\\gamma_{t_\\star}^{(i)}$。\n\n- **朴素方法：**$\\gamma_{t_\\star, \\text{naive}}^{(i)} = \\tilde{\\gamma}_{t_\\star, \\text{naive}}^{(i)} / \\sum_j \\tilde{\\gamma}_{t_\\star, \\text{naive}}^{(j)}$。如果由于下溢导致总和为零，归一化会失败。\n- **对数域方法：**未归一化的对数权重为 $\\ell\\tilde{\\gamma}_{t_\\star, \\text{log}}^{(i)} = \\log w_{t_\\star}^{(i)} + \\ell\\beta_{t_\\star}^{(i)}$。通过减去总和的对数来进行归一化：$\\ell\\gamma_{t_\\star, \\text{log}}^{(i)} = \\ell\\tilde{\\gamma}_{t_\\star, \\text{log}}^{(i)} - \\text{logsumexp}_j(\\ell\\tilde{\\gamma}_{t_\\star, \\text{log}}^{(j)})$。最终的权重通过指数运算恢复：$\\gamma_{t_\\star, \\text{log}}^{(i)} = \\exp(\\ell\\gamma_{t_\\star, \\text{log}}^{(i)})$。\n\n最后，两种方法之间的差异通过两个归一化权重分布之间的全变差距离来衡量：\n$$ \\mathrm{TV}(\\gamma_{t_\\star}^{\\mathrm{naive}}, \\gamma_{t_\\star}^{\\log}) = \\frac{1}{2} \\sum_{i=1}^N \\left| \\gamma_{t_\\star,\\mathrm{naive}}^{(i)} - \\gamma_{t_\\star,\\log}^{(i)} \\right| $$\n根据问题规范，如果朴素方法产生一个无法归一化的全零权重向量，则 TV 距离定义为 $1.0$，表示与稳定的对数域结果完全不一致。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import logsumexp\n\ndef systematic_resample_indices(weights, n_particles, rng):\n    \"\"\"\n    Performs systematic resampling and returns ancestor indices.\n\n    Args:\n        weights (np.ndarray): A 1D array of normalized particle weights.\n        n_particles (int): The number of particles.\n        rng (np.random.Generator): A random number generator instance.\n\n    Returns:\n        np.ndarray: An array of ancestor indices.\n    \"\"\"\n    cdf = np.cumsum(weights)\n    cdf[-1] = 1.0  # Ensure the cdf ends at 1.0 to avoid float precision issues\n    u = (rng.uniform() + np.arange(n_particles)) / n_particles\n    return np.searchsorted(cdf, u, side='left')\n\ndef run_case(s, T, N, a, sigma_q, c, sigma_r, t_star):\n    \"\"\"\n    Runs a single test case for the forward-backward particle smoother,\n    comparing a naive implementation with a log-domain implementation.\n\n    Returns:\n        float: The total variation distance between the two methods at time t_star.\n    \"\"\"\n    sigma_0 = 2.0\n    rng = np.random.default_rng(s)\n\n    # 1. Simulate true trajectory and observations\n    x_true = np.zeros(T + 1)\n    y_obs = np.zeros(T)\n    x_true[0] = rng.normal(0, sigma_0)\n    for t in range(1, T + 1):\n        x_true[t] = a * x_true[t-1] + rng.normal(0, sigma_q)\n    y_obs = c * x_true[1:] + rng.normal(0, sigma_r, size=T)\n\n    # 2. Forward Pass: Bootstrap Particle Filter\n    particles = np.zeros((T + 1, N))\n    weights = np.zeros((T + 1, N))\n\n    particles[0, :] = rng.normal(0, sigma_0, size=N)\n    weights[0, :] = 1.0 / N\n\n    for t in range(1, T + 1):\n        ancestor_indices = systematic_resample_indices(weights[t-1, :], N, rng)\n        resampled_particles = particles[t-1, ancestor_indices]\n        particles[t, :] = a * resampled_particles + rng.normal(loc=0, scale=sigma_q, size=N)\n        \n        unnorm_weights = norm.pdf(y_obs[t-1], loc=c * particles[t, :], scale=sigma_r)\n        \n        sum_w = np.sum(unnorm_weights)\n        if sum_w > 1e-100:\n            weights[t, :] = unnorm_weights / sum_w\n        else:\n            weights[t, :] = 1.0 / N  # Handle filter collapse\n\n    # Pre-compute log weights for the log-domain backward pass\n    log_weights = np.log(weights + 1e-300)\n\n    # 3. Backward Pass: Naive method\n    beta_naive = np.ones(N)\n    for t in range(T - 1, t_star - 1, -1):\n        means = a * particles[t, :, np.newaxis]\n        f_matrix = norm.pdf(particles[t+1, :], loc=means, scale=sigma_q)\n        v = beta_naive * weights[t+1, :]\n        beta_naive = f_matrix @ v\n    \n    unnorm_gamma_naive = weights[t_star, :] * beta_naive\n    sum_gamma_naive = np.sum(unnorm_gamma_naive)\n    \n    naive_failed = sum_gamma_naive == 0\n    if naive_failed:\n        gamma_naive_norm = np.zeros(N)\n    else:\n        gamma_naive_norm = unnorm_gamma_naive / sum_gamma_naive\n\n    # 4. Backward Pass: Log-domain method\n    log_beta_log = np.zeros(N)\n    for t in range(T - 1, t_star - 1, -1):\n        means = a * particles[t, :, np.newaxis]\n        log_f_matrix = norm.logpdf(particles[t+1, :], loc=means, scale=sigma_q)\n        terms = log_beta_log[np.newaxis, :] + log_weights[t+1, np.newaxis, :] + log_f_matrix\n        log_beta_log = logsumexp(terms, axis=1)\n\n    log_unnorm_gamma_log = log_weights[t_star, :] + log_beta_log\n    log_sum_gamma = logsumexp(log_unnorm_gamma_log)\n    norm_log_gamma = log_unnorm_gamma_log - log_sum_gamma\n    gamma_log_norm = np.exp(norm_log_gamma)\n\n    # 5. Compute TV distance\n    if naive_failed:\n        return 1.0\n    else:\n        return 0.5 * np.sum(np.abs(gamma_naive_norm - gamma_log_norm))\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (s, T, N, a, sigma_q, c, sigma_r, t_star)\n        (123, 60, 100, 0.9, 1.0, 1.0, 1.0, 10),\n        (456, 400, 60, 0.9, 1.0, 1.0, 1.0, 1),\n        (789, 200, 80, 0.95, 0.5, 1.0, 0.2, 1),\n    ]\n\n    results = [run_case(*case) for case in test_cases]\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}