{
    "hands_on_practices": [
        {
            "introduction": "任何粒子滤波器的核心都是递归权重更新，它将来自观测的新信息与系统动态相结合。本练习将指导您实现非线性模型下的单步权重更新，并强调使用对数权重以保证数值稳定性。通过从第一性原理出发编写这一基本机制，您将对重要性权重的计算方式获得具体的理解。",
            "id": "3338877",
            "problem": "考虑一个隐马尔可夫模型，其潜状态序列为 $\\{x_t\\}_{t \\ge 0}$，观测序列为 $\\{y_t\\}_{t \\ge 1}$。其中，状态转移由一个密度为 $p(x_t \\mid x_{t-1})$ 的马尔可夫核给出，观测模型为非线性，其密度为 $p(y_t \\mid x_t)$，由 $y_t = h(x_t) + \\epsilon_t$ 导出，其中 $h(\\cdot)$ 是一个已知函数，$\\epsilon_t$ 是独立噪声。假设在时刻 $t-1$，你有一个加权粒子系统 $\\{x_{t-1}^{(i)}, w_{t-1}^{(i)}\\}_{i=1}^N$，它通过一个加权经验测度来近似滤波分布 $p(x_{t-1} \\mid y_{1:t-1})$。对于时刻 $t$ 的更新，你从一个提议分布 $q_t$ 中为 $i = 1, \\dots, N$ 独立地采样 $x_t^{(i)} \\sim q_t(\\cdot \\mid x_{t-1}^{(i)}, y_t)$，然后计算归一化的权重 $\\tilde{w}_t^{(i)}$。\n\n仅从重要性采样和贝叶斯法则的原理出发，针对以下特定情况，实现一个单时间步长 $t$ 的单步序列重要性采样（SIS）权重更新，无需重采样：\n- 状态转移密度是高斯的，\n$$\np(x_t \\mid x_{t-1}) = \\mathcal{N}\\!\\left(x_t; a x_{t-1} + b \\sin(x_{t-1}), \\sigma_x^2\\right),\n$$\n- 观测模型是\n$$\ny_t = h(x_t) + \\epsilon_t, \\quad h(x) = \\tfrac{1}{2} x^2, \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_y^2),\n$$\n因此\n$$\np(y_t \\mid x_t) = \\mathcal{N}\\!\\left(y_t; \\tfrac{1}{2} x_t^2, \\sigma_y^2\\right),\n$$\n- 提议 $q_t$ 要么是转移（自助提议），要么是均值依赖于 $x_{t-1}$ 的高斯分布，\n$$\nq_t(x_t \\mid x_{t-1}, y_t) =\n\\begin{cases}\n\\mathcal{N}\\!\\left(x_t; a x_{t-1} + b \\sin(x_{t-1}), \\sigma_x^2\\right),  \\text{自助情况}, \\\\\n\\mathcal{N}\\!\\left(x_t; a_p x_{t-1} + c x_{t-1}^3, \\sigma_q^2\\right),  \\text{通用情况}.\n\\end{cases}\n$$\n\n你的程序必须：\n- 使用对数权重以保持数值稳定性，然后通过取指数和归一化返回归一化权重 $\\tilde{w}_t^{(i)}$。\n- 对每个粒子 $i$，应用重要性采样原理于目标 $p(x_t, x_{t-1} \\mid y_{1:t})$ 和提议 $q_t(x_t \\mid x_{t-1}, y_t)$，计算未归一化的权重增量。\n- 归一化权重，使得 $\\sum_{i=1}^N \\tilde{w}_t^{(i)} = 1$。\n\n你必须为以下测试套件实现并评估更新。在每种情况下，使用指定的随机种子来可复现地生成提议 $x_t^{(i)}$。所有数组和常量都以必须保持不变的精确值给出。函数 $h(x)$ 在所有情况下都固定为 $h(x) = \\tfrac{1}{2} x^2$。\n\n测试用例A（通用提议，温和机制）：\n- 粒子数 $N = 5$，\n- 上一时刻的粒子 $x_{t-1}^{(i)}$ 设置为 $[-1.0, -0.2, 0.3, 1.0, 2.0]$，\n- 上一时刻的归一化权重 $w_{t-1}^{(i)}$ 设置为 $[0.1, 0.2, 0.4, 0.2, 0.1]$，\n- 参数：$a = 0.7$, $b = 0.3$, $\\sigma_x = 0.5$, $\\sigma_y = 0.8$，\n- 提议参数（通用）：$a_p = 0.5$, $c = -0.1$, $\\sigma_q = 0.7$，\n- 观测值 $y_t = 1.25$，\n- 随机种子 $= 123$。\n\n测试用例B（自助提议，随机游走转移）：\n- 粒子数 $N = 4$，\n- 上一时刻的粒子 $x_{t-1}^{(i)}$ 设置为 $[-0.5, 0.0, 0.5, 1.5]$，\n- 上一时刻的归一化权重 $w_{t-1}^{(i)}$ 设置为 $[0.25, 0.25, 0.25, 0.25]$，\n- 参数：$a = 1.0$, $b = 0.0$, $\\sigma_x = 0.3$, $\\sigma_y = 0.4$，\n- 提议：自助（即等于转移），\n- 观测值 $y_t = 0.5$，\n- 随机种子 $= 7$。\n\n测试用例C（通用提议，近简并似然机制）：\n- 粒子数 $N = 3$，\n- 上一时刻的粒子 $x_{t-1}^{(i)}$ 设置为 $[-2.0, 0.0, 2.0]$，\n- 上一时刻的归一化权重 $w_{t-1}^{(i)}$ 设置为 $[0.2, 0.6, 0.2]$，\n- 参数：$a = 0.9$, $b = 0.1$, $\\sigma_x = 0.2$, $\\sigma_y = 0.05$，\n- 提议参数（通用）：$a_p = 0.9$, $c = 0.0$, $\\sigma_q = 1.0$，\n- 观测值 $y_t = 10.0$，\n- 随机种子 $= 42$。\n\n测试用例D（单粒子边缘情况）：\n- 粒子数 $N = 1$，\n- 上一时刻的粒子 $x_{t-1}^{(i)}$ 设置为 $[0.3]$，\n- 上一时刻的归一化权重 $w_{t-1}^{(i)}$ 设置为 $[1.0]$，\n- 参数：$a = 0.5$, $b = 0.5$, $\\sigma_x = 1.2$, $\\sigma_y = 0.3$，\n- 提议参数（通用）：$a_p = 0.5$, $c = 0.0$, $\\sigma_q = 0.8$，\n- 观测值 $y_t = 0.1$，\n- 随机种子 $= 99$。\n\n你的程序必须完全按照每个测试用例的描述实现采样和权重更新。最终输出格式必须是一行，包含所有测试用例A、B、C、D的拼接归一化权重，以方括号括起来的逗号分隔列表形式。每个浮点数必须四舍五入到恰好 $10$ 位小数。例如，一个包含两个数字的输出看起来会像 $[0.1234567890,0.8765432110]$。不涉及物理单位或角度，因此不需要进行单位转换。最终答案所需的数据类型是浮点数，最终输出是这些浮点数的单个列表。",
            "solution": "当前任务是为序列重要性采样（SIS）粒子滤波器实现一个单步权重更新。更新规则的推导必须基于贝叶斯推断和重要性采样的基本原理。\n\n我们考虑一个由潜状态过程 $\\{x_t\\}_{t \\ge 0}$ 和观测过程 $\\{y_t\\}_{t \\ge 1}$ 定义的状态空间模型。该模型由以下部分指定：\n1.  一个初始分布 $p(x_0)$。\n2.  一个状态转移模型，其密度为 $p(x_t \\mid x_{t-1})$。\n3.  一个观测模型，其密度为 $p(y_t \\mid x_t)$。\n\n滤波的目标是递归地计算给定截至时刻 $t$ 的所有观测值时状态 $x_t$ 的后验分布，记为 $p(x_t \\mid y_{1:t})$。贝叶斯滤波递归过程分两步进行：预测和更新。\n\n预测步骤通过状态动态演化先前的滤波分布 $p(x_{t-1} \\mid y_{1:t-1})$，以获得 $x_t$ 的预测分布：\n$$\np(x_t \\mid y_{1:t-1}) = \\int p(x_t \\mid x_{t-1}) p(x_{t-1} \\mid y_{1:t-1}) \\mathrm{d}x_{t-1}\n$$\n然后，更新步骤通过贝叶斯法则并入新的观测值 $y_t$：\n$$\np(x_t \\mid y_{1:t}) = \\frac{p(y_t \\mid x_t) p(x_t \\mid y_{1:t-1})}{p(y_t \\mid y_{1:t-1})} \\propto p(y_t \\mid x_t) p(x_t \\mid y_{1:t-1})\n$$\n对于非线性或非高斯模型，这些步骤通常在解析上是难解的。SIS 提供了一种数值近似方法。\n\n在时刻 $t-1$，我们假设滤波分布由一个加权经验测度来近似：\n$$\np(x_{t-1} \\mid y_{1:t-1}) \\approx \\sum_{i=1}^N w_{t-1}^{(i)} \\delta_{x_{t-1}^{(i)}}(x_{t-1})\n$$\n其中 $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ 是粒子，$\\{w_{t-1}^{(i)}\\}_{i=1}^N$ 是它们对应的归一化权重（$\\sum_i w_{t-1}^{(i)} = 1$），而 $\\delta_a(x)$ 是在 $a$ 处的狄拉克δ测度。\n\n为了近似时刻 $t$ 的分布 $p(x_t \\mid y_{1:t})$，我们可以应用重要性采样。SIS 更新的目标分布与 $p(y_t \\mid x_t) p(x_t \\mid x_{t-1}) p(x_{t-1} \\mid y_{1:t-1})$ 成正比。我们从一个提议分布 $q_t(x_t \\mid x_{t-1}, y_t)$ 中抽取一组新的粒子 $\\{x_t^{(i)}\\}_{i=1}^N$。对于每个以 $x_{t-1}^{(i)}$ 结尾的现有粒子路径，我们采样一个新的状态 $x_t^{(i)} \\sim q_t(\\cdot \\mid x_{t-1}^{(i)}, y_t)$。\n\n状态轨迹的重要性权重是递归更新的。以 $(x_{t-1}^{(i)}, x_t^{(i)})$ 结尾的路径的权重 $w_t^{(i)}$ 由下式给出：\n$$\nw_t^{(i)} \\propto w_{t-1}^{(i)} \\times \\alpha_t^{(i)}\n$$\n其中 $\\alpha_t^{(i)}$ 是增量重要性权重。这个增量权重校正了目标动态和提议分布之间的差异。它被定义为目标密度与提议密度的比率：\n$$\n\\alpha_t^{(i)} = \\frac{p(y_t \\mid x_t^{(i)}) p(x_t^{(i)} \\mid x_{t-1}^{(i)})}{q_t(x_t^{(i)} \\mid x_{t-1}^{(i)}, y_t)}\n$$\n新的未归一化权重是 $w_t^{\\prime(i)} = w_{t-1}^{(i)} \\alpha_t^{(i)}$。然后将这些权重归一化以使它们的总和为一：$\\tilde{w}_t^{(i)} = w_t^{\\prime(i)} / \\sum_{j=1}^N w_t^{\\prime(j)}$。\n\n为了提高数值稳定性，尤其是在处理小概率的乘积时，计算在对数域中进行。未归一化的对数权重为：\n$$\n\\log w_t^{\\prime(i)} = \\log w_{t-1}^{(i)} + \\log \\alpha_t^{(i)}\n$$\n其中对数增量权重为：\n$$\n\\log \\alpha_t^{(i)} = \\log p(y_t \\mid x_t^{(i)}) + \\log p(x_t^{(i)} \\mid x_{t-1}^{(i)}) - \\log q_t(x_t^{(i)} \\mid x_{t-1}^{(i)}, y_t)\n$$\n高斯分布 $\\mathcal{N}(x; \\mu, \\sigma^2)$ 的对数概率密度由下式给出：\n$$\n\\log \\mathcal{N}(x; \\mu, \\sigma^2) = -\\frac{(x-\\mu)^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2)\n$$\n将此应用于问题特定的密度：\n1.  **对数似然：** 从 $p(y_t \\mid x_t) = \\mathcal{N}(y_t; \\tfrac{1}{2} x_t^2, \\sigma_y^2)$，我们有：\n    $$\n    \\log p(y_t \\mid x_t^{(i)}) = -\\frac{(y_t - \\frac{1}{2}(x_t^{(i)})^2)^2}{2\\sigma_y^2} - \\frac{1}{2}\\log(2\\pi\\sigma_y^2)\n    $$\n2.  **对数转移：** 从 $p(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; a x_{t-1} + b \\sin(x_{t-1}), \\sigma_x^2)$，令 $\\mu_{trans}^{(i)} = a x_{t-1}^{(i)} + b \\sin(x_{t-1}^{(i)})$。则：\n    $$\n    \\log p(x_t^{(i)} \\mid x_{t-1}^{(i)}) = -\\frac{(x_t^{(i)} - \\mu_{trans}^{(i)})^2}{2\\sigma_x^2} - \\frac{1}{2}\\log(2\\pi\\sigma_x^2)\n    $$\n3.  **对数提议：** 提议 $q_t$ 有两种形式。\n    - **通用情况：** $q_t(x_t \\mid x_{t-1}, y_t) = \\mathcal{N}(x_t; a_p x_{t-1} + c x_{t-1}^3, \\sigma_q^2)$。令 $\\mu_{prop}^{(i)} = a_p x_{t-1}^{(i)} + c (x_{t-1}^{(i)})^3$。则：\n      $$\n      \\log q_t(x_t^{(i)} \\mid x_{t-1}^{(i)}, y_t) = -\\frac{(x_t^{(i)} - \\mu_{prop}^{(i)})^2}{2\\sigma_q^2} - \\frac{1}{2}\\log(2\\pi\\sigma_q^2)\n      $$\n    - **自助情况：** 提议是状态转移先验，$q_t(x_t \\mid x_{t-1}, y_t) = p(x_t \\mid x_{t-1})$。在这种情况下，$\\mu_{prop}^{(i)} = \\mu_{trans}^{(i)}$ 且 $\\sigma_q^2 = \\sigma_x^2$。对数转移和对数提议项相互抵消，将对数增量权重简化为仅剩对数似然：\n      $$\n      \\log \\alpha_t^{(i)} = \\log p(y_t \\mid x_t^{(i)})\n      $$\n\n最后，为了将未归一化的对数权重 $\\log w_t^{\\prime(i)}$ 转换回归一化权重 $\\tilde{w}_t^{(i)}$，我们使用 log-sum-exp 技巧来防止数值上溢和下溢。令 $L_i = \\log w_t^{\\prime(i)}$ 和 $L_{max} = \\max_j L_j$。则：\n$$\n\\tilde{w}_t^{(i)} = \\frac{\\exp(L_i)}{\\sum_{j=1}^N \\exp(L_j)} = \\frac{\\exp(L_i - L_{max})}{\\sum_{j=1}^N \\exp(L_j - L_{max})}\n$$\n该过程提供了一种数值稳定的方法来执行单步 SIS 更新。实现将遵循为每个测试用例推导出的这些公式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases for the SIS weight update.\n    \"\"\"\n    test_cases = [\n        # Test case A (general proposal, moderate regime)\n        {\n            \"N\": 5,\n            \"x_tm1\": np.array([-1.0, -0.2, 0.3, 1.0, 2.0]),\n            \"w_tm1\": np.array([0.1, 0.2, 0.4, 0.2, 0.1]),\n            \"a\": 0.7, \"b\": 0.3, \"sigma_x\": 0.5, \"sigma_y\": 0.8,\n            \"proposal_type\": \"general\",\n            \"a_p\": 0.5, \"c\": -0.1, \"sigma_q\": 0.7,\n            \"y_t\": 1.25,\n            \"seed\": 123\n        },\n        # Test case B (bootstrap proposal, random-walk transition)\n        {\n            \"N\": 4,\n            \"x_tm1\": np.array([-0.5, 0.0, 0.5, 1.5]),\n            \"w_tm1\": np.array([0.25, 0.25, 0.25, 0.25]),\n            \"a\": 1.0, \"b\": 0.0, \"sigma_x\": 0.3, \"sigma_y\": 0.4,\n            \"proposal_type\": \"bootstrap\",\n            \"a_p\": None, \"c\": None, \"sigma_q\": None, # Will be set from transition\n            \"y_t\": 0.5,\n            \"seed\": 7\n        },\n        # Test case C (general proposal, near-degenerate likelihood regime)\n        {\n            \"N\": 3,\n            \"x_tm1\": np.array([-2.0, 0.0, 2.0]),\n            \"w_tm1\": np.array([0.2, 0.6, 0.2]),\n            \"a\": 0.9, \"b\": 0.1, \"sigma_x\": 0.2, \"sigma_y\": 0.05,\n            \"proposal_type\": \"general\",\n            \"a_p\": 0.9, \"c\": 0.0, \"sigma_q\": 1.0,\n            \"y_t\": 10.0,\n            \"seed\": 42\n        },\n        # Test case D (single-particle edge case)\n        {\n            \"N\": 1,\n            \"x_tm1\": np.array([0.3]),\n            \"w_tm1\": np.array([1.0]),\n            \"a\": 0.5, \"b\": 0.5, \"sigma_x\": 1.2, \"sigma_y\": 0.3,\n            \"proposal_type\": \"general\",\n            \"a_p\": 0.5, \"c\": 0.0, \"sigma_q\": 0.8,\n            \"y_t\": 0.1,\n            \"seed\": 99\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        # Extract parameters\n        N = case[\"N\"]\n        x_tm1 = case[\"x_tm1\"]\n        w_tm1 = case[\"w_tm1\"]\n        a, b, sigma_x, sigma_y = case[\"a\"], case[\"b\"], case[\"sigma_x\"], case[\"sigma_y\"]\n        y_t = case[\"y_t\"]\n        seed = case[\"seed\"]\n        \n        # Set up random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Define proposal parameters based on type\n        if case[\"proposal_type\"] == \"bootstrap\":\n            # For bootstrap, proposal is the transition prior\n            a_p, c, sigma_q = a, 0.0, sigma_x\n        else: # general\n            a_p, c, sigma_q = case[\"a_p\"], case[\"c\"], case[\"sigma_q\"]\n\n        # 1. Sample new particles x_t from the proposal distribution q_t\n        # mean of the proposal distribution\n        mu_prop = a_p * x_tm1 + c * x_tm1**3\n        # sample new particles\n        x_t = mu_prop + sigma_q * rng.standard_normal(size=N)\n\n        # 2. Compute importance weights in log-domain\n        # Log of previous weights\n        # np.log can produce -inf for w=0, but all given weights are  0.\n        log_w_tm1 = np.log(w_tm1)\n\n        # Calculate the three components of the log-incremental weight\n        # a) Log-likelihood: log p(y_t | x_t)\n        log_p_yt_xt = norm.logpdf(y_t, loc=0.5 * x_t**2, scale=sigma_y)\n        \n        # b) Log-transition: log p(x_t | x_{t-1})\n        mu_trans = a * x_tm1 + b * np.sin(x_tm1)\n        log_p_xt_xtm1 = norm.logpdf(x_t, loc=mu_trans, scale=sigma_x)\n\n        # c) Log-proposal: log q(x_t | x_{t-1}, y_t)\n        # The means mu_prop were already computed for sampling\n        log_q_xt = norm.logpdf(x_t, loc=mu_prop, scale=sigma_q)\n        \n        # Log-incremental weight\n        log_alpha_t = log_p_yt_xt + log_p_xt_xtm1 - log_q_xt\n\n        # Unnormalized log-weights at time t\n        log_w_t_unnormalized = log_w_tm1 + log_alpha_t\n        \n        # 3. Normalize weights using log-sum-exp trick for numerical stability\n        if N  0:\n            log_w_max = np.max(log_w_t_unnormalized)\n            # Subtract max for stability, then exponentiate\n            w_t_shifted = np.exp(log_w_t_unnormalized - log_w_max)\n            # Normalize\n            w_t_normalized = w_t_shifted / np.sum(w_t_shifted)\n        else:\n            w_t_normalized = np.array([])\n            \n        # Append formatted results to the final list\n        for w in w_t_normalized:\n            all_results.append(f\"{w:.10f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "粒子滤波中的一个普遍挑战是权重退化，即少数粒子的权重接近1，而大多数粒子的权重可以忽略不计，导致近似失效。有效样本量（Effective Sample Size, ESS）是诊断此问题的关键指标。本练习要求您从其统计定义出发，推导 ESS 的公式，并用它来判断是否需要执行作为标准补救措施的重采样。",
            "id": "3338911",
            "problem": "在一个序列重要性重采样 (SIR) 自举滤波器中，一个特定的时间步 $t$ 产生一个包含 $N = 5$ 个粒子 $\\{X_{t}^{(i)}\\}_{i=1}^{5}$ 的粒子系统，以及相关的未归一化的重要性权重 $\\{w_{t}^{(i)}\\}_{i=1}^{5}$。这些未归一化的权重仅在一个未知的正常数比例因子 $k  0$ 的意义下是已知的，其值为\n$$\nw_{t}^{(1)} = 3k,\\quad w_{t}^{(2)} = 0,\\quad w_{t}^{(3)} = k,\\quad w_{t}^{(4)} = k,\\quad w_{t}^{(5)} = 5k.\n$$\n令 $\\tilde{w}_{t}^{(i)} = w_{t}^{(i)}\\big/\\sum_{j=1}^{N} w_{t}^{(j)}$ 表示归一化权重。对于一个有界测试函数 $h$，其自归一化重要性采样估计量为\n$$\n\\hat{I}_{t}(h) = \\sum_{i=1}^{N} \\tilde{w}_{t}^{(i)}\\, h\\!\\left(X_{t}^{(i)}\\right).\n$$\n从有效样本量 (ESS) 的基本定义出发，即 ESS 是唯一的 $N_{\\mathrm{eff}}$，使得在独立性和等方差假设下 $\\hat{I}_{t}(h)$ 的方差与一个基于 $N_{\\mathrm{eff}}$ 个等权重且独立样本的标准蒙特卡洛估计量的方差相匹配。仅使用此定义和独立和的标准方差性质，首先推导出一个用归一化权重 $\\{\\tilde{w}_{t}^{(i)}\\}_{i=1}^{N}$ 表示 $N_{\\mathrm{eff}}$ 的表达式。然后使用您的表达式计算上述给定权重向量的 $N_{\\mathrm{eff}}$，并明确证明它对于未知的比例因子 $k$ 是不变的。\n\n在时间 $t$ 的算法重采样规则是，当且仅当 $N_{\\mathrm{eff}}  \\tau_{t}$ 时触发重采样，其中阈值为 $\\tau_{t} = \\alpha N$，且 $\\alpha = \\tfrac{1}{2}$。根据此规则，确定在时间 $t$ 是否应执行重采样。\n\n将您的最终答案表示为 $N_{\\mathrm{eff}}$ 的精确值，写成最简分数形式，不带单位。最终数值答案中不要包含重采样决策。",
            "solution": "首先对问题进行验证，以确保其科学上合理、良定且客观。\n\n### 步骤1：提取已知条件\n- 粒子数：$N = 5$\n- 未归一化的重要性权重：$\\{w_{t}^{(i)}\\}_{i=1}^{5}$，其值为 $w_{t}^{(1)} = 3k$, $w_{t}^{(2)} = 0$, $w_{t}^{(3)} = k$, $w_{t}^{(4)} = k$, $w_{t}^{(5)} = 5k$，其中 $k  0$ 是一个未知的正常数比例因子。\n- 归一化权重：$\\tilde{w}_{t}^{(i)} = w_{t}^{(i)}\\big/\\sum_{j=1}^{N} w_{t}^{(j)}$。\n- 自归一化重要性采样估计量：对于有界测试函数 $h$，$\\hat{I}_{t}(h) = \\sum_{i=1}^{N} \\tilde{w}_{t}^{(i)}\\, h\\!\\left(X_{t}^{(i)}\\right)$。\n- 有效样本量 ($N_{\\mathrm{eff}}$) 的基本定义：$N_{\\mathrm{eff}}$ 是唯一的数值，使得在独立性和等方差假设下 $\\hat{I}_{t}(h)$ 的方差与一个基于 $N_{\\mathrm{eff}}$ 个等权重且独立样本的标准蒙特卡洛估计量的方差相匹配。\n- 重采样规则：如果 $N_{\\mathrm{eff}}  \\tau_{t}$，则进行重采样。\n- 重采样阈值：$\\tau_{t} = \\alpha N$，其中 $\\alpha = \\tfrac{1}{2}$。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题是序列蒙特卡洛方法（粒子滤波器）领域的一个标准练习。它要求从其概念性定义出发，推导一个基本量——有效样本量 (ESS)，并随后为一组给定的参数计算其值。所有概念（重要性权重、自归一化估计量、ESS、重采样）在文献中都是标准且定义明确的。该问题是自洽的，数学上一致，并且没有任何科学或逻辑上的缺陷。所提供的值是用于教授该主题的典型“玩具”示例。\n\n### 步骤3：结论与行动\n该问题被认为是**有效的**。将提供完整解答。\n\n### 有效样本量 ($N_{\\mathrm{eff}}$) 的推导\n\n问题要求从 $N_{\\mathrm{eff}}$ 的基本定义出发推导其表达式。我们已知自归一化重要性采样估计量：\n$$\n\\hat{I}_{t}(h) = \\sum_{i=1}^{N} \\tilde{w}_{t}^{(i)}\\, h\\!\\left(X_{t}^{(i)}\\right)\n$$\n为求其方差，我们将归一化权重 $\\{\\tilde{w}_{t}^{(i)}\\}$ 视为固定常数，并将粒子 $\\{X_{t}^{(i)}\\}$ 视为独立抽取的。这意味着随机变量 $h(X_{t}^{(i)})$ 是独立的。问题要求假设它们具有相同的方差，我们将其记为 $\\sigma^2 = \\mathrm{Var}(h(X_{t}^{(i)}))$。\n\n使用独立随机变量加权和的方差性质 $\\mathrm{Var}(\\sum a_i Y_i) = \\sum a_i^2 \\mathrm{Var}(Y_i)$，我们有：\n$$\n\\mathrm{Var}(\\hat{I}_{t}(h)) = \\mathrm{Var}\\left(\\sum_{i=1}^{N} \\tilde{w}_{t}^{(i)}\\, h(X_{t}^{(i)})\\right) = \\sum_{i=1}^{N} (\\tilde{w}_{t}^{(i)})^2 \\mathrm{Var}(h(X_{t}^{(i)}))\n$$\n应用等方差假设：\n$$\n\\mathrm{Var}(\\hat{I}_{t}(h)) = \\sum_{i=1}^{N} (\\tilde{w}_{t}^{(i)})^2 \\sigma^2 = \\sigma^2 \\sum_{i=1}^{N} (\\tilde{w}_{t}^{(i)})^2\n$$\n接下来，考虑一个对相同数量的标准蒙特卡洛估计量，但它是基于从目标分布直接抽取的 $N_{\\mathrm{eff}}$ 个独立同分布样本 $\\{Y_j\\}_{j=1}^{N_{\\mathrm{eff}}}$。该估计量是样本均值：\n$$\n\\hat{J}_{N_{\\mathrm{eff}}}(h) = \\frac{1}{N_{\\mathrm{eff}}} \\sum_{j=1}^{N_{\\mathrm{eff}}} h(Y_j)\n$$\n假设 $\\mathrm{Var}(h(Y_j)) = \\sigma^2$，这个标准估计量的方差为：\n$$\n\\mathrm{Var}(\\hat{J}_{N_{\\mathrm{eff}}}(h)) = \\mathrm{Var}\\left(\\frac{1}{N_{\\mathrm{eff}}} \\sum_{j=1}^{N_{\\mathrm{eff}}} h(Y_j)\\right) = \\frac{1}{(N_{\\mathrm{eff}})^2} \\sum_{j=1}^{N_{\\mathrm{eff}}} \\mathrm{Var}(h(Y_j)) = \\frac{1}{(N_{\\mathrm{eff}})^2} N_{\\mathrm{eff}} \\sigma^2 = \\frac{\\sigma^2}{N_{\\mathrm{eff}}}\n$$\n$N_{\\mathrm{eff}}$ 的定义要求这两个方差相等：\n$$\n\\mathrm{Var}(\\hat{I}_{t}(h)) = \\mathrm{Var}(\\hat{J}_{N_{\\mathrm{eff}}}(h))\n$$\n$$\n\\sigma^2 \\sum_{i=1}^{N} (\\tilde{w}_{t}^{(i)})^2 = \\frac{\\sigma^2}{N_{\\mathrm{eff}}}\n$$\n假设一个非平凡测试函数 $h$ 使得 $\\sigma^2  0$，我们可以将两边同除以 $\\sigma^2$：\n$$\n\\sum_{i=1}^{N} (\\tilde{w}_{t}^{(i)})^2 = \\frac{1}{N_{\\mathrm{eff}}}\n$$\n解出 $N_{\\mathrm{eff}}$ 得到所需的表达式：\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^{N} (\\tilde{w}_{t}^{(i)})^2}\n$$\n\n### $N_{\\mathrm{eff}}$ 的计算与重采样决策\n\n我们已知 $N=5$ 和未归一化的权重：\n$w_{t}^{(1)} = 3k$, $w_{t}^{(2)} = 0$, $w_{t}^{(3)} = k$, $w_{t}^{(4)} = k$, $w_{t}^{(5)} = 5k$.\n\n首先，我们计算未归一化权重的和，即归一化常数：\n$$\n\\sum_{j=1}^{5} w_{t}^{(j)} = 3k + 0 + k + k + 5k = 10k\n$$\n接下来，我们计算归一化权重 $\\tilde{w}_{t}^{(i)} = w_{t}^{(i)} / \\sum_{j=1}^{5} w_{t}^{(j)}$：\n$$\n\\tilde{w}_{t}^{(1)} = \\frac{3k}{10k} = \\frac{3}{10}\n$$\n$$\n\\tilde{w}_{t}^{(2)} = \\frac{0}{10k} = 0\n$$\n$$\n\\tilde{w}_{t}^{(3)} = \\frac{k}{10k} = \\frac{1}{10}\n$$\n$$\n\\tilde{w}_{t}^{(4)} = \\frac{k}{10k} = \\frac{1}{10}\n$$\n$$\n\\tilde{w}_{t}^{(5)} = \\frac{5k}{10k} = \\frac{5}{10} = \\frac{1}{2}\n$$\n如图所示，在归一化过程中，未知的比例因子 $k0$ 被消掉了，这表明归一化权重以及因此得到的 $N_{\\mathrm{eff}}$ 对于这个因子是不变的。\n\n现在，我们计算归一化权重平方的和：\n$$\n\\sum_{i=1}^{5} (\\tilde{w}_{t}^{(i)})^2 = \\left(\\frac{3}{10}\\right)^2 + 0^2 + \\left(\\frac{1}{10}\\right)^2 + \\left(\\frac{1}{10}\\right)^2 + \\left(\\frac{1}{2}\\right)^2\n$$\n$$\n= \\frac{9}{100} + 0 + \\frac{1}{100} + \\frac{1}{100} + \\frac{1}{4} = \\frac{9}{100} + \\frac{1}{100} + \\frac{1}{100} + \\frac{25}{100}\n$$\n$$\n= \\frac{9+1+1+25}{100} = \\frac{36}{100}\n$$\n使用推导出的 $N_{\\mathrm{eff}}$ 公式：\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^{5} (\\tilde{w}_{t}^{(i)})^2} = \\frac{1}{36/100} = \\frac{100}{36}\n$$\n将分数化为最简形式：\n$$\nN_{\\mathrm{eff}} = \\frac{100 \\div 4}{36 \\div 4} = \\frac{25}{9}\n$$\n\n最后，我们确定是否应执行重采样。重采样阈值为 $\\tau_{t} = \\alpha N$，其中 $\\alpha = \\frac{1}{2}$ 且 $N=5$：\n$$\n\\tau_{t} = \\frac{1}{2} \\times 5 = \\frac{5}{2}\n$$\n规则是如果 $N_{\\mathrm{eff}}  \\tau_{t}$ 则进行重采样。我们将计算出的 $N_{\\mathrm{eff}}$ 与阈值 $\\tau_t$ 进行比较：\n$$\nN_{\\mathrm{eff}} = \\frac{25}{9} \\quad \\text{和} \\quad \\tau_{t} = \\frac{5}{2}\n$$\n为了比较这两个分数，我们可以找到一个公分母，即 $18$：\n$$\nN_{\\mathrm{eff}} = \\frac{25 \\times 2}{9 \\times 2} = \\frac{50}{18}\n$$\n$$\n\\tau_{t} = \\frac{5 \\times 9}{2 \\times 9} = \\frac{45}{18}\n$$\n因为 $\\frac{50}{18} > \\frac{45}{18}$，所以我们有 $N_{\\mathrm{eff}} > \\tau_{t}$。重采样的条件 ($N_{\\mathrm{eff}}  \\tau_{t}$) 未被满足。因此，在时间 $t$ 不应执行重采样。\n\n要求的最终答案是 $N_{\\mathrm{eff}}$ 以最简分数表示的精确值。",
            "answer": "$$\\boxed{\\frac{25}{9}}$$"
        },
        {
            "introduction": "现在，让我们通过整合传播、加权和重采样步骤，来构建自助法滤波器的完整体系。这个综合性练习要求您为一个线性高斯系统实现完整的滤波算法，该系统的精确解可以通过卡尔曼滤波器得到。通过将您的粒子滤波器输出与这一“黄金标准”进行比较，您将对滤波器的准确性、收敛特性及其在不同条件下的性能获得宝贵的直观认识。",
            "id": "3338910",
            "problem": "考虑一个标量线性高斯状态空间模型，其生成过程定义如下：对于时间索引 $t \\in \\{1,2,\\dots,T\\}$，潜变量状态 $x_t$ 和观测值 $y_t$ 按以下方式演化\n$$\nx_t = a\\,x_{t-1} + u_t,\\quad u_t \\sim \\mathcal{N}(0,q),\n$$\n$$\ny_t = c\\,x_t + v_t,\\quad v_t \\sim \\mathcal{N}(0,r),\n$$\n初始先验为 $x_0 \\sim \\mathcal{N}(m_0,P_0)$。目标是使用序列重要性重采样 (Sequential Importance Resampling, SIR)（也称为自举滤波器）来近似在固定时间 $t$ 的滤波分布 $p(x_t \\mid y_{1:t})$，并将其基于粒子的后验均值和方差与该模型的精确卡尔曼滤波器产生的结果进行比较。\n\n从后验的基础贝叶斯滤波递归出发，\n$$\np(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t)\\int p(x_t \\mid x_{t-1})\\,p(x_{t-1} \\mid y_{1:t-1})\\,\\mathrm{d}x_{t-1},\n$$\n设计并实现一个算法，该算法：\n- 对于指定的参数 $(a,c,q,r,m_0,P_0)$，使用固定的随机种子生成长度为 $T$ 的潜变量状态和观测序列的单个实现。\n- 实现自举滤波器，其提议分布等于转移先验 $p(x_t \\mid x_{t-1})$，并在每个时间步进行多项式重采样。在时间 $t$，从重采样前的归一化重要性权重计算 $p(x_t \\mid y_{1:t})$ 的基于粒子的后验均值和方差。\n- 对同一模型实现精确卡尔曼滤波器，并在给定 $y_{1:t}$ 的情况下，计算在时间 $t$ 的精确后验均值和方差。\n- 计算在时间 $t$ 的基于粒子的后验均值和方差与卡尔曼滤波器后验均值和方差之间的绝对误差。\n\n您的程序必须评估以下测试套件。对于每种情况，根据指定的参数和种子模拟单个轨迹，然后在指定的时间 $t$ 进行所述比较。通过将粒子滤波器的种子偏移+1，为模拟和粒子滤波器使用独立的随机数生成。\n\n测试套件（每种情况指定 $(a,c,q,r,m_0,P_0,T,t,N,\\text{seed})$）：\n1. 正常情况，中等噪声，大粒子数：$(a,c,q,r,m_0,P_0,T,t,N,\\text{seed}) = (0.9,\\,1.0,\\,0.5,\\,0.5,\\,0.0,\\,1.0,\\,25,\\,25,\\,1000,\\,11)$。\n2. 边界情况，极少粒子（退化压力测试）：$(a,c,q,r,m_0,P_0,T,t,N,\\text{seed}) = (0.9,\\,1.0,\\,0.5,\\,0.5,\\,0.0,\\,1.0,\\,25,\\,25,\\,10,\\,11)$，使用与情况1相同的模拟数据（相同的种子），但粒子数较少。\n3. 高观测噪声场景：$(a,c,q,r,m_0,P_0,T,t,N,\\text{seed}) = (0.9,\\,1.0,\\,0.5,\\,3.0,\\,0.0,\\,1.0,\\,25,\\,25,\\,500,\\,22)$。\n4. 近乎确定的观测（极小观测噪声）：$(a,c,q,r,m_0,P_0,T,t,N,\\text{seed}) = (0.9,\\,1.0,\\,0.5,\\,0.01,\\,0.0,\\,1.0,\\,25,\\,25,\\,200,\\,33)$。\n\n对于每种情况，您的程序应计算两个量：绝对均值误差 $| \\hat{\\mu}_t - m_t |$ 和绝对方差误差 $| \\hat{V}_t - P_t |$，其中 $\\hat{\\mu}_t$ 和 $\\hat{V}_t$ 是在时间 $t$ 的基于粒子的后验均值和方差，而 $m_t$ 和 $P_t$ 是在时间 $t$ 的卡尔曼滤波器后验均值和方差。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有测试用例的结果，形式为方括号括起来的逗号分隔列表，顺序遵循测试套件的顺序，并且在每种情况下为 $(\\text{均值误差}, \\text{方差误差})$。例如，最终输出必须具有以下形式：\n$$\n[\\text{err\\_mean\\_case1},\\text{err\\_var\\_case1},\\text{err\\_mean\\_case2},\\text{err\\_var\\_case2},\\text{err\\_mean\\_case3},\\text{err\\_var\\_case3},\\text{err\\_mean\\_case4},\\text{err\\_var\\_case4}]\n$$\n每个条目都是一个实数。",
            "solution": "用户希望通过以下步骤解决问题：\n1.  **问题验证**：首先，我将验证用户的问题陈述。\n2.  **详细解释**：如果问题有效，我将提供解决方案的详细解释。\n3.  **代码实现**：最后，我将编写 Python 代码来解决该问题。\n\n### 第 1 步：问题验证\n\n我已分析用户提供的问题陈述，并根据所需标准对其进行了验证。\n\n*   **提取已知条件**：\n    *   **模型**：一个标量线性高斯状态空间模型由以下公式定义：\n        *   状态转移方程：$x_t = a\\,x_{t-1} + u_t$，其中 $u_t \\sim \\mathcal{N}(0,q)$。\n        *   观测方程：$y_t = c\\,x_t + v_t$，其中 $v_t \\sim \\mathcal{N}(0,r)$。\n        *   初始先验：$x_0 \\sim \\mathcal{N}(m_0,P_0)$。\n    *   **任务**：使用自举滤波器近似滤波分布 $p(x_t \\mid y_{1:t})$，并将其后验均值和方差与精确卡尔曼滤波器的结果进行比较。\n    *   **贝叶斯滤波递归**：$p(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t)\\int p(x_t \\mid x_{t-1})\\,p(x_{t-1} \\mid y_{1:t-1})\\,\\mathrm{d}x_{t-1}$。\n    *   **自举滤波器规格**：\n        *   提议分布：$p(x_t \\mid x_{t-1})$。\n        *   重采样方法：每一步都进行多项式重采样。\n        *   统计量计算：根据*重采样前*的归一化重要性权重计算。\n    *   **随机数生成**：使用给定的 `seed` 模拟数据。使用 `seed + 1` 实现粒子滤波器。\n    *   **比较**：在指定时间 $t$ 计算绝对误差 $|\\hat{\\mu}_t - m_t|$ 和 $|\\hat{V}_t - P_t|$。\n    *   **测试套件**：提供了四个测试用例，每个都指定了 $(a,c,q,r,m_0,P_0,T,t,N,\\text{seed})$。用例2必须使用与用例1相同的模拟数据。\n\n*   **验证结论**：\n    1.  **科学依据充分**：该问题是合理的。它涉及比较统计信号处理中一个基本模型的精确解析解（卡尔曼滤波器）和数值近似方法（自举滤波器）。这两种方法都是标准且成熟的。\n    2.  **问题适定**：该问题是适定的。输入被明确定义，并且在给定的算法和随机种子下，期望的输出（绝对误差）是唯一可计算的。\n    3.  **客观性**：该问题以客观的数学语言陈述。\n    4.  **完整性**：该问题是自包含的，并提供了实现所需的所有必要参数和规格。\n    5.  **相关性**：该问题与序列重要性重采样和随机模拟方法的指定主题直接相关。\n\n*   **结论**：该问题是**有效的**。我将继续设计和解释解决方案。\n\n### 第 2 步：详细解释\n\n此问题要求实现并比较两种用于线性高斯状态空间模型的标准滤波算法：精确的卡尔曼滤波器和近似的自举粒子滤波器。\n\n**1. 状态空间模型和滤波问题**\n系统由一个潜（未观测）状态 $x_t$ 描述，它随时间 $t$ 根据一个线性随机过程演化；以及一个观测值 $y_t$，它是当前状态的一个带噪声的线性函数。模型方程为：\n$$\nx_t = a\\,x_{t-1} + u_t, \\quad u_t \\sim \\mathcal{N}(0,q) \\quad \\text{(状态方程)}\n$$\n$$\ny_t = c\\,x_t + v_t, \\quad v_t \\sim \\mathcal{N}(0,r) \\quad \\text{(观测方程)}\n$$\n滤波问题是递归地估计当前状态的后验概率分布 $p(x_t \\mid y_{1:t})$，给定截至当前时间的所有观测值 $y_{1:t} = \\{y_1, \\dots, y_t\\}$。这个后验分布代表了我们在整合了测量值 $y_t$ 的信息后对状态 $x_t$ 的信念。一般的贝叶斯滤波递归将此问题分解为两步过程：\n\n*   **预测**：使用状态转移模型，根据前一个后验来预测当前状态的分布：\n    $$\n    p(x_t \\mid y_{1:t-1}) = \\int p(x_t \\mid x_{t-1})\\,p(x_{t-1} \\mid y_{1:t-1})\\,\\mathrm{d}x_{t-1}\n    $$\n*   **更新**：使用最新的观测值 $y_t$，通过贝叶斯定理将预测分布更新为新的后验分布：\n    $$\n    p(x_t \\mid y_{1:t}) = \\frac{p(y_t \\mid x_t)\\,p(x_t \\mid y_{1:t-1})}{p(y_t \\mid y_{1:t-1})} \\propto p(y_t \\mid x_t)\\,p(x_t \\mid y_{1:t-1})\n    $$\n\n**2. 卡尔曼滤波器：精确解**\n对于线性高斯模型，所有涉及的分布（先验、后验、似然）都是高斯分布。这使得贝叶斯递归有了一个精确的闭式解。卡尔曼滤波器通过预测和更新步骤传播这些高斯分布的均值和方差。\n\n从上一步的后验 $p(x_{t-1} \\mid y_{1:t-1}) = \\mathcal{N}(x_{t-1}; m_{t-1|t-1}, P_{t-1|t-1})$ 开始，算法在每个时间 $t$ 按如下方式进行：\n\n*   **预测步骤**：\n    *   预测均值：$m_{t|t-1} = a \\cdot m_{t-1|t-1}$\n    *   预测方差：$P_{t|t-1} = a^2 \\cdot P_{t-1|t-1} + q$\n    这给出了预测分布 $p(x_t \\mid y_{1:t-1}) = \\mathcal{N}(x_t; m_{t|t-1}, P_{t|t-1})$。\n\n*   **更新步骤**：\n    *   新息（残差）：$\\tilde{y}_t = y_t - c \\cdot m_{t|t-1}$\n    *   新息方差：$S_t = c^2 \\cdot P_{t|t-1} + r$\n    *   卡尔曼增益：$K_t = \\frac{P_{t|t-1} \\cdot c}{S_t}$\n    *   更新（后验）均值：$m_{t|t} = m_{t|t-1} + K_t \\cdot \\tilde{y}_t$\n    *   更新（后验）方差：$P_{t|t} = (1 - K_t \\cdot c) \\cdot P_{t|t-1}$\n    这给出了时间 $t$ 的最终后验 $p(x_t \\mid y_{1:t}) = \\mathcal{N}(x_t; m_{t|t}, P_{t|t})$。算法以 $m_{0|0} = m_0$ 和 $P_{0|0} = P_0$ 初始化。\n\n**3. 自举滤波器：蒙特卡洛近似**\n对于非线性/非高斯模型，贝叶斯递归中的积分通常是难解的。序列重要性重采样 (SIR)，一种粒子滤波器，提供了一种数值近似方法。自举滤波器是一种特定的 SIR 滤波器，其中提议分布是状态转移先验。\n\n核心思想是用一组 $N$ 个加权随机样本，或称“粒子”，$\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ 来表示后验分布 $p(x_t \\mid y_{1:t})$。该分布被近似为一个离散测度：\n$$\np(x_t \\mid y_{1:t}) \\approx \\sum_{i=1}^N \\tilde{w}_t^{(i)} \\delta(x_t - x_t^{(i)})\n$$\n其中 $\\tilde{w}_t^{(i)}$ 是归一化权重，$\\delta(\\cdot)$ 是狄拉克δ函数。算法递归地更新这些粒子和权重：\n\n*   **初始化 ($t=0$)**：从初始先验中抽取 $N$ 个粒子：$x_0^{(i)} \\sim \\mathcal{N}(m_0, P_0)$。\n\n*   **递归步骤 (for $t=1, \\dots, T$)**：\n    1.  **传播**：对于上一步（重采样后）的每个粒子 $x_{t-1}^{(i)}$，使用状态转移模型将其向前传播以获得新粒子：\n        $$\n        x_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)}) \\implies x_t^{(i)} = a \\cdot x_{t-1}^{(i)} + u_t^{(i)}, \\quad u_t^{(i)} \\sim \\mathcal{N}(0,q)\n        $$\n    2.  **加权**：根据每个新粒子 $x_t^{(i)}$ 对当前观测值 $y_t$ 的解释程度，为其分配一个重要性权重。由于提议分布是先验分布，权重与给定粒子下观测值的似然成正比：\n        $$\n        w_t^{(i)} \\propto p(y_t \\mid x_t^{(i)}) = \\mathcal{N}(y_t; c \\cdot x_t^{(i)}, r)\n        $$\n        然后将权重归一化：$\\tilde{w}_t^{(i)} = w_t^{(i)} / \\sum_{j=1}^N w_t^{(j)}$。\n    3.  **计算统计量（重采样前）**：按要求，从加权粒子集中计算后验均值 $\\hat{\\mu}_t$ 和方差 $\\hat{V}_t$：\n        $$\n        \\hat{\\mu}_t = \\sum_{i=1}^N \\tilde{w}_t^{(i)} x_t^{(i)}\n        $$\n        $$\n        \\hat{V}_t = \\sum_{i=1}^N \\tilde{w}_t^{(i)} (x_t^{(i)} - \\hat{\\mu}_t)^2\n        $$\n    4.  **重采样**：粒子滤波中的一个关键问题是权重退化，即经过几步后，一个粒子的权重接近1，而所有其他粒子的权重都接近0。重采样通过用一个新集合替换当前粒子集合来缓解这个问题，新集合是根据其归一化权重从当前粒子中有放回地抽取的。权重高的粒子很可能被多次选中，而权重低的粒子很可能被丢弃。这一步有效地将计算资源集中在状态空间中更有希望的区域。这里使用多项式重采样，即我们根据概率 $\\{\\tilde{w}_t^{(i)}\\}_{i=1}^N$ 从集合 $\\{x_t^{(i)}\\}_{i=1}^N$ 中有放回地抽取 $N$ 个新粒子。重采样后，新粒子集成为下一个时间步传播阶段的输入。\n\n**4. 实现策略**\n实现将包括三个主要函数：一个用于生成数据，一个用于卡尔曼滤波器，一个用于自举滤波器。程序的主体部分将遍历测试用例，调用这些函数，计算绝对误差 $| \\hat{\\mu}_t - m_t |$ 和 $| \\hat{V}_t - P_t |$，并收集结果以供最终输出。为了处理两个测试使用相同模拟数据的情况，生成的数据将根据其随机种子进行缓存。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _generate_data(a, c, q, r, m0, P0, T, seed):\n    \"\"\"\n    Generates a single realization of the latent state and observation sequences.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # x has T+1 elements (x_0 to x_T)\n    x = np.zeros(T + 1)\n    # y has T+1 elements, index 0 is unused for 1-based time indexing\n    y = np.zeros(T + 1)\n    \n    # Initial state x_0\n    x[0] = rng.normal(loc=m0, scale=np.sqrt(P0))\n    \n    for t in range(1, T + 1):\n        # State transition\n        x[t] = a * x[t-1] + rng.normal(loc=0, scale=np.sqrt(q))\n        # Observation\n        y[t] = c * x[t] + rng.normal(loc=0, scale=np.sqrt(r))\n        \n    return x, y\n\ndef _kalman_filter(a, c, q, r, m0, P0, T, y_obs):\n    \"\"\"\n    Implements the exact Kalman filter for the scalar linear Gaussian model.\n    \"\"\"\n    m_post = np.zeros(T + 1)\n    P_post = np.zeros(T + 1)\n    \n    # Initialize with prior at t=0\n    m_post[0] = m0\n    P_post[0] = P0\n    \n    for t in range(1, T + 1):\n        # Prediction step\n        m_pred = a * m_post[t-1]\n        P_pred = a**2 * P_post[t-1] + q\n        \n        # Update step\n        residual = y_obs[t] - c * m_pred\n        S = c**2 * P_pred + r\n        K = (P_pred * c) / S\n        \n        m_post[t] = m_pred + K * residual\n        P_post[t] = (1 - K * c) * P_pred\n        \n    return m_post, P_post\n\ndef _bootstrap_filter(a, c, q, r, m0, P0, T, t_target, N, y_obs, seed):\n    \"\"\"\n    Implements the bootstrap filter (Sequential Importance Resampling).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Initialization (t=0): Draw particles from the prior\n    particles = rng.normal(loc=m0, scale=np.sqrt(P0), size=N)\n    \n    post_mean_at_t = None\n    post_var_at_t = None\n\n    # Pre-calculate constant part of log-likelihood for efficiency\n    log_likelihood_const = -0.5 * np.log(2 * np.pi * r)\n\n    for t in range(1, T + 1):\n        # 1. Propagate particles using the state transition model\n        noise = rng.normal(loc=0, scale=np.sqrt(q), size=N)\n        particles = a * particles + noise\n        \n        # 2. Weight particles based on the likelihood of the observation\n        log_weights = log_likelihood_const - (y_obs[t] - c * particles)**2 / (2 * r)\n\n        # Normalize weights using log-sum-exp trick for numerical stability\n        max_log_weight = np.max(log_weights)\n        weights = np.exp(log_weights - max_log_weight)\n        normalized_weights = weights / np.sum(weights)\n        \n        # 3. Compute statistics (before resampling) if at the target time\n        if t == t_target:\n            # Weighted mean\n            post_mean_at_t = np.sum(normalized_weights * particles)\n            # Weighted variance\n            post_var_at_t = np.sum(normalized_weights * (particles - post_mean_at_t)**2)\n\n        # 4. Resample particles to combat weight degeneracy\n        # The new set of particles is drawn with replacement based on their weights\n        indices = rng.choice(N, size=N, p=normalized_weights)\n        particles = particles[indices]\n        # After resampling, weights are implicitly reset to 1/N for the next step.\n        \n    return post_mean_at_t, post_var_at_t\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a,  c,   q,   r,   m0, P0,  T,  t, N,    seed)\n        (0.9, 1.0, 0.5, 0.5, 0.0, 1.0, 25, 25, 1000, 11),\n        (0.9, 1.0, 0.5, 0.5, 0.0, 1.0, 25, 25, 10,   11),\n        (0.9, 1.0, 0.5, 3.0, 0.0, 1.0, 25, 25, 500,  22),\n        (0.9, 1.0, 0.5, 0.01, 0.0, 1.0, 25, 25, 200, 33),\n    ]\n\n    all_results = []\n    \n    # A cache to store generated data to ensure case 2 uses data from case 1\n    data_cache = {}\n    \n    for case in test_cases:\n        a, c, q, r, m0, P0, T, t_target, N, seed = case\n        \n        # 1. Generate data or retrieve from cache if the seed is repeated\n        if seed in data_cache:\n            x_true, y_obs = data_cache[seed]\n        else:\n            # The parameters for data generation are (a, c, q, r, m0, P0, T)\n            data_gen_params = (a, c, q, r, m0, P0, T, seed)\n            x_true, y_obs = _generate_data(*data_gen_params)\n            data_cache[seed] = (x_true, y_obs)\n            \n        # 2. Run Kalman Filter for the exact posterior\n        # The parameters for the KF model are (a, c, q, r, m0, P0)\n        kf_params = (a, c, q, r, m0, P0)\n        m_kf_seq, P_kf_seq = _kalman_filter(*kf_params, T, y_obs)\n        m_t_exact = m_kf_seq[t_target]\n        P_t_exact = P_kf_seq[t_target]\n        \n        # 3. Run Bootstrap Filter for the approximate posterior\n        # The seed for the particle filter is offset by +1 as specified\n        pf_seed = seed + 1\n        pf_params = (a, c, q, r, m0, P0)\n        mu_hat, V_hat = _bootstrap_filter(*pf_params, T, t_target, N, y_obs, pf_seed)\n        \n        # 4. Compute absolute errors and store results\n        err_mean = np.abs(mu_hat - m_t_exact)\n        err_var = np.abs(V_hat - P_t_exact)\n        \n        all_results.append(err_mean)\n        all_results.append(err_var)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}