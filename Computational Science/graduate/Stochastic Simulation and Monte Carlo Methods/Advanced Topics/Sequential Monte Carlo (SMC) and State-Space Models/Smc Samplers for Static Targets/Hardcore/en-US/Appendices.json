{
    "hands_on_practices": [
        {
            "introduction": "The resampling step is a cornerstone of SMC methods, essential for preventing the collapse of the particle system to a single unique particle. However, this step introduces additional Monte Carlo variance. This exercise provides a concrete, hands-on calculation to compare the variance in offspring counts for four common resampling schemes, building intuition about their relative statistical efficiency .",
            "id": "3345072",
            "problem": "Consider a Sequential Monte Carlo (SMC) sampler for a static target distribution. At a given iteration, suppose a weighted particle system has particle weights $\\mathbf{w} = (0.10,\\,0.25,\\,0.05,\\,0.30,\\,0.30)$ for $K=5$ distinct particles, and the total number of offspring to be generated is $N=12$. Let $N_i$ denote the random number of offspring assigned to particle $i$ by the resampling step, conditional on the weight vector $\\mathbf{w}$. Focus on particle $i=3$, which has weight $w_3 = 0.05$. Using the standard definitions of multinomial resampling, stratified resampling, residual resampling, and systematic resampling, compute the conditional variance $\\operatorname{Var}(N_3 \\mid \\mathbf{w})$ under each scheme.\n\nProvide the four variances as exact values and then rank the schemes by ascending conditional variance. Express your final answer as a single row matrix listing the four variances in ascending order (left-to-right), with entries corresponding to the schemes in ascending order of variance. No rounding is required. No physical units are involved.",
            "solution": "The problem requires the computation and comparison of the conditional variance of the number of offspring for a specific particle, denoted $N_3$, under four distinct resampling schemes in a Sequential Monte Carlo (SMC) framework. The given parameters are the total number of offspring to be generated, $N=12$, and the vector of normalized particle weights $\\mathbf{w} = (0.10, 0.25, 0.05, 0.30, 0.30)$ for $K=5$ particles. We are interested in particle $i=3$, which has a weight of $w_3 = 0.05$. The sum of weights is $\\sum_{i=1}^5 w_i = 0.10 + 0.25 + 0.05 + 0.30 + 0.30 = 1.00$, confirming they are normalized. We will calculate the quantity $\\operatorname{Var}(N_3 \\mid \\mathbf{w})$ for each of the four specified resampling schemes.\n\n**1. Multinomial Resampling**\nIn multinomial resampling, the vector of offspring counts $(N_1, N_2, \\dots, N_K)$ is a random draw from a multinomial distribution with $N$ trials and success probabilities given by the weight vector $\\mathbf{w}$.\n$$\n(N_1, N_2, \\dots, N_K) \\sim \\text{Multinomial}(N, \\mathbf{w})\n$$\nThe marginal distribution of the number of offspring $N_i$ for a single particle $i$ is a binomial distribution with $N$ trials and probability of success $w_i$.\n$$\nN_i \\sim \\text{Binomial}(N, w_i)\n$$\nThe variance of a binomial distribution $\\text{Binomial}(n, p)$ is given by $np(1-p)$. Therefore, the conditional variance of $N_i$ is:\n$$\n\\operatorname{Var}(N_i \\mid \\mathbf{w}) = N w_i (1-w_i)\n$$\nFor particle $i=3$, with $N=12$ and $w_3=0.05$, the variance is:\n$$\n\\operatorname{Var}(N_3 \\mid \\mathbf{w}) = 12 \\times 0.05 \\times (1 - 0.05) = 12 \\times 0.05 \\times 0.95 = 0.6 \\times 0.95 = 0.57\n$$\n\n**2. Stratified Resampling**\nIn stratified resampling, the interval $[0,1)$ is divided into $N$ strata of equal length $1/N$. A single uniform random number is drawn from each stratum. These $N$ ordered pointers are then used to select ancestors. The number of offspring $N_i$ for particle $i$ is a random variable that can only take two adjacent integer values: $\\lfloor N w_i \\rfloor$ or $\\lceil N w_i \\rceil$.\nLet $N w_i = I_i + f_i$, where $I_i = \\lfloor N w_i \\rfloor$ is the integer part and $f_i = \\{N w_i\\}$ is the fractional part. The number of offspring is given by $N_i = I_i$ with probability $1-f_i$ and $N_i = I_i+1$ with probability $f_i$. This implies that the random variable $N_i - I_i$ follows a Bernoulli distribution with parameter $f_i$.\n$$\nN_i - \\lfloor N w_i \\rfloor \\sim \\text{Bernoulli}(\\{N w_i\\})\n$$\nThe variance of a Bernoulli trial with parameter $p$ is $p(1-p)$. Thus, the variance of $N_i$ is dependent only on its non-integer part when scaled by $N$:\n$$\n\\operatorname{Var}(N_i \\mid \\mathbf{w}) = \\operatorname{Var}(N_i - I_i) = f_i(1-f_i) = \\{N w_i\\}(1 - \\{N w_i\\})\n$$\nFor particle $i=3$, we first compute $N w_3 = 12 \\times 0.05 = 0.6$. The fractional part is $f_3 = \\{0.6\\} = 0.6$. The variance is:\n$$\n\\operatorname{Var}(N_3 \\mid \\mathbf{w}) = 0.6 \\times (1 - 0.6) = 0.6 \\times 0.4 = 0.24\n$$\n\n**3. Systematic Resampling**\nSystematic resampling is similar to stratified resampling, but instead of drawing $N$ independent random numbers, a single random number $u \\sim U[0,1)$ is drawn to define a grid of pointers $\\{ \\frac{u}{N}, \\frac{u+1}{N}, \\dots, \\frac{u+N-1}{N} \\}$. This introduces strong negative correlations among the offspring counts $N_i$. However, the marginal distribution for a single count $N_i$ is identical to that in stratified resampling. The number of offspring $N_i$ can still only be $\\lfloor N w_i \\rfloor$ or $\\lceil N w_i \\rceil$, and $N_i = \\lfloor N w_i \\rfloor + 1$ with probability $\\{N w_i\\}$.\nTherefore, the formula for the marginal variance of $N_i$ is the same as for stratified resampling:\n$$\n\\operatorname{Var}(N_i \\mid \\mathbf{w}) = \\{N w_i\\}(1 - \\{N w_i\\})\n$$\nFor particle $i=3$, the calculation is identical to the stratified case:\n$$\n\\operatorname{Var}(N_3 \\mid \\mathbf{w}) = 0.24\n$$\n\n**4. Residual Resampling**\nResidual resampling is a two-stage procedure designed to reduce sampling variance.\nIn the first stage, a deterministic number of offspring, $\\tilde{N}_i = \\lfloor N w_i \\rfloor$, is assigned to each particle $i$.\nThe total number of deterministically assigned offspring is $N_{det} = \\sum_{i=1}^K \\tilde{N}_i$.\nIn the second stage, the remaining $N_{res} = N - N_{det}$ offspring are distributed stochastically. This is done by performing a resampling (typically multinomial) for $N_{res}$ new particles using a new set of weights $\\mathbf{w}'$, where each $w'_i$ is proportional to the residual part of the original weights, i.e., $w'_i \\propto N w_i - \\lfloor N w_i \\rfloor = \\{N w_i\\}$. The properly normalized weights are $w'_i = \\frac{\\{N w_i\\}}{\\sum_{j=1}^K \\{N w_j\\}} = \\frac{\\{N w_i\\}}{N_{res}}$.\nThe total number of offspring for particle $i$ is $N_i = \\tilde{N}_i + R_i$, where $R_i$ is the random number of offspring from the second stage. Since $\\tilde{N}_i$ is deterministic, $\\operatorname{Var}(N_i \\mid \\mathbf{w}) = \\operatorname{Var}(R_i \\mid \\mathbf{w})$. The vector $(R_1, \\dots, R_K)$ follows a $\\text{Multinomial}(N_{res}, \\mathbf{w}')$ distribution.\nThe marginal variance of $R_i$ is thus $\\operatorname{Var}(R_i \\mid \\mathbf{w}) = N_{res} w'_i (1-w'_i)$.\n\nLet's compute the necessary quantities for all particles:\nThe scaled weights are $N\\mathbf{w} = (1.2, 3.0, 0.6, 3.6, 3.6)$.\nThe deterministic counts are $\\tilde{\\mathbf{N}} = (\\lfloor 1.2 \\rfloor, \\lfloor 3.0 \\rfloor, \\lfloor 0.6 \\rfloor, \\lfloor 3.6 \\rfloor, \\lfloor 3.6 \\rfloor) = (1, 3, 0, 3, 3)$.\n$N_{det} = 1+3+0+3+3 = 10$.\n$N_{res} = N - N_{det} = 12 - 10 = 2$.\nThe fractional parts are $\\{\\mathbf{Nw}\\} = (0.2, 0.0, 0.6, 0.6, 0.6)$. The sum of these is $0.2+0.0+0.6+0.6+0.6 = 2.0$, which correctly equals $N_{res}$.\nThe new weights for the residual resampling are $\\mathbf{w}' = \\frac{1}{2.0}(0.2, 0.0, 0.6, 0.6, 0.6) = (0.1, 0.0, 0.3, 0.3, 0.3)$.\nFor particle $i=3$, we have $w'_3 = 0.3$. The variance is:\n$$\n\\operatorname{Var}(N_3 \\mid \\mathbf{w}) = N_{res} w'_3 (1-w'_3) = 2 \\times 0.3 \\times (1 - 0.3) = 2 \\times 0.3 \\times 0.7 = 0.42\n$$\n\n**Summary and Ranking**\nThe computed conditional variances for $N_3$ are:\n- Multinomial Resampling: $0.57$\n- Stratified Resampling: $0.24$\n- Systematic Resampling: $0.24$\n- Residual Resampling: $0.42$\n\nRanking the resampling schemes by ascending conditional variance of $N_3$:\n1. Stratified Resampling (Variance = $0.24$)\n1. Systematic Resampling (Variance = $0.24$)\n3. Residual Resampling (Variance = $0.42$)\n4. Multinomial Resampling (Variance = $0.57$)\n\nThe ordered list of variances is $(0.24, 0.24, 0.42, 0.57)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.24 & 0.24 & 0.42 & 0.57 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While understanding the variance of offspring counts is insightful, our ultimate goal is to minimize the variance of the final estimator for a quantity of interest, $\\mathbb{E}_\\pi[f(X)]$. This practice moves from concrete numbers to general principles by asking you to derive the analytical expressions for the estimator's conditional variance under different resampling schemes . These derivations reveal the fundamental mathematical structure that dictates the performance of each method.",
            "id": "3345091",
            "problem": "Consider a static target distribution $\\pi$ on a measurable state space and a weighted particle approximation $\\{(X_i,W_i)\\}_{i=1}^{N}$, where $X_i \\in \\mathcal{X}$, $W_i > 0$, and $\\sum_{i=1}^{N} W_i = 1$. Let $f:\\mathcal{X} \\to \\mathbb{R}$ be a Lipschitz function with Lipschitz constant $L \\in (0,\\infty)$. You apply a resampling step to produce $N$ equally weighted offspring particles by drawing indices according to one of four resampling schemes: multinomial, residual, stratified, or systematic resampling. Define the Sequential Monte Carlo (SMC) estimator of the target expectation as\n$$\n\\hat{\\mathbb{E}}_{\\pi}[f(X)] \\;=\\; \\frac{1}{N} \\sum_{k=1}^{N} f\\bigl(X_{A_k}\\bigr),\n$$\nwhere $A_k \\in \\{1,\\dots,N\\}$ is the index selected for the $k$-th offspring, and let $f_i := f(X_i)$.\n\nYou are asked to compute the conditional variance of $\\hat{\\mathbb{E}}_{\\pi}[f(X)]$ with respect to the resampling randomness, given the fixed set $\\{(X_i,W_i)\\}_{i=1}^{N}$. Express your answer for each of the following four resampling schemes as a closed-form analytical expression in terms of $\\{f_i\\}_{i=1}^{N}$, $\\{W_i\\}_{i=1}^{N}$, and the scheme-specific constructs defined below. Your expressions must be the exact conditional variances, not bounds, and must be functions only of the provided quantities.\n\n1. Multinomial resampling: Draw counts $(n_1,\\dots,n_N)$ from a multinomial distribution with $N$ trials and cell probabilities $(W_1,\\dots,W_N)$, and set $A_k$ by expanding the counts into indices. The estimator can be written as $\\hat{\\mathbb{E}}_{\\pi}[f(X)] = \\frac{1}{N}\\sum_{i=1}^{N} n_i f_i$.\n\n2. Residual resampling: Let $a_i := \\lfloor N W_i \\rfloor$ and $R := N - \\sum_{i=1}^{N} a_i$. Deterministically allocate $a_i$ copies of index $i$, and for the remaining $R$ offspring draw counts $(m_1,\\dots,m_N)$ from a multinomial distribution with $R$ trials and probabilities $\\bar{W}_i := \\frac{N W_i - a_i}{R}$. The total count is $n_i := a_i + m_i$, and the estimator is $\\hat{\\mathbb{E}}_{\\pi}[f(X)] = \\frac{1}{N}\\sum_{i=1}^{N} n_i f_i$.\n\n3. Stratified resampling: For $k \\in \\{1,\\dots,N\\}$ draw $U_k \\sim \\mathrm{Uniform}\\bigl(\\frac{k-1}{N},\\frac{k}{N}\\bigr)$, independently across $k$, and select index $I_k$ such that $U_k$ falls into the cumulative weight interval associated with that index. Define cumulative weights $C_0 := 0$ and $C_i := \\sum_{j=1}^{i} W_j$, and for each index $i$ define its weight interval $J_i := [C_{i-1},C_i) \\subset [0,1)$. For each stratum $k$, define $I_k := \\bigl(\\frac{k-1}{N},\\frac{k}{N}\\bigr)$ and the overlap length\n$$\nc_{i,k} := \\bigl|J_i \\cap I_k\\bigr|,\n$$\nwhere $|\\cdot|$ denotes Lebesgue measure on $\\mathbb{R}$. Let $p_{i,k} := N\\, c_{i,k}$ denote the probability that stratum $k$ selects index $i$. Then the count for index $i$ can be represented as $n_i := \\sum_{k=1}^{N} \\xi_{i,k}$ where, for each fixed $k$, the random vector $(\\xi_{1,k},\\dots,\\xi_{N,k})$ is a one-hot categorical draw with $\\mathbb{P}\\{\\xi_{i,k}=1\\} = p_{i,k}$, $\\sum_{i=1}^{N} \\xi_{i,k} = 1$, and the vectors are independent across $k$. The estimator is $\\hat{\\mathbb{E}}_{\\pi}[f(X)] = \\frac{1}{N}\\sum_{i=1}^{N} n_i f_i$.\n\n4. Systematic resampling: Draw a single $V \\sim \\mathrm{Uniform}(0,1)$ and set $U_k := \\frac{k-1+V}{N}$ for $k \\in \\{1,\\dots,N\\}$. Select index $I_k$ such that $U_k$ falls into the corresponding weight interval $J_i$. For each pair $(i,k)$ define the set\n$$\nA_{i,k} := \\Bigl\\{V \\in [0,1] \\,:\\, \\frac{k-1+V}{N} \\in J_i \\Bigr\\},\n$$\nand denote its Lebesgue measure by $|A_{i,k}|$. Define the estimator as a function of $V$ by $S(V) := \\frac{1}{N}\\sum_{k=1}^{N} f\\bigl(X_{I_k(V)}\\bigr)$, where $I_k(V)$ is the selected index at stratum $k$ given $V$. The conditional variance is understood as $\\operatorname{Var}\\bigl(S(V)\\,\\big|\\,\\{(X_i,W_i)\\}_{i=1}^{N}\\bigr) = \\int_{0}^{1} \\bigl(S(V) - \\sum_{i=1}^{N} W_i f_i\\bigr)^{2}\\,\\mathrm{d}V$.\n\nDerive the exact conditional variance $\\operatorname{Var}\\bigl(\\hat{\\mathbb{E}}_{\\pi}[f(X)]\\,\\big|\\,\\{(X_i,W_i)\\}_{i=1}^{N}\\bigr)$ for each scheme as an analytical expression. Your final answer must present the four expressions in the order listed above, as a single row matrix. No numerical approximation is required.",
            "solution": "The problem asks for the conditional variance of the Sequential Monte Carlo (SMC) estimator of $\\mathbb{E}_{\\pi}[f(X)]$ for four different resampling schemes, given a fixed set of weighted particles $\\{(X_i, W_i)\\}_{i=1}^{N}$. The conditioning implies that the particles $X_i$, their corresponding weights $W_i$, and the function values $f_i := f(X_i)$ are treated as fixed constants. The only source of randomness is the resampling mechanism itself. The Lipschitz property of the function $f$ is a typical assumption for proving theoretical properties of SMC methods, but it is not required for the specific variance calculation at hand.\n\nThe general form of the estimator after resampling is $\\hat{\\mathbb{E}}_{\\pi}[f(X)] = \\frac{1}{N} \\sum_{k=1}^{N} f(X_{A_k})$, where $A_k$ is the index of the parent of the $k$-th new particle. Let $Y_k = f(X_{A_k})$ be the function value associated with the $k$-th draw. The estimator is the sample mean $\\frac{1}{N}\\sum_{k=1}^N Y_k$. Its conditional variance is $\\operatorname{Var}\\left(\\frac{1}{N}\\sum_{k=1}^N Y_k\\right) = \\frac{1}{N^2} \\operatorname{Var}\\left(\\sum_{k=1}^N Y_k\\right) = \\frac{1}{N^2} \\sum_{k=1}^N \\sum_{l=1}^N \\operatorname{Cov}(Y_k, Y_l)$.\n\nAnother perspective is to consider the number of times, $n_i$, each particle $i$ is chosen as a parent. The estimator can then be written as $\\hat{\\mathbb{E}}_{\\pi}[f(X)] = \\frac{1}{N} \\sum_{i=1}^{N} n_i f_i$. The variance is $\\operatorname{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N} n_i f_i\\right) = \\frac{1}{N^2} \\operatorname{Var}\\left(\\sum_{i=1}^{N} n_i f_i\\right) = \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j=1}^N f_i f_j \\operatorname{Cov}(n_i, n_j)$.\n\nWe will now compute this variance for each of the four specified schemes. Let $\\bar{f} = \\sum_{i=1}^{N} W_i f_i$ denote the expectation of the estimator, which is the same for all unbiased resampling schemes.\n\n**1. Multinomial Resampling**\nIn multinomial resampling, the $N$ offspring indices $\\{A_k\\}_{k=1}^N$ are drawn independently and identically (i.i.d.) from the categorical distribution over $\\{1, \\dots, N\\}$ with probabilities $\\{W_1, \\dots, W_N\\}$.\nTherefore, the random variables $Y_k = f(X_{A_k})$ are i.i.d. for $k=1, \\dots, N$.\nThe expectation of each $Y_k$ is $\\mathbb{E}[Y_k] = \\sum_{i=1}^{N} W_i f_i = \\bar{f}$.\nThe variance of each $Y_k$ is $\\operatorname{Var}(Y_k) = \\mathbb{E}[Y_k^2] - (\\mathbb{E}[Y_k])^2 = \\left(\\sum_{i=1}^{N} W_i f_i^2\\right) - \\bar{f}^2$.\nSince the $Y_k$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}\\left(\\frac{1}{N}\\sum_{k=1}^N Y_k\\right) = \\frac{1}{N^2} \\sum_{k=1}^N \\operatorname{Var}(Y_k) = \\frac{1}{N^2} (N \\operatorname{Var}(Y_1)) = \\frac{1}{N} \\operatorname{Var}(Y_1)\n$$\nSubstituting the expression for $\\operatorname{Var}(Y_1)$, we obtain the conditional variance for multinomial resampling:\n$$\nV_1 = \\frac{1}{N} \\left( \\sum_{i=1}^{N} W_i f_i^2 - \\left(\\sum_{i=1}^{N} W_i f_i\\right)^2 \\right)\n$$\n\n**2. Residual Resampling**\nThe estimator is $\\hat{\\mathbb{E}}_{\\pi}[f(X)] = \\frac{1}{N}\\sum_{i=1}^{N} n_i f_i$, where $n_i = a_i + m_i$. The counts $a_i = \\lfloor N W_i \\rfloor$ are deterministic. The randomness arises from the counts $(m_1, \\dots, m_N)$, which are drawn from a multinomial distribution with $R = N - \\sum_{i=1}^{N} a_i$ trials and probabilities $\\bar{W}_i = \\frac{N W_i - a_i}{R}$.\nThe variance is:\n$$\n\\operatorname{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N} (a_i + m_i) f_i\\right) = \\frac{1}{N^2} \\operatorname{Var}\\left(\\sum_{i=1}^{N} m_i f_i\\right)\n$$\nThe term $\\sum_{i=1}^{N} m_i f_i$ is the sum of $R$ i.i.d. random variables, where each is a draw from the discrete distribution that takes value $f_i$ with probability $\\bar{W}_i$. The logic is analogous to the multinomial case, but with $R$ trials and probabilities $\\bar{W}_i$.\nThe variance of one such draw is $\\sigma^2_{\\text{res}} = \\left(\\sum_{i=1}^{N} \\bar{W}_i f_i^2\\right) - \\left(\\sum_{i=1}^{N} \\bar{W}_i f_i\\right)^2$.\nThe variance of the sum $\\sum_{i=1}^{N} m_i f_i$ is $R \\times \\sigma^2_{\\text{res}}$, based on the properties of the multinomial distribution.\nTherefore, the conditional variance for residual resampling is:\n$$\nV_2 = \\frac{R}{N^2} \\left( \\sum_{i=1}^{N} \\bar{W}_i f_i^2 - \\left(\\sum_{i=1}^{N} \\bar{W}_i f_i\\right)^2 \\right)\n$$\nwhere $a_i = \\lfloor N W_i \\rfloor$, $R = N - \\sum_{i=1}^{N} a_i$, and $\\bar{W}_i = (N W_i - a_i)/R$.\n\n**3. Stratified Resampling**\nThe estimator is $\\hat{\\mathbb{E}}_{\\pi}[f(X)] = \\frac{1}{N} \\sum_{i=1}^{N} n_i f_i = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\sum_{k=1}^{N} \\xi_{i,k}\\right) f_i$.\nWe can swap the order of summation: $\\hat{\\mathbb{E}}_{\\pi}[f(X)] = \\frac{1}{N} \\sum_{k=1}^{N} \\left(\\sum_{i=1}^{N} \\xi_{i,k} f_i\\right)$.\nLet $Y_k = \\sum_{i=1}^{N} \\xi_{i,k} f_i$. For each stratum $k$, $(\\xi_{1,k}, \\dots, \\xi_{N,k})$ is a single categorical draw, meaning exactly one $\\xi_{i,k}$ is $1$ and the others are $0$. The probability that $\\xi_{i,k}=1$ is $p_{i,k}$. Thus, $Y_k$ is a random variable that takes the value $f_i$ with probability $p_{i,k}$.\nThe draws for different strata $k$ are independent, so the random variables $Y_1, \\dots, Y_N$ are independent.\nThe variance of the estimator is the sum of the variances of each $Y_k$, scaled by $1/N^2$:\n$$\n\\operatorname{Var}\\left(\\frac{1}{N}\\sum_{k=1}^N Y_k\\right) = \\frac{1}{N^2} \\sum_{k=1}^N \\operatorname{Var}(Y_k)\n$$\nThe variance of each $Y_k$ is $\\operatorname{Var}(Y_k) = \\mathbb{E}[Y_k^2] - (\\mathbb{E}[Y_k])^2 = \\left(\\sum_{i=1}^{N} p_{i,k} f_i^2\\right) - \\left(\\sum_{i=1}^{N} p_{i,k} f_i\\right)^2$.\nSumming over all strata $k$ gives the conditional variance for stratified resampling:\n$$\nV_3 = \\frac{1}{N^2} \\sum_{k=1}^{N} \\left( \\sum_{i=1}^{N} p_{i,k} f_i^2 - \\left(\\sum_{i=1}^{N} p_{i,k} f_i\\right)^2 \\right)\n$$\nwhere $p_{i,k} = N \\cdot |[C_{i-1},C_i) \\cap (\\frac{k-1}{N},\\frac{k}{N})|$.\n\n**4. Systematic Resampling**\nThe estimator is $S(V) = \\frac{1}{N}\\sum_{k=1}^{N} f(X_{I_k(V)})$, where $I_k(V)$ is the index selected for stratum $k$, which depends on a single uniform random variable $V \\sim \\mathrm{Uniform}(0,1)$.\nLet $Y_k(V) = f(X_{I_k(V)})$ be the value chosen in stratum $k$. The estimator is $S(V) = \\frac{1}{N}\\sum_{k=1}^N Y_k(V)$.\nThe crucial difference from stratified sampling is that the $Y_k(V)$ are not independent, as they all depend on the same $V$. We must compute the full covariance matrix.\nThe variance is $\\operatorname{Var}(S(V)) = \\frac{1}{N^2} \\sum_{k=1}^N \\sum_{l=1}^N \\operatorname{Cov}(Y_k(V), Y_l(V))$.\nThe covariance term is $\\operatorname{Cov}(Y_k(V), Y_l(V)) = \\mathbb{E}[Y_k(V)Y_l(V)] - \\mathbb{E}[Y_k(V)]\\mathbb{E}[Y_l(V)]$.\nThe index $I_k(V)$ is equal to $i$ if and only if $V \\in A_{i,k}$, where $A_{i,k} := \\{V \\in [0,1] : \\frac{k-1+V}{N} \\in J_i \\}$. Thus we can write $Y_k(V) = \\sum_{i=1}^N f_i \\mathbb{I}(V \\in A_{i,k})$, where $\\mathbb{I}(\\cdot)$ is the indicator function. The expectation is found by integrating over $V$:\n$$\n\\mathbb{E}[Y_k(V)] = \\int_0^1 \\sum_{i=1}^N f_i \\mathbb{I}(V \\in A_{i,k}) dV = \\sum_{i=1}^N f_i |A_{i,k}|\n$$\nSimilarly, the expectation of the product is:\n\\begin{align*}\n\\mathbb{E}[Y_k(V)Y_l(V)] &= \\int_0^1 \\left(\\sum_{i=1}^N f_i \\mathbb{I}(V \\in A_{i,k})\\right)\\left(\\sum_{j=1}^N f_j \\mathbb{I}(V \\in A_{j,l})\\right) dV \\\\\n&= \\sum_{i=1}^N \\sum_{j=1}^N f_i f_j \\int_0^1 \\mathbb{I}(V \\in A_{i,k} \\cap A_{j,l}) dV \\\\\n&= \\sum_{i=1}^N \\sum_{j=1}^N f_i f_j |A_{i,k} \\cap A_{j,l}|\n\\end{align*}\nCombining these gives the covariance:\n$$\n\\operatorname{Cov}(Y_k(V), Y_l(V)) = \\sum_{i=1}^N \\sum_{j=1}^N f_i f_j \\left( |A_{i,k} \\cap A_{j,l}| - |A_{i,k}| |A_{j,l}| \\right)\n$$\nThe total variance is obtained by summing these covariance terms over all pairs $(k,l)$ and dividing by $N^2$:\n$$\nV_4 = \\frac{1}{N^2} \\sum_{k=1}^{N} \\sum_{l=1}^{N} \\sum_{i=1}^{N} \\sum_{j=1}^{N} f_i f_j \\left( |A_{i,k} \\cap A_{j,l}| - |A_{i,k}| |A_{j,l}| \\right)\n$$\nThis is the exact analytical expression for the variance in terms of the provided constructs.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{N}\\left(\\sum_{i=1}^{N} W_i f_i^2 - \\left(\\sum_{i=1}^{N} W_i f_i\\right)^2\\right) & \\frac{R}{N^2}\\left(\\sum_{i=1}^{N} \\bar{W}_i f_i^2 - \\left(\\sum_{i=1}^{N} \\bar{W}_i f_i\\right)^2\\right) & \\frac{1}{N^2}\\sum_{k=1}^{N} \\left(\\sum_{i=1}^{N} p_{i,k} f_i^2 - \\left(\\sum_{i=1}^{N} p_{i,k} f_i\\right)^2\\right) & \\frac{1}{N^2}\\sum_{k=1}^{N}\\sum_{l=1}^{N}\\sum_{i=1}^{N}\\sum_{j=1}^{N} f_i f_j \\left(|A_{i,k} \\cap A_{j,l}| - |A_{i,k}||A_{j,l}|\\right) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Effective use of SMC in practice requires moving beyond analysis to design, balancing statistical accuracy with computational cost. This exercise places you in the role of an algorithm designer tasked with optimizing a tempering-based SMC sampler . By minimizing the total cost subject to an error tolerance on the normalizing constant estimate, you will explore the critical trade-offs between the number of stages, the effective sample size, and MCMC rejuvenation effort.",
            "id": "3345022",
            "problem": "Consider a Sequential Monte Carlo (SMC) sampler for a static target that employs a tempering path $\\{\\pi_{t}: t \\in [0,1]\\}$ with a discretization $\\{0=t_{0} < t_{1} < \\cdots < t_{K}=1\\}$ and increments $\\delta_{k} \\equiv t_{k}-t_{k-1}$. At each stage $k \\in \\{1,\\dots,K\\}$, the algorithm performs importance reweighting followed by resampling and then applies $m_{k}$ steps of a Markov Chain Monte Carlo (MCMC) move kernel targeting $\\pi_{t_{k}}$. Assume the following modeling primitives, which are standard small-step approximations for tempering-based SMC samplers:\n\n- The Effective Sample Size (ESS) after reweighting at stage $k$ satisfies $\\mathrm{ESS}_{k} \\approx \\dfrac{N}{1+\\omega^{2}\\delta_{k}^{2}}$, where $N$ is the number of particles and $\\omega^{2} > 0$ is a local curvature constant characterizing the variance of incremental log-weights along the path.\n- The variance of the logarithm of the estimated normalizing constant $\\hat{Z}$ produced by the SMC estimator with resampling at each stage satisfies $\\mathrm{Var}[\\ln \\hat{Z}] \\approx \\dfrac{\\omega^{2}}{N} \\sum_{k=1}^{K} \\delta_{k}^{2}$.\n- The MCMC move kernel at stage $k$ has spectral gap proportional to the local tempering step, with gap $\\gamma \\delta_{k}$ for some $\\gamma>0$ that quantifies the mixing rate per unit tempering. To reach a fixed intra-stage decorrelation level, the number of MCMC iterations scales as $m_{k} = \\dfrac{\\vartheta}{\\gamma \\delta_{k}}$ for some constant $\\vartheta>0$.\n- The computational cost per stage per particle consists of a base evaluation cost $\\eta>0$ (e.g., for weight computations) plus the cost of $m_{k}$ MCMC steps, and the total cost over the run is $C = \\sum_{k=1}^{K} N\\big(\\eta + \\dfrac{\\vartheta}{\\gamma \\delta_{k}}\\big)$.\n\nYou are tasked with designing an ESS-threshold schedule $\\{\\tau_{k}\\}_{k=1}^{K}$, where $\\tau_{k} \\in (0,1]$ is enforced by choosing $\\delta_{k}$ so that $\\dfrac{\\mathrm{ESS}_{k}}{N} = \\tau_{k}$, hence $\\tau_{k} = \\dfrac{1}{1+\\omega^{2}\\delta_{k}^{2}}$. Your objective is to minimize the total cost $C$ subject to a target error tolerance on the log-normalizing constant estimator, $\\mathrm{Var}[\\ln \\hat{Z}] \\le \\epsilon^{2}$ for a given $\\epsilon>0$, and the path constraint $\\sum_{k=1}^{K} \\delta_{k} = 1$.\n\nWork from first principles and the modeling primitives above to do the following:\n\n- Derive the optimality conditions characterizing the minimizing schedule, and show that any optimal schedule has equal step sizes, i.e., $\\delta_{k} \\equiv \\delta$ for all $k \\in \\{1,\\dots,K\\}$, hence a constant ESS fraction $\\tau_{k} \\equiv \\tau^{\\star}$.\n- Express the corresponding optimal number of stages $K^{\\star}$ and step size $\\delta^{\\star}$ in terms of $N$, $\\epsilon$, and $\\omega$.\n- Provide the closed-form expression for the optimal constant ESS fraction $\\tau^{\\star}$ as a function of $N$, $\\epsilon$, and $\\omega$.\n\nIn addition, explain how the MCMC kernel mixing rate parameter $\\gamma$ enters the first-order optimality conditions and the minimized total cost, and whether it affects the optimal ESS fraction $\\tau^{\\star}$. Your final reported answer must be the single closed-form expression for $\\tau^{\\star}$ only. No rounding is required.",
            "solution": "The problem asks us to find an optimal scheduling for a Sequential Monte Carlo (SMC) sampler to minimize computational cost subject to a variance constraint on the log-normalizing constant estimator.\n\nThe optimization problem can be formally stated as follows. We want to minimize the total cost function:\n$$\nC(\\{\\delta_{k}\\}_{k=1}^{K}, K) = \\sum_{k=1}^{K} N\\left(\\eta + \\frac{\\vartheta}{\\gamma \\delta_{k}}\\right)\n$$\nsubject to two constraints:\n1.  Variance constraint: $\\mathrm{Var}[\\ln \\hat{Z}] \\approx \\dfrac{\\omega^{2}}{N} \\sum_{k=1}^{K} \\delta_{k}^{2} \\le \\epsilon^{2}$\n2.  Path constraint: $\\sum_{k=1}^{K} \\delta_{k} = 1$\nwhere $\\delta_{k} > 0$ for all $k \\in \\{1, \\dots, K\\}$. The optimization is over the set of step sizes $\\{\\delta_{k}\\}_{k=1}^{K}$ and the number of stages $K$.\n\nWe first derive the optimality conditions for the step sizes $\\{\\delta_k\\}$ for a fixed number of stages $K$. The cost function is a sum of a constant term $N K \\eta$ and a term proportional to $\\sum_{k=1}^{K} \\frac{1}{\\delta_{k}}$. To minimize $C$, we must minimize $\\sum_{k=1}^{K} \\frac{1}{\\delta_{k}}$. At the optimum, the inequality constraint on the variance must be active. If it were not, the cost could be lowered by adjusting the $\\{\\delta_{k}\\}$ to decrease the cost term $\\sum 1/\\delta_k$ while remaining within the variance budget. Therefore, we solve the problem with the constraint $\\frac{\\omega^{2}}{N} \\sum_{k=1}^{K} \\delta_{k}^{2} = \\epsilon^{2}$.\n\nWe use the method of Lagrange multipliers. The Lagrangian $\\mathcal{L}$ for this constrained optimization problem (omitting constant terms for clarity) is:\n$$\n\\mathcal{L}(\\{\\delta_{k}\\}, \\lambda, \\mu) = \\sum_{k=1}^{K} \\frac{1}{\\delta_{k}} + \\lambda_1 \\left( \\sum_{k=1}^{K} \\delta_{k}^{2} - \\frac{N\\epsilon^{2}}{\\omega^{2}} \\right) + \\lambda_2 \\left( \\sum_{k=1}^{K} \\delta_{k} - 1 \\right)\n$$\nTo find the minimum, we compute the partial derivative of $\\mathcal{L}$ with respect to an arbitrary step size $\\delta_j$ and set it to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\delta_j} = -\\frac{1}{\\delta_j^2} + 2\\lambda_1\\delta_j + \\lambda_2 = 0\n$$\nThis condition must hold for all $j \\in \\{1, \\dots, K\\}$. Let's define a function $f(x) = -x^{-2} + 2\\lambda_1 x + \\lambda_2$. The optimality condition is $f(\\delta_j) = 0$ for all $j$. The derivative of this function is $f'(x) = 2x^{-3} + 2\\lambda_1$. The multiplier $\\lambda_1$ must be positive, since increasing the allowed variance (the right-hand side of the constraint) would lead to a lower cost. For $x > 0$ and $\\lambda_1 > 0$, $f'(x) > 0$, which means $f(x)$ is strictly increasing. A strictly monotonic function can only equal zero at a single point. Since $f(\\delta_j) = 0$ for all values of $j$, it must be that all $\\delta_j$ are equal.\n$$\n\\delta_1 = \\delta_2 = \\dots = \\delta_K \\equiv \\delta^{\\star}\n$$\nThis demonstrates that the optimal schedule must have equal step sizes. This also implies that the ESS fraction is constant across stages: $\\tau_k = \\frac{1}{1+\\omega^2 \\delta_k^2} = \\frac{1}{1+\\omega^2 (\\delta^\\star)^2} \\equiv \\tau^\\star$.\n\nNow, we can express the constraints and the cost function in terms of the constant step size $\\delta$ and the number of stages $K$.\nThe path constraint becomes:\n$$\n\\sum_{k=1}^{K} \\delta = K\\delta = 1 \\implies \\delta = \\frac{1}{K}\n$$\nThe active variance constraint becomes:\n$$\n\\frac{\\omega^2}{N} \\sum_{k=1}^{K} \\delta^2 = \\frac{\\omega^2}{N} K \\left(\\frac{1}{K}\\right)^2 = \\frac{\\omega^2}{NK} = \\epsilon^2\n$$\nFrom this, we can solve for the optimal number of stages, $K^\\star$. We treat $K$ as a continuous variable for the purpose of optimization, a standard approach in such analyses.\n$$\nK^{\\star} = \\frac{\\omega^2}{N\\epsilon^2}\n$$\nThe corresponding optimal step size $\\delta^{\\star}$ is:\n$$\n\\delta^{\\star} = \\frac{1}{K^{\\star}} = \\frac{N\\epsilon^2}{\\omega^2}\n$$\nThis solution is valid in the regime where $K^\\star \\ge 1$, i.e., when $\\omega^2/N > \\epsilon^2$, which is the case where a single step from $t=0$ to $t=1$ violates the variance tolerance, necessitating a multi-stage approach.\n\nThe total cost function can now be written as a function of $K$ only:\n$$\nC(K) = \\sum_{k=1}^{K} N\\left(\\eta + \\frac{\\vartheta}{\\gamma (1/K)}\\right) = NK\\left(\\eta + \\frac{\\vartheta K}{\\gamma}\\right) = N\\eta K + \\frac{N\\vartheta}{\\gamma}K^2\n$$\nThe derivative with respect to $K$ is $\\frac{dC}{dK} = N\\eta + \\frac{2N\\vartheta}{\\gamma}K$. Since all parameters $N, \\eta, \\vartheta, \\gamma$ are positive, $\\frac{dC}{dK} > 0$ for $K>0$. Thus, $C(K)$ is a strictly increasing function of $K$. To minimize the cost, we must choose the smallest possible value of $K$ that satisfies the variance constraint, which is precisely the value $K^\\star$ we found.\n\nNow, we can derive the expression for the optimal constant ESS fraction, $\\tau^{\\star}$. By definition:\n$$\n\\tau^{\\star} = \\frac{1}{1 + \\omega^2 (\\delta^{\\star})^2}\n$$\nSubstituting the expression for $\\delta^{\\star}$:\n$$\n\\tau^{\\star} = \\frac{1}{1 + \\omega^2 \\left(\\frac{N\\epsilon^2}{\\omega^2}\\right)^2} = \\frac{1}{1 + \\omega^2 \\frac{N^2\\epsilon^4}{\\omega^4}} = \\frac{1}{1 + \\frac{N^2\\epsilon^4}{\\omega^2}}\n$$\nThis can be written in a more compact form:\n$$\n\\tau^{\\star} = \\frac{\\omega^2}{\\omega^2 + N^2\\epsilon^4}\n$$\n\nFinally, we address the role of the MCMC kernel mixing rate parameter, $\\gamma$.\nThe first-order optimality condition, $\\frac{\\partial \\mathcal{L}}{\\partial \\delta_j} = 0$, can be written using the full cost function as:\n$$\n-\\frac{N\\vartheta}{\\gamma\\delta_j^2} + 2\\lambda_1' \\delta_j + \\lambda_2' = 0\n$$\nwhere $\\lambda_1'$ and $\\lambda_2'$ are the Lagrange multipliers. The parameter $\\gamma$ scales the first term, which arises from the MCMC cost. However, the logic that all $\\delta_j$ must be equal is based on the functional form of the equation and its monotonicity, not on the specific values of the coefficients. Therefore, the conclusion that the optimal schedule has equal step sizes is independent of $\\gamma$.\nConsequently, the optimal number of stages $K^\\star = \\frac{\\omega^2}{N\\epsilon^2}$ and the optimal step size $\\delta^\\star = \\frac{N\\epsilon^2}{\\omega^2}$ are also independent of $\\gamma$. As a direct result, the optimal ESS fraction $\\tau^\\star$, which depends only on $\\omega$, $N$, and $\\epsilon$, is not affected by the MCMC mixing rate $\\gamma$.\nThe parameter $\\gamma$ does, however, enter the minimized total cost $C^\\star = C(K^\\star)$:\n$$\nC^\\star = N\\eta K^\\star + \\frac{N\\vartheta}{\\gamma}(K^\\star)^2 = \\frac{\\eta\\omega^2}{\\epsilon^2} + \\frac{N\\vartheta}{\\gamma}\\left(\\frac{\\omega^2}{N\\epsilon^2}\\right)^2 = \\frac{\\eta\\omega^2}{\\epsilon^2} + \\frac{\\vartheta \\omega^4}{N\\gamma\\epsilon^4}\n$$\nThe minimized cost decreases as $\\gamma$ increases. This is because a larger $\\gamma$ implies a faster-mixing MCMC kernel, which reduces the number of iterations $m_k$ needed per stage and thus lowers the computational cost associated with MCMC rejuvenation.\n\nThe final answer is the closed-form expression for $\\tau^{\\star}$.",
            "answer": "$$\\boxed{\\frac{\\omega^{2}}{\\omega^{2} + N^{2}\\epsilon^{4}}}$$"
        }
    ]
}