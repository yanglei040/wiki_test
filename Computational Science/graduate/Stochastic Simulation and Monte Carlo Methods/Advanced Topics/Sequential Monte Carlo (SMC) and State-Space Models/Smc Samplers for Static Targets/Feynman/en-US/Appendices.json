{
    "hands_on_practices": [
        {
            "introduction": "The resampling step is central to the success of Sequential Monte Carlo, preventing weight degeneracy by focusing computational effort on promising regions of the state space. This practice provides a concrete exercise to understand the statistical properties of different resampling schemes by calculating the variance they introduce. By comparing multinomial, stratified, residual, and systematic resampling, you will gain a hands-on appreciation for why some schemes are preferred for their ability to reduce sampling noise .",
            "id": "3345072",
            "problem": "Consider a Sequential Monte Carlo (SMC) sampler for a static target distribution. At a given iteration, suppose a weighted particle system has particle weights $\\mathbf{w} = (0.10,\\,0.25,\\,0.05,\\,0.30,\\,0.30)$ for $K=5$ distinct particles, and the total number of offspring to be generated is $N=12$. Let $N_i$ denote the random number of offspring assigned to particle $i$ by the resampling step, conditional on the weight vector $\\mathbf{w}$. Focus on particle $i=3$, which has weight $w_3 = 0.05$. Using the standard definitions of multinomial resampling, stratified resampling, residual resampling, and systematic resampling, compute the conditional variance $\\operatorname{Var}(N_3 \\mid \\mathbf{w})$ under each scheme.\n\nProvide the four variances as exact values and then rank the schemes by ascending conditional variance. Express your final answer as a single row matrix listing the four variances in ascending order (left-to-right), with entries corresponding to the schemes in ascending order of variance. No rounding is required. No physical units are involved.",
            "solution": "The problem requires the computation and comparison of the conditional variance of the number of offspring for a specific particle, denoted $N_3$, under four distinct resampling schemes in a Sequential Monte Carlo (SMC) framework. The given parameters are the total number of offspring to be generated, $N=12$, and the vector of normalized particle weights $\\mathbf{w} = (0.10, 0.25, 0.05, 0.30, 0.30)$ for $K=5$ particles. We are interested in particle $i=3$, which has a weight of $w_3 = 0.05$. The sum of weights is $\\sum_{i=1}^5 w_i = 0.10 + 0.25 + 0.05 + 0.30 + 0.30 = 1.00$, confirming they are normalized. We will calculate the quantity $\\operatorname{Var}(N_3 \\mid \\mathbf{w})$ for each of the four specified resampling schemes.\n\n**1. Multinomial Resampling**\nIn multinomial resampling, the vector of offspring counts $(N_1, N_2, \\dots, N_K)$ is a random draw from a multinomial distribution with $N$ trials and success probabilities given by the weight vector $\\mathbf{w}$.\n$$\n(N_1, N_2, \\dots, N_K) \\sim \\text{Multinomial}(N, \\mathbf{w})\n$$\nThe marginal distribution of the number of offspring $N_i$ for a single particle $i$ is a binomial distribution with $N$ trials and probability of success $w_i$.\n$$\nN_i \\sim \\text{Binomial}(N, w_i)\n$$\nThe variance of a binomial distribution $\\text{Binomial}(n, p)$ is given by $np(1-p)$. Therefore, the conditional variance of $N_i$ is:\n$$\n\\operatorname{Var}(N_i \\mid \\mathbf{w}) = N w_i (1-w_i)\n$$\nFor particle $i=3$, with $N=12$ and $w_3=0.05$, the variance is:\n$$\n\\operatorname{Var}(N_3 \\mid \\mathbf{w}) = 12 \\times 0.05 \\times (1 - 0.05) = 12 \\times 0.05 \\times 0.95 = 0.6 \\times 0.95 = 0.57\n$$\n\n**2. Stratified Resampling**\nIn stratified resampling, the interval $[0,1)$ is divided into $N$ strata of equal length $1/N$. A single uniform random number is drawn from each stratum. These $N$ ordered pointers are then used to select ancestors. The number of offspring $N_i$ for particle $i$ is a random variable that can only take two adjacent integer values: $\\lfloor N w_i \\rfloor$ or $\\lceil N w_i \\rceil$.\nLet $N w_i = I_i + f_i$, where $I_i = \\lfloor N w_i \\rfloor$ is the integer part and $f_i = \\{N w_i\\}$ is the fractional part. The number of offspring is given by $N_i = I_i$ with probability $1-f_i$ and $N_i = I_i+1$ with probability $f_i$. This implies that the random variable $N_i - I_i$ follows a Bernoulli distribution with parameter $f_i$.\n$$\nN_i - \\lfloor N w_i \\rfloor \\sim \\text{Bernoulli}(\\{N w_i\\})\n$$\nThe variance of a Bernoulli trial with parameter $p$ is $p(1-p)$. Thus, the variance of $N_i$ is dependent only on its non-integer part when scaled by $N$:\n$$\n\\operatorname{Var}(N_i \\mid \\mathbf{w}) = \\operatorname{Var}(N_i - I_i) = f_i(1-f_i) = \\{N w_i\\}(1 - \\{N w_i\\})\n$$\nFor particle $i=3$, we first compute $N w_3 = 12 \\times 0.05 = 0.6$. The fractional part is $f_3 = \\{0.6\\} = 0.6$. The variance is:\n$$\n\\operatorname{Var}(N_3 \\mid \\mathbf{w}) = 0.6 \\times (1 - 0.6) = 0.6 \\times 0.4 = 0.24\n$$\n\n**3. Systematic Resampling**\nSystematic resampling is similar to stratified resampling, but instead of drawing $N$ independent random numbers, a single random number $u \\sim U[0,1)$ is drawn to define a grid of pointers $\\{ \\frac{u}{N}, \\frac{u+1}{N}, \\dots, \\frac{u+N-1}{N} \\}$. This introduces strong negative correlations among the offspring counts $N_i$. However, the marginal distribution for a single count $N_i$ is identical to that in stratified resampling. The number of offspring $N_i$ can still only be $\\lfloor N w_i \\rfloor$ or $\\lceil N w_i \\rceil$, and $N_i = \\lfloor N w_i \\rfloor + 1$ with probability $\\{N w_i\\}$.\nTherefore, the formula for the marginal variance of $N_i$ is the same as for stratified resampling:\n$$\n\\operatorname{Var}(N_i \\mid \\mathbf{w}) = \\{N w_i\\}(1 - \\{N w_i\\})\n$$\nFor particle $i=3$, the calculation is identical to the stratified case:\n$$\n\\operatorname{Var}(N_3 \\mid \\mathbf{w}) = 0.24\n$$\n\n**4. Residual Resampling**\nResidual resampling is a two-stage procedure designed to reduce sampling variance.\nIn the first stage, a deterministic number of offspring, $\\tilde{N}_i = \\lfloor N w_i \\rfloor$, is assigned to each particle $i$.\nThe total number of deterministically assigned offspring is $N_{det} = \\sum_{i=1}^K \\tilde{N}_i$.\nIn the second stage, the remaining $N_{res} = N - N_{det}$ offspring are distributed stochastically. This is done by performing a resampling (typically multinomial) for $N_{res}$ new particles using a new set of weights $\\mathbf{w}'$, where each $w'_i$ is proportional to the residual part of the original weights, i.e., $w'_i \\propto N w_i - \\lfloor N w_i \\rfloor = \\{N w_i\\}$. The properly normalized weights are $w'_i = \\frac{\\{N w_i\\}}{\\sum_{j=1}^K \\{N w_j\\}} = \\frac{\\{N w_i\\}}{N_{res}}$.\nThe total number of offspring for particle $i$ is $N_i = \\tilde{N}_i + R_i$, where $R_i$ is the random number of offspring from the second stage. Since $\\tilde{N}_i$ is deterministic, $\\operatorname{Var}(N_i \\mid \\mathbf{w}) = \\operatorname{Var}(R_i \\mid \\mathbf{w})$. The vector $(R_1, \\dots, R_K)$ follows a $\\text{Multinomial}(N_{res}, \\mathbf{w}')$ distribution.\nThe marginal variance of $R_i$ is thus $\\operatorname{Var}(R_i \\mid \\mathbf{w}) = N_{res} w'_i (1-w'_i)$.\n\nLet's compute the necessary quantities for all particles:\nThe scaled weights are $N\\mathbf{w} = (1.2, 3.0, 0.6, 3.6, 3.6)$.\nThe deterministic counts are $\\tilde{\\mathbf{N}} = (\\lfloor 1.2 \\rfloor, \\lfloor 3.0 \\rfloor, \\lfloor 0.6 \\rfloor, \\lfloor 3.6 \\rfloor, \\lfloor 3.6 \\rfloor) = (1, 3, 0, 3, 3)$.\n$N_{det} = 1+3+0+3+3 = 10$.\n$N_{res} = N - N_{det} = 12 - 10 = 2$.\nThe fractional parts are $\\{\\mathbf{Nw}\\} = (0.2, 0.0, 0.6, 0.6, 0.6)$. The sum of these is $0.2+0.0+0.6+0.6+0.6 = 2.0$, which correctly equals $N_{res}$.\nThe new weights for the residual resampling are $\\mathbf{w}' = \\frac{1}{2.0}(0.2, 0.0, 0.6, 0.6, 0.6) = (0.1, 0.0, 0.3, 0.3, 0.3)$.\nFor particle $i=3$, we have $w'_3 = 0.3$. The variance is:\n$$\n\\operatorname{Var}(N_3 \\mid \\mathbf{w}) = N_{res} w'_3 (1-w'_3) = 2 \\times 0.3 \\times (1 - 0.3) = 2 \\times 0.3 \\times 0.7 = 0.42\n$$\n\n**Summary and Ranking**\nThe computed conditional variances for $N_3$ are:\n- Multinomial Resampling: $0.57$\n- Stratified Resampling: $0.24$\n- Systematic Resampling: $0.24$\n- Residual Resampling: $0.42$\n\nRanking the resampling schemes by ascending conditional variance of $N_3$:\n1. Stratified Resampling (Variance = $0.24$)\n1. Systematic Resampling (Variance = $0.24$)\n3. Residual Resampling (Variance = $0.42$)\n4. Multinomial Resampling (Variance = $0.57$)\n\nThe ordered list of variances is $(0.24, 0.24, 0.42, 0.57)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.24 & 0.24 & 0.42 & 0.57 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Building upon the understanding of resampling variance, we now investigate its impact on the overall quality of our final estimates. This exercise uses a simplified two-state system to reveal a profound difference in the asymptotic behavior of estimators generated by multinomial versus residual resampling. By working through this example, you will see how a clever choice of resampling scheme can dramatically improve the convergence rate of an SMC estimator, a key insight for designing efficient samplers .",
            "id": "3345037",
            "problem": "Consider a Sequential Monte Carlo (SMC) sampler targeting a static probability measure. Let the static target measure $\\pi$ be supported on two atoms $\\{x_{1}, x_{2}\\}$ with $\\pi(\\{x_{1}\\}) = p \\in (0,1)$ and $\\pi(\\{x_{2}\\}) = 1-p$. Let $f$ be a bounded measurable function with $f(x_{1}) = f_{1} \\in \\mathbb{R}$ and $f(x_{2}) = f_{2} \\in \\mathbb{R}$. Suppose the SMC sampler uses the identity mutation kernel (i.e., the Markov transition does not move particles) and applies a single resampling step to convert a weighted particle system into an unweighted particle system of size $N$, yielding the estimator $\\hat{\\pi}_{t}^{N}(f)$, defined as the sample average of $f$ over the resampled particles.\n\nAssume that immediately prior to resampling, the weighted particle system is idealized as follows: the set of particles at $x_{1}$ collectively carry normalized total weight $p$ and the set of particles at $x_{2}$ collectively carry normalized total weight $1-p$. Consequently, the probability of drawing a parent at $x_{1}$ in any resampling draw is $p$, and at $x_{2}$ is $1-p$.\n\nTwo resampling schemes are considered:\n- Multinomial resampling: $N$ independent parent draws are made with probabilities $p$ at $x_{1}$ and $1-p$ at $x_{2}$.\n- Residual resampling: Each location receives $\\lfloor N p \\rfloor$ and $\\lfloor N(1-p) \\rfloor$ deterministic copies, respectively, and the remaining $R = N - \\lfloor N p \\rfloor - \\lfloor N(1-p) \\rfloor$ copies (with $R \\in \\{0,1\\}$ in this two-atom case) are allocated by a single random draw with probabilities proportional to the residual fractional weights.\n\nStarting only from these definitions and standard properties of binomial and Bernoulli variances, derive for each resampling scheme the asymptotic variance constant $\\sigma^{2}$ entering the Central Limit Theorem (CLT) of the form\n$$\n\\sqrt{N}\\big(\\hat{\\pi}_{t}^{N}(f) - \\pi(f)\\big) \\Longrightarrow \\mathcal{N}(0,\\sigma^{2}),\n$$\nwhere $\\Longrightarrow$ denotes convergence in distribution. Then compute the difference\n$$\n\\Delta \\sigma^{2} \\equiv \\sigma^{2}_{\\text{multinomial}} - \\sigma^{2}_{\\text{residual}}\n$$\nas a closed-form analytic expression in terms of $p$, $f_{1}$, and $f_{2}$. Express your final answer as a single closed-form expression. No rounding is required, and no units are associated with the quantity.",
            "solution": "The expected value of $f$ with respect to the target measure $\\pi$ is\n$$\n\\pi(f) = E_{\\pi}[f(X)] = f(x_1) \\pi(\\{x_1\\}) + f(x_2) \\pi(\\{x_2\\}) = p f_1 + (1-p) f_2.\n$$\nThe asymptotic variance constant $\\sigma^2$ in the Central Limit Theorem (CLT) is given by the limit\n$$\n\\sigma^2 = \\lim_{N\\to\\infty} \\mathrm{Var}\\left[\\sqrt{N}\\left(\\hat{\\pi}_{t}^{N}(f) - \\pi(f)\\right)\\right] = \\lim_{N\\to\\infty} N \\mathrm{Var}\\left[\\hat{\\pi}_{t}^{N}(f)\\right].\n$$\n\n### Asymptotic Variance for Multinomial Resampling\nIn multinomial resampling, $N$ particles are drawn independently from a categorical distribution with probabilities $(p, 1-p)$ for locations $x_1$ and $x_2$, respectively. Let the new particles be $\\{X_1, \\dots, X_N\\}$. The estimator is\n$$\n\\hat{\\pi}_{\\text{multi}}^{N}(f) = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i).\n$$\nThis is an average of $N$ independent and identically distributed (i.i.d.) random variables. Each $f(X_i)$ is a random variable $Z_i$ that takes the value $f_1$ with probability $p$ and $f_2$ with probability $1-p$.\nThe mean of $Z_i$ is $E[Z_i] = p f_1 + (1-p) f_2 = \\pi(f)$.\nThe variance of $Z_i$ is\n$$\n\\mathrm{Var}(Z_i) = E[Z_i^2] - (E[Z_i])^2 = (p f_1^2 + (1-p) f_2^2) - (p f_1 + (1-p) f_2)^2.\n$$\nExpanding this expression:\n$$\n\\mathrm{Var}(Z_i) = p f_1^2 + (1-p) f_2^2 - (p^2 f_1^2 + 2p(1-p)f_1 f_2 + (1-p)^2 f_2^2)\n$$\n$$\n= (p-p^2)f_1^2 + ( (1-p) - (1-p)^2 )f_2^2 - 2p(1-p)f_1 f_2\n$$\n$$\n= p(1-p)f_1^2 + (1-p)(1-(1-p))f_2^2 - 2p(1-p)f_1 f_2\n$$\n$$\n= p(1-p)f_1^2 + p(1-p)f_2^2 - 2p(1-p)f_1 f_2\n$$\n$$\n= p(1-p)(f_1^2 - 2f_1 f_2 + f_2^2) = p(1-p)(f_1 - f_2)^2.\n$$\nBy the standard CLT for i.i.d. samples, $\\sqrt{N}(\\hat{\\pi}_{\\text{multi}}^{N}(f) - \\pi(f)) \\Longrightarrow \\mathcal{N}(0, \\mathrm{Var}(Z_i))$.\nTherefore, the asymptotic variance for multinomial resampling is\n$$\n\\sigma^2_{\\text{multinomial}} = \\mathrm{Var}(Z_i) = p(1-p)(f_1 - f_2)^2.\n$$\n\n### Asymptotic Variance for Residual Resampling\nLet $M_1$ be the number of particles at location $x_1$ and $M_2$ be the number at $x_2$ after residual resampling, where $M_1 + M_2 = N$. The estimator is\n$$\n\\hat{\\pi}_{\\text{res}}^{N}(f) = \\frac{1}{N}(M_1 f_1 + M_2 f_2) = \\frac{1}{N}(M_1 f_1 + (N-M_1)f_2) = \\frac{M_1}{N}(f_1-f_2) + f_2.\n$$\nThe number of particles at $x_1$ is given by a deterministic part and a random part:\n$$\nM_1 = \\lfloor Np \\rfloor + R_1,\n$$\nwhere $R_1$ is the number of residual particles assigned to $x_1$. The total number of residual particles is $R = N - \\lfloor Np \\rfloor - \\lfloor N(1-p) \\rfloor$. As $p \\in (0,1)$, $Np + N(1-p) = N$. Let $\\{x\\} = x - \\lfloor x \\rfloor$ be the fractional part of $x$. Then $N = \\lfloor Np \\rfloor + \\{Np\\} + \\lfloor N(1-p) \\rfloor + \\{N(1-p)\\}$, which implies $R = \\{Np\\} + \\{N(1-p)\\}$. For any $x \\notin \\mathbb{Z}$, $\\{x\\} + \\{-x\\}=1$. Thus if $Np$ is not an integer, $\\{Np\\} + \\{N(1-p)\\} = \\{Np\\} + \\{N-Np\\} = \\{Np\\} + \\{-(\\{Np\\})\\}=1$. If $Np$ is an integer, $\\{Np\\}=0$ and $\\{N(1-p)\\}=0$, so $R=0$. Thus $R \\in \\{0, 1\\}$.\n\nThe $R$ residual particles are allocated by a single random draw.\nIf $R=0$, then $R_1=0$.\nIf $R=1$, the draw is from a distribution with probabilities proportional to the residual fractional weights, which are $\\{Np\\}$ for $x_1$ and $\\{N(1-p)\\}$ for $x_2$. The sum is $R=1$. The probability of assigning the single residual particle to $x_1$ is $p_{res} = \\frac{\\{Np\\}}{R} = \\{Np\\}$. Thus, $R_1$ is a Bernoulli random variable, $R_1 \\sim \\text{Bernoulli}(\\{Np\\})$.\n\nThe expected value of $M_1$ is $E[M_1] = \\lfloor Np \\rfloor + E[R_1]$.\nIf $R=0$, $E[R_1]=0$, so $E[M_1] = \\lfloor Np \\rfloor = Np$.\nIf $R=1$, $E[R_1]=\\{Np\\}$, so $E[M_1] = \\lfloor Np \\rfloor + \\{Np\\} = Np$.\nIn both cases, $E[M_1]=Np$.\nThe estimator is unbiased: $E[\\hat{\\pi}_{\\text{res}}^{N}(f)] = \\frac{E[M_1]}{N}(f_1-f_2) + f_2 = p(f_1-f_2) + f_2 = \\pi(f)$.\n\nNow we compute the variance. The error of the estimator is\n$$\n\\hat{\\pi}_{\\text{res}}^{N}(f) - \\pi(f) = \\left(\\frac{M_1}{N}(f_1-f_2) + f_2\\right) - (p f_1 + (1-p)f_2) = \\frac{M_1-Np}{N}(f_1-f_2).\n$$\nThe variance is\n$$\n\\mathrm{Var}\\left[\\hat{\\pi}_{\\text{res}}^{N}(f)\\right] = \\mathrm{Var}\\left[\\frac{M_1-Np}{N}(f_1-f_2)\\right] = \\frac{(f_1-f_2)^2}{N^2} \\mathrm{Var}[M_1].\n$$\nThe variance of $M_1$ is $\\mathrm{Var}[M_1] = \\mathrm{Var}[\\lfloor Np \\rfloor + R_1] = \\mathrm{Var}[R_1]$.\nIf $R=0$, $R_1=0$ deterministically, so $\\mathrm{Var}[R_1]=0$. This equals $\\{Np\\}(1-\\{Np\\})$ since $\\{Np\\}=0$.\nIf $R=1$, $R_1 \\sim \\text{Bernoulli}(\\{Np\\})$, so $\\mathrm{Var}[R_1] = \\{Np\\}(1-\\{Np\\})$.\nThus, in all cases, $\\mathrm{Var}[M_1] = \\{Np\\}(1-\\{Np\\})$.\n\nThe variance of the estimator for a fixed $N$ is\n$$\n\\mathrm{Var}\\left[\\hat{\\pi}_{\\text{res}}^{N}(f)\\right] = \\frac{(f_1-f_2)^2}{N^2} \\{Np\\}(1-\\{Np\\}).\n$$\nThe asymptotic variance constant is\n$$\n\\sigma^2_{\\text{residual}} = \\lim_{N\\to\\infty} N \\mathrm{Var}\\left[\\hat{\\pi}_{\\text{res}}^{N}(f)\\right] = \\lim_{N\\to\\infty} N \\frac{(f_1-f_2)^2}{N^2} \\{Np\\}(1-\\{Np\\})\n$$\n$$\n= \\lim_{N\\to\\infty} \\frac{(f_1-f_2)^2}{N} \\{Np\\}(1-\\{Np\\}).\n$$\nThe term $\\{Np\\}$ is a value in $[0,1)$, and $1-\\{Np\\}$ is in $(0,1]$. Thus, the product $\\{Np\\}(1-\\{Np\\})$ is bounded, specifically $\\{Np\\}(1-\\{Np\\}) \\in [0, 1/4]$. As the numerator is bounded and the denominator $N$ tends to infinity, the limit is zero.\n$$\n\\sigma^2_{\\text{residual}} = 0.\n$$\nA zero asymptotic variance constant means that $\\sqrt{N}(\\hat{\\pi}_{\\text{res}}^{N}(f) - \\pi(f))$ converges in probability to $0$. The limiting distribution is a point mass at $0$, which is a Gaussian distribution with zero variance, $\\mathcal{N}(0,0)$.\n\n### Difference in Asymptotic Variances\nWe are asked to compute $\\Delta \\sigma^{2} = \\sigma^{2}_{\\text{multinomial}} - \\sigma^{2}_{\\text{residual}}$.\nSubstituting the results derived above:\n$$\n\\Delta \\sigma^{2} = p(1-p)(f_1 - f_2)^2 - 0.\n$$\n$$\n\\Delta \\sigma^{2} = p(1-p)(f_1 - f_2)^2.\n$$\nThis result quantifies the reduction in asymptotic variance achieved by using residual resampling instead of multinomial resampling in this idealized setting. The variance due to the sampling step is effectively eliminated at the $\\mathcal{O}(1/N)$ level.",
            "answer": "$$\n\\boxed{p(1-p)(f_1-f_2)^2}\n$$"
        },
        {
            "introduction": "The final practice moves from analyzing individual components to the holistic design and tuning of an entire SMC algorithm. In practical applications, we must balance statistical accuracy with computational cost, which involves carefully choosing the tempering schedule and MCMC rejuvenation effort. This problem places you in the role of an algorithm designer, tasking you with minimizing total cost subject to a desired precision, revealing the optimal strategy for setting the adaptive ESS threshold .",
            "id": "3345022",
            "problem": "Consider a Sequential Monte Carlo (SMC) sampler for a static target that employs a tempering path $\\{\\pi_{t}: t \\in [0,1]\\}$ with a discretization $\\{0=t_{0} < t_{1} < \\cdots < t_{K}=1\\}$ and increments $\\delta_{k} \\equiv t_{k}-t_{k-1}$. At each stage $k \\in \\{1,\\dots,K\\}$, the algorithm performs importance reweighting followed by resampling and then applies $m_{k}$ steps of a Markov Chain Monte Carlo (MCMC) move kernel targeting $\\pi_{t_{k}}$. Assume the following modeling primitives, which are standard small-step approximations for tempering-based SMC samplers:\n\n- The Effective Sample Size (ESS) after reweighting at stage $k$ satisfies $\\mathrm{ESS}_{k} \\approx \\dfrac{N}{1+\\omega^{2}\\delta_{k}^{2}}$, where $N$ is the number of particles and $\\omega^{2} > 0$ is a local curvature constant characterizing the variance of incremental log-weights along the path.\n- The variance of the logarithm of the estimated normalizing constant $\\hat{Z}$ produced by the SMC estimator with resampling at each stage satisfies $\\mathrm{Var}[\\ln \\hat{Z}] \\approx \\dfrac{\\omega^{2}}{N} \\sum_{k=1}^{K} \\delta_{k}^{2}$.\n- The MCMC move kernel at stage $k$ has spectral gap proportional to the local tempering step, with gap $\\gamma \\delta_{k}$ for some $\\gamma>0$ that quantifies the mixing rate per unit tempering. To reach a fixed intra-stage decorrelation level, the number of MCMC iterations scales as $m_{k} = \\dfrac{\\vartheta}{\\gamma \\delta_{k}}$ for some constant $\\vartheta>0$.\n- The computational cost per stage per particle consists of a base evaluation cost $\\eta>0$ (e.g., for weight computations) plus the cost of $m_{k}$ MCMC steps, and the total cost over the run is $C = \\sum_{k=1}^{K} N\\big(\\eta + \\dfrac{\\vartheta}{\\gamma \\delta_{k}}\\big)$.\n\nYou are tasked with designing an ESS-threshold schedule $\\{\\tau_{k}\\}_{k=1}^{K}$, where $\\tau_{k} \\in (0,1]$ is enforced by choosing $\\delta_{k}$ so that $\\dfrac{\\mathrm{ESS}_{k}}{N} = \\tau_{k}$, hence $\\tau_{k} = \\dfrac{1}{1+\\omega^{2}\\delta_{k}^{2}}$. Your objective is to minimize the total cost $C$ subject to a target error tolerance on the log-normalizing constant estimator, $\\mathrm{Var}[\\ln \\hat{Z}] \\le \\epsilon^{2}$ for a given $\\epsilon>0$, and the path constraint $\\sum_{k=1}^{K} \\delta_{k} = 1$.\n\nWork from first principles and the modeling primitives above to do the following:\n\n- Derive the optimality conditions characterizing the minimizing schedule, and show that any optimal schedule has equal step sizes, i.e., $\\delta_{k} \\equiv \\delta$ for all $k \\in \\{1,\\dots,K\\}$, hence a constant ESS fraction $\\tau_{k} \\equiv \\tau^{\\star}$.\n- Express the corresponding optimal number of stages $K^{\\star}$ and step size $\\delta^{\\star}$ in terms of $N$, $\\epsilon$, and $\\omega$.\n- Provide the closed-form expression for the optimal constant ESS fraction $\\tau^{\\star}$ as a function of $N$, $\\epsilon$, and $\\omega$.\n\nIn addition, explain how the MCMC kernel mixing rate parameter $\\gamma$ enters the first-order optimality conditions and the minimized total cost, and whether it affects the optimal ESS fraction $\\tau^{\\star}$. Your final reported answer must be the single closed-form expression for $\\tau^{\\star}$ only. No rounding is required.",
            "solution": "The problem asks us to find an optimal scheduling for a Sequential Monte Carlo (SMC) sampler to minimize computational cost subject to a variance constraint on the log-normalizing constant estimator.\n\nThe optimization problem can be formally stated as follows. We want to minimize the total cost function:\n$$\nC(\\{\\delta_{k}\\}_{k=1}^{K}, K) = \\sum_{k=1}^{K} N\\left(\\eta + \\frac{\\vartheta}{\\gamma \\delta_{k}}\\right)\n$$\nsubject to two constraints:\n1.  Variance constraint: $\\mathrm{Var}[\\ln \\hat{Z}] \\approx \\dfrac{\\omega^{2}}{N} \\sum_{k=1}^{K} \\delta_{k}^{2} \\le \\epsilon^{2}$\n2.  Path constraint: $\\sum_{k=1}^{K} \\delta_{k} = 1$\nwhere $\\delta_{k} > 0$ for all $k \\in \\{1, \\dots, K\\}$. The optimization is over the set of step sizes $\\{\\delta_{k}\\}_{k=1}^{K}$ and the number of stages $K$.\n\nWe first derive the optimality conditions for the step sizes $\\{\\delta_k\\}$ for a fixed number of stages $K$. The cost function is a sum of a constant term $N K \\eta$ and a term proportional to $\\sum_{k=1}^{K} \\frac{1}{\\delta_{k}}$. To minimize $C$, we must minimize $\\sum_{k=1}^{K} \\frac{1}{\\delta_{k}}$. At the optimum, the inequality constraint on the variance must be active. If it were not, we could, for instance, slightly decrease the largest $\\delta_k$ and increase a smaller one, which would decrease $\\sum \\delta_k^2$, moving us further from the boundary while keeping the sum constant, or adjust the $\\delta_k$'s to decrease $\\sum 1/\\delta_k$. Therefore, we solve the problem with the constraint $\\frac{\\omega^{2}}{N} \\sum_{k=1}^{K} \\delta_{k}^{2} = \\epsilon^{2}$.\n\nWe use the method of Lagrange multipliers. The Lagrangian $\\mathcal{L}$ for this constrained optimization problem (omitting constant terms for clarity) is:\n$$\n\\mathcal{L}(\\{\\delta_{k}\\}, \\lambda_1, \\lambda_2) = \\sum_{k=1}^{K} \\frac{1}{\\delta_{k}} + \\lambda_1 \\left( \\sum_{k=1}^{K} \\delta_{k}^{2} - \\frac{N\\epsilon^{2}}{\\omega^{2}} \\right) + \\lambda_2 \\left( \\sum_{k=1}^{K} \\delta_{k} - 1 \\right)\n$$\nTo find the minimum, we compute the partial derivative of $\\mathcal{L}$ with respect to an arbitrary step size $\\delta_j$ and set it to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\delta_j} = -\\frac{1}{\\delta_j^2} + 2\\lambda_1\\delta_j + \\lambda_2 = 0\n$$\nThis condition must hold for all $j \\in \\{1, \\dots, K\\}$. Let's define a function $f(x) = -x^{-2} + 2\\lambda_1 x + \\lambda_2$. The optimality condition is $f(\\delta_j) = 0$ for all $j$. The derivative of this function is $f'(x) = 2x^{-3} + 2\\lambda_1$. The multiplier $\\lambda_1$ must be positive, since increasing the allowed variance (the right-hand side of the constraint) would lead to a lower cost. For $x > 0$ and $\\lambda_1 > 0$, $f'(x) > 0$, which means $f(x)$ is strictly increasing. A strictly monotonic function can only equal zero at a single point. Since $f(\\delta_j) = 0$ for all values of $j$, it must be that all $\\delta_j$ are equal.\n$$\n\\delta_1 = \\delta_2 = \\dots = \\delta_K \\equiv \\delta^{\\star}\n$$\nThis demonstrates that the optimal schedule must have equal step sizes. This also implies that the ESS fraction is constant across stages: $\\tau_k = \\frac{1}{1+\\omega^2 \\delta_k^2} = \\frac{1}{1+\\omega^2 (\\delta^\\star)^2} \\equiv \\tau^\\star$.\n\nNow, we can express the constraints and the cost function in terms of the constant step size $\\delta$ and the number of stages $K$.\nThe path constraint becomes:\n$$\n\\sum_{k=1}^{K} \\delta = K\\delta = 1 \\implies \\delta = \\frac{1}{K}\n$$\nThe active variance constraint becomes:\n$$\n\\frac{\\omega^2}{N} \\sum_{k=1}^{K} \\delta^2 = \\frac{\\omega^2}{N} K \\left(\\frac{1}{K}\\right)^2 = \\frac{\\omega^2}{NK} = \\epsilon^2\n$$\nFrom this, we can solve for the optimal number of stages, $K^\\star$. We treat $K$ as a continuous variable for the purpose of optimization, a standard approach in such analyses.\n$$\nK^{\\star} = \\frac{\\omega^2}{N\\epsilon^2}\n$$\nThe corresponding optimal step size $\\delta^{\\star}$ is:\n$$\n\\delta^{\\star} = \\frac{1}{K^{\\star}} = \\frac{N\\epsilon^2}{\\omega^2}\n$$\nThis solution is valid in the regime where $K^\\star \\ge 1$, i.e., when $\\omega^2/N \\ge \\epsilon^2$, which is the case where a single step from $t=0$ to $t=1$ violates the variance tolerance, necessitating a multi-stage approach.\n\nThe total cost function can now be written as a function of $K$ only:\n$$\nC(K) = \\sum_{k=1}^{K} N\\left(\\eta + \\frac{\\vartheta}{\\gamma (1/K)}\\right) = NK\\left(\\eta + \\frac{\\vartheta K}{\\gamma}\\right) = N\\eta K + \\frac{N\\vartheta}{\\gamma}K^2\n$$\nThe derivative with respect to $K$ is $\\frac{dC}{dK} = N\\eta + \\frac{2N\\vartheta}{\\gamma}K$. Since all parameters $N, \\eta, \\vartheta, \\gamma$ are positive, $\\frac{dC}{dK} > 0$ for $K>0$. Thus, $C(K)$ is a strictly increasing function of $K$. To minimize the cost, we must choose the smallest possible value of $K$ that satisfies the variance constraint, which is precisely the value $K^\\star$ we found.\n\nNow, we can derive the expression for the optimal constant ESS fraction, $\\tau^{\\star}$. By definition:\n$$\n\\tau^{\\star} = \\frac{1}{1 + \\omega^2 (\\delta^{\\star})^2}\n$$\nSubstituting the expression for $\\delta^{\\star}$:\n$$\n\\tau^{\\star} = \\frac{1}{1 + \\omega^2 \\left(\\frac{N\\epsilon^2}{\\omega^2}\\right)^2} = \\frac{1}{1 + \\omega^2 \\frac{N^2\\epsilon^4}{\\omega^4}} = \\frac{1}{1 + \\frac{N^2\\epsilon^4}{\\omega^2}}\n$$\nThis can be written in a more compact form:\n$$\n\\tau^{\\star} = \\frac{\\omega^2}{\\omega^2 + N^2\\epsilon^4}\n$$\n\nFinally, we address the role of the MCMC kernel mixing rate parameter, $\\gamma$.\nThe first-order optimality condition, $\\frac{\\partial \\mathcal{L}}{\\partial \\delta_j} = 0$, can be written using the full cost function as:\n$$\n-\\frac{N\\vartheta}{\\gamma\\delta_j^2} + 2\\lambda_1' \\delta_j + \\lambda_2' = 0\n$$\nwhere $\\lambda_1'$ and $\\lambda_2'$ are the Lagrange multipliers. The parameter $\\gamma$ scales the first term, which arises from the MCMC cost. However, the logic that all $\\delta_j$ must be equal is based on the functional form of the equation and its monotonicity, not on the specific values of the coefficients. Therefore, the conclusion that the optimal schedule has equal step sizes is independent of $\\gamma$.\nConsequently, the optimal number of stages $K^\\star = \\frac{\\omega^2}{N\\epsilon^2}$ and the optimal step size $\\delta^\\star = \\frac{N\\epsilon^2}{\\omega^2}$ are also independent of $\\gamma$. As a direct result, the optimal ESS fraction $\\tau^\\star$, which depends only on $\\omega$, $N$, and $\\epsilon$, is not affected by the MCMC mixing rate $\\gamma$.\nThe parameter $\\gamma$ does, however, enter the minimized total cost $C^\\star = C(K^\\star)$:\n$$\nC^\\star = N\\eta K^\\star + \\frac{N\\vartheta}{\\gamma}(K^\\star)^2 = \\frac{\\eta\\omega^2}{\\epsilon^2} + \\frac{N\\vartheta}{\\gamma}\\left(\\frac{\\omega^2}{N\\epsilon^2}\\right)^2 = \\frac{\\eta\\omega^2}{\\epsilon^2} + \\frac{\\vartheta \\omega^4}{N\\gamma\\epsilon^4}\n$$\nThe minimized cost decreases as $\\gamma$ increases. This is because a larger $\\gamma$ implies a faster-mixing MCMC kernel, which reduces the number of iterations $m_k$ needed per stage and thus lowers the computational cost associated with MCMC rejuvenation.\n\nThe final answer is the closed-form expression for $\\tau^{\\star}$.",
            "answer": "$$\\boxed{\\frac{\\omega^{2}}{\\omega^{2} + N^{2}\\epsilon^{4}}}$$"
        }
    ]
}