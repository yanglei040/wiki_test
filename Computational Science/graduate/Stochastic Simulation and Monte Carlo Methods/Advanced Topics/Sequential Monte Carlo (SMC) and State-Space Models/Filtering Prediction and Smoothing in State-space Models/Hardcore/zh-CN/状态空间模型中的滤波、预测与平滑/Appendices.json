{
    "hands_on_practices": [
        {
            "introduction": "粒子滤波器依靠重采样步骤来解决权重退化问题，但为了信任这些算法，我们必须理解其关键组件的统计特性。本练习 () 提供了一个分析性的实践机会，通过推导来证明残差重采样（一种旨在减小方差的常用技术）的条件无偏性。这不仅能加深您对重采样机制的理解，还能让您亲自验证算法背后的一个基本理论保证。",
            "id": "3308535",
            "problem": "考虑一个离散时间状态空间模型，其潜过程为 $\\{X_{t}\\}_{t \\geq 0}$，观测为 $\\{Y_{t}\\}_{t \\geq 1}$，分别由转移密度 $f(x_{t} \\mid x_{t-1})$ 和观测似然 $g(y_{t} \\mid x_{t})$ 给出。令 $y_{1:t}$ 表示截至时间 $t$ 观测到的固定数据。假设一种序贯蒙特卡洛（SMC）方法，也称为粒子滤波，在时间 $t$ 产生一个加权粒子系统 $\\{(x_{t}^{i}, \\tilde{w}_{t}^{i})\\}_{i=1}^{N}$，用于近似滤波分布 $p(x_{t} \\mid y_{1:t})$，其中 $\\tilde{w}_{t}^{i} \\geq 0$ 且 $\\sum_{i=1}^{N} \\tilde{w}_{t}^{i} = 1$。\n\n要求您使用残差重采样，为滤波期望 $\\int \\varphi(x) p(x \\mid y_{1:t}) \\, dx$ 构建一个条件无偏估计量，该无偏性是关于由重采样引入的随机性而言的。残差重采样的操作如下：对 $i \\in \\{1,\\dots,N\\}$ 定义整数分配 $n_{i} = \\lfloor N \\tilde{w}_{t}^{i} \\rfloor$，令 $R = N - \\sum_{i=1}^{N} n_{i}$，并计算归一化残差 $r_{i} = \\frac{N \\tilde{w}_{t}^{i} - n_{i}}{R}$（如果 $R  0$），否则 $r_{i} = 0$。构造索引 $i$ 的 $n_{i}$ 个确定性副本，然后从 $\\{1,\\dots,N\\}$ 上的分类分布（概率为 $\\{r_{i}\\}_{i=1}^{N}$）中独立抽取剩余的 $R$ 个祖先索引。将这 $N$ 个祖先索引的完整集合表示为 $A_{1:N}$，并将基于重采样的滤波期望估计量定义为\n$$\n\\widehat{I}_{\\mathrm{res}} \\;=\\; \\frac{1}{N} \\sum_{j=1}^{N} \\varphi\\!\\big(x_{t}^{A_{j}}\\big).\n$$\n\n任务：\n- 仅使用残差重采样的定义和期望的线性性，证明估计量 $\\widehat{I}_{\\mathrm{res}}$ 关于经验加权测度是条件无偏的；即证明\n$$\n\\mathbb{E}\\!\\left[ \\widehat{I}_{\\mathrm{res}} \\,\\middle|\\, x_{t}^{1:N}, \\tilde{w}_{t}^{1:N} \\right]\n\\;=\\; \\sum_{i=1}^{N} \\tilde{w}_{t}^{i} \\,\\varphi(x_{t}^{i}).\n$$\n清楚地解释为什么这构成了经验加权期望的一个无偏估计量，并讨论它在何种意义上为 $\\int \\varphi(x) p(x \\mid y_{1:t}) \\, dx$ 提供了一个估计量。\n\n- 推导残差重采样产生的确定性副本的期望数量的表达式（以权重 $\\{\\tilde{w}_{t}^{i}\\}_{i=1}^{N}$ 为条件），然后用以下权重向量为 $N$ 个粒子计算该值：\n$$\nN \\;=\\; 200, \\quad \\big(\\tilde{w}_{t}^{1},\\dots,\\tilde{w}_{t}^{6}\\big) \\;=\\; \\big(0.041,\\, 0.157,\\, 0.233,\\, 0.089,\\, 0.271,\\, 0.209\\big).\n$$\n\n您的最终答案必须是一个单一数字，等于在指定的 $N$ 和权重下，残差重采样产生的确定性副本的期望数量。无需四舍五入，最终答案中不应报告单位。",
            "solution": "问题陈述是适定的，在随机模拟领域有其科学基础，并为得到唯一解提供了所有必要信息。我们可以继续。\n\n第一个任务是证明残差重采样估计量 $\\widehat{I}_{\\mathrm{res}}$ 对于经验加权期望是条件无偏的。该估计量定义为\n$$\n\\widehat{I}_{\\mathrm{res}} = \\frac{1}{N} \\sum_{j=1}^{N} \\varphi\\big(x_{t}^{A_{j}}\\big)\n$$\n其中 $\\{A_{j}\\}_{j=1}^{N}$ 是由残差重采样算法选择的祖先索引集合。我们需要证明\n$$\n\\mathbb{E}\\!\\left[ \\widehat{I}_{\\mathrm{res}} \\,\\middle|\\, x_{t}^{1:N}, \\tilde{w}_{t}^{1:N} \\right] \\;=\\; \\sum_{i=1}^{N} \\tilde{w}_{t}^{i} \\,\\varphi(x_{t}^{i}).\n$$\n该期望以粒子位置 $\\{x_{t}^{i}\\}_{i=1}^{N}$ 及其相关的归一化权重 $\\{\\tilde{w}_{t}^{i}\\}_{i=1}^{N}$ 为条件。在此条件下，这些量被视为固定常数。唯一的随机性来源是祖先索引 $\\{A_j\\}$ 的选择。\n\n设 $K_i$ 为表示原始粒子索引 $i$ 在重采样步骤中被选为祖先的次数的随机变量。祖先索引的集合 $\\{A_j\\}_{j=1}^N$ 是一个多重集，其中索引 $i$ 出现 $K_i$ 次。因此，我们可以将估计量中的和重写为：\n$$\n\\sum_{j=1}^{N} \\varphi\\big(x_{t}^{A_{j}}\\big) = \\sum_{i=1}^{N} K_i \\varphi(x_{t}^{i}).\n$$\n那么估计量为 $\\widehat{I}_{\\mathrm{res}} = \\frac{1}{N} \\sum_{i=1}^{N} K_i \\varphi(x_{t}^{i})$。\n\n利用期望的线性性，我们有：\n$$\n\\mathbb{E}\\!\\left[ \\widehat{I}_{\\mathrm{res}} \\,\\middle|\\, x_{t}^{1:N}, \\tilde{w}_{t}^{1:N} \\right] = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\!\\left[ K_i \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right] \\varphi(x_{t}^{i}).\n$$\n我们的目标是计算 $K_i$ 的条件期望。残差重采样过程将 $K_i$ 定义为一个确定性部分和一个随机部分之和。\n确定性部分是整数副本的数量，$n_i = \\lfloor N \\tilde{w}_{t}^{i} \\rfloor$。\n随机部分来自抽取剩余的 $R = N - \\sum_{k=1}^{N} n_k$ 个粒子。这 $R$ 个粒子是从 $\\{1, \\dots, N\\}$ 上的分类分布（概率为 $\\{r_i\\}_{i=1}^N$）中独立抽取的，其中当 $R  0$ 时 $r_i = \\frac{N \\tilde{w}_{t}^{i} - n_{i}}{R}$。\n设 $K_i^{\\mathrm{rand}}$ 为在这 $R$ 次随机抽取中索引 $i$ 被选中的次数。那么，$K_i = n_i + K_i^{\\mathrm{rand}}$。\n随机变量 $K_i^{\\mathrm{rand}}$ 服从二项分布，$K_i^{\\mathrm{rand}} \\sim \\mathrm{Binomial}(R, r_i)$，因为它是 $R$ 次独立的伯努利试验中“成功”（抽到索引 $i$）的次数，每次试验的成功概率为 $r_i$。\n\n因此，$K_i$ 的条件期望为：\n$$\n\\mathbb{E}\\!\\left[ K_i \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right] = \\mathbb{E}\\!\\left[ n_i + K_i^{\\mathrm{rand}} \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right] = n_i + \\mathbb{E}\\!\\left[ K_i^{\\mathrm{rand}} \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right].\n$$\n二项随机变量 $\\mathrm{Binomial}(n,p)$ 的期望是 $np$。因此，对于 $R  0$：\n$$\n\\mathbb{E}\\!\\left[ K_i^{\\mathrm{rand}} \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right] = R \\cdot r_i = R \\cdot \\frac{N \\tilde{w}_{t}^{i} - n_{i}}{R} = N \\tilde{w}_{t}^{i} - n_{i}.\n$$\n如果 $R=0$，那么 $N = \\sum_{k=1}^N n_k = \\sum_{k=1}^N \\lfloor N \\tilde{w}_t^k \\rfloor$。由于 $\\sum_{k=1}^N N \\tilde{w}_t^k = N$，这意味着 $\\sum_{k=1}^N (N \\tilde{w}_t^k - \\lfloor N \\tilde{w}_t^k \\rfloor) = 0$。因为和中的每一项都是非负的，所以对于所有的 $k$，我们必须有 $N \\tilde{w}_t^k - \\lfloor N \\tilde{w}_t^k \\rfloor = 0$。这意味着对于所有的 $k$，$N \\tilde{w}_t^k$ 都是整数，所以 $n_k = N \\tilde{w}_t^k$。在这种情况下，$N \\tilde{w}_{t}^{i} - n_{i} = 0$。同样，当 $R=0$ 时，没有进行随机抽取，所以 $K_i^{\\mathrm{rand}} = 0$，其期望也为 0。公式 $\\mathbb{E}[K_i^{\\mathrm{rand}}] = N \\tilde{w}_{t}^{i} - n_{i}$ 对 $R=0$ 的情况也成立。\n\n将此结果代回 $\\mathbb{E}[K_i]$ 的表达式中：\n$$\n\\mathbb{E}\\!\\left[ K_i \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right] = n_i + (N \\tilde{w}_{t}^{i} - n_{i}) = N \\tilde{w}_{t}^{i}.\n$$\n最后，我们将其代入估计量期望值的表达式中：\n$$\n\\mathbb{E}\\!\\left[ \\widehat{I}_{\\mathrm{res}} \\,\\middle|\\, x_{t}^{1:N}, \\tilde{w}_{t}^{1:N} \\right] = \\frac{1}{N} \\sum_{i=1}^{N} (N \\tilde{w}_{t}^{i}) \\varphi(x_{t}^{i}) = \\sum_{i=1}^{N} \\tilde{w}_{t}^{i} \\varphi(x_{t}^{i}).\n$$\n证明完毕。\n\n这个结果意味着 $\\widehat{I}_{\\mathrm{res}}$ 是经验期望 $\\sum_{i=1}^{N} \\tilde{w}_{t}^{i} \\varphi(x_{t}^{i})$ 的一个无偏估计量，其中无偏性仅相对于重采样步骤引入的随机性而言。量 $\\sum_{i=1}^{N} \\tilde{w}_{t}^{i} \\varphi(x_{t}^{i})$ 是基于重采样前的加权粒子系统对真实滤波期望 $I_t = \\int \\varphi(x) p(x \\mid y_{1:t}) \\, dx$ 的标准蒙特卡洛近似。对于有限数量的粒子 $N$，该经验期望通常是 $I_t$ 的一个有偏估计量。然而，它是一个一致估计量，意味着在适当的正则性条件下，当 $N \\to \\infty$ 时它会收敛到 $I_t$。因此，$\\widehat{I}_{\\mathrm{res}}$ 为真实滤波期望 $I_t$ 提供了一个估计量，其意义在于它是 $I_t$ 的一个一致估计量的条件无偏估计量。\n\n第二个任务是推导确定性副本的期望数量的表达式并进行求值。\n粒子 $i$ 的确定性副本数量定义为 $n_i = \\lfloor N \\tilde{w}_{t}^{i} \\rfloor$。确定性副本的总数是所有粒子的总和，$N_{\\mathrm{det}} = \\sum_{i=1}^{N} n_i$。\n问题要求的是以权重 $\\{\\tilde{w}_{t}^{i}\\}_{i=1}^N$ 为条件的确定性副本的期望数量。由于权重是给定的，数量 $N_{\\mathrm{det}}$ 是一个固定的确定性值。一个常数的期望就是它本身。\n因此，表达式为：\n$$\n\\mathbb{E}\\!\\left[ N_{\\mathrm{det}} \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right] = \\sum_{i=1}^{N} \\lfloor N \\tilde{w}_{t}^{i} \\rfloor.\n$$\n我们已知 $N = 200$ 且有 6 个粒子的非零权重：$\\tilde{w}_{t}^{1}=0.041$, $\\tilde{w}_{t}^{2}=0.157$, $\\tilde{w}_{t}^{3}=0.233$, $\\tilde{w}_{t}^{4}=0.089$, $\\tilde{w}_{t}^{5}=0.271$, $\\tilde{w}_{t}^{6}=0.209$。对于所有 $i6$，$\\tilde{w}_{t}^{i}=0$。\n\n我们为 6 个粒子中的每一个计算 $N \\tilde{w}_{t}^{i}$：\n- $n_1 = \\lfloor 200 \\times 0.041 \\rfloor = \\lfloor 8.2 \\rfloor = 8$\n- $n_2 = \\lfloor 200 \\times 0.157 \\rfloor = \\lfloor 31.4 \\rfloor = 31$\n- $n_3 = \\lfloor 200 \\times 0.233 \\rfloor = \\lfloor 46.6 \\rfloor = 46$\n- $n_4 = \\lfloor 200 \\times 0.089 \\rfloor = \\lfloor 17.8 \\rfloor = 17$\n- $n_5 = \\lfloor 200 \\times 0.271 \\rfloor = \\lfloor 54.2 \\rfloor = 54$\n- $n_6 = \\lfloor 200 \\times 0.209 \\rfloor = \\lfloor 41.8 \\rfloor = 41$\n\n对于 $i  6$，$\\tilde{w}_{t}^{i} = 0$，所以 $n_i = \\lfloor 200 \\times 0 \\rfloor = 0$。\n确定性副本的总数是这些值的和：\n$$\n\\sum_{i=1}^{200} n_i = n_1 + n_2 + n_3 + n_4 + n_5 + n_6 + \\sum_{i=7}^{200} n_i\n$$\n$$\n= 8 + 31 + 46 + 17 + 54 + 41 + 0 = 197.\n$$\n在给定权重条件下，确定性副本的期望数量是 197。",
            "answer": "$$\n\\boxed{197}\n$$"
        },
        {
            "introduction": "一个理想的滤波器应具备“遗忘”初始状态的能力，即滤波器的稳定性。本练习 () 引导您将理论付诸实践，通过编写代码实现一个隐马尔可夫模型（HMM）滤波器，并直接观察其稳定性。您将通过模拟精心设计的对抗性场景，直观地理解导致滤波器稳定性失效的根本原因，并亲自测试如何通过“退火”这一实用技术来恢复滤波器的性能。",
            "id": "3308531",
            "problem": "考虑一个有限状态隐马尔可夫模型 (HMM)，其隐状态过程 $\\{X_t\\}_{t \\geq 1}$ 在 $\\{1,2,\\dots,K\\}$ 中取值，观测过程为 $\\{Y_t\\}_{t \\geq 1}$，由转移核 $p(x_t \\mid x_{t-1})$ 和观测似然 $p(y_t \\mid x_t)$ 指定。在给定观测 $y_{1:t}$ 和 $X_1$ 的先验分布 $\\mu$ 的情况下，时间 $t$ 的滤波分布记为 $\\pi_t^\\mu$，其中 $\\pi_t^\\mu(i) = \\mathbb{P}(X_t = i \\mid y_{1:t})$。两个滤波分布之间的距离由全变差 (TV) 范数度量，对于离散分布，其定义为\n$$\n\\left\\lVert \\pi_t^\\mu - \\pi_t^\\nu \\right\\rVert_{\\mathrm{TV}} = \\frac{1}{2} \\sum_{i=1}^K \\left| \\pi_t^\\mu(i) - \\pi_t^\\nu(i) \\right|.\n$$\n根据马尔可夫链和条件概率的基本定义，有限状态HMM的滤波递归通过预测后更新获得：一步预测为 $\\alpha_{t \\mid t-1}^\\mu(j) = \\sum_{i=1}^K \\pi_{t-1}^\\mu(i) \\, p(j \\mid i)$，更新步骤为\n$$\n\\pi_t^\\mu(j) = \\frac{\\alpha_{t \\mid t-1}^\\mu(j) \\, g_j(y_t)}{\\sum_{k=1}^K \\alpha_{t \\mid t-1}^\\mu(k) \\, g_k(y_t)},\n$$\n其中 $g_j(y_t) = p(y_t \\mid X_t = j)$。滤波器稳定性指的是对于任意两个先验 $\\mu$ 和 $\\nu$，$\\left\\lVert \\pi_t^\\mu - \\pi_t^\\nu \\right\\rVert_{\\mathrm{TV}}$ 随着 $t$ 的增加而衰减的特性。已知，遗忘的一个充分机制是转移核中存在混合性以及信息丰富的观测，而失效模式包括非连通的动态系统以及不可识别或对称的似然函数。\n\n你的任务是编写一个完整、可运行的程序，该程序能够：\n- 实现有限状态HMM的滤波递归，并计算最终时刻的 $\\left\\lVert \\pi_t^\\mu - \\pi_t^\\nu \\right\\rVert_{\\mathrm{TV}}$。\n- 为每个测试用例从指定的HMM中模拟一条观测路径以驱动滤波器。\n- 通过检查最终TV距离是否低于稳定性阈值来诊断非遗忘机制。\n- 提出并测试一种基于调和的修复方法，通过均匀混合修改转移核，并可选择地对似然进行调和。\n\n调和修复定义如下。给定一个转移矩阵 $P$，其元素为 $P_{ij} = p(j \\mid i)$，定义一个调和转移矩阵\n$$\n\\widetilde{P} = (1 - \\varepsilon) P + \\varepsilon U,\n$$\n其中 $U$ 是一个 $K \\times K$ 矩阵，其所有行均为 $\\{1,\\dots,K\\}$ 上的均匀分布，且 $\\varepsilon \\in [0,1]$。给定似然 $g_j(y)$，使用参数 $\\beta \\in (0,1]$ 的似然调和在更新步骤中使用 $g_j(y)^\\beta$：\n$$\n\\pi_t^{\\mu,\\mathrm{temp}}(j) = \\frac{\\alpha_{t \\mid t-1}^\\mu(j) \\, g_j(y_t)^\\beta}{\\sum_{k=1}^K \\alpha_{t \\mid t-1}^\\mu(k) \\, g_k(y_t)^\\beta}.\n$$\n\n你必须为以下由三个参数集组成的测试套件实现上述功能，每个参数集描述了 $(K, P, \\text{发射参数}, T, \\mu, \\nu, \\varepsilon, \\beta)$，其中观测值是具有状态依赖均值和标准差的高斯分布。在所有情况下，观测路径的长度为 $T$，状态 $j$ 的高斯似然使用均值 $m_j$ 和标准差 $s_j$：\n$$\ng_j(y) = \\frac{1}{\\sqrt{2\\pi} s_j} \\exp\\left( -\\frac{(y - m_j)^2}{2 s_j^2} \\right).\n$$\n所有随机模拟必须是可复现的。\n\n测试套件：\n- 案例1（稳定的混合和信息丰富的观测）：\n  - $K = 2$,\n  - $P = \\begin{pmatrix} 0.9  0.1 \\\\ 0.1  0.9 \\end{pmatrix}$,\n  - 均值 $m = (0, 3)$,\n  - 标准差 $s = (1, 1)$,\n  - $T = 50$,\n  - 先验 $\\mu = (0.99, 0.01)$ 和 $\\nu = (0.01, 0.99)$,\n  - 调和参数 $\\varepsilon = 0.02$, $\\beta = 0.7$.\n- 案例2（对抗性：恒等动态和无信息量的观测）：\n  - $K = 2$,\n  - $P = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$,\n  - 均值 $m = (0, 0)$,\n  - 标准差 $s = (5, 5)$,\n  - $T = 50$,\n  - 先验 $\\mu = (0.99, 0.01)$ 和 $\\nu = (0.01, 0.99)$,\n  - 调和参数 $\\varepsilon = 0.05$, $\\beta = 0.7$.\n- 案例3（对抗性：非连通的块和跨块不可识别）：\n  - $K = 4$,\n  - $P = \\begin{pmatrix} 0.95  0.05  0  0 \\\\ 0.05  0.95  0  0 \\\\ 0  0  0.95  0.05 \\\\ 0  0  0.05  0.95 \\end{pmatrix}$,\n  - 均值 $m = (0, 0, 0, 0)$,\n  - 标准差 $s = (1, 1, 1, 1)$,\n  - $T = 50$,\n  - 先验 $\\mu = (0.5, 0.5, 0, 0)$ 和 $\\nu = (0, 0, 0.5, 0.5)$,\n  - 调和参数 $\\varepsilon = 0.02$, $\\beta = 0.7$.\n\n对于每个案例，执行以下操作：\n1. 使用固定的随机种子和固定的初始隐状态分布（在 $\\{1,\\dots,K\\}$ 上均匀分布），从指定的 $P$ 和高斯发射分布中模拟一条观测路径 $\\{Y_t\\}_{t=1}^T$。\n2. 使用相同的观测，分别从 $\\mu$ 和 $\\nu$ 开始，使用未调和模型（$\\varepsilon = 0$, $\\beta = 1$）运行两个滤波器。计算在时间 $T$ 的最终TV距离：\n   $$\n   D_{\\mathrm{std}} = \\left\\lVert \\pi_T^\\mu - \\pi_T^\\nu \\right\\rVert_{\\mathrm{TV}}.\n   $$\n3. 使用相同的观测，分别从 $\\mu$ 和 $\\nu$ 开始，使用调和模型（$\\varepsilon$ 和 $\\beta$ 如指定）运行两个调和滤波器。计算在时间 $T$ 的最终TV距离：\n   $$\n   D_{\\mathrm{temp}} = \\left\\lVert \\pi_T^{\\mu,\\mathrm{temp}} - \\pi_T^{\\nu,\\mathrm{temp}} \\right\\rVert_{\\mathrm{TV}}.\n   $$\n4. 对于稳定性阈值 $\\tau = 0.1$，生成布尔值\n   $$\n   S_{\\mathrm{std}} = \\mathbf{1}\\{ D_{\\mathrm{std}} \\leq \\tau \\}, \\quad S_{\\mathrm{temp}} = \\mathbf{1}\\{ D_{\\mathrm{temp}} \\leq \\tau \\}.\n   $$\n\n你的程序应生成一行输出，其中包含三个测试用例的结果，格式为方括号内的逗号分隔列表。每个用例的结果必须是列表\n$$\n\\left[ D_{\\mathrm{std}}, D_{\\mathrm{temp}}, S_{\\mathrm{std}}, S_{\\mathrm{temp}} \\right],\n$$\n其中 $D_{\\mathrm{std}}$ 和 $D_{\\mathrm{temp}}$ 为浮点数，$S_{\\mathrm{std}}$ 和 $S_{\\mathrm{temp}}$ 为布尔值。因此，最终打印的行必须如下所示\n$$\n\\left[ [d_{1,\\mathrm{std}}, d_{1,\\mathrm{temp}}, s_{1,\\mathrm{std}}, s_{1,\\mathrm{temp}}], [d_{2,\\mathrm{std}}, d_{2,\\mathrm{temp}}, s_{2,\\mathrm{std}}, s_{2,\\mathrm{temp}}], [d_{3,\\mathrm{std}}, d_{3,\\mathrm{temp}}, s_{3,\\mathrm{std}}, s_{3,\\mathrm{temp}}] \\right].\n$$\n本问题不涉及物理单位；所有输出均为无量纲的数值。",
            "solution": "问题陈述已经过仔细验证，被确定为科学上合理、定义明确、客观且完整。它构成了计算统计学中的一个标准练习，特别是关于有限状态隐马尔可夫模型 (HMM) 滤波器的稳定性。因此，我们可以着手进行形式化的求解。\n\n该问题要求分析离散时间、有限状态HMM的滤波器稳定性。滤波器的稳定性，或称遗忘特性，指的是其“遗忘”初始状态的能力，使得两个以不同先验分布开始的滤波器随时间收敛到相同的后验分布。收敛程度使用全变差 (TV) 距离来度量。我们将实现HMM滤波递归，模拟观测数据，并研究稳定性失效的场景。此外，我们将实现并测试一种基于调和的正则化技术，旨在恢复稳定性。\n\nHMM由以下部分定义：\n1.  一个隐状态过程 $\\{X_t\\}_{t \\ge 1}$，在一个有限集 $\\{1, 2, \\dots, K\\}$ 中取值。状态的演化由一个时间同质的马尔可夫链控制，其转移概率矩阵为 $K \\times K$ 的 $P$，其中 $P_{ij} = p(X_t=j \\mid X_{t-1}=i)$。\n2.  一个观测过程 $\\{Y_t\\}_{t \\ge 1}$。在每个时间 $t$，观测值 $Y_t$ 从一个仅依赖于当前隐状态 $X_t$ 的分布中抽取。条件概率密度函数记为 $g_j(y) = p(y \\mid X_t=j)$。在本问题中，发射分布是高斯的：$Y_t \\mid \\{X_t=j\\} \\sim \\mathcal{N}(m_j, s_j^2)$。\n3.  一个在 $\\{1, 2, \\dots, K\\}$ 上的初始状态分布 $\\mu$。\n\n为了解决这个问题，我们将实现以下组件。\n\n**1. HMM 数据模拟**\n对于每个测试用例，我们必须首先生成一个单一的、可复现的观测序列 $\\{y_t\\}_{t=1}^T$。这通过首先模拟潜在状态序列 $\\{x_t\\}_{t=1}^T$，然后模拟相应的观测来实现。\n-   **状态模拟**：初始状态 $x_1$ 从指定的 $\\{1, \\dots, K\\}$ 上的均匀分布中抽取。随后的状态 $x_t$（对于 $t=2, \\dots, T$）从由转移矩阵 $P$ 的第 $x_{t-1}$ 行定义的分类分布中抽取。\n-   **观测模拟**：对于模拟路径中的每个状态 $x_t=j$，相应的观测 $y_t$ 从高斯发射分布 $\\mathcal{N}(m_j, s_j^2)$ 中抽取。\n-   **可复现性**：所有用于模拟的随机数均使用以固定种子初始化的伪随机数生成器生成，确保整个过程是确定性和可复现的。我们将使用种子 $0$。\n\n**2. 贝叶斯滤波递归**\n分析的核心是滤波算法，它计算在时间 $t$ 的隐状态的后验分布，$\\pi_t(j) = \\mathbb{P}(X_t=j \\mid y_{1:t})$，给定截至时间 $t$ 的观测序列。对于每个时间 $t=1, \\dots, T$，递归分两步进行：\n\n-   **预测**：基于时间 $t-1$ 的后验计算时间 $t$ 的状态的先验分布。在向量表示法中，如果 $\\pi_{t-1}$ 是时间 $t-1$ 的后验概率行向量，则预测分布 $\\alpha_{t|t-1}$ 由下式给出：\n    $$\n    \\alpha_{t|t-1} = \\pi_{t-1} P\n    $$\n-   **更新**：使用新的观测 $y_t$ 通过贝叶斯法则更新预测分布。更新后的后验 $\\pi_t$ 是：\n    $$\n    \\pi_t(j) = \\frac{\\alpha_{t|t-1}(j) \\, g_j(y_t)}{\\sum_{k=1}^K \\alpha_{t|t-1}(k) \\, g_k(y_t)}\n    $$\n为了防止重复乘以小概率值导致的数值下溢，计算在对数域中进行。更新步骤变为对数概率的相加，然后使用 `log-sum-exp` 操作进行归一化。\n\n**3. 滤波器稳定性与全变差距离**\n滤波器稳定性是指对于任何两个初始先验 $\\mu$ 和 $\\nu$，相应的滤波分布 $\\pi_t^\\mu$ 和 $\\pi_t^\\nu$ 随着 $t \\to \\infty$ 而收敛的性质。这种收敛由全变差 (TV) 距离度量：\n$$\nD_t = \\left\\lVert \\pi_t^\\mu - \\pi_t^\\nu \\right\\rVert_{\\mathrm{TV}} = \\frac{1}{2} \\sum_{i=1}^K \\left| \\pi_t^\\mu(i) - \\pi_t^\\nu(i) \\right|\n$$\n如果 $\\lim_{t \\to \\infty} D_t = 0$，则认为滤波器是稳定的。对于本问题，稳定性是在有限时间 $T$ 处根据阈值 $\\tau = 0.1$ 进行评估。\n\n**4. 作为正则化的调和**\n问题提出了两种常见的滤波器稳定性失效模式，我们用调和方法来解决：\n\n-   **转移调和**：为了对抗非连通或缓慢混合的动态，转移矩阵 $P$ 被一个扰动版本 $\\widetilde{P}$ 替代：\n    $$\n    \\widetilde{P} = (1 - \\varepsilon) P + \\varepsilon U\n    $$\n    其中 $U$ 是一个所有行均为 $\\{1, \\dots, K\\}$ 上均匀分布的矩阵，$\\varepsilon \\in (0,1]$ 是一个小的混合参数。这确保了得到的马尔可夫链是遍历的，保证了唯一的平稳分布并强制所有状态之间进行混合。\n\n-   **似然调和**：为了调节观测的影响，特别是当它们信息量不足或具有误导性时，似然函数 $g_j(y)$ 被提升到 $\\beta \\in (0,1]$ 的幂：\n    $$\n    \\pi_t(j) \\propto \\alpha_{t|t-1}(j) \\, g_j(y_t)^\\beta\n    $$\n    在对数域中，这对应于将对数似然乘以 $\\beta$。$\\beta  1$ 的值会“平滑”似然，使滤波器更多地依赖于动态模型而不是数据。\n\n**5. 测试用例分析**\n三个测试用例旨在展示特定的行为：\n\n-   **案例1（稳定）：** 转移矩阵是混合的，发射分布是良好分离的。我们预期标准滤波器是稳定的，即 $D_{\\mathrm{std}} \\le \\tau=0.1$。调和滤波器也将是稳定的。\n\n-   **案例2（对抗性）：** 转移矩阵是单位矩阵，意味着状态永不改变（零混合）。两种状态的发射分布相同，意味着观测不携带关于状态的任何信息。我们预期标准滤波器将无法遗忘其初始先验，导致 $D_{\\mathrm{std}} \\gg \\tau$。使用 $\\varepsilon  0$ 的转移调和引入了混合，即使观测信息不足，也将导致后验收敛到 $\\widetilde{P}$ 的平稳分布。我们预期 $D_{\\mathrm{temp}} \\le \\tau$。\n\n-   **案例3（对抗性）：** 转移矩阵是块对角的，创建了两个不连通的状态类别。先验 $\\mu$ 和 $\\nu$ 将其所有质量分别置于不同的块中。所有状态的发射分布都相同。标准滤波器将无法在块之间移动概率质量，从而保持初始分离。我们预期 $D_{\\mathrm{std}} \\approx 1$。转移调和连接了这些块，从而实现了收敛和稳定性。我们预期 $D_{\\mathrm{temp}} \\le \\tau$。\n\n实现过程将首先为每个案例模拟数据，然后针对两个指定的先验 $\\mu$ 和 $\\nu$ 运行标准滤波器和调和滤波器，以计算最终的TV距离和稳定性标志。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Main function to run the HMM filtering analysis for the test suite.\n    \"\"\"\n    \n    # Test suite from the problem statement.\n    # Each case: (K, P, m, s, T, mu, nu, epsilon, beta)\n    test_cases = [\n        # Case 1: Stable mixing and informative observations\n        (\n            2,\n            np.array([[0.9, 0.1], [0.1, 0.9]]),\n            np.array([0.0, 3.0]),\n            np.array([1.0, 1.0]),\n            50,\n            np.array([0.99, 0.01]),\n            np.array([0.01, 0.99]),\n            0.02,\n            0.7\n        ),\n        # Case 2: Adversarial - identity dynamics and uninformative observations\n        (\n            2,\n            np.array([[1.0, 0.0], [0.0, 1.0]]),\n            np.array([0.0, 0.0]),\n            np.array([5.0, 5.0]),\n            50,\n            np.array([0.99, 0.01]),\n            np.array([0.01, 0.99]),\n            0.05,\n            0.7\n        ),\n        # Case 3: Adversarial - non-communicating blocks and non-identifiable likelihood\n        (\n            4,\n            np.array([\n                [0.95, 0.05, 0.0, 0.0],\n                [0.05, 0.95, 0.0, 0.0],\n                [0.0, 0.0, 0.95, 0.05],\n                [0.0, 0.0, 0.05, 0.95]\n            ]),\n            np.array([0.0, 0.0, 0.0, 0.0]),\n            np.array([1.0, 1.0, 1.0, 1.0]),\n            50,\n            np.array([0.5, 0.5, 0.0, 0.0]),\n            np.array([0.0, 0.0, 0.5, 0.5]),\n            0.02,\n            0.7\n        )\n    ]\n\n    # Global settings for simulation and analysis\n    stability_threshold = 0.1\n    # Use a fixed seed for reproducible simulations\n    rng = np.random.default_rng(0)\n    \n    final_results = []\n\n    for case in test_cases:\n        K, P, m, s, T, mu, nu, epsilon, beta = case\n\n        # 1. Simulate one observation path from the HMM\n        x_path = np.zeros(T, dtype=int)\n        y_path = np.zeros(T)\n        \n        # Initial state drawn uniformly\n        x_path[0] = rng.choice(K) \n        \n        for t in range(T):\n            if t > 0:\n                x_path[t] = rng.choice(K, p=P[x_path[t-1], :])\n            # Draw observation from Gaussian emission\n            y_path[t] = rng.normal(loc=m[x_path[t]], scale=s[x_path[t]])\n\n        # 2. Run standard filters and compute TV distance\n        pi_T_mu_std = run_filter(y_path, T, K, P, m, s, mu, beta=1.0)\n        pi_T_nu_std = run_filter(y_path, T, K, P, m, s, nu, beta=1.0)\n        D_std = 0.5 * np.sum(np.abs(pi_T_mu_std - pi_T_nu_std))\n        S_std = D_std = stability_threshold\n\n        # 3. Run tempered filters and compute TV distance\n        U = np.full((K, K), 1.0 / K)\n        P_tempered = (1.0 - epsilon) * P + epsilon * U\n        \n        pi_T_mu_temp = run_filter(y_path, T, K, P_tempered, m, s, mu, beta)\n        pi_T_nu_temp = run_filter(y_path, T, K, P_tempered, m, s, nu, beta)\n        D_temp = 0.5 * np.sum(np.abs(pi_T_mu_temp - pi_T_nu_temp))\n        S_temp = D_temp = stability_threshold\n\n        final_results.append([D_std, D_temp, S_std, S_temp])\n\n    # Final print statement in the exact required format.\n    print(f\"[\" + \",\".join(map(str, final_results)) + \"]\")\n\ndef run_filter(y_path, T, K, P, m, s, prior, beta):\n    \"\"\"\n    Runs the HMM filtering recursion.\n\n    Args:\n        y_path (np.ndarray): The sequence of observations.\n        T (int): The length of the observation sequence.\n        K (int): The number of hidden states.\n        P (np.ndarray): The transition matrix.\n        m (np.ndarray): The means of the Gaussian emissions.\n        s (np.ndarray): The standard deviations of the Gaussian emissions.\n        prior (np.ndarray): The initial prior distribution over states.\n        beta (float): The likelihood tempering parameter.\n\n    Returns:\n        np.ndarray: The final filtering distribution pi_T.\n    \"\"\"\n    # Initialize with the log of the prior distribution\n    # A small constant is added to the prior to avoid log(0) for deterministic priors.\n    log_pi = np.log(prior + 1e-100) \n\n    for t in range(T):\n        # Prediction step\n        # Convert back to probability space for matrix multiplication\n        pi = np.exp(log_pi)\n        alpha_pred = pi @ P\n\n        # Update step (in log-space for numerical stability)\n        # Handle cases where predicted probability is 0\n        log_alpha_pred = np.log(alpha_pred, where=alpha_pred > 0, out=np.full_like(alpha_pred, -np.inf))\n        \n        # Calculate log-likelihoods for the current observation y_t\n        log_likelihoods = np.array([\n            norm.logpdf(y_path[t], loc=m[j], scale=s[j]) for j in range(K)\n        ])\n\n        # Compute unnormalized log posterior\n        unnormalized_log_post = log_alpha_pred + beta * log_likelihoods\n        \n        # Normalize using log-sum-exp trick\n        log_norm_const = logsumexp(unnormalized_log_post)\n        log_pi = unnormalized_log_post - log_norm_const\n\n    # Return the final filtering distribution in probability space\n    return np.exp(log_pi)\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了基本的滤波概念后，本高级练习 () 将带您进入更复杂的平滑问题，并聚焦于算法性能优化的实际挑战。您需要实现一个完整的序贯蒙特卡洛（SMC）平滑器，并通过系统性的经验评估来校准触发重采样的有效样本量（ESS）阈值 $\\tau$。这项任务将让您深刻体会到在缓解粒子权重退化和避免路径退化之间的关键权衡，这是优化高级随机模拟算法性能的核心技能。",
            "id": "3308538",
            "problem": "您将研究一个标量线性高斯状态空间模型，并设计一个通过有效样本量 (ESS) 阈值触发重采样的序贯蒙特卡洛 (SMC) 平滑估计器。您的任务是从基本原理出发，为平滑泛函推导一个中心极限定理 (CLT)，然后校准 ESS 阈值，以最小化平滑估计器在时间上的期望均方误差 (MSE)。最后，您将实现一个完整的、可运行的程序，该程序使用固定的测试套件对阈值进行经验性校准，并产生所需的输出。\n\n该问题的基本基础是状态空间模型中滤波和平滑的 Feynman–Kac 表示、序贯蒙特卡洛 (SMC) 中重要性采样和重采样的定义，以及在混合和正则性条件下的经典中心极限定理 (CLT) 结果。\n\n模型和定义：\n- 考虑具有隐藏状态 $x_t$ 和观测值 $y_t$ 的标量线性高斯状态空间模型。\n- 初始状态为 $x_0 \\sim \\mathcal{N}(\\mu_0, P_0)$，状态动力学为 $x_t = a x_{t-1} + \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0,q)$，观测模型为 $y_t = c x_t + \\epsilon_t$，其中 $\\epsilon_t \\sim \\mathcal{N}(0,r)$。\n- 设观测序列为 $y_{1:T} = (y_1, \\dots, y_T)$，时间范围 $T$ 固定。\n- 对于一个可测函数 $g$，将时间 $t$ 的平滑目标泛函定义为 $\\varphi_t = \\mathbb{E}[g(x_t) \\mid y_{1:T}]$。在本问题中，使用 $g(x) = x$，因此 $\\varphi_t = \\mathbb{E}[x_t \\mid y_{1:T}]$。\n- SMC 平滑估计器 $\\hat{\\varphi}_t^{(N)}$ 是由一个具有 $N$ 个粒子的自助粒子滤波器构建的，该滤波器通过 ESS 阈值触发系统重采样，并对粒子集进行前向滤波-后向平滑。\n\n序贯蒙特卡洛设置和 ESS 触发的重采样：\n- 在每个时间点 $t$，自助提议为 $x_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(a^{(i)})})$，其中 $p(x_t \\mid x_{t-1}) = \\mathcal{N}(a x_{t-1}, q)$，祖先索引 $a^{(i)}$ 是通过重采样（当触发时）选择的。\n- 在时间 $t$ 的未归一化权重为 $w_t^{(i)} \\propto p(y_t \\mid x_t^{(i)})$，其中 $p(y_t \\mid x_t)=\\mathcal{N}(c x_t, r)$。归一化权重为 $\\tilde{w}_t^{(i)} = w_t^{(i)}\\big/\\sum_{j=1}^N w_t^{(j)}$。\n- 时间 $t$ 的有效样本量 (ESS) 定义为 $\\mathrm{ESS}_t = \\left(\\sum_{i=1}^N \\tilde{w}_t^{(i)}\\right)^2 \\big/ \\sum_{i=1}^N \\left(\\tilde{w}_t^{(i)}\\right)^2 = 1 \\big/ \\sum_{i=1}^N \\left(\\tilde{w}_t^{(i)}\\right)^2$。如果 $\\mathrm{ESS}_t  \\tau N$，则触发重采样，其中 $\\tau \\in (0,1]$ 是 ESS 阈值。\n- 使用系统重采样。重采样后，将权重重置为 $\\tilde{w}_t^{(i)} = 1/N$。\n\n对粒子集进行平滑：\n- 使用由马尔可夫转移隐含的后向核，在粒子集上执行前向滤波-后向平滑。定义 $\\kappa_{t}(x_t^{(i)}, x_{t+1}^{(j)}) = p(x_{t+1}^{(j)} \\mid x_t^{(i)})$，其中对于标量高斯模型，$p(x_{t+1} \\mid x_t) = \\mathcal{N}(a x_t, q)$。在实现中，为了数值稳定性和归一化不变性，仅使用指数部分并省略常数因子，即 $\\exp\\left(-\\frac{1}{2 q} \\left(x_{t+1}^{(j)} - a x_t^{(i)}\\right)^2\\right)$。\n- 对所有 $i$，用 $\\beta_T^{(i)} = 1$ 初始化后向消息。对于 $t = T-1, T-2, \\dots, 1$，更新 $\\beta_t^{(i)} = \\sum_{j=1}^N \\beta_{t+1}^{(j)} \\tilde{w}_{t+1}^{(j)} \\kappa_t(x_t^{(i)}, x_{t+1}^{(j)})$。\n- 时间 $t$ 的平滑权重为 $\\tilde{w}_{t,\\mathrm{sm}}^{(i)} \\propto \\tilde{w}_t^{(i)} \\beta_t^{(i)}$，归一化使其总和为 $1$。\n- 对于 $t=1,\\dots,T-1$，平滑估计器为 $\\hat{\\varphi}_t^{(N)} = \\sum_{i=1}^N \\tilde{w}_{t,\\mathrm{sm}}^{(i)} g(x_t^{(i)})$，对于 $t=T$，设置 $\\hat{\\varphi}_T^{(N)} = \\sum_{i=1}^N \\tilde{w}_T^{(i)} g(x_T^{(i)})$。\n\nCLT 目标和校准目标：\n- 在 Feynman–Kac 模型带有重采样的标准混合和正则性假设下，您将为每个固定时间 $t$ 推导出 CLT $\\sqrt{N}\\left(\\hat{\\varphi}_t^{(N)} - \\varphi_t\\right) \\Rightarrow \\mathcal{N}(0,\\sigma_t^2)$。渐近方差 $\\sigma_t^2$ 是局部贡献的总和，这些贡献取决于 $g(x_t)$ 在平滑分布下的变异性以及通过后向核在时间上传播的变异性。\n- 使用 CLT，论证校准 $\\tau$ 以控制有效样本量和谱系退化，从而最小化期望均方误差 $\\mathrm{MSE} = \\frac{1}{T} \\sum_{t=1}^T \\mathbb{E}\\left[\\left(\\hat{\\varphi}_t^{(N)} - \\varphi_t\\right)^2\\right]$。\n\n程序要求：\n- 实现一个完整的程序，对于每个测试用例，从指定的模型参数生成合成数据，使用 Rauch–Tung–Striebel (RTS) 卡尔曼平滑器计算精确的平滑均值 $\\varphi_t$，对一组 $\\tau$ 候选值运行 SMC 平滑器，估计时间平均 MSE，并输出最小化估计 MSE 的 $\\tau$。当出现平局时，选择具有最小 MSE 的 $\\tau$ 中最小的一个。\n- 如上所述，使用恒等函数 $g(x) = x$。\n- 当 $\\mathrm{ESS}_t  \\tau N$ 时，使用系统重采样。\n- 对于每个测试用例，在指定数量的重复实验中对时间平均 MSE 进行平均，以近似目标中的期望。\n\n测试套件：\n- 案例 1：参数 $(\\mu_0,P_0,a,q,c,r,T,N,R)$ 设置为 $(0.0,1.0,0.9,1.0,1.0,0.5,40,200,12)$，$\\tau$ 候选值为 $[0.3, 0.5, 0.7, 0.9]$。\n- 案例 2：参数 $(\\mu_0,P_0,a,q,c,r,T,N,R)$ 设置为 $(0.0,5.0,1.0,1.0,1.0,2.0,60,150,12)$，$\\tau$ 候选值为 $[0.2, 0.4, 0.6, 0.8, 1.0]$。\n- 案例 3：参数 $(\\mu_0,P_0,a,q,c,r,T,N,R)$ 设置为 $(0.0,1.0,0.5,0.5,1.0,0.1,50,120,12)$，$\\tau$ 候选值为 $[0.1, 0.3, 0.5, 0.7]$。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含三个测试用例选择的阈值，格式为逗号分隔的浮点数列表，四舍五入到两位小数，并用方括号括起来。例如，如果选择的阈值为 $\\tau_1 = 0.5$，$\\tau_2 = 0.6$，和 $\\tau_3 = 0.3$，则输出必须是 `[0.50,0.60,0.30]`。\n\n重要约束：\n- 最终程序必须完全自包含，不需要用户输入，并且只能使用下面指定的 Python 标准库、NumPy 和 SciPy。此问题中不涉及角度或物理单位，因此除上述定义外，不需要任何单位规范。",
            "solution": "该问题已经过验证，被确定为计算统计学和随机模拟领域中一个适定、有科学依据且完整的问题。它要求实现和经验性比较标准算法（Rauch-Tung-Striebel 平滑器、序贯蒙特卡洛平滑器）来解决一个实际的参数校准任务。所有必要的模型、参数和目标都已明确定义。\n\n目标是经验性地确定序贯蒙特卡洛 (SMC) 平滑器中用于重采样的最优有效样本量 (ESS) 阈值，记为 $\\tau$。最优性是根据最小化平滑状态估计的时间平均均方误差 (MSE) 来定义的。我们给定一个标量线性高斯状态空间模型：\n- 状态动力学：$x_t = a x_{t-1} + \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0,q)$\n- 观测模型：$y_t = c x_t + \\epsilon_t$，其中 $\\epsilon_t \\sim \\mathcal{N}(0,r)$\n\n估计的目标是状态的平滑均值，$\\varphi_t = \\mathbb{E}[x_t \\mid y_{1:T}]$，对于每个时间步 $t \\in \\{1, \\dots, T\\}$。\n\n**理论动机和基于原理的设计**\n\n$\\varphi_t$ 的估计器记为 $\\hat{\\varphi}_t^{(N)}$，它是使用具有 $N$ 个粒子的 SMC 平滑器构建的。在适当的正则性和混合条件下，SMC 估计器的中心极限定理 (CLT) 成立。对于固定的时间 $t$ 且当粒子数 $N \\to \\infty$ 时，估计误差的分布收敛于：\n$$\n\\sqrt{N}\\left(\\hat{\\varphi}_t^{(N)} - \\varphi_t\\right) \\Rightarrow \\mathcal{N}(0,\\sigma_t^2)\n$$\n其中 ‘$\\Rightarrow$’ 表示依分布收敛。这个 CLT 意味着对于一个大而有限的 $N$，均方误差 (MSE) 可以近似为：\n$$\n\\mathrm{MSE}_t = \\mathbb{E}\\left[\\left(\\hat{\\varphi}_t^{(N)} - \\varphi_t\\right)^2\\right] \\approx \\frac{\\sigma_t^2}{N}\n$$\n渐近方差 $\\sigma_t^2$ 是一个复杂的量，它依赖于粒子滤波器的整个历史，包括随机的重采样步骤。我们的目标是选择重采样阈值 $\\tau$，以最小化时间平均 MSE，即 $\\frac{1}{T} \\sum_{t=1}^T \\mathrm{MSE}_t$，这近似等价于最小化时间平均渐近方差。\n\n$\\tau \\in (0,1]$ 的选择控制着重采样的频率，并涉及一个关键的权衡：\n1.  **不频繁重采样 (低 $\\tau$)**：如果很少执行重采样，粒子权重可能会变得高度倾斜，即一个或少数几个粒子的权重接近 $1$，其余的则接近 $0$。这种现象称为权重退化，它会增加 SMC 估计器的方差，因为对估计有贡献的有效粒子数量非常少。ESS 定义为 $\\mathrm{ESS}_t = 1 / \\sum_{i=1}^N (\\tilde{w}_t^{(i)})^2$，是衡量这种退化程度的指标。\n2.  **频繁重采样 (高 $\\tau$)**：如果几乎每一步都进行重采样（例如，$\\tau$ 接近 $1$），权重退化问题会得到缓解。然而，这会引入另一个问题：路径退化。频繁的重采样导致粒子集中许多粒子共享一个共同的近期祖先。粒子轨迹多样性的丧失同样会增加平滑估计的方差，特别是对于较早的时间步，因为来自未来的信息可以回溯传播的不同祖先路径较少。\n\n因此，存在一个最优的 $\\tau$ 值，它在减小权重方差和增加谱系方差之间取得平衡。由于将总方差表示为 $\\tau$ 的函数的解析表达式是难以处理的，我们采用一种基于经验和模拟的方法进行校准。\n\n**算法和实现策略**\n\n我们的策略涉及一个嵌套循环结构，以经验性地评估测试套件中提供的每个候选 $\\tau$。\n\n1.  **外层循环**：遍历 $\\tau$ 的每个候选值。\n2.  **中层循环 (重复实验)**：对于每个 $\\tau$，运行 $R$ 次独立的模拟（重复实验），以获得对 MSE 的稳定估计。\n3.  **内层循环 (单次重复实验的逻辑)**：\n    a.  **数据生成**：从线性高斯模型合成一个真实状态序列 $\\{x_t\\}_{t=0}^T$ 和一个对应的观测序列 $\\{y_t\\}_{t=1}^T$。\n    b.  **基准真相计算**：给定的模型是线性和高斯的，因此精确的后验平滑分布是高斯分布，可以解析计算。我们使用标准的 **Rauch-Tung-Striebel (RTS) 平滑器** 来计算真实的平滑均值 $\\varphi_t = \\mathbb{E}[x_t \\mid y_{1:T}]$。RTS 算法包括一个前向传递（卡尔曼滤波器）来计算滤波估计 $\\mathbb{E}[x_t \\mid y_{1:t}]$ 和一个后向传递来将这些估计与未来信息结合。\n    c.  **SMC 估计**：我们实现指定的 SMC 平滑器来计算估计值 $\\hat{\\varphi}_t^{(N)}$。这包括两个主要阶段：\n        i.  **前向传递 (粒子滤波)**：一个自助粒子滤波器从 $t=1$ 推进到 $T$。在每一步，粒子根据状态动力学进行传播，然后根据观测的似然进行加权。计算 ESS，如果它低于阈值 $\\tau N$，则执行**系统重采样**。对于后续的平滑步骤至关重要的是，要为每个时间步存储完整的粒子集及其在任何重采样*之前*的权重。\n        ii. **后向传递 (FFBS)**：执行一个**前向滤波-后向平滑 (FFBS)** 算法。从终端消息 $\\beta_T^{(i)} = 1$ 开始，我们从 $t=T-1$ 向后迭代到 $t=1$，根据递归式 $\\beta_t^{(i)} = \\sum_{j=1}^N \\beta_{t+1}^{(j)} \\tilde{w}_{t+1}^{(j)} \\kappa_t(x_t^{(i)}, x_{t+1}^{(j)})$ 更新后向消息 $\\beta_t^{(i)}$，其中 $\\kappa_t$ 是状态转移密度。这个递归是在存储的预重采样粒子集上执行的。然后，使用与前向权重 $\\tilde{w}_t^{(i)}$ 和后向消息 $\\beta_t^{(i)}$ 的乘积成比例的权重来计算平滑估计 $\\hat{\\varphi}_t^{(N)}$。\n    d.  **MSE 计算**：该次重复实验的时间平均 MSE 计算为 $\\frac{1}{T}\\sum_{t=1}^T (\\hat{\\varphi}_t^{(N)} - \\varphi_t)^2$。\n\n4.  **选择**：在完成给定 $\\tau$ 的所有重复实验后，记录其平均 MSE。最后，选择导致最低平均 MSE 的 $\\tau$ 作为该测试用例的最优值，若出现平局则选择最小的 $\\tau$。对三个测试用例中的每一个都重复此整个过程。\n\n这种严谨的、基于模拟的方法论允许通过优化期望的性能指标 MSE，在问题中定义的特定模型类别上直接校准超参数 $\\tau$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the final results.\n    \"\"\"\n    # Set a global random seed for complete reproducibility of the experiment.\n    np.random.seed(42)\n    \n    test_cases = [\n        {\n            'mu0': 0.0, 'P0': 1.0, 'a': 0.9, 'q': 1.0, 'c': 1.0, 'r': 0.5, \n            'T': 40, 'N': 200, 'R': 12, 'taus': [0.3, 0.5, 0.7, 0.9]\n        },\n        {\n            'mu0': 0.0, 'P0': 5.0, 'a': 1.0, 'q': 1.0, 'c': 1.0, 'r': 2.0, \n            'T': 60, 'N': 150, 'R': 12, 'taus': [0.2, 0.4, 0.6, 0.8, 1.0]\n        },\n        {\n            'mu0': 0.0, 'P0': 1.0, 'a': 0.5, 'q': 0.5, 'c': 1.0, 'r': 0.1, \n            'T': 50, 'N': 120, 'R': 12, 'taus': [0.1, 0.3, 0.5, 0.7]\n        }\n    ]\n\n    final_results = []\n    for params in test_cases:\n        best_tau = calibrate_tau_for_case(params)\n        final_results.append(f\"{best_tau:.2f}\")\n\n    print(f\"[{','.join(final_results)}]\")\n\ndef calibrate_tau_for_case(params):\n    \"\"\"\n    For a single test case, finds the optimal tau from a list of candidates.\n    \"\"\"\n    taus = params['taus']\n    R = params['R']\n    \n    mse_by_tau = {}\n    for tau in taus:\n        replicate_mses = []\n        for _ in range(R):\n            x_true, y_obs = generate_data(params)\n            \n            # Ground truth via RTS Kalman Smoother\n            phi_true, _ = rts_smoother(y_obs, params)\n            \n            # SMC estimation\n            phi_hat = smc_smoother(y_obs, params, tau)\n            \n            # Time-averaged MSE for this replicate (t=1..T)\n            mse = np.mean((phi_hat[1:] - phi_true[1:])**2) \n            replicate_mses.append(mse)\n        \n        mse_by_tau[tau] = np.mean(replicate_mses)\n\n    # Find best tau (min MSE, with tie-breaking by choosing smallest tau)\n    min_mse = float('inf')\n    best_tau = -1.0\n    # Iterate through taus in their given (sorted) order.\n    for tau in taus:\n        if mse_by_tau[tau]  min_mse:\n            min_mse = mse_by_tau[tau]\n            best_tau = tau\n            \n    return best_tau\n    \ndef generate_data(params):\n    \"\"\"\n    Generates synthetic data from the linear Gaussian state-space model.\n    \"\"\"\n    T, mu0, P0, a, q, c, r = params['T'], params['mu0'], params['P0'], params['a'], params['q'], params['c'], params['r']\n    x = np.zeros(T + 1)\n    y = np.zeros(T + 1) # y_1 to y_T stored at indices 1..T\n    \n    x[0] = np.random.normal(mu0, np.sqrt(P0))\n    for t in range(1, T + 1):\n        x[t] = a * x[t-1] + np.random.normal(0, np.sqrt(q))\n        y[t] = c * x[t] + np.random.normal(0, np.sqrt(r))\n    return x, y\n\ndef rts_smoother(y_obs, params):\n    \"\"\"\n    Computes exact smoothed means and covariances using the RTS smoother.\n    \"\"\"\n    T, mu0, P0, a, q, c, r = params['T'], params['mu0'], params['P0'], params['a'], params['q'], params['c'], params['r']\n    \n    x_filt = np.zeros(T + 1)\n    P_filt = np.zeros(T + 1)\n    x_pred = np.zeros(T + 1)\n    P_pred = np.zeros(T + 1)\n\n    x_filt[0], P_filt[0] = mu0, P0\n    \n    # Kalman Filter (forward pass)\n    for t in range(1, T + 1):\n        x_pred[t] = a * x_filt[t-1]\n        P_pred[t] = a**2 * P_filt[t-1] + q\n        \n        S = c**2 * P_pred[t] + r\n        K = (P_pred[t] * c) / S\n        x_filt[t] = x_pred[t] + K * (y_obs[t] - c * x_pred[t])\n        P_filt[t] = (1 - K * c) * P_pred[t]\n        \n    x_smooth = np.zeros(T + 1)\n    P_smooth = np.zeros(T + 1)\n    \n    x_smooth[T], P_smooth[T] = x_filt[T], P_filt[T]\n    \n    # RTS (backward pass)\n    for t in range(T - 1, -1, -1):\n        J = (P_filt[t] * a) / P_pred[t+1]\n        x_smooth[t] = x_filt[t] + J * (x_smooth[t+1] - x_pred[t+1])\n        P_smooth[t] = P_filt[t] + J**2 * (P_smooth[t+1] - P_pred[t+1])\n\n    return x_smooth, P_smooth\n\ndef systematic_resample_indices(weights, N):\n    \"\"\"\n    Performs systematic resampling and returns the indices of resampled particles.\n    \"\"\"\n    c = np.cumsum(weights)\n    u0 = np.random.uniform(0, 1/N)\n    u = u0 + np.arange(N) / N\n    return np.searchsorted(c, u)\n\ndef smc_smoother(y_obs, params, tau):\n    \"\"\"\n    Performs SMC smoothing with ESS-triggered systematic resampling and FFBS.\n    \"\"\"\n    T, N, mu0, P0, a, q, c, r = params['T'], params['N'], params['mu0'], params['P0'], params['a'], params['q'], params['c'], params['r']\n    \n    all_particles = np.zeros((T + 1, N))\n    all_weights = np.zeros((T + 1, N))\n    \n    current_particles = np.random.normal(mu0, np.sqrt(P0), size=N)\n    all_particles[0, :] = current_particles\n    all_weights[0, :] = 1.0 / N\n\n    # Forward filter pass\n    for t in range(1, T + 1):\n        propagated_particles = a * current_particles + np.random.normal(0, np.sqrt(q), size=N)\n        \n        log_w = -0.5 * ((y_obs[t] - c * propagated_particles)**2) / r\n        log_w -= np.max(log_w)\n        w = np.exp(log_w)\n        normalized_weights = w / np.sum(w)\n\n        all_particles[t, :] = propagated_particles\n        all_weights[t, :] = normalized_weights\n        \n        ess = 1.0 / np.sum(normalized_weights**2)\n        if ess  tau * N:\n            indices = systematic_resample_indices(normalized_weights, N)\n            current_particles = propagated_particles[indices]\n        else:\n            current_particles = propagated_particles\n    \n    # Backward smoothing pass (FFBS)\n    smoothed_means = np.zeros(T + 1)\n    smoothed_means[T] = np.sum(all_weights[T, :] * all_particles[T, :])\n\n    beta = np.ones(N)\n    for t in range(T - 1, 0, -1):\n        # Calculate backward kernel matrix between particle sets at t and t+1\n        diff_sq = (all_particles[t+1, :][np.newaxis, :] - a * all_particles[t, :][:, np.newaxis])**2\n        kernel = np.exp(-0.5 * diff_sq / q)\n        \n        weighted_beta_next = beta * all_weights[t+1, :]\n        beta = np.dot(kernel, weighted_beta_next)\n        \n        w_sm = all_weights[t, :] * beta\n        w_sm_sum = np.sum(w_sm)\n        if w_sm_sum > 1e-100: # Handle potential underflow\n            w_sm /= w_sm_sum\n        else:\n            w_sm = np.ones(N) / N\n            \n        smoothed_means[t] = np.sum(w_sm * all_particles[t, :])\n        \n    return smoothed_means\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}