{
    "hands_on_practices": [
        {
            "introduction": "Estimating the underlying parameters of a state-space model from observed data is a central challenge in many scientific and engineering fields. The Expectation-Maximization (EM) algorithm provides a robust iterative framework for this task by treating the unobserved states as missing data. This practice problem  demystifies the 'Maximization' step of the algorithm by guiding you through the analytical derivation and application of update rules for a linear Gaussian model, building a concrete understanding of how smoothed state estimates refine model parameters.",
            "id": "3346823",
            "problem": "Consider a scalar linear Gaussian state-space model (also known as a Linear Gaussian State-Space Model (LGSSM)) with latent state $x_t$ and observation $y_t$ given by\n$x_{t+1} = A x_t + w_t$ and $y_t = C x_t + v_t$, where $w_t \\sim \\mathcal{N}(0,Q)$ and $v_t \\sim \\mathcal{N}(0,R)$ are mutually independent Gaussian noises and independent of the latent state. Assume the initial prior $p(x_1)$ is fixed and not updated. You observe a sequence $\\{y_t\\}_{t=1}^{T}$ with $T = 5$, and you have completed the Expectation step of the Expectation-Maximization (EM) algorithm under some current parameters, obtaining the following smoothed posterior moments:\n- Posterior means $m_t = \\mathbb{E}[x_t \\mid y_{1:T}]$:\n$m_1 = 0.8$, $m_2 = 0.95$, $m_3 = 0.75$, $m_4 = 0.5$, $m_5 = 0.2$.\n- Posterior second moments $S_t = \\mathbb{E}[x_t^2 \\mid y_{1:T}]$:\n$S_1 = 0.82$, $S_2 = 0.93$, $S_3 = 0.77$, $S_4 = 0.53$, $S_5 = 0.21$.\n- Posterior lag-one cross-moments $S_{t,t-1} = \\mathbb{E}[x_t x_{t-1} \\mid y_{1:T}]$ for $t \\in \\{2,3,4,5\\}$:\n$S_{2,1} = 0.76$, $S_{3,2} = 0.72$, $S_{4,3} = 0.58$, $S_{5,4} = 0.32$.\nThe observed data are $y_1 = 0.9$, $y_2 = 1.2$, $y_3 = 0.7$, $y_4 = 0.4$, $y_5 = 0.1$.\n\nStarting from the complete-data likelihood for the LGSSM and the definition of the EM objective as the expectation (with respect to the smoothed posterior over latent states) of the complete-data log-likelihood, derive the maximization step (M-step) update rules for the parameters $A$, $Q$, $C$, and $R$. Then, using only the provided smoothed moments and observations, compute the updated values of $A$, $Q$, $C$, and $R$ exactly. Express your final answers as exact rational numbers, and report them in the order $A$, $Q$, $C$, $R$. No rounding is required. Your final answer must be a single row vector.",
            "solution": "The problem requires the derivation of the Maximization-step (M-step) update rules for the parameters of a scalar Linear Gaussian State-Space Model (LGSSM) and their computation from provided E-step results.\nThe model is defined as:\nState equation: $x_{t+1} = A x_t + w_t$, with process noise $w_t \\sim \\mathcal{N}(0,Q)$.\nObservation equation: $y_t = C x_t + v_t$, with observation noise $v_t \\sim \\mathcal{N}(0,R)$.\n\nThe goal of the M-step in the Expectation-Maximization (EM) algorithm is to find the parameters $\\theta = \\{A, Q, C, R\\}$ that maximize the expected complete-data log-likelihood, where the expectation is taken with respect to the posterior distribution of the latent states $x_{1:T}$ given the observations $y_{1:T}$ and the current parameters $\\theta_{old}$. This objective function is denoted as $\\mathcal{Q}(\\theta, \\theta_{old})$.\n\nThe complete-data log-likelihood $L(\\theta) = \\ln p(x_{1:T}, y_{1:T} | \\theta)$ is given by:\n$$L(\\theta) = \\ln p(x_1) + \\sum_{t=2}^{T} \\ln p(x_t|x_{t-1}, A, Q) + \\sum_{t=1}^{T} \\ln p(y_t|x_t, C, R)$$\nwhere $T=5$ is the number of time steps.\nThe Gaussian probability densities are:\n$p(x_t | x_{t-1}, A, Q) = \\frac{1}{\\sqrt{2\\pi Q}} \\exp\\left(-\\frac{(x_t - Ax_{t-1})^2}{2Q}\\right)$\n$p(y_t | x_t, C, R) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{(y_t - Cx_t)^2}{2R}\\right)$\nIgnoring terms that are constant with respect to the parameters $\\theta$, the log-likelihood is:\n$$L(\\theta) \\propto -\\frac{T-1}{2} \\ln Q - \\frac{1}{2Q} \\sum_{t=2}^{T} (x_t - Ax_{t-1})^2 - \\frac{T}{2} \\ln R - \\frac{1}{2R} \\sum_{t=1}^{T} (y_t - Cx_t)^2$$\nThe objective function for the M-step is $\\mathcal{Q}(\\theta, \\theta_{old}) = \\mathbb{E}_{x_{1:T}|y_{1:T}, \\theta_{old}}[L(\\theta)]$. Let $\\mathbb{E}[\\cdot]$ denote this expectation.\n$$\\mathcal{Q}(\\theta) \\propto -\\frac{T-1}{2} \\ln Q - \\frac{1}{2Q} \\sum_{t=2}^{T} \\mathbb{E}[(x_t - Ax_{t-1})^2] - \\frac{T}{2} \\ln R - \\frac{1}{2R} \\sum_{t=1}^{T} \\mathbb{E}[(y_t - Cx_t)^2]$$\nThe function $\\mathcal{Q}(\\theta)$ conveniently separates into terms depending on $(A,Q)$ and terms depending on $(C,R)$. We can maximize them independently.\n\n**Update rules for $A$ and $Q$**\nLet $\\mathcal{Q}_{A,Q} = -\\frac{T-1}{2} \\ln Q - \\frac{1}{2Q} \\sum_{t=2}^{T} \\mathbb{E}[(x_t - Ax_{t-1})^2]$.\nThe expectation term is $\\mathbb{E}[x_t^2 - 2Ax_t x_{t-1} + A^2 x_{t-1}^2] = \\mathbb{E}[x_t^2] - 2A\\mathbb{E}[x_t x_{t-1}] + A^2\\mathbb{E}[x_{t-1}^2]$.\nUsing the provided E-step moments, $S_t = \\mathbb{E}[x_t^2 | y_{1:T}]$ and $S_{t,t-1} = \\mathbb{E}[x_t x_{t-1} | y_{1:T}]$, this becomes $S_t - 2AS_{t,t-1} + A^2S_{t-1}$.\nTo find the optimal $A$, we differentiate $\\mathcal{Q}_{A,Q}$ with respect to $A$ and set it to zero:\n$$\\frac{\\partial \\mathcal{Q}_{A,Q}}{\\partial A} = -\\frac{1}{2Q} \\sum_{t=2}^{T} (-2S_{t,t-1} + 2AS_{t-1}) = 0$$\n$$\\sum_{t=2}^{T} (AS_{t-1} - S_{t,t-1}) = 0 \\implies A\\sum_{t=2}^{T}S_{t-1} = \\sum_{t=2}^{T}S_{t,t-1}$$\nThus, the update rule for $A$ is:\n$$A_{new} = \\frac{\\sum_{t=2}^{T}S_{t,t-1}}{\\sum_{t=1}^{T-1}S_{t}}$$\nTo find the optimal $Q$, we differentiate $\\mathcal{Q}_{A,Q}$ with respect to $Q$ and set to zero, using $A=A_{new}$:\n$$\\frac{\\partial \\mathcal{Q}_{A,Q}}{\\partial Q} = -\\frac{T-1}{2Q} + \\frac{1}{2Q^2} \\sum_{t=2}^{T} \\mathbb{E}[(x_t - A_{new}x_{t-1})^2] = 0$$\nThis gives the update rule for $Q$:\n$$Q_{new} = \\frac{1}{T-1} \\sum_{t=2}^{T} \\mathbb{E}[(x_t - A_{new}x_{t-1})^2] = \\frac{1}{T-1} \\sum_{t=2}^{T} (S_t - 2A_{new}S_{t,t-1} + A_{new}^2S_{t-1})$$\nSubstituting $A_{new}\\sum S_{t-1} = \\sum S_{t,t-1}$, this simplifies to:\n$$Q_{new} = \\frac{1}{T-1} \\left( \\sum_{t=2}^{T}S_t - A_{new}\\sum_{t=2}^{T}S_{t,t-1} \\right)$$\n\n**Update rules for $C$ and $R$**\nLet $\\mathcal{Q}_{C,R} = -\\frac{T}{2} \\ln R - \\frac{1}{2R} \\sum_{t=1}^{T} \\mathbb{E}[(y_t - Cx_t)^2]$.\nThe expectation term is $\\mathbb{E}[y_t^2 - 2Cy_tx_t + C^2x_t^2] = y_t^2 - 2Cy_t\\mathbb{E}[x_t] + C^2\\mathbb{E}[x_t^2]$.\nUsing the E-step moments, $m_t = \\mathbb{E}[x_t | y_{1:T}]$ and $S_t=\\mathbb{E}[x_t^2 | y_{1:T}]$, this becomes $y_t^2 - 2Cy_tm_t + C^2S_t$.\nTo find the optimal $C$, we differentiate with respect to $C$ and set to zero:\n$$\\frac{\\partial \\mathcal{Q}_{C,R}}{\\partial C} = -\\frac{1}{2R} \\sum_{t=1}^{T} (-2y_tm_t + 2CS_t) = 0$$\n$$\\sum_{t=1}^{T} (CS_t - y_tm_t) = 0 \\implies C\\sum_{t=1}^{T}S_{t} = \\sum_{t=1}^{T}y_{t}m_{t}$$\nThus, the update rule for $C$ is:\n$$C_{new} = \\frac{\\sum_{t=1}^{T}y_t m_t}{\\sum_{t=1}^{T}S_t}$$\nTo find the optimal $R$, we differentiate with respect to $R$ and set to zero, using $C=C_{new}$:\n$$\\frac{\\partial \\mathcal{Q}_{C,R}}{\\partial R} = -\\frac{T}{2R} + \\frac{1}{2R^2} \\sum_{t=1}^{T} \\mathbb{E}[(y_t - C_{new}x_t)^2] = 0$$\nThis gives the update rule for $R$:\n$$R_{new} = \\frac{1}{T} \\sum_{t=1}^{T} (y_t^2 - 2C_{new}y_tm_t + C_{new}^2S_t)$$\nSubstituting $C_{new}\\sum S_t = \\sum y_t m_t$, this simplifies to:\n$$R_{new} = \\frac{1}{T} \\left( \\sum_{t=1}^{T}y_t^2 - C_{new}\\sum_{t=1}^{T}y_t m_t \\right)$$\n\n**Numerical Computation**\nWith $T=5$, we first compute the required sums from the provided data.\nFor $A$ and $Q$:\n$\\sum_{t=2}^{5} S_{t,t-1} = 0.76 + 0.72 + 0.58 + 0.32 = 2.38$\n$\\sum_{t=1}^{4} S_{t} = S_1 + S_2 + S_3 + S_4 = 0.82 + 0.93 + 0.77 + 0.53 = 3.05$\n$\\sum_{t=2}^{5} S_{t} = S_2 + S_3 + S_4 + S_5 = 0.93 + 0.77 + 0.53 + 0.21 = 2.44$\n\nFor $C$ and $R$:\n$\\sum_{t=1}^{5} y_t m_t = (0.9)(0.8) + (1.2)(0.95) + (0.7)(0.75) + (0.4)(0.5) + (0.1)(0.2) = 0.72 + 1.14 + 0.525 + 0.2 + 0.02 = 2.605$\n$\\sum_{t=1}^{5} S_t = S_1 + S_2 + S_3 + S_4 + S_5 = 3.05 + 0.21 = 3.26$\n$\\sum_{t=1}^{5} y_t^2 = 0.9^2 + 1.2^2 + 0.7^2 + 0.4^2 + 0.1^2 = 0.81 + 1.44 + 0.49 + 0.16 + 0.01 = 2.91$\n\nNow, we substitute these sums into the update rules:\n$A_{new} = \\frac{2.38}{3.05} = \\frac{238}{305}$\n\n$Q_{new} = \\frac{1}{5-1} \\left( \\sum_{t=2}^{5}S_t - A_{new}\\sum_{t=2}^{5}S_{t,t-1} \\right) = \\frac{1}{4} \\left( 2.44 - \\frac{238}{305} \\times 2.38 \\right) = \\frac{1}{4} \\left( \\frac{244}{100} - \\frac{238 \\times 238}{305 \\times 100} \\right)$\n$Q_{new} = \\frac{1}{400} \\left( \\frac{244 \\times 305 - 238^2}{305} \\right) = \\frac{74420 - 56644}{400 \\times 305} = \\frac{17776}{122000} = \\frac{1111}{7625}$\n\n$C_{new} = \\frac{\\sum_{t=1}^{5}y_t m_t}{\\sum_{t=1}^{5}S_t} = \\frac{2.605}{3.26} = \\frac{2605}{3260} = \\frac{521}{652}$\n\n$R_{new} = \\frac{1}{5} \\left( \\sum_{t=1}^{5}y_t^2 - C_{new}\\sum_{t=1}^{5}y_t m_t \\right) = \\frac{1}{5} \\left( 2.91 - \\frac{521}{652} \\times 2.605 \\right) = \\frac{1}{5} \\left( \\frac{291}{100} - \\frac{521}{652} \\frac{2605}{1000} \\right)$\n$R_{new} = \\frac{1}{5} \\left( \\frac{291}{100} - \\frac{521 \\times 521}{652 \\times 200} \\right) = \\frac{1}{5} \\left( \\frac{291}{100} - \\frac{271441}{130400} \\right) = \\frac{1}{5} \\left( \\frac{291 \\times 1304 - 271441}{130400} \\right)$\n$R_{new} = \\frac{1}{5} \\left( \\frac{379464 - 271441}{130400} \\right) = \\frac{1}{5} \\frac{108023}{130400} = \\frac{108023}{652000}$\n\nThe updated parameters are:\n$A = \\frac{238}{305}$\n$Q = \\frac{1111}{7625}$\n$C = \\frac{521}{652}$\n$R = \\frac{108023}{652000}$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{238}{305}  \\frac{1111}{7625}  \\frac{521}{652}  \\frac{108023}{652000} \\end{pmatrix} } $$"
        },
        {
            "introduction": "While exact inference is possible for linear Gaussian state-space models, most real-world systems exhibit non-linearities that necessitate approximate methods like Sequential Monte Carlo (SMC), also known as particle filters. A critical component of these filters is the resampling step, which combats particle degeneracy and is key to algorithmic efficiency. This exercise  provides a deep dive into the mechanics of resampling, asking you to analytically compare the variance of two popular schemes and thus revealing the statistical advantages of more sophisticated approaches.",
            "id": "3346829",
            "problem": "Consider a Sequential Monte Carlo (SMC) particle filter with $N$ particles and normalized importance weights $w_1,\\dots,w_N$ satisfying $\\sum_{i=1}^{N} w_i = 1$ and $w_i \\in (0,1)$ for all $i \\in \\{1,\\dots,N\\}$. Two resampling schemes are considered for generating offspring counts for each particle: multinomial resampling and residual resampling.\n\nUnder multinomial resampling, the offspring count vector is drawn from a multinomial distribution with $N$ trials and class probabilities $(w_1,\\dots,w_N)$.\n\nUnder residual resampling, for each $i \\in \\{1,\\dots,N\\}$, define the deterministic allocation $a_i = \\lfloor N w_i \\rfloor$, the residual $r_i = N w_i - a_i \\in [0,1)$, and the total residual count $R = \\sum_{j=1}^{N} r_j \\in \\{0,1,\\dots,N-1\\}$. Then $a_i$ offspring are first deterministically assigned to particle $i$ for all $i$, and the remaining $R$ offspring are allocated by sampling from a multinomial distribution with $R$ trials and class probabilities proportional to the residuals $(r_1,\\dots,r_N)$, that is, probabilities $(r_1/R,\\dots,r_N/R)$. Assume $R \\ge 1$ to avoid the degenerate case.\n\nLet $O_i^{\\text{mult}}$ denote the offspring count of particle $i$ under multinomial resampling and $O_i^{\\text{res}}$ the offspring count under residual resampling. Starting only from the above definitions and standard properties of the binomial and multinomial distributions, derive the distribution of $O_i^{\\text{res}}$, compute $\\operatorname{Var}(O_i^{\\text{res}})$, compute $\\operatorname{Var}(O_i^{\\text{mult}})$, and then simplify the ratio\n$$\\rho_i \\equiv \\frac{\\operatorname{Var}(O_i^{\\text{res}})}{\\operatorname{Var}(O_i^{\\text{mult}})}.$$\nProvide your final answer as a single closed-form analytic expression for $\\rho_i$ in terms of $N$, $w_i$, $r_i$, and $R$. No numerical approximation is required.",
            "solution": "The problem requires the derivation of the distribution and variance of offspring counts for two resampling schemes, multinomial and residual, and then the calculation of the ratio of their variances. We will proceed by analyzing each scheme separately.\n\nFirst, we analyze the multinomial resampling scheme.\nThe vector of offspring counts, $(O_1^{\\text{mult}}, \\dots, O_N^{\\text{mult}})$, is specified to be drawn from a multinomial distribution with $N$ trials and probabilities $(w_1, \\dots, w_N)$. A standard property of the multinomial distribution is that the marginal distribution for a single component, $O_i^{\\text{mult}}$, follows a binomial distribution. This is because $O_i^{\\text{mult}}$ can be viewed as counting the number of successes in $N$ independent Bernoulli trials, where a \"success\" corresponds to selecting particle $i$ with probability $w_i$.\nTherefore, the distribution of $O_i^{\\text{mult}}$ is:\n$$O_i^{\\text{mult}} \\sim \\text{Binomial}(N, w_i)$$\nThe probability mass function (PMF) is given by $P(O_i^{\\text{mult}} = k) = \\binom{N}{k} w_i^k (1-w_i)^{N-k}$ for integers $k$ from $0$ to $N$.\nThe variance of a binomial distribution $\\text{Binomial}(n, p)$ is known to be $np(1-p)$. Applying this formula, we find the variance of the offspring count for particle $i$ under multinomial resampling:\n$$\\operatorname{Var}(O_i^{\\text{mult}}) = N w_i (1 - w_i)$$\n\nNext, we analyze the residual resampling scheme.\nThe offspring count for particle $i$, $O_i^{\\text{res}}$, is constructed in two stages. First, a deterministic number of offspring, $a_i = \\lfloor N w_i \\rfloor$, is assigned to particle $i$. The remaining $R = \\sum_{j=1}^{N} r_j$ offspring, where $r_j = N w_j - a_j$, are then distributed randomly. This random allocation is performed by drawing from a multinomial distribution with $R$ trials and probabilities $(r_1/R, \\dots, r_N/R)$.\nLet $O_{i, \\text{res}}$ denote the number of offspring assigned to particle $i$ in this second, random stage. The total offspring count for particle $i$ is:\n$$O_i^{\\text{res}} = a_i + O_{i, \\text{res}}$$\nAs in the multinomial case, the marginal distribution of $O_{i, \\text{res}}$ is binomial:\n$$O_{i, \\text{res}} \\sim \\text{Binomial}\\left(R, \\frac{r_i}{R}\\right)$$\nSince $O_i^{\\text{res}}$ is a sum of a constant $a_i$ and a binomially distributed random variable $O_{i, \\text{res}}$, its distribution is a shifted binomial distribution. The possible values for $O_i^{\\text{res}}$ are $\\{a_i, a_i+1, \\dots, a_i+R\\}$. The PMF is:\n$$P(O_i^{\\text{res}} = k) = P(O_{i, \\text{res}} = k - a_i) = \\binom{R}{k-a_i} \\left(\\frac{r_i}{R}\\right)^{k-a_i} \\left(1 - \\frac{r_i}{R}\\right)^{R-(k-a_i)}$$\nfor $k \\in \\{a_i, a_i+1, \\dots, a_i+R\\}$.\n\nTo compute the variance of $O_i^{\\text{res}}$, we use the property that adding a constant does not change the variance of a random variable:\n$$\\operatorname{Var}(O_i^{\\text{res}}) = \\operatorname{Var}(a_i + O_{i, \\text{res}}) = \\operatorname{Var}(O_{i, \\text{res}})$$\nUsing the variance formula for the binomial distribution $\\text{Binomial}\\left(R, \\frac{r_i}{R}\\right)$, we get:\n$$\\operatorname{Var}(O_{i, \\text{res}}) = R \\cdot \\frac{r_i}{R} \\cdot \\left(1 - \\frac{r_i}{R}\\right) = r_i \\left(1 - \\frac{r_i}{R}\\right)$$\nThus, the variance of the offspring count under residual resampling is:\n$$\\operatorname{Var}(O_i^{\\text{res}}) = r_i \\left(1 - \\frac{r_i}{R}\\right)$$\n\nFinally, we are asked to simplify the ratio $\\rho_i \\equiv \\frac{\\operatorname{Var}(O_i^{\\text{res}})}{\\operatorname{Var}(O_i^{\\text{mult}})}$.\nWe substitute the expressions for the two variances we have derived:\n$$\\rho_i = \\frac{r_i \\left(1 - \\frac{r_i}{R}\\right)}{N w_i(1 - w_i)}$$\nThis is the final closed-form expression for the ratio $\\rho_i$ in terms of the given quantities $N$, $w_i$, $r_i$, and $R$.",
            "answer": "$$\\boxed{\\frac{r_i \\left(1 - \\frac{r_i}{R}\\right)}{N w_i(1 - w_i)}}$$"
        }
    ]
}