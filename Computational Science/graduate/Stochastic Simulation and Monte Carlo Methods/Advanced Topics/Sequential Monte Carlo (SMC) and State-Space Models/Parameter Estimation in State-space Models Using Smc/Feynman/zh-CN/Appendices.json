{
    "hands_on_practices": [
        {
            "introduction": "理论知识需要通过实践来巩固。这个练习是您学习序列蒙特卡洛（SMC）方法进行参数估计的第一步。您将为一个线性高斯状态空间模型实现一个自助粒子滤波器（bootstrap particle filter），这是一种基础但功能强大的SMC算法。通过这个练习，您不仅能掌握通过网格搜索估计静态参数的核心逻辑，还将亲手实现并比较多种重采样方案，从而深刻理解它们对粒子多样性和估计性能的影响。",
            "id": "3326835",
            "problem": "考虑一个用于潜状态 $x_{t}$ 和观测值 $y_{t}$ 的线性高斯状态空间模型，其中自回归系数 $\\phi$ 和过程噪声标准差 $q$ 已知，而观测噪声标准差 $r$ 未知。该模型由以下两个方程给出：\n- 状态转移：$x_{t} = \\phi x_{t-1} + q \\,\\eta_{t}$，其中 $\\eta_{t} \\sim \\mathcal{N}(0,1)$。\n- 观测：$y_{t} = x_{t} + r \\,\\epsilon_{t}$，其中 $\\epsilon_{t} \\sim \\mathcal{N}(0,1)$。\n\n您的任务是使用序贯蒙特卡洛（SMC）方法和自举粒子滤波器，为 $r$ 实现一个参数估计程序，并比较不同重采样方案及其在参数估计精度方面的特性。\n\n您的工作必须基于以下基本原理和定义：\n- 马尔可夫性质：$p(x_{t} \\mid x_{1:t-1}) = p(x_{t} \\mid x_{t-1})$ 和观测值的条件独立性：$p(y_{t} \\mid x_{1:t}) = p(y_{t} \\mid x_{t})$。\n- 用于序贯推断的贝叶斯定理：滤波分布 $p(x_{t} \\mid y_{1:t})$ 可以通过似然 $p(y_{t} \\mid x_{t})$ 和转移 $p(x_{t} \\mid x_{t-1})$ 从 $p(x_{t-1} \\mid y_{1:t-1})$ 更新得到。\n- 自举粒子滤波器从转移分布 $p(x_{t} \\mid x_{t-1})$ 中抽取粒子，并根据似然 $p(y_{t} \\mid x_{t})$ 成比例地更新权重。边缘似然分解 $p(y_{1:T}) = \\prod_{t=1}^{T} p(y_{t} \\mid y_{1:t-1})$ 源于概率的链式法则。\n- 有效样本量（ESS）定义为 $\\mathrm{ESS} = \\left(\\sum_{i=1}^{N} w_{t}^{(i)2}\\right)^{-1}$，其中 $w_{t}^{(i)}$ 是归一化权重，重采样触发条件由 $\\mathrm{ESS}/N \\le \\tau$ 决定，其中 $\\tau \\in [0,1]$ 是一个阈值。\n\n实现要求：\n- 使用固定的随机种子和指定的模型参数，模拟一个长度为 $T$ 的单个数据集。使用平稳先验 $x_{0} \\sim \\mathcal{N}\\!\\left(0, \\frac{q^{2}}{1-\\phi^{2}}\\right)$，并根据模型模拟 $x_{t}$ 和 $y_{t}$。\n- 实现一个带有 $N$ 个粒子的自举粒子滤波器，该滤波器能为任何提供的候选值 $r$ 估计对数边缘似然 $\\log p(y_{1:T} \\mid r)$。\n- 实现四种重采样方案：多项式重采样、分层重采样、系统重采样和残差重采样。当发生重采样时，用重采样后的祖先粒子替换粒子，并将权重重置为均匀分布。\n- 使用基于ESS的规则 $\\mathrm{ESS}/N \\le \\tau$ 来实现重采样触发器。\n- 对于参数估计，评估一个候选观测噪声标准差的网格，并选择估计的对数边缘似然的最大化者作为点估计 $\\hat{r}$。\n\n数据集生成协议：\n- 使用 $\\phi = 0.9$, $q = 1.0$, $r_{\\text{true}} = 0.5$, $T = 200$。\n- 使用固定的数据集生成种子 $s_{\\text{data}} = 20240517$。\n- 生成 $x_{0} \\sim \\mathcal{N}\\!\\left(0, \\frac{q^{2}}{1-\\phi^{2}}\\right)$，然后相应地模拟 $\\{x_{t}\\}_{t=1}^{T}$ 和 $\\{y_{t}\\}_{t=1}^{T}$。\n\n粒子滤波和重采样细节：\n- 使用 $N = 1000$ 个粒子。\n- 在时间 $t$，通过转移密度传播每个粒子，并计算与在 $y_{t}$ 处的观测似然成比例的权重。\n- 将每个时间步的增量归一化常数计算为未归一化权重的平均值，并累积其对数以形成对数边缘似然估计。\n- 使用归一化权重计算ESS，并应用重采样触发器 $\\mathrm{ESS}/N \\le \\tau$。\n- 实现以下重采样方案：\n  - 多项式重采样。\n  - 分层重采样。\n  - 系统重采样。\n  - 残差重采样。\n\n参数网格和可复现性：\n- 使用候选网格 $\\mathcal{R} = \\{0.25, 0.5, 0.75, 1.0\\}$。\n- 为了在测试用例和候选参数之间实现可复现性，使用一个基础种子 $s_{\\text{base}} = 987654321$，并将测试用例 $j \\in \\{0,1,2,3,4,5\\}$ 中索引为 $k \\in \\{0,1,2,3\\}$ 的候选参数的粒子滤波器随机种子设置为 $s_{\\text{base}} + 1000\\,j + k$。该种子控制该次评估中粒子滤波器的所有内部随机性，包括初始粒子和所有重采样随机数。\n\n测试套件：\n- 共有 $6$ 个测试用例，每个都是一个对 $(\\text{scheme}, \\tau)$：\n  1. $(\\text{multinomial}, 0.5)$\n  2. $(\\text{stratified}, 0.5)$\n  3. $(\\text{systematic}, 0.5)$\n  4. $(\\text{residual}, 0.5)$\n  5. $(\\text{multinomial}, 1.0)$\n  6. $(\\text{stratified}, 0.0)$\n\n对于每个测试用例，通过在 $r \\in \\mathcal{R}$ 上最大化估计的 $\\log p(y_{1:T} \\mid r)$ 来计算 $\\hat{r}$，并报告绝对误差 $|\\hat{r} - r_{\\text{true}}|$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，其中第 $j$ 个条目是第 $j$ 个测试用例的绝对误差，顺序如上所列。例如，一个有效的输出格式为 $[\\text{err}_{1},\\text{err}_{2},\\ldots,\\text{err}_{6}]$。",
            "solution": "所提出的问题是一个有效且定义明确的计算统计学练习，特别关注在线性高斯状态空间模型中使用序贯蒙特卡洛（SMC）方法进行参数估计。所有必要的参数、模型和程序细节都已提供，并且该问题在科学上基于贝叶斯推断和随机模拟的原理。我们将着手提供一个完整的解决方案。\n\n问题的核心是估计状态空间模型的一个静态参数，即观测噪声标准差 $r$。该模型由一个状态转移方程和一个观测方程定义。\n\n状态转移方程描述了潜（未观测）状态 $x_t$ 随时间 $t$ 的演变：\n$$x_{t} = \\phi x_{t-1} + q \\,\\eta_{t}, \\quad \\eta_{t} \\sim \\mathcal{N}(0,1)$$\n这是一个一阶自回归过程（AR($1$)），具有已知的系数 $\\phi$ 和标准差为 $q$ 的过程噪声。\n\n观测方程将潜状态 $x_t$ 与观测数据 $y_t$ 联系起来：\n$$y_{t} = x_{t} + r \\,\\epsilon_{t}, \\quad \\epsilon_{t} \\sim \\mathcal{N}(0,1)$$\n观测值是状态的带噪版本，其中观测噪声的标准差 $r$ 是未知的。\n\n我们的目标是给定一个观测序列 $y_{1:T} = \\{y_1, y_2, \\dots, y_T\\}$，找到 $r$ 的一个估计 $\\hat{r}$。一个标准且强大的方法是最大似然估计。我们旨在找到使观测值的边缘似然 $p(y_{1:T} \\mid r)$ 最大化的 $r$ 值。\n$$\\hat{r} = \\underset{r}{\\arg\\max} \\, p(y_{1:T} \\mid r) = \\underset{r}{\\arg\\max} \\, \\log p(y_{1:T} \\mid r)$$\n对于状态空间模型，边缘似然通常难以直接计算，因为它需要对潜状态进行积分：\n$$p(y_{1:T} \\mid r) = \\int p(y_{1:T}, x_{0:T} \\mid r) \\,dx_{0:T}$$\nSMC方法，也称为粒子滤波器，提供了一种近似这个量的方法。粒子滤波器的关键思想是用一组 $N$ 个加权样本或“粒子” $\\{x_t^{(i)}, W_t^{(i)}\\}_{i=1}^N$ 来表示滤波分布 $p(x_t \\mid y_{1:t})$。该分布随后由一个经验测度近似：\n$$p(x_t \\mid y_{1:t}) \\approx \\sum_{i=1}^N W_t^{(i)} \\delta(x_t - x_t^{(i)})$$\n其中 $\\delta(\\cdot)$ 是狄拉克δ函数。\n\n自举粒子滤波器是此处采用的一种特定类型的SMC算法。它按时间顺序操作，在每个新观测 $y_t$ 到达时更新粒子集。它利用了边缘似然的分解：\n$$\\log p(y_{1:T} \\mid r) = \\sum_{t=1}^T \\log p(y_t \\mid y_{1:t-1}, r)$$\n粒子滤波器为每一项 $p(y_t \\mid y_{1:t-1}, r)$ 提供一个蒙特卡洛估计。\n\n算法流程如下：\n1.  **初始化 ($t=0$)：**\n    我们首先从先验分布 $p(x_0)$ 中抽取 $N$ 个粒子 $\\{x_0^{(i)}\\}_{i=1}^N$。问题指定使用AR($1$)过程的平稳分布，即 $x_0 \\sim \\mathcal{N}(0, q^2/(1-\\phi^2))$。初始权重是均匀的，对于所有 $i \\in \\{1, \\dots, N\\}$，$W_0^{(i)} = 1/N$。\n\n2.  **序贯更新 (对于 $t=1, \\dots, T$)：**\n    对于每个时间步，执行以下操作：\n    a. **传播：** 根据状态转移模型演化每个粒子。由于自举滤波器使用转移动态作为其提议分布，我们采样：\n    $$x_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)}) = \\mathcal{N}(x_t; \\phi x_{t-1}^{(i)}, q^2)$$\n    b. **加权：** 基于每个新粒子 $x_t^{(i)}$ 对新观测 $y_t$ 的解释程度来计算其重要性权重。未归一化的权重 $\\tilde{w}_t^{(i)}$ 由给定粒子状态的观测似然给出：\n    $$\\tilde{w}_t^{(i)} = p(y_t \\mid x_t^{(i)}, r) = \\frac{1}{\\sqrt{2\\pi}r} \\exp\\left(-\\frac{(y_t - x_t^{(i)})^2}{2r^2}\\right)$$\n    c. **似然估计：** 预测似然 $p(y_t \\mid y_{1:t-1}, r)$ 通过未归一化权重的平均值来近似：\n    $$\\hat{p}(y_t \\mid y_{1:t-1}, r) \\approx \\frac{1}{N} \\sum_{i=1}^N \\tilde{w}_t^{(i)}$$\n    该值的对数被加到总的对数边缘似然估计中。\n    d. **归一化：** 权重被归一化，使其总和为 $1$：\n    $$W_t^{(i)} = \\frac{\\tilde{w}_t^{(i)}}{\\sum_{j=1}^N \\tilde{w}_t^{(j)}}$$\n    e. **重采样：** 粒子滤波中的一个常见问题是权重退化，即经过几个步骤后，一个粒子的权重接近 $1$，而所有其他粒子的权重接近 $0$。这通过有效样本量（ESS）来监控，估计为：\n    $$\\mathrm{ESS} = \\left(\\sum_{i=1}^{N} (W_{t}^{(i)})^2\\right)^{-1}$$\n    如果ESS下降到总粒子数的某个分数 $\\tau$ 以下，即 $\\mathrm{ESS}/N \\le \\tau$，则执行重采样步骤。重采样涉及从当前集合 $\\{x_t^{(i)}\\}_{i=1}^N$ 中有放回地抽取 $N$ 个新粒子，其中抽取粒子 $i$ 的概率是其权重 $W_t^{(i)}$。这消除了低权重粒子并复制了高权重粒子。重采样后，权重被重置为均匀的 $W_t^{(i)} = 1/N$。\n\n问题要求比较四种重采样方案：\n*   **多项式重采样：** 这是最直接的方法，等同于从由权重定义的分类分布中抽取 $N$ 个样本。\n*   **系统重采样：** 该方案减少了重采样步骤的蒙特卡洛方差。它生成一个单一的随机数 $u \\in [0, 1/N)$，然后创建一系列 $N$ 个有序指针 $u, u+1/N, \\dots, u+(N-1)/N$。这些指针用于从权重的累积分布中选择粒子。\n*   **分层重采样：** 这是一种相关的方差缩减技术。它将区间 $[0, 1)$ 分为 $N$ 个层，并从每个层中抽取一个随机样本。然后，这 $N$ 个样本像系统情况一样充当指针。\n*   **残差重采样：** 这是一种混合方法。它首先确定性地复制权重 $W_t^{(i)} > 1/N$ 的粒子。具体来说，每个粒子 $i$ 获得 $\\lfloor N W_t^{(i)} \\rfloor$ 个副本。剩余数量的粒子然后通过使用权重的残差部分进行多项式重采样来抽取。\n\n为了实现该解决方案，首先使用真实参数 $\\phi=0.9$、$q=1.0$ 和 $r_{\\text{true}}=0.5$ 以及指定的随机种子生成一个长度为 $T=200$ 的单个数据集。然后，对于由（重采样方案，阈值 $\\tau$）对定义的每个测试用例，为网格 $\\mathcal{R}=\\{0.25, 0.5, 0.75, 1.0\\}$ 中的每个候选 $r$ 估计对数边缘似然 $\\log \\hat{p}(y_{1:T} \\mid r)$。选择产生最高对数似然的候选 $\\hat{r}$ 作为估计值。然后计算并报告绝对误差 $|\\hat{r} - r_{\\text{true}}|$。种子协议确保粒子滤波器的每次评估都是确定性的。测试用例探讨了不同重采样方案和触发阈值的行为：$\\tau=0.5$ 是一个常见的选择，$\\tau=1.0$ 强制在每一步都进行重采样，而 $\\tau=0.0$ 则完全阻止重采样。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\n#\n# --- Problem Constants and Definitions ---\n#\n\n# Dataset parameters\nPHI = 0.9\nQ = 1.0\nR_TRUE = 0.5\nT = 200\nDATA_SEED = 20240517\n\n# Particle filter parameters\nN_PARTICLES = 1000\nR_GRID = [0.25, 0.5, 0.75, 1.0]\n\n# Reproducibility parameters\nBASE_SEED = 987654321\n\n# Test cases: (resampling_scheme_name, resampling_threshold_tau)\nTEST_CASES = [\n    ('multinomial', 0.5),\n    ('stratified', 0.5),\n    ('systematic', 0.5),\n    ('residual', 0.5),\n    ('multinomial', 1.0),\n    ('stratified', 0.0),\n]\n\n\n#\n# --- Data Generation ---\n#\n\ndef generate_data(phi, q, r_true, T, seed):\n    \"\"\"\n    Generates a single time series dataset from the linear Gaussian state-space model.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Initial state from stationary distribution\n    std_x0 = q / np.sqrt(1 - phi**2)\n    x = np.zeros(T + 1)\n    y = np.zeros(T)\n    x[0] = rng.normal(0, std_x0)\n    \n    # Simulate states and observations for t = 1 to T\n    for t in range(1, T + 1):\n        x[t] = phi * x[t-1] + q * rng.normal()\n        y[t-1] = x[t] + r_true * rng.normal()\n        \n    return y\n\n\n#\n# --- Resampling Schemes ---\n#\n\ndef multinomial_resample(weights, num_samples, rng):\n    \"\"\"Performs multinomial resampling.\"\"\"\n    return rng.choice(len(weights), size=num_samples, p=weights, replace=True)\n\ndef systematic_resample(weights, num_samples, rng):\n    \"\"\"Performs systematic resampling.\"\"\"\n    positions = (rng.random() + np.arange(num_samples)) / num_samples\n    cumulative_weights = np.cumsum(weights)\n    return np.searchsorted(cumulative_weights, positions)\n\ndef stratified_resample(weights, num_samples, rng):\n    \"\"\"Performs stratified resampling.\"\"\"\n    positions = (rng.random(size=num_samples) + np.arange(num_samples)) / num_samples\n    cumulative_weights = np.cumsum(weights)\n    return np.searchsorted(cumulative_weights, positions)\n\ndef residual_resample(weights, num_samples, rng):\n    \"\"\"Performs residual resampling.\"\"\"\n    num_copies = np.floor(num_samples * weights).astype(int)\n    num_residual = num_samples - np.sum(num_copies)\n    \n    indices = np.repeat(np.arange(num_samples), num_copies)\n    \n    if num_residual > 0:\n        residual_weights = (num_samples * weights) - num_copies\n        # Normalize residual weights to form a probability distribution\n        residual_sum = np.sum(residual_weights)\n        if residual_sum > 1e-12:\n            residual_weights /= residual_sum\n            residual_indices = rng.choice(num_samples, size=num_residual, p=residual_weights, replace=True)\n            indices = np.concatenate([indices, residual_indices])\n\n    return indices\n\nRESAMPLING_SCHEMES = {\n    'multinomial': multinomial_resample,\n    'stratified': stratified_resample,\n    'systematic': systematic_resample,\n    'residual': residual_resample,\n}\n\n\n#\n# --- Particle Filter Implementation ---\n#\n\ndef run_particle_filter(obs, phi, q, r_candidate, N, scheme, tau, seed):\n    \"\"\"\n    Runs a bootstrap particle filter to estimate the log marginal likelihood.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    T_obs = len(obs)\n    \n    # 1. Initialization\n    std_x0 = q / np.sqrt(1 - phi**2)\n    particles = rng.normal(0, std_x0, N)\n    log_likelihood = 0.0\n    \n    # 2. Sequential Importance Sampling and Resampling\n    for t in range(T_obs):\n        # a. Propagate particles\n        particles = phi * particles + q * rng.normal(size=N)\n\n        # b. Compute unnormalized weights\n        unnormalized_weights = norm.pdf(obs[t], loc=particles, scale=r_candidate)\n\n        # c. Update log-likelihood estimate\n        mean_weight = np.mean(unnormalized_weights)\n        if mean_weight > 1e-100: # Avoid log(0)\n            log_likelihood += np.log(mean_weight)\n        else:\n            return -np.inf # Log-likelihood is effectively -infinity\n\n        # d. Normalize weights\n        sum_weights = np.sum(unnormalized_weights)\n        if sum_weights > 1e-100:\n            normalized_weights = unnormalized_weights / sum_weights\n        else: # All particles have zero weight\n            # This can happen if r_candidate is very wrong. Re-initialize to avoid crash.\n            particles = rng.normal(0, std_x0, N)\n            normalized_weights = np.full(N, 1.0 / N)\n\n        # e. Resampling\n        ess = 1.0 / np.sum(normalized_weights**2)\n        if (ess / N) = tau:\n            resampling_func = RESAMPLING_SCHEMES[scheme]\n            indices = resampling_func(normalized_weights, N, rng)\n            particles = particles[indices]\n            # Weights are reset to uniform implicitly for the next propagation step\n            # because the next unnormalized weights will be calculated from a\n            # uniformly weighted particle set. We only need to store the normalized\n            # weights for the ESS check for the current step.\n\n    return log_likelihood\n\n\n#\n# --- Main Solver ---\n#\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem specification.\n    \"\"\"\n    y_obs = generate_data(PHI, Q, R_TRUE, T, DATA_SEED)\n    \n    results = []\n\n    for j, (scheme, tau) in enumerate(TEST_CASES):\n        log_likelihoods = []\n        for k, r_candidate in enumerate(R_GRID):\n            \n            # Set seed for this specific run for reproducibility\n            run_seed = BASE_SEED + 1000 * j + k\n            \n            log_lik = run_particle_filter(\n                obs=y_obs,\n                phi=PHI,\n                q=Q,\n                r_candidate=r_candidate,\n                N=N_PARTICLES,\n                scheme=scheme,\n                tau=tau,\n                seed=run_seed\n            )\n            log_likelihoods.append(log_lik)\n        \n        # Find the r that maximizes the log-likelihood\n        best_r_index = np.argmax(log_likelihoods)\n        r_hat = R_GRID[best_r_index]\n        \n        # Calculate absolute error\n        error = abs(r_hat - R_TRUE)\n        results.append(error)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solver\nsolve()\n\n```"
        },
        {
            "introduction": "成功估计出模型参数只是任务的一半，验证模型的有效性同样至关重要。此练习将引导您超越参数估计，进入模型诊断这一高级领域。您将学习如何利用粒子滤波器产生的预测残差，特别是通过概率积分变换（Probability Integral Transform, PIT）来评估模型是否正确指定。通过构建和分析综合性的差异统计量，您将获得检测模型与数据之间系统性偏差的实用技能，这是严谨科学建模中不可或缺的一环。",
            "id": "3326831",
            "problem": "考虑一个单变量线性高斯状态空间模型，其潜在状态和观测值由时间索引 $t \\in \\{1,2,\\dots,T\\}$ 的以下关系定义\n$$\nx_t = a \\, x_{t-1} + \\sqrt{q} \\, \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0,1),\n$$\n$$\ny_t = c \\, x_t + \\sqrt{r} \\, \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0,1),\n$$\n其中 $(a,q,r,c)$ 是汇集到 $\\theta = (a,q,r,c)$ 中的未知参数，$x_t$ 是潜在状态，$y_t$ 是观测值。令 $y_{1:t} = (y_1,\\dots,y_t)$ 表示截至时间 $t$ 的观测序列，并令 $\\mathcal{N}(0,1)$ 表示标准正态分布。\n\n序贯蒙特卡洛（SMC）方法，特别是粒子滤波器（PF），通过一个加权粒子系综来近似滤波分布 $p(x_t \\mid y_{1:t}, \\theta)$，这些粒子通过潜在动态进行传播，并由观测值的似然重新加权。在正确的模型设定下，一步向前预测分布 $p(y_t \\mid y_{1:t-1}, \\theta)$ 与实现的 $y_t$ 是一致的。一种诊断模型设定错误的原则性方法是检查由基于 PF 的预测矩构建的预测残差，并执行后验预测检验。\n\n你的任务是实现一个基于 SMC 的后验预测检验，该检验比较不同 $\\theta$ 样本的 PF 残差，并标记出潜在动态中的系统性差异。从以下基本依据出发：\n\n- 上述状态空间模型的定义及其所蕴含的条件独立结构。\n- 应用于预测矩的全期望定律和全方差定律，以及概率积分变换（PIT）的定义：如果 $Y$ 的累积分布函数为 $F_Y$，那么在正确的设定下，$U = F_Y(Y)$ 在 $(0,1)$ 上服从均匀分布。\n- 在正确设定的线性高斯模型下，标准化新息近似为独立同分布的标准正态分布，因此 PIT 值近似为在 $(0,1)$ 上独立同分布的均匀分布这一原理。\n\n对每个候选参数向量 $\\theta$ 执行以下步骤：\n\n- 使用带有 $N$ 个粒子的自助粒子滤波器（PF）和多项式或系统重采样来近似 $p(x_{t-1} \\mid y_{1:t-1}, \\theta)$。根据此经验预测集，形成给定 $y_{1:t-1}$ 时 $y_t$ 的一步向前预测矩，\n$$\n\\hat{\\mu}_t(\\theta) = c \\, \\mathbb{E}_{\\text{PF}}[x_t \\mid y_{1:t-1}, \\theta], \\quad \\hat{\\sigma}^2_t(\\theta) = c^2 \\, \\operatorname{Var}_{\\text{PF}}[x_t \\mid y_{1:t-1}, \\theta] + r,\n$$\n其中期望和方差是根据观测到 $y_t$ 之前对 $x_t$ 的 PF 预测粒子计算得出的。\n- 定义标准化一步向前预测残差\n$$\nz_t(\\theta) = \\frac{y_t - \\hat{\\mu}_t(\\theta)}{\\hat{\\sigma}_t(\\theta)},\n$$\n及概率积分变换\n$$\nu_t(\\theta) = \\Phi\\!\\left(z_t(\\theta)\\right),\n$$\n其中 $\\Phi$ 是 $\\mathcal{N}(0,1)$ 的累积分布函数。在正确的设定下，序列 $\\{u_t(\\theta)\\}_{t=1}^T$ 应近似为在 $(0,1)$ 上独立同分布的均匀分布。\n- 构建一个差异性统计量，该统计量联合度量边际均匀性和时间独立性。令 $u_{(1)} \\le \\dots \\le u_{(T)}$ 为排序后的 PIT 值。定义双边 Kolmogorov–Smirnov 距离\n$$\nD_{\\mathrm{KS}}(\\theta) = \\max\\!\\left\\{\\max_{1 \\le k \\le T} \\left(\\frac{k}{T} - u_{(k)}\\right), \\; \\max_{1 \\le k \\le T} \\left(u_{(k)} - \\frac{k-1}{T}\\right)\\right\\},\n$$\n和中心化 PIT 序列的滞后-1 自相关\n$$\n\\rho_1(\\theta) = \\frac{\\sum_{t=2}^T \\left(u_t(\\theta) - \\bar{u}(\\theta)\\right)\\left(u_{t-1}(\\theta) - \\bar{u}(\\theta)\\right)}{\\sum_{t=1}^T \\left(u_t(\\theta) - \\bar{u}(\\theta)\\right)^2},\n$$\n其中 $\\bar{u}(\\theta) = \\frac{1}{T}\\sum_{t=1}^T u_t(\\theta)$。最后，定义组合差异性\n$$\nD(\\theta) = D_{\\mathrm{KS}}(\\theta) + \\left|\\rho_1(\\theta)\\right|.\n$$\n- 如果 $D(\\theta)  \\tau$（其中 $\\tau$ 是一个预先指定的阈值），则将 $\\theta$ 标记为设定错误。\n\n为下述每个测试用例实现上述检验。在每个用例中，通过在指定的真实参数 $\\theta^\\star$ 和固定的随机种子下模拟潜在序列和观测序列来生成合成数据。然后，使用具有指定粒子数 $N$ 和指定阈值 $\\tau$ 的 PF，在一组候选 $\\theta$ 值上评估差异性 $D(\\theta)$。使用初始分布 $x_0 \\sim \\mathcal{N}\\!\\left(0, \\sigma_0^2(\\theta)\\right)$，其中当 $a^2  1$ 时，$\\sigma_0^2(\\theta) = \\frac{q}{1-a^2}$；如果 $a^2 \\ge 1$，则设 $\\sigma_0^2(\\theta) = 10\\,q$。\n\n测试套件：\n\n- 用例 1（一般情况，包括正确设定的动态）：\n  - 真实参数 $\\theta^\\star = (a^\\star,q^\\star,r^\\star,c^\\star) = (0.9, 0.16, 0.04, 1.0)$。\n  - 时间长度 $T = 300$。\n  - 粒子数量 $N = 1500$。\n  - 阈值 $\\tau = 0.12$。\n  - 随机种子 $s = 12345$。\n  - 候选参数列表：\n    - $\\theta_1 = (0.9, 0.16, 0.04, 1.0)$,\n    - $\\theta_2 = (0.88, 0.16, 0.04, 1.0)$,\n    - $\\theta_3 = (0.5, 0.16, 0.04, 1.0)$。\n- 用例 2（候选参数中存在严重设定错误的动态）：\n  - 真实参数 $\\theta^\\star = (0.2, 0.25, 0.09, 1.0)$。\n  - 时间长度 $T = 300$。\n  - 粒子数量 $N = 1500$。\n  - 阈值 $\\tau = 0.12$。\n  - 随机种子 $s = 54321$。\n  - 候选参数列表：\n    - $\\theta_1 = (0.2, 0.25, 0.09, 1.0)$,\n    - $\\theta_2 = (0.9, 0.25, 0.09, 1.0)$,\n    - $\\theta_3 = (0.2, 0.05, 0.09, 1.0)$。\n- 用例 3（接近单位根的边界动态）：\n  - 真实参数 $\\theta^\\star = (0.99, 0.01, 0.01, 1.0)$。\n  - 时间长度 $T = 300$。\n  - 粒子数量 $N = 2000$。\n  - 阈值 $\\tau = 0.12$。\n  - 随机种子 $s = 999$。\n  - 候选参数列表：\n    - $\\theta_1 = (0.99, 0.01, 0.01, 1.0)$,\n    - $\\theta_2 = (0.9, 0.01, 0.01, 1.0)$,\n    - $\\theta_3 = (0.99, 0.25, 0.01, 1.0)$。\n\n程序要求：\n\n- 你的程序必须为每个用例使用 $\\theta^\\star$ 生成合成数据，然后，对于该用例中的每个候选 $\\theta$，运行 PF 来计算 $\\{u_t(\\theta)\\}_{t=1}^T$，评估 $D(\\theta)$，并在 $D(\\theta)  \\tau$ 时标记该候选参数。\n- 对于每个用例，计算被标记的候选参数的整数数量。\n- 最终输出格式：你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，即按顺序排列的用例 1、2 和 3 中被标记的候选参数数量（例如，$[n_1,n_2,n_3]$）。\n- 此纯数学公式中不涉及角度，物理单位也不适用；无需进行单位转换。",
            "solution": "用户提供的问题是有效的。它提出了计算统计学中一个定义明确的任务，具体涉及使用序贯蒙特卡洛（SMC）方法对状态空间模型进行模型验证。该问题在科学上基于粒子滤波器和后验预测检验的理论，是自包含的，提供了所有必要的参数和规范，并以客观、正式的语言表述。满足了有效问题的所有标准。\n\n解决方案涉及实现一个程序，用于计算一组候选参数 $\\theta$ 相对于观测时间序列 $y_{1:T}$ 的差异性统计量 $D(\\theta)$。该程序建立在自助粒子滤波器的基础上。对于所提供的三个测试用例中的每一个，我们首先使用指定的真实参数 $\\theta^\\star$ 生成一个合成观测序列。然后，对于测试用例列表中的每个候选参数向量 $\\theta$，我们通过计算 $D(\\theta)$ 并将其与阈值 $\\tau$ 进行比较来评估其适用性。\n\n### 1. 合成数据生成\n对于每个测试用例，从由参数 $\\theta^\\star = (a^\\star, q^\\star, r^\\star, c^\\star)$ 指定的真实模型中生成一个长度为 $T$ 的合成观测序列 $y_{1:T} = (y_1, \\dots, y_T)$。该过程首先从其平稳分布中抽取一个初始状态 $x_0$。对于一个稳定的过程，其中 $(a^\\star)^2  1$，这是 $x_0 \\sim \\mathcal{N}(0, \\sigma_0^2(\\theta^\\star))$，方差为 $\\sigma_0^2(\\theta^\\star) = q^\\star / (1 - (a^\\star)^2)$。如果过程是非平稳的（$(a^\\star)^2 \\ge 1$），则初始方差设为 $\\sigma_0^2(\\theta^\\star) = 10q^\\star$。随后，根据模型方程模拟 $t \\in \\{1, \\dots, T\\}$ 的潜在状态 $x_t$ 和观测值 $y_t$：\n$$\nx_t = a^\\star x_{t-1} + \\sqrt{q^\\star} \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0,1)\n$$\n$$\ny_t = c^\\star x_t + \\sqrt{r^\\star} \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0,1)\n$$\n每个用例都使用固定的随机种子，以确保整个过程（包括数据生成和随后的粒子滤波）是可复现的。\n\n### 2. 通过粒子滤波器计算差异性\n对于给定的候选参数集 $\\theta = (a, q, r, c)$ 和观测序列 $y_{1:T}$，使用一个有 $N$ 个粒子的自助粒子滤波器来计算差异性。在每个时间步 $t \\in \\{1, \\dots, T\\}$，执行以下操作：\n\n**a. 预测矩估计**：在步骤 $t$ 开始时，我们有一组粒子 $\\{x_{t-1}^{(i)}\\}_{i=1}^N$，它提供了滤波分布 $p(x_{t-1} \\mid y_{1:t-1}, \\theta)$ 的一个蒙特卡洛近似。由于在每一步结束时都执行系统重采样，这些粒子是等权重的。经验均值 $\\mathbb{E}_{\\text{PF}}[x_{t-1}]$ 和方差 $\\operatorname{Var}_{\\text{PF}}[x_{t-1}]$ 是从这个粒子集中计算出来的。利用全期望定律和全方差定律，将这些矩向前传播以获得 $x_t$ 的预测矩：\n$$\n\\mathbb{E}_{\\text{PF}}[x_t \\mid y_{1:t-1}, \\theta] = a \\, \\mathbb{E}_{\\text{PF}}[x_{t-1}]\n$$\n$$\n\\operatorname{Var}_{\\text{PF}}[x_t \\mid y_{1:t-1}, \\theta] = a^2 \\operatorname{Var}_{\\text{PF}}[x_{t-1}] + q\n$$\n由此，我们推导出观测值 $y_t$ 的一步向前预测矩：\n$$\n\\hat{\\mu}_t(\\theta) = c \\, \\mathbb{E}_{\\text{PF}}[x_t \\mid y_{1:t-1}, \\theta]\n$$\n$$\n\\hat{\\sigma}^2_t(\\theta) = c^2 \\operatorname{Var}_{\\text{PF}}[x_t \\mid y_{1:t-1}, \\theta] + r\n$$\n\n**b. 残差和 PIT 计算**：使用预测矩，计算标准化一步向前预测残差 $z_t(\\theta)$：\n$$\nz_t(\\theta) = \\frac{y_t - \\hat{\\mu}_t(\\theta)}{\\sqrt{\\hat{\\sigma}^2_t(\\theta)}}\n$$\n然后使用标准正态累积分布函数 $\\Phi$ 将此残差转换为概率积分变换（PIT）值：\n$$\nu_t(\\theta) = \\Phi(z_t(\\theta))\n$$\n如果模型设定正确，PIT 值序列 $\\{u_t(\\theta)\\}_{t=1}^T$ 应近似为根据 $(0,1)$ 上的均匀分布独立同分布。\n\n**c. 粒子滤波器更新**：更新粒子集以近似 $p(x_t \\mid y_{1:t}, \\theta)$。这包括三个步骤：\n1.  **传播**：每个粒子 $x_{t-1}^{(i)}$ 都使用状态方程向前传播，以获得一个预测粒子 $\\tilde{x}_t^{(i)} = a x_{t-1}^{(i)} + \\sqrt{q} \\epsilon_t^{(i)}$。\n2.  **加权**：每个粒子 $\\tilde{x}_t^{(i)}$ 的重要性权重是根据观测到 $y_t$ 的似然计算的，即 $w_t^{(i)} \\propto p(y_t \\mid \\tilde{x}_t^{(i)}, \\theta) = \\mathcal{N}(y_t; c\\tilde{x}_t^{(i)}, r)$。然后对权重进行归一化。\n3.  **重采样**：根据归一化的权重 $\\{w_t^{(j)}\\}_{j=1}^N$，从传播的集合 $\\{\\tilde{x}_t^{(j)}\\}_{j=1}^N$ 中有放回地抽取一组新的 $N$ 个粒子 $\\{x_t^{(i)}\\}_{i=1}^N$。使用系统重采样来减少蒙特卡洛估计的方差。新粒子是等权重的，并作为下一步时间的基础。\n\n### 3. 差异性统计量和标记\n在遍历所有 $T$ 个观测值并收集 PIT 序列 $\\{u_t(\\theta)\\}_{t=1}^T$ 之后，构建组合差异性统计量 $D(\\theta)$。该统计量度量了 PIT 值偏离理想的独立同分布均匀性质的程度。\n- **Kolmogorov-Smirnov 距离** $D_{\\mathrm{KS}}(\\theta)$ 通过将 PIT 的经验分布函数与均匀累积分布函数进行比较，来度量与边际均匀性的偏离程度：\n$$\nD_{\\mathrm{KS}}(\\theta) = \\max\\!\\left\\{\\max_{1 \\le k \\le T} \\left(\\frac{k}{T} - u_{(k)}\\right), \\; \\max_{1 \\le k \\le T} \\left(u_{(k)} - \\frac{k-1}{T}\\right)\\right\\}\n$$\n其中 $u_{(k)}$ 是排序后的 PIT 值。\n- **滞后-1 自相关** $\\rho_1(\\theta)$ 度量时间依赖性：\n$$\n\\rho_1(\\theta) = \\frac{\\sum_{t=2}^T \\left(u_t(\\theta) - \\bar{u}(\\theta)\\right)\\left(u_{t-1}(\\theta) - \\bar{u}(\\theta)\\right)}{\\sum_{t=1}^T \\left(u_t(\\theta) - \\bar{u}(\\theta)\\right)^2}\n$$\n最终的差异性是总和 $D(\\theta) = D_{\\mathrm{KS}}(\\theta) + |\\rho_1(\\theta)|$。如果候选参数向量 $\\theta$ 的差异性超过给定的阈值，即 $D(\\theta)  \\tau$，则将其标记为设定错误。\n\n将完整程序应用于每个测试用例中的每个候选 $\\theta$，并统计每个用例中被标记的候选参数数量，以产生最终结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the SMC-based posterior predictive check for all test cases.\n    \"\"\"\n\n    def get_initial_variance(a, q):\n        \"\"\"Calculates the variance for the initial state distribution.\"\"\"\n        if a**2  1:\n            return q / (1 - a**2)\n        else:\n            return 10 * q\n\n    def systematic_resample(particles, weights, rng):\n        \"\"\"Performs systematic resampling.\"\"\"\n        N = len(particles)\n        positions = (np.arange(N) + rng.random()) / N\n        cumulative_sum = np.cumsum(weights)\n        indexes = np.searchsorted(cumulative_sum, positions)\n        return particles[indexes]\n\n    def generate_data(theta_star, T, rng):\n        \"\"\"Generates synthetic data from the state-space model.\"\"\"\n        a_star, q_star, r_star, c_star = theta_star\n        \n        x = np.zeros(T)\n        y = np.zeros(T)\n        \n        # Initial state x0 from its stationary distribution\n        sigma0_sq_star = get_initial_variance(a_star, q_star)\n        x0 = rng.normal(0, np.sqrt(sigma0_sq_star))\n        \n        # Generate state and observation sequences\n        x_prev = x0\n        for t in range(T):\n            x[t] = a_star * x_prev + np.sqrt(q_star) * rng.normal()\n            y[t] = c_star * x[t] + np.sqrt(r_star) * rng.normal()\n            x_prev = x[t]\n            \n        return y\n\n    def calculate_discrepancy(theta, y_obs, N, rng):\n        \"\"\"\n        Calculates the discrepancy statistic D(theta) for a candidate parameter set.\n        \"\"\"\n        a, q, r, c = theta\n        T = len(y_obs)\n        \n        u_values = np.zeros(T)\n\n        # Initialize particles for x_0 from its stationary distribution under theta\n        sigma0_sq = get_initial_variance(a, q)\n        particles_prev = rng.normal(0, np.sqrt(sigma0_sq), N)\n\n        for t in range(T):\n            y_t = y_obs[t]\n            \n            # 1. Compute one-step-ahead predictive moments for y_t\n            mean_x_prev = np.mean(particles_prev)\n            var_x_prev = np.var(particles_prev)\n            \n            pred_mean_x_t = a * mean_x_prev\n            pred_var_x_t = a**2 * var_x_prev + q\n            \n            mu_hat_t = c * pred_mean_x_t\n            sigma2_hat_t = c**2 * pred_var_x_t + r\n            if sigma2_hat_t = 0: sigma2_hat_t = 1e-9 # Avoid sqrt of non-positive\n            sigma_hat_t = np.sqrt(sigma2_hat_t)\n            \n            # 2. Define standardized residual and Probability Integral Transform (PIT)\n            z_t = (y_t - mu_hat_t) / sigma_hat_t\n            u_t = norm.cdf(z_t)\n            u_values[t] = u_t\n            \n            # 3. Particle filter update (propagate, weight, resample)\n            particles_propagated = a * particles_prev + np.sqrt(q) * rng.normal(size=N)\n            \n            log_weights = -0.5 * ((y_t - c * particles_propagated)**2 / r)\n            log_weights -= np.max(log_weights)\n            weights = np.exp(log_weights)\n            weights_sum = np.sum(weights)\n            \n            if weights_sum == 0:\n                weights = np.ones(N) / N\n            else:\n                weights /= weights_sum\n            \n            particles_curr = systematic_resample(particles_propagated, weights, rng)\n            particles_prev = particles_curr\n\n        # 4. Construct discrepancy statistic D(theta)\n        # Kolmogorov-Smirnov distance\n        u_sorted = np.sort(u_values)\n        k = np.arange(1, T + 1)\n        d_ks = np.max([\n            np.max(k / T - u_sorted),\n            np.max(u_sorted - (k - 1) / T)\n        ])\n        \n        # Lag-1 autocorrelation\n        u_bar = np.mean(u_values)\n        centered_u = u_values - u_bar\n        denominator = np.sum(centered_u**2)\n        if denominator == 0:\n            rho_1 = 0\n        else:\n            numerator = np.sum(centered_u[1:] * centered_u[:-1])\n            rho_1 = numerator / denominator\n        \n        # Combined discrepancy\n        D = d_ks + np.abs(rho_1)\n        \n        return D\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"theta_star\": (0.9, 0.16, 0.04, 1.0), \"T\": 300, \"N\": 1500, \"tau\": 0.12, \"seed\": 12345,\n            \"candidates\": [(0.9, 0.16, 0.04, 1.0), (0.88, 0.16, 0.04, 1.0), (0.5, 0.16, 0.04, 1.0)]\n        },\n        {\n            \"theta_star\": (0.2, 0.25, 0.09, 1.0), \"T\": 300, \"N\": 1500, \"tau\": 0.12, \"seed\": 54321,\n            \"candidates\": [(0.2, 0.25, 0.09, 1.0), (0.9, 0.25, 0.09, 1.0), (0.2, 0.05, 0.09, 1.0)]\n        },\n        {\n            \"theta_star\": (0.99, 0.01, 0.01, 1.0), \"T\": 300, \"N\": 2000, \"tau\": 0.12, \"seed\": 999,\n            \"candidates\": [(0.99, 0.01, 0.01, 1.0), (0.9, 0.01, 0.01, 1.0), (0.99, 0.25, 0.01, 1.0)]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        theta_star, T, N, tau, seed, candidates = case.values()\n        \n        # A single random number generator for the entire case ensures reproducibility\n        rng = np.random.default_rng(seed)\n        \n        y_obs = generate_data(theta_star, T, rng)\n        \n        flagged_count = 0\n        for theta_cand in candidates:\n            # The same rng is passed to maintain a consistent random number stream\n            D_val = calculate_discrepancy(theta_cand, y_obs, N, rng)\n            if D_val > tau:\n                flagged_count += 1\n        \n        results.append(flagged_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}