{
    "hands_on_practices": [
        {
            "introduction": "The first step in any numerical analysis is to ensure your tools work correctly, and a powerful way to do this is with a \"null test\" where the expected answer is known. For a set of points distributed randomly (a Poisson process), the true two-point correlation function $\\xi(r)$ is zero for all $r \\gt 0$, and any measured signal is purely statistical \"shot noise\". This exercise  guides you through implementing and validating a 2PCF estimator against this fundamental baseline, ensuring your code correctly recovers a null signal and that its variance matches theoretical predictions.",
            "id": "3499918",
            "problem": "You are asked to design and implement a numerical experiment to test the unbiasedness and sampling distribution of an estimator of the two-point correlation function in a homogeneous Poisson point process within a three-dimensional periodic box.\n\nBase definitions and assumptions:\n- The two-point correlation function (2PCF) is defined by the statement that for a statistically homogeneous and isotropic point field of number density $n$ in a volume $V$, the differential probability to find a pair of points in infinitesimal volume elements $dV_1$ and $dV_2$ separated by a distance $r$ is $n^2\\left[1+\\xi(r)\\right]\\,dV_1\\,dV_2$. For a homogeneous Poisson process, $\\xi(r)=0$ for all $r$.\n- Consider a periodic cube of side length $L$, total volume $V=L^3$, containing $N$ points drawn from a Poisson process with fixed $N$ and uniform spatial distribution, so that the number density is $n=N/V$. Use periodic boundary conditions with the minimal image convention to define pair separations.\n- The estimator $\\hat{\\xi}(r)$ is to be constructed from first principles by comparing the observed number of pairs in a separation bin with the expected number of pairs for a spatially uniform (Poisson) distribution. For a bin with inner radius $r_{\\min}$ and outer radius $r_{\\max}$, the expected number of pairs per realization under the Poisson hypothesis is $\\mathbb{E}[RR]=\\frac{N(N-1)}{2}\\times \\frac{\\frac{4\\pi}{3}\\left(r_{\\max}^{3}-r_{\\min}^{3}\\right)}{V}$. The estimator $\\hat{\\xi}(r)$ is defined as the ratio of the observed pair count in that bin to $\\mathbb{E}[RR]$, minus $1$.\n- For a homogeneous Poisson process, the counting noise of the observed pairs in a bin is well-approximated by a Poisson distribution with mean $\\mathbb{E}[RR]$ provided $\\mathbb{E}[RR]$ is sufficiently large. Consequently, for large $\\mathbb{E}[RR]$, the distribution of $\\hat{\\xi}(r)$ over many independent realizations is approximately Gaussian with mean $0$ and variance $\\operatorname{Var}\\left[\\hat{\\xi}(r)\\right]\\approx 1/\\mathbb{E}[RR]$. The sampling distribution of the average of $\\hat{\\xi}(r)$ over $M$ independent realizations is then approximately Gaussian with mean $0$ and variance $1/\\left(M\\mathbb{E}[RR]\\right)$.\n\nTask:\n1. Implement an algorithm that, for each parameter set:\n   - Generates $M$ independent realizations of $N$ points uniformly at random in the periodic cube $[0,L)^3$.\n   - Computes all pairwise separations using the minimal image convention. That is, for a component-wise difference $\\Delta x$, map it to $\\Delta x-L \\times \\mathrm{round}(\\Delta x/L)$, and likewise for $\\Delta y$ and $\\Delta z$, before computing the Euclidean distance.\n   - Bins the pair separations into $B$ linearly spaced radial bins between $0$ and $r_{\\max}$, with the restriction that $r_{\\max} \\leq L/2$ to avoid geometric complications, and for each bin computes the estimator $\\hat{\\xi}(r)$ as described above.\n   - Across the $M$ realizations, for each radial bin $i$, computes the sample mean $\\overline{\\hat{\\xi}}_i$ and sample variance $s_i^2$ of the estimator.\n2. Verification criteria:\n   - Only consider bins where the per-realization expected number of pairs $\\mathbb{E}[RR]_i$ satisfies $\\mathbb{E}[RR]_i \\geq 25$.\n   - For each considered bin $i$, form the $z$-score of the sample mean, $z_i = \\frac{\\overline{\\hat{\\xi}}_i - 0}{\\sqrt{1/(M\\mathbb{E}[RR]_i)}}$. The mean-unbiasedness check passes if $\\max_i |z_i| < 4.0$.\n   - For each considered bin $i$, check the variance against the Poisson prediction via the relative error $\\delta_i = \\left|s_i^2 - 1/\\mathbb{E}[RR]_i\\right| \\big/ \\left(1/\\mathbb{E}[RR]_i\\right)$. The variance-consistency check passes if $\\max_i \\delta_i \\leq 0.5$.\n   - The overall pass/fail for a parameter set is the conjunction of the two checks above across all considered bins. If no bins meet $\\mathbb{E}[RR]_i \\geq 25$, mark the parameter set as failing.\n3. Numerical notes:\n   - Use only distances up to $r_{\\max} \\le L/2$.\n   - Exclude self-pairs; count only unique unordered pairs.\n   - All distances and lengths are dimensionless.\n   - Random number generation must be seeded to ensure reproducible results.\n4. Test suite:\n   - Case 1 (happy path): $L=1.0$, $N=300$, $M=300$, $B=12$, $r_{\\max}=0.5$.\n   - Case 2 (moderate counts): $L=1.0$, $N=80$, $M=600$, $B=10$, $r_{\\max}=0.5$.\n   - Case 3 (smaller $N$): $L=1.0$, $N=50$, $M=1000$, $B=8$, $r_{\\max}=0.5$.\n5. Required final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each entry is a boolean indicating whether the corresponding test case passes the checks.\n\nYour solution must be a single, complete, runnable program that performs the experiment exactly as specified and prints the boolean results in the required format. No user input is permitted. The program must be self-contained and use the random seeds it sets internally to be reproducible.",
            "solution": "The problem statement is a valid, well-posed, and scientifically grounded exercise in numerical cosmology. It directs the implementation of a Monte Carlo simulation to verify the statistical properties of a simple estimator for the two-point correlation function (2PCF) applied to a homogeneous Poisson point process. The provided definitions, constraints, and verification criteria are clear, consistent, and standard within the field.\n\nThe solution proceeds by implementing a numerical experiment for each of the three specified parameter sets. The parameters for a given experiment are the cube side length $L$, the number of points $N$, the number of Monte Carlo realizations $M$, the number of radial bins $B$, and the maximum radius of interest $r_{\\max}$.\n\nThe overall algorithm is as follows:\n\nFirst, for a given parameter set, we pre-calculate values that are constant across all $M$ realizations. The radial domain $[0, r_{\\max}]$ is divided into $B$ bins of equal width. For each bin $i$, with inner radius $r_{\\min,i}$ and outer radius $r_{\\max,i}$, the volume of the corresponding spherical shell is $V_{\\text{shell},i} = \\frac{4\\pi}{3}(r_{\\max,i}^3 - r_{\\min,i}^3)$. The total volume of the periodic cube is $V = L^3$. For a set of $N$ points, there are $\\frac{N(N-1)}{2}$ unique pairs. The expected number of pairs from a uniform random distribution falling into bin $i$, denoted $\\mathbb{E}[RR_i]$, is given by the product of the total number of pairs and the ratio of the shell volume to the total volume:\n$$\n\\mathbb{E}[RR_i] = \\frac{N(N-1)}{2} \\frac{V_{\\text{shell},i}}{V}\n$$\nThese $\\mathbb{E}[RR_i]$ values are computed for all $B$ bins.\n\nNext, a loop over $M$ realizations is executed. In each realization $m \\in \\{1, \\dots, M\\}$:\n1.  $N$ points are generated with coordinates drawn from a uniform distribution over the half-open interval $[0, L)^3$.\n2.  The pairwise separations for all $\\frac{N(N-1)}{2}$ unique pairs are calculated. This is performed efficiently using vectorization. For a set of point coordinates $\\{ \\vec{p}_k \\}_{k=1}^N$, all separation vectors $\\Delta \\vec{p}_{jk} = \\vec{p}_j - \\vec{p}_k$ are computed. The periodic boundary conditions are enforced via the minimal image convention, where each component of the separation vector, e.g., $\\Delta x$, is adjusted according to $\\Delta x' = \\Delta x - L \\cdot \\text{round}(\\Delta x/L)$. The scalar separation is then the Euclidean norm of the adjusted vector, $r = |\\Delta \\vec{p}'|$.\n3.  The calculated pair separations are sorted into the $B$ predefined radial bins, yielding the observed data-data pair counts, $DD_i^{(m)}$, for each bin $i$.\n4.  The estimator for the 2PCF, $\\hat{\\xi}_i^{(m)}$, is computed for each bin using the problem's definition:\n    $$\n    \\hat{\\xi}_i^{(m)} = \\frac{DD_i^{(m)}}{\\mathbb{E}[RR_i]} - 1\n    $$\nThe values of $\\hat{\\xi}_i^{(m)}$ for all bins $i=1, \\dots, B$ and all realizations $m=1, \\dots, M$ are stored.\n\nAfter completing all $M$ realizations, a statistical analysis is performed on the collected data. First, bins that do not meet the statistical significance criterion $\\mathbb{E}[RR_i] \\geq 25$ are excluded from the analysis. If no bins satisfy this condition, the test for the parameter set fails.\n\nFor the remaining valid bins, the following statistics are computed:\n- The sample mean of the estimator across the $M$ realizations for each bin $i$:\n  $$\n  \\overline{\\hat{\\xi}}_i = \\frac{1}{M} \\sum_{m=1}^{M} \\hat{\\xi}_i^{(m)}\n  $$\n- The sample variance for each bin $i$, using the standard unbiased estimator with $ddof=1$:\n  $$\n  s_i^2 = \\frac{1}{M-1} \\sum_{m=1}^{M} (\\hat{\\xi}_i^{(m)} - \\overline{\\hat{\\xi}}_i)^2\n  $$\n\nFinally, two verification checks are performed on these statistics for all valid bins:\n1.  **Mean-Unbiasedness Check**: The theoretical mean of $\\hat{\\xi}_i$ for a Poisson process is $0$. The theoretical variance of the sample mean $\\overline{\\hat{\\xi}}_i$ is $\\operatorname{Var}(\\overline{\\hat{\\xi}}_i) = \\frac{\\operatorname{Var}(\\hat{\\xi}_i)}{M} \\approx \\frac{1}{M \\mathbb{E}[RR_i]}$. The $z$-score for the sample mean in each bin is calculated as:\n    $$\n    z_i = \\frac{\\overline{\\hat{\\xi}}_i - 0}{\\sqrt{1/(M\\mathbb{E}[RR]_i)}}\n    $$\n    The check passes if the maximum absolute $z$-score across all valid bins is less than $4.0$, i.e., $\\max_i |z_i| < 4.0$.\n\n2.  **Variance-Consistency Check**: The sample variance $s_i^2$ is compared to its theoretical prediction, $\\operatorname{Var}(\\hat{\\xi}_i) \\approx 1/\\mathbb{E}[RR_i]$. The relative error $\\delta_i$ is computed:\n    $$\n    \\delta_i = \\frac{\\left|s_i^2 - 1/\\mathbb{E}[RR_i]\\right|}{1/\\mathbb{E}[RR_i]}\n    $$\n    This check passes if the maximum relative error across all valid bins is no more than $0.5$, i.e., $\\max_i \\delta_i \\leq 0.5$.\n\nA given parameter set receives a final verdict of 'pass' (True) if and only if both the mean and variance checks pass. Otherwise, it fails (False). The random number generator is seeded to ensure the entire process is reproducible.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_experiment(L, N, M, B, r_max, seed):\n    \"\"\"\n    Performs the numerical experiment for a single parameter set.\n\n    Args:\n        L (float): Side length of the periodic cube.\n        N (int): Number of points per realization.\n        M (int): Number of independent realizations.\n        B (int): Number of radial bins.\n        r_max (float): Maximum separation distance to consider.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        bool: True if the test case passes, False otherwise.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Pre-calculate bin properties and expected pair counts (RR)\n    # These are constant for a given parameter set.\n    r_bins = np.linspace(0.0, r_max, B + 1)\n    r_min_sq = r_bins[:-1]**2\n    r_max_sq = r_bins[1:]**2\n    # The volume of a spherical shell\n    v_shell = (4.0 / 3.0) * np.pi * (r_bins[1:]**3 - r_bins[:-1]**3)\n    V = L**3\n    total_pairs = N * (N - 1) / 2.0\n    expected_rr = total_pairs * v_shell / V\n\n    # This array will store the xi_hat values for each realization and bin\n    all_xi_hats = np.zeros((M, B))\n\n    # 2. Main loop over M realizations\n    for m in range(M):\n        # Generate N points uniformly in the cube [0, L)^3\n        points = np.random.uniform(0.0, L, size=(N, 3))\n\n        # Vectorized calculation of pairwise separations with Minimal Image Convention\n        # Broadcasting points array to get all pairwise difference vectors\n        deltas = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n        # Apply periodic boundary conditions\n        deltas -= L * np.round(deltas / L)\n\n        # Calculate squared distances\n        dists_sq_matrix = np.sum(deltas**2, axis=-1)\n\n        # Extract unique pairs (upper triangle of the distance matrix, k=1 to exclude self-pairs)\n        iu = np.triu_indices(N, k=1)\n        pairwise_dists_sq = dists_sq_matrix[iu]\n\n        # Binning the pair counts (DD)\n        # We can bin squared distances to avoid a sqrt calculation for every pair\n        dd_counts, _ = np.histogram(pairwise_dists_sq, bins=r_bins**2)\n\n        # Calculate the 2PCF estimator xi_hat for this realization\n        # Using np.divide for safe division where expected_rr might be zero\n        xi_hat_m = np.divide(\n            dd_counts, expected_rr, \n            out=np.zeros_like(expected_rr, dtype=float), \n            where=(expected_rr != 0)\n        ) - 1.0\n        \n        all_xi_hats[m, :] = xi_hat_m\n\n    # 3. Filter bins based on expected counts\n    valid_bins_mask = expected_rr >= 25.0\n    if not np.any(valid_bins_mask):\n        return False\n\n    # Apply mask to all relevant arrays\n    valid_expected_rr = expected_rr[valid_bins_mask]\n    valid_all_xi_hats = all_xi_hats[:, valid_bins_mask]\n\n    # 4. Compute statistics (mean and variance) over M realizations for valid bins\n    mean_xi_hat = np.mean(valid_all_xi_hats, axis=0)\n    # ddof=1 for unbiased sample variance\n    var_xi_hat = np.var(valid_all_xi_hats, axis=0, ddof=1)\n\n    # 5. Perform verification checks\n\n    # Mean-unbiasedness check\n    # Theoretical variance of the sample mean\n    theory_var_of_mean = 1.0 / (M * valid_expected_rr)\n    theory_std_of_mean = np.sqrt(theory_var_of_mean)\n    z_scores = np.abs(mean_xi_hat / theory_std_of_mean)\n    mean_check_passed = np.max(z_scores)  4.0\n\n    # Variance-consistency check\n    # Theoretical variance of the estimator xi_hat\n    theory_var = 1.0 / valid_expected_rr\n    rel_errors = np.abs(var_xi_hat - theory_var) / theory_var\n    var_check_passed = np.max(rel_errors) = 0.5\n    \n    return mean_check_passed and var_check_passed\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (L, N, M, B, r_max)\n        (1.0, 300, 300, 12, 0.5),  # Case 1\n        (1.0, 80, 600, 10, 0.5),   # Case 2\n        (1.0, 50, 1000, 8, 0.5),   # Case 3\n    ]\n\n    results = []\n    # Using a different seed for each case to ensure their random streams are independent\n    # while keeping the entire run reproducible.\n    for i, case in enumerate(test_cases):\n        L, N, M, B, r_max = case\n        result = run_experiment(L, N, M, B, r_max, seed=i)\n        results.append(str(result).lower()) # Convert boolean to string ('true'/'false')\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\n# The original problem asked for boolean output (True/False). \n# However, the example format was \"[result1,result2,result3]\".\n# Let's assume the string representation of boolean is sufficient.\n# `str(True)` is 'True', `str(False)` is 'False'.\n# The example `[result1,result2,result3]` could be `[True,False,True]`.\n# A common format is all lowercase: `[true,false,true]`. Let's use that for robustness.\n# Changed results.append(str(result).lower())\n# But wait, the problem says \"where each entry is a boolean\". A boolean is not a string.\n# A literal json-like list of booleans might be `[true, false, true]`.\n# `print(f\"[{','.join(results)}]\")` with results being ['True','True','True'] gives `[True,True,True]`. This looks fine. Let's revert to `str(result)`.\n\ndef run_experiment(L, N, M, B, r_max, seed):\n    np.random.seed(seed)\n    r_bins = np.linspace(0.0, r_max, B + 1)\n    v_shell = (4.0 / 3.0) * np.pi * (r_bins[1:]**3 - r_bins[:-1]**3)\n    V = L**3\n    total_pairs = N * (N - 1) / 2.0\n    expected_rr = total_pairs * v_shell / V\n    all_xi_hats = np.zeros((M, B))\n    for m in range(M):\n        points = np.random.uniform(0.0, L, size=(N, 3))\n        deltas = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n        deltas -= L * np.round(deltas / L)\n        dists_sq_matrix = np.sum(deltas**2, axis=-1)\n        iu = np.triu_indices(N, k=1)\n        pairwise_dists_sq = dists_sq_matrix[iu]\n        dd_counts, _ = np.histogram(pairwise_dists_sq, bins=r_bins**2)\n        xi_hat_m = np.divide(\n            dd_counts, expected_rr, \n            out=np.zeros_like(expected_rr, dtype=float), \n            where=(expected_rr != 0)\n        ) - 1.0\n        all_xi_hats[m, :] = xi_hat_m\n    valid_bins_mask = expected_rr >= 25.0\n    if not np.any(valid_bins_mask):\n        return False\n    valid_expected_rr = expected_rr[valid_bins_mask]\n    valid_all_xi_hats = all_xi_hats[:, valid_bins_mask]\n    mean_xi_hat = np.mean(valid_all_xi_hats, axis=0)\n    var_xi_hat = np.var(valid_all_xi_hats, axis=0, ddof=1)\n    theory_var_of_mean = 1.0 / (M * valid_expected_rr)\n    theory_std_of_mean = np.sqrt(theory_var_of_mean)\n    z_scores = np.abs(mean_xi_hat / theory_std_of_mean)\n    mean_check_passed = np.max(z_scores)  4.0\n    theory_var = 1.0 / valid_expected_rr\n    rel_errors = np.abs(var_xi_hat - theory_var) / theory_var\n    var_check_passed = np.max(rel_errors) = 0.5\n    return mean_check_passed and var_check_passed\n\ndef solve():\n    test_cases = [\n        (1.0, 300, 300, 12, 0.5),\n        (1.0, 80, 600, 10, 0.5),\n        (1.0, 50, 1000, 8, 0.5),\n    ]\n    results = []\n    for i, case in enumerate(test_cases):\n        L, N, M, B, r_max = case\n        result = run_experiment(L, N, M, B, r_max, seed=i)\n        results.append(str(result))\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While we measure clustering in real space using the two-point correlation function $\\xi(r)$, theoretical models of structure formation are often simpler in Fourier space, described by the power spectrum $P(k)$. The 2PCF and power spectrum are a Fourier transform pair, meaning the shape of one directly dictates the shape of the other. This analytical exercise  will build your theoretical intuition by having you derive the real-space $\\xi(r)$ that corresponds to a simple power-law $P(k)$, revealing the intimate connection between these two essential descriptions of cosmic structure.",
            "id": "3499904",
            "problem": "Consider a statistically homogeneous and isotropic matter density contrast field with three-dimensional power spectrum $P(k)$, where $k$ is the comoving wavenumber. The real-space two-point correlation function $\\xi(r)$, with $r$ the comoving separation, is related to $P(k)$ by the isotropic Fourier transform. Suppose the power spectrum is a pure power law with a finite dynamical range,\n$$\nP(k) = A\\,k^{n}\\,\\Theta(k-k_{\\min})\\,\\Theta(k_{\\max}-k),\n$$\nwhere $A$ is a real amplitude, $n$ is a real spectral index, $k_{\\min}0$ is an infrared cutoff, $k_{\\max}k_{\\min}$ is an ultraviolet cutoff, and $\\Theta$ is the Heaviside step function. Working from the fundamental relation between $\\xi(r)$ and $P(k)$ for an isotropic field, do the following:\n\n1) In the asymptotic separation regime $k_{\\max}^{-1} \\ll r \\ll k_{\\min}^{-1}$, determine how $\\xi(r)$ scales with $r$ and express the proportionality constant as a closed-form function of $n$ and $A$.\n\n2) Determine the range of spectral indices $n$ for which the continuum-limit integral (obtained by sending $k_{\\min}\\to 0^{+}$ and $k_{\\max}\\to \\infty$) for $\\xi(r)$ converges.\n\nYour final answer must be a single closed-form analytic expression for $\\xi(r)$ in the continuum limit, expressed in terms of $A$, $n$, and $r$. Do not include any inequalities or verbal conditions in the final answer. No numerical approximation is required, and no physical units are needed.",
            "solution": "The problem asks for properties of the two-point correlation function $\\xi(r)$ corresponding to a power-law power spectrum $P(k)$ with infrared and ultraviolet cutoffs. The analysis begins with the fundamental relation between $\\xi(r)$ and $P(k)$ for a statistically homogeneous and isotropic field.\n\nThe three-dimensional Fourier transform relating the power spectrum $P(k)$ to the two-point correlation function $\\xi(r)$ is given by:\n$$ \\xi(\\vec{r}) = \\int \\frac{d^3k}{(2\\pi)^3} P(\\vec{k}) e^{i\\vec{k}\\cdot\\vec{r}} $$\nFor an isotropic field, $P(\\vec{k})$ depends only on the magnitude $k = |\\vec{k}|$, and $\\xi(\\vec{r})$ depends only on the magnitude $r = |\\vec{r}|$. The integral can be simplified by performing the angular integration in spherical coordinates, which yields the zeroth-order spherical Bessel function $j_0(kr) = \\frac{\\sin(kr)}{kr}$.\n$$ \\xi(r) = \\frac{1}{2\\pi^2} \\int_0^\\infty k^2 P(k) \\frac{\\sin(kr)}{kr} dk = \\frac{1}{2\\pi^2 r} \\int_0^\\infty k P(k) \\sin(kr) dk $$\nThe given power spectrum is:\n$$ P(k) = A\\,k^{n}\\,\\Theta(k-k_{\\min})\\,\\Theta(k_{\\max}-k) $$\nwhere $\\Theta(x)$ is the Heaviside step function. The step functions restrict the integration domain to the interval $[k_{\\min}, k_{\\max}]$.\nSubstituting this $P(k)$ into the expression for $\\xi(r)$ gives:\n$$ \\xi(r) = \\frac{A}{2\\pi^2 r} \\int_{k_{\\min}}^{k_{\\max}} k (k^n) \\sin(kr) dk = \\frac{A}{2\\pi^2 r} \\int_{k_{\\min}}^{k_{\\max}} k^{n+1} \\sin(kr) dk $$\nTo analyze this integral, we perform a change of variables to $x = kr$. This implies $k = x/r$ and $dk = dx/r$. The integration limits become $x_{\\min} = k_{\\min}r$ and $x_{\\max} = k_{\\max}r$.\n$$ \\xi(r) = \\frac{A}{2\\pi^2 r} \\int_{k_{\\min}r}^{k_{\\max}r} \\left(\\frac{x}{r}\\right)^{n+1} \\sin(x) \\frac{dx}{r} = \\frac{A}{2\\pi^2 r^{n+3}} \\int_{k_{\\min}r}^{k_{\\max}r} x^{n+1} \\sin(x) dx $$\n\n1) Asymptotic separation regime ($k_{\\max}^{-1} \\ll r \\ll k_{\\min}^{-1}$):\nThis regime implies that $k_{\\min}r \\ll 1$ and $k_{\\max}r \\gg 1$. We can therefore approximate the integration limits as $k_{\\min}r \\to 0$ and $k_{\\max}r \\to \\infty$.\n$$ \\xi(r) \\approx \\frac{A}{2\\pi^2 r^{n+3}} \\int_{0}^{\\infty} x^{n+1} \\sin(x) dx $$\nThe integral $\\int_{0}^{\\infty} x^{p} \\sin(x) dx$ converges for $-2  p  0$. With $p = n+1$, this requires $-2  n+1  0$, which means $-3  n  -1$. Assuming $n$ is in this range, the integral is a constant. Therefore, the scaling of $\\xi(r)$ with $r$ is:\n$$ \\xi(r) \\propto r^{-(n+3)} $$\nThe proportionality constant is determined by the value of the definite integral. The value of this integral can be found from standard tables or by using the Mellin transform of $\\sin(x)$. The result is $\\Gamma(p+1)\\sin(\\pi(p+1)/2)$ for $-1  p  0$. By analytic continuation, this result can be extended. A more convenient form is $\\Gamma(p+1)\\cos(\\pi p/2)$. Let's use $p=n+1$:\n$$ \\int_{0}^{\\infty} x^{n+1} \\sin(x) dx = \\Gamma(n+2)\\cos\\left(\\frac{\\pi(n+1)}{2}\\right) $$\nThis expression is valid for $-3  n  -1$.\nThus, the proportionality constant is $\\frac{A}{2\\pi^2} \\Gamma(n+2)\\cos\\left(\\frac{\\pi(n+1)}{2}\\right)$.\n\n2) Convergence of the continuum-limit integral:\nIn the continuum limit, $k_{\\min} \\to 0^{+}$ and $k_{\\max} \\to \\infty$. The expression for $\\xi(r)$ becomes:\n$$ \\xi(r) = \\frac{A}{2\\pi^2 r} \\int_{0}^{\\infty} k^{n+1} \\sin(kr) dk $$\nFor this integral to converge, we analyze its behavior at the limits of integration.\n-   At the lower limit ($k \\to 0$): $\\sin(kr) \\approx kr$. The integrand behaves as $k^{n+1}(kr) = r k^{n+2}$. The integral $\\int_0^\\epsilon k^{n+2} dk$ converges if the power of $k$ plus one is greater than $0$, i.e., $(n+2)+1  0$, which gives $n  -3$.\n-   At the upper limit ($k \\to \\infty$): The integrand $k^{n+1}\\sin(kr)$ is an oscillating function. For the integral to converge (conditionally), the amplitude of the oscillations must tend to zero. By Dirichlet's test for improper integrals, this requires $\\lim_{k\\to\\infty} k^{n+1} = 0$, which means $n+1  0$, or $n  -1$.\n\nCombining these two conditions, the range of spectral indices $n$ for which the continuum-limit integral converges is $-3  n  -1$.\n\nFinal expression for $\\xi(r)$ in the continuum limit:\nFor $n$ in the convergence range $-3  n  -1$, we can evaluate the integral. As established in part 1:\n$$ \\xi(r) = \\frac{A}{2\\pi^2 r^{n+3}} \\int_{0}^{\\infty} x^{n+1} \\sin(x) dx $$\nUsing the value of the integral:\n$$ \\int_{0}^{\\infty} x^{n+1} \\sin(x) dx = \\Gamma(n+2)\\cos\\left(\\frac{\\pi(n+1)}{2}\\right) $$\nwhere $\\Gamma$ is the Gamma function.\nSubstituting this into the expression for $\\xi(r)$ gives the final answer.\n$$ \\xi(r) = \\frac{A}{2\\pi^2 r^{n+3}} \\Gamma(n+2) \\cos\\left(\\frac{\\pi(n+1)}{2}\\right) $$\nThis expression can be rearranged to highlight the power-law dependence on $r$:\n$$ \\xi(r) = \\frac{A\\,\\Gamma(n+2)}{2\\pi^2} \\cos\\left(\\frac{\\pi(n+1)}{2}\\right) r^{-(n+3)} $$\nThis is the closed-form analytic expression for $\\xi(r)$ in the continuum limit, valid for $-3  n  -1$. The problem asks for the single expression without conditions.",
            "answer": "$$ \\boxed{ \\frac{A\\,\\Gamma(n+2)}{2\\pi^2} \\cos\\left(\\frac{\\pi(n+1)}{2}\\right) r^{-(n+3)} } $$"
        },
        {
            "introduction": "Our theoretical models are often developed in idealized \"snapshots\" of the universe, yet real surveys observe galaxies across vast distances and times on our past light-cone. To properly interpret these observations, we must account for the fact that galaxy properties and the clustering amplitude itself evolve with redshift, and that our view is shaped by the survey's selection function. This advanced practice  bridges the gap between simple models and observational reality by tasking you with deriving and computing how these effects are imprinted on the light-cone averaged correlation function, a crucial skill for modern cosmological data analysis.",
            "id": "3499958",
            "problem": "Consider a statistically homogeneous and isotropic galaxy distribution observed on a past light-cone in a spatially flat Friedmann–Lemaître–Robertson–Walker cosmology with matter density parameter $\\Omega_{\\mathrm{m},0}$, dark energy density parameter $\\Omega_{\\Lambda,0}$, and Hubble parameter $H_0$. The two-point correlation function $\\xi(r)$ of the galaxy overdensity field at comoving separation $r$ quantifies the excess probability of finding pairs beyond a random Poisson sample. The observed sample is modulated by a redshift-dependent selection function $S(z)$ that multiplies the intrinsic comoving number density, and the galaxy overdensity $\\delta_{\\mathrm{g}}$ is assumed to be linearly biased with respect to the matter overdensity $\\delta_{\\mathrm{m}}$ through a redshift-dependent bias $b(z)$, i.e., $\\delta_{\\mathrm{g}}(z) = b(z)\\,\\delta_{\\mathrm{m}}(z)$. The matter clustering amplitude evolves with the linear growth factor $D(z)$, defined as the growing-mode solution normalized to $D(0) = 1$.\n\nStarting from the core definition that the excess joint probability of finding two galaxies in comoving volume elements $\\mathrm{d}V_1$ and $\\mathrm{d}V_2$ at separation $r$ is $\\mathrm{d}P = \\bar{n}^2(z)\\,[1+\\xi_{\\mathrm{g}}(r,z)]\\,\\mathrm{d}V_1\\,\\mathrm{d}V_2$, where $\\bar{n}(z)$ is the intrinsic comoving number density, derive from first principles how the expectation value of the estimator for the light-cone measured two-point correlation function $\\xi_{\\mathrm{LC}}(r)$ at fixed comoving separation $r$ is impacted by a redshift-dependent selection function $S(z)$ and linear bias $b(z)$, under the following scientifically standard and self-consistent assumptions:\n\n- The bias is linear and deterministic: $\\delta_{\\mathrm{g}}(z) = b(z)\\,\\delta_{\\mathrm{m}}(z)$.\n- The matter two-point correlation function factorizes into an amplitude and a fixed comoving-shape component: $\\xi_{\\mathrm{m}}(r,z) = D^2(z)\\,\\xi_{\\mathrm{m}}(r,0)$, where $D(z)$ is the linear growth factor normalized to $D(0)=1$.\n- The selection function $S(z)$ multiplies the observed comoving number density, such that the observed density is $S(z)\\,\\bar{n}(z)$, and the probability of finding pairs in a thin redshift shell is proportional to $[S(z)\\,\\bar{n}(z)]^2\\,\\mathrm{d}V$.\n- The small-separation, thin-shell approximation holds for the pair counts at fixed separation $r$ (with $r \\ll \\chi(z)$, where $\\chi(z)$ is comoving distance), so that the dominant contributions to pair counts at separation $r$ come from galaxies within the same thin redshift shells, and cross-shell mixing can be neglected at leading order.\n- The comoving differential volume element per unit redshift is $\\mathrm{d}V/\\mathrm{d}z \\propto \\chi^2(z)\\,\\mathrm{d}\\chi/\\mathrm{d}z$, with $\\mathrm{d}\\chi/\\mathrm{d}z = c/H(z)$ and $H(z) = H_0\\,E(z)$, where $E(z) = \\sqrt{\\Omega_{\\mathrm{m},0}(1+z)^3 + \\Omega_{\\Lambda,0}}$.\n- The snapshot two-point correlation function $\\xi_{\\mathrm{snap}}(r\\,|\\,z_{\\mathrm{snap}})$ refers to the value measured at a single redshift $z_{\\mathrm{snap}}$, with fixed bias $b(z_{\\mathrm{snap}})$ and growth factor $D(z_{\\mathrm{snap}})$.\n\nYour task is to:\n\n1. Using the above assumptions and the fundamental definitions, derive an explicit expression for the effective, light-cone–averaged clustering amplitude at fixed comoving separation $r$, expressed as a redshift-weighted average of the amplitude factor in $\\xi_{\\mathrm{g}}(r,z) = b^2(z)\\,D^2(z)\\,\\xi_{\\mathrm{m}}(r,0)$. Carefully justify the weighting by selection-modulated pair counts and the comoving volume element. Then, define the dimensionless ratio $R_{\\mathrm{both}}(r)$ of the light-cone–averaged amplitude to the snapshot amplitude at $z_{\\mathrm{snap}}$, which isolates the combined impact of evolution and selection. State all steps and assumptions clearly and in LaTeX.\n\n2. Define two additional diagnostic ratios that separately isolate the impact of (i) evolution alone and (ii) selection alone on the inferred clustering amplitude, each normalized by the snapshot amplitude at $z_{\\mathrm{snap}}$:\n   - $R_{\\mathrm{evol}}(r)$: the ratio when the selection is fixed to be flat in redshift, i.e., $S(z)=1$, but the redshift evolution of $b(z)$ and $D(z)$ is included.\n   - $R_{\\mathrm{sel}}(r)$: the ratio when the bias is fixed to $b(z_{\\mathrm{snap}})$ at all redshifts, but the actual selection function $S(z)$ is applied, with the physical growth $D(z)$ included.\n\n3. Implement a complete, runnable program that numerically evaluates the three ratios $R_{\\mathrm{both}}(r)$, $R_{\\mathrm{evol}}(r)$, and $R_{\\mathrm{sel}}(r)$ by performing redshift integrals over the specified ranges for a flat $\\Lambda$ Cold Dark Matter ($\\Lambda$CDM) cosmology with $\\Omega_{\\mathrm{m},0} = 0.3$, $\\Omega_{\\Lambda,0} = 0.7$, $H_0 = 70\\,\\mathrm{km\\,s^{-1}\\,Mpc^{-1}}$, and speed of light $c = 299792.458\\,\\mathrm{km\\,s^{-1}}$. Use the well-tested approximation $D(z) = g(z)/(1+z)$ with\n$$\ng(z) = \\frac{5\\,\\Omega_{\\mathrm{m}}(z)}{2}\\left[\\Omega_{\\mathrm{m}}(z)^{4/7} - \\Omega_{\\Lambda}(z) + \\left(1 + \\frac{\\Omega_{\\mathrm{m}}(z)}{2}\\right)\\left(1 + \\frac{\\Omega_{\\Lambda}(z)}{70}\\right)\\right]^{-1},\n$$\nnormalized to $D(0) = 1$, where $\\Omega_{\\mathrm{m}}(z) = \\Omega_{\\mathrm{m},0}(1+z)^3 / E^2(z)$ and $\\Omega_{\\Lambda}(z) = \\Omega_{\\Lambda,0} / E^2(z)$. Compute the comoving distance $\\chi(z)$ via\n$$\n\\chi(z) = \\frac{c}{H_0}\\int_0^z \\frac{\\mathrm{d}z'}{E(z')}.\n$$\n\n4. Use the following scientifically plausible test suite. In all cases, evaluate the ratios at a fiducial comoving separation $r = 10\\,h^{-1}\\,\\mathrm{Mpc}$ (you do not need to convert units in the code if the ratio is independent of $r$ under the above assumptions; this separation is specified for context, and all outputs are dimensionless). For each case, specify the redshift range $[z_{\\min}, z_{\\max}]$, the selection function $S(z)$, the bias $b(z)$, and the snapshot redshift $z_{\\mathrm{snap}}$:\n   - Case 1 (general, evolving selection and bias):\n     - $[z_{\\min}, z_{\\max}] = [0.1, 1.0]$,\n     - $S(z) = \\exp\\!\\left(-\\left(\\frac{z - 0.6}{0.25}\\right)^2\\right)$,\n     - $b(z) = 1 + 0.8\\,z$,\n     - $z_{\\mathrm{snap}} = 0.6$.\n   - Case 2 (boundary: no evolution and flat selection):\n     - $[z_{\\min}, z_{\\max}] = [0.2, 0.8]$,\n     - $S(z) = 1$,\n     - $b(z) = 1.5$,\n     - $z_{\\mathrm{snap}} = 0.5$.\n   - Case 3 (narrow selection around a lower redshift):\n     - $[z_{\\min}, z_{\\max}] = [0.1, 0.6]$,\n     - $S(z) = \\exp\\!\\left(-\\left(\\frac{z - 0.3}{0.05}\\right)^2\\right)$,\n     - $b(z) = 1 + 1.2\\,z$,\n     - $z_{\\mathrm{snap}} = 0.3$.\n   - Case 4 (selection increasing with redshift and rapidly increasing bias):\n     - $[z_{\\min}, z_{\\max}] = [0.1, 1.2]$,\n     - $S(z) = z^2$,\n     - $b(z) = 1 + 2.0\\,z$,\n     - $z_{\\mathrm{snap}} = 1.0$.\n\nYour program must:\n\n- Numerically integrate over $z$ to compute the redshift-weighted averages using the comoving volume element, implementing the three ratios $R_{\\mathrm{both}}(r)$, $R_{\\mathrm{evol}}(r)$, and $R_{\\mathrm{sel}}(r)$ exactly as defined in items 1 and 2.\n- Produce a single line of output containing the results as a comma-separated list of lists, in the form $\\left[[R_{\\mathrm{both}}, R_{\\mathrm{evol}}, R_{\\mathrm{sel}}]_{\\text{Case 1}}, [R_{\\mathrm{both}}, R_{\\mathrm{evol}}, R_{\\mathrm{sel}}]_{\\text{Case 2}}, [R_{\\mathrm{both}}, R_{\\mathrm{evol}}, R_{\\mathrm{sel}}]_{\\text{Case 3}}, [R_{\\mathrm{both}}, R_{\\mathrm{evol}}, R_{\\mathrm{sel}}]_{\\text{Case 4}}\\right]$.\n- Each numerical answer must be a decimal float. Angles do not appear and therefore require no unit specification. Distances are in $h^{-1}\\,\\mathrm{Mpc}$ for contextual interpretation; the requested ratios are dimensionless and should be output as decimal floats.\n\nEnsure scientific realism by adhering to the stated cosmology, definitions, and approximations. Do not assume or use any formulas for the target ratios beyond what you derive from the base definitions. The final program must be complete and runnable with no external input.",
            "solution": "The problem requires the derivation and computation of several ratios that quantify the impact of cosmological evolution and observational selection effects on the measured two-point correlation function of galaxies, $\\xi(r)$, observed on a past light-cone. We begin by deriving the expression for the light-cone-averaged correlation function, $\\xi_{\\mathrm{LC}}(r)$, from first principles.\n\nThe starting point is the excess probability of finding two galaxies in comoving volume elements $\\mathrm{d}V_1$ and $\\mathrm{d}V_2$ at the same redshift $z$, separated by a comoving distance $r$. This is given by $\\mathrm{d}P = \\bar{n}^2(z)\\,[1+\\xi_{\\mathrm{g}}(r,z)]\\,\\mathrm{d}V_1\\,\\mathrm{d}V_2$, where $\\bar{n}(z)$ is the intrinsic comoving number density and $\\xi_{\\mathrm{g}}(r,z)$ is the galaxy two-point correlation function.\n\nThe problem states that the observed sample is modulated by a redshift-dependent selection function $S(z)$, so the observed number density is $n_{\\mathrm{obs}}(z) = S(z)\\,\\bar{n}(z)$. The number of observed galaxies in a volume element $\\mathrm{d}V$ is thus $\\mathrm{d}N(z) = S(z)\\,\\bar{n}(z)\\,\\mathrm{d}V$.\n\nThe number of pairs of galaxies is proportional to the square of the number density. Specifically, the number of excess pairs (above a random distribution) in volume elements $\\mathrm{d}V_1$ and $\\mathrm{d}V_2$ at redshift $z$ and separation $r$ is:\n$$ \\mathrm{d}N_{\\mathrm{pairs, excess}}(r, z) = [S(z)\\,\\bar{n}(z)]^2\\,\\xi_{\\mathrm{g}}(r, z)\\,\\mathrm{d}V_1\\,\\mathrm{d}V_2 $$\nThe total number of excess pairs with separation $r$ in a thin redshift shell of thickness $\\mathrm{d}z$ is obtained by integrating over one volume element and over the shell of radius $r$ around it. This yields a quantity proportional to $[S(z)\\,\\bar{n}(z)]^2\\,\\xi_{\\mathrm{g}}(r, z)\\,\\frac{\\mathrm{d}V}{\\mathrm{d}z}\\mathrm{d}z$, where $\\frac{\\mathrm{d}V}{\\mathrm{d}z}$ is the comoving volume per unit redshift. Similarly, the number of pairs in a random sample is proportional to $[S(z)\\,\\bar{n}(z)]^2\\,\\frac{\\mathrm{d}V}{\\mathrm{d}z}\\mathrm{d}z$.\n\nThe light-cone measured correlation function, $\\xi_{\\mathrm{LC}}(r)$, is the total number of excess pairs at separation $r$ over the entire survey volume, divided by the total number of pairs from a random distribution. This is equivalent to a weighted average of the redshift-dependent correlation function $\\xi_{\\mathrm{g}}(r, z)$:\n$$ \\xi_{\\mathrm{LC}}(r) = \\frac{\\int_{z_{\\min}}^{z_{\\max}} [S(z)\\,\\bar{n}(z)]^2\\,\\xi_{\\mathrm{g}}(r, z)\\,\\frac{\\mathrm{d}V}{\\mathrm{d}z}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} [S(z)\\,\\bar{n}(z)]^2\\,\\frac{\\mathrm{d}V}{\\mathrm{d}z}\\,\\mathrm{d}z} $$\nThe term $[S(z)\\bar{n}(z)]^2 \\frac{\\mathrm{d}V}{\\mathrm{d}z}$ acts as the weighting function, representing the number of pairs available at each redshift $z$. Assuming the intrinsic comoving density $\\bar{n}(z)$ is constant, it cancels from the numerator and denominator.\n\nWe are given the factorization of the galaxy correlation function based on linear bias and linear growth:\n$$ \\xi_{\\mathrm{g}}(r, z) = b^2(z)\\,\\xi_{\\mathrm{m}}(r, z) = b^2(z)\\,D^2(z)\\,\\xi_{\\mathrm{m}}(r, 0) $$\nwhere $b(z)$ is the galaxy bias, $D(z)$ is the linear growth factor, and $\\xi_{\\mathrm{m}}(r, 0)$ is the matter correlation function at redshift $z=0$. The shape of the correlation function, encoded in $\\xi_{\\mathrm{m}}(r, 0)$, is assumed to be constant in comoving coordinates, while its amplitude evolves as $b^2(z)D^2(z)$.\n\nSubstituting this into the expression for $\\xi_{\\mathrm{LC}}(r)$ and factoring out the redshift-independent term $\\xi_{\\mathrm{m}}(r, 0)$:\n$$ \\xi_{\\mathrm{LC}}(r) = \\left( \\frac{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,b^2(z)\\,D^2(z)\\,\\frac{\\mathrm{d}V}{\\mathrm{d}z}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,\\frac{\\mathrm{d}V}{\\mathrm{d}z}\\,\\mathrm{d}z} \\right) \\xi_{\\mathrm{m}}(r, 0) $$\nThe term in the parentheses is the effective, light-cone–averaged clustering amplitude, which we will denote as $A_{\\mathrm{LC}}$.\nThe differential comoving volume element over a solid angle $\\mathrm{d}\\Omega$ is $\\mathrm{d}V = \\chi^2(z)\\,\\mathrm{d}\\chi\\,\\mathrm{d}\\Omega$. Integrating over the full sky ($\\mathrm{d}\\Omega = 4\\pi$) and using the relation $\\mathrm{d}\\chi = \\frac{c}{H(z)}\\mathrm{d}z$, we have $\\frac{\\mathrm{d}V}{\\mathrm{d}z} = 4\\pi\\,\\chi^2(z)\\frac{c}{H(z)}$. The constants $4\\pi$ and $c$ will cancel in the ratio, so we can use the proportionality $\\frac{\\mathrm{d}V}{\\mathrm{d}z} \\propto \\frac{\\chi^2(z)}{H(z)}$.\nThe effective amplitude is thus:\n$$ A_{\\mathrm{LC}} = \\frac{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,b^2(z)\\,D^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z} $$\n\nA \"snapshot\" measurement at a single redshift $z_{\\mathrm{snap}}$ would measure the amplitude $A_{\\mathrm{snap}} = b^2(z_{\\mathrm{snap}})D^2(z_{\\mathrm{snap}})$.\n\n**1. The ratio $R_{\\mathrm{both}}(r)$**\nThis ratio quantifies the combined impact of redshift evolution and selection effects. It is defined as the ratio of the light-cone averaged amplitude to the snapshot amplitude. Since the spatial dependence $\\xi_{\\mathrm{m}}(r,0)$ is the same, this ratio is independent of separation $r$.\n$$ R_{\\mathrm{both}} = \\frac{A_{\\mathrm{LC}}}{A_{\\mathrm{snap}}} = \\frac{1}{b^2(z_{\\mathrm{snap}})D^2(z_{\\mathrm{snap}})} \\frac{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,b^2(z)\\,D^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z} $$\n\n**2. Diagnostic Ratios $R_{\\mathrm{evol}}(r)$ and $R_{\\mathrm{sel}}(r)$**\nWe define two additional ratios to isolate the effects of evolution and selection.\n\n- **$R_{\\mathrm{evol}}(r)$**: This ratio isolates the impact of evolution alone by assuming a flat selection function, $S(z)=1$. The redshift evolution of bias $b(z)$ and growth $D(z)$ is retained.\n$$ R_{\\mathrm{evol}} = \\frac{1}{b^2(z_{\\mathrm{snap}})D^2(z_{\\mathrm{snap}})} \\frac{\\int_{z_{\\min}}^{z_{\\max}} b^2(z)\\,D^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} \\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z} $$\nThis expression represents the volume-weighted average of the clustering amplitude $b^2(z)D^2(z)$, normalized to the snapshot value.\n\n- **$R_{\\mathrm{sel}}(r)$**: This ratio isolates the impact of the selection function by fixing the bias to its snapshot value, $b(z)=b(z_{\\mathrm{snap}})$, while keeping the actual selection function $S(z)$ and physical growth $D(z)$.\n$$ R_{\\mathrm{sel}} = \\frac{1}{b^2(z_{\\mathrm{snap}})D^2(z_{\\mathrm{snap}})} \\frac{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,b^2(z_{\\mathrm{snap}})\\,D^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z} $$\nThe constant term $b^2(z_{\\mathrm{snap}})$ cancels, yielding:\n$$ R_{\\mathrm{sel}} = \\frac{1}{D^2(z_{\\mathrm{snap}})} \\frac{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,D^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z} $$\nThis expression measures how the selection function $S(z)$ re-weights the redshift evolution of the matter clustering amplitude $D^2(z)$, relative to its value at the snapshot redshift $z_{\\mathrm{snap}}$.\n\nThe numerical implementation will proceed by evaluating these final expressions for the three ratios using the provided cosmological model and test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.interpolate import interp1d\n\ndef solve():\n    \"\"\"\n    Derives and computes the impact of evolution and selection effects on the\n    light-cone-averaged two-point correlation function.\n    \"\"\"\n\n    # Define cosmological constants and parameters\n    C = 299792.458  # Speed of light in km/s\n    H0 = 70.0       # Hubble constant in km/s/Mpc\n    OMEGA_M0 = 0.3\n    OMEGA_L0 = 0.7\n\n    # Cosmological functions\n    def E(z):\n        return np.sqrt(OMEGA_M0 * (1 + z)**3 + OMEGA_L0)\n\n    def inv_E(z):\n        return 1.0 / E(z)\n\n    # Pre-compute an interpolator for comoving distance chi(z) for efficiency.\n    # The maximum redshift in the test cases is 1.2. We integrate up to 1.5.\n    Z_MAX_GLOBAL = 1.5\n    z_grid = np.linspace(0, Z_MAX_GLOBAL, 2000)\n    \n    # Use cumulative integration for chi. SciPy 1.6.0+ has cumulative_trapezoid\n    # For compatibility, we can implement it with a loop and quad.\n    chi_integral_vals = np.array([quad(inv_E, 0, z_val)[0] for z_val in z_grid])\n    chi_vals = (C / H0) * chi_integral_vals\n    chi_interp = interp1d(z_grid, chi_vals, kind='cubic', fill_value=\"extrapolate\")\n\n    def chi(z):\n        is_scalar = np.isscalar(z)\n        z = np.atleast_1d(z)\n        results = chi_interp(z)\n        results[z  0] = 0.0 # Guard against small negative z from interpolation\n        return results[0] if is_scalar else results\n\n    # Growth factor D(z) calculation, normalized to D(0)=1\n    def Omega_m_z(z):\n        E_z = E(z)\n        return OMEGA_M0 * (1 + z)**3 / E_z**2\n\n    def Omega_Lambda_z(z):\n        E_z = E(z)\n        return OMEGA_L0 / E_z**2\n\n    def g_unnorm(z):\n        om_z = Omega_m_z(z)\n        ol_z = Omega_Lambda_z(z)\n        # Per problem statement\n        term1 = om_z**(4.0/7.0)\n        term2 = ol_z\n        term3 = (1 + om_z / 2.0) * (1 + ol_z / 70.0)\n        return (5.0 * om_z / 2.0) * (term1 - term2 + term3)**(-1)\n\n    def D_unnorm(z):\n        return g_unnorm(z) / (1.0 + z)\n\n    D0_val = D_unnorm(0.0)\n\n    def D(z):\n        # Normalized growth factor\n        return D_unnorm(z) / D0_val\n\n    # Differential comoving volume element factor dV/dz ~ chi(z)^2 / H(z)\n    def dV_dz_prop(z):\n        # H(z) = H0 * E(z)\n        return chi(z)**2 / (H0 * E(z))\n\n    def calculate_ratios(z_min, z_max, S_func, b_func, z_snap):\n        \"\"\"\n        Calculates the three ratios R_both, R_evol, and R_sel for a given case.\n        \"\"\"\n        # Snapshot values\n        b_snap = b_func(z_snap)\n        D_snap = D(z_snap)\n\n        # --- R_both ---\n        A_snap_both = b_snap**2 * D_snap**2\n        \n        integrand_num_both = lambda z: S_func(z)**2 * b_func(z)**2 * D(z)**2 * dV_dz_prop(z)\n        integrand_den_both = lambda z: S_func(z)**2 * dV_dz_prop(z)\n        \n        num_both, _ = quad(integrand_num_both, z_min, z_max, limit=200, epsabs=1.49e-11, epsrel=1.49e-11)\n        den_both, _ = quad(integrand_den_both, z_min, z_max, limit=200, epsabs=1.49e-11, epsrel=1.49e-11)\n        \n        R_both = (num_both / den_both) / A_snap_both if den_both > 0 else np.nan\n\n        # --- R_evol ---\n        A_snap_evol = b_snap**2 * D_snap**2\n\n        integrand_num_evol = lambda z: b_func(z)**2 * D(z)**2 * dV_dz_prop(z)\n        integrand_den_evol = lambda z: dV_dz_prop(z)\n\n        num_evol, _ = quad(integrand_num_evol, z_min, z_max, limit=200, epsabs=1.49e-11, epsrel=1.49e-11)\n        den_evol, _ = quad(integrand_den_evol, z_min, z_max, limit=200, epsabs=1.49e-11, epsrel=1.49e-11)\n        \n        R_evol = (num_evol / den_evol) / A_snap_evol if den_evol > 0 else np.nan\n\n        # --- R_sel ---\n        A_snap_sel = D_snap**2\n\n        integrand_num_sel = lambda z: S_func(z)**2 * D(z)**2 * dV_dz_prop(z)\n        den_sel = den_both\n        \n        num_sel, _ = quad(integrand_num_sel, z_min, z_max, limit=200, epsabs=1.49e-11, epsrel=1.49e-11)\n        \n        R_sel = (num_sel / den_sel) / A_snap_sel if den_sel > 0 else np.nan\n        \n        return [R_both, R_evol, R_sel]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'z_range': [0.1, 1.0], 'S_func': lambda z: np.exp(-((z - 0.6)/0.25)**2), 'b_func': lambda z: 1.0 + 0.8*z, 'z_snap': 0.6},\n        {'z_range': [0.2, 0.8], 'S_func': lambda z: 1.0, 'b_func': lambda z: 1.5, 'z_snap': 0.5},\n        {'z_range': [0.1, 0.6], 'S_func': lambda z: np.exp(-((z - 0.3)/0.05)**2), 'b_func': lambda z: 1.0 + 1.2*z, 'z_snap': 0.3},\n        {'z_range': [0.1, 1.2], 'S_func': lambda z: z**2, 'b_func': lambda z: 1.0 + 2.0*z, 'z_snap': 1.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        res = calculate_ratios(\n            case['z_range'][0],\n            case['z_range'][1],\n            case['S_func'],\n            case['b_func'],\n            case['z_snap']\n        )\n        results.append(res)\n    \n    # Format the final output string exactly as required.\n    formatted_results = []\n    for res_list in results:\n        formatted_list = [f\"{x:.8f}\" for x in res_list]\n        formatted_results.append(f\"[{','.join(formatted_list)}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}