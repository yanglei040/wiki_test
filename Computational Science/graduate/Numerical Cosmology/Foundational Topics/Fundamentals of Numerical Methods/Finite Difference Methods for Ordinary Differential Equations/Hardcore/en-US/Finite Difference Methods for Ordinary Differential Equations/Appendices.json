{
    "hands_on_practices": [
        {
            "introduction": "In numerical computations, the pursuit of accuracy by simply decreasing the step size $h$ encounters a fundamental barrier: finite-machine precision. This practice delves into the trade-off between the mathematical truncation error, which decreases with $h$, and the catastrophic cancellation or rounding error, which increases as $h$ becomes smaller. By analyzing the central difference approximation, you will derive the optimal step size that balances these two competing effects, revealing a crucial aspect of practical numerical differentiation .",
            "id": "3471898",
            "problem": "In numerical cosmology, derivatives of smooth cosmological observables such as the scale factor $a(t)$, the Hubble expansion rate $H(t)$, or distance measures $D(t)$ are routinely evaluated from ordinary differential equations using finite differences. Let $y(t)$ denote any such smooth observable that is twice continuously differentiable near a fixed epoch $t$, with higher derivatives existing. Consider the central difference estimator of the first derivative,\n$$\nD_{c}(h;t) \\equiv \\frac{y(t+h)-y(t-h)}{2h},\n$$\ncomputed in floating-point arithmetic that follows the Institute of Electrical and Electronics Engineers (IEEE) 754 standard, rounding to nearest, with unit roundoff $\\varepsilon$.\n\nStarting from foundational bases appropriate to the context, namely Taylor expansion for smooth functions and the standard floating-point rounding model $\\mathrm{fl}(z)=z(1+\\delta)$ with $|\\delta|\\leq \\varepsilon$ for each elementary operation, carry out the following:\n\n- Derive the leading-order truncation error of $D_{c}(h;t)$ as a function of $h$ and the derivatives of $y(t)$.\n- Quantify the cancellation error that arises when $h$ is very small by modeling the effect of floating-point rounding on the subtraction $y(t+h)-y(t-h)$, and obtain a worst-case absolute error bound that reveals its scaling with $h$ and $y(t)$.\n- Combine these contributions into a single leading-order worst-case absolute error model $E(h)$ and, by minimizing $E(h)$ with respect to $h$, determine the optimal step size $h^{\\star}$ that balances truncation and cancellation errors.\n\nFinally, propose a compensated differencing strategy that mitigates cancellation in the subtraction $y(t+h)-y(t-h)$ using an error-free transformation of floating-point subtraction, and explain qualitatively how this changes the $h$-dependence of the rounding term in the error model. Your derivation must remain general, without substituting specific numerical values for cosmological parameters.\n\nExpress your final answer as the closed-form analytic expression for the optimal step size $h^{\\star}$ in terms of $\\varepsilon$, $y(t)$, and the appropriate derivative of $y(t)$. No numerical approximation or rounding is required for the final answer.",
            "solution": "The problem requires a thorough analysis of the errors involved in computing the first derivative of a smooth function $y(t)$ using the central difference formula,\n$$\nD_{c}(h;t) = \\frac{y(t+h)-y(t-h)}{2h},\n$$\nand to determine the optimal step size $h^{\\star}$ that minimizes the total error. The analysis involves two primary sources of error: the truncation error inherent to the finite difference approximation and the rounding error originating from floating-point arithmetic.\n\n### Step 1: Derivation of the Truncation Error\n\nThe truncation error, $E_{\\text{trunc}}(h)$, is the error made by approximating the true derivative $y'(t)$ with the finite difference formula, assuming exact arithmetic. It can be derived by means of Taylor expansion of the function $y(t)$ around the point $t$. Since $y(t)$ is assumed to be sufficiently differentiable, we can write the Taylor series for $y(t+h)$ and $y(t-h)$:\n\n$$\ny(t+h) = y(t) + h y'(t) + \\frac{h^2}{2!} y''(t) + \\frac{h^3}{3!} y'''(t) + \\frac{h^4}{4!} y^{(4)}(t) + O(h^5)\n$$\n$$\ny(t-h) = y(t) - h y'(t) + \\frac{h^2}{2!} y''(t) - \\frac{h^3}{3!} y'''(t) + \\frac{h^4}{4!} y^{(4)}(t) - O(h^5)\n$$\n\nSubtracting the second expansion from the first eliminates the even-powered terms in $h$:\n$$\ny(t+h) - y(t-h) = 2h y'(t) + \\frac{2h^3}{3!} y'''(t) + O(h^5)\n$$\n$$\ny(t+h) - y(t-h) = 2h y'(t) + \\frac{h^3}{3} y'''(t) + O(h^5)\n$$\n\nNow, we divide by $2h$ to obtain the expression for the central difference estimator:\n$$\n\\frac{y(t+h) - y(t-h)}{2h} = y'(t) + \\frac{h^2}{6} y'''(t) + O(h^4)\n$$\n\nThe truncation error is the difference between the estimator and the true value:\n$$\nE_{\\text{trunc}}(h) = D_{c}(h;t) - y'(t) = \\frac{h^2}{6} y'''(t) + O(h^4)\n$$\n\nThe leading-order truncation error is the first term in this series, which scales with the square of the step size, $h^2$. Its magnitude is given by:\n$$\n|E_{\\text{trunc}}(h)| \\approx \\frac{h^2}{6} |y'''(t)|\n$$\n\n### Step 2: Quantification of the Rounding Error\n\nRounding error arises from the finite precision of floating-point arithmetic. The standard model for a single floating-point operation on a value $z$ is $\\mathrm{fl}(z) = z(1+\\delta)$, where $|\\delta| \\leq \\varepsilon$ and $\\varepsilon$ is the unit roundoff.\n\nThe computation of $D_{c}(h;t)$ involves evaluating $y(t+h)$ and $y(t-h)$, subtracting them, and dividing by $2h$. Let's analyze the error in the numerator, which is the critical step. The floating-point representations of the function values are:\n$$\n\\tilde{y}_{+} = \\mathrm{fl}(y(t+h)) = y(t+h)(1+\\delta_1)\n$$\n$$\n\\tilde{y}_{-} = \\mathrm{fl}(y(t-h)) = y(t-h)(1+\\delta_2)\n$$\nwhere $|\\delta_1|, |\\delta_2| \\leq \\varepsilon$. We assume for simplicity that the evaluation of $y$ results in a single rounding error.\n\nThe subtraction is then performed in floating-point arithmetic:\n$$\n\\mathrm{fl}(\\tilde{y}_{+} - \\tilde{y}_{-}) = (\\tilde{y}_{+} - \\tilde{y}_{-})(1+\\delta_3)\n$$\nwhere $|\\delta_3| \\leq \\varepsilon$.\n\nThe absolute rounding error in the numerator is the difference between the computed value and the true value, $y(t+h)-y(t-h)$. The computed numerator is:\n$$\n(y(t+h)(1+\\delta_1) - y(t-h)(1+\\delta_2))(1+\\delta_3)\n$$\nExpanding this and keeping only first-order terms in $\\delta_i$ gives:\n$$\n(y(t+h) - y(t-h)) + (y(t+h)-y(t-h))\\delta_3 + y(t+h)\\delta_1 - y(t-h)\\delta_2\n$$\nThe error in the numerator is approximately $(y(t+h)-y(t-h))\\delta_3 + y(t+h)\\delta_1 - y(t-h)\\delta_2$.\nFor small $h$, $y(t+h) \\approx y(t-h) \\approx y(t)$, and the difference $y(t+h)-y(t-h) \\approx 2hy'(t)$ is small. This is the condition for catastrophic cancellation. The absolute error in the computed values of $y(t\\pm h)$ is approximately $\\varepsilon |y(t)|$. When these nearly equal numbers are subtracted, their absolute errors are roughly added. The dominant part of the numerator error comes from the inherited errors: $y(t+h)\\delta_1 - y(t-h)\\delta_2$.\n\nA worst-case bound on the absolute error of the numerator is:\n$$\n|E_{\\text{num}}| \\le |y(t+h)\\delta_1| + |y(t-h)\\delta_2| \\approx \\varepsilon |y(t+h)| + \\varepsilon |y(t-h)|\n$$\nFor small $h$, this is approximately $2\\varepsilon|y(t)|$.\n\nThe total rounding error for $D_c(h;t)$ is this numerator error divided by the denominator $2h$ (assuming the denominator is computed with negligible error).\n$$\n|E_{\\text{round}}(h)| \\approx \\frac{2\\varepsilon|y(t)|}{2h} = \\frac{\\varepsilon|y(t)|}{h}\n$$\nThis error term grows as $h$ becomes smaller, which is the signature of catastrophic cancellation.\n\n### Step 3: Optimal Step Size\n\nThe total absolute error $E(h)$ is the sum of the magnitudes of the leading-order truncation and rounding errors:\n$$\nE(h) \\approx |E_{\\text{trunc}}(h)| + |E_{\\text{round}}(h)| = \\frac{h^2}{6} |y'''(t)| + \\frac{\\varepsilon|y(t)|}{h}\n$$\nTo find the optimal step size $h^{\\star}$ that minimizes this total error, we differentiate $E(h)$ with respect to $h$ and set the result to zero, assuming $y(t)$ and $y'''(t)$ are non-zero.\n$$\n\\frac{dE}{dh} = \\frac{2h}{6} |y'''(t)| - \\frac{\\varepsilon|y(t)|}{h^2} = \\frac{h}{3} |y'''(t)| - \\frac{\\varepsilon|y(t)|}{h^2}\n$$\nSetting $\\frac{dE}{dh} = 0$:\n$$\n\\frac{h^{\\star}}{3} |y'''(t)| = \\frac{\\varepsilon|y(t)|}{(h^{\\star})^2}\n$$\nSolving for $h^{\\star}$:\n$$\n(h^{\\star})^3 = \\frac{3\\varepsilon|y(t)|}{|y'''(t)|}\n$$\n$$\nh^{\\star} = \\left(\\frac{3\\varepsilon|y(t)|}{|y'''(t)|}\\right)^{1/3}\n$$\nThis expression gives the optimal step size that balances the decreasing truncation error and the increasing rounding error.\n\n### Step 4: Compensated Differencing Strategy\n\nThe catastrophic cancellation in the standard central difference formula arises because the subtraction $\\mathrm{fl}(y(t+h)) - \\mathrm{fl}(y(t-h))$ loses significant relative precision when $h$ is small. A compensated differencing strategy mitigates this specific source of error.\n\nThe proposed strategy is to use an error-free transformation (EFT) for floating-point subtraction, such as the `TwoDiff` algorithm by Ogita, Rump, and Oishi. Given two floating-point numbers $x$ and $y$, such an algorithm computes a pair of floating-point numbers $(s, e)$ such that $s = \\mathrm{fl}(x-y)$ and, crucially, $s+e=x-y$ holds exactly (barring overflow/underflow). The variable $e$ captures the rounding error from the primary subtraction.\n\nBy applying this EFT to the numerator subtraction, we are essentially computing the value of $\\mathrm{fl}(y(t+h)) - \\mathrm{fl}(y(t-h))$ to a higher precision (e.g., as a double-precision number represented by the pair $(s,e)$). This procedure effectively eliminates the rounding error introduced by the subtraction operation itself.\n\nHowever, this does not eliminate all rounding error. The error inherited from the initial floating-point evaluations of $y(t+h)$ and $y(t-h)$ remains. The error in the compensated numerator is now dominated by $(\\mathrm{fl}(y(t+h)) - \\mathrm{fl}(y(t-h))) - (y(t+h) - y(t-h)) = y(t+h)\\delta_1 - y(t-h)\\delta_2$. Dividing by $2h$, the rounding error in the compensated scheme is:\n$$\nE_{\\text{round}}^{\\text{comp}}(h) \\approx \\frac{y(t+h)\\delta_1 - y(t-h)\\delta_2}{2h}\n$$\nExpanding the terms as before gives:\n$$\nE_{\\text{round}}^{\\text{comp}}(h) \\approx \\frac{y(t)(\\delta_1 - \\delta_2)}{2h} + \\frac{y'(t)(\\delta_1 + \\delta_2)}{2} + O(h)\n$$\nQualitatively, the $h$-dependence of the rounding error changes due to a phenomenon known as \"benign cancellation\". When evaluating a smooth function $y$ at two very close points $t+h$ and $t-h$, the computational paths are nearly identical. This can result in highly correlated rounding errors, i.e., $\\delta_1 \\approx \\delta_2$. Consequently, the difference $\\delta_1 - \\delta_2$ is much smaller than $\\varepsilon$. This benign cancellation suppresses the problematic $O(1/h)$ term. The dominant rounding error term then becomes the next term in the expansion, which is approximately constant with respect to $h$:\n$$\n|E_{\\text{round}}^{\\text{comp}}(h)| \\approx \\left|\\frac{y'(t)(\\delta_1 + \\delta_2)}{2}\\right| \\le \\frac{|y'(t)|(2\\varepsilon)}{2} = \\varepsilon|y'(t)|\n$$\nTherefore, a compensated differencing strategy, by leveraging both an EFT and the correlated nature of rounding errors for smooth functions, qualitatively changes the scaling of the rounding error from $O(\\varepsilon/h)$ to $O(\\varepsilon)$. This allows for the use of much smaller step sizes $h$ before the total error is dominated by rounding effects, yielding a much more accurate derivative estimate.",
            "answer": "$$\n\\boxed{\\left(\\frac{3\\varepsilon|y(t)|}{|y'''(t)|}\\right)^{1/3}}\n$$"
        },
        {
            "introduction": "A cornerstone of reliable scientific computing is code verification—the process of ensuring that your program correctly solves the mathematical model it is based on. This hands-on exercise guides you through building a test harness to measure the observed order of accuracy for several time-integration schemes applied to a simplified Friedmann equation. Completing this practice  will equip you with a vital skill for debugging numerical code and quantifying its performance, including how practical step-size constraints can affect the theoretical convergence rate.",
            "id": "3471890",
            "problem": "You are to construct a complete, runnable program that implements a test harness to measure the observed order of accuracy under grid refinement for finite difference time-integration methods applied to an ordinary differential equation motivated by cosmology. Start from the Friedmann equation for a spatially flat, matter-dominated Einstein–de Sitter universe, which yields a scale factor $a(t)$ obeying $H(a) = H_0 a^{-3/2}$ and, in cosmic time $t$, $da/dt = a H(a)$. Introduce the nondimensional time $\\tau = H_0 t$ so that the evolution equation reduces to\n$$\n\\frac{da}{d\\tau} = a^{-1/2}.\n$$\nThis equation has the exact solution\n$$\na(\\tau) = \\left(\\frac{2}{3}\\,\\tau\\right)^{2/3},\n$$\nwhich follows from separation of variables, with the big bang at $\\tau = 0$. To avoid the singularity at $\\tau = 0$, integrate from a strictly positive start time $\\tau_0  0$ with initial condition $a(\\tau_0)$ consistent with the exact solution.\n\nYour program must implement a finite difference time-stepping harness that, for each specified method and refinement level, integrates the nondimensional Friedmann ordinary differential equation from $\\tau_0$ to $\\tau_f$ and computes the global error at the final time. Use this to estimate the observed order by Richardson-style comparison across refinement levels. Specifically, given a sequence of refinements indexed by $k$ with characteristic steps $h_k$, construct errors $E_k = \\lvert a_k(\\tau_f) - a(\\tau_f)\\rvert$, where $a_k(\\tau_f)$ is the numerical solution at $\\tau_f$ at refinement level $k$. For uniform halving refinements, the observed order between two successive levels is\n$$\np_{\\mathrm{obs}} = \\log_2\\!\\left(\\frac{E_k}{E_{k+1}}\\right).\n$$\nWhen step-size constraints or adaptivity produce non-uniform changes of $h$ across the interval, apply the same formula to the sequence generated by halving a reference parameter, and interpret $p_{\\mathrm{obs}}$ as an empirical estimate that may deviate from the nominal order.\n\nImplement the following one-step methods:\n- An explicit midpoint method of nominal order $p=2$.\n- A classical Runge–Kutta method of nominal order $p=4$.\n\nFor constrained or adaptive stepping, model a cosmology-motivated step-size cap based on the Hubble time. In nondimensional variables, the Hubble time is $H(\\tau)^{-1} = a(\\tau)^{3/2}$. Impose a pointwise step-size constraint\n$$\nh(\\tau) \\le \\eta\\,a(\\tau)^{3/2},\n$$\nwith a fixed constant $\\eta  0$, while also attempting to halve a reference step $h_{\\mathrm{ref}}$ across refinement levels. Use $h(\\tau) = \\min\\!\\big(h_{\\mathrm{ref}},\\,\\eta\\,a(\\tau)^{3/2}\\big)$ inside the integrator, and force the final step to land exactly on $\\tau_f$ by shortening the last step if necessary.\n\nUse the following test suite of parameter values, which is designed to exercise a happy path, a higher-order case, and a constrained/adaptive scenario:\n- Cosmological interval and initial condition shared by all tests: $\\tau_0 = 0.1$, $\\tau_f = 1.0$, with $a(\\tau_0) = \\left(\\frac{2}{3}\\,\\tau_0\\right)^{2/3}$.\n- Test case $\\#1$ (happy path, second order): explicit midpoint method on a uniform grid with refinements $\\{N_0, 2N_0, 4N_0\\}$, where $N_0 = 50$ and $h_k = (\\tau_f - \\tau_0)/N_k$.\n- Test case $\\#2$ (happy path, fourth order): classical Runge–Kutta method on a uniform grid with refinements $\\{N_0, 2N_0, 4N_0\\}$, where $N_0 = 20$ and $h_k = (\\tau_f - \\tau_0)/N_k$.\n- Test case $\\#3$ (discrepancy under constraint): classical Runge–Kutta method with constrained step size $h(\\tau) = \\min\\!\\big(h_{\\mathrm{ref}},\\,\\eta\\,a(\\tau)^{3/2}\\big)$, at refinements $h_{\\mathrm{ref}} \\in \\{h_0, h_0/2, h_0/4\\}$ with $h_0 = 0.05$ and $\\eta = 0.1$.\n\nFor each test case, compute the observed order $p_{\\mathrm{obs}}$ between the last two refinement levels only (i.e., between $\\{2N_0, 4N_0\\}$ for uniform-grid tests or between $\\{h_0/2, h_0/4\\}$ for the constrained test). Your program should produce a single line of output containing the three observed orders as a comma-separated list enclosed in square brackets, for example, $\"[p_1,p_2,p_3]\"$. The values must be printed as floating-point numbers. All quantities are nondimensional, so no physical units are required, and angles are not involved. Percentages are not used; any ratio should be represented as a decimal number.",
            "solution": "The problem requires the construction of a numerical test harness to measure the observed order of accuracy for finite difference methods applied to a simplified cosmological ordinary differential equation (ODE). The problem is valid as it is scientifically grounded in the Friedmann equation, mathematically well-posed, and all specifications for its implementation are provided completely and unambiguously.\n\nThe physical basis is the Friedmann equation for a spatially flat, matter-dominated universe, where the Hubble parameter $H$ dependency on the scale factor $a$ is $H(a) = H_0 a^{-3/2}$. The evolution of the scale factor in cosmic time $t$ is given by the ODE $\\frac{da}{dt} = a H(a)$. By introducing a nondimensional time $\\tau = H_0 t$, this equation simplifies to:\n$$\n\\frac{da}{d\\tau} = a^{-1/2}\n$$\nThis ODE describes the expansion of the universe in this simplified model. The initial value is taken at a time $\\tau_0 = 0.1$ to avoid the \"big bang\" singularity at $\\tau = 0$. The equation can be solved analytically by separation of variables, yielding the exact solution for the scale factor:\n$$\na(\\tau) = \\left(\\frac{2}{3}\\,\\tau\\right)^{2/3}\n$$\nThe program will integrate the ODE from an initial time $\\tau_0 = 0.1$ to a final time $\\tau_f = 1.0$. The initial condition $a(\\tau_0)$ is set to be consistent with the exact solution, $a(\\tau_0) = (\\frac{2}{3}\\tau_0)^{2/3}$.\n\nThe core of the task is to solve this initial value problem (IVP) numerically using two one-step finite difference methods and to analyze their accuracy. A general one-step method approximates the solution $y(t)$ to an IVP $y'(t) = f(t, y(t))$ by producing a sequence of values $y_n \\approx y(t_n)$ at discrete time points $t_n = t_0 + n h$. The two methods specified are:\n\n$1$. **Explicit Midpoint Method**: This is a second-order Runge-Kutta method ($p=2$). The update rule from $a_n$ at time $\\tau_n$ to $a_{n+1}$ at time $\\tau_{n+1} = \\tau_n + h$ is:\n$$\n\\begin{align*}\nk_1 = f(\\tau_n, a_n) \\\\\nk_2 = f\\left(\\tau_n + \\frac{h}{2}, a_n + \\frac{h}{2} k_1\\right) \\\\\na_{n+1} = a_n + h k_2\n\\end{align*}\n$$\nwhere $f(\\tau, a) = a^{-1/2}$.\n\n$2$. **Classical Runge–Kutta Method (RK4)**: This is a fourth-order method ($p=4$). Its update rule is:\n$$\n\\begin{align*}\nk_1 = f(\\tau_n, a_n) \\\\\nk_2 = f\\left(\\tau_n + \\frac{h}{2}, a_n + \\frac{h}{2} k_1\\right) \\\\\nk_3 = f\\left(\\tau_n + \\frac{h}{2}, a_n + \\frac{h}{2} k_2\\right) \\\\\nk_4 = f\\left(\\tau_n + h, a_n + h k_3\\right) \\\\\na_{n+1} = a_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n\\end{align*}\n$$\n\nThe accuracy of these methods is quantified by the **global error** at the final time, $E = |a_{\\text{numerical}}(\\tau_f) - a_{\\text{exact}}(\\tau_f)|$. For a method of order $p$, the error is expected to scale as $E \\propto h^p$, where $h$ is a characteristic step size. By computing the error for a sequence of refined step sizes, we can estimate the method's **observed order of accuracy**, $p_{\\text{obs}}$. For two successive refinements with errors $E_k$ and $E_{k+1}$ where the step size is halved ($h_{k+1} = h_k/2$), the observed order is given by Richardson-style comparison:\n$$\np_{\\text{obs}} = \\log_2\\left(\\frac{E_k}{E_{k+1}}\\right)\n$$\n\nThe implementation will test three scenarios:\n- **Test Case $1$**: The explicit midpoint method with a uniform step size $h_k = (\\tau_f - \\tau_0) / N_k$ for a sequence of grid points $N_k = \\{50, 100, 200\\}$. The expected observed order is close to $p=2$.\n- **Test Case $2$**: The classical RK4 method with a uniform step size $h_k = (\\tau_f - \\tau_0) / N_k$ for a sequence of grid points $N_k = \\{20, 40, 80\\}$. The expected observed order is close to $p=4$.\n- **Test Case $3$**: The classical RK4 method with a variable, constrained step size. This models a common practice in cosmological simulations where the timestep is limited by the dynamical timescale of the system, here the Hubble time $H^{-1} = a^{3/2}$. The step size $h$ at each point $(\\tau, a)$ is determined by:\n$$\nh(\\tau) = \\min\\!\\big(h_{\\mathrm{ref}},\\,\\eta\\,a(\\tau)^{3/2}\\big)\n$$\nwhere $\\eta = 0.1$ is a safety factor. The refinement is achieved by halving a reference step size, $h_{\\mathrm{ref}} \\in \\{0.05, 0.025, 0.0125\\}$. Since the step size is not uniform and depends on the solution path, the observed order may deviate from the nominal order of $4$. The integrator logic ensures the final step lands exactly on $\\tau_f$ by shortening the last step if necessary.\n\nFor each of the three test cases, the program will compute the observed order using the errors from the last two refinement levels. The result will be a list of these three computed orders.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs a test harness to measure the observed order of accuracy for\n    finite difference time-integration methods on a simplified Friedmann ODE.\n    \"\"\"\n    \n    # --- Problem Definition ---\n    # Global parameters, ODE, and its exact solution\n    TAU_0 = 0.1\n    TAU_F = 1.0\n\n    def ode_rhs(tau, a):\n        \"\"\" The right-hand side of the ODE: da/d(tau) = a**(-1/2) \"\"\"\n        return a**(-0.5)\n\n    def exact_solution(tau):\n        \"\"\" The exact analytical solution: a(tau) = (2/3 * tau)**(2/3) \"\"\"\n        return (2.0/3.0 * tau)**(2.0/3.0)\n\n    A_0 = exact_solution(TAU_0)\n    A_F_EXACT = exact_solution(TAU_F)\n\n    # --- Numerical Method Steppers ---\n    def step_midpoint(f, t, y, h):\n        \"\"\"\n        Performs a single step using the explicit midpoint method (p=2).\n        \"\"\"\n        k1 = f(t, y)\n        y_mid = y + 0.5 * h * k1\n        k2 = f(t + 0.5 * h, y_mid)\n        return y + h * k2\n\n    def step_rk4(f, t, y, h):\n        \"\"\"\n        Performs a single step using the classical Runge-Kutta method (p=4).\n        \"\"\"\n        k1 = f(t, y)\n        k2 = f(t + 0.5 * h, y + 0.5 * h * k1)\n        k3 = f(t + 0.5 * h, y + 0.5 * h * k2)\n        k4 = f(t + h, y + h * k3)\n        return y + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n\n    # --- Generic Integration Driver ---\n    def integrate(stepper, step_logic_func):\n        \"\"\"\n        Integrates the ODE from TAU_0 to TAU_F using a given stepper and\n        step-size logic. The final step is adjusted to land exactly on TAU_F.\n        \"\"\"\n        t = TAU_0\n        a = A_0\n        \n        while not np.isclose(t, TAU_F):\n            h = step_logic_func(t, a)\n            \n            # Ensure the final step lands exactly on the final time\n            if t + h > TAU_F:\n                h = TAU_F - t\n            \n            a = stepper(ode_rhs, t, a, h)\n            t += h\n            \n        return a\n\n    # --- Test Harness ---\n    results = []\n    \n    # Test Case #1: Explicit Midpoint, Uniform Grid\n    case1_errors = []\n    case1_N0 = 50\n    case1_refinements = [case1_N0, 2 * case1_N0, 4 * case1_N0]\n    for N in case1_refinements:\n        h_uniform = (TAU_F - TAU_0) / N\n        step_logic = lambda t, a: h_uniform\n        a_final = integrate(step_midpoint, step_logic)\n        case1_errors.append(abs(a_final - A_F_EXACT))\n    \n    p_obs1 = np.log2(case1_errors[1] / case1_errors[2])\n    results.append(p_obs1)\n\n    # Test Case #2: Classical RK4, Uniform Grid\n    case2_errors = []\n    case2_N0 = 20\n    case2_refinements = [case2_N0, 2 * case2_N0, 4 * case2_N0]\n    for N in case2_refinements:\n        h_uniform = (TAU_F - TAU_0) / N\n        step_logic = lambda t, a: h_uniform\n        a_final = integrate(step_rk4, step_logic)\n        case2_errors.append(abs(a_final - A_F_EXACT))\n        \n    p_obs2 = np.log2(case2_errors[1] / case2_errors[2])\n    results.append(p_obs2)\n\n    # Test Case #3: Classical RK4, Constrained Step Size\n    case3_errors = []\n    case3_h0 = 0.05\n    case3_eta = 0.1\n    case3_refinements = [case3_h0, case3_h0 / 2.0, case3_h0 / 4.0]\n    for h_ref in case3_refinements:\n        # The lambda captures h_ref and case3_eta from the enclosing scope\n        step_logic = lambda t, a: min(h_ref, case3_eta * a**1.5)\n        a_final = integrate(step_rk4, step_logic)\n        case3_errors.append(abs(a_final - A_F_EXACT))\n    \n    p_obs3 = np.log2(case3_errors[1] / case3_errors[2])\n    results.append(p_obs3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A numerical method does more than just approximate a solution; it defines a new discrete dynamical system which can sometimes exhibit behaviors absent in the original continuous problem. This exercise serves as a critical cautionary tale, investigating how a seemingly accurate finite difference scheme can introduce spurious, non-physical fixed points. By analyzing a second-order Taylor scheme, you will identify the conditions that lead to these artifacts, a crucial step toward interpreting numerical simulation results with proper scientific skepticism .",
            "id": "3471966",
            "problem": "Consider an autonomous ordinary differential equation in a rescaled cosmological time variable $t$ for a dimensionless scalar field amplitude $y(t)$ governed by $y' = f(y)$. In numerical cosmology, discrete-time finite difference schemes are used to approximate the evolution. A discrete fixed point of a one-step map $y_{n+1} = G(y_n)$ is a value $y^{\\star}$ such that $G(y^{\\star}) = y^{\\star}$. The continuous fixed points of the flow are the zeros of $f$, i.e., any $y^{\\dagger}$ satisfying $f(y^{\\dagger}) = 0$. It is desirable that discrete fixed points coincide with continuous fixed points, but some schemes may introduce spurious discrete fixed points $y^{\\star}$ with $f(y^{\\star}) \\neq 0$.\n\nStarting from the exact flow expansion $y(t+h) = y(t) + h y'(t) + \\frac{h^{2}}{2} y''(t) + \\mathcal{O}(h^{3})$, and using the chain rule $y'(t) = f(y(t))$ and $y''(t) = f'(y(t)) f(y(t))$, consider the second-order Taylor finite difference scheme\n$$\ny_{n+1} = y_{n} + h f(y_{n}) + \\frac{h^{2}}{2} f'(y_{n}) f(y_{n}),\n$$\nwith constant step size $h  0$. \n\nDerive, from first principles, the discrete fixed point condition for this scheme, and determine the condition under which the scheme introduces spurious discrete fixed points, i.e., values $y$ for which $y_{n+1} = y_{n}$ while $f(y) \\neq 0$. Then, illustrate this phenomenon with the nonlinear right-hand side\n$$\nf(y) = \\lambda y - \\mu y^{3},\n$$\nwhere $\\lambda  0$ and $\\mu  0$ are constants corresponding to a quartic self-interaction in a simplified cosmological scalar field model. Assuming parameter values and $h$ are such that a positive spurious fixed point exists and is distinct from the continuous fixed points, determine the closed-form analytic expression for the smallest positive spurious discrete fixed point in terms of $\\lambda$, $\\mu$, and $h$. Express your final answer as a single analytic expression. No rounding is required.",
            "solution": "The problem requires the derivation of the smallest positive spurious discrete fixed point for a given numerical scheme applied to a specific ordinary differential equation (ODE).\n\nThe autonomous ODE is given by $y' = f(y)$. The numerical scheme provided is the second-order Taylor method:\n$$\ny_{n+1} = y_{n} + h f(y_{n}) + \\frac{h^{2}}{2} f'(y_{n}) f(y_{n})\n$$\nwhere $h  0$ is the constant step size.\n\nFirst, we establish the general condition for a discrete fixed point, $y^{\\star}$. A fixed point of the discrete map $y_{n+1} = G(y_n)$ is a value $y^{\\star}$ such that $y_{n+1} = y_n = y^{\\star}$. Substituting this into the scheme's equation yields:\n$$\ny^{\\star} = y^{\\star} + h f(y^{\\star}) + \\frac{h^{2}}{2} f'(y^{\\star}) f(y^{\\star})\n$$\nSubtracting $y^{\\star}$ from both sides, we obtain the general condition for a discrete fixed point:\n$$\n0 = h f(y^{\\star}) + \\frac{h^{2}}{2} f'(y^{\\star}) f(y^{\\star})\n$$\nSince $h  0$, we can divide the entire equation by $h$ without loss of generality:\n$$\n0 = f(y^{\\star}) + \\frac{h}{2} f'(y^{\\star}) f(y^{\\star})\n$$\nThis equation can be factored by extracting the term $f(y^{\\star})$:\n$$\nf(y^{\\star}) \\left( 1 + \\frac{h}{2} f'(y^{\\star}) \\right) = 0\n$$\nThis condition is satisfied if either $f(y^{\\star}) = 0$ or $1 + \\frac{h}{2} f'(y^{\\star}) = 0$.\n\nThe solutions to $f(y^{\\star}) = 0$ are, by definition, the continuous fixed points of the original ODE, $y' = f(y)$. These are always also fixed points of the discrete scheme.\nA spurious discrete fixed point is a fixed point of the scheme that is *not* a fixed point of the continuous system. Therefore, a spurious fixed point $y^{\\star}$ must satisfy the second condition while not satisfying the first. The defining conditions for a spurious fixed point are:\n$$\n1 + \\frac{h}{2} f'(y^{\\star}) = 0 \\quad \\text{and} \\quad f(y^{\\star}) \\neq 0\n$$\n\nThe problem specifies the function $f(y)$ as:\n$$\nf(y) = \\lambda y - \\mu y^{3}\n$$\nwhere the constants $\\lambda  0$ and $\\mu  0$. The derivative of $f(y)$ with respect to $y$ is:\n$$\nf'(y) = \\frac{d}{dy}(\\lambda y - \\mu y^{3}) = \\lambda - 3\\mu y^{2}\n$$\nThe continuous fixed points are found by solving $f(y) = 0$:\n$$\n\\lambda y - \\mu y^{3} = y(\\lambda - \\mu y^{2}) = 0\n$$\nThis yields the continuous fixed points $y^{\\dagger}_0 = 0$, $y^{\\dagger}_1 = \\sqrt{\\frac{\\lambda}{\\mu}}$, and $y^{\\dagger}_2 = -\\sqrt{\\frac{\\lambda}{\\mu}}$.\n\nTo find the spurious fixed points, we substitute the expression for $f'(y)$ into the condition $1 + \\frac{h}{2} f'(y) = 0$:\n$$\n1 + \\frac{h}{2} (\\lambda - 3\\mu y^{2}) = 0\n$$\nWe now solve this algebraic equation for $y$:\n$$\n1 + \\frac{h\\lambda}{2} - \\frac{3h\\mu}{2} y^{2} = 0\n$$\nRearranging the terms to isolate $y^2$:\n$$\n\\frac{3h\\mu}{2} y^{2} = 1 + \\frac{h\\lambda}{2}\n$$\n$$\ny^{2} = \\frac{2}{3h\\mu} \\left( 1 + \\frac{h\\lambda}{2} \\right) = \\frac{2}{3h\\mu} + \\frac{\\lambda}{3\\mu}\n$$\nCombining the terms on the right-hand side gives:\n$$\ny^{2} = \\frac{2 + h\\lambda}{3h\\mu}\n$$\nThis equation gives two solutions for $y$:\n$$\ny = \\pm \\sqrt{\\frac{2 + h\\lambda}{3h\\mu}}\n$$\nThese are the candidates for the spurious fixed points. Since $\\lambda, \\mu, h$ are all positive, the argument of the square root is always positive, so two real solutions always exist. The problem asks for the smallest positive spurious discrete fixed point. There is only one positive solution:\n$$\ny^{\\star}_{\\text{pos}} = \\sqrt{\\frac{2 + h\\lambda}{3h\\mu}}\n$$\nThe problem statement assumes that this fixed point is distinct from the continuous fixed points. This is equivalent to requiring $f(y^{\\star}_{\\text{pos}}) \\neq 0$, which means $y^{\\star}_{\\text{pos}}$ must not be equal to $0$ or $\\pm\\sqrt{\\lambda/\\mu}$. It is clear that $y^{\\star}_{\\text{pos}}  0$. The case where $y^{\\star}_{\\text{pos}} = \\sqrt{\\lambda/\\mu}$ would imply $\\frac{2+h\\lambda}{3h\\mu} = \\frac{\\lambda}{\\mu}$, which simplifies to $h\\lambda=1$. The problem's phrasing ensures we are not considering this special case.\n\nThus, the smallest (and only) positive spurious discrete fixed point is the expression we derived.",
            "answer": "$$\\boxed{\\sqrt{\\frac{2 + h\\lambda}{3h\\mu}}}$$"
        }
    ]
}