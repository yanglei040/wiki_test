{
    "hands_on_practices": [
        {
            "introduction": "The foundation of any Fisher matrix forecast lies in accurately computing the derivatives of your theoretical model with respect to its parameters. This first practice focuses on the numerical nuts-and-bolts of this task, guiding you to derive an optimal finite-difference scheme from first principles. By balancing the competing effects of truncation and round-off error, you will develop a robust method for calculating the gradients that are the essential inputs for the Fisher matrix .",
            "id": "3472421",
            "problem": "You are tasked with building a self-contained numerical workflow to compute parameter derivatives of a cosmological observable vector for use in Fisher information matrix forecasting. The objective is to design a central-difference strategy for estimating parameter gradients, quantify the leading truncation and round-off errors, and select an optimal step size for the finite difference that balances these errors from first principles.\n\nDefine the observable vector $\\boldsymbol{\\mu}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^m$ with parameters $\\boldsymbol{\\theta} = (\\theta_1,\\theta_2)$ as follows. For a fixed set of wavenumbers $\\{k_j\\}_{j=1}^m$ expressed in units of $h/\\mathrm{Mpc}$, fixed matter density parameter $\\Omega_m = 0.315$, and fixed Hubble parameter $h=0.674$, let\n$$\n\\mu_j(\\boldsymbol{\\theta}) = \\exp(\\theta_1)\\left(\\frac{k_j}{k_0}\\right)^{\\theta_2} T^2(k_j;\\Omega_m,h), \\quad j=1,\\dots,m,\n$$\nwhere $k_0 = 0.05\\,h/\\mathrm{Mpc}$ and $T(k;\\Omega_m,h)$ is the Bardeen-Bond-Kaiser-Szalay (BBKS) transfer function written in terms of $q = k/\\Gamma$ with $\\Gamma = \\Omega_m h$, defined by\n$$\nT(q) = \\frac{\\ln(1 + 2.34\\,q)}{2.34\\,q} \\left[1 + 3.89\\,q + (16.1\\,q)^2 + (5.46\\,q)^3 + (6.71\\,q)^4\\right]^{-1/4}.\n$$\nYou should treat $\\boldsymbol{\\mu}(\\boldsymbol{\\theta})$ as the predicted vector of power spectrum amplitudes in $m$ bins, and you will compute its parameter-gradient components $\\partial_i \\boldsymbol{\\mu}$ needed for Fisher information forecasts.\n\nStarting from the central concepts of numerical analysis and floating-point arithmetic, and without introducing any shortcut formulas, you must derive from first principles:\n- The central-difference estimator for the directional derivative of each component of $\\boldsymbol{\\mu}$ with respect to $\\theta_i$, using a symmetric step of size $h_i$.\n- An asymptotic model for the leading truncation error and the leading round-off error in this derivative estimation, obtained via Taylor expansion and a standard floating-point error model. You must express how these error contributions scale as functions of $h_i$ and appropriate derivatives of the target function.\n- A principled choice of an optimal step size $h_i$ that balances the truncation error and round-off error for each parameter direction, and a robust aggregation rule to obtain a single $h_i$ per parameter from component-wise estimates.\n\nImplement your strategy in a program that:\n1. Uses central differences to compute the gradient vectors $\\partial_1 \\boldsymbol{\\mu}$ and $\\partial_2 \\boldsymbol{\\mu}$ at given $\\boldsymbol{\\theta}$.\n2. Selects the step sizes $h_1$ and $h_2$ by minimizing the derived composite error model using quantities available from $\\boldsymbol{\\mu}$ and its local behavior, and aggregates component-wise optimal steps into a single step per parameter in a robust manner.\n3. Computes a reference “analytic” gradient using calculus for $\\partial_1 \\boldsymbol{\\mu}$ and $\\partial_2 \\boldsymbol{\\mu}$ for validation, based solely on the defined $\\boldsymbol{\\mu}(\\boldsymbol{\\theta})$.\n4. Reports the maximum absolute discrepancy between the central-difference gradient and the analytic gradient across the $m$ components for each parameter.\n\nUse the following test suite of parameter values and wavenumber bins:\n- The wavenumber bins are $k_j \\in \\{0.02, 0.07, 0.15, 0.25\\}$ in $h/\\mathrm{Mpc}$, with $m=4$.\n- Test case $1$: $\\theta_1 = \\ln(2.1\\times 10^{-9})$, $\\theta_2 = 0.965$.\n- Test case $2$: $\\theta_1 = \\ln(1.0\\times 10^{-12})$, $\\theta_2 = 0.900$.\n- Test case $3$: $\\theta_1 = \\ln(1.0\\times 10^{-8})$, $\\theta_2 = 1.100$.\n\nYour program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list of four floating-point numbers in the order $[h_1, h_2, e_1, e_2]$, with $h_i$ denoting the chosen optimal step size for parameter $\\theta_i$, and $e_i$ denoting the maximum absolute error between the central-difference and analytic gradient across the $m$ components for parameter $\\theta_i$. For example, the final printed line should look like\n$$\n[\\,[h_{1}^{(1)},h_{2}^{(1)},e_{1}^{(1)},e_{2}^{(1)}],[h_{1}^{(2)},h_{2}^{(2)},e_{1}^{(2)},e_{2}^{(2)}],[h_{1}^{(3)},h_{2}^{(3)},e_{1}^{(3)},e_{2}^{(3)}]\\,].\n$$\nNo angles are used in this problem. All physical quantities that involve units are specified in the statement, and all outputs are dimensionless floating-point numbers.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and self-contained. It presents a standard, albeit detailed, numerical analysis task within the context of physical cosmology. All necessary functions, parameters, and test conditions are explicitly defined, and there are no internal contradictions, factual errors, or ambiguities. The problem is therefore deemed valid and a solution will be provided.\n\nThe task is to compute the parameter-gradient of a cosmological observable $\\boldsymbol{\\mu}(\\boldsymbol{\\theta})$ and to determine the optimal step size for the finite-difference method by deriving an error model from first principles.\n\nLet the function of interest be $f(x)$. We wish to compute its derivative, $f'(x)$. We begin by invoking the Taylor series expansion of $f(x)$ around a point $x_0$:\n$$ f(x_0 + h) = f(x_0) + h f'(x_0) + \\frac{h^2}{2!} f''(x_0) + \\frac{h^3}{3!} f'''(x_0) + O(h^4) $$\n$$ f(x_0 - h) = f(x_0) - h f'(x_0) + \\frac{h^2}{2!} f''(x_0) - \\frac{h^3}{3!} f'''(x_0) + O(h^4) $$\nSubtracting the second expansion from the first yields:\n$$ f(x_0 + h) - f(x_0 - h) = 2h f'(x_0) + \\frac{2h^3}{6} f'''(x_0) + O(h^5) $$\nRearranging to solve for the derivative $f'(x_0)$:\n$$ f'(x_0) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h} - \\frac{h^2}{6} f'''(x_0) + O(h^4) $$\nThe first term on the right-hand side is the central-difference formula for the first derivative, which we denote $D_h[f](x_0)$. The second term, $-\\frac{h^2}{6} f'''(x_0)$, is the leading-order truncation error of this approximation. The magnitude of the truncation error is thus:\n$$ \\epsilon_{\\text{trunc}}(h) = \\left| D_h[f](x_0) - f'(x_0) \\right| \\approx \\frac{h^2}{6} |f'''(x_0)| $$\nThis error arises from approximating the true function with a finite-order polynomial and scales as $O(h^2)$.\n\nThe second source of error is round-off error, which originates from the finite precision of floating-point arithmetic. Let $\\tilde{f}(x)$ denote the machine-representable value of $f(x)$. A standard model for floating-point representation is $\\tilde{f}(x) = f(x)(1+\\delta)$, where $|\\delta| \\le \\epsilon_{\\text{mach}}$, and $\\epsilon_{\\text{mach}}$ is the machine epsilon (the smallest number such that $1 + \\epsilon_{\\text{mach}} \\neq 1$ in floating-point arithmetic).\n\nThe computed numerical derivative is not $D_h[f](x_0)$, but rather $\\tilde{D}_h[f](x_0)$:\n$$ \\tilde{D}_h[f](x_0) = \\frac{\\tilde{f}(x_0 + h) - \\tilde{f}(x_0 - h)}{2h} = \\frac{f(x_0 + h)(1+\\delta_1) - f(x_0 - h)(1+\\delta_2)}{2h} $$\nwhere $|\\delta_1|, |\\delta_2| \\le \\epsilon_{\\text{mach}}$. Expanding this expression gives:\n$$ \\tilde{D}_h[f](x_0) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h} + \\frac{f(x_0 + h)\\delta_1 - f(x_0 - h)\\delta_2}{2h} $$\nThe second term is the round-off error. In the worst-case scenario, its magnitude is bounded by:\n$$ \\epsilon_{\\text{round}}(h) \\approx \\frac{|f(x_0 + h)|\\epsilon_{\\text{mach}} + |f(x_0 - h)|\\epsilon_{\\text{mach}}}{2h} $$\nFor a small step size $h$, we can approximate $f(x_0 \\pm h) \\approx f(x_0)$. The round-off error is therefore:\n$$ \\epsilon_{\\text{round}}(h) \\approx \\frac{2|f(x_0)|\\epsilon_{\\text{mach}}}{2h} = \\frac{|f(x_0)|\\epsilon_{\\text{mach}}}{h} $$\nThis error scales as $O(1/h)$ and grows as the step size $h$ decreases.\n\nThe total numerical error, $\\epsilon_{\\text{total}}(h)$, is the sum of the magnitudes of the truncation and round-off errors:\n$$ \\epsilon_{\\text{total}}(h) \\approx \\epsilon_{\\text{trunc}}(h) + \\epsilon_{\\text{round}}(h) = \\frac{h^2}{6} |f'''(x_0)| + \\frac{|f(x_0)|\\epsilon_{\\text{mach}}}{h} $$\nTo find the optimal step size $h_{\\text{opt}}$ that minimizes this total error, we differentiate $\\epsilon_{\\text{total}}(h)$ with respect to $h$ and set the result to zero:\n$$ \\frac{d\\epsilon_{\\text{total}}}{dh} = \\frac{2h}{6} |f'''(x_0)| - \\frac{|f(x_0)|\\epsilon_{\\text{mach}}}{h^2} = 0 $$\n$$ \\frac{h}{3} |f'''(x_0)| = \\frac{|f(x_0)|\\epsilon_{\\text{mach}}}{h^2} $$\nSolving for $h$ yields the optimal step size:\n$$ h_{\\text{opt}} = \\left( \\frac{3 |f(x_0)| \\epsilon_{\\text{mach}}}{|f'''(x_0)|} \\right)^{1/3} $$\n\nWe now apply this framework to the problem's observable vector $\\boldsymbol{\\mu}(\\boldsymbol{\\theta})$, where $f(\\theta_i)$ corresponds to a component $\\mu_j(\\boldsymbol{\\theta})$ differentiated with respect to a parameter $\\theta_i$. The optimal step size for parameter $\\theta_i$ and component $j$ is:\n$$ h_{i,j}^{\\text{opt}} = \\left( \\frac{3 |\\mu_j(\\boldsymbol{\\theta})| \\epsilon_{\\text{mach}}}{|\\partial_i^3 \\mu_j(\\boldsymbol{\\theta})|} \\right)^{1/3} $$\n\nThe functional form of $\\mu_j(\\boldsymbol{\\theta})$ is $\\mu_j(\\boldsymbol{\\theta}) = \\exp(\\theta_1)\\left(\\frac{k_j}{k_0}\\right)^{\\theta_2} T^2(k_j)$. We must compute the third partial derivatives.\n\nFor $\\theta_1$:\nThe dependence is on $\\exp(\\theta_1)$. Thus, each partial derivative with respect to $\\theta_1$ simply multiplies the function by $1$.\n$$ \\partial_1 \\mu_j(\\boldsymbol{\\theta}) = \\mu_j(\\boldsymbol{\\theta}), \\quad \\partial_1^2 \\mu_j(\\boldsymbol{\\theta}) = \\mu_j(\\boldsymbol{\\theta}), \\quad \\partial_1^3 \\mu_j(\\boldsymbol{\\theta}) = \\mu_j(\\boldsymbol{\\theta}) $$\nSubstituting this into the optimal step size formula:\n$$ h_{1,j}^{\\text{opt}} = \\left( \\frac{3 |\\mu_j(\\boldsymbol{\\theta})| \\epsilon_{\\text{mach}}}{|\\mu_j(\\boldsymbol{\\theta})|} \\right)^{1/3} = (3 \\epsilon_{\\text{mach}})^{1/3} $$\nThis step size is independent of the component $j$. Thus, a single optimal step $h_1 = (3 \\epsilon_{\\text{mach}})^{1/3}$ can be used for all components.\n\nFor $\\theta_2$:\nThe dependence is on $C^{\\theta_2}$ where $C = k_j/k_0$. The derivative of $C^x$ is $(\\ln C) C^x$.\n$$ \\partial_2 \\mu_j(\\boldsymbol{\\theta}) = \\mu_j(\\boldsymbol{\\theta}) \\ln\\left(\\frac{k_j}{k_0}\\right) $$\n$$ \\partial_2^2 \\mu_j(\\boldsymbol{\\theta}) = \\mu_j(\\boldsymbol{\\theta}) \\left[\\ln\\left(\\frac{k_j}{k_0}\\right)\\right]^2 $$\n$$ \\partial_2^3 \\mu_j(\\boldsymbol{\\theta}) = \\mu_j(\\boldsymbol{\\theta}) \\left[\\ln\\left(\\frac{k_j}{k_0}\\right)\\right]^3 $$\nSubstituting into the optimal step size formula:\n$$ h_{2,j}^{\\text{opt}} = \\left( \\frac{3 |\\mu_j(\\boldsymbol{\\theta})| \\epsilon_{\\text{mach}}}{|\\mu_j(\\boldsymbol{\\theta}) [\\ln(k_j/k_0)]^3|} \\right)^{1/3} = \\frac{(3 \\epsilon_{\\text{mach}})^{1/3}}{|\\ln(k_j/k_0)|} $$\nThis step size, $h_{2,j}^{\\text{opt}}$, depends on the component $j$ through the wavenumber $k_j$. As the problem requires a single step size $h_2$ for the parameter $\\theta_2$, we must aggregate the component-wise optimal steps $\\{h_{2,j}^{\\text{opt}}\\}_{j=1}^m$. A robust aggregation rule insensitive to outliers is the median. For the given problem with $m=4$ components, the median of the sorted values $\\{s_1, s_2, s_3, s_4\\}$ is $(s_2+s_3)/2$.\n\nFor validation, we require the analytic gradients. As derived above:\n- The analytic gradient with respect to $\\theta_1$ is $\\partial_1 \\boldsymbol{\\mu} = \\boldsymbol{\\mu}$.\n- The analytic gradient with respect to $\\theta_2$ has components $(\\partial_2 \\boldsymbol{\\mu})_j = \\mu_j(\\boldsymbol{\\theta}) \\ln(k_j/k_0)$.\n\nThe implementation plan is as follows:\n1. For each test case, define the parameters $\\boldsymbol{\\theta}$ and constants.\n2. Implement the BBKS transfer function $T(k)$ and the observable vector $\\boldsymbol{\\mu}(\\boldsymbol{\\theta})$.\n3. Calculate the optimal step size $h_1 = (3\\epsilon_{\\text{mach}})^{1/3}$.\n4. For each component $j$, calculate $h_{2,j}^{\\text{opt}}$, then find the median of these values to determine $h_2$.\n5. Compute the numerical derivatives $\\partial_1 \\boldsymbol{\\mu}$ and $\\partial_2 \\boldsymbol{\\mu}$ using the central-difference formula with the determined step sizes $h_1$ and $h_2$.\n6. Compute the analytic derivatives $\\partial_1 \\boldsymbol{\\mu}$ and $\\partial_2 \\boldsymbol{\\mu}$ using the exact expressions.\n7. Calculate the maximum absolute error $e_i = \\max_j |(\\partial_i \\boldsymbol{\\mu})_{\\text{num},j} - (\\partial_i \\boldsymbol{\\mu})_{\\text{an},j}|$ for each parameter $i \\in \\{1,2\\}$.\n8. Collate and format the results $[h_1, h_2, e_1, e_2]$ for each test case as required.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes parameter derivatives for a cosmological observable using a\n    central-difference scheme with an optimally chosen step size, and\n    validates the result against an analytic derivative.\n    \"\"\"\n\n    # Fixed cosmological and model parameters\n    OMEGA_M = 0.315\n    H_PARAM = 0.674\n    K0 = 0.05  # in h/Mpc\n\n    # Wavenumber bins\n    K_VALUES = np.array([0.02, 0.07, 0.15, 0.25])  # in h/Mpc\n    \n    # Machine epsilon for double precision floating-point numbers\n    EPS_MACH = np.finfo(float).eps\n\n    # Test cases for theta = [theta1, theta2]\n    test_cases = [\n        (np.log(2.1e-9), 0.965),\n        (np.log(1.0e-12), 0.900),\n        (np.log(1.0e-8), 1.100),\n    ]\n\n    def bbks_transfer(k, omega_m, h):\n        \"\"\"\n        Computes the Bardeen-Bond-Kaiser-Szalay (BBKS) transfer function.\n        \"\"\"\n        gamma = omega_m * h\n        q = k / gamma\n        \n        # Handle the case q=0 to avoid division by zero, T(q->0) -> 1\n        if np.isscalar(q):\n            if q == 0: return 1.0\n        else:\n            q[q == 0] = 1e-9 # Avoid division by zero, small q approx is fine\n\n        log_term = np.log(1 + 2.34 * q) / (2.34 * q)\n        poly_term = (\n            1 + 3.89 * q + (16.1 * q)**2 + (5.46 * q)**3 + (6.71 * q)**4\n        )**(-0.25)\n        return log_term * poly_term\n\n    def mu_vector(theta, k_vals, omega_m, h, k0):\n        \"\"\"\n        Computes the observable vector mu(theta).\n        \"\"\"\n        theta1, theta2 = theta\n        t_sq = bbks_transfer(k_vals, omega_m, h)**2\n        return np.exp(theta1) * (k_vals / k0)**theta2 * t_sq\n\n    results = []\n    \n    for theta_val in test_cases:\n        theta1, theta2 = theta_val\n\n        # 1. Determine optimal step sizes h1 and h2\n        \n        # For theta1, the optimal step size is constant\n        h1 = (3 * EPS_MACH)**(1/3)\n\n        # For theta2, the optimal step size depends on k_j. We aggregate using the median.\n        log_k_ratios = np.log(K_VALUES / K0)\n        # Handle case where k_j = k0 -> ln(1) = 0 -> division by zero\n        # In this problem, k_j is never equal to k0.\n        h2_j_optimal = h1 / np.abs(log_k_ratios)\n        \n        # Median for m=4 components is the average of the two central elements\n        sorted_h2_j = np.sort(h2_j_optimal)\n        h2 = (sorted_h2_j[1] + sorted_h2_j[2]) / 2.0\n\n        # 2. Compute numerical gradients using central differences\n        \n        # Gradient w.r.t. theta1\n        theta1_plus = (theta1 + h1, theta2)\n        theta1_minus = (theta1 - h1, theta2)\n        mu_plus1 = mu_vector(theta1_plus, K_VALUES, OMEGA_M, H_PARAM, K0)\n        mu_minus1 = mu_vector(theta1_minus, K_VALUES, OMEGA_M, H_PARAM, K0)\n        grad1_num = (mu_plus1 - mu_minus1) / (2 * h1)\n        \n        # Gradient w.r.t. theta2\n        theta2_plus = (theta1, theta2 + h2)\n        theta2_minus = (theta1, theta2 - h2)\n        mu_plus2 = mu_vector(theta2_plus, K_VALUES, OMEGA_M, H_PARAM, K0)\n        mu_minus2 = mu_vector(theta2_minus, K_VALUES, OMEGA_M, H_PARAM, K0)\n        grad2_num = (mu_plus2 - mu_minus2) / (2 * h2)\n\n        # 3. Compute analytic gradients for validation\n        mu_at_theta = mu_vector(theta_val, K_VALUES, OMEGA_M, H_PARAM, K0)\n        \n        # d/d(theta1) [exp(theta1) * f(theta2)] = exp(theta1) * f(theta2) = mu\n        grad1_an = mu_at_theta\n        \n        # d/d(theta2) [C * a^theta2] = C * a^theta2 * ln(a) = mu * ln(a)\n        grad2_an = mu_at_theta * log_k_ratios\n\n        # 4. Compute maximum absolute errors\n        e1 = np.max(np.abs(grad1_num - grad1_an))\n        e2 = np.max(np.abs(grad2_num - grad2_an))\n\n        results.append([h1, h2, e1, e2])\n\n    # Final print statement in the exact required format\n    # Using a nested list comprehension and repr() for floating point precision\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Once a Fisher matrix is calculated for a set of model parameters, its real power comes from propagating those constraints to physically meaningful derived quantities. This exercise demonstrates how to use the covariance matrix, the inverse of the Fisher matrix, to forecast the uncertainty on the dark energy equation of state $w(z)$ at any redshift . You will discover the powerful concept of a 'pivot redshift,' the specific point at which the experiment provides the tightest constraint, a crucial insight for survey design and interpretation.",
            "id": "3472326",
            "problem": "A Stage IV spectroscopic galaxy redshift survey constrains the dark energy equation-of-state parameters in the Chevallier–Polarski–Linder (CPL) model, where the equation of state is written as $w(z)=w_0+w_a\\,z/(1+z)$. Near the maximum of the likelihood, assume the posterior for the parameter vector $\\boldsymbol{\\theta}=(w_0,w_a)$ is well approximated by a Gaussian. The Fisher information matrix for $(w_0,w_a)$ is measured to be\n$$\n\\mathbf{F} \\;=\\; \\begin{pmatrix}\n100 & 40 \\\\\n40 & 30\n\\end{pmatrix}.\n$$\nStarting from the definition of the Fisher information matrix and the Gaussian approximation to the likelihood, derive how to propagate constraints from $(w_0,w_a)$ to the derived quantity $w(z)$ at a given redshift $z$ by treating $w(z)$ as a function of $(w_0,w_a)$. Then, without assuming any pre-derived pivot formulas, analytically determine the redshift $z_\\star$ at which the forecasted $1\\sigma$ variance of $w(z)$ is minimized. Finally, evaluate $z_\\star$ using the Fisher matrix above. Round your final numerical answer for $z_\\star$ to four significant figures.",
            "solution": "The problem requires us to first derive the method for propagating constraints on the Chevallier–Polarski–Linder (CPL) parameters, $\\boldsymbol{\\theta}=(w_0, w_a)$, to the derived quantity $w(z)$, and then find the redshift $z_\\star$ at which the variance on $w(z)$ is minimized.\n\nThe problem states that near the maximum of the likelihood $\\mathcal{L}$, the posterior for the parameters $\\boldsymbol{\\theta}$ is well approximated by a Gaussian distribution. The logarithm of such a likelihood function can be expressed as:\n$$\n\\ln \\mathcal{L}(\\boldsymbol{\\theta}) \\approx \\text{const} - \\frac{1}{2} \\sum_{i,j} (\\theta_i - \\hat{\\theta}_i) F_{ij} (\\theta_j - \\hat{\\theta}_j)\n$$\nwhere $\\hat{\\boldsymbol{\\theta}}$ is the maximum-likelihood estimate of the parameters and $\\mathbf{F}$ is the Fisher information matrix. The elements of the Fisher matrix are given by $F_{ij} = -\\left\\langle \\frac{\\partial^2 \\ln \\mathcal{L}}{\\partial \\theta_i \\partial \\theta_j} \\right\\rangle$. This Gaussian form for the likelihood implies that the covariance matrix of the parameters, $\\mathbf{C}$, is the inverse of the Fisher matrix:\n$$\n\\mathbf{C} = \\mathbf{F}^{-1}\n$$\nThe elements of the covariance matrix are $C_{ij} = \\text{cov}(\\theta_i, \\theta_j)$, with the diagonal elements being the variances, $C_{ii} = \\sigma^2_{\\theta_i}$.\n\nThe CPL model for the dark energy equation of state is given by:\n$$\nw(z) = w_0 + w_a \\frac{z}{1+z}\n$$\nThis is a derived quantity that is a function of the parameters $w_0$ and $w_a$. We wish to find the variance of $w(z)$, denoted by $\\sigma^2_{w(z)}$. For a general derived quantity $Q(\\boldsymbol{\\theta})$, its variance can be found using the standard formula for propagation of uncertainty:\n$$\n\\sigma_Q^2 = \\sum_{i,j} \\frac{\\partial Q}{\\partial \\theta_i} \\frac{\\partial Q}{\\partial \\theta_j} C_{ij}\n$$\nIn our case, the parameters are $\\theta_1 = w_0$ and $\\theta_2 = w_a$. The derived quantity is $Q = w(z)$. We first compute the partial derivatives of $w(z)$ with respect to the parameters:\n$$\n\\frac{\\partial w(z)}{\\partial w_0} = 1\n$$\n$$\n\\frac{\\partial w(z)}{\\partial w_a} = \\frac{z}{1+z}\n$$\nLet the covariance matrix for $(w_0, w_a)$ be denoted as:\n$$\n\\mathbf{C} = \\begin{pmatrix} C_{00} & C_{0a} \\\\ C_{0a} & C_{aa} \\end{pmatrix} = \\begin{pmatrix} \\sigma^2_{w_0} & \\text{cov}(w_0, w_a) \\\\ \\text{cov}(w_0, w_a) & \\sigma^2_{w_a} \\end{pmatrix}\n$$\nSubstituting the partial derivatives into the error propagation formula, we obtain the variance of $w(z)$:\n$$\n\\sigma^2_{w(z)} = \\left(\\frac{\\partial w}{\\partial w_0}\\right)^2 C_{00} + \\left(\\frac{\\partial w}{\\partial w_a}\\right)^2 C_{aa} + 2 \\left(\\frac{\\partial w}{\\partial w_0}\\right) \\left(\\frac{\\partial w}{\\partial w_a}\\right) C_{0a}\n$$\n$$\n\\sigma^2_{w(z)} = (1)^2 C_{00} + \\left(\\frac{z}{1+z}\\right)^2 C_{aa} + 2(1)\\left(\\frac{z}{1+z}\\right) C_{0a}\n$$\nThis expression provides the propagated $1\\sigma$ variance on $w(z)$ as a function of redshift $z$.\n\nNext, we must find the redshift $z_\\star$ at which $\\sigma^2_{w(z)}$ is minimized. To do this, we differentiate $\\sigma^2_{w(z)}$ with respect to $z$ and set the result to zero. The derivative of the term $\\frac{z}{1+z}$ with respect to $z$ is:\n$$\n\\frac{d}{dz}\\left(\\frac{z}{1+z}\\right) = \\frac{(1)(1+z) - (z)(1)}{(1+z)^2} = \\frac{1}{(1+z)^2}\n$$\nNow, differentiating $\\sigma^2_{w(z)}$:\n$$\n\\frac{d}{dz}\\sigma^2_{w(z)} = \\frac{d}{dz} \\left[ C_{00} + C_{aa}\\left(\\frac{z}{1+z}\\right)^2 + 2C_{0a}\\left(\\frac{z}{1+z}\\right) \\right]\n$$\n$$\n\\frac{d}{dz}\\sigma^2_{w(z)} = 2C_{aa}\\left(\\frac{z}{1+z}\\right) \\frac{d}{dz}\\left(\\frac{z}{1+z}\\right) + 2C_{0a}\\frac{d}{dz}\\left(\\frac{z}{1+z}\\right)\n$$\n$$\n\\frac{d}{dz}\\sigma^2_{w(z)} = \\left[ 2C_{aa}\\left(\\frac{z}{1+z}\\right) + 2C_{0a} \\right] \\frac{1}{(1+z)^2}\n$$\nSetting the derivative to zero to find the extremum at $z=z_\\star$:\n$$\n\\left[ 2C_{aa}\\left(\\frac{z_\\star}{1+z_\\star}\\right) + 2C_{0a} \\right] \\frac{1}{(1+z_\\star)^2} = 0\n$$\nSince $z_\\star \\ge 0$, the term $\\frac{1}{(1+z_\\star)^2}$ is non-zero. Therefore, the term in the brackets must be zero:\n$$\nC_{aa}\\left(\\frac{z_\\star}{1+z_\\star}\\right) + C_{0a} = 0\n$$\n$$\n\\frac{z_\\star}{1+z_\\star} = -\\frac{C_{0a}}{C_{aa}}\n$$\nTo confirm this is a minimum, we would check the second derivative. The second derivative at $z_\\star$ is $\\frac{d^2\\sigma^2_{w(z)}}{dz^2}\\big|_{z=z_\\star} = \\frac{2 C_{aa}}{(1+z_\\star)^4}$. Since a covariance matrix must be positive-definite, its diagonal elements are positive, so $C_{aa} = \\sigma^2_{w_a} > 0$. Thus, the second derivative is positive, confirming a minimum.\n\nSolving for $z_\\star$:\n$$\nz_\\star = -\\frac{C_{0a}}{C_{aa}}(1+z_\\star) \\implies z_\\star = -\\frac{C_{0a}}{C_{aa}} - \\frac{C_{0a}}{C_{aa}}z_\\star\n$$\n$$\nz_\\star\\left(1 + \\frac{C_{0a}}{C_{aa}}\\right) = -\\frac{C_{0a}}{C_{aa}} \\implies z_\\star\\left(\\frac{C_{aa} + C_{0a}}{C_{aa}}\\right) = -\\frac{C_{0a}}{C_{aa}}\n$$\n$$\nz_\\star = -\\frac{C_{0a}}{C_{aa} + C_{0a}}\n$$\nThis is the analytical expression for the redshift $z_\\star$ that minimizes the variance of $w(z)$.\n\nFinally, we evaluate $z_\\star$ using the given Fisher matrix:\n$$\n\\mathbf{F} = \\begin{pmatrix} 100 & 40 \\\\ 40 & 30 \\end{pmatrix}\n$$\nFirst, we calculate the covariance matrix $\\mathbf{C} = \\mathbf{F}^{-1}$. The determinant of $\\mathbf{F}$ is:\n$$\n\\det(\\mathbf{F}) = (100)(30) - (40)(40) = 3000 - 1600 = 1400\n$$\nThe inverse of a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ is $\\frac{1}{ad-bc}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$. Thus:\n$$\n\\mathbf{C} = \\frac{1}{1400} \\begin{pmatrix} 30 & -40 \\\\ -40 & 100 \\end{pmatrix}\n$$\nThe components of the covariance matrix are:\n$C_{0a} = \\text{cov}(w_0, w_a) = -\\frac{40}{1400} = -\\frac{4}{140} = -\\frac{1}{35}$\n$C_{aa} = \\sigma^2_{w_a} = \\frac{100}{1400} = \\frac{10}{140} = \\frac{1}{14}$\n\nNow we substitute these values into the expression for $\\frac{z_\\star}{1+z_\\star}$:\n$$\n\\frac{z_\\star}{1+z_\\star} = -\\frac{C_{0a}}{C_{aa}} = -\\frac{-1/35}{1/14} = \\frac{1}{35} \\times 14 = \\frac{14}{35} = \\frac{2}{5}\n$$\nSolving for $z_\\star$:\n$$\n5z_\\star = 2(1+z_\\star) = 2 + 2z_\\star\n$$\n$$\n3z_\\star = 2 \\implies z_\\star = \\frac{2}{3}\n$$\nThe problem asks for a numerical answer rounded to four significant figures.\n$$\nz_\\star = \\frac{2}{3} \\approx 0.666666...\n$$\nRounding to four significant figures gives $z_\\star = 0.6667$.",
            "answer": "$$\n\\boxed{0.6667}\n$$"
        },
        {
            "introduction": "A forecast is only as reliable as its underlying assumptions. The Fisher matrix formalism assumes the likelihood is well-approximated by a Gaussian, an assumption that can fail dramatically in certain physical regimes. This final practice presents a thought experiment involving low-count statistics to explore a scenario where the Fisher forecast significantly underestimates the true uncertainty . By comparing the forecast to the exact Bayesian result, you will gain critical insight into the limitations of the Fisher matrix and the importance of parameter boundaries and non-Gaussian likelihoods.",
            "id": "3472477",
            "problem": "In a large-volume cosmological survey, suppose the detection of a rare tracer population (for example, extremely massive galaxy clusters arising from primordial non-Gaussian features) is modeled by a single amplitude parameter $A \\ge 0$ that scales the expected number of detections. Let the survey sensitivity be summarized by a known positive constant $s > 0$, such that the expected count is $\\mu(A) = A s$. The observed count $N$ is modeled as a Poisson random variable with mean $\\mu(A)$.\n\nYou are asked to perform a Fisher information matrix forecast for $A$ about a small, positive fiducial value $A_0 > 0$, and then to compare it against the exact posterior uncertainty in a realized data case where no detections are found, $N = 0$, under a flat prior on $A$ for $A \\ge 0$.\n\nTasks:\n1. Starting from the definition of Fisher information for a scalar parameter, derive the forecast variance for $A$ at the fiducial $A_0$.\n2. For a realized outcome $N = 0$ and a flat prior $p(A) \\propto 1$ for $A \\ge 0$, derive the exact posterior variance of $A$.\n3. Provide the exact closed-form expression for the ratio $R$ of the exact posterior variance to the Fisher forecast variance, in terms of $A_0$ and $s$.\n4. Briefly, but rigorously, articulate the epistemic reasons why the Fisher forecast underestimates uncertainties in this setup, focusing on the role of the boundary $A \\ge 0$ and the non-Gaussian shape of the likelihood.\n\nYour final answer must be the expression for $R$. No numerical approximation is required, and the ratio $R$ is dimensionless, so no units are needed in the final answer. Ensure all derivations are self-contained and do not invoke any shortcuts beyond fundamental definitions of the Poisson likelihood and Fisher information.",
            "solution": "The problem requires a comparison between a Fisher information forecast and an exact posterior uncertainty for a Poisson-distributed observable. The problem is scientifically grounded, well-posed, and contains all necessary information for a solution. It is therefore deemed valid.\n\n### Step 1: Validation of the Problem Statement\n\n**1. Extraction of Givens:**\n- A single amplitude parameter $A \\ge 0$.\n- A known positive survey sensitivity constant $s > 0$.\n- The expected count is a function of $A$: $\\mu(A) = A s$.\n- The observed count $N$ is a Poisson random variable with mean $\\mu(A)$.\n- The analysis is centered around a small, positive fiducial value $A_0 > 0$.\n- A realized data outcome is $N = 0$.\n- The prior probability distribution for $A$ is flat: $p(A) \\propto 1$ for $A \\ge 0$.\n\n**2. Validation using Extracted Givens:**\n- **Scientific Grounding:** The problem is set in the context of cosmological surveys and uses standard, fundamental statistical tools: the Poisson distribution, Fisher information, and Bayesian inference. The scenario of forecasting constraints on a parameter governing rare event counts is a common practice in cosmology. Thus, the problem is scientifically sound.\n- **Well-Posedness:** The problem asks for specific, well-defined derivations and a conclusive ratio. All variables and models are explicitly defined, leading to a unique and meaningful solution.\n- **Objectivity:** The problem statement is formulated using precise, objective, and unbiased scientific language.\n\n**3. Verdict and Action:**\nThe problem is valid. We will proceed to derive the solution.\n\n### Step 2: Solution Derivation\n\n**Task 1: Fisher Information Forecast Variance**\n\nThe probability of observing $N$ counts for a given parameter value $A$ is described by the Poisson likelihood function:\n$$L(A; N) = P(N|\\mu(A)) = \\frac{(\\mu(A))^N \\exp(-\\mu(A))}{N!} = \\frac{(As)^N \\exp(-As)}{N!}$$\nThe log-likelihood, $\\mathcal{L}(A;N) = \\ln L(A;N)$, is:\n$$\\mathcal{L}(A; N) = N \\ln(As) - As - \\ln(N!) = N \\ln(A) + N \\ln(s) - As - \\ln(N!)$$\nThe Fisher information $I(A)$ for a single parameter $A$ is defined as the negative expectation of the second derivative of the log-likelihood with respect to the parameter:\n$$I(A) = - E\\left[\\frac{\\partial^2 \\mathcal{L}}{\\partial A^2}\\right]$$\nwhere the expectation $E[\\cdot]$ is taken over the distribution of the data $N$, i.e., $N \\sim \\text{Poisson}(\\mu(A))$.\n\nFirst, we compute the derivatives of the log-likelihood with respect to $A$:\nThe first derivative is:\n$$\\frac{\\partial \\mathcal{L}}{\\partial A} = \\frac{N}{A} - s$$\nThe second derivative is:\n$$\\frac{\\partial^2 \\mathcal{L}}{\\partial A^2} = -\\frac{N}{A^2}$$\nNow, we take the expectation of the second derivative. The expectation of the observable $N$ is the mean of the Poisson distribution, $E[N] = \\mu(A) = As$.\n$$I(A) = - E\\left[-\\frac{N}{A^2}\\right] = \\frac{E[N]}{A^2} = \\frac{As}{A^2} = \\frac{s}{A}$$\nThe Fisher information forecast for the variance of $A$, $\\sigma_{\\text{forecast}}^2(A)$, is given by the inverse of the Fisher information evaluated at the fiducial value $A_0$. This corresponds to the Cramér-Rao lower bound.\n$$\\sigma_{\\text{forecast}}^2(A) = [I(A_0)]^{-1} = \\left(\\frac{s}{A_0}\\right)^{-1} = \\frac{A_0}{s}$$\n\n**Task 2: Exact Posterior Variance for $N=0$**\n\nFor the realized outcome $N=0$, the likelihood function becomes:\n$$L(A; N=0) = \\frac{(As)^0 \\exp(-As)}{0!} = \\exp(-As)$$\nAccording to Bayes' theorem, the posterior probability distribution $p(A|N=0)$ is proportional to the product of the likelihood and the prior $p(A)$:\n$$p(A|N=0) \\propto L(A; N=0) \\times p(A)$$\nWith the flat prior $p(A) \\propto 1$ for $A \\ge 0$, the unnormalized posterior is:\n$$p(A|N=0) \\propto \\exp(-As) \\quad \\text{for } A \\ge 0$$\nTo find the normalized posterior, we must find the normalization constant $C$ such that the integral over the allowed parameter range is unity:\n$$\\int_0^{\\infty} C \\exp(-As) dA = 1$$\n$$C \\left[-\\frac{1}{s} \\exp(-As)\\right]_0^{\\infty} = C \\left(0 - \\left(-\\frac{1}{s}\\right)\\right) = \\frac{C}{s} = 1$$\nThis gives $C=s$. The normalized posterior distribution for $A$ is therefore:\n$$p(A|N=0) = s \\exp(-As) \\quad \\text{for } A \\ge 0$$\nThis is the probability density function of an exponential distribution with rate parameter $\\lambda = s$.\n\nThe variance of an exponential distribution with rate parameter $\\lambda$ is $\\frac{1}{\\lambda^2}$. Thus, the exact posterior variance of $A$ is:\n$$\\sigma_{\\text{posterior}}^2(A) = \\text{Var}(A|N=0) = \\frac{1}{s^2}$$\n\n**Task 3: Ratio of Variances**\n\nThe ratio $R$ of the exact posterior variance to the Fisher forecast variance is:\n$$R = \\frac{\\sigma_{\\text{posterior}}^2(A)}{\\sigma_{\\text{forecast}}^2(A)} = \\frac{1/s^2}{A_0/s} = \\frac{1}{s^2} \\cdot \\frac{s}{A_0} = \\frac{1}{A_0 s}$$\n\n**Task 4: Epistemic Reasons for Discrepancy**\n\nThe Fisher forecast underestimates the uncertainty ($R > 1$, since $A_0 s = \\mu(A_0) > 0$ is expected to be small for a rare process, often $\\mu(A_0) \\ll 1$) for two primary, interrelated reasons:\n\n1.  **Failure of the Gaussian Approximation:** The Fisher information formalism is fundamentally a local approximation that assumes the likelihood function is well-approximated by a Gaussian distribution around its peak. The variance is then estimated from the curvature (the second derivative) at this peak. For a Poisson process, this Gaussian approximation is valid only in the high-count limit (large $\\mu(A)$). The problem specifies a fiducial value $A_0$ that is small, corresponding to a low expected count $\\mu(A_0) = A_0 s$. In the realized case of $N=0$, the likelihood $L(A) = \\exp(-As)$ is a one-sided exponential function. This is maximally non-Gaussian. Its log-likelihood $\\mathcal{L}(A) = -As$ is linear, meaning its second derivative is zero. The Fisher information calculation circumvents this by taking an expectation over $N$, but this does not change the fact that the underlying likelihood for the actual observation $N=0$ is not \"Gaussian-like\" in any way.\n\n2.  **Boundary Effects and Asymmetry:** The parameter $A$ is physically constrained to be non-negative ($A \\ge 0$). The Fisher forecast is calculated at a point $A_0 > 0$ and is intrinsically local; it is ignorant of the hard boundary at $A=0$. However, the true posterior distribution for the $N=0$ case, $p(A|N=0) = s \\exp(-As)$, is an exponential distribution that has its mode (peak) at the boundary $A=0$. This distribution is highly asymmetric and is entirely contained in the region $A \\ge 0$. Describing the uncertainty of such a one-sided distribution with a single variance parameter derived from a symmetric Gaussian approximation is fundamentally flawed. The Fisher forecast implicitly assumes the posterior will be symmetric around the true value, while the actual posterior piles up against the physical boundary, leading to a long tail for positive $A$ and thus a much larger true variance ($1/s^2$) than what the local curvature at $A_0$ would suggest ($A_0/s$).\n\nIn essence, the Fisher method forecasts the uncertainty for a hypothetical ensemble of experiments centered on $A_0$, assuming a well-behaved, symmetric likelihood. The exact calculation confronts the reality of a specific, information-poor outcome ($N=0$) where the posterior is skewed against a physical boundary, revealing a much larger state of uncertainty.",
            "answer": "$$\\boxed{\\frac{1}{A_0 s}}$$"
        }
    ]
}