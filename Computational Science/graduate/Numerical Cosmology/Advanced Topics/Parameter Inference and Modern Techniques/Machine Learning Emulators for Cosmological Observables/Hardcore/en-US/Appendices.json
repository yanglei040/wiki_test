{
    "hands_on_practices": [
        {
            "introduction": "The first step in building an emulator is selecting the right tool for the job. This exercise challenges you to compare two workhorse methods: Polynomial Chaos Expansions (PCE) and Gaussian Processes (GP). By deriving the minimal sample size for a PCE and contrasting it with a GP, you will develop intuition for how model choice impacts sample complexity—especially the 'curse of dimensionality' with dimension $d$ and polynomial order $p$—computational cost, and the ability to quantify uncertainty .",
            "id": "3478376",
            "problem": "You are building a machine learning emulator for a cosmological observable $y(\\boldsymbol{\\theta})$ (for example, the nonlinear matter power spectrum at fixed wavenumbers) defined on a bounded parameter domain $\\Theta \\subset \\mathbb{R}^{d}$, where $\\boldsymbol{\\theta} \\in \\Theta$ encodes $d$ cosmological parameters. You decide to fit a Polynomial Chaos Expansion (PCE) surrogate of total order $p$ using a non-intrusive linear regression on $N$ model evaluations (high-fidelity simulations) at distinct design points in $\\Theta$. Assume the input measure on $\\Theta$ is absolutely continuous and bounded so that an orthonormal basis of multivariate polynomials $\\{\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\theta})\\}$ indexed by multi-indices $\\boldsymbol{\\alpha} \\in \\mathbb{N}_{0}^{d}$ exists, and you truncate the expansion to all multi-indices with total degree $\\|\\boldsymbol{\\alpha}\\|_{1} \\le p$. Further assume noiseless evaluations, and that the design is such that the regression design matrix has full column rank whenever algebraically possible.\n\nStarting from the definition of the truncated total-order polynomial space and the linear-algebraic requirement for uniquely determining all coefficients in the truncated PCE via regression, derive the exact minimal number of simulations $N_{\\min}(d,p)$ required to uniquely identify all expansion coefficients in terms of $d$ and $p$.\n\nThen, still grounded in first principles, discuss the key trade-offs in sample complexity and computational cost when choosing this PCE design versus a Gaussian Process (GP) emulator with a stationary kernel in $d$ dimensions, including how each approach scales with $d$ and $p$, and implications for uncertainty quantification. Your discussion should be qualitative and justified from core definitions and widely used properties; do not invoke any specialized or pre-derived sampling formulas.\n\nProvide your final answer for $N_{\\min}(d,p)$ as a single closed-form expression. No numerical evaluation is required. Do not include units. Do not round.",
            "solution": "The problem statement is critically evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Cosmological observable: $y(\\boldsymbol{\\theta})$\n- Parameter domain: $\\boldsymbol{\\theta} \\in \\Theta \\subset \\mathbb{R}^{d}$\n- Emulator type: Polynomial Chaos Expansion (PCE) of total order $p$.\n- Fitting method: Non-intrusive linear regression on $N$ model evaluations.\n- Design points: $N$ distinct points in $\\Theta$.\n- Input measure: Absolutely continuous and bounded.\n- Polynomial basis: Orthonormal basis $\\{\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\theta})\\}$ with $\\boldsymbol{\\alpha} \\in \\mathbb{N}_{0}^{d}$.\n- Truncation rule: Total degree $\\|\\boldsymbol{\\alpha}\\|_{1} \\le p$.\n- Evaluations: Noiseless.\n- Design matrix: Assumed to have full column rank whenever algebraically possible.\n- First objective: Derive the exact minimal number of simulations $N_{\\min}(d,p)$.\n- Second objective: Discuss trade-offs in sample complexity and computational cost for PCE versus a Gaussian Process (GP) emulator, focusing on scaling with $d$ and $p$, and implications for uncertainty quantification.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n- **Scientifically Grounded**: The problem is well-grounded in the fields of numerical cosmology, uncertainty quantification, and machine learning. Polynomial Chaos Expansions and Gaussian Processes are standard, state-of-the-art techniques for emulating complex computer models. The mathematical formulation is correct and standard.\n- **Well-Posed**: The problem is well-posed. The first part asks for the derivation of a specific quantity, $N_{\\min}(d,p)$, under clear and sufficient assumptions. The condition that the design matrix has full column rank is the key to making the problem of finding a minimal number of points solvable. The second part requests a qualitative but reasoned discussion based on established principles, which is a well-defined task.\n- **Objective**: The problem is stated in precise, objective, and technical language, free from ambiguity or subjective claims.\n- **Completeness and Consistency**: The problem provides all necessary information and definitions to proceed. The assumptions (e.g., total-order truncation, linear regression, full-rank matrix) are self-consistent and sufficient for the derivation.\n- **Feasibility and Realism**: The scenario described is a standard, albeit idealized, setup for constructing surrogate models. It is a fundamental problem in the field.\n\nThe problem statement exhibits none of the flaws listed in the validation checklist. It is scientifically sound, well-posed, objective, and formally specified.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Derivation of $N_{\\min}(d,p)$\n\nThe Polynomial Chaos Expansion (PCE) surrogate model, $\\hat{y}(\\boldsymbol{\\theta})$, approximates the true model output $y(\\boldsymbol{\\theta})$. Given the specified truncation scheme, the expansion includes all multivariate polynomials $\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\theta})$ where the multi-index $\\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2, \\ldots, \\alpha_d) \\in \\mathbb{N}_0^d$ satisfies the condition that its $L_1$-norm (total degree) is at most $p$:\n$$ \\hat{y}(\\boldsymbol{\\theta}) = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}} c_{\\boldsymbol{\\alpha}} \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\theta}) \\quad \\text{where} \\quad \\mathcal{A} = \\left\\{ \\boldsymbol{\\alpha} \\in \\mathbb{N}_0^d : \\|\\boldsymbol{\\alpha}\\|_1 = \\sum_{i=1}^d \\alpha_i \\le p \\right\\} $$\nThe unknown quantities to be determined are the coefficients $\\{c_{\\boldsymbol{\\alpha}}\\}_{\\boldsymbol{\\alpha} \\in \\mathcal{A}}$. To find the minimal number of simulations required to uniquely determine these coefficients, we must first find the total number of such coefficients. Let this number be $K = |\\mathcal{A}|$.\n\nThe problem of finding $K$ is a combinatorial one: counting the number of non-negative integer solutions to the inequality $\\alpha_1 + \\alpha_2 + \\ldots + \\alpha_d \\le p$. By introducing a non-negative slack variable $s$, we can convert this inequality into an equation:\n$$ \\alpha_1 + \\alpha_2 + \\ldots + \\alpha_d + s = p $$\nThis is a classic \"stars and bars\" problem. We are looking for the number of ways to partition $p$ (the \"stars\") into $d+1$ non-negative integer bins (the variables $\\alpha_1, \\ldots, \\alpha_d, s$). This is equivalent to arranging $p$ stars and $(d+1)-1 = d$ bars. The total number of arrangements, and thus the number of solutions, is given by the binomial coefficient:\n$$ K = \\binom{p + (d+1) - 1}{(d+1) - 1} = \\binom{p+d}{d} $$\nSo, there are $K = \\binom{p+d}{d}$ coefficients to be determined.\n\nThe coefficients are found via non-intrusive linear regression using $N$ simulation runs. We have $N$ input-output pairs $(\\boldsymbol{\\theta}_j, y_j)$ for $j=1, \\ldots, N$. This sets up a system of $N$ linear equations:\n$$ y_j = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}} c_{\\boldsymbol{\\alpha}} \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\theta}_j), \\quad j=1, \\ldots, N $$\nThis can be written in matrix form as $\\mathbf{y} = \\mathbf{\\Psi c}$, where:\n- $\\mathbf{y}$ is an $N \\times 1$ column vector of the observed simulation outputs.\n- $\\mathbf{c}$ is a $K \\times 1$ column vector of the unknown PCE coefficients.\n- $\\mathbf{\\Psi}$ is the $N \\times K$ design matrix, with entries $\\Psi_{j, \\boldsymbol{\\alpha}} = \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\theta}_j)$.\n\nFor the vector of coefficients $\\mathbf{c}$ to be \"uniquely identified\", the linear system $\\mathbf{y} = \\mathbf{\\Psi c}$ must have a unique solution. A fundamental result from linear algebra states that for an $N \\times K$ system, a unique solution for $\\mathbf{c}$ exists if and only if the matrix $\\mathbf{\\Psi}$ has full column rank, which requires that the number of rows is greater than or equal to the number of columns, i.e., $N \\ge K$. If $N < K$, the system is underdetermined, and there are infinitely many solutions for $\\mathbf{c}$.\n\nThe problem states to assume the design (the choice of points $\\boldsymbol{\\theta}_j$) is such that $\\mathbf{\\Psi}$ has full column rank whenever it is algebraically possible. The minimal value of $N$ for which full column rank is possible is $N=K$. Therefore, the minimal number of simulations required to uniquely determine all $K$ coefficients is:\n$$ N_{\\min}(d, p) = K = \\binom{p+d}{d} $$\n\n### Discussion of PCE vs. GP Emulator Trade-offs\n\nHerein, we discuss the trade-offs between the Polynomial Chaos Expansion (as designed above) and a Gaussian Process emulator.\n\n**Sample Complexity and Scaling:**\n\n- **PCE:** The sample complexity is rigidly determined by the model structure. As derived, the minimum number of samples is $N_{\\min} = \\binom{p+d}{d}$. For a fixed polynomial order $p$, this number grows polynomially with the dimension $d$ as $N_{\\min} \\propto d^p$ for large $d$. For a fixed dimension $d$, it grows polynomially with the order $p$ as $N_{\\min} \\propto p^d$ for large $p$. This rapid growth, often termed the \"curse of dimensionality,\" makes total-order PCEs impractical for problems with even moderate dimension ($d \\gtrsim 10$) and order ($p \\gtrsim 3$), as the required number of high-fidelity simulations becomes prohibitively large.\n- **GP:** A GP does not have a hard lower bound on the number of samples $N$ based on its structural parameters. A GP model can be constructed with any number of points $N > 0$. The quality of the emulator (e.g., its accuracy) improves as $N$ increases. However, GPs also suffer from the curse of dimensionality, as the parameter space volume grows exponentially with $d$. To maintain a given sampling density, $N$ must grow exponentially, meaning that for a fixed $N$ in high dimensions, the data points become very sparse. This can degrade the performance of standard stationary kernels (e.g., squared exponential), which may struggle to learn correlations between distant points.\n\n**Computational Cost:**\n\n- **PCE:** The primary computational cost in fitting the PCE via linear regression is solving the normal equations $(\\mathbf{\\Psi}^T \\mathbf{\\Psi}) \\mathbf{c} = \\mathbf{\\Psi}^T \\mathbf{y}$. This involves the inversion of the $K \\times K$ matrix $\\mathbf{\\Psi}^T \\mathbf{\\Psi}$, which scales as $O(K^3)$, where $K = \\binom{p+d}{d}$. Thus, the training cost scales severely with both dimension $d$ and order $p$. However, once the coefficients $\\mathbf{c}$ are computed, making a new prediction is extremely fast, involving only the evaluation of a polynomial sum of $K$ terms, which scales as $O(K)$.\n- **GP:** The primary computational cost in training a standard GP is the inversion of the $N \\times N$ covariance matrix of the training data, which scales as $O(N^3)$. This cost depends on the number of training points $N$, not directly on the dimension $d$. For large datasets ($N \\gg 1000$), this becomes the main bottleneck. Prediction at a new point requires matrix-vector operations, scaling as $O(N)$ for the mean and $O(N^2)$ for the variance. Thus, GP prediction is computationally more expensive than PCE prediction.\n\n**Implications for Uncertainty Quantification (UQ):**\n\n- **PCE:** In the non-intrusive regression setting described, the PCE provides a deterministic point estimate $\\hat{y}(\\boldsymbol{\\theta})$. It does not inherently provide a measure of its own predictive (epistemic) uncertainty. To quantify the confidence in the PCE model itself, one would need to use external methods like bootstrapping or cross-validation. However, PCEs excel at propagating *input* (aleatoric) uncertainty. If the input parameters $\\boldsymbol{\\theta}$ are described by a probability distribution, the orthonormal property of the basis polynomials allows for the analytical computation of the statistical moments (e.g., mean, variance) of the output $\\hat{y}$. The variance is simply the sum of the squares of the coefficients (excluding the constant term), $\\text{Var}[\\hat{y}] = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}, \\boldsymbol{\\alpha} \\neq \\mathbf{0}} c_{\\boldsymbol{\\alpha}}^2$.\n- **GP:** UQ is a native and principal feature of the GP framework. A GP is a probabilistic model that returns a full posterior predictive distribution (a Gaussian) for any new input point $\\boldsymbol{\\theta}_{\\text{new}}$. This distribution is characterized by a predictive mean (the emulation) and a predictive variance. This variance is a measure of the epistemic uncertainty, which naturally increases in regions of the parameter space that are sparsely populated by training data. This makes GPs particularly powerful for applications like active learning or Bayesian optimization, where principled exploration of the parameter space is required.\n\nIn summary, the choice between PCE and GP involves a trade-off between the rigid, global structure of PCE with its severe sample requirements in high dimensions but fast predictions, and the flexible, non-parametric nature of GPs with their built-in UQ but high computational cost for large $N$.",
            "answer": "$$\\boxed{\\binom{p+d}{d}}$$"
        },
        {
            "introduction": "A powerful emulator is useless without a credible estimate of its accuracy. This practice moves beyond textbook validation scenarios to a more realistic setting, where training data from simulations are clustered and do not perfectly represent the target parameter space . You will learn to design a statistically sound validation protocol that accounts for data dependencies and distribution mismatch, a critical skill for avoiding optimistic performance estimates and building reliable scientific tools.",
            "id": "3478357",
            "problem": "You are building a machine learning emulator for the matter power spectrum $P(k)$ conditioned on a cosmological parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^6$ with components $\\boldsymbol{\\theta} = (\\Omega_{\\mathrm{m}}, \\Omega_{\\mathrm{b}}, h, n_{\\mathrm{s}}, \\sigma_8, w_0)$. The simulator provides outputs $\\mathbf{y}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^M$ on a fixed wavenumber grid $k$. The training design consists of $N = 120$ simulations arranged in $C = 10$ clusters in parameter space due to a two-stage design: $8$ dense clusters around a fiducial region and $2$ exploratory clusters at extremes. Each cluster contains $12$ points. Distances are measured in the Mahalanobis metric with respect to a positive-definite matrix $\\mathbf{\\Sigma}^{-1}$ (for example, a Fisher-information-informed metric), so that the distance between two parameter points is\n$$\nd_{\\mathrm{M}}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\sqrt{(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}')^\\top \\mathbf{\\Sigma}^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}')}.\n$$\nWithin each cluster, all pairwise distances satisfy $d_{\\mathrm{M}}(\\boldsymbol{\\theta}_i, \\boldsymbol{\\theta}_j) \\le \\ell/2$ for some correlation length $\\ell$, and between clusters, $d_{\\mathrm{M}}(\\boldsymbol{\\theta}_i, \\boldsymbol{\\theta}_j) \\ge \\ell$. You may assume that the simulator outputs are smooth in $\\boldsymbol{\\theta}$ on scales $\\gtrsim \\ell$ so that within-cluster outputs are strongly correlated.\n\nYour goal is to estimate and compare the generalization performance of emulators (for example, a Gaussian Process (GP) or a Neural Network (NN)) with respect to a target prior density $\\pi(\\boldsymbol{\\theta})$ that is not equal to the design density $q(\\boldsymbol{\\theta})$ that generated the simulator inputs. Let the target risk be\n$$\nR = \\mathbb{E}_{\\boldsymbol{\\theta} \\sim \\pi}\\left[ L\\!\\left(\\widehat{\\mathbf{y}}(\\boldsymbol{\\theta}), \\mathbf{y}(\\boldsymbol{\\theta})\\right) \\right],\n$$\nfor a fixed nonnegative loss $L(\\cdot,\\cdot)$, where $\\widehat{\\mathbf{y}}(\\boldsymbol{\\theta})$ is the emulator prediction. You want an assessment protocol (train/validation/test splits and a $K$-fold cross-validation procedure) that yields a low-bias estimate of $R$ and supports reliable hyperparameter selection, while appropriately accounting for the clustered design and for $\\pi \\neq q$.\n\nWhich of the following assessment designs are statistically sound for this purpose? Select all that apply.\n\n- A. Randomly split the $N = 120$ points into $80$ train, $20$ validation, and $20$ test points at the point level (ignoring cluster membership). Perform standard $K = 10$ pointwise $K$-fold cross-validation on the $100$ non-test points to choose hyperparameters and report the held-out test loss averaged equally over the $20$ test points as the estimate of $R$.\n\n- B. Reserve $T = 2$ entire clusters as the test set a priori, selected to approximate high probability mass under $\\pi(\\boldsymbol{\\theta})$. On the remaining $C - T = 8$ clusters, perform group $K$-fold cross-validation where folds are formed by whole clusters (that is, no cluster is split across folds). Use importance weights $w_i \\propto \\pi(\\boldsymbol{\\theta}_i)/q(\\boldsymbol{\\theta}_i)$ within validation folds to select hyperparameters by minimizing the weighted validation loss. Retrain on all $C - T$ training clusters with the chosen hyperparameters, and report importance-weighted test loss on the $2$ held-out clusters as the estimate of $R$.\n\n- C. Perform leave-one-cluster-out cross-validation over all $C = 10$ clusters, using the cross-validated loss (averaged equally over clusters) both to select hyperparameters and to report the final performance. Do not create a separate test set, and do not use importance weights.\n\n- D. Construct a distance-blocked $K$-fold cross-validation by assigning points to folds so that for any validation point $\\boldsymbol{\\theta}_v$ in a fold, all training points $\\boldsymbol{\\theta}_t$ in the corresponding training split satisfy $d_{\\mathrm{M}}(\\boldsymbol{\\theta}_v, \\boldsymbol{\\theta}_t) \\ge \\ell$. Choose $K$ so that each fold contains contiguous regions in parameter space rather than fragments of many clusters. Use importance weights $w_i \\propto \\pi(\\boldsymbol{\\theta}_i)/q(\\boldsymbol{\\theta}_i)$ within validation folds for hyperparameter selection. For final testing, hold out $2$ spatially contiguous clusters or regions, disjoint from all training/validation points by $d_{\\mathrm{M}} \\ge \\ell$, and report the importance-weighted loss on this test set.\n\n- E. Stratify the $N = 120$ points into bins of $\\sigma_8$ and $w_0$, ignoring other parameters and cluster labels. Perform pointwise $K = 10$ stratified $K$-fold cross-validation within bins, choose hyperparameters by unweighted validation loss, and use a random $20$-point test set drawn pointwise to report unweighted test loss.\n\n- F. Oversample validation points from the $2$ exploratory clusters so that each fold has an equal number of points from all clusters. Choose hyperparameters by averaging unweighted loss across folds, retrain on all $N = 120$ points with the chosen hyperparameters, and report the unweighted cross-validated loss as the final estimate, without a separate test set.\n\nAnswer choices may be multiple. Justify your selections by appealing to first principles of risk estimation, exchangeability, and dependence induced by clustered designs, not by heuristic rules.",
            "solution": "The problem requires the design of a statistically sound assessment protocol for a machine learning emulator. The protocol must yield a low-bias estimate of the target risk, $R = \\mathbb{E}_{\\boldsymbol{\\theta} \\sim \\pi}\\left[ L\\!\\left(\\widehat{\\mathbf{y}}(\\boldsymbol{\\theta}), \\mathbf{y}(\\boldsymbol{\\theta})\\right) \\right]$, and enable reliable hyperparameter selection. There are three primary statistical challenges that a sound protocol must address:\n\n1.  **Dependence in the Data**: The training points are not independent and identically distributed (i.i.d.). They are organized into $C=10$ distinct clusters. Within each cluster, points are close ($d_{\\mathrm{M}} \\le \\ell/2$), and the simulator output $\\mathbf{y}(\\boldsymbol{\\theta})$ is smooth, implying strong correlation between the outputs of nearby points. Between clusters, points are well-separated ($d_{\\mathrm{M}} \\ge \\ell$). A naive pointwise random split into training and validation/test sets would place highly correlated points on both sides of the split. This allows the model to achieve artificially low error on the validation/test set by simply \"memorizing\" or interpolating between very close training examples, leading to an optimistically biased (i.e., underestimated) assessment of the true generalization error. The fundamental assumption of exchangeability is violated at the point level but holds at the cluster level. Therefore, any splits of the data (for cross-validation or for a final test set) must be performed at the level of entire clusters or, more generally, by spatial blocks in parameter space that respect the correlation structure. This ensures that the validation/test data is truly \"unseen\" by the model during training.\n\n2.  **Distribution Mismatch**: The training data points $\\{\\boldsymbol{\\theta}_i\\}$ were generated from a design density $q(\\boldsymbol{\\theta})$, but the target risk $R$ is defined with respect to a different target density $\\pi(\\boldsymbol{\\theta})$. An unweighted empirical average of the loss over a set of points drawn from $q(\\boldsymbol{\\theta})$ provides an estimate of $\\mathbb{E}_{\\boldsymbol{\\theta} \\sim q}[L]$, not $\\mathbb{E}_{\\boldsymbol{\\theta} \\sim \\pi}[L]$. To obtain a low-bias estimate of the target risk $R$, one must use importance sampling. The loss for each point $\\boldsymbol{\\theta}_i$ in the validation or test set must be weighted by the importance weight $w_i \\propto \\pi(\\boldsymbol{\\theta}_i)/q(\\boldsymbol{\\theta}_i)$. The importance-weighted risk estimate is $\\hat{R}_{\\mathrm{IW}} = \\frac{\\sum_i w_i L_i}{\\sum_i w_i}$, which is an unbiased estimator of $R$ if the points are drawn from $q(\\boldsymbol{\\theta})$.\n\n3.  **Unbiased Performance Estimation**: Hyperparameter selection involves training and evaluating a model on validation data multiple times. The selected hyperparameters are therefore adapted to the validation data. If the final performance is reported using this same data (e.g., reporting the cross-validation score itself), the estimate will be optimistically biased. To obtain a truly unbiased estimate of the generalization performance of the *final, chosen model*, it is essential to evaluate it on a held-out test set that was not used in any way during the training or hyperparameter selection process.\n\nBased on these principles, a statistically sound protocol must:\n- Use group/blocked splits for all cross-validation and test set creation.\n- Use importance weighting on both validation and test sets.\n- Reserve a separate, held-out test set for final performance evaluation.\n\nWe now evaluate each option against these criteria.\n\n- **A. Randomly split the $N = $ $120$ points into $80$ train, $20$ validation, and $20$ test points at the point level (ignoring cluster membership). Perform standard $K = $ $10$ pointwise $K$-fold cross-validation on the $100$ non-test points to choose hyperparameters and report the held-out test loss averaged equally over the $20$ test points as the estimate of $R$.**\n\nThis protocol fails on two critical accounts. First, by performing pointwise splits (\"ignoring cluster membership\"), it guarantees that the validation and test sets will contain points that are highly correlated with the training set. This leads to an underestimation of the true generalization error. Second, by using an unweighted average loss (\"averaged equally\"), it estimates the risk with respect to the design density $q(\\boldsymbol{\\theta})$, not the target density $\\pi(\\boldsymbol{\\theta})$, resulting in a biased estimate of $R$.\n\n**Verdict:** Incorrect.\n\n- **B. Reserve $T = $ $2$ entire clusters as the test set a priori, selected to approximate high probability mass under $\\pi(\\boldsymbol{\\theta})$. On the remaining $C - T = $ $8$ clusters, perform group $K$-fold cross-validation where folds are formed by whole clusters (that is, no cluster is split across folds). Use importance weights $w_i \\propto \\pi(\\boldsymbol{\\theta}_i)/q(\\boldsymbol{\\theta}_i)$ within validation folds to select hyperparameters by minimizing the weighted validation loss. Retrain on all $C - T$ training clusters with the chosen hyperparameters, and report importance-weighted test loss on the $2$ held-out clusters as the estimate of $R$.**\n\nThis protocol correctly addresses all three challenges.\n1.  **Dependence:** It uses cluster-based splits for both the test set (\"reserve $T=2$ entire clusters\") and for cross-validation (\"group $K$-fold cross-validation where folds are formed by whole clusters\"). This respects the data's correlation structure.\n2.  **Distribution Mismatch:** It correctly employs importance weights (\"use importance weights $w_i \\propto \\pi(\\boldsymbol{\\theta}_i)/q(\\boldsymbol{\\theta}_i)$\") for both hyperparameter selection and final risk estimation.\n3.  **Unbiased Estimation:** It uses a separate, held-out test set that is untouched during hyperparameter tuning. The overall procedure (split test, CV for hyperparameters, retrain, evaluate on test) is standard best practice.\n\n**Verdict:** Correct.\n\n- **C. Perform leave-one-cluster-out cross-validation over all $C = $ $10$ clusters, using the cross-validated loss (averaged equally over clusters) both to select hyperparameters and to report the final performance. Do not create a separate test set, and do not use importance weights.**\n\nThis protocol has two major flaws. First, it uses the same cross-validation procedure to both select hyperparameters and report final performance. This introduces an optimistic bias, as the hyperparameters are chosen to optimize performance on this very metric. An independent, held-out test set is required for an unbiased final estimate. Second, it does not use importance weights, so it estimates the wrong quantity ($\\mathbb{E}_q[L]$ instead of $\\mathbb{E}_{\\pi}[L]$). While leave-one-cluster-out cross-validation correctly handles the data dependence, the other flaws make the procedure unsound for the stated goals.\n\n**Verdict:** Incorrect.\n\n- **D. Construct a distance-blocked $K$-fold cross-validation by assigning points to folds so that for any validation point $\\boldsymbol{\\theta}_v$ in a fold, all training points $\\boldsymbol{\\theta}_t$ in the corresponding training split satisfy $d_{\\mathrm{M}}(\\boldsymbol{\\theta}_v, \\boldsymbol{\\theta}_t) \\ge \\ell$. Choose $K$ so that each fold contains contiguous regions in parameter space rather than fragments of many clusters. Use importance weights $w_i \\propto \\pi(\\boldsymbol{\\theta}_i)/q(\\boldsymbol{\\theta}_i)$ within validation folds for hyperparameter selection. For final testing, hold out $2$ spatially contiguous clusters or regions, disjoint from all training/validation points by $d_{\\mathrm{M}} \\ge \\ell$, and report the importance-weighted loss on this test set.**\n\nThis protocol is a more general and rigorous implementation of the same principles as in option B.\n1.  **Dependence:** It directly uses the distance metric to enforce separation ($d_{\\mathrm{M}} \\ge \\ell$) between training and validation/test sets. This is the first-principles approach to handling spatially correlated data, of which cluster-based splitting is a specific instance.\n2.  **Distribution Mismatch:** It correctly uses importance weights for both hyperparameter selection and final evaluation.\n3.  **Unbiased Estimation:** It correctly uses a held-out test set (\"hold out $2$ spatially contiguous clusters or regions\") that maintains the required spatial separation from the training data.\n\nThis design is statistically sound and robust.\n\n**Verdict:** Correct.\n\n- **E. Stratify the $N = $ $120$ points into bins of $\\sigma_8$ and $w_0$, ignoring other parameters and cluster labels. Perform pointwise $K = $ $10$ stratified $K$-fold cross-validation within bins, choose hyperparameters by unweighted validation loss, and use a random $20$-point test set drawn pointwise to report unweighted test loss.**\n\nThis protocol is flawed. Stratification on a subset of parameters does not address the primary issue of correlation induced by the clustering in the full $6$-dimensional parameter space. By ignoring cluster labels and performing pointwise splits, it suffers from the same optimistic bias as option A. Furthermore, it fails to use importance weights, leading to a biased estimate of the target risk $R$.\n\n**Verdict:** Incorrect.\n\n- **F. Oversample validation points from the $2$ exploratory clusters so that each fold has an equal number of points from all clusters. Choose hyperparameters by averaging unweighted loss across folds, retrain on all $N = $ $120$ points with the chosen hyperparameters, and report the unweighted cross-validated loss as the final estimate, without a separate test set.**\n\nThis protocol is unsound on multiple grounds. First, it does not use a separate test set, reporting the CV loss as the final estimate, which is biased. Second, the proposed splitting scheme (\"each fold has an equal number of points from all clusters\") necessitates breaking up clusters, which violates the principle of keeping correlated groups of data intact during splits. Third, it uses unweighted loss, failing to address the distribution mismatch. The oversampling is an ad-hoc procedure, unlike the principled approach of importance weighting.\n\n**Verdict:** Incorrect.\n\nIn summary, options B and D both describe statistically sound methodologies that correctly address the data dependencies, the distribution shift, and the need for unbiased performance estimation.",
            "answer": "$$\\boxed{BD}$$"
        },
        {
            "introduction": "In computational cosmology, budget is not just money but wall-clock time, and high-fidelity simulations are the most expensive currency. This hands-on exercise introduces the powerful concept of multi-fidelity emulation, where we learn from both cheap, low-resolution simulations and expensive, high-resolution ones . By implementing a co-kriging model, you will solve a real-world optimization problem: how to allocate your computational budget to achieve a target accuracy with minimal total cost.",
            "id": "3478372",
            "problem": "You are tasked with designing and solving a multi-fidelity allocation problem for an emulator of the matter power spectrum $P(k)$ in numerical cosmology. The emulator combines low-resolution Particle Mesh (PM) simulations and high-resolution $N$-body simulations through a co-kriging model, following the auto-regressive formulation used in multi-fidelity Gaussian Process (GP) co-kriging. The goal is to meet a target predictive error constraint on the high-fidelity $P(k)$ at a single target wave number $k_\\star$ while minimizing total runtime. You must write a complete, runnable program that solves the allocation problem for a specified test suite and prints the required outputs in the exact format described below.\n\nModeling assumptions and fundamental base:\n- The multi-fidelity auto-regressive model is given by the Kennedy–O’Hagan structure $f_H(\\theta) = \\rho\\, f_L(\\theta) + \\delta(\\theta)$ at a single design point $\\theta_\\star$ aligned with $k_\\star$, where $f_H$ denotes the latent high-fidelity response of $P(k)$ at $k_\\star$, $f_L$ denotes the latent low-fidelity response, $\\rho$ is the cross-fidelity scaling factor, and $\\delta$ is the discrepancy process.\n- At the single location $\\theta_\\star$, treat $f_L$ and $\\delta$ as independent Gaussian random variables under a Gaussian Process prior restricted to the point: $f_L \\sim \\mathcal{N}(0, \\tau_L^2)$ and $\\delta \\sim \\mathcal{N}(0, \\tau_\\delta^2)$.\n- Observations are noisy and co-located at $\\theta_\\star$: each low-fidelity PM run returns $y^L_i = f_L + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0, s_L^2)$ and each high-fidelity $N$-body run returns $y^H_j = \\rho\\, f_L + \\delta + \\eta_j$ with $\\eta_j \\sim \\mathcal{N}(0, s_H^2)$. All noises are independent and identically distributed across replicates and independent of $(f_L, \\delta)$.\n- The predictive target is the posterior standard deviation of the latent high-fidelity quantity $z \\equiv \\rho\\, f_L + \\delta$ at $\\theta_\\star$ under the joint linear-Gaussian model, after $n_L$ low-fidelity and $n_H$ high-fidelity runs. You must enforce the constraint that the posterior standard deviation of $z$ is at most the specified tolerance $\\epsilon$.\n- The runtime model is additive: total runtime $T$ is $T = n_L\\, t_L + n_H\\, t_H$, where $t_L$ and $t_H$ are the wall-clock times per PM and $N$-body run, respectively. All runtimes must be reported in $\\mathrm{s}$ (seconds).\n\nDesign goal:\n- For each test case in the suite below, choose nonnegative integers $n_L$ and $n_H$ to minimize $T$ subject to the predictive posterior standard deviation of $z$ being less than or equal to $\\epsilon$.\n\nWhat you must compute:\n- Use a first-principles derivation from linear-Gaussian Bayesian updating, starting with the auto-regressive co-kriging structure, to express the posterior variance of $z$ in terms of $(n_L, n_H)$ and the model hyperparameters $(\\rho, \\tau_L^2, \\tau_\\delta^2, s_L^2, s_H^2)$.\n- Determine feasibility of the constraint for given $(n_L, n_H)$ and derive a correct selection rule for optimal $(n_L, n_H)$ that minimizes $T$.\n- Your program must implement this derivation and optimization in a numerically stable and self-contained manner, without external input or files.\n\nTest suite:\n- For each test case, the parameters are $(\\rho, \\tau_L^2, \\tau_\\delta^2, s_L^2, s_H^2, t_L, t_H, \\epsilon)$ with units as applicable. All quantities in the test suite are dimensionless except $t_L$ and $t_H$, which must be treated in $\\mathrm{s}$.\n    1. Happy path with moderate coupling and tight tolerance: $\\rho = 0.9$, $\\tau_L^2 = 0.25$, $\\tau_\\delta^2 = 0.04$, $s_L^2 = 0.01$, $s_H^2 = 0.0025$, $t_L = 10$ s, $t_H = 200$ s, $\\epsilon = 0.05$.\n    2. High-fidelity expensive, weak coupling: $\\rho = 0.5$, $\\tau_L^2 = 0.36$, $\\tau_\\delta^2 = 0.09$, $s_L^2 = 0.02$, $s_H^2 = 0.01$, $t_L = 20$ s, $t_H = 800$ s, $\\epsilon = 0.15$.\n    3. Loose tolerance, zero-simulation feasible: $\\rho = 0.8$, $\\tau_L^2 = 0.01$, $\\tau_\\delta^2 = 0.005$, $s_L^2 = 0.01$, $s_H^2 = 0.01$, $t_L = 5$ s, $t_H = 100$ s, $\\epsilon = 0.12$.\n    4. Very tight tolerance requiring multiple high-fidelity runs: $\\rho = 0.95$, $\\tau_L^2 = 0.49$, $\\tau_\\delta^2 = 0.01$, $s_L^2 = 0.04$, $s_H^2 = 0.0025$, $t_L = 30$ s, $t_H = 300$ s, $\\epsilon = 0.02$.\n    5. Near-decoupled low fidelity, high-fidelity relatively cheap: $\\rho = 0.1$, $\\tau_L^2 = 1.0$, $\\tau_\\delta^2 = 0.25$, $s_L^2 = 0.1$, $s_H^2 = 0.01$, $t_L = 10$ s, $t_H = 50$ s, $\\epsilon = 0.3$.\n\nFinal output format:\n- For each test case, your program must compute the optimal nonnegative integers $n_L$ and $n_H$ and the minimal total runtime $T$ in $\\mathrm{s}$ rounded to three decimal places.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list of the form $[n_L, n_H, T]$. For example: $[[n_L^{(1)}, n_H^{(1)}, T^{(1)}],[n_L^{(2)}, n_H^{(2)}, T^{(2)}],\\dots]$.\n\nAngle units do not apply. No percentages are used; any fraction-like quantities must appear as decimal numbers.\n\nYour program must be complete, runnable, and self-contained, using only the specified libraries and execution environment. No user input is permitted. Ensure scientific realism by adhering to the linear-Gaussian co-kriging assumptions and consistent units.",
            "solution": "The problem requires finding the optimal allocation of computational resources, specifically the number of low-fidelity Particle Mesh (PM) simulations, $n_L$, and high-fidelity $N$-body simulations, $n_H$, to minimize the total runtime $T = n_L t_L + n_H t_H$, while satisfying a constraint on the predictive uncertainty of the matter power spectrum, $P(k)$, at a specific wave number $k_\\star$.\n\nThe problem has been validated and is determined to be a well-posed, scientifically grounded optimization problem based on the established Kennedy-O'Hagan framework for multi-fidelity emulation. All necessary parameters are provided, and the problem is internally consistent. We can therefore proceed with a solution.\n\nThe solution process involves two main steps:\n1.  Derivation of the posterior variance of the high-fidelity quantity of interest as a function of $n_L$ and $n_H$.\n2.  Formulation and solution of the constrained integer optimization problem for $(n_L, n_H)$.\n\n### Step 1: Derivation of the Posterior Variance\n\nThe model is defined at a single design point $\\theta_\\star$. The latent low-fidelity response $f_L$ and the discrepancy term $\\delta$ are modeled as independent Gaussian random variables with priors:\n$$f_L \\sim \\mathcal{N}(0, \\tau_L^2)$$\n$$\\delta \\sim \\mathcal{N}(0, \\tau_\\delta^2)$$\nThe latent high-fidelity response is $z \\equiv f_H(\\theta_\\star) = \\rho f_L + \\delta$. We seek to find the posterior variance of $z$.\n\nAn observation from a low-fidelity simulation is $y_i^L = f_L + \\epsilon_i$ with noise $\\epsilon_i \\sim \\mathcal{N}(0, s_L^2)$. An observation from a high-fidelity simulation is $y_j^H = z + \\eta_j$ with noise $\\eta_j \\sim \\mathcal{N}(0, s_H^2)$. We have $n_L$ low-fidelity and $n_H$ high-fidelity observations.\n\nThis can be formulated as a Bayesian linear-Gaussian model. Let the state vector be $\\mathbf{x} = [f_L, \\delta]^T$. The prior on $\\mathbf{x}$ is $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_0)$, where the prior covariance is:\n$$\\mathbf{\\Sigma}_0 = \\begin{pmatrix} \\tau_L^2 & 0 \\\\ 0 & \\tau_\\delta^2 \\end{pmatrix}$$\nThe data consists of the sample means of the observations, which are sufficient statistics. For $n_L>0$, $\\bar{y}_L = \\frac{1}{n_L}\\sum_{i=1}^{n_L} y_i^L = f_L + \\bar{\\epsilon}$, where $\\bar{\\epsilon} \\sim \\mathcal{N}(0, s_L^2/n_L)$. For $n_H>0$, $\\bar{y}_H = \\frac{1}{n_H}\\sum_{j=1}^{n_H} y_j^H = \\rho f_L + \\delta + \\bar{\\eta}$, where $\\bar{\\eta} \\sim \\mathcal{N}(0, s_H^2/n_H)$.\n\nThe posterior precision (inverse covariance) matrix $\\mathbf{\\Sigma}_n^{-1}$ for $\\mathbf{x}$ after observing the data is given by the standard update rule $\\mathbf{\\Sigma}_n^{-1} = \\mathbf{\\Sigma}_0^{-1} + \\mathbf{L}$, where $\\mathbf{L}$ is the data precision (log-likelihood Hessian).\nThe prior precision is $\\mathbf{\\Sigma}_0^{-1} = \\text{diag}(1/\\tau_L^2, 1/\\tau_\\delta^2)$.\nThe data precision is the sum of contributions from the low- and high-fidelity data. For $n_L>0$, the observation $\\bar{y}_L$ provides information only on $f_L$. For $n_H>0$, $\\bar{y}_H$ provides information on $\\rho f_L + \\delta$.\nThe full data precision matrix from $n_L$ low-fidelity runs and $n_H$ high-fidelity runs is:\n$$\\mathbf{L} = \\begin{pmatrix} n_L/s_L^2 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} \\rho \\\\ 1 \\end{pmatrix} (s_H^2/n_H)^{-1} \\begin{pmatrix} \\rho & 1 \\end{pmatrix} = \\begin{pmatrix} n_L/s_L^2 + n_H\\rho^2/s_H^2 & n_H\\rho/s_H^2 \\\\ n_H\\rho/s_H^2 & n_H/s_H^2 \\end{pmatrix}$$\nThe posterior precision matrix is thus:\n$$\\mathbf{\\Sigma}_n^{-1} = \\begin{pmatrix} 1/\\tau_L^2 + n_L/s_L^2 + n_H\\rho^2/s_H^2 & n_H\\rho/s_H^2 \\\\ n_H\\rho/s_H^2 & 1/\\tau_\\delta^2 + n_H/s_H^2 \\end{pmatrix}$$\nThe quantity of interest is $z = \\rho f_L + \\delta = \\mathbf{c}^T \\mathbf{x}$ with $\\mathbf{c} = [\\rho, 1]^T$. The posterior variance of $z$ is $V(n_L, n_H) = \\text{Var}(z|\\text{data}) = \\mathbf{c}^T \\mathbf{\\Sigma}_n \\mathbf{c}$.\nA direct but algebraically intensive calculation yields:\n$$V(n_L, n_H) = \\frac{1/\\tau_L^2 + n_L/s_L^2 + \\rho^2/\\tau_\\delta^2}{(1/\\tau_L^2 + n_L/s_L^2)(1/\\tau_\\delta^2 + n_H/s_H^2) + n_H\\rho^2/(s_H^2\\tau_\\delta^2)}$$\nA more insightful form is obtained by expressing the posterior precision (information) of $z$, denoted $I(n_L, n_H) = 1/V(n_L, n_H)$:\n$$I(n_L, n_H) = \\frac{(1/\\tau_L^2 + n_L/s_L^2) / \\tau_\\delta^2}{1/\\tau_L^2 + n_L/s_L^2 + \\rho^2/\\tau_\\delta^2} + \\frac{n_H}{s_H^2}$$\nThis structure shows that the total information on $z$ is a sum of two terms: one term, $n_H/s_H^2$, represents the information gained directly from high-fidelity observations, and the other term represents the combined information from the prior and the low-fidelity data. Let's denote the latter term as $I_L(n_L)$.\n\n### Step 2: Optimization Problem and Solution Strategy\n\nThe problem is to minimize the cost $T(n_L, n_H) = n_L t_L + n_H t_H$ subject to the constraint on the posterior standard deviation, $\\sqrt{V(n_L, n_H)} \\le \\epsilon$. This is equivalent to constraining the information:\n$$I(n_L, n_H) \\ge 1/\\epsilon^2$$\nSubstituting the derived expression for information gives the full constraint on the non-negative integers $(n_L, n_H)$:\n$$I_L(n_L) + \\frac{n_H}{s_H^2} \\ge \\frac{1}{\\epsilon^2}$$\nFor a fixed number of low-fidelity runs $n_L$, we can find the minimum required number of high-fidelity runs, $n_H^*(n_L)$, by rearranging the inequality:\n$$n_H \\ge s_H^2 \\left(\\frac{1}{\\epsilon^2} - I_L(n_L)\\right)$$\nSince $n_H$ must be a non-negative integer, the minimum value is:\n$$n_H^*(n_L) = \\left\\lceil \\max\\left(0, s_H^2 \\left(\\frac{1}{\\epsilon^2} - I_L(n_L)\\right)\\right) \\right\\rceil$$\nThe optimization problem reduces to finding the minimum of a single-variable function over non-negative integers $n_L$:\n$$\\min_{n_L \\ge 0} T(n_L) = n_L t_L + n_H^*(n_L) t_H$$\nThe function $I_L(n_L)$ is monotonically increasing with $n_L$. Consequently, $n_H^*(n_L)$ is a non-increasing step function of $n_L$. The total cost $T(n_L)$ is the sum of a linearly increasing term $n_L t_L$ and a non-increasing step function. A global minimum must exist.\n\nWe can solve this by performing a search over $n_L$.\n1.  First, check the case with zero simulations ($n_L=0, n_H=0$). The variance is the prior variance $V(0,0)=\\rho^2\\tau_L^2+\\tau_\\delta^2$. If $V(0,0) \\le \\epsilon^2$, the optimal solution is $(0, 0, 0)$.\n2.  Otherwise, we establish an initial solution and an upper bound on cost by considering the high-fidelity-only case ($n_L=0$). We compute $n_H^*(0)$ and the corresponding cost $T_{min} = n_H^*(0) t_H$. This is our current best solution: $(0, n_H^*(0), T_{min})$.\n3.  Any solution with $n_L > 0$ can only be better if its total cost is less than $T_{min}$. This implies that $n_L t_L < T_{min}$, which gives a finite search bound for $n_L$: $n_{L,max} = \\lfloor (T_{min} - 1)/t_L \\rfloor$.\n4.  We then iterate $n_L$ from $1$ up to this bound, calculating $T(n_L)$ at each step. If a lower cost is found, we update $T_{min}$ and the optimal $(n_L, n_H)$, and we also update the search bound $n_{L,max}$ based on the new, lower $T_{min}$. This dynamically prunes the search space.\n5.  The final stored $(n_L, n_H, T_{min})$ after the search completes is the optimal solution.\n\nThis algorithm is guaranteed to find the global minimum.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the multi-fidelity allocation problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        # (rho, tau_L^2, tau_delta^2, s_L^2, s_H^2, t_L, t_H, epsilon)\n        (0.9, 0.25, 0.04, 0.01, 0.0025, 10, 200, 0.05),\n        (0.5, 0.36, 0.09, 0.02, 0.01, 20, 800, 0.15),\n        (0.8, 0.01, 0.005, 0.01, 0.01, 5, 100, 0.12),\n        (0.95, 0.49, 0.01, 0.04, 0.0025, 30, 300, 0.02),\n        (0.1, 1.0, 0.25, 0.1, 0.01, 10, 50, 0.3),\n    ]\n\n    results = []\n    for case in test_cases:\n        rho, tau_L2, tau_d2, s_L2, s_H2, t_L, t_H, epsilon = case\n\n        # Inverse variances (precisions)\n        U_L = 1.0 / tau_L2\n        U_d = 1.0 / tau_d2\n        P_L = 1.0 / s_L2\n        P_H = 1.0 / s_H2\n        rho2 = rho**2\n        \n        target_info = 1.0 / (epsilon**2)\n\n        # Check for zero-simulation case\n        prior_var = rho2 * tau_L2 + tau_d2\n        if prior_var <= epsilon**2:\n            results.append([0, 0, 0.0])\n            continue\n\n        def get_i_l(n_L):\n            \"\"\"Calculates the information contribution from low-fidelity data.\"\"\"\n            num_term = U_L + n_L * P_L\n            den_term = num_term + rho2 * U_d\n            if den_term == 0: # Should not happen with valid inputs\n                return 0.0\n            return (num_term * U_d) / den_term\n        \n        def get_n_h(n_L):\n            \"\"\"Calculates the minimum required n_H for a given n_L.\"\"\"\n            i_l = get_i_l(n_L)\n            if target_info - i_l <= 0:\n                return 0\n            \n            required_n_h_float = s_H2 * (target_info - i_l)\n            return math.ceil(required_n_h_float)\n\n        # Start with the high-fidelity only case (n_L = 0)\n        best_nl = 0\n        best_nh = get_n_h(0)\n        best_T = float(best_nh * t_H)\n\n        # Determine the maximum n_L to search\n        # If best_T is 0, t_L must also be 0 for loop to run, which is not the case\n        if t_L > 0:\n            n_L_max = math.floor(best_T / t_L)\n        else:\n            n_L_max = 0\n\n        # Search for a better solution by increasing n_L\n        for n_L_current in range(1, n_L_max + 1):\n            n_h_current = get_n_h(n_L_current)\n            T_current = float(n_L_current * t_L + n_h_current * t_H)\n            \n            if T_current < best_T:\n                best_T = T_current\n                best_nl = n_L_current\n                best_nh = n_h_current\n                # Update search bound with better T\n                if t_L > 0:\n                    n_L_max = math.floor(best_T / t_L)\n\n        results.append([best_nl, best_nh, best_T])\n\n    # Format output according to specification\n    # e.g., [[n_L1,n_H1,T1],[n_L2,n_H2,T2]] with no spaces\n    inner_results_str = [f\"[{r[0]},{r[1]},{r[2]:.3f}]\" for r in results]\n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n\n```"
        }
    ]
}