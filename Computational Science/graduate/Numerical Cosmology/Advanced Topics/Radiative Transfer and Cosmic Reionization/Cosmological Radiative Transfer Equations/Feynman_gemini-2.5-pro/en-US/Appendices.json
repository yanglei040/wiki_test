{
    "hands_on_practices": [
        {
            "introduction": "In numerical cosmology, we often decouple intertwined physical processes, such as radiative transfer and chemistry, by solving them sequentially over a time step using operator splitting. This practical approach, however, introduces a splitting error that depends on the non-commutativity of the operators. This exercise  delves into the core of solver accuracy by using the Baker-Campbell-Hausdorff framework to analyze and numerically verify the error-scaling properties of first-order Lie-Trotter and second-order Strang splitting schemes, providing essential insights into their theoretical foundations.",
            "id": "3469637",
            "problem": "Consider an autonomous linear system arising from the coupling of cosmological radiative transfer and chemistry, written in operator form as $$\\frac{d\\mathbf{x}}{dt} = \\left(\\mathcal{L}_\\mathrm{RT} + \\mathcal{L}_\\mathrm{chem}\\right)\\mathbf{x},$$ where $$\\mathcal{L}_\\mathrm{RT}$$ denotes the radiative transfer operator and $$\\mathcal{L}_\\mathrm{chem}$$ denotes the chemistry operator. In a single-zone, linearized setting, represent $$\\mathcal{L}_\\mathrm{RT}$$ and $$\\mathcal{L}_\\mathrm{chem}$$ by constant matrices $$A$$ and $$B$$ acting on a finite-dimensional state vector $$\\mathbf{x} \\in \\mathbb{R}^n$$. The exact one-step propagator over a time step $$\\Delta t$$ is $$\\exp\\!\\left(\\Delta t\\,(A+B)\\right)$$. Two common operator-splitting schemes are: Lie–Trotter splitting, which applies $$\\exp(\\Delta t\\,A)$$ followed by $$\\exp(\\Delta t\\,B)$$, and Strang splitting, which applies $$\\exp\\!\\left(\\tfrac{\\Delta t}{2}A\\right)$$, then $$\\exp(\\Delta t\\,B)$$, then $$\\exp\\!\\left(\\tfrac{\\Delta t}{2}A\\right)$$. The Baker–Campbell–Hausdorff (BCH) expansion connects these split propagators to the exact propagator through commutators such as $$[A,B] \\equiv AB - BA$$.\n\nYour task is to quantify operator-splitting errors using the BCH framework and validate the predicted time-step scalings through a sweep of $$\\Delta t$$ values. For each test case below, you must:\n- Compute the commutator $$[A,B]$$ and its matrix two-norm $$\\lVert [A,B] \\rVert_2$$.\n- For a given set of time steps $$\\{\\Delta t_i\\}$$, compute the operator-norm errors of Lie–Trotter and Strang splitting relative to the exact propagator, defined as $$\\epsilon_\\mathrm{Lie}(\\Delta t_i) = \\left\\lVert \\exp\\!\\left(\\Delta t_i(A+B)\\right) - \\exp\\!\\left(\\Delta t_i A\\right)\\exp\\!\\left(\\Delta t_i B\\right)\\right\\rVert_2$$ and $$\\epsilon_\\mathrm{Str}(\\Delta t_i) = \\left\\lVert \\exp\\!\\left(\\Delta t_i(A+B)\\right) - \\exp\\!\\left(\\tfrac{\\Delta t_i}{2} A\\right)\\exp\\!\\left(\\Delta t_i B\\right)\\exp\\!\\left(\\tfrac{\\Delta t_i}{2} A\\right)\\right\\rVert_2$$.\n- Estimate the empirical scaling exponents $$p_\\mathrm{Lie}$$ and $$p_\\mathrm{Str}$$ by performing a least-squares fit of $$\\log \\epsilon$$ versus $$\\log \\Delta t$$ over the time-step sweep for each splitting scheme. If the errors are numerically zero across the sweep (for example, if $$[A,B] = 0$$), define the corresponding exponent as $$0.0$$.\n- For Lie–Trotter splitting only, compute the coefficient $$k_\\mathrm{Lie}$$ at the smallest time step, defined as $$k_\\mathrm{Lie} = \\epsilon_\\mathrm{Lie}(\\Delta t_\\mathrm{min})\\big/\\left(\\Delta t_\\mathrm{min}^2 \\,\\lVert [A,B] \\rVert_2\\right)$$, and set $$k_\\mathrm{Lie} = 0.0$$ when $$\\lVert [A,B] \\rVert_2 = 0$$.\n\nUse the matrix two-norm for all operator norms. Treat all quantities as dimensionless. The acronym BCH refers to Baker–Campbell–Hausdorff, and RT refers to Radiative Transfer.\n\nTest suite:\n- Case 1 (commuting operators, boundary condition): $$A = \\begin{pmatrix}-0.1 & 0 \\\\ 0 & -0.5\\end{pmatrix}, \\quad B = \\begin{pmatrix}-0.2 & 0 \\\\ 0 & -0.3\\end{pmatrix}.$$\n- Case 2 (non-commuting, moderate coupling): $$A = \\begin{pmatrix}-1.0 & 5.0 \\\\ 0.0 & -0.5\\end{pmatrix}, \\quad B = \\begin{pmatrix}-0.3 & 0.0 \\\\ 2.0 & -2.0\\end{pmatrix}.$$\n- Case 3 (non-commuting, stiff coupling): $$A = \\begin{pmatrix}-50.0 & 100.0 \\\\ 0.0 & -30.0\\end{pmatrix}, \\quad B = \\begin{pmatrix}-2.0 & -40.0 \\\\ 0.0 & -1.0\\end{pmatrix}.$$\n\nUse the time-step sweep $$\\Delta t \\in \\{10^{-5},\\,2\\times 10^{-5},\\,5\\times 10^{-5},\\,10^{-4},\\,2\\times 10^{-4},\\,5\\times 10^{-4},\\,10^{-3}\\}$$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, for each test case in order, the measured scaling exponent for Lie–Trotter splitting $$p_\\mathrm{Lie}$$, the measured scaling exponent for Strang splitting $$p_\\mathrm{Str}$$, and the coefficient $$k_\\mathrm{Lie}$$ at $$\\Delta t_\\mathrm{min}$$. For example, the output must be of the form $$[p_{\\mathrm{Lie},1},p_{\\mathrm{Str},1},k_{\\mathrm{Lie},1},p_{\\mathrm{Lie},2},p_{\\mathrm{Str},2},k_{\\mathrm{Lie},2},p_{\\mathrm{Lie},3},p_{\\mathrm{Str},3},k_{\\mathrm{Lie},3}]$$.\n\nYour implementation must be a complete, runnable program.",
            "solution": "The user wants to validate a problem statement and, if valid, provide a complete solution.\n\n### Step 1: Extract Givens\n- **Governing Equation**: The system is described by the linear ordinary differential equation (ODE) $\\frac{d\\mathbf{x}}{dt} = \\left(\\mathcal{L}_\\mathrm{RT} + \\mathcal{L}_\\mathrm{chem}\\right)\\mathbf{x}$.\n- **Operator Representation**: In a simplified single-zone model, the operators are represented by constant matrices: $\\mathcal{L}_\\mathrm{RT} \\to A$ and $\\mathcal{L}_\\mathrm{chem} \\to B$. The state vector is $\\mathbf{x} \\in \\mathbb{R}^n$.\n- **Exact Propagator**: The exact evolution operator over a time step $\\Delta t$ is $P_\\mathrm{exact}(\\Delta t) = \\exp\\!\\left(\\Delta t(A+B)\\right)$.\n- **Lie–Trotter Splitting**: The first-order approximate propagator is $P_\\mathrm{Lie}(\\Delta t) = \\exp(\\Delta t A)\\exp(\\Delta t B)$.\n- **Strang Splitting**: The second-order approximate propagator is $P_\\mathrm{Str}(\\Delta t) = \\exp\\!\\left(\\tfrac{\\Delta t}{2} A\\right)\\exp(\\Delta t B)\\exp\\!\\left(\\tfrac{\\Delta t}{2} A\\right)$.\n- **Quantities to Compute**:\n    1.  **Commutator and Norm**: $[A,B] \\equiv AB - BA$ and its matrix two-norm $\\lVert [A,B] \\rVert_2$.\n    2.  **Splitting Errors**:\n        - Lie–Trotter error: $\\epsilon_\\mathrm{Lie}(\\Delta t_i) = \\left\\lVert P_\\mathrm{exact}(\\Delta t_i) - P_\\mathrm{Lie}(\\Delta t_i) \\right\\rVert_2$.\n        - Strang error: $\\epsilon_\\mathrm{Str}(\\Delta t_i) = \\left\\lVert P_\\mathrm{exact}(\\Delta t_i) - P_\\mathrm{Str}(\\Delta t_i) \\right\\rVert_2$.\n    3.  **Scaling Exponents**: $p_\\mathrm{Lie}$ and $p_\\mathrm{Str}$, estimated from the slope of a least-squares fit to $\\log \\epsilon$ versus $\\log \\Delta t$. If errors are numerically zero, the exponent is defined as $0.0$.\n    4.  **Lie–Trotter Coefficient**: $k_\\mathrm{Lie} = \\epsilon_\\mathrm{Lie}(\\Delta t_\\mathrm{min})\\big/\\left(\\Delta t_\\mathrm{min}^2 \\,\\lVert [A,B] \\rVert_2\\right)$. If $\\lVert [A,B] \\rVert_2 = 0$, $k_\\mathrm{Lie} = 0.0$.\n- **Norm**: All norms are the matrix two-norm ($\\lVert \\cdot \\rVert_2$).\n- **Time Steps**: $\\Delta t \\in \\{10^{-5},\\,2\\times 10^{-5},\\,5\\times 10^{-5},\\,10^{-4},\\,2\\times 10^{-4},\\,5\\times 10^{-4},\\,10^{-3}\\}$.\n- **Test Cases**:\n    - Case 1: $A = \\begin{pmatrix}-0.1 & 0 \\\\ 0 & -0.5\\end{pmatrix}, B = \\begin{pmatrix}-0.2 & 0 \\\\ 0 & -0.3\\end{pmatrix}$.\n    - Case 2: $A = \\begin{pmatrix}-1.0 & 5.0 \\\\ 0.0 & -0.5\\end{pmatrix}, B = \\begin{pmatrix}-0.3 & 0.0 \\\\ 2.0 & -2.0\\end{pmatrix}$.\n    - Case 3: $A = \\begin{pmatrix}-50.0 & 100.0 \\\\ 0.0 & -30.0\\end{pmatrix}, B = \\begin{pmatrix}-2.0 & -40.0 \\\\ 0.0 & -1.0\\endpmatrix}$.\n- **Output Format**: A single line representing a comma-separated list of results: $[p_{\\mathrm{Lie},1}, p_{\\mathrm{Str},1}, k_{\\mathrm{Lie},1}, \\dots]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to rigorous validation.\n1.  **Scientifically Grounded**: The problem is rooted in the well-established field of numerical analysis for ordinary differential equations. Operator splitting methods (Lie–Trotter, Strang) are standard techniques for solving complex systems where the dynamics can be split into individually solvable parts. The use of the Baker–Campbell–Hausdorff (BCH) formula to analyze splitting errors is a cornerstone of the theory. The predicted error scalings, $\\epsilon_\\mathrm{Lie} \\propto (\\Delta t)^2$ and $\\epsilon_\\mathrm{Str} \\propto (\\Delta t)^3$ for non-commuting operators, are fundamental results. The problem sets up a standard numerical experiment to verify these theoretical predictions. It is scientifically sound.\n2.  **Well-Posed**: The problem is well-posed. All matrices, time steps, and formulas for the quantities to be computed are explicitly provided. Calculations involve standard matrix operations (multiplication, addition), matrix exponentiation, and matrix norms, all of which yield unique, well-defined results. The procedure for estimating scaling exponents via a least-squares fit is also a standard, deterministic process. Special cases, such as when the commutator is zero, are handled with explicit rules, ensuring there is no ambiguity.\n3.  **Objective**: The problem is stated using precise and unambiguous mathematical language, free from subjective or opinion-based content.\n\nThe problem does not exhibit any of the flaws listed in the problem validation checklist (e.g., scientific unsoundness, incompleteness, contradiction, infeasibility). Therefore, it is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Design\nThe solution is a numerical experiment designed to verify the theoretical error properties of operator-splitting methods. The theoretical foundation is the Baker–Campbell–Hausdorff (BCH) formula, which provides an expression for $\\log(\\exp(X)\\exp(Y))$ in terms of $X$, $Y$, and their nested commutators.\n\nFor the Lie–Trotter splitting, the BCH formula implies:\n$$ \\exp(\\Delta t A)\\exp(\\Delta t B) = \\exp\\left(\\Delta t(A+B) + \\frac{(\\Delta t)^2}{2}[A,B] + \\mathcal{O}((\\Delta t)^3)\\right) $$\nThe difference between the approximate and exact propagators is therefore:\n$$ P_\\mathrm{Lie}(\\Delta t) - P_\\mathrm{exact}(\\Delta t) \\approx \\frac{(\\Delta t)^2}{2}[A,B] $$\nfor small $\\Delta t$. Taking the norm, we find the leading-order error:\n$$ \\epsilon_\\mathrm{Lie}(\\Delta t) = \\lVert P_\\mathrm{Lie}(\\Delta t) - P_\\mathrm{exact}(\\Delta t) \\rVert_2 \\approx \\frac{(\\Delta t)^2}{2} \\lVert [A,B] \\rVert_2 $$\nThis predicts that the error scales quadratically with the time step, so $p_\\mathrm{Lie} \\approx 2$. It also predicts that the coefficient $k_\\mathrm{Lie} = \\epsilon_\\mathrm{Lie}/(\\Delta t^2 \\lVert [A,B] \\rVert_2)$ should be approximately $0.5$.\n\nFor the Strang splitting, the symmetric composition leads to the cancellation of the $(\\Delta t)^2$ error term. The leading error term is of order $(\\Delta t)^3$ and involves higher-order commutators:\n$$ P_\\mathrm{Str}(\\Delta t) - P_\\mathrm{exact}(\\Delta t) = C_1 (\\Delta t)^3 [B,[B,A]] + C_2 (\\Delta t)^3 [A,[A,B]] + \\mathcal{O}((\\Delta t)^4) $$\nwhere $C_1$ and $C_2$ are constants. This predicts that the error scales cubically with the time step, so $p_\\mathrm{Str} \\approx 3$.\n\nIf the operators commute, i.e., $[A,B]=0$, then all higher-order commutators are also zero. In this case, $\\exp(\\Delta t A)\\exp(\\Delta t B) = \\exp(\\Delta t(A+B))$, and both splitting schemes become exact. The errors $\\epsilon_\\mathrm{Lie}$ and $\\epsilon_\\mathrm{Str}$ are zero, and per the problem, the scaling exponents are defined as $0.0$.\n\nThe algorithm will systematically implement these computations:\n1.  For each test case, the matrices $A$ and $B$ are defined.\n2.  The commutator $[A,B]$ and its two-norm are calculated. This immediately identifies whether the commuting or non-commuting case applies.\n3.  A loop iterates through the prescribed set of time steps $\\{\\Delta t_i\\}$. In each iteration:\n    a. The exact propagator $P_\\mathrm{exact} = \\exp(\\Delta t (A+B))$ is computed.\n    b. The approximate propagators, $P_\\mathrm{Lie}$ and $P_\\mathrm{Str}$, are computed using their definitions.\n    c. The two-norms of the differences, $\\epsilon_\\mathrm{Lie}$ and $\\epsilon_\\mathrm{Str}$, are calculated and stored.\n4.  After the loop, the collected error data is used to find the scaling exponents. A linear least-squares fit is applied to the logarithms of the errors versus the logarithms of the time steps. The slope of the resulting line corresponds to the scaling exponent. This is equivalent to fitting a power law $\\epsilon = C(\\Delta t)^p$.\n5.  Finally, the coefficient $k_\\mathrm{Lie}$ is computed at the smallest time step, $\\Delta t_\\mathrm{min}$, using its definition. This value is compared to the theoretical prediction of $0.5$.\n6.  Special conditions for numerically zero commutators or errors are handled as specified.\n\nThe implementation will rely on `numpy` for matrix objects and operations, and `scipy.linalg.expm` for the matrix exponential, which is a robust and accurate algorithm suitable for this task.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Computes operator-splitting errors and validates time-step scalings for three test cases,\n    as specified in the problem statement.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[-0.1, 0.0], [0.0, -0.5]]),\n            \"B\": np.array([[-0.2, 0.0], [0.0, -0.3]]),\n        },\n        {\n            \"A\": np.array([[-1.0, 5.0], [0.0, -0.5]]),\n            \"B\": np.array([[-0.3, 0.0], [2.0, -2.0]]),\n        },\n        {\n            \"A\": np.array([[-50.0, 100.0], [0.0, -30.0]]),\n            \"B\": np.array([[-2.0, -40.0], [0.0, -1.0]]),\n        }\n    ]\n\n    # Define the time-step sweep. All quantities are treated as dimensionless.\n    delta_t_sweep = np.array([1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3])\n    log_dts = np.log(delta_t_sweep)\n\n    results = []\n    \n    # A small tolerance for floating point comparisons to zero.\n    ZERO_TOLERANCE = 1e-15\n\n    for case in test_cases:\n        A = case[\"A\"]\n        B = case[\"B\"]\n        \n        # 1. Compute the commutator [A,B] and its matrix two-norm.\n        commutator = A @ B - B @ A\n        norm_commutator = np.linalg.norm(commutator, 2)\n        \n        errors_lie = []\n        errors_str = []\n        \n        # 2. Compute operator-norm errors for the sweep of time steps.\n        for dt in delta_t_sweep:\n            # Exact propagator\n            P_exact = expm(dt * (A + B))\n            \n            # Lie-Trotter propagator and error\n            # Propagator is exp(dt*A)exp(dt*B) as per the error formula\n            P_lie = expm(dt * A) @ expm(dt * B)\n            err_lie = np.linalg.norm(P_exact - P_lie, 2)\n            errors_lie.append(err_lie)\n            \n            # Strang propagator and error\n            P_str = expm(0.5 * dt * A) @ expm(dt * B) @ expm(0.5 * dt * A)\n            err_str = np.linalg.norm(P_exact - P_str, 2)\n            errors_str.append(err_str)\n            \n        errors_lie = np.array(errors_lie)\n        errors_str = np.array(errors_str)\n\n        # 3. Estimate empirical scaling exponents p_Lie and p_Str.\n        \n        # Lie-Trotter exponent p_lie\n        if np.all(errors_lie < ZERO_TOLERANCE):\n            p_lie = 0.0\n        else:\n            # Filter out non-positive errors to avoid log errors\n            valid_indices = errors_lie > ZERO_TOLERANCE\n            # Perform least-squares fit on log-log data; slope is the exponent\n            p_lie, _ = np.polyfit(log_dts[valid_indices], np.log(errors_lie[valid_indices]), 1)\n            \n        # Strang exponent p_str\n        if np.all(errors_str < ZERO_TOLERANCE):\n            p_str = 0.0\n        else:\n            valid_indices = errors_str > ZERO_TOLERANCE\n            p_str, _ = np.polyfit(log_dts[valid_indices], np.log(errors_str[valid_indices]), 1)\n            \n        # 4. Compute the coefficient k_Lie.\n        if norm_commutator < ZERO_TOLERANCE:\n            k_lie = 0.0\n        else:\n            dt_min = delta_t_sweep[0]\n            err_lie_min = errors_lie[0]\n            k_lie = err_lie_min / (dt_min**2 * norm_commutator)\n            \n        results.extend([p_lie, p_str, k_lie])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A stable and efficient simulation requires a time step that adapts to the fastest relevant physical processes. In cosmological radiative transfer, these processes can range from light-crossing times and frequency shifts due to Hubble expansion to the rapid timescales of atomic ionization and recombination. This practice  guides you through the fundamental derivation of a composite, CFL-like time-step constraint that synthesizes these multiple physical limits, a critical skill for developing or utilizing any explicit time-marching numerical code.",
            "id": "3469642",
            "problem": "You are tasked with deriving and implementing a Courant–Friedrichs–Lewy (CFL)-like adaptive time-step constraint for an explicit update of cosmological radiative transfer and ionization kinetics under expansion. The derivation must begin from fundamental balances and kinematic definitions for radiation in an expanding universe, explicit numerical stability requirements for hyperbolic advection, and first-order kinetic rate equations.\n\nStart from the following foundational base:\n- The cosmological radiative transfer equation in a spatially homogeneous, isotropic, expanding universe for specific intensity $I(\\nu)$ under Hubble expansion rate $H(t)$, written in terms of comoving spatial coordinates and frequency advection,\n$$\n\\frac{\\partial I}{\\partial t} + c\\,\\mathbf{n}\\cdot\\nabla I - H\\,\\nu\\,\\frac{\\partial I}{\\partial \\nu} = - c\\,\\kappa\\, I + j,\n$$\nwhere $c$ is the speed of light, $\\kappa$ is the absorption coefficient with units $\\mathrm{m}^{-1}$, $j$ is the emissivity, and $H$ is the Hubble parameter with units $\\mathrm{s}^{-1}$. For the purposes of this problem, you will consider an explicit time integrator for the transport and chemistry substeps, subject to stability and accuracy constraints.\n- The scalar hydrogen ionization fraction $x(t)$ obeys a first-order rate equation,\n$$\n\\frac{dx}{dt} = \\Gamma\\,(1 - x) - \\alpha_{\\mathrm{B}}\\,n_{\\mathrm{H}}\\,x^2,\n$$\nwhere $\\Gamma$ is the photoionization rate with units $\\mathrm{s}^{-1}$, $\\alpha_{\\mathrm{B}}$ is the case-B recombination coefficient with units $\\mathrm{m}^3\\,\\mathrm{s}^{-1}$, and $n_{\\mathrm{H}}$ is the total hydrogen number density with units $\\mathrm{m}^{-3}$.\n\nFrom these bases, derive a composite explicit time-step constraint that simultaneously enforces:\n- Light travel constraint for spatial advection over a uniform cell size $\\Delta x$, i.e., the requirement that characteristics do not cross more than one cell in one step.\n- Frequency-space advection constraint induced by cosmological redshifting across a uniform frequency bin width $\\Delta \\nu$.\n- An absorption-induced intensity change constraint limiting the fractional decrement in $I$ per step due to $-c\\,\\kappa\\,I$.\n- An ionization fraction change constraint limiting the fractional change in $x$ per step as dictated by the kinetic rate equation.\n\nFrom first principles, derive each constraint in terms of the parameters $\\Delta x$, $\\Delta \\nu$, $\\nu$, $H$, $\\kappa$, $c$, $x$, $n_{\\mathrm{H}}$, $\\Gamma$, and $\\alpha_{\\mathrm{B}}$, together with user-specified stability/accuracy fractions $C_x$, $C_\\nu$, $\\varepsilon_I$, and $\\varepsilon_x$ that limit, respectively, the Courant numbers for spatial and frequency advection and the allowed fractional changes in intensity and ionization per step. Combine these into a single proposed time step $\\Delta t_{\\mathrm{prop}}$ by taking the minimum of the individual constraints.\n\nImplement an adaptive step acceptance procedure for the ionization update using an explicit forward Euler method with an embedded two half-step error estimator:\n- Given a candidate step $\\Delta t$, compute a full-step update $x_{\\mathrm{full}} = x + \\Delta t\\,f(x)$, where $f(x) = \\Gamma\\,(1-x) - \\alpha_{\\mathrm{B}}\\,n_{\\mathrm{H}}\\,x^2$.\n- Compute a two half-step estimate $x_{\\mathrm{half}} = x + \\frac{\\Delta t}{2}\\,f(x)$, then $x_{\\mathrm{two}} = x_{\\mathrm{half}} + \\frac{\\Delta t}{2}\\,f(x_{\\mathrm{half}})$.\n- Estimate the local error $e = |x_{\\mathrm{full}} - x_{\\mathrm{two}}|$ and accept the step if $e \\le \\mathrm{tol}_x$, otherwise reduce $\\Delta t$ according to a standard adaptive step size control rule that scales with $(\\mathrm{tol}_x/e)^{1/2}$ and retry, subject to a reasonable safety factor and iteration limit. Enforce physical bounds $0 \\le x \\le 1$ on the trial updates.\n\nNumerical and output requirements:\n- All outputs must be expressed in seconds and rounded to the nearest integer second.\n- Your program must produce a single line of output containing the accepted time steps (one per test case) as a comma-separated list enclosed in square brackets (e.g., $[t_1,t_2,t_3,t_4]$).\n- No user input is permitted; all parameters are specified below.\n\nImplement the above for the following test suite of parameter values to exercise different regimes. Use the constants and parameters exactly as stated.\n\n- Test Case 1 (general case):\n  - $\\Delta x = 3.086\\times 10^{19}\\,\\mathrm{m}$, $\\Delta \\nu = 1.0\\times 10^{14}\\,\\mathrm{Hz}$, $\\nu = 3.0\\times 10^{15}\\,\\mathrm{Hz}$,\n  - $H = 1.0\\times 10^{-17}\\,\\mathrm{s}^{-1}$, $\\kappa = 1.3\\times 10^{-19}\\,\\mathrm{m}^{-1}$,\n  - $n_{\\mathrm{H}} = 200.0\\,\\mathrm{m}^{-3}$, $\\Gamma = 1.0\\times 10^{-12}\\,\\mathrm{s}^{-1}$, $\\alpha_{\\mathrm{B}} = 2.6\\times 10^{-19}\\,\\mathrm{m}^3\\,\\mathrm{s}^{-1}$,\n  - $x_0 = 0.10$, $C_x = 0.90$, $C_\\nu = 0.90$, $\\varepsilon_I = 0.02$, $\\varepsilon_x = 0.01$, $\\mathrm{tol}_x = 1.0\\times 10^{-4}$.\n- Test Case 2 (small spatial cell boundary):\n  - $\\Delta x = 1.0\\times 10^{16}\\,\\mathrm{m}$, $\\Delta \\nu = 1.0\\times 10^{14}\\,\\mathrm{Hz}$, $\\nu = 3.0\\times 10^{15}\\,\\mathrm{Hz}$,\n  - $H = 1.0\\times 10^{-17}\\,\\mathrm{s}^{-1}$, $\\kappa = 1.3\\times 10^{-19}\\,\\mathrm{m}^{-1}$,\n  - $n_{\\mathrm{H}} = 200.0\\,\\mathrm{m}^{-3}$, $\\Gamma = 5.0\\times 10^{-13}\\,\\mathrm{s}^{-1}$, $\\alpha_{\\mathrm{B}} = 2.6\\times 10^{-19}\\,\\mathrm{m}^3\\,\\mathrm{s}^{-1}$,\n  - $x_0 = 0.50$, $C_x = 0.90$, $C_\\nu = 0.90$, $\\varepsilon_I = 0.02$, $\\varepsilon_x = 0.02$, $\\mathrm{tol}_x = 1.0\\times 10^{-5}$.\n- Test Case 3 (strong absorption edge case):\n  - $\\Delta x = 3.086\\times 10^{19}\\,\\mathrm{m}$, $\\Delta \\nu = 1.0\\times 10^{14}\\,\\mathrm{Hz}$, $\\nu = 3.0\\times 10^{15}\\,\\mathrm{Hz}$,\n  - $H = 1.0\\times 10^{-17}\\,\\mathrm{s}^{-1}$, $\\kappa = 1.0\\times 10^{-16}\\,\\mathrm{m}^{-1}$,\n  - $n_{\\mathrm{H}} = 200.0\\,\\mathrm{m}^{-3}$, $\\Gamma = 1.0\\times 10^{-13}\\,\\mathrm{s}^{-1}$, $\\alpha_{\\mathrm{B}} = 2.6\\times 10^{-19}\\,\\mathrm{m}^3\\,\\mathrm{s}^{-1}$,\n  - $x_0 = 0.01$, $C_x = 0.90$, $C_\\nu = 0.90$, $\\varepsilon_I = 0.01$, $\\varepsilon_x = 0.005$, $\\mathrm{tol}_x = 1.0\\times 10^{-6}$.\n- Test Case 4 (cosmological redshift dominated):\n  - $\\Delta x = 3.086\\times 10^{22}\\,\\mathrm{m}$, $\\Delta \\nu = 1.0\\times 10^{9}\\,\\mathrm{Hz}$, $\\nu = 3.0\\times 10^{15}\\,\\mathrm{Hz}$,\n  - $H = 5.0\\times 10^{-16}\\,\\mathrm{s}^{-1}$, $\\kappa = 1.3\\times 10^{-19}\\,\\mathrm{m}^{-1}$,\n  - $n_{\\mathrm{H}} = 200.0\\,\\mathrm{m}^{-3}$, $\\Gamma = 1.0\\times 10^{-14}\\,\\mathrm{s}^{-1}$, $\\alpha_{\\mathrm{B}} = 2.6\\times 10^{-19}\\,\\mathrm{m}^3\\,\\mathrm{s}^{-1}$,\n  - $x_0 = 0.90$, $C_x = 0.90$, $C_\\nu = 0.90$, $\\varepsilon_I = 0.02$, $\\varepsilon_x = 0.01$, $\\mathrm{tol}_x = 1.0\\times 10^{-4}$.\n\nAdditional requirements:\n- Use $c = 2.99792458\\times 10^{8}\\,\\mathrm{m}\\,\\mathrm{s}^{-1}$.\n- The program must be written in Python and produce exactly one printed line in the specified format, containing the accepted time steps for the above cases in seconds, rounded to the nearest integer.",
            "solution": "The problem requires the derivation and implementation of a composite time-step constraint for an explicit numerical scheme that solves the cosmological radiative transfer equation coupled with ionization kinetics. The derivation must originate from first principles for each contributing physical process: spatial advection, frequency-space advection (cosmological redshift), absorption, and chemical reactions. Subsequently, this time step is to be used as an initial guess in an adaptive step-size control algorithm for the ionization update.\n\nThe derivation proceeds by analyzing each term of the governing equations and establishing a condition that ensures numerical stability and accuracy for an explicit forward Euler time-stepping method. These individual time-step constraints, denoted $\\Delta t_i$, are then combined to find the most restrictive limit.\n\nThe governing equations provided are:\n1.  The cosmological radiative transfer equation (RTE) for specific intensity $I(\\nu, t)$:\n    $$\n    \\frac{\\partial I}{\\partial t} + c\\,\\mathbf{n}\\cdot\\nabla I - H\\,\\nu\\,\\frac{\\partial I}{\\partial \\nu} = - c\\,\\kappa\\, I + j\n    $$\n2.  The ionization fraction rate equation for $x(t)$:\n    $$\n    \\frac{dx}{dt} = \\Gamma\\,(1 - x) - \\alpha_{\\mathrm{B}}\\,n_{\\mathrm{H}}\\,x^2\n    $$\n    The rate function is $f(x) = \\frac{dx}{dt} = \\Gamma\\,(1-x) - \\alpha_{\\mathrm{B}}\\,n_{\\mathrm{H}}\\,x^2$.\n\nWe will now derive the four required time-step constraints from these equations.\n\n**1. Spatial Advection Constraint (Courant-Friedrichs-Lewy condition)**\n\nThe term $c\\,\\mathbf{n}\\cdot\\nabla I$ in the RTE represents the spatial transport of photons at the speed of light, $c$. For a one-dimensional explicit finite-difference scheme on a uniform grid with cell size $\\Delta x$, the Courant-Friedrichs-Lewy (CFL) condition must be satisfied to ensure numerical stability. This principle dictates that the numerical domain of dependence must contain the physical domain of dependence. In the context of advection, this means that information (here, photons) cannot travel more than one grid cell in a single time step $\\Delta t$. This is expressed as:\n$$\nc \\Delta t \\le \\Delta x\n$$\nThe problem introduces a user-specified Courant number, $C_x \\in (0, 1]$, as a safety factor. The constraint is thus:\n$$\nc \\Delta t \\le C_x \\Delta x\n$$\nSolving for $\\Delta t$ gives the spatial advection time-step limit, $\\Delta t_x$:\n$$\n\\Delta t_x = C_x \\frac{\\Delta x}{c}\n$$\n\n**2. Frequency-Space Advection Constraint (Redshift)**\n\nThe term $-H\\,\\nu\\,\\frac{\\partial I}{\\partial \\nu}$ in the RTE describes the advection of photons in frequency space due to the Hubble expansion of the universe. The \"velocity\" of this advection is $v_\\nu = \\frac{d\\nu}{dt} = -H\\nu$. Analogous to the spatial CFL condition, for an explicit scheme on a frequency grid with bin width $\\Delta \\nu$, the change in a photon's frequency over a time step $\\Delta t$ must not be larger than the frequency bin width. The magnitude of frequency change is $|\\Delta \\nu_{\\text{adv}}| = |v_\\nu| \\Delta t = H\\nu\\Delta t$. The stability condition is:\n$$\nH\\nu\\Delta t \\le \\Delta \\nu\n$$\nIntroducing the frequency-space Courant number, $C_\\nu$, as a safety factor, we have:\n$$\nH\\nu\\Delta t \\le C_\\nu \\Delta \\nu\n$$\nSolving for $\\Delta t$ yields the frequency advection time-step limit, $\\Delta t_\\nu$:\n$$\n\\Delta t_\\nu = C_\\nu \\frac{\\Delta \\nu}{H \\nu}\n$$\n\n**3. Absorption-Induced Intensity Change Constraint**\n\nThe term $-c\\,\\kappa\\,I$ on the right-hand side of the RTE is a sink term representing photon absorption. The time evolution of intensity due to absorption alone is $\\frac{\\partial I}{\\partial t} = -c\\,\\kappa\\,I$. An explicit forward Euler update step is $I^{n+1} = I^n - \\Delta t (c\\kappa I^n)$. The problem requires limiting the fractional change in intensity, $\\frac{|I^{n+1} - I^n|}{I^n}$, to a user-specified tolerance $\\varepsilon_I$.\n$$\n\\frac{|I^n - \\Delta t c\\kappa I^n - I^n|}{I^n} = \\frac{|-\\Delta t c\\kappa I^n|}{I^n} = c\\kappa\\Delta t \\le \\varepsilon_I\n$$\nThis directly gives the absorption time-step limit, $\\Delta t_I$:\n$$\n\\Delta t_I = \\frac{\\varepsilon_I}{c \\kappa}\n$$\nThis condition ensures that the intensity does not change by a large fraction in a single step due to absorption, which is critical for accuracy.\n\n**4. Ionization Fraction Change Constraint**\n\nThe evolution of the ionization fraction $x$ is governed by the ordinary differential equation (ODE) $\\frac{dx}{dt} = f(x)$, where $f(x) = \\Gamma(1-x) - \\alpha_{\\mathrm{B}} n_{\\mathrm{H}} x^2$. Similar to the absorption constraint, we must limit the fractional change in $x$ per time step to a specified accuracy parameter $\\varepsilon_x$. The change in $x$ over a step $\\Delta t$ is approximately $\\Delta x_{\\text{change}} \\approx \\Delta t \\left|\\frac{dx}{dt}\\right|$. The fractional change is $\\frac{|\\Delta x_{\\text{change}}|}{x}$.\n$$\n\\frac{\\Delta t \\left| f(x) \\right|}{x} \\le \\varepsilon_x\n$$\nSolving for $\\Delta t$ provides the chemistry time-step limit, which we denote $\\Delta t_{\\text{chem}}$:\n$$\n\\Delta t_{\\text{chem}} = \\frac{\\varepsilon_x x}{\\left| f(x) \\right|} = \\frac{\\varepsilon_x x}{\\left|\\Gamma(1-x) - \\alpha_{\\mathrm{B}} n_{\\mathrm{H}} x^2\\right|}\n$$\nIf the system is near equilibrium, the denominator $|f(x)|$ approaches $0$, and $\\Delta t_{\\text{chem}}$ becomes very large. This is physically sensible, as the timescale for change is long near equilibrium.\n\n**Composite Time-Step and Adaptive Control**\n\nTo ensure stability and accuracy across all physical processes, the overall proposed time step, $\\Delta t_{\\mathrm{prop}}$, must be the minimum of the individual constraints:\n$$\n\\Delta t_{\\mathrm{prop}} = \\min(\\Delta t_x, \\Delta t_\\nu, \\Delta t_I, \\Delta t_{\\text{chem}})\n$$\nThis $\\Delta t_{\\mathrm{prop}}$ serves as an initial guess for an adaptive time-stepping scheme for the ionization ODE. The problem specifies an embedded error estimator based on a single forward Euler step and two half-steps.\nGiven a current state $x_n$ and a candidate time step $\\Delta t$:\n1.  Compute a full-step update: $x_{\\text{full}} = x_n + \\Delta t \\, f(x_n)$.\n2.  Compute a two-step update: $x_{\\text{half}} = x_n + \\frac{\\Delta t}{2} f(x_n)$, followed by $x_{\\text{two}} = x_{\\text{half}} + \\frac{\\Delta t}{2} f(x_{\\text{half}})$.\n3.  The trial solutions must be kept within the physical bounds $[0, 1]$.\n4.  The local truncation error is estimated as $e = |x_{\\text{full}} - x_{\\text{two}}|$. For this method, the error of the less accurate solution ($x_{\\text{full}}$) is of order $\\mathcal{O}(\\Delta t^2)$.\n5.  The step is accepted if $e \\le \\mathrm{tol}_x$, where $\\mathrm{tol}_x$ is a given tolerance.\n6.  If rejected, a new, smaller time step $\\Delta t_{\\text{new}}$ is computed using the standard control law for embedded Runge-Kutta methods, which aims for $e_{\\text{new}} \\approx \\mathrm{tol}_x$:\n    $$\n    \\Delta t_{\\text{new}} = S \\cdot \\Delta t_{\\text{old}} \\left( \\frac{\\mathrm{tol}_x}{e} \\right)^{1/p}\n    $$\n    where $p=2$ is the order of the error estimator and $S$ is a safety factor (e.g., $S=0.9$). The process is repeated with $\\Delta t_{\\text{new}}$ until the step is accepted. The final accepted time step is the result for the given test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements an adaptive time-step constraint for cosmological\n    radiative transfer and ionization kinetics.\n    \"\"\"\n    # Universal constant\n    C_LIGHT = 2.99792458e8  # Speed of light in m/s\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1 (general case)\n        {'dx': 3.086e19, 'dnu': 1.0e14, 'nu': 3.0e15, 'H': 1.0e-17, 'kappa': 1.3e-19,\n         'nH': 200.0, 'Gamma': 1.0e-12, 'alphaB': 2.6e-19, 'x0': 0.10,\n         'Cx': 0.90, 'Cnu': 0.90, 'eps_I': 0.02, 'eps_x': 0.01, 'tol_x': 1.0e-4},\n        # Test Case 2 (small spatial cell boundary)\n        {'dx': 1.0e16, 'dnu': 1.0e14, 'nu': 3.0e15, 'H': 1.0e-17, 'kappa': 1.3e-19,\n         'nH': 200.0, 'Gamma': 5.0e-13, 'alphaB': 2.6e-19, 'x0': 0.50,\n         'Cx': 0.90, 'Cnu': 0.90, 'eps_I': 0.02, 'eps_x': 0.02, 'tol_x': 1.0e-5},\n        # Test Case 3 (strong absorption edge case)\n        {'dx': 3.086e19, 'dnu': 1.0e14, 'nu': 3.0e15, 'H': 1.0e-17, 'kappa': 1.0e-16,\n         'nH': 200.0, 'Gamma': 1.0e-13, 'alphaB': 2.6e-19, 'x0': 0.01,\n         'Cx': 0.90, 'Cnu': 0.90, 'eps_I': 0.01, 'eps_x': 0.005, 'tol_x': 1.0e-6},\n        # Test Case 4 (cosmological redshift dominated)\n        {'dx': 3.086e22, 'dnu': 1.0e9, 'nu': 3.0e15, 'H': 5.0e-16, 'kappa': 1.3e-19,\n         'nH': 200.0, 'Gamma': 1.0e-14, 'alphaB': 2.6e-19, 'x0': 0.90,\n         'Cx': 0.90, 'Cnu': 0.90, 'eps_I': 0.02, 'eps_x': 0.01, 'tol_x': 1.0e-4},\n    ]\n\n    results = []\n    \n    for params in test_cases:\n        # --- Part 1: Calculate the composite proposed time-step ---\n\n        # Spatial advection constraint (CFL)\n        dt_x = params['Cx'] * params['dx'] / C_LIGHT\n\n        # Frequency-space advection constraint (Redshift)\n        denom_nu = params['H'] * params['nu']\n        dt_nu = (params['Cnu'] * params['dnu'] / denom_nu) if denom_nu > 0 else np.inf\n\n        # Absorption constraint\n        denom_I = C_LIGHT * params['kappa']\n        dt_I = (params['eps_I'] / denom_I) if denom_I > 0 else np.inf\n\n        # Define the ionization rate function f(x) = dx/dt\n        def f_chem(x, p):\n            ion_term = p['Gamma'] * (1.0 - x)\n            recomb_term = p['alphaB'] * p['nH'] * x**2\n            return ion_term - recomb_term\n\n        # Chemistry constraint\n        rate_at_x0 = f_chem(params['x0'], params)\n        if abs(rate_at_x0) < np.finfo(float).eps:\n            dt_chem = np.inf\n        else:\n            dt_chem = params['eps_x'] * params['x0'] / abs(rate_at_x0)\n        \n        # Composite proposed time-step is the minimum of all constraints\n        dt_prop = min(dt_x, dt_nu, dt_I, dt_chem)\n\n        # --- Part 2: Adaptive step acceptance for ionization update ---\n        \n        dt_current = dt_prop\n        accepted_dt = None\n        \n        # Parameters for adaptive control\n        max_iterations = 50\n        safety_factor = 0.9 \n        # The exponent in the update rule is 1/p, where p=2 for this scheme.\n        power = 0.5 \n\n        for _ in range(max_iterations):\n            f = lambda x: f_chem(x, params)\n            \n            # Compute full-step update (Euler method)\n            x_full = params['x0'] + dt_current * f(params['x0'])\n            x_full = np.clip(x_full, 0.0, 1.0) # Enforce physical bounds\n            \n            # Compute two half-step update (embedded method)\n            x_half = params['x0'] + (dt_current / 2.0) * f(params['x0'])\n            x_half = np.clip(x_half, 0.0, 1.0) # Enforce physical bounds\n            \n            x_two = x_half + (dt_current / 2.0) * f(x_half)\n            x_two = np.clip(x_two, 0.0, 1.0) # Enforce physical bounds\n            \n            # Estimate local error\n            error = abs(x_full - x_two)\n            \n            # Acceptance criterion\n            if error <= params['tol_x']:\n                accepted_dt = dt_current\n                break\n            \n            # Step rejection and update of dt\n            # If error is zero, the step should be accepted.\n            # Add a small epsilon to denominator for robustness in case error is tiny.\n            if error > 0:\n                dt_current = safety_factor * dt_current * (params['tol_x'] / error)**power\n            else:\n                # Should not happen as error=0 implies acceptance, but as a failsafe\n                # If the error is truly zero, the step is perfect. We can increase it. \n                # Capping the increase to avoid runaway.\n                dt_current *= 2.0 \n\n        if accepted_dt is None:\n            # If the loop finishes without acceptance (unlikely for this problem),\n            # we use the last computed time step as a failsafe.\n            accepted_dt = dt_current\n            \n        # Round the accepted time step to the nearest integer second.\n        # (int(x + 0.5) for positive x rounds half up)\n        results.append(int(accepted_dt + 0.5))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "To capture the vast range of scales in cosmic structure formation, modern simulations employ Adaptive Mesh Refinement (AMR), which focuses resolution where it is most needed. A non-negotiable requirement of any AMR implementation is the strict conservation of physical quantities like mass, momentum, and photon number when mapping data between coarse and fine grids. This hands-on problem  tackles this crucial challenge by tasking you with developing and verifying a conservative remapping algorithm using piecewise-linear reconstruction and slope limiters, ensuring that the total photon number is conserved to machine precision across refinement and coarsening operations.",
            "id": "3469649",
            "problem": "Consider the moment formulation of cosmological radiative transfer in a one-dimensional periodic domain. Let the radiation field be represented by its moments per unit length: the radiation energy density $E$, the radiation flux $F$, and the photon number density $N$. For each cell $i$ with width $\\Delta x$ and center position $x_i$, define the cell-averaged moments as $M_i = (E_i, F_i, N_i)$, where $E_i$, $F_i$, and $N_i$ are the averages over the cell. The Adaptive Mesh Refinement (AMR) operation maps moments between a coarse grid and a fine grid. Your task is to implement a conservative remapping algorithm for the vector of moments $M = (E, F, N)$ across AMR refinement and coarsening operations and to demonstrate conservation of the $L^1$ norm of the photon number to machine precision.\n\nFundamental base:\n- The photon number moment $N$ is the zeroth angular-frequency moment of the specific intensity $I_\\nu$, $N = \\int \\int (I_\\nu / h\\nu)\\,\\mathrm{d}\\Omega\\,\\mathrm{d}\\nu$, and is non-negative in physically realistic scenarios.\n- The conservation of a quantity under grid remapping is demonstrated by equality of its volume integral before and after the operation.\n- The $L^1$ norm of a non-negative scalar field $N(x)$ over a spatial domain is $\\|N\\|_1 = \\int |N(x)|\\,\\mathrm{d}x = \\int N(x)\\,\\mathrm{d}x$.\n\nAlgorithmic requirements:\n1. Implement a one-dimensional periodic domain of length $L$ discretized into $n$ coarse cells of width $\\Delta x = L/n$, with coarse cell centers $x_i = (i+1/2)\\Delta x$ for integer $i$.\n2. Define a refinement factor $r \\in \\mathbb{N}$ for AMR. Refinement maps each coarse cell into $r$ fine child cells of width $\\Delta x_f = \\Delta x/r$, with child cell midpoints inside the parent cell.\n3. Refinement must use a piecewise-linear reconstruction within each coarse cell for each moment component:\n   - For each coarse cell $i$, reconstruct $m(x) = a_i + s_i (x - x_i)$, where $a_i$ is the coarse cell average of the moment and $s_i$ is a limited slope.\n   - Estimate the slope using the MinMod limiter with periodic neighbors to prevent spurious oscillations: $s_i = \\operatorname{minmod}\\big((a_i - a_{i-1})/\\Delta x,(a_{i+1} - a_i)/\\Delta x\\big)$, where $\\operatorname{minmod}(u,v)$ equals $0$ if $uv \\le 0$, and otherwise equals $\\operatorname{sign}(u)\\min(|u|,|v|)$.\n   - For the photon number density $N$, enforce positivity under refinement by bounding $|s_i| \\le a_i / \\delta_{\\max}$, where $\\delta_{\\max}$ is the maximum absolute offset of any child midpoint from $x_i$; this ensures $a_i + s_i \\delta \\ge 0$ for all child midpoints $\\delta$ inside the parent cell.\n   - Assign each fine child cell the moment value equal to the linear reconstruction evaluated at its midpoint. For a linear function, the midpoint rule is exact for subcell integrals, ensuring conservation of the parent cell average under refinement.\n4. Coarsening must map fine child moments back to coarse moments by volume-weighted averaging. For equal-size children, this reduces to the arithmetic mean of child values within each parent cell.\n5. Demonstrate numerically that the $L^1$ norm of photon number, defined as the integral $\\sum_i N_i \\Delta x$, is conserved across refinement followed by coarsening to within machine precision.\n\nScientific realism:\n- Treat the domain as periodic.\n- Ensure $N_i \\ge 0$ for all coarse cells; positivity should be preserved during refinement by the chosen limiter.\n\nYour program must implement the above operations and output, for each test case, the absolute difference between the integrated photon number before and after performing refinement followed by coarsening. Express each difference as a floating-point number representing the number of photons. No angles are involved. The program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").\n\nTest suite:\n- Test 1 (uniform field, happy path): $L = 1.0$, $n = 8$, $r = 2$, coarse moments initialized to $E_i = 10.0$, $F_i = 0.5$, $N_i = 3.0$ for all $i$.\n- Test 2 (smooth nonuniform, periodic): $L = 2.0$, $n = 16$, $r = 4$, coarse moments initialized using $x_i = (i+1/2)\\Delta x$:\n  - $E_i = 2.0 + 0.1 \\cos(4\\pi x_i / L)$,\n  - $F_i = 0.05 \\sin(2\\pi x_i / L)$,\n  - $N_i = 1.0 + 0.2 \\sin(2\\pi x_i / L)$.\n- Test 3 (edge case with extreme magnitudes and zeros): $L = 1.0$, $n = 4$, $r = 3$, coarse moments $E_i = 1.0$ for all $i$, $F_i = 0.0$ for all $i$, and $N = [10^{-300}, 10^{5}, 2.0, 0.0]$.\n- Test 4 (strong gradients, positivity stress): $L = 1.0$, $n = 10$, $r = 5$, coarse moments initialized with $E_i = 1.0 + 0.01 i$, $F_i = 0.3 i (-1)^i$, and $N_i = i^2$ for $i = 0,1,\\dots,9$.\n\nFinal output format:\n- Your program should produce a single line of output containing the absolute $L^1$ photon-number differences for Test 1 through Test 4, as a comma-separated list enclosed in square brackets, in units of photons, e.g., \"[d1,d2,d3,d4]\".",
            "solution": "The problem statement requires the implementation and verification of a conservative remapping algorithm for radiation moment vectors on a one-dimensional periodic grid undergoing Adaptive Mesh Refinement (AMR). The core task is to demonstrate that the total photon number, represented by the $L^1$ norm of the photon number density $N$, is conserved to machine precision after a cycle of grid refinement followed by coarsening.\n\nThe principle of conservation is paramount in numerical solutions to physical laws expressed as continuity equations. A numerical scheme is conservative if the total integrated quantity within the computational domain remains constant in the absence of sources, sinks, or fluxes through the domain boundary. For AMR, this means that the total quantity in a parent cell must equal the sum of the quantities in its child cells after refinement, and vice versa for coarsening. The proposed algorithm is designed to be perfectly conservative by construction.\n\nLet the one-dimensional periodic domain have length $L$, discretized into $n$ coarse cells. Each coarse cell $i$ (for $i \\in \\{0, \\dots, n-1\\}$) has a width $\\Delta x = L/n$ and its center is at $x_i = (i+1/2)\\Delta x$. The state of the system is described by cell-averaged moment vectors $M_i = (E_i, F_i, N_i)$. The total photon number in the domain, which is equivalent to the $L^1$ norm of $N$ since $N \\ge 0$, is given by $\\sum_{i=0}^{n-1} N_i \\Delta x$.\n\nThe AMR operations are defined as follows:\n\n**1. Refinement (Coarse-to-Fine Mapping)**\n\nRefinement replaces a single coarse parent cell with $r$ smaller, equal-width child cells, where $r$ is the refinement factor. Each child cell has a width $\\Delta x_f = \\Delta x / r$. To assign values to these new fine cells while maintaining conservation, a sub-grid reconstruction of the data is performed within each parent cell.\n\n- **Piecewise-Linear Reconstruction**: For each moment component (e.g., $E, F,$ or $N$), a linear function $m(x)$ is reconstructed within each coarse cell $i$. This function is chosen such that its average value over the cell is equal to the known coarse cell average, $a_i$. The form of the reconstruction is $m(x) = a_i + s_i (x - x_i)$, where $s_i$ is a carefully chosen slope. Taking the integral of $m(x)$ over cell $i$, from $x_i - \\Delta x/2$ to $x_i + \\Delta x/2$, confirms that the average is $a_i$, as the integral of the odd term $s_i(x-x_i)$ is zero.\n\n- **Slope Limiting**: A naive slope calculation could introduce spurious oscillations (Gibbs phenomenon) near sharp gradients. To prevent this, a slope limiter is employed. The problem specifies the MinMod limiter, which is a standard choice for ensuring monotonicity. The slope $s_i$ is calculated by comparing the backward and forward difference slopes:\n$$s_i = \\operatorname{minmod}\\left(\\frac{a_i - a_{i-1}}{\\Delta x}, \\frac{a_{i+1} - a_i}{\\Delta x}\\right)$$\nThe neighbors $a_{i-1}$ and $a_{i+1}$ are determined using periodic boundary conditions. The $\\operatorname{minmod}(u,v)$ function returns the argument with the smallest absolute value if both have the same sign, and zero otherwise. This choice flattens the reconstruction at local extrema, preventing overshoots and undershoots.\n\n- **Positivity Preservation**: The photon number density $N$ is a physically non-negative quantity. The linear reconstruction $N(x) = N_i + s_i(x - x_i)$ could potentially yield negative values at the edges of the cell if the slope $s_i$ is too steep. To enforce positivity, the magnitude of the slope for the $N$ component is further constrained. The reconstructed value must be non-negative at all fine-cell midpoints. The maximum distance of a fine-cell midpoint from its parent-cell center is $\\delta_{\\max}$. For a parent cell with $r$ children, this distance is $\\delta_{\\max} = \\frac{\\Delta x}{2}(1 - 1/r)$. The condition $N(x) \\ge 0$ for all evaluation points within the cell implies $|s_i| \\le N_i / \\delta_{\\max}$. This bound is applied to the slope $s_i$ after it has been calculated by the MinMod limiter.\n\n- **Fine Cell Assignment**: The problem specifies that the value assigned to a fine child cell is the value of the linear reconstruction evaluated at the child cell's midpoint. A key property of linear functions is that the value at the midpoint of an interval is exactly equal to the average value of the function over that interval. Therefore, this assignment method accurately calculates the cell average for each fine cell under the reconstructed profile. Summing the total photon number over the $r$ child cells yields $\\sum_{j=0}^{r-1} N^{\\text{fine}}_{ij} \\Delta x_f = \\sum_{j=0}^{r-1} [N_i + s_i(x_{ij} - x_i)] \\Delta x_f = r N_i \\Delta x_f + s_i \\Delta x_f \\sum_{j=0}^{r-1} (x_{ij} - x_i)$. Due to the symmetric placement of the child cells, the sum of the offsets $\\sum (x_{ij} - x_i)$ is zero. Thus, the total photon number in the children is $r N_i \\Delta x_f = r N_i (\\Delta x / r) = N_i \\Delta x$, which is exactly the photon number of the parent cell. The refinement step is therefore perfectly conservative.\n\n**2. Coarsening (Fine-to-Coarse Mapping)**\n\nCoarsening is the inverse operation of refinement. It maps the values from $r$ child cells back to a single parent cell. The principle of conservation dictates that the coarsened value must be the volume-weighted average of the child cell values. Since all child cells have the same width $\\Delta x_f$, this simplifies to the arithmetic mean:\n$$a_i^{\\text{coarse}} = \\frac{1}{r} \\sum_{j=0}^{r-1} a_{ij}^{\\text{fine}}$$\nThe total quantity in the new coarse cell is $a_i^{\\text{coarse}} \\Delta x = (\\frac{1}{r} \\sum_{j=0}^{r-1} a_{ij}^{\\text{fine}}) (r \\Delta x_f) = \\sum_{j=0}^{r-1} a_{ij}^{\\text{fine}} \\Delta x_f$, which is simply the sum of the quantities in the fine cells. Thus, the coarsening step is also perfectly conservative.\n\n**3. Verification of Conservation**\n\nSince both the refinement and coarsening operations are mathematically designed to be perfectly conservative, performing a cycle of refinement followed by coarsening on an initial dataset should return the original dataset exactly. The program will compute the total photon number before this cycle, $I_{\\text{initial}} = \\sum (N_i \\Delta x)$, and after, $I_{\\text{final}} = \\sum (N'_i \\Delta x)$. The absolute difference $|I_{\\text{initial}} - I_{\\text{final}}|$ should be zero, within the limits of floating-point arithmetic precision. The implemented code will calculate this difference for several test cases to numerically demonstrate this conservation property.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the AMR remapping simulation for all test cases.\n    \"\"\"\n\n    def minmod(u, v):\n        \"\"\"\n        Calculates the MinMod function.\n        Returns the argument with the smallest absolute value if signs are the same,\n        otherwise returns 0.\n        \"\"\"\n        if u * v <= 0:\n            return 0.0\n        elif abs(u) < abs(v):\n            return u\n        else:\n            return v\n\n    def remap(coarse_moments, L, n, r):\n        \"\"\"\n        Performs a full refinement-then-coarsening cycle.\n        \n        Args:\n            coarse_moments (np.array): Array of shape (n, 3) for (E, F, N).\n            L (float): Domain length.\n            n (int): Number of coarse cells.\n            r (int): Refinement factor.\n\n        Returns:\n            np.array: The new coarse moments array of shape (n, 3) after the cycle.\n        \"\"\"\n        \n        # --- Refinement Step ---\n        dx = L / n\n        dx_f = dx / r\n        num_fine_cells = n * r\n        fine_moments = np.zeros((num_fine_cells, 3))\n        \n        # Max offset for positivity constraint on N\n        # This is max(|x_fine_midpoint - x_coarse_midpoint|)\n        # delta_max is 0 if r=1, but problem constraints ensure r>1\n        delta_max = (dx / 2.0) * (1.0 - 1.0 / r) if r > 1 else 0.0\n\n        # Process each moment component (E, F, N)\n        for k in range(3):\n            moment_component = coarse_moments[:, k]\n            \n            for i in range(n):\n                # Periodic boundary conditions for neighbors\n                i_prev = (i - 1 + n) % n\n                i_next = (i + 1) % n\n                \n                a_i = moment_component[i]\n                a_prev = moment_component[i_prev]\n                a_next = moment_component[i_next]\n\n                # Calculate slope with MinMod limiter\n                s_i = minmod((a_i - a_prev) / dx, (a_next - a_i) / dx)\n\n                # Enforce positivity for the N component (k=2)\n                if k == 2 and a_i > 0 and delta_max > 0:\n                    max_slope_mag = a_i / delta_max\n                    s_i = np.sign(s_i) * min(abs(s_i), max_slope_mag)\n                elif k == 2 and a_i == 0:\n                    s_i = 0.0\n\n                # Assign values to the r fine child cells\n                for j in range(r):\n                    # Offset of fine cell midpoint from coarse cell midpoint\n                    offset = dx * ((j + 0.5) / r - 0.5)\n                    fine_val = a_i + s_i * offset\n                    \n                    fine_idx = i * r + j\n                    fine_moments[fine_idx, k] = fine_val\n\n        # --- Coarsening Step ---\n        coarsened_moments = np.zeros((n, 3))\n        for i in range(n):\n            # The children of coarse cell i are in the slice [i*r : (i+1)*r]\n            start_idx = i * r\n            end_idx = (i + 1) * r\n            \n            # Average the fine cell values to get the new coarse cell value\n            for k in range(3):\n                coarsened_moments[i, k] = np.mean(fine_moments[start_idx:end_idx, k])\n                \n        return coarsened_moments\n\n\n    test_cases = [\n        {\n            \"L\": 1.0, \"n\": 8, \"r\": 2,\n            \"moments\": np.array([[10.0, 0.5, 3.0] for _ in range(8)])\n        },\n        {\n            \"L\": 2.0, \"n\": 16, \"r\": 4,\n            \"init_func\": lambda x, L: [\n                2.0 + 0.1 * np.cos(4 * np.pi * x / L),\n                0.05 * np.sin(2 * np.pi * x / L),\n                1.0 + 0.2 * np.sin(2 * np.pi * x / L)\n            ]\n        },\n        {\n            \"L\": 1.0, \"n\": 4, \"r\": 3,\n            \"moments\": np.array([\n                [1.0, 0.0, 1e-300],\n                [1.0, 0.0, 1e5],\n                [1.0, 0.0, 2.0],\n                [1.0, 0.0, 0.0]\n            ])\n        },\n        {\n            \"L\": 1.0, \"n\": 10, \"r\": 5,\n            \"moments\": np.array([\n                [1.0 + 0.01 * i, 0.3 * i * ((-1)**i), float(i**2)]\n                for i in range(10)\n            ])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        L = case[\"L\"]\n        n = case[\"n\"]\n        r = case[\"r\"]\n        dx = L / n\n        \n        if \"moments\" in case:\n            initial_moments = case[\"moments\"]\n        else:\n            # Initialize moments from function\n            x_centers = (np.arange(n) + 0.5) * dx\n            initial_moments = np.array([case[\"init_func\"](x, L) for x in x_centers])\n\n        # Get initial photon number density vector N\n        N_initial = initial_moments[:, 2]\n        \n        # Calculate total initial photon number (L1 norm of N)\n        total_N_initial = np.sum(N_initial) * dx\n\n        # Perform the remapping cycle\n        final_moments = remap(initial_moments, L, n, r)\n        \n        # Get final photon number density vector N\n        N_final = final_moments[:, 2]\n        \n        # Calculate total final photon number\n        total_N_final = np.sum(N_final) * dx\n        \n        # Calculate the absolute difference to check for conservation\n        abs_difference = abs(total_N_initial - total_N_final)\n        results.append(abs_difference)\n    \n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}