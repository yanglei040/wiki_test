{
    "hands_on_practices": [
        {
            "introduction": "To effectively simulate physical systems with features at vastly different scales, such as shock waves in fluid dynamics, we must resolve different spatial regions with different time steps. This exercise provides foundational, hands-on experience in implementing a multirate time-stepping scheme for a finite-volume method . The core challenge, and the focus of this practice, is to ensure that the fundamental principle of conservation is strictly maintained at the interface between regions advancing with fine and coarse time steps by carefully accumulating and communicating numerical fluxes.",
            "id": "3516710",
            "problem": "Consider a one-dimensional, periodic, finite-volume discretization of a scalar conservation law with constant-coefficient linear advection. The governing equation is the conservation law in integral form over each control volume: the rate of change of the cell average equals the net flux through its boundaries. Begin from the conservation principle that, for a control volume with cell index $i$ and width $\\Delta x$, the cell average $u_i(t)$ evolves according to\n$$\n\\frac{d}{dt}\\left(u_i(t)\\right) = -\\frac{1}{\\Delta x}\\left(f_{i+\\frac{1}{2}}(t) - f_{i-\\frac{1}{2}}(t)\\right),\n$$\nwhere $f_{i+\\frac{1}{2}}(t)$ is the physical flux at the right interface of cell $i$. For linear advection with constant velocity $a$ (dimensionless), the flux is $f(x,t)=a\\,u(x,t)$. On a uniform grid with $N$ cells and periodic boundary conditions, denote the interface between cells $i$ and $i+1$ by the index $i+\\frac{1}{2}$, with periodic wrap so that cell index arithmetic is modulo $N$.\n\nA first-order, upwind, explicit finite-volume method with constant advection speed $a$ uses the numerical flux\n$$\nF_{i+\\frac{1}{2}}(t) = \n\\begin{cases}\na\\,u_i(t), & a \\ge 0,\\\\\na\\,u_{i+1}(t), & a < 0,\n\\end{cases}\n$$\nthat is, $F_{i+\\frac{1}{2}}(t) = a\\,u_{\\text{upwind}}(t)$, where $u_{\\text{upwind}}(t)$ is the upwind cell average at time $t$.\n\nYou are to design a multirate local time-stepping scheme with subcycling for flux updates that couples cells advancing with a fine time step to cells advancing with a coarse time step over one macro-step. Let a Boolean mask indicate which cells are fine and which are coarse. All fine cells advance using $m$ uniform substeps of size $\\Delta t_f$, while all coarse cells advance once with the macro-step $\\Delta t_c = m\\,\\Delta t_f$. The Courant–Friedrichs–Lewy condition must be enforced in all regions, namely $|a|\\,\\Delta t_f/\\Delta x \\le 1$ and $|a|\\,\\Delta t_c/\\Delta x \\le 1$, to ensure numerical stability for the upwind scheme.\n\nYour tasks:\n\n1. From the conservation law and the definition of the numerical flux, justify why the coarse-cell update over the macro-step must employ the time-integrated interface fluxes accumulated during the $m$ substeps taken by the fine cells, i.e., for each interface $i+\\frac{1}{2}$, the time integral $\\int_{t^n}^{t^{n}+\\Delta t_c} F_{i+\\frac{1}{2}}(t)\\,dt$ should be approximated by the accumulated sum $\\sum_{k=1}^m F_{i+\\frac{1}{2}}^{(k)}\\,\\Delta t_f$, where $F_{i+\\frac{1}{2}}^{(k)}$ is the numerical flux evaluated during substep $k$. Use this to write a conservative update for every coarse cell over the macro-step in terms of these accumulated flux sums.\n\n2. For fine cells, derive the $m$-substep update using the upwind flux at each substep with time step $\\Delta t_f$, and apply periodic boundary conditions.\n\n3. Implement a program that, for each test case below, advances the system by exactly one macro-step $\\Delta t_c = m\\,\\Delta t_f$ using the multirate subcycling strategy described. The implementation must:\n- Maintain a running accumulation of the interface flux integrals $S_{i+\\frac{1}{2}} = \\sum_{k=1}^m F_{i+\\frac{1}{2}}^{(k)}\\,\\Delta t_f$ during the $m$ substeps.\n- Update only the fine cells during each substep using the explicit upwind method.\n- After completing the $m$ substeps, update each coarse cell exactly once using the accumulated $S_{i\\pm\\frac{1}{2}}$ to ensure conservation.\n- Use periodic boundaries, so that index arithmetic is modulo $N$.\n\nAll quantities are nondimensional. Angles do not appear, and no percentages are used. The final outputs are cell averages, which are real numbers without units. For numerical output, round each cell average to $8$ decimal places.\n\nTest suite:\n\nFor each test case, you are given the tuple $(N,\\Delta x,a,\\Delta t_f,m,\\text{fine\\_mask},u^0)$, where $N$ is the number of cells, $\\Delta x$ is the cell width, $a$ is the advection speed, $\\Delta t_f$ is the fine time step, $m$ is the number of fine substeps per macro-step, $\\text{fine\\_mask}$ is a Boolean list of length $N$ where a value of $\\text{True}$ marks a fine cell and a value of $\\text{False}$ marks a coarse cell, and $u^0$ is the list of initial cell averages at time $t^n$. The macro-step is $\\Delta t_c=m\\,\\Delta t_f$. The tests are:\n\n- **Case 1:** `(N=4, dx=1.0, a=1.0, dt_f=0.4, m=2, fine_mask=[True, True, False, False], u0=[1.0, 2.0, 3.0, 4.0])`\n- **Case 2:** `(N=4, dx=1.0, a=-1.0, dt_f=0.3, m=3, fine_mask=[False, True, True, False], u0=[0.0, 1.0, 0.0, 0.0])`\n- **Case 3:** `(N=4, dx=1.0, a=0.0, dt_f=0.2, m=5, fine_mask=[True, False, True, False], u0=[0.5, 0.0, -0.5, 1.0])`\n- **Case 4:** `(N=4, dx=1.0, a=2.0, dt_f=0.5, m=1, fine_mask=[True, True, True, True], u0=[1.0, 0.0, 0.0, 0.0])`\n- **Case 5:** `(N=4, dx=1.0, a=1.0, dt_f=0.25, m=4, fine_mask=[False, False, True, True], u0=[1.0, 0.0, 0.0, 0.0])`\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is itself a bracketed, comma-separated list of the $N$ updated cell averages (at time $t^{n}+\\Delta t_c$) for that test case, rounded to $8$ decimal places. For example, the output format must be exactly like $[[\\dots],[\\dots],\\dots]$ with no spaces.",
            "solution": "We begin with the finite-volume statement of conservation for a one-dimensional control volume of width $\\Delta x$ with periodic boundaries. For cell index $i$, let $u_i(t)$ denote the cell average of the conserved quantity. The conservation law gives\n$$\n\\frac{d}{dt}\\left(u_i(t)\\right) = -\\frac{1}{\\Delta x}\\left(f_{i+\\frac{1}{2}}(t) - f_{i-\\frac{1}{2}}(t)\\right).\n$$\nFor linear advection with constant speed $a$, the physical flux is $f(x,t) = a\\,u(x,t)$. A standard first-order upwind numerical flux for constant $a$ is\n$$\nF_{i+\\frac{1}{2}}(t) = a\\,u_{\\text{upwind}}(t) = \n\\begin{cases}\na\\,u_i(t), & a \\ge 0,\\\\\na\\,u_{i+1}(t), & a < 0.\n\\end{cases}\n$$\nDiscretizing in time with an explicit Euler method over a step $\\Delta t$ yields the well-known first-order upwind finite-volume update\n$$\nu_i^{n+1} = u_i^n - \\frac{\\Delta t}{\\Delta x}\\left(F_{i+\\frac{1}{2}}^n - F_{i-\\frac{1}{2}}^n\\right).\n$$\nFor stability of this explicit upwind method, the Courant–Friedrichs–Lewy condition must hold:\n$$\n\\frac{|a|\\,\\Delta t}{\\Delta x} \\le 1.\n$$\n\nTo construct a multirate local time-stepping method with subcycling, we partition the set of cells into two subsets: fine cells (to be advanced with a small step $\\Delta t_f$) and coarse cells (to be advanced with a larger step $\\Delta t_c$). Let there be an integer $m \\ge 1$ such that $\\Delta t_c = m\\,\\Delta t_f$. Consider advancing the solution by exactly one macro-step $\\Delta t_c$ starting from time $t^n$. The scheme proceeds as follows.\n\nFor each substep index $k=1,\\dots,m$, we do:\n- Evaluate the numerical fluxes $F_{i+\\frac{1}{2}}^{(k)}$ at time $t^{n} + (k-1)\\Delta t_f$ using the upwind rule and the current cell averages. Fine cells will have been updated in previous substeps, while coarse cells remain at their initial value $u_i^n$ throughout the $m$ substeps.\n- Update each fine cell $i$ according to the explicit upwind update over $\\Delta t_f$:\n$$\nu_i^{(k)} = u_i^{(k-1)} - \\frac{\\Delta t_f}{\\Delta x}\\left(F_{i+\\frac{1}{2}}^{(k)} - F_{i-\\frac{1}{2}}^{(k)}\\right),\n$$\nwith $u_i^{(0)} = u_i^n$. This leaves the coarse cells unchanged during subcycling.\n- Accumulate the time-integrated interface fluxes over the macro-step using the Riemann sum\n$$\nS_{i+\\frac{1}{2}} \\leftarrow S_{i+\\frac{1}{2}} + F_{i+\\frac{1}{2}}^{(k)}\\,\\Delta t_f,\n$$\ninitializing $S_{i+\\frac{1}{2}}=0$ at the start of the macro-step.\n\nAfter completing all $m$ substeps, we update each coarse cell $j$ once over the macro-step by using the time-integrated fluxes to maintain conservation:\n$$\nu_j^{n+1} = u_j^n - \\frac{1}{\\Delta x}\\left(S_{j+\\frac{1}{2}} - S_{j-\\frac{1}{2}}\\right).\n$$\nThis formula follows directly from integrating the conservation law over the time interval $[t^n,t^n+\\Delta t_c]$:\n$$\nu_j^{n+1} - u_j^n = -\\frac{1}{\\Delta x}\\int_{t^n}^{t^n+\\Delta t_c} \\left(F_{j+\\frac{1}{2}}(t) - F_{j-\\frac{1}{2}}(t)\\right)\\,dt,\n$$\nand approximating each interface time integral $\\int F\\,dt$ by the accumulated Riemann sum $S$ constructed from the substeps. This construction ensures that the flux transmitted across any interface over the macro-step is consistently used by both adjacent cells: for fine cells through their substep updates and for coarse cells through the integrated update, guaranteeing conservation at the discrete level.\n\nPeriodic boundary conditions are enforced by taking all cell indices modulo $N$, so that $i+1$ wraps from $N-1$ to $0$, and $i-1$ wraps from $0$ to $N-1$. The interface index $i+\\frac{1}{2}$ is represented by the integer $i$ for the interface between cells $i$ and $i+1$.\n\nStability: Because coarse cells are not updated during substeps, their interface fluxes with other coarse neighbors remain constant during the macro-step and their contribution to $S$ is exactly $F\\,\\Delta t_c$ provided by the repeated accumulation. The explicit updates applied to fine cells require that the fine Courant number $|a|\\,\\Delta t_f/\\Delta x \\le 1$, and for coarse–coarse interactions the coarse Courant number $|a|\\,\\Delta t_c/\\Delta x \\le 1$ maintains stability over the macro-step.\n\nAlgorithmic steps for the program for each test case:\n- Initialize the state $u^0$ at time $t^n$.\n- Initialize the flux integrals $S_{i+\\frac{1}{2}}=0$ for all interfaces $i=0,\\dots,N-1$.\n- For $k=1$ to $m$:\n    - Compute all interface fluxes $F_{i+\\frac{1}{2}}$ using the upwind rule with the current state (fine cells updated, coarse cells still at $u^n$).\n    - Accumulate $S_{i+\\frac{1}{2}} \\leftarrow S_{i+\\frac{1}{2}} + F_{i+\\frac{1}{2}}\\,\\Delta t_f$ for all interfaces.\n    - Update each fine cell using $u_i \\leftarrow u_i - (\\Delta t_f/\\Delta x)\\,(F_{i+\\frac{1}{2}} - F_{i-\\frac{1}{2}})$.\n- After $m$ substeps, set the macro-step solution $u^{n+1}$ by:\n    - For fine cells, use their current values after $m$ substeps.\n    - For coarse cells, compute $u_j^{n+1} = u_j^n - (1/\\Delta x)\\,(S_{j+\\frac{1}{2}} - S_{j-\\frac{1}{2}})$ using the accumulated $S$.\n\nFinally, round each component of $u^{n+1}$ to $8$ decimal places and output the list of cell averages for each test case. The output formatting must be a single line that is a list of lists without any spaces, i.e., of the form $[[\\dots],[\\dots],\\dots]$.\n\nThis method satisfies conservation and respects multirate subcycling for finite-volume flux updates by consistently integrating interface fluxes over the macro-step and applying them to both fine and coarse cells in a way derived directly from the fundamental conservation law.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef upwind_flux(u, a):\n    \"\"\"\n    Compute upwind numerical fluxes F_{i+1/2} for constant advection speed a\n    on a periodic 1D grid. Interface index i corresponds to interface between\n    cell i and cell (i+1) mod N.\n    \"\"\"\n    N = u.size\n    F = np.empty(N, dtype=float)\n    if a >= 0.0:\n        # Upwind from the left cell: F_{i+1/2} = a * u_i\n        F[:] = a * u\n    else:\n        # Upwind from the right cell: F_{i+1/2} = a * u_{i+1}\n        F[:] = a * np.roll(u, -1)\n    return F\n\ndef multirate_one_macrostep(u0, dx, a, dt_f, m, fine_mask):\n    \"\"\"\n    Advance solution by one macro-step dt_c = m * dt_f using multirate subcycling.\n    - u0: initial cell averages (numpy array, shape (N,))\n    - dx: cell width\n    - a: advection speed (constant)\n    - dt_f: fine time step\n    - m: number of fine substeps per macro-step\n    - fine_mask: boolean numpy array of length N; True for fine cells, False for coarse\n    Returns: u1 (numpy array), the solution after one macro-step\n    \"\"\"\n    N = u0.size\n    u_current = u0.copy()\n    # Accumulate time-integrated flux at each interface over the macro-step\n    sumFdt = np.zeros(N, dtype=float)\n    # Subcycling: fine cells advance m times; coarse cells remain frozen during substeps\n    for _ in range(m):\n        F = upwind_flux(u_current, a)\n        sumFdt += F * dt_f\n        # Update only fine cells explicitly with upwind scheme over dt_f\n        # u_i -= (dt_f/dx)*(F_{i+1/2} - F_{i-1/2})\n        if np.any(fine_mask):\n            # Compute flux divergence term\n            divF = F - np.roll(F, 1)\n            u_current[fine_mask] -= (dt_f / dx) * divF[fine_mask]\n        # Coarse cells remain unchanged in u_current during subcycling\n    # After subcycling, fine cells are already at macro-step time.\n    # Update coarse cells once using the accumulated flux integrals sumFdt.\n    u1 = u_current.copy()\n    if np.any(~fine_mask):\n        divS = sumFdt - np.roll(sumFdt, 1)\n        u1[~fine_mask] = u0[~fine_mask] - (1.0 / dx) * divS[~fine_mask]\n    return u1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (N, dx, a, dt_f, m, fine_mask, u0)\n    test_cases = [\n        (4, 1.0, 1.0, 0.4, 2, [True, True, False, False], [1.0, 2.0, 3.0, 4.0]),\n        (4, 1.0, -1.0, 0.3, 3, [False, True, True, False], [0.0, 1.0, 0.0, 0.0]),\n        (4, 1.0, 0.0, 0.2, 5, [True, False, True, False], [0.5, 0.0, -0.5, 1.0]),\n        (4, 1.0, 2.0, 0.5, 1, [True, True, True, True], [1.0, 0.0, 0.0, 0.0]),\n        (4, 1.0, 1.0, 0.25, 4, [False, False, True, True], [1.0, 0.0, 0.0, 0.0]),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, dx, a, dt_f, m, fine_mask_list, u0_list = case\n        fine_mask = np.array(fine_mask_list, dtype=bool)\n        u0 = np.array(u0_list, dtype=float)\n        # Sanity checks for dimensions\n        assert u0.size == N\n        assert fine_mask.size == N\n        # Perform one macro-step\n        u1 = multirate_one_macrostep(u0, dx, a, dt_f, m, fine_mask)\n        # Round to 8 decimals\n        u1_rounded = [float(f\"{val:.8f}\") for val in u1.tolist()]\n        results.append(u1_rounded)\n\n    # Format the output exactly as required: single line, no spaces, nested lists\n    def format_results(res):\n        def fmt_list(lst):\n            return \"[\" + \",\".join(f\"{x:.8f}\" for x in lst) + \"]\"\n        return \"[\" + \",\".join(fmt_list(lst) for lst in res) + \"]\"\n\n    print(format_results(results))\n\nsolve()\n```"
        },
        {
            "introduction": "In modern multiphysics engineering, complex systems are often simulated by coupling specialized, standalone solvers in a co-simulation framework. A critical challenge in this approach is the numerical instability that can arise from the communication lag between solvers operating on different clocks. This practice explores this issue by asking you to analyze the stability of a coupled system where one component subcycles, using extrapolation to predict the coupling data and mitigate the lag . By constructing the system's amplification matrix and computing its spectral radius, you will learn how to formally assess the stability of a co-simulation scheme, a vital skill for designing robust multiphysics workflows.",
            "id": "3516696",
            "problem": "Consider a Functional Mock-up Interface (FMI) co-simulation of two linearly coupled subsystems with independent clocks $t_1$ and $t_2$ that synchronize at macro times $t^n = t_0 + n H$, where $H$ is the macro-step size in seconds. Subsystem $1$ (clock $t_1$) advances its state only at macro times, while Subsystem $2$ (clock $t_2$) subcycles between macro times with a fine-step size $\\delta t = H/m$ where $m \\in \\mathbb{N}$ is the number of subcycles. The continuous-time models are\n$$\n\\frac{dx}{dt} = -\\alpha x,\\qquad \\frac{dy}{dt} = -\\beta y + \\gamma\\, u(t),\n$$\nwhere $x(t)$ is the state of Subsystem $1$, $y(t)$ is the state of Subsystem $2$, and $u(t)$ is the coupling input received by Subsystem $2$ that is constructed from Subsystem $1$'s macro samples by an extrapolation operator. The constants $\\alpha$, $\\beta$, and $\\gamma$ are nonnegative real numbers with units of inverse seconds, $\\mathrm{s}^{-1}$. Time is measured in seconds.\n\nThe co-simulation protocol over one macro interval $[t^n, t^{n+1}]$ is:\n- Subsystem $1$ advances its state from $x^n = x(t^n)$ to $x^{n+1} = x(t^{n+1})$ using the Forward Euler method with macro-step size $H$.\n- Subsystem $2$ advances its state from $y^n = y(t^n)$ to $y^{n+1} = y(t^{n+1})$ using the Forward Euler method with fine-step size $\\delta t = H/m$ over $m$ substeps, consuming at each fine substep an extrapolated input $u(t)$ constructed from past macro samples of $x$ via a degree-$k$ extrapolation operator $\\mathcal{E}_k$.\n\nLet $\\theta \\in [0,1]$ denote the normalized time within the macro interval, defined by $\\theta = (t - t^n)/H$. The extrapolation operator $\\mathcal{E}_k$ uses the most recent $k+1$ macro samples $\\{x^n, x^{n-1}, \\dots, x^{n-k}\\}$ to construct a polynomial of degree $k$ in $\\theta$ that extrapolates forward from $\\theta = 0$ to $\\theta \\in [0,1]$. Specifically:\n- For $k=0$ (zero-order hold), $\\mathcal{E}_0$ is constant over the macro interval: $u(t) = x^n$.\n- For $k=1$ (linear extrapolation), $\\mathcal{E}_1$ is the Lagrange polynomial using nodes at $\\theta=0$ and $\\theta=-1$: $u(t) = (\\theta+1)\\,x^n - \\theta\\,x^{n-1}$.\n- For $k=2$ (quadratic extrapolation), $\\mathcal{E}_2$ is the Lagrange polynomial using nodes at $\\theta=0,-1,-2$: $u(t) = \\tfrac{1}{2}(\\theta^2 + 3\\theta + 2)\\,x^n - (\\theta^2 + 2\\theta)\\,x^{n-1} + \\tfrac{1}{2}(\\theta^2 + \\theta)\\,x^{n-2}$.\n\nThe numerical integration schemes are defined by the Forward Euler updates:\n$$\nx^{n+1} = x^n + H\\,(-\\alpha x^n),\n$$\nand for Subsystem $2$ with $m$ substeps indexed by $j=0,1,\\dots,m-1$ at times $t_j = t^n + j \\delta t$ (so $\\theta_j = j/m$),\n$$\ny_{j+1} = y_j + \\delta t \\left(-\\beta y_j + \\gamma\\,u(t_j)\\right),\\qquad y_0 = y^n,\\qquad y^{n+1} = y_m.\n$$\nThis FMI co-simulation defines a macro-step linear mapping from the macro state at time $t^n$ to the macro state at time $t^{n+1}$. Depending on the extrapolation order $k$, the macro-step mapping is over an augmented state that includes past macro samples of Subsystem $1$ needed by $\\mathcal{E}_k$. The linear amplification matrix for one macro step is denoted by $M_k$, and stability of the coupled co-simulation under repeated macro steps is determined by the spectral radius $\\rho(M_k)$; the macro update is stable if and only if $\\rho(M_k) < 1$. The spectral radius is dimensionless.\n\nTask:\n1. Starting from the continuous-time model and the defined numerical schemes, derive the exact macro-step amplification matrix $M_k$ for $k \\in \\{0,1,2\\}$ using the described extrapolation operators $\\mathcal{E}_k$ and Forward Euler updates. Your derivation must use only fundamental laws and core definitions, together with well-tested formulas. Express all mathematical objects in proper LaTeX.\n2. Implement a program that, for given parameters $(\\alpha,\\beta,\\gamma,H,m,k)$, constructs $M_k$ and computes the spectral radius $\\rho(M_k)$.\n3. Use the following test suite of parameter sets, all with time in seconds and rates in inverse seconds, to evaluate $\\rho(M_k)$:\n   - Test Case 1 (happy path, subcycling with zero-order hold): $(\\alpha,\\beta,\\gamma,H,m,k) = (5,\\,40,\\,8,\\,0.02,\\,10,\\,0)$.\n   - Test Case 2 (same as Case 1, linear extrapolation): $(5,\\,40,\\,8,\\,0.02,\\,10,\\,1)$.\n   - Test Case 3 (same as Case 1, quadratic extrapolation): $(5,\\,40,\\,8,\\,0.02,\\,10,\\,2)$.\n   - Test Case 4 (boundary-like macro step without subcycling, zero-order hold): $(5,\\,40,\\,8,\\,0.0499,\\,1,\\,0)$.\n   - Test Case 5 (fine subcycling with zero-order hold): $(5,\\,40,\\,8,\\,0.05,\\,50,\\,0)$.\n4. Your program should produce a single line of output containing the spectral radii for the five test cases as a comma-separated list enclosed in square brackets, ordered as listed above. For example, the output format must be exactly like: \"[result1,result2,result3,result4,result5]\". Each \"result\" must be a floating-point number.\n\nAll outputs are dimensionless floats. Do not use any units in the output. Time inputs $\\alpha$, $\\beta$, and $\\gamma$ must be interpreted in $\\mathrm{s}^{-1}$, while $H$ and $\\delta t$ must be interpreted in seconds. Angles do not appear in this problem. Percentages must not be used anywhere; express comparisons numerically.\n\nThe problem requires the derivation of the macro-step amplification matrices from first principles and the computation of the spectral radius to assess stability. Avoid shortcut formulas in the problem statement; rigorous derivation is expected in the solution.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of numerical analysis for ordinary differential equations, specifically in the context of co-simulation and multirate methods. The problem is well-posed, objective, and contains all necessary information to derive a unique, verifiable solution.\n\nThe objective is to derive the macro-step amplification matrix $M_k$ for a coupled system and compute its spectral radius $\\rho(M_k)$ for $k \\in \\{0, 1, 2\\}$. The stability of the co-simulation scheme is determined by the condition $\\rho(M_k) < 1$. The derivation proceeds from first principles as specified.\n\nThe system is described by two coupled linear ordinary differential equations:\n$$\n\\frac{dx}{dt} = -\\alpha x\n$$\n$$\n\\frac{dy}{dt} = -\\beta y + \\gamma\\, u(t)\n$$\nSubsystem $1$ (state $x$) is advanced with a Forward Euler step of size $H$ at each macro-step $n$:\n$$\nx^{n+1} = x^n + H(-\\alpha x^n) = (1 - \\alpha H) x^n\n$$\nSubsystem $2$ (state $y$) is advanced with $m$ Forward Euler sub-steps of size $\\delta t = H/m$ over the macro-interval $[t^n, t^{n+1}]$. For a sub-step $j \\in \\{0, 1, \\dots, m-1\\}$ at time $t_j = t^n + j\\delta t$:\n$$\ny_{j+1} = y_j + \\delta t(-\\beta y_j + \\gamma u(t_j)) = (1 - \\beta \\delta t) y_j + \\gamma \\delta t u(t_j)\n$$\nwith $y_0 = y^n$. Let $A_y = 1 - \\beta \\delta t$. The solution of this linear recurrence relation after $m$ steps is:\n$$\ny^{n+1} = y_m = A_y^m y_n + \\gamma \\delta t \\sum_{j=0}^{m-1} A_y^{m-1-j} u(t_j)\n$$\nThe coupling input $u(t_j)$ is an extrapolation of degree $k$ based on past macro-states $\\{x^n, x^{n-1}, \\ldots, x^{n-k}\\}$. The macro-step update is a linear map $Z^{n+1} = M_k Z^n$, where $Z^n$ is an augmented state vector that includes the necessary history for the extrapolation.\n\n**Case $k=0$ (Zero-Order Hold)**\nThe extrapolation is $u(t) = x^n$. The augmented state is $Z^n = [x^n, y^n]^T$.\nThe update for $y^{n+1}$ becomes:\n$$\ny^{n+1} = A_y^m y^n + \\gamma \\delta t \\sum_{j=0}^{m-1} A_y^{m-1-j} x^n = A_y^m y^n + \\left( \\gamma \\delta t \\sum_{j=0}^{m-1} A_y^{m-1-j} \\right) x^n\n$$\nThe sum is a geometric series $\\sum_{p=0}^{m-1} A_y^p$. If $\\beta = 0$, $A_y=1$ and the sum is $m$. If $\\beta \\neq 0$, the sum is $\\frac{1-A_y^m}{1-A_y} = \\frac{1-A_y^m}{\\beta \\delta t}$. In both cases, the term multiplying $x^n$ is $\\frac{\\gamma}{\\beta}(1-A_y^m)$.\nThe macro-step equations are:\n$$\n\\begin{align*}\nx^{n+1} &= (1 - \\alpha H) x^n \\\\\ny^{n+1} &= \\frac{\\gamma}{\\beta}(1 - (1 - \\beta H/m)^m) x^n + (1 - \\beta H/m)^m y^n\n\\end{align*}\n$$\nThis defines the $2 \\times 2$ amplification matrix $M_0$:\n$$\nM_0 = \\begin{pmatrix}\n1 - \\alpha H & 0 \\\\\n\\frac{\\gamma}{\\beta}(1 - (1 - \\frac{\\beta H}{m})^m) & (1 - \\frac{\\beta H}{m})^m\n\\end{pmatrix}\n$$\n\n**Case $k=1$ (Linear Extrapolation)**\nThe extrapolation is $u(t) = (\\theta+1)x^n - \\theta x^{n-1}$, where $\\theta(t) = (t-t^n)/H$. At sub-step $j$, $\\theta_j = j/m$. The augmented state must include the history $x^{n-1}$, so $Z^n = [x^n, x^{n-1}, y^n]^T$. The next state is $Z^{n+1} = [x^{n+1}, x^n, y^{n+1}]^T$.\nThe update for $y^{n+1}$ is:\n$$\ny^{n+1} = A_y^m y^n + \\gamma \\delta t \\sum_{j=0}^{m-1} A_y^{m-1-j} \\left( (\\frac{j}{m}+1)x^n - \\frac{j}{m}x^{n-1} \\right)\n$$\nThis can be written as $y^{n+1} = C_1 x^n + D_1 x^{n-1} + A_y^m y^n$, where\n$$\nC_1 = \\gamma \\delta t \\sum_{j=0}^{m-1} A_y^{m-1-j} (1 + j/m)\n$$\n$$\nD_1 = -\\gamma \\delta t \\sum_{j=0}^{m-1} A_y^{m-1-j} (j/m)\n$$\nThe state update equations are:\n$$\n\\begin{align*}\nx^{n+1} &= (1 - \\alpha H) x^n \\\\\nx^n &= x^n \\\\\ny^{n+1} &= C_1 x^n + D_1 x^{n-1} + (1 - \\beta H/m)^m y^n\n\\end{align*}\n$$\nThis defines the $3 \\times 3$ amplification matrix $M_1$:\n$$\nM_1 = \\begin{pmatrix}\n1 - \\alpha H & 0 & 0 \\\\\n1 & 0 & 0 \\\\\nC_1 & D_1 & (1 - \\frac{\\beta H}{m})^m\n\\end{pmatrix}\n$$\nThe coefficients $C_1$ and $D_1$ are computed by numerically evaluating the sums.\n\n**Case $k=2$ (Quadratic Extrapolation)**\nThe extrapolation is $u(t) = \\tfrac{1}{2}(\\theta^2 + 3\\theta + 2)x^n - (\\theta^2 + 2\\theta)x^{n-1} + \\tfrac{1}{2}(\\theta^2 + \\theta)x^{n-2}$. The augmented state is $Z^n = [x^n, x^{n-1}, x^{n-2}, y^n]^T$, and $Z^{n+1} = [x^{n+1}, x^n, x^{n-1}, y^{n+1}]^T$.\nThe update for $y^{n+1}$ is:\n$$\ny^{n+1} = A_y^m y^n + \\gamma \\delta t \\sum_{j=0}^{m-1} A_y^{m-1-j} u(t_j)\n$$\nThis can be written as $y^{n+1} = C_2 x^n + D_2 x^{n-1} + E_2 x^{n-2} + A_y^m y^n$, where the coefficients are:\n$$\nC_2 = \\gamma \\delta t \\sum_{j=0}^{m-1} A_y^{m-1-j} \\left( \\tfrac{1}{2}(\\theta_j^2 + 3\\theta_j + 2) \\right)\n$$\n$$\nD_2 = \\gamma \\delta t \\sum_{j=0}^{m-1} A_y^{m-1-j} \\left( -(\\theta_j^2 + 2\\theta_j) \\right)\n$$\n$$\nE_2 = \\gamma \\delta t \\sum_{j=0}^{m-1} A_y^{m-1-j} \\left( \\tfrac{1}{2}(\\theta_j^2 + \\theta_j) \\right)\n$$\nwith $\\theta_j = j/m$. The state update equations are:\n$$\n\\begin{align*}\nx^{n+1} &= (1 - \\alpha H) x^n \\\\\nx^n &= x^n \\\\\nx^{n-1} &= x^{n-1} \\\\\ny^{n+1} &= C_2 x^n + D_2 x^{n-1} + E_2 x^{n-2} + (1 - \\beta H/m)^m y^n\n\\end{align*}\n$$\nThis defines the $4 \\times 4$ amplification matrix $M_2$:\n$$\nM_2 = \\begin{pmatrix}\n1 - \\alpha H & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\nC_2 & D_2 & E_2 & (1 - \\frac{\\beta H}{m})^m\n\\end{pmatrix}\n$$\nThe coefficients $C_2$, $D_2$, and $E_2$ are computed by numerically evaluating the sums. The spectral radius $\\rho(M_k)$ is then found by computing the eigenvalues of $M_k$ and finding the maximum of their magnitudes.",
            "answer": "```python\nimport numpy as np\n\ndef get_amplification_matrix(alpha, beta, gamma, H, m, k):\n    \"\"\"\n    Constructs the macro-step amplification matrix M_k.\n\n    Args:\n        alpha (float): Rate for Subsystem 1.\n        beta (float): Rate for Subsystem 2.\n        gamma (float): Coupling strength.\n        H (float): Macro-step size.\n        m (int): Number of subcycles.\n        k (int): Degree of extrapolation.\n\n    Returns:\n        numpy.ndarray: The amplification matrix M_k.\n    \"\"\"\n    if m  1:\n        raise ValueError(\"Number of subcycles m must be a positive integer.\")\n    if k not in [0, 1, 2]:\n        raise ValueError(\"Extrapolation order k must be 0, 1, or 2.\")\n\n    dt = H / m\n    Ay = 1.0 - beta * dt\n\n    # The size of the matrix is (k+1) for x_history + 1 for y\n    matrix_size = k + 2\n    M = np.zeros((matrix_size, matrix_size))\n\n    # Top-left block for x-state propagation\n    # x^{n+1} = (1 - alpha*H) * x^n\n    M[0, 0] = 1.0 - alpha * H\n    # x^{n-i+1} = x^{n-i} for i = 1..k\n    for i in range(1, k + 1):\n        M[i, i - 1] = 1.0\n\n    # Bottom-right entry for y-state decay\n    # This is the (k+1, k+1) element in 0-based indexing\n    M[k + 1, k + 1] = Ay**m\n\n    # Last row for coupling terms from x to y\n    # These are the coefficients C, D, E...\n    # y_coeffs will store [C, D, E, ...]\n    y_coeffs = np.zeros(k + 1)\n\n    # Numerically compute the sums for the coefficients\n    for j in range(m):\n        theta = j / m\n        term_Ay = Ay**(m - 1 - j)\n        \n        # Lagrange polynomial basis values at theta_j\n        if k == 0:\n            L = [1.0]\n        elif k == 1:\n            L0 = theta + 1.0\n            L1 = -theta\n            L = [L0, L1]\n        elif k == 2:\n            L0 = 0.5 * (theta**2 + 3.0 * theta + 2.0)\n            L1 = -(theta**2 + 2.0 * theta)\n            L2 = 0.5 * (theta**2 + theta)\n            L = [L0, L1, L2]\n        \n        for i in range(k + 1):\n            y_coeffs[i] += term_Ay * L[i]\n\n    # Final multiplication by gamma * dt\n    y_coeffs *= (gamma * dt)\n    \n    # Place coefficients in the last row of M\n    M[k + 1, 0:k + 1] = y_coeffs\n\n    return M\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        (5.0, 40.0, 8.0, 0.02, 10, 0),\n        (5.0, 40.0, 8.0, 0.02, 10, 1),\n        (5.0, 40.0, 8.0, 0.02, 10, 2),\n        (5.0, 40.0, 8.0, 0.0499, 1, 0),\n        (5.0, 40.0, 8.0, 0.05, 50, 0),\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, beta, gamma, H, m, k = case\n        \n        # Handle the special case of beta=0, where Ay=1 and the closed-form for k=0 fails.\n        # The numerical summation handles this naturally. However, for k=0, a\n        # closed-form solution is more efficient and direct.\n        if k == 0 and beta != 0:\n            # Using the derived closed-form for k=0\n            matrix_size = 2\n            M = np.zeros((matrix_size, matrix_size))\n            M[0, 0] = 1.0 - alpha * H\n            Ay_m = (1.0 - beta * H / m)**m\n            M[1, 1] = Ay_m\n            M[1, 0] = (gamma / beta) * (1.0 - Ay_m)\n        else:\n            # Use the general numerical summation for k=1, k=2, or k=0 with beta=0\n            M = get_amplification_matrix(alpha, beta, gamma, H, m, k)\n        \n        eigenvalues = np.linalg.eigvals(M)\n        spectral_radius = np.max(np.abs(eigenvalues))\n        results.append(spectral_radius)\n\n    # Format the output as a comma-separated list in brackets.\n    # The repr() function is used to ensure full precision floating-point representation.\n    print(f\"[{','.join(map(repr, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond fixed subcycling rates, the efficiency and accuracy of multirate methods can be significantly enhanced by using adaptive strategies. This is especially true for coupled oscillatory systems where the phase of the fast dynamics is paramount. This advanced exercise guides you through the design and implementation of an adaptive synchronization policy that intelligently selects the optimal moment to exchange data between a fast and a slow subsystem . By implementing a \"phase detector\" that aims to minimize the end-of-step error, you will gain insight into building more sophisticated coupling algorithms that respond dynamically to the system's state.",
            "id": "3516676",
            "problem": "Consider a two-field multiphysics model with a fast subsystem and a slow subsystem that are coupled at synchronization points during a macro time step. The fast subsystem is a harmonic oscillator, and the slow subsystem is a linear relaxation driven by the fast variable sampled at a selected synchronization time within each macro step. The goal is to design a multirate adaptive synchronization policy that selects synchronization times based on the fast subsystem phase to minimize the end-of-step mismatch.\n\nFundamental base:\n- The fast subsystem state $y_f(t)$ is governed by the second-order ordinary differential equation $y_f''(t) + \\omega^2 y_f(t) = 0$, where $\\omega$ is the angular frequency (in rad/s). For initial conditions $y_f(0) = A$ and $y_f'(0) = 0$, the exact solution is $y_f(t) = A \\cos(\\omega t)$, and the instantaneous phase is $\\phi(t) = \\omega t$ (radians).\n- The slow subsystem obeys the first-order linear ordinary differential equation $y_s'(t) = -k y_s(t) + \\beta y_f(t_\\mathrm{sync})$, where $k$ and $\\beta$ both have units of inverse seconds, and $y_f(t_\\mathrm{sync})$ is the fast state sampled at a selected synchronization time within the macro step.\n\nMultirate discretization:\n- Let the macro step be $[t^n, t^{n+1}]$ with $t^{n+1} = t^n + \\Delta t$, where $\\Delta t$ is the macro step size (in seconds).\n- The fast subsystem is subcycled with $N_\\mathrm{fast}$ uniform substeps per macro step, so candidate synchronization times are $t_\\mathrm{sync}^j = t^n + \\theta_j \\Delta t$, with $\\theta_j = \\frac{j}{N_\\mathrm{fast}}$ and $j \\in \\{1,2,\\dots,N_\\mathrm{fast}-1\\}$ ensuring $\\theta_j \\in (0,1)$.\n- The slow subsystem is advanced over the macro step using the explicit Euler method applied with a single macro update using the sampled fast value: $y_s^{n+1} = y_s^n + \\Delta t \\left(-k y_s^n + \\beta y_f(t_\\mathrm{sync})\\right)$.\n\nAdaptive phase-based synchronization policy:\n- Define the end-of-step mismatch as $m^n(\\theta) = \\left| y_f(t^{n+1}) - y_s^{n+1}(\\theta) \\right|$, where $y_s^{n+1}(\\theta)$ is obtained using the selected $t_\\mathrm{sync} = t^n + \\theta \\Delta t$.\n- Using the exact fast solution $y_f(t) = A \\cos(\\phi(t))$ with $\\phi(t) = \\omega t$, the slow update becomes $y_s^{n+1}(\\theta) = y_s^n + \\Delta t \\left(-k y_s^n + \\beta A \\cos(\\phi(t^n) + \\omega \\theta \\Delta t)\\right)$.\n- At the end of the macro step, the fast state is $y_f(t^{n+1}) = A \\cos(\\phi(t^n) + \\omega \\Delta t)$. The mismatch can be reduced by selecting a $\\theta$ such that $\\cos(\\phi(t^n) + \\omega \\theta \\Delta t)$ is close to a target value inferred from matching $y_s^{n+1}$ to $y_f(t^{n+1})$. Solving $y_s^{n+1}(\\theta) \\approx y_f(t^{n+1})$ gives a target cosine value:\n$$\nC^\\star = \\frac{A \\cos\\left(\\phi(t^n) + \\omega \\Delta t\\right) - y_s^n + \\Delta t\\,k\\,y_s^n}{\\beta A \\Delta t}.\n$$\n- A phase detector computes $\\phi(t_m) = \\phi(t^n) + \\omega \\frac{m}{N_\\mathrm{fast}} \\Delta t$ at the subcycle grid points $m \\in \\{1,\\dots,N_\\mathrm{fast}-1\\}$. It then selects the index $j$ that minimizes $\\left|\\cos(\\phi(t_j)) - \\tilde{C}^\\star\\right|$, where $\\tilde{C}^\\star = \\min\\{1, \\max\\{-1, C^\\star\\}\\}$ clamps the target to the feasible cosine range. The selected synchronization time is $t_\\mathrm{sync} = t^n + \\theta_j \\Delta t$ with $\\theta_j = \\frac{j}{N_\\mathrm{fast}}$.\n\nObjective and evaluation:\n- Over a total of $M$ macro steps, compute the average end-of-step mismatch $\\bar{m} = \\frac{1}{M} \\sum_{n=0}^{M-1} m^n(\\theta_j)$ produced by the adaptive policy described above.\n- All angles must be in radians. Time must be in seconds. The average mismatch $\\bar{m}$ is dimensionless (unitless) as it is a difference of amplitudes.\n\nYour task:\n- Implement the policy above in a complete program that, for each test case, simulates $M$ macro steps, performs fast subcycling with a phase detector to select the synchronization point within each macro step, updates the slow subsystem using the selected $y_f(t_\\mathrm{sync})$, and computes the average end-of-step mismatch $\\bar{m}$.\n- Use the exact fast solution $y_f(t) = A \\cos(\\omega t)$ for subcycle sampling and end-of-step values.\n\nTest suite:\nProvide results for the following four parameter sets, each yielding one scalar output $\\bar{m}$:\n1. Happy path: $A=1.0$, $\\omega=80.0$ rad/s, $\\Delta t=0.025$ s, $N_\\mathrm{fast}=50$, $k=5.0$ s$^{-1}$, $\\beta=2.0$ s$^{-1}$, $M=20$, $y_s(0)=A$.\n2. Boundary clamp case: $A=1.0$, $\\omega=80.0$ rad/s, $\\Delta t=0.025$ s, $N_\\mathrm{fast}=50$, $k=5.0$ s$^{-1}$, $\\beta=0.05$ s$^{-1}$, $M=20$, $y_s(0)=A$.\n3. Small phase advance per macro step: $A=1.0$, $\\omega=10.0$ rad/s, $\\Delta t=0.010$ s, $N_\\mathrm{fast}=5$, $k=1.0$ s$^{-1}$, $\\beta=2.0$ s$^{-1}$, $M=50$, $y_s(0)=A$.\n4. Multiple cycles per macro step: $A=1.0$, $\\omega=500.0$ rad/s, $\\Delta t=0.020$ s, $N_\\mathrm{fast}=100$, $k=3.0$ s$^{-1}$, $\\beta=1.5$ s$^{-1}$, $M=25$, $y_s(0)=A$.\n\nFinal output format:\nYour program should produce a single line of output containing the four results as a comma-separated list enclosed in square brackets (for example, to show four floats in a single list: \"[result1,result2,result3,result4]\"). Each result must be the average end-of-step mismatch $\\bar{m}$ for the corresponding test case, expressed as a unitless float.",
            "solution": "The problem requires the implementation of an adaptive multirate time integration scheme for a coupled system of two ordinary differential equations. The system consists of a fast subsystem, described by a harmonic oscillator with state $y_f(t)$, and a slow subsystem, described by a linear relaxation process with state $y_s(t)$. The core of the problem is to devise and implement an adaptive synchronization policy that selects a synchronization time $t_\\mathrm{sync}$ within each macro time step $[t^n, t^{n+1}]$ to minimize the mismatch between the fast and slow states at the end of the step, $t^{n+1}$.\n\nThe governing equations are:\nFast subsystem: $y_f''(t) + \\omega^2 y_f(t) = 0$. With initial conditions $y_f(0) = A$ and $y_f'(0) = 0$, the exact solution is $y_f(t) = A \\cos(\\omega t)$. The instantaneous phase is $\\phi(t) = \\omega t$.\nSlow subsystem: $y_s'(t) = -k y_s(t) + \\beta y_f(t_\\mathrm{sync})$.\n\nThe simulation proceeds over $M$ macro steps, each of duration $\\Delta t$. For each macro step $n$, starting from an initial state $(t^n, y_s^n)$, the following algorithmic steps are performed to compute the subsequent state $y_s^{n+1}$ and the corresponding mismatch $m^n$.\n\n1.  **Define the Goal within a Macro Step**: The primary objective within the step $[t^n, t^{n+1}]$ is to select a synchronization time fraction $\\theta \\in (0,1)$ such that the resulting slow state at the end of the step, $y_s^{n+1}(\\theta)$, is as close as possible to the true fast state at that time, $y_f(t^{n+1})$. The end-of-step mismatch is defined as $m^n(\\theta) = \\left| y_f(t^{n+1}) - y_s^{n+1}(\\theta) \\right|$.\n\n2.  **Formulate the Target Condition**: The slow subsystem is updated using a single explicit Euler step over the macro interval:\n    $$y_s^{n+1} = y_s^n + \\Delta t \\left(-k y_s^n + \\beta y_f(t_\\mathrm{sync})\\right) = y_s^n (1 - k \\Delta t) + \\beta \\Delta t y_f(t_\\mathrm{sync})$$\n    The synchronization time is $t_\\mathrm{sync} = t^n + \\theta \\Delta t$, and we use the exact solution for the fast variable: $y_f(t_\\mathrm{sync}) = A \\cos(\\omega(t^n+\\theta\\Delta t)) = A \\cos(\\phi(t^n) + \\omega \\theta \\Delta t)$. The fast variable at the end of the step is $y_f(t^{n+1}) = A \\cos(\\omega(t^n+\\Delta t)) = A \\cos(\\phi(t^n) + \\omega \\Delta t)$.\n    To minimize the mismatch, we ideally set $y_s^{n+1} \\approx y_f(t^{n+1})$, which gives:\n    $$A \\cos(\\phi(t^n) + \\omega \\Delta t) \\approx y_s^n (1 - k \\Delta t) + \\beta \\Delta t A \\cos(\\phi(t^n) + \\omega \\theta \\Delta t)$$\n    We can solve for the cosine term involving the unknown $\\theta$, which we define as our target cosine value, $C^\\star$:\n    $$\\cos(\\phi(t^n) + \\omega \\theta \\Delta t) \\approx \\frac{A \\cos\\left(\\phi(t^n) + \\omega \\Delta t\\right) - y_s^n (1 - k \\Delta t)}{\\beta A \\Delta t}$$\n    This is the expression for $C^\\star$ given in the problem statement.\n\n3.  **Implement the Adaptive Synchronization Policy**:\n    a.  At the beginning of step $n$, calculate the target cosine value $C^\\star$ using the current state values $y_s^n$ and $\\phi(t^n) = \\omega t^n = \\omega n \\Delta t$.\n    b.  The value of $C^\\star$ may fall outside the valid range of the cosine function, $[-1, 1]$. Therefore, it is clamped to this range to produce a feasible target: $\\tilde{C}^\\star = \\min\\{1, \\max\\{-1, C^\\star\\}\\}$.\n    c.  The set of candidate synchronization time fractions are determined by subcycling the macro step into $N_\\mathrm{fast}$ uniform intervals. The candidate fractions are $\\theta_j = \\frac{j}{N_\\mathrm{fast}}}$ for $j \\in \\{1, 2, \\dots, N_\\mathrm{fast}-1\\}$.\n    d.  For each candidate index $j$, the cosine of the phase is computed: $\\cos(\\phi(t^n) + \\omega \\theta_j \\Delta t)$.\n    e.  The algorithm selects the optimal index $j^\\star$ that minimizes the absolute difference between the candidate cosine and the clamped target: $j^\\star = \\arg\\min_{j \\in \\{1, \\dots, N_\\mathrm{fast}-1\\}} \\left|\\cos(\\phi(t^n) + \\omega \\theta_j \\Delta t) - \\tilde{C}^\\star\\right|$.\n\n4.  **Update State and Compute Mismatch**:\n    a.  Once the optimal index $j^\\star$ is found, the synchronization time is fixed as $t_\\mathrm{sync} = t^n + \\frac{j^\\star}{N_\\mathrm{fast}}} \\Delta t$.\n    b.  The value of the fast variable at this time, $y_f(t_\\mathrm{sync}) = A \\cos(\\omega t_\\mathrm{sync})$, is used to update the slow state:\n        $$y_s^{n+1} = y_s^n (1 - k \\Delta t) + \\beta \\Delta t y_f(t_\\mathrm{sync})$$\n    c.  The end-of-step mismatch for step $n$ is then calculated:\n        $$m^n = \\left| y_f(t^{n+1}) - y_s^{n+1} \\right| = \\left| A \\cos(\\omega t^{n+1}) - y_s^{n+1} \\right|$$\n    d.  The current state is updated for the next iteration: $y_s^n$ becomes $y_s^{n+1}$.\n\n5.  **Calculate Average Mismatch**: The process is repeated for a total of $M$ macro steps. The total mismatch is accumulated, and the final average mismatch is computed as $\\bar{m} = \\frac{1}{M} \\sum_{n=0}^{M-1} m^n$.\n\nThis entire procedure is encapsulated into a program and executed for each of the four specified test cases to obtain the final results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(A, omega, delta_t, N_fast, k, beta, M, y_s_initial):\n    \"\"\"\n    Runs the multiphysics simulation for one set of parameters.\n\n    Args:\n        A (float): Amplitude of the fast subsystem.\n        omega (float): Angular frequency of the fast subsystem (rad/s).\n        delta_t (float): Macro step size (s).\n        N_fast (int): Number of fast subcycles per macro step.\n        k (float): Relaxation constant for the slow subsystem (s^-1).\n        beta (float): Coupling constant (s^-1).\n        M (int): Total number of macro steps.\n        y_s_initial (float): Initial value of the slow subsystem state.\n\n    Returns:\n        float: The average end-of-step mismatch over M steps.\n    \"\"\"\n    y_s = y_s_initial\n    total_mismatch = 0.0\n\n    # Candidate indices for synchronization j in {1, 2, ..., N_fast-1}\n    j_candidates = np.arange(1, N_fast)\n    theta_candidates = j_candidates / N_fast\n\n    for n in range(M):\n        t_n = n * delta_t\n        phi_n = omega * t_n\n\n        # 1. Calculate the target cosine value C_star\n        phi_end = phi_n + omega * delta_t\n        y_f_end = A * np.cos(phi_end)\n\n        # Denominator of C_star\n        denominator = beta * A * delta_t\n        if np.abs(denominator)  1e-15: # Avoid division by zero\n            # If beta or A or delta_t is zero, C_star is ill-defined.\n            # In this context, we can assume it will not happen based on problem constraints.\n            # However, for robustness, we can set C_star to a value that leads to a default behavior.\n            # Let's say we pick the middle of the interval.\n            C_star = 0.0\n        else:\n            C_star = (y_f_end - y_s * (1.0 - k * delta_t)) / denominator\n\n        # 2. Clamp C_star to the feasible range [-1, 1]\n        C_star_tilde = np.clip(C_star, -1.0, 1.0)\n        \n        # 3. Find the best synchronization index j\n        candidate_phases = phi_n + omega * theta_candidates * delta_t\n        candidate_cosines = np.cos(candidate_phases)\n        \n        diffs = np.abs(candidate_cosines - C_star_tilde)\n        best_j_idx = np.argmin(diffs)\n        \n        # 4. Get the cosine value for the chosen synchronization point\n        cos_y_f_sync = candidate_cosines[best_j_idx]\n        y_f_sync = A * cos_y_f_sync\n        \n        # 5. Update the slow state y_s\n        y_s_next = y_s * (1.0 - k * delta_t) + beta * delta_t * y_f_sync\n        \n        # 6. Calculate the end-of-step mismatch\n        mismatch = np.abs(y_f_end - y_s_next)\n        total_mismatch += mismatch\n        \n        # 7. Update state for the next step\n        y_s = y_s_next\n\n    # 8. Compute the average mismatch\n    average_mismatch = total_mismatch / M\n    return average_mismatch\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (A, omega, delta_t, N_fast, k, beta, M, y_s(0))\n        (1.0, 80.0, 0.025, 50, 5.0, 2.0, 20, 1.0), # Happy path\n        (1.0, 80.0, 0.025, 50, 5.0, 0.05, 20, 1.0), # Boundary clamp case\n        (1.0, 10.0, 0.010, 5, 1.0, 2.0, 50, 1.0), # Small phase advance\n        (1.0, 500.0, 0.020, 100, 3.0, 1.5, 25, 1.0), # Multiple cycles\n    ]\n\n    results = []\n    for case in test_cases:\n        A, omega, delta_t, N_fast, k, beta, M, y_s0 = case\n        result = run_simulation(A, omega, delta_t, N_fast, k, beta, M, y_s0)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}