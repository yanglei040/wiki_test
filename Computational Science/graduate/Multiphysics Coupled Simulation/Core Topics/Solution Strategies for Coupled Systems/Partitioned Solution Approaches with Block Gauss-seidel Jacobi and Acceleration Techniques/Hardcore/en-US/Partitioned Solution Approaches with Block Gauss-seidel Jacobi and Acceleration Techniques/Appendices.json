{
    "hands_on_practices": [
        {
            "introduction": "To grasp the core challenge in partitioned multiphysics simulations, it is invaluable to study a simplified \"toy model.\" This exercise  strips away complex details to reveal the fundamental relationship between the physical coupling strength, the mathematical conditioning of the monolithic system, and the convergence rate of basic partitioned schemes. By analyzing this model, you will directly see why strongly coupled problems, where the coupling parameter product $|\\alpha\\beta|$ approaches $1$, are inherently difficult to solve and necessitate the use of advanced acceleration techniques.",
            "id": "3520332",
            "problem": "Consider a two-field, linearly coupled, steady multiphysics system written in block form as\n$$\n\\begin{pmatrix}\nA  B \\\\\nC  D\n\\end{pmatrix}\n\\begin{pmatrix}\nu \\\\\nv\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf \\\\\ng\n\\end{pmatrix},\n$$\nwhere $u$ and $v$ represent two interacting physical fields (for example, fluid velocity and structural displacement), and $A$, $B$, $C$, $D$ are square operators of compatible dimension. Assume the operators are bounded and invertible where required. In a partitioned (segregated) solution approach, one may eliminate $v$ to form a reduced system for $u$ characterized by an effective operator built from block elimination. In this toy model, let $A = I$, $D = I$, $B = \\alpha I$, and $C = \\beta I$, where $I$ is the identity operator and $\\alpha, \\beta \\in \\mathbb{R}$ are coupling parameters that quantify the strength of inter-physics interaction.\n\nTasks:\n- By performing exact block elimination of $v$, obtain the reduced operator acting on $u$ and express it in closed form in terms of $\\alpha$, $\\beta$, and $I$. Provide your final result for this operator in symbolic form.\n- Starting from the contraction mapping principle and the spectral-radius criterion for linear fixed-point iterations, derive the error-propagation operators for block Jacobi and block Gauss–Seidel iterations applied to the above toy model, and use these to explain how the magnitude $|\\alpha\\beta|$ controls both the algebraic difficulty of the reduced operator and the convergence rate of the partitioned iterations. In your explanation, relate the limit $|\\alpha\\beta| \\to 1$ to conditioning of the reduced operator and to the iteration’s contraction factor, and comment on how simple linear mixing (relaxation) modifies the effective contraction factor.\n\nAnswer format:\n- Express the reduced operator as a single closed-form analytic expression. No numerical evaluation is required.",
            "solution": "The multiphysics system is given in block matrix form:\n$$\n\\begin{pmatrix}\nA  B \\\\\nC  D\n\\end{pmatrix}\n\\begin{pmatrix}\nu \\\\\nv\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf \\\\\ng\n\\end{pmatrix}\n$$\nThis corresponds to a system of two coupled linear equations:\n$$\nAu + Bv = f \\quad (1)\n$$\n$$\nCu + Dv = g \\quad (2)\n$$\n\n### Part 1: Derivation of the Reduced Operator\n\nTo obtain the reduced operator for the field $u$, we perform a block elimination of the field $v$. This is analogous to solving for one variable and substituting it into the other equation. Assuming the operator $D$ is invertible, we can formally solve equation $(2)$ for $v$:\n$$\nDv = g - Cu\n$$\n$$\nv = D^{-1}(g - Cu) \\quad (3)\n$$\nNext, we substitute this expression for $v$ back into equation $(1)$:\n$$\nAu + B \\left[ D^{-1}(g - Cu) \\right] = f\n$$\nWe distribute the operator $B$ and rearrange the terms to group those involving $u$ on the left-hand side and the remaining known terms on the right-hand side.\n$$\nAu + BD^{-1}g - BD^{-1}Cu = f\n$$\n$$\nAu - BD^{-1}Cu = f - BD^{-1}g\n$$\nFactoring out $u$ on the left side, we obtain the reduced system:\n$$\n(A - BD^{-1}C)u = f - BD^{-1}g\n$$\nThe operator acting on $u$ is the reduced operator, commonly known as the Schur complement of the block $D$ in the system matrix. Let us denote this operator as $S$:\n$$\nS = A - BD^{-1}C\n$$\nNow, we substitute the specific forms of the operators given in the problem: $A = I$, $D = I$, $B = \\alpha I$, and $C = \\beta I$, where $I$ is the identity operator and $\\alpha, \\beta \\in \\mathbb{R}$. The inverse of the identity operator is the identity operator itself, $I^{-1} = I$.\n$$\nS = I - (\\alpha I)(I^{-1})(\\beta I)\n$$\nSince scalar multiplication is commutative and associative with operator multiplication, and $I \\cdot I = I$, we can simplify:\n$$\nS = I - \\alpha \\beta (I \\cdot I \\cdot I)\n$$\n$$\nS = I - \\alpha \\beta I\n$$\nThis can be factored to yield the final closed form for the reduced operator:\n$$\nS = (1 - \\alpha\\beta)I\n$$\n\n### Part 2: Analysis of Partitioned Iteration Schemes\n\nWe now analyze the convergence of partitioned (segregated) iterative schemes, specifically block Jacobi and block Gauss-Seidel, based on the contraction mapping principle. An iterative method of the form $x_{k+1} = G x_k + c$ converges for any initial guess if and only if the spectral radius of the iteration matrix $G$, denoted $\\rho(G)$, is less than $1$. The spectral radius is the maximum absolute value of the eigenvalues of $G$. The value of $\\rho(G)$ also serves as the asymptotic contraction factor of the error.\n\n**Block Jacobi Iteration**\nIn the block Jacobi method, the field values from the previous iteration, $k$, are used to update all fields for the new iteration, $k+1$.\n$$\nA u^{k+1} = f - B v^k\n$$\n$$\nD v^{k+1} = g - C u^k\n$$\nTo derive the error propagation operator, we let $u$ and $v$ be the exact solutions satisfying equations $(1)$ and $(2)$, and define the errors as $e_u^k = u^k - u$ and $e_v^k = v^k - v$. Subtracting the exact equations from the iteration equations yields:\n$$\nA(u^{k+1} - u) = -B(v^k - v) \\implies A e_u^{k+1} = -B e_v^k\n$$\n$$\nD(v^{k+1} - v) = -C(u^k - u) \\implies D e_v^{k+1} = -C e_u^k\n$$\nAssuming $A$ and $D$ are invertible, we get:\n$$\ne_u^{k+1} = -A^{-1}B e_v^k\n$$\n$$\ne_v^{k+1} = -D^{-1}C e_u^k\n$$\nWriting this in block matrix form for the error vector $e^k = \\begin{pmatrix} e_u^k \\\\ e_v^k \\end{pmatrix}$:\n$$\n\\begin{pmatrix} e_u^{k+1} \\\\ e_v^{k+1} \\end{pmatrix} = \\begin{pmatrix} 0  -A^{-1}B \\\\ -D^{-1}C  0 \\end{pmatrix} \\begin{pmatrix} e_u^k \\\\ e_v^k \\end{pmatrix}\n$$\nThe block Jacobi iteration matrix is $G_{BJ} = \\begin{pmatrix} 0  -A^{-1}B \\\\ -D^{-1}C  0 \\end{pmatrix}$. Substituting the toy model operators:\n$$\nG_{BJ} = \\begin{pmatrix} 0  -I^{-1}(\\alpha I) \\\\ -I^{-1}(\\beta I)  0 \\end{pmatrix} = \\begin{pmatrix} 0  -\\alpha I \\\\ -\\beta I  0 \\end{pmatrix}\n$$\nTo find the spectral radius, we find the eigenvalues $\\lambda$ from $\\det(G_{BJ} - \\lambda I_{block}) = 0$:\n$$\n\\det \\begin{pmatrix} -\\lambda I  -\\alpha I \\\\ -\\beta I  -\\lambda I \\end{pmatrix} = \\det((-\\lambda I)(-\\lambda I) - (-\\alpha I)(-\\beta I)) = \\det(\\lambda^2 I - \\alpha\\beta I) = 0\n$$\nThis implies that for any non-trivial solution, we must have $\\lambda^2 - \\alpha\\beta = 0$, so $\\lambda = \\pm\\sqrt{\\alpha\\beta}$. The spectral radius is the maximum magnitude of these eigenvalues:\n$$\n\\rho(G_{BJ}) = |\\pm\\sqrt{\\alpha\\beta}| = \\sqrt{|\\alpha\\beta|}\n$$\nConvergence is guaranteed if $\\rho(G_{BJ})  1$, which requires $|\\alpha\\beta|  1$.\n\n**Block Gauss-Seidel Iteration**\nIn the block Gauss-Seidel method, the most recently computed values are used. Assuming we solve for $u$ first:\n$$\nA u^{k+1} = f - B v^k\n$$\n$$\nD v^{k+1} = g - C u^{k+1}\n$$\nThe error propagation equations are:\n$$\nA e_u^{k+1} = -B e_v^k \\implies e_u^{k+1} = -A^{-1}B e_v^k\n$$\n$$\nD e_v^{k+1} = -C e_u^{k+1} \\implies e_v^{k+1} = -D^{-1}C e_u^{k+1}\n$$\nTo find the overall iteration matrix, $G_{BGS}$, we can express the error at step $k+1$ in terms of the error at step $k$.\n$$\n\\begin{pmatrix} e_u^{k+1} \\\\ e_v^{k+1} \\end{pmatrix} = \\begin{pmatrix} 0  -A^{-1}B \\\\ 0  D^{-1}CA^{-1}B \\end{pmatrix} \\begin{pmatrix} e_u^k \\\\ e_v^k \\end{pmatrix}\n$$\nThe iteration matrix is therefore $G_{BGS} = \\begin{pmatrix} 0  -A^{-1}B \\\\ 0  D^{-1}CA^{-1}B \\end{pmatrix}$.\nThis is a block upper triangular matrix, so its eigenvalues are the eigenvalues of its diagonal blocks, which are the zero operator $0$ and the operator $D^{-1}CA^{-1}B$. For our toy model, this operator becomes:\n$$\nD^{-1}CA^{-1}B = I^{-1}(\\beta I)I^{-1}(\\alpha I) = \\alpha\\beta I\n$$\nThe eigenvalues of $G_{BGS}$ are therefore $0$ and $\\alpha\\beta$. The spectral radius is:\n$$\n\\rho(G_{BGS}) = \\max(|0|, |\\alpha\\beta|) = |\\alpha\\beta|\n$$\nConvergence is guaranteed if $\\rho(G_{BGS})  1$, which again requires $|\\alpha\\beta|  1$.\n\n### Part 3: Relationship between Coupling, Conditioning, and Convergence\n\nThe analysis reveals that the quantity $|\\alpha\\beta|$, which represents the strength of the two-way coupling, is the critical parameter.\n\n**Relation to Algebraic Difficulty:** The reduced operator is $S = (1 - \\alpha\\beta)I$. The \"algebraic difficulty\" of solving a system is related to its conditioning. An operator is singular (and thus infinitely ill-conditioned) if it is not invertible. For $S$ to be invertible, we require $1 - \\alpha\\beta \\neq 0$, or $\\alpha\\beta \\neq 1$. As $|\\alpha\\beta| \\to 1$, the scalar factor $(1 - \\alpha\\beta)$ approaches $0$. The operator $S$ becomes \"closer\" to the singular zero operator. The norm of its inverse, $||S^{-1}|| = ||(1-\\alpha\\beta)^{-1}I||$, grows without bound. This means the Schur complement system becomes extremely ill-conditioned, making a direct solution via block elimination numerically unstable and difficult.\n\n**Relation to Convergence Rate:** The convergence rates for the partitioned schemes are determined by their respective spectral radii, $\\rho(G_{BJ}) = \\sqrt{|\\alpha\\beta|}$ and $\\rho(G_{BGS}) = |\\alpha\\beta|$. As $|\\alpha\\beta| \\to 1$, both spectral radii approach $1$. A spectral radius of $1$ corresponds to a stalled iteration, where the error is not reduced from one step to the next. Therefore, strong coupling (i.e., $|\\alpha\\beta|$ close to $1$) leads to a drastic slowdown in the convergence of both block Jacobi and block Gauss-Seidel schemes.\n\nIn summary, the same parameter $\\alpha\\beta$ that governs the conditioning of the fully-coupled (monolithic) Schur complement system also governs the convergence rate of the uncoupled (segregated) iterative schemes. This is a fundamental property of coupled systems: strong coupling makes the problem inherently difficult, whether one attempts to solve it monolithically or iteratively.\n\n**Modification by Linear Mixing (Relaxation)**\nSimple linear relaxation modifies an iterative scheme $x^{k+1} = G x^k + c$ as follows:\n$$\nx^{k+1} = (1-\\omega)x^k + \\omega(G x^k + c) = [(1-\\omega)I + \\omega G] x^k + \\omega c\n$$\nwhere $\\omega$ is the relaxation parameter. The new iteration matrix is $G_\\omega = (1-\\omega)I + \\omega G$. Its eigenvalues $\\mu_i$ are related to the eigenvalues $\\lambda_i$ of $G$ by $\\mu_i = (1-\\omega) + \\omega\\lambda_i$. The eigenvalues of $G_{BGS}$ are $0$ and $\\alpha\\beta$. Thus, the eigenvalues of the relaxed matrix $G_\\omega$ are $(1-\\omega)$ and $(1-\\omega) + \\omega(\\alpha\\beta) = 1 - \\omega(1-\\alpha\\beta)$. The effective contraction factor is $\\rho(G_\\omega) = \\max(|1-\\omega|, |1-\\omega(1-\\alpha\\beta)|)$.\n\nFor strongly coupled problems where $|\\alpha\\beta| \\to 1$, we have $1-\\alpha\\beta \\to 0$. The second eigenvalue term approaches $1-\\omega$. By choosing an appropriate $\\omega > 1$ (over-relaxation), it is possible to make $\\rho(G_\\omega)$ smaller than the original $\\rho(G_{BGS})=|\\alpha\\beta|$, thus accelerating convergence. For instance, if $\\alpha\\beta=0.99$, $\\rho(G_{BGS})=0.99$. With $\\omega=1.5$, $\\rho(G_\\omega) = \\max(|-0.5|, |1-1.5(0.01)|) = \\max(0.5, 0.985) = 0.985$, which is an improvement. The optimal $\\omega$ can provide substantial speed-up. However, as $|\\alpha\\beta| \\to 1$ from below, even the optimal relaxation parameter $\\omega_{opt}$ leads to an effective spectral radius $\\rho_{opt}$ that approaches $1$. Relaxation helps mitigate the slowdown but cannot overcome the fundamental limit posed by singular behavior at $|\\alpha\\beta|=1$.",
            "answer": "$$\n\\boxed{(1 - \\alpha\\beta)I}\n$$"
        },
        {
            "introduction": "Having established that the convergence of partitioned schemes is governed by the spectral radius of an iteration operator, we now explore a simple yet powerful method for influencing this rate: under-relaxation. This practice  provides a concrete example of how to analyze the effect of a relaxation parameter $\\omega$ on an iteration's convergence factor. Mastering this analysis is a fundamental step in learning how to stabilize and control the behavior of iterative solvers, even though in some cases, like the one explored here, it might not necessarily accelerate an already converging scheme.",
            "id": "3520308",
            "problem": "Consider a partitioned multiphysics fixed-point scheme in which a displacement field is updated in a segregated manner using a linear block Gauss-Seidel (GS) step. Let the unrelaxed displacement update be represented as a linear fixed-point map $\\Phi(u) = T u + b$, where $T$ is the iteration operator induced by the block splitting and $b$ is a constant arising from the right-hand side and the coupling data at the current outer iteration. Assume a consistent vector norm is used so that the unrelaxed contraction factor is characterized by the spectral radius $\\rho(T)$ through $\\|\\Phi(u) - \\Phi(v)\\| \\le \\rho(T) \\|u - v\\|$ for all vectors $u$ and $v$ in the relevant space. Suppose the dominant eigenvalue of $T$ responsible for the contraction factor is real and positive, which is typical for monotone linearized couplings in block Gauss-Seidel (GS) or block Jacobi schemes.\n\nAn acceleration technique is introduced via scalar under-relaxation: the relaxed displacement update is\n$$\nu^{(k+1)} \\;=\\; u^{(k)} \\;+\\; \\omega\\big(\\Phi(u^{(k)}) - u^{(k)}\\big),\n$$\nwith relaxation parameter $\\omega \\in (0,1)$. This is the standard linear mixing used to stabilize and control the convergence of segregated iterations.\n\nStarting from the definition of contraction for linear fixed-point maps and basic facts about eigenvalues of affine linear operators, derive the contraction factor of the under-relaxed map in terms of the eigenvalues of $T$. Then, for $\\omega = 0.5$ and an unrelaxed contraction factor $\\rho(T) = 0.8$, determine the minimal integer number of iterations required for the residual norm of the displacement update to be reduced by a factor of $10^{-3}$ relative to its initial value, under the conservative assumption that the maximal-modulus eigenvalue of $T$ is positive and equals $\\rho(T)$ so that the under-relaxed contraction factor is attained on that eigenmode. Express your final answer as a single number without units. No rounding instruction is necessary because the requested quantity is an integer count.",
            "solution": "The solution is presented in two parts as requested: first, the derivation of the relaxed contraction factor, and second, the calculation of the required number of iterations.\n\n**Part 1: Derivation of the Relaxed Contraction Factor**\n\nThe unrelaxed fixed-point iteration is given by $u^{(k+1)} = \\Phi(u^{(k)})$, where the map is $\\Phi(u) = T u + b$. The under-relaxed iteration is defined as:\n$$\nu^{(k+1)} = u^{(k)} + \\omega\\big(\\Phi(u^{(k)}) - u^{(k)}\\big)\n$$\nWe can define a new, relaxed fixed-point map, $\\Psi(u)$, such that $u^{(k+1)} = \\Psi(u^{(k)})$. Substituting the expression for $\\Phi(u)$, we get:\n$$\n\\Psi(u) = u + \\omega\\big((T u + b) - u\\big)\n$$\n$$\n\\Psi(u) = u + \\omega(T u - u + b)\n$$\n$$\n\\Psi(u) = u + \\omega(T - I)u + \\omega b\n$$\nFactoring out $u$, we can write this in the standard affine form $\\Psi(u) = T_{\\omega} u + b_{\\omega}$:\n$$\n\\Psi(u) = \\big(I + \\omega(T - I)\\big)u + (\\omega b) = \\big((1-\\omega)I + \\omega T\\big)u + \\omega b\n$$\nThe iteration operator for the relaxed scheme is therefore $T_{\\omega} = (1-\\omega)I + \\omega T$.\n\nThe asymptotic convergence rate of a stationary linear iterative method is governed by the spectral radius of its iteration operator. Let $\\lambda$ be an eigenvalue of the original operator $T$ with a corresponding eigenvector $v$, such that $T v = \\lambda v$. We can find the corresponding eigenvalue of the relaxed operator $T_{\\omega}$ by applying it to the same eigenvector $v$:\n$$\nT_{\\omega} v = \\big((1-\\omega)I + \\omega T\\big)v = (1-\\omega)Iv + \\omega(Tv)\n$$\n$$\nT_{\\omega} v = (1-\\omega)v + \\omega(\\lambda v) = \\big((1-\\omega) + \\omega\\lambda\\big)v\n$$\nThis shows that if $\\lambda$ is an eigenvalue of $T$, then $\\lambda_{\\omega} = (1-\\omega) + \\omega\\lambda$ is an eigenvalue of $T_{\\omega}$. The spectral radius of the relaxed operator, $\\rho(T_{\\omega})$, which defines the relaxed contraction factor, is the maximum modulus of its eigenvalues:\n$$\n\\rho(T_{\\omega}) = \\max_{\\lambda \\in \\sigma(T)} |\\lambda_{\\omega}| = \\max_{\\lambda \\in \\sigma(T)} |(1-\\omega) + \\omega\\lambda|\n$$\nwhere $\\sigma(T)$ is the spectrum (the set of all eigenvalues) of $T$. This expression gives the contraction factor of the under-relaxed map in terms of the eigenvalues of $T$ and the relaxation parameter $\\omega$.\n\n**Part 2: Calculation of Iterations**\n\nWe are given the unrelaxed contraction factor $\\rho(T) = 0.8$ and the relaxation parameter $\\omega = 0.5$. The problem states we should make the conservative assumption that the maximal-modulus eigenvalue of $T$ is real and positive, so we take $\\lambda_{max} = \\rho(T) = 0.8$. Furthermore, we assume that the contraction factor of the relaxed scheme, $\\rho(T_{\\omega})$, is attained for this specific eigenvalue.\n\nUsing the formula derived above, the new dominant eigenvalue is:\n$$\n\\lambda_{\\omega, max} = (1 - \\omega) + \\omega \\lambda_{max}\n$$\nSubstituting the given values:\n$$\n\\lambda_{\\omega, max} = (1 - 0.5) + (0.5)(0.8) = 0.5 + 0.4 = 0.9\n$$\nThe contraction factor for the relaxed scheme is $\\rho(T_{\\omega}) = |\\lambda_{\\omega, max}| = 0.9$. Note that under-relaxation has slowed down the convergence in this case (from $0.8$ to $0.9$), which is a possible outcome when applying it to an already convergent scheme, although it often improves stability.\n\nWe need to find the minimal integer number of iterations, $k$, such that the residual norm is reduced by a factor of at least $10^{-3}$. The ratio of the residual norm at iteration $k$ to the initial residual norm is bounded by the $k$-th power of the contraction factor:\n$$\n\\frac{\\|r^{(k)}\\|}{\\|r^{(0)}\\|} \\le (\\rho(T_{\\omega}))^k\n$$\nWe need to solve for the smallest integer $k$ that satisfies:\n$$\n(0.9)^k \\le 10^{-3}\n$$\nTo solve for $k$, we take the natural logarithm of both sides:\n$$\n\\ln\\left((0.9)^k\\right) \\le \\ln(10^{-3})\n$$\n$$\nk \\ln(0.9) \\le -3 \\ln(10)\n$$\nSince $\\ln(0.9)$ is a negative number, dividing by it reverses the inequality sign:\n$$\nk \\ge \\frac{-3 \\ln(10)}{\\ln(0.9)}\n$$\nNow, we can evaluate the right-hand side:\n$$\nk \\ge \\frac{-3 \\times 2.302585...}{-0.105360...} \\approx 65.56\n$$\nSince the number of iterations $k$ must be an integer, we must take the smallest integer that is greater than or equal to $65.56$. This is achieved by taking the ceiling of this value.\n$$\nk = \\lceil 65.56 \\rceil = 66\n$$\nTherefore, a minimum of $66$ iterations are required to achieve the desired reduction in the residual norm.",
            "answer": "$$\\boxed{66}$$"
        },
        {
            "introduction": "Simple relaxation methods are often insufficient for the strongly coupled systems encountered in practice, which calls for more sophisticated acceleration strategies like quasi-Newton methods that dynamically learn about the system as the iteration progresses. This exercise  offers a hands-on application of the Interface Quasi-Newton Inverse Least Squares (IQN-ILS) technique, where you will use data from past iterations to construct a superior approximation to the system's inverse Jacobian. This practice demonstrates a powerful, modern approach to dramatically accelerate convergence in challenging multiphysics problems.",
            "id": "3520284",
            "problem": "In a partitioned (segregated) multiphysics coupling solved by a block Gauss-Seidel iteration, the interface variable update is driven by a residual mapping $r(x)$ that quantifies the mismatch between subdomains across the interface. A quasi-Newton acceleration constructs an approximation $B$ to the inverse of the interface Jacobian $\\frac{\\partial x}{\\partial r}$ such that the correction step is $-\\!B r$. The Interface Quasi-Newton Inverse Least Squares (IQN-ILS) method seeks $B$ that best satisfies the secant conditions $B\\,\\Delta r_{i} \\approx \\Delta x_{i}$ in the least-squares sense over collected iteration history, where $\\Delta r_{i}$ and $\\Delta x_{i}$ are residual and interface-update differences across iterations.\n\nStarting from the principle of least squares, define the matrices $R$ and $X$ whose columns are the two-dimensional secant pairs $\\Delta r_{i}$ and $\\Delta x_{i}$, respectively, and derive the minimizer $B$ of the Frobenius-norm objective $\\|B R - X\\|_{F}$. Then, using the resulting $B$, predict the next interface correction $-\\!B r$ for a given residual $r$.\n\nData:\n- $\\Delta r_{0} = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}$, $\\Delta x_{0} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$,\n- $\\Delta r_{1} = \\begin{pmatrix}3 \\\\ 1\\end{pmatrix}$, $\\Delta x_{1} = \\begin{pmatrix}5 \\\\ 3\\end{pmatrix}$,\n- $r = \\begin{pmatrix}2 \\\\ -1\\end{pmatrix}$.\n\nConstruct $R = [\\Delta r_{0}\\ \\Delta r_{1}]$ and $X = [\\Delta x_{0}\\ \\Delta x_{1}]$, compute one IQN-ILS update of $B$, and use it to form $-\\!B r$. Report, as a single scalar, the squared Euclidean norm $\\|-\\!B r\\|_{2}^{2}$. Express the final answer as an exact value with no units and do not round.",
            "solution": "The problem requires the calculation of an interface correction step using the Interface Quasi-Newton Inverse Least Squares (IQN-ILS) method and then finding the squared Euclidean norm of that correction.\n\nFirst, we must derive the expression for the matrix $B$ that minimizes the Frobenius-norm objective function $J(B) = \\|B R - X\\|_{F}$. Minimizing $J(B)$ is equivalent to minimizing its square, $J(B)^2 = \\|B R - X\\|_{F}^2$. The squared Frobenius norm of a matrix $A$ is given by $\\|A\\|_{F}^2 = \\text{Tr}(A^T A)$.\n$$ J(B)^2 = \\text{Tr}\\left((B R - X)^T (B R - X)\\right) $$\nThe minimization problem can be decoupled for each row of the matrix $B$. Let $\\mathbf{b}_i^T$ be the $i$-th row of $B$ and $\\mathbf{x}_i^T$ be the $i$-th row of $X$. The objective function becomes the sum of the squared Euclidean norms of the row-wise errors:\n$$ \\|B R - X\\|_{F}^2 = \\sum_{i} \\|\\mathbf{b}_i^T R - \\mathbf{x}_i^T\\|_2^2 = \\sum_{i} \\|R^T \\mathbf{b}_i - \\mathbf{x}_i\\|_2^2 $$\nEach term in the sum is an independent linear least-squares problem for the vector $\\mathbf{b}_i$. The solution to $\\min_{\\mathbf{b}_i} \\|R^T \\mathbf{b}_i - \\mathbf{x}_i\\|_2^2$ is given by the normal equations:\n$$ (R^T)^T R^T \\mathbf{b}_i = (R^T)^T \\mathbf{x}_i \\implies R R^T \\mathbf{b}_i = R \\mathbf{x}_i $$\nAssuming that the matrix $R R^T$ is invertible, we can solve for each $\\mathbf{b}_i$:\n$$ \\mathbf{b}_i = (R R^T)^{-1} R \\mathbf{x}_i $$\nAssembling the row vectors $\\mathbf{b}_i^T$ back into the matrix $B$, we arrive at the general solution for the IQN-ILS update:\n$$ B^T = (R R^T)^{-1} R X^T \\implies B = \\left((R R^T)^{-1} R X^T\\right)^T = X R^T (R R^T)^{-1} $$\nNow, we apply this to the specific data provided. The matrices $R$ and $X$ are constructed from the given secant pairs:\n$$ \\Delta r_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\Delta r_{1} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} \\implies R = [\\Delta r_{0}\\ \\Delta r_{1}] = \\begin{pmatrix} 1  3 \\\\ 2  1 \\end{pmatrix} $$\n$$ \\Delta x_{0} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\Delta x_{1} = \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} \\implies X = [\\Delta x_{0}\\ \\Delta x_{1}] = \\begin{pmatrix} 0  5 \\\\ 1  3 \\end{pmatrix} $$\nIn this problem, the matrix $R$ is a $2 \\times 2$ square matrix. We check if it is invertible by computing its determinant:\n$$ \\det(R) = (1)(1) - (3)(2) = 1 - 6 = -5 $$\nSince $\\det(R) \\neq 0$, $R$ is invertible. In this special case, the least-squares problem $\\|B R - X\\|_{F}$ has an exact solution where the objective function is zero, i.e., $B R - X = 0$. This simplifies the general formula for $B$:\n$$ B = X R^{-1} $$\nWe now compute the inverse of $R$:\n$$ R^{-1} = \\frac{1}{\\det(R)}\\begin{pmatrix} 1  -3 \\\\ -2  1 \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 1  -3 \\\\ -2  1 \\end{pmatrix} = \\begin{pmatrix} -1/5  3/5 \\\\ 2/5  -1/5 \\end{pmatrix} $$\nNext, we compute the matrix $B$ by multiplying $X$ and $R^{-1}$:\n$$ B = X R^{-1} = \\begin{pmatrix} 0  5 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} -1/5  3/5 \\\\ 2/5  -1/5 \\end{pmatrix} $$\n$$ B = \\begin{pmatrix} 0(-\\frac{1}{5}) + 5(\\frac{2}{5})  0(\\frac{3}{5}) + 5(-\\frac{1}{5}) \\\\ 1(-\\frac{1}{5}) + 3(\\frac{2}{5})  1(\\frac{3}{5}) + 3(-\\frac{1}{5}) \\end{pmatrix} = \\begin{pmatrix} \\frac{10}{5}  -\\frac{5}{5} \\\\ \\frac{-1+6}{5}  \\frac{3-3}{5} \\end{pmatrix} = \\begin{pmatrix} 2  -1 \\\\ 1  0 \\end{pmatrix} $$\nWith the approximated inverse Jacobian $B$, we compute the next interface correction vector, $-\\!B r$, using the given residual $r$:\n$$ r = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} $$\n$$ -\\!B r = - \\begin{pmatrix} 2  -1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = - \\begin{pmatrix} (2)(2) + (-1)(-1) \\\\ (1)(2) + (0)(-1) \\end{pmatrix} = - \\begin{pmatrix} 4+1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -5 \\\\ -2 \\end{pmatrix} $$\nFinally, we compute the squared Euclidean norm of this correction vector, denoted as $\\|-\\!B r\\|_{2}^{2}$:\n$$ \\|-\\!B r\\|_{2}^{2} = (-5)^2 + (-2)^2 = 25 + 4 = 29 $$\nThe result is an exact integer value.",
            "answer": "$$ \\boxed{29} $$"
        }
    ]
}