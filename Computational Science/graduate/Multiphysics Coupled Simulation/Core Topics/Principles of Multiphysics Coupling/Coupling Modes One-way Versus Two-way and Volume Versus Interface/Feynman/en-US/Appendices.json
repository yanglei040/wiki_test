{
    "hands_on_practices": [
        {
            "introduction": "In multiphysics modeling, a common simplification is to treat a two-way coupled system as a one-way problem, which significantly reduces computational complexity. However, this approximation introduces an error. This practice provides a quantitative framework to understand the consequences of this simplification by relating the magnitude of the error to the strength of the neglected physical feedback . By computing the error, residual, and a formal coupling metric for a benchmark linear system, you will develop a concrete intuition for when a one-way approximation is justifiable and when it is likely to fail.",
            "id": "3500851",
            "problem": "Consider a dimensionless linearized benchmark for a two-field multiphysics system that distinguishes between one-way and two-way coupling, and between interface and volume coupling. Let the state vector be $x = [u, T]^\\top$, representing two scalar fields aggregated over a computational domain, and let the governing linear system be $J x = f$ with\n$$\nJ = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}, \\quad f = \\begin{bmatrix} f_u \\\\ f_T \\end{bmatrix},\n$$\nwhere $a > 0$ and $d > 0$ represent the self-coupling (diagonal) stiffness-like terms, while $b$ and $c$ represent off-diagonal coupling contributions. Interface coupling is represented by highly asymmetric off-diagonal magnitudes (for example $|b| \\ll |c|$), while volume coupling is represented by comparable off-diagonal magnitudes (for example $|b| \\approx |c|$). The two-way coupled solution is the exact solution $x^{\\text{two}}$ of the full system $J x = f$. The one-way coupled solution $x^{\\text{one}}$ assumes $u$ drives $T$ but not vice versa, obtained by first solving $a u^{\\text{one}} = f_u$ and then solving $c u^{\\text{one}} + d T^{\\text{one}} = f_T$.\n\nYour task is to quantify the error introduced by switching from two-way to one-way coupling and relate it to a coupling strength metric and to observed residuals when the one-way solution is inserted into the full two-way equations. Work entirely with dimensionless quantities. Angles do not appear. No physical units are required.\n\nFor each test case, do the following:\n- Compute the two-way solution $x^{\\text{two}}$ by solving $J x = f$.\n- Compute the one-way solution $x^{\\text{one}}$ by solving $a u^{\\text{one}} = f_u$ and then $c u^{\\text{one}} + d T^{\\text{one}} = f_T$.\n- Compute the residual vector $r = J x^{\\text{one}} - f$.\n- Compute the error vector $e = x^{\\text{two}} - x^{\\text{one}}$ and its Euclidean norm $\\|e\\|_2$.\n- Compute the residual Euclidean norm $\\|r\\|_2$.\n- Compute the coupling strength metric\n$$\n\\Gamma = \\sqrt{\\left(\\frac{|b|}{a}\\right)\\left(\\frac{|c|}{d}\\right)}.\n$$\n- Compute the Singular Value Decomposition (SVD)-based induced two-norm of the inverse matrix $\\|J^{-1}\\|_2$ using the smallest singular value $\\sigma_{\\min}$ of $J$, and then compute the product $\\|J^{-1}\\|_2 \\cdot \\|r\\|_2$.\n\nTest suite:\n- Case $1$ (weak, symmetric volume-like coupling): $a = 1.0$, $d = 1.0$, $b = 0.05$, $c = 0.05$, $f_u = 1.0$, $f_T = 0.0$.\n- Case $2$ (strong, symmetric volume-like coupling): $a = 1.0$, $d = 1.0$, $b = 0.6$, $c = 0.6$, $f_u = 1.0$, $f_T = 0.0$.\n- Case $3$ (asymmetric interface-like coupling): $a = 1.0$, $d = 1.0$, $b = 0.02$, $c = 0.5$, $f_u = 1.0$, $f_T = 0.5$.\n- Case $4$ (near-singular strong coupling): $a = 1.0$, $d = 1.0$, $b = 0.95$, $c = 0.95$, $f_u = 1.0$, $f_T = 1.0$.\n- Case $5$ (one off-diagonal zero, interface-like): $a = 1.0$, $d = 1.0$, $b = 0.0$, $c = 0.5$, $f_u = 1.0$, $f_T = 1.0$.\n\nAnswer specification:\n- For each test case, produce a list $[E, R, \\Gamma, B]$ where $E = \\|e\\|_2$, $R = \\|r\\|_2$, $\\Gamma$ is as defined above, and $B = \\|J^{-1}\\|_2 \\cdot R$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a comma-separated list enclosed in square brackets. For example, a valid output format is $[[E_1,R_1,\\Gamma_1,B_1],[E_2,R_2,\\Gamma_2,B_2],\\ldots]$.\n\nYour implementation must solve all cases and print the single-line aggregate result in the exact format described above.",
            "solution": "The problem is valid as it is scientifically grounded in linear algebra and the theory of numerical methods for coupled systems, is well-posed with invertible matrices for all test cases, and is stated objectively using precise mathematical language. We will now proceed with the solution.\n\nThe core of the problem is to analyze the error introduced by a one-way coupling approximation in a two-field multiphysics system. The system is described by the linear equation $J x = f$, where the state vector is $x = [u, T]^\\top$, the system matrix is $J = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$, and the forcing vector is $f = [f_u, f_T]^\\top$. The diagonal terms $a > 0$ and $d > 0$ are given.\n\nFirst, we define the two solutions to be compared.\n\nThe **two-way coupled solution**, denoted as $x^{\\text{two}} = [u^{\\text{two}}, T^{\\text{two}}]^\\top$, is the exact solution to the full system. It is found by solving the complete set of equations, which in matrix form is:\n$$\n\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} u^{\\text{two}} \\\\ T^{\\text{two}} \\end{bmatrix} = \\begin{bmatrix} f_u \\\\ f_T \\end{bmatrix}\n$$\nThis solution can be formally written as $x^{\\text{two}} = J^{-1} f$, and it is computed numerically using a standard linear solver.\n\nThe **one-way coupled solution**, $x^{\\text{one}} = [u^{\\text{one}}, T^{\\text{one}}]^\\top$, is based on the assumption that the field $u$ influences $T$, but $T$ does not influence $u$. This simplifies the problem by removing the coupling term $b$ from the first equation, effectively setting it to $0$. This leads to a sequential solution procedure:\n1. Solve for $u^{\\text{one}}$ from the first equation, ignoring the influence of $T^{\\text{one}}$:\n   $$\n   a u^{\\text{one}} = f_u \\implies u^{\\text{one}} = \\frac{f_u}{a}\n   $$\n2. Substitute this known $u^{\\text{one}}$ into the second equation and solve for $T^{\\text{one}}$:\n   $$\n   c u^{\\text{one}} + d T^{\\text{one}} = f_T \\implies T^{\\text{one}} = \\frac{f_T - c u^{\\text{one}}}{d}\n   $$\n\nNext, we define the metrics to quantify the effect of this approximation.\n\nThe **error vector** $e$ is the difference between the exact two-way solution and the approximate one-way solution:\n$$\ne = x^{\\text{two}} - x^{\\text{one}}\n$$\nThe magnitude of this error is measured by its Euclidean norm, $E = \\|e\\|_2$.\n\nThe **residual vector** $r$ is obtained by substituting the approximate solution $x^{\\text{one}}$ back into the original two-way system equation and finding the difference from the forcing vector $f$:\n$$\nr = J x^{\\text{one}} - f\n$$\nLet's analyze the components of $r = [r_u, r_T]^\\top$. The first component is:\n$$\nr_u = a u^{\\text{one}} + b T^{\\text{one}} - f_u\n$$\nSubstituting $u^{\\text{one}} = f_u/a$, we get:\n$$\nr_u = a \\left(\\frac{f_u}{a}\\right) + b T^{\\text{one}} - f_u = f_u + b T^{\\text{one}} - f_u = b T^{\\text{one}}\n$$\nThe second component is:\n$$\nr_T = c u^{\\text{one}} + d T^{\\text{one}} - f_T\n$$\nBy the definition of $T^{\\text{one}}$, we have $c u^{\\text{one}} + d T^{\\text{one}} = f_T$. Therefore:\n$$\nr_T = f_T - f_T = 0\n$$\nThus, the residual vector has a specific structure: $r = [b T^{\\text{one}}, 0]^\\top$. The magnitude of the residual is measured by its Euclidean norm, $R = \\|r\\|_2 = \\sqrt{(b T^{\\text{one}})^2 + 0^2} = |b T^{\\text{one}}|$. This shows that the residual is non-zero only if the neglected coupling term $b$ is non-zero.\n\nThe **coupling strength metric** $\\Gamma$ is defined as the geometric mean of the scaled off-diagonal terms:\n$$\n\\Gamma = \\sqrt{\\left(\\frac{|b|}{a}\\right)\\left(\\frac{|c|}{d}\\right)}\n$$\nThis metric provides a heuristic measure of the system's coupling intensity.\n\nFinally, we relate the error and the residual. From their definitions, we have $J x^{\\text{two}} = f$ and $J x^{\\text{one}} = r + f$. Subtracting these equations gives:\n$$\nJ x^{\\text{two}} - J x^{\\text{one}} = f - (r+f)\n$$\n$$\nJ (x^{\\text{two}} - x^{\\text{one}}) = -r\n$$\n$$\nJ e = -r \\implies e = -J^{-1} r\n$$\nTaking the Euclidean norm and applying the property of induced matrix norms yields the well-known error bound:\n$$\n\\|e\\|_2 = \\|J^{-1} r\\|_2 \\le \\|J^{-1}\\|_2 \\|r\\|_2\n$$\nThe problem asks to compute this bound, $B = \\|J^{-1}\\|_2 \\cdot R$. The term $\\|J^{-1}\\|_2$ is the induced $2$-norm (or spectral norm) of the matrix $J^{-1}$. This norm is equal to the largest singular value of $J^{-1}$, which is the reciprocal of the smallest singular value of $J$, denoted $\\sigma_{\\min}(J)$.\n$$\n\\|J^{-1}\\|_2 = \\frac{1}{\\sigma_{\\min}(J)}\n$$\nThe singular values of $J$ are found via Singular Value Decomposition (SVD).\n\nThe algorithm for each test case is as follows:\n1.  Construct the matrix $J$ and vector $f$ from the given parameters $a, b, c, d, f_u, f_T$.\n2.  Compute $x^{\\text{one}} = [u^{\\text{one}}, T^{\\text{one}}]^\\top$ using the sequential formulas.\n3.  Compute $x^{\\text{two}}$ by solving the linear system $J x = f$.\n4.  Calculate the error vector $e = x^{\\text{two}} - x^{\\text{one}}$ and its norm $E = \\|e\\|_2$.\n5.  Calculate the residual vector $r = J x^{\\text{one}} - f$ and its norm $R = \\|r\\|_2$.\n6.  Calculate the coupling metric $\\Gamma$.\n7.  Perform SVD on $J$ to find $\\sigma_{\\min}(J)$.\n8.  Calculate the bound $B = (1/\\sigma_{\\min}(J)) \\cdot R$.\n9.  Collect the four values $[E, R, \\Gamma, B]$ for the final output.\nThis procedure is repeated for all provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# The scipy library is not required for this problem.\n\ndef solve():\n    \"\"\"\n    Solves the multiphysics coupling benchmark for a set of test cases.\n    \"\"\"\n    # Test cases defined in the problem statement.\n    # Format: (a, d, b, c, f_u, f_T)\n    test_cases = [\n        # Case 1 (weak, symmetric volume-like coupling)\n        (1.0, 1.0, 0.05, 0.05, 1.0, 0.0),\n        # Case 2 (strong, symmetric volume-like coupling)\n        (1.0, 1.0, 0.6, 0.6, 1.0, 0.0),\n        # Case 3 (asymmetric interface-like coupling)\n        (1.0, 1.0, 0.02, 0.5, 1.0, 0.5),\n        # Case 4 (near-singular strong coupling)\n        (1.0, 1.0, 0.95, 0.95, 1.0, 1.0),\n        # Case 5 (one off-diagonal zero, interface-like)\n        (1.0, 1.0, 0.0, 0.5, 1.0, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        a, d, b, c, f_u, f_T = case\n\n        # 1. Define J matrix and f vector\n        J = np.array([[a, b], [c, d]])\n        f = np.array([f_u, f_T])\n\n        # 2. Compute the two-way coupled solution x_two\n        # This is the exact solution to Jx = f\n        x_two = np.linalg.solve(J, f)\n\n        # 3. Compute the one-way coupled solution x_one\n        # Assumes u drives T, but not vice versa.\n        u_one = f_u / a\n        T_one = (f_T - c * u_one) / d\n        x_one = np.array([u_one, T_one])\n\n        # 4. Compute the residual vector r and its norm R\n        # r = J * x_one - f\n        r = J @ x_one - f\n        R = np.linalg.norm(r, ord=2)\n\n        # 5. Compute the error vector e and its norm E\n        # e = x_two - x_one\n        e = x_two - x_one\n        E = np.linalg.norm(e, ord=2)\n\n        # 6. Compute the coupling strength metric Gamma\n        Gamma = np.sqrt((np.abs(b) / a) * (np.abs(c) / d))\n\n        # 7. Compute the SVD-based bound B\n        # B = ||J_inv||_2 * ||r||_2\n        # ||J_inv||_2 = 1 / sigma_min(J)\n        singular_values = np.linalg.svd(J, compute_uv=False)\n        sigma_min = singular_values[-1] # SVD returns values in descending order\n\n        # Handle the case where the matrix is singular (sigma_min is zero).\n        # For this problem set, all matrices are invertible.\n        if sigma_min > 1e-15:\n            norm_J_inv = 1.0 / sigma_min\n            B = norm_J_inv * R\n        else:\n            # If r is nearly zero, the bound is 0. Otherwise, it's infinite.\n            B = 0.0 if R  1e-15 else np.inf\n\n        results.append([E, R, Gamma, B])\n\n    # Format the output as a single-line string: [[...],[...],...]\n    # Create a list of string representations for each inner list\n    inner_results_str = [f\"[{','.join(map(str, res))}]\" for res in results]\n    # Join the inner list strings and enclose in the outer brackets\n    final_output = f\"[{','.join(inner_results_str)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "When tackling a fully two-way coupled problem, engineers must choose a solution strategy, with the primary options being monolithic and partitioned approaches. This exercise delves into the fundamental trade-off between these methods, exploring how coupling strength dictates their relative efficiency . You will compare the rapid but costly iterations of a monolithic Conjugate Gradient solver against the cheaper but potentially slow-converging iterations of a partitioned block Gauss-Seidel scheme, learning to estimate performance based on the system's mathematical properties.",
            "id": "3500811",
            "problem": "Consider a toy two-field linear model for multiphysics coupled simulation. Let the unknowns be $u$ (e.g., structural displacement) and $p$ (e.g., fluid pressure). The discrete, linearized coupled system is represented by a $2 \\times 2$ block matrix,\n$$\n\\begin{bmatrix}\nA  C^{\\top} \\\\\nC  B\n\\end{bmatrix}\n\\begin{bmatrix}\nu \\\\\np\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nf \\\\\ng\n\\end{bmatrix},\n$$\nwhere $A \\in \\mathbb{R}$ and $B \\in \\mathbb{R}$ are positive scalars modeling the dominant volume operators for the two physics, and $C \\in \\mathbb{R}$ is a scalar coupling coefficient. For two-way coupling, $C$ represents mutual influence and the matrix is symmetric positive definite (SPD) provided $A > 0$, $B > 0$, and $C^{2}  A B$. For one-way coupling, the limiting case is $C = 0$, which removes feedback and decouples the fields. Interface coupling is characterized by smaller $|C|$ relative to $\\sqrt{A B}$ (weak coupling localized at the interface), while volume coupling yields larger $|C|$ relative to $\\sqrt{A B}$ (stronger distributed coupling).\n\nYou will compare the computational cost and accuracy between a partitioned approach and a monolithic approach on this toy model, by estimating iteration counts versus residual reduction per step. Use only linear algebra and convergence theory for linear iterations.\n\nDefinitions for the methods to compare:\n- Partitioned approach: use block Gauss–Seidel fixed-point iteration, viewed as solving $u$ with $p$ held at its current iterate, then solving $p$ with the updated $u$. Assume the cost per partitioned iteration is $c_A + c_B$, where $c_A$ and $c_B$ are given positive scalars representing the costs of solving the individual subproblems.\n- Monolithic approach: use Conjugate Gradient (CG) for the full $2 \\times 2$ SPD system. Assume the cost per monolithic iteration is $c_M$, a given positive scalar.\n\nAssume an initial residual norm of $1$ for both methods. For the partitioned approach, the residual reduction factor per iteration is equal to the spectral radius of the block Gauss–Seidel error propagation operator derived from the model. For the monolithic CG approach, use the standard bound for SPD systems that the error norm reduction per iteration is governed by the condition number $\\kappa$ of the matrix, via a reduction factor that depends on $\\kappa$. For $C = 0$ (one-way decoupled case), interpret the partitioned method as converging in a single iteration to zero residual and the monolithic method as having an iteration reduction factor consistent with the condition number of the diagonal matrix.\n\nYour task:\n1. Derive expressions needed to compute the per-iteration residual reduction factor for block Gauss–Seidel on the given $2 \\times 2$ scalar-block model.\n2. Derive how to compute the condition number $\\kappa$ of the SPD $2 \\times 2$ matrix and the corresponding per-iteration residual reduction factor for Conjugate Gradient (CG).\n3. For each test case, compute:\n   - the estimated iteration count to reduce the residual norm from $1$ down to a tolerance $\\mathrm{tol}$, using the derived reduction factor for each method,\n   - the total computational cost up to that iteration count (partitioned: $n_{\\mathrm{part}} \\cdot (c_A + c_B)$; monolithic: $n_{\\mathrm{mono}} \\cdot c_M$),\n   - the final residual norm after that many iterations (partitioned: $r_{\\mathrm{part}}^{n_{\\mathrm{part}}}$; monolithic: $r_{\\mathrm{mono}}^{n_{\\mathrm{mono}}}$),\n   where $n_{\\mathrm{part}}$ and $n_{\\mathrm{mono}}$ are the iteration counts and $r_{\\mathrm{part}}$, $r_{\\mathrm{mono}}$ are the per-iteration reduction factors.\n\nUse the following test suite, covering one-way versus two-way and interface versus volume coupling, as well as weak and strong coupling regimes:\n- Case $1$ (one-way, interface-like decoupled): $A = 1.0$, $B = 1.0$, $C = 0.0$, $c_A = 1.0$, $c_B = 1.0$, $c_M = 2.5$, $\\mathrm{tol} = 10^{-6}$.\n- Case $2$ (two-way, weak interface coupling): $A = 1.0$, $B = 1.0$, $C = 0.2$, $c_A = 1.0$, $c_B = 1.0$, $c_M = 2.5$, $\\mathrm{tol} = 10^{-6}$.\n- Case $3$ (two-way, strong interface coupling): $A = 1.0$, $B = 1.0$, $C = 0.9$, $c_A = 1.0$, $c_B = 1.0$, $c_M = 2.5$, $\\mathrm{tol} = 10^{-6}$.\n- Case $4$ (two-way, volume coupling moderate): $A = 10.0$, $B = 10.0$, $C = 5.0$, $c_A = 20.0$, $c_B = 20.0$, $c_M = 50.0$, $\\mathrm{tol} = 10^{-6}$.\n- Case $5$ (two-way, near-singular strong coupling): $A = 1.0$, $B = 4.0$, $C = 1.99$, $c_A = 2.0$, $c_B = 8.0$, $c_M = 16.0$, $\\mathrm{tol} = 10^{-6}$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list in the form $[n_{\\mathrm{part}}, \\mathrm{cost}_{\\mathrm{part}}, r_{\\mathrm{final,part}}, n_{\\mathrm{mono}}, \\mathrm{cost}_{\\mathrm{mono}}, r_{\\mathrm{final,mono}}]$, with entries as real numbers or integers. Aggregate the five case results into a single top-level list printed as one line, for example, $[[\\ldots],[\\ldots],[\\ldots],[\\ldots],[\\ldots]]$.\n\nAngle units are not applicable. No physical units are required in the output. Express any fractional values as decimals.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of numerical linear algebra, well-posed with all necessary information provided, and objective in its formulation. The parameters for all test cases satisfy the symmetric positive definite (SPD) condition $C^2  AB$. We can therefore proceed with the derivation and solution.\n\nThe multiphysics system is described by the $2 \\times 2$ block matrix equation:\n$$\n\\begin{bmatrix}\nA  C \\\\\nC  B\n\\end{bmatrix}\n\\begin{bmatrix}\nu \\\\\np\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nf \\\\\ng\n\\end{bmatrix}\n$$\nIn this scalar toy model, $A, B, C \\in \\mathbb{R}$. The system matrix is $K = \\begin{bmatrix} A  C \\\\ C  B \\end{bmatrix}$.\n\n### 1. Partitioned Approach: Block Gauss-Seidel\nThe block Gauss-Seidel method corresponds to a matrix splitting $K = M - N$, where $M$ is the lower block triangular part of $K$. The iteration scheme is $M x_{k+1} = N x_k + b$. The error $e_k = x_k - x^*$ propagates according to $e_{k+1} = M^{-1}N e_k$. The convergence rate is determined by the spectral radius of the iteration matrix $G = M^{-1}N$.\n\nFor our system, the splitting is:\n$$\nM = \\begin{bmatrix} A  0 \\\\ C  B \\end{bmatrix}, \\quad N = M - K = \\begin{bmatrix} 0  -C \\\\ 0  0 \\end{bmatrix}\n$$\nSince $A, B, C$ are scalars, the inverse of $M$ is:\n$$\nM^{-1} = \\frac{1}{AB - 0 \\cdot C} \\begin{bmatrix} B  0 \\\\ -C  A \\end{bmatrix} = \\begin{bmatrix} 1/A  0 \\\\ -C/(AB)  1/B \\end{bmatrix}\n$$\nThe iteration matrix $G$ is:\n$$\nG = M^{-1}N = \\begin{bmatrix} 1/A  0 \\\\ -C/(AB)  1/B \\end{bmatrix} \\begin{bmatrix} 0  -C \\\\ 0  0 \\end{bmatrix} = \\begin{bmatrix} 0  -C/A \\\\ 0  C^2/(AB) \\end{bmatrix}\n$$\nThe eigenvalues of this upper triangular matrix are its diagonal entries: $\\lambda_1 = 0$ and $\\lambda_2 = C^2/(AB)$. The spectral radius is the maximum of the absolute values of the eigenvalues:\n$$\nr_{\\mathrm{part}} = \\rho(G) = \\max \\left( |0|, \\left| \\frac{C^2}{AB} \\right| \\right) = \\frac{C^2}{AB}\n$$\nThis is the residual reduction factor per iteration for the partitioned approach. The condition $C^2  AB$ ensures that $r_{\\mathrm{part}}  1$, guaranteeing convergence. For the one-way coupling case where $C=0$, $r_{\\mathrm{part}}=0$, and the method converges in one iteration as specified.\n\n### 2. Monolithic Approach: Conjugate Gradient (CG)\nThe monolithic approach solves the full system using the Conjugate Gradient method. The convergence rate of CG for an SPD matrix $K$ is bounded by its condition number $\\kappa(K) = \\lambda_{\\max}(K) / \\lambda_{\\min}(K)$.\n\nFirst, we find the eigenvalues of $K$ from the characteristic equation $\\det(K - \\lambda I) = 0$:\n$$\n\\det \\begin{bmatrix} A-\\lambda  C \\\\ C  B-\\lambda \\end{bmatrix} = (A-\\lambda)(B-\\lambda) - C^2 = \\lambda^2 - (A+B)\\lambda + AB - C^2 = 0\n$$\nUsing the quadratic formula, the eigenvalues are:\n$$\n\\lambda = \\frac{(A+B) \\pm \\sqrt{(A+B)^2 - 4(AB-C^2)}}{2} = \\frac{(A+B) \\pm \\sqrt{(A-B)^2 + 4C^2}}{2}\n$$\nThe maximum and minimum eigenvalues are:\n$$\n\\lambda_{\\max} = \\frac{1}{2} \\left( A+B + \\sqrt{(A-B)^2 + 4C^2} \\right)\n$$\n$$\n\\lambda_{\\min} = \\frac{1}{2} \\left( A+B - \\sqrt{(A-B)^2 + 4C^2} \\right)\n$$\nThe condition number is $\\kappa = \\lambda_{\\max} / \\lambda_{\\min}$. The per-iteration reduction factor for the error norm in CG is bounded by:\n$$\nr_{\\mathrm{mono}} = \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\n$$\nWe use this as the estimated residual reduction factor per iteration. If $\\kappa=1$, then $r_{\\mathrm{mono}}=0$, and we assume convergence in one iteration.\n\n### 3. Iteration Count and Cost Calculation\nGiven an initial residual norm of $1$ and a target tolerance $\\mathrm{tol}$, the number of iterations $n$ required for a method with a reduction factor $r$ is the smallest integer such that $r^n \\le \\mathrm{tol}$. For $r \\in (0, 1)$, this is:\n$$\nn = \\left\\lceil \\frac{\\log(\\mathrm{tol})}{\\log(r)} \\right\\rceil\n$$\nIf $r=0$, we take $n=1$.\n\nThe total computational costs are then:\n- Partitioned: $\\mathrm{cost}_{\\mathrm{part}} = n_{\\mathrm{part}} \\cdot (c_A + c_B)$\n- Monolithic: $\\mathrm{cost}_{\\mathrm{mono}} = n_{\\mathrm{mono}} \\cdot c_M$\n\nThe final residual norm after $n$ iterations is calculated as $r_{\\mathrm{final}} = r^n$, given the initial residual norm is $1$.\n\nThe following calculations will be performed for each test case:\n- $n_{\\mathrm{part}}$: Partitioned iteration count.\n- $\\mathrm{cost}_{\\mathrm{part}}$: Partitioned total cost.\n- $r_{\\mathrm{final,part}}$: Partitioned final residual norm.\n- $n_{\\mathrm{mono}}$: Monolithic iteration count.\n- $\\mathrm{cost}_{\\mathrm{mono}}$: Monolithic total cost.\n- $r_{\\mathrm{final,mono}}$: Monolithic final residual norm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares partitioned and monolithic solver performance\n    for a 2x2 scalar coupled system based on linear iteration theory.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (one-way, interface-like decoupled)\n        {'A': 1.0, 'B': 1.0, 'C': 0.0, 'cA': 1.0, 'cB': 1.0, 'cM': 2.5, 'tol': 1e-6},\n        # Case 2 (two-way, weak interface coupling)\n        {'A': 1.0, 'B': 1.0, 'C': 0.2, 'cA': 1.0, 'cB': 1.0, 'cM': 2.5, 'tol': 1e-6},\n        # Case 3 (two-way, strong interface coupling)\n        {'A': 1.0, 'B': 1.0, 'C': 0.9, 'cA': 1.0, 'cB': 1.0, 'cM': 2.5, 'tol': 1e-6},\n        # Case 4 (two-way, volume coupling moderate)\n        {'A': 10.0, 'B': 10.0, 'C': 5.0, 'cA': 20.0, 'cB': 20.0, 'cM': 50.0, 'tol': 1e-6},\n        # Case 5 (two-way, near-singular strong coupling)\n        {'A': 1.0, 'B': 4.0, 'C': 1.99, 'cA': 2.0, 'cB': 8.0, 'cM': 16.0, 'tol': 1e-6},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        A, B, C = case['A'], case['B'], case['C']\n        cA, cB, cM = case['cA'], case['cB'], case['cM']\n        tol = case['tol']\n\n        # --- Partitioned (Block Gauss-Seidel) Calculation ---\n        if C == 0.0:\n            r_part = 0.0\n            n_part = 1\n        else:\n            # Spectral radius of Gauss-Seidel iteration matrix\n            r_part = (C**2) / (A * B)\n            if r_part >= 1.0:\n                 # Should not happen with valid SPD problems\n                 n_part = np.inf\n            else:\n                 n_part = int(np.ceil(np.log(tol) / np.log(r_part)))\n        \n        cost_part = n_part * (cA + cB)\n        r_final_part = r_part**n_part\n\n        # --- Monolithic (Conjugate Gradient) Calculation ---\n        # Eigenvalues of the 2x2 system matrix\n        sqrt_term = np.sqrt((A - B)**2 + 4 * C**2)\n        lambda_max = 0.5 * (A + B + sqrt_term)\n        lambda_min = 0.5 * (A + B - sqrt_term)\n        \n        # Condition number\n        if lambda_min == 0:\n            kappa = np.inf\n        else:\n            kappa = lambda_max / lambda_min\n\n        if kappa == 1.0:\n            # Ideal convergence for kappa=1\n            r_mono = 0.0\n            n_mono = 1\n        elif np.isinf(kappa):\n            r_mono = 1.0\n            n_mono = np.inf\n        else:\n            # CG convergence factor estimate\n            sqrt_kappa = np.sqrt(kappa)\n            r_mono = (sqrt_kappa - 1) / (sqrt_kappa + 1)\n            n_mono = int(np.ceil(np.log(tol) / np.log(r_mono)))\n\n        cost_mono = n_mono * cM\n        r_final_mono = r_mono**n_mono\n\n        case_results = [\n            n_part, float(cost_part), r_final_part,\n            n_mono, float(cost_mono), r_final_mono\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string as a list of lists\n    result_str = \"[\" + \",\".join([f\"[{','.join(map(str, res))}]\" for res in all_results]) + \"]\"\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from steady-state to time-dependent simulations introduces the critical challenge of numerical stability. An explicit, partitioned (or \"weak\") coupling scheme might be simple to implement, but the feedback inherent in a two-way coupled system can create numerical instabilities that cause the simulation to diverge. This hands-on practice demonstrates this crucial concept by having you analyze a simple dynamic system . By deriving the discrete update matrix and finding the eigenvalues, you will discover the precise stability boundary and prove that for explicit schemes, strong two-way coupling can lead to failure, a vital lesson for designing robust time-domain simulations.",
            "id": "3500848",
            "problem": "A pair of linearly coupled physics subsystems are modeled, after consistent linearization about a steady state and spatial discretization, by the first-order ordinary differential equations\n$$\n\\dot{x}(t) \\;=\\; -a\\,x(t) \\;+\\; b\\,y(t),\\qquad \\dot{y}(t) \\;=\\; -c\\,y(t) \\;+\\; d\\,x(t),\n$$\nwhere $a0$ and $c0$ encode intrinsic dissipation (relaxation) in each subsystem, and $b$ and $d$ encode the strength of inter-physics coupling. Consider a partitioned explicit Gauss–Seidel coupling with forward Euler time-stepping of size $h0$: update $x$ using $y$ from the previous time level, then update $y$ using the newly updated $x$. This scheme maps $(x^n,y^n)$ to $(x^{n+1},y^{n+1})$. Interpret the case $d=0$ as one-way coupling (subsystem $y$ drives $x$ but not vice versa), and the case $b\\neq 0$, $d\\neq 0$ as two-way coupling.\n\nYou are asked to analyze stability in the sense of discrete-time linear stability (both eigenvalues of the one-step update matrix lie strictly inside the unit disk in the complex plane), and to connect the analysis to interface versus volume coupling. Proceed from first principles and do not assume any pre-existing formula for eigenvalues of the update. Instead, derive the update matrix and reason directly from its characteristic polynomial.\n\n1. Derive the $2\\times 2$ linear update matrix $M(a,b,c,d,h)$ such that \n$$\n\\begin{pmatrix} x^{n+1} \\\\ y^{n+1} \\end{pmatrix} \\;=\\; M(a,b,c,d,h)\\,\\begin{pmatrix} x^{n} \\\\ y^{n} \\end{pmatrix}.\n$$\n\n2. Using the characteristic polynomial of $M(a,b,c,d,h)$ and the discrete-time stability criteria for $2\\times 2$ real matrices, show that for one-way coupling with $d=0$, the scheme is stable provided the uncoupled forward Euler steps are stable, namely $0ha2$ and $0hc2$.\n\n3. For two-way coupling with $b\\neq 0$ and $d\\neq 0$, determine the exact closed-form expression for the critical feedback condition at which an eigenvalue crosses $+1$ on the unit circle, thereby defining a sharp stability boundary in the $(b,d)$-plane. Your expression must be written purely in terms of $a$, $c$, $b$, and $d$ (and may depend on $h$ if it does not cancel).\n\n4. To compare interface versus volume coupling, adopt the idealized calibration that interface coupling enforces action–reaction symmetry $d=b$, whereas volume coupling introduces a dilution factor $0\\eta1$ so that $d=\\eta\\,b$. For the parameter values $a=2$, $c=5$, $h=0.1$, and $\\eta=\\tfrac{1}{4}$, compute the exact critical values of $b_{\\mathrm{crit,int}}$ for interface coupling and for $b_{\\mathrm{crit,vol}}$ volume coupling that place the scheme exactly on the stability boundary you derived in item $3$. Express the final answer as a two-entry row matrix $\\bigl(b_{\\mathrm{crit,int}}\\;\\;b_{\\mathrm{crit,vol}}\\bigr)$ in exact closed form. Do not include units. Do not approximate; rounding is not required.",
            "solution": "We begin from the given linearized model\n$$\n\\dot{x}(t) \\;=\\; -a\\,x(t) \\;+\\; b\\,y(t),\\qquad \\dot{y}(t) \\;=\\; -c\\,y(t) \\;+\\; d\\,x(t),\n$$\nwith $a0$ and $c0$. The partitioned explicit Gauss–Seidel update with forward Euler time-stepping of size $h0$ first advances $x$ using $y$ at time level $n$, then advances $y$ using the updated $x$ at time level $n+1$.\n\nFrom forward Euler applied to the first equation,\n$$\nx^{n+1} \\;=\\; x^{n} \\;+\\; h\\bigl(-a\\,x^{n} + b\\,y^{n}\\bigr) \\;=\\; (1 - h a)\\,x^{n} + h b\\,y^{n}.\n$$\nNext, advance $y$ using $x^{n+1}$ and $y^{n}$:\n$$\ny^{n+1} \\;=\\; y^{n} \\;+\\; h\\bigl(-c\\,y^{n} + d\\,x^{n+1}\\bigr)\n\\;=\\; y^{n} + h\\bigl(-c\\,y^{n} + d\\bigl((1-h a)\\,x^{n} + h b\\,y^{n}\\bigr)\\bigr).\n$$\nCollecting terms yields\n$$\ny^{n+1} \\;=\\; \\bigl(1 - h c + h^{2} b d\\bigr)\\,y^{n} \\;+\\; h d\\,(1 - h a)\\,x^{n}.\n$$\nTherefore the linear update matrix $M(a,b,c,d,h)$ mapping $(x^{n},y^{n})$ to $(x^{n+1},y^{n+1})$ is\n$$\nM(a,b,c,d,h) \\;=\\;\n\\begin{pmatrix}\n1 - h a  h b \\\\\nh d\\,(1 - h a)  1 - h c + h^{2} b d\n\\end{pmatrix}.\n$$\n\nTo analyze discrete-time stability, we consider the characteristic polynomial of $M$,\n$$\np(\\lambda) \\;=\\; \\lambda^{2} - \\operatorname{tr}(M)\\,\\lambda + \\det(M).\n$$\nCompute the trace:\n$$\n\\operatorname{tr}(M) \\;=\\; (1 - h a) + (1 - h c + h^{2} b d) \\;=\\; 2 - h(a + c) + h^{2} b d.\n$$\nCompute the determinant:\n$$\n\\det(M) \\;=\\; (1 - h a)\\,(1 - h c + h^{2} b d) - (h b)\\bigl(h d\\,(1 - h a)\\bigr).\n$$\nThe last two terms cancel:\n$$\n\\det(M) \\;=\\; (1 - h a)\\Bigl[(1 - h c + h^{2} b d) - h^{2} b d\\Bigr] \\;=\\; (1 - h a)\\,(1 - h c).\n$$\nA notable structural property of this explicit staggered scheme is that $\\det(M)$ is independent of the coupling strengths $b$ and $d$.\n\nDiscrete-time linear stability (both eigenvalues strictly inside the unit disk) for a real $2\\times 2$ matrix is characterized by the Jury conditions on the characteristic polynomial $p(\\lambda) = \\lambda^{2} + \\alpha_{1}\\lambda + \\alpha_{0}$; written in terms of trace and determinant, the necessary and sufficient inequalities are\n$$\n1 - \\operatorname{tr}(M) + \\det(M) \\;\\; 0,\\qquad\n1 + \\operatorname{tr}(M) + \\det(M) \\;\\; 0,\\qquad\n1 - \\det(M) \\;\\; 0.\n$$\n\nFirst, consider one-way coupling with $d=0$. The update matrix becomes upper triangular:\n$$\nM(a,b,c,0,h) \\;=\\; \\begin{pmatrix} 1 - h a  h b \\\\ 0  1 - h c \\end{pmatrix},\n$$\nso its eigenvalues are simply $\\lambda_{1} = 1 - h a$ and $\\lambda_{2} = 1 - h c$. The condition that both eigenvalues lie strictly inside the unit disk is $|1 - h a|  1$ and $|1 - h c|  1$, which is equivalent to $0  ha  2$ and $0  hc  2$. Hence, provided the uncoupled forward Euler steps are themselves stable, one-way coupling is stable regardless of the value of $b$.\n\nNext, consider two-way coupling with $b\\neq 0$ and $d\\neq 0$. To detect a crossing of the unit circle through $+1$, evaluate the characteristic polynomial at $\\lambda = 1$:\n$$\np(1) \\;=\\; 1 - \\operatorname{tr}(M) + \\det(M).\n$$\nUsing the expressions above,\n\\begin{align*}\np(1)\n= 1 - \\bigl(2 - h(a + c) + h^{2} b d\\bigr) + (1 - h a)(1 - h c) \\\\\n= \\bigl(1 - 2 + 1\\bigr) + h(a + c) - h^{2} b d - \\bigl(h a + h c - h^{2} a c\\bigr) \\\\\n= h^{2} a c - h^{2} b d \\\\\n= h^{2}\\,(a c - b d).\n\\end{align*}\nThus $p(1)=0$ if and only if $b d = a c$. Under the baseline uncoupled step-size conditions $0ha2$ and $0hc2$ (so that $1 - \\det(M)  0$ and $1 + \\operatorname{tr}(M) + \\det(M)  0$ initially hold for sufficiently small $b d$), increasing the two-way feedback $b d$ to exceed $a c$ makes $p(1)0$, which implies that one eigenvalue has crossed through $+1$ and the spectral radius has reached or exceeded $1$, leading to divergence. Therefore, the exact critical feedback condition for the eigenvalue crossing at $+1$ is\n$$\nb d \\;=\\; a c,\n$$\nindependent of the time-step size $h$.\n\nWe now specialize to interface versus volume coupling. For interface coupling, impose action–reaction symmetry $d=b$. The critical boundary $b d = a c$ then gives\n$$\nb_{\\mathrm{crit,int}}^{2} \\;=\\; a c \\quad\\Longrightarrow\\quad b_{\\mathrm{crit,int}} \\;=\\; \\sqrt{a c}.\n$$\nFor volume coupling, impose $d=\\eta\\,b$ with a dilution factor $0\\eta1$. The critical boundary becomes\n$$\n\\eta\\,b_{\\mathrm{crit,vol}}^{2} \\;=\\; a c \\quad\\Longrightarrow\\quad b_{\\mathrm{crit,vol}} \\;=\\; \\sqrt{\\frac{a c}{\\eta}}.\n$$\nWith the given values $a=2$, $c=5$, $h=0.1$, and $\\eta=\\tfrac{1}{4}$, we obtain\n$$\nac \\;=\\; 10,\\qquad b_{\\mathrm{crit,int}} \\;=\\; \\sqrt{10},\\qquad b_{\\mathrm{crit,vol}} \\;=\\; \\sqrt{\\frac{10}{1/4}} \\;=\\; \\sqrt{40} \\;=\\; 2\\sqrt{10}.\n$$\nThese are exact closed forms, and no rounding is required. The final requested row matrix is therefore\n$$\n\\bigl(b_{\\mathrm{crit,int}}\\;\\;b_{\\mathrm{crit,vol}}\\bigr) \\;=\\; \\bigl(\\sqrt{10}\\;\\;2\\sqrt{10}\\bigr).\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\sqrt{10}  2\\sqrt{10}\\end{pmatrix}}$$"
        }
    ]
}