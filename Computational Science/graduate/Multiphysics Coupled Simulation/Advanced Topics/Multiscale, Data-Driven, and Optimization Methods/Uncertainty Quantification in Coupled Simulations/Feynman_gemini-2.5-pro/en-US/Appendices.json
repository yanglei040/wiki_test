{
    "hands_on_practices": [
        {
            "introduction": "This first practice tackles the foundational task of uncertainty propagation, where we aim to understand how uncertainties in model inputs affect its outputs. We will employ a powerful and efficient non-intrusive method known as stochastic collocation, which treats the complex simulation as a 'black box.' By evaluating a nonlinearly coupled system at a few intelligently chosen points defined by a Smolyak sparse grid, you will learn to approximate the expected value of a key system response, a common task when assessing model performance under uncertainty .",
            "id": "3531582",
            "problem": "Consider a two-module steady-state multiphysics coupling with a thermal module and a structural module. The thermal module computes a scalar state $u$ and the structural module computes a scalar state $v$. The modules are coupled nonlinearly and depend on independent uncertain inputs $\\xi_1$ and $\\xi_2$, each distributed uniformly on the interval $[-1,1]$. For any realization $(\\xi_1,\\xi_2)$, the coupled algebraic system is\n$$\nu \\;=\\; 1 \\;+\\; 0.5\\,\\xi_1 \\;+\\; 0.1\\,v,\n$$\n$$\nv \\;=\\; 2 \\;+\\; 0.25\\,\\xi_2 \\;+\\; 0.2\\,u^{2}.\n$$\nAssume the physically admissible solution branch corresponds to the smaller real root when the system is reduced to a scalar polynomial for $v$.\n\nYou will approximate the expectation of the structural state $v$ using Non-Intrusive Stochastic Collocation with a two-dimensional level-$2$ Smolyak sparse grid constructed from one-dimensional Gauss–Legendre rules on $[-1,1]$. The one-dimensional quadrature rules are:\n- Order-$1$: node $0$ with weight $2$.\n- Order-$3$: nodes $-\\sqrt{3/5}$, $0$, and $\\sqrt{3/5}$ with weights $5/9$, $8/9$, and $5/9$, respectively.\n\nUse the Smolyak combination for two dimensions and level $2$ to form the five collocation nodes and their area-integration weights. Then convert these area-integration weights to probability weights appropriate for the expectation with respect to the uniform distribution on $[-1,1]^2$ by noting that the joint probability density is $1/4$ on the square. At each sparse-grid node, treat the modules non-intrusively by solving the coupled system to obtain $u$ and $v$ for that collocation input, and then evaluate the weighted sum to approximate $\\mathbb{E}[v]$.\n\nCompute the resulting expected value and express your final answer as a dimensionless real number. Round your answer to four significant figures.",
            "solution": "The user wants to find the expected value of the structural state variable $v$, denoted as $\\mathbb{E}[v]$, using a non-intrusive stochastic collocation method. The method is based on a level-$2$, $2$-dimensional Smolyak sparse grid.\n\nFirst, we must validate the problem statement.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- A coupled system of two nonlinear algebraic equations for scalar states $u$ and $v$:\n  $$u = 1 + 0.5\\xi_1 + 0.1v$$\n  $$v = 2 + 0.25\\xi_2 + 0.2u^2$$\n- Two independent uncertain inputs, $\\xi_1$ and $\\xi_2$, both following a uniform distribution on the interval $[-1, 1]$.\n- A condition for selecting the solution: the physically admissible branch corresponds to the smaller real root for $v$ when the system is reduced to a scalar polynomial.\n- The numerical integration method is a Smolyak sparse grid for two dimensions ($d=2$) and level $L=2$.\n- The 1D quadrature rules to be used are Gauss-Legendre rules:\n    - Order-$1$ (level $1$): $1$ node at $x=0$ with weight $w=2$. Let's call this rule $\\mathcal{Q}_1$.\n    - Order-$3$ (level $2$): $3$ nodes at $x \\in \\{-\\sqrt{3/5}, 0, \\sqrt{3/5}\\}$ with weights $w \\in \\{5/9, 8/9, 5/9\\}$, respectively. Let's call this rule $\\mathcal{Q}_2$.\n- The procedure is to:\n    1. Form the five collocation nodes and their area-integration weights.\n    2. Convert these to probability weights.\n    3. Solve the coupled system at each node to get $v(\\boldsymbol{\\xi}_i)$.\n    4. Compute the weighted sum to approximate $\\mathbb{E}[v]$.\n- The final answer should be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a standard application of uncertainty quantification in computational science. It is well-posed, as the problem specifies a unique solution branch to follow. The language is objective and precise. All necessary data and definitions are provided, and there are no contradictions. The problem is not trivial and requires a multi-step, rigorous application of a standard numerical method. Therefore, the problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\n### Solution Derivation\n\nThe goal is to approximate the expectation $\\mathbb{E}[v]$:\n$$ \\mathbb{E}[v] = \\int_{-1}^{1}\\int_{-1}^{1} v(\\xi_1, \\xi_2) p(\\xi_1, \\xi_2) \\,d\\xi_1 d\\xi_2 $$\nSince $\\xi_1$ and $\\xi_2$ are independent and uniformly distributed on $[-1, 1]$, the joint probability density function (PDF) is $p(\\xi_1, \\xi_2) = p_1(\\xi_1)p_2(\\xi_2) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$ on the domain $[-1, 1]^2$.\nThe expectation is therefore:\n$$ \\mathbb{E}[v] = \\frac{1}{4} \\int_{-1}^{1}\\int_{-1}^{1} v(\\xi_1, \\xi_2) \\,d\\xi_1 d\\xi_2 $$\nThe integral will be approximated using a Smolyak sparse grid quadrature rule.\n\n**1. Construct the Smolyak Sparse Grid Quadrature Rule**\n\nThe problem specifies a level-$2$ ($L=2$), $2$-dimensional ($d=2$) Smolyak grid, built from a level-$1$ rule $\\mathcal{Q}_1$ (1-point Gauss-Legendre) and a level-$2$ rule $\\mathcal{Q}_2$ (3-point Gauss-Legendre). The Smolyak quadrature operator $\\mathcal{A}_{L,d}$ is constructed using the combination technique:\n$$ \\mathcal{A}_{L,d} = \\mathcal{A}_{2,2} = \\mathcal{Q}_1 \\otimes \\mathcal{Q}_2 + \\mathcal{Q}_2 \\otimes \\mathcal{Q}_1 - \\mathcal{Q}_1 \\otimes \\mathcal{Q}_1 $$\nThe nodes of this sparse grid are the union of the nodes of the constituent tensor product grids. Let $\\alpha = \\sqrt{3/5}$.\n- Nodes of $\\mathcal{Q}_1$: $\\{0\\}$\n- Nodes of $\\mathcal{Q}_2$: $\\{-\\alpha, 0, \\alpha\\}$\nThe union of nodes from $\\mathcal{X}_1 \\!\\times\\! \\mathcal{X}_2$, $\\mathcal{X}_2 \\!\\times\\! \\mathcal{X}_1$, and $\\mathcal{X}_1 \\!\\times\\! \\mathcal{X}_1$ gives the 5-point set:\n$$ \\mathcal{H}_{2,2} = \\{ (0,0), (\\pm \\alpha, 0), (0, \\pm \\alpha) \\} $$\nThe area-integration weights $w_i^{\\text{area}}$ are derived from the combination formula. For a function $f(\\xi_1, \\xi_2)$:\n$$ \\int_{-1}^{1}\\int_{-1}^{1} f(\\xi_1, \\xi_2) \\,d\\xi_1 d\\xi_2 \\approx \\sum_{i} w_i^{\\text{area}} f(\\boldsymbol{\\xi}_i) $$\nApplying the combination formula to the weights:\n- For the point $(0,0)$: The weight is $w_1(0)w_2(0) + w_2(0)w_1(0) - w_1(0)w_1(0) = (2)(\\frac{8}{9}) + (\\frac{8}{9})(2) - (2)(2) = \\frac{16}{9} + \\frac{16}{9} - 4 = \\frac{32-36}{9} = -\\frac{4}{9}$.\n- For points $(\\pm \\alpha, 0)$: The weight is $w_2(\\pm \\alpha)w_1(0) = (\\frac{5}{9})(2) = \\frac{10}{9}$.\n- For points $(0, \\pm \\alpha)$: The weight is $w_1(0)w_2(\\pm \\alpha) = (2)(\\frac{5}{9}) = \\frac{10}{9}$.\n\nThe five collocation nodes $\\boldsymbol{\\xi}_i = (\\xi_{1,i}, \\xi_{2,i})$ and their area-integration weights $w_i^{\\text{area}}$ are:\n1. $\\boldsymbol{\\xi}_1 = (-\\sqrt{3/5}, 0)$, $w_1^{\\text{area}} = 10/9$\n2. $\\boldsymbol{\\xi}_2 = (\\sqrt{3/5}, 0)$, $w_2^{\\text{area}} = 10/9$\n3. $\\boldsymbol{\\xi}_3 = (0, -\\sqrt{3/5})$, $w_3^{\\text{area}} = 10/9$\n4. $\\boldsymbol{\\xi}_4 = (0, \\sqrt{3/5})$, $w_4^{\\text{area}} = 10/9$\n5. $\\boldsymbol{\\xi}_5 = (0, 0)$, $w_5^{\\text{area}} = -4/9$\n\nThe probability weights $w_i^{\\text{prob}}$ are obtained by dividing the area-integration weights by the area of the domain, which is $4$:\n$$ w_i^{\\text{prob}} = \\frac{w_i^{\\text{area}}}{4} $$\n1. $w_{1,2,3,4}^{\\text{prob}} = \\frac{10/9}{4} = \\frac{10}{36} = \\frac{5}{18}$\n2. $w_5^{\\text{prob}} = \\frac{-4/9}{4} = -\\frac{4}{36} = -\\frac{1}{9}$\nThe sum of these weights is $4 \\cdot (5/18) - 1/9 = 20/18 - 2/18 = 18/18 = 1$, as required.\n\n**2. Solve the Coupled System at Each Node**\n\nFirst, we reduce the system to a single quadratic equation for $v$. Substitute $u$ from the first equation into the second:\n$$ v = 2 + 0.25\\xi_2 + 0.2(1 + 0.5\\xi_1 + 0.1v)^2 $$\n$$ v = 2 + 0.25\\xi_2 + 0.2[ (1 + 0.5\\xi_1)^2 + 0.2(1 + 0.5\\xi_1)v + 0.01v^2 ] $$\n$$ v = 2 + 0.25\\xi_2 + 0.2(1 + \\xi_1 + 0.25\\xi_1^2) + 0.04(1 + 0.5\\xi_1)v + 0.002v^2 $$\nRearranging into the form $Av^2 + Bv + C = 0$:\n$$ 0.002v^2 + [0.04 + 0.02\\xi_1 - 1]v + [2.2 + 0.2\\xi_1 + 0.05\\xi_1^2 + 0.25\\xi_2] = 0 $$\nThe coefficients are:\n$$ A = 0.002 $$\n$$ B = -0.96 + 0.02\\xi_1 $$\n$$ C = 2.2 + 0.2\\xi_1 + 0.05\\xi_1^2 + 0.25\\xi_2 $$\nThe solution for $v$ is given by the quadratic formula, $v = \\frac{-B \\pm \\sqrt{B^2-4AC}}{2A}$. The problem specifies using the smaller real root, which corresponds to the minus sign in the numerator since $A > 0$.\n$$ v(\\xi_1, \\xi_2) = \\frac{-B - \\sqrt{B^2 - 4AC}}{2A} $$\nWe now evaluate $v$ at each of the five collocation nodes. Let $\\alpha = \\sqrt{3/5} \\approx 0.77459667$.\n\n- **Node 1: $\\boldsymbol{\\xi}_1 = (-\\alpha, 0)$**\n  $\\xi_1 = -\\alpha, \\xi_2 = 0$.\n  $B = -0.96 - 0.02\\alpha \\approx -0.975492$\n  $C = 2.2 - 0.2\\alpha + 0.05\\alpha^2 = 2.2 - 0.2\\alpha + 0.05(0.6) = 2.23 - 0.2\\alpha \\approx 2.075081$\n  $v_1 \\approx \\frac{0.975492 - \\sqrt{(-0.975492)^2 - 4(0.002)(2.075081)}}{0.004} \\approx 2.136577$\n\n- **Node 2: $\\boldsymbol{\\xi}_2 = (\\alpha, 0)$**\n  $\\xi_1 = \\alpha, \\xi_2 = 0$.\n  $B = -0.96 + 0.02\\alpha \\approx -0.944508$\n  $C = 2.2 + 0.2\\alpha + 0.05\\alpha^2 = 2.23 + 0.2\\alpha \\approx 2.384919$\n  $v_2 \\approx \\frac{0.944508 - \\sqrt{(-0.944508)^2 - 4(0.002)(2.384919)}}{0.004} \\approx 2.538684$\n\n- **Node 3: $\\boldsymbol{\\xi}_3 = (0, -\\alpha)$**\n  $\\xi_1 = 0, \\xi_2 = -\\alpha$.\n  $B = -0.96$\n  $C = 2.2 - 0.25\\alpha \\approx 2.006351$\n  $v_3 \\approx \\frac{0.96 - \\sqrt{(-0.96)^2 - 4(0.002)(2.006351)}}{0.004} \\approx 2.099129$\n\n- **Node 4: $\\boldsymbol{\\xi}_4 = (0, \\alpha)$**\n  $\\xi_1 = 0, \\xi_2 = \\alpha$.\n  $B = -0.96$\n  $C = 2.2 + 0.25\\alpha \\approx 2.393649$\n  $v_4 \\approx \\frac{0.96 - \\sqrt{(-0.96)^2 - 4(0.002)(2.393649)}}{0.004} \\approx 2.506473$\n\n- **Node 5: $\\boldsymbol{\\xi}_5 = (0, 0)$**\n  $\\xi_1 = 0, \\xi_2 = 0$.\n  $B = -0.96$\n  $C = 2.2$\n  $v_5 \\approx \\frac{0.96 - \\sqrt{(-0.96)^2 - 4(0.002)(2.2)}}{0.004} \\approx 2.302714$\n\n**3. Compute the Expected Value**\n\nThe approximation for the expected value of $v$ is the weighted sum of the values $v_i$:\n$$ \\mathbb{E}[v] \\approx \\sum_{i=1}^{5} w_i^{\\text{prob}} v_i $$\n$$ \\mathbb{E}[v] \\approx \\frac{5}{18}(v_1 + v_2 + v_3 + v_4) - \\frac{1}{9}v_5 $$\nSubstituting the computed values:\n$$ \\mathbb{E}[v] \\approx \\frac{5}{18}(2.136577 + 2.538684 + 2.099129 + 2.506473) - \\frac{1}{9}(2.302714) $$\n$$ \\mathbb{E}[v] \\approx \\frac{5}{18}(9.280863) - \\frac{1}{9}(2.302714) $$\n$$ \\mathbb{E}[v] \\approx 2.577906 - 0.255857 $$\n$$ \\mathbb{E}[v] \\approx 2.322049 $$\nUsing higher precision for the intermediate calculations gives:\n$$ \\mathbb{E}[v] \\approx 2.32216035 $$\nRounding the result to four significant figures gives $2.322$.",
            "answer": "$$\\boxed{2.322}$$"
        },
        {
            "introduction": "Having explored how to propagate uncertainty forward, we now turn to the inverse problem: learning about uncertain model parameters from experimental data. This exercise guides you through a Bayesian inference problem for a linearized coupled system, a cornerstone of modern uncertainty quantification . You will derive and compute the posterior distribution of a coupling parameter by rigorously combining prior knowledge with information from both direct observations and the underlying physics equations, demonstrating how data reduces our uncertainty about the model.",
            "id": "3531589",
            "problem": "Consider a linearized multiphysics coupling between a thermal field and a mechanical displacement field in a steady state setting, where the interface coupling strength is an unknown scalar parameter $\\theta \\in \\mathbb{R}$. Let the state vector be $u \\in \\mathbb{R}^{2}$, where the first component represents a coarse discretization of temperature and the second component represents a coarse discretization of displacement near the interface. You are given a Gaussian prior on the state and the parameter, assumed independent: $u \\sim \\mathcal{N}(0, C_{u})$ and $\\theta \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$. The observation operator couples both physics into a data vector $y \\in \\mathbb{R}^{2}$ as a linear function of $u$ and $\\theta$ with additive Gaussian noise, and the linearized physics residual acts as a “soft” constraint with additive model error. Specifically, the data and physics models are\n$$y = H u + \\theta\\, b + e,$$\n$$0 = A u - \\theta\\, G + w,$$\nwhere $e \\sim \\mathcal{N}(0, R)$ is the observation noise and $w \\sim \\mathcal{N}(0, Q)$ is the model error, with $e$ and $w$ independent of each other and of $(u,\\theta)$. All distributions are Gaussian and all operators are linear, so the posterior is Gaussian.\n\nStarting from Bayes’ theorem, and using only the definitions of Gaussian densities and linear algebraic identities, derive the marginal posterior variance of $\\theta$ as a Schur complement of the joint posterior precision of $(u, \\theta)$. Then, for the following concrete, scientifically plausible multiphysics setting,\n$$C_{u} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix}, \\quad \\mu_{0} = 0, \\quad \\sigma_{0}^{2} = 1,$$\n$$H = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\quad R = \\begin{pmatrix} \\tfrac{1}{4} & 0 \\\\ 0 & \\tfrac{1}{4} \\end{pmatrix},$$\n$$A = \\begin{pmatrix} 2 & -1 \\\\ 0 & 1 \\end{pmatrix}, \\quad G = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad Q = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\tfrac{1}{2} \\end{pmatrix},$$\ncompute the exact value of the marginal posterior variance of $\\theta$. Express your final answer as an exact rational number. No rounding is required. State any intermediate matrix you construct and justify each step from first principles of Gaussian conditioning and block matrix inversion.\n\nNote: The task is a Bayesian inverse problem for a coupled simulation in the context of Uncertainty Quantification (UQ) for multiphysics coupling. You may treat the physics residual equation as an additional “data” equation with known error statistics. The final answer must be a single real number (dimensionless). Do not include any units in your final answer.",
            "solution": "The user wants me to solve a Bayesian inverse problem.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- State vector: $u \\in \\mathbb{R}^{2}$, representing temperature and displacement.\n- Unknown scalar parameter: $\\theta \\in \\mathbb{R}$, representing interface coupling strength.\n- Prior distributions (independent):\n    - $u \\sim \\mathcal{N}(0, C_{u})$ with $C_{u} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix}$.\n    - $\\theta \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$ with $\\mu_{0} = 0$ and $\\sigma_{0}^{2} = 1$.\n- Data model equation: $y = H u + \\theta\\, b + e$, where $y \\in \\mathbb{R}^{2}$ is the data vector.\n    - $H = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n    - $b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n    - Observation noise: $e \\sim \\mathcal{N}(0, R)$ with $R = \\begin{pmatrix} \\tfrac{1}{4} & 0 \\\\ 0 & \\tfrac{1}{4} \\end{pmatrix}$.\n- Physics model equation (soft constraint): $0 = A u - \\theta\\, G + w$.\n    - $A = \\begin{pmatrix} 2 & -1 \\\\ 0 & 1 \\end{pmatrix}$.\n    - $G = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n    - Model error: $w \\sim \\mathcal{N}(0, Q)$ with $Q = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\tfrac{1}{2} \\end{pmatrix}$.\n- The noise terms $e$ and $w$ are independent of each other and of $(u, \\theta)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined Bayesian inverse problem for a linear system with Gaussian priors and noise.\n- **Scientifically Grounded**: The problem uses the framework of Bayesian inference, a standard and rigorous method for uncertainty quantification in scientific and engineering models. The setup of a linearized coupled physics problem with uncertain parameters is a common and realistic scenario in computational science and engineering. There are no violations of mathematical logic or scientific principles.\n- **Well-Posed**: The problem is well-posed. The combination of prior information and data (including the physics constraint) regularizes the problem, ensuring that a unique posterior distribution exists. The linearity of the models and the Gaussian nature of all distributions guarantee that the posterior is also a Gaussian, for which the mean and covariance are well-defined.\n- **Objective**: The problem is stated using precise mathematical language (matrices, vectors, probability distributions). All terms are defined, and the task is specific and unambiguous.\n- **Self-contained and Consistent**: All necessary matrices, parameters, and distributions are provided. There are no contradictions in the setup. The dimensions of all matrices and vectors are consistent for the defined operations.\n- **Relevant**: The problem is directly related to the topic of uncertainty quantification (UQ) in coupled multiphysics simulations, as stated in the note.\n\n### Step 3: Verdict and Action\nThe problem is valid and self-contained. I will proceed with the full solution.\n\n### Part 1: Theoretical Derivation\n\nThe goal is to find the marginal posterior variance of the parameter $\\theta$. We start from Bayes' theorem, which states that the posterior probability density function (PDF) is proportional to the product of the likelihood and the prior PDF:\n$$ p(u, \\theta | y) \\propto p(y | u, \\theta) p(u, \\theta) $$\nThe problem is set up within a linear-Gaussian framework. We first combine the state vector $u$ and the parameter $\\theta$ into a single augmented state vector $x \\in \\mathbb{R}^{3}$:\n$$ x = \\begin{pmatrix} u \\\\ \\theta \\end{pmatrix} $$\nThe prior distributions for $u$ and $\\theta$ are independent Gaussians: $u \\sim \\mathcal{N}(0, C_u)$ and $\\theta \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$. Therefore, the prior for the augmented state $x$ is also a Gaussian, $x \\sim \\mathcal{N}(\\mu_{prior}, C_{prior})$, with mean vector and covariance matrix given by:\n$$ \\mu_{prior} = \\begin{pmatrix} 0 \\\\ \\mu_0 \\end{pmatrix}, \\quad C_{prior} = \\begin{pmatrix} C_u & 0 \\\\ 0 & \\sigma_0^2 \\end{pmatrix} $$\nThe PDF of the prior is proportional to $\\exp\\left(-\\frac{1}{2}(x - \\mu_{prior})^T P_{prior} (x - \\mu_{prior})\\right)$, where $P_{prior} = C_{prior}^{-1}$ is the prior precision matrix.\n\nNext, we combine the data model and the physics constraint into a single linear system. The problem statement advises treating the physics residual as additional data. Let's rewrite the given equations:\n$$ y = H u + \\theta b + e \\implies y - (H u + \\theta b) = e $$\n$$ 0 = A u - \\theta G + w \\implies 0 - (A u - \\theta G) = w $$\nThis can be expressed in a unified matrix-vector form:\n$$ \\begin{pmatrix} y \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} H & b \\\\ A & -G \\end{pmatrix} \\begin{pmatrix} u \\\\ \\theta \\end{pmatrix} + \\begin{pmatrix} e \\\\ w \\end{pmatrix} $$\nLet's define the augmented data vector $d = \\begin{pmatrix} y \\\\ 0 \\end{pmatrix}$, the augmented forward operator $\\mathcal{M} = \\begin{pmatrix} H & b \\\\ A & -G \\end{pmatrix}$, and the augmented noise vector $\\xi = \\begin{pmatrix} e \\\\ w \\end{pmatrix}$. The system is now a standard linear model:\n$$ d = \\mathcal{M} x + \\xi $$\nSince $e$ and $w$ are independent Gaussian random vectors, $\\xi$ is also a zero-mean Gaussian, $\\xi \\sim \\mathcal{N}(0, C_{\\xi})$, with a block-diagonal covariance matrix:\n$$ C_{\\xi} = \\begin{pmatrix} R & 0 \\\\ 0 & Q \\end{pmatrix} $$\nThe likelihood function $p(d|x)$ is the PDF of the noise $\\xi = d - \\mathcal{M}x$, which is proportional to $\\exp\\left(-\\frac{1}{2}(d - \\mathcal{M}x)^T P_{\\xi} (d - \\mathcal{M}x)\\right)$, where $P_{\\xi} = C_{\\xi}^{-1}$ is the noise precision matrix.\n\nAccording to Bayes' theorem, the posterior PDF $p(x|d)$ is proportional to the product of the likelihood and prior PDFs:\n$$ p(x|d) \\propto \\exp\\left(-\\frac{1}{2}(d - \\mathcal{M}x)^T P_{\\xi} (d - \\mathcal{M}x)\\right) \\exp\\left(-\\frac{1}{2}(x - \\mu_{prior})^T P_{prior} (x - \\mu_{prior})\\right) $$\nThe posterior is proportional to $\\exp(-\\frac{1}{2} J(x))$, where $J(x)$ is the sum of the quadratic forms:\n$$ J(x) = (d - \\mathcal{M}x)^T P_{\\xi} (d - \\mathcal{M}x) + (x - \\mu_{prior})^T P_{prior} (x - \\mu_{prior}) $$\nExpanding the terms, we get:\n$$ J(x) = x^T (\\mathcal{M}^T P_{\\xi} \\mathcal{M} + P_{prior}) x - 2 x^T (\\mathcal{M}^T P_{\\xi} d + P_{prior} \\mu_{prior}) + \\text{const.} $$\nThis is a quadratic form in $x$, which implies that the posterior distribution is also Gaussian, $x|d \\sim \\mathcal{N}(\\mu_{post}, C_{post})$. By completing the square, we identify the posterior precision matrix $P_{post} = C_{post}^{-1}$ as the term multiplying $x^T(\\dots)x$:\n$$ P_{post} = \\mathcal{M}^T P_{\\xi} \\mathcal{M} + P_{prior} $$\nThe posterior covariance $C_{post}$ is the inverse of the posterior precision, $C_{post} = P_{post}^{-1}$. Notice that the posterior covariance is independent of the observation data $d$.\n\nTo find the marginal posterior variance of $\\theta$, we partition the posterior precision matrix $P_{post}$ and its inverse $C_{post}$ according to the components of $x = (u^T, \\theta)^T$:\n$$ P_{post} = \\begin{pmatrix} P_{uu} & P_{u\\theta} \\\\ P_{\\theta u} & P_{\\theta\\theta} \\end{pmatrix}, \\quad C_{post} = \\begin{pmatrix} C_{uu} & C_{u\\theta} \\\\ C_{\\theta u} & C_{\\theta\\theta} \\end{pmatrix} $$\nThe marginal covariance of $\\theta$ is the scalar block $C_{\\theta\\theta}$. Using the formula for the inverse of a block matrix, this block is given by:\n$$ C_{\\theta\\theta} = (P_{\\theta\\theta} - P_{\\theta u} P_{uu}^{-1} P_{u\\theta})^{-1} $$\nThe term $S_u = P_{\\theta\\theta} - P_{\\theta u} P_{uu}^{-1} P_{u\\theta}$ is the Schur complement of the block $P_{uu}$ in the precision matrix $P_{post}$. Therefore, the marginal posterior variance of $\\theta$, denoted $\\sigma_{\\theta|d}^2$, is the inverse of this Schur complement. This completes the required derivation.\n\n### Part 2: Numerical Computation\n\nWe now substitute the given values to compute the posterior variance of $\\theta$.\n\n1.  **Prior Precision Matrix ($P_{prior}$)**\n    $C_u = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix} \\implies C_u^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1/4 \\end{pmatrix}$.\n    $\\sigma_0^2 = 1 \\implies 1/\\sigma_0^2 = 1$.\n    The state vector is $x = (u_1, u_2, \\theta)^T$. The prior precision matrix is:\n    $$ P_{prior} = C_{prior}^{-1} = \\begin{pmatrix} C_u^{-1} & 0 \\\\ 0 & 1/\\sigma_0^2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{4} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} $$\n\n2.  **Augmented Operators ($\\mathcal{M}, P_{\\xi}$)**\n    $$ \\mathcal{M} = \\begin{pmatrix} H & b \\\\ A & -G \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 2 \\\\ 2 & -1 & -1 \\\\ 0 & 1 & 1 \\end{pmatrix} $$\n    $R = \\begin{pmatrix} 1/4 & 0 \\\\ 0 & 1/4 \\end{pmatrix} \\implies R^{-1} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 4 \\end{pmatrix}$.\n    $Q = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1/2 \\end{pmatrix} \\implies Q^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}$.\n    The noise precision matrix is:\n    $$ P_{\\xi} = C_{\\xi}^{-1} = \\begin{pmatrix} R^{-1} & 0 \\\\ 0 & Q^{-1} \\end{pmatrix} = \\begin{pmatrix} 4 & 0 & 0 & 0 \\\\ 0 & 4 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 2 \\end{pmatrix} $$\n\n3.  **Likelihood Precision Term ($\\mathcal{M}^T P_{\\xi} \\mathcal{M}$)**\n    We compute the contribution from the likelihood, which we denote as $P_{like}$.\n    $$ \\mathcal{M}^T P_{\\xi} = \\begin{pmatrix} 1 & 0 & 2 & 0 \\\\ 0 & 1 & -1 & 1 \\\\ 1 & 2 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & 0 & 0 & 0 \\\\ 0 & 4 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 & 2 & 0 \\\\ 0 & 4 & -1 & 2 \\\\ 4 & 8 & -1 & 2 \\end{pmatrix} $$\n    $$ P_{like} = (\\mathcal{M}^T P_{\\xi}) \\mathcal{M} = \\begin{pmatrix} 4 & 0 & 2 & 0 \\\\ 0 & 4 & -1 & 2 \\\\ 4 & 8 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 2 \\\\ 2 & -1 & -1 \\\\ 0 & 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 8 & -2 & 2 \\\\ -2 & 7 & 11 \\\\ 2 & 11 & 23 \\end{pmatrix} $$\n\n4.  **Posterior Precision Matrix ($P_{post}$)**\n    $$ P_{post} = P_{like} + P_{prior} = \\begin{pmatrix} 8 & -2 & 2 \\\\ -2 & 7 & 11 \\\\ 2 & 11 & 23 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{4} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 9 & -2 & 2 \\\\ -2 & 7+\\frac{1}{4} & 11 \\\\ 2 & 11 & 24 \\end{pmatrix} $$\n    $$ P_{post} = \\begin{pmatrix} 9 & -2 & 2 \\\\ -2 & \\frac{29}{4} & 11 \\\\ 2 & 11 & 24 \\end{pmatrix} $$\n\n5.  **Schur Complement Calculation**\n    We partition $P_{post}$ into blocks for $u$ and $\\theta$:\n    $$ P_{uu} = \\begin{pmatrix} 9 & -2 \\\\ -2 & \\frac{29}{4} \\end{pmatrix}, \\quad P_{u\\theta} = \\begin{pmatrix} 2 \\\\ 11 \\end{pmatrix}, \\quad P_{\\theta u} = \\begin{pmatrix} 2 & 11 \\end{pmatrix}, \\quad P_{\\theta\\theta} = 24 $$\n    First, we compute the inverse of $P_{uu}$:\n    $$ \\det(P_{uu}) = 9 \\left(\\frac{29}{4}\\right) - (-2)(-2) = \\frac{261}{4} - 4 = \\frac{261 - 16}{4} = \\frac{245}{4} $$\n    $$ P_{uu}^{-1} = \\frac{1}{\\det(P_{uu})} \\begin{pmatrix} \\frac{29}{4} & 2 \\\\ 2 & 9 \\end{pmatrix} = \\frac{4}{245} \\begin{pmatrix} \\frac{29}{4} & 2 \\\\ 2 & 9 \\end{pmatrix} = \\frac{1}{245} \\begin{pmatrix} 29 & 8 \\\\ 8 & 36 \\end{pmatrix} $$\n    Now we compute the term $P_{\\theta u} P_{uu}^{-1} P_{u\\theta}$:\n    $$ P_{\\theta u} P_{uu}^{-1} = \\begin{pmatrix} 2 & 11 \\end{pmatrix} \\frac{1}{245} \\begin{pmatrix} 29 & 8 \\\\ 8 & 36 \\end{pmatrix} = \\frac{1}{245} \\begin{pmatrix} 2(29)+11(8) & 2(8)+11(36) \\end{pmatrix} $$\n    $$ = \\frac{1}{245} \\begin{pmatrix} 58+88 & 16+396 \\end{pmatrix} = \\frac{1}{245} \\begin{pmatrix} 146 & 412 \\end{pmatrix} $$\n    $$ P_{\\theta u} P_{uu}^{-1} P_{u\\theta} = \\frac{1}{245} \\begin{pmatrix} 146 & 412 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 11 \\end{pmatrix} = \\frac{146(2) + 412(11)}{245} = \\frac{292 + 4532}{245} = \\frac{4824}{245} $$\n    The Schur complement (which is the marginal precision for $\\theta$) is:\n    $$ S_u = P_{\\theta\\theta} - P_{\\theta u} P_{uu}^{-1} P_{u\\theta} = 24 - \\frac{4824}{245} = \\frac{24 \\times 245 - 4824}{245} $$\n    $$ = \\frac{5880 - 4824}{245} = \\frac{1056}{245} $$\n\n6.  **Marginal Posterior Variance of $\\theta$**\n    The marginal variance is the inverse of the Schur complement:\n    $$ \\sigma_{\\theta|d}^2 = S_u^{-1} = \\left(\\frac{1056}{245}\\right)^{-1} = \\frac{245}{1056} $$\n    The prime factorization of the numerator is $245 = 5 \\times 7^2$. The prime factorization of the denominator is $1056 = 32 \\times 33 = 2^5 \\times 3 \\times 11$. There are no common factors, so the fraction is irreducible.",
            "answer": "$$\\boxed{\\frac{245}{1056}}$$"
        },
        {
            "introduction": "Real-world multiphysics models often contain dozens or even hundreds of uncertain parameters, leading to the 'curse of dimensionality.' This final practice introduces a modern, gradient-based dimension reduction technique called Active Subspace analysis to discover the few most influential combinations of parameters. In this hands-on coding exercise, you will implement a Monte Carlo estimator to compute the active subspace for a coupled model, providing a powerful method for model simplification, targeted experimentation, and building accurate, low-dimensional surrogate models .",
            "id": "3531593",
            "problem": "Consider the following uncertainty quantification setting for a coupled multiphysics surrogate model. Let the random input be a vector $x \\in \\mathbb{R}^6$ whose components are independent and identically distributed according to the standard normal distribution $\\mathcal{N}(0,1)$. Define two subsystem response functions,\none representing a simplified thermomechanical surrogate and the other a simplified fluid-structure surrogate, by\n$$\nu(x) = a_A^\\top x + \\sigma_A \\tanh\\!\\big(\\eta_A^\\top x\\big),\n\\qquad\nv(x) = a_B^\\top x + \\sigma_B \\sin\\!\\big(\\eta_B^\\top x\\big),\n$$\nwhere $a_A, a_B, \\eta_A, \\eta_B \\in \\mathbb{R}^6$ are fixed coefficient vectors, and $\\sigma_A, \\sigma_B \\in \\mathbb{R}$ are fixed scalars. The coupling between the subsystems is given by\n$$\nu_{\\mathrm{cpl}}(x) = u(x) + \\lambda\\, v(x),\n\\qquad\nv_{\\mathrm{cpl}}(x) = v(x) + \\mu\\, u(x),\n$$\nwith coupling parameters $\\lambda, \\mu \\in \\mathbb{R}$. The scalar quantity of interest is\n$$\nJ(x) = \\alpha\\, u_{\\mathrm{cpl}}(x)^2 + \\beta\\, v_{\\mathrm{cpl}}(x) + \\gamma\\, u_{\\mathrm{cpl}}(x)\\, v_{\\mathrm{cpl}}(x),\n$$\nfor fixed coefficients $\\alpha, \\beta, \\gamma \\in \\mathbb{R}$. All trigonometric functions in this problem interpret their arguments in radians. The Active Subspace (AS) dimension is defined using the dominant eigenspace of a second-moment matrix of gradients under the input measure. In this problem, you must derive and implement a Monte Carlo estimator to determine the AS dimension from first principles, using only the core definitions of expectation, gradient, and eigendecomposition.\n\nUse the following fixed constants:\n- $a_A = \\big[3,\\,3,\\,0.1,\\,0,\\,0,\\,0\\big]^\\top$,\n- $a_B = \\big[0,\\,0,\\,4,\\,0,\\,0,\\,0\\big]^\\top$,\n- $\\eta_A = \\big[1,\\,-1,\\,0,\\,0,\\,0,\\,0\\big]^\\top$,\n- $\\eta_B = \\big[0,\\,0,\\,1,\\,-1,\\,0,\\,0\\big]^\\top$,\n- $\\sigma_A = 0.5$, $\\sigma_B = 0.3$,\n- $\\alpha = 1.0$, $\\beta = 0.5$, $\\gamma = 0.6$.\n\nStarting from the fundamental definitions of expectation and gradient, implement a Monte Carlo estimator of the second-moment matrix of gradients of $J(x)$ under the distribution of $x$. Then, compute its eigensystem and select the AS dimension $k$ as the smallest integer such that the cumulative sum of the largest $k$ eigenvalues reaches at least a specified energy fraction threshold $\\tau$ times the sum of all eigenvalues. Explicitly:\n- Draw $N$ samples of $x$ using a fixed random seed $12345$ to ensure reproducibility.\n- Estimate the gradient of $J(x)$ for each sample using the chain rule on the coupled model defined above.\n- Approximate the second-moment matrix of gradients by averaging the outer products of sample gradients.\n- Compute its eigendecomposition and order eigenvalues in nonincreasing order.\n- Determine the smallest $k$ such that the cumulative energy fraction meets or exceeds the specified threshold.\n\nYour program must implement the above for the following test suite of parameter values $\\big(N,\\tau,\\lambda,\\mu\\big)$, with $N \\in \\mathbb{N}$, $\\tau \\in (0,1)$, and $\\lambda,\\mu \\in \\mathbb{R}$:\n- Test $1$: $\\big(4000,\\,0.9,\\,0.8,\\,0.2\\big)$,\n- Test $2$: $\\big(4000,\\,0.99,\\,0.8,\\,0.2\\big)$,\n- Test $3$: $\\big(500,\\,0.9,\\,0.8,\\,0.2\\big)$,\n- Test $4$: $\\big(4000,\\,0.9,\\,1.5,\\,0.0\\big)$.\n\nAll numerical computations must be performed in dimension $6$ and all trigonometric function arguments are in radians. There are no physical units in this problem. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, i.e., an array of integers $[k_1,k_2,k_3,k_4]$, where $k_i$ is the selected AS dimension for Test $i$.",
            "solution": "The problem has been validated and is confirmed to be scientifically and mathematically sound. It is well-posed, objective, self-contained, and algorithmically specified, which permits a direct path to a solution.\n\nThe central task is to determine the Active Subspace (AS) dimension for a given quantity of interest (QoI), $J(x)$, which is a function of a $6$-dimensional standard normal random vector $x$. The AS dimension is determined by analyzing the eigenspectrum of the second-moment matrix of the gradient of the QoI.\n\nThe matrix of interest is defined as:\n$$\nC = \\mathbb{E}\\left[ \\nabla_x J(x) \\left(\\nabla_x J(x)\\right)^\\top \\right]\n$$\nwhere $\\mathbb{E}[\\cdot]$ denotes the expectation with respect to the probability distribution of $x \\sim \\mathcal{N}(0, I_6)$. The matrix $C$ is a $6 \\times 6$ symmetric positive semidefinite matrix. The AS methodology focuses on the dominant eigenspace of $C$.\n\nThe problem requires a Monte Carlo estimation of this matrix. Given $N$ independent samples $\\{x_i\\}_{i=1}^N$ drawn from $\\mathcal{N}(0, I_6)$, the matrix $C$ is approximated by the sample average:\n$$\n\\hat{C} = \\frac{1}{N} \\sum_{i=1}^N \\nabla_x J(x_i) \\left(\\nabla_x J(x_i)\\right)^\\top\n$$\nTo compute $\\hat{C}$, we must first derive an analytical expression for the gradient vector $\\nabla_x J(x)$.\n\nThe QoI is given by:\n$$\nJ(x) = \\alpha\\, u_{\\mathrm{cpl}}(x)^2 + \\beta\\, v_{\\mathrm{cpl}}(x) + \\gamma\\, u_{\\mathrm{cpl}}(x)\\, v_{\\mathrm{cpl}}(x)\n$$\nUsing the chain rule, its gradient with respect to the vector $x$ is:\n$$\n\\nabla_x J(x) = \\frac{\\partial J}{\\partial u_{\\mathrm{cpl}}} \\nabla_x u_{\\mathrm{cpl}}(x) + \\frac{\\partial J}{\\partial v_{\\mathrm{cpl}}} \\nabla_x v_{\\mathrm{cpl}}(x)\n$$\nThe partial derivatives of $J$ are:\n$$\n\\frac{\\partial J}{\\partial u_{\\mathrm{cpl}}} = 2\\alpha\\, u_{\\mathrm{cpl}}(x) + \\gamma\\, v_{\\mathrm{cpl}}(x)\n$$\n$$\n\\frac{\\partial J}{\\partial v_{\\mathrm{cpl}}} = \\beta + \\gamma\\, u_{\\mathrm{cpl}}(x)\n$$\nNext, we find the gradients of the coupled response functions $u_{\\mathrm{cpl}}(x)$ and $v_{\\mathrm{cpl}}(x)$. These are defined as:\n$$\nu_{\\mathrm{cpl}}(x) = u(x) + \\lambda\\, v(x)\n$$\n$$\nv_{\\mathrm{cpl}}(x) = v(x) + \\mu\\, u(x)\n$$\nTheir gradients are:\n$$\n\\nabla_x u_{\\mathrm{cpl}}(x) = \\nabla_x u(x) + \\lambda\\, \\nabla_x v(x)\n$$\n$$\n\\nabla_x v_{\\mathrm{cpl}}(x) = \\nabla_x v(x) + \\mu\\, \\nabla_x u(x)\n$$\nTo complete the derivation, we need the gradients of the uncoupled subsystem functions, $u(x)$ and $v(x)$:\n$$\nu(x) = a_A^\\top x + \\sigma_A \\tanh(\\eta_A^\\top x)\n$$\n$$\nv(x) = a_B^\\top x + \\sigma_B \\sin(\\eta_B^\\top x)\n$$\nUsing the chain rule and the derivatives $\\frac{d}{dz}\\tanh(z) = \\mathrm{sech}^2(z) = 1 - \\tanh^2(z)$ and $\\frac{d}{dz}\\sin(z) = \\cos(z)$, we obtain their gradients:\n$$\n\\nabla_x u(x) = a_A + \\sigma_A \\cdot \\left(1 - \\tanh^2(\\eta_A^\\top x)\\right) \\cdot \\eta_A\n$$\n$$\n\\nabla_x v(x) = a_B + \\sigma_B \\cdot \\cos(\\eta_B^\\top x) \\cdot \\eta_B\n$$\nBy substituting these expressions back into the equation for $\\nabla_x J(x)$, we can construct a function to evaluate the gradient for any given sample $x$. For computational efficiency, we can rearrange the expression for $\\nabla_x J(x)$ by grouping terms with $\\nabla_x u(x)$ and $\\nabla_x v(x)$:\n$$\n\\nabla_x J(x) = \\left( (2\\alpha u_{\\mathrm{cpl}} + \\gamma v_{\\mathrm{cpl}}) + \\mu (\\beta + \\gamma u_{\\mathrm{cpl}}) \\right) \\nabla_x u(x) + \\left( \\lambda (2\\alpha u_{\\mathrm{cpl}} + \\gamma v_{\\mathrm{cpl}}) + (\\beta + \\gamma u_{\\mathrm{cpl}}) \\right) \\nabla_x v(x)\n$$\nLet's define the scalar coefficients for a given $x$ as:\n$$\ns_1(x) = 2\\alpha u_{\\mathrm{cpl}}(x) + \\gamma v_{\\mathrm{cpl}}(x)\n$$\n$$\ns_2(x) = \\beta + \\gamma u_{\\mathrm{cpl}}(x)\n$$\nThen the gradient of the QoI can be written more compactly as:\n$$\n\\nabla_x J(x) = (s_1(x) + \\mu s_2(x)) \\nabla_x u(x) + (\\lambda s_1(x) + s_2(x)) \\nabla_x v(x)\n$$\nThe overall algorithm is as follows:\n1.  Set the random number generator seed to $12345$ for reproducibility.\n2.  For each test case with parameters $(N, \\tau, \\lambda, \\mu)$:\n    a. Generate $N$ samples $x_1, \\dots, x_N$ from the standard normal distribution in $\\mathbb{R}^6$.\n    b. Initialize a $6 \\times 6$ matrix $\\hat{C}$ to all zeros.\n    c. For each sample $x_i$:\n        i. Evaluate the scalar arguments $\\eta_A^\\top x_i$ and $\\eta_B^\\top x_i$.\n        ii. Evaluate the uncoupled responses $u(x_i)$ and $v(x_i)$.\n        iii. Evaluate the coupled responses $u_{\\mathrm{cpl}}(x_i)$ and $v_{\\mathrm{cpl}}(x_i)$.\n        iv. Evaluate the gradients $\\nabla_x u(x_i)$ and $\\nabla_x v(x_i)$.\n        v. Evaluate the final gradient $\\nabla_x J(x_i)$ using the derived expression. Let this be $g_i$.\n        vi. Compute the outer product $g_i g_i^\\top$ and add it to a running sum.\n    d. Compute the final estimate $\\hat{C} = \\frac{1}{N} \\sum_{i=1}^N g_i g_i^\\top$.\n    e. Compute the eigendecomposition of the symmetric matrix $\\hat{C}$ to obtain its eigenvalues $\\lambda_1, \\dots, \\lambda_6$.\n    f. Sort the eigenvalues in nonincreasing order: $\\lambda_{(1)} \\ge \\lambda_{(2)} \\ge \\dots \\ge \\lambda_{(6)}$.\n    g. Calculate the total spectral energy $S = \\sum_{j=1}^6 \\lambda_{(j)}$.\n    h. Find the smallest integer $k$ such that the cumulative energy fraction meets or exceeds the threshold $\\tau$:\n    $$\n    k = \\min \\left\\{ m \\in \\{1, \\dots, 6\\} \\, \\Bigg| \\, \\frac{\\sum_{j=1}^m \\lambda_{(j)}}{S} \\ge \\tau \\right\\}\n    $$\n    This value of $k$ is the AS dimension for the test case.\n\nThe final implementation will perform this procedure for each of the four specified test cases and report the resulting AS dimensions.\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Active Subspace dimension for a coupled multiphysics surrogate model\n    using a Monte Carlo estimator for the second-moment matrix of gradients.\n    \"\"\"\n    # Fixed constants from the problem statement\n    a_A = np.array([3.0, 3.0, 0.1, 0.0, 0.0, 0.0])\n    a_B = np.array([0.0, 0.0, 4.0, 0.0, 0.0, 0.0])\n    eta_A = np.array([1.0, -1.0, 0.0, 0.0, 0.0, 0.0])\n    eta_B = np.array([0.0, 0.0, 1.0, -1.0, 0.0, 0.0])\n    sigma_A = 0.5\n    sigma_B = 0.3\n    alpha = 1.0\n    beta = 0.5\n    gamma = 0.6\n    \n    # Test suite of parameter values (N, tau, lambda, mu)\n    test_cases = [\n        (4000, 0.9, 0.8, 0.2),\n        (4000, 0.99, 0.8, 0.2),\n        (500, 0.9, 0.8, 0.2),\n        (4000, 0.9, 1.5, 0.0),\n    ]\n\n    results = []\n    \n    # Initialize random number generator with the fixed seed\n    rng = np.random.default_rng(12345)\n    \n    for N, tau, lamb, mu in test_cases:\n        # Generate N samples from the 6D standard normal distribution\n        x_samples = rng.standard_normal(size=(N, 6))\n        \n        # Initialize the second-moment matrix estimator\n        C_hat = np.zeros((6, 6))\n        \n        for i in range(N):\n            x = x_samples[i, :]\n            \n            # --- Evaluate functions and their gradients ---\n            \n            # Scalar arguments for nonlinear functions\n            eta_A_dot_x = np.dot(eta_A, x)\n            eta_B_dot_x = np.dot(eta_B, x)\n            \n            # Compute tanh and sin/cos values\n            tanh_val = np.tanh(eta_A_dot_x)\n            sin_val = np.sin(eta_B_dot_x)\n            cos_val = np.cos(eta_B_dot_x)\n            \n            # Uncoupled responses u(x) and v(x)\n            u_x = np.dot(a_A, x) + sigma_A * tanh_val\n            v_x = np.dot(a_B, x) + sigma_B * sin_val\n            \n            # Coupled responses u_cpl(x) and v_cpl(x)\n            u_cpl = u_x + lamb * v_x\n            v_cpl = v_x + mu * u_x\n            \n            # Gradients of uncoupled responses\n            grad_u = a_A + sigma_A * (1.0 - tanh_val**2) * eta_A\n            grad_v = a_B + sigma_B * cos_val * eta_B\n            \n            # Scalar prefactors for the final gradient calculation\n            s1 = 2.0 * alpha * u_cpl + gamma * v_cpl\n            s2 = beta + gamma * u_cpl\n            \n            # Gradient of the quantity of interest J(x)\n            grad_J = (s1 + mu * s2) * grad_u + (lamb * s1 + s2) * grad_v\n            \n            # Accumulate outer product of gradients\n            C_hat += np.outer(grad_J, grad_J)\n            \n        # Finalize Monte Carlo estimate\n        C_hat /= N\n        \n        # --- Eigendecomposition and AS dimension ---\n        \n        # Compute eigenvalues of the symmetric matrix C_hat\n        eigenvalues = np.linalg.eigh(C_hat)[0]\n        \n        # Sort eigenvalues in nonincreasing order\n        eigenvalues = np.sort(eigenvalues)[::-1]\n        \n        # Calculate total energy (sum of eigenvalues)\n        total_energy = np.sum(eigenvalues)\n        \n        if total_energy = 0.0:\n            # In case of zero energy (e.g., constant gradient), all directions are equivalent.\n            # The problem context ensures this will not happen with the given parameters.\n            # A 1D active subspace is a reasonable default.\n            k = 1\n        else:\n            # Calculate cumulative energy fractions\n            cumulative_energy = np.cumsum(eigenvalues)\n            \n            # Find the smallest k such that the cumulative energy fraction >= tau\n            # np.searchsorted finds the first index where the condition is met.\n            # Add 1 to convert 0-based index to 1-based dimension count.\n            k = np.searchsorted(cumulative_energy, tau * total_energy, side='left') + 1\n        \n        results.append(k)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\n# To generate the answer for the XML file, one would run the solve() function.\n# The expected output is printed to the console.\n# solve()\n```",
            "answer": "[2,3,2,1]"
        }
    ]
}