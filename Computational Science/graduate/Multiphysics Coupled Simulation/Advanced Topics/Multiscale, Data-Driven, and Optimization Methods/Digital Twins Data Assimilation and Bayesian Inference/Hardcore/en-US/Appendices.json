{
    "hands_on_practices": [
        {
            "introduction": "A key challenge in data assimilation is ensuring the statistical consistency of the model forecast. This practice introduces the rank histogram, a critical diagnostic tool to assess whether an ensemble data assimilation system is well-calibrated, meaning its predicted uncertainty matches the observed errors. By implementing the test from scratch, you will gain a fundamental understanding of how to detect common issues like filter bias and incorrect ensemble spread, which are crucial skills for validating and tuning any digital twin that relies on ensemble methods .",
            "id": "3502567",
            "problem": "You are asked to build a self-contained program that constructs innovation-based rank histograms for the Ensemble Kalman Filter (EnKF) and evaluates their uniformity under different assumptions about model and noise specification. The context is a scalar, linear, observation model within a multiphysics digital twin where ensemble forecasting and Bayesian inference are used for data assimilation. The derivation and algorithm must start only from fundamental probability principles, the linear Gaussian model definition, and the definition of exchangeability of independent and identically distributed variables.\n\nConsider the following setting. The true state $x$ is a scalar random variable with a Gaussian prior $x \\sim \\mathcal{N}(\\mu_{\\text{true}}, P_{\\text{true}})$. The observation operator is the identity, so the observed quantity is $y = x + \\varepsilon$, with observation noise $\\varepsilon \\sim \\mathcal{N}(0, R_{\\text{true}})$, independent of $x$. The Ensemble Kalman Filter (EnKF) produces a forecast ensemble $\\{x_f^{(i)}\\}_{i=1}^m$, where $m$ is the ensemble size. The EnKF uses an assumed Gaussian forecast distribution $x_f^{(i)} \\sim \\mathcal{N}(\\mu_{\\text{assumed}}, P_{\\text{assumed}})$ and an assumed observation noise for perturbed observations $e^{(i)} \\sim \\mathcal{N}(0, R_{\\text{assumed}})$, all independent across $i$ and independent of $x$ and $\\varepsilon$. The ensemble predictive observations are $y_f^{(i)} = x_f^{(i)} + e^{(i)}$.\n\nDefine the innovation-based predictive rank as follows. For each assimilation cycle, draw one realization of $y$, draw an ensemble $\\{y_f^{(i)}\\}_{i=1}^m$, and compute the rank $r \\in \\{0,1,\\dots,m\\}$ of $y$ among the $m$ i.i.d. values $\\{y_f^{(i)}\\}_{i=1}^m$, that is, $r$ is the number of ensemble predictive observations strictly less than $y$. Under perfect model and noise tuning, the variables $\\{y, y_f^{(1)},\\dots,y_f^{(m)}\\}$ are independent and identically distributed, and the rank $r$ is expected to be uniformly distributed over $\\{0,\\dots,m\\}$.\n\nYour tasks are:\n- Construct the rank histogram by repeating the above experiment for $N$ independent assimilation cycles and tallying the frequency of each rank $r \\in \\{0,\\dots,m\\}$.\n- Quantitatively test the uniformity of the histogram using the chi-squared goodness-of-fit test with significance level $\\alpha$, where the null hypothesis is that the histogram is uniform with each bin probability equal to $1/(m+1)$. Use the chi-squared statistic and compute the $p$-value against the reference distribution with $m$ degrees of freedom.\n- Output, for each test case in the test suite, a boolean indicating whether the null hypothesis of uniformity is accepted at level $\\alpha$.\n\nYou must solve this problem from first principles. The derivations and algorithmic logic should begin from fundamental probability laws, the Gaussian model and the properties of order statistics and exchangeability, not from any pre-specified rank histogram formulas beyond these fundamentals.\n\nThere are no physical units in this problem. Angles are not used. All probabilities must be expressed as real numbers in $[0,1]$.\n\nTest suite to implement:\n- Case A (perfect tuning, large ensemble and sample size):\n  - $m = 20$, $N = 10000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.0$, $P_{\\text{assumed}} = 1.0$, $R_{\\text{assumed}} = 1.0$\n- Case B (underdispersed predictive ensemble and noise):\n  - $m = 20$, $N = 10000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.0$, $P_{\\text{assumed}} = 0.25$, $R_{\\text{assumed}} = 0.25$\n- Case C (overdispersed predictive ensemble and noise):\n  - $m = 20$, $N = 10000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.0$, $P_{\\text{assumed}} = 4.0$, $R_{\\text{assumed}} = 4.0$\n- Case D (biased forecast mean):\n  - $m = 20$, $N = 10000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.5$, $P_{\\text{assumed}} = 1.0$, $R_{\\text{assumed}} = 1.0$\n- Case E (boundary case with small ensemble):\n  - $m = 3$, $N = 2000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.0$, $P_{\\text{assumed}} = 1.0$, $R_{\\text{assumed}} = 1.0$\n\nImplementation requirements:\n- For each case, generate $N$ independent draws of $y$ and an $m$-member ensemble of $\\{y_f^{(i)}\\}_{i=1}^m$ per draw, compute the rank $r \\in \\{0,\\dots,m\\}$, and accumulate the histogram counts.\n- Perform a chi-squared goodness-of-fit test with expected frequency $N/(m+1)$ in each of the $m+1$ bins. Use the resulting $p$-value to decide acceptance or rejection of uniformity at level $\\alpha$.\n- Use a fixed random seed so that the results are reproducible.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases A, B, C, D, E, for example, $[b_A,b_B,b_C,b_D,b_E]$ where each $b_{\\cdot}$ is either the string $\\texttt{True}$ or $\\texttt{False}$ indicating whether uniformity at level $\\alpha$ is accepted for that case.\n\nYour program must be complete and runnable as is, without any user input or external files.",
            "solution": "The problem requires the construction and statistical validation of innovation-based rank histograms for an Ensemble Kalman Filter (EnKF) under various model and noise specification scenarios. The derivation and implementation will proceed from first principles of probability theory.\n\nFirst, we define the probability distributions governing the problem. The \"true\" state $x$ is a scalar random variable drawn from a Gaussian distribution, $x \\sim \\mathcal{N}(\\mu_{\\text{true}}, P_{\\text{true}})$. An observation $y$ is obtained through an identity observation operator with additive Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, R_{\\text{true}})$, independent of $x$. Thus, $y = x + \\varepsilon$. As the sum of two independent Gaussian random variables, $y$ is also a Gaussian random variable. Its distribution, which we can call the \"true predictive distribution\" of the observation, is derived as follows:\nThe mean is $\\mathbb{E}[y] = \\mathbb{E}[x + \\varepsilon] = \\mathbb{E}[x] + \\mathbb{E}[\\varepsilon] = \\mu_{\\text{true}} + 0 = \\mu_{\\text{true}}$.\nThe variance is $\\text{Var}(y) = \\text{Var}(x + \\varepsilon) = \\text{Var}(x) + \\text{Var}(\\varepsilon) = P_{\\text{true}} + R_{\\text{true}}$, due to the independence of $x$ and $\\varepsilon$.\nTherefore, any single true observation $y$ is a sample from the distribution $y \\sim \\mathcal{N}(\\mu_{\\text{true}}, P_{\\text{true}} + R_{\\text{true}})$.\n\nNext, we define the distribution of the ensemble-based predictive observations. The EnKF uses an assumed forecast distribution, from which an ensemble of states $\\{x_f^{(i)}\\}_{i=1}^m$ is drawn, with each member $x_f^{(i)} \\sim \\mathcal{N}(\\mu_{\\text{assumed}}, P_{\\text{assumed}})$. The observations are assumed to be corrupted by noise from a distribution $\\mathcal{N}(0, R_{\\text{assumed}})$. To construct the predictive observation ensemble, each forecast state $x_f^{(i)}$ is perturbed by a random draw $e^{(i)}$ from this assumed noise distribution, $e^{(i)} \\sim \\mathcal{N}(0, R_{\\text{assumed}})$. The resulting ensemble predictive observations are $y_f^{(i)} = x_f^{(i)} + e^{(i)}$.\nSimilar to the true observation, each $y_f^{(i)}$ is a sum of two independent Gaussian variables. Its distribution is derived as:\nThe mean is $\\mathbb{E}[y_f^{(i)}] = \\mathbb{E}[x_f^{(i)} + e^{(i)}] = \\mathbb{E}[x_f^{(i)}] + \\mathbb{E}[e^{(i)}] = \\mu_{\\text{assumed}} + 0 = \\mu_{\\text{assumed}}$.\nThe variance is $\\text{Var}(y_f^{(i)}) = \\text{Var}(x_f^{(i)} + e^{(i)}) = \\text{Var}(x_f^{(i)}) + \\text{Var}(e^{(i)}) = P_{\\text{assumed}} + R_{\\text{assumed}}$.\nThus, each ensemble predictive observation $y_f^{(i)}$ for $i \\in \\{1, \\dots, m\\}$ is an independent and identically distributed (i.i.d.) sample from the distribution $y_f^{(i)} \\sim \\mathcal{N}(\\mu_{\\text{assumed}}, P_{\\text{assumed}} + R_{\\text{assumed}})$.\n\nThe fundamental principle behind rank histograms is based on the concept of exchangeability. A set of random variables is exchangeable if its joint probability distribution is invariant under any permutation of the variables. A set of i.i.d. random variables is always exchangeable.\nConsider the set of $m+1$ random variables $\\{y, y_f^{(1)}, \\dots, y_f^{(m)}\\}$. If the filter is perfectly tuned, the \"true predictive distribution\" of the observation must be identical to the \"ensemble predictive distribution\". This condition requires:\n1. Equality of means: $\\mu_{\\text{true}} = \\mu_{\\text{assumed}}$.\n2. Equality of variances: $P_{\\text{true}} + R_{\\text{true}} = P_{\\text{assumed}} + R_{\\text{assumed}}$.\n\nWhen these conditions are met, all $m+1$ variables are i.i.d. and thus exchangeable. For any such set of i.i.d. continuous random variables, if we sort them, any specific variable (in this case, $y$) has an equal probability of appearing at any of the $m+1$ possible positions in the sorted list.\nThe rank $r$ is defined as the number of ensemble members $y_f^{(i)}$ that are strictly less than the observation $y$. This is equivalent to stating that $y$ occupies the $(r+1)$-th position in the sorted list of the combined set $\\{y, y_f^{(1)}, \\dots, y_f^{(m)}\\}$. Consequently, under the perfect model hypothesis, the rank $r$ must be uniformly distributed over the integers $\\{0, 1, \\dots, m\\}$. The probability of any specific rank $k$ is $P(r=k) = \\frac{1}{m+1}$ for $k \\in \\{0, 1, \\dots, m\\}$.\n\nThe algorithm to construct and test the rank histogram is as follows:\n1. Initialize an integer array of counts, `rank_counts`, of size $m+1$ to zeros.\n2. For each of $N$ independent assimilation cycles:\n    a. Draw a single true observation $y$ from $\\mathcal{N}(\\mu_{\\text{true}}, P_{\\text{true}} + R_{\\text{true}})$.\n    b. Draw an ensemble of $m$ predictive observations $\\{y_f^{(i)}\\}_{i=1}^m$ from $\\mathcal{N}(\\mu_{\\text{assumed}}, P_{\\text{assumed}} + R_{\\text{assumed}})$.\n    c. Compute the rank $r = \\sum_{i=1}^{m} \\mathbb{I}(y_f^{(i)} < y)$, where $\\mathbb{I}(\\cdot)$ is the indicator function, which is $1$ if the condition is true and $0$ otherwise. The rank $r$ will be an integer between $0$ and $m$.\n    d. Increment the corresponding counter: `rank_counts[r] = rank_counts[r] + 1`.\n3. After $N$ cycles, the array `rank_counts` contains the observed frequencies $\\{O_k\\}_{k=0}^m$ for each rank.\n\nTo test for uniformity, we employ the chi-squared ($\\chi^2$) goodness-of-fit test.\nThe null hypothesis, $H_0$, is that the ranks are uniformly distributed.\nThe expected frequency for each rank bin $k$ under $H_0$ is $E_k = N \\times P(r=k) = \\frac{N}{m+1}$.\nThe $\\chi^2$ test statistic is calculated as:\n$$ \\chi^2 = \\sum_{k=0}^{m} \\frac{(O_k - E_k)^2}{E_k} $$\nThis statistic is compared against the chi-squared distribution. The number of degrees of freedom ($df$) is the number of bins minus one, as the distribution is fully specified by the null hypothesis. Thus, $df = (m+1) - 1 = m$.\nThe $p$-value is the probability of obtaining a test statistic at least as extreme as the one observed, assuming $H_0$ is true. It is calculated as $p = P(\\chi^2_{df=m} \\geq \\chi^2_{\\text{observed}}) = 1 - F_{\\chi^2_m}(\\chi^2_{\\text{observed}})$, where $F_{\\chi^2_m}$ is the cumulative distribution function (CDF) of the chi-squared distribution with $m$ degrees of freedom.\nThe decision rule is: if the calculated $p$-value is less than the specified significance level $\\alpha$, we reject the null hypothesis $H_0$. Otherwise, we fail to reject (i.e., accept) $H_0$. The test result is `True` if $p \\ge \\alpha$ and `False` if $p < \\alpha$.\n\nThis entire procedure is applied to each test case specified in the problem statement. The fixed random seed ensures reproducibility of the Monte Carlo simulation.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Constructs and validates innovation-based rank histograms for an EnKF.\n    \"\"\"\n    # A fixed random seed is used for reproducibility.\n    rng = np.random.default_rng(42)\n\n    # Test suite parameters for Cases A through E.\n    test_cases = [\n        {\n            \"name\": \"Case A\", \"m\": 20, \"N\": 10000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.0, \"P_assumed\": 1.0, \"R_assumed\": 1.0\n        },\n        {\n            \"name\": \"Case B\", \"m\": 20, \"N\": 10000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.0, \"P_assumed\": 0.25, \"R_assumed\": 0.25\n        },\n        {\n            \"name\": \"Case C\", \"m\": 20, \"N\": 10000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.0, \"P_assumed\": 4.0, \"R_assumed\": 4.0\n        },\n        {\n            \"name\": \"Case D\", \"m\": 20, \"N\": 10000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.5, \"P_assumed\": 1.0, \"R_assumed\": 1.0\n        },\n        {\n            \"name\": \"Case E\", \"m\": 3, \"N\": 2000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.0, \"P_assumed\": 1.0, \"R_assumed\": 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        m = case[\"m\"]\n        N = case[\"N\"]\n        alpha = case[\"alpha\"]\n\n        # Parameters for the \"true\" predictive observation distribution\n        mu_y_true = case[\"mu_true\"]\n        var_y_true = case[\"P_true\"] + case[\"R_true\"]\n        std_y_true = np.sqrt(var_y_true)\n\n        # Parameters for the \"ensemble\" predictive observation distribution\n        mu_y_assumed = case[\"mu_assumed\"]\n        var_y_assumed = case[\"P_assumed\"] + case[\"R_assumed\"]\n        std_y_assumed = np.sqrt(var_y_assumed)\n\n        # Initialize the histogram for ranks {0, 1, ..., m}\n        # rank_counts has m+1 bins\n        rank_counts = np.zeros(m + 1, dtype=int)\n\n        # Run N independent assimilation cycles\n        for _ in range(N):\n            # 1. Generate one \"true\" observation y\n            y_true = rng.normal(loc=mu_y_true, scale=std_y_true)\n\n            # 2. Generate an m-member ensemble of predictive observations {y_f}\n            y_f_ensemble = rng.normal(loc=mu_y_assumed, scale=std_y_assumed, size=m)\n\n            # 3. Compute the rank of y_true among the ensemble\n            # The rank is the number of ensemble members strictly less than y_true.\n            rank = np.sum(y_f_ensemble < y_true)\n            \n            # 4. Increment the count for the computed rank\n            rank_counts[rank] += 1\n        \n        # Perform the Chi-squared goodness-of-fit test for uniformity\n        \n        # The null hypothesis H0 is that the ranks are uniformly distributed.\n        # The expected count in each of the m+1 bins is N / (m+1).\n        expected_count = N / (m + 1)\n        \n        # Calculate the chi-squared statistic\n        # chi2_stat = sum_{k=0 to m} ( (observed_k - expected_k)^2 / expected_k )\n        chi2_stat = np.sum((rank_counts - expected_count)**2 / expected_count)\n\n        # The degrees of freedom is the number of bins minus 1.\n        # df = (m + 1) - 1 = m\n        df = m\n        \n        # Calculate the p-value.\n        # p_value = P(X^2_df >= chi2_stat), where X^2_df is a chi-squared random variable.\n        # This is 1 - CDF(chi2_stat).\n        p_value = 1.0 - chi2.cdf(chi2_stat, df)\n        \n        # Decision: Accept the null hypothesis (uniformity) if p_value >= alpha.\n        is_uniform = p_value >= alpha\n        results.append(is_uniform)\n\n    # Format the final output as a comma-separated list of booleans\n    # The output format must be exactly \"[True,False,False,False,True]\" (example)\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world sensor data is often contaminated with outliers that can corrupt standard data assimilation methods built on Gaussian assumptions. This exercise provides hands-on experience in building robust estimators using the Student-$t$ likelihood, a heavy-tailed distribution that automatically down-weights the influence of spurious measurements. You will implement an iteratively reweighted least squares (IRLS) algorithm, a powerful and widely used technique for solving the optimization problems that arise from such robust statistical models .",
            "id": "3502607",
            "problem": "Consider a simplified digital twin for thermo-fracture propagation in which the acoustic emission amplitude recorded by sensors is modeled as a linear image of a latent fracture activity produced by a multiphysics coupled simulator. Let the forward model for the sensor reading be $y_i = A s_i + \\varepsilon_i$, where $s_i$ is a known deterministic signal predicted by the digital twin and $A$ is an unknown scalar amplitude parameter to be inferred. Assume a Gaussian prior $A \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$. Two observation models are considered: (i) a Normal (Gaussian) likelihood with scale parameter $\\sigma$, and (ii) a Student-$t$ likelihood with degrees of freedom $\\nu$ and the same scale parameter $\\sigma$. The Student-$t$ likelihood is used to test robustness against heavy-tailed sensor noise. Use Bayes’ theorem and the definitions of the Normal and Student-$t$ distributions as the only starting points. Do not assume any closed-form posterior other than what can be derived from these definitions. Use the iteratively reweighted least squares principle derived from the scale-mixture representation of the Student-$t$ distribution or an equivalent first-principles argument to implement robust inference.\n\nThe task is to compute the Maximum a posteriori (MAP) estimator for $A$ under both likelihoods and test robustness to outliers by comparing the MAP errors against a known ground truth. For the Student-$t$ model, also compute the Laplace approximation curvature (the Hessian of the negative log-posterior at the MAP) and check its positivity, which is necessary for a valid local Gaussian approximation.\n\nAll quantities are dimensionless, and all outputs must be reported as dimensionless values.\n\nDefinitions for data and parameters are as follows. For all test cases, the prior parameters and the forward-model signal are fixed:\n- Prior mean $\\mu_0 = 0.0$ and prior variance $\\tau_0^2 = 10.0$.\n- Noise scale $\\sigma = 0.1$.\n- Ground-truth amplitude $A_{\\text{true}} = 2.0$.\n- Time index $i \\in \\{1,2,\\dots,25\\}$ and forward signal $s_i = \\exp(-0.05 i)\\left(1 + 0.5 \\sin(0.3 i)\\right)$.\n- Baseline deterministic pseudo-noise term $n_i = 0.05 \\sin(0.7 i)$.\n\nThe observed data $y_i$ for each test case is defined as $y_i = A_{\\text{true}} s_i + n_i + o_i$, where $o_i$ is an outlier term that differs per test. The Student-$t$ degrees-of-freedom parameter $\\nu$ also differs per test.\n\nTest suite:\n- Test case 1 (happy path): No outliers, so $o_i = 0$ for all $i$. Use $\\nu = 5$ for the Student-$t$ likelihood.\n- Test case 2 (heavy-tailed contamination): Outliers at indices $i \\in \\{5,12,20\\}$ with $o_5 = 1.5$, $o_{12} = -2.0$, $o_{20} = 2.5$, and $o_i = 0$ otherwise. Use $\\nu = 3$.\n- Test case 3 (extreme outlier): A single extreme outlier at index $i = 8$ with $o_8 = 6.0$ and $o_i = 0$ otherwise. Use $\\nu = 1$.\n\nFor each test case, do the following:\n1. Compute the MAP estimator $\\hat{A}_{t}$ under the Student-$t$ likelihood using only first principles and the scale-mixture or equivalent derivation to obtain an algorithm. Compute the curvature $H_t$ of the negative log-posterior at $\\hat{A}_t$ (the scalar Hessian in this $1$-dimensional parameter problem). Report a boolean indicating whether $H_t > 0$.\n2. Compute the MAP estimator $\\hat{A}_{g}$ under the Normal likelihood using only first principles.\n3. Compute the absolute errors $e_t = |\\hat{A}_t - A_{\\text{true}}|$ and $e_g = |\\hat{A}_g - A_{\\text{true}}|$.\n4. Report a boolean indicating whether the Student-$t$ MAP is strictly closer to the ground truth than the Normal MAP, that is, whether $e_t < e_g$.\n\nYour program must implement the computations in a fully deterministic way with no randomness. For each of the three test cases, the program should return a list of the form $[\\hat{A}_t,\\hat{A}_g,e_t,e_g,\\text{is\\_robust},\\text{hessian\\_positive}]$, where $\\text{is\\_robust}$ is the boolean for $e_t < e_g$ and $\\text{hessian\\_positive}$ is the boolean for $H_t > 0$.\n\nFinal output format: Your program should produce a single line of output containing a list with the three per-case lists, with no spaces, as in $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$. All numbers are dimensionless. Angles inside trigonometric functions are in radians. No physical units are required or permitted in the output.",
            "solution": "The problem requires the computation and comparison of Maximum a posteriori (MAP) estimators for a scalar parameter `$A$` in a linear model, under two different likelihood assumptions: Normal (Gaussian) and Student-$t$. The robustness of these estimators will be evaluated against outliers. All derivations will proceed from first principles.\n\nThe core of the problem lies in Bayesian inference. According to Bayes' theorem, the posterior probability distribution of the parameter `$A$` given the observed data `$\\mathbf{y} = \\{y_i\\}_{i=1}^{N}$` is proportional to the product of the likelihood and the prior:\n$$\np(A | \\mathbf{y}, \\mathbf{s}) \\propto p(\\mathbf{y} | A, \\mathbf{s}) p(A)\n$$\nwhere `$\\mathbf{s} = \\{s_i\\}_{i=1}^{N}$` is the known signal. The MAP estimate, `$\\hat{A}$`, is the value of `$A$` that maximizes this posterior probability. Maximizing the posterior is equivalent to minimizing its negative logarithm. We define the negative log-posterior as `$L(A) = - \\ln p(\\mathbf{y} | A, \\mathbf{s}) - \\ln p(A)$`, ignoring constant terms that do not depend on `$A$`.\n\nThe model components are defined as follows:\n- **Forward Model**: `$y_i = A s_i + \\varepsilon_i$`, where `$\\varepsilon_i$` is the noise term.\n- **Prior Distribution**: The prior belief about `$A$` is modeled as a Gaussian distribution, `$A \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$`. Its probability density function (PDF) is `$p(A) \\propto \\exp\\left(-\\frac{(A - \\mu_0)^2}{2\\tau_0^2}\\right)$`. The negative log-prior is `$\\ln p(A) = - \\frac{(A - \\mu_0)^2}{2\\tau_0^2} + \\text{const}$`.\n- **Likelihood Functions**: Assuming the observations `$y_i$` are conditionally independent given `$A$`, the total likelihood is the product of individual likelihoods, `$p(\\mathbf{y} | A, \\mathbf{s}) = \\prod_{i=1}^{N} p(y_i | A, s_i)$`. We consider two forms for `$p(y_i | A, s_i)$`:\n    1.  **Normal Likelihood**: `$p(y_i | A, s_i) = \\mathcal{N}(y_i | A s_i, \\sigma^2) \\propto \\exp\\left(-\\frac{(y_i - A s_i)^2}{2\\sigma^2}\\right)$`. This corresponds to assuming the noise `$\\varepsilon_i$` is Gaussian.\n    2.  **Student-$t$ Likelihood**: `$p(y_i | A, s_i) = \\text{St}(y_i | A s_i, \\sigma^2, \\nu) \\propto \\left(1 + \\frac{(y_i - A s_i)^2}{\\nu \\sigma^2}\\right)^{-\\frac{\\nu+1}{2}}$`. This corresponds to assuming the noise `$\\varepsilon_i$` follows a Student-$t$ distribution, which has heavier tails than a Gaussian and is more robust to outliers.\n\n### 1. MAP Estimation with Normal Likelihood\n\nFor the Normal likelihood, the negative log-posterior, `$L_g(A)$`, is:\n$$\nL_g(A) = - \\sum_{i=1}^{N} \\ln \\mathcal{N}(y_i | A s_i, \\sigma^2) - \\ln \\mathcal{N}(A | \\mu_0, \\tau_0^2) + C_g\n$$\n$$\nL_g(A) = \\sum_{i=1}^{N} \\frac{(y_i - A s_i)^2}{2\\sigma^2} + \\frac{(A - \\mu_0)^2}{2\\tau_0^2} + C'_g\n$$\nTo find the MAP estimate `$\\hat{A}_g$`, we differentiate `$L_g(A)$` with respect to `$A$` and set the result to zero:\n$$\n\\frac{d L_g}{d A} = \\sum_{i=1}^{N} \\frac{-s_i(y_i - A s_i)}{\\sigma^2} + \\frac{A - \\mu_0}{\\tau_0^2} = 0\n$$\nRearranging the terms to solve for `$A$`:\n$$\n\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} (A s_i^2 - s_i y_i) + \\frac{A - \\mu_0}{\\tau_0^2} = 0\n$$\n$$\nA \\left( \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} s_i^2 + \\frac{1}{\\tau_0^2} \\right) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} s_i y_i + \\frac{\\mu_0}{\\tau_0^2}\n$$\nThis yields a closed-form solution for the Gaussian MAP estimate, `$\\hat{A}_g$`:\n$$\n\\hat{A}_g = \\frac{\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} s_i y_i + \\frac{\\mu_0}{\\tau_0^2}}{\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} s_i^2 + \\frac{1}{\\tau_0^2}}\n$$\nThis expression represents a precision-weighted average of the maximum-likelihood estimate and the prior mean.\n\n### 2. MAP Estimation with Student-$t$ Likelihood (IRLS)\n\nFor the Student-$t$ likelihood, the negative log-posterior, `$L_t(A)$`, is:\n$$\nL_t(A) = - \\sum_{i=1}^{N} \\ln \\text{St}(y_i | A s_i, \\sigma^2, \\nu) - \\ln \\mathcal{N}(A | \\mu_0, \\tau_0^2) + C_t\n$$\n$$\nL_t(A) = \\sum_{i=1}^{N} \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{(y_i - A s_i)^2}{\\nu \\sigma^2}\\right) + \\frac{(A - \\mu_0)^2}{2\\tau_0^2} + C'_t\n$$\nDifferentiating with respect to `$A$` gives a non-linear equation:\n$$\n\\frac{d L_t}{d A} = \\sum_{i=1}^{N} \\frac{(\\nu+1) s_i (A s_i - y_i)}{\\nu \\sigma^2 + (y_i - A s_i)^2} + \\frac{A - \\mu_0}{\\tau_0^2} = 0\n$$\nNo closed-form solution for `$A$` exists. We derive an iterative algorithm based on the scale-mixture representation of the Student-$t$ distribution, as requested. A Student-$t$ variable can be expressed as a Gaussian whose variance is integrated over a Gamma distribution. Specifically, `$y_i \\sim \\text{St}(A s_i, \\sigma^2, \\nu)$` is equivalent to the hierarchical model:\n$$\ny_i | \\lambda_i \\sim \\mathcal{N}(A s_i, \\sigma^2/\\lambda_i) \\quad \\text{with} \\quad \\lambda_i \\sim \\text{Gamma}(\\nu/2, \\nu/2)\n$$\nHere, `$\\lambda_i$` are latent precision variables. The complete-data negative log-posterior for `$A$` and `$\\{\\lambda_i\\}$` is:\n$$\nL_t(A, \\{\\lambda_i\\}) = \\sum_{i=1}^{N} \\frac{\\lambda_i(y_i - A s_i)^2}{2\\sigma^2} + \\frac{(A - \\mu_0)^2}{2\\tau_0^2} - \\sum_{i=1}^{N} \\ln p(\\lambda_i) + \\text{const}\n$$\nThe Iteratively Reweighted Least Squares (IRLS) algorithm is an Expectation-Maximization (EM) variant that iteratively updates `$A$`.\n- **E-Step**: Compute the expectation of the latent variables `$\\lambda_i$` given the current estimate of `$A$`, denoted `$A^{(k)}$`. This expectation serves as the weight for each data point. The posterior for `$\\lambda_i$` is `$\\text{Gamma}(\\lambda_i | \\frac{\\nu+1}{2}, \\frac{\\nu\\sigma^2 + (y_i-A^{(k)}s_i)^2}{2\\sigma^2})$`. The expected value is:\n$$\nw_i^{(k+1)} = E[\\lambda_i | y_i, A^{(k)}] = \\frac{(\\nu+1)/2}{(\\nu\\sigma^2 + (y_i-A^{(k)}s_i)^2)/(2\\sigma^2)} = \\frac{\\nu+1}{\\nu + \\frac{(y_i - A^{(k)}s_i)^2}{\\sigma^2}}\n$$\n- **M-Step**: Update the estimate of `$A$` by minimizing the expected complete-data negative log-posterior, which is a weighted least squares problem:\n$$\nA^{(k+1)} = \\arg\\min_A \\left( \\sum_{i=1}^{N} \\frac{w_i^{(k+1)}(y_i - A s_i)^2}{2\\sigma^2} + \\frac{(A - \\mu_0)^2}{2\\tau_0^2} \\right)\n$$\nThe solution to this minimization is identical in form to the Gaussian case, but with weights `$w_i$` applied to each term in the likelihood sum:\n$$\nA^{(k+1)} = \\frac{\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} w_i^{(k+1)} s_i y_i + \\frac{\\mu_0}{\\tau_0^2}}{\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} w_i^{(k+1)} s_i^2 + \\frac{1}{\\tau_0^2}}\n$$\nStarting with an initial guess (e.g., `$A^{(0)} = \\hat{A}_g$`), this process is iterated until `$A^{(k)}$` converges to the Student-$t$ MAP estimate, `$\\hat{A}_t$`.\n\n### 3. Curvature of the Negative Log-Posterior\n\nThe curvature of the negative log-posterior at the MAP estimate is the second derivative, `$H_t = \\frac{d^2 L_t}{dA^2}\\Big|_{A=\\hat{A}_t}$`. This value is the inverse of the variance of the Laplace approximation, a Gaussian centered at `$\\hat{A}_t$`. Positivity (`$H_t > 0$`) is required for `$\\hat{A}_t$` to be a local minimum and for the Laplace approximation to be a valid Gaussian distribution (i.e., have a positive variance).\n\nDifferentiating `$\\frac{d L_t}{d A}$` with respect to `$A$` yields:\n$$\nH_t(A) = \\frac{d^2 L_t}{d A^2} = \\sum_{i=1}^{N} (\\nu+1)s_i^2 \\frac{\\nu\\sigma^2 - (y_i - A s_i)^2}{(\\nu\\sigma^2 + (y_i - A s_i)^2)^2} + \\frac{1}{\\tau_0^2}\n$$\nWe evaluate this expression at `$A = \\hat{A}_t$` to find the required curvature `$H_t$`.\n\n### 4. Computational Steps\nFor each test case:\n1.  The data vector `$\\mathbf{y}$` is constructed using the given parameters `$A_{\\text{true}}$`, `$s_i$`, `$n_i$`, and `$o_i$`.\n2.  `$\\hat{A}_g$` is computed directly using its closed-form analytical solution.\n3.  `$\\hat{A}_t$` is computed by executing the IRLS algorithm for a fixed number of iterations to ensure deterministic convergence.\n4.  The curvature `$H_t$` is calculated using its analytical formula evaluated at `$\\hat{A}_t$`. The condition `$H_t > 0$` is checked.\n5.  The absolute errors `$e_g = |\\hat{A}_g - A_{\\text{true}}|` and `$e_t = |\\hat{A}_t - A_{\\text{true}}|` are computed.\n6.  The robustness condition `$e_t < e_g$` is evaluated.\nThe results are then collected and formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes MAP estimators for a linear model with Gaussian and Student-t likelihoods\n    and assesses their robustness to outliers.\n    \"\"\"\n    # Fixed parameters\n    mu0 = 0.0\n    tau0_sq = 10.0\n    sigma = 0.1\n    sigma_sq = sigma**2\n    A_true = 2.0\n    N = 25\n    IRLS_ITERATIONS = 100\n\n    # Generate forward model signal and baseline noise\n    i_vals = np.arange(1, N + 1)\n    s = np.exp(-0.05 * i_vals) * (1.0 + 0.5 * np.sin(0.3 * i_vals))\n    n = 0.05 * np.sin(0.7 * i_vals)\n\n    # Define test cases\n    test_cases = [\n        # Case 1: No outliers, nu=5\n        {'nu': 5.0, 'outliers': {}},\n        # Case 2: Heavy-tailed contamination, nu=3\n        {'nu': 3.0, 'outliers': {5: 1.5, 12: -2.0, 20: 2.5}},\n        # Case 3: Extreme outlier, nu=1\n        {'nu': 1.0, 'outliers': {8: 6.0}},\n    ]\n\n    results = []\n\n    def compute_gaussian_map(y, s_vec, mu0_val, tau0_sq_val, sigma_sq_val):\n        \"\"\"Computes the MAP estimate for A under a Gaussian likelihood.\"\"\"\n        s_sq_sum = np.sum(s_vec**2)\n        sy_sum = np.sum(s_vec * y)\n        \n        inv_sigma_sq = 1.0 / sigma_sq_val\n        inv_tau0_sq = 1.0 / tau0_sq_val\n\n        numerator = inv_sigma_sq * sy_sum + inv_tau0_sq * mu0_val\n        denominator = inv_sigma_sq * s_sq_sum + inv_tau0_sq\n        \n        return numerator / denominator\n\n    def compute_student_t_map(y, s_vec, mu0_val, tau0_sq_val, sigma_sq_val, nu_val, A_init):\n        \"\"\"Computes the MAP estimate for A under a Student-t likelihood using IRLS.\"\"\"\n        A_k = A_init\n        inv_sigma_sq = 1.0 / sigma_sq_val\n        inv_tau0_sq = 1.0 / tau0_sq_val\n\n        for _ in range(IRLS_ITERATIONS):\n            residuals_sq = (y - A_k * s_vec)**2\n            weights = (nu_val + 1.0) / (nu_val + residuals_sq * inv_sigma_sq)\n            \n            w_s_sq_sum = np.sum(weights * s_vec**2)\n            w_sy_sum = np.sum(weights * s_vec * y)\n            \n            numerator = inv_sigma_sq * w_sy_sum + inv_tau0_sq * mu0_val\n            denominator = inv_sigma_sq * w_s_sq_sum + inv_tau0_sq\n            \n            A_k = numerator / denominator\n\n        A_hat_t = A_k\n\n        # Compute curvature (Hessian of negative log-posterior) at A_hat_t\n        residuals = y - A_hat_t * s_vec\n        \n        term1_num = nu_val * sigma_sq_val - residuals**2\n        term1_den = (nu_val * sigma_sq_val + residuals**2)**2\n        \n        hessian_sum = np.sum((nu_val + 1.0) * s_vec**2 * term1_num / term1_den)\n        H_t = hessian_sum + inv_tau0_sq\n        \n        return A_hat_t, H_t\n\n    for case in test_cases:\n        # Construct outlier vector and observed data y\n        o = np.zeros(N)\n        for idx, val in case['outliers'].items():\n            o[idx - 1] = val  # Convert 1-based index to 0-based\n        \n        y = A_true * s + n + o\n        nu = case['nu']\n\n        # 1. Compute MAP for Normal likelihood\n        A_hat_g = compute_gaussian_map(y, s, mu0, tau0_sq, sigma_sq)\n\n        # 2. Compute MAP and Hessian for Student-t likelihood\n        # Initialize IRLS with the Gaussian MAP estimate\n        A_hat_t, H_t = compute_student_t_map(y, s, mu0, tau0_sq, sigma_sq, nu, A_hat_g)\n\n        # 3. Compute errors\n        e_t = abs(A_hat_t - A_true)\n        e_g = abs(A_hat_g - A_true)\n        \n        # 4. Check robustness and Hessian positivity\n        is_robust = e_t < e_g\n        hessian_positive = H_t > 0.0\n        \n        # Collect results for this case\n        case_results = [A_hat_t, A_hat_g, e_t, e_g, is_robust, hessian_positive]\n        results.append(case_results)\n\n    # Format final output string as a list of lists with no spaces\n    # Convert bools to lowercase 'true'/'false' as per python str() default\n    result_str = str(results).replace(\" \", \"\")\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A truly adaptive digital twin must not only estimate its state but also learn and refine its underlying model parameters from data. This often involves an alternating schedule of state estimation and parameter training, creating a coupled dynamical system whose stability is not guaranteed. This advanced practice bridges Bayesian inference with numerical analysis, guiding you to formalize this schedule as an affine fixed-point iteration and analyze its convergence, a vital skill for designing stable and reliable joint estimation algorithms .",
            "id": "3502593",
            "problem": "A developer is tasked with designing a training–assimilation schedule for a digital twin in a multiphysics coupled simulation that alternates between parameter learning and state estimation, and analyzing convergence to a fixed point under contraction assumptions using Bayesian inference and data assimilation. Consider a linear-Gaussian model defined as follows. The state is $x \\in \\mathbb{R}^n$, the scalar parameter is $\\theta \\in \\mathbb{R}$, and the observation is $y \\in \\mathbb{R}^n$. The observation model is $y = C x + \\varepsilon$, with $C \\in \\mathbb{R}^{n \\times n}$ and Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, R)$ where $R \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite. The prior for the state given the parameter is $x \\mid \\theta \\sim \\mathcal{N}(B \\theta, P)$ with $B \\in \\mathbb{R}^{n \\times 1}$ and $P \\in \\mathbb{R}^{n \\times n}$ symmetric positive definite. The prior for the parameter is $\\theta \\sim \\mathcal{N}(\\theta_{\\text{prior}}, S)$ with $S \\in \\mathbb{R}_{>0}$.\n\nDefine the Maximum A Posteriori (MAP) state estimator as the minimizer $x_{\\text{MAP}}(\\theta)$ of the quadratic objective\n$$\nJ(x \\mid \\theta) = (y - C x)^\\top R^{-1} (y - C x) + (x - B \\theta)^\\top P^{-1} (x - B \\theta),\n$$\nand the MAP parameter estimator $\\theta_{\\text{MAP}}(x)$ as the minimizer of\n$$\nL(\\theta \\mid x) = (x - B \\theta)^\\top P^{-1} (x - B \\theta) + (\\theta - \\theta_{\\text{prior}})^\\top S^{-1} (\\theta - \\theta_{\\text{prior}}).\n$$\nConsider an alternating, under-relaxed training–assimilation schedule\n$$\nx_{k+1} = x_k + \\alpha_x \\left( x_{\\text{MAP}}(\\theta_k) - x_k \\right),\n\\quad\n\\theta_{k+1} = \\theta_k + \\alpha_\\theta \\left( \\theta_{\\text{MAP}}(x_{k+1}) - \\theta_k \\right),\n$$\nwith relaxation coefficients $ \\alpha_x \\in \\mathbb{R}$ and $ \\alpha_\\theta \\in \\mathbb{R}$. Show that this schedule can be written as a single affine iteration on the augmented variable $z_k \\in \\mathbb{R}^{n+1}$ defined by $z_k = [x_k; \\theta_k]$:\n$$\nz_{k+1} = A z_k + b,\n$$\nfor some matrix $A \\in \\mathbb{R}^{(n+1)\\times(n+1)}$ and vector $b \\in \\mathbb{R}^{n+1}$ that depend on $C, R, P, B, S, y, \\theta_{\\text{prior}}, \\alpha_x, \\alpha_\\theta$. Starting from the core definitions of Gaussian posteriors and the normal equations for linear least squares, derive explicit expressions for $A$ and $b$. Then, using the Banach fixed-point principle in finite dimensions and the Neumann series characterization, analyze convergence conditions in terms of the spectral radius of $A$. Specifically, prove that if the spectral radius $\\rho(A) < 1$, then there exists a unique fixed point $z^\\star$ satisfying\n$$\nz^\\star = A z^\\star + b,\n$$\nand the iteration converges to $z^\\star$ for any initial $z_0$. Derive the closed-form fixed point $z^\\star = (I - A)^{-1} b$ when $\\rho(A) < 1$.\n\nYour program must:\n- Construct $A$ and $b$ from the given model parameters.\n- Compute the spectral radius $\\rho(A)$.\n- Run the iteration $z_{k+1} = A z_k + b$ from the specified initial condition $z_0$ up to a fixed maximum number of iterations, stopping early if the iteration difference satisfies $\\|z_{k+1} - z_k\\|_2 < \\varepsilon$ for a tolerance $\\varepsilon$.\n- If $\\rho(A) < 1$, compute the fixed point $z^\\star = (I - A)^{-1} b$ and report the final error $e = \\|z_N - z^\\star\\|_2$, where $z_N$ is the last iterate.\n- If $\\rho(A) \\ge 1$, set the error $e$ to $+\\infty$.\n- Report a convergence indicator $c$ as a decimal number equal to $1.0$ if $ \\rho(A) < 1$ and the final error $e$ is less than the tolerance $\\varepsilon$, and $0.0$ otherwise.\n\nUse the following test suite with $n = 2$ in all cases, and the specified numeric values. All numbers provided below are to be implemented exactly as given:\n- Test case $1$ (happy path):\n  - $C = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$,\n  - $R = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$,\n  - $P = \\begin{bmatrix} 10 & 0 \\\\ 0 & 10 \\end{bmatrix}$,\n  - $B = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$,\n  - $S = 0.2$,\n  - $y = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$,\n  - $\\theta_{\\text{prior}} = 0.0$,\n  - $\\alpha_x = 0.6$,\n  - $\\alpha_\\theta = 0.4$,\n  - $z_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n- Test case $2$ (near-boundary contraction):\n  - $C = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$,\n  - $R = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$,\n  - $P = \\begin{bmatrix} 0.01 & 0 \\\\ 0 & 0.01 \\end{bmatrix}$,\n  - $B = \\begin{bmatrix} 10 \\\\ 10 \\end{bmatrix}$,\n  - $S = 1000000.0$,\n  - $y = \\begin{bmatrix} 2 \\\\ -2 \\end{bmatrix}$,\n  - $\\theta_{\\text{prior}} = 0.0$,\n  - $\\alpha_x = 1.0$,\n  - $\\alpha_\\theta = 1.0$,\n  - $z_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n- Test case $3$ (non-contraction via over-relaxation):\n  - Same as test case $2$, except $\\alpha_x = 1.2$ and $\\alpha_\\theta = 1.0$.\n\nAngle units do not apply. No physical units are involved, so answering in mathematical units is sufficient. The maximum number of iterations is $N_{\\max} = 200$ and the tolerance is $\\varepsilon = 10^{-8}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order and types:\n$$\n[\\rho_1, c_1, e_1, \\rho_2, c_2, e_2, \\rho_3, c_3, e_3],\n$$\nwhere $\\rho_i$ is the spectral radius for test case $i$ (a float), $c_i$ is the convergence indicator for test case $i$ (a float equal to either $1.0$ or $0.0$), and $e_i$ is the final error for test case $i$ (a float; use $+\\infty$ if $\\rho_i \\ge 1$). The output must be a single line in this exact format.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in the domain of numerical analysis, Bayesian inference and data assimilation, with all necessary data and definitions provided. We will proceed with a solution.\n\nThe objective is to derive an affine iteration $z_{k+1} = A z_k + b$ for the coupled training-assimilation schedule and then to implement a numerical analysis of its convergence. The augmented state is $z_k = [x_k^\\top, \\theta_k]^\\top \\in \\mathbb{R}^{n+1}$.\n\nFirst, we derive the explicit forms of the Maximum A Posteriori (MAP) estimators $x_{\\text{MAP}}(\\theta)$ and $\\theta_{\\text{MAP}}(x)$. These are found by minimizing their respective quadratic objective functions, which is achieved by setting their gradients to zero.\n\nThe objective for the state $x$ is:\n$$\nJ(x \\mid \\theta) = (y - C x)^\\top R^{-1} (y - C x) + (x - B \\theta)^\\top P^{-1} (x - B \\theta)\n$$\nThe gradient with respect to $x$ is:\n$$\n\\nabla_x J(x \\mid \\theta) = -2 C^\\top R^{-1} (y - C x) + 2 P^{-1} (x - B \\theta) = 0\n$$\n$$\nC^\\top R^{-1} C x - C^\\top R^{-1} y + P^{-1}x - P^{-1}B\\theta = 0\n$$\n$$\n(C^\\top R^{-1} C + P^{-1}) x = C^\\top R^{-1} y + P^{-1} B \\theta\n$$\nSince $R$ and $P$ are symmetric positive definite (SPD), so are $R^{-1}$ and $P^{-1}$. The matrix $H_x = C^\\top R^{-1} C + P^{-1}$ is the sum of a positive semi-definite matrix and a positive definite matrix, and is therefore positive definite and invertible. Solving for $x$ gives the MAP estimator:\n$$\nx_{\\text{MAP}}(\\theta) = (C^\\top R^{-1} C + P^{-1})^{-1} (C^\\top R^{-1} y + P^{-1} B \\theta)\n$$\nLet us define $M_x = (C^\\top R^{-1} C + P^{-1})^{-1}$. The expression for $x_{\\text{MAP}}(\\theta)$ is an affine function of $\\theta$:\n$$\nx_{\\text{MAP}}(\\theta) = (M_x P^{-1} B) \\theta + (M_x C^\\top R^{-1} y)\n$$\n\nNext, we find the MAP estimator for the scalar parameter $\\theta$. The objective function is:\n$$\nL(\\theta \\mid x) = (x - B \\theta)^\\top P^{-1} (x - B \\theta) + (\\theta - \\theta_{\\text{prior}})^\\top S^{-1} (\\theta - \\theta_{\\text{prior}})\n$$\nSince $\\theta$ is a scalar, this is $L(\\theta \\mid x) = (x - B \\theta)^\\top P^{-1} (x - B \\theta) + S^{-1} (\\theta - \\theta_{\\text{prior}})^2$.\nThe derivative with respect to $\\theta$ is:\n$$\n\\frac{dL}{d\\theta} = -2 B^\\top P^{-1} (x - B \\theta) + 2 S^{-1} (\\theta - \\theta_{\\text{prior}}) = 0\n$$\n$$\n-B^\\top P^{-1} x + (B^\\top P^{-1} B) \\theta + S^{-1} \\theta - S^{-1} \\theta_{\\text{prior}} = 0\n$$\n$$\n(B^\\top P^{-1} B + S^{-1}) \\theta = B^\\top P^{-1} x + S^{-1} \\theta_{\\text{prior}}\n$$\nThe scalar term $H_\\theta = B^\\top P^{-1} B + S^{-1}$ is strictly positive, since $P^{-1}$ is SPD and $S > 0$. Thus, it is invertible. Solving for $\\theta$ gives the MAP estimator:\n$$\n\\theta_{\\text{MAP}}(x) = (B^\\top P^{-1} B + S^{-1})^{-1} (B^\\top P^{-1} x + S^{-1} \\theta_{\\text{prior}})\n$$\nLet us define the scalar $M_\\theta = (B^\\top P^{-1} B + S^{-1})^{-1}$. The expression for $\\theta_{\\text{MAP}}(x)$ is an affine function of $x$:\n$$\n\\theta_{\\text{MAP}}(x) = (M_\\theta B^\\top P^{-1}) x + (M_\\theta S^{-1} \\theta_{\\text{prior}})\n$$\n\nNow, we substitute these estimators into the under-relaxed iteration schedule:\n1. $x_{k+1} = x_k + \\alpha_x ( x_{\\text{MAP}}(\\theta_k) - x_k ) = (1-\\alpha_x)x_k + \\alpha_x x_{\\text{MAP}}(\\theta_k)$\n2. $\\theta_{k+1} = \\theta_k + \\alpha_\\theta ( \\theta_{\\text{MAP}}(x_{k+1}) - \\theta_k ) = (1-\\alpha_\\theta)\\theta_k + \\alpha_\\theta \\theta_{\\text{MAP}}(x_{k+1})$\n\nSubstituting the derived expression for $x_{\\text{MAP}}(\\theta_k)$ into the first equation:\n$$\nx_{k+1} = (1-\\alpha_x)x_k + \\alpha_x \\left[ (M_x P^{-1} B) \\theta_k + (M_x C^\\top R^{-1} y) \\right]\n$$\n$$\nx_{k+1} = (1-\\alpha_x)I x_k + (\\alpha_x M_x P^{-1} B) \\theta_k + (\\alpha_x M_x C^\\top R^{-1} y)\n$$\nThis provides the top block of our affine system $z_{k+1}=Az_k+b$.\n\nNext, we substitute the expression for $x_{k+1}$ into the equation for $\\theta_{k+1}$:\n$$\n\\theta_{k+1} = (1-\\alpha_\\theta)\\theta_k + \\alpha_\\theta \\left[ (M_\\theta B^\\top P^{-1}) x_{k+1} + (M_\\theta S^{-1} \\theta_{\\text{prior}}) \\right]\n$$\n$$\n\\theta_{k+1} = (1-\\alpha_\\theta)\\theta_k + \\alpha_\\theta M_\\theta B^\\top P^{-1} \\left[ (1-\\alpha_x)I x_k + (\\alpha_x M_x P^{-1} B) \\theta_k + (\\alpha_x M_x C^\\top R^{-1} y) \\right] + \\alpha_\\theta M_\\theta S^{-1} \\theta_{\\text{prior}}\n$$\nWe group terms by $x_k$, $\\theta_k$, and constants:\n- Term with $x_k$: $(\\alpha_\\theta (1-\\alpha_x) M_\\theta B^\\top P^{-1}) x_k$\n- Term with $\\theta_k$: $\\left( (1-\\alpha_\\theta) + \\alpha_\\theta \\alpha_x M_\\theta B^\\top P^{-1} M_x P^{-1} B \\right) \\theta_k$\n- Constant term: $\\alpha_\\theta M_\\theta \\alpha_x B^\\top P^{-1} M_x C^\\top R^{-1} y + \\alpha_\\theta M_\\theta S^{-1} \\theta_{\\text{prior}}$\n\nFrom these expressions, we can construct the matrix $A \\in \\mathbb{R}^{(n+1)\\times(n+1)}$ and vector $b \\in \\mathbb{R}^{n+1}$ in block form, where $z_k = [x_k^\\top, \\theta_k]^\\top$:\n$$\nA = \\begin{bmatrix} A_{xx} & A_{x\\theta} \\\\ A_{\\theta x} & A_{\\theta\\theta} \\end{bmatrix}, \\quad b = \\begin{bmatrix} b_x \\\\ b_\\theta \\end{bmatrix}\n$$\nThe blocks are:\n$A_{xx} = (1 - \\alpha_x) I_{n \\times n}$\n$A_{x\\theta} = \\alpha_x M_x P^{-1} B$\n$A_{\\theta x} = \\alpha_\\theta (1 - \\alpha_x) M_\\theta B^\\top P^{-1}$\n$A_{\\theta\\theta} = (1 - \\alpha_\\theta) + \\alpha_\\theta \\alpha_x M_\\theta (B^\\top P^{-1} M_x P^{-1} B)$\n\n$b_x = \\alpha_x M_x C^\\top R^{-1} y$\n$b_\\theta = \\alpha_\\theta M_\\theta ( \\alpha_x B^\\top P^{-1} M_x C^\\top R^{-1} y + S^{-1} \\theta_{\\text{prior}})$\n\nThe iteration $z_{k+1} = A z_k + b$ is an affine transformation. By the Banach fixed-point theorem, this iteration converges to a unique fixed point $z^\\star$ for any starting point $z_0$ if and only if $A$ is a contraction mapping. For a linear operator in a finite-dimensional space, this condition is equivalent to its spectral radius being less than one: $\\rho(A) < 1$. The spectral radius is defined as $\\rho(A) = \\max_i |\\lambda_i|$, where $\\lambda_i$ are the eigenvalues of $A$.\n\nIf $\\rho(A) < 1$, the fixed point $z^\\star$ satisfies $z^\\star = A z^\\star + b$, which can be rearranged to $(I - A) z^\\star = b$. Since $\\rho(A) < 1$, $1$ is not an eigenvalue of $A$, so $(I-A)$ is invertible. The unique fixed point is given by $z^\\star = (I - A)^{-1} b$. The inverse $(I-A)^{-1}$ can be expressed by the convergent Neumann series $\\sum_{k=0}^{\\infty} A^k$.\n\nThe numerical implementation will construct $A$ and $b$, compute $\\rho(A)$, perform the iteration, and calculate the final error with respect to the analytically determined fixed point $z^\\star$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It orchestrates the calculation for each case and prints the final result.\n    \"\"\"\n\n    # Constants\n    N_max = 200\n    epsilon = 1e-8\n    n = 2\n\n    # Test Case 1: Happy path\n    case1 = {\n        \"C\": np.eye(2),\n        \"R\": np.eye(2),\n        \"P\": np.array([[10.0, 0.0], [0.0, 10.0]]),\n        \"B\": np.array([[3.0], [3.0]]),\n        \"S\": 0.2,\n        \"y\": np.array([[1.0], [-1.0]]),\n        \"theta_prior\": 0.0,\n        \"alpha_x\": 0.6,\n        \"alpha_theta\": 0.4,\n        \"z0\": np.zeros((n + 1, 1))\n    }\n\n    # Test Case 2: Near-boundary contraction\n    case2 = {\n        \"C\": np.eye(2),\n        \"R\": np.eye(2),\n        \"P\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n        \"B\": np.array([[10.0], [10.0]]),\n        \"S\": 1000000.0,\n        \"y\": np.array([[2.0], [-2.0]]),\n        \"theta_prior\": 0.0,\n        \"alpha_x\": 1.0,\n        \"alpha_theta\": 1.0,\n        \"z0\": np.zeros((n + 1, 1))\n    }\n\n    # Test Case 3: Non-contraction via over-relaxation\n    case3 = {\n        \"C\": np.eye(2),\n        \"R\": np.eye(2),\n        \"P\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n        \"B\": np.array([[10.0], [10.0]]),\n        \"S\": 1000000.0,\n        \"y\": np.array([[2.0], [-2.0]]),\n        \"theta_prior\": 0.0,\n        \"alpha_x\": 1.2,\n        \"alpha_theta\": 1.0,\n        \"z0\": np.zeros((n + 1, 1))\n    }\n\n    test_cases = [case1, case2, case3]\n    results = []\n\n    for case_params in test_cases:\n        rho, c, e = process_case(case_params, n, N_max, epsilon)\n        results.extend([rho, c, e])\n\n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef process_case(params, n, N_max, epsilon):\n    \"\"\"\n    Processes a single test case: constructs A and b, computes convergence\n    properties, runs the iteration, and calculates the final error.\n    \"\"\"\n    # Unpack parameters\n    C, R, P, B, S, y = params[\"C\"], params[\"R\"], params[\"P\"], params[\"B\"], params[\"S\"], params[\"y\"]\n    theta_prior, alpha_x, alpha_theta = params[\"theta_prior\"], params[\"alpha_x\"], params[\"alpha_theta\"]\n    z0 = params[\"z0\"]\n\n    # Compute matrix inverses\n    try:\n        R_inv = np.linalg.inv(R)\n        P_inv = np.linalg.inv(P)\n    except np.linalg.LinAlgError:\n        # Handle singular matrices, though not expected from problem spec\n        return np.inf, 0.0, np.inf\n\n    # Compute intermediate quantities M_x and M_theta\n    Mx_inv = C.T @ R_inv @ C + P_inv\n    Mx = np.linalg.inv(Mx_inv)\n    \n    M_theta_inv_val = (B.T @ P_inv @ B)[0, 0] + 1.0/S\n    M_theta = 1.0 / M_theta_inv_val\n\n    # Construct the iteration matrix A\n    A_xx = (1.0 - alpha_x) * np.eye(n)\n    A_xtheta = alpha_x * Mx @ P_inv @ B\n    A_thetax = alpha_theta * (1.0 - alpha_x) * M_theta * (B.T @ P_inv)\n    A_thetatheta_val = (1.0 - alpha_theta) + alpha_theta * alpha_x * M_theta * (B.T @ P_inv @ Mx @ P_inv @ B)[0, 0]\n    A_thetatheta = np.array([[A_thetatheta_val]])\n    A = np.block([[A_xx, A_xtheta], [A_thetax, A_thetatheta]])\n\n    # Construct the constant vector b\n    b_x = alpha_x * Mx @ C.T @ R_inv @ y\n    b_theta_val = alpha_theta * M_theta * (alpha_x * (B.T @ P_inv @ Mx @ C.T @ R_inv @ y)[0, 0] + (1.0/S) * theta_prior)\n    b_theta = np.array([[b_theta_val]])\n    b = np.vstack((b_x, b_theta))\n\n    # Compute spectral radius of A\n    eigenvalues = np.linalg.eigvals(A)\n    rho = np.max(np.abs(eigenvalues))\n\n    # Perform the iteration\n    z = z0.copy()\n    z_N = z0\n    for _ in range(N_max):\n        z_next = A @ z + b\n        diff = np.linalg.norm(z_next - z)\n        z = z_next\n        if diff < epsilon:\n            break\n    z_N = z\n\n    # Compute fixed point and final error\n    if rho >= 1:\n        e = np.inf\n        c = 0.0\n    else:\n        # Compute the true fixed point z_star\n        I_mat = np.eye(n + 1)\n        z_star = np.linalg.inv(I_mat - A) @ b\n        \n        # Compute the final error\n        e = np.linalg.norm(z_N - z_star)\n        \n        # Determine convergence indicator\n        c = 1.0 if e < epsilon else 0.0\n\n    return rho, c, e\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}