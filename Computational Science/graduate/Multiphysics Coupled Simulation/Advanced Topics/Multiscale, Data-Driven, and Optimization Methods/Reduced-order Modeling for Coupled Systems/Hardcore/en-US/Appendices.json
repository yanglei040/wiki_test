{
    "hands_on_practices": [
        {
            "introduction": "In partitioned reduced-order models (ROMs), a key step is constructing a low-dimensional basis to represent the complex behavior at the interface between different physical subsystems. A powerful and widely used method for this is the Singular Value Decomposition (SVD), which provides a hierarchically optimal basis. This analytical exercise explores the fundamental relationship between the singular values of the interface transfer operator and the approximation error incurred when truncating the basis. By completing this practice , you will derive a core result in ROM theory that precisely quantifies the worst-case error, linking the ROM's predictive accuracy directly to the spectral properties of the high-fidelity coupling operator.",
            "id": "3524751",
            "problem": "Consider a linear time-invariant (LTI) partitioned multiphysics system with two subsystems coupled through an interface, where subsystem $\\mathcal{A}$ is characterized locally by a linear transfer operator $T \\in \\mathbb{R}^{m \\times p}$ that maps interface data (e.g., generalized interface forces) $g \\in \\mathbb{R}^{p}$ to subsystem interface responses (e.g., generalized interface displacements) $y \\in \\mathbb{R}^{m}$ via $y = T g$. Assume $T$ is bounded and arises from a well-posed discretization of a coercive elliptic or parabolic boundary value problem under standard interface conditions, so that the mapping is linear and stable for physically admissible $g$. In a partitioned Reduced-Order Modeling (ROM) framework, interface modes for subsystem $\\mathcal{A}$ are constructed from the Singular Value Decomposition (SVD), that is, the Singular Value Decomposition (SVD) $T = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{p \\times p}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times p}$ containing singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{q} \\geq 0$ on the diagonal, where $q = \\min\\{m,p\\}$. Define the ROM interface basis as the first $r$ left singular vectors $U_{r} \\in \\mathbb{R}^{m \\times r}$, and approximate the full interface response by orthogonal projection $y_{r} = U_{r} U_{r}^{\\top} y$.\n\nUsing only fundamental properties of linear operators, orthogonal projections, and the Singular Value Decomposition, derive the worst-case interface matching error induced by truncating the interface basis to dimension $r$, measured as the operator-induced residual\n$$\nE_{r}^{\\star} \\equiv \\sup_{\\|g\\|_{2} = 1} \\left\\| y - y_{r} \\right\\|_{2} = \\sup_{\\|g\\|_{2} = 1} \\left\\| (I - U_{r} U_{r}^{\\top}) T g \\right\\|_{2}.\n$$\nExpress the final answer as a single closed-form analytic expression in terms of the singular values of $T$. The answer must be a single expression and contain no inequalities or additional text. No rounding is required, and there are no physical units to report.",
            "solution": "We begin from the linear mapping $y = T g$, with $T \\in \\mathbb{R}^{m \\times p}$ and $g \\in \\mathbb{R}^{p}$. By definition, the partitioned Reduced-Order Modeling (ROM) interface basis is chosen as the first $r$ columns of the left singular vectors matrix $U$ in the Singular Value Decomposition (SVD) of $T = U \\Sigma V^{\\top}$. The projection of $y$ onto the ROM interface subspace is $y_{r} = U_{r} U_{r}^{\\top} y$, where $U_{r} \\in \\mathbb{R}^{m \\times r}$ has orthonormal columns and $U_{r}^{\\top} U_{r} = I_{r}$.\n\nThe interface matching error for a given $g$ is\n$$\ne_{r}(g) \\equiv \\left\\| y - y_{r} \\right\\|_{2} = \\left\\| (I - U_{r} U_{r}^{\\top}) y \\right\\|_{2} = \\left\\| (I - U_{r} U_{r}^{\\top}) T g \\right\\|_{2}.\n$$\nWe are to compute the worst-case error over unit-norm inputs,\n$$\nE_{r}^{\\star} = \\sup_{\\|g\\|_{2} = 1} \\left\\| (I - U_{r} U_{r}^{\\top}) T g \\right\\|_{2}.\n$$\nTo proceed, use the SVD $T = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal. Write the orthogonal projector onto the orthogonal complement of $\\operatorname{range}(U_{r})$ as $P_{\\perp} \\equiv I - U_{r} U_{r}^{\\top}$. Note that $P_{\\perp} U_{r} = 0$ and $P_{\\perp}$ preserves orthogonality with respect to $U$. Consider the decomposition of $U$ as $U = [U_{r} \\;\\; U_{\\perp}]$, where $U_{\\perp} \\in \\mathbb{R}^{m \\times (m-r)}$ contains the remaining left singular vectors. Then $P_{\\perp} U = [0 \\;\\; U_{\\perp}]$, so\n$$\nP_{\\perp} T = P_{\\perp} U \\Sigma V^{\\top} = [0 \\;\\; U_{\\perp}] \\Sigma V^{\\top}.\n$$\nLet the singular values be grouped compatibly with the partition: $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\ldots, \\sigma_{r}, \\sigma_{r+1}, \\ldots, \\sigma_{q})$ on the $q \\times q$ principal diagonal, with zero padding if $m \\neq p$. Then\n$$\nP_{\\perp} T = U_{\\perp} \\Sigma_{\\perp} V^{\\top},\n$$\nwhere $\\Sigma_{\\perp} = \\operatorname{diag}(\\sigma_{r+1}, \\ldots, \\sigma_{q})$ on the $(q-r) \\times (q-r)$ diagonal, and the remaining entries are zero.\n\nTherefore, for any $g$,\n$$\n\\left\\| (I - U_{r} U_{r}^{\\top}) T g \\right\\|_{2} = \\left\\| U_{\\perp} \\Sigma_{\\perp} V^{\\top} g \\right\\|_{2}.\n$$\nBecause $U_{\\perp}$ is orthogonal (its columns are orthonormal and extend $U_{r}$ to an orthonormal basis of $\\mathbb{R}^{m}$), the $2$-norm is invariant under left multiplication by $U_{\\perp}$, yielding\n$$\n\\left\\| U_{\\perp} \\Sigma_{\\perp} V^{\\top} g \\right\\|_{2} = \\left\\| \\Sigma_{\\perp} V^{\\top} g \\right\\|_{2}.\n$$\nSimilarly, since $V$ is orthogonal, define $h \\equiv V^{\\top} g$, so that $\\|h\\|_{2} = \\|g\\|_{2}$. For unit-norm $g$, we have $\\|h\\|_{2} = 1$, and thus\n$$\n\\left\\| \\Sigma_{\\perp} h \\right\\|_{2}^{2} = \\sum_{i=r+1}^{q} \\sigma_{i}^{2} h_{i}^{2}.\n$$\nThe supremum of this quantity over all $h$ with $\\|h\\|_{2} = 1$ is achieved by choosing $h$ aligned with the coordinate corresponding to the largest discarded singular value, i.e., $h = e_{j}$ where $j$ maximizes $\\sigma_{j}$ over $j \\in \\{r+1, \\ldots, q\\}$. Because the singular values are sorted nonincreasingly, the largest among the discarded singular values is $\\sigma_{r+1}$. Hence,\n$$\n\\sup_{\\|g\\|_{2} = 1} \\left\\| (I - U_{r} U_{r}^{\\top}) T g \\right\\|_{2} = \\sup_{\\|h\\|_{2} = 1} \\left\\| \\Sigma_{\\perp} h \\right\\|_{2} = \\sigma_{r+1}.\n$$\nThis is exactly the operator norm $\\|P_{\\perp} T\\|_{2}$, which equals the next singular value of $T$ after truncation at rank $r$. Therefore, the worst-case interface matching error induced by truncation of the SVD-based interface basis to dimension $r$ is the $(r+1)$-th singular value of $T$.\n\nThe final expression requested is\n$$\nE_{r}^{\\star} = \\sigma_{r+1}.\n$$",
            "answer": "$$\\boxed{\\sigma_{r+1}}$$"
        },
        {
            "introduction": "A primary motivation for reduced-order modeling is computational speedup, but even after reducing the number of state variables, evaluating nonlinear physical terms can remain a bottleneck. Hyper-reduction techniques address this by approximating these terms using information from only a small subset of the original computational mesh. However, for models based on conservation laws, a naive sampling approach can break the delicate mathematical structure that ensures physical consistency. This hands-on coding practice  demonstrates this critical issue within a finite-volume context, guiding you to see why a simplistic scheme fails and how to design a conservative alternative that correctly preserves the underlying physical principles.",
            "id": "3524769",
            "problem": "Consider a one-dimensional, two-field, coupled conservation problem posed on a uniform mesh with $N$ control volumes (cells), cell width $\\Delta x$, and $N+1$ faces indexed by $f=0,1,\\dots,N$. The state fields are $a_i$ and $b_i$ defined at cell centers $i=0,1,\\dots,N-1$. The governing equations are constructed from the integral form of conservation laws and discretized via the finite-volume method: for each cell $i$, the semi-discrete residuals are\n$$\nR_i^a = F_{i+1/2}^a - F_{i-1/2}^a + S_i^a,\\qquad R_i^b = F_{i+1/2}^b - F_{i-1/2}^b + S_i^b,\n$$\nwhere $F_{i\\pm 1/2}^{(\\cdot)}$ are numerical fluxes at faces and $S_i^{(\\cdot)}$ are local source terms. Let the advection speed be $c>0$, diffusion coefficient for $b$ be $D\\ge 0$, and linear coupling be $k\\ge 0$, with source terms\n$$\nS_i^a = -k\\,(a_i - b_i),\\qquad S_i^b = +k\\,(a_i - b_i).\n$$\nAssume inflow Dirichlet boundary conditions at the left boundary for both $a$ and $b$ with prescribed values $a_{\\text{in}}$ and $b_{\\text{in}}$, and outflow with zero diffusive flux at the right boundary. Use upwind advection for both $a$ and $b$ with $c>0$ and central differencing for the diffusive flux of $b$. Specifically, for faces $f=0,\\dots,N$ and cells $i=0,\\dots,N-1$:\n- For advection, $F_f^a = c \\, \\tilde{a}_f$ and $F_f^{\\text{adv},b} = c \\, \\tilde{b}_f$, where $\\tilde{a}_0 = a_{\\text{in}}$, $\\tilde{a}_f = a_{f-1}$ for $f\\ge 1$, and similarly $\\tilde{b}_0=b_{\\text{in}}$, $\\tilde{b}_f=b_{f-1}$ for $f\\ge 1$.\n- For diffusion, $F_f^{\\text{diff},b} = -D\\,\\frac{b_{R(f)} - b_{L(f)}}{\\Delta x}$ with $L(f)=f-1$, $R(f)=f$ for interior faces $f=1,\\dots,N-1$, $F_0^{\\text{diff},b}=-D\\,\\frac{b_0 - b_{\\text{in}}}{\\Delta x}$, and $F_N^{\\text{diff},b}=0$ (zero gradient).\n- The total flux for $b$ is $F_f^b = F_f^{\\text{adv},b} + F_f^{\\text{diff},b}$.\n\nThis discrete construction enforces conservation at the control-volume level: telescoping sums of face fluxes cancel for interior faces, leaving only boundary contributions, while sources contribute locally. In reduced-order modeling (ROM), one often employs hyper-reduction, where only a subset of control volumes is sampled for residual evaluation, and non-sampled contributions are neglected or approximated. Consider a sampled set of control volumes $\\mathcal{S}\\subset\\{0,1,\\dots,N-1\\}$.\n\nDefine two hyper-reduction schemes applied to the discrete finite-volume residuals:\n- A naive scheme that, for each $i\\in\\mathcal{S}$, forms the residual using only faces that are shared by two sampled control volumes (i.e., faces $f$ such that both adjacent cells of $f$ are in $\\mathcal{S}$). Source terms $S_i^{(\\cdot)}$ are always included.\n- A conservative scheme that, for each $i\\in\\mathcal{S}$, forms the residual using both adjacent faces $i-1/2$ and $i+1/2$ with unit weights, regardless of whether the neighboring cell is sampled, and includes source terms.\n\nYour task is to:\n1. Derive, from the finite-volume integral conservation law, why the naive hyper-reduction generally violates discrete conservation at sampled control volumes by breaking the telescoping cancellation of interior face fluxes and omitting boundary or external faces of the sampled region.\n2. Design a sampling and weighting scheme that restores exact discrete conservation at sampled control volumes. Specifically, propose a flux-oriented residual assembly for sampled cells that uses the oriented incidence of faces and unit weights on the two faces adjacent to each sampled cell, ensuring that for each $i\\in\\mathcal{S}$, the hyper-reduced residual equals the full-order residual exactly.\n3. Implement a program that constructs a test problem with the above fluxes and sources, evaluates the exact residuals, and computes, for each test case, the maximum absolute deviation between the hyper-reduced residuals and the exact residuals over sampled cells. Report two numbers per test case: the maximum deviation for the naive scheme and the maximum deviation for the conservative scheme.\n\nUse the following test suite with fixed parameters and sampled sets:\n- Test case $1$: $N=10$, $\\Delta x = 1/N$, $c = 1.2$, $D = 0.05$, $k = 2.0$, $a_{\\text{in}}=1.0$, $b_{\\text{in}}=0.5$, state initialized deterministically as $a_i = \\sin(2\\pi i/N) + 1.0$ and $b_i = \\cos(2\\pi i/N) + 0.5$, sampled set $\\mathcal{S}=\\{3,4,5\\}$.\n- Test case $2$: Same parameters, sampled set $\\mathcal{S}=\\{0\\}$.\n- Test case $3$: Same parameters, sampled set $\\mathcal{S}=\\{0,1,2,3,4,5,6,7,8,9\\}$ (all control volumes).\n- Test case $4$: Same parameters, sampled set $\\mathcal{S}=\\{5,6\\}$.\n\nYour program should:\n- Assemble face fluxes and exact residuals for $a$ and $b$.\n- Construct hyper-reduced residuals under both schemes for the specified sampled sets.\n- For each test case, compute the maximum absolute error over sampled cells and both fields, first for the naive scheme and then for the conservative scheme.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list of floats for one test case. For example, the output should look like $[[e_{1,\\text{naive}},e_{1,\\text{cons}}],[e_{2,\\text{naive}},e_{2,\\text{cons}}],\\dots]$, with each $e$ reported in plain decimal notation.",
            "solution": "The problem statement is scientifically grounded, well-posed, and complete. It describes a canonical finite-volume discretization of a coupled advection-diffusion-reaction system and poses a standard question regarding conservation properties of hyper-reduction schemes used in reduced-order modeling. All parameters and definitions are clear and mathematically consistent. Therefore, the problem is valid, and a full solution is provided below.\n\nThe core of this problem rests on the principle of discrete conservation inherent in the finite-volume method. A valid numerical scheme must ensure that the rate of change of a conserved quantity within a volume is exactly balanced by the fluxes across its boundary and any sources or sinks within it. We will first formalize this principle and then analyze how different hyper-reduction schemes uphold or violate it.\n\nThe integral form of a conservation law for a quantity $u$ is given by\n$$\n\\frac{d}{dt}\\int_{V} u \\,dV + \\int_{\\partial V} \\mathbf{F} \\cdot d\\mathbf{A} = \\int_{V} S \\,dV\n$$\nwhere $V$ is a control volume, $\\partial V$ is its boundary, $\\mathbf{F}$ is the flux vector, and $S$ is the source term. In one dimension, discretizing over a cell $i$ of width $\\Delta x$ from face $i-1/2$ to $i+1/2$, this becomes\n$$\n\\Delta x \\frac{du_i}{dt} + F_{i+1/2} - F_{i-1/2} = \\Delta x S_i\n$$\nThe semi-discrete residual, $R_i$, represents the spatial operator, such that $\\frac{du_i}{dt} = - \\frac{1}{\\Delta x} R_i$. The problem defines the residuals without the $\\Delta x$ scaling as\n$$\nR_i = F_{i+1/2} - F_{i-1/2} + S_i^{\\text{vol}}\n$$\nwhere $S_i^{\\text{vol}} = \\Delta x S_i$. The provided source terms $S_i^a$ and $S_i^b$ are already integrated over the cell volume. For clarity, we will denote the faces by their integer indices, where face $f$ lies between cell $f-1$ and cell $f$. The residual for cell $i$ is thus $R_i = F_{i+1} - F_i + S_i$.\n\nA key property of this formulation is that when residuals are summed over a contiguous block of cells, the fluxes at interior faces cancel out in a telescoping sum. For example, for cells $i$ and $i+1$:\n$$\nR_i + R_{i+1} = (F_{i+1} - F_i + S_i) + (F_{i+2} - F_{i+1} + S_{i+1}) = F_{i+2} - F_i + S_i + S_{i+1}\n$$\nThe flux $F_{i+1}$ at the shared face is eliminated. Summing over the entire domain $\\{0, \\dots, N-1\\}$ yields:\n$$\n\\sum_{i=0}^{N-1} R_i = \\sum_{i=0}^{N-1} (F_{i+1} - F_i) + \\sum_{i=0}^{N-1} S_i = (F_N - F_0) + \\sum_{i=0}^{N-1} S_i\n$$\nThis demonstrates global discrete conservation: the total change is governed solely by fluxes at the domain boundaries ($F_0$ and $F_N$) and the sum of all sources.\n\n### 1. Conservation Violation in the Naive Hyper-Reduction Scheme\n\nHyper-reduction approximates the full system by evaluating residuals only on a small subset of sampled control volumes, $\\mathcal{S} \\subset \\{0, 1, \\dots, N-1\\}$. The naive scheme attempts this approximation by further simplifying the residual calculation itself.\n\nThe problem defines the naive scheme as one that \"forms the residual using only faces that are shared by two sampled control volumes.\" Let's formalize this. A face $f$ is shared by two sampled control volumes if its adjacent cells, $f-1$ and $f$, are both in $\\mathcal{S}$. For a given sampled cell $i \\in \\mathcal{S}$, its residual involves faces $i$ and $i+1$.\n- The flux $F_{i+1}$ contributes to $R_i$ if face $i+1$ is \"shared,\" which requires both cell $i$ and cell $i+1$ to be in $\\mathcal{S}$.\n- The flux $F_i$ contributes to $R_i$ if face $i$ is \"shared,\" which requires both cell $i-1$ and cell $i$ to be in $\\mathcal{S}$.\n\nThe naive hyper-reduced residual, $\\hat{R}_i^{\\text{naive}}$, for a cell $i \\in \\mathcal{S}$ is therefore:\n$$\n\\hat{R}_i^{\\text{naive}} = \\delta_{i+1 \\in \\mathcal{S}} F_{i+1} - \\delta_{i-1 \\in \\mathcal{S}} F_i + S_i\n$$\nwhere $\\delta$ is an indicator function, evaluating to $1$ if its condition is true and $0$ otherwise. (For boundary cells $i=0$ and $i=N-1$, $i-1$ and $i+1$ are outside the domain index set, so their membership in $\\mathcal{S}$ is false).\n\nThe exact full-order residual is $R_i = F_{i+1} - F_i + S_i$. The error in the naive residual is:\n$$\nE_i = R_i - \\hat{R}_i^{\\text{naive}} = (1 - \\delta_{i+1 \\in \\mathcal{S}}) F_{i+1} - (1 - \\delta_{i-1 \\in \\mathcal{S}}) F_i\n$$\nThis error is non-zero whenever a face adjacent to the sampled cell $i$ is a boundary of the sampled region. Specifically:\n- If $i \\in \\mathcal{S}$ but its right neighbor $i+1 \\notin \\mathcal{S}$, then $\\delta_{i+1 \\in \\mathcal{S}} = 0$. The naive residual omits the term $F_{i+1}$, introducing an error of $F_{i+1}$.\n- If $i \\in \\mathcal{S}$ but its left neighbor $i-1 \\notin \\mathcal{S}$, then $\\delta_{i-1 \\in \\mathcal{S}} = 0$. The naive residual omits the term $-F_i$, introducing an error of $-F_i$.\n\nThis omission breaks the fundamental principle of conservation. The boundary fluxes of the sampled region (the set of faces separating $\\mathcal{S}$ from its complement) are discarded, meaning the hyper-reduced system does not account for the interaction between the sampled and non-sampled parts of the domain. This leads to an incorrect, non-conservative approximation.\n\n### 2. Design of a Conservative Hyper-Reduction Scheme\n\nTo restore discrete conservation at the level of the sampled cells, the hyper-reduced residual for each $i \\in \\mathcal{S}$ must equal the exact full-order residual. This is not an approximation of the residual formula, but rather a selective evaluation of the true residuals. A sampling and weighting scheme that achieves this is, by definition, one that applies a weight of unity to all terms present in the full-order residual.\n\nThe problem statement describes such a \"conservative scheme\" that for each $i \\in \\mathcal{S}$, uses \"both adjacent faces $i-1/2$ and $i+1/2$ with unit weights\". This leads to the following formulation for the conservative hyper-reduced residual, $\\hat{R}_i^{\\text{cons}}$, for a cell $i \\in \\mathcal{S}$:\n$$\n\\hat{R}_i^{\\text{cons}} = (1) \\cdot F_{i+1} - (1) \\cdot F_i + S_i = R_i\n$$\nThis design is \"conservative\" because it correctly computes the full residual for the sampled cell, thereby exactly honoring the conservation law for that control volume. The key implication, which is central to methods like DEIM (Discrete Empirical Interpolation Method), is that evaluating $R_i$ requires knowledge of the state variables (e.g., $a_i$, $b_i$) in the stencil of the operator. For the given finite-volume scheme, the stencil for $R_i$ consists of cells $\\{i-1, i, i+1\\}$ (due to the nearest-neighbor flux calculations).\n\nTherefore, a truly conservative hyper-reduction scheme entails two components:\n1.  **Equation Sampling**: Selecting the set of control volumes $\\mathcal{S}$ where residuals will be evaluated.\n2.  **State Sampling/Reconstruction**: Ensuring the state variables on the full stencil of each sampled residual are available. In the context of this problem, the full state vector is given, so we can directly access states in neighboring non-sampled cells (e.g., $a_{i-1}$ if $i-1 \\notin \\mathcal{S}$).\n\nBy this construction, the deviation between the \"conservative\" hyper-reduced residual and the exact residual for any sampled cell $i \\in \\mathcal{S}$ is, by definition, zero. The implementation will verify this property numerically.\n\n### 3. Implementation and Evaluation\n\nWe will now implement a Python program to compute the exact residuals and the two hyper-reduced residuals for the given test cases. The program will calculate the maximum absolute deviation over all sampled cells and both fields ($a$ and $b$) for each scheme.\n\nThe procedure is as follows:\n- Define system parameters ($N, \\Delta x, c, D, k, a_{\\text{in}}, b_{\\text{in}}$).\n- Initialize the state vectors $a$ and $b$ as specified.\n- Compute the full set of face fluxes $F^a$ and $F^b$ for all $N+1$ faces.\n- Compute the exact residuals $R^a$ and $R^b$ for all $N$ cells.\n- For each test case defined by a sampled set $\\mathcal{S}$:\n    - Calculate the naive residuals $\\hat{R}^{\\text{naive}}$ for $i \\in \\mathcal{S}$ by including only fluxes at faces internal to $\\mathcal{S}$.\n    - Calculate the conservative residuals $\\hat{R}^{\\text{cons}}$ for $i \\in \\mathcal{S}$ by using the full residual formula.\n    - Compute the errors $E_i = R_i - \\hat{R}_i$ for both schemes and both fields.\n    - Find the maximum absolute error for the naive scheme and the conservative scheme.\n- Collect and print the results in the specified format. The error for the conservative scheme is expected to be identically zero.",
            "answer": "```python\nimport numpy as np\n\ndef compute_full_residuals(N, dx, c, D, k, a_in, b_in, a, b):\n    \"\"\"\n    Computes the full-order finite-volume residuals for the coupled system.\n\n    Args:\n        N (int): Number of control volumes.\n        dx (float): Width of a control volume.\n        c (float): Advection speed.\n        D (float): Diffusion coefficient for field b.\n        k (float): Coupling coefficient.\n        a_in (float): Inflow boundary value for a.\n        b_in (float): Inflow boundary value for b.\n        a (np.ndarray): State vector for field a (size N).\n        b (np.ndarray): State vector for field b (size N).\n\n    Returns:\n        tuple: A tuple containing:\n            - Ra (np.ndarray): Exact residual vector for field a.\n            - Rb (np.ndarray): Exact residual vector for field b.\n            - Fa (np.ndarray): Face flux vector for field a.\n            - Fb (np.ndarray): Face flux vector for field b.\n    \"\"\"\n    # Initialize flux arrays of size N+1\n    Fa = np.zeros(N + 1)\n    Fb = np.zeros(N + 1)\n\n    # Face f=0 (left boundary)\n    Fa[0] = c * a_in\n    Fb[0] = c * b_in - D * (b[0] - b_in) / dx\n\n    # Interior faces f=1 to N-1\n    for f in range(1, N):\n        # Advection flux (upwind)\n        Fa[f] = c * a[f - 1]\n        F_adv_b = c * b[f - 1]\n        # Diffusion flux (central)\n        F_diff_b = -D * (b[f] - b[f - 1]) / dx\n        Fb[f] = F_adv_b + F_diff_b\n\n    # Face f=N (right boundary)\n    Fa[N] = c * a[N - 1]\n    F_adv_b = c * b[N - 1]\n    F_diff_b = 0.0  # Zero diffusive flux\n    Fb[N] = F_adv_b + F_diff_b\n\n    # Initialize residual arrays of size N\n    Ra = np.zeros(N)\n    Rb = np.zeros(N)\n\n    # Compute residuals for cells i=0 to N-1\n    for i in range(N):\n        # Source terms\n        Sa = -k * (a[i] - b[i])\n        Sb = k * (a[i] - b[i])\n        # Residuals\n        Ra[i] = Fa[i + 1] - Fa[i] + Sa\n        Rb[i] = Fb[i + 1] - Fb[i] + Sb\n\n    return Ra, Rb, Fa, Fb\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and compute hyper-reduction errors.\n    \"\"\"\n    # Fixed problem parameters\n    N = 10\n    dx = 1.0 / N\n    c = 1.2\n    D = 0.05\n    k = 2.0\n    a_in = 1.0\n    b_in = 0.5\n\n    # Test cases defined by their sampled sets\n    test_cases = [\n        {\"S\": {3, 4, 5}},\n        {\"S\": {0}},\n        {\"S\": set(range(N))},\n        {\"S\": {5, 6}},\n    ]\n\n    # Initialize state vectors based on the problem description\n    i_vals = np.arange(N)\n    a_init = np.sin(2 * np.pi * i_vals / N) + 1.0\n    b_init = np.cos(2 * np.pi * i_vals / N) + 0.5\n    \n    # Compute the exact full-order residuals and fluxes once\n    Ra_exact, Rb_exact, Fa, Fb = compute_full_residuals(N, dx, c, D, k, a_in, b_in, a_init, b_init)\n\n    results = []\n    \n    for case in test_cases:\n        S = case[\"S\"]\n        max_error_naive = 0.0\n        max_error_cons = 0.0\n\n        for i in S:\n            # Source terms are always included in both schemes\n            Sa = -k * (a_init[i] - b_init[i])\n            Sb = k * (a_init[i] - b_init[i])\n\n            # --- Naive Scheme Residual Calculation ---\n            # Include flux terms only if the face is shared by two sampled cells\n            is_left_neighbor_sampled = (i > 0) and ((i - 1) in S)\n            is_right_neighbor_sampled = (i < N - 1) and ((i + 1) in S)\n            \n            # Use indicator flags to include/exclude flux terms\n            Ra_naive_i = (is_right_neighbor_sampled * Fa[i + 1]) - (is_left_neighbor_sampled * Fa[i]) + Sa\n            Rb_naive_i = (is_right_neighbor_sampled * Fb[i + 1]) - (is_left_neighbor_sampled * Fb[i]) + Sb\n\n            # --- Conservative Scheme Residual Calculation ---\n            # Use all flux terms with unit weights\n            Ra_cons_i = Fa[i + 1] - Fa[i] + Sa\n            Rb_cons_i = Fb[i + 1] - Fb[i] + Sb\n\n            # --- Error Calculation for cell i ---\n            # Compare naive residuals with exact residuals\n            error_a_naive = abs(Ra_naive_i - Ra_exact[i])\n            error_b_naive = abs(Rb_naive_i - Rb_exact[i])\n            max_error_naive = max(max_error_naive, error_a_naive, error_b_naive)\n\n            # Compare conservative residuals with exact residuals\n            error_a_cons = abs(Ra_cons_i - Ra_exact[i])\n            error_b_cons = abs(Rb_cons_i - Rb_exact[i])\n            max_error_cons = max(max_error_cons, error_a_cons, error_b_cons)\n\n        results.append([max_error_naive, max_error_cons])\n\n    # Format the output string as specified\n    # e.g., [[err1_n, err1_c], [err2_n, err2_c]]\n    output_str = \"[\" + \",\".join([f\"[{naive_err},{cons_err}]\" for naive_err, cons_err in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Constructing a reduced-order model is only half the battle; for strongly coupled problems, you are often left with a nonlinear system of algebraic equations that must be solved efficiently. Quasi-Newton methods are a powerful class of iterative solvers that avoid the high cost of computing the exact Jacobian matrix at every step. This exercise  shifts the focus from model construction to the numerical solution algorithm itself. By analyzing a quasi-Newton solver applied to a reduced interface problem, you will derive its convergence rate, gaining crucial insight into how the solver's design and parameters directly impact the performance and robustness of the coupled ROM simulation.",
            "id": "3524781",
            "problem": "Consider a two-physics strongly coupled system whose full-order monolithic interface reduction yields a symmetric positive definite Schur complement operator $S \\in \\mathbb{R}^{3 \\times 3}$ mapping interface states to residual tractions. A Reduced-Order Model (ROM) is constructed by a Galerkin projection onto a two-dimensional orthonormal interface basis $U_{r} \\in \\mathbb{R}^{3 \\times 2}$ with columns\n$u_{1} = [1, 0, 0]^{\\top}$ and $u_{2} = [0, 1/\\sqrt{2}, 1/\\sqrt{2}]^{\\top}$. The full-order operator is\n$$\nS = \\operatorname{diag}(6, 5, 4).\n$$\nThe reduced residual is $R_{r}(y) = \\tilde{J}_{\\star} y - \\tilde{g}$ with $y \\in \\mathbb{R}^{2}$, where the exact reduced interface Jacobian is the Galerkin projection $\\tilde{J}_{\\star} = U_{r}^{\\top} S U_{r}$ and $\\tilde{g} \\in \\mathbb{R}^{2}$ is the reduced forcing. A quasi-Newton interface solver in reduced space is applied to $R_{r}(y) = 0$ with iterations\n$$\ny_{k+1} = y_{k} - \\tilde{J}_{k}^{-1} R_{r}(y_{k}),\n$$\nwhere $\\tilde{J}_{k}$ is an approximation to $\\tilde{J}_{\\star}$. Assume $y_{0} = 0$ and choose $\\tilde{g}$ such that the initial reduced residual equals the first canonical vector, i.e., $R_{r}(y_{0}) = r_{0} = e_{1}$. The initial Jacobian guess is isotropic, $\\tilde{J}_{0} = \\gamma I_{2}$ with $\\gamma > 0$, and the Jacobian approximation is updated using a single secant condition and the least-change principle in the Frobenius norm (that is, the Jacobian approximation solves a minimum-deviation problem subject to the secant constraint).\n\nStarting from first principles appropriate to this multiphysics reduced-order setting:\n- Derive the exact reduced interface Jacobian $\\tilde{J}_{\\star}$ from $S$ and $U_{r}$.\n- Using the secant condition and the least-change principle, derive the first quasi-Newton reduced Jacobian $\\tilde{J}_{1}$ after one iteration, expressed explicitly in terms of $\\gamma$ and problem data.\n- Analyze the local linear convergence by computing the spectral radius of the reduced iteration matrix $I - \\tilde{J}_{1}^{-1} \\tilde{J}_{\\star}$, and simplify the result to a single closed-form analytic expression in $\\gamma$.\n\nReport as your final answer the simplified expression for the spectral radius. No rounding is required. The final answer must be a single closed-form expression with no units.",
            "solution": "The problem requires the derivation of the spectral radius of the local convergence matrix for a quasi-Newton method applied to a reduced-order model. The process involves three main steps: first, determining the exact reduced Jacobian $\\tilde{J}_{\\star}$; second, computing the first updated quasi-Newton Jacobian approximation $\\tilde{J}_{1}$; and third, calculating the spectral radius of the resulting iteration error propagation matrix.\n\nFirst, we derive the exact reduced interface Jacobian, $\\tilde{J}_{\\star}$. This is obtained by a Galerkin projection of the full-order Schur complement operator $S$ onto the reduced basis $U_r$. The formula is given as $\\tilde{J}_{\\star} = U_{r}^{\\top} S U_{r}$.\n\nThe full-order operator is $S = \\operatorname{diag}(6, 5, 4)$, which can be written as the matrix:\n$$\nS = \\begin{pmatrix} 6 & 0 & 0 \\\\ 0 & 5 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix}\n$$\nThe reduced basis $U_r$ is composed of two orthonormal columns $u_1 = [1, 0, 0]^{\\top}$ and $u_2 = [0, 1/\\sqrt{2}, 1/\\sqrt{2}]^{\\top}$:\n$$\nU_r = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} \\\\ 0 & \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n$$\nThe transpose of the basis matrix is:\n$$\nU_{r}^{\\top} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n$$\nWe now perform the matrix multiplication to find $\\tilde{J}_{\\star}$:\n$$\n\\tilde{J}_{\\star} = U_{r}^{\\top} S U_{r} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} 6 & 0 & 0 \\\\ 0 & 5 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} \\\\ 0 & \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n$$\n$$\n\\tilde{J}_{\\star} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} 6 & 0 \\\\ 0 & \\frac{5}{\\sqrt{2}} \\\\ 0 & \\frac{4}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 6 & 0 \\\\ 0 & \\frac{5}{2} + \\frac{4}{2} \\end{pmatrix} = \\begin{pmatrix} 6 & 0 \\\\ 0 & \\frac{9}{2} \\end{pmatrix}\n$$\nThus, the exact reduced Jacobian is $\\tilde{J}_{\\star} = \\operatorname{diag}(6, 4.5)$.\n\nSecond, we derive the first quasi-Newton reduced Jacobian $\\tilde{J}_{1}$. The update from the initial guess $\\tilde{J}_{0}$ to $\\tilde{J}_{1}$ is based on satisfying a secant condition while minimizing the change from $\\tilde{J}_{0}$ in the Frobenius norm. This is known as Broyden's \"good\" method, and the update formula is:\n$$\n\\tilde{J}_{k+1} = \\tilde{J}_{k} + \\frac{(v_k - \\tilde{J}_{k} s_k) s_k^{\\top}}{s_k^{\\top} s_k}\n$$\nwhere $s_k = y_{k+1} - y_k$ and $v_k = R_r(y_{k+1}) - R_r(y_k)$. We need to calculate these quantities for $k=0$.\n\nThe initial state is $y_0 = 0$. The initial Jacobian is $\\tilde{J}_{0} = \\gamma I_{2} = \\operatorname{diag}(\\gamma, \\gamma)$. The initial reduced residual is given as $R_r(y_0) = r_0 = e_1 = [1, 0]^{\\top}$.\nThe first step of the quasi-Newton iteration is:\n$$\ny_1 = y_0 - \\tilde{J}_{0}^{-1} R_r(y_0) = 0 - (\\gamma I_2)^{-1} e_1 = -\\frac{1}{\\gamma} e_1 = \\begin{pmatrix} -1/\\gamma \\\\ 0 \\end{pmatrix}\n$$\nFrom this, the step vector $s_0$ is:\n$$\ns_0 = y_1 - y_0 = \\begin{pmatrix} -1/\\gamma \\\\ 0 \\end{pmatrix}\n$$\nNext, we determine the change in the residual, $v_0$. The reduced residual function is $R_r(y) = \\tilde{J}_{\\star} y - \\tilde{g}$. Using the initial condition, $R_r(y_0) = \\tilde{J}_{\\star} \\cdot 0 - \\tilde{g} = e_1$, which implies $\\tilde{g} = -e_1$. Therefore, $R_r(y) = \\tilde{J}_{\\star} y + e_1$.\nWe compute the residual at $y_1$:\n$$\nR_r(y_1) = \\tilde{J}_{\\star} y_1 + e_1 = \\begin{pmatrix} 6 & 0 \\\\ 0 & \\frac{9}{2} \\end{pmatrix} \\begin{pmatrix} -1/\\gamma \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -6/\\gamma \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - 6/\\gamma \\\\ 0 \\end{pmatrix}\n$$\nThe change in residual $v_0$ is:\n$$\nv_0 = R_r(y_1) - R_r(y_0) = \\begin{pmatrix} 1 - 6/\\gamma \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -6/\\gamma \\\\ 0 \\end{pmatrix}\n$$\nNow we compute the terms for the Broyden update:\n$$\n\\tilde{J}_{0} s_0 = \\begin{pmatrix} \\gamma & 0 \\\\ 0 & \\gamma \\end{pmatrix} \\begin{pmatrix} -1/\\gamma \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n$$\n$$\nv_0 - \\tilde{J}_{0} s_0 = \\begin{pmatrix} -6/\\gamma \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - 6/\\gamma \\\\ 0 \\end{pmatrix}\n$$\n$$\ns_0^{\\top} s_0 = (-1/\\gamma)^2 + 0^2 = \\frac{1}{\\gamma^2}\n$$\nThe update term is:\n$$\n\\frac{(v_0 - \\tilde{J}_{0} s_0) s_0^{\\top}}{s_0^{\\top} s_0} = \\frac{\\begin{pmatrix} 1 - 6/\\gamma \\\\ 0 \\end{pmatrix} \\begin{pmatrix} -1/\\gamma & 0 \\end{pmatrix}}{1/\\gamma^2} = \\gamma^2 \\begin{pmatrix} (1-6/\\gamma)(-1/\\gamma) & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} -\\gamma(1-6/\\gamma) & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 6-\\gamma & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nFinally, we compute $\\tilde{J}_1$:\n$$\n\\tilde{J}_1 = \\tilde{J}_0 + \\begin{pmatrix} 6-\\gamma & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\gamma & 0 \\\\ 0 & \\gamma \\end{pmatrix} + \\begin{pmatrix} 6-\\gamma & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 6 & 0 \\\\ 0 & \\gamma \\end{pmatrix}\n$$\n\nThird, we analyze the local linear convergence by computing the spectral radius of the reduced iteration matrix $M = I - \\tilde{J}_{1}^{-1} \\tilde{J}_{\\star}$. The spectral radius, $\\rho(M)$, is the maximum absolute value of the eigenvalues of $M$.\nThe inverse of $\\tilde{J}_1$ is (given $\\gamma > 0$):\n$$\n\\tilde{J}_{1}^{-1} = \\begin{pmatrix} 1/6 & 0 \\\\ 0 & 1/\\gamma \\end{pmatrix}\n$$\nWe compute the product $\\tilde{J}_{1}^{-1} \\tilde{J}_{\\star}$:\n$$\n\\tilde{J}_{1}^{-1} \\tilde{J}_{\\star} = \\begin{pmatrix} 1/6 & 0 \\\\ 0 & 1/\\gamma \\end{pmatrix} \\begin{pmatrix} 6 & 0 \\\\ 0 & \\frac{9}{2} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{9}{2\\gamma} \\end{pmatrix}\n$$\nNow, we find the iteration matrix $M$:\n$$\nM = I - \\tilde{J}_{1}^{-1} \\tilde{J}_{\\star} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{9}{2\\gamma} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 - \\frac{9}{2\\gamma} \\end{pmatrix}\n$$\nThe eigenvalues of a diagonal matrix are its diagonal entries. The eigenvalues of $M$ are $\\lambda_1 = 0$ and $\\lambda_2 = 1 - \\frac{9}{2\\gamma}$.\nThe spectral radius is the maximum of the absolute values of the eigenvalues:\n$$\n\\rho(M) = \\max(|\\lambda_1|, |\\lambda_2|) = \\max\\left( |0|, \\left|1 - \\frac{9}{2\\gamma}\\right| \\right) = \\left|1 - \\frac{9}{2\\gamma}\\right|\n$$\nThis is the final, simplified closed-form expression for the spectral radius as a function of $\\gamma$.",
            "answer": "$$\n\\boxed{\\left|1 - \\frac{9}{2\\gamma}\\right|}\n$$"
        }
    ]
}