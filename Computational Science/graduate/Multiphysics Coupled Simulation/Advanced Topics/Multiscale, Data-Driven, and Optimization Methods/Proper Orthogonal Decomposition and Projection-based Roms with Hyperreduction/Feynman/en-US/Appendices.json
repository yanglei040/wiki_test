{
    "hands_on_practices": [
        {
            "introduction": "Constructing a reduced-order model involves more than just finding an optimal basis; it also requires projecting the governing equations onto that basis. This projection step is not unique, and different methods can lead to different reduced models. This first practice delves into the fundamental differences between two prominent approaches: the classic Galerkin projection and the optimization-based Least-Squares Petrov–Galerkin (LSPG) method. By deriving the condition under which they become equivalent and working through a concrete example, you will develop a deeper intuition for how the choice of projection impacts the ROM's solution .",
            "id": "3524021",
            "problem": "Consider a semi-discrete multiphysics system obtained after spatial discretization of coupled governing laws, given by $M \\frac{d y}{d t} = f(y,t)$, where $y \\in \\mathbb{R}^{N}$ is the state vector, $M \\in \\mathbb{R}^{N \\times N}$ is a symmetric positive definite mass matrix, and $f : \\mathbb{R}^{N} \\times \\mathbb{R} \\to \\mathbb{R}^{N}$ is a sufficiently smooth right-hand side representing the coupled physics. Let a reduced-order model (ROM) be constructed via Proper Orthogonal Decomposition (POD), yielding an $M$-orthonormal trial basis $\\Phi \\in \\mathbb{R}^{N \\times r}$ satisfying $\\Phi^{\\top} M \\Phi = I_{r}$, where $I_{r}$ is the $r \\times r$ identity. The ROM state is parameterized as $y \\approx y_{\\mathrm{ref}} + \\Phi a$, with $a \\in \\mathbb{R}^{r}$ the reduced coordinates and $y_{\\mathrm{ref}}$ a reference state.\n\nThe Galerkin projection enforces $M$-inner-product orthogonality of the continuous residual to the trial space, while the Least-Squares Petrov–Galerkin (LSPG) method enforces optimality of a time-discrete residual in a weighted least-squares sense, possibly with hyperreduction through a sampling-and-weighting operator $W \\in \\mathbb{R}^{s \\times N}$ (with $s \\le N$). Use the backward Euler method with time step $\\Delta t$ to define the discrete residual $r_{n}(y_{n}) = M (y_{n} - y_{n-1}) - \\Delta t\\, f(y_{n}, t_{n})$ at time $t_{n}$.\n\nTasks:\n1) Starting from the definitions above and first principles of projection, derive the algebraic condition on the weighting and Jacobian under which the first-order optimality conditions of the LSPG method (with backward Euler and hyperreduction) reduce exactly to the Galerkin orthogonality condition for the same time discretization. Express the condition as a single matrix equality involving $W$, $M$, the Jacobian $J_{f}(y_{n}) = \\frac{\\partial f}{\\partial y}(y_{n}, t_{n})$, and the trial basis $\\Phi$.\n\n2) Construct a concrete example that demonstrates a difference between Galerkin and LSPG for the same backward Euler time step. Let $N = 2$, $M = I_{2}$, $f(y) = A y$ with $A = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}$, $\\Delta t = 1$, $y_{\\mathrm{ref}} = 0$, $y_{n-1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, and $\\Phi = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. For LSPG, take $W = I_{2}$ (no hyperreduction). Compute the one-step backward Euler update produced by Galerkin and by LSPG, and then compute the Euclidean norm of the difference between these two ROM updates at time $t_{n}$.\n\nAnswer specification:\n- Provide the final answer as the exact Euclidean norm of the difference between the LSPG and Galerkin one-step updates for the specified example.\n- No rounding is required.\n- The final answer must be a single real number.",
            "solution": "This problem consists of two parts. The first part requires the derivation of a condition under which the Least-Squares Petrov–Galerkin (LSPG) method becomes equivalent to the Galerkin projection method. The second part requires the computation of a one-step update for a specific system using both methods and finding the norm of their difference.\n\nPart 1: Derivation of the Equivalence Condition\n\nThe state of the reduced-order model (ROM) at time $t_n$ is approximated as $y_n = y_{\\mathrm{ref}} + \\Phi a_n$, where $a_n \\in \\mathbb{R}^r$ are the reduced coordinates. The backward Euler time discretization of the governing equation $M \\frac{dy}{dt} = f(y,t)$ leads to the definition of the full-order discrete residual at step $n$:\n$$\nr_n(y_n) = M (y_n - y_{n-1}) - \\Delta t\\, f(y_n, t_n)\n$$\nBoth Galerkin and LSPG methods formulate a system of equations to solve for the unknown reduced coordinates $a_n$.\n\nThe Galerkin method enforces that the residual $r_n(y_n)$ is orthogonal to the trial subspace $\\text{span}(\\Phi)$ using the Euclidean inner product. This is expressed as:\n$$\n\\Phi^{\\top} r_n(y_n(a_n)) = 0\n$$\nThis is a system of $r$ nonlinear equations for the $r$ components of $a_n$. The condition is derived from $\\Phi^{\\top}[M(\\Phi a_n - \\Phi a_{n-1} + y_{\\mathrm{ref}} - y_{n-1}) - \\Delta t f(y_{\\mathrm{ref}}+\\Phi a_n, t_n)]=0$ along with the $M$-orthonormality condition $\\Phi^{\\top}M\\Phi = I_r$, which simplifies $\\Phi^{\\top}M\\Phi(a_n-a_{n-1})$ to $a_n-a_{n-1}$. This leads back to the standard formulation $\\frac{a_n - a_{n-1}}{\\Delta t} = \\Phi^\\top f(y_n,t_n)$ which is equivalent to $\\Phi^\\top r_n(y_n)=0$.\n\nThe LSPG method seeks to find the reduced coordinates $a_n$ that minimize the weighted squared norm of the residual, possibly with hyperreduction. The objective function is:\n$$\nJ_{\\text{LSPG}}(a_n) = \\frac{1}{2} \\| W r_n(y_n(a_n)) \\|_2^2 = \\frac{1}{2} r_n(y_n(a_n))^{\\top} W^{\\top} W r_n(y_n(a_n))\n$$\nThe first-order optimality condition is that the gradient of $J_{\\text{LSPG}}$ with respect to $a_n$ is zero:\n$$\n\\frac{\\partial J_{\\text{LSPG}}}{\\partial a_n} = 0\n$$\nUsing the chain rule, this gradient is:\n$$\n\\frac{\\partial J_{\\text{LSPG}}}{\\partial a_n} = \\left(\\frac{\\partial y_n}{\\partial a_n}\\right)^{\\top} \\left(\\frac{\\partial r_n}{\\partial y_n}\\right)^{\\top} W^{\\top} W r_n(y_n)\n$$\nWe have the following derivatives:\n$\\frac{\\partial y_n}{\\partial a_n} = \\Phi$\n$\\frac{\\partial r_n}{\\partial y_n} = M - \\Delta t \\frac{\\partial f}{\\partial y}(y_n, t_n) = M - \\Delta t J_f(y_n)$\nSubstituting these into the optimality condition yields the LSPG nonlinear system for $a_n$:\n$$\n\\Phi^{\\top} (M - \\Delta t J_f(y_n))^{\\top} W^{\\top} W r_n(y_n(a_n)) = 0\n$$\nFor the LSPG first-order optimality condition to reduce exactly to the Galerkin orthogonality condition, the two systems of equations must be equivalent. That is, for any residual vector $r_n$, the following equivalence must hold:\n$$\n\\Phi^{\\top} (M - \\Delta t J_f)^{\\top} W^{\\top} W r_n = 0 \\iff \\Phi^{\\top} r_n = 0\n$$\nThis equivalence holds if and only if the row space of the operator on the left is identical to the row space of the operator on the right. This means there must exist an invertible matrix $C \\in \\mathbb{R}^{r \\times r}$ such that:\n$$\n\\Phi^{\\top} (M - \\Delta t J_f)^{\\top} W^{\\top} W = C \\Phi^{\\top}\n$$\nTransposing both sides (and noting that $W^{\\top}W$ is symmetric) gives:\n$$\n(W^{\\top} W (M - \\Delta t J_f) \\Phi)^{\\top} = (\\Phi C^{\\top})^{\\top}\n$$\n$$\nW^{\\top} W (M - \\Delta t J_f) \\Phi = \\Phi C^{\\top}\n$$\nLet $K = C^{\\top}$, which is also an invertible matrix in $\\mathbb{R}^{r \\times r}$. The condition is that there exists an invertible matrix $K$ such that:\n$$\nW^{\\top} W (M - \\Delta t J_f) \\Phi = \\Phi K\n$$\nThis equation states that the subspace spanned by the columns of $\\Phi$, denoted $\\text{span}(\\Phi)$, must be an invariant subspace of the operator $W^{\\top} W (M - \\Delta t J_f)$. The invertibility of $K$ ensures that the mapping does not collapse the dimension of the subspace, thus preserving the equivalence of the equation systems.\n\nPart 2: Concrete Example Calculation\n\nWe are given the following parameters:\n$N=2$, $M=I_2$, $\\Delta t=1$, $y_{\\mathrm{ref}}=0$, $W=I_2$.\n$f(y) = Ay$ with $A = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}$.\n$y_{n-1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n$\\Phi = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nSince $r=1$, the reduced coordinate $a_n$ is a scalar. The ROM state is $y_n = \\Phi a_n = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} a_n = \\begin{pmatrix} a_n \\\\ 0 \\end{pmatrix}$.\nFirst, we compute the residual $r_n(y_n)$:\n$$\nr_n(a_n) = M(y_n - y_{n-1}) - \\Delta t f(y_n) = I_2 \\left( \\begin{pmatrix} a_n \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right) - (1) \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} a_n \\\\ 0 \\end{pmatrix}\n$$\n$$\nr_n(a_n) = \\begin{pmatrix} a_n \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ a_n \\end{pmatrix} = \\begin{pmatrix} a_n \\\\ -1-a_n \\end{pmatrix}\n$$\n\nGalerkin Update:\nThe Galerkin condition is $\\Phi^{\\top} r_n = 0$:\n$$\n\\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} a_n \\\\ -1-a_n \\end{pmatrix} = 0 \\implies a_n^G = 0\n$$\nThe Galerkin state update is $y_n^G = \\Phi a_n^G = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} (0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nLSPG Update:\nThe LSPG condition is $\\Phi^{\\top} (M - \\Delta t J_f)^{\\top} W^{\\top} W r_n = 0$.\nSince $f(y)=Ay$ is linear, the Jacobian is constant: $J_f = A = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}$.\nWith $M=I_2$, $\\Delta t=1$, and $W=I_2$, the condition becomes $\\Phi^{\\top} (I_2 - A)^{\\top} r_n = 0$.\nThe operator term is:\n$$\nI_2 - A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\end{pmatrix}\n$$\n$$\n(I_2 - A)^{\\top} = \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe full LSPG operator acting on the residual is:\n$$\n\\Phi^{\\top} (I_2 - A)^{\\top} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\end{pmatrix}\n$$\nThe LSPG equation is thus:\n$$\n\\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} a_n \\\\ -1-a_n \\end{pmatrix} = 0\n$$\n$$\na_n - (-1-a_n) = 0 \\implies a_n + 1 + a_n = 0 \\implies 2a_n = -1 \\implies a_n^L = -\\frac{1}{2}\n$$\nThe LSPG state update is $y_n^L = \\Phi a_n^L = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left(-\\frac{1}{2}\\right) = \\begin{pmatrix} -1/2 \\\\ 0 \\end{pmatrix}$.\n\nDifference between Updates:\nThe difference vector between the LSPG and Galerkin updates is:\n$$\ny_n^L - y_n^G = \\begin{pmatrix} -1/2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ 0 \\end{pmatrix}\n$$\nThe Euclidean norm of this difference is:\n$$\n\\| y_n^L - y_n^G \\|_2 = \\sqrt{\\left(-\\frac{1}{2}\\right)^2 + 0^2} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}\n$$",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Having explored the nuances of projection, we now turn to the complete construction of a modern, computationally efficient ROM. This practice provides a step-by-step guide to deriving a hyperreduced model from first principles, starting with a general semi-discrete system . By systematically applying a Backward Differentiation Formula (BDF) for time integration, a Petrov–Galerkin projection, and the Discrete Empirical Interpolation Method (DEIM) for hyperreduction, you will see how these theoretical components integrate into a cohesive and practical algorithm. This exercise is crucial for understanding the mechanics behind turning a full-order model into a fast and accurate surrogate.",
            "id": "3524024",
            "problem": "Consider a semi-discrete multiphysics system obtained by the method of lines applied to coupled partial differential equations with algebraic constraints. Let the state be $u(t) \\in \\mathbb{R}^{N}$ and define the residual\n$$\nR(u,\\dot{u},t) := M(u)\\,\\dot{u} - f(u) - A\\,u - b(t),\n$$\nwhere $M(u) \\in \\mathbb{R}^{N \\times N}$ is a possibly state-dependent mass/capacitance/inertia operator arising from the coupling, $f(u) \\in \\mathbb{R}^{N}$ is a nonlinear state-dependent term aggregating flux, reaction, and inter-field coupling effects, $A \\in \\mathbb{R}^{N \\times N}$ is a known linear operator, and $b(t) \\in \\mathbb{R}^{N}$ is a known forcing. Let the time grid be $t^{n} = t^{0} + n\\,\\Delta t$ with constant time step $\\Delta t > 0$.\n\nLet the time derivative be discretized by a $k$-step Backward Differentiation Formula (BDF), which approximates the time derivative at $t^{n+1}$ by\n$$\n\\dot{u}^{n+1} \\approx \\frac{1}{\\Delta t} \\sum_{j=0}^{k} \\alpha_{j}\\,u^{n+1-j},\n$$\nwhere the coefficients $\\alpha_{j}$ are the standard $k$-step BDF coefficients satisfying $\\sum_{j=0}^{k} \\alpha_{j} = 0$ and $\\alpha_{0} > 0$. Assume an implicit treatment of the state-dependent operators at $t^{n+1}$ to obtain a consistent fully discrete residual.\n\nIntroduce a Proper Orthogonal Decomposition (POD) trial basis $V \\in \\mathbb{R}^{N \\times r}$ with $r \\ll N$, and a Petrov–Galerkin test basis $W \\in \\mathbb{R}^{N \\times r}$ such that $W^{T}V = I$. The Reduced-Order Model (ROM) unknowns are the reduced coordinates $q^{n} \\in \\mathbb{R}^{r}$ so that $u^{n} \\approx V\\,q^{n}$. Let $b^{n+1} := b(t^{n+1})$.\n\nStarting from these definitions alone (without invoking any target formulas), do the following:\n- Derive the consistent fully discrete Full-Order Model residual at step $n+1$ in terms of $u^{n+1}$ and the history states $\\{u^{n},\\dots,u^{n+1-k}\\}$.\n- Apply Petrov–Galerkin projection to obtain the ROM residual $r^{n+1}(q^{n+1})$ in terms of $V$, $W$, $q^{n+1}$, the history $\\{q^{n},\\dots,q^{n+1-k}\\}$, and the operators defined above.\n- Factor the ROM residual to expose clearly which terms are state-dependent at $t^{n+1}$ and which are history-dependent, using only algebraic manipulations grounded in the BDF definition and the ROM ansatz.\n- Then, using the principle of interpolatory hyperreduction for vector- and matrix-valued nonlinear terms, construct a Discrete Empirical Interpolation Method (DEIM)-style hyperreduced ROM residual by introducing collateral bases $U_{g} \\in \\mathbb{R}^{N \\times m_{g}}$ for $f(V\\,q)$ and $U_{M} \\in \\mathbb{R}^{N \\times m_{M}}$ for $M(V\\,q)\\,V$, along with sampling (row-selection) matrices $P_{g} \\in \\mathbb{R}^{N \\times m_{g}}$ and $P_{M} \\in \\mathbb{R}^{N \\times m_{M}}$. Your construction must follow directly from enforcing interpolation at sampled rows and projecting onto the collateral bases, without assuming any pre-stated formula.\n\nProvide, as your final answer, a single closed-form analytic expression for the hyperreduced ROM residual at step $n+1$ in terms of the symbols introduced above. Do not include any explanatory text in the final answer. The final answer must be a single symbolic mathematical expression. No units are required.",
            "solution": "The problem requires the derivation of a hyperreduced Reduced-Order Model (ROM) residual for a general multiphysics system, starting from first principles. The derivation proceeds in four sequential steps as outlined in the problem statement.\n\n### Step 1: Problem Validation\n\nFirst, I will extract the given information verbatim from the problem statement.\n- System state: $u(t) \\in \\mathbb{R}^{N}$\n- Semi-discrete residual: $R(u,\\dot{u},t) := M(u)\\,\\dot{u} - f(u) - A\\,u - b(t)$\n- State-dependent operator: $M(u) \\in \\mathbb{R}^{N \\times N}$\n- Nonlinear term: $f(u) \\in \\mathbb{R}^{N}$\n- Linear operator: $A \\in \\mathbb{R}^{N \\times N}$\n- Forcing term: $b(t) \\in \\mathbb{R}^{N}$\n- Time discretization: $t^{n} = t^{0} + n\\,\\Delta t$ for constant $\\Delta t > 0$\n- $k$-step BDF approximation: $\\dot{u}^{n+1} \\approx \\frac{1}{\\Delta t} \\sum_{j=0}^{k} \\alpha_{j}\\,u^{n+1-j}$\n- BDF coefficients properties: $\\sum_{j=0}^{k} \\alpha_{j} = 0$ and $\\alpha_{0} > 0$\n- POD trial basis: $V \\in \\mathbb{R}^{N \\times r}$ with $r \\ll N$\n- Petrov–Galerkin test basis: $W \\in \\mathbb{R}^{N \\times r}$\n- Orthonormality condition: $W^{T}V = I$, where $I$ is the $r \\times r$ identity matrix.\n- ROM ansatz: $u^{n} \\approx V\\,q^{n}$ for reduced coordinates $q^{n} \\in \\mathbb{R}^{r}$\n- Forcing at time step $n+1$: $b^{n+1} := b(t^{n+1})$\n- DEIM basis for $f(Vq)$: $U_{g} \\in \\mathbb{R}^{N \\times m_{g}}$\n- DEIM basis for $M(Vq)V$: $U_{M} \\in \\mathbb{R}^{N \\times m_{M}}$\n- DEIM sampling matrix for $f(Vq)$: $P_{g} \\in \\mathbb{R}^{N \\times m_{g}}$\n- DEIM sampling matrix for $M(Vq)V$: $P_{M} \\in \\mathbb{R}^{N \\times m_{M}}$\n\nNext, I will validate the problem statement against the required criteria.\n- **Scientifically Grounded:** The problem is firmly rooted in the established field of computational science, specifically in model order reduction for systems of differential-algebraic equations. Proper Orthogonal Decomposition (POD), Petrov-Galerkin projection, Backward Differentiation Formulae (BDF), and the Discrete Empirical Interpolation Method (DEIM) are all standard, well-documented techniques. The setup is a canonical representation of a large-scale multiphysics problem discretized in space.\n- **Well-Posed:** The task is to perform a mathematical derivation. The starting point and the target (a formula for the hyperreduced residual) are clearly defined. The steps are logically sequenced, leading to a unique analytical expression.\n- **Objective:** The problem is stated using precise mathematical language and notation, free from any subjectivity, ambiguity, or opinion.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, self-contained, and presents a non-trivial but solvable challenge in applied mathematics. Therefore, the problem is deemed **valid**.\n\n### Step 2: Derivation of the ROM and Hyperreduced ROM Residual\n\nHerein, I will derive the requested expressions by following the specified sequence of operations.\n\n**Derivation of the Fully Discrete Full-Order Model (FOM) Residual**\nThe semi-discrete system residual is set to zero: $R(u, \\dot{u}, t) = 0$. For a fully implicit time-stepping scheme, we evaluate all terms at the new time step $t^{n+1}$, letting $u^{j}$ denote $u(t^{j})$.\n$$\nM(u^{n+1})\\,\\dot{u}^{n+1} - f(u^{n+1}) - A\\,u^{n+1} - b^{n+1} = 0\n$$\nThe time derivative $\\dot{u}^{n+1}$ is replaced by its $k$-step BDF approximation:\n$$\n\\dot{u}^{n+1} \\approx \\frac{1}{\\Delta t} \\sum_{j=0}^{k} \\alpha_{j}\\,u^{n+1-j}\n$$\nSubstituting this into the system equation yields the fully discrete FOM residual, $R_{\\text{FOM}}^{n+1}$, which must be zero for a solution $u^{n+1}$:\n$$\nR_{\\text{FOM}}^{n+1}(u^{n+1}) := M(u^{n+1})\\left(\\frac{1}{\\Delta t} \\sum_{j=0}^{k} \\alpha_{j}\\,u^{n+1-j}\\right) - f(u^{n+1}) - A\\,u^{n+1} - b^{n+1}\n$$\nThis residual is a function of the unknown state $u^{n+1}$ and the known history states $\\{u^{n}, \\dots, u^{n+1-k}\\}$.\n\n**Application of Petrov-Galerkin Projection to Obtain the ROM Residual**\nThe ROM is constructed by introducing the ansatz $u^{n+1-j} \\approx V\\,q^{n+1-j}$ for $j=0, \\dots, k$ into the FOM residual and projecting it onto the test basis $W$. The ROM residual, $r^{n+1}(q^{n+1}) = W^{T}R_{\\text{FOM}}^{n+1}$, is obtained by projecting the FOM residual onto the test basis $W$.\n$$\nr^{n+1}(q^{n+1}) = W^{T} \\left[ M(Vq^{n+1})\\left(\\frac{1}{\\Delta t} \\sum_{j=0}^{k} \\alpha_{j}\\,Vq^{n+1-j}\\right) - f(Vq^{n+1}) - A Vq^{n+1} - b^{n+1} \\right]\n$$\nUsing the linearity of the transpose and projection, we can distribute $W^T$ to each term:\n$$\nr^{n+1}(q^{n+1}) = \\frac{1}{\\Delta t} W^{T}M(Vq^{n+1})V \\left(\\sum_{j=0}^{k} \\alpha_{j}\\,q^{n+1-j}\\right) - W^{T}f(Vq^{n+1}) - W^{T}AVq^{n+1} - W^{T}b^{n+1}\n$$\n\n**Factoring the ROM Residual**\nTo distinguish state-dependent and history-dependent terms, we separate the sum over index $j$:\n$$\n\\sum_{j=0}^{k} \\alpha_{j}\\,q^{n+1-j} = \\alpha_{0}\\,q^{n+1} + \\sum_{j=1}^{k} \\alpha_{j}\\,q^{n+1-j}\n$$\nThe ROM residual can then be written as:\n$$\nr^{n+1}(q^{n+1}) = \\left(\\frac{\\alpha_0}{\\Delta t}W^T M(Vq^{n+1})V - W^TAV\\right)q^{n+1} - W^Tf(Vq^{n+1}) + \\frac{1}{\\Delta t}W^T M(Vq^{n+1})V\\left(\\sum_{j=1}^{k} \\alpha_{j}\\,q^{n+1-j}\\right) - W^T b^{n+1}\n$$\nThe terms involving $M(Vq^{n+1})$ and $f(Vq^{n+1})$ are nonlinear functions of the current state $q^{n+1}$ and are computationally expensive as they depend on the full-order dimension $N$. The summation term involves the history states $\\{q^{n}, \\dots, q^{n+1-k}\\}$.\n\n**Construction of the Hyperreduced ROM Residual**\nHyperreduction aims to approximate the expensive nonlinear terms.\n\n1.  **Hyperreduction of the nonlinear function $f(Vq^{n+1})$:**\n    The vector-valued function $g(q) := f(Vq)$ is approximated via projection onto the collateral basis $U_g$: $g(q^{n+1}) \\approx U_g c_g^{n+1}$. The DEIM principle determines the coefficients $c_g^{n+1}$ by enforcing equality at a set of $m_g$ \"magic points\" or rows, selected by the matrix $P_g^T$:\n    $$P_g^T g(q^{n+1}) = P_g^T U_g c_g^{n+1} \\implies c_g^{n+1} = (P_g^T U_g)^{-1} P_g^T g(q^{n+1})$$\n    The DEIM approximation for $f(Vq^{n+1})$ is therefore:\n    $$\n    f(Vq^{n+1}) \\approx U_g (P_g^T U_g)^{-1} P_g^T f(Vq^{n+1})\n    $$\n    The corresponding projected term in the residual is approximated as:\n    $$\n    W^T f(Vq^{n+1}) \\approx W^T U_g (P_g^T U_g)^{-1} P_g^T f(Vq^{n+1})\n    $$\n    The online cost is dominated by evaluating $P_g^T f(Vq^{n+1})$, which requires computing only $m_g$ components of the nonlinear function $f$.\n\n2.  **Hyperreduction of the matrix-dependent term $M(Vq^{n+1})V$:**\n    The problem provides a basis $U_M$ for the matrix-valued function $\\mathcal{M}(q) := M(Vq)V \\in \\mathbb{R}^{N \\times r}$. We approximate the column space of this matrix: $\\mathcal{M}(q^{n+1}) \\approx U_M C_M^{n+1}$, where $C_M^{n+1} \\in \\mathbb{R}^{m_M \\times r}$. Applying the same DEIM logic, we enforce equality on sampled rows:\n    $$P_M^T \\mathcal{M}(q^{n+1}) = P_M^T U_M C_M^{n+1} \\implies C_M^{n+1} = (P_M^T U_M)^{-1} P_M^T \\mathcal{M}(q^{n+1})$$\n    The approximation for $\\mathcal{M}(q^{n+1})$ is:\n    $$\n    M(Vq^{n+1})V \\approx U_M (P_M^T U_M)^{-1} P_M^T M(Vq^{n+1})V\n    $$\n    This approximation can be used for all terms involving the matrix product $M(Vq^{n+1})V$. The full term from the BDF discretization can be compactly written before substitution:\n    $$\n    \\frac{1}{\\Delta t} W^{T}M(Vq^{n+1})V \\left(\\sum_{j=0}^{k} \\alpha_{j}\\,q^{n+1-j}\\right)\n    $$\n    Applying the hyperreduction approximation yields:\n    $$\n    \\approx \\frac{1}{\\Delta t} W^{T} \\left( U_M (P_M^T U_M)^{-1} P_M^T M(Vq^{n+1})V \\right) \\left(\\sum_{j=0}^{k} \\alpha_{j}\\,q^{n+1-j}\\right)\n    $$\n    The online cost here is in evaluating the $m_M \\times r$ matrix $P_M^T M(Vq^{n+1})V$.\n\n**Final Expression for the Hyperreduced ROM Residual**\nCombining the approximated terms with the exact linear and forcing terms, the hyperreduced residual $r_H^{n+1}(q^{n+1})$ is:\n$$\nr_H^{n+1}(q^{n+1}) = \\frac{1}{\\Delta t} W^T U_M (P_M^T U_M)^{-1} \\left(P_M^T M(Vq^{n+1})V\\right) \\left(\\sum_{j=0}^{k} \\alpha_{j} q^{n+1-j}\\right) - W^T U_g (P_g^T U_g)^{-1} P_g^T f(Vq^{n+1}) - W^T A V q^{n+1} - W^T b^{n+1}\n$$\nThis expression represents the final hyperreduced residual to be solved for $q^{n+1}$ at each time step.",
            "answer": "$$\n\\boxed{\\frac{1}{\\Delta t} W^T U_M (P_M^T U_M)^{-1} \\left(P_M^T M(Vq^{n+1})V\\right) \\left(\\sum_{j=0}^{k} \\alpha_{j} q^{n+1-j}\\right) - W^T U_g (P_g^T U_g)^{-1} P_g^T f(Vq^{n+1}) - W^T A V q^{n+1} - W^T b^{n+1}}\n$$"
        },
        {
            "introduction": "A theoretically sound ROM can still fail in practice if it is not constructed with numerical robustness in mind. Real-world snapshot data is often imperfect, containing near-redundancies or coupling physics with vastly different scales, which can lead to ill-conditioned and unstable models. This final practice shifts our focus from mechanics to the art of building reliable ROMs by addressing these critical challenges . You will analyze and select a comprehensive strategy that combines proper data scaling, rank-revealing techniques to handle collinear snapshots, and robust point-selection for hyperreduction, ensuring the final model is not just fast, but also physically meaningful and numerically stable.",
            "id": "3524023",
            "problem": "Consider a coupled multiphysics system with two primary fields, a velocity-like variable $u(\\mathbf{x},t)$ and a temperature-like variable $T(\\mathbf{x},t)$, governed by partial differential equations with nonlinear coupling. Assume you have collected $m$ snapshots in time and formed a block snapshot matrix $X \\in \\mathbb{R}^{n \\times m}$ by concatenating the state vectors for the two physics after applying unknown scaling weights, so that $X = \\left[W_u U_s \\quad W_T T_s\\right]$, where $U_s \\in \\mathbb{R}^{n_u \\times m}$ and $T_s \\in \\mathbb{R}^{n_T \\times m}$ are the raw snapshot blocks, $W_u \\in \\mathbb{R}^{n_u \\times n_u}$ and $W_T \\in \\mathbb{R}^{n_T \\times n_T}$ are diagonal scaling matrices, and $n = n_u + n_T$. You aim to construct a Proper Orthogonal Decomposition (POD) basis for a projection-based Reduced Order Model (ROM), and to build a hyperreduction operator for a nonlinear residual $f(u,T)$ via a sampling-based approach (e.g., an empirical interpolation method).\n\nFundamental base for reasoning:\n- The Singular Value Decomposition (SVD) of $X$ is $X = U \\Sigma V^\\top$, with $U \\in \\mathbb{R}^{n \\times r}$ column-orthonormal, $V \\in \\mathbb{R}^{m \\times r}$ column-orthonormal, and $\\Sigma = \\operatorname{diag}(\\sigma_1,\\dots,\\sigma_r)$ with $\\sigma_1 \\ge \\dots \\ge \\sigma_r \\ge 0$, where $r = \\operatorname{rank}(X)$.\n- The POD subspace is spanned by columns of $U$ associated with the largest singular values under the chosen inner product, and truncation determines the numerical rank as a function of the singular value spectrum and machine precision.\n- Near collinearity of snapshots (e.g., columns $x_i$ and $x_j$ satisfying $x_j \\approx \\alpha x_i + \\delta$ with small $\\|\\delta\\|_2$) leads to small singular values and an ill-conditioned Gramian $G = X^\\top X$ with large condition number $\\kappa(G)$.\n- Column-pivoted QR decompositions and rank-revealing variants can identify a well-conditioned subset of columns; Tikhonov regularization modifies $G$ to $G_\\lambda = X^\\top X + \\lambda I$ to damp the influence of nearly collinear directions.\n- Hyperreduction point selection based on interpolation of a nonlinear term typically relies on a basis $U_f$ for $f(u,T)$ and a pivoting strategy that avoids selecting redundant or nearly dependent sample points.\n\nYou discover that your snapshot set contains many nearly collinear columns due to quasi-steady phases and repeated states, leading to a numerically rank-deficient $X$ with a cluster of tiny singular values near machine precision. You also observe that the two physics have different magnitudes and units so that unscaled concatenation can bias the POD basis toward one field. Finally, in the hyperreduction, a naive selection of sample points tends to pick multiple neighboring points that are nearly redundant, causing instability in the interpolatory projection.\n\nUsing only the fundamental base above, reason about how rank-deficiency and near collinearity manifest in the SVD and QR factorizations, how scaling affects the POD energy distribution across the coupled fields, and how pivoting and regularization impact the conditioning of interpolation operators. Then, pick the strategy that best avoids spurious POD modes, preserves physical balance across the coupled fields, and yields a stable hyperreduction operator in the presence of near collinearity.\n\nWhich option is the most robust and scientifically justified?\n\nA. Normalize each physics block by physically meaningful weights (e.g., inverse root-mean-square or energy-based $W_u$ and $W_T$) to balance magnitudes, compute an economy SVD of $X$, and truncate all singular values $\\sigma_k$ below a relative threshold proportional to machine precision, e.g., discard those with $\\sigma_k / \\sigma_1 \\le \\tau$ where $\\tau$ is chosen on the order of $\\varepsilon_{\\text{mach}}^{1/2}$ to account for accumulated roundoff and problem conditioning. If the singular spectrum exhibits a flat tail, cross-validate truncation by inspecting the conditioning of the projected Gramian. In addition, apply a strong rank-revealing QR with column pivoting to $X$ (or to $U$) to expose numerical rank and eliminate nearly collinear snapshots before computing the final POD basis. For hyperreduction, build $U_f$ from scaled nonlinear snapshots, then use a QR-based Discrete Empirical Interpolation Method with column pivoting (Q-DEIM) augmented by oversampling and leverage-score weighting to avoid selecting redundant points. If the interpolation operator is ill-conditioned, regularize via a small Tikhonov parameter $\\lambda$ selected by generalized cross-validation for $G_\\lambda$.\n\nB. Keep all snapshots without any scaling to avoid loss of information, compute an unpivoted QR of $X$ to obtain a basis, and perform hyperreduction by uniformly random sampling of points without any pivoting or oversampling. Rely on the law of large numbers to average out collinearity and spurious modes.\n\nC. Scale each physics block to unit variance, compute an SVD, and retain enough modes to capture $99.9\\%$ of cumulative energy without considering machine precision or condition number. For hyperreduction, use a standard Discrete Empirical Interpolation Method (DEIM) without pivoting, because the energy criterion already ensures robustness.\n\nD. Whiten the snapshots by pre-multiplying by $G^{-1/2}$ where $G = X^\\top X$, compute principal components, retain all components to avoid truncation, and select hyperreduction points by sampling columns of $V$ according to their leverage scores, without any QR pivoting or regularization, since whitening removes collinearity effects.\n\nE. Apply column-pivoted QR directly to $X$ to discard nearly collinear columns, compute the POD on the reduced set without additional scaling, and construct hyperreduction using strong rank-revealing QR on the nonlinear basis $U_f$ with a tight tolerance. Regularize the Gramian by adding a fixed small ridge parameter $\\lambda$ independent of problem conditioning; this provides stability without needing energy-based scaling.",
            "solution": "The problem requires an evaluation of strategies for constructing a projection-based Reduced Order Model (ROM) for a coupled multiphysics system, specifically addressing challenges related to data scaling, numerical rank deficiency in snapshots, and instability in hyperreduction. I will analyze the problem based on the provided fundamental principles.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **System**: A coupled multiphysics system with two fields, $u(\\mathbf{x},t)$ and $T(\\mathbf{x},t)$.\n-   **Snapshot Data**: $m$ snapshots are collected.\n-   **Snapshot Matrix**: A block matrix $X \\in \\mathbb{R}^{n \\times m}$ is formed as $X = \\left[W_u U_s \\quad W_T T_s\\right]$, where $U_s \\in \\mathbb{R}^{n_u \\times m}$ and $T_s \\in \\mathbb{R}^{n_T \\times m}$ are raw snapshots, $W_u \\in \\mathbb{R}^{n_u \\times n_u}$ and $W_T \\in \\mathbb{R}^{n_T \\times n_T}$ are diagonal scaling matrices, and $n = n_u + n_T$.\n-   **Goal**: Construct a Proper Orthogonal Decomposition (POD) basis and a hyperreduction operator for a nonlinear residual $f(u,T)$ using a sampling-based method.\n-   **Provided Principles**: The reasoning must be based on Singular Value Decomposition (SVD) properties, the definition of the POD subspace, the consequences of near collinearity (ill-conditioning of the Gramian $G = X^\\top X$), the use of column-pivoted QR for rank-revelation, Tikhonov regularization, and a pivoting-based point selection for hyperreduction.\n-   **Observed Issues**:\n    1.  The snapshot set contains nearly collinear columns, causing numerical rank deficiency and a cluster of small singular values.\n    2.  The two physics fields, $u$ and $T$, have different magnitudes and units.\n    3.  Naive hyperreduction point selection leads to redundancy and instability.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically sound and well-posed. It describes a classic and realistic set of challenges encountered in model order reduction for complex engineering systems.\n-   **Scientific Grounding**: The concepts of POD, SVD, hyperreduction (DEIM-family methods), snapshot scaling, numerical rank, and regularization are all standard and fundamental in computational science and engineering. The described issues are well-documented phenomena.\n-   **Well-Posedness**: The question asks for the \"most robust and scientifically justified\" strategy, which can be objectively determined by comparing the proposed methods against established principles of numerical stability and physical fidelity.\n-   **Objectivity**: The language is technical and precise.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-formulated question about numerical best practices in a specific scientific computing context. I will proceed with the derivation and evaluation.\n\n### Derivation of a Robust Strategy\n\nA robust strategy must address the three core issues identified: scaling, rank deficiency, and hyperreduction instability.\n\n1.  **Scaling for Coupled Physics**: The two fields $u$ and $T$ are physically different. To ensure the POD basis represents both physics fairly, their contributions to the total energy of the snapshot matrix, $\\|X\\|_F^2 = \\sum_{k=1}^r \\sigma_k^2$, must be balanced. Without a proper choice of the scaling matrices $W_u$ and $W_T$, the field with larger-magnitude snapshots will dominate the SVD, and the resulting POD modes will poorly represent the other field. A scientifically justified approach is to use energy-based weights, for instance, by setting the diagonal entries of $W_u$ and $W_T$ to the inverse of a characteristic measure of the corresponding degree of freedom, such as the root-mean-square value or the time-averaged energy. This ensures that the scaled blocks $\\|W_u U_s\\|_F$ and $\\|W_T T_s\\|_F$ are of comparable magnitude.\n\n2.  **Handling Rank Deficiency for POD**: The presence of nearly collinear snapshots manifests as a set of very small singular values $\\sigma_k$ that are close to machine precision $\\varepsilon_{\\text{mach}}$. Including the corresponding left singular vectors (POD modes) in the basis makes the projection ill-conditioned and sensitive to noise. A simple truncation based on cumulative energy (e.g., $99.9\\%$) is insufficient as it may retain these numerically problematic modes. A robust strategy must explicitly account for numerical rank. This can be done in two primary ways:\n    a.  **SVD Truncation**: Truncate singular values below a threshold relative to the largest one, i.e., discard modes for which $\\sigma_k / \\sigma_1 \\le \\tau$. The threshold $\\tau$ should be chosen based on machine precision and problem conditioning. A value on the order of $\\sqrt{\\varepsilon_{\\text{mach}}}$ is a common heuristic to guard against accumulated round-off error.\n    b.  **Snapshot Selection**: Use a rank-revealing algorithm, such as a column-pivoted QR decomposition on $X$ or $X^\\top$, to identify and remove the nearly linearly dependent snapshots *before* computing the SVD. This produces a well-conditioned subset of snapshots from which a stable POD basis can be computed.\n\n3.  **Stable Hyperreduction**: The goal of hyperreduction is to approximate the nonlinear term $f(u,T)$ by interpolating it at a small number of selected grid points. If these points are clustered or provide redundant information, the interpolation matrix becomes ill-conditioned, leading to instability. This is analogous to the collinearity issue with snapshots. To avoid this, a pivoting strategy is essential during point selection.\n    a.  The Discrete Empirical Interpolation Method (DEIM) uses a greedy algorithm that is equivalent to LU decomposition with pivoting. A more numerically robust variant is the QR-based DEIM (Q-DEIM), which uses column-pivoted QR on the basis $U_f$ of the nonlinear term snapshots.\n    b.  Advanced techniques like sampling based on leverage scores (which measure the \"importance\" of each grid point) and oversampling can further improve the stability and accuracy of the interpolation.\n    c.  If ill-conditioning remains, Tikhonov regularization can be applied to the interpolation problem. The regularization parameter $\\lambda$ should be chosen adaptively (e.g., via generalized cross-validation, GCV) rather than being a fixed constant, to be robust to changes in problem conditioning.\n\nIn summary, the most robust strategy involves: (1) appropriate physical scaling, (2) numerically-aware truncation of the POD basis via a precision-based threshold or rank-revealing QR, and (3) a stable, pivoting-based hyperreduction point selection, possibly enhanced with advanced sampling and adaptive regularization.\n\n### Option-by-Option Analysis\n\n**A. Normalize each physics block by physically meaningful weights... compute an economy SVD... truncate... below a relative threshold proportional to machine precision... apply a strong rank-revealing QR... For hyperreduction, build $U_f$ from scaled nonlinear snapshots, then use a QR-based Discrete Empirical Interpolation Method with column pivoting (Q-DEIM) augmented by oversampling and leverage-score weighting... regularize via a small Tikhonov parameter $\\lambda$ selected by generalized cross-validation...**\nThis option aligns perfectly with the derived robust strategy. It correctly addresses all three challenges:\n1.  It uses \"physically meaningful weights\" for scaling.\n2.  It employs both a precision-aware SVD truncation ($\\sigma_k/\\sigma_1 \\le \\tau \\sim \\varepsilon_{\\text{mach}}^{1/2}$) and a rank-revealing QR to manage rank deficiency.\n3.  It uses the most robust hyperreduction techniques: Q-DEIM, oversampling, leverage-score weighting, and adaptive Tikhonov regularization via GCV.\nThe mention of $G_\\lambda$ (which is for the Gramian) in the context of the interpolation operator is a minor ambiguity, but the principle of adaptive regularization is correct and the overall strategy is sound.\n**Verdict: Correct.**\n\n**B. Keep all snapshots without any scaling to obtain a basis... compute an unpivoted QR... uniformly random sampling... Rely on the law of large numbers...**\nThis option is fundamentally flawed.\n1.  Failure to scale will bias the POD basis toward the higher-magnitude field.\n2.  An unpivoted QR decomposition is not rank-revealing and is not robust to collinearity. Also, a QR basis is not the energy-optimal POD basis.\n3.  Uniform random sampling for hyperreduction is a naive approach that ignores the structure of the nonlinear function and provides no guarantee of a well-conditioned interpolation.\n4.  Relying on the \"law of large numbers\" to fix ill-conditioning in a deterministic numerical algorithm is scientifically incorrect.\n**Verdict: Incorrect.**\n\n**C. Scale each physics block to unit variance... retain enough modes to capture $99.9\\%$ of cumulative energy without considering machine precision or condition number... use a standard DEIM... because the energy criterion already ensures robustness.**\nThis option contains critical flaws.\n1.  Scaling to unit variance is a reasonable, though not always optimal, choice.\n2.  The primary flaw is using a pure energy-based truncation criterion ($99.9\\%$) while \"without considering machine precision or condition number\". This is precisely the naive approach that fails in the presence of a numerically rank-deficient snapshot matrix.\n3.  The claim that an energy criterion for the state POD basis \"ensures robustness\" for the hyperreduction of a separate nonlinear term is a non-sequitur and false. The stability of the two problems is not directly linked in this way.\n**Verdict: Incorrect.**\n\n**D. Whiten the snapshots by pre-multiplying by $G^{-1/2}$ where $G = X^\\top X$... retain all components... select hyperreduction points by sampling columns of $V$...**\nThis option is numerically catastrophic and conceptually confused.\n1.  The problem states that $X$ is numerically rank-deficient, which means the Gramian $G = X^\\top X$ is severely ill-conditioned. Attempting to compute $G^{-1/2}$ would amplify numerical errors, rendering the results meaningless. This \"whitening\" step is not numerically viable.\n2.  Retaining \"all components\" defeats the purpose of model reduction.\n3.  Sampling columns of $V$ (the right singular vectors, or temporal modes) to select hyperreduction points (which are spatial locations) is a categorical error.\n**Verdict: Incorrect.**\n\n**E. Apply column-pivoted QR directly to $X$ to discard nearly collinear columns, compute the POD on the reduced set without additional scaling... Regularize the Gramian by adding a fixed small ridge parameter $\\lambda$...**\nThis option mixes good ideas with significant errors.\n1.  Using column-pivoted QR to prune snapshots is a good technique. Using it on $U_f$ for hyperreduction is also good.\n2.  The failure to perform \"additional scaling\" (i.e., any scaling) is a major flaw for a coupled multiphysics problem, as it leaves the basis biased. The pruning of columns by QR will also be biased by the unscaled magnitudes.\n3.  Using a \"fixed small ridge parameter $\\lambda$\" for regularization is not a robust strategy. An optimal $\\lambda$ is problem-dependent, and a robust method should determine it adaptively.\n**Verdict: Incorrect.**\n\nBased on the analysis, Option A presents the most complete and correct collection of modern, robust numerical techniques to address the specific challenges outlined in the problem statement.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}