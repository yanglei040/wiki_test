{
    "hands_on_practices": [
        {
            "introduction": "To truly understand an algorithm, it's often best to perform a calculation by hand. This first practice guides you through a single iteration of the Parareal algorithm for a simple logistic growth model (). By applying the coarse predictor and fine corrector steps manually, you will develop a concrete intuition for how the algorithm refines its solution in parallel.",
            "id": "2158974",
            "problem": "The parareal algorithm is a parallel-in-time integration method designed to accelerate the solution of an Initial Value Problem (IVP). The algorithm partitions the total time interval $[T_0, T_{final}]$ into $N$ smaller sub-intervals $[T_n, T_{n+1}]$ where $T_n = T_0 + n \\Delta T$ and $\\Delta T = (T_{final}-T_0)/N$. It then uses two numerical solvers: a computationally inexpensive but inaccurate coarse solver, $\\mathcal{G}$, and a computationally expensive but accurate fine solver, $\\mathcal{F}$.\n\nThe core of the parareal method is an iterative correction scheme. Starting with an initial guess for the solution at all time points $T_n$ ( denoted $y_n^0$), each iteration $k$ refines the solution values from $y_n^k$ to $y_n^{k+1}$. The update for the solution at the end of the $n$-th sub-interval, $y_{n+1}^{k+1}$, is given by the formula:\n$$y_{n+1}^{k+1} = \\mathcal{G}(T_{n+1}, T_n, y_n^{k+1}) + \\left[ \\mathcal{F}(T_{n+1}, T_n, y_n^k) - \\mathcal{G}(T_{n+1}, T_n, y_n^k) \\right]$$\nHere, $\\mathcal{S}(T_b, T_a, y_{init})$ denotes the result of applying solver $\\mathcal{S}$ over the time interval $[T_a, T_b]$ with initial condition $y(T_a) = y_{init}$. The term in brackets is the correction term. Note that the coarse solver $\\mathcal{G}$ on the right-hand side is propagated from the newly computed value $y_n^{k+1}$. The initial guess for the entire trajectory at iteration $k=0$ is generated by a sequential application of the coarse solver: $y_{n+1}^0 = \\mathcal{G}(T_{n+1}, T_n, y_n^0)$, starting with the true initial condition $y_0^0 = y_0$.\n\nConsider the logistic growth IVP given by the ordinary differential equation:\n$$y'(t) = \\rho y(t) \\left(1 - \\frac{y(t)}{K}\\right)$$\nwith parameters $\\rho = 0.5$, $K = 100$, and initial condition $y(0) = 10$. We want to solve this IVP over the time interval $[0, 2]$.\n\nThe time interval is partitioned into $N=2$ sub-intervals: $[0, 1]$ and $[1, 2]$. Thus, $T_0=0$, $T_1=1$, and $T_2=2$, with a coarse time step of $\\Delta T = 1$.\n\nThe coarse solver $\\mathcal{G}$ is the Forward Euler method, which takes a single step of size $\\Delta T$ across each sub-interval:\n$$\\mathcal{G}(T_{n+1}, T_n, y_n) = y_n + \\Delta T \\cdot f(T_n, y_n)$$\nwhere $f(t,y) = y'(t)$.\n\nThe fine solver $\\mathcal{F}$ is the classical 4th-order Runge-Kutta (RK4) method. To cross one sub-interval of width $\\Delta T=1$, it takes two steps of size $\\delta t = 0.5$. The update for a single RK4 step from time $t_i$ to $t_{i+1} = t_i + \\delta t$ with value $y_i$ is given by:\n$$y_{i+1} = y_i + \\frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4)$$\nwhere\n$k_1 = \\delta t \\cdot f(t_i, y_i)$\n$k_2 = \\delta t \\cdot f(t_i + \\frac{\\delta t}{2}, y_i + \\frac{k_1}{2})$\n$k_3 = \\delta t \\cdot f(t_i + \\frac{\\delta t}{2}, y_i + \\frac{k_2}{2})$\n$k_4 = \\delta t \\cdot f(t_i + \\delta t, y_i + k_3)$\n\nYour task is to calculate the value of the numerical solution at $t=T_2=2$ after the first parareal correction, i.e., find the value of $y_2^1$. Round your final answer to four significant figures.",
            "solution": "We are solving the IVP $y'(t)=\\rho y(t)\\left(1-\\frac{y(t)}{K}\\right)$ with $\\rho=0.5$, $K=100$, $y(0)=10$ over $[0,2]$, partitioned into $N=2$ sub-intervals with $\\Delta T=1$. Define $f(t,y)=\\rho y\\left(1-\\frac{y}{K}\\right)=0.5\\,y-0.005\\,y^{2}$. The parareal update is\n$$\ny_{n+1}^{k+1}=\\mathcal{G}(T_{n+1},T_{n},y_{n}^{k+1})+\\left[\\mathcal{F}(T_{n+1},T_{n},y_{n}^{k})-\\mathcal{G}(T_{n+1},T_{n},y_{n}^{k})\\right],\n$$\nwith $\\mathcal{G}$ the Forward Euler step of size $\\Delta T=1$:\n$$\n\\mathcal{G}(T_{n+1},T_{n},y_{n})=y_{n}+f(T_{n},y_{n}),\n$$\nand $\\mathcal{F}$ the RK4 method using two steps of size $\\delta t=0.5$ on each sub-interval. One RK4 step from time $t_{i}$ to $t_{i+1}=t_{i}+\\delta t$ with value $y_{i}$ is\n$$\n\\begin{aligned}\nk_{1}&=\\delta t\\,f(t_{i},y_{i}),\\\\\nk_{2}&=\\delta t\\,f\\!\\left(t_{i}+\\tfrac{\\delta t}{2},\\,y_{i}+\\tfrac{k_{1}}{2}\\right),\\\\\nk_{3}&=\\delta t\\,f\\!\\left(t_{i}+\\tfrac{\\delta t}{2},\\,y_{i}+\\tfrac{k_{2}}{2}\\right),\\\\\nk_{4}&=\\delta t\\,f\\!\\left(t_{i}+\\delta t,\\,y_{i}+k_{3}\\right),\\\\\ny_{i+1}&=y_{i}+\\tfrac{1}{6}\\left(k_{1}+2k_{2}+2k_{3}+k_{4}\\right).\n\\end{aligned}\n$$\n\nStep 1: Initial coarse trajectory ($k=0$). With $y_{0}^{0}=10$ and $\\Delta T=1$,\n$$\ny_{1}^{0}=\\mathcal{G}(1,0,10)=10+f(0,10)=10+\\left(0.5\\cdot 10-0.005\\cdot 10^{2}\\right)=10+4.5=14.5,\n$$\n$$\n\\begin{aligned}\nf(1,14.5)&=0.5\\cdot 14.5-0.005\\cdot(14.5)^{2}=7.25-1.05125=6.19875,\\\\\ny_{2}^{0}&=\\mathcal{G}(2,1,14.5)=14.5+6.19875=20.69875.\n\\end{aligned}\n$$\n\nStep 2: First parareal correction at $n=0$ gives $y_{1}^{1}=\\mathcal{G}(1,0,y_{0}^{1})+\\left[\\mathcal{F}(1,0,y_{0}^{0})-\\mathcal{G}(1,0,y_{0}^{0})\\right]=\\mathcal{F}(1,0,10)$, so we compute $\\mathcal{F}(1,0,10)$ by RK4 with two half-steps.\n\nSub-interval $[0,0.5]$, start $y=10$:\n$$\n\\delta t=0.5,\\quad k_{1}=0.5\\,f(0,10)=0.5\\cdot 4.5=2.25,\n$$\n$$\nk_{2}=0.5\\,f\\!\\left(0.25,\\,10+\\tfrac{2.25}{2}\\right)=0.5\\,f(11.125)=0.5\\cdot 4.943671875=2.4718359375,\n$$\n$$\nk_{3}=0.5\\,f\\!\\left(0.25,\\,10+\\tfrac{2.4718359375}{2}\\right)=0.5\\,f(11.23591796875)\\approx 2.49336486068625,\n$$\n$$\nk_{4}=0.5\\,f\\!\\left(0.5,\\,10+2.49336486068625\\right)=0.5\\,f(12.49336486068625)\\approx 2.73313080137868,\n$$\n$$\ny(0.5)=10+\\tfrac{1}{6}\\left(2.25+2\\cdot 2.4718359375+2\\cdot 2.49336486068625+2.73313080137868\\right)\\approx 12.4855887329585.\n$$\n\nSub-interval $[0.5,1]$, start $y\\approx 12.4855887329585$:\n$$\nk_{1}=0.5\\,f(12.4855887329585)\\approx 2.73167236817963,\\quad\nk_{2}=0.5\\,f(13.8514249170483)\\approx 2.98320129867894,\n$$\n$$\nk_{3}=0.5\\,f(13.9771893822979)\\approx 3.00589278881362,\\quad\nk_{4}=0.5\\,f(15.4914815217721)\\approx 3.27290538106303,\n$$\n$$\ny(1)=12.4855887329585+\\tfrac{1}{6}\\left(2.73167236817963+2\\cdot 2.98320129867894+2\\cdot 3.00589278881362+3.27290538106303\\right)\\approx 15.4827163869965.\n$$\nHence $y_{1}^{1}\\approx 15.4827163869965$.\n\nStep 3: Compute the bracket term on $[1,2]$ using $y_{1}^{0}=14.5$:\nWe need $\\mathcal{F}(2,1,14.5)$ and $\\mathcal{G}(2,1,14.5)=20.69875$.\n\nApply RK4 with two half-steps starting at $y=14.5$.\n\nSub-interval $[1,1.5]$, start $y=14.5$:\n$$\nk_{1}=0.5\\,f(14.5)=3.099375,\\quad\nk_{2}=0.5\\,f(16.0496875)\\approx 3.36844070288086,\n$$\n$$\nk_{3}=0.5\\,f(16.1842203514404)\\approx 3.39123261690612,\\quad\nk_{4}=0.5\\,f(17.8912326169061)\\approx 3.67256764286451,\n$$\n$$\ny(1.5)=14.5+\\tfrac{1}{6}\\left(3.099375+2\\cdot 3.36844070288086+2\\cdot 3.39123261690612+3.67256764286451\\right)\\approx 17.8818815470731.\n$$\n\nSub-interval $[1.5,2]$, start $y\\approx 17.8818815470731$:\n$$\nk_{1}=0.5\\,f(17.8818815470731)\\approx 3.67106616760706,\\quad\nk_{2}=0.5\\,f(19.7174146308766)\\approx 3.95741255839837,\n$$\n$$\nk_{3}=0.5\\,f(19.8605878262723)\\approx 3.97903958455471,\\quad\nk_{4}=0.5\\,f(21.8609211316278)\\approx 4.27048060109804,\n$$\n$$\ny(2)=17.8818815470731+\\tfrac{1}{6}\\left(3.67106616760706+2\\cdot 3.95741255839837+2\\cdot 3.97903958455471+4.27048060109804\\right)\\approx 21.8509567228416.\n$$\nThus $\\mathcal{F}(2,1,14.5)\\approx 21.8509567228416$ and the bracket term is\n$$\n\\mathcal{F}(2,1,14.5)-\\mathcal{G}(2,1,14.5)\\approx 21.8509567228416-20.69875=1.1522067228416.\n$$\n\nStep 4: First parareal correction at $n=1$:\nCompute $\\mathcal{G}(2,1,y_{1}^{1})=y_{1}^{1}+f(y_{1}^{1})$ with $y_{1}^{1}\\approx 15.4827163869965$,\n$$\nf(15.4827163869965)=0.5\\cdot 15.4827163869965-0.005\\cdot(15.4827163869965)^{2}\\approx 6.54278565989877,\n$$\n$$\n\\mathcal{G}(2,1,y_{1}^{1})\\approx 15.4827163869965+6.54278565989877=22.0255020468953.\n$$\nTherefore,\n$$\ny_{2}^{1}=\\mathcal{G}(2,1,y_{1}^{1})+\\left[\\mathcal{F}(2,1,y_{1}^{0})-\\mathcal{G}(2,1,y_{1}^{0})\\right]\\approx 22.0255020468953+1.1522067228416=23.1777087697369.\n$$\n\nRounded to four significant figures, $y_{2}^{1}\\approx 23.18$.",
            "answer": "$$\\boxed{23.18}$$"
        },
        {
            "introduction": "After seeing how Parareal works, the next critical question is *when* it converges. This exercise delves into the theoretical stability of the method by having you derive the error amplification factor for a linear test problem (). This analysis is fundamental to understanding how the interplay between the coarse and fine solvers dictates the algorithm's convergence behavior and overall effectiveness.",
            "id": "1126848",
            "problem": "The Parareal algorithm is a parallel-in-time integration method designed to solve initial value problems (IVPs) of the form $y'(t) = f(y(t), t)$. The time domain $[T_0, T_f]$ is partitioned into $N$ coarse time intervals $[T_n, T_{n+1}]$ of equal duration $\\Delta T = T_{n+1} - T_n$. The algorithm iteratively approximates the solution $U_n \\approx y(T_n)$ at the coarse time points.\n\nLet $\\mathcal{G}(T_{n+1}, T_n, V)$ denote a low-cost, low-accuracy **coarse propagator** that approximates the solution at $T_{n+1}$ starting from value $V$ at $T_n$. Similarly, let $\\mathcal{F}(T_{n+1}, T_n, V)$ be a high-cost, high-accuracy **fine propagator**. The Parareal iteration, indexed by $k$, updates the solution approximations $U_n^k$ according to the formula:\n$$\nU_{n+1}^{k+1} = \\mathcal{G}(T_{n+1}, T_n, U_n^{k+1}) + \\mathcal{F}(T_{n+1}, T_n, U_n^k) - \\mathcal{G}(T_{n+1}, T_n, U_n^k)\n$$\nThe iteration starts from an initial guess $U_n^0$ obtained by serially applying the coarse propagator: $U_{n+1}^0 = \\mathcal{G}(T_{n+1}, T_n, U_n^0)$.\n\nConsider the application of the Parareal algorithm to the linear test problem $y'(t) = \\lambda y(t)$, where $\\lambda$ is a complex constant. Let the coarse propagator $\\mathcal{G}$ be a single step of the **forward Euler method** over the coarse interval $\\Delta T$. Let the fine propagator $\\mathcal{F}$ be composed of $M$ steps of the **backward Euler method**, each with a fine time step of $\\Delta t = \\Delta T / M$.\n\nThe convergence of the algorithm can be analyzed by studying the evolution of the error with respect to the serially computed fine solution, $\\epsilon_n^k = U_n^k - U_n^*$. For the linear test problem, applying the z-transform in the time-slice index $n$ (i.e., $\\hat{\\epsilon}^k(z) = \\sum_{n=0}^{\\infty} \\epsilon_n^k z^{-n}$) reveals that the error transforms from one iteration to the next according to the relation $\\hat{\\epsilon}^{k+1}(z) = \\rho(z) \\hat{\\epsilon}^k(z)$. The function $\\rho(z)$ is the **error amplification factor**, and its magnitude governs the convergence of the Parareal iteration.\n\nDetermine the error amplification factor $\\rho(z)$ for this specific Parareal setup. Express your answer in terms of the z-transform variable $z$, the problem parameter $\\lambda$, the coarse time step $\\Delta T$, and the number of fine steps $M$.",
            "solution": "1. Coarse and fine propagators for $y'= \\lambda y$ over $\\Delta T$:\n   $g = \\mathcal{G} = 1 + \\lambda\\,\\Delta T,$\n   $f = \\mathcal{F} = \\Bigl(1 - \\lambda\\,\\tfrac{\\Delta T}{M}\\Bigr)^{-M}.$\n\n2. Parareal error iteration for $E_n^k = U_n^k - U_n^*$:\n   $E_{n+1}^{\\,k+1} = g\\,E_n^{\\,k+1} + (f - g)\\,E_n^k.$\n\n3. Taking the $z$-transform $\\hat E^k(z)=\\sum_{n\\ge0}E_n^k\\,z^{-n}$, and using $E_0^k=0$,\n   \n$$\n     \\hat E^{\\,k+1}(z)\n     = g\\,z^{-1}\\,\\hat E^{\\,k+1}(z)\n       + (f-g)\\,z^{-1}\\,\\hat E^k(z).\n   $$\n\n   Rearranging,\n   $(1 - g\\,z^{-1})\\,\\hat E^{\\,k+1}(z) = (f-g)\\,z^{-1}\\,\\hat E^k(z).$\n\n4. Thus the error amplification factor is\n   $\\rho(z) = \\frac{\\hat E^{\\,k+1}(z)}{\\hat E^k(z)}\n               = \\frac{(f-g)\\,z^{-1}}{1 - g\\,z^{-1}}\n               = \\frac{f-g}{\\,z - g\\,}.$\n\n5. Substituting $f$ and $g$,\n   $\\rho(z)\n     = \\frac{\\,(1 - \\frac{\\lambda\\,\\Delta T}{M})^{-M} - (1 + \\lambda\\,\\Delta T)\\,}\n            {\\,z - (1 + \\lambda\\,\\Delta T)\\,}.$",
            "answer": "$$\\boxed{\\frac{\\bigl(1 - \\tfrac{\\lambda\\Delta T}{M}\\bigr)^{-M} - \\bigl(1 + \\lambda\\Delta T\\bigr)}{z - \\bigl(1 + \\lambda\\Delta T\\bigr)}}$$"
        },
        {
            "introduction": "The primary motivation for parallel-in-time methods is to achieve a speedup over serial time-stepping. This practice challenges you to model the performance of the Parareal algorithm from first principles (). By deriving a rigorous upper bound on the achievable speedup, you will gain insight into the fundamental trade-offs between the cost of the propagators, the number of processors, and the inherent sequential nature of the iteration.",
            "id": "3329274",
            "problem": "You are modeling an unsteady computational fluid dynamics (CFD) simulation over a time interval subdivided into $N_t$ contiguous time slices. The Parareal algorithm uses a coarse propagator $G$ and a fine propagator $F$ to accelerate time integration by exploiting concurrency across time slices. The coarse propagator $G$ is a stable, low-accuracy time integrator, and the fine propagator $F$ is a high-accuracy time integrator that defines the target serial solution. Let the computational cost to advance one time slice by one application of $G$ be $t_G$ and of $F$ be $t_F$, with $t_F \\gg t_G$. Assume a homogeneous platform of $P=N_t$ identical processors dedicated to time-parallel execution, negligible communication and synchronization overheads, and perfect load balance.\n\n(a) Define the Parareal algorithm over $N_t$ slices with coarse propagator $G$ and fine propagator $F$, including the initialization and iteration steps that express the update of the iterates $\\{y_n^{k}\\}_{n=0}^{N_t}$ at iteration $k$.\n\n(b) Starting from first principles of parallel performance modeling, use the standard definitions of speedup $S$, work, and span on a parallel machine to derive a rigorous upper bound on the achievable speedup after $K$ Parareal iterations. Specifically, proceed from:\n- the serial fine-only execution time,\n- a lower bound on the Parareal parallel execution time based on work conservation with $P=N_t$ processors, and\n- the causality constraints induced by the algorithmâ€™s dependency structure,\nand then relax this bound as needed to show that the speedup $S$ satisfies\n$$\nS \\leq \\frac{N_t\\, t_F}{\\,t_G + \\frac{t_F}{K}\\,}.\n$$\n\nYour derivation must not assume any undocumented overlap beyond what is implied by work and dependency constraints, and must explicitly justify each inequality used to relax the bound. Express your final answer as a single analytic expression in terms of $N_t$, $t_F$, $t_G$, and $K$. No numerical evaluation is required, and no units are needed in the final expression.",
            "solution": "The problem asks for the definition of the Parareal algorithm and the derivation of an upper bound on its speedup under specific idealized assumptions.\n\n### Problem Validation\nThe problem statement is self-contained and scientifically valid. The Parareal algorithm, its components ($F$ and $G$ propagators), and the concepts of computational cost ($t_F$, $t_G$), speedup ($S$), work, and span are standard in the field of parallel-in-time integration methods and high-performance computing. The provided assumptions ($P=N_t$, negligible overheads, perfect load balancing) are common idealizations for theoretical performance analysis. The task is to perform a standard derivation in parallel algorithm analysis, which is well-posed and objective. The problem is therefore deemed valid.\n\n### (a) Definition of the Parareal Algorithm\n\nLet the problem be the time integration of a system of ordinary differential equations (ODEs) $u'(t) = f(u(t), t)$ over the interval $[T_0, T_{end}]$, with a given initial condition $u(T_0) = u_0$. The time interval is partitioned into $N_t$ contiguous subintervals, or time slices, $[T_{n-1}, T_n]$ for $n=1, \\dots, N_t$.\n\nThe Parareal algorithm employs two propagators to approximate the solution $u(T_n)$ at the end of each time slice:\n1.  A **fine propagator** $F$, which is computationally expensive (costing $t_F$ per slice) but high-fidelity. The serial solution is defined by successive applications of $F$: $y_n = F(y_{n-1})$.\n2.  A **coarse propagator** $G$, which is computationally inexpensive (costing $t_G$ per slice, with $t_G \\ll t_F$) but low-fidelity.\n\nLet $y_n^k$ be the approximation of the solution $u(T_n)$ at iteration $k$ of the Parareal algorithm. The algorithm proceeds as follows:\n\n**1. Initialization ($k=0$):**\nAn initial approximation across all time slices is computed sequentially using the coarse propagator. This provides a starting guess for the iterative correction process.\n$$\ny_0^0 = u_0\n$$\n$$\ny_n^0 = G(y_{n-1}^0) \\quad \\text{for } n = 1, 2, \\dots, N_t\n$$\nThis step is inherently sequential and performs $N_t$ coarse solves.\n\n**2. Iteration (for $k=0, 1, \\dots, K-1$):**\nFor each iteration $k$, the algorithm corrects the previous approximation $y_n^k$ to produce a new approximation $y_n^{k+1}$. This involves two main steps:\n\n*   **Step 2a (Parallel Fine Solve):** The expensive fine propagator $F$ is applied concurrently on all $N_t$ time slices. Processor $n$ computes the fine solution on slice $[T_{n-1}, T_n]$ starting from the solution of the previous iteration, $y_{n-1}^k$. Since there are $P=N_t$ processors, each processor is assigned one time slice, and all fine solves are executed in parallel.\n*   **Step 2b (Sequential Correction):** The results from the parallel fine solves are used to correct the next coarse propagation. The update formula for the state at the next iteration, $y_n^{k+1}$, is given by:\n    $$\n    y_n^{k+1} = G(y_{n-1}^{k+1}) + F(y_{n-1}^k) - G(y_{n-1}^k)\n    $$\n    This formula can be interpreted as propagating the solution with the coarse solver, $G(y_{n-1}^{k+1})$, and then adding a correction term, $F(y_{n-1}^k) - G(y_{n-1}^k)$. The correction is the difference between the more accurate fine solution on the previous slice (computed in Step 2a) and the coarse solution that was used to generate the $k$-th iterate. Note the dependence on $y_{n-1}^{k+1}$, which makes this correction step sequential across the time slices.\n\nAfter $K$ iterations, the final approximation is $\\{y_n^K\\}_{n=0}^{N_t}$.\n\n### (b) Derivation of the Speedup Upper Bound\n\nThe speedup $S$ is defined as the ratio of the serial execution time to the parallel execution time, $S = \\frac{T_{serial}}{T_{par}(K)}$. To derive an upper bound on $S$, we require a lower bound on the parallel execution time $T_{par}(K)$.\n\n**1. Serial Execution Time ($T_{serial}$):**\nThe baseline serial execution is defined by using only the high-fidelity fine propagator $F$ sequentially across all $N_t$ time slices. The total time is:\n$$\nT_{serial} = N_t t_F\n$$\n\n**2. Lower Bound on Parallel Execution Time ($T_{par}(K)$):**\nWe establish a lower bound on the parallel execution time by first calculating the total computational work required by the algorithm and then applying the principle of work conservation.\n\n*   **Total Work ($W$):** The total work is the sum of the computational costs of all tasks performed by the algorithm.\n    *   **Initialization:** This step involves $N_t$ sequential applications of the coarse propagator $G$. The work is $W_{init} = N_t t_G$.\n    *   **K Iterations:** Each of the $K$ iterations involves:\n        *   $N_t$ applications of the fine propagator $F$, one for each time slice. Total work for fine solves per iteration is $N_t t_F$.\n        *   $N_t$ applications of the coarse propagator $G$ in the sequential correction step. Total work for coarse solves per iteration is $N_t t_G$.\n    The work for $K$ iterations is $W_{iters} = K (N_t t_F + N_t t_G)$.\n    *   **Total Work:** The total work for the entire algorithm is the sum of initialization and iteration work.\n        $$\n        W = W_{init} + W_{iters} = N_t t_G + K N_t (t_F + t_G) = N_t (K t_F + (K+1) t_G)\n        $$\n\n*   **Lower Bound from Work Conservation:** A fundamental principle of parallel computing is that the parallel execution time $T_{par}$ on $P$ processors cannot be less than the total work $W$ divided by $P$. This yields a lower bound based on the average workload per processor. Given $P = N_t$:\n    $$\n    T_{par}(K) \\ge \\frac{W}{P} = \\frac{N_t (K t_F + (K+1) t_G)}{N_t} = K t_F + (K+1) t_G\n    $$\n    This bound is a direct consequence of the total work required by the algorithm and the number of available processors. The calculation of work implicitly accounts for the algorithm's causality structure, as it sums all necessary computational steps.\n\n**3. Relaxation of the Lower Bound:**\nThe problem asks for a specific form of the speedup bound, which may be looser than the one derived directly from our work conservation bound. We can obtain the desired form by relaxing the lower bound on $T_{par}(K)$. We have established:\n$$\nT_{par}(K) \\ge K t_F + (K+1) t_G\n$$\nWe now show that $K t_F + (K+1) t_G \\ge t_G + \\frac{t_F}{K}$, thereby establishing $t_G + \\frac{t_F}{K}$ as a valid (though weaker) lower bound for $T_{par}(K)$.\n\nLet's inspect the terms separately:\n*   For the term involving $t_F$: As the number of iterations $K$ must be at least $1$ for the algorithm to perform any correction, we have $K \\ge 1$. This implies $K^2 \\ge 1$, and therefore $K \\ge \\frac{1}{K}$. Multiplying by $t_F > 0$ gives $K t_F \\ge \\frac{t_F}{K}$.\n*   For the term involving $t_G$: Since $K \\ge 0$, we have $K+1 \\ge 1$. Multiplying by $t_G > 0$ gives $(K+1) t_G \\ge t_G$.\n\nCombining these two inequalities, we can construct a weaker lower bound for the right-hand side of our work-based inequality:\n$$\nK t_F + (K+1) t_G \\ge \\frac{t_F}{K} + t_G\n$$\nThus, we have successfully relaxed the lower bound on the parallel time:\n$$\nT_{par}(K) \\ge t_G + \\frac{t_F}{K}\n$$\n\n**4. Derivation of the Speedup Upper Bound:**\nUsing the definition of speedup, $S = \\frac{T_{serial}}{T_{par}(K)}$, and substituting the relaxed lower bound for $T_{par}(K)$ into the denominator, we obtain an upper bound for $S$:\n$$\nS = \\frac{N_t t_F}{T_{par}(K)} \\le \\frac{N_t t_F}{t_G + \\frac{t_F}{K}}\n$$\nThis is the desired expression for the upper bound on the achievable speedup.\n\nThe final expression for the upper bound on the speedup $S$ is:\n$$\nS \\leq \\frac{N_t\\, t_F}{\\,t_G + \\frac{t_F}{K}\\,}\n$$",
            "answer": "$$\n\\boxed{\\frac{N_t t_F}{t_G + \\frac{t_F}{K}}}\n$$"
        }
    ]
}