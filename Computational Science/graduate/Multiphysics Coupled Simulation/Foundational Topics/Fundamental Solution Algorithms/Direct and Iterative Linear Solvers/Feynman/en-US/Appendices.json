{
    "hands_on_practices": [
        {
            "introduction": "Many multiphysics problems, particularly those involving coupled thermal and mechanical effects, result in symmetric positive definite (SPD) linear systems. For such well-behaved systems, the Cholesky factorization represents a robust and highly efficient direct solution method. This exercise provides fundamental practice in applying this method by hand, building intuition for the process of factorization followed by forward and backward substitution. ",
            "id": "3503388",
            "problem": "A monolithic implicit time-integration of a two-field multiphysics model couples linear isotropic thermal conduction to linear elastic small-strain mechanics. Linearization of the weak form at a Newton step yields a symmetric positive definite (SPD) linear system for the incremental update, where the system matrix arises from the Hessian of the convex energy functional. In a reduced two-degree-of-freedom setting corresponding to a Schur-complement-condensed interface, the linear system is $A x = b$ with\n$$\nA = \\begin{bmatrix} 4  2 \\\\ 2  3 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 6 \\\\ 5 \\end{bmatrix}.\n$$\nStarting from the definition of symmetric positive definiteness and the existence of a Cholesky factorization for SPD matrices, factor $A$ as $A = L L^{\\top}$ with $L$ lower triangular having strictly positive diagonal entries, and use forward and backward substitution to solve for $x$. Express all intermediate quantities exactly in radicals where appropriate, and report the final solution vector $x$ explicitly. No rounding is required. Provide the final $x$ as a row vector.",
            "solution": "The problem is subjected to validation prior to any attempt at a solution.\n\n**Step 1: Extract Givens**\n- The context is a monolithic implicit time-integration of a two-field multiphysics model (thermal-mechanical).\n- The resulting linear system is symmetric positive definite (SPD).\n- The linear system is given by $A x = b$.\n- The system matrix is $A = \\begin{bmatrix} 4  2 \\\\ 2  3 \\end{bmatrix}$.\n- The right-hand side vector is $b = \\begin{bmatrix} 6 \\\\ 5 \\end{bmatrix}$.\n- The task is to factor $A$ as $A = L L^{\\top}$, where $L$ is a lower triangular matrix with strictly positive diagonal entries.\n- The solution method is specified as using forward and backward substitution with the Cholesky factors.\n- All intermediate and final quantities must be expressed exactly.\n- The final solution vector $x$ is to be reported as a row vector.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-posed and scientifically grounded.\n1.  **Scientific Soundness**: The problem describes a standard procedure in computational mechanics where the linearization of a variational problem based on a convex energy functional leads to a symmetric positive definite (SPD) linear system. We verify the SPD property of the given matrix $A$.\n    -   Symmetry: The matrix $A$ is symmetric since $A_{12} = A_{21} = 2$.\n    -   Positive Definiteness: We check the leading principal minors. The first minor is $\\det([4]) = 4  0$. The second minor is $\\det(A) = (4)(3) - (2)(2) = 12 - 4 = 8  0$. Since all leading principal minors are positive, the matrix $A$ is indeed positive definite.\n2.  **Well-Posedness**: Since $A$ is SPD, it is non-singular ($\\det(A) \\neq 0$), which guarantees that a unique solution $x$ to the system $A x = b$ exists. The Cholesky factorization for a real SPD matrix is unique, ensuring a well-defined solution path.\n3.  **Completeness**: All necessary components ($A$ and $b$) are provided. The instructions are clear and unambiguous.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be developed.\n\nThe problem requires solving the linear system $A x = b$ using Cholesky factorization. The given matrix $A$ is symmetric and positive definite, which guarantees the existence of a unique lower triangular matrix $L$ with strictly positive diagonal entries such that $A = L L^{\\top}$.\n\nThe system to solve is:\n$$\n\\begin{bmatrix} 4  2 \\\\ 2  3 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 5 \\end{bmatrix}\n$$\n\nFirst, we compute the Cholesky factorization $A = L L^{\\top}$. Let the lower triangular matrix $L$ be\n$$\nL = \\begin{bmatrix} L_{11}  0 \\\\ L_{21}  L_{22} \\end{bmatrix}\n$$\nwhere $L_{11}  0$ and $L_{22}  0$.\n\nThen $L L^{\\top}$ is:\n$$\nL L^{\\top} = \\begin{bmatrix} L_{11}  0 \\\\ L_{21}  L_{22} \\end{bmatrix} \\begin{bmatrix} L_{11}  L_{21} \\\\ 0  L_{22} \\end{bmatrix} = \\begin{bmatrix} L_{11}^2  L_{11}L_{21} \\\\ L_{21}L_{11}  L_{21}^2 + L_{22}^2 \\end{bmatrix}\n$$\nEquating the components of $L L^{\\top}$ with the components of $A$:\n$$\n\\begin{bmatrix} L_{11}^2  L_{11}L_{21} \\\\ L_{21}L_{11}  L_{21}^2 + L_{22}^2 \\end{bmatrix} = \\begin{bmatrix} 4  2 \\\\ 2  3 \\end{bmatrix}\n$$\nWe solve for the elements of $L$ sequentially:\n1.  From the $(1,1)$ element: $L_{11}^2 = 4$. Since $L_{11}$ must be positive, $L_{11} = 2$.\n2.  From the $(2,1)$ element: $L_{21}L_{11} = 2$. Substituting $L_{11}=2$, we get $2 L_{21} = 2$, which gives $L_{21} = 1$.\n3.  From the $(2,2)$ element: $L_{21}^2 + L_{22}^2 = 3$. Substituting $L_{21}=1$, we get $1^2 + L_{22}^2 = 3$, so $L_{22}^2 = 2$. Since $L_{22}$ must be positive, $L_{22} = \\sqrt{2}$.\n\nThus, the Cholesky factor $L$ is:\n$$\nL = \\begin{bmatrix} 2  0 \\\\ 1  \\sqrt{2} \\end{bmatrix}\n$$\nThe original system $A x = b$ is now rewritten as $L L^{\\top} x = b$. This is solved in two steps:\n1.  Solve $L y = b$ for an intermediate vector $y$ (forward substitution).\n2.  Solve $L^{\\top} x = y$ for the final solution vector $x$ (backward substitution).\n\n**Step 1: Forward Substitution**\nWe solve the system $L y = b$:\n$$\n\\begin{bmatrix} 2  0 \\\\ 1  \\sqrt{2} \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 5 \\end{bmatrix}\n$$\nFrom the first row:\n$$\n2 y_1 = 6 \\implies y_1 = 3\n$$\nFrom the second row:\n$$\ny_1 + \\sqrt{2} y_2 = 5\n$$\nSubstituting $y_1=3$:\n$$\n3 + \\sqrt{2} y_2 = 5 \\implies \\sqrt{2} y_2 = 2 \\implies y_2 = \\frac{2}{\\sqrt{2}} = \\sqrt{2}\n$$\nThe intermediate vector is $y = \\begin{bmatrix} 3 \\\\ \\sqrt{2} \\end{bmatrix}$.\n\n**Step 2: Backward Substitution**\nWe solve the system $L^{\\top} x = y$:\n$$\n\\begin{bmatrix} 2  1 \\\\ 0  \\sqrt{2} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ \\sqrt{2} \\end{bmatrix}\n$$\nFrom the second row (solving backwards from the last variable):\n$$\n\\sqrt{2} x_2 = \\sqrt{2} \\implies x_2 = 1\n$$\nFrom the first row:\n$$\n2 x_1 + x_2 = 3\n$$\nSubstituting $x_2=1$:\n$$\n2 x_1 + 1 = 3 \\implies 2 x_1 = 2 \\implies x_1 = 1\n$$\nThe solution vector is $x = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\nAs requested, the final solution is reported as a row vector.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While direct solvers are effective, iterative methods become essential for the large, sparse systems common in high-fidelity simulations. The Jacobi method is a foundational iterative scheme that illustrates the core concept of a fixed-point iteration. This practice focuses on the critical skill of analyzing an iterative method's convergence by calculating the spectral radius of its iteration matrix, which determines whether the scheme will succeed. ",
            "id": "3503366",
            "problem": "Consider a two-field linearized, steady multiphysics coupling between a diffusion-like process and a reaction-like process discretized over a single representative control volume, resulting in a $2 \\times 2$ algebraic system $A x = b$ with \n$$A = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}.$$\nAssume the system arises from a consistent finite-volume balance and that the matrix $A$ is symmetric positive definite (SPD), a property often satisfied for coupled elliptic operators after appropriate stabilization and consistent coupling. Starting solely from the core definition of the Jacobi method as a fixed-point iteration that decouples the diagonal part of $A$ from its off-diagonal remainder, and using the fundamental fact from linear fixed-point theory that a linear iteration $x^{k+1} = T x^{k} + c$ converges for any initial guess if and only if the spectral radius $\\rho(T)$ is strictly less than $1$, perform the following:\n\n1. Derive the Jacobi iteration matrix for this system by splitting $A$ into its diagonal and off-diagonal parts. \n2. Compute the spectral radius of the derived Jacobi iteration matrix by exact eigenanalysis.\n3. Based on the spectral radius, determine whether the Jacobi method converges for any initial guess.\n\nExpress your final answer as a single row matrix with six entries, containing the four entries of the Jacobi iteration matrix in row-major order, followed by the spectral radius, followed by an indicator variable for convergence, where $1$ denotes convergence and $0$ denotes non-convergence. No rounding is required; provide exact analytic values. The final answer must be unitless.",
            "solution": "The problem is well-posed, scientifically grounded in numerical linear algebra, and provides a complete and consistent setup for analysis. The matrix $A$ is confirmed to be symmetric positive definite as stated, thus the problem is valid. We proceed with the solution.\n\nThe problem asks us to analyze the convergence of the Jacobi iterative method for the linear system $A x = b$, where\n$$ A = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}, \\quad x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} $$\nThe Jacobi method is a fixed-point iteration derived by splitting the matrix $A$ into its diagonal part $D$, its strictly lower triangular part $L$, and its strictly upper triangular part $U$, such that $A = D + L + U$. The system $Ax=b$ can be rewritten as $(D + L + U)x = b$. The Jacobi iteration is defined by rearranging this equation to solve for the next iteration $x^{k+1}$ using the entries on the diagonal:\n$$ D x^{k+1} = -(L+U)x^k + b $$\nThis can be expressed in the standard form of a linear fixed-point iteration, $x^{k+1} = T_J x^k + c$, by isolating $x^{k+1}$:\n$$ x^{k+1} = -D^{-1}(L+U)x^k + D^{-1}b $$\nThe matrix $T_J = -D^{-1}(L+U)$ is the Jacobi iteration matrix. The convergence of the method for any initial guess $x^0$ is guaranteed if and only if the spectral radius of $T_J$, denoted by $\\rho(T_J)$, is strictly less than $1$. The spectral radius is the maximum absolute value of the eigenvalues of $T_J$.\n\n**1. Derive the Jacobi iteration matrix**\n\nFirst, we decompose the given matrix $A$ into its components $D$, $L$, and $U$.\n$$ A = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix} $$\nThe diagonal part $D$ is:\n$$ D = \\begin{bmatrix} 4  0 \\\\ 0  3 \\end{bmatrix} $$\nThe strictly lower and upper triangular parts are:\n$$ L = \\begin{bmatrix} 0  0 \\\\ 1  0 \\end{bmatrix}, \\quad U = \\begin{bmatrix} 0  1 \\\\ 0  0 \\end{bmatrix} $$\nThe sum of the off-diagonal parts is:\n$$ L+U = \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix} $$\nNext, we compute the inverse of the diagonal matrix $D$:\n$$ D^{-1} = \\begin{bmatrix} 4^{-1}  0 \\\\ 0  3^{-1} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{4}  0 \\\\ 0  \\frac{1}{3} \\end{bmatrix} $$\nNow we can compute the Jacobi iteration matrix $T_J = -D^{-1}(L+U)$:\n$$ T_J = - \\begin{bmatrix} \\frac{1}{4}  0 \\\\ 0  \\frac{1}{3} \\end{bmatrix} \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix} = - \\begin{bmatrix} (\\frac{1}{4})(0) + (0)(1)  (\\frac{1}{4})(1) + (0)(0) \\\\ (0)(0) + (\\frac{1}{3})(1)  (0)(1) + (\\frac{1}{3})(0) \\end{bmatrix} $$\n$$ T_J = - \\begin{bmatrix} 0  \\frac{1}{4} \\\\ \\frac{1}{3}  0 \\end{bmatrix} = \\begin{bmatrix} 0  -\\frac{1}{4} \\\\ -\\frac{1}{3}  0 \\end{bmatrix} $$\n\n**2. Compute the spectral radius of the Jacobi matrix**\n\nTo find the spectral radius $\\rho(T_J)$, we must find the eigenvalues of $T_J$ by solving the characteristic equation $\\det(T_J - \\lambda I) = 0$, where $I$ is the identity matrix.\n$$ \\det \\left( \\begin{bmatrix} 0  -\\frac{1}{4} \\\\ -\\frac{1}{3}  0 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} \\right) = 0 $$\n$$ \\det \\begin{bmatrix} -\\lambda  -\\frac{1}{4} \\\\ -\\frac{1}{3}  -\\lambda \\end{bmatrix} = 0 $$\nThe determinant is calculated as:\n$$ (-\\lambda)(-\\lambda) - \\left(-\\frac{1}{4}\\right)\\left(-\\frac{1}{3}\\right) = 0 $$\n$$ \\lambda^2 - \\frac{1}{12} = 0 $$\nSolving for $\\lambda$, we find the eigenvalues:\n$$ \\lambda^2 = \\frac{1}{12} $$\n$$ \\lambda = \\pm \\sqrt{\\frac{1}{12}} = \\pm \\frac{1}{\\sqrt{4 \\times 3}} = \\pm \\frac{1}{2\\sqrt{3}} $$\nTo rationalize the denominator, we multiply the numerator and denominator by $\\sqrt{3}$:\n$$ \\lambda = \\pm \\frac{\\sqrt{3}}{2\\sqrt{3}\\sqrt{3}} = \\pm \\frac{\\sqrt{3}}{6} $$\nThe two eigenvalues are $\\lambda_1 = \\frac{\\sqrt{3}}{6}$ and $\\lambda_2 = -\\frac{\\sqrt{3}}{6}$. The spectral radius $\\rho(T_J)$ is the maximum of the absolute values of these eigenvalues:\n$$ \\rho(T_J) = \\max\\left( \\left|\\frac{\\sqrt{3}}{6}\\right|, \\left|-\\frac{\\sqrt{3}}{6}\\right| \\right) = \\max\\left( \\frac{\\sqrt{3}}{6}, \\frac{\\sqrt{3}}{6} \\right) = \\frac{\\sqrt{3}}{6} $$\n\n**3. Determine convergence**\n\nThe Jacobi method converges for any initial guess if and only if $\\rho(T_J)  1$. We must check if $\\frac{\\sqrt{3}}{6}  1$.\nSince $1  3  4$, we have $\\sqrt{1}  \\sqrt{3}  \\sqrt{4}$, which means $1  \\sqrt{3}  2$.\nDividing by $6$, we get $\\frac{1}{6}  \\frac{\\sqrt{3}}{6}  \\frac{2}{6} = \\frac{1}{3}$.\nSince $\\frac{1}{3}  1$, it is clear that $\\frac{\\sqrt{3}}{6}  1$.\nAlternatively, we can square both sides of the inequality $\\frac{\\sqrt{3}}{6}  1$, as both sides are positive:\n$$ \\left(\\frac{\\sqrt{3}}{6}\\right)^2 = \\frac{3}{36} = \\frac{1}{12} $$\nSince $\\frac{1}{12}  1$, the original inequality holds.\nBecause the spectral radius $\\rho(T_J) = \\frac{\\sqrt{3}}{6}$ is strictly less than $1$, the Jacobi method is guaranteed to converge for any initial guess. The corresponding convergence indicator is $1$.\n\nThe final answer is a row matrix containing the four entries of $T_J$ ($T_{11}, T_{12}, T_{21}, T_{22}$), the spectral radius $\\rho(T_J)$, and the convergence indicator ($1$).\nThe entries are: $0$, $-\\frac{1}{4}$, $-\\frac{1}{3}$, $0$, $\\frac{\\sqrt{3}}{6}$, and $1$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  -\\frac{1}{4}  -\\frac{1}{3}  0  \\frac{\\sqrt{3}}{6}  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "To achieve rapid convergence, modern simulations rely on sophisticated iterative techniques like the Preconditioned Conjugate Gradient (PCG) method. PCG is an optimal Krylov subspace method that systematically minimizes an energy functional, with preconditioning used to dramatically improve its performance. This exercise guides you through the first principles of PCG, applying it to a preconditioned system to understand how search directions and step lengths are chosen to ensure fast and robust convergence. ",
            "id": "3503384",
            "problem": "A two-field coupled diffusionâ€“reaction model arising in a multiphysics simulation leads, after static condensation of interface unknowns, to a symmetric positive-definite Schur complement linear system of the form $A x = b$. In a reduced test problem that preserves the essential algebraic features of the coupled operator and the separate-physics scaling of a block-Jacobi preconditioner, consider\n$$\nA=\\begin{bmatrix}4  1 \\\\ 1  3\\end{bmatrix},\\quad b=\\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n$$\nand let the preconditioner be the diagonal scaling $M=\\mathrm{diag}(A)=\\mathrm{diag}(4,3)$, as is common when each physics field is scaled by its own stiffness. Starting from $x_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, apply two iterations of the Preconditioned Conjugate Gradient (PCG) method, where PCG (Preconditioned Conjugate Gradient) is to be constructed from first principles by viewing the solution as the minimizer of the quadratic energy functional $\\phi(x)=\\tfrac{1}{2}x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$ in successive affine Krylov subspaces, with search directions chosen to enforce $A$-conjugacy and steps chosen by line minimization in the $M^{-1}$-weighted framework. Your task is to:\n- derive the update relations you need from these principles (without quoting pre-packaged algorithmic formulas), and\n- carry out exactly two iterations starting from $x_{0}$.\n\nReport the Euclidean norms of the residuals $r_{k}=b-Ax_{k}$ for $k=0,1,2$ as exact values. The final answer must be the three residual norms $\\|r_{0}\\|_{2}$, $\\|r_{1}\\|_{2}$, and $\\|r_{2}\\|_{2}$, provided as exact expressions. Do not round.",
            "solution": "The problem is validated as well-posed, scientifically grounded, and free of any inconsistencies or ambiguities. The matrix $A = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}$ is symmetric and positive-definite, as its leading principal minors are $4  0$ and $\\det(A) = 4 \\times 3 - 1 \\times 1 = 11  0$. The preconditioner $M = \\mathrm{diag}(A) = \\begin{bmatrix} 4  0 \\\\ 0  3 \\end{bmatrix}$ is also symmetric and positive-definite. These conditions ensure that the Preconditioned Conjugate Gradient (PCG) method is applicable and guaranteed to converge.\n\nFirst, we derive the PCG algorithm from the specified first principles. The solution to the linear system $Ax=b$ is equivalent to the unique minimizer of the quadratic energy functional $\\phi(x) = \\frac{1}{2}x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$. The gradient of this functional is $\\nabla \\phi(x) = Ax - b$, which we define as the negative of the residual, $-r(x)$.\n\nThe PCG method is an iterative procedure that generates a sequence of approximations $x_k$ that minimize $\\phi(x)$ over expanding affine subspaces. The update at each step is given by $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is a search direction and $\\alpha_k$ is a step size.\n\nThe step size $\\alpha_k$ is chosen to minimize $\\phi(x_k + \\alpha_k p_k)$ along the direction $p_k$. This is a one-dimensional minimization problem (a line search). We set the derivative with respect to $\\alpha$ to zero:\n$$\n\\frac{d}{d\\alpha} \\phi(x_k + \\alpha p_k) \\Big|_{\\alpha=\\alpha_k} = p_k^{\\mathsf{T}} (A(x_k + \\alpha_k p_k) - b) = 0\n$$\n$$\np_k^{\\mathsf{T}} (Ax_k - b + \\alpha_k A p_k) = 0\n$$\n$$\np_k^{\\mathsf{T}} (-r_k + \\alpha_k A p_k) = 0\n$$\nSolving for $\\alpha_k$ yields the optimal step size:\n$$\n\\alpha_k = \\frac{p_k^{\\mathsf{T}} r_k}{p_k^{\\mathsf{T}} A p_k}\n$$\nThe search directions $\\{p_k\\}$ are chosen to be $A$-conjugate, meaning $p_i^{\\mathsf{T}} A p_j = 0$ for $i \\neq j$. This property guarantees that the minimization over the expanded search space at step $k$ does not spoil the minimization from previous steps.\n\nIn the preconditioned version of the algorithm, the search directions are constructed using the preconditioned residual, $z_k = M^{-1}r_k$. The initial search direction is $p_0 = z_0$. Subsequent directions are constructed by making the new preconditioned residual $z_{k+1}$ $A$-conjugate to the previous search direction $p_k$. This is achieved via a Gram-Schmidt-like process:\n$$\np_{k+1} = z_{k+1} + \\beta_{k+1} p_k\n$$\nWe enforce the $A$-conjugacy condition $p_{k+1}^{\\mathsf{T}} A p_k = 0$:\n$$\n(z_{k+1} + \\beta_{k+1} p_k)^{\\mathsf{T}} A p_k = 0 \\implies z_{k+1}^{\\mathsf{T}} A p_k + \\beta_{k+1} p_k^{\\mathsf{T}} A p_k = 0\n$$\nThis gives the coefficient $\\beta_{k+1} = -\\frac{z_{k+1}^{\\mathsf{T}} A p_k}{p_k^{\\mathsf{T}} A p_k}$.\n\nFor computational efficiency, these expressions for $\\alpha_k$ and $\\beta_k$ are simplified. The residuals have the property that $r_k^{\\mathsf{T}} z_j = 0$ for $j  k$. From $p_j = z_j + \\beta_j p_{j-1}$, it follows that $p_j$ is in the span of $\\{z_0, \\dots, z_j\\}$, and therefore $r_k^{\\mathsf{T}} p_j = 0$ for $j  k$.\nUsing this, the numerator of $\\alpha_k$ becomes:\n$p_k^{\\mathsf{T}} r_k = (z_k + \\beta_k p_{k-1})^{\\mathsf{T}} r_k = z_k^{\\mathsf{T}} r_k + \\beta_k p_{k-1}^{\\mathsf{T}} r_k = z_k^{\\mathsf{T}} r_k + 0 = r_k^{\\mathsf{T}} z_k$.\nSo, $\\alpha_k = \\frac{r_k^{\\mathsf{T}} z_k}{p_k^{\\mathsf{T}} A p_k}$.\n\nFor $\\beta_{k+1}$, we use the residual update formula $r_{k+1} = r_k - \\alpha_k A p_k$, which implies $A p_k = \\frac{1}{\\alpha_k}(r_k - r_{k+1})$.\nThe numerator of the expression for $\\beta_{k+1}$ becomes:\n$z_{k+1}^{\\mathsf{T}} A p_k = z_{k+1}^{\\mathsf{T}} \\frac{1}{\\alpha_k}(r_k - r_{k+1}) = \\frac{1}{\\alpha_k} (r_{k+1}^{\\mathsf{T}} M^{-1} r_k - r_{k+1}^{\\mathsf{T}} M^{-1} r_{k+1})$.\nThe $z_j$ are $M$-self-adjoint, and the residuals are $M^{-1}$-orthogonal, $r_{k+1}^{\\mathsf{T}} M^{-1} r_k =0$. So the numerator simplifies to $-\\frac{1}{\\alpha_k} r_{k+1}^{\\mathsf{T}} z_{k+1}$.\nThe denominator is $p_k^{\\mathsf{T}} A p_k = \\frac{r_k^{\\mathsf{T}} z_k}{\\alpha_k}$.\nThus, $\\beta_{k+1} = \\frac{-r_{k+1}^{\\mathsf{T}} z_{k+1} / \\alpha_k}{r_k^{\\mathsf{T}} z_k / \\alpha_k} = \\frac{r_{k+1}^{\\mathsf{T}} z_{k+1}}{r_k^{\\mathsf{T}} z_k}$.\n\nThe PCG algorithm to be implemented is:\n1. $r_0 = b - Ax_0$\n2. $z_0 = M^{-1}r_0$\n3. $p_0 = z_0$\n4. For $k=0, 1, \\dots$:\n   a. $\\alpha_k = \\frac{r_k^{\\mathsf{T}} z_k}{p_k^{\\mathsf{T}} A p_k}$\n   b. $x_{k+1} = x_k + \\alpha_k p_k$\n   c. $r_{k+1} = r_k - \\alpha_k A p_k$\n   d. $z_{k+1} = M^{-1} r_{k+1}$\n   e. $\\beta_{k+1} = \\frac{r_{k+1}^{\\mathsf{T}} z_{k+1}}{r_k^{\\mathsf{T}} z_k}$\n   f. $p_{k+1} = z_{k+1} + \\beta_{k+1} p_k$\n\nWe now execute two iterations with the given data:\n$A=\\begin{bmatrix}41\\\\13\\end{bmatrix}$, $b=\\begin{bmatrix}1\\\\1\\end{bmatrix}$, $M=\\begin{bmatrix}40\\\\03\\end{bmatrix}$, $M^{-1}=\\begin{bmatrix}1/40\\\\01/3\\end{bmatrix}$, $x_{0}=\\begin{bmatrix}0\\\\0\\end{bmatrix}$.\n\n**Iteration $k=0$**\n$r_0 = b - A x_0 = \\begin{bmatrix}1\\\\1\\end{bmatrix} - \\begin{bmatrix}0\\\\0\\end{bmatrix} = \\begin{bmatrix}1\\\\1\\end{bmatrix}$.\nThe Euclidean norm is $\\|r_0\\|_2 = \\sqrt{1^2+1^2} = \\sqrt{2}$.\n\n$z_0 = M^{-1} r_0 = \\begin{bmatrix}1/40\\\\01/3\\end{bmatrix} \\begin{bmatrix}1\\\\1\\end{bmatrix} = \\begin{bmatrix}1/4\\\\1/3\\end{bmatrix}$.\n$p_0 = z_0 = \\begin{bmatrix}1/4\\\\1/3\\end{bmatrix}$.\n\nNumerator for $\\alpha_0$: $r_0^{\\mathsf{T}} z_0 = \\begin{bmatrix}11\\end{bmatrix} \\begin{bmatrix}1/4\\\\1/3\\end{bmatrix} = \\frac{1}{4} + \\frac{1}{3} = \\frac{7}{12}$.\nDenominator for $\\alpha_0$:\n$A p_0 = \\begin{bmatrix}41\\\\13\\end{bmatrix} \\begin{bmatrix}1/4\\\\1/3\\end{bmatrix} = \\begin{bmatrix}1+1/3\\\\1/4+1\\end{bmatrix} = \\begin{bmatrix}4/3\\\\5/4\\end{bmatrix}$.\n$p_0^{\\mathsf{T}} A p_0 = \\begin{bmatrix}1/41/3\\end{bmatrix} \\begin{bmatrix}4/3\\\\5/4\\end{bmatrix} = \\frac{1}{4}\\frac{4}{3} + \\frac{1}{3}\\frac{5}{4} = \\frac{1}{3} + \\frac{5}{12} = \\frac{4+5}{12} = \\frac{9}{12} = \\frac{3}{4}$.\n$\\alpha_0 = \\frac{7/12}{3/4} = \\frac{7}{12} \\cdot \\frac{4}{3} = \\frac{7}{9}$.\n\n$x_1 = x_0 + \\alpha_0 p_0 = \\begin{bmatrix}0\\\\0\\end{bmatrix} + \\frac{7}{9}\\begin{bmatrix}1/4\\\\1/3\\end{bmatrix} = \\begin{bmatrix}7/36\\\\7/27\\end{bmatrix}$.\n$r_1 = r_0 - \\alpha_0 A p_0 = \\begin{bmatrix}1\\\\1\\end{bmatrix} - \\frac{7}{9}\\begin{bmatrix}4/3\\\\5/4\\end{bmatrix} = \\begin{bmatrix}1 - 28/27 \\\\ 1 - 35/36\\end{bmatrix} = \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix}$.\nThe Euclidean norm is $\\|r_1\\|_2 = \\sqrt{(-\\frac{1}{27})^2 + (\\frac{1}{36})^2} = \\sqrt{\\frac{1}{729} + \\frac{1}{1296}} = \\sqrt{\\frac{16+9}{11664}} = \\sqrt{\\frac{25}{11664}} = \\frac{5}{108}$.\n\n**Iteration $k=1$**\n$z_1 = M^{-1} r_1 = \\begin{bmatrix}1/40\\\\01/3\\end{bmatrix} \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix} = \\begin{bmatrix}-1/108\\\\1/108\\end{bmatrix}$.\n\nNumerator for $\\beta_1$: $r_1^{\\mathsf{T}} z_1 = \\begin{bmatrix}-1/271/36\\end{bmatrix} \\begin{bmatrix}-1/108\\\\1/108\\end{bmatrix} = \\frac{1}{27 \\cdot 108} + \\frac{1}{36 \\cdot 108} = \\frac{4+3}{108 \\cdot 108} = \\frac{7}{11664}$.\nDenominator for $\\beta_1$ is $r_0^{\\mathsf{T}} z_0 = 7/12$.\n$\\beta_1 = \\frac{r_1^{\\mathsf{T}} z_1}{r_0^{\\mathsf{T}} z_0} = \\frac{7/11664}{7/12} = \\frac{12}{11664} = \\frac{1}{972}$.\n\n$p_1 = z_1 + \\beta_1 p_0 = \\begin{bmatrix}-1/108\\\\1/108\\end{bmatrix} + \\frac{1}{972}\\begin{bmatrix}1/4\\\\1/3\\end{bmatrix} = \\begin{bmatrix}-1/108+1/3888\\\\1/108+1/2916\\end{bmatrix} = \\begin{bmatrix}(-36+1)/3888 \\\\ (27+1)/2916\\end{bmatrix} = \\begin{bmatrix}-35/3888\\\\28/2916\\end{bmatrix} = \\begin{bmatrix}-35/3888\\\\7/729\\end{bmatrix}$.\n\nNumerator for $\\alpha_1$: $r_1^{\\mathsf{T}} z_1 = 7/11664$.\nDenominator for $\\alpha_1$:\n$A p_1 = \\begin{bmatrix}41\\\\13\\end{bmatrix} \\begin{bmatrix}-35/3888\\\\7/729\\end{bmatrix} = \\begin{bmatrix}4(-\\frac{35}{3888}) + \\frac{7}{729} \\\\ -\\frac{35}{3888} + 3(\\frac{7}{729}) \\end{bmatrix}$. Using $3888 = \\frac{16}{3} \\cdot 729$:\n$A p_1 = \\begin{bmatrix} -\\frac{140}{3888} + \\frac{3 \\cdot 16/3}{3888} \\cdot 7 \\\\ -\\frac{35}{3888} + \\frac{21}{729} \\end{bmatrix} = \\begin{bmatrix} \\frac{-140+35 \\cdot 16/3}{3888}.. \\end{bmatrix}$ This is complex. Let's use simplified intermediate values.\n$p_1 = \\frac{7}{108}\\begin{bmatrix}-5/36\\\\4/27\\end{bmatrix}$. $A p_1 = \\frac{7}{108} \\begin{bmatrix}41\\\\13\\end{bmatrix} \\begin{bmatrix}-5/36\\\\4/27\\end{bmatrix} = \\frac{7}{108} \\begin{bmatrix}-20/36+4/27\\\\-5/36+12/27\\end{bmatrix} = \\frac{7}{108} \\begin{bmatrix}-5/9+4/27\\\\-5/36+4/9\\end{bmatrix} = \\frac{7}{108} \\begin{bmatrix}(-15+4)/27\\\\(-5+16)/36\\end{bmatrix} = \\frac{7}{108} \\begin{bmatrix}-11/27\\\\11/36\\end{bmatrix} = \\frac{77}{108} \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix}$.\n$p_1^{\\mathsf{T}} A p_1 = (\\frac{7}{108}\\begin{bmatrix}-5/364/27\\end{bmatrix}) (\\frac{77}{108}\\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix}) = \\frac{539}{11664} (\\frac{5}{36 \\cdot 27} + \\frac{4}{27 \\cdot 36}) = \\frac{539}{11664} \\frac{9}{972} = \\frac{539}{11664 \\cdot 108}$.\n$\\alpha_1 = \\frac{r_1^{\\mathsf{T}} z_1}{p_1^{\\mathsf{T}} A p_1} = \\frac{7/11664}{(539)/(11664 \\cdot 108)} = \\frac{7 \\cdot 108}{539} = \\frac{7 \\cdot 108}{7 \\cdot 77} = \\frac{108}{77}$.\n\n$r_2 = r_1 - \\alpha_1 A p_1 = \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix} - \\frac{108}{77} \\left( \\frac{77}{108} \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix} \\right) = \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix} - \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}$.\nThe Euclidean norm is $\\|r_2\\|_2 = \\sqrt{0^2+0^2}=0$. This is expected, as PCG converges in at most $n=2$ iterations for an $n \\times n$ system.\n\nThe Euclidean norms of the residuals for $k=0,1,2$ are:\n$\\|r_0\\|_2 = \\sqrt{2}$\n$\\|r_1\\|_2 = \\frac{5}{108}$\n$\\|r_2\\|_2 = 0$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{2}  \\frac{5}{108}  0 \\end{pmatrix}}\n$$"
        }
    ]
}