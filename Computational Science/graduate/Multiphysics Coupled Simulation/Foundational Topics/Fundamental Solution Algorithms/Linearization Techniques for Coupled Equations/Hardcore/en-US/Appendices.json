{
    "hands_on_practices": [
        {
            "introduction": "Before building a full-scale solver, it is essential to master the fundamental mechanics of the Newton-Raphson method for a coupled system. This exercise provides a concrete, step-by-step application of a single damped Newton iteration. By manually computing the Jacobian matrix, the residual vector, and the resulting update for a simple but representative nonlinear system, you will solidify your understanding of the 'atomic operation' that forms the core of Newton-based solvers. ",
            "id": "3512855",
            "problem": "Consider a nondimensionalized two-field residual vector for a coupled thermo-mechanical toy model, defined by\n$$\nR(U) \\equiv \\begin{bmatrix} R_{1}(T,u) \\\\ R_{2}(T,u) \\end{bmatrix}\n= \\begin{bmatrix} \\sin(T) + \\alpha\\, u \\\\ u^{3} - T \\end{bmatrix},\n$$\nwhere $U \\equiv \\begin{bmatrix} T \\\\ u \\end{bmatrix}$, the scalar parameter $\\alpha \\in \\mathbb{R}$ represents a coupling strength, and all quantities are nondimensional. Assume that $\\alpha \\neq -3$ to avoid a singular Jacobian at the state of interest. Let the current iterate be $U^{(k)} = \\begin{bmatrix} T^{(k)} \\\\ u^{(k)} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, and let the damping factor for the Newton update be $\\lambda \\in (0,1]$.\n\nStarting from the first-order Taylor linearization of the residual about $U^{(k)}$ and the definition of the Newton step for coupled systems, perform a single damped Newton iteration at $U^{(k)}$ to compute:\n- the Jacobian matrix $J(U^{(k)}) \\equiv \\dfrac{\\partial R}{\\partial U}\\big|_{U^{(k)}}$,\n- the Newton increment $\\Delta U$ solving $J(U^{(k)})\\,\\Delta U = -R(U^{(k)})$,\n- the damped update $U^{+} = U^{(k)} + \\lambda\\, \\Delta U$.\n\nReport your final answer as the row vector\n$$\n\\big(J_{11},\\, J_{12},\\, J_{21},\\, J_{22},\\, \\Delta T,\\, \\Delta u,\\, T^{+},\\, u^{+}\\big),\n$$\nwhere $J_{ij}$ are the entries of $J(U^{(k)})$, and $\\Delta T$, $\\Delta u$, $T^{+}$, $u^{+}$ are the components of $\\Delta U$ and $U^{+}$, respectively. No rounding is required. Express your answer symbolically in terms of $\\alpha$ and $\\lambda$.",
            "solution": "The problem is first validated to ensure it is self-contained, scientifically grounded, and well-posed.\n\n### Step 1: Extract Givens\n- **Residual Vector**: $R(U) \\equiv \\begin{bmatrix} R_{1}(T,u) \\\\ R_{2}(T,u) \\end{bmatrix} = \\begin{bmatrix} \\sin(T) + \\alpha\\, u \\\\ u^{3} - T \\end{bmatrix}$\n- **State Vector**: $U \\equiv \\begin{bmatrix} T \\\\ u \\end{bmatrix}$\n- **Coupling Parameter**: $\\alpha \\in \\mathbb{R}$\n- **Constraint**: $\\alpha \\neq -3$\n- **Current Iterate**: $U^{(k)} = \\begin{bmatrix} T^{(k)} \\\\ u^{(k)} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$\n- **Damping Factor**: $\\lambda \\in (0,1]$\n- **Task**: Compute the Jacobian $J(U^{(k)})$, the Newton increment $\\Delta U$, and the damped update $U^{+} = U^{(k)} + \\lambda\\,\\Delta U$.\n- **Required Output**: The row vector $\\big(J_{11},\\, J_{12},\\, J_{21},\\, J_{22},\\, \\Delta T,\\, \\Delta u,\\, T^{+},\\, u^{+}\\big)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in applying Newton's method to a system of nonlinear equations, a fundamental topic in numerical analysis and computational science.\n- **Scientific/Factual Soundness**: The problem is mathematically sound. It employs established concepts: vector functions, partial derivatives, Taylor series linearization, and Newton's method for root-finding.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary functions, constants, and initial values. The constraint $\\alpha \\neq -3$ is crucial as it ensures the Jacobian matrix is invertible at the given point, guaranteeing a unique solution for the Newton step.\n- **Objectivity**: The problem is stated using precise, objective mathematical language.\n- **Conclusion**: The problem is valid. There are no flaws.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution proceeds as follows.\n\nThe core of Newton's method for a system of equations is to solve the linear system $J(U^{(k)}) \\Delta U = -R(U^{(k)})$, where $J$ is the Jacobian matrix of the residual vector $R$.\n\n**1. Compute the Jacobian Matrix**\nThe Jacobian matrix $J(U)$ is the matrix of all first-order partial derivatives of the residual vector $R(U)$.\n$$\nJ(U) = \\frac{\\partial R}{\\partial U} = \\begin{bmatrix} \\frac{\\partial R_1}{\\partial T} & \\frac{\\partial R_1}{\\partial u} \\\\ \\frac{\\partial R_2}{\\partial T} & \\frac{\\partial R_2}{\\partial u} \\end{bmatrix}\n$$\nGiven $R_1(T,u) = \\sin(T) + \\alpha u$ and $R_2(T,u) = u^3 - T$, the partial derivatives are:\n- $\\frac{\\partial R_1}{\\partial T} = \\cos(T)$\n- $\\frac{\\partial R_1}{\\partial u} = \\alpha$\n- $\\frac{\\partial R_2}{\\partial T} = -1$\n- $\\frac{\\partial R_2}{\\partial u} = 3u^2$\n\nThus, the Jacobian matrix is:\n$$\nJ(T,u) = \\begin{bmatrix} \\cos(T) & \\alpha \\\\ -1 & 3u^2 \\end{bmatrix}\n$$\n\n**2. Evaluate the Jacobian at the Current Iterate $U^{(k)}$**\nWe evaluate $J(T,u)$ at the point $U^{(k)} = (T^{(k)}, u^{(k)}) = (0, 1)$.\n$$\nJ(U^{(k)}) = J(0,1) = \\begin{bmatrix} \\cos(0) & \\alpha \\\\ -1 & 3(1)^2 \\end{bmatrix} = \\begin{bmatrix} 1 & \\alpha \\\\ -1 & 3 \\end{bmatrix}\n$$\nThe components of this matrix are the first four elements of the final answer:\n- $J_{11} = 1$\n- $J_{12} = \\alpha$\n- $J_{21} = -1$\n- $J_{22} = 3$\n\n**3. Evaluate the Residual at the Current Iterate $U^{(k)}$**\nWe evaluate $R(U)$ at the point $U^{(k)} = (0, 1)$.\n$$\nR(U^{(k)}) = R(0,1) = \\begin{bmatrix} \\sin(0) + \\alpha(1) \\\\ (1)^3 - 0 \\end{bmatrix} = \\begin{bmatrix} \\alpha \\\\ 1 \\end{bmatrix}\n$$\n\n**4. Solve for the Newton Increment $\\Delta U$**\nThe linear system to solve is $J(U^{(k)})\\,\\Delta U = -R(U^{(k)})$, where $\\Delta U = \\begin{bmatrix} \\Delta T \\\\ \\Delta u \\end{bmatrix}$.\n$$\n\\begin{bmatrix} 1 & \\alpha \\\\ -1 & 3 \\end{bmatrix} \\begin{bmatrix} \\Delta T \\\\ \\Delta u \\end{bmatrix} = -\\begin{bmatrix} \\alpha \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -\\alpha \\\\ -1 \\end{bmatrix}\n$$\nTo solve this $2 \\times 2$ system, we compute the inverse of the Jacobian. The determinant of $J(U^{(k)})$ is:\n$$\n\\det(J(U^{(k)})) = (1)(3) - (\\alpha)(-1) = 3 + \\alpha\n$$\nThe condition $\\alpha \\neq -3$ ensures $\\det(J(U^{(k)})) \\neq 0$, so the matrix is invertible. The inverse is:\n$$\n[J(U^{(k)})]^{-1} = \\frac{1}{3+\\alpha} \\begin{bmatrix} 3 & -\\alpha \\\\ 1 & 1 \\end{bmatrix}\n$$\nNow, we find $\\Delta U$ by multiplying both sides of the linear system by the inverse Jacobian:\n$$\n\\Delta U = [J(U^{(k)})]^{-1} (-R(U^{(k)})) = \\frac{1}{3+\\alpha} \\begin{bmatrix} 3 & -\\alpha \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} -\\alpha \\\\ -1 \\end{bmatrix}\n$$\nPerforming the matrix-vector multiplication:\n$$\n\\begin{bmatrix} \\Delta T \\\\ \\Delta u \\end{bmatrix} = \\frac{1}{3+\\alpha} \\begin{bmatrix} (3)(-\\alpha) + (-\\alpha)(-1) \\\\ (1)(-\\alpha) + (1)(-1) \\end{bmatrix} = \\frac{1}{3+\\alpha} \\begin{bmatrix} -3\\alpha + \\alpha \\\\ -\\alpha - 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{-2\\alpha}{3+\\alpha} \\\\ \\frac{-(\\alpha+1)}{3+\\alpha} \\end{bmatrix}\n$$\nThe components of the Newton increment are:\n- $\\Delta T = \\frac{-2\\alpha}{3+\\alpha}$\n- $\\Delta u = -\\frac{\\alpha+1}{3+\\alpha}$\n\n**5. Compute the Damped Update $U^{+}$**\nThe new state $U^{+}$ is computed using the damped update rule: $U^{+} = U^{(k)} + \\lambda \\Delta U$.\n$$\nU^{+} = \\begin{bmatrix} T^{+} \\\\ u^{+} \\end{bmatrix} = \\begin{bmatrix} T^{(k)} \\\\ u^{(k)} \\end{bmatrix} + \\lambda \\begin{bmatrix} \\Delta T \\\\ \\Delta u \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} + \\lambda \\begin{bmatrix} \\frac{-2\\alpha}{3+\\alpha} \\\\ -\\frac{\\alpha+1}{3+\\alpha} \\end{bmatrix}\n$$\nThe components of the updated state are:\n- $T^{+} = 0 + \\lambda \\left( \\frac{-2\\alpha}{3+\\alpha} \\right) = \\frac{-2\\lambda\\alpha}{3+\\alpha}$\n- $u^{+} = 1 + \\lambda \\left( -\\frac{\\alpha+1}{3+\\alpha} \\right) = 1 - \\frac{\\lambda(\\alpha+1)}{3+\\alpha} = \\frac{3+\\alpha - \\lambda\\alpha - \\lambda}{3+\\alpha} = \\frac{3-\\lambda + \\alpha(1-\\lambda)}{3+\\alpha}$\n\n**6. Assemble the Final Answer**\nThe final answer is the row vector $\\big(J_{11}, J_{12}, J_{21}, J_{22}, \\Delta T, \\Delta u, T^{+}, u^{+}\\big)$. Collecting all computed components:\n- $J_{11} = 1$\n- $J_{12} = \\alpha$\n- $J_{21} = -1$\n- $J_{22} = 3$\n- $\\Delta T = \\frac{-2\\alpha}{3+\\alpha}$\n- $\\Delta u = -\\frac{\\alpha+1}{3+\\alpha}$\n- $T^{+} = \\frac{-2\\lambda\\alpha}{3+\\alpha}$\n- $u^{+} = \\frac{3-\\lambda + \\alpha(1-\\lambda)}{3+\\alpha}$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & \\alpha & -1 & 3 & \\frac{-2\\alpha}{3+\\alpha} & -\\frac{\\alpha+1}{3+\\alpha} & \\frac{-2\\lambda\\alpha}{3+\\alpha} & \\frac{3-\\lambda + \\alpha(1-\\lambda)}{3+\\alpha} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Moving from a single step to a complete solver, this practice guides you through implementing the Picard (or fixed-point) iteration method for a coupled system of partial differential equations. You will discretize a steady-state thermo-diffusive model using finite differences and build the iterative loop to solve the resulting nonlinear algebraic system. This problem illustrates an important alternative to Newton's method and provides hands-on experience in how linearization techniques are applied in a practical computational physics context, from the governing equations to the final code. ",
            "id": "3512832",
            "problem": "Consider a one-dimensional, steady-state, two-field multiphysics model on the domain $[0,1]$ with Dirichlet boundary conditions. The coupled system consists of a heat conduction equation with temperature-dependent conductivity and a diffusion-like equation with temperature-dependent mobility. Starting from divergence-form partial differential equations (PDEs), the governing equations in one spatial dimension $x$ are\n$$\n-\\frac{d}{dx}\\left(k(T)\\frac{dT}{dx}\\right) = q(u), \\quad \\frac{d}{dx}\\left(C(T)\\frac{du}{dx}\\right) = 0,\n$$\nwhere $T(x)$ is temperature, $u(x)$ is a second field (for instance, concentration or electric potential), $k(T)$ is a positive conductivity, $C(T)$ is a positive mobility, and $q(u)$ is a source term. These equations are to be linearized and solved by the Picard (Fixed-Point) method: for Picard iteration index $k=0,1,2,\\ldots$, compute $(T^{k+1},u^{k+1})$ by freezing the nonlinear coefficients and sources at the previous iterate $(T^k,u^k)$, namely\n$$\n-\\frac{d}{dx}\\left(k(T^k)\\frac{d T^{k+1}}{dx}\\right) = q(u^k), \\quad \\frac{d}{dx}\\left(C(T^k)\\frac{d u^{k+1}}{dx}\\right) = 0.\n$$\nThis produces, at each Picard iteration, two linear subproblems that are coupled only through the coefficients frozen at the previous iterate. Your task is to implement this Picard iteration in a robust and self-consistent way, discretizing the spatial operators with a second-order finite-difference method on a uniform grid, and solving the resulting tridiagonal linear systems at each iteration.\n\nFundamental base for design:\n- Conservation-laws-based divergence form is used to construct discrete fluxes across cell faces.\n- Finite differences approximate spatial derivatives with second-order central differences under smoothness assumptions and enforce Dirichlet boundary conditions by eliminating boundary unknowns.\n\nDiscretization requirements:\n- Use a uniform grid with $N$ interior nodes, spacing $h=1/(N+1)$, interior grid points $x_i = i h$, $i = 1,\\ldots,N$.\n- For variable coefficients, discretize the divergence form using face-centered arithmetic averages:\n$$\nk_{i+\\frac{1}{2}} = \\frac{1}{2}\\left(k(T_i^k) + k(T_{i+1}^k)\\right), \\quad C_{i+\\frac{1}{2}} = \\frac{1}{2}\\left(C(T_i^k) + C(T_{i+1}^k)\\right),\n$$\nwith boundary faces using boundary values for $T$ where needed.\n- The linear operator for each subproblem must be assembled in tridiagonal form consistent with the discrete divergence operator, and Dirichlet boundary conditions are imposed by shifting boundary contributions to the right-hand side.\n\nAlgorithmic requirements:\n- Implement two coupling sequences for the Picard iteration within each global iteration:\n  1. $T$-then-$u$ sequence: solve the $T$-subproblem first, then the $u$-subproblem.\n  2. $u$-then-$T$ sequence: solve the $u$-subproblem first, then the $T$-subproblem.\n- In both sequences, each subproblem uses coefficients and sources frozen at the previous iterate $(T^k,u^k)$.\n- Use initial guesses $T^0$ and $u^0$ as linear interpolants between their respective boundary values.\n- The stopping criterion is defined by the infinity norm of the update:\n$$\n\\max\\left(\\|T^{k+1}-T^k\\|_{\\infty}, \\|u^{k+1}-u^k\\|_{\\infty}\\right) < \\varepsilon,\n$$\nwith a maximum number of iterations $k_{\\max}$ to prevent infinite loops.\n\nNumerical parameters common to all test cases:\n- Use $N=50$ interior points.\n- Set the tolerance $\\varepsilon = 10^{-8}$ (unitless).\n- Set the maximum iterations $k_{\\max}=200$.\n\nTest suite:\nImplement and run the Picard iteration under both coupling sequences for the following four test cases. In each case, define the coefficient functions $k(T)$, $C(T)$, and $q(u)$, and Dirichlet boundary values $T(0)$, $T(1)$, $u(0)$, $u(1)$ as specified. Ensure $k(T)>0$ and $C(T)>0$ on the domain for the provided parameter ranges.\n\n- Case 1 (moderate nonlinearity, moderate coupling):\n  - $k(T) = 1 + 0.5\\,T$\n  - $C(T) = \\exp(0.3\\,T)$\n  - $q(u) = 0.1 + u^2$\n  - Boundaries: $T(0)=0$, $T(1)=1$; $u(0)=1$, $u(1)=0$\n\n- Case 2 (constant coefficients and zero source; boundary consistency):\n  - $k(T) = 1$\n  - $C(T) = 1$\n  - $q(u) = 0$\n  - Boundaries: $T(0)=0$, $T(1)=0$; $u(0)=1$, $u(1)=1$\n\n- Case 3 (strong nonlinearity, stronger coupling):\n  - $k(T) = 0.5 + 5\\,T^2$\n  - $C(T) = 0.1 + 2\\,T$\n  - $q(u) = 0.5\\,u + 2\\,u^2$\n  - Boundaries: $T(0)=2$, $T(1)=0$; $u(0)=0$, $u(1)=1$\n\n- Case 4 (nonlinearity in mobility with constant source):\n  - $k(T) = 2$\n  - $C(T) = 0.5 + 3\\,\\exp(T)$\n  - $q(u) = 0.2$\n  - Boundaries: $T(0)=0$, $T(1)=1$; $u(0)=0.5$, $u(1)=0.5$\n\nFinal output specification:\n- For each case, record the number of Picard iterations performed until the stopping criterion is met (or until $k_{\\max}$ is reached) for both coupling sequences: $T$-then-$u$ and $u$-then-$T$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by case and sequence:\n  $[n_{TU}^{(1)},n_{UT}^{(1)},n_{TU}^{(2)},n_{UT}^{(2)},n_{TU}^{(3)},n_{UT}^{(3)},n_{TU}^{(4)},n_{UT}^{(4)}]$,\nwhere $n_{TU}^{(j)}$ is the iteration count for case $j$ using the $T$-then-$u$ sequence, and $n_{UT}^{(j)}$ is the iteration count for case $j$ using the $u$-then-$T$ sequence. All entries must be integers.",
            "solution": "The problem presented is a valid, well-posed, and self-contained statement describing the numerical solution of a coupled multiphysics system. It is grounded in the standard principles of continuum mechanics (heat conduction, diffusion) and numerical analysis (finite differences, iterative methods). All parameters, equations, boundary conditions, and algorithmic requirements are specified with sufficient precision to permit a unique and verifiable implementation.\n\nThe core of the problem is to solve a system of two coupled, nonlinear, second-order ordinary differential equations on the domain $x \\in [0, 1]$ using a fixed-point (Picard) iteration scheme. The governing equations are:\n$$\n-\\frac{d}{dx}\\left(k(T)\\frac{dT}{dx}\\right) = q(u) \\quad \\text{(Equation T)}\n$$\n$$\n\\frac{d}{dx}\\left(C(T)\\frac{du}{dx}\\right) = 0 \\quad \\text{(Equation U)}\n$$\nwith Dirichlet boundary conditions for both fields $T(x)$ and $u(x)$.\n\nThe Picard iteration method linearizes this system by evaluating the nonlinear coefficients and source terms at the previous iteration's solution. For iteration $k+1$, given the solution $(T^k, u^k)$ from iteration $k$, we solve the following two linear equations for $(T^{k+1}, u^{k+1})$:\n$$\n-\\frac{d}{dx}\\left(k(T^k)\\frac{dT^{k+1}}{dx}\\right) = q(u^k) \\quad \\text{(Linearized T-Subproblem)}\n$$\n$$\n-\\frac{d}{dx}\\left(C(T^k)\\frac{du^{k+1}}{dx}\\right) = 0 \\quad \\text{(Linearized U-Subproblem)}\n$$\nNote that we have multiplied Equation U by $-1$ to cast both subproblems into the standard form of a Poisson-like equation, which is convenient for developing a general discretization.\n\nA crucial aspect of this formulation is that the two linearized subproblems are decoupled within a single iteration. The equation for $T^{k+1}$ depends only on quantities from the previous iteration ($T^k, u^k$), and similarly for the equation for $u^{k+1}$. Consequently, the order in which these two subproblems are solved—referred to as the '$T$-then-$u$' and '$u$-then-$T$' sequences—has no impact on the resulting solutions $T^{k+1}$ and $u^{k+1}$. Both sequences will produce the exact same series of iterates and therefore converge in the identical number of iterations. This is a characteristic of the Picard (or Jacobi-like) scheme, as opposed to a Gauss-Seidel-like scheme where the most recently updated information would be used immediately within the same iteration.\n\nWe will now derive the finite-difference discretization for a generic steady-state diffusion equation in divergence form on a uniform grid with $N$ interior nodes $x_i = i h$ for $i=1,\\ldots,N$, where $h=1/(N+1)$ is the grid spacing. The generic equation is:\n$$\n-\\frac{d}{dx}\\left(A(x)\\frac{dy}{dx}\\right) = S(x)\n$$\nWe integrate this equation over a control volume $[x_{i-1/2}, x_{i+1/2}]$ centered around node $x_i$, where $x_{i\\pm1/2} = x_i \\pm h/2$. This yields:\n$$\n-\\int_{x_{i-1/2}}^{x_{i+1/2}} \\frac{d}{dx}\\left(A(x)\\frac{dy}{dx}\\right) dx = \\int_{x_{i-1/2}}^{x_{i+1/2}} S(x) dx\n$$\nApplying the fundamental theorem of calculus to the left side and approximating the right side gives:\n$$\n-\\left[A\\frac{dy}{dx}\\right]_{x_{i+1/2}} + \\left[A\\frac{dy}{dx}\\right]_{x_{i-1/2}} \\approx h S(x_i)\n$$\nThe flux terms at the cell faces are approximated using second-order central differences:\n$$\n\\left[A\\frac{dy}{dx}\\right]_{x_{i+1/2}} \\approx A(x_{i+1/2}) \\frac{y(x_{i+1}) - y(x_i)}{h}\n$$\n$$\n\\left[A\\frac{dy}{dx}\\right]_{x_{i-1/2}} \\approx A(x_{i-1/2}) \\frac{y(x_i) - y(x_{i-1})}{h}\n$$\nSubstituting these into the integrated equation and dividing by $h$ leads to the discrete equation at node $i$:\n$$\n-\\frac{1}{h^2} \\left( A_{i+1/2}(y_{i+1} - y_i) - A_{i-1/2}(y_i - y_{i-1}) \\right) = S_i\n$$\nwhere $y_i \\approx y(x_i)$, $S_i = S(x_i)$, and the face coefficients $A_{i\\pm1/2}$ are evaluated using the specified arithmetic mean of the nodal values from the previous Picard iteration. Rearranging the terms, we obtain the algebraic equation for node $i$:\n$$\n-A_{i-1/2} y_{i-1} + (A_{i-1/2} + A_{i+1/2}) y_i - A_{i+1/2} y_{i+1} = h^2 S_i\n$$\nThis system of equations for $i=1, \\ldots, N$ forms a tridiagonal linear system. The boundary conditions $y_0 = y(0)$ and $y_{N+1} = y(1)$ are incorporated by moving the terms involving these known values to the right-hand side of the equations for nodes $i=1$ and $i=N$.\n\n**Applying to the T-Subproblem:**\nThe discrete equation for $T^{k+1}_i$ is:\n$$\n-k^k_{i-1/2} T^{k+1}_{i-1} + (k^k_{i-1/2} + k^k_{i+1/2}) T^{k+1}_i - k^k_{i+1/2} T^{k+1}_{i+1} = h^2 q(u^k_i)\n$$\nwhere $k^k_{i+1/2} = \\frac{1}{2}(k(T^k_i) + k(T^k_{i+1}))$. For the boundary nodes:\n- For $i=1$: The term $-k^k_{1/2} T^{k+1}_0$ becomes a known value, $k^k_{1/2} T(0)$, added to the right-hand side.\n- For $i=N$: The term $-k^k_{N+1/2} T^{k+1}_{N+1}$ becomes $k^k_{N+1/2} T(1)$, added to the right-hand side.\n\n**Applying to the U-Subproblem:**\nThe discrete equation for $u^{k+1}_i$ is:\n$$\n-C^k_{i-1/2} u^{k+1}_{i-1} + (C^k_{i-1/2} + C^k_{i+1/2}) u^{k+1}_i - C^k_{i+1/2} u^{k+1}_{i+1} = 0\n$$\nwhere $C^k_{i+1/2} = \\frac{1}{2}(C(T^k_i) + C(T^k_{i+1}))$. For the boundary nodes:\n- For $i=1$: The term $-C^k_{1/2} u^{k+1}_0$ becomes $C^k_{1/2} u(0)$, added to the right-hand side.\n- For $i=N$: The term $-C^k_{N+1/2} u^{k+1}_{N+1}$ becomes $C^k_{N+1/2} u(1)$, added to the right-hand side.\n\nThe overall algorithm is as follows:\n1.  Initialize $k=0$ and the solution vectors $T^0$ and $u^0$ as linear interpolants between their respective boundary values.\n2.  Begin the iteration loop, repeated until convergence or until $k$ reaches $k_{\\max}$.\n3.  Inside the loop, construct and solve the tridiagonal linear system for $T^{k+1}$ using coefficients from $T^k$ and the source term from $u^k$.\n4.  Construct and solve the tridiagonal linear system for $u^{k+1}$ using coefficients from $T^k$.\n5.  Calculate the infinity norm of the updates, $\\delta_T = \\|T^{k+1}-T^k\\|_{\\infty}$ and $\\delta_u = \\|u^{k+1}-u^k\\|_{\\infty}$.\n6.  If $\\max(\\delta_T, \\delta_u) < \\varepsilon$, the solution has converged. Terminate the loop and record the number of iterations.\n7.  Update the solution for the next iteration: $T^k \\leftarrow T^{k+1}$, $u^k \\leftarrow u^{k+1}$.\n\nThis procedure is implemented for each test case to determine the number of iterations required for convergence. The tridiagonal systems are solved efficiently using `scipy.linalg.solve_banded`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef picard_solver(k_func, C_func, q_func, T0, T1, u0, u1, N, epsilon, k_max):\n    \"\"\"\n    Solves the coupled 1D system using Picard iteration and finite differences.\n    \"\"\"\n    # 1. Grid and Initial Guess Setup\n    h = 1.0 / (N + 1)\n    x = np.linspace(0, 1, N + 2)\n\n    # Initialize full solution vectors (including boundaries)\n    T_full = np.linspace(T0, T1, N + 2)\n    u_full = np.linspace(u0, u1, N + 2)\n    \n    # Extract interior points for iteration\n    T_k = T_full[1:-1].copy()\n    u_k = u_full[1:-1].copy()\n\n    for k_iter in range(k_max):\n        T_prev = T_k.copy()\n        u_prev = u_k.copy()\n        \n        # Update full solution vectors with previous iterate's values\n        T_full[1:-1] = T_k\n        u_full[1:-1] = u_k\n\n        # 2. Solve T-subproblem for T^{k+1}\n        # Assemble coefficients k(T^k)\n        k_vals = k_func(T_full)\n        k_half = 0.5 * (k_vals[:-1] + k_vals[1:]) # Face values, size N+1\n        \n        # Assemble tridiagonal matrix for T\n        # scipy.linalg.solve_banded expects matrix `ab` in a specific format:\n        # ab[0,:] = super-diagonal (shifted)\n        # ab[1,:] = main diagonal\n        # ab[2,:] = sub-diagonal (shifted)\n        A_T = np.zeros((3, N))\n        A_T[0, 1:]  = -k_half[1:N]\n        A_T[1, :]   = k_half[0:N] + k_half[1:N+1]\n        A_T[2, :-1] = -k_half[1:N]\n        \n        # Assemble right-hand side vector for T\n        b_T = h**2 * q_func(u_k)\n        b_T[0] += k_half[0] * T0\n        b_T[-1] += k_half[-1] * T1\n        \n        T_k_next = solve_banded((1, 1), A_T, b_T)\n\n        # 3. Solve U-subproblem for u^{k+1}\n        # Assemble coefficients C(T^k)\n        C_vals = C_func(T_full)\n        C_half = 0.5 * (C_vals[:-1] + C_vals[1:]) # Face values, size N+1\n\n        # Assemble tridiagonal matrix for U\n        A_U = np.zeros((3, N))\n        A_U[0, 1:]  = -C_half[1:N]\n        A_U[1, :]   = C_half[0:N] + C_half[1:N+1]\n        A_U[2, :-1] = -C_half[1:N]\n        \n        # Assemble right-hand side vector for U\n        b_U = np.zeros(N)\n        b_U[0] += C_half[0] * u0\n        b_U[-1] += C_half[-1] * u1\n        \n        u_k_next = solve_banded((1, 1), A_U, b_U)\n\n        # 4. Check for convergence\n        err_T = np.linalg.norm(T_k_next - T_prev, np.inf)\n        err_U = np.linalg.norm(u_k_next - u_prev, np.inf)\n\n        if max(err_T, err_U) < epsilon:\n            return k_iter + 1\n\n        # 5. Update for next iteration\n        T_k = T_k_next\n        u_k = u_k_next\n\n    return k_max\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Global parameters\n    N = 50\n    epsilon = 1e-8\n    k_max = 200\n\n    test_cases = [\n        # Case 1\n        {\n            \"k_func\": lambda T: 1.0 + 0.5 * T,\n            \"C_func\": lambda T: np.exp(0.3 * T),\n            \"q_func\": lambda u: 0.1 + u**2,\n            \"T_bcs\": (0.0, 1.0),\n            \"u_bcs\": (1.0, 0.0)\n        },\n        # Case 2\n        {\n            \"k_func\": lambda T: 1.0,\n            \"C_func\": lambda T: 1.0,\n            \"q_func\": lambda u: 0.0,\n            \"T_bcs\": (0.0, 0.0),\n            \"u_bcs\": (1.0, 1.0)\n        },\n        # Case 3\n        {\n            \"k_func\": lambda T: 0.5 + 5.0 * T**2,\n            \"C_func\": lambda T: 0.1 + 2.0 * T,\n            \"q_func\": lambda u: 0.5 * u + 2.0 * u**2,\n            \"T_bcs\": (2.0, 0.0),\n            \"u_bcs\": (0.0, 1.0)\n        },\n        # Case 4\n        {\n            \"k_func\": lambda T: 2.0,\n            \"C_func\": lambda T: 0.5 + 3.0 * np.exp(T),\n            \"q_func\": lambda u: 0.2,\n            \"T_bcs\": (0.0, 1.0),\n            \"u_bcs\": (0.5, 0.5)\n        }\n    ]\n\n    # Vectorize functions for performance\n    for case in test_cases:\n        case[\"k_func\"] = np.vectorize(case[\"k_func\"])\n        case[\"C_func\"] = np.vectorize(case[\"C_func\"])\n        case[\"q_func\"] = np.vectorize(case[\"q_func\"])\n\n    results = []\n    for case in test_cases:\n        # Run the solver for the current case\n        num_iterations = picard_solver(\n            k_func=case[\"k_func\"],\n            C_func=case[\"C_func\"],\n            q_func=case[\"q_func\"],\n            T0=case[\"T_bcs\"][0],\n            T1=case[\"T_bcs\"][1],\n            u0=case[\"u_bcs\"][0],\n            u1=case[\"u_bcs\"][1],\n            N=N,\n            epsilon=epsilon,\n            k_max=k_max\n        )\n        \n        # As per the problem's strict Picard definition, the order of solving\n        # the uncoupled subproblems does not matter. The iteration count for\n        # T-then-u and u-then-T sequences is identical.\n        results.append(num_iterations)  # n_TU\n        results.append(num_iterations)  # n_UT\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A critical aspect of solving large-scale coupled systems is understanding the structure and numerical properties of the linearized equations. This advanced practice delves into the analysis of the block matrix arising from a discretized thermoelastic problem. By deriving and computationally analyzing the Schur complements that result from different block elimination strategies, you will investigate how physical parameters like stiffness and coupling strength influence the conditioning of the linear subproblems, a key consideration in designing robust and efficient segregated solvers. ",
            "id": "3512972",
            "problem": "Consider the linear thermoelastic coupling between mechanical displacement and temperature in a quasi-static, steady-state setting. Starting from conservation of linear momentum and steady energy balance, together with linearized constitutive relations for small strains and small temperature deviations, one arrives (after spatial discretization by a standard second-order central difference scheme with homogeneous Dirichlet boundary conditions on a one-dimensional unit interval) at a coupled linear system for increments of displacement and temperature. The algebraic system takes the block form\n$$\n\\mathbf{J}\n=\n\\begin{bmatrix}\n\\mathbf{K}_u & \\mathbf{B} \\\\\n\\mathbf{B}^\\top & \\mathbf{K}_T\n\\end{bmatrix},\n$$\nwhere $\\mathbf{K}_u$ and $\\mathbf{K}_T$ are symmetric positive definite stiffness-like matrices, and $\\mathbf{B}$ represents thermoelastic coupling that is linear in the temperature and displacement increments. Throughout this problem, all quantities are dimensionless.\n\nYou will work with the following concrete, purely algebraic specialization that abstracts the essential features of the thermoelastic Jacobian. For a given integer $N \\geq 2$, define the $N \\times N$ tridiagonal matrix\n$$\n\\mathbf{K}_N = \\operatorname{tridiag}(-1,2,-1),\n$$\nthat is, $(\\mathbf{K}_N)_{ii} = 2$, $(\\mathbf{K}_N)_{i,i+1} = (\\mathbf{K}_N)_{i+1,i} = -1$ for $i = 1,\\dots,N-1$, and zero otherwise. For given positive scalars $s_u$ and $s_T$, define\n$$\n\\mathbf{K}_u = s_u \\, \\mathbf{K}_N, \\quad \\mathbf{K}_T = s_T \\, \\mathbf{K}_N.\n$$\nLet $\\gamma > 0$ be a scalar coupling amplitude. Consider two forms of the coupling block $\\mathbf{B}$:\n- Uniform coupling: $\\mathbf{B} = \\gamma \\, \\mathbf{I}_N$.\n- Heterogeneous coupling: $\\mathbf{B} = \\gamma \\, \\operatorname{diag}(\\mathbf{w})$, with entries $w_i = 1 + 0.5 \\sin(2\\pi x_i)$, where $x_i = \\frac{i}{N+1}$ for $i=1,\\dots,N$.\n\nFrom the fundamental definition of block Gaussian elimination, eliminating one field first produces a reduced system for the remaining field, characterized by a Schur complement. Specifically, eliminating the displacement first produces a Schur complement on temperature, and eliminating the temperature first produces a Schur complement on displacement. Without using any pre-derived formulas, and starting from the definition of the Schur complement as the algebraic result of exact block elimination, derive the expressions for the two Schur complements and then use those expressions to assess their spectral conditioning.\n\nYour tasks are:\n1. Derive, from the definition of block Gaussian elimination and the properties of symmetric positive definite matrices, explicit expressions for the two Schur complements corresponding to the two elimination orderings, in terms of $\\mathbf{K}_u$, $\\mathbf{K}_T$, and $\\mathbf{B}$.\n2. Using the definition of the spectral condition number $\\kappa_2(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\,\\|\\mathbf{A}^{-1}\\|_2$, which equals the ratio of the largest to the smallest singular value of $\\mathbf{A}$, design an algorithm to compute $\\kappa_2$ for each of the two Schur complements. You must not assume that the Schur complements are positive definite; if a Schur complement is indefinite, you must still compute its $2$-norm condition number via singular values.\n3. Implement a program that, for each test case below, constructs $\\mathbf{K}_N$, $\\mathbf{K}_u$, $\\mathbf{K}_T$, and $\\mathbf{B}$; forms the two Schur complements by exact algebraic elimination; computes their spectral condition numbers; and returns the ratio\n$$\nR = \\frac{\\kappa_2(\\text{Schur complement on } T \\text{ when eliminating } \\mathbf{u} \\text{ first})}{\\kappa_2(\\text{Schur complement on } \\mathbf{u} \\text{ when eliminating } T \\text{ first})}.\n$$\nOutput $R$ as a floating-point number for each test case.\n\nAll computations are dimensionless real arithmetic. Do not introduce any additional physical units. Angles, where present in definitions, are in radians.\n\nTest suite (each tuple is $(N, s_u, s_T, \\gamma, \\text{heterogeneous})$, where the heterogeneous flag is $0$ for uniform coupling and $1$ for heterogeneous coupling):\n- Case A (happy path, uniform, mild coupling): $(10, 1.0, 1.0, 0.1, 0)$.\n- Case B (uniform, stiffness contrast): $(10, 100.0, 1.0, 1.0, 0)$.\n- Case C (uniform, size effect with near-resonant coupling): $(50, 1.0, 1.0, 1.0, 0)$.\n- Case D (heterogeneous, moderate coupling): $(10, 1.0, 1.0, 1.0, 1)$.\n- Case E (heterogeneous, stiffness contrast): $(10, 100.0, 1.0, 0.5, 1)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[rA,rB,rC,rD,rE]\"), where each entry is the floating-point value of $R$ for the corresponding test case in the order specified above. There must be no extra text or lines in the output.",
            "solution": "The problem is evaluated as valid. It is scientifically grounded in the mechanics of coupled systems, well-posed with all necessary information provided, and stated objectively. The premises are consistent and the tasks are computationally verifiable.\n\nThe problem asks for the derivation and comparison of the spectral conditioning of two Schur complements arising from a block linear system representative of discretized linear thermoelasticity. The system is given by\n$$\n\\mathbf{J} \\begin{pmatrix} \\delta \\mathbf{u} \\\\ \\delta \\mathbf{T} \\end{pmatrix} = \\begin{bmatrix} \\mathbf{K}_u & \\mathbf{B} \\\\ \\mathbf{B}^\\top & \\mathbf{K}_T \\end{bmatrix} \\begin{pmatrix} \\delta \\mathbf{u} \\\\ \\delta \\mathbf{T} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{f}_u \\\\ \\mathbf{f}_T \\end{pmatrix}\n$$\nwhere $\\delta \\mathbf{u}$ and $\\delta \\mathbf{T}$ represent vectors of displacement and temperature increments, respectively. The matrices $\\mathbf{K}_u$ and $\\mathbf{K}_T$ are specified as symmetric positive definite (SPD), and $\\mathbf{B}$ is the coupling matrix.\n\n**1. Derivation of the Schur Complements**\n\nThe Schur complement arises from the process of block Gaussian elimination. We derive the two Schur complements corresponding to the two possible elimination orders.\n\n**Case 1: Eliminating Displacement Increments ($\\delta \\mathbf{u}$) first to find the Schur complement on Temperature ($\\mathbf{S}_T$)**\n\nWe begin with the two block equations:\n$$\n(1) \\quad \\mathbf{K}_u \\delta \\mathbf{u} + \\mathbf{B} \\delta \\mathbf{T} = \\mathbf{f}_u\n$$\n$$\n(2) \\quad \\mathbf{B}^\\top \\delta \\mathbf{u} + \\mathbf{K}_T \\delta \\mathbf{T} = \\mathbf{f}_T\n$$\nFrom equation (1), we solve for $\\delta \\mathbf{u}$. Since $\\mathbf{K}_u$ is given as SPD, it is invertible.\n$$\n\\mathbf{K}_u \\delta \\mathbf{u} = \\mathbf{f}_u - \\mathbf{B} \\delta \\mathbf{T} \\implies \\delta \\mathbf{u} = \\mathbf{K}_u^{-1} (\\mathbf{f}_u - \\mathbf{B} \\delta \\mathbf{T})\n$$\nNext, we substitute this expression for $\\delta \\mathbf{u}$ into equation (2):\n$$\n\\mathbf{B}^\\top \\left[ \\mathbf{K}_u^{-1} (\\mathbf{f}_u - \\mathbf{B} \\delta \\mathbf{T}) \\right] + \\mathbf{K}_T \\delta \\mathbf{T} = \\mathbf{f}_T\n$$\nExpanding and rearranging the terms to isolate $\\delta \\mathbf{T}$:\n$$\n\\mathbf{B}^\\top \\mathbf{K}_u^{-1} \\mathbf{f}_u - \\mathbf{B}^\\top \\mathbf{K}_u^{-1} \\mathbf{B} \\delta \\mathbf{T} + \\mathbf{K}_T \\delta \\mathbf{T} = \\mathbf{f}_T\n$$\n$$\n(\\mathbf{K}_T - \\mathbf{B}^\\top \\mathbf{K}_u^{-1} \\mathbf{B}) \\delta \\mathbf{T} = \\mathbf{f}_T - \\mathbf{B}^\\top \\mathbf{K}_u^{-1} \\mathbf{f}_u\n$$\nThe matrix multiplying $\\delta \\mathbf{T}$ on the left-hand side is the Schur complement of the block $\\mathbf{K}_u$ in the matrix $\\mathbf{J}$. This is the effective matrix for the temperature system after eliminating the displacements. We denote it by $\\mathbf{S}_T$.\n$$\n\\mathbf{S}_T = \\mathbf{K}_T - \\mathbf{B}^\\top \\mathbf{K}_u^{-1} \\mathbf{B}\n$$\n\n**Case 2: Eliminating Temperature Increments ($\\delta \\mathbf{T}$) first to find the Schur complement on Displacement ($\\mathbf{S}_u$)**\n\nThis process is symmetric to the first case. We start by solving for $\\delta \\mathbf{T}$ from equation (2). Since $\\mathbf{K}_T$ is SPD, it is invertible.\n$$\n\\mathbf{K}_T \\delta \\mathbf{T} = \\mathbf{f}_T - \\mathbf{B}^\\top \\delta \\mathbf{u} \\implies \\delta \\mathbf{T} = \\mathbf{K}_T^{-1} (\\mathbf{f}_T - \\mathbf{B}^\\top \\delta \\mathbf{u})\n$$\nSubstitute this expression for $\\delta \\mathbf{T}$ into equation (1):\n$$\n\\mathbf{K}_u \\delta \\mathbf{u} + \\mathbf{B} \\left[ \\mathbf{K}_T^{-1} (\\mathbf{f}_T - \\mathbf{B}^\\top \\delta \\mathbf{u}) \\right] = \\mathbf{f}_u\n$$\nExpanding and rearranging to isolate $\\delta \\mathbf{u}$:\n$$\n\\mathbf{K}_u \\delta \\mathbf{u} + \\mathbf{B} \\mathbf{K}_T^{-1} \\mathbf{f}_T - \\mathbf{B} \\mathbf{K}_T^{-1} \\mathbf{B}^\\top \\delta \\mathbf{u} = \\mathbf{f}_u\n$$\n$$\n(\\mathbf{K}_u - \\mathbf{B} \\mathbf{K}_T^{-1} \\mathbf{B}^\\top) \\delta \\mathbf{u} = \\mathbf{f}_u - \\mathbf{B} \\mathbf{K}_T^{-1} \\mathbf{f}_T\n$$\nThe Schur complement of the block $\\mathbf{K}_T$ in $\\mathbf{J}$ is the matrix operating on $\\delta \\mathbf{u}$, which we denote by $\\mathbf{S}_u$.\n$$\n\\mathbf{S}_u = \\mathbf{K}_u - \\mathbf{B} \\mathbf{K}_T^{-1} \\mathbf{B}^\\top\n$$\n\n**2. Algorithm for Spectral Condition Number Calculation**\n\nThe spectral condition number, or $2$-norm condition number, of a matrix $\\mathbf{A}$ is defined as $\\kappa_2(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\|\\mathbf{A}^{-1}\\|_2$. For any matrix $\\mathbf{A}$, this is equivalent to the ratio of its largest singular value, $\\sigma_{\\max}(\\mathbf{A})$, to its smallest singular value, $\\sigma_{\\min}(\\mathbf{A})$:\n$$\n\\kappa_2(\\mathbf{A}) = \\frac{\\sigma_{\\max}(\\mathbf{A})}{\\sigma_{\\min}(\\mathbf{A})}\n$$\nThe singular values of $\\mathbf{A}$ are the square roots of the eigenvalues of the symmetric positive semi-definite matrix $\\mathbf{A}^\\top \\mathbf{A}$. Crucially, this definition holds for any matrix, including non-symmetric, singular, or indefinite matrices. The problem correctly requires that we must not assume the Schur complements $\\mathbf{S}_T$ and $\\mathbf{S}_u$ are positive definite. While they are symmetric (as can be verified by taking their transpose), the subtraction of a positive semi-definite term $\\mathbf{B}^\\top \\mathbf{K}_u^{-1} \\mathbf{B}$ from an SPD matrix $\\mathbf{K}_T$ can result in an indefinite matrix.\n\nThe algorithm to compute $\\kappa_2(\\mathbf{A})$ for a given Schur complement matrix $\\mathbf{A}$ is as follows:\n1.  Compute the singular values of $\\mathbf{A}$. Standard numerical libraries provide robust SVD (Singular Value Decomposition) algorithms for this purpose.\n2.  Identify the maximum singular value, $\\sigma_{\\max}$.\n3.  Identify the minimum singular value, $\\sigma_{\\min}$.\n4.  If $\\sigma_{\\min}$ is zero or numerically indistinguishable from zero, the matrix is singular, and its condition number is infinite. In floating-point arithmetic, this will result in a very large number.\n5.  If $\\sigma_{\\min} > 0$, the condition number is the ratio $\\sigma_{\\max} / \\sigma_{\\min}$.\n\n**3. Implementation Plan**\n\nThe program will execute the following steps for each test case $(N, s_u, s_T, \\gamma, \\text{heterogeneous})$:\n1.  **Construct Matrices**:\n    *   Form the $N \\times N$ matrix $\\mathbf{K}_N = \\operatorname{tridiag}(-1, 2, -1)$.\n    *   Scale it to get $\\mathbf{K}_u = s_u \\mathbf{K}_N$ and $\\mathbf{K}_T = s_T \\mathbf{K}_N$.\n    *   Construct the coupling matrix $\\mathbf{B}$ based on the `heterogeneous` flag:\n        *   If `heterogeneous` is $0$: $\\mathbf{B} = \\gamma \\mathbf{I}_N$.\n        *   If `heterogeneous` is $1$: calculate the vector $\\mathbf{w}$ with entries $w_i = 1 + 0.5 \\sin(2\\pi x_i)$ where $x_i = i/(N+1)$, and form the diagonal matrix $\\mathbf{B} = \\gamma \\operatorname{diag}(\\mathbf{w})$.\n2.  **Form Schur Complements**:\n    *   Compute the inverses $\\mathbf{K}_u^{-1}$ and $\\mathbf{K}_T^{-1}$.\n    *   Calculate $\\mathbf{S}_T = \\mathbf{K}_T - \\mathbf{B}^\\top \\mathbf{K}_u^{-1} \\mathbf{B}$.\n    *   Calculate $\\mathbf{S}_u = \\mathbf{K}_u - \\mathbf{B} \\mathbf{K}_T^{-1} \\mathbf{B}^\\top$. Note that for both coupling types considered, $\\mathbf{B}$ is diagonal, hence $\\mathbf{B}^\\top = \\mathbf{B}$.\n3.  **Compute Condition Numbers**:\n    *   Compute $\\kappa_2(\\mathbf{S}_T)$ using a numerical library function that calculates the $2$-norm condition number.\n    *   Compute $\\kappa_2(\\mathbf{S}_u)$ using the same function.\n4.  **Calculate Ratio**:\n    *   Compute the required ratio $R = \\kappa_2(\\mathbf{S}_T) / \\kappa_2(\\mathbf{S}_u)$.\n5.  **Store and Output**:\n    *   Append the computed value of $R$ to a results list.\n    *   After processing all test cases, format the list into the specified string `\"[rA,rB,rC,rD,rE]\"` and print it.\n\nThis procedure adheres to the derivations and algorithms defined above.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the thermoelastic Schur complement conditioning problem for a suite of test cases.\n    \"\"\"\n    # Test suite (N, s_u, s_T, gamma, heterogeneous_flag)\n    # heterogeneous_flag: 0 for uniform, 1 for heterogeneous coupling\n    test_cases = [\n        (10, 1.0, 1.0, 0.1, 0),    # Case A\n        (10, 100.0, 1.0, 1.0, 0),   # Case B\n        (50, 1.0, 1.0, 1.0, 0),    # Case C\n        (10, 1.0, 1.0, 1.0, 1),    # Case D\n        (10, 100.0, 1.0, 0.5, 1),   # Case E\n    ]\n\n    results = []\n    for case in test_cases:\n        N, s_u, s_T, gamma, heterogeneous = case\n\n        # 1. Construct matrices\n        \n        # K_N is tridiag(-1, 2, -1)\n        K_N = 2 * np.eye(N) - np.diag(np.ones(N - 1), k=1) - np.diag(np.ones(N - 1), k=-1)\n        \n        K_u = s_u * K_N\n        K_T = s_T * K_N\n        \n        if heterogeneous == 0:\n            # Uniform coupling: B = gamma * I\n            B = gamma * np.eye(N)\n        else:\n            # Heterogeneous coupling: B = gamma * diag(w)\n            x_i = (np.arange(N) + 1.0) / (N + 1.0)\n            w_i = 1.0 + 0.5 * np.sin(2 * np.pi * x_i)\n            B = gamma * np.diag(w_i)\n            \n        # 2. Form Schur complements\n        # Both coupling matrices B are symmetric (diagonal), so B.T = B.\n        \n        # Compute inverses\n        try:\n            K_u_inv = np.linalg.inv(K_u)\n            K_T_inv = np.linalg.inv(K_T)\n        except np.linalg.LinAlgError:\n            # This should not happen since K_u and K_T are SPD\n            results.append(np.nan)\n            continue\n            \n        # Schur complement on T (temperature)\n        S_T = K_T - B.T @ K_u_inv @ B\n        \n        # Schur complement on u (displacement)\n        S_u = K_u - B @ K_T_inv @ B.T\n        \n        # 3. Compute spectral condition numbers (p=2 for 2-norm)\n        cond_S_T = np.linalg.cond(S_T, p=2)\n        cond_S_u = np.linalg.cond(S_u, p=2)\n        \n        # 4. Calculate the ratio R\n        if cond_S_u == 0:\n             # Avoid division by zero, though this is unlikely for the given problems\n             R = np.inf\n        else:\n             R = cond_S_T / cond_S_u\n\n        results.append(R)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}