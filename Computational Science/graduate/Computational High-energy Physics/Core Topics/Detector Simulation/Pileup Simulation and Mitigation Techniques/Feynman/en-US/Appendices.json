{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in high-energy physics analyses is to ensure that the simulated Monte Carlo (MC) events accurately reflect the conditions of the real data. This exercise focuses on a fundamental procedure: pileup reweighting. You will implement a scheme to correct the distribution of pileup interactions in simulation to match a target \"data\" distribution, and then evaluate the impact of this correction on a crucial experimental observableâ€”the trigger efficiency curve . This practice provides a concrete understanding of why accurate pileup modeling is essential and how its associated uncertainties are propagated.",
            "id": "3528671",
            "problem": "You are tasked with constructing and evaluating a self-consistent pileup reweighting scheme in a simplified, but physically grounded, computational model of high-energy proton-proton collisions. The goal is to map the Monte Carlo (MC) pileup multiplicity spectrum to a target \"data-like\" spectrum, assess the impact on trigger turn-on behavior, and propagate an uncertainty from the inelastic proton-proton cross-section to a physics observable. Your program must be a complete, runnable implementation that performs all steps and returns the requested quantities for a provided set of test cases.\n\nFundamental base and definitions:\n- The number of additional inelastic interactions per bunch crossing (pileup) is modeled as a Poisson random variable. For a given mean pileup multiplicity $\\lambda$, the probability mass function is\n$$\nP(k \\mid \\lambda) = \\frac{e^{-\\lambda}\\,\\lambda^k}{k!}, \\quad k \\in \\{0,1,2,\\dots\\}.\n$$\n- A data-to-Monte-Carlo reweighting scheme that reshapes an MC-distributed variable $X$ to match a target \"data-like\" distribution uses weights\n$$\nw(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{MC}}(x)},\n$$\nso that the weighted MC expectation of any integrable function $g$ equals the target expectation: $\\mathbb{E}_{\\text{MC}}[w(X) g(X)] = \\mathbb{E}_{\\text{data}}[g(X)]$.\n- A single-object trigger response with finite resolution is modeled by a smooth detection probability as a function of the offline transverse momentum $p_T$. We use the logistic function (a smooth approximation to an error-function turn-on) parameterized by a threshold offset and resolution:\n$$\nS(p_T,\\mu) = \\frac{1}{1 + \\exp\\!\\left(-\\frac{p_T - T_0 - a\\,\\mu}{b}\\right)},\n$$\nwhere $T_0$ is a nominal hardware threshold, $a$ encodes the linear degradation of the effective threshold with pileup multiplicity $\\mu$, and $b$ controls the steepness of the turn-on.\n- The inclusive offline $p_T$ spectrum for the physics process of interest is modeled as a normalized modified power law,\n$$\nf(p_T) \\propto \\left(1 + \\frac{p_T}{p_0}\\right)^{-n}, \\quad p_T \\ge 0,\n$$\nwith parameters $p_0$ and $n$, independent of $\\mu$.\n\nComputational tasks to implement for each test case:\n1. Construct a discrete pileup multiplicity grid $\\mu \\in \\{0,1,2,\\dots,\\mu_{\\max}\\}$ with $\\mu_{\\max} = 200$. Compute the MC pileup distribution $p_{\\text{MC}}(\\mu)$ as a Poisson distribution with mean $\\lambda_{\\text{MC}}$, and the target \"data-like\" pileup distribution $p_{\\text{data}}(\\mu)$ as a Poisson distribution with mean $\\lambda_{\\text{data}}$. Renormalize each to sum to $1$ over the truncated grid.\n2. Define weights $w(\\mu) = p_{\\text{data}}(\\mu)/p_{\\text{MC}}(\\mu)$ on the grid. Use these to construct a weighted pileup distribution $p_{\\text{w}}(\\mu)$ proportional to $p_{\\text{MC}}(\\mu)\\,w(\\mu)$, and renormalize it to sum to $1$.\n3. On a transverse momentum grid $p_T \\in \\{0,1,2,\\dots,200\\}$ in units of $\\text{GeV}$, compute two trigger turn-on curves:\n   - The unweighted MC-averaged curve\n   $$\n   \\varepsilon_{\\text{before}}(p_T) = \\sum_{\\mu=0}^{\\mu_{\\max}} p_{\\text{MC}}(\\mu)\\, S(p_T,\\mu).\n   $$\n   - The reweighted (data-like) curve\n   $$\n   \\varepsilon_{\\text{after}}(p_T) = \\sum_{\\mu=0}^{\\mu_{\\max}} p_{\\text{w}}(\\mu)\\, S(p_T,\\mu).\n   $$\n   Quantify the reweighting impact on the turn-on by the root-mean-square curve difference\n   $$\n   D_{\\text{curve}} = \\sqrt{\\frac{1}{N_{p_T}} \\sum_{i=1}^{N_{p_T}} \\left(\\varepsilon_{\\text{after}}(p_T^{(i)}) - \\varepsilon_{\\text{before}}(p_T^{(i)})\\right)^2},\n   $$\n   where $N_{p_T}$ is the number of $p_T$ grid points.\n4. Define the physics observable as the trigger-averaged inclusive efficiency\n   $$\n   Q = \\frac{\\int_0^{\\infty} f(p_T)\\, \\varepsilon_{\\text{after}}(p_T)\\, dp_T}{\\int_0^{\\infty} f(p_T)\\, dp_T}.\n   $$\n   Approximate this integral numerically on the same $p_T$ grid using a Riemann sum with bin width $\\Delta p_T = 1\\,\\text{GeV}$; ensure $f(p_T)$ is normalized on the discrete grid.\n5. Propagate the uncertainty from the inelastic cross-section $\\sigma_{\\text{inel}}$ to the observable $Q$ by assuming that the target mean pileup scales linearly with $\\sigma_{\\text{inel}}$, i.e., $\\lambda_{\\text{data}} \\mapsto \\lambda_{\\text{data}}(1 \\pm \\delta)$ for a relative uncertainty $\\delta = \\Delta \\sigma_{\\text{inel}} / \\sigma_{\\text{inel}}$. Recompute the reweighted distribution and the corresponding $Q_{\\text{up}}$ and $Q_{\\text{down}}$ using $\\lambda_{\\text{data}}^{\\text{up}} = \\lambda_{\\text{data}}(1+\\delta)$ and $\\lambda_{\\text{data}}^{\\text{down}} = \\lambda_{\\text{data}}(1-\\delta)$. Report the symmetric uncertainty as\n   $$\n   \\Delta Q = \\max\\!\\left(\\left|Q_{\\text{up}} - Q\\right|, \\left|Q_{\\text{down}} - Q\\right|\\right).\n   $$\n\nTest suite:\nProvide three test cases, each specified by the tuple $(\\lambda_{\\text{MC}}, \\lambda_{\\text{data}}, \\delta, T_0, a, b, p_0, n)$ with the following values:\n- Case $1$ (general): $(30, 40, 0.05, 28, 0.35, 3.0, 35, 5.0)$\n- Case $2$ (low pileup): $(2, 3, 0.02, 20, 0.10, 2.0, 20, 4.0)$\n- Case $3$ (mismatched distributions): $(50, 20, 0.10, 30, 0.50, 5.0, 40, 6.0)$\n\nUnits and numerical conventions:\n- Use $p_T$ in $\\text{GeV}$. All outputs are pure numbers (dimensionless).\n- The relative uncertainty input $\\delta$ is given as a decimal (for example, $0.05$ for a five percent relative uncertainty).\n\nFinal output format:\n- Your program should produce a single line of output containing all results as a comma-separated list enclosed in square brackets.\n- For each test case, output the following three floats in order: $D_{\\text{curve}}$, $Q$, and $\\Delta Q$. Concatenate these triplets for the three test cases in the given order to produce a single flat list of $9$ floats.\n- Print each float rounded to six decimal places.\n- Example format (with placeholders): $[D_1,Q_1,\\Delta Q_1,D_2,Q_2,\\Delta Q_2,D_3,Q_3,\\Delta Q_3]$.\n\nYour program must be self-contained, accept no input, and implement the definitions and computations specified above without using any external files or network access.",
            "solution": "The problem requires the implementation of a pileup reweighting procedure and the evaluation of its impact on a trigger efficiency curve and a physics observable. The solution will be structured to process each test case by performing the five specified computational tasks in sequence.\n\nFirst, I will define the discrete grids for the pileup multiplicity, $\\mu$, and the transverse momentum, $p_T$. As specified, $\\mu$ ranges from $0$ to $\\mu_{\\max} = 200$, and $p_T$ ranges from $0$ to $200$ GeV with a step of $1$ GeV. These will be represented as one-dimensional NumPy arrays.\n\nFor each test case, the solution proceeds as follows:\n\n**Task 1  2: Pileup Distributions and Weights**\nThe number of pileup interactions is modeled by a Poisson distribution, $P(k \\mid \\lambda) = e^{-\\lambda}\\lambda^k / k!$. I will compute the probability mass function (PMF) for both the Monte Carlo (MC) mean pileup, $\\lambda_{\\text{MC}}$, and the data-like target mean, $\\lambda_{\\text{data}}$, over the discrete grid $\\mu \\in \\{0, 1, \\dots, 200\\}$. Since the Poisson distribution is defined for $k \\in [0, \\infty)$, the PMF calculated on the truncated grid $\\{\\mu_i\\}$ will not sum to unity. Therefore, I will renormalize both distributions, $p_{\\text{MC}}(\\mu)$ and $p_{\\text{data}}(\\mu)$, by dividing by their respective sums over the grid.\n\nThe problem defines the weighted distribution $p_{\\text{w}}(\\mu)$ to be proportional to $p_{\\text{MC}}(\\mu) w(\\mu)$, where the weight is $w(\\mu) = p_{\\text{data}}(\\mu)/p_{\\text{MC}}(\\mu)$. Substituting the weight definition, we find $p_{\\text{MC}}(\\mu)w(\\mu) = p_{\\text{MC}}(\\mu) \\frac{p_{\\text{data}}(\\mu)}{p_{\\text{MC}}(\\mu)} = p_{\\text{data}}(\\mu)$. Thus, $p_{\\text{w}}(\\mu)$ is simply the renormalized $p_{\\text{data}}(\\mu)$ distribution. This is a fundamental consistency check of reweighting: the weighted MC distribution should be identical to the target data distribution.\n\n**Task 3: Trigger Efficiency Curves and Impact**\nThe trigger response is given by the logistic function $S(p_T, \\mu; T_0, a, b) = [1 + \\exp(-[p_T - T_0 - a\\mu]/b)]^{-1}$. This function will be evaluated for every pair of $(p_T, \\mu)$ on our discrete grids. This can be efficiently computed by creating a $2D$ matrix of $S$ values, with dimensions $(N_{p_T}, N_{\\mu})$, using NumPy's broadcasting capabilities.\n\nThe unweighted (MC) and reweighted (data-like) trigger efficiencies, $\\varepsilon_{\\text{before}}(p_T)$ and $\\varepsilon_{\\text{after}}(p_T)$, are the expectations of $S(p_T, \\mu)$ with respect to the $p_{\\text{MC}}(\\mu)$ and $p_{\\text{w}}(\\mu)$ distributions, respectively. These are calculated by summing over $\\mu$:\n$$\n\\varepsilon(p_T) = \\sum_{\\mu=0}^{\\mu_{\\max}} p(\\mu) S(p_T, \\mu)\n$$\nThis operation is equivalent to a matrix-vector product between the $S(p_T, \\mu)$ matrix and the corresponding pileup probability vector.\n\nThe impact of reweighting, $D_{\\text{curve}}$, is the root-mean-square difference between the two efficiency curves, computed over all $p_T$ grid points.\n$$\nD_{\\text{curve}} = \\sqrt{\\frac{1}{N_{p_T}} \\sum_{i=1}^{N_{p_T}} (\\varepsilon_{\\text{after}}(p_T^{(i)}) - \\varepsilon_{\\text{before}}(p_T^{(i)}))^2}\n$$\n\n**Task 4: Physics Observable $Q$**\nThe physics observable $Q$ is the trigger-averaged efficiency, calculated by integrating the product of the offline $p_T$ spectrum, $f(p_T)$, and the reweighted trigger efficiency, $\\varepsilon_{\\text{after}}(p_T)$. The spectrum is $f(p_T) \\propto (1 + p_T/p_0)^{-n}$. The problem specifies using a discrete Riemann sum with $\\Delta p_T = 1$. The formula is:\n$$\nQ = \\frac{\\sum_i f(p_T^{(i)}) \\varepsilon_{\\text{after}}(p_T^{(i)}) \\Delta p_T}{\\sum_i f(p_T^{(i)}) \\Delta p_T}\n$$\nWith $\\Delta p_T = 1$ and the instruction to normalize $f(p_T)$ on the discrete grid, the denominator becomes $1$, and $Q$ simplifies to the dot product of the normalized spectrum vector and the $\\varepsilon_{\\text{after}}(p_T)$ vector: $Q = \\sum_i f_{\\text{norm}}(p_T^{(i)}) \\varepsilon_{\\text{after}}(p_T^{(i)})$.\n\n**Task 5: Uncertainty Propagation**\nTo evaluate the uncertainty on $Q$ due to the inelastic cross-section uncertainty $\\delta$, we vary the target mean pileup to $\\lambda_{\\text{data}}^{\\text{up}} = \\lambda_{\\text{data}}(1+\\delta)$ and $\\lambda_{\\text{data}}^{\\text{down}} = \\lambda_{\\text{data}}(1-\\delta)$. For each variation, a new weighted pileup distribution ($p_{\\text{w}}^{\\text{up}}$ and $p_{\\text{w}}^{\\text{down}}$) is calculated. These are used to recompute the reweighted trigger efficiency curves ($\\varepsilon_{\\text{after}}^{\\text{up}}$ and $\\varepsilon_{\\text{after}}^{\\text{down}}$) and, subsequently, the observable ($Q_{\\text{up}}$ and $Q_{\\text{down}}$). The final symmetric uncertainty, $\\Delta Q$, is the larger of the two absolute deviations from the nominal value $Q$:\n$$\n\\Delta Q = \\max(|Q_{\\text{up}} - Q|, |Q_{\\text{down}} - Q|)\n$$\nThis entire procedure is encapsulated into a function that processes one test case and returns the triplet $(D_{\\text{curve}}, Q, \\Delta Q)$. A main loop iterates through the test suite, collects the results, and formats them into the specified final output string.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import poisson\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the pileup reweighting simulation.\n    It defines test cases, processes each, and prints the final results.\n    \"\"\"\n    \n    # Define problem constants and grids\n    MU_MAX = 200\n    PT_MAX = 200\n    MU_GRID = np.arange(MU_MAX + 1)\n    PT_GRID = np.arange(PT_MAX + 1)\n\n    # --- Helper functions for physical models and calculations ---\n\n    def get_normalized_poisson(lam, grid):\n        \"\"\"Computes a normalized Poisson PMF on a truncated grid.\"\"\"\n        dist = poisson.pmf(grid, lam)\n        dist_sum = np.sum(dist)\n        if dist_sum > 0:\n            return dist / dist_sum\n        return dist\n\n    def s_func(p_T, mu, T0, a, b):\n        \"\"\"Computes the trigger turn-on probability (logistic function).\"\"\"\n        # Using np.clip to prevent overflow in np.exp for large negative arguments\n        arg = np.clip(-(p_T - T0 - a * mu) / b, -700, 700)\n        return 1.0 / (1.0 + np.exp(arg))\n\n    def f_spec(p_T, p0, n):\n        \"\"\"Computes the unnormalized pT spectrum (modified power law).\"\"\"\n        # Adding a small epsilon to p0 to avoid division by zero if p0=0\n        return (1.0 + p_T / (p0 + 1e-9))**(-n)\n\n    def process_case(case_params):\n        \"\"\"\n        Processes a single test case and returns D_curve, Q, and Delta_Q.\n        \"\"\"\n        lambda_mc, lambda_data, delta, T0, a, b, p0, n = case_params\n\n        # --- Pre-calculate constant matrices/vectors for the case ---\n\n        # 2D matrix of trigger probabilities S(pT, mu)\n        # S_matrix[i, j] = S(pT_i, mu_j)\n        S_matrix = s_func(PT_GRID[:, None], MU_GRID[None, :], T0, a, b)\n        \n        # Normalized offline pT spectrum\n        f_values = f_spec(PT_GRID, p0, n)\n        f_sum = np.sum(f_values)\n        f_norm = f_values / f_sum if f_sum > 0 else f_values\n\n        # --- Main Calculations ---\n\n        # Task 1: Pileup Distributions\n        p_mc = get_normalized_poisson(lambda_mc, MU_GRID)\n        p_data_nominal = get_normalized_poisson(lambda_data, MU_GRID)\n\n        # Task 2: Weighted Distribution\n        # As per the problem, p_w is the normalized p_data distribution.\n        p_w_nominal = p_data_nominal\n\n        # Task 3: Trigger Curves and D_curve\n        eps_before = S_matrix @ p_mc\n        eps_after_nominal = S_matrix @ p_w_nominal\n        D_curve = np.sqrt(np.mean((eps_after_nominal - eps_before)**2))\n\n        # Task 4: Physics Observable Q\n        Q_nominal = np.dot(f_norm, eps_after_nominal)\n\n        # Task 5: Uncertainty Propagation\n        lambda_data_up = lambda_data * (1.0 + delta)\n        lambda_data_down = lambda_data * (1.0 - delta)\n\n        p_w_up = get_normalized_poisson(lambda_data_up, MU_GRID)\n        p_w_down = get_normalized_poisson(lambda_data_down, MU_GRID)\n        \n        eps_after_up = S_matrix @ p_w_up\n        eps_after_down = S_matrix @ p_w_down\n        \n        Q_up = np.dot(f_norm, eps_after_up)\n        Q_down = np.dot(f_norm, eps_after_down)\n        \n        delta_Q = max(abs(Q_up - Q_nominal), abs(Q_down - Q_nominal))\n\n        return D_curve, Q_nominal, delta_Q\n        \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (30, 40, 0.05, 28, 0.35, 3.0, 35, 5.0),    # Case 1\n        (2, 3, 0.02, 20, 0.10, 2.0, 20, 4.0),       # Case 2\n        (50, 20, 0.10, 30, 0.50, 5.0, 40, 6.0),    # Case 3\n    ]\n\n    all_results = []\n    for case in test_cases:\n        D, Q, dQ = process_case(case)\n        all_results.extend([D, Q, dQ])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{val:.6f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond correcting the overall pileup level, modern experiments actively mitigate its impact by identifying and removing contributions from pileup interactions on an object-by-object basis. This practice delves into a powerful technique that uses precision timing information to distinguish tracks originating from the primary interaction from those produced in pileup events. By modeling the timing distributions from first principles, you will derive and compute key performance metrics like the Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) to quantify the effectiveness of this pileup rejection strategy .",
            "id": "3528655",
            "problem": "You are tasked with constructing a principled simulation and analytic study of pileup mitigation using precision timing in computational high-energy physics. The objective is to model timing calibration residuals and quantify their effect on rejecting tracks not originating from the Primary Vertex (PV). You must compute Receiver Operating Characteristic (ROC) curves for varying timing resolutions and analyze false associations to the PV.\n\nBase the derivation on the following fundamental definitions and facts:\n- A time measurement residual is modeled as a random variable with a Gaussian (Normal) distribution. If a detector has an intrinsic timing noise with standard deviation $\\sigma_{\\mathrm{det}}$ and a calibration residual $\\delta t$ with standard deviation $\\sigma_{\\mathrm{cal}}$, then the total timing resolution is $\\sigma_t$ obeying the root-sum-square relation $\\,\\sigma_t^2 = \\sigma_{\\mathrm{det}}^2 + \\sigma_{\\mathrm{cal}}^2\\,$ due to independence of Gaussian contributions.\n- The time offset between a non-PV (pileup) interaction and the PV time is modeled as a Gaussian random variable with standard deviation $\\sigma_b$, representing the bunch time spread.\n- For a track, its measured time relative to the PV is the sum of its true time offset (zero for PV tracks, distributed with spread $\\sigma_b$ for pileup tracks) and the total timing noise. Classify a track as originating from the PV if the absolute measured time proximity to the PV is less than or equal to a cut threshold $\\tau$.\n- The Receiver Operating Characteristic (ROC) is defined by the True Positive Rate (TPR) as a function of the False Positive Rate (FPR) as the decision threshold $\\tau$ is varied. Here:\n  - True Positive Rate (TPR): the fraction of PV tracks correctly classified as PV.\n  - False Positive Rate (FPR): the fraction of pileup tracks incorrectly classified as PV.\n- The Area Under Curve (AUC) for the ROC is the probability that a randomly drawn PV score is more favorable (i.e., smaller absolute time proximity) than a randomly drawn pileup score.\n\nWork in purely mathematical terms with the following modeling assumptions:\n- The total timing noise is Gaussian with zero mean and standard deviation $\\sigma_t$.\n- The pileup time offset relative to the PV is Gaussian with zero mean and standard deviation $\\sigma_b$ and is independent of the timing noise.\n- The discriminant score for each track is the absolute value of the measured time proximity to the PV, and the classification threshold is $\\tau$ applied to this absolute value.\n- The time quantities are expressed in picoseconds (ps), but the output metrics are unitless probabilities.\n\nImplement a program that, for each specified parameter set, performs the following:\n1. Compute the ROC curve points over a dense grid of threshold values $\\tau$ and the corresponding AUC, derived from first principles under the above Gaussian assumptions.\n2. Compute the False Positive Rate (FPR) at three specific thresholds: $\\tau = 0.5\\,\\sigma_t$, $\\tau = 1.0\\,\\sigma_t$, and $\\tau = 2.0\\,\\sigma_t$.\n3. Aggregate and report, for each parameter set, the AUC and the three FPR values as decimal fractions.\n\nUse the following test suite of parameter sets, each specified by $(\\sigma_{\\mathrm{det}}, \\sigma_{\\mathrm{cal}}, \\sigma_b)$, all in picoseconds:\n- Case $1$: $(\\sigma_{\\mathrm{det}}, \\sigma_{\\mathrm{cal}}, \\sigma_b) = (\\,20\\,\\mathrm{ps},\\,10\\,\\mathrm{ps},\\,180\\,\\mathrm{ps}\\,)$.\n- Case $2$: $(\\sigma_{\\mathrm{det}}, \\sigma_{\\mathrm{cal}}, \\sigma_b) = (\\,30\\,\\mathrm{ps},\\,30\\,\\mathrm{ps},\\,180\\,\\mathrm{ps}\\,)$.\n- Case $3$: $(\\sigma_{\\mathrm{det}}, \\sigma_{\\mathrm{cal}}, \\sigma_b) = (\\,20\\,\\mathrm{ps},\\,60\\,\\mathrm{ps},\\,180\\,\\mathrm{ps}\\,)$.\n- Case $4$: $(\\sigma_{\\mathrm{det}}, \\sigma_{\\mathrm{cal}}, \\sigma_b) = (\\,0\\,\\mathrm{ps},\\,0\\,\\mathrm{ps},\\,180\\,\\mathrm{ps}\\,)$.\n- Case $5$: $(\\sigma_{\\mathrm{det}}, \\sigma_{\\mathrm{cal}}, \\sigma_b) = (\\,150\\,\\mathrm{ps},\\,100\\,\\mathrm{ps},\\,180\\,\\mathrm{ps}\\,)$.\n\nNumerical and output requirements:\n- Express all threshold computations in picoseconds internally.\n- All reported probabilities must be expressed as decimal fractions.\n- Round all reported floating-point numbers to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The output for each case must be an inner list in the form $[\\,\\mathrm{AUC},\\,\\mathrm{FPR}_{0.5\\sigma_t},\\,\\mathrm{FPR}_{1.0\\sigma_t},\\,\\mathrm{FPR}_{2.0\\sigma_t}\\,]$. Aggregate the five inner lists into one outer list, e.g., $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],\\dots]$.\n\nScientific realism and derivation requirements:\n- Begin from the independence of Gaussian contributions and the root-sum-square combination of variances to obtain $\\sigma_t$.\n- Explicitly derive, from the Gaussian model and the absolute-value decision rule, the distributions governing the PV and pileup scores and the resulting TPR and FPR as functions of $\\tau$.\n- Derive the ROC and an analytic expression for the AUC using first principles, not by quoting pre-existing shortcut formulas.",
            "solution": "The user has provided a valid, well-posed, and scientifically grounded problem. The task is to derive and compute key performance metrics for pileup mitigation in high-energy physics using precision timing, based on a Gaussian model for time measurements. The problem is self-contained and its parameters are within a realistic regime for collider experiments.\n\nThe solution proceeds in a stepwise, principled manner, beginning with the foundational statistical models and deriving the expressions for the required metrics: the Receiver Operating Characteristic (ROC) Area Under Curve (AUC) and specific False Positive Rates (FPR).\n\n### 1. Statistical Model of Measured Time\n\nThe problem defines the sources of timing uncertainty. The total timing resolution for a single track, $\\sigma_t$, is the combination of the intrinsic detector noise, $\\sigma_{\\mathrm{det}}$, and calibration residuals, $\\sigma_{\\mathrm{cal}}$. As these are independent Gaussian sources, their variances add in quadrature:\n$$\n\\sigma_t^2 = \\sigma_{\\mathrm{det}}^2 + \\sigma_{\\mathrm{cal}}^2 \\implies \\sigma_t = \\sqrt{\\sigma_{\\mathrm{det}}^2 + \\sigma_{\\mathrm{cal}}^2}\n$$\nA measured time, $t_{meas}$, is modeled as the sum of the track's true time of origin, $t_{true}$, and a random measurement error, $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_t^2)$, where $\\mathcal{N}(\\mu, \\sigma^2)$ denotes a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$.\n\nFor a track originating from the Primary Vertex (PV), its true time is $t_{true, PV} = 0$ by definition. The measured time is therefore:\n$$\nt_{meas, PV} = 0 + \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_t^2)\n$$\n\nFor a track from a pileup (PU) interaction, its true time of origin, $t_{true, PU}$, is offset from the PV. This offset is modeled as a Gaussian random variable with zero mean and standard deviation $\\sigma_b$, representing the bunch time spread. Thus, $t_{true, PU} \\sim \\mathcal{N}(0, \\sigma_b^2)$. The measured time for a PU track is:\n$$\nt_{meas, PU} = t_{true, PU} + \\epsilon_t\n$$\nSince $t_{true, PU}$ and $\\epsilon_t$ are independent Gaussian variables, their sum is also a Gaussian. The mean of the sum is the sum of the means ($0+0=0$), and the variance of the sum is the sum of the variances. Let's denote the total effective standard deviation for PU tracks as $\\sigma_{PU}$:\n$$\n\\sigma_{PU}^2 = \\sigma_b^2 + \\sigma_t^2 \\implies \\sigma_{PU} = \\sqrt{\\sigma_b^2 + \\sigma_t^2}\n$$\nTherefore, the distribution of measured times for PU tracks is:\n$$\nt_{meas, PU} \\sim \\mathcal{N}(0, \\sigma_{PU}^2)\n$$\n\n### 2. Derivation of True Positive and False Positive Rates\n\nA track is classified as originating from the PV if its measured time proximity to the PV, $|t_{meas}|$, is less than or equal to a decision threshold $\\tau$.\n\nThe True Positive Rate, $TPR(\\tau)$, is the probability that a PV track is correctly classified:\n$$\nTPR(\\tau) = P(|t_{meas, PV}| \\le \\tau) = P(-\\tau \\le t_{meas, PV} \\le \\tau)\n$$\nGiven $t_{meas, PV} \\sim \\mathcal{N}(0, \\sigma_t^2)$, this probability is the integral of the Gaussian PDF from $-\\tau$ to $\\tau$. In terms of the standard normal cumulative distribution function (CDF), $\\Phi(z) = \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} dx$:\n$$\nTPR(\\tau) = \\Phi\\left(\\frac{\\tau}{\\sigma_t}\\right) - \\Phi\\left(\\frac{-\\tau}{\\sigma_t}\\right)\n$$\nUsing the symmetry property $\\Phi(-z) = 1 - \\Phi(z)$, this simplifies to $2\\Phi(\\frac{\\tau}{\\sigma_t}) - 1$. This is equivalent to the error function, $\\text{erf}(z) = \\frac{2}{\\sqrt{\\pi}}\\int_0^z e^{-x^2} dx$, as follows:\n$$\nTPR(\\tau) = \\text{erf}\\left(\\frac{\\tau}{\\sqrt{2}\\sigma_t}\\right)\n$$\n\nThe False Positive Rate, $FPR(\\tau)$, is the probability that a PU track is incorrectly classified as a PV track:\n$$\nFPR(\\tau) = P(|t_{meas, PU}| \\le \\tau) = P(-\\tau \\le t_{meas, PU} \\le \\tau)\n$$\nFollowing the same logic, with $t_{meas, PU} \\sim \\mathcal{N}(0, \\sigma_{PU}^2)$:\n$$\nFPR(\\tau) = \\text{erf}\\left(\\frac{\\tau}{\\sqrt{2}\\sigma_{PU}}\\right)\n$$\nThe ROC curve is the locus of points $(FPR(\\tau), TPR(\\tau))$ for $\\tau \\in [0, \\infty)$.\n\n### 3. Derivation of the Area Under the ROC Curve (AUC)\n\nThe problem defines the AUC as the probability that a randomly drawn PV score is more favorable (i.e., smaller) than a randomly drawn PU score. The discriminant score is $s = |t_{meas}|$. So, we must calculate $AUC = P(|t_{meas, PV}|  |t_{meas, PU}|)$.\n\nLet $X = t_{meas, PV} \\sim \\mathcal{N}(0, \\sigma_t^2)$ and $Y = t_{meas, PU} \\sim \\mathcal{N}(0, \\sigma_{PU}^2)$. We need to compute $P(|X|  |Y|)$. This can be formulated by conditioning on the value of $Y$ and integrating over its distribution:\n$$\nAUC = \\int_{-\\infty}^{\\infty} P(|X|  |y|) f_Y(y) dy\n$$\nwhere $f_Y(y)$ is the PDF of $Y$. The inner probability is $P(-|y|  X  |y|)$, which is precisely the form we saw for the TPR, yielding $\\text{erf}\\left(\\frac{|y|}{\\sqrt{2}\\sigma_t}\\right)$. The PDF for $Y$ is $f_Y(y) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{PU}} e^{-y^2/(2\\sigma_{PU}^2)}$.\nSubstituting these into the integral:\n$$\nAUC = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}\\sigma_{PU}} e^{-y^2/(2\\sigma_{PU}^2)} \\text{erf}\\left(\\frac{|y|}{\\sqrt{2}\\sigma_t}\\right) dy\n$$\nThe integrand is an even function of $y$, so we can integrate from $0$ to $\\infty$ and multiply by $2$:\n$$\nAUC = 2 \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi}\\sigma_{PU}} e^{-y^2/(2\\sigma_{PU}^2)} \\text{erf}\\left(\\frac{y}{\\sqrt{2}\\sigma_t}\\right) dy\n$$\nTo solve this, we use the substitution $u = y/(\\sqrt{2}\\sigma_{PU})$, which gives $y = u\\sqrt{2}\\sigma_{PU}$ and $dy = \\sqrt{2}\\sigma_{PU} du$:\n$$\nAUC = 2 \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi}\\sigma_{PU}} e^{-u^2} \\text{erf}\\left(\\frac{u\\sqrt{2}\\sigma_{PU}}{\\sqrt{2}\\sigma_t}\\right) (\\sqrt{2}\\sigma_{PU} du)\n$$\n$$\nAUC = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{\\infty} e^{-u^2} \\text{erf}\\left(u \\frac{\\sigma_{PU}}{\\sigma_t}\\right) du\n$$\nThis integral has a standard form. Using the identity $\\int_0^\\infty e^{-c^2x^2}\\text{erf}(ax)dx = \\frac{1}{c\\sqrt{\\pi}}\\arctan(\\frac{a}{c})$, with $c=1$ and $a = \\sigma_{PU}/\\sigma_t$:\n$$\nAUC = \\frac{2}{\\sqrt{\\pi}} \\left[ \\frac{1}{1 \\cdot \\sqrt{\\pi}} \\arctan\\left(\\frac{\\sigma_{PU}/\\sigma_t}{1}\\right) \\right]\n$$\nThis simplifies to a remarkably elegant result:\n$$\nAUC = \\frac{2}{\\pi} \\arctan\\left(\\frac{\\sigma_{PU}}{\\sigma_t}\\right) = \\frac{2}{\\pi} \\arctan\\left(\\frac{\\sqrt{\\sigma_t^2 + \\sigma_b^2}}{\\sigma_t}\\right) = \\frac{2}{\\pi} \\arctan\\left(\\sqrt{1 + \\left(\\frac{\\sigma_b}{\\sigma_t}\\right)^2}\\right)\n$$\nIn the ideal limit where $\\sigma_t \\to 0$, the argument of arctan goes to $\\infty$, so $\\arctan(\\infty) = \\pi/2$, and $AUC \\to \\frac{2}{\\pi}(\\frac{\\pi}{2}) = 1$, which corresponds to perfect discrimination.\n\n### 4. Summary of Formulas for Implementation\n\nFor each parameter set $(\\sigma_{\\mathrm{det}}, \\sigma_{\\mathrm{cal}}, \\sigma_b)$:\n1.  Calculate the total timing resolution: $\\sigma_t = \\sqrt{\\sigma_{\\mathrm{det}}^2 + \\sigma_{\\mathrm{cal}}^2}$. If $\\sigma_{\\mathrm{det}} = \\sigma_{\\mathrm{cal}} = 0$, then $\\sigma_t=0$.\n2.  Calculate the effective pileup time spread: $\\sigma_{PU} = \\sqrt{\\sigma_t^2 + \\sigma_b^2}$.\n3.  Calculate the AUC. If $\\sigma_t  0$: $AUC = \\frac{2}{\\pi} \\arctan(\\sigma_{PU}/\\sigma_t)$. If $\\sigma_t=0$, $AUC=1$.\n4.  Calculate the FPR for three thresholds $\\tau_k = k \\cdot \\sigma_t$ for $k \\in \\{0.5, 1.0, 2.0\\}$. The general formula is:\n    $$\n    FPR(\\tau_k) = \\text{erf}\\left(\\frac{k \\cdot \\sigma_t}{\\sqrt{2}\\sigma_{PU}}\\right)\n    $$\n    If $\\sigma_t = 0$, the thresholds are all $0$, and $FPR(0) = \\text{erf}(0) = 0$.\n\nThese formulas will be implemented to compute the required metrics for the $5$ test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\nimport math\n\ndef solve():\n    \"\"\"\n    Computes pileup mitigation metrics based on a Gaussian timing model.\n\n    For each parameter set (sigma_det, sigma_cal, sigma_b), this function\n    calculates:\n    1. The Area Under the ROC Curve (AUC).\n    2. The False Positive Rate (FPR) at three thresholds: tau = 0.5*sigma_t,\n       1.0*sigma_t, and 2.0*sigma_t.\n\n    The derivations are based on first principles as outlined in the solution.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # (sigma_det, sigma_cal, sigma_b) in picoseconds.\n    test_cases = [\n        (20.0, 10.0, 180.0),  # Case 1\n        (30.0, 30.0, 180.0),  # Case 2\n        (20.0, 60.0, 180.0),  # Case 3\n        (0.0, 0.0, 180.0),    # Case 4 (ideal resolution)\n        (150.0, 100.0, 180.0) # Case 5\n    ]\n\n    results_as_strings = []\n\n    for case in test_cases:\n        sigma_det, sigma_cal, sigma_b = case\n\n        # Step 1: Calculate total timing resolution (sigma_t)\n        sigma_t_sq = sigma_det**2 + sigma_cal**2\n        sigma_t = np.sqrt(sigma_t_sq)\n\n        # Handle the ideal case of perfect timing resolution (sigma_t = 0)\n        if sigma_t == 0.0:\n            # AUC is 1, representing perfect discrimination.\n            auc = 1.0\n            # Thresholds are tau = k * sigma_t = 0.\n            # FPR at tau=0 is erf(0) = 0.\n            fpr_0_5 = 0.0\n            fpr_1_0 = 0.0\n            fpr_2_0 = 0.0\n        else:\n            # Step 2: Calculate effective pileup time spread (sigma_PU)\n            sigma_pu_sq = sigma_t_sq + sigma_b**2\n            sigma_pu = np.sqrt(sigma_pu_sq)\n\n            # Step 3: Calculate AUC\n            # AUC = (2/pi) * arctan(sigma_PU / sigma_t)\n            auc = (2.0 / np.pi) * np.arctan(sigma_pu / sigma_t)\n\n            # Step 4: Calculate FPRs at specified thresholds\n            # FPR(tau) = erf(tau / (sqrt(2) * sigma_PU))\n            # Thresholds are tau_k = k * sigma_t\n            \n            # For tau = 0.5 * sigma_t\n            arg_0_5 = (0.5 * sigma_t) / (np.sqrt(2) * sigma_pu)\n            fpr_0_5 = erf(arg_0_5)\n\n            # For tau = 1.0 * sigma_t\n            arg_1_0 = (1.0 * sigma_t) / (np.sqrt(2) * sigma_pu)\n            fpr_1_0 = erf(arg_1_0)\n\n            # For tau = 2.0 * sigma_t\n            arg_2_0 = (2.0 * sigma_t) / (np.sqrt(2) * sigma_pu)\n            fpr_2_0 = erf(arg_2_0)\n\n        # Aggregate and format results for the current case\n        current_result = [auc, fpr_0_5, fpr_1_0, fpr_2_0]\n        \n        # Format each number to 6 decimal places and join into a string \"[v1,v2,...]\"\n        result_str_list = [f\"{x:.6f}\" for x in current_result]\n        case_string = f\"[{','.join(result_str_list)}]\"\n        results_as_strings.append(case_string)\n\n    # Final print statement in the exact required format.\n    # The format is '[[...],[...],...]'\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Pileup mitigation techniques and their underlying calibrations are never perfect; they carry their own systematic uncertainties that must be accounted for in any final physics measurement. This practice provides a powerful framework for understanding this connection by propagating uncertainties from several pileup-related nuisance parameters into a simplified physics fit. You will use the Asimov dataset and the Fisher information matrix formalism to calculate the total uncertainty on a signal strength parameter and, critically, to assess the individual impact of each source of pileup uncertainty . This exercise bridges the gap between detector-level corrections and the precision of a final physics result.",
            "id": "3528710",
            "problem": "You are tasked with implementing a principled, end-to-end uncertainty propagation for a simplified high-energy physics fit with pileup mitigation nuisance parameters. Starting from a Poisson counting model and Gaussian-constrained nuisance parameters, you must construct the expected Fisher information at the nominal point, invert it to obtain the covariance of the physics parameter of interest, and assess the impact of each nuisance on the uncertainty of the physics parameter.\n\nFundamental base and definitions:\n- The total expected yield in each bin $j$ is modeled as a Poisson mean $\\lambda_j(\\mu,\\theta)$, where $\\mu$ is a dimensionless physics signal strength scaling a known signal template and $\\theta$ are nuisance parameters describing pileup (multiple proton-proton interactions per bunch crossing) mitigation uncertainties.\n- For Poisson observations with means $\\lambda_j(\\mu,\\theta)$ and an Asimov dataset at the nominal point, the expected Fisher information matrix element for parameters $\\vartheta_a$ and $\\vartheta_b$ is\n$$\nI_{ab} \\;=\\; \\sum_{j} \\frac{1}{\\lambda_j(\\mu_0,\\theta_0)} \\left( \\frac{\\partial \\lambda_j}{\\partial \\vartheta_a} \\right)_{\\mu_0,\\theta_0} \\left( \\frac{\\partial \\lambda_j}{\\partial \\vartheta_b} \\right)_{\\mu_0,\\theta_0} \\;+\\; \\Pi_{ab},\n$$\nwhere $\\Pi$ is the prior precision matrix from Gaussian constraints on nuisance parameters. The covariance matrix is the inverse $C = I^{-1}$, and the marginal standard deviation of $\\mu$ is $\\sigma_\\mu = \\sqrt{C_{\\mu\\mu}}$.\n- The nuisance parameters considered are:\n  - $\\rho$ scale $s_\\rho$ (dimensionless), defined as a multiplicative scale on the pileup density correction; prior $s_\\rho \\sim \\mathcal{N}(1, \\delta_\\rho)$, with $\\delta_\\rho$ a dimensionless width.\n  - Timing resolution $\\sigma_t$ (in picoseconds), parameterizing neutral-particle time-of-flight rejection; prior $\\sigma_t \\sim \\mathcal{N}(\\sigma_t^0, \\delta_t)$.\n  - Track-to-vertex efficiency $\\varepsilon_{\\mathrm{tv}}$ (dimensionless, fraction), parameterizing the probability of correct charged-track association to the primary vertex; prior $\\varepsilon_{\\mathrm{tv}} \\sim \\mathcal{N}(\\varepsilon_{\\mathrm{tv}}^0, \\delta_e)$.\n- The effect of these nuisances on the background expectation in bin $j$ is modeled by a first-order Taylor expansion around nominal values:\n$$\nb_j(s_\\rho,\\sigma_t,\\varepsilon_{\\mathrm{tv}}) \\;\\approx\\; b_j^0 \\left[ 1 + a_j (s_\\rho - 1) + t_j \\frac{\\sigma_t - \\sigma_t^0}{\\sigma_t^0} + e_j \\frac{\\varepsilon_{\\mathrm{tv}} - \\varepsilon_{\\mathrm{tv}}^0}{\\varepsilon_{\\mathrm{tv}}^0} \\right],\n$$\nwhere $b_j^0$ is the nominal background in bin $j$; $a_j$, $t_j$, and $e_j$ are dimensionless sensitivity coefficients describing how background changes with each nuisance to first order. The expected Poisson mean is\n$$\n\\lambda_j(\\mu,s_\\rho,\\sigma_t,\\varepsilon_{\\mathrm{tv}}) \\;=\\; \\mu\\, s_j \\;+\\; b_j(s_\\rho,\\sigma_t,\\varepsilon_{\\mathrm{tv}}),\n$$\nwith $s_j$ the nominal signal in bin $j$. The nominal parameter point is $\\mu_0 = 1$, $s_{\\rho,0}=1$, $\\sigma_{t,0}=\\sigma_t^0$, $\\varepsilon_{\\mathrm{tv},0}=\\varepsilon_{\\mathrm{tv}}^0$. Gaussian prior precisions contribute additively to the Fisher matrix as diagonal elements $\\Pi_{s_\\rho s_\\rho} = 1/\\delta_\\rho^2$, $\\Pi_{\\sigma_t \\sigma_t} = 1/\\delta_t^2$, and $\\Pi_{\\varepsilon_{\\mathrm{tv}}\\varepsilon_{\\mathrm{tv}}} = 1/\\delta_e^2$, with zero off-diagonal prior correlations.\n\nRequired derivations to implement:\n- Compute the Asimov expectations at the nominal point, $\\lambda_{j,0} = \\lambda_j(\\mu_0,s_{\\rho,0},\\sigma_{t,0},\\varepsilon_{\\mathrm{tv},0})$.\n- Derive the partial derivatives at the nominal point:\n$$\n\\frac{\\partial \\lambda_j}{\\partial \\mu} = s_j,\\quad\n\\frac{\\partial \\lambda_j}{\\partial s_\\rho} = b_j^0\\, a_j,\\quad\n\\frac{\\partial \\lambda_j}{\\partial \\sigma_t} = b_j^0\\, t_j \\frac{1}{\\sigma_t^0},\\quad\n\\frac{\\partial \\lambda_j}{\\partial \\varepsilon_{\\mathrm{tv}}} = b_j^0\\, e_j \\frac{1}{\\varepsilon_{\\mathrm{tv}}^0}.\n$$\n- Assemble the Fisher information matrix as the sum over bins of rank-$1$ contributions plus the prior precision matrix, invert it to obtain the covariance matrix, and report $\\sigma_\\mu$.\n- Define the impact of fixing a single nuisance parameter $k \\in \\{ s_\\rho, \\sigma_t, \\varepsilon_{\\mathrm{tv}} \\}$ on the uncertainty in $\\mu$ as the quadrature component\n$$\n\\Delta_k \\;=\\; \\sqrt{\\max\\!\\left(0,\\, \\sigma_{\\mu,\\mathrm{all}}^2 \\;-\\; \\sigma_{\\mu,\\mathrm{fixed}\\;k}^2 \\right)}.\n$$\nFixing a nuisance is to be implemented by taking its prior precision to be effectively infinite (e.g., adding a very large diagonal prior element), which corresponds to conditioning on that nuisance being known.\n\nImplementation constraints and units:\n- All timing quantities (both $\\sigma_t^0$ and $\\delta_t$) must be in picoseconds.\n- All outputs must be reported as decimal floats rounded to six places after the decimal point.\n- No angles are involved.\n- All quantities are dimensionless unless specified.\n\nTest suite to implement and evaluate:\n- Case $1$ (three bins, moderate uncertainties):\n  - $s = [30,22,12]$, $b^0 = [100,80,60]$, $a = [0.15,0.20,0.25]$, $t = [-0.10,-0.06,-0.04]$, $e = [-0.30,-0.20,-0.15]$.\n  - Priors: $\\delta_\\rho = 0.10$, $\\sigma_t^0 = 50$ ps, $\\delta_t = 10$ ps, $\\varepsilon_{\\mathrm{tv}}^0 = 0.95$, $\\delta_e = 0.02$.\n- Case $2$ (three bins, no timing sensitivity):\n  - $s = [5,5,5]$, $b^0 = [200,150,100]$, $a = [0.05,0.04,0.03]$, $t = [0,0,0]$, $e = [-0.10,-0.08,-0.06]$.\n  - Priors: $\\delta_\\rho = 0.05$, $\\sigma_t^0 = 30$ ps, $\\delta_t = 5$ ps, $\\varepsilon_{\\mathrm{tv}}^0 = 0.98$, $\\delta_e = 0.01$.\n- Case $3$ (four bins, signal dominated, weak priors):\n  - $s = [120,90,70,50]$, $b^0 = [60,45,35,25]$, $a = [0.20,0.20,0.20,0.20]$, $t = [-0.05,-0.05,-0.05,-0.05]$, $e = [-0.15,-0.15,-0.15,-0.15]$.\n  - Priors: $\\delta_\\rho = 0.20$, $\\sigma_t^0 = 80$ ps, $\\delta_t = 40$ ps, $\\varepsilon_{\\mathrm{tv}}^0 = 0.90$, $\\delta_e = 0.05$.\n\nProgram requirements:\n- Implement the expected Fisher information construction, covariance inversion, and uncertainty extraction as specified above.\n- For each test case, compute:\n  - $\\sigma_{\\mu,\\mathrm{all}}$ with all nuisances floating under their Gaussian priors.\n  - $\\sigma_{\\mu,\\mathrm{fixed}\\;s_\\rho}$, $\\sigma_{\\mu,\\mathrm{fixed}\\;\\sigma_t}$, and $\\sigma_{\\mu,\\mathrm{fixed}\\;\\varepsilon_{\\mathrm{tv}}}$, by effectively fixing each nuisance one at a time via an infinite prior precision.\n  - The impacts $\\Delta_{s_\\rho}$, $\\Delta_{\\sigma_t}$, and $\\Delta_{\\varepsilon_{\\mathrm{tv}}}$ as defined above.\n- Final output format: Your program should produce a single line of output containing all results, flattened across cases, as a comma-separated list enclosed in square brackets. For each case, append in order the seven floats\n  $[\\sigma_{\\mu,\\mathrm{all}}, \\sigma_{\\mu,\\mathrm{fixed}\\;s_\\rho}, \\sigma_{\\mu,\\mathrm{fixed}\\;\\sigma_t}, \\sigma_{\\mu,\\mathrm{fixed}\\;\\varepsilon_{\\mathrm{tv}}}, \\Delta_{s_\\rho}, \\Delta_{\\sigma_t}, \\Delta_{\\varepsilon_{\\mathrm{tv}}}]$,\n  each rounded to six decimal places. The final output is thus a single list with $21$ floats for the three cases.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Model**: The total expected yield in bin $j$ is a Poisson mean $\\lambda_j(\\mu, \\theta)$, where $\\mu$ is the signal strength and $\\theta$ are nuisance parameters.\n- **Fisher Information**: The expected Fisher information matrix for parameters $\\vartheta_a, \\vartheta_b$ is $I_{ab} = \\sum_{j} \\frac{1}{\\lambda_j(\\mu_0,\\theta_0)} \\left( \\frac{\\partial \\lambda_j}{\\partial \\vartheta_a} \\right)_{\\mu_0,\\theta_0} \\left( \\frac{\\partial \\lambda_j}{\\partial \\vartheta_b} \\right)_{\\mu_0,\\theta_0} + \\Pi_{ab}$.\n- **Covariance**: Covariance matrix $C = I^{-1}$. Marginal standard deviation of $\\mu$ is $\\sigma_\\mu = \\sqrt{C_{\\mu\\mu}}$.\n- **Nuisance Parameters (NPs)**:\n    - $\\rho$ scale $s_\\rho$: prior $s_\\rho \\sim \\mathcal{N}(1, \\delta_\\rho)$.\n    - Timing resolution $\\sigma_t$: prior $\\sigma_t \\sim \\mathcal{N}(\\sigma_t^0, \\delta_t)$.\n    - Track-to-vertex efficiency $\\varepsilon_{\\mathrm{tv}}$: prior $\\varepsilon_{\\mathrm{tv}} \\sim \\mathcal{N}(\\varepsilon_{\\mathrm{tv}}^0, \\delta_e)$.\n- **Background Model**: $b_j(s_\\rho,\\sigma_t,\\varepsilon_{\\mathrm{tv}}) \\approx b_j^0 \\left[ 1 + a_j (s_\\rho - 1) + t_j \\frac{\\sigma_t - \\sigma_t^0}{\\sigma_t^0} + e_j \\frac{\\varepsilon_{\\mathrm{tv}} - \\varepsilon_{\\mathrm{tv}}^0}{\\varepsilon_{\\mathrm{tv}}^0} \\right]$.\n- **Total Yield Model**: $\\lambda_j(\\mu,s_\\rho,\\sigma_t,\\varepsilon_{\\mathrm{tv}}) = \\mu s_j + b_j(s_\\rho,\\sigma_t,\\varepsilon_{\\mathrm{tv}})$.\n- **Nominal Point**: $\\mu_0 = 1$, $s_{\\rho,0}=1$, $\\sigma_{t,0}=\\sigma_t^0$, $\\varepsilon_{\\mathrm{tv},0}=\\varepsilon_{\\mathrm{tv}}^0$.\n- **Prior Precision Matrix ($\\Pi$) Elements**: $\\Pi_{s_\\rho s_\\rho} = 1/\\delta_\\rho^2$, $\\Pi_{\\sigma_t \\sigma_t} = 1/\\delta_t^2$, $\\Pi_{\\varepsilon_{\\mathrm{tv}}\\varepsilon_{\\mathrm{tv}}} = 1/\\delta_e^2$. Off-diagonal elements are zero. No prior on $\\mu$.\n- **Partial Derivatives at Nominal Point**:\n    - $\\frac{\\partial \\lambda_j}{\\partial \\mu} = s_j$\n    - $\\frac{\\partial \\lambda_j}{\\partial s_\\rho} = b_j^0\\, a_j$\n    - $\\frac{\\partial \\lambda_j}{\\partial \\sigma_t} = b_j^0\\, t_j \\frac{1}{\\sigma_t^0}$\n    - $\\frac{\\partial \\lambda_j}{\\partial \\varepsilon_{\\mathrm{tv}}} = b_j^0\\, e_j \\frac{1}{\\varepsilon_{\\mathrm{tv}}^0}$\n- **Asimov Expectation**: $\\lambda_{j,0} = \\lambda_j(\\mu_0,s_{\\rho,0},\\sigma_{t,0},\\varepsilon_{\\mathrm{tv},0})$.\n- **Impact of Fixing NP $k$**: $\\Delta_k = \\sqrt{\\max(0, \\sigma_{\\mu,\\mathrm{all}}^2 - \\sigma_{\\mu,\\mathrm{fixed}\\;k}^2)}$. Fixing is implemented by taking the prior precision to be effectively infinite.\n- **Test Cases**: Three sets of parameters are provided.\n- **Output Format**: Single-line list of $21$ floats, rounded to six decimal places.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is based on the well-established statistical methodology of likelihood-based inference using the Fisher information matrix approximation, a standard practice in experimental high-energy physics for sensitivity studies (Asimov dataset). The modeling of nuisance parameters and their effects, while simplified, is representative of real-world scenarios in pileup mitigation. The problem is free of pseudoscience.\n2.  **Well-Posed**: The problem is mathematically well-defined. It requires the construction and inversion of a matrix derived from provided inputs. This process leads to a unique, stable, and meaningful solution for each test case.\n3.  **Objective**: The language is precise and quantitative. All terms are formally defined within the context of statistical modeling.\n4.  **Completeness and Consistency**: The problem provides all necessary data and formulas. The given partial derivatives are consistent with the defined model for $\\lambda_j$. The setup is self-contained.\n5.  **Feasibility**: The required calculations are standard linear algebra operations, feasible to implement numerically. The provided data values are physically plausible within a simplified model context.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-posed, scientifically grounded exercise in statistical data analysis commonly found in physics. A full solution will be provided.\n\nThe objective is to compute the uncertainty on a signal strength parameter, $\\sigma_\\mu$, and the impact of several nuisance parameters on this uncertainty. This requires constructing and inverting the Fisher information matrix. The parameter vector for our model is $\\vartheta = (\\mu, s_\\rho, \\sigma_t, \\varepsilon_{\\mathrm{tv}})$. The Fisher information matrix $I$ is a $4 \\times 4$ matrix for these parameters.\n\n**1. Fisher Information Matrix Construction**\n\nThe Fisher information matrix $I$ is the sum of two components: a statistical part $I^{\\mathrm{stat}}$ derived from the Poisson likelihood of the binned counts, and a prior part $\\Pi$ from the Gaussian constraints on the nuisance parameters.\n$I_{ab} = I^{\\mathrm{stat}}_{ab} + \\Pi_{ab}$\nThe statistical part is given by:\n$$\nI^{\\mathrm{stat}}_{ab} = \\sum_{j} \\frac{1}{\\lambda_{j,0}} \\left( \\frac{\\partial \\lambda_j}{\\partial \\vartheta_a} \\right)_{\\vartheta_0} \\left( \\frac{\\partial \\lambda_j}{\\partial \\vartheta_b} \\right)_{\\vartheta_0}\n$$\nwhere the sum runs over all analysis bins $j$, and the evaluation is at the nominal parameter point $\\vartheta_0 = (\\mu_0, s_{\\rho,0}, \\sigma_{t,0}, \\varepsilon_{\\mathrm{tv},0})$.\n\nFirst, we compute the expected yields $\\lambda_{j,0}$ in each bin $j$ for the Asimov dataset, evaluated at the nominal point $\\mu_0=1$, $s_{\\rho,0}=1$, $\\sigma_{t,0}=\\sigma_t^0$, $\\varepsilon_{\\mathrm{tv},0}=\\varepsilon_{\\mathrm{tv}}^0$. At this point, the background expansion simplifies, and $b_j(1, \\sigma_t^0, \\varepsilon_{\\mathrm{tv}}^0) = b_j^0$.\n$$\n\\lambda_{j,0} = \\lambda_j(1, 1, \\sigma_t^0, \\varepsilon_{\\mathrm{tv}}^0) = 1 \\cdot s_j + b_j^0 = s_j + b_j^0\n$$\n\nNext, we establish the vectors of partial derivatives with respect to each parameter, evaluated at the nominal point. Let the vector of derivatives for parameter $\\vartheta_a$ across all bins be $D_a$.\n\\begin{align*}\n(D_\\mu)_j = \\frac{\\partial \\lambda_j}{\\partial \\mu} = s_j \\\\\n(D_{s_\\rho})_j = \\frac{\\partial \\lambda_j}{\\partial s_\\rho} = b_j^0 a_j \\\\\n(D_{\\sigma_t})_j = \\frac{\\partial \\lambda_j}{\\partial \\sigma_t} = b_j^0 t_j \\frac{1}{\\sigma_t^0} \\\\\n(D_{\\varepsilon_{\\mathrm{tv}}})_j = \\frac{\\partial \\lambda_j}{\\partial \\varepsilon_{\\mathrm{tv}}} = b_j^0 e_j \\frac{1}{\\varepsilon_{\\mathrm{tv}}^0}\n\\end{align*}\nUsing these derivative vectors, an element of the statistical information matrix can be written as an inner product:\n$$\nI^{\\mathrm{stat}}_{ab} = \\sum_j \\frac{(D_a)_j (D_b)_j}{\\lambda_{j,0}}\n$$\n\nThe prior precision matrix $\\Pi$ is diagonal, as the nuisance parameters are assumed to have uncorrelated Gaussian priors. The signal strength $\\mu$ has no prior.\n$$\n\\Pi = \\mathrm{diag}\\left(0, \\frac{1}{\\delta_\\rho^2}, \\frac{1}{\\delta_t^2}, \\frac{1}{\\delta_e^2}\\right)\n$$\n\nThe full Fisher information matrix $I$ is then assembled by adding $I^{\\mathrm{stat}}$ and $\\Pi$.\n\n**2. Covariance and Uncertainty Calculation ($\\sigma_{\\mu,\\mathrm{all}}$)**\n\nThe covariance matrix $C$ is the inverse of the Fisher information matrix, $C = I^{-1}$. The variance of the parameter $\\mu$ is the diagonal element corresponding to $\\mu$, which is the first element in our chosen parameter ordering.\n$$\n\\sigma_{\\mu,\\mathrm{all}}^2 = C_{\\mu\\mu} = (I^{-1})_{\\mu\\mu}\n$$\nThe standard deviation, or uncertainty, is the square root of the variance:\n$$\n\\sigma_{\\mu,\\mathrm{all}} = \\sqrt{C_{\\mu\\mu}}\n$$\n\n**3. Uncertainty with a Fixed Nuisance Parameter ($\\sigma_{\\mu,\\mathrm{fixed}\\;k}$)**\n\nTo calculate the uncertainty on $\\mu$ when a specific nuisance parameter $k \\in \\{s_\\rho, \\sigma_t, \\varepsilon_{\\mathrm{tv}}\\}$ is held fixed, we modify the Fisher information matrix. Fixing a parameter is equivalent to knowing its value perfectly, which corresponds to zero uncertainty on that parameter. In the Bayesian framework, this is achieved by imposing an infinitely precise prior. The precision is the inverse of the variance, so we let the prior precision $\\Pi_{kk} \\to \\infty$.\n\nNumerically, we replace the corresponding diagonal element of the prior matrix $\\Pi$ with a very large number (e.g., $10^{30}$). For example, to fix $s_\\rho$, we construct a modified prior precision matrix:\n$$\n\\Pi_{\\mathrm{fixed}\\;s_\\rho} = \\mathrm{diag}\\left(0, \\infty, \\frac{1}{\\delta_t^2}, \\frac{1}{\\delta_e^2}\\right)\n$$\nWe then form the modified Fisher matrix $I_{\\mathrm{fixed}\\;s_\\rho} = I^{\\mathrm{stat}} + \\Pi_{\\mathrm{fixed}\\;s_\\rho}$, invert it to get $C_{\\mathrm{fixed}\\;s_\\rho}$, and extract the new uncertainty $\\sigma_{\\mu,\\mathrm{fixed}\\;s_\\rho} = \\sqrt{(C_{\\mathrm{fixed}\\;s_\\rho})_{\\mu\\mu}}$. This procedure is repeated for each nuisance parameter.\n\n**4. Impact Calculation ($\\Delta_k$)**\n\nThe impact of a nuisance parameter $k$ on the uncertainty of $\\mu$ is defined as the amount by which the total variance on $\\mu$ is reduced when that nuisance parameter is fixed. This is calculated as a difference in quadrature:\n$$\n\\Delta_k = \\sqrt{\\max(0, \\sigma_{\\mu,\\mathrm{all}}^2 - \\sigma_{\\mu,\\mathrm{fixed}\\;k}^2)}\n$$\nThe $\\max(0, \\dots)$ function ensures that the result is non-negative, protecting against small numerical floating-point inaccuracies that might make the difference slightly negative. This calculation quantifies the contribution of each nuisance parameter to the total uncertainty on the signal strength parameter $\\mu$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    # Test cases defined in the problem statement.\n    test_cases = [\n        {\n            # Case 1 (three bins, moderate uncertainties)\n            's': np.array([30, 22, 12]),\n            'b0': np.array([100, 80, 60]),\n            'a': np.array([0.15, 0.20, 0.25]),\n            't': np.array([-0.10, -0.06, -0.04]),\n            'e': np.array([-0.30, -0.20, -0.15]),\n            'delta_rho': 0.10,\n            'sigma_t0': 50.0,\n            'delta_t': 10.0,\n            'eps_tv0': 0.95,\n            'delta_e': 0.02,\n        },\n        {\n            # Case 2 (three bins, no timing sensitivity)\n            's': np.array([5, 5, 5]),\n            'b0': np.array([200, 150, 100]),\n            'a': np.array([0.05, 0.04, 0.03]),\n            't': np.array([0.0, 0.0, 0.0]),\n            'e': np.array([-0.10, -0.08, -0.06]),\n            'delta_rho': 0.05,\n            'sigma_t0': 30.0,\n            'delta_t': 5.0,\n            'eps_tv0': 0.98,\n            'delta_e': 0.01,\n        },\n        {\n            # Case 3 (four bins, signal dominated, weak priors)\n            's': np.array([120, 90, 70, 50]),\n            'b0': np.array([60, 45, 35, 25]),\n            'a': np.array([0.20, 0.20, 0.20, 0.20]),\n            't': np.array([-0.05, -0.05, -0.05, -0.05]),\n            'e': np.array([-0.15, -0.15, -0.15, -0.15]),\n            'delta_rho': 0.20,\n            'sigma_t0': 80.0,\n            'delta_t': 40.0,\n            'eps_tv0': 0.90,\n            'delta_e': 0.05,\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = compute_uncertainties_and_impacts(case)\n        all_results.extend(results)\n\n    # Format the final output string as required.\n    formatted_results = [f\"{r:.6f}\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef compute_uncertainties_and_impacts(params: dict) -> list:\n    \"\"\"\n    Computes the uncertainties and impacts for a single set of parameters.\n\n    The order of parameters in the Fisher matrix is:\n    0: mu (signal strength)\n    1: s_rho (rho scale)\n    2: sigma_t (timing resolution)\n    3: eps_tv (track-to-vertex efficiency)\n    \"\"\"\n    # Large number to represent infinite precision for fixing a parameter.\n    FIX_PRECISION = 1.0e30\n\n    # Extract parameters from the dictionary\n    s, b0, a, t, e = params['s'], params['b0'], params['a'], params['t'], params['e']\n    delta_rho, sigma_t0, delta_t, eps_tv0, delta_e = \\\n        params['delta_rho'], params['sigma_t0'], params['delta_t'], params['eps_tv0'], params['delta_e']\n\n    # 1. Compute nominal yields and partial derivatives\n    lambda_0 = s + b0\n    \n    d_mu = s\n    d_s_rho = b0 * a\n    # Handle division by zero if sigma_t0 is zero, although not in test cases\n    d_sigma_t = (b0 * t) / sigma_t0 if sigma_t0 != 0 else np.zeros_like(b0)\n    d_eps_tv = (b0 * e) / eps_tv0 if eps_tv0 != 0 else np.zeros_like(b0)\n    \n    derivatives = [d_mu, d_s_rho, d_sigma_t, d_eps_tv]\n    \n    # 2. Construct the statistical part of the Fisher information matrix\n    n_params = len(derivatives)\n    I_stat = np.zeros((n_params, n_params))\n    for i in range(n_params):\n        for j in range(i, n_params):\n            # Sum over bins\n            val = np.sum(derivatives[i] * derivatives[j] / lambda_0)\n            I_stat[i, j] = val\n            if i != j:\n                I_stat[j, i] = val\n\n    # 3. Define prior precision matrices and compute uncertainties\n    # Base prior precisions\n    pi_s_rho = 1.0 / delta_rho**2\n    pi_sigma_t = 1.0 / delta_t**2\n    pi_eps_tv = 1.0 / delta_e**2\n    \n    prior_precisions = [\n        np.diag([0, pi_s_rho, pi_sigma_t, pi_eps_tv]),        # All floating\n        np.diag([0, FIX_PRECISION, pi_sigma_t, pi_eps_tv]),   # s_rho fixed\n        np.diag([0, pi_s_rho, FIX_PRECISION, pi_eps_tv]),     # sigma_t fixed\n        np.diag([0, pi_s_rho, pi_sigma_t, FIX_PRECISION])     # eps_tv fixed\n    ]\n\n    variances = []\n    for Pi in prior_precisions:\n        I_full = I_stat + Pi\n        try:\n            C = np.linalg.inv(I_full)\n            variances.append(C[0, 0])\n        except np.linalg.LinAlgError:\n            # Should not happen with well-posed problems\n            variances.append(np.inf)\n            \n    sigma_mu_all_sq = variances[0]\n    sigma_mu_all = np.sqrt(sigma_mu_all_sq)\n    \n    sigma_mu_fixed_s_rho = np.sqrt(variances[1])\n    sigma_mu_fixed_sigma_t = np.sqrt(variances[2])\n    sigma_mu_fixed_eps_tv = np.sqrt(variances[3])\n\n    # 4. Compute impacts\n    delta_s_rho = np.sqrt(max(0, sigma_mu_all_sq - sigma_mu_fixed_s_rho**2))\n    delta_sigma_t = np.sqrt(max(0, sigma_mu_all_sq - sigma_mu_fixed_sigma_t**2))\n    delta_eps_tv = np.sqrt(max(0, sigma_mu_all_sq - sigma_mu_fixed_eps_tv**2))\n    \n    return [\n        sigma_mu_all,\n        sigma_mu_fixed_s_rho,\n        sigma_mu_fixed_sigma_t,\n        sigma_mu_fixed_eps_tv,\n        delta_s_rho,\n        delta_sigma_t,\n        delta_eps_tv\n    ]\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}