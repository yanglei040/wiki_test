{
    "hands_on_practices": [
        {
            "introduction": "Understanding the receiver operating characteristic (ROC) curve begins with its mathematical foundation. When the probability distributions of a classifier's score are known for both signal and background events, we can derive the ROC curve's exact parametric form. This exercise  guides you through this fundamental process for a scenario with Laplace-distributed scores, providing a concrete link between the statistical properties of the score and the resulting classifier performance, including its concavity and total area under the curve (AUC).",
            "id": "3529673",
            "problem": "In a binary classification task for protonâ€“proton collision events in computational high-energy physics, a scalar score $S$ is used to separate a signal hypothesis (e.g., Higgs boson production) from a background hypothesis (e.g., Quantum Chromodynamics multijet). Assume that, after a physics-motivated calibration, the conditional distributions of $S$ under the two hypotheses are Laplace with a common scale parameter $b>0$ but different means: the signal score follows $S \\mid \\text{signal} \\sim \\mathrm{Laplace}(\\mu_{s}, b)$ and the background score follows $S \\mid \\text{background} \\sim \\mathrm{Laplace}(\\mu_{b}, b)$, with $\\mu_{s} > \\mu_{b}$. The Laplace probability density function is $f(s \\mid \\mu, b) = \\frac{1}{2 b} \\exp\\!\\left(-\\frac{|s - \\mu|}{b}\\right)$.\n\nA decision rule classifies an event as signal if $S \\geq t$, where $t \\in \\mathbb{R}$ is a threshold. Starting only from the definitions of true positive rate $\\,\\mathrm{TPR}(t) = \\mathbb{P}(S \\geq t \\mid \\text{signal})\\,$ and false positive rate $\\,\\mathrm{FPR}(t) = \\mathbb{P}(S \\geq t \\mid \\text{background})\\,$, and the Laplace distribution specified above:\n\n1. Derive the parametric receiver operating characteristic (ROC) curve given by $\\,(\\mathrm{FPR}(t), \\mathrm{TPR}(t))\\,$ as $t$ varies over $\\mathbb{R}$. Your derivation must account for all regimes of $t$ relative to $\\mu_{b}$ and $\\mu_{s}$ and express the ROC as functions of $\\,\\mathrm{FPR}\\,$ and $\\,\\mathrm{TPR}\\,$ without leaving unevaluated probabilities.\n2. Characterize the concavity of this ROC by computing $\\,\\frac{d\\,\\mathrm{TPR}}{d\\,\\mathrm{FPR}}\\,$ and $\\,\\frac{d^{2}\\,\\mathrm{TPR}}{d\\,\\mathrm{FPR}^{2}}\\,$ wherever they are defined, and determining the sign of the second derivative.\n3. Compute the exact area under the ROC curve (AUC) as a closed-form analytic expression in terms of $\\Delta \\equiv \\mu_{s} - \\mu_{b} > 0$ and $b > 0$.\n\nExpress your final answer as the exact closed-form expression for the AUC in terms of $\\Delta$ and $b$.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in statistical decision theory applied to a plausible scenario in high-energy physics. All necessary information is provided, and the problem is free of contradictions or vagueness. We proceed to the solution.\n\nThe problem requires the derivation of the receiver operating characteristic (ROC) curve, an analysis of its concavity, and the calculation of the area under the curve (AUC) for a binary classifier based on a score $S$. The score $S$ follows a Laplace distribution for both signal and background hypotheses, with a common scale parameter $b$ but different means $\\mu_{s}$ and $\\mu_{b}$ (where $\\mu_{s} > \\mu_{b}$).\n\nThe probability density function (PDF) of a Laplace distribution is given by $f(s \\mid \\mu, b) = \\frac{1}{2 b} \\exp\\left(-\\frac{|s - \\mu|}{b}\\right)$. The corresponding cumulative distribution function (CDF), $F(s \\mid \\mu, b) = \\int_{-\\infty}^{s} f(x \\mid \\mu, b) dx$, is:\n$$ F(s \\mid \\mu, b) = \\begin{cases} \\frac{1}{2} \\exp\\left(\\frac{s - \\mu}{b}\\right) & \\text{if } s \\leq \\mu \\\\ 1 - \\frac{1}{2} \\exp\\left(-\\frac{s - \\mu}{b}\\right) & \\text{if } s > \\mu \\end{cases} $$\n\nThe true positive rate ($\\mathrm{TPR}$) and false positive rate ($\\mathrm{FPR}$) are defined as functions of the decision threshold $t$:\n$\\mathrm{TPR}(t) = \\mathbb{P}(S \\geq t \\mid \\text{signal}) = 1 - F(t \\mid \\mu_{s}, b)$\n$\\mathrm{FPR}(t) = \\mathbb{P}(S \\geq t \\mid \\text{background}) = 1 - F(t \\mid \\mu_{b}, b)$\n\nUsing the CDF, we can write the explicit expressions for $\\mathrm{TPR}(t)$ and $\\mathrm{FPR}(t)$:\n$$ \\mathrm{TPR}(t) = \\begin{cases} 1 - \\frac{1}{2} \\exp\\left(\\frac{t - \\mu_{s}}{b}\\right) & \\text{if } t \\leq \\mu_{s} \\\\ \\frac{1}{2} \\exp\\left(-\\frac{t - \\mu_{s}}{b}\\right) & \\text{if } t > \\mu_{s} \\end{cases} $$\n$$ \\mathrm{FPR}(t) = \\begin{cases} 1 - \\frac{1}{2} \\exp\\left(\\frac{t - \\mu_{b}}{b}\\right) & \\text{if } t \\leq \\mu_{b} \\\\ \\frac{1}{2} \\exp\\left(-\\frac{t - \\mu_{b}}{b}\\right) & \\text{if } t > \\mu_{b} \\end{cases} $$\n\n**1. Derivation of the Parametric ROC Curve**\n\nThe ROC curve is the set of points $(\\mathrm{FPR}(t), \\mathrm{TPR}(t))$ for $t \\in (-\\infty, \\infty)$. We denote $\\mathrm{FPR}$ by $x$ and $\\mathrm{TPR}$ by $y$. To find the relationship $y(x)$, we eliminate the parameter $t$. Since $\\mu_{s} > \\mu_{b}$, we consider three regimes for $t$. Let $\\Delta \\equiv \\mu_{s} - \\mu_{b} > 0$.\n\n**Regime 1: $t > \\mu_{s}$** (which implies $t > \\mu_{b}$)\nIn this case:\n$x = \\frac{1}{2} \\exp\\left(-\\frac{t - \\mu_{b}}{b}\\right)$\n$y = \\frac{1}{2} \\exp\\left(-\\frac{t - \\mu_{s}}{b}\\right)$\nFrom the expression for $y$, we have $2y = \\exp(-t/b) \\exp(\\mu_{s}/b)$, which gives $\\exp(-t/b) = 2y \\exp(-\\mu_{s}/b)$.\nSubstituting this into the expression for $x$:\n$x = \\frac{1}{2} \\left[2y \\exp\\left(-\\frac{\\mu_{s}}{b}\\right)\\right] \\exp\\left(\\frac{\\mu_{b}}{b}\\right) = y \\exp\\left(\\frac{\\mu_{b} - \\mu_{s}}{b}\\right) = y \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\nThus, $y = x \\exp\\left(\\frac{\\Delta}{b}\\right)$.\nThe range of $x$ for this regime is found by considering the limits for $t$. As $t \\to \\infty$, $x \\to 0$. When $t=\\mu_s$, $x = \\frac{1}{2} \\exp\\left(-\\frac{\\mu_s - \\mu_b}{b}\\right) = \\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\nSo, this segment applies for $x \\in \\left[0, \\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right]$.\n\n**Regime 2: $\\mu_{b} < t \\leq \\mu_{s}$**\nIn this case:\n$x = \\frac{1}{2} \\exp\\left(-\\frac{t - \\mu_{b}}{b}\\right)$\n$y = 1 - \\frac{1}{2} \\exp\\left(\\frac{t - \\mu_{s}}{b}\\right)$\nFrom the expression for $x$, $2x = \\exp(-t/b) \\exp(\\mu_{b}/b)$, so $\\exp(t/b) = \\frac{1}{2x} \\exp(\\mu_{b}/b)$.\nSubstitute this into the expression for $y$:\n$y = 1 - \\frac{1}{2} \\left[\\frac{1}{2x} \\exp\\left(\\frac{\\mu_{b}}{b}\\right)\\right] \\exp\\left(-\\frac{\\mu_{s}}{b}\\right) = 1 - \\frac{1}{4x} \\exp\\left(\\frac{\\mu_{b} - \\mu_{s}}{b}\\right)$.\nThus, $y = 1 - \\frac{1}{4x} \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\nThe range of $x$ is from $t=\\mu_s$ (lower bound on $x$, as found before) to $t=\\mu_b$. At $t=\\mu_b$, $x = \\frac{1}{2}\\exp(0) = \\frac{1}{2}$.\nSo, this segment applies for $x \\in \\left(\\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right), \\frac{1}{2}\\right]$.\n\n**Regime 3: $t \\leq \\mu_{b}$** (which implies $t < \\mu_{s}$)\nIn this case:\n$x = 1 - \\frac{1}{2} \\exp\\left(\\frac{t - \\mu_{b}}{b}\\right)$\n$y = 1 - \\frac{1}{2} \\exp\\left(\\frac{t - \\mu_{s}}{b}\\right)$\nFrom the expression for $x$, $2(1-x) = \\exp(t/b) \\exp(-\\mu_{b}/b)$, so $\\exp(t/b) = 2(1-x) \\exp(\\mu_{b}/b)$.\nSubstitute this into the expression for $y$:\n$y = 1 - \\frac{1}{2} \\left[2(1-x) \\exp\\left(\\frac{\\mu_{b}}{b}\\right)\\right] \\exp\\left(-\\frac{\\mu_{s}}{b}\\right) = 1 - (1-x) \\exp\\left(\\frac{\\mu_{b} - \\mu_{s}}{b}\\right)$.\nThus, $y = 1 - (1-x) \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\nThe range of $x$ is from $t=\\mu_b$ ($x=1/2$) to $t \\to -\\infty$ ($x \\to 1$).\nSo, this segment applies for $x \\in \\left(\\frac{1}{2}, 1\\right]$.\n\nThe complete ROC curve $y(x) = \\mathrm{TPR}(\\mathrm{FPR})$ is:\n$$ y(x) = \\begin{cases} x \\exp\\left(\\frac{\\Delta}{b}\\right) & \\text{if } 0 \\leq x \\leq \\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right) \\\\ 1 - \\frac{1}{4x} \\exp\\left(-\\frac{\\Delta}{b}\\right) & \\text{if } \\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right) < x \\leq \\frac{1}{2} \\\\ 1 - (1-x) \\exp\\left(-\\frac{\\Delta}{b}\\right) & \\text{if } \\frac{1}{2} < x \\leq 1 \\end{cases} $$\n\n**2. Characterization of Concavity**\n\nWe compute the first and second derivatives of $y(x)$ with respect to $x$ in each regime.\nFor $x \\in \\left(0, \\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right)$:\n$\\frac{dy}{dx} = \\exp\\left(\\frac{\\Delta}{b}\\right)$\n$\\frac{d^2y}{dx^2} = 0$\n\nFor $x \\in \\left(\\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right), \\frac{1}{2}\\right)$:\n$\\frac{dy}{dx} = \\frac{d}{dx} \\left(1 - \\frac{1}{4} \\exp\\left(-\\frac{\\Delta}{b}\\right) x^{-1}\\right) = \\frac{1}{4x^2} \\exp\\left(-\\frac{\\Delta}{b}\\right)$\n$\\frac{d^2y}{dx^2} = \\frac{d}{dx} \\left(\\frac{1}{4} \\exp\\left(-\\frac{\\Delta}{b}\\right) x^{-2}\\right) = -\\frac{1}{2x^3} \\exp\\left(-\\frac{\\Delta}{b}\\right)$\n\nFor $x \\in \\left(\\frac{1}{2}, 1\\right)$:\n$\\frac{dy}{dx} = \\frac{d}{dx} \\left(1 - \\exp\\left(-\\frac{\\Delta}{b}\\right) + x \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right) = \\exp\\left(-\\frac{\\Delta}{b}\\right)$\n$\\frac{d^2y}{dx^2} = 0$\n\nThe first derivative $\\frac{dy}{dx}$ is continuous across the transition points. The second derivative $\\frac{d^2y}{dx^2}$ is $0$ on the two linear segments and strictly negative on the central segment, since $x>0$, $b>0$, and $\\Delta>0$. A function with a non-positive second derivative (where defined) is concave. The ROC curve is composed of two line segments connected by a strictly concave curve, making the overall curve concave. This is consistent with the general property of ROC curves derived from a likelihood ratio test, which this classifier represents.\n\n**3. Computation of the Area Under the Curve (AUC)**\n\nThe AUC is the integral of the ROC function $y(x)$ from $x=0$ to $x=1$. We split the integral into the three regimes identified above. Let $x_1 = \\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)$ and $x_2 = \\frac{1}{2}$.\n$\\mathrm{AUC} = \\int_{0}^{1} y(x) dx = \\int_{0}^{x_1} y(x) dx + \\int_{x_1}^{x_2} y(x) dx + \\int_{x_2}^{1} y(x) dx$.\n\nIntegral 1:\n$\\int_{0}^{x_1} x \\exp\\left(\\frac{\\Delta}{b}\\right) dx = \\exp\\left(\\frac{\\Delta}{b}\\right) \\left[\\frac{x^2}{2}\\right]_{0}^{x_1} = \\frac{1}{2} \\exp\\left(\\frac{\\Delta}{b}\\right) \\left(\\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right)^2 = \\frac{1}{8} \\exp\\left(\\frac{\\Delta}{b}\\right) \\exp\\left(-\\frac{2\\Delta}{b}\\right) = \\frac{1}{8} \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\n\nIntegral 2:\n$\\int_{x_1}^{x_2} \\left(1 - \\frac{1}{4x} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right) dx = \\left[x - \\frac{1}{4}\\exp\\left(-\\frac{\\Delta}{b}\\right) \\ln(x)\\right]_{x_1}^{x_2}$\n$= \\left(\\frac{1}{2} - \\frac{1}{4}\\exp\\left(-\\frac{\\Delta}{b}\\right) \\ln\\left(\\frac{1}{2}\\right)\\right) - \\left(\\frac{1}{2}\\exp\\left(-\\frac{\\Delta}{b}\\right) - \\frac{1}{4}\\exp\\left(-\\frac{\\Delta}{b}\\right) \\ln\\left(\\frac{1}{2}\\exp\\left(-\\frac{\\Delta}{b}\\right)\\right)\\right)$\nUsing $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(\\exp(-\\Delta/b)) = -\\Delta/b$:\n$= \\frac{1}{2} - \\frac{1}{2}\\exp\\left(-\\frac{\\Delta}{b}\\right) + \\frac{1}{4}\\exp\\left(-\\frac{\\Delta}{b}\\right) \\left( -\\ln\\left(\\frac{1}{2}\\right) + \\ln\\left(\\frac{1}{2}\\right) - \\frac{\\Delta}{b} \\right)$\n$= \\frac{1}{2} - \\frac{1}{2}\\exp\\left(-\\frac{\\Delta}{b}\\right) - \\frac{\\Delta}{4b}\\exp\\left(-\\frac{\\Delta}{b}\\right)$.\n\nIntegral 3:\n$\\int_{x_2}^{1} \\left(1 - (1-x) \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right) dx = \\left[x + \\frac{(1-x)^2}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right]_{1/2}^{1}$\n$= (1 + 0) - \\left(\\frac{1}{2} + \\frac{(1-1/2)^2}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right) = 1 - \\frac{1}{2} - \\frac{1}{8} \\exp\\left(-\\frac{\\Delta}{b}\\right) = \\frac{1}{2} - \\frac{1}{8} \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\n\nSumming the three integrals:\n$\\mathrm{AUC} = \\left(\\frac{1}{8} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right) + \\left(\\frac{1}{2} - \\frac{1}{2}\\exp\\left(-\\frac{\\Delta}{b}\\right) - \\frac{\\Delta}{4b}\\exp\\left(-\\frac{\\Delta}{b}\\right)\\right) + \\left(\\frac{1}{2} - \\frac{1}{8} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right)$\n$\\mathrm{AUC} = (\\frac{1}{2}+\\frac{1}{2}) + \\left(\\frac{1}{8} - \\frac{1}{2} - \\frac{1}{8}\\right)\\exp\\left(-\\frac{\\Delta}{b}\\right) - \\frac{\\Delta}{4b}\\exp\\left(-\\frac{\\Delta}{b}\\right)$\n$\\mathrm{AUC} = 1 - \\frac{1}{2}\\exp\\left(-\\frac{\\Delta}{b}\\right) - \\frac{\\Delta}{4b}\\exp\\left(-\\frac{\\Delta}{b}\\right)$\n$\\mathrm{AUC} = 1 - \\left(\\frac{1}{2} + \\frac{\\Delta}{4b}\\right) \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\n\nThis expression gives the exact area under the ROC curve in terms of the separation of means $\\Delta = \\mu_s - \\mu_b$ and the common scale parameter $b$.",
            "answer": "$$\n\\boxed{1 - \\left(\\frac{1}{2} + \\frac{\\Delta}{4b}\\right) \\exp\\left(-\\frac{\\Delta}{b}\\right)}\n$$"
        },
        {
            "introduction": "While analytical derivations provide insight, in practice, ROC curves are estimated from finite datasets, a process fraught with potential pitfalls. Ensuring the resulting curve provides an unbiased estimate of performance on new data is a cornerstone of rigorous machine learning practice, especially in high-energy physics where datasets can have complex structures. This exercise  hones your ability to design a leak-free cross-validation protocol, correctly handling common complexities like grouped data and event weights to produce a trustworthy performance evaluation.",
            "id": "3529679",
            "problem": "In a binary classification study typical of computational high-energy physics (HEP), you aim to separate Higgs boson signal from background using a score function $s = f_{\\theta}(x)$ learned by a supervised algorithm with parameters $\\theta$. You will evaluate performance using the receiver operating characteristic (ROC) curve, defined as the set of points $\\{(\\mathrm{FPR}(t), \\mathrm{TPR}(t)) : t \\in \\mathbb{R}\\}$, where $\\mathrm{TPR}(t)$ is the true positive rate at threshold $t$ and $\\mathrm{FPR}(t)$ is the false positive rate at threshold $t$. The dataset is $\\mathcal{D} = \\{(x_i, y_i, w_i, g_i)\\}_{i=1}^{N}$ with binary labels $y_i \\in \\{0,1\\}$, event weights $w_i > 0$ reflecting known cross section and integrated luminosity factors, and a group identifier $g_i$ such that multiple entries can share the same $g_i$ when they originate from the same generator-level event but correspond to different detector systematic variations. You plan to use $k$-fold cross-validation (CV) to estimate the ROC.\n\nThe following conditions are known to hold and may be used as fundamental bases:\n- For a given threshold $t$, the weighted definitions $\\mathrm{TPR}(t) = \\dfrac{\\sum_{i: y_i = 1} w_i \\,\\mathbb{I}[s_i > t]}{\\sum_{i: y_i = 1} w_i}$ and $\\mathrm{FPR}(t) = \\dfrac{\\sum_{i: y_i = 0} w_i \\,\\mathbb{I}[s_i > t]}{\\sum_{i: y_i = 0} w_i}$ are unbiased estimators of the corresponding population probabilities when the scores $s_i$ are generated by a model that did not train on, or condition on, the test instances (or their near-duplicates), and when the sample is independent and identically distributed (i.i.d.) with respect to the target population once the weights are applied.\n- Empirical risk on training data is, in expectation, optimistically biased relative to generalization risk whenever the same data influence the learned parameters $\\theta$ and the evaluation, and this bias can increase with model capacity and with any mechanism that leaks test information into training.\n\nYou are told that some prior attempts computed the ROC by concatenating predictions from all $k$ models on all $N$ examples, including in-fold training predictions, and that folds were formed at the example level without respecting the group structure $g_i$, so that near-duplicate systematic variants of the same event often landed in different folds. You must reason from first principles and select all statements that correctly characterize the bias mechanisms in this setting and prescribe a protocol that yields an unbiased ROC estimate for the fixed training procedure.\n\nWhich of the following statements are correct?\n\nA. Using predictions on training examples to populate the ROC induces an optimistic bias, i.e., an upward shift of $\\mathrm{TPR}(t)$ at fixed $\\mathrm{FPR}(t)$, because the same examples contributed to fitting $\\theta$. When event-level near-duplicates are split across folds, the learned function effectively sees information about the test examples during training, further inflating the ROC. This bias can persist regardless of $k$ if in-fold predictions are included.\n\nB. A leak-free protocol is to perform group-aware $k$-fold splitting so that all items sharing the same $g_i$ are assigned to the same fold; train on $k-1$ folds and generate scores only for the held-out fold to obtain out-of-fold predictions for every unique event; pool these out-of-fold predictions across all folds; compute the weighted ROC using $(w_i)$ on this pooled set. If hyperparameters are tuned, nest the tuning within the training folds to avoid bias from reusing the held-out fold.\n\nC. It is acceptable to form the ROC by concatenating predictions for all examples from all folds, including predictions on the training portions of each fold, as the bias vanishes for large $N$ by virtue of cross-validation averaging.\n\nD. Averaging per-fold ROC curves by taking the arithmetic mean of $\\mathrm{TPR}(t)$ at common score thresholds across folds is guaranteed to be equivalent to the ROC computed from the pooled out-of-fold predictions, independently of class imbalance and fold size heterogeneity.\n\nE. When per-event weights $w_i$ are present, they should be ignored in ROC estimation to avoid introducing bias from weighting, since ROC depends only on score rankings.\n\nF. A practical way to eliminate leakage bias while retaining all data for training and evaluation is to randomly permute labels within each fold before computing the ROC; this preserves score distributions and removes dependence between $\\theta$ and evaluation labels.\n\nSelect all that apply.",
            "solution": "The user has provided a problem concerning the estimation of a Receiver Operating Characteristic (ROC) curve using $k$-fold cross-validation in the context of a high-energy physics (HEP) classification task. The problem requires a critical analysis of potential biases and the identification of a correct evaluation protocol.\n\n### Problem Validation\n\nFirst, I will validate the problem statement.\n\n**Step 1: Extract Givens**\n- **Task:** Binary classification (Higgs signal vs. background) using a score function $s = f_{\\theta}(x)$.\n- **Evaluation:** Receiver Operating Characteristic (ROC) curve, defined as the set of points $\\{(\\mathrm{FPR}(t), \\mathrm{TPR}(t)) : t \\in \\mathbb{R}\\}$.\n- **Data:** $\\mathcal{D} = \\{(x_i, y_i, w_i, g_i)\\}_{i=1}^{N}$, where $y_i \\in \\{0,1\\}$ are labels, $w_i > 0$ are event weights, and $g_i$ is a group identifier for related events (e.g., systematic variations).\n- **Validation Scheme:** $k$-fold cross-validation (CV).\n- **Definitions:**\n  - True Positive Rate: $\\mathrm{TPR}(t) = \\dfrac{\\sum_{i: y_i = 1} w_i \\,\\mathbb{I}[s_i > t]}{\\sum_{i: y_i = 1} w_i}$\n  - False Positive Rate: $\\mathrm{FPR}(t) = \\dfrac{\\sum_{i: y_i = 0} w_i \\,\\mathbb{I}[s_i > t]}{\\sum_{i: y_i = 0} w_i}$\n- **Stated Principles:**\n  1. The weighted TPR/FPR estimators are unbiased if scores $s_i$ come from a model that did not train on the test instances and if the weighted sample is i.i.d. with respect to the target population.\n  2. Evaluating a model on the same data used for training results in an optimistically biased performance estimate. This bias increases with model capacity and information leakage from the test set.\n- **Flawed Procedure Described:**\n  1. ROC computed from predictions on all $N$ examples, including in-fold training predictions.\n  2. Folds created at the example level, ignoring the group identifiers $g_i$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is set in a realistic and standard scenario within computational HEP. The concepts of event weights, systematic variations (grouped data), cross-validation, ROC curves, and information leakage are all fundamental and correctly described. The provided definitions and principles are standard in statistical learning. The problem is scientifically sound.\n- **Well-Posed:** The problem is well-defined. It presents a flawed methodology and asks for a diagnosis of the flaws and a description of the correct procedure by evaluating a set of statements. It is structured to have a unique set of correct answers based on established principles.\n- **Objective:** The language is technical, precise, and free of subjectivity.\n\nThe problem statement passes all criteria for validity. There are no scientific or logical contradictions, no missing information required to evaluate the options, and the context is realistic and formalizable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the analysis and solution.\n\n### Derivation and Option-by-Option Analysis\n\nThe goal of $k$-fold cross-validation is to estimate the generalization performance of a model, i.e., its performance on unseen data. The core principle for an unbiased estimate is strict separation between data used for training the model parameters $\\theta$ and data used for evaluating its performance. Any violation of this principle leads to bias, typically optimistic.\n\n**Analysis of Option A:**\nThis statement identifies two sources of optimistic bias in the described flawed procedure.\n\n1.  **Using predictions on training examples:** A model $f_{\\theta}$ is optimized to minimize error on its training data. Consequently, it will almost invariably perform better on this data than on new, unseen data. Using these \"in-fold\" predictions to compute an ROC curve measures training performance, not generalization performance. This leads to an ROC curve that is shifted upwards and to the left (higher $\\mathrm{TPR}(t)$ for a given $\\mathrm{FPR}(t)$) compared to the true generalization curve. This is a classic manifestation of overfitting. The statement correctly identifies this as an optimistic bias.\n\n2.  **Splitting near-duplicates across folds:** The data contains groups of examples with the same identifier $g_i$, corresponding to systematic variations of the same underlying generator-level event. These examples are highly correlated. If a standard random split is used, it is highly probable that one variation of an event lands in the training set of a fold, while another variation of the *same* event lands in the corresponding test set. The model, having been trained on a near-duplicate of a test example, will likely perform exceptionally well on it. This compromises the \"unseen\" nature of the test set, constitutes a form of information leakage, and further inflates the performance estimate, leading to optimistic bias.\n\nThe statement that this bias can persist regardless of the number of folds $k$ if in-fold predictions are included is also correct. The inclusion of training performance is a fundamental methodological flaw, not a statistical artifact that can be averaged away or reduced by changing $k$.\n\nTherefore, statement A correctly diagnoses the problems in the described procedure.\n\n**Verdict on A: Correct**\n\n**Analysis of Option B:**\nThis statement prescribes a protocol to obtain a leak-free ROC estimate. Let's analyze its components.\n\n1.  **Group-aware $k$-fold splitting:** It proposes ensuring all items with the same $g_i$ are in the same fold. This directly remedies the information leakage from near-duplicates described in the analysis of option A. This ensures that when a model is trained, it has seen no variations of any event that appears in its corresponding test set. This is the correct way to handle grouped or clustered data in cross-validation.\n\n2.  **Out-of-fold predictions:** It states that for each fold, one trains on the other $k-1$ folds and generates scores *only* for the held-out fold. This is the fundamental principle of cross-validation for performance estimation: evaluation must be done on data that was not used for training.\n\n3.  **Pooling predictions:** The protocol is to collect these out-of-fold predictions for all $N$ examples (each example is in exactly one held-out fold over the course of the $k$ iterations) and then compute a single ROC curve from this pooled set. This is a robust method that uses all data points for the final curve calculation, resulting in a stable estimate. Using the weights $w_i$ is correct, as per the problem's stated principle that the weighted estimators are unbiased for the target population.\n\n4.  **Nested cross-validation for hyperparameter tuning:** It correctly notes that if hyperparameters are tuned, this tuning process must be \"nested\" within the training partition of each fold. This means the held-out fold of the outer loop is not used in any way for hyperparameter selection. This prevents information about the final test set from leaking into the model selection process, which would itself be a source of optimistic bias.\n\nThis statement describes what is considered the best-practice, gold-standard protocol for this exact scenario.\n\n**Verdict on B: Correct**\n\n**Analysis of Option C:**\nThis statement claims that including in-fold (training) predictions is acceptable because the bias \"vanishes for large $N$\". This is fundamentally false. The bias from evaluating on training data is systematic, stemming from the model's optimization process. While the variance of an estimator might decrease with more data points (large $N$), a systematic bias does not simply disappear. The performance on training data is an inherently different quantity from generalization performance. Mixing them will always yield a biased estimate of the latter. The principles provided in the problem statement itself contradict this claim.\n\n**Verdict on C: Incorrect**\n\n**Analysis of Option D:**\nThis statement claims that averaging per-fold ROC curves is \"guaranteed to be equivalent\" to computing the ROC from pooled out-of-fold predictions. This is incorrect. There are two main ways to \"average\" ROC curves:\n1.  **Horizontal/Threshold averaging:** Average TPRs at common score thresholds $t$. This is invalid because a given threshold $t$ can correspond to very different FPRs for different models trained on different folds.\n2.  **Vertical averaging:** Average TPRs at common FPR values. This is more meaningful but is still not equivalent to pooling.\n\nLet's consider the calculation for the average of ratios versus the ratio of sums (pooling). Let $TP_k(t) = \\sum_{i \\in \\text{fold}_k, y_i=1} w_i \\mathbb{I}[s_i > t]$ and $P_k = \\sum_{i \\in \\text{fold}_k, y_i=1} w_i$.\n- Pooled TPR: $\\mathrm{TPR}_{\\text{pooled}}(t) = \\frac{\\sum_k TP_k(t)}{\\sum_k P_k}$\n- Averaged TPR (at fixed $t$): $\\mathrm{TPR}_{\\text{avg}}(t) = \\frac{1}{k} \\sum_k \\frac{TP_k(t)}{P_k}$\nThese two quantities are not equal in general. They are only equal if the total weight of positives, $P_k$, is the same for every fold $k$. The problem explicitly mentions \"fold size heterogeneity\" and class imbalance, making this condition highly unlikely to be met. Therefore, the claim of guaranteed equivalence is false.\n\n**Verdict on D: Incorrect**\n\n**Analysis of Option E:**\nThis statement suggests ignoring weights $w_i$ because the ROC curve \"depends only on score rankings\". This confuses the unweighted ROC with the weighted ROC. The problem explicitly defines weighted TPR and FPR. In HEP, weights are physically motivated and crucial. They correct the simulated sample distribution to match the real-world data distribution, accounting for factors like production cross sections and detector efficiencies. Ignoring them would mean evaluating the model on a biased sample, leading to an ROC estimate that is not representative of the classifier's performance on the target physical population. The weights are essential to obtain an unbiased estimate, as stated in the problem's principles. Ignoring them introduces bias, it does not avoid it.\n\n**Verdict on E: Incorrect**\n\n**Analysis of Option F:**\nThis statement suggests permuting labels to \"eliminate leakage bias\". This is a misunderstanding of the purpose of a permutation test. Randomly permuting labels breaks any true correlation between the features $x_i$ and the labels $y_i$. Training and evaluating a model on such data estimates its performance under the null hypothesis (i.e., that there is no real signal to be learned). The resulting ROC curve will, on average, lie on the diagonal line $\\mathrm{TPR} = \\mathrm{FPR}$. This procedure is useful for assessing the statistical significance of an observed result (is it better than random chance?), but it does not provide a corrected or unbiased estimate of the model's *actual* generalization performance. It throws away the information that the model is supposed to learn.\n\n**Verdict on F: Incorrect**\n\n### Conclusion\n\nBased on the analysis, statements A and B are correct. Statement A correctly identifies sources of bias in a flawed protocol, and statement B correctly prescribes a leak-free, unbiased protocol.",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "A full ROC curve provides a complete picture of classifier performance, but often a single summary statistic is needed for model comparison and optimization. In high-energy physics searches, where extreme background rejection is critical, the standard Area Under the Curve (AUC) can be misleading. This practice  focuses on the partial AUC, a more relevant metric that quantifies performance in the crucial low-false-positive-rate region, and challenges you to implement its calculation from first principles.",
            "id": "3529666",
            "problem": "You are analyzing binary classifiers used in High-Energy Physics (HEP) searches where the signal yield is extremely small and the background rate must be suppressed to ultra-low leakage. In such settings, Receiver Operating Characteristic (ROC) analysis focuses on the region of very small False Positive Rate (FPR) because triggers and selection pipelines must keep the background acceptance near zero while maximizing the discovery potential. Starting from foundational definitions of binary classification under thresholding, derive a mathematically rigorous definition of the partial Area Under the Curve (partial AUC) restricted to the interval of False Positive Rate within $[0,\\alpha]$ for a given $0 \\le \\alpha \\le 1$, and justify why this quantity captures performance in the ultra-low background regime. Then, construct an algorithm to compute this partial AUC from empirical ROC points generated by finite samples of scored events, including support for per-event nonnegative sample weights representing expected yields or livetime scaling factors. Your derivation must start from the definitions of True Positive Rate (TPR) and False Positive Rate (FPR) under thresholded classification and proceed to an integral expression over the ROC curve restricted to $[0,\\alpha]$ without assuming any pre-given formulas.\n\nDefine the following base quantities explicitly and use them consistently:\n\n- A scored dataset consisting of $n$ events indexed by $i \\in \\{1,\\dots,n\\}$, each with a real-valued score $s_i \\in \\mathbb{R}$, a binary label $y_i \\in \\{0,1\\}$ (where $y_i = 1$ denotes signal and $y_i = 0$ denotes background), and an optional nonnegative sample weight $w_i \\in [0,\\infty)$.\n- For any threshold $\\tau \\in \\mathbb{R}$, a classifier that predicts $\\hat{y}_i(\\tau) = 1$ if $s_i \\ge \\tau$ and $\\hat{y}_i(\\tau) = 0$ otherwise.\n- The weighted counts $P = \\sum_{i=1}^n w_i \\,\\mathbb{1}\\{y_i = 1\\}$ and $N = \\sum_{i=1}^n w_i \\,\\mathbb{1}\\{y_i = 0\\}$, assumed finite and strictly positive for meaningful rates, and the corresponding rates\n$$\nTPR(\\tau) = \\frac{\\sum_{i=1}^n w_i \\,\\mathbb{1}\\{y_i = 1,\\, s_i \\ge \\tau\\}}{P}, \\qquad\nFPR(\\tau) = \\frac{\\sum_{i=1}^n w_i \\,\\mathbb{1}\\{y_i = 0,\\, s_i \\ge \\tau\\}}{N}.\n$$\n\nYour program must implement the following:\n\n1. Construct empirical ROC points by sweeping $\\tau$ from $+\\infty$ down to $-\\infty$ over the set of distinct score values $\\{s_i\\}$, aggregating tied scores at identical thresholds. Use the weighted definitions to form a nondecreasing sequence of $FPR$ values in $[0,1]$ and the corresponding $TPR$ values in $[0,1]$, starting at $(0,0)$ and ending at $(1,1)$.\n2. Define the partial AUC over the interval $FPR \\in [0,\\alpha]$ from first principles as the area under the curve traced by the function $TPR(FPR)$ when restricted to $[0,\\alpha]$, and compute it from the empirical ROC points via a mathematically consistent integration procedure that treats the finite-sample ROC as piecewise linear between consecutive empirical points and clips integrals to the interval $[0,\\alpha]$.\n3. Handle ties in scores (multiple events sharing the same $s_i$) by aggregating their contributions at the same threshold before updating rates, and support optional sample weights $w_i$; if weights are not provided, take $w_i = 1$ for all $i$.\n4. Validate that $0 \\le \\alpha \\le 1$; if $P = 0$ or $N = 0$, the rates are ill-defined, but such cases will not appear in the test suite below.\n\nRelevance requirement: Based on the above first-principles definition, explain why the partial AUC concentrated on $FPR \\in [0,\\alpha]$ is the correct figure of merit for searches constrained to ultra-low background leakage (that is, small $FPR$), and why integrating only over that region faithfully reflects discovery sensitivity in the operational regime of HEP triggers and selections.\n\nTest suite and required output: Your program must compute the partial AUC for each of the following parameter sets. For clarity, each list below is ordered; all numbers are given explicitly as decimals, and the same order must be reproduced in the code.\n\n- Case $1$ (balanced, moderate separation, unweighted): scores $[\\,5.1,\\,4.9,\\,3.8,\\,3.5,\\,2.0,\\,4.7,\\,3.9,\\,3.2,\\,1.9,\\,1.0,\\,0.3,\\,-0.5\\,]$, labels $[\\,1,\\,1,\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0\\,]$, $\\alpha = 0.1$, no weights.\n- Case $2$ (same data, boundary $\\alpha = 0$): scores $[\\,5.1,\\,4.9,\\,3.8,\\,3.5,\\,2.0,\\,4.7,\\,3.9,\\,3.2,\\,1.9,\\,1.0,\\,0.3,\\,-0.5\\,]$, labels $[\\,1,\\,1,\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0\\,]$, $\\alpha = 0.0$, no weights.\n- Case $3$ (ties present, full AUC with $\\alpha = 1$): scores $[\\,0.8,\\,0.8,\\,0.2,\\,-0.1,\\,0.8,\\,0.5,\\,0.0,\\,-0.2,\\,-0.2\\,]$, labels $[\\,1,\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,]$, $\\alpha = 1.0$, no weights.\n- Case $4$ (weighted, ultra-low leakage focus): scores $[\\,3.2,\\,2.9,\\,1.7,\\,3.1,\\,2.5,\\,0.7,\\,0.2\\,]$, labels $[\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0\\,]$, weights $[\\,1.0,\\,0.8,\\,0.5,\\,1000.0,\\,200.0,\\,50.0,\\,10.0\\,]$, $\\alpha = 0.001$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4]$), where $r_k$ is the partial AUC for case $k$ expressed as a decimal number. No other text should be printed.",
            "solution": "The problem requires the derivation of a rigorous definition for the partial Area Under the Curve (pAUC), its justification as a key metric in High-Energy Physics (HEP), and the construction of an algorithm to compute it from empirical data. The derivation and algorithm must be based on first principles, starting from the provided definitions of a threshold-based binary classifier.\n\nLet the dataset consist of $n$ events, each described by a tuple $(s_i, y_i, w_i)$ for $i \\in \\{1,\\dots,n\\}$, where $s_i \\in \\mathbb{R}$ is the classifier score, $y_i \\in \\{0,1\\}$ is the true label ($1$ for signal, $0$ for background), and $w_i \\in [0,\\infty)$ is a nonnegative event weight. If weights are not provided, we assume $w_i=1$ for all $i$. The total weighted sum of signal events is $P = \\sum_{i: y_i=1} w_i$ and the total weighted sum of background events is $N = \\sum_{i: y_i=0} w_i$. Both $P$ and $N$ are assumed to be strictly positive.\n\nA decision threshold $\\tau \\in \\mathbb{R}$ classifies events as positive if their score $s_i \\ge \\tau$. The True Positive Rate (TPR) and False Positive Rate (FPR) at this threshold are defined as:\n$$ TPR(\\tau) = \\frac{\\sum_{i=1}^n w_i \\,\\mathbb{1}\\{y_i = 1,\\, s_i \\ge \\tau\\}}{P} $$\n$$ FPR(\\tau) = \\frac{\\sum_{i=1}^n w_i \\,\\mathbb{1}\\{y_i = 0,\\, s_i \\ge \\tau\\}}{N} $$\nwhere $\\mathbb{1}\\{\\cdot\\}$ is the indicator function.\n\nThe Receiver Operating Characteristic (ROC) curve is the set of points $(FPR(\\tau), TPR(\\tau))$ parameterized by $\\tau$ as it sweeps from $+\\infty$ to $-\\infty$. As $\\tau$ decreases, both $FPR(\\tau)$ and $TPR(\\tau)$ are non-decreasing, tracing a path in the unit square from $(0,0)$ to $(1,1)$. The ROC curve can be viewed as a function $TPR(FPR)$.\n\nThe standard Area Under the Curve (AUC) is the integral of this function over its entire domain:\n$$ \\text{AUC} = \\int_{0}^{1} TPR(FPR) \\, d(FPR) $$\nThe partial AUC (pAUC), restricted to the False Positive Rate interval $[0, \\alpha]$ for a given $\\alpha \\in [0, 1]$, is the natural extension of this definition to a limited domain:\n$$ \\text{pAUC}(\\alpha) = \\int_{0}^{\\alpha} TPR(FPR) \\, d(FPR) $$\n\nFor a finite dataset, the functions $TPR(\\tau)$ and $FPR(\\tau)$ are step functions that change values only at the scores $s_i$ present in the data. To construct the empirical ROC curve, we consider the set of unique scores $\\{\\tau_1, \\dots, \\tau_m\\}$ sorted in descending order, $\\tau_1 > \\dots > \\tau_m$. We can define a sequence of ROC points $(x_j, y_j)$ for $j \\in \\{0, 1, \\dots, m\\}$. The starting point is $(x_0, y_0) = (0,0)$, corresponding to a threshold $\\tau > \\tau_1$.\n\nFor each unique score $\\tau_j$, we aggregate the weights of all signal and background events with that score. Let $\\Delta P_{w,j} = \\sum_{i: s_i=\\tau_j, y_i=1} w_i$ and $\\Delta N_{w,j} = \\sum_{i: s_i=\\tau_j, y_i=0} w_i$. We define cumulative weighted counts of true positives ($TP_w$) and false positives ($FP_w$) by summing these contributions as the threshold is lowered. The $j$-th ROC point $(x_j, y_j)$ is generated by considering all scores greater than or equal to $\\tau_j$:\n$$ TP_{w,j} = \\sum_{k=1}^{j} \\Delta P_{w,k} \\qquad FP_{w,j} = \\sum_{k=1}^{j} \\Delta N_{w,k} $$\n$$ x_j = \\frac{FP_{w,j}}{N} \\qquad y_j = \\frac{TP_{w,j}}{P} $$\nThis process generates an ordered sequence of ROC points $(x_0, y_0), (x_1, y_1), \\dots, (x_m, y_m)$, where $0=x_0 \\le x_1 \\le \\dots \\le x_m=1$ and $0=y_0 \\le y_1 \\le \\dots \\le y_m=1$.\n\nThe integral for pAUC can thus be computed using the trapezoidal rule, treating the curve between these empirical points as piecewise linear. The area of one such trapezoid is $\\frac{1}{2}(y_j + y_{j-1})(x_j - x_{j-1})$.\n\n**Relevance to High-Energy Physics:**\nIn High-Energy Physics (HEP) searches for rare phenomena, signal events are vastly outnumbered by background events. For instance, at the Large Hadron Collider (LHC), the rate of uninteresting background collisions can be $10^9$ to $10^{12}$ times higher than the rate of a signal process like Higgs boson production. Consequently, data acquisition systems (triggers) and offline analysis selections must apply extremely stringent criteria to reject background while retaining as much signal as possible.\n\nThis operational constraint means that any useful classifier must perform well in the regime of very small False Positive Rate (FPR), typically $FPR \\ll 10^{-3}$. A classifier's performance at moderate or high FPR (e.g., $FPR > 0.01$) is irrelevant, because such a loose selection would allow an overwhelming number of background events into the final dataset, rendering any signal unobservable. Standard AUC, which integrates over the full range $FPR \\in [0,1]$, averages performance over both relevant and irrelevant regimes. A classifier might achieve a high overall AUC due to excellent performance at high FPR, while being useless for a HEP search due to poor performance at low FPR.\n\nThe partial AUC, $\\text{pAUC}(\\alpha)$ for a small $\\alpha$ (e.g., $\\alpha = 10^{-4}$), resolves this issue. It is a figure of merit that quantifies the classifier's performance exclusively within the operationally accessible, low-FPR region. It measures the average True Positive Rate (signal efficiency) that can be achieved subject to the constraint that the background leakage (FPR) does not exceed $\\alpha$. A higher $\\text{pAUC}(\\alpha)$ in this regime directly indicates a greater ability to separate signal from background under realistic experimental conditions, which translates to enhanced discovery sensitivity or improved precision for measurements. Therefore, it is the correct metric for evaluating and optimizing classifiers for discovery-oriented searches in HEP.\n\nThe algorithm to compute $\\text{pAUC}(\\alpha)$ is as follows:\n1. Initialize $\\text{pAUC} = 0$.\n2. Iterate through the ROC points from $j=1$ to $m$. Let the previous point be $(x_{j-1}, y_{j-1})$ and the current point be $(x_j, y_j)$.\n3. If $x_{j-1} \\ge \\alpha$, the integration range has been fully covered. Terminate the summation.\n4. If $x_j \\le \\alpha$, the entire trapezoid lies within the integration range. Add its full area to the total:\n   $$ \\Delta A = \\frac{1}{2}(y_j + y_{j-1})(x_j - x_{j-1}) $$\n5. If $x_{j-1} < \\alpha < x_j$, this is the final segment intersecting the boundary $\\alpha$. We must clip the trapezoid at $FPR=\\alpha$. The TPR value at this boundary, $y(\\alpha)$, is found by linear interpolation:\n   $$ y(\\alpha) = y_{j-1} + (y_j - y_{j-1})\\frac{\\alpha - x_{j-1}}{x_j - x_{j-1}} $$\n   The area of the clipped trapezoid is:\n   $$ \\Delta A = \\frac{1}{2}(y(\\alpha) + y_{j-1})(\\alpha - x_{j-1}) $$\n   Add this area to the total and then terminate the summation.\nThis procedure provides a mathematically consistent computation of the partial AUC for an empirical, piecewise linear ROC curve.\n\n```python\nimport numpy as np\n\ndef compute_partial_auc(scores: list[float], labels: list[int], alpha: float, weights: list[float] = None) -> float:\n    \"\"\"\n    Computes the partial Area Under the ROC Curve (pAUC) for FPR in [0, alpha].\n\n    Args:\n        scores: A list of real-valued scores from a classifier.\n        labels: A list of binary true labels (1 for signal, 0 for background).\n        alpha: The upper bound of the False Positive Rate interval [0, alpha].\n        weights: An optional list of non-negative weights for each event. If None,\n                 all events are weighted equally (weight = 1).\n\n    Returns:\n        The partial AUC value.\n    \"\"\"\n    if not (0.0 <= alpha <= 1.0):\n        raise ValueError(\"alpha must be between 0 and 1.\")\n\n    scores = np.array(scores, dtype=np.float64)\n    labels = np.array(labels, dtype=np.int32)\n    if weights is None:\n        weights = np.ones_like(scores, dtype=np.float64)\n    else:\n        weights = np.array(weights, dtype=np.float64)\n\n    # Calculate total weight of positives (P) and negatives (N)\n    pos_mask = (labels == 1)\n    neg_mask = (labels == 0)\n    P = np.sum(weights[pos_mask])\n    N = np.sum(weights[neg_mask])\n\n    if P == 0 or N == 0:\n        # Undefined rates, but problem guarantees this won't occur in test suite.\n        return 0.0\n\n    # Get unique scores and sort them in descending order\n    unique_scores = np.unique(scores)[::-1]\n\n    # Generate ROC points\n    roc_points = []\n    tp_w, fp_w = 0.0, 0.0\n    roc_points.append((0.0, 0.0))  # Start at (0,0)\n\n    for score_thresh in unique_scores:\n        # Find all events at the current score threshold\n        at_thresh_mask = (scores == score_thresh)\n        \n        # Sum weights of TPs and FPs at this threshold\n        d_tp = np.sum(weights[at_thresh_mask & pos_mask])\n        d_fp = np.sum(weights[at_thresh_mask & neg_mask])\n\n        tp_w += d_tp\n        fp_w += d_fp\n        roc_points.append((fp_w / N, tp_w / P))\n\n    # Calculate partial AUC using the trapezoidal rule, clipped at alpha\n    p_auc = 0.0\n    prev_x, prev_y = roc_points[0]\n\n    for curr_x, curr_y in roc_points[1:]:\n        if prev_x >= alpha:\n            break\n        \n        if curr_x <= alpha:\n            # Full trapezoid is within the [0, alpha] range\n            p_auc += 0.5 * (curr_y + prev_y) * (curr_x - prev_x)\n        else:\n            # This trapezoid is clipped by alpha.\n            # Linearly interpolate TPR at FPR = alpha\n            if curr_x > prev_x: # Avoid division by zero for vertical segments\n                interp_y = prev_y + (curr_y - prev_y) * (alpha - prev_x) / (curr_x - prev_x)\n            else:\n                interp_y = curr_y # Vertical line, take upper y\n            \n            p_auc += 0.5 * (interp_y + prev_y) * (alpha - prev_x)\n            break\n        \n        prev_x, prev_y = curr_x, curr_y\n\n    return p_auc\n```",
            "answer": "$$\n\\boxed{[0.04,0.0,0.7,0.00043478260869565216]}\n$$"
        }
    ]
}