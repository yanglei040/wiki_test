{
    "hands_on_practices": [
        {
            "introduction": "我们从深入研究提升决策树（BDT）的内部训练机制开始。现代梯度提升方法不仅仅是拟合残差，更是在函数空间中执行梯度下降。本练习将展示如何通过对损失函数进行二阶泰勒展开，推导出每一步提升中最佳更新（即叶节点输出值）的一个简洁而优雅的公式。理解这一推导过程是掌握诸如 XGBoost 等先进算法工作原理的关键，并能让您明白为何正则化不仅是一个附加项，更是保证训练稳定性和性能的内在组成部分。",
            "id": "3506549",
            "problem": "考虑在计算高能物理中使用提升决策树（BDTs）进行二元分类，其中，一棵树被生长以划分事件，每个终端节点（叶节点）输出一个恒定的分数，加到当前模型上。在使用二次可微凸损失函数的梯度提升中，对于一个输出为 $v$ 的单个叶节点，其上的正则化目标函数（针对被路由到该叶节点的事件 $i$）可以通过围绕当前间隔（margin）的二阶泰勒展开来近似。设 $g_i$ 表示事件 $i$ 的损失函数相对于间隔的一阶导数（梯度），$h_i$ 表示事件 $i$ 的二阶导数（Hessian），$\\lambda$ 是叶节点输出的平方 $\\ell_2$ 正则化强度。从正则化目标的二阶展开式出发，并利用凸损失函数产生 $h_i \\ge 0$ 这一事实，推导唯一最小化器 $v$（用 $\\{g_i\\}$, $\\{h_i\\}$ 和 $\\lambda$ 表示），并解释为什么当 $\\sum_i h_i$ 很小时这个选择是数值稳定的，以及 $\\lambda$ 如何控制过拟合的趋势。\n\n实现一个程序，对于每个测试用例，输入一个有限的实数列表 $\\{g_i\\}$、一个有限的实数列表 $\\{h_i\\}$（其中每个 $h_i \\ge 0$）和一个实数 $\\lambda \\ge 0$，并返回推导出的正则化叶节点输出 $v$。为了数值稳健性，采用以下规则：定义 $S_g = \\sum_i g_i$ 和 $S_h = \\sum_i h_i$，如果 $S_h + \\lambda \\le \\varepsilon$ 且 $\\varepsilon = 10^{-12}$，则输出 $0.0$；否则输出推导出的唯一最小化器。当曲率和正则化消失或极小时，此规则确保了安全的输出。\n\n您的程序必须评估以下测试套件：\n- 情况 1：$\\{g_i\\} = [\\,0.8,\\,-0.5,\\,0.1\\,]$，$\\{h_i\\} = [\\,0.25,\\,0.2,\\,0.1\\,]$，$\\lambda = 1.0$。\n- 情况 2：$\\{g_i\\} = [\\,0.1,\\,0.1,\\,-0.1\\,]$，$\\{h_i\\} = [\\,10^{-12},\\,2\\times 10^{-12},\\,0.0\\,]$，$\\lambda = 0.0$。\n- 情况 3：$\\{g_i\\} = [\\,0.1,\\,0.1,\\,-0.1\\,]$，$\\{h_i\\} = [\\,10^{-12},\\,2\\times 10^{-12},\\,0.0\\,]$，$\\lambda = 10^{-6}$。\n- 情况 4：$\\{g_i\\} = [\\,-2.0,\\,1.0,\\,-0.5,\\,0.3\\,]$，$\\{h_i\\} = [\\,0.5,\\,0.4,\\,0.3,\\,0.2\\,]$，$\\lambda = 1000.0$。\n- 情况 5：$\\{g_i\\} = [\\,1000.0,\\,-950.0,\\,25.0,\\,-10.0\\,]$，$\\{h_i\\} = [\\,0.01,\\,0.01,\\,0.01,\\,0.01\\,]$，$\\lambda = 1.0$。\n- 情况 6：$\\{g_i\\} = [\\,0.0,\\,0.0,\\,0.0\\,]$，$\\{h_i\\} = [\\,0.2,\\,0.3,\\,0.1\\,]$，$\\lambda = 0.5$。\n- 情况 7：$\\{g_i\\} = [\\,0.5,\\,-0.5\\,]$，$\\{h_i\\} = [\\,0.0,\\,0.0\\,]$，$\\lambda = 0.0$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如 $[\\,\\text{result}_1,\\text{result}_2,\\ldots\\,]$）。每个结果必须是实数（浮点数）。此计算不涉及任何物理单位或角度，也无需百分比。",
            "solution": "在进行求解之前，对问题陈述的有效性进行了严格评估。\n\n### 第 1 步：提取已知条件\n- **主题**：在计算高能物理中使用提升决策树（BDTs）进行二元分类。\n- **模型组件**：决策树中的单个终端节点（叶节点）。\n- **目标函数**：叶节点的正则化目标通过围绕当前间隔的二阶泰勒展开来近似。\n- **变量**：\n    - $v$：叶节点的恒定输出分数。\n    - $i$：路由到叶节点的事件索引。\n    - $g_i$：事件 $i$ 的损失函数相对于间隔的一阶导数（梯度）。\n    - $h_i$：事件 $i$ 的损失函数相对于间隔的二阶导数（Hessian）。\n- **常数和条件**：\n    - 损失函数是二次可微且凸的，这意味着 $h_i \\ge 0$。\n    - $\\lambda$：叶节点输出的平方 $\\ell_2$ 正则化强度，且 $\\lambda \\ge 0$。\n- **任务**：\n    1.  推导唯一最小化器 $v$（用 $\\{g_i\\}$, $\\{h_i\\}$ 和 $\\lambda$ 表示）。\n    2.  解释为什么当 $\\sum_i h_i$ 很小时这个选择是数值稳定的。\n    3.  解释 $\\lambda$ 如何控制过拟合。\n    4.  实现一个程序，为多个测试用例计算 $v$。\n- **数值稳健性规则**：\n    - 定义 $S_g = \\sum_i g_i$ 和 $S_h = \\sum_i h_i$。\n    - 如果 $S_h + \\lambda \\le \\varepsilon$ 且 $\\varepsilon = 10^{-12}$，则输出 $v$ 必须为 $0.0$。\n    - 否则，输出推导出的唯一最小化器。\n- **测试套件**：\n    - 情况 1：$\\{g_i\\} = [\\,0.8,\\,-0.5,\\,0.1\\,]$，$\\{h_i\\} = [\\,0.25,\\,0.2,\\,0.1\\,]$，$\\lambda = 1.0$。\n    - 情况 2：$\\{g_i\\} = [\\,0.1,\\,0.1,\\,-0.1\\,]$，$\\{h_i\\} = [\\,10^{-12},\\,2\\times 10^{-12},\\,0.0\\,]$，$\\lambda = 0.0$。\n    - 情况 3：$\\{g_i\\} = [\\,0.1,\\,0.1,\\,-0.1\\,]$，$\\{h_i\\} = [\\,10^{-12},\\,2\\times 10^{-12},\\,0.0\\,]$，$\\lambda = 10^{-6}$。\n    - 情况 4：$\\{g_i\\} = [\\,-2.0,\\,1.0,\\,-0.5,\\,0.3\\,]$，$\\{h_i\\} = [\\,0.5,\\,0.4,\\,0.3,\\,0.2\\,]$，$\\lambda = 1000.0$。\n    - 情况 5：$\\{g_i\\} = [\\,1000.0,\\,-950.0,\\,25.0,\\,-10.0\\,]$，$\\{h_i\\} = [\\,0.01,\\,0.01,\\,0.01,\\,0.01\\,]$，$\\lambda = 1.0$。\n    - 情况 6：$\\{g_i\\} = [\\,0.0,\\,0.0,\\,0.0\\,]$，$\\{h_i\\} = [\\,0.2,\\,0.3,\\,0.1\\,]$，$\\lambda = 0.5$。\n    - 情况 7：$\\{g_i\\} = [\\,0.5,\\,-0.5\\,]$，$\\{h_i\\} = [\\,0.0,\\,0.0\\,]$，$\\lambda = 0.0$。\n\n### 第 2 步：使用提取的已知条件进行验证\n根据指定标准对问题进行验证。\n- **科学基础**：该问题描述了在梯度提升机（一种标准且广泛应用的机器学习算法）中确定叶节点权重的核心优化步骤。对损失函数使用二阶泰勒近似、梯度（$g_i$）、Hessian矩阵（$h_i$）和 $\\ell_2$ 正则化，是像 XGBoost 这类算法的基础。将其应用于高能物理是 BDTs 的一个常见且合适的用例。该问题在科学和数学上是合理的。\n- **良态问题**：该问题要求推导一个二次函数的最小化器，这是一个明确定义的数学任务。所有必需量（$g_i, h_i, \\lambda$）和条件（$h_i \\ge 0, \\lambda \\ge 0$）都已提供。只要二次项的系数为正，就保证存在唯一最小值。数值规则处理了系数为零或接近零的情况。因此，存在一个唯一的、有意义的解。\n- **客观性**：问题以精确、无歧义的数学术语陈述。没有主观或基于观点的内容。\n\n该问题没有表现出任何列出的无效性缺陷。它是自洽、一致且有科学依据的。\n\n### 第 3 步：结论与行动\n问题有效。将提供完整解答。\n\n### 解法推导与说明\n\n设路由到特定叶节点的事件集由索引 $i$ 标记。该叶节点的总目标函数 $J(v)$ 是每个事件的近似损失之和，再加上叶节点输出值 $v$ 的正则化项。每个事件 $i$ 的损失通过二阶泰勒展开来近似。正则化项是叶节点输出的平方 $\\ell_2$ 范数，由 $\\lambda$ 缩放。为了数学上的方便，通常在两项中都包含一个因子 $1/2$，以简化求导。\n\n需要对 $v$ 最小化的目标函数是：\n$$\nJ(v) = \\sum_{i \\in \\text{leaf}} \\left( g_i v + \\frac{1}{2} h_i v^2 \\right) + \\frac{1}{2} \\lambda v^2\n$$\n在这里，$g_i v + \\frac{1}{2} h_i v^2$ 是来自事件 $i$ 的损失函数贡献的二阶泰勒近似，不包括对 $v$ 的优化无关的常数项。项 $\\frac{1}{2} \\lambda v^2$ 是 $\\ell_2$ 正则化惩罚。\n\n为了找到最小化 $J(v)$ 的最优值 $v$，我们可以重新组合这些项：\n$$\nJ(v) = \\left( \\sum_i g_i \\right) v + \\frac{1}{2} \\left( \\sum_i h_i + \\lambda \\right) v^2\n$$\n设 $S_g = \\sum_i g_i$ 和 $S_h = \\sum_i h_i$。目标函数简化为：\n$$\nJ(v) = S_g v + \\frac{1}{2} (S_h + \\lambda) v^2\n$$\n这是一个关于 $v$ 的二次函数。为了找到它的极值，我们计算它对 $v$ 的一阶导数并令其为零：\n$$\n\\frac{dJ}{dv} = S_g + (S_h + \\lambda) v = 0\n$$\n求解 $v$ 得到最优叶节点值：\n$$\nv (S_h + \\lambda) = -S_g \\implies v = - \\frac{S_g}{S_h + \\lambda}\n$$\n为确认这是一个最小值，我们检查二阶导数：\n$$\n\\frac{d^2J}{dv^2} = S_h + \\lambda = \\sum_i h_i + \\lambda\n$$\n问题陈述损失函数是凸函数，所以它的二阶导数是非负的，即对所有 $i$ 都有 $h_i \\ge 0$。正则化强度也是非负的，$\\lambda \\ge 0$。因此，它们的和 $\\sum_i h_i$ 以及 $\\sum_i h_i + \\lambda$ 也都是非负的。如果 $\\sum_i h_i + \\lambda > 0$，则二阶导数为正，推导出的 $v$ 是唯一的最小值。如果 $\\sum_i h_i + \\lambda = 0$，则目标函数是 $v$ 的线性函数（如果 $S_g$ 也为 $0$，则为常数），除非 $S_g=0$，否则它没有唯一的有限最小值。\n\n**数值稳定性：**\n推导出的 $v$ 的表达式涉及除以 $S_h + \\lambda$。如果这个分母非常接近于零，$v$ 的值可能会变得极大，导致数值不稳定和大的更新，从而使提升过程发散。这种情况通常发生在叶节点定义的特征空间区域包含的数据点很少，或者在该区域损失函数的曲率很小（$h_i \\approx 0$）且没有使用正则化（$\\lambda = 0$）时。正则化参数 $\\lambda$ 在确保稳定性方面起着至关重要的作用。作为一个非负常数，它将分母“抬离”零，防止了除以一个非常小的数。问题中明确的数值规则——如果 $S_h + \\lambda \\le 10^{-12}$ 则设置 $v = 0.0$——是针对这种不稳定性的一种直接、实用的保障措施，有效地将曲率或正则化不足的情况视为没有更新。\n\n**过拟合控制：**\nBDTs 中的过拟合发生于模型对训练数据（包括其噪声）适应得过于紧密，导致在未见过的数据上表现不佳。这通常表现为单棵树做出非常大的贡献（大的 $|v|$）来纠正少数训练样本的错误分类。$\\ell_2$ 正则化项 $\\frac{1}{2} \\lambda v^2$ 惩罚大的叶节点值。从公式 $v = -S_g / (S_h + \\lambda)$ 可以明显看出，增加正则化强度 $\\lambda$ 会增大大分母的量级。这反过来又将 $v$ 的最优值“收缩”到零。这种收缩降低了每棵树的影响力，迫使算法依赖于更多弱学习器的共识。这个过程使模型对训练集中的噪声不那么敏感，并提高了其泛化能力，从而控制了过拟合。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the BDT leaf value problem for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {'g': [0.8, -0.5, 0.1], 'h': [0.25, 0.2, 0.1], 'lambda': 1.0},\n        # Case 2\n        {'g': [0.1, 0.1, -0.1], 'h': [1e-12, 2e-12, 0.0], 'lambda': 0.0},\n        # Case 3\n        {'g': [0.1, 0.1, -0.1], 'h': [1e-12, 2e-12, 0.0], 'lambda': 1e-6},\n        # Case 4\n        {'g': [-2.0, 1.0, -0.5, 0.3], 'h': [0.5, 0.4, 0.3, 0.2], 'lambda': 1000.0},\n        # Case 5\n        {'g': [1000.0, -950.0, 25.0, -10.0], 'h': [0.01, 0.01, 0.01, 0.01], 'lambda': 1.0},\n        # Case 6\n        {'g': [0.0, 0.0, 0.0], 'h': [0.2, 0.3, 0.1], 'lambda': 0.5},\n        # Case 7\n        {'g': [0.5, -0.5], 'h': [0.0, 0.0], 'lambda': 0.0},\n    ]\n\n    def calculate_leaf_output(g_list, h_list, lam):\n        \"\"\"\n        Calculates the regularized leaf output value v.\n\n        Args:\n            g_list: A list of gradient values {g_i}.\n            h_list: A list of Hessian values {h_i}.\n            lam: The L2 regularization strength lambda.\n\n        Returns:\n            The calculated leaf output value v as a float.\n        \"\"\"\n        # Epsilon for the numerical stability check, as defined in the problem.\n        epsilon = 1e-12\n        \n        # Calculate the sum of gradients (S_g) and Hessians (S_h).\n        # Convert lists to numpy arrays for efficient summation.\n        S_g = np.sum(np.array(g_list))\n        S_h = np.sum(np.array(h_list))\n        \n        denominator = S_h + lam\n        \n        # Apply the numerical robustness rule.\n        if denominator = epsilon:\n            return 0.0\n        else:\n            # Calculate the unique minimizer for the leaf value v.\n            v = -S_g / denominator\n            return v\n\n    results = []\n    for case in test_cases:\n        g = case['g']\n        h = case['h']\n        lam = case['lambda']\n        result = calculate_leaf_output(g, h, lam)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在了解了单个叶节点如何更新之后，我们现在将视角扩展到指导整个训练过程的损失函数上。选择不同的损失函数决定了算法如何惩罚预测错误，这个练习将探讨损失函数的梯度如何决定每个数据点对训练过程的“影响力”。通过分析不同损失函数在处理被严重错分的样本（即离群点）时的行为，您将对为何某些 BDT 变体比其他变体更具鲁棒性建立起深刻的直觉，这对于处理充满噪声的真实高能物理数据至关重要。",
            "id": "3506546",
            "problem": "在计算高能物理中的一个二元分类任务中，考虑训练提升决策树 (BDTs) 来基于特征 $\\mathbf{x}$ 区分信号事件和背景事件。设真实标签为 $y \\in \\{-1,+1\\}$，模型预测为一个实值分数 $f(\\mathbf{x})$，间隔定义为 $m = y f(\\mathbf{x})$。在将梯度提升视为对经验风险进行函数梯度下降的观点中，用于拟合下一棵树的每样本伪残差是损失函数关于预测值的负梯度，即 $r(\\mathbf{x}) = -\\partial L / \\partial f(\\mathbf{x})$。根据链式法则，$\\partial L / \\partial f = (\\partial L / \\partial m) \\cdot (\\partial m/\\partial f) = y \\cdot \\partial L / \\partial m$，因此每个样本对下一个弱学习器的影响大小（在常数因子内）由 $I(m) \\equiv \\left|\\partial L / \\partial m\\right|$ 控制。在此背景下，离群点是指具有非常大负间隔（$m \\to -\\infty$）的事件，例如，由于严重的误重建导致背景事件的得分远大于信号事件的得分，反之亦然。\n\n考虑以下广泛使用的分类损失函数，它们是关于间隔 $m$ 的函数：\n- 指数损失 $L_{\\exp}(m) = \\exp(-m)$。\n- Hinge损失 $L_{\\text{hinge}}(m) = \\max(0, 1 - m)$，其次梯度在 $m  1$ 时为 $\\partial L_{\\text{hinge}}/\\partial m = -1$，在 $m > 1$ 时为 $0$。\n- 逻辑损失 $L_{\\text{log}}(m) = \\log\\!\\big(1 + \\exp(-m)\\big)$。\n- 平方Hinge损失 $L_{\\text{sq}}(m) = \\max(0, 1 - m)^2$。\n\n基于提升更新的影响与 $I(m) = \\left|\\partial L / \\partial m\\right|$ 成正比这一基本原理，通过刻画 $I(m)$ 在 $m \\to -\\infty$ 区域的行为来分析这些损失函数对离群点的鲁棒性。以下哪些陈述是正确的？\n\nA. 对于指数损失 $L_{\\exp}(m) = \\exp(-m)$，影响大小 $I_{\\exp}(m)$ 随着 $m \\to -\\infty$ 无界增长，因此严重误分类的离群点会获得指数级大的权重。\n\nB. 对于Hinge损失 $L_{\\text{hinge}}(m) = \\max(0, 1 - m)$，影响大小 $I_{\\text{hinge}}(m)$ 是有界的，并且对于 $m  1$ 等于 $1$，因此即使是极端的离群点也不会获得超过恒定值的权重。\n\nC. 对于逻辑损失 $L_{\\text{log}}(m) = \\log\\!\\big(1 + \\exp(-m)\\big)$，影响大小 $I_{\\text{log}}(m)$ 随着 $m \\to -\\infty$ 衰减到 $0$，使其对离群点的鲁棒性强于Hinge损失。\n\nD. 对于平方Hinge损失 $L_{\\text{sq}}(m) = \\max(0, 1 - m)^2$，当 $m  1$ 时，影响大小 $I_{\\text{sq}}(m)$ 随 $-m$ 线性增长，因此随着 $m \\to -\\infty$ 是无界的。\n\nE. 在所有损失函数中，梯度提升中对离群点的鲁棒性与 $I(m)$ 在 $m \\to -\\infty$ 时是否保持有界相关，因此具有有界斜率的类Hinge损失比指数损失更鲁棒。",
            "solution": "问题要求分析在梯度提升二元分类中使用的几种常见损失函数的鲁棒性。对离群点（定义为具有大负间隔 $m \\to -\\infty$ 的样本）的鲁棒性，是通过考察影响大小 $I(m) \\equiv \\left|\\partial L / \\partial m\\right|$ 的行为来评估的。输入到下一个弱学习器的伪残差与 $\\partial L / \\partial m$ 成正比，因此 $I(m)$ 量化了在提升更新期间，一个间隔为 $m$ 的样本被赋予的权重大小。如果这个影响大小在 $m \\to -\\infty$ 时保持有界，则认为该损失函数对此类离群点是鲁棒的。我们现在将为每个指定的损失函数计算 $I(m)$，并评估其在此极限下的行为。\n\n让我们分析每个损失函数及相应的陈述。\n\n**指数损失（$L_{\\exp}$）分析**\n指数损失由 $L_{\\exp}(m) = \\exp(-m)$ 给出。\n关于间隔 $m$ 的导数是：\n$$ \\frac{\\partial L_{\\exp}}{\\partial m} = \\frac{\\partial}{\\partial m} \\exp(-m) = -\\exp(-m) $$\n因此，影响大小为：\n$$ I_{\\exp}(m) = \\left| -\\exp(-m) \\right| = \\exp(-m) $$\n我们考察 $m \\to -\\infty$ 时的极限：\n$$ \\lim_{m \\to -\\infty} I_{\\exp}(m) = \\lim_{m \\to -\\infty} \\exp(-m) = +\\infty $$\n随着间隔变得更负，影响呈指数级增长。这意味着严重误分类的样本（离群点）在后续的提升迭代中被赋予指数级增长的权重。\n\n*   **选项A**：“对于指数损失 $L_{\\exp}(m) = \\exp(-m)$，影响大小 $I_{\\exp}(m)$ 随着 $m \\to -\\infty$ 无界增长，因此严重误分类的离群点会获得指数级大的权重。”\n    这个陈述准确地描述了我们推导出的 $I_{\\exp}(m)$ 的行为。\n    **结论：正确。**\n\n**Hinge损失（$L_{\\text{hinge}}$）分析**\nHinge损失由 $L_{\\text{hinge}}(m) = \\max(0, 1 - m)$ 给出。\n对于此分析，我们关心的是 $m \\to -\\infty$ 的区域，它落在 $m  1$ 的定义域内。在此定义域内，$1 - m > 0$，因此 $L_{\\text{hinge}}(m) = 1 - m$。\n对于 $m  1$，关于 $m$ 的导数（或如问题中正确指出的次梯度）是：\n$$ \\frac{\\partial L_{\\text{hinge}}}{\\partial m} = \\frac{\\partial}{\\partial m} (1 - m) = -1 $$\n对于 $m  1$，影响大小为：\n$$ I_{\\text{hinge}}(m) = |-1| = 1 $$\n对于所有 $m  1$，此影响是恒定的。因此，在 $m \\to -\\infty$ 的极限下，影响保持为常数值1。\n\n*   **选项B**：“对于Hinge损失 $L_{\\text{hinge}}(m) = \\max(0, 1 - m)$，影响大小 $I_{\\text{hinge}}(m)$ 是有界的，并且对于 $m  1$ 等于 $1$，因此即使是极端的离群点也不会获得超过恒定值的权重。”\n    这个陈述与我们的分析完全一致。影响是恒定且有界的。\n    **结论：正确。**\n\n**逻辑损失（$L_{\\text{log}}$）分析**\n逻辑损失由 $L_{\\text{log}}(m) = \\log\\!\\big(1 + \\exp(-m)\\big)$ 给出。\n关于间隔 $m$ 的导数使用链式法则计算：\n$$ \\frac{\\partial L_{\\text{log}}}{\\partial m} = \\frac{1}{1 + \\exp(-m)} \\cdot \\frac{\\partial}{\\partial m} (1 + \\exp(-m)) = \\frac{-\\exp(-m)}{1 + \\exp(-m)} $$\n这可以通过分子和分母同乘以 $\\exp(m)$ 来重写：\n$$ \\frac{\\partial L_{\\text{log}}}{\\partial m} = \\frac{-\\exp(-m)\\exp(m)}{(1 + \\exp(-m))\\exp(m)} = \\frac{-1}{\\exp(m) + 1} $$\n影响大小为：\n$$ I_{\\text{log}}(m) = \\left| \\frac{-1}{\\exp(m) + 1} \\right| = \\frac{1}{\\exp(m) + 1} $$\n现在我们考察 $m \\to -\\infty$ 时的极限：\n$$ \\lim_{m \\to -\\infty} I_{\\text{log}}(m) = \\lim_{m \\to -\\infty} \\frac{1}{\\exp(m) + 1} $$\n由于 $\\lim_{m \\to -\\infty} \\exp(m) = 0$，极限变为：\n$$ \\frac{1}{0 + 1} = 1 $$\n影响大小趋向于一个常数值1，就像Hinge损失一样。它不会衰减到0。\n\n*   **选项C**：“对于逻辑损失 $L_{\\text{log}}(m) = \\log\\!\\big(1 + \\exp(-m)\\big)$，影响大小 $I_{\\text{log}}(m)$ 随着 $m \\to -\\infty$ 衰减到 $0$，使其对离群点的鲁棒性强于Hinge损失。”\n    这个陈述是错误的。我们的计算表明 $I_{\\text{log}}(m)$ 趋近于 $1$，而不是 $0$。因此，它对极端离群点的鲁棒性与Hinge损失相当，而不是如描述的那样更优越。\n    **结论：错误。**\n\n**平方Hinge损失（$L_{\\text{sq}}$）分析**\n平方Hinge损失由 $L_{\\text{sq}}(m) = \\max(0, 1 - m)^2$ 给出。\n在我们感兴趣的区域 $m \\to -\\infty$，我们处于 $m  1$ 的定义域，此时 $L_{\\text{sq}}(m) = (1 - m)^2$。\n对于 $m  1$，关于 $m$ 的导数是：\n$$ \\frac{\\partial L_{\\text{sq}}}{\\partial m} = \\frac{\\partial}{\\partial m} (1 - m)^2 = 2(1 - m) \\cdot (-1) = -2(1 - m) $$\n对于 $m  1$，影响大小为：\n$$ I_{\\text{sq}}(m) = |-2(1 - m)| = 2(1 - m) $$\n由于 $m  1$，项 $(1-m)$ 是正的。随着 $m \\to -\\infty$，$(1-m) \\to +\\infty$。增长与 $-m$ 呈线性关系：$2(1-m) = 2 - 2m = 2 + 2(-m)$。\n$$ \\lim_{m \\to -\\infty} I_{\\text{sq}}(m) = \\lim_{m \\to -\\infty} 2(1 - m) = +\\infty $$\n影响呈线性无界增长。\n\n*   **选项D**：“对于平方Hinge损失 $L_{\\text{sq}}(m) = \\max(0, 1 - m)^2$，当 $m  1$ 时，影响大小 $I_{\\text{sq}}(m)$ 随 $-m$ 线性增长，因此随着 $m \\to -\\infty$ 是无界的。”\n    这个陈述准确地反映了我们的推导。影响 $I_{\\text{sq}}(m) = 2(1-m)$ 是 $m$（因此也是 $-m$）的线性函数，并且当 $m \\to -\\infty$ 时是无界的。\n    **结论：正确。**\n\n**关于鲁棒性的一般性陈述分析**\n这个陈述综合了以上发现。问题的假设是，对离群点（$m \\to -\\infty$）的鲁棒性与影响大小 $I(m)$ 是否保持有界相关。\n我们的分析表明：\n-   $I_{\\exp}(m) \\to \\infty$ (指数级地)\n-   $I_{\\text{hinge}}(m) \\to 1$ (有界的)\n-   $I_{\\text{log}}(m) \\to 1$ (有界的)\n-   $I_{\\text{sq}}(m) \\to \\infty$ (线性地)\n\n具有有界影响的损失（Hinge损失、逻辑损失）对极端离群点的敏感度低于具有无界影响的损失（指数损失、平方Hinge损失）。该陈述将“具有有界斜率的类Hinge损失”与指数损失进行了比较。Hinge损失和逻辑损失都符合此描述，因为它们的影响大小是有界的。指数损失具有无界的影响。因此，根据所提供的鲁棒性定义，Hinge损失和逻辑损失比指数损失对离群点更鲁棒。\n\n*   **选项E**：“在所有损失函数中，梯度提升中对离群点的鲁棒性与 $I(m)$ 在 $m \\to -\\infty$ 时是否保持有界相关，因此具有有界斜率的类Hinge损失比指数损失更鲁棒。”\n    陈述的第一部分正确地阐述了在此背景下的鲁棒性原则。第二部分正确地应用了这一原则：Hinge损失（以及逻辑损失）在 $m \\to -\\infty$ 时具有有界斜率（影响），而指数损失则没有。因此，它们更鲁棒。该陈述在概念上是合理的，并与我们的推导一致。\n    **结论：正确。**\n\n总而言之，基于对损失函数导数的严格分析，陈述A、B、D和E是正确的。",
            "answer": "$$\\boxed{ABDE}$$"
        },
        {
            "introduction": "训练完 BDT 分类器后，我们会得到一个为每个事件分配的连续分数。接下来的关键步骤是，我们必须在这个分数上选择一个阈值（或称“工作点”）来区分信号和背景区域。这个练习将演示如何通过数学推导，找到能够最大化特定物理性能指标（在此为加权 $F_1$ 分数）的最优阈值。这项实践将机器学习模型的抽象输出与物理分析的具体目标直接联系起来，因为在物理分析中，优化发现显著性或测量精度通常就等同于最大化一个能够平衡信噪比和效率的特定指标。",
            "id": "3506489",
            "problem": "在计算高能物理中的一项分类分析使用了一种提升决策树（BDT），其输出经过校准以表示信号类别的后验概率，记为 $s \\in [0,1]$。考虑一个包含两类事件的质子-质子对撞测试样本：信号和本底。每个事件都带有一个物理权重，该权重由截面、积分光度和选择效率的乘积得出。测试样本中信号和本底的总加权产额分别由 $W_{S}$ 和 $W_{B}$ 给出。对于信号事件，BDT输出 $s$ 由密度函数 $f_{S}(s) = 2s$（在 $[0,1]$上）建模；对于本底事件，由密度函数 $f_{B}(s) = 2(1-s)$（在 $[0,1]$上）建模，这与一个经过校准的分类器一致，该分类器更倾向于为信号产生较大的 $s$ 值，为本底产生较小的 $s$ 值。\n\n使用以下具有物理动机的参数来计算总加权产额：\n- 积分光度 $L = 100\\,\\mathrm{fb}^{-1}$。\n- 信号截面 $\\sigma_{S} = 0.5\\,\\mathrm{pb}$ 和选择效率 $\\epsilon_{S} = 0.4$。\n- 本底截面 $\\sigma_{B} = 2.0\\,\\mathrm{pb}$ 和选择效率 $\\epsilon_{B} = 0.3$。\n\n假设 $1\\,\\mathrm{pb} = 10^{-12}\\,\\mathrm{b}$ 和 $1\\,\\mathrm{fb}^{-1} = 10^{15}\\,\\mathrm{b}^{-1}$，因此 $W_{S} = \\sigma_{S} L \\epsilon_{S}$ 和 $W_{B} = \\sigma_{B} L \\epsilon_{B}$ 是无量纲的预期事件产额。\n\n定义加权真阳性 $\\mathrm{TP}(t)$、假阳性 $\\mathrm{FP}(t)$ 和假阴性 $\\mathrm{FN}(t)$，其中阈值为 $t \\in [0,1]$，如果 $s \\geq t$，则将事件分类为信号，否则分类为本底。加权 $F_{1}$ 分数由 $F_{1}(t) = \\dfrac{2\\,\\mathrm{TP}(t)}{2\\,\\mathrm{TP}(t) + \\mathrm{FP}(t) + \\mathrm{FN}(t)}$ 给出。从这些定义和给定的密度函数出发，推导出使 $F_{1}(t)$ 最大化的阈值 $t^{\\star}$，并使用所提供的参数计算其数值。将最终阈值表示为四舍五入到四位有效数字的小数。该阈值是无量纲的。",
            "solution": "该问题是有效的，因为它具有科学依据、提法明确且客观。它为高能物理领域机器学习背景下的一个标准优化问题提供了完整且一致的设定。\n\n目标是找到使加权 $F_{1}$ 分数最大化的阈值 $t^{\\star}$，其定义为 $F_{1}(t) = \\dfrac{2\\,\\mathrm{TP}(t)}{2\\,\\mathrm{TP}(t) + \\mathrm{FP}(t) + \\mathrm{FN}(t)}$，其中 $\\mathrm{TP}(t)$、$\\mathrm{FP}(t)$ 和 $\\mathrm{FN}(t)$ 分别是分类阈值为 $t \\in [0,1]$ 时的加权真阳性、假阳性和假阴性。\n\n首先，我们使用给定的参数计算信号（$W_S$）和本底（$W_B$）的总加权产额：\n积分光度 $L = 100\\,\\mathrm{fb}^{-1}$\n信号截面 $\\sigma_{S} = 0.5\\,\\mathrm{pb}$\n信号选择效率 $\\epsilon_{S} = 0.4$\n本底截面 $\\sigma_{B} = 2.0\\,\\mathrm{pb}$\n本底选择效率 $\\epsilon_{B} = 0.3$\n\n产额由公式 $W = \\sigma L \\epsilon$ 给出。使用所提供的转换因子 $1\\,\\mathrm{pb} = 10^{-12}\\,\\mathrm{b}$ 和 $1\\,\\mathrm{fb}^{-1} = 10^{15}\\,\\mathrm{b}^{-1}$：\n$$W_{S} = \\sigma_{S} L \\epsilon_{S} = (0.5 \\times 10^{-12}\\,\\mathrm{b}) \\times (100 \\times 10^{15}\\,\\mathrm{b}^{-1}) \\times 0.4 = 20000$$\n$$W_{B} = \\sigma_{B} L \\epsilon_{B} = (2.0 \\times 10^{-12}\\,\\mathrm{b}) \\times (100 \\times 10^{15}\\,\\mathrm{b}^{-1}) \\times 0.3 = 60000$$\n\n接下来，我们将 $\\mathrm{TP}(t)$、$\\mathrm{FP}(t)$ 和 $\\mathrm{FN}(t)$ 表示为阈值 $t$ 的函数。如果一个事件的BDT分数 $s$ 满足 $s \\geq t$，则该事件被分类为信号。\n\n加权真阳性数 $\\mathrm{TP}(t)$ 是被分类为信号的加权信号事件数。这是总信号产额 $W_S$ 乘以一个信号事件具有 $s \\geq t$ 的概率：\n$$\\mathrm{TP}(t) = W_{S} \\int_{t}^{1} f_{S}(s) \\,ds = W_{S} \\int_{t}^{1} 2s \\,ds = W_{S} [s^2]_{t}^{1} = W_{S}(1 - t^2)$$\n\n加权假阳性数 $\\mathrm{FP}(t)$ 是被分类为信号的加权本底事件数。这是总本底产额 $W_B$ 乘以一个本底事件具有 $s \\geq t$ 的概率：\n$$\\mathrm{FP}(t) = W_{B} \\int_{t}^{1} f_{B}(s) \\,ds = W_{B} \\int_{t}^{1} 2(1-s) \\,ds = W_{B} [2s - s^2]_{t}^{1} = W_{B}((2-1) - (2t-t^2)) = W_{B}(1 - 2t + t^2) = W_{B}(1-t)^2$$\n\n加权假阴性数 $\\mathrm{FN}(t)$ 是被分类为本底（$s  t$）的加权信号事件数。这可以计算为总信号产额减去真阳性数：\n$$\\mathrm{FN}(t) = W_{S} - \\mathrm{TP}(t) = W_{S} - W_{S}(1-t^2) = W_{S}t^2$$\n\n现在我们构建 $F_1(t)$ 函数：\n$$F_{1}(t) = \\frac{2\\,\\mathrm{TP}(t)}{2\\,\\mathrm{TP}(t) + \\mathrm{FP}(t) + \\mathrm{FN}(t)} = \\frac{2 W_{S}(1-t^2)}{2 W_{S}(1-t^2) + W_{B}(1-t)^2 + W_{S}t^2}$$\n为了找到最大值，我们将导数 $\\frac{dF_1(t)}{dt}$ 设为 $0$。设 $N(t) = 2 W_{S}(1-t^2)$ 为分子， $D(t) = 2 W_{S}(1-t^2) + W_{B}(1-t)^2 + W_{S}t^2$ 为分母。当 $N'(t)D(t) - N(t)D'(t) = 0$ 时，导数为零。\n分母可以简化为：\n$$D(t) = 2W_{S} - 2W_{S}t^2 + W_{B}(1-2t+t^2) + W_{S}t^2 = (W_{B} - W_{S})t^2 - 2W_{B}t + (2W_{S} + W_{B})$$\n导数是：\n$$N'(t) = -4W_{S}t$$\n$$D'(t) = 2(W_{B} - W_{S})t - 2W_{B}$$\n将商法则导数的分子设为零：\n$$(-4W_{S}t)[(W_{B} - W_{S})t^2 - 2W_{B}t + (2W_{S} + W_{B})] - [2W_{S}(1-t^2)][2(W_{B} - W_{S})t - 2W_{B}] = 0$$\n假设 $W_S > 0$ 且 $t \\neq 1$，我们可以除以 $-4W_S$：\n$$t[(W_{B} - W_{S})t^2 - 2W_{B}t + (2W_{S} + W_{B})] + (1-t^2)[(W_{B}-W_{S})t - W_B] = 0$$\n展开并合并同类项：\n$$(W_{B}-W_{S})t^3 - 2W_{B}t^2 + (2W_{S}+W_{B})t + (W_{B}-W_{S})t - W_B - (W_{B}-W_{S})t^3 + W_{B}t^2 = 0$$\n三次项消掉了。我们得到了一个二次方程：\n$$(-2W_{B}+W_{B})t^2 + (2W_{S}+W_{B} + W_{B}-W_{S})t - W_B = 0$$\n$$-W_{B}t^2 + (W_{S} + 2W_{B})t - W_{B} = 0$$\n$$W_{B}t^2 - (W_{S} + 2W_{B})t + W_{B} = 0$$\n我们使用二次公式求解 $t$：\n$$t = \\frac{(W_{S} + 2W_{B}) \\pm \\sqrt{(-(W_{S} + 2W_{B}))^2 - 4(W_{B})(W_{B})}}{2W_{B}}$$\n$$t = \\frac{W_{S} + 2W_{B} \\pm \\sqrt{W_{S}^2 + 4W_{S}W_{B} + 4W_{B}^2 - 4W_{B}^2}}{2W_{B}}$$\n$$t = \\frac{W_{S} + 2W_{B} \\pm \\sqrt{W_{S}(W_{S} + 4W_{B})}}{2W_{B}}$$\n解必须在区间 $[0,1]$ 内。\n考虑带加号的解：\n$$t_{+} = \\frac{W_{S} + 2W_{B} + \\sqrt{W_{S}^2 + 4W_{S}W_{B}}}{2W_{B}} = \\frac{W_{S}}{2W_{B}} + 1 + \\frac{\\sqrt{W_{S}^2 + 4W_{S}W_{B}}}{2W_{B}}$$\n因为 $W_S > 0$ 和 $W_B > 0$，所以 $t_{+} > 1$。这个解在物理上是无效的。\n考虑带减号的解：\n$$t^{\\star} = t_{-} = \\frac{W_{S} + 2W_{B} - \\sqrt{W_{S}(W_{S} + 4W_{B})}}{2W_{B}}$$\n为验证 $t^{\\star} \\in [0,1]$，我们观察到 $\\sqrt{W_{S}^2 + 4W_{S}W_{B}}  \\sqrt{W_{S}^2 + 4W_{S}W_{B} + 4W_{B}^2} = W_{S} + 2W_{B}$，所以分子为正，且 $t^{\\star} > 0$。同时，$\\sqrt{W_{S}^2 + 4W_{S}W_{B}} > \\sqrt{W_{S}^2} = W_{S}$，所以 $W_{S} - \\sqrt{W_{S}(W_{S} + 4W_{B})}  0$，这意味着 $W_{S} + 2W_{B} - \\sqrt{W_{S}(W_{S} + 4W_{B})}  2W_{B}$。因此，$t^{\\star}  1$。这是正确的物理解。\n\n现在，我们代入数值 $W_{S}=20000$ 和 $W_{B}=60000$：\n$$t^{\\star} = \\frac{20000 + 2(60000) - \\sqrt{20000(20000 + 4 \\cdot 60000)}}{2(60000)}$$\n$$t^{\\star} = \\frac{140000 - \\sqrt{20000(260000)}}{120000} = \\frac{140000 - \\sqrt{5.2 \\times 10^9}}{120000}$$\n$$t^{\\star} = \\frac{140000 - 10^4\\sqrt{52}}{120000} = \\frac{14 - 2\\sqrt{13}}{12} = \\frac{7 - \\sqrt{13}}{6}$$\n计算最终数值：\n$$t^{\\star} \\approx \\frac{7 - 3.605551275}{6} \\approx \\frac{3.394448725}{6} \\approx 0.56574145$$\n四舍五入到四位有效数字，我们得到 $0.5657$。",
            "answer": "$$\\boxed{0.5657}$$"
        }
    ]
}