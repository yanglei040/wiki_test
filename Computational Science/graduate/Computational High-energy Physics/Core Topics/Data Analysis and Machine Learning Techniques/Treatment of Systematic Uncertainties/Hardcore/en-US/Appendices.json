{
    "hands_on_practices": [
        {
            "introduction": "Before constructing complex likelihood models, it is crucial to understand the fundamental building blocks of uncertainty. This first practice guides you through a classic cross-section measurement scenario, helping you to distinguish between aleatoric (statistical) uncertainty from event counting and epistemic (systematic) uncertainty from an instrument's calibration. By applying first-order error propagation, you will quantify the separate impact of these two sources on the final measurement, a foundational skill in experimental physics .",
            "id": "3540032",
            "problem": "In computational high-energy physics (HEP), the inclusive cross section $\\sigma$ for a process is often estimated from an observed event count and detector acceptance. Consider a measurement in which the observed number of signal events $N$ follows a Poisson distribution due to counting statistics, and the detection efficiency $\\epsilon$ is known from calibration but subject to an unknown small fractional calibration shift. The integrated luminosity $L$ is known exactly. The point estimator used for the cross section is the ratio of the observed yield to the exposure, given by the product of integrated luminosity and the nominal efficiency.\n\nAssume the following measurement model and inputs:\n- The observed count is $N = 1692$.\n- The integrated luminosity is $L = 140\\,\\mathrm{fb}^{-1}$.\n- The nominal efficiency is $\\epsilon_{0} = 0.60$.\n- The true efficiency is related to the nominal by a calibration shift $\\epsilon = \\epsilon_{0}\\left(1 + \\delta\\right)$, where the calibration nuisance $\\delta$ is a mean-zero Gaussian with standard deviation $s_{\\delta} = 0.035$. The counting $N$ and the calibration shift $\\delta$ are independent.\n\nUsing first-order error propagation starting from the definitions of the Poisson model for counting and the linearization for uncertainty propagation, compute the separate contributions to the standard deviation of the cross-section estimator $\\hat{\\sigma}$ due to:\n1. Aleatoric uncertainty from Poisson counting.\n2. Epistemic uncertainty from the efficiency calibration shift.\n\nExpress both contributions as numerical values in femtobarns (fb), and round each to three significant figures.",
            "solution": "The problem is valid as it presents a self-contained, scientifically grounded, and well-posed scenario common in experimental high-energy physics. All necessary data are provided, and the task is to apply a standard method of uncertainty propagation.\n\nThe true cross section, $\\sigma$, is related to the true mean number of signal events, $\\mu$, the integrated luminosity, $L$, and the true detection efficiency, $\\epsilon$, by the equation $\\mu = \\sigma L \\epsilon$. We are given an observed count, $N$, which is a single realization of a Poisson random variable with mean $\\mu$. The true efficiency is modeled as $\\epsilon = \\epsilon_0(1+\\delta)$, where $\\epsilon_0$ is the nominal efficiency and $\\delta$ is a nuisance parameter representing the unknown calibration shift.\n\nWe can express the cross section as a function of the quantities we measure or have probabilistic information about, namely $N$ and $\\delta$. By using $N$ as the estimator for $\\mu$, we get:\n$$ \\sigma(N, \\delta) = \\frac{N}{L \\epsilon} = \\frac{N}{L \\epsilon_0 (1 + \\delta)} $$\nThe central value of the cross section, $\\hat{\\sigma}$, is found by evaluating this function at the observed value of $N = 1692$ and the mean value of $\\delta$, which is $\\langle\\delta\\rangle = 0$:\n$$ \\hat{\\sigma} = \\sigma(1692, 0) = \\frac{1692}{L \\epsilon_0} $$\nTo find the uncertainty in $\\sigma$, we use first-order error propagation. Since the problem states that the counting statistic $N$ and the calibration shift $\\delta$ are independent, the total variance $V[\\sigma]$ is approximated by the sum of the individual contributions:\n$$ V[\\sigma] \\approx \\left(\\frac{\\partial\\sigma}{\\partial N}\\right)^2 V[N] + \\left(\\frac{\\partial\\sigma}{\\partial \\delta}\\right)^2 V[\\delta] $$\nThe problem asks for the separate contributions to the standard deviation, which we denote as $\\Delta\\sigma_N$ and $\\Delta\\sigma_\\delta$. These are calculated as:\n$$ \\Delta\\sigma_N = \\left| \\frac{\\partial\\sigma}{\\partial N} \\right| \\sqrt{V[N]} \\quad \\text{and} \\quad \\Delta\\sigma_\\delta = \\left| \\frac{\\partial\\sigma}{\\partial \\delta} \\right| \\sqrt{V[\\delta]} $$\nThe partial derivatives are evaluated at the central values of the parameters, i.e., at $N = 1692$ and $\\delta = 0$.\n\n1. Contribution from Aleatoric Uncertainty (Poisson Counting of $N$)\nThe observed count $N$ follows a Poisson distribution. For a Poisson process, the variance is equal to the mean, $V[N] = E[N]$. The best available estimate for the mean $E[N]$ is the observed count $N$ itself. Therefore, we approximate the variance as $V[N] \\approx N$ and the standard deviation as $\\sqrt{V[N]} \\approx \\sqrt{N}$.\nThe partial derivative of $\\sigma$ with respect to $N$ is:\n$$ \\frac{\\partial\\sigma}{\\partial N} = \\frac{\\partial}{\\partial N} \\left[ \\frac{N}{L \\epsilon_0 (1 + \\delta)} \\right] = \\frac{1}{L \\epsilon_0 (1 + \\delta)} $$\nEvaluating this derivative at $\\delta=0$:\n$$ \\left.\\frac{\\partial\\sigma}{\\partial N}\\right|_{\\delta=0} = \\frac{1}{L \\epsilon_0} $$\nThe contribution to the standard deviation from the counting uncertainty is:\n$$ \\Delta\\sigma_N = \\left| \\frac{1}{L \\epsilon_0} \\right| \\sqrt{N} = \\frac{\\sqrt{N}}{L \\epsilon_0} $$\nSubstituting the given values, $N = 1692$, $L = 140\\,\\mathrm{fb}^{-1}$, and $\\epsilon_0 = 0.60$:\n$$ \\Delta\\sigma_N = \\frac{\\sqrt{1692}}{140 \\times 0.60}\\,\\mathrm{fb} = \\frac{\\sqrt{1692}}{84}\\,\\mathrm{fb} \\approx \\frac{41.133928...}{84}\\,\\mathrm{fb} \\approx 0.4896896... \\,\\mathrm{fb} $$\nRounding to three significant figures, the aleatoric uncertainty contribution is $0.490\\,\\mathrm{fb}$.\n\n2. Contribution from Epistemic Uncertainty (Efficiency Calibration Shift $\\delta$)\nThe calibration shift $\\delta$ is described as a Gaussian random variable with mean $0$ and standard deviation $s_\\delta = 0.035$. The standard deviation of $\\delta$ is therefore $\\sqrt{V[\\delta]} = s_\\delta$.\nThe partial derivative of $\\sigma$ with respect to $\\delta$ is:\n$$ \\frac{\\partial\\sigma}{\\partial \\delta} = \\frac{\\partial}{\\partial \\delta} \\left[ \\frac{N}{L \\epsilon_0} (1+\\delta)^{-1} \\right] = \\frac{N}{L \\epsilon_0} \\left[ -1 \\cdot (1+\\delta)^{-2} \\right] = -\\frac{N}{L \\epsilon_0 (1+\\delta)^2} $$\nEvaluating this derivative at the central values $N=1692$ and $\\delta=0$:\n$$ \\left.\\frac{\\partial\\sigma}{\\partial \\delta}\\right|_{N=1692, \\delta=0} = -\\frac{1692}{L \\epsilon_0 (1+0)^2} = -\\frac{N}{L \\epsilon_0} = -\\hat{\\sigma} $$\nThe contribution to the standard deviation from the efficiency uncertainty is:\n$$ \\Delta\\sigma_\\delta = \\left| -\\frac{N}{L \\epsilon_0} \\right| s_\\delta = \\hat{\\sigma} \\cdot s_\\delta $$\nFirst, we calculate the estimated cross section $\\hat{\\sigma}$:\n$$ \\hat{\\sigma} = \\frac{1692}{140 \\times 0.60}\\,\\mathrm{fb} = \\frac{1692}{84}\\,\\mathrm{fb} = 20.142857... \\,\\mathrm{fb} $$\nNow we can compute the uncertainty contribution:\n$$ \\Delta\\sigma_\\delta = (20.142857... \\,\\mathrm{fb}) \\times 0.035 \\approx 0.704999... \\,\\mathrm{fb} $$\nRounding to three significant figures, the epistemic uncertainty contribution is $0.705\\,\\mathrm{fb}$.\n\nThe two requested uncertainty contributions are:\n- Aleatoric uncertainty: $0.490\\,\\mathrm{fb}$\n- Epistemic uncertainty: $0.705\\,\\mathrm{fb}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.490  0.705\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Systematic uncertainties often do more than just shift an overall normalization; they can distort the shape of a measured distribution. This hands-on exercise introduces \"template morphing,\" a powerful and widely used technique for modeling such shape variations in binned analyses. You will implement a vertical morphing algorithm to interpolate between nominal, \"up,\" and \"down\" systematic variations, gaining practical experience in a core component of modern statistical modeling .",
            "id": "3540041",
            "problem": "You are given three binned templates representing a differential event yield distribution in High Energy Physics (HEP) under a single systematic uncertainty source: a nominal template at parameter value $\\theta=0$, an \"up\" template at $\\theta=+1$, and a \"down\" template at $\\theta=-1$. The parameter $\\theta$ is a dimensionless nuisance parameter controlling the systematic shift. The objective is to implement vertical morphing for the bin contents and compute the expected event yield in each bin at an intermediate parameter value $\\theta=0.3$. In vertical morphing, the morphing is performed \"vertically\" on bin contents, meaning that for each bin $i$, one defines a smooth function $y_i(\\theta)$ that interpolates the three control points $\\{(\\theta,y_i(\\theta))\\} = \\{(-1,y_i^{-}), (0,y_i^{0}), (1,y_i^{+})\\}$, where $y_i^{-}$, $y_i^{0}$, and $y_i^{+}$ are the down, nominal, and up bin contents, respectively. Assume that $\\theta$ lies in the interval $[-1,1]$. The expected event yield is the sum over bin contents, and event counts are dimensionless.\n\nFundamental base for this problem: In binned template-based predictions of event yields, the expected yield in bin $i$ is the integral of the differential expectation over the bin range, which reduces computationally to the bin content $y_i$ when working with discrete histograms. When an uncertainty is shape-only, the total expected yield (sum of all bins) is preserved across systematic variations. A template morphing scheme must satisfy the boundary conditions $y_i(-1)=y_i^{-}$, $y_i(0)=y_i^{0}$, and $y_i(1)=y_i^{+}$ in every bin in order to be consistent with the up/down variations and the nominal prediction. Without specifying any particular shortcut formula, implement the unique lowest-degree polynomial per bin consistent with these three boundary conditions and use it to interpolate to $\\theta=0.3$. Then verify whether the interpolation preserves the total yield whenever the up and down templates are pre-normalized to the same total as the nominal template.\n\nImplement the following test suite. For each test case, compute the interpolated bin yields at $\\theta=0.3$, and compute two booleans: one indicating whether the total yield after interpolation equals the nominal total within an absolute tolerance of $10^{-12}$, and one indicating whether total-yield preservation is intended (that is, whether $\\sum_i y_i^{+} = \\sum_i y_i^{0} = \\sum_i y_i^{-}$). The program must return, for each test case, a list containing: the list of per-bin interpolated yields at $\\theta=0.3$, the boolean for \"preserved total\", and the boolean for \"intended preservation\".\n\nTest suite:\n- Case A (shape-only, pre-normalized up/down): $y^{0}=[100.0, 150.0, 80.0, 70.0]$, $y^{+}=[110.0, 140.0, 85.0, 65.0]$, $y^{-}=[90.0, 160.0, 75.0, 75.0]$.\n- Case B (not pre-normalized): $y^{0}=[50.0, 50.0, 50.0]$, $y^{+}=[60.0, 55.0, 50.0]$, $y^{-}=[45.0, 45.0, 45.0]$.\n- Case C (edge case with zero-content bins, shape-only pre-normalized): $y^{0}=[0.0, 100.0, 0.0, 50.0]$, $y^{+}=[0.0, 110.0, 0.0, 40.0]$, $y^{-}=[0.0, 90.0, 0.0, 60.0]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this top-level list corresponds to one test case and is itself a list of the form $[\\text{list of bin yields at } \\theta=0.3, \\text{preserved boolean}, \\text{intended boolean}]$. The output must include no spaces anywhere. For example, the syntactic structure must be exactly like \"[[...,...,...],[...,...,...],[...,...,...]]\" with all numbers and booleans represented in plain text. No physical units are involved; all quantities are dimensionless counts. Angles or angle units do not appear in this problem.\n\nThe precise steps to solve:\n- For each bin $i$ in a test case, construct the unique lowest-degree polynomial $y_i(\\theta)$ consistent with the three given boundary conditions at $\\theta=-1,0,1$.\n- Evaluate $y_i(0.3)$ for all bins.\n- Compute the total yield $\\sum_i y_i(0.3)$ and compare with $\\sum_i y_i^{0}$ using an absolute tolerance of $10^{-12}$ to form the \"preserved\" boolean.\n- Determine the \"intended\" boolean by checking whether $\\sum_i y_i^{+}$, $\\sum_i y_i^{0}$, and $\\sum_i y_i^{-}$ are equal within the same tolerance.\n- Aggregate and print the final result exactly in the specified single-line format with no spaces.",
            "solution": "The problem requires the implementation of a vertical morphing algorithm for binned event-yield templates, a standard technique for modeling the effect of systematic uncertainties in high-energy physics. The core of the task is to find an interpolated template for a nuisance parameter value $\\theta=0.3$ given three input templates: a nominal one ($y^0$) at $\\theta=0$, an \"up\" variation ($y^+$) at $\\theta=+1$, and a \"down\" variation ($y^-$) at $\\theta=-1$. We must also verify a property related to the preservation of the total event yield.\n\nThe problem is valid as it is scientifically grounded in common physics analysis practices, mathematically well-posed, and provides a complete and unambiguous specification.\n\nThe first step is to determine the interpolation function for the content of each bin $i$. The problem specifies using the unique lowest-degree polynomial that passes through the three given control points: $(-1, y_i^{-})$, $(0, y_i^{0})$, and $(1, y_i^{+})$. A polynomial that can fit three points is, in general, a quadratic of the form:\n$$y_i(\\theta) = a_i \\theta^2 + b_i \\theta + c_i$$\n\nThe coefficients $a_i$, $b_i$, and $c_i$ for each bin $i$ are determined by enforcing the boundary conditions:\n$1$. At $\\theta=0$:\n$$y_i(0) = a_i(0)^2 + b_i(0) + c_i = y_i^0 \\implies c_i = y_i^0$$\n$2$. At $\\theta=1$:\n$$y_i(1) = a_i(1)^2 + b_i(1) + c_i = a_i + b_i + c_i = y_i^+$$\nSubstituting $c_i=y_i^0$, we get:\n$$a_i + b_i = y_i^+ - y_i^0 \\quad (\\text{Eq. 1})$$\n$3$. At $\\theta=-1$:\n$$y_i(-1) = a_i(-1)^2 + b_i(-1) + c_i = a_i - b_i + c_i = y_i^-$$\nSubstituting $c_i=y_i^0$, we get:\n$$a_i - b_i = y_i^- - y_i^0 \\quad (\\text{Eq. 2})$$\n\nWe now have a system of two linear equations for $a_i$ and $b_i$. Adding Eq. $1$ and Eq. $2$ yields:\n$$2a_i = (y_i^+ - y_i^0) + (y_i^- - y_i^0) = y_i^+ + y_i^- - 2y_i^0$$\n$$a_i = \\frac{y_i^+ + y_i^- - 2y_i^0}{2}$$\nSubtracting Eq. $2$ from Eq. $1$ yields:\n$$2b_i = (y_i^+ - y_i^0) - (y_i^- - y_i^0) = y_i^+ - y_i^-$$\n$$b_i = \\frac{y_i^+ - y_i^-}{2}$$\n\nThus, the explicit formula for the interpolated bin content $y_i$ at any parameter value $\\theta$ is:\n$$y_i(\\theta) = \\left( \\frac{y_i^+ + y_i^- - 2y_i^0}{2} \\right) \\theta^2 + \\left( \\frac{y_i^+ - y_i^-}{2} \\right) \\theta + y_i^0$$\nThis formula is evaluated for each bin at $\\theta=0.3$ to find the interpolated template.\n\nNext, we analyze the preservation of the total event yield. The total yield at a given $\\theta$ is the sum of all bin contents, $S(\\theta) = \\sum_i y_i(\\theta)$. Using the interpolation formula and the linearity of the summation operator:\n$$S(\\theta) = \\sum_i \\left[ \\left( \\frac{y_i^+ + y_i^- - 2y_i^0}{2} \\right) \\theta^2 + \\left( \\frac{y_i^+ - y_i^-}{2} \\right) \\theta + y_i^0 \\right]$$\n$$S(\\theta) = \\left( \\frac{\\sum_i y_i^+ + \\sum_i y_i^- - 2\\sum_i y_i^0}{2} \\right) \\theta^2 + \\left( \\frac{\\sum_i y_i^+ - \\sum_i y_i^-}{2} \\right) \\theta + \\sum_i y_i^0$$\nLet $S_0 = \\sum_i y_i^0$, $S_+ = \\sum_i y_i^+$, and $S_- = \\sum_i y_i^-$ be the total yields of the nominal, up, and down templates, respectively. The expression for the total interpolated yield simplifies to:\n$$S(\\theta) = \\left( \\frac{S_+ + S_- - 2S_0}{2} \\right) \\theta^2 + \\left( \\frac{S_+ - S_-}{2} \\right) \\theta + S_0$$\nThe problem defines \"intended preservation\" as the case where the input templates are pre-normalized, i.e., $S_+ = S_0$ and $S_- = S_0$. This corresponds to a \"shape-only\" uncertainty. If these conditions hold, the coefficients of the $\\theta^2$ and $\\theta$ terms in the expression for $S(\\theta)$ become zero:\nThe $\\theta^2$ coefficient: $\\frac{S_0 + S_0 - 2S_0}{2} = 0$.\nThe $\\theta$ coefficient: $\\frac{S_0 - S_0}{2} = 0$.\nTherefore, if intended preservation holds, $S(\\theta) = S_0$ for all $\\theta \\in [-1, 1]$. This proves that quadratic vertical morphing preserves the total event yield if and only if the input up and down templates have the same total yield as the nominal template.\n\nThe algorithm to be implemented will perform these steps for each test case:\n$1$. For a given test case with templates $y^0$, $y^+$, and $y^-$, calculate their total sums $S_0$, $S_+$, and $S_-$.\n$2$. Determine the `intended_preservation` boolean by checking if $|S_+ - S_0|  10^{-12}$ and $|S_- - S_0|  10^{-12}$.\n$3$. For each bin $i$, calculate the coefficients $a_i, b_i, c_i$ using the derived formulae. This is done efficiently using vectorized operations on the template arrays.\n$4$. Evaluate $y_i(0.3) = a_i(0.3)^2 + b_i(0.3) + c_i$ for all bins to obtain the interpolated template.\n$5$. Calculate the total yield of the interpolated template, $S_{0.3} = \\sum_i y_i(0.3)$.\n$6$. Determine the `preserved_total` boolean by checking if $|S_{0.3} - S_0|  10^{-12}$.\n$7$. Collect the list of interpolated yields, the `preserved_total` boolean, and the `intended_preservation` boolean.\n$8$. Format the collected results from all test cases into a single-line string with no spaces, as specified.",
            "answer": "[[[103.0,147.0,81.5,68.5],true,true],[[52.475,51.5,50.525],false,false],[[0.0,103.0,0.0,47.0],true,true]]"
        },
        {
            "introduction": "The ultimate goal of treating systematic uncertainties is to correctly incorporate them into a full statistical model to quantify their impact on our physics conclusions. This capstone practice challenges you to build a complete binned likelihood function, incorporating nuisance parameters for both normalization and shape uncertainties with their corresponding Gaussian constraints. By analyzing an Asimov dataset, you will use the profile likelihood ratio to calculate the expected discovery significance, a key metric in high-energy physics, and directly observe how systematics can degrade an experiment's sensitivity .",
            "id": "3540042",
            "problem": "Consider a binned counting experiment in computational high-energy physics where the expected signal and background yields are modeled per bin and the data are analyzed using the concept of an Asimov dataset. The core aim is to quantify the impact of systematic uncertainties on the expected discovery significance by constructing and profiling a likelihood that includes nuisance parameters with Gaussian constraints. The calculation must be done in purely mathematical terms and must not rely on any external data source.\n\nAssume there are $N$ bins indexed by $i \\in \\{0,1,\\dots,N-1\\}$. Let $s_i \\ge 0$ denote the expected signal yield in bin $i$ and $b_i  0$ the expected background yield in bin $i$. Define the Asimov dataset (deterministic dataset that coincides with the expectation under the signal-plus-background hypothesis at nominal nuisance parameters) with counts\n$$\nn_i^{A} = \\mu_{\\text{true}} s_i + b_i,\n$$\nwhere $\\mu_{\\text{true}}$ is the true signal strength under which the Asimov dataset is constructed. In this problem, take $\\mu_{\\text{true}} = 1$.\n\nThe statistical model for the expected counts in bin $i$ under a hypothesized signal strength $\\mu \\ge 0$ and two nuisance parameters is:\n$$\n\\lambda_i(\\mu,\\theta_n,\\theta_s) = \\mu s_i + b_i \\exp(\\theta_n) \\exp(\\theta_s k_i),\n$$\nwhere:\n- $\\theta_n$ is the background normalization nuisance parameter with a Gaussian prior centered at $0$ and relative standard deviation $\\sigma_n$ (expressed as a decimal fraction).\n- $\\theta_s$ is the shape nuisance parameter with a Gaussian prior centered at $0$ and standard deviation $\\sigma_s$.\n- $k_i$ encodes a linearly tilting shape pattern across bins and is defined by\n$$\nk_i = r_i - \\frac{1}{N} \\sum_{j=0}^{N-1} r_j, \\quad \\text{with} \\quad r_i = -1 + \\frac{2i}{N-1},\n$$\nso that $\\sum_i k_i = 0$, ensuring a pure tilt without changing the overall normalization to first order.\n\nUnder the assumption of independent Poisson fluctuations in each bin and independent Gaussian constraints for the nuisance parameters, the negative log-likelihood (up to additive constants independent of parameters) for the Asimov data is:\n$$\n\\text{NLL}(\\mu,\\theta_n,\\theta_s) = \\sum_{i=0}^{N-1} \\left[ \\lambda_i(\\mu,\\theta_n,\\theta_s) - n_i^{A} \\ln \\lambda_i(\\mu,\\theta_n,\\theta_s) \\right]\n+ \\frac{1}{2}\\left(\\frac{\\theta_n}{\\sigma_n}\\right)^2 \\mathbb{I}(\\sigma_n > 0)\n+ \\frac{1}{2}\\left(\\frac{\\theta_s}{\\sigma_s}\\right)^2 \\mathbb{I}(\\sigma_s > 0),\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator that includes a Gaussian penalty only if the corresponding standard deviation is strictly positive. When a standard deviation is zero, the associated nuisance parameter is fixed at its nominal value $0$.\n\nDefine the profile likelihood ratio for testing the background-only hypothesis $\\mu=0$ as:\n$$\n\\lambda(0) = \\frac{L(0,\\hat{\\hat{\\theta}})}{L(\\hat{\\mu},\\hat{\\theta})},\n$$\nwhere $L$ is the likelihood corresponding to the above negative log-likelihood $\\text{NLL}$, $\\hat{\\hat{\\theta}}$ denotes the nuisance parameters that minimize $\\text{NLL}$ when $\\mu$ is fixed at $0$, and $(\\hat{\\mu},\\hat{\\theta})$ are the parameters that jointly minimize $\\text{NLL}$ over $\\mu \\ge 0$ and over the nuisance parameters. The discovery test statistic based on the likelihood ratio is\n$$\nq_0 = -2 \\ln \\lambda(0),\n$$\nand the Asimov expected discovery significance is\n$$\nZ_A = \\sqrt{q_0}.\n$$\nThis $Z_A$ is unitless and should be reported as a float.\n\nImplement a program that, for each test case defined below, constructs the Asimov dataset $n_i^{A}$, profiles the negative log-likelihood to obtain $\\hat{\\hat{\\theta}}$ at $\\mu=0$ and $(\\hat{\\mu},\\hat{\\theta})$ at free $\\mu \\ge 0$, computes $q_0$, and returns $Z_A$. The modeling choices above, including the exponential parameterization of systematic effects, enforce positivity of the expectations $\\lambda_i(\\mu,\\theta_n,\\theta_s)$.\n\nYour program must output the results for all test cases as a single line containing a comma-separated list enclosed in square brackets, with each entry rounded to six decimal places, for example, \"[0.123456,1.234568]\". No other text may be printed.\n\nTest suite:\n- Case 1 (general case with both normalization and shape uncertainties):\n  - $N=6$\n  - $b = [120, 100, 90, 80, 70, 60]$\n  - $s = [10, 12, 15, 18, 20, 22]$\n  - $\\sigma_n = 0.05$\n  - $\\sigma_s = 1.0$\n- Case 2 (normalization uncertainty only; shape disabled):\n  - $N=6$\n  - $b = [120, 100, 90, 80, 70, 60]$\n  - $s = [10, 12, 15, 18, 20, 22]$\n  - $\\sigma_n = 0.05$\n  - $\\sigma_s = 0.0$\n- Case 3 (shape uncertainty only; normalization disabled):\n  - $N=6$\n  - $b = [120, 100, 90, 80, 70, 60]$\n  - $s = [10, 12, 15, 18, 20, 22]$\n  - $\\sigma_n = 0.0$\n  - $\\sigma_s = 1.0$\n- Case 4 (weak signal; both uncertainties active):\n  - $N=6$\n  - $b = [120, 100, 90, 80, 70, 60]$\n  - $s = [1, 1.2, 1.5, 1.8, 2.0, 2.2]$\n  - $\\sigma_n = 0.05$\n  - $\\sigma_s = 1.0$\n- Case 5 (strong signal; both uncertainties active):\n  - $N=6$\n  - $b = [120, 100, 90, 80, 70, 60]$\n  - $s = [50, 60, 75, 90, 100, 110]$\n  - $\\sigma_n = 0.05$\n  - $\\sigma_s = 1.0$\n- Case 6 (edge case with low background in some bins; both uncertainties active):\n  - $N=6$\n  - $b = [5, 10, 15, 5, 3, 1]$\n  - $s = [2, 2, 3, 3, 4, 5]$\n  - $\\sigma_n = 0.05$\n  - $\\sigma_s = 1.0$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order of the cases listed above. Each number must be a float rounded to six decimal places, and there must be no additional spaces within the brackets.",
            "solution": "The problem requires the computation of the Asimov discovery significance, $Z_A$, for a binned counting experiment under various systematic uncertainty scenarios. The calculation relies on the profile likelihood ratio method, a cornerstone of statistical inference in high-energy physics.\n\n### Scientific Principles and Methodology\n\nThe core of the problem is to quantify the expected separation between the signal-plus-background hypothesis ($\\mu=1$) and the background-only hypothesis ($\\mu=0$) using a test statistic derived from the likelihood function.\n\n1.  **Likelihood Function**: The statistical model assumes that the number of observed events in each bin $i$ follows an independent Poisson distribution with a mean $\\lambda_i$. The total likelihood is the product of these Poisson probabilities, augmented by penalty terms for systematic uncertainties, which are modeled by nuisance parameters. The nuisance parameters $\\theta_n$ and $\\theta_s$ are constrained by Gaussian priors, reflecting prior knowledge about their expected values (zero) and uncertainties ($\\sigma_n$ and $\\sigma_s$). For computational convenience, we work with the negative log-likelihood ($\\text{NLL}$), which is a sum of terms rather than a product:\n    $$\n    \\text{NLL}(\\mu,\\theta_n,\\theta_s) = \\sum_{i=0}^{N-1} \\left[ \\lambda_i(\\mu,\\theta_n,\\theta_s) - n_i \\ln \\lambda_i(\\mu,\\theta_n,\\theta_s) \\right] + \\text{Penalty}(\\theta_n, \\theta_s)\n    $$\n    where the penalty terms are quadratic, corresponding to the Gaussian priors.\n\n2.  **Asimov Dataset**: To calculate the *expected* significance without resorting to computationally expensive Monte Carlo simulations, we use the Asimov dataset. This is a deterministic, representative dataset where the \"observed\" counts $n_i^A$ are set equal to the expected counts under a specific hypothesis. Here, this is the signal-plus-background hypothesis with a signal strength $\\mu_{\\text{true}}=1$ and nominal nuisance parameters ($\\theta_n=0, \\theta_s=0$). Thus, $n_i^A = s_i + b_i$.\n\n3.  **Profile Likelihood Ratio and Test Statistic $q_0$**: The separation between hypotheses is quantified using the profile likelihood ratio test statistic $q_0$.\n    $$\n    q_0 = -2 \\ln \\frac{ L(\\mu=0, \\hat{\\hat{\\theta}}) }{ L(\\hat{\\mu}, \\hat{\\theta}) } = 2 \\left[ \\text{NLL}(0, \\hat{\\hat{\\theta}}) - \\text{NLL}(\\hat{\\mu}, \\hat{\\theta}) \\right]\n    $$\n    This requires finding the parameters that minimize the NLL in two scenarios:\n    -   **Global Minimum**: $(\\hat{\\mu}, \\hat{\\theta})$ are the parameters that give the best possible fit to the data, representing the global minimum of the NLL. A key property of the Asimov dataset is that the global minimum of the NLL occurs exactly at the true parameter values under which the dataset was generated. In our case, this is $(\\hat{\\mu}, \\hat{\\theta}_n, \\hat{\\theta}_s) = (1, 0, 0)$.\n    -   **Constrained Minimum**: $\\hat{\\hat{\\theta}}$ are the nuisance parameters that give the best fit to the data under the constraint of the background-only hypothesis ($\\mu=0$). This gives the minimum of the NLL on the subspace where $\\mu=0$.\n\n4.  **Asimov Significance $Z_A$**: For the Asimov dataset, the significance is simply $Z_A = \\sqrt{q_0}$. This value represents the median expected discovery significance of the experiment.\n\n### Algorithmic Implementation\n\nThe solution is implemented by following these steps for each test case:\n\n1.  **Initialization**: Define the input parameters: number of bins $N$, signal yields $s_i$, background yields $b_i$, and the systematic uncertainties $\\sigma_n$ and $\\sigma_s$.\n\n2.  **Pre-computation**:\n    -   The shape factors $k_i$ are computed based on their definition. For the given linear form of $r_i$, this simplifies to $k_i = r_i$.\n    -   The Asimov dataset counts are constructed as $n_i^A = s_i + b_i$.\n\n3.  **Calculate NLL at the Global Minimum**:\n    -   The global minimum of the NLL occurs at $(\\mu, \\theta_n, \\theta_s) = (1, 0, 0)$.\n    -   The value $\\text{NLL}(\\hat{\\mu}, \\hat{\\theta}) = \\text{NLL}(1, 0, 0)$ is calculated by substituting these values into the NLL formula, which simplifies to $\\sum_i [n_i^A - n_i^A \\ln(n_i^A)]$.\n\n4.  **Calculate NLL at the Constrained Minimum**:\n    -   Finding $\\text{NLL}(0, \\hat{\\hat{\\theta}})$ requires minimizing the NLL function with respect to the nuisance parameters $(\\theta_n, \\theta_s)$ while holding $\\mu$ fixed at $0$.\n    -   This is a numerical optimization problem. We define an objective function `nll_mu0(thetas)` that computes $\\text{NLL}(0, \\theta_n, \\theta_s)$.\n    -   The `scipy.optimize.minimize` function with the `L-BFGS-B` method is used to find the minimum of this objective function. The initial guess for the nuisance parameters is $(0, 0)$.\n    -   Cases where $\\sigma_n=0$ or $\\sigma_s=0$ correspond to a fixed nuisance parameter. This is handled by setting the bounds of the respective parameter to $(0, 0)$ in the optimizer.\n\n5.  **Compute Significance $Z_A$**:\n    -   The test statistic $q_0$ is calculated as $q_0 = 2 \\left[ \\text{NLL}(0, \\hat{\\hat{\\theta}}) - \\text{NLL}(1, 0, 0) \\right]$. To prevent issues from minor numerical floating-point inaccuracies, $q_0$ is floored at $0$.\n    -   The Asimov significance is then $Z_A = \\sqrt{q_0}$.\n\nThe final result is a list of the computed $Z_A$ values for all test cases, formatted as required.",
            "answer": "[5.249219,4.981881,5.048703,0.524922,26.246093,5.187315]"
        }
    ]
}