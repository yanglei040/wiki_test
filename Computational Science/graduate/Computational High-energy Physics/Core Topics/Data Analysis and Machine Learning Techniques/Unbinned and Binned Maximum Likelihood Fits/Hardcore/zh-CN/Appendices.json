{
    "hands_on_practices": [
        {
            "introduction": "为了简化分析和可视化，将数据分箱是一种常用操作，但这不可避免地会丢失信息。本练习将使用统计学中一个强大的工具——费雪信息（Fisher information），来精确量化这种信息损失。通过直接计算和比较非分箱拟合与其分箱对应版本的费雪信息，你将亲身体验并理解这两种方法在根本上的权衡。",
            "id": "3540423",
            "problem": "您正在研究在一个有限观测窗口内，指数寿命模型的非分组与分组最大似然拟合的信息论效率。考虑从定义在非负数轴上的指数概率密度函数 $f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}$（其中 $x \\ge 0$ 且参数 $\\lambda > 0$）中抽取的独立同分布样本，但假设数据采集被限制在一个有限窗口 $[0, T]$ 内。条件是在此窗口内观测到恰好 $N$ 个事件。对于非分组情况，似然函数是在 $[0, T]$ 上的截断密度的乘积。对于分组情况，将窗口 $[0, T]$ 划分成 $m$ 个等宽分箱，并使用以 $N$ 为条件的分箱计数的的多项分布似然。使用下方的基本定义，推导解析表达式并进行数值评估。\n\n基本定义：\n- 对于来自密度函数 $f(x \\mid \\lambda)$ 的独立观测值 $x_1, \\dots, x_N$，其对数似然为 $\\ell(\\lambda) = \\sum_{i=1}^N \\log f(x_i \\mid \\lambda)$。\n- 来自 $N$ 个独立样本的参数 $\\lambda$ 的费雪信息为 $I(\\lambda) = - \\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right]$，其中期望是针对真实数据生成分布计算的。\n- 对于一个多项模型，其概率 $p_k(\\lambda)$ 依赖于 $\\lambda$ 且总计数为 $N$，参数 $\\lambda$ 的费雪信息为 $I(\\lambda) = N \\sum_{k=1}^m \\frac{1}{p_k(\\lambda)} \\left(\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}\\right)^2$，前提是对于所有 $k$ 都有 $p_k(\\lambda) > 0$。\n\n任务：\n1. 截断下的非分组信息。给定观测窗口 $[0, T]$，将数据视为从 $[0, T]$ 上的截断指数分布中抽取，即条件密度为 $f_T(x \\mid \\lambda) = \\frac{\\lambda e^{-\\lambda x}}{1 - e^{-\\lambda T}}$（其中 $0 \\le x \\le T$）。从费雪信息的定义出发，推导在此截断下单事件的 $\\lambda$ 的费雪信息，并由此获得在 $[0, T]$ 内 $N$ 个观测值的总费雪信息。\n2. 等宽分箱的分组信息。将 $[0, T]$ 划分成 $m$ 个等宽分箱，边界为 $0, \\Delta, 2\\Delta, \\dots, T$，其中 $\\Delta = T/m$。设 $p_k(\\lambda)$ 表示在密度 $f_T(x \\mid \\lambda)$ 下，事件落入第 $k$ 个分箱（$k \\in \\{1, \\dots, m\\}$）的概率。从上述多项分布费雪信息的定义出发，推导用 $p_k(\\lambda)$ 和 $\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}$ 表示的 $\\lambda$ 的总费雪信息的表达式。\n3. 在最小期望占据数约束下优化 $m$。为保证分组拟合在实践中的数值稳定性，施加一个约束，即每个分箱中的期望计数必须至少为一个正常数阈值 $c_{\\min}$。对于总共 $N$ 个事件，这意味着对于所有 $k \\in \\{1, \\dots, m\\}$，都有 $N \\cdot p_k(\\lambda) \\ge c_{\\min}$。在所有满足此约束的整数 $m \\in \\{1, 2, \\dots, N\\}$ 中，选择能使分组费雪信息最大化（等效于相对于非分组情况的信息损失最小化）的 $m^\\star$。如果多个 $m$ 值在浮点容差范围内达到相同的最大值，则选择其中最大的 $m$。\n4. 实现一个程序，对于下面指定的每个测试用例，计算：\n   - 总非分组费雪信息 $I_{\\text{unb}}(\\lambda, T, N)$。\n   - 使用在占据数约束下优化得到的 $m^\\star$ 计算出的总分组费雪信息 $I_{\\text{bin}}(\\lambda, T, N, m^\\star)$。\n   - 比率 $r = I_{\\text{bin}} / I_{\\text{unb}}$。\n   - 优化后的分箱数 $m^\\star$。\n   报告浮点数结果，四舍五入至六位小数。\n\n假设与澄清：\n- 所有量都是无量纲的；不使用物理单位。\n- 不涉及角度。\n- 所有概率之和必须为一，且所有 $p_k(\\lambda)$ 必须为严格正数，以确保费雪信息为有限值。\n- 对于非分组和分组两种情况，均一致使用以N为条件的框架。\n\n测试套件：\n为以下参数集 $(\\lambda, T, N, c_{\\min})$ 计算输出：\n- Case A: $(0.5, 6.0, 1000, 5)$\n- Case B: $(0.2, 20.0, 500, 10)$\n- Case C: $(1.0, 3.0, 30, 5)$\n\n最终输出格式：\n您的程序应生成一行结果，格式为逗号分隔的列表的列表，每个测试用例一个列表，顺序为 [Case A, Case B, Case C]。每个内部列表的形式必须是 $[m^\\star, I_{\\text{unb}}, I_{\\text{bin}}, r]$，其中 $m^\\star$ 是一个整数，其他条目是四舍五入到六位小数的浮点数。例如：[[mA, IunbA, IbinA, rA],[mB, IunbB, IbinB, rB],[mC, IunbC, IbinC, rC]]。",
            "solution": "根据指定标准对问题进行验证。\n\n### 步骤 1：提取已知条件\n- **概率密度函数 (PDF)**: 指数分布，位于非负数轴上，$f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}$，其中 $x \\ge 0$ 且 $\\lambda > 0$。\n- **观测窗口**: 数据采集限于区间 $[0, T]$。\n- **截断条件密度**: 对于在 $[0, T]$ 内观测到的 $N$ 个事件，其密度为 $f_T(x \\mid \\lambda) = \\frac{\\lambda e^{-\\lambda x}}{1 - e^{-\\lambda T}}$，其中 $0 \\le x \\le T$。\n- **对数似然定义**: 对于独立同分布观测值 $x_1, \\dots, x_N$，$\\ell(\\lambda) = \\sum_{i=1}^N \\log f(x_i \\mid \\lambda)$。\n- **非分组费雪信息定义**: $I(\\lambda) = - \\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right]$。\n- **分组模型**: 窗口 $[0, T]$ 被划分为 $m$ 个等宽分箱。分箱宽度为 $\\Delta = T/m$。\n- **多项分组费雪信息定义**: 对于总计数 $N$ 和分箱概率 $p_k(\\lambda)$，$I(\\lambda) = N \\sum_{k=1}^m \\frac{1}{p_k(\\lambda)} \\left(\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}\\right)^2$，其中 $p_k(\\lambda) > 0$。\n- **优化约束**: 对于选定的分箱数 $m$，每个分箱中的期望计数必须至少为 $c_{\\min}$，即对于所有 $k \\in \\{1, \\dots, m\\}$，有 $N \\cdot p_k(\\lambda) \\ge c_{\\min}$。\n- **优化目标**: 找到满足约束条件并能最大化分组费雪信息的 $m^\\star \\in \\{1, 2, \\dots, N\\}$。如果出现平局，则选择其中最大的 $m$。\n- **测试用例**:\n    - Case A: $(\\lambda, T, N, c_{\\min}) = (0.5, 6.0, 1000, 5)$\n    - Case B: $(\\lambda, T, N, c_{\\min}) = (0.2, 20.0, 500, 10)$\n    - Case C: $(\\lambda, T, N, c_{\\min}) = (1.0, 3.0, 30, 5)$\n- **输出要求**: 对每个测试用例，计算 $[m^\\star, I_{\\text{unb}}, I_{\\text{bin}}, r]$，其中 $I_{\\text{unb}}$、$I_{\\text{bin}}$ 和 $r = I_{\\text{bin}} / I_{\\text{unb}}$ 四舍五入到六位小数。\n\n### 步骤 2：使用提取的已知条件进行验证\n对问题有效性进行评估：\n- **科学依据**: 问题基于统计推断的基本原理，即最大似然估计和费雪信息理论，并应用于指数分布。这是计算物理和统计学中一个标准且易于理解的课题。该设置在科学上是合理的。\n- **定义明确的**: 问题明确了所有必要的定义、约束和目标。推导任务表述清晰。$m^\\star$ 的优化任务有明确的目标函数（最大化 $I_{\\text{bin}}$）、明确定义的约束集和打破平局的规则，确保了唯一解的存在。\n- **客观性**: 问题使用精确的数学语言陈述，没有主观或含糊的术语。\n- **其他缺陷**: 问题没有违反任何无效性标准。它自成体系、逻辑一致且计算上可行。数学框架是标准的。提供的测试用例是合理的，没有引入矛盾。\n\n### 步骤 3：结论与行动\n问题被判定为**有效**。下面提供完整的、有理据的解答。\n\n解决方案的步骤是先推导所需数量的解析表达式，然后进行数值实现。\n\n**1. 非分组费雪信息, $I_{\\text{unb}}$**\n\n来自截断密度 $f_T(x \\mid \\lambda) = \\frac{\\lambda e^{-\\lambda x}}{1 - e^{-\\lambda T}}$ 的单个观测值 $x$ 的对数似然为：\n$$ \\ell_1(\\lambda) = \\log(f_T(x \\mid \\lambda)) = \\log(\\lambda) - \\lambda x - \\log(1 - e^{-\\lambda T}) $$\n关于 $\\lambda$ 的一阶导数（得分函数）为：\n$$ \\frac{\\partial \\ell_1(\\lambda)}{\\partial \\lambda} = \\frac{1}{\\lambda} - x - \\frac{1}{1 - e^{-\\lambda T}} \\cdot (-e^{-\\lambda T}) \\cdot (-T) = \\frac{1}{\\lambda} - x - \\frac{T e^{-\\lambda T}}{1 - e^{-\\lambda T}} $$\n二阶导数为：\n$$ \\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2} = -\\frac{1}{\\lambda^2} - \\frac{\\partial}{\\partial \\lambda}\\left(\\frac{T e^{-\\lambda T}}{1 - e^{-\\lambda T}}\\right) $$\n使用商法则，第二项的导数为 $\\frac{-T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2}$。因此：\n$$ \\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2} = -\\frac{1}{\\lambda^2} + \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2} $$\n此表达式与随机变量 $x$ 无关。因此，期望 $\\mathbb{E}\\left[\\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2}\\right]$ 就是该表达式本身。单个事件的费雪信息为：\n$$ I_1(\\lambda) = -\\mathbb{E}\\left[\\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2}\\right] = - \\left(-\\frac{1}{\\lambda^2} + \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2}\\right) = \\frac{1}{\\lambda^2} - \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2} $$\n对于 $N$ 个独立同分布的观测值，总费雪信息是可加的：\n$$ I_{\\text{unb}}(\\lambda, T, N) = N \\cdot I_1(\\lambda) = N \\left(\\frac{1}{\\lambda^2} - \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2}\\right) $$\n该表达式是非负的，这可以通过不等式 $\\sinh(v) \\ge v$ (对于 $v \\ge 0$) 来证明。\n\n**2. 分组费雪信息, $I_{\\text{bin}}$**\n\n对于 $m$ 个等宽（宽度 $\\Delta = T/m$）的分箱，一个事件落入第 $k$ 个分箱（$k \\in \\{1, \\dots, m\\}$，边界为 $[(k-1)\\Delta, k\\Delta]$）的概率由截断密度的积分给出：\n$$ p_k(\\lambda) = \\int_{(k-1)\\Delta}^{k\\Delta} f_T(x \\mid \\lambda) dx = \\frac{1}{1 - e^{-\\lambda T}} \\int_{(k-1)\\Delta}^{k\\Delta} \\lambda e^{-\\lambda x} dx $$\n$$ p_k(\\lambda) = \\frac{[-e^{-\\lambda x}]_{(k-1)\\Delta}^{k\\Delta}}{1 - e^{-\\lambda T}} = \\frac{e^{-\\lambda(k-1)\\Delta} - e^{-\\lambda k\\Delta}}{1 - e^{-\\lambda T}} $$\n为保证数值稳定性，可以使用 `expm1` 函数重写此式，其中 $\\text{expm1}(z) = e^z - 1$：\n$$ p_k(\\lambda) = \\frac{e^{-\\lambda k\\Delta}(e^{\\lambda\\Delta} - 1)}{e^{-\\lambda T}(e^{\\lambda T} - 1)} = e^{\\lambda(T-k\\Delta)} \\frac{\\text{expm1}(\\lambda\\Delta)}{\\text{expm1}(\\lambda T)} $$\n分组模型的费雪信息为 $I_{\\text{bin}}(\\lambda) = N \\sum_{k=1}^m \\frac{1}{p_k(\\lambda)} \\left(\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}\\right)^2$。这可以重写为 $N \\sum_{k=1}^m p_k(\\lambda) \\left(\\frac{\\partial \\log p_k(\\lambda)}{\\partial \\lambda}\\right)^2$。对 $p_k(\\lambda)$ 的稳定表达式取对数：\n$$ \\log p_k(\\lambda) = \\lambda(T-k\\Delta) + \\log(\\text{expm1}(\\lambda\\Delta)) - \\log(\\text{expm1}(\\lambda T)) $$\n对 $\\lambda$ 求导：\n$$ \\frac{\\partial \\log p_k(\\lambda)}{\\partial \\lambda} = (T - k\\Delta) + \\frac{\\Delta e^{\\lambda\\Delta}}{\\text{expm1}(\\lambda\\Delta)} - \\frac{T e^{\\lambda T}}{\\text{expm1}(\\lambda T)} $$\n这为计算 $I_{\\text{bin}}$ 的求和项提供了一种数值稳定的方法。\n\n**3. 分箱数 $m^\\star$ 的优化**\n\n约束条件是对于所有 $k = 1, \\dots, m$ 都有 $N \\cdot p_k(\\lambda) \\ge c_{\\min}$。分箱概率 $p_k(\\lambda)$ 是 $k$ 的单调递减函数。因此，该约束对最后一个分箱（即 $k=m$）最具限制性。条件简化为：\n$$ N \\cdot p_m(\\lambda) \\ge c_{\\min} $$\n使用 $k=m$ 和 $\\Delta=T/m$ 时 $p_k(\\lambda)$ 的稳定表达式：\n$$ p_m(\\lambda) = \\frac{\\text{expm1}(\\lambda T/m)}{\\text{expm1}(\\lambda T)} $$\n费雪信息 $I_{\\text{bin}}(m)$ 预期是 $m$ 的单调递增函数（因为更精细的分箱保留了更多信息），在 $m \\to \\infty$ 时趋近于 $I_{\\text{unb}}$。因此，为了在约束条件下最大化 $I_{\\text{bin}}(m)$，我们应该选择满足条件的最大可能的 $m$。函数 $p_m(\\lambda)$ 是 $m$ 的单调递减函数。这意味着我们可以搜索满足 $N \\cdot \\frac{\\text{expm1}(\\lambda T/m)}{\\text{expm1}(\\lambda T)} \\ge c_{\\min}$ 的最大整数 $m \\in \\{1, \\dots, N\\}$。对于给定的问题约束，从 $m=1$ 开始向上进行线性扫描是足够高效的。第一个违反约束的 $m$ 值确立了上限，而 $m^\\star$ 则是紧邻其前的值。\n\n实现将遵循这些推导出的公式。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all specified test cases.\n    It orchestrates the calculation for each case and formats the final output.\n    \"\"\"\n    test_cases = [\n        (0.5, 6.0, 1000, 5),    # Case A\n        (0.2, 20.0, 500, 10),   # Case B\n        (1.0, 3.0, 30, 5),      # Case C\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = compute_results_for_case(*case)\n        all_results.append(result)\n\n    # Format the final output string as specified\n    formatted_results = []\n    for res in all_results:\n        # res is [m_star, i_unb, i_bin, ratio]\n        s = f'[{res[0]},{res[1]:.6f},{res[2]:.6f},{res[3]:.6f}]'\n        formatted_results.append(s)\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef compute_results_for_case(lam, T, N, c_min):\n    \"\"\"\n    Computes all required quantities for a single test case.\n\n    Args:\n        lam (float): The lambda parameter of the exponential distribution.\n        T (float): The upper bound of the observation window.\n        N (int): The total number of events.\n        c_min (int): The minimum required expected occupancy per bin.\n\n    Returns:\n        list: A list containing [m_star, I_unb, I_bin, ratio].\n    \"\"\"\n    # 1. Compute the unbinned Fisher information\n    i_unb = calculate_i_unb(lam, T, N)\n\n    # 2. Find the optimal number of bins m_star\n    m_star = find_m_star(lam, T, N, c_min)\n\n    # 3. Compute the binned Fisher information for m_star\n    i_bin = 0.0\n    if m_star > 0:\n        i_bin = calculate_i_bin(lam, T, N, m_star)\n\n    # 4. Compute the ratio of binned to unbinned information\n    ratio = i_bin / i_unb if i_unb > 0 else 0.0\n\n    # 5. Return the results\n    return [m_star, i_unb, i_bin, ratio]\n\n\ndef calculate_i_unb(lam, T, N):\n    \"\"\"\n    Calculates the total unbinned Fisher information for N events.\n    Formula: N * (1/lambda^2 - (T^2 * exp(-lambda*T)) / (1 - exp(-lambda*T))^2)\n    \"\"\"\n    lam_T = lam * T\n    # The parameters ensure lam > 0, T > 0, so lam_T > 0.\n    # No risk of division by zero in the denominator for lam_T > 0.\n    exp_m_lam_T = np.exp(-lam_T)\n    one_minus_exp = 1.0 - exp_m_lam_T\n    \n    term2 = (T**2 * exp_m_lam_T) / (one_minus_exp**2)\n    i_one_event = (1.0 / lam**2) - term2\n    \n    return N * i_one_event\n\n\ndef find_m_star(lam, T, N, c_min):\n    \"\"\"\n    Finds the optimal number of bins m_star by finding the largest m\n    that satisfies the minimum occupancy constraint.\n    \"\"\"\n    m_star_found = 0\n    lam_T = lam * T\n\n    # Constraint: N * p_m >= c_min => expm1(lam*T/m) >= (c_min/N_tot) * expm1(lam*T)\n    # Target for comparison to avoid recomputing the RHS in the loop\n    if np.isclose(lam_T, 0): # Should not happen with given parameters\n        return 1 if N >= c_min else 0\n        \n    threshold = (c_min / N) * np.expm1(lam_T)\n\n    # Linearly scan for m from 1 to N.\n    # p_m is monotonically decreasing in m, so we can stop when the constraint fails.\n    for m in range(1, N + 1):\n        lam_T_over_m = lam_T / m\n        expm1_val = np.expm1(lam_T_over_m)\n        if expm1_val >= threshold:\n            m_star_found = m\n        else:\n            break\n            \n    return m_star_found\n\n\ndef calculate_i_bin(lam, T, N, m):\n    \"\"\"\n    Calculates the total binned Fisher information for a given number of bins m.\n    \"\"\"\n    delta = T / m\n    lam_T = lam * T\n    lam_delta = lam * delta\n\n    def h(x):\n        \"\"\"Helper function for x*exp(x)/(exp(x)-1), stable for x near 0.\"\"\"\n        if np.isclose(x, 0):\n            return 1.0\n        return (x * np.exp(x)) / np.expm1(x)\n\n    h_lam_T = h(lam_T)\n    h_lam_delta = h(lam_delta)\n    \n    if np.isclose(lam_T, 0):\n        return 0.0\n        \n    expm1_lam_T = np.expm1(lam_T)\n    expm1_lam_delta = np.expm1(lam_delta)\n    \n    common_pk_factor = expm1_lam_delta / expm1_lam_T\n    common_dpk_term = (h_lam_delta - h_lam_T) / lam\n\n    info_sum = 0.0\n    for k in range(1, m + 1):\n        T_minus_k_delta = T - k * delta\n        \n        # Calculate p_k = exp(lam*(T-k*delta)) * common_pk_factor\n        pk = np.exp(lam * T_minus_k_delta) * common_pk_factor\n        \n        # Calculate d(log p_k)/d(lam) = (T-k*delta) + common_dpk_term\n        dlogpk_dlam = T_minus_k_delta + common_dpk_term\n        \n        # Term in sum for info is p_k * (d(log p_k)/d(lam))^2\n        if pk > 0:\n            info_sum += pk * (dlogpk_dlam**2)\n    \n    return N * info_sum\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "在分箱分析的基础上，本练习探讨了一种更为简化的方法：仅使用一部分选定的箱（即摘要统计量）进行拟合，这在处理大型数据集或复杂模型时非常有用。你将学习如何策略性地选择信息最丰富的箱，并利用费雪信息来量化因数据简化而导致的参数灵敏度损失。 这种方法为你理解近似贝叶斯计算（Approximate Bayesian Computation, $ABC$）等更前沿的技术奠定了基础。",
            "id": "3540393",
            "problem": "考虑一个计算高能物理中典型的一维分箱计数实验，有 $m$ 个等宽的箱，覆盖范围为 $x \\in [0,1]$。事件被建模为每个箱中独立的泊松计数。箱 $i$（其箱边缘为 $x_{i-1} = (i-1)/m$ 和 $x_i = i/m$）中的期望计数由下式给出：\n$$\n\\mu_i(\\theta) \\;=\\; \\lambda_b \\,\\Delta \\;+\\; \\lambda_s \\int_{x_{i-1}}^{x_i} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\,dx,\n$$\n其中 $\\Delta = 1/m$ 是箱宽，$\\lambda_b$ 是均匀本底率，$\\lambda_s$ 是信号幅度，$\\sigma$ 是信号的固定宽度，而 $\\theta \\in (0,1)$ 是信号峰值的未知位置参数。所有量都是无量纲的。假设每个箱中的泊松涨落是独立的。\n\n为期望计数 $\\mu_i(\\theta)$ 和观测计数 $n_i$ 定义完整的分箱泊松似然为\n$$\n\\mathcal{L}_{\\text{full}}(\\theta) \\;=\\; \\prod_{i=1}^{m} \\mathrm{Poisson}\\!\\left(n_i \\,\\middle|\\, \\mu_i(\\theta)\\right).\n$$\n在阿西莫夫机制（逐点用 $\\mu_i(\\theta)$ 替换 $n_i$）下工作，完整分箱泊松模型下 $\\theta$ 的费雪信息为\n$$\nI_{\\text{full}}(\\theta) \\;=\\; \\sum_{i=1}^{m} \\frac{\\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^2}{\\mu_i(\\theta)}.\n$$\n\n现在考虑一种近似贝叶斯计算 (ABC) 风格的概括统计量拟合：我们不使用完整的计数向量，而是选择一个维度为 $p$ 的概括向量，该向量由索引集 $S \\subset \\{1,\\dots,m\\}$（其中 $|S|=p$）中各箱的计数 $n_i$ 组成。用一个高斯合成似然来对这些概括统计量进行建模，其对角协方差等于相应的泊松方差，\n$$\n\\mathcal{L}_{\\text{sum}}(\\theta) \\;\\propto\\; \\exp\\!\\left(-\\frac{1}{2}\\sum_{i \\in S} \\frac{\\big(n_i - \\mu_i(\\theta)\\big)^2}{\\mu_i(\\theta)}\\right),\n$$\n这得出了概括统计量模型下 $\\theta$ 的费雪信息\n$$\nI_{\\text{sum}}(\\theta) \\;=\\; \\sum_{i \\in S} \\frac{\\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^2}{\\mu_i(\\theta)}.\n$$\n\n定义灵敏度比率\n$$\nR(p, S, \\theta) \\;=\\; \\frac{I_{\\text{sum}}(\\theta)}{I_{\\text{full}}(\\theta)},\n$$\n和相应的灵敏度损失\n$$\nL(p, S, \\theta) \\;=\\; 1 - R(p, S, \\theta).\n$$\n目标是量化当概括维度 $p$ 变化以及索引集 $S$ 的选择改变时，灵敏度的损失。\n\n您的任务是实现一个完整的程序，该程序：\n- 使用 $m = 64$、$\\sigma = 0.05$、$\\lambda_b = 400$ 和 $\\lambda_s = 1200$ 作为基线模型参数。\n- 对每个箱，通过每箱至少200个均匀样本的中点求积法，数值计算 $\\mu_i(\\theta)$ 和 $\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}$。\n- 使用阿西莫夫规定计算 $I_{\\text{full}}(\\theta)$。\n- 对于给定的概括维度 $p$ 和 $S$ 的选择规则，计算 $I_{\\text{sum}}(\\theta)$ 和灵敏度比率 $R(p, S, \\theta)$。\n\n必须支持两种对 $S$ 的选择规则：\n- 最佳-$p$：选择索引集 $S$，由 $|\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}|$ 最大的 $p$ 个箱组成。\n- 朴素-前-$p$：选择 $S = \\{1,2,\\dots,p\\}$（按索引顺序的前 $p$ 个箱）。\n\n为测试解决方案的不同方面，请使用以下参数元组 $(p, \\theta, \\text{rule}, \\alpha)$ 的测试套件，其中 $\\text{rule} \\in \\{ \\text{\"best\"}, \\text{\"first\"} \\}$，$\\alpha$ 通过将 $\\lambda_s$ 替换为 $\\alpha \\lambda_s$ 来缩放信号幅度：\n1. $(4, 0.50, \\text{\"best\"}, 1.0)$\n2. $(16, 0.50, \\text{\"best\"}, 1.0)$\n3. $(64, 0.50, \\text{\"best\"}, 1.0)$\n4. $(16, 0.05, \\text{\"best\"}, 1.0)$\n5. $(16, 0.50, \\text{\"first\"}, 1.0)$\n6. $(16, 0.50, \\text{\"best\"}, 0.2)$\n\n对于每个测试用例，以浮点数形式输出灵敏度比率 $R(p, S, \\theta)$。您的程序应生成单行输出，其中包含六个结果，形式为逗号分隔的列表，并用方括号括起来，每个浮点数四舍五入到六位小数（例如，$[0.123456,0.654321,\\dots]$）。不涉及物理单位；所有量都是无量纲的。不出现角度。不得使用百分比；输出为原始小数值。",
            "solution": "该问题陈述清晰、科学上合理，并为获得唯一解提供了所有必要信息。这是计算统计学在高能物理领域的一个标准练习，涉及分箱似然、费雪信息和概括统计量。因此，该问题被认为是有效的。\n\n问题的核心是计算灵敏度比率 $R(p, S, \\theta) = I_{\\text{sum}}(\\theta) / I_{\\text{full}}(\\theta)$，该比率量化了从全套箱计数转移到较小子集的概括统计量时的信息损失。这需要为分箱计数实验的两个不同统计模型计算费雪信息。\n\n首先，我们定义箱 $i$ 中期望事件数 $\\mu_i(\\theta)$ 的模型。$m$ 个箱将域 $x \\in [0, 1]$ 划分为区间 $[x_{i-1}, x_i]$，其中 $x_i = i/m$，$i=1, \\dots, m$。箱宽为 $\\Delta = 1/m$。期望计数是均匀本底分量和高斯信号分量之和：\n$$\n\\mu_i(\\theta) \\;=\\; \\lambda_b \\,\\Delta \\;+\\; \\lambda_s \\int_{x_{i-1}}^{x_i} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\,dx\n$$\n这里，$\\lambda_b$ 是本底率，$\\lambda_s$ 是信号幅度，$\\sigma$ 是信号宽度，$\\theta$ 是信号峰位置。\n\n在阿西莫夫机制中，观测计数 $n_i$ 被其期望值 $\\mu_i(\\theta)$ 替代，费雪信息由下式给出：\n$$\nI(\\theta) \\;=\\; \\sum_{i} \\frac{\\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^2}{\\mu_i(\\theta)}\n$$\n对于完整模型 ($I_{\\text{full}}$)，求和遍及所有箱；对于概括模型 ($I_{\\text{sum}}$)，求和遍及选定的箱子集 $S$。为了计算这个，我们需要导数 $\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}$。由于箱的边界不依赖于 $\\theta$，我们可以在积分号下进行微分（莱布尼茨法则）：\n$$\n\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta} \\;=\\; \\frac{\\partial}{\\partial \\theta} \\left( \\lambda_s \\int_{x_{i-1}}^{x_i} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\,dx \\right) \\;=\\; \\lambda_s \\int_{x_{i-1}}^{x_i} \\frac{\\partial}{\\partial \\theta} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\,dx\n$$\n被积函数的导数是：\n$$\n\\frac{\\partial}{\\partial \\theta} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right) \\;=\\; \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right) \\cdot \\left( -\\frac{2(x-\\theta)(-1)}{2\\sigma^2} \\right) \\;=\\; \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right) \\frac{x-\\theta}{\\sigma^2}\n$$\n因此，期望计数的导数表示为另一个积分：\n$$\n\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta} \\;=\\; \\lambda_s \\int_{x_{i-1}}^{x_i} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\frac{x-\\theta}{\\sigma^2}\\,dx\n$$\n问题规定，$\\mu_i(\\theta)$ 的积分和其导数的积分都必须使用中点求积法进行数值计算，每箱至少有 $N_{quad} \\ge 200$ 个样本。对于一个通用积分 $\\int_a^b f(x)dx$，该方法将其值近似为 $(b-a) \\cdot \\frac{1}{N_{quad}} \\sum_{k=1}^{N_{quad}} f(x_k)$，其中点 $x_k$ 是划分 $[a,b]$ 的 $N_{quad}$ 个相等子区间的中点。对于宽度为 $\\Delta$ 的箱 $i$，我们使用 $N_{quad}=200$ 个点。\n\n对于每个测试用例 $(p, \\theta, \\text{rule}, \\alpha)$，总体算法如下：\n1. 设置模型参数：$m=64$，$\\sigma=0.05$，$\\lambda_b=400$，以及 $\\lambda_s = \\alpha \\cdot 1200$。\n2. 对于每个箱 $i=1, \\dots, m$：\n   a. 定义箱边缘 $x_{i-1}=(i-1)/m$ 和 $x_i=i/m$。\n   b. 在 $[x_{i-1}, x_i]$ 内生成 $N_{quad}=200$ 个求积点。\n   c. 数值计算 $\\mu_i(\\theta)$ 的信号积分部分和 $\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}$ 的积分。\n   d. 将 $\\mu_i(\\theta)$ 和 $\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}$ 的完整值存储在数组中。\n3. 计算所有 $m$ 个箱的每箱费雪信息项，即 $\\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^2 / \\mu_i(\\theta)$。\n4. 通过对所有 $m$ 个箱的这些项求和，计算完整的费雪信息 $I_{\\text{full}}(\\theta)$。\n5. 根据给定的 `rule` 确定 $p$ 个箱的索引集 $S$：\n   - `\"best\"`：选择 $|\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}|$ 值最大的 $p$ 个箱。这对应于对 $\\theta$ 变化最敏感的箱。\n   - `\"first\"`：选择前 $p$ 个箱，即 $S = \\{1, 2, \\dots, p\\}$。\n6. 通过对集合 $S$ 中的箱的每箱信息项求和，计算概括统计量费雪信息 $I_{\\text{sum}}(\\theta)$。\n7. 计算灵敏度比率 $R(p, S, \\theta) = I_{\\text{sum}}(\\theta) / I_{\\text{full}}(\\theta)$。对于 $p=m$ 的情况，此比率应为 1。\n\n此过程在下面的 Python 程序中实现。它遍历指定的测试用例，为每个用例计算比率，并将结果格式化为所需的单行输出。",
            "answer": "```python\nimport numpy as np\n\ndef compute_sensitivity_ratio(p, theta, rule, alpha, m, sigma, lambda_b, lambda_s_base, n_quad):\n    \"\"\"\n    Computes the sensitivity ratio R(p, S, theta) for a given set of parameters.\n    \"\"\"\n    # Set model parameters for the current test case\n    lambda_s = alpha * lambda_s_base\n    delta = 1.0 / m\n    sigma2 = sigma**2\n\n    # Arrays to store bin-wise calculations\n    mu = np.zeros(m)\n    dmu_dtheta = np.zeros(m)\n    \n    # Pre-calculate quadrature offsets for re-use in each bin\n    # These are the midpoints of n_quad sub-intervals of width 1.\n    quad_offsets = (np.arange(n_quad) + 0.5) / n_quad\n\n    # Loop over each bin to calculate mu_i and its derivative\n    for i in range(m):\n        bin_edge_low = i / m\n        \n        # Generate N_quad sample points at the midpoints of sub-intervals within the current bin\n        x_samples = bin_edge_low + quad_offsets * delta\n        \n        # Evaluate the integrands at the sample points\n        signal_integrand_values = np.exp(-(x_samples - theta)**2 / (2 * sigma2))\n        deriv_integrand_values = signal_integrand_values * (x_samples - theta) / sigma2\n        \n        # Approximate the integrals using the mean of the integrand values\n        integral_signal = np.mean(signal_integrand_values) * delta\n        integral_deriv = np.mean(deriv_integrand_values) * delta\n        \n        # Compute the final mu_i and d(mu_i)/d(theta)\n        mu[i] = lambda_b * delta + lambda_s * integral_signal\n        dmu_dtheta[i] = lambda_s * integral_deriv\n\n    # Calculate the per-bin terms for Fisher information\n    # Adding a small epsilon to the denominator for numerical stability, though mu > 0 is guaranteed by background.\n    info_terms = (dmu_dtheta**2) / (mu + 1e-15)\n\n    # Calculate the full Fisher Information by summing over all bins\n    I_full = np.sum(info_terms)\n\n    # If I_full is zero, the ratio is ill-defined. This should not happen with the given parameters.\n    # Return a meaningful value based on p.\n    if I_full == 0:\n        return 1.0 if p == m else 0.0\n\n    # Determine the index set S based on the selection rule\n    if rule == \"best\":\n        # Select indices of the p bins with the largest absolute derivative\n        abs_dmu = np.abs(dmu_dtheta)\n        # argsort sorts in ascending order, so we take the last p indices\n        indices_S = np.argsort(abs_dmu)[-p:]\n    elif rule == \"first\":\n        # Select the first p bins\n        indices_S = np.arange(p)\n    else:\n        # This case should not be reached with the given problem statement\n        raise ValueError(f\"Unknown selection rule: {rule}\")\n\n    # Calculate the summary-statistic Fisher Information by summing over the selected bins\n    I_sum = np.sum(info_terms[indices_S])\n\n    # Compute the final sensitivity ratio\n    R = I_sum / I_full\n    \n    return R\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Baseline model parameters from the problem statement\n    m = 64\n    sigma = 0.05\n    lambda_b = 400.0\n    lambda_s_base = 1200.0\n    n_quad = 200\n\n    # Test suite of parameter tuples (p, theta, rule, alpha)\n    test_cases = [\n        (4, 0.50, \"best\", 1.0),\n        (16, 0.50, \"best\", 1.0),\n        (64, 0.50, \"best\", 1.0),\n        (16, 0.05, \"best\", 1.0),\n        (16, 0.50, \"first\", 1.0),\n        (16, 0.50, \"best\", 0.2),\n    ]\n\n    results = []\n    for p, theta, rule, alpha in test_cases:\n        ratio = compute_sensitivity_ratio(\n            p, theta, rule, alpha,\n            m, sigma, lambda_b, lambda_s_base, n_quad\n        )\n        # Format the result to six decimal places\n        results.append(f\"{ratio:.6f}\")\n\n    # Print the final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后的这个练习将我们的关注点从参数估计转向了数据分析中的一个核心任务：模型选择。我们将使用赤池信息准则（$AIC$）和贝叶斯信息准则（$BIC$）来判断数据究竟支持一个简单模型还是一个更复杂的模型。通过这个练习，你将深入比较非分箱和分箱分析在模型选择任务中的表现，并理解数据离散化如何对这些准则的效力产生微妙而重要的影响。",
            "id": "3540403",
            "problem": "给定一个一维高斯概率密度函数（PDF）的混合模型，作为独立事件级观测值的生成模型；以及其对应的分箱（binned）形式，作为箱计数（bin counts）的泊松过程。考虑两个由高斯分量数 $K \\in \\{1,2\\}$ 索引的竞争模型族，其未知参数将通过最大似然法进行拟合。对于未分箱数据，假设独立同分布的事件级观测值 $\\{x_i\\}_{i=1}^N$ 是从一个归一化的混合密度 $p(x \\mid \\theta, K) = \\sum_{j=1}^{K} w_j \\, \\phi(x; \\mu_j, \\sigma_j)$ 中抽取的，其中 $\\phi(x;\\mu,\\sigma)$ 是一个均值为 $\\mu$、标准差为 $\\sigma$ 的高斯PDF，混合权重满足 $\\sum_{j=1}^{K} w_j = 1$ 和 $w_j \\ge 0$，$\\theta$ 表示完整的参数矢量。对于分箱数据，观测值是一个包含 $B$ 个箱的独立泊松计数的矢量 $\\{n_b\\}_{b=1}^{B}$，这些箱的边界为 $\\{a_b,b_b\\}$，在参数 $\\theta$ 下，箱 $b$ 中的期望计数为 $E_b(\\theta, K) = N \\, P_b(\\theta, K)$，其中 $P_b(\\theta, K) = \\int_{a_b}^{b_b} p(x \\mid \\theta, K)\\, dx$，而 $N = \\sum_{b=1}^B n_b$ 是固定的事件总数。\n\n从下方的基本定义出发，在未分箱和分箱设置下实现并比较赤池信息准则（AIC）和贝叶斯信息准则（BIC），并研究离散化（分箱）如何改变这些惩罚项在选择高斯分量数量方面的有效性。您的实现必须是一个完整的、可运行的程序，该程序执行最大似然拟合并计算AIC/BIC以在 $K=1$ 和 $K=2$ 之间进行选择。\n\n需要使用的基本定律和定义：\n- 对于未分箱的独立数据 $\\{x_i\\}_{i=1}^N$，其对数似然为 $\\ell_{\\mathrm{unb}}(\\theta, K) = \\sum_{i=1}^{N} \\log p(x_i \\mid \\theta, K)$，其中 $p(x \\mid \\theta, K)$ 是一个归一化的PDF。\n- 对于具有期望计数 $\\{E_b(\\theta, K)\\}_{b=1}^B$ 的分箱独立泊松计数 $\\{n_b\\}_{b=1}^B$，其扩展对数似然为 $\\ell_{\\mathrm{bin}}(\\theta, K) = \\sum_{b=1}^B \\left[n_b \\log E_b(\\theta, K) - E_b(\\theta, K) - \\log(n_b!)\\right]$。在最大化和模型比较中，如果与 $\\theta$ 无关的项在不同模型间能够抵消，则可以舍弃它们。\n- 赤池信息准则（AIC）是 $\\mathrm{AIC} = 2k - 2 \\, \\ell(\\hat{\\theta}, K)$，其中 $k$ 是自由参数的数量，$\\hat{\\theta}$ 是最大似然估计量。贝叶斯信息准则（BIC）是 $\\mathrm{BIC} = k \\log M - 2 \\, \\ell(\\hat{\\theta}, K)$，其中 $M$ 是样本大小：对于未分箱拟合，使用 $M=N$；对于分箱拟合，使用 $M=B$（独立泊松计数的数量）。在这两个模型族中，$K=1$ 时使用 $k=2$（一个均值和一个标准差），$K=2$ 时使用 $k=5$（两个均值、两个标准差和一个独立的混合权重）。\n- 当对一个高斯PDF在箱上积分时，会涉及到高斯累积分布函数（CDF），即 $P_b(\\theta, K) = \\sum_{j=1}^K w_j \\left[\\Phi\\!\\left(\\frac{b_b - \\mu_j}{\\sigma_j}\\right) - \\Phi\\!\\left(\\frac{a_b - \\mu_j}{\\sigma_j}\\right)\\right]$，其中 $\\Phi$ 是高斯CDF。\n\n你的程序必须：\n- 为每个测试用例，使用固定的随机种子从指定的真实混合模型中模拟未分箱数据，然后通过在由真实混合模型的总体均值和方差确定的范围内，将数据直方图化为指定数量的等宽箱，来构建分箱计数。\n- 对每个候选模型 $K \\in \\{1,2\\}$，分别为未分箱似然和分箱似然执行最大似然估计。对于未分箱情况，$K=2$ 时可使用期望最大化（EM）算法，$K=1$ 时可使用闭式最大似然估计量。对于分箱情况，执行分箱对数似然的数值优化（可以舍弃在模型选择中抵消的常数项），并使用合适的无约束重新参数化来强制执行正定性和混合权重约束。\n- 在未分箱和分箱设置中为 $K=1$ 和 $K=2$ 计算 $\\mathrm{AIC}$ 和 $\\mathrm{BIC}$，并选择使每个准则最小化的 $K$。\n- 对于每个测试用例，输出一个包含四个整数的列表 $[K_{\\mathrm{unb,AIC}}, K_{\\mathrm{unb,BIC}}, K_{\\mathrm{bin,AIC}}, K_{\\mathrm{bin,BIC}}]$。\n\n确保覆盖率的测试套件：\n- 用例1（单分量，粗分箱）：seed $= 12345$，$N=1000$，真实分量 $[(1.0, 0.0, 1.0)]$，$B=12$。\n- 用例2（单分量，细分箱）：seed $= 12345$，$N=1000$，真实分量 $[(1.0, 0.0, 1.0)]$，$B=60$。\n- 用例3（良好分离的双分量，粗分箱）：seed $= 12346$，$N=1000$，真实分量 $[(0.5, -2.0, 0.7), (0.5, 2.0, 0.7)]$，$B=12$。\n- 用例4（良好分离的双分量，细分箱）：seed $= 12346$，$N=1000$，真实分量 $[(0.5, -2.0, 0.7), (0.5, 2.0, 0.7)]$，$B=60$。\n- 用例5（重叠的双分量，粗分箱）：seed $= 12347$，$N=500$，真实分量 $[(0.7, 0.0, 1.0), (0.3, 0.8, 1.0)]$，$B=12$。\n- 用例6（重叠的双分量，细分箱）：seed $= 12347$，$N=500$，真实分量 $[(0.7, 0.0, 1.0), (0.3, 0.8, 1.0)]$，$B=60$。\n- 用例7（小样本，双分量，粗分箱）：seed $= 12348$，$N=80$，真实分量 $[(0.5, -1.0, 1.0), (0.5, 1.0, 1.0)]$，$B=12$。\n- 用例8（小样本，双分量，细分箱）：seed $= 12348$，$N=80$，真实分量 $[(0.5, -1.0, 1.0), (0.5, 1.0, 1.0)]$，$B=60$。\n\n范围和分箱说明：\n- 对每个用例，将分箱范围定义为 $[\\mu_{\\mathrm{mix}} - 5 \\sigma_{\\mathrm{mix}}, \\mu_{\\mathrm{mix}} + 5 \\sigma_{\\mathrm{mix}}]$，其中 $\\mu_{\\mathrm{mix}} = \\sum_j w_j \\mu_j$，$\\sigma_{\\mathrm{mix}}^2 = \\sum_j w_j (\\sigma_j^2 + \\mu_j^2) - \\mu_{\\mathrm{mix}}^2$。在此范围内使用 $B$ 个等宽的箱。\n\n最终输出格式：\n- 你的程序应生成单行输出，包含一个无空格、逗号分隔的列表之列表，按用例1到8的顺序排列结果，例如：\"[[k11,k12,k13,k14],[k21,k22,k23,k24],...,[k81,k82,k83,k84]]\"，其中每个 $k$ 是一个在 $\\{1,2\\}$ 中的整数，表示在指定准则和数据类型下所选的分量数。",
            "solution": "该问题是有效的，因为它具有科学依据、提法明确，并包含了获得唯一、可验证解所需的所有信息。它要求在情境为高能物理数据分析的背景下，实现标准的统计方法。解决方案的步骤是首先生成数据，然后在两个模型假设（$K=1$ 和 $K=2$ 个高斯分量）下，对数据的未分箱和分箱表示执行最大似然拟合，最后应用信息准则来选择首选模型。\n\n### 1. 数据模拟与准备\n对于每个测试用例，模拟一个包含 $N$ 个独立事件的数据集 $\\{x_i\\}_{i=1}^N$。每个事件都从真实的生成概率密度函数（PDF）中抽取，该函数是高斯分布的混合：\n$$p(x \\mid \\theta_{\\mathrm{true}}, K_{\\mathrm{true}}) = \\sum_{j=1}^{K_{\\mathrm{true}}} w_j \\, \\phi(x; \\mu_j, \\sigma_j)$$\n其中 $\\phi(x; \\mu, \\sigma)$ 是高斯PDF，$\\{w_j, \\mu_j, \\sigma_j\\}$ 是真实的混合参数。这通过首先以概率 $w_j$ 抽样一个分量索引 $j$，然后从相应的高斯分布 $\\mathcal{N}(\\mu_j, \\sigma_j^2)$ 中抽取一个值来实现。\n\n对于分箱分析，数据被离散化为一个包含 $B$ 个箱的直方图。直方图的范围由真实混合分布的总体均值 $\\mu_{\\mathrm{mix}}$ 和标准差 $\\sigma_{\\mathrm{mix}}$ 决定，范围为 $[\\mu_{\\mathrm{mix}} - 5 \\sigma_{\\mathrm{mix}}, \\mu_{\\mathrm{mix}} + 5 \\sigma_{\\mathrm{mix}}]$。混合均值为 $\\mu_{\\mathrm{mix}} = \\sum_j w_j \\mu_j$。混合方差 $\\sigma_{\\mathrm{mix}}^2$ 使用全方差定律计算：\n$$\\sigma_{\\mathrm{mix}}^2 = E[\\sigma^2(X|C)] + \\mathrm{Var}(E[X|C]) = \\sum_{j=1}^{K_{\\mathrm{true}}} w_j \\sigma_j^2 + \\left( \\sum_{j=1}^{K_{\\mathrm{true}}} w_j \\mu_j^2 \\right) - \\mu_{\\mathrm{mix}}^2$$\n生成的直方图包含每个箱中的计数 $\\{n_b\\}_{b=1}^B$，其中 $\\sum_b n_b = N$。\n\n### 2. 未分箱最大似然估计\n目标是找到使对数似然函数 $\\ell_{\\mathrm{unb}}(\\theta, K) = \\sum_{i=1}^{N} \\log p(x_i \\mid \\theta, K)$ 最大化的参数矢量 $\\hat{\\theta}$。\n\n对于 $K=1$ 模型，$p(x \\mid \\theta, 1) = \\phi(x; \\mu, \\sigma)$。参数的最大似然估计量（MLEs）具有闭式解：样本均值 $\\hat{\\mu} = \\frac{1}{N}\\sum_i x_i$ 和样本方差 $\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_i (x_i - \\hat{\\mu})^2$。最大化的对数似然则为 $\\ell_{\\mathrm{unb}}(\\hat{\\theta}, 1) = \\sum_{i=1}^{N} \\log \\phi(x_i; \\hat{\\mu}, \\hat{\\sigma})$。\n\n对于 $K=2$ 模型，$p(x \\mid \\theta, 2) = w_1 \\phi(x; \\mu_1, \\sigma_1) + (1-w_1) \\phi(x; \\mu_2, \\sigma_2)$。该混合模型的似然函数没有闭式最大值。期望最大化（EM）算法是适用于此问题的迭代过程。\n1.  **初始化**：参数 $(\\mu_1, \\sigma_1, \\mu_2, \\sigma_2, w_1)$ 被初始化。使用一种确定性方法，将排序后的数据分成两部分，每部分的矩为初始均值和标准差提供依据。\n2.  **E步**：计算分量 $j$ 生成数据点 $x_i$ 的后验概率，或称“责任”（responsibility）：\n    $$\\gamma_{ij} = \\frac{w_j \\phi(x_i; \\mu_j, \\sigma_j)}{\\sum_{l=1}^{2} w_l \\phi(x_i; \\mu_l, \\sigma_l)}$$\n3.  **M步**：在给定当前责任的情况下，更新参数以最大化期望对数似然：\n    $$N_j = \\sum_{i=1}^{N} \\gamma_{ij}, \\quad w_j^{\\mathrm{new}} = \\frac{N_j}{N}, \\quad \\mu_j^{\\mathrm{new}} = \\frac{1}{N_j} \\sum_{i=1}^{N} \\gamma_{ij} x_i, \\quad (\\sigma_j^{\\mathrm{new}})^2 = \\frac{1}{N_j} \\sum_{i=1}^{N} \\gamma_{ij} (x_i - \\mu_j^{\\mathrm{new}})^2$$\n重复E步和M步，直到总对数似然 $\\ell_{\\mathrm{unb}}(\\theta, 2) = \\sum_i \\log p(x_i \\mid \\theta, 2)$ 的变化量低于一个容差。\n\n### 3. 分箱最大似然估计\n对于分箱数据，计数 $\\{n_b\\}$ 被视为期望为 $\\{E_b(\\theta, K)\\}$ 的独立泊松随机变量。模型选择基于分箱扩展对数似然，其中与参数 $\\theta$ 无关的项（即 $\\sum_b \\log(n_b!)$）被省略，因为它们在模型比较中会抵消：\n$$\\ell^*_{\\mathrm{bin}}(\\theta, K) = \\sum_{b=1}^{B} \\left[ n_b \\log E_b(\\theta, K) - E_b(\\theta, K) \\right]$$\n在边界为 $[a_b, b_b]$ 的箱 $b$ 中的期望计数由 $E_b(\\theta, K) = N \\cdot P_b(\\theta, K)$ 给出，其中 $P_b(\\theta, K)$ 是一个事件落入该箱的概率。这个概率通过在箱上对PDF进行积分来计算，这涉及到高斯累积分布函数（CDF），$\\Phi(z)$：\n$$P_b(\\theta, K) = \\int_{a_b}^{b_b} p(x \\mid \\theta, K) \\, dx = \\sum_{j=1}^{K} w_j \\left[ \\Phi\\left(\\frac{b_b - \\mu_j}{\\sigma_j}\\right) - \\Phi\\left(\\frac{a_b - \\mu_j}{\\sigma_j}\\right) \\right]$$\n最大化 $\\ell^*_{\\mathrm{bin}}$ 是通过数值优化进行的。为处理参数约束，优化在无约束空间中进行。标准差 $\\sigma_j > 0$ 被重新参数化为 $\\sigma_j = \\exp(\\log \\sigma_j)$。$K=2$ 情况下的混合权重 $w_1 \\in [0, 1]$ 使用 logit 变换重新参数化，$w_1 = (1 + \\exp(-\\alpha_1))^{-1}$。从未分箱拟合中找到的参数作为数值最小化器的初始值，以促进稳定和高效的收敛。\n\n### 4. 模型选择\n赤池信息准则（AIC）和贝叶斯信息准则（BIC）用于比较 $K=1$ 和 $K=2$ 模型的性能。值较低的准则所对应的模型是更优选的。\n$$\\mathrm{AIC} = 2k - 2 \\ell_{\\mathrm{max}}$$\n$$\\mathrm{BIC} = k \\log M - 2 \\ell_{\\mathrm{max}}$$\n这里，$\\ell_{\\mathrm{max}}$是最大化的对数似然（$\\ell_{\\mathrm{unb}}$ 或 $\\ell^*_{\\mathrm{bin}}$），$k$ 是模型中自由参数的数量。根据规定，$K=1$ 模型 $(\\mu, \\sigma)$ 的 $k=2$，$K=2$ 模型 $(\\mu_1, \\sigma_1, \\mu_2, \\sigma_2, w_1)$ 的 $k=5$。样本大小 $M$ 定义为独立观测值的数量：对于未分箱拟合，$M=N$（事件数）；对于分箱拟合，$M=B$（箱数）。此分析对未分箱和分箱数据都进行，为每个测试用例产生四个模型选择结果。",
            "answer": "```python\nimport numpy as np\nfrom scipy import stats, optimize, special\n\ndef solve():\n    \"\"\"\n    Implements the full analysis pipeline for comparing AIC/BIC in unbinned and binned\n    Gaussian mixture model fits, as specified in the problem statement.\n    \"\"\"\n\n    test_cases = [\n        {'seed': 12345, 'N': 1000, 'components': [(1.0, 0.0, 1.0)], 'B': 12},\n        {'seed': 12345, 'N': 1000, 'components': [(1.0, 0.0, 1.0)], 'B': 60},\n        {'seed': 12346, 'N': 1000, 'components': [(0.5, -2.0, 0.7), (0.5, 2.0, 0.7)], 'B': 12},\n        {'seed': 12346, 'N': 1000, 'components': [(0.5, -2.0, 0.7), (0.5, 2.0, 0.7)], 'B': 60},\n        {'seed': 12347, 'N': 500, 'components': [(0.7, 0.0, 1.0), (0.3, 0.8, 1.0)], 'B': 12},\n        {'seed': 12347, 'N': 500, 'components': [(0.7, 0.0, 1.0), (0.3, 0.8, 1.0)], 'B': 60},\n        {'seed': 12348, 'N': 80, 'components': [(0.5, -1.0, 1.0), (0.5, 1.0, 1.0)], 'B': 12},\n        {'seed': 12348, 'N': 80, 'components': [(0.5, -1.0, 1.0), (0.5, 1.0, 1.0)], 'B': 60},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # --- 1. Data Generation and Preparation ---\n        rng = np.random.default_rng(case['seed'])\n        N = case['N']\n        components = case['components']\n        n_comp = len(components)\n        \n        if n_comp == 1:\n            w, mu, sigma = components[0]\n            unbinned_data = rng.normal(mu, sigma, N)\n        else:\n            weights, means, sigmas = zip(*components)\n            component_indices = rng.choice(n_comp, size=N, p=weights)\n            unbinned_data = np.concatenate([\n                rng.normal(means[i], sigmas[i], np.sum(component_indices == i))\n                for i in range(n_comp)\n            ])\n\n        B = case['B']\n        k_params = {1: 2, 2: 5}\n\n        # --- 2. Unbinned Fits ---\n        # K=1\n        mu1_unb, sigma1_unb = np.mean(unbinned_data), np.std(unbinned_data)\n        logL_unb_k1 = np.sum(stats.norm.logpdf(unbinned_data, loc=mu1_unb, scale=sigma1_unb))\n        \n        # K=2 (EM algorithm)\n        x_sorted = np.sort(unbinned_data)\n        split = N // 2\n        mu1_init, mu2_init = np.mean(x_sorted[:split]), np.mean(x_sorted[split:])\n        sigma1_init, sigma2_init = np.std(x_sorted[:split]), np.std(x_sorted[split:])\n        w1_init = 0.5\n        \n        mu1_em, mu2_em = mu1_init, mu2_init\n        sigma1_em, sigma2_em = max(sigma1_init, 1e-6), max(sigma2_init, 1e-6)\n        w1_em = w1_init\n        \n        for _ in range(100):\n            pdf1 = stats.norm.pdf(unbinned_data, mu1_em, sigma1_em)\n            pdf2 = stats.norm.pdf(unbinned_data, mu2_em, sigma2_em)\n            \n            resp_num1 = w1_em * pdf1\n            denominator = resp_num1 + (1 - w1_em) * pdf2\n            denominator[denominator  1e-9] = 1e-9\n            resp1 = resp_num1 / denominator\n            \n            N1 = np.sum(resp1)\n            if N1  1e-6 or N - N1  1e-6: break\n\n            w1_em = N1 / N\n            mu1_em = np.sum(resp1 * unbinned_data) / N1\n            mu2_em = np.sum((1 - resp1) * unbinned_data) / (N - N1)\n            sigma1_em = np.sqrt(np.sum(resp1 * (unbinned_data - mu1_em)**2) / N1 + 1e-9)\n            sigma2_em = np.sqrt(np.sum((1 - resp1) * (unbinned_data - mu2_em)**2) / (N - N1) + 1e-9)\n\n        logL_unb_k2 = np.sum(np.log(w1_em * stats.norm.pdf(unbinned_data, mu1_em, sigma1_em) + \n                                     (1 - w1_em) * stats.norm.pdf(unbinned_data, mu2_em, sigma2_em) + 1e-12))\n        \n        # Unbinned model selection\n        aic_unb_k1, bic_unb_k1 = 2 * k_params[1] - 2 * logL_unb_k1, k_params[1] * np.log(N) - 2 * logL_unb_k1\n        aic_unb_k2, bic_unb_k2 = 2 * k_params[2] - 2 * logL_unb_k2, k_params[2] * np.log(N) - 2 * logL_unb_k2\n        K_unb_aic = 1 if aic_unb_k1 = aic_unb_k2 else 2\n        K_unb_bic = 1 if bic_unb_k1 = bic_unb_k2 else 2\n\n        # --- 3. Binned Fits ---\n        true_w, true_mu, true_s = zip(*components)\n        mu_mix = np.sum(np.array(true_w) * np.array(true_mu))\n        var_mix = np.sum(np.array(true_w) * (np.array(true_s)**2 + np.array(true_mu)**2)) - mu_mix**2\n        bin_range = (mu_mix - 5 * np.sqrt(var_mix), mu_mix + 5 * np.sqrt(var_mix))\n        binned_counts, bin_edges = np.histogram(unbinned_data, bins=B, range=bin_range)\n        \n        # Binned Negative Log-Likelihood\n        def binned_nll(params, K):\n            if K == 1:\n                mu1, log_sigma1 = params\n                prob = stats.norm.cdf(bin_edges[1:], mu1, np.exp(log_sigma1)) - \\\n                       stats.norm.cdf(bin_edges[:-1], mu1, np.exp(log_sigma1))\n            else: # K=2\n                mu1, log_sigma1, mu2, log_sigma2, alpha1 = params\n                w1 = special.expit(alpha1)\n                p1 = stats.norm.cdf(bin_edges[1:], mu1, np.exp(log_sigma1)) - \\\n                     stats.norm.cdf(bin_edges[:-1], mu1, np.exp(log_sigma1))\n                p2 = stats.norm.cdf(bin_edges[1:], mu2, np.exp(log_sigma2)) - \\\n                     stats.norm.cdf(bin_edges[:-1], mu2, np.exp(log_sigma2))\n                prob = w1 * p1 + (1 - w1) * p2\n            \n            E_b = N * prob\n            nll = -np.sum(binned_counts * np.log(E_b + 1e-12) - E_b)\n            return nll if np.isfinite(nll) else np.inf\n\n        # K=1 binned fit\n        init_bin_k1 = [mu1_unb, np.log(sigma1_unb)]\n        res_bin_k1 = optimize.minimize(binned_nll, init_bin_k1, args=(1,), method='Nelder-Mead')\n        logL_bin_k1 = -res_bin_k1.fun\n\n        # K=2 binned fit\n        w1_init_clip = np.clip(w1_em, 1e-6, 1 - 1e-6)\n        init_bin_k2 = [mu1_em, np.log(sigma1_em), mu2_em, np.log(sigma2_em), special.logit(w1_init_clip)]\n        res_bin_k2 = optimize.minimize(binned_nll, init_bin_k2, args=(2,), method='Nelder-Mead')\n        logL_bin_k2 = -res_bin_k2.fun\n\n        # Binned model selection\n        aic_bin_k1, bic_bin_k1 = 2 * k_params[1] - 2 * logL_bin_k1, k_params[1] * np.log(B) - 2 * logL_bin_k1\n        aic_bin_k2, bic_bin_k2 = 2 * k_params[2] - 2 * logL_bin_k2, k_params[2] * np.log(B) - 2 * logL_bin_k2\n        K_bin_aic = 1 if aic_bin_k1 = aic_bin_k2 else 2\n        K_bin_bic = 1 if bic_bin_k1 = bic_bin_k2 else 2\n\n        results.append([K_unb_aic, K_unb_bic, K_bin_aic, K_bin_bic])\n\n    # --- 4. Final Output ---\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}