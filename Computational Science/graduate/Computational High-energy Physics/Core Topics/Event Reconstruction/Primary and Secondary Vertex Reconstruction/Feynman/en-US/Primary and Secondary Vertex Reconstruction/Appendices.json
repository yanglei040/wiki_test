{
    "hands_on_practices": [
        {
            "introduction": "Our journey into vertex reconstruction begins with the fundamentals of combining information from multiple tracks. This first practice establishes the theoretical bedrock of a weighted least-squares fit, a method equivalent to Maximum Likelihood Estimation under Gaussian uncertainties. By deriving the expected vertex resolution from the Fisher Information Matrix, you will gain a first-principles understanding of how the precision of a reconstructed vertex depends on the number and quality of the constituent tracks .",
            "id": "3528910",
            "problem": "Consider a simplified linearized primary vertex reconstruction in a collider detector where the transverse coordinate along the local detector $x$-axis and the longitudinal coordinate $z$ are estimated independently from track-level constraints. Each of $N$ reconstructed charged-particle tracks provides two one-dimensional residuals relative to the unknown primary vertex location, modeled as\n$$\ny_{x,i} \\;=\\; x \\;+\\; \\epsilon_{x,i}, \\qquad y_{z,i} \\;=\\; z \\;+\\; \\epsilon_{z,i}, \\qquad i = 1,\\dots,N,\n$$\nwhere $x$ and $z$ are the true primary vertex coordinates to be estimated, and $\\epsilon_{x,i}$ and $\\epsilon_{z,i}$ are independent Gaussian noises with zero mean and known variances $\\sigma_{d_0,i}^{2}$ and $\\sigma_{z_0,i}^{2}$, respectively. Assume all tracks are mutually independent, and that the transverse and longitudinal fits are decoupled so that cross-derivatives vanish in the linearization.\n\nA robust vertex fit assigns to each track a deterministic weight $w_i \\in [0,1]$ that modulates its contribution to the objective without altering the per-track Gaussian noise model. The fit minimizes the weighted sum-of-squares objective\n$$\n\\mathcal{L}(x,z) \\;=\\; \\frac{1}{2}\\sum_{i=1}^{N} w_i\\left(\\frac{(y_{x,i}-x)^{2}}{\\sigma_{d_0,i}^{2}} \\;+\\; \\frac{(y_{z,i}-z)^{2}}{\\sigma_{z_0,i}^{2}}\\right).\n$$\nUnder the standard Gaussian approximation near the optimum, the covariance of the estimator obtained by Maximum Likelihood Estimation (MLE) is the inverse of the Fisher Information Matrix (FIM) accumulated from the independent track constraints.\n\nStarting solely from the above Gaussian measurement model, independence assumptions, and the definition of the Fisher Information Matrix for Gaussian noise, derive the closed-form analytical expressions for the expected primary vertex resolutions (one standard deviation) in $x$ and $z$ as functions of $N$, the track weights $w_i$, and the per-track uncertainties $\\sigma_{d_0,i}$ and $\\sigma_{z_0,i}$. Express your final answer as a single row matrix containing the two resolutions. Do not substitute any numerical values; provide the analytic expressions only. No unit conversion is required. For clarity, present the final expressions in terms of $w_i$, $\\sigma_{d_0,i}$, and $\\sigma_{z_0,i}$.",
            "solution": "The problem is valid as it presents a self-contained, scientifically grounded, and well-posed question in statistical estimation, relevant to high-energy physics data analysis.\n\nThe goal is to find the resolutions for the estimated vertex coordinates, $\\hat{x}$ and $\\hat{z}$. The resolution is defined as the standard deviation of the estimator. The problem states that the covariance matrix of the estimators is the inverse of the Fisher Information Matrix (FIM). The parameters to be estimated are $\\theta = \\begin{pmatrix} x \\\\ z \\end{pmatrix}$. The FIM, denoted by $\\mathbf{I}$, is a $2 \\times 2$ matrix whose elements are given by\n$$\nI_{jk} = -E\\left[\\frac{\\partial^2 \\ln L}{\\partial \\theta_j \\partial \\theta_k}\\right]\n$$\nwhere $L$ is the likelihood function. For a least-squares problem with a Gaussian noise model, minimizing the sum-of-squares objective function is equivalent to maximizing the likelihood. The provided objective function is\n$$\n\\mathcal{L}(x,z) = \\frac{1}{2}\\sum_{i=1}^{N} w_i\\left(\\frac{(y_{x,i}-x)^{2}}{\\sigma_{d_0,i}^{2}} + \\frac{(y_{z,i}-z)^{2}}{\\sigma_{z_0,i}^{2}}\\right)\n$$\nThis weighted sum-of-squares function plays the role of the negative log-likelihood (up to an additive constant). Let us define an effective log-likelihood, $l_{\\text{eff}}(x,z)$, as\n$$\nl_{\\text{eff}}(x,z) = - \\mathcal{L}(x,z) = -\\frac{1}{2}\\sum_{i=1}^{N} w_i\\left(\\frac{(y_{x,i}-x)^{2}}{\\sigma_{d_0,i}^{2}} + \\frac{(y_{z,i}-z)^{2}}{\\sigma_{z_0,i}^{2}}\\right)\n$$\nWe compute the FIM based on this effective log-likelihood. The elements of the FIM are the negative expectation values of the second partial derivatives of $l_{\\text{eff}}$.\n\nFirst, we compute the second partial derivative with respect to $x$:\n$$\n\\frac{\\partial l_{\\text{eff}}}{\\partial x} = -\\frac{1}{2} \\sum_{i=1}^{N} w_i \\frac{\\partial}{\\partial x} \\left(\\frac{(y_{x,i}-x)^{2}}{\\sigma_{d_0,i}^{2}}\\right) = -\\frac{1}{2} \\sum_{i=1}^{N} w_i \\frac{2(y_{x,i}-x)(-1)}{\\sigma_{d_0,i}^{2}} = \\sum_{i=1}^{N} w_i \\frac{y_{x,i}-x}{\\sigma_{d_0,i}^{2}}\n$$\n$$\n\\frac{\\partial^2 l_{\\text{eff}}}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\left( \\sum_{i=1}^{N} w_i \\frac{y_{x,i}-x}{\\sigma_{d_0,i}^{2}} \\right) = \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d_0,i}^{2}} (-1) = - \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d_0,i}^{2}}\n$$\nThis second derivative is a constant with respect to the random variables $y_{x,i}$, so its expectation is the value itself. The $(1,1)$ element of the FIM is:\n$$\nI_{xx} = -E\\left[\\frac{\\partial^2 l_{\\text{eff}}}{\\partial x^2}\\right] = - \\left( - \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d_0,i}^{2}} \\right) = \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d_0,i}^{2}}\n$$\nSimilarly, for the $z$ coordinate:\n$$\n\\frac{\\partial^2 l_{\\text{eff}}}{\\partial z^2} = - \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{z_0,i}^{2}}\n$$\nThe $(2,2)$ element of the FIM is:\n$$\nI_{zz} = -E\\left[\\frac{\\partial^2 l_{\\text{eff}}}{\\partial z^2}\\right] = - \\left( - \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{z_0,i}^{2}} \\right) = \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{z_0,i}^{2}}\n$$\nFinally, we compute the off-diagonal element. Since the objective function $\\mathcal{L}(x,z)$ is separable into functions of $x$ and $z$, the mixed partial derivative is zero:\n$$\n\\frac{\\partial^2 l_{\\text{eff}}}{\\partial x \\partial z} = \\frac{\\partial}{\\partial x} \\left( \\frac{\\partial l_{\\text{eff}}}{\\partial z} \\right) = \\frac{\\partial}{\\partial x} \\left( \\sum_{i=1}^{N} w_i \\frac{y_{z,i}-z}{\\sigma_{z_0,i}^{2}} \\right) = 0\n$$\nThus, the off-diagonal elements of the FIM are zero: $I_{xz} = I_{zx} = 0$. This confirms the problem statement's condition that the fits are decoupled.\n\nThe FIM is a diagonal matrix:\n$$\n\\mathbf{I} = \\begin{pmatrix} \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d_0,i}^{2}} & 0 \\\\ 0 & \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{z_0,i}^{2}} \\end{pmatrix}\n$$\nThe covariance matrix of the estimators, $\\mathbf{C}$, is the inverse of the FIM:\n$$\n\\mathbf{C} = \\mathbf{I}^{-1} = \\begin{pmatrix} \\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d_0,i}^{2}}\\right)^{-1} & 0 \\\\ 0 & \\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{z_0,i}^{2}}\\right)^{-1} \\end{pmatrix}\n$$\nThe variances of the estimators $\\hat{x}$ and $\\hat{z}$ are the diagonal elements of the covariance matrix:\n$$\n\\text{Var}(\\hat{x}) = C_{11} = \\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d_0,i}^{2}}\\right)^{-1}\n$$\n$$\n\\text{Var}(\\hat{z}) = C_{22} = \\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{z_0,i}^{2}}\\right)^{-1}\n$$\nThe expected resolutions are the standard deviations, which are the square roots of the variances.\nThe resolution in $x$, $\\sigma_{\\hat{x}}$, is:\n$$\n\\sigma_{\\hat{x}} = \\sqrt{\\text{Var}(\\hat{x})} = \\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d_0,i}^{2}}\\right)^{-1/2}\n$$\nThe resolution in $z$, $\\sigma_{\\hat{z}}$, is:\n$$\n\\sigma_{\\hat{z}} = \\sqrt{\\text{Var}(\\hat{z})} = \\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{z_0,i}^{2}}\\right)^{-1/2}\n$$\nThe final answer is presented as a row matrix containing these two resolutions.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d_0,i}^{2}}\\right)^{-1/2} & \\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{z_0,i}^{2}}\\right)^{-1/2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the standard least-squares fit is optimal for clean, Gaussian data, real-world collision events are often contaminated by tracks from other sources, such as displaced secondary vertices. This exercise explores the critical vulnerability of the standard estimator to such contamination, guiding you to derive and compute the bias introduced by a population of outlier tracks . Understanding this failure mode is essential for appreciating the need for the robust algorithms used in modern experiments.",
            "id": "3528935",
            "problem": "You are tasked with developing a mathematically principled and computationally robust maximum-likelihood primary vertex fitter for charged-particle tracks in high-energy physics, integrating a Newton method update and an analysis of estimator bias in the presence of displaced secondary decays. Each track provides a three-dimensional measurement of its point-of-closest-approach to a common vertex. For track $i$, its measured position is the vector $\\vec{r}_i \\in \\mathbb{R}^3$ (in $\\mathrm{mm}$) with a symmetric positive-definite covariance matrix $C_i \\in \\mathbb{R}^{3 \\times 3}$ (in $\\mathrm{mm}^2$). Assume independent Gaussian measurement errors for all tracks. The primary vertex position is $\\vec{v} \\in \\mathbb{R}^3$ (in $\\mathrm{mm}$). The goal is to estimate $\\vec{v}$ by minimizing the quadratic objective\n$$\n\\chi^2(\\vec{v}) = \\sum_{i=1}^N (\\vec{r}_i - \\vec{v})^\\top C_i^{-1} (\\vec{r}_i - \\vec{v}),\n$$\nwhich is the negative log-likelihood (up to an additive constant), consistent with independent Gaussian uncertainties.\n\nStarting from the definitions of the Gaussian likelihood, the independence of measurements, and the properties of symmetric positive-definite covariance matrices, derive from first principles the stationary condition for the estimator $\\hat{\\vec{v}}$ that minimizes $\\chi^2(\\vec{v})$, and derive the Newton method update using the gradient and Hessian of $\\chi^2(\\vec{v})$. Explain why for a quadratic objective the Newton method converges in one step to the unique minimizer. Implement the Newton update to compute $\\hat{\\vec{v}}$ for the provided test suites.\n\nThen, analyze the estimator bias when a fraction $\\epsilon$ of tracks originate from a displaced secondary vertex at $\\vec{v}_0 + \\vec{d}$, while the remaining fraction $1 - \\epsilon$ originate from the true primary vertex at $\\vec{v}_0$. Assume that the displaced tracks have measurements centered at $\\vec{v}_0 + \\vec{d}$ and the primary tracks at $\\vec{v}_0$, with covariances $C_i$ as specified. Derive the expected estimator bias vector $\\mathbb{E}[\\hat{\\vec{v}} - \\vec{v}_0]$ in terms of the inverse covariances $W_i = C_i^{-1}$, the displacement $\\vec{d}$, and the contamination set $S$ of displaced tracks. Explain under what conditions the bias reduces to $\\epsilon \\vec{d}$, and how covariance heterogeneity changes the bias. For each test case below, compute the fitted $\\hat{\\vec{v}}$ via the Newton update, and report the bias magnitude $\\|\\hat{\\vec{v}} - \\vec{v}_0\\|_2$ in $\\mathrm{mm}$.\n\nUse strictly the following test suite, where all vectors are in $\\mathrm{mm}$ and all covariance matrices are in $\\mathrm{mm}^2$. For every test case, the program must construct the observations as follows: for tracks in the primary set $P$, set $\\vec{r}_i = \\vec{v}_0$; for tracks in the displaced secondary set $S$, set $\\vec{r}_i = \\vec{v}_0 + \\vec{d}$. No random sampling is permitted; all inputs are deterministic. In all cases, the Newton method must be applied in three dimensions with full $3 \\times 3$ matrices.\n\nTest Case $1$ (isotropic, equal weights, partial contamination):\n- $N = 12$\n- $\\vec{v}_0 = (0.0, 0.0, 0.0)$\n- $\\vec{d} = (0.4, -0.1, 0.2)$\n- $\\epsilon = 0.25$, with $S = \\{0, 1, 2\\}$ and $P = \\{3, 4, 5, 6, 7, 8, 9, 10, 11\\}$\n- $C_i = \\operatorname{diag}(0.05^2, 0.05^2, 0.05^2)$ for all $i$\n\nTest Case $2$ (isotropic, equal weights, no contamination):\n- $N = 8$\n- $\\vec{v}_0 = (1.0, -1.0, 0.5)$\n- $\\vec{d} = (0.3, 0.3, -0.1)$\n- $\\epsilon = 0.0$, with $S = \\emptyset$ and $P = \\{0, 1, 2, 3, 4, 5, 6, 7\\}$\n- $C_i = \\operatorname{diag}(0.02^2, 0.02^2, 0.02^2)$ for all $i$\n\nTest Case $3$ (isotropic, equal weights, full contamination):\n- $N = 5$\n- $\\vec{v}_0 = (0.0, 0.0, 0.0)$\n- $\\vec{d} = (-0.2, 0.5, 0.1)$\n- $\\epsilon = 1.0$, with $S = \\{0, 1, 2, 3, 4\\}$ and $P = \\emptyset$\n- $C_i = \\operatorname{diag}(0.02^2, 0.02^2, 0.02^2)$ for all $i$\n\nTest Case $4$ (anisotropic, heterogeneous weights, contamination correlated with larger covariance):\n- $N = 10$\n- $\\vec{v}_0 = (0.2, -0.3, 0.1)$\n- $\\vec{d} = (0.8, 0.0, -0.2)$\n- $\\epsilon = 0.4$, with $S = \\{6, 7, 8, 9\\}$ and $P = \\{0, 1, 2, 3, 4, 5\\}$\n- For $i \\in P$: $C_i = \\operatorname{diag}(0.01^2, 0.01^2, 0.01^2)$\n- For $i \\in S$: $C_i = \\operatorname{diag}(0.10^2, 0.05^2, 0.02^2)$\n\nTest Case $5$ (anisotropic, direction-dependent weights, contamination among tracks precise along $\\vec{d}$):\n- $N = 9$\n- $\\vec{v}_0 = (0.0, 0.0, 0.0)$\n- $\\vec{d} = (0.05, 0.60, 0.0)$\n- $\\epsilon = \\tfrac{1}{3}$, with $S = \\{0, 1, 2\\}$ and $P = \\{3, 4, 5, 6, 7, 8\\}$\n- For $i \\in \\{0, 1, 2\\}$: $C_i = \\operatorname{diag}(0.2^2, 0.02^2, 0.02^2)$\n- For $i \\in \\{3, 4, 5\\}$: $C_i = \\operatorname{diag}(0.02^2, 0.2^2, 0.02^2)$\n- For $i \\in \\{6, 7, 8\\}$: $C_i = \\operatorname{diag}(0.02^2, 0.02^2, 0.2^2)$\n\nYour program must:\n- For each test case, construct the set of measurements $\\{\\vec{r}_i\\}_{i=1}^N$ and covariances $\\{C_i\\}_{i=1}^N$ as specified.\n- Compute the Newton update for $\\chi^2(\\vec{v})$ starting from any initial guess $\\vec{v}^{(0)}$ and obtain the fitted vertex $\\hat{\\vec{v}}$.\n- Compute the bias magnitude $\\|\\hat{\\vec{v}} - \\vec{v}_0\\|_2$ in $\\mathrm{mm}$ for each test case.\n\nFinal Output Format:\nYour program should produce a single line of output containing the bias magnitudes for the five test cases as a comma-separated list enclosed in square brackets, in $\\mathrm{mm}$, each rounded to six decimal places (for example, $[\\text{bias}_1,\\text{bias}_2,\\text{bias}_3,\\text{bias}_4,\\text{bias}_5]$).",
            "solution": "The problem is valid as it is scientifically grounded in statistical estimation theory, well-posed with a unique solution, and described with objective, precise mathematical formalism. It represents a standard weighted least-squares problem, which is equivalent to maximum likelihood estimation for independent Gaussian errors, a fundamental task in experimental data analysis. All parameters and conditions are clearly specified.\n\nThe problem asks for the derivation and implementation of a maximum-likelihood primary vertex fitter, including an analysis of estimator bias. We will proceed by first deriving the estimator and the Newton-Raphson optimization step, and then analyzing the bias under conditions of track sample contamination.\n\n**Part 1: Derivation of the Vertex Estimator**\n\nThe foundation of the estimator is the principle of maximum likelihood. For a set of $N$ independent track measurements $\\{\\vec{r}_i\\}_{i=1}^N$, the probability of observing these measurements, given a true vertex position $\\vec{v}$, is described by the likelihood function $L(\\vec{v})$. Since the measurement errors are assumed to be Gaussian and independent, the likelihood is the product of individual Gaussian probability density functions:\n$$\nL(\\vec{v}; \\{\\vec{r}_i\\}) = \\prod_{i=1}^N \\frac{1}{\\sqrt{(2\\pi)^3 \\det(C_i)}} \\exp\\left(-\\frac{1}{2} (\\vec{r}_i - \\vec{v})^\\top C_i^{-1} (\\vec{r}_i - \\vec{v})\\right)\n$$\nwhere $\\vec{r}_i \\in \\mathbb{R}^3$ is the measured point of closest approach for track $i$, and $C_i \\in \\mathbb{R}^{3 \\times 3}$ is its corresponding covariance matrix.\n\nMaximizing the likelihood function $L(\\vec{v})$ is equivalent to maximizing its logarithm, the log-likelihood $\\ln L(\\vec{v})$, which is computationally more tractable:\n$$\n\\ln L(\\vec{v}) = \\sum_{i=1}^N \\left( -\\frac{3}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\det(C_i)) - \\frac{1}{2} (\\vec{r}_i - \\vec{v})^\\top C_i^{-1} (\\vec{r}_i - \\vec{v}) \\right)\n$$\nTo find the vertex position $\\hat{\\vec{v}}$ that maximizes $\\ln L(\\vec{v})$, we can equivalently minimize the negative log-likelihood. Dropping terms that are constant with respect to $\\vec{v}$, this is equivalent to minimizing the objective function $\\chi^2(\\vec{v})$:\n$$\n\\chi^2(\\vec{v}) = \\sum_{i=1}^N (\\vec{r}_i - \\vec{v})^\\top C_i^{-1} (\\vec{r}_i - \\vec{v})\n$$\nThis is a weighted least-squares problem, where the weight matrix for each measurement is the inverse of its covariance matrix, $W_i = C_i^{-1}$. Given that each $C_i$ is symmetric and positive-definite, so is each $W_i$.\n\n**Part 2: Stationary Condition and Newton's Method**\n\nTo find the minimum of $\\chi^2(\\vec{v})$, we find the stationary point where the gradient with respect to $\\vec{v}$ is zero. The gradient of a single term in the sum is:\n$$\n\\nabla_{\\vec{v}} \\left( (\\vec{r}_i - \\vec{v})^\\top W_i (\\vec{r}_i - \\vec{v}) \\right) = -2 W_i (\\vec{r}_i - \\vec{v})\n$$\nThe gradient of the full $\\chi^2(\\vec{v})$ function, denoted as $\\vec{g}(\\vec{v})$, is the sum of these individual gradients:\n$$\n\\vec{g}(\\vec{v}) = \\nabla_{\\vec{v}} \\chi^2(\\vec{v}) = \\sum_{i=1}^N -2 W_i (\\vec{r}_i - \\vec{v}) = 2 \\left( \\sum_{i=1}^N W_i \\right) \\vec{v} - 2 \\left( \\sum_{i=1}^N W_i \\vec{r}_i \\right)\n$$\nSetting the gradient to zero, $\\vec{g}(\\hat{\\vec{v}}) = \\vec{0}$, yields the stationary condition:\n$$\n\\left( \\sum_{i=1}^N W_i \\right) \\hat{\\vec{v}} = \\sum_{i=1}^N W_i \\vec{r}_i\n$$\nLet $W = \\sum_{i=1}^N W_i$ be the total weight matrix. The solution for the estimated vertex $\\hat{\\vec{v}}$ is:\n$$\n\\hat{\\vec{v}} = \\left( \\sum_{i=1}^N W_i \\right)^{-1} \\left( \\sum_{i=1}^N W_i \\vec{r}_i \\right) = W^{-1} \\sum_{i=1}^N W_i \\vec{r}_i\n$$\nThis solution is a generalized weighted average of the measurement vectors $\\vec{r}_i$, with matrix weights $W_i$.\n\nThe Newton-Raphson method provides an iterative scheme to find the minimum of a function: $\\vec{v}^{(k+1)} = \\vec{v}^{(k)} - [H(\\vec{v}^{(k)})]^{-1} \\vec{g}(\\vec{v}^{(k)})$, where $H$ is the Hessian matrix (the matrix of second partial derivatives).\nThe Hessian of $\\chi^2(\\vec{v})$ is:\n$$\nH(\\vec{v}) = \\nabla_{\\vec{v}} (\\vec{g}(\\vec{v})^\\top) = \\nabla_{\\vec{v}} \\left( 2 \\vec{v}^\\top \\sum_{i=1}^N W_i - 2 \\sum_{i=1}^N \\vec{r}_i^\\top W_i \\right) = 2 \\sum_{i=1}^N W_i = 2W\n$$\nThe Hessian is a constant matrix, independent of $\\vec{v}$. This is a characteristic feature of quadratic objective functions.\nSubstituting the gradient and Hessian into the Newton's method update rule from an arbitrary starting point $\\vec{v}^{(0)}$:\n$$\n\\vec{v}^{(1)} = \\vec{v}^{(0)} - (2W)^{-1} \\left( 2W\\vec{v}^{(0)} - 2 \\sum_{i=1}^N W_i \\vec{r}_i \\right)\n$$\n$$\n\\vec{v}^{(1)} = \\vec{v}^{(0)} - (2W)^{-1}(2W) \\vec{v}^{(0)} + (2W)^{-1}(2) \\sum_{i=1}^N W_i \\vec{r}_i\n$$\n$$\n\\vec{v}^{(1)} = \\vec{v}^{(0)} - \\vec{v}^{(0)} + W^{-1} \\sum_{i=1}^N W_i \\vec{r}_i = W^{-1} \\sum_{i=1}^N W_i \\vec{r}_i\n$$\nThe result of the first step, $\\vec{v}^{(1)}$, is precisely the analytical solution $\\hat{\\vec{v}}$. The update is independent of the starting point $\\vec{v}^{(0)}$. This demonstrates that for a quadratic objective function, Newton's method converges to the unique minimizer in a single step.\n\n**Part 3: Estimator Bias Analysis**\n\nWe now analyze the bias of the estimator $\\hat{\\vec{v}}$ when the track sample is contaminated. A fraction $1-\\epsilon$ of tracks originate from the true primary vertex at $\\vec{v}_0$, and a fraction $\\epsilon$ originate from a displaced secondary vertex at $\\vec{v}_0 + \\vec{d}$. Let $P$ be the set of primary tracks and $S$ be the set of secondary tracks. The problem specifies deterministic measurements for this analysis: $\\vec{r}_i = \\vec{v}_0$ for $i \\in P$ and $\\vec{r}_i = \\vec{v}_0 + \\vec{d}$ for $i \\in S$. These can be interpreted as the expectation values $\\mathbb{E}[\\vec{r}_i]$.\n\nThe expected value of the estimator is:\n$$\n\\mathbb{E}[\\hat{\\vec{v}}] = \\mathbb{E}\\left[ W^{-1} \\sum_{i=1}^N W_i \\vec{r}_i \\right] = W^{-1} \\sum_{i=1}^N W_i \\mathbb{E}[\\vec{r}_i]\n$$\nSplitting the sum over the primary and secondary sets:\n$$\n\\mathbb{E}[\\hat{\\vec{v}}] = W^{-1} \\left( \\sum_{i \\in P} W_i \\mathbb{E}[\\vec{r}_i] + \\sum_{i \\in S} W_i \\mathbb{E}[\\vec{r}_i] \\right) = W^{-1} \\left( \\sum_{i \\in P} W_i \\vec{v}_0 + \\sum_{i \\in S} W_i (\\vec{v}_0 + \\vec{d}) \\right)\n$$\nLet $W_P = \\sum_{i \\in P} W_i$ and $W_S = \\sum_{i \\in S} W_i$. Then $W = W_P + W_S$.\n$$\n\\mathbb{E}[\\hat{\\vec{v}}] = W^{-1} \\left( (W_P + W_S) \\vec{v}_0 + W_S \\vec{d} \\right) = W^{-1} (W \\vec{v}_0 + W_S \\vec{d}) = \\vec{v}_0 + W^{-1} W_S \\vec{d}\n$$\nThe estimator bias vector is the difference between the estimator's expectation and the true value $\\vec{v}_0$:\n$$\n\\text{Bias} = \\mathbb{E}[\\hat{\\vec{v}}] - \\vec{v}_0 = W^{-1} W_S \\vec{d}\n$$\nThe bias reduces to the simple form $\\epsilon \\vec{d}$ under a specific condition. If all tracks have identical covariance matrices, $C_i = C_0$ for all $i=1, \\dots, N$, then their weights are also identical, $W_i = W_0$. In this case, $W_S = |S| W_0$ and $W = N W_0$. The bias becomes:\n$$\n\\text{Bias} = (N W_0)^{-1} (|S| W_0) \\vec{d} = \\frac{|S|}{N} (W_0^{-1} W_0) \\vec{d} = \\frac{|S|}{N} \\vec{d} = \\epsilon \\vec{d}\n$$\nThis condition of homogeneous, isotropic covariances is met in Test Cases 1, 2, and 3.\n\nWhen covariance matrices are heterogeneous, the bias is modified. The matrix $W^{-1} W_S$ acts as a linear transformation on the displacement vector $\\vec{d}$. The bias is no longer necessarily parallel to $\\vec{d}$ or scaled by $\\epsilon$. Tracks with smaller uncertainty (larger $W_i$) contribute more to the total weight and pull the fitted vertex more strongly. If contaminated tracks ($i \\in S$) are less precise (have smaller weights) than primary tracks, the factor $W^{-1}W_S$ will down-weight the displacement $\\vec{d}$, resulting in a smaller bias. Conversely, if contaminated tracks are more precise than primary tracks, they will exert a stronger pull, leading to a larger bias. This effect is explored in Test Cases 4 and 5.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the primary vertex fit and bias for several test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"name\": \"Case 1: Isotropic, equal weights, partial contamination\",\n            \"N\": 12,\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([0.4, -0.1, 0.2]),\n            \"S_indices\": {0, 1, 2},\n            \"cov_map\": {\n                \"all\": np.diag([0.05**2, 0.05**2, 0.05**2])\n            },\n            \"track_cov_map_indices\": {\"all_tracks\": list(range(12))}\n        },\n        {\n            \"name\": \"Case 2: Isotropic, equal weights, no contamination\",\n            \"N\": 8,\n            \"v0\": np.array([1.0, -1.0, 0.5]),\n            \"d\": np.array([0.3, 0.3, -0.1]),\n            \"S_indices\": set(),\n            \"cov_map\": {\n                \"all\": np.diag([0.02**2, 0.02**2, 0.02**2])\n            },\n            \"track_cov_map_indices\": {\"all_tracks\": list(range(8))}\n        },\n        {\n            \"name\": \"Case 3: Isotropic, equal weights, full contamination\",\n            \"N\": 5,\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([-0.2, 0.5, 0.1]),\n            \"S_indices\": {0, 1, 2, 3, 4},\n            \"cov_map\": {\n                \"all\": np.diag([0.02**2, 0.02**2, 0.02**2])\n            },\n            \"track_cov_map_indices\": {\"all_tracks\": list(range(5))}\n        },\n        {\n            \"name\": \"Case 4: Anisotropic, heterogeneous weights\",\n            \"N\": 10,\n            \"v0\": np.array([0.2, -0.3, 0.1]),\n            \"d\": np.array([0.8, 0.0, -0.2]),\n            \"S_indices\": {6, 7, 8, 9},\n            \"cov_map\": {\n                \"primary\": np.diag([0.01**2, 0.01**2, 0.01**2]),\n                \"secondary\": np.diag([0.10**2, 0.05**2, 0.02**2])\n            },\n            \"track_cov_map_indices\": {\n                \"primary\": [0, 1, 2, 3, 4, 5],\n                \"secondary\": [6, 7, 8, 9]\n            }\n        },\n        {\n            \"name\": \"Case 5: Anisotropic, direction-dependent weights\",\n            \"N\": 9,\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([0.05, 0.60, 0.0]),\n            \"S_indices\": {0, 1, 2},\n            \"cov_map\": {\n                \"s_group\": np.diag([0.2**2, 0.02**2, 0.02**2]),\n                \"p1_group\": np.diag([0.02**2, 0.2**2, 0.02**2]),\n                \"p2_group\": np.diag([0.02**2, 0.02**2, 0.2**2])\n            },\n            \"track_cov_map_indices\": {\n                \"s_group\": [0, 1, 2],\n                \"p1_group\": [3, 4, 5],\n                \"p2_group\": [6, 7, 8]\n            }\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N = case[\"N\"]\n        v0 = case[\"v0\"]\n        d = case[\"d\"]\n        S_indices = case[\"S_indices\"]\n        cov_map = case[\"cov_map\"]\n        track_cov_map_indices = case[\"track_cov_map_indices\"]\n        \n        # Initialize lists for measurements and covariances\n        r_list = [np.zeros(3) for _ in range(N)]\n        C_list = [np.zeros((3, 3)) for _ in range(N)]\n\n        # Construct measurements r_i and covariances C_i\n        for i in range(N):\n            if i in S_indices:\n                r_list[i] = v0 + d\n            else:\n                r_list[i] = v0\n        \n        if \"all_tracks\" in track_cov_map_indices:\n            C_val = cov_map[\"all\"]\n            for i in range(N):\n                C_list[i] = C_val\n        else:\n            if \"primary\" in track_cov_map_indices:\n                primary_indices = set(track_cov_map_indices[\"primary\"])\n                c_primary = cov_map[\"primary\"]\n                c_secondary = cov_map[\"secondary\"]\n                for i in range(N):\n                    if i in primary_indices:\n                         C_list[i] = c_primary\n                    else:\n                         C_list[i] = c_secondary\n            else: # Case 5 special structure\n                for group_name, indices in track_cov_map_indices.items():\n                    c_val = cov_map[group_name]\n                    for idx in indices:\n                        C_list[idx] = c_val\n\n        # Calculate weight matrices W_i = C_i^{-1}\n        W_list = [np.linalg.inv(C) for C in C_list]\n\n        # Calculate total weight matrix W and weighted sum of measurements\n        W_total = np.sum(W_list, axis=0)\n        sum_Wr = np.zeros(3)\n        for i in range(N):\n            sum_Wr += W_list[i] @ r_list[i]\n            \n        # Compute fitted vertex v_hat using the one-step Newton update\n        # which is equivalent to solving the linear system W * v_hat = sum(W_i * r_i)\n        v_hat = np.linalg.solve(W_total, sum_Wr)\n\n        # Compute bias magnitude ||v_hat - v0||\n        bias_magnitude = np.linalg.norm(v_hat - v0)\n        results.append(bias_magnitude)\n\n    # Print final results in the required format\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having established the shortcomings of standard least-squares fitting in the presence of outliers, we now turn to the solution: robust estimation. This comprehensive practice introduces M-estimators, such as the Huber and Student-$t$ losses, which are designed to automatically down-weight the influence of contaminating tracks . You will derive the associated weight functions, implement a full Iteratively Reweighted Least Squares (IRLS) algorithm, and empirically verify its superior performance by studying its breakdown point in a simulated high-contamination environment.",
            "id": "3528981",
            "problem": "You are tasked with designing, deriving, and evaluating robust estimators for the three-dimensional primary vertex position in a collider event, where each reconstructed track provides a point-of-closest-approach position and an associated covariance. The vertex position is denoted by the three-dimensional vector $\\vec{v} \\in \\mathbb{R}^{3}$, and the $i$-th track measurement is $\\vec{r}_{i} \\in \\mathbb{R}^{3}$ with a symmetric positive-definite covariance matrix $C_{i} \\in \\mathbb{R}^{3 \\times 3}$. The Mahalanobis residual norm is defined by\n$$\n\\rho_{i}(\\vec{v}) = \\sqrt{(\\vec{r}_{i} - \\vec{v})^{\\top} C_{i}^{-1} (\\vec{r}_{i} - \\vec{v})}.\n$$\nThe robust fitting paradigm is to estimate $\\vec{v}$ by minimizing a sum of robust losses,\n$$\n\\min_{\\vec{v} \\in \\mathbb{R}^{3}} \\sum_{i=1}^{N} \\phi(\\rho_{i}(\\vec{v})),\n$$\nwhere $\\phi(\\cdot)$ is a nonnegative, differentiable loss function. Start from the following fundamental base:\n- Maximum likelihood under Gaussian errors yields least squares, i.e., $\\phi(\\rho) = \\tfrac{1}{2}\\rho^{2}$.\n- For general robust losses, the first-order optimality condition leads to an iteratively reweighted least squares scheme, based on the score function $\\psi(\\rho) = \\tfrac{d\\phi(\\rho)}{d\\rho}$ and the weight function $w(\\rho) = \\psi(\\rho)/\\rho$, with the convention that $w(0)$ is the limit $\\lim_{\\rho \\to 0^{+}} \\psi(\\rho)/\\rho$ when needed.\n- Given weights $w_{i} \\equiv w(\\rho_{i})$, one iteration of the reweighted normal equations for $\\vec{v}$ has the form\n$$\n\\left(\\sum_{i=1}^{N} w_{i} C_{i}^{-1}\\right)\\vec{v} = \\sum_{i=1}^{N} w_{i} C_{i}^{-1} \\vec{r}_{i}.\n$$\nYour tasks are:\n1) Using the definitions above, derive the weight functions $w_{i}(\\rho_{i})$ for the following two robust losses, expressed in terms of the residual norm $\\rho$:\n- Huber loss with cutoff parameter $k>0$, defined by a piecewise function $\\phi(\\rho)$ that is quadratic for small $\\rho$ and linear for large $\\rho$.\n- Multivariate Student-$t$ negative log-likelihood for a $d$-dimensional observation with $\\nu>0$ degrees of freedom, which yields a loss function of the form $\\phi(\\rho)$ that depends on $d$, $\\nu$, and $\\rho$ via a logarithmic term involving $1+\\rho^{2}/\\nu$.\n2) Implement an iteratively reweighted least squares algorithm to estimate the primary vertex $\\vec{v}$ given $\\{\\vec{r}_{i}, C_{i}\\}_{i=1}^{N}$ for each of the two robust losses. Use the dimension $d=3$. Ensure numerical stability by using a tolerance for convergence and a maximum number of iterations. All coordinates are in millimeters (mm); the output quantities in this part that you will be asked to report are dimensionless, but the algorithm must internally handle positions in millimeters.\n3) Study the empirical breakdown behavior under contamination by simulating data with an increasing fraction of outliers. Use the following physically and numerically sound scenario:\n- True vertex $\\vec{v}_{\\text{true}} = (0,0,0)$ mm.\n- Number of tracks $N=100$.\n- Inliers: $N_{\\text{in}} = \\lfloor (1-p) N \\rfloor$ tracks sampled independently from a normal distribution centered at $\\vec{v}_{\\text{true}}$ with isotropic covariance $\\sigma_{\\text{in}}^{2} I_{3}$, where $\\sigma_{\\text{in}} = 0.03$ mm. Each inlier has covariance $C_{i} = \\sigma_{\\text{in}}^{2} I_{3}$.\n- Outliers: $N_{\\text{out}} = N - N_{\\text{in}}$ tracks sampled independently from a normal distribution with mean $\\vec{\\mu}_{\\text{out}} = (10,-10,5)$ mm and isotropic covariance $\\sigma_{\\text{out}}^{2} I_{3}$, where $\\sigma_{\\text{out}} = 2.0$ mm. For simplicity, use the same per-track covariance $C_{i} = \\sigma_{\\text{in}}^{2} I_{3}$ also for outliers, representing underestimation of uncertainties for bad tracks.\n- Consider outlier fractions $p \\in \\{0.0, 0.2, 0.4, 0.6\\}$.\n- Define failure (breakdown) at a given $p$ if the estimated vertex $\\hat{\\vec{v}}$ satisfies $\\|\\hat{\\vec{v}} - \\vec{v}_{\\text{true}}\\|_{2} > 1.0$ mm.\n- For each robust loss, report the smallest $p$ in the given set at which failure occurs. If no failure occurs for any $p$ in the set, report $1.0$.\nFor numerical reproducibility, use a fixed random seed for the simulation. Angles are not involved; no angle unit is required. All distances are in millimeters; the outputs requested below are dimensionless.\nTest suite and required outputs:\n- Weight-function evaluation tests at residual norms $\\rho \\in \\{0.5, 1.5, 3.0\\}$ with the following parameters:\n  - Huber: $k = 1.5$.\n  - Student-$t$: $\\nu = 4$, $d = 3$.\n- Breakdown study with $p \\in \\{0.0, 0.2, 0.4, 0.6\\}$ and the simulation design described above.\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n$[w_{H}(0.5), w_{H}(1.5), w_{H}(3.0), w_{T}(0.5), w_{T}(1.5), w_{T}(3.0), p_{\\text{break,Huber}}, p_{\\text{break,Student}}]$,\nwhere $w_{H}(\\cdot)$ and $w_{T}(\\cdot)$ are the derived weight functions for Huber and Student-$t$, respectively, and $p_{\\text{break,Huber}}$ and $p_{\\text{break,Student}}$ are the smallest failure fractions in the test set as defined above. The eight outputs are dimensionless real numbers. The single line must contain exactly this list format with no additional text.",
            "solution": "The problem of primary vertex reconstruction is a fundamental task in experimental particle physics. It can be framed as an M-estimation problem in statistics, where the goal is to find a vertex position $\\vec{v} \\in \\mathbb{R}^{3}$ that minimizes a sum of loss functions applied to the residuals of measured tracks. The use of robust loss functions is critical for down-weighting the influence of outlier tracks, which do not originate from the primary interaction. This solution proceeds by first deriving the weight functions for the specified robust losses, then describing the iteratively reweighted least squares (IRLS) algorithm for finding the vertex, and finally detailing the simulation designed to test the breakdown point of the estimators.\n\n**1. Derivation of Weight Functions**\n\nThe IRLS algorithm relies on a weight function, $w(\\rho)$, which is derived from the chosen loss function, $\\phi(\\rho)$. The weight function is defined via the score function $\\psi(\\rho) = \\frac{d\\phi}{d\\rho}$ as $w(\\rho) = \\psi(\\rho)/\\rho$.\n\n**Huber Loss**\nThe Huber loss function with a cutoff parameter $k > 0$ is a hybrid of quadratic loss for small residuals and linear loss for large residuals. As the Mahalanobis norm $\\rho$ is always non-negative, the loss function is:\n$$\n\\phi_H(\\rho) = \\begin{cases} \\frac{1}{2}\\rho^2 & \\text{if } 0 \\le \\rho \\le k \\\\ k\\rho - \\frac{1}{2}k^2 & \\text{if } \\rho > k \\end{cases}\n$$\nThe corresponding score function, $\\psi_H(\\rho) = \\frac{d\\phi_H}{d\\rho}$, is obtained by differentiating with respect to $\\rho$:\n$$\n\\psi_H(\\rho) = \\begin{cases} \\rho & \\text{if } 0 \\le \\rho \\le k \\\\ k & \\text{if } \\rho > k \\end{cases}\n$$\nThis can be written more compactly as $\\psi_H(\\rho) = \\min(\\rho, k)$. The weight function, $w_H(\\rho) = \\psi_H(\\rho)/\\rho$, is therefore:\n$$\nw_H(\\rho) = \\begin{cases} 1 & \\text{if } 0 \\le \\rho \\le k \\\\ k/\\rho & \\text{if } \\rho > k \\end{cases}\n$$\nThis can be expressed as $w_H(\\rho) = \\min(1, k/\\rho)$. This function assigns a constant weight of $1$ (equivalent to standard least squares) to residuals within the cutoff $k$ and down-weights residuals that are larger than $k$.\n\n**Student-t Loss**\nThe loss function is derived from the negative log-likelihood of a $d$-dimensional multivariate Student-t distribution with $\\nu$ degrees of freedom. The probability density function is proportional to $(1 + \\frac{1}{\\nu} \\rho^2)^{-(\\nu+d)/2}$, where $\\rho^2 = (\\vec{r} - \\vec{v})^\\top C^{-1} (\\vec{r} - \\vec{v})$ is the squared Mahalanobis distance. The negative log-likelihood, ignoring additive constants, gives the loss function:\n$$\n\\phi_T(\\rho) = \\frac{\\nu+d}{2} \\log\\left(1 + \\frac{\\rho^2}{\\nu}\\right)\n$$\nThe score function, $\\psi_T(\\rho) = \\frac{d\\phi_T}{d\\rho}$, is found using the chain rule:\n$$\n\\psi_T(\\rho) = \\frac{\\nu+d}{2} \\cdot \\frac{1}{1 + \\rho^2/\\nu} \\cdot \\frac{d}{d\\rho}\\left(\\frac{\\rho^2}{\\nu}\\right) = \\frac{\\nu+d}{2} \\cdot \\frac{\\nu}{\\nu + \\rho^2} \\cdot \\frac{2\\rho}{\\nu} = \\frac{(\\nu+d)\\rho}{\\nu + \\rho^2}\n$$\nThe corresponding weight function, $w_T(\\rho) = \\psi_T(\\rho)/\\rho$, is:\n$$\nw_T(\\rho) = \\frac{\\nu+d}{\\nu + \\rho^2}\n$$\nThis weight function smoothly decreases as the residual norm $\\rho$ increases, providing a soft down-weighting of outliers. Unlike Huber loss, it does not have a sharp cutoff.\n\n**2. Iteratively Reweighted Least Squares (IRLS) Algorithm**\n\nThe minimization of $\\sum_i \\phi(\\rho_i(\\vec{v}))$ is a non-linear optimization problem. The IRLS algorithm is an iterative procedure to find the solution. Each iteration involves solving a weighted least-squares problem, where the weights are updated based on the residuals from the previous iteration. The general update step is governed by the normal equations:\n$$\n\\vec{v}^{(t+1)} = \\left(\\sum_{i=1}^{N} w_{i}^{(t)} C_{i}^{-1}\\right)^{-1} \\left(\\sum_{i=1}^{N} w_{i}^{(t)} C_{i}^{-1} \\vec{r}_{i}\\right)\n$$\nwhere $w_i^{(t)} = w(\\rho_i(\\vec{v}^{(t)}))$ are the weights computed using the vertex estimate $\\vec{v}^{(t)}$ from the previous iteration.\n\nA significant simplification arises from the problem's simulation design, where all covariance matrices are identical and isotropic: $C_i = \\sigma_{\\text{in}}^2 I_3$ for all $i$. Consequently, the inverse is $C_i^{-1} = (1/\\sigma_{\\text{in}}^2) I_3$. Substituting this into the normal equations:\n$$\n\\left(\\sum_{i=1}^{N} w_{i}^{(t)} \\frac{1}{\\sigma_{\\text{in}}^2} I_3\\right)\\vec{v}^{(t+1)} = \\sum_{i=1}^{N} w_{i}^{(t)} \\frac{1}{\\sigma_{\\text{in}}^2} I_3 \\vec{r}_{i}\n$$\nThe constant matrix term $(1/\\sigma_{\\text{in}}^2) I_3$ can be factored out and cancelled from both sides, yielding:\n$$\n\\left(\\sum_{i=1}^{N} w_{i}^{(t)}\\right) \\vec{v}^{(t+1)} = \\sum_{i=1}^{N} w_{i}^{(t)} \\vec{r}_{i}\n$$\nThis shows that the updated vertex estimate is simply the weighted average of the track positions:\n$$\n\\vec{v}^{(t+1)} = \\frac{\\sum_{i=1}^{N} w_i^{(t)} \\vec{r}_i}{\\sum_{i=1}^{N} w_i^{(t)}}\n$$\nThe Mahalanobis norm also simplifies:\n$$\n\\rho_i(\\vec{v}) = \\sqrt{(\\vec{r}_i - \\vec{v})^\\top \\left(\\frac{1}{\\sigma_{\\text{in}}^2}I_3\\right) (\\vec{r}_i - \\vec{v})} = \\frac{1}{\\sigma_{\\text{in}}} \\sqrt{(\\vec{r}_i - \\vec{v})^\\top (\\vec{r}_i - \\vec{v})} = \\frac{\\|\\vec{r}_i - \\vec{v}\\|_2}{\\sigma_{\\text{in}}}\n$$\nThe algorithm proceeds as follows:\n1.  Initialize $\\vec{v}^{(0)}$ as the unweighted mean of all $\\vec{r}_i$.\n2.  For $t = 0, 1, 2, \\dots$ up to a maximum number of iterations:\n    a.  Compute norms $\\rho_i^{(t)} = \\|\\vec{r}_i - \\vec{v}^{(t)}\\|_2 / \\sigma_{\\text{in}}$.\n    b.  Compute weights $w_i^{(t)} = w(\\rho_i^{(t)})$ using the appropriate derived function (Huber or Student-t).\n    c.  Update the vertex estimate $\\vec{v}^{(t+1)}$ using the weighted average formula.\n    d.  Check for convergence: if $\\|\\vec{v}^{(t+1)} - \\vec{v}^{(t)}\\|_2$ is less than a specified tolerance, terminate.\n3.  The final $\\vec{v}^{(t+1)}$ is the estimated vertex position.\n\n**3. Empirical Breakdown Study**\n\nThe breakdown point of an estimator is, informally, the largest fraction of contaminated data it can tolerate before producing an arbitrarily wrong estimate. The study analyzes this behavior empirically.\nFor each outlier fraction $p \\in \\{0.0, 0.2, 0.4, 0.6\\}$:\n- A dataset of $N=100$ tracks is simulated.\n- $N_{\\text{in}} = \\lfloor (1-p)N \\rfloor$ inlier positions are drawn from a narrow Gaussian centered at the true vertex $\\vec{v}_{\\text{true}} = (0,0,0)$ mm, with $\\sigma_{\\text{in}} = 0.03$ mm.\n- $N_{\\text{out}} = N - N_{\\text{in}}$ outlier positions are drawn from a different Gaussian centered at $\\vec{\\mu}_{\\text{out}} = (10,-10,5)$ mm with a large spread, $\\sigma_{\\text{out}} = 2.0$ mm.\n- The IRLS algorithm is run on this combined dataset for both Huber and Student-t losses to obtain estimates $\\hat{\\vec{v}}$. The covariance for all tracks is assumed to be $C_i = \\sigma_{\\text{in}}^2 I_3$.\n- A \"breakdown\" or \"failure\" is declared if the distance of the estimate from the true vertex exceeds a threshold: $\\|\\hat{\\vec{v}} - \\vec{v}_{\\text{true}}\\|_2 > 1.0$ mm.\n- The reported quantity for each loss function is the smallest value of $p$ in the test set that causes a failure. If no failure occurs across the tested fractions, a value of $1.0$ is reported, indicating robustness up to at least $p=0.6$. The fixed random seed ensures reproducibility of the simulation.\n\nThis concludes the theoretical and methodological framework for solving the problem. The implementation will follow these principles directly.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the robust vertex reconstruction problem by:\n    1. Deriving and evaluating weight functions for Huber and Student-t losses.\n    2. Implementing an Iteratively Reweighted Least Squares (IRLS) algorithm.\n    3. Performing a breakdown study to find the failure point for each estimator.\n    \"\"\"\n\n    # --- Part 1: Weight Function Definitions and Evaluation ---\n\n    def w_huber(rho, k):\n        \"\"\"Calculates the Huber weight function.\"\"\"\n        rho = np.asarray(rho)\n        # The weight is min(1, k/|rho|). Since rho is a norm, it's non-negative.\n        # We need to handle the case rho = 0, where the weight is 1.\n        weights = np.ones_like(rho, dtype=float)\n        mask = rho > 0\n        weights[mask] = np.minimum(1.0, k / rho[mask])\n        return weights\n\n    def w_student_t(rho, nu, d):\n        \"\"\"Calculates the Student-t weight function.\"\"\"\n        rho = np.asarray(rho)\n        return (nu + d) / (nu + rho**2)\n\n    # Test parameters\n    rhos_test = np.array([0.5, 1.5, 3.0])\n    k_huber = 1.5\n    nu_student_t = 4.0\n    d_student_t = 3.0\n\n    # Evaluate weights for the test cases\n    weights_huber = w_huber(rhos_test, k=k_huber)\n    weights_student_t = w_student_t(rhos_test, nu=nu_student_t, d=d_student_t)\n\n    # --- Part 2: IRLS Algorithm ---\n\n    def irls_vertex_fit(r_tracks, sigma_in, weight_func, *args, max_iter=100, tol=1e-6):\n        \"\"\"\n        Performs an Iteratively Reweighted Least Squares fit for the vertex position.\n        This simplified version assumes C_i = sigma_in^2 * I_3 for all i.\n        \"\"\"\n        # Initial guess: unweighted mean (least squares solution)\n        v = np.mean(r_tracks, axis=0)\n\n        for _ in range(max_iter):\n            v_old = v\n            \n            # Calculate Mahalanobis norms (simplified form)\n            distances = np.linalg.norm(r_tracks - v, axis=1)\n            rhos = distances / sigma_in\n            \n            # Calculate weights\n            weights = weight_func(rhos, *args)\n            \n            # Update vertex as a weighted average\n            sum_weights = np.sum(weights)\n            if sum_weights  1e-9: # Avoid division by zero if all weights are tiny\n                # This could happen if all points are extreme outliers. Reset to mean.\n                v = np.mean(r_tracks, axis=0)\n            else:\n                v = np.sum(r_tracks * weights[:, np.newaxis], axis=0) / sum_weights\n\n            # Check for convergence\n            if np.linalg.norm(v - v_old)  tol:\n                break\n        \n        return v\n\n    # --- Part 3: Breakdown Study ---\n\n    def run_breakdown_study():\n        \"\"\"\n        Simulates track data with outliers and finds the breakdown point.\n        \"\"\"\n        # Fixed random seed for reproducibility\n        rng = np.random.default_rng(12345)\n        \n        # Simulation parameters from the problem description\n        v_true = np.array([0.0, 0.0, 0.0])\n        num_tracks = 100\n        sigma_in = 0.03  # mm\n        mu_out = np.array([10.0, -10.0, 5.0]) # mm\n        sigma_out = 2.0  # mm\n        outlier_fractions = [0.0, 0.2, 0.4, 0.6]\n        breakdown_threshold = 1.0  # mm\n\n        p_break_huber = 1.0\n        p_break_student = 1.0\n        huber_failed = False\n        student_failed = False\n\n        for p in sorted(outlier_fractions):\n            # Generate data\n            num_inliers = int(np.floor((1 - p) * num_tracks))\n            num_outliers = num_tracks - num_inliers\n\n            inliers = rng.normal(loc=v_true, scale=sigma_in, size=(num_inliers, 3))\n            outliers = rng.normal(loc=mu_out, scale=sigma_out, size=(num_outliers, 3))\n\n            if num_outliers > 0:\n                tracks = np.vstack((inliers, outliers))\n            else:\n                tracks = inliers\n\n            # Run IRLS for Huber loss\n            if not huber_failed:\n                v_huber = irls_vertex_fit(tracks, sigma_in, w_huber, k_huber)\n                error_huber = np.linalg.norm(v_huber - v_true)\n                if error_huber > breakdown_threshold:\n                    p_break_huber = p\n                    huber_failed = True\n\n            # Run IRLS for Student-t loss\n            if not student_failed:\n                v_student = irls_vertex_fit(tracks, sigma_in, w_student_t, nu_student_t, d_student_t)\n                error_student = np.linalg.norm(v_student - v_true)\n                if error_student > breakdown_threshold:\n                    p_break_student = p\n                    student_failed = True\n            \n            # If both have failed, we can stop early\n            if huber_failed and student_failed:\n                break\n\n        return p_break_huber, p_break_student\n\n    p_break_huber, p_break_student = run_breakdown_study()\n\n    # --- Final Output Formatting ---\n    results = [\n        *weights_huber,\n        *weights_student_t,\n        p_break_huber,\n        p_break_student\n    ]\n    \n    # Format the final output string exactly as required.\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}