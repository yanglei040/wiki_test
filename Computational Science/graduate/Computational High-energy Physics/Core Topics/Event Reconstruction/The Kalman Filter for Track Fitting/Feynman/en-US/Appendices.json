{
    "hands_on_practices": [
        {
            "introduction": "The prediction step of the Extended Kalman Filter linearizes the non-linear equations of motion for a particle's trajectory. This practice allows you to directly quantify the error introduced by this crucial first-order approximation. By comparing it to a more accurate second-order prediction, you will develop an intuition for how trajectory curvature and step size impact the filter's accuracy and learn to establish a criterion to control this error. ",
            "id": "3538947",
            "problem": "A charged particle of charge $q$ and momentum $p$ traversing a uniform magnetic field aligned along the $\\hat{z}$ axis follows circular motion in the transverse plane. In computational high-energy physics track fitting, the Extended Kalman Filter (EKF) propagates the track state and its uncertainty between detector layers by locally linearizing the nonlinear motion model. Consider a simplified, scientifically realistic kinematic model parameterized by arc length $s$ with the state vector $\\mathbf{x}(s) = (x(s), y(s), \\varphi(s))$, where $\\varphi$ is the heading angle in radians. Assume the curvature $\\kappa$ in inverse meters is constant over a small propagation step $\\Delta s$ and given by $\\kappa = \\frac{q B}{p}$, with $B$ the magnetic field magnitude. Neglect energy loss and material effects within the step. The equations of motion are\n$$\n\\frac{d x}{d s} = \\cos \\varphi, \\quad \\frac{d y}{d s} = \\sin \\varphi, \\quad \\frac{d \\varphi}{d s} = \\kappa.\n$$\nLet the nonlinear drift function be $f(\\mathbf{x}) = (\\cos \\varphi, \\sin \\varphi, \\kappa)$ and let $J(\\mathbf{x}) = \\frac{\\partial f}{\\partial \\mathbf{x}}$ denote its Jacobian. The EKF's first-order prediction of the mean state increment over a step $\\Delta s$ uses the linearization about the current mean to approximate the flow by the first-order term. The second-order Taylor expansion of the flow involves $J(\\mathbf{x}) f(\\mathbf{x})$ and quantifies the local curvature of the dynamics.\n\nYour task is to write a complete, runnable program that, for each specified test case, performs the following computations in meters and radians:\n- Compute the EKF first-order predicted increment for the position components,\n$$\n\\Delta \\mathbf{r}_{\\mathrm{EKF}} = \\begin{pmatrix} \\Delta x \\\\ \\Delta y \\end{pmatrix} = \\Delta s \\begin{pmatrix} \\cos \\varphi_0 \\\\ \\sin \\varphi_0 \\end{pmatrix},\n$$\nwhere $\\varphi_0$ is the initial angle in radians.\n- Compute the second-order Taylor predicted increment for the position components,\n$$\n\\Delta \\mathbf{r}_{2} = \\Delta s \\begin{pmatrix} \\cos \\varphi_0 \\\\ \\sin \\varphi_0 \\end{pmatrix} + \\frac{1}{2} \\Delta s^2 \\, \\big[J(\\mathbf{x}_0) f(\\mathbf{x}_0)\\big]_{x,y},\n$$\nwhere $\\mathbf{x}_0 = (x_0, y_0, \\varphi_0)$ is the initial state and $[\\,\\cdot\\,]_{x,y}$ selects the position components. Explicitly, use $J(\\mathbf{x}_0) f(\\mathbf{x}_0) = \\kappa \\, (-\\sin \\varphi_0, \\cos \\varphi_0, 0)$.\n- Define and compute the linearization error,\n$$\ne_{\\mathrm{lin}} = \\left\\| \\Delta \\mathbf{r}_{2} - \\Delta \\mathbf{r}_{\\mathrm{EKF}} \\right\\|,\n$$\nexpressed in meters, using the Euclidean norm over the position components.\n- Propose and compute a step-size criterion $\\Delta s_{\\max}$ to keep the neglected second-order term below a specified tolerance $\\tau$ (in meters). Use the bound derived from the model,\n$$\n\\left\\| \\Delta \\mathbf{r}_{2} - \\Delta \\mathbf{r}_{\\mathrm{EKF}} \\right\\| \\le \\frac{1}{2} \\, |\\kappa| \\, \\Delta s^2,\n$$\nand set\n$$\n\\Delta s_{\\max} = \n\\begin{cases}\n\\sqrt{\\dfrac{2 \\tau}{|\\kappa|}}, & \\text{if } \\kappa \\ne 0, \\\\\n10^9, & \\text{if } \\kappa = 0,\n\\end{cases}\n$$\nwhere the value $10^9$ meters serves as a large sentinel representing an effectively unbounded step in the straight-line case.\n- Return two booleans per test case: whether the actual computed $e_{\\mathrm{lin}}$ is less than or equal to the tolerance $\\tau$, and whether the provided step size $\\Delta s$ satisfies the criterion $\\Delta s \\le \\Delta s_{\\max}$.\n\nAll angles must be in radians, arc lengths in meters, curvature in inverse meters, and tolerances in meters. The final program output must aggregate the results of all test cases into a single line printed to standard output as a comma-separated list enclosed in square brackets, where each test case contributes a list of the form `[e_lin, ok_actual, ok_criterion, ds_max]`. The quantities $e_{\\mathrm{lin}}$ and $\\Delta s_{\\max}$ must be provided in meters.\n\nTest suite:\n- General case: $\\varphi_0 = 0.0$ radians, $\\kappa = 0.001$ inverse meters, $\\Delta s = 0.1$ meters, $\\tau = 0.0001$ meters.\n- Boundary case: $\\varphi_0 = \\frac{\\pi}{4}$ radians, $\\kappa = 0.01$ inverse meters, $\\Delta s = \\sqrt{10^{-5}}$ meters, $\\tau = 0.00005$ meters.\n- Edge case (straight line): $\\varphi_0 = \\pi$ radians, $\\kappa = 0.0$ inverse meters, $\\Delta s = 10.0$ meters, $\\tau = 0.000001$ meters.\n- High-curvature violation: $\\varphi_0 = \\frac{\\pi}{3}$ radians, $\\kappa = 1.0$ inverse meters, $\\Delta s = 0.05$ meters, $\\tau = 0.001$ meters.\n- Negative curvature: $\\varphi_0 = \\frac{\\pi}{6}$ radians, $\\kappa = -0.5$ inverse meters, $\\Delta s = 0.03$ meters, $\\tau = 0.0005$ meters.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, `[ [a,b,c,d], [\\dots], \\dots ]`), where $a$ and $d$ are floats in meters, and $b$ and $c$ are booleans.",
            "solution": "The problem at hand is a well-posed and scientifically sound exercise in kinematics and numerical methods, specifically pertaining to track fitting in high-energy physics. It asks for the analysis of the linearization error in an Extended Kalman Filter (EKF) context for a simplified particle trajectory model. We shall first validate the provided formulas and then outline the computational procedure.\n\nA charged particle with momentum $p$ and charge $q$ travels through a uniform magnetic field $B$ aligned with the $\\hat{z}$-axis. Its trajectory in the transverse $(x, y)$ plane is a circle. The state of the particle is described by the vector $\\mathbf{x}(s) = (x(s), y(s), \\varphi(s))^T$, parameterized by the arc length $s$. The components are the Cartesian coordinates $x$ and $y$, and the heading angle $\\varphi$.\n\nThe dynamics are governed by the system of ordinary differential equations:\n$$\n\\frac{d\\mathbf{x}}{ds} = \n\\begin{pmatrix} dx/ds \\\\ dy/ds \\\\ d\\varphi/ds \\end{pmatrix} =\n\\begin{pmatrix} \\cos\\varphi \\\\ \\sin\\varphi \\\\ \\kappa \\end{pmatrix} = f(\\mathbf{x})\n$$\nHere, $\\kappa = qB/p$ is the signed curvature, assumed constant over a small propagation step $\\Delta s$. The function $f(\\mathbf{x})$ is the nonlinear drift function.\n\nThe EKF propagates the state estimate by linearizing the dynamics. A Taylor expansion of the state vector increment $\\Delta\\mathbf{x} = \\mathbf{x}(s_0 + \\Delta s) - \\mathbf{x}(s_0)$ around the initial state $\\mathbf{x}_0 = \\mathbf{x}(s_0)$ gives:\n$$\n\\Delta\\mathbf{x} = \\Delta s \\left.\\frac{d\\mathbf{x}}{ds}\\right|_{s_0} + \\frac{(\\Delta s)^2}{2!} \\left.\\frac{d^2\\mathbf{x}}{ds^2}\\right|_{s_0} + O((\\Delta s)^3)\n$$\nThe derivatives are given by:\n$$\n\\frac{d\\mathbf{x}}{ds} = f(\\mathbf{x})\n$$\n$$\n\\frac{d^2\\mathbf{x}}{ds^2} = \\frac{d}{ds}f(\\mathbf{x}(s)) = \\frac{\\partial f}{\\partial \\mathbf{x}} \\frac{d\\mathbf{x}}{ds} = J(\\mathbf{x}) f(\\mathbf{x})\n$$\nwhere $J(\\mathbf{x}) = \\frac{\\partial f}{\\partial \\mathbf{x}}$ is the Jacobian matrix of the drift function. The increment up to second order is thus:\n$$\n\\Delta\\mathbf{x} \\approx \\Delta s \\, f(\\mathbf{x}_0) + \\frac{(\\Delta s)^2}{2} J(\\mathbf{x}_0) f(\\mathbf{x}_0)\n$$\nThe EKF's first-order prediction for the position increment $\\Delta\\mathbf{r}_{\\mathrm{EKF}}$ corresponds to the first term, restricted to the position components $(x, y)$:\n$$\n\\Delta\\mathbf{r}_{\\mathrm{EKF}} = \\Delta s \\begin{pmatrix} \\cos\\varphi_0 \\\\ \\sin\\varphi_0 \\end{pmatrix}\n$$\nThe second-order Taylor prediction, $\\Delta\\mathbf{r}_2$, includes the next term:\n$$\n\\Delta\\mathbf{r}_{2} = \\Delta s \\begin{pmatrix} \\cos\\varphi_0 \\\\ \\sin\\varphi_0 \\end{pmatrix} + \\frac{(\\Delta s)^2}{2} \\big[J(\\mathbf{x}_0) f(\\mathbf{x}_0)\\big]_{x,y}\n$$\nTo verify the term provided in the problem, we compute the Jacobian $J(\\mathbf{x})$:\n$$\nJ(\\mathbf{x}) = \\frac{\\partial}{\\partial (x, y, \\varphi)} \\begin{pmatrix} \\cos\\varphi \\\\ \\sin\\varphi \\\\ \\kappa \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & -\\sin\\varphi \\\\ 0 & 0 & \\cos\\varphi \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\nThen we compute the product $J(\\mathbf{x})f(\\mathbf{x})$:\n$$\nJ(\\mathbf{x})f(\\mathbf{x}) = \\begin{pmatrix} 0 & 0 & -\\sin\\varphi \\\\ 0 & 0 & \\cos\\varphi \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\cos\\varphi \\\\ \\sin\\varphi \\\\ \\kappa \\end{pmatrix} = \\begin{pmatrix} -\\kappa\\sin\\varphi \\\\ \\kappa\\cos\\varphi \\\\ 0 \\end{pmatrix}\n$$\nThis confirms the expression $J(\\mathbf{x}_0) f(\\mathbf{x}_0) = \\kappa(-\\sin\\varphi_0, \\cos\\varphi_0, 0)^T$ given in the problem statement.\n\nThe linearization error, $e_{\\mathrm{lin}}$, is defined as the magnitude of the difference between the second-order and first-order predictions for the position increment:\n$$\ne_{\\mathrm{lin}} = \\| \\Delta\\mathbf{r}_2 - \\Delta\\mathbf{r}_{\\mathrm{EKF}} \\| = \\left\\| \\frac{(\\Delta s)^2}{2} \\big[J(\\mathbf{x}_0) f(\\mathbf{x}_0)\\big]_{x,y} \\right\\|\n$$\nSubstituting the derived expression:\n$$\ne_{\\mathrm{lin}} = \\left\\| \\frac{(\\Delta s)^2}{2} \\begin{pmatrix} -\\kappa\\sin\\varphi_0 \\\\ \\kappa\\cos\\varphi_0 \\end{pmatrix} \\right\\| = \\frac{(\\Delta s)^2}{2} |\\kappa| \\sqrt{(-\\sin\\varphi_0)^2 + (\\cos\\varphi_0)^2} = \\frac{1}{2} |\\kappa| (\\Delta s)^2\n$$\nThis result is notable. It shows that the error bound given in the problem, $\\left\\| \\Delta \\mathbf{r}_{2} - \\Delta \\mathbf{r}_{\\mathrm{EKF}} \\right\\| \\le \\frac{1}{2} |\\kappa| (\\Delta s)^2$, is in fact an equality for this specific kinematic model.\n\nThe task requires computing two boolean flags. The first, $ok_{\\mathrm{actual}}$, checks if the computed linearization error is within a specified tolerance $\\tau$:\n$$\nok_{\\mathrm{actual}} \\iff e_{\\mathrm{lin}} \\le \\tau \\iff \\frac{1}{2} |\\kappa| (\\Delta s)^2 \\le \\tau\n$$\nThe second, $ok_{\\mathrm{criterion}}$, checks if the step size $\\Delta s$ adheres to a derived maximum step size $\\Delta s_{\\max}$. This maximum step size is defined to keep the error bound below $\\tau$. For $\\kappa \\ne 0$:\n$$\n\\frac{1}{2} |\\kappa| (\\Delta s)^2 \\le \\tau \\implies (\\Delta s)^2 \\le \\frac{2\\tau}{|\\kappa|} \\implies \\Delta s \\le \\sqrt{\\frac{2\\tau}{|\\kappa|}}\n$$\nThis gives $\\Delta s_{\\max} = \\sqrt{\\frac{2\\tau}{|\\kappa|}}$. The condition $\\Delta s \\le \\Delta s_{\\max}$ is therefore mathematically equivalent to $e_{\\mathrm{lin}} \\le \\tau$. For the case $\\kappa = 0$, $e_{\\mathrm{lin}} = 0$, so $e_{\\mathrm{lin}} \\le \\tau$ is true for any $\\tau \\ge 0$. The corresponding $\\Delta s_{\\max}$ is taken as a large number ($10^9$ m), so $\\Delta s \\le \\Delta s_{\\max}$ will also be true for any reasonable step size. Consequently, the two boolean flags, $ok_{\\mathrm{actual}}$ and $ok_{\\mathrm{criterion}}$, must be identical for all valid test cases.\n\nThe computational steps for each test case $(\\varphi_0, \\kappa, \\Delta s, \\tau)$ are as follows:\n$1$. Compute the linearization error: $e_{\\mathrm{lin}} = \\frac{1}{2} |\\kappa| (\\Delta s)^2$.\n$2$. Determine the first boolean: $ok_{\\mathrm{actual}} = (e_{\\mathrm{lin}} \\le \\tau)$.\n$3$. Compute the maximum allowed step size: $\\Delta s_{\\max} = \\sqrt{2\\tau/|\\kappa|}$ if $\\kappa \\ne 0$, and $\\Delta s_{\\max} = 10^9$ if $\\kappa = 0$.\n$4$. Determine the second boolean: $ok_{\\mathrm{criterion}} = (\\Delta s \\le \\Delta s_{\\max})$.\n$5$. Collect the four results: $[e_{\\mathrm{lin}}, ok_{\\mathrm{actual}}, ok_{\\mathrm{criterion}}, \\Delta s_{\\max}]$.\nThis procedure will be implemented for each test case provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs computations related to the EKF linearization error for a simplified\n    particle tracking model, as specified in the problem statement.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (phi0_rad, kappa_inv_m, ds_m, tau_m)\n    test_cases = [\n        # General case\n        (0.0, 0.001, 0.1, 0.0001),\n        # Boundary case\n        (np.pi / 4, 0.01, np.sqrt(1e-5), 0.00005),\n        # Edge case (straight line)\n        (np.pi, 0.0, 10.0, 0.000001),\n        # High-curvature violation\n        (np.pi / 3, 1.0, 0.05, 0.001),\n        # Negative curvature\n        (np.pi / 6, -0.5, 0.03, 0.0005),\n    ]\n\n    results = []\n    for case in test_cases:\n        phi0, kappa, ds, tau = case\n\n        # 1. Compute the linearization error, e_lin\n        # As derived, e_lin = 0.5 * |kappa| * ds^2\n        e_lin = 0.5 * np.abs(kappa) * ds**2\n\n        # 2. Check if the actual computed e_lin is less than or equal to the tolerance tau\n        ok_actual = e_lin <= tau\n\n        # 3. Compute the step-size criterion, ds_max\n        if kappa != 0:\n            ds_max = np.sqrt(2 * tau / np.abs(kappa))\n        else:\n            # Sentinel value for an effectively unbounded step in the straight-line case\n            ds_max = 1e9\n        \n        # 4. Check if the provided step size ds satisfies the criterion ds <= ds_max\n        ok_criterion = ds <= ds_max\n        \n        # Aggregate the results for the current test case.\n        # As established by the derivation, ok_actual and ok_criterion should be identical.\n        # This serves as an internal consistency check of the model and its implementation.\n        results.append([e_lin, ok_actual, ok_criterion, ds_max])\n\n    # Final print statement in the exact required format.\n    # The format is a string representation of a list of lists, constructed by joining\n    # the string representations of the inner lists with a comma.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While mathematically sound, a Kalman filter's performance in the real world is dictated by its numerical implementation. This exercise delves into the critical issue of numerical stability in the covariance update step, which can fail catastrophically in finite-precision arithmetic. You will compare the common but fragile simplified update form with the robust Joseph form, and learn to diagnose and prevent numerical failures like the loss of positive-definiteness in the covariance matrix. ",
            "id": "3538970",
            "problem": "Consider a linearized track-parameter update in computational high-energy physics for a charged particle traversing a silicon detector plane. Let the state vector be $x \\in \\mathbb{R}^4$ containing the local parameters $x$-position, $y$-position, $x$-slope, and $y$-slope, in that order. The positions are measured in millimeters, while slopes are dimensionless. A single measurement $z \\in \\mathbb{R}^2$ provides the transverse positions with a known measurement noise covariance. Assume a linear Gaussian measurement model with a known Jacobian matrix that maps state to measurement. You are given a predicted state covariance $P^- \\in \\mathbb{R}^{4 \\times 4}$, a measurement Jacobian $H \\in \\mathbb{R}^{2 \\times 4}$, and a measurement noise covariance $R \\in \\mathbb{R}^{2 \\times 2}$. Your task is to derive from first principles and implement, in single precision arithmetic, two distinct covariance update implementations: the numerically stable update that follows from an exact Bayesian treatment of linear Gaussian models, and a commonly used algebraic simplification. Then, quantify in finite-precision arithmetic the risk of loss of positive semidefiniteness and symmetry in the updated covariance, and propose a mixed-precision safeguard strategy that reduces these risks.\n\nUse the following context-appropriate foundational base and assumptions: linear Gaussian measurement model, the definition of conditional covariance for jointly Gaussian variables, and basic matrix identities. Express all mathematical entities in LaTeX. Do not use any pre-derived shortcut formulas for the covariance update in the problem statement.\n\nImplementation requirements:\n- Work with $4 \\times 4$ covariance matrices $P^-$. Positions are in millimeters and slopes are dimensionless.\n- The measurement Jacobian $H$ maps the state to the measured transverse positions, representing the local planar approximation: specifically, use $H$ that selects the first two state components.\n- The measurement noise covariance $R$ is diagonal, with entries equal to the squared measurement resolution in millimeters.\n- Implement three covariance updates:\n  1. A single-precision arithmetic implementation of the algebraically simplified covariance update obtained from the linear Gaussian Bayesian derivation.\n  2. A single-precision arithmetic implementation of the numerically stable covariance update obtained from the same derivation that preserves positive semidefiniteness in exact arithmetic.\n  3. A mixed-precision safeguard variant that performs critical matrix products and inversions in double precision, applies a symmetry restoration, and returns a single-precision covariance.\n\nRisk quantification metrics:\n- For each updated covariance $P^+ \\in \\mathbb{R}^{4 \\times 4}$, compute the minimal eigenvalue of the symmetrized matrix $\\frac{1}{2}(P^+ + (P^+)^\\top)$, denoted $\\lambda_{\\min}$, as a float. A negative $\\lambda_{\\min}$ indicates a loss of positive semidefiniteness. This quantity has mixed physical units across state components and should be reported as a float without units.\n- Compute the relative symmetry error $\\|\\Delta\\|_F / \\|P^+\\|_F$, where $\\Delta = P^+ - (P^+)^\\top$ and $\\|\\cdot\\|_F$ denotes the Frobenius norm. Report as a float.\n- Compute a physically meaningful diagonal inflation magnitude for the position sub-block only, defined as the minimal non-negative jitter $j_{\\text{pos}}$ in $\\text{mm}^2$ that must be added to the $2 \\times 2$ position block on the diagonal to make that position block positive semidefinite. Concretely, if the minimal eigenvalue of the symmetrized $2 \\times 2$ top-left block of $P^+$ is $\\lambda_{\\min}^{\\text{pos}}$, then $j_{\\text{pos}} = \\max(0, -\\lambda_{\\min}^{\\text{pos}})$. Express $j_{\\text{pos}}$ in $\\text{mm}^2$ as a float.\n\nTest suite and parameters:\nUse the following four test cases to exercise different numerical regimes. For each case, construct $P^-$ as $P^- = D + U U^\\top$, where $D$ is diagonal with entries equal to the squared standard deviations of the state components, and $U \\in \\mathbb{R}^{4 \\times 2}$ is a fixed low-rank perturbation supplying correlations. In each case, use $H$ that selects positions: $$ H = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\end{bmatrix} $$.\n\n- Case $1$ (well-conditioned, typical silicon hit resolution):\n  - State standard deviations: $\\sigma_x = 0.05\\,\\text{mm}$, $\\sigma_y = 0.05\\,\\text{mm}$, $\\sigma_{t_x} = 0.001$, $\\sigma_{t_y} = 0.001$.\n  - Measurement resolution: $r_x = 0.02\\,\\text{mm}$, $r_y = 0.02\\,\\text{mm}$, so $R = \\operatorname{diag}(r_x^2, r_y^2)$.\n  - Low-rank perturbation:\n    $$U = \\begin{bmatrix}\n    0.005 & -0.003 \\\\\n    0.004 & 0.002 \\\\\n    0.0005 & -0.0002 \\\\\n    0.0004 & 0.0006\n    \\end{bmatrix}.$$\n- Case $2$ (near-singular position covariances with tiny measurement noise, stressing inversion and single-precision rounding):\n  - State standard deviations: $\\sigma_x = 10^{-6}\\,\\text{mm}$, $\\sigma_y = 10^{-6}\\,\\text{mm}$, $\\sigma_{t_x} = 10^{2}$, $\\sigma_{t_y} = 10^{2}$.\n  - Measurement resolution: $r_x = 10^{-6}\\,\\text{mm}$, $r_y = 10^{-6}\\,\\text{mm}$.\n  - Low-rank perturbation:\n    $$U = \\begin{bmatrix}\n    10^{-9} & -10^{-9} \\\\\n    10^{-9} & 10^{-9} \\\\\n    0.1 & -0.1 \\\\\n    0.1 & 0.1\n    \\end{bmatrix}.$$\n- Case $3$ (high measurement noise dominating the update):\n  - State standard deviations: $\\sigma_x = 0.01\\,\\text{mm}$, $\\sigma_y = 0.01\\,\\text{mm}$, $\\sigma_{t_x} = 0.005$, $\\sigma_{t_y} = 0.005$.\n  - Measurement resolution: $r_x = 5.0\\,\\text{mm}$, $r_y = 5.0\\,\\text{mm}$.\n  - Low-rank perturbation:\n    $$U = \\begin{bmatrix}\n    0.002 & 0.0015 \\\\\n    0.001 & -0.002 \\\\\n    0.0005 & 0.0003 \\\\\n    0.0004 & -0.0001\n    \\end{bmatrix}.$$\n- Case $4$ (strongly ill-conditioned covariance with large dynamic range):\n  - State standard deviations: $\\sigma_x = 10^{3}\\,\\text{mm}$, $\\sigma_y = 10^{-6}\\,\\text{mm}$, $\\sigma_{t_x} = 10.0$, $\\sigma_{t_y} = 10^{-3}$.\n  - Measurement resolution: $r_x = 0.1\\,\\text{mm}$, $r_y = 0.1\\,\\text{mm}$.\n  - Low-rank perturbation:\n    $$U = \\begin{bmatrix}\n    1.0 & 0.0 \\\\\n    0.0 & 10^{-7} \\\\\n    0.01 & -0.02 \\\\\n    10^{-4} & 5 \\times 10^{-5}\n    \\end{bmatrix}.$$\n\nProgram outputs:\nFor each test case, compute and collect the following six floats in this exact order:\n`[lam_min_simp32, lam_min_joseph32, lam_min_mixed, sym_err_simp32, sym_err_joseph32, j_pos_simp32]`,\nwhere the superscripts indicate which update was used. Aggregate the results for all four test cases into a single list in case order, flattening the per-case lists. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, `[r_1,r_2,...,r_24]`). All jitter magnitudes $j_{\\text{pos}}$ must be expressed in $\\text{mm}^2$ and reported as floats. Angles are not used in this problem. Percentages are not used; all quantities are reported as decimals (floats).",
            "solution": "The problem requires the derivation of two forms of the Kalman filter covariance update from first principles and an analysis of their numerical stability in finite-precision arithmetic.\n\n### **1. Derivation from First Principles**\n\nWe begin by formalizing the linear Gaussian measurement model. Let the state vector prior to the measurement be a random variable $x \\in \\mathbb{R}^4$, distributed according to a Gaussian probability density function with mean $x^-$ and covariance $P^- \\in \\mathbb{R}^{4 \\times 4}$. We write this as $x \\sim \\mathcal{N}(x^-, P^-)$. The state vector components are $(x_{\\text{pos}}, y_{\\text{pos}}, t_x, t_y)$, where positions are in millimeters and slopes $t_x, t_y$ are dimensionless.\n\nThe measurement process is modeled as a linear transformation of the state corrupted by additive Gaussian noise. A measurement $z \\in \\mathbb{R}^2$ is related to the state $x$ by the equation:\n$$z = Hx + v$$\nwhere $H \\in \\mathbb{R}^{2 \\times 4}$ is the measurement Jacobian and $v \\in \\mathbb{R}^2$ is the measurement noise. The noise is assumed to be a zero-mean Gaussian random variable, $v \\sim \\mathcal{N}(0, R)$, with a known covariance $R \\in \\mathbb{R}^{2 \\times 2}$. The state $x$ and the noise $v$ are assumed to be uncorrelated.\n\nThe goal of the filtering step is to find the posterior distribution of the state $x$ conditioned on the measurement $z$, i.e., $p(x|z)$. For a linear Gaussian system, this posterior is also Gaussian, $x|z \\sim \\mathcal{N}(x^+, P^+)$. The updated covariance $P^+$ is independent of the state means and the specific measurement value $z$; it depends only on the prior covariance $P^-$ and the parameters of the measurement model, $H$ and $R$.\n\nTo derive $P^+$, we consider the joint distribution of the stacked vector $\\begin{pmatrix} x \\\\ z \\end{pmatrix}$. Since $x$ and $z$ are jointly Gaussian, this vector follows a multivariate normal distribution. Its mean is:\n$$E\\left[\\begin{pmatrix} x \\\\ z \\end{pmatrix}\\right] = \\begin{pmatrix} E[x] \\\\ E[Hx + v] \\end{pmatrix} = \\begin{pmatrix} x^- \\\\ H E[x] + E[v] \\end{pmatrix} = \\begin{pmatrix} x^- \\\\ Hx^- \\end{pmatrix}$$\nThe covariance matrix of the joint vector is a block matrix:\n$$\\text{Cov}\\left(\\begin{pmatrix} x \\\\ z \\end{pmatrix}\\right) = \\begin{pmatrix} \\text{Cov}(x, x) & \\text{Cov}(x, z) \\\\ \\text{Cov}(z, x) & \\text{Cov}(z, z) \\end{pmatrix}$$\nWe compute each block:\n- $\\text{Cov}(x, x) = P^-$ (by definition).\n- $\\text{Cov}(x, z) = \\text{Cov}(x, Hx + v) = \\text{Cov}(x, Hx) + \\text{Cov}(x, v) = \\text{Cov}(x, x)H^\\top + 0 = P^- H^\\top$, as $x$ and $v$ are uncorrelated.\n- $\\text{Cov}(z, x) = \\text{Cov}(Hx + v, x) = (\\text{Cov}(x, Hx+v))^\\top = (P^- H^\\top)^\\top = H (P^-)^\\top$. Since covariance matrices are symmetric, $(P^-)^\\top = P^-$, so $\\text{Cov(z, x)} = H P^-$.\n- $\\text{Cov}(z, z) = \\text{Cov}(Hx + v, Hx + v) = \\text{Cov}(Hx, Hx) + \\text{Cov}(Hx, v) + \\text{Cov}(v, Hx) + \\text{Cov}(v, v)$. The cross-terms are zero. Thus, $\\text{Cov}(z, z) = H \\text{Cov}(x, x) H^\\top + R = H P^- H^\\top + R$. This term is the *residual covariance*, denoted $S$.\n\nThe joint covariance matrix is therefore:\n$$\\text{Cov}\\left(\\begin{pmatrix} x \\\\ z \\end{pmatrix}\\right) = \\begin{pmatrix} P^- & P^- H^\\top \\\\ H P^- & S \\end{pmatrix} \\quad \\text{where} \\quad S = H P^- H^\\top + R$$\n\nFor a partitioned Gaussian vector $\\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}$ with covariance $\\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$, the conditional covariance of $y_1$ given $y_2$ is given by the formula $\\text{Cov}(y_1|y_2) = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}$.\nApplying this to our system with $y_1 \\leftrightarrow x$ and $y_2 \\leftrightarrow z$, the updated state covariance $P^+$ is:\n$$P^+ = P^- - (P^- H^\\top) S^{-1} (H P^-)$$\n\nThis equation provides the basis for the two required implementations.\n\n#### **Form 1: Algebraically Simplified Update**\n\nWe can define the Kalman gain matrix $K \\in \\mathbb{R}^{4 \\times 2}$ as $K = P^- H^\\top S^{-1}$. Substituting this into the equation for $P^+$ gives:\n$$P^+ = P^- - K S K^\\top$$\nHowever, a more common algebraic simplification is:\n$$P^+ = P^- - K (H P^-) = (I - KH) P^-$$\nwhere $I$ is the $4 \\times 4$ identity matrix. This form is computationally efficient, requiring one $2 \\times 2$ matrix inversion and a few matrix multiplications. However, it is known to be numerically unstable. In finite-precision arithmetic, both $(I - KH)$ and $P^-$ can be large, and their product followed by subtraction from $P^-$ can lead to a loss of precision. If $P^-$ is nearly singular in the subspace measured by $H$, the subtraction can result in a $P^+$ that is numerically no longer symmetric or positive semidefinite, violating the properties of a covariance matrix.\n\n#### **Form 2: Numerically Stable (Joseph) Update**\n\nA more numerically robust formulation, known as the Joseph form, is derived by rewriting the update to avoid the subtraction of large matrices. It can be shown to be algebraically equivalent to the form above:\n$$P^+ = (I - KH) P^- (I - KH)^\\top + K R K^\\top$$\nThe equivalence is verified by expanding the terms:\n\\begin{align*} P^+ &= (P^- - KHP^-)(I - H^\\top K^\\top) + KRK^\\top \\\\ &= P^- - P^-H^\\top K^\\top - KHP^- + KHP^-H^\\top K^\\top + KRK^\\top \\\\ &= P^- - (KHP^-)^\\top - KHP^- + K(HP^-H^\\top + R)K^\\top \\\\ &= P^- - (KHP^-)^\\top - KHP^- + K S K^\\top \\end{align*}\nSubstituting $K = P^- H^\\top S^{-1}$ and using the symmetry of $P^-$ and $S$:\n\\begin{align*} P^+ &= P^- - (P^-H^\\top S^{-1} H P^-)^\\top - P^-H^\\top S^{-1} H P^- + (P^-H^\\top S^{-1}) S (S^{-1}H P^-) \\\\ &= P^- - P^-H^\\top S^{-1} H P^- - P^-H^\\top S^{-1} H P^- + P^-H^\\top S^{-1} H P^- \\\\ &= P^- - P^- H^\\top S^{-1} H P^- \\end{align*}\nThis confirms the algebraic equivalence. The numerical stability of the Joseph form arises because it is expressed as a sum of two matrices. The term $(I - KH) P^- (I - KH)^\\top$ has the structure $A \\Sigma A^\\top$ and thus preserves the positive semidefiniteness of $P^-$. The term $K R K^\\top$ also has this structure (since $R$ is positive definite) and is therefore positive semidefinite. The sum of two positive semidefinite matrices is always positive semidefinite. This structure ensures that, in exact arithmetic, $P^+$ is always a valid covariance matrix. In finite-precision arithmetic, this form is substantially less prone to producing a non-positive-semidefinite result.\n\n### **2. Mixed-Precision Safeguard Strategy**\n\nNumerical issues are exacerbated by using single-precision (`float32`) arithmetic, which has a limited number of significant digits (approximately $7$). Ill-conditioned problems, such as those with a large dynamic range in the covariance matrix entries or near-singular residual covariance $S$, are particularly vulnerable.\n\nA robust safeguard is to perform the most numerically sensitive operations in double precision (`float64`, approx. $16$ digits). The proposed mixed-precision strategy leverages the superior stability of the Joseph form and enhances it with higher precision for key steps:\n1.  Cast the input matrices $P^-$, $H$, and $R$ from single to double precision.\n2.  Compute all intermediate quantities—$S$, $S^{-1}$, $K$, and the two terms of the Joseph form update—in double precision.\n3.  Sum the terms to get the updated covariance $P^+_{64}$ in double precision.\n4.  Apply an explicit symmetrization step: $P^+_{\\text{sym}, 64} = \\frac{1}{2}(P^+_{64} + (P^+_{64})^\\top)$. This corrects any small asymmetries introduced by the non-associativity of floating-point matrix multiplication.\n5.  Cast the final, symmetrized, double-precision result back to a single-precision matrix.\n\nThis approach mitigates the risk of catastrophic cancellation and inaccurate matrix inversion while ensuring the final matrix is symmetric by construction. The final cast to single precision is necessary to conform to memory or storage constraints often present in high-energy physics data processing frameworks.\n\n### **3. Risk Quantification Metrics**\n\nTo quantify the numerical robustness of each implementation, the following metrics are used:\n-   **Minimal Eigenvalue ($\\lambda_{\\min}$)**: The minimal eigenvalue of the symmetrized updated covariance, $\\frac{1}{2}(P^+ + (P^+)^\\top)$. A negative value for $\\lambda_{\\min}$ indicates that the resulting matrix has lost the crucial property of positive semidefiniteness.\n-   **Relative Symmetry Error**: The Frobenius norm of the skew-symmetric part of $P^+$, normalized by the norm of $P^+$, i.e., $\\|P^+ - (P^+)^\\top\\|_F / \\|P^+\\|_F$. This measures the degree of asymmetry introduced by floating-point rounding errors.\n-   **Positional Jitter ($j_{\\text{pos}}$)**: A physically motivated metric defined as $j_{\\text{pos}} = \\max(0, -\\lambda_{\\min}^{\\text{pos}})$, where $\\lambda_{\\min}^{\\text{pos}}$ is the minimal eigenvalue of the symmetrized $2 \\times 2$ position sub-block of $P^+$. This value represents the minimum squared error (in $\\text{mm}^2$) that must be added to the diagonal position components to restore positive semidefiniteness to the position-related part of the covariance.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef update_simp32(p_minus: np.ndarray, h: np.ndarray, r: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the covariance update using the algebraically simplified form\n    P+ = (I - K*H)*P- in single precision.\n    \"\"\"\n    s = h @ p_minus @ h.T + r\n    k = p_minus @ h.T @ np.linalg.inv(s)\n    p_plus = (np.eye(4, dtype=np.float32) - k @ h) @ p_minus\n    return p_plus\n\ndef update_joseph32(p_minus: np.ndarray, h: np.ndarray, r: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the covariance update using the numerically stable Joseph form\n    P+ = (I-K*H)*P-*(I-K*H).T + K*R*K.T in single precision.\n    \"\"\"\n    s = h @ p_minus @ h.T + r\n    k = p_minus @ h.T @ np.linalg.inv(s)\n    \n    i_kh = np.eye(4, dtype=np.float32) - k @ h\n    term1 = i_kh @ p_minus @ i_kh.T\n    term2 = k @ r @ k.T\n    p_plus = term1 + term2\n    return p_plus\n\ndef update_mixed(p_minus_32: np.ndarray, h_32: np.ndarray, r_32: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the covariance update using the Joseph form in double precision\n    with an explicit symmetrization step, then casts back to single precision.\n    \"\"\"\n    p_minus_64 = p_minus_32.astype(np.float64)\n    h_64 = h_32.astype(np.float64)\n    r_64 = r_32.astype(np.float64)\n\n    s_64 = h_64 @ p_minus_64 @ h_64.T + r_64\n    k_64 = p_minus_64 @ h_64.T @ np.linalg.inv(s_64)\n\n    i_kh_64 = np.eye(4, dtype=np.float64) - k_64 @ h_64\n    term1_64 = i_kh_64 @ p_minus_64 @ i_kh_64.T\n    term2_64 = k_64 @ r_64 @ k_64.T\n    p_plus_64 = term1_64 + term2_64\n\n    # Explicit symmetry restoration\n    p_plus_sym_64 = 0.5 * (p_plus_64 + p_plus_64.T)\n\n    return p_plus_sym_64.astype(np.float32)\n\ndef calculate_min_eigenvalue(p_plus: np.ndarray) -> float:\n    \"\"\"Computes the minimum eigenvalue of the symmetrized matrix.\"\"\"\n    p_sym = 0.5 * (p_plus + p_plus.T)\n    # Use eigvalsh for hermitian (real-symmetric) matrices\n    min_eig = np.min(np.linalg.eigvalsh(p_sym))\n    return min_eig.item()\n\ndef calculate_rel_symmetry_error(p_plus: np.ndarray) -> float:\n    \"\"\"Computes the relative symmetry error.\"\"\"\n    delta = p_plus - p_plus.T\n    norm_delta = np.linalg.norm(delta, 'fro')\n    norm_p = np.linalg.norm(p_plus, 'fro')\n    if norm_p == 0:\n        return 0.0\n    return (norm_delta / norm_p).item()\n\ndef calculate_positional_jitter(p_plus: np.ndarray) -> float:\n    \"\"\"Computes the positional jitter j_pos.\"\"\"\n    p_pos = p_plus[:2, :2]\n    p_pos_sym = 0.5 * (p_pos + p_pos.T)\n    min_eig_pos = np.min(np.linalg.eigvalsh(p_pos_sym))\n    jitter = np.maximum(0.0, -min_eig_pos, dtype=np.float32)\n    return jitter.item()\n\ndef solve():\n    test_cases = [\n        # Case 1: well-conditioned\n        {'s': [0.05, 0.05, 0.001, 0.001], 'r': [0.02, 0.02], 'u': np.array([[0.005, -0.003], [0.004, 0.002], [0.0005, -0.0002], [0.0004, 0.0006]])},\n        # Case 2: near-singular\n        {'s': [1e-6, 1e-6, 1e2, 1e2], 'r': [1e-6, 1e-6], 'u': np.array([[1e-9, -1e-9], [1e-9, 1e-9], [0.1, -0.1], [0.1, 0.1]])},\n        # Case 3: high measurement noise\n        {'s': [0.01, 0.01, 0.005, 0.005], 'r': [5.0, 5.0], 'u': np.array([[0.002, 0.0015], [0.001, -0.002], [0.0005, 0.0003], [0.0004, -0.0001]])},\n        # Case 4: ill-conditioned\n        {'s': [1e3, 1e-6, 10.0, 1e-3], 'r': [0.1, 0.1], 'u': np.array([[1.0, 0.0], [0.0, 1e-7], [0.01, -0.02], [1e-4, 5e-5]])}\n    ]\n\n    h_matrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], dtype=np.float32)\n    results = []\n\n    for case in test_cases:\n        sigmas = np.array(case['s'], dtype=np.float32)\n        res = np.array(case['r'], dtype=np.float32)\n        u_matrix = case['u'].astype(np.float32)\n\n        d_matrix = np.diag(sigmas**2)\n        p_minus = (d_matrix + u_matrix @ u_matrix.T).astype(np.float32)\n        r_matrix = np.diag(res**2).astype(np.float32)\n\n        # Run the three update implementations\n        p_simp32 = update_simp32(p_minus, h_matrix, r_matrix)\n        p_joseph32 = update_joseph32(p_minus, h_matrix, r_matrix)\n        p_mixed = update_mixed(p_minus, h_matrix, r_matrix)\n\n        # Calculate metrics as per output specification\n        lam_simp32 = calculate_min_eigenvalue(p_simp32)\n        lam_joseph32 = calculate_min_eigenvalue(p_joseph32)\n        lam_mixed = calculate_min_eigenvalue(p_mixed)\n        \n        sym_err_simp32 = calculate_rel_symmetry_error(p_simp32)\n        sym_err_joseph32 = calculate_rel_symmetry_error(p_joseph32)\n\n        j_pos_simp32 = calculate_positional_jitter(p_simp32)\n\n        case_results = [\n            lam_simp32, lam_joseph32, lam_mixed,\n            sym_err_simp32, sym_err_joseph32,\n            j_pos_simp32\n        ]\n        results.extend(case_results)\n\n    # Format output to match problem specification\n    print(f\"[{','.join(f'{r:.8e}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The Kalman filter is more than just a state estimator; it is a powerful tool for statistical inference and hypothesis testing. This practice moves from parameter estimation to model selection by tackling a classic problem in particle tracking: determining the sign of a particle's electric charge. You will implement two competing filter hypotheses and use the resulting goodness-of-fit statistic, the $\\chi^2$, to make a principled decision. ",
            "id": "3539001",
            "problem": "You are tasked with implementing a numerical study of the sensitivity of charge sign determination in track fitting using the Kalman filter, by comparing innovation sequences from two competing fits initialized with opposite signs of the charge-over-momentum parameter. Work in the transverse $x-y$ plane under a uniform magnetic field $B$ oriented along the $z$ axis. Use physics-based modeling and algorithmic derivation starting from fundamental laws and well-tested formulas, and produce a single, runnable program that carries out the simulation and the fits for a set of specified test cases.\n\nThe fundamental base to use is as follows. The motion of a relativistic charged particle in a uniform magnetic field is governed by the Lorentz force law and results in circular motion in the transverse plane. The curvature is related to the charge-over-momentum by the widely used relation $R = \\frac{p}{0.3\\,|q|\\,B}$ expressed in meters when $B$ is in Tesla and $p$ is in $\\mathrm{GeV}/c$, giving a curvature $\\kappa = \\frac{1}{R} = \\frac{0.3\\,q\\,B}{p}$ in units of $\\mathrm{m}^{-1}$. For multiple scattering in thin materials, use the Highland formula for the root mean square scattering angle $\\theta_0$ in radians: $\\theta_0 \\approx \\frac{13.6\\,\\mathrm{MeV}}{\\beta\\,p\\,c}\\sqrt{\\frac{x}{X_0}}\\left[1 + 0.038 \\ln\\left(\\frac{x}{X_0}\\right)\\right]$, where $\\beta \\approx 1$ for relativistic particles, $p$ is the momentum, $x$ is the thickness, and $X_0$ is the radiation length. In this study, you may assume $\\beta \\approx 1$ and express $13.6\\,\\mathrm{MeV}$ as $0.0136\\,\\mathrm{GeV}$.\n\nDerive a small-angle, linearized kinematic model in the $x$-projection suitable for a Kalman filter state parameterization that includes $q/p$. The state should be represented as $x_k = [y_k, t_k, (q/p)_k]^T$, where $y_k$ is the transverse position in meters, $t_k = \\frac{dy}{dx}$ is the local slope (dimensionless, equal to the tangent of the angle, which under small-angle approximation is approximately an angle in radians), and $(q/p)_k$ is the charge-over-momentum in $(\\mathrm{GeV}/c)^{-1}$. Under a uniform magnetic field $B$, derive the linearized propagation over a step $\\Delta x_k$ and formulate the corresponding state transition model and process noise model informed by multiple scattering as a Gaussian with variance on the slope component derived from the Highland formula. The measurement model consists of noisy measurements of $y$ with Gaussian noise of known variance.\n\nThe Kalman filter should compute the innovation $e_k = z_k - H x_{k|k-1}$ at each layer, the innovation covariance $S_k$, and the whitened innovation sequence $w_k = \\frac{e_k}{\\sqrt{S_k}}$. Define a discrimination statistic for each fit as the sum of squared whitened innovations $J = \\sum_k w_k^2$. Perform two fits initialized with identical prior means and covariances except for the sign of the initial $(q/p)$ component (one positive and one negative of the same magnitude). Decide the charge sign by selecting the fit with the lower $J$. If the absolute difference in $J$ between the two fits is less than a fixed threshold $\\tau$ (which must be chosen and justified based on the expected scale of $J$), resolve the tie by selecting the positive sign.\n\nYou must implement a full simulation of tracks and hits for the following test suite. For each case, generate $N$ measurement layers spaced by $\\Delta x$ along the $x$ axis starting at $x = 0$, with initial transverse position $y_0 = 0\\,\\mathrm{m}$ and initial slope $t_0 = 0$ (angle in radians). In each step, apply the kinematics for the true track parameters, add a Gaussian multiple scattering kick to the slope drawn with standard deviation $\\theta_0$ computed from the Highland formula using the specified $x/X_0$, and finally produce a noisy measurement $z_k$ of $y_k$ with Gaussian noise of standard deviation $\\sigma_{\\mathrm{meas}}$. Angles must be treated in radians, and lengths in meters. The magnetic field must be in Tesla; momentum in $\\mathrm{GeV}/c$.\n\nUse the following test suite parameter sets, with all non-specified physical quantities defaulting to the descriptions above. For each test case, the ground-truth charge sign is specified and must be used to generate the hits; your program should report whether the algorithm’s selected sign matches the ground truth. The multiple scattering fraction $x/X_0$ is per layer.\n\n- Case $1$: $N = 12$, $\\Delta x = 0.05\\,\\mathrm{m}$, $B = 2.0\\,\\mathrm{T}$, $p = 3.0\\,\\mathrm{GeV}/c$, $q = +1$, $\\sigma_{\\mathrm{meas}} = 5.0\\times 10^{-4}\\,\\mathrm{m}$, $x/X_0 = 5.0\\times 10^{-3}$.\n- Case $2$: $N = 10$, $\\Delta x = 0.03\\,\\mathrm{m}$, $B = 3.8\\,\\mathrm{T}$, $p = 1.5\\,\\mathrm{GeV}/c$, $q = -1$, $\\sigma_{\\mathrm{meas}} = 8.0\\times 10^{-4}\\,\\mathrm{m}$, $x/X_0 = 1.0\\times 10^{-2}$.\n- Case $3$: $N = 15$, $\\Delta x = 0.04\\,\\mathrm{m}$, $B = 0.02\\,\\mathrm{T}$, $p = 10.0\\,\\mathrm{GeV}/c$, $q = -1$, $\\sigma_{\\mathrm{meas}} = 1.0\\times 10^{-3}\\,\\mathrm{m}$, $x/X_0 = 5.0\\times 10^{-3}$.\n- Case $4$: $N = 12$, $\\Delta x = 0.06\\,\\mathrm{m}$, $B = 1.0\\,\\mathrm{T}$, $p = 2.0\\,\\mathrm{GeV}/c$, $q = +1$, $\\sigma_{\\mathrm{meas}} = 5.0\\times 10^{-3}\\,\\mathrm{m}$, $x/X_0 = 2.0\\times 10^{-2}$.\n- Case $5$: $N = 3$, $\\Delta x = 0.08\\,\\mathrm{m}$, $B = 2.0\\,\\mathrm{T}$, $p = 4.0\\,\\mathrm{GeV}/c$, $q = -1$, $\\sigma_{\\mathrm{meas}} = 2.0\\times 10^{-3}\\,\\mathrm{m}$, $x/X_0 = 5.0\\times 10^{-3}$.\n\nYour program must:\n\n- Implement physically consistent kinematics derived from the Lorentz force, linearized for small angles, and a Kalman filter model including a process noise term informed by multiple scattering in the slope component.\n- For each test case, simulate the hit sequence and perform two Kalman filter fits initialized with $(q/p)_0 = \\pm |q/p|_{\\mathrm{true}}$ and identical prior covariances; compute the innovation sequences, the corresponding $J$ values, and select the sign based on the smallest $J$ with the tie-break rule for $\\Delta J < \\tau$.\n- Report, for each test case, a boolean indicating whether the selected sign equals the ground-truth sign used in simulation.\n\nAngle units must be radians, magnetic field in Tesla, momentum in $\\mathrm{GeV}/c$, distances in meters. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for instance `[result_1,result_2,...]`, where each `result_i` is a boolean value computed for the corresponding test case.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of particle physics and state estimation, well-posed with a clear objective and sufficient information, and uses objective, formal language. The task is to implement a numerical study of charge sign determination for a tracked particle in a magnetic field using a Kalman filter, a standard and non-trivial problem in computational high-energy physics.\n\nThe solution proceeds by first deriving the necessary state-space model, then specifying the Kalman filter algorithm, outlining the simulation procedure, and finally detailing the charge discrimination methodology.\n\n**1. State-Space Model Derivation**\n\nThe motion of a charged particle in the transverse $x-y$ plane under a uniform magnetic field $B$ along the $z$-axis is analyzed. We adopt a state-space representation suitable for a Kalman filter, where the state is evolved along the $x$-axis.\n\n**State Vector:** The state of the particle at a given plane $x=x_k$ is described by the vector:\n$$\n\\mathbf{x}_k = \\begin{pmatrix} y_k \\\\ t_k \\\\ \\eta_k \\end{pmatrix}\n$$\nwhere $y_k$ is the transverse position in meters, $t_k = \\frac{dy}{dx}$ is the track slope (approximated as the tangent of the pitch angle, $\\tan\\phi_y$), and $\\eta_k = (q/p)_k$ is the signed charge-over-momentum ratio in $(\\mathrm{GeV}/c)^{-1}$.\n\n**State Transition Model (Kinematics):**\nThe Lorentz force law dictates a circular trajectory in the $x-y$ plane. For small angles, where the path length $ds \\approx dx$, the change in slope with respect to $x$ is given by:\n$$\n\\frac{dt}{dx} \\approx \\frac{d^2y}{dx^2} \\approx \\kappa = \\frac{1}{R}\n$$\nwhere $\\kappa$ is the curvature. Using the provided formula, $\\kappa = c_B \\eta$, with the constant $c_B = 0.3 B$. The momentum $p$ is assumed constant (no energy loss), so $\\eta_k$ is constant. The differential equations governing the deterministic motion are:\n$$\n\\frac{dy}{dx} = t(x), \\quad \\frac{dt}{dx} = c_B \\eta, \\quad \\frac{d\\eta}{dx} = 0\n$$\nIntegrating these equations over a step from $x_{k-1}$ to $x_k = x_{k-1} + \\Delta x$ yields the exact propagation for the state variables, assuming $\\eta$ is constant over the step:\n\\begin{align*}\nt_k &= t_{k-1} + \\int_{x_{k-1}}^{x_k} c_B \\eta_{k-1} \\,dx' = t_{k-1} + c_B \\eta_{k-1} \\Delta x \\\\\ny_k &= y_{k-1} + \\int_{x_{k-1}}^{x_k} t(x') \\,dx' = y_{k-1} + \\int_{x_{k-1}}^{x_k} (t_{k-1} + c_B \\eta_{k-1} (x' - x_{k-1})) \\,dx' \\\\\n&= y_{k-1} + t_{k-1} \\Delta x + \\frac{1}{2} c_B \\eta_{k-1} (\\Delta x)^2 \\\\\n\\eta_k &= \\eta_{k-1}\n\\end{align*}\nThis leads to the linearized state transition model $\\mathbf{x}_{k|k-1} = F_k \\mathbf{x}_{k-1|k-1}$, where the state transition matrix $F_k$ (constant for fixed $\\Delta x$) is:\n$$\nF = \\begin{pmatrix}\n1 & \\Delta x & \\frac{1}{2} c_B (\\Delta x)^2 \\\\\n0 & 1 & c_B \\Delta x \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\n\n**Process Noise Model (Multiple Scattering):**\nAs the particle traverses material, it undergoes multiple Coulomb scattering, which is a stochastic process that primarily deflects its angle. This is modeled as process noise. The variance of the projected scattering angle over a step of length $\\Delta x$ is given by the square of the Highland formula, $\\theta_0^2$:\n$$\n\\theta_0^2 = \\left( \\frac{0.0136\\,\\mathrm{GeV}}{p} \\sqrt{\\frac{\\Delta x}{X_0}} \\left[1 + 0.038 \\ln\\left(\\frac{\\Delta x}{X_0}\\right)\\right] \\right)^2\n$$\nwhere $p = |q|/|\\eta| \\approx 1/|\\eta_{k-1|k-1}|$. Treating scattering as a continuous random walk (Wiener process) in angle along the path, the resulting covariance on the state $(y, t)$ over a step $\\Delta x$ can be found by integrating the effects. This yields the process noise covariance matrix $Q_k$:\n$$\nQ_k = \\theta_0^2 \\begin{pmatrix}\n(\\Delta x)^2/3 & \\Delta x/2 & 0 \\\\\n\\Delta x/2 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n$$\nThe full state transition with noise is $\\mathbf{x}_k = F \\mathbf{x}_{k-1} + \\mathbf{w}_{k-1}$, where $\\mathbf{w}_{k-1}$ is a zero-mean Gaussian noise with covariance $Q_k$.\n\n**Measurement Model:**\nThe detectors provide noisy measurements of the transverse position $y_k$.\n$$\nz_k = y_k + \\nu_k, \\quad \\nu_k \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{meas}}^2)\n$$\nIn state-space form, $z_k = H \\mathbf{x}_k + \\nu_k$, with the measurement matrix $H$ and measurement noise covariance $R_k$ given by:\n$$\nH = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}, \\quad R_k = \\sigma_{\\mathrm{meas}}^2\n$$\n\n**2. Kalman Filter Algorithm**\n\nThe filter iteratively predicts the state to the next measurement layer and then updates the prediction using the measurement information. Starting with a prior estimate $\\mathbf{x}_{0|0}$ and its covariance $C_{0|0}$, for each measurement $z_k$ at layer $k=1, \\dots, N$:\n\n1.  **Prediction:**\n    \\begin{align*}\n    \\mathbf{x}_{k|k-1} &= F \\mathbf{x}_{k-1|k-1} \\quad (\\text{Predicted state}) \\\\\n    C_{k|k-1} &= F C_{k-1|k-1} F^T + Q_{k-1} \\quad (\\text{Predicted covariance})\n    \\end{align*}\n    where $Q_{k-1}$ is computed using the momentum estimate from $\\mathbf{x}_{k-1|k-1}$.\n\n2.  **Update:**\n    \\begin{align*}\n    e_k &= z_k - H \\mathbf{x}_{k|k-1} \\quad (\\text{Innovation}) \\\\\n    S_k &= H C_{k|k-1} H^T + R_k \\quad (\\text{Innovation covariance}) \\\\\n    K_k &= C_{k|k-1} H^T S_k^{-1} \\quad (\\text{Kalman gain}) \\\\\n    \\mathbf{x}_{k|k} &= \\mathbf{x}_{k|k-1} + K_k e_k \\quad (\\text{Updated state}) \\\\\n    C_{k|k} &= (I - K_k H) C_{k|k-1} \\quad (\\text{Updated covariance})\n    \\end{align*}\n\n**3. Simulation and Charge Discrimination**\n\nFor each test case, a ground truth track trajectory and corresponding measurements are simulated.\n\n**Simulation:**\n- A true initial state $\\mathbf{x}_{\\mathrm{true},0} = [0, 0, (q/p)_{\\mathrm{true}}]^T$ is defined.\n- For each layer $k=1, \\dots, N$:\n    1. The true state is propagated deterministically using the kinematic model to find the position $y_{\\mathrm{true}, k}$ and slope $t_{\\mathrm{true}, k, \\text{no scatter}}$.\n    2. A random angular kick $\\delta t_k \\sim \\mathcal{N}(0, \\theta_{0, \\mathrm{true}}^2)$ is added to the slope to simulate multiple scattering: $t_{\\mathrm{true}, k} = t_{\\mathrm{true},k, \\text{no scatter}} + \\delta t_k$.\n    3. A measurement $z_k = y_{\\mathrm{true},k} + \\nu_k$ is generated, with $\\nu_k \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{meas}}^2)$.\n\n**Charge Sign Determination:**\nTwo separate Kalman filters are run on the simulated measurement set $\\{z_k\\}_{k=1}^N$:\n- **Fit 1 (Positive Hypothesis):** Initialized with $\\mathbf{x}_{0|0}^+ = [0, 0, |(q/p)_{\\mathrm{true}}|]^T$.\n- **Fit 2 (Negative Hypothesis):** Initialized with $\\mathbf{x}_{0|0}^- = [0, 0, -|(q/p)_{\\mathrm{true}}|]^T$.\n\nBoth fits use an identical initial covariance matrix $C_{0|0}$, chosen to reflect high confidence in the initial position and slope ($y_0=0, t_0=0$) and a moderate uncertainty in the initial momentum guess.\n\nFor each fit, the discrimination statistic $J$, representing the total chi-squared of the fit, is computed as the sum of squared whitened innovations:\n$$\nJ = \\sum_{k=1}^{N} w_k^2 = \\sum_{k=1}^{N} \\frac{e_k^2}{S_k}\n$$\nThe charge sign is determined by comparing $J_+$ and $J_-$:\n- If $|J_+ - J_-| < \\tau$, the ambiguity is resolved by choosing the positive charge sign.\n- Otherwise, the sign corresponding to the lower $J$ value is chosen.\n\nA fixed threshold $\\tau=1.0$ is chosen. This is a common choice corresponding to a $\\Delta\\chi^2=1$ criterion, which relates to a one-standard-deviation confidence interval for a single parameter estimate. It provides a reasonable, albeit not exceptionally strong, threshold to declare two hypotheses as statistically indistinguishable.\n\nThe program then compares the determined sign with the ground-truth sign used in the simulation and reports the outcome as a boolean value for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and Kalman filter fits for all test cases.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of the simulation.\n    np.random.seed(42)\n\n    # Test cases as specified in the problem statement.\n    # Format: (N, dx, B, p, q, sigma_meas, dx/X0)\n    test_cases = [\n        (12, 0.05, 2.0, 3.0, +1, 5.0e-4, 5.0e-3),\n        (10, 0.03, 3.8, 1.5, -1, 8.0e-4, 1.0e-2),\n        (15, 0.04, 0.02, 10.0, -1, 1.0e-3, 5.0e-3),\n        (12, 0.06, 1.0, 2.0, +1, 5.0e-3, 2.0e-2),\n        (3, 0.08, 2.0, 4.0, -1, 2.0e-3, 5.0e-3),\n    ]\n\n    results = []\n    \n    # Threshold for charge-sign ambiguity. Justified as a standard Delta chi^2 = 1 cut.\n    tau = 1.0\n\n    for params in test_cases:\n        _, _, _, p_true, q_true, _, _ = params\n        \n        # 1. Simulate the track to generate measurements.\n        measurements = _simulate_track(params)\n        \n        # 2. Run two Kalman filter fits with opposite charge hypotheses.\n        # Positive charge hypothesis\n        q_p_pos_hyp = abs(q_true / p_true)\n        J_pos = _run_kalman_filter(measurements, params, q_p_pos_hyp)\n\n        # Negative charge hypothesis\n        q_p_neg_hyp = -abs(q_true / p_true)\n        J_neg = _run_kalman_filter(measurements, params, q_p_neg_hyp)\n\n        # 3. Apply the decision logic to determine the charge sign.\n        determined_q = 0\n        if abs(J_pos - J_neg) < tau:\n            determined_q = +1  # Tie-breaking rule: select positive sign\n        elif J_pos < J_neg:\n            determined_q = +1\n        else:\n            determined_q = -1\n        \n        # 4. Compare the determined sign with the ground truth.\n        is_correct = (determined_q == q_true)\n        results.append(is_correct)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\".lower())\n\n\ndef _highland_formula(p_gev, dx_over_X0):\n    \"\"\"\n    Calculates the RMS multiple scattering angle using the Highland formula.\n    \n    Args:\n        p_gev (float): Momentum in GeV/c.\n        dx_over_X0 (float): Material thickness in units of radiation length.\n        \n    Returns:\n        float: Root mean square of the projected scattering angle (theta_0) in radians.\n    \"\"\"\n    if p_gev <= 0: return float('inf')\n    # The term dx_over_X0 is assumed to be > 0.\n    if dx_over_X0 <= 0: return 0.0\n    \n    term_sqrt = np.sqrt(dx_over_X0)\n    term_log = 1.0 + 0.038 * np.log(dx_over_X0) if dx_over_X0 > 0 else 1.0\n    theta0 = (0.0136 / p_gev) * term_sqrt * term_log\n    return theta0\n\n\ndef _simulate_track(params):\n    \"\"\"\n    Generates a sequence of measurements for a particle traversing a detector.\n    This function uses a more precise propagation model than the filter for realism.\n    \n    Args:\n        params (tuple): A tuple containing the simulation parameters.\n        \n    Returns:\n        list: A list of noisy position measurements (z_k).\n    \"\"\"\n    N, dx, B, p_true, q_true, sigma_meas, dx_over_X0 = params\n    eta_true = q_true / p_true\n    C_B = 0.3 * B\n\n    y_true = 0.0\n    t_true = 0.0\n    measurements = []\n    \n    for _ in range(N):\n        # Propagate deterministically (bend in B-field)\n        y_next = y_true + t_true * dx + 0.5 * C_B * eta_true * dx**2\n        t_next_no_scatter = t_true + C_B * eta_true * dx\n\n        # Add multiple scattering kick at the end of the step\n        theta0_true = _highland_formula(p_true, dx_over_X0)\n        delta_t = np.random.normal(0.0, theta0_true)\n        t_next = t_next_no_scatter + delta_t\n\n        # Update true state for the next step\n        y_true, t_true = y_next, t_next\n\n        # Generate a noisy measurement of the position\n        measurement_noise = np.random.normal(0.0, sigma_meas)\n        z = y_true + measurement_noise\n        measurements.append(z)\n\n    return measurements\n\n\ndef _run_kalman_filter(measurements, params, initial_q_over_p):\n    \"\"\"\n    Runs a Kalman filter over a sequence of measurements.\n    \n    Args:\n        measurements (list): The sequence of position measurements.\n        params (tuple): A tuple containing the simulation parameters.\n        initial_q_over_p (float): The initial guess for the q/p state component.\n        \n    Returns:\n        float: The total chi-squared (J) for the fit.\n    \"\"\"\n    N, dx, B, _, _, sigma_meas, dx_over_X0 = params\n    C_B = 0.3 * B\n    \n    # Initial state vector: [y, t, q/p]\n    x_est = np.array([0.0, 0.0, initial_q_over_p])\n\n    # Initial covariance matrix C_0|0 reflects prior knowledge at x=0.\n    # y0=0, t0=0 are known, so their initial variances are small.\n    # The initial q/p guess has a larger uncertainty (e.g., 50% on momentum).\n    sigma_y0_sq = (1e-6)**2\n    sigma_t0_sq = (1e-4)**2\n    sigma_q_p_sq = (0.5 * initial_q_over_p)**2 if initial_q_over_p != 0 else 1.0\n    C_est = np.diag([sigma_y0_sq, sigma_t0_sq, sigma_q_p_sq])\n\n    H = np.array([[1.0, 0.0, 0.0]])\n    R_k = np.array([[sigma_meas**2]])\n    F = np.array([\n        [1.0, dx, 0.5 * C_B * dx**2],\n        [0.0, 1.0, C_B * dx],\n        [0.0, 0.0, 1.0]\n    ])\n\n    J_statistic = 0.0\n\n    for z_k in measurements:\n        # --- PREDICTION STEP ---\n        # Process noise covariance Q depends on the momentum from the previous step.\n        q_p_est = x_est[2]\n        p_est = abs(1.0 / q_p_est) if q_p_est != 0 else float('inf')\n        theta0 = _highland_formula(p_est, dx_over_X0)\n        theta0_sq = theta0**2\n        \n        Q_k_minus_1 = np.zeros((3, 3))\n        if theta0_sq > 0 and not np.isinf(theta0_sq):\n            Q_k_minus_1[0, 0] = theta0_sq * (dx**2) / 3.0\n            Q_k_minus_1[0, 1] = theta0_sq * dx / 2.0\n            Q_k_minus_1[1, 0] = theta0_sq * dx / 2.0\n            Q_k_minus_1[1, 1] = theta0_sq\n            \n        x_pred = F @ x_est\n        C_pred = F @ C_est @ F.T + Q_k_minus_1\n\n        # --- UPDATE STEP ---\n        e_k = z_k - (H @ x_pred)[0]\n        S_k = (H @ C_pred @ H.T + R_k)[0, 0]\n        \n        if S_k <= 0: return float('inf')\n\n        K_k = (C_pred @ H.T) / S_k\n        x_est = x_pred + K_k.flatten() * e_k\n        C_est = (np.eye(3) - K_k @ H) @ C_pred\n        \n        # Accumulate the chi-squared statistic J.\n        w_k_sq = (e_k**2) / S_k\n        J_statistic += w_k_sq\n\n    return J_statistic\n\n# Execute the main function when the script is run.\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}