{
    "hands_on_practices": [
        {
            "introduction": "At high-luminosity colliders, multiple proton-proton interactions can occur in the same event, creating a diffuse background of soft particles known as pileup. This practice  introduces a foundational technique for mitigating this effect: the area-based subtraction method. You will apply the simple but powerful model $p_T^{\\text{corr}} = p_T^{\\text{raw}} - \\rho A$ to correct a jet's transverse momentum and propagate the uncertainty from the measured pileup density, $\\rho$, providing a first hands-on look at the components of jet energy correction.",
            "id": "3519002",
            "problem": "In a high-luminosity proton-proton collision environment, the diffuse soft background from multiple interactions contributes an approximately uniform transverse momentum density per unit area across the rapidity-azimuth plane, denoted by $\\rho$ with units $\\mathrm{GeV}$ per unit area in the $(y, \\phi)$ plane. Consider a reconstructed jet clustered with the anti-$k_T$ algorithm of radius parameter $R=0.4$ whose raw transverse momentum (before any background mitigation) is measured to be $p_{T}^{\\mathrm{raw}}$. The jet’s susceptibility to uniform diffuse background is quantified by its active area $A$ (dimensionless, measured in units of $(\\Delta y)(\\Delta \\phi)$), which characterizes how much background enters the jet on average under uniform density assumptions.\n\nAssume that the uniform background adds linearly to the jet’s transverse momentum such that the expected contribution scales with the jet area and the background density. Under these assumptions:\n- Start from the definition of $\\rho$ as the mean transverse momentum density per unit area and the linear superposition of background and signal contributions to $p_{T}^{\\mathrm{raw}}$.\n- Derive an estimator for the true jet transverse momentum (free of diffuse background), denoted $p_{T}^{\\mathrm{corr}}$, in terms of $p_{T}^{\\mathrm{raw}}$, $\\rho$, and $A$.\n- Assuming the statistical uncertainty in $\\rho$ is Gaussian with standard deviation $\\sigma_{\\rho}$, independent of the jet measurement and $A$, propagate this uncertainty to obtain the standard deviation of $p_{T}^{\\mathrm{corr}}$ due solely to the uncertainty in $\\rho$.\n\nYou are given the following event-level and jet-level quantities for a single jet:\n- $p_{T}^{\\mathrm{raw}} = 247.8~\\mathrm{GeV}$,\n- $A = 0.503$,\n- $\\rho = 18.7~\\mathrm{GeV}$ per unit area,\n- $\\sigma_{\\rho} = 1.6~\\mathrm{GeV}$ per unit area.\n\nCompute the numerical value of $p_{T}^{\\mathrm{corr}}$ and the propagated standard deviation due to $\\sigma_{\\rho}$ alone. Express both the corrected transverse momentum and the propagated standard deviation in $\\mathrm{GeV}$. Round your answers to four significant figures.",
            "solution": "The user has provided a problem in the domain of computational high-energy physics, specifically concerning the correction of jet transverse momentum for background contributions from pileup.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Diffuse soft background transverse momentum density: $\\rho$ ($\\mathrm{GeV}$ per unit area in the $(y,\\phi)$ plane).\n-   Jet clustering algorithm: anti-$k_{T}$ with radius parameter $R=0.4$.\n-   Raw jet transverse momentum: $p_{T}^{\\mathrm{raw}}$.\n-   Jet active area: $A$ (dimensionless, in units of $(\\Delta y)(\\Delta \\phi)$).\n-   Physical Model: The background adds linearly to the jet's transverse momentum, with the contribution being a product of density and area. The raw momentum is a linear superposition: $p_{T}^{\\mathrm{raw}} = p_{T}^{\\mathrm{corr}} + p_{T}^{\\mathrm{bkgd}}$.\n-   Uncertainty in $\\rho$: Gaussian with standard deviation $\\sigma_{\\rho}$, independent of other variables.\n-   Numerical values:\n    -   $p_{T}^{\\mathrm{raw}} = 247.8~\\mathrm{GeV}$\n    -   $A = 0.503$\n    -   $\\rho = 18.7~\\mathrm{GeV}$ per unit area\n    -   $\\sigma_{\\rho} = 1.6~\\mathrm{GeV}$ per unit area\n-   Required outputs:\n    1.  An analytical expression for the corrected jet transverse momentum, $p_{T}^{\\mathrm{corr}}$.\n    2.  An analytical expression for the standard deviation of $p_{T}^{\\mathrm{corr}}$ due to the uncertainty in $\\rho$.\n    3.  Numerical values for $p_{T}^{\\mathrm{corr}}$ and its uncertainty, rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem describes a standard, well-established technique in experimental particle physics for mitigating the effects of pileup (multiple simultaneous proton-proton interactions) on reconstructed jets. The concepts of transverse momentum density $\\rho$, jet active area $A$, and the linear subtraction model are fundamental to this procedure, widely used by experiments at the Large Hadron Collider (LHC). The values provided are physically realistic for jets produced in a high-luminosity environment.\n-   **Well-Posed:** The problem provides a clear physical model and all necessary data to derive the requested expressions and compute the numerical results. It asks for a unique, stable, and meaningful solution.\n-   **Objective:** The language is formal, precise, and free of any subjective or opinion-based statements.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically sound, well-posed, objective, and complete. It is a valid problem statement. I will proceed with the full solution.\n\n### Derivation and Calculation\n\n**Part 1: Derivation of the Corrected Transverse Momentum Estimator**\n\nThe problem states that the raw measured transverse momentum, $p_{T}^{\\mathrm{raw}}$, is the linear sum of the true jet transverse momentum, $p_{T}^{\\mathrm{corr}}$, and the contribution from the uniform background, $p_{T}^{\\mathrm{bkgd}}$.\n$$p_{T}^{\\mathrm{raw}} = p_{T}^{\\mathrm{corr}} + p_{T}^{\\mathrm{bkgd}}$$\nThe background contribution is modeled as the product of the transverse momentum density $\\rho$ and the jet's active area $A$. The units are consistent: $[\\rho] \\times [A] = (\\mathrm{GeV}/\\text{area}) \\times \\text{area} = \\mathrm{GeV}$.\n$$p_{T}^{\\mathrm{bkgd}} = \\rho \\cdot A$$\nSubstituting this into the first equation yields the relationship between the raw and corrected momenta:\n$$p_{T}^{\\mathrm{raw}} = p_{T}^{\\mathrm{corr}} + \\rho \\cdot A$$\nSolving for the corrected transverse momentum, $p_{T}^{\\mathrm{corr}}$, provides the desired estimator:\n$$p_{T}^{\\mathrm{corr}} = p_{T}^{\\mathrm{raw}} - \\rho \\cdot A$$\n\n**Part 2: Propagation of Uncertainty**\n\nWe are asked to find the standard deviation of $p_{T}^{\\mathrm{corr}}$, denoted $\\sigma_{p_{T}^{\\mathrm{corr}}}$, resulting solely from the uncertainty in the background density, $\\sigma_{\\rho}$. For this calculation, $p_{T}^{\\mathrm{raw}}$ and $A$ are treated as constants with zero uncertainty.\n\nThe general formula for the propagation of uncertainty for a function $f(x_1, x_2, \\ldots, x_n)$ with uncorrelated variables $x_i$ having standard deviations $\\sigma_{x_i}$ is:\n$$\\sigma_{f}^{2} = \\sum_{i=1}^{n} \\left(\\frac{\\partial f}{\\partial x_i}\\right)^2 \\sigma_{x_i}^2$$\nIn our case, the function is $p_{T}^{\\mathrm{corr}} = p_{T}^{\\mathrm{corr}}(p_{T}^{\\mathrm{raw}}, \\rho, A)$, and we are only considering the uncertainty in $\\rho$. Thus, the variance of $p_{T}^{\\mathrm{corr}}$ is:\n$$\\sigma_{p_{T}^{\\mathrm{corr}}}^{2} = \\left(\\frac{\\partial p_{T}^{\\mathrm{corr}}}{\\partial \\rho}\\right)^2 \\sigma_{\\rho}^2$$\nWe compute the partial derivative of $p_{T}^{\\mathrm{corr}}$ with respect to $\\rho$:\n$$\\frac{\\partial p_{T}^{\\mathrm{corr}}}{\\partial \\rho} = \\frac{\\partial}{\\partial \\rho} \\left( p_{T}^{\\mathrm{raw}} - \\rho \\cdot A \\right) = -A$$\nSubstituting this back into the variance equation:\n$$\\sigma_{p_{T}^{\\mathrm{corr}}}^{2} = (-A)^2 \\sigma_{\\rho}^2 = A^2 \\sigma_{\\rho}^2$$\nTaking the square root gives the standard deviation. Since the active area $A$ is a physical quantity and must be non-negative, we have $|A|=A$.\n$$\\sigma_{p_{T}^{\\mathrm{corr}}} = \\sqrt{A^2 \\sigma_{\\rho}^2} = A \\cdot \\sigma_{\\rho}$$\n\n**Part 3: Numerical Computation**\n\nWe now substitute the provided numerical values into the derived expressions.\nGiven:\n-   $p_{T}^{\\mathrm{raw}} = 247.8~\\mathrm{GeV}$\n-   $A = 0.503$\n-   $\\rho = 18.7~\\mathrm{GeV}$\n-   $\\sigma_{\\rho} = 1.6~\\mathrm{GeV}$\n\nFirst, we calculate the corrected transverse momentum, $p_{T}^{\\mathrm{corr}}$:\n$$p_{T}^{\\mathrm{corr}} = p_{T}^{\\mathrm{raw}} - \\rho \\cdot A = 247.8 - (18.7 \\cdot 0.503)$$\n$$p_{T}^{\\mathrm{corr}} = 247.8 - 9.4061 = 238.3939$$\nRounding to four significant figures, we get:\n$$p_{T}^{\\mathrm{corr}} \\approx 238.4~\\mathrm{GeV}$$\n\nNext, we calculate the propagated uncertainty, $\\sigma_{p_{T}^{\\mathrm{corr}}}$:\n$$\\sigma_{p_{T}^{\\mathrm{corr}}} = A \\cdot \\sigma_{\\rho} = 0.503 \\cdot 1.6$$\n$$\\sigma_{p_{T}^{\\mathrm{corr}}} = 0.8048$$\nThis value already has four significant figures. The units are $\\mathrm{GeV}$.\n$$\\sigma_{p_{T}^{\\mathrm{corr}}} = 0.8048~\\mathrm{GeV}$$\n\nThe final numerical results for the corrected transverse momentum and its standard deviation due to the uncertainty in $\\rho$ are $238.4~\\mathrm{GeV}$ and $0.8048~\\mathrm{GeV}$, respectively.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n238.4  0.8048\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond pileup, a jet's measured energy is subject to complex dependencies on its kinematics and particle composition, arising from detector geometry and the non-compensating nature of calorimeters. This comprehensive exercise  simulates the full workflow of building a multi-dimensional jet energy calibration to correct for these effects. By working with a detailed generative model, you will fit a non-linear correction function $C(\\eta, p_T)$ and validate its performance, gaining practical experience in the central task of a jet calibration expert.",
            "id": "3519032",
            "problem": "You are tasked with constructing and validating a computational calibration for jets in a non-compensating calorimeter where the electromagnetic-to-hadronic response ratio satisfies $e/h \\neq 1$. The ultimate objective is to find and fit a non-linear multiplicative correction function $C(\\eta,p_T)$ such that the corrected response $R(E) = E_{\\text{meas}}/E_{\\text{true}}$ is flattened within $\\pm 0.01$ across particle species and energies. Although this is inspired by the Geometry and Tracking 4 (GEANT4) simulation environment, you must work with a self-contained parametric generative model that mimics the salient features of non-compensating calorimeter behavior and then perform a numerical fit. All energies must be expressed in $\\text{GeV}$, pseudorapidity $\\eta$ is dimensionless, and transverse momentum $p_T$ is in $\\text{GeV}$. The final outputs are dimensionless booleans indicating whether the flattening criterion is met.\n\nFoundational modeling assumptions and generative model:\n- Begin from the standard jet kinematics relation for massless jets, $E_{\\text{true}} = p_T \\cosh(\\eta)$, which follows from the definition of transverse momentum and pseudorapidity for highly relativistic particles.\n- In a non-compensating calorimeter with $e/h = r$, the baseline electromagnetic/hadronic response to a single jet with electromagnetic energy fraction $f_{\\text{em}}$ is modeled as $R_{\\text{base}}(f_{\\text{em}};r) = f_{\\text{em}} + \\frac{1 - f_{\\text{em}}}{r}$, a widely used phenomenological representation derived from linear superposition of subshowers weighted by their respective response constants.\n- The calorimeter response also exhibits pseudorapidity and transverse-momentum dependencies as well as a mild energy nonlinearity. Define the following dimensionless multiplicative factors, each bounded and smooth, to emulate realistic detector non-idealities:\n  1. Pseudorapidity dependence: $S(\\eta) = 1 + a_2 |\\eta|^2 + a_4 |\\eta|^4$.\n  2. Transverse-momentum dependence: $G(p_T) = 1 - \\frac{b_1}{1 + p_T/p_0} + \\frac{b_2}{1 + (p_T/p_1)^2}$.\n  3. Energy nonlinearity: $N(E_{\\text{true}}) = 1 + n_1 \\frac{\\ln(E_{\\text{true}}/E_0)}{1 + [\\ln(E_{\\text{true}}/E_0)]^2}$.\n- The measured energy is then $E_{\\text{meas}} = E_{\\text{true}} \\cdot R_{\\text{base}}(f_{\\text{em}};r) \\cdot S(\\eta) \\cdot G(p_T) \\cdot N(E_{\\text{true}}) \\cdot \\epsilon$, where $\\epsilon$ encodes stochastic smearing with a resolution that follows the standard hadronic calorimeter parameterization:\n  $$\\frac{\\sigma}{E_{\\text{true}}} = \\sqrt{\\left(\\frac{S_{\\text{res}}}{\\sqrt{E_{\\text{true}}}}\\right)^2 + \\left(\\frac{N_{\\text{res}}}{E_{\\text{true}}}\\right)^2 + C_{\\text{res}}^2},$$\n  and the smearing is applied as a multiplicative factor $\\epsilon = 1 + \\delta$, with $\\delta \\sim \\mathcal{N}(0, \\sigma/E_{\\text{true}})$.\n\nElectromagnetic fraction model:\n- Consider three jet species denoted $s \\in \\{\\text{quark}, \\text{gluon}, \\text{charged}\\}$, representing light-quark jets, gluon jets, and jets with a high fraction of charged hadrons. Their mean electromagnetic fractions depend on $p_T$ via\n  $$\\mu_s(p_T) = \\mu_{0,s} + \\alpha \\tanh\\left(\\ln\\left(\\frac{p_T}{p_*}\\right)\\right),$$\n  where $\\mu_{0,\\text{quark}} = 0.35$, $\\mu_{0,\\text{gluon}} = 0.30$, $\\mu_{0,\\text{charged}} = 0.25$, $\\alpha = 0.12$, and $p_* = 50\\,\\text{GeV}$. For realism, $f_{\\text{em}}$ is drawn from a Beta distribution with mean $\\mu_s(p_T)$ and concentration $\\kappa = 20$, i.e., parameters $a = \\mu_s(p_T)\\kappa$ and $b = (1 - \\mu_s(p_T))\\kappa$, then clamped to $[0.01, 0.99]$ to avoid degeneracy.\n\nFixed physical and model parameters:\n- Non-compensation ratio: $r = 1.20$.\n- Pseudorapidity response parameters: $a_2 = 0.015$, $a_4 = 0.001$.\n- Transverse-momentum response parameters: $b_1 = 0.03$, $p_0 = 30\\,\\text{GeV}$, $b_2 = 0.02$, $p_1 = 100\\,\\text{GeV}$.\n- Energy nonlinearity parameters: $n_1 = 0.02$, $E_0 = 50\\,\\text{GeV}$.\n- Resolution parameters: $S_{\\text{res}} = 0.8$, $N_{\\text{res}} = 1.0\\,\\text{GeV}$, $C_{\\text{res}} = 0.03$.\n\nCalibration objective:\n- You must fit a strictly positive correction function $C(\\eta, p_T)$ that depends only on $\\eta$ and $p_T$, defined through an exponential of a polynomial basis to ensure positivity:\n  $$C(\\eta, p_T) = \\exp\\left(\\theta_0 + \\theta_1 |\\eta| + \\theta_2 |\\eta|^2 + \\theta_3 \\ln p_T + \\theta_4 (\\ln p_T)^2 + \\theta_5 |\\eta| \\ln p_T + \\theta_6 |\\eta|^2 \\ln p_T\\right).$$\n- Given a training sample simulated from the generative model above, determine the parameters $\\{\\theta_i\\}_{i=0}^6$ by minimizing the sum of squared residuals in the relative response,\n  $$\\mathcal{L}(\\theta) = \\sum_{j} \\left(\\frac{C(\\eta_j, p_{T,j})\\,E_{\\text{meas},j}}{E_{\\text{true},j}} - 1\\right)^2.$$\n\nValidation and test suite:\n- After fitting $C(\\eta,p_T)$ on a sufficiently large training set, evaluate performance on the following scenarios. Each scenario defines a sampling domain for $(\\eta, p_T)$ and species mixture, from which a validation sample is drawn. For each scenario, compute the corrected response $R_{\\text{corr}} = \\frac{C(\\eta,p_T)\\,E_{\\text{meas}}}{E_{\\text{true}}}$, bin in $E_{\\text{true}}$ over the edges $\\{20,50,100,200,400,800,1600,3200\\}$ in $\\text{GeV}$, and for each species compute the mean response in each bin. Define the worst-case absolute deviation as $\\max_{s,\\text{bins}} | \\langle R_{\\text{corr}} \\rangle_{s,\\text{bin}} - 1 |$. The scenario passes if this maximum is less than or equal to $0.01$.\n- Scenarios (energies in $\\text{GeV}$):\n  1. Scenario $1$ (general coverage): $\\eta \\sim \\mathcal{U}(-2.5, 2.5)$, $p_T \\sim \\mathcal{U}(20, 1000)$, species probabilities $(0.4, 0.4, 0.2)$, sample size $5000$.\n  2. Scenario $2$ (central, wide energy): $\\eta = 0$, $p_T \\sim \\mathcal{U}(20, 2000)$, species probabilities $(0.5, 0.3, 0.2)$, sample size $3000$.\n  3. Scenario $3$ (forward region): $\\eta$ uniformly in two bands with equal probability, either $\\mathcal{U}(2.0, 2.5)$ or $\\mathcal{U}(-2.5, -2.0)$, $p_T \\sim \\mathcal{U}(50, 800)$, species probabilities $(0.3, 0.5, 0.2)$, sample size $4000$.\n  4. Scenario $4$ (low $p_T$ stress): $\\eta \\sim \\mathcal{U}(-1.0, 1.0)$, $p_T \\sim \\mathcal{U}(20, 40)$, species probabilities $(0.2, 0.3, 0.5)$, sample size $3000$.\n  5. Scenario $5$ (ultra-high energy): $\\eta \\sim \\mathcal{U}(-1.5, 1.5)$, $p_T \\sim \\mathcal{U}(1000, 3000)$, species probabilities $(0.4, 0.4, 0.2)$, sample size $3000$.\n\nProgram requirements and final output:\n- Implement the generative model, perform the fit of $C(\\eta, p_T)$ by minimizing $\\mathcal{L}(\\theta)$ over a training set with $6000$ jets drawn from $\\eta \\sim \\mathcal{U}(-2.5, 2.5)$ and $p_T \\sim \\mathcal{U}(20, 1500)$ with species probabilities $(0.4,0.4,0.2)$.\n- Use a deterministic random seed for reproducibility.\n- For each of the $5$ scenarios above, compute whether the worst-case absolute deviation is $\\le 0.01$ and report the booleans.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true,false,true]\"), using Python boolean literals \"True\" and \"False\".\n\nAll mathematical entities in this problem statement have been written in LaTeX. Ensure all numerical quantities and variables in your solution are treated consistently with these definitions. The final results are dimensionless booleans. Energies must be expressed in $\\text{GeV}$, pseudorapidity $\\eta$ is dimensionless, and $p_T$ is in $\\text{GeV}$.",
            "solution": "The problem presented is a well-posed and scientifically grounded exercise in computational high-energy physics, specifically focusing on the paramount task of jet energy calibration. The provided generative model, while parametric, encapsulates the essential physical phenomena governing the response of a non-compensating hadronic calorimeter. These include the difference in response to electromagnetic and hadronic shower components ($e/h \\neq 1$), dependencies on jet kinematics ($\\eta$, $p_T$), a mild energy non-linearity, and stochastic resolution effects. The objective is to derive and validate a multiplicative correction function $C(\\eta, p_T)$ to restore response uniformity and linearity. The problem is valid as it is self-contained, logically consistent, and rooted in established phenomenological models.\n\nOur solution methodology is structured into three principle-based stages:\n1.  **Generative Model Implementation**: We first construct a computational realization of the detector and jet physics as specified. This involves creating functions that produce simulated jet events, where the measured energy $E_{\\text{meas}}$ is derived from the true energy $E_{\\text{true}}$ according to the chain of specified physical effects.\n2.  **Calibration via Numerical Optimization**: We then address the core task of determining the parameters of the correction function $C(\\eta, p_T)$. The functional form is given as $C(\\eta, p_T) = \\exp(\\boldsymbol{X} \\boldsymbol{\\theta})$, where $\\boldsymbol{X}$ is a design matrix constructed from a polynomial basis in $|\\eta|$ and $\\ln p_T$, and $\\boldsymbol{\\theta}$ is the vector of unknown parameters $\\{\\theta_i\\}_{i=0}^6$. To find the optimal parameters, we minimize the specified loss function, $\\mathcal{L}(\\boldsymbol{\\theta}) = \\sum_{j} \\left(C(\\eta_j, p_{T,j}) \\cdot E_{\\text{meas},j}/E_{\\text{true},j} - 1\\right)^2$, using a training sample of simulated jets. This is a non-linear least-squares problem. A robust approach, which we adopt, is to employ an iterative numerical solver. Specifically, we will use the Levenberg-Marquardt algorithm as implemented in `scipy.optimize.least_squares` to directly minimize $\\mathcal{L}(\\boldsymbol{\\theta})$ by finding the `theta` that minimizes the sum of squares of the residuals $R_{\\text{corr},j} - 1$.\n3.  **Systematic Validation**: Once the parameters $\\boldsymbol{\\theta}$ are determined, the efficacy of the calibration must be rigorously assessed. We generate independent validation datasets for five distinct kinematic and jet-population scenarios. For each scenario, we apply the derived correction $C(\\eta, p_T)$ to obtain the corrected response $R_{\\text{corr}} = (C \\cdot E_{\\text{meas}}) / E_{\\text{true}}$. We then bin the data in true energy $E_{\\text{true}}$ and, for each jet species, calculate the mean of $R_{\\text{corr}}$ in each bin. The calibration is deemed successful for a given scenario if the maximal deviation of these mean responses from unity, $\\max_{s,\\text{bins}} | \\langle R_{\\text{corr}} \\rangle_{s,\\text{bin}} - 1 |$, does not exceed the required tolerance of $0.01$.\n\nThe entire process, from data generation to final validation, is performed computationally. A deterministic random seed is used to ensure the reproducibility of the pseudo-random processes involved, such as the sampling of jet kinematics, the fluctuation of the electromagnetic fraction $f_{\\text{em}}$ (modeled by a Beta distribution), and the Gaussian smearing of the final measured energy.\n\nLet us detail the mathematical components. The true jet energy is $E_{\\text{true}} = p_T \\cosh(\\eta)$. The uncorrected measured energy is a product of factors:\n$$ E_{\\text{meas}} = E_{\\text{true}} \\cdot \\underbrace{\\left(f_{\\text{em}} + \\frac{1 - f_{\\text{em}}}{r}\\right)}_{R_{\\text{base}}} \\cdot \\underbrace{\\left(1 + a_2 |\\eta|^2 + a_4 |\\eta|^4\\right)}_{S(\\eta)} \\cdot \\underbrace{\\left(1 - \\frac{b_1}{1 + p_T/p_0} + \\frac{b_2}{1 + (p_T/p_1)^2}\\right)}_{G(p_T)} \\cdot \\underbrace{\\left(1 + n_1 \\frac{\\ln(E_{\\text{true}}/E_0)}{1 + [\\ln(E_{\\text{true}}/E_0)]^2}\\right)}_{N(E_{\\text{true}})} \\cdot \\underbrace{(1 + \\delta)}_{\\epsilon} $$\nwhere the smearing $\\delta$ is drawn from a normal distribution $\\mathcal{N}(0, \\sigma/E_{\\text{true}})$, with the resolution given by:\n$$ \\frac{\\sigma}{E_{\\text{true}}} = \\sqrt{\\left(\\frac{S_{\\text{res}}}{\\sqrt{E_{\\text{true}}}}\\right)^2 + \\left(\\frac{N_{\\text{res}}}{E_{\\text{true}}}\\right)^2 + C_{\\text{res}}^2} $$\nThe electromagnetic fraction $f_{\\text{em}}$ for a jet of species $s$ is drawn from a Beta distribution whose mean $\\mu_s(p_T)$ varies with transverse momentum.\n\nThe fitting procedure finds the parameter vector $\\boldsymbol{\\theta} = (\\theta_0, \\theta_1, ..., \\theta_6)^T$ for the correction function:\n$$ C(\\eta, p_T) = \\exp\\left( \\begin{pmatrix} 1  |\\eta|  |\\eta|^2  \\ln p_T  (\\ln p_T)^2  |\\eta|\\ln p_T  |\\eta|^2\\ln p_T \\end{pmatrix} \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_6 \\end{pmatrix} \\right) $$\nThis function is designed to invert, on average, the effects of $R_{\\text{base}}$, $S(\\eta)$, $G(p_T)$, and $N(E_{\\text{true}})$. Crucially, $C(\\eta, p_T)$ does not have access to the event-by-event $f_{\\text{em}}$ or the true energy $E_{\\text{true}}$ (which determines $N(E_{\\text{true}})$), mimicking a real experimental constraint. The fit must therefore find an optimal average correction over the distributions of these hidden variables. The final output is an array of booleans, one for each validation scenario, indicating whether this complex, multi-faceted calibration meets the stringent flatness criterion.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import least_squares\n\ndef solve():\n    \"\"\"\n    Implements the full chain of jet calibration: data generation,\n    fitting a correction function, and validating against multiple scenarios.\n    \"\"\"\n    # Use a fixed seed for reproducibility as required.\n    SEED = 42\n    RNG = np.random.default_rng(SEED)\n\n    # --- Fixed physical and model parameters ---\n    # Non-compensation ratio\n    R_EH = 1.20\n    # Pseudorapidity response parameters\n    A2, A4 = 0.015, 0.001\n    # Transverse-momentum response parameters\n    B1, P0, B2, P1 = 0.03, 30.0, 0.02, 100.0\n    # Energy nonlinearity parameters\n    N1, E0 = 0.02, 50.0\n    # Resolution parameters\n    S_RES, N_RES, C_RES = 0.8, 1.0, 0.03\n    # Electromagnetic fraction model parameters\n    MU0_MAP = {'quark': 0.35, 'gluon': 0.30, 'charged': 0.25}\n    SPECIES_ID_MAP = {'quark': 0, 'gluon': 1, 'charged': 2}\n    ID_SPECIES_MAP = {v: k for k, v in SPECIES_ID_MAP.items()}\n    ALPHA, P_STAR = 0.12, 50.0\n    KAPPA = 20.0\n\n    # --- Generative Model Functions ---\n    def generate_jets(n_jets, eta_config, pt_config, species_probs):\n        \"\"\"Generates a sample of jets based on specified distributions.\"\"\"\n        \n        # 1. Sample Kinematics and Species\n        if eta_config[0] == 'uniform':\n            eta = RNG.uniform(eta_config[1], eta_config[2], n_jets)\n        elif eta_config[0] == 'fixed':\n            eta = np.full(n_jets, eta_config[1])\n        elif eta_config[0] == 'bands':\n            # 50% probability for each band\n            # u ~ U(0,1), if u  0.5 sample from band 1, else from band 2\n            low_band_mask = RNG.random(n_jets)  0.5\n            n_low_band = np.sum(low_band_mask)\n            n_high_band = n_jets - n_low_band\n            eta = np.zeros(n_jets)\n            eta[low_band_mask] = RNG.uniform(eta_config[1][0], eta_config[1][1], n_low_band)\n            eta[~low_band_mask] = RNG.uniform(eta_config[2][0], eta_config[2][1], n_high_band)\n\n        pT = RNG.uniform(pt_config[1], pt_config[2], n_jets)\n        \n        species_ids = RNG.choice(len(species_probs), n_jets, p=species_probs)\n        \n        # 2. Calculate Derived Quantities\n        E_true = pT * np.cosh(eta)\n\n        # 3. Model Electromagnetic Fraction (f_em)\n        mu0_vals = np.array([MU0_MAP[ID_SPECIES_MAP[sid]] for sid in species_ids])\n        mu_s = mu0_vals + ALPHA * np.tanh(np.log(pT / P_STAR))\n        \n        # Beta distribution parameters a, b\n        a = mu_s * KAPPA\n        b = (1 - mu_s) * KAPPA\n        f_em = RNG.beta(a, b)\n        f_em = np.clip(f_em, 0.01, 0.99)\n\n        # 4. Calculate Detector Response Components\n        R_base = f_em + (1 - f_em) / R_EH\n        S_eta = 1 + A2 * np.abs(eta)**2 + A4 * np.abs(eta)**4\n        G_pt = 1 - B1 / (1 + pT / P0) + B2 / (1 + (pT / P1)**2)\n        log_E_term = np.log(E_true / E0)\n        N_Etrue = 1 + N1 * log_E_term / (1 + log_E_term**2)\n        \n        # 5. Calculate Measured Energy with Stochastic Smearing\n        sigma_over_E = np.sqrt(\n            (S_RES / np.sqrt(E_true))**2 + (N_RES / E_true)**2 + C_RES**2\n        )\n        epsilon = RNG.normal(loc=1.0, scale=sigma_over_E)\n        \n        E_meas = E_true * R_base * S_eta * G_pt * N_Etrue * epsilon\n\n        return {\n            'eta': eta, 'pT': pT, 'E_true': E_true, \n            'E_meas': E_meas, 'species_id': species_ids\n        }\n\n    # --- Calibration and Validation Functions ---\n    def build_design_matrix(eta, pT):\n        \"\"\"Constructs the design matrix X for the linear model in the exponent.\"\"\"\n        abs_eta = np.abs(eta)\n        log_pt = np.log(pT)\n        return np.stack([\n            np.ones_like(pT),\n            abs_eta,\n            abs_eta**2,\n            log_pt,\n            log_pt**2,\n            abs_eta * log_pt,\n            abs_eta**2 * log_pt\n        ], axis=1)\n\n    def fit_calibration(training_data):\n        \"\"\"Fits the calibration parameters theta.\"\"\"\n        X = build_design_matrix(training_data['eta'], training_data['pT'])\n        r = training_data['E_meas'] / training_data['E_true']\n\n        def residuals(theta, X, r):\n            \"\"\"Residuals for the non-linear least squares fit.\"\"\"\n            corrections = np.exp(X @ theta)\n            R_corr = corrections * r\n            return R_corr - 1\n\n        theta_initial = np.zeros(X.shape[1])\n        result = least_squares(residuals, theta_initial, args=(X, r), method='lm')\n        return result.x\n\n    def validate_scenario(theta, scenario):\n        \"\"\"Validates the calibration for a given scenario.\"\"\"\n        val_data = generate_jets(\n            scenario['n_jets'], scenario['eta_config'], \n            scenario['pt_config'], scenario['species_probs']\n        )\n        \n        X_val = build_design_matrix(val_data['eta'], val_data['pT'])\n        corrections = np.exp(X_val @ theta)\n        R_corr = corrections * val_data['E_meas'] / val_data['E_true']\n        \n        E_bins = np.array([20, 50, 100, 200, 400, 800, 1600, 3200])\n        bin_indices = np.digitize(val_data['E_true'], E_bins)\n\n        max_deviation = 0.0\n        \n        for species_id in range(len(scenario['species_probs'])):\n            species_mask = (val_data['species_id'] == species_id)\n            for i in range(1, len(E_bins)):\n                bin_mask = (bin_indices == i)\n                full_mask = species_mask  bin_mask\n                \n                if np.any(full_mask):\n                    mean_response = np.mean(R_corr[full_mask])\n                    deviation = np.abs(mean_response - 1.0)\n                    if deviation  max_deviation:\n                        max_deviation = deviation\n        \n        return max_deviation = 0.01\n\n    # --- Main Execution ---\n    # 1. Generate Training Data and Fit Calibration\n    training_params = {\n        'n_jets': 6000,\n        'eta_config': ('uniform', -2.5, 2.5),\n        'pt_config': ('uniform', 20, 1500),\n        'species_probs': (0.4, 0.4, 0.2) # quark, gluon, charged\n    }\n    training_data = generate_jets(\n        training_params['n_jets'], training_params['eta_config'],\n        training_params['pt_config'], training_params['species_probs']\n    )\n    theta_fitted = fit_calibration(training_data)\n\n    # 2. Define and Run Validation Scenarios\n    scenarios = [\n        {'n_jets': 5000, 'eta_config': ('uniform', -2.5, 2.5), 'pt_config': ('uniform', 20, 1000), 'species_probs': (0.4, 0.4, 0.2)},\n        {'n_jets': 3000, 'eta_config': ('fixed', 0), 'pt_config': ('uniform', 20, 2000), 'species_probs': (0.5, 0.3, 0.2)},\n        {'n_jets': 4000, 'eta_config': ('bands', (-2.5, -2.0), (2.0, 2.5)), 'pt_config': ('uniform', 50, 800), 'species_probs': (0.3, 0.5, 0.2)},\n        {'n_jets': 3000, 'eta_config': ('uniform', -1.0, 1.0), 'pt_config': ('uniform', 20, 40), 'species_probs': (0.2, 0.3, 0.5)},\n        {'n_jets': 3000, 'eta_config': ('uniform', -1.5, 1.5), 'pt_config': ('uniform', 1000, 3000), 'species_probs': (0.4, 0.4, 0.2)}\n    ]\n    \n    results = []\n    for scenario in scenarios:\n        passed = validate_scenario(theta_fitted, scenario)\n        results.append(passed)\n\n    # 3. Format and Print Final Output\n    # The output should be Python boolean literals 'True' or 'False'\n    result_str = ','.join(map(str, results))\n    print(f\"[{result_str}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "A calibrated jet energy scale is only useful for physics analysis when accompanied by a robust estimate of its systematic uncertainties. These uncertainties are typically correlated across different kinematic regions, making them complex to handle. This final practice  explores the standard method for managing this complexity: using Principal Component Analysis (PCA) to decompose the uncertainty covariance matrix into a set of orthogonal nuisance parameters, a crucial technique for publishing experimental results.",
            "id": "3518981",
            "problem": "Consider a binned Jet Energy Scale (JES) calibration problem in which jet response uncertainties over bins indexed by pseudorapidity and transverse momentum are modeled as a zero-mean multivariate Gaussian with covariance matrix $C$. The goal is to construct an orthogonal basis of nuisance parameter templates that reproduces $C$ and minimizes correlations in Gaussian-constrained fits. Use Principal Component Analysis (PCA) on $C$ to obtain an orthogonal set of eigenvectors and non-negative eigenvalues, and define nuisance templates as columns of a template matrix $T$.\n\nStarting from the following fundamental definitions and facts:\n- A covariance matrix $C$ of a real-valued random vector is symmetric and positive semidefinite by construction.\n- Principal Component Analysis (PCA) diagonalizes a symmetric positive semidefinite matrix $C$ via $C = E \\Lambda E^\\top$, where $E$ has orthonormal columns of eigenvectors and $\\Lambda$ is a diagonal matrix of eigenvalues sorted in descending order.\n- In a Gaussian-constrained linear fit with prior covariance $C$, nuisance templates that are orthogonal with respect to the metric induced by $C^{-1}$ minimize parameter correlations.\n\nDerive mathematically how to construct the template matrix $T$ from the PCA of $C$ so that:\n- The covariance is exactly reproduced by the template outer products, i.e., $C = T T^\\top$.\n- The templates are orthonormal with respect to the $C^{-1}$-metric, i.e., $T^\\top C^{-1} T = I$, where $I$ is the identity matrix.\n\nThen, implement a program that for each provided test covariance matrix:\n- Performs PCA to obtain eigenvalues $\\{\\lambda_i\\}$ and corresponding orthonormal eigenvectors $\\{e_i\\}$, ordered such that $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots$.\n- Constructs the template matrix $T$ with columns $t_i = \\sqrt{\\lambda_i}\\, e_i$.\n- Computes the following three scalar diagnostics:\n  1. $m_1$: the maximum absolute off-diagonal element of $G = T^\\top C^{-1} T$.\n  2. $m_2$: the relative Frobenius reconstruction error, $m_2 = \\dfrac{\\lVert C - T T^\\top \\rVert_F}{\\lVert C \\rVert_F}$.\n  3. $m_3$: the fraction of total variance explained by the leading component, $m_3 = \\dfrac{\\lambda_1}{\\operatorname{tr}(C)}$.\n\nAll quantities are dimensionless, so no physical unit is required. Angles are not involved. Fractions must be expressed as decimals.\n\nTest Suite:\nUse the following three covariance matrices as the test suite. Each $C^{(k)}$ is symmetric and positive definite, with entries given explicitly.\n\n- Test case $1$:\n$$\nC^{(1)} =\n\\begin{pmatrix}\n0.04  0.018  0  0 \\\\\n0.018  0.09  0  0 \\\\\n0  0  0.0225  0.015 \\\\\n0  0  0.015  0.04\n\\end{pmatrix}.\n$$\n\n- Test case $2$:\n$$\nC^{(2)} =\n\\begin{pmatrix}\n0.02001  0.016  0.012  0.008  0.004 \\\\\n0.016  0.02281  0.0096  0.0164  0.0032 \\\\\n0.012  0.0096  0.00721  0.0048  0.0024 \\\\\n0.008  0.0164  0.0048  0.01321  0.0016 \\\\\n0.004  0.0032  0.0024  0.0016  0.00081\n\\end{pmatrix}.\n$$\n\n- Test case $3$:\n$$\nC^{(3)} =\n\\begin{pmatrix}\n0.01  0  0 \\\\\n0  0.04  0 \\\\\n0  0  0.09\n\\end{pmatrix}.\n$$\n\nAnswer Specification:\n- For each test case $k \\in \\{1,2,3\\}$, compute $(m_1^{(k)}, m_2^{(k)}, m_3^{(k)})$ as defined above.\n- Your program should produce a single line of output containing the nine floating-point results in the following fixed order as a comma-separated list enclosed in square brackets:\n$[m_1^{(1)}, m_2^{(1)}, m_3^{(1)}, m_1^{(2)}, m_2^{(2)}, m_3^{(2)}, m_1^{(3)}, m_2^{(3)}, m_3^{(3)}]$.\n- Each float should be rounded to $6$ decimal places.",
            "solution": "The user-provided problem has been analyzed and is deemed **valid**. It is scientifically grounded, well-posed, objective, and contains all necessary information for a complete solution.\n\nThe problem consists of two parts: a mathematical derivation and a computational implementation.\n\n### Part 1: Mathematical Derivation\n\nThe objective is to construct a template matrix $T$ from the Principal Component Analysis (PCA) of a covariance matrix $C$, such that two conditions are met:\n1. The covariance is perfectly reproduced: $C = T T^\\top$.\n2. The templates are orthonormal with respect to the metric induced by $C^{-1}$: $T^\\top C^{-1} T = I$.\n\nLet $C$ be an $n \\times n$ real, symmetric, positive semidefinite covariance matrix. The spectral theorem guarantees that $C$ can be diagonalized by an orthogonal matrix. This is the foundation of PCA. The decomposition is given by:\n$$C = E \\Lambda E^\\top$$\nwhere:\n- $E$ is an $n \\times n$ orthogonal matrix whose columns, $e_i$, are the orthonormal eigenvectors of $C$. The orthogonality condition implies $E^\\top E = E E^\\top = I$, where $I$ is the $n \\times n$ identity matrix.\n- $\\Lambda$ is an $n \\times n$ diagonal matrix containing the real, non-negative eigenvalues of $C$, $\\lambda_i$, corresponding to the eigenvectors $e_i$. We assume they are sorted in descending order, $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$.\n$$\\Lambda = \\operatorname{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)$$\n\nThe problem defines the nuisance parameter templates, $t_i$, as the columns of the template matrix $T$. This construction is given as $t_i = \\sqrt{\\lambda_i} \\, e_i$. In matrix form, this can be expressed as:\n$$T = E \\Lambda^{1/2}$$\nwhere $\\Lambda^{1/2}$ is a diagonal matrix with entries $\\sqrt{\\lambda_i}$:\n$$\\Lambda^{1/2} = \\operatorname{diag}(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}, \\dots, \\sqrt{\\lambda_n})$$\n\nWe will now verify that this construction of $T$ satisfies the two required conditions.\n\n**Condition 1: Covariance Reproduction ($C = T T^\\top$)**\n\nWe compute the outer product $T T^\\top$ using the matrix form of $T$:\n$$T T^\\top = (E \\Lambda^{1/2}) (E \\Lambda^{1/2})^\\top$$\nUsing the property of the transpose of a product, $(AB)^\\top = B^\\top A^\\top$, we get:\n$$T T^\\top = (E \\Lambda^{1/2}) ((\\Lambda^{1/2})^\\top E^\\top)$$\nSince $\\Lambda^{1/2}$ is a diagonal matrix, it is symmetric, so $(\\Lambda^{1/2})^\\top = \\Lambda^{1/2}$. The expression becomes:\n$$T T^\\top = E \\Lambda^{1/2} \\Lambda^{1/2} E^\\top = E \\Lambda E^\\top$$\nFrom the initial PCA decomposition, we know that $E \\Lambda E^\\top = C$. Therefore, we have proven:\n$$C = T T^\\top$$\nThis demonstrates that the covariance matrix is exactly reproduced.\n\n**Condition 2: Orthonormality ($T^\\top C^{-1} T = I$)**\n\nTo verify this condition, we first need an expression for the inverse of the covariance matrix, $C^{-1}$. The problem statement specifies that the test matrices are positive definite, which means all eigenvalues $\\lambda_i$ are strictly positive, and thus $C$ is invertible.\nStarting from $C = E \\Lambda E^\\top$, we can find the inverse:\n$$C^{-1} = (E \\Lambda E^\\top)^{-1} = (E^\\top)^{-1} \\Lambda^{-1} E^{-1}$$\nSince $E$ is an orthogonal matrix, its inverse is its transpose, $E^{-1} = E^\\top$. Consequently, $(E^\\top)^{-1} = (E^{-1})^{-1} = E$. The inverse of the diagonal matrix $\\Lambda$ is $\\Lambda^{-1} = \\operatorname{diag}(1/\\lambda_1, 1/\\lambda_2, \\dots, 1/\\lambda_n)$.\nSubstituting these into the expression for $C^{-1}$, we get:\n$$C^{-1} = E \\Lambda^{-1} E^\\top$$\nNow, we can substitute the expressions for $T$, $T^\\top$, and $C^{-1}$ into the term $T^\\top C^{-1} T$:\n$$T^\\top C^{-1} T = (E \\Lambda^{1/2})^\\top (E \\Lambda^{-1} E^\\top) (E \\Lambda^{1/2})$$\nExpanding the first transpose and grouping terms:\n$$T^\\top C^{-1} T = (\\Lambda^{1/2} E^\\top) (E \\Lambda^{-1} E^\\top) (E \\Lambda^{1/2}) = \\Lambda^{1/2} (E^\\top E) \\Lambda^{-1} (E^\\top E) \\Lambda^{1/2}$$\nUsing the orthogonality property $E^\\top E = I$:\n$$T^\\top C^{-1} T = \\Lambda^{1/2} I \\Lambda^{-1} I \\Lambda^{1/2} = \\Lambda^{1/2} \\Lambda^{-1} \\Lambda^{1/2}$$\nSince diagonal matrices commute, the product of the powers of $\\Lambda$ is:\n$$\\Lambda^{1/2} \\Lambda^{-1} \\Lambda^{1/2} = \\Lambda^{1/2 - 1 + 1/2} = \\Lambda^0 = I$$\nThus, we have proven:\n$$T^\\top C^{-1} T = I$$\nThis confirms that the templates defined by $t_i = \\sqrt{\\lambda_i} e_i$ are orthonormal with respect to the metric $C^{-1}$.\n\n### Part 2: Computational Implementation\n\nThe implementation follows the derivation. For each test covariance matrix $C^{(k)}$, the algorithm is as follows:\n\n1.  **PCA of $C$**: We use a numerical routine to perform the eigendecomposition of the symmetric matrix $C$. `numpy.linalg.eigh` is ideal for this, as it is optimized for symmetric/Hermitian matrices and guarantees real eigenvalues and an orthonormal eigenvector basis. This function returns eigenvalues in ascending order, so they and their corresponding eigenvectors are re-sorted into descending order to match the convention $\\lambda_1 \\ge \\lambda_2 \\ge \\dots$.\n\n2.  **Construct $T$**: The template matrix $T$ is constructed with columns $t_i = \\sqrt{\\lambda_i} e_i$. Computationally, this is achieved by creating a matrix whose columns are the sorted eigenvectors $e_i$, and then scaling each column by the corresponding $\\sqrt{\\lambda_i}$.\n\n3.  **Compute Diagnostics**:\n    - $m_1$: The matrix $G = T^\\top C^{-1} T$ is computed. Theoretically, $G$ is the identity matrix $I$. Due to floating-point precision limitations, its off-diagonal elements will be small non-zero numbers. $m_1$ is the maximum absolute value of these off-diagonal elements.\n    - $m_2$: The reconstruction $T T^\\top$ is computed. Theoretically, this equals $C$. The relative Frobenius reconstruction error, $m_2 = \\frac{\\lVert C - T T^\\top \\rVert_F}{\\lVert C \\rVert_F}$, quantifies the numerical deviation. $\\lVert A \\rVert_F = \\sqrt{\\sum_{i,j} |a_{ij}|^2}$ is the Frobenius norm.\n    - $m_3$: The fraction of total variance explained by the first principal component is calculated as $m_3 = \\frac{\\lambda_1}{\\operatorname{tr}(C)}$. The trace of $C$, $\\operatorname{tr}(C)$, is the sum of its diagonal elements and also equals the sum of its eigenvalues, representing the total variance in the system.\n\nThe code will iterate through the three provided test matrices, compute $(m_1^{(k)}, m_2^{(k)}, m_3^{(k)})$ for each, and format the final results as a single comma-separated list.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the jet energy calibration problem for the given test cases.\n\n    This function performs PCA on three covariance matrices, constructs\n    the corresponding nuisance parameter template matrices, and computes\n    three diagnostic metrics for each case. The results are formatted and\n    printed as a single line of comma-separated values.\n    \"\"\"\n\n    # Define the covariance matrices for the three test cases.\n    C1 = np.array([\n        [0.04, 0.018, 0.0, 0.0],\n        [0.018, 0.09, 0.0, 0.0],\n        [0.0, 0.0, 0.0225, 0.015],\n        [0.0, 0.0, 0.015, 0.04]\n    ])\n\n    C2 = np.array([\n        [0.02001, 0.016, 0.012, 0.008, 0.004],\n        [0.016, 0.02281, 0.0096, 0.0164, 0.0032],\n        [0.012, 0.0096, 0.00721, 0.0048, 0.0024],\n        [0.008, 0.0164, 0.0048, 0.01321, 0.0016],\n        [0.004, 0.0032, 0.0024, 0.0016, 0.00081]\n    ])\n\n    C3 = np.array([\n        [0.01, 0.0, 0.0],\n        [0.0, 0.04, 0.0],\n        [0.0, 0.0, 0.09]\n    ])\n\n    test_cases = [C1, C2, C3]\n    all_results = []\n\n    for C in test_cases:\n        # Step 1: Perform PCA (Eigendecomposition).\n        # np.linalg.eigh is used for symmetric matrices. It returns eigenvalues\n        # in ascending order, so we must reverse the order.\n        eigenvalues, eigenvectors = np.linalg.eigh(C)\n        \n        # Get sorting indices for descending order.\n        idx = eigenvalues.argsort()[::-1]\n        lambdas_desc = eigenvalues[idx]\n        E_desc = eigenvectors[:, idx]\n\n        # Step 2: Construct the template matrix T.\n        # The columns of T are t_i = sqrt(lambda_i) * e_i.\n        # This is equivalent to scaling the columns of the eigenvector matrix.\n        sqrt_lambdas = np.sqrt(lambdas_desc)\n        T = E_desc * sqrt_lambdas  # Broadcasting scales each column\n\n        # Step 3: Compute the three scalar diagnostics.\n        \n        # Diagnostic m1: Maximum absolute off-diagonal element of G = T^T C^-1 T.\n        # The problem states Cs are positive definite, so inv(C) exists.\n        C_inv = np.linalg.inv(C)\n        G = T.T @ C_inv @ T\n        # Create a copy and fill diagonal with 0 to isolate off-diagonal elements.\n        G_off_diagonal = G.copy()\n        np.fill_diagonal(G_off_diagonal, 0.0)\n        m1 = np.max(np.abs(G_off_diagonal))\n\n        # Diagnostic m2: Relative Frobenius reconstruction error.\n        C_recon = T @ T.T\n        norm_diff = np.linalg.norm(C - C_recon, 'fro')\n        norm_C = np.linalg.norm(C, 'fro')\n        # Handle case where norm_C is zero to avoid division by zero.\n        m2 = norm_diff / norm_C if norm_C != 0.0 else 0.0\n\n        # Diagnostic m3: Fraction of total variance from the leading component.\n        lambda_1 = lambdas_desc[0]\n        trace_C = np.trace(C)\n        # Handle case where trace_C is zero.\n        m3 = lambda_1 / trace_C if trace_C != 0.0 else 0.0\n\n        all_results.extend([m1, m2, m3])\n\n    # Format the final list of results as specified.\n    # Each float is rounded to 6 decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the main function.\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}