{
    "hands_on_practices": [
        {
            "introduction": "A central goal in experimental high-energy physics is the search for new phenomena, a process often quantified by a \"discovery significance.\" This exercise demonstrates how to forecast this significance before an experiment is even run, a critical step in experimental design. By introducing the concept of the Asimov dataset—a hypothetical dataset where all measurements equal their expected values—we can derive an analytic formula for the median expected significance, directly linking the fundamental likelihood function to a key planning metric .",
            "id": "3526337",
            "problem": "A single-bin counting experiment is performed in a search for a new high-energy physics signal. The observed event count is modeled as a Poisson random variable with mean $\\nu(\\mu)=\\mu s + b$, where $\\mu \\geq 0$ is the signal strength parameter that scales the nominal expected signal yield $s$, and $b$ is the known expected background yield. The likelihood for an observed count $n$ is the Poisson probability mass function $L(n \\mid \\mu)=\\text{Pois}\\!\\left(n \\mid \\mu s + b\\right)$. For testing the discovery hypothesis, the null hypothesis is $H_{0}:\\mu=0$, and the alternative is $H_{1}:\\mu0$. The one-sided profile likelihood ratio test statistic for discovery is defined as $q_{0}=-2\\ln\\lambda(0)$ with $\\lambda(0)=\\dfrac{L(n \\mid \\mu=0)}{L(n \\mid \\hat{\\mu})}$, where $\\hat{\\mu}$ is the maximum likelihood estimate of $\\mu$ subject to the physical constraint $\\mu\\geq 0$.\n\nDefine the Asimov dataset in this context. Then, under the Asimov assumption for the signal-plus-background hypothesis (i.e., the data are taken to be equal to their expectation with $\\mu=1$), derive the closed-form analytic expression for the median expected discovery test statistic $q_{0}$ and the corresponding median expected significance $Z=\\sqrt{q_{0}}$ as functions of $s$ and $b$. Your final answer must be a single closed-form expression or a pair of closed-form expressions presented together, and must not include any numerical substitution. No rounding is required. Express the final significance as a pure number without units.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, objective, and complete. We may proceed with the solution.\n\nThe problem asks for the median expected discovery test statistic, $q_{0}$, and the corresponding median expected significance, $Z$, under the Asimov assumption for the signal-plus-background hypothesis ($\\mu=1$). The derivation proceeds in four steps: (1) defining the Asimov dataset, (2) finding the maximum likelihood estimate ($\\hat{\\mu}$) for this dataset, (3) calculating the test statistic $q_{0}$, and (4) deriving the significance $Z$.\n\nFirst, we define the Asimov dataset. The Asimov dataset is a representative dataset where the observed data are set to their expectation values under a specific hypothesis. For this problem, the hypothesis is the signal-plus-background hypothesis with signal strength $\\mu=1$. The expected number of events, $\\nu(\\mu)$, is given by $\\nu(\\mu) = \\mu s + b$. Under the hypothesis $\\mu=1$, the expected event count is $\\nu(1) = (1)s + b = s+b$. Therefore, the Asimov dataset consists of a single observed count $n_{A} = s+b$. Note that while an observed count must be an integer, the Asimov dataset is a theoretical construct for which $n_A$ can be a non-integer real number, as the Poisson probability mass function can be generalized to non-integer arguments using the Gamma function.\n\nSecond, we determine the maximum likelihood estimate (MLE), $\\hat{\\mu}$, for the signal strength parameter $\\mu$ given the Asimov observation $n = n_{A} = s+b$. The likelihood function is the Poisson probability $L(n \\mid \\mu) = \\frac{(\\mu s + b)^{n} \\exp(-(\\mu s + b))}{n!}$. It is more convenient to work with the log-likelihood function, $\\ln L$:\n$$ \\ln L(n \\mid \\mu) = n \\ln(\\mu s + b) - (\\mu s + b) - \\ln(n!) $$\nTo find the unconstrained MLE, $\\tilde{\\mu}$, we take the derivative of $\\ln L$ with respect to $\\mu$ and set it to zero:\n$$ \\frac{\\partial}{\\partial \\mu} \\ln L(n \\mid \\mu) = \\frac{ns}{\\mu s + b} - s = 0 $$\nAssuming $s0$, we can solve for $\\mu$:\n$$ \\frac{ns}{\\tilde{\\mu} s + b} = s \\implies n = \\tilde{\\mu} s + b \\implies \\tilde{\\mu} = \\frac{n-b}{s} $$\nSubstituting the Asimov data $n = n_{A} = s+b$ into this expression gives the unconstrained MLE for the Asimov dataset:\n$$ \\tilde{\\mu}_{A} = \\frac{(s+b) - b}{s} = \\frac{s}{s} = 1 $$\nThe problem states the physical constraint $\\mu \\geq 0$. The constrained MLE, $\\hat{\\mu}$, is therefore given by $\\hat{\\mu} = \\max(0, \\tilde{\\mu})$. Since we found $\\tilde{\\mu}_{A}=1$, which is greater than $0$, the constrained MLE for the Asimov dataset is $\\hat{\\mu}_{A} = 1$.\n\nThird, we calculate the test statistic $q_{0}$. The test statistic is defined as $q_{0} = -2\\ln\\lambda(0)$, where the profile likelihood ratio $\\lambda(0)$ is given by:\n$$ \\lambda(0) = \\frac{L(n \\mid \\mu=0)}{L(n \\mid \\hat{\\mu})} $$\nWe evaluate this for the Asimov dataset $n=n_A=s+b$ and its corresponding MLE $\\hat{\\mu}=\\hat{\\mu}_A=1$.\nThe likelihood in the numerator is evaluated under the null hypothesis ($H_{0}: \\mu=0$):\n$$ L(n_{A} \\mid \\mu=0) = \\text{Pois}(s+b \\mid b) = \\frac{b^{s+b} \\exp(-b)}{(s+b)!} $$\nThe likelihood in the denominator is evaluated at the MLE, $\\hat{\\mu}=1$:\n$$ L(n_{A} \\mid \\hat{\\mu}=1) = \\text{Pois}(s+b \\mid s+b) = \\frac{(s+b)^{s+b} \\exp(-(s+b))}{(s+b)!} $$\nThe ratio $\\lambda(0)$ is then:\n$$ \\lambda(0) = \\frac{b^{s+b} \\exp(-b)}{(s+b)^{s+b} \\exp(-(s+b))} = \\left(\\frac{b}{s+b}\\right)^{s+b} \\exp(-(b-(s+b))) = \\left(\\frac{b}{s+b}\\right)^{s+b} \\exp(s) $$\nNow, we compute $q_{0}$ by taking $-2\\ln$ of this expression:\n$$ q_{0} = -2\\ln\\left[ \\left(\\frac{b}{s+b}\\right)^{s+b} \\exp(s) \\right] = -2 \\left[ (s+b)\\ln\\left(\\frac{b}{s+b}\\right) + s \\right] $$\nUsing the property $\\ln(x/y) = -\\ln(y/x)$, we can rewrite this as:\n$$ q_{0} = -2 \\left[ -(s+b)\\ln\\left(\\frac{s+b}{b}\\right) + s \\right] = 2(s+b)\\ln\\left(\\frac{s+b}{b}\\right) - 2s $$\nFactoring out the $2$ and rewriting the argument of the logarithm gives the final expression for the median expected test statistic, which is often denoted as $q_{0,A}$:\n$$ q_{0} = 2 \\left[ (s+b)\\ln\\left(1+\\frac{s}{b}\\right) - s \\right] $$\nThis expression is valid for $s0$ and $b0$.\n\nFinally, the median expected significance $Z$ is defined as $Z = \\sqrt{q_{0}}$. Taking the square root of the expression for $q_{0}$ gives:\n$$ Z = \\sqrt{2 \\left[ (s+b)\\ln\\left(1+\\frac{s}{b}\\right) - s \\right]} $$\nThis is the well-known \"Asimov formula\" for the median significance of a simple counting experiment.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 2 \\left[ (s+b)\\ln\\left(1+\\frac{s}{b}\\right) - s \\right]    \\sqrt{2 \\left[ (s+b)\\ln\\left(1+\\frac{s}{b}\\right) - s \\right]} \\end{pmatrix} } $$"
        },
        {
            "introduction": "Beyond asking \"is there a signal?\", we often need to measure its properties and report a confidence interval for the parameter of interest. While simple methods based on the curvature of the log-likelihood are common, they can fail spectacularly in challenging but realistic scenarios, such as when a signal is very weak or the data happens to align closely with the background-only expectation. This practice guides you through a scenario where such methods break down, comparing them to more robust techniques like the likelihood-ratio scan and the parametric bootstrap, revealing why a deep understanding of interval construction is essential for reliable scientific results .",
            "id": "3526375",
            "problem": "You will design and implement a complete, runnable program that constructs and compares three interval estimators for a single signal-strength parameter in a simplified high-energy physics counting experiment. The context is a weakly coupled interaction whose event yield depends quadratically on a real nonnegative amplitude parameter. The key challenge is the regime near a likelihood saddle where the second derivative of the log-likelihood with respect to the parameter is approximately zero, causing curvature-based uncertainty estimates to fail.\n\nThe physical setup is idealized but standard in computational high-energy physics: a single-bin counting experiment with known background and a signal yield that scales with the square of a nonnegative amplitude. The data are a single integer count $n$ drawn from a Poisson distribution with mean\n$$\n\\lambda(\\theta) \\equiv b + s\\,\\theta^2,\n$$\nwhere $b \\ge 0$ is the known expected background count, $s  0$ is a known signal normalization, and $\\theta \\ge 0$ is the real amplitude parameter of interest. No physical units are required because all quantities are dimensionless event counts. Your program must implement three interval constructions for $\\theta$ at a central probability mass of $0.6827$ (commonly called one standard deviation for a Gaussian), and compare their behaviors, particularly near the saddle regime where the observed curvature of the log-likelihood at its maximum is close to zero.\n\nFundamental base and definitions:\n- The likelihood for observing $n$ given $\\theta$ is\n$$\nL(\\theta; n, b, s) = \\mathrm{Pois}\\big(n \\mid \\lambda(\\theta)\\big) = \\frac{\\lambda(\\theta)^n e^{-\\lambda(\\theta)}}{n!}.\n$$\n- The log-likelihood (up to an additive constant independent of $\\theta$) is\n$$\n\\ell(\\theta) = n \\log \\lambda(\\theta) - \\lambda(\\theta).\n$$\n- The maximum likelihood estimate (MLE) $\\hat{\\theta}$ is any maximizer of $\\ell(\\theta)$ subject to $\\theta \\ge 0$.\n- The observed Fisher information is defined as $J(\\theta) \\equiv -\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta^2}$ evaluated at the MLE, namely $J(\\hat{\\theta})$.\n- The likelihood-ratio statistic for a given $\\theta$ is\n$$\nq(\\theta) \\equiv 2\\left[\\ell(\\hat{\\theta}) - \\ell(\\theta)\\right].\n$$\n\nYour program must:\n1. Compute the MLE $\\hat{\\theta}$ by maximizing $\\ell(\\theta)$ over $\\theta \\ge 0$.\n2. Construct the following three intervals for $\\theta$ at central probability mass $0.6827$:\n   - Curvature-based (Wald) interval: Use the observed Fisher information $J(\\hat{\\theta})$ to form a normal-approximation interval. Treat the target as a two-sided central interval. If $J(\\hat{\\theta}) \\le 0$ or numerically indistinguishable from zero (below a strict positive tolerance), the curvature-based interval is undefined; in that case, you must report the curvature-based interval as $[0, +\\infty)$.\n   - Likelihood-ratio scan interval: Invert the test based on $q(\\theta)$, finding the connected interval of $\\theta \\ge 0$ for which $q(\\theta) \\le c$, where $c$ corresponds to the one-dimensional central probability mass of $0.6827$ under the Gaussian approximation. Use the canonical choice $c = 1$. Implement numerical root finding by scanning $q(\\theta)$ and bracketing each endpoint rigorously before invoking a robust solver. If the lower endpoint would be negative, report it as $0$.\n   - Parametric bootstrap interval: Generate pseudo-experiments $n^\\star \\sim \\mathrm{Pois}\\big(b + s \\hat{\\theta}^2\\big)$ with a fixed random seed for reproducibility. For each pseudo-experiment, compute the bootstrap MLE $\\hat{\\theta}^\\star$. Report the empirical central interval defined by the lower and upper quantiles at $0.15865$ and $0.84135$. Use at least $10000$ bootstrap replicates for each test case to ensure stability.\n\n3. Provide results for the following test suite of distinct regimes:\n   - Case A (saddle regime): $n = 25$, $b = 25.0$, $s = 100.0$.\n   - Case B (slight excess near saddle): $n = 27$, $b = 25.0$, $s = 100.0$.\n   - Case C (deficit with boundary MLE): $n = 20$, $b = 25.0$, $s = 100.0$.\n   - Case D (low-count regime): $n = 1$, $b = 0.5$, $s = 5.0$.\n\nAlgorithmic and numerical requirements:\n- Start from the fundamental definitions above and compute derivatives as needed from $\\ell(\\theta)$. Do not assume any pre-supplied formulas; derive what you need from first principles.\n- All numerical solvers must handle the constraint $\\theta \\ge 0$. Use a clear finite tolerance when deciding whether $J(\\hat{\\theta})$ is nonpositive.\n- Use a fixed seed for any pseudo-random generation to ensure determinism.\n- Report each interval as an ordered pair $[\\text{lower}, \\text{upper}]$ with the lower bound truncated at $0$ when necessary. If the upper bound is $+\\infty$, that must be reported explicitly as an infinite floating-point value.\n\nFinal output format:\n- For each case in the specified order (A, B, C, D), produce a list containing the three intervals in the order curvature-based, likelihood-ratio scan, parametric bootstrap, each interval represented as two floating-point numbers rounded to $6$ decimal places. The final output must therefore be a single line containing a list of four elements (one per test case), where each element is a list of six floats in the order\n$$\n[\\text{wald\\_low}, \\text{wald\\_high}, \\text{lr\\_low}, \\text{lr\\_high}, \\text{boot\\_low}, \\text{boot\\_high}].\n$$\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\,[\\cdots], [\\cdots], [\\cdots], [\\cdots]\\,]$.\n\nAll quantities are dimensionless, and no physical units are required. All angles, if any appear, must be in radians; however, this problem does not involve angles.",
            "solution": "We begin from the maximum likelihood principle and the Poisson model appropriate for a single-bin counting experiment in high-energy physics. The experiment observes a count $n$ modeled as Poisson with mean $\\lambda(\\theta) = b + s \\theta^2$ for nonnegative $\\theta$. The likelihood function is $L(\\theta) = \\mathrm{Pois}(n \\mid \\lambda(\\theta))$ and the log-likelihood, dropping additive constants independent of $\\theta$, is\n$$\n\\ell(\\theta) = n \\log \\lambda(\\theta) - \\lambda(\\theta), \\quad \\lambda(\\theta) \\equiv b + s \\theta^2.\n$$\n\nMaximum likelihood estimate:\nBecause the Poisson likelihood with respect to its mean is maximized at $\\hat{\\lambda} = n$, the constrained maximizer over $\\theta \\ge 0$ must satisfy\n$$\n\\hat{\\theta}^2 = \\max\\left( \\frac{n - b}{s}, 0 \\right), \\quad \\hat{\\theta} = \\sqrt{\\max\\left( \\frac{n - b}{s}, 0 \\right)}.\n$$\nThis follows by maximizing $\\ell(\\theta)$ as a function of $\\lambda(\\theta)$ with respect to $\\theta$, observing monotonicity and the nonnegativity constraint. If $n \\le b$ then $\\hat{\\theta} = 0$ (boundary MLE). If $n  b$ then $\\hat{\\theta} = \\sqrt{(n - b)/s}$ (interior MLE), for which the corresponding $\\hat{\\lambda} = n$.\n\nObserved Fisher information and curvature-based (Wald) interval:\nThe observed Fisher information is $J(\\theta) \\equiv -\\ell''(\\theta)$, where the prime denotes differentiation with respect to $\\theta$. We compute derivatives from first principles. First,\n$$\n\\ell'(\\theta) = \\frac{n}{\\lambda(\\theta)} \\cdot \\lambda'(\\theta) - \\lambda'(\\theta) = \\left(\\frac{n - \\lambda(\\theta)}{\\lambda(\\theta)}\\right) \\lambda'(\\theta).\n$$\nSince $\\lambda'(\\theta) = 2 s \\theta$, we have\n$$\n\\ell'(\\theta) = \\left(\\frac{n - \\lambda(\\theta)}{\\lambda(\\theta)}\\right) 2 s \\theta.\n$$\nDifferentiating again, using the product rule and that\n$$\n\\frac{d}{d\\theta}\\left(\\frac{n - \\lambda(\\theta)}{\\lambda(\\theta)}\\right) = -\\lambda'(\\theta)\\left(\\frac{1}{\\lambda(\\theta)} + \\frac{n - \\lambda(\\theta)}{\\lambda(\\theta)^2}\\right) = -\\lambda'(\\theta) \\frac{n}{\\lambda(\\theta)^2},\n$$\nwe obtain\n$$\n\\ell''(\\theta) = 2 s \\frac{n - \\lambda(\\theta)}{\\lambda(\\theta)} - 4 s^2 \\theta^2 \\frac{n}{\\lambda(\\theta)^2}.\n$$\nEvaluated at the interior MLE where $\\lambda(\\hat{\\theta}) = n$, this simplifies to\n$$\n\\ell''(\\hat{\\theta}) = - \\frac{4 s (n - b)}{n}, \\quad J(\\hat{\\theta}) = -\\ell''(\\hat{\\theta}) = \\frac{4 s (n - b)}{n}.\n$$\nAt the boundary MLE $\\hat{\\theta} = 0$, we have $\\lambda(0) = b$ and\n$$\n\\ell''(0) = 2 s \\frac{n - b}{b}, \\quad J(0) = -\\ell''(0) = 2 s \\frac{b - n}{b}.\n$$\nThe saddle regime occurs when $n \\approx b$, for which $\\ell''(\\hat{\\theta}) \\approx 0$ and hence $J(\\hat{\\theta}) \\approx 0$. In that case the quadratic (Gaussian) approximation breaks down because it predicts an arbitrarily large variance or fails to define one when $J(\\hat{\\theta}) \\le 0$ under numerical noise.\n\nThe curvature-based (Wald) $0.6827$-mass interval is the two-sided normal-approximation interval\n$$\n\\left[ \\max\\left(0, \\hat{\\theta} - \\frac{1}{\\sqrt{J(\\hat{\\theta})}}\\right), \\ \\hat{\\theta} + \\frac{1}{\\sqrt{J(\\hat{\\theta})}} \\right],\n$$\nwith the understanding that if $J(\\hat{\\theta}) \\le 0$ or is numerically indistinguishable from zero within a strict tolerance, this construction is invalid and we report $[0, +\\infty)$.\n\nLikelihood-ratio scan interval:\nDefine the likelihood-ratio statistic\n$$\nq(\\theta) \\equiv 2 \\left[\\ell(\\hat{\\theta}) - \\ell(\\theta)\\right].\n$$\nWe construct the $0.6827$-mass interval by inverting the test at threshold $c = 1$ (the canonical one-dimensional threshold corresponding to one standard deviation under a Gaussian approximation). The interval is the set\n$$\n\\{\\theta \\ge 0 \\mid q(\\theta) \\le 1\\}.\n$$\nOperationally, we find the connected interval around $\\hat{\\theta}$ by root finding on $q(\\theta) - 1 = 0$. The lower endpoint is either the unique root in $(0, \\hat{\\theta})$ when it exists or $0$ otherwise (e.g., if $q(0) \\le 1$ or $\\hat{\\theta} = 0$). The upper endpoint is the unique root in $(\\hat{\\theta}, \\infty)$, which exists because $\\ell(\\theta) \\to -\\infty$ as $\\theta \\to \\infty$ and hence $q(\\theta) \\to \\infty$. We bracket each root by scanning outward and apply a robust bisection-based solver.\n\nParametric bootstrap interval:\nWe perform a parametric bootstrap at the plug-in parameter value $\\hat{\\theta}$. Generate $N_{\\mathrm{boot}}$ pseudo-experiments $n^\\star \\sim \\mathrm{Pois}(b + s \\hat{\\theta}^2)$ with a fixed seed. For each $n^\\star$, compute the bootstrap MLE\n$$\n\\hat{\\theta}^\\star = \\sqrt{\\max\\left(\\frac{n^\\star - b}{s}, 0\\right)}.\n$$\nThe bootstrap $0.6827$-mass central interval is given by the empirical quantiles at probabilities $0.15865$ and $0.84135$. Because $\\theta \\ge 0$, it is common that the lower quantile may be $0$ in the boundary regime.\n\nTest suite and implementation details:\nWe apply the above constructions to four cases: $(n, b, s) \\in \\{(25, 25.0, 100.0), (27, 25.0, 100.0), (20, 25.0, 100.0), (1, 0.5, 5.0)\\}$. For each, we compute\n- the MLE $\\hat{\\theta}$ from first principles as above;\n- the curvature-based interval using $J(\\hat{\\theta})$ derived from $\\ell''(\\theta)$;\n- the likelihood-ratio interval by solving $q(\\theta) = 1$ with bracketing and a robust root finder;\n- the parametric bootstrap interval with $N_{\\mathrm{boot}} \\ge 10000$ replicates and fixed seed.\n\nNumerical stability considerations:\n- We use the analytic expression for $\\ell''(\\theta)$ to evaluate $J(\\hat{\\theta})$ accurately at the MLE. When $b$ is very small, we avoid evaluating divisions by zero by ensuring $b  0$ in the test suite.\n- We implement a strict positive tolerance for $J(\\hat{\\theta})$. If $J(\\hat{\\theta})$ is nonpositive or below tolerance, we declare the Wald interval undefined and report $[0, +\\infty)$.\n- For likelihood-ratio bracketing, we exponentially increase the bracket until $q(\\theta)$ exceeds $1$ to guarantee existence of the upper root. The lower root is sought only if $q(0)  1$ and $\\hat{\\theta}  0$.\n\nFinal output:\nFor each case, we report the six numbers\n$$\n[\\text{wald\\_low}, \\text{wald\\_high}, \\text{lr\\_low}, \\text{lr\\_high}, \\text{boot\\_low}, \\text{boot\\_high}],\n$$\nrounded to $6$ decimal places, producing a single-line list of four such lists. This design demonstrates the failure of curvature-based intervals near the saddle ($J(\\hat{\\theta}) \\approx 0$), the robustness of likelihood-ratio intervals obtained by scanning the exact likelihood, and the data-driven behavior of bootstrap intervals, especially in boundary and low-count regimes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef loglik(n, b, s, theta):\n    # Log-likelihood up to additive constant independent of theta\n    # Ensure theta = 0\n    if theta  0:\n        return -np.inf\n    lam = b + s * theta**2\n    if lam = 0:\n        return -np.inf\n    return n * np.log(lam) - lam\n\ndef mle_theta(n, b, s):\n    # Constrained MLE theta = 0\n    if n  b:\n        val = (n - b) / s\n        # numerical safeguard\n        val = max(val, 0.0)\n        return np.sqrt(val)\n    else:\n        return 0.0\n\ndef ell_pp(n, b, s, theta):\n    # Second derivative of log-likelihood wrt theta\n    lam = b + s * theta**2\n    if lam = 0:\n        return np.nan\n    term1 = 2.0 * s * (n - lam) / lam\n    term2 = 4.0 * s**2 * theta**2 * n / (lam**2)\n    return term1 - term2\n\ndef observed_info(n, b, s, theta_hat):\n    # Observed Fisher information J = -ell''\n    lpp = ell_pp(n, b, s, theta_hat)\n    if not np.isfinite(lpp):\n        return np.nan\n    return -lpp\n\ndef wald_interval(n, b, s, theta_hat, tol=1e-12):\n    J = observed_info(n, b, s, theta_hat)\n    if not np.isfinite(J) or J = tol:\n        # Undefined curvature interval near saddle or negative curvature\n        return (0.0, float('inf'))\n    sigma = 1.0 / np.sqrt(J)\n    low = max(0.0, theta_hat - sigma)\n    high = theta_hat + sigma\n    return (low, high)\n\ndef q_stat(n, b, s, theta, theta_hat):\n    return 2.0 * (loglik(n, b, s, theta_hat) - loglik(n, b, s, theta))\n\ndef lr_interval(n, b, s, theta_hat, c=1.0):\n    # Find interval {theta = 0 | q(theta) = c}\n    # Lower endpoint\n    def f(th):\n        return q_stat(n, b, s, th, theta_hat) - c\n\n    # Handle lower endpoint\n    if theta_hat = 0.0:\n        lower = 0.0\n    else:\n        q0 = q_stat(n, b, s, 0.0, theta_hat)\n        if q0 = c:\n            lower = 0.0\n        else:\n            # Find root in (0, theta_hat)\n            a, bnd = 0.0, theta_hat\n            # f(bnd) = -c = 0; f(a) = 0\n            # For numerical stability, ensure sign change\n            fa = f(a + 1e-16)  # slightly above zero\n            fb = f(bnd)\n            # Ensure fb  0 (should be true)\n            if fb = 0:\n                # fallback: shrink bnd slightly\n                bnd = theta_hat * 0.999999\n                fb = f(bnd)\n            if fa = 0 or fb = 0:\n                # If still not a valid bracket due to numerical issues, set lower to 0\n                lower = 0.0\n            else:\n                lower = brentq(f, a + 1e-16, bnd, maxiter=1000, xtol=1e-12, rtol=1e-10)\n    # Upper endpoint\n    # Bracket by expanding until f(high) = 0 (i.e., q(high) = c)\n    lo = max(theta_hat, 0.0)\n    hi = max(lo * 2.0, 1e-9) if lo  0 else 1e-6\n    # Ensure f(lo)  0 (since q(lo) = 0 at theta_hat)\n    flo = f(lo)\n    # For boundary MLE lo = 0, q(0) may be 0; adjust slightly above 0\n    if lo == 0.0:\n        lo = 1e-12\n        flo = f(lo)\n    # Increase hi until sign change\n    fhi = f(hi)\n    iters = 0\n    while not np.isfinite(fhi) or fhi  0.0:\n        hi *= 2.0\n        fhi = f(hi)\n        iters += 1\n        if iters  200:\n            break\n    if not np.isfinite(fhi) or fhi  0.0:\n        # As a last resort, use a very large upper bound\n        hi = max(1.0, hi)\n        for _ in range(1000):\n            hi *= 1.5\n            fhi = f(hi)\n            if np.isfinite(fhi) and fhi = 0.0:\n                break\n    if np.isfinite(fhi) and flo  0.0 and fhi = 0.0:\n        upper = brentq(f, lo, hi, maxiter=1000, xtol=1e-12, rtol=1e-10)\n    else:\n        upper = float('inf')\n    return (max(0.0, lower), upper)\n\ndef bootstrap_interval(n, b, s, theta_hat, n_boot=10000, seed=12345, qlow=0.15865, qhigh=0.84135):\n    rng = np.random.default_rng(seed)\n    lam_hat = b + s * theta_hat**2\n    # Draw Poisson pseudo data\n    samples = rng.poisson(lam=lam_hat, size=n_boot)\n    # Compute bootstrap MLEs\n    thetas = np.sqrt(np.maximum(samples - b, 0.0) / s)\n    # Empirical quantiles\n    low = float(np.quantile(thetas, qlow, method='linear'))\n    high = float(np.quantile(thetas, qhigh, method='linear'))\n    return (max(0.0, low), max(0.0, high))\n\ndef format_float(x):\n    if np.isinf(x):\n        return \"inf\"\n    if np.isnan(x):\n        return \"nan\"\n    return f\"{x:.6f}\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is (n, b, s)\n    test_cases = [\n        (25, 25.0, 100.0),  # Case A: saddle regime (n ~ b)\n        (27, 25.0, 100.0),  # Case B: slight excess near saddle\n        (20, 25.0, 100.0),  # Case C: deficit with boundary MLE\n        (1, 0.5, 5.0),      # Case D: low-count regime\n    ]\n\n    results = []\n    for (n, b, s) in test_cases:\n        theta_hat = mle_theta(n, b, s)\n        w_low, w_high = wald_interval(n, b, s, theta_hat)\n        lr_low, lr_high = lr_interval(n, b, s, theta_hat, c=1.0)\n        # Increase bootstrap replicates modestly in low-count regimes to stabilize\n        n_boot = 20000 if n = 1 else 10000\n        b_low, b_high = bootstrap_interval(n, b, s, theta_hat, n_boot=n_boot, seed=123456)\n        # Append formatted results\n        results.append([\n            format_float(w_low), format_float(w_high),\n            format_float(lr_low), format_float(lr_high),\n            format_float(b_low), format_float(b_high)\n        ])\n\n    # Final print statement in the exact required format.\n    # We need a single-line list of lists of 6 floats (as strings).\n    # Construct the string manually to ensure exact formatting.\n    inner_strs = []\n    for res in results:\n        inner_strs.append(\"[\" + \",\".join(res) + \"]\")\n    print(\"[\" + \",\".join(inner_strs) + \"]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our statistical models are rarely perfect representations of reality, a complication that must be handled with care. This practice confronts the crucial issue of model misspecification, where the assumed model for a fit (e.g., a simple Gaussian) differs from the true data-generating process (e.g., a distribution with non-Gaussian tails). You will learn to compute the \"pseudo-true\" parameters that the fit will converge to and, more importantly, derive and apply the \"sandwich\" covariance matrix to obtain valid uncertainty estimates, a cornerstone of robust statistical inference in the face of imperfect models .",
            "id": "3526396",
            "problem": "You will implement and analyze a quasi-maximum likelihood estimator under model misspecification in a high-energy physics resolution-modeling setting. A true, non-Gaussian resolution distribution $p_0(x)$ is observed, but a parametric Gaussian model $p_\\theta(x)$ with parameter $\\theta=(\\mu,\\sigma)$, where $\\sigma0$, is used for fitting. The task is to determine the pseudo-true parameter $\\theta^\\ast$ obtained by minimizing the Kullback–Leibler divergence and to compute the asymptotic covariance of the quasi-maximum likelihood estimator.\n\nStarting from the foundational definitions:\n- The Kullback–Leibler divergence between $p_0$ and $p_\\theta$ is $\\,\\mathrm{KL}(p_0\\Vert p_\\theta)=\\mathbb{E}_{p_0}\\big[\\log p_0(X)-\\log p_\\theta(X)\\big]$.\n- The pseudo-true parameter is defined as $\\,\\theta^\\ast=\\arg\\min_{\\theta}\\mathrm{KL}(p_0\\Vert p_\\theta)$.\n- The quasi-maximum likelihood estimator is defined by maximizing the sample log-likelihood $\\sum_{i=1}^{n}\\log p_\\theta(X_i)$, where $\\{X_i\\}_{i=1}^n$ are independent and identically distributed draws from $p_0$.\n- The asymptotic covariance (also called the sandwich covariance) is obtained from the Hessian and the score covariance evaluated at $\\theta^\\ast$ under $p_0$.\n\nIn this problem, the true distribution $p_0$ is either:\n1. A double-sided Crystal Ball distribution (used for non-Gaussian detector resolution tails in high-energy physics) with parameters $(\\mu_0,\\sigma_0,\\alpha_L,n_L,\\alpha_R,n_R)$, or\n2. A two-component Gaussian mixture with a common mean (a Gaussian core with a broader Gaussian tail).\n\nDefine the double-sided Crystal Ball (DSCB) distribution as follows. Let $t=(x-\\mu_0)/\\sigma_0$. Define $g(t)$ as:\n$$\ng(t)=\n\\begin{cases}\nA_L\\,(B_L-t)^{-n_L},  t-\\alpha_L \\\\\n\\exp\\!\\left(-\\tfrac{t^2}{2}\\right),  -\\alpha_L\\le t\\le \\alpha_R \\\\\nA_R\\,(B_R+t)^{-n_R},  t\\alpha_R\n\\end{cases}\n$$\nwith constants\n$$\nA_L=\\left(\\frac{n_L}{\\alpha_L}\\right)^{n_L}\\exp\\!\\left(-\\tfrac{\\alpha_L^2}{2}\\right),\\quad\nB_L=\\frac{n_L}{\\alpha_L}-\\alpha_L,\\quad\nA_R=\\left(\\frac{n_R}{\\alpha_R}\\right)^{n_R}\\exp\\!\\left(-\\tfrac{\\alpha_R^2}{2}\\right),\\quad\nB_R=\\frac{n_R}{\\alpha_R}-\\alpha_R.\n$$\nThe normalized probability density function in $x$ is\n$$\np_0(x)=\\frac{1}{\\sigma_0\\,I}\\,g\\!\\left(\\frac{x-\\mu_0}{\\sigma_0}\\right),\n$$\nwhere\n$$\nI=\\int_{-\\infty}^{\\infty} g(t)\\,dt\n=\\int_{-\\infty}^{-\\alpha_L} A_L\\,(B_L-t)^{-n_L}\\,dt\n+\\int_{-\\alpha_L}^{\\alpha_R} e^{-t^2/2}\\,dt\n+\\int_{\\alpha_R}^{\\infty} A_R\\,(B_R+t)^{-n_R}\\,dt.\n$$\nAssume $n_L4$ and $n_R4$ so that the first four moments exist.\n\nDefine the two-component Gaussian mixture as\n$$\np_0(x)=(1-\\varepsilon)\\,\\mathcal{N}(x\\mid \\mu_0,\\sigma_0^2)+\\varepsilon\\,\\mathcal{N}\\!\\big(x\\mid \\mu_0,(k\\,\\sigma_0)^2\\big),\n$$\nwith parameters $\\mu_0\\in\\mathbb{R}$, $\\sigma_00$, $k0$, and mixing weight $\\varepsilon\\in[0,1]$.\n\nLet the Gaussian model be\n$$\np_\\theta(x)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right),\n\\quad \\theta=(\\mu,\\sigma),\\ \\sigma0.\n$$\n\nTasks:\n1. Using only the above definitions and standard results from probability theory and asymptotic maximum likelihood theory, determine the pseudo-true parameter $\\theta^\\ast$ that minimizes $\\mathrm{KL}(p_0\\Vert p_\\theta)$ for the Gaussian model, in terms of the moments of $p_0$.\n2. Derive the asymptotic covariance matrix of the quasi-maximum likelihood estimator at $\\theta^\\ast$ in terms of expectations under $p_0$.\n3. Implement a program that, for each specified test case of $p_0$, computes:\n   - The pseudo-true parameter $\\theta^\\ast=(\\mu^\\ast,\\sigma^\\ast)$,\n   - The entries of the asymptotic covariance matrix, denoted as $\\mathrm{Var}[\\hat\\mu]$, $\\mathrm{Cov}[\\hat\\mu,\\hat\\sigma]$, and $\\mathrm{Var}[\\hat\\sigma]$, for a given sample size $n$.\n4. For the double-sided Crystal Ball distribution, your program must compute the required expectations by numerical integration over $t$ as defined above. For the Gaussian mixture, you may use the known closed-form moments of the normal distribution to compute expectations exactly.\n5. Numerical outputs must be reported as dimensionless real numbers. No physical units are involved.\n6. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list $[\\mu^\\ast,\\sigma^\\ast,\\mathrm{Var}[\\hat\\mu],\\mathrm{Cov}[\\hat\\mu,\\hat\\sigma],\\mathrm{Var}[\\hat\\sigma]]$ with each entry rounded to six decimal places. The final line must be a list of such lists, e.g., $[[\\dots],[\\dots],\\dots]$.\n\nUse the following test suite to ensure coverage of general behavior, symmetry, tail-heaviness, and a boundary case approaching correct specification:\n- Test case $1$ (asymmetric double-sided Crystal Ball): $\\mu_0=0.2$, $\\sigma_0=1.3$, $\\alpha_L=1.5$, $n_L=6$, $\\alpha_R=2.0$, $n_R=7$, $n=5000$.\n- Test case $2$ (symmetric double-sided Crystal Ball): $\\mu_0=0.0$, $\\sigma_0=1.0$, $\\alpha_L=1.5$, $n_L=6$, $\\alpha_R=1.5$, $n_R=6$, $n=10000$.\n- Test case $3$ (Gaussian mixture with non-Gaussian tails): $\\mu_0=0.0$, $\\sigma_0=1.0$, $k=3.0$, $\\varepsilon=0.1$, $n=8000$.\n- Test case $4$ (Gaussian mixture boundary, nearly well-specified): $\\mu_0=1.5$, $\\sigma_0=2.0$, $k=3.0$, $\\varepsilon=0.0$, $n=12000$.\n\nYour program must compute the required quantities for each test case and print a single line of the form $[[\\mu^\\ast,\\sigma^\\ast,\\mathrm{Var}[\\hat\\mu],\\mathrm{Cov}[\\hat\\mu,\\hat\\sigma],\\mathrm{Var}[\\hat\\sigma]],\\dots]$ with each number rounded to six decimal places and no additional whitespace or text.",
            "solution": "The user-provided problem has been critically validated and is deemed valid. It is scientifically grounded in statistical theory and high-energy physics modeling, well-posed, and all necessary information is provided for a unique solution.\n\nThe problem requires the determination of the pseudo-true parameters and the asymptotic covariance matrix for a quasi-maximum likelihood estimator under model misspecification. The true data-generating distribution, $p_0(x)$, is non-Gaussian, but the model used for estimation is a Gaussian distribution, $p_\\theta(x)$, parameterized by $\\theta = (\\mu, \\sigma)$.\n\n**1. Derivation of the Pseudo-True Parameter $\\theta^\\ast$**\n\nThe pseudo-true parameter $\\theta^\\ast = (\\mu^\\ast, \\sigma^\\ast)$ is defined as the minimizer of the Kullback–Leibler (KL) divergence between the true distribution $p_0$ and the model distribution $p_\\theta$:\n$$\n\\theta^\\ast = \\arg\\min_{\\theta} \\mathrm{KL}(p_0\\Vert p_\\theta) = \\arg\\min_{\\theta} \\mathbb{E}_{p_0}\\big[\\log p_0(X) - \\log p_\\theta(X)\\big]\n$$\nSince the term $\\mathbb{E}_{p_0}[\\log p_0(X)]$ does not depend on the parameter $\\theta$, minimizing the KL divergence is equivalent to maximizing the expected log-likelihood of the model under the true distribution:\n$$\n\\theta^\\ast = \\arg\\max_{\\theta} \\mathbb{E}_{p_0}\\big[\\log p_\\theta(X)\\big]\n$$\nThe model is the Gaussian PDF: $p_\\theta(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$, so its logarithm is:\n$$\n\\log p_\\theta(x) = -\\frac{1}{2}\\log(2\\pi) - \\log\\sigma - \\frac{(x-\\mu)^2}{2\\sigma^2}\n$$\nWe must maximize its expectation with respect to $X \\sim p_0(x)$:\n$$\nL(\\theta) = \\mathbb{E}_{p_0}[\\log p_\\theta(X)] = -\\frac{1}{2}\\log(2\\pi) - \\log\\sigma - \\frac{1}{2\\sigma^2}\\mathbb{E}_{p_0}\\left[(X-\\mu)^2\\right]\n$$\nTo find the maximum, we compute the partial derivatives of $L(\\theta)$ with respect to $\\mu$ and $\\sigma$ and set them to zero. Let $m_1 = \\mathbb{E}_{p_0}[X]$ and $m_2 = \\mathbb{E}_{p_0}[X^2]$ be the first two raw moments of $p_0$. Then $\\mathbb{E}_{p_0}\\left[(X-\\mu)^2\\right] = m_2 - 2\\mu m_1 + \\mu^2$.\n\nThe partial derivative with respect to $\\mu$ is:\n$$\n\\frac{\\partial L}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\frac{\\partial}{\\partial \\mu}(m_2 - 2\\mu m_1 + \\mu^2) = -\\frac{1}{2\\sigma^2}(-2m_1 + 2\\mu) = \\frac{m_1 - \\mu}{\\sigma^2}\n$$\nSetting $\\frac{\\partial L}{\\partial \\mu} = 0$ yields $\\mu = m_1$. Thus, the pseudo-true mean $\\mu^\\ast$ is the true mean of $p_0$:\n$$\n\\mu^\\ast = \\mathbb{E}_{p_0}[X]\n$$\nSubstituting $\\mu = \\mu^\\ast$ back into $L$, we get:\n$$\nL(\\mu^\\ast, \\sigma) = C - \\log\\sigma - \\frac{1}{2\\sigma^2}\\mathbb{E}_{p_0}\\left[(X-\\mu^\\ast)^2\\right]\n$$\nwhere $C$ is a constant and $\\mathbb{E}_{p_0}\\left[(X-\\mu^\\ast)^2\\right] = \\mathrm{Var}_{p_0}[X]$ is the true variance of $p_0$.\nThe partial derivative with respect to $\\sigma$ is:\n$$\n\\frac{\\partial L}{\\partial \\sigma} = -\\frac{1}{\\sigma} + \\frac{\\mathrm{Var}_{p_0}[X]}{\\sigma^3}\n$$\nSetting $\\frac{\\partial L}{\\partial \\sigma} = 0$ yields $\\sigma^2 = \\mathrm{Var}_{p_0}[X]$. Thus, the pseudo-true variance $(\\sigma^\\ast)^2$ is the true variance of $p_0$:\n$$\n(\\sigma^\\ast)^2 = \\mathrm{Var}_{p_0}[X]\n$$\nThe pseudo-true parameters are the true mean and standard deviation of the data-generating distribution $p_0$.\n\n**2. Derivation of the Asymptotic Covariance Matrix**\n\nThe asymptotic covariance matrix of the quasi-maximum likelihood estimator $\\hat\\theta_n$ for a sample of size $n$ is given by the sandwich formula:\n$$\nV = \\mathrm{Cov}(\\hat\\theta_n) = \\frac{1}{n} H(\\theta^\\ast)^{-1} J(\\theta^\\ast) H(\\theta^\\ast)^{-1}\n$$\nwhere $H(\\theta) = -\\mathbb{E}_{p_0}[\\nabla^2_\\theta \\log p_\\theta(X)]$ and $J(\\theta) = \\mathbb{E}_{p_0}[(\\nabla_\\theta \\log p_\\theta(X))(\\nabla_\\theta \\log p_\\theta(X))^T]$.\n\nFirst, we compute the gradient (score) vector $s(x, \\theta) = \\nabla_\\theta \\log p_\\theta(x)$:\n$$\ns(x, \\theta) = \\begin{pmatrix} \\frac{\\partial}{\\partial \\mu} \\log p_\\theta(x) \\\\ \\frac{\\partial}{\\partial \\sigma} \\log p_\\theta(x) \\end{pmatrix} = \\begin{pmatrix} \\frac{x-\\mu}{\\sigma^2} \\\\ \\frac{(x-\\mu)^2}{\\sigma^3} - \\frac{1}{\\sigma} \\end{pmatrix}\n$$\nNext, we compute the Hessian matrix $\\nabla^2_\\theta \\log p_\\theta(x)$:\n$$\n\\nabla^2_\\theta \\log p_\\theta(x) = \\begin{pmatrix} -\\frac{1}{\\sigma^2}  -\\frac{2(x-\\mu)}{\\sigma^3} \\\\ -\\frac{2(x-\\mu)}{\\sigma^3}  \\frac{1}{\\sigma^2} - \\frac{3(x-\\mu)^2}{\\sigma^4} \\end{pmatrix}\n$$\nNow we compute $H(\\theta^\\ast)$ by taking the negative expectation of the Hessian under $p_0$ and evaluating at $\\theta^\\ast = (\\mu^\\ast, \\sigma^\\ast)$:\n$$\nH_{11}(\\theta^\\ast) = -\\mathbb{E}_{p_0}\\left[-\\frac{1}{(\\sigma^\\ast)^2}\\right] = \\frac{1}{(\\sigma^\\ast)^2}\n$$\n$$\nH_{12}(\\theta^\\ast) = -\\mathbb{E}_{p_0}\\left[-\\frac{2(X-\\mu^\\ast)}{(\\sigma^\\ast)^3}\\right] = \\frac{2}{(\\sigma^\\ast)^3}\\mathbb{E}_{p_0}[X-\\mu^\\ast] = 0\n$$\n$$\nH_{22}(\\theta^\\ast) = -\\mathbb{E}_{p_0}\\left[\\frac{1}{(\\sigma^\\ast)^2} - \\frac{3(X-\\mu^\\ast)^2}{(\\sigma^\\ast)^4}\\right] = -\\frac{1}{(\\sigma^\\ast)^2} + \\frac{3\\mathbb{E}_{p_0}[(X-\\mu^\\ast)^2]}{(\\sigma^\\ast)^4} = -\\frac{1}{(\\sigma^\\ast)^2} + \\frac{3(\\sigma^\\ast)^2}{(\\sigma^\\ast)^4} = \\frac{2}{(\\sigma^\\ast)^2}\n$$\nThus, $H(\\theta^\\ast)$ is a diagonal matrix:\n$H(\\theta^\\ast) = \\mathrm{diag}\\left(\\frac{1}{(\\sigma^\\ast)^2}, \\frac{2}{(\\sigma^\\ast)^2}\\right)$, and its inverse is $H(\\theta^\\ast)^{-1} = \\mathrm{diag}((\\sigma^\\ast)^2, \\frac{(\\sigma^\\ast)^2}{2})$.\n\nNext, we compute $J(\\theta^\\ast)$ by taking the expectation of the outer product of the score vector evaluated at $\\theta^\\ast$. Let $M_k = \\mathbb{E}_{p_0}[(X-\\mu^\\ast)^k]$ be the $k$-th central moment of $p_0$. Note that $M_1=0$ and $M_2=(\\sigma^\\ast)^2$.\n$$\nJ_{11}(\\theta^\\ast) = \\mathbb{E}_{p_0}\\left[\\left(\\frac{X-\\mu^\\ast}{(\\sigma^\\ast)^2}\\right)^2\\right] = \\frac{M_2}{(\\sigma^\\ast)^4} = \\frac{(\\sigma^\\ast)^2}{(\\sigma^\\ast)^4} = \\frac{1}{(\\sigma^\\ast)^2}\n$$\n$$\nJ_{12}(\\theta^\\ast) = \\mathbb{E}_{p_0}\\left[\\left(\\frac{X-\\mu^\\ast}{(\\sigma^\\ast)^2}\\right)\\left(\\frac{(X-\\mu^\\ast)^2}{(\\sigma^\\ast)^3} - \\frac{1}{\\sigma^\\ast}\\right)\\right] = \\frac{M_3}{(\\sigma^\\ast)^5} - \\frac{M_1}{(\\sigma^\\ast)^3} = \\frac{M_3}{(\\sigma^\\ast)^5}\n$$\n$$\nJ_{22}(\\theta^\\ast) = \\mathbb{E}_{p_0}\\left[\\left(\\frac{(X-\\mu^\\ast)^2}{(\\sigma^\\ast)^3} - \\frac{1}{\\sigma^\\ast}\\right)^2\\right] = \\frac{M_4}{(\\sigma^\\ast)^6} - \\frac{2M_2}{(\\sigma^\\ast)^4} + \\frac{1}{(\\sigma^\\ast)^2} = \\frac{M_4 - (\\sigma^\\ast)^4}{(\\sigma^\\ast)^6}\n$$\nSo, $J(\\theta^\\ast) = \\begin{pmatrix} \\frac{1}{(\\sigma^\\ast)^2}  \\frac{M_3}{(\\sigma^\\ast)^5} \\\\ \\frac{M_3}{(\\sigma^\\ast)^5}  \\frac{M_4 - (\\sigma^\\ast)^4}{(\\sigma^\\ast)^6} \\end{pmatrix}$.\n\nFinally, we assemble the sandwich covariance matrix for $\\hat\\theta_n$:\n$$\nV = \\frac{1}{n} \\begin{pmatrix} (\\sigma^\\ast)^2  0 \\\\ 0  \\frac{(\\sigma^\\ast)^2}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{(\\sigma^\\ast)^2}  \\frac{M_3}{(\\sigma^\\ast)^5} \\\\ \\frac{M_3}{(\\sigma^\\ast)^5}  \\frac{M_4 - (\\sigma^\\ast)^4}{(\\sigma^\\ast)^6} \\end{pmatrix} \\begin{pmatrix} (\\sigma^\\ast)^2  0 \\\\ 0  \\frac{(\\sigma^\\ast)^2}{2} \\end{pmatrix}\n$$\n$$\nV = \\frac{1}{n} \\begin{pmatrix} (\\sigma^\\ast)^2  \\frac{M_3}{2\\sigma^\\ast} \\\\ \\frac{M_3}{2\\sigma^\\ast}  \\frac{(\\sigma^\\ast)^2}{4}\\left(\\frac{M_4}{(\\sigma^\\ast)^4} - 1\\right) \\end{pmatrix}\n$$\nThe required components are:\n$$\n\\mathrm{Var}[\\hat\\mu] = V_{11} = \\frac{(\\sigma^\\ast)^2}{n} = \\frac{M_2}{n}\n$$\n$$\n\\mathrm{Cov}[\\hat\\mu,\\hat\\sigma] = V_{12} = \\frac{M_3}{2n\\sigma^\\ast} = \\frac{M_3}{2n\\sqrt{M_2}}\n$$\n$$\n\\mathrm{Var}[\\hat\\sigma] = V_{22} = \\frac{(\\sigma^\\ast)^2}{4n}\\left(\\frac{M_4}{(\\sigma^\\ast)^4} - 1\\right) = \\frac{M_2}{4n}\\left(\\frac{M_4}{M_2^2} - 1\\right)\n$$\nThe kurtosis of $p_0$ is $\\kappa = M_4/M_2^2$.\n\n**3. Computational Strategy**\n\nThe calculations depend entirely on the first four central moments of $p_0$.\n- For the **two-component Gaussian mixture**, $p_0(x)=(1-\\varepsilon)\\,\\mathcal{N}(x\\mid \\mu_0,\\sigma_0^2)+\\varepsilon\\,\\mathcal{N}\\!\\big(x\\mid \\mu_0,(k\\,\\sigma_0)^2\\big)$, the moments can be calculated in closed form. Both components share the same mean $\\mu_0$, so $\\mu^\\ast = \\mu_0$. The central moments are weighted sums:\n  - $M_2 = (1-\\varepsilon)\\sigma_0^2 + \\varepsilon(k\\sigma_0)^2 = \\sigma_0^2(1-\\varepsilon+\\varepsilon k^2)$\n  - $M_3 = 0$ (due to symmetry)\n  - $M_4 = (1-\\varepsilon)(3\\sigma_0^4) + \\varepsilon(3(k\\sigma_0)^4) = 3\\sigma_0^4(1-\\varepsilon+\\varepsilon k^4)$\n- For the **double-sided Crystal Ball** distribution, $p_0(x)=\\frac{1}{\\sigma_0\\,I}\\,g\\!\\left(\\frac{x-\\mu_0}{\\sigma_0}\\right)$, we use the change of variables $t = (x-\\mu_0)/\\sigma_0$. The PDF of $t$ is $p_T(t) = g(t)/I$. We first compute the raw moments of $t$, $E_k = \\mathbb{E}[T^k] = \\frac{1}{I_0}\\int_{-\\infty}^{\\infty} t^k g(t) dt$, where $I_k = \\int_{-\\infty}^{\\infty} t^k g(t) dt$ are computed via numerical integration. The central moments of $X$ are then found using the moments of $t$:\n  - $\\mu^\\ast = \\mu_0 + \\sigma_0 E_1$\n  - $M_2 = \\sigma_0^2 (E_2 - E_1^2)$\n  - $M_3 = \\sigma_0^3 (E_3 - 3E_1 E_2 + 2E_1^3)$\n  - $M_4 = \\sigma_0^4 (E_4 - 4E_1 E_3 + 6E_1^2 E_2 - 3E_1^4)$\nThese moments are then used in the formulas for the covariance matrix elements.",
            "answer": "```python\nimport numpy as np\nfrom scipy import integrate\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all specified test cases.\n    \"\"\"\n\n    test_cases = [\n        # case_type, params_dict\n        ('dscb', {'mu0': 0.2, 'sigma0': 1.3, 'alpha_L': 1.5, 'n_L': 6.0, 'alpha_R': 2.0, 'n_R': 7.0, 'n': 5000}),\n        ('dscb', {'mu0': 0.0, 'sigma0': 1.0, 'alpha_L': 1.5, 'n_L': 6.0, 'alpha_R': 1.5, 'n_R': 6.0, 'n': 10000}),\n        ('gm', {'mu0': 0.0, 'sigma0': 1.0, 'k': 3.0, 'eps': 0.1, 'n': 8000}),\n        ('gm', {'mu0': 1.5, 'sigma0': 2.0, 'k': 3.0, 'eps': 0.0, 'n': 12000}),\n    ]\n\n    results = []\n    for case_type, params in test_cases:\n        if case_type == 'dscb':\n            result = compute_dscb_case(**params)\n        elif case_type == 'gm':\n            result = compute_gm_case(**params)\n        results.append(result)\n\n    # Format the final output string as specified\n    formatted_results = []\n    for res in results:\n        formatted_list = f\"[{','.join(f'{x:.6f}' for x in res)}]\"\n        formatted_results.append(formatted_list)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\ndef compute_dscb_case(mu0, sigma0, alpha_L, n_L, alpha_R, n_R, n):\n    \"\"\"\n    Computes pseudo-true parameters and covariance for the double-sided Crystal Ball case.\n    \"\"\"\n    A_L = (n_L / alpha_L)**n_L * np.exp(-0.5 * alpha_L**2)\n    B_L = n_L / alpha_L - alpha_L\n    A_R = (n_R / alpha_R)**n_R * np.exp(-0.5 * alpha_R**2)\n    B_R = n_R / alpha_R - alpha_R\n\n    def g(t):\n        if t  -alpha_L:\n            return A_L * (B_L - t)**(-n_L)\n        elif t = alpha_R:\n            return np.exp(-0.5 * t**2)\n        else: # t  alpha_R\n            return A_R * (B_R + t)**(-n_R)\n    \n    # Vectorize the function for performance with quad\n    g_vec = np.vectorize(g)\n\n    # Compute integrals of t^k * g(t) to get raw moments of t\n    I = np.zeros(5)\n    for k in range(5):\n        integrand = lambda t: t**k * g_vec(t)\n        I[k], _ = integrate.quad(integrand, -np.inf, np.inf)\n\n    # Raw moments of t\n    E = I[1:] / I[0]\n    E1, E2, E3, E4 = E[0], E[1], E[2], E[3]\n\n    # Pseudo-true mean of X\n    mu_star = mu0 + sigma0 * E1\n    \n    # Central moments of X\n    M2 = sigma0**2 * (E2 - E1**2)\n    M3 = sigma0**3 * (E3 - 3*E1*E2 + 2*E1**3)\n    M4 = sigma0**4 * (E4 - 4*E1*E3 + 6*E1**2*E2 - 3*E1**4)\n\n    # Derived quantities\n    sigma_star = np.sqrt(M2)\n    \n    var_mu_hat = M2 / n\n    cov_mu_sigma_hat = M3 / (2 * n * sigma_star)\n    var_sigma_hat = (M2 / (4 * n)) * (M4 / M2**2 - 1)\n    \n    return [mu_star, sigma_star, var_mu_hat, cov_mu_sigma_hat, var_sigma_hat]\n\ndef compute_gm_case(mu0, sigma0, k, eps, n):\n    \"\"\"\n    Computes pseudo-true parameters and covariance for the Gaussian mixture case.\n    \"\"\"\n    # Pseudo-true mean of X\n    mu_star = mu0\n\n    # Central moments of X\n    M2 = sigma0**2 * (1 - eps + eps * k**2)\n    M3 = 0.0  # By symmetry\n    M4 = 3 * sigma0**4 * (1 - eps + eps * k**4)\n\n    # Derived quantities\n    sigma_star = np.sqrt(M2)\n\n    var_mu_hat = M2 / n\n    cov_mu_sigma_hat = M3 / (2 * n * sigma_star) if sigma_star  0 else 0.0\n    \n    # Handle the M2=0 case to avoid division by zero\n    if M2  0:\n        var_sigma_hat = (M2 / (4 * n)) * (M4 / M2**2 - 1)\n    else: # This would happen if sigma0=0\n        var_sigma_hat = 0.0\n\n    return [mu_star, sigma_star, var_mu_hat, cov_mu_sigma_hat, var_sigma_hat]\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}