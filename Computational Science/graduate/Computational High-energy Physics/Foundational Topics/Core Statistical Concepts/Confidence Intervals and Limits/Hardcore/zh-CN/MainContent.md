## 引言
在任何依赖于实验数据的科学探索中，量化测量结果的不确定性是得出可靠结论的基石。在高能物理领域，当科学家们在海量数据中寻找新粒子或精确测量基本参数时，[置信区间](@entry_id:142297)与极限的设定便成为描述[发现显著性](@entry_id:748491)与排除范围的核心语言。然而，从原始数据到发表一个具有统计意义的[置信区间](@entry_id:142297)，其过程远非简单的公式应用，而是充满了理论上的微妙之处与实践中的挑战，例如如何处理系统误差、如何避免因统计涨落得出误导性结论等。本文旨在为[计算高能物理](@entry_id:747619)领域的研究生和实践者提供一份关于[置信区间](@entry_id:142297)与极限的全面指南，系统性地解决这些难题。

本文分为三个核心部分。在“原理与机制”一章中，我们将深入频率派统计的根基，从覆盖率的概念出发，详细阐述[Neyman构造](@entry_id:752484)、[似然比](@entry_id:170863)方法以及处理边界效应和[多重检验](@entry_id:636512)等高级课题。随后的“应用与跨学科联系”一章将展示这些理论在真实[高能物理](@entry_id:181260)分析（如灵敏度估计和[不确定性建模](@entry_id:268420)）以及遗传学、宇宙学等其他科学领域中的强大实践能力。最后，通过“动手实践”部分提供的一系列计算练习，读者将有机会亲手实现关键算法，将理论知识转化为实际技能。本指南将带领读者一步步掌握从基本原理到前沿应用的完整知识体系，为严谨的科学数据分析打下坚实的基础。

## 原理与机制

在上一章对[置信区间](@entry_id:142297)和极限的基本概念进行介绍之后，本章将深入探讨其背后的核心统计学原理和在[高能物理](@entry_id:181260)（High-Energy Physics, HEP）计算分析中广泛应用的具体机制。我们将从频率派统计的基石——覆盖率（coverage）——出发，系统地建立起置信区间的构造理论。随后，我们将通过一系列典型的例子，从理想化的[正态分布](@entry_id:154414)模型到实际的泊松计数实验，来阐[明区](@entry_id:273235)间构造的具体方法。本章还将重点介绍基于[似然函数](@entry_id:141927)（likelihood function）的[渐近方法](@entry_id:177759)，特别是[剖面似然比](@entry_id:753793)（profile likelihood ratio）和[Wilks定理](@entry_id:169826)（Wilks' theorem）的应用。最后，我们将讨论在真实HEP分析中遇到的若干关键挑战，例如如何处理[讨厌参数](@entry_id:171802)（nuisance parameters）、如何应对参数的物理边界效应、如何校正“别处寻找效应”（look-elsewhere effect），以及在[高能物理](@entry_id:181260)领域中为保证结果稳健性而发展的特殊方法，如CLs准则。

### 频率派[置信区间](@entry_id:142297)的核心原理

在频率派统计的框架中，参数 $\theta$ 被视为一个固定的、未知的常数，而数据 $X$ 是从一个以 $\theta$ 为特征的[概率分布](@entry_id:146404)中抽样的[随机变量](@entry_id:195330)。因此，任何由数据计算出的量，包括置信区间的端点，都是随机的。

#### 覆盖率的定义与诠释

一个置信区间构造**程序** $C(X)$ 的性能由其**覆盖率**（coverage probability）来衡量。对于[参数空间](@entry_id:178581)中的任意一个固定[真值](@entry_id:636547) $\theta_0$，该程序在 $\theta_0$ 处的覆盖率定义为，在以 $\theta_0$ 为真值的重复实验中，随机区间 $C(X)$ 包含该[真值](@entry_id:636547) $\theta_0$ 的概率。数学上，这表示为 $P_{\theta_0}(\theta_0 \in C(X))$，其中概率 $P_{\theta_0}$ 是根据数据 $X$ 在参数为 $\theta_0$ 时的[抽样分布](@entry_id:269683)计算的 。

一个[置信水平](@entry_id:182309)（confidence level）为 $1-\alpha$ 的[置信区间](@entry_id:142297)程序，其要求是对于参数 $\theta$ 的所有可能值，其覆盖率都**至少**为 $1-\alpha$。即：
$$
\inf_{\theta} P_{\theta}(\theta \in C(X)) \ge 1-\alpha
$$
这里的[置信水平](@entry_id:182309) $1-\alpha$ 是一种关于**程序长期表现**的陈述。它意味着，如果我们使用这个程序进行大量完全相同的独立实验（每个实验都对应着相同的未知真值 $\theta$），那么由这些实验数据产生的大约 $1-\alpha$ 比例的置信区间将会包含这个固定的真值 $\theta$ 。

一个常见的误解是，在观测到具体数据 $x_{\mathrm{obs}}$ 并计算出一个具体的置信区间 $C(x_{\mathrm{obs}})$（例如，$[1.2, 3.4]$）后，这个区间包含[真值](@entry_id:636547) $\theta$ 的概率是 $1-\alpha$。这种说法是错误的。在频率派框架中，一旦区间被计算出来，它就是一个固定的[数值范围](@entry_id:752817)，而 $\theta$ 也是一个固定的数，$\theta$ 要么在这个区间内，要么不在，不存在概率可言。$1-\alpha$ 所描述的是产生这个区间的那个**[随机过程](@entry_id:159502)**的成功率，而非单个结果的确定性。

#### Neyman置信带构造法

保证覆盖率的通用方法是**Neyman置信带构造法**（Neyman belt construction）。这个方法分为两个步骤 ：

1.  **构建接受域（Acceptance Regions）**: 对于参数 $\theta$ 的每一个可[能值](@entry_id:187992)，我们在观测数据 $x$ 的[样本空间](@entry_id:275301)中定义一个**接受域** $A_\theta$。这个区域的选择需要满足条件：如果 $\theta$ 是[真值](@entry_id:636547)，那么观测到数据 $x$ 落在 $A_\theta$ 内的概率至少是 $1-\alpha$。即，$\sum_{x \in A_\theta} P(x|\theta) \ge 1-\alpha$。所有这些 $( \theta, A_\theta )$ 的集合构成了一个在参数-数据空间中的“置信带”。

2.  **反演求置信集（Inverting the Belt）**: 在一个具体的实验中，我们观测到了数据 $x_{\mathrm{obs}}$。那么，对应的置信集 $C(x_{\mathrm{obs}})$ 就是参数空间中所有那些其接受域包含了 $x_{\mathrm{obs}}$ 的 $\theta$ 值的集合。即：
    $$
    C(x_{\mathrm{obs}}) = \{ \theta : x_{\mathrm{obs}} \in A_\theta \}
    $$
这种构造方式通过其内在逻辑保证了覆盖率。因为“真值 $\theta$ 在置信集 $C(X)$ 中”这个事件，等价于“观测数据 $X$ 落在[真值](@entry_id:636547) $\theta$ 对应的接受域 $A_\theta$ 中”。因此，由接受域的定义，我们有 $P_\theta(\theta \in C(X)) = P_\theta(X \in A_\theta) \ge 1-\alpha$，这对所有 $\theta$ 都成立。

#### 离散数据与过覆盖

对于具有[离散样本空间](@entry_id:263580)（如泊松分布或二项分布）的模型，精确达到 $1-\alpha$ 的接受域概率通常是不可能的。为了满足 $\ge 1-\alpha$ 的条件，我们必须包含足够的离散数据点，直到累积概率超过 $1-\alpha$。这常常导致实际的覆盖率在某些 $\theta$ 值上严格大于名义的[置信水平](@entry_id:182309) $1-\alpha$，这种现象被称为**过覆盖**（over-coverage）。这是在处理离散数据时，即使是精确（非渐近）构造方法也固有的特性。

### 区间构造的典型方法

#### 理想模型：[方差](@entry_id:200758)已知的[正态分布](@entry_id:154414)

最简单的置信区间构造范例是处理一个均值为 $\mu$、[方差](@entry_id:200758) $\sigma^2$ 已知的[正态分布](@entry_id:154414)。假设我们有一系列独立同分布的观测值 $X_1, \dots, X_n$，其样本均值为 $\bar{X}$。根据[中心极限定理](@entry_id:143108)，[枢轴量](@entry_id:168397)（pivotal quantity）$Z = \frac{\sqrt{n}(\bar{X}-\mu)}{\sigma}$ 服从标准正态分布 $\mathcal{N}(0,1)$，其[分布](@entry_id:182848)不依赖于未知参数 $\mu$。

我们可以为 $Z$ 构建一个对称的接受域 $[-z_{1-\alpha/2}, z_{1-\alpha/2}]$，其中 $z_{1-\alpha/2}$ 是[标准正态分布](@entry_id:184509)的上 $(1-\alpha/2)$ 分位数。这个区间的概率恰好是 $1-\alpha$。
$$
P\left(-z_{1-\alpha/2} \le \frac{\sqrt{n}(\bar{X}-\mu)}{\sigma} \le z_{1-\alpha/2}\right) = 1-\alpha
$$
通过对不等式进行代数变换以分离出 $\mu$，我们得到 $\mu$ 的 $1-\alpha$ [置信区间](@entry_id:142297) ：
$$
\left[ \bar{X} - z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}}, \bar{X} + z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} \right]
$$
在这个理想化的模型中，由于[枢轴量](@entry_id:168397)的[分布](@entry_id:182848)是精确且不依赖于参数的，所构造区间的实际覆盖率对于所有 $\mu$ 值都精确地等于名义[置信水平](@entry_id:182309) $1-\alpha$ 。然而，在真实的[高能物理](@entry_id:181260)分析中，模型的不确定性、系统误差的存在等因素都会导致实际覆盖率偏离名义值。

#### 精确区间：[泊松分布](@entry_id:147769)的Garwood区间

在[高能物理](@entry_id:181260)中，事件计数通常遵循[泊松分布](@entry_id:147769)。对于一个观测到 $n$ 个事件的泊松过程，其均值 $\lambda$ 的精确[置信区间](@entry_id:142297)可以通过反演[泊松分布](@entry_id:147769)的累积分布函数（CDF）来获得。这种等尾（equal-tailed）区间通常被称为**Garwood区间**。

其端点 $\lambda_L$ 和 $\lambda_U$ 由以下两个方程定义，每个尾部概率为 $\alpha/2$：
1.  $P(N \ge n | \lambda_L) = \sum_{k=n}^{\infty} \frac{\exp(-\lambda_L)\lambda_L^k}{k!} = \frac{\alpha}{2}$
2.  $P(N \le n | \lambda_U) = \sum_{k=0}^{n} \frac{\exp(-\lambda_U)\lambda_U^k}{k!} = \frac{\alpha}{2}$

利用泊松和与卡方（$\chi^2$）[分布](@entry_id:182848)CDF之间的关系，这些方程可以被求解，得到解析表达式 ：
$$
\lambda_L(n) = \frac{1}{2} F_{\chi^2_{2n}}^{-1}\left(\frac{\alpha}{2}\right) \quad (\text{对于 } n>0, \lambda_L(0)=0)
$$
$$
\lambda_U(n) = \frac{1}{2} F_{\chi^2_{2(n+1)}}^{-1}\left(1 - \frac{\alpha}{2}\right)
$$
其中 $F_{\chi^2_{\nu}}^{-1}(p)$ 是自由度为 $\nu$ 的 $\chi^2$ [分布](@entry_id:182848)的 $p$-[分位数](@entry_id:178417)。例如，对于 $90\%$ 的[置信水平](@entry_id:182309)（$\alpha=0.1$），观测到 $n=5$ 个事件时，对应的置信区间约为 $[1.970, 10.51]$ 。

#### 排序规则与Feldman-Cousins统一方法

[Neyman构造](@entry_id:752484)中的一个关键自由度是选择接受域 $A_\theta$ 的**排序规则**（ordering rule），即对于给定的 $\theta$，如何对所有可能的观测值 $x$ 进行排序以决定哪些被优先放入接受域。不同的排序规则会产生具有不同性质（如区间长度、单边或双边）的[置信区间](@entry_id:142297)，但只要满足概率条件，都能保证覆盖率。

在HEP中，一个经典的问题是“**翻转**”（flip-flopping）：当观测数据不显著时（例如，观测到的事件数小于预期的本底），分析人员可能会放弃报告双边区间，转而报告一个单边上限。这种依赖于数据的、事后的程序选择会破坏名义覆盖率 。

**Feldman-Cousins统一方法**（Feldman-Cousins unified approach）通过引入一个固定的、基于[似然比](@entry_id:170863)的排序规则来解决这个问题 。对于给定的参数 $\mu$（例如信号强度），观测值 $n$ 按照似然比 $R(n;\mu)$ 的大小进行排序：
$$
R(n;\mu) = \frac{L(n|\mu)}{L(n|\hat{\mu}(n))}
$$
其中 $L(n|\mu)$ 是在参数为 $\mu$ 时的似然，而 $\hat{\mu}(n)$ 是在给定观测值 $n$ 的情况下，在物理允许范围内（例如 $\mu \ge 0$）最大化似然的参数估计值。接受域 $A_\mu$ 由 $R$ 值最大的那些 $n$ 组成。

这个方法的巧妙之处在于，它自然地处理了物理边界。当观测值 $n$ 远大于本底时，$\hat{\mu}(n)$ 通常为正，排序规则产生的接受域会导致双边置信区间。而当 $n$ 接近或小于本底时，$\hat{\mu}(n)$ 会饱和在边界值 $0$ 上，这使得排序规则倾向于将小的 $n$ 值排除在接受域之外，从而自然地产生单边上限形式的置信区间。通过在分析前固定这一排序规则，[Feldman-Cousins方法](@entry_id:749276)提供了一个“统一的”程序，避免了“翻转”问题，并严格保证了频率派的覆盖率 。

### 基于[似然](@entry_id:167119)的[渐近方法](@entry_id:177759)

对于复杂的多参数模型，精确的[Neyman构造](@entry_id:752484)可能在计算上不可行。在这种情况下，基于[似然函数](@entry_id:141927)[渐近性质](@entry_id:177569)的方法变得至关重要。

#### [剖面似然比](@entry_id:753793)与[Wilks定理](@entry_id:169826)

当模型中除了我们感兴趣的参数 $\theta$ 外，还存在其他不感兴趣的**[讨厌参数](@entry_id:171802)** $\eta$ 时，一个强大的工具是**[剖面似然比](@entry_id:753793)**（profile likelihood ratio）。其定义为：
$$
\lambda(\theta) = \frac{\sup_{\eta} L(\theta, \eta)}{\sup_{\theta', \eta'} L(\theta', \eta')} = \frac{L(\theta, \hat{\hat{\eta}}(\theta))}{L(\hat{\theta}, \hat{\eta})}
$$
其中 $(\hat{\theta}, \hat{\eta})$ 是似然函数的[全局最大值](@entry_id:174153)点，而 $\hat{\hat{\eta}}(\theta)$ 是在固定 $\theta$ 时，最大化[似然](@entry_id:167119)的 $\eta$ 的值。

**[Wilks定理](@entry_id:169826)**指出，在一系列“[正则性条件](@entry_id:166962)”下，当样本量趋于无穷时，检验统计量 $q_\theta = -2\ln\lambda(\theta)$ 的[分布](@entry_id:182848)会收敛到一个自由度为 $k$ 的 $\chi^2$ [分布](@entry_id:182848)，其中 $k$ 是被[原假设](@entry_id:265441)固定的参数个数。对于单个参数 $\theta$， $k=1$ 。这些[正则性条件](@entry_id:166962)包括：似然函数足够平滑、参数可识别、真实的参数值位于参数空间的**内部**等。

这个定理非常强大，因为它提供了一个近似的、不依赖于参数具体值的[枢轴量](@entry_id:168397)[分布](@entry_id:182848)，从而可以方便地构造近似[置信区间](@entry_id:142297)。例如，一个近似的 $95\%$ 置信区间可以通过求解 $q_\theta \le z$ 来获得，其中 $z$ 是 $\chi^2_1$ [分布](@entry_id:182848)的第95百[分位数](@entry_id:178417)（约为 $3.84$）。

在Wald近似下，[对数似然函数](@entry_id:168593)在最大似然估计 $\hat{\mu}$ 附近可以被展开为一个二次型。这揭示了 $-2\ln\lambda(\mu)$ 与 $(\mu - \hat{\mu})^2 / \sigma_{\hat{\mu}}^2$ 之间的直接联系，其中 $\sigma_{\hat{\mu}}^2$ 是 $\hat{\mu}$ 的[方差](@entry_id:200758)，由[对数似然函数](@entry_id:168593)[二阶导数](@entry_id:144508)的倒数（即Fisher信息的逆）给出 。例如，在一个泊松计数实验中，观测到 $n=26$ 个事件，预期本底 $b=20.3$，信号模板产额 $s_0=12.7$，利用这种[渐近方法](@entry_id:177759)可以估计出信号强度 $\mu$ 的 $95\%$ 单边上限约为 $1.109$ 。

### 实际应用中的挑战与高等课题

#### [讨厌参数](@entry_id:171802)的处理：剖面法与边缘化

在多参数模型中，[讨厌参数](@entry_id:171802)（nuisance parameters）是普遍存在的，例如本底产额、探测器效率、[能量尺度](@entry_id:196201)等。它们虽然不是我们首要关心的对象，但其不确定性会影响对主要参数（如信号强度 $s$）的推断 。

*   **剖面法（Profiling）**: 这是频率派统计的标准方法，正如前述。它通过最大化来消除[讨厌参数](@entry_id:171802)。只要满足[正则性条件](@entry_id:166962)，基于[剖面似然比](@entry_id:753793)构造的[置信区间](@entry_id:142297)就具有渐近的名义覆盖率。当用于约束[讨厌参数](@entry_id:171802)的[辅助测量](@entry_id:143842)信息越强（即[讨厌参数](@entry_id:171802)的不确定性越小），[渐近性质](@entry_id:177569)就表现得越好，实际覆盖率也越接近名义值 。

*   **[边缘化](@entry_id:264637)（Marginalization）**: 这是贝叶斯统计的方法。它通过乘以一个[先验概率](@entry_id:275634)[分布](@entry_id:182848) $\pi(b)$，然后对[讨厌参数](@entry_id:171802) $b$ 进行积分来将其消除，从而得到主要参数 $s$ 的边缘[后验概率](@entry_id:153467)[分布](@entry_id:182848)。由此构造出的是贝叶斯**可信区间**（credible interval）。重要的是，一个[贝叶斯可信区间](@entry_id:183625)不保证具有名义的频率派覆盖率。特别是，如果选择的[先验分布](@entry_id:141376)与事实严重不符（例如，一个过于集中的先验远离了真实值），即使模型本身是正确的，最终得到的可信区间也可能存在严重的**欠覆盖**（under-coverage） 。

#### [Wilks定理](@entry_id:169826)的失效：物理边界效应

[Wilks定理](@entry_id:169826)的一个关键[正则性条件](@entry_id:166962)是，被检验的参数真值必须在参数空间的内部。在高能物理中，许多参数（如信号强度 $\mu$、粒子质量的平方 $m^2$）具有物理边界，例如 $\mu \ge 0$。当检验原假设 $H_0: \mu=0$ 时，我们恰好在参数空间的边界上，这违反了[Wilks定理](@entry_id:169826)的条件 。

在这种情况下，检验统计量 $q_0 = -2\ln\lambda(0)$ 的[渐近分布](@entry_id:272575)不再是标准的 $\chi^2_1$ [分布](@entry_id:182848)。通过一个简单的例子可以说明这一点：假设我们有一个观测值 $x \sim \mathcal{N}(\mu, \sigma^2)$，且有约束 $\mu \ge 0$。当真实值为 $\mu=0$ 时，有一半的几率我们会观测到 $x \le 0$，此时最大似然估计 $\hat{\mu}=0$，导致 $q_0=0$。另一半几率我们会观测到 $x>0$，此时 $\hat{\mu}=x$，导致 $q_0 = (x/\sigma)^2 \sim \chi^2_1$。因此，$q_0$ 的[分布](@entry_id:182848)是一个[混合分布](@entry_id:276506) ：
$$
q_0 \sim \frac{1}{2}\delta(0) + \frac{1}{2}\chi^2_1
$$
其中 $\delta(0)$ 是在0处的点质量。这意味着，如果我们仍然使用标准 $\chi^2_1$ [分布](@entry_id:182848)的临界值（如95%对应3.84）来进行[假设检验](@entry_id:142556)或构造置信区间，我们的结论将会过于保守（即实际的覆盖率会高于名义值，例如达到97.5%）。正确的做法是使用这个[混合分布](@entry_id:276506)来计算临界值。例如，对于一个[显著性水平](@entry_id:170793)为 $\alpha=0.05$ 的检验，正确的临界值 $c$ 应该满足 $0.5 \times P(\chi^2_1 > c) = 0.05$，即 $P(\chi^2_1 > c) = 0.10$，解得 $c \approx 2.71$ 。值得注意的是，Feldman-Cousins这类精确的[Neyman构造](@entry_id:752484)方法由于不依赖[渐近理论](@entry_id:162631)，天生就能正确处理边界问题 。

#### [多重检验](@entry_id:636512)与“别处寻找效应”

在许多高能物理分析中，我们并非检验一个预先指定的信号假设，而是在一个连续的参数范围（如未知的[粒子质量](@entry_id:156313) $m$）内进行扫描搜索。这导致了**[多重假设检验](@entry_id:171420)**（multiple hypothesis testing）问题。即使在没有信号的情况下，当你在很多地方寻找时，仅凭统计涨落就看到一个局部显著的“信号”的可能性会大大增加。这种效应被称为“**别处寻找效应**”（Look-Elsewhere Effect, LEE）。

因此，报告在扫描中发现的最显著的涨落的**[局部p值](@entry_id:751406)**（local p-value）会产生误导。我们需要计算**[全局p值](@entry_id:749928)**（global p-value），即在纯本底假设下，在整个扫描范围内观测到至少一个与所见涨落同样或更显著的涨落的概率。如果一个扫描可以被近似为 $N_{\text{eff}}$ 个独立的检验，那么[局部p值](@entry_id:751406) $p_{\text{loc}}$ 和[全局p值](@entry_id:749928) $p_{\text{glob}}$ 的关系是 ：
$$
p_{\text{glob}} = 1 - (1 - p_{\text{loc}})^{N_{\text{eff}}} \approx N_{\text{eff}} \times p_{\text{loc}} \quad (\text{当 } p_{\text{loc}} \ll 1)
$$
例如，在一个有效独立检验次数 $N_{\text{eff}}=150$ 的扫描中，一个局部显著性为 $Z_{\text{loc}}=3.0$（对应 $p_{\text{loc}} \approx 1.35 \times 10^{-3}$）的涨落，其[全局p值](@entry_id:749928)约为 $0.18$，这通常不被认为是显著的信号证据。反之，在设定排除限时，为了控制在整个扫描范围内的族系误差率（family-wise error rate）为 $\alpha$，每个点的[局部p值](@entry_id:751406)阈值需要更严格，例如满足 $p_{\text{loc}} \le \alpha / N_{\text{eff}}$（[Bonferroni校正](@entry_id:261239)）。

#### 保守性极限：CLs方法

在信号微弱、实验灵敏度不足的情况下，纯粹的频率派检验（基于 $p_{s+b}$ 值）可能会因为本底的向下统计涨落而“排除”一个真实存在的信号。为了避免这种具有误导性的强结论，高能物理领域发展了**CLs方法**。

该方法通过修改检验判据，引入了对背景涨落的“惩罚”。它定义的统计量是 ：
$$
\mathrm{CL}_s = \frac{\mathrm{CL}_{s+b}}{\mathrm{CL}_b} = \frac{P(q \ge q_{\mathrm{obs}} | s+b)}{P(q \ge q_{\mathrm{obs}} | b)}
$$
其中 $q$ 是一个[检验统计量](@entry_id:167372)，$\mathrm{CL}_{s+b}$ 是信号+本底假设的p值，而 $\mathrm{CL}_b$ 是纯本底假设的[p值](@entry_id:136498)。排除判据变为 $\mathrm{CL}_s \le \alpha$。

由于 $\mathrm{CL}_b \le 1$，所以 $\mathrm{CL}_s \ge \mathrm{CL}_{s+b}$。这意味着 $\mathrm{CL}_s$ 方法比标准方法更难达到排除条件。当实验缺乏灵敏度时（即观测结果与纯本底假设也相差甚远，导致 $\mathrm{CL}_b$ 很小），$\mathrm{CL}_s$ 的值会被放大，从而防止了排除。例如，在预期本底 $b=5$、预期信号 $s=1$ 的实验中观测到 $n=1$ 个事件，标准检验可能会因为 $\mathrm{CL}_{s+b} \approx 0.0175  0.05$ 而排除信号，但因为此时 $\mathrm{CL}_b \approx 0.0404$ 也很小，导致 $\mathrm{CL}_s \approx 0.433$，从而避免了排除 。当实验灵敏度很高时（即观测结果与本底预期非常一致，$\mathrm{CL}_b \approx 1$），$\mathrm{CL}_s$ 方法则会收敛到标准的 $\mathrm{CL}_{s+b}$ 检验 。这种方法本质上是**保守的**，它以牺牲部分检验效力为代价，换取了在低灵敏度区域的稳健性，确保所设定的极限不是由统计侥幸驱动的。