{
    "hands_on_practices": [
        {
            "introduction": "The foundation of error propagation often begins with the linear approximation, where the variance of a function $f(x)$ is estimated as $\\sigma_f^2 \\approx (\\partial f/\\partial x)^2 \\sigma_x^2$. While powerful, this is a first-order Taylor approximation that neglects the curvature of the function. In precision measurements, or when an estimator exhibits significant non-linearity with respect to a nuisance parameter, these second-order effects can become a non-negligible source of uncertainty. This exercise  provides a clear, first-principles derivation to quantify this curvature-induced correction, offering a deeper understanding of error propagation beyond the linear regime.",
            "id": "3513019",
            "problem": "In a computational extraction of a resonance pole mass from a high-statistics sample, consider a maximum-likelihood estimator $\\hat{M}$ for the resonance mass that depends on a single dominant nuisance parameter $\\varepsilon$ representing a fractional global energy-scale offset. The calibration procedure estimates $\\varepsilon$ by averaging many independent channels, so by the Central Limit Theorem (CLT) the distribution of $\\varepsilon$ is well approximated as Gaussian with mean $0$ and known variance $\\sigma_{\\varepsilon}^{2}$. For sufficiently small $|\\varepsilon|$, the mapping $\\hat{M}(\\varepsilon)$ is smooth and can be expanded to second order about $\\varepsilon=0$ as\n$$\n\\hat{M}(\\varepsilon) = M_{0} + A\\,\\varepsilon + B\\,\\varepsilon^{2} + \\mathcal{O}(\\varepsilon^{3}),\n$$\nwhere $M_{0}$, $A$, and $B$ are constants determined by the fit model and data-taking conditions. Assume that any statistical fluctuations of $\\hat{M}$ unrelated to $\\varepsilon$ are negligible for this exercise, so that the only source of uncertainty is the randomness of $\\varepsilon$.\n\nStarting from first principles (the definition of variance and a second-order Taylor expansion), and assuming $\\varepsilon$ is Gaussian with mean $0$, derive an expression for $\\mathrm{Var}[\\hat{M}]$ up to and including terms of order $\\sigma_{\\varepsilon}^{4}$. In your derivation, identify the linear error-propagation contribution $\\mathrm{Var}_{\\mathrm{lin}} = A^{2}\\sigma_{\\varepsilon}^{2}$ and the additional curvature-induced correction $\\Delta \\mathrm{Var}$ that appears at order $\\sigma_{\\varepsilon}^{4}$. Provide the final symbolic expression for $\\Delta \\mathrm{Var}$ in terms of $A$, $B$, and $\\sigma_{\\varepsilon}$.\n\nDefine the ratio $R \\equiv \\Delta \\mathrm{Var}/\\mathrm{Var}_{\\mathrm{lin}}$ and evaluate it numerically for $A = 125.1\\,\\text{GeV}$, $B = 350\\,\\text{GeV}$, and $\\sigma_{\\varepsilon} = 3.0 \\times 10^{-3}$. Express the final numerical answer for $R$ as a pure number rounded to four significant figures. Do not include units in your final answer.",
            "solution": "The problem requires the derivation of the variance of a maximum-likelihood estimator $\\hat{M}$ which is a function of a Gaussian-distributed nuisance parameter $\\varepsilon$. The estimator is given by the second-order Taylor expansion:\n$$\n\\hat{M}(\\varepsilon) = M_{0} + A\\,\\varepsilon + B\\,\\varepsilon^{2}\n$$\nwhere we neglect terms of $\\mathcal{O}(\\varepsilon^{3})$ and higher for this derivation. The nuisance parameter $\\varepsilon$ follows a Gaussian distribution with mean $E[\\varepsilon]=0$ and variance $\\mathrm{Var}[\\varepsilon]=\\sigma_{\\varepsilon}^{2}$.\n\nThe variance of a random variable $X$ is defined as $\\mathrm{Var}[X] = E[(X - E[X])^2]$, where $E[\\cdot]$ denotes the expectation value. We will apply this definition to $X = \\hat{M}(\\varepsilon)$.\n\nFirst, we calculate the expectation value of $\\hat{M}$:\n$$\nE[\\hat{M}] = E[M_{0} + A\\,\\varepsilon + B\\,\\varepsilon^{2}]\n$$\nUsing the linearity of the expectation operator, and since $M_0$, $A$, and $B$ are constants:\n$$\nE[\\hat{M}] = M_{0} + A\\,E[\\varepsilon] + B\\,E[\\varepsilon^{2}]\n$$\nWe are given that $E[\\varepsilon] = 0$. The expectation value $E[\\varepsilon^{2}]$ is related to the variance of $\\varepsilon$ by the formula $\\mathrm{Var}[\\varepsilon] = E[\\varepsilon^{2}] - (E[\\varepsilon])^2$. Since $E[\\varepsilon]=0$, we have $E[\\varepsilon^{2}] = \\mathrm{Var}[\\varepsilon] = \\sigma_{\\varepsilon}^{2}$.\nSubstituting these values, we get the expectation of $\\hat{M}$:\n$$\nE[\\hat{M}] = M_{0} + A(0) + B(\\sigma_{\\varepsilon}^{2}) = M_{0} + B\\sigma_{\\varepsilon}^{2}\n$$\nNext, we calculate the term $\\hat{M} - E[\\hat{M}]$:\n$$\n\\hat{M} - E[\\hat{M}] = (M_{0} + A\\,\\varepsilon + B\\,\\varepsilon^{2}) - (M_{0} + B\\sigma_{\\varepsilon}^{2}) = A\\,\\varepsilon + B(\\varepsilon^{2} - \\sigma_{\\varepsilon}^{2})\n$$\nNow, we can compute the variance of $\\hat{M}$:\n$$\n\\mathrm{Var}[\\hat{M}] = E\\left[ \\left( A\\,\\varepsilon + B(\\varepsilon^{2} - \\sigma_{\\varepsilon}^{2}) \\right)^2 \\right]\n$$\nExpanding the square:\n$$\n\\mathrm{Var}[\\hat{M}] = E\\left[ A^2\\varepsilon^2 + 2AB\\varepsilon(\\varepsilon^2 - \\sigma_{\\varepsilon}^2) + B^2(\\varepsilon^2 - \\sigma_{\\varepsilon}^2)^2 \\right]\n$$\nBy linearity of expectation:\n$$\n\\mathrm{Var}[\\hat{M}] = A^2E[\\varepsilon^2] + 2AB\\,E[\\varepsilon^3 - \\varepsilon\\sigma_{\\varepsilon}^2] + B^2E[(\\varepsilon^2 - \\sigma_{\\varepsilon}^2)^2]\n$$\nTo evaluate this, we need the moments of the centered Gaussian distribution $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^2)$. The odd moments are zero, and the even moments are given by $E[\\varepsilon^n] = (n-1)!!\\,\\sigma_{\\varepsilon}^n$, where $(n-1)!! = (n-1)(n-3)\\cdots 1$.\nThe required moments are:\n$E[\\varepsilon] = 0$\n$E[\\varepsilon^2] = 1!!\\,\\sigma_{\\varepsilon}^2 = \\sigma_{\\varepsilon}^2$\n$E[\\varepsilon^3] = 0$\n$E[\\varepsilon^4] = 3!!\\,\\sigma_{\\varepsilon}^4 = 3\\sigma_{\\varepsilon}^4$\n\nLet's evaluate each term in the variance expression:\nThe first term is $A^2E[\\varepsilon^2] = A^2\\sigma_{\\varepsilon}^2$.\nThe second term is $2AB\\,(E[\\varepsilon^3] - \\sigma_{\\varepsilon}^2E[\\varepsilon]) = 2AB\\,(0 - \\sigma_{\\varepsilon}^2(0)) = 0$.\nThe third term is $B^2E[(\\varepsilon^2 - \\sigma_{\\varepsilon}^2)^2] = B^2E[\\varepsilon^4 - 2\\varepsilon^2\\sigma_{\\varepsilon}^2 + \\sigma_{\\varepsilon}^4]$.\nUsing linearity of expectation for the third term:\n$B^2(E[\\varepsilon^4] - 2\\sigma_{\\varepsilon}^2E[\\varepsilon^2] + E[\\sigma_{\\varepsilon}^4]) = B^2(3\\sigma_{\\varepsilon}^4 - 2\\sigma_{\\varepsilon}^2(\\sigma_{\\varepsilon}^2) + \\sigma_{\\varepsilon}^4) = B^2(3\\sigma_{\\varepsilon}^4 - 2\\sigma_{\\varepsilon}^4 + \\sigma_{\\varepsilon}^4) = 2B^2\\sigma_{\\varepsilon}^4$.\n\nCombining all terms, the total variance of $\\hat{M}$ up to order $\\sigma_{\\varepsilon}^4$ is:\n$$\n\\mathrm{Var}[\\hat{M}] = A^2\\sigma_{\\varepsilon}^2 + 2B^2\\sigma_{\\varepsilon}^4\n$$\nThe problem asks to identify the linear error propagation term, $\\mathrm{Var}_{\\mathrm{lin}} = A^2\\sigma_{\\varepsilon}^2$. This is the standard first-order approximation and corresponds to the first term in our derived expression. The remaining term is the curvature-induced correction $\\Delta\\mathrm{Var}$:\n$$\n\\Delta\\mathrm{Var} = 2B^2\\sigma_{\\varepsilon}^4\n$$\nThis is the required symbolic expression for $\\Delta\\mathrm{Var}$.\n\nThe ratio $R$ is defined as $R \\equiv \\Delta\\mathrm{Var}/\\mathrm{Var}_{\\mathrm{lin}}$. We can write this as:\n$$\nR = \\frac{2B^2\\sigma_{\\varepsilon}^4}{A^2\\sigma_{\\varepsilon}^2} = 2\\left(\\frac{B\\sigma_{\\varepsilon}}{A}\\right)^2\n$$\nNow, we substitute the given numerical values: $A = 125.1\\,\\text{GeV}$, $B = 350\\,\\text{GeV}$, and $\\sigma_{\\varepsilon} = 3.0 \\times 10^{-3}$. Note that $\\varepsilon$ is a fractional offset, so $\\sigma_\\varepsilon$ is dimensionless. The units of $A$ and $B$ will cancel in the ratio.\n$$\nR = 2\\left(\\frac{(350) \\cdot (3.0 \\times 10^{-3})}{125.1}\\right)^2\n$$\nFirst, calculate the argument of the square:\n$$\n\\frac{B\\sigma_{\\varepsilon}}{A} = \\frac{350 \\cdot 0.003}{125.1} = \\frac{1.05}{125.1} \\approx 0.0083932854\n$$\nNow, square this value and multiply by $2$:\n$$\nR \\approx 2 \\cdot (0.0083932854)^2 \\approx 2 \\cdot (7.044724 \\times 10^{-5}) \\approx 1.408945 \\times 10^{-4}\n$$\nRounding this result to four significant figures gives:\n$$\nR \\approx 1.409 \\times 10^{-4}\n$$",
            "answer": "$$\\boxed{1.409 \\times 10^{-4}}$$"
        },
        {
            "introduction": "Modern high-energy physics analyses frequently involve measurements across multiple kinematic bins or experimental channels. Systematic uncertainties, such as those related to detector calibration or theoretical modeling, often induce correlations between these channels. A common but perilous simplification is to ignore these off-diagonal terms in the covariance matrix, treating all uncertainties as independent. This practice can lead to a dramatic overestimation of the analysis's sensitivity and the significance of a potential signal. This hands-on coding exercise  demonstrates this pitfall numerically, tasking you with quantifying the spurious significance that arises when correlations are neglected.",
            "id": "3513018",
            "problem": "Consider a search for a small, spatially uniform excess across $n$ statistically similar analysis bins. Let the background-subtracted yield vector be $X \\in \\mathbb{R}^n$. Assume, by the Central Limit Theorem (CLT), that $X$ is approximately multivariate normal with mean $s \\in \\mathbb{R}^n$ (the excess template) and a covariance matrix $C \\in \\mathbb{R}^{n \\times n}$ that captures both statistical and systematic uncertainties. Suppose $C$ has an equicorrelation structure, namely $C_{ii} = \\sigma^2$ and $C_{ij} = \\rho \\,\\sigma^2$ for $i \\neq j$, where $-1/(n-1) < \\rho < 1$, and the excess template is flat with $s_i = s_0$ for all $i \\in \\{1,\\dots,n\\}$. Analysts sometimes neglect correlations by replacing $C$ with its diagonal $D = \\mathrm{diag}(C)$, which can lead to a spuriously large estimated significance when $\\rho > 0$.\n\nYour task is to formalize this as follows. Using only fundamental definitions of multivariate normal models and linear error propagation, define two asymptotic local significance estimates for the Asimov dataset:\n- The correlation-aware significance $Z_{\\text{true}}$ obtained by proper generalized least squares standardization of the linear shift $s$, and\n- The correlation-neglecting significance $Z_{\\text{naive}}$ obtained by replacing $C$ with $D$.\n\nThen, for specified parameter sets $(n,\\rho,\\sigma,s_0)$, compute numerically:\n- $Z_{\\text{true}}$,\n- $Z_{\\text{naive}}$,\n- the additive bias $B = Z_{\\text{naive}} - Z_{\\text{true}}$, and\n- the multiplicative bias $R = Z_{\\text{naive}} / Z_{\\text{true}}$.\n\nBase your derivation on the following fundamental facts only:\n- If $X \\sim \\mathcal{N}(\\mu, C)$ and $a \\in \\mathbb{R}^n$ is fixed, then $a^\\top X$ is univariate normal with mean $a^\\top \\mu$ and variance $a^\\top C a$.\n- For linear models with known covariance, generalized least squares standardization uses the quadratic form induced by $C^{-1}$.\n\nDesign and implement a program that, for each parameter set, constructs $C$ and $D$, forms $s$, and computes $Z_{\\text{true}}$ and $Z_{\\text{naive}}$ by appropriate linear-algebra operations. Do not assume or hard-code any special-case closed forms; your implementation should rely on general matrix-vector computations valid for any $n$ and any $\\rho$ in the stated range.\n\nTest suite to ensure coverage:\n- Case A (happy path, positive correlation): $n = 10$, $\\rho = 0.5$, $\\sigma = 1.0$, $s_0 = 1.0$.\n- Case B (boundary, no correlation): $n = 10$, $\\rho = 0.0$, $\\sigma = 1.0$, $s_0 = 1.0$.\n- Case C (edge, strong positive correlation): $n = 10$, $\\rho = 0.9$, $\\sigma = 1.0$, $s_0 = 1.0$.\n- Case D (edge, negative correlation within the allowed range): $n = 10$, $\\rho = -0.1$, $\\sigma = 1.0$, $s_0 = 1.0$.\n\nAll answers must be expressed as dimensionless floating-point numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each case reported as a sub-list of the form $[Z_{\\text{true}}, Z_{\\text{naive}}, B, R]$, and each floating-point entry rounded to $6$ decimal places. For example, the final output format must be like $[[x_{11},x_{12},x_{13},x_{14}],[x_{21},x_{22},x_{23},x_{24}],\\dots]$ for the four test cases in the order A, B, C, D.",
            "solution": "The user-provided problem is assessed to be **valid**. It is scientifically grounded in statistical principles commonly used in high-energy physics, self-contained, and well-posed. The problem asks for the derivation and computation of two significance estimates for a flat signal excess across multiple correlated analysis bins, highlighting the effect of neglecting these correlations.\n\nThe solution proceeds by first formalizing the definition of significance based on the principles of generalized least squares (GLS), and then applying this definition to both the true, correlated model and a naive, uncorrelated approximation.\n\n### Theoretical Framework for Significance\n\nThe problem considers a background-subtracted yield vector $X \\in \\mathbb{R}^n$ that follows a multivariate normal distribution, $X \\sim \\mathcal{N}(s, C)$, where $s$ is the mean signal vector and $C$ is the $n \\times n$ covariance matrix. The significance of detecting the signal $s$ is a measure of how distinguishable the signal hypothesis ($H_1: \\mu = s$) is from the null hypothesis ($H_0: \\mu = 0$) given the measurement uncertainties encapsulated by $C$.\n\nThe \"Asimov dataset\" is used to compute the expected significance, which means we evaluate the test statistic at the expectation value of the data under the signal hypothesis, i.e., at $X=s$.\n\nTo find the optimal test statistic, we seek a linear combination of the measurements, $y = a^\\top X$ for some constant vector $a \\in \\mathbb{R}^n$, that maximizes the signal-to-noise ratio. From the provided fact, if $X \\sim \\mathcal{N}(s, C)$, then the scalar $y$ is univariate normal, $y \\sim \\mathcal{N}(a^\\top s, a^\\top C a)$. The significance of this scalar measurement is its mean divided by its standard deviation:\n$$ Z(a) = \\frac{a^\\top s}{\\sqrt{a^\\top C a}} $$\nThe vector $a$ that maximizes this quantity is the one that optimally weights the contributions from each bin according to their signals and correlated uncertainties. This is a standard result from generalized least squares or matched filtering, and the optimal weight vector is given by $a \\propto C^{-1}s$. Substituting $a = C^{-1}s$ gives the maximized significance:\n$$ Z = \\frac{(C^{-1}s)^\\top s}{\\sqrt{(C^{-1}s)^\\top C (C^{-1}s)}} = \\frac{s^\\top (C^{-1})^\\top s}{\\sqrt{s^\\top C^{-1} C C^{-1} s}} $$\nSince $C$ is symmetric, its inverse $C^{-1}$ is also symmetric, thus $(C^{-1})^\\top = C^{-1}$. The expression simplifies to:\n$$ Z = \\frac{s^\\top C^{-1} s}{\\sqrt{s^\\top C^{-1} s}} = \\sqrt{s^\\top C^{-1} s} $$\nThis quadratic form, $s^\\top C^{-1} s$, is the squared Mahalanobis distance of the signal vector $s$ from the origin, scaled by the covariance $C$. It represents the squared significance.\n\n### Correlation-Aware Significance ($Z_{\\text{true}}$)\n\nThe true significance, $Z_{\\text{true}}$, is calculated using the full covariance matrix $C$ as specified in the problem. The model parameters are:\n- A flat signal template: $s \\in \\mathbb{R}^n$ with $s_i = s_0$ for all $i=1, \\dots, n$.\n- An equicorrelated covariance matrix $C \\in \\mathbb{R}^{n \\times n}$ with diagonal elements $C_{ii} = \\sigma^2$ and off-diagonal elements $C_{ij} = \\rho \\sigma^2$ for $i \\neq j$.\n\nApplying the general formula for significance, we get:\n$$ Z_{\\text{true}} = \\sqrt{s^\\top C^{-1} s} $$\nThe computation requires constructing the matrix $C$ and its inverse $C^{-1}$ (or, more preferably for numerical stability, solving the linear system $Cy = s$) to evaluate the quadratic form.\n\n### Correlation-Neglecting Significance ($Z_{\\text{naive}}$)\n\nThe naive significance estimate, $Z_{\\text{naive}}$, arises from incorrectly assuming the measurements are uncorrelated. This is equivalent to replacing the true covariance matrix $C$ with its diagonal part, $D = \\mathrm{diag}(C)$. For the given model:\n- $D$ is a diagonal matrix with $D_{ii} = C_{ii} = \\sigma^2$ for all $i$. Thus, $D = \\sigma^2 I$, where $I$ is the $n \\times n$ identity matrix.\n\nThe significance is computed using the same formula but with $C$ replaced by $D$:\n$$ Z_{\\text{naive}} = \\sqrt{s^\\top D^{-1} s} $$\nSince $D$ is diagonal, its inverse is trivial: $D^{-1} = (1/\\sigma^2)I$. The quadratic form becomes:\n$$ s^\\top D^{-1} s = s^\\top \\left(\\frac{1}{\\sigma^2}I\\right) s = \\frac{1}{\\sigma^2} s^\\top s = \\frac{1}{\\sigma^2} \\sum_{i=1}^n s_i^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^n s_0^2 = \\frac{n s_0^2}{\\sigma^2} $$\nThus, the naive significance has a simple closed form:\n$$ Z_{\\text{naive}} = \\sqrt{\\frac{n s_0^2}{\\sigma^2}} = \\frac{|s_0| \\sqrt{n}}{\\sigma} $$\nHowever, per the problem specification, the implementation will use general linear algebra operations rather than this analytical result.\n\n### Bias Metrics\n\nThe discrepancy between the naive and true significance is quantified by:\n- The additive bias: $B = Z_{\\text{naive}} - Z_{\\text{true}}$\n- The multiplicative bias (or ratio): $R = Z_{\\text{naive}} / Z_{\\text{true}}$\n\n### Algorithmic Implementation\n\nFor each parameter set $(n, \\rho, \\sigma, s_0)$, the following steps are executed:\n1.  Construct the signal vector $s \\in \\mathbb{R}^n$ where each element is $s_0$.\n2.  Construct the true covariance matrix $C \\in \\mathbb{R}^{n \\times n}$ such that $C_{ii} = \\sigma^2$ and $C_{ij} = \\rho \\sigma^2$ for $i \\neq j$.\n3.  Calculate $Z_{\\text{true}}$:\n    - Solve the linear system of equations $C y = s$ for the vector $y = C^{-1}s$.\n    - Compute the dot product $q_{\\text{true}} = s^\\top y$.\n    - $Z_{\\text{true}} = \\sqrt{q_{\\text{true}}}$.\n4.  Construct the naive (diagonal) covariance matrix $D = \\sigma^2 I$.\n5.  Calculate $Z_{\\text{naive}}$:\n    - Solve the linear system $D z = s$ for the vector $z = D^{-1}s$.\n    - Compute the dot product $q_{\\text{naive}} = s^\\top z$.\n    - $Z_{\\text{naive}} = \\sqrt{q_{\\text{naive}}}$.\n6.  Compute the biases $B = Z_{\\text{naive}} - Z_{\\text{true}}$ and $R = Z_{\\text{naive}} / Z_{\\text{true}}$.\n7.  Collect the four results $[Z_{\\text{true}}, Z_{\\text{naive}}, B, R]$ for each parameter set and format them for the final output. This numerical procedure adheres to the problem requirement of using general matrix operations.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes true and naive significance estimates for a correlated signal search.\n    \"\"\"\n    test_cases = [\n        # Case A: (n, rho, sigma, s0)\n        (10, 0.5, 1.0, 1.0),\n        # Case B:\n        (10, 0.0, 1.0, 1.0),\n        # Case C:\n        (10, 0.9, 1.0, 1.0),\n        # Case D:\n        (10, -0.1, 1.0, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, rho, sigma, s0 = case\n\n        # 1. Construct the signal vector s\n        s = np.full(n, s0, dtype=np.float64)\n\n        # 2. Construct the true covariance matrix C\n        # C = sigma^2 * ((1-rho)*I + rho*J)\n        # where I is identity and J is matrix of ones.\n        C = np.full((n, n), rho * sigma**2, dtype=np.float64)\n        # Add the diagonal part\n        np.fill_diagonal(C, sigma**2)\n\n        # 3. Calculate Z_true\n        # Z_true = sqrt(s^T * C^-1 * s)\n        # We solve C*y = s for y = C^-1*s, then compute sqrt(s^T * y)\n        # This is more numerically stable than computing the inverse explicitly.\n        y = np.linalg.solve(C, s)\n        z_true_sq = s @ y\n        # The quadratic form must be non-negative for a valid covariance matrix.\n        # Small numerical errors might make it slightly negative.\n        z_true = np.sqrt(max(0, z_true_sq))\n\n        # 4. Construct the naive (diagonal) covariance matrix D\n        # D = sigma^2 * I\n        D = np.diag(np.full(n, sigma**2, dtype=np.float64))\n\n        # 5. Calculate Z_naive\n        # Z_naive = sqrt(s^T * D^-1 * s)\n        # The problem requires using general matrix operations.\n        # We solve D*z = s for z = D^-1*s.\n        z = np.linalg.solve(D, s)\n        z_naive_sq = s @ z\n        z_naive = np.sqrt(max(0, z_naive_sq))\n\n        # 6. Compute biases B and R\n        bias_additive = z_naive - z_true\n        # Avoid division by zero if z_true is zero (e.g., s0=0).\n        # In this problem, z_true > 0 for all test cases.\n        bias_multiplicative = z_naive / z_true if z_true != 0 else np.inf\n\n        results.append([z_true, z_naive, bias_additive, bias_multiplicative])\n\n    # Format the output exactly as specified.\n    # The output is a string representation of a list of lists.\n    # Each float is formatted to 6 decimal places.\n    # Example: [[v11,v12,...],[v21,v22,...]] with no spaces.\n    sub_list_strs = [\n        f\"[{','.join(f'{x:.6f}' for x in sub)}]\" for sub in results\n    ]\n    final_output = f\"[{','.join(sub_list_strs)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Having appreciated the importance of non-linearity and correlations, the final step is to master the formal techniques for combining different experimental results. This practice focuses on two mathematically equivalent pillars of data combination: the Best Linear Unbiased Estimator (BLUE), which operates on the effective total covariance matrix, and the joint likelihood fit, which explicitly profiles the underlying nuisance parameters. This comprehensive exercise  will guide you through implementing both approaches from scratch, providing a robust computational tool for data combination and a profound insight into the statistical machinery that underpins it.",
            "id": "3513023",
            "problem": "You are given a set of linear-Gaussian measurement models for combining experiment-level estimates of a dimensionless signal strength. Each experiment produces a scalar estimator that, by the Central Limit Theorem, may be treated as approximately Gaussian. The experiments share some systematic effects that are common across experiments and modeled as Gaussian-constrained nuisance parameters, and they also include uncorrelated effects. From a computational perspective, the task is to implement two mathematically equivalent combinations of these measurements: a Best Linear Unbiased Estimator that uses the full covariance matrix, and a joint-likelihood combination that profiles Gaussian-constrained nuisance parameters. The combination must be carried out using linear algebra only, and all reported numbers are dimensionless.\n\nFundamental starting assumptions:\n- The Central Limit Theorem implies that the sampling distribution of per-experiment estimators is well-approximated by a multivariate normal distribution for sufficiently large counts, even for counting experiments common in high-energy physics.\n- Gaussian error propagation in linear models with Gaussian priors is closed under marginalization and profiling, producing a multivariate normal form with an effective covariance obtained by summing uncorrelated and correlated contributions.\n\nGeneral setting for a single test case:\n- There are $N$ experiments providing an observed vector $y \\in \\mathbb{R}^{N}$, each component an approximate Gaussian draw centered around a common but unknown scalar parameter $\\mu$ (the dimensionless signal strength).\n- The measurement model is linear: $y = \\mu \\mathbf{1} + B \\theta + \\varepsilon$. Here, $\\mathbf{1}$ is the $N$-vector of ones, $B \\in \\mathbb{R}^{N \\times m}$ encodes how the $m$ shared nuisance parameters $\\theta \\in \\mathbb{R}^{m}$ shift the experiments, and $\\varepsilon \\sim \\mathcal{N}(0, D)$ is zero-mean Gaussian noise with diagonal covariance $D = \\mathrm{diag}(\\sigma_1^2, \\ldots, \\sigma_N^2)$ capturing uncorrelated components (statistical plus experiment-specific systematics). The nuisance parameters have a zero-mean Gaussian prior $\\theta \\sim \\mathcal{N}(0, T)$ with symmetric positive-definite covariance $T \\in \\mathbb{R}^{m \\times m}$. All quantities are dimensionless.\n- The effective total covariance after propagating the shared systematics is $S = D + B T B^{\\top}$.\n\nTasks to implement per test case:\n1. Construct $D$, $B$, and $T$ from the provided inputs and compute $S = D + B T B^{\\top}$.\n2. Compute the Best Linear Unbiased Estimator (BLUE) for $\\mu$ using the full covariance $S$, along with its standard deviation. Do not use any prepackaged combination formula; derive the estimator from the normal equations implied by linear-Gaussian error propagation.\n3. Compute the joint-likelihood combination by forming the penalized Gaussian negative log-likelihood for $(\\mu, \\theta)$ with the given $D$ and $T$, and then solving the corresponding linear system to profile out $\\theta$ and obtain the maximum-likelihood estimate for $\\mu$ and its standard deviation from the curvature. Again, use linear algebra; no numerical optimization loops are permitted.\n4. Report the absolute differences between the two estimates and between the two standard deviations as checks of equivalence.\n5. All quantities are dimensionless. All matrix inversions must be performed in a numerically stable way using linear solvers rather than explicit inverses where possible.\n\nTest suite:\nFor each test case $i$, you are given the tuple $(y^{(i)}, \\sigma^{(i)}, B^{(i)}, T^{(i)})$, where $y^{(i)}$ is a list of length $N$, $\\sigma^{(i)}$ is a list of length $N$ whose square defines the diagonal of $D$, $B^{(i)}$ is an $N \\times m$ matrix (possibly with $m=0$), and $T^{(i)}$ is an $m \\times m$ symmetric positive-definite matrix (possibly empty when $m=0$).\n\nUse the following test cases:\n- Case A: $N=3$, one shared nuisance ($m=1$).\n  - $y = [\\,1.05,\\,0.98,\\,1.10\\,]$\n  - $\\sigma = [\\,0.10,\\,0.12,\\,0.08\\,]$\n  - $$B = \\begin{bmatrix} 0.20 \\\\ 0.10 \\\\ 0.15 \\end{bmatrix}$$\n  - $$T = \\begin{bmatrix} 1.0 \\end{bmatrix}$$\n- Case B: $N=4$, two shared nuisances ($m=2$).\n  - $y = [\\,0.90,\\,1.20,\\,1.00,\\,1.05\\,]$\n  - $\\sigma = [\\,0.25,\\,0.20,\\,0.15,\\,0.18\\,]$\n  - $$B = \\begin{bmatrix} 0.30 & 0.05 \\\\ 0.25 & 0.05 \\\\ 0.05 & 0.20 \\\\ 0.05 & 0.25 \\end{bmatrix}$$\n  - $$T = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$$\n- Case C: $N=2$, one shared nuisance ($m=1$), near-strong correlation.\n  - $y = [\\,1.01,\\,0.99\\,]$\n  - $\\sigma = [\\,0.02,\\,0.02\\,]$\n  - $$B = \\begin{bmatrix} 0.05 \\\\ 0.05 \\end{bmatrix}$$\n  - $$T = \\begin{bmatrix} 1.0 \\end{bmatrix}$$\n- Case D: $N=5$, no shared nuisances ($m=0$).\n  - $y = [\\,1.00,\\,1.02,\\,0.97,\\,1.03,\\,0.99\\,]$\n  - $\\sigma = [\\,0.10,\\,0.10,\\,0.10,\\,0.10,\\,0.10\\,]$\n  - $B$ has shape $5 \\times 0$ (empty), $T$ is $0 \\times 0$ (empty).\n- Case E: $N=3$, two shared nuisances ($m=2$) with correlated priors and nearly collinear couplings.\n  - $y = [\\,0.95,\\,1.05,\\,1.00\\,]$\n  - $\\sigma = [\\,0.05,\\,0.05,\\,0.05\\,]$\n  - $$B = \\begin{bmatrix} 0.20 & 0.40 \\\\ 0.19 & 0.39 \\\\ 0.21 & 0.41 \\end{bmatrix}$$\n  - $$T = \\begin{bmatrix} 1.0 & 0.6 \\\\ 0.6 & 2.0 \\end{bmatrix}$$\n\nRequired outputs:\n- For each test case, compute and return a list of six floating-point numbers:\n  - $[\\mu_{\\mathrm{BLUE}},\\,\\sigma_{\\mathrm{BLUE}},\\,\\mu_{\\mathrm{joint}},\\,\\sigma_{\\mathrm{joint}},\\,|\\mu_{\\mathrm{BLUE}}-\\mu_{\\mathrm{joint}}|,\\,|\\sigma_{\\mathrm{BLUE}}-\\sigma_{\\mathrm{joint}}|]$.\n- Your program should produce a single line of output containing the results as a comma-separated list of per-case lists, with no spaces, enclosed in a single pair of square brackets, for example: \"[[a,b,c,d,e,f],[...],...]\".\n- To ensure deterministic comparison, each floating-point number in the final line must be rounded to $12$ significant digits before printing.\n\nAll quantities are dimensionless. Angles are not involved. Express differences as plain decimals (not percentages).",
            "solution": "The user has provided a problem concerning the combination of measurements in the presence of correlated systematic uncertainties, a common task in experimental science, particularly high-energy physics. The problem is to implement and verify the equivalence of two statistical methods for this combination: the Best Linear Unbiased Estimator (BLUE) using a full covariance matrix, and a direct joint-likelihood fit where nuisance parameters are profiled. The problem is scientifically grounded, well-posed, and all necessary information is provided. It constitutes a valid and substantive exercise in computational statistics and linear algebra.\n\n### Theoretical Derivations\n\nLet there be $N$ measurements provided as a vector $y \\in \\mathbb{R}^{N}$. Each measurement $y_i$ is an estimator of a common true signal strength $\\mu$. The measurement model is given as:\n$$\ny = \\mu \\mathbf{1} + B \\theta + \\varepsilon\n$$\nwhere $\\mathbf{1}$ is the $N$-vector of ones, $B \\in \\mathbb{R}^{N \\times m}$ is a matrix describing the response of each measurement to $m$ shared nuisance parameters $\\theta \\in \\mathbb{R}^{m}$, and $\\varepsilon \\in \\mathbb{R}^{N}$ represents uncorrelated statistical and systematic uncertainties. The uncertainties are modeled as zero-mean Gaussian distributions: $\\varepsilon \\sim \\mathcal{N}(0, D)$ where $D = \\mathrm{diag}(\\sigma_1^2, \\ldots, \\sigma_N^2)$ is a diagonal covariance matrix. The nuisance parameters are constrained by a Gaussian prior, $\\theta \\sim \\mathcal{N}(0, T)$, where $T$ is their $m \\times m$ covariance matrix.\n\n#### Method 1: Best Linear Unbiased Estimator (BLUE)\n\nThis method first marginalizes over the nuisance parameters $\\theta$ to obtain an effective likelihood for $\\mu$ alone. The variance from the nuisance parameters, $B \\mathrm{Cov}(\\theta) B^\\top = B T B^\\top$, is propagated to the measurements. The total covariance matrix $S$ for the vector $y$ is the sum of the uncorrelated and correlated components:\n$$\nS = D + B T B^{\\top}\n$$\nThe measurement vector $y$ is now considered a single draw from a multivariate normal distribution:\n$$\ny \\sim \\mathcal{N}(\\mu \\mathbf{1}, S)\n$$\nThe negative log-likelihood function (up to an additive constant) for $\\mu$ is:\n$$\n-\\ln L(\\mu) = \\frac{1}{2} (y - \\mu \\mathbf{1})^{\\top} S^{-1} (y - \\mu \\mathbf{1})\n$$\nTo find the maximum likelihood estimator $\\hat{\\mu}_{\\mathrm{BLUE}}$, we differentiate with respect to $\\mu$ and set the result to zero:\n$$\n\\frac{\\partial(-\\ln L)}{\\partial \\mu} = -\\mathbf{1}^{\\top} S^{-1} (y - \\hat{\\mu}_{\\mathrm{BLUE}} \\mathbf{1}) = 0\n$$\nSolving for $\\hat{\\mu}_{\\mathrm{BLUE}}$ yields:\n$$\n\\hat{\\mu}_{\\mathrm{BLUE}} = \\frac{\\mathbf{1}^{\\top} S^{-1} y}{\\mathbf{1}^{\\top} S^{-1} \\mathbf{1}}\n$$\nThe variance of this estimator, $\\sigma^2_{\\hat{\\mu}_{\\mathrm{BLUE}}}$, is found from the inverse of the second derivative of the negative log-likelihood (the Fisher information):\n$$\n\\frac{\\partial^2(-\\ln L)}{\\partial \\mu^2} = \\mathbf{1}^{\\top} S^{-1} \\mathbf{1}\n$$\nThus, the variance and standard deviation are:\n$$\n\\sigma^2_{\\hat{\\mu}_{\\mathrm{BLUE}}} = (\\mathbf{1}^{\\top} S^{-1} \\mathbf{1})^{-1} \\quad \\implies \\quad \\sigma_{\\hat{\\mu}_{\\mathrm{BLUE}}} = \\sqrt{(\\mathbf{1}^{\\top} S^{-1} \\mathbf{1})^{-1}}\n$$\nFor numerical stability, instead of computing $S^{-1}$ explicitly, we can solve the linear system $S v = \\mathbf{1}$ for $v = S^{-1}\\mathbf{1}$. The estimator and its variance become:\n$$\n\\hat{\\mu}_{\\mathrm{BLUE}} = \\frac{v^{\\top} y}{v^{\\top} \\mathbf{1}}, \\qquad \\sigma^2_{\\hat{\\mu}_{\\mathrm{BLUE}}} = \\frac{1}{v^{\\top} \\mathbf{1}}\n$$\n\n#### Method 2: Joint Likelihood with Nuisance Parameter Profiling\n\nThis method constructs a joint likelihood for both the parameter of interest $\\mu$ and the nuisance parameters $\\theta$. The penalized negative log-likelihood, or $\\chi^2$, is the sum of the term for the measurements and the term for the nuisance parameter constraints:\n$$\n\\chi^2(\\mu, \\theta) = (y - \\mu \\mathbf{1} - B \\theta)^{\\top} D^{-1} (y - \\mu \\mathbf{1} - B \\theta) + \\theta^{\\top} T^{-1} \\theta\n$$\nWe minimize this $\\chi^2$ jointly with respect to $\\mu$ and $\\theta$ by setting the partial derivatives to zero. This yields a system of linear equations. Let $p = (\\mu, \\theta_1, \\ldots, \\theta_m)^\\top$ be the vector of all parameters. The system is $H p = V$, where $H$ is the Hessian matrix of $\\frac{1}{2}\\chi^2$ and $V$ is derived from the gradient.\n$$\nH = \\frac{1}{2}\\frac{\\partial^2 \\chi^2}{\\partial p^2} = \\begin{pmatrix}\n\\mathbf{1}^{\\top} D^{-1} \\mathbf{1} & \\mathbf{1}^{\\top} D^{-1} B \\\\\nB^{\\top} D^{-1} \\mathbf{1} & B^{\\top} D^{-1} B + T^{-1}\n\\end{pmatrix}\n$$\n$$\nV = \\begin{pmatrix}\n\\mathbf{1}^{\\top} D^{-1} y \\\\\nB^{\\top} D^{-1} y\n\\end{pmatrix}\n$$\nThe solution to $H p = V$ provides the estimates $(\\hat{\\mu}_{\\mathrm{joint}}, \\hat{\\theta}_{\\mathrm{joint}}^\\top)^\\top$. The first component is the desired estimate for $\\mu$. The covariance matrix of the parameters is given by $H^{-1}$. The variance of $\\hat{\\mu}_{\\mathrm{joint}}$ is the top-left element of this inverse matrix:\n$$\n\\sigma^2_{\\hat{\\mu}_{\\mathrm{joint}}} = (H^{-1})_{00}\n$$\nThe standard deviation $\\sigma_{\\hat{\\mu}_{\\mathrm{joint}}}$ is the square root of this variance.\n\nThe mathematical equivalence of the two methods can be shown using the Sherman-Morrison-Woodbury formula for matrix inverses, which states:\n$$\n(D + B T B^{\\top})^{-1} = D^{-1} - D^{-1}B(T^{-1} + B^{\\top}D^{-1}B)^{-1}B^{\\top}D^{-1}\n$$\nUsing this identity, one can demonstrate that the expressions for both the estimator $\\hat{\\mu}$ and its variance $\\sigma^2_{\\hat{\\mu}}$ are identical in both formalisms. The implementation will demonstrate this equivalence numerically.\n\n### Implementation Strategy\n\nFor each test case, the implementation will proceed as follows:\n1.  Construct the matrices $D$, $B$, and $T$ from the input lists and convert them to `numpy` arrays. The vector of ones, $\\mathbf{1}$, is also created.\n\n2.  For the **BLUE method**:\n    a. Compute the total covariance matrix $S = D + B T B^{\\top}$. Note that if $m=0$, $BTB^{\\top}$ is a zero matrix.\n    b. Solve the linear system $S v = \\mathbf{1}$ for the vector $v$.\n    c. Calculate $\\hat{\\mu}_{\\mathrm{BLUE}} = (v^\\top y) / (v^\\top \\mathbf{1})$ and $\\sigma_{\\mathrm{BLUE}} = \\sqrt{1 / (v^\\top \\mathbf{1})}$.\n\n3.  For the **joint likelihood method**:\n    a. A special case is handled for $m=0$ nuisances. Here, the model simplifies to a standard weighted average, $y \\sim \\mathcal{N}(\\mu\\mathbf{1}, D)$, and the results are computed directly from the formulas $\\hat{\\mu}_{\\mathrm{joint}} = (\\sum y_i/\\sigma_i^2) / (\\sum 1/\\sigma_i^2)$ and $\\sigma^2_{\\mathrm{joint}} = 1 / (\\sum 1/\\sigma_i^2)$.\n    b. For $m>0$, construct the $(m+1) \\times (m+1)$ Hessian matrix $H$ and the $(m+1)$ right-hand side vector $V$ as defined in the theoretical derivation. This involves inverting $T$ and using the diagonal $D^{-1}$.\n    c. Solve the linear system $H p = V$ for the parameter vector $p$. The estimate $\\hat{\\mu}_{\\mathrm{joint}}$ is the first element of $p$.\n    d. Compute the inverse of the Hessian, $H^{-1}$. The variance $\\sigma^2_{\\hat{\\mu}_{\\mathrm{joint}}}$ is the first diagonal element, $(H^{-1})_{00}$. The standard deviation is its square root.\n\n4.  Finally, the absolute differences $|\\hat{\\mu}_{\\mathrm{BLUE}} - \\hat{\\mu}_{\\mathrm{joint}}|$ and $|\\sigma_{\\mathrm{BLUE}} - \\sigma_{\\mathrm{joint}}|$ are computed to verify the equivalence.\n\n5.  All six resulting floating-point numbers for each case are rounded to $12$ significant digits and formatted into the required output string.",
            "answer": "```python\nimport numpy as np\n\ndef format_num__to_12_sig_digits(n):\n    \"\"\"Formats a number to a string with 12 significant digits.\"\"\"\n    return f\"{n:.12g}\"\n\ndef solve():\n    \"\"\"\n    Solves the measurement combination problem for a suite of test cases,\n    demonstrating the equivalence of the BLUE and joint-likelihood profiling methods.\n    \"\"\"\n    test_cases = [\n        # Case A: N=3, m=1\n        {\n            'y': np.array([1.05, 0.98, 1.10]),\n            'sigma': np.array([0.10, 0.12, 0.08]),\n            'B': np.array([[0.20], [0.10], [0.15]]),\n            'T': np.array([[1.0]])\n        },\n        # Case B: N=4, m=2\n        {\n            'y': np.array([0.90, 1.20, 1.00, 1.05]),\n            'sigma': np.array([0.25, 0.20, 0.15, 0.18]),\n            'B': np.array([[0.30, 0.05], [0.25, 0.05], [0.05, 0.20], [0.05, 0.25]]),\n            'T': np.array([[1.0, 0.0], [0.0, 1.0]])\n        },\n        # Case C: N=2, m=1\n        {\n            'y': np.array([1.01, 0.99]),\n            'sigma': np.array([0.02, 0.02]),\n            'B': np.array([[0.05], [0.05]]),\n            'T': np.array([[1.0]])\n        },\n        # Case D: N=5, m=0\n        {\n            'y': np.array([1.00, 1.02, 0.97, 1.03, 0.99]),\n            'sigma': np.array([0.10, 0.10, 0.10, 0.10, 0.10]),\n            'B': np.empty((5, 0)),\n            'T': np.empty((0, 0))\n        },\n        # Case E: N=3, m=2\n        {\n            'y': np.array([0.95, 1.05, 1.00]),\n            'sigma': np.array([0.05, 0.05, 0.05]),\n            'B': np.array([[0.20, 0.40], [0.19, 0.39], [0.21, 0.41]]),\n            'T': np.array([[1.0, 0.6], [0.6, 2.0]])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        y, sigma, B, T = case['y'], case['sigma'], case['B'], case['T']\n        N = len(y)\n        m = B.shape[1]\n        \n        ones = np.ones(N)\n        D = np.diag(sigma**2)\n\n        # --- Method 1: BLUE ---\n        # S = D + B @ T @ B.T handles m=0 case correctly (B@T@B.T is zero matrix)\n        S = D + B @ T @ B.T\n        \n        # Solve Sv = 1 for v = S^-1 * 1\n        v = np.linalg.solve(S, ones)\n        \n        # mu = (v.T @ y) / (v.T @ 1)\n        mu_blue = (v @ y) / (v @ ones)\n        # sigma^2 = 1 / (v.T @ 1)\n        sigma_blue = np.sqrt(1 / (v @ ones))\n\n        # --- Method 2: Joint Likelihood Profiling ---\n        D_inv_diag = 1 / sigma**2\n        D_inv = np.diag(D_inv_diag)\n        \n        if m == 0:\n            # Simplified case: weighted average\n            weights = D_inv_diag\n            mu_joint = np.sum(weights * y) / np.sum(weights)\n            sigma_joint = np.sqrt(1 / np.sum(weights))\n        else:\n            # Build the Hessian H and vector V for the system Hp = V\n            # p = [mu, theta_1, ..., theta_m].T\n            A_block = np.sum(D_inv_diag)\n            C_block = ones @ D_inv @ B\n            \n            T_inv = np.linalg.inv(T)\n            G_block = B.T @ D_inv @ B + T_inv\n            \n            H = np.zeros((1 + m, 1 + m))\n            H[0, 0] = A_block\n            H[0, 1:] = C_block\n            H[1:, 0] = C_block\n            H[1:, 1:] = G_block\n\n            V = np.zeros(1 + m)\n            V[0] = np.sum(D_inv_diag * y)\n            V[1:] = B.T @ D_inv @ y\n\n            # Solve for parameters [mu, theta]\n            p = np.linalg.solve(H, V)\n            mu_joint = p[0]\n            \n            # Variance of mu is the top-left element of H^-1\n            H_inv = np.linalg.inv(H)\n            sigma_joint = np.sqrt(H_inv[0, 0])\n\n        diff_mu = abs(mu_blue - mu_joint)\n        diff_sigma = abs(sigma_blue - sigma_joint)\n\n        case_results = [mu_blue, sigma_blue, mu_joint, sigma_joint, diff_mu, diff_sigma]\n        results.append(case_results)\n\n    # Format the final output string\n    formatted_results = []\n    for res_list in results:\n        formatted_list = [format_num__to_12_sig_digits(r) for r in res_list]\n        formatted_results.append(f\"[{','.join(formatted_list)}]\")\n    \n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}