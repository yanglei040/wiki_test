{
    "hands_on_practices": [
        {
            "introduction": "This first practice provides a crucial foundation in numerical integration. By applying Simpson's rule and then using Richardson extrapolation to improve the result, you will gain a concrete understanding of how quadrature formulas are constructed and how their systematic errors can be cancelled to achieve higher accuracy.",
            "id": "3525139",
            "problem": "In computational high-energy physics, numerical integration schemes are employed to evaluate dimensionless moments of distributions and test the reliability of quadrature rules under mesh refinement. Consider the toy moment integral of a scalar distribution, given by the integral of the polynomial $f(x)=x^{4}$ over the interval $[0,1]$. Your tasks are as follows:\n\n1. Starting from the definition that Simpson’s rule is constructed by integrating the quadratic Lagrange interpolant through the equally spaced nodes $x=a$, $x=(a+b)/2$, and $x=b$, determine whether Simpson’s rule is exact for $f(x)=x^{4}$ on $[0,1]$ by explicitly comparing the analytic integral to a single-panel Simpson approximation. State your conclusion regarding exactness.\n\n2. Compute the composite Simpson approximations to $\\int_{0}^{1} x^{4}\\,dx$ using two different meshes: $N=4$ subintervals (with step size $h=1/4$) and $N=8$ subintervals (with step size $h=1/8$). Use the canonical composite Simpson assembly that alternates weights $1,4,2,\\dots,4,1$ over equally spaced nodes.\n\n3. Based on first-principles error modeling that the leading global error for composite Simpson is a smooth power-law in $h$ for sufficiently regular $f$, construct a Richardson extrapolation that cancels the leading-order error using the two composite approximations you computed. Derive the extrapolation expression by eliminating the leading error term between the two meshes and evaluate it numerically.\n\nExpress the final answer as an exact fraction with no rounding and no units.",
            "solution": "This problem requires evaluating the integral of $f(x)=x^4$ on $[0,1]$ using Simpson's rule and Richardson extrapolation.\n\n### Part 1: Exactness of Single-Panel Simpson's Rule\n\nFirst, we compute the exact value of the integral analytically:\n$$\nI_{\\text{exact}} = \\int_0^1 x^4 \\,dx = \\left[ \\frac{x^5}{5} \\right]_0^1 = \\frac{1}{5}\n$$\n\nNext, we apply the single-panel Simpson's rule on the interval $[a, b] = [0, 1]$. The formula is:\n$$\nS_1 = \\frac{b-a}{6} \\left[ f(a) + 4f\\left(\\frac{a+b}{2}\\right) + f(b) \\right]\n$$\nWith $a=0$, $b=1$, and $f(x)=x^4$, we have:\n$$\nS_1 = \\frac{1}{6} \\left[ f(0) + 4f(1/2) + f(1) \\right] = \\frac{1}{6} \\left[ 0^4 + 4\\left(\\frac{1}{2}\\right)^4 + 1^4 \\right] = \\frac{1}{6} \\left[ 0 + 4\\left(\\frac{1}{16}\\right) + 1 \\right] = \\frac{1}{6} \\left[ \\frac{1}{4} + 1 \\right] = \\frac{1}{6} \\left( \\frac{5}{4} \\right) = \\frac{5}{24}\n$$\nSince $I_{\\text{exact}} = 1/5 \\neq 5/24 = S_1$, we conclude that Simpson's rule is **not exact** for the polynomial $f(x)=x^4$. This is expected, as Simpson's rule has a degree of exactness of 3.\n\n### Part 2: Composite Simpson's Rule Approximations\n\nThe formula for composite Simpson's rule with $N$ subintervals (where $N$ is even) and step size $h=(b-a)/N$ is:\n$$\nS_N = \\frac{h}{3} \\left[ f(x_0) + 4\\sum_{i=1}^{N/2} f(x_{2i-1}) + 2\\sum_{i=1}^{N/2-1} f(x_{2i}) + f(x_N) \\right]\n$$\n\n**For N=4 subintervals ($h=1/4$):**\nThe nodes are $\\{0, 1/4, 2/4, 3/4, 1\\}$.\n$$\nS_4 = \\frac{1/4}{3} \\left[ f(0) + 4f(1/4) + 2f(1/2) + 4f(3/4) + f(1) \\right]\n$$\n$$\nS_4 = \\frac{1}{12} \\left[ 0 + 4\\left(\\frac{1}{256}\\right) + 2\\left(\\frac{1}{16}\\right) + 4\\left(\\frac{81}{256}\\right) + 1 \\right] = \\frac{1}{12} \\left[ \\frac{4}{256} + \\frac{32}{256} + \\frac{324}{256} + \\frac{256}{256} \\right] = \\frac{1}{12} \\left( \\frac{616}{256} \\right) = \\frac{77}{384}\n$$\n\n**For N=8 subintervals ($h=1/8$):**\nThe nodes are $\\{0, 1/8, \\dots, 8/8\\}$. The sum of weighted function values is:\n$$\n\\sum = f(0) + 4f(1/8) + 2f(2/8) + 4f(3/8) + 2f(4/8) + 4f(5/8) + 2f(6/8) + 4f(7/8) + f(8/8)\n$$\n$$\n\\sum = \\frac{1}{8^4} \\left[ 1(0^4) + 4(1^4) + 2(2^4) + 4(3^4) + 2(4^4) + 4(5^4) + 2(6^4) + 4(7^4) + 1(8^4) \\right] = \\frac{19664}{4096}\n$$\n$$\nS_8 = \\frac{h}{3} \\sum = \\frac{1/8}{3} \\frac{19664}{4096} = \\frac{1}{24} \\frac{1229}{256} = \\frac{1229}{6144}\n$$\n\n### Part 3: Richardson Extrapolation\n\nThe global error for composite Simpson's rule is of the form $I_{\\text{exact}} - S(h) = Ch^4 + \\mathcal{O}(h^6)$. We can write:\n$$\nI_{\\text{exact}} \\approx S(h_1) + C h_1^4 \\quad (\\text{for } N=4, h_1=1/4)\n$$\n$$\nI_{\\text{exact}} \\approx S(h_2) + C h_2^4 \\quad (\\text{for } N=8, h_2=1/8)\n$$\nSince $h_2 = h_1/2$, we have $h_2^4 = h_1^4/16$. We can eliminate the constant $C$ to find a more accurate estimate $I_{\\text{extrap}}$. Multiplying the second equation by 16 and subtracting the first gives:\n$$\n15 I_{\\text{extrap}} = 16 S(h_2) - S(h_1) \\implies I_{\\text{extrap}} = \\frac{16 S_8 - S_4}{15}\n$$\nSubstituting the computed values:\n$$\nI_{\\text{extrap}} = \\frac{1}{15} \\left( 16 \\cdot \\frac{1229}{6144} - \\frac{77}{384} \\right) = \\frac{1}{15} \\left( \\frac{1229}{384} - \\frac{77}{384} \\right) = \\frac{1}{15} \\left( \\frac{1152}{384} \\right)\n$$\nSince $1152 / 384 = 3$, the result is:\n$$\nI_{\\text{extrap}} = \\frac{3}{15} = \\frac{1}{5}\n$$\nThe extrapolated result is exactly the true value of the integral. This is because the error term for $f(x)=x^4$ only contains the $h^4$ term, and all higher-order derivatives that would contribute to $\\mathcal{O}(h^6)$ and higher error terms are zero.\nThe final answer is $1/5$.",
            "answer": "$$\n\\boxed{\\frac{1}{5}}\n$$"
        },
        {
            "introduction": "Numerical differentiation presents a classic conflict between mathematical theory and computational reality. This exercise delves into the essential trade-off between truncation error, which vanishes as the step size $h$ approaches zero, and floating-point round-off error, which is amplified. Mastering this concept by deriving and testing the optimal step size is fundamental to obtaining reliable derivatives in any numerical simulation.",
            "id": "3525181",
            "problem": "Consider the numerical evaluation of derivatives that arises in computational high-energy physics when differentiating exponential factors in generating functionals and propagators. You will analyze the central finite-difference approximation for the derivative and determine the step size that balances truncation and floating-point roundoff errors in Institute of Electrical and Electronics Engineers (IEEE) double precision arithmetic.\n\nStarting from first principles appropriate to numerical analysis and physics computation:\n- Use the Taylor expansion about a point for a smooth scalar function $f(x)$ and the definition of the derivative $f'(x)$.\n- Model floating-point arithmetic under IEEE double precision with unit roundoff $\\epsilon$ satisfying $\\epsilon \\approx 2^{-53}$.\n\nYour tasks are:\n1. Derive, from Taylor expansion and the definition of the derivative, the leading-order truncation error for the central difference formula\n$$\nD_h f(x) \\equiv \\frac{f(x+h) - f(x-h)}{2h}\n$$\nas an approximation to $f'(x)$, identifying an explicit leading coefficient multiplying $h^2$ in terms of derivatives of $f$ evaluated at $x$.\n2. Using a standard floating-point rounding model in which each elementary function evaluation incurs a relative error on the order of $\\epsilon$, and noting the subtraction of nearly equal quantities in the numerator, construct a leading-order rounding error model for $D_h f(x)$ of the form $C\\,\\epsilon/h$ with a coefficient $C$ expressed in terms of $f(x)$.\n3. Combine the truncation and rounding error models into a single leading-order error estimate $E(h)$ and analytically minimize $E(h)$ with respect to $h$ to obtain a formula for the optimal step $h_{\\mathrm{opt}}$ in terms of $f(x)$, $f'''(x)$, and $\\epsilon$.\n4. Specialize your result to the case $f(x) = \\exp(k x)$ at $x=1$, where $k$ is a real parameter. Using IEEE double precision with $\\epsilon = 2^{-53}$, compute $h_{\\mathrm{opt}}$ numerically from your formula.\n5. For each parameter value, estimate the expected number of correct base-$10$ digits in $f'(1)$ when using the central difference approximation with your computed $h_{\\mathrm{opt}}$. Use the leading-order error model to obtain the expected relative error $\\delta_{\\mathrm{exp}}$, and then report the expected digit count as $-\\log_{10}(\\delta_{\\mathrm{exp}})$.\n6. Validate your model by actually computing the central difference approximation $D_{h_{\\mathrm{opt}}} f(1)$ in IEEE double precision and measuring the realized relative error $\\delta_{\\mathrm{meas}}$ against the exact derivative $f'(1)$, then reporting the measured digit count as $-\\log_{10}(\\delta_{\\mathrm{meas}})$.\n\nTest Suite:\n- Use the following parameter values for $k$ at $x=1$:\n    - Case A (general reference scale): $k = 1$.\n    - Case B (steep exponential relevant to stiff scales): $k = 10$.\n    - Case C (gentle exponential): $k = 0.1$.\n    - Case D (very steep exponential, but without overflow in double precision): $k = 100$.\n\nFor each case, compute and report:\n- The optimal step $h_{\\mathrm{opt}}$.\n- The expected digit count $-\\log_{10}(\\delta_{\\mathrm{exp}})$.\n- The measured digit count $-\\log_{10}(\\delta_{\\mathrm{meas}})$ computed using the central difference at $h_{\\mathrm{opt}}$.\n- A boolean that is $\\mathrm{True}$ if and only if the measured digit count differs from the expected digit count by no more than $0.5$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order for Cases A, B, C, and D respectively, the quadruples flattened into a single list:\n$$\n[ h_A, d^{\\mathrm{exp}}_A, d^{\\mathrm{meas}}_A, \\mathrm{ok}_A,\\; h_B, d^{\\mathrm{exp}}_B, d^{\\mathrm{meas}}_B, \\mathrm{ok}_B,\\; h_C, d^{\\mathrm{exp}}_C, d^{\\mathrm{meas}}_C, \\mathrm{ok}_C,\\; h_D, d^{\\mathrm{exp}}_D, d^{\\mathrm{meas}}_D, \\mathrm{ok}_D ]\n$$\nwhere each $h$ is a float, each $d$ is a float, and each $\\mathrm{ok}$ is a boolean. No physical units are required, and angles are not involved. Express all digit counts as real numbers (floats), not as percentages.",
            "solution": "The problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, objective, and complete. It represents a standard and instructive exercise in the numerical analysis of scientific computations. We may therefore proceed with a formal solution.\n\nThe task is to analyze the central finite-difference approximation for the first derivative of a function $f(x)$, denoted $D_h f(x)$, balancing its inherent truncation error with the rounding error introduced by floating-point arithmetic.\n\n$$\nD_h f(x) \\equiv \\frac{f(x+h) - f(x-h)}{2h}\n$$\n\n### 1. Derivation of the Truncation Error\n\nThe truncation error is the mathematical error incurred by approximating the true derivative $f'(x)$ with the finite-difference formula, assuming exact arithmetic. We derive this error by expanding the function $f(x)$ in a Taylor series about the point $x$. For a sufficiently smooth function, the expansions for $f(x+h)$ and $f(x-h)$ are:\n\n$$\nf(x+h) = f(x) + hf'(x) + \\frac{h^2}{2!}f''(x) + \\frac{h^3}{3!}f'''(x) + \\frac{h^4}{4!}f^{(4)}(x) + \\frac{h^5}{5!}f^{(5)}(x) + \\mathcal{O}(h^6)\n$$\n$$\nf(x-h) = f(x) - hf'(x) + \\frac{h^2}{2!}f''(x) - \\frac{h^3}{3!}f'''(x) + \\frac{h^4}{4!}f^{(4)}(x) - \\frac{h^5}{5!}f^{(5)}(x) + \\mathcal{O}(h^6)\n$$\n\nSubtracting the second expansion from the first cancels all even-powered terms in $h$:\n$$\nf(x+h) - f(x-h) = 2hf'(x) + 2\\frac{h^3}{3!}f'''(x) + 2\\frac{h^5}{5!}f^{(5)}(x) + \\mathcal{O}(h^7)\n$$\n\nDividing by $2h$ gives an expression for the central difference formula:\n$$\n\\frac{f(x+h) - f(x-h)}{2h} = f'(x) + \\frac{h^2}{6}f'''(x) + \\frac{h^4}{120}f^{(5)}(x) + \\mathcal{O}(h^6)\n$$\n\nThe truncation error, $E_{\\mathrm{trunc}}(h)$, is the difference between the approximation and the exact value. The leading-order term dominates for small $h$:\n$$\nE_{\\mathrm{trunc}}(h) = D_h f(x) - f'(x) = \\frac{h^2}{6}f'''(x) + \\mathcal{O}(h^4)\n$$\n\nThe magnitude of the leading-order truncation error is thus $|E_{\\mathrm{trunc}}(h)| \\approx \\left|\\frac{f'''(x)}{6}\\right| h^2$. The coefficient multiplying $h^2$ is $\\frac{1}{6}f'''(x)$.\n\n### 2. Modeling the Rounding Error\n\nIn floating-point arithmetic, the evaluation of a function $f(z)$ yields a computed value $\\hat{f}(z)$ with a relative error on the order of the machine precision or unit roundoff, $\\epsilon$. We model this as $\\hat{f}(z) = f(z)(1+\\delta)$, where $|\\delta| \\le \\epsilon$. The problem specifies IEEE double precision, for which $\\epsilon = 2^{-53}$.\n\nThe numerator of $D_h f(x)$ involves the subtraction of two nearly equal quantities, $f(x+h)$ and $f(x-h)$, which is a primary source of rounding error. Let the computed values be $\\hat{f}(x+h) = f(x+h)(1+\\delta_1)$ and $\\hat{f}(x-h) = f(x-h)(1+\\delta_2)$. The error in the computed numerator is approximately:\n$$\n\\Delta_{\\mathrm{num}} \\approx f(x+h)\\delta_1 - f(x-h)\\delta_2\n$$\n\nFor small $h$, we have $f(x+h) \\approx f(x-h) \\approx f(x)$. The magnitude of the absolute error in the numerator is then bounded by:\n$$\n|\\Delta_{\\mathrm{num}}| \\lesssim |f(x)||\\delta_1| + |f(x)||\\delta_2| \\approx 2\\epsilon|f(x)|\n$$\n\nA more direct analysis considers that the floating-point subtraction $a \\ominus b$ of two nearly equal numbers $a \\approx b$ has a rounding error whose magnitude is on the order of $\\epsilon|a|$. The absolute rounding error in the final calculation of $D_h f(x)$ is this numerator error divided by $2h$. Thus, the magnitude of the rounding error $E_{\\mathrm{round}}(h)$ is:\n$$\n|E_{\\mathrm{round}}(h)| \\approx \\frac{\\epsilon |f(x)|}{h}\n$$\n\nThis matches the requested form $C\\epsilon/h$ with the coefficient $C = |f(x)|$.\n\n### 3. Derivation of the Optimal Step Size\n\nThe total error $E(h)$ is the sum of the magnitudes of the truncation and rounding errors:\n$$\nE(h) \\approx |E_{\\mathrm{trunc}}(h)| + |E_{\\mathrm{round}}(h)| = \\left|\\frac{f'''(x)}{6}\\right| h^2 + \\frac{|f(x)| \\epsilon}{h}\n$$\n\nTo find the step size $h_{\\mathrm{opt}}$ that minimizes this total error, we differentiate $E(h)$ with respect to $h$ and set the result to zero:\n$$\n\\frac{dE}{dh} = 2\\left|\\frac{f'''(x)}{6}\\right| h - \\frac{|f(x)| \\epsilon}{h^2} = 0\n$$\n\nSolving for $h$ gives the optimal step size $h_{\\mathrm{opt}}$:\n$$\n2\\left|\\frac{f'''(x)}{6}\\right| h^3 = |f(x)| \\epsilon \\implies h^3 = \\frac{3|f(x)|\\epsilon}{|f'''(x)|}\n$$\n$$\nh_{\\mathrm{opt}} = \\left( \\frac{3|f(x)|\\epsilon}{|f'''(x)|} \\right)^{1/3}\n$$\n\n### 4. Specialization to $f(x) = \\exp(kx)$\n\nWe now specialize this result for the function $f(x) = \\exp(kx)$ evaluated at $x=1$. The required derivatives are:\n$$\nf(x) = e^{kx} \\implies f(1) = e^k\n$$\n$$\nf'(x) = ke^{kx} \\implies f'(1) = ke^k\n$$\n$$\nf'''(x) = k^3e^{kx} \\implies f'''(1) = k^3e^k\n$$\n\nSubstituting these into the formula for $h_{\\mathrm{opt}}$:\n$$\nh_{\\mathrm{opt}} = \\left( \\frac{3|e^k|\\epsilon}{|k^3e^k|} \\right)^{1/3} = \\left( \\frac{3\\epsilon}{|k|^3} \\right)^{1/3} = \\frac{(3\\epsilon)^{1/3}}{|k|}\n$$\nThe value of machine epsilon for IEEE double precision is $\\epsilon = 2^{-53}$.\n\n### 5. Expected Number of Correct Digits\n\nThe expected relative error, $\\delta_{\\mathrm{exp}}$, is the minimum total error $E(h_{\\mathrm{opt}})$ divided by the magnitude of the true derivative, $|f'(1)|$. At $h=h_{\\mathrm{opt}}$, the two error contributions are of the same order of magnitude. The total error is:\n$$\nE(h_{\\mathrm{opt}}) = \\left|\\frac{f'''(1)}{6}\\right| h_{\\mathrm{opt}}^2 + \\frac{|f(1)| \\epsilon}{h_{\\mathrm{opt}}}\n$$\nThe optimal relative error $\\delta_{\\mathrm{exp}} = E(h_{\\mathrm{opt}})/|f'(1)|$ is:\n$$\n\\delta_{\\mathrm{exp}} = \\frac{1}{|ke^k|}\\left( \\left|\\frac{k^3e^k}{6}\\right| \\left(\\frac{3\\epsilon}{|k|^3}\\right)^{2/3} + \\frac{e^k \\epsilon}{|k|^{-1}(3\\epsilon)^{1/3}} \\right)\n$$\nSimplifying, we substitute $h_{\\mathrm{opt}} = (3\\epsilon/|k|^3)^{1/3}$ into the relative error expression $\\delta(h) = |\\frac{k^2}{6}| h^2 + \\frac{\\epsilon}{|k|h}$.\n$$\n\\delta_{\\mathrm{exp}} = \\frac{|k|^2}{6} \\left(\\frac{3\\epsilon}{|k|^3}\\right)^{2/3} + \\frac{\\epsilon}{|k|} \\left(\\frac{3\\epsilon}{|k|^3}\\right)^{-1/3} = \\frac{|k|^2}{6} \\frac{(3\\epsilon)^{2/3}}{|k|^2} + \\frac{\\epsilon}{|k|} \\frac{|k|}{(3\\epsilon)^{1/3}} = \\frac{(3\\epsilon)^{2/3}}{6} + \\frac{3\\epsilon}{3(3\\epsilon)^{1/3}} = \\frac{(3\\epsilon)^{2/3}}{6} + \\frac{(3\\epsilon)^{2/3}}{3} = \\frac{3(3\\epsilon)^{2/3}}{6} = \\frac{(3\\epsilon)^{2/3}}{2}\n$$\n$$\n\\delta_{\\mathrm{exp}} = \\frac{3^{2/3}}{2} \\epsilon^{2/3}\n$$\nRemarkably, the expected optimal relative error is independent of the parameter $k$.\nThe expected number of correct decimal digits is given by $d^{\\mathrm{exp}} = -\\log_{10}(\\delta_{\\mathrm{exp}})$.\n\n### 6. Measured Number of Correct Digits\n\nFor each value of $k$, we will numerically compute $h_{\\mathrm{opt}}$, then evaluate $D_{h_{\\mathrm{opt}}} f(1)$ using standard double-precision arithmetic. The measured relative error $\\delta_{\\mathrm{meas}}$ is then calculated as:\n$$\n\\delta_{\\mathrm{meas}} = \\frac{|D_{h_{\\mathrm{opt}}}f(1) - f'(1)|}{|f'(1)|}\n$$\nThe measured digit count is $d^{\\mathrm{meas}} = -\\log_{10}(\\delta_{\\mathrm{meas}})$. The comparison of $d^{\\mathrm{exp}}$ and $d^{\\mathrm{meas}}$ validates the robustness of the error model. Catastrophic cancellation is expected to degrade the accuracy for large values of $k$, potentially causing the measured error to be significantly larger (and $d^{\\mathrm{meas}}$ smaller) than predicted by our simplified model.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the central finite-difference method for f(x) = exp(kx),\n    calculating the optimal step size and comparing theoretical vs. measured error.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Case parameters are values of k, with x=1 fixed.\n    test_cases = [\n        1.0,    # Case A: General reference scale\n        10.0,   # Case B: Steep exponential\n        0.1,    # Case C: Gentle exponential\n        100.0   # Case D: Very steep exponential\n    ]\n\n    # IEEE 754 double precision unit roundoff\n    epsilon = 2**-53\n\n    results = []\n    \n    # The expected relative error, derived in the solution, is independent of k.\n    # δ_exp = (3^(2/3) / 2) * ε^(2/3)\n    delta_exp = (3**(2/3) / 2) * (epsilon**(2/3))\n    \n    # The expected number of correct digits.\n    d_exp = -np.log10(delta_exp)\n\n    for k in test_cases:\n        x = 1.0\n\n        # Task 4: Compute the optimal step size h_opt for f(x) = exp(kx).\n        # h_opt = (|k|)^-1 * (3*ε)^(1/3)\n        # Using abs(k) to handle potential negative k, though not in test suite.\n        h_opt = (1 / np.abs(k)) * (3 * epsilon)**(1/3)\n\n        # Task 5: The expected number of correct digits, d_exp, is already computed.\n\n        # Task 6: Validate the model by direct computation.\n        # Exact derivative: f'(x) = k * exp(kx)\n        f_prime_exact = k * np.exp(k * x)\n\n        # Numerical derivative using the central difference formula.\n        # All calculations are implicitly in float64 (double precision).\n        f_plus_h = np.exp(k * (x + h_opt))\n        f_minus_h = np.exp(k * (x - h_opt))\n        f_prime_numerical = (f_plus_h - f_minus_h) / (2 * h_opt)\n\n        # Calculate the measured relative error.\n        # Add a small quantity to the denominator to prevent division by zero\n        # if the exact derivative happens to be zero.\n        delta_meas = np.abs(f_prime_numerical - f_prime_exact) / (np.abs(f_prime_exact) + epsilon)\n        \n        # Calculate the measured number of correct digits.\n        # If delta_meas is zero, it implies precision to the limits of float64.\n        # In that rare case, we can use a proxy for a very large number of digits.\n        if delta_meas == 0.0:\n            d_meas = -np.log10(epsilon) # A reasonable upper bound on measurable precision\n        else:\n            d_meas = -np.log10(delta_meas)\n\n        # Compare expected vs. measured digit counts.\n        ok = np.abs(d_exp - d_meas) = 0.5\n        \n        results.extend([h_opt, d_exp, d_meas, ok])\n\n    # Final print statement in the exact required format.\n    # The boolean values will be correctly converted to 'True' or 'False' by str().\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from one to many dimensions dramatically increases computational cost, a challenge known as the \"curse of dimensionality.\" This practice tackles a three-dimensional integral, directly comparing a brute-force tensor-product method with the far more efficient Smolyak sparse grid construction. Such advanced techniques are indispensable for evaluating the high-dimensional phase-space and path integrals common in computational high-energy physics.",
            "id": "3525193",
            "problem": "Consider the integral of a separable exponential weight over the three-dimensional unit cube, defined as $\\int_{[0,1]^3} \\exp(-\\alpha \\sum_{i=1}^3 x_i)\\,dx$, where $\\alpha$ is a positive parameter and $(x_1,x_2,x_3)\\in[0,1]^3$. In computational high-energy physics, such integrals model simplified phase-space averages with exponential suppression, and their accurate numerical evaluation is essential when exact expressions are available for validation. Starting from fundamental facts of calculus and numerical analysis, design and implement a program that compares two numerical integration strategies for this integral at the fixed parameter value $\\alpha=10$: a tensor-product Gaussian quadrature and a Smolyak sparse grid of level $2$ in three dimensions.\n\nYour program must adhere to the following requirements:\n\n1. Implement a tensor-product Gaussian quadrature using the Gauss–Legendre family on $[-1,1]$ transformed to $[0,1]$, with $N$ points per spatial dimension. The three-dimensional tensor-product rule should use the Cartesian product of the one-dimensional nodes and weights.\n\n2. Implement a level-$2$ Smolyak sparse grid in $d=3$ dimensions using a sequence of one-dimensional Gauss–Legendre rules $U^{(l)}$ with $N(l)=2l-1$ nodes for $l\\in\\{1,2\\}$, transformed to $[0,1]$. Construct the Smolyak operator at level $2$ via the canonical difference-of-levels formulation using $\\Delta^{(l)}=U^{(l)}-U^{(l-1)}$ with $U^{(0)}=0$, combining all multi-indices $(l_1,l_2,l_3)$ satisfying $l_i\\ge 1$ and $l_1+l_2+l_3=4$. Your implementation must explicitly build the combined three-dimensional quadrature nodes and weights, correctly accounting for negative weights and merging duplicate nodes by summing their weights, and it must report the total number of unique function evaluation points used.\n\n3. Compute the exact value of the integral analytically using first principles of calculus and the separability of the integrand, without relying on external resources or pre-tabulated results. Use this exact value to compute absolute errors for each numerical method.\n\n4. Use the fixed parameter value $\\alpha=10$ for all computations. No physical unit is involved in this integral; all quantities are dimensionless.\n\n5. Test Suite:\n   - Evaluate the tensor-product Gaussian quadrature for $N\\in\\{1,3,5\\}$ points per dimension.\n   - For each $N$ in the test suite, compute:\n     - The absolute error of the tensor-product Gaussian quadrature.\n     - The absolute error of the level-$2$ Smolyak sparse grid (which is independent of $N$).\n     - The total number of points used by the tensor-product rule, which is $N^3$.\n     - The total number of unique points used by the Smolyak sparse grid implementation (as constructed with the specified rules and merging duplicates).\n   This test suite covers a boundary case ($N=1$), a typical moderate-accuracy case ($N=3$), and a higher-accuracy case ($N=5$).\n\n6. Final Output Format:\n   Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a sub-list of the form $[E_{\\mathrm{tensor}},E_{\\mathrm{smolyak}},P_{\\mathrm{tensor}},P_{\\mathrm{smolyak}}]$, where $E_{\\mathrm{tensor}}$ and $E_{\\mathrm{smolyak}}$ are floats (absolute errors), and $P_{\\mathrm{tensor}}$ and $P_{\\mathrm{smolyak}}$ are integers (point counts). For example, the final printed line must look like $[[e_{1},e_{2},p_{1},p_{2}],[e_{3},e_{4},p_{3},p_{4}],\\ldots]$ with no spaces.\n\nYour implementation must be a complete, runnable program and must not read input from files or the network. The program must compute the exact integral internally, and perform all numerical quadrature computations as specified.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It poses a specific and formalizable task in numerical analysis, comparing two standard quadrature methods for a well-defined integral. All necessary parameters and definitions are provided.\n\n### 1. Analytical Solution\n\nThe integral to be evaluated is:\n$$\nI = \\int_{[0,1]^3} \\exp\\left(-\\alpha \\sum_{i=1}^3 x_i\\right) d\\mathbf{x}\n$$\nwhere $d\\mathbf{x} = dx_1 dx_2 dx_3$ and the domain is the unit cube $[0,1]^3$. The parameter $\\alpha$ is given as $10$.\n\nThe integrand $f(x_1, x_2, x_3) = \\exp(-\\alpha(x_1+x_2+x_3))$ is a separable function, as it can be written as a product of functions of a single variable:\n$$\nf(x_1, x_2, x_3) = e^{-\\alpha x_1} e^{-\\alpha x_2} e^{-\\alpha x_3}\n$$\nDue to the separability of the integrand and the rectangular domain of integration, the three-dimensional integral can be expressed as the cube of a one-dimensional integral:\n$$\nI = \\left( \\int_0^1 e^{-\\alpha x} dx \\right) \\left( \\int_0^1 e^{-\\alpha y} dy \\right) \\left( \\int_0^1 e^{-\\alpha z} dz \\right) = \\left( \\int_0^1 e^{-\\alpha x} dx \\right)^3\n$$\nThe one-dimensional integral is evaluated using the fundamental theorem of calculus:\n$$\n\\int_0^1 e^{-\\alpha x} dx = \\left[ -\\frac{1}{\\alpha} e^{-\\alpha x} \\right]_0^1 = -\\frac{1}{\\alpha} (e^{-\\alpha \\cdot 1} - e^{-\\alpha \\cdot 0}) = -\\frac{1}{\\alpha} (e^{-\\alpha} - 1) = \\frac{1 - e^{-\\alpha}}{\\alpha}\n$$\nSubstituting this result back, the exact value of the three-dimensional integral is:\n$$\nI_{\\text{exact}} = \\left( \\frac{1 - e^{-\\alpha}}{\\alpha} \\right)^3\n$$\nFor the given parameter $\\alpha=10$, the exact value is:\n$$\nI_{\\text{exact}} = \\left( \\frac{1 - e^{-10}}{10} \\right)^3 \\approx (0.09999546...)^3 \\approx 9.9986389... \\times 10^{-4}\n$$\n\n### 2. Numerical Integration Methods\n\nWe will implement and compare two numerical quadrature methods. For any quadrature rule, the integral of a function $g(\\mathbf{p})$ over a domain $\\Omega$ is approximated by a weighted sum of function evaluations at specific points (nodes) $\\mathbf{p}_j$:\n$$\n\\int_\\Omega g(\\mathbf{p}) d\\mathbf{p} \\approx \\sum_{j=1}^M w_j g(\\mathbf{p}_j)\n$$\n\nThe one-dimensional Gauss-Legendre quadrature rules are defined on the interval $[-1,1]$. To apply them to an integral on $[0,1]$, the nodes $x_i \\in [-1,1]$ and weights $w_i$ must be transformed. The linear mapping is $x' = (x+1)/2$. The new nodes $x'_i$ and weights $w'_i$ for the interval $[0,1]$ are:\n$$\nx'_i = \\frac{x_i + 1}{2}, \\quad w'_i = \\frac{w_i}{2}\n$$\n\n#### 2.1. Tensor-Product Gaussian Quadrature\n\nA three-dimensional tensor-product rule is constructed from a one-dimensional rule. Given a 1D Gauss-Legendre rule with $N$ nodes $\\{x'_i\\}_{i=1}^N$ and weights $\\{w'_i\\}_{i=1}^N$ on $[0,1]$, the 3D tensor-product rule uses $N^3$ points. The nodes are the Cartesian product of the 1D nodes, forming a grid: $\\mathbf{p}_{ijk} = (x'_i, x'_j, x'_k)$. The corresponding weights are the products of the 1D weights: $W_{ijk} = w'_i w'_j w'_k$. The integral approximation is:\n$$\nI_{\\text{tensor}} \\approx \\sum_{i=1}^N \\sum_{j=1}^N \\sum_{k=1}^N (w'_i w'_j w'_k) f(x'_i, x'_j, x'_k)\n$$\nThe total number of function evaluation points for this method is $P_{\\text{tensor}} = N^3$.\n\n#### 2.2. Smolyak Sparse Grid Quadrature\n\nThe problem specifies a particular construction for a level-$2$ Smolyak sparse grid in $d=3$ dimensions. The construction is based on a sequence of 1D Gauss-Legendre rules, $U^{(l)}$, where the $l$-th rule uses $N(l) = 2l-1$ points.\n-   $l=1 \\implies N(1)=1$ point (midpoint rule).\n-   $l=2 \\implies N(2)=3$ points.\n\nThe construction uses the difference operator $\\Delta^{(l)} = U^{(l)} - U^{(l-1)}$, with the convention $U^{(0)}=0$. Thus, $\\Delta^{(1)} = U^{(1)}$.\nThe specified Smolyak operator is formulated by combining tensor products of these difference operators for all multi-indices $\\mathbf{l} = (l_1, l_2, l_3)$ with $l_i \\ge 1$ such that their sum is $|\\mathbf{l}|_1 = l_1+l_2+l_3=4$. The set of qualifying multi-indices is $\\{(2,1,1), (1,2,1), (1,1,2)\\}$.\n\nThe Smolyak quadrature rule $Q_{\\text{smolyak}}$ is therefore:\n$$\nQ_{\\text{smolyak}} = \\Delta^{(2)} \\otimes \\Delta^{(1)} \\otimes \\Delta^{(1)} + \\Delta^{(1)} \\otimes \\Delta^{(2)} \\otimes \\Delta^{(1)} + \\Delta^{(1)} \\otimes \\Delta^{(1)} \\otimes \\Delta^{(2)}\n$$\nSubstituting $\\Delta^{(1)} = U^{(1)}$ and $\\Delta^{(2)} = U^{(2)} - U^{(1)}$, this expands to:\n$$\nQ_{\\text{smolyak}} = (U^{(2)}-U^{(1)}) \\otimes U^{(1)} \\otimes U^{(1)} + U^{(1)} \\otimes (U^{(2)}-U^{(1)}) \\otimes U^{(1)} + U^{(1)} \\otimes U^{(1)} \\otimes (U^{(2)}-U^{(1)})\n$$\n\nTo implement this, we compute the nodes and weights for each term and sum them, merging duplicate nodes by adding their weights.\n-   $U^{(1)}$ on $[0,1]$: 1 node $x_1^{(1)}=0.5$, weight $w_1^{(1)}=1.0$.\n-   $U^{(2)}$ on $[0,1]$: 3 nodes $\\{x_1^{(2)}, x_2^{(2)}, x_3^{(2)}\\} = \\{(1-\\sqrt{3/5})/2, 0.5, (1+\\sqrt{3/5})/2\\}$, weights $\\{w_1^{(2)}, w_2^{(2)}, w_3^{(2)}\\} = \\{5/18, 8/18, 5/18\\}$.\n\nThe operator $\\Delta^{(2)} = U^{(2)} - U^{(1)}$ uses the nodes of $U^{(2)}$. The weight for the node $x_2^{(2)}=0.5$, which is also the node of $U^{(1)}$, is modified: $w_2^{\\Delta(2)} = w_2^{(2)} - w_1^{(1)} = 8/18 - 1 = -10/18$. The other weights are unchanged. The nodes and weights for $\\Delta^{(2)}$ are:\n-   Nodes: $\\{(1-\\sqrt{3/5})/2, 0.5, (1+\\sqrt{3/5})/2\\}$\n-   Weights: $\\{5/18, -10/18, 5/18\\}$\n\nThe full sparse grid is built by creating a list of all points and their corresponding weights from the three tensor product terms in the expression for $Q_{\\text{smolyak}}$.\n-   $\\Delta^{(2)}\\otimes\\Delta^{(1)}\\otimes\\Delta^{(1)}$ contributes 3 points of the form (node from $\\Delta^{(2)}$, $0.5, 0.5$).\n-   $\\Delta^{(1)}\\otimes\\Delta^{(2)}\\otimes\\Delta^{(1)}$ contributes 3 points of the form ($0.5$, node from $\\Delta^{(2)}$, $0.5$).\n-   $\\Delta^{(1)}\\otimes\\Delta^{(1)}\\otimes\\Delta^{(2)}$ contributes 3 points of the form ($0.5, 0.5$, node from $\\Delta^{(2)}$).\n\nThis results in a total of $3+3+3=9$ initial points. Several points are duplicates. For example, $(0.5, 0.5, 0.5)$ is generated by all three terms. After combining duplicates and summing their weights, we are left with $7$ unique points.\n-   1 central point: $(0.5, 0.5, 0.5)$. Its weight is the sum of the weights from the three terms: $3 \\times (-10/18) = -30/18$.\n-   6 \"axial\" points: e.g., $((1\\pm\\sqrt{3/5})/2, 0.5, 0.5)$ and its permutations. Each of these unique points appears once, with a weight of $5/18$.\n\nThe total number of unique function evaluations is $P_{\\text{smolyak}} = 7$.\n\n### 3. Error Computation\n\nThe absolute error for each numerical method is calculated as the absolute difference between the numerical approximation and the exact analytical value:\n$$\nE = |I_{\\text{numerical}} - I_{\\text{exact}}|\n$$\nThis will be computed for both $I_{\\text{tensor}}$ and $I_{\\text{smolyak}}$.\n\n### 4. Test Cases\n\nThe test suite requires these computations for each $N \\in \\{1, 3, 5\\}$:\n1.  Absolute error $E_{\\text{tensor}}$ for the $N$-point tensor-product rule.\n2.  Absolute error $E_{\\text{smolyak}}$ for the level-2 Smolyak rule (this will be constant).\n3.  Total points $P_{\\text{tensor}} = N^3$.\n4.  Total points $P_{\\text{smolyak}} = 7$ (this will be constant).\n\nThe final program will execute these calculations and format the output as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import roots_legendre\n\ndef solve():\n    \"\"\"\n    Computes and compares tensor-product and Smolyak sparse grid quadratures\n    for a 3D exponential integral.\n    \"\"\"\n    # Define the problem parameter and test cases from the problem statement.\n    ALPHA = 10.0\n    TEST_N_VALUES = [1, 3, 5]\n    \n    # 1. ANALYTICAL SOLUTION\n    # The exact value of the integral I = ( (1 - exp(-alpha)) / alpha )^3\n    exact_value = ((1.0 - np.exp(-ALPHA)) / ALPHA)**3\n\n    # Define the integrand function\n    # It accepts a batch of points (N, 3) and returns (N,) results.\n    def integrand(points):\n        return np.exp(-ALPHA * np.sum(points, axis=1))\n\n    def get_transformed_gl_rule(n):\n        \"\"\"\n        Gets Gauss-Legendre nodes and weights for interval [0, 1].\n        \"\"\"\n        nodes_std, weights_std = roots_legendre(n)\n        # Transform from [-1, 1] to [0, 1]\n        nodes_transformed = (nodes_std + 1.0) / 2.0\n        weights_transformed = weights_std / 2.0\n        return nodes_transformed, weights_transformed\n\n    # 2. SMOLYAK SPARSE GRID IMPLEMENTATION\n    def calculate_smolyak_quadrature():\n        \"\"\"\n        Constructs the specified level-2 Smolyak sparse grid and computes the integral.\n        This is a fixed rule, so its result is computed once.\n        \"\"\"\n        # Get 1D base rules U^(l) for l=1, 2 on [0,1]\n        # U^(1) corresponds to n=1, U^(2) to n=3 (since N(l)=2l-1)\n        u1_nodes, u1_weights = get_transformed_gl_rule(1)\n        u2_nodes, u2_weights = get_transformed_gl_rule(3)\n        \n        # Define difference operators Delta^(l)\n        # Delta^(1) = U^(1)\n        d1_nodes, d1_weights = u1_nodes, u1_weights\n        \n        # Delta^(2) = U^(2) - U^(1)\n        d2_nodes = u2_nodes\n        d2_weights = u2_weights.copy()\n        # Find the index of the common node (0.5) in U^(2)\n        common_node_idx = np.where(u2_nodes == u1_nodes[0])[0][0]\n        d2_weights[common_node_idx] -= u1_weights[0]\n\n        # Use a dictionary to store points and sum weights for duplicates\n        # Rounding keys to handle floating point inaccuracies\n        sparse_grid = {}\n        \n        # Rule is sum over permutations of (Delta^2, Delta^1, Delta^1)\n        deltas = [(d1_nodes, d1_weights), (d2_nodes, d2_weights)]\n        multi_indices = [(2, 1, 1), (1, 2, 1), (1, 1, 2)]\n\n        for l1, l2, l3 in multi_indices:\n            nodes_x, weights_x = deltas[l1 - 1]\n            nodes_y, weights_y = deltas[l2 - 1]\n            nodes_z, weights_z = deltas[l3 - 1]\n            \n            for i, (nx, wx) in enumerate(zip(nodes_x, weights_x)):\n                for j, (ny, wy) in enumerate(zip(nodes_y, weights_y)):\n                    for k, (nz, wz) in enumerate(zip(nodes_z, weights_z)):\n                        point = (nx, ny, nz)\n                        # Round to ensure consistent keys for duplicate points\n                        key = tuple(np.round(point, 15))\n                        weight = wx * wy * wz\n                        sparse_grid[key] = sparse_grid.get(key, 0.0) + weight\n\n        # Extract unique points and final weights\n        final_points = np.array(list(sparse_grid.keys()))\n        final_weights = np.array(list(sparse_grid.values()))\n        \n        # Calculate the integral value\n        integral_value = np.sum(final_weights * integrand(final_points))\n        num_points = len(final_points)\n        \n        return integral_value, num_points\n\n    # 3. TENSOR-PRODUCT GAUSSIAN QUADRATURE IMPLEMENTATION\n    def calculate_tensor_product_quadrature(N):\n        \"\"\"\n        Constructs an N-point (per dim) tensor-product grid and computes the integral.\n        \"\"\"\n        nodes_1d, weights_1d = get_transformed_gl_rule(N)\n        \n        # Create 3D grid points and weights\n        x, y, z = np.meshgrid(nodes_1d, nodes_1d, nodes_1d, indexing='ij')\n        points = np.vstack([x.ravel(), y.ravel(), z.ravel()]).T\n        \n        wx, wy, wz = np.meshgrid(weights_1d, weights_1d, weights_1d, indexing='ij')\n        weights = (wx * wy * wz).ravel()\n\n        # Calculate the integral value\n        integral_value = np.sum(weights * integrand(points))\n        num_points = N**3\n        \n        return integral_value, num_points\n\n    # 4. EXECUTE TEST SUITE AND COLLECT RESULTS\n    results = []\n\n    # Pre-calculate the Smolyak result as it's constant for all test cases\n    smolyak_value, smolyak_points_count = calculate_smolyak_quadrature()\n    error_smolyak = abs(smolyak_value - exact_value)\n\n    for N in TEST_N_VALUES:\n        # Calculate tensor product quadrature for the current N\n        tensor_value, tensor_points_count = calculate_tensor_product_quadrature(N)\n        error_tensor = abs(tensor_value - exact_value)\n        \n        # Store results for this test case\n        case_result = [\n            error_tensor,\n            error_smolyak,\n            tensor_points_count,\n            smolyak_points_count\n        ]\n        results.append(case_result)\n\n    # 5. FINAL OUTPUT FORMATTING\n    # Format each sub-list: [val1,val2,...]\n    sub_lists_str = [f\"[{','.join(map(str, r))}]\" for r in results]\n    # Join sub-lists into the final format: [[...],[...],...]\n    final_output = f\"[{','.join(sub_lists_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}