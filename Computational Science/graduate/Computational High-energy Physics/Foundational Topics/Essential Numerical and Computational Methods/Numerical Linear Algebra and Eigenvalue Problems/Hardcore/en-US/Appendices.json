{
    "hands_on_practices": [
        {
            "introduction": "Many algorithms in computational physics, such as those simulating time evolution, require applying a function to a large matrix. This exercise explores a powerful method for this task: polynomial approximation. By connecting the spectral theory of Hermitian matrices with classical results from complex approximation theory, you will derive a rigorous error bound for approximating a matrix function using Chebyshev polynomials . This practice is fundamental for understanding the convergence behavior of polynomial-based methods and for developing a deeper appreciation of the interplay between pure and applied mathematics in algorithm analysis.",
            "id": "3525838",
            "problem": "In lattice computations for Quantum Chromodynamics (QCD) and related models in computational high-energy physics, one often needs to apply a smooth function of a large Hermitian matrix that represents a discretized Hamiltonian or a Euclidean Dirac operator. Let $A \\in \\mathbb{C}^{N \\times N}$ be Hermitian and diagonalizable as $A = Q \\Lambda Q^{\\ast}$ with $Q$ unitary and $\\Lambda = \\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{N})$. Define a matrix function via the spectral calculus: for a scalar function $f$ defined on an interval containing $\\{\\lambda_{i}\\}_{i=1}^{N}$, set $f(A) = Q f(\\Lambda) Q^{\\ast}$ where $f(\\Lambda) = \\operatorname{diag}(f(\\lambda_{1}),\\dots,f(\\lambda_{N}))$. Suppose the spectrum of $A$ lies in a known compact interval $[a,b] \\subset \\mathbb{R}$ with $a  b$, as is the case for a discretized positive-definite operator with physically bounded energy scales.\n\nConsider approximating $f(A)$ by a degree-$n$ polynomial $p_{n}(A)$ constructed from Chebyshev polynomials on the interval $[a,b]$, using the standard affine mapping $x = c + d t$ with $c = (a+b)/2$ and $d = (b-a)/2$ from $t \\in [-1,1]$ to $x \\in [a,b]$. Assume $f$ is analytic on and inside a Bernstein ellipse in the complex $t$-plane with parameter $\\rho  1$ (the ellipse with foci at $-1$ and $1$ whose sum of semiaxes equals $\\rho$), and that $|f(c + d t)| \\leq M$ holds uniformly for all points $t$ on this ellipse, for some constant $M  0$. These analyticity and boundedness assumptions are satisfied by physically relevant functions such as the imaginary-time propagator $f(x) = \\exp(-\\beta x)$ with $\\beta  0$ or rational filters away from singularities, provided the spectral interval and ellipse are chosen consistently with the energy scales.\n\nStarting from the spectral calculus and the unitary invariance of the spectral norm, and using well-tested facts from complex approximation theory for Chebyshev polynomials and analytic functions on Bernstein ellipses, derive a rigorous upper bound on the operator $2$-norm error $\\|f(A) - p_{n}(A)\\|_{2}$ in terms of the ellipse parameter $\\rho$, the bound $M$, and the polynomial degree $n$. Then, provide the resulting single closed-form analytic expression for the bound on $\\|f(A) - p_{n}(A)\\|_{2}$ as a function of $\\rho$, $M$, and $n$. Your final answer must be a single closed-form analytic expression. No numerical rounding is required. Express the result purely symbolically.",
            "solution": "We begin with a Hermitian matrix $A \\in \\mathbb{C}^{N \\times N}$, so by the spectral theorem, $A$ admits a unitary diagonalization $A = Q \\Lambda Q^{\\ast}$ where $Q$ is unitary and $\\Lambda = \\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{N})$ with $\\lambda_{i} \\in \\mathbb{R}$. For a scalar function $f$ defined on a set containing the spectrum $\\{\\lambda_{i}\\}$, the spectral calculus defines\n$$\nf(A) = Q f(\\Lambda) Q^{\\ast}, \\quad f(\\Lambda) = \\operatorname{diag}(f(\\lambda_{1}),\\dots,f(\\lambda_{N})).\n$$\nLet $p_{n}$ be a polynomial of degree $n$. Then $p_{n}(A) = Q p_{n}(\\Lambda) Q^{\\ast}$, with $p_{n}(\\Lambda) = \\operatorname{diag}(p_{n}(\\lambda_{1}),\\dots,p_{n}(\\lambda_{N}))$. Therefore,\n$$\nf(A) - p_{n}(A) = Q \\left( f(\\Lambda) - p_{n}(\\Lambda) \\right) Q^{\\ast} = Q \\,\\operatorname{diag}\\big(f(\\lambda_{1}) - p_{n}(\\lambda_{1}),\\dots,f(\\lambda_{N}) - p_{n}(\\lambda_{N})\\big) Q^{\\ast}.\n$$\nThe spectral norm $\\|\\cdot\\|_{2}$ is unitarily invariant, so\n$$\n\\|f(A) - p_{n}(A)\\|_{2} = \\left\\| \\operatorname{diag}\\big(f(\\lambda_{1}) - p_{n}(\\lambda_{1}),\\dots,f(\\lambda_{N}) - p_{n}(\\lambda_{N})\\big) \\right\\|_{2}.\n$$\nFor a diagonal matrix, the operator $2$-norm equals the maximum absolute entry on the diagonal. Thus,\n$$\n\\|f(A) - p_{n}(A)\\|_{2} = \\max_{1 \\leq i \\leq N} |f(\\lambda_{i}) - p_{n}(\\lambda_{i})|.\n$$\nIf the spectrum lies in $[a,b]$, then\n$$\n\\|f(A) - p_{n}(A)\\|_{2} \\leq \\max_{x \\in [a,b]} |f(x) - p_{n}(x)|.\n$$\nThis shows that the matrix approximation error in the operator $2$-norm is controlled by the scalar uniform approximation error over the spectral interval. Consequently, it suffices to bound $\\max_{x \\in [a,b]} |f(x) - p_{n}(x)|$.\n\nWe map the interval $[a,b]$ to $[-1,1]$ via the affine transformation $x = c + d t$ with $c = (a+b)/2$ and $d = (b-a)/2$. Define $g(t) = f(c + d t)$, so that the approximation problem on $[a,b]$ translates to approximating $g$ on $[-1,1]$ by a degree-$n$ polynomial in $t$, say $q_{n}(t) = p_{n}(c + d t)$.\n\nWe assume $f$ is analytic on and inside a Bernstein ellipse in the complex $t$-plane with parameter $\\rho  1$, and that $|g(t)| \\leq M$ holds uniformly for all $t$ on this ellipse. The Bernstein ellipse $E_{\\rho}$ is the image of the circle under the Joukowski map and has foci at $-1$ and $1$. A classical result from complex approximation theory for Chebyshev expansions (derived using the Cauchy integral formula for the Chebyshev coefficients and the maximum modulus principle) states that if $g$ is analytic on and inside $E_{\\rho}$ and bounded by $M$ there, then the error of the best degree-$n$ polynomial approximation to $g$ on $[-1,1]$ satisfies\n$$\n\\min_{\\deg(q_{n}) \\leq n} \\max_{t \\in [-1,1]} |g(t) - q_{n}(t)| \\leq \\frac{2 M \\,\\rho^{-n}}{\\rho - 1}.\n$$\nThis bound can be established by representing $g$ via its Chebyshev series $g(t) = \\sum_{k=0}^{\\infty} a_{k} T_{k}(t)$, showing that $|a_{k}| \\leq \\frac{2 M}{\\rho^{k}}$ for $k \\geq 1$ under the analyticity and boundedness assumptions, and summing the tail from $k = n+1$ to infinity using the geometric series bound. The factor $(\\rho - 1)^{-1}$ arises from the summation $\\sum_{k=n+1}^{\\infty} \\rho^{-k} = \\rho^{-(n+1)} \\frac{1}{1 - \\rho^{-1}} = \\frac{\\rho^{-n}}{\\rho - 1}$ up to a constant prefactor controlled by the coefficient estimates.\n\nTranslating back to $x \\in [a,b]$, for the corresponding polynomial $p_{n}$ in $x$ obtained by composition, we have\n$$\n\\max_{x \\in [a,b]} |f(x) - p_{n}(x)| \\leq \\frac{2 M \\,\\rho^{-n}}{\\rho - 1}.\n$$\nCombining this with the matrix inequality derived from the spectral calculus and unitary invariance yields\n$$\n\\|f(A) - p_{n}(A)\\|_{2} \\leq \\max_{x \\in [a,b]} |f(x) - p_{n}(x)| \\leq \\frac{2 M \\,\\rho^{-n}}{\\rho - 1}.\n$$\nTherefore, the desired rigorous upper bound on the operator $2$-norm error is given by the closed-form analytic expression\n$$\n\\frac{2 M \\,\\rho^{-n}}{\\rho - 1}.\n$$\nThis expression depends only on the Bernstein ellipse parameter $\\rho  1$, the bound $M$ on $f$ over the ellipse in the mapped variable, and the polynomial degree $n$, and is independent of the matrix dimension $N$ thanks to the spectral characterization for Hermitian matrices.",
            "answer": "$$\\boxed{\\frac{2 M \\,\\rho^{-n}}{\\rho - 1}}$$"
        },
        {
            "introduction": "Iterative methods for large-scale eigenvalue problems, such as the Arnoldi or Lanczos methods, must be restarted to remain practical. This exercise delves into the crucial design choice of a restart strategy, comparing the mechanisms of thick-restart and implicit-restart techniques . You will analyze how these approaches differ in preserving desired spectral information and filtering out unwanted components, developing the critical intuition needed to select and tune modern eigensolvers for performance and robustness.",
            "id": "3525847",
            "problem": "In large-scale simulations of Quantum Chromodynamics (QCD), one frequently needs extremal eigenpairs of discretized lattice operators such as the Hermitian form of the Wilson-Dirac operator. Let $A \\in \\mathbb{C}^{n \\times n}$ be a large, sparse matrix arising from such a discretization, and consider computing $r$ extremal eigenpairs using a restarted projection method on a Krylov subspace of dimension $m$ with $r  m \\ll n$. The Rayleigh-Ritz process in a Krylov subspace $\\mathcal{K}_m(A, v_1)$ yields Ritz values and Ritz vectors from the projected Hessenberg or tridiagonal matrix. Two restart strategies are commonly employed: thick-restart, which retains $r$ selected Ritz vectors to seed the next cycle, and implicit restart, which applies a sequence of $k$ shifts via the implicit shifted QR process to the projected matrix prior to restarting.\n\nStarting from the Arnoldi relation $A V_m = V_m H_m + h_{m+1,m} v_{m+1} e_m^\\top$ with $V_m \\in \\mathbb{C}^{n \\times m}$ having orthonormal columns spanning $\\mathcal{K}_m(A, v_1)$ and $H_m \\in \\mathbb{C}^{m \\times m}$ upper Hessenberg, use the definition of the Krylov subspace and the Rayleigh-Ritz projection to reason about the effect of $k$ implicit QR shifts at scalars $\\mu_1, \\dots, \\mu_k$ chosen near unwanted Ritz values on the restarted starting vector and subspace. In particular, justify that this process can be viewed as inducing a polynomial filter on components of $v_1$ along eigenvectors associated with unwanted eigenvalues, and contrast this mechanism with thick-restart which explicitly retains $r$ Ritz vectors.\n\nBased on this derivation and considering typical computational workflows in high-energy physics (for example, deflation of low modes of a Hermitian Dirac operator), select all statements that are correct:\n\nA. In implicit-restart Arnoldi with shifts equal to unwanted Ritz values $\\mu_1, \\dots, \\mu_k$, the restarted starting vector is proportional to $p(A) v_1$, where $p(\\lambda) = \\prod_{i=1}^{k} (\\lambda - \\mu_i)$, which suppresses components along eigenvectors associated with those unwanted Ritz values.\n\nB. Thick-restart Arnoldi applies the same polynomial filter $p(A)$ to the starting vector but additionally preserves $r$ Ritz vectors, thereby providing stronger suppression of unwanted components than implicit restart.\n\nC. For a Hermitian matrix $A$ with a cluster of target eigenvalues, thick-restart Lanczos tends to be robust because retaining an approximate invariant subspace of dimension $r$ reduces the risk of losing converged Ritz directions during restarts.\n\nD. Per cycle memory footprint is larger for implicit-restart Arnoldi than for thick-restart Arnoldi because the former must store $m$ basis vectors plus additional storage for the retained Ritz vectors, whereas the latter only needs to store the $m - r$ new expansion vectors.\n\nE. Choosing shifts $\\mu_i$ equal to the desired target eigenvalues accelerates convergence in implicit restart by amplifying their components in the restarted starting vector.",
            "solution": "The problem statement provides a detailed and accurate description of restarted Krylov subspace methods used for large-scale eigenvalue problems, specifically within the context of computational high-energy physics. It is scientifically grounded, well-posed, and objective. All terms are standard within the field of numerical linear algebra. The premises are factually correct, and the question posed is a substantive one about the mechanisms and properties of these algorithms. Therefore, the problem statement is valid.\n\nWe begin by deriving the operational principles of the two restart strategies mentioned. The foundation for both is the Arnoldi factorization generated after $m$ steps:\n$$ A V_m = V_m H_m + h_{m+1,m} v_{m+1} e_m^\\top $$\nHere, $V_m = [v_1, v_2, \\dots, v_m] \\in \\mathbb{C}^{n \\times m}$ is a matrix with orthonormal columns that span the Krylov subspace $\\mathcal{K}_m(A, v_1) = \\text{span}\\{v_1, A v_1, \\dots, A^{m-1} v_1\\}$. $H_m \\in \\mathbb{C}^{m \\times m}$ is an upper Hessenberg matrix representing the projection of the operator $A$ onto $\\mathcal{K}_m(A, v_1)$, and $v_1$ is the starting vector with $\\|v_1\\|_2 = 1$. The term $h_{m+1,m} v_{m+1} e_m^\\top$ is the residual term. The Rayleigh-Ritz procedure approximates eigenpairs of $A$ by computing the eigenpairs of $H_m$. If $(\\theta_j, s_j)$ is an eigenpair of $H_m$, i.e., $H_m s_j = \\theta_j s_j$, then $(\\theta_j, y_j = V_m s_j)$ is the corresponding Ritz pair (Ritz value and Ritz vector) for $A$.\n\n### Implicit Restart Mechanism\n\nThe Implicitly Restarted Arnoldi Method (IRAM) aims to distill the \"wanted\" spectral information from $\\mathcal{K}_m(A, v_1)$ into a smaller subspace of dimension $r$ without explicitly forming Ritz vectors. This is achieved by applying $k = m-r$ steps of the implicitly shifted QR algorithm to the small Hessenberg matrix $H_m$. Let the shifts be $\\mu_1, \\dots, \\mu_k$.\n\nA single implicit QR step with a shift $\\mu$ transforms $H_m$ into $H_m' = Q_1^* H_m Q_1$, where $Q_1$ is a unitary matrix. The key is that this transformation is initiated by computing the first column of $Q_1$. The first step of the QR algorithm is to form $H_m - \\mu I = Q_1 R_1$. The first column of $Q_1$ is thus proportional to the first column of $H_m - \\mu I$, which is $(h_{11}-\\mu) e_1 + h_{21} e_2$. More generally, after $k$ shifts, the total transformation is $Q = Q_1 Q_2 \\dots Q_k$, and the resulting Hessenberg matrix is $H_m' = Q^* H_m Q$.\n\nCrucially, the transformation $Q$ is constructed such that its first column, $q_1$, is proportional to $p(H_m)e_1$, where $p(\\lambda) = \\prod_{i=1}^k (\\lambda - \\mu_i)$ is the filter polynomial. The Arnoldi relation is updated by post-multiplying by $Q$:\n$$ A (V_m Q) = (V_m Q) (Q^* H_m Q) + h_{m+1,m} v_{m+1} e_m^\\top Q $$\nLet $V_m' = V_m Q$ and $H_m' = Q^* H_m Q$. The restarted Arnoldi factorization of dimension $r$ is obtained by truncating this relation to the first $r$ columns. The new starting vector for the implicitly restarted process is the first column of $V_m'$, which we denote $v_1^{(+)}$.\n$$ v_1^{(+)} = V_m' e_1 = V_m Q e_1 = V_m q_1 $$\nSince $q_1$ is proportional to $p(H_m)e_1$, the new starting vector is proportional to $V_m p(H_m) e_1$. We can demonstrate a direct link to $p(A)v_1$. For a single shift $\\mu$, $q_1 \\propto (H_m - \\mu I)e_1$. The new start vector is thus proportional to $V_m(H_m - \\mu I)e_1 = V_m H_m e_1 - \\mu V_m e_1$. From the Arnoldi relation, $A v_1 = A V_m e_1 = V_m H_m e_1 + h_{m+1,m} v_{m+1} e_m^\\top e_1$. For $m1$, $e_m^\\top e_1=0$, so $A v_1 = V_m H_m e_1$. Substituting this in, we find the new start vector is proportional to $A v_1 - \\mu v_1 = (A - \\mu I) v_1$.\n\nBy induction, for $k$ shifts, the new starting vector $v_1^{(+)}$ is proportional to $p(A)v_1 = \\prod_{i=1}^k (A - \\mu_i)v_1$. If $v_1 = \\sum_{j=1}^n c_j u_j$ is the expansion of the initial vector in the eigenbasis of $A$ (where $A u_j = \\lambda_j u_j$), then the restarted vector is proportional to:\n$$ p(A)v_1 = \\sum_{j=1}^n c_j p(\\lambda_j) u_j = \\sum_{j=1}^n c_j \\left(\\prod_{i=1}^k (\\lambda_j - \\mu_i)\\right) u_j $$\nBy choosing the shifts $\\mu_i$ to be the unwanted Ritz values, which approximate the unwanted eigenvalues of $A$, the polynomial $p(\\lambda_j)$ becomes small for these eigenvalues, thus suppressing their components in the restarted vector. This purifies the vector, enriching it with components of the desired eigenvectors.\n\n### Thick Restart Mechanism\n\nThick restart is a more explicit procedure. After an $m$-step Arnoldi/Lanczos cycle, one computes the $m$ Ritz pairs $(\\theta_j, y_j=V_m s_j)$ by finding the eigenpairs of $H_m$. One then selects the $r$ \"best\" Ritz pairs that approximate the desired eigenpairs of $A$. Let these be $\\{(\\theta_j, y_j)\\}_{j=1}^r$.\n\nThe next cycle is then \"restarted\" not from a single vector, but from the subspace spanned by these $r$ Ritz vectors. The set of orthonormalized Ritz vectors $\\{y_1, \\dots, y_r\\}$ forms the first $r$ columns of the basis for the next cycle. The Krylov subspace is then expanded by $m-r$ further steps, typically starting from a vector related to the residuals of the retained Ritz vectors. This procedure explicitly retains or \"locks\" an approximate invariant subspace, which is a very direct way of preserving the progress made in locating the desired eigenvectors.\n\n---\n\n### Analysis of the Options\n\n**A. In implicit-restart Arnoldi with shifts equal to unwanted Ritz values $\\mu_1, \\dots, \\mu_k$, the restarted starting vector is proportional to $p(A) v_1$, where $p(\\lambda) = \\prod_{i=1}^{k} (\\lambda - \\mu_i)$, which suppresses components along eigenvectors associated with those unwanted Ritz values.**\n\nThis statement is a direct consequence of the derivation above. The restarted vector is $v_1^{(+)} \\propto p(A)v_1$. If the shifts $\\mu_i$ are chosen to match unwanted Ritz values, they serve as good approximations of unwanted eigenvalues $\\lambda_j$. For such a $\\lambda_j$, the term $(\\lambda_j - \\mu_i)$ will be small for some $i$, making the polynomial value $p(\\lambda_j)$ small. This suppresses the contribution of the corresponding eigenvector $u_j$ to the new starting vector, which is the intended purpose of the filtering process. This statement is a correct description of the IRAM mechanism.\n**Verdict: Correct.**\n\n**B. Thick-restart Arnoldi applies the same polynomial filter $p(A)$ to the starting vector but additionally preserves $r$ Ritz vectors, thereby providing stronger suppression of unwanted components than implicit restart.**\n\nThis statement is fundamentally incorrect as it conflates the mechanisms of the two methods. Thick-restart does not operate by applying a polynomial filter to the initial vector. Its mechanism is the explicit retention of a set of $r$ Ritz vectors that form an approximate invariant subspace. The concept of a filter polynomial $p(A)$ is specific to the implicit restart method.\n**Verdict: Incorrect.**\n\n**C. For a Hermitian matrix $A$ with a cluster of target eigenvalues, thick-restart Lanczos tends to be robust because retaining an approximate invariant subspace of dimension $r$ reduces the risk of losing converged Ritz directions during restarts.**\n\nWhen eigenvalues are clustered, Krylov methods may need a larger subspace dimension $m$ to resolve them. Once good approximations to the eigenvectors spanning the invariant subspace associated with the cluster are found, it is crucial not to lose this information in a restart. Thick restart, by its very design, explicitly keeps the $r$-dimensional space spanned by the selected Ritz vectors. This direct preservation of the approximate invariant subspace makes it a very robust strategy in the presence of clustered eigenvalues, as it prevents the algorithm from inadvertently discarding the hard-won information about the cluster. The term Lanczos is correctly used, as it is the specialization of the Arnoldi method for Hermitian matrices.\n**Verdict: Correct.**\n\n**D. Per cycle memory footprint is larger for implicit-restart Arnoldi than for thick-restart Arnoldi because the former must store $m$ basis vectors plus additional storage for the retained Ritz vectors, whereas the latter only needs to store the $m - r$ new expansion vectors.**\n\nThis statement contains flawed reasoning about the memory usage of both methods. The dominant memory cost for both algorithms is the storage of the basis for the Krylov subspace, which is of size $m$ (or $m+1$ to include the next vector).\n- **Implicit Restart:** At its peak, it holds the basis $V_m \\in \\mathbb{C}^{n \\times m}$. The restart is performed \"in-place\" on $V_m$ and $H_m$. It does *not* require additional storage for Ritz vectors, as they are not explicitly formed.\n- **Thick Restart:** This method also builds a basis of size $m$. It then computes $r$ Ritz vectors, $Y_r = V_m S_r$. In the next cycle, it starts with an orthonormal basis for $Y_r$ and expands it by $m-r$ new vectors. The new basis still grows to size $m$. The statement that it \"only needs to store the $m-r$ new expansion vectors\" is false; it must also store the $r$ vectors it retained.\nIn summary, both methods have a peak memory footprint dominated by the storage of approximately $m$ vectors of size $n$, leading to a memory complexity of $\\mathcal{O}(mn)$. The statement's premise and conclusion are incorrect.\n**Verdict: Incorrect.**\n\n**E. Choosing shifts $\\mu_i$ equal to the desired target eigenvalues accelerates convergence in implicit restart by amplifying their components in the restarted starting vector.**\n\nThis statement describes the opposite of the correct strategy. As shown in the derivation for implicit restart, the effect of the filter is to multiply eigenvector components by $p(\\lambda_j) = \\prod_{i=1}^k (\\lambda_j - \\mu_i)$. If the shifts $\\mu_i$ are chosen to be equal to the desired target eigenvalues $\\lambda_j^*$, then $p(\\lambda_j^*)$ would be zero. This would annihilate, not amplify, the components of the starting vector along the desired eigenvectors, effectively purging the subspace of the very information we seek. The correct strategy is to choose shifts equal to the *unwanted* eigenvalues to suppress their components.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "The ultimate test of a numerical algorithm is its performance on target hardware. This practice moves from theoretical analysis to practical optimization by asking you to implement a performance model for a distributed-memory QR algorithm . Your task is to find the optimal 'bulge-chasing' block size that minimizes runtime by balancing the trade-off between floating-point operations and inter-process communication, a core challenge in high-performance scientific computing.",
            "id": "3525899",
            "problem": "You are tasked with constructing and using a principled performance model for the eigenvalue computation of a large, real symmetric tridiagonal matrix via the implicit multishift $QR$ algorithm with bulge chasing on a distributed-memory system. Such matrices arise in computational high-energy physics when discretizing Hermitian Hamiltonians, where the tridiagonal form is obtained after similarity transformations. Your program must select an integer bulge-chasing block size $b$ that optimizes the trade-off between floating-point operations and inter-process communication.\n\nThe starting point is the latency-bandwidth-compute performance model, which is widely used for large-scale scientific computing. It states that the time to perform an algorithm on a distributed-memory system can be approximated by\n$$\nT \\approx \\alpha M + \\beta W + \\gamma F,\n$$\nwhere $\\alpha$ is the message latency (in seconds per message), $M$ is the number of messages, $\\beta$ is the inverse bandwidth (in seconds per word), $W$ is the number of words communicated, $\\gamma$ is the time per floating-point operation (in seconds per flop), and $F$ is the number of floating-point operations.\n\nConsider an $n \\times n$ real symmetric tridiagonal input matrix partitioned into $p$ contiguous blocks across $p$ processes. In the implicit multishift $QR$ algorithm with bulge chasing, a block of $b$ shifts produces a chain of $b$ bulges that traverse the matrix in one sweep. In a simplified, conservative model:\n\n- The floating-point work per sweep is modeled as\n$$\nF_{\\mathrm{sweep}}(b) = c_f \\, n \\, b,\n$$\nwhere $c_f$ is a dimensionless constant capturing the average flops required per matrix position per shift during the application and propagation of bulges in the tridiagonal structure.\n\n- The number of messages per sweep is modeled as\n$$\nM_{\\mathrm{sweep}}(b) = 2(p-1)\\,\\left\\lceil \\frac{p}{b} \\right\\rceil,\n$$\nreflecting that bulge chains cross each of the $p-1$ process boundaries, and aggregation by a factor $b$ reduces the number of distinct communication rounds by approximately the same factor, with the ceiling accounting for discrete rounds of synchronization. The factor $2$ accounts for a forward and a backward direction of halo exchanges or synchronization per round.\n\n- The number of words communicated per sweep is modeled as\n$$\nW_{\\mathrm{sweep}}(b) = 2(p-1)\\,c_w\\, b,\n$$\nwhere $c_w$ is a dimensionless constant modeling the payload size per boundary exchange, scaling linearly with the bulge size $b$ due to halo data associated with the bulge front and its immediate neighborhood in the tridiagonal.\n\nAssuming $S$ sweeps are required to converge and deflate all eigenvalues (this is an input parameter), the total predicted time is\n$$\nT(b) = S\\left[\\alpha\\,M_{\\mathrm{sweep}}(b) + \\beta\\,W_{\\mathrm{sweep}}(b) + \\gamma\\,F_{\\mathrm{sweep}}(b)\\right].\n$$\n\nYour program must, for each provided test case, search over integer block sizes $b \\in \\{1,2,\\dots,b_{\\max}\\}$ to find the minimizer\n$$\nb_\\star = \\arg\\min_{b \\in \\{1,\\dots,b_{\\max}\\}} T(b),\n$$\nbreaking ties in favor of the smallest $b$. Report both $b_\\star$ and the corresponding $T(b_\\star)$.\n\nPhysical units and output requirements:\n- $\\alpha$ must be used in seconds per message.\n- $\\beta$ must be used in seconds per word.\n- $\\gamma$ must be used in seconds per flop.\n- $T(b_\\star)$ must be reported in seconds, rounded to six decimal places.\n\nTest suite:\nCompute $b_\\star$ and $T(b_\\star)$ for the following parameter sets, which are designed to probe different computational regimes:\n1. $n=8192$, $p=32$, $S=8$, $\\alpha=5\\times 10^{-6}$ s/message, $\\beta=2\\times 10^{-9}$ s/word, $\\gamma=1.0\\times 10^{-12}$ s/flop, $c_f=12$, $c_w=8$, $b_{\\max}=64$.\n2. $n=16384$, $p=1$, $S=10$, $\\alpha=5\\times 10^{-6}$ s/message, $\\beta=2\\times 10^{-9}$ s/word, $\\gamma=1.0\\times 10^{-12}$ s/flop, $c_f=12$, $c_w=8$, $b_{\\max}=64$.\n3. $n=4096$, $p=16$, $S=12$, $\\alpha=50\\times 10^{-6}$ s/message, $\\beta=3\\times 10^{-9}$ s/word, $\\gamma=1.5\\times 10^{-12}$ s/flop, $c_f=10$, $c_w=6$, $b_{\\max}=32$.\n4. $n=16384$, $p=64$, $S=6$, $\\alpha=2\\times 10^{-6}$ s/message, $\\beta=1\\times 10^{-9}$ s/word, $\\gamma=5\\times 10^{-12}$ s/flop, $c_f=8$, $c_w=8$, $b_{\\max}=128$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order for the above four cases, the optimal block size followed by the corresponding total predicted time in seconds with six decimal places, i.e.,\n$$\n[\\,b_\\star^{(1)},T(b_\\star^{(1)}),b_\\star^{(2)},T(b_\\star^{(2)}),b_\\star^{(3)},T(b_\\star^{(3)}),b_\\star^{(4)},T(b_\\star^{(4)})\\,].\n$$",
            "solution": "The user has provided a performance modeling problem for a numerical algorithm on a distributed-memory system. A critical prerequisite is the validation of the problem statement for scientific soundness, clarity, and completeness.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\nThe problem defines a performance model for the total time $T$ to compute eigenvalues of an $n \\times n$ matrix using the implicit multishift $QR$ algorithm. The model depends on an integer bulge-chasing block size, $b$.\n\nThe total time for $S$ sweeps is given by:\n$$\nT(b) = S\\left[\\alpha\\,M_{\\mathrm{sweep}}(b) + \\beta\\,W_{\\mathrm{sweep}}(b) + \\gamma\\,F_{\\mathrm{sweep}}(b)\\right]\n$$\nThe components of the model are defined as:\n- Number of floating-point operations per sweep: $F_{\\mathrm{sweep}}(b) = c_f \\, n \\, b$.\n- Number of messages per sweep: $M_{\\mathrm{sweep}}(b) = 2(p-1)\\,\\left\\lceil \\frac{p}{b} \\right\\rceil$.\n- Number of words communicated per sweep: $W_{\\mathrm{sweep}}(b) = 2(p-1)\\,c_w\\, b$.\n\nHere, $n$ is the matrix dimension, $p$ is the number of processes, $S$ is the number of sweeps, $\\alpha$ is message latency, $\\beta$ is inverse bandwidth, $\\gamma$ is time per floating-point operation, and $c_f$ and $c_w$ are dimensionless modeling constants.\n\nThe objective is to find the optimal block size $b_\\star$ that minimizes $T(b)$:\n$$\nb_\\star = \\arg\\min_{b \\in \\{1,2,\\dots,b_{\\max}\\}} T(b)\n$$\nA tie-breaking rule specifies selecting the smallest $b$ in case of multiple minima.\n\nFour specific test cases are provided with all necessary parameter values ($n, p, S, \\alpha, \\beta, \\gamma, c_f, c_w, b_{\\max}$).\n\n**Step 2: Validate Using Extracted Givens**\n1.  **Scientifically Grounded**: The problem is well-grounded in the field of high-performance computing (HPC) and numerical linear algebra. The latency-bandwidth-compute model ($T \\approx \\alpha M + \\beta W + \\gamma F$) is a standard, albeit simplified, tool for performance analysis of parallel algorithms. The specific models for $F$, $M$, and $W$ as functions of the block size $b$ are plausible abstractions for the behavior of a bulge-chasing algorithm. The context—discretized Hamiltonians in high-energy physics leading to tridiagonal eigenvalue problems—is authentic. The problem does not violate any scientific or mathematical principles.\n2.  **Well-Posed**: The optimization problem is well-posed. The objective function $T(b)$ is clearly defined for a finite, discrete domain of integers $b \\in \\{1, 2, \\dots, b_{\\max}\\}$. A minimum value is guaranteed to exist. The tie-breaking rule ensures that the solution $b_\\star$ is unique.\n3.  **Objective**: The problem is stated using precise, objective, and formal mathematical language. All terms are defined, and all required data are provided. There are no subjective or ambiguous statements.\n4.  **Complete and Consistent**: The problem is self-contained. It provides all necessary equations, parameters, and constraints to arrive at a solution. There are no internal contradictions. For instance, the case where $p=1$ correctly results in zero communication cost, as the terms $(p-1)$ become zero.\n5.  **Feasible**: The parameter values provided in the test suite are physically realistic for modern distributed computing systems.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. It is a standard numerical optimization task based on a scientifically sound performance model. I will now proceed with the solution.\n\n### Solution\n\nThe task is to find the optimal block size $b_\\star$ that minimizes the total computation time $T(b)$ for a given set of parameters. The function to be minimized is:\n$$\nT(b) = S\\left[\\alpha\\,M_{\\mathrm{sweep}}(b) + \\beta\\,W_{\\mathrm{sweep}}(b) + \\gamma\\,F_{\\mathrm{sweep}}(b)\\right]\n$$\nSubstituting the given expressions for the sweep components, we obtain:\n$$\nT(b) = S \\left[ \\alpha \\cdot 2(p-1)\\left\\lceil \\frac{p}{b} \\right\\rceil + \\beta \\cdot 2(p-1)c_w b + \\gamma \\cdot c_f n b \\right]\n$$\nThis expression can be rearranged to highlight the dependency on $b$:\n$$\nT(b) = \\left( 2 S \\alpha (p-1) \\right) \\left\\lceil \\frac{p}{b} \\right\\rceil + \\left( S(2\\beta(p-1)c_w + \\gamma c_f n) \\right) b\n$$\nThe structure of this function reveals a fundamental trade-off in parallel algorithm design.\n1.  The first term, proportional to $\\left\\lceil p/b \\right\\rceil$, represents the total time spent on communication latency. This is a monotonically non-increasing, piecewise constant function of $b$. Increasing the block size $b$ allows for the aggregation of work, reducing the number of communication rounds and thus amortizing latency costs.\n2.  The second term, which is linear in $b$, represents the combined cost of communication bandwidth and floating-point computation. Both the volume of data transferred ($W_{\\mathrm{sweep}}$) and the computational work ($F_{\\mathrm{sweep}}$) are modeled to scale linearly with $b$. This term is monotonically increasing.\n\nThe optimal block size $b_\\star$ is the integer value that best balances these two competing effects. For small $b$, the latency term dominates, while for large $b$, the bandwidth and computation term dominates. The minimum of $T(b)$ typically lies at an intermediate value.\n\nA special case arises when $p=1$. In this single-process scenario, the term $(p-1)$ becomes $0$, which correctly eliminates both communication terms ($M_{\\mathrm{sweep}}$ and $W_{\\mathrm{sweep}}$). The time function simplifies to:\n$$\nT(b) = S \\gamma c_f n b \\quad (\\text{for } p=1)\n$$\nThis is a purely linear, increasing function of $b$. Therefore, for any single-process case, the minimum time will always occur at the smallest possible block size, i.e., $b_\\star=1$.\n\nGiven that the search space for $b$ is the small, finite set of integers from $1$ to $b_{\\max}$, the most direct and robust method for finding the minimum is an exhaustive search. The algorithm proceeds as follows for each test case:\n1.  Initialize a minimum time found so far, $T_{\\min}$, to a value of infinity and the best block size, $b_\\star$, to an invalid marker.\n2.  Iterate through each integer $b$ from $1$ to $b_{\\max}$.\n3.  For each $b$, calculate the total time $T(b)$ using the provided formula. The ceiling function $\\lceil p/b \\rceil$ is computed using standard mathematical library functions.\n4.  Compare the calculated $T(b)$ with the current minimum, $T_{\\min}$. If $T(b)  T_{\\min}$, update $T_{\\min} = T(b)$ and $b_\\star = b$. The strict inequality handles the specified tie-breaking rule, ensuring that the smallest $b$ that achieves the minimum time is selected.\n5.  After iterating through all possible values of $b$, the final pair $(b_\\star, T_{\\min})$ represents the solution for that test case.\n\nThe final program implements this search procedure for each of the four parameter sets and formats the output as required.",
            "answer": "$$\\boxed{[16,0.010471,1,0.019661,16,0.010834,32,0.021677]}$$"
        }
    ]
}