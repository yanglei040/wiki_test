{
    "hands_on_practices": [
        {
            "introduction": "格点计算的原始输出通常是自相关的马尔可夫链蒙特卡洛（MCMC）时间序列数据。在进行任何连续极限外推之前，我们必须首先为每个格点间距 $a$ 上的观测量均值估算出可靠的统计误差。这项实践  将引导您完成从处理原始数据到获得最终结果的整个标准流程，包括估算积分自相关时间、应用“分箱”方法以获得近似独立的样本，以及使用“自助法”（bootstrap）来将统计误差正确地传递到连续极限外推的最终结果中。掌握这一流程是进行任何可靠的格点数据分析的基础。",
            "id": "3509811",
            "problem": "你的任务是为一个在多个格点间距下测量的格点可观测量实现一个统计上可靠的连续谱外推，并考虑时间序列数据中的自相关。你编写的程序必须估计积分自相关时间，选择一个能控制残余相关性的分块大小，并通过分块 bootstrap 程序将统计误差传播到连续谱极限估计中。\n\n从以下基本原理出发：\n- 对于一个均值为 $m$、方差为 $\\sigma^{2}$、在整数延迟 $t$ 处的归一化自相关函数为 $\\rho(t)$ 的平稳时间序列， $N$ 个相关观测值的样本均值的方差会因积分自相关时间 $\\tau_{\\mathrm{int}}$ 而增大，其定义为\n$$\n\\tau_{\\mathrm{int}}=\\frac{1}{2}+\\sum_{t=1}^{\\infty}\\rho(t),\n$$\n一个实用的估计量通过一个由自洽规则选择的有限窗口 $W$ 来代替 $\\infty$。\n- 在 Symanzik 有效理论对适当改进的格点作用量和算符的假设下，许多可观测量的领头截断效应的标度行为为 $\\mathcal{O}(a^{2})$，因此可以将其对 $a$ 的依赖关系建模为\n$$\n\\mathcal{O}(a)=\\mathcal{O}_{0}+c_{1}\\,a^{2},\n$$\n并通过对 $a^{2}$ 进行线性拟合来确定连续谱极限 $\\mathcal{O}_{0}$。\n\n实现以下内容，所有量均为无量纲（不需要物理单位）：\n\n1) 时间序列生成。对于测试用例中的每个格点间距 $a$，生成一个一阶确定性自回归过程 (AR(1))。其均值由 Symanzik 模型给出，新息项由一个固定的线性同余生成器 (LCG) 生成且服从均匀分布。具体来说，对于给定的 $a$，定义目标均值\n$$\n\\mu(a)=\\mathcal{O}_{0}^{\\mathrm{true}}+c_{1}^{\\mathrm{true}}\\,a^{2},\n$$\n并令序列 $\\{X_{t}^{(a)}\\}_{t=1}^{N}$ 的 AR(1) 递归式为\n$$\nX_{t+1}^{(a)}=\\mu(a)+\\phi(a)\\left(X_{t}^{(a)}-\\mu(a)\\right)+\\epsilon_{t}^{(a)},\n$$\n其中 $\\phi(a)=1-\\frac{1}{L_{\\mathrm{corr}}(a)}$，新息 $\\epsilon_{t}^{(a)}=\\sigma(a)\\left(2U_{t}^{(a)}-1\\right)$，而 $\\{U_{t}^{(a)}\\}$ 是来自 LCG 的确定性伪随机序列\n$$\ns_{t+1}=(\\alpha s_{t}+\\gamma)\\bmod M,\\quad U_{t}=\\frac{s_{t}}{M},\n$$\n参数为 $M=2^{32}$，$\\alpha=1664525$，$\\gamma=1013904223$。对每个序列使用为其提供的特定于 $a$ 的整数种子 $s_{0}$。对于每个时间序列，生成 $N_{\\mathrm{burn}}=50+10\\,L_{\\mathrm{corr}}(a)$ 个预烧步并将其丢弃，然后保留接下来的 $N$ 个点作为该 $a$ 的数据集。\n\n2) 积分自相关时间估计。对于每个生成的时间序列，使用带有自洽窗口选择的标准加窗估计量来估计积分自相关时间 $\\tau_{\\mathrm{int}}$。设 $\\bar{X}$ 为样本均值，$C(t)$ 为延迟 $t$ 处的无偏样本自协方差。定义归一化自相关 $\\rho(t)=C(t)/C(0)$。对于选定的窗口大小 $W$，定义\n$$\n\\hat{\\tau}_{\\mathrm{int}}(W)=\\frac{1}{2}+\\sum_{t=1}^{W}\\rho(t).\n$$\n选择 $W$ 为满足 $W\\geq c\\,\\hat{\\tau}_{\\mathrm{int}}(W)$（其中 $c=6$）的最小延迟，同时受限于 $W\\leq W_{\\max}=\\min\\{N-1,1000\\}$。如果在 $W_{\\max}$ 范围内不存在这样的 $W$，则设 $W=W_{\\max}$。报告估计值 $\\hat{\\tau}_{\\mathrm{int}}=\\max\\{\\frac{1}{2},\\hat{\\tau}_{\\mathrm{int}}(W)\\}$。\n\n3) 分块与分块 bootstrap。对每个时间序列：\n- 选择整数分块大小 $B_{\\mathrm{blk}}=\\max\\left\\{1,\\left\\lceil 2\\,\\hat{\\tau}_{\\mathrm{int}}\\right\\rceil\\right\\}$。\n- 将序列划分为 $M=\\left\\lfloor N/B_{\\mathrm{blk}}\\right\\rfloor$ 个长度为 $B_{\\mathrm{blk}}$ 的不重叠的块，并计算这 $M$ 个块的均值。如果 $M=0$，则使用一个等于总体样本均值的单独的块。\n- 实现一个具有 $R$ 个 bootstrap 复制的分块 bootstrap。在每次复制中，有放回地重抽样 $M$ 个块，并取重抽样后块均值的平均值，以获得在每个格点间距下的一个均值的 bootstrap 估计。使用与上述相同的固定 LCG 来生成 bootstrap 重抽样索引，其中复制和格点间距序列从一个指定的 bootstrap 种子开始连续生成。\n\n4) 连续谱外推。对于每个 bootstrap 复制，将给定 $a$ 值下的 bootstrap 均值 $\\{Y^{(a)}\\}$ 对模型\n$$\nY^{(a)}=\\mathcal{O}_{0}+c_{1}\\,a^{2},\n$$\n进行无权线性最小二乘拟合，并记录 $\\mathcal{O}_{0}$ 的 bootstrap 值。使用 $\\mathcal{O}_{0}$ 的 bootstrap 分布的样本均值作为点估计，使用样本标准差（分母为 $R-1$）作为标准误。\n\n数值要求与测试套件：\n- 你必须为以下三个测试用例实现上述工作流程。下面所有数字都是精确的，必须按给定的值使用。\n\n测试用例 A：\n- 格点间距: $a\\in\\{0.12,0.09,0.06\\}$。\n- 序列长度: $N=512$。\n- 相关长度: $L_{\\mathrm{corr}}(a)\\in\\{12,24,48\\}$，分别对应于 $a=0.12,0.09,0.06$。\n- 真实参数: $\\mathcal{O}_{0}^{\\mathrm{true}}=1.2345$, $c_{1}^{\\mathrm{true}}=2.75$。\n- 新息振幅: $\\sigma(a)\\equiv 0.25$ 对所有 $a$。\n- 新息种子: $s_{0}\\in\\{123456789,987654321,42424242\\}$，分别对应于 $a=0.12,0.09,0.06$。\n- Bootstrap 复制次数: $R=499$。\n- Bootstrap 种子: $s_{0}^{\\mathrm{boot}}=20250601$。\n\n测试用例 B：\n- 格点间距: $a\\in\\{0.10,0.07,0.05,0.04\\}$。\n- 序列长度: $N=384$。\n- 相关长度: $L_{\\mathrm{corr}}(a)\\in\\{8,16,24,32\\}$，分别对应于 $a=0.10,0.07,0.05,0.04$。\n- 真实参数: $\\mathcal{O}_{0}^{\\mathrm{true}}=0.5$, $c_{1}^{\\mathrm{true}}=-1.2$。\n- 新息振幅: $\\sigma(a)\\equiv 0.20$ 对所有 $a$。\n- 新息种子: $s_{0}\\in\\{13579,24680,112233,998877\\}$，分别对应于 $a=0.10,0.07,0.05,0.04$。\n- Bootstrap 复制次数: $R=399$。\n- Bootstrap 种子: $s_{0}^{\\mathrm{boot}}=1357911$。\n\n测试用例 C：\n- 格点间距: $a\\in\\{0.16,0.08,0.04\\}$。\n- 序列长度: $N=256$。\n- 相关长度: $L_{\\mathrm{corr}}(a)\\in\\{2,4,6\\}$，分别对应于 $a=0.16,0.08,0.04$。\n- 真实参数: $\\mathcal{O}_{0}^{\\mathrm{true}}=-0.2$, $c_{1}^{\\mathrm{true}}=0.8$。\n- 新息振幅: $\\sigma(a)\\equiv 0.15$ 对所有 $a$。\n- 新息种子: $s_{0}\\in\\{314159,271828,161803\\}$，分别对应于 $a=0.16,0.08,0.04$。\n- Bootstrap 复制次数: $R=299$。\n- Bootstrap 种子: $s_{0}^{\\mathrm{boot}}=424242$。\n\n实现规范和输出规格：\n- 对于自协方差估计，使用无偏估计量 $C(t)=\\frac{1}{N-t}\\sum_{i=1}^{N-t}(X_{i}-\\bar{X})(X_{i+t}-\\bar{X})$ 直到 $t=W_{\\max}$，并且当 $C(0)>0$ 时，$\\rho(t)=C(t)/C(0)$。如果 $C(0)\\leq 0$，则设 $\\hat{\\tau}_{\\mathrm{int}}=\\frac{1}{2}$。\n- 对于自洽窗口选择，使用 $c=6$ 和 $W_{\\max}=\\min\\{N-1,1000\\}$。\n- 对于分块，使用 $B_{\\mathrm{blk}}=\\max\\left\\{1,\\left\\lceil 2\\,\\hat{\\tau}_{\\mathrm{int}}\\right\\rceil\\right\\}$ 和 $M=\\left\\lfloor N/B_{\\mathrm{blk}}\\right\\rfloor$。如果 $M=0$，则使用一个其均值为样本均值的单独的块。\n- 对于分块 bootstrap 重抽样，在每次复制中，使用指定 LCG 从 $\\{0,1,\\dots,M-1\\}$ 中独立均匀地抽取 $M$ 个索引。该 LCG 的状态初始化为给定的 bootstrap 种子，并在该测试用例内的所有抽取和格点间距中顺序推进。\n- 对于线性拟合，对每个 bootstrap 复制，执行 $Y$ 对 $a^{2}$ 的普通最小二乘拟合，以估计 $\\mathcal{O}_{0}$ 和 $c_{1}$。\n\n最终输出格式：\n- 对于每个测试用例，报告一个列表，其中包含：\n  1) 按照给定 $a$ 的顺序排列的三个或四个 $\\hat{\\tau}_{\\mathrm{int}}$ 值，每个值四舍五入到6位小数，\n  2) 相应的三个或四个整数分块大小 $B_{\\mathrm{blk}}$，\n  3) 所有复制中 $\\mathcal{O}_{0}$ 的 bootstrap 均值，四舍五入到6位小数，\n  4) 所有复制中 $\\mathcal{O}_{0}$ 的 bootstrap 标准差，四舍五入到6位小数。\n- 你的程序应生成单行输出，其中按顺序包含测试用例 A、测试用例 B 和测试用例 C 的结果，形式为一个用方括号括起来的逗号分隔列表，其中每个元素是上述描述的每个测试的列表。例如，输出结构必须为\n$$\n[\\,[\\hat{\\tau}_{\\mathrm{int}}(a_{1}),\\dots,\\hat{\\tau}_{\\mathrm{int}}(a_{K}),B_{\\mathrm{blk}}(a_{1}),\\dots,B_{\\mathrm{blk}}(a_{K}),\\widehat{\\mathcal{O}}_{0},\\mathrm{SE}(\\widehat{\\mathcal{O}}_{0})],\\;[\\dots],\\;[\\dots]\\,].\n$$\n的形式。所有四舍五入操作必须在打印前应用，每个数值条目都应是标准十进制数。",
            "solution": "用户提供的问题是计算物理学中一个全面且定义明确的练习，特别关注格点模拟数据的统计分析。任务是执行格点可观测量的连续谱外推，这涉及模拟相关数据、分析其自相关特性，并使用分块 bootstrap 方法将统计不确定性传播到最终的连续谱极限估计中。该问题在科学上基于统计力学、时间序列分析以及格点离散化的 Symanzik 有效理论的原理。所有参数、算法和程序都以足够的精度指定，从而可以得到一个唯一的、确定性的解。因此，该问题被认为是有效的，并将提供完整的解决方案。\n\n该解决方案通过遵循一系列规定的步骤来实现：\n\n1.  **时间序列生成**：对于每个给定的格点间距 $a$，生成一个可观测量的合成时间序列。该序列被建模为一阶自回归 (AR(1)) 过程。该过程被构建为围绕一个均值 $\\mu(a)$ 波动，该均值本身根据 Symanzik 有效理论模型依赖于格点间距，即 $\\mu(a)=\\mathcal{O}_{0}^{\\mathrm{true}}+c_{1}^{\\mathrm{true}}\\,a^{2}$。AR(1) 递归式由 $X_{t+1}^{(a)}=\\mu(a)+\\phi(a)\\left(X_{t}^{(a)}-\\mu(a)\\right)+\\epsilon_{t}^{(a)}$ 给出。参数 $\\phi(a)=1-1/L_{\\mathrm{corr}}(a)$ 控制序列的自相关时间，其中 $L_{\\mathrm{corr}}(a)$ 是给定的相关长度。新息 $\\epsilon_{t}^{(a)}$ 是由具有指定参数（$M=2^{32}$, $\\alpha=1664525$, $\\gamma=1013904223$）和种子的确定性线性同余生成器 (LCG) 生成的随机噪声项。每个序列的初始部分，即预烧期 $N_{\\mathrm{burn}}=50+10\\,L_{\\mathrm{corr}}(a)$，被丢弃，以确保生成的数据代表过程的平稳态。\n\n2.  **积分自相关时间估计**：对相关数据进行正确误差分析的关键是积分自相关时间 $\\tau_{\\mathrm{int}}$。对于每个生成的时间序列，我们使用加窗估计量来估计 $\\tau_{\\mathrm{int}}$。首先，计算从延迟 $t=0$ 到最大值 $W_{\\max}$ 的样本自协方差函数 $C(t)$。遵循问题的规定，我们使用公式 $C(t)=\\frac{1}{N-t}\\sum_{i=1}^{N-t}(X_{i}-\\bar{X})(X_{i+t}-\\bar{X})$，其中 $\\bar{X}$ 是整个长度为 $N$ 的时间序列的均值。然后，归一化自相关函数为 $\\rho(t)=C(t)/C(0)$。对于给定的窗口大小 $W$，估计的积分自相关时间为 $\\hat{\\tau}_{\\mathrm{int}}(W)=\\frac{1}{2}+\\sum_{t=1}^{W}\\rho(t)$。窗口大小 $W$ 通过自洽程序选择：它是满足条件 $W\\geq c\\,\\hat{\\tau}_{\\mathrm{int}}(W)$（指定常数 $c=6$）的最小整数延迟。当 $\\rho(t)$ 中的噪声开始主导信号时，此程序会截断求和。如果在 $W \\leq W_{\\max}$ 的范围内任何 $W$ 都不满足此条件，则使用 $W=W_{\\max}$。最终报告的估计值为 $\\hat{\\tau}_{\\mathrm{int}}=\\max\\{\\frac{1}{2},\\hat{\\tau}_{\\mathrm{int}}(W)\\}$。\n\n3.  **分块与分块 Bootstrap**：为了处理自相关并传播统计误差，我们采用了一种分块 bootstrap 方法。首先，每个时间序列被划分为不重叠的块。这些块的大小 $B_{\\mathrm{blk}}=\\max\\left\\{1,\\left\\lceil 2\\,\\hat{\\tau}_{\\mathrm{int}}\\right\\rceil\\right\\}$，被选择为大于自相关时间，从而使得块的均值近似统计独立。序列被分成 $M=\\left\\lfloor N/B_{\\mathrm{blk}}\\right\\rfloor$ 个块，并计算每个块的均值。对于每个格点间距 $a$，这将产生一个新的、短得多的包含 $M$ 个块均值的时间序列。\n    然后执行 bootstrap 分析。对于 $R$ 个 bootstrap 复制中的每一个，都会创建一个新的数据集。对于每个格点间距 $a$，从原始的 $M$ 个块均值集合中有放回地重抽样 $M$ 个块均值。这些重抽样块均值的平均值给出了在该格点间距下可观测量的一个 bootstrap 估计值 $Y^{(a)}$。对所有格点间距重复此过程，使用一个顺序推进的 LCG 实例，为一次 bootstrap 复制生成一整套 $\\{Y^{(a)}\\}$。\n\n4.  **连续谱外推**：对于 $R$ 个 bootstrap 复制中的每一个，都估计连续谱极限 $\\mathcal{O}_{0}$。这是通过对模型 $Y^{(a)}=\\mathcal{O}_{0}+c_{1}\\,a^{2}$ 进行无权线性最小二乘拟合完成的，其中使用 bootstrap 均值 $Y^{(a)}$ 对相应的 $a^2$ 值进行拟合。该拟合的截距提供了 $\\mathcal{O}_{0}$ 的一个 bootstrap 估计。\n\n5.  **最终估计**：在为 $\\mathcal{O}_{0}$ 生成 $R$ 个 bootstrap 估计后，就得到了一个 bootstrap 分布。连续谱值的最终点估计 $\\widehat{\\mathcal{O}}_{0}$ 是该分布的样本均值。相应的统计不确定性 $\\mathrm{SE}(\\widehat{\\mathcal{O}}_{0})$ 由该分布的样本标准差给出，使用分母 $R-1$。\n\n由于使用了为 LCG 指定的种子，整个过程是确定性的。每个测试用例的最终结果被收集起来，四舍五入到六位小数，并格式化为指定的嵌套列表结构。",
            "answer": "```python\nimport numpy as np\nimport math\n\nclass LCG:\n    \"\"\"A Linear Congruential Generator as specified in the problem.\"\"\"\n    def __init__(self, seed, M=2**32, alpha=1664525, gamma=1013904223):\n        self.state = seed\n        self.M = M\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def next_int(self):\n        \"\"\"Generates the next integer in the sequence.\"\"\"\n        self.state = (self.alpha * self.state + self.gamma) % self.M\n        return self.state\n\n    def next_float(self):\n        \"\"\"Generates the next float in [0, 1) in the sequence.\"\"\"\n        return self.next_int() / self.M\n\ndef generate_ar1_series(a, N, L_corr, O0_true, c1_true, sigma, seed):\n    \"\"\"Generates a single AR(1) time series.\"\"\"\n    rng = LCG(seed)\n    mu_a = O0_true + c1_true * a**2\n    phi_a = 1.0 - 1.0 / L_corr\n    N_burn = 50 + 10 * L_corr\n\n    # Initial value at the mean\n    x_t = mu_a\n\n    # Burn-in phase\n    for _ in range(N_burn):\n        u_t = rng.next_float()\n        epsilon_t = sigma * (2 * u_t - 1)\n        x_t = mu_a + phi_a * (x_t - mu_a) + epsilon_t\n\n    # Data generation phase\n    series = np.zeros(N)\n    for t in range(N):\n        u_t = rng.next_float()\n        epsilon_t = sigma * (2 * u_t - 1)\n        x_t = mu_a + phi_a * (x_t - mu_a) + epsilon_t\n        series[t] = x_t\n        \n    return series\n\ndef estimate_tau_int(series, c, W_max):\n    \"\"\"Estimates the integrated autocorrelation time with a self-consistent window.\"\"\"\n    N = len(series)\n    if N  2:\n        return 0.5\n        \n    mean = np.mean(series)\n    series_centered = series - mean\n\n    # Calculate C(0)\n    C0 = np.dot(series_centered, series_centered) / N\n    if C0 = 0:\n        return 0.5\n\n    tau_int_sum = 0.0\n    final_W = W_max\n    \n    for W in range(1, W_max + 1):\n        if W >= N:\n            final_W = W - 1\n            break\n        \n        # Calculate C(W)\n        CW = np.dot(series_centered[:-W], series_centered[W:]) / (N - W)\n        rho_W = CW / C0\n        tau_int_sum += rho_W\n        \n        tau_int_W = 0.5 + tau_int_sum\n        \n        if W >= c * tau_int_W:\n            final_W = W\n            break\n    \n    # Recalculate tau_int up to final_W if the loop finished\n    if W == W_max:\n        tau_int_W_final = 0.5 + tau_int_sum\n    else: # if loop broke early\n        tau_int_W_final = tau_int_W\n\n    return max(0.5, tau_int_W_final)\n\ndef solve_case(case_params):\n    \"\"\"\n    Processes a single test case from data generation to final result.\n    \"\"\"\n    a_s = np.array(case_params['a'])\n    N = case_params['N']\n    L_corrs = case_params['L_corr']\n    O0_true = case_params['O0_true']\n    c1_true = case_params['c1_true']\n    sigmas = case_params['sigma']\n    seeds = case_params['seeds']\n    R = case_params['R']\n    boot_seed = case_params['boot_seed']\n    \n    num_a = len(a_s)\n    \n    all_series = []\n    # Part 1: Generate time series for all lattice spacings\n    for i in range(num_a):\n        series = generate_ar1_series(a_s[i], N, L_corrs[i], O0_true, c1_true, sigmas[i], seeds[i])\n        all_series.append(series)\n\n    tau_ints = []\n    block_sizes = []\n    blocked_data = []\n\n    # Part 2  3: Estimate tau_int and create block means\n    for i in range(num_a):\n        series = all_series[i]\n        W_max = min(N - 1, 1000)\n        \n        tau = estimate_tau_int(series, c=6, W_max=W_max)\n        tau_ints.append(tau)\n        \n        B_blk = max(1, math.ceil(2 * tau))\n        block_sizes.append(B_blk)\n        \n        M = N // B_blk\n        if M == 0:\n            block_means = np.array([np.mean(series)])\n        else:\n            num_pts_to_use = M * B_blk\n            block_means = np.mean(series[:num_pts_to_use].reshape(M, B_blk), axis=1)\n        blocked_data.append(block_means)\n\n    # Part 3  4: Blocked bootstrap and continuum extrapolation\n    boot_rng = LCG(boot_seed)\n    O0_bootstrap_dist = []\n    a_squared = a_s**2\n\n    for _ in range(R):\n        boot_means_Y = []\n        for i in range(num_a):\n            blocks = blocked_data[i]\n            M_i = len(blocks)\n            indices = [int(boot_rng.next_float() * M_i) for _ in range(M_i)]\n            resampled_blocks = blocks[indices]\n            boot_mean = np.mean(resampled_blocks)\n            boot_means_Y.append(boot_mean)\n        \n        # OLS fit to Y = O0 + c1 * a^2\n        Y = np.array(boot_means_Y)\n        X = a_squared\n        \n        X_bar = np.mean(X)\n        Y_bar = np.mean(Y)\n        \n        # Handle case of vertical line (all a are same, not in this problem)\n        if np.sum((X - X_bar)**2) == 0:\n             c1_hat = 0\n        else:\n            c1_hat = np.sum((X - X_bar) * (Y - Y_bar)) / np.sum((X - X_bar)**2)\n        \n        O0_hat = Y_bar - c1_hat * X_bar\n        O0_bootstrap_dist.append(O0_hat)\n    \n    O0_bootstrap_dist = np.array(O0_bootstrap_dist)\n    final_O0_mean = np.mean(O0_bootstrap_dist)\n    final_O0_std_err = np.std(O0_bootstrap_dist, ddof=1)\n\n    # Assemble results\n    results = []\n    for tau in tau_ints:\n        results.append(f\"{tau:.6f}\")\n    for bs in block_sizes:\n        results.append(str(bs))\n    results.append(f\"{final_O0_mean:.6f}\")\n    results.append(f\"{final_O0_std_err:.6f}\")\n    \n    return results\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases_params = [\n        {\n            'a': [0.12, 0.09, 0.06], 'N': 512, 'L_corr': [12, 24, 48],\n            'O0_true': 1.2345, 'c1_true': 2.75, 'sigma': [0.25, 0.25, 0.25],\n            'seeds': [123456789, 987654321, 42424242], 'R': 499, 'boot_seed': 20250601\n        },\n        {\n            'a': [0.10, 0.07, 0.05, 0.04], 'N': 384, 'L_corr': [8, 16, 24, 32],\n            'O0_true': 0.5, 'c1_true': -1.2, 'sigma': [0.20, 0.20, 0.20, 0.20],\n            'seeds': [13579, 24680, 112233, 998877], 'R': 399, 'boot_seed': 1357911\n        },\n        {\n            'a': [0.16, 0.08, 0.04], 'N': 256, 'L_corr': [2, 4, 6],\n            'O0_true': -0.2, 'c1_true': 0.8, 'sigma': [0.15, 0.15, 0.15],\n            'seeds': [314159, 271828, 161803], 'R': 299, 'boot_seed': 424242\n        }\n    ]\n    \n    all_results = []\n    for params in test_cases_params:\n        case_result = solve_case(params)\n        all_results.append(f\"[{','.join(case_result)}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "当我们获得了每个格点间距下可靠的数据点及其误差后，下一个挑战便是为连续极限外推选择合适的拟合模型和数据范围。Symanzik 有效理论预测的渐进行为仅在足够小的格点间距 $a$ 时才成立，而来自较粗格点的数据点可能偏离简单的渐近形式，从而引入系统偏差。这项练习  介绍了一种强大的统计工具——“留一交叉验证”（LOOCV），它提供了一种客观、数据驱动的方法来决定是否应在拟合中包含那些可能存在问题的最粗格点数据，从而提高连续极限估计的可靠性。",
            "id": "3509803",
            "problem": "考虑一个无量纲可观测量的一系列格点测量，该测量根据Symanzik有效理论在渐进行为区进行建模。对于一个给定的、具有已知主离散化指数 $p$ 的作用量，格点间距为 $a$ 时的可观测量 $O(a)$ 假定遵循一个展开式，其领头截断只保留第一个非平凡次幂：\n$$\nO(a) = \\beta_0 + \\beta_1 \\, a^{p} + \\varepsilon,\n$$\n其中 $\\beta_0$ 是连续极限，$\\beta_1$ 代表了主导的离散化效应，而 $\\varepsilon$ 是一个随机误差，其分量是具有已知标准差的独立高斯分布。在实践中，最粗的格点间距 $a_{\\max}$ 可能会违背渐进行为区的假设，导致该数据点无法被所保留的截断式很好地描述。\n\n您的任务是实现一个加权线性回归和一个留一法交叉验证程序，以决定是否从拟合中排除最粗格点间距 $a$ 的数据点，然后报告从所选拟合中得到的连续极限估计值及其统计不确定性。仅使用以下基本假设：独立高斯误差导致加权最小二乘估计量作为最大似然解，以及交叉验证用于比较在留出数据上的预测性能。不要引入任何额外的先验或超参数。\n\n使用的定义：\n- 对每个数据集，定义 $x_i = a_i^{p}$ 和 $y_i = O_i$，其已知标准差为 $\\sigma_i$。\n- 对 $y$ 关于 $x$ 进行加权线性拟合，权重为 $w_i = 1/\\sigma_i^2$。\n- 对数据集 $D = \\{(x_i,y_i,\\sigma_i)\\}_{i=1}^N$，将留一法预测损失定义为\n$$\n\\mathcal{L}_{\\mathrm{LOO}}(D) = \\sum_{j=1}^{N} \\left( \\frac{y_j - \\widehat{y}_j^{(-j)}}{\\sigma_j} \\right)^2,\n$$\n其中 $\\widehat{y}_j^{(-j)}$ 是通过在数据集 $D \\setminus \\{(x_j,y_j,\\sigma_j)\\}$ 上重新拟合加权线性模型得到的在 $x_j$ 处的预测值。\n- 对每个测试用例，计算以下数据集上的 $\\mathcal{L}_{\\mathrm{LOO}}$：\n  1. 完整数据集 $D_{\\mathrm{full}}$。\n  2. 移除具有最大 $a$（即 $a_{\\max}$）的单个数据点后得到的数据集 $D_{\\mathrm{excl}}$。\n- 决策规则：当且仅当 $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{excl}})$ 严格小于 $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{full}})$ 时，排除最粗的点。如果相等，则不排除。\n\n应用决策规则后，对选定的数据集进行拟合，并报告：\n- 是否排除了最粗点的布尔决策。\n- 从所选拟合中得到的连续极限估计值 $\\widehat{\\beta}_0$。\n- $\\widehat{\\beta}_0$ 的统计 $1$-sigma 不确定性，定义为加权最小二乘估计量协方差矩阵的 $(0,0)$ 分量的平方根。\n- 用于比较的两个留一法分数 $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{full}})$ 和 $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{excl}})$。\n\n所有量均为无量纲；不涉及物理单位。\n\n测试套件的输入数据：\n- 测试用例 A (主指数 $p = 2$):\n  - $a$ 值：[$0.12$, $0.09$, $0.06$, $0.045$, $0.03$]\n  - $O(a)$ 值：[$0.5230$, $0.50995$, $0.50410$, $0.50230$, $0.50120$]\n  - $\\sigma$ 值：[$0.0010$, $0.0008$, $0.0007$, $0.0007$, $0.0007$]\n- 测试用例 B (主指数 $p = 2$):\n  - $a$ 值：[$0.12$, $0.09$, $0.06$, $0.045$, $0.03$]\n  - $O(a)$ 值：[$0.51336$, $0.50720$, $0.50330$, $0.50190$, $0.50078$]\n  - $\\sigma$ 值：[$0.0010$, $0.0009$, $0.0008$, $0.0008$, $0.0008$]\n- 测试用例 C (主指数 $p = 1$):\n  - $a$ 值：[$0.16$, $0.10$, $0.06$, $0.04$]\n  - $O(a)$ 值：[$1.1105$, $1.1495$, $1.1702$, $1.1803$]\n  - $\\sigma$ 值：[$0.0040$, $0.0030$, $0.0030$, $0.0030$]\n\n对程序的要求：\n- 对每个测试用例，计算两个留一法分数，决定是否排除最粗的点，然后对选定的数据集进行拟合，并提取 $\\widehat{\\beta}_0$ 及其统计不确定性。\n- 您的程序必须生成单行输出，其中包含一个由三个列表组成的逗号分隔列表（每个测试用例一个），每个内部列表按 [$\\text{exclude}$, $\\widehat{\\beta}_0$, $\\mathrm{uncertainty}(\\widehat{\\beta}_0)$, $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{full}})$, $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{excl}})$] 的顺序排列。\n- 因此，最终打印的行必须具有以下形式：\n  [[exclude_A, beta0_A, s_A, LOO_full_A, LOO_excl_A],[exclude_B, beta0_B, s_B, LOO_full_B, LOO_excl_B],[exclude_C, beta0_C, s_C, LOO_full_C, LOO_excl_C]]。\n- 每个布尔值或数值条目都是无量纲的。不使用角度。\n\n设计覆盖范围：\n- 测试用例A是一种典型情况，其中最粗的点可能违反渐进标度，应被排除。\n- 测试用例B探讨了一种临界情况，其中最粗的点只有很小的偏差，可能会被保留。\n- 测试用例C将主指数更改为 $p = 1$，并用较少的点和不同的斜率来测试该方法。\n\n您的解决方案必须是一个完整的、可运行的程序，该程序执行这些计算并严格按照上述指定格式打印一行输出，不得包含任何额外文本。",
            "solution": "该问题要求为连续极限外推拟合实现一个模型选择程序，这是格点场论中的一种标准做法。其基础物理模型源于Symanzik有效理论，该理论假定对于小格点间距 $a$，无量纲可观测量 $O(a)$ 可以按 $a$ 的幂次进行展开。问题指定了一个截断模型，\n$$\nO(a) = \\beta_0 + \\beta_1 a^p + \\varepsilon\n$$\n其中 $\\beta_0$ 是所求的连续极限 ($a \\to 0$)，$\\beta_1$ 是主导离散误差的系数，$p$ 是已知的整数指数，$\\varepsilon$ 代表独立的、具有已知标准差 $\\sigma_i$ 的高斯测量误差，对应于每个数据点 $(a_i, O_i)$。\n\n主要任务是决定是否包含具有最粗格点间距 $a_{\\max}$ 的数据点，该点可能不在截断模型有效的渐进区域内。这个决定基于留一法交叉验证（LOOCV），这是一种评估模型预测准确性的技术。\n\n分析的关键是加权线性回归。我们将自变量定义为 $x_i = a_i^p$，因变量定义为 $y_i = O_i$。模型变成一个简单的线性关系 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$。由于误差 $\\varepsilon_i$ 是独立的，并服从方差为 $\\sigma_i^2$ 的高斯分布，因此参数 $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^\\top$ 的最大似然估计量就是加权最小二乘（WLS）估计量。\n\nWLS估计量 $\\widehat{\\boldsymbol{\\beta}}$ 最小化加权残差平方和，即卡方 $\\chi^2$：\n$$\n\\chi^2(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} w_i (y_i - (\\beta_0 + \\beta_1 x_i))^2\n$$\n其中权重为 $w_i = 1/\\sigma_i^2$。用矩阵形式表示，即 $\\chi^2(\\boldsymbol{\\beta}) = (\\mathbf{y} - X\\boldsymbol{\\beta})^\\top W (\\mathbf{y} - X\\boldsymbol{\\beta})$，其中 $\\mathbf{y}$ 是观测值向量，$X$ 是设计矩阵，$W$ 是权重对角矩阵。对于这个问题，包含 $N$ 个数据点的设计矩阵是：\n$$\nX = \\begin{pmatrix} 1  x_1 \\\\ 1  x_2 \\\\ \\vdots  \\vdots \\\\ 1  x_N \\end{pmatrix}\n$$\n最小化 $\\chi^2$ 的解由正规方程给出，从而得到估计量：\n$$\n\\widehat{\\boldsymbol{\\beta}} = (X^\\top W X)^{-1} X^\\top \\mathbf{y}\n$$\n$\\widehat{\\boldsymbol{\\beta}}$ 的第一个分量即 $\\widehat{\\beta}_0$，是我们对连续极限值的估计。估计参数的统计不确定性由协方差矩阵 $C$ 给出。对于WLS拟合，该矩阵为：\n$$\nC = (X^\\top W X)^{-1}\n$$\n连续极限估计 $\\widehat{\\beta}_0$ 的不确定性是协方差矩阵相应对角元素的平方根：$\\Delta\\widehat{\\beta}_0 = \\sqrt{C_{00}}$。\n\n为了决定是否排除最粗的点，我们使用LOOCV。问题将总预测损失定义为：\n$$\n\\mathcal{L}_{\\mathrm{LOO}}(D) = \\sum_{j=1}^{N} \\left( \\frac{y_j - \\widehat{y}_j^{(-j)}}{\\sigma_j} \\right)^2\n$$\n此处，$\\widehat{y}_j^{(-j)}$ 是在移除了第 $j$ 个点的数据集 $D$ 上进行WLS拟合得到的在 $x_j$ 处的预测值。通过执行 $N$ 次单独的拟合来计算这个值在计算上是低效的。一个更直接的方法是使用“帽子”矩阵 $H$。在完整数据集上拟合得到的预测值向量是 $\\widehat{\\mathbf{y}} = X\\widehat{\\boldsymbol{\\beta}} = X(X^\\top W X)^{-1} X^\\top \\mathbf{y} = H \\mathbf{y}$。因此，帽子矩阵是 $H = X(X^\\top W X)^{-1} X^\\top W$。\n\n回归诊断中的一个标准结果表明，留一法残差可以从完整数据集拟合的残差 $r_j = y_j - \\widehat{y}_j$ 和第 $j$ 个点的杠杆值 $h_{jj} = H_{jj}$ 计算得出：\n$$\ny_j - \\widehat{y}_j^{(-j)} = \\frac{y_j - \\widehat{y}_j}{1 - h_{jj}} = \\frac{r_j}{1 - h_{jj}}\n$$\n这使得我们可以通过对给定数据集进行单次WLS拟合来高效地计算LOOCV损失：\n$$\n\\mathcal{L}_{\\mathrm{LOO}}(D) = \\sum_{j=1}^{N} \\left( \\frac{r_j}{\\sigma_j (1 - h_{jj})} \\right)^2\n$$\n每个测试用例的步骤如下：\n1. 定义两个数据集：$D_{\\mathrm{full}}$，包含所有点；以及 $D_{\\mathrm{excl}}$，移除了对应于 $a_{\\max}$ 的点。\n2. 对每个数据集，进行WLS拟合以找到参数 $\\widehat{\\boldsymbol{\\beta}}$，并使用上述公式计算LOOCV损失 $\\mathcal{L}_{\\mathrm{LOO}}$。这将得到 $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{full}})$ 和 $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{excl}})$。\n3. 比较损失。如果 $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{excl}})  \\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{full}})$，则排除最粗的点。否则，保留该点。\n4. 最终的连续极限估计值 $\\widehat{\\beta}_0$ 及其不确定性 $\\sqrt{C_{00}}$ 从对应于所选数据集（$D_{\\mathrm{full}}$ 或 $D_{\\mathrm{excl}}$）的拟合中获取。\n5. 对每个用例，报告所有五个量——布尔决策、最终的 $\\widehat{\\beta}_0$、其不确定性以及计算出的两个LOOCV损失。\n\n这个过程构成了一个在此背景下完整、自洽且客观的模型选择方法，它严格遵守最大似然估计和交叉验证的原则，没有引入外部信息或主观偏见。",
            "answer": "```python\n# language: Python\n# version: 3.12\n# libraries:\n#   - name: numpy\n#     version: 1.23.5\n#   - name: scipy\n#     version: 1.11.4\nimport numpy as np\n\ndef calculate_fit_and_loo(a, O, sigma, p):\n    \"\"\"\n    Performs a weighted linear fit and calculates the leave-one-out cross-validation loss.\n\n    Args:\n        a (np.ndarray): Array of lattice spacings.\n        O (np.ndarray): Array of observable values.\n        sigma (np.ndarray): Array of standard deviations for O.\n        p (int): The leading discretization exponent.\n\n    Returns:\n        tuple: A tuple containing:\n            - beta0_hat (float): The continuum extrapolated value (intercept).\n            - unc_beta0 (float): The uncertainty on beta0_hat.\n            - loo_loss (float): The leave-one-out cross-validation loss.\n    \"\"\"\n    x = a**p\n    y = O\n    weights = 1 / sigma**2\n    \n    # Design matrix X\n    X = np.vstack([np.ones_like(x), x]).T\n    \n    # Weight matrix W\n    W = np.diag(weights)\n    \n    # Perform WLS fit\n    # Covariance matrix C = (X^T * W * X)^-1\n    try:\n        XTWX = X.T @ W @ X\n        C = np.linalg.inv(XTWX)\n    except np.linalg.LinAlgError:\n        # This case occurs if the matrix is singular, e.g., if there are fewer\n        # points than parameters, which is 2. The problem ensures this doesn't happen\n        # for D_excl if N_full >= 4.\n        return np.nan, np.nan, np.inf\n\n    # Parameter estimates beta_hat = C * X^T * W * y\n    XTWy = X.T @ W @ y\n    beta_hat = C @ XTWy\n    \n    beta0_hat = beta_hat[0]\n    unc_beta0 = np.sqrt(C[0, 0])\n    \n    # Calculate LOOCV loss using the leverage/hat matrix method\n    # Hat matrix H = X * C * X^T * W\n    H = X @ C @ X.T @ W\n    leverages = np.diag(H)\n    \n    # Predictions from the full fit: y_hat = X * beta_hat\n    y_hat = X @ beta_hat\n    \n    # Standard residuals: r = y - y_hat\n    residuals = y - y_hat\n    \n    # LOOCV residuals, scaled by sigma\n    # The term (y_j - yhat_j-1) / sigma_j\n    loo_scaled_residuals = (residuals / sigma) / (1 - leverages)\n    \n    # LOOCV loss: sum of squares of the scaled LOOCV residuals\n    loo_loss = np.sum(loo_scaled_residuals**2)\n    \n    return beta0_hat, unc_beta0, loo_loss\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Test case A\n        {'p': 2, 'a': [0.12, 0.09, 0.06, 0.045, 0.03], 'O': [0.5230, 0.50995, 0.50410, 0.50230, 0.50120], 'sigma': [0.0010, 0.0008, 0.0007, 0.0007, 0.0007]},\n        # Test case B\n        {'p': 2, 'a': [0.12, 0.09, 0.06, 0.045, 0.03], 'O': [0.51336, 0.50720, 0.50330, 0.50190, 0.50078], 'sigma': [0.0010, 0.0009, 0.0008, 0.0008, 0.0008]},\n        # Test case C\n        {'p': 1, 'a': [0.16, 0.10, 0.06, 0.04], 'O': [1.1105, 1.1495, 1.1702, 1.1803], 'sigma': [0.0040, 0.0030, 0.0030, 0.0030]}\n    ]\n\n    results_all_cases = []\n    \n    for case in test_cases:\n        p = case['p']\n        a_full = np.array(case['a'])\n        O_full = np.array(case['O'])\n        sigma_full = np.array(case['sigma'])\n        \n        # Identify the index of the coarsest point (largest 'a')\n        idx_coarsest = np.argmax(a_full)\n        \n        # --- Analysis on the full dataset D_full ---\n        beta0_full, unc_full, loo_full = calculate_fit_and_loo(a_full, O_full, sigma_full, p)\n        \n        # --- Analysis on the dataset D_excl with the coarsest point removed ---\n        a_excl = np.delete(a_full, idx_coarsest)\n        O_excl = np.delete(O_full, idx_coarsest)\n        sigma_excl = np.delete(sigma_full, idx_coarsest)\n        \n        beta0_excl, unc_excl, loo_excl = calculate_fit_and_loo(a_excl, O_excl, sigma_excl, p)\n        \n        # --- Decision Rule ---\n        # Exclude if the loss is strictly smaller\n        exclude = loo_excl  loo_full\n        \n        if exclude:\n            final_beta0 = beta0_excl\n            final_unc = unc_excl\n        else:\n            final_beta0 = beta0_full\n            final_unc = unc_full\n            \n        case_result = [exclude, final_beta0, final_unc, loo_full, loo_excl]\n        results_all_cases.append(case_result)\n        \n    # Format the final output string as per the problem specification\n    # e.g., [[True,0.5,0.001,10.0,5.0],[...]]\n    inner_lists_str = []\n    for res_list in results_all_cases:\n        # str(bool) gives 'True'/'False', str(float) gives standard float representation.\n        str_elements = [str(x) for x in res_list]\n        inner_lists_str.append(f\"[{','.join(str_elements)}]\")\n    \n    final_output_string = f\"[{','.join(inner_lists_str)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "在实际分析中，我们常常面临多种理论上都看似合理的连续极限外推函数形式（ansätze）的选择。与其试图挑选一个所谓的“最佳”模型，一种更稳健的方法是同时考虑所有 plausible 的模型，并对它们的结果进行加权平均。这项实践  引入了贝叶斯模型平均（BMA）方法，这是一种先进的统计技术，它根据数据对每个模型的支持程度（即模型证据）来赋予其权重。通过这种方式，我们可以得到一个综合了多种模型可能性的连续极限估计值及其可信区间，从而将模型选择所带来的系统不确定性自然地包含在最终的误差预算中。",
            "id": "3509855",
            "problem": "实现一个完整的程序，该程序使用三种相互竞争的拟设（ansätze），对格点数据进行连续谱外推的贝叶斯模型平均（BMA），并返回模型平均的连续谱值及其等尾可信区间。此问题中的所有量均为无量纲。这三个模型由格点间距 $a$ 的基函数定义如下：\n- $M_1$：常数加线性项，其 $f(a) = f_0 + c_1\\,a$，\n- $M_2$：常数加二次项，其 $f(a) = f_0 + c_2\\,a^2$，\n- $M_3$：常数加二次项乘以对数项，其 $f(a) = f_0 + c_3\\,a^2 \\log a$，其中对数为自然对数，且 $a \\in (0,1)$，因此 $\\log a$ 有良好定义且为负值。\n\n您必须在一个完全贝叶斯线性模型中处理该问题，该模型具有高斯观测噪声和对系数的独立高斯先验。具体而言：\n- 似然：给定数据 $(a_i, y_i, \\sigma_i)$（$i=1,\\dots,n$），模型 $M$ 通过系数的线性设计矩阵来预测 $y_i$，数据似然是具有已知标准差 $\\sigma_i$ 的高斯分布的乘积。\n- 每个模型 $M_j$ 的参数先验：系数向量 $\\theta = (f_0, c_j)$ 被赋予一个零均值高斯先验，其协方差为对角矩阵 $\\mathrm{diag}(\\tau_0^2, \\tau_c^2)$，其中 $\\tau_0 = 10$ 且 $\\tau_c = 1$。\n- 模型先验：模型 $M_1$、$M_2$ 和 $M_3$ 具有相等的先验概率。\n\n您的程序必须：\n1. 对每个模型 $M_j$，通过在高斯先验和线性高斯似然下对参数进行积分来计算模型证据。\n2. 对每个模型 $M_j$，计算在该模型下给定数据的连续谱参数 $f_0$（即常数基函数的系数）的后验分布。\n3. 计算每个模型的后验概率。\n4. 将 $f_0$ 的模型平均后验构造为三个特定模型高斯后验的离散混合，其权重等于后验模型概率。\n5. 从该混合分布中计算 $f_0$ 的模型平均后验均值及其可信度水平为 $0.68$ 的中心等尾可信区间（即下分位点在 $0.16$，上分位点在 $0.84$）。\n\n您必须使用的基本原理：\n- 用于模型选择和参数推断的贝叶斯定理，\n- 具有高斯先验的线性高斯模型的多元正态分布性质，\n- 中心等尾可信区间的定义。\n\n约束和要求：\n- 将所有输入视为无量纲数。此问题不涉及角度；无需单位转换。\n- 实现数值稳定的线性代数。您可以假设观测误差协方差是对角矩阵，其对角线元素为 $\\sigma_i^2$。\n- 使用自然对数计算 $\\log a$。\n- 可信区间水平应解释为概率（例如 $0.68$），而不是百分比。报告区间的端点，而不是半宽度。\n\n测试套件：\n您的程序必须在以下五个数据集上运行。每个数据集提供格点间距 $a$、观测值 $y$ 和观测标准差 $\\sigma$ 的数组。请完全按照规定的顺序使用这些值。\n\n- 案例 1（典型的二次截断效应）：\n  - $a = [\\,0.12,\\, 0.09,\\, 0.06,\\, 0.045\\,]$\n  - $y = [\\,0.52028,\\, 0.50772,\\, 0.50582,\\, 0.50143\\,]$\n  - $\\sigma = [\\,0.005,\\, 0.004,\\, 0.003,\\, 0.003\\,]$\n\n- 案例 2（主导的线性截断效应）：\n  - $a = [\\,0.16,\\, 0.12,\\, 0.08,\\, 0.04\\,]$\n  - $y = [\\,0.877,\\, 0.902,\\, 0.939,\\, 0.964\\,]$\n  - $\\sigma = [\\,0.01,\\, 0.01,\\, 0.01,\\, 0.01\\,]$\n\n- 案例 3（对数增强的二次修正）：\n  - $a = [\\,0.10,\\, 0.08,\\, 0.06,\\, 0.05,\\, 0.04\\,]$\n  - $y = [\\,0.189987074535,\\, 0.1909176683392,\\, 0.1954358607098302,\\, 0.1957553346575,\\, 0.19842489934\\,]$\n  - $\\sigma = [\\,0.004,\\, 0.004,\\, 0.004,\\, 0.004,\\, 0.004\\,]$\n\n- 案例 4（稀疏数据）：\n  - $a = [\\,0.10,\\, 0.05\\,]$\n  - $y = [\\,0.311,\\, 0.29875\\,]$\n  - $\\sigma = [\\,0.02,\\, 0.02\\,]$\n\n- 案例 5（高噪声情况）：\n  - $a = [\\,0.15,\\, 0.12,\\, 0.09,\\, 0.06,\\, 0.03\\,]$\n  - $y = [\\,0.845,\\, 0.78,\\, 0.805,\\, 0.765,\\, 0.77\\,]$\n  - $\\sigma = [\\,0.05,\\, 0.05,\\, 0.05,\\, 0.05,\\, 0.05\\,]$\n\n输出规格：\n- 对每个案例，计算 $f_0$ 的模型平均后验均值以及可信度水平为 $0.68$ 的中心等尾可信区间的端点。\n- 将报告的每个数字四舍五入到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个案例对应一个 $[\\,\\text{mean},\\, \\text{lower},\\, \\text{upper}\\,]$ 形式的子列表。例如，整体格式必须类似于 $[[m_1,\\ell_1,u_1],[m_2,\\ell_2,u_2],\\dots,[m_5,\\ell_5,u_5]]$，并按上述案例顺序包含恰好五个三元组。",
            "solution": "用户要求实现一个用于格点数据连续谱外推的贝叶斯模型平均（BMA）程序。该问题定义明确，具有科学依据，并包含了解决该问题所需的所有必要信息。它直接属于计算物理和贝叶斯统计的领域。\n\n我们将首先推导贝叶斯线性模型所需的数学公式，然后实现这些公式来解决所提供的测试案例。\n\n### 贝叶斯线性回归形式体系\n\n设给定模型 $M_j$ 由线性关系 $\\mathbf{y} = X_j \\boldsymbol{\\theta}_j + \\boldsymbol{\\epsilon}$ 描述，其中：\n- $\\mathbf{y}$ 是观测值的 $n \\times 1$ 向量。\n- $X_j$ 是 $n \\times k$ 的设计矩阵，所有模型的参数个数均为 $k=2$。\n- $\\boldsymbol{\\theta}_j$ 是模型参数的 $k \\times 1$ 向量，即 $(f_0, c_j)^T$。\n- $\\boldsymbol{\\epsilon}$ 是观测误差向量，假设其服从均值为 $\\mathbf{0}$、已知对角协方差矩阵为 $\\Sigma = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$ 的高斯分布。\n\n给定参数下数据的似然为：\n$$p(\\mathbf{y} | \\boldsymbol{\\theta}_j, M_j) = \\mathcal{N}(\\mathbf{y} | X_j \\boldsymbol{\\theta}_j, \\Sigma) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - X_j \\boldsymbol{\\theta}_j)^\\top \\Sigma^{-1} (\\mathbf{y} - X_j \\boldsymbol{\\theta}_j)\\right)$$\n\n问题指定参数 $\\boldsymbol{\\theta}_j$ 的高斯先验为：\n$$p(\\boldsymbol{\\theta}_j | M_j) = \\mathcal{N}(\\boldsymbol{\\theta}_j | \\boldsymbol{\\mu}_0, \\Sigma_0)$$\n其中先验均值为 $\\boldsymbol{\\mu}_0 = \\mathbf{0}$，先验协方差为 $\\Sigma_0 = \\mathrm{diag}(\\tau_0^2, \\tau_c^2)$，$\\tau_0 = 10$ 且 $\\tau_c = 1$。\n\n#### 1. 参数后验分布\n由于高斯先验与线性高斯似然的共轭性，参数的后验分布也是高斯分布：\n$$p(\\boldsymbol{\\theta}_j | \\mathbf{y}, M_j) = \\mathcal{N}(\\boldsymbol{\\theta}_j | \\boldsymbol{\\mu}_{n,j}, \\Sigma_{n,j})$$\n后验协方差 $\\Sigma_{n,j}$ 和均值 $\\boldsymbol{\\mu}_{n,j}$ 由下式给出：\n$$\n\\Sigma_{n,j} = (\\Sigma_0^{-1} + X_j^\\top \\Sigma^{-1} X_j)^{-1} \\\\\n\\boldsymbol{\\mu}_{n,j} = \\Sigma_{n,j} (X_j^\\top \\Sigma^{-1} \\mathbf{y})\n$$\n由于 $\\boldsymbol{\\mu}_0=\\mathbf{0}$，涉及先验均值的项消失。令 $W = \\Sigma^{-1}$。方程变为：\n$$\nA_j = \\Sigma_0^{-1} + X_j^\\top W X_j \\\\\n\\Sigma_{n,j} = A_j^{-1} \\\\\n\\boldsymbol{\\mu}_{n,j} = \\Sigma_{n,j} (X_j^\\top W \\mathbf{y})\n$$\n我们感兴趣的参数是 $f_0$，它是 $\\boldsymbol{\\theta}_j$ 的第一个分量。其在模型 $M_j$ 下的边际后验分布是一个单变量高斯分布：\n$$p(f_0 | \\mathbf{y}, M_j) = \\mathcal{N}(f_0 | \\mu_{f_0, j}, \\sigma_{f_0, j}^2)$$\n其中 $\\mu_{f_0, j} = (\\boldsymbol{\\mu}_{n,j})_1$ 且 $\\sigma_{f_0, j}^2 = (\\Sigma_{n,j})_{11}$。\n\n#### 2. 模型证据（边际似然）\n模型 $M_j$ 的证据是在所有可能的参数值上积分得到的数据概率：\n$$p(\\mathbf{y} | M_j) = \\int p(\\mathbf{y} | \\boldsymbol{\\theta}_j, M_j) p(\\boldsymbol{\\theta}_j | M_j) d\\boldsymbol{\\theta}_j$$\n对于线性高斯情况，该积分可以解析求解。结果是：\n$$p(\\mathbf{y} | M_j) = \\mathcal{N}(\\mathbf{y} | X_j\\boldsymbol{\\mu}_0, \\Sigma + X_j\\Sigma_0 X_j^\\top)$$\n为了稳定地计算，我们使用证据的对数。一个方便且稳定的表达式是：\n$$\n\\log p(\\mathbf{y}|M_j) = -\\frac{n}{2} \\log (2\\pi) -\\frac{1}{2}\\left( \\log\\det\\Sigma + \\log\\det\\Sigma_0 + \\log\\det A_j \\right) -\\frac{1}{2}\\left( \\mathbf{y}^\\top W \\mathbf{y} - \\boldsymbol{\\mu}_{n,j}^\\top A_j \\boldsymbol{\\mu}_{n,j} \\right)\n$$\n其中 $\\boldsymbol{\\mu}_{n,j}^\\top A_j \\boldsymbol{\\mu}_{n,j} = (X_j^\\top W \\mathbf{y})^\\top \\Sigma_{n,j} (X_j^\\top W \\mathbf{y})$。\n\n#### 3. 后验模型概率\n使用模型的贝叶斯定理，模型 $M_j$ 的后验概率为：\n$$P(M_j | \\mathbf{y}) = \\frac{p(\\mathbf{y} | M_j) P(M_j)}{\\sum_{k=1}^3 p(\\mathbf{y} | M_k) P(M_k)}$$\n由于先验概率 $P(M_j)$ 相等（为 $1/3$），它们会相互抵消，我们得到：\n$$P(M_j | \\mathbf{y}) = \\frac{p(\\mathbf{y} | M_j)}{\\sum_{k=1}^3 p(\\mathbf{y} | M_k)}$$\n为处理潜在的数值下溢或上溢，我们使用 log-sum-exp 技巧从对数证据 $\\ell_j = \\log p(\\mathbf{y}|M_j)$ 计算这些概率：\n令 $\\ell_{\\max} = \\max(\\ell_1, \\ell_2, \\ell_3)$。那么后验权重 $w_j = P(M_j|\\mathbf{y})$ 为：\n$$w_j = \\frac{\\exp(\\ell_j - \\ell_{\\max})}{\\sum_{k=1}^3 \\exp(\\ell_k - \\ell_{\\max})}$$\n\n#### 4. BMA 后验、均值和可信区间\n$f_0$ 的 BMA 后验分布是各个模型后验的加权混合：\n$$p(f_0 | \\mathbf{y}) = \\sum_{j=1}^3 w_j \\, p(f_0 | \\mathbf{y}, M_j) = \\sum_{j=1}^3 w_j \\, \\mathcal{N}(f_0 | \\mu_{f_0, j}, \\sigma_{f_0, j}^2)$$\n这个混合分布的均值是 BMA 后验均值：\n$$E[f_0 | \\mathbf{y}] = \\sum_{j=1}^3 w_j \\mu_{f_0, j}$$\n中心等尾可信区间由混合分布的分位数定义。对于一个 $68\\%$ 的可信区间，我们需要 $16\\%$ 和 $84\\%$ 的分位数。混合分布的累积分布函数（CDF）是：\n$$F(x) = P(f_0 \\le x | \\mathbf{y}) = \\sum_{j=1}^3 w_j \\, \\Phi\\left(\\frac{x - \\mu_{f_0, j}}{\\sigma_{f_0, j}}\\right)$$\n其中 $\\Phi$ 是标准正态 CDF。我们必须找到值 $x_{0.16}$ 和 $x_{0.84}$ 使得 $F(x_{0.16}) = 0.16$ 且 $F(x_{0.84}) = 0.84$。这些方程使用寻根算法进行数值求解。\n\n### 算法实现\n\n对每个测试案例，算法按以下步骤进行：\n1. 初始化先验参数：$\\tau_0=10, \\tau_c=1$。\n2. 对三个模型（$M_1, M_2, M_3$）中的每一个：\n    a. 使用模型的基函数（$a_i$，$a_i^2$ 和 $a_i^2 \\log a_i$）从格点间距 $a_i$ 构建设计矩阵 $X_j$。\n    b. 计算后验均值 $\\boldsymbol{\\mu}_{n,j}$ 和协方差 $\\Sigma_{n,j}$。\n    c. 提取参数 $f_0$ 的均值 $\\mu_{f_0, j}$ 和标准差 $\\sigma_{f_0, j}$。\n    d. 计算对数模型证据 $\\ell_j = \\log p(\\mathbf{y}|M_j)$。\n3. 从对数证据计算后验模型概率 $w_j$。\n4. 计算 $f_0$ 的 BMA 后验均值。\n5. 定义混合累积分布函数 $F(x)$。\n6. 使用数值寻根器（例如 `scipy.optimize.brentq`），通过求解 $F(x) - q = 0$（其中 $q=0.16$ 和 $q=0.84$）来找到 $68\\%$ 可信区间的下界（$x_{0.16}$）和上界（$x_{0.84}$）。\n7. 收集并格式化 BMA 均值、下界和上界，四舍五入到六位小数。\n\n将此过程系统地应用于所有五个测试案例，以生成最终结果。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import brentq\n\ndef bma_extrapolation(a_data, y_data, sigma_data):\n    \"\"\"\n    Performs Bayesian Model Averaging for continuum extrapolation.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    a = np.array(a_data)\n    y = np.array(y_data)\n    sigma = np.array(sigma_data)\n    \n    # Priors\n    tau_0 = 10.0\n    tau_c = 1.0\n    prior_cov_inv = np.diag([1.0 / tau_0**2, 1.0 / tau_c**2])\n    log_det_prior_cov = 2.0 * (np.log(tau_0) + np.log(tau_c))\n\n    # Data-related quantities\n    n = len(y)\n    W = np.diag(1.0 / sigma**2)\n    log_det_obs_cov = np.sum(2.0 * np.log(sigma))\n    yT_W_y = y @ W @ y\n    \n    # Basis functions for the three models\n    basis_functions = [\n        lambda x: x,\n        lambda x: x**2,\n        lambda x: x**2 * np.log(x)\n    ]\n    \n    model_posteriors = []\n    log_evidences = []\n\n    for basis_func in basis_functions:\n        # Build design matrix X\n        phi = basis_func(a)\n        X = np.vstack([np.ones_like(phi), phi]).T\n        \n        # Posterior calculation\n        A = prior_cov_inv + X.T @ W @ X\n        A_inv = np.linalg.inv(A)\n        b = X.T @ W @ y\n        \n        post_mean = A_inv @ b\n        post_cov = A_inv\n        \n        f0_mean = post_mean[0]\n        f0_var = post_cov[0, 0]\n        \n        model_posteriors.append({'mean': f0_mean, 'std': np.sqrt(f0_var)})\n        \n        # Log evidence calculation\n        sign, log_det_A = np.linalg.slogdet(A)\n        energy_term = yT_W_y - b @ post_mean\n        \n        log_evidence = -0.5 * (n * np.log(2 * np.pi) + log_det_prior_cov +\n                               log_det_obs_cov + log_det_A + energy_term)\n        log_evidences.append(log_evidence)\n    \n    # Model probabilities (weights)\n    log_evidences = np.array(log_evidences)\n    max_log_evidence = np.max(log_evidences)\n    evidences = np.exp(log_evidences - max_log_evidence)\n    post_probs = evidences / np.sum(evidences)\n    \n    # BMA mean\n    bma_mean = np.sum([p['mean'] * w for p, w in zip(model_posteriors, post_probs)])\n    \n    # BMA credible interval\n    def mixture_cdf(x):\n        cdf_val = 0.0\n        for i, p in enumerate(model_posteriors):\n            cdf_val += post_probs[i] * norm.cdf(x, loc=p['mean'], scale=p['std'])\n        return cdf_val\n\n    # Find a reasonable search interval for the root finder\n    all_means = [p['mean'] for p in model_posteriors]\n    all_stds = [p['std'] for p in model_posteriors]\n    search_min = min(all_means) - 10 * max(all_stds) if all_stds else min(all_means) - 1\n    search_max = max(all_means) + 10 * max(all_stds) if all_stds else max(all_means) + 1\n\n\n    # Functions to find roots for quantiles\n    lower_quantile_func = lambda x: mixture_cdf(x) - 0.16\n    upper_quantile_func = lambda x: mixture_cdf(x) - 0.84\n    \n    # Find lower and upper bounds of the 68% CI\n    ci_lower = brentq(lower_quantile_func, search_min, search_max)\n    ci_upper = brentq(upper_quantile_func, search_min, search_max)\n    \n    return [bma_mean, ci_lower, ci_upper]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"a\": [0.12, 0.09, 0.06, 0.045],\n            \"y\": [0.52028, 0.50772, 0.50582, 0.50143],\n            \"sigma\": [0.005, 0.004, 0.003, 0.003]\n        },\n        # Case 2\n        {\n            \"a\": [0.16, 0.12, 0.08, 0.04],\n            \"y\": [0.877, 0.902, 0.939, 0.964],\n            \"sigma\": [0.01, 0.01, 0.01, 0.01]\n        },\n        # Case 3\n        {\n            \"a\": [0.10, 0.08, 0.06, 0.05, 0.04],\n            \"y\": [0.189987074535, 0.1909176683392, 0.1954358607098302, 0.1957553346575, 0.19842489934],\n            \"sigma\": [0.004, 0.004, 0.004, 0.004, 0.004]\n        },\n        # Case 4\n        {\n            \"a\": [0.10, 0.05],\n            \"y\": [0.311, 0.29875],\n            \"sigma\": [0.02, 0.02]\n        },\n        # Case 5\n        {\n            \"a\": [0.15, 0.12, 0.09, 0.06, 0.03],\n            \"y\": [0.845, 0.78, 0.805, 0.765, 0.77],\n            \"sigma\": [0.05, 0.05, 0.05, 0.05, 0.05]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = bma_extrapolation(case[\"a\"], case[\"y\"], case[\"sigma\"])\n        results.append(result)\n\n    # Format the final output string exactly as specified.\n    result_strings = []\n    for res in results:\n        mean, lower, upper = res\n        result_strings.append(f\"[{mean:.6f},{lower:.6f},{upper:.6f}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}