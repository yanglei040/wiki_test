{
    "hands_on_practices": [
        {
            "introduction": "The heart of the Hybrid Monte Carlo (HMC) algorithm is the molecular dynamics evolution, which explores the configuration space by numerically integrating Hamilton's equations of motion. This practice focuses on implementing the most common choice of integrator, the second-order symmetric leapfrog scheme, for a simple but non-trivial SU(2) gauge theory. By deriving the force from first principles and building the integrator step-by-step, you will gain a fundamental understanding of how Hamiltonian dynamics are simulated on a Lie group manifold, a core skill for lattice QCD .",
            "id": "3516854",
            "problem": "Implement, analyze, and test a single-step symmetric leapfrog update for one gauge link in a Hybrid Monte Carlo (HMC) integrator for Lattice Quantum Chromodynamics (QCD), specialized to the special unitary group of degree two ($\\mathrm{SU}(2)$). You must derive the force from first principles and quantify the leading local error of the integrator in terms of the Hamiltonian drift after a single step.\n\nStart from the following fundamental base:\n- The gauge link $U$ is an element of $\\mathrm{SU}(2)$, i.e., a $2\\times 2$ complex unitary matrix with determinant $1$.\n- The conjugate momentum $P$ is an element of the Lie algebra $\\mathfrak{su}(2)$, i.e., a $2\\times 2$ traceless anti-Hermitian matrix.\n- The Hamiltonian is separable: $H(U,P) = T(P) + S(U)$ with kinetic term $T(P) = -\\tfrac{1}{2}\\operatorname{Re}\\operatorname{Tr}(P^{2})$ (induced by the right-invariant metric $\\langle A,B\\rangle = -\\operatorname{Re}\\operatorname{Tr}(A B)$ on $\\mathfrak{su}(2)$) and potential term $S(U) = -\\alpha\\, \\operatorname{Re}\\operatorname{Tr}(U M^{\\dagger})$, where $\\alpha > 0$ is a real coupling and $M\\in \\mathrm{SU}(2)$ is a fixed staple-like matrix.\n- Variations of $U$ are of the form $\\delta U = A U$ with $A\\in \\mathfrak{su}(2)$, and the force $F(U)\\in \\mathfrak{su}(2)$ is defined by $\\delta S = \\langle F(U), A\\rangle$ for all such $A$.\n- Symplectic dynamics on the group manifold follow Hamilton’s equations: $\\dot U = P U$ and $\\dot P = -F(U)$.\n- The symmetric leapfrog (also known as velocity Verlet) update with step size $\\epsilon$ is the composition of a half momentum kick, a full link drift, and a half momentum kick.\n\nTasks:\n1) Derive from the above base an explicit expression for the force $F(U)$ in terms of $U$ and $M$, making clear use of the inner product $\\langle \\cdot,\\cdot\\rangle$ and the projection onto $\\mathfrak{su}(2)$. You must use only the properties that $\\delta U = A U$ with $A\\in \\mathfrak{su}(2)$ and that $\\mathrm{SU}(2)$ is a subgroup of the unitary group with the given inner product. Your final expression must be a projection of a matrix built only from $U$ and $M$ onto $\\mathfrak{su}(2)$.\n2) Using the derived force, implement a single symmetric leapfrog step:\n- Half kick: $P \\leftarrow P - \\tfrac{\\epsilon}{2} F(U)$.\n- Drift: $U \\leftarrow \\exp(\\epsilon P)\\, U$, where the exponential is the matrix exponential mapping $\\mathfrak{su}(2)$ to $\\mathrm{SU}(2)$.\n- Half kick: $P \\leftarrow P - \\tfrac{\\epsilon}{2} F(U)$.\nEnsure $U\\in \\mathrm{SU}(2)$ and $P\\in \\mathfrak{su}(2)$ are preserved up to numerical roundoff. You may use the closed-form exponential in $\\mathrm{SU}(2)$ or a general matrix exponential.\n3) For each test case below, compute the Hamiltonian value $H_{\\text{before}} = H(U,P)$ at the initial state and $H_{\\text{after}} = H(U',P')$ after one leapfrog step. Define the local energy error $\\Delta H = H_{\\text{after}} - H_{\\text{before}}$. Estimate the leading $\\mathcal{O}(\\epsilon^{3})$ error coefficient by the scaled quantity $C \\approx \\Delta H/\\epsilon^{3}$.\n4) Angles must be interpreted and computed in radians.\n5) Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, i.e., $[C_{1},C_{2},C_{3}]$, where $C_{k}$ is the float for test case $k$.\n\nTest suite specification. Use the Pauli matrices $\\sigma_{1},\\sigma_{2},\\sigma_{3}$ and the parametrizations:\n- For any real axis vector $\\mathbf{n}$ with $\\lVert \\mathbf{n}\\rVert=1$ and angle $\\theta$, define $U(\\theta,\\mathbf{n}) = \\exp\\!\\big(i\\,\\theta\\, \\mathbf{n}\\cdot \\boldsymbol{\\sigma}\\big) = \\cos(\\theta)\\,\\mathbf{1}_{2} + i \\sin(\\theta)\\,\\mathbf{n}\\cdot \\boldsymbol{\\sigma}$, which lies in $\\mathrm{SU}(2)$.\n- For any real vector $\\mathbf{w}$, define $P(\\mathbf{w}) = i\\,\\mathbf{w}\\cdot \\boldsymbol{\\sigma}\\in \\mathfrak{su}(2)$.\n\nImplement and evaluate the following three cases:\n- Case $1$ (general case):\n  - $\\alpha = 1.1$, $\\epsilon = 0.05$.\n  - $U = U(\\theta_{U}, \\mathbf{n}_{U})$ with $\\theta_{U} = 0.8$, $\\mathbf{n}_{U} = \\frac{(1,2,3)}{\\lVert(1,2,3)\\rVert}$.\n  - $M = U(\\theta_{M}, \\mathbf{n}_{M})$ with $\\theta_{M} = 0.3$, $\\mathbf{n}_{M} = (0,1,0)$.\n  - $P = P(\\mathbf{w})$ with $\\mathbf{w} = 0.4\\,(1,0,0)$.\n- Case $2$ (small step size to probe scaling):\n  - $\\alpha = 0.9$, $\\epsilon = 0.01$.\n  - $U = U(\\theta_{U}, \\mathbf{n}_{U})$ with $\\theta_{U} = 1.2$, $\\mathbf{n}_{U} = \\frac{(0.3,0.4,0.5)}{\\lVert(0.3,0.4,0.5)\\rVert}$.\n  - $M = U(\\theta_{M}, \\mathbf{n}_{M})$ with $\\theta_{M} = 0.7$, $\\mathbf{n}_{M} = (0,0,1)$.\n  - $P = P(\\mathbf{w})$ with $\\mathbf{w} = 0.2\\,\\frac{(0.2,0.8,0.5)}{\\lVert(0.2,0.8,0.5)\\rVert}$.\n- Case $3$ (edge case with zero initial momentum):\n  - $\\alpha = 1.5$, $\\epsilon = 0.02$.\n  - $U = U(\\theta_{U}, \\mathbf{n}_{U})$ with $\\theta_{U} = 0.5$, $\\mathbf{n}_{U} = \\frac{(0,1,1)}{\\lVert(0,1,1)\\rVert}$.\n  - $M = U(\\theta_{M}, \\mathbf{n}_{M})$ with $\\theta_{M} = 1.0$, $\\mathbf{n}_{M} = (1,0,0)$.\n  - $P = P(\\mathbf{w})$ with $\\mathbf{w} = (0,0,0)$.\n\nFinal output format:\n- Your program must print exactly one line of the form $[C_{1},C_{2},C_{3}]$, where each $C_{k}$ is the floating-point estimate of $\\Delta H/\\epsilon^{3}$ for the corresponding case, with no additional text.",
            "solution": "We base the construction on Hamiltonian dynamics on the Lie group $\\mathrm{SU}(2)$ with right-invariant kinetic energy and a gauge-like potential. The fundamental definitions are:\n- The link $U\\in \\mathrm{SU}(2)$ evolves via $\\dot U = P U$, where $P\\in \\mathfrak{su}(2)$ is the right-invariant momentum.\n- The momentum evolves via $\\dot P = -F(U)$, where the force $F(U)$ is the variational derivative of the potential $S(U)$ with respect to the right-invariant metric $\\langle A,B\\rangle = -\\operatorname{Re}\\operatorname{Tr}(A B)$ on $\\mathfrak{su}(2)$.\n\nDerivation of the force. Let the action be $S(U) = -\\alpha\\, \\operatorname{Re}\\operatorname{Tr}(U M^{\\dagger})$ with given $M\\in \\mathrm{SU}(2)$ and $\\alpha>0$. A general variation of $U$ tangent to $\\mathrm{SU}(2)$ at $U$ is $\\delta U = A U$ with $A\\in \\mathfrak{su}(2)$. Compute the first variation:\n\n$$\n\\delta S \\;=\\; -\\alpha\\, \\operatorname{Re}\\operatorname{Tr}(\\delta U\\, M^{\\dagger})\n\\;=\\; -\\alpha\\, \\operatorname{Re}\\operatorname{Tr}(A\\, U M^{\\dagger}).\n$$\n\nWe want to express $\\delta S$ in the form $\\delta S = \\langle F(U), A\\rangle = -\\operatorname{Re}\\operatorname{Tr}(F(U)\\, A)$ that holds for all $A\\in \\mathfrak{su}(2)$. Note the two facts:\n- For any $A\\in \\mathfrak{su}(2)$ and any matrix $X$, $\\operatorname{Re}\\operatorname{Tr}(A X) = \\operatorname{Re}\\operatorname{Tr}\\big(A\\, \\tfrac{1}{2}(X - X^{\\dagger})\\big)$ since $A$ is anti-Hermitian and pairs to zero with the Hermitian part in the real trace.\n- If we add any multiple of the identity to $F(U)$, it pairs to zero with $A$ because $\\operatorname{Tr}(A) = 0$.\n\nTherefore, for $\\delta S = -\\alpha\\, \\operatorname{Re}\\operatorname{Tr}(A\\, U M^{\\dagger})$ to equal $-\\operatorname{Re}\\operatorname{Tr}(F(U) A)$ for all $A\\in \\mathfrak{su}(2)$, it suffices to choose $F(U)$ to be the projection of $\\alpha\\, U M^{\\dagger}$ onto $\\mathfrak{su}(2)$ using the inner product induced by $-\\operatorname{Re}\\operatorname{Tr}(\\cdot\\,\\cdot)$. Writing the anti-Hermitian projection as $\\mathrm{AH}(X) = \\tfrac{1}{2}(X - X^{\\dagger})$ and then removing trace, the force is\n\n$$F(U) \\;=\\; \\mathrm{proj}_{\\mathfrak{su}(2)}\\!\\big(\\alpha\\, U M^{\\dagger}\\big) \\;=\\; \\alpha\\left[\\tfrac{1}{2}\\big( U M^{\\dagger} - M U^{\\dagger}\\big) \\;-\\; \\tfrac{1}{2}\\operatorname{Tr}\\!\\left(\\tfrac{1}{2}\\big( U M^{\\dagger} - M U^{\\dagger}\\big)\\right)\\mathbf{1}_{2}\\right].$$\n\nBy construction, $F(U)\\in \\mathfrak{su}(2)$ and satisfies $\\delta S = -\\operatorname{Re}\\operatorname{Tr}(F(U) A)$ for all $A\\in \\mathfrak{su}(2)$, so the Hamilton equation $\\dot P = -F(U)$ follows.\n\nLeapfrog integrator. For a separable Hamiltonian $H(U,P)=T(P)+S(U)$, the symmetric leapfrog with step size $\\epsilon$ composes the exact flows of $T$ and $S$ in a Strang splitting:\n- Half kick under $S$: $P \\leftarrow P - \\tfrac{\\epsilon}{2} F(U)$ with $U$ fixed.\n- Drift under $T$: $U \\leftarrow \\exp(\\epsilon P)\\, U$ with $P$ fixed.\n- Half kick under $S$: $P \\leftarrow P - \\tfrac{\\epsilon}{2} F(U)$ with updated $U$.\n\nOn $\\mathrm{SU}(2)$, if $P\\in \\mathfrak{su}(2)$ is anti-Hermitian traceless, then $P = i\\,\\mathbf{w}\\cdot \\boldsymbol{\\sigma}$ for some $\\mathbf{w}\\in \\mathbb{R}^{3}$ and $P^{2} = -\\lVert \\mathbf{w}\\rVert^{2}\\,\\mathbf{1}_{2}$. Hence the exponential has a closed form\n\n$$\\exp(\\epsilon P) \\;=\\; \\cos(\\epsilon \\lVert \\mathbf{w}\\rVert)\\,\\mathbf{1}_{2} \\;+\\; \\frac{\\sin(\\epsilon \\lVert \\mathbf{w}\\rVert)}{\\lVert \\mathbf{w}\\rVert}\\, P,$$\n\nwith the limiting value $\\mathbf{1}_{2} + \\epsilon P$ as $\\lVert \\mathbf{w}\\rVert \\to 0$. This guarantees that the drift stays on $\\mathrm{SU}(2)$ up to floating-point error, provided $P$ is kept in $\\mathfrak{su}(2)$ by construction of the kicks.\n\nHamiltonian and local error. With $T(P) = -\\tfrac{1}{2}\\operatorname{Re}\\operatorname{Tr}(P^{2})$ and $S(U) = -\\alpha\\, \\operatorname{Re}\\operatorname{Tr}(U M^{\\dagger})$, the Hamiltonian evaluated before and after one leapfrog step yields\n\n$$\\Delta H \\;=\\; H_{\\text{after}} - H_{\\text{before}}.$$\n\nBecause the leapfrog method is a second-order, symmetric, symplectic integrator, its local energy error per step scales as $\\mathcal{O}(\\epsilon^{3})$ for smooth $H$. One can estimate the leading coefficient by computing $C \\approx \\Delta H / \\epsilon^{3}$ for a given small $\\epsilon$. For symmetric splittings, the modified (shadow) Hamiltonian that is exactly conserved differs from the true $H$ by $\\mathcal{O}(\\epsilon^{2})$ terms, and the observed one-step drift in $H$ itself is $\\mathcal{O}(\\epsilon^{3})$ due to cancellation of even-order terms in a single-step symmetric update.\n\nNumerical implementation details:\n- Parameterization of $U$ as $U(\\theta,\\mathbf{n}) = \\cos(\\theta)\\,\\mathbf{1}_{2} + i \\sin(\\theta)\\,\\mathbf{n}\\cdot \\boldsymbol{\\sigma}$ for unit $\\mathbf{n}$ ensures $U\\in \\mathrm{SU}(2)$.\n- Parameterization of $P$ as $P(\\mathbf{w}) = i\\,\\mathbf{w}\\cdot \\boldsymbol{\\sigma}$ ensures $P\\in \\mathfrak{su}(2)$.\n- The projection $\\mathrm{proj}_{\\mathfrak{su}(2)}(X)$ is implemented as $\\tfrac{1}{2}(X - X^{\\dagger})$ followed by removal of the trace by subtracting $\\tfrac{1}{2}\\operatorname{Tr}(\\cdot)\\,\\mathbf{1}_{2}$.\n- The drift uses the closed form exponential on $\\mathrm{SU}(2)$ with careful handling of the small-norm limit.\n- The Hamiltonian terms are scalars computed via real parts of traces, with no physical units and all angles in radians as required.\n\nTest suite coverage:\n- Case $1$ uses moderate noncommuting $U$, $M$, and nonzero $P$ with $\\epsilon = 0.05$ to test the general path.\n- Case $2$ uses a smaller $\\epsilon = 0.01$ to highlight the $\\epsilon^{3}$ scaling.\n- Case $3$ uses zero initial momentum $P=0$ and nontrivial $U$, $M$ to probe the correctness of force kicks from rest.\n\nThe program computes $C_{k} = \\Delta H_{k}/\\epsilon_{k}^{3}$ for all three cases and prints a single line $[C_{1},C_{2},C_{3}]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Pauli matrices\nsigma1 = np.array([[0, 1], [1, 0]], dtype=complex)\nsigma2 = np.array([[0, -1j], [1j, 0]], dtype=complex)\nsigma3 = np.array([[1, 0], [0, -1]], dtype=complex)\npaulis = np.array([sigma1, sigma2, sigma3], dtype=complex)\nI2 = np.eye(2, dtype=complex)\n\ndef su2_from_axis_angle(theta: float, axis: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Construct U = cos(theta) I + i sin(theta) (n · sigma), for unit axis n.\n    \"\"\"\n    axis = np.asarray(axis, dtype=float)\n    nrm = np.linalg.norm(axis)\n    if nrm == 0.0:\n        return I2.copy()\n    n = axis / nrm\n    H = n[0] * sigma1 + n[1] * sigma2 + n[2] * sigma3  # Hermitian\n    return np.cos(theta) * I2 + 1j * np.sin(theta) * H\n\ndef p_from_vec(w: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Construct P = i (w · sigma), which is anti-Hermitian and traceless.\n    \"\"\"\n    w = np.asarray(w, dtype=float)\n    H = w[0] * sigma1 + w[1] * sigma2 + w[2] * sigma3  # Hermitian traceless\n    return 1j * H\n\ndef proj_su2(X: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Project a 2x2 complex matrix X onto su(2) using the inner product -Re Tr(A B).\n    This is the anti-Hermitian part with trace removed.\n    \"\"\"\n    AH = 0.5 * (X - X.conj().T)  # anti-Hermitian\n    tr = 0.5 * np.trace(AH)      # complex scalar; for su(2), subtract (tr)*I\n    return AH - tr * I2\n\ndef exp_su2(eps: float, P: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute exp(eps * P) for P in su(2) using the closed form:\n    exp(eps P) = cos(eps*|w|) I + (sin(eps*|w|)/|w|) P with P = i w·sigma.\n    Handles the small-norm limit stably.\n    \"\"\"\n    # Norm |w| satisfies: P^2 = -|w|^2 I, and -Tr(P^2)/2 = |w|^2\n    P2 = P @ P\n    # Tr(P^2) should be negative real\n    trP2 = np.trace(P2)\n    # Use real part for safety\n    w2 = -np.real(trP2) / 2.0\n    # Numerical guard\n    if w2 < 0:\n        # Clamp tiny negative due to numerical rounding\n        if w2 > -1e-14:\n            w2 = 0.0\n        else:\n            raise RuntimeError(\"Encountered negative w^2 beyond rounding.\")\n    wnorm = np.sqrt(w2)\n    if wnorm < 1e-14:\n        # Use series expansion: exp(eps P) ≈ I + eps P + (eps^2/2) P^2\n        return I2 + eps * P + 0.5 * (eps ** 2) * (P2)\n    c = np.cos(eps * wnorm)\n    s_over_w = np.sin(eps * wnorm) / wnorm\n    return c * I2 + s_over_w * P\n\ndef force(U: np.ndarray, alpha: float, M: np.ndarray) -> np.ndarray:\n    \"\"\"\n    F(U) = proj_su2(alpha * U * M†)\n    \"\"\"\n    return proj_su2(alpha * (U @ M.conj().T))\n\ndef kinetic(P: np.ndarray) -> float:\n    # T(P) = -0.5 Re Tr(P^2)\n    return float(-0.5 * np.real(np.trace(P @ P)))\n\ndef potential(U: np.ndarray, alpha: float, M: np.ndarray) -> float:\n    # S(U) = - alpha Re Tr(U M†)\n    return float(-alpha * np.real(np.trace(U @ M.conj().T)))\n\ndef hamiltonian(U: np.ndarray, P: np.ndarray, alpha: float, M: np.ndarray) -> float:\n    return kinetic(P) + potential(U, alpha, M)\n\ndef leapfrog_step(U: np.ndarray, P: np.ndarray, alpha: float, M: np.ndarray, eps: float):\n    \"\"\"\n    Perform one symmetric leapfrog step:\n    P <- P - (eps/2) F(U)\n    U <- exp(eps P) U\n    P <- P - (eps/2) F(U)\n    \"\"\"\n    # Half-kick\n    F = force(U, alpha, M)\n    P_half = P - 0.5 * eps * F\n    # Drift\n    R = exp_su2(eps, P_half)\n    U_new = R @ U\n    # Half-kick\n    F_new = force(U_new, alpha, M)\n    P_new = P_half - 0.5 * eps * F_new\n    return U_new, P_new\n\ndef case_general():\n    alpha = 1.1\n    eps = 0.05\n    # U: theta=0.8, axis=(1,2,3)/||...||\n    axis_U = np.array([1.0, 2.0, 3.0])\n    axis_U /= np.linalg.norm(axis_U)\n    U = su2_from_axis_angle(0.8, axis_U)\n    # M: theta=0.3, axis=(0,1,0)\n    M = su2_from_axis_angle(0.3, np.array([0.0, 1.0, 0.0]))\n    # P: w=0.4*(1,0,0)\n    P = p_from_vec(np.array([0.4, 0.0, 0.0]))\n    return U, P, alpha, M, eps\n\ndef case_small_step():\n    alpha = 0.9\n    eps = 0.01\n    # U: theta=1.2, axis=(0.3,0.4,0.5)/||...||\n    axis = np.array([0.3, 0.4, 0.5])\n    axis /= np.linalg.norm(axis)\n    U = su2_from_axis_angle(1.2, axis)\n    # M: theta=0.7, axis=(0,0,1)\n    M = su2_from_axis_angle(0.7, np.array([0.0, 0.0, 1.0]))\n    # P: w=0.2 * normalized (0.2,0.8,0.5)\n    w = np.array([0.2, 0.8, 0.5])\n    w = 0.2 * w / np.linalg.norm(w)\n    P = p_from_vec(w)\n    return U, P, alpha, M, eps\n\ndef case_zero_momentum():\n    alpha = 1.5\n    eps = 0.02\n    # U: theta=0.5, axis=(0,1,1)/||...||\n    axis = np.array([0.0, 1.0, 1.0])\n    axis /= np.linalg.norm(axis)\n    U = su2_from_axis_angle(0.5, axis)\n    # M: theta=1.0, axis=(1,0,0)\n    M = su2_from_axis_angle(1.0, np.array([1.0, 0.0, 0.0]))\n    # P: zero\n    P = np.zeros((2, 2), dtype=complex)\n    return U, P, alpha, M, eps\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        case_general(),\n        case_small_step(),\n        case_zero_momentum(),\n    ]\n\n    results = []\n    for U, P, alpha, M, eps in test_cases:\n        # Compute initial Hamiltonian\n        H_before = hamiltonian(U, P, alpha, M)\n        # One leapfrog step\n        U_new, P_new = leapfrog_step(U, P, alpha, M, eps)\n        # Compute final Hamiltonian\n        H_after = hamiltonian(U_new, P_new, alpha, M)\n        dH = H_after - H_before\n        # Estimate leading O(eps^3) coefficient\n        if eps == 0:\n            c = float('nan')\n        else:\n            c = dH / (eps ** 3)\n        results.append(c)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "In theory, the leapfrog integrator is perfectly time-reversible, a property that is essential for the detailed balance condition of the HMC algorithm. In practice, numerical errors—especially from inexact force calculations in systems with fermions—can break this crucial symmetry. This practice guides you through a hands-on investigation of this effect using a simplified model where force calculations are deliberately contaminated with a controlled error, mimicking the residual from an iterative solver . You will implement the standard \"forward-backward\" test to quantify the resulting reversibility violation, learning a critical diagnostic technique for ensuring the quality and reliability of any HMC simulation.",
            "id": "3516849",
            "problem": "Given a Hybrid Monte Carlo (HMC) algorithm for Lattice Quantum Chromodynamics (QCD), consider a simplified Abelian lattice gauge model where the gauge field on each site is an angle variable. Let a configuration be represented by a vector of angles $U \\in \\mathbb{R}^{V}$ with $V$ sites, and conjugate momenta $P \\in \\mathbb{R}^{V}$. Use the Wilson gauge action for a one-link approximation,\n$$\nS_{\\text{g}}[U] = \\beta \\sum_{i=1}^{V} \\left(1 - \\cos U_i\\right),\n$$\nand define the Hamiltonian\n$$\nH(U,P) = S_{\\text{g}}[U] + \\frac{1}{2} \\sum_{i=1}^{V} P_i^2.\n$$\nThe exact gauge force is\n$$\nF(U) = \\frac{\\partial S_{\\text{g}}}{\\partial U} = \\beta \\, \\sin U,\n$$\napplied component-wise. Hamilton's equations are\n$$\n\\frac{dU}{d\\tau} = \\frac{\\partial H}{\\partial P} = P, \\quad \\frac{dP}{d\\tau} = -\\frac{\\partial H}{\\partial U} = -F(U).\n$$\nUse the standard leapfrog integrator with step size $\\delta \\tau$ and $L$ steps:\n1. Half-step update of momenta: $P \\leftarrow P - \\frac{\\delta \\tau}{2} \\tilde{F}(U;\\epsilon)$.\n2. For $s = 1,\\dots,L-1$ do:\n   a. Full-step update of coordinates: $U \\leftarrow U + \\delta \\tau \\, P$.\n   b. Full-step update of momenta: $P \\leftarrow P - \\delta \\tau \\, \\tilde{F}(U;\\epsilon)$.\n3. Final full-step update of coordinates: $U \\leftarrow U + \\delta \\tau \\, P$.\n4. Final half-step update of momenta: $P \\leftarrow P - \\frac{\\delta \\tau}{2} \\tilde{F}(U;\\epsilon)$.\n\nIn exact arithmetic with exact forces, the leapfrog map is time-reversible. However, in realistic HMC with fermions, an inexact linear solve introduces a finite residual $\\epsilon$ that contaminates the force. Model this finite-precision and solver-residual contamination by an approximate force\n$$\n\\tilde{F}(U;\\epsilon) = F(U) + \\epsilon \\, \\lVert F(U) \\rVert_2 \\, \\hat{r},\n$$\nwhere $\\hat{r}$ is a unit vector constructed deterministically from a pseudorandom number generator seed and changes with each force evaluation, and $\\lVert \\cdot \\rVert_2$ is the Euclidean norm. This mimics the effect that a Conjugate Gradient (CG) method with residual tolerance $\\epsilon$ introduces a force error proportional to $\\epsilon$ times a norm of the true force. The forward-backward reversibility test uses the following cycle:\n- Start from $(U_0, P_0)$ and compute $H_0 = H(U_0,P_0)$.\n- Run a forward trajectory using $\\tilde{F}(U;\\epsilon)$ for $L$ steps to get $(U_1, P_1)$.\n- Flip the momentum, $(U_1, -P_1)$, and run a backward trajectory using $\\tilde{F}(U;\\epsilon)$ for $L$ steps to obtain $(U_2, P_2)$.\n- Compute the cycle energy difference\n$$\n\\Delta H_{\\text{cycle}} = H(U_2, P_2) - H(U_0, P_0).\n$$\n\nYour task is to implement this model and quantify reversibility violations due to finite precision by measuring $\\Delta H_{\\text{cycle}}$ as a function of the linear solver residual $\\epsilon$. Map and verify the scaling trend $\\Delta H_{\\text{cycle}} \\propto \\epsilon \\times \\lVert \\partial H/\\partial U \\rVert_2$ within this toy model, where $\\partial H/\\partial U = F(U)$.\n\nImplementation requirements:\n- Use the leapfrog integrator defined above.\n- Use the approximate force model $\\tilde{F}(U;\\epsilon)$.\n- Use a deterministic pseudorandom number generator with specified integer seeds for:\n  - Initial momenta (Gaussian components with zero mean and unit variance).\n  - Construction of the unit vector $\\hat{r}$ at each force call. Use one seed for the forward trajectory and a different seed for the backward trajectory to emulate direction-dependent finite precision differences.\n- Compute $\\lVert \\partial H/\\partial U \\rVert_2$ at the starting configuration $(U_0,P_0)$ as $\\lVert F(U_0) \\rVert_2$ for diagnostic purposes in the solution. The program's final output is only the list of $\\Delta H_{\\text{cycle}}$ across the test suite.\n\nUnits:\n- All quantities are dimensionless. No physical units are required.\n\nTest suite:\nProvide results for the following parameter sets, each producing a single scalar $\\Delta H_{\\text{cycle}}$ (a float):\n1. General case with moderate residual:\n   - $V = 128$, $\\beta = 5.7$, $\\delta \\tau = 0.1$, $L = 50$, $\\epsilon = 10^{-8}$, initial $U_0$ drawn i.i.d. from a normal distribution with mean $0$ and standard deviation $1$ using seed $123456$, initial $P_0$ drawn i.i.d. from a normal distribution with mean $0$ and standard deviation $1$ using seed $123457$, forward force-noise seed $987650$, backward force-noise seed $987651$.\n2. Boundary case with exact force (no residual):\n   - $V = 128$, $\\beta = 5.7$, $\\delta \\tau = 0.1$, $L = 50$, $\\epsilon = 0$, initial $U_0$ seed $123456$, initial $P_0$ seed $123457$, forward seed $111111$, backward seed $222222$.\n3. Small residual:\n   - $V = 128$, $\\beta = 5.7$, $\\delta \\tau = 0.1$, $L = 50$, $\\epsilon = 10^{-10}$, initial $U_0$ seed $123456$, initial $P_0$ seed $123457$, forward seed $333333$, backward seed $444444$.\n4. Larger residual:\n   - $V = 128$, $\\beta = 5.7$, $\\delta \\tau = 0.1$, $L = 50$, $\\epsilon = 10^{-6}$, initial $U_0$ seed $123456$, initial $P_0$ seed $123457$, forward seed $555555$, backward seed $666666$.\n5. Edge case with vanishing gauge-force norm at the start:\n   - $V = 128$, $\\beta = 5.7$, $\\delta \\tau = 0.1$, $L = 50$, $\\epsilon = 10^{-8}$, initial $U_0$ set to the zero vector, initial $P_0$ seed $777777$, forward seed $888888$, backward seed $999999$.\n\nFinal output format:\nYour program should produce a single line of output containing the $\\Delta H_{\\text{cycle}}$ results for the five test cases as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$). The $r_i$ must be printed as standard Python floats. No additional text should be printed before or after this line.",
            "solution": "The problem requires an implementation and analysis of a toy model for reversibility violations in the Hybrid Monte Carlo (HMC) algorithm, which is a cornerstone of modern lattice Quantum Chromodynamics (QCD) simulations. The HMC algorithm generates configurations $U$ distributed according to a probability density proportional to $e^{-S[U]}$, where $S[U]$ is the action. It achieves this by introducing fictitious momenta $P$ and simulating Hamiltonian dynamics in the phase space $(U, P)$, followed by a Metropolis-Hastings acceptance step. The validity of this acceptance step relies crucially on the properties of the numerical integrator used for the molecular dynamics (MD) trajectory, specifically its time-reversibility and area-preservation (symplecticity).\n\nThe provided model simplifies the complex dynamics of QCD to an Abelian gauge model on $V$ sites, where the configuration is described by a vector of angles $U = (U_1, \\dots, U_V)$. The system's evolution is governed by the Hamiltonian:\n$$\nH(U,P) = S_{\\text{g}}[U] + \\frac{1}{2} \\sum_{i=1}^{V} P_i^2\n$$\nwhere $S_{\\text{g}}[U] = \\beta \\sum_{i=1}^{V} (1 - \\cos U_i)$ is the Wilson gauge action and the second term is the kinetic energy. The equations of motion are given by Hamilton's equations:\n$$\n\\frac{dU_i}{d\\tau} = \\frac{\\partial H}{\\partial P_i} = P_i\n$$\n$$\n\\frac{dP_i}{d\\tau} = -\\frac{\\partial H}{\\partial U_i} = -\\frac{\\partial S_{\\text{g}}}{\\partial U_i} = -\\beta \\sin U_i = -F_i(U)\n$$\nHere, $\\tau$ is the fictitious simulation time, and $F(U)$ is the gauge force.\n\nThese equations are solved numerically using the leapfrog integrator, a second-order symplectic method. A single step of the leapfrog integrator from time $\\tau$ to $\\tau + \\delta\\tau$ can be written as a composition of operators: a half-step drift in momentum, a full-step drift in position, and another half-step drift in momentum. Being symplectic ensures that it preserves phase space volume. Crucially, it is also time-reversible. If we denote the map from $(U(\\tau), P(\\tau))$ to $(U(\\tau+\\delta\\tau), P(\\tau+\\delta\\tau))$ as $\\Phi_{\\delta\\tau}$, then reversing the time step, $\\delta\\tau \\to -\\delta\\tau$, is equivalent to inverting the map: $\\Phi_{-\\delta\\tau} = \\Phi_{\\delta\\tau}^{-1}$. A practical test of reversibility involves running a trajectory forward for a total time $T = L\\delta\\tau$, flipping the final momentum $P \\to -P$, and running the trajectory backward for the same duration. For a perfectly reversible integrator, the system returns to its exact initial state, $(U_0, P_0)$.\n\nIn realistic HMC simulations, particularly those including dynamical fermions, the force calculation is computationally expensive. It requires solving a large sparse linear system of the form $D\\psi = \\eta$, where $D$ is the fermion matrix. This is typically done with iterative solvers like the Conjugate Gradient (CG) algorithm, which is terminated when the norm of the residual reaches a certain tolerance, $\\epsilon$. This introduces a small, effectively random error into the force calculation. The problem models this with an approximate force:\n$$\n\\tilde{F}(U;\\epsilon) = F(U) + \\epsilon \\, \\lVert F(U) \\rVert_2 \\, \\hat{r}\n$$\nwhere $F(U)$ is the exact gauge force, $\\lVert F(U) \\rVert_2$ is its Euclidean norm, and $\\hat{r}$ is a randomly oriented unit vector that changes at each force evaluation. The magnitude of the error is proportional to the solver tolerance $\\epsilon$ and the force magnitude, which is a realistic model.\n\nThe introduction of this error term $\\epsilon \\, \\lVert F(U) \\rVert_2 \\, \\hat{r}$ breaks the time-reversibility of the leapfrog integrator. The error vector $\\hat{r}$ is generated from a pseudorandom number generator (PRNG). When the trajectory is reversed by flipping the momentum, the sequence of states $U$ traced in the backward path is approximately the same as in the forward path, but the PRNG continues to advance. It generates a new, independent sequence of random vectors $\\hat{r}$. The errors from the forward trajectory are not canceled by the errors in the backward trajectory, leading to a deviation from the initial state.\n\nThe task is to quantify this violation by measuring the change in the Hamiltonian over a full forward-backward cycle, $\\Delta H_{\\text{cycle}} = H(U_2, P_2) - H(U_0, P_0)$. The deviation of the final state from the initial state, $(U_2 - U_0, P_2 - P_0)$, is expected to be of order $\\mathcal{O}(\\epsilon)$. By Taylor expansion, the change in Hamiltonian is $\\Delta H_{\\text{cycle}} \\approx \\nabla H \\cdot (U_2 - U_0, P_2 - P_0)$, which implies $\\Delta H_{\\text{cycle}}$ should also be of order $\\mathcal{O}(\\epsilon)$. The problem further suggests a specific scaling relationship, $\\Delta H_{\\text{cycle}} \\propto \\epsilon \\times \\lVert F(U_0) \\rVert_2$, which we will investigate.\n\nThe implementation will consist of the following components:\n1.  Functions to calculate the action $S_{\\text{g}}[U]$ and the total Hamiltonian $H(U, P)$.\n2.  A function to calculate the approximate force $\\tilde{F}(U; \\epsilon)$, which internally computes the exact force $F(U) = \\beta \\sin U$ and adds the error term. This function will take a PRNG object as an argument to correctly manage the state of the random number sequence.\n3.  A function for the leapfrog integrator, as specified in the problem, which performs $L$ integration steps of size $\\delta\\tau$.\n4.  A main routine that executes the full reversibility test for each parameter set:\n    a. Initialize phase space coordinates $(U_0, P_0)$ using the specified seeds.\n    b. Calculate the initial energy $H_0$.\n    c. Run a forward MD trajectory of $L$ steps starting from $(U_0, P_0)$ to get $(U_1, P_1)$, using a specific PRNG seed for the force noise.\n    d. Run a backward MD trajectory of $L$ steps starting from $(U_1, -P_1)$ to get $(U_2, P_2)$, using a different PRNG seed for the force noise.\n    e. Calculate the final energy $H_2 = H(U_2, P_2)$ and the difference $\\Delta H_{\\text{cycle}} = H_2 - H_0$.\n\nThe test suite is designed to probe the behavior of $\\Delta H_{\\text{cycle}}$:\n-   Case 1 ($V=128$, $\\beta=5.7$, $\\epsilon=10^{-8}$): A typical scenario that should yield a small but measurable reversibility violation.\n-   Case 2 ($\\epsilon=0$): The control case. With an exact force, the leapfrog integrator is reversible up to standard floating-point arithmetic errors. $\\Delta H_{\\text{cycle}}$ should be close to zero (e.g., $\\approx 10^{-14}$ or smaller).\n-   Cases 3 and 4 ($\\epsilon=10^{-10}$ and $\\epsilon=10^{-6}$): These, along with Case 1, test the linear scaling of $\\Delta H_{\\text{cycle}}$ with $\\epsilon$. We expect the results to scale by factors of $10^{-2}$ and $10^2$ relative to Case 1.\n-   Case 5 ($U_0 = \\vec{0}$): An edge case where the initial force is zero, $\\lVert F(U_0) \\rVert_2 = 0$. This implies the error term is also zero at the very first step. However, as the system evolves under the influence of the non-zero initial momenta $P_0$, $U$ becomes non-zero, leading to a non-zero force and subsequent accumulation of reversibility-breaking errors. This case demonstrates that the simple propoosed scaling with $\\lVert F(U_0) \\rVert_2$ can be misleading; a more accurate scaling relation would depend on the integrated force norm over the whole trajectory. The result should still be non-zero and roughly proportional to $\\epsilon$.\n\nBy executing these tests, we can numerically verify the theoretical understanding of how solver inaccuracies in HMC manifest as violations of fundamental conservation laws and symmetries.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_action(U, beta):\n    \"\"\"Calculates the Wilson gauge action for the U(1) model.\"\"\"\n    return beta * np.sum(1.0 - np.cos(U))\n\ndef calculate_hamiltonian(U, P, beta):\n    \"\"\"Calculates the Hamiltonian for the system.\"\"\"\n    action = calculate_action(U, beta)\n    kinetic_energy = 0.5 * np.sum(P**2)\n    return action + kinetic_energy\n\ndef exact_force(U, beta):\n    \"\"\"Calculates the exact gauge force F(U) = beta * sin(U).\"\"\"\n    return beta * np.sin(U)\n\ndef approximate_force(U, beta, epsilon, rng):\n    \"\"\"\n    Calculates the approximate force, including a modeled error term.\n    F_tilde = F(U) + epsilon * ||F(U)||_2 * r_hat\n    \"\"\"\n    F = exact_force(U, beta)\n    if epsilon == 0.0:\n        return F\n    \n    F_norm = np.linalg.norm(F)\n    if F_norm == 0.0:\n        return F\n        \n    # Generate a random vector and normalize it to get the unit vector r_hat\n    r = rng.standard_normal(size=U.shape)\n    r_norm = np.linalg.norm(r)\n    \n    # Handle the unlikely case where the random vector is zero\n    if r_norm == 0.0:\n      r_hat = np.zeros_like(r)\n    else:\n      r_hat = r / r_norm\n    \n    # Calculate the contaminated force\n    F_tilde = F + epsilon * F_norm * r_hat\n    return F_tilde\n\ndef leapfrog_integrator(U_start, P_start, dt, L, beta, epsilon, force_rng):\n    \"\"\"\n    Performs a molecular dynamics trajectory using the leapfrog algorithm.\n    \"\"\"\n    U = U_start.copy()\n    P = P_start.copy()\n\n    # Initial half-step for momentum (\"P-U-P\" scheme)\n    P -= 0.5 * dt * approximate_force(U, beta, epsilon, force_rng)\n\n    # L-1 full steps for position and momentum\n    for _ in range(L - 1):\n        U += dt * P\n        P -= dt * approximate_force(U, beta, epsilon, force_rng)\n\n    # Final full step for position\n    U += dt * P\n    \n    # Final half-step for momentum\n    P -= 0.5 * dt * approximate_force(U, beta, epsilon, force_rng)\n\n    return U, P\n\ndef run_reversibility_test(V, beta, dt, L, epsilon, u0_seed, p0_seed, fwd_seed, bwd_seed, u0_is_zero=False):\n    \"\"\"\n    Runs the full forward-backward reversibility test for a given set of parameters.\n    \"\"\"\n    # 1. Initialize initial conditions\n    p0_rng = np.random.default_rng(p0_seed)\n    \n    if u0_is_zero:\n        U0 = np.zeros(V, dtype=np.float64)\n    else:\n        u0_rng = np.random.default_rng(u0_seed)\n        U0 = u0_rng.normal(loc=0.0, scale=1.0, size=V)\n    \n    P0 = p0_rng.normal(loc=0.0, scale=1.0, size=V)\n\n    # 2. Calculate the initial Hamiltonian\n    H0 = calculate_hamiltonian(U0, P0, beta)\n\n    # 3. Run the forward trajectory\n    fwd_rng = np.random.default_rng(fwd_seed)\n    U1, P1 = leapfrog_integrator(U0, P0, dt, L, beta, epsilon, fwd_rng)\n\n    # 4. Run the backward trajectory with flipped momentum\n    bwd_rng = np.random.default_rng(bwd_seed)\n    U2, P2 = leapfrog_integrator(U1, -P1, dt, L, beta, epsilon, bwd_rng)\n    \n    # 5. Calculate the final Hamiltonian and the cycle energy difference\n    H2 = calculate_hamiltonian(U2, P2, beta)\n    delta_H_cycle = H2 - H0\n    \n    return delta_H_cycle\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (V, beta, dt, L, epsilon, u0_seed, p0_seed, fwd_seed, bwd_seed, u0_is_zero)\n        (128, 5.7, 0.1, 50, 1e-8, 123456, 123457, 987650, 987651, False),\n        (128, 5.7, 0.1, 50, 0.0, 123456, 123457, 111111, 222222, False),\n        (128, 5.7, 0.1, 50, 1e-10, 123456, 123457, 333333, 444444, False),\n        (128, 5.7, 0.1, 50, 1e-6, 123456, 123457, 555555, 666666, False),\n        (128, 5.7, 0.1, 50, 1e-8, 0, 777777, 888888, 999999, True), \n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_reversibility_test(*params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Generating a valid Markov chain of gauge configurations is only the first step; extracting statistically sound results is the crucial second half. The data from an HMC simulation forms an autocorrelated time series, meaning successive measurements are not independent. This exercise challenges you to apply the correct statistical methods for handling such data to produce unbiased expectation values and, most importantly, reliable error estimates . Through a series of conceptual verifications and a concrete calculation involving binning and the jackknife method, you will master the essential techniques for converting raw simulation output into trustworthy physical measurements.",
            "id": "3516757",
            "problem": "A Hybrid Monte Carlo (HMC) simulation for lattice Quantum Chromodynamics (QCD) generates a Markov chain of gauge configurations $U_t$ targeting the QCD path integral measure. Let $O[U]$ be a gauge-invariant observable, and suppose after discarding an initial thermalization segment, you record $N$ sequential measurements $O_t = O[U_t]$ from the stationary chain. You are asked to reason from first principles of Markov Chain Monte Carlo (MCMC) and time series analysis for error estimation with autocorrelation.\n\nConsider the following setup and data. Assume the chain is ergodic and satisfies detailed balance with respect to the target measure, and that thermalization has been properly discarded so that the recorded sequence is stationary. You measure the integer-valued topological charge $Q[U]$ at $N = 8$ successive trajectories and obtain the time series\n$$\nO_1,\\dots,O_8 = 0,\\,0,\\,1,\\,1,\\,0,\\,0,\\,1,\\,1.\n$$\nYou intend to estimate the ensemble average $\\langle O \\rangle$ and its statistical uncertainty. To mitigate autocorrelation, you consider nonoverlapping binning with bin size $B = 2$ (so there are $M = N/B = 4$ bins), and you apply the group jackknife to the $M$ bin means to estimate the standard error of the sample mean.\n\nSelect all statements that are correct:\n\nA. The simple sample mean $\\bar{O} = \\frac{1}{N}\\sum_{t=1}^{N} O_t$ computed after discarding thermalization is an unbiased estimator of $\\langle O \\rangle$ for a stationary, ergodic HMC chain satisfying detailed balance. Autocorrelation does not bias $\\bar{O}$ but inflates its variance.\n\nB. Applying a naive jackknife directly to the $N$ sequential measurements $\\{O_t\\}$ without any binning is asymptotically sufficient to account for autocorrelation in HMC outputs, so binning is unnecessary for consistent error bars.\n\nC. If the bin size $B$ greatly exceeds the (unknown) integrated autocorrelation time, then the $M$ bin means are approximately independent and identically distributed. A jackknife over these $M$ bin means yields a consistent estimator of the standard error of $\\bar{O}$ as $N \\to \\infty$.\n\nD. Thinning the time series by keeping every $k$-th sample while discarding the rest strictly improves the efficiency of $\\bar{O}$ at fixed total molecular dynamics cost in HMC, because reduced autocorrelation always outweighs the information lost by discarding samples.\n\nE. For the given data and $B = 2$, the group jackknife standard error of $\\bar{O}$ computed from the $M = 4$ bin means equals $\\sqrt{1/12}$ (approximately $0.289$).",
            "solution": "The problem statement has been validated and found to be scientifically sound, well-posed, and objective. It presents a standard scenario in the statistical analysis of Monte Carlo simulation data in computational physics.\n\nHere is a detailed, option-by-option analysis.\n\n**A. The simple sample mean $\\bar{O} = \\frac{1}{N}\\sum_{t=1}^{N} O_t$ computed after discarding thermalization is an unbiased estimator of $\\langle O \\rangle$ for a stationary, ergodic HMC chain satisfying detailed balance. Autocorrelation does not bias $\\bar{O}$ but inflates its variance.**\n\nThis statement makes two claims: (1) the sample mean $\\bar{O}$ is an unbiased estimator of the true ensemble average $\\langle O \\rangle$, and (2) autocorrelation inflates the variance of $\\barO$, but does not bias it.\n\n1.  **Unbiasedness of the Estimator**: The expectation of the sample mean is given by\n    $$ E[\\bar{O}] = E\\left[\\frac{1}{N}\\sum_{t=1}^{N} O_t\\right] $$\n    By the linearity of the expectation operator, this becomes\n    $$ E[\\bar{O}] = \\frac{1}{N}\\sum_{t=1}^{N} E[O_t] $$\n    The problem states that the Markov chain is stationary. This implies that the probability distribution of $O_t$ is the same for all $t$ and is the target distribution of the HMC simulation. Therefore, the expectation of each measurement $O_t$ is the true ensemble average, $E[O_t] = \\langle O \\rangle$. Substituting this into the equation gives\n    $$ E[\\bar{O}] = \\frac{1}{N}\\sum_{t=1}^{N} \\langle O \\rangle = \\frac{1}{N} (N \\langle O \\rangle) = \\langle O \\rangle $$\n    Thus, the sample mean $\\bar{O}$ is an unbiased estimator of $\\langle O \\rangle$, irrespective of any autocorrelation in the time series $\\{O_t\\}$.\n\n2.  **Effect of Autocorrelation on Variance**: The variance of the sample mean is\n    $$ \\text{Var}(\\bar{O}) = \\text{Var}\\left(\\frac{1}{N}\\sum_{t=1}^{N} O_t\\right) = \\frac{1}{N^2} \\sum_{t=1}^{N}\\sum_{s=1}^{N} \\text{Cov}(O_t, O_s) $$\n    Let $\\sigma_O^2 = \\text{Var}(O_t)$ be the variance of a single measurement (constant due to stationarity) and $\\rho(\\tau) = \\text{Cov}(O_t, O_{t+\\tau})/\\sigma_O^2$ be the normalized autocorrelation function. The variance of the mean can be expressed as\n    $$ \\text{Var}(\\bar{O}) = \\frac{\\sigma_O^2}{N} \\left[1 + 2 \\sum_{\\tau=1}^{N-1} \\left(1 - \\frac{\\tau}{N}\\right) \\rho(\\tau)\\right] $$\n    For independent and identically distributed (IID) samples, $\\rho(\\tau) = 0$ for $\\tau > 0$, and the variance simplifies to the familiar $\\text{Var}(\\bar{O})_{\\text{IID}} = \\sigma_O^2/N$. In physical systems simulated with HMC, we typically observe positive short-range autocorrelations, $\\rho(\\tau) > 0$. This means the term in the square brackets is greater than $1$. This factor is often expressed in terms of the integrated autocorrelation time, $\\tau_{\\text{int}} = \\frac{1}{2} + \\sum_{\\tau=1}^{\\infty} \\rho(\\tau)$, such that for large $N$, $\\text{Var}(\\bar{O}) \\approx \\frac{\\sigma_O^2}{N} (2\\tau_{\\text{int}})$. Since $2\\tau_{\\text{int}} > 1$ for positively correlated data, the variance is inflated compared to the IID case.\n\nBoth parts of the statement are correct. Autocorrelation does not introduce a bias into the sample mean but increases its variance, making naive error estimates that assume independence misleadingly small.\n\n**Verdict: Correct**\n\n**B. Applying a naive jackknife directly to the $N$ sequential measurements $\\{O_t\\}$ without any binning is asymptotically sufficient to account for autocorrelation in HMC outputs, so binning is unnecessary for consistent error bars.**\n\nThe naive jackknife estimator for the variance of the sample mean $\\bar{O}$ is given by\n$$ \\sigma^2_{\\text{jack}} = \\frac{N-1}{N} \\sum_{i=1}^{N} (\\bar{O}_{(i)} - \\bar{O})^2 $$\nwhere $\\bar{O}_{(i)}$ is the mean of the sample with the $i$-th observation removed. It can be shown that this expression is algebraically equivalent to\n$$ \\sigma^2_{\\text{jack}} = \\frac{1}{N(N-1)} \\sum_{i=1}^{N} (O_i - \\bar{O})^2 = \\frac{s^2}{N} $$\nwhere $s^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (O_i - \\bar{O})^2$ is the sample variance. This is precisely the formula for the variance of the mean for IID samples. As established in the analysis of statement A, the true variance for autocorrelated data is approximately $\\frac{s^2}{N}(2\\tau_{\\text{int}})$. The naive jackknife estimator misses the crucial factor of $2\\tau_{\\text{int}}$ that accounts for correlations. Therefore, it systematically underestimates the true variance for positively autocorrelated data and is not a consistent estimator. Methods like binning or blocking are specifically designed to overcome this deficiency.\n\n**Verdict: Incorrect**\n\n**C. If the bin size $B$ greatly exceeds the (unknown) integrated autocorrelation time, then the $M$ bin means are approximately independent and identically distributed. A jackknife over these $M$ bin means yields a consistent estimator of the standard error of $\\bar{O}$ as $N \\to \\infty$.**\n\nThis statement describes the entire rationale behind the binning (or blocking) method for error analysis. Let the bin means be $Y_k = \\frac{1}{B} \\sum_{t=(k-1)B+1}^{kB} O_t$ for $k=1, \\dots, M$.\n1.  **Independence of Bin Means**: Because the original time series $\\{O_t\\}$ is stationary, the bin means $\\{Y_k\\}$ are identically distributed. The separation in time between data points in different bins grows with the bin size $B$. If $B$ is much larger than the integrated autocorrelation time $\\tau_{\\text{int}}$ (i.e., $B \\gg \\tau_{\\text{int}}$), the correlations between data in bin $k$ and bin $j$ (for $k \\neq j$) become negligible. Consequently, the bin means $Y_k$ and $Y_j$ become approximately uncorrelated, and for many distributions, approximately independent.\n2.  **Consistency of Jackknife on Bin Means**: If the bin means $\\{Y_k\\}$ can be treated as approximately IID samples, then standard statistical methods can be applied to them. The jackknife method is a valid technique for estimating the variance of a statistic (in this case, the mean $\\bar{O} = \\bar{Y}$) from a set of samples. When applied to the $M$ approximately IID bin means, the jackknife estimator for the variance of $\\bar{O}$ becomes a consistent estimator. Consistency requires that as the total number of samples $N$ goes to infinity, both the bin size $B$ and the number of bins $M=N/B$ must also go to infinity. The condition $B \\gg \\tau_{\\text{int}}$ ensures that the bias from residual inter-bin correlations is small, and $M \\to \\infty$ ensures that the variance of the variance estimator itself goes to zero. The statement correctly captures these essential conditions.\n\n**Verdict: Correct**\n\n**D. Thinning the time series by keeping every $k$-th sample while discarding the rest strictly improves the efficiency of $\\bar{O}$ at fixed total molecular dynamics cost in HMC, because reduced autocorrelation always outweighs the information lost by discarding samples.**\n\nThe efficiency of an estimator is related to its variance for a fixed computational cost. To improve efficiency is to reduce the variance of the estimator without increasing the cost. The statement claims that by \"thinning\" (or subsampling) a given time series of $N$ points to a new series of $N/k$ points, the variance of the sample mean is reduced. Let $\\bar{O}$ be the mean of the original series and $\\bar{O}'$ be the mean of the thinned series. It is a well-established result in MCMC theory that for a reversible, stationary Markov chain,\n$$ \\text{Var}(\\bar{O}) \\le \\text{Var}(\\bar{O}') $$\nThis inequality means that by discarding data, one cannot obtain a more precise estimate of the mean. While thinning does reduce the autocorrelation of the resulting subsample, it also reduces the total number of data points, and the latter effect is always dominant or equal. Information is lost by discarding samples, even if they are correlated. Efficiency is therefore decreased (or at best, unchanged in the trivial case of $k=1$ or perfectly correlated data). Thinning can be useful for reducing data storage or for preparing samples for algorithms that assume independence, but it does not improve the statistical precision of the sample mean for a fixed simulation effort.\n\n**Verdict: Incorrect**\n\n**E. For the given data and $B = 2$, the group jackknife standard error of $\\bar{O}$ computed from the $M = 4$ bin means equals $\\sqrt{1/12}$ (approximately $0.289$).**\n\nWe are given the time series of $N=8$ measurements: $O_1, \\dots, O_8 = 0, 0, 1, 1, 0, 0, 1, 1$.\nThe binning parameters are bin size $B=2$ and number of bins $M=N/B = 8/2 = 4$.\n\n1.  **Compute the bin means**:\n    - Bin 1: $\\{O_1, O_2\\} = \\{0, 0\\} \\implies Y_1 = \\frac{0+0}{2} = 0$.\n    - Bin 2: $\\{O_3, O_4\\} = \\{1, 1\\} \\implies Y_2 = \\frac{1+1}{2} = 1$.\n    - Bin 3: $\\{O_5, O_6\\} = \\{0, 0\\} \\implies Y_3 = \\frac{0+0}{2} = 0$.\n    - Bin 4: $\\{O_7, O_8\\} = \\{1, 1\\} \\implies Y_4 = \\frac{1+1}{2} = 1$.\n    The set of bin means is $\\{Y_k\\} = \\{0, 1, 0, 1\\}$.\n\n2.  **Compute the overall mean**:\n    The mean of the original data is $\\bar{O} = (0+0+1+1+0+0+1+1)/8 = 4/8 = 0.5$.\n    The mean of the bin means is $\\bar{Y} = (0+1+0+1)/4 = 2/4 = 0.5$. As expected, $\\bar{O}=\\bar{Y}$.\n\n3.  **Compute the jackknife variance**:\n    The jackknife variance of the mean of $M$ samples $\\{Y_k\\}$ is given by the formula:\n    $$ \\sigma^2_{\\text{jack}} = \\frac{1}{M(M-1)} \\sum_{k=1}^{M} (Y_k - \\bar{Y})^2 $$\n    We have $M=4$ and $\\bar{Y}=0.5$. The sum of squares is:\n    $$ \\sum_{k=1}^{4} (Y_k - \\bar{Y})^2 = (0 - 0.5)^2 + (1 - 0.5)^2 + (0 - 0.5)^2 + (1 - 0.5)^2 $$\n    $$ = (-0.5)^2 + (0.5)^2 + (-0.5)^2 + (0.5)^2 = 0.25 + 0.25 + 0.25 + 0.25 = 1 $$\n    Substituting this into the variance formula:\n    $$ \\sigma^2_{\\text{jack}} = \\frac{1}{4(4-1)} \\times 1 = \\frac{1}{4 \\times 3} = \\frac{1}{12} $$\n    The problem asks for the standard error, which is the square root of the variance:\n    $$ \\text{SE}_{\\text{jack}} = \\sqrt{\\sigma^2_{\\text{jack}}} = \\sqrt{\\frac{1}{12}} $$\n    The numerical value is $\\sqrt{1/12} \\approx 0.288675$, which is approximately $0.289$. The calculation matches the statement.\n\n**Verdict: Correct**",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}