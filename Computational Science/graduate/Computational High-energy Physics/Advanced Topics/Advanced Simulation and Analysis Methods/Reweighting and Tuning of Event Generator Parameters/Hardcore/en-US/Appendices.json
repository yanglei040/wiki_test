{
    "hands_on_practices": [
        {
            "introduction": "This first exercise goes to the heart of the reweighting technique by connecting the QCD factorization theorem to a concrete calculation. By computing the weight change for a single event due to a different set of Parton Distribution Functions (PDFs), you will see how theoretical model variations can be tested efficiently. This practice is essential for understanding how uncertainties related to the proton's internal structure are evaluated without generating new, computationally expensive event samples. ",
            "id": "3532078",
            "problem": "An event generator has produced a single proton–proton event whose hard process is initiated by an up quark and an anti-up quark, i.e., the incoming flavors are $(u,\\bar{u})$. The first parton carries longitudinal momentum fraction $x_1=0.1$ from beam $1$ and the second parton carries $x_2=0.02$ from beam $2$. The event was generated at the factorization scale $\\mu_F=100\\ \\mathrm{GeV}$, and the renormalization scale is set equal to the factorization scale, $\\mu_R=\\mu_F$. Assume the leading-order partonic subprocess for $(u,\\bar{u})$ is unchanged under reweighting and that the only modification when moving between two parton distribution function (PDF) sets is in the PDFs themselves. Here, a PDF refers to a Parton Distribution Function (PDF), $f_i(x,\\mu_F)$, which gives the probability density to find a parton of flavor $i$ carrying momentum fraction $x$ at scale $\\mu_F$.\n\nYou are given central-member values from two different PDF sets, denoted $S_A$ (the original set used to generate the event) and $S_B$ (the target set to which the event will be reweighted). The numerical values (dimensionless) are:\n- For $S_A$: $f_u^{A}(x_1=0.1,\\mu_F=100\\ \\mathrm{GeV})=2.70$ and $f_{\\bar{u}}^{A}(x_2=0.02,\\mu_F=100\\ \\mathrm{GeV})=0.25$.\n- For $S_B$: $f_u^{B}(x_1=0.1,\\mu_F=100\\ \\mathrm{GeV})=2.35$ and $f_{\\bar{u}}^{B}(x_2=0.02,\\mu_F=100\\ \\mathrm{GeV})=0.27$.\n\nStarting from the factorization theorem for hadronic cross sections and basic properties of event weights in factorized Monte Carlo event generation, derive the reweighting factor $R$ that rescales the event weight from $S_A$ to $S_B$ for this event and compute its value. Express the final ratio $R$ as a dimensionless number and round your answer to four significant figures.\n\nIn addition, explain, using first principles and without invoking any pre-provided shortcut formulas, how uncertainty eigenvectors in a Hessian PDF set $\\{(k,+),(k,-)\\}_{k=1}^{N}$ can be propagated to an arbitrary observable via event-by-event PDF reweighting. Your explanation should clearly articulate the construction of eigenvector-specific event weights and how they are combined to yield the uncertainty of a weighted observable, but your final numeric answer must be only the reweighting factor $R$.",
            "solution": "The problem requires the derivation and calculation of a reweighting factor for a single high-energy physics event when changing the underlying Parton Distribution Function (PDF) set. It also asks for a conceptual explanation of how PDF uncertainties from a Hessian set are propagated to an observable. We will address these two parts in sequence.\n\nFirst, we validate the problem statement.\nThe givens are:\n- Incoming parton flavors: $(u,\\bar{u})$\n- Parton momentum fractions: $x_1=0.1$ from beam $1$ and $x_2=0.02$ from beam $2$.\n- Factorization scale: $\\mu_F=100\\ \\mathrm{GeV}$.\n- Renormalization scale: $\\mu_R=\\mu_F$.\n- Original PDF set $S_A$ values: $f_u^{A}(x_1=0.1,\\mu_F=100\\ \\mathrm{GeV})=2.70$ and $f_{\\bar{u}}^{A}(x_2=0.02,\\mu_F=100\\ \\mathrm{GeV})=0.25$.\n- Target PDF set $S_B$ values: $f_u^{B}(x_1=0.1,\\mu_F=100\\ \\mathrm{GeV})=2.35$ and $f_{\\bar{u}}^{B}(x_2=0.02,\\mu_F=100\\ \\mathrm{GeV})=0.27$.\n- Key assumptions: the leading-order partonic subprocess is unchanged, and only the PDFs themselves are modified between sets.\n\nThe problem is scientifically grounded in the theory of perturbative Quantum Chromodynamics (QCD) and its application in Monte Carlo event generation. The framework of PDF reweighting is standard practice in high-energy physics. The provided values are realistic, and the question is well-posed, self-contained, and objective. It does not violate any fundamental principles and is directly relevant to the specified topic. Hence, the problem is deemed valid.\n\nWe now proceed with the solution.\n\nPart 1: Derivation and Calculation of the Reweighting Factor $R$.\n\nThe foundation for calculating cross sections in hadronic collisions is the QCD factorization theorem. It states that the differential cross section $d\\sigma$ for a process $pp \\to X$ can be calculated by summing over all possible initial parton flavors $i$ and $j$, integrating over all possible momentum fractions $x_1$ and $x_2$, and convolving the PDFs with the partonic hard-scattering cross section $d\\hat{\\sigma}_{ij}$:\n$$\nd\\sigma = \\sum_{i,j} \\int dx_1 \\int dx_2 f_i(x_1, \\mu_F) f_j(x_2, \\mu_F) d\\hat{\\sigma}_{ij}(x_1, x_2, \\mu_F, \\mu_R)\n$$\nHere, $f_i(x, \\mu_F)$ is the PDF for parton flavor $i$ carrying momentum fraction $x$ of the proton's momentum at factorization scale $\\mu_F$. The term $d\\hat{\\sigma}_{ij}$ is the differential cross section for the partonic subprocess $i+j \\to F$, where $F$ is the final state of the hard scatter.\n\nA Monte Carlo event generator produces events corresponding to specific points in the available phase space. For a single event generated for a specific hard process, the initial partons and their momentum fractions are fixed. The weight $w$ of such an unweighted event (before showering, hadronization, and detector simulation) is proportional to the value of the integrand of the cross section formula at that specific kinematic point.\n\nFor the given event, the incoming partons are an up quark ($u$) from proton $1$ and an anti-up quark ($\\bar{u}$) from proton $2$. Their respective momentum fractions are $x_1=0.1$ and $x_2=0.02$. The event was generated using PDF set $S_A$. Therefore, its original weight, $w_A$, is proportional to the product of the relevant PDFs from set $S_A$ and the partonic cross section $\\hat{\\sigma}_{u\\bar{u}}$:\n$$\nw_A \\propto f_u^A(x_1, \\mu_F) f_{\\bar{u}}^A(x_2, \\mu_F) \\hat{\\sigma}_{u\\bar{u}}\n$$\nWe wish to determine the weight this exact same event would have if it had been generated using PDF set $S_B$. Let this new weight be $w_B$. According to the problem statement, the partonic cross section $\\hat{\\sigma}_{u\\bar{u}}$ and all other aspects of the event kinematics are unchanged. The only change is the PDF set. Thus, the new weight $w_B$ is:\n$$\nw_B \\propto f_u^B(x_1, \\mu_F) f_{\\bar{u}}^B(x_2, \\mu_F) \\hat{\\sigma}_{u\\bar{u}}\n$$\nThe reweighting factor $R$ is defined as the ratio of the new weight to the old weight, $R = w_B / w_A$. The proportionality constant and the partonic cross section $\\hat{\\sigma}_{u\\bar{u}}$ cancel in the ratio:\n$$\nR = \\frac{f_u^B(x_1, \\mu_F) f_{\\bar{u}}^B(x_2, \\mu_F)}{f_u^A(x_1, \\mu_F) f_{\\bar{u}}^A(x_2, \\mu_F)}\n$$\nWe can now substitute the provided numerical values into this expression.\nGiven:\n- $x_1 = 0.1$, $x_2 = 0.02$, $\\mu_F = 100\\ \\mathrm{GeV}$\n- $f_u^A(x_1, \\mu_F) = 2.70$\n- $f_{\\bar{u}}^A(x_2, \\mu_F) = 0.25$\n- $f_u^B(x_1, \\mu_F) = 2.35$\n- $f_{\\bar{u}}^B(x_2, \\mu_F) = 0.27$\n\nThe reweighting factor $R$ is:\n$$\nR = \\frac{(2.35) \\times (0.27)}{(2.70) \\times (0.25)} = \\frac{0.6345}{0.675}\n$$\nCalculating this value gives:\n$$\nR = 0.94\n$$\nThe problem requires the answer to be rounded to four significant figures. Therefore, $R = 0.9400$.\n\nPart 2: Explanation of Hessian PDF Uncertainty Propagation.\n\nThe second part of the task is to explain, from first principles, the propagation of PDF uncertainties using Hessian eigenvector sets.\n\nA modern PDF set, such as one following the Hessian method, provides not just a central (best-fit) PDF, denoted $S_0$, but also a collection of $N$ pairs of \"eigenvector\" sets, $\\{(S_k^+, S_k^-)\\}_{k=1}^N$. These eigenvector sets represent systematic excursions from the central fit along $N$ orthogonal directions in the multi-dimensional parameter space used for the PDF determination. Each pair $(S_k^+, S_k^-)$ corresponds to a $\\pm 1\\sigma$ deviation along the $k$-th eigenvector.\n\nThe process of propagating these uncertainties to a predicted observable $\\mathcal{O}$ relies on event-by-event reweighting, following the same principle derived in the first part. Let us assume a large sample of $M$ Monte Carlo events has been generated using the central PDF set $S_0$. Each event $e$ (for $e=1, \\dots, M$) has a weight $w_{0,e}$ and a value $\\mathcal{O}_e$ for the observable in question. The prediction for the observable using the central PDF set is the weighted average:\n$$\n\\langle \\mathcal{O} \\rangle_0 = \\frac{\\sum_{e=1}^M w_{0,e} \\mathcal{O}_e}{\\sum_{e=1}^M w_{0,e}}\n$$\nTo determine the impact of the PDF uncertainty, one does not need to generate new event samples for each of the $2N$ eigenvector sets. Instead, one can calculate $2N$ new weights for each event. For a single event $e$ with initial partons $i,j$ at momentum fractions $(x_{1,e}, x_{2,e})$, the reweighting factors from the central set $S_0$ to an eigenvector set $S_k^\\pm$ are:\n$$\nR_{k,e}^\\pm = \\frac{f_i^{(k,\\pm)}(x_{1,e}, \\mu_F) f_j^{(k,\\pm)}(x_{2,e}, \\mu_F)}{f_i^{(0)}(x_{1,e}, \\mu_F) f_j^{(0)}(x_{2,e}, \\mu_F)}\n$$\nThe new weight for event $e$ corresponding to PDF set $S_k^\\pm$ is $w_{k,e}^\\pm = w_{0,e} \\times R_{k,e}^\\pm$. Using these new weights, one can compute the value of the observable $\\mathcal{O}$ as it would be predicted by each eigenvector set:\n$$\n\\langle \\mathcal{O} \\rangle_k^\\pm = \\frac{\\sum_{e=1}^M w_{k,e}^\\pm \\mathcal{O}_e}{\\sum_{e=1}^M w_{k,e}^\\pm}\n$$\nThis procedure yields a central value $\\langle \\mathcal{O} \\rangle_0$ and $2N$ systematic variations, $\\langle \\mathcal{O} \\rangle_k^+$ and $\\langle \\mathcal{O} \\rangle_k^-$, for $k=1, \\dots, N$.\n\nThe final step is to combine these variations into a single total uncertainty. The Hessian formalism provides a master formula for this. Because the eigenvectors are constructed to be orthogonal in the parameter space of the PDF fit, their contributions to the total uncertainty can be added in quadrature. The symmetric formula for the total PDF uncertainty, $\\Delta \\mathcal{O}$, is:\n$$\n(\\Delta\\mathcal{O})^2 = \\sum_{k=1}^N \\left( \\frac{\\langle \\mathcal{O} \\rangle_k^+ - \\langle \\mathcal{O} \\rangle_k^-}{2} \\right)^2\n$$\nThe term $(\\langle \\mathcal{O} \\rangle_k^+ - \\langle \\mathcal{O} \\rangle_k^-)/2$ represents the change in the observable $\\mathcal{O}$ for a $+1\\sigma$ shift along the $k$-th eigenvector. Summing these shifts in quadrature gives the total variance due to the uncertainties in all PDF parameters. Asymmetric formulas also exist to handle cases where the upward and downward variations are not symmetric, but the principle of combining orthogonal contributions remains the same. This reweighting technique is computationally efficient, as it allows the estimation of PDF uncertainties from a single, centrally-generated event sample.",
            "answer": "$$\n\\boxed{0.9400}\n$$"
        },
        {
            "introduction": "While powerful, reweighting is not without its statistical costs, a challenge this exercise directly addresses. You will confront the crucial concept of effective sample size ($N_{\\text{eff}}$), a metric that quantifies the statistical power of a weighted event sample, particularly in the presence of negative weights common in Next-to-Leading Order (NLO) calculations. Understanding how to calculate and interpret $N_{\\text{eff}}$ is fundamental to assessing the stability of any reweighted analysis and ensuring the reliability of its conclusions. ",
            "id": "3532093",
            "problem": "You are performing parameter reweighting and tuning of a Next-to-Leading Order (NLO) event generator with a parton shower. A Monte Carlo (MC) sample of $N$ events is produced at a reference parameter value $\\theta_{0}$, with signed base event weights $\\{w_{i}^{(0)}\\}_{i=1}^{N}$ that include negative weights from subtraction terms. You consider a small retune to a nearby value $\\theta$, and approximate the per-event reweight factor by a first-order linear response model $r_{i}(\\theta)=1+a_{i}(\\theta-\\theta_{0})$, where the sensitivities $a_{i}$ are event-dependent and known. The reweighted event weights are $w_{i}=w_{i}^{(0)}\\,r_{i}(\\theta)$, and the estimator for the total cross section at $\\theta$ is the weighted sum $\\hat{\\sigma}=\\sum_{i=1}^{N} w_{i}$. Assume events are independent and that the standard MC variance estimate for a weighted sum applies.\n\nStarting only from the following foundations:\n- The definition of a weighted-sum estimator $\\hat{\\sigma}=\\sum_{i=1}^{N} w_{i}$ for independent events.\n- The well-tested fact that the variance of an independent weighted-sum estimator is $\\mathrm{Var}(\\hat{\\sigma})=\\sum_{i=1}^{N} w_{i}^{2}$ under standard MC sampling assumptions.\n- The definition of the effective sample size $N_{\\text{eff}}$ as the size of an equivalent unweighted sample that would yield the same relative statistical uncertainty, implying $N_{\\text{eff}}=(\\sum_{i=1}^{N} w_{i})^{2}/\\sum_{i=1}^{N} w_{i}^{2}$ and the relative statistical uncertainty $u_{\\text{rel}}=\\sqrt{\\mathrm{Var}(\\hat{\\sigma})}/|\\hat{\\sigma}|$.\n\nConsider the concrete sample with $N=10$, parameter shift $\\theta-\\theta_{0}=0.1$, and the following per-event data:\n- Event $1$: $w_{1}^{(0)}=1.2$, $a_{1}=0.05$.\n- Event $2$: $w_{2}^{(0)}=0.8$, $a_{2}=0.10$.\n- Event $3$: $w_{3}^{(0)}=-0.6$, $a_{3}=0.20$.\n- Event $4$: $w_{4}^{(0)}=0.5$, $a_{4}=-0.15$.\n- Event $5$: $w_{5}^{(0)}=-1.0$, $a_{5}=0.05$.\n- Event $6$: $w_{6}^{(0)}=2.0$, $a_{6}=0.00$.\n- Event $7$: $w_{7}^{(0)}=1.1$, $a_{7}=-0.10$.\n- Event $8$: $w_{8}^{(0)}=-0.9$, $a_{8}=0.15$.\n- Event $9$: $w_{9}^{(0)}=0.7$, $a_{9}=0.05$.\n- Event $10$: $w_{10}^{(0)}=-0.4$, $a_{10}=-0.20$.\n\nTasks:\n1. Compute the reweighted weights $w_{i}=w_{i}^{(0)}\\,[1+a_{i}(\\theta-\\theta_{0})]$ for all $i$, then compute $N_{\\text{eff}}$ and the relative statistical uncertainty $u_{\\text{rel}}=\\sqrt{\\sum_{i=1}^{N} w_{i}^{2}}/\\left|\\sum_{i=1}^{N} w_{i}\\right|$ on $\\hat{\\sigma}$ at $\\theta$. Report $N_{\\text{eff}}$ as a pure number and $u_{\\text{rel}}$ as a decimal fraction.\n2. Briefly justify, from the definitions above, why $N_{\\text{eff}}=(\\sum_{i} w_{i})^{2}/\\sum_{i} w_{i}^{2}$ and why negative weights tend to reduce $N_{\\text{eff}}$ for fixed $\\sum_{i} w_{i}^{2}$.\n3. Identify and explain at least three principled strategies that can mitigate variance blow-up when reweighting and tuning in the presence of signed weights, ensuring that each strategy is compatible with preserving unbiasedness in the target observable.\n\nRound both $N_{\\text{eff}}$ and $u_{\\text{rel}}$ to four significant figures. Express the final answer as a row vector $\\big(N_{\\text{eff}},\\,u_{\\text{rel}}\\big)$ with no units, where $u_{\\text{rel}}$ is a pure number (decimal fraction).",
            "solution": "The problem asks for calculations and conceptual explanations related to parameter reweighting in a Next-to-Leading Order (NLO) Monte Carlo event generator. The problem is validated as self-contained, scientifically grounded, and well-posed. We will address each of the three tasks in sequence.\n\nFirst, we address Task 1: computing the reweighted weights $w_i$, the effective sample size $N_{\\text{eff}}$, and the relative statistical uncertainty $u_{\\text{rel}}$.\n\nThe parameter shift is given as $\\theta - \\theta_0 = 0.1$. The reweight factor for each event $i$ is calculated using the provided linear response model: $r_i(\\theta) = 1 + a_i(\\theta - \\theta_0) = 1 + a_i \\times 0.1$. The reweighted weight is then $w_i = w_i^{(0)} r_i(\\theta)$.\n\nWe compute the reweighted weight $w_i$ for each of the $N=10$ events:\n- Event $1$: $r_1 = 1 + 0.05 \\times 0.1 = 1.005$. $w_1 = 1.2 \\times 1.005 = 1.206$.\n- Event $2$: $r_2 = 1 + 0.10 \\times 0.1 = 1.01$. $w_2 = 0.8 \\times 1.01 = 0.808$.\n- Event $3$: $r_3 = 1 + 0.20 \\times 0.1 = 1.02$. $w_3 = -0.6 \\times 1.02 = -0.612$.\n- Event $4$: $r_4 = 1 - 0.15 \\times 0.1 = 0.985$. $w_4 = 0.5 \\times 0.985 = 0.4925$.\n- Event $5$: $r_5 = 1 + 0.05 \\times 0.1 = 1.005$. $w_5 = -1.0 \\times 1.005 = -1.005$.\n- Event $6$: $r_6 = 1 + 0.00 \\times 0.1 = 1.00$. $w_6 = 2.0 \\times 1.00 = 2.0$.\n- Event $7$: $r_7 = 1 - 0.10 \\times 0.1 = 0.99$. $w_7 = 1.1 \\times 0.99 = 1.089$.\n- Event $8$: $r_8 = 1 + 0.15 \\times 0.1 = 1.015$. $w_8 = -0.9 \\times 1.015 = -0.9135$.\n- Event $9$: $r_9 = 1 + 0.05 \\times 0.1 = 1.005$. $w_9 = 0.7 \\times 1.005 = 0.7035$.\n- Event $10$: $r_{10} = 1 - 0.20 \\times 0.1 = 0.98$. $w_{10} = -0.4 \\times 0.98 = -0.392$.\n\nNext, we compute the sum of the reweighted weights, $\\sum_{i=1}^{N} w_i$, and the sum of their squares, $\\sum_{i=1}^{N} w_i^2$.\nThe sum of weights is:\n$$ \\hat{\\sigma} = \\sum_{i=1}^{10} w_i = 1.206 + 0.808 - 0.612 + 0.4925 - 1.005 + 2.0 + 1.089 - 0.9135 + 0.7035 - 0.392 = 3.3765 $$\nThe sum of weights squared is:\n$$ \\sum_{i=1}^{10} w_i^2 = (1.206)^2 + (0.808)^2 + (-0.612)^2 + (0.4925)^2 + (-1.005)^2 + (2.0)^2 + (1.089)^2 + (-0.9135)^2 + (0.7035)^2 + (-0.392)^2 $$\n$$ \\sum_{i=1}^{10} w_i^2 = 1.454436 + 0.652864 + 0.374544 + 0.24255625 + 1.010025 + 4.0 + 1.185921 + 0.83448225 + 0.49491225 + 0.153664 \\approx 10.403405 $$\n\nNow we can compute $N_{\\text{eff}}$ and $u_{\\text{rel}}$.\nUsing the provided formula for effective sample size:\n$$ N_{\\text{eff}} = \\frac{(\\sum_{i=1}^{N} w_i)^2}{\\sum_{i=1}^{N} w_i^2} = \\frac{(3.3765)^2}{10.403405} = \\frac{11.40073225}{10.403405} \\approx 1.095866 $$\nRounding to four significant figures, $N_{\\text{eff}} = 1.096$.\n\nThe relative statistical uncertainty is given by:\n$$ u_{\\text{rel}} = \\frac{\\sqrt{\\sum_{i=1}^{N} w_i^2}}{\\left|\\sum_{i=1}^{N} w_i\\right|} = \\frac{\\sqrt{10.403405}}{|3.3765|} \\approx \\frac{3.225431}{3.3765} \\approx 0.955255 $$\nRounding to four significant figures, $u_{\\text{rel}} = 0.9553$.\nAs a consistency check, we can verify that $u_{\\text{rel}} \\approx 1/\\sqrt{N_{\\text{eff}}}$. Indeed, $1/\\sqrt{1.095866} \\approx 0.955255$, which matches our result for $u_{\\text{rel}}$.\n\nSecond, we address Task 2: justifying the formula for $N_{\\text{eff}}$ and the effect of negative weights.\n\nThe justification for $N_{\\text{eff}} = (\\sum w_i)^2 / \\sum w_i^2$ starts from its definition: $N_{\\text{eff}}$ is the size of an equivalent unweighted sample that would yield the same relative statistical uncertainty.\nLet us consider an unweighted sample of size $N_{\\text{eff}}$. An unweighted sample means all events have the same weight. Let the total cross section estimate be $\\hat{\\sigma}$. Then each of the $N_{\\text{eff}}$ events has a weight $w' = \\hat{\\sigma}/N_{\\text{eff}}$. The estimator is $\\sum_{i=1}^{N_{\\text{eff}}} w' = N_{\\text{eff}} \\times (\\hat{\\sigma}/N_{\\text{eff}}) = \\hat{\\sigma}$. The variance of this estimator is $\\sum_{i=1}^{N_{\\text{eff}}} (w')^2 = N_{\\text{eff}} \\times (\\hat{\\sigma}/N_{\\text{eff}})^2 = \\hat{\\sigma}^2/N_{\\text{eff}}$. The relative uncertainty for this unweighted sample is therefore $u'_{\\text{rel}} = \\sqrt{\\text{Var}} / |\\hat{\\sigma}| = \\sqrt{\\hat{\\sigma}^2/N_{\\text{eff}}} / |\\hat{\\sigma}| = 1/\\sqrt{N_{\\text{eff}}}$.\nFor the original weighted sample, the relative uncertainty is given as $u_{\\text{rel}} = \\sqrt{\\sum w_i^2} / |\\sum w_i|$.\nBy equating the relative uncertainties, $u'_{\\text{rel}} = u_{\\text{rel}}$, we get $1/\\sqrt{N_{\\text{eff}}} = \\sqrt{\\sum w_i^2} / |\\sum w_i|$. Squaring both sides gives $1/N_{\\text{eff}} = (\\sum w_i^2) / (\\sum w_i)^2$. Inverting this expression yields the desired formula: $N_{\\text{eff}} = (\\sum w_i)^2 / \\sum w_i^2$.\n\nNegative weights tend to reduce $N_{\\text{eff}}$ for a fixed sum of squares $\\sum w_i^2$. The formula for $N_{\\text{eff}}$ has the sum of squares, $\\sum w_i^2$, in the denominator and the square of the sum, $(\\sum w_i)^2$, in the numerator. The term $\\sum w_i^2$ is always non-negative. For a fixed value of this denominator, $N_{\\text{eff}}$ is maximized by maximizing the numerator, $(\\sum w_i)^2$.\nWhen some weights $w_i$ are positive and others are negative, cancellations occur in the sum $\\sum w_i$. By the triangle inequality, $|\\sum w_i| \\leq \\sum |w_i|$. For a mixed-sign set of weights, this inequality is typically strict, i.e., $|\\sum w_i|  \\sum |w_i|$.\nIf we consider a hypothetical sample with weights $w'_i = |w_i|$, it has the same sum of squares, $\\sum (w'_i)^2 = \\sum |w_i|^2 = \\sum w_i^2$. However, its sum of weights is $\\sum w'_i = \\sum |w_i|$.\nSince $|\\sum w_i|  \\sum |w_i|$, it follows that $(\\sum w_i)^2  (\\sum |w_i|)^2$.\nTherefore, a sample with negative weights will have a smaller numerator $(\\sum w_i)^2$ compared to a sample where all weights are made positive, while the denominator $\\sum w_i^2$ remains the same. This leads to a smaller $N_{\\text{eff}}$. A smaller $N_{\\text{eff}}$ signifies a larger relative statistical uncertainty $u_{\\text{rel}} \\approx 1/\\sqrt{N_{\\text{eff}}}$.\n\nThird, we address Task 3: identifying three principled strategies to mitigate variance blow-up while preserving unbiasedness.\n\n1.  **Optimization of Matching and Shower Parameters**: In NLO with Parton Shower (NLO+PS) generators, large positive and negative weights often arise from a mismatch between the fixed-order real emission matrix element and the parton shower's approximation of it. By tuning parameters that control the separation between the hard and soft/collinear regimes (e.g., the shower starting scale, often called `hdamp` in POWHEG), one can achieve a smoother transition. This reduces the size of the subtraction terms and, consequently, the magnitudes of the resulting signed weights $|w_i^{(0)}|$ from the start. This directly reduces the sum of squares $\\sum w_i^2$ for a given total cross section $\\sum w_i$, thus increasing $N_{\\text{eff}}$. This procedure is compatible with unbiasedness because these parameters are part of the definition of how the matching is performed; changing them corresponds to a different, but formally equally valid, scheme for combining the NLO calculation with the shower. The integrated cross section remains unbiased.\n\n2.  **Definition of a Fiducial Phase Space**: Large weights are frequently generated in extreme regions of phase space where the theoretical model may be less reliable (e.g., very high transverse momentum) or that are outside the acceptance of an experimental analysis. A standard strategy is to define a fiducial volume by applying cuts on the final-state particles' kinematics (e.g., on momenta and pseudorapidities). Events falling outside this volume are discarded. This physically removes the source of the largest weights from the sample used for analysis and tuning. The variance is thus controlled. This preserves unbiasedness for the *fiducial cross section*, which is the quantity of interest in almost all comparisons between theory and experiment. The estimator for the fiducial cross section remains unbiased.\n\n3.  **Increasing the Size of the Monte Carlo Sample**: The effective sample size $N_{\\text{eff}}$ is proportional to the total number of generated events, $N$. Specifically, $N_{\\text{eff}} = N \\cdot (\\langle w \\rangle^2 / \\langle w^2 \\rangle)$, where $\\langle w \\rangle$ and $\\langle w^2 \\rangle$ are the sample averages of the weights and weights squared. For a given weight distribution, doubling the number of generated events $N$ will, on average, double $N_{\\text{eff}}$. This in turn reduces the relative statistical uncertainty $u_{\\text{rel}} \\approx 1/\\sqrt{N_{\\text{eff}}}$ by a factor of $1/\\sqrt{2}$. While computationally expensive, this \"brute force\" method is the most fundamental way to improve statistical precision. It is inherently unbiased and acts as a final recourse when other methods to improve the weight distribution are insufficient.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1.096  0.9553 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Ultimately, reweighting and tuning are performed to compare theoretical predictions with data, often in the form of differential distributions or histograms. This final practice moves from the properties of an event sample to the statistical properties of the binned observables themselves. You will explore the correct calculation of variances and covariances for weighted histogram bins, a critical skill for performing a goodness-of-fit test or extracting a physical parameter from a shape-based analysis. ",
            "id": "3532061",
            "problem": "Consider a Monte Carlo (MC) event generator used to simulate high-energy physics processes. Events are generated independently and each event $e$ carries a deterministic reweighting factor $w_e(\\boldsymbol{\\theta})$ that depends on a parameter vector $\\boldsymbol{\\theta}$. You build a histogram for an observable $x$ with bins indexed by $b$, and you fill the unnormalized bin content as $S_b(\\boldsymbol{\\theta}) = \\sum_{e \\in b} w_e(\\boldsymbol{\\theta})$, where the sum is over events that contribute an entry to bin $b$. Assume events are sampled from a Poisson point process over phase space with intensity measure consistent with the underlying cross section, and that different events are independent. For disjoint bins that do not share a single entry from the same event, Poisson point process properties imply counts in disjoint regions are independent. However, if the same event can contribute entries to both bins (for example, two different objects reconstructed from the same event, each falling into a different bin), the bin contents can become correlated through the event-level weight.\n\nStarting from the definition of variance $\\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\left(\\mathbb{E}[X]\\right)^2$, properties of Poisson point processes (independence across disjoint regions and Campbell’s theorem for sums over points), and linear error propagation for transformations of random variables, reason about the variance of weighted histogram bins, the covariance between two bins that share events, and the effect of unit-area normalization in shape-only fits. Consider the unit-area normalization $y_b(\\boldsymbol{\\theta}) = S_b(\\boldsymbol{\\theta}) / T(\\boldsymbol{\\theta})$ with $T(\\boldsymbol{\\theta}) = \\sum_k S_k(\\boldsymbol{\\theta})$.\n\nWhich of the following statements are correct under the assumptions stated, and for the special case in which each event contributes at most one entry per bin and uses the same per-event weight $w_e(\\boldsymbol{\\theta})$ across all bins it contributes to?\n\nA. For a single bin $b$, an unbiased estimator of the variance of the weighted content is $\\sigma_b^2 \\approx \\sum_{e \\in b} w_e(\\boldsymbol{\\theta})^2$.\n\nB. For two bins $a$ and $b$ that can share events, an unbiased estimator of their covariance is $\\mathrm{Cov}(S_a, S_b) \\approx \\sum_{e \\in a \\cap b} w_e(\\boldsymbol{\\theta})^2$, where the sum runs over events that contribute one entry to bin $a$ and one entry to bin $b$.\n\nC. After unit-area normalization $y_b = S_b/T$, the covariance between bins transforms as $\\mathrm{Cov}(y_a, y_b) = \\mathrm{Cov}(S_a, S_b)/T^2$, i.e., all elements are scaled by the same factor $1/T^2$ and no new correlations appear.\n\nD. The Jacobian of the unit-area normalization with respect to the unnormalized bin contents is $\\dfrac{\\partial y_a}{\\partial S_c} = \\dfrac{\\delta_{ac} T - S_a}{T^2}$, and hence the normalized covariance follows $\\mathbf{C}' = \\mathbf{J}\\,\\mathbf{C}\\,\\mathbf{J}^\\top$, where $\\mathbf{C}$ is the covariance of $\\{S_b\\}$, $\\mathbf{J}$ is the Jacobian with elements $J_{ac} = \\partial y_a / \\partial S_c$, and $\\mathbf{C}'$ is the covariance of $\\{y_b\\}$.\n\nE. In a pure shape-only fit where the constraint $\\sum_b y_b = 1$ is enforced, the normalized covariance $\\mathbf{C}'$ is singular due to the normalization mode; a standard remedy is to remove one bin from the fit or to project out the normalization eigenvector before constructing the goodness-of-fit (e.g., chi-square).",
            "solution": "The problem statement is critically validated as scientifically grounded, well-posed, and objective. It describes a standard scenario in high-energy physics data analysis involving weighted Monte Carlo events and the statistical properties of the resulting histograms. The assumptions, such as events being drawn from a Poisson point process, are explicit and appropriate for the derivations requested. The problem is valid.\n\nThe core of the problem relies on the statistical properties of sums over points drawn from a Poisson point process. Let the process be defined over a phase space with intensity measure $\\nu$. The number of events is a Poisson-distributed random variable. For a function $f$ of the event properties, Campbell's theorems state:\n- First moment: $\\mathbb{E}\\left[\\sum_{e} f(e)\\right] = \\int f(e) d\\nu(e)$\n- Variance: $\\mathrm{Var}\\left(\\sum_{e} f(e)\\right) = \\int f(e)^2 d\\nu(e)$\n- Covariance: $\\mathrm{Cov}\\left(\\sum_{e} f(e), \\sum_{e} g(e)\\right) = \\int f(e) g(e) d\\nu(e)$\n\nThe unnormalized content of a bin $b$ is given by $S_b(\\boldsymbol{\\theta}) = \\sum_{e \\in b} w_e(\\boldsymbol{\\theta})$. This can be written as a sum over the entire point process as $S_b = \\sum_e I_b(e) w_e$, where $I_b(e)$ is an indicator function that is $1$ if event $e$ contributes to bin $b$ and $0$ otherwise. The weight $w_e$ is deterministic for a given event $e$.\n\n---\n**Option A Evaluation**\n\nThe statement concerns the variance of the weighted bin content $S_b$. Using the formula for the variance from Campbell's theorem with $f(e) = I_b(e) w_e$:\n$$ \\mathrm{Var}(S_b) = \\mathrm{Var}\\left(\\sum_e I_b(e) w_e\\right) = \\int (I_b(e) w_e)^2 d\\nu(e) = \\int I_b(e) w_e^2 d\\nu(e) $$\nThe integral on the right is the expectation of the sum of squared weights for events falling in bin $b$. Let's define a new quantity, the sum of squared weights, $S_b^{(2)} = \\sum_{e \\in b} w_e^2$. Let's find its expectation using Campbell's first moment theorem:\n$$ \\mathbb{E}[S_b^{(2)}] = \\mathbb{E}\\left[\\sum_e I_b(e) w_e^2\\right] = \\int I_b(e) w_e^2 d\\nu(e) $$\nComparing the two results, we see that $\\mathrm{Var}(S_b) = \\mathbb{E}[S_b^{(2)}]$. This means that the observed sum of squared weights, $S_b^{(2)} = \\sum_{e \\in b} w_e^2$, is an unbiased estimator for the variance of the sum of weights, $\\mathrm{Var}(S_b)$. The notation $\\sigma_b^2$ is standard for this variance. The use of the symbol `\\approx` is a slightly informal way to state that one quantity is an estimator for another, but the underlying statistical statement is correct.\n\n**Verdict: Correct**\n\n---\n**Option B Evaluation**\n\nThis statement addresses the covariance between the contents of two bins, $a$ and $b$. The contents are $S_a = \\sum_e I_a(e) w_e$ and $S_b = \\sum_e I_b(e) w_e$. Using the formula for covariance from Campbell's theorem with $f(e) = I_a(e) w_e$ and $g(e) = I_b(e) w_e$:\n$$ \\mathrm{Cov}(S_a, S_b) = \\int (I_a(e) w_e) (I_b(e) w_e) d\\nu(e) = \\int I_a(e) I_b(e) w_e^2 d\\nu(e) $$\nThe product of indicator functions $I_a(e) I_b(e)$ is equivalent to a single indicator function $I_{a \\cap b}(e)$, which is $1$ if event $e$ contributes to both bin $a$ and bin $b$, and $0$ otherwise. The problem states that a single weight $w_e$ is used for all contributions from an event, which is consistent with this formula. Thus:\n$$ \\mathrm{Cov}(S_a, S_b) = \\int I_{a \\cap b}(e) w_e^2 d\\nu(e) $$\nThis is the true covariance. To find an unbiased estimator, we consider the sum of squared weights over the events that are observed to be in the intersection of both bins, $\\sum_{e \\in a \\cap b} w_e^2$. Its expectation is:\n$$ \\mathbb{E}\\left[\\sum_{e \\in a \\cap b} w_e^2\\right] = \\mathbb{E}\\left[\\sum_e I_{a \\cap b}(e) w_e^2 \\right] = \\int I_{a \\cap b}(e) w_e^2 d\\nu(e) $$\nThis shows that $\\sum_{e \\in a \\cap b} w_e^2$ is an unbiased estimator for $\\mathrm{Cov}(S_a, S_b)$. The text \"where the sum runs over events that contribute one entry to bin $a$ and one entry to bin $b$\" correctly describes the set of events $e \\in a \\cap b$.\n\n**Verdict: Correct**\n\n---\n**Option C Evaluation**\n\nThe statement claims that the covariance transforms by a simple scaling, $\\mathrm{Cov}(y_a, y_b) = \\mathrm{Cov}(S_a, S_b)/T^2$. The normalization is $y_b = S_b / T$, where $T = \\sum_k S_k$. Since $T$ is a sum of random variables ($S_k$), $T$ is itself a random variable. The transformation from $\\{S_k\\}$ to $\\{y_b\\}$ is non-linear.\nFor a function of random variables, uncertainties and covariances are propagated using a first-order Taylor expansion. The general formula for the covariance of two functions $f(\\mathbf{X})$ and $g(\\mathbf{X})$ is $\\mathrm{Cov}(f, g) \\approx \\sum_{i,j} \\frac{\\partial f}{\\partial X_i} \\frac{\\partial g}{\\partial X_j} \\mathrm{Cov}(X_i, X_j)$.\nIn our case, $y_a = S_a/T$ and $y_b=S_b/T$. Let's approximate the covariance using the multivariate delta method:\n$$ \\mathrm{Cov}(y_a, y_b) \\approx \\frac{1}{\\mathbb{E}[T]^2} \\mathrm{Cov}(S_a, S_b) - \\frac{\\mathbb{E}[S_b]}{\\mathbb{E}[T]^3} \\mathrm{Cov}(S_a, T) - \\frac{\\mathbb{E}[S_a]}{\\mathbb{E}[T]^3} \\mathrm{Cov}(S_b, T) + \\frac{\\mathbb{E}[S_a]\\mathbb{E}[S_b]}{\\mathbb{E}[T]^4} \\mathrm{Var}(T) $$\nThis formula clearly contains terms beyond a simple scaling of $\\mathrm{Cov}(S_a, S_b)$. The normalization introduces correlations between all bin pairs because they share the common denominator $T$. The statement that \"no new correlations appear\" is false. Therefore, the simple scaling law is incorrect.\n\n**Verdict: Incorrect**\n\n---\n**Option D Evaluation**\n\nThis statement has two parts. First, the calculation of the Jacobian elements $J_{ac} = \\frac{\\partial y_a}{\\partial S_c}$. The transformation is $y_a = S_a / T$ with $T = \\sum_k S_k$. Using the quotient rule for differentiation:\n$$ \\frac{\\partial y_a}{\\partial S_c} = \\frac{\\frac{\\partial S_a}{\\partial S_c} T - S_a \\frac{\\partial T}{\\partial S_c}}{T^2} $$\nThe partial derivatives are $\\frac{\\partial S_a}{\\partial S_c} = \\delta_{ac}$ (the Kronecker delta) and $\\frac{\\partial T}{\\partial S_c} = \\frac{\\partial}{\\partial S_c} \\sum_k S_k = 1$. Substituting these gives:\n$$ \\frac{\\partial y_a}{\\partial S_c} = \\frac{\\delta_{ac} T - S_a}{T^2} $$\nThis matches the first part of the statement, which is a correct mathematical derivation.\nThe second part states that the normalized covariance matrix $\\mathbf{C}'$ is given by $\\mathbf{C}' = \\mathbf{J}\\,\\mathbf{C}\\,\\mathbf{J}^\\top$. This is the standard formula for the first-order propagation of uncertainties (or error propagation) for a vector transformation $\\mathbf{y} = f(\\mathbf{S})$. It provides a linear approximation to the covariance of the transformed variables $\\mathbf{y}$ based on the covariance of the original variables $\\mathbf{S}$ and the Jacobian of the transformation. This is a fundamental result in statistics. Both parts of the statement are correct.\n\n**Verdict: Correct**\n\n---\n**Option E Evaluation**\n\nThe unit-area normalization enforces the constraint $\\sum_b y_b = \\sum_b (S_b/T) = (1/T) \\sum_b S_b = T/T = 1$. This is a strict linear constraint on the components of the normalized vector $\\mathbf{y} = (y_1, y_2, \\ldots, y_n)$.\nIf a set of random variables has a linear constraint, their covariance matrix is singular. To see this, consider the vector $\\mathbf{v} = (1, 1, \\ldots, 1)^\\top$. Let's left-multiply the covariance matrix $\\mathbf{C}'$ by $\\mathbf{v}^\\top$ and right-multiply by $\\mathbf{v}$:\n$$ \\mathbf{v}^\\top \\mathbf{C}' \\mathbf{v} = \\mathrm{Var}(\\mathbf{v}^\\top \\mathbf{y}) = \\mathrm{Var}\\left(\\sum_b y_b\\right) $$\nSince $\\sum_b y_b = 1$ (a constant), its variance is zero. So, $\\mathrm{Var}(\\sum_b y_b) = 0$. This implies that $\\mathbf{C}'$ is a positive semi-definite matrix, not positive definite, and it has at least one zero eigenvalue, making it singular. The vector $\\mathbf{v}$ is the corresponding eigenvector with eigenvalue $0$, since $(\\mathbf{C}' \\mathbf{v})_i = \\sum_j \\mathrm{Cov}(y_i, y_j) = \\mathrm{Cov}(y_i, \\sum_j y_j) = \\mathrm{Cov}(y_i, 1) = 0$.\nA singular covariance matrix cannot be inverted. This is a problem for constructing a goodness-of-fit statistic like the chi-square, $\\chi^2 = (\\mathbf{y}-\\boldsymbol{\\mu})^\\top (\\mathbf{C}')^{-1} (\\mathbf{y}-\\boldsymbol{\\mu})$, which requires the inverse of the covariance matrix.\nThe proposed remedies are standard statistical practice.\n1. Removing one bin (e.g., bin $n$) from the fit: The remaining $n-1$ variables are typically linearly independent, and the corresponding $(n-1) \\times (n-1)$ sub-matrix of $\\mathbf{C}'$ is invertible.\n2. Projecting out the normalization eigenvector: This involves using a Moore-Penrose pseudoinverse or defining the $\\chi^2$ test on the subspace orthogonal to the null space of $\\mathbf{C}'$.\nBoth the diagnosis (singularity) and the proposed remedies are correct.\n\n**Verdict: Correct**",
            "answer": "$$\\boxed{ABDE}$$"
        }
    ]
}