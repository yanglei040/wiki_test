{
    "hands_on_practices": [
        {
            "introduction": "在模型无关的搜索中，我们常常需要扫描大量的参数空间（例如，质量窗口）来寻找未知的信号。这种“别处张望效应” (look-elsewhere effect) 增加了仅由背景起伏导致显著信号假象的概率。本练习将通过一个典型场景，教你如何使用 Bonferroni 和 Šidák 校正来计算全局 $p$-value 和显著性，这是正确评估发现声明的关键统计技能。",
            "id": "3504747",
            "problem": "在对连续不变质量谱进行模型无关的异常搜索中，执行滑动窗口扫描以识别局部超出，而不预设特定的信号模型。考虑一个扫描，它评估 $K = 100$ 个不相交的窗口，每个窗口都给出一个局部的单边 $p$ 值，用于检验“仅有本底”的原假设与“本底加局部超出”的备择假设。观测到的最小局部 $p$ 值为 $p_{\\min} = 10^{-4}$。假设局部检验经过校准，使得在原假设下，每个窗口的局部 $p$ 值在 $[0,1]$ 上均匀分布，并且不同窗口间的检验是相互独立的。\n\n从 $p$ 值和族错误率（FWER）的基本定义出发，并利用概率的补集法则以及布尔不等式（联合界），推导在以下两种情况下，观测到至少一个像最小局部 $p$ 值一样极端的涨落的全局 $p$ 值：\n\n- Bonferroni 校正，它在任意依赖关系下提供有效的 FWER 控制。\n- Šidák 校正，它在独立性假设下提供精确的 FWER。\n\n然后，对于每种校正，使用标准正态累积分布函数 $\\Phi$ 将每个全局 $p$ 值转换为单边高斯等效显著性 $Z$（以标准差 $\\sigma$ 为单位），通过求解 $Z = \\Phi^{-1}(1 - p_{\\mathrm{global}})$。\n\n将最终的 $Z$ 值四舍五入到四位有效数字并报告。将最终数值结果以上述定义的 $\\sigma$ 为单位表示。在最终的方框答案中不要包含单位。",
            "solution": "用户提供了一个问题陈述，在尝试解决之前必须对其进行验证。\n\n### 第 1 步：提取已知条件\n-   不相交窗口数：$K = 100$\n-   观测到的最小局部单边 $p$ 值：$p_{\\min} = 10^{-4}$\n-   原假设为“仅有本底”。\n-   在原假设下，每个窗口的局部 $p$ 值在 $[0,1]$ 上均匀分布。\n-   不同窗口间的检验是相互独立的。\n-   任务 1：推导 Bonferroni 校正和 Šidák 校正的全局 $p$ 值 $p_{\\mathrm{global}}$。\n-   任务 2：通过关系式 $Z = \\Phi^{-1}(1 - p_{\\mathrm{global}})$ 将每个 $p_{\\mathrm{global}}$ 转换为单边高斯等效显著性 $Z$，其中 $\\Phi$ 是标准正态累积分布函数 (CDF)。\n-   任务 3：将最终的 $Z$ 值四舍五入到四位有效数字。\n\n### 第 2 步：使用提取的已知条件进行验证\n1.  **科学依据**：该问题在统计学及其在高能物理中的应用方面有充分的依据。$p$ 值、“别处观看效应”（look-elsewhere effect）、Bonferroni 校正、Šidák 校正和高斯显著性等概念是该领域模型无关搜索中的标准工具。\n2.  **良态问题**：问题定义清晰，提供了所有必要信息。明确陈述了独立性和原假设下局部 $p$ 值均匀分布的假设，从而可以得到唯一且有意义的解。\n3.  **客观性**：语言精确、量化，没有任何主观或模糊的术语。\n4.  **完整性**：问题是自洽的。它提供了检验次数 $K$、观测到的最小局部 $p$ 值 $p_{\\min}$ 以及必要的统计假设。\n5.  **现实性**：所描述的场景是实验粒子物理中异常搜索的一个简化但现实的表示。所提供的值是合理的。\n6.  **其他缺陷**：该问题既不琐碎也不是同义反复。它要求正确推导和应用已建立的统计公式。它不是比喻性的，并且直接切题。\n\n### 第 3 步：结论与行动\n问题有效。现在开始解题过程。\n\n全局 $p$ 值 $p_{\\mathrm{global}}$ 解决了“别处观看效应”或多重检验问题。它被定义为，在全局原假设（即所有 $K$ 个窗口中都不存在信号）下，观测到至少一个局部 $p$ 值小于或等于实际找到的最小值 $p_{\\min}$ 的概率。设 $P_1, P_2, \\ldots, P_K$ 是代表 $K$ 个窗口局部 $p$ 值的随机变量。全局 $p$ 值是这些局部 $p$ 值的最小值小于或等于 $p_{\\min}$ 的事件的概率：\n$$p_{\\mathrm{global}} = P(\\min_{i=1, \\ldots, K} P_i \\le p_{\\min})$$\n这等价于至少一个局部 $p$ 值小于或等于 $p_{\\min}$ 的事件的并集的概率。设 $A_i$ 为事件 $P_i \\le p_{\\min}$。那么：\n$$p_{\\mathrm{global}} = P\\left(\\bigcup_{i=1}^{K} A_i\\right)$$\n在原假设下，每个 $P_i$ 在 $[0,1]$ 上均匀分布，因此 $P(A_i) = P(P_i \\le p_{\\min}) = p_{\\min}$。\n\n**1. Bonferroni 校正**\n\nBonferroni 校正源于布尔不等式，也称为联合界。对于任意事件集合 $A_1, \\ldots, A_K$，该不等式表明，它们并集的概率不大于它们各自概率的总和：\n$$P\\left(\\bigcup_{i=1}^{K} A_i\\right) \\le \\sum_{i=1}^{K} P(A_i)$$\n这个界限对于事件间的任何依赖结构都成立。将其应用于我们的问题，我们得到全局 $p$ 值的一个上界：\n$$p_{\\mathrm{global}} \\le \\sum_{i=1}^{K} P(P_i \\le p_{\\min}) = \\sum_{i=1}^{K} p_{\\min} = K \\cdot p_{\\min}$$\n经过 Bonferroni 校正的 $p$ 值被定义为这个上界，通常以 $1$ 为上限：\n$$p_{\\mathrm{global, Bonf}} = \\min(1, K \\cdot p_{\\min})$$\n这提供了一个保守的估计，保证了在任何期望水平 $\\alpha$ 下对族错误率（FWER）的控制。\n对于给定的值 $K=100$ 和 $p_{\\min}=10^{-4}$：\n$$p_{\\mathrm{global, Bonf}} = 100 \\times 10^{-4} = 10^{-2} = 0.01$$\n由于 $0.01  1$，最小值函数未起作用。\n\n**2. Šidák 校正**\n\nŠidák 校正是在各项独立检验的假设下提供精确全局 $p$ 值的方法，这一假设在问题中已明确说明。推导从概率的补集法则开始。“至少有一个 $P_i \\le p_{\\min}$”的补集是“所有 $P_i  p_{\\min}$”。\n$$p_{\\mathrm{global}} = P(\\min_{i} P_i \\le p_{\\min}) = 1 - P(\\forall i, P_i  p_{\\min})$$\n因为检验是独立的，所以联合事件的概率是单个事件概率的乘积：\n$$P(\\forall i, P_i  p_{\\min}) = \\prod_{i=1}^{K} P(P_i  p_{\\min})$$\n由于在原假设下每个 $P_i$ 在 $[0,1]$ 上均匀分布，概率 $P(P_i  p_{\\min})$ 为：\n$$P(P_i  p_{\\min}) = 1 - P(P_i \\le p_{\\min}) = 1 - p_{\\min}$$\n将此代回，我们得到独立性假设下的精确全局 $p$ 值，即 Šidák 校正：\n$$p_{\\mathrm{global, Šidák}} = 1 - (1 - p_{\\min})^K$$\n对于给定的值 $K=100$ 和 $p_{\\min}=10^{-4}$：\n$$p_{\\mathrm{global, Šidák}} = 1 - (1 - 10^{-4})^{100} = 1 - (0.9999)^{100}$$\n\n**3. 转换为显著性 $Z$**\n\n显著性 $Z$ 是通过对标准正态累积分布函数 $\\Phi$ 求逆得到的。它是标准正态分布上的一个值，使得从 $Z$ 到无穷大的尾部积分等于 $p$ 值。这由关系式 $p_{\\mathrm{global}} = 1 - \\Phi(Z)$ 给出，可重排为 $Z = \\Phi^{-1}(1 - p_{\\mathrm{global}})$。\n\n对于 Bonferroni 校正：\n$$Z_{\\mathrm{Bonf}} = \\Phi^{-1}(1 - p_{\\mathrm{global, Bonf}}) = \\Phi^{-1}(1 - 0.01) = \\Phi^{-1}(0.99)$$\n使用标准统计表或计算软件，$\\Phi^{-1}(0.99) \\approx 2.32634787$。\n\n对于 Šidák 校正：\n$$Z_{\\mathrm{Šidák}} = \\Phi^{-1}(1 - p_{\\mathrm{global, Šidák}}) = \\Phi^{-1}\\left(1 - \\left[1 - (1 - p_{\\min})^K\\right]\\right) = \\Phi^{-1}\\left((1 - p_{\\min})^K\\right)$$\n$$Z_{\\mathrm{Šidák}} = \\Phi^{-1}\\left((1 - 10^{-4})^{100}\\right) = \\Phi^{-1}\\left((0.9999)^{100}\\right)$$\n数值计算，$(0.9999)^{100} \\approx 0.990049833$。\n$$Z_{\\mathrm{Šidák}} = \\Phi^{-1}(0.990049833) \\approx 2.3283592$$\n\n**4. 最终数值结果**\n\n问题要求将最终的 $Z$ 值四舍五入到四位有效数字。\n-   Bonferroni 显著性：$Z_{\\mathrm{Bonf}} \\approx 2.32634787 \\rightarrow 2.326$。\n-   Šidák 显著性：$Z_{\\mathrm{Šidák}} \\approx 2.3283592 \\rightarrow 2.328$。\n\nBonferroni 校正提供了真实 $p$ 值的一个保守上界，导致 $p$ 值稍大，因此与精确的 Šidák 校正相比，显著性稍小。由于检验的独立性，Šidák 校正在此适用。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.326  2.328\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在许多物理分析中，数据以分箱直方图的形式呈现，我们假设观测到的数据是已知背景和未知异常信号的混合。期望最大化（EM）算法是估计这种混合模型中信号组分比例 $\\epsilon$ 的经典而强大的统计工具。通过这个编码练习，你将亲手实现 EM 算法来从分箱数据中提取微弱的污染组分，并学习如何使用费雪信息（Fisher information）来估计其不确定性。",
            "id": "3504732",
            "problem": "给定一个由高能对撞机搜索聚合的、跨离散区间的事件计数直方图。工作假设是，观测到的分布是一个已知背景形状与一个具有不同形状的可能的小污染（异常）的混合。令事件总数为 $N$，区间索引为 $i \\in \\{1,\\dots,K\\}$，背景模板为 $\\{b_i\\}_{i=1}^K$ 且 $\\sum_{i=1}^K b_i = 1$，异常模板为 $\\{s_i\\}_{i=1}^K$ 且 $\\sum_{i=1}^K s_i = 1$。该混合模型假设一个事件落入区间 $i$ 的概率为\n$$\np_i(\\epsilon) = (1-\\epsilon)\\,b_i + \\epsilon\\,s_i,\n$$\n其中 $\\epsilon \\in [0,1]$ 是待估计的未知污染分数。观测数据为计数 $\\{n_i\\}_{i=1}^K$，其中 $\\sum_{i=1}^K n_i = N$。假设这些计数是参数为 $N$ 和概率为 $\\{p_i(\\epsilon)\\}_{i=1}^K$ 的多项式随机变量的实现。\n\n从多项式似然的定义出发，\n$$\n\\mathcal{L}(\\epsilon) \\propto \\prod_{i=1}^K p_i(\\epsilon)^{n_i},\n$$\n及其对数似然，\n$$\n\\ell(\\epsilon) = \\sum_{i=1}^K n_i \\log p_i(\\epsilon),\n$$\n推导一个期望最大化 (EM; Expectation-Maximization) 算法，该算法使用潜变量公式来估计 $\\epsilon$，其中每个事件被分配一个未观测到的指示符，以表明它来自背景还是污染。利用EM算法最大化关于潜变量当前后验的期望完整数据对数似然这一原理。从第一性原理出发，证明得到的关于 $\\epsilon$ 的不动点更新与在混合模型下最大化观测数据似然是一致的。最后，推导在最大似然估计 $\\hat{\\epsilon}$ 处的 $\\epsilon$ 的观测费雪信息，其定义为\n$$\n\\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon}) \\equiv -\\left.\\frac{\\partial^2 \\ell(\\epsilon)}{\\partial \\epsilon^2}\\right|_{\\epsilon=\\hat{\\epsilon}},\n$$\n并用它来报告不确定度估计 $\\sigma_\\epsilon = \\sqrt{1 / \\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon})}$。如果 $\\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon}) = 0$，则定义 $\\sigma_\\epsilon$ 为 $+\\infty$。\n\n您的任务是编写一个完整的程序，该程序：\n- 实现EM过程来估计 $\\epsilon$，仅使用分箱计数和模板 $\\{b_i\\}$ 与 $\\{s_i\\}$，其中E步的责任（responsibilities）需与分箱数据一致地计算，M步则更新 $\\epsilon$。\n- 迭代直至收敛，对 $\\epsilon$ 使用绝对容差 $\\tau = 10^{-12}$ 或最大迭代次数 $10{,}000$ 次，以先达到的为准。\n- 在收敛的 $\\hat{\\epsilon}$ 处计算观测费雪信息，并报告如上定义的 $\\sigma_\\epsilon$。\n- 为每个测试用例生成一个双元素列表 $[\\hat{\\epsilon}, \\sigma_\\epsilon]$ 作为结果，其中两个条目均为四舍五入到六位小数的小数（无百分号）。\n\n使用以下测试套件，每个套件由 $(\\{n_i\\}, \\{b_i\\}, \\{s_i\\}, \\epsilon_0)$ 指定，其中 $\\epsilon_0$ 是EM迭代的初始值：\n\n- 测试用例 1（理想路径，中度污染，平滑的 $b$ 和有峰值的 $s$）：\n  - $\\{n_i\\} = \\{143, 121, 98, 76, 40, 22\\}$\n  - $\\{b_i\\} = \\{0.30, 0.25, 0.20, 0.15, 0.07, 0.03\\}$\n  - $\\{s_i\\} = \\{0.05, 0.10, 0.15, 0.20, 0.25, 0.25\\}$\n  - $\\epsilon_0 = 0.02$\n\n- 测试用例 2（边界情况，无污染，纯背景计数）：\n  - $\\{n_i\\} = \\{100, 200, 300, 250, 150\\}$\n  - $\\{b_i\\} = \\{0.10, 0.20, 0.30, 0.25, 0.15\\}$\n  - $\\{s_i\\} = \\{0.05, 0.05, 0.10, 0.30, 0.50\\}$\n  - $\\epsilon_0 = 0.01$\n\n- 测试用例 3（低信息量情况，$s$ 接近 $b$）：\n  - $\\{n_i\\} = \\{113, 112, 112, 112, 112, 112, 127\\}$\n  - $\\{b_i\\} = \\{0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.16\\}$\n  - $\\{s_i\\} = \\{0.15, 0.14, 0.14, 0.14, 0.14, 0.14, 0.15\\}$\n  - $\\epsilon_0 = 0.05$\n\n- 测试用例 4（边缘支撑情况，背景有零概率区间）：\n  - $\\{n_i\\} = \\{12, 180, 291, 117\\}$\n  - $\\{b_i\\} = \\{0.00, 0.30, 0.50, 0.20\\}$\n  - $\\{s_i\\} = \\{0.40, 0.30, 0.20, 0.10\\}$\n  - $\\epsilon_0 = 0.02$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的列表形式的结果，每个测试用例的结果本身就是一个双元素列表。例如，输出格式必须为\n$$\n\\big[ [\\hat{\\epsilon}_1,\\sigma_{\\epsilon,1}], [\\hat{\\epsilon}_2,\\sigma_{\\epsilon,2}], [\\hat{\\epsilon}_3,\\sigma_{\\epsilon,3}], [\\hat{\\epsilon}_4,\\sigma_{\\epsilon,4}] \\big],\n$$\n所有数值条目都四舍五入到六位小数，并表示为小数。",
            "solution": "用户提供了一个适定的统计推断问题。该问题具有科学依据，内容完备且客观。唯一解所需的所有必要参数和数据均已提供。因此，该问题是有效的，并将提供解答。\n\n### 理论推导\n\n该问题要求基于观测到的分箱计数 $\\{n_i\\}_{i=1}^K$，为混合模型中的污染分数 $\\epsilon$ 求解一个估计量。一个事件落入区间 $i$ 的概率模型为 $p_i(\\epsilon) = (1-\\epsilon)b_i + \\epsilon s_i$，其中 $\\{b_i\\}$ 和 $\\{s_i\\}$ 分别是已知的背景和异常概率模板。数据 $\\{n_i\\}$ 被假定服从多项式分布，其总计数为 $N = \\sum_{i=1}^K n_i$，概率为 $\\{p_i(\\epsilon)\\}_{i=1}^K$。\n\n#### 1. 期望最大化 (EM) 算法\n\nEM算法是一种在含有潜变量的统计模型中寻找参数的最大似然估计的迭代方法。\n\n**潜变量公式**：对于 $N$ 个独立事件中的每一个，我们引入一个潜指示变量 $Z_j \\in \\{0, 1\\}$，其中 $j=1, \\dots, N$。$Z_j=1$ 表示事件源于异常成分，而 $Z_j=0$ 表示其源于背景。先验概率为 $P(Z_j=1) = \\epsilon$。观测数据是每个事件的区间索引 $X_j \\in \\{1, \\dots, K\\}$。分箱计数为 $n_i = \\sum_{j=1}^N \\mathbb{I}(X_j=i)$，其中 $\\mathbb{I}(\\cdot)$ 是指示函数。\n\n**完整数据对数似然**：完整数据由数据对 $\\{(X_j, Z_j)\\}_{j=1}^N$ 组成。完整数据似然为：\n$$ \\mathcal{L}_{\\text{comp}}(\\epsilon) = \\prod_{j=1}^N P(X_j, Z_j | \\epsilon) = \\prod_{j=1}^N \\left[ (1-\\epsilon) b_{X_j} \\right]^{1-Z_j} \\left[ \\epsilon s_{X_j} \\right]^{Z_j} $$\n相应的对数似然为：\n$$ \\ell_{\\text{comp}}(\\epsilon) = \\sum_{j=1}^N \\left[ (1-Z_j)\\log(1-\\epsilon) + Z_j\\log(\\epsilon) \\right] + \\sum_{j=1}^N \\left[ (1-Z_j)\\log(b_{X_j}) + Z_j\\log(s_{X_j}) \\right] $$\n忽略不依赖于 $\\epsilon$ 的项，我们得到：\n$$ \\ell_{\\text{comp}}(\\epsilon) \\propto \\left(\\sum_{j=1}^N (1-Z_j)\\right) \\log(1-\\epsilon) + \\left(\\sum_{j=1}^N Z_j\\right) \\log(\\epsilon) $$\n令 $N_s = \\sum_{j=1}^N Z_j$ 为异常事件的总（未观测）数。那么 $\\sum_{j=1}^N (1-Z_j) = N - N_s$。\n$$ \\ell_{\\text{comp}}(\\epsilon) \\propto (N - N_s) \\log(1-\\epsilon) + N_s \\log(\\epsilon) $$\n\n**E步（期望）**：在E步中，我们计算完整数据对数似然关于给定观测数据 $X$ 和当前参数估计 $\\epsilon^{(t)}$ 下潜变量 $Z$ 的后验分布的期望。这就是 $Q$ 函数：\n$$ Q(\\epsilon | \\epsilon^{(t)}) = E_{Z|X, \\epsilon^{(t)}}[\\ell_{\\text{comp}}(\\epsilon)] $$\n这简化为计算 $N_s$ 的期望：\n$$ N_s^{(t)} = E[N_s | X, \\epsilon^{(t)}] = E\\left[\\sum_{j=1}^N Z_j \\middle| X, \\epsilon^{(t)}\\right] = \\sum_{j=1}^N E[Z_j | X_j, \\epsilon^{(t)}] $$\n期望 $E[Z_j | X_j, \\epsilon^{(t)}]$ 是在事件 $j$ 被观测到位于区间 $X_j$ 的条件下，该事件为异常事件的后验概率。这就是“责任”（responsibility） $r_{X_j}^{(t)}$：\n$$ r_i^{(t)} \\equiv P(Z_j=1 | X_j=i, \\epsilon^{(t)}) = \\frac{P(X_j=i|Z_j=1)P(Z_j=1|\\epsilon^{(t)})}{P(X_j=i|\\epsilon^{(t)})} = \\frac{s_i \\epsilon^{(t)}}{(1-\\epsilon^{(t)})b_i + \\epsilon^{(t)}s_i} = \\frac{\\epsilon^{(t)} s_i}{p_i(\\epsilon^{(t)})} $$\n异常事件的期望总数是所有观测事件的责任之和，按区间分组后为：\n$$ N_s^{(t)} = \\sum_{i=1}^K n_i r_i^{(t)} $$\n$Q$ 函数则为：\n$$ Q(\\epsilon | \\epsilon^{(t)}) = (N - N_s^{(t)}) \\log(1-\\epsilon) + N_s^{(t)} \\log(\\epsilon) + \\text{constant} $$\n\n**M步（最大化）**：在M步中，我们关于 $\\epsilon$ 最大化 $Q(\\epsilon | \\epsilon^{(t)})$ 以找到下一个估计值 $\\epsilon^{(t+1)}$。\n$$ \\frac{\\partial Q}{\\partial \\epsilon} = -\\frac{N - N_s^{(t)}}{1-\\epsilon} + \\frac{N_s^{(t)}}{\\epsilon} = 0 $$\n求解 $\\epsilon$ 可得：\n$$ \\epsilon^{(t+1)} = \\frac{N_s^{(t)}}{N} = \\frac{1}{N} \\sum_{i=1}^K n_i r_i^{(t)} $$\n代入 $r_i^{(t)}$ 的表达式，得到最终的更新规则：\n$$ \\epsilon^{(t+1)} = \\frac{1}{N} \\sum_{i=1}^K n_i \\frac{\\epsilon^{(t)} s_i}{(1-\\epsilon^{(t)})b_i + \\epsilon^{(t)}s_i} $$\n此迭代过程保证在每一步都不会降低观测数据的对数似然。\n\n#### 2. 与最大似然估计的一致性\n\n观测数据的对数似然（相差一个常数）为：\n$$ \\ell(\\epsilon) = \\sum_{i=1}^K n_i \\log p_i(\\epsilon) = \\sum_{i=1}^K n_i \\log((1-\\epsilon)b_i + \\epsilon s_i) $$\n为了找到最大似然估计 (MLE) $\\hat{\\epsilon}$，我们将其一阶导数（得分函数）设为零：\n$$ \\frac{\\partial \\ell}{\\partial \\epsilon} = \\sum_{i=1}^K n_i \\frac{s_i - b_i}{p_i(\\epsilon)} = 0 $$\n我们可以将此导数重写为另一种形式。由于 $p_i(\\epsilon) = (1-\\epsilon)b_i + \\epsilon s_i$，我们有 $s_i = (p_i(\\epsilon) - (1-\\epsilon)b_i) / \\epsilon$。一个更有用的恒等式是 $s_i - b_i = (s_i - p_i(\\epsilon)) / (1-\\epsilon)$。使用这个恒等式：\n$$ \\frac{\\partial \\ell}{\\partial \\epsilon} = \\sum_{i=1}^K \\frac{n_i}{p_i(\\epsilon)} \\frac{s_i - p_i(\\epsilon)}{1-\\epsilon} = \\frac{1}{1-\\epsilon} \\left( \\sum_{i=1}^K \\frac{n_i s_i}{p_i(\\epsilon)} - \\sum_{i=1}^K \\frac{n_i p_i(\\epsilon)}{p_i(\\epsilon)} \\right) = \\frac{1}{1-\\epsilon} \\left( \\sum_{i=1}^K \\frac{n_i s_i}{p_i(\\epsilon)} - N \\right) $$\n对于 $\\epsilon \\in (0,1)$，将导数设为零意味着 $\\sum_{i=1}^K \\frac{n_i s_i}{p_i(\\hat{\\epsilon})} = N$。\n\n现在，考虑EM更新规则的一个不动点 $\\hat{\\epsilon}$，其中 $\\hat{\\epsilon} = \\epsilon^{(t+1)} = \\epsilon^{(t)}$：\n$$ \\hat{\\epsilon} = \\frac{1}{N} \\sum_{i=1}^K n_i \\frac{\\hat{\\epsilon} s_i}{p_i(\\hat{\\epsilon})} $$\n对于 $\\hat{\\epsilon} \\in (0,1)$，我们可以除以 $\\hat{\\epsilon}$：\n$$ 1 = \\frac{1}{N} \\sum_{i=1}^K \\frac{n_i s_i}{p_i(\\hat{\\epsilon})} \\implies \\sum_{i=1}^K \\frac{n_i s_i}{p_i(\\hat{\\epsilon})} = N $$\n这恰好是观测数据对数似然的临界点条件。因此，EM算法的任何内部不动点都对应于对数似然函数的一个驻点，从而证明了一致性。\n\n#### 3. 观测费雪信息与不确定度\n\n观测费雪信息定义为对数似然的二阶导数在最大似然估计 $\\hat{\\epsilon}$ 处取值的负数：\n$$ \\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon}) = -\\left.\\frac{\\partial^2 \\ell(\\epsilon)}{\\partial \\epsilon^2}\\right|_{\\epsilon=\\hat{\\epsilon}} $$\n我们计算二阶导数：\n$$ \\frac{\\partial^2 \\ell}{\\partial \\epsilon^2} = \\frac{\\partial}{\\partial \\epsilon} \\left( \\sum_{i=1}^K n_i \\frac{s_i - b_i}{p_i(\\epsilon)} \\right) = \\sum_{i=1}^K n_i (s_i - b_i) \\left( -\\frac{1}{p_i(\\epsilon)^2} \\right) \\frac{\\partial p_i(\\epsilon)}{\\partial \\epsilon} $$\n由于 $\\frac{\\partial p_i(\\epsilon)}{\\partial \\epsilon} = s_i - b_i$，我们得到：\n$$ \\frac{\\partial^2 \\ell}{\\partial \\epsilon^2} = - \\sum_{i=1}^K n_i \\frac{(s_i - b_i)^2}{p_i(\\epsilon)^2} $$\n因此，观测费雪信息为：\n$$ \\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon}) = \\sum_{i=1}^K n_i \\frac{(s_i - b_i)^2}{((1-\\hat{\\epsilon})b_i + \\hat{\\epsilon} s_i)^2} $$\n估计值 $\\hat{\\epsilon}$ 的不确定度由克拉默-拉奥下界（Cramér-Rao lower bound）给出，它可以通过观测费雪信息的平方根的倒数来近似：\n$$ \\sigma_\\epsilon = \\sqrt{1 / \\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon})} $$\n如果 $\\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon}) = 0$，这种情况在对于所有 $n_i0$ 的区间都有 $s_i=b_i$ 时发生，那么参数 $\\epsilon$ 是不可辨识的，不确定度 $\\sigma_\\epsilon$ 为无穷大。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the contamination fraction epsilon and its uncertainty using an EM algorithm\n    for a set of test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        (\n            np.array([143, 121, 98, 76, 40, 22]),\n            np.array([0.30, 0.25, 0.20, 0.15, 0.07, 0.03]),\n            np.array([0.05, 0.10, 0.15, 0.20, 0.25, 0.25]),\n            0.02\n        ),\n        # Test case 2\n        (\n            np.array([100, 200, 300, 250, 150]),\n            np.array([0.10, 0.20, 0.30, 0.25, 0.15]),\n            np.array([0.05, 0.05, 0.10, 0.30, 0.50]),\n            0.01\n        ),\n        # Test case 3\n        (\n            np.array([113, 112, 112, 112, 112, 112, 127]),\n            np.array([0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.16]),\n            np.array([0.15, 0.14, 0.14, 0.14, 0.14, 0.14, 0.15]),\n            0.05\n        ),\n        # Test case 4\n        (\n            np.array([12, 180, 291, 117]),\n            np.array([0.00, 0.30, 0.50, 0.20]),\n            np.array([0.40, 0.30, 0.20, 0.10]),\n            0.02\n        ),\n    ]\n\n    TOL = 1e-12\n    MAX_ITER = 10000\n    \n    results_str = []\n\n    for n, b, s, eps0 in test_cases:\n        N = np.sum(n)\n        eps = float(eps0)\n\n        for _ in range(MAX_ITER):\n            eps_old = eps\n            \n            # Mixture probability for each bin\n            p = (1.0 - eps) * b + eps * s\n            \n            # The EM update can be written in a numerically stable form that avoids\n            # explicitly calculating the responsibilities, which can be unstable if eps is small.\n            # eps_new = (1/N) * sum(n_i * r_i) = (1/N) * sum(n_i * eps * s_i / p_i)\n            #           = (eps/N) * sum(n_i * s_i / p_i)\n            \n            # To handle cases where p_i might be zero, we use np.divide to return 0 for 0/0.\n            # This is correct as if p_i=0, n_i must also be 0 for a non-infinite log-likelihood,\n            # so the contribution to the sum is 0.\n            sum_term = np.sum(np.divide(n * s, p, out=np.zeros_like(p), where=p!=0))\n            \n            eps = eps * sum_term / N\n            \n            # Constrain epsilon to the valid range [0, 1] to handle potential floating point inaccuracies.\n            # The update rule theoretically preserves this property.\n            eps = max(0.0, min(1.0, eps))\n            \n            if abs(eps - eps_old)  TOL:\n                break\n        \n        eps_hat = eps\n        \n        # Calculate Observed Fisher Information\n        p_hat = (1.0 - eps_hat) * b + eps_hat * s\n        diff_s_b = s - b\n        \n        # Terms for the Fisher information sum: n_i * ((s_i - b_i) / p_hat_i)^2\n        # Use np.divide for numerical stability, similar to the EM step.\n        numer = n * (diff_s_b**2)\n        denom = p_hat**2\n        fisher_terms = np.divide(numer, denom, out=np.zeros_like(numer), where=denom!=0)\n        \n        fisher_info = np.sum(fisher_terms)\n        \n        sigma_eps = 0.0\n        if fisher_info > 1e-30:  # Use a small threshold to avoid division by zero\n            sigma_eps = (1.0 / fisher_info)**0.5\n        else:\n            sigma_eps = float('inf')\n\n        results_str.append(f\"[{eps_hat:.6f},{sigma_eps:.6f}]\")\n\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}