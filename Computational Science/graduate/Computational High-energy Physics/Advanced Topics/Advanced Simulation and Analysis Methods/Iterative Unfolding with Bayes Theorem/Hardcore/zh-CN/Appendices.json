{
    "hands_on_practices": [
        {
            "introduction": "任何迭代算法的第一步都为所有后续计算奠定了基础。在第一个练习中，我们将剖析贝叶斯展开过程的初始迭代。通过从一个简单的、无信息的“平坦”先验开始，我们将直接从贝叶斯定理推导出更新规则，并揭示其作为一个对测量数据进行“归一化反向投影”的优雅解释。这项实践不仅巩固了基础数学，而且直接展示了这种迭代方法相对于朴素且不稳定的直接矩阵求逆法的优越性。",
            "id": "3518211",
            "problem": "考虑一个计算高能物理中的简化探测器模型，该模型具有三个由 $i \\in \\{1,2,3\\}$ 索引的真实仓和三个由 $j \\in \\{1,2,3\\}$ 索引的测量仓。探测器响应由响应矩阵 $A_{ji}$ 描述，其中 $A_{ji}$ 是条件概率 $P(j \\mid i)$，表示源于真实仓 $i$ 的事件在测量仓 $j$ 中被重建的概率。设测量到的计数为向量 $m = (m_{1}, m_{2}, m_{3})$，未知的真实计数为 $n = (n_{1}, n_{2}, n_{3})$。假设接受度效应被编码在 $A_{ji}$ 的列中（因此列和可以小于1），并忽略任何接受度外的仓。\n\n仅从贝叶斯定理和定义 $A_{ji} = P(j \\mid i)$ 出发，推导在平坦先验 $n_{i}^{(0)} = c$（其中 $c > 0$ 对所有 $i$ 均为常数）下的第一个贝叶斯迭代展开估计 $n_{i}^{(1)}$ 的表达式。证明这第一次迭代可以解释为通过 $A_{ji}$ 对测量计数进行的归一化反投影。\n\n然后，对于具有以下矩阵和向量的三仓特例\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0.70  0.10  0.05 \\\\\n0.20  0.70  0.20 \\\\\n0.05  0.15  0.60\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nm \\;=\\; \\begin{pmatrix} 1200 \\\\ 800 \\\\ 500 \\end{pmatrix},\n$$\n计算：\n1. 使用平坦先验 $n_{i}^{(0)} = c$，从你基于贝叶斯定理的推导中获得的第一代迭代估计 $n^{(1)}$。\n2. 通过求解 $A\\,n^{\\mathrm{MI}} = m$ 定义的矩阵求逆展开估计 $n^{\\mathrm{MI}}$。\n\n最后，使用相对欧几里得范数\n$$\nr \\;=\\; \\frac{\\|\\,n^{(1)} - n^{\\mathrm{MI}}\\,\\|_{2}}{\\|\\,n^{\\mathrm{MI}}\\,\\|_{2}}.\n$$\n量化第一次迭代结果与矩阵求逆结果之间的偏差。将 $r$ 的最终答案表示为无单位的小数，并四舍五入到四位有效数字。",
            "solution": "问题陈述已经过验证，被认为是可靠、适定且有科学依据的。它提出了一个计算高能物理中标准的、尽管是简化的-问题，可以使用已建立的数学方法严格解决。\n\n该问题要求三个主要部分：一个推导，两个估计量的数值计算，以及它们之间的比较。\n\n**第1部分：第一次迭代公式的推导**\n\n目标是从贝叶斯定理和给定定义出发，推导第一次迭代展开估计 $n_i^{(1)}$ 的表达式。\n\n贝叶斯定理将给定一个效应（在测量仓 $j$ 中的事件）下某个原因（在真实仓 $i$ 中的事件）的后验概率，与给定原因下效应的条件概率以及原因的先验概率联系起来。形式上，\n$$P(i \\mid j) = \\frac{P(j \\mid i) P(i)}{P(j)}$$\n其中：\n- $P(i \\mid j)$ 是在测量仓 $j$ 中测到的事件源于真实仓 $i$ 的后验概率。\n- $P(j \\mid i)$ 是给定事件源于真实仓 $i$ 时，在测量仓 $j$ 中测到该事件的条件概率。这由响应矩阵元素 $A_{ji}$ 给出。\n- $P(i)$ 是事件源于真实仓 $i$ 的先验概率。\n- $P(j)$ 是在测量仓 $j$ 中测到事件的总概率，它作为一个归一化常数。它通过对所有可能的原因进行边缘化得到：$P(j) = \\sum_k P(j \\mid k) P(k)$。\n\n真实仓 $i$ 中展开后的事件数，我们记为 $n_i'$，可以通过考虑测量计数 $m_j$ 来估计。对于每个测量仓 $j$，计数为 $m_j$。我们可以根据后验概率 $P(i \\mid j)$ 将这些计数重新分配回真实仓。对所有测量仓的贡献求和，得到 $n_i'$ 的估计值：\n$$n_i' = \\sum_j m_j P(i \\mid j)$$\n该公式假定展开后的事件总数等于测量的事件总数，这是在没有单独步骤明确校正效率时，此特定贝叶斯更新步骤的一个特点。\n\n在迭代贝叶斯展开方案中，给定迭代的先验概率 $P(i)$ 由前一次迭代的结果确定。对于第一次迭代 $n_i^{(1)}$，先验概率 $P_0(i)$ 基于初始猜测 $n_i^{(0)}$。问题指定了一个平坦先验，$n_i^{(0)} = c$，其中常数 $c>0$，对所有 $i \\in \\{1, 2, 3\\}$ 成立。这意味着一个均匀的先验概率分布：\n$$P_0(i) = \\frac{n_i^{(0)}}{\\sum_k n_k^{(0)}} = \\frac{c}{3c} = \\frac{1}{3}$$\n先验概率 $P_0(i)$ 对所有 $i$ 都是常数。我们用 $K$ 表示这个常数。\n\n现在，我们可以计算这次迭代中贝叶斯定理的各项：\n$P(j \\mid i) = A_{ji}$\n$P_0(i) = K$\n$P(j) = \\sum_k P(j \\mid k) P_0(k) = \\sum_k A_{jk} K = K \\sum_k A_{jk}$\n\n将这些代入后验概率 $P(i \\mid j)$ 的表达式中：\n$$P(i \\mid j) = \\frac{A_{ji} K}{K \\sum_k A_{jk}} = \\frac{A_{ji}}{\\sum_k A_{jk}}$$\n然后，通过将此后验概率代回重新分配公式，找到第一次迭代的估计值 $n_i^{(1)}$：\n$$n_i^{(1)} = \\sum_j m_j \\frac{A_{ji}}{\\sum_k A_{jk}}$$\n这个 $n_i^{(1)}$ 的表达式与平坦先验中的常数 $c$ 无关，这是一个定义良好的程序所要求的。\n\n**解释为归一化反投影**\n\n推导出的公式 $n_i^{(1)} = \\sum_j m_j \\frac{A_{ji}}{\\sum_k A_{jk}}$ 可以解释为归一化反投影。\n- 项 $A_{ji}$ 对应于转置响应矩阵 $(A^T)_{ij}$ 的一个元素。将测量计数向量 $m$ 乘以 $A^T$ 被称为“反投影”。项 $m_j A_{ji}$ 表示在仓 $j$ 中的测量对真实仓 $i$ 的反投影贡献。\n- 分母 $\\sum_k A_{jk}$ 是矩阵 $A$ 的第 $j$ 行之和。如推导所示，在平坦先验 $P_0(i)$ 的特定假设下，此项作为将正向条件概率 $P(j \\mid i) = A_{ji}$ 转换为反向条件概率 $P(i \\mid j)$ 所需的归一化因子。\n- 因此，总和中的每一项 $m_j \\frac{A_{ji}}{\\sum_k A_{jk}}$ 代表了在仓 $j$ 中测量的计数，根据在 $j$ 中看到的事件源于 $i$ 的归一化概率，重新分配给真实仓 $i$。对所有测量仓 $j$ 求和，提供了真实仓 $i$ 的总估计计数。\n\n**第2部分：数值计算**\n\n给定的数据是：\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0.70  0.10  0.05 \\\\\n0.20  0.70  0.20 \\\\\n0.05  0.15  0.60\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nm \\;=\\; \\begin{pmatrix} 1200 \\\\ 800 \\\\ 500 \\end{pmatrix}\n$$\n\n**1. 计算第一次迭代估计 $n^{(1)}$**\n\n首先，我们计算 $A$ 的行和，$R_j = \\sum_k A_{jk}$：\n$R_1 = 0.70 + 0.10 + 0.05 = 0.85$\n$R_2 = 0.20 + 0.70 + 0.20 = 1.10$\n$R_3 = 0.05 + 0.15 + 0.60 = 0.80$\n\n现在，我们应用 $n_i^{(1)}$ 的公式：\n$n_1^{(1)} = \\sum_j m_j \\frac{A_{j1}}{R_j} = m_1 \\frac{A_{11}}{R_1} + m_2 \\frac{A_{21}}{R_2} + m_3 \\frac{A_{31}}{R_3}$\n$n_1^{(1)} = (1200) \\frac{0.70}{0.85} + (800) \\frac{0.20}{1.10} + (500) \\frac{0.05}{0.80} \\approx 988.2353 + 145.4545 + 31.25 = 1164.9398$\n\n$n_2^{(1)} = \\sum_j m_j \\frac{A_{j2}}{R_j} = m_1 \\frac{A_{12}}{R_1} + m_2 \\frac{A_{22}}{R_2} + m_3 \\frac{A_{32}}{R_3}$\n$n_2^{(1)} = (1200) \\frac{0.10}{0.85} + (800) \\frac{0.70}{1.10} + (500) \\frac{0.15}{0.80} \\approx 141.1765 + 509.0909 + 93.75 = 744.0174$\n\n$n_3^{(1)} = \\sum_j m_j \\frac{A_{j3}}{R_j} = m_1 \\frac{A_{13}}{R_1} + m_2 \\frac{A_{23}}{R_2} + m_3 \\frac{A_{33}}{R_3}$\n$n_3^{(1)} = (1200) \\frac{0.05}{0.85} + (800) \\frac{0.20}{1.10} + (500) \\frac{0.60}{0.80} \\approx 70.5882 + 145.4545 + 375 = 591.0427$\n\n所以，第一次迭代的估计值为 $n^{(1)} \\approx (1164.94, 744.02, 591.04)$。\n\n**2. 计算矩阵求逆估计 $n^{\\mathrm{MI}}$**\n\n矩阵求逆估计是线性系统 $A n^{\\mathrm{MI}} = m$ 的解，即 $n^{\\mathrm{MI}} = A^{-1} m$。\n首先，我们求 $A$ 的行列式：\n$\\det(A) = 0.70(0.70 \\cdot 0.60 - 0.20 \\cdot 0.15) - 0.10(0.20 \\cdot 0.60 - 0.20 \\cdot 0.05) + 0.05(0.20 \\cdot 0.15 - 0.70 \\cdot 0.05)$\n$\\det(A) = 0.70(0.39) - 0.10(0.11) + 0.05(-0.005) = 0.273 - 0.011 - 0.00025 = 0.26175$。\n由于 $\\det(A) \\neq 0$，该矩阵是可逆的。其逆矩阵为 $A^{-1} = \\frac{1}{\\det(A)}\\text{adj}(A)$，其中 $\\text{adj}(A)$ 是 $A$ 的伴随矩阵。\n$A$ 的伴随矩阵是其代数余子式矩阵的转置：\n$\\text{adj}(A) = \n\\begin{pmatrix}\n0.39  -0.0525  -0.015 \\\\\n-0.11  0.4175  -0.13 \\\\\n-0.005  -0.10  0.47\n\\end{pmatrix}$\n\n现在我们计算 $n^{\\mathrm{MI}} = A^{-1}m$：\n$n^{\\mathrm{MI}} = \\frac{1}{0.26175}\n\\begin{pmatrix}\n0.39  -0.0525  -0.015 \\\\\n-0.11  0.4175  -0.13 \\\\\n-0.005  -0.10  0.47\n\\end{pmatrix}\n\\begin{pmatrix}\n1200 \\\\\n800 \\\\\n500\n\\end{pmatrix}$\n$n^{\\mathrm{MI}} = \\frac{1}{0.26175}\n\\begin{pmatrix}\n0.39(1200) - 0.0525(800) - 0.015(500) \\\\\n-0.11(1200) + 0.4175(800) - 0.13(500) \\\\\n-0.005(1200) - 0.10(800) + 0.47(500)\n\\end{pmatrix}\n= \\frac{1}{0.26175}\n\\begin{pmatrix}\n468 - 42 - 7.5 \\\\\n-132 + 334 - 65 \\\\\n-6 - 80 + 235\n\\end{pmatrix}\n= \\frac{1}{0.26175}\n\\begin{pmatrix}\n418.5 \\\\\n137 \\\\\n149\n\\end{pmatrix}$\n$n^{\\mathrm{MI}} \\approx \n\\begin{pmatrix}\n1598.8548 \\\\\n523.4986 \\\\\n569.2593\n\\end{pmatrix}$\n\n**第3部分：量化偏差**\n\n最后，我们计算相对欧几里得范数 $r = \\frac{\\|\\,n^{(1)} - n^{\\mathrm{MI}}\\,\\|_{2}}{\\|\\,n^{\\mathrm{MI}}\\,\\|_{2}}$。\n首先，差向量 $d = n^{(1)} - n^{\\mathrm{MI}}$：\n$d \\approx (1164.9398 - 1598.8548, 744.0174 - 523.4986, 591.0427 - 569.2593)$\n$d \\approx (-433.9150, 220.5188, 21.7834)$\n\n接下来，计算欧几里得范数：\n$\\|\\,d\\,\\|_{2} = \\sqrt{(-433.9150)^2 + (220.5188)^2 + (21.7834)^2} \\approx \\sqrt{188282.1 + 48628.5 + 474.5} = \\sqrt{237385.1} \\approx 487.2218$\n$\\|\\,n^{\\mathrm{MI}}\\,\\|_{2} = \\sqrt{(1598.8548)^2 + (523.4986)^2 + (569.2593)^2} \\approx \\sqrt{2556336.8 + 274050.8 + 324056.2} = \\sqrt{3154443.8} \\approx 1776.0754$\n\n相对范数 $r$ 为：\n$r = \\frac{487.2218}{1776.0754} \\approx 0.2743256$\n\n四舍五入到四位有效数字，我们得到 $r \\approx 0.2743$。",
            "answer": "$$\\boxed{0.2743}$$"
        },
        {
            "introduction": "贝叶斯方法的威力通常在于对先验的审慎选择，在展开问题中尤其如此，因为先验在此扮演着至关重要的正则化角色。本练习将深入探讨如何为泊松分布的计数数据选择一个有原则的、“客观”的先验。我们将推导杰弗里斯先验（Jeffreys prior），并了解它如何为迭代提供一个稳健的起点，从而防止低统计量区间的估计值被错误地锁定为零。",
            "id": "3518183",
            "problem": "在计算高能物理（HEP）中，展开（unfolding）旨在从记录在重建箱（reconstructed bins）中的测量值（用 $R_j$ 表示）推断一个离散为多个箱（bins）的真实量（true quantity）的分布，这些真实箱用 $T_i$ 表示，其中 $i$ 属于一个有限的箱索引集。 在迭代贝叶斯展开（IBU）中，人们利用贝叶斯定理和探测器响应来更新对真实箱均值的初始先验。考虑一个双箱（$i \\in \\{1,2\\}$）的真实分布和一个双箱（$j \\in \\{1,2\\}$）的重建分布。探测器响应概率为 $A_{j i} \\equiv P(R_j \\mid T_i)$，假设已知，并由 $A_{11} = 0.8$，$A_{12} = 0.2$，$A_{21} = 0.2$，$A_{22} = 0.8$ 给出。观测到的重建计数为 $m_1 = 0$ 和 $m_2 = 3$。\n\n(a) 从第一性原理出发，使用 Jeffreys 先验的定义 $\\pi(\\theta) \\propto \\sqrt{I(\\theta)}$（其中 $I(\\theta)$ 是 Fisher 信息）以及给定均值 $\\mu$ 时观测计数为 $k$ 的泊松似然，推导箱 $i$ 中泊松均值 $\\mu_i$ 的 Jeffreys 先验，并证明 $\\pi(\\mu_i) \\propto \\mu_i^{-1/2}$。\n\n(b) 基于(a)部分的结果，解释在初始化 IBU 时，如何将此先验信息编码为每个真实箱的初始伪计数 $n_i^{(0)}$，并分析此选择在低统计量情况下的影响（例如，当某个 $j$ 的 $m_j = 0$ 时）。您的解释应基于 Jeffreys 先验与泊松模型的共轭贝叶斯更新之间的联系，以及贝叶斯定理和全概率定律，而非启发式规则。\n\n(c) 在没有额外形状信息（因此真实箱成员的初始先验是对称的）的两个真实箱的情况下，使用(b)部分的结论，执行第一次 IBU 更新，以计算真实箱 $i=1$ 的展开估计值 $n_1^{(1)}$。您可以将真实箱上的初始先验概率视为与由 Jeffreys 先验产生的初始伪计数 $n_i^{(0)}$ 成正比，并在各箱之间进行归一化。报告 $n_1^{(1)}$ 的精确值，不进行四舍五入，也不带单位。",
            "solution": "这个问题是有效的，因为它科学地基于贝叶斯统计学的原理及其在高能物理数据分析中的应用，问题陈述清晰，给出的条件完整一致，并以客观、正式的语言表达。我们可以开始解答。\n\n这个问题分为三个部分。我们将按顺序解答，每个部分的解答都建立在前一部分结果的基础上。\n\n**(a) 泊松均值的 Jeffreys 先验推导**\n\n我们的任务是推导泊松分布均值 $\\mu_i$ 的 Jeffreys 先验。参数 $\\theta$ 的 Jeffreys 先验定义为 $\\pi(\\theta) \\propto \\sqrt{I(\\theta)}$，其中 $I(\\theta)$ 是 Fisher 信息。\n\n让我们考虑单个箱 $i$，为简单起见，将其均值表示为 $\\mu$。在给定均值 $\\mu$ 的情况下，在该箱中观测到 $k$ 个计数的概率遵循泊松分布。其似然函数为：\n$$L(\\mu \\mid k) = P(k \\mid \\mu) = \\frac{\\mu^k \\exp(-\\mu)}{k!}$$\n似然的自然对数，即对数似然，是：\n$$\\ln L(\\mu \\mid k) = k \\ln \\mu - \\mu - \\ln(k!)$$\nFisher 信息 $I(\\mu)$ 定义为对数似然函数关于参数 $\\mu$ 的二阶导数的期望值的负数。首先，我们计算一阶和二阶导数：\n$$\\frac{\\partial}{\\partial \\mu} \\ln L(\\mu \\mid k) = \\frac{k}{\\mu} - 1$$\n$$\\frac{\\partial^2}{\\partial \\mu^2} \\ln L(\\mu \\mid k) = -\\frac{k}{\\mu^2}$$\n于是，Fisher 信息为：\n$$I(\\mu) = -E\\left[\\frac{\\partial^2}{\\partial \\mu^2} \\ln L(\\mu \\mid k)\\right]$$\n期望是关于数据 $k$ 的分布（即均值为 $\\mu$ 的泊松分布）计算的。因此，$E[k] = \\mu$。将此代入 $I(\\mu)$ 的表达式中：\n$$I(\\mu) = -E\\left[-\\frac{k}{\\mu^2}\\right] = \\frac{E[k]}{\\mu^2} = \\frac{\\mu}{\\mu^2} = \\frac{1}{\\mu}$$\nJeffreys 先验 $\\pi(\\mu)$ 与 Fisher 信息的平方根成正比：\n$$\\pi(\\mu) \\propto \\sqrt{I(\\mu)} = \\sqrt{\\frac{1}{\\mu}} = \\mu^{-1/2}$$\n这个推导对任何箱 $i$ 的均值 $\\mu_i$ 都成立。因此，我们证明了 $\\pi(\\mu_i) \\propto \\mu_i^{-1/2}$。\n\n**(b) 将 Jeffreys 先验解释为初始伪计数**\n\n为了理解如何编码此先验信息，我们考察泊松模型的共轭贝叶斯更新框架。泊松似然的共轭先验是伽马（Gamma）分布。均值 $\\mu$ 的伽马分布（形状参数为 $\\alpha$，率参数为 $\\beta$）由下式给出：\n$$\\pi(\\mu \\mid \\alpha, \\beta) \\propto \\mu^{\\alpha-1} \\exp(-\\beta\\mu)$$\n在(a)部分推导出的 Jeffreys 先验 $\\pi(\\mu) \\propto \\mu^{-1/2}$，可以看作是参数为 $\\alpha = 1/2$ 和 $\\beta=0$ 的伽马分布。这是一个非正常先验（improper prior），因为它在 $\\mu \\in [0, \\infty)$ 上的积分是发散的。\n\n当我们用泊松过程中的 $k$ 个计数观测值来更新此先验时，$\\mu$ 的后验分布也是一个伽马分布。后验分布与似然和先验的乘积成正比：\n$$P(\\mu \\mid k) \\propto L(k \\mid \\mu) \\pi(\\mu) \\propto \\left(\\mu^k \\exp(-\\mu)\\right) \\left(\\mu^{\\alpha-1} \\exp(-\\beta\\mu)\\right) = \\mu^{k+\\alpha-1} \\exp(-(\\beta+1)\\mu)$$\n对于 Jeffreys 先验（$\\alpha=1/2, \\beta=0$），后验分布为：\n$$P(\\mu \\mid k) \\propto \\mu^{k+1/2-1} \\exp(-\\mu)$$\n这是一个正常（proper）的伽马分布，具体为 $\\text{Gamma}(k+1/2, 1)$。该后验分布的均值为 $E[\\mu \\mid k] = \\frac{k+1/2}{1} = k + 1/2$。\n\n这一结果提供了一个清晰的解释：从 Jeffreys 先验开始，等价于通过将观测到的计数 $k$ 加上 $1/2$ 来获得均值的后验估计。这表明 Jeffreys 先验可以编码为每个真实箱 $i$ 的初始“伪计数”$n_i^{(0)} = 1/2$。这在考虑任何数据之前，作为迭代展开过程的起点。\n\n这对低统计量情况具有重要意义。迭代贝叶斯展开（IBU）算法在每次迭代 $k$ 中更新真实箱计数的估计值 $n_i^{(k)}$。一个常见的更新规则是 $n_i^{(k+1)} = \\sum_j m_j P^{(k)}(T_i \\mid R_j)$，其中 $P^{(k)}(T_i \\mid R_j)$ 依赖于上一步的先验概率 $P^{(k)}(T_i) \\propto n_i^{(k)}$。如果我们对某个箱 $i$ 初始化为 $n_i^{(0)} = 0$，那么它的先验概率 $P^{(0)}(T_i)$ 将为零，因此，无论观测数据如何，其展开估计值 $n_i^{(k)}$ 在所有后续迭代中都将保持为零。这是一个不希望出现的性质，因为它过早地排除了一个真实箱作为观测效应可能原因的可能性。\n\n通过为所有 $i$ 选择 $n_i^{(0)} = 1/2$，我们确保每个真实箱都以一个非零计数开始。这对问题进行了正则化，防止任何箱的估计值被永久固定为零。即使某个重建箱 $j$ 的观测计数为零（$m_j = 0$），那些可能对其有贡献的真实箱也不会被排除在对其他观测箱贡献的考虑之外。这种初始化保证了初始先验 $P^{(0)}(T_i)$ 是良定义且非零的，从而允许算法根据响应概率将观测到的计数 $m_j$ 分配给所有可能的真实原因。\n\n**(c) 第一次展开迭代**\n\n我们被要求执行第一次 IBU 更新，以计算真实箱 $i=1$ 的展开估计值 $n_1^{(1)}$。\n基于(b)部分的结果，以及没有额外形状信息（意味着对称性）的条件，我们将两个真实箱的伪计数初始化为：\n$$n_1^{(0)} = \\frac{1}{2}, \\quad n_2^{(0)} = \\frac{1}{2}$$\n初始伪计数总数为 $N^{(0)} = n_1^{(0)} + n_2^{(0)} = 1/2 + 1/2 = 1$。对真实箱的初始先验概率进行归一化：\n$$P^{(0)}(T_1) = \\frac{n_1^{(0)}}{N^{(0)}} = \\frac{1/2}{1} = \\frac{1}{2}$$\n$$P^{(0)}(T_2) = \\frac{n_2^{(0)}}{N^{(0)}} = \\frac{1/2}{1} = \\frac{1}{2}$$\n第一次迭代的 IBU 更新公式为：\n$$n_i^{(1)} = \\sum_{j=1}^{2} m_j P(T_i \\mid R_j)$$\n其中后验概率 $P(T_i \\mid R_j)$ 使用贝叶斯定理和初始先验 $P^{(0)}(T_i)$ 计算得出：\n$$P(T_i \\mid R_j) = \\frac{P(R_j \\mid T_i) P^{(0)}(T_i)}{\\sum_{l=1}^{2} P(R_j \\mid T_l) P^{(0)}(T_l)} = \\frac{A_{ji} P^{(0)}(T_i)}{\\sum_{l=1}^{2} A_{jl} P^{(0)}(T_l)}$$\n我们需要计算 $n_1^{(1)}$：\n$$n_1^{(1)} = m_1 P(T_1 \\mid R_1) + m_2 P(T_1 \\mid R_2)$$\n代入后验概率的公式：\n$$n_1^{(1)} = m_1 \\frac{A_{11} P^{(0)}(T_1)}{A_{11} P^{(0)}(T_1) + A_{12} P^{(0)}(T_2)} + m_2 \\frac{A_{21} P^{(0)}(T_1)}{A_{21} P^{(0)}(T_1) + A_{22} P^{(0)}(T_2)}$$\n我们已知以下值：\n\\begin{itemize}\n    \\item 观测计数：$m_1 = 0$, $m_2 = 3$\n    \\item 响应矩阵元素：$A_{11} = 0.8$, $A_{12} = 0.2$, $A_{21} = 0.2$, $A_{22} = 0.8$\n    \\item 初始先验：$P^{(0)}(T_1) = 1/2$, $P^{(0)}(T_2) = 1/2$\n\\end{itemize}\n$n_1^{(1)}$ 表达式中的第一项为零，因为 $m_1 = 0$。我们只需要计算第二项：\n$$n_1^{(1)} = 0 + 3 \\times \\frac{(0.2) \\times (1/2)}{(0.2) \\times (1/2) + (0.8) \\times (1/2)}$$\n简化分数内的表达式：\n$$n_1^{(1)} = 3 \\times \\frac{0.1}{0.1 + 0.4} = 3 \\times \\frac{0.1}{0.5}$$\n$$n_1^{(1)} = 3 \\times \\frac{1}{5} = \\frac{3}{5}$$\n转换为小数形式，结果是 $0.6$。问题要求的是精确值。",
            "answer": "$$\\boxed{\\frac{3}{5}}$$"
        },
        {
            "introduction": "迭代展开算法虽然功能强大，但它并非一个孤立的方法，而是一个更普适的统计学原理的具体应用。这最后一个综合性练习将贝叶斯展开与基础的期望最大化（Expectation-Maximization, EM）算法联系起来。通过在一个含潜变量的框架中重新表述问题，你将从最大似然和最大后验估计的第一性原理出发，推导出展开的更新规则，然后通过编程实现该算法，观察其在各种具有挑战性的真实场景中的表现。",
            "id": "3518215",
            "problem": "给定一个用于计算高能物理中分箱探测器观测的正演模型。设有$I$个真实分布箱（truth bin），索引为$i \\in \\{1,\\dots,I\\}$，以及$J$个探测器箱（detector bin），索引为$j \\in \\{1,\\dots,J\\}$。探测器响应被编码在一个已知的响应矩阵中，其元素为$R_{ji}$。对于每个固定的$i$，列向量$\\{R_{ji}\\}_{j=1}^J$是关于$j$的概率分布（即，其值为非负且总和为$1$）。设真实分布为$\\boldsymbol{\\theta} = (\\theta_1,\\dots,\\theta_I)$，其中$\\theta_i \\ge 0$且$\\sum_{i=1}^I \\theta_i = 1$。可观测的探测器箱概率为$p_j = \\sum_{i=1}^I R_{ji}\\,\\theta_i$。给定总事件数$N = \\sum_{j=1}^J y_j$，观测到的计数$\\mathbf{y} = (y_1,\\dots,y_J)$被建模为来自多项式分布Multinomial$(N,\\mathbf{p})$的抽样，其中$\\mathbf{p}=(p_1,\\dots,p_J)$。\n\n从以下基本依据出发：\n- 用于条件概率的贝叶斯定理。\n- 多项式似然及其对数。\n- 关于$\\boldsymbol{\\theta}$的狄利克雷先验密度，其超参数为$\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_I)$，其中$\\alpha_i > 0$。\n- 用于与上述多项式混合结构一致的潜在分配的期望最大化（EM）方法。\n\n你的任务是：\n1) 仅使用上述基本依据，推导出一个有原则的迭代方案来估计$\\boldsymbol{\\theta}$，具体包括：\n   - 当不使用先验信息时，用于$\\boldsymbol{\\theta}$的基于计数的最大似然EM迭代。\n   - 当使用带有超参数$\\boldsymbol{\\alpha}$的狄利克雷先验时，用于$\\boldsymbol{\\theta}$的最大后验估计的广义迭代贝叶斯更新。\n   你的推导必须通过贝叶斯定理证明的期望来消除任何潜在分配，并且必须在每次迭代中强制执行归一化约束$\\sum_i \\theta_i = 1$。\n2) 证明当对所有$i$都有$\\alpha_i = 1$时，广义迭代贝叶斯更新简化为基于计数的EM更新。\n3) 实现这两种迭代过程，并在提供的测试套件上比较它们的数值行为。除非另有明确说明，否则对所有$i$使用均匀初始化$\\theta_i^{(0)} = 1/I$。\n\n测试套件（每个测试案例指定$(R,\\mathbf{y},\\boldsymbol{\\alpha},\\boldsymbol{\\theta}^\\star, T)$；所有数值条目均为实数，所有计数均为整数；迭代次数为$T$）：\n\n- 案例A（方形、良态、高统计量、无信息先验）：\n  - $I=3$, $J=3$.\n  - $R$的列向量：\n    - $i=1$: $[0.8, 0.15, 0.05]$,\n    - $i=2$: $[0.1, 0.8, 0.1]$,\n    - $i=3$: $[0.05, 0.1, 0.85]$.\n    组装$R$，使得$R_{j i}$是上述第$i$列的第$j$个条目。\n  - $\\boldsymbol{\\theta}^\\star = [0.2, 0.5, 0.3]$。\n  - $N = 1000$, $\\mathbf{y} = [225, 460, 315]$。\n  - $\\boldsymbol{\\alpha} = [1, 1, 1]$。\n  - $T = 50$.\n\n- 案例B（方形、病态、低统计量、有信息但错误指定的先验）：\n  - $I=5$, $J=5$.\n  - $R$的列向量：\n    - $i=1$: $[0.7, 0.2, 0.1, 0.0, 0.0]$,\n    - $i=2$: $[0.2, 0.5, 0.25, 0.05, 0.0]$,\n    - $i=3$: $[0.1, 0.25, 0.4, 0.2, 0.05]$,\n    - $i=4$: $[0.0, 0.05, 0.2, 0.5, 0.25]$,\n    - $i=5$: $[0.0, 0.0, 0.05, 0.25, 0.7]$.\n  - $\\boldsymbol{\\theta}^\\star = [0.05, 0.15, 0.6, 0.15, 0.05]$。\n  - $N = 50$, $\\mathbf{y} = [6, 12, 16, 11, 5]$。\n  - $\\boldsymbol{\\alpha} = [5, 1, 1, 1, 1]$。\n  - $T = 200$.\n\n- 案例C（矩形、中等统计量、轻度有信息的近均匀先验）：\n  - $I=5$, $J=4$.\n  - $R$的列向量：\n    - $i=1$: $[0.85, 0.10, 0.05, 0.0]$,\n    - $i=2$: $[0.10, 0.70, 0.15, 0.05]$,\n    - $i=3$: $[0.05, 0.15, 0.65, 0.15]$,\n    - $i=4$: $[0.0, 0.05, 0.15, 0.80]$,\n    - $i=5$: $[0.0, 0.05, 0.25, 0.70]$.\n  - $\\boldsymbol{\\theta}^\\star = [0.18, 0.22, 0.25, 0.20, 0.15]$。\n  - $N = 200$, $\\mathbf{y} = [38, 45, 54, 63]$。\n  - $\\boldsymbol{\\alpha} = [2, 2, 2, 2, 2]$。\n  - $T = 100$.\n\n- 案例D（矩形、零计数箱、非对称先验）：\n  - $I=3$, $J=4$.\n  - $R$的列向量：\n    - $i=1$: $[0.6, 0.3, 0.1, 0.0]$,\n    - $i=2$: $[0.2, 0.5, 0.25, 0.05]$,\n    - $i=3$: $[0.1, 0.2, 0.3, 0.4]$.\n  - $\\boldsymbol{\\theta}^\\star = [0.34, 0.33, 0.33]$。\n  - $N = 30$, $\\mathbf{y} = [10, 12, 8, 0]$。\n  - $\\boldsymbol{\\alpha} = [1, 2, 3]$。\n  - $T = 80$.\n\n实现要求：\n- 对所有$i$使用均匀初始化$\\theta_i^{(0)} = 1/I$。\n- 对于每个案例，运行两种方法各$T$次迭代。在最后，为每个案例计算：\n  - EM估计值与$\\boldsymbol{\\theta}^\\star$的$L_1$距离，即$\\sum_i |\\hat{\\theta}^{\\text{EM}}_i - \\theta^\\star_i|$。\n  - 广义迭代贝叶斯估计值（使用狄利克雷先验）与$\\boldsymbol{\\theta}^\\star$的$L_1$距离。\n  - 两种估计值之间的最大绝对差，即$\\max_i |\\hat{\\theta}^{\\text{EM}}_i - \\hat{\\theta}^{\\text{MAP}}_i|$。\n  - 一个标志位，当且仅当所有$\\alpha_i = 1$且最大绝对差小于容差$\\varepsilon = 10^{-12}$时等于1；否则该标志位为0。\n- 数值稳定性：在计算$p_j = \\sum_i R_{ji} \\theta_i$时，如果任何$p_j = 0$，为了除法运算的目的，将其替换为$\\max(p_j, \\varepsilon)$，其中$\\varepsilon = 10^{-15}$。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含一个逗号分隔的列表，列表中的每个元素是一个案例的结果，每个案例的结果形式为$[L1_{\\text{EM}}, L1_{\\text{MAP}}, \\Delta_{\\max}, \\text{flag}]$。例如，一个包含两个案例的有效输出看起来像$[[0.1,0.2,0.0,1],[0.3,0.25,0.05,0]]$。对于本问题，输出一行形式为$[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]]$的结果，依次对应案例A、B、C和D。",
            "solution": "所提出的问题是一个经典的统计逆问题，在实验科学中很常见。在这类问题中，必须从观测数据$\\mathbf{y}$中推断出潜在的真实分布$\\boldsymbol{\\theta}$，而这些观测数据经历了一个由响应矩阵$R$描述的已知畸变或弥散过程。正演模型假设，观测到的事件计数$\\mathbf{y} = (y_1, \\dots, y_J)$是多项式分布$\\mathbf{y} \\sim \\text{Multinomial}(N, \\mathbf{p})$的一次实现，其中概率向量$\\mathbf{p} = (p_1, \\dots, p_J)$通过线性变换$p_j = \\sum_{i=1}^I R_{ji} \\theta_i$与真实分布$\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_I)$相关联。我们的任务是在两种范式下推导用于估计$\\boldsymbol{\\theta}$的迭代算法：最大似然（ML）和使用狄利克雷先验的最大后验（MAP）。\n\n推导的核心依赖于期望最大化（EM）算法，这是一种在具有潜在变量的模型中寻找ML或MAP估计的强大技术。在此背景下，潜在变量是源于真实分布箱$i$并在探测器箱$j$中测得的事件的未观测计数$n_{ji}$。观测数据是边际量$y_j = \\sum_{i=1}^I n_{ji}$。\n\n在给定参数$\\boldsymbol{\\theta}$的情况下，观测数据$\\mathbf{y}$的对数似然为：\n$$ \\log L(\\boldsymbol{\\theta} | \\mathbf{y}) = \\log \\left( N! \\prod_{j=1}^J \\frac{p_j^{y_j}}{y_j!} \\right) = \\text{const} + \\sum_{j=1}^J y_j \\log(p_j) = \\text{const} + \\sum_{j=1}^J y_j \\log\\left(\\sum_{i=1}^I R_{ji}\\theta_i\\right) $$\n由于对数内部存在求和，直接最大化此函数变得复杂。EM算法可以规避这个问题。\n\n**1. 最大似然EM迭代的推导**\n\nEM算法包括两个步骤，迭代进行直至收敛。设$\\boldsymbol{\\theta}^{(k)}$为第$k$次迭代时的参数估计值。\n\n**E步（期望）：** 我们计算完整数据对数似然的期望，该期望以观测数据$\\mathbf{y}$和当前参数估计$\\boldsymbol{\\theta}^{(k)}$为条件。完整数据为$\\{n_{ji}\\}$。完整数据的对数似然更简单，围绕着源于每个真实分布箱$i$的计数$N_i = \\sum_j n_{ji}$。其形式为$\\log L_c(\\boldsymbol{\\theta}) = \\sum_i N_i \\log \\theta_i + \\text{不含 } \\boldsymbol{\\theta}\\text{ 的项}$。所需的期望是$Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(k)}) = E[\\log L_c(\\boldsymbol{\\theta}) | \\mathbf{y}, \\boldsymbol{\\theta}^{(k)}]$。\n我们需要潜在计数$n_{ji}$的条件期望。对于探测器箱$j$中的$y_j$个事件中的每一个，它源于真实分布箱$i$的概率可以使用贝叶斯定理找到：\n$$ P(\\text{origin } i | \\text{seen in } j, \\boldsymbol{\\theta}^{(k)}) = \\frac{P(\\text{seen in } j | \\text{origin } i) P(\\text{origin } i)^{(k)}}{P(\\text{seen in } j)^{(k)}} = \\frac{R_{ji} \\theta_i^{(k)}}{\\sum_{i'=1}^I R_{ji'} \\theta_{i'}^{(k)}} $$\n因此，从真实分布箱$i$发出并在探测器箱$j$中观测到的事件的期望数量为：\n$$ \\hat{n}_{ji}^{(k)} = E[n_{ji} | \\mathbf{y}, \\boldsymbol{\\theta}^{(k)}] = y_j \\cdot P(\\text{origin } i | \\text{seen in } j, \\boldsymbol{\\theta}^{(k)}) = y_j \\frac{R_{ji} \\theta_i^{(k)}}{\\sum_{i'=1}^I R_{ji'} \\theta_{i'}^{(k)}} $$\n从真实分布箱$i$发出的事件的期望总数为$\\hat{N}_i^{(k)} = E[N_i | \\mathbf{y}, \\boldsymbol{\\theta}^{(k)}] = \\sum_{j=1}^J \\hat{n}_{ji}^{(k)}$。\n在M步中需要最大化的函数变为：\n$$ Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(k)}) = \\sum_{i=1}^I \\hat{N}_i^{(k)} \\log \\theta_i $$\n\n**M步（最大化）：** 我们寻找参数向量$\\boldsymbol{\\theta}^{(k+1)}$，以在约束$\\sum_i \\theta_i = 1$下最大化$Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(k)})$。使用拉格朗日乘子$\\lambda$，我们最大化$\\mathcal{L} = \\sum_i \\hat{N}_i^{(k)} \\log \\theta_i - \\lambda(\\sum_i \\theta_i - 1)$。将偏导数设为零，$\\frac{\\partial\\mathcal{L}}{\\partial\\theta_i} = \\frac{\\hat{N}_i^{(k)}}{\\theta_i} - \\lambda = 0$，得到$\\theta_i = \\hat{N}_i^{(k)}/\\lambda$。\n对$i$求和并应用约束条件得到$\\sum_i \\theta_i = 1 = \\frac{1}{\\lambda} \\sum_i \\hat{N}_i^{(k)}$。求和$\\sum_i \\hat{N}_i^{(k)} = \\sum_i \\sum_j \\hat{n}_{ji}^{(k)} = \\sum_j y_j \\sum_i P(i|j) = \\sum_j y_j = N$。因此，$\\lambda = N$。\nML估计的更新规则是$\\theta_i^{(k+1)} = \\hat{N}_i^{(k)} / N$。代入$\\hat{N}_i^{(k)}$的表达式：\n$$ \\theta_i^{(k+1)} = \\frac{1}{N} \\sum_{j=1}^J y_j \\frac{R_{ji} \\theta_i^{(k)}}{\\sum_{i'=1}^I R_{ji'} \\theta_{i'}^{(k)}} $$\n这可以写成一种乘法形式，使其与前一次估计的联系更加明确：\n$$ \\theta_i^{(k+1)} = \\frac{\\theta_i^{(k)}}{N} \\sum_{j=1}^J \\frac{y_j R_{ji}}{\\sum_{i'=1}^I R_{ji'} \\theta_{i'}^{(k)}} $$\n这就是广泛使用的迭代展开公式，是EM算法的直接应用。\n\n**2. 广义迭代贝叶斯更新（MAP）的推导**\n\n为了找到最大后验（MAP）估计，我们最大化后验概率$P(\\boldsymbol{\\theta}|\\mathbf{y}) \\propto P(\\mathbf{y}|\\boldsymbol{\\theta}) P(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha})$，其中$P(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha})$是狄利克雷先验：\n$$ P(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha}) \\propto \\prod_{i=1}^I \\theta_i^{\\alpha_i - 1} $$\n需要最大化的对数后验是$\\log P(\\boldsymbol{\\theta}|\\mathbf{y}) \\propto \\log L(\\boldsymbol{\\theta}|\\mathbf{y}) + \\log P(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha})$。\n通过在M步中用对数先验项增强$Q$函数，EM框架被调整用于MAP估计。E步保持不变。\n\n**M步（MAP）：** 我们最大化$Q_{MAP}(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(k)}) = Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(k)}) + \\log P(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha})$：\n$$ Q_{MAP}(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(k)}) = \\sum_{i=1}^I \\hat{N}_i^{(k)} \\log \\theta_i + \\sum_{i=1}^I (\\alpha_i - 1) \\log \\theta_i = \\sum_{i=1}^I (\\hat{N}_i^{(k)} + \\alpha_i - 1) \\log \\theta_i $$\n在归一化约束$\\sum_i \\theta_i = 1$下最大化此$\\boldsymbol{\\theta}$的表达式，等价于寻找参数为$\\beta_i = \\hat{N}_i^{(k)} + \\alpha_i$的狄利克雷分布的众数。解为：\n$$ \\theta_i^{(k+1)} = \\frac{(\\hat{N}_i^{(k)} + \\alpha_i - 1)}{\\sum_{i'=1}^I (\\hat{N}_{i'}^{(k)} + \\alpha_{i'} - 1)} $$\n分母可以简化：$\\sum_i (\\hat{N}_i^{(k)} + \\alpha_i - 1) = \\sum_i \\hat{N}_i^{(k)} + \\sum_i \\alpha_i - \\sum_i 1 = N + A_0 - I$，其中$A_0 = \\sum_i \\alpha_i$。\n代入$\\hat{N}_i^{(k)} = \\theta_i^{(k)} \\sum_j \\frac{y_j R_{ji}}{\\sum_{i'} R_{ji'} \\theta_{i'}^{(k)}}$，我们得到广义迭代贝叶斯更新：\n$$ \\theta_i^{(k+1)} = \\frac{\\alpha_i - 1 + \\theta_i^{(k)} \\sum_{j=1}^J \\frac{y_j R_{ji}}{\\sum_{i'=1}^I R_{ji'} \\theta_{i'}^{(k)}}}{N + \\sum_{i'=1}^I \\alpha_{i'} - I} $$\n这个更新规则迭代地寻找后验分布的众数。\n\n**3. 均匀先验（$\\alpha_i=1$）下等价性的证明**\n\n我们将证明，当超参数对所有$i \\in \\{1,\\dots,I\\}$都设置为$\\alpha_i = 1$时，广义迭代贝叶斯更新简化为基于计数的EM更新。这一特定选择对应于均匀狄利克雷先验，它不对任何特定的分布$\\boldsymbol{\\theta}$引入先验偏好。\n\n从MAP更新公式开始：\n$$ \\theta_i^{(k+1)} = \\frac{\\alpha_i - 1 + \\hat{N}_i^{(k)}}{N + A_0 - I} $$\n我们对所有$i$代入$\\alpha_i=1$。\n分子变为：$\\alpha_i - 1 + \\hat{N}_i^{(k)} = 1 - 1 + \\hat{N}_i^{(k)} = \\hat{N}_i^{(k)}$。\n超参数的和为$A_0 = \\sum_{i=1}^I \\alpha_i = \\sum_{i=1}^I 1 = I$。\n分母变为：$N + A_0 - I = N + I - I = N$。\n将这些代回MAP更新公式，得到：\n$$ \\theta_i^{(k+1)} = \\frac{\\hat{N}_i^{(k)}}{N} $$\n这恰好是最大似然EM算法的更新规则。因此，证明完成。ML估计是使用均匀先验的MAP估计的一个特例。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the iterative unfolding problem for the given test cases.\n    \"\"\"\n    \n    # Numerical stability constant\n    EPSILON = 1e-15\n\n    # Define test cases\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"R_cols\": [\n                [0.8, 0.15, 0.05],\n                [0.1, 0.8, 0.1],\n                [0.05, 0.1, 0.85]\n            ],\n            \"y\": np.array([225, 460, 315]),\n            \"alpha\": np.array([1, 1, 1]),\n            \"theta_star\": np.array([0.2, 0.5, 0.3]),\n            \"T\": 50\n        },\n        {\n            \"name\": \"B\",\n            \"R_cols\": [\n                [0.7, 0.2, 0.1, 0.0, 0.0],\n                [0.2, 0.5, 0.25, 0.05, 0.0],\n                [0.1, 0.25, 0.4, 0.2, 0.05],\n                [0.0, 0.05, 0.2, 0.5, 0.25],\n                [0.0, 0.0, 0.05, 0.25, 0.7]\n            ],\n            \"y\": np.array([6, 12, 16, 11, 5]),\n            \"alpha\": np.array([5, 1, 1, 1, 1]),\n            \"theta_star\": np.array([0.05, 0.15, 0.6, 0.15, 0.05]),\n            \"T\": 200\n        },\n        {\n            \"name\": \"C\",\n            \"R_cols\": [\n                [0.85, 0.10, 0.05, 0.0],\n                [0.10, 0.70, 0.15, 0.05],\n                [0.05, 0.15, 0.65, 0.15],\n                [0.0, 0.05, 0.15, 0.80],\n                [0.0, 0.05, 0.25, 0.70]\n            ],\n            \"y\": np.array([38, 45, 54, 63]),\n            \"alpha\": np.array([2, 2, 2, 2, 2]),\n            \"theta_star\": np.array([0.18, 0.22, 0.25, 0.20, 0.15]),\n            \"T\": 100\n        },\n        {\n            \"name\": \"D\",\n            \"R_cols\": [\n                [0.6, 0.3, 0.1, 0.0],\n                [0.2, 0.5, 0.25, 0.05],\n                [0.1, 0.2, 0.3, 0.4]\n            ],\n            \"y\": np.array([10, 12, 8, 0]),\n            \"alpha\": np.array([1, 2, 3]),\n            \"theta_star\": np.array([0.34, 0.33, 0.33]),\n            \"T\": 80\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        R_cols = case[\"R_cols\"]\n        y = case[\"y\"]\n        alpha = case[\"alpha\"]\n        theta_star = case[\"theta_star\"]\n        T = case[\"T\"]\n        \n        # Assemble response matrix R (shape JxI) from columns\n        R = np.array(R_cols).T\n        J, I = R.shape\n        \n        # Total number of events\n        N = np.sum(y)\n        \n        # Initialize theta for both methods\n        theta_em = np.full(I, 1.0 / I)\n        theta_map = np.full(I, 1.0 / I)\n        \n        # Iteration loops\n        for _ in range(T):\n            # --- EM Update (Maximum Likelihood) ---\n            p_em = R @ theta_em\n            p_em = np.maximum(p_em, EPSILON)\n            \n            # efficiency_i = sum_j (y_j / p_j) * R_ji\n            efficiency_em = R.T @ (y / p_em)\n            \n            theta_em = theta_em * efficiency_em / N\n            # Renormalize to correct potential floating point drift\n            theta_em /= np.sum(theta_em)\n\n            # --- MAP Update (Generalized Bayes) ---\n            p_map = R @ theta_map\n            p_map = np.maximum(p_map, EPSILON)\n            \n            efficiency_map = R.T @ (y / p_map)\n            \n            N_hat = theta_map * efficiency_map\n            \n            A0 = np.sum(alpha)\n            \n            theta_map = (N_hat + alpha - 1) / (N + A0 - I)\n            # Renormalize\n            theta_map /= np.sum(theta_map)\n            \n        # At the end of iterations, calculate metrics\n        l1_em = np.sum(np.abs(theta_em - theta_star))\n        l1_map = np.sum(np.abs(theta_map - theta_star))\n        delta_max = np.max(np.abs(theta_em - theta_map))\n        \n        # Check for flag condition\n        is_uniform_prior = np.all(alpha == 1)\n        flag_tolerance = 1e-12\n        flag = 1 if is_uniform_prior and delta_max  flag_tolerance else 0\n\n        results.append([l1_em, l1_map, delta_max, flag])\n\n    # Print results in the required format\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}