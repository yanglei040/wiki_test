{
    "hands_on_practices": [
        {
            "introduction": "将高能物理中的喷注视为图像，是应用深度学习的早期且强大的方法之一。这种方法允许我们利用在计算机视觉领域取得巨大成功的卷积神经网络（CNN）。本练习将带你亲手操作一个简化的喷注图像，通过计算一个卷积核的响应，来具体理解CNN如何提取局部特征。通过这个过程，你不仅会掌握卷积运算的核心机制，还会学习如何将网络的基本单元——感受野（receptive field）——与喷注物理中的关键度量（角距离 $\\Delta R$）联系起来。",
            "id": "3510623",
            "problem": "您正在分析一个基于卷积神经网络 (CNN) 构建的喷注子结构分类器，该分类器用于高能物理事件分析中的离散化赝快度-方位角平面。喷注中心位于赝快度 $\\eta \\approx 0$ 处，您使用的是一幅喷注图像的局部 $3 \\times 3$ 图块，其强度是经过单位归一化的无量纲值：\n$$\nI \\;=\\; \\begin{pmatrix}\n0.12 & 0.08 & 0.05\\\\\n0.20 & 0.15 & 0.10\\\\\n0.18 & 0.12 & 0.06\n\\end{pmatrix}.\n$$\n一个已学习的 $2 \\times 2$ 核，它近似于一个定向对比度算子，由下式给出\n$$\nK \\;=\\; \\begin{pmatrix}\n0.5 & -0.25\\\\\n-0.5 & 0.25\n\\end{pmatrix}.\n$$\n在 CNN 的前向传播中，采用标准的深度学习惯例，即该层执行离散互相关（无核翻转），步长 $s=1$，且无填充。输出是有效的 $2 \\times 2$ 激活图 $Y$，其条目为\n$$\nY_{i,j} \\;=\\; \\sum_{u=0}^{1}\\sum_{v=0}^{1} K_{u,v}\\, I_{i+u,\\,j+v}, \\quad \\text{for } i,j \\in \\{1,2\\}.\n$$\n假设 $\\eta$–$\\phi$ 平面是方形像素化的，其中 $\\Delta \\eta = 0.1$ 且 $\\Delta \\phi = 0.1$，$\\phi$ 以弧度为单位。在标准的喷注分析度量中，角距离定义为 $\\Delta R = \\sqrt{(\\Delta \\eta)^{2} + (\\Delta \\phi)^{2}}$。对于 $Y$ 的单个激活，其感受野是对应的 $2 \\times 2$ 输入图块；将其角跨度 $\\Delta R_{\\text{span}}$ 定义为该感受野在 $\\eta$–$\\phi$ 平面中对角线的欧几里得长度。\n\n请按顺序计算以下内容：\n- 完整的 $2 \\times 2$ 激活图 $Y$。\n- 等于 $Y$ 中所有条目之和的标量 $S$。\n- 单个感受野的角跨度 $\\Delta R_{\\text{span}}$。\n- 最终的标量 $P \\equiv S \\times \\Delta R_{\\text{span}}$。\n\n将最终的标量 $P$ 表示为一个纯数（无量纲），并四舍五入到四位有效数字。在应用最终的四舍五入指令之前，所有中间量都可以保持精确值。角度以弧度为单位。",
            "solution": "该问题需要按顺序进行四项计算：激活图 $Y$、其和 $S$、感受野的角跨度 $\\Delta R_{\\text{span}}$ 以及最终的乘积 $P$。\n\n首先，我们使用提供的离散互相关公式计算 $2 \\times 2$ 激活图 $Y$。输入矩阵是强度 $I$ 和核 $K$。\n$$\nI = \\begin{pmatrix}\n0.12 & 0.08 & 0.05\\\\\n0.20 & 0.15 & 0.10\\\\\n0.18 & 0.12 & 0.06\n\\end{pmatrix}, \\quad K = \\begin{pmatrix}\n0.5 & -0.25\\\\\n-0.5 & 0.25\n\\end{pmatrix}\n$$\n$Y$ 的元素计算如下：\n\n$Y_{1,1} = (0.5)(0.12) + (-0.25)(0.08) + (-0.5)(0.20) + (0.25)(0.15)$\n$Y_{1,1} = 0.06 - 0.02 - 0.10 + 0.0375 = -0.0225$\n\n$Y_{1,2} = (0.5)(0.08) + (-0.25)(0.05) + (-0.5)(0.15) + (0.25)(0.10)$\n$Y_{1,2} = 0.04 - 0.0125 - 0.075 + 0.025 = -0.0225$\n\n$Y_{2,1} = (0.5)(0.20) + (-0.25)(0.15) + (-0.5)(0.18) + (0.25)(0.12)$\n$Y_{2,1} = 0.10 - 0.0375 - 0.09 + 0.03 = 0.0025$\n\n$Y_{2,2} = (0.5)(0.15) + (-0.25)(0.10) + (-0.5)(0.12) + (0.25)(0.06)$\n$Y_{2,2} = 0.075 - 0.025 - 0.06 + 0.015 = 0.0050$\n\n完整的激活图为：\n$$\nY = \\begin{pmatrix}\n-0.0225 & -0.0225 \\\\\n0.0025 & 0.0050\n\\end{pmatrix}\n$$\n其次，我们计算标量 $S$，即 $Y$ 中所有元素之和。\n$$\nS = (-0.0225) + (-0.0225) + 0.0025 + 0.0050 = -0.0450 + 0.0075 = -0.0375\n$$\n第三，我们计算角跨度 $\\Delta R_{\\text{span}}$。单个激活的感受野是一个 $2 \\times 2$ 的像素块。每个像素的尺寸为 $\\Delta \\eta = 0.1$ 和 $\\Delta \\phi = 0.1$。$2 \\times 2$ 图块在 $\\eta$-$\\phi$ 平面上的总边长为 $L_{\\eta} = 2 \\times 0.1 = 0.2$ 和 $L_{\\phi} = 2 \\times 0.1 = 0.2$。角跨度是该区域对角线的长度。\n$$\n\\Delta R_{\\text{span}} = \\sqrt{L_{\\eta}^2 + L_{\\phi}^2} = \\sqrt{(0.2)^2 + (0.2)^2}\n$$\n$$\n\\Delta R_{\\text{span}} = \\sqrt{0.04 + 0.04} = \\sqrt{0.08} = 0.2\\sqrt{2}\n$$\n第四，我们计算最终的标量 $P = S \\times \\Delta R_{\\text{span}}$。\n$$\nP = (-0.0375) \\times (0.2\\sqrt{2}) = -0.0075\\sqrt{2}\n$$\n最后，我们将 $P$ 的值四舍五入到四位有效数字。\n$$\nP \\approx -0.0075 \\times 1.41421356 \\approx -0.0106066\n$$\n四舍五入后，我们得到：\n$$\nP \\approx -0.01061\n$$",
            "answer": "$$\n\\boxed{-0.01061}\n$$"
        },
        {
            "introduction": "为了更好地处理粒子事件的无序、可变长度的特性，研究人员转向了图神经网络（GNN）。GNN将事件中的每个粒子视为一个节点，通过节点间的“消息传递”来学习粒子间的复杂关系。本练习将引导你完成一个消息传递层的核心计算步骤，让你深入了解GNN是如何聚合来自邻近粒子的信息来更新特定粒子表示的。这个过程特别强调了如何设计模型以尊重物理对称性，例如方位角旋转不变性。",
            "id": "3510657",
            "problem": "一个用于计算高能物理中事件级分析的图神经网络作用于一个由四个重建粒子（节点）构成的有向完全图，每个节点都具有运动学特征 $x_i=(p_{T,i},\\eta_i,\\phi_i)$，其中 $p_{T,i}$ 是横向动量，$\\eta_i$ 是赝快度，$\\phi_i$ 是方位角。节点 $i$ 的初始标量隐藏状态由一个线性嵌入定义 $h_i^{(0)}=w_p \\ln p_{T,i} + w_{\\eta}\\,\\eta_i$，该定义仅使用 $\\ln p_{T,i}$ 和 $\\eta_i$ 以遵守方位旋转不变性。对于每条有向边 $(i \\leftarrow j)$，定义边特征 $e_{ij}=(\\Delta \\eta_{ij}, \\Delta \\phi_{ij}, \\ln p_{T,j})$，其中 $\\Delta \\eta_{ij}=\\eta_j-\\eta_i$，而 $\\Delta \\phi_{ij}$ 由区间 $[-\\pi,\\pi]$ 上的环绕差值给出，使其成为方位角上的最小有符号间隔。从节点 $j$ 到节点 $i$ 的消息计算如下\n$$\nm_{ij}=\\tanh\\!\\left(a\\,h_j^{(0)}+b_1\\,\\Delta \\eta_{ij}+b_2\\,\\cos(\\Delta \\phi_{ij})+b_3\\,\\ln p_{T,j}+c\\right),\n$$\n节点更新使用求和聚合，\n$$\na_i=\\sum_{j\\neq i} m_{ij},\\quad h_i^{(1)}=h_i^{(0)}+\\alpha\\,a_i.\n$$\n给定以下常数和节点特征：\n- 嵌入权重：$w_p=0.6$, $w_{\\eta}=0.3$。\n- 消息权重：$a=0.8$, $b_1=0.15$, $b_2=0.2$, $b_3=0.05$, $c=-0.02$。\n- 更新系数：$\\alpha=0.7$。\n- 节点 $1$：$(p_{T,1},\\eta_1,\\phi_1)=\\left(50,\\ 0.2,\\ 0.10\\right)$。\n- 节点 $2$：$(p_{T,2},\\eta_2,\\phi_2)=\\left(40,\\ -0.3,\\ 3.05\\right)$。\n- 节点 $3$：$(p_{T,3},\\eta_3,\\phi_3)=\\left(25,\\ 1.1,\\ -3.00\\right)$。\n- 节点 $4$：$(p_{T,4},\\eta_4,\\phi_4)=\\left(10,\\ -2.0,\\ 2.90\\right)$。\n\n所有角度均以弧度为单位，$\\ln$ 表示自然对数。使用上述定义，计算节点 $2$ 更新后的隐藏状态 $h_2^{(1)}$。将您的最终结果表示为一个实数，并四舍五入到四位有效数字。最终答案不需要物理单位。",
            "solution": "为了计算节点 $2$ 更新后的隐藏状态 $h_2^{(1)}$，我们需要分步进行。\n首先，计算所有相关节点的初始隐藏状态 $h_j^{(0)}$。然后，计算从其他所有节点 $j \\neq 2$ 到节点 $2$ 的消息 $m_{2j}$。接着，将这些消息聚合得到 $a_2$。最后，使用更新规则计算 $h_2^{(1)}$。\n\n**1. 计算初始隐藏状态 $h_j^{(0)}$**\n使用公式 $h_j^{(0)} = w_p \\ln p_{T,j} + w_{\\eta} \\eta_j$，其中 $w_p = 0.6$，$w_{\\eta} = 0.3$。\n- $\\ln p_{T,1} = \\ln(50) \\approx 3.91202$\n- $\\ln p_{T,2} = \\ln(40) \\approx 3.68888$\n- $\\ln p_{T,3} = \\ln(25) \\approx 3.21888$\n- $\\ln p_{T,4} = \\ln(10) \\approx 2.30259$\n\n- $h_1^{(0)} = (0.6)(3.91202) + (0.3)(0.2) = 2.347212 + 0.06 = 2.407212$\n- $h_2^{(0)} = (0.6)(3.68888) + (0.3)(-0.3) = 2.213328 - 0.09 = 2.123328$\n- $h_3^{(0)} = (0.6)(3.21888) + (0.3)(1.1) = 1.931328 + 0.33 = 2.261328$\n- $h_4^{(0)} = (0.6)(2.30259) + (0.3)(-2.0) = 1.381554 - 0.6 = 0.781554$\n\n**2. 计算到节点 $2$ 的消息 $m_{2j}$**\n消息公式为 $m_{ij}=\\tanh(a\\,h_j^{(0)}+b_1\\,\\Delta \\eta_{ij}+b_2\\,\\cos(\\Delta \\phi_{ij})+b_3\\,\\ln p_{T,j}+c)$，其中 $a=0.8, b_1=0.15, b_2=0.2, b_3=0.05, c=-0.02$。\n\n**消息 $m_{21}$ (从 $j=1$ 到 $i=2$)**:\n- $\\Delta \\eta_{21} = \\eta_1 - \\eta_2 = 0.2 - (-0.3) = 0.5$\n- $\\Delta \\phi_{21} = \\phi_1 - \\phi_2 = 0.10 - 3.05 = -2.95$ (在 $[-\\pi, \\pi]$ 区间内)\n- $\\cos(\\Delta \\phi_{21}) \\approx \\cos(-2.95) \\approx -0.98506$\n- $\\tanh$ 的参数为：\n  $Z_{21} \\approx (0.8)(2.40721) + (0.15)(0.5) + (0.2)(-0.98506) + (0.05)(3.91202) - 0.02 \\approx 1.92577 + 0.075 - 0.19701 + 0.19560 - 0.02 = 1.97936$\n- $m_{21} = \\tanh(1.97936) \\approx 0.96255$\n\n**消息 $m_{23}$ (从 $j=3$ 到 $i=2$)**:\n- $\\Delta \\eta_{23} = \\eta_3 - \\eta_2 = 1.1 - (-0.3) = 1.4$\n- $\\Delta \\phi_{23} = \\phi_3 - \\phi_2 = -3.00 - 3.05 = -6.05$。环绕处理：$-6.05 + 2\\pi \\approx 0.23318$\n- $\\cos(\\Delta \\phi_{23}) \\approx \\cos(0.23318) \\approx 0.97290$\n- $\\tanh$ 的参数为：\n  $Z_{23} \\approx (0.8)(2.26133) + (0.15)(1.4) + (0.2)(0.97290) + (0.05)(3.21888) - 0.02 \\approx 1.80906 + 0.21 + 0.19458 + 0.16094 - 0.02 = 2.35458$\n- $m_{23} = \\tanh(2.35458) \\approx 0.98207$\n\n**消息 $m_{24}$ (从 $j=4$ 到 $i=2$)**:\n- $\\Delta \\eta_{24} = \\eta_4 - \\eta_2 = -2.0 - (-0.3) = -1.7$\n- $\\Delta \\phi_{24} = \\phi_4 - \\phi_2 = 2.90 - 3.05 = -0.15$ (在 $[-\\pi, \\pi]$ 区间内)\n- $\\cos(\\Delta \\phi_{24}) \\approx \\cos(-0.15) \\approx 0.98877$\n- $\\tanh$ 的参数为：\n  $Z_{24} \\approx (0.8)(0.78155) + (0.15)(-1.7) + (0.2)(0.98877) + (0.05)(2.30259) - 0.02 \\approx 0.62524 - 0.255 + 0.19775 + 0.11513 - 0.02 = 0.66312$\n- $m_{24} = \\tanh(0.66312) \\approx 0.58055$\n\n**3. 计算聚合消息 $a_2$ 和更新后的状态 $h_2^{(1)}$**\n聚合消息 $a_2$ 是所有传入消息之和：\n$$a_2 = m_{21} + m_{23} + m_{24} \\approx 0.96255 + 0.98207 + 0.58055 = 2.52517$$\n最后，使用更新规则 $h_2^{(1)} = h_2^{(0)} + \\alpha a_2$ 及 $\\alpha = 0.7$：\n$$h_2^{(1)} \\approx 2.123328 + (0.7)(2.52517) \\approx 2.123328 + 1.767619 = 3.890947$$\n将结果四舍五入到四位有效数字，得到 $3.891$。",
            "answer": "$$\n\\boxed{3.891}\n$$"
        },
        {
            "introduction": "注意力机制，作为Transformer架构的核心，为处理粒子集合提供了另一种强大的范式，它允许模型动态地学习粒子之间的相互重要性。本练习将揭开缩放点积注意力的神秘面紗，让你手动计算其输出。一个关键的亮点是引入了基于角距离 $\\Delta R$ 的“局部性掩码”（locality mask），这展示了如何将物理先验知识（如局部相互作用更重要）注入到一个通用架构中，以引导模型的学习过程。",
            "id": "3510640",
            "problem": "考虑一个计算高能物理中的简化事件，该事件包含 $3$ 个重建粒子，其角坐标 $(\\eta, \\phi)$ 分别为：粒子 $1$：$(\\eta_1, \\phi_1) = (0.0, 0.0)$，粒子 $2$：$(\\eta_2, \\phi_2) = (0.2, 0.1)$，以及粒子 $3$：$(\\eta_3, \\phi_3) = (0.8, 0.7)$，其中角度以弧度为单位。我们研究应用于这些粒子的单头缩放点积注意力机制，该机制与用于事件分析的现代深度学习架构一致。使用以下基本定义：\n\n- 对于查询索引 $i$ 和键索引 $j$，缩放点积注意力分数为 $s_{ij} = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}} + M_{ij}$，其中 $Q_i \\in \\mathbb{R}^{d_k}$ 是粒子 $i$ 的查询，$K_j \\in \\mathbb{R}^{d_k}$ 是粒子 $j$ 的键，$d_k$ 是键的维度，而 $M_{ij}$ 是一个掩码。\n- 注意力权重为 $a_{ij} = \\frac{\\exp(s_{ij})}{\\sum_{j'=1}^{3} \\exp(s_{ij'})}$。\n- 粒子 $i$ 的输出为 $O_i = \\sum_{j=1}^{3} a_{ij} V_j$，其中 $V_j \\in \\mathbb{R}^{d_v}$ 是粒子 $j$ 的值。\n\n采用一个局部性掩码，该掩码在事件拓扑中编码了一种最近邻注意力：对于每个查询粒子 $i$，如果 $j = i$ 或者 $j$ 是 $i$ 在角距离 $\\Delta R_{ij} = \\sqrt{(\\eta_i - \\eta_j)^2 + (\\phi_i - \\phi_j)^2}$ 上的唯一最近邻，则设 $M_{ij} = 0$；否则设 $M_{ij} = -\\infty$。假设对于给定的坐标不会出现距离相等的情况。\n\n设单头参数为 $d_k = 2$，$d_v = 2$，查询、键和值矩阵（行对应粒子 $1,2,3$）为\n$$\nQ = \\begin{pmatrix}\n1 & -1 \\\\\n0 & 2 \\\\\n-1 & 0\n\\end{pmatrix}, \\quad\nK = \\begin{pmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix}, \\quad\nV = \\begin{pmatrix}\n1 & 2 \\\\\n2 & -1 \\\\\n0 & 3\n\\end{pmatrix}.\n$$\n\n仅使用上述定义和所提供的数据，计算在局部性掩码下三个粒子的单头注意力输出 $O_1$、$O_2$ 和 $O_3$。然后，计算标量\n$$S = \\|O_1\\|_2^2 + \\|O_2\\|_2^2 + \\|O_3\\|_2^2,$$\n其中 $\\|\\cdot\\|_2$ 表示 $\\mathbb{R}^{2}$ 上的欧几里得范数。请提供 $S$ 的最终值，作为一个精确的实数。无需四舍五入。",
            "solution": "为了计算标量 $S$，我们必须依次执行以下步骤：1. 基于粒子坐标构建局部性掩码。2. 计算注意力权重。3. 计算每个粒子的输出向量 $O_i$。4. 计算并求和每个输出向量的范数平方。\n\n**1. 构建局部性掩码**\n首先，我们计算粒子对之间的角距离的平方 $\\Delta R_{ij}^2$。\n- $\\Delta R_{12}^2 = (0.0 - 0.2)^2 + (0.0 - 0.1)^2 = 0.04 + 0.01 = 0.05$\n- $\\Delta R_{13}^2 = (0.0 - 0.8)^2 + (0.0 - 0.7)^2 = 0.64 + 0.49 = 1.13$\n- $\\Delta R_{23}^2 = (0.2 - 0.8)^2 + (0.1 - 0.7)^2 = (-0.6)^2 + (-0.6)^2 = 0.36 + 0.36 = 0.72$\n\n接下来，为每个粒子 $i$ 确定其最近邻 $j$：\n- 对于粒子 $1$：$\\Delta R_{12}^2 < \\Delta R_{13}^2$，所以最近邻是粒子 $2$。\n- 对于粒子 $2$：$\\Delta R_{21}^2 < \\Delta R_{23}^2$，所以最近邻是粒子 $1$。\n- 对于粒子 $3$：$\\Delta R_{32}^2 < \\Delta R_{31}^2$，所以最近邻是粒子 $2$。\n\n根据掩码规则（$M_{ij}=0$ 如果 $j=i$ 或 $j$ 是 $i$ 的最近邻，否则 $M_{ij}=-\\infty$），我们构建掩码矩阵 $M$：\n$$\nM = \\begin{pmatrix} 0 & 0 & -\\infty \\\\ 0 & 0 & -\\infty \\\\ -\\infty & 0 & 0 \\end{pmatrix}\n$$\n\n**2. 计算注意力权重**\n注意力分数 $s_{ij} = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}} + M_{ij}$。由于键矩阵 $K$ 是零矩阵，对于所有 $i, j$，点积项 $Q_i \\cdot K_j = 0$。因此，分数矩阵 $s$ 等于掩码矩阵 $M$。\n现在我们计算注意力权重 $a_{ij} = \\frac{\\exp(s_{ij})}{\\sum_{j'} \\exp(s_{ij'})}$。利用 $\\exp(0)=1$ 和 $\\exp(-\\infty)=0$：\n- 对于行 $i=1$：分母是 $\\exp(0) + \\exp(0) + \\exp(-\\infty) = 1 + 1 + 0 = 2$。权重为 $(a_{11}, a_{12}, a_{13}) = (\\frac{1}{2}, \\frac{1}{2}, 0)$。\n- 对于行 $i=2$：分母是 $\\exp(0) + \\exp(0) + \\exp(-\\infty) = 1 + 1 + 0 = 2$。权重为 $(a_{21}, a_{22}, a_{23}) = (\\frac{1}{2}, \\frac{1}{2}, 0)$。\n- 对于行 $i=3$：分母是 $\\exp(-\\infty) + \\exp(0) + \\exp(0) = 0 + 1 + 1 = 2$。权重为 $(a_{31}, a_{32}, a_{33}) = (0, \\frac{1}{2}, \\frac{1}{2})$。\n注意力权重矩阵 $A$ 为：\n$$\nA = \\begin{pmatrix} 1/2 & 1/2 & 0 \\\\ 1/2 & 1/2 & 0 \\\\ 0 & 1/2 & 1/2 \\end{pmatrix}\n$$\n\n**3. 计算输出向量**\n输出向量 $O_i = \\sum_j a_{ij} V_j$。设 $V_1=(1,2), V_2=(2,-1), V_3=(0,3)$。\n- $O_1 = \\frac{1}{2}V_1 + \\frac{1}{2}V_2 = \\frac{1}{2}(1,2) + \\frac{1}{2}(2,-1) = (0.5, 1) + (1, -0.5) = (1.5, 0.5) = (\\frac{3}{2}, \\frac{1}{2})$\n- $O_2 = \\frac{1}{2}V_1 + \\frac{1}{2}V_2 = (\\frac{3}{2}, \\frac{1}{2})$\n- $O_3 = \\frac{1}{2}V_2 + \\frac{1}{2}V_3 = \\frac{1}{2}(2,-1) + \\frac{1}{2}(0,3) = (1, -0.5) + (0, 1.5) = (1, 1)$\n\n**4. 计算并求和范数平方**\n最后，我们计算 $S = \\|O_1\\|_2^2 + \\|O_2\\|_2^2 + \\|O_3\\|_2^2$。\n- $\\|O_1\\|_2^2 = (\\frac{3}{2})^2 + (\\frac{1}{2})^2 = \\frac{9}{4} + \\frac{1}{4} = \\frac{10}{4} = \\frac{5}{2}$\n- $\\|O_2\\|_2^2 = (\\frac{3}{2})^2 + (\\frac{1}{2})^2 = \\frac{10}{4} = \\frac{5}{2}$\n- $\\|O_3\\|_2^2 = 1^2 + 1^2 = 2$\n\n将它们相加：\n$$\nS = \\frac{5}{2} + \\frac{5}{2} + 2 = 5 + 2 = 7\n$$",
            "answer": "$$\n\\boxed{7}\n$$"
        }
    ]
}