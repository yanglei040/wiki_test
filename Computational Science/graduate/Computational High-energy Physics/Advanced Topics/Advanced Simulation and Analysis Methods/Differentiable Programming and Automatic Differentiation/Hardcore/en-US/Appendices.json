{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation in differentiable programming, we begin with the mechanics of forward-mode automatic differentiation. This exercise  tasks you with implementing forward-mode AD from first principles using dual numbers, extending the concept to handle the complex arithmetic prevalent in physics amplitudes. You will then validate your exact, algorithmically derived gradient against the high-precision complex-step numerical method, providing a direct and practical insight into the power and numerical stability of AD.",
            "id": "3511503",
            "problem": "You are asked to implement and validate gradients using Automatic Differentiation (AD) and the complex-step differentiation method for a real-valued function built from complex amplitudes encountered in computational high-energy physics. The core objective is to benchmark the numerical accuracy of the complex-step differentiation estimate against AD in scenarios that exhibit catastrophic cancellation.\n\nConsider the following real-valued function of the real scalar variable $E$:\n$$\nf(E) \\equiv \\sum_{k=1}^{N} \\left( \\frac{c_k}{E - m_k + i \\frac{\\Gamma_k}{2}} + \\frac{\\tilde{c}_k}{E - m_k - i \\frac{\\Gamma_k}{2}} \\right),\n$$\nwhere $c_k \\in \\mathbb{C}$ and $\\tilde{c}_k \\in \\mathbb{C}$ are fixed complex coefficients, $m_k \\in \\mathbb{R}$ are masses, and $\\Gamma_k \\in \\mathbb{R}_{>0}$ are widths. The parameters are constrained such that $E \\in \\mathbb{R}$ and $\\tilde{c}_k$ is chosen equal to the complex conjugate of $c_k$, but this equality is encoded as a numeric parameter and no complex conjugation operation is performed in the program. Under these parameter choices, $f(E)$ is real-valued for real $E$, even though its internal computations involve complex arithmetic, and each summand is holomorphic in $E$.\n\nYour tasks:\n- Implement forward-mode Automatic Differentiation (AD) from first principles for scalar input $E$ with complex arithmetic support, using dual numbers to propagate derivatives through addition, subtraction, multiplication, and division. Treat $E$ as the independent variable and all other parameters as constants.\n- Implement the complex-step differentiation estimate for the derivative:\n$$\n\\frac{\\partial f}{\\partial E}(E) \\approx \\frac{\\operatorname{Im}\\left( f(E + i h) \\right)}{h},\n$$\nwhere $h \\in \\mathbb{R}$ is a small step, $i$ is the imaginary unit, and $\\operatorname{Im}(\\cdot)$ denotes the imaginary part.\n- Compute the absolute error between the complex-step estimate and the AD derivative:\n$$\n\\varepsilon(E,h) \\equiv \\left| \\frac{\\operatorname{Im}\\left( f(E + i h) \\right)}{h} - \\frac{\\partial f}{\\partial E}(E) \\right|.\n$$\n- Design the program to evaluate the following fixed test suite and output the list of absolute errors. All energies and widths are to be understood in giga-electronvolts (GeV), while the output errors are dimensionless real numbers. You must not use any complex conjugation operation in your program; instead, supply $\\tilde{c}_k$ numerically equal to the complex conjugate of $c_k$.\n\nTest suite (each test case is a tuple consisting of $E$, $h$, and a list of resonance pairs $(c_k, \\tilde{c}_k, m_k, \\Gamma_k)$):\n- Case $1$ (catastrophic cancellation at resonance center): $E = 91.1876$, $h = 10^{-30}$, $N=1$, parameters $[(c_1 = 1 + 0 i,\\ \\tilde{c}_1 = 1 - 0 i,\\ m_1 = 91.1876,\\ \\Gamma_1 = 2.4952)]$.\n- Case $2$ (near-resonance interference, two-pair cancellation): $E = 124.9$, $h = 10^{-20}$, $N=2$, parameters $[(c_1 = 1 + 5 i,\\ \\tilde{c}_1 = 1 - 5 i,\\ m_1 = 125.0,\\ \\Gamma_1 = 4.0),\\ (c_2 = -0.8 + 4 i,\\ \\tilde{c}_2 = -0.8 - 4 i,\\ m_2 = 124.0,\\ \\Gamma_2 = 3.0)]$.\n- Case $3$ (narrow-width sharp feature): $E = 500.0 + 0.5 \\times 10^{-6}$, $h = 10^{-40}$, $N=1$, parameters $[(c_1 = 2 + 0 i,\\ \\tilde{c}_1 = 2 - 0 i,\\ m_1 = 500.0,\\ \\Gamma_1 = 10^{-6})]$.\n\nYour program must:\n- Implement AD using dual numbers in a manner consistent with the derivative definition $df/dE$ and propagate derivatives through rational operations.\n- Implement the complex-step estimate as specified above and compute the absolute error $\\varepsilon(E,h)$ for each case.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[e_1,e_2,e_3]$), where $e_k$ are the absolute error values for the corresponding test cases, represented as standard floating-point numbers.\n\nNo external inputs are required. The output is dimensionless real numbers; no physical units are to be included in the output line. The angle unit is not applicable in this problem. The test suite has been selected to probe a general case, catastrophic cancellation at a resonance, and a sharp feature due to narrow width, thereby covering typical edge cases for numerical differentiation in the presence of complex amplitudes.",
            "solution": "The user problem is valid as it is scientifically grounded, well-posed, and objective. It presents a standard problem from computational physics—comparing differentiation techniques for a function common in high-energy physics, the Breit-Wigner resonance formula. The problem is self-contained, providing all necessary parameters and definitions.\n\nThe solution requires implementing two methods for computing the derivative of a real-valued function $f(E)$ that is constructed from complex arithmetic. The function is given by:\n$$\nf(E) = \\sum_{k=1}^{N} \\left( \\frac{c_k}{E - m_k + i \\frac{\\Gamma_k}{2}} + \\frac{\\tilde{c}_k}{E - m_k - i \\frac{\\Gamma_k}{2}} \\right)\n$$\nwhere the parameters are provided such that $f(E)$ is real for a real input $E$. Consequently, its derivative with respect to $E$, $\\frac{\\partial f}{\\partial E}$, must also be a real number.\n\nThe two methods for computing this derivative are:\n1.  **Forward-Mode Automatic Differentiation (AD):** This method computes the exact derivative up to machine precision by propagating function values and their derivatives simultaneously through every arithmetic operation. We will use dual numbers for this purpose. Since the intermediate calculations involve complex numbers, we must implement a `ComplexDual` number type. A complex dual number $z$ is represented as $z = z_r + z_d \\varepsilon$, where $z_r$ (the \"real\" part) and $z_d$ (the \"dual\" part) are both standard complex numbers, and $\\varepsilon$ is a nilpotent element with the property $\\varepsilon^2 = 0$. For a function $g(x)$, if we evaluate it with the dual input $x_0 + 1\\varepsilon$, the result will be a dual number $g(x_0) + g'(x_0)\\varepsilon$. The dual part of the result is the derivative. The arithmetic rules for dual numbers are derived from the standard rules of calculus:\n    -   $(a+b\\varepsilon) + (c+d\\varepsilon) = (a+c) + (b+d)\\varepsilon$\n    -   $(a+b\\varepsilon) \\cdot (c+d\\varepsilon) = ac + (ad+bc)\\varepsilon$\n    -   $\\frac{a+b\\varepsilon}{c+d\\varepsilon} = \\frac{a}{c} + \\frac{bc-ad}{c^2}\\varepsilon$\n    To compute $\\frac{\\partial f}{\\partial E}$, we evaluate $f$ with the input $E_{dual} = E + 1\\varepsilon$. All other parameters ($c_k, m_k, \\Gamma_k$) are treated as constants, represented by dual numbers with a zero dual part.\n\n2.  **Complex-Step (CS) Differentiation:** This is a high-precision numerical approximation method. It leverages the Taylor series expansion of a holomorphic function $g(z)$ around a real point $x$:\n    $$\n    g(x+ih) = g(x) + ihg'(x) - \\frac{h^2}{2}g''(x) - i\\frac{h^3}{6}g'''(x) + \\mathcal{O}(h^4)\n    $$\n    Taking the imaginary part of both sides gives:\n    $$\n    \\operatorname{Im}(g(x+ih)) = h g'(x) - \\frac{h^3}{6}g'''(x) + \\mathcal{O}(h^5)\n    $$\n    Dividing by $h$ yields the approximation for the derivative:\n    $$\n    g'(x) = \\frac{\\operatorname{Im}(g(x+ih))}{h} + \\frac{h^2}{6}g'''(x) + \\mathcal{O}(h^4)\n    $$\n    The error of this approximation is of order $\\mathcal{O}(h^2)$, which is exceptionally small for a small step size $h$. Crucially, this method avoids the subtractive cancellation that plagues finite-difference methods, making it numerically stable even for extremely small $h$. We apply this to our function $f(E)$, which is constructed from holomorphic components.\n\nThe implementation will consist of:\n-   A Python class `ComplexDual` that overloads arithmetic operators to correctly implement the rules for dual numbers with complex components.\n-   A function to compute the derivative using AD by evaluating $f(E)$ with `ComplexDual` numbers.\n-   A function to compute the derivative using the CS formula by evaluating $f(E+ih)$ with standard complex numbers.\n-   A main loop that iterates through the provided test cases, computes the derivative using both methods, and calculates the absolute error $\\varepsilon(E,h) = \\left| \\frac{\\partial f}{\\partial E}|_{AD} - \\frac{\\partial f}{\\partial E}|_{CS} \\right|$.\n\nThe problem gives specific test cases designed to probe numerical behavior, such as on-resonance cancellation and sharp features from narrow-width resonances. The very small step sizes ($h$) for the CS method highlight its robustness against the round-off errors that would typically dominate finite-difference schemes. The AD result serves as the ground truth, as it is algorithmically exact.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements AD and Complex-Step differentiation to find the absolute error between them\n    for a given function from high-energy physics.\n    \"\"\"\n\n    class ComplexDual:\n        \"\"\"\n        A complex dual number representation for forward-mode automatic differentiation.\n        A number is represented as z = real + dual * ε, where ε^2 = 0.\n        Both `real` and `dual` components are complex numbers (np.complex128).\n        \"\"\"\n        def __init__(self, real, dual):\n            self.real = np.complex128(real)\n            self.dual = np.complex128(dual)\n\n        def __repr__(self):\n            return f\"ComplexDual(real={self.real}, dual={self.dual})\"\n\n        def __add__(self, other):\n            if not isinstance(other, ComplexDual):\n                other = ComplexDual(other, 0.0)\n            return ComplexDual(self.real + other.real, self.dual + other.dual)\n\n        def __radd__(self, other):\n            return self.__add__(other)\n\n        def __sub__(self, other):\n            if not isinstance(other, ComplexDual):\n                other = ComplexDual(other, 0.0)\n            return ComplexDual(self.real - other.real, self.dual - other.dual)\n\n        def __rsub__(self, other):\n            # other - self\n            if not isinstance(other, ComplexDual):\n                other = ComplexDual(other, 0.0)\n            return ComplexDual(other.real - self.real, other.dual - self.dual)\n\n        def __mul__(self, other):\n            if not isinstance(other, ComplexDual):\n                other = ComplexDual(other, 0.0)\n            # (a + bε) * (c + dε) = ac + (ad + bc)ε\n            return ComplexDual(self.real * other.real,\n                             self.real * other.dual + self.dual * other.real)\n        \n        def __rmul__(self, other):\n            return self.__mul__(other)\n\n        def __truediv__(self, other):\n            if not isinstance(other, ComplexDual):\n                other = ComplexDual(other, 0.0)\n            # D[f/g] = (f'g - fg') / g^2\n            # dual part: (self.dual * other.real - self.real * other.dual) / other.real**2\n            den_real_sq = other.real * other.real\n            return ComplexDual(self.real / other.real,\n                             (self.dual * other.real - self.real * other.dual) / den_real_sq)\n\n        def __rtruediv__(self, other):\n            # constant / self\n            const_val = np.complex128(other)\n            # D[c/g] = -c*g'/g^2\n            # dual part: -const * self.dual / self.real**2\n            den_real_sq = self.real * self.real\n            return ComplexDual(const_val / self.real,\n                             -const_val * self.dual / den_real_sq)\n\n    def get_ad_derivative(E_val, params):\n        \"\"\"Computes the derivative using forward-mode Automatic Differentiation.\"\"\"\n        E_dual = ComplexDual(E_val, 1.0)\n        total = ComplexDual(0.0, 0.0)\n        i = np.complex128(0, 1)\n    \n        for c_k, ctilde_k, m_k, Gamma_k in params:\n            # Denominators are functions of E, so they are dual numbers\n            den1 = E_dual - m_k + i * Gamma_k / 2.0\n            den2 = E_dual - m_k - i * Gamma_k / 2.0\n            \n            # Numerators are constants\n            term1 = c_k / den1\n            term2 = ctilde_k / den2\n            \n            total = total + term1 + term2\n        \n        # The derivative of a real function wrt a real variable is real.\n        return total.dual.real\n\n    def get_cs_derivative(E_val, h, params):\n        \"\"\"Computes the derivative using the complex-step method.\"\"\"\n        E_h = np.complex128(E_val, h)\n        total = np.complex128(0.0)\n        i = np.complex128(0, 1)\n\n        for c_k, ctilde_k, m_k, Gamma_k in params:\n            c_k_complex = np.complex128(c_k)\n            ctilde_k_complex = np.complex128(ctilde_k)\n\n            den1 = E_h - m_k + i * Gamma_k / 2.0\n            term1 = c_k_complex / den1\n            \n            den2 = E_h - m_k - i * Gamma_k / 2.0\n            term2 = ctilde_k_complex / den2\n            \n            total += term1 + term2\n        \n        return total.imag / h\n\n    test_cases = [\n        (91.1876, 1e-30, [(complex(1, 0), complex(1, -0), 91.1876, 2.4952)]),\n        (124.9, 1e-20, [\n            (complex(1, 5), complex(1, -5), 125.0, 4.0),\n            (complex(-0.8, 4), complex(-0.8, -4), 124.0, 3.0)\n        ]),\n        (500.0 + 0.5e-6, 1e-40, [(complex(2, 0), complex(2, -0), 500.0, 1e-6)])\n    ]\n    \n    results = []\n    for E, h, resonance_params in test_cases:\n        grad_ad = get_ad_derivative(E, resonance_params)\n        grad_cs = get_cs_derivative(E, h, resonance_params)\n        \n        error = abs(grad_ad - grad_cs)\n        results.append(error)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While forward-mode AD is intuitive, most large-scale applications in high-energy physics rely on the more memory-efficient reverse-mode AD. This practice  introduces the core component of reverse-mode AD: the vector-Jacobian product (VJP). You will tackle the crucial real-world task of designing and verifying a custom, numerically stable VJP for the log-sum-exp function, an operator central to likelihood-based inference, and demonstrate its superiority over a naive, unstable implementation.",
            "id": "3511336",
            "problem": "You are given the task of designing and verifying a custom vector-Jacobian product (VJP) for a numerically stable implementation of the log-sum-exp function that arises in computational high-energy physics (HEP) likelihoods. You will work in the context of differentiable programming and automatic differentiation for functions relevant to HEP mixture likelihoods. The goal is to construct a stable VJP for the log-sum-exp, verify its correctness using the definition of a VJP, and quantify the reduction in occurrences of undefined numerical values due to instability when compared to a naive differentiation rule.\n\nConsider an event-level mixture model with per-event scores represented as a matrix $S \\in \\mathbb{R}^{M \\times K}$, where row $j$ contains the values $S_{j,1},\\dots,S_{j,K}$ that enter a per-event log-marginalization through the log-sum-exp map. Define the row-wise log-sum-exp operator $F: \\mathbb{R}^{M \\times K} \\to \\mathbb{R}^{M}$ by applying, for each row $j$, the scalar function\n$$\n\\operatorname{LSE}(x_1,\\dots,x_K) \\equiv \\log\\left(\\sum_{i=1}^{K} \\exp(x_i)\\right).\n$$\nCompute $F(S)$ by applying $\\operatorname{LSE}$ to each row of $S$. In code, you must implement a numerically stable forward rule for $\\operatorname{LSE}$ using the standard shift-by-maximum technique, i.e., for each row $j$, let $m_j = \\max_i S_{j,i}$ and compute\n$$\n\\operatorname{LSE}_{\\text{stable}}(S_{j,\\cdot}) \\equiv m_j + \\log\\left(\\sum_{i=1}^{K} \\exp\\left(S_{j,i}-m_j\\right)\\right).\n$$\n\nYour tasks are:\n1. Implement the forward map $F(S)$ using the stable row-wise definition above.\n2. Design two VJP implementations for $F$ that map an output cotangent $c \\in \\mathbb{R}^M$ to an input cotangent of shape $\\mathbb{R}^{M \\times K}$:\n   - A naive VJP obtained by differentiating the non-shifted definition of $\\operatorname{LSE}$ directly.\n   - A stable VJP derived in a way that avoids intermediate numerical overflow and underflow by using the same row-wise shift $m_j$ that is used in the forward pass.\n   You must not reuse the target gradient formula explicitly given in the problem; instead, derive the VJP from first principles in your solution. The implementation must be fully vectorized over rows.\n3. Verify the stable VJP numerically by testing the fundamental definition of a VJP. For a general output cotangent $c \\in \\mathbb{R}^M$, define the scalar functional\n$$\n\\phi(S) \\equiv \\langle c, F(S) \\rangle = \\sum_{j=1}^{M} c_j \\operatorname{LSE}_{\\text{stable}}(S_{j,\\cdot}).\n$$\nFor a chosen perturbation direction $V \\in \\mathbb{R}^{M \\times K}$ and a small step size $\\varepsilon \\in \\mathbb{R}$, estimate the directional derivative using a central finite difference:\n$$\nD\\phi(S)[V] \\approx \\frac{\\phi(S+\\varepsilon V)-\\phi(S-\\varepsilon V)}{2\\varepsilon}.\n$$\nCheck that this equals the inner product between the candidate input cotangent from your VJP and $V$, i.e., verify\n$$\n\\left\\langle \\operatorname{VJP}_F(S)[c], V \\right\\rangle \\approx D\\phi(S)[V].\n$$\nQuantify the absolute difference between the two sides as a nonnegative scalar error for each test case.\n4. Quantify numerical stability by counting how many entries of each VJP output contain undefined values when implemented in finite precision arithmetic. For each test case, report the number of entries that are “not a number” (NaN) produced by the naive VJP, and the number produced by the stable VJP. Also report the fractional reduction computed as\n$$\nr \\equiv \\frac{n_{\\text{naive}} - n_{\\text{stable}}}{\\max(1, n_{\\text{naive}})}.\n$$\n\nTest Suite:\nUse the following five test cases. Each case provides a matrix $S \\in \\mathbb{R}^{M \\times K}$, an output cotangent vector $c \\in \\mathbb{R}^M$, and a perturbation direction $V \\in \\mathbb{R}^{M \\times K}$. Angles do not appear, and there are no physical units; all quantities are dimensionless.\n\n- Case 1 (general well-scaled values):\n  - $S = \\begin{bmatrix}\n  -1.2 & 0.3 & 2.0 & -0.7 & 1.5 \\\\\n  0.1 & -0.2 & 0.3 & -0.4 & 0.0 \\\\\n  2.5 & 1.5 & -3.0 & 0.5 & -0.1 \\\\\n  -4.0 & -0.3 & 0.9 & 1.2 & -2.1\n  \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 1.0 & -0.5 & 2.0 & 0.7 \\end{bmatrix}$,\n  - $V$ is the reshape of a linearly spaced vector from $-0.3$ to $0.3$ with $20$ entries into the shape of $S$.\n- Case 2 (large positive scales):\n  - $S = \\begin{bmatrix}\n  1000.0 & 1001.0 & 999.0 & 1005.0 & 1002.0 \\\\\n  800.0 & 805.0 & 790.0 & 795.0 & 810.0 \\\\\n  1200.0 & 1200.0 & 1200.0 & 1199.0 & 1201.0\n  \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 1.0 & 1.0 & 1.0 \\end{bmatrix}$,\n  - $V$ is the same shape as $S$ with every entry equal to $0.1$.\n- Case 3 (large negative scales):\n  - $S = \\begin{bmatrix}\n  -1000.0 & -1001.0 & -999.0 & -1005.0 & -1002.0 \\\\\n  -800.0 & -805.0 & -790.0 & -795.0 & -810.0 \\\\\n  -1200.0 & -1200.0 & -1200.0 & -1199.0 & -1201.0\n  \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 1.0 & 1.0 & 1.0 \\end{bmatrix}$,\n  - $V$ is the reshape of a linearly spaced vector from $-0.2$ to $0.2$ with $15$ entries into the shape of $S$.\n- Case 4 (mixed extremes):\n  - $S = \\begin{bmatrix}\n  -1000.0 & 0.0 & 1000.0 & -500.0 & 500.0 \\\\\n  500.0 & -500.0 & 0.0 & 1000.0 & -1000.0\n  \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 1.0 & -2.0 \\end{bmatrix}$,\n  - $V = \\begin{bmatrix}\n  0.05 & -0.1 & 0.2 & -0.05 & 0.1 \\\\\n  -0.02 & 0.03 & -0.04 & 0.05 & -0.06\n  \\end{bmatrix}$.\n- Case 5 (single-component rows as a boundary condition):\n  - $S = \\begin{bmatrix}\n  1000.0 \\\\ -1000.0 \\\\ 0.0\n  \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 0.3 & -0.4 & 1.2 \\end{bmatrix}$,\n  - $V = \\begin{bmatrix}\n  1.0 \\\\ -1.0 \\\\ 0.5\n  \\end{bmatrix}$.\n\nSet the finite difference step to $\\varepsilon = 10^{-6}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case must contribute a sublist of four values in the following order: the number of NaNs in the naive VJP output (an integer), the number of NaNs in the stable VJP output (an integer), the fractional reduction $r$ as defined above (a float), and the absolute error in the VJP verification for the stable rule (a float). The final line should look like\n$[ [\\dots], [\\dots], [\\dots], [\\dots], [\\dots] ]$\nbut without any spaces, i.e., strictly formatted as\n\"[[n1_naive,n1_stable,r1,e1],[n2_naive,n2_stable,r2,e2],...,[n5_naive,n5_stable,r5,e5]]\".",
            "solution": "The objective is to design, implement, and verify a numerically stable vector-Jacobian product (VJP) for the row-wise log-sum-exp (LSE) operator, a common function in computational physics and machine learning likelihoods. This involves deriving the VJP, testing its correctness against a finite-difference approximation, and quantifying the numerical stability improvements over a naive implementation.\n\nLet the operator be $F: \\mathbb{R}^{M \\times K} \\to \\mathbb{R}^{M}$, which applies the LSE function to each of the $M$ rows of an input matrix $S \\in \\mathbb{R}^{M \\times K}$. For a single row vector $x = (x_1, \\dots, x_K)$, the LSE function is defined as:\n$$\n\\operatorname{LSE}(x) = \\log\\left(\\sum_{i=1}^{K} \\exp(x_i)\\right)\n$$\n\nThe direct computation of this function is numerically unstable. For large positive values of $x_i$, $\\exp(x_i)$ can overflow standard floating-point representations. For large negative values, $\\exp(x_i)$ can underflow to zero, leading to $\\log(0)=-\\infty$ or a loss of precision if some values are not as large and negative. The standard mitigation is the shift-by-maximum technique. For each row $S_{j,\\cdot}$ of the matrix $S$, we find the maximum value $m_j = \\max_i S_{j,i}$ and compute:\n$$\ny_j = F(S)_{j} = \\operatorname{LSE}_{\\text{stable}}(S_{j,\\cdot}) = m_j + \\log\\left(\\sum_{i=1}^{K} \\exp(S_{j,i}-m_j)\\right)\n$$\nThis is mathematically equivalent to the original definition due to the identity $\\log(\\sum \\alpha e^{x_i}) = \\log(\\alpha) + \\log(\\sum e^{x_i})$, where we have used $\\alpha = e^{m_j}$. Numerically, the largest argument to $\\exp$ inside the sum is now $0$, which prevents overflow. It also ensures that the sum is at least $1$, preventing the argument to $\\log$ from being zero unless all inputs are $-\\infty$. This constitutes the implementation for Task 1.\n\nFor Task 2, we derive two VJP implementations. A VJP computes the action of the Jacobian-transpose on an output cotangent vector $c \\in \\mathbb{R}^M$. The result is a matrix of the same shape as the input $S$, representing the gradient of the scalar objective $\\phi(S) = \\langle c, F(S) \\rangle = \\sum_{j=1}^M c_j F(S)_j$ with respect to $S$. The $(j, k)$-th component of the VJP is $(\\operatorname{VJP}_F(S)[c])_{jk} = \\frac{\\partial \\phi}{\\partial S_{jk}} = c_j \\frac{\\partial F(S)_j}{\\partial S_{jk}}$.\n\nFirst, we derive the naive VJP. The partial derivative of the unshifted LSE function for row $j$ with respect to an element $S_{jk}$ is:\n$$\n\\frac{\\partial y_j}{\\partial S_{jk}} = \\frac{\\partial}{\\partial S_{jk}} \\log\\left(\\sum_{i=1}^{K} \\exp(S_{j,i})\\right) = \\frac{1}{\\sum_{i=1}^{K} \\exp(S_{j,i})} \\cdot \\exp(S_{jk}) = \\frac{\\exp(S_{jk})}{\\sum_{i=1}^{K} \\exp(S_{j,i})}\n$$\nThis expression is the softmax function applied to the row vector $S_{j,\\cdot}$. The naive VJP is therefore computed row-wise as:\n$$\n(\\operatorname{VJP}_{\\text{naive}}(S)[c])_{jk} = c_j \\cdot \\operatorname{softmax}(S_{j,\\cdot})_k\n$$\nThis calculation inherits the numerical instabilities of the naive LSE forward pass, as it involves computing $\\exp(S_{jk})$ directly, which can overflow, and $\\sum_i \\exp(S_{j,i})$, which can underflow to zero, leading to division by zero (`inf`) or indeterminate forms (`nan`).\n\nSecond, we derive the stable VJP from the stable LSE definition. Let $y_j = m_j + \\log(\\sum_i \\exp(S_{j,i}-m_j))$ with $m_j = \\max_i S_{j,i}$. The partial derivative of $m_j$ with respect to $S_{jk}$ is $1$ if $S_{jk}$ is a unique maximum and $0$ otherwise (this is the gradient a.e.). Using the chain rule:\n$$\n\\frac{\\partial y_j}{\\partial S_{jk}} = \\frac{\\partial m_j}{\\partial S_{jk}} + \\frac{\\partial}{\\partial S_{jk}} \\left[ \\log\\left(\\sum_{i=1}^{K} \\exp(S_{j,i}-m_j)\\right) \\right]\n$$\nThe second term expands to:\n$$\n\\frac{1}{\\sum_i \\exp(S_{j,i}-m_j)} \\cdot \\sum_{i=1}^{K} \\left( \\exp(S_{j,i}-m_j) \\cdot \\frac{\\partial}{\\partial S_{jk}}(S_{j,i}-m_j) \\right)\n$$\n$$\n= \\frac{1}{\\sum_i \\exp(S_{j,i}-m_j)} \\cdot \\sum_{i=1}^{K} \\left( \\exp(S_{j,i}-m_j) \\cdot (\\delta_{ik} - \\frac{\\partial m_j}{\\partial S_{jk}}) \\right)\n$$\n$$\n= \\frac{\\exp(S_{jk}-m_j)}{\\sum_i \\exp(S_{j,i}-m_j)} - \\frac{\\sum_i \\exp(S_{j,i}-m_j)}{\\sum_i \\exp(S_{j,i}-m_j)} \\cdot \\frac{\\partial m_j}{\\partial S_{jk}} = \\operatorname{softmax}(S_{j,\\cdot}-m_j)_k - \\frac{\\partial m_j}{\\partial S_{jk}}\n$$\nSubstituting this back into the expression for $\\frac{\\partial y_j}{\\partial S_{jk}}$:\n$$\n\\frac{\\partial y_j}{\\partial S_{jk}} = \\frac{\\partial m_j}{\\partial S_{jk}} + \\left( \\operatorname{softmax}(S_{j,\\cdot}-m_j)_k - \\frac{\\partial m_j}{\\partial S_{jk}} \\right) = \\operatorname{softmax}(S_{j,\\cdot}-m_j)_k\n$$\nRemarkably, the terms involving the derivative of the max function cancel. The gradient of the stable LSE is the softmax of the mean-subtracted inputs. This form is numerically stable as the arguments to $\\exp$ are bounded above by $0$. The stable VJP is:\n$$\n(\\operatorname{VJP}_{\\text{stable}}(S)[c])_{jk} = c_j \\cdot \\frac{\\exp(S_{jk}-m_j)}{\\sum_i \\exp(S_{j,i}-m_j)}\n$$\n\nFor Task 3, we verify the correctness of the stable VJP. The definition of a VJP implies that for any perturbation vector $V$, the directional derivative of the scalar objective $\\phi(S) = \\langle c, F(S) \\rangle$ in the direction of $V$ is equal to the inner product of the VJP with $V$. We test this identity:\n$$\nD\\phi(S)[V] = \\langle \\nabla_S \\phi(S), V \\rangle = \\left\\langle \\operatorname{VJP}_F(S)[c], V \\right\\rangle\n$$\nThe directional derivative is approximated using a central finite difference with a small step size $\\varepsilon = 10^{-6}$:\n$$\nD\\phi(S)[V] \\approx \\frac{\\phi(S+\\varepsilon V)-\\phi(S-\\varepsilon V)}{2\\varepsilon}\n$$\nThe absolute difference between the finite-difference approximation and the VJP-based inner product $\\sum_{j,k} (\\operatorname{VJP}_{\\text{stable}})_{jk} V_{jk}$ quantifies the implementation error.\n\nFor Task 4, numerical stability is assessed by counting the number of `NaN` (not a number) entries in the VJP results for both the naive ($n_{\\text{naive}}$) and stable ($n_{\\text{stable}}$) methods. The fractional reduction in these undefined values, $r = (n_{\\text{naive}} - n_{\\text{stable}})/\\max(1, n_{\\text{naive}})$, measures the improvement. We expect $n_{\\text{stable}}$ to be zero for all valid finite inputs, and $n_{\\text{naive}}$ to be non-zero for inputs with large positive or negative magnitudes, resulting in $r=1.0$ in such cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the problem of designing, verifying, and comparing VJPs for the LSE function.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([\n                [-1.2, 0.3, 2.0, -0.7, 1.5],\n                [0.1, -0.2, 0.3, -0.4, 0.0],\n                [2.5, 1.5, -3.0, 0.5, -0.1],\n                [-4.0, -0.3, 0.9, 1.2, -2.1]\n            ]),\n            np.array([1.0, -0.5, 2.0, 0.7]),\n            np.linspace(-0.3, 0.3, 20).reshape(4, 5)\n        ),\n        (\n            np.array([\n                [1000.0, 1001.0, 999.0, 1005.0, 1002.0],\n                [800.0, 805.0, 790.0, 795.0, 810.0],\n                [1200.0, 1200.0, 1200.0, 1199.0, 1201.0]\n            ]),\n            np.array([1.0, 1.0, 1.0]),\n            np.full((3, 5), 0.1)\n        ),\n        (\n            np.array([\n                [-1000.0, -1001.0, -999.0, -1005.0, -1002.0],\n                [-800.0, -805.0, -790.0, -795.0, -810.0],\n                [-1200.0, -1200.0, -1200.0, -1199.0, -1201.0]\n            ]),\n            np.array([1.0, 1.0, 1.0]),\n            np.linspace(-0.2, 0.2, 15).reshape(3, 5)\n        ),\n        (\n            np.array([\n                [-1000.0, 0.0, 1000.0, -500.0, 500.0],\n                [500.0, -500.0, 0.0, 1000.0, -1000.0]\n            ]),\n            np.array([1.0, -2.0]),\n            np.array([\n                [0.05, -0.1, 0.2, -0.05, 0.1],\n                [-0.02, 0.03, -0.04, 0.05, -0.06]\n            ])\n        ),\n        (\n            np.array([\n                [1000.0],\n                [-1000.0],\n                [0.0]\n            ]),\n            np.array([0.3, -0.4, 1.2]),\n            np.array([\n                [1.0],\n                [-1.0],\n                [0.5]\n            ])\n        )\n    ]\n    \n    epsilon = 1e-6\n\n    def F_stable(S):\n        \"\"\"Task 1: Numerically stable forward map F(S).\"\"\"\n        if S.shape[1] == 0:\n            return np.full(S.shape[0], -np.inf)\n        max_S = np.max(S, axis=1, keepdims=True)\n        # The result is -inf if the entire row is -inf, handled correctly by numpy\n        # as max is -inf, log(sum(exp(S-max))) = log(sum(exp(nan))) = nan.\n        # But exp(-inf) -> 0, so sum is 0, log(0) is -inf. max + log(sum) = -inf-inf = -inf. Correct.\n        log_sum_exp = max_S + np.log(np.sum(np.exp(S - max_S), axis=1, keepdims=True))\n        return log_sum_exp.flatten()\n\n    def vjp_naive(S, c):\n        \"\"\"Task 2a: Naive VJP implementation.\"\"\"\n        with np.errstate(over='ignore', invalid='ignore'):\n            exp_S = np.exp(S)\n            sum_exp_S = np.sum(exp_S, axis=1, keepdims=True)\n            softmax_naive = exp_S / sum_exp_S\n            vjp = c[:, np.newaxis] * softmax_naive\n        return vjp\n\n    def vjp_stable(S, c):\n        \"\"\"Task 2b: Stable VJP implementation.\"\"\"\n        if S.shape[1] == 0:\n            return np.zeros_like(S)\n        max_S = np.max(S, axis=1, keepdims=True)\n        S_shifted = S - max_S\n        exp_S_shifted = np.exp(S_shifted)\n        sum_exp_S_shifted = np.sum(exp_S_shifted, axis=1, keepdims=True)\n        # This division is safe from overflow and underflow leading to 0/0\n        softmax_stable = exp_S_shifted / sum_exp_S_shifted\n        vjp = c[:, np.newaxis] * softmax_stable\n        return vjp\n\n    def phi(S, c):\n        \"\"\"Task 3: Scalar functional for verification.\"\"\"\n        return np.dot(c, F_stable(S))\n\n    results = []\n    for S, c, V in test_cases:\n        # Task 4: Quantify numerical stability\n        vjp_n_output = vjp_naive(S, c)\n        n_naive = np.isnan(vjp_n_output).sum()\n        \n        vjp_s_output = vjp_stable(S, c)\n        n_stable = np.isnan(vjp_s_output).sum()\n        \n        r = (n_naive - n_stable) / max(1, n_naive)\n\n        # Task 3: Verify the stable VJP and quantify error\n        fd_approx = (phi(S + epsilon * V, c) - phi(S - epsilon * V, c)) / (2 * epsilon)\n        vjp_dot_V = np.sum(vjp_s_output * V)\n        error = np.abs(fd_approx - vjp_dot_V)\n\n        results.append([int(n_naive), int(n_stable), float(r), float(error)])\n    \n    # Format the final output string exactly as requested, without spaces.\n    inner_parts = [f\"[{','.join(map(str, r))}]\" for r in results]\n    final_output_str = f\"[{','.join(inner_parts)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Many challenges in high-energy physics involve optimizing systems with inherent randomness, such as event simulators. This requires differentiating expectations, a task that goes beyond standard AD on deterministic functions. This exercise  explores the two dominant techniques for this purpose: the pathwise (reparameterization) and score-function gradient estimators. By implementing both and comparing their statistical bias and variance, you will gain crucial insights into their respective strengths and weaknesses, a fundamental consideration for building and training differentiable probabilistic models.",
            "id": "3511433",
            "problem": "Consider a differentiable programming task in computational high-energy physics in which an observable $f(x)$ is evaluated on simulated event kinematics $x$, and the simulator depends on a tunable parameter $\\theta$. Two gradient estimators for the sensitivity of the expected observable to the simulator parameter are commonly used: the pathwise (reparameterization) estimator and the score-function estimator. Your goal is to implement a program that empirically compares their bias and variance under heavy-tailed sampling, using a mathematically well-defined setup and reporting quantitative results.\n\nFoundational base and setup:\n- Let $x$ be generated from a location-scale Student's $t$ distribution with degrees of freedom $\\nu$, location parameter $\\mu$ (the parameter of interest, denoted $\\theta \\equiv \\mu$), and scale $\\sigma$. That is, draw $z \\sim \\mathrm{StudentT}(\\nu)$ and set $x = \\mu + \\sigma z$. The probability density function of the standard Student's $t$ distribution is\n$$\np_{\\nu}(z) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}} \\left(1 + \\frac{z^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}.\n$$\n- Consider the observable\n$$\nf(x) = \\log\\left(1 + x^2\\right).\n$$\nThis observable is continuous, differentiable, and its derivative $f'(x)$ is bounded, which is suitable for heavy-tailed sampling.\n- The target sensitivity is the gradient\n$$\n\\nabla_{\\mu} \\mathbb{E}_{x \\sim p_{\\mu,\\sigma,\\nu}}\\left[f(x)\\right],\n$$\nwhere $p_{\\mu,\\sigma,\\nu}$ denotes the location-scale $t$ distribution for $x$ induced by $z \\sim \\mathrm{StudentT}(\\nu)$ and $x = \\mu + \\sigma z$.\n\nPrinciple-based gradient estimation:\n- Pathwise (reparameterization) estimator: use the fact that, under the above reparameterization, the derivative with respect to $\\mu$ passes through the expectation over $z$ to give an estimator based on $f'(x)$ evaluated at $x = \\mu + \\sigma z$.\n- Score-function estimator: use the identity (from statistical estimation theory) that, for suitable regularity conditions,\n$$\n\\nabla_{\\mu} \\mathbb{E}_{x \\sim p_{\\mu,\\sigma,\\nu}}\\left[f(x)\\right] = \\mathbb{E}_{x \\sim p_{\\mu,\\sigma,\\nu}}\\left[f(x)\\,\\nabla_{\\mu}\\log p_{\\mu,\\sigma,\\nu}(x)\\right],\n$$\nwhere $\\nabla_{\\mu}\\log p_{\\mu,\\sigma,\\nu}(x)$ is the derivative of the log-density with respect to $\\mu$. Starting from the provided density of the standard $t$ distribution, derive $\\nabla_{\\mu}\\log p_{\\mu,\\sigma,\\nu}(x)$ for the location parameter $\\mu$ in a location-scale family.\n\nBias and variance quantification protocol:\n- For each estimator, perform $R$ independent replicates. In each replicate, draw $N$ independent samples from $z \\sim \\mathrm{StudentT}(\\nu)$, construct $x = \\mu + \\sigma z$, and compute the corresponding single-replicate Monte Carlo gradient estimate:\n    - Pathwise estimator single-replicate estimate: the sample average of $f'(x)$.\n    - Score-function estimator single-replicate estimate: the sample average of $f(x)\\,\\nabla_{\\mu}\\log p_{\\mu,\\sigma,\\nu}(x)$.\n- Compute an accurate reference value for the true gradient using numerical integration over $z$:\n$$\ng_{\\mathrm{true}}(\\mu,\\sigma,\\nu) = \\int_{-\\infty}^{\\infty} f'\\big(\\mu + \\sigma z\\big)\\,p_{\\nu}(z)\\,\\mathrm{d}z.\n$$\nThis reduces the reference gradient to a one-dimensional integral over the standard $t$ density, which can be evaluated to high precision with numerical quadrature.\n- For each estimator, compute:\n    - The empirical bias as the difference between the mean of the $R$ replicate estimates and $g_{\\mathrm{true}}(\\mu,\\sigma,\\nu)$.\n    - The empirical variance as the unbiased sample variance across the $R$ replicate estimates.\n\nTest suite:\nUse the following parameter sets to probe heavy tails, symmetry, and light tails:\n- Case $1$ (heavy tails, nonzero shift): $\\nu = 2.5$, $\\mu = 1.0$, $\\sigma = 1.0$, $N = 4000$, $R = 200$, base seed $= 123456$.\n- Case $2$ (moderate tails, symmetry at zero): $\\nu = 5.0$, $\\mu = 0.0$, $\\sigma = 1.0$, $N = 4000$, $R = 200$, base seed $= 234567$.\n- Case $3$ (light tails near Gaussian): $\\nu = 30.0$, $\\mu = -0.5$, $\\sigma = 0.5$, $N = 4000$, $R = 200$, base seed $= 345678$.\n\nRequired outputs and format:\n- For each case in the above order, compute and return four floating-point numbers:\n    - Bias of the pathwise estimator.\n    - Variance of the pathwise estimator.\n    - Bias of the score-function estimator.\n    - Variance of the score-function estimator.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the sequence corresponding to Case $1$ followed by Case $2$ followed by Case $3$, with each case contributing its four values in the order specified above. For example, the output format is\n$$\n[\\text{bias\\_pathwise\\_case1},\\text{var\\_pathwise\\_case1},\\text{bias\\_score\\_case1},\\text{var\\_score\\_case1},\\text{bias\\_pathwise\\_case2},\\ldots,\\text{var\\_score\\_case3}],\n$$\nwhere each placeholder is a floating-point number.\n\nNo physical units or angle units are involved in this problem; all quantities are dimensionless. Ensure scientific realism by using the provided foundation and by deriving any needed expressions, such as $\\nabla_{\\mu}\\log p_{\\mu,\\sigma,\\nu}(x)$, from first principles of probability and calculus rather than shortcut formulas. The program must be self-contained, require no input, and use the specified libraries.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique, verifiable solution. It requires the implementation and comparison of two fundamental gradient estimation techniques in statistics and machine learning—the pathwise estimator and the score-function estimator—within a precisely defined mathematical framework. We will proceed with a solution.\n\nThe objective is to compute the bias and variance of two estimators for the gradient $\\nabla_{\\mu} \\mathbb{E}[f(x)]$, where the expectation is over $x \\sim p_{\\mu,\\sigma,\\nu}(x)$, a location-scale Student's $t$-distribution. The parameter of interest is the location $\\mu$. The observable is $f(x) = \\log(1 + x^2)$.\n\nFirst, we must derive the analytical expressions for the quantities used in each estimator and for the ground-truth reference value.\n\n### 1. True Gradient Value\n\nThe true gradient, which serves as our reference value $g_{\\mathrm{true}}$, can be calculated by leveraging the reparameterization $x = \\mu + \\sigma z$, where $z$ is a random variable drawn from the standard Student's $t$-distribution, $z \\sim \\mathrm{StudentT}(\\nu)$. This allows us to move the derivative with respect to $\\mu$ inside the expectation over the base distribution of $z$:\n$$\ng_{\\mathrm{true}} = \\nabla_{\\mu} \\mathbb{E}_{x \\sim p_{\\mu,\\sigma,\\nu}}[f(x)] = \\nabla_{\\mu} \\mathbb{E}_{z \\sim p_{\\nu}}[f(\\mu + \\sigma z)] = \\mathbb{E}_{z \\sim p_{\\nu}}[\\nabla_{\\mu} f(\\mu + \\sigma z)]\n$$\nBy the chain rule, $\\nabla_{\\mu} f(\\mu + \\sigma z) = f'(\\mu + \\sigma z) \\cdot \\frac{\\partial}{\\partial \\mu}(\\mu + \\sigma z) = f'(\\mu + \\sigma z) \\cdot 1$. The derivative of the observable $f(x) = \\log(1+x^2)$ is $f'(x) = \\frac{2x}{1+x^2}$.\nSubstituting this into the expectation gives the integral form of the true gradient:\n$$\ng_{\\mathrm{true}} = \\mathbb{E}_{z \\sim p_{\\nu}}[f'(\\mu+\\sigma z)] = \\int_{-\\infty}^{\\infty} f'(\\mu + \\sigma z) p_{\\nu}(z) \\mathrm{d}z\n$$\nwhere $p_{\\nu}(z)$ is the probability density function (PDF) of the standard Student's $t$-distribution with $\\nu$ degrees of freedom, given as:\n$$\np_{\\nu}(z) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}} \\left(1 + \\frac{z^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}\n$$\nThis one-dimensional integral can be computed to high precision using numerical quadrature, for instance, with `scipy.integrate.quad`.\n\nA special case arises when $\\mu=0$. The integrand becomes $f'(\\sigma z)p_{\\nu}(z) = \\frac{2\\sigma z}{1+(\\sigma z)^2} p_{\\nu}(z)$. Since $p_{\\nu}(z)$ is an even function of $z$ and $\\frac{2\\sigma z}{1+(\\sigma z)^2}$ is an odd function of $z$, their product is an odd function. The integral of an odd function over the symmetric interval $(-\\infty, \\infty)$ is zero. Thus, for Case 2 where $\\mu=0.0$, we have $g_{\\mathrm{true}} = 0$.\n\n### 2. Pathwise Estimator\n\nThe pathwise estimator, also known as the reparameterization trick, is directly derived from the expression for the true gradient. The Monte Carlo estimate is obtained by drawing $N$ samples $z_i \\sim \\mathrm{StudentT}(\\nu)$, constructing $x_i = \\mu + \\sigma z_i$, and averaging the term inside the expectation:\n$$\n\\hat{g}_{\\mathrm{path}} = \\frac{1}{N} \\sum_{i=1}^{N} f'(x_i) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{2x_i}{1 + x_i^2}\n$$\nThis estimator is generally low-variance, particularly when $f'(x)$ is bounded, which is true in our case as $|\\frac{2x}{1+x^2}| \\le 1$ for all real $x$.\n\n### 3. Score-Function Estimator\n\nThe score-function estimator (also known as the REINFORCE estimator) uses the log-derivative trick, which states that $\\nabla_{\\theta} p(x; \\theta) = p(x; \\theta) \\nabla_{\\theta} \\log p(x; \\theta)$. This yields:\n$$\n\\nabla_{\\mu} \\mathbb{E}_{x \\sim p_{\\mu,\\sigma,\\nu}}[f(x)] = \\mathbb{E}_{x \\sim p_{\\mu,\\sigma,\\nu}}[f(x) \\nabla_{\\mu} \\log p_{\\mu,\\sigma,\\nu}(x)]\n$$\nTo use this, we must derive the score function, $\\nabla_{\\mu} \\log p_{\\mu,\\sigma,\\nu}(x)$. The PDF of the location-scale Student's $t$-distribution is obtained by a change of variables from $z$ to $x$:\n$$\np_{\\mu,\\sigma,\\nu}(x) = \\frac{1}{\\sigma}p_{\\nu}\\left(\\frac{x-\\mu}{\\sigma}\\right)\n$$\nThe log-PDF is:\n$$\n\\log p_{\\mu,\\sigma,\\nu}(x) = -\\log \\sigma + \\log p_{\\nu}\\left(\\frac{x-\\mu}{\\sigma}\\right)\n$$\nDifferentiating with respect to $\\mu$:\n$$\n\\nabla_{\\mu} \\log p_{\\mu,\\sigma,\\nu}(x) = \\frac{\\partial}{\\partial \\mu} \\log p_{\\nu}\\left(\\frac{x-\\mu}{\\sigma}\\right) = \\frac{p'_{\\nu}(\\frac{x-\\mu}{\\sigma})}{p_{\\nu}(\\frac{x-\\mu}{\\sigma})} \\cdot \\frac{\\partial}{\\partial \\mu}\\left(\\frac{x-\\mu}{\\sigma}\\right) = \\left. \\frac{\\mathrm{d}}{\\mathrm{d}z}\\log p_{\\nu}(z) \\right|_{z=\\frac{x-\\mu}{\\sigma}} \\cdot \\left(-\\frac{1}{\\sigma}\\right)\n$$\nFrom the definition of $p_{\\nu}(z)$, we have $\\log p_{\\nu}(z) = C - \\frac{\\nu+1}{2}\\log\\left(1 + \\frac{z^2}{\\nu}\\right)$, where $C$ is a constant. The derivative is:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}z}\\log p_{\\nu}(z) = -\\frac{\\nu+1}{2} \\cdot \\frac{1}{1 + z^2/\\nu} \\cdot \\frac{2z}{\\nu} = -\\frac{(\\nu+1)z}{\\nu+z^2}\n$$\nSubstituting this back, and letting $z = (x-\\mu)/\\sigma$:\n$$\n\\nabla_{\\mu} \\log p_{\\mu,\\sigma,\\nu}(x) = \\left( -\\frac{(\\nu+1)z}{\\nu+z^2} \\right) \\cdot \\left(-\\frac{1}{\\sigma}\\right) = \\frac{(\\nu+1)z}{\\sigma(\\nu+z^2)}\n$$\nThe single-replicate Monte Carlo estimate is therefore:\n$$\n\\hat{g}_{\\mathrm{score}} = \\frac{1}{N} \\sum_{i=1}^{N} f(x_i) \\nabla_{\\mu} \\log p_{\\mu,\\sigma,\\nu}(x_i) = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\log(1+x_i^2) \\cdot \\frac{(\\nu+1)z_i}{\\sigma(\\nu+z_i^2)} \\right]\n$$\nwhere $z_i = (x_i-\\mu)/\\sigma$. This form, expressed in terms of $z_i$, is computationally efficient and numerically stable.\n\n### 4. Empirical Bias and Variance\n\nFollowing the problem protocol, for each estimator (pathwise and score-function), we generate $R$ independent replicate estimates, denoted $\\{\\hat{g}_1, \\dots, \\hat{g}_R\\}$.\n- The empirical mean is $\\bar{g} = \\frac{1}{R}\\sum_{r=1}^R \\hat{g}_r$.\n- The empirical bias is calculated as $\\text{Bias} = \\bar{g} - g_{\\mathrm{true}}$.\n- The empirical variance is the unbiased sample variance: $\\text{Var} = \\frac{1}{R-1}\\sum_{r=1}^R (\\hat{g}_r - \\bar{g})^2$.\n\nThese steps will be implemented for each of the three test cases specified in the problem statement. The use of fixed seeds for the random number generator ensures reproducibility.",
            "answer": "```python\nimport numpy as np\nfrom scipy import integrate, stats\n\ndef solve():\n    \"\"\"\n    Main function to run the comparison of gradient estimators for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (nu, mu, sigma, N, R, seed)\n        (2.5, 1.0, 1.0, 4000, 200, 123456),   # Case 1: heavy tails, nonzero shift\n        (5.0, 0.0, 1.0, 4000, 200, 234567),   # Case 2: moderate tails, symmetry\n        (30.0, -0.5, 0.5, 4000, 200, 345678), # Case 3: light tails, near Gaussian\n    ]\n\n    all_results = []\n    for case in test_cases:\n        nu, mu, sigma, N, R, seed = case\n        results = calculate_case_metrics(nu, mu, sigma, N, R, seed)\n        all_results.extend(results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef calculate_case_metrics(nu, mu, sigma, N, R, seed):\n    \"\"\"\n    Calculates bias and variance for pathwise and score-function estimators\n    for a single parameter configuration.\n\n    Args:\n        nu (float): Degrees of freedom for Student's t-distribution.\n        mu (float): Location parameter.\n        sigma (float): Scale parameter.\n        N (int): Number of samples per replicate.\n        R (int): Number of replicates.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        list: A list of four floats [bias_path, var_path, bias_score, var_score].\n    \"\"\"\n    # 1. Calculate the true gradient reference value using numerical quadrature.\n    if mu == 0.0:\n        # For mu=0, the integrand is an odd function, so the integral over\n        # a symmetric domain is zero.\n        g_true = 0.0\n    else:\n        # Define the integrand: f'(mu + sigma*z) * p_nu(z)\n        integrand = lambda z: (2 * (mu + sigma * z) / (1 + (mu + sigma * z)**2)) * stats.t.pdf(z, df=nu)\n        # Perform numerical integration\n        g_true, _ = integrate.quad(integrand, -np.inf, np.inf, epsabs=1e-12, epsrel=1e-12)\n\n    # 2. Perform Monte Carlo simulation for R replicates.\n    rng = np.random.default_rng(seed)\n    path_estimates = np.zeros(R)\n    score_estimates = np.zeros(R)\n\n    for r in range(R):\n        # Generate N samples from the standard Student's t-distribution.\n        z = rng.standard_t(nu, size=N)\n        # Transform samples to the location-scale family.\n        x = mu + sigma * z\n\n        # Pathwise estimator calculation for this replicate\n        # Estimate is the sample mean of f'(x)\n        df_dx = 2 * x / (1 + x**2)\n        path_estimates[r] = np.mean(df_dx)\n\n        # Score-function estimator calculation for this replicate\n        # Estimate is the sample mean of f(x) * grad_mu(log p(x))\n        f_x = np.log(1 + x**2)\n        score_log_p = (nu + 1) * z / (sigma * (nu + z**2))\n        score_samples = f_x * score_log_p\n        score_estimates[r] = np.mean(score_samples)\n\n    # 3. Compute empirical bias and variance for each estimator.\n    \n    # Pathwise estimator metrics\n    mean_path = np.mean(path_estimates)\n    bias_path = mean_path - g_true\n    var_path = np.var(path_estimates, ddof=1) # Unbiased sample variance\n\n    # Score-function estimator metrics\n    mean_score = np.mean(score_estimates)\n    bias_score = mean_score - g_true\n    var_score = np.var(score_estimates, ddof=1) # Unbiased sample variance\n\n    return [bias_path, var_path, bias_score, var_score]\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}