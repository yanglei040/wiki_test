{
    "hands_on_practices": [
        {
            "introduction": "The ability to compute gradients of physical observables with respect to model parameters is the cornerstone of modern, large-scale optimization in high-energy physics. This first exercise provides a foundational hands-on practice, connecting a key theoretical construct—the Wilson action from lattice gauge theory—to the essential tasks of analytical differentiation and numerical verification. By manually deriving the gradient of the action and checking it against a finite-difference approximation , you will build the fundamental workflow and intuition required for tackling more complex differentiable programs where analytical derivatives are intractable.",
            "id": "3511484",
            "problem": "Consider a pure gauge lattice theory in two Euclidean dimensions with the Special Unitary group of degree two (SU(2)) as gauge group. Let the lattice be periodic in both directions with sizes $L_x=L_y=3$, and directions labeled by $\\mu \\in \\{0,1\\}$. At each lattice site $x=(n_x,n_y)$ with $n_x \\in \\{0,1,2\\}$ and $n_y \\in \\{0,1,2\\}$, associate link variables $U_\\mu(x) \\in \\mathrm{SU}(2)$ on the link from $x$ to $x+\\hat{\\mu}$, with periodic boundary conditions. The elementary plaquette at site $x$ in the $(\\mu,\\nu)$-plane for $\\mu\\nu$ is defined as\n$$\nU_{\\mu\\nu}(x) = U_\\mu(x)\\,U_\\nu(x+\\hat{\\mu})\\,U_\\mu(x+\\hat{\\nu})^\\dagger\\,U_\\nu(x)^\\dagger.\n$$\nDefine the Wilson plaquette action\n$$\nS[U;\\beta(\\theta)] = \\beta(\\theta)\\sum_{x}\\sum_{\\mu\\nu}\\left(1 - \\frac{1}{N}\\,\\Re\\operatorname{Tr}\\,U_{\\mu\\nu}(x)\\right),\n$$\nwhere $N=2$, $\\Re$ denotes the real part, and $\\operatorname{Tr}$ is the matrix trace. The coupling is a differentiable scalar function of a real parameter $\\theta$, given by\n$$\n\\beta(\\theta) = e^{\\theta} + \\theta^{2}.\n$$\nYour tasks are:\n- Starting only from core definitions in multivariable calculus and the above action definition, derive an expression for the gradient $\\frac{dS}{d\\theta}$.\n- Implement a program that constructs a fixed, reproducible lattice configuration and numerically verifies the analytic gradient against a central finite-difference approximation.\n\nImplementation requirements:\n- Lattice and group:\n  - Use a two-dimensional periodic lattice of size $L_x=L_y=3$.\n  - Use $\\mathrm{SU}(2)$ link variables represented as $2\\times 2$ complex matrices.\n  - Generate each link $U_\\mu(x)$ by first drawing a four-component real vector $(a_0,a_1,a_2,a_3)$ with independent standard normal entries, normalizing it to unit length, and mapping it to $\\mathrm{SU}(2)$ via the quaternionic parameterization\n    $$\n    U = a_0\\,\\mathbf{1}_{2} + i\\,(a_1\\,\\sigma_1 + a_2\\,\\sigma_2 + a_3\\,\\sigma_3),\n    $$\n    where $\\sigma_1,\\sigma_2,\\sigma_3$ are the Pauli matrices and $\\mathbf{1}_2$ is the $2\\times 2$ identity. Use a fixed pseudorandom number generator seed $12345$ to ensure reproducibility.\n- Action evaluation:\n  - Use the definition of $U_{\\mu\\nu}(x)$ above with periodic boundary conditions.\n  - Compute $S[U;\\beta(\\theta)]$ exactly as given, in IEEE $64$-bit floating-point arithmetic.\n- Analytic gradient:\n  - Derive $\\frac{dS}{d\\theta}$ from first principles, without assuming any result beyond the chain rule and linearity of differentiation.\n- Finite-difference check:\n  - For a given $\\theta$ and a step size $h>0$, compute the central finite-difference estimate\n    $$\n    g_{\\mathrm{FD}}(\\theta;h) = \\frac{S[U;\\beta(\\theta+h)] - S[U;\\beta(\\theta-h)]}{2h}.\n    $$\n- Comparison:\n  - For each test case, compute the absolute difference between the analytic gradient and the finite-difference estimate,\n    $$\n    \\Delta(\\theta,h) = \\left|\\frac{dS}{d\\theta} - g_{\\mathrm{FD}}(\\theta;h)\\right|.\n    $$\n\nTest suite:\n- Use the single fixed lattice configuration generated as specified above for all test cases.\n- Evaluate $\\Delta(\\theta,h)$ for the following $(\\theta,h)$ pairs:\n  - $(\\theta,h)=(0.0,10^{-6})$,\n  - $(\\theta,h)=(0.5,10^{-6})$,\n  - $(\\theta,h)=(-1.0,10^{-6})$,\n  - $(\\theta,h)=(2.0,10^{-6})$,\n  - $(\\theta,h)=(-3.0,10^{-6})$,\n  - $(\\theta,h)=(0.3,10^{-8})$.\n- All quantities are dimensionless; no physical units are required.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite above. Each list element must be a floating-point number corresponding to $\\Delta(\\theta,h)$ for that test case. For example, an output could look like\n$$\n[\\Delta(\\theta_1,h_1),\\Delta(\\theta_2,h_2),\\ldots,\\Delta(\\theta_6,h_6)].\n$$",
            "solution": "The problem is well-posed, scientifically sound, and contains all necessary information for a complete solution. It presents a standard exercise in computational lattice gauge theory. The task is to derive the gradient of the Wilson action with respect to a parameter in the coupling constant and then to implement a numerical verification.\n\nThe Wilson plaquette action $S$ is given as a function of the lattice configuration $U$ and a parameter $\\theta$:\n$$\nS[U;\\beta(\\theta)] = \\beta(\\theta)\\sum_{x}\\sum_{\\mu\\nu}\\left(1 - \\frac{1}{N}\\,\\Re\\operatorname{Tr}\\,U_{\\mu\\nu}(x)\\right)\n$$\nThe lattice configuration, consisting of the set of all link variables $\\{U_\\mu(x)\\}$, is generated once and is held fixed. Therefore, it has no dependence on the parameter $\\theta$. The only dependence of the action $S$ on $\\theta$ is through the coupling function $\\beta(\\theta)$.\n\nTo simplify the notation, let us define the quantity $W$ as the total plaquette sum, which is a functional of the fixed lattice configuration $U$ and is independent of $\\theta$:\n$$\nW[U] = \\sum_{x}\\sum_{\\mu\\nu}\\left(1 - \\frac{1}{N}\\,\\Re\\operatorname{Tr}\\,U_{\\mu\\nu}(x)\\right)\n$$\nWith this definition, the action can be written as a simple product:\n$$\nS(\\theta) = \\beta(\\theta) \\cdot W\n$$\nWe are asked to find the derivative of $S$ with respect to $\\theta$, which is $\\frac{dS}{d\\theta}$. Since $W$ is a constant with respect to $\\theta$, we can apply the rule for differentiating a product where one factor is a constant. Using the chain rule from introductory calculus, where $S$ is a composition of functions $S(\\beta(\\theta))$:\n$$\n\\frac{dS}{d\\theta} = \\frac{d S}{d\\beta} \\frac{d\\beta}{d\\theta}\n$$\nThe derivative of $S$ with respect to $\\beta$ is simply $W$:\n$$\n\\frac{dS}{d\\beta} = \\frac{d}{d\\beta}(\\beta \\cdot W) = W\n$$\nThe coupling function is given by:\n$$\n\\beta(\\theta) = e^{\\theta} + \\theta^{2}\n$$\nIts derivative with respect to $\\theta$ is found by applying standard differentiation rules for exponential and polynomial functions:\n$$\n\\frac{d\\beta}{d\\theta} = \\frac{d}{d\\theta} (e^\\theta + \\theta^2) = e^\\theta + 2\\theta\n$$\nCombining these results, we obtain the analytic expression for the gradient of the action:\n$$\n\\frac{dS}{d\\theta} = W \\cdot \\frac{d\\beta}{d\\theta} = \\left( \\sum_{x}\\sum_{\\mu\\nu}\\left(1 - \\frac{1}{N}\\,\\Re\\operatorname{Tr}\\,U_{\\mu\\nu}(x)\\right) \\right) \\cdot (e^\\theta + 2\\theta)\n$$\nThis expression is the final analytic form of the gradient. In the implementation, it is computationally efficient to first compute the constant term $W$ by summing over all plaquettes on the lattice. Then, for each value of $\\theta$ in the test suite, this pre-computed value $W$ can be used to evaluate both the action $S(\\theta) = \\beta(\\theta)W$ and its analytic gradient $\\frac{dS}{d\\theta} = (e^\\theta + 2\\theta)W$.\n\nFor numerical verification, the central finite-difference approximation for the gradient is used:\n$$\ng_{\\mathrm{FD}}(\\theta;h) = \\frac{S[U;\\beta(\\theta+h)] - S[U;\\beta(\\theta-h)]}{2h}\n$$\nSubstituting the expression for $S$, this becomes:\n$$\ng_{\\mathrm{FD}}(\\theta;h) = \\frac{\\beta(\\theta+h)W - \\beta(\\theta-h)W}{2h} = W \\cdot \\frac{\\beta(\\theta+h) - \\beta(\\theta-h)}{2h}\n$$\nThe implementation will calculate this value and compare it against the analytic gradient to find the absolute difference $\\Delta(\\theta,h) = \\left|\\frac{dS}{d\\theta} - g_{\\mathrm{FD}}(\\theta;h)\\right|$. The expected error for a central difference scheme is of order $O(h^2)$, so for the small values of $h$ specified, $\\Delta(\\theta,h)$ should be very close to zero, limited primarily by floating-point precision. This provides a robust check of the derived expression and its implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the lattice gauge theory problem by deriving and verifying the gradient of the Wilson action.\n    \"\"\"\n\n    # Define the problem parameters and test cases.\n    L_X, L_Y = 3, 3\n    N = 2\n    RNG_SEED = 12345\n    test_cases = [\n        (0.0, 1e-6),\n        (0.5, 1e-6),\n        (-1.0, 1e-6),\n        (2.0, 1e-6),\n        (-3.0, 1e-6),\n        (0.3, 1e-8),\n    ]\n\n    # Set the seed for reproducibility\n    np.random.seed(RNG_SEED)\n\n    # Define SU(2) generators (Pauli matrices) and identity\n    identity = np.eye(2, dtype=complex)\n    sigma1 = np.array([[0, 1], [1, 0]], dtype=complex)\n    sigma2 = np.array([[0, -1j], [1j, 0]], dtype=complex)\n    sigma3 = np.array([[1, 0], [0, -1]], dtype=complex)\n\n    def generate_su2_matrix():\n        \"\"\"\n        Generates a random SU(2) matrix using quaternionic parameterization.\n        A 4-vector of standard normal random numbers is normalized and mapped to SU(2).\n        \"\"\"\n        v = np.random.randn(4)\n        a = v / np.linalg.norm(v)  # Normalize to get (a0, a1, a2, a3)\n        u_matrix = a[0] * identity + 1j * (a[1] * sigma1 + a[2] * sigma2 + a[3] * sigma3)\n        return u_matrix\n\n    def create_lattice_config(l_y, l_x):\n        \"\"\"\n        Creates a 2D lattice with SU(2) link variables.\n        The lattice shape is (L_Y, L_X, n_directions, 2, 2).\n        \"\"\"\n        n_directions = 2  # mu in {0, 1}\n        lattice = np.zeros((l_y, l_x, n_directions, 2, 2), dtype=complex)\n        for n_y in range(l_y):\n            for n_x in range(l_x):\n                for mu in range(n_directions):\n                    lattice[n_y, n_x, mu] = generate_su2_matrix()\n        return lattice\n\n    def calculate_plaquette_sum(U, l_y, l_x, n_c):\n        \"\"\"\n        Calculates the plaquette sum term W = sum_{x, munu} (1 - 1/N ReTr U_{mu,nu}(x)).\n        In 2D, we only have the (mu, nu) = (0, 1) plane.\n        \"\"\"\n        plaquette_term_sum = 0.0\n        # Sum over all lattice sites x\n        for n_y in range(l_y):\n            for n_x in range(l_x):\n                # Directions for the plaquette U_{01}(x)\n                mu, nu = 0, 1\n\n                # Periodic boundary conditions\n                n_x_plus_mu = (n_x + 1) % l_x\n                n_y_plus_nu = (n_y + 1) % l_y\n\n                # Get links for plaquette U_{01}(x) = U_0(x) U_1(x+0) U_0(x+1)^+ U_1(x)^+\n                U_mu_x = U[n_y, n_x, mu]\n                U_nu_x_plus_mu = U[n_y, n_x_plus_mu, nu]\n                U_mu_x_plus_nu = U[n_y_plus_nu, n_x, mu]\n                U_nu_x = U[n_y, n_x, nu]\n\n                # Plaquette calculation\n                plaquette = U_mu_x @ U_nu_x_plus_mu @ U_mu_x_plus_nu.conj().T @ U_nu_x.conj().T\n                \n                # Add contribution to the sum\n                plaquette_term_sum += 1.0 - (1.0 / n_c) * np.trace(plaquette).real\n        \n        return plaquette_term_sum\n\n    # Coupling function and its derivative\n    def beta(theta):\n        return np.exp(theta) + theta**2\n\n    def dbeta_dtheta(theta):\n        return np.exp(theta) + 2 * theta\n\n    # Generate the fixed lattice configuration\n    lattice_U = create_lattice_config(L_Y, L_X)\n    \n    # Pre-calculate the theta-independent part of the action\n    W = calculate_plaquette_sum(lattice_U, L_Y, L_X, N)\n\n    results = []\n    for theta, h in test_cases:\n        # Calculate analytic gradient\n        grad_analytic = dbeta_dtheta(theta) * W\n\n        # Calculate finite-difference gradient\n        # g_FD = W * (beta(theta+h) - beta(theta-h)) / (2*h)\n        # S(theta+h) and S(theta-h) are used for clarity as per problem statement\n        S_plus_h = beta(theta + h) * W\n        S_minus_h = beta(theta - h) * W\n        grad_fd = (S_plus_h - S_minus_h) / (2 * h)\n\n        # Compute the absolute difference\n        delta = np.abs(grad_analytic - grad_fd)\n        results.append(delta)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While manual differentiation is instructive, its complexity grows rapidly. This practice transitions to the core mechanism of automatic differentiation (AD) by having you implement a forward-mode AD system from first principles using dual numbers. Critically, this exercise extends the concept to handle complex-valued arithmetic, a necessity for working with the quantum mechanical amplitudes ubiquitous in high-energy physics, such as the Breit-Wigner resonance forms used here . You will also replace the standard finite-difference check with the more robust and precise complex-step method, demonstrating a powerful technique for validating gradients while avoiding the catastrophic cancellation errors that plague other numerical methods.",
            "id": "3511503",
            "problem": "You are asked to implement and validate gradients using Automatic Differentiation (AD) and the complex-step differentiation method for a real-valued function built from complex amplitudes encountered in computational high-energy physics. The core objective is to benchmark the numerical accuracy of the complex-step differentiation estimate against AD in scenarios that exhibit catastrophic cancellation.\n\nConsider the following real-valued function of the real scalar variable $E$:\n$$\nf(E) \\equiv \\sum_{k=1}^{N} \\left( \\frac{c_k}{E - m_k + i \\frac{\\Gamma_k}{2}} + \\frac{\\tilde{c}_k}{E - m_k - i \\frac{\\Gamma_k}{2}} \\right),\n$$\nwhere $c_k \\in \\mathbb{C}$ and $\\tilde{c}_k \\in \\mathbb{C}$ are fixed complex coefficients, $m_k \\in \\mathbb{R}$ are masses, and $\\Gamma_k \\in \\mathbb{R}_{0}$ are widths. The parameters are constrained such that $E \\in \\mathbb{R}$ and $\\tilde{c}_k$ is chosen equal to the complex conjugate of $c_k$, but this equality is encoded as a numeric parameter and no complex conjugation operation is performed in the program. Under these parameter choices, $f(E)$ is real-valued for real $E$, even though its internal computations involve complex arithmetic, and each summand is holomorphic in $E$.\n\nYour tasks:\n- Implement forward-mode Automatic Differentiation (AD) from first principles for scalar input $E$ with complex arithmetic support, using dual numbers to propagate derivatives through addition, subtraction, multiplication, and division. Treat $E$ as the independent variable and all other parameters as constants.\n- Implement the complex-step differentiation estimate for the derivative:\n$$\n\\frac{\\partial f}{\\partial E}(E) \\approx \\frac{\\operatorname{Im}\\left( f(E + i h) \\right)}{h},\n$$\nwhere $h \\in \\mathbb{R}$ is a small step, $i$ is the imaginary unit, and $\\operatorname{Im}(\\cdot)$ denotes the imaginary part.\n- Compute the absolute error between the complex-step estimate and the AD derivative:\n$$\n\\varepsilon(E,h) \\equiv \\left| \\frac{\\operatorname{Im}\\left( f(E + i h) \\right)}{h} - \\frac{\\partial f}{\\partial E}(E) \\right|.\n$$\n- Design the program to evaluate the following fixed test suite and output the list of absolute errors. All energies and widths are to be understood in giga-electronvolts (GeV), while the output errors are dimensionless real numbers. You must not use any complex conjugation operation in your program; instead, supply $\\tilde{c}_k$ numerically equal to the complex conjugate of $c_k$.\n\nTest suite (each test case is a tuple consisting of $E$, $h$, and a list of resonance pairs $(c_k, \\tilde{c}_k, m_k, \\Gamma_k)$):\n- Case $1$ (catastrophic cancellation at resonance center): $E = 91.1876$, $h = 10^{-30}$, $N=1$, parameters $[(c_1 = 1 + 0 i, \\tilde{c}_1 = 1 - 0 i, m_1 = 91.1876, \\Gamma_1 = 2.4952)]$.\n- Case $2$ (near-resonance interference, two-pair cancellation): $E = 124.9$, $h = 10^{-20}$, $N=2$, parameters $[(c_1 = 1 + 5 i, \\tilde{c}_1 = 1 - 5 i, m_1 = 125.0, \\Gamma_1 = 4.0), (c_2 = -0.8 + 4 i, \\tilde{c}_2 = -0.8 - 4 i, m_2 = 124.0, \\Gamma_2 = 3.0)]$.\n- Case $3$ (narrow-width sharp feature): $E = 500.0 + 0.5 \\times 10^{-6}$, $h = 10^{-40}$, $N=1$, parameters $[(c_1 = 2 + 0 i, \\tilde{c}_1 = 2 - 0 i, m_1 = 500.0, \\Gamma_1 = 10^{-6})]$.\n\nYour program must:\n- Implement AD using dual numbers in a manner consistent with the derivative definition $df/dE$ and propagate derivatives through rational operations.\n- Implement the complex-step estimate as specified above and compute the absolute error $\\varepsilon(E,h)$ for each case.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[e_1,e_2,e_3]$), where $e_k$ are the absolute error values for the corresponding test cases, represented as standard floating-point numbers.\n\nNo external inputs are required. The output is dimensionless real numbers; no physical units are to be included in the output line. The angle unit is not applicable in this problem. The test suite has been selected to probe a general case, catastrophic cancellation at a resonance, and a sharp feature due to narrow width, thereby covering typical edge cases for numerical differentiation in the presence of complex amplitudes.",
            "solution": "The user problem is valid as it is scientifically grounded, well-posed, and objective. It presents a standard problem from computational physics—comparing differentiation techniques for a function common in high-energy physics, the Breit-Wigner resonance formula. The problem is self-contained, providing all necessary parameters and definitions.\n\nThe solution requires implementing two methods for computing the derivative of a real-valued function $f(E)$ that is constructed from complex arithmetic. The function is given by:\n$$\nf(E) = \\sum_{k=1}^{N} \\left( \\frac{c_k}{E - m_k + i \\frac{\\Gamma_k}{2}} + \\frac{\\tilde{c}_k}{E - m_k - i \\frac{\\Gamma_k}{2}} \\right)\n$$\nwhere the parameters are provided such that $f(E)$ is real for a real input $E$. Consequently, its derivative with respect to $E$, $\\frac{\\partial f}{\\partial E}$, must also be a real number.\n\nThe two methods for computing this derivative are:\n1.  **Forward-Mode Automatic Differentiation (AD):** This method computes the exact derivative up to machine precision by propagating function values and their derivatives simultaneously through every arithmetic operation. We will use dual numbers for this purpose. Since the intermediate calculations involve complex numbers, we must implement a `ComplexDual` number type. A complex dual number $z$ is represented as $z = z_r + z_d \\varepsilon$, where $z_r$ (the \"real\" part) and $z_d$ (the \"dual\" part) are both standard complex numbers, and $\\varepsilon$ is a nilpotent element with the property $\\varepsilon^2 = 0$. For a function $g(x)$, if we evaluate it with the dual input $x_0 + 1\\varepsilon$, the result will be a dual number $g(x_0) + g'(x_0)\\varepsilon$. The dual part of the result is the derivative. The arithmetic rules for dual numbers are derived from the standard rules of calculus:\n    -   $(a+b\\varepsilon) + (c+d\\varepsilon) = (a+c) + (b+d)\\varepsilon$\n    -   $(a+b\\varepsilon) \\cdot (c+d\\varepsilon) = ac + (ad+bc)\\varepsilon$\n    -   $\\frac{a+b\\varepsilon}{c+d\\varepsilon} = \\frac{a}{c} + \\frac{bc-ad}{c^2}\\varepsilon$\n    To compute $\\frac{\\partial f}{\\partial E}$, we evaluate $f$ with the input $E_{dual} = E + 1\\varepsilon$. All other parameters ($c_k, m_k, \\Gamma_k$) are treated as constants, represented by dual numbers with a zero dual part.\n\n2.  **Complex-Step (CS) Differentiation:** This is a high-precision numerical approximation method. It leverages the Taylor series expansion of a holomorphic function $g(z)$ around a real point $x$:\n    $$\n    g(x+ih) = g(x) + ihg'(x) - \\frac{h^2}{2}g''(x) - i\\frac{h^3}{6}g'''(x) + \\mathcal{O}(h^4)\n    $$\n    Taking the imaginary part of both sides gives:\n    $$\n    \\operatorname{Im}(g(x+ih)) = h g'(x) - \\frac{h^3}{6}g'''(x) + \\mathcal{O}(h^5)\n    $$\n    Dividing by $h$ yields the approximation for the derivative:\n    $$\n    g'(x) = \\frac{\\operatorname{Im}(g(x+ih))}{h} + \\frac{h^2}{6}g'''(x) + \\mathcal{O}(h^4)\n    $$\n    The error of this approximation is of order $\\mathcal{O}(h^2)$, which is exceptionally small for a small step size $h$. Crucially, this method avoids the subtractive cancellation that plagues finite-difference methods, making it numerically stable even for extremely small $h$. We apply this to our function $f(E)$, which is constructed from holomorphic components.\n\nThe implementation will consist of:\n-   A Python class `ComplexDual` that overloads arithmetic operators to correctly implement the rules for dual numbers with complex components.\n-   A function to compute the derivative using AD by evaluating $f(E)$ with `ComplexDual` numbers.\n-   A function to compute the derivative using the CS formula by evaluating $f(E+ih)$ with standard complex numbers.\n-   A main loop that iterates through the provided test cases, computes the derivative using both methods, and calculates the absolute error $\\varepsilon(E,h) = \\left| \\frac{\\partial f}{\\partial E}\\Big|_{\\text{AD}} - \\frac{\\partial f}{\\partial E}\\Big|_{\\text{CS}} \\right|$.\n\nThe problem gives specific test cases designed to probe numerical behavior, such as on-resonance cancellation and sharp features from narrow-width resonances. The very small step sizes ($h$) for the CS method highlight its robustness against the round-off errors that would typically dominate finite-difference schemes. The AD result serves as the ground truth, as it is algorithmically exact.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements AD and Complex-Step differentiation to find the absolute error between them\n    for a given function from high-energy physics.\n    \"\"\"\n\n    class ComplexDual:\n        \"\"\"\n        A complex dual number representation for forward-mode automatic differentiation.\n        A number is represented as z = real + dual * ε, where ε^2 = 0.\n        Both `real` and `dual` components are complex numbers (np.complex128).\n        \"\"\"\n        def __init__(self, real, dual):\n            self.real = np.complex128(real)\n            self.dual = np.complex128(dual)\n\n        def __repr__(self):\n            return f\"ComplexDual(real={self.real}, dual={self.dual})\"\n\n        def __add__(self, other):\n            if not isinstance(other, ComplexDual):\n                other = ComplexDual(other, 0.0)\n            return ComplexDual(self.real + other.real, self.dual + other.dual)\n\n        def __radd__(self, other):\n            return self.__add__(other)\n\n        def __sub__(self, other):\n            if not isinstance(other, ComplexDual):\n                other = ComplexDual(other, 0.0)\n            return ComplexDual(self.real - other.real, self.dual - other.dual)\n\n        def __rsub__(self, other):\n            # other - self\n            if not isinstance(other, ComplexDual):\n                other = ComplexDual(other, 0.0)\n            return ComplexDual(other.real - self.real, other.dual - self.dual)\n\n        def __mul__(self, other):\n            if not isinstance(other, ComplexDual):\n                other = ComplexDual(other, 0.0)\n            # (a + bε) * (c + dε) = ac + (ad + bc)ε\n            return ComplexDual(self.real * other.real,\n                             self.real * other.dual + self.dual * other.real)\n        \n        def __rmul__(self, other):\n            return self.__mul__(other)\n\n        def __truediv__(self, other):\n            if not isinstance(other, ComplexDual):\n                other = ComplexDual(other, 0.0)\n            # D[f/g] = (f'g - fg') / g^2\n            # dual part: (self.dual * other.real - self.real * other.dual) / other.real**2\n            den_real_sq = other.real * other.real\n            return ComplexDual(self.real / other.real,\n                             (self.dual * other.real - self.real * other.dual) / den_real_sq)\n\n        def __rtruediv__(self, other):\n            # constant / self\n            const_val = np.complex128(other)\n            # D[c/g] = -c*g'/g^2\n            # dual part: -const * self.dual / self.real**2\n            den_real_sq = self.real * self.real\n            return ComplexDual(const_val / self.real,\n                             -const_val * self.dual / den_real_sq)\n\n    def get_ad_derivative(E_val, params):\n        \"\"\"Computes the derivative using forward-mode Automatic Differentiation.\"\"\"\n        E_dual = ComplexDual(E_val, 1.0)\n        total = ComplexDual(0.0, 0.0)\n        i = np.complex128(0, 1)\n    \n        for c_k, ctilde_k, m_k, Gamma_k in params:\n            # Denominators are functions of E, so they are dual numbers\n            den1 = E_dual - m_k + i * Gamma_k / 2.0\n            den2 = E_dual - m_k - i * Gamma_k / 2.0\n            \n            # Numerators are constants\n            term1 = c_k / den1\n            term2 = ctilde_k / den2\n            \n            total = total + term1 + term2\n        \n        # The derivative of a real function wrt a real variable is real.\n        return total.dual.real\n\n    def get_cs_derivative(E_val, h, params):\n        \"\"\"Computes the derivative using the complex-step method.\"\"\"\n        E_h = np.complex128(E_val, h)\n        total = np.complex128(0.0)\n        i = np.complex128(0, 1)\n\n        for c_k, ctilde_k, m_k, Gamma_k in params:\n            c_k_complex = np.complex128(c_k)\n            ctilde_k_complex = np.complex128(ctilde_k)\n\n            den1 = E_h - m_k + i * Gamma_k / 2.0\n            term1 = c_k_complex / den1\n            \n            den2 = E_h - m_k - i * Gamma_k / 2.0\n            term2 = ctilde_k_complex / den2\n            \n            total += term1 + term2\n        \n        return total.imag / h\n\n    test_cases = [\n        (91.1876, 1e-30, [(complex(1, 0), complex(1, -0), 91.1876, 2.4952)]),\n        (124.9, 1e-20, [\n            (complex(1, 5), complex(1, -5), 125.0, 4.0),\n            (complex(-0.8, 4), complex(-0.8, -4), 124.0, 3.0)\n        ]),\n        (500.0 + 0.5e-6, 1e-40, [(complex(2, 0), complex(2, -0), 500.0, 1e-6)])\n    ]\n    \n    results = []\n    for E, h, resonance_params in test_cases:\n        grad_ad = get_ad_derivative(E, resonance_params)\n        grad_cs = get_cs_derivative(E, h, resonance_params)\n        \n        error = abs(grad_ad - grad_cs)\n        results.append(error)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having explored the mechanics of forward-mode AD, we now turn to a concept central to the efficiency of modern deep learning frameworks: reverse-mode AD. This practice focuses on designing a custom vector-Jacobian product (VJP), the elementary building block of reverse-mode differentiation, for the numerically stable `log-sum-exp` (LSE) function . As the LSE operator is fundamental to probabilistic modeling and likelihood calculations, learning to create a stable and efficient VJP for it is not just an academic exercise—it is a critical skill for developing robust, large-scale differentiable analyses in physics.",
            "id": "3511336",
            "problem": "You are given the task of designing and verifying a custom vector-Jacobian product (VJP) for a numerically stable implementation of the log-sum-exp function that arises in computational high-energy physics (HEP) likelihoods. You will work in the context of differentiable programming and automatic differentiation for functions relevant to HEP mixture likelihoods. The goal is to construct a stable VJP for the log-sum-exp, verify its correctness using the definition of a VJP, and quantify the reduction in occurrences of undefined numerical values due to instability when compared to a naive differentiation rule.\n\nConsider an event-level mixture model with per-event scores represented as a matrix $S \\in \\mathbb{R}^{M \\times K}$, where row $j$ contains the values $S_{j,1},\\dots,S_{j,K}$ that enter a per-event log-marginalization through the log-sum-exp map. Define the row-wise log-sum-exp operator $F: \\mathbb{R}^{M \\times K} \\to \\mathbb{R}^{M}$ by applying, for each row $j$, the scalar function\n$$\n\\operatorname{LSE}(x_1,\\dots,x_K) \\equiv \\log\\left(\\sum_{i=1}^{K} \\exp(x_i)\\right).\n$$\nCompute $F(S)$ by applying $\\operatorname{LSE}$ to each row of $S$. In code, you must implement a numerically stable forward rule for $\\operatorname{LSE}$ using the standard shift-by-maximum technique, i.e., for each row $j$, let $m_j = \\max_i S_{j,i}$ and compute\n$$\n\\operatorname{LSE}_{\\text{stable}}(S_{j,\\cdot}) \\equiv m_j + \\log\\left(\\sum_{i=1}^{K} \\exp\\left(S_{j,i}-m_j\\right)\\right).\n$$\n\nYour tasks are:\n1. Implement the forward map $F(S)$ using the stable row-wise definition above.\n2. Design two VJP implementations for $F$ that map an output cotangent $c \\in \\mathbb{R}^M$ to an input cotangent of shape $\\mathbb{R}^{M \\times K}$:\n   - A naive VJP obtained by differentiating the non-shifted definition of $\\operatorname{LSE}$ directly.\n   - A stable VJP derived in a way that avoids intermediate numerical overflow and underflow by using the same row-wise shift $m_j$ that is used in the forward pass.\n   You must not reuse the target gradient formula explicitly given in the problem; instead, derive the VJP from first principles in your solution. The implementation must be fully vectorized over rows.\n3. Verify the stable VJP numerically by testing the fundamental definition of a VJP. For a general output cotangent $c \\in \\mathbb{R}^M$, define the scalar functional\n$$\n\\phi(S) \\equiv \\langle c, F(S) \\rangle = \\sum_{j=1}^{M} c_j \\operatorname{LSE}_{\\text{stable}}(S_{j,\\cdot}).\n$$\nFor a chosen perturbation direction $V \\in \\mathbb{R}^{M \\times K}$ and a small step size $\\varepsilon \\in \\mathbb{R}$, estimate the directional derivative using a central finite difference:\n$$\nD\\phi(S)[V] \\approx \\frac{\\phi(S+\\varepsilon V)-\\phi(S-\\varepsilon V)}{2\\varepsilon}.\n$$\nCheck that this equals the inner product between the candidate input cotangent from your VJP and $V$, i.e., verify\n$$\n\\left\\langle \\operatorname{VJP}_F(S)[c], V \\right\\rangle \\approx D\\phi(S)[V].\n$$\nQuantify the absolute difference between the two sides as a nonnegative scalar error for each test case.\n4. Quantify numerical stability by counting how many entries of each VJP output contain undefined values when implemented in finite precision arithmetic. For each test case, report the number of entries that are “not a number” (NaN) produced by the naive VJP, and the number produced by the stable VJP. Also report the fractional reduction computed as\n$$\nr \\equiv \\frac{n_{\\text{naive}} - n_{\\text{stable}}}{\\max(1, n_{\\text{naive}})}.\n$$\n\nTest Suite:\nUse the following five test cases. Each case provides a matrix $S \\in \\mathbb{R}^{M \\times K}$, an output cotangent vector $c \\in \\mathbb{R}^M$, and a perturbation direction $V \\in \\mathbb{R}^{M \\times K}$. Angles do not appear, and there are no physical units; all quantities are dimensionless.\n\n- Case 1 (general well-scaled values):\n  - $S = \\begin{bmatrix}\n  -1.2  0.3  2.0  -0.7  1.5 \\\\\n  0.1  -0.2  0.3  -0.4  0.0 \\\\\n  2.5  1.5  -3.0  0.5  -0.1 \\\\\n  -4.0  -0.3  0.9  1.2  -2.1\n  \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 1.0  -0.5  2.0  0.7 \\end{bmatrix}$,\n  - $V$ is the reshape of a linearly spaced vector from $-0.3$ to $0.3$ with $20$ entries into the shape of $S$.\n- Case 2 (large positive scales):\n  - $S = \\begin{bmatrix}\n  1000.0  1001.0  999.0  1005.0  1002.0 \\\\\n  800.0  805.0  790.0  795.0  810.0 \\\\\n  1200.0  1200.0  1200.0  1199.0  1201.0\n  \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 1.0  1.0  1.0 \\end{bmatrix}$,\n  - $V$ is the same shape as $S$ with every entry equal to $0.1$.\n- Case 3 (large negative scales):\n  - $S = \\begin{bmatrix}\n  -1000.0  -1001.0  -999.0  -1005.0  -1002.0 \\\\\n  -800.0  -805.0  -790.0  -795.0  -810.0 \\\\\n  -1200.0  -1200.0  -1200.0  -1199.0  -1201.0\n  \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 1.0  1.0  1.0 \\end{bmatrix}$,\n  - $V$ is the reshape of a linearly spaced vector from $-0.2$ to $0.2$ with $15$ entries into the shape of $S$.\n- Case 4 (mixed extremes):\n  - $S = \\begin{bmatrix}\n  -1000.0  0.0  1000.0  -500.0  500.0 \\\\\n  500.0  -500.0  0.0  1000.0  -1000.0\n  \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 1.0  -2.0 \\end{bmatrix}$,\n  - $V = \\begin{bmatrix}\n  0.05  -0.1  0.2  -0.05  0.1 \\\\\n  -0.02  0.03  -0.04  0.05  -0.06\n  \\end{bmatrix}$.\n- Case 5 (single-component rows as a boundary condition):\n  - $S = \\begin{bmatrix}\n  1000.0 \\\\ -1000.0 \\\\ 0.0\n  \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 0.3  -0.4  1.2 \\end{bmatrix}$,\n  - $V = \\begin{bmatrix}\n  1.0 \\\\ -1.0 \\\\ 0.5\n  \\end{bmatrix}$.\n\nSet the finite difference step to $\\varepsilon = 10^{-6}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case must contribute a sublist of four values in the following order: the number of NaNs in the naive VJP output (an integer), the number of NaNs in the stable VJP output (an integer), the fractional reduction $r$ as defined above (a float), and the absolute error in the VJP verification for the stable rule (a float). The final line should look like\n$[ [\\dots], [\\dots], [\\dots], [\\dots], [\\dots] ]$\nbut without any spaces, i.e., strictly formatted as\n\"[[n1_naive,n1_stable,r1,e1],[n2_naive,n2_stable,r2,e2],\\dots,[n5_naive,n5_stable,r5,e5]]\".",
            "solution": "The objective is to design, implement, and verify a numerically stable vector-Jacobian product (VJP) for the row-wise log-sum-exp (LSE) operator, a common function in computational physics and machine learning likelihoods. This involves deriving the VJP, testing its correctness against a finite-difference approximation, and quantifying the numerical stability improvements over a naive implementation.\n\nLet the operator be $F: \\mathbb{R}^{M \\times K} \\to \\mathbb{R}^{M}$, which applies the LSE function to each of the $M$ rows of an input matrix $S \\in \\mathbb{R}^{M \\times K}$. For a single row vector $x = (x_1, \\dots, x_K)$, the LSE function is defined as:\n$$\n\\operatorname{LSE}(x) = \\log\\left(\\sum_{i=1}^{K} \\exp(x_i)\\right)\n$$\n\nThe direct computation of this function is numerically unstable. For large positive values of $x_i$, $\\exp(x_i)$ can overflow standard floating-point representations. For large negative values, $\\exp(x_i)$ can underflow to zero, leading to $\\log(0)=-\\infty$ or a loss of precision if some values are not as large and negative. The standard mitigation is the shift-by-maximum technique. For each row $S_{j,\\cdot}$ of the matrix $S$, we find the maximum value $m_j = \\max_i S_{j,i}$ and compute:\n$$\ny_j = F(S)_{j} = \\operatorname{LSE}_{\\text{stable}}(S_{j,\\cdot}) = m_j + \\log\\left(\\sum_{i=1}^{K} \\exp(S_{j,i}-m_j)\\right)\n$$\nThis is mathematically equivalent to the original definition due to the identity $\\log(\\sum \\alpha e^{x_i}) = \\log(\\alpha) + \\log(\\sum e^{x_i})$, where we have used $\\alpha = e^{m_j}$. Numerically, the largest argument to $\\exp$ inside the sum is now $0$, which prevents overflow. It also ensures that the sum is at least $1$, preventing the argument to $\\log$ from being zero unless all inputs are $-\\infty$. This constitutes the implementation for Task 1.\n\nFor Task 2, we derive two VJP implementations. A VJP computes the action of the Jacobian-transpose on an output cotangent vector $c \\in \\mathbb{R}^M$. The result is a matrix of the same shape as the input $S$, representing the gradient of the scalar objective $\\phi(S) = \\langle c, F(S) \\rangle = \\sum_{j=1}^M c_j F(S)_j$ with respect to $S$. The $(j, k)$-th component of the VJP is $(\\operatorname{VJP}_F(S)[c])_{jk} = \\frac{\\partial \\phi}{\\partial S_{jk}} = c_j \\frac{\\partial F(S)_j}{\\partial S_{jk}}$.\n\nFirst, we derive the naive VJP. The partial derivative of the unshifted LSE function for row $j$ with respect to an element $S_{jk}$ is:\n$$\n\\frac{\\partial y_j}{\\partial S_{jk}} = \\frac{\\partial}{\\partial S_{jk}} \\log\\left(\\sum_{i=1}^{K} \\exp(S_{j,i})\\right) = \\frac{1}{\\sum_{i=1}^{K} \\exp(S_{j,i})} \\cdot \\exp(S_{jk}) = \\frac{\\exp(S_{jk})}{\\sum_{i=1}^{K} \\exp(S_{j,i})}\n$$\nThis expression is the softmax function applied to the row vector $S_{j,\\cdot}$. The naive VJP is therefore computed row-wise as:\n$$\n(\\operatorname{VJP}_{\\text{naive}}(S)[c])_{jk} = c_j \\cdot \\operatorname{softmax}(S_{j,\\cdot})_k\n$$\nThis calculation inherits the numerical instabilities of the naive LSE forward pass, as it involves computing $\\exp(S_{jk})$ directly, which can overflow, and $\\sum_i \\exp(S_{j,i})$, which can underflow to zero, leading to division by zero (`inf`) or indeterminate forms (`nan`).\n\nSecond, we derive the stable VJP from the stable LSE definition. Let $y_j = m_j + \\log(\\sum_i \\exp(S_{j,i}-m_j))$ with $m_j = \\max_i S_{j,i}$. The partial derivative of $m_j$ with respect to $S_{jk}$ is $1$ if $S_{jk}$ is a unique maximum and $0$ otherwise (this is the gradient a.e.). Using the chain rule:\n$$\n\\frac{\\partial y_j}{\\partial S_{jk}} = \\frac{\\partial m_j}{\\partial S_{jk}} + \\frac{\\partial}{\\partial S_{jk}} \\left[ \\log\\left(\\sum_{i=1}^{K} \\exp(S_{j,i}-m_j)\\right) \\right]\n$$\nThe second term expands to:\n$$\n\\frac{1}{\\sum_i \\exp(S_{j,i}-m_j)} \\cdot \\sum_{i=1}^{K} \\left( \\exp(S_{j,i}-m_j) \\cdot \\frac{\\partial}{\\partial S_{jk}}(S_{j,i}-m_j) \\right)\n$$\n$$\n= \\frac{1}{\\sum_i \\exp(S_{j,i}-m_j)} \\cdot \\sum_{i=1}^{K} \\left( \\exp(S_{j,i}-m_j) \\cdot (\\delta_{ik} - \\frac{\\partial m_j}{\\partial S_{jk}}) \\right)\n$$\n$$\n= \\frac{\\exp(S_{jk}-m_j)}{\\sum_i \\exp(S_{j,i}-m_j)} - \\frac{\\sum_i \\exp(S_{j,i}-m_j)}{\\sum_i \\exp(S_{j,i}-m_j)} \\cdot \\frac{\\partial m_j}{\\partial S_{jk}} = \\operatorname{softmax}(S_{j,\\cdot}-m_j)_k - \\frac{\\partial m_j}{\\partial S_{jk}}\n$$\nSubstituting this back into the expression for $\\frac{\\partial y_j}{\\partial S_{jk}}$:\n$$\n\\frac{\\partial y_j}{\\partial S_{jk}} = \\frac{\\partial m_j}{\\partial S_{jk}} + \\left( \\operatorname{softmax}(S_{j,\\cdot}-m_j)_k - \\frac{\\partial m_j}{\\partial S_{jk}} \\right) = \\operatorname{softmax}(S_{j,\\cdot}-m_j)_k\n$$\nRemarkably, the terms involving the derivative of the max function cancel. The gradient of the stable LSE is the softmax of the mean-subtracted inputs. This form is numerically stable as the arguments to $\\exp$ are bounded above by $0$. The stable VJP is:\n$$\n(\\operatorname{VJP}_{\\text{stable}}(S)[c])_{jk} = c_j \\cdot \\frac{\\exp(S_{jk}-m_j)}{\\sum_i \\exp(S_{j,i}-m_j)}\n$$\n\nFor Task 3, we verify the correctness of the stable VJP. The definition of a VJP implies that for any perturbation vector $V$, the directional derivative of the scalar objective $\\phi(S) = \\langle c, F(S) \\rangle$ in the direction of $V$ is equal to the inner product of the VJP with $V$. We test this identity:\n$$\nD\\phi(S)[V] = \\langle \\nabla_S \\phi(S), V \\rangle = \\left\\langle \\operatorname{VJP}_F(S)[c], V \\right\\rangle\n$$\nThe directional derivative is approximated using a central finite difference with a small step size $\\varepsilon = 10^{-6}$:\n$$\nD\\phi(S)[V] \\approx \\frac{\\phi(S+\\varepsilon V)-\\phi(S-\\varepsilon V)}{2\\varepsilon}\n$$\nThe absolute difference between the finite-difference approximation and the VJP-based inner product $\\sum_{j,k} (\\operatorname{VJP}_{\\text{stable}})_{jk} V_{jk}$ quantifies the implementation error.\n\nFor Task 4, numerical stability is assessed by counting the number of `NaN` (not a number) entries in the VJP results for both the naive ($n_{\\text{naive}}$) and stable ($n_{\\text{stable}}$) methods. The fractional reduction in these undefined values, $r = (n_{\\text{naive}} - n_{\\text{stable}})/\\max(1, n_{\\text{naive}})$, measures the improvement. We expect $n_{\\text{stable}}$ to be zero for all valid finite inputs, and $n_{\\text{naive}}$ to be non-zero for inputs with large positive or negative magnitudes, resulting in $r=1.0$ in such cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of designing, verifying, and comparing VJPs for the LSE function.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([\n                [-1.2, 0.3, 2.0, -0.7, 1.5],\n                [0.1, -0.2, 0.3, -0.4, 0.0],\n                [2.5, 1.5, -3.0, 0.5, -0.1],\n                [-4.0, -0.3, 0.9, 1.2, -2.1]\n            ]),\n            np.array([1.0, -0.5, 2.0, 0.7]),\n            np.linspace(-0.3, 0.3, 20).reshape(4, 5)\n        ),\n        (\n            np.array([\n                [1000.0, 1001.0, 999.0, 1005.0, 1002.0],\n                [800.0, 805.0, 790.0, 795.0, 810.0],\n                [1200.0, 1200.0, 1200.0, 1199.0, 1201.0]\n            ]),\n            np.array([1.0, 1.0, 1.0]),\n            np.full((3, 5), 0.1)\n        ),\n        (\n            np.array([\n                [-1000.0, -1001.0, -999.0, -1005.0, -1002.0],\n                [-800.0, -805.0, -790.0, -795.0, -810.0],\n                [-1200.0, -1200.0, -1200.0, -1199.0, -1201.0]\n            ]),\n            np.array([1.0, 1.0, 1.0]),\n            np.linspace(-0.2, 0.2, 15).reshape(3, 5)\n        ),\n        (\n            np.array([\n                [-1000.0, 0.0, 1000.0, -500.0, 500.0],\n                [500.0, -500.0, 0.0, 1000.0, -1000.0]\n            ]),\n            np.array([1.0, -2.0]),\n            np.array([\n                [0.05, -0.1, 0.2, -0.05, 0.1],\n                [-0.02, 0.03, -0.04, 0.05, -0.06]\n            ])\n        ),\n        (\n            np.array([\n                [1000.0],\n                [-1000.0],\n                [0.0]\n            ]),\n            np.array([0.3, -0.4, 1.2]),\n            np.array([\n                [1.0],\n                [-1.0],\n                [0.5]\n            ])\n        )\n    ]\n    \n    epsilon = 1e-6\n\n    def F_stable(S):\n        \"\"\"Task 1: Numerically stable forward map F(S).\"\"\"\n        if S.shape[1] == 0:\n            return np.full(S.shape[0], -np.inf)\n        max_S = np.max(S, axis=1, keepdims=True)\n        # The result is -inf if the entire row is -inf, handled correctly by numpy\n        # as max is -inf, log(sum(exp(S-max))) = log(sum(exp(nan))) = nan.\n        # But exp(-inf) -> 0, so sum is 0, log(0) is -inf. max + log(sum) = -inf-inf = -inf. Correct.\n        with np.errstate(divide='ignore'):\n            log_sum_exp = max_S + np.log(np.sum(np.exp(S - max_S), axis=1, keepdims=True))\n        return log_sum_exp.flatten()\n\n    def vjp_naive(S, c):\n        \"\"\"Task 2a: Naive VJP implementation.\"\"\"\n        with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n            exp_S = np.exp(S)\n            sum_exp_S = np.sum(exp_S, axis=1, keepdims=True)\n            softmax_naive = exp_S / sum_exp_S\n            vjp = c[:, np.newaxis] * softmax_naive\n        return vjp\n\n    def vjp_stable(S, c):\n        \"\"\"Task 2b: Stable VJP implementation.\"\"\"\n        if S.shape[1] == 0:\n            return np.zeros_like(S)\n        max_S = np.max(S, axis=1, keepdims=True)\n        S_shifted = S - max_S\n        exp_S_shifted = np.exp(S_shifted)\n        sum_exp_S_shifted = np.sum(exp_S_shifted, axis=1, keepdims=True)\n        # This division is safe from overflow and underflow leading to 0/0\n        softmax_stable = exp_S_shifted / sum_exp_S_shifted\n        vjp = c[:, np.newaxis] * softmax_stable\n        return vjp\n\n    def phi(S, c):\n        \"\"\"Task 3: Scalar functional for verification.\"\"\"\n        return np.dot(c, F_stable(S))\n\n    results = []\n    for S, c, V in test_cases:\n        # Task 4: Quantify numerical stability\n        vjp_n_output = vjp_naive(S, c)\n        n_naive = np.isnan(vjp_n_output).sum()\n        \n        vjp_s_output = vjp_stable(S, c)\n        n_stable = np.isnan(vjp_s_output).sum()\n        \n        r = (n_naive - n_stable) / max(1, n_naive)\n\n        # Task 3: Verify the stable VJP and quantify error\n        fd_approx = (phi(S + epsilon * V, c) - phi(S - epsilon * V, c)) / (2 * epsilon)\n        vjp_dot_V = np.sum(vjp_s_output * V)\n        error = np.abs(fd_approx - vjp_dot_V)\n\n        results.append([int(n_naive), int(n_stable), float(r), float(error)])\n    \n    # Format the final output string exactly as requested, without spaces.\n    inner_parts = [f\"[{','.join(map(str, r))}]\" for r in results]\n    final_output_str = f\"[{','.join(inner_parts)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}