{
    "hands_on_practices": [
        {
            "introduction": "A primary challenge in applying generative models to physics is ensuring they respect fundamental conservation laws. This exercise provides a hands-on guide to implementing a differentiable physics layer, a technique that integrates principles like momentum conservation directly into the model's architecture. By working through this problem, you will learn to build physics-informed models and quantify the impact of such constraints on the final generated data distribution .",
            "id": "3515492",
            "problem": "You are given a stylized setup for a physics-informed generative model used in computational high-energy physics. A generative network such as a Generative Adversarial Network (GAN) or a Variational Autoencoder (VAE) produces unconstrained final-state particle momenta, which are then corrected by a differentiable physics layer to approximately enforce momentum conservation. You must derive the layer from first principles as a differentiable optimization, quantify its impact on the missing momentum distribution, and implement a program that computes specific analytic metrics for a set of given test cases.\n\nThe context and definitions are as follows.\n\n- There is an event with $N$ final-state particles in $D$ spatial momentum dimensions, and an incoming total momentum $\\vec{p}_{\\text{in}} \\in \\mathbb{R}^D$. The unconstrained generator output is $\\{\\vec{x}_i\\}_{i=1}^N$, where $\\vec{x}_i \\in \\mathbb{R}^D$ are random vectors.\n- The unconstrained generator is modeled as independent Gaussian outputs for each particle, with $\\vec{x}_i \\sim \\mathcal{N}(\\vec{\\mu}_i, \\Sigma_i)$, where $\\vec{\\mu}_i \\in \\mathbb{R}^D$ and $\\Sigma_i \\in \\mathbb{R}^{D \\times D}$ are positive-definite. Independence across particles implies $\\sum_{i=1}^N \\vec{x}_i$ is Gaussian with mean $\\sum_{i=1}^N \\vec{\\mu}_i$ and covariance $\\sum_{i=1}^N \\Sigma_i$. Consequently, the pre-correction missing momentum is $\\vec{p}_{\\text{miss,pre}} = \\vec{p}_{\\text{in}} - \\sum_{i=1}^N \\vec{x}_i \\sim \\mathcal{N}(\\vec{\\mu}_0, \\Sigma_0)$ with\n$\\vec{\\mu}_0 = \\vec{p}_{\\text{in}} - \\sum_{i=1}^N \\vec{\\mu}_i$ and $\\Sigma_0 = \\sum_{i=1}^N \\Sigma_i$.\n\n- A differentiable physics layer produces corrected momenta $\\{\\vec{y}_i\\}_{i=1}^N$ by solving a weighted least-squares problem with a soft constraint parameter $\\alpha \\in [0,1]$:\nminimize $\\sum_{i=1}^N w_i \\lVert \\vec{y}_i - \\vec{x}_i \\rVert_2^2$ subject to $\\sum_{i=1}^N \\vec{y}_i = \\vec{x}_{\\text{sum}} + \\alpha \\left(\\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}}\\right)$, where $\\vec{x}_{\\text{sum}} = \\sum_{i=1}^N \\vec{x}_i$ and $w_i > 0$ are user-specified weights. The case $\\alpha = 1$ corresponds to a hard constraint enforcing exact momentum conservation, while $\\alpha = 0$ leaves the outputs unchanged. This layer is linear and differentiable for any fixed $\\alpha \\in (0,1)$.\n\n- Let $\\beta_i = \\dfrac{1/w_i}{\\sum_{j=1}^N 1/w_j}$ so that $\\sum_{i=1}^N \\beta_i = 1$. The unique optimizer of the weighted least-squares problem produces the update $\\vec{y}_i = \\vec{x}_i + \\alpha \\,\\beta_i \\,\\Delta$, where $\\Delta = \\vec{p}_{\\text{in}} - \\sum_{j=1}^N \\vec{x}_j$. This implies the post-correction missing momentum is $\\vec{p}_{\\text{miss,post}} = \\vec{p}_{\\text{in}} - \\sum_{i=1}^N \\vec{y}_i = (1-\\alpha)\\,\\Delta$.\n\n- By linear-Gaussian propagation, $\\vec{p}_{\\text{miss,post}} \\sim \\mathcal{N}(\\vec{\\mu}_\\alpha, \\Sigma_\\alpha)$ with $\\vec{\\mu}_\\alpha = (1-\\alpha)\\,\\vec{\\mu}_0$ and $\\Sigma_\\alpha = (1-\\alpha)^2 \\Sigma_0$.\n\n- The Kullback–Leibler divergence between two multivariate normal distributions $\\mathcal{N}(\\vec{m}_1,\\Sigma_1)$ and $\\mathcal{N}(\\vec{m}_2,\\Sigma_2)$ in $D$ dimensions is given by\n$\\mathrm{KL}\\!\\left(\\mathcal{N}(\\vec{m}_1,\\Sigma_1)\\,\\Vert\\,\\mathcal{N}(\\vec{m}_2,\\Sigma_2)\\right) = \\dfrac{1}{2}\\left(\\mathrm{tr}(\\Sigma_2^{-1}\\Sigma_1) + (\\vec{m}_2-\\vec{m}_1)^\\top \\Sigma_2^{-1}(\\vec{m}_2-\\vec{m}_1) - D + \\ln \\dfrac{\\det \\Sigma_2}{\\det \\Sigma_1}\\right)$, a well-tested formula. You will use this to quantify the impact on the missing momentum distribution, taking $\\mathrm{KL}\\!\\left(\\mathcal{N}(\\vec{\\mu}_\\alpha,\\Sigma_\\alpha)\\,\\Vert\\,\\mathcal{N}(\\vec{\\mu}_0,\\Sigma_0)\\right)$.\n\n- The expected total squared correction magnitude over all particles is $\\mathbb{E}\\!\\left[\\sum_{i=1}^N \\lVert \\vec{y}_i - \\vec{x}_i \\rVert_2^2\\right] = \\alpha^2 \\left(\\sum_{i=1}^N \\beta_i^2\\right)\\, \\mathbb{E}\\!\\left[\\lVert \\Delta \\rVert_2^2\\right]$, with $\\mathbb{E}\\!\\left[\\lVert \\Delta \\rVert_2^2\\right] = \\lVert \\vec{\\mu}_0 \\rVert_2^2 + \\mathrm{tr}(\\Sigma_0)$ by properties of the multivariate normal. This yields a closed-form, differentiable, and testable metric.\n\nAll momentum vectors must be interpreted in units of $\\mathrm{GeV}/c$. Any reported expected squared correction magnitude must therefore be in $(\\mathrm{GeV}/c)^2$. Angles are not used in this problem. Your program will not sample; it must compute all quantities analytically from the provided parameters.\n\nYour task is to write a complete, runnable program that:\n- For each test case, constructs $\\vec{\\mu}_0$ and $\\Sigma_0$ from the given $\\{\\vec{\\mu}_i,\\Sigma_i\\}$ and $\\vec{p}_{\\text{in}}$.\n- Computes the following three floats for each test case:\n  1. The Kullback–Leibler divergence $\\mathrm{KL}\\!\\left(\\mathcal{N}(\\vec{\\mu}_\\alpha,\\Sigma_\\alpha)\\,\\Vert\\,\\mathcal{N}(\\vec{\\mu}_0,\\Sigma_0)\\right)$ as defined above, where $\\vec{\\mu}_\\alpha=(1-\\alpha)\\vec{\\mu}_0$ and $\\Sigma_\\alpha=(1-\\alpha)^2\\Sigma_0$.\n  2. The variance scaling factor $s = \\dfrac{\\mathrm{tr}(\\Sigma_\\alpha)}{\\mathrm{tr}(\\Sigma_0)}$, which must equal $(1-\\alpha)^2$ for any positive-definite $\\Sigma_0$.\n  3. The expected total squared correction magnitude $\\mathbb{E}\\!\\left[\\sum_{i=1}^N \\lVert \\vec{y}_i - \\vec{x}_i \\rVert_2^2\\right]$, expressed in $(\\mathrm{GeV}/c)^2$.\n\n- Aggregates the results across all test cases into a single line of output containing a comma-separated list enclosed in square brackets. The list must contain the three floats for the first test case, followed by the three floats for the second, and so on.\n\nTest suite:\n- Case $1$ (happy path): $D=2$, $N=4$, $\\alpha=0.7$, $w=(1,1,1,1)$, $\\vec{\\mu}_i=\\vec{0}$ for all $i$, $\\Sigma_i=\\sigma^2 I_2$ with $\\sigma=5.0$, $\\vec{p}_{\\text{in}}=(0,0)$.\n- Case $2$ (anisotropic, small correction): $D=3$, $N=2$, $\\alpha=0.05$, $w=(1.0,2.0)$, $\\vec{\\mu}_1=(1,-2,0.5)$, $\\vec{\\mu}_2=(0,1,-0.5)$, $\\Sigma_1=\\mathrm{diag}(2.0^2,1.0^2,0.5^2)$, $\\Sigma_2=\\mathrm{diag}(1.5^2,0.5^2,0.25^2)$, $\\vec{p}_{\\text{in}}=(3,-1,2)$.\n- Case $3$ (near-hard constraint): $D=2$, $N=6$, $\\alpha=0.95$, $w=(1.0,1.0,0.5,2.0,3.0,1.5)$, $\\vec{\\mu}_i=\\vec{0}$ for all $i$, $\\Sigma_i=\\sigma_i^2 I_2$ with $\\sigma=(3.0,4.0,2.0,1.0,5.0,2.5)$, $\\vec{p}_{\\text{in}}=(10,-7)$.\n- Case $4$ (one-dimensional, heterogeneous weights): $D=1$, $N=3$, $\\alpha=0.3$, $w=(1.0,10.0,0.1)$, $\\vec{\\mu}=(2.0,-1.0,0.0)$, $\\Sigma=\\mathrm{diag}(1.0^2,2.0^2,0.5^2)$ as particle variances in one dimension, $p_{\\text{in}}=5.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\mathrm{KL}_1, s_1, C_1, \\mathrm{KL}_2, s_2, C_2, \\mathrm{KL}_3, s_3, C_3, \\mathrm{KL}_4, s_4, C_4]$, where $\\mathrm{KL}_k$ is the Kullback–Leibler divergence for case $k$ (a float), $s_k$ is the variance scaling factor for case $k$ (a float), and $C_k$ is the expected total squared correction magnitude in $(\\mathrm{GeV}/c)^2$ for case $k$ (a float). No additional text must be printed.",
            "solution": "The objective is to analyze a differentiable physics-informed layer used to correct particle momenta from a generative model. We are asked to compute three specific metrics that quantify the layer's effects: the Kullback-Leibler (KL) divergence between the pre- and post-correction missing momentum distributions, the scaling of the variance of this distribution, and the expected magnitude of the corrections applied. This analysis will be performed analytically based on the provided a priori distributions.\n\nThe foundational step is to characterize the distribution of the pre-correction missing momentum, denoted $\\vec{\\Delta} = \\vec{p}_{\\text{miss,pre}}$. The problem states that the unconstrained momenta $\\vec{x}_i$ are drawn from independent normal distributions, $\\vec{x}_i \\sim \\mathcal{N}(\\vec{\\mu}_i, \\Sigma_i)$, for $i=1, \\dots, N$. The total generated momentum is $\\vec{x}_{\\text{sum}} = \\sum_{i=1}^N \\vec{x}_i$. Due to the properties of the multivariate normal distribution, $\\vec{x}_{\\text{sum}}$ is also normally distributed with a mean equal to the sum of the individual means and a covariance equal to the sum of the individual covariances:\n$$\n\\vec{x}_{\\text{sum}} \\sim \\mathcal{N}\\left(\\sum_{i=1}^N \\vec{\\mu}_i, \\sum_{i=1}^N \\Sigma_i\\right)\n$$\nThe pre-correction missing momentum is defined as $\\vec{\\Delta} = \\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}}$. As this is a linear transformation of a normally distributed variable, $\\vec{\\Delta}$ is also normally distributed. Its parameters, which we denote $\\vec{\\mu}_0$ and $\\Sigma_0$, are:\n$$\n\\vec{\\mu}_0 = \\mathbb{E}[\\vec{\\Delta}] = \\mathbb{E}[\\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}}] = \\vec{p}_{\\text{in}} - \\mathbb{E}[\\vec{x}_{\\text{sum}}] = \\vec{p}_{\\text{in}} - \\sum_{i=1}^N \\vec{\\mu}_i\n$$\n$$\n\\Sigma_0 = \\mathrm{Cov}(\\vec{\\Delta}) = \\mathrm{Cov}(\\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}}) = \\mathrm{Cov}(\\vec{x}_{\\text{sum}}) = \\sum_{i=1}^N \\Sigma_i\n$$\nThus, the pre-correction missing momentum follows the distribution $\\vec{\\Delta} \\sim \\mathcal{N}(\\vec{\\mu}_0, \\Sigma_0)$.\n\nThe physics layer produces corrected momenta $\\{\\vec{y}_i\\}$ such that the post-correction total momentum is $\\sum_{i=1}^N \\vec{y}_i = \\vec{x}_{\\text{sum}} + \\alpha(\\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}})$. The post-correction missing momentum, $\\vec{p}_{\\text{miss,post}}$, is then:\n$$\n\\vec{p}_{\\text{miss,post}} = \\vec{p}_{\\text{in}} - \\sum_{i=1}^N \\vec{y}_i = \\vec{p}_{\\text{in}} - (\\vec{x}_{\\text{sum}} + \\alpha(\\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}})) = (1-\\alpha)(\\vec{p}_{\\text{in}} - \\vec{x}_{\\text{sum}}) = (1-\\alpha)\\vec{\\Delta}\n$$\nThis implies that the post-correction missing momentum distribution is also normal, $\\vec{p}_{\\text{miss,post}} \\sim \\mathcal{N}(\\vec{\\mu}_\\alpha, \\Sigma_\\alpha)$, with parameters given by scaling the pre-correction parameters:\n$$\n\\vec{\\mu}_\\alpha = \\mathbb{E}[(1-\\alpha)\\vec{\\Delta}] = (1-\\alpha)\\vec{\\mu}_0\n$$\n$$\n\\Sigma_\\alpha = \\mathrm{Cov}((1-\\alpha)\\vec{\\Delta}) = (1-\\alpha)^2 \\Sigma_0\n$$\nWith these distributions established, we can compute the required metrics.\n\n**1. Kullback-Leibler Divergence**\nWe compute $\\mathrm{KL}\\!\\left(\\mathcal{N}(\\vec{\\mu}_\\alpha,\\Sigma_\\alpha)\\,\\Vert\\,\\mathcal{N}(\\vec{\\mu}_0,\\Sigma_0)\\right)$. The general formula for the KL divergence between two $D$-dimensional normal distributions $\\mathcal{N}_1 = \\mathcal{N}(\\vec{m}_1, S_1)$ and $\\mathcal{N}_2 = \\mathcal{N}(\\vec{m}_2, S_2)$ is:\n$$\n\\mathrm{KL}(\\mathcal{N}_1 \\Vert \\mathcal{N}_2) = \\frac{1}{2}\\left( \\mathrm{tr}(S_2^{-1}S_1) + (\\vec{m}_2 - \\vec{m}_1)^\\top S_2^{-1}(\\vec{m}_2 - \\vec{m}_1) - D + \\ln\\frac{\\det S_2}{\\det S_1} \\right)\n$$\nIn our case, $\\vec{m}_1 = \\vec{\\mu}_\\alpha = (1-\\alpha)\\vec{\\mu}_0$, $S_1 = \\Sigma_\\alpha = (1-\\alpha)^2\\Sigma_0$, $\\vec{m}_2 = \\vec{\\mu}_0$, and $S_2 = \\Sigma_0$. Substituting these into the formula, we can simplify each term:\n-   Trace term: $\\mathrm{tr}(\\Sigma_0^{-1}\\Sigma_\\alpha) = \\mathrm{tr}(\\Sigma_0^{-1}(1-\\alpha)^2\\Sigma_0) = (1-\\alpha)^2 \\mathrm{tr}(I_D) = D(1-\\alpha)^2$.\n-   Mean difference term: $(\\vec{\\mu}_0 - \\vec{\\mu}_\\alpha)^\\top \\Sigma_0^{-1}(\\vec{\\mu}_0 - \\vec{\\mu}_\\alpha) = (\\vec{\\mu}_0 - (1-\\alpha)\\vec{\\mu}_0)^\\top \\Sigma_0^{-1}(\\vec{\\mu}_0 - (1-\\alpha)\\vec{\\mu}_0) = (\\alpha\\vec{\\mu}_0)^\\top \\Sigma_0^{-1}(\\alpha\\vec{\\mu}_0) = \\alpha^2 \\vec{\\mu}_0^\\top \\Sigma_0^{-1}\\vec{\\mu}_0$.\n-   Log-determinant term: $\\ln\\frac{\\det \\Sigma_0}{\\det \\Sigma_\\alpha} = \\ln\\frac{\\det \\Sigma_0}{\\det((1-\\alpha)^2\\Sigma_0)} = \\ln\\frac{\\det \\Sigma_0}{(1-\\alpha)^{2D}\\det \\Sigma_0} = \\ln((1-\\alpha)^{-2D}) = -2D\\ln(1-\\alpha)$.\n\nCombining these simplified terms yields the final expression for the KL divergence:\n$$\n\\mathrm{KL} = \\frac{1}{2} \\left[ D(1-\\alpha)^2 + \\alpha^2 \\vec{\\mu}_0^\\top \\Sigma_0^{-1}\\vec{\\mu}_0 - D - 2D\\ln(1-\\alpha) \\right]\n$$\nThis expression is computationally stable for $\\alpha \\in [0, 1)$ and directly implemented.\n\n**2. Variance Scaling Factor**\nThe variance scaling factor $s$ is defined as the ratio of the traces of the post- and pre-correction covariance matrices:\n$$\ns = \\frac{\\mathrm{tr}(\\Sigma_\\alpha)}{\\mathrm{tr}(\\Sigma_0)}\n$$\nUsing the relation $\\Sigma_\\alpha = (1-\\alpha)^2 \\Sigma_0$ and the linearity of the trace operator:\n$$\ns = \\frac{\\mathrm{tr}((1-\\alpha)^2 \\Sigma_0)}{\\mathrm{tr}(\\Sigma_0)} = \\frac{(1-\\alpha)^2 \\mathrm{tr}(\\Sigma_0)}{\\mathrm{tr}(\\Sigma_0)} = (1-\\alpha)^2\n$$\nThis metric quantifies the reduction in the overall variance (i.e., statistical fluctuation or \"jitter\") of the total momentum due to the correction layer. It depends only on the mixing parameter $\\alpha$.\n\n**3. Expected Total Squared Correction Magnitude**\nThe final metric is the expected total squared correction applied to the particle momenta, $C = \\mathbb{E}\\!\\left[\\sum_{i=1}^N \\lVert \\vec{y}_i - \\vec{x}_i \\rVert_2^2\\right]$. The problem provides the formula derived from the least-squares solution:\n$$\nC = \\alpha^2 \\left(\\sum_{i=1}^N \\beta_i^2\\right)\\, \\mathbb{E}\\!\\left[\\lVert \\Delta \\rVert_2^2\\right]\n$$\nwhere $\\beta_i = \\frac{1/w_i}{\\sum_{j=1}^N 1/w_j}$. The term $\\mathbb{E}\\!\\left[\\lVert \\Delta \\rVert_2^2\\right]$ is the expected squared norm of the pre-correction missing momentum $\\vec{\\Delta}$. For a random vector $\\vec{z} \\sim \\mathcal{N}(\\vec{m}, S)$, this is given by $\\mathbb{E}[\\lVert \\vec{z} \\rVert_2^2] = \\lVert \\vec{m} \\rVert_2^2 + \\mathrm{tr}(S)$. Applying this to $\\vec{\\Delta} \\sim \\mathcal{N}(\\vec{\\mu}_0, \\Sigma_0)$:\n$$\n\\mathbb{E}\\!\\left[\\lVert \\Delta \\rVert_2^2\\right] = \\lVert \\vec{\\mu}_0 \\rVert_2^2 + \\mathrm{tr}(\\Sigma_0)\n$$\nThis quantity represents the total \"error\" signal that the layer acts upon, comprising a systematic component ($\\lVert \\vec{\\mu}_0 \\rVert_2^2$) from the mean momentum mismatch and a stochastic component ($\\mathrm{tr}(\\Sigma_0)$) from the generator's variance. The factors $\\alpha^2$ and $\\sum_i \\beta_i^2$ determine how strongly this error signal is translated into corrections and how those corrections are distributed among particles according to their weights $w_i$.\n\nThese three metrics are then systematically computed for each test case by first constructing $\\vec{\\mu}_0$ and $\\Sigma_0$ and then applying the derived formulas.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes analytics for a physics-informed generative model correction layer.\n    \"\"\"\n    test_cases = [\n        {\n            \"D\": 2, \"N\": 4, \"alpha\": 0.7, \"w\": [1.0, 1.0, 1.0, 1.0],\n            \"mus\": [np.zeros(2) for _ in range(4)],\n            \"Sigmas\": [5.0**2 * np.identity(2) for _ in range(4)],\n            \"p_in\": np.zeros(2)\n        },\n        {\n            \"D\": 3, \"N\": 2, \"alpha\": 0.05, \"w\": [1.0, 2.0],\n            \"mus\": [np.array([1.0, -2.0, 0.5]), np.array([0.0, 1.0, -0.5])],\n            \"Sigmas\": [\n                np.diag([2.0**2, 1.0**2, 0.5**2]),\n                np.diag([1.5**2, 0.5**2, 0.25**2])\n            ],\n            \"p_in\": np.array([3.0, -1.0, 2.0])\n        },\n        {\n            \"D\": 2, \"N\": 6, \"alpha\": 0.95, \"w\": [1.0, 1.0, 0.5, 2.0, 3.0, 1.5],\n            \"mus\": [np.zeros(2) for _ in range(6)],\n            \"Sigmas\": [s**2 * np.identity(2) for s in [3.0, 4.0, 2.0, 1.0, 5.0, 2.5]],\n            \"p_in\": np.array([10.0, -7.0])\n        },\n        {\n            \"D\": 1, \"N\": 3, \"alpha\": 0.3, \"w\": [1.0, 10.0, 0.1],\n            \"mus\": [np.array([2.0]), np.array([-1.0]), np.array([0.0])],\n            \"Sigmas\": [np.array([[1.0**2]]), np.array([[2.0**2]]), np.array([[0.5**2]])],\n            \"p_in\": np.array([5.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        D = case[\"D\"]\n        alpha = case[\"alpha\"]\n        \n        # Construct mu_0 and Sigma_0 for the pre-correction missing momentum\n        mu_sum = np.sum(case['mus'], axis=0)\n        mu_0 = case['p_in'] - mu_sum\n        \n        Sigma_sum = np.sum(case['Sigmas'], axis=0)\n        Sigma_0 = Sigma_sum\n        \n        # Metric 1: Kullback-Leibler divergence\n        # KL = 0.5 * [ D*(1-alpha)^2 + alpha^2 * mu_0.T @ inv(Sigma_0) @ mu_0 - D - 2*D*ln(1-alpha) ]\n        if alpha == 1.0:\n            # The post-correction distribution is singular, KL divergence is infinite.\n            # This case is avoided by the problem statement test values.\n            kl_div = float('inf')\n        elif alpha == 0.0:\n            kl_div = 0.0\n        else:\n            Sigma_0_inv = np.linalg.inv(Sigma_0)\n            mahalanobis_term = alpha**2 * mu_0.T @ Sigma_0_inv @ mu_0\n            term1 = D * (1 - alpha)**2\n            term3 = -D\n            term4 = -2 * D * np.log(1 - alpha)\n            kl_div = 0.5 * (term1 + mahalanobis_term + term3 + term4)\n        \n        # Metric 2: Variance scaling factor\n        s_factor = (1 - alpha)**2\n        \n        # Metric 3: Expected total squared correction magnitude\n        # C = alpha^2 * sum(beta_i^2) * (norm(mu_0)^2 + trace(Sigma_0))\n        weights = np.array(case['w'])\n        inv_weights = 1.0 / weights\n        sum_inv_weights = np.sum(inv_weights)\n        betas = inv_weights / sum_inv_weights\n        sum_beta_sq = np.sum(betas**2)\n        \n        e_delta_sq = np.linalg.norm(mu_0)**2 + np.trace(Sigma_0)\n        \n        correction_mag = alpha**2 * sum_beta_sq * e_delta_sq\n        \n        results.extend([kl_div, s_factor, correction_mag])\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In practice, there is no single \"best\" generative model; instead, we face a trade-off between generation speed and simulation fidelity. This exercise introduces a formal framework for navigating this trade-off by comparing different models, such as GANs, VAEs, and Diffusion models. You will use the Wasserstein distance as a rigorous metric for distribution fidelity and apply the concept of Pareto optimality to identify the set of models that offer the most efficient balance between accuracy and computational cost .",
            "id": "3515649",
            "problem": "You are given synthetic, discretized observables that represent high-energy physics shower properties. Each test case consists of a small set of models that aim to emulate a reference distribution. For each model, compute a fidelity metric based on the Wasserstein-1 distance and compare it to the model’s average wall-time per event. Your task is to identify the Pareto-optimal subset for each test case: those models for which no other model is simultaneously better or equal in both fidelity (lower is better) and time (lower is better), with at least one strict improvement.\n\nStart from fundamental definitions and well-tested formulas. In particular, use the following base:\n- The fidelity metric is the first Wasserstein distance (also called the Earth Mover’s Distance in one dimension), defined for probability distributions $p$ and $q$ on a metric space $(\\mathbb{R}, d)$ as\n$$\nW_1(p,q) = \\inf_{\\gamma \\in \\Pi(p,q)} \\int_{\\mathbb{R} \\times \\mathbb{R}} d(x,y) \\, \\mathrm{d}\\gamma(x,y),\n$$\nwhere $\\Pi(p,q)$ denotes the set of all couplings with marginals $p$ and $q$, and $d(x,y) = |x - y|$.\n- For one-dimensional distributions with cumulative distribution functions $F_p(x)$ and $F_q(x)$, the Wasserstein-1 distance is equivalently\n$$\nW_1(p,q) = \\int_{-\\infty}^{\\infty} \\left| F_p(x) - F_q(x) \\right| \\, \\mathrm{d}x.\n$$\n- For discretized histograms on bin edges $\\{x_0, x_1, \\dots, x_n\\}$ with per-bin probabilities $\\{p_i\\}_{i=1}^n$ and $\\{q_i\\}_{i=1}^n$ and bin widths $\\Delta x_i = x_i - x_{i-1}$, a consistent one-dimensional discretization is\n$$\nW_1(p,q) \\approx \\sum_{i=1}^{n} \\left| \\sum_{j=1}^{i} (p_j - q_j) \\right| \\, \\Delta x_i,\n$$\nwhere the inner sum is the cumulative difference of bin probabilities up to bin index $i$.\n\nAggregate fidelity across multiple observables by forming a dimensionless score\n$$\n\\mathcal{F} = \\sum_{k=1}^{K} \\frac{W_1^{(k)}}{s_k},\n$$\nwhere $W_1^{(k)}$ is the Wasserstein-1 distance for observable $k$, and $s_k$ is a characteristic scale with the same physical unit as observable $k$ so that $W_1^{(k)}/s_k$ is dimensionless.\n\nDefine Pareto optimality precisely: given pairs $(\\mathcal{F}_i, T_i)$ for models $i \\in \\{0,1,\\dots,M-1\\}$, model $i$ is Pareto-optimal if there is no other model $j$ such that $\\mathcal{F}_j \\le \\mathcal{F}_i$ and $T_j \\le T_i$ and at least one of these inequalities is strict. Here $T_i$ is the average wall-time per event, expressed in seconds.\n\nImplement the following algorithm:\n1. For each test case, and for each observable $k$, normalize each histogram’s counts to probabilities so that $\\sum_{i=1}^{n_k} p_i^{(k)} = 1$ and $\\sum_{i=1}^{n_k} q_i^{(k)} = 1$.\n2. Compute $W_1^{(k)}$ between the truth and each model using the discretized formula above with the given bin edges.\n3. Compute the dimensionless aggregated fidelity $\\mathcal{F}_i$ for each model by summing $W_1^{(k)}/s_k$ across observables $k$.\n4. Using $(\\mathcal{F}_i, T_i)$, determine the set of indices that are Pareto-optimal under the definition above, with lower $\\mathcal{F}$ and lower $T$ being better.\n5. For each test case, return the sorted list of Pareto-optimal indices.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a list of $0$-based model indices that are Pareto-optimal for the corresponding test case. For example, print $[[0,2],[1],[0,1,2]]$ if there are three test cases and the respective Pareto fronts contain indices $\\{0,2\\}$, $\\{1\\}$, and $\\{0,1,2\\}$.\n\nPhysical and numerical units:\n- Wall-time per event $T$ must be handled in seconds.\n- Energy-related observables are in gigaelectronvolts, denoted $\\mathrm{GeV}$.\n- Spatial observables are in millimeters, denoted $\\mathrm{mm}$.\n- Normalize each $W_1^{(k)}$ by its corresponding scale $s_k$ (in the same unit as observable $k$) to ensure $\\mathcal{F}$ is dimensionless.\n- There are no angles in this problem.\n\nTest Suite:\nFor each test case, there are three models in order: $\\text{GAN}$ with index $0$, $\\text{VAE}$ with index $1$, and $\\text{Diffusion}$ with index $2$. Each test case provides two observables: calorimeter shower energy in $\\mathrm{GeV}$ and longitudinal shower length in $\\mathrm{mm}$.\n\nUse the following parameter sets exactly as given:\n\n- Test Case $1$ (happy path trade-offs):\n  - Observable $1$ (energy in $\\mathrm{GeV}$): bin edges $[\\,0,10,20,30,35,42,50\\,]$, truth counts $[\\,5,20,35,25,10,5\\,]$, $\\text{GAN}$ counts $[\\,4,18,36,26,12,4\\,]$, $\\text{VAE}$ counts $[\\,6,22,34,23,10,5\\,]$, $\\text{Diffusion}$ counts $[\\,5,19,35,25,11,5\\,]$, scale $s_1 = 50$.\n  - Observable $2$ (length in $\\mathrm{mm}$): bin edges $[\\,0,50,100,200,400\\,]$, truth counts $[\\,10,30,40,20\\,]$, $\\text{GAN}$ counts $[\\,12,28,40,20\\,]$, $\\text{VAE}$ counts $[\\,9,31,39,21\\,]$, $\\text{Diffusion}$ counts $[\\,10,30,41,19\\,]$, scale $s_2 = 400$.\n  - Times in seconds: $[\\,0.003, 0.005, 0.05\\,]$.\n\n- Test Case $2$ (ties on fidelity and time):\n  - Observable $1$ (energy in $\\mathrm{GeV}$): bin edges $[\\,0,10,20,30,35,42,50\\,]$, truth counts $[\\,8,22,30,25,10,5\\,]$, $\\text{GAN}$ counts $[\\,8,22,30,25,10,5\\,]$, $\\text{VAE}$ counts $[\\,8,22,30,25,10,5\\,]$, $\\text{Diffusion}$ counts $[\\,7,21,31,25,11,5\\,]$, scale $s_1 = 50$.\n  - Observable $2$ (length in $\\mathrm{mm}$): bin edges $[\\,0,50,100,200,400\\,]$, truth counts $[\\,15,35,30,20\\,]$, $\\text{GAN}$ counts $[\\,15,35,30,20\\,]$, $\\text{VAE}$ counts $[\\,15,35,30,20\\,]$, $\\text{Diffusion}$ counts $[\\,14,36,30,20\\,]$, scale $s_2 = 400$.\n  - Times in seconds: $[\\,0.004, 0.004, 0.04\\,]$.\n\n- Test Case $3$ (strong trade-off extremes):\n  - Observable $1$ (energy in $\\mathrm{GeV}$): bin edges $[\\,0,10,20,30,35,42,50\\,]$, truth counts $[\\,10,25,30,20,10,5\\,]$, $\\text{GAN}$ counts $[\\,15,20,25,20,15,5\\,]$, $\\text{VAE}$ counts $[\\,11,24,30,20,10,5\\,]$, $\\text{Diffusion}$ counts $[\\,10,25,30,20,10,5\\,]$, scale $s_1 = 50$.\n  - Observable $2$ (length in $\\mathrm{mm}$): bin edges $[\\,0,50,100,200,400\\,]$, truth counts $[\\,12,28,40,20\\,]$, $\\text{GAN}$ counts $[\\,20,30,35,15\\,]$, $\\text{VAE}$ counts $[\\,13,27,39,21\\,]$, $\\text{Diffusion}$ counts $[\\,12,28,40,20\\,]$, scale $s_2 = 400$.\n  - Times in seconds: $[\\,0.001, 0.003, 0.1\\,]$.\n\n- Test Case $4$ (complete duplication):\n  - Observable $1$ (energy in $\\mathrm{GeV}$): bin edges $[\\,0,10,20,30,35,42,50\\,]$, truth counts $[\\,5,20,35,25,10,5\\,]$, $\\text{GAN}$ counts $[\\,5,20,35,25,10,5\\,]$, $\\text{VAE}$ counts $[\\,5,20,35,25,10,5\\,]$, $\\text{Diffusion}$ counts $[\\,5,20,35,25,10,5\\,]$, scale $s_1 = 50$.\n  - Observable $2$ (length in $\\mathrm{mm}$): bin edges $[\\,0,50,100,200,400\\,]$, truth counts $[\\,10,30,40,20\\,]$, $\\text{GAN}$ counts $[\\,10,30,40,20\\,]$, $\\text{VAE}$ counts $[\\,10,30,40,20\\,]$, $\\text{Diffusion}$ counts $[\\,10,30,40,20\\,]$, scale $s_2 = 400$.\n  - Times in seconds: $[\\,0.01, 0.01, 0.01\\,]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing a single list, with one sub-list per test case. Each sub-list must contain the sorted $0$-based indices of the Pareto-optimal models for that test case. For example, return $[[0,2],[0,1],[0,1,2],[1]]$ if those are the computed fronts.",
            "solution": "The overarching objective is to compare the fidelity and computational cost of different generative models used in computational high-energy physics for fast simulation, specifically Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Diffusion models. The procedure rests on formal probability metrics and algorithmic decision theory.\n\nBegin with the fidelity metric. We adopt the first Wasserstein distance. The foundational definition is\n$$\nW_1(p,q) = \\inf_{\\gamma \\in \\Pi(p,q)} \\int_{\\mathbb{R} \\times \\mathbb{R}} |x-y| \\, \\mathrm{d}\\gamma(x,y),\n$$\nwhich captures the minimum expected transport cost between $p$ and $q$ when the cost is the absolute distance. For one-dimensional distributions with cumulative distribution functions $F_p(x)$ and $F_q(x)$, it holds that\n$$\nW_1(p,q) = \\int_{-\\infty}^{\\infty} \\left| F_p(x) - F_q(x) \\right| \\, \\mathrm{d}x,\n$$\na well-tested integral representation that is particularly suitable for numerical approximations using histograms.\n\nTo discretize, consider a histogram with bin edges $\\{x_0, x_1, \\dots, x_n\\}$, per-bin counts $\\{c_i\\}_{i=1}^{n}$, and normalized probabilities $p_i = c_i / \\sum_{j=1}^{n} c_j$. Likewise for the model, $q_i = d_i / \\sum_{j=1}^{n} d_j$. Within each bin, the probability density is approximated as constant, and the cumulative differences at bin boundaries capture the discrepancy of the cumulative distribution functions. A consistent one-dimensional discretization is\n$$\nW_1(p,q) \\approx \\sum_{i=1}^{n} \\left| \\sum_{j=1}^{i} (p_j - q_j) \\right| \\, \\Delta x_i,\n$$\nwhere $\\Delta x_i = x_i - x_{i-1}$ is the bin width. This arises from summing the absolute difference of cumulative mass across bins, each weighted by its geometric span, aligning with the integral of absolute CDF differences.\n\nSince different observables have different units, directly summing their Wasserstein distances would mix dimensions. To ensure a dimensionless aggregate fidelity, define characteristic scales $s_k$ (with the same unit as observable $k$) and compute\n$$\n\\mathcal{F} = \\sum_{k=1}^{K} \\frac{W_1^{(k)}}{s_k},\n$$\nwhich is dimensionless by construction. This choice is a well-tested normalization that allows cross-observable comparability.\n\nFor computational cost, we use the average wall-time per event $T_i$ in seconds, a direct measure of speed. The comparison is framed by Pareto optimality in two dimensions $(\\mathcal{F}_i, T_i)$ with lower being better for both. Formally, model $i$ is Pareto-optimal if there does not exist a model $j$ such that\n$$\n\\mathcal{F}_j \\le \\mathcal{F}_i \\quad \\text{and} \\quad T_j \\le T_i,\n$$\nwith at least one strict inequality. This definition captures the concept that a Pareto-optimal model is not dominated by any other model in both objectives simultaneously.\n\nAlgorithmic steps:\n1. For each observable, normalize the histogram counts of the truth and each model to obtain per-bin probabilities $\\{p_i^{(k)}\\}$ and $\\{q_i^{(k)}\\}$.\n2. Compute $W_1^{(k)}$ using the discretized cumulative-difference formula. Specifically, compute the per-bin difference $\\delta_i^{(k)} = p_i^{(k)} - q_i^{(k)}$, then the cumulative difference $S_i^{(k)} = \\sum_{j=1}^{i} \\delta_j^{(k)}$, and finally sum $\\sum_{i=1}^{n_k} |S_i^{(k)}| \\, \\Delta x_i^{(k)}$.\n3. Sum the normalized distances $W_1^{(k)}/s_k$ over observables to get $\\mathcal{F}_i$ for each model $i$.\n4. Determine the Pareto front by checking, for each model $i$, whether there exists any model $j$ that has both $\\mathcal{F}_j \\le \\mathcal{F}_i$ and $T_j \\le T_i$, with at least one strict inequality. If such a $j$ exists, then $i$ is dominated and not Pareto-optimal; otherwise, $i$ is Pareto-optimal.\n5. Sort the resulting indices of Pareto-optimal models for each test case to produce a canonical, testable output.\n\nThe provided four test cases cover different aspects:\n- A typical trade-off where speed and fidelity are in tension.\n- Ties where multiple models achieve identical $(\\mathcal{F}, T)$, ensuring duplicate points are correctly retained in the Pareto set.\n- An extreme trade-off where one model is very fast but less faithful, and another is very faithful but slow.\n- A complete duplication where all models are identical in fidelity and time, testing that all are included.\n\nFinally, return the list of Pareto indices per test case in the specified single-line format. This design integrates the fundamental probability metric, physically meaningful units and normalizations, and principled multi-objective decision analysis, yielding a robust, verifiable computational comparison across generative models for fast simulation in high-energy physics.",
            "answer": "```python\nimport numpy as np\n\ndef w1_histogram(bin_edges, truth_counts, model_counts):\n    \"\"\"\n    Compute the 1D Wasserstein-1 distance (Earth Mover's Distance) between two\n    discretized distributions given as histograms with specified bin edges.\n\n    The formula used is:\n        W1 ≈ sum_i | sum_{j<=i} (p_j - q_j) | * Δx_i\n    where p_j and q_j are per-bin probabilities (normalized counts),\n    and Δx_i is the width of bin i.\n\n    Parameters:\n        bin_edges: array of length n+1 with bin boundaries (monotonic increasing).\n        truth_counts: array of length n with nonnegative counts for the truth distribution.\n        model_counts: array of length n with nonnegative counts for the model distribution.\n\n    Returns:\n        float: Wasserstein-1 distance with units of the observable (same as bin_edges).\n    \"\"\"\n    bin_edges = np.asarray(bin_edges, dtype=float)\n    t = np.asarray(truth_counts, dtype=float)\n    m = np.asarray(model_counts, dtype=float)\n    if len(bin_edges) != len(t) + 1 or len(t) != len(m):\n        raise ValueError(\"Inconsistent histogram shapes: edges must be length n+1 and counts length n.\")\n    if t.sum() <= 0 or m.sum() <= 0:\n        raise ValueError(\"Counts must be positive to form a probability distribution.\")\n\n    # Normalize to probabilities.\n    p = t / t.sum()\n    q = m / m.sum()\n\n    # Differences and cumulative differences.\n    diff = p - q\n    cumdiff = np.cumsum(diff)\n\n    widths = np.diff(bin_edges)\n    w1 = np.sum(np.abs(cumdiff) * widths)\n    return float(w1)\n\n\ndef aggregated_fidelity(observables, model_index):\n    \"\"\"\n    Compute the dimensionless aggregated fidelity for a given model_index\n    across all observables, as sum_k W1_k / s_k.\n\n    Parameters:\n        observables: list of dicts, each with keys:\n            - 'edges': array of bin edges\n            - 'truth': array of truth counts\n            - 'models': list of arrays of model counts (same length as truth)\n            - 'scale': characteristic scale s_k (same unit as observable)\n        model_index: int, index of the model in each observable's 'models' list.\n\n    Returns:\n        float: dimensionless aggregated fidelity.\n    \"\"\"\n    total = 0.0\n    for obs in observables:\n        edges = obs[\"edges\"]\n        truth = obs[\"truth\"]\n        model_counts = obs[\"models\"][model_index]\n        scale = float(obs[\"scale\"])\n        w1 = w1_histogram(edges, truth, model_counts)\n        total += w1 / scale\n    return total\n\n\ndef pareto_optimal_indices(fidelities, times):\n    \"\"\"\n    Identify Pareto-optimal indices given fidelity (lower is better) and time (lower is better).\n\n    A point i is Pareto-optimal if there is no point j such that:\n        fidelities[j] <= fidelities[i] and times[j] <= times[i]\n    with at least one strict inequality.\n\n    Parameters:\n        fidelities: list or array of floats\n        times: list or array of floats\n\n    Returns:\n        list of ints: sorted indices of Pareto-optimal points.\n    \"\"\"\n    n = len(fidelities)\n    is_pareto = [True] * n\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                continue\n            if (fidelities[j] <= fidelities[i] and times[j] <= times[i]) and \\\n               ((fidelities[j] < fidelities[i]) or (times[j] < times[i])):\n                is_pareto[i] = False\n                break\n    result = [i for i, flag in enumerate(is_pareto) if flag]\n    result.sort()\n    return result\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"observables\": [\n                {\n                    \"edges\": np.array([0, 10, 20, 30, 35, 42, 50], dtype=float),\n                    \"truth\": np.array([5, 20, 35, 25, 10, 5], dtype=float),\n                    \"models\": [\n                        np.array([4, 18, 36, 26, 12, 4], dtype=float),  # GAN\n                        np.array([6, 22, 34, 23, 10, 5], dtype=float),  # VAE\n                        np.array([5, 19, 35, 25, 11, 5], dtype=float),  # Diffusion\n                    ],\n                    \"scale\": 50.0,  # GeV\n                },\n                {\n                    \"edges\": np.array([0, 50, 100, 200, 400], dtype=float),\n                    \"truth\": np.array([10, 30, 40, 20], dtype=float),\n                    \"models\": [\n                        np.array([12, 28, 40, 20], dtype=float),  # GAN\n                        np.array([9, 31, 39, 21], dtype=float),   # VAE\n                        np.array([10, 30, 41, 19], dtype=float),  # Diffusion\n                    ],\n                    \"scale\": 400.0,  # mm\n                },\n            ],\n            \"times\": np.array([0.003, 0.005, 0.05], dtype=float),  # seconds\n        },\n        {\n            \"observables\": [\n                {\n                    \"edges\": np.array([0, 10, 20, 30, 35, 42, 50], dtype=float),\n                    \"truth\": np.array([8, 22, 30, 25, 10, 5], dtype=float),\n                    \"models\": [\n                        np.array([8, 22, 30, 25, 10, 5], dtype=float),  # GAN\n                        np.array([8, 22, 30, 25, 10, 5], dtype=float),  # VAE\n                        np.array([7, 21, 31, 25, 11, 5], dtype=float),  # Diffusion\n                    ],\n                    \"scale\": 50.0,  # GeV\n                },\n                {\n                    \"edges\": np.array([0, 50, 100, 200, 400], dtype=float),\n                    \"truth\": np.array([15, 35, 30, 20], dtype=float),\n                    \"models\": [\n                        np.array([15, 35, 30, 20], dtype=float),  # GAN\n                        np.array([15, 35, 30, 20], dtype=float),  # VAE\n                        np.array([14, 36, 30, 20], dtype=float),  # Diffusion\n                    ],\n                    \"scale\": 400.0,  # mm\n                },\n            ],\n            \"times\": np.array([0.004, 0.004, 0.04], dtype=float),  # seconds\n        },\n        {\n            \"observables\": [\n                {\n                    \"edges\": np.array([0, 10, 20, 30, 35, 42, 50], dtype=float),\n                    \"truth\": np.array([10, 25, 30, 20, 10, 5], dtype=float),\n                    \"models\": [\n                        np.array([15, 20, 25, 20, 15, 5], dtype=float),  # GAN\n                        np.array([11, 24, 30, 20, 10, 5], dtype=float),  # VAE\n                        np.array([10, 25, 30, 20, 10, 5], dtype=float),  # Diffusion\n                    ],\n                    \"scale\": 50.0,  # GeV\n                },\n                {\n                    \"edges\": np.array([0, 50, 100, 200, 400], dtype=float),\n                    \"truth\": np.array([12, 28, 40, 20], dtype=float),\n                    \"models\": [\n                        np.array([20, 30, 35, 15], dtype=float),  # GAN\n                        np.array([13, 27, 39, 21], dtype=float),  # VAE\n                        np.array([12, 28, 40, 20], dtype=float),  # Diffusion\n                    ],\n                    \"scale\": 400.0,  # mm\n                },\n            ],\n            \"times\": np.array([0.001, 0.003, 0.1], dtype=float),  # seconds\n        },\n        {\n            \"observables\": [\n                {\n                    \"edges\": np.array([0, 10, 20, 30, 35, 42, 50], dtype=float),\n                    \"truth\": np.array([5, 20, 35, 25, 10, 5], dtype=float),\n                    \"models\": [\n                        np.array([5, 20, 35, 25, 10, 5], dtype=float),  # GAN\n                        np.array([5, 20, 35, 25, 10, 5], dtype=float),  # VAE\n                        np.array([5, 20, 35, 25, 10, 5], dtype=float),  # Diffusion\n                    ],\n                    \"scale\": 50.0,  # GeV\n                },\n                {\n                    \"edges\": np.array([0, 50, 100, 200, 400], dtype=float),\n                    \"truth\": np.array([10, 30, 40, 20], dtype=float),\n                    \"models\": [\n                        np.array([10, 30, 40, 20], dtype=float),  # GAN\n                        np.array([10, 30, 40, 20], dtype=float),  # VAE\n                        np.array([10, 30, 40, 20], dtype=float),  # Diffusion\n                    ],\n                    \"scale\": 400.0,  # mm\n                },\n            ],\n            \"times\": np.array([0.01, 0.01, 0.01], dtype=float),  # seconds\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        observables = case[\"observables\"]\n        times = case[\"times\"]\n        num_models = len(observables[0][\"models\"])\n        fidelities = []\n        for mi in range(num_models):\n            f = aggregated_fidelity(observables, mi)\n            fidelities.append(f)\n        pareto = pareto_optimal_indices(fidelities, times)\n        results.append(pareto)\n\n    # Final print statement in the exact required format.\n    # A single line with a list of lists of integers.\n    print(str(results))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Once a generative model is trained and selected, it can be used as a fast surrogate in a full physics analysis. A critical step in this process is to correctly propagate the model's uncertainties into the final scientific result. This practice demonstrates how to quantify the statistical uncertainty of a generative model due to finite sampling and propagate it to a downstream measurement, such as a cross section, using the powerful and widely-used nonparametric bootstrap method .",
            "id": "3515622",
            "problem": "Consider a fast detector simulation surrogate based on a generative model such as a Generative Adversarial Network (GAN) or a Variational Autoencoder (VAE). The generator takes a latent variable $z$ and produces a single event-level scalar observable $x$ representing, for concreteness, a transverse momentum-like quantity measured in gigaelectronvolts (GeV). The latent variable is modeled as $z \\sim \\mathcal{N}(0,1)$, i.e., standard normal. The generator mapping is defined as a deterministic function\n$$\nG(z) = \\max\\left(0, \\, \\mu + \\sigma z + \\kappa \\left(z^2 - 1\\right)\\right),\n$$\nwhere $\\mu$ is a location parameter in $\\mathrm{GeV}$, $\\sigma$ is a scale parameter in $\\mathrm{GeV}$, and $\\kappa$ controls a non-Gaussian correction in $\\mathrm{GeV}$. An event is considered selected if it passes a threshold predicate $S(x)$ given by\n$$\nS(x) = \\begin{cases}\n1 & \\text{if } x \\ge \\tau, \\\\\n0 & \\text{otherwise},\n\\end{cases}\n$$\nwith threshold $\\tau$ in $\\mathrm{GeV}$. Let $\\sigma_{\\mathrm{ref}}$ be a known reference production cross section in picobarns ($\\mathrm{pb}$) for the underlying process before the selection. The post-selection cross section is\n$$\n\\sigma = \\sigma_{\\mathrm{ref}} \\, A,\n$$\nwhere $A = \\mathbb{E}\\left[S\\left(G(z)\\right)\\right]$ is the acceptance, defined as the expected value of the selection indicator under the latent distribution. Using $N$ independent latent draws $z_i \\sim \\mathcal{N}(0,1)$ for $i = 1, \\ldots, N$, the Monte Carlo estimator of acceptance is\n$$\n\\hat{A} = \\frac{1}{N} \\sum_{i=1}^N S\\left(G(z_i)\\right), \\quad \\text{and} \\quad \\hat{\\sigma} = \\sigma_{\\mathrm{ref}} \\, \\hat{A}.\n$$\nTo propagate generator uncertainty arising from finite latent sampling, use the nonparametric bootstrap over the latent draws as follows: construct $B$ bootstrap replicates by sampling, for each replicate $b$, an index multiset $\\{i_1^{(b)}, \\ldots, i_N^{(b)}\\}$ from $\\{1, \\ldots, N\\}$ with replacement, compute\n$$\n\\hat{A}^{(b)} = \\frac{1}{N} \\sum_{j=1}^N S\\left(G\\left(z_{i_j^{(b)}}\\right)\\right), \\quad \\hat{\\sigma}^{(b)} = \\sigma_{\\mathrm{ref}} \\, \\hat{A}^{(b)},\n$$\nand form the equal-tailed bootstrap confidence interval at significance level $\\alpha$ using the lower and upper quantiles\n$$\nQ_{\\alpha/2} = \\text{quantile}\\left(\\{\\hat{\\sigma}^{(b)}\\}_{b=1}^B, \\alpha/2\\right), \\quad Q_{1-\\alpha/2} = \\text{quantile}\\left(\\{\\hat{\\sigma}^{(b)}\\}_{b=1}^B, 1-\\alpha/2\\right).\n$$\nDefine the symmetric half-width of the confidence interval by\n$$\n\\Delta \\hat{\\sigma} = \\frac{Q_{1-\\alpha/2} - Q_{\\alpha/2}}{2}.\n$$\nYour task is to implement this procedure and report, for each test case, the pair $[\\hat{\\sigma}, \\Delta \\hat{\\sigma}]$ with both quantities expressed in picobarns ($\\mathrm{pb}$). All generated event-level observables $x$ and threshold parameters $\\tau$ are in gigaelectronvolts ($\\mathrm{GeV}$). Angles do not appear in this problem. No percentages should be used; any fractional values should be reported as decimal numbers.\n\nFundamental base to use:\n- The definition of acceptance as an expectation $A = \\mathbb{E}\\left[S\\left(G(z)\\right)\\right]$ under the distribution of $z$.\n- The Monte Carlo estimator of expectations $\\hat{A} = \\frac{1}{N} \\sum_{i=1}^N S\\left(G(z_i)\\right)$ using independent samples $z_i \\sim \\mathcal{N}(0,1)$.\n- The nonparametric bootstrap for estimating the sampling distribution of $\\hat{\\sigma}$ by resampling the observed latent draws with replacement and recomputing the estimator.\n\nImplement the following test suite. Each test case specifies $(\\text{seed}, N, \\mu, \\sigma, \\kappa, \\tau, \\sigma_{\\mathrm{ref}}, B, \\alpha)$, with all $\\mu$, $\\sigma$, $\\kappa$, and $\\tau$ in $\\mathrm{GeV}$ and $\\sigma_{\\mathrm{ref}}$ in $\\mathrm{pb}$:\n- Test case $1$: $(12345, 4000, 50, 10, 2, 40, 80, 300, 0.32)$.\n- Test case $2$: $(24680, 4000, 50, 10, 2, 70, 80, 300, 0.32)$.\n- Test case $3$: $(98765, 4000, 50, 10, 2, 10, 80, 300, 0.32)$.\n- Test case $4$: $(13579, 200, 50, 10, 2, 50, 80, 800, 0.32)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of per-test-case pairs enclosed in square brackets. Each pair is a two-element list $[\\hat{\\sigma}, \\Delta \\hat{\\sigma}]$ in picobarns ($\\mathrm{pb}$). For example, the format must be like \n$$\n[[s_1, d_1],[s_2, d_2],[s_3, d_3],[s_4, d_4]],\n$$\nwhere $s_i$ and $d_i$ are decimal numbers in picobarns ($\\mathrm{pb}$) computed for test case $i$.",
            "solution": "The problem is valid. It presents a clear, self-contained, and scientifically sound task in computational high-energy physics, requiring the implementation of a standard statistical procedure (Monte Carlo estimation with bootstrap uncertainty) for a well-defined toy model of a generative simulation. All parameters, definitions, and procedures are specified unambiguously.\n\nThe task is to compute the estimated post-selection cross section, $\\hat{\\sigma}$, and the symmetric half-width of its confidence interval, $\\Delta \\hat{\\sigma}$, for a physics process modeled by a generative surrogate. The procedure is executed for several test cases, each defined by a specific set of parameters.\n\nThe logical flow of the calculation for each test case is as follows:\n\n1.  **Generation of Latent Variables:** The process starts from a latent space. We are instructed to draw $N$ independent samples, denoted $z_i$ for $i=1, \\ldots, N$, from a standard normal distribution, $z_i \\sim \\mathcal{N}(0,1)$. This is performed using a pseudo-random number generator initialized with a specific seed for reproducibility.\n\n2.  **Generation of Observable Events:** Each latent variable $z_i$ is mapped to a physical observable $x_i$ using the generator function $G(z)$. The function is given by:\n    $$\n    x_i = G(z_i) = \\max\\left(0, \\, \\mu + \\sigma z_i + \\kappa \\left(z_i^2 - 1\\right)\\right)\n    $$\n    Here, $\\mu$, $\\sigma$, and $\\kappa$ are parameters of the generator model, specified in units of gigaelectronvolts ($\\mathrm{GeV}$). This function transforms the Gaussian latent distribution into a non-Gaussian distribution for the observable $x$, which represents a quantity like transverse momentum. The $\\max(0, \\cdot)$ operation ensures the observable is non-negative, a physical requirement for momentum.\n\n3.  **Event Selection and Acceptance Estimation:** An event is \"selected\" if its observable $x_i$ passes a certain cut, defined by the predicate $S(x)$. An event is kept if $x_i \\ge \\tau$, where $\\tau$ is a given threshold in $\\mathrm{GeV}$. The selection predicate is mathematically $S(x_i) = 1$ if $x_i \\ge \\tau$ and $S(x_i) = 0$ otherwise. The acceptance, $A$, is the probability that a randomly generated event passes this selection. We estimate it using a Monte Carlo average over the $N$ generated events:\n    $$\n    \\hat{A} = \\frac{1}{N} \\sum_{i=1}^N S\\left(G(z_i)\\right) = \\frac{\\text{Number of events with } x_i \\ge \\tau}{N}\n    $$\n\n4.  **Central Cross Section Estimation:** The estimated post-selection cross section, $\\hat{\\sigma}$, is calculated by scaling the known reference cross section, $\\sigma_{\\mathrm{ref}}$ (in picobarns, $\\mathrm{pb}$), by the estimated acceptance $\\hat{A}$:\n    $$\n    \\hat{\\sigma} = \\sigma_{\\mathrm{ref}} \\cdot \\hat{A}\n    $$\n    This value, $\\hat{\\sigma}$, constitutes the first part of the required output pair $[\\hat{\\sigma}, \\Delta \\hat{\\sigma}]$.\n\n5.  **Uncertainty Estimation via Nonparametric Bootstrap:** The statistical uncertainty on $\\hat{\\sigma}$ arises from the finite size $N$ of the Monte Carlo sample. This uncertainty is estimated using the nonparametric bootstrap method. The procedure is as follows:\n    a. We create $B$ bootstrap replicates. For each replicate $b=1, \\ldots, B$:\n    b. A new set of $N$ events is formed by sampling *with replacement* from the original set of $N$ generated events. More efficiently, we can resample the selection outcomes, $\\{S(x_1), \\ldots, S(x_N)\\}$. Let the resampled outcomes for replicate $b$ be $\\{S_1^{(b)}, \\ldots, S_N^{(b)}\\}$.\n    c. A bootstrap estimate of the acceptance, $\\hat{A}^{(b)}$, is computed from this new set:\n    $$\n    \\hat{A}^{(b)} = \\frac{1}{N} \\sum_{j=1}^N S_j^{(b)}\n    $$\n    d. This is then used to compute the bootstrap replicate of the cross section:\n    $$\n    \\hat{\\sigma}^{(b)} = \\sigma_{\\mathrm{ref}} \\cdot \\hat{A}^{(b)}\n    $$\n    Repeating this for all $B$ replicates yields a distribution of estimators, $\\{\\hat{\\sigma}^{(1)}, \\hat{\\sigma}^{(2)}, \\ldots, \\hat{\\sigma}^{(B)}\\}$, which serves as an empirical approximation to the true sampling distribution of $\\hat{\\sigma}$.\n\n6.  **Confidence Interval Half-Width Calculation:** An equal-tailed confidence interval for $\\hat{\\sigma}$ is constructed from the sorted distribution of bootstrap replicates. For a given significance level $\\alpha$, the lower and upper bounds of the interval are the quantiles $Q_{\\alpha/2}$ and $Q_{1-\\alpha/2}$ of the bootstrap distribution.\n    $$\n    Q_{\\alpha/2} = \\text{quantile}\\left(\\{\\hat{\\sigma}^{(b)}\\}_{b=1}^B, \\alpha/2\\right)\n    $$\n    $$\n    Q_{1-\\alpha/2} = \\text{quantile}\\left(\\{\\hat{\\sigma}^{(b)}\\}_{b=1}^B, 1-\\alpha/2\\right)\n    $$\n    The problem asks for the symmetric half-width of this confidence interval, $\\Delta \\hat{\\sigma}$, which is defined as:\n    $$\n    \\Delta \\hat{\\sigma} = \\frac{Q_{1-\\alpha/2} - Q_{\\alpha/2}}{2}\n    $$\n    This value, $\\Delta \\hat{\\sigma}$, is the second part of the output pair $[\\hat{\\sigma}, \\Delta \\hat{\\sigma}]$.\n\nThis entire procedure is implemented for each of the four test cases provided, using their respective parameter sets.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases, calculating the estimated cross section\n    and its bootstrap uncertainty half-width.\n    \"\"\"\n    test_cases = [\n        # (seed, N, mu, sigma, kappa, tau, sigma_ref, B, alpha)\n        (12345, 4000, 50, 10, 2, 40, 80, 300, 0.32),\n        (24680, 4000, 50, 10, 2, 70, 80, 300, 0.32),\n        (98765, 4000, 50, 10, 2, 10, 80, 300, 0.32),\n        (13579, 200, 50, 10, 2, 50, 80, 800, 0.32),\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, N, mu, sigma, kappa, tau, sigma_ref, B, alpha = case\n\n        # Initialize a random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Generate N latent variables z from N(0,1).\n        z_samples = rng.standard_normal(size=N)\n\n        # Step 2: Apply the generator function G(z) to get observables x.\n        # G(z) = max(0, mu + sigma*z + kappa*(z^2 - 1))\n        poly_part = mu + sigma * z_samples + kappa * (np.square(z_samples) - 1)\n        x_samples = np.maximum(0, poly_part)\n\n        # Step 3: Apply the selection predicate S(x) and get a boolean mask.\n        # S(x) = 1 if x >= tau, 0 otherwise.\n        selected_mask = (x_samples >= tau)\n\n        # Step 4: Calculate the central estimate of acceptance and cross section.\n        # A_hat = mean(S(G(z_i)))\n        A_hat = np.mean(selected_mask)\n        # sigma_hat = sigma_ref * A_hat\n        sigma_hat = sigma_ref * A_hat\n\n        # Step 5: Perform nonparametric bootstrap to estimate uncertainty.\n        sigma_hat_bootstrap = np.empty(B)\n        \n        # We can efficiently bootstrap by resampling the boolean selection mask.\n        # This avoids recomputing G(z) and S(x) for each bootstrap replicate.\n        for b in range(B):\n            # Resample indices with replacement from the N original samples.\n            bootstrap_indices = rng.choice(N, size=N, replace=True)\n            # Create a resampled selection mask.\n            resampled_mask = selected_mask[bootstrap_indices]\n            # Calculate the bootstrap replicate of acceptance.\n            A_hat_b = np.mean(resampled_mask)\n            # Calculate the bootstrap replicate of the cross section.\n            sigma_hat_bootstrap[b] = sigma_ref * A_hat_b\n\n        # Step 6: Compute the confidence interval and its symmetric half-width.\n        q_low_frac = alpha / 2.0\n        q_high_frac = 1.0 - alpha / 2.0\n        \n        # Calculate quantiles of the bootstrap distribution.\n        q_low = np.quantile(sigma_hat_bootstrap, q_low_frac)\n        q_high = np.quantile(sigma_hat_bootstrap, q_high_frac)\n        \n        # Calculate the symmetric half-width.\n        delta_sigma_hat = (q_high - q_low) / 2.0\n        \n        results.append([sigma_hat, delta_sigma_hat])\n\n    # Format the final output as a string representation of a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}