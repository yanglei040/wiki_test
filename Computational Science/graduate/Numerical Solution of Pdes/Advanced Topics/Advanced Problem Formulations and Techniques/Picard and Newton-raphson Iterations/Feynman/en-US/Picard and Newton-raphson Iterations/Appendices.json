{
    "hands_on_practices": [
        {
            "introduction": "Solving nonlinear partial differential equations (PDEs) numerically requires a process of linearization, where the complex problem is approximated by a sequence of solvable linear systems. The Picard and Newton-Raphson iterations represent two of the most fundamental strategies for this linearization. This first practice  provides a foundational, hands-on opportunity to derive and implement the discrete matrices for both methods, applying them to a nonlinear diffusion problem. By assembling and analyzing these matrices, you will gain direct insight into their structural differences and numerical properties, such as symmetry and conditioning, which are critical for understanding their performance.",
            "id": "3431358",
            "problem": "Consider the nonlinear scalar Partial Differential Equation (PDE) in conservative form on the unit interval with homogeneous Dirichlet boundary conditions,\n$$\n-\\frac{d}{dx}\\left(\\left(1+u(x)^2\\right)\\frac{du}{dx}(x)\\right) = f(x), \\quad x \\in (0,1), \\quad u(0) = 0, \\quad u(1) = 0.\n$$\nYou will construct a uniform one-dimensional mesh with $n$ interior nodes, mesh spacing $h = \\frac{1}{n+1}$, and node locations $x_i = i h$ for $i = 1,2,\\dots,n$. Use a conservative finite-difference discretization derived from central differences on the flux. Specifically:\n- Define the coefficient function $a(u) = 1 + u^2$.\n- Approximate the edge-centered coefficient at $x_{i+\\frac{1}{2}}$ by the arithmetic average of node-centered values,\n$$\na_{i+\\frac{1}{2}} \\approx 1 + \\frac{1}{2}\\left(u_i^2 + u_{i+1}^2\\right),\n$$\nwhere $u_i \\approx u(x_i)$ and the boundary values $u_0 = u(0)$ and $u_{n+1} = u(1)$ are fixed and equal to zero.\n- Approximate the flux at the edge by $q_{i+\\frac{1}{2}} \\approx a_{i+\\frac{1}{2}} \\frac{u_{i+1} - u_i}{h}$, and approximate the divergence by the edge difference $\\frac{q_{i+\\frac{1}{2}} - q_{i-\\frac{1}{2}}}{h}$.\n\nStarting from a given iterate $u^{(k)}$, implement:\n1. One Picard (fixed-point) linearization step: freeze the coefficient at the current iterate and form the linear system corresponding to\n$$\n-\\frac{1}{h}\\left(a_{i+\\frac{1}{2}}^{(k)} \\frac{u_{i+1}^{(k+1)} - u_i^{(k+1)}}{h} - a_{i-\\frac{1}{2}}^{(k)} \\frac{u_i^{(k+1)} - u_{i-1}^{(k+1)}}{h}\\right) = f_i, \\quad i = 1,\\dots,n,\n$$\nwith $u_0^{(k+1)} = 0$ and $u_{n+1}^{(k+1)} = 0$. Assemble the corresponding matrix $A_{\\mathrm{Pic}}(u^{(k)})$ and right-hand side vector $\\mathbf{f}$.\n2. One Newton-Raphson step: derive the discrete residual vector $\\mathbf{R}(u)$ from the conservative discretization and form the Jacobian matrix $J(u^{(k)})$ by differentiating the residual with respect to the interior nodal values. Solve\n$$\nJ(u^{(k)}) \\, \\delta^{(k)} = -\\mathbf{R}(u^{(k)}),\n$$\nand update $u^{(k+1)} = u^{(k)} + \\delta^{(k)}$ on the interior nodes (the boundary values remain fixed and are not part of the unknown vector).\n\nYour program must:\n- Assemble the Picard matrix $A_{\\mathrm{Pic}}(u^{(k)})$ and the Newton Jacobian $J(u^{(k)})$ using the scheme above for each specified test case.\n- Compute the two-norm condition numbers $\\kappa_2\\left(A_{\\mathrm{Pic}}(u^{(k)})\\right)$ and $\\kappa_2\\left(J(u^{(k)})\\right)$.\n- Compute the ratio $\\rho = \\kappa_2\\left(J(u^{(k)})\\right) / \\kappa_2\\left(A_{\\mathrm{Pic}}(u^{(k)})\\right)$.\n- Verify whether $A_{\\mathrm{Pic}}(u^{(k)})$ is Symmetric Positive Definite (SPD) by checking symmetry and positive definiteness via its eigenvalues.\n- Check whether $J(u^{(k)})$ is symmetric within a relative Frobenius norm tolerance.\n\nUse the following test suite of parameter values, where $n$ is the number of interior nodes, the initial iterate is $u_i^{(k)} = \\alpha \\sin(\\pi x_i)$, and the forcing samples are $f_i = f(x_i)$:\n- Test case 1 (happy path, linear baseline): $n = 8$, $\\alpha = 0$, $f(x) = \\sin(\\pi x)$.\n- Test case 2 (moderate nonlinearity): $n = 32$, $\\alpha = 0.5$, $f(x) = \\sin(\\pi x)$.\n- Test case 3 (strong nonlinearity and mixed forcing): $n = 64$, $\\alpha = 2.0$, $f(x) = 1 + \\sin(2\\pi x)$.\n\nAngles in trigonometric functions must be interpreted in radians. No physical units are involved.\n\nFinal output format:\n- For each test case, produce five values in the order: $\\kappa_2\\left(A_{\\mathrm{Pic}}\\right)$ (float), $\\kappa_2\\left(J\\right)$ (float), $\\rho$ (float), $\\mathrm{SPD}(A_{\\mathrm{Pic}})$ (boolean), and $\\mathrm{Sym}(J)$ (boolean).\n- Round all float outputs to six decimal places.\n- Aggregate the results of all test cases into a single line printed as a comma-separated list enclosed in square brackets, flattened across test cases, for example:\n$$\n[\\text{condP}_1,\\text{condN}_1,\\text{ratio}_1,\\text{SPD}_1,\\text{Sym}_1,\\text{condP}_2,\\dots,\\text{Sym}_3].\n$$\nYour program must be a complete, runnable implementation that performs the steps above and prints the single required line.",
            "solution": "The user-provided problem has been analyzed and is determined to be **valid**. It is a well-posed, scientifically grounded problem in numerical analysis that is self-contained and free of contradictions or ambiguities.\n\n### Analysis of the Discretized System\n\nThe nonlinear Partial Differential Equation (PDE) is given in conservative form:\n$$\n-\\frac{d}{dx}\\left(\\left(1+u(x)^2\\right)\\frac{du}{dx}(x)\\right) = f(x), \\quad x \\in (0,1)\n$$\nwith homogeneous Dirichlet boundary conditions $u(0) = 0$ and $u(1) = 0$.\nThe problem specifies a finite-difference discretization on a uniform mesh with $n$ interior nodes $x_i = ih$ for $i=1, \\dots, n$ and mesh spacing $h=1/(n+1)$. The flux an its divergence are approximated as:\n$$\nq_{i+\\frac{1}{2}} \\approx a_{i+\\frac{1}{2}} \\frac{u_{i+1} - u_i}{h}, \\quad \\text{where} \\quad a_{i+\\frac{1}{2}} = 1 + \\frac{1}{2}\\left(u_i^2 + u_{i+1}^2\\right)\n$$\n$$\n-\\left.\\frac{dq}{dx}\\right|_{x_i} \\approx -\\frac{q_{i+\\frac{1}{2}} - q_{i-\\frac{1}{2}}}{h}\n$$\nwhere $u_i \\approx u(x_i)$. The boundary values are $u_0 = 0$ and $u_{n+1}=0$.\nSubstituting the flux approximation into the divergence approximation gives the discrete equation at each interior node $i$:\n$$\n-\\frac{1}{h^2}\\left[ a_{i+\\frac{1}{2}}(u_{i+1} - u_i) - a_{i-\\frac{1}{2}}(u_i - u_{i-1}) \\right] = f_i\n$$\nWe define the discrete residual vector $\\mathbf{R}(\\mathbf{u})$ for the vector of unknowns $\\mathbf{u} = [u_1, u_2, \\dots, u_n]^T$. The $i$-th component of the residual is:\n$$\nR_i(\\mathbf{u}) = -\\frac{1}{h^2}\\left[ \\left(1 + \\frac{u_i^2+u_{i+1}^2}{2}\\right)(u_{i+1} - u_i) - \\left(1 + \\frac{u_{i-1}^2+u_i^2}{2}\\right)(u_i - u_{i-1}) \\right] - f_i\n$$\nThe problem is to find $\\mathbf{u}$ such that $\\mathbf{R}(\\mathbf{u}) = \\mathbf{0}$. We will now derive the matrices for one step of the Picard and Newton-Raphson methods.\n\n### 1. Picard Linearization\n\nThe Picard (or fixed-point) method linearizes the problem by evaluating the nonlinear coefficient function $a(u)$ at the previous iterate, $\\mathbf{u}^{(k)}$. The system for the new iterate, $\\mathbf{u}^{(k+1)}$, is:\n$$\n-\\frac{1}{h^2}\\left[ a_{i+\\frac{1}{2}}^{(k)}(u_{i+1}^{(k+1)} - u_i^{(k+1)}) - a_{i-\\frac{1}{2}}^{(k)}(u_i^{(k+1)} - u_{i-1}^{(k+1)}) \\right] = f_i\n$$\nwhere $a_{i+\\frac{1}{2}}^{(k)} = 1 + \\frac{1}{2}\\left((u_i^{(k)})^2 + (u_{i+1}^{(k)})^2\\right)$. This equation can be written as a linear system $A_{\\mathrm{Pic}}(\\mathbf{u}^{(k)}) \\mathbf{u}^{(k+1)} = \\mathbf{f}$. By rearranging the terms, we can identify the entries of the matrix $A_{\\mathrm{Pic}}(\\mathbf{u}^{(k)})$. For row $i$, which corresponds to the equation for $u_i^{(k+1)}$, we have:\n$$\n\\frac{1}{h^2}\\left[ -a_{i-\\frac{1}{2}}^{(k)}u_{i-1}^{(k+1)} + \\left(a_{i+\\frac{1}{2}}^{(k)} + a_{i-\\frac{1}{2}}^{(k)}\\right)u_i^{(k+1)} - a_{i+\\frac{1}{2}}^{(k)}u_{i+1}^{(k+1)} \\right] = f_i\n$$\nThe matrix $A_{\\mathrm{Pic}}(\\mathbf{u}^{(k)})$ is therefore an $n \\times n$ tridiagonal matrix with the following entries:\n- Diagonal: $(A_{\\mathrm{Pic}})_{i,i} = \\frac{1}{h^2} \\left(a_{i+\\frac{1}{2}}^{(k)} + a_{i-\\frac{1}{2}}^{(k)}\\right)$\n- Sub-diagonal: $(A_{\\mathrm{Pic}})_{i,i-1} = -\\frac{1}{h^2} a_{i-\\frac{1}{2}}^{(k)}$\n- Super-diagonal: $(A_{\\mathrm{Pic}})_{i,i+1} = -\\frac{1}{h^2} a_{i+\\frac{1}{2}}^{(k)}$\n\nThe matrix $A_{\\mathrm{Pic}}$ is symmetric because $(A_{\\mathrm{Pic}})_{i,i+1} = -\\frac{1}{h^2} a_{i+\\frac{1}{2}}^{(k)} = (A_{\\mathrm{Pic}})_{i+1,i}$. Since $a(u) = 1+u^2 \\geq 1$, all coefficients $a_{i\\pm 1/2}^{(k)}$ are positive. The matrix is also diagonally dominant and irreducible, which guarantees it is positive definite. Thus, $A_{\\mathrm{Pic}}$ is Symmetric Positive Definite (SPD).\n\n### 2. Newton-Raphson Method\n\nThe Newton-Raphson method solves $\\mathbf{R}(\\mathbf{u}) = \\mathbf{0}$ by iteratively finding a correction $\\delta^{(k)}$ from the linear system $J(\\mathbf{u}^{(k)})\\delta^{(k)} = -\\mathbf{R}(\\mathbf{u}^{(k)})$ and updating $\\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + \\delta^{(k)}$. The matrix $J$ is the Jacobian of the residual vector $\\mathbf{R}$, with entries $J_{ij} = \\frac{\\partial R_i}{\\partial u_j}$. Given that $R_i$ only depends on $u_{i-1}$, $u_i$, and $u_{i+1}$, the Jacobian is also a tridiagonal matrix.\nWe compute the partial derivatives of $R_i(\\mathbf{u})$ (omitting the superscript $(k)$ for clarity):\n\n- **Main diagonal, $J_{i,i} = \\frac{\\partial R_i}{\\partial u_i}$:**\n$$\nJ_{i,i} = \\frac{\\partial}{\\partial u_i} \\left( -\\frac{1}{h^2}\\left[ a_{i+\\frac{1}{2}}(u_{i+1} - u_i) - a_{i-\\frac{1}{2}}(u_i - u_{i-1}) \\right] \\right)\n$$\nUsing the product rule and $\\frac{\\partial a_{i\\pm 1/2}}{\\partial u_i} = u_i$:\n$$\nJ_{i,i} = -\\frac{1}{h^2} \\left[ (u_i(u_{i+1}-u_i) - a_{i+\\frac{1}{2}}) - (u_i(u_i-u_{i-1}) + a_{i-\\frac{1}{2}}) \\right]\n$$\n$$\nJ_{i,i} = \\frac{1}{h^2} \\left[ a_{i+\\frac{1}{2}} + a_{i-\\frac{1}{2}} - u_i(u_{i+1} - 2u_i + u_{i-1}) \\right]\n$$\n\n- **Super-diagonal, $J_{i,i+1} = \\frac{\\partial R_i}{\\partial u_{i+1}}$:**\n$$\nJ_{i,i+1} = \\frac{\\partial}{\\partial u_{i+1}} \\left( -\\frac{1}{h^2} a_{i+\\frac{1}{2}}(u_{i+1} - u_i) \\right)\n$$\nUsing the product rule and $\\frac{\\partial a_{i+1/2}}{\\partial u_{i+1}} = u_{i+1}$:\n$$\nJ_{i,i+1} = -\\frac{1}{h^2} \\left[ u_{i+1}(u_{i+1}-u_i) + a_{i+\\frac{1}{2}}(1) \\right] = -\\frac{1}{h^2} \\left[ a_{i+\\frac{1}{2}} + u_{i+1}(u_{i+1}-u_i) \\right]\n$$\n\n- **Sub-diagonal, $J_{i,i-1} = \\frac{\\partial R_i}{\\partial u_{i-1}}$:**\n$$\nJ_{i,i-1} = \\frac{\\partial}{\\partial u_{i-1}} \\left( \\frac{1}{h^2} a_{i-\\frac{1}{2}}(u_i - u_{i-1}) \\right)\n$$\nUsing the product rule and $\\frac{\\partial a_{i-1/2}}{\\partial u_{i-1}} = u_{i-1}$:\n$$\nJ_{i,i-1} = \\frac{1}{h^2} \\left[ u_{i-1}(u_i-u_{i-1}) + a_{i-\\frac{1}{2}}(-1) \\right] = \\frac{1}{h^2} \\left[ u_{i-1}(u_i-u_{i-1}) - a_{i-\\frac{1}{2}} \\right]\n$$\n\nThe Jacobian $J$ is generally not symmetric. To check symmetry, we compare $J_{i,i+1}$ with $J_{i+1,i}$. From the sub-diagonal formula with index $i+1$:\n$$\nJ_{i+1,i} = \\frac{1}{h^2} \\left[ u_i(u_{i+1}-u_i) - a_{i+\\frac{1}{2}} \\right]\n$$\nComparing this with $J_{i,i+1}$, we see that $J_{i,i+1} \\neq J_{i+1,i}$ unless $\\mathbf{u}=\\mathbf{0}$, in which case the problem is linear and $J=A_{\\mathrm{Pic}}$.\n\n### Summary of Computations\n\nFor each test case, we will:\n1.  Construct the $n \\times n$ matrix $A_{\\mathrm{Pic}}$ based on the initial iterate $\\mathbf{u}^{(k)}$.\n2.  Construct the $n \\times n$ matrix $J$ based on $\\mathbf{u}^{(k)}$.\n3.  Compute the $2$-norm condition numbers $\\kappa_2(A_{\\mathrm{Pic}})$ and $\\kappa_2(J)$.\n4.  Compute the ratio $\\rho = \\kappa_2(J) / \\kappa_2(A_{\\mathrm{Pic}})$.\n5.  Verify if $A_{\\mathrm{Pic}}$ is SPD by checking for symmetry and for all-positive eigenvalues.\n6.  Verify if $J$ is symmetric by checking if $J$ is close to its transpose $J^T$.\n\nThe implementation will follow these steps to generate the required outputs.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases, assembling Picard and Newton\n    matrices, and computing their properties.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (8, 0.0, lambda x: np.sin(np.pi * x)),\n        (32, 0.5, lambda x: np.sin(np.pi * x)),\n        (64, 2.0, lambda x: 1 + np.sin(2 * np.pi * x)),\n    ]\n\n    results = []\n    \n    for n, alpha, f_func in test_cases:\n        # 1. Setup grid and initial iterate\n        h = 1.0 / (n + 1)\n        x_nodes = np.arange(1, n + 1) * h\n        u_k = alpha * np.sin(np.pi * x_nodes)\n        \n        # Create an extended u vector including boundary values u_0 = u_{n+1} = 0\n        u_ext = np.concatenate(([0.0], u_k, [0.0]))\n\n        # Pre-calculate edge-centered coefficients a_{i+1/2}\n        # a_coeffs[i] corresponds to a_{i+1/2} where i is from 0 to n\n        a_coeffs = 1.0 + 0.5 * (u_ext[:-1]**2 + u_ext[1:]**2)\n        \n        # 2. Assemble Picard Matrix A_Pic\n        # Main diagonal of A_Pic\n        diag_p = (a_coeffs[1:] + a_coeffs[:-1]) / h**2\n        # Off-diagonal of A_Pic (symmetric matrix)\n        off_diag_p = -a_coeffs[1:-1] / h**2\n        \n        A_pic = np.diag(diag_p) + np.diag(off_diag_p, k=1) + np.diag(off_diag_p, k=-1)\n\n        # 3. Assemble Newton Jacobian J\n        # Main diagonal of J\n        u_i = u_ext[1:-1]\n        u_im1 = u_ext[:-2]\n        u_ip1 = u_ext[2:]\n        term = -u_i * (u_ip1 - 2*u_i + u_im1)\n        diag_j = (a_coeffs[1:] + a_coeffs[:-1] + term) / h**2\n\n        # Super-diagonal of J\n        term_s = -u_ip1[:-1] * (u_ip1[:-1] - u_i[:-1])\n        super_diag_j = (-a_coeffs[1:-1] + term_s) / h**2\n\n        # Sub-diagonal of J\n        term_l = u_im1[1:] * (u_i[1:] - u_im1[1:])\n        sub_diag_j = (-a_coeffs[1:-1] + term_l) / h**2\n        \n        J = np.diag(diag_j) + np.diag(super_diag_j, k=1) + np.diag(sub_diag_j, k=-1)\n\n        # 4. Compute required values\n        cond_p = np.linalg.cond(A_pic, 2)\n        cond_j = np.linalg.cond(J, 2)\n        ratio = cond_j / cond_p\n        \n        # Check SPD for A_pic.\n        # It is symmetric by construction, so we only check positive definiteness.\n        # Use eigvalsh for symmetric matrices. Small tolerance for floating point.\n        eigvals_p = np.linalg.eigvalsh(A_pic)\n        is_spd = np.all(eigvals_p > 1e-12)\n\n        # Check symmetry for J using a standard tolerance\n        is_sym_j = np.allclose(J, J.T)\n        \n        # Append formatted results\n        results.extend([\n            f\"{cond_p:.6f}\",\n            f\"{cond_j:.6f}\",\n            f\"{ratio:.6f}\",\n            str(is_spd),\n            str(is_sym_j),\n        ])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "In practical applications, we often face a trade-off between the global convergence of simpler methods and the rapid local convergence of more complex ones. A hybrid approach, leveraging the strengths of both, is a powerful algorithmic strategy. This exercise  guides you in creating such a hybrid solver, starting with the robust but linearly-converging Picard iteration and switching to the quadratically-converging Newton-Raphson method once the iterate is sufficiently close to the solution. You will develop a sophisticated switching criterion based on the stabilization of the Picard iteration's convergence rate, connecting an empirical heuristic to the theoretical properties of the underlying Jacobian.",
            "id": "3431395",
            "problem": "Consider the stationary one-dimensional nonlinear reaction–diffusion boundary value problem posed on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions. Let $u:[0,1]\\to\\mathbb{R}$ be the unknown function. The partial differential equation (PDE) is\n$$\n-\\frac{d^2 u}{dx^2} + \\beta\\, u^3 = f(x), \\quad x\\in(0,1), \\quad u(0) = 0, \\quad u(1) = 0,\n$$\nwhere $\\beta>0$ is a given parameter and $f$ is a prescribed source term.\n\nDiscretize the PDE using the standard second-order centered finite difference method on $N$ interior grid points with mesh size $h = \\frac{1}{N+1}$, grid nodes $x_i = i h$ for $i=1,\\dots,N$, and homogeneous Dirichlet boundary conditions $u_0 = u_{N+1}=0$. Let $\\mathbf{u}\\in\\mathbb{R}^N$ be the vector of nodal values $u_i \\approx u(x_i)$, and define the discrete Laplacian matrix $\\mathbf{K}\\in\\mathbb{R}^{N\\times N}$ by\n$$\n\\mathbf{K} = \\frac{1}{h^2}\\begin{bmatrix}\n2 & -1 & & & \\\\\n-1 & 2 & -1 & & \\\\\n& \\ddots & \\ddots & \\ddots & \\\\\n& & -1 & 2 & -1 \\\\\n& & & -1 & 2\n\\end{bmatrix}.\n$$\nLet the discrete residual map $\\mathbf{F}:\\mathbb{R}^N\\to\\mathbb{R}^N$ be\n$$\n\\mathbf{F}(\\mathbf{u}) = \\mathbf{K}\\,\\mathbf{u} + \\beta\\, \\mathbf{u}^{\\odot 3} - \\mathbf{f},\n$$\nwhere $\\mathbf{u}^{\\odot 3}$ denotes the entrywise cube and $\\mathbf{f}\\in\\mathbb{R}^N$ is the discrete load vector $f_i = f(x_i)$.\n\nConsider two iterative solvers for the nonlinear system $\\mathbf{F}(\\mathbf{u})=\\mathbf{0}$:\n- Picard iteration: Given $\\mathbf{u}^{(k)}$, form the symmetric positive definite (SPD) matrix\n$$\n\\mathbf{M}(\\mathbf{u}^{(k)}) = \\mathbf{K} + \\beta\\,\\mathrm{diag}\\!\\left(\\big(\\mathbf{u}^{(k)}\\big)^{\\odot 2}\\right),\n$$\nand compute $\\mathbf{u}^{(k+1)}$ as the solution of the linear system\n$$\n\\mathbf{M}(\\mathbf{u}^{(k)})\\, \\mathbf{u}^{(k+1)} = \\mathbf{f}.\n$$\n- Newton–Raphson iteration: Given $\\mathbf{u}^{(k)}$, form the Jacobian\n$$\n\\mathbf{J}(\\mathbf{u}^{(k)}) = \\mathbf{K} + 3\\beta\\,\\mathrm{diag}\\!\\left(\\big(\\mathbf{u}^{(k)}\\big)^{\\odot 2}\\right),\n$$\nsolve for the update $\\boldsymbol{\\delta}^{(k)}$ in\n$$\n\\mathbf{J}(\\mathbf{u}^{(k)})\\, \\boldsymbol{\\delta}^{(k)} = -\\mathbf{F}\\big(\\mathbf{u}^{(k)}\\big),\n$$\nand set $\\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + \\boldsymbol{\\delta}^{(k)}$.\n\nDefine the Picard increment $\\mathbf{d}^{(k)} = \\mathbf{u}^{(k+1)} - \\mathbf{u}^{(k)}$ and the Picard contraction factor\n$$\nc_k = \\frac{\\left\\|\\mathbf{d}^{(k)}\\right\\|_2}{\\left\\|\\mathbf{d}^{(k-1)}\\right\\|_2}, \\quad k\\ge 1.\n$$\nDevise a residual-based indicator by defining\n$$\n\\gamma_k = \\frac{\\left\\|\\mathbf{d}^{(k)}\\right\\|_2}{\\left\\|\\mathbf{F}\\big(\\mathbf{u}^{(k)}\\big)\\right\\|_2}.\n$$\nImplement a switching rule from Picard to Newton–Raphson that triggers when the Picard contraction factor stabilizes, operationalized by the criterion\n$$\n\\left|c_k - c_{k-1}\\right| \\le \\varepsilon_c,\n$$\nand the residual-based indicator stabilizes,\n$$\n\\left|\\gamma_k - \\gamma_{k-1}\\right| \\le \\varepsilon_\\gamma,\n$$\nfor prescribed small thresholds $\\varepsilon_c>0$ and $\\varepsilon_\\gamma>0$.\n\nStarting from $\\mathbf{u}^{(0)} = \\mathbf{0}$, apply Picard iteration until the switching rule is satisfied, then switch to Newton–Raphson and iterate until the discrete residual norm $\\left\\|\\mathbf{F}(\\mathbf{u})\\right\\|_2$ is below a tolerance $T>0$ or a fixed maximum number of iterations is reached. Use the following source term:\n$$\nf(x) = \\sin(\\pi x),\n$$\nand the following test suite of parameter sets $(N,\\beta)$:\n- Test case 1 (happy path): $(N,\\beta) = (32,\\, 1.0)$.\n- Test case 2 (stronger nonlinearity): $(N,\\beta) = (32,\\, 4.0)$.\n- Test case 3 (coarser grid): $(N,\\beta) = (16,\\, 0.5)$.\n\nFor each test case, at the switching iteration index $k_\\star$, compute:\n- The residual-based indicator $\\gamma_{k_\\star}$.\n- The inverse Lipschitz constant of the Jacobian, interpreted as the operator norm of the inverse in the Euclidean norm for SPD matrices,\n$$\n\\left\\|\\mathbf{J}\\big(\\mathbf{u}^{(k_\\star)}\\big)^{-1}\\right\\|_2 = \\frac{1}{\\lambda_{\\min}\\!\\big(\\mathbf{J}(\\mathbf{u}^{(k_\\star)})\\big)},\n$$\nwhere $\\lambda_{\\min}(\\cdot)$ denotes the smallest eigenvalue.\n\nYour program must output, for each test case, the ratio\n$$\nq = \\frac{\\gamma_{k_\\star}}{\\left\\|\\mathbf{J}\\big(\\mathbf{u}^{(k_\\star)}\\big)^{-1}\\right\\|_2},\n$$\nas a floating-point number. The final output must be a single line containing the three ratios for the test suite as a comma-separated list enclosed in square brackets, for example $[q_1,q_2,q_3]$.\n\nMathematical tasks:\n- Starting from the definitions of the discrete operators and the iterative schemes, derive the relation\n$$\n\\mathbf{d}^{(k)} = -\\mathbf{M}\\big(\\mathbf{u}^{(k)}\\big)^{-1}\\,\\mathbf{F}\\big(\\mathbf{u}^{(k)}\\big),\n$$\nand show that the residual-based indicator $\\gamma_k$ equals the directional operator norm of $\\mathbf{M}\\big(\\mathbf{u}^{(k)}\\big)^{-1}$ applied to the discrete residual. Prove that, for $\\beta>0$, the Jacobian satisfies the Loewner ordering $\\mathbf{J}(\\mathbf{u}) \\succeq \\mathbf{M}(\\mathbf{u})$ for all $\\mathbf{u}$, which implies\n$$\n\\left\\|\\mathbf{J}(\\mathbf{u})^{-1}\\right\\|_2 \\le \\left\\|\\mathbf{M}(\\mathbf{u})^{-1}\\right\\|_2.\n$$\nConclude that stabilization of $\\gamma_k$ is equivalent to stabilization of an upper bound estimate on the inverse Lipschitz constant of the Jacobian, and explain why triggering the switch under the stated stabilization criteria is justified by the local linear behavior of Picard iteration near the solution.\n\nImplementation requirements:\n- Use $\\varepsilon_c = 10^{-3}$, $\\varepsilon_\\gamma = 10^{-3}$, maximum Picard iterations $100$, and Newton tolerance $T=10^{-10}$ with a maximum of $100$ Newton iterations.\n- All computations are to be carried out in consistent unitless terms; no physical units are required.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases listed above.",
            "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded in the numerical analysis of nonlinear partial differential equations, well-posed, and provides a complete and consistent set of definitions, parameters, and objectives.\n\nWe will first address the required mathematical derivations and justifications, and then outline the algorithm for the numerical computation.\n\nLet the discrete nonlinear system be $\\mathbf{F}(\\mathbf{u}) = \\mathbf{0}$, where the residual map $\\mathbf{F}:\\mathbb{R}^N\\to\\mathbb{R}^N$ is given by\n$$\n\\mathbf{F}(\\mathbf{u}) = \\mathbf{K}\\,\\mathbf{u} + \\beta\\, \\mathbf{u}^{\\odot 3} - \\mathbf{f}.\n$$\nHere, $\\mathbf{u} \\in \\mathbb{R}^N$ is the vector of unknowns, $\\mathbf{K} \\in \\mathbb{R}^{N \\times N}$ is the symmetric positive definite (SPD) discrete Laplacian, $\\beta > 0$ is a constant, $\\mathbf{u}^{\\odot 3}$ denotes the element-wise cube of the vector $\\mathbf{u}$, and $\\mathbf{f} \\in \\mathbb{R}^N$ is the discretized source term.\n\n### 1. Mathematical Derivations and Analysis\n\n**Relation for the Picard Increment**\n\nThe Picard iteration is defined by solving the linear system $\\mathbf{M}(\\mathbf{u}^{(k)})\\, \\mathbf{u}^{(k+1)} = \\mathbf{f}$ for the next iterate $\\mathbf{u}^{(k+1)}$, given the current iterate $\\mathbf{u}^{(k)}$. The matrix $\\mathbf{M}(\\mathbf{u}^{(k)})$ is given by\n$$\n\\mathbf{M}(\\mathbf{u}^{(k)}) = \\mathbf{K} + \\beta\\,\\mathrm{diag}\\!\\left(\\big(\\mathbf{u}^{(k)}\\big)^{\\odot 2}\\right).\n$$\nThe Picard increment is defined as $\\mathbf{d}^{(k)} = \\mathbf{u}^{(k+1)} - \\mathbf{u}^{(k)}$. To derive the requested relation, we manipulate the definition of the Picard step:\n$$\n\\mathbf{M}(\\mathbf{u}^{(k)})\\, \\mathbf{u}^{(k+1)} = \\mathbf{f}.\n$$\nWe subtract $\\mathbf{M}(\\mathbf{u}^{(k)})\\, \\mathbf{u}^{(k)}$ from both sides:\n$$\n\\mathbf{M}(\\mathbf{u}^{(k)}) \\left(\\mathbf{u}^{(k+1)} - \\mathbf{u}^{(k)}\\right) = \\mathbf{f} - \\mathbf{M}(\\mathbf{u}^{(k)})\\, \\mathbf{u}^{(k)}.\n$$\nThe left-hand side is $\\mathbf{M}(\\mathbf{u}^{(k)})\\, \\mathbf{d}^{(k)}$. For the right-hand side, we substitute the definition of $\\mathbf{M}(\\mathbf{u}^{(k)})$:\n$$\n\\mathbf{M}(\\mathbf{u}^{(k)})\\, \\mathbf{d}^{(k)} = \\mathbf{f} - \\left(\\mathbf{K} + \\beta\\,\\mathrm{diag}\\!\\left(\\big(\\mathbf{u}^{(k)}\\big)^{\\odot 2}\\right)\\right)\\mathbf{u}^{(k)}.\n$$\nRecognizing that for any vector $\\mathbf{v}$, $\\mathrm{diag}(\\mathbf{v}^{\\odot 2})\\mathbf{v} = \\mathbf{v}^{\\odot 3}$, we have:\n$$\n\\mathbf{M}(\\mathbf{u}^{(k)})\\, \\mathbf{d}^{(k)} = \\mathbf{f} - \\left(\\mathbf{K}\\mathbf{u}^{(k)} + \\beta\\left(\\mathbf{u}^{(k)}\\right)^{\\odot 3}\\right).\n$$\nThe term in the parentheses is, by definition, $\\mathbf{F}(\\mathbf{u}^{(k)}) + \\mathbf{f}$. Thus,\n$$\n\\mathbf{M}(\\mathbf{u}^{(k)})\\, \\mathbf{d}^{(k)} = \\mathbf{f} - (\\mathbf{F}(\\mathbf{u}^{(k)}) + \\mathbf{f}) = -\\mathbf{F}(\\mathbf{u}^{(k)}).\n$$\nSince $\\mathbf{K}$ is SPD and $\\beta\\,\\mathrm{diag}\\!\\left(\\big(\\mathbf{u}^{(k)}\\big)^{\\odot 2}\\right)$ is a positive semi-definite diagonal matrix, their sum $\\mathbf{M}(\\mathbf{u}^{(k)})$ is SPD and therefore invertible. We can solve for $\\mathbf{d}^{(k)}$:\n$$\n\\mathbf{d}^{(k)} = -\\mathbf{M}\\big(\\mathbf{u}^{(k)}\\big)^{-1}\\,\\mathbf{F}\\big(\\mathbf{u}^{(k)}\\big).\n$$\nThis completes the derivation.\n\n**Interpretation of the Residual-Based Indicator $\\gamma_k$**\n\nThe indicator $\\gamma_k$ is defined as\n$$\n\\gamma_k = \\frac{\\left\\|\\mathbf{d}^{(k)}\\right\\|_2}{\\left\\|\\mathbf{F}\\big(\\mathbf{u}^{(k)}\\big)\\right\\|_2}.\n$$\nUsing the result from the previous section, we can write\n$$\n\\gamma_k = \\frac{\\left\\|-\\mathbf{M}\\big(\\mathbf{u}^{(k)}\\big)^{-1}\\,\\mathbf{F}\\big(\\mathbf{u}^{(k)}\\big)\\right\\|_2}{\\left\\|\\mathbf{F}\\big(\\mathbf{u}^{(k)}\\big)\\right\\|_2} = \\frac{\\left\\|\\mathbf{M}\\big(\\mathbf{u}^{(k)}\\big)^{-1}\\,\\mathbf{F}\\big(\\mathbf{u}^{(k)}\\big)\\right\\|_2}{\\left\\|\\mathbf{F}\\big(\\mathbf{u}^{(k)}\\big)\\right\\|_2}.\n$$\nThis quantity measures the amplification factor of the matrix $\\mathbf{M}(\\mathbf{u}^{(k)})^{-1}$ when applied to the specific vector $\\mathbf{F}(\\mathbf{u}^{(k)})$. It is not the operator norm $\\left\\|\\mathbf{M}(\\mathbf{u}^{(k)})^{-1}\\right\\|_2$, which is the maximum such amplification over all possible non-zero vectors. Instead, it can be described as the norm of the operator $\\mathbf{M}(\\mathbf{u}^{(k)})^{-1}$ restricted to the one-dimensional subspace spanned by the residual vector $\\mathbf{F}(\\mathbf{u}^{(k)})$. As such, it provides a directional measure of the response of the Picard update to the current residual.\n\n**Loewner Ordering of Jacobian and Picard Matrix**\n\nThe Jacobian of the residual map $\\mathbf{F}(\\mathbf{u})$ is the matrix of partial derivatives, $\\mathbf{J}_{ij}(\\mathbf{u}) = \\frac{\\partial F_i}{\\partial u_j}$.\n$$\nF_i(\\mathbf{u}) = \\sum_{j=1}^N K_{ij}u_j + \\beta u_i^3 - f_i.\n$$\nThe partial derivatives are:\n$$\n\\frac{\\partial F_i}{\\partial u_j} = K_{ij} + 3\\beta u_i^2 \\delta_{ij},\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. In matrix form, this is:\n$$\n\\mathbf{J}(\\mathbf{u}) = \\mathbf{K} + 3\\beta\\,\\mathrm{diag}\\!\\left(\\mathbf{u}^{\\odot 2}\\right).\n$$\nTo prove the Loewner ordering $\\mathbf{J}(\\mathbf{u}) \\succeq \\mathbf{M}(\\mathbf{u})$, we must show that the difference matrix $\\mathbf{J}(\\mathbf{u}) - \\mathbf{M}(\\mathbf{u})$ is positive semi-definite (PSD).\n$$\n\\mathbf{J}(\\mathbf{u}) - \\mathbf{M}(\\mathbf{u}) = \\left(\\mathbf{K} + 3\\beta\\,\\mathrm{diag}\\!\\left(\\mathbf{u}^{\\odot 2}\\right)\\right) - \\left(\\mathbf{K} + \\beta\\,\\mathrm{diag}\\!\\left(\\mathbf{u}^{\\odot 2}\\right)\\right) = 2\\beta\\,\\mathrm{diag}\\!\\left(\\mathbf{u}^{\\odot 2}\\right).\n$$\nThis difference is a diagonal matrix. Its diagonal entries are $2\\beta (u_i)^2$. Since $\\beta > 0$ and $(u_i)^2 \\ge 0$ for all $i=1,\\dots,N$, all diagonal entries are non-negative. A diagonal matrix with non-negative entries on its diagonal is, by definition, positive semi-definite. Thus, $\\mathbf{J}(\\mathbf{u}) \\succeq \\mathbf{M}(\\mathbf{u})$.\n\nA standard result in matrix analysis states that if $\\mathbf{A}$ and $\\mathbf{B}$ are SPD matrices and $\\mathbf{A} \\succeq \\mathbf{B}$, then $\\mathbf{B}^{-1} \\succeq \\mathbf{A}^{-1}$. Since both $\\mathbf{J}(\\mathbf{u})$ and $\\mathbf{M}(\\mathbf{u})$ are SPD for any $\\mathbf{u}$ (as they are the sum of the SPD matrix $\\mathbf{K}$ and a PSD diagonal matrix), we can apply this result to conclude that $\\mathbf{M}(\\mathbf{u})^{-1} \\succeq \\mathbf{J}(\\mathbf{u})^{-1}$. This implies that for any vector $\\mathbf{v}$, $\\mathbf{v}^T \\mathbf{M}(\\mathbf{u})^{-1} \\mathbf{v} \\ge \\mathbf{v}^T \\mathbf{J}(\\mathbf{u})^{-1} \\mathbf{v}$.\nFor any SPD matrix $\\mathbf{S}$, the operator $2$-norm is $\\|\\mathbf{S}\\|_2 = \\lambda_{\\max}(\\mathbf{S})$. Since $\\mathbf{M}(\\mathbf{u})^{-1} - \\mathbf{J}(\\mathbf{u})^{-1}$ is PSD, its eigenvalues are non-negative, so $\\lambda_{\\max}(\\mathbf{M}(\\mathbf{u})^{-1} - \\mathbf{J}(\\mathbf{u})^{-1}) \\ge 0$. This leads to $\\lambda_{\\max}(\\mathbf{M}(\\mathbf{u})^{-1}) \\ge \\lambda_{\\max}(\\mathbf{J}(\\mathbf{u})^{-1})$, which directly implies:\n$$\n\\left\\|\\mathbf{M}(\\mathbf{u})^{-1}\\right\\|_2 \\ge \\left\\|\\mathbf{J}(\\mathbf{u})^{-1}\\right\\|_2.\n$$\nAs the problem states `for SPD matrices`, the inverse norm is also given by the reciprocal of the minimum eigenvalue of the original matrix. Hence, $\\frac{1}{\\lambda_{\\min}(\\mathbf{M}(\\mathbf{u}))} \\ge \\frac{1}{\\lambda_{\\min}(\\mathbf{J}(\\mathbf{u}))}$.\n\n**Justification for the Switching Rule**\n\nThe strategy of switching from a Picard iteration to a Newton-Raphson iteration is a common technique in solving nonlinear systems.\n- **Picard Iteration:** This is a form of fixed-point iteration. It typically exhibits global convergence (or at least a large basin of attraction) but converges only linearly. The convergence rate is governed by the spectral radius of the iteration's Jacobian.\n- **Newton-Raphson Iteration:** This method converges quadratically when the iterate is sufficiently close to the solution but may diverge if started far away.\n\nThe hybrid strategy leverages the strengths of both: use the robust Picard method to generate a sequence of iterates that approach the solution, and once the iterates are in the basin of quadratic convergence for Newton's method, switch to the latter for rapid convergence. The challenge is to identify when to switch.\n\nThe proposed switching criteria are based on monitoring the convergence behavior of the Picard iteration.\n$1.$ **Stabilization of the Contraction Factor, $c_k$:** The quantity $c_k = \\|\\mathbf{d}^{(k)}\\|_2 / \\|\\mathbf{d}^{(k-1)}\\|_2$ is an empirical measure of the linear convergence rate. When an iteration enters its asymptotic linear convergence phase, this ratio approaches a constant value. The criterion $|c_k - c_{k-1}| \\le \\varepsilon_c$ checks for this stabilization, indicating that the iteration has settled into a predictable, linear regime.\n$2.$ **Stabilization of the Residual-Based Indicator, $\\gamma_k$:** The quantity $\\gamma_k$ serves as a directional probe of the norm of $\\mathbf{M}(\\mathbf{u}^{(k)})^{-1}$. As $\\mathbf{u}^{(k)}$ approaches a limit, we expect $\\mathbf{M}(\\mathbf{u}^{(k)})$ and the direction of the residual vector $\\mathbf{F}(\\mathbf{u}^{(k)})$ to stabilize. Consequently, $\\gamma_k$ should also approach a constant value. The criterion $|\\gamma_k - \\gamma_{k-1}| \\le \\varepsilon_\\gamma$ detects this stabilization.\n\nAs we have shown, $\\left\\|\\mathbf{M}(\\mathbf{u})^{-1}\\right\\|_2$ provides an upper bound for $\\left\\|\\mathbf{J}(\\mathbf{u})^{-1}\\right\\|_2$. The local convergence theory of Newton's method depends on properties of the Jacobian and its inverse near the solution. The stabilization of $\\gamma_k$ suggests that a related quantity, the upper bound on the inverse Jacobian norm estimate, has also stabilized.\n\nTaken together, the stabilization of both $c_k$ and $\\gamma_k$ provides a strong heuristic that the Picard iteration is well within the region of linear convergence where the local dynamics are stable and predictable. This stability implies that the current iterate $\\mathbf{u}^{(k)}$ is likely close enough to the true solution for the Newton-Raphson method to converge quadratically. Therefore, switching at this point is a well-justified strategy to accelerate convergence.\n\n### 2. Algorithmic Implementation\n\nThe algorithm proceeds as follows for each test case $(N, \\beta)$:\n1.  **Initialization:** Set grid parameters $h = 1/(N+1)$, construct the grid points $x_i$, the discrete Laplacian matrix $\\mathbf{K}$, and the source vector $\\mathbf{f}$. Initialize the solution vector $\\mathbf{u} = \\mathbf{0}$, and set up data structures to store the history of increments, contraction factors, and gamma indicators.\n2.  **Iterative Solver:** Enter a main loop that runs until convergence or a maximum number of iterations is reached.\n3.  **Picard Phase:** For the initial iterations (up to a maximum of $100$ or until a switch is triggered):\n    a. Calculate the residual $\\mathbf{F}(\\mathbf{u}^{(k)})$.\n    b. Construct the Picard matrix $\\mathbf{M}(\\mathbf{u}^{(k)})$.\n    c. Solve the linear system $\\mathbf{M}(\\mathbf{u}^{(k)}) \\mathbf{u}^{(k+1)} = \\mathbf{f}$ to find the next iterate $\\mathbf{u}^{(k+1)}$.\n    d. Compute the increment $\\mathbf{d}^{(k)} = \\mathbf{u}^{(k+1)} - \\mathbf{u}^{(k)}$ and its norm.\n    e. Compute $\\gamma_k$ and, if $k \\ge 1$, $c_k$. Store these values.\n    f. If $k \\ge 2$, check the switching conditions: $|c_k - c_{k-1}| \\le \\varepsilon_c$ and $|\\gamma_k - \\gamma_{k-1}| \\le \\varepsilon_\\gamma$.\n    g. If the conditions are met, this is the switching iteration $k_\\star = k$.\n        i.  Store $\\gamma_{k_\\star} = \\gamma_k$.\n        ii. Construct the Jacobian $\\mathbf{J}(\\mathbf{u}^{(k_\\star)})$ using the current iterate $\\mathbf{u}^{(k)}$.\n        iii.Compute its smallest eigenvalue $\\lambda_{\\min}$ and the inverse norm $\\left\\|\\mathbf{J}^{-1}\\right\\|_2 = 1/\\lambda_{\\min}$.\n        iv. Calculate the ratio $q = \\gamma_{k_\\star} / \\left\\|\\mathbf{J}^{-1}\\right\\|_2$.\n        v.  Set a flag to `True` to indicate the switch has occurred.\n    h. Update $\\mathbf{u} \\leftarrow \\mathbf{u}^{(k+1)}$.\n4.  **Newton-Raphson Phase:** Once the switching flag is set, for all subsequent iterations:\n    a. Calculate the residual $\\mathbf{F}(\\mathbf{u}^{(k)})$. Check if its norm is below the tolerance $T=10^{-10}$. If so, terminate.\n    b. Construct the Jacobian matrix $\\mathbf{J}(\\mathbf{u}^{(k)})$.\n    c. Solve the linear system $\\mathbf{J}(\\mathbf{u}^{(k)}) \\boldsymbol{\\delta}^{(k)} = -\\mathbf{F}(\\mathbf{u}^{(k)})$ for the update $\\boldsymbol{\\delta}^{(k)}$.\n    d. Update the solution $\\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + \\boldsymbol{\\delta}^{(k)}$.\n5.  **Output:** After the loop terminates for a given test case, the computed value of $q$ is stored. The final output is a list of these $q$ values for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the nonlinear reaction-diffusion problem using a hybrid Picard/Newton-Raphson method\n    and computes the specified ratio q at the switching point for three test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (32, 1.0),  # Test case 1\n        (32, 4.0),  # Test case 2\n        (16, 0.5),  # Test case 3\n    ]\n\n    # Implementation parameters\n    eps_c = 1e-3\n    eps_gamma = 1e-3\n    max_picard_iter = 100\n    max_newton_iter = 100\n    newton_tol = 1e-10\n\n    results = []\n\n    for N, beta in test_cases:\n        # --- Setup for the current test case ---\n        h = 1.0 / (N + 1)\n        x_nodes = np.linspace(h, 1.0 - h, N)\n        f_vec = np.sin(np.pi * x_nodes)\n        \n        # Construct the discrete Laplacian matrix K\n        diag_main = np.full(N, 2.0)\n        diag_sub = np.ones(N - 1)\n        K = (np.diag(diag_main) - np.diag(diag_sub, k=1) - np.diag(diag_sub, k=-1)) / h**2\n\n        # --- Iteration Initialization ---\n        u = np.zeros(N)\n        switched = False\n        q_ratio = np.nan  # Use NaN as a sentinel for the result\n\n        d_norm_hist = []\n        c_hist = []\n        gamma_hist = []\n        \n        total_max_iter = max_picard_iter + max_newton_iter\n\n        # Combined Picard/Newton iteration loop\n        for k in range(total_max_iter):\n            \n            # Since u is updated at the end of the loop, u here is u^(k)\n            # The residual F(u^(k)) is needed by both methods\n            F_u = K @ u + beta * u**3 - f_vec\n            norm_F = np.linalg.norm(F_u)\n\n            if norm_F < newton_tol:\n                break # Converged\n\n            # --- Picard Phase ---\n            if not switched and k < max_picard_iter:\n                u_sq = u**2\n                M_u = K + beta * np.diag(u_sq)\n                \n                try:\n                    u_next = np.linalg.solve(M_u, f_vec)\n                except np.linalg.LinAlgError:\n                    # In case of instability, break and result in NaN\n                    break\n                \n                d = u_next - u\n                norm_d = np.linalg.norm(d)\n                d_norm_hist.append(norm_d)\n                \n                gamma_k = norm_d / norm_F if norm_F > 0 else 0.0\n                gamma_hist.append(gamma_k)\n\n                if k >= 1:\n                    norm_d_prev = d_norm_hist[k-1]\n                    c_k = norm_d / norm_d_prev if norm_d_prev > 0 else 0.0\n                    c_hist.append(c_k)\n\n                # Check for switch condition, requires k>=2 for c_k and c_{k-1}\n                if k >= 2:\n                    c_k_val = c_hist[-1]\n                    c_k_minus_1_val = c_hist[-2]\n                    gamma_k_val = gamma_hist[-1]\n                    gamma_k_minus_1_val = gamma_hist[-2]\n\n                    if abs(c_k_val - c_k_minus_1_val) <= eps_c and \\\n                       abs(gamma_k_val - gamma_k_minus_1_val) <= eps_gamma:\n                        \n                        switched = True\n                        k_star = k\n                        \n                        # --- Compute q at the switching point k_star ---\n                        # Use gamma at k_star\n                        gamma_k_star = gamma_k_val\n\n                        # Use u at k_star (which is the current u)\n                        u_star = u\n                        u_star_sq = u_star**2\n                        J_u_star = K + 3 * beta * np.diag(u_star_sq)\n                        \n                        # J_u_star is symmetric, use eigvalsh for efficiency and stability\n                        eigenvalues = np.linalg.eigvalsh(J_u_star)\n                        lambda_min = np.min(eigenvalues)\n\n                        norm_J_inv = 1.0 / lambda_min\n                        \n                        q_ratio = gamma_k_star / norm_J_inv\n\n                # Update state for the next iteration\n                u = u_next \n\n            # --- Newton-Raphson Phase ---\n            else:\n                u_sq = u**2\n                J_u = K + 3.0 * beta * np.diag(u_sq)\n                \n                try:\n                    delta = np.linalg.solve(J_u, -F_u)\n                except np.linalg.LinAlgError:\n                    break\n                \n                u = u + delta\n        \n        results.append(q_ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many physical systems described by PDEs exhibit bifurcations, where solution branches turn or split as a parameter is varied. Near such critical points, like the 'turning point' in the Bratu equation, standard fixed-parameter solvers like Picard iteration inevitably fail because the solution is no longer a single-valued function of the parameter. This culminating practice  challenges you to implement a robust pseudo-arclength continuation method, a powerful extension of Newton's method that treats the parameter $\\lambda$ as a variable. By successfully tracing the solution curve through the turning point, you will directly witness the failure of simpler methods and appreciate the necessity of path-following algorithms for exploring complex solution landscapes.",
            "id": "3431387",
            "problem": "Consider the one-parameter family of nonlinear boundary value problems given by the Bratu equation, a prototypical model for fold (turning point) bifurcations in parameterized partial differential equations:\n$$\n\\text{Find } u: [0,1] \\to \\mathbb{R} \\text{ such that } u''(x) + \\lambda \\exp(u(x)) = 0,\\quad u(0) = 0,\\quad u(1) = 0,\n$$\nwhere $\\lambda \\in \\mathbb{R}$ is a continuation parameter. The numerical solution is sought via a finite difference discretization on a uniform grid with $N$ interior points. Let $h = \\frac{1}{N+1}$ and let $u \\in \\mathbb{R}^N$ be the vector of unknowns at the interior nodes $x_i = ih$, $i=1,\\dots,N$. The semidiscrete nonlinear system is\n$$\nF(u,\\lambda) = L u + \\lambda \\exp(u) = 0,\n$$\nwhere $L \\in \\mathbb{R}^{N \\times N}$ is the standard second-order centered finite difference approximation of the Laplacian with Dirichlet boundary conditions,\n$$\nL = \\frac{1}{h^2}\n\\begin{bmatrix}\n-2 & 1  &        &        & \\\\\n1  & -2 & 1      &        & \\\\\n   & \\ddots & \\ddots & \\ddots & \\\\\n   &        & 1      & -2 & 1 \\\\\n   &        &        & 1 & -2\n\\end{bmatrix},\n$$\nand $\\exp(u)$ denotes the vector with entries $\\exp(u_i)$.\n\nYou will implement and compare two iterative strategies:\n\n1. Picard iteration with adaptive relaxation (damped fixed-point):\n   Starting from an initial guess $u^{(0)}$, define the linear Poisson solve\n   $$\n   L \\tilde{u}^{(k+1)} = -\\lambda \\exp\\left(u^{(k)}\\right),\n   $$\n   and then update with relaxation\n   $$\n   u^{(k+1)} = (1-\\omega^{(k)}) u^{(k)} + \\omega^{(k)} \\tilde{u}^{(k+1)},\n   $$\n   where the relaxation parameter $\\omega^{(k)} \\in (0,1]$ is chosen adaptively at each iteration to ensure decrease of the residual norm $\\lVert F(u^{(k+1)},\\lambda)\\rVert_2$ (use a simple backtracking strategy on $\\omega^{(k)}$).\n\n2. Newton–Raphson with pseudo-arclength continuation:\n   The standard Newton–Raphson method for fixed $\\lambda$ solves\n   $$\n   J(u,\\lambda) \\,\\delta u = -F(u,\\lambda), \\quad \\text{with } J(u,\\lambda) = L + \\lambda \\operatorname{diag}(\\exp(u)),\n   $$\n   and updates $u \\leftarrow u + \\delta u$. To traverse the solution branch across turning points, use pseudo-arclength continuation: given two previously converged solutions $(u_0,\\lambda_0)$ and $(u_1,\\lambda_1)$, define the secant-based tangent\n   $$\n   s = \\frac{1}{\\sqrt{\\lVert u_1 - u_0 \\rVert_2^2 + (\\lambda_1 - \\lambda_0)^2}} \\left( u_1 - u_0, \\ \\lambda_1 - \\lambda_0 \\right),\n   $$\n   form a predictor $(u_{\\mathrm{pred}},\\lambda_{\\mathrm{pred}}) = (u_1,\\lambda_1) + d s$, where $d>0$ is the arclength step size, and correct via Newton on the bordered system\n   $$\n   \\begin{cases}\n   F(u,\\lambda) = 0, \\\\\n   \\langle u - u_{\\mathrm{pred}}, s_u \\rangle + (\\lambda - \\lambda_{\\mathrm{pred}}) s_\\lambda = 0,\n   \\end{cases}\n   $$\n   where $s = (s_u,s_\\lambda)$ and $\\langle \\cdot,\\cdot \\rangle$ is the Euclidean inner product. Implement a Schur-complement-based Newton correction requiring only solves with $J(u,\\lambda)$, and an Armijo-type backtracking on the full step $(\\delta u,\\delta\\lambda)$ to ensure residual decrease of the augmented system.\n\nYour task is to implement both methods on the described discretization and to run the following test suite. Use $N=80$, tolerance $\\varepsilon = 10^{-8}$ on the Euclidean norm of $F$, and a maximum of $1000$ iterations for Picard and $50$ iterations for each Newton correction. In all tests, use the zero vector as the initial guess when a previous solution is not specified.\n\nTest Suite:\n\n- Test 1 (Happy path): For $\\lambda = 1.0$, run Picard iteration with adaptive relaxation from $u^{(0)} = 0$. Return a boolean indicating whether convergence has been achieved within the iteration limit (residual norm $\\le \\varepsilon$).\n\n- Test 2 (Near-turning fixed-parameter attempt): For $\\lambda = 3.2$, run Picard iteration with adaptive relaxation from $u^{(0)} = 0$. Return a boolean indicating whether convergence has been achieved within the iteration limit (residual norm $\\le \\varepsilon$).\n\n- Test 3 (Pseudo-arclength crossing): Use standard Newton–Raphson to find solutions at $\\lambda_0 = 0.1$ and $\\lambda_1 = 0.2$. Initialize pseudo-arclength continuation with arclength step $d = 0.05$ and run $40$ continuation steps. Return a boolean indicating whether the continuation crossed a turning point, defined as a strict sign change in the discrete derivative of the parameter sequence (i.e., the sequence of $\\lambda$ values first strictly increases and later strictly decreases at least once).\n\n- Test 4 (Coverage comparison): Perform naive parameter continuation using Picard with adaptive relaxation: starting from $\\lambda = 0.1$ and step $\\Delta \\lambda = 0.1$, increment $\\lambda$ using the previously converged solution as the initial guess, stopping when Picard fails to converge within the iteration limit. In parallel, use pseudo-arclength continuation initialized as in Test 3 and record the maximum $\\lambda$ attained over its trajectory. Return the float\n$$\n\\lambda_{\\max}^{\\mathrm{PA}} - \\lambda_{\\mathrm{last}}^{\\mathrm{Picard}},\n$$\nwhere $\\lambda_{\\max}^{\\mathrm{PA}}$ is the maximum $\\lambda$ reached by pseudo-arclength continuation over all its steps, and $\\lambda_{\\mathrm{last}}^{\\mathrm{Picard}}$ is the last $\\lambda$ at which Picard converged. Express this answer as a decimal number.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). The four entries must respectively be the outputs of Tests 1 through 4 in the order and types specified above.",
            "solution": "The problem statement is valid. It presents a well-defined numerical analysis task grounded in the established theory of solving nonlinear partial differential equations and analyzing their bifurcations. All parameters and methods are specified with sufficient clarity to permit a unique and verifiable implementation.\n\nThe problem centers on the numerical solution of the one-dimensional Bratu equation, a canonical model for spontaneous combustion and a classic example of a fold (or turning-point) bifurcation. The equation is\n$$\nu''(x) + \\lambda \\exp(u(x)) = 0, \\quad x \\in [0,1],\n$$\nwith homogeneous Dirichlet boundary conditions $u(0) = u(1) = 0$. The behavior of the solutions $u(x)$ depends critically on the parameter $\\lambda$.\n\nFirst, the continuous problem is discretized. We use a second-order centered finite difference scheme on a uniform grid of $N$ interior points with spacing $h = 1/(N+1)$. The second derivative $u''(x_i)$ at a node $x_i$ is approximated as $(u_{i-1} - 2u_i + u_{i+1})/h^2$. Applying this at each interior node $i=1, \\dots, N$ and incorporating the boundary conditions, $u_0=u_{N+1}=0$, yields a system of $N$ nonlinear algebraic equations for the vector of unknowns $u = (u_1, \\dots, u_N)^T$. This system is written in vector form as:\n$$\nF(u, \\lambda) = L u + \\lambda \\exp(u) = 0,\n$$\nwhere $L \\in \\mathbb{R}^{N \\times N}$ is the sparse, tridiagonal matrix representing the discretized negative Laplacian, and $\\exp(u)$ is the element-wise exponential function.\n\nTwo iterative methods are to be implemented to solve this nonlinear system.\n\n1.  **Picard Iteration with Adaptive Relaxation**: This method is a form of fixed-point iteration. The system $Lu + \\lambda\\exp(u)=0$ is rearranged to the fixed-point form $u = -L^{-1}(\\lambda\\exp(u))$. The basic iteration is thus $u^{(k+1)} = -L^{-1}(\\lambda \\exp(u^{(k)}))$. In practice, we solve the linear system $L \\tilde{u}^{(k+1)} = -\\lambda \\exp(u^{(k)})$ for $\\tilde{u}^{(k+1)}$. For the Bratu problem, this iteration converges only for small values of $\\lambda$. To extend the convergence radius, a relaxation (or damping) step is introduced:\n    $$\n    u^{(k+1)} = (1-\\omega^{(k)}) u^{(k)} + \\omega^{(k)} \\tilde{u}^{(k+1)}.\n    $$\n    The relaxation parameter $\\omega^{(k)} \\in (0, 1]$ is chosen adaptively via a backtracking search at each iteration $k$ to ensure that the norm of the residual decreases, i.e., $\\lVert F(u^{(k+1)}, \\lambda) \\rVert_2 < \\lVert F(u^{(k)}, \\lambda) \\rVert_2$.\n\n2.  **Newton-Raphson with Pseudo-Arclength Continuation**:\n    For a fixed $\\lambda$, Newton's method finds the root of $F(u, \\lambda)=0$ by iteratively solving a linear system for a correction $\\delta u$. Given an iterate $u^{(k)}$, the update $u^{(k+1)} = u^{(k)} + \\delta u$ is found by solving\n    $$\n    J(u^{(k)}, \\lambda) \\delta u = -F(u^{(k)}, \\lambda),\n    $$\n    where $J(u, \\lambda) = \\frac{\\partial F}{\\partial u} = L + \\lambda \\operatorname{diag}(\\exp(u))$ is the Jacobian matrix. Newton's method fails near the turning point of the Bratu curve, where the Jacobian $J$ becomes singular.\n\n    Pseudo-arclength continuation is a powerful technique to trace the entire solution curve $(u(s), \\lambda(s))$ parameterized by arclength $s$, including passage through turning points. It consists of a predictor-corrector sequence:\n    -   **Predictor**: An initial guess for the next point on the curve is made by extrapolating from the last converged point $(u_1, \\lambda_1)$ along the (normalized) secant vector $s = (s_u, s_\\lambda)$ defined by the two most recent points, $(u_0, \\lambda_0)$ and $(u_1, \\lambda_1)$. The predictor is $(u_{\\mathrm{pred}}, \\lambda_{\\mathrm{pred}}) = (u_1, \\lambda_1) + d \\cdot s$, where $d$ is the arclength step size.\n    -   **Corrector**: The predictor point $(u_{\\mathrm{pred}}, \\lambda_{\\mathrm{pred}})$ is corrected back to the solution curve by solving an augmented system of $N+1$ equations for the $N+1$ unknowns $(u, \\lambda)$. This system couples the original equation with an arclength constraint that forces the solution onto a hyperplane orthogonal to the tangent vector $s$:\n        $$\n        \\begin{cases}\n        F(u, \\lambda) = 0, \\\\\n        g(u,\\lambda) := \\langle u - u_{\\mathrm{pred}}, s_u \\rangle + (\\lambda - \\lambda_{\\mathrm{pred}}) s_\\lambda = 0.\n        \\end{cases}\n        $$\n    This augmented system is solved with Newton's method. Its Jacobian is a bordered matrix that remains non-singular at the turning point. The linear solve within each Newton step is handled efficiently via a Schur complement method. For the system $\\begin{psmallmatrix} J & \\exp(u) \\\\ s_u^T & s_\\lambda \\end{psmallmatrix} \\begin{psmallmatrix} \\delta u \\\\ \\delta \\lambda \\end{psmallmatrix} = -\\begin{psmallmatrix} F \\\\ g \\end{psmallmatrix}$, we first solve two linear systems involving only the original Jacobian $J$ to find vectors $w_F=J^{-1}F$ and $w_\\lambda=J^{-1}\\exp(u)$. This allows for the scalar correction $\\delta\\lambda$ to be solved for explicitly:\n    $$\n    \\delta\\lambda = \\frac{\\langle s_u, w_F \\rangle - g}{s_\\lambda - \\langle s_u, w_\\lambda \\rangle}.\n    $$\n    The vector correction $\\delta u$ is then recovered as $\\delta u = -w_F - \\delta\\lambda \\cdot w_\\lambda$. This approach avoids forming and solving the full $(N+1) \\times (N+1)$ bordered system directly, leveraging the efficiency of solves with the sparse matrix $J$. Backtracking on the step length ensures robust convergence of the corrector.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Implements and compares Picard and Newton-Raphson-based solvers for the\n    discretized Bratu equation, executing a predefined test suite.\n    \"\"\"\n    # --- Problem Parameters ---\n    N = 80\n    TOL = 1e-8\n    PICARD_MAX_ITER = 1000\n    NEWTON_MAX_ITER = 50\n    H = 1.0 / (N + 1)\n\n    # --- Core Functions ---\n\n    def create_laplacian_sparse(n, h):\n        \"\"\"Creates the 1D finite difference Laplacian as a sparse matrix.\"\"\"\n        diagonals = [np.ones(n - 1), -2 * np.ones(n), np.ones(n - 1)]\n        offsets = [-1, 0, 1]\n        L = sparse.diags(diagonals, offsets, shape=(n, n), format='csc')\n        return L / h**2\n\n    def compute_residual(u, lam, L_sparse):\n        \"\"\"Computes the residual F(u, lam) = L*u + lam*exp(u).\"\"\"\n        return L_sparse @ u + lam * np.exp(u)\n\n    # --- Solver Implementations ---\n\n    def picard_solver(lam, u_init, L_sparse, tol, max_iter):\n        \"\"\"\n        Solves the Bratu equation using Picard iteration with adaptive relaxation.\n        \"\"\"\n        u = u_init.copy()\n        \n        for _ in range(max_iter):\n            res_norm = np.linalg.norm(compute_residual(u, lam, L_sparse))\n            \n            if res_norm < tol:\n                return u, True\n            \n            # Linear solve for Picard update\n            rhs = -lam * np.exp(u)\n            u_tilde = spsolve(L_sparse, rhs)\n\n            # Adaptive relaxation (backtracking on omega)\n            omega = 1.0\n            found_omega = False\n            for _ in range(10):  # Max 10 backtracking steps\n                u_next = (1 - omega) * u + omega * u_tilde\n                res_next_norm = np.linalg.norm(compute_residual(u_next, lam, L_sparse))\n                if res_next_norm < res_norm:\n                    u = u_next\n                    found_omega = True\n                    break\n                omega /= 2.0\n\n            if not found_omega:\n                return u, False\n\n        return u, np.linalg.norm(compute_residual(u, lam, L_sparse)) < tol\n\n    def newton_solver(lam, u_init, L_sparse, tol, max_iter):\n        \"\"\"\n        Solves the Bratu equation for a fixed lambda using Newton-Raphson.\n        \"\"\"\n        u = u_init.copy()\n\n        for _ in range(max_iter):\n            res = compute_residual(u, lam, L_sparse)\n            if np.linalg.norm(res) < tol:\n                return u, True\n            \n            exp_u = np.exp(u)\n            J = L_sparse + sparse.diags(lam * exp_u, 0, format='csc')\n            \n            try:\n                delta_u = spsolve(J, -res)\n            except np.linalg.LinAlgError:\n                return u, False\n            u += delta_u\n        \n        return u, np.linalg.norm(compute_residual(u, lam, L_sparse)) < tol\n\n    def pa_continuation(u0, lam0, u1, lam1, d, num_steps, L_sparse, tol, max_newton_iter):\n        \"\"\"\n        Performs pseudo-arclength continuation.\n        \"\"\"\n        branch = [(u0, lam0), (u1, lam1)]\n        u_curr, lam_curr = u1, lam1\n        u_prev, lam_prev = u0, lam0\n\n        for _ in range(num_steps):\n            # --- Predictor ---\n            u_diff = u_curr - u_prev\n            lam_diff = lam_curr - lam_prev\n            \n            tangent_norm = np.sqrt(np.dot(u_diff, u_diff) + lam_diff**2)\n            if tangent_norm < 1e-12: break # Stalled\n            s_u = u_diff / tangent_norm\n            s_lam = lam_diff / tangent_norm\n\n            u_pred = u_curr + d * s_u\n            lam_pred = lam_curr + d * s_lam\n            \n            u_corr, lam_corr = u_pred, lam_pred\n\n            # --- Corrector (Newton on bordered system) ---\n            converged = False\n            for _ in range(max_newton_iter):\n                res_F = compute_residual(u_corr, lam_corr, L_sparse)\n                res_g = np.dot(s_u, u_corr - u_pred) + s_lam * (lam_corr - lam_pred)\n                aug_res_norm = np.sqrt(np.dot(res_F, res_F) + res_g**2)\n                if aug_res_norm < tol:\n                    converged = True\n                    break\n\n                # Schur complement solve\n                exp_u = np.exp(u_corr)\n                J_corr = L_sparse + sparse.diags(lam_corr * exp_u, 0, format='csc')\n                \n                try:\n                    w_F = spsolve(J_corr, res_F)\n                    w_lam = spsolve(J_corr, exp_u)\n                except (np.linalg.LinAlgError, RuntimeError): break\n\n                delta_lam_den = s_lam - np.dot(s_u, w_lam)\n                if abs(delta_lam_den) < 1e-12: break\n\n                delta_lam_num = np.dot(s_u, w_F) - res_g\n                delta_lam = delta_lam_num / delta_lam_den\n                delta_u = -w_F - delta_lam * w_lam\n\n                # Backtracking line search\n                alpha = 1.0\n                found_alpha = False\n                for _ in range(10):\n                    u_next = u_corr + alpha * delta_u\n                    lam_next = lam_corr + alpha * delta_lam\n                    \n                    res_F_next = compute_residual(u_next, lam_next, L_sparse)\n                    res_g_next = np.dot(s_u, u_next - u_pred) + s_lam * (lam_next - lam_pred)\n                    aug_res_norm_next = np.sqrt(np.dot(res_F_next, res_F_next) + res_g_next**2)\n\n                    if aug_res_norm_next < aug_res_norm:\n                        u_corr, lam_corr = u_next, lam_next\n                        found_alpha = True\n                        break\n                    alpha /= 2.0\n                \n                if not found_alpha: break\n            \n            if not converged: break\n            \n            branch.append((u_corr, lam_corr))\n            u_prev, lam_prev = u_curr, lam_curr\n            u_curr, lam_curr = u_corr, lam_corr\n\n        return branch\n\n    # --- Main Execution ---\n    L = create_laplacian_sparse(N, H)\n    u_zero = np.zeros(N)\n    results = []\n\n    # Test 1: Picard at lam=1.0\n    _, conv1 = picard_solver(1.0, u_zero, L, TOL, PICARD_MAX_ITER)\n    results.append(conv1)\n\n    # Test 2: Picard at lam=3.2\n    _, conv2 = picard_solver(3.2, u_zero, L, TOL, PICARD_MAX_ITER)\n    results.append(conv2)\n\n    # Test 3: PA turning point detection\n    u_sol_01, conv_01 = newton_solver(0.1, u_zero, L, TOL, NEWTON_MAX_ITER)\n    u_sol_02, conv_02 = newton_solver(0.2, u_sol_01, L, TOL, NEWTON_MAX_ITER)\n    \n    pa_branch_test3 = []\n    if conv_01 and conv_02:\n        pa_branch_test3 = pa_continuation(u_sol_01, 0.1, u_sol_02, 0.2, 0.05, 40, L, TOL, NEWTON_MAX_ITER)\n    \n    lams_pa = [p[1] for p in pa_branch_test3]\n    deltas_lam = np.diff(lams_pa)\n    crossed_turning_point = False\n    if len(deltas_lam) > 1 and deltas_lam[0] > 0:\n        if np.any(deltas_lam[1:] < 0):\n            crossed_turning_point = True\n    results.append(crossed_turning_point)\n    \n    # Test 4: Coverage comparison\n    lam_picard = 0.1\n    delta_lam_picard = 0.1\n    u_guess_picard = np.zeros(N)\n    lam_last_picard = 0.0\n    while lam_picard < 4.0: # Safety break\n        _, conv = picard_solver(lam_picard, u_guess_picard, L, TOL, PICARD_MAX_ITER)\n        if conv:\n            u_sol, _ = newton_solver(lam_picard, u_guess_picard, L, TOL, NEWTON_MAX_ITER)\n            lam_last_picard = lam_picard\n            u_guess_picard = u_sol\n            lam_picard += delta_lam_picard\n        else:\n            break\n\n    lam_max_pa = 0.0\n    if lams_pa:\n        lam_max_pa = max(lams_pa)\n\n    result4 = lam_max_pa - lam_last_picard\n    results.append(result4)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}