{
    "hands_on_practices": [
        {
            "introduction": "广义多项式混沌（gPC）展开是随机有限元方法的核心，它将随机输出量表示为与输入随机变量相关的正交多项式级数。该展开的计算成本主要取决于所用多项式基函数（即截断集）的数量。本练习将指导您推导标准“总阶”截断集的大小，并向您介绍一种更高效的各向异性截断策略，它通过对不同随机维度赋予不同权重来优化基的构成 。",
            "id": "3448334",
            "problem": "考虑一个带有随机系数的线性椭圆偏微分方程，该随机系数通过由独立标准高斯随机变量驱动的截断Karhunen–Loève展开（KLE）进行建模。具体来说，假设该系数字段表示为 $a(x,\\omega) = \\bar{a}(x) + \\sum_{i=1}^{M} \\sqrt{\\lambda_{i}}\\, \\phi_{i}(x)\\, Y_{i}(\\omega)$，其中 $x$ 位于 $\\mathbb{R}^{d}$ 的一个有界域中，$(\\lambda_{i}, \\phi_{i})$ 是协方差算子的确定性特征对，而 $Y_{i}$ 是独立的标准正态随机变量。使用由多重指标 $\\alpha = (\\alpha_{1},\\dots,\\alpha_{M}) \\in \\mathbb{N}_{0}^{M}$ 索引的张量化概率论者Hermite多项式，在 $M$ 维空间中构建一个Hermite广义多项式混沌(gPC)展开。\n\n定义各向同性总次数截断集为\n$$\n\\mathcal{A}_{\\mathrm{iso}}(M,p) = \\left\\{\\alpha \\in \\mathbb{N}_{0}^{M} : \\sum_{i=1}^{M} \\alpha_{i} \\leq p \\right\\},\n$$\n其中 $p \\in \\mathbb{N}_{0}$ 是最大总多项式次数。从 $\\mathcal{A}_{\\mathrm{iso}}(M,p)$ 的定义和基本计数原理出发，推导 $\\mathcal{A}_{\\mathrm{iso}}(M,p)$ 的基数作为 $M$ 和 $p$ 的函数的闭式表达式。\n\n接下来，考虑一种各向异性加权总次数截断，它通过权重 $\\gamma = (\\gamma_{1},\\dots,\\gamma_{M})$（其中 $\\gamma_{i}  0$）来编码维度重要性。定义\n$$\n\\mathcal{A}_{\\mathrm{ani}}(\\gamma,p_{\\mathrm{w}}) = \\left\\{\\alpha \\in \\mathbb{N}_{0}^{M} : \\sum_{i=1}^{M} \\gamma_{i}\\,\\alpha_{i} \\leq p_{\\mathrm{w}} \\right\\},\n$$\n其中 $p_{\\mathrm{w}} \\in \\mathbb{R}_{+}$。解释在Hermite gPC背景下，权重 $\\gamma_{i}$ 对截断和收敛的影响，并针对 $M=3$, $\\gamma = (1,2,1)$, $p_{\\mathrm{w}} = 4$ 的具体情况，计算精确基数 $\\left|\\mathcal{A}_{\\mathrm{ani}}(\\gamma,p_{\\mathrm{w}})\\right|$。请将各向同性情况的闭式表达式和计算出的各向异性情况的整数值作为最终结果。无需四舍五入，也不涉及物理单位。",
            "solution": "该问题分为两部分。第一部分要求推导各向同性总次数多项式指标集的基数的闭式表达式。第二部分要求解释各向异性截断，并计算一个特定各向异性指标集的基数。\n\n首先，我们推导各向同性总次数截断集的基数，该集合定义为\n$$\n\\mathcal{A}_{\\mathrm{iso}}(M,p) = \\left\\{\\alpha \\in \\mathbb{N}_{0}^{M} : \\sum_{i=1}^{M} \\alpha_{i} \\leq p \\right\\}.\n$$\n此处，$\\alpha = (\\alpha_1, \\dots, \\alpha_M)$ 是一个多重指标，其中每个分量 $\\alpha_i$ 都是一个非负整数，$M$ 是维度数，$p$ 是最大总多项式次数。\n\n为了求出该集合中的元素数量，记为 $|\\mathcal{A}_{\\mathrm{iso}}(M,p)|$，我们必须计算不等式 $\\alpha_1 + \\alpha_2 + \\dots + \\alpha_M \\leq p$ 的非负整数解的数量。通过引入一个松弛变量 $\\alpha_{M+1} \\in \\mathbb{N}_{0}$，这个问题可以转化为一个等式问题。该不等式等价于方程\n$$\n\\alpha_1 + \\alpha_2 + \\dots + \\alpha_M + \\alpha_{M+1} = p.\n$$\n我们现在需要求这个方程关于 $M+1$ 个变量 $(\\alpha_1, \\dots, \\alpha_{M+1})$ 的非负整数解的数量。这是一个经典的组合问题，可以用“隔板法”解决。我们正在将 $p$ 个相同的物品（星）分配到 $M+1$ 个不同的箱子（变量 $\\alpha_i$）中。这需要 $M$ 个隔板来分隔这 $M+1$ 个箱子。$p$ 个星和 $M$ 个隔板的总排列数，是从总共 $p+M$ 个位置中选择 $M$ 个位置放置隔板的方法数。这由二项式系数给出：\n$$\n|\\mathcal{A}_{\\mathrm{iso}}(M,p)| = \\binom{p+M}{M} = \\frac{(p+M)!}{p!M!}.\n$$\n\n接下来，我们讨论各向异性加权总次数截断。该集合定义为\n$$\n\\mathcal{A}_{\\mathrm{ani}}(\\gamma,p_{\\mathrm{w}}) = \\left\\{\\alpha \\in \\mathbb{N}_{0}^{M} : \\sum_{i=1}^{M} \\gamma_{i}\\,\\alpha_{i} \\leq p_{\\mathrm{w}} \\right\\},\n$$\n权重 $\\gamma_i  0$。引入权重 $\\gamma_i$ 是为了创建一种更有效的截断策略。在Karhunen–Loève展开的背景下，随机变量 $Y_i$ 对随机场 $a(x, \\omega)$ 方差的贡献并不相等；$Y_i$ 的贡献由 $\\sqrt{\\lambda_i}$ 进行缩放。协方差算子的特征值 $\\lambda_i$ 通常会衰减，这意味着某些随机维度的重要性高于其他维度。各向异性截断利用了这一点，通过为不同维度的多项式次数分配不同的“成本”（即权重 $\\gamma_i$）。通常，将较大的权重 $\\gamma_i$ 分配给较不重要的维度（对应于较小特征值 $\\lambda_i$ 的维度）。对于固定的预算 $p_{\\mathrm{w}}$，这会惩罚较不重要方向上的高阶多项式项 $\\alpha_i$，迫使其变小或为零，同时允许在更重要的方向上（权重较小的方向）使用高阶项。对于给定数量的基函数（即指标集的基数），这种量身定制的计算资源分配通常会带来显著更精确的近似，从而改善广义多项式混沌展开的收敛性，尤其是在高维问题中。\n\n现在，我们计算 $M=3$、权重 $\\gamma = (1,2,1)$、最大加权次数 $p_{\\mathrm{w}} = 4$ 时的具体基数。我们需要找到不等式\n$$\n1 \\cdot \\alpha_1 + 2 \\cdot \\alpha_2 + 1 \\cdot \\alpha_3 \\leq 4\n$$\n的非负整数解 $(\\alpha_1, \\alpha_2, \\alpha_3)$ 的数量。\n由于没有计算该数量的通用闭式公式，我们通过遍历 $\\alpha_2$ 的可能值来进行直接枚举，因为它具有最大的权重。各项的非负性意味着 $2\\alpha_2 \\leq 4$，所以 $\\alpha_2$ 的取值范围是 $\\{0, 1, 2\\}$。\n\n情况1：$\\alpha_2 = 0$。\n不等式简化为 $\\alpha_1 + \\alpha_3 \\leq 4$。这是一个二维、最大次数为4的各向同性问题。解的数量由之前推导的公式给出：$\\binom{4+2}{2} = \\binom{6}{2} = \\frac{6 \\times 5}{2} = 15$。\n\n情况2：$\\alpha_2 = 1$。\n不等式变为 $\\alpha_1 + 2(1) + \\alpha_3 \\leq 4$，简化为 $\\alpha_1 + \\alpha_3 \\leq 2$。解的数量为 $\\binom{2+2}{2} = \\binom{4}{2} = \\frac{4 \\times 3}{2} = 6$。\n\n情况3：$\\alpha_2 = 2$。\n不等式变为 $\\alpha_1 + 2(2) + \\alpha_3 \\leq 4$，简化为 $\\alpha_1 + \\alpha_3 \\leq 0$。由于 $\\alpha_1, \\alpha_3$ 必须为非负数，唯一的解是 $(\\alpha_1, \\alpha_3) = (0,0)$。这给出了 1 个解。该公式也得出 $\\binom{0+2}{2} = \\binom{2}{2} = 1$。\n\n总基数 $|\\mathcal{A}_{\\mathrm{ani}}((1,2,1),4)|$ 是所有情况计数之和：\n$$\n|\\mathcal{A}_{\\mathrm{ani}}| = 15 + 6 + 1 = 22.\n$$\n\n最终答案包括两部分：各向同性情况的闭式表达式和特定各向异性情况的数值。",
            "answer": "$$\n\\boxed{\\binom{M+p}{M}, 22}\n$$"
        },
        {
            "introduction": "在不确定性量化中，一个核心任务是计算目标量（QoI）的统计矩（如均值和方差），这通常需要求解高维参数空间上的积分。由于维数灾难，标准的张量积求积法则不切实际。本练习将通过Smolyak稀疏网格方法，为您展示一种强大的高维积分数值技术。您将从一维Clenshaw-Curtis法则出发，逐步构建一个多维稀疏网格，并用它来精确计算一个简单函数的统计矩 。",
            "id": "3448347",
            "problem": "考虑一个随机有限元法 (SFEM) 的设定，其中有 $M=3$ 个独立的标准化不确定性输入 $\\boldsymbol{\\xi}=(\\xi_1,\\xi_2,\\xi_3)$，每个输入都在 $[-1,1]$ 上均匀分布。您将构建一个基于嵌套 Clenshaw–Curtis 横坐标的各向同性 2 级 Smolyak 稀疏网格求积，以近似一个依赖于参数化偏微分方程解 $u(\\boldsymbol{\\xi})$ 的标量关注量 $Q(u)$ 的统计矩。\n\n任务：\n\n$1.$ 定义在 $[-1,1]$ 上 $i=1$ 和 $i=2$ 级的一维 Clenshaw-Curtis 求积法则，给出它们的节点和权重。仅使用求积对多项式精度的基本性质来确定所需的任何权重。\n\n$2.$ 使用 Smolyak 构造，在 $\\ell=2$ 级和 $M=3$ 维下，采用各向同性指标集 $\\mathcal{I}_{\\ell,M}=\\{\\mathbf{i}\\in\\mathbb{N}^M:\\ |\\mathbf{i}|_1\\le \\ell+M-1,\\ i_k\\ge 1\\}$，通过张量化一维差分算子 $\\Delta_i=U_i-U_{i-1}$（其中 $U_0$ 定义为零法则）来形成三维稀疏网格法则。列出 $\\mathbb{R}^3$ 中所有唯一的稀疏网格节点及其用于在 $[-1,1]^3$ 上积分函数的聚合求积权重。\n\n$3.$ 根据第一性原理，概述如何使用任务 2 中的稀疏网格节点和权重，在 $[-1,1]^3$ 上的乘积均匀密度下计算 $Q(u)$ 的均值 $\\mathbb{E}[Q(u)]$ 和方差 $\\operatorname{Var}[Q(u)]$。\n\n$4.$ 令 $Q(u)$ 为测试泛函 $Q(\\boldsymbol{\\xi})=\\xi_1+\\xi_2+\\xi_3$。仅使用任务 2 中的节点和权重以及 $[-1,1]^3$ 上的均匀密度，计算方差 $\\operatorname{Var}[Q(u)]$ 的稀疏网格估计值。请将您的最终答案以一个实数形式给出，无需四舍五入。",
            "solution": "### 任务 1：一维 Clenshaw-Curtis 求积法则\n\n我们被要求定义在区间 $[-1,1]$ 上 $i=1$ 和 $i=2$ 级的一维 Clenshaw-Curtis 求积法则，记为 $U_i$。嵌套 Clenshaw-Curtis 法则的节点是 Chebyshev 多项式的极值点。在 $i=1$ 级时节点数为 $n_i=1$，在 $i1$ 级时节点数为 $n_i=2^{i-1}+1$。权重通过强制对某一阶数的多项式精确来确定。\n\n**$i=1$ 级：**\n节点数为 $n_1=1$。节点是 $T_{n_1-1}(x) = T_0(x)=1$ 的极值点，即 $x_1^{(1)}=0$。\n求积法则是 $U_1(f) = w_1^{(1)}f(x_1^{(1)})$。\n我们要求该法则对常数多项式（例如 $f(x)=1$）是精确的。\n$$\n\\int_{-1}^{1} 1 \\, dx = 2\n$$\n求积给出 $U_1(1) = w_1^{(1)}f(0) = w_1^{(1)} \\cdot 1$。\n令两者相等，得到权重 $w_1^{(1)}=2$。\n因此，对于 $i=1$：\n- 节点集：$X_1 = \\{0\\}$\n- 权重集：$W_1 = \\{2\\}$\n\n**$i=2$ 级：**\n节点数为 $n_2 = 2^{2-1}+1 = 3$。节点是 Chebyshev 多项式 $T_{n_2-1}(x) = T_2(x) = 2x^2-1$ 的极值点，即对于 $j=0,1,2$，$x_j^{(2)} = \\cos\\left(\\frac{j\\pi}{n_2-1}\\right)$。\n- $x_1^{(2)} = \\cos(0) = 1$\n- $x_2^{(2)} = \\cos(\\pi/2) = 0$\n- $x_3^{(2)} = \\cos(\\pi) = -1$\n节点集为 $X_2 = \\{-1, 0, 1\\}$。注意 $X_1 \\subset X_2$，所以该法则是嵌套的。\n求积法则是 $U_2(f) = w_1^{(2)}f(-1) + w_2^{(2)}f(0) + w_3^{(2)}f(1)$。\n该法则是对称的，所以 $w_1^{(2)} = w_3^{(2)}$。我们强制对多项式 $f(x)=1$ 和 $f(x)=x^2$ 精确。\n对于 $f(x)=1$：\n$$\n\\int_{-1}^{1} 1 \\, dx = 2 \\quad \\implies \\quad w_1^{(2)} + w_2^{(2)} + w_3^{(2)} = 2w_1^{(2)} + w_2^{(2)} = 2\n$$\n对于 $f(x)=x^2$：\n$$\n\\int_{-1}^{1} x^2 \\, dx = \\frac{2}{3} \\quad \\implies \\quad w_1^{(2)}(-1)^2 + w_2^{(2)}(0)^2 + w_3^{(2)}(1)^2 = w_1^{(2)} + w_3^{(2)} = 2w_1^{(2)} = \\frac{2}{3}\n$$\n从第二个方程得到 $w_1^{(2)} = 1/3$。因为 $w_3^{(2)} = w_1^{(2)}$，所以我们有 $w_3^{(2)} = 1/3$。\n代入第一个方程：$2(1/3) + w_2^{(2)} = 2$，得到 $w_2^{(2)} = 2 - 2/3 = 4/3$。\n因此，对于 $i=2$：\n- 节点集：$X_2 = \\{-1, 0, 1\\}$\n- 权重集：$W_2 = \\{1/3, 4/3, 1/3\\}$，分别对应节点 $\\{-1, 0, 1\\}$。\n\n### 任务 2：Smolyak 稀疏网格构造\n\n稀疏网格求积法则构造为 $\\mathcal{A}_{\\ell,M}(f) = \\sum_{\\mathbf{i} \\in \\mathcal{I}_{\\ell,M}} (\\Delta_{i_1} \\otimes \\Delta_{i_2} \\otimes \\Delta_{i_3})(f)$。\n给定等级 $\\ell=2$ 和维度 $M=3$。各向同性指标集为 $\\mathcal{I}_{2,3} = \\{\\mathbf{i} \\in \\mathbb{N}^3 : |\\mathbf{i}|_1 := i_1+i_2+i_3 \\le \\ell+M-1 = 4, \\text{ 且 } i_k \\ge 1\\}$。\n此集合中的指标为：\n- $|\\mathbf{i}|_1 = 3$: $(1,1,1)$\n- $|\\mathbf{i}|_1 = 4$: $(2,1,1)$, $(1,2,1)$, $(1,1,2)$\n\n我们需要差分算子 $\\Delta_i = U_i - U_{i-1}$（其中 $U_0$ 是零算子/法则）。\n- $\\Delta_1 = U_1 - U_0 = U_1$。其节点为 $X_1=\\{0\\}$，权重为 $W_1=\\{2\\}$。\n- $\\Delta_2 = U_2 - U_1$。$\\Delta_2$ 的权重是为 $X_2$ 中的每个节点计算的。\n    - 对于 $x=0 \\in X_1$：$w_0^{\\Delta_2} = w_0^{U_2} - w_0^{U_1} = 4/3 - 2 = -2/3$。\n    - 对于 $x \\in X_2 \\setminus X_1 = \\{-1, 1\\}$：$w_{\\pm 1}^{\\Delta_2} = w_{\\pm 1}^{U_2} - 0 = 1/3$。\n    - 因此，$\\Delta_2$ 作用于节点 $\\{-1, 0, 1\\}$，权重为 $\\{1/3, -2/3, 1/3\\}$。\n\n稀疏网格节点的总集合是与每个指标 $\\mathbf{i} \\in \\mathcal{I}_{2,3}$ 对应的张量积网格的并集。一个点的权重是其所属的每个张量积法则中权重的总和。\n\n- 对于 $\\mathbf{i}=(1,1,1)$：法则是 $\\Delta_1 \\otimes \\Delta_1 \\otimes \\Delta_1$。\n    - 节点：$(0,0,0)$。权重：$2 \\cdot 2 \\cdot 2 = 8$。\n- 对于 $\\mathbf{i}=(2,1,1)$：法则是 $\\Delta_2 \\otimes \\Delta_1 \\otimes \\Delta_1$。\n    - 节点：$\\{-1,0,1\\} \\otimes \\{0\\} \\otimes \\{0\\} = \\{(-1,0,0), (0,0,0), (1,0,0)\\}$。\n    - 权重：$(1/3 \\cdot 2 \\cdot 2)$, $(-2/3 \\cdot 2 \\cdot 2)$, $(1/3 \\cdot 2 \\cdot 2)$，即 $\\{4/3, -8/3, 4/3\\}$。\n- 对于 $\\mathbf{i}=(1,2,1)$：法则是 $\\Delta_1 \\otimes \\Delta_2 \\otimes \\Delta_1$。\n    - 节点：$\\{0\\} \\otimes \\{-1,0,1\\} \\otimes \\{0\\} = \\{(0,-1,0), (0,0,0), (0,1,0)\\}$。\n    - 权重：$\\{4/3, -8/3, 4/3\\}$。\n- 对于 $\\mathbf{i}=(1,1,2)$：法则是 $\\Delta_1 \\otimes \\Delta_1 \\otimes \\Delta_2$。\n    - 节点：$\\{0\\} \\otimes \\{0\\} \\otimes \\{-1,0,1\\} = \\{(0,0,-1), (0,0,0), (0,0,1)\\}$。\n    - 权重：$\\{4/3, -8/3, 4/3\\}$。\n\n唯一的稀疏网格节点集是 $\\{(0,0,0), (\\pm 1,0,0), (0,\\pm 1,0), (0,0,\\pm 1)\\}$。共有 $1+6=7$ 个唯一节点。\n现在我们聚合权重。设 $w(\\mathbf{x})$ 是节点 $\\mathbf{x}$ 的最终权重。\n- 对于 $\\mathbf{x}=(0,0,0)$：此节点出现在所有四个法则中。\n$w(0,0,0) = 8 + (-8/3) + (-8/3) + (-8/3) = 8 - 3(8/3) = 0$。\n- 对于 $\\mathbf{x}=(1,0,0)$：此节点仅出现在 $\\mathbf{i}=(2,1,1)$ 的法则中。\n$w(1,0,0) = 4/3$。根据对称性，$w(-1,0,0)=4/3$。\n- 对于 $\\mathbf{x}=(0,1,0)$：此节点仅出现在 $\\mathbf{i}=(1,2,1)$ 的法则中。\n$w(0,1,0) = 4/3$。根据对称性，$w(0,-1,0)=4/3$。\n- 对于 $\\mathbf{x}=(0,0,1)$：此节点仅出现在 $\\mathbf{i}=(1,1,2)$ 的法则中。\n$w(0,0,1) = 4/3$。根据对称性，$w(0,0,-1)=4/3$。\n\n稀疏网格节点和权重的总结：\n- 唯一节点：$\\{(0,0,0), (1,0,0), (-1,0,0), (0,1,0), (0,-1,0), (0,0,1), (0,0,-1)\\}$。\n- 聚合权重：\n    - $w(0,0,0) = 0$。\n    - $w(\\pm 1,0,0) = w(0,\\pm 1,0) = w(0,0,\\pm 1) = 4/3$。\n\n### 任务 3：均值和方差的计算\n\n给定一个标量关注量 $Q(u(\\boldsymbol{\\xi}))$，并且每个 $\\xi_k$ 在 $[-1,1]$ 上均匀分布，则对于 $\\boldsymbol{\\xi} \\in [-1,1]^3$，乘积概率密度函数为 $p(\\boldsymbol{\\xi}) = (1/2)^M = (1/2)^3 = 1/8$。\n\n均值 $\\mathbb{E}[Q(u)]$ 由以下积分定义：\n$$\n\\mathbb{E}[Q(u)] = \\int_{[-1,1]^3} Q(u(\\boldsymbol{\\xi})) p(\\boldsymbol{\\xi}) \\,d\\boldsymbol{\\xi} = \\frac{1}{8} \\int_{[-1,1]^3} Q(u(\\boldsymbol{\\xi})) \\,d\\boldsymbol{\\xi}\n$$\n使用具有节点 $\\{\\boldsymbol{x}_j\\}_{j=1}^{N_p}$ 和权重 $\\{w_j\\}_{j=1}^{N_p}$ 的稀疏网格求积，我们近似该积分为：\n$$\n\\mathbb{E}[Q(u)] \\approx \\mu_Q = \\frac{1}{8} \\sum_{j=1}^{N_p} w_j Q(u(\\boldsymbol{x}_j))\n$$\n\n方差 $\\operatorname{Var}[Q(u)]$ 定义为 $\\mathbb{E}[Q(u)^2] - (\\mathbb{E}[Q(u)])^2$。\n我们首先如上所述近似均值 $\\mu_Q$。然后，我们近似平方的期望值 $\\mathbb{E}[Q(u)^2]$：\n$$\n\\mathbb{E}[Q(u)^2] = \\frac{1}{8} \\int_{[-1,1]^3} [Q(u(\\boldsymbol{\\xi}))]^2 \\,d\\boldsymbol{\\xi} \\approx E_2 = \\frac{1}{8} \\sum_{j=1}^{N_p} w_j [Q(u(\\boldsymbol{x}_j))]^2\n$$\n方差的稀疏网格估计值则为：\n$$\n\\operatorname{Var}[Q(u)] \\approx \\sigma_Q^2 = E_2 - \\mu_Q^2\n$$\n\n### 任务 4：$Q(\\boldsymbol{\\xi}) = \\xi_1 + \\xi_2 + \\xi_3$ 的方差计算\n\n我们使用任务 2 的节点和权重以及任务 3 的步骤。设 7 个节点的集合为 $\\{\\boldsymbol{x}_j\\}_{j=0}^6$。\n节点是 $\\boldsymbol{x}_0=(0,0,0)$ 和六个轴向点 $(\\pm 1,0,0)$ 等。我们将它们称为 $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_6$。权重为 $w_0=0$ 和 $w_j=4/3$ (对于 $j=1,\\dots,6$)。\n\n首先，我们计算均值估计值 $\\mu_Q$：\n$$\n\\mu_Q = \\frac{1}{8} \\sum_{j=0}^{6} w_j Q(\\boldsymbol{x}_j)\n$$\n我们在每个节点上计算 $Q(\\boldsymbol{\\xi})=\\xi_1+\\xi_2+\\xi_3$：\n- $Q(\\boldsymbol{x}_0) = Q(0,0,0) = 0$。\n- 对于六个轴向点，$Q$ 的值是 $1$ 或 $-1$。具体来说：$Q(1,0,0)=1$, $Q(-1,0,0)=-1$, $Q(0,1,0)=1$, $Q(0,-1,0)=-1$, $Q(0,0,1)=1$, $Q(0,0,-1)=-1$。\n总和变为：\n$$\n\\sum_{j=0}^{6} w_j Q(\\boldsymbol{x}_j) = 0 \\cdot Q(\\boldsymbol{x}_0) + \\frac{4}{3} [1 + (-1) + 1 + (-1) + 1 + (-1)] = \\frac{4}{3} \\cdot 0 = 0\n$$\n因此，均值估计值为 $\\mu_Q = \\frac{1}{8} \\cdot 0 = 0$。\n\n接下来，我们计算平方的均值估计值 $E_2$：\n$$\nE_2 = \\frac{1}{8} \\sum_{j=0}^{6} w_j [Q(\\boldsymbol{x}_j)]^2\n$$\n$[Q(\\boldsymbol{x}_j)]^2$ 的值为：\n- $[Q(\\boldsymbol{x}_0)]^2 = 0^2=0$。\n- 对于所有六个轴向点，$[Q(\\boldsymbol{x}_j)]^2 = (\\pm 1)^2 = 1$。\n总和为：\n$$\n\\sum_{j=0}^{6} w_j [Q(\\boldsymbol{x}_j)]^2 = 0 \\cdot 0^2 + \\frac{4}{3}[1^2 + (-1)^2 + 1^2 + (-1)^2 + 1^2 + (-1)^2] = \\frac{4}{3} \\cdot 6 = 8\n$$\n因此，平方的均值估计值为 $E_2 = \\frac{1}{8} \\cdot 8 = 1$。\n\n最后，方差估计值为：\n$$\n\\sigma_Q^2 = E_2 - \\mu_Q^2 = 1 - 0^2 = 1\n$$\n精确方差为 $\\operatorname{Var}[\\xi_1+\\xi_2+\\xi_3] = \\operatorname{Var}[\\xi_1]+\\operatorname{Var}[\\xi_2]+\\operatorname{Var}[\\xi_3] = 3 \\cdot \\operatorname{Var}[U(-1,1)] = 3 \\cdot \\frac{(1-(-1))^2}{12} = 3 \\cdot \\frac{4}{12} = 1$。对于这个特定的多项式，稀疏网格法则给出了精确的结果。",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "在许多实际工程问题中，我们常常可以构建一个计算成本低但精度有限的代理模型（如低阶gPC展开），同时还有一个计算成本高昂但精确的“高保真”模型。本练习将介绍一种巧妙的统计方法——控制变量法，它能将廉价代理模型的信息与少量高保真模型的样本相结合，从而以极低的计算代价获得高精度的均值估计。通过推导最优估计量和方差缩减因子，您将从根本上理解如何量化并实现计算效率的显著提升 。",
            "id": "3448294",
            "problem": "考虑一个带有随机输入的线性椭圆偏微分方程 (PDE)，该方程通过使用侵入式广义多项式混沌 (gPC) 近似的随机有限元方法 (SFEM) 进行离散化。设关注量为解的一个实值泛函 $Q$，因此高保真模型输出为 $Y = Q(u)$，并令 $X = Q(u_{p})$ 表示同一量的低阶侵入式多项式混沌展开 (PCE) 代理模型，其中 $u_{p}$ 是总阶为 $p$ 的 gPC 解。假设侵入式 gPC 系数已通过 Galerkin 投影计算得出，因此 $X$ 的均值，记为 $\\mu_{X} = \\mathbb{E}[X]$，可以精确地作为零次基函数的系数获得。您需要通过控制变量构造，将低阶侵入式 PCE 与 $Y$ 的高保真 Monte Carlo (MC) 采样相结合。\n\n从期望、方差、协方差和相关性的基本定义出发，并仅使用多项式基的正交性和期望的线性性质，按以下步骤进行：\n\n1. 构造一个用于 $\\mu_{Y} = \\mathbb{E}[Y]$ 的无偏线性估计量。该估计量通过形成一个控制变量估计量，将来自相同输入分布的 $n$ 个独立同分布样本对 $\\{(Y_{i}, X_{i})\\}_{i=1}^{n}$ 与已知的均值 $\\mu_{X} = \\mathbb{E}[X]$ 相结合，线性地混合了 $Y$ 的高保真样本和来自 $X$ 的低阶 PCE 信息。\n\n2. 以闭合形式推导在无偏约束下最小化估计量方差的最优混合系数。用协方差 $\\operatorname{Cov}(Y,X)$ 和方差 $\\sigma_{Y}^{2} = \\operatorname{Var}(Y)$ 及 $\\sigma_{X}^{2} = \\operatorname{Var}(X)$ 来表示该最优系数。\n\n3. 将最优估计量的最小化方差表示为仅使用 $Y$ 样本的普通 Monte Carlo 估计量方差的一个乘法因子。然后将此方差缩减因子表示为仅与相关系数 $\\rho = \\operatorname{Corr}(Y,X)$ 有关的函数。\n\n4. 在侵入式 PCE 用作高保真量 $Y = Q(u)$ 的控制变量的背景下解释结果，并评论 $\\sigma_{Y}$、$\\sigma_{X}$ 和 $\\rho$ 的作用。\n\n您的程序必须实现所得的闭合形式表达式，以便为给定的一组参数值测试套件生成数值结果。程序不应执行任何采样；它必须计算上面推导出的闭合形式表达式。\n\n输入数据和输出均为无量纲；不涉及物理单位或角度单位。输出必须是四舍五入到 $6$ 位小数的浮点数。\n\n测试套件：\n- 案例 A (理想情况)：$\\rho = 0.0$，$\\sigma_{Y} = 1.0$，$\\sigma_{X} = 1.0$。\n- 案例 B (中等正相关)：$\\rho = 0.5$，$\\sigma_{Y} = 2.0$，$\\sigma_{X} = 1.0$。\n- 案例 C (中等负相关)：$\\rho = -0.7$，$\\sigma_{Y} = 1.5$，$\\sigma_{X} = 3.0$。\n- 案例 D (接近完美的正相关边界)：$\\rho = 0.99$，$\\sigma_{Y} = 1.0$，$\\sigma_{X} = 0.8$。\n- 案例 E (接近完美的负相关边界)：$\\rho = -0.99$，$\\sigma_{Y} = 0.5$，$\\sigma_{X} = 4.0$。\n\n对于每种情况，您的程序必须计算：\n- 出现在将 $Y$ 与 $X$ 和 $\\mu_{X}$ 结合的无偏线性估计量中的最优控制变量混合系数 $\\beta^{\\star}$。\n- 方差缩减因子 $\\phi$，定义为在相同样本量 $n$ 的情况下，控制变量估计量的最小化方差与仅基于 $Y$ 样本的普通 Monte Carlo 估计量方差的比率。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。该列表必须按 A, B, C, D, E 的顺序包含各案例的数值对，并展平为单个列表，格式为 $[\\beta^{\\star}_{A}, \\phi_{A}, \\beta^{\\star}_{B}, \\phi_{B}, \\beta^{\\star}_{C}, \\phi_{C}, \\beta^{\\star}_{D}, \\phi_{D}, \\beta^{\\star}_{E}, \\phi_{E}]$，其中每个浮点数都四舍五入到 $6$ 位小数。",
            "solution": "推导过程按要求从基本定义开始，分为四个部分。\n\n### 第 1 部分：构造无偏控制变量估计量\n\n令 $\\mu_Y = \\mathbb{E}[Y]$ 为高保真关注量 $Y$ 的未知均值。我们有 $n$ 个独立同分布 (i.i.d.) 的样本 $\\{Y_i\\}_{i=1}^n$。$\\mu_Y$ 的标准 Monte Carlo 估计量是样本均值：\n$$\n\\hat{\\mu}_{Y,MC} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\n$$\n该估计量是无偏的，因为 $\\mathbb{E}[\\hat{\\mu}_{Y,MC}] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[Y_i] = \\frac{1}{n} (n \\mu_Y) = \\mu_Y$。\n\n我们还有一个相关的随机变量 $X$，即低阶代理模型，其均值 $\\mu_X = \\mathbb{E}[X]$ 是精确已知的。这使我们能够构造一个新的随机变量 $Z = X - \\mu_X$，其均值为零是已知的：$\\mathbb{E}[Z] = \\mathbb{E}[X - \\mu_X] = \\mathbb{E}[X] - \\mu_X = \\mu_X - \\mu_X = 0$。\n\n$\\mu_Y$ 的一个控制变量估计量是通过定义一个新的随机变量 $Y(\\beta)$ 来形成的，该变量将 $Y$ 与 $Z$ 的一个缩放版本相结合：\n$$\nY(\\beta) = Y - \\beta (X - \\mu_X)\n$$\n其中 $\\beta$ 是一个待定的混合系数。新的估计量 $\\hat{\\mu}_{Y,CV}$ 是 $Y_i(\\beta)$ 在 $n$ 个配对样本 $\\{(Y_i, X_i)\\}_{i=1}^n$ 上的样本均值：\n$$\n\\hat{\\mu}_{Y,CV}(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} Y_i(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} [Y_i - \\beta(X_i - \\mu_X)]\n$$\n为了验证对于任何 $\\beta$ 的选择，该估计量都是无偏的，我们取其期望。根据期望的线性性质：\n$$\n\\mathbb{E}[\\hat{\\mu}_{Y,CV}(\\beta)] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} [Y_i - \\beta(X_i - \\mu_X)]\\right] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[Y_i - \\beta(X_i - \\mu_X)]\n$$\n由于每个样本对 $(Y_i, X_i)$ 与 $(Y, X)$ 具有相同的分布，\n$$\n\\mathbb{E}[\\hat{\\mu}_{Y,CV}(\\beta)] = \\mathbb{E}[Y - \\beta(X - \\mu_X)] = \\mathbb{E}[Y] - \\beta \\mathbb{E}[X - \\mu_X] = \\mu_Y - \\beta(0) = \\mu_Y\n$$\n因此，对于任何常数 $\\beta$，$\\hat{\\mu}_{Y,CV}(\\beta)$ 都是 $\\mu_Y$ 的一个无偏线性估计量。\n\n### 第 2 部分：最优混合系数的推导\n\n最优系数 $\\beta^{\\star}$ 是使控制变量估计量方差 $\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta))$ 最小化的系数。该估计量的方差由下式给出：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta)) = \\operatorname{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} Y_i(\\beta)\\right)\n$$\n由于样本是独立同分布的，和的方差是单项方差的 $n$ 倍，而因子 $\\frac{1}{n}$ 变为 $\\frac{1}{n^2}$：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta)) = \\frac{1}{n^2} \\cdot n \\cdot \\operatorname{Var}(Y(\\beta)) = \\frac{1}{n} \\operatorname{Var}(Y - \\beta(X - \\mu_X))\n$$\n由于加上一个常数（$\\beta\\mu_X$）不会改变方差，上式简化为：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta)) = \\frac{1}{n} \\operatorname{Var}(Y - \\beta X)\n$$\n使用随机变量线性组合的方差的一般性质 $\\operatorname{Var}(A - cB) = \\operatorname{Var}(A) + c^2\\operatorname{Var}(B) - 2c\\operatorname{Cov}(A,B)$，我们得到：\n$$\n\\operatorname{Var}(Y - \\beta X) = \\operatorname{Var}(Y) + \\beta^2 \\operatorname{Var}(X) - 2\\beta \\operatorname{Cov}(Y, X)\n$$\n我们用 $\\sigma_Y^2 = \\operatorname{Var}(Y)$、$\\sigma_X^2 = \\operatorname{Var}(X)$ 和 $\\operatorname{Cov}(Y,X)$ 来表示。估计量的方差作为 $\\beta$ 的函数是：\n$$\nV(\\beta) = \\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta)) = \\frac{1}{n} (\\sigma_Y^2 - 2\\beta \\operatorname{Cov}(Y, X) + \\beta^2 \\sigma_X^2)\n$$\n为了找到最小值，我们将 $V(\\beta)$ 对 $\\beta$ 求导，并令其导数为零：\n$$\n\\frac{d V}{d \\beta} = \\frac{1}{n} (-2 \\operatorname{Cov}(Y, X) + 2\\beta \\sigma_X^2) = 0\n$$\n假设 $\\sigma_X^2 \\neq 0$，我们解出 $\\beta$：\n$$\n2\\beta \\sigma_X^2 = 2 \\operatorname{Cov}(Y, X) \\implies \\beta = \\frac{\\operatorname{Cov}(Y, X)}{\\sigma_X^2}\n$$\n这就是最优混合系数 $\\beta^{\\star}$。二阶导数 $\\frac{d^2 V}{d \\beta^2} = \\frac{2\\sigma_X^2}{n}$ 为正，证实了这个 $\\beta$ 值使方差最小化。\n因此，最优混合系数为：\n$$\n\\beta^{\\star} = \\frac{\\operatorname{Cov}(Y, X)}{\\operatorname{Var}(X)}\n$$\n\n### 第 3 部分：最小化方差与缩减因子\n\n为了求得最小化方差，我们将 $\\beta^{\\star}$ 代回 $V(\\beta)$ 的表达式中：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta^{\\star})) = \\frac{1}{n} \\left(\\sigma_Y^2 - 2 \\left(\\frac{\\operatorname{Cov}(Y, X)}{\\sigma_X^2}\\right) \\operatorname{Cov}(Y, X) + \\left(\\frac{\\operatorname{Cov}(Y, X)}{\\sigma_X^2}\\right)^2 \\sigma_X^2\\right)\n$$\n$$\n= \\frac{1}{n} \\left(\\sigma_Y^2 - \\frac{2(\\operatorname{Cov}(Y, X))^2}{\\sigma_X^2} + \\frac{(\\operatorname{Cov}(Y, X))^2}{\\sigma_X^2}\\right)\n$$\n$$\n= \\frac{1}{n} \\left(\\sigma_Y^2 - \\frac{(\\operatorname{Cov}(Y, X))^2}{\\sigma_X^2}\\right)\n$$\n现在，我们引入相关系数 $\\rho = \\operatorname{Corr}(Y,X) = \\frac{\\operatorname{Cov}(Y, X)}{\\sigma_Y \\sigma_X}$。这可以得到 $\\operatorname{Cov}(Y, X) = \\rho \\sigma_Y \\sigma_X$。将此代入最小化方差的表达式中：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta^{\\star})) = \\frac{1}{n} \\left(\\sigma_Y^2 - \\frac{(\\rho \\sigma_Y \\sigma_X)^2}{\\sigma_X^2}\\right) = \\frac{1}{n} \\left(\\sigma_Y^2 - \\frac{\\rho^2 \\sigma_Y^2 \\sigma_X^2}{\\sigma_X^2}\\right)\n$$\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta^{\\star})) = \\frac{1}{n} (\\sigma_Y^2 - \\rho^2 \\sigma_Y^2) = \\frac{\\sigma_Y^2}{n} (1 - \\rho^2)\n$$\n普通 Monte Carlo 估计量的方差为 $\\operatorname{Var}(\\hat{\\mu}_{Y,MC}) = \\operatorname{Var}(\\frac{1}{n}\\sum Y_i) = \\frac{1}{n}\\operatorname{Var}(Y) = \\frac{\\sigma_Y^2}{n}$。\n\n方差缩减因子 $\\phi$ 是最小化控制变量方差与普通 Monte Carlo 方差的比率：\n$$\n\\phi = \\frac{\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta^{\\star}))}{\\operatorname{Var}(\\hat{\\mu}_{Y,MC})} = \\frac{\\frac{\\sigma_Y^2}{n} (1 - \\rho^2)}{\\frac{\\sigma_Y^2}{n}}\n$$\n$$\n\\phi = 1 - \\rho^2\n$$\n\n### 第 4 部分：解释\n\n该分析揭示了两个用于实现的关键结果。首先，最优系数 $\\beta^{\\star}$ 的公式可以用相关系数 $\\rho$ 表示：\n$$\n\\beta^{\\star} = \\frac{\\rho \\sigma_Y \\sigma_X}{\\sigma_X^2} = \\rho \\frac{\\sigma_Y}{\\sigma_X}\n$$\n其次，方差缩减因子为 $\\phi = 1 - \\rho^2$。\n\n这些结果为在此背景下的控制变量方法提供了关键的见解：\n1.  **相关系数 $\\rho$ 的作用**：因子 $\\phi = 1 - \\rho^2$ 表明，控制变量技术的有效性完全由高保真量 $Y$ 与其低阶代理模型 $X$ 之间相关系数的平方决定。更强的线性关系（即 $|\\rho|$ 更接近 1）会导致更大的方差缩减。如果 $X$ 和 $Y$ 完全相关（$|\\rho| = 1$），方差变为零（$\\phi=0$），这意味着可以找到精确的均值。如果它们不相关（$\\rho=0$），则没有方差缩减（$\\phi=1$），控制变量是无用的。\n\n2.  **方差 $\\sigma_Y^2$ 和 $\\sigma_X^2$ 的作用**：这些方差不影响方差缩减的百分比，后者仅取决于 $\\rho$。然而，它们决定了控制变量校正项的最优缩放。系数 $\\beta^{\\star} = \\rho \\frac{\\sigma_Y}{\\sigma_X}$ 是 $Y$ 对 $X$ 的最佳线性拟合（回归）的斜率。它缩放代理模型与其均值的偏差 $(X - \\mu_X)$，以最好地抵消 $Y$ 中的相应偏差。\n\n在侵入式 PCE 的背景下，高质量（高阶）的 PCE 代理模型预计与完整模型具有非常高的相关性，使其成为一个绝佳的控制变量。结果 $\\phi = 1 - \\rho^2$ 精确地量化了构造这样一个代理模型的好处：计算侵入式 PCE 模型（以获得 $\\mu_X$ 和 $X_i$ 的样本）的成本，被用来换取为达到 $\\mu_Y$ 的目标精度所需的高成本高保真样本 $Y_i$ 数量的显著减少。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal control variate coefficient and variance reduction factor\n    for a series of test cases based on derived closed-form expressions.\n    \"\"\"\n\n    # Test suite:\n    # Each tuple represents a case: (rho, sigma_Y, sigma_X)\n    test_cases = [\n        # Case A: rho = 0.0, sigma_Y = 1.0, sigma_X = 1.0\n        (0.0, 1.0, 1.0),\n        # Case B: rho = 0.5, sigma_Y = 2.0, sigma_X = 1.0\n        (0.5, 2.0, 1.0),\n        # Case C: rho = -0.7, sigma_Y = 1.5, sigma_X = 3.0\n        (-0.7, 1.5, 3.0),\n        # Case D: rho = 0.99, sigma_Y = 1.0, sigma_X = 0.8\n        (0.99, 1.0, 0.8),\n        # Case E: rho = -0.99, sigma_Y = 0.5, sigma_X = 4.0\n        (-0.99, 0.5, 4.0),\n    ]\n\n    results = []\n    for rho, sigma_y, sigma_x in test_cases:\n        # The derived closed-form expressions are:\n        # 1. Optimal mixing coefficient: beta_star = rho * (sigma_Y / sigma_X)\n        # 2. Variance reduction factor: phi = 1 - rho^2\n\n        # Check for the case where sigma_x is zero to avoid division by zero,\n        # although this is not present in the test cases.\n        if sigma_x == 0:\n            # If sigma_x is 0, X is a constant. If X is a constant, Cov(Y, X) = 0\n            # and rho = 0. beta_star is ill-defined, but since the covariance\n            # is zero, the optimal beta is 0.\n            beta_star = 0.0\n        else:\n            beta_star = rho * (sigma_y / sigma_x)\n\n        # Calculate the variance reduction factor.\n        phi = 1 - rho**2\n\n        # Append the calculated values, rounded to 6 decimal places, to the results list.\n        results.append(round(beta_star, 6))\n        results.append(round(phi, 6))\n\n    # Format the final output string as a comma-separated list within square brackets.\n    # The map(str, ...) converts each float in the results list to its string representation.\n    # ','.join(...) concatenates these strings with a comma in between.\n    # The f-string places this joined string inside the required brackets.\n    output_string = f\"[{','.join(map(str, results))}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_string)\n\nsolve()\n```"
        }
    ]
}