{
    "hands_on_practices": [
        {
            "introduction": "在构建降阶模型之前，我们必须首先确定一个高效的低维子空间来近似高维解。恰当正交分解（Proper Orthogonal Decomposition, POD）是为此目的服务的基石技术之一。它通过对一组“快照”（即高保真解的样本）进行奇异值分解，能够提取出在平均意义下最优的基向量。本练习  将引导你通过推导，揭示POD截断误差与被忽略的奇异值之间的直接关系，从而从数学上理解为什么POD是捕获解空间主要能量的最优线性方法。",
            "id": "3438787",
            "problem": "考虑一个定义在有界域 $\\Omega \\subset \\mathbb{R}^{d}$ 上的参数化非线性抛物型偏微分方程，其带有齐次狄利克雷边界条件，并使用基函数为 $\\{\\varphi_{i}\\}_{i=1}^{n}$ 的协调有限元方法在空间上进行离散。设半离散模型写作\n$$\nM \\frac{d x(t,\\mu)}{d t} + A(\\mu)\\, x(t,\\mu) + g\\!\\left(x(t,\\mu);\\mu\\right) = b(\\mu),\n$$\n其中 $M \\in \\mathbb{R}^{n \\times n}$ 是与 $L^{2}(\\Omega)$ 内积相关的对称正定质量矩阵，$A(\\mu) \\in \\mathbb{R}^{n \\times n}$ 是依赖于参数 $\\mu \\in \\mathcal{P}$ 的类刚度算子，$x(t,\\mu) \\in \\mathbb{R}^{n}$ 是时刻 $t$ 的有限元解的系数向量，$g(\\cdot;\\mu)$ 是一个非线性项，旨在通过离散经验插值方法 (DEIM) 以在线高效的方式进行近似，而 $b(\\mu) \\in \\mathbb{R}^{n}$ 是一个源项。假设一个候选降阶基将通过在 $\\mu \\in \\mathcal{P}$ 上的贪心程序来选择，但对于本问题，我们只关注由解快照构建的本征正交分解 (POD)。\n\n设 $\\{x_{k}\\}_{k=1}^{m}$ 是在参数和时间的训练集上采样的 $m$ 个解快照的集合，它们被组装成快照矩阵 $X \\in \\mathbb{R}^{n \\times m}$，其列为 $x_{k}$。定义质量加权快照矩阵 $Y := M^{1/2} X \\in \\mathbb{R}^{n \\times m}$，其中 $M^{1/2}$ 表示 $M$ 的唯一对称正定平方根。设 $Y$ 的奇异值分解为 $Y = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{m \\times m}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{n \\times m}$ 是对角矩阵，其意义在于其非零元素是主对角线上的奇异值 $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{q} > 0$，其中 $q = \\min\\{n,m\\}$。秩-$r$ POD 空间由 $Y$ 的前 $r$ 个左奇异向量通过 $M^{-1/2}$ 映射回来得到，即 $\\Psi_{r} := M^{-1/2} U_{r} \\in \\mathbb{R}^{n \\times r}$，其中 $U_{r} \\in \\mathbb{R}^{n \\times r}$ 集合了 $U$ 的前 $r$ 列。由于 $\\Psi_{r}^{\\top} M \\Psi_{r} = I_{r}$，到 $\\operatorname{span}(\\Psi_{r})$ 上的 $M$-正交投影算子为 $P_{r} := \\Psi_{r} \\Psi_{r}^{\\top} M$，快照的秩-$r$ POD 重构为 $X_{r} := P_{r} X$。\n\n将 $L^{2}(\\Omega)$ 意义下的总平方快照重构误差定义为\n$$\nE_{r}^{2} := \\sum_{k=1}^{m} \\|x_{k} - X_{r}(:,k)\\|_{L^{2}(\\Omega)}^{2} = \\sum_{k=1}^{m} \\left( x_{k} - X_{r}(:,k) \\right)^{\\top} M \\left( x_{k} - X_{r}(:,k) \\right),\n$$\n它可以紧凑地写为\n$$\nE_{r}^{2} = \\|X - X_{r}\\|_{M,F}^{2} := \\operatorname{trace}\\!\\left((X - X_{r})^{\\top} M (X - X_{r})\\right).\n$$\n\n仅从质量加权内积、奇异值分解和正交投影的定义出发，推导 $E_{r}^{2}$ 关于 $Y$ 的被忽略奇异值 $\\{\\sigma_{i}\\}_{i=r+1}^{q}$ 的闭式表达式，并由此将 POD 截断误差与快照重构误差的 $L^{2}(\\Omega)$ 范数联系起来。您的最终答案必须是一个用 $\\{\\sigma_{i}\\}_{i=r+1}^{q}$ 表示且精确等于 $E_{r}^{2}$ 的单一解析表达式。",
            "solution": "该问题是有效的，因为它在模型降阶的本征正交分解（POD）既定框架内，提出了一个定义明确的数学推导。所有术语都有明确定义，且其前提在科学和数学上都是合理的。\n\n目标是推导总平方快照重构误差 $E_r^2$ 的闭式表达式。我们从该误差在质量矩阵加权弗罗贝尼乌斯范数下的给定定义开始：\n$$\nE_{r}^{2} = \\|X - X_{r}\\|_{M,F}^{2} = \\operatorname{trace}\\!\\left((X - X_{r})^{\\top} M (X - X_{r})\\right)\n$$\n其中 $X \\in \\mathbb{R}^{n \\times m}$ 是快照矩阵，$X_r \\in \\mathbb{R}^{n \\times m}$ 是其秩-$r$ POD 重构。\n\n质量矩阵 $M$ 是对称正定的，这保证了其存在唯一的对称正定平方根 $M^{1/2}$。我们可以通过代入 $M = M^{1/2} M^{1/2}$ 来重写迹的表达式。由于 $M$ 是对称的，$(M^{1/2})^{\\top} = M^{1/2}$。\n$$\nE_{r}^{2} = \\operatorname{trace}\\!\\left((X - X_{r})^{\\top} M^{1/2} M^{1/2} (X - X_{r})\\right) = \\operatorname{trace}\\!\\left(\\left(M^{1/2}(X - X_{r})\\right)^{\\top} \\left(M^{1/2}(X - X_{r})\\right)\\right)\n$$\n该表达式是矩阵 $M^{1/2}(X - X_{r})$ 的标准弗罗贝尼乌斯范数的平方。\n$$\nE_{r}^{2} = \\|M^{1/2}(X - X_{r})\\|_{F}^{2}\n$$\n现在，我们使用给定的定义来表示范数内的项。重构 $X_r$ 定义为 $X$ 在 POD 空间上的投影：\n$$\nX_{r} = P_{r} X\n$$\n其中投影算子 $P_r$ 由 $P_{r} = \\Psi_{r} \\Psi_{r}^{\\top} M$ 给出。POD 基 $\\Psi_r$ 定义为 $\\Psi_{r} = M^{-1/2} U_{r}$，其中 $U_r$ 包含质量加权快照矩阵 $Y = M^{1/2} X$ 的前 $r$ 个左奇异向量。\n\n让我们将 $\\Psi_r$ 的定义代入 $P_r$ 的表达式中：\n$$\nP_{r} = (M^{-1/2} U_{r}) (\\Psi_{r}^{\\top}) M = (M^{-1/2} U_{r}) (M^{-1/2} U_{r})^{\\top} M = (M^{-1/2} U_{r}) (U_{r}^{\\top} (M^{-1/2})^{\\top}) M\n$$\n由于 $M^{-1/2}$ 是对称的，$(M^{-1/2})^{\\top} = M^{-1/2}$。\n$$\nP_{r} = M^{-1/2} U_{r} U_{r}^{\\top} M^{-1/2} M = M^{-1/2} U_{r} U_{r}^{\\top} M^{1/2}\n$$\n现在我们可以分析 $M^{1/2}(X - X_r)$ 这一项：\n$$\nM^{1/2}(X - X_{r}) = M^{1/2}(X - P_{r}X) = M^{1/2}X - M^{1/2}P_{r}X\n$$\n代入 $P_r$ 的表达式：\n$$\nM^{1/2} P_{r} X = M^{1/2} (M^{-1/2} U_{r} U_{r}^{\\top} M^{1/2}) X = (M^{1/2} M^{-1/2}) U_{r} U_{r}^{\\top} (M^{1/2} X) = I U_{r} U_{r}^{\\top} (M^{1/2} X)\n$$\n使用定义 $Y = M^{1/2}X$，我们有：\n$$\nM^{1/2} P_{r} X = U_{r} U_{r}^{\\top} Y\n$$\n因此，误差项变为：\n$$\nM^{1/2}(X - X_{r}) = M^{1/2}X - U_{r} U_{r}^{\\top} Y = Y - U_{r} U_{r}^{\\top} Y\n$$\n因此，问题从计算 $X$ 中误差的加权范数，转换为计算 $Y$ 在投影到由 $U_r$ 的列所张成的空间上时产生的误差的标准弗罗贝尼乌斯范数。\n$$\nE_{r}^{2} = \\|Y - U_{r} U_{r}^{\\top} Y\\|_{F}^{2}\n$$\n矩阵 $U_r U_r^T$ 是到由 $U$ 的前 $r$ 列所张成的子空间上的正交投影算子。我们现在使用 $Y$ 的奇异值分解，即 $Y = U \\Sigma V^{\\top}$。其求和形式为：\n$$\nY = \\sum_{i=1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\n$$\n其中 $u_i$ 和 $v_i$ 分别是 $U$ 和 $V$ 的第 $i$ 列，$\\sigma_i$ 是奇异值，且 $q = \\min\\{n,m\\}$。列向量 $\\{u_i\\}$ 构成一个标准正交基。$Y$ 在 $\\{u_i\\}_{i=1}^r$ 的张成空间上的投影为：\n$$\nU_{r} U_{r}^{\\top} Y = \\left(\\sum_{j=1}^{r} u_j u_j^{\\top}\\right) \\left(\\sum_{i=1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\\right) = \\sum_{j=1}^{r} \\sum_{i=1}^{q} \\sigma_{i} u_{j} (u_{j}^{\\top} u_{i}) v_{i}^{\\top}\n$$\n由于向量 $\\{u_i\\}$ 的标准正交性，我们有 $u_{j}^{\\top} u_{i} = \\delta_{ji}$，其中 $\\delta_{ji}$ 是克罗内克符号。该表达式简化为：\n$$\nU_{r} U_{r}^{\\top} Y = \\sum_{i=1}^{r} \\sigma_{i} u_{i} v_{i}^{\\top}\n$$\n这是 $Y$ 的截断 SVD，即 $Y$ 在弗罗贝尼乌斯范数下的最佳秩-$r$ 近似（Eckart-Young-Mirsky 定理）。投影误差为：\n$$\nY - U_{r} U_{r}^{\\top} Y = \\sum_{i=1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top} - \\sum_{i=1}^{r} \\sigma_{i} u_{i} v_{i}^{\\top} = \\sum_{i=r+1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\n$$\n最后，我们计算这个误差矩阵的弗罗贝尼乌斯范数的平方：\n$$\nE_{r}^{2} = \\left\\|\\sum_{i=r+1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\\right\\|_{F}^{2} = \\operatorname{trace}\\!\\left(\\left(\\sum_{j=r+1}^{q} \\sigma_{j} v_{j} u_{j}^{\\top}\\right)^{\\top} \\left(\\sum_{i=r+1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\\right)\\right)\n$$\n$$\nE_{r}^{2} = \\operatorname{trace}\\!\\left(\\left(\\sum_{j=r+1}^{q} \\sigma_{j} v_{j} u_{j}^{\\top}\\right) \\left(\\sum_{i=r+1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\\right)\\right) = \\operatorname{trace}\\!\\left(\\sum_{j=r+1}^{q} \\sum_{i=r+1}^{q} \\sigma_{j} \\sigma_{i} v_{j} (u_{j}^{\\top} u_{i}) v_{i}^{\\top}\\right)\n$$\n再次使用 $u_{j}^{\\top} u_{i} = \\delta_{ji}$：\n$$\nE_{r}^{2} = \\operatorname{trace}\\!\\left(\\sum_{i=r+1}^{q} \\sigma_{i}^{2} v_{i} v_{i}^{\\top}\\right)\n$$\n根据迹算子的线性性质：\n$$\nE_{r}^{2} = \\sum_{i=r+1}^{q} \\sigma_{i}^{2} \\operatorname{trace}(v_{i} v_{i}^{\\top})\n$$\n利用迹的循环性质，$\\operatorname{trace}(v_{i} v_{i}^{\\top}) = \\operatorname{trace}(v_{i}^{\\top} v_{i})$。由于 $v_i$ 是正交矩阵 $V$ 的一个列向量，它是一个单位向量，因此 $v_{i}^{\\top} v_{i} = 1$。\n$$\nE_{r}^{2} = \\sum_{i=r+1}^{q} \\sigma_{i}^{2} \\cdot 1 = \\sum_{i=r+1}^{q} \\sigma_{i}^{2}\n$$\n因此，$L^2(\\Omega)$ 意义下的总平方快照重构误差等于质量加权快照矩阵 $Y$ 的被忽略奇异值的平方和。",
            "answer": "$$\n\\boxed{\\sum_{i=r+1}^{q} \\sigma_{i}^{2}}\n$$"
        },
        {
            "introduction": "有了生成候选基向量的POD方法后，下一个关键问题是：如何从庞大的参数空间中“聪明地”选择快照，以构建一个对所有参数都有效的紧凑基？贪婪算法（Greedy algorithms）是解决此问题的标准答案，但其实现方式会直接影响离线训练阶段的计算成本。本实践  将要求你亲手实现并对比两种贪婪策略：“强”贪婪算法使用精确但计算昂贵的后验误差估计器，而“弱”贪婪算法则依赖于一种由离散经验插值法（DEIM）赋能的廉价代理指标。通过这个过程，你将深刻体会到模型降阶中精度与效率之间核心的权衡。",
            "id": "3438765",
            "problem": "构建一个完全指定的计算实验，用于对比弱贪婪选择（使用代理误差指示器 $\\eta(\\mu)$）和强贪婪选择（使用真实后验估计器）在对参数化线性椭圆偏微分方程进行降阶基（RB）近似时的表现，其中非仿射右端项通过离散经验插值方法（DEIM）进行超约化。您的程序必须实现这两种贪婪策略，并量化快照选择序列随 DEIM 基大小变化的差异。\n\n考虑在单位区间上的边值问题，其边界条件为齐次狄利克雷边界条件：\n- 求解 $u(x;\\mu)$ 使得 $-a(\\mu)\\,u''(x;\\mu) + u(x;\\mu) = g(x;\\mu)$ 对于 $x \\in (0,1)$ 成立，同时满足 $u(0;\\mu) = u(1;\\mu) = 0$。\n- 其中 $a(\\mu) = 1 + 0.5\\,\\mu$，且 $g(x;\\mu) = \\exp(\\mu x) + \\sin(3\\pi x)$。\n\n使用标准的二阶中心有限差分在 $N$ 个等距内节点上进行离散化。令 $h = 1/(N+1)$ 且 $x_i = i\\,h$（$i = 1,2,\\dots,N$）。离散系统为\n$$\nA(\\mu)\\,u(\\mu) = f(\\mu),\n$$\n其中 $A(\\mu) = a(\\mu)\\,K + I$，$K$ 是三对角刚度矩阵 $K = \\frac{1}{h^2}\\,\\mathrm{tridiag}(-1, 2,-1)$，$I$ 是 $N \\times N$ 单位矩阵。载荷向量 $f(\\mu)$ 由 $f_i(\\mu) = \\exp(\\mu x_i) + \\sin(3\\pi x_i)$ 给出。\n\n定义降阶基空间 $V_k = \\mathrm{span}\\{u(\\mu^{(1)}),\\dots,u(\\mu^{(k)})\\}$，其中 $V_k \\in \\mathbb{R}^{N \\times k}$ 的列在欧几里得内积下是正交归一化的。对于给定的 $\\mu$ 和基 $V_k$，通过伽辽金投影获得降阶解 $u_k(\\mu) = V_k y(\\mu)$：\n$$\n(V_k^\\top A(\\mu) V_k)\\,y(\\mu) = V_k^\\top \\widehat{f}(\\mu),\n$$\n其中 $\\widehat{f}(\\mu)$ 是用于降阶求解的右端项（在下文针对每种贪婪变体具体说明）。\n\n为非仿射项 $f(\\mu)$ 引入离散经验插值方法（DEIM）超约化，具体如下。通过计算在指定 DEIM 训练集上 $f(\\mu)$ 的快照矩阵的前 $m$ 个左奇异向量，构建 DEIM 基 $U_m \\in \\mathbb{R}^{N \\times m}$。通过标准的 DEIM 贪婪过程选择 DEIM 插值索引（选择使残差分量绝对值最大化的索引）。用 $P \\in \\mathbb{R}^{N \\times m}$ 表示与所选索引关联的列选择矩阵。$f(\\mu)$ 的 DEIM 近似为\n$$\nf_{\\mathrm{DEIM}}(\\mu) = U_m\\,(P^\\top U_m)^{-1} P^\\top f(\\mu).\n$$\n\n实现并比较以下贪婪策略，以构建两个独立的、大小为 $k_{\\max}$ 的降阶基。每种策略都通过从固定的训练集 $\\mathcal{S}_{\\mathrm{train}}$ 中迭代选择使误差指示器最大化的 $\\mu$ 来构建基。在第 $k$ 次迭代时，给定当前基 $V_{k-1}$：\n\n- 强贪婪（真实后验估计器）：\n  - 使用真实的右端项计算降阶解，即 $\\widehat{f}(\\mu) = f(\\mu)$。\n  - 构造真实残差 $r(\\mu) = A(\\mu)\\,u_{k-1}(\\mu) - f(\\mu)$，其中 $u_{k-1}(\\mu)$ 是在 $V_{k-1}$ 中的当前降阶解（若 $k=1$ 则 $u_0(\\mu) = 0$）。\n  - 使用精确的能量范数后验估计器\n    $$\n    \\Delta_{\\mathrm{strong}}(\\mu) = \\|r(\\mu)\\|_{A(\\mu)^{-1}} = \\sqrt{r(\\mu)^\\top A(\\mu)^{-1} r(\\mu)},\n    $$\n    对于对称正定矩阵 $A(\\mu)$，该值等于能量范数下的误差 $\\|u(\\mu) - u_{k-1}(\\mu)\\|_{A(\\mu)}$。\n  - 选择 $\\mu^{(k)} = \\arg\\max_{\\mu \\in \\mathcal{S}_{\\mathrm{train}} \\setminus \\{\\mu^{(1)},\\dots,\\mu^{(k-1)}\\}} \\Delta_{\\mathrm{strong}}(\\mu)$，计算全阶解 $u(\\mu^{(k)})$，并将其正交归一化后加入 $V_k$。\n\n- 弱贪婪（DEIM 超约化下的代理指示器）：\n  - 使用 DEIM 右端项计算降阶解，即 $\\widehat{f}(\\mu) = f_{\\mathrm{DEIM}}(\\mu)$。\n  - 构造 DEIM 残差 $r_{\\mathrm{DEIM}}(\\mu) = A(\\mu)\\,u_{k-1}(\\mu) - f_{\\mathrm{DEIM}}(\\mu)$。\n  - 使用代理指示器\n    $$\n    \\eta_{\\mathrm{weak}}(\\mu) = \\frac{\\|r_{\\mathrm{DEIM}}(\\mu)\\|_2}{\\alpha_{\\mathrm{LB}}(\\mu)},\n    $$\n    其中 $\\alpha_{\\mathrm{LB}}(\\mu)$ 是 $A(\\mu)$ 在欧几里得范数下的一个严格矫顽性下界。对于给定的离散化， $K$ 的最小特征值精确为\n    $$\n    \\lambda_1(K) = \\frac{4}{h^2}\\,\\sin^2\\!\\left(\\frac{\\pi}{2(N+1)}\\right),\n    $$\n    因此 $\\alpha_{\\mathrm{LB}}(\\mu) = 1 + a(\\mu)\\,\\lambda_1(K)$。\n  - 类似地，通过在未使用的训练参数上最大化 $\\eta_{\\mathrm{weak}}(\\mu)$ 来选择 $\\mu^{(k)}$，然后计算（真实的）$u(\\mu^{(k)})$ 并将其正交归一化后加入弱贪婪基。\n\n两种贪婪过程都从空基开始，并禁止重复选择已选参数。在这两种方法中，添加到基中的快照始终是全阶解 $u(\\mu^{(k)})$。\n\n通过以下两个标量来量化两种策略之间快照选择的差异，对于给定的一次运行，产生有序序列 $(\\mu^{(1)}_{\\mathrm{weak}},\\dots,\\mu^{(k_{\\max})}_{\\mathrm{weak}})$ 和 $(\\mu^{(1)}_{\\mathrm{strong}},\\dots,\\mu^{(k_{\\max})}_{\\mathrm{strong}})$：\n- 有序位置差异度 $D_{\\mathrm{ord}} = \\frac{1}{k_{\\max}}\\,\\left|\\{k \\in \\{1,\\dots,k_{\\max}\\} : \\mu^{(k)}_{\\mathrm{weak}} \\neq \\mu^{(k)}_{\\mathrm{strong}}\\}\\right|$（一个在 $[0,1]$ 内的浮点数）。\n- 集合对称差差异度 $D_{\\mathrm{set}} = \\frac{1}{k_{\\max}}\\,\\left|\\{\\mu^{(k)}_{\\mathrm{weak}} : 1 \\le k \\le k_{\\max}\\} \\,\\Delta\\, \\{\\mu^{(k)}_{\\mathrm{strong}} : 1 \\le k \\le k_{\\max}\\}\\right|$，其中 $\\Delta$ 表示集合的对称差（一个在 $[0,1]$ 内的浮点数）。\n\n必须遵守的实现细节：\n- 使用 $N = 60$ 个内部点。\n- 使用训练参数集 $\\mathcal{S}_{\\mathrm{train}} = \\{0, 0.05, 0.10, \\dots, 1.00\\}$，即在 $[0,1]$ 上的 $21$ 个均匀分布的参数。\n- 使用 $k_{\\max} = 6$ 次贪婪迭代。\n- 使用在 $[0,1]$ 区间内 $41$ 个均匀分布参数处的载荷快照构建 DEIM 基 $U_m$，使用前 $m$ 个左奇异向量，DEIM 插值索引由上述标准 DEIM 贪婪规则选择。\n\n测试套件：\n- 情况 1: $m = 1$。\n- 情况 2: $m = 3$。\n- 情况 3: $m = 8$。\n- 情况 4: $m = 20$。\n\n对于每种情况，独立构建 DEIM 算子并运行两种贪婪过程以获得 $(D_{\\mathrm{ord}}, D_{\\mathrm{set}})$。最终程序输出必须是单行，包含一个展平列表，其中按顺序包含 $8$ 个浮点数：$[D_{\\mathrm{ord}}^{(1)}, D_{\\mathrm{set}}^{(1)}, D_{\\mathrm{ord}}^{(2)}, D_{\\mathrm{set}}^{(2)}, D_{\\mathrm{ord}}^{(3)}, D_{\\mathrm{set}}^{(3)}, D_{\\mathrm{ord}}^{(4)}, D_{\\mathrm{set}}^{(4)}]$。\n\n您的程序必须是完整且可运行的，仅使用指定的库，且不要求任何输入。最终的打印输出必须是单行，包含一个具有方括号和逗号分隔十进制数的精确格式的列表。不涉及任何物理单位。不使用角度。任何地方都不得使用百分比；所有比率必须是小数。",
            "solution": "用户提供的问题经评估是**有效**的。该问题在偏微分方程数值分析这一成熟领域具有科学依据，特别是在使用降阶基（RB）方法的模型降阶方面。该问题是适定的，所有必要的参数、方程和算法过程都得到了清晰明确的定义。它提出了一个正式且非平凡的计算任务，旨在比较两种用于生成降阶基的标准贪婪算法变体，这是科学计算中的一个相关课题。该问题不包含事实错误、矛盾或主观因素。我们注意到了一个小的不一致之处：问题陈述声称度量 $D_{\\mathrm{set}}$ 位于 $[0,1]$ 区间内，而其给定公式 $D_{\\mathrm{set}} = k_{\\max}^{-1} |S_{\\mathrm{weak}} \\Delta S_{\\mathrm{strong}}|$ 产生的值最高可达 2。这被认为是一个描述性错误，而非程序性错误，因为公式本身是明确的。实现将严格遵循所提供的公式。\n\n### 方法论框架\n\n目标是实现并对比两种贪婪算法，用于为参数化椭圆偏微分方程（PDE）构建降阶基。这两种算法的关键区别在于用于选择新基快照的误差度量：一种采用严格的后验误差估计器（“强贪婪”），而另一种使用基于超约化系统的计算成本更低的代理指示器（“弱贪婪”）。\n\n### 1. 全阶模型（FOM）\n\n物理系统由在域 $x \\in (0,1)$ 上的边值问题描述：\n$$\n-a(\\mu)\\,u''(x;\\mu) + u(x;\\mu) = g(x;\\mu), \\quad u(0;\\mu) = u(1;\\mu) = 0\n$$\n其中，依赖于参数的扩散系数为 $a(\\mu) = 1 + 0.5\\,\\mu$，非仿射右端项为 $g(x;\\mu) = \\exp(\\mu x) + \\sin(3\\pi x)$。参数 $\\mu$ 属于一个指定的训练集 $\\mathcal{S}_{\\mathrm{train}}$。\n\n我们使用二阶中心有限差分法，在 $N$ 个内部点 $\\{x_i\\}_{i=1}^N$ 的均匀网格上离散化此 PDE，网格间距为 $h = 1/(N+1)$。这将 PDE 转换为一个大小为 $N \\times N$ 的线性方程组：\n$$\nA(\\mu)\\,u(\\mu) = f(\\mu)\n$$\n这里，$u(\\mu) \\in \\mathbb{R}^N$ 是近似解的节点值向量。系统矩阵 $A(\\mu)$ 具有仿射参数依赖性：\n$$\nA(\\mu) = a(\\mu)\\,K + I\n$$\n其中 $K \\in \\mathbb{R}^{N \\times N}$ 是离散拉普拉斯算子（刚度）矩阵，$K = \\frac{1}{h^2}\\,\\mathrm{tridiag}(-1, 2, -1)$，$I \\in \\mathbb{R}^{N \\times N}$ 是单位矩阵。载荷向量 $f(\\mu) \\in \\mathbb{R}^N$ 通过在网格点上计算 $g(x_i; \\mu)$ 得到，即 $f_i(\\mu) = \\exp(\\mu x_i) + \\sin(3\\pi x_i)$。由于对于 $\\mu \\ge 0$，$a(\\mu) > 0$，并且 $K$ 是对称正定的（在零边界值的函数空间上），因此对于所有相关的 $\\mu$，矩阵 $A(\\mu)$ 也是对称且正定的。\n\n### 2. 降阶建模与超约化\n\n降阶基方法旨在低维子空间 $V_k = \\mathrm{span}\\{u(\\mu^{(1)}), \\dots, u(\\mu^{(k)})\\} \\subset \\mathbb{R}^N$ 中寻找近似解 $u_k(\\mu)$。基向量是为精心选择的参数值 $\\{\\mu^{(j)}\\}_{j=1}^k$ 求解全阶模型得到的解，称为“快照”。我们用一个正交矩阵 $V_k \\in \\mathbb{R}^{N \\times k}$ 来表示这个子空间。\n\n近似解为 $u_k(\\mu) = V_k y(\\mu)$，其中坐标向量 $y(\\mu) \\in \\mathbb{R}^k$ 通过将全阶模型伽辽金投影到子空间 $V_k$ 上求得：\n$$\n[V_k^\\top A(\\mu) V_k]\\,y(\\mu) = V_k^\\top \\widehat{f}(\\mu)\n$$\n这会得到一个规模小得多的 $k \\times k$ 系统。项 $\\widehat{f}(\\mu)$ 表示在在线阶段使用的右端项，它在两种贪婪策略中有所不同。\n\n$f(\\mu)$ 的非仿射性质会带来一个计算瓶颈，因为构造投影后的右端项 $V_k^\\top f(\\mu)$ 需要与大维度 $N$ 成比例的运算。为了解决这个问题，采用了离散经验插值方法（DEIM）。DEIM 将向量 $f(\\mu)$ 近似为：\n$$\nf_{\\mathrm{DEIM}}(\\mu) = U_m (P^\\top U_m)^{-1} P^\\top f(\\mu)\n$$\n这里，$U_m \\in \\mathbb{R}^{N \\times m}$ 是右端项向量的基，通常是 $f(\\mu)$ 值快照矩阵的奇异值分解（SVD）得到的前 $m$ 个左奇异向量。矩阵 $P \\in \\mathbb{R}^{N \\times m}$ 是一个选择矩阵，它选取向量的 $m$ 个特定行，对应于 $m$ 个插值索引。这些索引通过一个旨在最小化投影误差的贪婪过程来选择。这种近似使得在线阶段的评估可以快速进行。\n\n### 3. 贪婪选择算法\n\n降阶基方法的核心是用于选择快照以构建基 $V_k$ 的贪婪算法。该过程从一个空基开始，从训练集 $\\mathcal{S}_{\\mathrm{train}}$ 中迭代选择使当前降阶基 $V_{k-1}$ 的误差估计最大的参数 $\\mu^{(k)}$。然后计算相应的全阶模型解 $u(\\mu^{(k)})$，与现有基进行正交化，并作为新的基向量添加。\n\n#### 强贪婪选择\n此方法使用数学上严格的后验误差估计器。对于对称正定系统，能量范数下的误差由残差在算子对偶范数下的范数精确给出：\n$$\n\\Delta_{\\mathrm{strong}}(\\mu) = \\|u(\\mu) - u_{k-1}(\\mu)\\|_{A(\\mu)} = \\|r(\\mu)\\|_{A(\\mu)^{-1}} = \\sqrt{r(\\mu)^\\top A(\\mu)^{-1} r(\\mu)}\n$$\n其中 $r(\\mu) = A(\\mu)u_{k-1}(\\mu) - f(\\mu)$ 是真实残差。降阶解 $u_{k-1}(\\mu)$ 使用真实的右端项计算，即 $\\widehat{f}(\\mu) = f(\\mu)$。为了评估 $\\Delta_{\\mathrm{strong}}(\\mu)$，必须计算 $A(\\mu)^{-1}$ 在 $r(\\mu)$ 上的作用，这需要在每个贪婪步骤中为每个 $\\mu \\in \\mathcal{S}_{\\mathrm{train}}$ 求解一个全阶线性系统。这使得离线基生成过程的计算成本很高。\n\n第 $k$ 步的选择规则是：\n$$\n\\mu^{(k)} = \\arg\\max_{\\mu \\in \\mathcal{S}_{\\mathrm{train}} \\setminus \\{\\mu^{(1)}, \\dots\\}} \\Delta_{\\mathrm{strong}}(\\mu)\n$$\n\n#### 弱贪婪选择\n这种方法旨在通过使用一个评估成本更低的代理误差指示器来降低离线成本。首先，使用 DEIM 近似的右端项计算降阶解，即 $\\widehat{f}(\\mu) = f_{\\mathrm{DEIM}}(\\mu)$。这会产生一个基于 DEIM 的残差 $r_{\\mathrm{DEIM}}(\\mu) = A(\\mu)u_{k-1}(\\mu) - f_{\\mathrm{DEIM}}(\\mu)$。然后，误差指示器定义为：\n$$\n\\eta_{\\mathrm{weak}}(\\mu) = \\frac{\\|r_{\\mathrm{DEIM}}(\\mu)\\|_2}{\\alpha_{\\mathrm{LB}}(\\mu)}\n$$\n其中 $\\|\\cdot\\|_2$ 是标准的欧几里得范数，$\\alpha_{\\mathrm{LB}}(\\mu)$ 是 $A(\\mu)$ 最小特征值（矫顽性常数）的下界。对于给定的问题，这是 $\\alpha_{\\mathrm{LB}}(\\mu) = 1 + a(\\mu)\\,\\lambda_1(K)$，其中 $\\lambda_1(K) = \\frac{4}{h^2}\\sin^2(\\frac{\\pi}{2(N+1)})$ 是已知的离散一维拉普拉斯算子的最小特征值。该指示器避免了与 $A(\\mu)^{-1}$ 进行昂贵的线性求解，但引入了两个近似来源：使用 $f_{\\mathrm{DEIM}}(\\mu)$ 以及用缩放的欧几里得范数代替能量范数。\n\n选择规则类似：\n$$\n\\mu^{(k)} = \\arg\\max_{\\mu \\in \\mathcal{S}_{\\mathrm{train}} \\setminus \\{\\mu^{(1)}, \\dots\\}} \\eta_{\\mathrm{weak}}(\\mu)\n$$\n\n### 4. 差异度度量\n\n该实验使用两个度量来量化所选参数的有序序列 $(\\mu^{(1)}_{\\mathrm{weak}}, \\dots, \\mu^{(k_{\\max})}_{\\mathrm{weak}})$ 和 $(\\mu^{(1)}_{\\mathrm{strong}}, \\dots, \\mu^{(k_{\\max})}_{\\mathrm{strong}})$ 之间的差异：\n- **有序位置差异度 ($D_{\\mathrm{ord}}$):** 所选参数不同的位置所占的比例：\n$$D_{\\mathrm{ord}} = \\frac{1}{k_{\\max}}\\,|\\{k \\in \\{1,\\dots,k_{\\max}\\} : \\mu^{(k)}_{\\mathrm{weak}} \\neq \\mu^{(k)}_{\\mathrm{strong}}\\}|$$\n- **集合对称差差异度 ($D_{\\mathrm{set}}$):** 衡量所选参数集合的差异，并按基的大小进行归一化：\n$$D_{\\mathrm{set}} = \\frac{1}{k_{\\max}}\\,|\\{\\mu^{(k)}_{\\mathrm{weak}} : 1 \\le k \\le k_{\\max}\\} \\,\\Delta\\, \\{\\mu^{(k)}_{\\mathrm{strong}} : 1 \\le k \\le k_{\\max}\\}|$$\n其中 $\\Delta$ 表示集合的对称差。\n\n所提供的程序实现了这整个计算框架，以针对不同的 DEIM 基大小 $m$ 产生这些度量，从而说明 DEIM 近似的质量如何影响弱贪婪算法的快照选择。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and runs a computational experiment to compare weak greedy and\n    strong greedy selection for a reduced basis approximation of a parametrized\n    elliptic PDE, with the right-hand side hyperreduced via DEIM.\n    \"\"\"\n\n    # --- 1. Problem Definition  Discretization Parameters ---\n    N = 60\n    k_max = 6\n    s_train_params = np.linspace(0.0, 1.0, 21)\n    deim_train_params = np.linspace(0.0, 1.0, 41)\n    test_cases_m = [1, 3, 8, 20]\n\n    h = 1.0 / (N + 1)\n    x_grid = np.linspace(h, 1.0 - h, N)\n\n    # --- 2. Full-Order Model (FOM) Assembly ---\n    # Stiffness matrix K (from -u'') and Identity I\n    diag_main = np.full(N, 2.0)\n    diag_off = np.full(N - 1, -1.0)\n    K = (np.diag(diag_main) + np.diag(diag_off, k=1) + np.diag(diag_off, k=-1)) / h**2\n    I = np.eye(N)\n\n    # Parameter-dependent functions\n    def a_func(mu):\n        return 1.0 + 0.5 * mu\n\n    def f_func(mu, x):\n        return np.exp(mu * x) + np.sin(3 * np.pi * x)\n\n    def assemble_A(mu):\n        return a_func(mu) * K + I\n\n    def assemble_f(mu):\n        return f_func(mu, x_grid)\n\n    def solve_fom(mu):\n        A_mu = assemble_A(mu)\n        f_mu = assemble_f(mu)\n        return np.linalg.solve(A_mu, f_mu)\n\n    # --- 3. DEIM and RB Helper Functions ---\n    def build_deim_assets(m):\n        \"\"\"\n        Builds the DEIM basis U_m, interpolation indices, and the matrix\n        needed for the DEIM approximation.\n        \"\"\"\n        if m == 0:\n            return None, None\n\n        # Collect snapshots of the RHS function f(mu)\n        f_snapshots = np.array([assemble_f(mu) for mu in deim_train_params]).T\n\n        # SVD for DEIM basis\n        U, _, _ = np.linalg.svd(f_snapshots, full_matrices=False)\n        U_m = U[:, :m]\n\n        # Greedy selection of DEIM interpolation indices\n        indices = []\n        # First index\n        res = U_m[:, 0]\n        p_idx = np.argmax(np.abs(res))\n        indices.append(p_idx)\n\n        # Subsequent indices\n        for j in range(1, m):\n            u_j = U_m[:, j]\n            U_j_minus_1 = U_m[:, :j]\n            \n            # Solve for coefficients c: U_m[p, :j] c = u_j[p]\n            PT_U = U_j_minus_1[indices, :]\n            PT_u = u_j[indices]\n            try:\n                coeffs = np.linalg.solve(PT_U, PT_u)\n            except np.linalg.LinAlgError:\n                # Use least squares for stability if matrix is ill-conditioned\n                coeffs, _, _, _ = np.linalg.lstsq(PT_U, PT_u, rcond=None)\n            \n            # Compute residual\n            res = u_j - U_j_minus_1 @ coeffs\n            new_idx = np.argmax(np.abs(res))\n            indices.append(new_idx)\n        \n        # Pre-compute inverse for DEIM approximator\n        PT_Um = U_m[indices, :]\n        deim_inv = np.linalg.inv(PT_Um)\n\n        def deim_approximator(f_vec):\n            return U_m @ (deim_inv @ f_vec[indices])\n            \n        return deim_approximator\n\n    def gram_schmidt(v, V_basis):\n        \"\"\"Orthonormalizes a vector v against the columns of an orthonormal matrix V_basis.\"\"\"\n        if V_basis.shape[1] == 0:\n            norm_v = np.linalg.norm(v)\n            return v / norm_v if norm_v > 1e-12 else np.zeros_like(v)\n        \n        proj = V_basis @ (V_basis.T @ v)\n        v_orth = v - proj\n        norm_v_orth = np.linalg.norm(v_orth)\n        \n        if norm_v_orth > 1e-12:\n            return v_orth / norm_v_orth\n        else:\n            return np.zeros_like(v)\n\n    def run_greedy_procedure(strategy, deim_approximator_func=None, coercivity_func=None):\n        \"\"\"Performs the greedy algorithm for either the strong or weak strategy.\"\"\"\n        selected_mus = []\n        selected_indices = set()\n        V_basis = np.zeros((N, 0))\n        \n        for k in range(k_max):\n            max_error = -1.0\n            best_mu = -1.0\n            \n            # Iterate over parameters not yet selected\n            current_train_indices = [i for i, _ in enumerate(s_train_params) if i not in selected_indices]\n\n            for idx in current_train_indices:\n                mu = s_train_params[idx]\n                A_mu = assemble_A(mu)\n                \n                if k == 0:\n                    u_rb = np.zeros(N)\n                else:\n                    A_rb = V_basis.T @ A_mu @ V_basis\n                    f_mu_true = assemble_f(mu)\n                    \n                    if strategy == 'strong':\n                        f_rb = V_basis.T @ f_mu_true\n                    else:  # weak\n                        f_deim = deim_approximator_func(f_mu_true)\n                        f_rb = V_basis.T @ f_deim\n                    \n                    y_rb = np.linalg.solve(A_rb, f_rb)\n                    u_rb = V_basis @ y_rb\n                \n                # Calculate error based on strategy\n                if strategy == 'strong':\n                    f_mu_true = assemble_f(mu)\n                    residual = A_mu @ u_rb - f_mu_true\n                    w = np.linalg.solve(A_mu, residual)\n                    error = np.sqrt(np.dot(residual, w))\n                else:  # weak\n                    f_mu_true = assemble_f(mu)\n                    f_deim = deim_approximator_func(f_mu_true)\n                    residual = A_mu @ u_rb - f_deim\n                    error = np.linalg.norm(residual) / coercivity_func(mu)\n                    \n                if error > max_error:\n                    max_error = error\n                    best_mu = mu\n            \n            best_mu_idx = np.where(s_train_params == best_mu)[0][0]\n            selected_indices.add(best_mu_idx)\n            selected_mus.append(best_mu)\n\n            # Augment basis with the new FOM snapshot\n            new_snapshot = solve_fom(best_mu)\n            new_basis_vec = gram_schmidt(new_snapshot, V_basis)\n            V_basis = np.hstack([V_basis, new_basis_vec.reshape(-1, 1)]) if V_basis.size > 0 else new_basis_vec.reshape(-1, 1)\n\n        return selected_mus\n\n    # --- 4. Main Experiment Loop ---\n    final_results = []\n    \n    # Pre-calculate coercivity constant part\n    lambda1_K = (4.0 / h**2) * (np.sin(np.pi / (2.0 * (N + 1))))**2\n    def alpha_LB(mu):\n        return 1.0 + a_func(mu) * lambda1_K\n\n    # Run strong greedy once (results are independent of m)\n    strong_mus = run_greedy_procedure(strategy='strong')\n    \n    for m in test_cases_m:\n        # Build DEIM approximator for current m\n        deim_approximator = build_deim_assets(m)\n        \n        # Run weak greedy with the specific DEIM approximator\n        weak_mus = run_greedy_procedure(strategy='weak',\n                                        deim_approximator_func=deim_approximator,\n                                        coercivity_func=alpha_LB)\n\n        # Compute divergence metrics\n        # D_ord: Ordered-position divergence\n        ord_diff_count = sum(1 for i in range(k_max) if not np.isclose(strong_mus[i], weak_mus[i]))\n        D_ord = float(ord_diff_count) / k_max\n        \n        # D_set: Set-symmetric-difference divergence\n        set_strong = set(strong_mus)\n        set_weak = set(weak_mus)\n        sym_diff_size = len(set_strong.symmetric_difference(set_weak))\n        D_set = float(sym_diff_size) / k_max\n\n        final_results.extend([D_ord, D_set])\n        \n    print(f\"[{','.join(f'{x:.8f}' for x in final_results)}]\")\n\nsolve()\n```"
        }
    ]
}