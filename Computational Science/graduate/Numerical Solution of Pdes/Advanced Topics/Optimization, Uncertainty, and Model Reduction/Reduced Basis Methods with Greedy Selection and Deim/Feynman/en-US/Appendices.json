{
    "hands_on_practices": [
        {
            "introduction": "The first step in building a reduced basis model is often to extract the most dominant \"modes\" or \"patterns\" from a set of solution snapshots. Proper Orthogonal Decomposition (POD) provides a mathematically optimal way to do this for a given set of data. This foundational practice  will guide you through deriving the famous relationship between the POD truncation error in the $L^2$ norm and the singular values of the snapshot data matrix, cementing your understanding of why this method is so powerful for data compression and model reduction.",
            "id": "3438787",
            "problem": "Consider a parameterized, nonlinear, parabolic partial differential equation posed on a bounded domain $\\Omega \\subset \\mathbb{R}^{d}$ with homogeneous Dirichlet boundary conditions, discretized in space by a conforming finite element method with basis functions $\\{\\varphi_{i}\\}_{i=1}^{n}$. Let the semi-discrete model be written as\n$$\nM \\frac{d x(t,\\mu)}{d t} + A(\\mu)\\, x(t,\\mu) + g\\!\\left(x(t,\\mu);\\mu\\right) = b(\\mu),\n$$\nwhere $M \\in \\mathbb{R}^{n \\times n}$ is the symmetric positive definite mass matrix associated with the $L^{2}(\\Omega)$ inner product, $A(\\mu) \\in \\mathbb{R}^{n \\times n}$ is the stiffness-like operator depending on a parameter $\\mu \\in \\mathcal{P}$, $x(t,\\mu) \\in \\mathbb{R}^{n}$ is the coefficient vector of the finite element solution at time $t$, $g(\\cdot;\\mu)$ is a nonlinear term intended to be approximated in an online-efficient way via the Discrete Empirical Interpolation Method (DEIM), and $b(\\mu) \\in \\mathbb{R}^{n}$ is a source term. Assume that a candidate Reduced Basis is to be selected by a greedy procedure over $\\mu \\in \\mathcal{P}$, but for this problem focus only on the Proper Orthogonal Decomposition (POD) constructed from solution snapshots.\n\nLet $\\{x_{k}\\}_{k=1}^{m}$ be a collection of $m$ solution snapshots sampled over a training set of parameters and times, assembled into the snapshot matrix $X \\in \\mathbb{R}^{n \\times m}$ with columns $x_{k}$. Define the mass-weighted snapshot matrix $Y := M^{1/2} X \\in \\mathbb{R}^{n \\times m}$, where $M^{1/2}$ denotes the unique symmetric positive definite square root of $M$. Let the singular value decomposition of $Y$ be $Y = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{m \\times m}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{n \\times m}$ is diagonal in the sense that its nonzero entries are the singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{q} > 0$ on the main diagonal with $q = \\min\\{n,m\\}$. The rank-$r$ POD space is defined by the leading $r$ left singular vectors of $Y$ mapped back through $M^{-1/2}$, i.e., $\\Psi_{r} := M^{-1/2} U_{r} \\in \\mathbb{R}^{n \\times r}$, where $U_{r} \\in \\mathbb{R}^{n \\times r}$ collects the first $r$ columns of $U$. Since $\\Psi_{r}^{\\top} M \\Psi_{r} = I_{r}$, the $M$-orthogonal projector onto $\\operatorname{span}(\\Psi_{r})$ is $P_{r} := \\Psi_{r} \\Psi_{r}^{\\top} M$, and the rank-$r$ POD reconstruction of the snapshots is $X_{r} := P_{r} X$.\n\nDefine the total squared snapshot reconstruction error in the $L^{2}(\\Omega)$ sense as\n$$\nE_{r}^{2} := \\sum_{k=1}^{m} \\|x_{k} - X_{r}(:,k)\\|_{L^{2}(\\Omega)}^{2} = \\sum_{k=1}^{m} \\left( x_{k} - X_{r}(:,k) \\right)^{\\top} M \\left( x_{k} - X_{r}(:,k) \\right),\n$$\nwhich can be written compactly as\n$$\nE_{r}^{2} = \\|X - X_{r}\\|_{M,F}^{2} := \\operatorname{trace}\\!\\left((X - X_{r})^{\\top} M (X - X_{r})\\right).\n$$\n\nStarting only from the definitions of the mass-weighted inner product, the singular value decomposition, and orthogonal projection, derive the closed-form expression for $E_{r}^{2}$ in terms of the neglected singular values $\\{\\sigma_{i}\\}_{i=r+1}^{q}$ of $Y$, and thereby relate the POD truncation error to the $L^{2}(\\Omega)$ norm of the snapshot reconstruction error. Your final answer must be a single analytic expression in terms of $\\{\\sigma_{i}\\}_{i=r+1}^{q}$ that exactly equals $E_{r}^{2}$.",
            "solution": "The problem is valid as it presents a well-defined mathematical derivation within the established framework of Proper Orthogonal Decomposition (POD) for model order reduction. All terms are clearly defined, and the premises are scientifically and mathematically sound.\n\nThe objective is to derive a closed-form expression for the total squared snapshot reconstruction error, $E_r^2$. We begin with the provided definition of this error in the mass-matrix-weighted Frobenius norm:\n$$\nE_{r}^{2} = \\|X - X_{r}\\|_{M,F}^{2} = \\operatorname{trace}\\!\\left((X - X_{r})^{\\top} M (X - X_{r})\\right)\n$$\nwhere $X \\in \\mathbb{R}^{n \\times m}$ is the snapshot matrix and $X_r \\in \\mathbb{R}^{n \\times m}$ is its rank-$r$ POD reconstruction.\n\nThe mass matrix $M$ is symmetric and positive definite, which guarantees the existence of a unique symmetric positive definite square root, $M^{1/2}$. We can rewrite the trace expression by inserting $M = M^{1/2} M^{1/2}$. Since $M$ is symmetric, $(M^{1/2})^{\\top} = M^{1/2}$.\n$$\nE_{r}^{2} = \\operatorname{trace}\\!\\left((X - X_{r})^{\\top} M^{1/2} M^{1/2} (X - X_{r})\\right) = \\operatorname{trace}\\!\\left(\\left(M^{1/2}(X - X_{r})\\right)^{\\top} \\left(M^{1/2}(X - X_{r})\\right)\\right)\n$$\nThis expression is the squared standard Frobenius norm of the matrix $M^{1/2}(X - X_{r})$.\n$$\nE_{r}^{2} = \\|M^{1/2}(X - X_{r})\\|_{F}^{2}\n$$\nNow, we express the term inside the norm using the provided definitions. The reconstruction $X_r$ is defined as the projection of $X$ onto the POD space:\n$$\nX_{r} = P_{r} X\n$$\nwhere the projector $P_r$ is given by $P_{r} = \\Psi_{r} \\Psi_{r}^{\\top} M$. The POD basis $\\Psi_r$ is defined as $\\Psi_{r} = M^{-1/2} U_{r}$, where $U_r$ contains the leading $r$ left singular vectors of the mass-weighted snapshot matrix $Y = M^{1/2} X$.\n\nLet's substitute the definition of $\\Psi_r$ into the expression for $P_r$:\n$$\nP_{r} = (M^{-1/2} U_{r}) (\\Psi_{r}^{\\top}) M = (M^{-1/2} U_{r}) (M^{-1/2} U_{r})^{\\top} M = (M^{-1/2} U_{r}) (U_{r}^{\\top} (M^{-1/2})^{\\top}) M\n$$\nSince $M^{-1/2}$ is symmetric, $(M^{-1/2})^{\\top} = M^{-1/2}$.\n$$\nP_{r} = M^{-1/2} U_{r} U_{r}^{\\top} M^{-1/2} M = M^{-1/2} U_{r} U_{r}^{\\top} M^{1/2}\n$$\nNow we can analyze the term $M^{1/2}(X - X_r)$:\n$$\nM^{1/2}(X - X_{r}) = M^{1/2}(X - P_{r}X) = M^{1/2}X - M^{1/2}P_{r}X\n$$\nSubstituting the expression for $P_r$:\n$$\nM^{1/2} P_{r} X = M^{1/2} (M^{-1/2} U_{r} U_{r}^{\\top} M^{1/2}) X = (M^{1/2} M^{-1/2}) U_{r} U_{r}^{\\top} (M^{1/2} X) = I U_{r} U_{r}^{\\top} (M^{1/2} X)\n$$\nUsing the definition $Y = M^{1/2}X$, we have:\n$$\nM^{1/2} P_{r} X = U_{r} U_{r}^{\\top} Y\n$$\nTherefore, the error term becomes:\n$$\nM^{1/2}(X - X_{r}) = M^{1/2}X - U_{r} U_{r}^{\\top} Y = Y - U_{r} U_{r}^{\\top} Y\n$$\nThe problem is thus transformed from calculating a weighted norm of the error in $X$ to calculating the standard Frobenius norm of the error in $Y$ when projected onto the space spanned by the columns of $U_r$.\n$$\nE_{r}^{2} = \\|Y - U_{r} U_{r}^{\\top} Y\\|_{F}^{2}\n$$\nThe matrix $U_r U_r^T$ is the orthogonal projector onto the subspace spanned by the first $r$ columns of $U$. We now use the singular value decomposition of $Y$, which is given as $Y = U \\Sigma V^{\\top}$. In summation form, this is:\n$$\nY = \\sum_{i=1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\n$$\nwhere $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$ respectively, $\\sigma_i$ are the singular values, and $q = \\min\\{n,m\\}$. The columns $\\{u_i\\}$ form an orthonormal basis. The projection of $Y$ onto the span of $\\{u_i\\}_{i=1}^r$ is:\n$$\nU_{r} U_{r}^{\\top} Y = \\left(\\sum_{j=1}^{r} u_j u_j^{\\top}\\right) \\left(\\sum_{i=1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\\right) = \\sum_{j=1}^{r} \\sum_{i=1}^{q} \\sigma_{i} u_{j} (u_{j}^{\\top} u_{i}) v_{i}^{\\top}\n$$\nDue to the orthonormality of the vectors $\\{u_i\\}$, we have $u_{j}^{\\top} u_{i} = \\delta_{ji}$, where $\\delta_{ji}$ is the Kronecker delta. The expression simplifies to:\n$$\nU_{r} U_{r}^{\\top} Y = \\sum_{i=1}^{r} \\sigma_{i} u_{i} v_{i}^{\\top}\n$$\nThis is the truncated SVD of $Y$, which is the best rank-$r$ approximation of $Y$ in the Frobenius norm (Eckart-Young-Mirsky theorem). The projection error is:\n$$\nY - U_{r} U_{r}^{\\top} Y = \\sum_{i=1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top} - \\sum_{i=1}^{r} \\sigma_{i} u_{i} v_{i}^{\\top} = \\sum_{i=r+1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\n$$\nFinally, we compute the squared Frobenius norm of this error matrix:\n$$\nE_{r}^{2} = \\left\\|\\sum_{i=r+1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\\right\\|_{F}^{2} = \\operatorname{trace}\\!\\left(\\left(\\sum_{j=r+1}^{q} \\sigma_{j} u_{j} v_{j}^{\\top}\\right)^{\\top} \\left(\\sum_{i=r+1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\\right)\\right)\n$$\n$$\nE_{r}^{2} = \\operatorname{trace}\\!\\left(\\left(\\sum_{j=r+1}^{q} \\sigma_{j} v_{j} u_{j}^{\\top}\\right) \\left(\\sum_{i=r+1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\\right)\\right) = \\operatorname{trace}\\!\\left(\\sum_{j=r+1}^{q} \\sum_{i=r+1}^{q} \\sigma_{j} \\sigma_{i} v_{j} (u_{j}^{\\top} u_{i}) v_{i}^{\\top}\\right)\n$$\nAgain, using $u_{j}^{\\top} u_{i} = \\delta_{ji}$:\n$$\nE_{r}^{2} = \\operatorname{trace}\\!\\left(\\sum_{i=r+1}^{q} \\sigma_{i}^{2} v_{i} v_{i}^{\\top}\\right)\n$$\nBy linearity of the trace operator:\n$$\nE_{r}^{2} = \\sum_{i=r+1}^{q} \\sigma_{i}^{2} \\operatorname{trace}(v_{i} v_{i}^{\\top})\n$$\nUsing the cyclic property of the trace, $\\operatorname{trace}(v_{i} v_{i}^{\\top}) = \\operatorname{trace}(v_{i}^{\\top} v_{i})$. Since $v_i$ is a column of the orthogonal matrix $V$, it is a unit vector, so $v_{i}^{\\top} v_{i} = 1$.\n$$\nE_{r}^{2} = \\sum_{i=r+1}^{q} \\sigma_{i}^{2} \\cdot 1 = \\sum_{i=r+1}^{q} \\sigma_{i}^{2}\n$$\nThus, the total squared snapshot reconstruction error in the $L^2(\\Omega)$ sense is equal to the sum of the squares of the neglected singular values of the mass-weighted snapshot matrix $Y$.",
            "answer": "$$\n\\boxed{\\sum_{i=r+1}^{q} \\sigma_{i}^{2}}\n$$"
        },
        {
            "introduction": "While POD optimally compresses a given set of snapshots, a crucial question in building a reduced basis is which snapshots to generate in the first place. The greedy algorithm offers a powerful, near-optimal solution by iteratively selecting the parameter that yields the worst-approximated solution. In this hands-on coding exercise , you will implement and contrast two fundamental variants of the greedy algorithm: a \"strong\" version using a rigorous but expensive error estimator, and a \"weak\" version using a cheaper surrogate enabled by DEIM hyperreduction. This practice provides critical insight into the accuracy-versus-cost trade-offs that are central to practical model order reduction.",
            "id": "3438765",
            "problem": "Construct a fully specified computational experiment to contrast weak greedy selection (using a surrogate error indicator $\\eta(\\mu)$) and strong greedy selection (using a true a posteriori estimator) for a reduced basis (RB) approximation of a parametrized linear elliptic partial differential equation, when the nonaffine right-hand side is hyperreduced via the Discrete Empirical Interpolation Method (DEIM). Your program must implement both greedy strategies and quantify the divergence in snapshot selection sequences as a function of the DEIM basis size.\n\nConsider the boundary value problem on the unit interval with homogeneous Dirichlet boundary conditions:\n- Find $u(x;\\mu)$ such that $-a(\\mu)\\,u''(x;\\mu) + u(x;\\mu) = g(x;\\mu)$ for $x \\in (0,1)$, with $u(0;\\mu) = u(1;\\mu) = 0$,\n- where $a(\\mu) = 1 + 0.5\\,\\mu$, and $g(x;\\mu) = \\exp(\\mu x) + \\sin(3\\pi x)$.\n\nDiscretize using standard second-order centered finite differences on $N$ equally spaced interior nodes. Let $h = 1/(N+1)$ and $x_i = i\\,h$ for $i = 1,2,\\dots,N$. The discrete system is\n$$\nA(\\mu)\\,u(\\mu) = f(\\mu),\n$$\nwhere $A(\\mu) = a(\\mu)\\,K + I$, with $K$ the tridiagonal stiffness matrix $K = \\frac{1}{h^2}\\,\\mathrm{tridiag}(-1, 2,-1)$ and $I$ the $N \\times N$ identity matrix. The load vector $f(\\mu)$ is given by $f_i(\\mu) = \\exp(\\mu x_i) + \\sin(3\\pi x_i)$.\n\nDefine the reduced basis space $V_k = \\mathrm{span}\\{u(\\mu^{(1)}),\\dots,u(\\mu^{(k)})\\}$ with columns of $V_k \\in \\mathbb{R}^{N \\times k}$ orthonormalized in the Euclidean inner product. For a given $\\mu$ and basis $V_k$, the reduced solution $u_k(\\mu) = V_k y(\\mu)$ is obtained by Galerkin projection:\n$$\n(V_k^\\top A(\\mu) V_k)\\,y(\\mu) = V_k^\\top \\widehat{f}(\\mu),\n$$\nwhere $\\widehat{f}(\\mu)$ is the right-hand side used in the reduced solve (specified below for each greedy variant).\n\nIntroduce the Discrete Empirical Interpolation Method (DEIM) hyperreduction for the nonaffine $f(\\mu)$ as follows. Construct a DEIM basis $U_m \\in \\mathbb{R}^{N \\times m}$ by computing the first $m$ left singular vectors of a snapshot matrix of $f(\\mu)$ over a specified DEIM training set. Select DEIM interpolation indices via the standard DEIM greedy procedure (selecting indices maximizing componentwise absolute values of the residuals). Denote by $P \\in \\mathbb{R}^{N \\times m}$ the column selection matrix associated with the chosen indices. The DEIM approximation of $f(\\mu)$ is\n$$\nf_{\\mathrm{DEIM}}(\\mu) = U_m\\,(P^\\top U_m)^{-1} P^\\top f(\\mu).\n$$\n\nImplement and compare the following greedy strategies to build two separate reduced bases of size $k_{\\max}$, each by iteratively selecting $\\mu$ from a fixed training set $\\mathcal{S}_{\\mathrm{train}}$ that maximizes an error indicator. At iteration $k$, given the current basis $V_{k-1}$:\n\n- Strong greedy (true a posteriori estimator):\n  - Compute the reduced solution using the true right-hand side, i.e., $\\widehat{f}(\\mu) = f(\\mu)$.\n  - Form the true residual $r(\\mu) = A(\\mu)\\,u_{k-1}(\\mu) - f(\\mu)$, where $u_{k-1}(\\mu)$ is the current reduced solution in $V_{k-1}$ (with $u_0(\\mu) = 0$ if $k=1$).\n  - Use the exact energy-norm a posteriori estimator\n    $$\n    \\Delta_{\\mathrm{strong}}(\\mu) = \\|r(\\mu)\\|_{A(\\mu)^{-1}} = \\sqrt{r(\\mu)^\\top A(\\mu)^{-1} r(\\mu)},\n    $$\n    which is equal to the error in the energy norm $\\|u(\\mu) - u_{k-1}(\\mu)\\|_{A(\\mu)}$ for symmetric positive definite $A(\\mu)$.\n  - Select $\\mu^{(k)} = \\arg\\max_{\\mu \\in \\mathcal{S}_{\\mathrm{train}} \\setminus \\{\\mu^{(1)},\\dots,\\mu^{(k-1)}\\}} \\Delta_{\\mathrm{strong}}(\\mu)$, compute the full-order solution $u(\\mu^{(k)})$, and orthonormalize it into $V_k$.\n\n- Weak greedy (surrogate indicator under DEIM hyperreduction):\n  - Compute the reduced solution using the DEIM right-hand side, i.e., $\\widehat{f}(\\mu) = f_{\\mathrm{DEIM}}(\\mu)$.\n  - Form the DEIM residual $r_{\\mathrm{DEIM}}(\\mu) = A(\\mu)\\,u_{k-1}(\\mu) - f_{\\mathrm{DEIM}}(\\mu)$.\n  - Use the surrogate indicator\n    $$\n    \\eta_{\\mathrm{weak}}(\\mu) = \\frac{\\|r_{\\mathrm{DEIM}}(\\mu)\\|_2}{\\alpha_{\\mathrm{LB}}(\\mu)},\n    $$\n    where $\\alpha_{\\mathrm{LB}}(\\mu)$ is a rigorous coercivity lower bound for $A(\\mu)$ in the Euclidean norm. For the given discretization, the smallest eigenvalue of $K$ is exactly\n    $$\n    \\lambda_1(K) = \\frac{4}{h^2}\\,\\sin^2\\!\\left(\\frac{\\pi}{2(N+1)}\\right),\n    $$\n    so $\\alpha_{\\mathrm{LB}}(\\mu) = 1 + a(\\mu)\\,\\lambda_1(K)$.\n  - Select $\\mu^{(k)}$ analogously by maximizing $\\eta_{\\mathrm{weak}}(\\mu)$ over unused training parameters, then compute $u(\\mu^{(k)})$ (truth) and orthonormalize it into the weak-greedy basis.\n\nBoth greedy processes start with empty basis and prohibit reselection of previously chosen parameters. In both, the snapshot added to the basis is always the full-order solution $u(\\mu^{(k)})$.\n\nQuantify the divergence of snapshot selection between the two strategies by the following two scalars for a given run producing ordered sequences $(\\mu^{(1)}_{\\mathrm{weak}},\\dots,\\mu^{(k_{\\max})}_{\\mathrm{weak}})$ and $(\\mu^{(1)}_{\\mathrm{strong}},\\dots,\\mu^{(k_{\\max})}_{\\mathrm{strong}})$:\n- The ordered-position divergence $D_{\\mathrm{ord}} = \\frac{1}{k_{\\max}}\\,\\left|\\{k \\in \\{1,\\dots,k_{\\max}\\} : \\mu^{(k)}_{\\mathrm{weak}} \\neq \\mu^{(k)}_{\\mathrm{strong}}\\}\\right|$ (a float in $[0,1]$).\n- The set-symmetric-difference divergence $D_{\\mathrm{set}} = \\frac{1}{k_{\\max}}\\,\\left|\\{\\mu^{(k)}_{\\mathrm{weak}} : 1 \\le k \\le k_{\\max}\\} \\,\\Delta\\, \\{\\mu^{(k)}_{\\mathrm{strong}} : 1 \\le k \\le k_{\\max}\\}\\right|$, where $\\Delta$ denotes symmetric difference of sets (a float in $[0,1]$).\n\nImplementation details to adhere to:\n- Use $N = 60$ interior points.\n- Use training parameter set $\\mathcal{S}_{\\mathrm{train}} = \\{0, 0.05, 0.10, \\dots, 1.00\\}$, i.e., $21$ uniformly spaced parameters on $[0,1]$.\n- Use $k_{\\max} = 6$ greedy iterations.\n- Construct the DEIM basis $U_m$ from load snapshots at $41$ uniformly spaced parameters in $[0,1]$ using the first $m$ left singular vectors, with DEIM interpolation indices chosen by the standard DEIM greedy rule described above.\n\nTest suite:\n- Case $1$: $m = 1$.\n- Case $2$: $m = 3$.\n- Case $3$: $m = 8$.\n- Case $4$: $m = 20$.\n\nFor each case, independently build the DEIM operator and run both greedy procedures to obtain $(D_{\\mathrm{ord}}, D_{\\mathrm{set}})$. The final program output must be a single line containing a flattened list of the $8$ floats in the order $[D_{\\mathrm{ord}}^{(1)}, D_{\\mathrm{set}}^{(1)}, D_{\\mathrm{ord}}^{(2)}, D_{\\mathrm{set}}^{(2)}, D_{\\mathrm{ord}}^{(3)}, D_{\\mathrm{set}}^{(3)}, D_{\\mathrm{ord}}^{(4)}, D_{\\mathrm{set}}^{(4)}]$.\n\nYour program must be complete and runnable, using only the specified libraries, and must not require any input. The final print must be a single line containing the list in the exact format with square brackets and comma-separated decimal numbers. No physical units are involved. Angles are not used. Percentages must not be used anywhere; all ratios must be decimals.",
            "solution": "The user-provided problem is assessed to be **valid**. It is scientifically grounded in the well-established field of numerical analysis for partial differential equations, specifically in model order reduction using reduced basis (RB) methods. The problem is well-posed, with all necessary parameters, equations, and algorithmic procedures clearly and unambiguously defined. It presents a formal and non-trivial computational task to compare two standard variants of greedy algorithms for RB generation, which is a relevant topic in scientific computing. The problem contains no factual errors, contradictions, or subjective elements. A minor inconsistency was noted where the problem statement claims the metric $D_{\\text{set}}$ lies in $[0,1]$, while its given formula, $D_{\\text{set}} = k_{\\text{max}}^{-1} |S_{\\text{weak}} \\Delta S_{\\text{strong}}|$, can produce values up to $2$. This is considered a descriptive error, not a procedural one, as the formula itself is unambiguous. The implementation will strictly follow the provided formula.\n\n### Methodological Framework\n\nThe goal is to implement and contrast two greedy algorithms for constructing a reduced basis for a parametrized elliptic partial differential equation (PDE). The key difference between the algorithms lies in the error metric used to select new basis snapshots: one employs a rigorous a posteriori error estimator (\"strong greedy\"), while the other uses a computationally cheaper surrogate indicator derived from a hyperreduced system (\"weak greedy\").\n\n### 1. Full-Order Model (FOM)\n\nThe physical system is described by the boundary value problem on the domain $x \\in (0,1)$:\n$$\n-a(\\mu)\\,u''(x;\\mu) + u(x;\\mu) = g(x;\\mu), \\quad u(0;\\mu) = u(1;\\mu) = 0\n$$\nwhere the parameter-dependent diffusion coefficient is $a(\\mu) = 1 + 0.5\\,\\mu$ and the non-affine right-hand side is $g(x;\\mu) = \\exp(\\mu x) + \\sin(3\\pi x)$. The parameter $\\mu$ belongs to a specified training set $\\mathcal{S}_{\\mathrm{train}}$.\n\nWe discretize this PDE using a second-order centered finite difference scheme on a uniform grid of $N$ interior points $\\{x_i\\}_{i=1}^N$, with spacing $h = 1/(N+1)$. This transforms the PDE into a linear system of equations of size $N \\times N$:\n$$\nA(\\mu)\\,u(\\mu) = f(\\mu)\n$$\nHere, $u(\\mu) \\in \\mathbb{R}^N$ is the vector of nodal values of the approximate solution. The system matrix $A(\\mu)$ has an affine parameter dependence:\n$$\nA(\\mu) = a(\\mu)\\,K + I\n$$\nwhere $K \\in \\mathbb{R}^{N \\times N}$ is the discrete Laplacian (stiffness) matrix, $K = \\frac{1}{h^2}\\,\\mathrm{tridiag}(-1, 2, -1)$, and $I \\in \\mathbb{R}^{N \\times N}$ is the identity matrix. The load vector $f(\\mu) \\in \\mathbb{R}^N$ is given by evaluating $g(x_i; \\mu)$ at the grid points, i.e., $f_i(\\mu) = \\exp(\\mu x_i) + \\sin(3\\pi x_i)$. Since $a(\\mu) > 0$ for $\\mu \\ge 0$ and $K$ is symmetric positive definite (on the space of functions with zero boundary values), the matrix $A(\\mu)$ is also symmetric and positive definite for all relevant $\\mu$.\n\n### 2. Reduced-Order Modeling and Hyperreduction\n\nThe reduced basis method seeks an approximate solution $u_k(\\mu)$ in a low-dimensional subspace $V_k = \\mathrm{span}\\{u(\\mu^{(1)}), \\dots, u(\\mu^{(k)})\\} \\subset \\mathbb{R}^N$. The basis vectors are solutions of the FOM, called \"snapshots,\" for carefully selected parameter values $\\{\\mu^{(j)}\\}_{j=1}^k$. We represent this subspace by an orthonormal matrix $V_k \\in \\mathbb{R}^{N \\times k}$.\n\nThe approximate solution is $u_k(\\mu) = V_k y(\\mu)$, where the coordinate vector $y(\\mu) \\in \\mathbb{R}^k$ is found by a Galerkin projection of the FOM onto the subspace $V_k$:\n$$\n[V_k^\\top A(\\mu) V_k]\\,y(\\mu) = V_k^\\top \\widehat{f}(\\mu)\n$$\nThis results in a much smaller $k \\times k$ system. The term $\\widehat{f}(\\mu)$ represents the right-hand side used in the online stage, which differs between the two greedy strategies.\n\nA computational bottleneck arises from the non-affine nature of $f(\\mu)$, as forming the projected right-hand side $V_k^\\top f(\\mu)$ requires operations scaling with the large dimension $N$. To circumvent this, the Discrete Empirical Interpolation Method (DEIM) is employed. DEIM approximates the vector $f(\\mu)$ as:\n$$\nf_{\\mathrm{DEIM}}(\\mu) = U_m (P^\\top U_m)^{-1} P^\\top f(\\mu)\n$$\nHere, $U_m \\in \\mathbb{R}^{N \\times m}$ is a basis for the right-hand side vectors, typically the first $m$ left singular vectors from an SVD of a snapshot matrix of $f(\\mu)$ values. The matrix $P \\in \\mathbb{R}^{N \\times m}$ is a selection matrix that picks $m$ specific rows of a vector, corresponding to $m$ interpolation indices. These indices are chosen via a greedy procedure that aims to minimize the projection error. This approximation allows for rapid evaluation in the online stage.\n\n### 3. Greedy Selection Algorithms\n\nThe core of the RB method is the greedy algorithm for selecting the snapshots to build the basis $V_k$. The procedure starts with an empty basis and iteratively selects the parameter $\\mu^{(k)}$ from the training set $\\mathcal{S}_{\\mathrm{train}}$ that maximizes an error estimate for the current reduced basis $V_{k-1}$. The corresponding FOM solution $u(\\mu^{(k)})$ is computed, orthonormalized against the existing basis, and added as the new basis vector.\n\n#### Strong Greedy Selection\nThis method uses a mathematically rigorous a posteriori error estimator. For a symmetric positive definite system, the error in the energy norm is exactly given by the norm of the residual in the dual norm of the operator:\n$$\n\\Delta_{\\mathrm{strong}}(\\mu) = \\|u(\\mu) - u_{k-1}(\\mu)\\|_{A(\\mu)} = \\|r(\\mu)\\|_{A(\\mu)^{-1}} = \\sqrt{r(\\mu)^\\top A(\\mu)^{-1} r(\\mu)}\n$$\nwhere $r(\\mu) = A(\\mu)u_{k-1}(\\mu) - f(\\mu)$ is the true residual. The reduced solution $u_{k-1}(\\mu)$ is computed using the true right-hand side, i.e., $\\widehat{f}(\\mu) = f(\\mu)$. To evaluate $\\Delta_{\\mathrm{strong}}(\\mu)$, one must compute the action of $A(\\mu)^{-1}$ on $r(\\mu)$, which requires solving a full-order linear system for each $\\mu \\in \\mathcal{S}_{\\mathrm{train}}$ at each greedy step. This makes the offline basis generation computationally expensive.\n\nThe selection rule at step $k$ is:\n$$\n\\mu^{(k)} = \\arg\\max_{\\mu \\in \\mathcal{S}_{\\mathrm{train}} \\setminus \\{\\mu^{(1)}, \\dots\\}} \\Delta_{\\mathrm{strong}}(\\mu)\n$$\n\n#### Weak Greedy Selection\nThis approach aims to reduce the offline cost by using a surrogate error indicator that is cheaper to evaluate. First, the reduced solution is computed using the DEIM-approximated right-hand side, i.e., $\\widehat{f}(\\mu) = f_{\\mathrm{DEIM}}(\\mu)$. This leads to a DEIM-based residual $r_{\\mathrm{DEIM}}(\\mu) = A(\\mu)u_{k-1}(\\mu) - f_{\\mathrm{DEIM}}(\\mu)$. The error indicator is then defined as:\n$$\n\\eta_{\\mathrm{weak}}(\\mu) = \\frac{\\|r_{\\mathrm{DEIM}}(\\mu)\\|_2}{\\alpha_{\\mathrm{LB}}(\\mu)}\n$$\nwhere $\\|\\cdot\\|_2$ is the standard Euclidean norm and $\\alpha_{\\mathrm{LB}}(\\mu)$ is a lower bound for the smallest eigenvalue (coercivity constant) of $A(\\mu)$. For the given problem, this is $\\alpha_{\\mathrm{LB}}(\\mu) = 1 + a(\\mu)\\,\\lambda_1(K)$, where $\\lambda_1(K) = \\frac{4}{h^2}\\sin^2(\\frac{\\pi}{2(N+1)})$ is the known smallest eigenvalue of the discrete 1D Laplacian. This indicator avoids the expensive linear solve with $A(\\mu)^{-1}$ but introduces two sources of approximation: the use of $f_{\\mathrm{DEIM}}(\\mu)$ and the replacement of the energy norm with a scaled Euclidean norm.\n\nThe selection rule is analogous:\n$$\n\\mu^{(k)} = \\arg\\max_{\\mu \\in \\mathcal{S}_{\\mathrm{train}} \\setminus \\{\\mu^{(1)}, \\dots\\}} \\eta_{\\mathrm{weak}}(\\mu)\n$$\n\n### 4. Divergence Metrics\n\nThe experiment quantifies the difference in the ordered sequences of selected parameters, $(\\mu^{(1)}_{\\mathrm{weak}}, \\dots, \\mu^{(k_{\\max})}_{\\mathrm{weak}})$ and $(\\mu^{(1)}_{\\mathrm{strong}}, \\dots, \\mu^{(k_{\\max})}_{\\mathrm{strong}})$, using two metrics:\n- **Ordered-Position Divergence ($D_{\\mathrm{ord}}$):** The fraction of positions at which the selected parameters differ:\n$$D_{\\mathrm{ord}} = \\frac{1}{k_{\\max}}\\,|\\{k \\in \\{1,\\dots,k_{\\max}\\} : \\mu^{(k)}_{\\mathrm{weak}} \\neq \\mu^{(k)}_{\\mathrm{strong}}\\}|$$\n- **Set-Symmetric-Difference Divergence ($D_{\\mathrm{set}}$):** Measures the disagreement in the sets of chosen parameters, normalized by the basis size:\n$$D_{\\mathrm{set}} = \\frac{1}{k_{\\max}}\\,|\\{\\mu^{(k)}_{\\mathrm{weak}} : 1 \\le k \\le k_{\\max}\\} \\,\\Delta\\, \\{\\mu^{(k)}_{\\mathrm{strong}} : 1 \\le k \\le k_{\\max}\\}|$$\nwhere $\\Delta$ denotes the symmetric difference of sets.\n\nThe provided program implements this entire computational framework to produce these metrics for varying DEIM basis sizes $m$, illustrating how the quality of the DEIM approximation impacts the snapshot selection of the weak greedy algorithm.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and runs a computational experiment to compare weak greedy and\n    strong greedy selection for a reduced basis approximation of a parametrized\n    elliptic PDE, with the right-hand side hyperreduced via DEIM.\n    \"\"\"\n\n    # --- 1. Problem Definition & Discretization Parameters ---\n    N = 60\n    k_max = 6\n    s_train_params = np.linspace(0.0, 1.0, 21)\n    deim_train_params = np.linspace(0.0, 1.0, 41)\n    test_cases_m = [1, 3, 8, 20]\n\n    h = 1.0 / (N + 1)\n    x_grid = np.linspace(h, 1.0 - h, N)\n\n    # --- 2. Full-Order Model (FOM) Assembly ---\n    # Stiffness matrix K (from -u'') and Identity I\n    diag_main = np.full(N, 2.0)\n    diag_off = np.full(N - 1, -1.0)\n    K = (np.diag(diag_main) + np.diag(diag_off, k=1) + np.diag(diag_off, k=-1)) / h**2\n    I = np.eye(N)\n\n    # Parameter-dependent functions\n    def a_func(mu):\n        return 1.0 + 0.5 * mu\n\n    def f_func(mu, x):\n        return np.exp(mu * x) + np.sin(3 * np.pi * x)\n\n    def assemble_A(mu):\n        return a_func(mu) * K + I\n\n    def assemble_f(mu):\n        return f_func(mu, x_grid)\n\n    def solve_fom(mu):\n        A_mu = assemble_A(mu)\n        f_mu = assemble_f(mu)\n        return np.linalg.solve(A_mu, f_mu)\n\n    # --- 3. DEIM and RB Helper Functions ---\n    def build_deim_assets(m):\n        \"\"\"\n        Builds the DEIM basis U_m, interpolation indices, and the matrix\n        needed for the DEIM approximation.\n        \"\"\"\n        if m == 0:\n            return None, None\n\n        # Collect snapshots of the RHS function f(mu)\n        f_snapshots = np.array([assemble_f(mu) for mu in deim_train_params]).T\n\n        # SVD for DEIM basis\n        U, _, _ = np.linalg.svd(f_snapshots, full_matrices=False)\n        U_m = U[:, :m]\n\n        # Greedy selection of DEIM interpolation indices\n        indices = []\n        # First index\n        res = U_m[:, 0]\n        p_idx = np.argmax(np.abs(res))\n        indices.append(p_idx)\n\n        # Subsequent indices\n        for j in range(1, m):\n            u_j = U_m[:, j]\n            U_j_minus_1 = U_m[:, :j]\n            \n            # Solve for coefficients c: U_m[p, :j] c = u_j[p]\n            PT_U = U_j_minus_1[indices, :]\n            PT_u = u_j[indices]\n            try:\n                coeffs = np.linalg.solve(PT_U, PT_u)\n            except np.linalg.LinAlgError:\n                # Use least squares for stability if matrix is ill-conditioned\n                coeffs, _, _, _ = np.linalg.lstsq(PT_U, PT_u, rcond=None)\n            \n            # Compute residual\n            res = u_j - U_j_minus_1 @ coeffs\n            new_idx = np.argmax(np.abs(res))\n            indices.append(new_idx)\n        \n        # Pre-compute inverse for DEIM approximator\n        PT_Um = U_m[indices, :]\n        deim_inv = np.linalg.inv(PT_Um)\n\n        def deim_approximator(f_vec):\n            return U_m @ (deim_inv @ f_vec[indices])\n            \n        return deim_approximator\n\n    def gram_schmidt(v, V_basis):\n        \"\"\"Orthonormalizes a vector v against the columns of an orthonormal matrix V_basis.\"\"\"\n        if V_basis.shape[1] == 0:\n            norm_v = np.linalg.norm(v)\n            return v / norm_v if norm_v > 1e-12 else np.zeros_like(v)\n        \n        proj = V_basis @ (V_basis.T @ v)\n        v_orth = v - proj\n        norm_v_orth = np.linalg.norm(v_orth)\n        \n        if norm_v_orth > 1e-12:\n            return v_orth / norm_v_orth\n        else:\n            return np.zeros_like(v)\n\n    def run_greedy_procedure(strategy, deim_approximator_func=None, coercivity_func=None):\n        \"\"\"Performs the greedy algorithm for either the strong or weak strategy.\"\"\"\n        selected_mus = []\n        selected_indices = set()\n        V_basis = np.zeros((N, 0))\n        \n        for k in range(k_max):\n            max_error = -1.0\n            best_mu = -1.0\n            \n            # Iterate over parameters not yet selected\n            current_train_indices = [i for i, _ in enumerate(s_train_params) if i not in selected_indices]\n\n            for idx in current_train_indices:\n                mu = s_train_params[idx]\n                A_mu = assemble_A(mu)\n                \n                if k == 0:\n                    u_rb = np.zeros(N)\n                else:\n                    A_rb = V_basis.T @ A_mu @ V_basis\n                    f_mu_true = assemble_f(mu)\n                    \n                    if strategy == 'strong':\n                        f_rb = V_basis.T @ f_mu_true\n                    else:  # weak\n                        f_deim = deim_approximator_func(f_mu_true)\n                        f_rb = V_basis.T @ f_deim\n                    \n                    y_rb = np.linalg.solve(A_rb, f_rb)\n                    u_rb = V_basis @ y_rb\n                \n                # Calculate error based on strategy\n                if strategy == 'strong':\n                    f_mu_true = assemble_f(mu)\n                    residual = A_mu @ u_rb - f_mu_true\n                    w = np.linalg.solve(A_mu, residual)\n                    error = np.sqrt(np.dot(residual, w))\n                else:  # weak\n                    f_mu_true = assemble_f(mu)\n                    f_deim = deim_approximator_func(f_mu_true)\n                    residual = A_mu @ u_rb - f_deim\n                    error = np.linalg.norm(residual) / coercivity_func(mu)\n                    \n                if error > max_error:\n                    max_error = error\n                    best_mu = mu\n            \n            best_mu_idx = np.where(s_train_params == best_mu)[0][0]\n            selected_indices.add(best_mu_idx)\n            selected_mus.append(best_mu)\n\n            # Augment basis with the new FOM snapshot\n            new_snapshot = solve_fom(best_mu)\n            new_basis_vec = gram_schmidt(new_snapshot, V_basis)\n            V_basis = np.hstack([V_basis, new_basis_vec.reshape(-1, 1)]) if V_basis.size > 0 else new_basis_vec.reshape(-1, 1)\n\n        return selected_mus\n\n    # --- 4. Main Experiment Loop ---\n    final_results = []\n    \n    # Pre-calculate coercivity constant part\n    lambda1_K = (4.0 / h**2) * (np.sin(np.pi / (2.0 * (N + 1))))**2\n    def alpha_LB(mu):\n        return 1.0 + a_func(mu) * lambda1_K\n\n    # Run strong greedy once (results are independent of m)\n    strong_mus = run_greedy_procedure(strategy='strong')\n    \n    for m in test_cases_m:\n        # Build DEIM approximator for current m\n        deim_approximator = build_deim_assets(m)\n        \n        # Run weak greedy with the specific DEIM approximator\n        weak_mus = run_greedy_procedure(strategy='weak',\n                                        deim_approximator_func=deim_approximator,\n                                        coercivity_func=alpha_LB)\n\n        # Compute divergence metrics\n        # D_ord: Ordered-position divergence\n        ord_diff_count = sum(1 for i in range(k_max) if not np.isclose(strong_mus[i], weak_mus[i]))\n        D_ord = float(ord_diff_count) / k_max\n        \n        # D_set: Set-symmetric-difference divergence\n        set_strong = set(strong_mus)\n        set_weak = set(weak_mus)\n        sym_diff_size = len(set_strong.symmetric_difference(set_weak))\n        D_set = float(sym_diff_size) / k_max\n\n        final_results.extend([D_ord, D_set])\n        \n    print(f\"[{','.join(f'{x:.8f}' for x in final_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In many scientific and engineering applications, our ultimate goal is not to approximate the entire solution field, but rather a specific output quantity, such as an average temperature or a structural stress at a critical point. This calls for a goal-oriented approach to model reduction that focuses the approximation power where it matters most. This advanced practice  challenges you to implement a sophisticated greedy selection algorithm guided by a Dual-Weighted Residual (DWR) error estimator, which specifically targets the error in a quantity of interest. You will also explore how DEIM affects both the primary (primal) problem and the auxiliary (adjoint) problem within this goal-oriented framework.",
            "id": "3438800",
            "problem": "Consider the following semilinear boundary value problem parameterized by the two-dimensional parameter $\\mu = (a,b)$:\n$$\n-u''(x;\\mu) + a\\,u(x;\\mu) + b\\,\\sin(u(x;\\mu)) = s(x;\\mu), \\quad x \\in (0,1),\n$$\nwith homogeneous Dirichlet boundary conditions $u(0;\\mu) = 0$ and $u(1;\\mu) = 0$. The right-hand side is defined by\n$$\ns(x;\\mu) = a\\,\\sin(\\pi x) + b\\,x,\n$$\nand the quantity of interest is\n$$\nJ(u) = \\int_0^1 w(x)\\,u(x)\\,dx, \\quad w(x) = x.\n$$\nAll trigonometric functions must be evaluated in radians. Discretize $(0,1)$ with a uniform grid of $N$ interior points, let the grid spacing be $h=\\frac{1}{N+1}$, and approximate $-u''$ by the standard centered finite difference, so that the discrete linear operator corresponding to $-u'' + a\\,u$ is the tridiagonal matrix with diagonal entries $\\frac{2}{h^2} + a$ and off-diagonal entries $-\\frac{1}{h^2}$. Define the discrete nonlinear residual at a state vector $\\mathbf{u} \\in \\mathbb{R}^N$ by\n$$\n\\mathbf{R}(\\mathbf{u};\\mu) = A(a)\\,\\mathbf{u} + b\\,\\sin(\\mathbf{u}) - \\mathbf{s}(\\mu),\n$$\nwhere $A(a)$ is the discrete linear operator and $\\mathbf{s}(\\mu)$ is the grid evaluation of $s(x;\\mu)$ at interior points.\n\nUse a Reduced Basis (RB) approach with a goal-oriented greedy strategy guided by a Dual-Weighted Residual (DWR). Let $\\mathbf{u}_r(\\mu) = \\Phi\\,\\mathbf{y}(\\mu)$ be the RB approximation with column basis matrix $\\Phi \\in \\mathbb{R}^{N \\times r}$ and reduced coordinates $\\mathbf{y}(\\mu)\\in\\mathbb{R}^r$, obtained by a Galerkin projection of the discrete residual to the RB space. For a given approximate state $\\mathbf{u}_r(\\mu)$, define the adjoint by the linearized discrete adjoint problem\n$$\n\\left(A(a) + b\\,\\mathrm{diag}\\left(\\cos(\\mathbf{u}_r(\\mu))\\right)\\right)\\,\\mathbf{z}(\\mu) = \\mathbf{w},\n$$\nwhere $\\mathbf{w}$ is the grid evaluation of $w(x)$ and $\\mathrm{diag}(\\cdot)$ forms a diagonal matrix from its vector argument. The DWR estimator for the quantity of interest is\n$$\n\\eta(\\mu) = \\left| h\\,\\mathbf{z}(\\mu)^\\top \\mathbf{R}(\\mathbf{u}_r(\\mu);\\mu) \\right|.\n$$\n\nDesign a goal-oriented greedy algorithm over a fixed training set $\\mathcal{T}$ of parameters that iteratively selects the parameter $\\mu^\\star \\in \\mathcal{T}$ maximizing $\\eta(\\mu)$ under the current RB space and then augments the RB basis with the truth (full-order) solution at $\\mu^\\star$ (orthonormalized against the current basis). The Reduced Basis dimension should grow until it reaches a prescribed maximum $r_{\\max}$. In addition to the truth-based greedy selection, consider two Discrete Empirical Interpolation Method (DEIM) variants to approximate the nonlinear vector field:\n- Primal-DEIM: Replace $\\sin(\\mathbf{u})$ by its DEIM approximation in the primal residual and reduced solver, while keeping the adjoint linearization exact.\n- Primal-and-Adjoint-DEIM: Replace $\\sin(\\mathbf{u})$ in the primal residual and $\\cos(\\mathbf{u})$ in the adjoint linearization by their respective DEIM approximations.\n\nFor DEIM, build the approximation subspaces from the nonlinear snapshots collected at the parameters selected by the greedy algorithm up to the current iteration, using a truncated singular value decomposition and a standard DEIM point selection (greedy, via residual maximization). If at iteration $k$ the RB dimension is $k$, use a DEIM subspace of dimension $k$.\n\nImplement the following components from first principles:\n- A full-order Newton solver for the semilinear system to compute the truth solution $\\mathbf{u}(\\mu)$ at a given parameter $\\mu$, using a backtracking line search to ensure residual decrease.\n- A reduced-order Newton solver for the RB system to compute $\\mathbf{y}(\\mu)$ and $\\mathbf{u}_r(\\mu)=\\Phi\\mathbf{y}(\\mu)$, with and without DEIM as specified.\n- The adjoint solve defined above, with and without DEIM in the adjoint linearization as specified.\n- The DEIM construction for the primal nonlinearity $\\sin(\\mathbf{u})$ and for the adjoint linearization $\\cos(\\mathbf{u})$, built from snapshots corresponding to the parameters selected by each greedy variant.\n\nTraining set, RB size, and grid:\n- Use $N=200$ interior points.\n- Use $r_{\\max}=5$.\n- Use the training set $\\mathcal{T} = \\{(a,b): a \\in \\{1.0,2.0,3.0\\},\\, b \\in \\{0.5,1.0,1.5\\}\\}$.\n\nTest suite:\n- Case $1$: $\\mu=(2.0,1.0)$ (general case).\n- Case $2$: $\\mu=(1.0,0.1)$ (weak nonlinearity edge case).\n- Case $3$: $\\mu=(3.5,1.8)$ (strong nonlinearity stress test).\n\nFor each test case, compute:\n- $\\Delta J_{\\text{primal}} = J(u_{\\text{RB,Primal-DEIM}}) - J(u_{\\text{RB,truth-greedy}})$\n- $\\Delta J_{\\text{primal+adj}} = J(u_{\\text{RB,Primal+Adjoint-DEIM}}) - J(u_{\\text{RB,truth-greedy}})$\n- $E_{\\text{RB}} = J(u_{\\text{RB,truth-greedy}}) - J(u_{\\text{truth}})$\n\nAll quantities are scalars in the unit induced by the discretization (no physical units). Angles in $\\sin(\\cdot)$ and $\\cos(\\cdot)$ must be treated as dimensionless arguments in radians. Your program should produce a single line of output containing the results as a comma-separated list of lists, one inner list per test case, each inner list ordered as $[\\Delta J_{\\text{primal}},\\Delta J_{\\text{primal+adj}},E_{\\text{RB}}]$. For example, an output with three cases must have the format\n$$\n\\big[\\,[d_{1,1},d_{1,2},e_1],\\,[d_{2,1},d_{2,2},e_2],\\,[d_{3,1},d_{3,2},e_3]\\,\\big]\n$$\nwhere each $d_{i,j}$ and $e_i$ is a real number.",
            "solution": "The user wants me to solve a parametrized semilinear boundary value problem using reduced basis methods with a goal-oriented greedy selection strategy. I must also compare this baseline against two variants that use the Discrete Empirical Interpolation Method (DEIM) to approximate the nonlinear terms. The entire process, from the full-order Newton solver to the DEIM construction and greedy algorithm, must be implemented from first principles.\n\n### Problem Validation\n\nThe problem is a well-posed, scientifically sound exercise in numerical analysis and model order reduction. All parameters, equations, and algorithmic components are clearly defined and consistent with established literature in the field.\n- **Scientific Grounding**: The problem is based on a standard semilinear elliptic PDE, finite difference discretization, Newton's method for nonlinear solves, and well-known model reduction techniques (Reduced Basis, DEIM, Dual-Weighted Residual). All mathematical formulations are correct.\n- **Well-Posedness**: For the given parameter ranges, the discrete systems are expected to have unique solutions, as the linear differential operator is strongly elliptic and the nonlinear term is bounded.\n- **Completeness and Consistency**: The problem provides all necessary data: grid size ($N$), maximum basis size ($r_{\\max}$), training set $\\mathcal{T}$, and test cases. The instructions for building the reduced models, including the three distinct greedy strategies (truth-greedy, primal-DEIM greedy, primal-and-adjoint-DEIM greedy), are detailed and allow for a unique, albeit complex, implementation.\n- **Objectivity**: The problem is stated in precise, objective mathematical language.\n\nThe problem is deemed valid.\n\n### Algorithmic Design and Principles\n\nThe solution entails three major stages: (1) implementing the high-fidelity (\"truth\") full-order model (FOM) solver, (2) implementing three distinct greedy training algorithms to construct three different reduced-order models (ROMs), and (3) evaluating these ROMs and the FOM on a given test suite to compute the required error metrics.\n\n#### 1. Full-Order Model (FOM)\n\nThe FOM is a discrete representation of the semilinear PDE obtained via a centered finite difference scheme on a uniform grid with $N$ interior points. This results in a system of $N$ nonlinear algebraic equations, $\\mathbf{R}(\\mathbf{u};\\mu) = \\mathbf{0}$, where:\n- $\\mathbf{u} \\in \\mathbb{R}^N$ is the vector of nodal values of the solution.\n- $\\mu = (a, b)$ is the parameter vector.\n- $\\mathbf{R}(\\mathbf{u};\\mu) = A(a)\\,\\mathbf{u} + b\\,\\sin(\\mathbf{u}) - \\mathbf{s}(\\mu)$ is the residual vector.\n- $A(a)$ is the $N \\times N$ system matrix from the discretization of $-u'' + a\\,u$. It is a symmetric, tridiagonal, positive-definite matrix.\n- $\\sin(\\mathbf{u})$ is applied element-wise.\n- $\\mathbf{s}(\\mu)$ is the discretized source term.\n\nThis system is solved using Newton's method. Given an iterate $\\mathbf{u}_k$, the next iterate is $\\mathbf{u}_{k+1} = \\mathbf{u}_k + \\alpha \\delta\\mathbf{u}$, where the update $\\delta\\mathbf{u}$ is found by solving the linear system:\n$$\n\\mathbf{R}'(\\mathbf{u}_k;\\mu) \\delta\\mathbf{u} = -\\mathbf{R}(\\mathbf{u}_k;\\mu)\n$$\nThe Jacobian matrix is $\\mathbf{R}'(\\mathbf{u};\\mu) = A(a) + b\\,\\mathrm{diag}(\\cos(\\mathbf{u}))$. The step size $\\alpha \\in (0,1]$ is determined by a backtracking line search to ensure a decrease in the residual norm, i.e., $\\|\\mathbf{R}(\\mathbf{u}_{k+1})\\| < \\|\\mathbf{R}(\\mathbf{u}_k)\\|$.\n\n#### 2. Reduced-Order Model (ROM) Construction\n\nThe core of the task is to build three different ROMs by running a goal-oriented greedy algorithm with $r_{\\max}$ iterations. The goal is to approximate the Quantity of Interest (QoI), $J(u) = \\int_0^1 x u(x) dx$, whose discrete form is $J_h(\\mathbf{u}) = h \\mathbf{w}^\\top \\mathbf{u}$.\n\nThe greedy algorithm iteratively builds an orthonormal basis $\\Phi \\in \\mathbb{R}^{N \\times r}$ ($r=1, \\dots, r_{\\max}$) by selecting the parameter $\\mu^\\star$ from a training set $\\mathcal{T}$ that maximizes an error estimator, and then adding the corresponding FOM solution $\\mathbf{u}(\\mu^\\star)$ to the basis via Modified Gram-Schmidt orthonormalization.\n\nThe error estimator is the Dual-Weighted Residual (DWR): $\\eta(\\mu) = |h\\,\\mathbf{z}(\\mu)^\\top \\mathbf{R}(\\mathbf{u}_r(\\mu);\\mu)|$. Its computation requires:\n1.  **Solving the ROM**: The RB approximation is $\\mathbf{u}_r(\\mu) = \\Phi \\mathbf{y}(\\mu)$. The reduced coordinates $\\mathbf{y}(\\mu) \\in \\mathbb{R}^r$ are found by solving the projected nonlinear system (Galerkin projection): $\\Phi^\\top \\mathbf{R}(\\Phi \\mathbf{y}(\\mu);\\mu) = \\mathbf{0}$. This is a small $r \\times r$ system solved via Newton's method.\n2.  **Solving the Adjoint Problem**: The discrete adjoint vector $\\mathbf{z}(\\mu) \\in \\mathbb{R}^N$ is the solution to the linear system $(\\mathbf{R}'(\\mathbf{u}_r(\\mu);\\mu))^\\top \\mathbf{z}(\\mu) = \\mathbf{w}$. Since the Jacobian is symmetric, this is $(\\mathbf{R}'(\\mathbf{u}_r(\\mu);\\mu)) \\mathbf{z}(\\mu) = \\mathbf{w}$.\n3.  **Evaluating the Estimator**: Compute the full residual $\\mathbf{R}$ with the RB solution $\\mathbf{u}_r$ and take its weighted inner product with the adjoint solution $\\mathbf{z}$.\n\nThe three variants of the greedy algorithm differ in how they compute the DWR estimator:\n- **Truth-Greedy (TG)**: All steps are performed exactly as described above. This forms the baseline model $\\mathcal{M}_{TG} = \\{\\Phi_{TG}\\}$.\n- **Primal-DEIM Greedy (PD)**: The nonlinear term $\\sin(\\mathbf{u}_r)$ in the ROM solver is approximated by DEIM. The adjoint problem and residual evaluation remain exact. This produces model $\\mathcal{M}_{PD} = \\{\\Phi_{PD}, \\text{DEIM}_{\\sin}\\}$.\n- **Primal-and-Adjoint-DEIM Greedy (PA)**: The ROM solver uses DEIM for $\\sin(\\mathbf{u}_r)$, and the adjoint solver uses DEIM to approximate the vector $\\cos(\\mathbf{u}_r)$ when constructing the adjoint operator. This yields model $\\mathcal{M}_{PA} = \\{\\Phi_{PA}, \\text{DEIM}_{\\sin}, \\text{DEIM}_{\\cos}\\}$.\n\nFor the DEIM variants, at each greedy iteration $k$, the DEIM basis (of dimension $k$) is constructed from the set of nonlinear snapshots ($\\sin(\\mathbf{u})$ and/or $\\cos(\\mathbf{u})$) collected at the $k$ parameters chosen so far. DEIM construction itself involves a Singular Value Decomposition (SVD) of the snapshot matrix to find an empirical basis $U$, followed by a greedy algorithm to select $k$ optimal interpolation indices $\\mathbf{p}$. The DEIM approximation of a vector $\\mathbf{f}$ is then $\\hat{\\mathbf{f}} = U(U[\\mathbf{p},:])^{-1}\\mathbf{f}[\\mathbf{p}]$.\n\n#### 3. Testing and Evaluation\n\nAfter the three training procedures are complete, we have three distinct ROMs. For each test parameter $\\mu_{test}$:\n1.  The FOM is solved to get the truth solution $\\mathbf{u}_{truth}$ and QoI $J_{truth}$.\n2.  The TG-ROM is solved to get $\\mathbf{u}_{RB,TG}$ and $J_{RB,TG}$.\n3.  The PD-ROM (using its specific basis $\\Phi_{PD}$ and DEIM operator for $\\sin$) is solved to get $\\mathbf{u}_{RB,PD}$ and $J_{RB,PD}$.\n4.  The PA-ROM (using its basis $\\Phi_{PA}$ and DEIM operators) is solved to get $\\mathbf{u}_{RB,PA}$ and $J_{RB,PA}$.\n\nFrom these QoI values, the required metrics ($\\Delta J_{\\text{primal}}$, $\\Delta J_{\\text{primal+adj}}$, $E_{\\text{RB}}$) are calculated as specified in the problem statement. This provides a quantitative comparison of the accuracy of the different ROMs.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants for solvers\nNEWTON_TOL = 1e-10\nNEWTON_MAX_ITER = 30\nLS_BETA = 0.5  # Backtracking line search factor\n\ndef build_deim_approximation(snapshots, k):\n    \"\"\"\n    Builds the DEIM approximation for a given set of snapshots.\n    \n    Args:\n        snapshots (np.ndarray): Matrix of snapshot vectors (N x num_snapshots).\n        k (int): Dimension of the DEIM subspace to build.\n\n    Returns:\n        tuple: A tuple (U, P_inv_U_p, p_indices) containing the DEIM data.\n    \"\"\"\n    if snapshots.shape[1] < k:\n        k = snapshots.shape[1]\n\n    try:\n        U, _, _ = np.linalg.svd(snapshots, full_matrices=False)\n    except np.linalg.LinAlgError:\n         return None, None, None\n\n    U = U[:, :k]\n    \n    p_indices_list = [np.argmax(np.abs(U[:, 0]))]\n    \n    for j in range(1, k):\n        # Solve for coefficients to form the residual\n        c = np.linalg.solve(U[p_indices_list, :j], U[p_indices_list, j])\n        res = U[:, j] - U[:, :j] @ c\n        p_indices_list.append(np.argmax(np.abs(res)))\n    \n    p_indices = np.array(p_indices_list)\n    p_indices.sort()\n    \n    U_p = U[p_indices, :]\n    try:\n        P_inv_U_p = np.linalg.inv(U_p)\n    except np.linalg.LinAlgError:\n        P_inv_U_p = np.linalg.pinv(U_p)\n\n    return U, P_inv_U_p, p_indices\n\n\nclass SemilinearRBProblem:\n    \"\"\"\n    Encapsulates the logic for the semilinear RB problem, including FOM/ROM solvers\n    and greedy training algorithms.\n    \"\"\"\n    def __init__(self, N, r_max, training_set, test_cases):\n        self.N = N\n        self.r_max = r_max\n        self.training_set = training_set\n        self.test_cases = test_cases\n\n        self.h = 1.0 / (N + 1)\n        self.x_grid = np.linspace(self.h, 1.0 - self.h, N)\n        self.w_vec = self.x_grid.copy()\n\n        diag = np.full(N, 2.0 / self.h**2)\n        offdiag = np.full(N - 1, -1.0 / self.h**2)\n        self.laplacian = np.diag(diag) + np.diag(offdiag, k=1) + np.diag(offdiag, k=-1)\n\n    def _get_A(self, a):\n        return self.laplacian + a * np.identity(self.N)\n\n    def _get_s(self, mu):\n        a, b = mu\n        return a * np.sin(np.pi * self.x_grid) + b * self.x_grid\n\n    def _compute_full_residual(self, u, mu):\n        a, b = mu\n        return self._get_A(a) @ u + b * np.sin(u) - self._get_s(mu)\n\n    def solve_fom(self, mu, u_guess=None):\n        u = np.zeros(self.N) if u_guess is None else u_guess.copy()\n        a, b = mu\n        A = self._get_A(a)\n        for _ in range(NEWTON_MAX_ITER):\n            res = self._compute_full_residual(u, mu)\n            res_norm = np.linalg.norm(res)\n            if res_norm < NEWTON_TOL:\n                break\n            jac = A + b * np.diag(np.cos(u))\n            delta_u = np.linalg.solve(jac, -res)\n            alpha = 1.0\n            u_new = u + alpha * delta_u\n            while np.linalg.norm(self._compute_full_residual(u_new, mu)) >= res_norm:\n                alpha *= LS_BETA\n                if alpha < 1e-8: break\n                u_new = u + alpha * delta_u\n            u = u_new\n        return u\n\n    def compute_qoi(self, u):\n        return self.h * np.dot(self.w_vec, u)\n\n    def _deim_approx(self, vec, deim_data):\n        U, P_inv_U_p, p_indices = deim_data\n        return U @ (P_inv_U_p @ vec[p_indices])\n\n    def solve_rom(self, mu, Phi, deim_sin_data=None):\n        r = Phi.shape[1]\n        y = np.zeros(r)\n        a, b = mu\n        A_r = Phi.T @ self._get_A(a) @ Phi\n        s_r = Phi.T @ self._get_s(mu)\n        for _ in range(NEWTON_MAX_ITER):\n            u_r = Phi @ y\n            if deim_sin_data:\n                U_sin, P_inv_sin, p_sin = deim_sin_data\n                F_r = Phi.T @ (U_sin @ (P_inv_sin @ np.sin(u_r[p_sin])))\n            else:\n                F_r = Phi.T @ np.sin(u_r)\n            res_r = A_r @ y + b * F_r - s_r\n            if np.linalg.norm(res_r) < NEWTON_TOL:\n                break\n            if deim_sin_data:\n                U_sin, P_inv_sin, p_sin = deim_sin_data\n                jac_nl_r = Phi.T @ (U_sin @ (P_inv_sin @ (np.diag(np.cos(u_r[p_sin])) @ Phi[p_sin, :])))\n            else:\n                jac_nl_r = Phi.T @ (np.diag(np.cos(u_r)) @ Phi)\n            jac_r = A_r + b * jac_nl_r\n            try:\n                delta_y = np.linalg.solve(jac_r, -res_r)\n            except np.linalg.LinAlgError:\n                delta_y = np.linalg.pinv(jac_r) @ -res_r\n            y += delta_y\n        return y\n\n    def solve_adjoint(self, mu, u_r, deim_cos_data=None):\n        a, b = mu\n        A = self._get_A(a)\n        if deim_cos_data:\n            cos_u_r = np.cos(u_r)\n            cos_u_r_approx = self._deim_approx(cos_u_r, deim_cos_data)\n            adj_jac = A + b * np.diag(cos_u_r_approx)\n        else:\n            adj_jac = A + b * np.diag(np.cos(u_r))\n        return np.linalg.solve(adj_jac, self.w_vec)\n    \n    def run_greedy_training(self, variant):\n        Phi = np.zeros((self.N, self.r_max))\n        snapshots_sin, snapshots_cos = [], []\n        \n        mu_1 = self.training_set[0]\n        u_1 = self.solve_fom(mu_1)\n        Phi[:, 0] = u_1 / np.linalg.norm(u_1)\n\n        if variant in ['PD', 'PA']: snapshots_sin.append(np.sin(u_1))\n        if variant == 'PA': snapshots_cos.append(np.cos(u_1))\n\n        for k in range(1, self.r_max):\n            deim_sin, deim_cos = None, None\n            if variant in ['PD', 'PA'] and snapshots_sin:\n                deim_sin = build_deim_approximation(np.array(snapshots_sin).T, k)\n            if variant == 'PA' and snapshots_cos:\n                deim_cos = build_deim_approximation(np.array(snapshots_cos).T, k)\n\n            best_mu, max_eta = None, -1.0\n            current_Phi = Phi[:, :k]\n            for mu in self.training_set:\n                try:\n                    y = self.solve_rom(mu, current_Phi, deim_sin if variant in ['PD', 'PA'] else None)\n                    u_r = current_Phi @ y\n                    z = self.solve_adjoint(mu, u_r, deim_cos if variant == 'PA' else None)\n                    res_full = self._compute_full_residual(u_r, mu)\n                    eta = np.abs(self.h * np.dot(z, res_full))\n                    if eta > max_eta:\n                        max_eta, best_mu = eta, mu\n                except np.linalg.LinAlgError:\n                    continue # Skip parameters that cause singular systems\n            \n            if best_mu is None: best_mu = self.training_set[0]\n            \n            u_new = self.solve_fom(best_mu)\n            v = u_new.copy()\n            for i in range(k):\n                v -= np.dot(Phi[:, i], v) * Phi[:, i]\n            Phi[:, k] = v / np.linalg.norm(v)\n\n            if variant in ['PD', 'PA']: snapshots_sin.append(np.sin(u_new))\n            if variant == 'PA': snapshots_cos.append(np.cos(u_new))\n\n        final_deim_sin = build_deim_approximation(np.array(snapshots_sin).T, self.r_max) if variant in ['PD', 'PA'] else None\n        final_deim_cos = build_deim_approximation(np.array(snapshots_cos).T, self.r_max) if variant == 'PA' else None\n        \n        return {'Phi': Phi, 'deim_sin': final_deim_sin, 'deim_cos': final_deim_cos}\n\n    def run_all(self):\n        model_tg = self.run_greedy_training('TG')\n        model_pd = self.run_greedy_training('PD')\n        model_pa = self.run_greedy_training('PA')\n        \n        all_results = []\n        for mu_test in self.test_cases:\n            u_truth = self.solve_fom(mu_test)\n            J_truth = self.compute_qoi(u_truth)\n            \n            y_tg = self.solve_rom(mu_test, model_tg['Phi'])\n            u_rb_tg = model_tg['Phi'] @ y_tg\n            J_rb_tg = self.compute_qoi(u_rb_tg)\n\n            y_pd = self.solve_rom(mu_test, model_pd['Phi'], model_pd['deim_sin'])\n            u_rb_pd = model_pd['Phi'] @ y_pd\n            J_rb_pd = self.compute_qoi(u_rb_pd)\n            \n            y_pa = self.solve_rom(mu_test, model_pa['Phi'], model_pa['deim_sin'])\n            u_rb_pa = model_pa['Phi'] @ y_pa\n            J_rb_pa = self.compute_qoi(u_rb_pa)\n            \n            all_results.append([J_rb_pd - J_rb_tg, J_rb_pa - J_rb_tg, J_rb_tg - J_truth])\n            \n        return all_results\n\ndef solve():\n    \"\"\"\n    Main function to set up and run the problem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    N = 200\n    r_max = 5\n    training_set = [(a, b) for a in [1.0, 2.0, 3.0] for b in [0.5, 1.0, 1.5]]\n    test_cases = [(2.0, 1.0), (1.0, 0.1), (3.5, 1.8)]\n    \n    problem = SemilinearRBProblem(N, r_max, training_set, test_cases)\n    results = problem.run_all()\n\n    # Final print statement in the exact required format.\n    output_str = \"[\"\n    for i, case_res in enumerate(results):\n        # Format numbers to avoid excessive precision.\n        formatted_res = [f\"{x:.12e}\" for x in case_res]\n        output_str += f\"[{','.join(formatted_res)}]\"\n        if i < len(results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        }
    ]
}