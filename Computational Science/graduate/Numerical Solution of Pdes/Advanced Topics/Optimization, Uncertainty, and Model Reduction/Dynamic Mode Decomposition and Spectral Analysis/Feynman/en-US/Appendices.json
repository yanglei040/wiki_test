{
    "hands_on_practices": [
        {
            "introduction": "Dynamic Mode Decomposition (DMD) excels at extracting coherent structures from complex data by approximating the underlying linear dynamics. This first exercise provides a foundational, hands-on implementation of DMD on data generated from known linear partial differential equations. By testing the algorithm on this \"perfect\" synthetic data, you will not only verify its ability to accurately recover the system's true spectral properties but also investigate a crucial practical step: the impact of data preprocessing, revealing both the algorithm's invariance to scaling and a common pitfall associated with normalization .",
            "id": "3383195",
            "problem": "You are asked to implement a program that constructs a suite of linear, one-dimensional, periodic, constant-coefficient partial differential equations (PDEs) whose Koopman eigenfunctions are analytically known, generates exact snapshots from their semigroup evolution, applies Dynamic Mode Decomposition (DMD), and quantitatively tests the impact of snapshot normalization and uniform scaling on the recovered spectra. All variables are non-dimensional and unitless.\n\nFundamental base and setting: Consider linear PDEs on a periodic spatial domain $x \\in [0,2\\pi)$ with spatial Fourier eigenfunctions $\\varphi_k(x) = e^{i k x}$ for $k \\in \\mathbb{Z}$. For linear, constant-coefficient PDEs, the evolution operator $e^{t \\mathcal{L}}$ is diagonal on the Fourier basis with exact semigroup evolution\n$$\nu(x,t) \\;=\\; \\sum_{j=1}^{r} a_j \\, e^{\\lambda_{k_j} t} \\, e^{i k_j x},\n$$\nwhere $a_j \\in \\mathbb{C}$ are initial amplitudes, $k_j$ are selected spatial wavenumbers, and $\\lambda_{k_j}$ are the Koopman generator eigenvalues (temporal exponents). For the three PDE families used here:\n- Diffusion: $u_t = \\nu u_{xx}$, the eigenvalues are $\\lambda_k = -\\nu k^2$.\n- Advection: $u_t + c u_x = 0$, the eigenvalues are $\\lambda_k = - i c k$.\n- Reaction–diffusion: $u_t = \\alpha u + \\nu u_{xx}$, the eigenvalues are $\\lambda_k = \\alpha - \\nu k^2$.\n\nDynamic Mode Decomposition (DMD): Given snapshots $\\{u(\\cdot,t_n)\\}_{n=0}^{m-1}$ at uniform sampling interval $\\Delta t$, define data matrices $X = [u(\\cdot,t_0), \\dots, u(\\cdot,t_{m-2})]$ and $X' = [u(\\cdot,t_1), \\dots, u(\\cdot,t_{m-1})]$. For a linear evolution $u_{n+1} = A \\, u_n$ with $A = e^{\\mathcal{L}\\Delta t}$, exact data satisfy $X' = A X$. DMD computes a low-rank approximation of $A$ and its eigenvalues $\\{\\mu_j\\}$; the continuous-time exponents are then recovered by $\\lambda_j^{(\\mathrm{DMD})} = \\frac{1}{\\Delta t} \\log \\mu_j$, using the principal branch of the complex logarithm (radians).\n\nPreprocessing variations: You must implement three snapshot preprocessing choices before assembling $X$ and $X'$:\n1. Raw snapshots: use $u(\\cdot,t_n)$ as generated.\n2. Uniform scaling: multiply all snapshots by the same constant $s \\in \\mathbb{R}$.\n3. Column normalization: rescale each snapshot $u(\\cdot,t_n)$ by its spatial $\\ell^2$ norm to have unit norm. Note that this rescaling generally destroys the existence of a single linear map $A$ relating consecutive snapshots unless the norms are constant across time.\n\nTask: Implement a program that, for each specified test case, constructs exact snapshots by spectral synthesis on a uniform spatial grid, applies the chosen preprocessing, runs DMD with a specified rank $r$, converts discrete-time DMD eigenvalues to continuous-time exponents, and compares the recovered set $\\{\\lambda_j^{(\\mathrm{DMD})}\\}_{j=1}^r$ to the analytic set $\\{\\lambda_{k_j}\\}_{j=1}^r$ by optimal matching (permutation) that minimizes the average absolute complex discrepancy. A test passes if the mean absolute error is less than or equal to a specified tolerance $\\varepsilon$.\n\nAngle unit: When any angles arise from complex phases, all angles are in radians.\n\nFinal output format: Your program should produce a single line of output containing the boolean results for all test cases as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True]\"). No other text should be printed.\n\nTest suite: Use the following six test cases. In each case, the spatial grid has $N_x = 128$ points uniformly over $[0,2\\pi)$, all quantities are unitless, and the DMD rank is $r$ equal to the number of excited modes.\n\n- Test 1 (Diffusion, raw):\n  - PDE: $u_t = \\nu u_{xx}$ with $\\nu = 0.1$.\n  - Modes $k = [1, 3]$, amplitudes $a = [1.0, 0.3]$.\n  - Time step $\\Delta t = 0.01$, number of snapshots $m = 60$.\n  - Preprocessing: raw. Tolerance $\\varepsilon = 10^{-6}$.\n\n- Test 2 (Diffusion, uniform scaling invariance):\n  - Same as Test 1 but multiply all snapshots by $s = 5.0$ before forming $X$ and $X'$.\n  - Preprocessing: uniform scaling. Tolerance $\\varepsilon = 10^{-6}$.\n\n- Test 3 (Diffusion, column normalization failure on single decaying mode):\n  - PDE: $u_t = \\nu u_{xx}$ with $\\nu = 0.1$.\n  - Modes $k = [2]$, amplitudes $a = [1.0]$.\n  - Time step $\\Delta t = 0.01$, number of snapshots $m = 60$.\n  - Preprocessing: column normalization to unit $\\ell^2$ norm per snapshot. Tolerance $\\varepsilon = 10^{-3}$.\n\n- Test 4 (Advection, raw):\n  - PDE: $u_t + c u_x = 0$ with $c = 2.0$.\n  - Modes $k = [3]$, amplitudes $a = [0.8]$.\n  - Time step $\\Delta t = 0.02$, number of snapshots $m = 80$.\n  - Preprocessing: raw. Tolerance $\\varepsilon = 10^{-6}$.\n\n- Test 5 (Reaction–diffusion, raw):\n  - PDE: $u_t = \\alpha u + \\nu u_{xx}$ with $\\alpha = 0.05$, $\\nu = 0.02$.\n  - Modes $k = [1, 4]$, amplitudes $a = [1.0, 0.2]$.\n  - Time step $\\Delta t = 0.01$, number of snapshots $m = 60$.\n  - Preprocessing: raw. Tolerance $\\varepsilon = 10^{-6}$.\n\n- Test 6 (Reaction–diffusion, column normalization impact on mixed growth/decay):\n  - Same parameters as Test 5.\n  - Preprocessing: column normalization to unit $\\ell^2$ norm per snapshot. Tolerance $\\varepsilon = 10^{-3}$.\n\nRequired output: Your program should run all six tests in the order listed and print exactly one line with a Python-style list of six booleans indicating whether the DMD-recovered continuous-time exponents match the analytic eigenvalues within tolerance, after optimal matching of eigenvalues:\n- The line must have the form \"[b1,b2,b3,b4,b5,b6]\" where each bi is either True or False.",
            "solution": "The problem requires the implementation and verification of the Dynamic Mode Decomposition (DMD) algorithm on synthetically generated data from linear partial differential equations (PDEs). The core of the task is to assess the impact of different data preprocessing steps—specifically, no preprocessing (raw), uniform scaling, and per-snapshot normalization—on the accuracy of the DMD-recovered spectrum.\n\nFirst, we establish the analytical foundation for the data generation. The problem considers three families of one-dimensional, linear, constant-coefficient PDEs on a periodic domain $x \\in [0, 2\\pi)$:\n1.  Diffusion equation: $u_t = \\nu u_{xx}$\n2.  Advection equation: $u_t = -c u_x$\n3.  Reaction-diffusion equation: $u_t = \\alpha u + \\nu u_{xx}$\n\nFor such PDEs, the spatial Fourier modes $\\varphi_k(x) = e^{i k x}$ with wavenumber $k \\in \\mathbb{Z}$ are eigenfunctions of the spatial differential operator $\\mathcal{L}$. Applying $\\mathcal{L}$ to $\\varphi_k(x)$ yields $\\mathcal{L} e^{i k x} = \\lambda_k e^{i k x}$, where $\\lambda_k$ is the corresponding eigenvalue of the generator $\\mathcal{L}$. The evolution of an initial condition is governed by the semigroup $e^{t\\mathcal{L}}$. For the given PDE families, the eigenvalues $\\lambda_k$ are:\n-   Diffusion: Substituting $u = e^{i k x}$ into $u_t = \\nu u_{xx}$ leads to $\\lambda_k u = \\nu (i k)^2 u$, so $\\lambda_k = -\\nu k^2$.\n-   Advection: Substituting into $u_t = -c u_x$ gives $\\lambda_k u = -c(i k) u$, so $\\lambda_k = -i c k$.\n-   Reaction-diffusion: Substituting into $u_t = \\alpha u + \\nu u_{xx}$ yields $\\lambda_k u = \\alpha u + \\nu(i k)^2 u$, so $\\lambda_k = \\alpha - \\nu k^2$.\n\nThe solution to the PDE with an initial condition expressed as a superposition of a finite number of modes, $u(x,0) = \\sum_{j=1}^{r} a_j e^{i k_j x}$, is given by the exact formula:\n$$\nu(x,t) = \\sum_{j=1}^{r} a_j e^{\\lambda_{k_j} t} e^{i k_j x}\n$$\nThis formula is used to generate a sequence of \"snapshots\" of the system state, $\\{u(\\cdot, t_n)\\}_{n=0}^{m-1}$, at discrete times $t_n = n \\Delta t$. These snapshots form the input to the DMD algorithm.\n\nThe DMD algorithm aims to find a linear operator $A$ that best approximates the evolution between snapshots, i.e., $u_{n+1} \\approx A u_n$, where $u_n$ is the state vector at time $t_n$. The data is arranged into two matrices, $X = [u_0, u_1, \\dots, u_{m-2}]$ and $X' = [u_1, u_2, \\dots, u_{m-1}]$. The governing relationship is $X' \\approx A X$. DMD computes a low-rank approximation of $A$. The standard algorithm proceeds as follows:\n1.  Compute the Singular Value Decomposition (SVD) of the matrix $X$, truncated to a specified rank $r$: $X \\approx U_r \\Sigma_r V_r^*$, where $U_r \\in \\mathbb{C}^{N_x \\times r}$, $\\Sigma_r \\in \\mathbb{R}^{r \\times r}$ is a diagonal matrix of singular values, and $V_r \\in \\mathbb{C}^{(m-1) \\times r}$. The columns of $U_r$ are the Proper Orthogonal Decomposition (POD) modes.\n2.  The low-rank approximation of the operator $A$ is constructed in the basis of POD modes. The reduced operator $\\tilde{A} \\in \\mathbb{C}^{r \\times r}$ is computed as $\\tilde{A} = U_r^* X' V_r \\Sigma_r^{-1}$.\n3.  The eigenvalues $\\{\\mu_j\\}_{j=1}^r$ of $\\tilde{A}$ approximate the eigenvalues of the full operator $A$. These are the discrete-time DMD eigenvalues.\n4.  The continuous-time eigenvalues (DMD exponents), which correspond to the PDE's generator eigenvalues $\\{\\lambda_k\\}$, are recovered using the relationship $\\mu_j = e^{\\lambda_j^{(\\mathrm{DMD})} \\Delta t}$. This gives $\\lambda_j^{(\\mathrm{DMD})} = \\frac{1}{\\Delta t} \\log(\\mu_j)$, where the principal branch of the complex logarithm is used.\n\nThe problem investigates three preprocessing scenarios:\n1.  **Raw snapshots**: $X$ and $X'$ are constructed directly from the generated $u(\\cdot, t_n)$. Since the underlying dynamics are perfectly linear, DMD is expected to recover the analytic eigenvalues $\\{\\lambda_{k_j}\\}$ with high accuracy, limited only by floating-point precision.\n2.  **Uniform scaling**: All snapshots are multiplied by a constant $s \\in \\mathbb{R}$. The new data matrices are $sX$ and $sX'$. The operator equation becomes $sX' = A(sX)$, which simplifies to $X' = AX$. The underlying operator $A$ is unchanged. Therefore, DMD should be invariant to this transformation and produce the same eigenvalues as the raw case.\n3.  **Column normalization**: Each snapshot $u_n$ is normalized by its spatial $\\ell^2$ norm, $v_n = u_n / \\|u_n\\|_2$. The evolution from $v_n$ to $v_{n+1}$ is given by $v_{n+1} = \\frac{u_{n+1}}{\\|u_{n+1}\\|_2} = \\frac{A u_n}{\\|A u_n\\|_2} = \\frac{\\|u_n\\|_2}{\\|A u_n\\|_2} A v_n$. This map $v_n \\mapsto v_{n+1}$ is linear if and only if the scaling factor $\\frac{\\|u_n\\|_2}{\\|A u_n\\|_2}$ is constant for all $n$. This occurs if all active modes share the same growth/decay rate. If modes have different real parts of their eigenvalues (e.g., a mix of growing and decaying modes), this condition is violated. The underlying dynamics of the normalized data are no longer described by a single time-invariant linear operator, and DMD is expected to produce inaccurate eigenvalues. In the special case of a single mode, $u_n = c e^{\\lambda t_n} \\phi_k$, the norm is $\\|u_n\\|_2 \\propto e^{\\text{Re}(\\lambda) t_n}$. Normalization removes all amplitude information, replacing it with a constant. The resulting DMD eigenvalue will have a zero real part, failing to capture any growth or decay.\n\nFinally, to compare the set of $r$ recovered eigenvalues $\\{\\lambda_j^{(\\mathrm{DMD})}\\}$ with the $r$ analytical eigenvalues $\\{\\lambda_{k_j}\\}$, we must find the optimal pairing. This is an assignment problem. We compute a cost matrix $C$ where $C_{ij} = |\\lambda_{k_i} - \\lambda_j^{(\\mathrm{DMD})}|$. We then find the permutation $\\pi$ of the indices $\\{1, \\dots, r\\}$ that minimizes the total discrepancy, $\\sum_{j=1}^r |\\lambda_{k_j} - \\lambda_{\\pi(j)}^{(\\mathrm{DMD})}|$. The mean absolute error is this minimum sum divided by $r$. This error is then compared against the specified tolerance $\\varepsilon$ to determine if a test passes.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main function to run the suite of DMD tests.\n    \"\"\"\n    test_cases = [\n        # Test 1: Diffusion, raw\n        {'pde': 'diffusion', 'params': {'nu': 0.1},\n         'modes': {'k': [1, 3], 'a': [1.0, 0.3]},\n         'time': {'dt': 0.01, 'm': 60},\n         'preprocess': {'type': 'raw'},\n         'tolerance': 1e-6},\n        # Test 2: Diffusion, uniform scaling\n        {'pde': 'diffusion', 'params': {'nu': 0.1},\n         'modes': {'k': [1, 3], 'a': [1.0, 0.3]},\n         'time': {'dt': 0.01, 'm': 60},\n         'preprocess': {'type': 'uniform_scaling', 's': 5.0},\n         'tolerance': 1e-6},\n        # Test 3: Diffusion, column normalization\n        {'pde': 'diffusion', 'params': {'nu': 0.1},\n         'modes': {'k': [2], 'a': [1.0]},\n         'time': {'dt': 0.01, 'm': 60},\n         'preprocess': {'type': 'column_norm'},\n         'tolerance': 1e-3},\n        # Test 4: Advection, raw\n        {'pde': 'advection', 'params': {'c': 2.0},\n         'modes': {'k': [3], 'a': [0.8]},\n         'time': {'dt': 0.02, 'm': 80},\n         'preprocess': {'type': 'raw'},\n         'tolerance': 1e-6},\n        # Test 5: Reaction-diffusion, raw\n        {'pde': 'reaction_diffusion', 'params': {'alpha': 0.05, 'nu': 0.02},\n         'modes': {'k': [1, 4], 'a': [1.0, 0.2]},\n         'time': {'dt': 0.01, 'm': 60},\n         'preprocess': {'type': 'raw'},\n         'tolerance': 1e-6},\n        # Test 6: Reaction-diffusion, column normalization\n        {'pde': 'reaction_diffusion', 'params': {'alpha': 0.05, 'nu': 0.02},\n         'modes': {'k': [1, 4], 'a': [1.0, 0.2]},\n         'time': {'dt': 0.01, 'm': 60},\n         'preprocess': {'type': 'column_norm'},\n         'tolerance': 1e-3},\n    ]\n\n    results = []\n    Nx = 128\n    x = np.linspace(0, 2 * np.pi, Nx, endpoint=False)\n\n    for case in test_cases:\n        # 1. Get parameters and compute analytic eigenvalues\n        pde_type = case['pde']\n        params = case['params']\n        k_values = np.array(case['modes']['k'])\n        a_values = np.array(case['modes']['a'])\n        dt = case['time']['dt']\n        m = case['time']['m']\n        r = len(k_values)\n        \n        analytic_lambdas = []\n        if pde_type == 'diffusion':\n            nu = params['nu']\n            analytic_lambdas = -nu * k_values**2\n        elif pde_type == 'advection':\n            c = params['c']\n            analytic_lambdas = -1j * c * k_values\n        elif pde_type == 'reaction_diffusion':\n            alpha = params['alpha']\n            nu = params['nu']\n            analytic_lambdas = alpha - nu * k_values**2\n        \n        # 2. Generate snapshots\n        times = np.arange(m) * dt\n        snapshots = np.zeros((Nx, m), dtype=complex)\n        for i in range(r):\n            k = k_values[i]\n            a = a_values[i]\n            lambda_k = analytic_lambdas[i]\n            # Evolve each mode and add to total solution\n            temporal_evolution = np.exp(lambda_k * times)\n            spatial_mode = np.exp(1j * k * x)\n            snapshots += a * np.outer(spatial_mode, temporal_evolution)\n\n        # 3. Preprocess snapshots\n        preprocess_info = case['preprocess']\n        if preprocess_info['type'] == 'uniform_scaling':\n            snapshots *= preprocess_info['s']\n        elif preprocess_info['type'] == 'column_norm':\n            norms = np.linalg.norm(snapshots, axis=0)\n            # Avoid division by zero for null snapshots, though not expected here\n            non_zero_norms = norms > 1e-12\n            snapshots[:, non_zero_norms] /= norms[non_zero_norms]\n\n        # 4. Perform DMD\n        X = snapshots[:, :-1]\n        X_prime = snapshots[:, 1:]\n        \n        # SVD of X, truncated to rank r\n        U, S, Vh = np.linalg.svd(X, full_matrices=False)\n        Ur = U[:, :r]\n        Sr = S[:r]\n        Vr = Vh[:r, :].conj().T\n        \n        # Build reduced operator A_tilde\n        A_tilde = Ur.conj().T @ X_prime @ Vr @ np.diag(1 / Sr)\n        \n        # Eigenvalues of A_tilde (discrete-time)\n        mu_dmd = np.linalg.eigvals(A_tilde)\n        \n        # Convert to continuous-time eigenvalues\n        lambda_dmd = np.log(mu_dmd) / dt\n\n        # 5. Compare with analytic eigenvalues\n        # Create a cost matrix for the assignment problem\n        cost_matrix = np.abs(np.subtract.outer(analytic_lambdas, lambda_dmd))\n        \n        # Find optimal matching to minimize total error\n        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n        \n        # Calculate mean absolute error of the optimal matching\n        min_error_sum = cost_matrix[row_ind, col_ind].sum()\n        mean_abs_error = min_error_sum / r\n        \n        # 6. Check against tolerance\n        results.append(mean_abs_error <= case['tolerance'])\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A core principle of DMD is the reduction of a high-dimensional system to a low-rank model, which inevitably introduces an approximation error. This theoretical exercise moves from implementation to analysis, tasking you with quantifying this error precisely. By deriving a closed-form expression for the reconstruction residual, you will gain a deeper mathematical understanding of how the choice of truncation rank $r$ directly relates to the energy contained in the discarded singular values of the data .",
            "id": "3383131",
            "problem": "Consider a sequence of noise-free snapshots from a spatially discretized, conservative linear partial differential equation with periodic boundary conditions, advanced by one constant time step via a unitary time-step operator $A_{\\mathrm{true}} \\in \\mathbb{C}^{n \\times n}$ so that $X' = A_{\\mathrm{true}} X$, where $X, X' \\in \\mathbb{C}^{n \\times m}$ collect $m$ consecutive state vectors as columns. Let the Singular Value Decomposition (SVD) of $X$ be $X = U \\Sigma V^{*}$, where $U \\in \\mathbb{C}^{n \\times k}$, $\\Sigma = \\operatorname{diag}(\\sigma_{1},\\dots,\\sigma_{k}) \\in \\mathbb{R}^{k \\times k}$, $V \\in \\mathbb{C}^{m \\times k}$ with $k = \\operatorname{rank}(X)$, and $\\sigma_{1} \\ge \\cdots \\ge \\sigma_{k} > 0$. Suppose the singular values decay geometrically according to $\\sigma_{i} = \\sigma_{1} \\gamma^{\\,i-1}$ for $i = 1,\\dots,k$, with a fixed ratio $\\gamma \\in (0,1)$.\n\nDefine the rank-$r$ Dynamic Mode Decomposition (DMD) approximation $A_{r}$ via truncation of the SVD to the leading $r$ modes, $1 \\le r \\le k$. The DMD approximation $A_{r}$ is obtained by minimizing the Frobenius-norm residual over all $A \\in \\mathbb{C}^{n \\times n}$ acting on the retained $r$-dimensional subspace extracted from $X$. Starting from the foundational definitions of the Frobenius norm, the Moore–Penrose pseudoinverse, orthogonal projectors, and the SVD, derive a closed-form expression for the residual\n$$\n\\|X' - A_{r} X\\|_{F}\n$$\nas an explicit function of $\\sigma_{1}$, $\\gamma$, $r$, and $k$. Assume only the given structure, including the unitary property of $A_{\\mathrm{true}}$ and the stated geometric decay of the singular values of $X$. Your final answer must be a single closed-form analytic expression in terms of $\\sigma_{1}$, $\\gamma$, $r$, and $k$.",
            "solution": "We begin from the basic setup. The snapshot matrices $X, X' \\in \\mathbb{C}^{n \\times m}$ satisfy the exact linear relation\n$$\nX' = A_{\\mathrm{true}} X,\n$$\nwith $A_{\\mathrm{true}}$ unitary, so that for any vector $y \\in \\mathbb{C}^{n}$,\n$$\n\\|A_{\\mathrm{true}} y\\|_{2} = \\|y\\|_{2}.\n$$\nLet the rank-$k$ Singular Value Decomposition (SVD) of $X$ be\n$$\nX = U \\Sigma V^{*},\n$$\nwhere $U \\in \\mathbb{C}^{n \\times k}$ and $V \\in \\mathbb{C}^{m \\times k}$ have orthonormal columns, and $\\Sigma = \\operatorname{diag}(\\sigma_{1},\\dots,\\sigma_{k})$ with $\\sigma_{1} \\ge \\cdots \\ge \\sigma_{k} > 0$.\n\nDynamic Mode Decomposition (DMD) seeks a linear map $A$ minimizing the Frobenius-norm residual $\\|X' - A X\\|_{F}$. The unconstrained minimizer over $A \\in \\mathbb{C}^{n \\times n}$ is given by\n$$\nA_{\\star} = X' X^{+},\n$$\nwhere $X^{+}$ denotes the Moore–Penrose pseudoinverse. In practice, to promote stability and a low-dimensional model, one uses a truncated SVD of $X$ at rank $r$,\n$$\nX \\approx U_{r} \\Sigma_{r} V_{r}^{*},\n$$\nwhere $U_{r} \\in \\mathbb{C}^{n \\times r}$, $\\Sigma_{r} \\in \\mathbb{R}^{r \\times r}$, and $V_{r} \\in \\mathbb{C}^{m \\times r}$ collect the leading $r$ singular triplets. The corresponding rank-$r$ DMD operator is\n$$\nA_{r} = X' V_{r} \\Sigma_{r}^{-1} U_{r}^{*}.\n$$\nWe now compute the residual for $A_{r}$. Using the SVD factorization of $X$ and the definition of $A_{r}$,\n\\begin{align*}\nA_{r} X \n&= X' V_{r} \\Sigma_{r}^{-1} U_{r}^{*} \\, (U \\Sigma V^{*}) \\\\\n&= X' V_{r} \\Sigma_{r}^{-1} \\Sigma_{r} V_{r}^{*} \\\\\n&= X' V_{r} V_{r}^{*}.\n\\end{align*}\nTherefore the residual matrix is\n$$\nR_{r} \\equiv X' - A_{r} X = X' - X' V_{r} V_{r}^{*} = X' (I - V_{r} V_{r}^{*}),\n$$\nwhere $V_{r} V_{r}^{*}$ is the orthogonal projector in $\\mathbb{C}^{m}$ onto $\\operatorname{range}(V_{r})$, the subspace spanned by the leading $r$ right singular vectors of $X$. The Frobenius norm of the residual is thus\n$$\n\\|R_{r}\\|_{F} = \\|X' (I - V_{r} V_{r}^{*})\\|_{F}.\n$$\n\nTo express this in terms of the singular structure of $X$, we exploit the exact dynamics $X' = A_{\\mathrm{true}} X$ and the SVD of $X$. Let $V = [v_{1},\\dots,v_{k}]$ and $U = [u_{1},\\dots,u_{k}]$, with $X v_{i} = \\sigma_{i} u_{i}$. The projector onto the orthogonal complement of $\\operatorname{range}(V_{r})$ is\n$$\nI - V_{r} V_{r}^{*} = \\sum_{i=r+1}^{k} v_{i} v_{i}^{*},\n$$\nsince $V$ has orthonormal columns and $V_{r}$ collects the first $r$. Hence\n\\begin{align*}\nR_{r} \n&= X' \\sum_{i=r+1}^{k} v_{i} v_{i}^{*}\n= \\sum_{i=r+1}^{k} X' v_{i} v_{i}^{*}.\n\\end{align*}\nUsing $X' = A_{\\mathrm{true}} X$ and $X v_{i} = \\sigma_{i} u_{i}$, we obtain\n$$\nX' v_{i} = A_{\\mathrm{true}} X v_{i} = A_{\\mathrm{true}} (\\sigma_{i} u_{i}) = \\sigma_{i} (A_{\\mathrm{true}} u_{i}).\n$$\nThus,\n$$\nR_{r} = \\sum_{i=r+1}^{k} \\sigma_{i} (A_{\\mathrm{true}} u_{i}) v_{i}^{*}.\n$$\nBecause $\\{v_{i}\\}$ are orthonormal and $A_{\\mathrm{true}}$ is unitary, the Frobenius norm squares add:\n\\begin{align*}\n\\|R_{r}\\|_{F}^{2}\n&= \\sum_{i=r+1}^{k} \\|\\sigma_{i} (A_{\\mathrm{true}} u_{i}) v_{i}^{*}\\|_{F}^{2}\n= \\sum_{i=r+1}^{k} \\sigma_{i}^{2} \\,\\|A_{\\mathrm{true}} u_{i}\\|_{2}^{2} \\,\\|v_{i}^{*}\\|_{F}^{2}.\n\\end{align*}\nHere $\\|v_{i}^{*}\\|_{F}^{2} = \\|v_{i}\\|_{2}^{2} = 1$, and by unitarity $\\|A_{\\mathrm{true}} u_{i}\\|_{2}^{2} = \\|u_{i}\\|_{2}^{2} = 1$. Therefore,\n$$\n\\|R_{r}\\|_{F}^{2} = \\sum_{i=r+1}^{k} \\sigma_{i}^{2}.\n$$\n\nWe have reduced the DMD residual to the tail energy of the singular values of $X$ discarded by truncation. Under the given geometric decay $\\sigma_{i} = \\sigma_{1} \\gamma^{\\,i-1}$ with $\\gamma \\in (0,1)$, the residual becomes\n\\begin{align*}\n\\|R_{r}\\|_{F}^{2}\n&= \\sum_{i=r+1}^{k} \\left(\\sigma_{1} \\gamma^{\\,i-1}\\right)^{2}\n= \\sigma_{1}^{2} \\sum_{i=r+1}^{k} \\gamma^{\\,2(i-1)}\n= \\sigma_{1}^{2} \\sum_{j=r}^{k-1} \\gamma^{\\,2 j}.\n\\end{align*}\nThis is a finite geometric series with ratio $\\gamma^{2}$. Its closed form is\n\\begin{align*}\n\\|R_{r}\\|_{F}^{2}\n&= \\sigma_{1}^{2} \\,\\gamma^{\\,2 r} \\,\\frac{1 - \\gamma^{\\,2(k-r)}}{1 - \\gamma^{\\,2}}.\n\\end{align*}\nTaking the principal square root yields the Frobenius-norm residual\n$$\n\\|X' - A_{r} X\\|_{F} = \\sigma_{1} \\,\\gamma^{\\,r} \\,\\sqrt{\\frac{1 - \\gamma^{\\,2(k-r)}}{1 - \\gamma^{\\,2}}}.\n$$\nThis expression is valid for $1 \\le r \\le k$, and it correctly yields zero when $r = k$ since then the numerator factor becomes $1 - \\gamma^{0} = 0$.",
            "answer": "$$\\boxed{\\sigma_{1}\\,\\gamma^{\\,r}\\,\\sqrt{\\frac{1-\\gamma^{\\,2(k-r)}}{1-\\gamma^{\\,2}}}}$$"
        },
        {
            "introduction": "While DMD performs exceptionally well on clean, deterministic data, real-world measurements are invariably corrupted by noise. This final practice confronts this challenge by analyzing the robustness of DMD in a stochastic setting where the system is driven by temporally correlated, or \"colored,\" noise. You will derive the asymptotic bias in the DMD eigenvalue estimate, revealing a critical insight: DMD can be understood as a regression technique, and like all such methods, its accuracy is compromised when the noise structure violates its core assumptions .",
            "id": "3383158",
            "problem": "Consider the one-dimensional heat partial differential equation (PDE) $u_{t} = \\nu u_{xx}$ posed on the periodic domain $x \\in [0,2\\pi)$ with viscosity parameter $\\nu > 0$. Let $u(x,t)$ be expanded in the Fourier basis, and focus on the temporal dynamics of a single Fourier mode with wavenumber $k \\in \\mathbb{N}$. Under a stable time-stepping scheme with time step $\\Delta t > 0$, the unforced discrete-time evolution of the mode amplitude $a_{k}(n)$ can be represented as $a_{k}(n) = \\alpha a_{k}(n-1)$, where $\\alpha \\in (-1,1)$ is the associated amplification factor of the linear numerical propagator for that mode.\n\nNow, suppose the PDE is driven by a zero-mean colored-in-time source that couples additively into the same mode, so that the observed mode amplitude dynamics obey\n$$\nx_{n} = \\alpha x_{n-1} + r_{n}, \\quad n \\in \\mathbb{Z},\n$$\nwhere $x_{n} := a_{k}(n)$ is the scalar mode amplitude and $r_{n}$ is an Autoregressive of order $1$ (AR(1)) process given by\n$$\nr_{n} = \\phi r_{n-1} + \\eta_{n},\n$$\nwith $\\phi \\in (-1,1)$, and $\\eta_{n}$ are independent and identically distributed zero-mean innovations with finite variance $\\sigma_{\\eta}^{2} \\in (0,\\infty)$. Assume stationarity of $x_{n}$ and $r_{n}$ and $|\\alpha|<1$, $|\\phi|<1$.\n\nYou collect $m$ consecutive snapshots of the scalar amplitude, forming the data arrays $X = [x_{0}, x_{1}, \\dots, x_{m-1}]$ and $Y = [x_{1}, x_{2}, \\dots, x_{m}]$. You apply Dynamic Mode Decomposition (DMD), defined by $A_{\\mathrm{DMD}} = Y X^{+}$ where $X^{+}$ denotes the Moore–Penrose pseudoinverse, to estimate the dominant eigenvalue of the one-dimensional linear evolution. In the scalar case, the DMD estimate reduces to the least-squares slope mapping $x_{n-1} \\mapsto x_{n}$.\n\nAnalyze the robustness of the DMD eigenvalue estimate under this colored-in-time forcing by deriving, in the limit $m \\to \\infty$, the expected bias of the DMD eigenvalue estimate due to the AR(1) forcing. Express your final answer as a closed-form analytic expression for\n$$\n\\mathbb{E}[\\hat{\\alpha}] - \\alpha\n$$\nin terms of $\\alpha$ and $\\phi$. No numerical rounding is required.",
            "solution": "The problem asks for the asymptotic bias of the Dynamic Mode Decomposition (DMD) eigenvalue estimate for a scalar autoregressive process driven by colored noise. The process is defined by the coupled equations:\n$$\nx_{n} = \\alpha x_{n-1} + r_{n}\n$$\n$$\nr_{n} = \\phi r_{n-1} + \\eta_{n}\n$$\nwhere $x_n$ is the observed scalar amplitude, $\\alpha$ is the true dynamics eigenvalue we wish to estimate, and $r_n$ is an Autoregressive process of order $1$ (AR(1)) representing colored-in-time forcing. The parameters are constrained by $|\\alpha| < 1$ and $|\\phi| < 1$, ensuring stationarity of the processes. The innovations $\\eta_n$ are i.i.d. with zero mean, $\\mathbb{E}[\\eta_n] = 0$, and finite variance $\\sigma_\\eta^2$.\n\nThe DMD estimate for the scalar case, denoted $\\hat{\\alpha}$, is the least-squares solution that minimizes the residual in mapping $x_{n-1}$ to $x_n$. Given the data vectors $X = [x_{0}, x_{1}, \\dots, x_{m-1}]$ and $Y = [x_{1}, x_{2}, \\dots, x_{m}]$, the DMD eigenvalue is given by $A_{\\mathrm{DMD}} = Y X^{+}$, where $X^{+}$ is the Moore-Penrose pseudoinverse of the row vector $X$. This simplifies to:\n$$\n\\hat{\\alpha} = \\frac{\\sum_{n=1}^{m} x_{n} x_{n-1}}{\\sum_{n=1}^{m} x_{n-1}^2}\n$$\nThe problem requires us to find the expected bias, $\\mathbb{E}[\\hat{\\alpha}] - \\alpha$, in the limit of a large number of snapshots, $m \\to \\infty$.\n\nIn this limit, by the law of large numbers for stationary and ergodic processes, the sample averages converge in probability to their respective expected values. The process $x_n$ is stationary under the given conditions. Thus, the estimator $\\hat{\\alpha}$ converges in probability to a value we denote by $\\alpha_{\\infty}$:\n$$\n\\hat{\\alpha} \\xrightarrow{p}_{m\\to\\infty} \\alpha_{\\infty} = \\frac{\\mathbb{E}[x_{n} x_{n-1}]}{\\mathbb{E}[x_{n-1}^2]}\n$$\nDue to stationarity, the denominator is $\\mathbb{E}[x_{n-1}^2] = \\mathbb{E}[x_n^2]$. Let $\\gamma_x(k) = \\mathbb{E}[x_n x_{n-k}]$ be the autocovariance function of the process $x_n$. The asymptotic estimate is then:\n$$\n\\alpha_{\\infty} = \\frac{\\gamma_x(1)}{\\gamma_x(0)}\n$$\nThe asymptotic bias is therefore $\\alpha_{\\infty} - \\alpha$. To calculate this, we must first determine the statistical properties of the process $x_n$, specifically its autocovariance function.\n\nThe process $x_n$ can be expressed as a single equation by substituting for the noise term $r_n$. From the primary dynamic equation, we have $r_n = x_n - \\alpha x_{n-1}$. Substituting this into the equation for $r_n$:\n$$\n(x_n - \\alpha x_{n-1}) = \\phi (x_{n-1} - \\alpha x_{n-2}) + \\eta_n\n$$\nRearranging the terms, we find the evolution equation for $x_n$:\n$$\nx_n = (\\alpha + \\phi) x_{n-1} - \\alpha\\phi x_{n-2} + \\eta_n\n$$\nThis reveals that $x_n$ is an Autoregressive process of order $2$ (AR(2)). The stability of this process is guaranteed by the conditions $|\\alpha|<1$ and $|\\phi|<1$, as the roots of the characteristic polynomial $1 - (\\alpha+\\phi)z + \\alpha\\phi z^2 = (1-\\alpha z)(1-\\phi z)$ are $1/\\alpha$ and $1/\\phi$, which lie outside the unit circle.\n\nFor a stationary AR(p) process, the autocovariances satisfy the Yule-Walker equations. For our AR(2) process, multiplying the equation for $x_n$ by $x_{n-k}$ for $k > 0$ and taking the expectation yields:\n$$\n\\mathbb{E}[x_n x_{n-k}] = (\\alpha + \\phi) \\mathbb{E}[x_{n-1} x_{n-k}] - \\alpha\\phi \\mathbb{E}[x_{n-2} x_{n-k}] + \\mathbb{E}[\\eta_n x_{n-k}]\n$$\nSince $x_{n-k}$ for $k>0$ depends only on innovations up to time $n-k$, it is uncorrelated with $\\eta_n$. Thus, $\\mathbb{E}[\\eta_n x_{n-k}] = 0$. The equation becomes:\n$$\n\\gamma_x(k) = (\\alpha + \\phi) \\gamma_x(k-1) - \\alpha\\phi \\gamma_x(k-2) \\quad \\text{for } k > 0\n$$\nTo find the ratio $\\gamma_x(1)/\\gamma_x(0)$, we set $k=1$:\n$$\n\\gamma_x(1) = (\\alpha + \\phi) \\gamma_x(0) - \\alpha\\phi \\gamma_x(-1)\n$$\nUsing the property that $\\gamma_x(-1) = \\gamma_x(1)$, we have:\n$$\n\\gamma_x(1) = (\\alpha + \\phi) \\gamma_x(0) - \\alpha\\phi \\gamma_x(1)\n$$\nCollecting terms involving $\\gamma_x(1)$:\n$$\n\\gamma_x(1) (1 + \\alpha\\phi) = (\\alpha + \\phi) \\gamma_x(0)\n$$\nSolving for the ratio gives the asymptotic DMD estimate:\n$$\n\\alpha_{\\infty} = \\frac{\\gamma_x(1)}{\\gamma_x(0)} = \\frac{\\alpha + \\phi}{1 + \\alpha\\phi}\n$$\nThe asymptotic bias of the estimator is the difference between this value and the true parameter $\\alpha$:\n$$\n\\mathbb{E}[\\hat{\\alpha}] - \\alpha \\quad \\xrightarrow{m\\to\\infty} \\quad \\alpha_{\\infty} - \\alpha = \\frac{\\alpha + \\phi}{1 + \\alpha\\phi} - \\alpha\n$$\nWe place the terms over a common denominator to simplify:\n$$\n\\text{Bias} = \\frac{(\\alpha + \\phi) - \\alpha(1 + \\alpha\\phi)}{1 + \\alpha\\phi} = \\frac{\\alpha + \\phi - \\alpha - \\alpha^2\\phi}{1 + \\alpha\\phi} = \\frac{\\phi - \\alpha^2\\phi}{1 + \\alpha\\phi}\n$$\nFactoring out $\\phi$ from the numerator gives the final expression for the asymptotic bias:\n$$\n\\text{Bias} = \\frac{\\phi(1 - \\alpha^2)}{1 + \\alpha\\phi}\n$$\nThis result shows that the bias is zero only if $\\phi=0$, which corresponds to the case of white noise forcing. For any non-zero $\\phi$, the DMD estimate is biased, and the magnitude and sign of the bias depend on the interplay between the system's own dynamics ($\\alpha$) and the temporal correlation of the forcing ($\\phi$).",
            "answer": "$$\n\\boxed{\\frac{\\phi(1 - \\alpha^2)}{1 + \\alpha\\phi}}\n$$"
        }
    ]
}