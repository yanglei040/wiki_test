{
    "hands_on_practices": [
        {
            "introduction": "掌握伴随方法的第一步是透彻理解其数学推导。本练习将引导你通过一个经典的“纸上”推导，使用拉格朗日乘子法，为时变（抛物线）偏微分方程约束的优化问题导出其伴随方程和梯度表达式。完成这个练习将为你后续更复杂的应用打下坚实的理论基础。",
            "id": "3361153",
            "problem": "考虑一个有界 Lipschitz 域 $\\Omega \\subset \\mathbb{R}^{d}$，其边界为 $\\partial \\Omega$，并设一个固定的最终时间 $T>0$。令 $\\kappa(x)$ 为一个标量扩散系数，满足 $\\kappa \\in C^{1}(\\overline{\\Omega})$ 和 $0  \\kappa_{\\min}\\le \\kappa(x)\\le \\kappa_{\\max}  \\infty$。对于一个给定的源项 $s \\in L^{2}(\\Omega \\times (0,T))$，状态 $u:\\Omega \\times [0,T]\\to \\mathbb{R}$ 定义为以下抛物型初边值问题的唯一弱解\n$$\n\\begin{cases}\nu_{t}(x,t)-\\nabla \\cdot \\big(\\kappa(x)\\nabla u(x,t)\\big)=s(x,t),  (x,t)\\in \\Omega \\times (0,T),\\\\\nu(x,t)=0,  (x,t)\\in \\partial \\Omega \\times (0,T),\\\\\nu(x,0)=u_{0}(x),  x\\in \\Omega,\n\\end{cases}\n$$\n其中 $u_{0}\\in L^{2}(\\Omega)$ 是固定的。设 $d \\in L^{2}(\\Omega \\times (0,T))$ 是给定的时空数据，并定义追踪型目标泛函\n$$\nJ(u)=\\frac{1}{2}\\int_{0}^{T}\\!\\!\\int_{\\Omega} \\big(u(x,t)-d(x,t)\\big)^{2}\\,dx\\,dt.\n$$\n将状态方程视为连接 $u$ 和 $s$ 的约束，并通过状态 $u(s)$ 将 $J$ 视为 $s$ 的泛函。仅使用基础工具（巴拿赫空间中 Gâteaux 导数的链式法则、约束优化的拉格朗日乘子法，以及与给定边界和初始数据一致的时间和空间分部积分法），推导 $J$ 关于源项 $s(x,t)$ 的 $L^{2}(\\Omega\\times(0,T))$-梯度的显式解析表达式。你的最终答案必须是一个以适定偏微分方程定义的量表示的单一闭式解析表达式。不要引入任何正则化。将你的最终答案表示为单一的解析表达式，且不包含单位。",
            "solution": "所述问题是有效的。这是一个在偏微分方程最优控制领域内适定的、有科学依据的问题。所有必要的数据、函数和条件都已提供，并且它们是相互一致的。因此，我们可以着手推导解答。\n\n目标是求目标泛函 $J$ 关于源项 $s$ 的 $L^{2}(\\Omega\\times(0,T))$-梯度。状态变量 $u$ 通过状态方程与控制变量 $s$ 相关联，该方程作为一个约束。我们采用拉格朗日乘子法来处理这个约束。\n\n设状态空间是适用于 $u$ 的一个合适的函数空间，控制空间是 $s$ 所在的 $L^{2}(\\Omega\\times(0,T))$，而约束的对偶空间是拉格朗日乘子 $p$（通常称为伴随状态）所在的空间。拉格朗日泛函 $\\mathcal{L}(u,s,p)$ 定义为：\n$$\n\\mathcal{L}(u,s,p) = J(u) - \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\left(u_{t} - \\nabla \\cdot (\\kappa \\nabla u) - s\\right) p \\,dx\\,dt\n$$\n由于状态 $u$ 由 $s$ 唯一确定（对于固定的 $u_0$），我们可以将 $J$ 视为仅关于 $s$ 的泛函，即 $j(s) = J(u(s))$。对于任何 $p$ 的选择，如果 $u(s)$ 满足状态方程，则拉格朗日泛函中的积分项为零，因此 $j(s) = \\mathcal{L}(u(s), s, p)$。\n\n我们希望计算 $j(s)$ 在任意方向 $\\delta s \\in L^{2}(\\Omega\\times(0,T))$ 上的 Gâteaux 导数。设 $u(s)$ 是对应于 $s$ 的解，并设 $\\delta u$ 是解映射 $u(s)$ 在方向 $\\delta s$ 上的 Gâteaux 导数。对 $\\mathcal{L}(u(s), s, p)$ 应用链式法则，我们得到：\n$$\nDj(s)[\\delta s] = \\frac{\\partial \\mathcal{L}}{\\partial u}(u,s,p)[\\delta u] + \\frac{\\partial \\mathcal{L}}{\\partial s}(u,s,p)[\\delta s]\n$$\n伴随法的核心是选择伴随状态 $p$，使得依赖于未知灵敏度 $\\delta u$ 的第一项消失。这个选择将定义伴随方程。让我们计算 $\\mathcal{L}$ 关于 $u$ 在方向 $\\delta u$ 上的偏 Gâteaux 导数：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = \\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\mathcal{L}(u+\\epsilon\\delta u, s, p) = \\int_{0}^{T}\\!\\!\\int_{\\Omega} (u-d)\\delta u \\,dx\\,dt - \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\left(\\delta u_{t} - \\nabla \\cdot (\\kappa \\nabla \\delta u)\\right) p \\,dx\\,dt\n$$\n为了消除 $\\delta u$，我们对第二项使用分部积分，将导数转移到 $p$ 上。\n首先，我们对时间导数项关于时间 $t$ 进行分部积分：\n$$\n-\\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u_{t} p \\,dx\\,dt = -\\int_{\\Omega} \\left( [\\delta u(x,t) p(x,t)]_{t=0}^{t=T} - \\int_{0}^{T} \\delta u(x,t) p_{t}(x,t) \\,dt \\right) dx\n$$\n$$\n= -\\int_{\\Omega} \\delta u(x,T) p(x,T) \\,dx + \\int_{\\Omega} \\delta u(x,0) p(x,0) \\,dx + \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u p_{t} \\,dx\\,dt\n$$\n变分 $\\delta u$ 对应于源项中的变分 $\\delta s$，而初始条件 $u_0$ 是固定的。因此，$\\delta u$ 的初始条件是 $\\delta u(x,0)=0$。项 $\\int_{\\Omega} \\delta u(x,0) p(x,0) \\,dx$ 消失。\n\n接下来，我们对扩散项关于空间变量进行分部积分。使用格林第一恒等式 $\\int_{\\Omega} (\\nabla\\cdot\\mathbf{F})\\psi \\,dx = \\int_{\\partial\\Omega} (\\mathbf{F}\\cdot\\mathbf{n})\\psi \\,dS - \\int_{\\Omega} \\mathbf{F}\\cdot\\nabla\\psi \\,dx$，令 $\\mathbf{F} = \\kappa\\nabla\\delta u$ 和 $\\psi=p$，我们得到：\n$$\n\\int_{\\Omega} (\\nabla \\cdot (\\kappa \\nabla \\delta u)) p \\,dx = \\int_{\\partial\\Omega} p (\\kappa \\nabla \\delta u \\cdot \\mathbf{n}) \\,dS - \\int_{\\Omega} \\kappa \\nabla \\delta u \\cdot \\nabla p \\,dx\n$$\n为了使边界积分对任意 $\\delta u$ 都为零，我们对伴随状态施加齐次狄利克雷边界条件，$p(x,t)=0$ 对于 $(x,t) \\in \\partial\\Omega \\times (0,T)$。然后，再次应用分部积分：\n$$\n- \\int_{\\Omega} \\kappa \\nabla \\delta u \\cdot \\nabla p \\,dx = \\int_{\\Omega} \\delta u (\\nabla \\cdot (\\kappa \\nabla p)) \\,dx - \\int_{\\partial\\Omega} \\delta u (\\kappa \\nabla p \\cdot \\mathbf{n}) \\,dS\n$$\n状态 $u$ 在 $\\partial\\Omega \\times(0,T)$ 上满足 $u=0$，因此其变分 $\\delta u$ 在边界上也必须为零。因此，第二个边界积分也消失了。综合这些结果可得：\n$$\n\\int_{0}^{T}\\!\\!\\int_{\\Omega} (\\nabla \\cdot (\\kappa \\nabla \\delta u)) p \\,dx\\,dt = \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u (\\nabla \\cdot (\\kappa \\nabla p)) \\,dx\\,dt\n$$\n将这些结果代回到 $\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u]$ 的表达式中：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = \\int_{0}^{T}\\!\\!\\int_{\\Omega} (u-d)\\delta u \\,dx\\,dt - \\left( \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u p_{t} \\,dx\\,dt - \\int_{\\Omega} \\delta u(x,T) p(x,T) \\,dx + \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u (\\nabla \\cdot (\\kappa\\nabla p)) \\,dx\\,dt \\right)\n$$\n重新组合包含 $\\delta u$ 的项：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = -\\int_{0}^{T}\\!\\!\\int_{\\Omega} \\left( -p_{t} - \\nabla \\cdot (\\kappa \\nabla p) - (u-d) \\right) \\delta u \\,dx\\,dt + \\int_{\\Omega} \\delta u(x,T) p(x,T) \\,dx\n$$\n为了使此表达式对于任何容许的变分 $\\delta u$ 都为零，乘以 $\\delta u$ 的被积函数必须为零。这就得到了 $p$ 的伴随方程：\n$$\n\\begin{cases}\n-p_{t}(x,t)-\\nabla \\cdot \\big(\\kappa(x)\\nabla p(x,t)\\big) = u(x,t)-d(x,t),  (x,t)\\in \\Omega \\times (0,T)\\\\\np(x,t)=0,  (x,t)\\in \\partial \\Omega \\times (0,T)\\\\\np(x,T)=0,  x\\in \\Omega\n\\end{cases}\n$$\n这是一个适定的时间后向线性抛物型方程。通过这样选择 $p$，我们有 $\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = 0$。\n\n现在 $j(s)$ 的 Gâteaux 导数简化为：\n$$\nDj(s)[\\delta s] = \\frac{\\partial \\mathcal{L}}{\\partial s}(u,s,p)[\\delta s] = \\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\mathcal{L}(u, s+\\epsilon\\delta s, p)\n$$\n$$\n= \\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\left( J(u) - \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\left(u_{t} - \\nabla \\cdot (\\kappa \\nabla u) - (s+\\epsilon\\delta s)\\right) p \\,dx\\,dt \\right)\n$$\n$$\n= - \\int_{0}^{T}\\!\\!\\int_{\\Omega} (-\\delta s) p \\,dx\\,dt = \\int_{0}^{T}\\!\\!\\int_{\\Omega} p \\cdot \\delta s \\,dx\\,dt\n$$\n$L^{2}(\\Omega\\times(0,T))$-梯度，我们记为 $\\nabla_s J(s)$，通过内积由 Riesz 表示定理定义：\n$$\nDj(s)[\\delta s] = \\langle \\nabla_s J(s), \\delta s \\rangle_{L^{2}(\\Omega\\times(0,T))} = \\int_{0}^{T}\\!\\!\\int_{\\Omega} (\\nabla_s J(s)) \\delta s \\,dx\\,dt\n$$\n将此与我们推导出的 $Dj(s)[\\delta s]$ 表达式进行比较，我们确定梯度即为伴随状态 $p(x,t)$。\n$$\n\\nabla_s J(s) = p(x,t)\n$$\n其中 $u(x,t)$ 是正向问题的解，而 $p(x,t)$ 是上述定义的伴随问题的解。",
            "answer": "$$\n\\boxed{p(x,t)}\n$$"
        },
        {
            "introduction": "理论的正确性最终需要通过实践来检验。本练习将我们从连续世界的推导带入到离散计算的实践中，针对一个非线性椭圆方程约束问题，你将实现一个完整的伴随求解器。更重要的是，你将学习并应用泰勒余项检验（也称梯度检验），这是一个在实践中确保复杂梯度计算正确性的不可或缺的工具。",
            "id": "3361125",
            "problem": "考虑一个在空间区间 $(0,1)$ 上的偏微分方程（PDE）约束优化问题，其状态变量为 $u(x)$，分布式控制为 $m(x)$，后者作为源项出现。控制性非线性偏微分方程为\n$$\n- \\frac{d^2 u}{dx^2} + \\beta\\, u(x)^3 = m(x), \\quad x \\in (0,1),\n$$\n服从齐次 Dirichlet 边界条件\n$$\nu(0) = 0, \\quad u(1) = 0.\n$$\n设期望状态为 $u_d(x)$，并考虑目标泛函\n$$\nJ(u,m) = \\frac{1}{2} \\int_0^1 \\big(u(x) - u_d(x)\\big)^2 \\, dx + \\frac{\\gamma}{2} \\int_0^1 m(x)^2 \\, dx.\n$$\n从变分法和偏微分方程约束优化的 Lagrangian 框架出发，通过引入一个伴随变量 $\\lambda(x)$ 并要求 Lagrangian 泛函关于 $u$ 和 $m$ 的平稳性，来推导必要的一阶最优性条件。从这些条件中，获得伴随偏微分方程以及 $J$ 关于 $m$ 的梯度。\n\n在一个包含边界点 $x_0 = 0$ 和 $x_{N-1} = 1$ 的 $N$ 个点的均匀网格上离散化该问题，网格间距为 $h = 1/(N-1)$。在 $N-2$ 个内部节点上，对 $-d^2u/dx^2$ 使用二阶中心有限差分，并强制 $u_0=u_{N-1}=0$。所有积分都通过在内部节点上使用均匀权重 $h$ 的 Riemann 和来近似。对于非线性偏微分方程，实现一个带回溯线搜索的 Newton 方法来求解内部节点上的离散状态 $u$。然后求解离散伴随方程以获得内部节点上的离散伴随变量。使用这些来构建关于 $m$ 的离散梯度。\n\n使用随机扰动和 Taylor 余项检验来实现伴随梯度的验证程序。具体来说，对于一个固定的基准控制 $m$ 和一个在离散 $L^2$ 内积中归一化的随机扰动方向 $\\delta m$，计算\n$$\nR(\\varepsilon) = J\\big(m + \\varepsilon \\delta m\\big) - J(m) - \\varepsilon \\langle \\nabla J(m), \\delta m \\rangle_h,\n$$\n其中 $\\langle a, b \\rangle_h = h \\sum_i a_i b_i$ 表示在内部节点上的离散内积。通过计算证明，对于足够小的 $\\varepsilon$，$R(\\varepsilon)$ 与 $\\varepsilon$ 呈二次方关系，即 $R(\\varepsilon) = \\mathcal{O}(\\varepsilon^2)$。这通过对 $\\log |R(\\varepsilon)|$ 与 $\\log \\varepsilon$ 进行线性拟合来估计斜率 $p$ 实现。\n\n你的程序必须构建以下参数集的测试套件，并为每个测试报告从拟合中获得的估计阶数 $p$。在所有测试中，使用 $u_d(x) = \\sin(\\pi x)$ 和在内部节点上求值的基准控制 $m(x) = \\sin(2\\pi x) + 0.5 \\sin(\\pi x)$。通过抽取独立的标准正态分布项并归一化到单位离散 $L^2$ 范数来生成随机扰动 $\\delta m$；为保证可复现性，每个测试使用指定的随机种子。对于每个测试，使用相同的 $\\varepsilon$ 值集合\n$$\n\\varepsilon \\in \\left\\{10^{-1}, \\; 5 \\times 10^{-2}, \\; 2.5 \\times 10^{-2}, \\; 1.25 \\times 10^{-2} \\right\\}.\n$$\n\n测试套件：\n- 测试 A (happy path)：$N = 128$, $\\beta = 1$, $\\gamma = 10^{-3}$, 随机种子 $s = 7$。\n- 测试 B (boundary case: linear state equation)：$N = 96$, $\\beta = 0$, $\\gamma = 10^{-2}$, 随机种子 $s = 13$。\n- 测试 C (edge case: stronger nonlinearity and no regularization)：$N = 160, \\beta = 5, \\gamma = 0$, 随机种子 $s = 21$。\n\n你的程序应产生单行输出，其中包含按顺序排列的测试 A、B 和 C 的估计阶数 $p$，四舍五入到三位小数，并以逗号分隔的列表形式包含在方括号中（例如，$[2.000,2.000,2.000]$）。不涉及物理单位。如果存在角度，必须以弧度为单位，但此问题不涉及角度。所有数值输出必须是浮点数。",
            "solution": "用户希望解决一个偏微分方程约束的优化问题。我将首先验证问题陈述，然后提供一个完整的解决方案。\n\n### 第1步：提取已知信息\n- **控制性偏微分方程**：$- \\frac{d^2 u}{dx^2} + \\beta\\, u(x)^3 = m(x)$，在 $x \\in (0,1)$ 上。\n- **边界条件**：齐次 Dirichlet 条件，$u(0) = 0, u(1) = 0$。\n- **目标泛函**：$J(u,m) = \\frac{1}{2} \\int_0^1 \\big(u(x) - u_d(x)\\big)^2 \\, dx + \\frac{\\gamma}{2} \\int_0^1 m(x)^2 \\, dx$。\n- **期望状态**：$u_d(x) = \\sin(\\pi x)$。\n- **离散化**：使用 $N$ 个点的均匀网格，间距 $h = 1/(N-1)$。在 $N-2$ 个内部节点上对拉普拉斯算子使用二阶中心有限差分。积分通过在内部节点上使用权重 $h$ 的 Riemann 和来近似。\n- **状态求解器**：对离散非线性状态方程使用带回溯线搜索的 Newton 方法。\n- **验证任务**：实现 Taylor 余项检验来验证伴随法推导的梯度。\n- **Taylor 余项**：$R(\\varepsilon) = J\\big(m + \\varepsilon \\delta m\\big) - J(m) - \\varepsilon \\langle \\nabla J(m), \\delta m \\rangle_h$。\n- **离散内积**：$\\langle a, b \\rangle_h = h \\sum_i a_i b_i$，在内部节点上求和。\n- **验证目标**：通过对 $\\log |R(\\varepsilon)|$ 与 $\\log \\varepsilon$ 进行线性拟合来估计斜率 $p$，以显示 $R(\\varepsilon) = \\mathcal{O}(\\varepsilon^2)$。\n- **基准控制**：$m(x) = \\sin(2\\pi x) + 0.5 \\sin(\\pi x)$，在内部节点上求值。\n- **扰动**：来自标准正态分布的随机向量 $\\delta m$，归一化使其离散 $L^2$ 范数为1。\n- **Epsilon 值**：$\\varepsilon \\in \\{10^{-1}, 5 \\times 10^{-2}, 2.5 \\times 10^{-2}, 1.25 \\times 10^{-2}\\}$。\n- **测试套件**：\n  - 测试 A：$N = 128, \\beta = 1, \\gamma = 10^{-3}$, 随机种子 $s = 7$。\n  - 测试 B：$N = 96, \\beta = 0, \\gamma = 10^{-2}$, 随机种子 $s = 13$。\n  - 测试 C：$N = 160, \\beta = 5, \\gamma = 0$, 随机种子 $s = 21$。\n\n### 第2步：使用提取的已知信息进行验证\n该问题是数值优化和科学计算领域的一个标准练习，特别关注于偏微分方程控制系统的灵敏度分析的伴随方法。\n\n- **科学基础（关键）**：该问题在根本上是合理的。它涉及一个非线性椭圆偏微分方程（静态非线性 Schrödinger 方程或 Lane-Emden 方程的变体）、一个标准的二次目标泛函，并要求推导和验证基于伴随的梯度，这是偏微分方程约束优化的基石。所有概念在数学和数值分析中都已成熟。\n- **良态**：任务定义清晰：推导离散最优性系统，实现求解器，并进行数值验证。具有给定非线性和边界条件的偏微分方程预期有唯一解。目标是为给定的测试用例计算一个特定的数值（收敛阶数 $p$）。该问题是良态的。\n- **客观性（关键）**：问题以精确、客观的数学语言陈述。所有参数、方程和数值方法都已指定，没有留下歧义或主观解释的空间。\n\n该问题没有表现出任何无效性缺陷。它是自洽的、科学正确的和可形式化的。实现需要一套不平凡但标准的数值算法（有限差分、Newton 方法、线性系统求解器）。\n\n### 第3步：结论与行动\n问题是**有效**的。下面是一个完整的、有理有据的解决方案。\n\n### 解决方案推导与实现计划\n\n#### 1. 连续最优性条件\n我们通过将目标 $J$ 与由伴随变量（Lagrange 乘子）$\\lambda(x)$ 加权的偏微分方程约束相结合，引入 Lagrangian 泛函 $\\mathcal{L}(u, m, \\lambda)$：\n$$\n\\mathcal{L}(u, m, \\lambda) = J(u, m) + \\int_0^1 \\lambda(x) \\left( - \\frac{d^2 u}{dx^2} + \\beta u^3 - m \\right) dx.\n$$\n一阶必要最优性条件是通过要求 $\\mathcal{L}$ 关于 $u$、$m$ 和 $\\lambda$ 的 Fréchet 导数等于零来找到的。\n\n- **关于 $\\lambda$ 的平稳性**：$\\frac{\\delta \\mathcal{L}}{\\delta \\lambda} = 0$ 恢复了状态方程：\n  $$ - \\frac{d^2 u}{dx^2} + \\beta u^3 = m, \\quad u(0)=u(1)=0. $$\n- **关于 $u$ 的平稳性**：$\\frac{\\delta \\mathcal{L}}{\\delta u} = 0$。在方向 $\\delta u$ 上对 $u$ 求导，并应用分部积分（使用 $\\delta u(0)=\\delta u(1)=0$），得到**伴随方程**。我们必须施加 $\\lambda(0)=\\lambda(1)=0$ 来消除边界项。\n  $$ - \\frac{d^2 \\lambda}{dx^2} + (3\\beta u^2) \\lambda = -(u - u_d), \\quad \\lambda(0)=\\lambda(1)=0. $$\n- **关于 $m$ 的平稳性**：$\\frac{\\delta \\mathcal{L}}{\\delta m} = 0$。简化泛函 $\\hat{J}(m) = J(u(m), m)$ 对 $m$ 的导数定义了梯度。\n  $$ \\nabla_m J = \\gamma m - \\lambda. $$\n因此，最优性系统由状态方程、伴随方程和梯度表达式组成。\n\n#### 2. 离散化\n设网格由 $N$ 个点 $x_i = ih$（$i=0, \\dots, N-1$）组成，间距 $h=1/(N-1)$。我们在 $N-2$ 个内部节点上求解变量。令 $\\mathbf{u}, \\mathbf{m}, \\boldsymbol{\\lambda} \\in \\mathbb{R}^{N-2}$ 为这些变量的向量。\n\n- **离散状态方程**：我们使用二阶有限差分近似 $-d^2/dx^2$。令 $\\mathbf{A}$ 为表示此算子的 $(N-2) \\times (N-2)$ 矩阵：\n  $$ \\mathbf{A}_{ij} = \\frac{1}{h^2} \\begin{cases} 2   i=j \\\\ -1  |i-j|=1 \\\\ 0   \\text{其他情况} \\end{cases} $$\n  状态 $\\mathbf{u}$ 的离散非线性系统为：\n  $$ \\mathbf{R}(\\mathbf{u}, \\mathbf{m}) \\equiv \\mathbf{A}\\mathbf{u} + \\beta \\mathbf{u}^3 - \\mathbf{m} = \\mathbf{0}, $$\n  其中 $\\mathbf{u}^3$ 是逐元素的。\n\n- **离散伴随方程**：离散化连续伴随方程，得到伴随向量 $\\boldsymbol{\\lambda}$ 的线性系统：\n  $$ \\left( \\mathbf{A} + 3\\beta \\, \\text{diag}(\\mathbf{u}^2) \\right) \\boldsymbol{\\lambda} = -(\\mathbf{u} - \\mathbf{u}_d). $$\n  左侧的矩阵是状态残差 $\\mathbf{R}$ 关于 $\\mathbf{u}$ 的 Jacobian 矩阵的转置，即 $\\left( \\frac{\\partial \\mathbf{R}}{\\partial \\mathbf{u}} \\right)^T$。由于 $\\mathbf{A}$ 是对称的，且第二项是对角的，所以这正是 Jacobian 矩阵本身，$\\mathbf{K}(\\mathbf{u}) = \\frac{\\partial \\mathbf{R}}{\\partial \\mathbf{u}} = \\mathbf{A} + 3\\beta \\, \\text{diag}(\\mathbf{u}^2)$。\n\n- **离散梯度**：离散梯度向量 $\\mathbf{g}$ 为：\n  $$ \\mathbf{g} = \\gamma \\mathbf{m} - \\boldsymbol{\\lambda}. $$\n\n#### 3. 数值求解器\n- **状态求解器 (Newton 方法)**：为求解 $\\mathbf{R}(\\mathbf{u}) = \\mathbf{0}$，我们使用 Newton 方法。从初始猜测 $\\mathbf{u}_0$ 开始，我们迭代：\n  1. 求解线性系统以获得更新量 $\\delta \\mathbf{u}_k$：$\\mathbf{K}(\\mathbf{u}_k) \\delta \\mathbf{u}_k = -\\mathbf{R}(\\mathbf{u}_k)$。\n  2. 执行回溯线搜索以找到步长 $\\alpha_k \\in (0, 1]$，确保残差范数有足够下降。\n  3. 更新状态：$\\mathbf{u}_{k+1} = \\mathbf{u}_k + \\alpha_k \\delta \\mathbf{u}_k$。\n  当残差范数或更新步长范数低于容差时，迭代停止。\n\n- **伴随求解器**：这是一个线性系统求解问题：$\\mathbf{K}(\\mathbf{u})\\boldsymbol{\\lambda} = -(\\mathbf{u} - \\mathbf{u}_d)$。我们可以重用 Newton 求解器中的 Jacobian 矩阵结构。\n\n#### 4. 梯度验证\n简化泛函 $\\hat{J}(m)$ 的 Taylor 展开为：\n$$ \\hat{J}(m + \\varepsilon \\delta m) = \\hat{J}(m) + \\varepsilon \\langle \\nabla_m \\hat{J}(m), \\delta m \\rangle_h + \\mathcal{O}(\\varepsilon^2). $$\n余项 $R(\\varepsilon) = \\hat{J}(m + \\varepsilon \\delta m) - \\hat{J}(m) - \\varepsilon \\langle \\mathbf{g}, \\delta m \\rangle_h$ 预期为 $\\mathcal{O}(\\varepsilon^2)$。为了验证这一点，我们为一系列递减的 $\\varepsilon$ 值计算 $R(\\varepsilon)$。$|R(\\varepsilon)|$ vs. $\\varepsilon$ 的对数-对数图应产生一条斜率 $p \\approx 2$ 的直线。我们使用对数转换后的数据进行线性多项式拟合（`np.polyfit`）来估计此斜率。\n\n每个测试用例的总体算法是：\n1. 初始化参数（$N, \\beta, \\gamma, h$ 等）和基于网格的函数（$\\mathbf{m}, \\mathbf{u}_d$）。\n2. 给定基准控制 $\\mathbf{m}$，求解状态方程以获得基准状态 $\\mathbf{u}$。\n3. 计算基准目标值 $J_0$。\n4. 使用基准状态 $\\mathbf{u}$ 求解伴随方程以获得 $\\boldsymbol{\\lambda}$。\n5. 计算基准点的梯度 $\\mathbf{g}$。\n6. 生成一个随机、归一化的扰动方向 $\\delta \\mathbf{m}$。\n7. 对于测试集中的每个 $\\varepsilon$：\n    a. 计算扰动后的控制 $\\mathbf{m}_{\\text{pert}} = \\mathbf{m} + \\varepsilon \\delta \\mathbf{m}$。\n    b. 求解状态方程以获得扰动后的状态 $\\mathbf{u}_{\\text{pert}}$。\n    c. 计算扰动后的目标值 $J_{\\text{pert}}$。\n    d. 计算 Taylor 余项 $R(\\varepsilon)$。\n8. 对 $\\log |R(\\varepsilon)|$ vs. $\\log \\varepsilon$ 进行线性回归，以找到斜率 $p$。\n9. 将 $p$ 追加到结果列表中。\n处理完所有测试用例后，对结果进行格式化并打印。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the adjoint verification for the given test suite.\n    \"\"\"\n\n    class AdjointVerifier:\n        \"\"\"\n        Encapsulates the logic for the PDE-constrained optimization problem,\n        including state/adjoint solvers and gradient verification.\n        \"\"\"\n        def __init__(self, N, beta, gamma):\n            self.N = N\n            self.beta = beta\n            self.gamma = gamma\n            \n            if N = 2:\n                raise ValueError(\"N must be greater than 2 for interior points.\")\n                \n            self.h = 1.0 / (N - 1)\n            self.n_interior = N - 2\n            \n            # Grid (interior points only)\n            self.x_interior = np.linspace(0, 1, N)[1:-1]\n            \n            # Desired state on the interior grid\n            self.u_d = np.sin(np.pi * self.x_interior)\n            \n            # Finite difference matrix for -d^2/dx^2 with Dirichlet BCs\n            self._construct_laplacian_matrix()\n\n        def _construct_laplacian_matrix(self):\n            n = self.n_interior\n            h2_inv = 1.0 / self.h**2\n            \n            # A is a symmetric tridiagonal matrix\n            main_diag = np.full(n, 2.0 * h2_inv)\n            off_diag = np.full(n - 1, -1.0 * h2_inv)\n            \n            self.A = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n\n        def solve_state(self, m, u_init=None, tol=1e-12, max_iter=50):\n            \"\"\"\n            Solves the nonlinear state equation using Newton's method.\n            \"\"\"\n            u = np.zeros(self.n_interior) if u_init is None else u_init.copy()\n            \n            for _ in range(max_iter):\n                # Residual: R(u) = A*u + beta*u^3 - m\n                R = self.A @ u + self.beta * u**3 - m\n                norm_R = np.linalg.norm(R)\n                \n                if norm_R  tol:\n                    return u\n                \n                # Jacobian: K(u) = A + 3*beta*diag(u^2)\n                K = self.A + np.diag(3 * self.beta * u**2)\n                \n                # Solve for Newton update: K * du = -R\n                du = np.linalg.solve(K, -R)\n                \n                # Backtracking line search\n                alpha = 1.0\n                u_new = u + alpha * du\n                R_new = self.A @ u_new + self.beta * u_new**3 - m\n                \n                while np.linalg.norm(R_new) >= norm_R and alpha > 1e-8:\n                    alpha *= 0.5\n                    u_new = u + alpha * du\n                    R_new = self.A @ u_new + self.beta * u_new**3 - m\n                \n                u = u_new\n                \n                if np.linalg.norm(alpha * du)  tol:\n                    return u\n                    \n            raise RuntimeError(\"Newton solver for state equation did not converge.\")\n\n        def calculate_objective(self, u, m):\n            \"\"\"Computes the discrete objective functional J(u, m).\"\"\"\n            cost_u = 0.5 * self.h * np.sum((u - self.u_d)**2)\n            cost_m = 0.5 * self.gamma * self.h * np.sum(m**2)\n            return cost_u + cost_m\n\n        def solve_adjoint(self, u):\n            \"\"\"Solves the linear adjoint equation.\"\"\"\n            # Adjoint system matrix: K(u)\n            K = self.A + np.diag(3 * self.beta * u**2)\n            \n            # Right-hand side\n            rhs = -(u - self.u_d)\n            \n            # Solve K * lambda = rhs\n            adj_lambda = np.linalg.solve(K, rhs)\n            return adj_lambda\n\n        def calculate_gradient(self, m, adj_lambda):\n            \"\"\"Computes the gradient of J w.r.t. m.\"\"\"\n            # grad_J = gamma*m - lambda\n            return self.gamma * m - adj_lambda\n\n        def verify(self, seed):\n            \"\"\"Performs the Taylor remainder test to verify the gradient.\"\"\"\n            # Baseline control\n            m_base = np.sin(2 * np.pi * self.x_interior) + 0.5 * np.sin(np.pi * self.x_interior)\n            \n            # --- Baseline Calculations ---\n            u_base = self.solve_state(m_base)\n            J_base = self.calculate_objective(u_base, m_base)\n            \n            # --- Adjoint Gradient Calculation ---\n            adj_base = self.solve_adjoint(u_base)\n            grad_base = self.calculate_gradient(m_base, adj_base)\n            \n            # --- Taylor Test ---\n            rng = np.random.default_rng(seed)\n            dm = rng.standard_normal(self.n_interior)\n            \n            # Normalize dm in the discrete L^2 inner product\n            norm_dm_h = np.sqrt(self.h * np.sum(dm**2))\n            dm_normalized = dm / norm_dm_h\n            \n            # Directional derivative:\n            grad_dot_dm = self.h * np.sum(grad_base * dm_normalized)\n            \n            epsilons = [1e-1, 5e-2, 2.5e-2, 1.25e-2]\n            remainders = []\n            \n            for eps in epsilons:\n                # Perturb control\n                m_pert = m_base + eps * dm_normalized\n                \n                # Solve perturbed state\n                u_pert = self.solve_state(m_pert, u_init=u_base)\n                \n                # Calculate perturbed objective\n                J_pert = self.calculate_objective(u_pert, m_pert)\n                \n                # Taylor remainder\n                R_eps = J_pert - J_base - eps * grad_dot_dm\n                remainders.append(np.abs(R_eps))\n            \n            # Log-log linear regression to find the order of convergence\n            log_eps = np.log(epsilons)\n            log_rem = np.log(remainders)\n            \n            # polyfit returns [slope, intercept]\n            p, _ = np.polyfit(log_eps, log_rem, 1)\n            \n            return p\n\n    results = []\n    \n    test_suite = [\n        {'N': 128, 'beta': 1, 'gamma': 1e-3, 'seed': 7},\n        {'N': 96, 'beta': 0, 'gamma': 1e-2, 'seed': 13},\n        {'N': 160, 'beta': 5, 'gamma': 0, 'seed': 21},\n    ]\n\n    for test_params in test_suite:\n        verifier = AdjointVerifier(N=test_params['N'], beta=test_params['beta'], gamma=test_params['gamma'])\n        order_p = verifier.verify(seed=test_params['seed'])\n        results.append(order_p)\n\n    print(f\"[{results[0]:.3f},{results[1]:.3f},{results[2]:.3f}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在优化问题中，控制变量所在函数空间的选择对算法的性能有深远影响。本练习探讨了一个控制变量位于索博列夫空间 $H_0^1(\\Omega)$ 的问题。你将学习如何通过求解一个额外的偏微分方程（即 Riesz 映射）将伴随方法自然给出的 $L^2$ 梯度转化为更适用于该问题结构的 $H^1$ 梯度，并理解这一过程为何能有效充当梯度下降算法的预处理器。",
            "id": "3361157",
            "problem": "给定一个在单位正方形域 $\\Omega = (0,1) \\times (0,1)$ 上，带有齐次狄利克雷（Dirichlet）边界条件的偏微分方程（PDE）约束二次最优控制问题。状态 $y$ 和控制 $u$ 满足线性椭圆偏微分方程\n$$\n-\\Delta y = u \\quad \\text{in } \\Omega, \\qquad y = 0 \\quad \\text{on } \\partial\\Omega,\n$$\n其中 $\\Delta$ 是拉普拉斯算子。目标泛函为\n$$\nJ(u) = \\tfrac{1}{2}\\int_{\\Omega} (y - y_d)^2 \\, dx + \\tfrac{\\gamma}{2}\\int_{\\Omega} \\|\\nabla u\\|^2 \\, dx,\n$$\n其中 $y_d$ 是给定的期望状态，$\\gamma \\ge 0$ 是一个正则化参数。控制空间是 $H_0^1(\\Omega)$，即在边界上为零且其梯度平方可积的平方可积函数构成的索博列夫（Sobolev）空间。\n\n任务：\n1. 从 Gâteaux 导数和约束优化的拉格朗日量的基本定义出发，推导刻画 $J$ 对状态 $y$ 敏感性的伴随方程。然后，推导 $J$ 关于控制 $u$ 在 $L^2(\\Omega)$ 意义下的梯度表达式。\n2. 使用由\n$$\n\\langle p, q \\rangle_{H_0^1} := \\int_{\\Omega} \\nabla p \\cdot \\nabla q \\, dx,\n$$\n定义的标准 $H_0^1(\\Omega)$ 内积，推导将 $L^2(\\Omega)$ 梯度转换为 $H_0^1(\\Omega)$ 梯度的 Riesz 表示算子（Riesz 映射）。写出为获得 $H_0^1(\\Omega)$ 梯度而必须求解的相应边值问题。\n3. 在具有内部未知数和齐次狄利克雷边界条件的均匀网格上，实现 PDE 和 Riesz 映射的有限差分格式。使用预条件梯度下降算法，其中下降方向是通过将 Riesz 映射应用于伴随方法推导出的 $L^2$ 梯度而获得的 $H_0^1$ 梯度。下降过程使用固定步长。\n4. 对于下面测试套件中的每个测试用例，从零控制开始运行算法，迭代指定的次数，并以浮点数形式报告 $J(u)$ 的最终值。离散目标函数应使用网格间距通过适当的黎曼和来近似连续积分。正则化项的离散梯度应与离散拉普拉斯算子保持一致。\n\n推导的基本要求：\n- 使用泛函的 Gâteaux 导数定义。\n- 对应用于 PDE 的约束优化问题使用拉格朗日乘子法。\n- 使用 $H_0^1(\\Omega)$ 内积的定义和 Riesz 表示定理。\n- 对带有齐次狄利克雷边界条件的拉普拉斯算子使用标准有限差分近似。\n\n数值离散化细节：\n- 设 $N$ 为沿每个轴的内部网格点数，因此网格间距为 $h = \\tfrac{1}{N+1}$。对带有齐次狄利克雷边界条件的拉普拉斯算子使用标准的五点差分格式。\n- 将积分近似为乘以 $h^2$ 的求和。\n- 使用以下选项在内部网格上构建期望状态 $y_d$：\n  - 正弦目标：$y_d(x,y) = s \\sin(\\pi x)\\sin(\\pi y)$，其中尺度 $s  0$。\n  - 高斯目标：$y_d(x,y) = s \\exp\\left(-\\frac{(x - 0.5)^2 + (y - 0.5)^2}{\\sigma^2}\\right)$，其中 $\\sigma = 0.1$ 且尺度 $s  0$。\n  - 零目标：$y_d(x,y) = 0$。\n- 每次迭代的下降方向通过求解在任务2中推导出的 Riesz 映射边值问题得到。\n\n测试套件：\n对于每个元组 $(N, \\gamma, \\tau, \\text{iters}, \\text{target\\_type}, s)$，使用指定的参数：\n- 用例 1：$(32, 0.01, 0.8, 40, \\text{\"sine\"}, 1.0)$\n- 用例 2：$(24, 0.0, 0.6, 30, \\text{\"gauss\"}, 1.0)$\n- 用例 3：$(32, 0.5, 0.5, 50, \\text{\"sine\"}, 1.5)$\n- 用例 4：$(16, 0.1, 1.0, 20, \\text{\"zero\"}, 1.0)$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有测试用例的最终目标值，按顺序排列，形式为用方括号括起来的逗号分隔列表（例如，“[result1,result2,result3,result4]”）。每个结果都必须是浮点数。此问题不涉及物理单位或角度，因此输出中不要包含任何单位。",
            "solution": "所提出的问题是一个标准的PDE约束二次最优控制问题。该问题是适定的，具有科学依据，并为推导和实现提供了所有必要的信息。我们将首先使用伴随方法推导连续最优性条件，然后对系统进行离散化，最后描述指定的梯度下降算法。\n\n### 第 1 部分：伴随方程和 $L^2$ 梯度的推导\n\n问题是在状态方程约束\n$$\n-\\Delta y = u \\quad \\text{in } \\Omega, \\qquad y = 0 \\quad \\text{on } \\partial\\Omega.\n$$\n下，最小化目标泛函\n$$\nJ(u) = \\tfrac{1}{2}\\int_{\\Omega} (y - y_d)^2 \\, dx + \\tfrac{\\gamma}{2}\\int_{\\Omega} \\|\\nabla u\\|^2 \\, dx\n$$\n控制 $u$ 属于索博列夫空间 $H_0^1(\\Omega)$。为了处理该约束，我们引入拉格朗日乘子法。我们定义拉格朗日泛函 $\\mathcal{L}(y, u, p)$，其中 $p$ 是伴随状态（拉格朗日乘子），它位于一个合适的函数空间中（此处，$H_0^1(\\Omega)$ 是合适的）。\n\n$$\n\\mathcal{L}(y, u, p) = J(y, u) + \\int_{\\Omega} p (-\\Delta y - u) \\, dx\n$$\n$$\n\\mathcal{L}(y, u, p) = \\tfrac{1}{2}\\int_{\\Omega} (y - y_d)^2 \\, dx + \\tfrac{\\gamma}{2}\\int_{\\Omega} |\\nabla u|^2 \\, dx - \\int_{\\Omega} p (\\Delta y + u) \\, dx\n$$\n在最优解 $(y, u)$ 处，$\\mathcal{L}$ 关于 $y$、$u$ 和 $p$ 在所有容许方向上的 Gâteaux 导数必须为零。关于 $p$ 在方向 $\\delta p$ 上的导数恰好恢复了状态方程。我们关注关于 $y$ 和 $u$ 的导数。\n\n为了简化包含 $\\Delta y$ 的项，我们使用分部积分（格林第一恒等式），$\\int_{\\Omega} v \\Delta w \\, dx = -\\int_{\\Omega} \\nabla v \\cdot \\nabla w \\, dx + \\int_{\\partial\\Omega} v \\frac{\\partial w}{\\partial n} \\, dS$。由于状态 $y$ 及其变分 $\\delta y$ 在边界 $\\partial\\Omega$ 上为零，我们可以选择伴随状态 $p$ 也具有齐次狄利克雷边界条件，即在 $\\partial\\Omega$ 上 $p=0$。这使得边界积分为零。\n$$\n\\int_{\\Omega} p \\Delta y \\, dx = -\\int_{\\Omega} \\nabla p \\cdot \\nabla y \\, dx\n$$\n再次应用此式可得 $\\int_{\\Omega} p \\Delta y \\, dx = \\int_{\\Omega} y \\Delta p \\, dx$，因为在 $\\partial\\Omega$ 上 $y=0$。\n因此，拉格朗日量可以写成：\n$$\n\\mathcal{L}(y, u, p) = \\tfrac{1}{2}\\int_{\\Omega} (y - y_d)^2 \\, dx + \\tfrac{\\gamma}{2}\\int_{\\Omega} |\\nabla u|^2 \\, dx - \\int_{\\Omega} y (\\Delta p) \\, dx - \\int_{\\Omega} p u \\, dx\n$$\n现在，我们求平稳性条件。$\\mathcal{L}$ 关于 $y$ 在任意方向 $\\delta y$（其中在 $\\partial\\Omega$ 上 $\\delta y=0$）上的 Gâteaux 导数必须为零。\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y}(\\delta y) = \\int_{\\Omega} (y - y_d) \\delta y \\, dx - \\int_{\\Omega} (\\Delta p) \\delta y \\, dx = \\int_{\\Omega} (y - y_d - \\Delta p) \\delta y \\, dx = 0\n$$\n由于这对所有容许的 $\\delta y$ 都必须成立，被积函数必须为零。这就得到了**伴随方程**：\n$$\n-\\Delta p = y - y_d \\quad \\text{in } \\Omega, \\qquad p = 0 \\quad \\text{on } \\partial\\Omega.\n$$\n该方程确定了伴随状态 $p$，它表示目标泛函 $J$ 对状态 $y$ 扰动的敏感性。\n\n接下来，我们求解目标泛函关于控制 $u$ 的梯度。$J$ 关于 $u$ 在方向 $\\delta u$ 上的 Gâteaux 导数由拉格朗日量关于 $u$ 的偏导数给出，该偏导数在满足状态方程和伴随方程的点上计算。\n$$\ndJ(u)[\\delta u] = \\frac{\\partial \\mathcal{L}}{\\partial u}(\\delta u) = \\gamma \\int_{\\Omega} \\nabla u \\cdot \\nabla (\\delta u) \\, dx - \\int_{\\Omega} p \\, \\delta u \\, dx\n$$\n$L^2(\\Omega)$ 梯度（我们记为 $g_{L^2}$）由关系 $dJ(u)[\\delta u] = \\langle g_{L^2}, \\delta u \\rangle_{L^2} = \\int_{\\Omega} g_{L^2} \\, \\delta u \\, dx$ 定义。为了使我们的 $dJ(u)[\\delta u]$ 表达式与此形式匹配，我们对包含 $\\nabla(\\delta u)$ 的项应用分部积分，并利用 $\\delta u \\in H_0^1(\\Omega)$（因此在 $\\partial\\Omega$ 上 $\\delta u=0$）这一事实。\n$$\n\\int_{\\Omega} \\nabla u \\cdot \\nabla (\\delta u) \\, dx = - \\int_{\\Omega} (\\Delta u) \\delta u \\, dx\n$$\n将其代回 $dJ(u)[\\delta u]$ 的表达式中，得到：\n$$\ndJ(u)[\\delta u] = - \\gamma \\int_{\\Omega} (\\Delta u) \\delta u \\, dx - \\int_{\\Omega} p \\, \\delta u \\, dx = \\int_{\\Omega} (-\\gamma \\Delta u - p) \\, \\delta u \\, dx\n$$\n通过将其与 $L^2$ 梯度的定义进行比较，我们确定 **$J$ 关于 $u$ 的 $L^2$ 梯度**为：\n$$\ng_{L^2} \\equiv \\nabla_{L^2}J(u) = -p - \\gamma \\Delta u\n$$\n\n### 第 2 部分：$H_0^1$ 梯度和 Riesz 表示映射\n\n问题指定控制空间是 $H_0^1(\\Omega)$，并配备内积 $\\langle v, w \\rangle_{H_0^1} = \\int_{\\Omega} \\nabla v \\cdot \\nabla w \\, dx$。对于此空间中的优化算法，自然梯度是 $H_0^1$ 梯度，而不是 $L^2$ 梯度。使用 $H_0^1$ 梯度相当于使用一个预条件子，这使得梯度下降法更有效。\n\n$H_0^1$ 梯度（记为 $g_{H_0^1}$）由 Riesz 表示定理定义。对于任何 $\\delta u \\in H_0^1(\\Omega)$，它必须满足：\n$$\ndJ(u)[\\delta u] = \\langle g_{H_0^1}, \\delta u \\rangle_{H_0^1} = \\int_{\\Omega} \\nabla g_{H_0^1} \\cdot \\nabla (\\delta u) \\, dx\n$$\n我们有两个 $dJ(u)[\\delta u]$ 的表达式：一个用 $L^2$ 梯度表示，另一个用 $H_0^1$ 梯度表示。将它们相等可得：\n$$\n\\int_{\\Omega} \\nabla g_{H_0^1} \\cdot \\nabla (\\delta u) \\, dx = \\int_{\\Omega} g_{L^2} \\, \\delta u \\, dx\n$$\n将右侧项移到左侧，并对包含 $\\nabla g_{H_0^1}$ 的项应用分部积分（回顾在 $\\partial\\Omega$ 上 $\\delta u = 0$）：\n$$\n\\int_{\\Omega} \\nabla g_{H_0^1} \\cdot \\nabla (\\delta u) \\, dx = -\\int_{\\Omega} (\\Delta g_{H_0^1}) \\delta u \\, dx\n$$\n因此，我们有：\n$$\n\\int_{\\Omega} (-\\Delta g_{H_0^1} - g_{L^2}) \\, \\delta u \\, dx = 0\n$$\n由于这对所有容许的 $\\delta u$ 都必须成立，我们得到 $g_{H_0^1}$ 方程的强形式：\n$$\n-\\Delta g_{H_0^1} = g_{L^2} \\quad \\text{in } \\Omega, \\qquad g_{H_0^1} = 0 \\quad \\text{on } \\partial\\Omega.\n$$\n代入我们的 $g_{L^2}$ 表达式：\n$$\n-\\Delta g_{H_0^1} = -p - \\gamma \\Delta u\n$$\n这就是定义 Riesz 映射的边值问题。为了找到 $H_0^1$ 梯度，我们必须求解这个泊松（Poisson）方程，其源项是 $L^2$ 梯度。\n\n### 第 3 部分：离散化和算法\n\n我们使用均匀网格对区域 $\\Omega = (0,1) \\times (0,1)$ 进行离散化，每个方向有 $N$ 个内部点，因此网格间距为 $h = 1/(N+1)$。网格上的任何函数，如 $u$，都由其在内部点上的值的 $N \\times N$ 矩阵表示，然后将其展平为大小为 $N^2$ 的向量 $\\mathbf{u}$。\n\n带有齐次狄利克雷边界条件的负拉普拉斯算子 $-\\Delta$ 使用五点差分格式进行离散化。这会产生一个稀疏对称正定矩阵 $A \\in \\mathbb{R}^{N^2 \\times N^2}$，使得 $(A\\mathbf{v})_{i,j} \\approx (-\\Delta v)(x_i, y_j)$。\n\n连续问题的各组成部分被转换为离散线性代数表达式：\n- **状态方程**：$-\\Delta y = u \\implies A\\mathbf{y} = \\mathbf{u}$\n- **伴随方程**：$-\\Delta p = y - y_d \\implies A\\mathbf{p} = \\mathbf{y} - \\mathbf{y}_d$\n- **$L^2$ 梯度**：$g_{L^2} = -p - \\gamma \\Delta u \\implies \\mathbf{g}_{L^2} = -\\mathbf{p} + \\gamma A\\mathbf{u}$\n- **Riesz 映射**：$-\\Delta g_{H_0^1} = g_{L^2} \\implies A\\mathbf{g}_{H_0^1} = \\mathbf{g}_{L^2}$\n\n离散目标泛函 $J_h$ 为：\n$$\nJ_h(\\mathbf{u}) = \\frac{1}{2}h^2 \\|\\mathbf{y} - \\mathbf{y}_d\\|_2^2 + \\frac{\\gamma}{2} h^2 \\mathbf{u}^T A \\mathbf{u}\n$$\n项 $\\mathbf{u}^T A \\mathbf{u}$ 是 $\\int_\\Omega u(-\\Delta u) \\, dx = \\int_\\Omega |\\nabla u|^2 \\, dx$ 的离散模拟。\n\n我们使用带有固定步长 $\\tau$ 的预条件梯度下降算法。下降方向是 $H_0^1$ 梯度的负方向。\n\n**算法：预条件梯度下降**\n1.  初始化控制：$\\mathbf{u}_0 = \\mathbf{0}$。\n2.  对于 $k = 0, 1, \\dots, \\text{iters}-1$：\n    a.  **求解状态方程**：给定 $\\mathbf{u}_k$，求解线性系统 $A\\mathbf{y}_k = \\mathbf{u}_k$ 以找到状态 $\\mathbf{y}_k$。\n    b.  **求解伴随方程**：求解线性系统 $A\\mathbf{p}_k = \\mathbf{y}_k - \\mathbf{y}_d$ 以找到伴随状态 $\\mathbf{p}_k$。\n    c.  **计算 $L^2$ 梯度**：计算 $\\mathbf{g}_{L^2, k} = -\\mathbf{p}_k + \\gamma A\\mathbf{u}_k$。这是一个矩阵向量乘积和向量和。\n    d.  **计算下降方向 (Riesz 映射)**：通过求解线性系统 $A\\mathbf{g}_{H_0^1, k} = \\mathbf{g}_{L^2, k}$ 找到 $H_0^1$ 梯度 $\\mathbf{g}_{H_0^1, k}$。下降方向是 $\\mathbf{d}_k = -\\mathbf{g}_{H_0^1, k}$。\n    e.  **更新控制**：使用固定步长 $\\tau$ 更新控制：$\\mathbf{u}_{k+1} = \\mathbf{u}_k + \\tau \\mathbf{d}_k = \\mathbf{u}_k - \\tau \\mathbf{g}_{H_0^1, k}$。\n3. 在最后一次迭代后，从 $\\mathbf{u}_{\\text{final}}$ 计算最终状态 $\\mathbf{y}_{\\text{final}}$，并评估 $J_h(\\mathbf{u}_{\\text{final}})$。\n每个涉及 $A$ 的步骤都需要求解一个稀疏线性系统，对此有高效的求解器可用。",
            "answer": "```python\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.sparse.linalg import spsolve\n\ndef get_laplacian_matrix(N):\n    \"\"\"\n    Constructs the discrete Laplacian matrix A for the negative Laplacian operator\n    -Delta on a unit square with homogeneous Dirichlet boundary conditions.\n    The grid has N x N interior points.\n    \"\"\"\n    h = 1.0 / (N + 1)\n    \n    # 1D Laplacian matrix\n    D_1d = sparse.diags([-1, 2, -1], [-1, 0, 1], shape=(N, N), format='csc')\n    \n    # 2D Laplacian using Kronecker sum\n    I_n = sparse.identity(N, format='csc')\n    A = sparse.kron(I_n, D_1d) + sparse.kron(D_1d, I_n)\n    \n    return (1 / h**2) * A\n\ndef get_target_state(N, target_type, s):\n    \"\"\"\n    Generates the target state y_d on the N x N interior grid.\n    \"\"\"\n    h = 1.0 / (N + 1)\n    size = N * N\n    \n    if target_type == \"zero\":\n        return np.zeros(size)\n\n    x = np.linspace(h, 1.0 - h, N)\n    y = np.linspace(h, 1.0 - h, N)\n    X, Y = np.meshgrid(x, y)\n\n    if target_type == \"sine\":\n        y_d_grid = s * np.sin(np.pi * X) * np.sin(np.pi * Y)\n    elif target_type == \"gauss\":\n        sigma = 0.1\n        y_d_grid = s * np.exp(-((X - 0.5)**2 + (Y - 0.5)**2) / sigma**2)\n    else:\n        raise ValueError(\"Unknown target type\")\n\n    return y_d_grid.flatten()\n\ndef run_optimization(N, gamma, tau, iters, target_type, s):\n    \"\"\"\n    Runs the preconditioned gradient descent algorithm for one test case.\n    \"\"\"\n    h = 1.0 / (N + 1)\n    size = N * N\n\n    # 1. Setup\n    A = get_laplacian_matrix(N)\n    y_d = get_target_state(N, target_type, s)\n\n    # Initial control\n    u = np.zeros(size)\n\n    # 2. Gradient Descent Loop\n    for _ in range(iters):\n        # a. Solve state equation: A*y = u\n        y = spsolve(A, u)\n        \n        # b. Solve adjoint equation: A*p = y - y_d\n        adjoint_rhs = y - y_d\n        p = spsolve(A, adjoint_rhs)\n\n        # c. Compute L2 gradient: g_l2 = -p + gamma * A * u\n        # Discrete version of g_L2 = -p - gamma * Delta u = -p + gamma * (-Delta u)\n        g_l2 = -p + gamma * A.dot(u)\n\n        # d. Compute H1 gradient (descent direction) by solving Riesz map: A*g_h1 = g_l2\n        g_h1 = spsolve(A, g_l2)\n        \n        # e. Update control\n        u = u - tau * g_h1\n\n    # 3. Final objective value calculation\n    y_final = spsolve(A, u)\n    \n    # Misfit term: 0.5 * integral((y-y_d)^2) dx\n    misfit_term = 0.5 * h**2 * np.sum((y_final - y_d)**2)\n    \n    # Regularization term: 0.5 * gamma * integral(|grad(u)|^2) dx\n    # integral(|grad(u)|^2) dx = integral(u * (-Delta u)) dx\n    # Discrete approx: h^2 * u.T * (A*u)\n    reg_term = 0.5 * gamma * h**2 * np.dot(u, A.dot(u))\n\n    return misfit_term + reg_term\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (N, gamma, tau, iters, target_type, s)\n        (32, 0.01, 0.8, 40, \"sine\", 1.0),\n        (24, 0.0, 0.6, 30, \"gauss\", 1.0),\n        (32, 0.5, 0.5, 50, \"sine\", 1.5),\n        (16, 0.1, 1.0, 20, \"zero\", 1.0)\n    ]\n\n    results = []\n    for case in test_cases:\n        final_J = run_optimization(*case)\n        results.append(final_J)\n\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}