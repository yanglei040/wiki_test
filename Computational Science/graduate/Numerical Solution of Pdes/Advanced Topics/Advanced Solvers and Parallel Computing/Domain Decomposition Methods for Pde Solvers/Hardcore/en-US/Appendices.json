{
    "hands_on_practices": [
        {
            "introduction": "The Additive Schwarz method embodies the \"divide and conquer\" strategy by breaking a large problem into smaller, overlapping ones that can be solved in parallel. This exercise breaks down the application of an Additive Schwarz preconditioner into its fundamental steps for a simple 1D problem. By manually computing one step of the preconditioned conjugate gradient method , you will gain a concrete understanding of how local solves on subdomains are combined to accelerate the solution of the global system.",
            "id": "3382438",
            "problem": "Consider the symmetric positive definite linear system $A x = b$ arising from a one-dimensional ($1$D) finite element (FE) discretization of the Poisson equation with homogeneous Dirichlet boundary conditions on a uniform mesh. The stiffness matrix $A \\in \\mathbb{R}^{4 \\times 4}$ is given by\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 \\\\\n0 & -1 & 2 & -1 \\\\\n0 & 0 & -1 & 2\n\\end{pmatrix}.\n$$\nWe consider an additive Schwarz (AS) preconditioner with two overlapping subdomains. The restriction matrices $R_{1} \\in \\mathbb{R}^{3 \\times 4}$ and $R_{2} \\in \\mathbb{R}^{3 \\times 4}$ are defined by selecting the subdomain degrees of freedom $\\{1,2,3\\}$ and $\\{2,3,4\\}$, respectively:\n$$\nR_{1} \\;=\\; \\begin{pmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\n\\end{pmatrix},\n\\qquad\nR_{2} \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}.\n$$\nThe local subdomain operators are $A_{1} \\;=\\; R_{1} A R_{1}^{\\top}$ and $A_{2} \\;=\\; R_{2} A R_{2}^{\\top}$, and the additive Schwarz preconditioner is\n$$\nM^{-1} \\;=\\; R_{1}^{\\top} A_{1}^{-1} R_{1} \\;+\\; R_{2}^{\\top} A_{2}^{-1} R_{2}.\n$$\nUsing the left-preconditioned Conjugate Gradient (CG) method, initialized at $x^{(0)} = 0$, with residual $r^{(0)} = b - A x^{(0)} = b$, and preconditioned residual $z^{(0)} = M^{-1} r^{(0)}$, the search direction is $p^{(0)} = z^{(0)}$, and the step size is\n$$\n\\alpha_{0} \\;=\\; \\frac{(r^{(0)}, z^{(0)})}{(p^{(0)}, A p^{(0)})},\n$$\nwhere $(\\cdot,\\cdot)$ is the standard Euclidean inner product on $\\mathbb{R}^{4}$. Take the right-hand side to be\n$$\nb \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nCompute the exact value of the first step size $\\alpha_{0}$ for this additive Schwarz preconditioned CG iteration. Give your answer as an exact real number (no rounding).",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. It is a standard calculation in the field of numerical linear algebra and domain decomposition methods. I will proceed with the solution.\n\nThe objective is to compute the first step size, $\\alpha_{0}$, for the preconditioned Conjugate Gradient (CG) method. The formula for $\\alpha_{0}$ is given by:\n$$\n\\alpha_{0} \\;=\\; \\frac{(r^{(0)}, z^{(0)})}{(p^{(0)}, A p^{(0)})}\n$$\nwhere $(\\cdot,\\cdot)$ denotes the standard Euclidean inner product. This can be written using vector transposes as:\n$$\n\\alpha_{0} \\;=\\; \\frac{(r^{(0)})^{\\top} z^{(0)}}{(p^{(0)})^{\\top} (A p^{(0)})}\n$$\nWe will compute each term in this expression step-by-step.\n\nFirst, we determine the initial residual $r^{(0)}$. With the initial guess $x^{(0)} = 0$ and the right-hand side vector $b$ given as $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$, we have:\n$$\nr^{(0)} \\;=\\; b - A x^{(0)} \\;=\\; b - A \\cdot 0 \\;=\\; b \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n\nNext, we compute the preconditioned residual $z^{(0)} = M^{-1} r^{(0)}$. This requires the construction of the additive Schwarz preconditioner $M^{-1}$.\nThe preconditioner is defined as:\n$$\nM^{-1} \\;=\\; R_{1}^{\\top} A_{1}^{-1} R_{1} \\;+\\; R_{2}^{\\top} A_{2}^{-1} R_{2}\n$$\nwhere $A_{1} \\;=\\; R_{1} A R_{1}^{\\top}$ and $A_{2} \\;=\\; R_{2} A R_{2}^{\\top}$.\n\nLet's compute the subdomain stiffness matrices $A_{1}$ and $A_{2}$.\nThe matrix $A$ is given by:\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 \\\\\n0 & -1 & 2 & -1 \\\\\n0 & 0 & -1 & 2\n\\end{pmatrix}\n$$\nAnd the restriction matrices are:\n$$\nR_{1} \\;=\\; \\begin{pmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\n\\end{pmatrix},\n\\qquad\nR_{2} \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nThe operation $R_{1} A R_{1}^{\\top}$ extracts the principal submatrix of $A$ corresponding to the first three indices.\n$$\nA_1 \\;=\\; R_{1} A R_{1}^{\\top} \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}\n$$\nSimilarly, the operation $R_{2} A R_{2}^{\\top}$ extracts the principal submatrix of $A$ corresponding to the last three indices (indices $2$, $3$, $4$).\n$$\nA_2 \\;=\\; R_{2} A R_{2}^{\\top} \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}\n$$\nWe observe that $A_{1} = A_{2}$. Let's denote this $3 \\times 3$ matrix as $A_{sub}$. We need to compute its inverse, $A_{sub}^{-1}$.\nThe determinant of $A_{sub}$ is:\n$$\n\\det(A_{sub}) \\;=\\; 2(2 \\cdot 2 - (-1)(-1)) - (-1)(-1 \\cdot 2 - (-1) \\cdot 0) \\;=\\; 2(3) - 2 \\;=\\; 4\n$$\nThe inverse is given by $A_{sub}^{-1} = \\frac{1}{\\det(A_{sub})} \\text{adj}(A_{sub})$. The adjugate matrix is the transpose of the cofactor matrix.\nThe cofactor matrix $C$ of $A_{sub}$ is:\n$$\nC \\;=\\; \\begin{pmatrix}\n(4-1) & -(-2-0) & (1-0) \\\\\n-(-2-0) & (4-0) & -(-2-0) \\\\\n(1-0) & -(-2-0) & (4-1)\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n3 & 2 & 1 \\\\\n2 & 4 & 2 \\\\\n1 & 2 & 3\n\\end{pmatrix}\n$$\nSince $C$ is symmetric, $\\text{adj}(A_{sub}) = C^{\\top} = C$. Therefore:\n$$\nA_{1}^{-1} \\;=\\; A_{2}^{-1} \\;=\\; A_{sub}^{-1} \\;=\\; \\frac{1}{4} \\begin{pmatrix}\n3 & 2 & 1 \\\\\n2 & 4 & 2 \\\\\n1 & 2 & 3\n\\end{pmatrix}\n$$\nNow we assemble the preconditioner $M^{-1}$. The term $R_{1}^{\\top} A_{1}^{-1} R_{1}$ embeds $A_{1}^{-1}$ into the top-left $3 \\times 3$ block of a $4 \\times 4$ zero matrix. The term $R_{2}^{\\top} A_{2}^{-1} R_{2}$ embeds $A_{2}^{-1}$ into the bottom-right $3 \\times 3$ block (with indices $2,3,4$) of a $4 \\times 4$ zero matrix.\n$$\nR_{1}^{\\top} A_{1}^{-1} R_{1} \\;=\\; \\frac{1}{4} \\begin{pmatrix}\n3 & 2 & 1 & 0 \\\\\n2 & 4 & 2 & 0 \\\\\n1 & 2 & 3 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}\n$$\n$$\nR_{2}^{\\top} A_{2}^{-1} R_{2} \\;=\\; \\frac{1}{4} \\begin{pmatrix}\n0 & 0 & 0 & 0 \\\\\n0 & 3 & 2 & 1 \\\\\n0 & 2 & 4 & 2 \\\\\n0 & 1 & 2 & 3\n\\end{pmatrix}\n$$\nAdding these two matrices gives $M^{-1}$:\n$$\nM^{-1} \\;=\\; \\frac{1}{4} \\begin{pmatrix}\n3 & 2 & 1 & 0 \\\\\n2 & 4+3 & 2+2 & 1 \\\\\n1 & 2+2 & 3+4 & 2 \\\\\n0 & 1 & 2 & 3\n\\end{pmatrix} \\;=\\; \\frac{1}{4} \\begin{pmatrix}\n3 & 2 & 1 & 0 \\\\\n2 & 7 & 4 & 1 \\\\\n1 & 4 & 7 & 2 \\\\\n0 & 1 & 2 & 3\n\\end{pmatrix}\n$$\nNow we can compute $z^{(0)}$:\n$$\nz^{(0)} \\;=\\; M^{-1} r^{(0)} \\;=\\; \\frac{1}{4} \\begin{pmatrix}\n3 & 2 & 1 & 0 \\\\\n2 & 7 & 4 & 1 \\\\\n1 & 4 & 7 & 2 \\\\\n0 & 1 & 2 & 3\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\;=\\; \\frac{1}{4} \\begin{pmatrix} 3 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nWe can now calculate the numerator of $\\alpha_{0}$:\n$$\n(r^{(0)})^{\\top} z^{(0)} \\;=\\; \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} \\left( \\frac{1}{4} \\begin{pmatrix} 3 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right) \\;=\\; \\frac{3}{4}\n$$\nNext, we compute the denominator. The initial search direction is $p^{(0)} = z^{(0)}$. So, we need to calculate $(p^{(0)})^{\\top} (A p^{(0)})$.\nFirst, we compute the product $A p^{(0)}$:\n$$\nA p^{(0)} \\;=\\; A z^{(0)} \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 \\\\\n0 & -1 & 2 & -1 \\\\\n0 & 0 & -1 & 2\n\\end{pmatrix} \\left( \\frac{1}{4} \\begin{pmatrix} 3 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right) \\;=\\; \\frac{1}{4} \\begin{pmatrix}\n2(3) - 1(2) \\\\\n-1(3) + 2(2) - 1(1) \\\\\n-1(2) + 2(1) \\\\\n-1(1)\n\\end{pmatrix} \\;=\\; \\frac{1}{4} \\begin{pmatrix}\n4 \\\\ 0 \\\\ 0 \\\\ -1\n\\end{pmatrix}\n$$\nNow, we compute the inner product for the denominator:\n$$\n(p^{(0)})^{\\top} (A p^{(0)}) \\;=\\; (z^{(0)})^{\\top} (A z^{(0)}) \\;=\\; \\left( \\frac{1}{4} \\begin{pmatrix} 3 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right)^{\\top} \\left( \\frac{1}{4} \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix} \\right)\n$$\n$$\n(p^{(0)})^{\\top} (A p^{(0)}) \\;=\\; \\frac{1}{16} \\begin{pmatrix} 3 & 2 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix} \\;=\\; \\frac{1}{16} (3 \\cdot 4 + 2 \\cdot 0 + 1 \\cdot 0 + 0 \\cdot (-1)) \\;=\\; \\frac{12}{16} \\;=\\; \\frac{3}{4}\n$$\nFinally, we compute $\\alpha_{0}$ by taking the ratio of the numerator and the denominator:\n$$\n\\alpha_{0} \\;=\\; \\frac{(r^{(0)})^{\\top} z^{(0)}}{(p^{(0)})^{\\top} (A p^{(0)})} \\;=\\; \\frac{3/4}{3/4} \\;=\\; 1\n$$\nThe exact value of the first step size is $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Real-world engineering problems often involve complex geometries that are best discretized with non-matching grids across different components. This exercise introduces the mortar method, a sophisticated technique for coupling such non-conforming discretizations without forcing the meshes to align. You will assemble the coupling matrices that enforce continuity in a weak sense and solve the resulting saddle-point system , providing a hands-on look at the machinery of advanced non-conforming domain decomposition.",
            "id": "3382444",
            "problem": "Consider the scalar diffusion problem on the rectangular domain $\\Omega = [-1,1] \\times [0,1]$ with a vertical interface $\\Gamma = \\{0\\} \\times [0,1]$ that splits $\\Omega$ into two subdomains: $\\Omega_{1} = [-1,0] \\times [0,1]$ and $\\Omega_{2} = [0,1] \\times [0,1]$. The diffusion coefficient is piecewise constant, with $\\alpha_{1} = 2$ in $\\Omega_{1}$ and $\\alpha_{2} = 1$ in $\\Omega_{2}$. The governing equation in each subdomain is the weak form of $-\\nabla \\cdot (\\alpha \\nabla u) = 0$, and after eliminating interior degrees of freedom by static condensation, the subdomain interface problems reduce to symmetric positive definite (SPD) linear systems posed on the trace spaces of $\\Gamma$.\n\nAdopt a mortar domain decomposition with mismatched meshes on $\\Gamma$:\n- On the master side (trace of $\\Omega_{1}$), use a continuous, piecewise linear trace space with nodal points $y = 0,\\, 0.5,\\, 1$, and corresponding nodal basis functions $\\{\\phi_{1}, \\phi_{2}, \\phi_{3}\\}$.\n- On the slave side (trace of $\\Omega_{2}$), use a continuous, piecewise linear trace space with nodal points $y = 0,\\, \\tfrac{1}{3},\\, \\tfrac{2}{3},\\, 1$, and corresponding nodal basis functions $\\{\\psi_{1}, \\psi_{2}, \\psi_{3}, \\psi_{4}\\}$.\n- Use a mortar test space consisting of piecewise constants on two equal interface subintervals, $\\chi_{1}$ supported on $[0,0.5]$ and $\\chi_{2}$ supported on $[0.5,1]$.\n\nThe mortar coupling enforces weak continuity of the trace by $L^{2}$ projection: for any test $\\mu$ in the mortar space, the jump vanishes in the $L^{2}$ sense, i.e., $\\int_{\\Gamma} \\mu \\,(u_{1|\\Gamma} - u_{2|\\Gamma}) \\,\\mathrm{d}s = 0$. This yields linear constraints of the form $A \\mathbf{a} - B \\mathbf{b} = \\mathbf{0}$, where $\\mathbf{a} \\in \\mathbb{R}^{3}$ are the master trace coefficients, $\\mathbf{b} \\in \\mathbb{R}^{4}$ are the slave trace coefficients, and\n$$\nA_{i,k} = \\int_{0}^{1} \\chi_{i}(y) \\,\\phi_{k}(y)\\,\\mathrm{d}y, \n\\qquad\nB_{i,j} = \\int_{0}^{1} \\chi_{i}(y) \\,\\psi_{j}(y)\\,\\mathrm{d}y.\n$$\n\nAssume the subdomain Schur complements on the interface are approximated by mass-lumped SPD operators,\n$$\nS_{1} = 2 I_{3}, \\qquad S_{2} = I_{4},\n$$\nconsistent with piecewise constant Steklov–Poincaré surrogates weighted by the diffusion coefficients. After interior elimination, suppose the effective condensed load vectors are\n$$\n\\mathbf{r}_{1} = \\begin{pmatrix} 0 \\\\ \\tfrac{1}{10} \\\\ 0 \\end{pmatrix},\n\\qquad\n\\mathbf{r}_{2} = \\begin{pmatrix} 0 \\\\ \\tfrac{1}{20} \\\\ \\tfrac{1}{20} \\\\ 0 \\end{pmatrix}.\n$$\n\nForm the Karush–Kuhn–Tucker (KKT) saddle-point system for the constrained minimization of the interface energy subject to the mortar constraints,\n$$\n\\begin{pmatrix}\nS_{1} & 0 & A^{\\top} \\\\\n0 & S_{2} & -B^{\\top} \\\\\nA & -B & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{a} \\\\ \\mathbf{b} \\\\ \\boldsymbol{\\lambda}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{r}_{1} \\\\ \\mathbf{r}_{2} \\\\ \\mathbf{0}\n\\end{pmatrix},\n$$\nwhere $\\boldsymbol{\\lambda} \\in \\mathbb{R}^{2}$ are Lagrange multipliers enforcing the mortar constraints. Using the explicit definitions of the basis functions, first assemble the mortar coupling matrices $A$ and $B$ by direct $L^{2}$ integration. Then enforce the constraints and solve the resulting KKT system to determine the master trace coefficient $\\mathbf{a} = (a_{1}, a_{2}, a_{3})^{\\top}$. \n\nWhat is the exact value of the central master trace coefficient $a_{2}$? Provide your answer as a single exact fraction with no units.",
            "solution": "We start from the variational formulation of the scalar diffusion problem and the principle of domain decomposition: after eliminating interior degrees of freedom, the interface unknowns satisfy symmetric positive definite (SPD) problems characterized by subdomain Schur complements, and interfacial continuity is enforced weakly via mortar constraints. The weak continuity in the $L^{2}$ sense on $\\Gamma$ is\n$$\n\\int_{\\Gamma} \\mu \\,\\big(u_{1|\\Gamma} - u_{2|\\Gamma}\\big)\\,\\mathrm{d}s = 0 \\quad \\text{for all mortar tests } \\mu,\n$$\nwhich, upon expansion in the chosen trace bases, produces a linear set of constraints linking the master coefficients $\\mathbf{a}$ and the slave coefficients $\\mathbf{b}$ through coupling matrices assembled by $L^{2}$ projection.\n\nStep 1: Define the trace basis functions on $\\Gamma \\equiv [0,1]$.\n\n- Master (nodes $y=0,\\,0.5,\\,1$):\n  - $\\phi_{1}(y)$ supported on $[0,0.5]$, with $\\phi_{1}(y) = 1 - 2y$ for $y \\in [0,0.5]$, and $0$ otherwise.\n  - $\\phi_{2}(y)$ supported on $[0,1]$, with $\\phi_{2}(y) = 2y$ on $[0,0.5]$ and $\\phi_{2}(y) = 2(1-y)$ on $[0.5,1]$.\n  - $\\phi_{3}(y)$ supported on $[0.5,1]$, with $\\phi_{3}(y) = 2y - 1$ for $y \\in [0.5,1]$, and $0$ otherwise.\n\n- Slave (nodes $y=0,\\,\\tfrac{1}{3},\\,\\tfrac{2}{3},\\,1$):\n  - $\\psi_{1}(y)$ supported on $[0,\\tfrac{1}{3}]$, with $\\psi_{1}(y) = 1 - 3y$ there.\n  - $\\psi_{2}(y)$ supported on $[0,\\tfrac{2}{3}]$, with $\\psi_{2}(y) = 3y$ on $[0,\\tfrac{1}{3}]$ and $\\psi_{2}(y) = 2 - 3y$ on $[\\tfrac{1}{3},\\tfrac{2}{3}]$.\n  - $\\psi_{3}(y)$ supported on $[\\tfrac{1}{3},1]$, with $\\psi_{3}(y) = 3y - 1$ on $[\\tfrac{1}{3},\\tfrac{2}{3}]$ and $\\psi_{3}(y) = 3 - 3y$ on $[\\tfrac{2}{3},1]$.\n  - $\\psi_{4}(y)$ supported on $[\\tfrac{2}{3},1]$, with $\\psi_{4}(y) = 3y - 2$ there.\n\n- Mortar tests (piecewise constants):\n  - $\\chi_{1}(y) = 1$ for $y \\in [0,0.5]$, and $0$ otherwise.\n  - $\\chi_{2}(y) = 1$ for $y \\in [0.5,1]$, and $0$ otherwise.\n\nStep 2: Assemble the mortar coupling matrices $A$ and $B$ via $L^{2}$ inner products,\n$$\nA_{i,k} = \\int_{0}^{1} \\chi_{i}(y)\\,\\phi_{k}(y)\\,\\mathrm{d}y, \\qquad B_{i,j} = \\int_{0}^{1} \\chi_{i}(y)\\,\\psi_{j}(y)\\,\\mathrm{d}y.\n$$\n\nFor $A$:\n\n$$\n\\begin{aligned}\nA_{1,1} &= \\int_{0}^{0.5} (1-2y)\\,\\mathrm{d}y = \\left[y - y^{2}\\right]_{0}^{0.5} = \\tfrac{1}{2} - \\tfrac{1}{4} = \\tfrac{1}{4}, \\\\\nA_{1,2} &= \\int_{0}^{0.5} 2y\\,\\mathrm{d}y = \\left[y^{2}\\right]_{0}^{0.5} = \\tfrac{1}{4}, \\\\\nA_{1,3} &= 0, \\\\\nA_{2,1} &= 0, \\\\\nA_{2,2} &= \\int_{0.5}^{1} 2(1-y)\\,\\mathrm{d}y = 2\\left[y - \\tfrac{y^{2}}{2}\\right]_{0.5}^{1} = 2\\left(\\tfrac{1}{2} - \\tfrac{3}{8}\\right) = \\tfrac{1}{4}, \\\\\nA_{2,3} &= \\int_{0.5}^{1} (2y-1)\\,\\mathrm{d}y = \\left[y^{2} - y\\right]_{0.5}^{1} = \\left(1-1\\right) - \\left(\\tfrac{1}{4} - \\tfrac{1}{2}\\right) = \\tfrac{1}{4}.\n\\end{aligned}\n$$\n\nThus,\n$$\nA = \\begin{pmatrix}\n\\tfrac{1}{4} & \\tfrac{1}{4} & 0 \\\\\n0 & \\tfrac{1}{4} & \\tfrac{1}{4}\n\\end{pmatrix}.\n$$\n\nFor $B$ (row $1$ on $[0,0.5]$):\n\n$$\n\\begin{aligned}\n\\int_{0}^{0.5} \\psi_{1}(y)\\,\\mathrm{d}y &= \\int_{0}^{1/3} (1 - 3y)\\,\\mathrm{d}y = \\left[y - \\tfrac{3y^{2}}{2}\\right]_{0}^{1/3} = \\tfrac{1}{3} - \\tfrac{1}{6} = \\tfrac{1}{6}, \\\\\n\\int_{0}^{0.5} \\psi_{2}(y)\\,\\mathrm{d}y &= \\int_{0}^{1/3} 3y\\,\\mathrm{d}y + \\int_{1/3}^{0.5} (2 - 3y)\\,\\mathrm{d}y = \\tfrac{1}{6} + \\tfrac{1}{8} = \\tfrac{7}{24}, \\\\\n\\int_{0}^{0.5} \\psi_{3}(y)\\,\\mathrm{d}y &= \\int_{1/3}^{0.5} (3y - 1)\\,\\mathrm{d}y = \\left[\\tfrac{3y^{2}}{2} - y\\right]_{1/3}^{0.5} = -\\tfrac{1}{8} + \\tfrac{1}{6} = \\tfrac{1}{24}, \\\\\n\\int_{0}^{0.5} \\psi_{4}(y)\\,\\mathrm{d}y &= 0.\n\\end{aligned}\n$$\n\nFor $B$ (row $2$ on $[0.5,1]$):\n\n$$\n\\begin{aligned}\n\\int_{0.5}^{1} \\psi_{1}(y)\\,\\mathrm{d}y &= 0, \\\\\n\\int_{0.5}^{1} \\psi_{2}(y)\\,\\mathrm{d}y &= \\int_{0.5}^{2/3} (2 - 3y)\\,\\mathrm{d}y = \\tfrac{1}{24}, \\\\\n\\int_{0.5}^{1} \\psi_{3}(y)\\,\\mathrm{d}y &= \\int_{0.5}^{2/3} (3y - 1)\\,\\mathrm{d}y + \\int_{2/3}^{1} (3 - 3y)\\,\\mathrm{d}y = \\tfrac{1}{8} + \\tfrac{1}{6} = \\tfrac{7}{24}, \\\\\n\\int_{0.5}^{1} \\psi_{4}(y)\\,\\mathrm{d}y &= \\int_{2/3}^{1} (3y - 2)\\,\\mathrm{d}y = \\tfrac{1}{6}.\n\\end{aligned}\n$$\n\nThus,\n$$\nB = \\begin{pmatrix}\n\\tfrac{1}{6} & \\tfrac{7}{24} & \\tfrac{1}{24} & 0 \\\\\n0 & \\tfrac{1}{24} & \\tfrac{7}{24} & \\tfrac{1}{6}\n\\end{pmatrix}.\n$$\n\nStep 3: Write the Karush–Kuhn–Tucker (KKT) equations. With $S_{1} = 2 I_{3}$ and $S_{2} = I_{4}$, the KKT system is\n$$\n\\begin{cases}\n2\\mathbf{a} + A^{\\top}\\boldsymbol{\\lambda} = \\mathbf{r}_{1}, \\\\\n\\mathbf{b} - B^{\\top}\\boldsymbol{\\lambda} = \\mathbf{r}_{2}, \\\\\nA \\mathbf{a} - B \\mathbf{b} = \\mathbf{0}.\n\\end{cases}\n$$\nEliminate $\\mathbf{a}$ and $\\mathbf{b}$:\n$$\n\\mathbf{a} = \\dfrac{\\mathbf{r}_{1} - A^{\\top}\\boldsymbol{\\lambda}}{2}, \n\\qquad\n\\mathbf{b} = \\mathbf{r}_{2} + B^{\\top}\\boldsymbol{\\lambda}.\n$$\nSubstitute into the constraint:\n$$\nA\\left(\\dfrac{\\mathbf{r}_{1} - A^{\\top}\\boldsymbol{\\lambda}}{2}\\right) - B\\left(\\mathbf{r}_{2} + B^{\\top}\\boldsymbol{\\lambda}\\right) = \\mathbf{0},\n$$\nwhich simplifies to\n$$\n\\left(\\dfrac{A A^{\\top}}{2} + B B^{\\top}\\right)\\boldsymbol{\\lambda} = \\dfrac{A \\mathbf{r}_{1}}{2} - B \\mathbf{r}_{2}.\n$$\n\nStep 4: Compute $A A^{\\top}$, $B B^{\\top}$, $A \\mathbf{r}_{1}$, and $B \\mathbf{r}_{2}$.\n\nCompute $A A^{\\top}$:\n\n$$\nA A^{\\top} = \n\\begin{pmatrix}\n\\left(\\tfrac{1}{4}\\right)^{2} + \\left(\\tfrac{1}{4}\\right)^{2} & \\tfrac{1}{4}\\cdot 0 + \\tfrac{1}{4}\\cdot \\tfrac{1}{4} + 0\\cdot \\tfrac{1}{4} \\\\\n\\text{sym.} & \\left(\\tfrac{1}{4}\\right)^{2} + \\left(\\tfrac{1}{4}\\right)^{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\tfrac{1}{8} & \\tfrac{1}{16} \\\\\n\\tfrac{1}{16} & \\tfrac{1}{8}\n\\end{pmatrix}.\n$$\n\nCompute $B B^{\\top}$:\n\n$$\n\\begin{aligned}\n(B B^{\\top})_{11} &= \\left(\\tfrac{1}{6}\\right)^{2} + \\left(\\tfrac{7}{24}\\right)^{2} + \\left(\\tfrac{1}{24}\\right)^{2} = \\tfrac{1}{36} + \\tfrac{49}{576} + \\tfrac{1}{576} = \\tfrac{11}{96}, \\\\\n(B B^{\\top})_{12} &= \\tfrac{7}{24}\\cdot \\tfrac{1}{24} + \\tfrac{1}{24}\\cdot \\tfrac{7}{24} = \\tfrac{7}{576} + \\tfrac{7}{576} = \\tfrac{7}{288}, \\\\\n(B B^{\\top})_{22} &= \\left(\\tfrac{1}{24}\\right)^{2} + \\left(\\tfrac{7}{24}\\right)^{2} + \\left(\\tfrac{1}{6}\\right)^{2} = \\tfrac{1}{576} + \\tfrac{49}{576} + \\tfrac{1}{36} = \\tfrac{11}{96}.\n\\end{aligned}\n$$\n\nThus,\n$$\nB B^{\\top} = \\begin{pmatrix} \\tfrac{11}{96} & \\tfrac{7}{288} \\\\ \\tfrac{7}{288} & \\tfrac{11}{96} \\end{pmatrix}.\n$$\nCompute $\\dfrac{A A^{\\top}}{2} + B B^{\\top}$:\n\n$$\n\\dfrac{A A^{\\top}}{2} = \\begin{pmatrix} \\tfrac{1}{16} & \\tfrac{1}{32} \\\\ \\tfrac{1}{32} & \\tfrac{1}{16} \\end{pmatrix},\n\\quad\n\\Rightarrow\n\\quad\nH := \\dfrac{A A^{\\top}}{2} + B B^{\\top} = \\begin{pmatrix} \\tfrac{17}{96} & \\tfrac{1}{18} \\\\ \\tfrac{1}{18} & \\tfrac{17}{96} \\end{pmatrix}.\n$$\n\nCompute $\\dfrac{A \\mathbf{r}_{1}}{2}$ and $B \\mathbf{r}_{2}$:\n\n$$\nA \\mathbf{r}_{1} = \\begin{pmatrix} \\tfrac{1}{4}\\cdot 0 + \\tfrac{1}{4}\\cdot \\tfrac{1}{10} + 0\\cdot 0 \\\\ 0\\cdot 0 + \\tfrac{1}{4}\\cdot \\tfrac{1}{10} + \\tfrac{1}{4}\\cdot 0 \\end{pmatrix} = \\begin{pmatrix} \\tfrac{1}{40} \\\\ \\tfrac{1}{40} \\end{pmatrix},\n\\quad\n\\dfrac{A \\mathbf{r}_{1}}{2} = \\begin{pmatrix} \\tfrac{1}{80} \\\\ \\tfrac{1}{80} \\end{pmatrix}.\n$$\n\n\n$$\nB \\mathbf{r}_{2} = \\begin{pmatrix}\n\\tfrac{1}{6}\\cdot 0 + \\tfrac{7}{24}\\cdot \\tfrac{1}{20} + \\tfrac{1}{24}\\cdot \\tfrac{1}{20} + 0\\cdot 0 \\\\\n0\\cdot 0 + \\tfrac{1}{24}\\cdot \\tfrac{1}{20} + \\tfrac{7}{24}\\cdot \\tfrac{1}{20} + \\tfrac{1}{6}\\cdot 0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\tfrac{8}{480} \\\\ \\tfrac{8}{480}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\tfrac{1}{60} \\\\ \\tfrac{1}{60}\n\\end{pmatrix}.\n$$\n\nTherefore the right-hand side is\n\n$$\n\\mathbf{g} := \\dfrac{A \\mathbf{r}_{1}}{2} - B \\mathbf{r}_{2} = \\begin{pmatrix} \\tfrac{1}{80} - \\tfrac{1}{60} \\\\ \\tfrac{1}{80} - \\tfrac{1}{60} \\end{pmatrix} = \\begin{pmatrix} -\\tfrac{1}{240} \\\\ -\\tfrac{1}{240} \\end{pmatrix}.\n$$\n\n\nStep 5: Solve $H \\boldsymbol{\\lambda} = \\mathbf{g}$. Noting the symmetry $H_{11} = H_{22}$ and $H_{12} = H_{21}$ as well as equal components of $\\mathbf{g}$, we look for $\\boldsymbol{\\lambda} = (t, t)^{\\top}$. Then\n\n$$\n\\left(\\tfrac{17}{96} + \\tfrac{1}{18}\\right) t = -\\tfrac{1}{240}.\n$$\n\nCompute the coefficient:\n\n$$\n\\tfrac{17}{96} + \\tfrac{1}{18} = \\tfrac{51}{288} + \\tfrac{16}{288} = \\tfrac{67}{288}.\n$$\n\nHence\n\n$$\n\\tfrac{67}{288}\\, t = -\\tfrac{1}{240}\n\\quad \\Rightarrow \\quad\nt = -\\tfrac{1}{240} \\cdot \\tfrac{288}{67} = -\\tfrac{288}{240 \\cdot 67} = -\\tfrac{6}{335}.\n$$\n\nTherefore $\\boldsymbol{\\lambda} = \\left(-\\tfrac{6}{335}, -\\tfrac{6}{335}\\right)^{\\top}$.\n\nStep 6: Recover $\\mathbf{a}$ and extract $a_{2}$. From\n\n$$\n\\mathbf{a} = \\dfrac{\\mathbf{r}_{1} - A^{\\top}\\boldsymbol{\\lambda}}{2},\n$$\n\ncompute $A^{\\top}\\boldsymbol{\\lambda}$ using\n\n$$\nA^{\\top} = \n\\begin{pmatrix}\n\\tfrac{1}{4} & 0 \\\\\n\\tfrac{1}{4} & \\tfrac{1}{4} \\\\\n0 & \\tfrac{1}{4}\n\\end{pmatrix},\n\\quad\n\\boldsymbol{\\lambda} = \\begin{pmatrix} -\\tfrac{6}{335} \\\\ -\\tfrac{6}{335} \\end{pmatrix}.\n$$\n\nThus\n\n$$\nA^{\\top}\\boldsymbol{\\lambda} = \n\\begin{pmatrix}\n\\tfrac{1}{4}\\left(-\\tfrac{6}{335}\\right) + 0 \\\\\n\\tfrac{1}{4}\\left(-\\tfrac{6}{335}\\right) + \\tfrac{1}{4}\\left(-\\tfrac{6}{335}\\right) \\\\\n0 + \\tfrac{1}{4}\\left(-\\tfrac{6}{335}\\right)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-\\tfrac{3}{670} \\\\\n-\\tfrac{3}{335} \\\\\n-\\tfrac{3}{670}\n\\end{pmatrix}.\n$$\n\nTherefore\n\n$$\n\\mathbf{a} = \\dfrac{1}{2}\n\\begin{pmatrix}\n0 - \\left(-\\tfrac{3}{670}\\right) \\\\\n\\tfrac{1}{10} - \\left(-\\tfrac{3}{335}\\right) \\\\\n0 - \\left(-\\tfrac{3}{670}\\right)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\tfrac{3}{1340} \\\\\n\\dfrac{\\tfrac{1}{10} + \\tfrac{3}{335}}{2} \\\\\n\\tfrac{3}{1340}\n\\end{pmatrix}.\n$$\n\nCompute $a_{2}$:\n\n$$\na_{2} = \\dfrac{\\tfrac{1}{10} + \\tfrac{3}{335}}{2} = \\dfrac{\\tfrac{67}{670} + \\tfrac{6}{670}}{2} = \\dfrac{\\tfrac{73}{670}}{2} = \\tfrac{73}{1340}.\n$$\n\n\nThe central master trace coefficient $a_{2}$ is therefore the exact fraction $\\tfrac{73}{1340}$.",
            "answer": "$$\\boxed{\\frac{73}{1340}}$$"
        },
        {
            "introduction": "While one-level Schwarz methods are intuitive, their convergence degrades as we use more subdomains, limiting their scalability on parallel computers. The solution is to add a second, coarse-level correction that handles the global, low-frequency error components. This coding exercise  guides you through implementing a complete two-level additive Schwarz method and numerically verifying its scalability, a cornerstone result that makes domain decomposition a powerful tool for large-scale scientific computing.",
            "id": "3382437",
            "problem": "Consider the one-dimensional Poisson equation on the unit interval with homogeneous Dirichlet boundary conditions, whose strong form is given by $-u''(x)=f(x)$ for $x\\in(0,1)$ with $u(0)=u(1)=0$. Discretize the problem using the Finite Difference Method (FDM) on $N$ interior points with uniform spacing $h=1/(N+1)$ to obtain a Symmetric Positive Definite (SPD) linear system $A u = b$, where $A\\in\\mathbb{R}^{N\\times N}$ is the standard tridiagonal stiffness matrix with entries $A_{ii}=\\frac{2}{h^2}$ and $A_{i,i\\pm 1}=-\\frac{1}{h^2}$.\n\nPartition the $N$ degrees of freedom into $m$ contiguous subdomains forming a one-dimensional chain, and define for each subdomain an overlapping local index set with an overlap of $p$ fine-grid points. For each subdomain, let $R_i\\in\\mathbb{R}^{n_i\\times N}$ be the restriction operator that extracts the local unknowns corresponding to the overlapped subdomain index set, and define the local SPD matrix $A_i=R_i A R_i^T$. Construct a two-level additive Schwarz preconditioner $M^{-1}$ of the form\n$$\nM^{-1} \\;=\\; \\sum_{i=1}^m R_i^T A_i^{-1} R_i \\;+\\; R_0^T A_0^{-1} R_0,\n$$\nwhere $R_0\\in\\mathbb{R}^{(m-1)\\times N}$ is a coarse restriction operator built from an algebraic coarse space based on piecewise linear basis vectors over the one-dimensional chain of subdomains. Specifically, define the coarse mesh points as $x_j=jH$ for $j=0,1,\\dots,m$ with $H=1/m$, and form $(m-1)$ hat functions $\\{\\phi_j\\}_{j=1}^{m-1}$ supported on $[x_{j-1},x_{j+1}]$, each satisfying $\\phi_j(x_j)=1$ and $\\phi_j(x_{j\\pm 1})=0$, with linear variation on $[x_{j-1},x_j]$ and $[x_j,x_{j+1}]$. Evaluate these basis functions at the fine-grid points $x_k=(k+1)h$ for $k=0,\\dots,N-1$ to assemble $R_0$ such that $(R_0)_{j,k}=\\phi_j(x_k)$. Assemble the coarse matrix $A_0=R_0 A R_0^T$.\n\nStarting from the definitions above and the properties of SPD operators and additive Schwarz methods, derive a programmatic procedure to:\n- Construct $A$ from the FDM discretization.\n- Partition the degrees of freedom into $m$ contiguous subdomains with balanced sizes and build overlapping index sets with overlap $p$.\n- Assemble the local restriction operators $R_i$ and local matrices $A_i$.\n- Build the algebraic coarse restriction operator $R_0$ using piecewise linear basis vectors over the coarse mesh given by the subdomain endpoints, and assemble $A_0$.\n- Form the explicit dense matrix representation of the two-level additive Schwarz preconditioner $M^{-1}$ and evaluate the spectrum of $M^{-1}A$.\n- Compute the spectral condition number $\\kappa(M^{-1}A)=\\lambda_{\\max}/\\lambda_{\\min}$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ denote the largest and smallest eigenvalues of $M^{-1}A$.\n\nUse the following test suite of parameter values $(N,m,p)$, where $N$ is the number of interior points, $m$ is the number of subdomains, and $p$ is the overlap in fine-grid points:\n1. $(N,m,p)=(120,6,1)$\n2. $(N,m,p)=(240,6,1)$\n3. $(N,m,p)=(480,6,1)$\n4. $(N,m,p)=(96,12,2)$\n\nFor each test case, compute $\\kappa(M^{-1}A)$ and round the result to six decimal places. There are no physical units required for the outputs. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[3.142000,3.142000,3.142000,3.142000]\").",
            "solution": "The problem provides a complete and well-posed set of instructions for constructing a two-level additive Schwarz preconditioner for the one-dimensional Poisson equation and analyzing its performance. The task is to derive a programmatic procedure based on these instructions and compute the spectral condition number of the preconditioned matrix for a given set of parameters. The procedure is as follows.\n\n**Step 1: Finite Difference Discretization**\n\nThe one-dimensional Poisson equation $-u''(x)=f(x)$ on the interval $(0,1)$ with homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$ is discretized using a centered finite difference scheme. The domain $(0,1)$ is divided into $N+1$ equal subintervals of width $h = 1/(N+1)$. The grid points are $x_k = (k+1)h$ for $k=0, 1, \\dots, N-1$. The second derivative at a point $x_k$ is approximated as $u''(x_k) \\approx \\frac{u(x_{k-1}) - 2u(x_k) + u(x_{k+1})}{h^2}$. Substituting this into the Poisson equation leads to a system of linear equations $Au = b$, where $u \\in \\mathbb{R}^N$ is the vector of unknowns $u_k \\approx u(x_k)$, and $A$ is the $N \\times N$ stiffness matrix. The matrix $A$ is symmetric, positive definite, and tridiagonal, with entries given by:\n$$\nA_{ij} = \\frac{1}{h^2} \\begin{cases}\n2, & \\text{if } i=j \\\\\n-1, & \\text{if } |i-j|=1 \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\nThis matrix is constructed for a given number of interior points $N$.\n\n**Step 2: Subdomain Partitioning and Local Operators**\n\nThe set of $N$ global indices $\\{0, 1, \\dots, N-1\\}$ is partitioned into $m$ contiguous, non-overlapping base subdomains with balanced sizes. Let the base index set for subdomain $i$ be $I_i^{\\text{base}}$ for $i=1, \\dots, m$. An overlapping index set $I_i$ is then created for each subdomain by extending $I_i^{\\text{base}}$ by $p$ grid points in each direction, staying within the global index bounds $[0, N-1]$.\n\nFor each subdomain $i$, a restriction operator $R_i \\in \\mathbb{R}^{n_i \\times N}$ is defined, where $n_i$ is the number of indices in the overlapping set $I_i$. $R_i$ maps the global vector of unknowns to the local vector for subdomain $i$. In implementation, this corresponds to selecting rows and columns. The local stiffness matrix $A_i$ for each subdomain is a small, dense, and SPD matrix obtained by the Galerkin projection $A_i = R_i A R_i^T$. This is equivalent to extracting the submatrix of $A$ corresponding to the indices in $I_i$.\n\n**Step 3: Coarse Space Construction**\n\nA coarse space is introduced to handle the global propagation of information, which is essential for the scalability of the preconditioner. The coarse space is defined algebraically. A coarse grid is defined on $[0,1]$ with $m+1$ points $x_j^{\\text{coarse}} = jH$ for $j=0, 1, \\dots, m$, where $H=1/m$. The coarse space is spanned by $m-1$ piecewise linear \"hat\" basis functions $\\{\\phi_j\\}_{j=1}^{m-1}$. Each function $\\phi_j$ is supported on the coarse interval $[x_{j-1}^{\\text{coarse}}, x_{j+1}^{\\text{coarse}}]$ and satisfies $\\phi_j(x_j^{\\text{coarse}})=1$ and $\\phi_j(x_{k}^{\\text{coarse}})=0$ for $k \\neq j$.\n\nThe coarse restriction operator $R_0 \\in \\mathbb{R}^{(m-1) \\times N}$ is assembled by evaluating these basis functions at the fine grid points $x_k = (k+1)h$. The entries of $R_0$ are given by $(R_0)_{j-1, k} = \\phi_j(x_k)$ for $j=1, \\dots, m-1$ and $k=0, \\dots, N-1$. Specifically, for a fine grid point $x_k$:\n$$\n\\phi_j(x_k) = \\begin{cases}\n(x_k - x_{j-1}^{\\text{coarse}})/H, & \\text{if } x_{j-1}^{\\text{coarse}} \\le x_k \\le x_j^{\\text{coarse}} \\\\\n(x_{j+1}^{\\text{coarse}} - x_k)/H, & \\text{if } x_j^{\\text{coarse}} < x_k \\le x_{j+1}^{\\text{coarse}} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\nThe coarse matrix $A_0 \\in \\mathbb{R}^{(m-1) \\times (m-1)}$ is then formed using the Galerkin projection $A_0 = R_0 A R_0^T$. Like the local matrices, $A_0$ is SPD.\n\n**Step 4: Additive Schwarz Preconditioner Assembly**\n\nThe two-level additive Schwarz preconditioner $M^{-1}$ is constructed by summing the contributions from the local subproblems and the coarse problem. The formula is:\n$$\nM^{-1} \\;=\\; \\sum_{i=1}^m R_i^T A_i^{-1} R_i \\;+\\; R_0^T A_0^{-1} R_0\n$$\nEach term $R_i^T A_i^{-1} R_i$ represents a solve on a local subdomain followed by a prolongation (extension) back to the global grid. This is an $N \\times N$ matrix that is non-zero only in the block corresponding to the indices $I_i$. The term $R_0^T A_0^{-1} R_0$ represents the coarse-grid correction. Computationally, this involves inverting the small matrices $A_i$ and $A_0$, and then assembling the full $N \\times N$ dense matrix $M^{-1}$ by summing all these component matrices.\n\n**Step 5: Spectral Condition Number Calculation**\n\nThe effectiveness of the preconditioner is evaluated by the spectral condition number of the preconditioned matrix $M^{-1}A$. The eigenvalues of $M^{-1}A$ are real and positive. The condition number is defined as the ratio of the largest to the smallest eigenvalue:\n$$\n\\kappa(M^{-1}A) = \\frac{\\lambda_{\\max}(M^{-1}A)}{\\lambda_{\\min}(M^{-1}A)}\n$$\nA smaller condition number, ideally close to $1$ and independent of the problem size $N$, indicates a more effective preconditioner, leading to faster convergence of iterative solvers like the Conjugate Gradient method. To compute this, we first form the dense matrix product $P = M^{-1}A$, then find its eigenvalues using a numerical library, and finally compute the ratio of the maximum to the minimum real part of these eigenvalues.",
            "answer": "[1.583100,1.583102,1.583103,1.631868]"
        }
    ]
}