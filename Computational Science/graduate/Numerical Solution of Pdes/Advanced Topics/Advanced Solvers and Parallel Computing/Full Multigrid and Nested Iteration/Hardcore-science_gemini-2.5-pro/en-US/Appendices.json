{
    "hands_on_practices": [
        {
            "introduction": "Before we can appreciate the efficiency of multigrid methods, we must first understand the accuracy target they aim to hit. This exercise has you derive the local truncation error for the standard five-point discretization of the Poisson equation, a cornerstone of finite difference analysis. By quantifying how well the discrete operator approximates the continuous one, you establish the fundamental accuracy limit, $\\mathcal{O}(h^2)$, that a well-designed Full Multigrid cycle is built to match in optimal time .",
            "id": "3396892",
            "problem": "Consider the Poisson partial differential equation (PDE) $-\\Delta u = f$ on the unit square $\\Omega = (0,1)^{2}$ with Dirichlet boundary data $u = g$ on $\\partial \\Omega$, where $u \\in C^{6}(\\overline{\\Omega})$. Let $\\{ \\mathcal{T}_{h} \\}$ be a hierarchy of nested uniform Cartesian grids with mesh size $h$ in both coordinate directions, and suppose one employs Full Multigrid (FMG) with Nested Iteration (NI) to compute an approximation $u_{h}$ on the finest grid. On each grid $\\mathcal{T}_{h}$, the discrete Laplacian is defined at interior points $(x_{i}, y_{j})$ by the standard $5$-point stencil\n$$\n(L_{h} u)_{i,j} := \\frac{u(x_{i}+h, y_{j}) - 2 u(x_{i}, y_{j}) + u(x_{i}-h, y_{j})}{h^{2}} + \\frac{u(x_{i}, y_{j}+h) - 2 u(x_{i}, y_{j}) + u(x_{i}, y_{j}-h)}{h^{2}}.\n$$\nStarting from fundamental definitions and well-tested formulas (specifically, multivariate Taylor series for smooth functions and central-difference consistency arguments), derive the truncation error of $L_{h}$ at a generic interior point $(x_{i}, y_{j})$ in terms of spatial derivatives of $u$ evaluated at $(x_{i}, y_{j})$, and determine the order $p$ such that the consistency error $\\| L_{h} u - \\Delta u \\|_{\\infty} = \\mathcal{O}(h^{p})$ as $h \\to 0$. Your derivation should make clear how the leading error term arises, and how its magnitude informs the accuracy scale targeted by FMG with NI when transferring and refining approximations across levels.\n\nReport your final answer as two quantities: the leading-order truncation error at $(x_{i}, y_{j})$ and the order $p$. No numerical rounding is required, and no physical units are involved. Express the final answer as a single row matrix using the LaTeX $\\texttt{pmatrix}$ environment, with the first entry equal to the leading-order truncation error and the second entry equal to $p$.",
            "solution": "The problem is subjected to validation and is deemed valid as it is scientifically grounded, well-posed, objective, and conforms to all criteria for a solvable problem in numerical analysis.\n\nThe objective is to derive the local truncation error of the standard $5$-point central difference approximation to the Laplacian operator and to determine its order of consistency. The discrete Laplacian operator $L_{h}$ is applied to the exact solution $u$ of the continuous problem at an interior grid point $(x_{i}, y_{j})$. The truncation error $\\tau_{h}$ at this point is defined as the residual obtained by substituting the exact solution into the discrete operator and subtracting the exact differential operator acting on the solution:\n$$\n\\tau_{h}(x_i, y_j) = (L_{h} u)(x_i, y_j) - (\\Delta u)(x_i, y_j)\n$$\nThe operator $L_h$ is given by:\n$$\n(L_{h} u)_{i,j} = \\frac{u(x_{i}+h, y_{j}) - 2 u(x_{i}, y_{j}) + u(x_{i}-h, y_{j})}{h^{2}} + \\frac{u(x_{i}, y_{j}+h) - 2 u(x_{i}, y_{j}) + u(x_{i}, y_{j}-h)}{h^{2}}\n$$\nLet's analyze the two terms on the right-hand side separately. For clarity, we will suppress the indices $(i,j)$ and use $(x,y)$ for the point $(x_i, y_j)$, and denote partial derivatives with subscripts, e.g., $u_x = \\frac{\\partial u}{\\partial x}$, $u_{xx} = \\frac{\\partial^2 u}{\\partial x^2}$.\n\nThe problem states that the solution $u$ is of class $C^{6}(\\overline{\\Omega})$. This smoothness is sufficient to justify Taylor series expansions of $u$ around the point $(x, y)$.\n\nFirst, consider the terms for the $x$-direction. The Taylor expansion of $u(x+h, y)$ and $u(x-h, y)$ around $(x,y)$ are:\n$$\nu(x+h, y) = u(x,y) + h u_{x}(x,y) + \\frac{h^{2}}{2!} u_{xx}(x,y) + \\frac{h^{3}}{3!} u_{xxx}(x,y) + \\frac{h^{4}}{4!} u_{xxxx}(x,y) + \\frac{h^{5}}{5!} u_{xxxxx}(x,y) + \\mathcal{O}(h^{6})\n$$\n$$\nu(x-h, y) = u(x,y) - h u_{x}(x,y) + \\frac{h^{2}}{2!} u_{xx}(x,y) - \\frac{h^{3}}{3!} u_{xxx}(x,y) + \\frac{h^{4}}{4!} u_{xxxx}(x,y) - \\frac{h^{5}}{5!} u_{xxxxx}(x,y) + \\mathcal{O}(h^{6})\n$$\nThe $C^6$ regularity ensures the remainders are of order $\\mathcal{O}(h^6)$. Summing these two expansions, we observe that all odd-power terms in $h$ cancel:\n$$\nu(x+h, y) + u(x-h, y) = 2 u(x,y) + h^{2} u_{xx}(x,y) + \\frac{2h^{4}}{24} u_{xxxx}(x,y) + \\mathcal{O}(h^{6})\n$$\nRearranging this sum to form the central difference for the second derivative gives:\n$$\n\\frac{u(x+h, y) - 2u(x,y) + u(x-h, y)}{h^{2}} = u_{xx}(x,y) + \\frac{h^{2}}{12} u_{xxxx}(x,y) + \\mathcal{O}(h^{4})\n$$\nBy an identical argument for the $y$-direction, we obtain:\n$$\n\\frac{u(x, y+h) - 2u(x,y) + u(x, y-h)}{h^{2}} = u_{yy}(x,y) + \\frac{h^{2}}{12} u_{yyyy}(x,y) + \\mathcal{O}(h^{4})\n$$\nNow, we sum these two expressions to construct the action of the discrete operator $L_{h}$ on $u$:\n$$\n(L_{h} u)(x,y) = \\left( u_{xx}(x,y) + \\frac{h^{2}}{12} u_{xxxx}(x,y) \\right) + \\left( u_{yy}(x,y) + \\frac{h^{2}}{12} u_{yyyy}(x,y) \\right) + \\mathcal{O}(h^{4})\n$$\nGrouping terms, we get:\n$$\n(L_{h} u)(x,y) = \\left( u_{xx}(x,y) + u_{yy}(x,y) \\right) + \\frac{h^{2}}{12} \\left( u_{xxxx}(x,y) + u_{yyyy}(x,y) \\right) + \\mathcal{O}(h^{4})\n$$\nThe first term in parentheses is precisely the Laplacian of $u$, $\\Delta u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}$. Therefore, we can write:\n$$\n(L_{h} u)(x,y) = (\\Delta u)(x,y) + \\frac{h^{2}}{12} \\left( \\frac{\\partial^{4} u}{\\partial x^{4}}(x,y) + \\frac{\\partial^{4} u}{\\partial y^{4}}(x,y) \\right) + \\mathcal{O}(h^{4})\n$$\nThe truncation error $\\tau_{h}$ is the difference $(L_{h} u) - (\\Delta u)$:\n$$\n\\tau_{h}(x,y) = \\frac{h^{2}}{12} \\left( \\frac{\\partial^{4} u}{\\partial x^{4}}(x,y) + \\frac{\\partial^{4} u}{\\partial y^{4}}(x,y) \\right) + \\mathcal{O}(h^{4})\n$$\nThe leading-order term of the truncation error is the term with the lowest power of $h$, which is:\n$$\n\\frac{h^{2}}{12} \\left( \\frac{\\partial^{4} u}{\\partial x^{4}} + \\frac{\\partial^{4} u}{\\partial y^{4}} \\right)\n$$\nThe problem asks for the order $p$ of the consistency error, defined by $\\|L_h u - \\Delta u\\|_{\\infty} = \\mathcal{O}(h^p)$. This is the maximum of the absolute value of the local truncation error over all interior grid points. Since $u \\in C^{6}(\\overline{\\Omega})$, the fourth partial derivatives are continuous on a compact set, and are therefore bounded. Let $M$ be a constant such that $\\sup_{(x,y) \\in \\Omega} \\left| \\frac{1}{12} \\left( \\frac{\\partial^{4} u}{\\partial x^{4}} + \\frac{\\partial^{4} u}{\\partial y^{4}} \\right) \\right| \\leq M  \\infty$. Then:\n$$\n\\|L_h u - \\Delta u\\|_{\\infty} = \\sup_{i,j} |\\tau_h(x_i, y_j)| \\leq M h^{2} + \\mathcal{O}(h^{4})\n$$\nThis shows that the consistency error is of order $\\mathcal{O}(h^{2})$. Thus, the order $p$ is $2$.\n\nThe magnitude of this leading error term, and particularly its scaling as $\\mathcal{O}(h^{2})$, is central to the design and efficacy of Full Multigrid (FMG) with Nested Iteration (NI). The goal of FMG is to compute a discrete solution $u_h$ whose error $\\|u - u_h\\|_{\\infty}$ is on the same order as the truncation error, in this case $\\mathcal{O}(h^2_k)$, on the finest grid with mesh size $h_k$. The NI procedure starts on the coarsest grid and obtains a solution. This solution is then interpolated to the next finer grid to serve as an initial guess. For a second-order scheme ($p=2$), a sufficiently accurate interpolation (e.g., bilinear) of the coarse-grid solution provides an initial guess on the fine grid whose error is already $\\mathcal{O}(h^2)$. A small, fixed number of multigrid cycles (e.g., one V-cycle) is then sufficient to reduce the high-frequency components of the algebraic error to a level below that of the discretization error. The result is a solution $u_h$ that is accurate to $\\mathcal{O}(h^2)$ at a computational cost that is linear in the number of unknowns on the finest grid, which is asymptotically optimal. The magnitude of the leading error term $\\frac{h^2}{12}(\\dots)$ sets the scale for the desired accuracy of the algebraic solver on each level of the multigrid hierarchy.\n\nThe two quantities requested are the leading-order truncation error and the order $p$.\n1. Leading-order truncation error at $(x_{i},y_{j})$: $\\frac{h^{2}}{12} \\left( \\frac{\\partial^{4} u}{\\partial x^{4}} + \\frac{\\partial^{4} u}{\\partial y^{4}} \\right)$\n2. Order $p$: $2$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{h^{2}}{12} \\left( \\frac{\\partial^{4} u}{\\partial x^{4}} + \\frac{\\partial^{4} u}{\\partial y^{4}} \\right)  2 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The power of multigrid stems from the elegant interplay between smoothing on a fine grid and solving a correction on a coarse grid. This practice provides a rigorous, hands-on dive into Local Fourier Analysis (LFA), the primary tool for quantitatively analyzing the convergence of a two-grid cycle . Deriving the amplification factor will reveal precisely how smoothing and coarse-grid correction collaborate to damp all error frequencies, providing a concrete understanding of multigrid's rapid convergence.",
            "id": "3396909",
            "problem": "Consider the one-dimensional model problem for the Poisson equation $-u'' = f$ on the unit interval with periodic boundary conditions, discretized on a uniform fine grid with spacing $h$ using the standard second-order central difference operator $A_h$ with stencil $\\{-1, 2, -1\\}/h^2$. Let a two-grid method be defined with the following components:\n- Weighted Jacobi smoothing with weight $\\omega = 2/3$, applied as one pre-smoothing and one post-smoothing step.\n- Grid coarsening by a factor of $2$.\n- Full-weighting restriction from the fine grid to the coarse grid.\n- Linear interpolation (piecewise linear prolongation) from the coarse grid to the fine grid.\n- The coarse-grid operator defined by the Galerkin condition $A_{2h} = R A_h P$, and the coarse problem solved exactly.\n\nUse Local Fourier Analysis (LFA) to analyze the two-grid error-propagation operator. Work on the two-dimensional harmonic subspace spanned by the fine-grid Fourier modes with angles $\\theta$ and $\\theta+\\pi$, and consider the set of low-frequency angles that alias onto the coarse grid, namely $\\Theta_{\\text{low}} = \\{\\theta \\in (-\\pi, \\pi] : |\\theta| \\le \\pi/2\\}$. Starting from the definitions of the Fourier symbols of $A_h$, the weighted Jacobi smoother, the full-weighting restriction, and linear interpolation, derive the two-grid amplification factor $\\rho_{\\text{TG}}(\\theta)$ on this subspace. Then determine the worst-case two-grid amplification factor over $\\Theta_{\\text{low}}$.\n\nExpress your final answer as a single exact real number. No rounding is required, and no units are associated with the amplification factor.",
            "solution": "The user wants to find the worst-case two-grid amplification factor for a specific one-dimensional problem using Local Fourier Analysis (LFA). The analysis is performed on the two-dimensional harmonic subspace spanned by the fine-grid Fourier modes corresponding to frequencies (angles) $\\theta$ and $\\theta' = \\theta+\\pi$. The amplification factor is then maximized over the set of low frequencies $\\Theta_{\\text{low}} = \\{\\theta \\in (-\\pi, \\pi] : |\\theta| \\le \\pi/2\\}$.\n\nFirst, we determine the Fourier symbols for each component of the two-grid method. A discrete Fourier mode on the fine grid is given by $\\phi(\\theta)_j = \\exp(i j \\theta)$, where $j$ is the grid point index and $\\theta$ is the frequency.\n\n1.  **Fine-Grid Operator Symbol $\\tilde{A}_h(\\theta)$**:\n    The operator $A_h$ is the standard second-order central difference discretization of $-u''$ with stencil $\\frac{1}{h^2}[-1, 2, -1]$. Applying $A_h$ to $\\phi(\\theta)_j$:\n    $A_h \\phi(\\theta)_j = \\frac{1}{h^2}(-\\exp(i(j-1)\\theta) + 2\\exp(ij\\theta) - \\exp(i(j+1)\\theta))$\n    $= \\frac{\\exp(ij\\theta)}{h^2}(-e^{-i\\theta} + 2 - e^{i\\theta}) = \\frac{\\exp(ij\\theta)}{h^2}(2 - 2\\cos\\theta)$\n    $= \\frac{4}{h^2}\\sin^2(\\frac{\\theta}{2}) \\phi(\\theta)_j$.\n    The symbol (eigenvalue) is $\\tilde{A}_h(\\theta) = \\frac{4}{h^2}\\sin^2(\\frac{\\theta}{2})$. In LFA, the scaling factor $1/h^2$ can be ignored as it cancels out. We use $\\tilde{A}_h(\\theta) = 4\\sin^2(\\frac{\\theta}{2}) = 2(1-\\cos\\theta)$.\n\n2.  **Smoother Symbol $\\tilde{S}_h(\\theta)$**:\n    The error propagation operator for weighted Jacobi smoothing is $S_h = I - \\omega D_h^{-1} A_h$. The diagonal of $A_h$ is $D_h = 2/h^2$. The symbol of the smoother is:\n    $\\tilde{S}_h(\\theta) = 1 - \\omega \\frac{\\tilde{A}_h(\\theta)}{2/h^2} = 1 - \\omega \\frac{4/h^2 \\sin^2(\\theta/2)}{2/h^2} = 1 - 2\\omega \\sin^2(\\frac{\\theta}{2})$.\n    With the given weight $\\omega = 2/3$:\n    $\\tilde{S}_h(\\theta) = 1 - 2(\\frac{2}{3}) \\sin^2(\\frac{\\theta}{2}) = 1 - \\frac{4}{3} \\frac{1-\\cos\\theta}{2} = 1 - \\frac{2}{3}(1-\\cos\\theta) = \\frac{1}{3} + \\frac{2}{3}\\cos\\theta$.\n\n3.  **LFA on the 2D Subspace**:\n    We work in the basis of fine-grid modes $\\{\\phi(\\theta), \\phi(\\theta')\\}$ where $\\theta'=\\theta+\\pi$. Operators that are diagonal in the Fourier basis become $2 \\times 2$ diagonal matrices on this subspace. Let $c = \\cos\\theta$.\n    $\\tilde{A}_h(\\theta) = 2(1-c)$.\n    $\\tilde{A}_h(\\theta') = 2(1-\\cos(\\theta+\\pi)) = 2(1+c)$.\n    $\\tilde{S}_h(\\theta) = \\frac{1+2c}{3}$.\n    $\\tilde{S}_h(\\theta') = \\frac{1+2\\cos(\\theta+\\pi)}{3} = \\frac{1-2c}{3}$.\n    The matrix symbols are:\n    $$ \\tilde{A}_h(\\theta, \\theta') = \\begin{pmatrix} 2(1-c)  0 \\\\ 0  2(1+c) \\end{pmatrix}, \\quad \\tilde{S}_h(\\theta, \\theta') = \\begin{pmatrix} \\frac{1+2c}{3}  0 \\\\ 0  \\frac{1-2c}{3} \\end{pmatrix} $$\n\n4.  **Restriction and Prolongation Symbols**:\n    -   **Full-weighting restriction ($R$)**: The stencil is $\\frac{1}{4}[1, 2, 1]$. Its symbol is a $1 \\times 2$ matrix mapping $(\\phi(\\theta), \\phi(\\theta'))$ to the coarse-grid mode $\\phi(2\\theta)$:\n        $\\tilde{R}(\\theta) = (\\cos^2(\\frac{\\theta}{2}), \\sin^2(\\frac{\\theta}{2})) = (\\frac{1+c}{2}, \\frac{1-c}{2})$.\n    -   **Linear interpolation prolongation ($P$)**: This is the adjoint of full-weighting restriction. Its symbol is a $2 \\times 1$ matrix mapping $\\phi(2\\theta)$ to a linear combination of $\\phi(\\theta)$ and $\\phi(\\theta')$. For this standard pair in 1D, $\\tilde{P}(\\theta) = \\tilde{R}(\\theta)^T$.\n        $$ \\tilde{P}(\\theta) = \\begin{pmatrix} \\cos^2(\\frac{\\theta}{2}) \\\\ \\sin^2(\\frac{\\theta}{2}) \\end{pmatrix} = \\begin{pmatrix} \\frac{1+c}{2} \\\\ \\frac{1-c}{2} \\end{pmatrix} $$\n\n5.  **Coarse-Grid Operator Symbol $\\tilde{A}_{2h}(2\\theta)$**:\n    The coarse-grid operator is defined by the Galerkin condition $A_{2h} = R A_h P$. Its symbol is:\n    $$ \\tilde{A}_{2h}(2\\theta) = \\tilde{R}(\\theta) \\tilde{A}_h(\\theta, \\theta') \\tilde{P}(\\theta) $$\n    $$ \\tilde{A}_{2h}(2\\theta) = \\begin{pmatrix} \\frac{1+c}{2}  \\frac{1-c}{2} \\end{pmatrix} \\begin{pmatrix} 2(1-c)  0 \\\\ 0  2(1+c) \\end{pmatrix} \\begin{pmatrix} \\frac{1+c}{2} \\\\ \\frac{1-c}{2} \\end{pmatrix} $$\n    $$ = \\begin{pmatrix} (1+c)(1-c)  (1-c)(1+c) \\end{pmatrix} \\begin{pmatrix} \\frac{1+c}{2} \\\\ \\frac{1-c}{2} \\end{pmatrix} $$\n    $$ = (1-c^2)\\frac{1+c}{2} + (1-c^2)\\frac{1-c}{2} = (1-c^2) \\left(\\frac{1+c+1-c}{2}\\right) = 1-c^2 = \\sin^2\\theta $$\n\n6.  **Two-Grid Operator Symbol $\\tilde{M}_{\\text{TG}}(\\theta)$**:\n    The error propagation operator for the two-grid cycle is $M_{\\text{TG}} = S_h^{\\nu_2}(I - P A_{2h}^{-1} R A_h)S_h^{\\nu_1}$.\n    With $\\nu_1=1$, $\\nu_2=1$, the symbol is $\\tilde{M}_{\\text{TG}}(\\theta) = \\tilde{S}_h(\\theta, \\theta') \\left( I - \\tilde{P}(\\theta) \\tilde{A}_{2h}(2\\theta)^{-1} \\tilde{R}(\\theta) \\tilde{A}_h(\\theta, \\theta') \\right) \\tilde{S}_h(\\theta, \\theta')$.\n    Let's compute the coarse-grid correction operator symbol $C(\\theta) = I - \\tilde{P}(\\theta) \\tilde{A}_{2h}(2\\theta)^{-1} \\tilde{R}(\\theta) \\tilde{A}_h(\\theta, \\theta')$.\n    First, the projection-like term $K(\\theta)=\\tilde{P}(\\theta) \\tilde{A}_{2h}(2\\theta)^{-1} \\tilde{R}(\\theta) \\tilde{A}_h(\\theta, \\theta')$.\n    $$ \\tilde{P}\\tilde{A}_{2h}^{-1}\\tilde{R} = \\begin{pmatrix} \\frac{1+c}{2} \\\\ \\frac{1-c}{2} \\end{pmatrix} \\frac{1}{1-c^2} \\begin{pmatrix} \\frac{1+c}{2}  \\frac{1-c}{2} \\end{pmatrix} = \\frac{1}{4(1-c^2)} \\begin{pmatrix} (1+c)^2  1-c^2 \\\\ 1-c^2  (1-c)^2 \\end{pmatrix} $$\n    Multiplying by $\\tilde{A}_h(\\theta, \\theta')$:\n    $$ K(\\theta) = \\frac{1}{4(1-c^2)} \\begin{pmatrix} (1+c)^2  1-c^2 \\\\ 1-c^2  (1-c)^2 \\end{pmatrix} \\begin{pmatrix} 2(1-c)  0 \\\\ 0  2(1+c) \\end{pmatrix} $$\n    $$ = \\frac{1}{2(1-c^2)} \\begin{pmatrix} (1+c)^2(1-c)  (1-c^2)(1+c) \\\\ (1-c^2)(1-c)  (1-c)^2(1+c) \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1+c  1+c \\\\ 1-c  1-c \\end{pmatrix} $$\n    So, $C(\\theta) = I - K(\\theta) = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\frac{1}{2}\\begin{pmatrix} 1+c  1+c \\\\ 1-c  1-c \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 1-c  -(1+c) \\\\ -(1-c)  1+c \\end{pmatrix}$.\n    Now we assemble $\\tilde{M}_{\\text{TG}}(\\theta) = \\tilde{S}_h C \\tilde{S}_h$:\n    $$ \\tilde{M}_{\\text{TG}}(\\theta) = \\begin{pmatrix} \\frac{1+2c}{3}  0 \\\\ 0  \\frac{1-2c}{3} \\end{pmatrix} \\left( \\frac{1}{2}\\begin{pmatrix} 1-c  -(1+c) \\\\ -(1-c)  1+c \\end{pmatrix} \\right) \\begin{pmatrix} \\frac{1+2c}{3}  0 \\\\ 0  \\frac{1-2c}{3} \\end{pmatrix} $$\n    $$ = \\frac{1}{18} \\begin{pmatrix} (1+2c)(1-c)  -(1+2c)(1+c) \\\\ -(1-2c)(1-c)  (1-2c)(1+c) \\end{pmatrix} \\begin{pmatrix} 1+2c  0 \\\\ 0  1-2c \\end{pmatrix}  $$\n    $$ = \\frac{1}{18} \\begin{pmatrix} (1+2c)^2(1-c)  -(1+2c)(1-2c)(1+c) \\\\ -(1+2c)(1-2c)(1-c)  (1-2c)^2(1+c) \\end{pmatrix} $$\n\n7.  **Two-Grid Amplification Factor $\\rho_{\\text{TG}}(\\theta)$**:\n    The amplification factor $\\rho_{\\text{TG}}(\\theta)$ is the spectral radius of $\\tilde{M}_{\\text{TG}}(\\theta)$. The characteristic equation for a $2 \\times 2$ matrix $M$ is $\\lambda^2 - \\text{tr}(M)\\lambda + \\det(M) = 0$.\n    The determinant of $\\tilde{M}_{\\text{TG}}(\\theta)$ is:\n    $$ \\det(\\tilde{M}_{\\text{TG}}) = (\\det(\\tilde{S}_h))^2 \\det(C) $$\n    Since $C = I-K$ and $K$ is a rank-1 projector ($K^2=K$), $C$ is also a projector and has eigenvalues $0$ and $1$. Thus, $\\det(C) = 0$, which implies $\\det(\\tilde{M}_{\\text{TG}}) = 0$.\n    The characteristic equation simplifies to $\\lambda(\\lambda - \\text{tr}(\\tilde{M}_{\\text{TG}})) = 0$. The eigenvalues are $\\lambda_1 = 0$ and $\\lambda_2 = \\text{tr}(\\tilde{M}_{\\text{TG}})$.\n    The spectral radius is $\\rho_{\\text{TG}}(\\theta) = |\\text{tr}(\\tilde{M}_{\\text{TG}}(\\theta))|$.\n    $$ \\text{tr}(\\tilde{M}_{\\text{TG}}) = \\frac{1}{18} \\left[ (1+2c)^2(1-c) + (1-2c)^2(1+c) \\right] $$\n    $$ (1+4c+4c^2)(1-c) = 1+4c+4c^2 - c - 4c^2 - 4c^3 = 1+3c-4c^3 $$\n    $$ (1-4c+4c^2)(1+c) = 1-4c+4c^2 + c - 4c^2 + 4c^3 = 1-3c+4c^3 $$\n    $$ \\text{tr}(\\tilde{M}_{\\text{TG}}) = \\frac{1}{18} \\left[ (1+3c-4c^3) + (1-3c+4c^3) \\right] = \\frac{1}{18} (2) = \\frac{1}{9} $$\n    Thus, the two-grid amplification factor is constant for all $\\theta\n \\in (-\\pi, \\pi]$ where the analysis is valid:\n    $$ \\rho_{\\text{TG}}(\\theta) = \\frac{1}{9} $$\n\n8.  **Worst-Case Amplification Factor**:\n    The problem asks for the worst-case (maximum) amplification factor over the set of low frequencies $\\Theta_{\\text{low}} = \\{\\theta \\in (-\\pi, \\pi] : |\\theta| \\le \\pi/2\\}$. Since $\\rho_{\\text{TG}}(\\theta)$ is constant over this entire range, the maximum value is simply this constant.\n    $$ \\max_{\\theta \\in \\Theta_{\\text{low}}} \\rho_{\\text{TG}}(\\theta) = \\max_{|\\theta| \\le \\pi/2} \\frac{1}{9} = \\frac{1}{9} $$",
            "answer": "$$\\boxed{\\frac{1}{9}}$$"
        },
        {
            "introduction": "Moving from theory to practice, an effective Full Multigrid solver is not just about its components, but about tuning them to balance accuracy and computational cost. This exercise places you in the role of an algorithm designer, tasked with optimizing V-cycle parameters like depth and smoother effort to meet a target tolerance within a fixed time budget . It is a practical demonstration of the trade-offs involved in configuring high-performance numerical methods for real-world efficiency.",
            "id": "3396932",
            "problem": "Consider solving the linear system arising from a standard second-order centered finite difference discretization of the Poisson equation $-\\Delta u = f$ on the unit square with homogeneous Dirichlet boundary conditions. The resulting stiffness matrix $A$ is Symmetric Positive Definite (SPD), and we measure algebraic error in the energy norm defined by $\\|e\\|_{A} = \\sqrt{e^{\\top} A e}$. A Full Multigrid (FMG) algorithm with nested iteration is employed to produce a fine-grid solution in one cycle, where the fine grid spacing is $h = 1/(n-1)$ for a given integer $n \\ge 3$.\n\nYou will optimize the V-cycle depth and smoother effort within FMG subject to a fixed time budget and a target error tolerance. The V-cycle depth $k$ is the number of distinct grid levels traversed during each V-cycle, with uniform coarsening by a factor of $2$ per level. The smoother is weighted Jacobi with relaxation parameter chosen near its optimal value for the Poisson operator, and it is applied $\\nu_1$ times as pre-smoothing and $\\nu_2$ times as post-smoothing on every visited level.\n\nAssume the following widely used and well-tested modeling assumptions for two-dimensional geometric multigrid for Poisson:\n\n- Discretization (consistency) error bound on the fine grid: there exists a scheme-dependent constant $D  0$ such that the energy norm of the discretization error scales like $\\|e_{\\mathrm{disc}}\\|_{A} \\le D\\, h^{p}$ for a given integer order $p \\ge 1$.\n- Smoother contraction in the energy norm per pass is bounded by a scalar $\\sigma \\in (0,1)$, so $\\|S v\\|_{A} \\le \\sigma \\|v\\|_{A}$ for any vector $v$, where $S$ is the weighted Jacobi smoother. For concreteness, take $\\sigma = 0.6$.\n- Coarse-grid correction over $k$ levels contracts the low-frequency error by a factor bounded by $\\theta^{k}$ for some $\\theta \\in (0,1)$. For concreteness, take $\\theta = 0.25$.\n- The nested iteration initialization ensures the initial algebraic error on the fine grid is no larger than the fine-grid discretization scale, i.e., $\\|e_{\\mathrm{init}}\\|_{A} \\le D\\, h^{p}$.\n\nUnder these assumptions, an FMG cycle with a single V-cycle of depth $k$ and $(\\nu_1,\\nu_2)$ smoother sweeps produces an energy-norm error bounded by\n$$\n\\|e\\|_{A} \\le D\\, h^{p} \\left(1 + \\sigma^{\\nu_1+\\nu_2}\\, \\theta^{k}\\right).\n$$\n\nWork (time) is modeled in dimensionless work units based on per-level costs. Let the number of fine-grid unknowns be $N = n^{2}$. The work of one V-cycle with depth $k$ and $(\\nu_1,\\nu_2)$ smoothing is modeled by\n$$\nT(k,\\nu_1,\\nu_2;n) = \\left( a_s (\\nu_1+\\nu_2) + a_m \\right)\\; \\sum_{\\ell=0}^{k-1} \\frac{N}{4^{\\ell}},\n$$\nwhere $a_s  0$ models the work per smoothing pass per unknown and $a_m  0$ models the per-level work of residual computation and transfer operations (restriction and prolongation). For concreteness take $a_s = 3$ and $a_m = 2$.\n\nYou are given a target tolerance constant $C  0$ and a fixed time budget $T_{\\max}  0$. The optimization problem for each test case is:\n\nFind integers $k$, $\\nu_1$, and $\\nu_2$ that minimize $T(k,\\nu_1,\\nu_2;n)$ subject to\n$$\n\\|e\\|_{A} \\le C\\, h^{p}\n\\quad\\text{and}\\quad\nT(k,\\nu_1,\\nu_2;n) \\le T_{\\max},\n$$\nwith search ranges $k \\in \\{1,2,3,4\\}$ and $\\nu_1,\\nu_2 \\in \\{1,2,3,4,5\\}$. If multiple configurations attain the same minimal work, break ties by the smallest $k$, then the smallest $\\nu_1$, then the smallest $\\nu_2$.\n\nIf no configuration satisfies both constraints, report failure for that test case.\n\nUse the concrete constants $D = 0.5$, $\\sigma = 0.6$, $\\theta = 0.25$, $a_s = 3$, $a_m = 2$.\n\nTest Suite:\n- Case $1$: $(n, p, C, T_{\\max}) = (129, 2, 0.8, 160000)$.\n- Case $2$: $(n, p, C, T_{\\max}) = (129, 2, 0.53, 160000)$.\n- Case $3$: $(n, p, C, T_{\\max}) = (129, 2, 0.55, 160000)$.\n- Case $4$: $(n, p, C, T_{\\max}) = (257, 2, 0.53, 900000)$.\n- Case $5$: $(n, p, C, T_{\\max}) = (129, 2, 0.53, 300000)$.\n\nFor each test case, your program must output the chosen configuration and its predicted performance as a list $[k,\\nu_1,\\nu_2,E,T]$, where $E$ is the predicted energy-norm error bound and $T$ is the predicted work. If no feasible configuration exists, output the failure signature $[-1,-1,-1,-1.0,-1.0]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above. For example, if there were three cases, the output would look like $[[k_1,\\nu_{1,1},\\nu_{2,1},E_1,T_1],[k_2,\\nu_{1,2},\\nu_{2,2},E_2,T_2],[k_3,\\nu_{1,3},\\nu_{2,3},E_3,T_3]]$.",
            "solution": "The user has provided a well-defined constrained optimization problem related to the parameter tuning of a Full Multigrid (FMG) algorithm. I will first formalize the problem by re-stating the objective function and constraints, and then describe the methodology for finding the optimal parameter set $(k, \\nu_1, \\nu_2)$ for each given test case.\n\nThe problem is to find integers $k$, $\\nu_1$, and $\\nu_2$ from the given search ranges that minimize the computational work, $T$, subject to constraints on error and total work.\n\n**1. Problem Formulation**\n\nThe given quantities and model parameters are:\n- Discretization error constant: $D = 0.5$\n- Smoother contraction factor: $\\sigma = 0.6$\n- Coarse-grid correction factor base: $\\theta = 0.25$\n- Work model constants: $a_s = 3$, $a_m = 2$\n- Fine grid spacing: $h = 1/(n-1)$\n- Number of unknowns: $N = n^2$\n\nThe search space for the optimization variables is:\n- V-cycle depth: $k \\in \\{1, 2, 3, 4\\}$\n- Pre-smoothing sweeps: $\\nu_1 \\in \\{1, 2, 3, 4, 5\\}$\n- Post-smoothing sweeps: $\\nu_2 \\in \\{1, 2, 3, 4, 5\\}$\n\nFor a given configuration $(k, \\nu_1, \\nu_2)$ and a test case defined by $(n, p, C, T_{\\max})$, we must evaluate the feasibility and performance.\n\n**Error Model and Constraint:**\nThe problem posits a model for the energy norm of the error, $\\|e\\|_{A}$, after one FMG cycle. Based on the model's structure and the provided context, this error is interpreted as the total error, which is the sum of the discretization error and the final algebraic error. The bound is given by:\n$$\nE(k, \\nu_1, \\nu_2; n, p) = D h^p \\left(1 + \\sigma^{\\nu_1+\\nu_2} \\theta^k\\right)\n$$\nThe first constraint requires this error to be no larger than a target tolerance, which is scaled by the discretization error order:\n$$\nE(k, \\nu_1, \\nu_2; n, p) \\le C h^p\n$$\nSubstituting the expression for $E$ and simplifying by dividing by the positive term $h^p$ yields a constraint independent of $h$:\n$$\nD \\left(1 + \\sigma^{\\nu_1+\\nu_2} \\theta^k\\right) \\le C\n$$\n\n**Work Model and Constraint:**\nThe computational work (time) is modeled as a function of the number of smoothing steps and the number of grid levels. The work for a V-cycle of depth $k$ is proportional to the sum of unknowns over all visited grids:\n$$\nT(k, \\nu_1, \\nu_2; n) = \\left( a_s (\\nu_1+\\nu_2) + a_m \\right) N \\sum_{\\ell=0}^{k-1} \\frac{1}{4^{\\ell}}\n$$\nThe geometric series can be computed in closed form: $\\sum_{\\ell=0}^{k-1} (1/4)^{\\ell} = \\frac{1 - (1/4)^k}{1 - 1/4} = \\frac{4}{3}(1 - 4^{-k})$.\nThe second constraint is a hard limit on this computational work:\n$$\nT(k, \\nu_1, \\nu_2; n) \\le T_{\\max}\n$$\n\n**Optimization Problem:**\nFor each test case, we must solve:\n$$\n\\min_{k, \\nu_1, \\nu_2} T(k, \\nu_1, \\nu_2; n)\n$$\nsubject to:\n$$\n\\begin{cases}\nD \\left(1 + \\sigma^{\\nu_1+\\nu_2} \\theta^k\\right) \\le C \\\\\nT(k, \\nu_1, \\nu_2; n) \\le T_{\\max} \\\\\nk \\in \\{1, 2, 3, 4\\} \\\\\n\\nu_1, \\nu_2 \\in \\{1, 2, 3, 4, 5\\}\n\\end{cases}\n$$\nTies in minimal work are to be broken by selecting the configuration with the smallest $k$, then the smallest $\\nu_1$, and finally the smallest $\\nu_2$.\n\n**2. Solution Methodology**\n\nThe search space is finite and small, with a total of $4 \\times 5 \\times 5 = 100$ possible configurations $(k, \\nu_1, \\nu_2)$. An exhaustive search is therefore a direct and computationally inexpensive method to find the optimal solution.\n\nFor each test case specified by $(n, p, C, T_{\\max})$:\n1.  An empty list, `valid_configs`, is created to store all configurations that satisfy both the error and work constraints.\n2.  We iterate through all possible integer tuples $(k, \\nu_1, \\nu_2)$ in their respective search ranges.\n3.  For each tuple, we evaluate the two constraints:\n    a. **Error Constraint Check:** The inequality $D (1 + \\sigma^{\\nu_1+\\nu_2} \\theta^k) \\le C$ is tested.\n    b. **Work Constraint Check:** The work $T(k, \\nu_1, \\nu_2; n)$ is calculated and compared against the budget $T_{\\max}$.\n4.  If a configuration satisfies both constraints, it is considered feasible. A record containing the configuration and its performance metrics, $(k, \\nu_1, \\nu_2, E, T)$, is added to the `valid_configs` list. The error $E$ is calculated using its full expression for reporting purposes.\n5.  After iterating through all $100$ configurations, we analyze the `valid_configs` list:\n    a. If the list is empty, no feasible solution exists, and the failure signature `[-1,-1,-1,-1.0,-1.0]` is reported for this case.\n    b. If the list contains one or more feasible configurations, a single optimal choice must be made. This is achieved by sorting the list. The primary sorting key is the work, $T$, in ascending order. Subsequent keys for tie-breaking are $k$, $\\nu_1$, and $\\nu_2$, also in ascending order.\n6.  The first element of this sorted list represents the optimal configuration $(k^*, \\nu_1^*, \\nu_2^*)$ with minimum work $T^*$, satisfying the tie-breaking rules. This configuration's data, `[k*, \\nu_1*, \\nu_2*, E*, T*]`, is the result for the test case.\n\nThis procedure is repeated for all test cases provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the FMG parameter optimization problem for a suite of test cases.\n    \"\"\"\n    # Define the global constants from the problem statement.\n    D_CONST = 0.5\n    SIGMA_CONST = 0.6\n    THETA_CONST = 0.25\n    A_S_CONST = 3.0\n    A_M_CONST = 2.0\n\n    # Define the search ranges for the optimization variables.\n    k_range = [1, 2, 3, 4]\n    nu_range = [1, 2, 3, 4, 5]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (129, 2, 0.8, 160000),   # Case 1\n        (129, 2, 0.53, 160000),  # Case 2\n        (129, 2, 0.55, 160000),  # Case 3\n        (257, 2, 0.53, 900000),  # Case 4\n        (129, 2, 0.53, 300000),  # Case 5\n    ]\n\n    def find_best_config(n, p, C, T_max):\n        \"\"\"\n        Finds the optimal (k, nu1, nu2) for a single test case.\n        \"\"\"\n        h = 1.0 / (n - 1)\n        N = float(n**2)\n        \n        valid_configs = []\n\n        for k in k_range:\n            for nu1 in nu_range:\n                for nu2 in nu_range:\n                    nu_sum = nu1 + nu2\n                    \n                    # 1. Check error constraint\n                    # D * (1 + sigma**(nu1+nu2) * theta**k) = C\n                    error_factor = 1.0 + np.power(SIGMA_CONST, nu_sum) * np.power(THETA_CONST, k)\n                    if D_CONST * error_factor > C:\n                        continue\n\n                    # 2. Check work constraint\n                    # T = (a_s * (nu1+nu2) + a_m) * N * sum(1/4**l)\n                    # sum(1/4**l) from l=0 to k-1 = (4/3)*(1 - 4**(-k))\n                    sum_term = (4.0 / 3.0) * (1.0 - np.power(4.0, -k))\n                    work = (A_S_CONST * nu_sum + A_M_CONST) * N * sum_term\n                    \n                    if work > T_max:\n                        continue\n\n                    # This configuration is valid. Calculate its full metrics and store.\n                    error_bound = D_CONST * np.power(h, p) * error_factor\n                    valid_configs.append([k, nu1, nu2, error_bound, work])\n        \n        if not valid_configs:\n            return [-1, -1, -1, -1.0, -1.0]\n        \n        # Sort to find the optimal configuration\n        # Tie-breaking: min T, then min k, then min nu1, then min nu2\n        valid_configs.sort(key=lambda x: (x[4], x[0], x[1], x[2]))\n        \n        return valid_configs[0]\n\n    results = [find_best_config(*case) for case in test_cases]\n    \n    # Format the final output string as specified.\n    # e.g., [[k1,nu11,nu21,E1,T1],[k2,nu12,nu22,E2,T2]]\n    result_str = \",\".join([f\"[{c[0]},{c[1]},{c[2]},{c[3]},{c[4]}]\" for c in results])\n    \n    # The final print statement in the exact required format.\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        }
    ]
}