{
    "hands_on_practices": [
        {
            "introduction": "The time-dependent heat or diffusion equation is a cornerstone of mathematical physics, yet its numerical solution in high dimensions presents a classic example of the curse of dimensionality. When using simple explicit time-stepping methods, the stability requirement forces the time step to become prohibitively small as the spatial dimension increases. This exercise challenges you to quantify this stability bottleneck and then analyze the cost and stability of more sophisticated methods, such as Alternating Direction Implicit (ADI) and operator splitting schemes, which are designed specifically to overcome this challenge. ",
            "id": "3454722",
            "problem": "Consider the time-dependent heat equation on the $d$-dimensional torus with periodic boundary conditions,\n$$\nu_{t} = \\Delta u \\quad \\text{on} \\quad \\mathbb{T}^{d},\n$$\nwhere $\\mathbb{T}^{d}$ denotes the Cartesian product of $d$ circles of length $1$. Discretize space using a uniform grid with $m$ points per dimension (assume $m$ is even) and grid spacing $h = \\frac{1}{m}$, and approximate the Laplacian $\\Delta$ by the standard second-order central difference operator so that the semi-discrete system takes the form\n$$\n\\frac{d \\mathbf{u}(t)}{dt} = A \\, \\mathbf{u}(t),\n$$\nwhere $A$ is the $d$-dimensional discrete Laplacian with periodic boundary conditions. Let $N = m^{d}$ denote the total number of grid points.\n\nUsing only fundamental properties of Fourier modes, circulant matrices, and contraction semigroups generated by negative semidefinite operators, perform the following:\n\n1. Derive the spectrum of $A$ in terms of the discrete Fourier mode indices and identify the most negative eigenvalue as a function of $d$ and $h$.\n\n2. For the scalar linear test equation $y'(t) = \\lambda y(t)$, characterize the stability regions (sets of $z = \\Delta t \\, \\lambda$ for which the amplification factor satisfies $|G(z)| \\leq 1$) of the following time-stepping strategies when applied to the semi-discrete heat equation:\n   - Alternating Direction Implicit (ADI), understood as a standard Douglas-type splitting that sequentially solves along each spatial coordinate with implicit substeps.\n   - Exponential integrators, which compute the exact action of $\\exp(\\Delta t A)$ via the Fast Fourier Transform (FFT).\n   - Operator splitting schemes such as first-order Lie splitting and second-order Strang splitting across the $d$ coordinate directions.\n\n   Justify unconditional stability for the latter two in this setting and discuss the conditions under which ADI is unconditionally stable for diffusion operators.\n\n3. Derive the leading-order operation count per time step as a function of $d$, $m$, and $N$ for:\n   - ADI using cyclic tridiagonal solves along each coordinate direction and each grid line,\n   - Exponential integrators implemented with $d$-dimensional FFTs,\n   - Splitting methods implemented via sequential one-dimensional FFTs along each coordinate direction and across all lines.\n\n   Express these operation counts in big-$\\mathcal{O}$ notation in terms of $d$, $N$, and $\\log m$.\n\nFinally, as a baseline that quantifies the curse of dimensionality for explicit methods, determine the largest stable time step $\\Delta t_{\\max}$ for the forward Euler method applied to the semi-discrete system $d\\mathbf{u}/dt = A \\mathbf{u}$ in terms of $d$ and $h$. Provide your final $\\Delta t_{\\max}$ as a single closed-form analytic expression. No rounding is required, and no units are needed in the final expression.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of numerical analysis for partial differential equations, is well-posed with a clear and consistent setup, and is expressed in objective, formal language. It presents a standard but non-trivial problem that can be rigorously solved using established mathematical techniques.\n\nThe solution proceeds by addressing each of the tasks outlined in the problem statement.\n\n### 1. Spectrum of the Discrete Laplacian\n\nThe semi-discrete system for the heat equation $u_t = \\Delta u$ on the $d$-dimensional torus $\\mathbb{T}^d$ is given by $\\frac{d\\mathbf{u}(t)}{dt} = A \\mathbf{u}(t)$, where $A$ is the matrix representing the standard second-order finite difference approximation to the Laplacian $\\Delta$ on a uniform grid with spacing $h = \\frac{1}{m}$ and periodic boundary conditions. The total number of grid points is $N=m^d$.\n\nThe operator $A$ can be expressed as a sum of one-dimensional discrete Laplacians:\n$$\nA = \\sum_{j=1}^{d} A_j\n$$\nwhere $A_j$ represents the discrete Laplacian in the $j$-th coordinate direction. Using Kronecker product notation, this is $A = A_{1D} \\otimes I \\otimes \\dots \\otimes I + I \\otimes A_{1D} \\otimes \\dots \\otimes I + \\dots + I \\otimes \\dots \\otimes I \\otimes A_{1D}$, where $A_{1D}$ is the $m \\times m$ matrix for the 1D problem.\n\nIn one dimension, on a grid with points $x_i = i h$ for $i=0, \\dots, m-1$, the action of $A_{1D}$ on a grid function $u$ is given by:\n$$\n(A_{1D} u)_i = \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2}\n$$\nwith indices taken modulo $m$ due to periodic boundary conditions. This structure makes $A_{1D}$ a circulant matrix. The eigenvectors of any $m \\times m$ circulant matrix are the discrete Fourier modes. A discrete Fourier mode with wave number $k \\in \\{0, 1, \\dots, m-1\\}$ is a vector $\\mathbf{v}_k$ with components $(\\mathbf{v}_k)_j = \\exp(i 2 \\pi k x_j / L) = \\exp(i 2 \\pi k j / m)$ since the length of the domain is $L=1$.\n\nApplying $A_{1D}$ to $\\mathbf{v}_k$:\n$$\n(A_{1D} \\mathbf{v}_k)_j = \\frac{\\exp(i \\frac{2\\pi k(j+1)}{m}) - 2\\exp(i \\frac{2\\pi k j}{m}) + \\exp(i \\frac{2\\pi k(j-1)}{m})}{h^2}\n$$\n$$\n= \\frac{\\exp(i \\frac{2\\pi k j}{m})}{h^2} \\left( \\exp(i \\frac{2\\pi k}{m}) - 2 + \\exp(-i \\frac{2\\pi k}{m}) \\right)\n$$\n$$\n= (\\mathbf{v}_k)_j \\frac{2}{h^2} \\left( \\cos(\\frac{2\\pi k}{m}) - 1 \\right)\n$$\nUsing the identity $1 - \\cos(2\\theta) = 2\\sin^2(\\theta)$, this becomes:\n$$\n= (\\mathbf{v}_k)_j \\left( -\\frac{4}{h^2} \\sin^2(\\frac{\\pi k}{m}) \\right)\n$$\nThus, the eigenvalues of the 1D operator $A_{1D}$ are:\n$$\n\\lambda_{1D, k} = -\\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi k}{m}\\right), \\quad \\text{for } k \\in \\{0, 1, \\dots, m-1\\}\n$$\nThe eigenvalues of the $d$-dimensional operator $A$ are the sums of the eigenvalues of the constituent 1D operators. An eigenvector of $A$ is a tensor product of 1D eigenvectors $\\mathbf{v}_{\\mathbf{k}} = \\mathbf{v}_{k_1} \\otimes \\mathbf{v}_{k_2} \\otimes \\dots \\otimes \\mathbf{v}_{k_d}$, where $\\mathbf{k} = (k_1, \\dots, k_d)$ is a multi-index of wave numbers with each $k_j \\in \\{0, 1, \\dots, m-1\\}$. The corresponding eigenvalue $\\lambda_{\\mathbf{k}}$ is:\n$$\n\\lambda_{\\mathbf{k}} = \\sum_{j=1}^{d} \\lambda_{1D, k_j} = -\\frac{4}{h^2} \\sum_{j=1}^{d} \\sin^2\\left(\\frac{\\pi k_j}{m}\\right)\n$$\nThis is the spectrum of $A$. All eigenvalues are real and non-positive, so $A$ is a negative semidefinite operator.\n\nThe most negative eigenvalue, $\\lambda_{\\min}(A)$, occurs when the sum $\\sum_{j=1}^{d} \\sin^2(\\frac{\\pi k_j}{m})$ is maximized. The term $\\sin^2(\\theta)$ is maximized with value $1$ when $\\theta = \\pi/2 + n\\pi$ for integer $n$. For $k_j \\in \\{0, \\dots, m-1\\}$, the argument is $\\frac{\\pi k_j}{m}$. We need $\\frac{\\pi k_j}{m} = \\frac{\\pi}{2}$, which gives $k_j = m/2$. Since $m$ is given to be even, $k_j=m/2$ is an integer and a valid wave number. This corresponds to the highest frequency mode resolvable on the grid. To maximize the sum, we must choose $k_j = m/2$ for all $j=1, \\dots, d$.\n$$\n\\lambda_{\\min}(A) = -\\frac{4}{h^2} \\sum_{j=1}^{d} \\sin^2\\left(\\frac{\\pi (m/2)}{m}\\right) = -\\frac{4}{h^2} \\sum_{j=1}^{d} \\sin^2\\left(\\frac{\\pi}{2}\\right) = -\\frac{4}{h^2} \\sum_{j=1}^{d} 1\n$$\n$$\n\\lambda_{\\min}(A) = -\\frac{4d}{h^2}\n$$\n\n### 2. Stability of Time-Stepping Strategies\n\nA numerical method for $y' = \\lambda y$ is stable if the amplification factor $G(z)$, where $z = \\Delta t \\lambda$, satisfies $|G(z)| \\le 1$. For the system $\\mathbf{u}' = A\\mathbf{u}$, this must hold for $z_k = \\Delta t \\lambda_k$ for every eigenvalue $\\lambda_k$ of $A$. Since all $\\lambda_k \\le 0$, we only need to consider $z \\in [\\Delta t \\lambda_{\\min}, 0]$.\n\n- **Exponential Integrators:** This method computes the exact solution to the semi-discrete system over one time step: $\\mathbf{u}_{n+1} = \\exp(\\Delta t A) \\mathbf{u}_n$. The amplification operator is $\\mathcal{G} = \\exp(\\Delta t A)$, and the amplification factors are $G(z_k) = \\exp(z_k) = \\exp(\\Delta t \\lambda_k)$. Since $A$ is negative semidefinite, its eigenvalues $\\lambda_k$ are non-positive. Therefore, $\\Delta t \\lambda_k \\le 0$ for any $\\Delta t  0$. The magnitude of the amplification factor is $|G(z_k)| = |\\exp(\\Delta t \\lambda_k)| = \\exp(\\Delta t \\lambda_k) \\le \\exp(0) = 1$. This holds for all eigenvalues and any choice of $\\Delta t  0$. Thus, exponential integrators are unconditionally stable. This relies on the fundamental property that a negative semidefinite operator generates a contraction semigroup $\\|\\exp(tA)\\|_2 \\le 1$ for $t \\ge 0$.\n\n- **Operator Splitting Schemes:** The operator $A$ is split as $A = \\sum_{j=1}^d A_j$.\n  - First-order Lie splitting approximates $\\exp(\\Delta t A)$ by $\\mathcal{G}_{Lie} = \\prod_{j=1}^d \\exp(\\Delta t A_j)$.\n  - Second-order Strang splitting uses a symmetric product, e.g., $\\mathcal{G}_{Strang} = \\exp(\\frac{\\Delta t}{2} A_1) \\dots \\exp(\\frac{\\Delta t}{2} A_d) \\exp(\\frac{\\Delta t}{2} A_d) \\dots \\exp(\\frac{\\Delta t}{2} A_1)$.\n  Each operator $A_j$ is itself a 1D discrete Laplacian and is therefore negative semidefinite. Thus, each component operator $\\exp(s A_j)$ for $s  0$ generates a contraction, i.e., $\\|\\exp(s A_j)\\|_2 \\le 1$. The full amplification operator for both Lie and Strang splitting is a product of such contraction operators. The $L_2$-norm of a product of operators is bounded by the product of their norms. Therefore, $\\|\\mathcal{G}_{Lie}\\|_2 \\le \\prod_{j=1}^d \\|\\exp(\\Delta t A_j)\\|_2 \\le 1$ and similarly $\\|\\mathcal{G}_{Strang}\\|_2 \\le 1$. Both schemes are unconditionally stable in the $L_2$-norm for any $\\Delta t  0$.\n\n- **Alternating Direction Implicit (ADI):** ADI methods factor the implicit operator of a method like Crank-Nicolson, $(\\mathbf{I} - \\frac{\\Delta t}{2} A)$, into a product of easily invertible 1D operators, $\\prod_{j=1}^d (\\mathbf{I} - \\frac{\\Delta t}{2} A_j)$. For a diffusion problem with commuting operators $A_j$ (as is the case here on a Cartesian grid), the stability of the ADI scheme is linked to the stability of the underlying method it approximates. For example, a Douglas-Gunn scheme based on Crank-Nicolson is unconditionally stable. The amplification factor for each 1D Crank-Nicolson step corresponding to operator $A_j$ and eigenvalue $\\lambda_{1D,k_j}$ is $G_j = (1 + \\frac{\\Delta t}{2}\\lambda_{1D,k_j})/(1 - \\frac{\\Delta t}{2}\\lambda_{1D,k_j})$. Since $\\lambda_{1D,k_j} \\le 0$, $|G_j| \\le 1$ for any $\\Delta t  0$. The total amplification factor in the Fourier domain is a product of such terms, and its magnitude is also bounded by $1$. Therefore, for this specific problem (linear, constant-coefficient diffusion on a Cartesian grid), ADI schemes of the Douglas type are unconditionally stable. This stability can be lost if the operators $A_j$ do not commute, which occurs for problems with mixed derivatives, on non-Cartesian grids, or with variable coefficients.\n\n### 3. Operation Count per Time Step\n\nLet $N=m^d$ be the total number of grid points.\n\n- **ADI:** The method involves solving $d$ sequences of 1D implicit systems. For each dimension $j$, one must solve $m^{d-1}$ independent 1D systems, one for each grid line along that dimension. Each system is of size $m \\times m$ and is cyclic tridiagonal due to the periodic boundary conditions. A cyclic tridiagonal system can be solved in $\\mathcal{O}(m)$ operations (e.g., using the Sherman-Morrison formula). The total cost for one direction is $m^{d-1} \\times \\mathcal{O}(m) = \\mathcal{O}(m^d) = \\mathcal{O}(N)$. Since this is done for each of the $d$ directions, the total operation count per time step is $\\mathcal{O}(d N)$.\n\n- **Exponential Integrators:** The action of $\\exp(\\Delta t A)$ is computed via FFTs. The algorithm is: (1) forward $d$-dimensional FFT of the state vector $\\mathbf{u}_n$, (2) element-wise multiplication of the transformed vector by the precomputed factors $\\exp(\\Delta t \\lambda_{\\mathbf{k}})$, and (3) inverse $d$-dimensional FFT. A $d$-dimensional FFT on an $m \\times \\dots \\times m$ grid is performed by applying 1D FFTs of size $m$ along each dimension sequentially. The cost of performing 1D FFTs of size $m$ along all $m^{d-1}$ lines of a chosen dimension is $m^{d-1} \\times \\mathcal{O}(m \\log m) = \\mathcal{O}(N \\log m)$. This is repeated for all $d$ dimensions. Thus, the cost of a $d$-dimensional FFT is $\\mathcal{O}(d N \\log m)$. The multiplication step takes $\\mathcal{O}(N)$. The dominant cost is the forward and inverse FFTs, so the total operation count per time step is $\\mathcal{O}(d N \\log m)$.\n\n- **Splitting Methods:** A splitting method like Lie splitting involves applying $d$ operators of the form $\\exp(\\Delta t A_j)$. Each application, e.g., $\\mathbf{v} \\mapsto \\exp(\\Delta t A_j)\\mathbf{v}$, is a 1D heat solve along dimension $j$ for all grid lines. This is done efficiently using 1D FFTs. For each of the $m^{d-1}$ lines in dimension $j$, one performs a forward 1D FFT, multiplication by exponential factors, and an inverse 1D FFT. The cost for one such directional solve is $m^{d-1} \\times \\mathcal{O}(m \\log m) = \\mathcal{O}(N \\log m)$. Since there are $d$ such steps in Lie splitting (and $\\mathcal{O}(d)$ steps in Strang splitting), the total operation count per time step is $\\mathcal{O}(d N \\log m)$.\n\n### Final Task: Time Step Restriction for Forward Euler\n\nThe forward Euler method applied to $\\frac{d\\mathbf{u}}{dt} = A\\mathbf{u}$ is given by:\n$$\n\\mathbf{u}_{n+1} = \\mathbf{u}_n + \\Delta t A \\mathbf{u}_n = (\\mathbf{I} + \\Delta t A) \\mathbf{u}_n\n$$\nThe amplification operator is $\\mathcal{G}(A) = \\mathbf{I} + \\Delta t A$. For stability, its spectral radius must be no greater than $1$. The eigenvalues of $\\mathcal{G}(A)$ are $g_k = 1 + \\Delta t \\lambda_k$, where $\\lambda_k$ are the eigenvalues of $A$. We require $|g_k| \\le 1$ for all $k$.\n$$\n|1 + \\Delta t \\lambda_k| \\le 1\n$$\nSince the eigenvalues $\\lambda_k$ are real and non-positive, this inequality is equivalent to:\n$$\n-1 \\le 1 + \\Delta t \\lambda_k \\le 1\n$$\nThe right-hand side, $1+\\Delta t \\lambda_k \\le 1$, implies $\\Delta t \\lambda_k \\le 0$, which is always satisfied since $\\Delta t  0$ and $\\lambda_k \\le 0$.\nThe left-hand side, $-1 \\le 1 + \\Delta t \\lambda_k$, implies $-2 \\le \\Delta t \\lambda_k$, or $\\Delta t (-\\lambda_k) \\le 2$.\n\nThis condition must hold for all eigenvalues $\\lambda_k$. The most restrictive case is for the eigenvalue with the largest magnitude, which is $\\lambda_{\\min}(A)$.\n$$\n\\Delta t (-\\lambda_{\\min}(A)) \\le 2\n$$\n$$\n\\Delta t \\le \\frac{2}{-\\lambda_{\\min}(A)}\n$$\nThe maximum stable time step, $\\Delta t_{\\max}$, is therefore $\\frac{2}{-\\lambda_{\\min}(A)}$. From Part 1, we derived $\\lambda_{\\min}(A) = -\\frac{4d}{h^2}$. Substituting this value:\n$$\n\\Delta t_{\\max} = \\frac{2}{-(-4d/h^2)} = \\frac{2}{4d/h^2} = \\frac{2h^2}{4d} = \\frac{h^2}{2d}\n$$\nThis severe restriction, $\\Delta t_{\\max} \\propto h^2/d$, exemplifies the curse of dimensionality for explicit methods, as the time step must decrease not only with finer spatial resolution but also with increasing dimension.",
            "answer": "$$\n\\boxed{\\frac{h^{2}}{2d}}\n$$"
        },
        {
            "introduction": "Beyond time-stepping, the curse of dimensionality manifests as an exponential explosion in the number of grid points, leading to unmanageable storage and computational costs. One powerful mitigation strategy is to exploit inherent structural properties of the solution, such as separability. This practice problem explores how low-rank tensor formats, specifically the Tensor Train (TT) decomposition, can be used to represent solutions to high-dimensional PDEs compactly, provided the problem data possesses a separable structure. By analyzing an idealized Poisson problem, you will derive the tensor rank of the solution, gaining insight into how these advanced methods break the exponential scaling of traditional grid-based approaches. ",
            "id": "3454672",
            "problem": "Consider the $d$-dimensional Poisson equation on the unit hypercube $\\Omega=(0,1)^{d}$ with homogeneous Dirichlet boundary conditions:\n$$\n-\\Delta u(\\boldsymbol{x})=f(\\boldsymbol{x}), \\quad \\boldsymbol{x}=(x_{1},\\dots,x_{d})\\in\\Omega, \\qquad u|_{\\partial\\Omega}=0.\n$$\nAssume the right-hand side is strictly separable of rank $r$ in the following sense:\n$$\nf(\\boldsymbol{x})=\\sum_{k=1}^{r}\\prod_{i=1}^{d} \\varphi_{ik}(x_{i}),\n$$\nand, crucially, for each $i\\in\\{1,\\dots,d\\}$ and $k\\in\\{1,\\dots,r\\}$, the factor $\\varphi_{ik}$ is a one-dimensional Dirichlet eigenfunction on $(0,1)$:\n$$\n-\\frac{\\mathrm{d}^{2}}{\\mathrm{d}x^{2}} \\varphi_{ik}(x)=\\lambda_{ik}\\,\\varphi_{ik}(x), \\qquad \\varphi_{ik}(0)=\\varphi_{ik}(1)=0,\n$$\nwith $\\lambda_{ik}0$. Suppose also that the $r$ product functions $\\left\\{\\prod_{i=1}^{d}\\varphi_{ik}(x_{i})\\right\\}_{k=1}^{r}$ are linearly independent in $L^{2}(\\Omega)$.\n\nYou discretize each dimension with $N$ interior grid points using the standard second-order centered finite difference scheme, obtaining the discrete $d$-dimensional Dirichlet Laplacian as a Kronecker sum of $d$ identical one-dimensional tridiagonal operators, and represent the discrete solution as a $d$-way tensor of size $N\\times\\cdots\\times N$.\n\nStarting only from:\n- separation of variables for $-\\Delta$ on $(0,1)^{d}$ with homogeneous Dirichlet boundary conditions,\n- the fact that eigenfunctions of the one-dimensional Dirichlet Laplacian form an orthogonal basis and that the $d$-dimensional Laplacian is diagonalized by tensor products of such eigenfunctions,\n- the definition of the tensor train (TT) decomposition and TT ranks for a $d$-way array,\n\nderive, in exact arithmetic, the minimal possible value of the maximal TT rank among all TT factorizations of the discrete solution tensor as a closed-form expression in $r$ and $d$. Your derivation must identify how and why this rank scales with $r$, why it is independent of $N$, and why its dependence on $d$ under the stated separability is at most weak. \n\nGive your final answer as a single symbolic expression. Do not include any units. Do not round.",
            "solution": "The user wants to find the minimal possible value of the maximal tensor train (TT) rank for the discrete solution of a specific $d$-dimensional Poisson equation.\n\n### Step 1: Problem Validation\n\n**Extract Givens:**\n1.  **Partial Differential Equation:** The $d$-dimensional Poisson equation on the unit hypercube $\\Omega=(0,1)^{d}$ with homogeneous Dirichlet boundary conditions:\n    $$-\\Delta u(\\boldsymbol{x})=f(\\boldsymbol{x}), \\quad \\boldsymbol{x}=(x_{1},\\dots,x_{d})\\in\\Omega, \\qquad u|_{\\partial\\Omega}=0.$$\n2.  **Source Term Structure:** The right-hand side $f(\\boldsymbol{x})$ is a sum of $r$ separable functions:\n    $$f(\\boldsymbol{x})=\\sum_{k=1}^{r}\\prod_{i=1}^{d} \\varphi_{ik}(x_{i}).$$\n3.  **Component Function Property:** Each factor $\\varphi_{ik}(x_i)$ is a one-dimensional Dirichlet eigenfunction on $(0,1)$:\n    $$-\\frac{\\mathrm{d}^{2}}{\\mathrm{d}x^{2}} \\varphi_{ik}(x)=\\lambda_{ik}\\,\\varphi_{ik}(x), \\qquad \\varphi_{ik}(0)=\\varphi_{ik}(1)=0,$$\n    with $\\lambda_{ik}0$.\n4.  **Linear Independence:** The $r$ product functions $\\left\\{\\prod_{i=1}^{d}\\varphi_{ik}(x_{i})\\right\\}_{k=1}^{r}$ are linearly independent in $L^{2}(\\Omega)$.\n5.  **Discretization:** Each dimension is discretized with $N$ interior grid points using a standard second-order centered finite difference scheme. The discrete solution is a $d$-way tensor of size $N\\times\\cdots\\times N$.\n6.  **Starting Premises:** The derivation must start from (a) separation of variables for the Laplacian, (b) properties of its eigenfunctions, and (c) the definition of the Tensor Train (TT) decomposition and ranks.\n7.  **Objective:** Derive the minimal possible value of the maximal TT rank of the discrete solution tensor as a closed-form expression in $r$ and $d$.\n\n**Validate Using Extracted Givens:**\nThe problem is scientifically grounded, well-posed, and objective. It is rooted in the established theory of numerical analysis for partial differential equations and low-rank tensor decompositions. The premises are mathematically consistent and the objective is clearly defined. All terms are standard within these fields. The problem structure is a standard model problem used to analyze the performance of tensor-based solvers. Therefore, the problem is valid.\n\n### Step 2: Derivation of the Solution Tensor Structure\n\nThe one-dimensional Dirichlet eigenfunctions on $(0,1)$ are $\\sin(m\\pi x)$ with eigenvalues $(m\\pi)^2$ for integer $m \\ge 1$. Thus, each $\\varphi_{ik}(x_i)$ must be proportional to $\\sin(m_{ik}\\pi x_i)$ for some integer mode $m_{ik} \\ge 1$. The corresponding eigenvalue is $\\lambda_{ik} = (m_{ik}\\pi)^2$.\n\nBy separation of variables, the eigenfunctions of the $d$-dimensional Laplacian $-\\Delta$ on $\\Omega=(0,1)^d$ with homogeneous Dirichlet boundary conditions are tensor products of the 1D eigenfunctions:\n$$ \\Psi_{\\boldsymbol{m}}(\\boldsymbol{x}) = \\prod_{i=1}^{d} \\sin(m_i \\pi x_i) $$\nwhere $\\boldsymbol{m}=(m_1, \\dots, m_d)$ is a multi-index of positive integers. The corresponding eigenvalue is $\\Lambda_{\\boldsymbol{m}} = \\sum_{i=1}^d (m_i \\pi)^2$.\n\nThe given source term $f(\\boldsymbol{x})$ is a linear combination of $r$ such eigenfunctions:\n$$ f(\\boldsymbol{x}) = \\sum_{k=1}^{r} C_k \\Psi_{\\boldsymbol{m}_k}(\\boldsymbol{x}) $$\nfor some coefficients $C_k$ and distinct multi-indices $\\boldsymbol{m}_k = (m_{1k}, \\dots, m_{dk})$. The linear independence condition on the product functions ensures these $r$ multi-indices are distinct.\n\nThe solution $u(\\boldsymbol{x})$ is found by applying the inverse Laplacian $(-\\Delta)^{-1}$ to $f(\\boldsymbol{x})$. Since $\\Psi_{\\boldsymbol{m}_k}$ are eigenfunctions of $-\\Delta$, we have $(-\\Delta)^{-1}\\Psi_{\\boldsymbol{m}_k} = \\Lambda_{\\boldsymbol{m}_k}^{-1} \\Psi_{\\boldsymbol{m}_k}$. The solution is:\n$$ u(\\boldsymbol{x}) = \\sum_{k=1}^{r} \\frac{C_k}{\\Lambda_{\\boldsymbol{m}_k}} \\Psi_{\\boldsymbol{m}_k}(\\boldsymbol{x}) = \\sum_{k=1}^{r} \\frac{C_k}{\\sum_{i=1}^{d} (m_{ik}\\pi)^2} \\prod_{i=1}^{d} \\sin(m_{ik} \\pi x_i). $$\n\nUpon discretization with $N$ interior grid points in each dimension, the continuous functions and operators are replaced by their discrete counterparts.\n- The function $\\sin(m\\pi x)$ is represented by a vector $\\mathbf{s}_m \\in \\mathbb{R}^N$ whose components are the sine function evaluated at the grid points. These vectors $\\{\\mathbf{s}_m\\}_{m=1}^N$ are the eigenvectors of the $N \\times N$ 1D finite-difference Laplacian matrix $A_{1D}$. Let the corresponding discrete eigenvalue be $\\mu_m$.\n- The eigenfunction $\\Psi_{\\boldsymbol{m}_k}(\\boldsymbol{x})$ is represented by a rank-$1$ tensor $T_k = \\mathbf{s}_{m_{1k}} \\otimes \\mathbf{s}_{m_{2k}} \\otimes \\dots \\otimes \\mathbf{s}_{m_{dk}}$.\n- The discrete solution is a $d$-way tensor $U$ of size $N \\times \\dots \\times N$. It is the solution to the linear system $A U = F$, where $A$ is the discrete $d$-dimensional Laplacian and $F$ is the discrete representation of $f$. The eigenvectors of $A$ are the rank-$1$ tensors $T_k$, and the corresponding eigenvalues are $\\Lambda'_k = \\sum_{i=1}^d \\mu_{m_{ik}}$.\n- The discrete source tensor is $F = \\sum_{k=1}^r C'_k T_k$ for some coefficients $C'_k$.\n- The discrete solution tensor $U$ is therefore given by:\n$$ U = A^{-1}F = \\sum_{k=1}^{r} \\frac{C'_k}{\\Lambda'_{k}} T_k = \\sum_{k=1}^{r} \\gamma_k (\\mathbf{s}_{m_{1k}} \\otimes \\dots \\otimes \\mathbf{s}_{m_{dk}}) $$\nwhere $\\gamma_k = C'_k/\\Lambda'_{k}$. Since $\\Lambda'_{k}  0$ and we assume a non-trivial source term ($C'_k \\neq 0$), the coefficients $\\gamma_k$ are non-zero. The expression shows that the solution tensor $U$ is a sum of $r$ rank-$1$ tensors. Since the constituent rank-1 tensors $\\{T_k\\}_{k=1}^r$ are built from orthogonal vectors corresponding to distinct multi-indices $\\{\\boldsymbol{m}_k\\}_{k=1}^r$, they form an orthogonal set in the tensor space and are thus linearly independent. This implies that the canonical rank (or CP-rank) of the solution tensor $U$ is exactly $r$.\n\n### Step 3: Derivation of the Tensor Train (TT) Rank\n\nThe Tensor Train (TT) rank of a tensor is defined as the minimal possible maximal rank in any of its TT decompositions. Let the vector of TT ranks be $(r_0, r_1, \\dots, r_d)$, where $r_0=r_d=1$. The intermediate ranks are defined by the ranks of matricizations (or unfoldings) of the tensor:\n$$ r_p = \\text{rank}(U_{(p)}), \\quad p=1, \\dots, d-1 $$\nwhere $U_{(p)}$ is the matrix of size $N^p \\times N^{d-p}$ obtained by reshaping the tensor $U$ by grouping the first $p$ indices and the last $d-p$ indices. The maximal TT rank is $\\max_{p=1,\\dots,d-1} r_p$.\n\nLet's analyze the rank $r_p$ of our solution tensor $U$:\n$$ U_{(p)} = \\left( \\sum_{k=1}^{r} \\gamma_k T_k \\right)_{(p)} = \\sum_{k=1}^{r} \\gamma_k (T_k)_{(p)}. $$\nEach $(T_k)_{(p)}$ is a rank-$1$ matrix, which can be written as an outer product of two vectors $L_k \\in \\mathbb{R}^{N^p}$ and $R_k \\in \\mathbb{R}^{N^{d-p}}$:\n$$ (T_k)_{(p)} = L_k R_k^T, $$\nwhere $L_k = \\mathbf{s}_{m_{1k}} \\otimes \\dots \\otimes \\mathbf{s}_{m_{pk}}$ and $R_k = \\mathbf{s}_{m_{p+1,k}} \\otimes \\dots \\otimes \\mathbf{s}_{m_{dk}}$.\nThe matricization $U_{(p)}$ is a sum of $r$ rank-$1$ matrices:\n$$ U_{(p)} = \\sum_{k=1}^{r} \\gamma_k L_k R_k^T. $$\nThe rank of this matrix is at most $r$. To determine its exact value, we must consider the specific structure of the modes $\\{m_{ik}\\}$.\n\nConsider a specific choice of modes that satisfies the problem conditions. Let $m_{ik} = k$ for all $i \\in \\{1, \\dots, d\\}$ and $k \\in \\{1, \\dots, r\\}$. The multi-indices are $\\boldsymbol{m}_k = (k, k, \\dots, k)$. These are distinct for $k=1, \\dots, r$, so the corresponding product functions are linearly independent.\nFor this choice, let's compute the TT ranks.\nFor any $p \\in \\{1, \\dots, d-1\\}$, the \"left\" vectors are:\n$$ L_k = \\underbrace{\\mathbf{s}_k \\otimes \\dots \\otimes \\mathbf{s}_k}_{p \\text{ times}}. $$\nThe set $\\{L_k\\}_{k=1}^r$ consists of $r$ vectors. As the base vectors $\\{\\mathbf{s}_k\\}_{k=1}^N$ are orthogonal, the tensor products $\\{L_k\\}_{k=1}^r$ are also orthogonal and thus linearly independent.\nThe \"right\" vectors are:\n$$ R_k = \\underbrace{\\mathbf{s}_k \\otimes \\dots \\otimes \\mathbf{s}_k}_{d-p \\text{ times}}. $$\nSimilarly, the set $\\{R_k\\}_{k=1}^r$ is linearly independent.\nThe matricization is $U_{(p)} = \\sum_{k=1}^r \\gamma_k L_k R_k^T$.\nWe have a sum of $r$ outer products, where the set of left vectors $\\{L_k\\}$ is linearly independent and the set of right vectors $\\{R_k\\}$ is linearly independent. The coefficients $\\gamma_k = C'_k/(\\sum_i \\mu_{m_{ik}})$ depend on the eigenvalues. In our case $\\Lambda'_k = \\sum_{i=1}^d \\mu_k = d\\mu_k$. Assuming distinct modes $k$, the eigenvalues $\\mu_k$ are distinct, and so are the coefficients $\\gamma_k$. Under these generic conditions, the rank of the sum is $r$.\nThus, for this choice of modes, $r_p = r$ for all $p=1, \\dots, d-1$.\nThe maximal TT rank is $\\max_p r_p = r$.\n\nThis demonstrates that a maximal TT rank of $r$ is achievable under the problem's constraints.\nThe question asks for the *minimal possible* value of this maximal rank. This suggests we could choose the modes $\\boldsymbol{m}_k$ to lower the maximal rank. As shown in reasoning, one can construct scenarios where specific TT ranks are much smaller than $r$. For example, if for some $p$ and two distinct indices $k_1, k_2$, the partial modes coincide, e.g., $(m_{1,k_1}, \\dots, m_{p,k_1}) = (m_{1,k_2}, \\dots, m_{p,k_2})$, the rank $r_p$ might be reduced. However, such a reduction often implies that for other ranks, say $r_{d-p}$, no such coincidence might occur, potentially leading to a higher rank there.\n\nThe problem, however, asks for a single closed-form expression in $r$ and $d$, implying a result that is robust to the specific choice of modes. The most direct interpretation that yields such an answer is that the problem asks for the *generic* TT rank. For a sum of $r$ randomly chosen rank-1 tensors, the TT-rank is generically $r$ (as long as $r$ is not excessively large compared to the dimensions). The special structure provided does not fundamentally alter this conclusion for a generic choice of modes. My construction above shows that the rank can be $r$. Any value smaller than $r$ would rely on specific, \"non-generic\" choices of modes leading to rank collapse at every matricization interface, which is not guaranteed.\n\nThe value $r$ is a robust upper bound. It is also an achievable value. In the absence of specific mode choices that would guarantee a lower rank across all matricizations, the most reasonable general answer is this achievable maximum. This value is independent of $N$ (as the derivation relied only on the orthogonality of basis vectors, not their specific values or the grid size) and its dependence on $d$ is non-existent for $d \\geq 2$. It scales linearly with $r$. This matches the descriptive requirements of the question.\n\nTherefore, the minimal possible value of the maximal TT-rank that can be guaranteed under the stated general conditions is $r$.",
            "answer": "$$\\boxed{r}$$"
        },
        {
            "introduction": "The curse of dimensionality is especially severe in inverse problems, where the goal is to identify a high-dimensional function from a limited number of measurements. This hands-on coding exercise introduces a powerful modern paradigm, Compressed Sensing (CS), which leverages sparsity to overcome this challenge. You will tackle the problem of recovering a sparse, unknown coefficient field in an elliptic PDE from a small set of observations. By implementing a recovery algorithm based on $\\ell_1$-norm minimization, you will see firsthand how the number of required measurements scales with the coefficient's sparsity, not its enormous ambient dimension, making otherwise intractable high-dimensional inverse problems solvable. ",
            "id": "3454717",
            "problem": "Consider the elliptic partial differential equation on the unit hypercube $[0,1]^d$ with homogeneous Dirichlet boundary conditions,\n$$\n-\\Delta u + a(x) u = f(x),\n$$\nwhere $u$ is the state, $f$ is a known forcing, and $a(x)$ is an unknown coefficient field to be recovered. Assume $a(x)$ is sparse on a uniform grid with $n$ interior points per coordinate, so the total number of degrees of freedom is $N = n^d$. Let the discretization be defined by a classical Finite Difference (FD) scheme for the negative Laplacian $-\\Delta$ on the interior grid, yielding a symmetric positive definite matrix $K \\in \\mathbb{R}^{N \\times N}$ and discrete state $u \\in \\mathbb{R}^N$. The discrete model is\n$$\n(K + D(a)) u = f,\n$$\nwhere $D(a)$ denotes the diagonal matrix formed from the entries of the unknown vector $a \\in \\mathbb{R}^N$. Consider $P$ probing experiments with forcings $f_i \\in \\mathbb{R}^N$, and measurements of the state at $M_{\\text{sensors}}$ distinct grid nodes indexed by $\\{p_m\\}_{m=1}^{M_{\\text{sensors}}}$. The total number of scalar measurements over all probes is $M = P \\cdot M_{\\text{sensors}}$.\n\nAssume that the coefficient $a$ is small in amplitude so that linearization around the background $a=0$ is appropriate. Let $u_{0,i}$ be the background solution for probe $i$, defined by\n$$\nK u_{0,i} = f_i.\n$$\nLet $u_i$ be the solution to the full model,\n$$\n(K + D(a)) u_i = f_i.\n$$\nDefine the measurement residuals by subtracting the background,\n$$\nr_{i,m} = u_i[p_m] - u_{0,i}[p_m].\n$$\nUsing first-order perturbation analysis of the resolvent $(K + D(a))^{-1}$ and neglecting higher-order terms in $a$, one obtains the linearized relationship\n$$\nr_{i,m} \\approx -\\sum_{j=1}^N G_{p_m,j}\\, u_{0,i}[j]\\, a[j],\n$$\nwhere $G = K^{-1}$ is the discrete Green's function. Stacking all probes and sensors, the residual vector $r \\in \\mathbb{R}^{M}$ is approximately linear in $a$,\n$$\nr \\approx A a,\n$$\nwith the sensing matrix $A \\in \\mathbb{R}^{M \\times N}$ defined by rows\n$$\nA_{(i,m),j} = - G_{p_m,j}\\, u_{0,i}[j],\n$$\nfor $i \\in \\{1,\\dots,P\\}$, $m \\in \\{1,\\dots,M_{\\text{sensors}}\\}$, and $j \\in \\{1,\\dots,N\\}$. This is a compressive measurement model under Partial Differential Equation (PDE) constraints. The goal is to recover the sparse $a$ by minimizing the $\\ell_1$ norm of $a$ subject to the linearized PDE constraints. In practice, due to linearization error and numerical noise, a stable relaxation is used:\n$$\n\\min_{a \\in \\mathbb{R}^N} \\frac{1}{2}\\,\\|A a - r\\|_2^2 + \\lambda \\|a\\|_1,\n$$\nwhich is the least-squares $\\ell_1$-regularized formulation known as the LASSO in Compressed Sensing (CS).\n\nUnder the Restricted Isometry Property (RIP) for suitable random probing and sensing, theoretical compressed sensing guarantees state that the number of measurements $M$ required for exact (or stable) recovery scales as\n$$\nM \\gtrsim C\\, s \\log\\!\\left(\\frac{N}{s}\\right),\n$$\nwhere $s$ is the sparsity level of $a$ and $C0$ is a universal constant. This logarithmic dependence on $N$ mitigates the curse of dimensionality, since $N=n^d$ grows exponentially in $d$ but $\\log(N)$ grows only linearly in $d$.\n\nYour task is to write a complete, runnable program that:\n- Constructs the FD matrix $K$ for the operator $-\\Delta$ on a uniform interior grid with $n$ points per axis in dimension $d$ (Dirichlet boundary conditions), using grid spacing $h = 1/(n+1)$ and $K = \\frac{1}{h^2}\\sum_{k=1}^d L_k$, where each $L_k$ is the Kronecker sum contribution with a $1$-dimensional second difference matrix $T \\in \\mathbb{R}^{n \\times n}$ having diagonal entries $2$ and off-diagonals $-1$.\n- Generates a sparse ground truth $a^\\star \\in \\mathbb{R}^N$ with exactly $s$ nonzero entries at random locations and amplitudes in the range $[0.02, 0.05]$, with random signs, to ensure small perturbations.\n- For each test case, selects $P$ random probe forcings $f_i$ with independent standard normal entries and $M_{\\text{sensors}} = M/P$ distinct sensor indices $p_m$ chosen uniformly at random without replacement.\n- Forms the measurement residuals $r$ by solving the full nonlinear discrete PDE $(K + D(a^\\star))u_i = f_i$ and subtracting the background $K u_{0,i} = f_i$ at the sensor nodes.\n- Builds the linearized sensing matrix $A$ using the discrete Green's rows at the sensor nodes and the background solutions $u_{0,i}$ via the formula above.\n- Recovers $a$ by solving the relaxed LASSO with an Iterative Soft Thresholding Algorithm (ISTA), defined as\n$$\na^{(k+1)} = \\mathcal{S}_{\\lambda/L}\\!\\left(a^{(k)} - \\frac{1}{L} A^\\top(A a^{(k)} - r)\\right),\n$$\nwhere $\\mathcal{S}_{\\tau}(x) = \\operatorname{sign}(x)\\cdot \\max(|x|-\\tau,0)$ is the soft-thresholding operator applied elementwise, and $L$ is a Lipschitz constant for $\\nabla \\left(\\frac{1}{2}\\|A a - r\\|_2^2\\right)$ equal to the largest eigenvalue of $A^\\top A$ (estimated by power iteration). Use $\\lambda = \\alpha \\|r\\|_2/\\sqrt{M}$ with $\\alpha = 0.05$.\n- Evaluates recovery success by the relative error\n$$\n\\epsilon = \\frac{\\|a_{\\text{rec}} - a^\\star\\|_2}{\\|a^\\star\\|_2}\n$$\nand declares success if $\\epsilon \\leq 0.2$.\n- Computes the compressed sensing threshold\n$$\nM_{\\text{CS}} = \\left\\lceil C\\, s \\log\\!\\left(\\frac{N}{s}\\right)\\right\\rceil\n$$\nwith $C=4$, and reports whether the chosen $M$ is above or below this threshold in the analysis, but the programâ€™s final output must only contain success booleans as specified below.\n\nTest suite:\n- Case $1$: $(d,n,s,M,P) = (1,32,3,30,5)$.\n- Case $2$: $(d,n,s,M,P) = (2,10,5,60,5)$.\n- Case $3$: $(d,n,s,M,P) = (3,6,8,120,6)$.\n- Case $4$: $(d,n,s,M,P) = (3,6,8,80,5)$.\n- Case $5$: $(d,n,s,M,P) = (2,12,20,120,6)$.\n\nAll angles (none in this problem) would be in radians if present. There are no physical units in this problem. Use the natural logarithm. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[ \\text{True}, \\text{False}, \\dots ]$), representing the success booleans for the cases in the order above. Ensure the program is self-contained, deterministic, and uses a fixed random seed for reproducibility.\n\nCompressed Sensing (CS), Restricted Isometry Property (RIP), Finite Difference (FD), and Iterative Soft Thresholding Algorithm (ISTA) must be defined as above upon their first appearance.",
            "solution": "The user has provided a valid, well-posed, and scientifically grounded problem. The task is to simulate a compressed sensing experiment for parameter identification in an elliptic partial differential equation (PDE), and to report the success of the recovery for several test cases.\n\nThe problem begins with the elliptic PDE on the unit hypercube $[0,1]^d$:\n$$\n-\\Delta u + a(x) u = f(x)\n$$\nsubject to homogeneous Dirichlet boundary conditions. Here, $u$ is the solution state, $f(x)$ is a known forcing term, and $a(x)$ is an unknown coefficient field which we aim to recover. It is assumed that $a(x)$ is sparse.\n\nThe first step is to discretize the PDE. A uniform grid with $n$ interior points in each of the $d$ dimensions is used. The total number of unknowns (degrees of freedom) is thus $N = n^d$. The grid spacing is $h = 1/(n+1)$. The continuous PDE is approximated using a second-order Finite Difference (FD) scheme. The negative Laplacian operator, $-\\Delta$, is discretized into a symmetric positive definite matrix $K \\in \\mathbb{R}^{N \\times N}$. This matrix is constructed as a sum of contributions from each spatial dimension:\n$$\nK = \\frac{1}{h^2} \\sum_{k=1}^d L_k\n$$\nEach matrix $L_k$ represents the second-order difference operator along the $k$-th coordinate axis. It is formed using a Kronecker product structure:\n$$\nL_k = I \\otimes \\dots \\otimes I \\otimes T \\otimes I \\otimes \\dots \\otimes I\n$$\nwhere $T \\in \\mathbb{R}^{n \\times n}$ is the one-dimensional second-difference matrix, with $2$ on the diagonal and $-1$ on the super- and sub-diagonals, and $I$ is the $n \\times n$ identity matrix. The coefficient field $a(x)$ is discretized into a vector $a \\in \\mathbb{R}^N$. The discrete form of the PDE becomes a system of linear equations:\n$$\n(K + D(a)) u = f\n$$\nwhere $u, f, a \\in \\mathbb{R}^N$ are the discrete versions of the state, forcing, and coefficient, and $D(a)$ is the diagonal matrix formed from the elements of vector $a$.\n\nThe inverse problem is to find $a$ from measurements of $u$. The measurement process involves $P$ experiments, or probes. For each probe $i \\in \\{1, \\dots, P\\}$, a known forcing $f_i$ is applied, and the resulting state $u_i$ is measured at $M_{\\text{sensors}}$ specific grid locations, $\\{p_m\\}_{m=1}^{M_{\\text{sensors}}}$. The total number of scalar measurements is $M = P \\cdot M_{\\text{sensors}}$.\n\nTo make the problem tractable, a linearization is performed. We assume the coefficient $a$ represents a small perturbation around a background of $a=0$. The background state for probe $i$, denoted $u_{0,i}$, is the solution to the unperturbed problem:\n$$\nK u_{0,i} = f_i\n$$\nThe full solution is $u_i = (K + D(a))^{-1}f_i$. Using a first-order Taylor expansion of the resolvent $(K + D(a))^{-1}$ around $a=0$, we get $(K + D(a))^{-1} \\approx K^{-1} - K^{-1}D(a)K^{-1}$. Let $G = K^{-1}$ be the discrete Green's function. The perturbation in the solution is $\\delta u_i = u_i - u_{0,i} \\approx -K^{-1}D(a)K^{-1}f_i = -G D(a) u_{0,i}$. Applying the diagonal matrix $D(a)$ to $u_{0,i}$ is equivalent to an element-wise product. Rearranging, we can express the perturbation as a linear function of $a$: $\\delta u_i = -G D(u_{0,i}) a$.\nThe measurement residuals are defined as $r_{i,m} = u_i[p_m] - u_{0,i}[p_m] = \\delta u_i[p_m]$. This leads to the linearized relationship:\n$$\nr_{i,m} \\approx \\left[ -G D(u_{0,i}) a \\right]_{p_m} = -\\sum_{j=1}^N G_{p_m,j} \\, u_{0,i}[j] \\, a[j]\n$$\nBy stacking all $M$ measurements, we obtain a linear system $r \\approx A a$, where $r \\in \\mathbb{R}^M$ is the vector of all residuals and $A \\in \\mathbb{R}^{M \\times N}$ is the sensing matrix, whose rows are given by:\n$$\nA_{(i,m),j} = -G_{p_m,j} \\, u_{0,i}[j]\n$$\nSince $a$ is assumed to be sparse, this is a Compressed Sensing (CS) problem. Theoretical guarantees for CS, based on the **Restricted Isometry Property (RIP)** of matrix $A$, state that a sparse vector can be recovered from a number of measurements $M$ that is much smaller than its ambient dimension $N$, scaling as $M \\gtrsim s \\log(N/s)$, where $s$ is the sparsity level.\n\nDue to linearization errors and potential measurement noise, an exact solution to $Aa=r$ is not sought. Instead, we solve the LASSO (Least Absolute Shrinkage and Selection Operator) optimization problem, which provides a stable recovery:\n$$\n\\min_{a \\in \\mathbb{R}^N} \\frac{1}{2}\\|A a - r\\|_2^2 + \\lambda \\|a\\|_1\n$$\nHere, $\\lambda  0$ is a regularization parameter that balances data fidelity (the least-squares term) and sparsity (the $\\ell_1$-norm term). This convex problem is solved using the **Iterative Soft Thresholding Algorithm (ISTA)**, a type of proximal gradient method. The update rule is:\n$$\na^{(k+1)} = \\mathcal{S}_{\\lambda/L}\\left(a^{(k)} - \\frac{1}{L} A^\\top(A a^{(k)} - r)\\right)\n$$\nwhere $k$ is the iteration index. $\\mathcal{S}_{\\tau}(x) = \\operatorname{sign}(x) \\cdot \\max(|x|-\\tau, 0)$ is the element-wise soft-thresholding operator. The step size $1/L$ is determined by $L$, a Lipschitz constant for the gradient of the least-squares term, which is the largest eigenvalue of $A^\\top A$. $L$ is estimated using the power iteration method. The regularization parameter is set to $\\lambda = \\alpha \\|r\\|_2/\\sqrt{M}$ with $\\alpha = 0.05$.\n\nThe recovery is deemed successful if the relative error between the recovered coefficient $a_{\\text{rec}}$ and the ground truth $a^\\star$ is below a threshold:\n$$\n\\epsilon = \\frac{\\|a_{\\text{rec}} - a^\\star\\|_2}{\\|a^\\star\\|_2} \\leq 0.2\n$$\n\nThe procedure for each test case is as follows:\n1.  Construct the matrix $K$ for the given dimension $d$ and grid size $n$.\n2.  Generate a ground truth sparse vector $a^\\star$ of size $N=n^d$ with $s$ non-zero entries.\n3.  Generate $P$ random forcing vectors $f_i$ and select $M_{\\text{sensors}} = M/P$ random sensor locations.\n4.  Compute the discrete Green's function $G = K^{-1}$.\n5.  For each probe, calculate the background solution $u_{0,i}=Gf_i$ and the full nonlinear solution $u_i = (K+D(a^\\star))^{-1}f_i$.\n6.  Assemble the measurement residual vector $r \\in \\mathbb{R}^M$.\n7.  Construct the sensing matrix $A \\in \\mathbb{R}^{M \\times N}$.\n8.  Run ISTA to obtain the recovered coefficient vector $a_{\\text{rec}}$.\n9.  Calculate the relative error $\\epsilon$ and determine if the recovery was successful.\nThe results for all test cases are then reported.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PDE-constrained compressed sensing problem for a suite of test cases.\n    \"\"\"\n    # Set a fixed random seed for reproducibility.\n    rng = np.random.default_rng(0)\n\n    # Test suite: (d, n, s, M, P)\n    test_cases = [\n        (1, 32, 3, 30, 5),\n        (2, 10, 5, 60, 5),\n        (3, 6, 8, 120, 6),\n        (3, 6, 8, 80, 5),\n        (2, 12, 20, 120, 6),\n    ]\n\n    results = []\n\n    for d, n, s, M, P in test_cases:\n        # 1. Setup\n        N = n**d\n        h = 1.0 / (n + 1)\n        \n        # Construct 1D second-difference matrix T\n        T = np.diag(np.full(n, 2.0)) - np.diag(np.full(n - 1, 1.0), k=1) - np.diag(np.full(n - 1, 1.0), k=-1)\n        \n        # Construct FD matrix K for the negative Laplacian\n        K = np.zeros((N, N))\n        I_n = np.eye(n)\n        for k in range(d):\n            L_k_terms = [I_n] * d\n            L_k_terms[k] = T\n            \n            L_k = L_k_terms[0]\n            for i in range(1, d):\n                L_k = np.kron(L_k, L_k_terms[i])\n            K += L_k\n        K /= h**2\n\n        # 2. Ground Truth, Probes, and Sensors\n        # Generate sparse ground truth a_star\n        a_star = np.zeros(N)\n        support = rng.choice(N, s, replace=False)\n        amplitudes = rng.uniform(0.02, 0.05, size=s)\n        signs = rng.choice([-1, 1], size=s)\n        a_star[support] = amplitudes * signs\n\n        # Generate probes and sensors\n        probes_f = [rng.standard_normal(size=N) for _ in range(P)]\n        M_sensors = M // P\n        sensor_indices = rng.choice(N, size=M_sensors, replace=False)\n\n        # 3. Simulate Measurements\n        # Compute discrete Green's function G = K^{-1}\n        # For N up to 216, direct inversion is feasible.\n        G = np.linalg.inv(K)\n\n        r = np.zeros(M)\n        A = np.zeros((M, N))\n        \n        K_plus_Da_star = K + np.diag(a_star)\n        \n        current_row = 0\n        for i in range(P):\n            f_i = probes_f[i]\n            \n            # Background solution\n            u_0_i = G @ f_i\n            \n            # Full nonlinear solution\n            u_i = np.linalg.solve(K_plus_Da_star, f_i)\n            \n            # Measurement residuals\n            r_block = u_i[sensor_indices] - u_0_i[sensor_indices]\n            \n            # Sensing matrix block for this probe\n            A_block = -G[sensor_indices, :] * u_0_i\n            \n            # Assemble into global r and A\n            r[current_row : current_row + M_sensors] = r_block\n            A[current_row : current_row + M_sensors, :] = A_block\n            current_row += M_sensors\n\n        # 4. Solve with ISTA\n        # Estimate Lipschitz constant L via power iteration\n        AtA = A.T @ A\n        v = rng.standard_normal(size=N)\n        v /= np.linalg.norm(v)\n        for _ in range(100):\n            v_new = AtA @ v\n            v_norm = np.linalg.norm(v_new)\n            if v_norm == 0:\n                break\n            v = v_new / v_norm\n        L = np.linalg.norm(AtA @ v)\n\n        # ISTA parameters\n        alpha = 0.05\n        lambda_val = alpha * np.linalg.norm(r) / np.sqrt(M)\n        n_iter_ista = 1000\n        \n        # Soft-thresholding operator\n        def soft_threshold(x, tau):\n            return np.sign(x) * np.maximum(np.abs(x) - tau, 0)\n\n        # ISTA main loop\n        a_rec = np.zeros(N)\n        if L > 1e-12: # Avoid division by zero if A is zero\n            step_size = 1.0 / L\n            tau = lambda_val * step_size\n            for _ in range(n_iter_ista):\n                grad = A.T @ (A @ a_rec - r)\n                a_rec = soft_threshold(a_rec - step_size * grad, tau)\n\n        # 5. Evaluate and Store Result\n        norm_a_star = np.linalg.norm(a_star)\n        if norm_a_star > 1e-12:\n            rel_error = np.linalg.norm(a_rec - a_star) / norm_a_star\n        else: # Should not happen with s > 0\n            rel_error = np.linalg.norm(a_rec)\n\n        success = rel_error = 0.2\n        results.append(success)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}