{
    "hands_on_practices": [
        {
            "introduction": "物理信息神经网络（PINNs）的核心在于利用自动微分来计算偏微分方程（PDE）的残差。本练习旨在探讨这一过程背后的数学基础，特别是当处理二阶PDE时。通过推导神经网络的二阶导数的符号表达式，我们将揭示激活函数的选择与物理约束损失项的适定性之间的深刻联系，这对于构建稳定且准确的PINN模型至关重要。",
            "id": "3430986",
            "problem": "考虑一个用于一维偏微分方程残差的物理信息神经网络 (PINN) 近似，其形式为 $r(x)=\\mathcal{N}[u](x)$，其中 $u$ 由一个单隐藏层神经网络近似：\n$$\nu_{\\theta}(x) \\;=\\; \\sum_{k=1}^{m} a_{k}\\,\\sigma\\!\\left(w_{k}\\,x + b_{k}\\right),\n$$\n其中可训练参数为 $a_{k}, w_{k}, b_{k} \\in \\mathbb{R}$，$k=1,\\dots,m$，激活函数为标量函数 $\\sigma:\\mathbb{R}\\to\\mathbb{R}$。在为二阶算子（例如泊松算子）强制施加物理信息神经网络 (PINN) 典型的逐点强形式残差时，自动微分 (AD) 必须在紧凑域 $x \\in [\\alpha,\\beta]$（其中 $\\alpha,\\beta \\in \\mathbb{R}$ 且 $\\alpha  \\beta$）上精确且稳定地计算二阶导数 $u_{\\theta}''(x)$。\n\n从微积分的基本法则（微分的线性和复合函数的链式法则）出发，完成以下任务：\n\n- 推导二阶导数 $u_{\\theta}''(x)$ 的一个闭式符号表达式，该表达式用参数 $a_{k},w_{k},b_{k}$ 和 $\\sigma$ 的导数来表示。\n- 然后，利用可微性和连续性的经典概念，讨论 $\\sigma$ 应满足的充分的正则性和有界性条件（以及在 $x \\in [\\alpha,\\beta]$ 上对参数 $a_{k},w_{k},b_{k}$ 的一些温和约束），以确保 $u_{\\theta}''(x)$ 的自动微分 (AD) 是良定且数值稳定的。你的讨论应基于：\n  - 经典二阶导数存在的必要性，这与在强制强形式二阶残差时需要 $u_{\\theta}\\in C^{2}$ 相关联。\n  - 通过 $\\sigma''$ 和 $\\sigma'''$ 的上确界范数界来界定 $|u_{\\theta}''(x)|$ 以及 $u_{\\theta}''(x)$ 在 $[\\alpha,\\beta]$ 上关于 $x$ 的利普希茨常数。\n  - 常见激活函数选择的影响（例如，为什么使用像修正线性单元这样的非光滑激活函数对二阶导数有问题，以及为什么像双曲正切或 softplus 这样的光滑激活函数可能更可取）。\n\n提供 $u_{\\theta}''(x)$ 的表达式作为你的最终答案。无需进行数值四舍五入。最终答案中不要包含单位。",
            "solution": "该问题要求推导单隐藏层神经网络近似 $u_{\\theta}(x)$ 的二阶导数，并随后分析激活函数 $\\sigma$ 需要满足何种条件，才能确保通过自动微分 (AD) 对其进行求值是良定且数值稳定的。\n\n首先，我们来推导二阶导数 $u_{\\theta}''(x)$。神经网络近似被表示为输入 $x$ 的仿射变换经过激活函数后的线性组合：\n$$\nu_{\\theta}(x) = \\sum_{k=1}^{m} a_{k}\\,\\sigma\\!\\left(w_{k}\\,x + b_{k}\\right)\n$$\n其中 $\\theta = \\{a_k, w_k, b_k\\}_{k=1}^m$ 是可训练参数。我们假设激活函数 $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ 是充分可微的。\n\n为了求一阶导数 $u_{\\theta}'(x)$，我们将微分算子 $\\frac{d}{dx}$ 应用于 $u_{\\theta}(x)$ 的表达式。根据微分的线性性质，和的导数等于导数的和：\n$$\nu_{\\theta}'(x) = \\frac{d}{dx} \\left( \\sum_{k=1}^{m} a_{k}\\,\\sigma\\!\\left(w_{k}\\,x + b_{k}\\right) \\right) = \\sum_{k=1}^{m} \\frac{d}{dx} \\left[ a_{k}\\,\\sigma\\!\\left(w_{k}\\,x + b_{k}\\right) \\right]\n$$\n对和中的每一项，我们应用链式法则。令激活函数的参数为 $z_k(x) = w_k x + b_k$。那么 $\\frac{d}{dx} z_k(x) = w_k$。应用链式法则，我们得到：\n$$\n\\frac{d}{dx} \\left[ a_{k}\\,\\sigma\\!\\left(z_k(x)\\right) \\right] = a_k \\cdot \\sigma'(z_k(x)) \\cdot \\frac{d}{dx}z_k(x) = a_k \\cdot \\sigma'(w_k x + b_k) \\cdot w_k\n$$\n其中 $\\sigma'$ 表示 $\\sigma$ 对其参数的一阶导数。将此代入和中，得到网络的一阶导数表达式：\n$$\nu_{\\theta}'(x) = \\sum_{k=1}^{m} a_{k} w_{k} \\sigma'(w_{k}\\,x + b_{k})\n$$\n只要 $\\sigma$ 至少是一次可微的，该表达式就有效。\n\n接下来，我们通过对 $u_{\\theta}'(x)$ 关于 $x$ 求导来求二阶导数 $u_{\\theta}''(x)$。同样，我们使用微分的线性性质和链式法则：\n$$\nu_{\\theta}''(x) = \\frac{d}{dx} \\left( \\sum_{k=1}^{m} a_{k} w_{k} \\sigma'(w_{k}\\,x + b_{k}) \\right) = \\sum_{k=1}^{m} \\frac{d}{dx} \\left[ a_{k} w_{k} \\sigma'(w_{k}\\,x + b_{k}) \\right]\n$$\n对和中的每一项应用链式法则：\n$$\n\\frac{d}{dx} \\left[ a_{k} w_{k} \\sigma'(w_{k}\\,x + b_{k}) \\right] = a_k w_k \\cdot \\sigma''(w_k x + b_k) \\cdot \\frac{d}{dx}(w_k x + b_k) = a_k w_k \\cdot \\sigma''(w_k x + b_k) \\cdot w_k = a_k w_k^2 \\sigma''(w_k x + b_k)\n$$\n其中 $\\sigma''$ 表示 $\\sigma$ 的二阶导数。对所有项求和，我们得到神经网络二阶导数的闭式符号表达式：\n$$\nu_{\\theta}''(x) = \\sum_{k=1}^{m} a_{k} w_{k}^{2} \\sigma''(w_{k}\\,x + b_{k})\n$$\n这是在强制二阶偏微分方程残差时需要由 AD 计算的表达式。\n\n现在，我们讨论 $\\sigma$ 需要满足的充分的正则性和有界性条件，以使该计算在紧凑域 $x \\in [\\alpha, \\beta]$ 上是良定且数值稳定的。\n\n1.  **存在性与连续性（$C^2$ 正则性）**：为了使二阶偏微分方程的强形式残差是良定的，经典二阶导数 $u_{\\theta}''(x)$ 必须存在。如推导的表达式所示，在某点 $x$ 处 $u_{\\theta}''(x)$ 的存在性要求对所有 $k=1, \\dots, m$ 都存在 $\\sigma''(w_k x + b_k)$。为了使这对所有 $x \\in [\\alpha, \\beta]$ 都成立，$\\sigma(z)$ 必须在其输入 $z$ 的取值范围，即 $\\{w_k x + b_k \\mid x \\in [\\alpha, \\beta], k=1, \\dots, m\\}$，上是二次可微的。更严格地说，为了使解是二阶连续可微的（$u_{\\theta} \\in C^2([\\alpha, \\beta])$），这是二阶偏微分方程理论中的一个标准假设，$u_{\\theta}''(x)$ 必须是 $x$ 的连续函数。由于 $u_{\\theta}''(x)$ 是 $\\sigma''$ 经过缩放和平移后的版本的有限和，如果 $\\sigma''$ 本身是连续函数，则 $u_{\\theta}''(x)$ 的连续性得到保证。因此，一个充分条件是激活函数 $\\sigma$ 属于 $C^2(\\mathbb{R})$ 类。\n\n2.  **$u_{\\theta}''(x)$ 的有界性**：为保证数值稳定性，AD 计算过程中的值不能变得过大，这可能导致浮点溢出或其他数值病态问题。我们可以使用三角不等式来界定二阶导数在域 $[\\alpha, \\beta]$ 上的量级：\n    $$\n    |u_{\\theta}''(x)| = \\left| \\sum_{k=1}^{m} a_{k} w_{k}^{2} \\sigma''(w_{k}\\,x + b_{k}) \\right| \\le \\sum_{k=1}^{m} |a_{k}| w_{k}^{2} |\\sigma''(w_{k}\\,x + b_{k})|\n    $$\n    为确保对于任何有限参数 $a_k, w_k, b_k$ 的选择，该值都是有界的，我们要求 $\\sigma''$ 是有界的。令 $M_2 = \\sup_{z \\in \\mathbb{R}} |\\sigma''(z)|  \\infty$。那么我们就得到了二阶导数的一个一致界：\n    $$\n    \\sup_{x \\in [\\alpha, \\beta]} |u_{\\theta}''(x)| \\le \\left( \\sum_{k=1}^{m} |a_{k}| w_{k}^{2} \\right) M_2\n    $$\n    这个条件，即 $\\sigma''$ 的有界性，是数值稳定性的一个关键因素。\n\n3.  **$u_{\\theta}''(x)$ 的利普希茨常数的有界性**：优化算法的稳定性以及损失景观的行为受到偏微分方程残差变化快慢的影响。这由 $u_{\\theta}''(x)$ 的利普希茨常数来表征，其上界是其导数量级的上确界 $|u_{\\theta}'''(x)|$。对 $u_{\\theta}''(x)$ 求导得到：\n    $$\n    u_{\\theta}'''(x) = \\sum_{k=1}^{m} a_{k} w_{k}^{3} \\sigma'''(w_{k}\\,x + b_{k})\n    $$\n    这要求 $\\sigma$ 属于 $C^3(\\mathbb{R})$ 类。遵循与 $|u_{\\theta}''(x)|$ 的界类似的论证，我们有：\n    $$\n    |u_{\\theta}'''(x)| \\le \\sum_{k=1}^{m} |a_{k}| |w_{k}|^{3} |\\sigma'''(w_{k}\\,x + b_{k})|\n    $$\n    如果激活函数的三阶导数也是有界的，即 $M_3 = \\sup_{z \\in \\mathbb{R}} |\\sigma'''(z)|  \\infty$，那么 $u_{\\theta}''(x)$ 在 $[\\alpha, \\beta]$ 上的利普希茨常数是有界的：\n    $$\n    L = \\sup_{x \\in [\\alpha, \\beta]} |u_{\\theta}'''(x)| \\le \\left( \\sum_{k=1}^{m} |a_{k}| |w_{k}|^{3} \\right) M_3\n    $$\n    这确保了二阶导数不会振荡得太快，从而有助于为 PINN 优化提供一个更平滑、更稳定的损失景观。\n\n4.  **对常见激活函数的影响**：\n    - **ReLU ($\\sigma(z) = \\max(0, z)$)**：其一阶导数是不连续的亥维赛函数，二阶导数是狄拉克 δ 分布，这在经典意义上不是一个函数。因此，$u_{\\theta}(x)$ 不是 $C^2$ 的，并且在点 $w_k x + b_k = 0$ 处，$u_{\\theta}''(x)$ 的表达式是无定义的。这使得 ReLU 根本不适合需要强形式强制二阶偏微分方程残差的 PINN。\n    - **双曲正切 ($\\sigma(z) = \\tanh(z)$)**：该函数是无限次可微的 ($C^\\infty$)。它的所有导数在 $\\mathbb{R}$ 上都是有界的。例如，$\\sigma''(z) = -2\\tanh(z)\\text{sech}^2(z)$ 是有界的，其 $\\sup_z |\\sigma''(z)| \\approx 0.77$。这满足了正则性和有界性的所有条件，使其成为此项任务的高度适用的激活函数。\n    - **Softplus ($\\sigma(z) = \\ln(1 + \\exp(z))$)**：该函数也是 $C^\\infty$ 的。其二阶导数为 $\\sigma''(z) = \\exp(z) / (1 + \\exp(z))^2$，该导数是有界的，其 $\\sup_z |\\sigma''(z)| = 1/4$。它的所有高阶导数也都是有界的。因此，与 $\\tanh$ 一样，softplus 提供了对二阶残差进行良定且稳定评估所需的平滑度和导数界。\n\n总之，为了使 AD 能够以良定且稳定的方式为二阶 PINN 计算 $u_{\\theta}''(x)$，激活函数 $\\sigma$ 必须至少是 $C^2$ 的。为了增强数值稳定性和更好的优化行为，非常理想的是 $\\sigma$ 至少是 $C^3$ 的，并且其二阶和三阶导数是全局有界的。",
            "answer": "$$\n\\boxed{\\sum_{k=1}^{m} a_{k} w_{k}^{2} \\sigma''(w_{k}\\,x + b_{k})}\n$$"
        },
        {
            "introduction": "一个完整的物理问题不仅由控制方程定义，还必须包含相应的边界条件。在定义了域内的PDE残差后，下一步的关键是在模型的损失函数中恰当地施加这些边界条件。本练习将指导您如何将一个常见的诺伊曼（Neumann）边界条件（即通量边界条件）转化为一个可计算的损失项，这对于将PINNs应用于实际工程和科学问题是一项核心技能。",
            "id": "3431045",
            "problem": "考虑一个由物理信息神经网络 (PINN) 表示的标量场 $u_{\\theta}:\\mathbb{R}^{d}\\to\\mathbb{R}$，该网络经过训练，用于在带有边界 $\\partial\\Omega$ 的有界域 $\\Omega\\subset\\mathbb{R}^{d}$ 内逼近一个偏微分方程 (PDE) 的解。在边界上，模型必须满足诺伊曼边界条件 $\\partial_{n}u=h$ on $\\partial\\Omega$，其中 $h:\\partial\\Omega\\to\\mathbb{R}$ 是一个给定的通量，$\\partial_{n}u$ 表示外法向导数。边界 $\\partial\\Omega$ 由一个光滑函数 $\\phi:\\mathbb{R}^{d}\\to\\mathbb{R}$ 的零水平集隐式给出，其中对于所有 $x\\in\\partial\\Omega$ 都有 $\\nabla\\phi(x)\\neq 0$，单位外法向量为 $n(x)=\\nabla\\phi(x)/\\|\\nabla\\phi(x)\\|$。您获得了一个边界求积法则 $\\{(x_{i}^{b},w_{i})\\}_{i=1}^{N_{b}}$，其中 $x_{i}^{b}\\in\\partial\\Omega$ 且权重 $w_{i}>0$，用于近似 $\\partial\\Omega$ 上的曲面积分。PINN 通过最小化一个损失泛函进行训练，该泛函包含一个边界惩罚项，此惩罚项在平方 $L^{2}$ 意义下于 $\\partial\\Omega$ 上强制执行诺伊曼条件，并由给定的求积法则进行近似。可以使用自动微分 (AD) 来计算 $u_{\\theta}$ 关于其输入的空间导数，并且 $\\phi$ 的实现方式使其梯度也可以被计算。\n\n从法向导数 $\\partial_{n}u(x)=n(x)^{\\top}\\nabla_{x}u(x)$ 的定义和 $L^{2}$ 边界失配出发，推导出一个闭式解析表达式，用于表示边界损失 $L_{N}(\\theta)$，该损失惩罚了 $\\partial_{n}u_{\\theta}$ 与 $h$ 在 $\\partial\\Omega$ 上的差异，并使用所提供的求积法则。将法向导数完全用可通过自动微分 (AD) 计算的量和水平集几何来表示，并将损失写为边界样本上的归一化加权和。\n\n您的最终答案必须是关于 $\\{x_{i}^{b},w_{i}\\}_{i=1}^{N_{b}}$、$u_{\\theta}$、$\\phi$ 和 $h$ 的单个显式解析表达式 $L_{N}(\\theta)$。最终答案中不要包含任何解释性文本。",
            "solution": "该问题要求推导边界损失 $L_{N}(\\theta)$ 的闭式解析表达式，该损失为一个物理信息神经网络 (PINN) 强制执行诺伊曼边界条件。推导过程从边界条件失配在适当函数空间中的基本定义开始，然后对其进行离散化。\n\n首先，我们定义诺伊曼边界条件的残差。该条件由 $\\partial_{n}u = h$ 在边界 $\\partial\\Omega$ 上给出。对于神经网络近似 $u_{\\theta}$，在点 $x \\in \\partial\\Omega$ 处的边界残差由下式给出：\n$$\nR_{N}(x; \\theta) = \\partial_{n}u_{\\theta}(x) - h(x)\n$$\n问题指明边界惩罚是在平方 $L^{2}$ 意义下强制执行的。残差在边界 $\\partial\\Omega$ 上的平方 $L^{2}$ 范数由曲面积分定义：\n$$\n\\|R_{N}(\\cdot; \\theta)\\|_{L^{2}(\\partial\\Omega)}^{2} = \\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x)\n$$\n其中 $dS(x)$ 是微分曲面元。在机器学习和数值优化的背景下，通常使用平均误差，这使得损失项与域的大小无关。这可以通过除以边界的总表面积 $|\\partial\\Omega| = \\int_{\\partial\\Omega} 1 \\, dS(x)$ 来实现。因此，均方边界误差为：\n$$\n\\mathcal{E}_{N}(\\theta) = \\frac{1}{|\\partial\\Omega|} \\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x)\n$$\n下一步是用可通过自动微分 (AD) 计算的量来表示法向导数 $\\partial_{n}u_{\\theta}(x)$。问题提供了法向导数的定义，即 $u_{\\theta}$ 的梯度在单位外法向量 $n(x)$ 上的投影：\n$$\n\\partial_{n}u_{\\theta}(x) = n(x)^{\\top}\\nabla_{x}u_{\\theta}(x)\n$$\n单位外法向量 $n(x)$ 使用水平集函数 $\\phi(x)$ 定义，其中 $\\partial\\Omega = \\{x \\in \\mathbb{R}^{d} | \\phi(x)=0\\}$。公式为：\n$$\nn(x) = \\frac{\\nabla\\phi(x)}{\\|\\nabla\\phi(x)\\|}\n$$\n梯度的欧几里得范数是 $\\|\\nabla\\phi(x)\\| = \\sqrt{\\nabla\\phi(x)^{\\top}\\nabla\\phi(x)}$。将 $n(x)$ 的表达式代入法向导数的定义中，得到：\n$$\n\\partial_{n}u_{\\theta}(x) = \\left( \\frac{\\nabla\\phi(x)}{\\|\\nabla\\phi(x)\\|} \\right)^{\\top} \\nabla_{x}u_{\\theta}(x) = \\frac{\\nabla\\phi(x)^{\\top} \\nabla_{x}u_{\\theta}(x)}{\\sqrt{\\nabla\\phi(x)^{\\top}\\nabla\\phi(x)}}\n$$\n根据问题陈述，梯度 $\\nabla_{x}u_{\\theta}(x)$ 和 $\\nabla\\phi(x)$ 均可通过 AD 计算。\n\n最后一步是使用所提供的边界求积法则 $\\{(x_{i}^{b}, w_{i})\\}_{i=1}^{N_{b}}$ 来近似连续的均方误差 $\\mathcal{E}_{N}(\\theta)$。求积法则将曲面 $\\mathcal{S}$ 上的积分近似为 $\\int_{\\mathcal{S}} f(x) dS(x) \\approx \\sum_{i} w_{i} f(x_{i})$。将此应用于我们的分子和分母：\n平方残差的积分近似为：\n$$\n\\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x) \\approx \\sum_{i=1}^{N_{b}} w_{i} \\left( \\partial_{n}u_{\\theta}(x_{i}^{b}) - h(x_{i}^{b}) \\right)^{2}\n$$\n总表面积近似为：\n$$\n|\\partial\\Omega| = \\int_{\\partial\\Omega} 1 \\, dS(x) \\approx \\sum_{j=1}^{N_{b}} w_{j}\n$$\n损失 $L_{N}(\\theta)$ 是均方误差 $\\mathcal{E}_{N}(\\theta)$ 的离散近似。结合这些近似，我们得到归一化加权和：\n$$\nL_{N}(\\theta) = \\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\partial_{n}u_{\\theta}(x_{i}^{b}) - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}\n$$\n将每个边界点 $x_{i}^{b}$ 处法向导数的可由 AD 计算的表达式代入此公式，即可得到边界损失的最终闭式表达式：\n$$\nL_{N}(\\theta) = \\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\frac{\\nabla\\phi(x_{i}^{b})^{\\top} \\nabla_{x}u_{\\theta}(x_{i}^{b})}{\\sqrt{\\nabla\\phi(x_{i}^{b})^{\\top}\\nabla\\phi(x_{i}^{b})}} - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}\n$$\n此表达式仅依赖于网络输出 $u_{\\theta}$ 和水平集函数 $\\phi$（及其梯度）、边界点 $x_{i}^{b}$ 和权重 $w_{i}$，以及给定的通量函数 $h$，所有这些都是已知或可计算的。",
            "answer": "$$\n\\boxed{\\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\frac{\\nabla\\phi(x_{i}^{b})^{\\top} \\nabla_{x}u_{\\theta}(x_{i}^{b})}{\\sqrt{\\nabla\\phi(x_{i}^{b})^{\\top}\\nabla\\phi(x_{i}^{b})}} - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}}\n$$"
        },
        {
            "introduction": "在处理具有强各向异性物理属性的问题时，标准的PINN训练过程可能会因梯度病态（ill-conditioning）而变得缓慢甚至失败。本练习介绍了一种强大的高级技巧——坐标变换，来应对这一挑战。通过对坐标系进行适当的缩放，我们可以有效地对问题进行预处理，使得各向异性的物理在变换后的坐标系中显得“各向同性”，从而稳定优化过程并加速模型收敛。",
            "id": "3430989",
            "problem": "考虑在矩形域 $\\Omega \\subset \\mathbb{R}^{2}$ 上的稳态各向异性扩散方程，\n$$\n\\nabla \\cdot \\big(A \\nabla u(x,y)\\big) = f(x,y),\n$$\n其中 $A = \\operatorname{diag}(\\alpha,1)$，$\\alpha  0$ 是一个满足 $\\alpha \\gg 1$ 的常数。假设 $u_{\\theta}(x,y)$ 是一个参数化的神经网络，在物理信息神经网络（PINN）框架（Physics-Informed Neural Network (PINN)）中用于近似 $u(x,y)$，其方法是通过最小化一个基于 $u_{\\theta}(x,y)$ 自动微分的无数据残差损失。巨大的各向异性因子 $\\alpha$ 在优化过程中会导致严重的梯度病态问题。\n\n假设一个形式为 $x = s \\,\\xi$ 和 $y = \\eta$ 的坐标变换，其中 $s  0$ 是一个待选的标量，并且网络被重新参数化为 $u_{\\theta}(\\xi,\\eta)$ 与物理坐标 $(x,y) = (s \\,\\xi,\\eta)$ 的复合。从控制方程和标准多元微积分出发，确定 $s$ 的值，使得变换后的微分算子的主部在 $(\\xi,\\eta)$ 中是各向同性的，从而缓解 PINN 训练中的梯度病态问题。以 $\\alpha$ 的显式表达式报告您的最终答案。无需四舍五入，不涉及单位。最终答案必须是单一的闭式解析表达式。",
            "solution": "所述问题具有科学依据、是适定的、客观的，并包含了进行严格数学推导所需的所有信息。因此，该问题被认定为有效。\n\n控制方程是二维 $(x,y)$ 稳态各向异性扩散方程：\n$$\n\\nabla \\cdot \\big(A \\nabla u(x,y)\\big) = f(x,y)\n$$\n扩散张量 $A$ 由 $A = \\operatorname{diag}(\\alpha, 1)$ 给出，可以写成矩阵形式：\n$$\nA = \\begin{pmatrix} \\alpha  0 \\\\ 0  1 \\end{pmatrix}\n$$\n其中 $\\alpha  0$ 是一个常数。标量场 $u(x,y)$ 的梯度是 $\\nabla u = \\left( \\frac{\\partial u}{\\partial x}, \\frac{\\partial u}{\\partial y} \\right)^T$。因此，$A \\nabla u$ 项为：\n$$\nA \\nabla u = \\begin{pmatrix} \\alpha  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial u}{\\partial x} \\\\ \\frac{\\partial u}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} \\alpha \\frac{\\partial u}{\\partial x} \\\\ \\frac{\\partial u}{\\partial y} \\end{pmatrix}\n$$\n对该向量场取散度 $\\nabla \\cdot (\\cdot)$，得到偏微分方程（PDE）的展开形式：\n$$\n\\frac{\\partial}{\\partial x} \\left(\\alpha \\frac{\\partial u}{\\partial x}\\right) + \\frac{\\partial}{\\partial y} \\left(\\frac{\\partial u}{\\partial y}\\right) = \\alpha \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = f(x,y)\n$$\n问题提出了一个到新坐标系 $(\\xi, \\eta)$ 的坐标变换，定义为：\n$$\nx = s \\xi, \\quad y = \\eta\n$$\n其中 $s  0$ 是一个待确定的缩放因子。我们必须将该偏微分方程变换到这些新坐标下。为此，我们使用多元链式法则，将关于 $x$ 和 $y$ 的偏导数用关于 $\\xi$ 和 $\\eta$ 的偏导数来表示。最直接的方法是从表达关于新坐标的导数开始：\n$$\n\\frac{\\partial u}{\\partial \\xi} = \\frac{\\partial u}{\\partial x} \\frac{\\partial x}{\\partial \\xi} + \\frac{\\partial u}{\\partial y} \\frac{\\partial y}{\\partial \\xi}\n$$\n$$\n\\frac{\\partial u}{\\partial \\eta} = \\frac{\\partial u}{\\partial x} \\frac{\\partial x}{\\partial \\eta} + \\frac{\\partial u}{\\partial y} \\frac{\\partial y}{\\partial \\eta}\n$$\n从变换方程中，我们计算出所需的偏导数：\n$$\n\\frac{\\partial x}{\\partial \\xi} = s, \\quad \\frac{\\partial y}{\\partial \\xi} = 0\n$$\n$$\n\\frac{\\partial x}{\\partial \\eta} = 0, \\quad \\frac{\\partial y}{\\partial \\eta} = 1\n$$\n将这些代入链式法则表达式中，得到：\n$$\n\\frac{\\partial u}{\\partial \\xi} = s \\frac{\\partial u}{\\partial x} + 0 \\implies \\frac{\\partial u}{\\partial x} = \\frac{1}{s} \\frac{\\partial u}{\\partial \\xi}\n$$\n$$\n\\frac{\\partial u}{\\partial \\eta} = 0 + 1 \\frac{\\partial u}{\\partial y} \\implies \\frac{\\partial u}{\\partial y} = \\frac{\\partial u}{\\partial \\eta}\n$$\n这些关系给出了新坐标系下的一阶微分算子：$\\frac{\\partial}{\\partial x} = \\frac{1}{s}\\frac{\\partial}{\\partial \\xi}$ 和 $\\frac{\\partial}{\\partial y} = \\frac{\\partial}{\\partial \\eta}$。现在我们计算 PDE 所需的二阶偏导数：\n$$\n\\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial}{\\partial x}\\left(\\frac{\\partial u}{\\partial x}\\right) = \\left(\\frac{1}{s}\\frac{\\partial}{\\partial \\xi}\\right)\\left(\\frac{1}{s} \\frac{\\partial u}{\\partial \\xi}\\right) = \\frac{1}{s^2} \\frac{\\partial^2 u}{\\partial \\xi^2}\n$$\n$$\n\\frac{\\partial^2 u}{\\partial y^2} = \\frac{\\partial}{\\partial y}\\left(\\frac{\\partial u}{\\partial y}\\right) = \\left(\\frac{\\partial}{\\partial \\eta}\\right)\\left(\\frac{\\partial u}{\\partial \\eta}\\right) = \\frac{\\partial^2 u}{\\partial \\eta^2}\n$$\n我们将这些二阶导数的表达式代入展开的 PDE 中：\n$$\n\\alpha \\left(\\frac{1}{s^2} \\frac{\\partial^2 u}{\\partial \\xi^2}\\right) + \\frac{\\partial^2 u}{\\partial \\eta^2} = f(s\\xi, \\eta)\n$$\n这可以写成：\n$$\n\\frac{\\alpha}{s^2} \\frac{\\partial^2 u}{\\partial \\xi^2} + \\frac{\\partial^2 u}{\\partial \\eta^2} = \\tilde{f}(\\xi, \\eta)\n$$\n其中 $\\tilde{f}(\\xi, \\eta) = f(s\\xi, \\eta)$。这个变换后的微分算子的主部由最高阶导数项给出：$\\frac{\\alpha}{s^2} \\frac{\\partial^2}{\\partial \\xi^2} + \\frac{\\partial^2}{\\partial \\eta^2}$。为了使该算子在 $(\\xi, \\eta)$ 坐标中是各向同性的，二阶偏导数的系数必须相等。这确保了变换后的系统在新坐标空间中模拟了所有方向上均匀的扩散。$\\frac{\\partial^2 u}{\\partial \\xi^2}$ 的系数是 $\\frac{\\alpha}{s^2}$，而 $\\frac{\\partial^2 u}{\\partial \\eta^2}$ 的系数是 $1$。令它们相等，得到各向同性的条件：\n$$\n\\frac{\\alpha}{s^2} = 1\n$$\n解这个关于 $s$ 的方程，得到：\n$$\ns^2 = \\alpha\n$$\n考虑到约束条件 $s  0$，我们取正平方根：\n$$\ns = \\sqrt{\\alpha}\n$$\n这种对 $s$ 的选择缩放了 $x$ 坐标，以抵消该方向上的高扩散率 $\\alpha$，从而在变换后的坐标中得到一个各向同性算子，具体来说是拉普拉斯算子 $\\nabla^2_{\\xi, \\eta} = \\frac{\\partial^2}{\\partial \\xi^2} + \\frac{\\partial^2}{\\partial \\eta^2}$。这正是在 PINN 框架中改善优化问题条件数所期望的结果。",
            "answer": "$$\\boxed{\\sqrt{\\alpha}}$$"
        }
    ]
}