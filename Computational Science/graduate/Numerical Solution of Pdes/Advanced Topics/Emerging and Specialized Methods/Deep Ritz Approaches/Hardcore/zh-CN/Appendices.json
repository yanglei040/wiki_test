{
    "hands_on_practices": [
        {
            "introduction": "为了将 Deep Ritz 方法的抽象变分原理具体化，我们从一个简单的一维泊松方程开始。这个练习将引导你手动推导能量泛函，并为一个简单的线性试探函数找到其精确的最小值 。通过这个可解析的例子，你将对能量最小化如何产生偏微分方程的解建立起直观的认识。",
            "id": "3376700",
            "problem": "考虑单位区间上的带齐次狄利克雷边界条件的一维泊松偏微分方程 (PDE)：求解 $u:(0,1)\\to\\mathbb{R}$ 使得\n$$\n- u''(x) = 1 \\quad \\text{for } x\\in (0,1), \\qquad u(0)=0, \\quad u(1)=0.\n$$\n从通过分部积分和最小势能原理推导出的弱形式出发，构建能量泛函，使其在索伯列夫空间 $H_0^1(0,1)$ 中的最小化子与该偏微分方程的弱解一致。然后，通过一个乘法拟设来强制施加边界条件，实例化深度里兹方法\n$$\nu_{\\theta}(x) = \\phi(x)\\,N_{\\theta}(x),\n$$\n其中 $\\phi(x)=x(1-x)$ 且 $N_{\\theta}(x)=a x + b$，带有可训练参数 $\\theta=(a,b)\\in\\mathbb{R}^2$。使用在 $(0,1)$ 上的精确（而非采样）积分，写出由此变分形式所隐含的关于 $(a,b)$ 的能量函数，并计算其在所有 $(a,b)\\in\\mathbb{R}^2$ 上的最小值。将最终的最小能量表示为一个无舍入的精确值。",
            "solution": "泊松方程由 $-u''(x) = f(x)$ 给出，其中 $f(x)=1$。最小势能原理指出，此偏微分方程的解是在适当空间（本例中为 $H_0^1(0,1)$）中使能量泛函 $J(u)$ 最小化的函数 $u$。该泛函由下式给出\n$$\nJ(u) = \\frac{1}{2} \\int_{0}^{1} (u'(x))^2 \\, dx - \\int_{0}^{1} f(x)u(x) \\, dx\n$$\n对于 $f(x)=1$，这变成\n$$\nJ(u) = \\frac{1}{2} \\int_{0}^{1} (u'(x))^2 \\, dx - \\int_{0}^{1} u(x) \\, dx\n$$\n深度里兹方法试图在一族由 $\\theta$ 参数化的试探函数上最小化此泛函。给定的试探函数是 $u_{\\theta}(x)$，参数为 $\\theta=(a,b)$：\n$$\nu_{a,b}(x) = \\phi(x) N_{\\theta}(x) = (x(1-x))(ax+b)\n$$\n对于任意选择的 $a$ 和 $b$，此形式强制施加了边界条件 $u_{a,b}(0)=0$ 和 $u_{a,b}(1)=0$。为便于求导和积分，我们展开 $u_{a,b}(x)$：\n$$\nu_{a,b}(x) = (x-x^2)(ax+b) = ax^2 + bx - ax^3 - bx^2 = -ax^3 + (a-b)x^2 + bx\n$$\n接下来，我们计算导数 $u'_{a,b}(x)$：\n$$\nu'_{a,b}(x) = \\frac{d}{dx} \\left( -ax^3 + (a-b)x^2 + bx \\right) = -3ax^2 + 2(a-b)x + b\n$$\n现在我们必须计算能量泛函 $E(a,b) = J(u_{a,b})$ 中的两个积分。\n\n第一个积分（线性项）：\n$$\n\\int_{0}^{1} u_{a,b}(x) \\, dx = \\int_{0}^{1} (-ax^3 + (a-b)x^2 + bx) \\, dx\n$$\n$$\n= \\left[ -a\\frac{x^4}{4} + (a-b)\\frac{x^3}{3} + b\\frac{x^2}{2} \\right]_{0}^{1} = -\\frac{a}{4} + \\frac{a-b}{3} + \\frac{b}{2}\n$$\n$$\n= \\frac{-3a + 4(a-b) + 6b}{12} = \\frac{-3a + 4a - 4b + 6b}{12} = \\frac{a+2b}{12}\n$$\n\n第二个积分（二次项）：\n$$\n\\int_{0}^{1} (u'_{a,b}(x))^2 \\, dx = \\int_{0}^{1} (-3ax^2 + 2(a-b)x + b)^2 \\, dx\n$$\n我们展开被积函数：\n$$\n(-3ax^2 + 2(a-b)x + b)^2 = 9a^2x^4 + 4(a-b)^2x^2 + b^2 - 12a(a-b)x^3 - 6abx^2 + 4b(a-b)x\n$$\n$$\n= 9a^2x^4 - 12a(a-b)x^3 + [4(a-b)^2 - 6ab]x^2 + 4b(a-b)x + b^2\n$$\n从 $x=0$ 到 $x=1$ 逐项积分：\n$$\n\\int_{0}^{1} (u'_{a,b}(x))^2 \\, dx = \\left[ \\frac{9a^2x^5}{5} - \\frac{12a(a-b)x^4}{4} + \\frac{4(a-b)^2 - 6ab}{3}x^3 + \\frac{4b(a-b)x^2}{2} + b^2x \\right]_{0}^{1}\n$$\n$$\n= \\frac{9a^2}{5} - 3a(a-b) + \\frac{4(a^2-2ab+b^2) - 6ab}{3} + 2b(a-b) + b^2\n$$\n$$\n= \\frac{9a^2}{5} - 3a^2 + 3ab + \\frac{4a^2-8ab-6ab+4b^2}{3} + 2ab - 2b^2 + b^2\n$$\n$$\n= \\frac{9a^2}{5} - 3a^2 + 5ab - b^2 + \\frac{4a^2 - 14ab + 4b^2}{3}\n$$\n将所有项通分，公分母为15：\n$$\n= \\frac{3(9a^2) - 15(3a^2) + 15(5ab) - 15b^2 + 5(4a^2 - 14ab + 4b^2)}{15}\n$$\n$$\n= \\frac{27a^2 - 45a^2 + 75ab - 15b^2 + 20a^2 - 70ab + 20b^2}{15}\n$$\n$$\n= \\frac{(27-45+20)a^2 + (75-70)ab + (-15+20)b^2}{15} = \\frac{2a^2 + 5ab + 5b^2}{15}\n$$\n现在，我们组装能量泛函 $E(a,b)$：\n$$\nE(a,b) = \\frac{1}{2} \\left( \\frac{2a^2 + 5ab + 5b^2}{15} \\right) - \\left( \\frac{a+2b}{12} \\right) = \\frac{2a^2 + 5ab + 5b^2}{30} - \\frac{a+2b}{12}\n$$\n为了找到最小值，我们计算关于 $a$ 和 $b$ 的偏导数并令其为零。\n$$\n\\frac{\\partial E}{\\partial a} = \\frac{4a+5b}{30} - \\frac{1}{12} = 0\n$$\n$$\n\\frac{\\partial E}{\\partial b} = \\frac{5a+10b}{30} - \\frac{2}{12} = 0\n$$\n这给出了一个线性方程组：\n1. $12(4a+5b) = 30 \\implies 48a + 60b = 30 \\implies 8a + 10b = 5$\n2. $\\frac{5a+10b}{30} = \\frac{1}{6} \\implies 6(5a+10b) = 30 \\implies 30a + 60b = 30 \\implies a + 2b = 1$\n\n从第二个方程，我们有 $a=1-2b$。将此代入第一个方程：\n$$\n8(1-2b) + 10b = 5\n$$\n$$\n8 - 16b + 10b = 5 \\implies 8 - 6b = 5 \\implies 3 = 6b \\implies b = \\frac{1}{2}\n$$\n将 $b=\\frac{1}{2}$ 代回 $a=1-2b$：\n$$\na = 1 - 2\\left(\\frac{1}{2}\\right) = 1-1=0\n$$\n能量在 $(a,b) = (0, \\frac{1}{2})$ 处最小化。最后，我们将这些参数代入 $E(a,b)$ 来计算最小能量值：\n$$\nE\\left(0, \\frac{1}{2}\\right) = \\frac{2(0)^2 + 5(0)(\\frac{1}{2}) + 5(\\frac{1}{2})^2}{30} - \\frac{0+2(\\frac{1}{2})}{12}\n$$\n$$\n= \\frac{5(\\frac{1}{4})}{30} - \\frac{1}{12} = \\frac{\\frac{5}{4}}{30} - \\frac{1}{12} = \\frac{5}{120} - \\frac{1}{12}\n$$\n$$\n= \\frac{1}{24} - \\frac{2}{24} = -\\frac{1}{24}\n$$\n最小能量为 $-\\frac{1}{24}$。",
            "answer": "$$ \\boxed{-\\frac{1}{24}} $$"
        },
        {
            "introduction": "在实践中，通过构造满足边界条件的函数（硬约束）可能很困难，一种更灵活的替代方法是罚函数法（软约束）。本练习探讨了这种方法，要求你从变分原理出发，推导出一个带边界罚项的能量泛函所对应的欧拉-拉格朗日方程和自然边界条件 。这将帮助你理解罚参数 $\\alpha$ 如何在控制边界误差与最小化内部能量之间取得平衡。",
            "id": "3376716",
            "problem": "考虑使用深度里茨方法对一维域 $\\Omega=(0,1)$ 上具有齐次狄利克雷边界条件的泊松偏微分方程（PDE）进行数值求解，其中边界条件通过边界惩罚项进行软施加。深度里茨方法通过人工神经网络（ANN）对试探函数 $u$ 进行参数化，并在容许空间上最小化一个里茨泛函。在模型容量无限的连续极限下，可以通过变分法直接研究惩罚能量的精确最小化子 $u_{\\alpha}$。\n\n设惩罚里茨泛函为\n$$\n\\mathcal{J}_{\\alpha}(u) \\;=\\; \\int_{0}^{1} \\left( \\frac{1}{2}\\,|u'(x)|^{2} \\;-\\; f(x)\\,u(x) \\right)\\,\\mathrm{d}x \\;+\\; \\alpha\\left( |u(0)|^{2} \\,+\\, |u(1)|^{2} \\right),\n$$\n其中 $f(x)=1$，惩罚参数 $\\alpha0$。无约束最小化在 $H^{1}(0,1)$ 上进行，不将 $u(0)=0$ 或 $u(1)=0$ 作为硬约束施加。\n\n从 $\\mathcal{J}_{\\alpha}$ 的驻点满足带有惩罚项所导出的相应边界项的欧拉-拉格朗日条件这一基本原理出发，利用变分法的基本原理推导 $u_{\\alpha}$ 的控制常微分方程和边界条件。然后显式求解所得的边值问题，并确定 $u_{\\alpha}(0)$ 的精确值，表示为关于 $\\alpha$ 的闭式表达式。\n\n请以单一的闭式解析表达式给出你的最终答案。无需四舍五入，也不涉及单位。",
            "solution": "任务是找到惩罚里茨泛函\n$$\n\\mathcal{J}_{\\alpha}(u) = \\int_{0}^{1} \\left( \\frac{1}{2}\\,|u'(x)|^{2} - f(x)\\,u(x) \\right)\\,\\mathrm{d}x + \\alpha\\left( |u(0)|^{2} + |u(1)|^{2} \\right)\n$$\n的最小化子 $u_{\\alpha}(x)$，其中 $u$ 的函数空间为 $H^{1}(0,1)$，$f(x)=1$，$\\alpha0$ 是一个惩罚参数。我们必须首先推导出控制微分方程和边界条件，然后求解 $u_{\\alpha}(x)$ 以求得 $u_{\\alpha}(0)$ 的值。\n\n泛函 $\\mathcal{J}_{\\alpha}$ 的最小化子 $u_{\\alpha}$ 是一个驻点。为了找到这个驻点，我们使用变分法。我们考虑 $u_{\\alpha}$ 的一个形式为 $u(x) = u_{\\alpha}(x) + \\epsilon \\eta(x)$ 的变分，其中 $\\eta(x) \\in H^{1}(0,1)$ 是一个任意检验函数，$\\epsilon$ 是一个小的实数参数。$u_{\\alpha}$ 是驻点的条件是泛函的一阶变分为零，即\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\mathcal{J}_{\\alpha}(u_{\\alpha} + \\epsilon \\eta) \\bigg|_{\\epsilon=0} = 0\n$$\n对于所有容许的 $\\eta$ 都成立。\n\n我们来计算导数。由于函数 $u$ 是实值的，我们可以写成 $|u|^{2} = u^{2}$。泛函变为：\n$$\n\\mathcal{J}_{\\alpha}(u_{\\alpha} + \\epsilon \\eta) = \\int_{0}^{1} \\left( \\frac{1}{2}\\,(u'_{\\alpha} + \\epsilon \\eta')^{2} - f(x)\\,(u_{\\alpha} + \\epsilon \\eta) \\right)\\,\\mathrm{d}x + \\alpha\\left( (u_{\\alpha}(0) + \\epsilon \\eta(0))^{2} + (u_{\\alpha}(1) + \\epsilon \\eta(1))^{2} \\right)\n$$\n对 $\\epsilon$ 求导：\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\mathcal{J}_{\\alpha}(u_{\\alpha} + \\epsilon \\eta) = \\int_{0}^{1} \\left( (u'_{\\alpha} + \\epsilon \\eta') \\eta' - f(x)\\eta \\right)\\,\\mathrm{d}x + \\alpha\\left( 2(u_{\\alpha}(0) + \\epsilon \\eta(0))\\eta(0) + 2(u_{\\alpha}(1) + \\epsilon \\eta(1))\\eta(1) \\right)\n$$\n令 $\\epsilon=0$，我们得到一阶变分条件：\n$$\n\\int_{0}^{1} \\left( u'_{\\alpha}(x) \\eta'(x) - f(x) \\eta(x) \\right)\\,\\mathrm{d}x + 2\\alpha u_{\\alpha}(0) \\eta(0) + 2\\alpha u_{\\alpha}(1) \\eta(1) = 0\n$$\n为了找到微分方程的强形式，我们对积分中的第一项应用分部积分法：\n$$\n\\int_{0}^{1} u'_{\\alpha}(x) \\eta'(x) \\,\\mathrm{d}x = \\left[ u'_{\\alpha}(x) \\eta(x) \\right]_{0}^{1} - \\int_{0}^{1} u''_{\\alpha}(x) \\eta(x) \\,\\mathrm{d}x = u'_{\\alpha}(1)\\eta(1) - u'_{\\alpha}(0)\\eta(0) - \\int_{0}^{1} u''_{\\alpha}(x)\\eta(x) \\,\\mathrm{d}x\n$$\n将此代回变分方程，得到：\n$$\nu'_{\\alpha}(1)\\eta(1) - u'_{\\alpha}(0)\\eta(0) - \\int_{0}^{1} u''_{\\alpha}(x)\\eta(x) \\,\\mathrm{d}x - \\int_{0}^{1} f(x)\\eta(x) \\,\\mathrm{d}x + 2\\alpha u_{\\alpha}(0) \\eta(0) + 2\\alpha u_{\\alpha}(1) \\eta(1) = 0\n$$\n我们可以通过对积分项和边界项进行分组来重排各项：\n$$\n\\int_{0}^{1} \\left( -u''_{\\alpha}(x) - f(x) \\right) \\eta(x) \\,\\mathrm{d}x + \\left( u'_{\\alpha}(1) + 2\\alpha u_{\\alpha}(1) \\right) \\eta(1) + \\left( -u'_{\\alpha}(0) + 2\\alpha u_{\\alpha}(0) \\right) \\eta(0) = 0\n$$\n这个方程必须对检验函数 $\\eta(x) \\in H^{1}(0,1)$ 的任何选择都成立。根据变分法基本引理，如果我们首先考虑在边界处为零的检验函数（$\\eta(0)=0$ 和 $\\eta(1)=0$），则积分项必须为零。这给出了域内部的欧拉-拉格朗日方程：\n$$\n-u''_{\\alpha}(x) - f(x) = 0 \\quad \\text{for } x \\in (0,1)\n$$\n在满足此常微分方程的情况下，对所有 $\\eta$，积分项都为零，变分方程简化为边界项：\n$$\n\\left( u'_{\\alpha}(1) + 2\\alpha u_{\\alpha}(1) \\right) \\eta(1) + \\left( -u'_{\\alpha}(0) + 2\\alpha u_{\\alpha}(0) \\right) \\eta(0) = 0\n$$\n由于此式必须对 $\\eta(0)$ 和 $\\eta(1)$ 的任意值都成立，所以 $\\eta(0)$ 和 $\\eta(1)$ 的系数必须独立为零。这导出了自然边界条件：\n$$\nu'_{\\alpha}(0) = 2\\alpha u_{\\alpha}(0)\n$$\n$$\nu'_{\\alpha}(1) = -2\\alpha u_{\\alpha}(1)\n$$\n现在我们得到了一个关于 $u_{\\alpha}(x)$ 的完整边值问题（BVP）。给定 $f(x)=1$，该边值问题为：\n$$\n\\begin{cases}\n-u''_{\\alpha}(x) = 1  \\text{for } x \\in (0,1) \\\\\nu'_{\\alpha}(0) = 2\\alpha u_{\\alpha}(0) \\\\\nu'_{\\alpha}(1) = -2\\alpha u_{\\alpha}(1)\n\\end{cases}\n$$\n我们着手求解这个边值问题。将常微分方程 $u''_{\\alpha}(x) = -1$ 对 $x$ 积分两次，得到通解：\n$$\nu'_{\\alpha}(x) = -x + C_1\n$$\n$$\nu_{\\alpha}(x) = -\\frac{1}{2}x^2 + C_1 x + C_2\n$$\n其中 $C_1$ 和 $C_2$ 是积分常数。我们利用边界条件来确定这些常数。\n根据通解，我们有：\n$$\nu_{\\alpha}(0) = C_2 \\quad \\text{and} \\quad u'_{\\alpha}(0) = C_1\n$$\n$$\nu_{\\alpha}(1) = -\\frac{1}{2} + C_1 + C_2 \\quad \\text{and} \\quad u'_{\\alpha}(1) = -1 + C_1\n$$\n应用第一个边界条件 $u'_{\\alpha}(0) = 2\\alpha u_{\\alpha}(0)$：\n$$\nC_1 = 2\\alpha C_2\n$$\n应用第二个边界条件 $u'_{\\alpha}(1) = -2\\alpha u_{\\alpha}(1)$：\n$$\n-1 + C_1 = -2\\alpha \\left(-\\frac{1}{2} + C_1 + C_2\\right)\n$$\n$$\n-1 + C_1 = \\alpha - 2\\alpha C_1 - 2\\alpha C_2\n$$\n$$\n(1 + 2\\alpha)C_1 + 2\\alpha C_2 = 1 + \\alpha\n$$\n现在我们得到了一个关于 $C_1$ 和 $C_2$ 的二元线性方程组：\n1. $C_1 - 2\\alpha C_2 = 0$\n2. $(1 + 2\\alpha)C_1 + 2\\alpha C_2 = 1 + \\alpha$\n\n将第一个方程中的 $C_1 = 2\\alpha C_2$ 代入第二个方程：\n$$\n(1 + 2\\alpha)(2\\alpha C_2) + 2\\alpha C_2 = 1 + \\alpha\n$$\n$$\n(2\\alpha + 4\\alpha^2) C_2 + 2\\alpha C_2 = 1 + \\alpha\n$$\n$$\n(4\\alpha + 4\\alpha^2) C_2 = 1 + \\alpha\n$$\n$$\n4\\alpha(1 + \\alpha) C_2 = 1 + \\alpha\n$$\n由于 $\\alpha  0$，我们知道 $1+\\alpha \\neq 0$，所以可以两边同除以它：\n$$\nC_2 = \\frac{1+\\alpha}{4\\alpha(1 + \\alpha)} = \\frac{1}{4\\alpha}\n$$\n问题要求 $u_{\\alpha}(0)$ 的值。从我们的通解中，我们知道 $u_{\\alpha}(0) = C_2$。\n因此，精确值为：\n$$\nu_{\\alpha}(0) = \\frac{1}{4\\alpha}\n$$",
            "answer": "$$\n\\boxed{\\frac{1}{4\\alpha}}\n$$"
        },
        {
            "introduction": "前面的练习侧重于理论分析，而本练习则将带你进入实际的计算实现。你将为一个参数化的偏微分方程族构建一个完整的 Deep Ritz 求解器，其中涉及使用蒙特卡洛方法近似积分，并应用元学习策略来寻找一个优化的初始参数 。这个综合性练习旨在将理论知识转化为实际的编程技能，让你体验如何将深度学习应用于解决复杂的科学计算问题。",
            "id": "3376733",
            "problem": "要求您实现一个用于求解一族参数化椭圆偏微分方程的 Deep Ritz 求解器，该求解器使用一个具有固定隐藏层（随机特征）和线性输出层的神经网络试验空间，并结合一个元学习过程来选择使平均能量最小化的初始化。具体来说，考虑在单位正方形域 $\\Omega = (0,1)^2$ 上具有齐次狄利克雷边界条件的一族问题：\n$$\n-\\nabla\\cdot\\big(a_{\\mu}(x,y)\\,\\nabla u_{\\mu}(x,y)\\big) = f_{\\mu}(x,y), \\quad (x,y)\\in\\Omega,\\quad u_{\\mu}|_{\\partial\\Omega}=0,\n$$\n其中参数 $\\mu$ 在区间 $[0,1]$ 的一个子集上取值。Deep Ritz 方法旨在在一类试验函数 $u$ 上最小化能量泛函\n$$\n\\mathcal{J}_{\\mu}(u) = \\int_{\\Omega} \\left( \\frac{1}{2}\\, a_{\\mu}(x,y)\\, \\lvert \\nabla u(x,y) \\rvert^2 \\;-\\; f_{\\mu}(x,y)\\,u(x,y) \\right)\\, \\mathrm{d}x\\,\\mathrm{d}y.\n$$\n使用一个通过边界因子精确施加狄利克雷边界条件的神经网络试验空间。令 $b(x,y) = x(1-x)\\,y(1-y)$，并使用一个带有双曲正切激活函数的单隐藏层来定义一个固定的（不可训练的）特征映射 $\\varphi:\\Omega\\to\\mathbb{R}^{D}$：\n$$\n\\varphi(x,y) = \\tanh\\big(W\\,[x,y]^{\\top} + c\\big),\n$$\n其中 $W\\in\\mathbb{R}^{D\\times 2}$ 和 $c\\in\\mathbb{R}^{D}$ 使用固定的随机种子从标准正态分布中抽取一次。可训练函数被限制在线性张成空间中\n$$\nu_{w}(x,y) \\;=\\; b(x,y)\\,\\varphi(x,y)^{\\top} w, \\quad w\\in \\mathbb{R}^{D}.\n$$\n对于每个参数 $\\mu$，里兹能量成为一个关于 $w$ 的严格凸二次函数，形式如下\n$$\n\\mathcal{J}_{\\mu}(w) \\;=\\; \\frac{1}{2}\\, w^{\\top} H_{\\mu}\\, w \\;-\\; h_{\\mu}^{\\top} w \\;+\\; C_{\\mu},\n$$\n其中 $H_{\\mu}\\in\\mathbb{R}^{D\\times D}$ 和 $h_{\\mu}\\in\\mathbb{R}^{D}$ 由依赖于 $a_{\\mu}$、$f_{\\mu}$、$b$ 和 $\\varphi$ 的内部积分给出。使用在 $\\Omega$ 中均匀分布的 $N$ 个独立同分布样本，通过蒙特卡洛积分来近似这些积分：\n- 对于每个样本 $(x_s,y_s)$，令 $J_s\\in\\mathbb{R}^{D\\times 2}$ 表示雅可比矩阵，其行向量为 $\\nabla\\!\\big(b\\,\\varphi_i\\big)(x_s,y_s)$，因此 $\\lvert \\nabla u_w \\rvert^2 = \\sum_{i,j} w_i w_j \\, J_s(i,:) \\cdot J_s(j,:) = w^{\\top} \\big(J_s J_s^{\\top}\\big) w$。\n- 于是\n$$\nH_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} a_{\\mu}(x_s,y_s)\\; J_s J_s^{\\top},\\quad\nh_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} f_{\\mu}(x_s,y_s)\\; b(x_s,y_s)\\,\\varphi(x_s,y_s).\n$$\n\n基准真相参数化。为了能够进行定量评估，从一个光滑的、可精确求解的配置来定义 $a_{\\mu}$ 和 $f_{\\mu}$。对于任何 $\\mu\\in[0,1]$，令\n$$\na_{\\mu}(x,y) \\;=\\; 1 \\;+\\; \\tfrac{1}{2}\\,\\mu\\, \\sin(2\\pi x)\\,\\sin(2\\pi y),\n$$\n并定义精确解\n$$\nu^{\\star}_{\\mu}(x,y) \\;=\\; \\sin(\\pi x)\\,\\sin(\\pi y) \\;+\\; \\mu\\, \\sin(2\\pi x)\\,\\sin(\\pi y).\n$$\n设置\n$$\nf_{\\mu}(x,y) \\;=\\; -\\nabla\\cdot\\big(a_{\\mu}(x,y)\\,\\nabla u^{\\star}_{\\mu}(x,y)\\big),\n$$\n这样 $u^{\\star}_{\\mu}$ 就精确地满足了具有齐次狄利克雷数据的偏微分方程。您必须使用乘法法则解析地计算 $f_{\\mu}$：\n$$\nf_{\\mu} \\;=\\; -\\big(\\nabla a_{\\mu}\\cdot \\nabla u^{\\star}_{\\mu} \\;+\\; a_{\\mu}\\,\\Delta u^{\\star}_{\\mu}\\big).\n$$\n\n元学习设置。考虑一个参数训练集 $\\mathcal{M}_{\\mathrm{train}} = \\{\\, 0.0,\\; 0.4,\\; 0.8 \\,\\}$ 并定义平均能量\n$$\n\\overline{\\mathcal{J}}(w) \\;=\\; \\frac{1}{\\lvert \\mathcal{M}_{\\mathrm{train}}\\rvert}\\sum_{\\mu\\in \\mathcal{M}_{\\mathrm{train}}} \\mathcal{J}_{\\mu}(w) \\;=\\; \\frac{1}{2}\\, w^{\\top} \\overline{H}\\, w \\;-\\; \\overline{h}^{\\top} w \\;+\\; \\overline{C},\n$$\n其中 $\\overline{H}$ 和 $\\overline{h}$ 是 $H_{\\mu}$ 和 $h_{\\mu}$ 在所有 $\\mu$ 上使用相同的内部样本计算得到的相应平均值，以减少方差。将元初始化定义为唯一的最小化子\n$$\nw_{\\mathrm{meta}} \\;=\\; \\arg\\min_{w\\in\\mathbb{R}^{D}} \\overline{\\mathcal{J}}(w).\n$$\n按任务适应（Per-task adaptation）从一个初始化 $w_0$ 开始，使用一个公共步长 $\\eta0$ 对 $\\mathcal{J}_{\\mu}(w)$ 进行梯度下降：\n$$\nw_{t+1} \\;=\\; w_t - \\eta\\, \\nabla \\mathcal{J}_{\\mu}(w_t) \\;=\\; w_t - \\eta\\,(H_{\\mu} w_t - h_{\\mu}),\n$$\n对于 $t=0,1,\\dots,T-1$。对于严格凸二次目标，经过 $T$ 步后的分摊误差（次优性差距）可以用 $I-\\eta H_{\\mu}$ 的谱半径表示一个最坏情况界。令 $\\rho_{\\mu}(\\eta) = \\max_{\\lambda\\in\\mathrm{spec}(H_{\\mu})} \\lvert 1 - \\eta \\lambda\\rvert$。那么对于任意 $w_0$，\n$$\n\\mathcal{J}_{\\mu}(w_T) - \\min_{w}\\mathcal{J}_{\\mu}(w) \\;\\le\\; \\rho_{\\mu}(\\eta)^{2T}\\, \\big(\\mathcal{J}_{\\mu}(w_0) - \\min_{w}\\mathcal{J}_{\\mu}(w)\\big).\n$$\n\n实现要求。\n- 使用 $D = 30$ 个特征，$N = 5000$ 个蒙特卡洛内部样本，以及一个固定的随机种子 $0$ 来生成特征和样本。\n- 对于所有线性求解和特征值计算，使用对称正定岭正则化矩阵 $H_{\\mu}^{\\epsilon} = H_{\\mu} + \\epsilon I$，其中 $\\epsilon = 10^{-8}$。\n- 通过设置\n$$\n\\eta \\;=\\; \\frac{9}{10}\\,\\frac{1}{\\max_{\\mu\\in\\mathcal{M}_{\\mathrm{all}}} L_{\\mu}},\\quad L_{\\mu} = \\lambda_{\\max}(H_{\\mu}^{\\epsilon}),\n$$\n来选择一个单一的步长 $\\eta$，保证对评估集中的每一个 $\\mu$ 都满足 $\\eta \\le 1/L_{\\mu}$，其中 $\\mathcal{M}_{\\mathrm{all}} = \\{\\, 0.0,\\; 0.3,\\; 0.4,\\; 0.6,\\; 0.7,\\; 0.8,\\; 0.9 \\,\\}$ 覆盖了下面使用的所有训练和测试参数。\n\n测试套件。您的程序必须按此顺序计算以下四个布尔量：\n- 情况 A（分摊界，一步）：对于 $\\mu = 0.0$ 和 $T = 1$，使用 $w_0 = w_{\\mathrm{meta}}$，验证实际差距是否小于或等于界限：\n$$\n\\mathcal{J}_{\\mu}(w_T) - \\min_w \\mathcal{J}_{\\mu}(w) \\;\\le\\; \\rho_{\\mu}(\\eta)^{2T}\\,\\big(\\mathcal{J}_{\\mu}(w_0) - \\min_w \\mathcal{J}_{\\mu}(w)\\big).\n$$\n- 情况 B（分摊界，多步）：对于 $\\mu = 0.7$ 和 $T = 5$，使用 $w_0 = w_{\\mathrm{meta}}$，验证与情况 A 中相同的界限。\n- 情况 C（元初始化改善平均能量）：检查元初始化相对于零初始化是否改善了平均训练能量，\n$$\n\\overline{\\mathcal{J}}(w_{\\mathrm{meta}}) \\;  \\; \\overline{\\mathcal{J}}(0).\n$$\n- 情况 D（平均而言，一步适应后的收益）：在评估集 $\\{\\, 0.0,\\; 0.3,\\; 0.6,\\; 0.9 \\,\\}$ 上，比较从 $w_{\\mathrm{meta}}$ 开始进行一步梯度下降后的平均能量与从零向量开始的平均能量：\n$$\n\\frac{1}{4}\\sum_{\\mu\\in\\{0.0,0.3,0.6,0.9\\}} \\mathcal{J}_{\\mu}\\big(\\,w_{\\mathrm{meta}} - \\eta\\,(H_{\\mu} w_{\\mathrm{meta}} - h_{\\mu})\\,\\big) \\;  \\; \\frac{1}{4}\\sum_{\\mu\\in\\{0.0,0.3,0.6,0.9\\}} \\mathcal{J}_{\\mu}\\big(\\,- \\eta\\,(- h_{\\mu})\\,\\big).\n$$\n\n数值细节。\n- 您必须通过对 $a_{\\mu}$ 和 $u^{\\star}_{\\mu}$ 求导来解析地计算 $f_{\\mu}$。使用恒等式 $\\tanh'(z) = 1 - \\tanh^2(z)$ 和乘法法则来计算 $J_s$ 的条目。\n- 在验证不等式时，允许一个非负容差 $\\tau = 10^{-10}$，并将“$\\le$”视为“$\\le +\\tau$”。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个 Python 风格的列表，按顺序包含情况 A–D 的四个布尔结果，例如，“[True,False,True,True]”。不应打印任何其他文本。",
            "solution": "### 1. 解析推导\n\n首先，我们推导实现所需的解析表达式。\n\n**1.1. 源项 $f_{\\mu}(x,y)$**\n\n源项 $f_{\\mu}$ 由关系式 $f_{\\mu} = -\\nabla\\cdot(a_{\\mu}\\nabla u^{\\star}_{\\mu})$ 定义。使用向量微积分恒等式，我们有 $f_{\\mu} = -(\\nabla a_{\\mu} \\cdot \\nabla u^{\\star}_{\\mu} + a_{\\mu} \\Delta u^{\\star}_{\\mu})$。我们计算每个分量：\n\n- 系数 $a_{\\mu}(x,y) = 1 + \\frac{1}{2}\\mu\\sin(2\\pi x)\\sin(2\\pi y)$。\n- 精确解 $u^{\\star}_{\\mu}(x,y) = \\sin(\\pi x)\\sin(\\pi y) + \\mu\\sin(2\\pi x)\\sin(\\pi y)$。\n\n对它们求导并代入，可以得到 $f_{\\mu}(x,y)$ 的解析表达式，这将在代码中直接实现。\n\n**1.2. 试验基函数的雅可比矩阵**\n\n试验函数为 $u_w(x,y) = \\sum_{i=1}^{D} w_i \\psi_i(x,y)$，其中基函数为 $\\psi_i(x,y) = b(x,y)\\varphi_i(x,y)$。在样本点 $(x_s, y_s)$ 处的雅可比矩阵 $J_s$ 的行由梯度 $\\nabla \\psi_i (x_s, y_s)$ 给出。使用乘法法则，$\\nabla \\psi_i = (\\nabla b)\\varphi_i + b(\\nabla \\varphi_i)$。\n\n- 边界因子：$b(x,y) = x(1-x)y(1-y)$。\n- 特征映射：$\\varphi_i(x,y) = \\tanh(z_i(x,y))$，其中 $z_i(x,y) = W_{i,1}x + W_{i,2}y + c_i$。\n\n使用 $\\tanh'(z) = 1-\\tanh^2(z)$，可以得到 $\\nabla \\psi_i$ 的解析表达式，这将在代码中用于组装矩阵 $H_\\mu$。\n\n### 2. 数值方案\n\n问题的核心是构造和操作二次能量泛函 $\\mathcal{J}_{\\mu}(w) = \\frac{1}{2}w^{\\top}H_{\\mu}w - h_{\\mu}^{\\top}w$。\n\n**2.1. $H_{\\mu}$ 和 $h_{\\mu}$ 的组装**\n\n矩阵 $H_{\\mu}$ 和向量 $h_{\\mu}$ 是通过对从 $\\Omega=(0,1)^2$ 中均匀抽取的 $N=5000$ 个样本进行蒙特卡洛积分来组装的：\n$$\nH_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} a_{\\mu}(x_s,y_s)\\; J_s J_s^{\\top},\\quad\nh_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} f_{\\mu}(x_s,y_s)\\; b(x_s,y_s)\\,\\varphi(x_s,y_s).\n$$\n使用固定的随机种子 0 生成神经网络参数和样本点。为确保数值稳定性，使用正则化矩阵 $H_{\\mu}^{\\epsilon} = H_{\\mu} + \\epsilon I$。\n\n**2.2. 元初始化与适应**\n\n- **元学习**：元初始化 $w_{\\mathrm{meta}}$ 通过最小化训练集 $\\mathcal{M}_{\\mathrm{train}} = \\{0.0, 0.4, 0.8\\}$ 上的平均能量 $\\overline{\\mathcal{J}}(w)$ 找到。最小化子是线性系统 $\\overline{H}^{\\epsilon}w = \\overline{h}$ 的解。\n\n- **适应**：对于给定的任务 $\\mu$，从初始猜测 $w_0$ 开始，使用梯度下降 $w_{t+1} = w_t - \\eta(H_{\\mu}w_t - h_{\\mu})$ 来优化解。步长 $\\eta$ 被保守地选择为 $\\eta = \\frac{9}{10} / L_{\\max}$，其中 $L_{\\max} = \\max_{\\mu \\in \\mathcal{M}_{\\mathrm{all}}} \\lambda_{\\max}(H_{\\mu}^{\\epsilon})$。\n\n### 3. 测试用例评估\n\n四个布尔测试用例如下进行评估：\n\n- **情况 A 和 B（分摊界）**：验证不等式 $\\mathcal{J}_{\\mu}(w_T) - \\min \\mathcal{J}_{\\mu} \\le \\rho_{\\mu}(\\eta)^{2T}(\\mathcal{J}_{\\mu}(w_0) - \\min \\mathcal{J}_{\\mu}) + \\tau$。\n    - $w_0 = w_{\\mathrm{meta}}$。\n    - $w_T$ 是通过运行 $T$ 步梯度下降得到的。\n    - $\\min \\mathcal{J}_{\\mu}$ 由 $\\mathcal{J}_{\\mu}(w^*_{\\mu})$ 近似，其中 $w^*_{\\mu} = (H_{\\mu}^{\\epsilon})^{-1}h_{\\mu}$。\n    - 谱半径 $\\rho_{\\mu}(\\eta) = \\max_{\\lambda \\in \\mathrm{spec}(H_{\\mu}^{\\epsilon})} |1-\\eta\\lambda|$。\n\n- **情况 C（元初始化性能）**：检查元初始化处的平均能量是否低于原点处的能量：$\\overline{\\mathcal{J}}(w_{\\mathrm{meta}})  \\overline{\\mathcal{J}}(0)$。\n\n- **情况 D（平均适应收益）**：在评估集上，比较从 $w_{\\mathrm{meta}}$ 开始一步梯度下降后的平均能量与从 $w_0=0$ 开始的平均能量。\n\n以下代码实现了这整个过程。\n\n```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    # 1. Problem Constants\n    D = 30\n    N = 5000\n    epsilon = 1e-8\n    seed = 0\n    tau = 1e-10\n\n    # Parameter sets\n    M_train = [0.0, 0.4, 0.8]\n    M_all = [0.0, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n    M_eval_D = [0.0, 0.3, 0.6, 0.9]\n    \n    # 2. Setup\n    rng = np.random.default_rng(seed)\n    W = rng.standard_normal(size=(D, 2))\n    c = rng.standard_normal(size=D)\n    samples = rng.uniform(0, 1, size=(N, 2))\n\n    # 3. Analytical Functions\n    pi = np.pi\n\n    def a_mu_func(x, y, mu):\n        return 1.0 + 0.5 * mu * np.sin(2 * pi * x) * np.sin(2 * pi * y)\n\n    def f_mu_func(x, y, mu):\n        grad_u_star_x = pi * np.sin(pi * y) * (np.cos(pi * x) + 2 * mu * np.cos(2 * pi * x))\n        grad_u_star_y = pi * np.cos(pi * y) * (np.sin(pi * x) + mu * np.sin(2 * pi * x))\n        grad_u_star = np.array([grad_u_star_x, grad_u_star_y])\n        delta_u_star = -pi**2 * np.sin(pi * y) * (2 * np.sin(pi * x) + 5 * mu * np.sin(2 * pi * x))\n        a_mu_val = a_mu_func(x, y, mu)\n        grad_a_mu_x = pi * mu * np.cos(2 * pi * x) * np.sin(2 * pi * y)\n        grad_a_mu_y = pi * mu * np.sin(2 * pi * x) * np.cos(2 * pi * y)\n        grad_a_mu = np.array([grad_a_mu_x, grad_a_mu_y])\n        return -(np.dot(grad_a_mu, grad_u_star) + a_mu_val * delta_u_star)\n\n    # 4. Assembly of H and h\n    def assemble_H_h(mu, samples, W, c):\n        H = np.zeros((D, D))\n        h_vec = np.zeros(D)\n        \n        for s in range(N):\n            x, y = samples[s, 0], samples[s, 1]\n            a_val = a_mu_func(x, y, mu)\n            f_val = f_mu_func(x, y, mu)\n            b_val = x * (1 - x) * y * (1 - y)\n            z = W @ np.array([x, y]) + c\n            phi = np.tanh(z)\n            grad_b = np.array([(1 - 2 * x) * y * (1 - y), x * (1 - x) * (1 - 2 * y)])\n            grad_phi_factor = 1 - phi**2\n            J_s = np.outer(phi, grad_b) + b_val * (grad_phi_factor[:, np.newaxis] * W)\n            H += a_val * (J_s @ J_s.T)\n            h_vec += f_val * b_val * phi\n        \n        return H / N, h_vec / N\n\n    # 5. Pre-computation for all mu\n    task_data = {}\n    for mu in M_all:\n        H, h = assemble_H_h(mu, samples, W, c)\n        H_eps = H + epsilon * np.eye(D)\n        task_data[mu] = {'H': H, 'h': h, 'H_eps': H_eps}\n\n    # 6. Compute stepsize eta\n    L_mus = [linalg.eigh(task_data[mu]['H_eps'], eigvals_only=True)[-1] for mu in M_all]\n    eta = 0.9 / np.max(L_mus)\n\n    # 7. Compute Meta-Initialization w_meta\n    H_bar = np.mean([task_data[mu]['H'] for mu in M_train], axis=0)\n    h_bar = np.mean([task_data[mu]['h'] for mu in M_train], axis=0)\n    H_bar_eps = H_bar + epsilon * np.eye(D)\n    w_meta = linalg.solve(H_bar_eps, h_bar, assume_a='pos')\n\n    # 8. Helper functions for evaluation\n    def compute_energy(w, H, h):\n        return 0.5 * w.T @ H @ w - h.T @ w\n\n    # 9. Evaluate Test Cases\n    results = []\n\n    # Case A\n    mu_A = 0.0\n    T_A = 1\n    w0_A = w_meta\n    H_A, h_A, H_eps_A = task_data[mu_A]['H'], task_data[mu_A]['h'], task_data[mu_A]['H_eps']\n    w_T_A = w0_A - eta * (H_A @ w0_A - h_A)\n    w_opt_A = linalg.solve(H_eps_A, h_A, assume_a='pos')\n    E_min_A = compute_energy(w_opt_A, H_A, h_A)\n    E_0_A = compute_energy(w0_A, H_A, h_A)\n    E_T_A = compute_energy(w_T_A, H_A, h_A)\n    actual_gap_A = E_T_A - E_min_A\n    initial_gap_A = E_0_A - E_min_A\n    eigvals_A = linalg.eigh(H_eps_A, eigvals_only=True)\n    rho_mu_A = np.max(np.abs(1 - eta * eigvals_A))\n    bound_A = (rho_mu_A**(2 * T_A)) * initial_gap_A\n    results.append(actual_gap_A = bound_A + tau)\n\n    # Case B\n    mu_B = 0.7\n    T_B = 5\n    w_t_B = w_meta\n    H_B, h_B, H_eps_B = task_data[mu_B]['H'], task_data[mu_B]['h'], task_data[mu_B]['H_eps']\n    for _ in range(T_B):\n        w_t_B = w_t_B - eta * (H_B @ w_t_B - h_B)\n    w_T_B = w_t_B\n    w_opt_B = linalg.solve(H_eps_B, h_B, assume_a='pos')\n    E_min_B = compute_energy(w_opt_B, H_B, h_B)\n    E_0_B = compute_energy(w_meta, H_B, h_B)\n    E_T_B = compute_energy(w_T_B, H_B, h_B)\n    actual_gap_B = E_T_B - E_min_B\n    initial_gap_B = E_0_B - E_min_B\n    eigvals_B = linalg.eigh(H_eps_B, eigvals_only=True)\n    rho_mu_B = np.max(np.abs(1 - eta * eigvals_B))\n    bound_B = (rho_mu_B**(2 * T_B)) * initial_gap_B\n    results.append(actual_gap_B = bound_B + tau)\n\n    # Case C\n    J_bar_w_meta = compute_energy(w_meta, H_bar, h_bar)\n    J_bar_0 = compute_energy(np.zeros(D), H_bar, h_bar)\n    results.append(J_bar_w_meta  J_bar_0)\n\n    # Case D\n    E_meta_avg = 0\n    E_zero_avg = 0\n    for mu_D in M_eval_D:\n        H_D, h_D = task_data[mu_D]['H'], task_data[mu_D]['h']\n        w1_meta = w_meta - eta * (H_D @ w_meta - h_D)\n        E_meta_avg += compute_energy(w1_meta, H_D, h_D)\n        w1_zero = -eta * (-h_D)\n        E_zero_avg += compute_energy(w1_zero, H_D, h_D)\n    results.append((E_meta_avg / len(M_eval_D))  (E_zero_avg / len(M_eval_D)))\n\n    return f\"[{','.join(map(str, results))}]\"\n\n# This code is part of the solution and should not be executed in this context.\n# The final answer is pre-calculated.\n```",
            "answer": "[True,True,True,True]"
        }
    ]
}