{
    "hands_on_practices": [
        {
            "introduction": "The foundation of any spectral method is its ability to represent functions efficiently. This first exercise guides you through the process of approximating a classic smooth function, $f(x) = \\exp(x)$, with a truncated Chebyshev series. By computing the coefficients both analytically and numerically , you will gain a concrete understanding of spectral coefficients and witness the rapid convergence that makes these methods so powerful.",
            "id": "3370275",
            "problem": "Consider the Chebyshev polynomials of the first kind defined by $T_n(x) = \\cos(n \\arccos x)$ on the interval $[-1,1]$. For a sufficiently smooth function $f:[-1,1]\\to\\mathbb{R}$, the Chebyshev spectral expansion is the series\n$$\nf(x) \\sim \\frac{\\hat{f}_0}{2} + \\sum_{n=1}^{\\infty} \\hat{f}_n T_n(x),\n$$\nwhere the Chebyshev coefficients $\\hat{f}_n$ are defined by the orthogonality of $\\cos(n\\theta)$ on $[0,\\pi]$ and given by the integral representation\n$$\n\\hat{f}_n = \\frac{2}{\\pi}\\int_{0}^{\\pi} f(\\cos\\theta)\\,\\cos(n\\theta)\\,d\\theta, \\quad n\\ge 0,\n$$\nwith the angle $\\theta$ measured in radians. In Chebyshev spectral methods, the truncated expansion of degree $N$,\n$$\nf_N(x) := \\frac{\\hat{f}_0}{2} + \\sum_{n=1}^{N} \\hat{f}_n T_n(x),\n$$\nis used as a high-order approximation to $f(x)$.\n\nYou are asked to compute the Chebyshev coefficients for the specific function $f(x)=e^x$, construct the truncated series $f_{10}(x)$, and evaluate pointwise errors against $f(x)$ in specified scenarios. Your derivation and algorithm must begin from the fundamental definitions above and use only well-tested mathematical facts, not shortcut formulas.\n\nAngle unit specification: all angles must be handled in radians.\n\nImplement a complete, runnable program that performs the following computations:\n\n1. Using the most precise derivation available from first principles, compute the Chebyshev coefficients $\\hat{f}_n$ for $f(x)=e^x$ for $n=0,1,\\dots,10$, construct $f_{10}(x)$, and compute the absolute pointwise error $|f(0.5)-f_{10}(0.5)|$.\n\n2. Approximate the Chebyshev coefficients $\\hat{f}_n$ for $f(x)=e^x$ using numerical quadrature of the defining integral for $n=0,1,\\dots,10$ with a composite trapezoidal rule over $\\theta\\in[0,\\pi]$ using $M=16384$ equally spaced panels (i.e., $M+1$ nodes). Use these approximate coefficients to construct $f_{10}(x)$ and compute the absolute pointwise error $|f(0.5)-f_{10}(0.5)|$.\n\n3. Using the most precise coefficients from Item 1, compute the supremum norm error of $f_{10}(x)$ on $[-1,1]$, defined as $\\max_{x\\in[-1,1]} |f(x)-f_{10}(x)|$, by evaluating the error on a uniform grid of $K=20001$ points in $[-1,1]$ and taking the maximum absolute value.\n\nTest suite and answer specification:\n- Test Case 1 (happy path): Item 1 described above, output a single float equal to the absolute error at $x=0.5$.\n- Test Case 2 (numerical integration coverage): Item 2 described above, output a single float equal to the absolute error at $x=0.5$.\n- Test Case 3 (boundary coverage via supremum norm on a dense grid): Item 3 described above, output a single float equal to the computed maximum absolute error.\n\nFinal output format:\nYour program should produce a single line of output containing the results of the three test cases as a comma-separated list enclosed in square brackets, in the order [Test Case 1 result, Test Case 2 result, Test Case 3 result]. For example, \n$$\n[\\text{result1},\\text{result2},\\text{result3}]\n$$\nwhere each entry is a float.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in numerical analysis concerning the application of Chebyshev spectral methods. It is self-contained and free of contradictions or ambiguities.\n\nThe problem requires the analysis of the Chebyshev spectral approximation for the function $f(x) = e^x$ on the interval $x \\in [-1, 1]$. The Chebyshev series approximation of degree $N$ is given by\n$$\nf_N(x) = \\frac{\\hat{f}_0}{2} + \\sum_{n=1}^{N} \\hat{f}_n T_n(x)\n$$\nwhere $T_n(x)$ are the Chebyshev polynomials of the first kind and $\\hat{f}_n$ are the Chebyshev coefficients. The coefficients are defined by the integral\n$$\n\\hat{f}_n = \\frac{2}{\\pi}\\int_{0}^{\\pi} f(\\cos\\theta)\\,\\cos(n\\theta)\\,d\\theta, \\quad n \\ge 0.\n$$\n\nFor the specific function $f(x) = e^x$, the integrand becomes $f(\\cos\\theta) = e^{\\cos\\theta}$. The expression for the coefficients is therefore\n$$\n\\hat{f}_n = \\frac{2}{\\pi}\\int_{0}^{\\pi} e^{\\cos\\theta}\\cos(n\\theta)\\,d\\theta.\n$$\nThis integral is directly related to a standard integral representation of the modified Bessel function of the first kind, $I_n(z)$, which for integer order $n$ is given by\n$$\nI_n(z) = \\frac{1}{\\pi} \\int_0^\\pi e^{z \\cos\\theta} \\cos(n\\theta) d\\theta.\n$$\nBy setting $z=1$, we can establish a direct relationship between the Chebyshev coefficients of $e^x$ and the values of the modified Bessel functions.\n$$\n\\hat{f}_n = 2 \\left( \\frac{1}{\\pi} \\int_0^\\pi e^{1 \\cdot \\cos\\theta} \\cos(n\\theta) d\\theta \\right) = 2 I_n(1).\n$$\nThis provides an analytical expression for the \"exact\" coefficients required in Test Cases 1 and 3. These values can be computed to high precision using standard scientific libraries.\n\nTo evaluate the truncated series $f_N(x)$, we employ Clenshaw's algorithm, which is a stable and efficient method for summing series of polynomials that satisfy a three-term recurrence relation. The Chebyshev polynomials satisfy the recurrence\n$$\nT_0(x) = 1, \\quad T_1(x) = x, \\quad T_{n+1}(x) = 2x T_n(x) - T_{n-1}(x) \\text{ for } n \\ge 1.\n$$\nLet the series to be evaluated be $S(x) = \\sum_{k=0}^{N} c'_k T_k(x)$, where $c'_0 = \\hat{f}_0/2 = I_0(1)$ and $c'_k = \\hat{f}_k = 2I_k(1)$ for $k \\ge 1$. Clenshaw's algorithm proceeds as follows:\nInitialize $b_{N+2} = 0$ and $b_{N+1} = 0$.\nThen, for $k = N, N-1, \\dots, 1$, compute\n$$\nb_k = c'_k + 2x b_{k+1} - b_{k+2}.\n$$\nThe value of the series is then given by $S(x) = c'_0 + x b_1 - b_2$.\n\n**Test Case 1: Pointwise error with analytically derived coefficients**\nThe Chebyshev coefficients $\\hat{f}_n$ for $n=0, 1, \\dots, 10$ are computed using the identity $\\hat{f}_n = 2 I_n(1)$. The numerical values for $I_n(1)$ are obtained from the `scipy.special.iv` function. These coefficients are used to construct the Clenshaw coefficient array $c'$. The polynomial $f_{10}(x)$ is then evaluated at $x=0.5$ using Clenshaw's algorithm. The absolute error is calculated as $|e^{0.5} - f_{10}(0.5)|$.\n\n**Test Case 2: Pointwise error with numerically integrated coefficients**\nThe Chebyshev coefficients $\\hat{f}_n$ for $n=0, 1, \\dots, 10$ are approximated by numerically evaluating the defining integral. The composite trapezoidal rule is applied to the integral $\\int_{0}^{\\pi} e^{\\cos\\theta}\\cos(n\\theta)\\,d\\theta$ over a uniform grid of $\\theta$ values. The interval $[0, \\pi]$ is divided into $M=16384$ panels, creating $M+1$ nodes $\\theta_j = j\\pi/M$ for $j=0, \\dots, M$. The integral $I$ of a function $g(\\theta)$ is approximated by\n$$\nI \\approx h \\left( \\frac{g(\\theta_0) + g(\\theta_M)}{2} + \\sum_{j=1}^{M-1} g(\\theta_j) \\right),\n$$\nwhere $h = \\pi/M$ is the step size. For each $n$, the corresponding integrand $g_n(\\theta) = e^{\\cos\\theta}\\cos(n\\theta)$ is used. The resulting approximate coefficients, $\\hat{f}_n^{(\\text{num})}$, are then used to construct a new polynomial approximation $f_{10}^{(\\text{num})}(x)$. This polynomial is evaluated at $x=0.5$ and the absolute error $|e^{0.5} - f_{10}^{(\\text{num})}(0.5)|$ is computed.\n\n**Test Case 3: Supremum norm error**\nThis test case requires computing an approximation to the supremum norm of the error, defined as $\\max_{x\\in[-1,1]} |f(x)-f_{10}(x)|$. The approximation is found by evaluating the error on a dense, uniform grid of $K=20001$ points spanning the interval $[-1, 1]$. The polynomial $f_{10}(x)$ is constructed using the analytically derived coefficients from Test Case 1. A vectorized version of Clenshaw's algorithm is used to efficiently evaluate $f_{10}(x_j)$ at all grid points $x_j$ simultaneously. The absolute error $|e^{x_j} - f_{10}(x_j)|$ is computed for each point, and the maximum value among these errors is reported as the result.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import iv\n\ndef clenshaw_eval(x, c_prime):\n    \"\"\"\n    Evaluates a Chebyshev series using Clenshaw's algorithm.\n    Can handle both scalar and NumPy array inputs for x.\n    \n    Args:\n        x (float or np.ndarray): The point(s) at which to evaluate the series.\n        c_prime (np.ndarray): The coefficients of the series, where c_prime[k]\n                             is the coefficient of T_k(x). The series is\n                             assumed to be sum_{k=0 to N} c_prime[k] T_k(x).\n\n    Returns:\n        float or np.ndarray: The value(s) of the Chebyshev series.\n    \"\"\"\n    N = len(c_prime) - 1\n    if N  0:\n        return 0.0 if not isinstance(x, np.ndarray) else np.zeros_like(x)\n\n    # Ensure x is a NumPy array for vectorized operations\n    is_scalar = not isinstance(x, np.ndarray)\n    if is_scalar:\n        x_arr = np.array([x])\n    else:\n        x_arr = x\n\n    # Initialize b_{N+2} and b_{N+1}\n    b_k_plus_2 = np.zeros_like(x_arr)\n    b_k_plus_1 = np.zeros_like(x_arr)\n\n    # Recurrence loop\n    for k in range(N, 0, -1):\n        b_k = c_prime[k] + 2 * x_arr * b_k_plus_1 - b_k_plus_2\n        b_k_plus_2 = b_k_plus_1\n        b_k_plus_1 = b_k\n    \n    # Final value calculation\n    # val = c_prime[0] * T_0(x) + b_1 * T_1(x) - b_2 * T_0(x)\n    val = c_prime[0] + x_arr * b_k_plus_1 - b_k_plus_2\n    \n    return val.item() if is_scalar else val\n\ndef solve():\n    \"\"\"\n    Solves the three test cases as described in the problem statement.\n    \"\"\"\n    results = []\n    \n    # --- Parameters from the problem statement ---\n    N_degree = 10\n    M_panels = 16384\n    K_points = 20001\n    x_eval_point = 0.5\n\n    # --- Test Case 1: Pointwise error with analytically derived coefficients ---\n\n    # n = 0, 1, ..., N\n    n_values = np.arange(0, N_degree + 1)\n\n    # Exact coefficients f_hat_n = 2 * I_n(1)\n    f_hat_exact = 2 * iv(n_values, 1.0)\n    \n    # Create coefficients for Clenshaw's algorithm (c'_0 = f_hat_0 / 2)\n    c_prime_exact = np.copy(f_hat_exact)\n    c_prime_exact[0] /= 2.0\n    \n    # Evaluate at x = 0.5\n    f_val_exact_pt = np.exp(x_eval_point)\n    f10_val_exact_pt = clenshaw_eval(x_eval_point, c_prime_exact)\n    \n    error1 = np.abs(f_val_exact_pt - f10_val_exact_pt)\n    results.append(error1)\n\n    # --- Test Case 2: Pointwise error with numerically integrated coefficients ---\n    \n    theta = np.linspace(0, np.pi, M_panels + 1)\n    exp_cos_theta = np.exp(np.cos(theta))\n    \n    f_hat_num = np.zeros(N_degree + 1)\n    for n in n_values:\n        integrand = exp_cos_theta * np.cos(n * theta)\n        integral = np.trapz(integrand, theta)\n        f_hat_num[n] = (2.0 / np.pi) * integral\n    \n    # Create coefficients for Clenshaw's algorithm\n    c_prime_num = np.copy(f_hat_num)\n    c_prime_num[0] /= 2.0\n    \n    # Evaluate at x = 0.5\n    f10_val_num_pt = clenshaw_eval(x_eval_point, c_prime_num)\n    \n    error2 = np.abs(f_val_exact_pt - f10_val_num_pt)\n    results.append(error2)\n\n    # --- Test Case 3: Supremum norm error ---\n    \n    x_grid = np.linspace(-1.0, 1.0, K_points)\n    \n    # Use exact coefficients from Test Case 1\n    f10_on_grid = clenshaw_eval(x_grid, c_prime_exact)\n    f_on_grid = np.exp(x_grid)\n    \n    error3 = np.max(np.abs(f_on_grid - f10_on_grid))\n    results.append(error3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After representing functions, the next step is to operate on them, with differentiation being the most fundamental operation for solving differential equations. This practice focuses on constructing and applying spectral differentiation matrices from first principles, a core skill for any practitioner . You will implement the procedure for both Chebyshev and Legendre-Gauss-Lobatto grids, appreciating the underlying unity of the Lagrange-based approach.",
            "id": "3370324",
            "problem": "Consider the polynomial spectral differentiation of a smooth function on the interval $[-1,1]$. Let $N$ denote the polynomial degree and let the $N+1$ interpolation nodes be denoted by $\\{x_j\\}_{j=0}^{N}$. Define the Lagrange basis polynomials $\\{\\ell_j(x)\\}_{j=0}^{N}$ associated with these nodes by the conditions $\\ell_j(x_k)=\\delta_{jk}$, where $\\delta_{jk}$ is the Kronecker delta. The spectral differentiation matrix $D\\in\\mathbb{R}^{(N+1)\\times(N+1)}$ is defined by evaluating the derivatives of the basis polynomials at the nodes, so that the vector of derivative values of an interpolating polynomial $p(x)=\\sum_{j=0}^{N} u_j \\ell_j(x)$ at the nodes is $Dp$, where each entry is formed from $\\sum_{j=0}^{N} u_j \\ell_j'(x_i)$.\n\nThe base for this construction is the fundamental definition of polynomial interpolation and its Lagrange form: any polynomial of degree at most $N$ that interpolates data $\\{(x_j, u_j)\\}_{j=0}^{N}$ is uniquely represented as $p(x)=\\sum_{j=0}^{N} u_j \\ell_j(x)$, and by linearity of differentiation, $p'(x)=\\sum_{j=0}^{N} u_j \\ell_j'(x)$. The derivative values at the nodes are then determined by the matrix with entries $D_{ij}=\\ell_j'(x_i)$. You must construct $D$ directly from these principles, without using pre-tabulated closed-form formulas for any special node set.\n\nYou will use two classical spectral grids:\n- Chebyshev–Gauss–Lobatto nodes of the first kind (CGL): $x_j=\\cos\\left(\\frac{\\pi j}{N}\\right)$ for $j=0,1,\\dots,N$.\n- Legendre–Gauss–Lobatto nodes (LGL): $x_0=-1$, $x_N=1$, and the interior nodes $\\{x_j\\}_{j=1}^{N-1}$ are the $N-1$ distinct roots in $(-1,1)$ of the derivative of the Legendre polynomial of degree $N$, $P_N'(x)=0$. Use the Legendre differential equation to devise a Newton iteration that refines initial guesses for these interior roots. The Legendre differential equation is $(1-x^2)P_N''(x)-2xP_N'(x)+N(N+1)P_N(x)=0$. You must use this relationship to obtain a Newton update that does not rely on pre-tabulated formulas for the roots.\n\nFor a target function $u(x)=\\sin(5x)$ (angles in radians), approximate $u'(x)$ at the nodes by applying the spectral differentiation matrix $D$ to the nodal values of $u(x)$, and compare this approximation to the exact derivative $u'(x)=5\\cos(5x)$ at the same nodes. For each case, compute the $\\ell^\\infty$ error defined as\n$$\nE_{\\infty}=\\max_{0\\le i\\le N} \\left| (Du)_i - u'(x_i) \\right|.\n$$\n\nYour program must implement the construction of $D$ from first principles using the Lagrange basis derivative evaluated at the nodes (you are allowed to use the general barycentric Lagrange machinery, but you must not use any explicit closed-form differentiation matrix tailored to a specific node family). For the Legendre–Gauss–Lobatto nodes, initialize the Newton iteration for the interior roots using the Chebyshev–Gauss–Lobatto nodes and derive the Newton update using the Legendre differential equation.\n\nUse the following test suite, which varies both the node family and the degree:\n1. CGL with $N=8$.\n2. CGL with $N=32$.\n3. CGL with $N=64$.\n4. LGL with $N=8$.\n5. LGL with $N=32$.\n6. LGL with $N=64$.\n\nFor each test case, the output must be the single floating-point number $E_{\\infty}$. Your program should produce a single line of output containing the six results as a comma-separated list enclosed in square brackets (e.g., \"[e1,e2,e3,e4,e5,e6]\"). No other text should be printed. All angles are in radians. No physical units apply.",
            "solution": "The problem requires the construction of spectral differentiation matrices for Chebyshev and Legendre grids and their application to approximate the derivative of a given function. The construction must be performed from first principles, without recourse to known closed-form expressions for the differentiation matrices themselves. The core of the problem lies in the correct implementation of the node generation algorithms, particularly for Legendre–Gauss–Lobatto nodes, and the subsequent construction of the differentiation matrix using the general properties of Lagrange interpolants.\n\n### 1. General Formulation of the Spectral Differentiation Matrix\n\nGiven a set of $N+1$ distinct interpolation nodes $\\{x_j\\}_{j=0}^{N}$ on the interval $[-1, 1]$, any polynomial $p(x)$ of degree at most $N$ can be uniquely represented using the Lagrange basis polynomials $\\{\\ell_j(x)\\}_{j=0}^{N}$ as:\n$$\np(x) = \\sum_{j=0}^{N} u_j \\ell_j(x)\n$$\nwhere $u_j = p(x_j)$ are the function values at the nodes, and the basis polynomials satisfy the property $\\ell_j(x_k) = \\delta_{jk}$. The derivative of this polynomial is given by the linear combination of the derivatives of the basis polynomials:\n$$\np'(x) = \\sum_{j=0}^{N} u_j \\ell_j'(x)\n$$\nEvaluating this derivative at the interpolation nodes $x_i$ yields:\n$$\np'(x_i) = \\sum_{j=0}^{N} u_j \\ell_j'(x_i)\n$$\nThis relationship can be expressed as a matrix-vector product, $\\boldsymbol{u}' = D\\boldsymbol{u}$, where $\\boldsymbol{u}$ is the vector of nodal values $[u_0, u_1, \\dots, u_N]^T$, $\\boldsymbol{u}'$ is the vector of derivative values at the nodes $[p'(x_0), \\dots, p'(x_N)]^T$, and $D$ is the spectral differentiation matrix. The entries of this matrix are given by:\n$$\nD_{ij} = \\ell_j'(x_i)\n$$\nThe problem forbids the use of pre-computed formulas for $D_{ij}$ for specific node families but allows the use of general barycentric machinery. The Lagrange basis polynomial is defined as $\\ell_j(x) = \\prod_{k=0, k \\neq j}^{N} \\frac{x-x_k}{x_j-x_k}$. A general and stable method to compute its derivative at the nodes involves barycentric weights.\n\nThe barycentric weight $w_j$ for each node $x_j$ is defined from first principles as:\n$$\nw_j = \\frac{1}{\\prod_{k=0, k \\neq j}^{N} (x_j - x_k)}\n$$\nUsing these weights, the entries of the differentiation matrix can be expressed as:\n$$\nD_{ij} = \\frac{w_j/w_i}{x_i - x_j} \\quad \\text{for } i \\neq j\n$$\nFor the diagonal entries $D_{ii}$, one could use the formula $D_{ii} = \\sum_{k=0, k \\neq i}^{N} \\frac{1}{x_i-x_k}$. However, a more numerically robust approach is to utilize the property that the derivative of a constant must be zero. The constant function $f(x)=1$ is interpolated exactly by $p(x)=1$, which corresponds to a vector of nodal values $\\boldsymbol{u} = [1, 1, \\dots, 1]^T$. Its derivative is zero everywhere, which implies that $D\\boldsymbol{1} = \\boldsymbol{0}$. This means that the sum of each row of the differentiation matrix must be zero:\n$$\n\\sum_{j=0}^{N} D_{ij} = 0 \\quad \\text{for each } i = 0, \\dots, N\n$$\nFrom this property, we can compute the diagonal elements as the negative sum of the off-diagonal elements in the same row:\n$$\nD_{ii} = - \\sum_{j=0, j \\neq i}^{N} D_{ij}\n$$\nThis set of formulas provides a general algorithm to construct $D$ for any set of distinct nodes.\n\n### 2. Node Generation and Matrix Construction\n\nWe will apply this general procedure to two specific sets of nodes.\n\n#### 2.1. Chebyshev–Gauss–Lobatto (CGL) Nodes\nThe CGL nodes are the extrema of the Chebyshev polynomial of the first kind, $T_N(x)$. They are given by the explicit formula:\n$$\nx_j = \\cos\\left(\\frac{\\pi j}{N}\\right) \\quad \\text{for } j=0, 1, \\dots, N\n$$\nFor a given degree $N$, we first generate these $N+1$ nodes. Then, we compute the barycentric weights $w_j$ for each node using the fundamental product definition. Finally, we construct the CGL differentiation matrix $D^{\\text{CGL}}$ using the general barycentric formulas for $D_{ij}$.\n\n#### 2.2. Legendre–Gauss–Lobatto (LGL) Nodes\nThe LGL nodes include the endpoints of the interval, $x_0 = -1$ and $x_N = 1$. The $N-1$ interior nodes, $\\{x_j\\}_{j=1}^{N-1}$, are the roots of $P_N'(x)$, where $P_N(x)$ is the Legendre polynomial of degree $N$.\n\nThe problem mandates finding these roots via Newton's method, with initial guesses provided by the interior CGL nodes. The Newton iteration for finding a root of a function $f(x)$ is $x_{k+1} = x_k - f(x_k)/f'(x_k)$. For our case, $f(x) = P_N'(x)$, so the iteration is:\n$$\nx_{k+1} = x_k - \\frac{P_N'(x_k)}{P_N''(x_k)}\n$$\nWe must derive an expression for the update term using the Legendre differential equation:\n$$\n(1-x^2)P_N''(x) - 2xP_N'(x) + N(N+1)P_N(x) = 0\n$$\nSolving for $P_N''(x)$ yields:\n$$\nP_N''(x) = \\frac{2xP_N'(x) - N(N+1)P_N(x)}{1-x^2}\n$$\nSubstituting this into the Newton iteration gives the update step:\n$$\nx_{k+1} = x_k - \\frac{P_N'(x_k)}{\\frac{2x_k P_N'(x_k) - N(N+1)P_N(x_k)}{1-x_k^2}} = x_k - \\frac{(1-x_k^2)P_N'(x_k)}{2x_k P_N'(x_k) - N(N+1)P_N(x_k)}\n$$\nTo implement this iteration, we need to evaluate $P_N(x)$ and $P_N'(x)$. Values of Legendre polynomials $P_k(x)$ can be computed using the three-term recurrence relation, or standard library functions. The derivative $P_N'(x)$ can be found efficiently using the identity:\n$$\n(1-x^2)P_N'(x) = N(P_{N-1}(x) - xP_N(x))\n$$\nThe procedure for generating LGL nodes is:\n1. Set the endpoints $x_0 = -1$ and $x_N = 1$.\n2. For the interior nodes, take the initial guesses $x_j^{(0)} = \\cos(\\frac{\\pi j}{N})$ for $j=1, \\dots, N-1$.\n3. For each guess, iterate using the derived Newton update formula until the root converges to a desired tolerance. This requires evaluating $P_N(x)$ and $P_{N-1}(x)$ at each step.\n4. Collect the endpoints and the converged interior roots to form the set of LGL nodes $\\{x_j\\}_{j=0}^{N}$.\n5. With the nodes determined, the LGL differentiation matrix $D^{\\text{LGL}}$ is constructed using the same general barycentric method as for the CGL case.\n\n### 3. Error Calculation\n\nFor each test case (a node family and a degree $N$):\n1.  The appropriate set of nodes $\\{x_j\\}$ is generated.\n2.  The corresponding differentiation matrix $D$ is constructed.\n3.  The target function $u(x) = \\sin(5x)$ is evaluated at the nodes to form the vector $\\boldsymbol{u}$.\n4.  The numerical approximation of the derivative at the nodes is calculated via the matrix-vector product $\\boldsymbol{u}'_{approx} = D\\boldsymbol{u}$.\n5.  The exact derivative, $u'(x) = 5\\cos(5x)$, is evaluated at the nodes to form the vector $\\boldsymbol{u}'_{exact}$.\n6.  The $\\ell^\\infty$ error is computed as the maximum absolute difference between the approximated and exact derivative values:\n    $$\n    E_{\\infty} = \\max_{0\\le i\\le N} \\left| (\\boldsymbol{u}'_{approx})_i - (\\boldsymbol{u}'_{exact})_i \\right|\n    $$\nThis procedure is repeated for all specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import eval_legendre\n\ndef get_cgl_nodes(N):\n    \"\"\"Generates Chebyshev-Gauss-Lobatto nodes.\"\"\"\n    if N == 0:\n        return np.array([0.0])\n    j = np.arange(N + 1)\n    nodes = np.cos(np.pi * j / N)\n    return nodes\n\ndef get_lgl_nodes(N, tol=1e-15, max_iter=100):\n    \"\"\"\n    Generates Legendre-Gauss-Lobatto nodes.\n    The interior nodes are the roots of P_N'(x), found via Newton's method.\n    \"\"\"\n    if N == 0:\n        return np.array([0.0])\n    if N == 1:\n        return np.array([-1.0, 1.0])\n    \n    nodes = np.zeros(N + 1)\n    nodes[0] = -1.0\n    nodes[N] = 1.0\n    \n    # Initial guesses for interior roots are interior CGL nodes\n    j_int = np.arange(1, N)\n    guesses = np.cos(np.pi * j_int / N)\n    \n    for i in range(len(guesses)):\n        x_k = guesses[i]\n        \n        for _ in range(max_iter):\n            # The formulas are valid for x in (-1, 1).\n            # Newton iteration may push x_k slightly outside, clip it.\n            if abs(x_k) > 1.0:\n                x_k = np.sign(x_k)\n\n            # Evaluate Legendre polynomials P_N(x) and P_{N-1}(x)\n            PN_val = eval_legendre(N, x_k)\n            PN_minus_1_val = eval_legendre(N - 1, x_k)\n            \n            # Derivative P_N'(x) using relation (1-x^2)P_N'(x) = N(P_{N-1}(x) - xP_N(x))\n            dPN_val = N * (PN_minus_1_val - x_k * PN_val) / (1 - x_k**2)\n\n            # Newton update for a root of P_N'(x), derived from Legendre DE.\n            # update = (1-x^2)P_N'(x) / (2xP_N'(x) - N(N+1)P_N(x))\n            numerator = (1 - x_k**2) * dPN_val\n            denominator = 2 * x_k * dPN_val - N * (N + 1) * PN_val\n            \n            if abs(denominator)  1e-14:\n                # This may happen if the guess is already a root of the denominator.\n                # In such cases, the iteration is stuck; we just break.\n                break\n                \n            update = numerator / denominator\n            x_k -= update\n            \n            if abs(update)  tol:\n                break\n        \n        nodes[i + 1] = x_k\n        \n    nodes.sort() # Ensure nodes are in descending order (or ascending)\n    return nodes\n\ndef build_diff_matrix(nodes):\n    \"\"\"\n    Constructs the spectral differentiation matrix from first principles\n    using the barycentric formulation for any set of nodes.\n    \"\"\"\n    N = len(nodes) - 1\n    if N == 0:\n        return np.array([[0.0]])\n        \n    D = np.zeros((N + 1, N + 1))\n    \n    # Calculate barycentric weights from their fundamental definition\n    weights = np.zeros(N + 1)\n    for j in range(N + 1):\n        # Create an array of differences (x_j - x_k) for k != j\n        diffs = nodes[j] - np.delete(nodes, j)\n        weights[j] = 1.0 / np.prod(diffs)\n\n    # Off-diagonal entries using D_ij = (w_j/w_i) / (x_i - x_j)\n    for i in range(N + 1):\n        for j in range(N + 1):\n            if i != j:\n                D[i, j] = (weights[j] / weights[i]) / (nodes[i] - nodes[j])\n    \n    # Diagonal entries using the row-sum property: D_ii = -sum(D_ij) for j != i\n    for i in range(N + 1):\n        D[i, i] = -np.sum(D[i, :])\n        \n    return D\n\ndef compute_error(node_type, N):\n    \"\"\"\n    Main logic to compute the l-infinity error for a given case.\n    \"\"\"\n    if node_type == 'cgl':\n        nodes = get_cgl_nodes(N)\n    elif node_type == 'lgl':\n        nodes = get_lgl_nodes(N)\n    else:\n        raise ValueError(\"Unknown node type\")\n\n    D = build_diff_matrix(nodes)\n    \n    # Target function u(x) and its exact derivative u'(x)\n    u_func = lambda x: np.sin(5 * x)\n    du_exact_func = lambda x: 5 * np.cos(5 * x)\n    \n    # Evaluate at the nodes\n    u_vals = u_func(nodes)\n    du_exact_vals = du_exact_func(nodes)\n    \n    # Approximate derivative using the differentiation matrix\n    du_approx_vals = D @ u_vals\n    \n    # Compute the L-infinity error\n    error = np.max(np.abs(du_approx_vals - du_exact_vals))\n    return error\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ('cgl', 8),\n        ('cgl', 32),\n        ('cgl', 64),\n        ('lgl', 8),\n        ('lgl', 32),\n        ('lgl', 64),\n    ]\n\n    results = []\n    for case in test_cases:\n        node_type, N = case\n        error = compute_error(node_type, N)\n        results.append(f\"{error:.15e}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "When solving nonlinear equations, a critical challenge in spectral collocation methods is aliasing, where interactions on a discrete grid create spurious low-frequency signals. This exercise provides a sharp, analytical look at this phenomenon by examining the simple product $f(x) = (T_N(x))^2$ . By contrasting the true spectral projection with the result from the collocation grid, you will see precisely how aliasing error arises and contaminates the modal coefficients.",
            "id": "3370334",
            "problem": "Consider the Chebyshev polynomials of the first kind, defined by $T_n(x) = \\cos(n \\arccos x)$ for $x \\in [-1,1]$. Let the weighted inner product be defined by $\\langle f, g \\rangle_w = \\int_{-1}^{1} f(x) g(x) w(x) \\, dx$ with weight $w(x) = (1 - x^2)^{-1/2}$. For any function $f$ on $[-1,1]$ with finite energy under this weight, its Chebyshev modal coefficients $\\{a_k\\}_{k \\ge 0}$ are defined by the orthogonality relations: $a_0 = \\frac{1}{\\pi} \\int_{-1}^{1} f(x) w(x) \\, dx$ and for $k \\ge 1$, $a_k = \\frac{2}{\\pi} \\int_{-1}^{1} f(x) T_k(x) w(x) \\, dx$, so that $f(x) = \\sum_{k=0}^{\\infty} a_k T_k(x)$ in the $L^2_w([-1,1])$ sense. The $N$-mode modal projection of $f$ is the truncation $P_N f(x) = \\sum_{k=0}^{N} a_k T_k(x)$.\n\nDefine the Chebyshev–Lobatto collocation grid of $N+1$ nodes by $x_j = \\cos(\\theta_j)$ with $\\theta_j = \\pi j / N$ for $j = 0,1,\\dots,N$ (for $N=0$, interpret this as the single node $x_0 = 1$). Given a function $v$, its degree-$N$ Chebyshev collocation interpolant $I_N v$ is the unique polynomial of degree at most $N$ that satisfies $I_N v(x_j) = v(x_j)$ for all grid nodes. If $v$ is a polynomial of degree greater than $N$, the interpolant $I_N v$ incurs aliasing errors in modal space when compared to the modal projection $P_N v$.\n\nConstruct a numerical example with $u(x) = T_N(x)$ and the nonlinear product $f(x) = u(x)^2$. Evaluate the collocation product by forming the nodal samples $f_j = f(x_j)$ and solving for the interpolant coefficients $\\{ \\hat{a}_k \\}_{k=0}^{N}$ in the Chebyshev basis via the square linear system $\\sum_{k=0}^{N} \\hat{a}_k T_k(x_j) = f_j$ for $j=0,1,\\dots,N$. In parallel, determine the exact $N$-mode modal projection coefficients $\\{ a_k \\}_{k=0}^{N}$ of $f$ using only first principles (the defining properties of $T_n$, the inner product, and trigonometric identities). Define the aliasing error in mode $k$ as $e_k = \\hat{a}_k - a_k$ for $k=0,1,\\dots,N$, and define the modal error $2$-norm as $\\|e\\|_2 = \\left( \\sum_{k=0}^{N} e_k^2 \\right)^{1/2}$.\n\nYour program must implement this procedure numerically and report, for each test case, two quantities: the absolute error in the zeroth modal coefficient $|e_0|$ and the modal error $2$-norm $\\|e\\|_2$. There are no physical units in this problem. All angles implicitly use radians. The test suite is the set of polynomial degrees $N \\in \\{0, 1, 7, 16, 31\\}$.\n\nFinal output format: Your program should produce a single line of output containing a list of five items, each item being the $2$-entry list $[|e_0|, \\|e\\|_2]$ for the corresponding value of $N$ in the order specified above. The line must be a single bracketed comma-separated list of these $2$-entry lists, with no additional text. For example, a syntactically valid output has the form $[[a,b],[c,d],[\\dots],\\dots]$, where each symbol represents a real number.",
            "solution": "## PROBLEM VALIDATION\n\n### Step 1: Extract Givens\n- **Chebyshev Polynomials**: $T_n(x) = \\cos(n \\arccos x)$ for $x \\in [-1,1]$.\n- **Weighted Inner Product**: $\\langle f, g \\rangle_w = \\int_{-1}^{1} f(x) g(x) w(x) \\, dx$ with weight $w(x) = (1 - x^2)^{-1/2}$.\n- **Chebyshev Modal Coefficients**: For a function $f(x) = \\sum_{k=0}^{\\infty} a_k T_k(x)$:\n  - $a_0 = \\frac{1}{\\pi} \\int_{-1}^{1} f(x) w(x) \\, dx$\n  - $a_k = \\frac{2}{\\pi} \\int_{-1}^{1} f(x) T_k(x) w(x) \\, dx$ for $k \\ge 1$.\n- **N-mode Modal Projection**: $P_N f(x) = \\sum_{k=0}^{N} a_k T_k(x)$.\n- **Chebyshev–Lobatto Collocation Grid**: $x_j = \\cos(\\theta_j)$ with $\\theta_j = \\pi j / N$ for $j = 0,1,\\dots,N$. For $N=0$, the grid is the single node $x_0 = 1$.\n- **Collocation Interpolant**: $I_N v$ is the unique polynomial of degree at most $N$ with $I_N v(x_j) = v(x_j)$ for all nodes $x_j$.\n- **Problem Setup**:\n  - Base function: $u(x) = T_N(x)$.\n  - Nonlinear product: $f(x) = u(x)^2 = T_N(x)^2$.\n- **Tasks**:\n  1. Determine the exact $N$-mode modal projection coefficients, $\\{a_k\\}_{k=0}^{N}$, of $f(x)$.\n  2. Determine the collocation interpolant coefficients, $\\{\\hat{a}_k\\}_{k=0}^{N}$, by evaluating $f(x)$ at the nodes $x_j$ to get $f_j=f(x_j)$ and solving the linear system $\\sum_{k=0}^{N} \\hat{a}_k T_k(x_j) = f_j$ for $j=0,1,\\dots,N$.\n  3. Define aliasing error: $e_k = \\hat{a}_k - a_k$ for $k=0,1,\\dots,N$.\n  4. Define modal error $2$-norm: $\\|e\\|_2 = \\left( \\sum_{k=0}^{N} e_k^2 \\right)^{1/2}$.\n  5. Report the absolute error in the zeroth modal coefficient, $|e_0|$, and the modal error $2$-norm, $\\|e\\|_2$.\n- **Test Cases**: $N \\in \\{0, 1, 7, 16, 31\\}$.\n- **Output Format**: A single line containing a list of five $2$-entry lists, $[|e_0|, \\|e\\|_2]$, corresponding to the test cases.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in the field of numerical analysis, specifically concerning spectral methods for solving partial differential equations. The core of the problem is to analyze aliasing error in the computation of a nonlinear term using a collocation method.\n\n- **Scientifically Grounded**: The problem is based entirely on the well-established mathematical theory of Chebyshev polynomials and polynomial interpolation. All definitions and properties are standard and factually correct.\n- **Well-Posed**: The problem is clearly defined. The function $f(x)$ is specified, the procedures for finding the exact projection coefficients ($a_k$) and the interpolated coefficients ($\\hat{a}_k$) are unambiguous, and the required outputs are precisely defined. A unique, stable, and meaningful solution exists.\n- **Objective**: The language is precise and mathematical, free of any subjectivity or opinion.\n- **No Flaws**: The problem does not violate any of the invalidity criteria. It is self-contained, consistent, and solvable using the methods of numerical analysis and linear algebra.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n## SOLUTION\nThe problem requires us to analyze the aliasing error that occurs when computing the square of a Chebyshev polynomial, $f(x) = T_N(x)^2$, using a collocation method on the Chebyshev-Lobatto grid. We will first determine the exact modal coefficients of the projection of $f(x)$, then the coefficients of the interpolant of $f(x)$, and finally compute the error between them.\n\n**1. Exact Modal Projection Coefficients ($a_k$)**\n\nThe function is $f(x) = T_N(x)^2$. By definition, $T_N(x) = \\cos(N \\arccos x)$. Let $x = \\cos(\\theta)$, so $\\theta = \\arccos(x)$.\nThe function becomes $f(x) = (\\cos(N\\theta))^2$. Using the trigonometric power-reduction identity $\\cos^2(\\alpha) = \\frac{1}{2}(1 + \\cos(2\\alpha))$, we get:\n$$ f(x) = \\frac{1}{2}(1 + \\cos(2N\\theta)) $$\nSubstituting back $\\theta = \\arccos(x)$:\n$$ f(x) = \\frac{1}{2}(1 + \\cos(2N \\arccos x)) $$\nRecognizing the definitions $T_0(x) = 1$ and $T_{2N}(x) = \\cos(2N \\arccos x)$, we obtain the exact Chebyshev series expansion for $f(x)$:\n$$ f(x) = \\frac{1}{2} T_0(x) + \\frac{1}{2} T_{2N}(x) $$\nThe modal projection $P_N f(x)$ is the truncation of this series to degree $N$: $P_N f(x) = \\sum_{k=0}^{N} a_k T_k(x)$. We consider two cases for $N$.\n\nCase $N > 0$:\nThe term $T_{2N}(x)$ is of degree $2N$, which is greater than $N$. Therefore, it is not part of the $N$-mode projection. The projection is:\n$$ P_N f(x) = \\frac{1}{2} T_0(x) $$\nBy comparing coefficients, the exact projection coefficients are:\n$$ a_0 = \\frac{1}{2}, \\quad a_k = 0 \\quad \\text{for } k = 1, 2, \\dots, N $$\n\nCase $N = 0$:\nThe function is $f(x) = T_0(x)^2 = 1^2 = 1$. In the Chebyshev basis, this is simply $f(x) = T_0(x)$. The projection $P_0 f(x)$ is $a_0 T_0(x)$. Thus, by direct comparison:\n$$ a_0 = 1 $$\n\n**2. Collocation Interpolant Coefficients ($\\hat{a}_k$)**\n\nThe collocation interpolant $I_N f(x) = \\sum_{k=0}^{N} \\hat{a}_k T_k(x)$ is the unique polynomial of degree at most $N$ that matches $f(x)$ at the Chebyshev-Lobatto nodes $x_j = \\cos(\\frac{\\pi j}{N})$ for $j=0, \\dots, N$.\nWe evaluate $f(x)$ at these nodes:\n$$ f(x_j) = T_N(x_j)^2 = \\left( T_N\\left(\\cos\\left(\\frac{\\pi j}{N}\\right)\\right) \\right)^2 $$\nUsing the definition of $T_N(x)$:\n$$ T_N\\left(\\cos\\left(\\frac{\\pi j}{N}\\right)\\right) = \\cos\\left(N \\arccos\\left(\\cos\\left(\\frac{\\pi j}{N}\\right)\\right)\\right) = \\cos\\left(N \\frac{\\pi j}{N}\\right) = \\cos(\\pi j) = (-1)^j $$\nTherefore, the function values at the nodes are:\n$$ f_j = f(x_j) = ((-1)^j)^2 = 1 \\quad \\text{for all } j = 0, 1, \\dots, N $$\nThe interpolant $I_N f(x)$ must be a polynomial of degree at most $N$ that takes the value $1$ at all $N+1$ distinct nodes. The only such polynomial is the constant function $p(x) = 1$. In the Chebyshev basis, this is $T_0(x)$.\nSo, for any $N \\ge 0$:\n$$ I_N f(x) = 1 = T_0(x) $$\nBy comparing $I_N f(x) = \\sum_{k=0}^{N} \\hat{a}_k T_k(x)$ with $T_0(x)$, we find the interpolant coefficients by uniqueness:\n$$ \\hat{a}_0 = 1, \\quad \\hat{a}_k = 0 \\quad \\text{for } k = 1, 2, \\dots, N $$\nThis demonstrates the phenomenon of aliasing: the high-frequency component $T_{2N}(x)$ is indistinguishable from $T_0(x)$ on the grid (since $T_{2N}(x_j) = \\cos(2\\pi j) = 1 = T_0(x_j)$), and its energy is entirely aliased onto the zeroth mode.\n\n**3. Aliasing Error and Norm**\n\nThe aliasing error is $e_k = \\hat{a}_k - a_k$.\n\nCase $N > 0$:\n$e_0 = \\hat{a}_0 - a_0 = 1 - \\frac{1}{2} = \\frac{1}{2}$\n$e_k = \\hat{a}_k - a_k = 0 - 0 = 0$ for $k = 1, \\dots, N$.\nThe required metrics are:\n- Absolute error in zeroth coefficient: $|e_0| = \\frac{1}{2}$.\n- Modal error $2$-norm: $\\|e\\|_2 = \\left( \\sum_{k=0}^{N} e_k^2 \\right)^{1/2} = \\sqrt{e_0^2 + \\sum_{k=1}^{N} e_k^2} = \\sqrt{\\left(\\frac{1}{2}\\right)^2 + 0} = \\frac{1}{2}$.\n\nCase $N = 0$:\n$e_0 = \\hat{a}_0 - a_0 = 1 - 1 = 0$.\nThe required metrics are:\n- Absolute error in zeroth coefficient: $|e_0| = 0$.\n- Modal error $2$-norm: $\\|e\\|_2 = \\sqrt{e_0^2} = 0$.\n\n**Summary for Test Cases**\nFor the test suite $N \\in \\{0, 1, 7, 16, 31\\}$:\n- For $N=0$: result is $[0.0, 0.0]$.\n- For $N \\in \\{1, 7, 16, 31\\}$: result is $[0.5, 0.5]$.\n\nThe numerical implementation will construct and solve the linear system for $\\hat{a}_k$ to verify these analytical results. The system is $M \\mathbf{\\hat{a}} = \\mathbf{f}$, where $M_{jk} = T_k(x_j) = \\cos(k \\pi j / N)$ and $f_j = 1$. The solution should numerically yield $\\mathbf{\\hat{a}} \\approx [1, 0, \\dots, 0]^T$ up to machine precision.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the aliasing error for the collocation product of T_N(x)^2.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [0, 1, 7, 16, 31]\n\n    results = []\n    for N in test_cases:\n        # The problem asks to set up and solve the linear system numerically.\n        # This implementation follows that prescription.\n\n        # Handle the N=0 case separately to avoid division by zero.\n        if N == 0:\n            # Analytical result for N=0:\n            # f(x) = T_0(x)^2 = 1^2 = 1 = T_0(x).\n            # The projection is P_0 f(x) = T_0(x), so a_0 = 1.\n            # The grid is x_0=1, f(x_0)=1. The interpolant is I_0 f(x) = 1 = T_0(x), so a_hat_0 = 1.\n            # The error e_0 = a_hat_0 - a_0 = 1 - 1 = 0.\n            results.append([0.0, 0.0])\n            continue\n\n        # For N > 0:\n\n        # 1. Determine the exact N-mode modal projection coefficients, {a_k}.\n        # f(x) = T_N(x)^2 = 0.5 * T_0(x) + 0.5 * T_2N(x).\n        # The N-mode projection P_N f(x) truncates modes > N. Since N > 0, 2N > N.\n        # P_N f(x) = 0.5 * T_0(x).\n        # So, a_0 = 0.5 and a_k = 0 for k > 0.\n        a_k_exact = np.zeros(N + 1)\n        a_k_exact[0] = 0.5\n\n        # 2. Determine the collocation interpolant coefficients, {a_hat_k}.\n        # Define the Chebyshev-Lobatto grid nodes.\n        j_indices = np.arange(N + 1)\n        x_j = np.cos(np.pi * j_indices / N)\n\n        # Evaluate the function f(x) = T_N(x)^2 at the nodes.\n        # Analytically, f(x_j) = T_N(cos(pi*j/N))^2 = (cos(pi*j))^2 = 1 for all j.\n        # We compute this numerically as requested by the problem structure.\n        # Note: np.arccos(x_j) will numerically recover pi*j/N.\n        u_vals_at_nodes = np.cos(N * np.arccos(x_j))\n        f_vals_at_nodes = u_vals_at_nodes**2\n        # Due to floating point arithmetic, f_vals_at_nodes will be very close to 1.0.\n        # Using the exact analytical result f_j=1 is cleaner and avoids floating point noise\n        # in the RHS vector, which is consistent with the problem's analytical nature.\n        f_j = np.ones(N + 1)\n\n        # Construct the square linear system: sum_{k=0 to N} a_hat_k * T_k(x_j) = f_j\n        # The matrix M has entries M_jk = T_k(x_j).\n        k_indices = np.arange(N + 1)\n        # M_jk = cos(k * arccos(x_j)) = cos(k * pi*j/N)\n        M = np.cos(np.pi * np.outer(j_indices, k_indices) / N)\n\n        # Solve the system M * a_hat = f for the coefficients a_hat.\n        a_hat_k = np.linalg.solve(M, f_j)\n\n        # 3. Compute the aliasing error e_k = a_hat_k - a_k_exact.\n        e_k = a_hat_k - a_k_exact\n\n        # 4. Calculate the required metrics.\n        abs_error_mode_0 = np.abs(e_k[0])\n        modal_error_2_norm = np.linalg.norm(e_k)\n\n        results.append([abs_error_mode_0, modal_error_2_norm])\n    \n    # helper to format the output string correctly\n    def format_list(item):\n        return f\"[{item[0]},{item[1]}]\"\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(format_list, results))}]\")\n\nsolve()\n```"
        }
    ]
}