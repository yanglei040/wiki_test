## Applications and Interdisciplinary Connections

Having established the theoretical foundations and construction principles of serendipity finite elements in the preceding chapter, we now turn our attention to their practical utility. The theoretical elegance of these elements—achieving a desired order of accuracy with fewer degrees of freedom than their tensor-product Lagrangian counterparts—translates into tangible benefits across a spectrum of scientific and engineering disciplines. This chapter explores these applications, demonstrating how the core principles of serendipity formulations are leveraged to solve real-world problems, enhance computational efficiency, and enable advanced numerical strategies. We will examine their role from the foundational level of numerical implementation to their application in computational mechanics, fluid dynamics, and high-performance computing, thereby bridging the gap between abstract theory and applied computational science.

### Fundamental Implementation and Numerical Properties

The practical success of any [finite element method](@entry_id:136884) hinges on its correct and efficient implementation. For [serendipity elements](@entry_id:171371), this involves a nuanced understanding of their behavior under geometric mappings and the precise [numerical integration](@entry_id:142553) required to maintain accuracy.

#### Matrix Assembly, Scaling, and Approximation Properties

The assembly of element [mass and stiffness matrices](@entry_id:751703) is the cornerstone of [finite element analysis](@entry_id:138109). For a [serendipity element](@entry_id:754705) space on the reference domain, such as the quadratic space $S_2(\hat{K})$ on the reference square $\hat{K} = [-1,1]^2$, these matrices can be computed exactly. The element mass matrix $\hat{M}$, whose entries are $\hat{M}_{ij} = \int_{\hat{K}} \phi_i \phi_j \, d\xi d\eta$, is symmetric and positive definite, a direct consequence of the [linear independence](@entry_id:153759) of the basis functions $\phi_i$. The [element stiffness matrix](@entry_id:139369) $\hat{K}$, with entries $\hat{K}_{ij} = \int_{\hat{K}} \nabla \phi_i \cdot \nabla \phi_j \, d\xi d\eta$, is symmetric and positive semidefinite, with its [nullspace](@entry_id:171336) corresponding to the constant functions within the element space.

When these reference matrices are mapped to a physical element $K$ via an affine transformation $\mathbf{x} = B \hat{\mathbf{x}} + b$, they exhibit predictable scaling behavior. For an [anisotropic scaling](@entry_id:261477) matrix $B = \mathrm{diag}(h_x, h_y)$, the physical [mass matrix](@entry_id:177093) scales as $M_K = h_x h_y \hat{M}$, while the stiffness matrix transforms according to a combination of contributions from each coordinate direction, $K_K = \frac{h_y}{h_x} \hat{K}_x + \frac{h_x}{h_y} \hat{K}_y$, where $\hat{K}_x$ and $\hat{K}_y$ represent the stiffness contributions from derivatives in the $\xi$ and $\eta$ directions, respectively. Understanding these [scaling laws](@entry_id:139947) is crucial for pre-computation and efficient implementation in codes that handle structured or anisotropically refined meshes .

The accuracy of the isoparametric [serendipity element](@entry_id:754705) is intimately tied to the geometry of the physical element. A key property emerges when the physical element is a parallelogram. In this case, the [isoparametric mapping](@entry_id:173239) from the reference square becomes purely affine. Consequently, a polynomial function in the physical coordinates pulls back to a polynomial of the same degree in the reference coordinates. For example, if we wish to approximate a quadratic function, its pullback to the [reference element](@entry_id:168425) will be a complete quadratic polynomial in $(\xi, \eta)$. Since the quadratic serendipity space $S_2$ contains all quadratic polynomials, the Lagrange interpolant on the reference element is exact. This [exactness](@entry_id:268999) translates back to the physical domain, meaning the $S_2$ element can perfectly reproduce any [quadratic field](@entry_id:636261) on a parallelogram element . This property is a specific instance of the more general patch test, which assesses an element's ability to reproduce polynomial fields. While [serendipity elements](@entry_id:171371) pass the linear patch test on general distorted elements, their ability to pass higher-order patch tests is sensitive to geometric distortion. For instance, a 20-node serendipity hexahedron fails the quadratic patch test under a specific quadratic, one-directional distortion precisely because the composition of the physical [quadratic field](@entry_id:636261) with the nonlinear geometric map produces higher-order polynomial terms in the reference coordinates that fall outside the serendipity basis span .

#### Numerical Quadrature Requirements

The accuracy of the finite element solution depends critically on the precision with which the element matrices are integrated. This is managed by selecting a [numerical quadrature](@entry_id:136578) rule of sufficient order. For affine (parallelogram) [quadrilateral elements](@entry_id:176937), the integrand of the stiffness matrix for the serendipity space $S_r$ is a polynomial in $(\xi, \eta)$. The maximum degree of this polynomial in each variable separately can be shown to be $2r$. A tensor-product Gauss-Legendre quadrature rule with $p$ points in each direction is exact for polynomials of degree up to $2p-1$ in each variable. To ensure exact integration, we must have $2p-1 \ge 2r$, which implies the minimal integer number of points is $p = r+1$ .

When elements are non-affine (e.g., general bilinear quadrilaterals), the Jacobian of the mapping is no longer constant. The integrand for the stiffness matrix becomes a rational function, with a polynomial in the numerator and the Jacobian determinant in the denominator. To maintain the optimal [order of convergence](@entry_id:146394) predicted by [approximation theory](@entry_id:138536), Strang's first lemma requires that the [quadrature error](@entry_id:753905) be controlled. A [sufficient condition](@entry_id:276242) to achieve this is to choose a quadrature rule that exactly integrates the polynomial numerator of the integrand. For a [bilinear mapping](@entry_id:746795) and an $S_r$ [serendipity element](@entry_id:754705), this numerator can be shown to be a polynomial of degree at most $2r$ in each reference coordinate. This leads to the same requirement as in the affine case: a tensor-product rule with $m = r+1$ points per direction is sufficient to preserve the optimal $h^r$ convergence rate in the [energy norm](@entry_id:274966) . This rule, sometimes referred to as "full integration," provides a robust choice for practical implementations.

### Interdisciplinary Applications

The theoretical properties of [serendipity elements](@entry_id:171371) translate into distinct performance characteristics when applied to problems in different fields of science and engineering. The choice between a [serendipity element](@entry_id:754705) and its Lagrangian counterpart often involves a trade-off between computational cost and representational accuracy for specific physical phenomena.

#### Structural and Solid Mechanics: The Challenge of Bending

In [computational solid mechanics](@entry_id:169583), particularly in the analysis of plate and shell structures, the ability of an element to accurately represent bending modes is paramount. This is where a critical difference between the quadratic serendipity ($S_2$, often called Q8) and the biquadratic Lagrangian ($Q_2$, or Q9) elements becomes apparent. The 9-node Lagrangian element's basis is a tensor product of 1D quadratic polynomials, spanning the full biquadratic space $\mathbb{Q}_2 = \mathrm{span}\{1, \xi, \eta, \xi^2, \xi\eta, \eta^2, \xi^2\eta, \xi\eta^2, \xi^2\eta^2\}$. The 8-node [serendipity element](@entry_id:754705), by omitting the central node, lacks the ability to represent the highest-order monomial, $\xi^2\eta^2$  .

This seemingly minor omission has profound consequences for bending-dominated problems. A state of [pure bending](@entry_id:202969) or constant twist requires the displacement field to contain certain quadratic and higher-order terms. The inability of the $S_2$ element to represent the non-uniform twisting deformation associated with the $\xi^2\eta^2$ mode can lead to an artificially stiff response, a pathology known as **[shear locking](@entry_id:164115)**. This is especially problematic for thin plates and shells, where the element incorrectly resists [pure bending](@entry_id:202969) by generating spurious shear strains. This results in a significant underestimation of displacements and inaccurate stress predictions. The 9-node Lagrangian element, which spans the full $\mathbb{Q}_2$ space, can represent these modes correctly and is far less susceptible to locking . This behavior is critical in applications such as [computational geophysics](@entry_id:747618), where accurately modeling the flexure of tectonic plates or ice shelves under loading is essential. For such problems, the $Q_2$ element is often preferred, or specialized "locking-free" formulations must be employed with the $S_2$ element to achieve reliable results .

#### Computational Fluid Dynamics: Convection-Dominated Transport

In [computational fluid dynamics](@entry_id:142614) (CFD), [serendipity elements](@entry_id:171371) are often used to discretize the Navier-Stokes or [convection-diffusion](@entry_id:148742) equations. In advection-dominated regimes, standard Galerkin methods are prone to producing non-physical oscillations in the numerical solution. Stabilization techniques, such as the Streamline-Upwind/Petrov-Galerkin (SUPG) method, are required to obtain stable and accurate solutions.

The performance of both $Q_2$ and $S_2$ elements can be evaluated in this context. In diffusion-dominated scenarios (low Péclet number), both elements perform well, with the $Q_2$ element typically yielding slightly lower error due to its richer [polynomial space](@entry_id:269905). As advection becomes dominant (high Péclet number), the standard Galerkin formulation for both element types produces significant oscillations and large errors. The application of SUPG stabilization dramatically improves the solution for both, effectively damping the oscillations and reducing the error by orders of magnitude. In this stabilized regime, the performance difference between $Q_2$ and $S_2$ often diminishes, making the computationally cheaper $S_2$ element an attractive choice for large-scale CFD simulations where the cost savings from fewer interior degrees of freedom can be substantial .

#### Homogenization and Multiscale Modeling

Serendipity elements can also play a role in advanced multiscale modeling techniques, such as numerical homogenization. This theory is used to compute the effective properties of [composite materials](@entry_id:139856) with fine-scale periodic microstructures. The process typically involves solving a "cell problem" on a representative unit cell of the microstructure to determine the homogenized coefficients of a macroscopic model.

In a two-scale strategy, one might use a high-fidelity element, such as the $Q_p$ Lagrangian element, to accurately resolve the complex fields within the microscale cell problem. The resulting homogenized coefficients can then be used in a macroscopic simulation of the entire component. For this macro-scale problem, where the material properties are now smooth, a more computationally efficient element like the $S_p$ [serendipity element](@entry_id:754705) can be employed. However, this introduces a new source of error: the inability of the serendipity space to exactly represent the macroscopic solution, even if it is a simple polynomial. For example, if the exact macroscopic solution is the biquartic polynomial $u^0(x_1, x_2) = x_1^2 x_2^2$, the $S_2$ serendipity interpolant on a square domain fails to capture the $\xi^2\eta^2$ mode, leading to an [interpolation error](@entry_id:139425) of $e(x) = (x_1^2-1)(x_2^2-1)$. The magnitude of this serendipity-induced error in the [energy norm](@entry_id:274966) depends on the homogenized material properties, providing a quantitative measure of the trade-off between [computational efficiency](@entry_id:270255) at the macro-scale and the fidelity of the overall two-scale approximation .

### Advanced Computational Strategies and High-Performance Computing

The unique structure of [serendipity elements](@entry_id:171371), with their emphasis on boundary degrees of freedom, makes them particularly well-suited for several advanced computational techniques that aim to maximize efficiency and performance.

#### Hierarchical Bases and p-Refinement

For adaptive [finite element methods](@entry_id:749389), particularly the $p$-version where accuracy is improved by increasing the polynomial degree $p$ on a fixed mesh, hierarchical bases are highly advantageous. A basis is hierarchical if the basis functions for order $p$ form a subset of the basis for order $p+1$. This property ensures that the [stiffness matrix](@entry_id:178659) for order $p$ is a sub-block of the matrix for order $p+1$, allowing for efficient solution algorithms and reuse of computations.

Serendipity elements are naturally suited to a hierarchical construction. A hierarchical basis for $S_k$ can be built starting with the four bilinear vertex functions (for $k=1$). For higher orders, one adds a sequence of edge functions of increasing degree for each of the four edges. These edge functions are constructed from 1D hierarchical polynomials (e.g., integrals of Legendre polynomials) that vanish at the edge endpoints, ensuring they do not interfere with the vertex degrees of freedom. No interior (or face) functions are needed. This construction results in a basis of dimension $4k$ for the space $S_k$. The key benefits of this approach for $p$-refinement, compared to a nodal tensor-product $\mathbb{Q}_k$ basis, are:
1.  **Nestedness**: As $k$ is increased, previously computed solution coefficients associated with lower-order basis functions can be retained.
2.  **Efficiency**: The absence of interior degrees of freedom means $S_k$ has significantly fewer unknowns than $\mathbb{Q}_k$ for $k>1$ (a difference of $(k-1)^2$), leading to smaller [linear systems](@entry_id:147850).
3.  **Stability**: Using edge functions based on [orthogonal polynomials](@entry_id:146918) leads to better-conditioned stiffness matrices than high-order nodal bases, which can suffer from ill-conditioning .

#### Static Condensation

The separation of degrees of freedom into boundary (trace) and interior sets in [high-order elements](@entry_id:750303) is the key to a powerful algebraic technique known as [static condensation](@entry_id:176722). For [serendipity elements](@entry_id:171371) of order $r \ge 4$, which possess interior "bubble" functions, the corresponding interior DOFs are, by construction, not shared between elements. This allows them to be eliminated at the element level before [global assembly](@entry_id:749916).

The procedure involves partitioning the local [element stiffness matrix](@entry_id:139369) into blocks corresponding to trace and interior DOFs. The interior DOFs can then be expressed in terms of the trace DOFs and subsequently substituted back into the system, yielding a smaller, denser "Schur complement" matrix that involves only the trace DOFs. The global system is assembled using only these condensed element matrices. This significantly reduces the size of the global linear system that needs to be solved. For a mesh of $N_x \times N_y$ [serendipity elements](@entry_id:171371) of order $r$, [static condensation](@entry_id:176722) reduces the total number of unknowns by $N_x N_y \binom{r-2}{2}$. While the condensed matrix is denser, the dramatic reduction in global problem size often leads to major savings in both memory and computation time, especially for high polynomial orders .

#### Performance on Modern Hardware (GPUs)

The performance of finite element kernels on parallel architectures like Graphics Processing Units (GPUs) is often limited by memory bandwidth rather than raw arithmetic throughput. The roofline performance model provides a framework for analyzing this trade-off by considering the arithmetic intensity (FLOPs per byte of memory traffic) of a kernel.

Serendipity elements offer a compelling advantage in this context. By having fewer DOFs than their Lagrangian counterparts ($4p$ vs. $(p+1)^2$ for order $p$), an $S_p$ residual evaluation kernel requires reading and writing significantly less data from memory for the solution and residual vectors. While the total number of [floating-point operations](@entry_id:749454) (FLOPs) is also reduced, the reduction in memory traffic is often more pronounced. This leads to a higher arithmetic intensity for the $S_p$ kernel compared to the $Q_p$ kernel.

According to the [roofline model](@entry_id:163589), a higher arithmetic intensity allows the kernel to better utilize the GPU's computational power and be less constrained by [memory bandwidth](@entry_id:751847). This can result in a significant [speedup](@entry_id:636881) for the [serendipity element](@entry_id:754705) kernel, especially in [memory-bound](@entry_id:751839) regimes. The overall end-to-end solver speedup gained by using $S_p$ over $Q_p$ is a product of both the reduced computational work and the increased hardware efficiency, making [serendipity elements](@entry_id:171371) an attractive choice for high-performance, matrix-free FEM implementations on modern GPUs .

### Extension to Three Dimensions

The concepts and advantages of [serendipity elements](@entry_id:171371) extend naturally from two-dimensional quadrilaterals to three-dimensional hexahedra. The 3D analogue of the 8-node quadratic [serendipity element](@entry_id:754705) is the 20-node hexahedron, often denoted $S_2$ or H20. It is defined by 8 nodes at the vertices and 12 nodes at the midpoints of each edge, for a total of 20 DOFs. This is in contrast to the 27-node triquadratic Lagrangian hexahedron ($Q_2$ or H27), which also includes nodes at the center of each face and one at the element's [centroid](@entry_id:265015).

The 20-node serendipity basis can be constructed using the same principles as in 2D, resulting in shape functions that are quadratic along every edge. Like its 2D counterpart, the $S_2$ hexahedral space contains the [complete space](@entry_id:159932) of quadratic polynomials in three variables, $\mathcal{P}_2(\xi, \eta, \zeta)$. This ensures it passes the quadratic patch test on affine (parallelepiped) elements . The space omits the 7 modes corresponding to the face-center and volume-center nodes of the full $Q_2$ space, which include the tri-quartic term $\xi^2\eta^2\zeta^2$ and various face-[bubble functions](@entry_id:176111) . As with all standard Lagrange-type elements, the H20 [shape functions](@entry_id:141015) satisfy the partition of unity property and provide $C^0$ continuity across element boundaries. Their ability to exactly reproduce any affine displacement field ensures they pass the constant-strain patch test, a fundamental requirement for convergence in [solid mechanics](@entry_id:164042) . The computational savings offered by the 20-node element over the 27-node element, while preserving quadratic completeness, make it a popular choice for 3D analysis.

### Conclusion

Serendipity finite elements represent a powerful and efficient alternative to traditional tensor-product Lagrangian elements. By systematically omitting interior degrees of freedom while preserving [polynomial completeness](@entry_id:177462) up to a certain degree and maintaining full polynomial order along element boundaries, they strike an effective balance between accuracy and computational cost. As we have seen, this design philosophy has far-reaching implications, from the practicalities of numerical integration and matrix assembly to performance on advanced computer architectures. While their limitations, particularly in representing certain bending modes, must be carefully considered in specific applications like thin plate and shell analysis, their advantages in efficiency, suitability for $p$-refinement, and amenability to techniques like [static condensation](@entry_id:176722) make them a versatile and indispensable tool in the modern computational scientist's and engineer's toolkit.