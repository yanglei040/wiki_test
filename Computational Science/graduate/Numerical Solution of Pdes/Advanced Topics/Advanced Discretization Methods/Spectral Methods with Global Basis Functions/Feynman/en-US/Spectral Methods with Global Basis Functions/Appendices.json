{
    "hands_on_practices": [
        {
            "introduction": "The heart of spectral collocation methods is the differentiation matrix, which approximates the derivative operator on a discrete grid. This practice provides a foundational, hands-on exercise in constructing this operator for Chebyshev polynomials using the numerically stable barycentric formula. By building the first and second derivative matrices and calculating their spectral norms, you will gain essential implementation skills and crucial insights into the stability properties that govern the performance of spectral methods in solving differential equations.",
            "id": "3446562",
            "problem": "Construct a complete program that, for Chebyshev spectral collocation on the interval $[-1,1]$ with Gauss–Lobatto points, builds the first and second differentiation matrices using global basis functions and estimates their spectral norms as functions of $N$. The Gauss–Lobatto points are defined by $x_j = \\cos\\left(\\pi j / N\\right)$ for $j = 0,1,\\dots,N$, and the Chebyshev first differentiation matrix $D^{(1)} \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ is defined by $D^{(1)}_{j,k} = \\ell_k'(x_j)$, where $\\ell_k$ are the Lagrange basis polynomials at the nodes $\\{x_j\\}_{j=0}^N$. Use the barycentric formulation to derive and implement $D^{(1)}$ via weights $w_j$ and the identity $D^{(1)}_{j,k} = \\frac{w_k}{w_j}\\frac{1}{x_j - x_k}$ for $j \\neq k$, with $D^{(1)}_{j,j} = -\\sum_{k\\neq j} D^{(1)}_{j,k}$. For Chebyshev Gauss–Lobatto points, choose barycentric weights consistent with the Chebyshev first kind polynomial grid. Then construct the second differentiation matrix as $D^{(2)} = D^{(1)}D^{(1)}$, representing application of the first derivative twice to the polynomial interpolant.\n\nThe spectral norm of a matrix $A$, denoted $\\|A\\|_2$, is the operator norm induced by the Euclidean norm and equals the largest singular value of $A$. You must compute $\\|D^{(1)}\\|_2$ and $\\|D^{(2)}\\|_2$ using Singular Value Decomposition (SVD).\n\nStart your derivation from fundamental definitions of polynomial interpolation, Lagrange basis functions, and the barycentric representation of derivatives. Avoid using prepackaged formulas that skip these derivations. Clearly justify why each step is correct and how it follows from the base principles of spectral collocation with global basis functions.\n\nImplement the algorithm with the following test suite of parameter values:\n- $N \\in \\{1,2,8,32,128\\}$.\n\nFor each $N$ in the test suite, return a pair of floating-point values $[\\|D^{(1)}\\|_2,\\|D^{(2)}\\|_2]$ rounded to $10$ decimal places. The final output must aggregate the results for all provided test cases into a single line. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[[r_1^{(1)},r_1^{(2)}],[r_2^{(1)},r_2^{(2)}],\\dots]$), where each inner pair corresponds to a single value of $N$ in the specified order $[1,2,8,32,128]$ and each $r_i^{(\\cdot)}$ is a float rounded to $10$ decimal places. No physical units or angle units are required in this problem, and all outputs must be dimensionless real numbers.",
            "solution": "The problem requires the construction of Chebyshev first and second-order differentiation matrices, $D^{(1)}$ and $D^{(2)}$, on the Gauss-Lobatto grid, and the computation of their spectral norms. The derivation must begin from fundamental principles of polynomial interpolation.\n\n### 1. Polynomial Interpolation and Differentiation Matrices\nLet a function $u(x)$ be defined on the interval $[-1, 1]$. We approximate $u(x)$ by a unique polynomial $p(x)$ of degree at most $N$ that interpolates $u(x)$ at $N+1$ distinct collocation points $\\{x_j\\}_{j=0}^N$. This interpolating polynomial can be expressed in the Lagrange form:\n$$\np(x) = \\sum_{k=0}^{N} u_k \\ell_k(x)\n$$\nwhere $u_k = u(x_k)$ are the function values at the collocation points, and $\\ell_k(x)$ are the Lagrange basis polynomials. These are defined by the property $\\ell_k(x_j) = \\delta_{kj}$, where $\\delta_{kj}$ is the Kronecker delta.\n\nThe derivative of the interpolating polynomial $p(x)$ is given by:\n$$\np'(x) = \\sum_{k=0}^{N} u_k \\ell'_k(x)\n$$\nTo find the derivative of the polynomial at the collocation points, we evaluate $p'(x)$ at each $x_j$:\n$$\np'(x_j) = \\sum_{k=0}^{N} u_k \\ell'_k(x_j)\n$$\nThis equation can be expressed as a matrix-vector product. Let $\\mathbf{u} = [u_0, u_1, \\dots, u_N]^T$ be the vector of function values at the grid points, and $\\mathbf{u'} = [p'(x_0), p'(x_1), \\dots, p'(x_N)]^T$ be the vector of derivative values. Then, $\\mathbf{u'} = D^{(1)} \\mathbf{u}$, where $D^{(1)}$ is the first differentiation matrix with entries:\n$$\nD^{(1)}_{j,k} = \\ell'_k(x_j)\n$$\nThis is the fundamental definition of the spectral differentiation matrix.\n\n### 2. Barycentric Formulation for Numerical Stability\nThe direct evaluation of $\\ell'_k(x_j)$ from the standard Lagrange formula $\\ell_k(x) = \\prod_{i \\neq k} \\frac{x-x_i}{x_k-x_i}$ is numerically unstable. A more robust approach is the barycentric formulation.\n\nLet $L(x)$ be the nodal polynomial, defined as $L(x) = \\prod_{i=0}^{N}(x-x_i)$. The Lagrange basis polynomial $\\ell_k(x)$ can be written as:\n$$\n\\ell_k(x) = \\frac{L(x)}{(x-x_k)L'(x_k)}\n$$\nWe define the barycentric weights $w_k$ as:\n$$\nw_k = \\frac{1}{L'(x_k)} = \\frac{1}{\\prod_{i \\neq k}(x_k-x_i)}\n$$\nUsing these weights, the Lagrange polynomial becomes $\\ell_k(x) = w_k \\frac{L(x)}{x-x_k}$.\n\n### 3. Derivation of Differentiation Matrix Entries\n\n**Off-Diagonal Entries ($j \\neq k$):**\nTo find the entry $D^{(1)}_{j,k} = \\ell'_k(x_j)$ for $j \\neq k$, we differentiate the barycentric form of $\\ell_k(x)$:\n$$\n\\ell'_k(x) = w_k \\left( \\frac{L'(x)(x-x_k) - L(x)}{(x-x_k)^2} \\right)\n$$\nNow, we evaluate this expression at $x=x_j$. Since $x_j$ is a root of the nodal polynomial $L(x)$ for any $j$, we have $L(x_j)=0$.\n$$\n\\ell'_k(x_j) = w_k \\left( \\frac{L'(x_j)(x_j-x_k) - 0}{(x_j-x_k)^2} \\right) = w_k \\frac{L'(x_j)}{x_j-x_k}\n$$\nFrom the definition of the barycentric weight $w_j = 1/L'(x_j)$, we have $L'(x_j) = 1/w_j$. Substituting this into the expression for $\\ell'_k(x_j)$ yields the formula for the off-diagonal entries of $D^{(1)}$:\n$$\nD^{(1)}_{j,k} = \\frac{w_k}{w_j} \\frac{1}{x_j-x_k} \\quad \\text{for } j \\neq k\n$$\n\n**Diagonal Entries ($j = k$):**\nTo find the diagonal entries $D^{(1)}_{j,j}$, we use the property that the differentiation matrix must exactly differentiate constant functions. Let $u(x) = c$ be a constant function. This is a polynomial of degree $0$, which can be exactly represented by our interpolant for $N \\ge 0$. The vector of function values is $\\mathbf{u} = [c, c, \\dots, c]^T$. The derivative is $u'(x) = 0$, so the vector of derivative values must be $\\mathbf{u'} = \\mathbf{0}$.\nFrom $\\mathbf{u'} = D^{(1)}\\mathbf{u}$, we have $D^{(1)}[c, \\dots, c]^T = [0, \\dots, 0]^T$. This implies that the sum of each row of the differentiation matrix must be zero:\n$$\n\\sum_{k=0}^{N} D^{(1)}_{j,k} = 0 \\quad \\text{for each } j = 0, 1, \\dots, N\n$$\nFrom this, we can solve for the diagonal entry $D^{(1)}_{j,j}$:\n$$\nD^{(1)}_{j,j} = - \\sum_{k \\neq j} D^{(1)}_{j,k}\n$$\nThis derivation confirms the formulas provided in the problem statement.\n\n### 4. Grid Points and Barycentric Weights\nThe problem specifies Chebyshev-Gauss-Lobatto points, which are the extrema of the $N$-th degree Chebyshev polynomial of the first kind, $T_N(x)$. These points are given by:\n$$\nx_j = \\cos\\left(\\frac{\\pi j}{N}\\right) \\quad \\text{for } j = 0, 1, \\dots, N\n$$\nFor this specific set of points, the barycentric weights are known to be:\n$$\nw_j = (-1)^j \\delta_j\n$$\nwhere $\\delta_j = 1/2$ for $j=0$ and $j=N$, and $\\delta_j=1$ for $j=1, 2, \\dots, N-1$. Any constant multiple of these weights is also valid, as the constant cancels in the formula for $D^{(1)}_{j,k}$. We choose this specific form for convenience:\n$$\nw_0 = \\frac{1}{2}, \\quad w_j = (-1)^j \\text{ for } 1 \\le j \\le N-1, \\quad w_N = \\frac{(-1)^N}{2}\n$$\n\n### 5. Second Differentiation Matrix\nThe second differentiation matrix, $D^{(2)}$, represents the application of the first derivative operator twice. On the space of polynomials of degree at most $N$, this is equivalent to the matrix square of the first differentiation matrix:\n$$\nD^{(2)} = (D^{(1)})^2 = D^{(1)}D^{(1)}\n$$\nWe compute $D^{(2)}$ by performing matrix multiplication on the constructed $D^{(1)}$.\n\n### 6. Spectral Norm Calculation\nThe spectral norm of a matrix $A$, denoted $\\|A\\|_2$, is defined as the largest singular value of $A$, $\\sigma_{\\max}(A)$. The singular values of $A$ are the square roots of the eigenvalues of the positive-semidefinite matrix $A^H A$ (or $A^T A$ for real matrices). We compute the singular values of $D^{(1)}$ and $D^{(2)}$ using a standard Singular Value Decomposition (SVD) algorithm and take the maximum value in each case to find the spectral norm.\n\n### Algorithm Summary\nFor each given value of $N$:\n1.  Generate the $N+1$ Chebyshev-Gauss-Lobatto points $x_j = \\cos(\\pi j / N)$.\n2.  Generate the corresponding $N+1$ barycentric weights $w_j = (-1)^j \\delta_j$.\n3.  Construct the $(N+1) \\times (N+1)$ matrix $D^{(1)}$:\n    a. For $j \\neq k$, compute $D^{(1)}_{j,k} = \\frac{w_k}{w_j}\\frac{1}{x_j - x_k}$.\n    b. For the diagonal, compute $D^{(1)}_{j,j} = - \\sum_{k \\neq j} D^{(1)}_{j,k}$.\n4.  Compute $D^{(2)} = D^{(1)} @ D^{(1)}$.\n5.  Compute the singular values of $D^{(1)}$ and find the maximum to obtain $\\|D^{(1)}\\|_2$.\n6.  Compute the singular values of $D^{(2)}$ and find the maximum to obtain $\\|D^{(2)}\\|_2$.\n7.  Store the pair $[\\left\\|D^{(1)}\\right\\|_2, \\left\\|D^{(2)}\\right\\|_2]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries outside the Python standard library, numpy, or scipy are permitted.\n# Note: scipy is permitted but not necessary for this solution.\n\ndef build_chebyshev_diff_matrices(N):\n    \"\"\"\n    Constructs the Chebyshev first and second differentiation matrices.\n\n    Args:\n        N (int): The degree of the polynomial interpolant. The grid will have N+1 points.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing the first (D1) and second (D2)\n                                       differentiation matrices.\n    \"\"\"\n    if N == 0:\n        return np.array([[0.]]), np.array([[0.]])\n    \n    # Grid points (Chebyshev-Gauss-Lobatto)\n    j = np.arange(N + 1)\n    x = np.cos(np.pi * j / N)\n\n    # Barycentric weights\n    w = (-1.0)**j\n    w[0] *= 0.5\n    w[-1] *= 0.5\n\n    # Construct the first differentiation matrix D1\n    N_plus_1 = N + 1\n    D1 = np.zeros((N_plus_1, N_plus_1))\n    \n    # Off-diagonal entries using broadcasting\n    # w_k / w_j term\n    w_ratio = w[np.newaxis, :] / w[:, np.newaxis]\n    # x_j - x_k term\n    x_diff = x[:, np.newaxis] - x[np.newaxis, :]\n\n    # To avoid division by zero on the diagonal, we temporarily set diagonal of x_diff to 1.\n    # The diagonal of D1 will be correctly computed later.\n    np.fill_diagonal(x_diff, 1)\n    D1 = w_ratio / x_diff\n    \n    # Correct the diagonal of D1, which was filled with temporary values.\n    # We set it to 0 before summing the rows.\n    np.fill_diagonal(D1, 0)\n    \n    # Diagonal entries: D1_jj = -sum(D1_jk for k!=j)\n    # The sum of each row must be zero.\n    row_sums = np.sum(D1, axis=1)\n    np.fill_diagonal(D1, -row_sums)\n    \n    # Second differentiation matrix\n    D2 = D1 @ D1\n\n    return D1, D2\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [1, 2, 8, 32, 128]\n\n    results = []\n    for N in test_cases:\n        # Build the differentiation matrices for the current N.\n        D1, D2 = build_chebyshev_diff_matrices(N)\n        \n        # Compute the spectral norm (largest singular value) for D1.\n        # np.linalg.svd returns singular values in descending order.\n        # Using compute_uv=False is more efficient as we only need singular values.\n        s1 = np.linalg.svd(D1, compute_uv=False)\n        norm_D1 = s1[0] if len(s1) > 0 else 0.0\n\n        # Compute the spectral norm for D2.\n        s2 = np.linalg.svd(D2, compute_uv=False)\n        norm_D2 = s2[0] if len(s2) > 0 else 0.0\n\n        # Store the rounded results.\n        results.append((norm_D1, norm_D2))\n\n    # Format the final output string exactly as required.\n    # Each float must be formatted to 10 decimal places.\n    output_pairs = []\n    for res_pair in results:\n        norm1_str = f\"{res_pair[0]:.10f}\"\n        norm2_str = f\"{res_pair[1]:.10f}\"\n        output_pairs.append(f\"[{norm1_str},{norm2_str}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(output_pairs)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With the ability to construct differentiation matrices, we can now assemble and solve differential equations. This exercise tackles a simple ordinary differential equation (ODE) but introduces a critical practical challenge: how to incorporate an interior value constraint into the algebraic system. You will implement and compare two powerful techniques—basis modification and the Tau method—to understand their distinct effects on solution accuracy and the numerical conditioning of the linear system, revealing the trade-offs inherent in different constraint enforcement strategies.",
            "id": "3446566",
            "problem": "Consider the linear ordinary differential equation (ODE) $u^{\\prime}(x) = g(x)$ on the interval $[-1,1]$ with the interior constraint $u(0) = 0$. Let the unknown $u(x)$ be approximated by a global Chebyshev series of the first kind of polynomial degree $N$, namely $u_N(x) = \\sum_{k=0}^{N} a_k T_k(x)$, where $T_k(x)$ denotes the $k$-th Chebyshev polynomial of the first kind. The right-hand side is specified by $g(x) = e^x + \\sin(3x)$. The exact solution consistent with the constraint is $u(x) = \\int_{0}^{x} g(s)\\, ds = e^x - 1 + \\frac{1 - \\cos(3x)}{3}$.\n\nYour task is to compare two spectral formulations in the Chebyshev basis for enforcing the interior constraint, focusing on both accuracy and linear system conditioning:\n\n1. Basis modification by dropping the constant mode: eliminate the constant coefficient $a_0$ from the unknowns and solve a square reduced system for the remaining coefficients using the derivative equation $u^{\\prime}(x) = g(x)$ in Chebyshev coefficient space. After solving, set $a_0 = 0$. This approach does not use the interior constraint directly and relies on removing the nullspace associated with constants.\n\n2. Tau correction by constraint replacement: form the square system in Chebyshev coefficient space that equates the Chebyshev coefficients of $u^{\\prime}(x)$ with those of $g(x)$, then replace one equation (one row) with the linear constraint $u(0) = 0$, which can be written in coefficient space as $\\sum_{k=0}^{N} a_k T_k(0) = 0$, where $T_k(0) = \\cos(k \\pi / 2)$.\n\nUse the following foundational facts and definitions to design your algorithm from first principles:\n\n- Chebyshev polynomials of the first kind satisfy $T_k(\\cos \\theta) = \\cos(k \\theta)$.\n- A global Chebyshev series approximation of degree $N$ is $u_N(x) = \\sum_{k=0}^{N} a_k T_k(x)$.\n- Differentiation in coefficient space is linear: if $a = (a_0,\\dots,a_N)^{\\top}$ are the coefficients of $u$, there exists a matrix $M \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ such that the Chebyshev coefficients $b = (b_0,\\dots,b_N)^{\\top}$ of $u^{\\prime}$ satisfy $b = M a$. The operator $M$ has a one-dimensional nullspace corresponding to constants.\n- The interior constraint is linear in coefficients: $u(0) = \\sum_{k=0}^{N} a_k \\cos(k \\pi/2) = 0$.\n- Discrete Cosine Transform (DCT) of type I implements the discrete orthogonality of $T_k$ on the Chebyshev–Lobatto grid $x_j = \\cos(\\pi j/N)$, $j = 0,\\dots,N$, allowing conversion between function values and Chebyshev coefficients. If $y_j = f(x_j)$, then the Chebyshev coefficients $(\\hat{f}_k)_{k=0}^{N}$ of the degree-$N$ interpolant satisfy $\\hat{f} = c/N$ with end corrections $\\hat{f}_0 \\leftarrow \\hat{f}_0/2$, $\\hat{f}_N \\leftarrow \\hat{f}_N/2$, where $c$ is the DCT-I of $y$.\n\nAlgorithmic requirements:\n\n- Construct the coefficient-space differentiation operator $M$ by applying the exact recurrence for Chebyshev coefficients of derivatives to basis vectors:\n  - If $u(x) = \\sum_{k=0}^{N} a_k T_k(x)$, define $b = (b_k)_{k=0}^{N}$ by the standard differentiation recurrence for Chebyshev series, which yields a linear mapping $b = M a$. Implement this mapping and assemble $M$ by applying it to each canonical basis vector in $\\mathbb{R}^{N+1}$.\n- Obtain Chebyshev coefficients of $g$ by sampling $g$ at Chebyshev–Lobatto points and using the Discrete Cosine Transform (DCT) of type I to convert values to coefficients (as above).\n- Method 1 (basis modification): remove the column corresponding to $a_0$ from $M$ to obtain $M_{\\text{red}} \\in \\mathbb{R}^{(N+1)\\times N}$. To obtain a square system, remove one equation row (choose the last row) to get $\\tilde{M}_{\\text{red}} \\in \\mathbb{R}^{N \\times N}$ and match the first $N$ resulting equations with the corresponding entries of the $g$-coefficient vector. Solve for $(a_1,\\dots,a_N)$ and set $a_0 = 0$.\n- Method 2 (tau correction): form $M$ and replace one equation row (choose the last row) with the constraint $u(0) = 0$ written in coefficient space as $c^{\\top} a = 0$, where $c_k = \\cos(k \\pi/2)$. Solve the resulting square system for $(a_0,\\dots,a_N)$.\n\nAccuracy and conditioning metrics:\n\n- For each method, reconstruct the approximate solution on the Chebyshev–Lobatto grid using the Chebyshev series and compute the maximum absolute error $E_{\\infty} = \\max_{0 \\leq j \\leq N} |u_N(x_j) - u(x_j)|$, where $u(x)$ is the exact solution given above.\n- For each linear system, report the $2$-norm condition number $\\kappa_2(A)$, where $A$ is the square matrix actually solved in that method.\n\nAngle units must be in radians for all trigonometric functions.\n\nYour program must implement both methods and produce results for the following test suite of polynomial degrees:\n- $N = 8$ (coarse resolution),\n- $N = 16$ (happy path),\n- $N = 33$ (odd degree, no grid point at $x=0$),\n- $N = 64$ (finer resolution).\n\nFor each $N$ in the order above, compute and output the four-tuple $[E_{\\infty}^{\\text{basis}}, E_{\\infty}^{\\text{tau}}, \\kappa_2^{\\text{basis}}, \\kappa_2^{\\text{tau}}]$, where the errors are floats and the condition numbers are floats. Aggregate all results into a single flat list in the order specified by the test suite, and print a single line containing this list in a comma-separated format enclosed in square brackets, for example, $[r_1,r_2,\\dots]$ with no extra text. No physical units are involved, and all angles must be in radians.",
            "solution": "The provided problem is a valid exercise in the numerical solution of ordinary differential equations using spectral methods. It directs a comparison between two distinct formulations for enforcing an interior point constraint on a first-order ODE. The problem is well-posed, scientifically grounded in the principles of numerical analysis and approximation theory, and all required components for its solution are explicitly defined.\n\nThe core of the problem is to solve the linear ordinary differential equation $u^{\\prime}(x) = g(x)$ on the domain $x \\in [-1, 1]$ with the interior value constraint $u(0) = 0$. The right-hand side is given by $g(x) = e^x + \\sin(3x)$. The analytical solution, which satisfies both the differential equation and the constraint, is $u(x) = e^x - 1 + \\frac{1 - \\cos(3x)}{3}$.\n\nWe employ a spectral method based on a global polynomial approximation. The unknown function $u(x)$ is approximated by a truncated series of Chebyshev polynomials of the first kind, $u_N(x) = \\sum_{k=0}^{N} a_k T_k(x)$, where $a_k$ are the unknown spectral coefficients.\n\n**Spectral Discretization**\n\nThe first step is to transform the continuous differential equation into a system of algebraic equations for the coefficients $a_k$. The differentiation operator $\\frac{d}{dx}$ acts as a linear operator on the space of polynomials. If the vector $a = [a_0, a_1, \\dots, a_N]^\\top$ contains the Chebyshev coefficients of $u_N(x)$, then the coefficients of its derivative, $u_N^{\\prime}(x) = \\sum_{k=0}^{N} b_k T_k(x)$, are given by a linear transformation $b = Ma$. Here, $M$ is the $(N+1) \\times (N+1)$ Chebyshev differentiation matrix in coefficient space. The entries of $M$ are derived from the recurrence relations for the coefficients of the derivative of a Chebyshev series. Specifically, the $j$-th column of $M$ consists of the Chebyshev coefficients of the derivative of the $j$-th basis function, $T_j^{\\prime}(x)$.\n\nThe right-hand side function $g(x)$ is also represented in the Chebyshev basis with coefficients $\\hat{g} = [\\hat{g}_0, \\hat{g}_1, \\dots, \\hat{g}_N]^\\top$. These coefficients are efficiently computed by sampling $g(x)$ at the $N+1$ Chebyshev-Lobatto points, $x_j = \\cos(\\pi j / N)$ for $j = 0, \\dots, N$, and then applying a scaled Discrete Cosine Transform of Type I (DCT-I), as specified in the problem statement.\n\nThe discretized form of the differential equation is the linear system $Ma = \\hat{g}$. However, the differentiation operator has a one-dimensional nullspace corresponding to constant functions (since the derivative of a constant is zero). Consequently, the matrix $M$ is singular, and the system $Ma = \\hat{g}$ does not have a unique solution. The interior constraint $u(0)=0$ is required to select the unique, correct solution from the family of possible solutions $u(x) = C + \\int_0^x g(s) ds$.\n\nWe compare two methods for incorporating this constraint.\n\n**Method 1: Basis Modification**\n\nThis method enforces uniqueness by modifying the approximation space itself. It seeks a solution of the form $u_N(x) = \\sum_{k=1}^{N} a_k T_k(x)$, which is equivalent to forcing the coefficient $a_0$ to be zero. The coefficient $a_0$ is proportional to the weighted average of the function, $a_0 \\propto \\int_{-1}^1 u(x) (1-x^2)^{-1/2} dx$. Thus, setting $a_0=0$ enforces an integral constraint, not the point constraint $u(0)=0$. This method is therefore expected to produce a solution that is fundamentally incorrect for this problem, leading to a large error.\n\nAlgorithmically, this is implemented as follows:\n1.  The column of $M$ corresponding to $a_0$ is removed, yielding a rectangular matrix $M_{\\text{red}} \\in \\mathbb{R}^{(N+1)\\times N}$. The system becomes $M_{\\text{red}} [a_1, \\dots, a_N]^\\top = \\hat{g}$.\n2.  This is an overdetermined system with $N+1$ equations for $N$ unknowns. To create a square system, we discard one equation. Following the problem's directive, we remove the last equation (corresponding to the highest-frequency mode $T_N$). This yields a square system $\\tilde{M}_{\\text{red}} [a_1, \\dots, a_N]^\\top = [\\hat{g}_0, \\dots, \\hat{g}_{N-1}]^\\top$, where $\\tilde{M}_{\\text{red}} \\in \\mathbb{R}^{N\\times N}$.\n3.  This system is solved for the coefficients $[a_1, \\dots, a_N]^\\top$. The full coefficient vector is then assembled with $a_0=0$.\n\n**Method 2: Tau Correction**\n\nThis method, a variant of the Tau method, works with the full approximation space $u_N(x) = \\sum_{k=0}^{N} a_k T_k(x)$ and incorporates the constraint directly into the algebraic system.\n1.  Start with the full, singular system $Ma = \\hat{g}$.\n2.  The interior constraint $u(0)=0$ is a linear equation on the coefficients: $\\sum_{k=0}^{N} a_k T_k(0) = 0$. Since $T_k(0) = \\cos(k\\pi/2)$, this equation is $\\sum_{k=0}^{N} a_k \\cos(k\\pi/2) = 0$.\n3.  To make the system non-singular and incorporate the constraint, one of the original equations from $Ma = \\hat{g}$ is replaced by the constraint equation. The choice of which equation to replace is a key aspect of the Tau method; a common and effective choice is to replace the equation corresponding to the highest-frequency mode, as its contribution to the overall accuracy is typically smallest. Following the problem, we replace the last row of the system $(Ma)_N = \\hat{g}_N$ with the constraint row.\n4.  This results in a new square, non-singular system $A_{\\text{tau}} a = \\hat{g}_{\\text{tau}}$, which is solved for the full coefficient vector $a$. This method correctly enforces the specified constraint and is expected to be highly accurate.\n\n**Comparison Metrics**\n\nThe two methods are evaluated based on:\n1.  **Accuracy**: The maximum absolute error, $E_{\\infty} = \\max_{j} |u_N(x_j) - u(x_j)|$, evaluated on the Chebyshev-Lobatto grid.\n2.  **Conditioning**: The $2$-norm condition number, $\\kappa_2(A)$, of the final linear system matrix $A$ that is solved in each method. A large condition number indicates sensitivity to perturbations and potential numerical instability.\n\nThe implementation will proceed by constructing the necessary matrices and vectors for each specified polynomial degree $N$ and solving the corresponding linear systems to find the errors and condition numbers.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.fft import dct\n\ndef solve():\n    \"\"\"\n    Implements and compares two spectral methods for solving u'(x) = g(x) with u(0)=0.\n    1. Basis modification: assumes a_0=0, solves a reduced system.\n    2. Tau correction: replaces the highest-frequency equation with the constraint u(0)=0.\n    \"\"\"\n\n    def g_func(x):\n        \"\"\"The right-hand side function of the ODE.\"\"\"\n        return np.exp(x) + np.sin(3 * x)\n\n    def u_exact(x):\n        \"\"\"The exact solution of the ODE with the given constraint.\"\"\"\n        return np.exp(x) - 1.0 + (1.0 - np.cos(3 * x)) / 3.0\n\n    def get_cheb_diff_matrix(N):\n        \"\"\"\n        Constructs the (N+1)x(N+1) Chebyshev differentiation matrix M.\n        The j-th column of M contains the Chebyshev coefficients of T_j'(x).\n        \"\"\"\n        M = np.zeros((N + 1, N + 1))\n        # Apply differentiation to each basis vector T_j(x)\n        for j in range(N + 1):\n            a = np.zeros(N + 1)\n            a[j] = 1.0  # Coefficients of T_j(x)\n            \n            # Compute coefficients of the derivative using the standard recurrence.\n            # If u(x) = sum(a_k T_k(x)), u'(x) = sum(b_k T_k(x)) where\n            # b_k = (2/c_k) * sum_{p=k+1, p-k odd}^N p * a_p with c_0=2, c_k=1 else.\n            b = np.zeros(N + 1)\n            for k in range(N - 1, -1, -1):\n                s = 0.0\n                for p in range(k + 1, N + 1, 2):\n                    s += p * a[p]\n                \n                ck = 2.0 if k == 0 else 1.0\n                b[k] = (2.0 / ck) * s\n            M[:, j] = b\n        return M\n\n    def get_g_coeffs(N):\n        \"\"\"\n        Computes Chebyshev coefficients of g(x) on an (N+1)-point Lobatto grid\n        using the specified DCT-I based algorithm.\n        \"\"\"\n        x = np.cos(np.pi * np.arange(N + 1) / N)\n        y = g_func(x)\n        \n        # Following the recipe in the problem statement\n        c = dct(y, type=1)\n        ghat = c / N\n        ghat[0] /= 2.0\n        ghat[N] /= 2.0\n        return ghat\n\n    def reconstruct_from_coeffs(N, a):\n        \"\"\"\n        Reconstructs function values on the Lobatto grid from Chebyshev coefficients\n        using direct summation: u(x_j) = sum_k a_k T_k(x_j).\n        \"\"\"\n        j_indices = np.arange(N + 1).reshape(-1, 1)\n        k_indices = np.arange(N + 1)\n        # T_k(x_j) = cos(k*pi*j/N)\n        eval_matrix = np.cos(np.pi * j_indices * k_indices / N)\n        return eval_matrix @ a\n\n    test_cases = [8, 16, 33, 64]\n    all_results = []\n\n    for N in test_cases:\n        # --- Common Setup for degree N ---\n        lobatto_points = np.cos(np.pi * np.arange(N + 1) / N)\n        u_exact_vals = u_exact(lobatto_points)\n        \n        M = get_cheb_diff_matrix(N)\n        ghat = get_g_coeffs(N)\n\n        # --- Method 1: Basis Modification ---\n        # System for [a_1, ..., a_N] after removing a_0\n        M_red = M[:, 1:]  # Shape: (N+1) x N\n        # Make square by removing last equation row\n        tilde_M_red = M[:N, 1:] # Shape: N x N\n        g_red = ghat[:N] # Corresponding RHS\n\n        kappa_basis = np.linalg.cond(tilde_M_red, 2)\n        try:\n            a_tail = np.linalg.solve(tilde_M_red, g_red)\n            a_basis = np.concatenate(([0.0], a_tail))\n            u_basis_vals = reconstruct_from_coeffs(N, a_basis)\n            E_inf_basis = np.max(np.abs(u_basis_vals - u_exact_vals))\n        except np.linalg.LinAlgError:\n            E_inf_basis = np.inf\n            kappa_basis = np.inf\n\n\n        # --- Method 2: Tau Correction ---\n        A_tau = M.copy()\n        g_tau = ghat.copy()\n        \n        # Constraint row: c^T a = 0, where c_k = T_k(0) = cos(k*pi/2)\n        constraint_row = np.cos(np.pi * np.arange(N + 1) / 2.0)\n        \n        # Replace last row of system with the constraint\n        A_tau[-1, :] = constraint_row\n        g_tau[-1] = 0.0\n        \n        kappa_tau = np.linalg.cond(A_tau, 2)\n        try:\n            a_tau = np.linalg.solve(A_tau, g_tau)\n            u_tau_vals = reconstruct_from_coeffs(N, a_tau)\n            E_inf_tau = np.max(np.abs(u_tau_vals - u_exact_vals))\n        except np.linalg.LinAlgError:\n            E_inf_tau = np.inf\n            kappa_tau = np.inf\n\n        all_results.extend([E_inf_basis, E_inf_tau, kappa_basis, kappa_tau])\n            \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The true power of spectral methods is most evident in multi-dimensional problems, where their efficiency can far surpass that of local methods like finite differences. This problem explores the design of a \"fast direct solver\" for a 2D partial differential equation by combining Fourier series for a periodic direction with Chebyshev polynomials for a bounded direction. Rather than building the full solver, your task is to analyze its computational complexity, a crucial step in understanding how this hybrid approach achieves high performance and how spectral methods scale to tackle large scientific computing challenges.",
            "id": "3446498",
            "problem": "Consider the two-dimensional screened Poisson equation posed on a strip that is periodic in one direction and non-periodic in the other direction, with homogeneous Dirichlet boundary conditions on the non-periodic boundaries. Specifically, let $u(x,y)$ satisfy\n$$\n\\frac{\\partial^2 u}{\\partial x^2}(x,y) + \\frac{\\partial^2 u}{\\partial y^2}(x,y) - \\alpha\\,u(x,y) = f(x,y),\n$$\nfor $(x,y) \\in [0,2\\pi) \\times [-1,1]$, with $u(x,-1) = 0$ and $u(x,1) = 0$ for all $x$, and periodicity in $x$ with period $2\\pi$. Assume $\\alpha > 0$ is a fixed constant.\n\nDesign a fast direct solver that uses the Fast Fourier Transform (FFT) in the periodic $x$-direction and a banded solver in the Chebyshev global-basis direction for $y$, under the following principles and constraints:\n\n- In the periodic direction, expand in global Fourier basis functions and diagonalize the second derivative $\\frac{\\partial^2}{\\partial x^2}$ by transforming to Fourier space, thereby reducing the partial differential equation to a set of decoupled ordinary differential equations in $y$ indexed by Fourier wavenumber.\n- In the non-periodic direction, discretize using global Chebyshev polynomials of the first kind $T_n(y)$ on the $y$-domain $[-1,1]$ with Chebyshev–Gauss–Lobatto points, and enforce homogeneous Dirichlet boundary conditions. Use a representation and algorithmic design that leads to a banded linear system for each Fourier mode with bandwidth independent of the truncation size, so that a banded direct solver can be applied with complexity linear in the number of unknowns per mode.\n\nYou must start from the following fundamental base:\n- The Fourier series representation for functions periodic on $[0,2\\pi)$ and the property that the Fourier transform diagonalizes constant-coefficient linear differential operators in the periodic direction.\n- The global Chebyshev polynomial basis on $[-1,1]$, its collocation points $y_j = \\cos\\left(\\frac{\\pi j}{N_y-1}\\right)$ for $j=0,1,\\ldots,N_y-1$, and the principle that appropriately chosen ultraspherical or equivalent formulations can yield banded operators for constant-coefficient differential equations when expressed in coefficient space for Chebyshev-like bases.\n- The well-tested complexity facts: the Fast Fourier Transform on $N$ points requires $\\mathcal{O}(N \\log N)$ arithmetic operations, and direct Gaussian elimination on a banded linear system of size $M$ with fixed upper and lower bandwidths $\\beta_u$ and $\\beta_\\ell$ requires $\\mathcal{O}\\!\\left(M(\\beta_u+\\beta_\\ell)^2\\right)$ arithmetic operations.\n\nYour implementation task is to write a program that, given truncation sizes $N_x$ for the periodic Fourier direction and $N_y$ for the non-periodic Chebyshev direction, predicts the normalized arithmetic operation count of the designed fast direct solver, in terms of $N_x$ and $N_y$. You must use the following normalized complexity model:\n- The FFT stage across the $x$-direction for all $N_y$ collocation rows costs $C_{\\mathrm{FFT}}(N_x,N_y) = N_x\\,N_y\\,\\log_2(N_x)$ in normalized units.\n- The banded direct solve stage across all $N_x$ Fourier modes, each producing an $N_y$-by-$N_y$ banded system in the Chebyshev coefficient space, costs $C_{\\mathrm{band}}(N_x,N_y) = N_x\\,N_y\\,(\\beta_u+\\beta_\\ell)^2$ in normalized units, where $(\\beta_u+\\beta_\\ell)$ is the fixed total half-bandwidth of the Chebyshev-direction operator that results from the chosen global-basis banded formulation.\n- The total normalized arithmetic operation count is then $C_{\\mathrm{total}}(N_x,N_y) = C_{\\mathrm{FFT}}(N_x,N_y) + C_{\\mathrm{band}}(N_x,N_y)$.\n\nAssume a constant-coefficient operator and a standard ultraspherical-type construction in the Chebyshev direction that yields a fixed total half-bandwidth $(\\beta_u+\\beta_\\ell) = 2$. This represents two off-diagonals in the operator structure that do not grow with $N_y$.\n\nYour program must compute $C_{\\mathrm{total}}(N_x,N_y)$ for the following test suite of truncation sizes:\n- Test case $1$: $N_x = 64$, $N_y = 65$.\n- Test case $2$: $N_x = 1$, $N_y = 33$.\n- Test case $3$: $N_x = 1024$, $N_y = 257$.\n- Test case $4$: $N_x = 2$, $N_y = 2$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is the total normalized arithmetic operation count $C_{\\mathrm{total}}(N_x,N_y)$ for the corresponding test case, expressed as a floating-point number. No physical units are involved. Angles, if any, must be in radians; however, this problem does not require specifying angles in the output. Percentages are not used in this problem.\n\nThe program must be self-contained and require no user input.",
            "solution": "The problem has been validated and is determined to be sound. It is scientifically grounded, well-posed, and objective. All necessary data and definitions for calculating the specified arithmetic operation count are provided. The problem describes a standard Fourier-Chebyshev spectral method for a screened Poisson equation on a cylindrical domain, and the complexity models provided for the Fast Fourier Transform and banded linear system solvers are standard in numerical analysis. The premise of achieving a fixed-bandwidth operator for the Chebyshev discretization is a known result in advanced spectral methods, making the setup consistent with established theory.\n\nThe problem asks for the total normalized arithmetic operation count, $C_{\\mathrm{total}}(N_x, N_y)$, of a fast direct solver for the two-dimensional screened Poisson equation:\n$$\n\\frac{\\partial^2 u}{\\partial x^2}(x,y) + \\frac{\\partial^2 u}{\\partial y^2}(x,y) - \\alpha\\,u(x,y) = f(x,y)\n$$\non the domain $(x,y) \\in [0,2\\pi) \\times [-1,1]$. The boundary conditions are periodic in the $x$-direction and homogeneous Dirichlet in the $y$-direction, i.e., $u(x,-1) = 0$ and $u(x,1) = 0$. The constant $\\alpha$ is positive.\n\nThe solver strategy involves two main stages:\n$1$. A Fourier spectral method is applied in the periodic $x$-direction. The solution $u(x,y)$ and the source term $f(x,y)$ are expanded in a Fourier series:\n$$\nu(x,y) = \\sum_{k=-(N_x/2-1)}^{N_x/2} \\hat{u}_k(y) e^{ikx}\n$$\nwhere $N_x$ is the number of Fourier modes. The operator $\\frac{\\partial^2}{\\partial x^2}$ is diagonalized by the Fourier transform, acting as multiplication by $(ik)^2 = -k^2$ on the $k$-th mode. This reduces the two-dimensional partial differential equation (PDE) to a set of $N_x$ decoupled one-dimensional ordinary differential equations (ODEs) for the Fourier coefficients $\\hat{u}_k(y)$:\n$$\n\\frac{d^2 \\hat{u}_k}{d y^2}(y) - k^2 \\hat{u}_k(y) - \\alpha \\hat{u}_k(y) = \\hat{f}_k(y)\n$$\nwhich simplifies to:\n$$\n\\frac{d^2 \\hat{u}_k}{d y^2}(y) - (k^2 + \\alpha) \\hat{u}_k(y) = \\hat{f}_k(y)\n$$\nfor each wavenumber $k$. The boundary conditions become $\\hat{u}_k(-1) = 0$ and $\\hat{u}_k(1) = 0$.\n\n$2$. Each of these $N_x$ ODEs is solved using a Chebyshev collocation method on $N_y$ Chebyshev–Gauss–Lobatto points. The problem states that the chosen formulation results in a banded linear system for the Chebyshev coefficients of each $\\hat{u}_k(y)$.\n\nThe total complexity is the sum of the costs of the Fourier transform stage and the banded solver stage.\n\nThe cost of the Fourier transform stage, $C_{\\mathrm{FFT}}$, involves applying the Fast Fourier Transform (FFT) along the $x$-direction for each of the $N_y$ collocation rows. The normalized complexity is given as:\n$$\nC_{\\mathrm{FFT}}(N_x, N_y) = N_x\\,N_y\\,\\log_2(N_x)\n$$\n\nThe cost of the banded solver stage, $C_{\\mathrm{band}}$, involves solving $N_x$ independent banded linear systems, one for each Fourier mode $k$. Each system has a size on the order of $N_y$. The problem provides the total normalized complexity for this stage as:\n$$\nC_{\\mathrm{band}}(N_x, N_y) = N_x\\,N_y\\,(\\beta_u+\\beta_\\ell)^2\n$$\nwhere $(\\beta_u+\\beta_\\ell)$ is the total half-bandwidth. The problem specifies that this value is fixed at $(\\beta_u+\\beta_\\ell) = 2$. Therefore, the formula becomes:\n$$\nC_{\\mathrm{band}}(N_x, N_y) = N_x\\,N_y\\,(2)^2 = 4\\,N_x\\,N_y\n$$\n\nThe total normalized arithmetic operation count is the sum of these two costs:\n$$\nC_{\\mathrm{total}}(N_x, N_y) = C_{\\mathrm{FFT}}(N_x, N_y) + C_{\\mathrm{band}}(N_x, N_y) \\\\\nC_{\\mathrm{total}}(N_x, N_y) = N_x\\,N_y\\,\\log_2(N_x) + 4\\,N_x\\,N_y \\\\\nC_{\\mathrm{total}}(N_x, N_y) = N_x\\,N_y\\,(\\log_2(N_x) + 4)\n$$\nWe now apply this formula to the given test cases.\n\nTest case $1$: $N_x = 64$, $N_y = 65$.\n$\\log_2(64) = 6$.\n$C_{\\mathrm{total}}(64, 65) = 64 \\times 65 \\times (\\log_2(64) + 4) = 4160 \\times (6 + 4) = 4160 \\times 10 = 41600$.\n\nTest case $2$: $N_x = 1$, $N_y = 33$.\n$\\log_2(1) = 0$.\n$C_{\\mathrm{total}}(1, 33) = 1 \\times 33 \\times (\\log_2(1) + 4) = 33 \\times (0 + 4) = 33 \\times 4 = 132$.\n\nTest case $3$: $N_x = 1024$, $N_y = 257$.\n$\\log_2(1024) = 10$.\n$C_{\\mathrm{total}}(1024, 257) = 1024 \\times 257 \\times (\\log_2(1024) + 4) = 263168 \\times (10 + 4) = 263168 \\times 14 = 3684352$.\n\nTest case $4$: $N_x = 2$, $N_y = 2$.\n$\\log_2(2) = 1$.\n$C_{\\mathrm{total}}(2, 2) = 2 \\times 2 \\times (\\log_2(2) + 4) = 4 \\times (1 + 4) = 4 \\times 5 = 20$.\n\nThese values will be computed by the program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the normalized arithmetic operation count for a fast direct solver\n    on the 2D screened Poisson equation using a Fourier-Chebyshev spectral method.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple is of the form (N_x, N_y).\n    test_cases = [\n        (64, 65),\n        (1, 33),\n        (1024, 257),\n        (2, 2),\n    ]\n\n    results = []\n    \n    # The total half-bandwidth is given as a fixed constant.\n    total_half_bandwidth = 2.0\n\n    for case in test_cases:\n        Nx, Ny = case\n        \n        # The cost formula for the FFT stage is given as:\n        # C_FFT = Nx * Ny * log2(Nx)\n        # Note: log2(1) = 0, which is correctly handled by np.log2.\n        # If Nx is not a power of 2, the FFT is still well-defined,\n        # though practical implementations might be more efficient for powers of 2.\n        # The formula given must be used as is.\n        c_fft = float(Nx) * float(Ny) * np.log2(float(Nx))\n        \n        # The cost formula for the banded solver stage is given as:\n        # C_band = Nx * Ny * (beta_u + beta_l)^2\n        c_band = float(Nx) * float(Ny) * (total_half_bandwidth**2)\n        \n        # The total normalized arithmetic operation count is the sum of the two.\n        c_total = c_fft + c_band\n        \n        results.append(c_total)\n\n    # Final print statement in the exact required format.\n    # The output format must be \"[result1,result2,result3,result4]\" with floating-point numbers.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}