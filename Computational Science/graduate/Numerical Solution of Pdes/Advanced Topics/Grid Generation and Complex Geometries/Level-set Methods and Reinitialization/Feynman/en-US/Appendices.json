{
    "hands_on_practices": [
        {
            "introduction": "The signed distance function (SDF) is the cornerstone of the level-set method, and its defining property is that the magnitude of its gradient is unity, i.e., $|\\nabla \\phi| = 1$. While PDE-based reinitialization is used to maintain this property, we often need a robust method to generate an accurate SDF from an initial geometric description of an interface. This practice guides you through the implementation of the Fast Marching Method (FMM), a highly efficient, causality-preserving algorithm for solving the Eikonal equation to construct the distance field, providing a fundamental tool for any level-set toolkit. ",
            "id": "3415618",
            "problem": "You are asked to implement a numerical solver for the Eikonal equation using the Fast Marching Method (FMM) to compute a signed distance field to an oriented line segment on a two-dimensional uniform Cartesian grid, and to compare this numerical field against an analytical signed distance. The computation is purely geometric and non-dimensional; no physical units are involved. The program must be self-contained and produce a single-line output that aggregates numerical error metrics for a fixed test suite.\n\nFundamental base and definitions to be used:\n- The signed distance function $\\phi$ to an interface $\\Gamma$ satisfies the Eikonal equation $|\\nabla \\phi| = 1$ away from $\\Gamma$, with the boundary condition $\\phi = 0$ on $\\Gamma$. This is a well-tested property in level-set methods and follows from the geometric definition that the magnitude of the gradient of a true distance function equals one almost everywhere away from the interface.\n- The Fast Marching Method (FMM) is a causality-preserving single-pass method for solving static Hamilton-Jacobi equations of Eikonal type, exploiting the monotonicity of the solution with respect to boundary data and propagating the solution outward in order of increasing arrival time (distance).\n- The signed distance to an oriented curve in $\\mathbb{R}^2$ can be defined by multiplying the unsigned Euclidean distance by a sign that is determined by a continuously chosen unit normal. For an oriented line segment with endpoints $\\mathbf{p}_0$ and $\\mathbf{p}_1$ and unit tangent $\\mathbf{t} = (\\mathbf{p}_1 - \\mathbf{p}_0)/\\|\\mathbf{p}_1 - \\mathbf{p}_0\\|$, a consistent choice for the unit normal is $\\mathbf{n} = (-t_y, t_x)$. The sign at a point $\\mathbf{x}$ can then be taken as $\\operatorname{sign}((\\mathbf{x} - \\mathbf{p}_0) \\cdot \\mathbf{n})$, where $\\cdot$ denotes the Euclidean inner product.\n\nComputational domain and grid:\n- Work on the square domain $[0,1]\\times[0,1]$ with a uniform grid of $N\\times N$ nodes, with $N \\in \\mathbb{N}$, grid coordinates $\\{x_i = i h : i=0,\\dots,N-1\\}$ along each axis, and spacing $h = 1/(N-1)$.\n- Denote grid nodes by $(x_i,y_j)$ for $i,j \\in \\{0,\\dots,N-1\\}$.\n\nGeometry of the interface:\n- The interface $\\Gamma$ is an oriented line segment with endpoints $\\mathbf{p}_0 = (p_{0x}, p_{0y})$ and $\\mathbf{p}_1 = (p_{1x}, p_{1y})$ in the domain.\n- Define the unit tangent $\\mathbf{t} = (\\mathbf{p}_1 - \\mathbf{p}_0) / \\|\\mathbf{p}_1 - \\mathbf{p}_0\\|$ and the corresponding unit normal $\\mathbf{n} = (-t_y, t_x)$.\n- Define the analytical signed distance at any point $\\mathbf{x} \\in \\mathbb{R}^2$ by\n$$\n\\phi^\\star(\\mathbf{x}) = s(\\mathbf{x}) \\, d(\\mathbf{x}, \\text{segment}(\\mathbf{p}_0,\\mathbf{p}_1)),\n$$\nwhere $s(\\mathbf{x}) = \\operatorname{sign}((\\mathbf{x}-\\mathbf{p}_0)\\cdot\\mathbf{n})$, and $d(\\mathbf{x}, \\text{segment}(\\mathbf{p}_0,\\mathbf{p}_1))$ is the Euclidean distance from $\\mathbf{x}$ to the closed segment $\\overline{\\mathbf{p}_0\\mathbf{p}_1}$, computed by orthogonal projection onto the infinite line followed by clamping the projection parameter to the interval $[0,1]$.\n\nNumerical task:\n- Using the Fast Marching Method (FMM) on the grid, compute a numerical approximation $T$ to the unsigned distance from the set $\\Gamma$ (treated as the zero level set). Then define the numerical signed distance $\\phi(\\mathbf{x}_{ij}) = s(\\mathbf{x}_{ij}) \\, T_{ij}$ at grid nodes, where $s(\\mathbf{x}_{ij})$ is computed as above from the oriented line normal.\n- The Eikonal problem to solve numerically is\n$$\n|\\nabla T| = 1 \\quad \\text{in } [0,1]^2\\setminus\\Gamma, \\qquad T = 0 \\quad \\text{on } \\Gamma.\n$$\n- Discretize the Eikonal equation on the uniform grid with consistent upwind finite differences and use the causality-preserving accept-propagate strategy of the Fast Marching Method to compute $T$ everywhere on the grid. You must respect the monotonicity of $T$ and ensure that the local updates are consistent with the $|\\nabla T|=1$ constraint and use only already accepted (frozen) neighbors in the update at each step.\n- Because $\\Gamma$ is not aligned to the grid and may not pass through grid nodes, approximate the discrete boundary condition for FMM as follows: sample the continuous segment at uniform parameter values with spacing $\\Delta s = h/4$ along its arclength, snap each sample to its closest grid node by rounding to the nearest $(i,j)$ index, and set those snapped nodes as zero-distance boundary seeds. These are the initially accepted (frozen) nodes for the FMM. This creates a discrete approximation of the interface within $\\mathcal{O}(h)$.\n- After computing $T$ via FMM, compute $\\phi(\\mathbf{x}_{ij})$ by multiplying the unsigned field by the sign computed from the oriented normal as described above.\n\nAnalytical comparison:\n- For each grid node $(x_i,y_j)$, compute the analytical signed distance $\\phi^\\star(\\mathbf{x}_{ij})$ as defined above.\n- Compute the maximum absolute error over all grid nodes as\n$$\nE_\\infty = \\max_{i,j} \\left| \\phi(\\mathbf{x}_{ij}) - \\phi^\\star(\\mathbf{x}_{ij}) \\right|.\n$$\n\nTest suite:\nImplement your program to run the following test cases. In each case, the line segment is specified by its endpoints, and the grid size $N$ is given. For all cases, the computational domain is $[0,1]\\times[0,1]$ and the snapping and sign conventions are as described above.\n- Case 1 (horizontal interior): $N=101$, $\\mathbf{p}_0 = (0.2, 0.5)$, $\\mathbf{p}_1 = (0.8, 0.5)$.\n- Case 2 (diagonal interior): $N=101$, $\\mathbf{p}_0 = (0.2, 0.2)$, $\\mathbf{p}_1 = (0.8, 0.8)$.\n- Case 3 (vertical near boundary): $N=121$, $\\mathbf{p}_0 = (0.05, 0.1)$, $\\mathbf{p}_1 = (0.05, 0.9)$.\n- Case 4 (short oblique segment): $N=129$, $\\mathbf{p}_0 = (0.43, 0.47)$, $\\mathbf{p}_1 = (0.57, 0.52)$.\n\nRequired output:\n- For each test case, compute $E_\\infty$ as a floating-point number. Your program should produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, in the same order as the cases above, with each number rounded to six decimal places. For example: \"[0.012345,0.023456,0.034567,0.045678]\".\n\nDeliverables:\n- A complete, runnable program that carries out the described computation for the specified test suite and prints a single-line list of the four maximum absolute errors in the exact format specified.",
            "solution": "The user-provided problem statement has been analyzed and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to proceed with a solution. The problem requires the implementation of the Fast Marching Method (FMM) to compute a signed distance field to a line segment on a 2D grid and to compare the numerical result with the analytical solution.\n\nThe solution proceeds in four main steps for each test case:\n1.  Establish the computational grid and calculate the analytical signed distance function, $\\phi^\\star$, at each grid node. This serves as the ground truth for error evaluation.\n2.  Determine the initial set of grid nodes that approximate the interface $\\Gamma$, which will serve as the seeds for the Fast Marching Method.\n3.  Execute the Fast Marching Method to compute the unsigned distance field, $T$, across the grid.\n4.  Convert the unsigned distance field $T$ to a signed distance field $\\phi$ and compute the maximum absolute error $E_\\infty = \\max |\\phi - \\phi^\\star|$.\n\n**1. Analytical Signed Distance Function ($\\phi^\\star$)**\n\nThe problem is defined on a uniform Cartesian grid over the domain $[0,1]\\times[0,1]$. The grid consists of $N \\times N$ nodes $(x_i, y_j)$, where $x_i = i h$ and $y_j = j h$ for $i,j \\in \\{0, \\dots, N-1\\}$, with a grid spacing of $h = 1/(N-1)$.\n\nThe analytical signed distance $\\phi^\\star(\\mathbf{x})$ at any point $\\mathbf{x} = (x, y)$ to the oriented line segment from $\\mathbf{p}_0$ to $\\mathbf{p}_1$ is given by $\\phi^\\star(\\mathbf{x}) = s(\\mathbf{x}) \\cdot d(\\mathbf{x}, \\Gamma)$.\n\nThe unsigned distance, $d(\\mathbf{x}, \\Gamma)$, is the minimum Euclidean distance from $\\mathbf{x}$ to the closed line segment $\\overline{\\mathbf{p}_0\\mathbf{p}_1}$. This is calculated by first finding the projection of $\\mathbf{x}$ onto the infinite line containing the segment. Let $\\mathbf{v} = \\mathbf{p}_1 - \\mathbf{p}_0$ and $\\mathbf{w} = \\mathbf{x} - \\mathbf{p}_0$. The projection parameter is $t = (\\mathbf{w} \\cdot \\mathbf{v}) / (\\mathbf{v} \\cdot \\mathbf{v})$.\n- If $t \\in [0, 1]$, the closest point on the segment is the projection $\\mathbf{p}_0 + t\\mathbf{v}$.\n- If $t  0$, the closest point is the endpoint $\\mathbf{p}_0$.\n- If $t  1$, the closest point is the endpoint $\\mathbf{p}_1$.\nThe distance $d(\\mathbf{x}, \\Gamma)$ is the Euclidean norm of the vector connecting $\\mathbf{x}$ to this closest point.\n\nThe sign, $s(\\mathbf{x})$, is determined by the orientation of the segment. The unit tangent vector is $\\mathbf{t} = (\\mathbf{p}_1 - \\mathbf{p}_0)/\\|\\mathbf{p}_1 - \\mathbf{p}_0\\|$, and the corresponding unit normal is $\\mathbf{n} = (-t_y, t_x)$. The sign is then computed as $s(\\mathbf{x}) = \\operatorname{sign}((\\mathbf{x} - \\mathbf{p}_0) \\cdot \\mathbf{n})$. This calculation is performed for all grid nodes $(x_i, y_j)$ in a vectorized manner for efficiency.\n\n**2. Interface Discretization and FMM Initialization**\n\nThe Fast Marching Method requires an initial set of \"accepted\" or \"frozen\" nodes, typically the boundary where the distance is known. Here, the interface $\\Gamma$ is a line segment that is not aligned with the grid. As specified, we approximate its location on the grid by sampling points along the segment with an arclength spacing of $\\Delta s = h/4$. Each sample point is then \"snapped\" to its nearest grid node by rounding its coordinates to the nearest grid indices $(i, j)$. The unique set of these snapped grid nodes constitutes the discrete interface.\n\nThese nodes are initialized with a distance value $T_{ij} = 0$ and marked as belonging to the `ACCEPTED` set.\n\n**3. Fast Marching Method for Unsigned Distance ($T$)**\n\nThe FMM solves the Eikonal equation, $|\\nabla T| = 1$, by systematically propagating the solution outwards from the `ACCEPTED` set. The algorithm maintains three sets of nodes: `ACCEPTED` (or `FROZEN`), for which the distance value is final; `TRIAL`, which are neighbors of `ACCEPTED` nodes and have a tentative distance value; and `FAR`, which are all other nodes. A min-priority queue (min-heap) is used to efficiently manage the `TRIAL` set, always providing the `TRIAL` node with the minimum distance.\n\nThe algorithm proceeds as follows:\n- **Initialization**: Nodes on the discretized interface are marked `ACCEPTED` with $T=0$. All their neighbors not already `ACCEPTED` are marked `TRIAL`, their tentative distances are computed, and they are added to the priority queue. All other nodes are `FAR`.\n- **Propagation Loop**:\n    1. Extract the node $(i, j)$ with the minimum distance from the priority queue.\n    2. Mark this node as `ACCEPTED`.\n    3. For each neighbor $(n_i, n_j)$ of $(i, j)$ that is not `ACCEPTED`:\n        a. Compute a new tentative distance $T_{n_i n_j}$ using the values of its `ACCEPTED` neighbors. This update step uses a finite difference approximation of the Eikonal equation.\n        b. If this new distance is smaller than the neighbor's current distance, update the neighbor's distance and add/update its entry in the priority queue. Mark it as `TRIAL` if it was `FAR`.\n\nThe update rule for a node $(i,j)$ solves the discretized Eikonal equation:\n$$\n\\left(\\frac{T_{ij} - T_x}{h}\\right)^2 + \\left(\\frac{T_{ij} - T_y}{h}\\right)^2 = 1\n$$\nwhere $T_x = \\min(T_{i-1,j}, T_{i+1,j})$ and $T_y = \\min(T_{i,j-1}, T_{i,j+1})$ are the minimum distances among `ACCEPTED` neighbors in the x and y directions, respectively. This is a quadratic equation for $T_{ij}$. If only one of $T_x$ or $T_y$ is available, the update simplifies to a 1D propagation, e.g., $T_{ij} = T_x + h$. If the quadratic equation has no real solutions (which occurs when one gradient term dominates), the update also defaults to the 1D case, $T_{ij} = \\min(T_x, T_y) + h$.\n\nThe process terminates when the priority queue is empty, at which point all reachable nodes have their final unsigned distance values in the grid $T$.\n\n**4. Final Error Calculation**\n\nThe computed unsigned distance field $T$ is converted to a numerical signed distance field, $\\phi$, by multiplying each element $T_{ij}$ by the sign $s(\\mathbf{x}_{ij})$ computed analytically as described in step 1.\n\nFinally, the maximum absolute error $E_\\infty$ is computed by taking the maximum of the absolute differences between the numerical and analytical signed distance fields over all grid nodes:\n$$\nE_\\infty = \\max_{i,j} |\\phi(\\mathbf{x}_{ij}) - \\phi^\\star(\\mathbf{x}_{ij})|\n$$\nThis metric quantifies the accuracy of the numerical scheme, including errors from both the FMM solver and the initial discretization of the interface. The results for the four specified test cases are collected and formatted.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport heapq\n\ndef compute_analytical_sdf(grid_x, grid_y, p0, p1):\n    \"\"\"\n    Computes the analytical signed distance function to a line segment.\n    This is vectorized for efficiency over the entire grid.\n    \"\"\"\n    v = p1 - p0\n    v_norm_sq = np.dot(v, v)\n\n    # Handle the case of a zero-length segment (a single point)\n    if np.isclose(v_norm_sq, 0):\n        unsigned_dist = np.sqrt((grid_x - p0[0])**2 + (grid_y - p0[1])**2)\n        # Sign is ill-defined, problem statement does not cover this.\n        # We return the unsigned distance as a reasonable default.\n        return unsigned_dist\n\n    # Vectors from p0 to each grid point\n    w_x = grid_x - p0[0]\n    w_y = grid_y - p0[1]\n\n    # Projection parameter t\n    t = (w_x * v[0] + w_y * v[1]) / v_norm_sq\n    t_clamped = np.clip(t, 0, 1)\n\n    # Closest point on the segment to each grid point\n    proj_x = p0[0] + t_clamped * v[0]\n    proj_y = p0[1] + t_clamped * v[1]\n\n    # Unsigned distance\n    unsigned_dist = np.sqrt((grid_x - proj_x)**2 + (grid_y - proj_y)**2)\n\n    # Sign calculation\n    tangent = v / np.sqrt(v_norm_sq)\n    normal = np.array([-tangent[1], tangent[0]])\n    dot_product = w_x * normal[0] + w_y * normal[1]\n    signs = np.sign(dot_product)\n    # Per problem text, sign(0)=0 is used. If sign(0)=1 convention was used,\n    # signs[signs == 0] = 1 would be added.\n\n    return signs * unsigned_dist\n\ndef get_initial_frozen_set(N, h, p0, p1):\n    \"\"\"\n    Samples the line segment and snaps points to the grid to create\n    the initial set of frozen (accepted) nodes for FMM.\n    \"\"\"\n    frozen_set = set()\n    segment_vec = p1 - p0\n    length = np.linalg.norm(segment_vec)\n\n    if np.isclose(length, 0):\n        j = int(np.round(p0[1] / h))\n        i = int(np.round(p0[0] / h))\n        frozen_set.add((np.clip(j, 0, N - 1), np.clip(i, 0, N - 1)))\n        return frozen_set\n\n    num_intervals = int(np.ceil(length / (h / 4.0)))\n    if num_intervals == 0: num_intervals = 1\n    \n    for k in range(num_intervals + 1):\n        t = k / num_intervals\n        sample_point = p0 + t * segment_vec\n        j = int(np.round(sample_point[1] / h))\n        i = int(np.round(sample_point[0] / h))\n        j_clamped = np.clip(j, 0, N - 1)\n        i_clamped = np.clip(i, 0, N - 1)\n        frozen_set.add((j_clamped, i_clamped))\n        \n    return frozen_set\n\ndef solve_eikonal_update(i, j, N, h, T, states):\n    \"\"\"\n    Calculates the distance update for a trial point (i,j) based on its\n    frozen neighbors, by solving the discretized Eikonal equation.\n    \"\"\"\n    Tx_min = np.inf\n    if i  0 and states[j, i - 1] == 2:\n        Tx_min = min(Tx_min, T[j, i - 1])\n    if i  N - 1 and states[j, i + 1] == 2:\n        Tx_min = min(Tx_min, T[j, i + 1])\n        \n    Ty_min = np.inf\n    if j  0 and states[j - 1, i] == 2:\n        Ty_min = min(Ty_min, T[j - 1, i])\n    if j  N - 1 and states[j + 1, i] == 2:\n        Ty_min = min(Ty_min, T[j + 1, i])\n\n    if Tx_min == np.inf and Ty_min == np.inf:\n        return np.inf\n\n    # When only one direction has a frozen neighbor, update is 1D\n    if Tx_min == np.inf or Ty_min == np.inf:\n        return min(Tx_min, Ty_min) + h\n    \n    # Both directions have frozen neighbors, solve quadratic\n    h2 = h * h\n    sum_ab = Tx_min + Ty_min\n    diff_ab_sq = (Tx_min - Ty_min)**2\n\n    # Check if the quadratic equation has a real solution\n    if 2 * h2  diff_ab_sq:\n        # If not, the update is dominated by the minimum valued neighbor\n        return min(Tx_min, Ty_min) + h\n    else:\n        # Solve for the larger root of the quadratic equation\n        discriminant = 2 * h2 - diff_ab_sq\n        return (sum_ab + np.sqrt(discriminant)) / 2.0\n\ndef fast_marching_method(N, h, initial_frozen_nodes):\n    \"\"\"\n    Implements the Fast Marching Method to solve the Eikonal equation |grad(T)|=1.\n    \"\"\"\n    # States: 0=FAR, 1=TRIAL, 2=FROZEN/ACCEPTED\n    states = np.zeros((N, N), dtype=np.int8)\n    T = np.full((N, N), np.inf, dtype=np.float64)\n    trial_heap = []\n\n    # Initialize frozen nodes from the boundary\n    for j, i in initial_frozen_nodes:\n        T[j, i] = 0.0\n        states[j, i] = 2\n\n    # Initialize trial nodes (neighbors of frozen)\n    for j, i in initial_frozen_nodes:\n        for dj, di in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n            ni, nj = i + di, j + dj\n            if 0 = ni  N and 0 = nj  N and states[nj, ni] == 0: # If FAR\n                tentative_T = solve_eikonal_update(ni, nj, N, h, T, states)\n                if tentative_T  T[nj, ni]:\n                    T[nj, ni] = tentative_T\n                    states[nj, ni] = 1 # Mark as TRIAL\n                    heapq.heappush(trial_heap, (tentative_T, nj, ni))\n                    \n    # Main FMM loop\n    while trial_heap:\n        dist, j, i = heapq.heappop(trial_heap)\n        \n        # Stale entry in the heap, already found a better path\n        if dist  T[j, i]:\n            continue\n        \n        states[j, i] = 2 # Accept the node\n        \n        # Propagate to neighbors\n        for dj, di in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n            ni, nj = i + di, j + dj\n            if 0 = ni  N and 0 = nj  N and states[nj, ni] != 2:\n                tentative_T = solve_eikonal_update(ni, nj, N, h, T, states)\n                if tentative_T  T[nj, ni]:\n                    T[nj, ni] = tentative_T\n                    if states[nj, ni] == 0: # If it was FAR\n                        states[nj, ni] = 1\n                    heapq.heappush(trial_heap, (tentative_T, nj, ni))\n    return T\n\ndef apply_sign(grid_x, grid_y, T_grid, p0, p1):\n    \"\"\"\n    Applies the sign to the unsigned distance field T.\n    \"\"\"\n    v = p1 - p0\n    v_norm = np.linalg.norm(v)\n    if np.isclose(v_norm, 0):\n        return T_grid\n\n    tangent = v / v_norm\n    normal = np.array([-tangent[1], tangent[0]])\n    \n    w_x = grid_x - p0[0]\n    w_y = grid_y - p0[1]\n    \n    dot_product = w_x * normal[0] + w_y * normal[1]\n    signs = np.sign(dot_product)\n    \n    return signs * T_grid\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and generate the final output.\n    \"\"\"\n    test_cases = [\n        # (N, p0, p1)\n        (101, (0.2, 0.5), (0.8, 0.5)),  # Case 1\n        (101, (0.2, 0.2), (0.8, 0.8)),  # Case 2\n        (121, (0.05, 0.1), (0.05, 0.9)),# Case 3\n        (129, (0.43, 0.47), (0.57, 0.52)),# Case 4\n    ]\n\n    results = []\n    for N, p0_tuple, p1_tuple in test_cases:\n        p0 = np.array(p0_tuple, dtype=np.float64)\n        p1 = np.array(p1_tuple, dtype=np.float64)\n        \n        # 1. Grid Setup\n        h = 1.0 / (N - 1)\n        x_coords = np.linspace(0, 1, N)\n        y_coords = np.linspace(0, 1, N)\n        grid_x, grid_y = np.meshgrid(x_coords, y_coords)\n        \n        # 2. Analytical Solution\n        phi_star_grid = compute_analytical_sdf(grid_x, grid_y, p0, p1)\n        \n        # 3. Numerical Solution\n        initial_nodes = get_initial_frozen_set(N, h, p0, p1)\n        T_grid = fast_marching_method(N, h, initial_nodes)\n        phi_grid = apply_sign(grid_x, grid_y, T_grid, p0, p1)\n        \n        # 4. Error Calculation\n        E_infinity = np.max(np.abs(phi_grid - phi_star_grid))\n        results.append(f\"{E_infinity:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "An alternative to constructing a new signed distance field is to evolve an existing level-set function to restore the distance property using a Hamilton-Jacobi type PDE. This exercise dissects the reinitialization process to its most fundamental component: a single update step at a single grid node. By manually applying an upwind Godunov scheme, you will gain a concrete understanding of how the numerical method propagates information to enforce the $|\\nabla \\phi| = 1$ condition, illuminating the core mechanics behind the reinitialization equation. ",
            "id": "2606595",
            "problem": "In density-based structural topology optimization, it is common to hybridize Solid Isotropic Material with Penalization (SIMP) with a Level-Set Method (LSM) by projecting filtered densities onto an implicit boundary represented by a level-set field. To maintain numerical robustness of curvature-dependent velocities in the Level-Set Method, the level-set field must be periodically reinitialized toward a signed distance function by evolving the Hamilton–Jacobi-type reinitialization equation\n$$\n\\frac{\\partial \\phi}{\\partial \\tau} + S(\\phi_{0})\\left(|\\nabla \\phi| - 1\\right) = 0,\n$$\nwhere $\\tau$ is the pseudo-time, $\\phi_{0}$ is the frozen initial level-set field used in the sign function, and $S(\\phi_{0})$ is a smoothed sign defined by\n$$\nS(\\phi_{0}) = \\frac{\\phi_{0}}{\\sqrt{\\phi_{0}^{2} + \\varepsilon^{2}}}.\n$$\nConsider a one-dimensional uniform grid with nodes indexed by $i \\in \\{0,1,2,3,4,5,6\\}$, spacing $\\Delta x = 0.5$, and pseudo-time step $\\Delta \\tau = 0.1$. The initial field is\n$$\n\\phi_{0} = \\left[\\phi_{0,0}, \\phi_{0,1}, \\phi_{0,2}, \\phi_{0,3}, \\phi_{0,4}, \\phi_{0,5}, \\phi_{0,6}\\right] = \\left[1.2,\\, 0.7,\\, 0.2,\\, -0.1,\\, -0.6,\\, -1.1,\\, -1.6\\right],\n$$\nand the smoothing parameter is $\\varepsilon = 0.5$. Perform a single explicit pseudo-time update of the reinitialization equation at node $i=3$ using a monotone upwind Godunov discretization of the Hamilton–Jacobi term in one space dimension and one-sided finite differences,\n$$\nD^{-}\\phi_{i} = \\frac{\\phi_{i}-\\phi_{i-1}}{\\Delta x}, \\quad D^{+}\\phi_{i} = \\frac{\\phi_{i+1}-\\phi_{i}}{\\Delta x}.\n$$\nTreat $\\phi^{n}=\\phi_{0}$ at the current pseudo-time level and compute $\\phi^{n+1}_{3}$ after one pseudo-time step. Express your final answer as a real number rounded to four significant figures. No units are required.",
            "solution": "The problem requires the computation of a single pseudo-time step for a level-set reinitialization equation at a specific grid node. The problem is well-posed and contains all necessary physical quantities and numerical parameters. We shall proceed with the solution.\n\nThe governing Hamilton-Jacobi reinitialization equation is given as\n$$\n\\frac{\\partial \\phi}{\\partial \\tau} + S(\\phi_{0})\\left(|\\nabla \\phi| - 1\\right) = 0\n$$\nwhere $\\tau$ is pseudo-time, $\\phi$ is the level-set field, and $S(\\phi_{0})$ is a smoothed sign function based on the initial field $\\phi_0$. The equation can be rewritten as\n$$\n\\frac{\\partial \\phi}{\\partial \\tau} = - S(\\phi_{0})\\left(|\\nabla \\phi| - 1\\right)\n$$\nWe discretize this equation using an explicit forward Euler scheme in pseudo-time. For a given node $i$, the update rule is\n$$\n\\frac{\\phi_{i}^{n+1} - \\phi_{i}^{n}}{\\Delta \\tau} = - S(\\phi_{0,i})\\left((|\\nabla \\phi|)_i^n - 1\\right)\n$$\nwhere $(|\\nabla \\phi|)_i^n$ is a suitable numerical approximation of the gradient magnitude at node $i$ and pseudo-time level $n$. Rearranging for $\\phi_{i}^{n+1}$, we get:\n$$\n\\phi_{i}^{n+1} = \\phi_{i}^{n} - \\Delta \\tau S(\\phi_{0,i}) \\left( (|\\nabla \\phi|)_i^n - 1 \\right)\n$$\nThe problem specifies that the initial condition is $\\phi^n = \\phi_0$. We are tasked to compute $\\phi_{3}^{n+1}$ at node $i=3$. The update equation for this node is\n$$\n\\phi_{3}^{n+1} = \\phi_{0,3} - \\Delta \\tau S(\\phi_{0,3}) \\left( (|\\nabla \\phi|)_3^n - 1 \\right)\n$$\nFirst, we calculate the smoothed sign function $S(\\phi_{0,3})$ using the given definition $S(\\phi_{0}) = \\frac{\\phi_{0}}{\\sqrt{\\phi_{0}^{2} + \\varepsilon^{2}}}$. We are given $\\phi_{0,3} = -0.1$ and $\\varepsilon = 0.5$.\n$$\nS(\\phi_{0,3}) = \\frac{-0.1}{\\sqrt{(-0.1)^{2} + (0.5)^{2}}} = \\frac{-0.1}{\\sqrt{0.01 + 0.25}} = \\frac{-0.1}{\\sqrt{0.26}}\n$$\nNext, we must determine the numerical approximation of the gradient magnitude, $(|\\nabla \\phi|)_3^n$. The problem specifies a monotone upwind Godunov scheme. The choice of the upwind direction depends on the direction of information propagation, which is determined by the sign of the characteristic speed of the Hamilton-Jacobi equation. The spatial part of the equation is $H(\\phi_x) = S(\\phi_0)(|\\phi_x|-1)$. The characteristic speed is $c = H'(\\phi_x) = S(\\phi_0) \\frac{d}{d\\phi_x}(|\\phi_x|) = S(\\phi_0) \\text{sgn}(\\phi_x)$.\n\nWe approximate $\\phi_x$ at node $i=3$ using the provided one-sided finite difference formulas:\nThe backward difference is:\n$$\nD^{-}\\phi_{3}^{n} = \\frac{\\phi_{3}^{n}-\\phi_{2}^{n}}{\\Delta x} = \\frac{\\phi_{0,3}-\\phi_{0,2}}{\\Delta x} = \\frac{-0.1 - 0.2}{0.5} = \\frac{-0.3}{0.5} = -0.6\n$$\nThe forward difference is:\n$$\nD^{+}\\phi_{3}^{n} = \\frac{\\phi_{4}^{n}-\\phi_{3}^{n}}{\\Delta x} = \\frac{\\phi_{0,4}-\\phi_{0,3}}{\\Delta x} = \\frac{-0.6 - (-0.1)}{0.5} = \\frac{-0.5}{0.5} = -1.0\n$$\nSince both $D^{-}\\phi_{3}^{n}$ and $D^{+}\\phi_{3}^{n}$ are negative, the local gradient $\\phi_x$ is negative. Therefore, $\\text{sgn}(\\phi_x) = -1$.\nThe sign of $S(\\phi_{0,3})$ is negative since $\\phi_{0,3} = -0.1  0$.\nThe characteristic speed is $c = S(\\phi_{0,3}) \\text{sgn}(\\phi_x) = (\\text{negative}) \\times (-1) = (\\text{positive})$.\nA positive characteristic speed implies that information propagates from left to right. Therefore, the upwind scheme requires using a backward difference (looking \"upwind\" to the left).\nThe numerical gradient magnitude is thus approximated by the magnitude of the backward difference:\n$$\n(|\\nabla \\phi|)_3^n \\approx |D^{-}\\phi_{3}^{n}| = |-0.6| = 0.6\n$$\nNow we have all the components to compute $\\phi_{3}^{n+1}$. The given parameters are $\\Delta \\tau = 0.1$ and $\\phi_{0,3} = -0.1$.\nSubstituting the values into the update equation:\n$$\n\\phi_{3}^{n+1} = -0.1 - (0.1) \\left(\\frac{-0.1}{\\sqrt{0.26}}\\right) \\left( 0.6 - 1 \\right)\n$$\n$$\n\\phi_{3}^{n+1} = -0.1 - (0.1) \\left(\\frac{-0.1}{\\sqrt{0.26}}\\right) \\left( -0.4 \\right)\n$$\nThe product of the three terms in the subtraction is negative because of the three negative signs.\n$$\n\\phi_{3}^{n+1} = -0.1 - \\frac{(0.1)(0.1)(0.4)}{\\sqrt{0.26}} = -0.1 - \\frac{0.004}{\\sqrt{0.26}}\n$$\nWe now perform the final calculation:\n$$\n\\sqrt{0.26} \\approx 0.50990195\n$$\n$$\n\\phi_{3}^{n+1} \\approx -0.1 - \\frac{0.004}{0.50990195} \\approx -0.1 - 0.0078446\n$$\n$$\n\\phi_{3}^{n+1} \\approx -0.1078446\n$$\nThe problem requires the answer rounded to four significant figures. The first four significant figures are $1$, $0$, $7$, $8$. The following digit is $4$, so we do not round up.\n$$\n\\phi_{3}^{n+1} \\approx -0.1078\n$$",
            "answer": "$$\n\\boxed{-0.1078}\n$$"
        },
        {
            "introduction": "The theoretical elegance of the reinitialization PDE meets practical challenges during implementation, particularly in the choice of the regularization parameter $\\epsilon$ for the sign function. A poorly chosen $\\epsilon$ can lead to numerical instability or, more subtly, cause the zero level set to drift away from its true position, compromising the accuracy of the entire simulation. This practice simulates a common task in computational science: conducting a numerical experiment to quantify the trade-off between stability and accuracy, allowing you to empirically determine an optimal value for $\\epsilon$ and understand its deep connection to the underlying grid resolution. ",
            "id": "3415541",
            "problem": "You are asked to design and implement a numerical experiment that examines the role of smoothed sign-function regularizations in level-set reinitialization. The fundamental base is the Hamilton-Jacobi formulation of the level-set reinitialization partial differential equation and its discrete upwind (Godunov) approximation. Your experiment must determine how the smoothing parameter influences interface drift and convergence speed, and must provide an empirical optimization of the smoothing parameter alongside a theoretical prediction.\n\nStart from the following well-tested and widely used formulation. Let $\\phi(x,y,\\tau)$ be the level-set function evolving in reinitialization pseudo-time $\\tau$ to a signed-distance function. Given an initial level-set function $\\phi_0(x,y)$, consider the reinitialization equation\n$$\n\\frac{\\partial \\phi}{\\partial \\tau} + \\operatorname{sign}_\\epsilon(\\phi_0)\\left(\\lVert \\nabla \\phi \\rVert - 1\\right) = 0,\n$$\nwhere the smoothed sign function is\n$$\n\\operatorname{sign}_\\epsilon(\\phi) = \\frac{\\phi}{\\sqrt{\\phi^2 + \\epsilon^2}},\n$$\nand $\\epsilongt;0$ is a smoothing parameter. The stationary solution satisfies $\\lVert \\nabla \\phi \\rVert = 1$ while preserving the zero level set of $\\phi_0$ under ideal discretization.\n\nImplement a two-dimensional explicit scheme on a uniform Cartesian grid with cell-centered unknowns that uses a Godunov upwind approximation for the Hamiltonian $\\lVert \\nabla \\phi \\rVert$. Use zero-Neumann boundary conditions. The pseudo-time step $\\Delta \\tau$ must satisfy a stability condition consistent with the Hamilton-Jacobi equation, and you must monitor convergence to a tolerance threshold.\n\nInterface drift must be quantified with respect to a known analytic zero level set. Use an initial level-set function whose zero level set is a circle of radius $R_0$ centered at $(c_x,c_y)$, but whose gradient is not initially of unit magnitude. For specificity, define the radial distance $r(x,y) = \\sqrt{(x-c_x)^2 + (y-c_y)^2}$ and set\n$$\n\\phi_0(x,y) = \\left[r(x,y) - R_0\\right] + \\beta \\left[r(x,y) - R_0\\right]^3,\n$$\nwith a small cubic perturbation amplitude $\\beta$ so that the zero level set remains $r(x,y)=R_0$. The interface drift must be measured as the absolute difference between the average radius of the numerically reinitialized zero level set and $R_0$, expressed in domain length units.\n\nYour program must, for each test case, scan a small set of candidate values of the smoothing parameter $\\epsilon$ scaled by the grid spacing $\\Delta x$ as $\\epsilon = c \\,\\Delta x$ with $c$ in a given candidate set. For each candidate $c$, run the reinitialization until convergence and record:\n- the interface drift in domain length units, defined as $| \\bar{R} - R_0 |$, where $\\bar{R}$ is the average of zero-crossing radii sampled over uniformly spaced angles in radians,\n- the number of pseudo-time iterations required to meet the convergence tolerance.\n\nThen, select an empirically optimal $c$ by minimizing the drift; in case of ties, choose the one with the fewest iterations; if still tied, choose the one closest to the theoretical prediction $c_{\\mathrm{theory}}=1$. The program must also report $c_{\\mathrm{theory}}$ for each test case.\n\nDiscretization specifics to implement:\n- Domain: $[0,L]\\times[0,L]$ with $L=1$.\n- Grid: cell-centered points $(x_i,y_j)$ with $x_i = (i+1/2)\\Delta x$, $y_j = (j+1/2)\\Delta y$, $i=0,\\dots,N_x-1$, $j=0,\\dots,N_y-1$, and $\\Delta x = \\Delta y = L/N_x = L/N_y$ for square grids.\n- Boundary condition: zero-Neumann (copy boundary values to ghost layers or equivalent).\n- Godunov upwind gradient magnitude. For each grid point, compute one-sided differences $D_x^+$, $D_x^-$, $D_y^+$, $D_y^-$ and define\n  - For $\\operatorname{sign}_\\epsilon(\\phi_0)\\ge 0$,\n    $$\n    G = \\sqrt{\\max\\left(\\left[\\max(D_x^-,0)\\right]^2,\\left[\\min(D_x^+,0)\\right]^2\\right) + \\max\\left(\\left[\\max(D_y^-,0)\\right]^2,\\left[\\min(D_y^+,0)\\right]^2\\right)},\n    $$\n  - For $\\operatorname{sign}_\\epsilon(\\phi_0)lt; 0$,\n    $$\n    G = \\sqrt{\\max\\left(\\left[\\max(D_x^+,0)\\right]^2,\\left[\\min(D_x^-,0)\\right]^2\\right) + \\max\\left(\\left[\\max(D_y^+,0)\\right]^2,\\left[\\min(D_y^-,0)\\right]^2\\right)}.\n    $$\n  Use $G$ in the update $\\phi^{n+1} = \\phi^{n} - \\Delta \\tau \\operatorname{sign}_\\epsilon(\\phi_0)\\left(G - 1\\right)$.\n\nConvergence assessment:\n- At each pseudo-time step, compute the residual $R = \\max_{i,j}\\left|\\operatorname{sign}_\\epsilon(\\phi_0)\\left(G-1\\right)\\right|$.\n- Stop when $R \\le \\mathrm{tol}$ or when the maximum number of iterations is reached.\n\nInterface drift estimation:\n- Sample angles $\\theta_k$ uniformly in radians on $[0,2\\pi)$.\n- Along each ray from $(c_x,c_y)$ in direction $(\\cos\\theta_k,\\sin\\theta_k)$, bracket the zero crossing of $\\phi$ in radius $r$ and compute the root to tolerance by bisection using bilinear interpolation of $\\phi(x,y)$ at off-grid points.\n- Average the radii to obtain $\\bar{R}$ and compute $|\\bar{R} - R_0|$.\n\nTest suite:\n- Test case 1 (happy path): $N_x=N_y=64$, $R_0=0.28$, $(c_x,c_y)=(0.47,0.52)$, $\\beta=0.2$, $\\Delta \\tau = 0.3\\,\\Delta x$, $\\mathrm{tol}=10^{-3}$, $\\text{max\\_iter}=200$, candidate set $\\mathcal{C}=\\{0.5,1.0,2.0,3.0\\}$, angle samples $K=48$.\n- Test case 2 (boundary condition stress): $N_x=N_y=96$, same $R_0$, $(c_x,c_y)$, $\\beta$, $\\Delta \\tau=0.3\\,\\Delta x$, $\\mathrm{tol}=10^{-3}$, $\\text{max\\_iter}=250$, $\\mathcal{C}=\\{0.5,1.0,2.0,3.0\\}$, $K=64$.\n- Test case 3 (fine resolution edge case): $N_x=N_y=128$, same $R_0$, $(c_x,c_y)$, $\\beta$, $\\Delta \\tau=0.3\\,\\Delta x$, $\\mathrm{tol}=8\\times 10^{-4}$, $\\text{max\\_iter}=300$, $\\mathcal{C}=\\{0.5,1.0,2.0,3.0\\}$, $K=72$.\n\nTheoretical prediction:\n- Provide and use the theoretical $c_{\\mathrm{theory}}=1$, motivated by balancing the smoothing width $\\epsilon$ in $\\phi$-space with the grid resolution $\\Delta x$ so that the transition region of $\\operatorname{sign}_\\epsilon$ is resolved by approximately one grid cell.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case yields a list $[c_{\\mathrm{emp}}, d, n, c_{\\mathrm{theory}}]$ with $c_{\\mathrm{emp}}$ the empirically optimal scaling factor, $d$ the drift as a float in domain length units, $n$ the integer iteration count to meet tolerance, and $c_{\\mathrm{theory}}$ the theoretical scaling factor. For example: `[[c_emp1,d1,n1,c_theory1],[c_emp2,d2,n2,c_theory2],[c_emp3,d3,n3,c_theory3]]`. No other text should be printed.",
            "solution": "The posed problem is scientifically sound, well-posed, and contains all necessary information for a unique, verifiable solution. It addresses a standard topic in the numerical solution of partial differential equations: the parameterization of regularization in level-set methods. The problem is valid.\n\nHerein, we present the design and rationale for the numerical experiment.\n\nThe objective is to analyze the influence of the smoothing parameter $\\epsilon$ on the accuracy and efficiency of the level-set reinitialization process. Reinitialization is the procedure by which an arbitrary level-set function $\\phi$, whose zero level set implicitly defines a geometric interface, is transformed into a signed distance function (SDF) without altering the position of the zero level set. An SDF satisfies the Eikonal equation, $\\lVert \\nabla \\phi \\rVert = 1$.\n\nThe evolution of $\\phi$ towards an SDF is governed by the Hamilton-Jacobi partial differential equation:\n$$\n\\frac{\\partial \\phi}{\\partial \\tau} + S(\\phi_0) \\left( \\lVert \\nabla \\phi \\rVert - 1 \\right) = 0\n$$\nwhere $\\tau$ is a pseudo-time variable. The term $S(\\phi_0)$ is a sign function based on the initial state $\\phi_0(x,y)$. Its purpose is to ensure that the characteristics of this hyperbolic PDE propagate information away from the zero level set, both inwards and outwards, thereby re-distancing the entire domain while preserving the interface `$\\phi=0$`. A key insight is that the term $(\\lVert \\nabla \\phi \\rVert - 1)$ drives the evolution, which ceases when $\\phi$ becomes an SDF.\n\nA direct implementation of the discontinuous sign function $S(\\phi_0) = \\operatorname{sign}(\\phi_0)$ can introduce numerical artifacts. Therefore, a smoothed version is employed:\n$$\n\\operatorname{sign}_\\epsilon(\\phi) = \\frac{\\phi}{\\sqrt{\\phi^2 + \\epsilon^2}}\n$$\nThe parameter $\\epsilon$ controls the width of the transition region around $\\phi=0$. The choice of $\\epsilon$ is critical: if it is too small, numerical oscillations may occur; if it is too large, it can cause the zero level set to drift, corrupting the solution's accuracy. The problem prescribes investigating this trade-off by relating $\\epsilon$ to the grid spacing $\\Delta x$ via a scaling factor $c$, such that $\\epsilon = c \\, \\Delta x$.\n\nThe theoretical motivation for setting $c_{\\mathrm{theory}}=1$ is based on a scale-matching argument. Since the reinitialized function $\\phi$ approximates a distance function, a change of $\\Delta\\phi$ in function value corresponds to a spatial distance of approximately $\\Delta\\phi$. The smoothing parameter $\\epsilon$ defines the thickness of the transition band in $\\phi$-space for the regularized sign function. By setting $\\epsilon = \\Delta x$, we are essentially stipulating that this transition region should have a physical width on the order of one grid cell. This is a common heuristic to balance the need for a sharp-yet-smooth transition with the available grid resolution.\n\nThe numerical solution is implemented on a two-dimensional, uniform, cell-centered Cartesian grid. The spatial domain is $[0, L] \\times [0, L]$ with $L=1$, discretized into $N_x \\times N_y$ cells of size $\\Delta x = \\Delta y = L/N_x$. The grid points are $(x_i, y_j) = ((i+0.5)\\Delta x, (j+0.5)\\Delta y)$.\n\nThe time-evolution is performed using an explicit forward Euler method:\n$$\n\\phi_{i,j}^{n+1} = \\phi_{i,j}^{n} - \\Delta \\tau \\operatorname{sign}_\\epsilon(\\phi_{0,i,j}) \\left( G_{i,j} - 1 \\right)\n$$\nwhere $\\phi_{i,j}^{n}$ is the value of $\\phi$ at grid point $(i,j)$ and pseudo-time step $n$. The crucial component is $G_{i,j}$, a numerical approximation of the Hamiltonian $\\lVert \\nabla \\phi \\rVert$. The problem specifies a Godunov upwind scheme. This scheme respects the direction of information propagation inherent in the Hamilton-Jacobi equation. The choice of one-sided finite differences depends on the sign of $\\operatorname{sign}_\\epsilon(\\phi_0)$, which determines whether information flows \"uphill\" or \"downhill\" in $\\phi$.\n\nLet the one-sided difference operators be $D_x^\\pm$ and $D_y^\\pm$.\nFor a point $(i,j)$ where $\\operatorname{sign}_\\epsilon(\\phi_{0,i,j}) \\ge 0$, the system evolves points with $\\phi0$ towards a distance function property. Information flows away from the interface into the positive region. The appropriate upwind gradient magnitude approximation is:\n$$\nG = \\sqrt{\\max\\left(\\left[\\max(D_x^-\\phi,0)\\right]^2,\\left[\\min(D_x^+\\phi,0)\\right]^2\\right) + \\max\\left(\\left[\\max(D_y^-\\phi,0)\\right]^2,\\left[\\min(D_y^+\\phi,0)\\right]^2\\right)}\n$$\nConversely, where $\\operatorname{sign}_\\epsilon(\\phi_{0,i,j})  0$, information flows from the interface into the negative region. The upwind direction is reversed:\n$$\nG = \\sqrt{\\max\\left(\\left[\\max(D_x^+\\phi,0)\\right]^2,\\left[\\min(D_x^-\\phi,0)\\right]^2\\right) + \\max\\left(\\left[\\max(D_y^+\\phi,0)\\right]^2,\\left[\\min(D_y^-\\phi,0)\\right]^2\\right)}\n$$\nZero-Neumann boundary conditions are implemented by padding the grid with an extra layer of \"ghost\" cells and copying the values from the nearest interior cells.\n\nThe initial condition is set to\n$$\n\\phi_0(x,y) = \\left[r(x,y) - R_0\\right] + \\beta \\left[r(x,y) - R_0\\right]^3\n$$\nwhere $r(x,y)$ is the radial distance from a center $(c_x, c_y)$. This function is specifically designed such that its zero level set is a perfect circle of radius $R_0$, but its gradient magnitude deviates from unity, i.e., $\\lVert \\nabla \\phi_0 \\rVert \\ne 1$ for $r \\ne R_0$. This provides a non-trivial starting point for the reinitialization process.\n\nThe simulation for each candidate value of $c$ iterates until the maximum residual, $R = \\max_{i,j}\\left|\\operatorname{sign}_\\epsilon(\\phi_0)(G-1)\\right|$, falls below a tolerance $\\mathrm{tol}$, or a maximum number of iterations is reached.\n\nTo quantify accuracy, the interface drift is measured. This involves locating the zero level set of the final, converged function $\\phi_{\\text{final}}$. We sample $K$ uniformly-spaced angles $\\theta_k \\in [0, 2\\pi)$. For each angle, we find the root of the function $f(r) = \\phi_{\\text{final}}(c_x + r\\cos\\theta_k, c_y + r\\sin\\theta_k)$. Since the point $(c_x + r\\cos\\theta_k, c_y + r\\sin\\theta_k)$ may not lie on a grid point, bilinear interpolation is used to evaluate $\\phi_{\\text{final}}$ at off-grid locations. A bisection method is then employed to find the radius $r_k$ where $f(r_k)=0$. The average radius $\\bar{R} = \\frac{1}{K}\\sum_k r_k$ is computed, and the drift is defined as the absolute error $d = |\\bar{R} - R_0|$.\n\nThe final step is to determine the empirically optimal scaling factor, $c_{\\mathrm{emp}}$. For each test case, we gather the drift $d$ and iteration count $n$ for each candidate $c$. The optimal $c$ is the one that minimizes $d$. Ties are broken by choosing the one with the minimum $n$. Any remaining ties are broken by selecting the $c$ value closest to the theoretical optimum, $c_{\\mathrm{theory}}=1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import RegularGridInterpolator\n\ndef run_reinitialization(phi_0, dx, dy, dt, tol, max_iter, epsilon):\n    \"\"\"\n    Performs the level-set reinitialization using a Godunov upwind scheme.\n    \"\"\"\n    phi = phi_0.copy()\n    sign_field = phi_0 / np.sqrt(phi_0**2 + epsilon**2)\n    \n    Nx, Ny = phi.shape\n    \n    num_iter = 0\n    for i in range(max_iter):\n        num_iter = i + 1\n        \n        # Apply zero-Neumann boundary conditions using ghost cells\n        phi_pad = np.pad(phi, 1, mode='edge')\n        \n        # Compute one-sided differences\n        Dx_p = (phi_pad[1:-1, 2:] - phi_pad[1:-1, 1:-1]) / dx\n        Dx_m = (phi_pad[1:-1, 1:-1] - phi_pad[1:-1, :-2]) / dx\n        Dy_p = (phi_pad[2:, 1:-1] - phi_pad[1:-1, 1:-1]) / dy\n        Dy_m = (phi_pad[1:-1, 1:-1] - phi_pad[:-2, 1:-1]) / dy\n\n        # Godunov Hamiltonian for sign = 0\n        grad_norm_sq_pos = (\n            np.maximum(np.maximum(Dx_m, 0)**2, np.minimum(Dx_p, 0)**2) +\n            np.maximum(np.maximum(Dy_m, 0)**2, np.minimum(Dy_p, 0)**2)\n        )\n        \n        # Godunov Hamiltonian for sign  0\n        grad_norm_sq_neg = (\n            np.maximum(np.maximum(Dx_p, 0)**2, np.minimum(Dx_m, 0)**2) +\n            np.maximum(np.maximum(Dy_p, 0)**2, np.minimum(Dy_m, 0)**2)\n        )\n        \n        G_sq = np.where(sign_field = 0, grad_norm_sq_pos, grad_norm_sq_neg)\n        # Avoid sqrt(0) issues at steady state points, although it should be fine\n        G_sq[G_sq  0] = 0\n        G = np.sqrt(G_sq)\n        \n        update = sign_field * (G - 1)\n        \n        residual = np.max(np.abs(update))\n        if residual = tol:\n            break\n            \n        phi -= dt * update\n        \n    return phi, num_iter\n\ndef calculate_drift(phi, dx, dy, cx, cy, R0, K):\n    \"\"\"\n    Calculates the interface drift by finding the average radius of the zero level set.\n    \"\"\"\n    Nx, Ny = phi.shape\n    L = Nx * dx \n    \n    x_coords = (np.arange(Nx) + 0.5) * dx\n    y_coords = (np.arange(Ny) + 0.5) * dy\n    \n    # Create an interpolator for the phi field\n    # phi has shape (Nx, Ny), corresponding to points (x_coords, y_coords)\n    # The RegularGridInterpolator expects values with shape (len(x_coords), len(y_coords))\n    interpolator = RegularGridInterpolator((x_coords, y_coords), phi, \n                                           method='linear', bounds_error=False, fill_value=None)\n                                           \n    found_radii = []\n    angles = np.linspace(0, 2 * np.pi, K, endpoint=False)\n    \n    for theta in angles:\n        cos_theta, sin_theta = np.cos(theta), np.sin(theta)\n        \n        # Function whose root we want to find for radius r\n        def f(r):\n            pt_x = cx + r * cos_theta\n            pt_y = cy + r * sin_theta\n            # Interpolator takes a (N, 2) array of points\n            val = interpolator(np.array([pt_x, pt_y]))\n            return val[0]\n        \n        # Bisection method to find the root\n        # A robust bracket for this problem geometry\n        r_low, r_high = 0.1 * R0, 3.0 * R0\n\n        val_low = f(r_low)\n        val_high = f(r_high)\n        \n        # Check if the bracket is valid\n        if val_low is None or val_high is None or val_low * val_high = 0:\n            continue\n        \n        # 30 iterations give high precision, more than needed\n        for _ in range(30):\n            r_mid = (r_low + r_high) / 2.0\n            val_mid = f(r_mid)\n            if val_low * val_mid  0:\n                r_high = r_mid\n            else:\n                r_low = r_mid\n                val_low = val_mid\n        \n        found_radii.append((r_low + r_high) / 2.0)\n    \n    if not found_radii:\n        return np.inf # Return a large drift if no roots found\n\n    avg_R = np.mean(found_radii)\n    drift = np.abs(avg_R - R0)\n    return drift\n\ndef run_experiment(case_params):\n    \"\"\"\n    Runs the full numerical experiment for a single test case.\n    \"\"\"\n    Nx, Ny, R0, cx, cy, beta, dt_factor, tol, max_iter, c_candidates, K = case_params\n    \n    L = 1.0\n    dx = L / Nx\n    dy = L / Ny\n    dt = dt_factor * dx\n    c_theory = 1.0\n\n    # Create grid and initial condition phi_0\n    i = np.arange(Nx)\n    j = np.arange(Ny)\n    x = (i[:, np.newaxis] + 0.5) * dx\n    y = (j[np.newaxis, :] + 0.5) * dy\n    \n    r = np.sqrt((x - cx)**2 + (y - cy)**2)\n    phi_0 = (r - R0) + beta * (r - R0)**3\n    \n    results = []\n    for c_val in c_candidates:\n        epsilon = c_val * dx\n        \n        phi_final, iters = run_reinitialization(phi_0, dx, dy, dt, tol, max_iter, epsilon)\n        \n        drift = calculate_drift(phi_final, dx, dy, cx, cy, R0, K)\n        \n        results.append({'c': c_val, 'drift': drift, 'iters': iters})\n    \n    # Find the optimal c based on the criteria\n    # 1. Minimize drift\n    # 2. Minimize iterations (tie-breaker)\n    # 3. Closest to c_theory (tie-breaker)\n    results.sort(key=lambda res: (res['drift'], res['iters'], abs(res['c'] - c_theory)))\n    \n    best_result = results[0]\n    c_emp = best_result['c']\n    final_drift = best_result['drift']\n    final_iters = best_result['iters']\n\n    return [c_emp, final_drift, final_iters, c_theory]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (Nx, Ny, R0, cx, cy, beta, dt_factor, tol, max_iter, C_set, K)\n        (64, 64, 0.28, 0.47, 0.52, 0.2, 0.3, 1e-3, 200, [0.5, 1.0, 2.0, 3.0], 48),\n        (96, 96, 0.28, 0.47, 0.52, 0.2, 0.3, 1e-3, 250, [0.5, 1.0, 2.0, 3.0], 64),\n        (128, 128, 0.28, 0.47, 0.52, 0.2, 0.3, 8e-4, 300, [0.5, 1.0, 2.0, 3.0], 72),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_experiment(case)\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    results_str_list = []\n    for res in all_results:\n        # Format each sub-list: e.g., '[1.0,0.00123,150,1.0]'\n        c_emp, d, n, c_theory = res\n        s = f\"[{c_emp},{d},{n},{c_theory}]\"\n        results_str_list.append(s)\n    \n    final_output = f\"[{','.join(results_str_list)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}