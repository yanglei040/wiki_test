{
    "hands_on_practices": [
        {
            "introduction": "A fundamental strategic choice in using restarted GMRES is selecting the restart parameter $m$. This decision involves a direct trade-off: a larger $m$ allows the algorithm to explore a larger Krylov subspace, potentially leading to faster convergence per cycle, but at the cost of increased memory usage and computational work. This first exercise provides a practical framework for analyzing this trade-off, guiding you to determine a memory-constrained maximum $m$ and then use classical convergence estimates to predict performance for two distinct types of PDE discretizations. ",
            "id": "3440246",
            "problem": "Consider matrices $A_{\\mathrm{P}} \\in \\mathbb{R}^{N \\times N}$ and $A_{\\mathrm{H}} \\in \\mathbb{R}^{N \\times N}$ arising from finite-difference discretizations of two Partial Differential Equations (PDEs): the Poisson equation on the unit square with Dirichlet boundary conditions, and a Helmholtz equation with wavenumber $k$ and absorbing boundary conditions. We solve $A x = b$ using the Generalized Minimal Residual (GMRES) method with restarting, denoted GMRES($m$), where $m$ is the restart parameter.\n\nYou have a memory budget that allows storing at most $S$ vectors of length $N$ simultaneously. During one cycle of the Arnoldi process underlying GMRES($m$), assume the implementation stores the following $N$-length vectors: the current solution $x$, the current residual $r$, one work vector $w$ for forming and orthogonalizing $A v_{j}$, and the Arnoldi basis vectors $v_{1}, \\dots, v_{m+1}$. Neglect the storage cost of small dense structures such as the Hessenberg matrix (which is $(m+1) \\times m$). Under this model, determine the maximum feasible restart $m$ in terms of $S$.\n\nFor the Poisson case, assume an effective Symmetric Positive Definite (SPD) preconditioner has been applied so that the spectrum of the preconditioned operator lies in the real interval $[\\alpha_{\\mathrm{P}}, \\beta_{\\mathrm{P}}]$ with $\\alpha_{\\mathrm{P}} = 0.2$ and $\\beta_{\\mathrm{P}} = 2.0$. For the Helmholtz case, assume a shifted-Laplacian-type preconditioner yields a normal operator whose spectrum lies in the union of two real intervals symmetric about the origin, namely $[-\\beta_{\\mathrm{H}}, -\\alpha_{\\mathrm{H}}] \\cup [\\alpha_{\\mathrm{H}}, \\beta_{\\mathrm{H}}]$ with $\\alpha_{\\mathrm{H}} = 0.1$ and $\\beta_{\\mathrm{H}} = 2.0$. Let the initial residual norm be $\\|r_{0}\\|$, and target a reduction to $10^{-8} \\|r_{0}\\|$.\n\nUse the following foundational facts:\n- GMRES constructs a degree-$m$ residual polynomial $p_{m}$ with $p_{m}(0) = 1$ that minimizes the residual norm over the Krylov subspace; for normal matrices, $\\|r_{m}\\|/\\|r_{0}\\| \\le \\min_{p \\in \\Pi_{m},\\, p(0)=1} \\max_{\\lambda \\in \\Lambda(A)} |p(\\lambda)|$, where $\\Lambda(A)$ denotes the spectrum of $A$ and $\\Pi_{m}$ is the set of polynomials of degree at most $m$.\n- For spectra contained in a real interval $[\\alpha, \\beta] \\subset (0,\\infty)$, the minimax bound using Chebyshev polynomials gives $\\|r_{m}\\|/\\|r_{0}\\| \\le 2 \\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^{m}$, where $\\kappa = \\beta/\\alpha$ is the condition number of the interval.\n- For spectra contained in the symmetric pair of intervals $[-\\beta, -\\alpha] \\cup [\\alpha, \\beta]$ with $0<\\alpha<\\beta$, the optimal degree-$m$ polynomial with $p(0)=1$ can be taken even in $z$ (i.e., in $z^{2}$), effectively yielding a Chebyshev bound in the squared interval $[\\alpha^{2}, \\beta^{2}]$ and the degree reduces to $\\lfloor m/2 \\rfloor$: $\\|r_{m}\\|/\\|r_{0}\\| \\le 2 \\left(\\frac{\\sqrt{\\kappa_{s}}-1}{\\sqrt{\\kappa_{s}}+1}\\right)^{\\lfloor m/2 \\rfloor}$ with $\\kappa_{s} = \\beta^{2}/\\alpha^{2}$.\n\nAssume $S = 80$ and that the GMRES($m)$ algorithm follows the storage model above. Determine:\n1. The maximum feasible restart $m$.\n2. The minimum integer number of GMRES($m)$ cycles required to achieve $\\|r\\| \\le 10^{-8}\\|r_{0}\\|$ for the Poisson case.\n3. The minimum integer number of GMRES($m)$ cycles required to achieve $\\|r\\| \\le 10^{-8}\\|r_{0}\\|$ for the Helmholtz case.\n\nReport the three answers together as a single row matrix using the LaTeX pmatrix environment. If any numerical evaluation is needed, give the exact integers; no rounding by significant figures is required.",
            "solution": "The user has provided a problem in three parts concerning the Generalized Minimal Residual (GMRES) method. The problem is well-defined, scientifically sound, and contains all necessary information for a solution. I will address each part in the order presented.\n\n### Part 1: Maximum Feasible Restart Parameter $m$\n\nThe problem specifies a memory budget that allows for the simultaneous storage of at most $S$ vectors of length $N$. The storage model for a single cycle of the GMRES($m$) algorithm is explicitly defined. The total number of $N$-length vectors stored is the sum of:\n\\begin{itemize}\n    \\item The current solution vector, $x$: $1$ vector.\n    \\item The current residual vector, $r$: $1$ vector.\n    \\item A work vector, $w$: $1$ vector.\n    \\item The Arnoldi basis vectors, $v_1, \\dots, v_{m+1}$: $m+1$ vectors.\n\\end{itemize}\nThe total number of stored vectors is therefore $1 + 1 + 1 + (m+1)$, which simplifies to $m+4$.\n\nThis total must not exceed the memory budget $S$. Thus, we have the inequality:\n$$ m + 4 \\le S $$\nGiven that the storage budget is $S = 80$, we can substitute this value into the inequality:\n$$ m + 4 \\le 80 $$\nSolving for $m$, we find:\n$$ m \\le 80 - 4 $$\n$$ m \\le 76 $$\nThe maximum feasible restart parameter $m$ is the largest integer that satisfies this condition. Therefore, the maximum restart value is $m=76$. This value will be used for the subsequent parts of the problem.\n\n### Part 2: Minimum Cycles for the Poisson Case\n\nFor the preconditioned Poisson problem, the spectrum of the operator lies in the real interval $[\\alpha_{\\mathrm{P}}, \\beta_{\\mathrm{P}}]$ with $\\alpha_{\\mathrm{P}} = 0.2$ and $\\beta_{\\mathrm{P}} = 2.0$. The condition number of this spectral interval is:\n$$ \\kappa_{\\mathrm{P}} = \\frac{\\beta_{\\mathrm{P}}}{\\alpha_{\\mathrm{P}}} = \\frac{2.0}{0.2} = 10 $$\nThe problem provides the convergence bound for one cycle of GMRES($m$):\n$$ \\frac{\\|r_{m}\\|}{\\|r_{0}\\|} \\le 2 \\left(\\frac{\\sqrt{\\kappa_{\\mathrm{P}}}-1}{\\sqrt{\\kappa_{\\mathrm{P}}}+1}\\right)^{m} $$\nLet $\\rho_{\\mathrm{P}}$ be the residual reduction factor for one cycle. Using $m=76$ and $\\kappa_{\\mathrm{P}}=10$, we have:\n$$ \\rho_{\\mathrm{P}} = 2 \\left(\\frac{\\sqrt{10}-1}{\\sqrt{10}+1}\\right)^{76} $$\nLet $C_{\\mathrm{P}}$ be the minimum integer number of cycles required to achieve a total residual reduction of $10^{-8}$. The condition is:\n$$ (\\rho_{\\mathrm{P}})^{C_{\\mathrm{P}}} \\le 10^{-8} $$\nWe first evaluate $\\rho_{\\mathrm{P}}$. The term inside the parenthesis is $\\frac{\\sqrt{10}-1}{\\sqrt{10}+1} \\approx 0.5195$. Raising this to the power of $76$ yields an extremely small number. Let's analyze this more formally.\nFor one cycle ($C_{\\mathrm{P}}=1$), we check if $\\rho_{\\mathrm{P}} \\le 10^{-8}$:\n$$ 2 \\left(\\frac{\\sqrt{10}-1}{\\sqrt{10}+1}\\right)^{76} \\le 10^{-8} $$\nTaking the natural logarithm of both sides:\n$$ \\ln(2) + 76 \\ln\\left(\\frac{\\sqrt{10}-1}{\\sqrt{10}+1}\\right) \\le \\ln(10^{-8}) $$\n$$ \\ln(2) + 76 \\ln\\left(\\frac{\\sqrt{10}-1}{\\sqrt{10}+1}\\right) \\le -8 \\ln(10) $$\nUsing numerical values, $\\ln(2) \\approx 0.6931$ and $\\ln(10) \\approx 2.3026$. The term $\\ln\\left(\\frac{\\sqrt{10}-1}{\\sqrt{10}+1}\\right) \\approx -0.6549$.\n$$ 0.6931 + 76 \\times (-0.6549) \\approx 0.6931 - 49.7724 = -49.0793 $$\nThe right side of the inequality is $-8 \\times 2.3026 = -18.4208$.\nThe inequality becomes:\n$$ -49.0793 \\le -18.4208 $$\nThis inequality is true. The reduction factor in a single cycle is much smaller than the required tolerance of $10^{-8}$. Therefore, a single cycle is sufficient.\nThe minimum integer number of cycles is $C_{\\mathrm{P}} = 1$.\n\n### Part 3: Minimum Cycles for the Helmholtz Case\n\nFor the preconditioned Helmholtz problem, the spectrum is normal and contained in $[-\\beta_{\\mathrm{H}}, -\\alpha_{\\mathrm{H}}] \\cup [\\alpha_{\\mathrm{H}}, \\beta_{\\mathrm{H}}]$ with $\\alpha_{\\mathrm{H}} = 0.1$ and $\\beta_{\\mathrm{H}} = 2.0$. The convergence bound uses a modified condition number $\\kappa_s$:\n$$ \\kappa_s = \\frac{\\beta_{\\mathrm{H}}^2}{\\alpha_{\\mathrm{H}}^2} = \\frac{(2.0)^2}{(0.1)^2} = \\frac{4}{0.01} = 400 $$\nThe convergence factor for one cycle, $\\rho_{\\mathrm{H}}$, is given by:\n$$ \\rho_{\\mathrm{H}} = 2 \\left(\\frac{\\sqrt{\\kappa_s}-1}{\\sqrt{\\kappa_s}+1}\\right)^{\\lfloor m/2 \\rfloor} $$\nUsing $m=76$, we get $\\lfloor m/2 \\rfloor = \\lfloor 76/2 \\rfloor = 38$. And $\\sqrt{\\kappa_s} = \\sqrt(400)=20$.\n$$ \\rho_{\\mathrm{H}} = 2 \\left(\\frac{20-1}{20+1}\\right)^{38} = 2 \\left(\\frac{19}{21}\\right)^{38} $$\nLet $C_{\\mathrm{H}}$ be the minimum number of cycles. We need to find the smallest integer $C_{\\mathrm{H}}$ such that:\n$$ (\\rho_{\\mathrm{H}})^{C_{\\mathrm{H}}} \\le 10^{-8} $$\nTaking the natural logarithm:\n$$ C_{\\mathrm{H}} \\ln(\\rho_{\\mathrm{H}}) \\le \\ln(10^{-8}) = -8 \\ln(10) $$\nWe calculate $\\ln(\\rho_{\\mathrm{H}})$:\n$$ \\ln(\\rho_{\\mathrm{H}}) = \\ln\\left(2 \\left(\\frac{19}{21}\\right)^{38}\\right) = \\ln(2) + 38 \\ln\\left(\\frac{19}{21}\\right) $$\nUsing numerical values, $\\ln(2) \\approx 0.693147$ and $\\ln(19/21) \\approx -0.100083$.\n$$ \\ln(\\rho_{\\mathrm{H}}) \\approx 0.693147 + 38 \\times (-0.100083) \\approx 0.693147 - 3.803154 = -3.110007 $$\nThe inequality for $C_{\\mathrm{H}}$ becomes (note that we divide by a negative number, so the inequality flips):\n$$ C_{\\mathrm{H}} \\ge \\frac{-8 \\ln(10)}{\\ln(\\rho_{\\mathrm{H}})} \\approx \\frac{-18.42068}{-3.110007} \\approx 5.9229 $$\nSince $C_{\\mathrm{H}}$ must be an integer, we take the ceiling of this value:\n$$ C_{\\mathrm{H}} = \\lceil 5.9229 \\rceil = 6 $$\nThe minimum integer number of cycles required for the Helmholtz case is $C_{\\mathrm{H}} = 6$.\n\nIn summary, the three answers are:\n1.  Maximum feasible restart $m = 76$.\n2.  Minimum cycles for Poisson case $C_{\\mathrm{P}} = 1$.\n3.  Minimum cycles for Helmholtz case $C_{\\mathrm{H}} = 6$.\nThese will be reported in a single row matrix as requested.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n76 & 1 & 6\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While simple convergence bounds are useful, the true power of GMRES lies in its ability to adapt to the specific spectral properties of a matrix. This exercise explores a more sophisticated strategy for analyzing convergence when the spectrum is not a single, simple interval but instead contains a dense cluster and a few isolated 'outlier' eigenvalues. By constructing a custom residual polynomial that strategically places zeros on the outliers, you will gain insight into how GMRES can efficiently handle such common spectral distributions, a key concept behind advanced techniques like augmented Krylov methods. ",
            "id": "3440173",
            "problem": "Consider a linear system arising from a finite-volume discretization of a steady two-dimensional convection–diffusion Partial Differential Equation (PDE) on a uniform grid. After applying a right preconditioner constructed by one sweep of Incomplete Lower–Upper factorization (ILU), the preconditioned coefficient matrix $A \\in \\mathbb{C}^{n \\times n}$ is assumed to be normal (i.e., diagonalizable by a unitary transformation) and to have its spectrum contained in the union of a compact cluster and two isolated outliers:\n- The spectral cluster lies in the real interval $[0.9, 1.1]$.\n- Two outliers are located at the real points $0.2$ and $4.0$.\n\nYou apply Restarted Generalized Minimal Residual (GMRES) with cycle length $m=6$, denoted GMRES($6$). Let the initial residual be $r_{0}$, and let $r_{6}$ denote the residual after one GMRES($6$) cycle. It is known from the minimal residual characterization of GMRES that $r_{6} = p_{6}(A) r_{0}$ for some polynomial $p_{6}$ of degree at most $6$ satisfying $p_{6}(0)=1$.\n\nStarting from the minimal residual characterization and the assumption that $A$ is normal with an orthonormal eigenbasis, construct a degree-$6$ residual polynomial $p_{6}$ that:\n1. Places simple zeros at the two outlier eigenvalues to maximally damp those components.\n2. Minimizes the worst-case residual amplification over the cluster interval $[0.9,1.1]$ subject to the constraint $p_{6}(0)=1$ by using an appropriate scaled and shifted Chebyshev polynomial of the first kind.\n\nThen, using your constructed polynomial, derive an explicit bound for the worst-case residual reduction factor over one GMRES($6$) cycle,\n$$\n\\frac{\\|r_{6}\\|_{2}}{\\|r_{0}\\|_{2}},\n$$\nin terms of the supremum of $|p_{6}(\\lambda)|$ over the spectrum of $A$.\n\nFinally, evaluate your bound numerically to produce a single real number. Round your final numeric answer to four significant figures. No units are required.",
            "solution": "The user wants me to solve the problem.\nThe problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It is a standard problem in the analysis of iterative methods for numerical linear algebra. I will now proceed with a full solution.\n\nThe problem asks for a bound on the residual reduction factor for one cycle of Restarted GMRES($6$). For a normal matrix $A$, the GMRES residual $r_m$ after $m$ iterations with initial residual $r_0$ satisfies the inequality:\n$$\n\\frac{\\|r_m\\|_2}{\\|r_0\\|_2} \\le \\min_{p_m \\in \\mathcal{P}_m, p_m(0)=1} \\max_{\\lambda \\in \\sigma(A)} |p_m(\\lambda)|\n$$\nHere, $\\mathcal{P}_m$ is the space of polynomials of degree at most $m$, and $\\sigma(A)$ is the spectrum of $A$. The problem does not ask for the true GMRES minimal polynomial, but rather to construct a specific candidate polynomial $p_6(\\lambda)$ of degree $6$ and evaluate its maximum magnitude over the given spectral inclusion set. This maximum magnitude serves as an upper bound for the residual reduction factor.\n\nThe spectral inclusion set for the matrix $A$ is given as $\\Omega = [0.9, 1.1] \\cup \\{0.2, 4.0\\}$. The bound is therefore $\\max_{\\lambda \\in \\Omega} |p_6(\\lambda)|$.\n\nThe polynomial $p_6(\\lambda)$ must satisfy three conditions:\n1. It is a polynomial of degree at most $6$ satisfying $p_6(0) = 1$.\n2. It has simple zeros at the outliers, i.e., $p_6(0.2) = 0$ and $p_6(4.0) = 0$.\n3. It minimizes the worst-case amplification over the cluster interval $[0.9, 1.1]$ by using a scaled and shifted Chebyshev polynomial.\n\nWe construct $p_6(\\lambda)$ as a product of two polynomials, one for the outliers and one for the cluster.\nLet the component for the outliers be $P_{\\text{outlier}}(\\lambda)$. To place zeros at $\\lambda=0.2$ and $\\lambda=4.0$ and satisfy the condition at $\\lambda=0$, we define:\n$$\nP_{\\text{outlier}}(\\lambda) = \\frac{(\\lambda - 0.2)(\\lambda - 4.0)}{(0 - 0.2)(0 - 4.0)} = \\frac{\\lambda^2 - 4.2\\lambda + 0.8}{0.8} = 1.25(\\lambda^2 - 4.2\\lambda + 0.8)\n$$\nThis is a degree-$2$ polynomial with $P_{\\text{outlier}}(0.2)=0$, $P_{\\text{outlier}}(4.0)=0$, and $P_{\\text{outlier}}(0)=1$.\n\nLet the component for the cluster be $P_{\\text{cluster}}(\\lambda)$. This must be a polynomial of degree $m-2=4$. The problem states to use a Chebyshev polynomial to minimize its magnitude on $[0.9, 1.1]$, subject to the constraint that the final polynomial $p_6(0)=1$. Since we have already enforced $P_{\\text{outlier}}(0)=1$, we must also enforce $P_{\\text{cluster}}(0)=1$.\n\nThe polynomial of degree $k$ with the smallest maximum magnitude on an interval $[a,b]$ among all polynomials satisfying $p(x_0)=1$ for some $x_0 \\notin [a,b]$ is a scaled and shifted Chebyshev polynomial of the first kind, $T_k(x)$.\nFirst, we define a linear map $x(\\lambda)$ that transforms the interval $[0.9, 1.1]$ to $[-1, 1]$.\nLet $\\lambda = c x + d$.\nFor $x=-1$, $\\lambda=0.9 \\implies 0.9 = -c + d$.\nFor $x=1$, $\\lambda=1.1 \\implies 1.1 = c + d$.\nSolving this system gives $d=1.0$ and $c=0.1$.\nThus, $\\lambda = 0.1x + 1.0$, and the inverse mapping is $x(\\lambda) = \\frac{\\lambda - 1.0}{0.1} = 10(\\lambda - 1)$.\n\nThe polynomial for the cluster, $P_{\\text{cluster}}(\\lambda)$, is of degree $4$. It is given by:\n$$\nP_{\\text{cluster}}(\\lambda) = \\frac{T_4(x(\\lambda))}{T_4(x(0))}\n$$\nWe need to evaluate the denominator. The point $\\lambda=0$ maps to $x(0) = 10(0 - 1) = -10$.\nThe Chebyshev polynomial of degree $4$ is $T_4(x) = 8x^4 - 8x^2 + 1$.\nSo, $T_4(-10) = 8(-10)^4 - 8(-10)^2 + 1 = 8(10000) - 8(100) + 1 = 80000 - 800 + 1 = 79201$.\nThus, the cluster polynomial is:\n$$\nP_{\\text{cluster}}(\\lambda) = \\frac{T_4(10(\\lambda - 1))}{79201}\n$$\n\nThe final constructed polynomial $p_6(\\lambda)$ is the product of these two components:\n$$\np_6(\\lambda) = P_{\\text{outlier}}(\\lambda) P_{\\text{cluster}}(\\lambda) = \\left( \\frac{(\\lambda - 0.2)(\\lambda - 4.0)}{0.8} \\right) \\left( \\frac{T_4(10(\\lambda - 1))}{79201} \\right)\n$$\nThis polynomial has degree $2+4=6$, satisfies $p_6(0) = P_{\\text{outlier}}(0) P_{\\text{cluster}}(0) = 1 \\times 1 = 1$, and has zeros at $\\lambda=0.2$ and $\\lambda=4.0$.\n\nNow, we must find the maximum magnitude of $p_6(\\lambda)$ on the spectral inclusion set $\\Omega = [0.9, 1.1] \\cup \\{0.2, 4.0\\}$.\n$$\n\\frac{\\|r_6\\|_2}{\\|r_0\\|_2} \\le \\max_{\\lambda \\in \\Omega} |p_6(\\lambda)|\n$$\n- For the outlier eigenvalues $\\lambda=0.2$ and $\\lambda=4.0$, by construction, $|p_6(0.2)|=0$ and $|p_6(4.0)|=0$.\n- For the cluster interval $\\lambda \\in [0.9, 1.1]$, we need to find $\\max_{\\lambda \\in [0.9, 1.1]} |p_6(\\lambda)|$.\nOn this interval, the variable $x=10(\\lambda-1)$ is in $[-1,1]$. The property of Chebyshev polynomials is that $\\max_{x \\in [-1,1]} |T_4(x)| = 1$.\nTherefore, for $\\lambda \\in [0.9, 1.1]$, the cluster component is bounded by:\n$$\n|P_{\\text{cluster}}(\\lambda)| = \\frac{|T_4(10(\\lambda - 1))|}{79201} \\le \\frac{1}{79201}\n$$\nThe outlier component's magnitude is $|P_{\\text{outlier}}(\\lambda)| = |1.25(\\lambda^2 - 4.2\\lambda + 0.8)|$. The derivative of the inner quadratic is $2\\lambda - 4.2$, which is negative for $\\lambda<2.1$. So the quadratic is decreasing on $[0.9, 1.1]$.\n$P_{\\text{outlier}}(0.9) = 1.25(0.9-0.2)(0.9-4.0) = 1.25(0.7)(-3.1) = -2.7125$.\n$P_{\\text{outlier}}(1.1) = 1.25(1.1-0.2)(1.1-4.0) = 1.25(0.9)(-2.9) = -3.2625$.\nSince the function is monotonic on the interval, its magnitude $|P_{\\text{outlier}}(\\lambda)|$ is also monotonic (and increasing) on $[0.9, 1.1]$.\nIts maximum magnitude on the interval occurs at $\\lambda=1.1$, where $|P_{\\text{outlier}}(1.1)| = 3.2625$.\n\nNow we analyze the product $|p_6(\\lambda)| = |P_{\\text{outlier}}(\\lambda)| \\cdot |P_{\\text{cluster}}(\\lambda)|$.\n$|P_{\\text{outlier}}(\\lambda)|$ is an increasing function on $[0.9, 1.1]$.\n$|P_{\\text{cluster}}(\\lambda)|$ is an oscillatory function whose local maxima (where its value is $1/79201$) occur at points corresponding to $x$ where $|T_4(x)|=1$. These points are $x \\in \\{-1, -\\frac{\\sqrt{2}}{2}, 0, \\frac{\\sqrt{2}}{2}, 1\\}$, which correspond to $\\lambda \\in \\{0.9, 1-\\frac{\\sqrt{2}}{20}, 1, 1+\\frac{\\sqrt{2}}{20}, 1.1\\}$.\nBecause $|P_{\\text{outlier}}(\\lambda)|$ is strictly increasing, the maximum of the product will be achieved at the largest of these $\\lambda$ values, which is $\\lambda=1.1$.\n\nLet's evaluate $|p_6(1.1)|$:\n$$\n|p_6(1.1)| = |P_{\\text{outlier}}(1.1)| \\cdot |P_{\\text{cluster}}(1.1)|\n$$\n$$\n|P_{\\text{cluster}}(1.1)| = \\left| \\frac{T_4(10(1.1 - 1))}{79201} \\right| = \\left| \\frac{T_4(1)}{79201} \\right| = \\frac{1}{79201}\n$$\nSo, the maximum value on the cluster interval is:\n$$\n\\max_{\\lambda \\in [0.9, 1.1]} |p_6(\\lambda)| = |p_6(1.1)| = 3.2625 \\times \\frac{1}{79201} = \\frac{3.2625}{79201}\n$$\nThe overall maximum of $|p_6(\\lambda)|$ over $\\Omega$ is therefore this value. The bound on the residual reduction factor is:\n$$\n\\frac{\\|r_6\\|_2}{\\|r_0\\|_2} \\le \\frac{3.2625}{79201}\n$$\nFinally, we evaluate this numerically:\n$$\n\\frac{3.2625}{79201} \\approx 0.00004119266...\n$$\nRounding to four significant figures, we get $0.00004119$.",
            "answer": "$$\n\\boxed{0.00004119}\n$$"
        },
        {
            "introduction": "Theoretical convergence estimates based solely on eigenvalues can be misleading, as they are only rigorous for normal matrices. This hands-on practice delves into the critical issue of non-normality, a common feature of matrices from convection-dominated problems, which can cause GMRES to slow down dramatically or even stagnate completely. Through a guided implementation, you will construct a strongly non-normal advection operator and witness this stagnation firsthand, contrasting it with the robust convergence for a related normal operator to solidify your understanding of this crucial practical pitfall. ",
            "id": "3440218",
            "problem": "Consider the one-dimensional linear advection partial differential equation $u_t + a u_x = 0$ on the interval $[0,1]$ with constant advection speed $a > 0$. A standard grid-based discretization at $n$ interior points with explicit inflow boundary and absorbing outflow can be viewed from two complementary perspectives:\n\n1. A semi-discrete generator built from a one-sided (upwind) difference for $u_x$, which yields a lower-bidiagonal matrix with a single eigenvalue and a single Jordan chain, hence strongly non-normal.\n\n2. A discrete-time propagator that maps the state forward exactly one grid cell when the Courant number equals one, implemented as a strict lower shift, corresponding to a one-step advection over a uniform mesh with absorbing outflow.\n\nIn this problem, you will construct a discretized advection operator $A$ that exhibits strong non-normality via the strict lower shift and analyze the behavior of restarted Generalized Minimal Residual (GMRES) method of dimension $m$, denoted GMRES($m$), including the role of the residual polynomials $p_m(A)$ that satisfy $p_m(0) = 1$ and minimize $\\lVert p_m(A) r_0 \\rVert_2$ at each cycle for initial residual $r_0$. You will contrast this with a normal operator obtained from a central-difference periodic discretization.\n\nStarting from the fundamental base consisting of:\n- The advection law $u_t + a u_x = 0$,\n- Linear advection on a uniform grid with inflow and absorbing outflow yielding a discrete-time shift operator for Courant number one,\n- Krylov subspace definition $\\mathcal{K}_m(A,r_0) = \\mathrm{span}\\{r_0, A r_0, \\dots, A^{m-1} r_0\\}$,\n- The Generalized Minimal Residual (GMRES) method that minimizes the residual norm over $x_0 + \\mathcal{K}_m(A,r_0)$ and the residual polynomial characterization $r_m = p_m(A) r_0$ with $p_m(0)=1$,\n\ncomplete the following tasks:\n\na) Construct the strict lower shift matrix $A \\in \\mathbb{R}^{n \\times n}$ defined by $(A)_{i,i-1} = 1$ for $i=2,\\dots,n$ and all other entries zero. Interpret $A$ as the one-step discrete advection propagator with absorbing outflow. Explain why $A$ is strongly non-normal and describe its Jordan form.\n\nb) Implement restarted GMRES($m$) with $x_0 = 0$ using the Arnoldi process. At each cycle, perform $m$ inner steps, update the approximate solution, and record the residual norm at the end of each cycle. For $A$ given by the strict lower shift, show conditions under which GMRES($m$) stagnates, meaning the residual norm does not decrease across cycles.\n\nc) Analyze the role of the residual polynomial $p_m(A)$ for the strict lower shift when the initial residual has its first nonzero component at index $j$, i.e., $r_0 = \\alpha e_j + (\\text{higher indices})$. Determine how the constraint $p_m(0)=1$ and the image $A \\mathcal{K}_m(A,r_0)$ restrict the possible residual components, and explain why, for many right-hand sides (e.g., $r_0 = e_1$), the minimizing $p_m$ yields $r_m = r_0$, hence stagnation.\n\nd) Construct a normal advection-related operator $A_{\\mathrm{per}}$ on a periodic grid using the central-difference spatial discretization of $u_x$: define the forward shift $P$ by $(P)_{i,(i \\bmod n)+1} = 1$, let $h = 1/n$, and form $D = \\frac{1}{2h} (P - P^\\top)$, then set $A_{\\mathrm{per}} = I - \\gamma D$ for a fixed $\\gamma > 0$. Explain why $D$ is skew-symmetric and thus normal, and argue that $A_{\\mathrm{per}}$ is normal and better behaved for GMRES($m$), typically avoiding stagnation.\n\nYour program must implement the GMRES($m$) algorithm and evaluate the following test suite. For each test case, run a fixed number of restart cycles and output a boolean indicating stagnation, defined as the absolute difference between the initial and final residual norms being less than $10^{-10}$ and the residual norms across cycles being nonincreasing with changes below $10^{-10}$ at every cycle.\n\nTest suite:\n- Case 1: $A$ is the strict lower shift, $n=60$, $m=8$, $r_0 = e_1$, cycles $= 5$.\n- Case 2: $A$ is the strict lower shift, $n=60$, $m=8$, $r_0$ is a random vector uniformly distributed in $[-1,1]^n$ with deterministic seed $42$, cycles $= 5$.\n- Case 3: $A_{\\mathrm{per}} = I - \\gamma D$ with periodic central-difference $D$ as above, $n=60$, $\\gamma = 0.1$, $m=8$, $r_0$ is a random vector uniformly distributed in $[-1,1]^n$ with deterministic seed $7$, cycles $= 5$.\n- Case 4: $A$ is the strict lower shift, $n=30$, $m=1$, $r_0$ is a random vector uniformly distributed in $[-1,1]^n$ with deterministic seed $123$, cycles $= 6$.\n\nYour program should produce a single line of output containing the stagnation results for the four cases as a comma-separated list enclosed in square brackets (e.g., \"[$\\text{result1}$,$\\text{result2}$,$\\text{result3}$,$\\text{result4}$]\"), where each $\\text{result}$ is either \"True\" or \"False\". No physical units or angle units are involved; all values are dimensionless.",
            "solution": "The problem requires an analysis of the restarted Generalized Minimal Residual method, GMRES($m$), by contrasting its behavior on two different linear operators derived from the one-dimensional linear advection equation, $u_t + a u_x = 0$. One operator, a strict lower shift matrix $A$, is strongly non-normal and serves as a model for pure advection with absorbing outflow. The other, $A_{\\mathrm{per}}$, is a normal matrix derived from a central-difference discretization on a periodic domain. The core of the analysis lies in understanding how the algebraic properties of these operators, specifically normality, influence the convergence and potential stagnation of GMRES($m$).\n\nWe are tasked with solving the linear system $A x = b$. The GMRES($m$) algorithm starts with an initial guess $x_0$ and computes the initial residual $r_0 = b - A x_0$. Since the problem specifies $x_0=0$, the initial residual is simply the right-hand side, $r_0=b$. The algorithm then iteratively refines the solution.\n\n**a) The Strict Lower Shift Operator: Construction and Properties**\n\nThe strict lower shift matrix $A \\in \\mathbb{R}^{n \\times n}$ is defined by its entries $(A)_{i,j} = \\delta_{i, j+1}$, or more explicitly, $(A)_{i, i-1} = 1$ for $i=2, \\ldots, n$ and all other entries are zero.\n$$\nA = \\begin{pmatrix}\n0 & 0 & \\dots & 0 & 0 \\\\\n1 & 0 & \\dots & 0 & 0 \\\\\n0 & 1 & \\dots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & \\dots & 1 & 0\n\\end{pmatrix}\n$$\nThis operator models a discrete, one-step advection process on a uniform grid of $n$ points. If a vector $v \\in \\mathbb{R}^n$ represents the state on the grid, then $Av$ represents the state after one time step where the Courant number is unity. The value at grid point $j-1$ is shifted to grid point $j$. The value at grid point $n$ is shifted out of the domain (absorbing outflow), and a value of $0$ enters at grid point $1$ (zero inflow condition).\n\nA matrix is defined as normal if it commutes with its conjugate transpose, i.e., $A^*A = AA^*$. For a real matrix, this condition is $A^\\top A = AA^\\top$. The transpose of $A$ is the strict upper shift matrix:\n$$\nA^\\top = \\begin{pmatrix}\n0 & 1 & 0 & \\dots & 0 \\\\\n0 & 0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\dots & 1 \\\\\n0 & 0 & 0 & \\dots & 0\n\\end{pmatrix}\n$$\nWe compute the products:\n$$\nA A^\\top = \\text{diag}(0, 1, 1, \\dots, 1) \\in \\mathbb{R}^{n \\times n}\n$$\n$$\nA^\\top A = \\text{diag}(1, 1, \\dots, 1, 0) \\in \\mathbb{R}^{n \\times n}\n$$\nSince $A A^\\top \\neq AA^\\top$, the matrix $A$ is non-normal. The significant difference between these products indicates strong non-normality.\n\nThe eigenvalues of $A$ are the roots of its characteristic polynomial, $\\det(A - \\lambda I) = (-\\lambda)^n=0$. Thus, the only eigenvalue is $\\lambda=0$ with algebraic multiplicity $n$. The geometric multiplicity is the dimension of the null space $\\text{ker}(A)$, which is spanned by the vector $e_1 = [1, 0, \\dots, 0]^\\top$. Since the geometric multiplicity ($1$) is strictly less than the algebraic multiplicity ($n$), the matrix is defective and not diagonalizable. It is a nilpotent matrix, as $A^k \\neq 0$ for $k  n$ but $A^n=0$. Its minimal polynomial is $p(\\lambda) = \\lambda^n$. This structure implies that the Jordan form of $A$ consists of a single Jordan block of size $n$ corresponding to the eigenvalue $0$, which we denote $J_n(0)$. The matrix $A$ is permutation-similar to $J_n(0)^\\top$. This extreme defectiveness is a hallmark of strong non-normality.\n\n**b) Restarted GMRES($m$) and Stagnation Conditions**\n\nGMRES($m$) finds an approximate solution $x_m \\in x_0 + \\mathcal{K}_m(A, r_0)$ that minimizes the Euclidean norm of the residual $r_m = b - Ax_m$. The search space, $\\mathcal{K}_m(A, r_0) = \\text{span}\\{r_0, A r_0, \\dots, A^{m-1} r_0\\}$, is the Krylov subspace of dimension at most $m$. After $m$ iterations of the Arnoldi process, the solution is updated, and the algorithm is restarted using the new residual as the starting vector for the next Krylov subspace.\n\nStagnation occurs when a restart cycle fails to reduce the residual norm. For the shift operator $A$, let's analyze the structure of the residual update. The updated residual $r_m$ can be expressed as $r_m = r_0 - A z_m$ for some vector $z_m \\in \\mathcal{K}_m(A, r_0)$. A key property of the shift matrix $A$ is that for any vector $v \\in \\mathbb{R}^n$, the first component of $Av$ is zero: $(Av)_1 = 0$. Consequently, the first component of $A z_m$ must also be zero, regardless of the choice of $z_m \\in \\mathcal{K}_m(A, r_0)$.\n\nThis leads to a crucial observation for the first component of the residual:\n$$\n(r_m)_1 = (r_0)_1 - (A z_m)_1 = (r_0)_1 - 0 = (r_0)_1\n$$\nThe first component of the residual is invariant throughout the GMRES process. Therefore, if $(r_0)_1 \\neq 0$, the residual norm can never be reduced to zero. $\\lVert r_m \\rVert_2 \\geq |(r_m)_1| = |(r_0)_1|$.\n\nStagnation in the strictest sense (no reduction in norm) occurs if the minimizing choice for $z_m$ is $z_m = 0$. This happens when the subspace $A \\mathcal{K}_m(A, r_0)$ is orthogonal to $r_0$, or more generally, when any vector in $A \\mathcal{K}_m(A, r_0)$ increases the residual norm.\n\n**c) Residual Polynomial Analysis for Stagnation**\n\nThe GMRES residual can be characterized by a polynomial. The method finds a polynomial $p_m$ of degree at most $m$ satisfying $p_m(0)=1$ that minimizes $\\lVert p_m(A) r_0 \\rVert_2$. Any such polynomial can be written as $p_m(z) = 1 + q_m(z)$, where $q_m(z)$ is a polynomial of degree at most $m$ with $q_m(0)=0$, i.e., $q_m(z) = c_1 z + c_2 z^2 + \\dots + c_m z^m$.\n\nThe residual is $r_m = p_m(A)r_0 = (I + c_1 A + \\dots + c_m A^m)r_0$.\nConsider the case where the initial residual is $r_0 = e_1$. The Krylov vectors are $A^k r_0 = A^k e_1 = e_{k+1}$ for $k=0, \\dots, n-1$.\nThe residual vector is:\n$$\nr_m = e_1 + c_1 A e_1 + \\dots + c_m A^m e_1 = e_1 + c_1 e_2 + \\dots + c_m e_{m+1}\n$$\nThe squared norm of this residual is:\n$$\n\\lVert r_m \\rVert_2^2 = \\lVert e_1 + c_1 e_2 + \\dots + c_m e_{m+1} \\rVert_2^2 = \\lVert e_1 \\rVert_2^2 + \\sum_{k=1}^m |c_k|^2 = 1 + \\sum_{k=1}^m |c_k|^2\n$$\nThis expression is minimized when all coefficients $c_k=0$ for $k=1, \\dots, m$. This corresponds to the polynomial $p_m(z)=1$. The resulting minimal residual is $r_m = I r_0 = r_0$. The residual norm remains unchanged, and GMRES($m$) stagnates for any $m  n$.\n\nMore generally, if $r_0$ has its first non-zero component at index $j$ ($r_0 = \\alpha_j e_j + \\dots$), then any vector $v$ in the Krylov subspace $\\mathcal{K}_m(A, r_0)$ will have its first $j-1$ components equal to zero. Any vector $w$ in the image space $A \\mathcal{K}_m(A, r_0)$ will have its first $j$ components equal to zero. The residual is $r_m = r_0 - w$, so the first $j$ components of $r_m$ are identical to those of $r_0$. This severely restricts the ability of GMRES to reduce the residual, often leading to stagnation or extremely slow convergence.\n\n**d) The Normal Operator for Periodic Advection**\n\nA different discretization arises from considering the domain $[0,1]$ to be periodic. Using a central difference for the spatial derivative $u_x$ on a grid with spacing $h=1/n$ leads to a discrete operator $D$. Let $P$ be the forward circulant shift matrix, defined by $(P)_{i,j}=1$ if $j \\equiv i+1 \\pmod n$ (using 0-based indexing $j=(i+1)\\%n$ or 1-based $j=(i \\pmod n) + 1$), and $0$ otherwise. $P$ is a permutation matrix and is therefore orthogonal (unitary), so $P^{-1} = P^\\top$. $P^\\top$ is the backward circulant shift.\n\nThe central difference operator is $D = \\frac{1}{2h}(P - P^\\top)$. We check its symmetry:\n$$\nD^\\top = \\frac{1}{2h}(P - P^\\top)^\\top = \\frac{1}{2h}(P^\\top - (P^\\top)^\\top) = \\frac{1}{2h}(P^\\top - P) = -D\n$$\nSince $D^\\top = -D$, $D$ is skew-symmetric. Any skew-symmetric matrix is normal, because $D D^\\top = D(-D) = -D^2$ and $D^\\top D = (-D)D = -D^2$.\n\nThe problem defines the system matrix $A_{\\mathrm{per}} = I - \\gamma D$ for some $\\gamma  0$. We check if $A_{\\mathrm{per}}$ is normal:\n$$\n(A_{\\mathrm{per}})^\\top = (I - \\gamma D)^\\top = I^\\top - \\gamma D^\\top = I + \\gamma D\n$$\n$$\nA_{\\mathrm{per}}(A_{\\mathrm{per}})^\\top = (I - \\gamma D)(I + \\gamma D) = I + \\gamma D - \\gamma D - \\gamma^2 D^2 = I - \\gamma^2 D^2\n$$\n$$\n(A_{\\mathrm{per}})^\\top A_{\\mathrm{per}} = (I + \\gamma D)(I - \\gamma D) = I - \\gamma D + \\gamma D - \\gamma^2 D^2 = I - \\gamma^2 D^2\n$$\nSince $A_{\\mathrm{per}}(A_{\\mathrm{per}})^\\top = (A_{\\mathrm{per}})^\\top A_{\\mathrm{per}}$, the matrix $A_{\\mathrm{per}}$ is normal.\n\nA normal matrix is unitarily diagonalizable. The convergence of GMRES on a normal matrix is governed by the distribution of its eigenvalues. The eigenvalues of a real skew-symmetric matrix $D$ are purely imaginary, say $i\\mu_k$. The eigenvalues of $A_{\\mathrm{per}}$ are thus $\\lambda_k = 1 - \\gamma (i\\mu_k) = 1 - i\\gamma\\mu_k$. They all lie on the vertical line $\\text{Re}(z)=1$ in the complex plane. For a normal matrix, GMRES finds the optimal polynomial approximation on the eigenvalues. Unlike the non-normal case, there is no transient growth of powers of the operator, and the residual norm is guaranteed to be non-increasing in each step of the Arnoldi process. Stagnation of the type seen with the shift matrix, where the method makes zero progress, is not possible unless the solution has been found. Therefore, $A_{\\mathrm{per}}$ is expected to be much better behaved for GMRES($m$), avoiding the severe stagnation issues of the non-normal shift operator.\n\n```python\nimport numpy as np\n\ndef gmres_m(A, b, m, cycles, x0=None):\n    \"\"\"\n    Implements the restarted Generalized Minimal Residual (GMRES(m)) method.\n\n    Args:\n        A (np.ndarray): The coefficient matrix of the linear system.\n        b (np.ndarray): The right-hand side vector.\n        m (int): The restart parameter (dimension of the Krylov subspace).\n        cycles (int): The number of restart cycles to perform.\n        x0 (np.ndarray, optional): The initial guess. Defaults to zero vector.\n\n    Returns:\n        tuple: A tuple containing:\n            - list[float]: Residual norms at the end of each cycle (including initial).\n            - np.ndarray: The final approximate solution vector.\n    \"\"\"\n    n = A.shape[0]\n    if x0 is None:\n        x = np.zeros(n)\n    else:\n        x = x0.copy()\n\n    residual_norms = []\n    \n    # Initial residual\n    r = b - A @ x\n    r_norm = np.linalg.norm(r)\n    residual_norms.append(r_norm)\n\n    for _ in range(cycles):\n        if r_norm  1e-12: # System solved\n            break\n\n        # Arnoldi process to build orthonormal basis V for Krylov subspace\n        V = np.zeros((n, m + 1))\n        H = np.zeros((m + 1, m))\n        \n        V[:, 0] = r / r_norm\n        \n        # Effective dimension of Krylov subspace in case of breakdown\n        j = 0\n        for j_iter in range(m):\n            j = j_iter\n            # w = A * v_j\n            w = A @ V[:, j]\n            \n            # Orthogonalize w against previous V vectors (Modified Gram-Schmidt)\n            for i in range(j + 1):\n                H[i, j] = V[:, i].T @ w\n                w = w - H[i, j] * V[:, i]\n            \n            H[j + 1, j] = np.linalg.norm(w)\n            \n            if H[j + 1, j]  1e-12: # Lucky breakdown: exact solution found\n                j += 1 # To make slicing work correctly later\n                break\n            \n            V[:, j + 1] = w / H[j + 1, j] \n        else:\n            j += 1 # Loop completed normally\n\n        # Solve the small least-squares problem\n        # min || beta * e_1 - H * y ||_2\n        beta_e1 = np.zeros(j + 1)\n        beta_e1[0] = r_norm\n        \n        # We need to solve for y in H[:j+1, :j] y = beta_e1[:j+1]\n        y, _, _, _ = np.linalg.lstsq(H[:j+1, :j], beta_e1, rcond=None)\n        \n        # Update solution\n        # x_m = x_0 + V_m y_m\n        x += V[:, :j] @ y\n\n        # Recompute residual for robustness\n        r = b - A @ x\n        r_norm = np.linalg.norm(r)\n        residual_norms.append(r_norm)\n        \n    return residual_norms, x\n\ndef check_stagnation(norms: list[float]) -> bool:\n    \"\"\"\n    Checks for stagnation based on the problem's specific definition.\n\n    Stagnation is defined as:\n    1. The absolute difference between the initial and final residual norms is  1e-10.\n    2. The residual norms across cycles are nonincreasing with changes  1e-10 at every cycle.\n    \"\"\"\n    initial_norm = norms[0]\n    final_norm = norms[-1]\n    \n    # Condition 1\n    if abs(initial_norm - final_norm) >= 1e-10:\n        return False\n        \n    # Condition 2\n    for i in range(1, len(norms)):\n        # Check nonincreasing and change magnitude\n        if not (norms[i] = norms[i-1] and (norms[i-1] - norms[i])  1e-10):\n            return False\n            \n    return True\n\ndef solve():\n    \"\"\"\n    Sets up and runs the four test cases specified in the problem.\n    \"\"\"\n    results = []\n\n    # Case 1: Shift operator, r0 = e1\n    n1 = 60\n    m1 = 8\n    cycles1 = 5\n    A1 = np.diag(np.ones(n1 - 1), -1)\n    b1 = np.zeros(n1)\n    b1[0] = 1.0\n    norms1, _ = gmres_m(A1, b1, m=m1, cycles=cycles1)\n    results.append(check_stagnation(norms1))\n\n    # Case 2: Shift operator, random r0\n    n2 = 60\n    m2 = 8\n    cycles2 = 5\n    A2 = np.diag(np.ones(n2 - 1), -1)\n    rng2 = np.random.default_rng(42)\n    b2 = rng2.uniform(-1, 1, n2)\n    norms2, _ = gmres_m(A2, b2, m=m2, cycles=cycles2)\n    results.append(check_stagnation(norms2))\n\n    # Case 3: Normal periodic operator, random r0\n    n3 = 60\n    gamma3 = 0.1\n    m3 = 8\n    cycles3 = 5\n    h3 = 1.0 / n3\n    # P is a forward circulant shift matrix\n    # P_ij=1 for j=(i+1)%n (0-based)\n    # This corresponds to rolling columns to the left (or rows down)\n    P3 = np.roll(np.eye(n3), 1, axis=0)\n    D3 = (1.0 / (2.0 * h3)) * (P3 - P3.T)\n    A3 = np.eye(n3) - gamma3 * D3\n    rng3 = np.random.default_rng(7)\n    b3 = rng3.uniform(-1, 1, n3)\n    norms3, _ = gmres_m(A3, b3, m=m3, cycles=cycles3)\n    results.append(check_stagnation(norms3))\n\n    # Case 4: Shift operator, m=1, random r0\n    n4 = 30\n    m4 = 1\n    cycles4 = 6\n    A4 = np.diag(np.ones(n4 - 1), -1)\n    rng4 = np.random.default_rng(123)\n    b4 = rng4.uniform(-1, 1, n4)\n    norms4, _ = gmres_m(A4, b4, m=m4, cycles=cycles4)\n    results.append(check_stagnation(norms4))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n# This is commented out to prevent execution in the XML context,\n# but can be run to generate the answer.\n# solve()\n# Expected output: [True,False,False,True]\n```",
            "answer": "[True,False,False,True]"
        }
    ]
}