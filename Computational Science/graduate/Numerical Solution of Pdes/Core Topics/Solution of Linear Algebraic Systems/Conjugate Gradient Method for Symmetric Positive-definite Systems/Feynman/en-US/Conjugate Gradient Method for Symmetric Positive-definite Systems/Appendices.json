{
    "hands_on_practices": [
        {
            "introduction": "The Conjugate Gradient method is not just a sequence of arbitrary algebraic steps; it is elegantly derived from the principle of minimizing a quadratic energy functional. This first practice takes you back to these foundational ideas. By deriving the formulas for the optimal step length $\\alpha_k$ and the residual update yourself, you will gain a much deeper appreciation for the logic embedded within the algorithm's structure .",
            "id": "3373168",
            "problem": "Consider the linear system $A x = b$ arising from the centered finite-difference discretization of the one-dimensional Poisson operator on $(0,1)$ with homogeneous Dirichlet boundary conditions, leading to a symmetric positive-definite (SPD) stiffness matrix. Let the associated quadratic functional be $J(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$. The Conjugate Gradient (CG) method generates iterates $\\{x_k\\}$ in search directions $\\{p_k\\}$ with residuals $r_k = b - A x_k$.\n\nStarting from the fundamental definition of the line-minimization at iteration $k$, namely minimizing $J(x_k + \\alpha p_k)$ over $\\alpha \\in \\mathbb{R}$, and using only the facts that $A$ is SPD, $J$ is differentiable with gradient $\\nabla J(x) = A x - b$, and standard orthogonality and $A$-conjugacy properties of the CG search directions, do the following:\n\n1. Derive the closed-form expression for the step length $\\alpha_k$ that minimizes $J(x_k + \\alpha p_k)$, expressing the result explicitly in terms of $r_k$ and $p_k$, without using any result not derived from the stated premises.\n\n2. Using only the definition of the residual $r_k = b - A x_k$, derive an explicit expression for the updated residual $r_{k+1}$ after taking the minimizing step along $p_k$.\n\nThen, for the concrete SPD system of size $4 \\times 4$,\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 \\\\\n0 & -1 & 2 & -1 \\\\\n0 & 0 & -1 & 2\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix},\n$$\nwith initialization $x_0 = 0$ (so $r_0 = b$) and the standard CG choice $p_0 = r_0$, compute the exact value of the step length $\\alpha_0$ obtained from your derived formula. Provide your final numerical value in exact form (no rounding) and without units.",
            "solution": "We begin from the foundational setup of minimizing a quadratic functional in an SPD setting. The quadratic functional is $J(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$, where $A$ is symmetric positive-definite (SPD). Its gradient is $\\nabla J(x) = A x - b$. At iteration $k$ of the Conjugate Gradient (CG) method, we consider the line $x(\\alpha) = x_k + \\alpha p_k$ and define the scalar function $\\phi(\\alpha) = J(x_k + \\alpha p_k)$. We seek the minimizer over $\\alpha \\in \\mathbb{R}$.\n\n1. Derivation of the step length $\\alpha_k$:\n\nWe compute $\\phi(\\alpha)$ explicitly:\n$$\n\\phi(\\alpha) \\;=\\; \\tfrac{1}{2} (x_k + \\alpha p_k)^{\\top} A (x_k + \\alpha p_k) \\;-\\; b^{\\top} (x_k + \\alpha p_k).\n$$\nDifferentiating with respect to $\\alpha$ yields\n$$\n\\phi'(\\alpha) \\;=\\; p_k^{\\top} A (x_k + \\alpha p_k) \\;-\\; b^{\\top} p_k\n\\;=\\; p_k^{\\top} (A x_k - b) \\;+\\; \\alpha \\, p_k^{\\top} A p_k.\n$$\nUsing the residual definition $r_k = b - A x_k$, we have $A x_k - b = - r_k$, hence\n$$\n\\phi'(\\alpha) \\;=\\; - p_k^{\\top} r_k \\;+\\; \\alpha \\, p_k^{\\top} A p_k.\n$$\nThe minimizing step $\\alpha_k$ satisfies the first-order optimality condition $\\phi'(\\alpha_k)=0$, so\n$$\n- p_k^{\\top} r_k \\;+\\; \\alpha_k \\, p_k^{\\top} A p_k \\;=\\; 0,\n\\qquad\\Rightarrow\\qquad\n\\alpha_k \\;=\\; \\frac{p_k^{\\top} r_k}{p_k^{\\top} A p_k}.\n$$\nFor the standard CG recurrence, the search direction is $p_k = r_k + \\beta_{k-1} p_{k-1}$ with $r_k$ orthogonal to $p_{k-1}$ in the Euclidean inner product, which implies\n$$\np_k^{\\top} r_k \\;=\\; (r_k + \\beta_{k-1} p_{k-1})^{\\top} r_k \\;=\\; r_k^{\\top} r_k \\;+\\; \\beta_{k-1} \\, p_{k-1}^{\\top} r_k \\;=\\; r_k^{\\top} r_k.\n$$\nTherefore, under standard CG,\n$$\n\\alpha_k \\;=\\; \\frac{r_k^{\\top} r_k}{p_k^{\\top} A p_k}.\n$$\n\n2. Derivation of the residual update $r_{k+1}$:\n\nBy definition,\n$$\nr_{k+1} \\;=\\; b - A x_{k+1} \\;=\\; b - A(x_k + \\alpha_k p_k) \\;=\\; (b - A x_k) \\;-\\; \\alpha_k A p_k \\;=\\; r_k \\;-\\; \\alpha_k A p_k.\n$$\nThis expresses the updated residual directly in terms of the current residual, the step length, and the matrixâ€“direction product.\n\n3. Concrete computation for the given system:\n\nWe are given\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 \\\\\n0 & -1 & 2 & -1 \\\\\n0 & 0 & -1 & 2\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix},\n\\qquad\nx_0 \\;=\\; \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThus $r_0 = b - A x_0 = b$, and with the standard CG initialization $p_0 = r_0 = b$. Using the derived formula,\n$$\n\\alpha_0 \\;=\\; \\frac{r_0^{\\top} r_0}{p_0^{\\top} A p_0}\n\\;=\\; \\frac{b^{\\top} b}{b^{\\top} A b}.\n$$\nCompute $b^{\\top} b$:\n$$\nb^{\\top} b \\;=\\; 1^2 + 0^2 + 0^2 + 1^2 \\;=\\; 2.\n$$\nCompute $A b$:\n$$\nA b \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 \\\\\n0 & -1 & 2 & -1 \\\\\n0 & 0 & -1 & 2\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n2 \\cdot 1 + (-1) \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 1 \\\\\n-1 \\cdot 1 + 2 \\cdot 0 + (-1) \\cdot 0 + 0 \\cdot 1 \\\\\n0 \\cdot 1 + (-1) \\cdot 0 + 2 \\cdot 0 + (-1) \\cdot 1 \\\\\n0 \\cdot 1 + 0 \\cdot 0 + (-1) \\cdot 0 + 2 \\cdot 1\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 2 \\end{pmatrix}.\n$$\nThen\n$$\nb^{\\top} A b \\;=\\; \\begin{pmatrix} 1 & 0 & 0 & 1 \\end{pmatrix}\n\\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n\\;=\\; 2 + 0 + 0 + 2 \\;=\\; 4.\n$$\nHence\n$$\n\\alpha_0 \\;=\\; \\frac{2}{4} \\;=\\; \\frac{1}{2}.\n$$\nThis value is exact, so no rounding is needed.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "When monitoring an iterative solver, it is natural to watch the residual norm and expect it to decrease steadily to zero. However, the Conjugate Gradient method offers a subtle yet important lesson in convergence analysis. This hands-on coding exercise demonstrates that while the true error in the energy norm ($\\|e_k\\|_A$) decreases monotonically, the Euclidean norm of the residual ($\\|r_k\\|_2$) may not . Understanding this distinction is key to correctly interpreting the behavior of CG and trusting the process even when the residual norm temporarily increases.",
            "id": "3245089",
            "problem": "You are asked to design and implement a complete program that constructs a symmetric positive definite linear system for which the Conjugate Gradient (CG) method exhibits the following two behaviors simultaneously: the error measured in the matrix-induced norm decreases monotonically with iteration index, while the Euclidean norm of the residual does not necessarily decrease monotonically. You must start from fundamental definitions of symmetric positive definite matrices, quadratic minimization of the associated energy functional, and orthogonality conditions that characterize the Conjugate Gradient method, and use these to guide a correct and robust implementation.\n\nFundamental basis:\n- A matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite if $A = A^\\top$ and $x^\\top A x > 0$ for all nonzero vectors $x \\in \\mathbb{R}^n$.\n- Solving $A x = b$ is equivalent to minimizing the strictly convex quadratic functional $\\varphi(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x$ over $\\mathbb{R}^n$.\n- The gradient of $\\varphi$ at $x$ is $\\nabla \\varphi(x) = A x - b = -r$, where $r = b - A x$ is the residual.\n- The Conjugate Gradient method generates iterates $x_k$ in the affine Krylov subspace $x_0 + \\mathcal{K}_k(A, r_0)$ with search directions that are $A$-conjugate, ensuring $A$-orthogonality of error updates and orthogonality of residuals across iterations.\n\nTarget properties to observe:\n- For symmetric positive definite $A$, the A-norm of the error $\\|e_k\\|_A = \\sqrt{e_k^\\top A e_k}$, where $e_k = x_* - x_k$ and $A x_* = b$, is nonincreasing as $k$ increases for the classical Conjugate Gradient iterates starting from $x_0$. This stems from the optimality of $x_k$ as the minimizer of $\\varphi$ over $x_0 + \\mathcal{K}_k$.\n- In contrast, the Euclidean norm of the residual $\\|r_k\\|_2$ need not be monotone in $k$.\n\nYour program must:\n- Implement the classical Conjugate Gradient method for symmetric positive definite systems using only operations justified by the fundamental basis above. Use $x_0 = 0$ as the initial iterate.\n- For each iteration $k$, record both $\\|e_k\\|_A$ and $\\|r_k\\|_2$.\n- Terminate when either $k = n$ (with $n$ the dimension) or when $\\|r_k\\|_2 \\le \\varepsilon \\|r_0\\|_2$ with $\\varepsilon = 10^{-12}$.\n- Decide monotonicity using a relative tolerance $\\tau = 10^{-12}$ to account for floating-point rounding: a sequence $\\{s_k\\}$ is treated as nonincreasing if $s_k \\le (1 + \\tau) s_{k-1}$ for all $k \\ge 1$; the first strict increase index is the smallest $k \\ge 1$ such that $s_k > (1 + \\tau) s_{k-1}$, or $-1$ if none exists.\n\nTest suite:\nProvide results for each of the following symmetric positive definite systems with $x_0 = 0$.\n\n- Test case $1$ (ill-conditioned diagonal, designed to show nonmonotone Euclidean residual): \n  - $A_1 = \\operatorname{diag}(1, 1000)$,\n  - $b_1 = [1, 0.01]^\\top$.\n- Test case $2$ (identity matrix, boundary behavior where residual decreases monotonically in one step):\n  - $A_2 = I_3$,\n  - $b_2 = [1, 2, 3]^\\top$.\n- Test case $3$ (moderate, dense tridiagonal symmetric positive definite):\n  - $A_3 = \\begin{bmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}$,\n  - $b_3 = [1, 2, 3]^\\top$.\n\nFor each test case $j \\in \\{1, 2, 3\\}$, compute and return a triple $[M^{(A)}_j, M^{(r)}_j, k^{\\uparrow}_j]$ where:\n- $M^{(A)}_j$ is a boolean indicating whether $\\|e_k\\|_A$ is nonincreasing in $k$,\n- $M^{(r)}_j$ is a boolean indicating whether $\\|r_k\\|_2$ is nonincreasing in $k$,\n- $k^{\\uparrow}_j$ is the smallest iteration index $k \\ge 1$ such that $\\|r_k\\|_2 > \\|r_{k-1}\\|_2$ by the tolerance rule above, or $-1$ if no such $k$ exists.\n\nFinal output format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, where each per-case triple is itself a bracketed, comma-separated list without spaces. For example, the output must have the format\n$[ [\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot] ]$\nbut with no spaces anywhere, such as\n$[[\\text{True},\\text{False},1],[\\text{True},\\text{True},-1],[\\text{True},\\text{False},2]]$.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the established theory of numerical linear algebra, specifically the Conjugate Gradient (CG) method. The problem is well-posed, with all necessary data ($A$, $b$, $x_0$), algorithmic parameters ($\\varepsilon$, $\\tau$), and termination conditions specified. The test cases involve matrices that are confirmed to be symmetric and positive definite (SPD), and the required analysis is objective and computationally verifiable.\n\nThe core of the problem is to solve a linear system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite matrix, for an unknown vector $x \\in \\mathbb{R}^n$. This problem is equivalent to finding the unique minimizer of the strictly convex quadratic functional $\\varphi(x) = \\frac{1}{2} x^\\top A x - b^\\top x$. The gradient of this functional is $\\nabla \\varphi(x) = A x - b$, which is the negative of the residual, $r(x) = b - A x$.\n\nThe Conjugate Gradient (CG) method is an iterative algorithm that leverages this equivalence. Starting from an initial guess $x_0$ (in this problem, $x_0 = 0$), it generates a sequence of iterates $x_k$ that progressively minimize $\\varphi(x)$. At each step $k$, the iterate $x_k$ is the exact minimizer of $\\varphi(x)$ over the affine Krylov subspace $x_0 + \\mathcal{K}_k(A, r_0)$, where $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$.\n\nThe standard algorithm is as follows:\n1. Initialize: $k=0$, $x_0 = 0$, $r_0 = b - A x_0 = b$, $p_0 = r_0$.\n2. For $k = 0, 1, 2, \\ldots$ until convergence:\n   a. Compute step size: $\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k}$. This value minimizes $\\varphi(x_k + \\alpha p_k)$.\n   b. Update solution: $x_{k+1} = x_k + \\alpha_k p_k$.\n   c. Update residual: $r_{k+1} = r_k - \\alpha_k A p_k$. This is more numerically stable than computing $r_{k+1} = b - A x_{k+1}$ directly.\n   d. Compute improvement factor: $\\beta_k = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}$.\n   e. Update search direction: $p_{k+1} = r_{k+1} + \\beta_k p_k$. The search directions $\\{p_k\\}$ are constructed to be mutually $A$-conjugate (i.e., $p_i^\\top A p_j = 0$ for $i \\neq j$), and the residuals $\\{r_k\\}$ are mutually orthogonal (i.e., $r_i^\\top r_j = 0$ for $i \\neq j$).\n\nWe are tasked with monitoring two quantities:\n1. The $A$-norm of the error: $\\|e_k\\|_A = \\sqrt{e_k^\\top A e_k}$, where $e_k = x_* - x_k$ is the error at iteration $k$ and $x_*$ is the true solution ($A x_* = b$).\n2. The Euclidean norm of the residual: $\\|r_k\\|_2 = \\sqrt{r_k^\\top r_k}$.\n\nThe $A$-norm of the error possesses a fundamental monotonicity property. Since the iterate $x_k$ minimizes $\\varphi(x)$ over $x_0 + \\mathcal{K}_k(A, r_0)$, it also minimizes the $A$-norm of the error $\\|x_* - x\\|_A$ over the same subspace. Because the Krylov subspaces are nested, $\\mathcal{K}_k(A, r_0) \\subseteq \\mathcal{K}_{k+1}(A, r_0)$, the minimum over the larger subspace must be less than or equal to the minimum over the smaller one. Formally:\n$$ \\|e_{k+1}\\|_A = \\min_{x \\in x_0 + \\mathcal{K}_{k+1}(A, r_0)} \\|x_* - x\\|_A \\le \\min_{x \\in x_0 + \\mathcal{K}_k(A, r_0)} \\|x_* - x\\|_A = \\|e_k\\|_A $$\nThus, the sequence $\\{\\|e_k\\|_A\\}$ is guaranteed to be non-increasing. Our implementation should verify this property, accounting for floating-point precision using the provided tolerance $\\tau = 10^{-12}$.\n\nIn contrast, the Euclidean norm of the residual, $\\|r_k\\|_2$, is not guaranteed to decrease monotonically. While the residuals are orthogonal, their norms can fluctuate. This behavior is more pronounced for ill-conditioned matrices. A small step $\\alpha_k$ might be optimal for reducing the error in the $A$-norm, but the resulting residual $r_{k+1} = r_k - \\alpha_k A p_k$ can be larger in Euclidean norm than $r_k$ if $A p_k$ is very large and not well-aligned with $r_k$. The problem provides test cases designed to exhibit both monotonic and non-monotonic residual norm behavior.\n\nThe implementation will proceed as follows for each test case $(A_j, b_j)$:\n1. The exact solution $x_*$ is computed as $x_* = A_j^{-1} b_j$ to enable calculation of the error $e_k = x_* - x_k$.\n2. The CG algorithm is executed starting with $x_0 = 0$. The loop runs for a maximum of $n$ iterations, where $n$ is the dimension of the system, or until the relative residual norm drops below a tolerance $\\varepsilon=10^{-12}$, i.e., $\\|r_k\\|_2 \\le \\varepsilon \\|r_0\\|_2$.\n3. At each iteration $k$ (including the initial state $k=0$), the values $\\|e_k\\|_A$ and $\\|r_k\\|_2$ are computed and stored.\n4. After the algorithm terminates, the stored sequences of norms are analyzed.\n   - For the sequence of $A$-norms of the error, $\\{s_k\\}$, we check if $s_k \\le (1 + \\tau) s_{k-1}$ for all $k \\ge 1$. The result determines the boolean value of $M^{(A)}_j$. Based on the theory, this should always be true.\n   - For the sequence of Euclidean norms of the residual, we perform the same check to determine $M^{(r)}_j$. We also find the first iteration index $k^{\\uparrow}_j \\ge 1$ where the norm strictly increases, i.e., $s_k > (1 + \\tau) s_{k-1}$. If no such increase occurs, $k^{\\uparrow}_j$ is set to $-1$.\n5. The resulting triple $[M^{(A)}_j, M^{(r)}_j, k^{\\uparrow}_j]$ is recorded for each test case. The final output aggregates these triples into the specified list format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Conjugate Gradient method and analyzes its convergence\n    properties on three test cases as specified in the problem statement.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([[1.0, 0.0], [0.0, 1000.0]]),\n            np.array([1.0, 0.01])\n        ),\n        (\n            np.identity(3),\n            np.array([1.0, 2.0, 3.0])\n        ),\n        (\n            np.array([[4.0, 1.0, 0.0], [1.0, 3.0, 1.0], [0.0, 1.0, 2.0]]),\n            np.array([1.0, 2.0, 3.0])\n        )\n    ]\n\n    results = []\n    eps = 1e-12\n    tau = 1e-12\n\n    for A, b in test_cases:\n        n = A.shape[0]\n        \n        # Compute exact aolution to calculate error norm\n        x_star = np.linalg.solve(A, b)\n\n        # Initialize CG\n        x_k = np.zeros_like(b)\n        r_k = b - A @ x_k\n        p_k = r_k.copy()\n        rs_old = r_k @ r_k\n        \n        norm_r0 = np.sqrt(rs_old)\n        \n        e_A_norms = []\n        r_2_norms = []\n\n        # Store initial norms for k=0\n        e_k = x_star - x_k\n        e_A_norms.append(np.sqrt(e_k @ A @ e_k))\n        r_2_norms.append(norm_r0)\n\n        # CG iteration loop\n        for k in range(n):\n            Ap = A @ p_k\n            alpha = rs_old / (p_k @ Ap)\n            \n            x_k = x_k + alpha * p_k\n            r_k = r_k - alpha * Ap\n            \n            rs_new = r_k @ r_k\n            norm_r_new = np.sqrt(rs_new)\n\n            # Store norms for new iterate (k+1)\n            e_k = x_star - x_k\n            e_A_norms.append(np.sqrt(e_k @ A @ e_k))\n            r_2_norms.append(norm_r_new)\n            \n            # Check for termination\n            if norm_r_new = eps * norm_r0:\n                break\n                \n            # Update for next iteration\n            beta = rs_new / rs_old\n            p_k = r_k + beta * p_k\n            rs_old = rs_new\n\n        # Analyze norm sequences\n        # M_A: Is A-norm of error nonincreasing?\n        is_eA_nonincreasing = True\n        for k in range(1, len(e_A_norms)):\n            if e_A_norms[k] > e_A_norms[k-1] * (1 + tau):\n                is_eA_nonincreasing = False\n                break\n        \n        # M_r: Is 2-norm of residual nonincreasing? and k_up: first increase index\n        is_r2_nonincreasing = True\n        k_up = -1\n        for k in range(1, len(r_2_norms)):\n            if r_2_norms[k] > r_2_norms[k-1] * (1 + tau):\n                if is_r2_nonincreasing: # Only set this once\n                   is_r2_nonincreasing = False\n                if k_up == -1: # record first occurrence\n                    k_up = k\n                # Do not break here to correctly determine M_r if there are later decreases\n        \n        results.append([is_eA_nonincreasing, is_r2_nonincreasing, k_up])\n\n    # Final print statement in the exact required format.\n    case_strings = [f\"[{'True' if res[0] else 'False'},{'True' if res[1] else 'False'},{res[2]}]\" for res in results]\n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The theoretical elegance of the Conjugate Gradient method is matched by its practical efficiency, but its performance is deeply tied to the properties of the system matrix. The spectral condition number, $\\kappa$, plays a starring role in the convergence rate. This computational exercise allows you to empirically verify the famous theoretical error bound by testing the algorithm on matrices with deliberately constructed spectra . Witnessing the direct impact of a large $\\kappa$ on the iteration count provides a powerful and concrete motivation for the development and use of preconditioners.",
            "id": "3373133",
            "problem": "You are to design and implement a program that uses the Conjugate Gradient method to solve symmetric positive-definite linear systems arising from a deliberately constructed diagonal matrix with a geometrically distributed spectrum. The goal is to empirically illustrate slow convergence behavior consistent with the standard energy-norm error bound and to quantify the number of iterations required to achieve a prescribed error reduction.\n\nConstruct a diagonal symmetric positive-definite matrix $A \\in \\mathbb{R}^{n \\times n}$ whose eigenvalues are distributed along a geometric progression from $\\lambda_{\\min}$ to $\\lambda_{\\max}$, with condition number $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$. Specifically, set\n$$\n\\lambda_i = \\lambda_{\\min} \\, q^{\\,i-1}, \\quad i = 1,2,\\dots,n, \\quad q = \\left(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\\right)^{\\frac{1}{n-1}},\n$$\nand take $A = \\mathrm{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)$. Let the right-hand side be $b = A x^\\star$ where $x^\\star$ is the vector of all ones in $\\mathbb{R}^n$, and initialize the Conjugate Gradient method at $x_0 = 0$. Let the error be $e_k = x_k - x^\\star$, and define its energy norm reduction factor as\n$$\n\\rho_k = \\frac{\\|e_k\\|_A}{\\|e_0\\|_A}, \\quad \\text{where } \\|y\\|_A = \\sqrt{y^\\top A y}.\n$$\n\nYour tasks are:\n- Implement the Conjugate Gradient algorithm for $A$ and $b$ as specified, stopping when $\\rho_k \\le r$ for a prescribed reduction target $r \\in (0,1)$, or when the iteration count reaches $n$.\n- From a first-principles derivation of the standard energy-norm error bound for the Conjugate Gradient method on symmetric positive-definite systems, deduce a sufficient iteration count $k_{\\mathrm{bound}}$ in terms of the condition number $\\kappa$ and the target reduction $r$ such that the bound guarantees $\\rho_{k_{\\mathrm{bound}}} \\le r$.\n- For each test case, compute and report the observed iteration count $k_{\\mathrm{obs}}$ required to reach $\\rho_k \\le r$, and the theoretically sufficient count $k_{\\mathrm{bound}}$ deduced from the bound.\n\nUse the following test suite of parameter sets $(n,\\kappa,r)$:\n1. $(n,\\kappa,r) = (512, 10^4, 10^{-3})$ as a general case with substantial conditioning and a moderate reduction target.\n2. $(n,\\kappa,r) = (64, 1.2, 10^{-6})$ as a near-identity case that should converge rapidly.\n3. $(n,\\kappa,r) = (2048, 10^6, 10^{-2})$ as an extreme conditioning case to illustrate very slow convergence and the pessimism of the bound when $k_{\\mathrm{bound}}$ exceeds $n$.\n4. $(n,\\kappa,r) = (1024, 500, 10^{-4})$ as a moderately ill-conditioned case.\n\nAll quantities in this problem are dimensionless, and no physical units are involved.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, for each test case in order, the pair $(k_{\\mathrm{obs}}, k_{\\mathrm{bound}})$ flattened into two consecutive integers. Thus the final output format is\n$$\n[ k_{\\mathrm{obs}}^{(1)}, k_{\\mathrm{bound}}^{(1)}, k_{\\mathrm{obs}}^{(2)}, k_{\\mathrm{bound}}^{(2)}, k_{\\mathrm{obs}}^{(3)}, k_{\\mathrm{bound}}^{(3)}, k_{\\mathrm{obs}}^{(4)}, k_{\\mathrm{bound}}^{(4)} ].\n$$\nNo other text should be printed. The program must be self-contained and require no user input.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the well-established theory of numerical linear algebra, specifically the analysis of the Conjugate Gradient method. The problem is well-posed, with all parameters, definitions, and objectives specified in clear, unambiguous language. It presents a standard numerical experiment designed to illustrate theoretical convergence properties, which is a common and meaningful exercise in the field.\n\nThe solution is developed in three parts: first, the derivation of the theoretical iteration bound $k_{\\mathrm{bound}}$; second, the specification of the numerical experiment including the Conjugate Gradient algorithm and the practical evaluation of its stopping criterion; and third, the implementation of the complete procedure for the given test cases.\n\n### 1. Derivation of the Theoretical Iteration Bound\n\nThe standard energy-norm error bound for the Conjugate Gradient (CG) method applied to a symmetric positive-definite (SPD) linear system $Ax = b$ is:\n$$\n\\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le 2 \\left( \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1} \\right)^k\n$$\nHere, $e_k = x_k - x^\\star$ is the error at iteration $k$, $x^\\star$ is the exact solution, $\\|y\\|_A = \\sqrt{y^\\top A y}$ is the A-norm or energy norm, and $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ is the spectral condition number of the matrix $A$. The problem defines the error reduction factor as $\\rho_k = \\|e_k\\|_A / \\|e_0\\|_A$.\n\nWe are tasked with finding a sufficient iteration count, $k_{\\mathrm{bound}}$, that guarantees a reduction of at least $r$, i.e., $\\rho_{k_{\\mathrm{bound}}} \\le r$. This count is the smallest integer $k$ satisfying the inequality based on the bound:\n$$\n2 \\left( \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1} \\right)^k \\le r\n$$\nTo solve for $k$, we first rearrange the inequality:\n$$\n\\left( \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1} \\right)^k \\le \\frac{r}{2}\n$$\nTaking the natural logarithm of both sides. Since $\\kappa > 1$, the base of the power, $(\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1)$, is between $0$ and $1$, so its logarithm is negative. Thus, we must reverse the inequality sign when dividing by it:\n$$\nk \\ln\\left( \\frac{\\sqrt\\kappa-1}{\\sqrt\\kappa+1} \\right) \\le \\ln\\left( \\frac{r}{2} \\right)\n$$\n$$\nk \\ge \\frac{\\ln(r/2)}{\\ln\\left( \\frac{\\sqrt\\kappa-1}{\\sqrt\\kappa+1} \\right)}\n$$\nUsing the identity $\\ln(1/x) = -\\ln(x)$, we can write this in a more convenient form with positive numerator and denominator:\n$$\nk \\ge \\frac{\\ln(2/r)}{\\ln\\left( \\frac{\\sqrt\\kappa+1}{\\sqrt\\kappa-1} \\right)}\n$$\nThe smallest integer $k$ satisfying this is its ceiling. Therefore, the theoretical bound on the number of iterations is:\n$$\nk_{\\mathrm{bound}} = \\left\\lceil \\frac{\\ln(2/r)}{\\ln\\left( \\frac{\\sqrt{\\kappa}+1}{\\sqrt{\\kappa}-1} \\right)} \\right\\rceil\n$$\nThis formula will be used to compute $k_{\\mathrm{bound}}$ for each test case. Note that if $\\kappa = 1$, the denominator is undefined. In this case, CG converges in one iteration, so $k_{\\mathrm{bound}}=1$. The limit of the expression as $\\kappa \\to 1^+$ is $0$, and its ceiling is $1$, consistent with this fact.\n\n### 2. Numerical Experiment Design\n\nThe experiment requires implementing the CG algorithm and applying it to a specifically constructed SPD system.\n\n**Matrix and Vector Construction:**\nThe matrix $A \\in \\mathbb{R}^{n \\times n}$ is diagonal, with eigenvalues given by a geometric progression. Let $\\lambda_{\\min} = 1$. This choice does not affect the condition number $\\kappa$, and thus the convergence behavior related to it. Consequently, $\\lambda_{\\max} = \\kappa$. The eigenvalues are:\n$$\n\\lambda_i = 1 \\cdot q^{i-1}, \\quad i=1, \\dots, n, \\quad \\text{where } q = \\kappa^{\\frac{1}{n-1}}\n$$\nThus, $A = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$. The exact solution is $x^\\star$, the vector of all ones. The right-hand side vector $b$ is then $b = Ax^\\star$, which means $b_i = \\lambda_i$ for each $i$.\n\n**Conjugate Gradient Algorithm:**\nGiven the system $Ax=b$, an initial guess $x_0$, a maximum of $n$ iterations, and a tolerance $r$:\n1.  Initialize:\n    $k = 0$\n    $x_0 = 0$ (vector of zeros)\n    $r_0 = b - Ax_0 = b$\n    $p_0 = r_0$\n    $\\delta_0 = r_0^\\top r_0$\n2.  Iterate for $k = 0, 1, 2, \\dots, n-1$:\n    a. Compute $Ap_k$. Since $A$ is diagonal, this is an element-wise product.\n    b. $\\alpha_k = \\frac{\\delta_k}{p_k^\\top A p_k}$\n    c. $x_{k+1} = x_k + \\alpha_k p_k$\n    d. $r_{k+1} = r_k - \\alpha_k A p_k$\n    e. Check stopping criterion for $x_{k+1}$. If it's met, set $k_{\\mathrm{obs}} = k+1$ and terminate.\n    f. $\\delta_{k+1} = r_{k+1}^\\top r_{k+1}$\n    g. $\\beta_k = \\frac{\\delta_{k+1}}{\\delta_k}$\n    h. $p_{k+1} = r_{k+1} + \\beta_k p_k$\n    i. $\\delta_k \\leftarrow \\delta_{k+1}$\n3.  If the loop completes, set $k_{\\mathrm{obs}} = n$.\n\n**Stopping Criterion Evaluation:**\nThe stopping criterion is $\\rho_k = \\|e_k\\|_A / \\|e_0\\|_A \\le r$. Direct computation of $e_k = x_k-x^\\star$ is possible but not typical within the CG loop. A more standard and computationally elegant way uses the relationship between the error $e_k$ and the residual $r_k$:\n$$\nr_k = b - Ax_k = Ax^\\star - Ax_k = A(x^\\star - x_k) = -Ae_k \\implies e_k = -A^{-1}r_k\n$$\nSubstituting this into the energy norm definition:\n$$\n\\|e_k\\|_A^2 = e_k^\\top A e_k = (-A^{-1}r_k)^\\top A (-A^{-1}r_k) = r_k^\\top (A^{-1})^\\top A A^{-1} r_k\n$$\nSince $A$ is SPD, $(A^{-1})^\\top = A^{-1}$. This simplifies to:\n$$\n\\|e_k\\|_A^2 = r_k^\\top A^{-1} r_k = \\|r_k\\|_{A^{-1}}^2\n$$\nFor a diagonal matrix $A=\\mathrm{diag}(\\lambda_i)$, $A^{-1}=\\mathrm{diag}(1/\\lambda_i)$, so $\\|e_k\\|_A^2 = \\sum_{i=1}^n (r_k)_i^2 / \\lambda_i$.\n\nThe initial error is $e_0 = x_0 - x^\\star = -x^\\star$ (since $x_0=0$). The squared initial energy norm is:\n$$\n\\|e_0\\|_A^2 = (-x^\\star)^\\top A (-x^\\star) = (x^\\star)^\\top A x^\\star = \\mathbf{1}^\\top A \\mathbf{1} = \\sum_{i=1}^n \\lambda_i\n$$\nThe stopping criterion at iteration $k+1$ is therefore checked by computing:\n$$\n\\rho_{k+1} = \\sqrt{\\frac{\\sum_{i=1}^n (r_{k+1})_i^2 / \\lambda_i}{\\sum_{i=1}^n \\lambda_i}} \\le r\n$$\nThis expression is computed at each step to determine $k_{\\mathrm{obs}}$.\n\n### 3. Implementation\nThe final implementation will encapsulate these derivations in a Python script using the `numpy` library for numerical computations. A function will process each test case $(n, \\kappa, r)$ by first calculating $k_{\\mathrm{bound}}$, then setting up and running the CG-algorithm to find $k_{\\mathrm{obs}}$, and finally collecting the results for formatted output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases, calculating k_obs and k_bound.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (512, 1e4, 1e-3),   # (n, kappa, r)\n        (64, 1.2, 1e-6),\n        (2048, 1e6, 1e-2),\n        (1024, 500, 1e-4),\n    ]\n\n    results = []\n    for n, kappa, r in test_cases:\n        # --- Task: Calculate theoretical iteration bound k_bound ---\n        if kappa == 1.0:\n            k_bound = 1\n        else:\n            sqrt_kappa = np.sqrt(kappa)\n            # Bound formula: ceil( log(2/r) / log((sqrt(k)+1)/(sqrt(k)-1)) )\n            numerator = np.log(2.0 / r)\n            denominator = np.log((sqrt_kappa + 1.0) / (sqrt_kappa - 1.0))\n            k_bound = int(np.ceil(numerator / denominator))\n        \n        # --- Task: Implement Conjugate Gradient to find k_obs ---\n\n        # 1. Construct the matrix A (as a vector of eigenvalues) and vector b\n        if n == 1:\n            lambdas = np.array([1.0])\n        else:\n            # Set lambda_min = 1.0, so lambda_max = kappa\n            q = kappa**(1.0 / (n - 1.0))\n            # Eigenvalues: lambda_i = q^(i-1) for i=1,...,n\n            indices = np.arange(n, dtype=np.float64)\n            lambdas = q**indices\n        \n        # b = A * x_star, where x_star is all ones. For diagonal A, b_i = lambda_i.\n        b = lambdas\n\n        # 2. Initialize CG algorithm\n        x = np.zeros(n, dtype=np.float64)\n        residual = b.copy()  # r_0 = b - A*x_0 = b\n        direction = residual.copy()  # p_0 = r_0\n        rs_old_sq = residual @ residual\n\n        # 3. Calculate initial energy norm for stopping criterion\n        # ||e_0||_A^2 = x_star^T * A * x_star = 1^T * A * 1 = sum(diag(A)) = sum(lambdas)\n        e0_A_sq = np.sum(lambdas)\n        \n        k_obs = n  # Default if convergence is not met within n iterations\n        for k_iter in range(n):\n            # A is diagonal, so A*p is an element-wise product\n            A_p = lambdas * direction\n            \n            alpha = rs_old_sq / (direction @ A_p)\n            \n            x += alpha * direction\n            residual -= alpha * A_p\n            \n            # Check stopping criterion: rho_k = ||e_k||_A / ||e_0||_A = r\n            # ||e_k||_A^2 = r_k^T * A^-1 * r_k = sum(r_k[i]^2 / lambda[i])\n            ek_A_sq = np.sum(residual**2 / lambdas)\n            rho_k = np.sqrt(ek_A_sq / e0_A_sq)\n\n            if rho_k = r:\n                k_obs = k_iter + 1\n                break\n\n            # Update for next iteration\n            rs_new_sq = residual @ residual\n            beta = rs_new_sq / rs_old_sq\n            direction = residual + beta * direction\n            rs_old_sq = rs_new_sq\n\n        results.extend([k_obs, k_bound])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}