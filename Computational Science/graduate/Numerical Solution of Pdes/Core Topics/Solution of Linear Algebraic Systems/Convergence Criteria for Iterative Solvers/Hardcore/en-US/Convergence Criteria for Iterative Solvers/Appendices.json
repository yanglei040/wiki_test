{
    "hands_on_practices": [
        {
            "introduction": "A raw residual norm, such as $\\|r\\|$, is not a useful measure of convergence on its own; it must be normalized to create a dimensionless, comparable quantity. However, the choice of normalization strategy is not trivial and can significantly influence the perceived rate of convergence as problem parameters change. This hands-on exercise  guides you through a comparative analysis of three common normalization strategies, revealing how their behavior can differ under mesh refinement and variations in physical models.",
            "id": "3305239",
            "problem": "You are tasked with examining how different residual normalization choices influence perceived convergence of an iterative linear solver arising in the Newton linearization of the compressible Navier–Stokes equations. Work in a one-dimensional, nondimensional setting that captures the scaling of convective and diffusive transport and the compressibility stabilization. The foundation is the following widely used process: a nonlinear residual $F(U) = 0$ is solved by Newton’s method, producing a linear system $J(U^{(m)}) \\,\\Delta U^{(m)} = -F(U^{(m)})$ at nonlinear iteration index $m$, where $J$ is the Jacobian matrix of $F$. For a fixed nonlinear iterate $U^{(m)}$, the inner linear iterative solver maintains a current iterate $x^{(k)}$ solving $J x = b$ with $b = -F(U^{(m)})$, and defines the linear residual $r^{(k)} = b - J x^{(k)}$. Convergence of the inner solver is often declared by checking whether a normalized residual falls below a tolerance.\n\nStarting from this base, construct a synthetic linearized one-dimensional operator and investigate three residual normalization strategies: scaling by the initial residual norm, by the right-hand side norm, and by the diagonal of the Jacobian. Use the fundamental scalings from one-dimensional convection–diffusion with compressibility stabilization to define $J \\in \\mathbb{R}^{n \\times n}$ as a tridiagonal matrix:\n- The diagonal entries are $J_{ii} = \\alpha + \\dfrac{2 \\nu}{h^2}$.\n- The sub-diagonal entries are $J_{i,i-1} = -\\dfrac{\\nu}{h^2} - \\dfrac{U}{2 h}$ for $i \\geq 2$.\n- The super-diagonal entries are $J_{i,i+1} = -\\dfrac{\\nu}{h^2} + \\dfrac{U}{2 h}$ for $i \\leq n-1$.\nHere, $h = \\dfrac{1}{n}$ is the grid spacing on the unit interval, $U$ is a constant advection speed, and $\\nu = \\dfrac{1}{Re}$ is the kinematic viscosity in nondimensional form, with $Re$ the Reynolds number. The parameter $\\alpha  0$ represents compressibility or pseudo-time stabilization and may be taken as sufficiently large to ensure diagonal dominance across the test suite.\n\nDefine the right-hand side as $b_i = \\sin(\\pi x_i)$ with $x_i = i h$ for $i = 1, \\dots, n$. Use an initial guess $x^{(0)}_i = x_i$ for $i = 1, \\dots, n$, so that $r^{(0)} = b - J x^{(0)} \\neq b$.\n\nUse the Weighted Jacobi (WJ) iteration to advance the linear solve:\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} \\left(b - J x^{(k)}\\right),\n$$\nwhere $D = \\mathrm{diag}(J)$ and $\\omega \\in (0, 2)$ is a relaxation parameter. Choose a fixed number of inner iterations $K$ and a fixed relaxation parameter $\\omega$, and assume $D$ has strictly positive entries.\n\nFor each test case, after $K$ iterations, compute the residual $r^{(K)}$ and evaluate the following three normalized residual definitions:\n1. Scaling by the initial residual norm: $R_{r_0} = \\dfrac{\\lVert r^{(K)} \\rVert_2}{\\lVert r^{(0)} \\rVert_2}$.\n2. Scaling by the right-hand side norm: $R_{b} = \\dfrac{\\lVert r^{(K)} \\rVert_2}{\\lVert b \\rVert_2}$.\n3. Scaling by the Jacobian diagonal (row-wise equilibration): $R_{D} = \\dfrac{\\lVert D^{-1} r^{(K)} \\rVert_2}{\\lVert D^{-1} r^{(0)} \\rVert_2}$.\n\nIn addition, for each normalization define a boolean convergence flag by comparing to a fixed tolerance $\\tau$: for normalization $R_{\\star}$, set $C_{\\star} = \\mathrm{True}$ if $R_{\\star} \\le \\tau$ and $C_{\\star} = \\mathrm{False}$ otherwise.\n\nYour program must implement the above using the following fixed parameters:\n- Use $U = 1$.\n- Use $\\alpha = 500$.\n- Use $\\omega = 1$.\n- Use $K = 30$.\n- Use $\\tau = 10^{-6}$.\nAll quantities are dimensionless, and angles are not used.\n\nConstruct and run the following test suite, designed to probe mesh refinement and Reynolds number variation:\n- Case $1$: $n = 50$, $Re = 50$.\n- Case $2$: $n = 200$, $Re = 50$.\n- Case $3$: $n = 50$, $Re = 1000$.\n- Case $4$: $n = 200$, $Re = 1000$.\n- Case $5$: $n = 50$, $Re = 5$.\n\nFor each case, your program must return a list in the order $[R_{r_0}, R_b, R_D, C_{r_0}, C_b, C_D]$ containing the three floats and three booleans. Aggregate the results for all cases into a single list, so the final output is a single line containing a comma-separated list of the five case results, enclosed in square brackets. For example, the printed line must look like $[\\,[\\dots],\\,[\\dots],\\,[\\dots],\\,[\\dots],\\,[\\dots]\\,]$ with each inner list in the specified order. No additional text may be printed.",
            "solution": "The problem requires a critical examination of three distinct residual normalization strategies used to assess the convergence of an iterative linear solver. The context is a synthetic one-dimensional problem representative of those encountered in computational fluid dynamics (CFD), specifically stemming from the Newton linearization of the compressible Navier-Stokes equations. The analysis will be performed by implementing the specified numerical scheme and applying it to a suite of test cases.\n\nFirst, we formalize the problem setup. The core task is to iteratively solve the linear system of equations:\n$$\nJ x = b\n$$\nHere, $J \\in \\mathbb{R}^{n \\times n}$ is a sparse, tridiagonal matrix representing the discretized one-dimensional convection-diffusion operator with a stabilization term. Its entries are defined based on a uniform grid with spacing $h = \\dfrac{1}{n}$ on the unit interval. The parameters include a constant advection speed $U$, a kinematic viscosity $\\nu = \\dfrac{1}{Re}$ (where $Re$ is the Reynolds number), and a stabilization parameter $\\alpha$. The matrix entries are given as:\n- Diagonal: $J_{ii} = \\alpha + \\dfrac{2 \\nu}{h^2}$\n- Sub-diagonal: $J_{i,i-1} = -\\dfrac{\\nu}{h^2} - \\dfrac{U}{2 h}$ for $i = 2, \\dots, n$\n- Super-diagonal: $J_{i,i+1} = -\\dfrac{\\nu}{h^2} + \\dfrac{U}{2 h}$ for $i = 1, \\dots, n-1$\n\nThe right-hand side (RHS) vector $b \\in \\mathbb{R}^n$ is defined by evaluating a smooth function on the grid points $x_i = i h$ for $i = 1, \\dots, n$:\n$$\nb_i = \\sin(\\pi x_i)\n$$\nThe iterative solution process begins with an initial guess $x^{(0)} \\in \\mathbb{R}^n$, which is specified as the vector of grid coordinates:\n$$\nx^{(0)}_i = x_i\n$$\n\nThe iterative solver chosen is the Weighted Jacobi (WJ) method. The solution is advanced from iteration $k$ to $k+1$ using the formula:\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} \\left(b - J x^{(k)}\\right)\n$$\nwhere $D = \\mathrm{diag}(J)$ is the diagonal part of $J$, $\\omega$ is a relaxation parameter, and the term in the parentheses is the linear residual at iteration $k$, defined as $r^{(k)} = b - J x^{(k)}$. For this problem, the parameters are fixed: $\\omega = 1$ (reducing the method to the standard Jacobi iteration) and the number of iterations is fixed at $K = 30$. The problem assumes that $D$ has strictly positive entries, a condition satisfied here since $J_{ii} = \\alpha + \\frac{2\\nu}{h^2}$ with $\\alpha  0$ and $\\nu  0$.\n\nAfter performing $K=30$ iterations, we obtain the final state $x^{(K)}$ and the final residual $r^{(K)} = b - J x^{(K)}$. The central task is to evaluate three different normalized residuals:\n1.  **Scaling by the initial residual norm**: This metric, $R_{r_0}$, measures the reduction in the residual's magnitude relative to its starting value. The initial residual is $r^{(0)} = b - J x^{(0)}$.\n    $$\n    R_{r_0} = \\dfrac{\\lVert r^{(K)} \\rVert_2}{\\lVert r^{(0)} \\rVert_2}\n    $$\n2.  **Scaling by the right-hand side norm**: This metric, $R_{b}$, measures the final residual's magnitude relative to the forcing term $b$.\n    $$\n    R_{b} = \\dfrac{\\lVert r^{(K)} \\rVert_2}{\\lVert b \\rVert_2}\n    $$\n3.  **Scaling by the Jacobian diagonal (row-wise equilibration)**: This metric, $R_{D}$, measures the reduction in the norm of the preconditioned residual, where the preconditioner is the diagonal of the Jacobian. This often provides a scale-invariant measure of convergence.\n    $$\n    R_{D} = \\dfrac{\\lVert D^{-1} r^{(K)} \\rVert_2}{\\lVert D^{-1} r^{(0)} \\rVert_2}\n    $$\n\nFor each of these normalized residuals $R_{\\star}$, a boolean convergence flag $C_{\\star}$ is determined by comparing it against a fixed tolerance $\\tau = 10^{-6}$. The flag is set to $\\mathrm{True}$ if $R_{\\star} \\le \\tau$ and $\\mathrm{False}$ otherwise.\n\nThe computational procedure for each test case $(n, Re)$ is as follows:\n1.  Set the fixed parameters: $U = 1$, $\\alpha = 500$, $\\omega = 1$, $K = 30$, $\\tau = 10^{-6}$.\n2.  Calculate the derived parameters: $h = 1/n$ and $\\nu = 1/Re$.\n3.  Construct the $n \\times n$ tridiagonal matrix $J$.\n4.  Construct the grid vector $x_{grid}$ with entries $x_i = i h$.\n5.  Construct the RHS vector $b$ with entries $b_i = \\sin(\\pi x_i)$.\n6.  Set the initial solution vector $x^{(0)} = x_{grid}$.\n7.  Calculate the initial residual $r^{(0)} = b - Jx^{(0)}$.\n8.  Extract the diagonal matrix $D$ from $J$.\n9.  Compute and store the denominator norms: $\\lVert r^{(0)} \\rVert_2$, $\\lVert b \\rVert_2$, and $\\lVert D^{-1} r^{(0)} \\rVert_2$.\n10. Execute the Jacobi iteration loop for $k$ from $0$ to $K-1=29$:\n    $x^{(k+1)} = x^{(k)} + \\omega D^{-1} (b - J x^{(k)})$.\n11. Upon completion, compute the final residual $r^{(K)} = b - J x^{(K)}$.\n12. Compute the numerator norms: $\\lVert r^{(K)} \\rVert_2$ and $\\lVert D^{-1} r^{(K)} \\rVert_2$.\n13. Calculate the three ratios $R_{r_0}$, $R_{b}$, and $R_{D}$.\n14. Determine the three boolean flags $C_{r_0}$, $C_{b}$, and $C_{D}$.\n15. Collate the results into a list of the form $[R_{r_0}, R_b, R_D, C_{r_0}, C_b, C_D]$.\n\nThis procedure is systematically applied to all specified test cases, and the final results are aggregated into a single list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_case(n, Re, U, alpha, omega, K, tau):\n    \"\"\"\n    Solves the 1D convection-diffusion problem for one test case.\n\n    Args:\n        n (int): Number of grid points.\n        Re (float): Reynolds number.\n        U (float): Advection speed.\n        alpha (float): Stabilization parameter.\n        omega (float): Weighted Jacobi relaxation parameter.\n        K (int): Number of iterations.\n        tau (float): Convergence tolerance.\n\n    Returns:\n        list: A list containing [R_r0, R_b, R_D, C_r0, C_b, C_D].\n    \"\"\"\n    # 1. Calculate derived parameters\n    h = 1.0 / n\n    nu = 1.0 / Re\n\n    # 2. Construct the tridiagonal Jacobian matrix J\n    J = np.zeros((n, n))\n    \n    # Diagonal term\n    diag_val = alpha + (2 * nu / h**2)\n    # Sub-diagonal term\n    sub_diag_val = -nu / h**2 - U / (2 * h)\n    # Super-diagonal term\n    sup_diag_val = -nu / h**2 + U / (2 * h)\n\n    # Fill the matrix J using np.diag\n    J += np.diag(np.full(n, diag_val))\n    J += np.diag(np.full(n - 1, sub_diag_val), k=-1)\n    J += np.diag(np.full(n - 1, sup_diag_val), k=1)\n    \n    # 3. Construct grid, RHS vector b, and initial guess x_k\n    # Grid points x_i = i*h for i=1,...,n, corresponding to array indices 0,...,n-1\n    x_grid = h * (np.arange(n) + 1)\n    \n    b = np.sin(np.pi * x_grid)\n    x_k = x_grid.copy() # Initial guess x^(0)\n\n    # 4. Extract diagonal D and compute its inverse\n    D = np.diag(J)\n    D_inv = 1.0 / D\n\n    # 5. Calculate initial residual r^(0) and required norms\n    r0 = b - J @ x_k\n    norm_r0 = np.linalg.norm(r0, 2)\n    norm_b = np.linalg.norm(b, 2)\n    \n    # Preconditioned initial residual and its norm\n    # D_inv is a 1D array, so we use element-wise multiplication\n    D_inv_r0 = D_inv * r0\n    norm_D_inv_r0 = np.linalg.norm(D_inv_r0, 2)\n    \n    # Edge case: If initial residual is zero, convergence is perfect.\n    if np.isclose(norm_r0, 0.0):\n        return [0.0, 0.0, 0.0, True, True, True]\n\n    # 6. Perform K iterations of the Weighted Jacobi method\n    for _ in range(K):\n        r_k = b - J @ x_k\n        x_k += omega * D_inv * r_k\n        \n    # 7. Compute final residual r^(K) and its norms\n    r_K = b - J @ x_k\n    norm_rK = np.linalg.norm(r_K, 2)\n    \n    D_inv_r_K = D_inv * r_K\n    norm_D_inv_r_K = np.linalg.norm(D_inv_r_K, 2)\n\n    # 8. Calculate the three normalized residual metrics\n    R_r0 = norm_rK / norm_r0\n    R_b = norm_rK / norm_b if norm_b > 0 else np.inf\n    # Handle division by zero for R_D, although norm_D_inv_r0 is unlikely to be zero\n    # if norm_r0 wasn't.\n    R_D = norm_D_inv_r_K / norm_D_inv_r0 if norm_D_inv_r0 > 0 else np.inf\n\n    # 9. Determine the convergence flags\n    C_r0 = R_r0 = tau\n    C_b = R_b = tau\n    C_D = R_D = tau\n\n    return [R_r0, R_b, R_D, C_r0, C_b, C_D]\n\ndef solve():\n    # Define the fixed parameters from the problem statement.\n    U = 1.0\n    alpha = 500.0\n    omega = 1.0\n    K = 30\n    tau = 1.0e-6\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (50, 50.0),    # Case 1\n        (200, 50.0),   # Case 2\n        (50, 1000.0),  # Case 3\n        (200, 1000.0), # Case 4\n        (50, 5.0),     # Case 5\n    ]\n\n    all_results = []\n    for n, Re in test_cases:\n        result = run_case(n, Re, U, alpha, omega, K, tau)\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists matches the required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "After deciding how to measure the residual, the next critical question is when to stop iterating, as driving a residual to zero is an impossible goal in finite-precision arithmetic. This practice  provides a vital theoretical grounding, connecting the practical phenomenon of residual stagnation to the fundamental limits of floating-point computation and the concept of backward error. By completing this exercise, you will be able to define a principled \"floor\" for convergence, preventing wasted computation while ensuring the solution is as accurate as the machine allows.",
            "id": "3374577",
            "problem": "Consider the two-dimensional Poisson equation $-\\Delta u = f$ posed on the unit square $(0,1)^{2}$ with homogeneous Dirichlet boundary conditions. Discretize using the standard five-point second-order finite difference stencil on a uniform grid with $n=127$ interior points per spatial coordinate, so that the grid spacing is $h = 1/(n+1) = 1/128$ and the resulting linear system is $A x = b$ with $A \\in \\mathbb{R}^{m \\times m}$, $m = n^{2}$. Let the exact solution vector be chosen as $x^{\\star} \\in \\mathbb{R}^{m}$ with all entries equal to $1$, and set $b := A x^{\\star}$.\n\nSuppose an iterative Krylov subspace method such as Conjugate Gradient (CG) is applied to $A x = b$, and at each iteration the residual is formed as $r_{k} = b - A x_{k}$ using standard floating-point arithmetic with machine epsilon $\\epsilon_{\\text{mach}} = 2^{-53}$. Assume that the floating-point computation of a matrix-vector product satisfies the usual first-order rounding model, and that the spectral norm $\\|A\\|_{2}$ is dominated by the largest eigenvalue of the discrete Laplacian on this grid.\n\nUsing only fundamentals of the floating-point rounding model and backward error analysis, together with standard spectral properties of the discrete Laplacian, perform the following:\n\n1. Derive, to leading order in $\\epsilon_{\\text{mach}}$, a principled stagnation threshold $R_{\\star}$ for the attainable residual norm $\\|r_{k}\\|_{2}$ below which further reduction is inefficient because it is dominated by floating-point effects. Express $R_{\\star}$ in terms of $\\epsilon_{\\text{mach}}$, $\\|A\\|_{2}$, and $\\|x^{\\star}\\|_{2}$, and explain the backward error meaning of terminating when $\\|r_{k}\\|_{2} \\approx R_{\\star}$.\n\n2. For the specific matrix $A$ arising from this $n=127$ discretization, use the exact largest eigenvalue of the five-point discrete Laplacian on the unit square with homogeneous Dirichlet boundary conditions to evaluate $\\|A\\|_{2}$, and use the exact $\\|x^{\\star}\\|_{2}$ for the all-ones vector. Then compute the numerical value of\n$$\nR_{\\star} = \\epsilon_{\\text{mach}} \\,\\|A\\|_{2}\\, \\|x^{\\star}\\|_{2}\n$$\nin this setting. Round your final numerical answer for $R_{\\star}$ to four significant figures.\n\nYour final answer should be the single rounded value of $R_{\\star}$, with no units. Angles, if any appear in your work, must be in radians.",
            "solution": "The problem asks for a derivation of a stagnation threshold for the residual norm in an iterative solver due to floating-point effects, and then to compute its value for a specific discretized Poisson problem.\n\nPart 1: Derivation and Interpretation of the Stagnation Threshold $R_{\\star}$\n\nAn iterative method applied to the linear system $A x = b$ generates a sequence of approximate solutions $x_k$. The convergence is monitored by the norm of the true residual, $r_k = b - A x_k$. In a computer using floating-point arithmetic, we can only compute an approximate residual, $\\hat{r}_k$. The calculation of $\\hat{r}_k$ is performed as $\\hat{r}_k = \\text{fl}(b - \\text{fl}(A x_k))$, where $\\text{fl}(\\cdot)$ denotes the result of a floating-point computation.\n\nThe dominant source of error in this computation is the matrix-vector product $\\text{fl}(A x_k)$. According to the standard first-order forward error model for floating-point matrix-vector multiplication, the error is bounded by the norm of the participating quantities. Specifically, the norm of the error vector $e_k = \\text{fl}(A x_k) - A x_k$ can be bounded as:\n$$\n\\|e_k\\|_2 = \\|\\text{fl}(A x_k) - A x_k\\|_2 \\lesssim c \\cdot \\epsilon_{\\text{mach}} \\|A\\|_2 \\|x_k\\|_2\n$$\nwhere $\\epsilon_{\\text{mach}}$ is the machine epsilon and $c$ is a constant of order $1$, whose precise value depends on the matrix dimension and sparsity, but is often taken as $c \\approx 1$ for a leading-order analysis.\n\nThe computed residual $\\hat{r}_k$ is thus an approximation of the true residual $r_k$. Its value is determined by the floating-point subtraction of $\\text{fl}(A x_k)$ from $b$. To first order, the error in the matrix-vector product is the most significant contribution to the error in the computed residual, especially when the true residual is small. We can write:\n$$\n\\hat{r}_k \\approx (b - A x_k) - (\\text{fl}(A x_k) - A x_k) = r_k - e_k\n$$\nAs the iterative method converges, the approximate solution $x_k$ approaches the exact solution $x^{\\star}$, which means the true residual $r_k$ approaches the zero vector, i.e., $\\|r_k\\|_2 \\to 0$. However, the floating-point error term $e_k$ does not vanish. Its magnitude stabilizes as $\\|x_k\\|_2$ approaches $\\|x^{\\star}\\|_2$. Consequently, the norm of the computed residual $\\|\\hat{r}_k\\|_2$ will be dominated by the norm of the error, $\\|e_k\\|_2$.\n$$\n\\lim_{k \\to \\infty} \\|\\hat{r}_k\\|_2 \\approx \\|e_k\\|_2 \\approx \\epsilon_{\\text{mach}} \\|A\\|_2 \\|x^{\\star}\\|_2\n$$\nThis limiting value represents a floor below which the computed residual norm cannot be reliably reduced. Any further reduction in the true residual $\\|r_k\\|_2$ is masked by floating-point noise. Therefore, a principled stagnation threshold $R_{\\star}$ is given by:\n$$\nR_{\\star} = \\epsilon_{\\text{mach}} \\|A\\|_2 \\|x^{\\star}\\|_2\n$$\nThe backward error interpretation of this threshold is as follows. When the true residual norm satisfies $\\|r_k\\|_2 \\approx R_{\\star}$, we have:\n$$\n\\|b - A x_k\\|_2 \\approx \\epsilon_{\\text{mach}} \\|A\\|_2 \\|x^{\\star}\\|_2\n$$\nAssuming $\\|x_k\\|_2 \\approx \\|x^{\\star}\\|_2$ near convergence, this can be rearranged to:\n$$\n\\frac{\\|b - A x_k\\|_2}{\\|A\\|_2 \\|x_k\\|_2} \\approx \\epsilon_{\\text{mach}}\n$$\nThe term on the left is the normwise relative backward error for the approximate solution $x_k$. It represents the smallest relative perturbation $\\delta A$ such that $x_k$ is the exact solution to the perturbed system $(A + \\delta A) x_k = b$. Terminating the iteration when the residual norm reaches $R_{\\star}$ thus corresponds to having found an approximate solution $x_k$ that is the exact solution to a nearby problem, where the \"nearness\" is on the order of machine precision. This is typically the best achievable result in floating-point arithmetic.\n\nPart 2: Numerical Evaluation for the Given Problem\n\nWe need to compute $R_{\\star} = \\epsilon_{\\text{mach}} \\|A\\|_2 \\|x^{\\star}\\|_2$ for the given parameters.\n\n1.  **Given values**:\n    - Machine epsilon: $\\epsilon_{\\text{mach}} = 2^{-53}$.\n    - Number of interior grid points per dimension: $n=127$.\n    - The total number of unknowns is $m = n^2 = 127^2 = 16129$.\n    - The grid spacing is $h = 1/(n+1) = 1/128$.\n\n2.  **Compute $\\|x^{\\star}\\|_{2}$**:\n    The exact solution vector $x^{\\star} \\in \\mathbb{R}^{m}$ has all its $m=n^2$ entries equal to $1$. Its Euclidean norm is:\n    $$\n    \\|x^{\\star}\\|_2 = \\sqrt{\\sum_{i=1}^{n^2} 1^2} = \\sqrt{n^2} = n = 127\n    $$\n\n3.  **Compute $\\|A\\|_{2}$**:\n    The matrix $A$ arises from the standard five-point finite difference discretization of the negative Laplacian operator, $-\\Delta$. The matrix $A$ is symmetric, so its $2$-norm is equal to its largest eigenvalue (spectral radius): $\\|A\\|_2 = \\lambda_{\\max}(A)$. The eigenvalues of the discrete Laplacian on a unit square with grid parameter $n$ and homogeneous Dirichlet boundary conditions are known to be:\n    $$\n    \\lambda_{p,q} = \\frac{1}{h^2} \\left(4 - 2\\cos\\left(\\frac{p\\pi}{n+1}\\right) - 2\\cos\\left(\\frac{q\\pi}{n+1}\\right)\\right) \\quad \\text{for } p,q = 1, 2, \\ldots, n\n    $$\n    The largest eigenvalue, $\\lambda_{\\max}$, occurs when the cosine terms are minimal, which corresponds to the largest indices $p=n$ and $q=n$.\n    $$\n    \\|A\\|_2 = \\lambda_{n,n} = \\frac{2}{h^2} \\left(2 - 2\\cos\\left(\\frac{n\\pi}{n+1}\\right)\\right) = \\frac{4}{h^2} \\left(1 - \\cos\\left(\\frac{n\\pi}{n+1}\\right)\\right)\n    $$\n    Using the identity $\\cos(\\pi - \\theta) = -\\cos(\\theta)$, we can write $\\cos\\left(\\frac{n\\pi}{n+1}\\right) = \\cos\\left(\\pi - \\frac{\\pi}{n+1}\\right) = -\\cos\\left(\\frac{\\pi}{n+1}\\right)$. Substituting this into the expression for $\\|A\\|_2$:\n    $$\n    \\|A\\|_2 = \\frac{4}{h^2} \\left(1 + \\cos\\left(\\frac{\\pi}{n+1}\\right)\\right)\n    $$\n    With $n=127$, we have $n+1=128$ and $h=1/128$.\n    $$\n    \\|A\\|_2 = \\frac{4}{(1/128)^2} \\left(1 + \\cos\\left(\\frac{\\pi}{128}\\right)\\right) = 4 \\cdot 128^2 \\left(1 + \\cos\\left(\\frac{\\pi}{128}\\right)\\right)\n    $$\n\n4.  **Compute $R_{\\star}$**:\n    Now we assemble the expression for $R_{\\star}$:\n    $$\n    R_{\\star} = \\epsilon_{\\text{mach}} \\|A\\|_2 \\|x^{\\star}\\|_2 = 2^{-53} \\cdot \\left[4 \\cdot 128^2 \\left(1 + \\cos\\left(\\frac{\\pi}{128}\\right)\\right)\\right] \\cdot 127\n    $$\n    We can simplify the powers of $2$. Since $128 = 2^7$ and $4=2^2$, we have $4 \\cdot 128^2 = 2^2 \\cdot (2^7)^2 = 2^2 \\cdot 2^{14} = 2^{16}$.\n    $$\n    R_{\\star} = 2^{-53} \\cdot 2^{16} \\cdot 127 \\cdot \\left(1 + \\cos\\left(\\frac{\\pi}{128}\\right)\\right) = 2^{-37} \\cdot 127 \\cdot \\left(1 + \\cos\\left(\\frac{\\pi}{128}\\right)\\right)\n    $$\n    We now evaluate this expression numerically:\n    $R_{\\star} \\approx (7.2759576 \\times 10^{-12}) \\cdot 127 \\cdot (1 + 0.9996988) = (7.2759576 \\times 10^{-12}) \\cdot 127 \\cdot (1.9996988)$\n    $R_{\\star} \\approx 1.84784081699 \\times 10^{-9}$\n\nRounding this value to four significant figures, we get:\n$R_{\\star} \\approx 1.848 \\times 10^{-9}$.",
            "answer": "$$\n\\boxed{1.848 \\times 10^{-9}}\n$$"
        },
        {
            "introduction": "Achieving a small mathematical residual is often a proxy for the true goal: obtaining a physically accurate solution. This exercise  critically examines the assumption that these two goals are always aligned, demonstrating scenarios where a tightly converged residual can still correspond to an unacceptable physical imbalance. You will learn to construct and evaluate composite convergence criteria, a robust practice that pairs mathematical metrics with checks on key physical quantities like conservation laws.",
            "id": "3305180",
            "problem": "Consider a steady, incompressible lid-driven cavity flow in the context of Computational Fluid Dynamics (CFD), where iterative solvers are applied to the discrete momentum and continuity equations. The physical requirement of global mass conservation is expressed by the incompressibility condition $\\nabla \\cdot \\mathbf{u} = 0$, and numerically by ensuring that the discrete divergence is zero everywhere at steady state. Let $A \\in \\mathbb{R}^{n \\times n}$ be a symmetric positive-definite matrix arising from a consistent discretization of the pressure Poisson equation or velocity correction step, $x_k$ be the iterate at iteration $k$, and $b$ be the right-hand side. Define the linear residual vector $r_k = b - A x_k$ and its Euclidean norm $||r_k||_2$. In practice, stopping criteria based solely on reducing $||r_k||_2$ may not guarantee compliance with global mass conservation if the continuity enforcement or pressure-velocity coupling converges at a different rate or to a biased plateau.\n\nModel the convergence of two metrics during iterations $k = 0, 1, 2, \\dots$:\n- The relative residual ratio $||r_k||_2 / ||r_0||_2$, assumed to decay as $||r_k||_2 / ||r_0||_2 = \\alpha^k$ with a contraction factor $0  \\alpha  1$.\n- The normalized global mass imbalance $\\epsilon_k$, defined as the volume-integrated discrete divergence normalized by a characteristic flux scale, modeled as $\\epsilon_k = \\epsilon_\\infty + (\\epsilon_0 - \\epsilon_\\infty)\\,\\beta^k$ with $0 \\le \\epsilon_\\infty \\le \\epsilon_0$ and $0  \\beta  1$.\n\nAll quantities are dimensionless. Let the residual threshold be $\\tau_r  0$ and the mass imbalance threshold be $\\tau_m  0$, both specified as decimals (for example, $0.01$ represents a one percent level). A residual-only stopping rule stops at the minimal iteration $k_r$ such that $||r_k||_2 / ||r_0||_2 \\le \\tau_r$. A composite stopping rule requires both $||r_k||_2 / ||r_0||_2 \\le \\tau_r$ and $\\epsilon_k \\le \\tau_m$, and stops at the minimal iteration $k_c$ satisfying both. If $\\epsilon_\\infty \\ge \\tau_m$ and $\\epsilon_0  \\tau_m$, then no finite iteration can satisfy the mass imbalance target, and the composite rule is unreachable.\n\nYour task is to implement a program that, given a test suite of parameter sets $(\\alpha, \\beta, \\tau_r, \\tau_m, \\epsilon_\\infty, ||r_0||_2, \\epsilon_0)$, computes for each set:\n1. The minimal residual-only stopping iteration $k_r$.\n2. The mass imbalance at the residual-only stop, $\\epsilon_{k_r}$.\n3. The composite stopping iteration $k_c$ (or $-1$ if unreachable).\n4. A boolean indicating whether the composite criterion is reachable.\n\nUse the following test suite, designed to cover a canonical case, happy path, delayed mass convergence, and an equality boundary case. In Test Case $1$, the parameters are chosen to represent a steady lid-driven cavity scenario where stopping at a relative residual drop of $10^{-6}$ in $||r||_2$ still yields a mass imbalance of order $0.01$:\n- Test Case $1$: $(\\alpha, \\beta, \\tau_r, \\tau_m, \\epsilon_\\infty, ||r_0||_2, \\epsilon_0) = (0.8, 0.93, 10^{-6}, 0.005, 0.0102, 1.0, 0.1)$.\n- Test Case $2$: $(\\alpha, \\beta, \\tau_r, \\tau_m, \\epsilon_\\infty, ||r_0||_2, \\epsilon_0) = (0.9, 0.6, 10^{-6}, 10^{-4}, 0.0, 1.0, 0.05)$.\n- Test Case $3$: $(\\alpha, \\beta, \\tau_r, \\tau_m, \\epsilon_\\infty, ||r_0||_2, \\epsilon_0) = (0.7, 0.9, 10^{-6}, 0.001, 0.0004, 1.0, 0.1)$.\n- Test Case $4$: $(\\alpha, \\beta, \\tau_r, \\tau_m, \\epsilon_\\infty, ||r_0||_2, \\epsilon_0) = \\left(10^{-0.6}, 10^{-0.3}, 10^{-6}, 10^{-3}, 0.0, 1.0, 1.0\\right)$.\n\nAlgorithmic requirements:\n- Compute $k_r$ as the minimal integer $k$ such that $\\alpha^k \\le \\tau_r$. If $\\tau_r \\ge 1$, set $k_r = 0$.\n- Compute $\\epsilon_{k_r} = \\epsilon_\\infty + (\\epsilon_0 - \\epsilon_\\infty)\\,\\beta^{k_r}$.\n- Determine reachability of the mass criterion: if $\\epsilon_\\infty \\ge \\tau_m$ and $\\epsilon_0  \\tau_m$, then the composite rule is unreachable. Otherwise, compute the minimal integer $k_m$ such that $\\epsilon_k \\le \\tau_m$. Then set $k_c = \\max(k_r, k_m)$.\n- Use inclusive comparisons for thresholds (that is, $\\le$ satisfies the criterion).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a list of the form $[k_r, \\epsilon_{k_r}, k_c, \\text{reachable}]$. For example, output should look like $[[\\dots],[\\dots],[\\dots],[\\dots]]$.\n\nThe final answers must be decimals or integers, not percentages. No physical units are required because all quantities are dimensionless.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of computational fluid dynamics, specifically concerning the convergence characteristics of iterative solvers for incompressible flow. The problem is well-posed, with all parameters, models, and objectives clearly and mathematically defined. It is objective and free of any non-verifiable or ambiguous statements.\n\nThe task is to analyze the convergence behavior based on two distinct metrics: the relative residual of a linear system and the global mass imbalance. We will derive analytical expressions for the number of iterations required to meet the specified thresholds for each metric and for a composite criterion.\n\n### 1. Derivation of Residual-Only Stopping Iteration ($k_r$)\n\nThe stopping criterion for the residual is based on the relative residual ratio, $||r_k||_2 / ||r_0||_2$, modeled as $\\alpha^k$. The solver stops at the minimal integer iteration $k_r$ such that this ratio is less than or equal to a given threshold $\\tau_r  0$.\n\nThe condition is:\n$$\n\\alpha^{k} \\le \\tau_r\n$$\n\nPer the problem statement, if $\\tau_r \\ge 1$, the condition is met at iteration $k=0$ (since $\\alpha^0 = 1$), so we set $k_r = 0$.\n\nIf $\\tau_r  1$, we can solve for $k$. Since $0  \\alpha  1$, the logarithm $\\log(\\alpha)$ is negative. Taking the natural logarithm of both sides of the inequality, we get:\n$$\nk \\log(\\alpha) \\le \\log(\\tau_r)\n$$\nDividing by $\\log(\\alpha)$ reverses the inequality sign:\n$$\nk \\ge \\frac{\\log(\\tau_r)}{\\log(\\alpha)}\n$$\nSince $k$ must be an integer, the minimal integer $k$ satisfying this condition is the ceiling of the right-hand side. Therefore, the residual-only stopping iteration is:\n$$\nk_r = \\left\\lceil \\frac{\\log(\\tau_r)}{\\log(\\alpha)} \\right\\rceil \\quad \\text{for } \\tau_r  1\n$$\n\n### 2. Calculation of Mass Imbalance at $k_r$ ($\\epsilon_{k_r}$)\n\nThe normalized global mass imbalance at iteration $k$ is given by the model:\n$$\n\\epsilon_k = \\epsilon_\\infty + (\\epsilon_0 - \\epsilon_\\infty)\\,\\beta^k\n$$\nwhere $\\epsilon_0$ is the initial imbalance, $\\epsilon_\\infty$ is the asymptotic imbalance, and $0  \\beta  1$ is the contraction factor for this metric. To find the mass imbalance at the moment the residual-only criterion stops, we substitute $k = k_r$ into this equation:\n$$\n\\epsilon_{k_r} = \\epsilon_\\infty + (\\epsilon_0 - \\epsilon_\\infty)\\,\\beta^{k_r}\n$$\n\n### 3. Analysis of the Composite Stopping Criterion\n\nThe composite criterion requires satisfying both the residual and mass imbalance thresholds simultaneously. Let $k_c$ be the minimal iteration satisfying both $||r_k||_2 / ||r_0||_2 \\le \\tau_r$ and $\\epsilon_k \\le \\tau_m$.\n\n#### 3.1. Reachability of the Mass Imbalance Criterion\n\nThe mass imbalance $\\epsilon_k$ is a monotonically decreasing sequence that converges to $\\epsilon_\\infty$ as $k \\to \\infty$ (assuming $\\epsilon_0  \\epsilon_\\infty$). The condition to be met is $\\epsilon_k \\le \\tau_m$.\nIf the asymptotic limit of the imbalance, $\\epsilon_\\infty$, is itself greater than or equal to the threshold $\\tau_m$, the criterion can never be satisfied for any finite $k$, provided the iteration starts from an unsatisfactory state, i.e., $\\epsilon_0  \\tau_m$. This establishes the unreachability condition stated in the problem: the composite criterion is unreachable if $\\epsilon_\\infty \\ge \\tau_m$ and $\\epsilon_0  \\tau_m$. In this situation, we set $k_c = -1$. Otherwise, the criterion is reachable.\n\n#### 3.2. Derivation of Mass Imbalance Stopping Iteration ($k_m$)\n\nIf the criterion is reachable, we must find the minimal integer iteration $k_m$ such that $\\epsilon_k \\le \\tau_m$.\nIf the initial state already satisfies the condition, i.e., $\\epsilon_0 \\le \\tau_m$, then the minimal iteration is $k_m = 0$.\n\nIf $\\epsilon_0  \\tau_m$, we solve the inequality:\n$$\n\\epsilon_\\infty + (\\epsilon_0 - \\epsilon_\\infty)\\beta^k \\le \\tau_m\n$$\n$$\n(\\epsilon_0 - \\epsilon_\\infty)\\beta^k \\le \\tau_m - \\epsilon_\\infty\n$$\nAssuming $\\epsilon_0  \\epsilon_\\infty$ (the standard case for convergence), we can divide by the positive quantity $(\\epsilon_0 - \\epsilon_\\infty)$:\n$$\n\\beta^k \\le \\frac{\\tau_m - \\epsilon_\\infty}{\\epsilon_0 - \\epsilon_\\infty}\n$$\nSince the criterion is reachable and we are in the case $\\epsilon_0  \\tau_m$, it must be that $\\epsilon_\\infty  \\tau_m$, so the right-hand side is a positive value less than $1$. Taking the natural logarithm:\n$$\nk \\log(\\beta) \\le \\log\\left(\\frac{\\tau_m - \\epsilon_\\infty}{\\epsilon_0 - \\epsilon_\\infty}\\right)\n$$\nDividing by the negative term $\\log(\\beta)$ reverses the inequality:\n$$\nk \\ge \\frac{\\log\\left( (\\tau_m - \\epsilon_\\infty) / (\\epsilon_0 - \\epsilon_\\infty) \\right)}{\\log(\\beta)}\n$$\nThe minimal integer iteration $k_m$ is therefore:\n$$\nk_m = \\left\\lceil \\frac{\\log\\left( (\\tau_m - \\epsilon_\\infty) / (\\epsilon_0 - \\epsilon_\\infty) \\right)}{\\log(\\beta)} \\right\\rceil \\quad \\text{for } \\epsilon_0  \\tau_m \\text{ and reachable}\n$$\n\n#### 3.3. Composite Stopping Iteration ($k_c$)\n\nIf the composite criterion is reachable, the solver must continue until both conditions are met. This will occur at the iteration that is the maximum of the iterations required for each individual criterion.\n$$\nk_c = \\max(k_r, k_m)\n$$\n\nThese derived formulas are implemented in the provided program to compute the required quantities for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the convergence analysis problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: Canonical case where residual-only stop is insufficient.\n        {'alpha': 0.8, 'beta': 0.93, 'tau_r': 1e-6, 'tau_m': 0.005, 'e_inf': 0.0102, 'r0_norm': 1.0, 'e_0': 0.1},\n        # Test Case 2: Happy path, fast mass convergence.\n        {'alpha': 0.9, 'beta': 0.6, 'tau_r': 1e-6, 'tau_m': 1e-4, 'e_inf': 0.0, 'r0_norm': 1.0, 'e_0': 0.05},\n        # Test Case 3: Delayed mass convergence, composite stop is later.\n        {'alpha': 0.7, 'beta': 0.9, 'tau_r': 1e-6, 'tau_m': 0.001, 'e_inf': 0.0004, 'r0_norm': 1.0, 'e_0': 0.1},\n        # Test Case 4: Equality boundary case.\n        {'alpha': 10**-0.6, 'beta': 10**-0.3, 'tau_r': 1e-6, 'tau_m': 1e-3, 'e_inf': 0.0, 'r0_norm': 1.0, 'e_0': 1.0}\n    ]\n\n    results = []\n    for params in test_cases:\n        result = analyze_convergence(params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default str() representation of a list includes spaces after commas,\n    # which matches the problem's example format: [k_r, e_kr, k_c, reachable].\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef analyze_convergence(params):\n    \"\"\"\n    Computes convergence metrics for a single parameter set.\n\n    Args:\n        params (dict): A dictionary of parameters: \n                       alpha, beta, tau_r, tau_m, e_inf, r0_norm, e_0\n\n    Returns:\n        list: A list containing [k_r, e_kr, k_c, reachable].\n    \"\"\"\n    alpha = params['alpha']\n    beta = params['beta']\n    tau_r = params['tau_r']\n    tau_m = params['tau_m']\n    e_inf = params['e_inf']\n    e_0 = params['e_0']\n\n    # 1. Compute the minimal residual-only stopping iteration, k_r.\n    # The condition is alpha^k = tau_r.\n    if tau_r >= 1.0:\n        k_r = 0\n    else:\n        # k = log(tau_r) / log(alpha). Since alpha  1, log(alpha) is negative.\n        # np.log is the natural logarithm.\n        k_r = int(np.ceil(np.log(tau_r) / np.log(alpha)))\n\n    # 2. Compute the mass imbalance at the residual-only stop, e_kr.\n    e_kr = e_inf + (e_0 - e_inf) * (beta ** k_r)\n\n    # 3.  4. Determine reachability and compute the composite stopping iteration, k_c.\n    \n    # The composite criterion is unreachable if the asymptotic mass imbalance is\n    # not better than the threshold AND the starting imbalance is worse.\n    is_reachable = not (e_inf >= tau_m and e_0 > tau_m)\n\n    if not is_reachable:\n        k_c = -1\n    else:\n        # Mass criterion is reachable. Find the minimal iteration k_m for it.\n        # The condition is e_k = tau_m.\n        if e_0 = tau_m:\n            k_m = 0\n        else:\n            # e_inf + (e_0 - e_inf) * beta^k = tau_m\n            # beta^k = (tau_m - e_inf) / (e_0 - e_inf)\n            # k = log( (tau_m - e_inf) / (e_0 - e_inf) ) / log(beta)\n            # Since beta  1, log(beta) is negative, inequality flips.\n            # And since reachable with e_0  tau_m, we have e_inf  tau_m.\n            ratio = (tau_m - e_inf) / (e_0 - e_inf)\n            k_m = int(np.ceil(np.log(ratio) / np.log(beta)))\n\n        # The composite rule stops when both criteria are met.\n        k_c = max(k_r, k_m)\n    \n    return [k_r, e_kr, k_c, is_reachable]\n\n# Execute the main function.\nsolve()\n```"
        }
    ]
}