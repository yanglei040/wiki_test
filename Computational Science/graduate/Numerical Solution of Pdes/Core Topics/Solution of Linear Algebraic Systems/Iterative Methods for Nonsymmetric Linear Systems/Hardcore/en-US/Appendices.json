{
    "hands_on_practices": [
        {
            "introduction": "The Generalized Minimal Residual (GMRES) method relies on the Arnoldi process to build an orthonormal basis for the Krylov subspace. This fundamental exercise guides you through the first two steps of the Arnoldi iteration for a concrete nonsymmetric matrix. By manually computing the basis vectors and the entries of the upper-Hessenberg matrix, you will gain a tangible understanding of the Gram-Schmidt orthogonalization at the heart of the method and see firsthand how an 'exact breakdown' manifests when an invariant subspace is found .",
            "id": "3411908",
            "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ be the nonsymmetric upper-triangular matrix\n$$\nA=\\begin{bmatrix}\n2  10  0\\\\\n0  3  1\\\\\n0  0  4\n\\end{bmatrix},\n$$\nand consider the linear system $A x = b$ that could arise from a finite-dimensional discretization of a Partial Differential Equation (PDE). Let the initial guess be $x_0=0$ and the right-hand side be $b=\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$, so that the initial residual is $r_0=b-A x_0=b$. Using the Arnoldi process with the Euclidean inner product and classical Gram–Schmidt orthogonalization, construct the orthonormal basis vectors $v_1$, $v_2$, $v_3$ and the $(3 \\times 2)$ upper-Hessenberg matrix $\\underline{H}_2$ generated by carrying out exactly two Arnoldi steps starting from $v_1=r_0/\\|r_0\\|_2$. In your construction, explicitly compute all projection coefficients and normalization constants used in the orthonormalization, and assemble $V_3=\\begin{bmatrix}v_1  v_2  v_3\\end{bmatrix}$ and $\\underline{H}_2$. If an exact breakdown occurs, explain how it affects the definition of $v_3$ and the structure of $\\underline{H}_2$, and provide one explicit orthonormal completion to define $v_3$ consistent with the Arnoldi relation $A V_2 = V_3 \\underline{H}_2$. As your final answer, provide the scalar subdiagonal entry $h_{32}$ of $\\underline{H}_2$ in exact form. Do not round your answer.",
            "solution": "The problem requires the construction of an orthonormal basis and a Hessenberg matrix using the Arnoldi process for a given linear system. First, the problem is validated and found to be well-posed and scientifically sound, containing all necessary information for a unique solution process.\n\nThe Arnoldi process is an iterative method that constructs an orthonormal basis $\\{v_1, v_2, \\ldots, v_m\\}$ for the Krylov subspace $\\mathcal{K}_m(A, v_1) = \\text{span}\\{v_1, Av_1, \\ldots, A^{m-1}v_1\\}$. The process also generates an upper-Hessenberg matrix $\\underline{H}_{m-1} \\in \\mathbb{R}^{m \\times (m-1)}$ whose entries are the coefficients from the Gram-Schmidt orthogonalization. The governing relation is $AV_{m-1} = V_m \\underline{H}_{m-1}$, where $V_k = \\begin{bmatrix} v_1  \\cdots  v_k \\end{bmatrix}$.\n\nThe algorithm proceeds as follows. For $j=1, 2, \\ldots, m-1$:\n1. Compute $w = Av_j$.\n2. For $i=1, \\ldots, j$, compute the projection coefficient $h_{ij} = v_i^T w$.\n3. Compute the vector to be orthogonalized: $\\tilde{v}_{j+1} = w - \\sum_{i=1}^j h_{ij}v_i$.\n4. Compute the normalization constant $h_{j+1,j} = \\|\\tilde{v}_{j+1}\\|_2$.\n5. If $h_{j+1,j} = 0$, an exact breakdown occurs; stop. Otherwise, set $v_{j+1} = \\tilde{v}_{j+1}/h_{j+1,j}$.\n\nWe are given the matrix $A=\\begin{bmatrix} 2  10  0\\\\ 0  3  1\\\\ 0  0  4 \\end{bmatrix}$, the right-hand side vector $b=\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$, and the initial guess $x_0=0$.\n\n**Step 0: Initialization**\nThe initial residual is $r_0 = b - Ax_0 = b = \\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$.\nThe starting vector for the Arnoldi process is $v_1 = r_0 / \\|r_0\\|_2$.\nThe Euclidean norm of $r_0$ is $\\|r_0\\|_2 = \\sqrt{1^2+1^2+1^2} = \\sqrt{3}$.\nThus, the first orthonormal basis vector is $v_1 = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$.\n\n**Step 1: Construction of $v_2$ and the first column of $\\underline{H}_2$**\nFirst, we compute the product $w_1 = Av_1$:\n$$ w_1 = \\begin{bmatrix} 2  10  0\\\\ 0  3  1\\\\ 0  0  4 \\end{bmatrix} \\left( \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} \\right) = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}2+10\\\\3+1\\\\4\\end{pmatrix} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}12\\\\4\\\\4\\end{pmatrix} $$\nNext, we orthogonalize $w_1$ against $v_1$. The projection coefficient $h_{11}$ is:\n$$ h_{11} = v_1^T w_1 = \\left( \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1  1  1\\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{3}}\\begin{pmatrix}12\\\\4\\\\4\\end{pmatrix} \\right) = \\frac{1}{3}(12+4+4) = \\frac{20}{3} $$\nThe unnormalized next vector, $\\tilde{v}_2$, is:\n$$ \\tilde{v}_2 = w_1 - h_{11}v_1 = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}12\\\\4\\\\4\\end{pmatrix} - \\frac{20}{3} \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} = \\frac{1}{3\\sqrt{3}}\\left(3\\begin{pmatrix}12\\\\4\\\\4\\end{pmatrix} - 20\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}\\right) = \\frac{1}{3\\sqrt{3}}\\begin{pmatrix}36-20\\\\12-20\\\\12-20\\end{pmatrix} = \\frac{1}{3\\sqrt{3}}\\begin{pmatrix}16\\\\-8\\\\-8\\end{pmatrix} = \\frac{8}{3\\sqrt{3}}\\begin{pmatrix}2\\\\-1\\\\-1\\end{pmatrix} $$\nThe normalization constant, $h_{21}$, is the norm of $\\tilde{v}_2$:\n$$ h_{21} = \\|\\tilde{v}_2\\|_2 = \\left\\| \\frac{8}{3\\sqrt{3}}\\begin{pmatrix}2\\\\-1\\\\-1\\end{pmatrix} \\right\\|_2 = \\frac{8}{3\\sqrt{3}} \\sqrt{2^2+(-1)^2+(-1)^2} = \\frac{8}{3\\sqrt{3}}\\sqrt{6} = \\frac{8\\sqrt{2}}{3} $$\nSince $h_{21} \\neq 0$, no breakdown occurs. We normalize $\\tilde{v}_2$ to get $v_2$:\n$$ v_2 = \\frac{\\tilde{v}_2}{h_{21}} = \\frac{\\frac{8}{3\\sqrt{3}}\\begin{pmatrix}2\\\\-1\\\\-1\\end{pmatrix}}{\\frac{8\\sqrt{2}}{3}} = \\frac{1}{\\sqrt{3}\\sqrt{2}}\\begin{pmatrix}2\\\\-1\\\\-1\\end{pmatrix} = \\frac{1}{\\sqrt{6}}\\begin{pmatrix}2\\\\-1\\\\-1\\end{pmatrix} $$\n\n**Step 2: Construction of $v_3$ and the second column of $\\underline{H}_2$**\nWe compute the product $w_2 = Av_2$:\n$$ w_2 = \\begin{bmatrix} 2  10  0\\\\ 0  3  1\\\\ 0  0  4 \\end{bmatrix} \\left( \\frac{1}{\\sqrt{6}}\\begin{pmatrix}2\\\\-1\\\\-1\\end{pmatrix} \\right) = \\frac{1}{\\sqrt{6}}\\begin{pmatrix}4-10\\\\-3-1\\\\-4\\end{pmatrix} = \\frac{1}{\\sqrt{6}}\\begin{pmatrix}-6\\\\-4\\\\-4\\end{pmatrix} $$\nNext, we orthogonalize $w_2$ against $\\{v_1, v_2\\}$. The projection coefficients are:\n$$ h_{12} = v_1^T w_2 = \\left( \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1  1  1\\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{6}}\\begin{pmatrix}-6\\\\-4\\\\-4\\end{pmatrix} \\right) = \\frac{-6-4-4}{\\sqrt{18}} = \\frac{-14}{3\\sqrt{2}} = -\\frac{7\\sqrt{2}}{3} $$\n$$ h_{22} = v_2^T w_2 = \\left( \\frac{1}{\\sqrt{6}}\\begin{pmatrix}2  -1  -1\\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{6}}\\begin{pmatrix}-6\\\\-4\\\\-4\\end{pmatrix} \\right) = \\frac{1}{6}(-12+4+4) = \\frac{-4}{6} = -\\frac{2}{3} $$\nThe unnormalized vector $\\tilde{v}_3$ is:\n$$ \\tilde{v}_3 = w_2 - h_{12}v_1 - h_{22}v_2 = \\frac{1}{\\sqrt{6}}\\begin{pmatrix}-6\\\\-4\\\\-4\\end{pmatrix} - \\left(-\\frac{7\\sqrt{2}}{3}\\right)\\frac{1}{\\sqrt{3}}\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} - \\left(-\\frac{2}{3}\\right)\\frac{1}{\\sqrt{6}}\\begin{pmatrix}2\\\\-1\\\\-1\\end{pmatrix} $$\n$$ \\tilde{v}_3 = \\frac{1}{\\sqrt{6}}\\begin{pmatrix}-6\\\\-4\\\\-4\\end{pmatrix} + \\frac{7\\sqrt{2}}{3\\sqrt{3}}\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} + \\frac{2}{3\\sqrt{6}}\\begin{pmatrix}2\\\\-1\\\\-1\\end{pmatrix} $$\nTo combine these terms, we use a common factor of $\\frac{1}{3\\sqrt{6}}$. Note that $\\frac{7\\sqrt{2}}{3\\sqrt{3}} = \\frac{7\\sqrt{2}\\sqrt{2}}{3\\sqrt{3}\\sqrt{2}} = \\frac{14}{3\\sqrt{6}}$.\n$$ \\tilde{v}_3 = \\frac{1}{3\\sqrt{6}} \\left( 3\\begin{pmatrix}-6\\\\-4\\\\-4\\end{pmatrix} + 14\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} + 2\\begin{pmatrix}2\\\\-1\\\\-1\\end{pmatrix} \\right) = \\frac{1}{3\\sqrt{6}} \\begin{pmatrix}-18+14+4\\\\-12+14-2\\\\-12+14-2\\end{pmatrix} = \\frac{1}{3\\sqrt{6}}\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix} = 0 $$\nThe resulting vector $\\tilde{v}_3$ is the zero vector. This signifies an exact breakdown of the Arnoldi process.\n\n**Analysis of the Exact Breakdown**\nThe subdiagonal entry $h_{32}$ is the normalization constant, which is the norm of $\\tilde{v}_3$:\n$$ h_{32} = \\|\\tilde{v}_3\\|_2 = \\|0\\|_2 = 0 $$\nAn exact breakdown ($h_{j+1,j}=0$) implies that the Krylov subspace \"stagnates\"; its dimension is $j$. Here, the dimension of $\\mathcal{K}_m(A, v_1)$ is $2$ for all $m \\geq 2$. This occurs because the space $\\mathcal{K}_2(A, v_1) = \\text{span}\\{v_1, v_2\\}$ is an A-invariant subspace. The condition $\\tilde{v}_3=0$ is equivalent to $Av_2 = h_{12}v_1 + h_{22}v_2$, which demonstrates this invariance.\n\nThe Arnoldi relation is $AV_2 = V_3\\underline{H}_2$. Since $h_{32}=0$, the relation for the second column $Av_2 = h_{12}v_1 + h_{22}v_2 + h_{32}v_3$ simplifies to $Av_2 = h_{12}v_1 + h_{22}v_2$, which is independent of $v_3$.\nThe problem requires us to define $v_3$ via an explicit orthonormal completion. We must find a unit vector $v_3=(x,y,z)^T$ orthogonal to both $v_1$ and $v_2$.\n$ v_1^T v_3 = \\frac{1}{\\sqrt{3}}(x+y+z) = 0 \\implies x+y+z=0 $\n$ v_2^T v_3 = \\frac{1}{\\sqrt{6}}(2x-y-z) = 0 \\implies 2x-y-z=0 $\nAdding these two equations yields $3x=0$, so $x=0$. This implies $y+z=0$, or $z=-y$.\nThe vector must be of the form $\\begin{pmatrix}0\\\\y\\\\-y\\end{pmatrix}$. To be a unit vector, its norm must be $1$:\n$ \\| \\begin{pmatrix}0\\\\y\\\\-y\\end{pmatrix} \\|_2 = \\sqrt{0^2+y^2+(-y)^2} = \\sqrt{2y^2} = |y|\\sqrt{2} = 1 \\implies |y|=\\frac{1}{\\sqrt{2}} $.\nWe choose $y=\\frac{1}{\\sqrt{2}}$, which gives one possible completion: $v_3 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}0\\\\1\\\\-1\\end{pmatrix}$.\n\n**Final Constructed Matrices**\nThe orthonormal basis vectors are:\n$v_1 = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$, $v_2 = \\frac{1}{\\sqrt{6}}\\begin{pmatrix}2\\\\-1\\\\-1\\end{pmatrix}$, $v_3 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}0\\\\1\\\\-1\\end{pmatrix}$.\nThe matrix $V_3$ is:\n$$ V_3=\\begin{bmatrix}v_1  v_2  v_3\\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{3}}  \\frac{2}{\\sqrt{6}}  0 \\\\ \\frac{1}{\\sqrt{3}}  -\\frac{1}{\\sqrt{6}}  \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{3}}  -\\frac{1}{\\sqrt{6}}  -\\frac{1}{\\sqrt{2}} \\end{bmatrix} $$\nThe $(3 \\times 2)$ upper-Hessenberg matrix $\\underline{H}_2$ is assembled from the computed coefficients:\n$$ \\underline{H}_2 = \\begin{pmatrix} h_{11}  h_{12} \\\\ h_{21}  h_{22} \\\\ 0  h_{32} \\end{pmatrix} = \\begin{pmatrix} \\frac{20}{3}  -\\frac{7\\sqrt{2}}{3} \\\\ \\frac{8\\sqrt{2}}{3}  -\\frac{2}{3} \\\\ 0  0 \\end{pmatrix} $$\nThe subdiagonal entry of interest is $h_{32}$, which is $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "While high nonnormality is often associated with poor GMRES convergence, the actual behavior is more nuanced and depends critically on the initial residual. This practice explores a counterintuitive scenario where GMRES converges in a single step for a highly nonnormal system . By working through this example, you will discover that convergence is dictated not just by the matrix properties alone, but by the alignment of the initial residual with an invariant subspace, providing a deeper insight into the dynamics of Krylov subspace methods.",
            "id": "3411872",
            "problem": "Consider the linear system arising as a toy model of an upwind-biased discretization of a one-dimensional steady advection–diffusion Partial Differential Equation (PDE), which leads to a highly nonnormal upper-triangular coefficient matrix. Let \n$$\nA=\\begin{bmatrix}1100\\\\02\\end{bmatrix},\\quad b=\\begin{bmatrix}1\\\\0\\end{bmatrix},\\quad x_{0}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.\n$$\nApply the unrestarted Generalized Minimal Residual (GMRES) method (Generalized Minimal Residual (GMRES) is a Krylov subspace method that, at iteration $k$, selects $x_{k}$ from $x_{0}+\\mathcal{K}_{k}(A,r_{0})$ to minimize the Euclidean norm of the residual $r_{k}=b-Ax_{k}$, where $r_{0}=b-Ax_{0}$ and $\\mathcal{K}_{k}(A,r_{0})=\\operatorname{span}\\{r_{0},Ar_{0},\\dots,A^{k-1}r_{0}\\}$) using the standard Euclidean inner product and no preconditioning.\n\nStarting from $x_{0}$, compute explicitly the first two GMRES iterates $x_{1}$ and $x_{2}$ and the corresponding residuals $r_{1}$ and $r_{2}$, along with their Euclidean norms $\\|r_{1}\\|_{2}$ and $\\|r_{2}\\|_{2}$. Then, explain qualitatively, using only fundamental definitions (Krylov subspaces, residual polynomials, invariant subspaces, and nonnormality), the role of matrix nonnormality in the observed residual decrease for this data.\n\nFor the final numeric entry, report the exact value of the second-step residual Euclidean norm $\\|r_{2}\\|_{2}$. Do not round your answer.",
            "solution": "We begin from the defining properties of the Generalized Minimal Residual (GMRES) method. Given $A\\in\\mathbb{R}^{n\\times n}$, $b\\in\\mathbb{R}^{n}$, and $x_{0}\\in\\mathbb{R}^{n}$, GMRES selects at iteration $k$ an approximation $x_{k}\\in x_{0}+\\mathcal{K}_{k}(A,r_{0})$, where $\\mathcal{K}_{k}(A,r_{0})=\\operatorname{span}\\{r_{0},Ar_{0},\\dots,A^{k-1}r_{0}\\}$ and $r_{0}=b-Ax_{0}$, to minimize the Euclidean norm $\\|r_{k}\\|_{2}=\\|b-Ax_{k}\\|_{2}$. The Arnoldi process constructs an orthonormal basis of $\\mathcal{K}_{k}(A,r_{0})$ and an upper Hessenberg matrix that encodes the projection of $A$ onto that subspace; GMRES solves a small least-squares problem to enforce the residual minimization.\n\nWe compute from first principles. With the given data,\n$$\nA=\\begin{bmatrix}1100\\\\02\\end{bmatrix},\\quad b=\\begin{bmatrix}1\\\\0\\end{bmatrix},\\quad x_{0}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.\n$$\nThe initial residual is\n$$\nr_{0}=b-Ax_{0}=b=\\begin{bmatrix}1\\\\0\\end{bmatrix},\\quad \\|r_{0}\\|_{2}=\\left\\|\\begin{bmatrix}1\\\\0\\end{bmatrix}\\right\\|_{2}=1.\n$$\nThus the first Arnoldi vector is\n$$\nv_{1}=\\frac{r_{0}}{\\|r_{0}\\|_{2}}=\\begin{bmatrix}1\\\\0\\end{bmatrix}.\n$$\nApply $A$ to $v_{1}$:\n$$\nw=Av_{1}=\\begin{bmatrix}1\\\\0\\end{bmatrix}.\n$$\nProject $w$ onto $v_{1}$ to obtain the first Hessenberg entry $h_{11}$ and the next Arnoldi residual:\n$$\nh_{11}=v_{1}^{\\top}w=\\begin{bmatrix}10\\end{bmatrix}\\begin{bmatrix}1\\\\0\\end{bmatrix}=1,\\qquad w\\leftarrow w-h_{11}v_{1}=\\begin{bmatrix}1\\\\0\\end{bmatrix}-1\\cdot\\begin{bmatrix}1\\\\0\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.\n$$\nTherefore, $h_{21}=\\|w\\|_{2}=0$, which indicates an Arnoldi breakdown at step $1$. This breakdown is benign and signifies that $\\mathcal{K}_{1}(A,r_{0})$ is an invariant subspace of $A$.\n\nAt iteration $k=1$, GMRES solves the $1\\times 1$ least-squares problem\n$$\n\\min_{y\\in\\mathbb{R}}\\left\\|\\beta e_{1}-H_{1}y\\right\\|_{2},\\quad \\beta=\\|r_{0}\\|_{2}=1,\\quad H_{1}=[h_{11}]=[1].\n$$\nThe minimizer is $y_{1}=1$. The first iterate is\n$$\nx_{1}=x_{0}+V_{1}y_{1}=x_{0}+v_{1}\\,y_{1}=\\begin{bmatrix}0\\\\0\\end{bmatrix}+1\\cdot\\begin{bmatrix}1\\\\0\\end{bmatrix}=\\begin{bmatrix}1\\\\0\\end{bmatrix}.\n$$\nThe corresponding residual is\n$$\nr_{1}=b-Ax_{1}=\\begin{bmatrix}1\\\\0\\end{bmatrix}-\\begin{bmatrix}1\\\\0\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\end{bmatrix},\\quad \\|r_{1}\\|_{2}=0.\n$$\nBecause the residual is already zero at iteration $k=1$, GMRES terminates. If one formally proceeds to $k=2$, the iterate remains unchanged:\n$$\nx_{2}=x_{1}=\\begin{bmatrix}1\\\\0\\end{bmatrix},\\quad r_{2}=b-Ax_{2}=\\begin{bmatrix}0\\\\0\\end{bmatrix},\\quad \\|r_{2}\\|_{2}=0.\n$$\n\nWe now explain the role of nonnormality in the observed residual decrease using fundamental concepts. A matrix $A$ is nonnormal if $AA^{\\top}\\neq A^{\\top}A$; here,\n$$\nAA^{\\top}=\\begin{bmatrix}10001200\\\\2004\\end{bmatrix}\\neq \\begin{bmatrix}1100\\\\10010004\\end{bmatrix}=A^{\\top}A,\n$$\nso $A$ is nonnormal. Nonnormality is associated with nonorthogonal eigenvectors and the possibility of transient growth in norms of powers of $A$ or of residual polynomials applied to $A$, even when all eigenvalues lie in regions of the complex plane that might suggest decay. However, the behavior of Krylov subspace methods depends not only on the spectrum of $A$ but also on how the initial residual $r_{0}$ projects onto invariant subspaces of $A$.\n\nIn our case, $v_{1}=r_{0}/\\|r_{0}\\|_{2}=\\begin{bmatrix}1\\\\0\\end{bmatrix}$ is an eigenvector of $A$ with eigenvalue $\\lambda_{1}=1$, since\n$$\nA\\begin{bmatrix}1\\\\0\\end{bmatrix}=\\begin{bmatrix}1\\\\0\\end{bmatrix}.\n$$\nThus, $\\mathcal{K}_{1}(A,r_{0})=\\operatorname{span}\\{r_{0}\\}$ is an invariant subspace of $A$. The GMRES residual after one step can be written in terms of a degree-$1$ residual polynomial $p_{1}$ satisfying $p_{1}(0)=1$ as\n$$\nr_{1}=p_{1}(A)r_{0}.\n$$\nBecause $r_{0}$ lies in the invariant subspace associated with $\\lambda_{1}=1$, choosing $p_{1}(t)=1-t$ yields $r_{1}=(I-A)r_{0}=0$. GMRES, by minimizing the residual norm over $\\mathcal{K}_{1}(A,r_{0})$, effectively discovers this annihilating polynomial and produces $r_{1}=0$ in a single step. In other words, despite the strong nonnormality of $A$ (large upper off-diagonal), the initial residual aligns with an invariant eigen-direction that is decoupled from the nonnormal coupling; this prevents any transient amplification and yields immediate convergence. Had $r_{0}$ possessed a component along the other eigen-direction, the nonorthogonality of eigenvectors could have induced nontrivial transient behavior before eventual decay, but that scenario does not arise here due to the special alignment of $r_{0}$.\n\nTherefore, the exact value of the second-step residual Euclidean norm is $\\|r_{2}\\|_{2}=0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "In numerical practice, we often stop an iterative solver when the residual norm is small, assuming this implies the error in the solution is also small. This crucial exercise demonstrates why that assumption can be deeply flawed for nonnormal systems . You will analyze a system where an arbitrarily small residual can coexist with a solution error that remains large, and you will quantify this gap by calculating the norm of the inverse matrix, $\\|A^{-1}\\|_2$, which acts as an amplification factor. This provides a stark, practical lesson on the potential pitfalls of relying solely on the residual as a termination criterion.",
            "id": "3411893",
            "problem": "Consider the linear system $A x = b$ arising from a backward Euler time discretization of the one-dimensional constant-coefficient linear advection equation $u_{t} + c\\,u_{x} = f$ on a two-cell uniform grid with homogeneous Dirichlet boundary conditions, using a first-order upwind spatial discretization. For a single time step of size $\\Delta t$, with grid spacing $\\Delta x$, the resulting $2 \\times 2$ system matrix can be written in the form $A = I + N$, where $I$ is the identity and $N$ is strictly upper triangular with $(N)_{12} = M$ and all other entries zero, where $M = c\\,\\Delta t/\\Delta x  0$. This matrix $A$ is highly nonnormal for large $M$.\n\nLet $x$ be the exact solution and $x_{k}$ be the $k$th iterate produced by a Krylov subspace method such as Generalized Minimal Residual (GMRES). Denote the residual by $r_{k} = b - A x_{k}$ and the error by $e_{k} = x - x_{k}$. Fundamental relations for any nonsingular matrix give $A e_{k} = r_{k}$ and hence $e_{k} = A^{-1} r_{k}$.\n\nDefine the departure from normality via the Schur decomposition: if $A = Q T Q^{*}$ with unitary $Q$ and upper triangular $T = \\Lambda + N_{\\text{schur}}$, where $\\Lambda$ is diagonal and $N_{\\text{schur}}$ is strictly upper triangular, set $\\delta(A) = \\|N_{\\text{schur}}\\|_{2}$. For the present $2 \\times 2$ matrix, $A$ is already upper triangular, so $\\delta(A) = \\|N\\|_{2} = M$.\n\nSuppose after $k$ iterations the residual is $r_{k} = \\begin{pmatrix} 0 \\\\ \\varepsilon \\end{pmatrix}$ with $\\varepsilon = 10^{-6}$, which is small in norm. The amplification factor that quantifies the possible gap between residual and error norms is given by the operator norm $\\|A^{-1}\\|_{2}$, since $\\|e_{k}\\|_{2} \\leq \\|A^{-1}\\|_{2} \\,\\|r_{k}\\|_{2}$.\n\nUsing only the structural facts above and the definition of $\\delta(A)$, derive an analytic expression for $\\|A^{-1}\\|_{2}$ in terms of $M = \\delta(A)$, and then evaluate this expression at $M = 10^{6}$. Report the residual-to-error amplification factor estimate $G_{\\text{est}} = \\|A^{-1}\\|_{2}$ rounded to four significant figures. Do not include any units in your final reported value.",
            "solution": "The problem statement has been critically validated and is deemed to be a valid, well-posed problem in numerical linear algebra. It is scientifically grounded, self-contained, and objective. There are no contradictions, ambiguities, or unsound premises. We may therefore proceed with the solution.\n\nThe problem asks for an analytic expression for the residual-to-error amplification factor, given by $G_{\\text{est}} = \\|A^{-1}\\|_{2}$, and its numerical value for a specific case. The matrix $A$ is defined for a $2 \\times 2$ system as $A = I + N$, where $I$ is the identity matrix and $N$ is a strictly upper triangular matrix.\n\nFrom the problem description, we have:\n$I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$\nThe matrix $N$ is strictly upper triangular with $(N)_{12} = M$ and all other entries being zero. Thus,\n$N = \\begin{pmatrix} 0  M \\\\ 0  0 \\end{pmatrix}$\nThe system matrix $A$ is therefore:\n$$A = I + N = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 0  M \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  M \\\\ 0  1 \\end{pmatrix}$$\nThe problem states that the departure from normality is $\\delta(A) = M$.\n\nTo find the amplification factor $\\|A^{-1}\\|_{2}$, we first compute the inverse of $A$. For a $2 \\times 2$ matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, the inverse is given by $\\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$.\nFor our matrix $A$, we have $a=1$, $b=M$, $c=0$, and $d=1$. The determinant is $\\det(A) = (1)(1) - (M)(0) = 1$.\nThe inverse is:\n$$A^{-1} = \\frac{1}{1} \\begin{pmatrix} 1  -M \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  -M \\\\ 0  1 \\end{pmatrix}$$\n\nNext, we compute the 2-norm of $A^{-1}$. The 2-norm of any matrix $B$, denoted $\\|B\\|_{2}$, is the square root of the largest eigenvalue of the matrix product $B^*B$, where $B^*$ is the conjugate transpose of $B$. Here, $B = A^{-1}$. Since $A^{-1}$ has all real entries, its conjugate transpose is simply its transpose.\n$$(A^{-1})^* = (A^{-1})^T = \\begin{pmatrix} 1  0 \\\\ -M  1 \\end{pmatrix}$$\nNow, we form the product $(A^{-1})^*A^{-1}$:\n$$(A^{-1})^*A^{-1} = \\begin{pmatrix} 1  0 \\\\ -M  1 \\end{pmatrix} \\begin{pmatrix} 1  -M \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} (1)(1)+(0)(0)  (1)(-M)+(0)(1) \\\\ (-M)(1)+(1)(0)  (-M)(-M)+(1)(1) \\end{pmatrix} = \\begin{pmatrix} 1  -M \\\\ -M  M^2+1 \\end{pmatrix}$$\nThe eigenvalues, $\\lambda$, of this matrix are the roots of its characteristic equation, $\\det((A^{-1})^*A^{-1} - \\lambda I) = 0$.\n$$\\det\\begin{pmatrix} 1-\\lambda  -M \\\\ -M  M^2+1-\\lambda \\end{pmatrix} = 0$$\n$$(1-\\lambda)(M^2+1-\\lambda) - (-M)(-M) = 0$$\n$$M^2 + 1 - \\lambda - \\lambda M^2 - \\lambda + \\lambda^2 - M^2 = 0$$\n$$\\lambda^2 - (M^2+2)\\lambda + 1 = 0$$\nWe solve this quadratic equation for $\\lambda$ using the quadratic formula:\n$$\\lambda = \\frac{-[-(M^2+2)] \\pm \\sqrt{[-(M^2+2)]^2 - 4(1)(1)}}{2(1)} = \\frac{M^2+2 \\pm \\sqrt{(M^2+2)^2 - 4}}{2}$$\n$$\\lambda = \\frac{M^2+2 \\pm \\sqrt{M^4+4M^2+4 - 4}}{2} = \\frac{M^2+2 \\pm \\sqrt{M^4+4M^2}}{2}$$\nSince $M0$ is given, we can factor $M^2$ out of the square root:\n$$\\lambda = \\frac{M^2+2 \\pm \\sqrt{M^2(M^2+4)}}{2} = \\frac{M^2+2 \\pm M\\sqrt{M^2+4}}{2}$$\nThe two eigenvalues are $\\lambda_1 = \\frac{M^2+2 + M\\sqrt{M^2+4}}{2}$ and $\\lambda_2 = \\frac{M^2+2 - M\\sqrt{M^2+4}}{2}$.\nThe squared 2-norm, $\\|A^{-1}\\|_{2}^2$, is the larger of these two eigenvalues, $\\lambda_{max}$.\n$$\\lambda_{max} = \\frac{M^2+2 + M\\sqrt{M^2+4}}{2}$$\nTherefore, the first part of the problem is solved: the analytic expression for $\\|A^{-1}\\|_{2}$ in terms of $M = \\delta(A)$ is:\n$$\\|A^{-1}\\|_{2} = \\sqrt{\\frac{M^2+2 + M\\sqrt{M^2+4}}{2}}$$\nThe second part is to evaluate this expression for $M = 10^6$. Let $G_{\\text{est}} = \\|A^{-1}\\|_{2}$.\n$$G_{\\text{est}} = \\sqrt{\\frac{(10^6)^2+2 + 10^6\\sqrt{(10^6)^2+4}}{2}} = \\sqrt{\\frac{10^{12}+2 + 10^6\\sqrt{10^{12}+4}}{2}}$$\nFor a very large value of $M$, such as $M=10^6$, we can analyze the behavior of the expression. The term $\\sqrt{M^2+4}$ is very close to $M$. Using a binomial expansion, $\\sqrt{M^2+4} = M\\sqrt{1+4/M^2} \\approx M(1 + \\frac{1}{2}\\frac{4}{M^2}) = M + \\frac{2}{M}$.\nSubstituting this into the expression for $\\lambda_{max}$:\n$$\\lambda_{max} \\approx \\frac{M^2+2 + M(M+2/M)}{2} = \\frac{M^2+2+M^2+2}{2} = M^2+2$$\nSo, $\\|A^{-1}\\|_2 \\approx \\sqrt{M^2+2} \\approx M$.\nFor $M=10^6$, the value of $G_{\\text{est}}$ is expected to be extremely close to $10^6$.\nA precise numerical calculation gives:\n$$G_{\\text{est}} \\approx 1000000.000001$$\nIn scientific notation, this is $1.000000000001 \\times 10^6$.\nThe problem requires this value rounded to four significant figures.\n$$G_{\\text{est}} \\approx 1.000 \\times 10^6$$\nThis result highlights the potential for large error amplification in systems with highly nonnormal matrices, even when the residual is small. The small residual $r_k$ with norm $\\|r_k\\|_2 = \\varepsilon = 10^{-6}$ can correspond to an error $e_k$ with a norm $\\|e_k\\|_2$ as large as $\\|A^{-1}\\|_2\\|r_k\\|_2 \\approx (10^6)(10^{-6}) = 1$. The error norm can be of order unity, meaning the computed solution $x_k$ may have no correct digits, despite a very small residual.",
            "answer": "$$\\boxed{1.000 \\times 10^6}$$"
        }
    ]
}