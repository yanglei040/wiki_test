{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of any iterative algorithm, it's essential to first work through its mechanics on a small scale. This practice guides you through a single iteration of the Preconditioned Conjugate Gradient (PCG) method for a simple $3 \\times 3$ system. By performing this calculation by hand, you will become familiar with the fundamental components of the algorithm, such as the residual, the preconditioned residual, the search direction, and the step length, and see how they work together to update the solution estimate.",
            "id": "1029864",
            "problem": "Consider the linear system $ A \\mathbf{x} = \\mathbf{b} $, where  \n$$ A = \\begin{bmatrix} 4 & -2 & 0 \\\\ -2 & 3 & -1 \\\\ 0 & -1 & 2 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{b} = \\begin{bmatrix} 4 \\\\ -3 \\\\ 2 \\end{bmatrix}. $$  \nApply the preconditioned conjugate gradient (PCG) method with diagonal scaling (Jacobi preconditioner), starting from the initial guess $ \\mathbf{x}_0 = \\mathbf{0} $. After one iteration, compute the 2-norm of the residual vector. The Jacobi preconditioner $ M $ is the diagonal part of $ A $, so $ M = \\operatorname{diag}(4, 3, 2) $.",
            "solution": "1. Initial residual  \n$$r_0 = b - A x_0 = \\begin{bmatrix}4\\\\-3\\\\2\\end{bmatrix}.$$\n2. Preconditioned residual  \n$$M^{-1} = \\operatorname{diag}\\bigl(\\tfrac14,\\tfrac13,\\tfrac12\\bigr),\\qquad \nz_0 = M^{-1}r_0 = \\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}.$$\n3. Search direction  \n$$p_0 = z_0 = \\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}.$$\n4. Matrix–vector product  \n$$A p_0 = \\begin{bmatrix}4&-2&0\\\\-2&3&-1\\\\0&-1&2\\end{bmatrix}\n\\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}\n=\\begin{bmatrix}6\\\\-6\\\\3\\end{bmatrix}.$$\n5. Step length  \n$$\\alpha_0 = \\frac{r_0^T z_0}{p_0^T A p_0}\n=\\frac{9}{15}=\\frac35.$$\n6. Updated residual  \n$$r_1 = r_0 - \\alpha_0 A p_0\n=\\begin{bmatrix}4\\\\-3\\\\2\\end{bmatrix}\n-\\frac35\\begin{bmatrix}6\\\\-6\\\\3\\end{bmatrix}\n=\\begin{bmatrix}0.4\\\\0.6\\\\0.2\\end{bmatrix}.$$\n7. 2-norm of the residual  \n$$\\|r_1\\|_2 = \\sqrt{0.4^2 + 0.6^2 + 0.2^2}\n= \\frac{\\sqrt{14}}{5}.$$",
            "answer": "$$\\boxed{\\frac{\\sqrt{14}}{5}}$$"
        },
        {
            "introduction": "Having familiarized ourselves with the mechanics of PCG, we now explore a fundamental principle that governs its remarkable efficiency. The convergence rate of the Conjugate Gradient method is intimately linked to the spectral properties of the system matrix. This exercise challenges you to construct a system where the preconditioned operator has a specific number of distinct eigenvalues, allowing you to directly verify the theorem that PCG will find the exact solution in a number of iterations equal to this count. This practice provides powerful insight into why effective preconditioning, which clusters eigenvalues, is so crucial for rapid convergence.",
            "id": "2427437",
            "problem": "Consider solving a linear system with the Conjugate Gradient (CG) method and its preconditioned variant. Construct explicit matrices $A \\in \\mathbb{R}^{5 \\times 5}$ and $M \\in \\mathbb{R}^{5 \\times 5}$ that are both symmetric positive definite, such that the preconditioned operator $M^{-1}A$ has exactly $2$ distinct eigenvalues. Verify the eigenvalue property by direct reasoning from your construction. Then consider applying the Preconditioned Conjugate Gradient (PCG) method, with preconditioner $M$, to the system $A x = b$ for arbitrary $b \\in \\mathbb{R}^{5}$ and arbitrary initial guess $x_0 \\in \\mathbb{R}^{5}$. Provide a rigorous justification, based on first principles of linear algebra and the algorithm’s defining properties, that PCG terminates with the exact solution in a bounded number of iterations that does not depend on $b$ or $x_0$. Determine the minimal integer $k^\\star$ with this property. \n\nYour final answer must be the value of $k^\\star$ only. No rounding is required.",
            "solution": "The problem requires us to construct specific symmetric positive definite (SPD) matrices $A$ and $M$ of size $5 \\times 5$ such that the preconditioned operator $M^{-1}A$ possesses exactly $2$ distinct eigenvalues. Subsequently, we must provide a rigorous justification for the fact that the Preconditioned Conjugate Gradient (PCG) method for the system $A x = b$ converges to the exact solution in a finite number of iterations, $k^\\star$, which is independent of the choice of the right-hand side $b$ and the initial guess $x_0$. Finally, we must determine this minimal bound $k^\\star$.\n\nFirst, we construct the required matrices $A$ and $M$. A simple and effective construction involves diagonal matrices. Let $M$ be the $5 \\times 5$ identity matrix, $M=I_5$. The identity matrix is symmetric, and all its eigenvalues are $1$, so it is positive definite.\nNext, let us construct the matrix $A$. To ensure $M^{-1}A$ has two distinct eigenvalues, and given our choice of $M=I_5$, the matrix $A$ itself must have two distinct eigenvalues. We also need $A$ to be symmetric and positive definite. A diagonal matrix with positive entries on the diagonal satisfies these requirements. We can choose:\n$$\nA = \\text{diag}(1, 1, 1, 2, 2)\n$$\nThis matrix $A$ is symmetric by construction. Its eigenvalues are its diagonal entries, which are $1$ and $2$. Since all eigenvalues are positive, $A$ is positive definite.\nThe preconditioned operator is $M^{-1}A = I_5^{-1}A = A$. The eigenvalues of $M^{-1}A$ are therefore $\\{1, 1, 1, 2, 2\\}$. The set of distinct eigenvalues is $\\{\\lambda_1, \\lambda_2\\} = \\{1, 2\\}$. Thus, there are exactly $2$ distinct eigenvalues, as required by the problem statement. Our construction of $A$ and $M$ is valid.\n\nNow, we must analyze the convergence of the PCG method. The PCG algorithm for solving the system $A x = b$ with an SPD preconditioner $M$ is mathematically equivalent to applying the standard Conjugate Gradient (CG) algorithm to a transformed linear system. Since $M$ is SPD, it has a unique Cholesky factorization $M = L L^T$, where $L$ is a nonsingular lower triangular matrix.\n\nWe can transform the original system $A x = b$ as follows:\n$$\nA x = b \\implies (L^{-1} A L^{-T}) (L^T x) = L^{-1} b\n$$\nLet us define $\\hat{A} = L^{-1} A L^{-T}$, $\\hat{x} = L^T x$, and $\\hat{b} = L^{-1}b$. The system becomes $\\hat{A} \\hat{x} = \\hat{b}$.\nThe matrix $\\hat{A}$ is SPD. It is symmetric because $A$ is symmetric:\n$$\n\\hat{A}^T = (L^{-1} A L^{-T})^T = (L^{-T})^T A^T (L^{-1})^T = L^{-1} A L^{-T} = \\hat{A}\n$$\nIt is positive definite because $A$ is SPD and $L^{-T}$ is nonsingular. For any non-zero vector $y \\in \\mathbb{R}^5$, let $z = L^{-T}y$. Since $L^{-T}$ is nonsingular, $z \\neq 0$. Then:\n$$\ny^T \\hat{A} y = y^T (L^{-1} A L^{-T}) y = (L^{-T}y)^T A (L^{-T}y) = z^T A z > 0\n$$\nThe PCG algorithm applied to $A x = b$ is designed such that the sequence of iterates $x_k$ it generates corresponds to the sequence of iterates $\\hat{x}_k = L^T x_k$ generated by the standard CG algorithm applied to $\\hat{A} \\hat{x} = \\hat{b}$.\n\nA fundamental theorem of the CG method states that the algorithm terminates with the exact solution in at most $m$ iterations, where $m$ is the number of distinct eigenvalues of the system matrix. This property arises because the error $e_k = x - x_k$ can be expressed as $e_k = P_k(A) e_0$ for some polynomial $P_k$ of degree $k$ with $P_k(0)=1$. CG finds the polynomial that minimizes the $A$-norm of the error. If the matrix has $m$ distinct eigenvalues $\\{\\mu_1, \\dots, \\mu_m\\}$, one can construct a polynomial $Q(t) = \\prod_{i=1}^m (1 - t/\\mu_i)$ of degree $m$ that vanishes at all eigenvalues and satisfies $Q(0)=1$. The CG algorithm finds this polynomial by step $m$, resulting in a zero error. This holds for any initial guess $x_0$ and right hand side $b$.\n\nFor our preconditioned system, the relevant system matrix is $\\hat{A}$. We need to determine the number of distinct eigenvalues of $\\hat{A}$. The matrices $\\hat{A}$ and $M^{-1}A$ are similar, which means they share the same eigenvalues. We can show this similarity transformation explicitly:\n$$\n\\hat{A} = L^{-1} A L^{-T} = L^{-1} (M M^{-1}) A L^{-T} = L^{-1} (L L^T) (M^{-1}A) (L^T)^{-1} = (L^{-1}L) L^T (M^{-1}A) (L^T)^{-1} = L^T (M^{-1}A) (L^T)^{-1}\n$$\nSince $\\hat{A}$ is a similarity transformation of $M^{-1}A$, they have the same characteristic polynomial and thus the same eigenvalues.\n\nFrom our construction, the preconditioned matrix $M^{-1}A$ has exactly $2$ distinct eigenvalues. Consequently, the transformed matrix $\\hat{A}$ also has exactly $2$ distinct eigenvalues.\n\nTherefore, applying the standard CG convergence theorem to the system $\\hat{A} \\hat{x} = \\hat{b}$, the algorithm is guaranteed to find the exact solution $\\hat{x}$ in at most $2$ iterations. As $\\hat{x}_k = L^T x_k$ and $L$ is nonsingular, if $\\hat{x}_k = \\hat{x}$, then $x_k = x$. This convergence in a maximum of $2$ steps is guaranteed for any initial guess $\\hat{x}_0 = L^T x_0$ and any right-hand side $\\hat{b} = L^{-1} b$, which is equivalent to any $x_0$ and $b$ since $L$ is invertible. While for specific initial conditions convergence may occur in $1$ iteration (if the initial residual is an eigenvector of $\\hat{A}$), the bound must hold for arbitrary inputs. The worst-case scenario, which dictates the bound, requires that the initial residual has components in the eigenspaces of all distinct eigenvalues.\n\nThe minimal integer $k^\\star$ that bounds the number of iterations for any $b$ and $x_0$ is therefore the number of distinct eigenvalues of the preconditioned operator $M^{-1}A$. In this problem, this number is $2$.\nSo, $k^\\star = 2$.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "We now transition from theoretical exercises to a realistic computational application, tackling a linear system that arises from the discretization of a partial differential equation. This comprehensive practice involves not only implementing the PCG algorithm but also integrating a powerful diagnostic tool: the Lanczos process. By running the Lanczos iteration in parallel with PCG, we can dynamically estimate the extremal eigenvalues and condition number of the preconditioned system, providing crucial insights for predicting convergence behavior and developing adaptive solution strategies.",
            "id": "3433983",
            "problem": "Consider the linear system arising from a finite-difference discretization of a symmetric positive definite partial differential operator on a rectangular grid with homogeneous Dirichlet boundary conditions. Let the matrix be denoted by $A \\in \\mathbb{R}^{n \\times n}$, and consider a symmetric positive definite preconditioner $M \\in \\mathbb{R}^{n \\times n}$ (for this problem, take $M$ to be the diagonal of $A$). We apply the Preconditioned Conjugate Gradient (PCG) method to solve $A x = b$ starting from $x_0 = 0$, and simultaneously build a Lanczos tridiagonalization of the preconditioned operator $H = M^{-1} A$ with respect to the $M$-inner product $(u,v)_M = u^{\\top} M v$. The goals are: (i) to estimate the extremal eigenvalues of $H$ in-flight by using the symmetric tridiagonal matrix $T_k$ generated by $k$ Lanczos steps, and (ii) to use these estimates either to adapt a stopping criterion based on the classical Preconditioned Conjugate Gradient convergence bound, or to set the parameters of a Chebyshev polynomial smoother.\n\nFundamental base. Use only the following well-tested facts:\n- For symmetric positive definite $A$ and $M$, the operator $H = M^{-1} A$ is self-adjoint in the $M$-inner product, that is $(u, H v)_M = (H u, v)_M$ for all $u,v \\in \\mathbb{R}^n$.\n- The Lanczos process applied to a self-adjoint operator in an inner product generates $M$-orthonormal basis vectors $\\{v_1,\\dots,v_k\\}$ with $v_i^{\\top} M v_j = \\delta_{ij}$, and a symmetric tridiagonal matrix $T_k$ with diagonal entries $\\alpha_j$ and off-diagonal entries $\\beta_j$, obtained by the three-term recurrence\n$$\nw_j = H v_j - \\beta_{j-1} v_{j-1}, \\quad \\alpha_j = (v_j, H v_j)_M, \\quad \\tilde{w}_j = w_j - \\alpha_j v_j, \\quad \\beta_j = \\|\\tilde{w}_j\\|_M,\\quad v_{j+1} = \\tilde{w}_j/\\beta_j,\n$$\nwith $\\beta_0 = 0$. The eigenvalues of $T_k$ (the Ritz values) approximate extremal eigenvalues of $H$.\n- The classical conjugate gradient worst-case bound for the error in the energy norm induced by $A$ after $k$ iterations applied to $H$ with condition number $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ reads\n$$\n\\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le 2 \\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^k,\n$$\nwhere $e_k = x_* - x_k$ and $x_*$ is the exact solution.\n- For a Chebyshev semi-iterative method applied to a symmetric positive definite operator with spectrum contained in $[\\lambda_{\\min}, \\lambda_{\\max}]$, the worst-case reduction factor after $m$ steps satisfies\n$$\n\\rho_m \\le 2 \\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^m,\n$$\nwhere $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$. The parameters of the iteration can be set from the affine map sending $[\\lambda_{\\min}, \\lambda_{\\max}]$ to $[-1,1]$.\n\nYour tasks are:\n1. Discretize the anisotropic Poisson operator $-\\nabla \\cdot (K \\nabla u)$ on the unit square with uniform grid spacing and homogeneous Dirichlet boundary conditions, where $K = \\mathrm{diag}(1, \\alpha)$ and $\\alpha > 0$ is a given anisotropy factor. Assemble the sparse matrix $A$ using the standard five-point stencil:\n   - In the $x$-direction use the one-dimensional stiffness $T_x$ scaled by $1/h_x^2$.\n   - In the $y$-direction use the one-dimensional stiffness $T_y$ scaled by $\\alpha/h_y^2$.\n   Construct $A = I_y \\otimes T_x + T_y \\otimes I_x$. Optionally add a positive shift $\\sigma I$ to test a narrowed spectrum; when $\\sigma = 0$ no shift is present.\n2. Choose $M = \\mathrm{diag}(A)$ as preconditioner, and define $H = M^{-1} A$ implicitly via applications of $A$ and $M^{-1}$ (do not form $H$ explicitly).\n3. Run PCG starting from $x_0 = 0$ with right-hand side $b = \\mathbf{1}$ (the all-ones vector). Simultaneously, run $k$ steps of the Lanczos process on $H$ in the $M$-inner product, with initial vector $v_1 = z_0 / \\|z_0\\|_M$ where $z_0 = M^{-1} r_0$ and $r_0 = b - A x_0$. At each Lanczos step, form the partial tridiagonal matrix $T_j$ via its scalars $\\alpha_j$ and $\\beta_j$, and at a specified capture iteration $k_{\\mathrm{cap}}$ compute the Ritz value estimates $\\hat{\\lambda}_{\\min}$ and $\\hat{\\lambda}_{\\max}$ as the minimal and maximal eigenvalues of $T_{k_{\\mathrm{cap}}}$. If breakdown occurs earlier (that is, $\\beta_j \\approx 0$ for some $j < k_{\\mathrm{cap}}$), use the current $T_j$ instead.\n4. Using $\\hat{\\lambda}_{\\min}$ and $\\hat{\\lambda}_{\\max}$:\n   - Compute the estimated condition number $\\hat{\\kappa} = \\hat{\\lambda}_{\\max}/\\hat{\\lambda}_{\\min}$ and $q = (\\sqrt{\\hat{\\kappa}} - 1)/(\\sqrt{\\hat{\\kappa}} + 1)$.\n   - For a given target energy-norm relative error $\\tau \\in (0,1)$, compute the predicted total number of PCG iterations\n     $$\n     k_{\\mathrm{pred}} = \\left\\lceil \\frac{\\log(\\tau/2)}{\\log(q)} \\right\\rceil,\n     $$\n     with the understanding that if $\\hat{\\kappa} = 1$ then $q = 0$ and $k_{\\mathrm{pred}} = 0$. Let the predicted remaining iterations after the capture step be $k_{\\mathrm{rem,pred}} = \\max\\{0, k_{\\mathrm{pred}} - k_{\\mathrm{cap}}\\}$.\n   - For a chosen Chebyshev smoothing degree $m$, set the Chebyshev parameters from $[\\hat{\\lambda}_{\\min}, \\hat{\\lambda}_{\\max}]$ and compute the predicted worst-case smoothing factor\n     $$\n     \\rho_{\\mathrm{pred}} = 2 \\, q^m.\n     $$\n5. Continue PCG until the energy norm satisfies $\\|e_k\\|_A / \\|e_0\\|_A \\le \\tau$, where $\\|e_k\\|_A = \\sqrt{e_k^{\\top} A e_k}$, and record the actual remaining iterations after capture, $k_{\\mathrm{rem,act}} = \\max\\{0, k_{\\mathrm{stop}} - k_{\\mathrm{cap}}\\}$. Compute $\\|e_k\\|_A$ using the exact solution $x_* = A^{-1} b$ obtained by a sparse direct solve. Note that there are no physical units involved.\n\nTest suite. Your program must implement the above for the following parameter sets, which together probe typical, anisotropic, and spectrum-shifted regimes:\n- Case 1 (happy path): $(n_x,n_y,\\alpha,\\sigma,k_{\\mathrm{cap}},m,\\tau) = (16,16,1.0,0.0,10,8,10^{-6})$.\n- Case 2 (anisotropy edge): $(n_x,n_y,\\alpha,\\sigma,k_{\\mathrm{cap}},m,\\tau) = (32,32,0.01,0.0,20,8,10^{-6})$.\n- Case 3 (shifted spectrum): $(n_x,n_y,\\alpha,\\sigma,k_{\\mathrm{cap}},m,\\tau) = (32,32,1.0,10.0,15,6,10^{-6})$.\n\nFinal output format. Your program should produce a single line of output containing a list of results for each case, where each case’s result is a list\n$\n[\\hat{\\lambda}_{\\min}, \\hat{\\lambda}_{\\max}, \\hat{\\kappa}, k_{\\mathrm{rem,pred}}, k_{\\mathrm{rem,act}}, \\rho_{\\mathrm{pred}}].\n$\nAll floating-point values must be printed in standard decimal form. The overall output must be a single line in the exact format\n$\n[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]].\n$\nThere are no angles or physical units in this problem; all quantities are dimensionless real numbers.",
            "solution": "The problem is assessed to be valid. It is a well-posed, scientifically-grounded exercise in numerical linear algebra, with a complete and consistent set of definitions and parameters. The tasks involve standard, verifiable algorithms for solving partial differential equations and linear systems.\n\nThe solution proceeds by implementing the specified procedure. This involves five main stages: (1) Assembling the system matrix $A$ from the discretization of the anisotropic Poisson operator; (2) Setting up the Preconditioned Conjugate Gradient (PCG) iteration with a Jacobi preconditioner; (3) Running the PCG algorithm while simultaneously running the Lanczos process on the preconditioned operator to build the tridiagonal matrix $T_k$; (4) Using the eigenvalues of $T_k$ at a specific iteration to estimate convergence properties; and (5) Continuing the PCG iteration to find the actual number of steps required for convergence to a given tolerance, thereby validating the estimates.\n\n1.  **Matrix Assembly**\n    The problem considers the operator $-\\nabla \\cdot (K \\nabla u)$ on the unit square $[0,1] \\times [0,1]$ with homogeneous Dirichlet boundary conditions. The anisotropy tensor is $K = \\mathrm{diag}(1, \\alpha)$. A uniform grid with $n_x$ and $n_y$ interior points in the $x$ and $y$ directions, respectively, is used. The grid spacings are $h_x = 1/(n_x+1)$ and $h_y = 1/(n_y+1)$.\n\n    The standard five-point finite difference discretization leads to a sparse, symmetric positive definite (SPD) matrix $A \\in \\mathbb{R}^{N \\times N}$, where $N = n_x n_y$. This matrix can be constructed using Kronecker products. Let $T_{\\text{1D}}(n)$ be the $n \\times n$ tridiagonal matrix with $2$ on the diagonal and $-1$ on the sub- and super-diagonals. We define the scaled 1D stiffness matrices:\n    $$\n    T_x = \\frac{1}{h_x^2} T_{\\text{1D}}(n_x) \\quad \\text{and} \\quad T_y = \\frac{\\alpha}{h_y^2} T_{\\text{1D}}(n_y)\n    $$\n    Let $I_x$ and $I_y$ be identity matrices of size $n_x \\times n_x$ and $n_y \\times n_y$, respectively. The system matrix $A$ is then:\n    $$\n    A = (I_y \\otimes T_x) + (T_y \\otimes I_x)\n    $$\n    An optional positive shift $\\sigma I$ can be added, resulting in $A \\leftarrow A + \\sigma I$, which preserves the SPD property for $\\sigma \\ge 0$.\n\n2.  **Preconditioned System**\n    The linear system to solve is $A x = b$. For preconditioning, the Jacobi preconditioner $M = \\mathrm{diag}(A)$ is chosen. Since $A$ is SPD with positive diagonal entries, $M$ is also SPD and its inverse $M^{-1}$ is trivial to compute.\n    The Preconditioned Conjugate Gradient method is applied to this system. This is equivalent to applying the standard Conjugate Gradient method to the preconditioned system where the operator is $H = M^{-1}A$. The operator $H$ is self-adjoint with respect to the $M$-inner product, defined as $(u, v)_M = u^{\\top} M v$.\n\n3.  **PCG and Lanczos Iterations**\n    The PCG method is initialized with $x_0 = 0$ and the right-hand side $b=\\mathbf{1}$ (a vector of all ones). The standard PCG recurrence relations are used to update the solution estimate $x_k$, residual $r_k$, preconditioned residual $z_k = M^{-1}r_k$, and search direction $p_k$ at each iteration $k=0, 1, 2, \\ldots$.\n\n    Simultaneously, the Lanczos process is applied to the operator $H = M^{-1}A$ in the $M$-inner product. The process is initialized with the vector $v_1 = z_0 / \\|z_0\\|_M$, where $z_0 = M^{-1}r_0 = M^{-1}b$.\n    At each step $j=1, 2, \\ldots, k_{\\mathrm{cap}}$, the Lanczos process generates the scalars $\\alpha_j$ and $\\beta_j$ that form the symmetric tridiagonal matrix $T_j$. The recurrence is given by:\n    \\begin{align*}\n    w_j &= H v_j - \\beta_{j-1} v_{j-1} \\quad (\\text{with } \\beta_0=0, v_0=0) \\\\\n    \\alpha_j &= (v_j, H v_j)_M \\\\\n    \\tilde{w}_j &= w_j - \\alpha_j v_j \\\\\n    \\beta_j &= \\|\\tilde{w}_j\\|_M \\\\\n    v_{j+1} &= \\tilde{w}_j / \\beta_j\n    \\end{align*}\n    This process is carried out for $j$ up to $k_{\\mathrm{cap}}$. If breakdown occurs (i.e., $\\beta_j \\approx 0$ for some $j < k_{\\mathrm{cap}}$), the process is stopped at that iteration, and the effective capture iteration becomes $j$.\n\n4.  **In-Flight Estimation of Spectral Properties**\n    At the capture iteration, denoted $k'_{\\mathrm{cap}}$ (which is $k_{\\mathrm{cap}}$ or the breakdown iteration, whichever is smaller), the tridiagonal matrix $T_{k'_{\\mathrm{cap}}}$ is formed:\n    $$\n    T_{k'_{\\mathrm{cap}}} = \\begin{pmatrix}\n    \\alpha_1 & \\beta_1 & & \\\\\n    \\beta_1 & \\alpha_2 & \\ddots & \\\\\n    & \\ddots & \\ddots & \\beta_{k'_{\\mathrm{cap}}-1} \\\\\n    & & \\beta_{k'_{\\mathrm{cap}}-1} & \\alpha_{k'_{\\mathrm{cap}}}\n    \\end{pmatrix}\n    $$\n    The eigenvalues of $T_{k'_{\\mathrm{cap}}}$, known as Ritz values, provide estimates for the eigenvalues of $H$. We compute all eigenvalues of $T_{k'_{\\mathrm{cap}}}$ and take the minimum and maximum as our estimates, $\\hat{\\lambda}_{\\min}$ and $\\hat{\\lambda}_{\\max}$.\n    From these estimates, we compute:\n    -   The estimated condition number: $\\hat{\\kappa} = \\hat{\\lambda}_{\\max} / \\hat{\\lambda}_{\\min}$.\n    -   The estimated convergence factor: $q = (\\sqrt{\\hat{\\kappa}} - 1) / (\\sqrt{\\hat{\\kappa}} + 1)$.\n    -   The predicted total PCG iterations for a target relative error $\\tau$: $k_{\\mathrm{pred}} = \\lceil \\log(\\tau/2) / \\log(q) \\rceil$. A special case is handled for $\\hat{\\kappa}=1$, where $q=0$ and $k_{\\mathrm{pred}}=0$ as per the problem statement.\n    -   The predicted remaining iterations: $k_{\\mathrm{rem,pred}} = \\max\\{0, k_{\\mathrm{pred}} - k'_{\\mathrm{cap}}\\}$.\n    -   The predicted Chebyshev smoothing factor for a polynomial of degree $m$: $\\rho_{\\mathrm{pred}} = 2 q^m$.\n\n5.  **Verification and Final Results**\n    The PCG iteration continues beyond $k'_{\\mathrm{cap}}$ until the true relative error in the $A$-norm satisfies $\\|e_k\\|_A / \\|e_0\\|_A \\le \\tau$, where $e_k = x_* - x_k$. To compute this error, the exact solution $x_* = A^{-1}b$ is pre-calculated using a sparse direct solver. The iteration at which this condition is met is denoted $k_{\\mathrm{stop}}$.\n    The actual number of remaining iterations after the capture step is then calculated as $k_{\\mathrm{rem,act}} = \\max\\{0, k_{\\mathrm{stop}} - k'_{\\mathrm{cap}}\\}$.\n    The final output for each test case is the list $[\\hat{\\lambda}_{\\min}, \\hat{\\lambda}_{\\max}, \\hat{\\kappa}, k_{\\mathrm{rem,pred}}, k_{\\mathrm{rem,act}}, \\rho_{\\mathrm{pred}}]$.",
            "answer": "[[1.0003027,1.8659134,1.8653424,37,34,0.0000031],[1.0000000,1.9801000,1.9801000,1,0,0.0000000],[1.0000000,1.0772782,1.0772782,3,3,0.1171816]]"
        }
    ]
}