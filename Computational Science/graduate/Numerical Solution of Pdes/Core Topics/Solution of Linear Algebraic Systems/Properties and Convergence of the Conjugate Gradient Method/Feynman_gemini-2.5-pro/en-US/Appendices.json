{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a foundational, hands-on walkthrough of the Conjugate Gradient algorithm. By performing two iterations for a small $2 \\times 2$ system by hand, you will demystify the core mechanics of calculating residuals, step sizes, and search directions. This practice reinforces the algebraic steps and directly illustrates the method's property of finding the exact solution in at most $n$ steps for an $n \\times n$ system in exact arithmetic .",
            "id": "3436339",
            "problem": "Consider a symmetric positive definite linear system arising from a Galerkin discretization of an elliptic partial differential equation, with matrix $A=\\begin{pmatrix}4&1\\\\1&3\\end{pmatrix}$, right-hand side vector $b=\\begin{pmatrix}1\\\\2\\end{pmatrix}$, and initial guess $x_0=\\begin{pmatrix}0\\\\0\\end{pmatrix}$. The Conjugate Gradient (CG) method is applied using exact arithmetic and the standard residual $r_k=b-Ax_k$, search directions that are $A$-conjugate, and step sizes chosen to minimize the quadratic energy along each search direction. Let $x_\\star$ denote the unique solution of $Ax_\\star=b$, let $e_k=x_\\star-x_k$ denote the error at iteration $k$, and let the energy norm be defined by $\\|e\\|_A=\\sqrt{e^{\\top}Ae}$. Perform exactly two CG iterations starting from $x_0$ to compute $x_2$, $r_2$, and $\\|e_2\\|_A$. Express your final answer as a single row matrix containing, in order, the two components of $x_2$, the two components of $r_2$, and the scalar $\\|e_2\\|_A$. Provide the exact values; no rounding is required.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- The linear system is defined by the matrix $A=\\begin{pmatrix}4&1\\\\1&3\\end{pmatrix}$ and the right-hand side vector $b=\\begin{pmatrix}1\\\\2\\end{pmatrix}$.\n- The initial guess is $x_0=\\begin{pmatrix}0\\\\0\\end{pmatrix}$.\n- The method to be used is the Conjugate Gradient (CG) method with exact arithmetic.\n- The number of iterations to perform is exactly $2$.\n- The quantities to be computed are the solution estimate $x_2$, the residual $r_2$, and the energy norm of the error $\\|e_2\\|_A$.\n- Definitions provided are: residual $r_k=b-Ax_k$, error $e_k=x_\\star-x_k$ (where $x_\\star$ is the exact solution of $Ax_\\star=b$), and energy norm $\\|e\\|_A=\\sqrt{e^{\\top}Ae}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem requires the application of the Conjugate Gradient method, a canonical algorithm in numerical linear algebra for solving linear systems where the matrix is symmetric and positive definite (SPD). The problem is grounded in established mathematical principles.\n- **Well-Posed:** For the CG method to be applicable, the matrix $A$ must be SPD.\n  - Symmetry: The matrix $A$ is symmetric since $A_{12} = A_{21} = 1$.\n  - Positive Definiteness: We check the leading principal minors of $A$. The first minor is $\\det(4) = 4 > 0$. The second minor is $\\det(A) = 4 \\times 3 - 1 \\times 1 = 12 - 1 = 11 > 0$. Since all leading principal minors are positive, $A$ is positive definite by Sylvester's criterion.\n- Because $A$ is SPD, it is invertible, guaranteeing a unique solution $x_\\star$ exists. The CG algorithm is well-defined and is guaranteed to converge to the exact solution in at most $n=2$ steps for a $2 \\times 2$ system in exact arithmetic.\n- **Objective:** The problem is stated using precise mathematical terms, definitions, and numerical values, with no ambiguity or subjective elements.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid** as it is scientifically sound, well-posed, and objective. A complete solution will be provided.\n\n**Solution Derivation**\n\nThe Conjugate Gradient algorithm proceeds as follows.\n\n**Initialization (k=0):**\nThe initial guess is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe initial residual is $r_0 = b - A x_0 = b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe initial search direction is set to the residual: $p_0 = r_0 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n\n**Iteration 1 (k=0 to k=1):**\nFirst, we compute the step size $\\alpha_0$:\n$$ \\alpha_0 = \\frac{r_0^{\\top} r_0}{p_0^{\\top} A p_0} $$\nThe numerator is $r_0^{\\top} r_0 = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = 1^2 + 2^2 = 5$.\nFor the denominator, we first compute $A p_0$:\n$$ A p_0 = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4(1) + 1(2) \\\\ 1(1) + 3(2) \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} $$\nThen, $p_0^{\\top} A p_0 = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = 1(6) + 2(7) = 20$.\nSo, the step size is $\\alpha_0 = \\frac{5}{20} = \\frac{1}{4}$.\n\nNow, we update the solution and the residual:\n$$ x_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1/4 \\\\ 1/2 \\end{pmatrix} $$\n$$ r_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 1 - 6/4 \\\\ 2 - 7/4 \\end{pmatrix} = \\begin{pmatrix} 1 - 3/2 \\\\ 8/4 - 7/4 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ 1/4 \\end{pmatrix} $$\n\nNext, we prepare for the second iteration by computing $\\beta_0$ and the new search direction $p_1$:\n$$ \\beta_0 = \\frac{r_1^{\\top} r_1}{r_0^{\\top} r_0} $$\nThe new numerator is $r_1^{\\top} r_1 = \\begin{pmatrix} -1/2 & 1/4 \\end{pmatrix} \\begin{pmatrix} -1/2 \\\\ 1/4 \\end{pmatrix} = (-\\frac{1}{2})^2 + (\\frac{1}{4})^2 = \\frac{1}{4} + \\frac{1}{16} = \\frac{5}{16}$.\nThe denominator $r_0^{\\top} r_0 = 5$ was computed earlier.\nSo, $\\beta_0 = \\frac{5/16}{5} = \\frac{1}{16}$.\n\nThe new search direction is:\n$$ p_1 = r_1 + \\beta_0 p_0 = \\begin{pmatrix} -1/2 \\\\ 1/4 \\end{pmatrix} + \\frac{1}{16} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -8/16 + 1/16 \\\\ 4/16 + 2/16 \\end{pmatrix} = \\begin{pmatrix} -7/16 \\\\ 6/16 \\end{pmatrix} $$\n\n**Iteration 2 (k=1 to k=2):**\nWe compute the step size $\\alpha_1$:\n$$ \\alpha_1 = \\frac{r_1^{\\top} r_1}{p_1^{\\top} A p_1} $$\nThe numerator $r_1^{\\top} r_1 = 5/16$ was just computed.\nFor the denominator, we first compute $A p_1$:\n$$ A p_1 = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} -7/16 \\\\ 6/16 \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} 4(-7) + 1(6) \\\\ 1(-7) + 3(6) \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} -22 \\\\ 11 \\end{pmatrix} $$\nThen, $p_1^{\\top} A p_1 = \\begin{pmatrix} -7/16 & 6/16 \\end{pmatrix} \\frac{1}{16} \\begin{pmatrix} -22 \\\\ 11 \\end{pmatrix} = \\frac{1}{256} \\left( (-7)(-22) + 6(11) \\right) = \\frac{1}{256} (154 + 66) = \\frac{220}{256} = \\frac{55}{64}$.\nSo, the step size is $\\alpha_1 = \\frac{5/16}{55/64} = \\frac{5}{16} \\cdot \\frac{64}{55} = \\frac{1}{1} \\cdot \\frac{4}{11} = \\frac{4}{11}$.\n\nNow we update the solution to get $x_2$ and the residual to get $r_2$:\n$$ x_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 1/4 \\\\ 1/2 \\end{pmatrix} + \\frac{4}{11} \\begin{pmatrix} -7/16 \\\\ 6/16 \\end{pmatrix} = \\begin{pmatrix} 1/4 \\\\ 1/2 \\end{pmatrix} + \\begin{pmatrix} -7/44 \\\\ 6/44 \\end{pmatrix} = \\begin{pmatrix} 11/44 - 7/44 \\\\ 22/44 + 6/44 \\end{pmatrix} = \\begin{pmatrix} 4/44 \\\\ 28/44 \\end{pmatrix} = \\begin{pmatrix} 1/11 \\\\ 7/11 \\end{pmatrix} $$\n$$ r_2 = r_1 - \\alpha_1 A p_1 = \\begin{pmatrix} -1/2 \\\\ 1/4 \\end{pmatrix} - \\frac{4}{11} \\left( \\frac{1}{16} \\begin{pmatrix} -22 \\\\ 11 \\end{pmatrix} \\right) = \\begin{pmatrix} -1/2 \\\\ 1/4 \\end{pmatrix} - \\frac{1}{44} \\begin{pmatrix} -22 \\\\ 11 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ 1/4 \\end{pmatrix} - \\begin{pmatrix} -1/2 \\\\ 1/4 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n\nThe sought quantities are $x_2$, $r_2$, and $\\|e_2\\|_A$.\nWe have found $x_2 = \\begin{pmatrix} 1/11 \\\\ 7/11 \\end{pmatrix}$ and $r_2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe fact that $r_2 = b - A x_2 = 0$ implies that $A x_2 = b$. Therefore, $x_2$ is the exact solution, so $x_2 = x_\\star$.\nThe error at iteration $2$ is $e_2 = x_\\star - x_2 = x_2 - x_2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe energy norm of the error is computed as:\n$$ \\|e_2\\|_A = \\sqrt{e_2^{\\top} A e_2} = \\sqrt{\\begin{pmatrix} 0 & 0 \\end{pmatrix} \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}} = \\sqrt{0} = 0 $$\n\nThe final answer requires a single row matrix containing the components of $x_2$, the components of $r_2$, and the scalar $\\|e_2\\|_A$ in order.\nThe components of $x_2$ are $1/11$ and $7/11$.\nThe components of $r_2$ are $0$ and $0$.\nThe value of $\\|e_2\\|_A$ is $0$.\nThe resulting row matrix is $\\begin{pmatrix} \\frac{1}{11} & \\frac{7}{11} & 0 & 0 & 0 \\end{pmatrix}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{11} & \\frac{7}{11} & 0 & 0 & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from rote calculation to conceptual design, this problem challenges you to connect theory with practice. You will construct a $3 \\times 3$ system where the CG method terminates in just two steps, demonstrating that convergence speed is governed by the number of distinct eigenvalues, not just the matrix size. This exercise deepens your understanding of how the spectral properties of the matrix $A$ fundamentally dictate the performance of the CG algorithm .",
            "id": "3436325",
            "problem": "Consider the application of the Conjugate Gradient (CG) method to a linear system arising from a symmetric positive definite discretization, as commonly encountered in the numerical solution of elliptic partial differential equations (PDE). Your task is to construct and verify a concrete instance where CG terminates in exactly $2$ steps in exact arithmetic.\n\nProvide an explicit $3 \\times 3$ symmetric positive definite matrix $A$ with exactly two distinct eigenvalues, and a right-hand side vector $b$ whose components have nonzero projections onto the eigenspaces associated with both distinct eigenvalues of $A$. Start the CG iteration from $x_0 = 0$. Using only the fundamental definitions of the CG method (residuals, search directions, and step sizes defined by inner products induced by $A$ and by the Euclidean inner product), execute the first two iterations symbolically and exactly, and verify termination by showing that the error after $2$ steps, $e_2 := x_{\\ast} - x_2$, is the zero vector, where $x_{\\ast}$ denotes the exact solution of $A x = b$.\n\nReport, as your final answer, the exact value of the squared Euclidean norm $\\|e_2\\|_2^2$. No rounding is required, and no units are needed.",
            "solution": "### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Task:** Construct an explicit $3 \\times 3$ symmetric positive definite (SPD) matrix $A$.\n*   **Constraint 1:** $A$ must have exactly two distinct eigenvalues.\n*   **Task:** Construct a right-hand side vector $b$.\n*   **Constraint 2:** The initial error for $x_0=0$, which is $x_*=A^{-1}b$, must have nonzero projections onto the eigenspaces associated with both distinct eigenvalues of $A$. This is equivalent to stating that the initial residual $r_0 = b$ has nonzero projections onto these eigenspaces.\n*   **Initial Condition:** The Conjugate Gradient (CG) iteration starts from $x_0 = 0$.\n*   **Method:** Execute the first two iterations of the CG method symbolically and exactly, using its fundamental definitions.\n*   **Verification:** Show that the error after $2$ steps, $e_2 := x_* - x_2$, is the zero vector, thus verifying termination in exactly 2 steps.\n*   **Required Output:** Report the exact value of the squared Euclidean norm $\\|e_2\\|_2^2$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded. The Conjugate Gradient method and its convergence properties are a cornerstone of numerical linear algebra. A well-known theorem states that if the matrix $A$ in the system $Ax=b$ has $k$ distinct eigenvalues, the CG method will find the exact solution in at most $k$ iterations in exact arithmetic, provided the initial residual has components related to all corresponding eigenspaces. This problem asks for a concrete construction and verification of this theorem for the case of a $3 \\times 3$ matrix with $k=2$ distinct eigenvalues.\n\nThe problem is well-posed and objective. It provides clear, formal constraints for constructing a specific example ($3 \\times 3$ SPD, $2$ distinct eigenvalues, $x_0=0$) and asks for a deterministic verification procedure (running CG for 2 steps and showing the resulting error is zero). The requested output is a single, unambiguous numerical value.\n\nThe problem setup is complete and consistent. All necessary information to construct the example and perform the verification is either provided or is part of the construction task itself. The problem is a standard, non-trivial exercise in numerical analysis that tests the understanding of the CG method's theoretical foundations. It does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. I will proceed with the solution.\n\n### Solution\n\nThe core of the problem is to demonstrate the termination property of the Conjugate Gradient (CG) method. The method is guaranteed to find the exact solution to $Ax=b$ in at most $k$ steps, where $k$ is the number of distinct eigenvalues of the matrix $A$, assuming the initial residual has components associated with all corresponding eigenspaces. We will construct a $3 \\times 3$ SPD matrix $A$ with $k=2$ distinct eigenvalues and a vector $b$ such that the CG algorithm terminates in exactly $2$ steps.\n\n**1. Construction of the Matrix $A$ and Vector $b$**\n\nLet the two distinct eigenvalues be $\\lambda_1 = 1$ and $\\lambda_2 = 2$. For a $3 \\times 3$ matrix, one eigenvalue must have algebraic multiplicity $2$. Let's assign $\\lambda_1=1$ (multiplicity $1$) and $\\lambda_2=2$ (multiplicity $2$).\n\nTo construct $A$, we use the spectral decomposition $A = Q \\Lambda Q^T$. Let's choose a simple orthogonal basis of eigenvectors.\n*   For $\\lambda_1 = 1$, let the normalized eigenvector be $v_1 = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n*   For $\\lambda_2 = 2$, we need two orthogonal eigenvectors, both orthogonal to $v_1$. We can choose $v_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$ and $v_3 = \\frac{1}{\\sqrt{6}} \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix}$.\n\nThe matrix of eigenvectors is $Q = \\begin{pmatrix} v_1 & v_2 & v_3 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{3}} & 0 & -\\frac{2}{\\sqrt{6}} \\end{pmatrix}$, and the diagonal matrix of eigenvalues is $\\Lambda = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}$.\n\nCalculating $A = Q \\Lambda Q^T$:\n$$ A = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{3}} & 0 & -\\frac{2}{\\sqrt{6}} \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0 \\\\ \\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{6}} & -\\frac{2}{\\sqrt{6}} \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 5 & -1 & -1 \\\\ -1 & 5 & -1 \\\\ -1 & -1 & 5 \\end{pmatrix} $$\nThis matrix is symmetric by construction. Its eigenvalues are $1$ and $2$, which are both positive, so $A$ is positive definite.\n\nNext, we construct the vector $b$. The initial residual is $r_0 = b - A x_0$. Since $x_0 = 0$, we have $r_0 = b$. For termination in exactly $2$ steps, $r_0$ must have non-zero components in the eigenspace of $\\lambda_1$ and the eigenspace of $\\lambda_2$. A simple way to achieve this is to define $b$ as a linear combination of eigenvectors from each space. Let's choose $b = v_1' + v_2'$, where $v_1'$ is in the eigenspace of $\\lambda_1$ and $v_2'$ is in the eigenspace of $\\lambda_2$. For simplicity, let's use the unnormalized eigenvectors:\n$v_1' = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$ and $v_2' = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$.\n$$ b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix} $$\n\n**2. Conjugate Gradient Iterations**\n\nWe apply the CG algorithm to solve $Ax=b$ with $x_0=0$.\nThe system is:\n$$ \\frac{1}{3}\\begin{pmatrix} 5 & -1 & -1 \\\\ -1 & 5 & -1 \\\\ -1 & -1 & 5 \\end{pmatrix} x = \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\quad x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n\nInitial setup:\n$r_0 = b - Ax_0 = b = \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix}$\n$p_0 = r_0 = \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix}$\n\n**Iteration $1$ (for $k=0$):**\nStep-size $\\alpha_0 = \\frac{r_0^T r_0}{p_0^T A p_0}$:\n$r_0^T r_0 = 2^2 + 0^2 + 1^2 = 5$\n$A p_0 = \\frac{1}{3}\\begin{pmatrix} 5 & -1 & -1 \\\\ -1 & 5 & -1 \\\\ -1 & -1 & 5 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 10 - 1 \\\\ -2 - 1 \\\\ -2 + 5 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 9 \\\\ -3 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\\\ 1 \\end{pmatrix}$\n$p_0^T A p_0 = r_0^T (A p_0) = \\begin{pmatrix} 2 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\\\ 1 \\end{pmatrix} = 6 + 0 + 1 = 7$\n$\\alpha_0 = \\frac{5}{7}$\n\nUpdate solution and residual:\n$x_1 = x_0 + \\alpha_0 p_0 = 0 + \\frac{5}{7} \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 10/7 \\\\ 0 \\\\ 5/7 \\end{pmatrix}$\n$r_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\frac{5}{7} \\begin{pmatrix} 3 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 14/7 - 15/7 \\\\ 5/7 \\\\ 7/7 - 5/7 \\end{pmatrix} = \\frac{1}{7}\\begin{pmatrix} -1 \\\\ 5 \\\\ 2 \\end{pmatrix}$\nSince $r_1 \\neq 0$, the algorithm has not terminated.\n\nPrepare for next iteration:\n$\\beta_0 = \\frac{r_1^T r_1}{r_0^T r_0}$:\n$r_1^T r_1 = \\left(\\frac{1}{7}\\right)^2 ((-1)^2 + 5^2 + 2^2) = \\frac{1}{49}(1+25+4) = \\frac{30}{49}$\n$\\beta_0 = \\frac{30/49}{5} = \\frac{6}{49}$\n$p_1 = r_1 + \\beta_0 p_0 = \\frac{1}{7}\\begin{pmatrix} -1 \\\\ 5 \\\\ 2 \\end{pmatrix} + \\frac{6}{49}\\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{49} \\left( 7\\begin{pmatrix} -1 \\\\ 5 \\\\ 2 \\end{pmatrix} + 6\\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{49} \\begin{pmatrix} -7+12 \\\\ 35+0 \\\\ 14+6 \\end{pmatrix} = \\frac{1}{49}\\begin{pmatrix} 5 \\\\ 35 \\\\ 20 \\end{pmatrix} = \\frac{5}{49}\\begin{pmatrix} 1 \\\\ 7 \\\\ 4 \\end{pmatrix}$\n\n**Iteration $2$ (for $k=1$):**\nStep-size $\\alpha_1 = \\frac{r_1^T r_1}{p_1^T A p_1}$:\n$r_1^T r_1 = \\frac{30}{49}$\n$A p_1 = \\frac{5}{49} A \\begin{pmatrix} 1 \\\\ 7 \\\\ 4 \\end{pmatrix} = \\frac{5}{49} \\frac{1}{3}\\begin{pmatrix} 5-7-4 \\\\ -1+35-4 \\\\ -1-7+20 \\end{pmatrix} = \\frac{5}{147}\\begin{pmatrix} -6 \\\\ 30 \\\\ 12 \\end{pmatrix} = \\frac{30}{147}\\begin{pmatrix} -1 \\\\ 5 \\\\ 2 \\end{pmatrix} = \\frac{10}{49}\\begin{pmatrix} -1 \\\\ 5 \\\\ 2 \\end{pmatrix}$\n$p_1^T A p_1 = \\left(\\frac{5}{49}\\begin{pmatrix} 1 \\\\ 7 \\\\ 4 \\end{pmatrix}^T\\right) \\left(\\frac{10}{49}\\begin{pmatrix} -1 \\\\ 5 \\\\ 2 \\end{pmatrix}\\right) = \\frac{50}{49^2} (1(-1) + 7(5) + 4(2)) = \\frac{50}{49^2}(-1+35+8) = \\frac{50 \\cdot 42}{49^2} = \\frac{50 \\cdot 6 \\cdot 7}{49 \\cdot 49} = \\frac{300}{343}$\n$\\alpha_1 = \\frac{30/49}{300/343} = \\frac{30}{49} \\cdot \\frac{343}{300} = \\frac{1}{10} \\cdot \\frac{7^3}{7^2} = \\frac{7}{10}$\n\nUpdate solution and residual:\n$x_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 10/7 \\\\ 0 \\\\ 5/7 \\end{pmatrix} + \\frac{7}{10} \\left( \\frac{5}{49}\\begin{pmatrix} 1 \\\\ 7 \\\\ 4 \\end{pmatrix} \\right) = \\begin{pmatrix} 10/7 \\\\ 0 \\\\ 5/7 \\end{pmatrix} + \\frac{1}{14}\\begin{pmatrix} 1 \\\\ 7 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 20/14 + 1/14 \\\\ 7/14 \\\\ 10/14 + 4/14 \\end{pmatrix} = \\begin{pmatrix} 21/14 \\\\ 7/14 \\\\ 14/14 \\end{pmatrix} = \\begin{pmatrix} 3/2 \\\\ 1/2 \\\\ 1 \\end{pmatrix}$\n$r_2 = r_1 - \\alpha_1 A p_1 = \\frac{1}{7}\\begin{pmatrix} -1 \\\\ 5 \\\\ 2 \\end{pmatrix} - \\frac{7}{10} \\left( \\frac{10}{49}\\begin{pmatrix} -1 \\\\ 5 \\\\ 2 \\end{pmatrix} \\right) = \\frac{1}{7}\\begin{pmatrix} -1 \\\\ 5 \\\\ 2 \\end{pmatrix} - \\frac{1}{7}\\begin{pmatrix} -1 \\\\ 5 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$\nSince $r_2=0$, the CG algorithm terminates exactly at step $2$.\n\n**3. Verification of the Solution**\n\nTo verify that $x_2$ is the exact solution $x_*$, we solve $Ax=b$ directly.\nOur matrix is $A = 2I - \\frac{1}{3}J$, where $J$ is the all-ones matrix. Using the Sherman-Morrison-Woodbury formula or a specific identity for such matrices, its inverse is $A^{-1} = \\frac{1}{2}I + \\frac{1}{6}J$.\n$$ A^{-1} = \\frac{1}{2}\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} + \\frac{1}{6}\\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix} = \\frac{1}{6}\\begin{pmatrix} 3+1 & 1 & 1 \\\\ 1 & 3+1 & 1 \\\\ 1 & 1 & 3+1 \\end{pmatrix} = \\frac{1}{6}\\begin{pmatrix} 4 & 1 & 1 \\\\ 1 & 4 & 1 \\\\ 1 & 1 & 4 \\end{pmatrix} $$\nNow, we compute the exact solution $x_* = A^{-1}b$:\n$$ x_* = \\frac{1}{6}\\begin{pmatrix} 4 & 1 & 1 \\\\ 1 & 4 & 1 \\\\ 1 & 1 & 4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{6}\\begin{pmatrix} 4(2)+1(0)+1(1) \\\\ 1(2)+4(0)+1(1) \\\\ 1(2)+1(0)+4(1) \\end{pmatrix} = \\frac{1}{6}\\begin{pmatrix} 9 \\\\ 3 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 9/6 \\\\ 3/6 \\\\ 6/6 \\end{pmatrix} = \\begin{pmatrix} 3/2 \\\\ 1/2 \\\\ 1 \\end{pmatrix} $$\nComparing our CG result $x_2$ with the exact solution $x_*$:\n$$ x_2 = \\begin{pmatrix} 3/2 \\\\ 1/2 \\\\ 1 \\end{pmatrix} = x_* $$\nThe error after $2$ steps is $e_2 = x_* - x_2 = 0$.\nThe squared Euclidean norm of the error is $\\|e_2\\|_2^2 = \\|0\\|_2^2 = 0^2+0^2+0^2 = 0$. This completes the verification.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "This final practice transitions from small, abstract examples to a more realistic scenario involving a PDE discretization, highlighting a crucial nuance of practical CG implementation. You will investigate the behavior of two different convergence metrics: the energy norm of the error, which is theoretically guaranteed to decrease, and the computable residual norm, which is not. This coding exercise is essential for understanding why monitoring the residual norm can be misleading and for developing robust stopping criteria in real-world applications .",
            "id": "3436372",
            "problem": "Consider the boundary value problem for a one-dimensional reaction-diffusion equation on the open interval $(0,1)$ with homogeneous Dirichlet boundary conditions: find $u : [0,1] \\to \\mathbb{R}$ such that\n$$\n- \\frac{d^2 u}{dx^2} + \\alpha u = f(x), \\quad x \\in (0,1), \\quad u(0) = u(1) = 0,\n$$\nwhere $\\alpha \\ge 0$ is a constant reaction coefficient. Discretize the problem on $n$ interior grid points using the standard second-order centered finite difference method. Let $h = \\frac{1}{n+1}$ and $x_i = i h$ for $i = 1,2,\\ldots,n$. The discrete system has the form\n$$\nA \\mathbf{u} = \\mathbf{b},\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, with $A = \\frac{1}{h^2} \\operatorname{tridiag}(-1,2,-1) + \\alpha I$, $\\mathbf{u} \\in \\mathbb{R}^n$ approximates the solution at the interior points, and $\\mathbf{b} \\in \\mathbb{R}^n$ collects the values $f(x_i)$. For this class of problems, the Conjugate Gradient method (CG) is a fundamental iterative algorithm for solving $A \\mathbf{u} = \\mathbf{b}$ when $A$ is symmetric positive definite.\n\nDefine the error at iteration $k$ as $\\mathbf{e}_k = \\mathbf{x}_k - \\mathbf{x}^\\star$, where $\\mathbf{x}^\\star$ is the exact solution of the linear system, and the residual as $\\mathbf{r}_k = \\mathbf{b} - A \\mathbf{x}_k$. The energy norm induced by $A$ is $\\|\\mathbf{e}_k\\|_A = \\sqrt{\\mathbf{e}_k^\\top A \\mathbf{e}_k}$, and the Euclidean norm of the residual is $\\|\\mathbf{r}_k\\|_2 = \\sqrt{\\mathbf{r}_k^\\top \\mathbf{r}_k}$.\n\nStarting from the foundational facts that:\n- The finite difference discretization yields a symmetric positive definite matrix $A$ for all $\\alpha \\ge 0$.\n- The Conjugate Gradient method generates iterates $\\mathbf{x}_k$ in the affine Krylov sequence $\\mathbf{x}_0 + \\mathcal{K}_k(A,\\mathbf{r}_0)$ where $\\mathcal{K}_k(A,\\mathbf{r}_0) = \\operatorname{span}\\{\\mathbf{r}_0, A \\mathbf{r}_0, \\ldots, A^{k-1} \\mathbf{r}_0\\}$, with $\\mathbf{r}_0 = \\mathbf{b} - A \\mathbf{x}_0$.\n- The Conjugate Gradient method ensures that the energy norm of the error $\\|\\mathbf{e}_k\\|_A$ decreases monotonically in exact arithmetic (strictly, unless the solution is reached),\nformulate and implement a program that constructs specific instances of the above discrete partial differential equation to demonstrate that the residual norm $\\|\\mathbf{r}_k\\|_2$ can be non-monotone even when the energy norm $\\|\\mathbf{e}_k\\|_A$ decreases strictly. Use the following design requirements:\n\n1. Construct $\\mathbf{b}$ in each test case as a two-mode combination of discrete sine eigenvectors of $A$ to emphasize spectral separation effects. For the grid with $n$ interior points, define the discrete sine eigenvector $v_j \\in \\mathbb{R}^n$ componentwise by $(v_j)_i = \\sin\\left(\\frac{j \\pi i}{n+1}\\right)$ for $j=1,2,\\ldots,n$. In each test case, let\n$$\n\\mathbf{b} = c_1 v_1 + c_2 v_n,\n$$\nwith specified coefficients $c_1, c_2 \\in \\mathbb{R}$. Note that the addition of $\\alpha I$ does not change the sine eigenvectors of the discrete Laplacian, so these $v_j$ remain eigenvectors of $A$.\n\n2. Implement the standard Conjugate Gradient method for symmetric positive definite matrices starting from the initial guess $\\mathbf{x}_0 = \\mathbf{0}$, recording at each iteration $k$:\n   - The residual norm $\\|\\mathbf{r}_k\\|_2$.\n   - The energy norm of the error $\\|\\mathbf{e}_k\\|_A$, computed using the exact solution $\\mathbf{x}^\\star$ obtained by solving $A \\mathbf{x}^\\star = \\mathbf{b}$ with a direct method.\n\n3. Use the following stopping indices:\n   - The residual-based index $k_{\\mathrm{res}}$: the smallest iteration $k$ such that $\\|\\mathbf{r}_k\\|_2 \\le \\tau_{\\mathrm{res}} \\|\\mathbf{r}_0\\|_2$, where $\\tau_{\\mathrm{res}} = 10^{-8}$.\n   - The energy-based index $k_{\\mathrm{energy}}$: the smallest iteration $k$ such that $\\|\\mathbf{e}_k\\|_A \\le \\tau_{\\mathrm{energy}} \\|\\mathbf{e}_0\\|_A$, where $\\tau_{\\mathrm{energy}} = 10^{-8}$.\n\n4. Detect and report whether the residual norm sequence is non-monotone, that is, whether there exists an iteration $k \\ge 1$ such that $\\|\\mathbf{r}_k\\|_2 > \\|\\mathbf{r}_{k-1}\\|_2$. Also report whether the energy norm sequence is strictly decreasing across recorded iterations, i.e., whether $\\|\\mathbf{e}_k\\|_A < \\|\\mathbf{e}_{k-1}\\|_A$ for all $k \\ge 1$ until termination.\n\n5. Evaluate the implications of a naive stopping rule that incorrectly assumes the residual norm is monotone. Define a hypothetical early stop at the first iteration $k$ where $\\|\\mathbf{r}_k\\|_2 > \\|\\mathbf{r}_{k-1}\\|_2$. Report whether such a naive stop would occur strictly before reaching the energy-based tolerance, i.e., whether that iteration index is less than $k_{\\mathrm{energy}}$.\n\nImplement your program to run the following test suite, each test case parameterized by the triple $(n,\\alpha,(c_1,c_2))$:\n\n- Test Case 1 (general two-mode, strong spectral gap): $(n,\\alpha,(c_1,c_2)) = (50, 0, (1.0, 0.3))$.\n- Test Case 2 (two-mode under reaction shift): $(n,\\alpha,(c_1,c_2)) = (80, 5.0, (1.0, 0.2))$.\n- Test Case 3 (single-mode boundary case): $(n,\\alpha,(c_1,c_2)) = (60, 0, (1.0, 0.0))$.\n\nFor each test case, your program must output a list of five fundamental values:\n- A boolean indicating whether the residual norm is non-monotone.\n- A boolean indicating whether the energy norm decreases strictly at each iteration until tolerance is reached.\n- A boolean indicating whether the naive residual-increase stopping rule would stop before the energy-based tolerance is met.\n- The integer $k_{\\mathrm{res}}$.\n- The integer $k_{\\mathrm{energy}}$.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list of five values corresponding to a test case. For example: `[[true,true,false,2,2],[...],[...]]`. All booleans must be printed in lowercase as `true` or `false`, and integers in base-10 notation.\n\nNo physical units are involved in this problem, and all angles, if any appear, should be in radians; however, angles do not explicitly appear in this task.",
            "solution": "The problem is valid as it is scientifically sound, well-posed, and provides a complete, unambiguous specification for a computational experiment in numerical linear algebra. The experiment aims to demonstrate a well-known, non-trivial property of the Conjugate Gradient (CG) method. We will proceed with a solution.\n\n### 1. Problem Formulation and Discretization\n\nThe problem concerns the numerical solution of the one-dimensional reaction-diffusion boundary value problem:\n$$\n- \\frac{d^2 u}{dx^2} + \\alpha u = f(x), \\quad x \\in (0,1)\n$$\nwith homogeneous Dirichlet boundary conditions $u(0) = u(1) = 0$. The constant $\\alpha \\ge 0$ represents a reaction coefficient.\n\nWe discretize this equation on a uniform grid with $n$ interior points $x_i = i h$ for $i=1, \\dots, n$, where the grid spacing is $h = \\frac{1}{n+1}$. Using a second-order centered finite difference approximation for the second derivative, we obtain a linear system of equations $A \\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u} \\in \\mathbb{R}^n$ is the vector of approximate solution values $u(x_i)$. The matrix $A \\in \\mathbb{R}^{n \\times n}$ is given by:\n$$\nA = \\frac{1}{h^2} \\operatorname{tridiag}(-1, 2, -1) + \\alpha I\n$$\nwhere $I$ is the $n \\times n$ identity matrix. The matrix $A$ is symmetric and positive definite (SPD) for all $\\alpha \\ge 0$, which is a necessary condition for the application of the Conjugate Gradient method.\n\nThe right-hand side vector $\\mathbf{b} \\in \\mathbb{R}^n$ is constructed using a linear combination of two specific eigenvectors of $A$. The (unnormalized) eigenvectors of the discrete Laplacian, and thus of $A$, are the discrete sine vectors $v_j$, with components $(v_j)_i = \\sin\\left(\\frac{j \\pi i}{n+1}\\right)$. The problem specifies constructing $\\mathbf{b}$ as:\n$$\n\\mathbf{b} = c_1 v_1 + c_2 v_n\n$$\nThis construction excites the lowest-frequency mode ($v_1$) and the highest-frequency mode ($v_n$) of the system, which correspond to the smallest and largest eigenvalues of $A$, respectively. This is a standard technique to study the convergence behavior of iterative methods on problems with large condition numbers.\n\n### 2. The Conjugate Gradient Method and Convergence Metrics\n\nThe Conjugate Gradient (CG) method is an iterative algorithm for solving large, sparse linear systems $A \\mathbf{x} = \\mathbf{b}$ where $A$ is SPD. Starting with an initial guess $\\mathbf{x}_0$, CG generates a sequence of iterates $\\mathbf{x}_k$ that converge to the exact solution $\\mathbf{x}^\\star = A^{-1}\\mathbf{b}$.\n\nA key property of CG is that at each iteration $k$, the iterate $\\mathbf{x}_k$ minimizes the $A$-norm of the error, $\\|\\mathbf{e}_k\\|_A$, over the affine Krylov subspace $\\mathbf{x}_0 + \\mathcal{K}_k(A, \\mathbf{r}_0)$, where $\\mathbf{e}_k = \\mathbf{x}_k - \\mathbf{x}^\\star$ is the error and $\\mathbf{r}_0 = \\mathbf{b} - A\\mathbf{x}_0$ is the initial residual. The $A$-norm, or energy norm, is defined as:\n$$\n\\|\\mathbf{e}\\|_A = \\sqrt{\\mathbf{e}^\\top A \\mathbf{e}}\n$$\nSince the Krylov subspaces are nested ($\\mathcal{K}_k \\subseteq \\mathcal{K}_{k+1}$), this minimization property guarantees that the energy norm of the error is monotonically non-increasing:\n$$\n\\|\\mathbf{e}_{k+1}\\|_A \\le \\|\\mathbf{e}_k\\|_A\n$$\nIn exact arithmetic, this inequality is strict ($\\|\\mathbf{e}_{k+1}\\|_A < \\|\\mathbf{e}_k\\|_A$) unless the solution has already been found ($\\mathbf{e}_k = \\mathbf{0}$).\n\nThe second metric of interest is the Euclidean norm of the residual, $\\|\\mathbf{r}_k\\|_2$, where $\\mathbf{r}_k = \\mathbf{b} - A \\mathbf{x}_k$. While often used as a practical stopping criterion, the sequence $\\|\\mathbf{r}_k\\|_2$ is **not** guaranteed to be monotonic. The CG algorithm updates the residual via a short recurrence, which, while computationally efficient, can lead to a temporary increase in the residual norm. This phenomenon is most pronounced in problems with a large spectral condition number, where the algorithm attempts to eliminate error components corresponding to different parts of the spectrum. The construction of $\\mathbf{b}$ from eigenvectors associated with extremal eigenvalues is designed to elicit precisely this behavior.\n\n### 3. Implementation and Analysis Strategy\n\nThe core of the task is to implement the CG algorithm and track the two norms for specific problem instances.\n\n**Step 1: System Construction**\nFor each test case defined by $(n, \\alpha, (c_1, c_2))$, we first construct the matrix $A$ and the vector $\\mathbf{b}$ according to the formulas provided.\n\n**Step 2: Exact Solution**\nTo compute the error $\\mathbf{e}_k$ and its energy norm $\\|\\mathbf{e}_k\\|_A$, the exact solution $\\mathbf{x}^\\star$ is required. We compute this once at the beginning of each test case by solving $A \\mathbf{x}^\\star = \\mathbf{b}$ using a direct solver (e.g., `numpy.linalg.solve`).\n\n**Step 3: CG Iteration and Data Collection**\nWe implement the standard CG algorithm, starting with $\\mathbf{x}_0 = \\mathbf{0}$. We iterate for a sufficient number of steps (e.g., $n+1$) to observe the convergence behavior, recording the values of $\\|\\mathbf{r}_k\\|_2$ and $\\|\\mathbf{e}_k\\|_A$ at each iteration $k=0, 1, 2, \\dots$.\n\n**Step 4: Post-Processing and Reporting**\nAfter collecting the norm histories, we analyze them to determine the five required outputs for each test case:\n1.  **Residual Non-Monotonicity**: We check if there exists any $k \\ge 1$ such that $\\|\\mathbf{r}_k\\|_2 > \\|\\mathbf{r}_{k-1}\\|_2$.\n2.  **Energy Norm Strict Decrease**: We verify if $\\|\\mathbf{e}_k\\|_A < \\|\\mathbf{e}_{k-1}\\|_A$ for all $k \\ge 1$ where $\\|\\mathbf{e}_{k-1}\\|_A$ is non-negligible. In exact arithmetic, this is always true before convergence.\n3.  **Naive Stop Condition**: If the residual norm is non-monotonic, we identify the first iteration $k_{naive}$ where an increase occurs. We then determine the iteration $k_{\\mathrm{energy}}$ where the energy norm tolerance $\\|\\mathbf{e}_k\\|_A \\le 10^{-8} \\|\\mathbf{e}_0\\|_A$ is met. The output is `True` if $k_{naive} < k_{\\mathrm{energy}}$, indicating that a naive stopping rule based on residual increase would terminate prematurely.\n4.  **$k_{\\mathrm{res}}$**: We find the smallest iteration $k$ satisfying the residual-based stopping criterion $\\|\\mathbf{r}_k\\|_2 \\le 10^{-8} \\|\\mathbf{r}_0\\|_2$.\n5.  **$k_{\\mathrm{energy}}$**: We find the smallest iteration $k$ satisfying the energy-based stopping criterion.\n\nThis systematic approach will allow us to rigorously demonstrate the specified convergence properties of the Conjugate Gradient method. For Test Case 3, where $\\mathbf{b}$ is a single eigenvector ($c_2=0.0$), we expect CG to converge in a single iteration, leading to monotonic norm decreases and no premature stop.",
            "answer": "```python\nimport numpy as np\n\ndef run_cg_test(n, alpha_const, c_coeffs):\n    \"\"\"\n    Runs a single test case for the Conjugate Gradient method analysis.\n\n    For a given problem configuration (n, alpha, c_coeffs), this function:\n    1. Constructs the problem matrix A and right-hand side b.\n    2. Solves for the exact solution x_star to enable error calculations.\n    3. Implements the Conjugate Gradient algorithm, storing the history of\n       the residual norm and the energy norm of the error.\n    4. Analyzes the norm histories to determine the five required metrics:\n       - Whether the residual norm was non-monotone.\n       - Whether the energy norm was strictly decreasing.\n       - Whether a naive stop based on residual increase would be premature.\n       - The iteration count to meet the residual tolerance (k_res).\n       - The iteration count to meet the energy norm tolerance (k_energy).\n    \n    Returns a tuple of these five results.\n    \"\"\"\n    c1, c2 = c_coeffs\n    h = 1.0 / (n + 1)\n    \n    # Construct the matrix A for the 1D Poisson problem with reaction\n    main_diag = 2.0 * np.ones(n)\n    off_diag = -1.0 * np.ones(n - 1)\n    A = (1.0 / h**2) * (np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1))\n    if alpha_const > 0:\n        A += alpha_const * np.eye(n)\n\n    # Construct the right-hand side vector b from eigenvectors\n    i_vec = np.arange(1, n + 1)\n    v1 = np.sin(np.pi * i_vec / (n + 1))\n    vn = np.sin(n * np.pi * i_vec / (n + 1))\n    b = c1 * v1 + c2 * vn\n\n    # Compute exact solution using a direct solver for error calculation\n    x_star = np.linalg.solve(A, b)\n\n    # CG algorithm initialization\n    x = np.zeros(n)\n    r = b.copy() # Since x_0=0, r_0 = b - A*0 = b\n    p = r.copy()\n    rs_old_sq = r.T @ r\n\n    r0_norm = np.sqrt(rs_old_sq)\n    e0 = x - x_star\n    e0_A_norm = np.sqrt(e0.T @ A @ e0)\n\n    res_norms = []\n    energy_norms = []\n\n    # Run CG for a fixed number of iterations to collect a full history\n    max_iter = n + 1 \n    for k in range(max_iter):\n        # Calculate and store norms for the current iteration k\n        e = x - x_star\n        res_norms.append(np.sqrt(r.T @ r))\n        energy_norms.append(np.sqrt(e.T @ A @ e))\n\n        # Check for convergence to avoid division by zero and unnecessary computation\n        if np.sqrt(rs_old_sq)  1e-16:\n             # Fill remaining norm history if we stopped early\n             for _ in range(k + 1, max_iter):\n                 res_norms.append(res_norms[-1])\n                 energy_norms.append(energy_norms[-1])\n             break\n        \n        # Standard CG step\n        Ap = A @ p\n        alpha_cg = rs_old_sq / (p.T @ Ap)\n        x += alpha_cg * p\n        r -= alpha_cg * Ap\n        rs_new_sq = r.T @ r\n        beta = rs_new_sq / rs_old_sq\n        p = r + beta * p\n        rs_old_sq = rs_new_sq\n\n    # --- Analyze the collected data ---\n    \n    # 1. Check if the residual norm sequence is non-monotone\n    is_res_non_monotone = False\n    naive_stop_iter = -1\n    for k in range(1, len(res_norms)):\n        if res_norms[k] > res_norms[k-1] and res_norms[k-1] > 1e-15:\n            is_res_non_monotone = True\n            if naive_stop_iter == -1: # Record first occurrence\n                naive_stop_iter = k\n    \n    # 2. Check if the energy norm is strictly decreasing\n    is_energy_strictly_decreasing = True\n    for k in range(1, len(energy_norms)):\n        if energy_norms[k-1] > 1e-15: # If previous norm was non-zero\n            if energy_norms[k] >= energy_norms[k-1]:\n                is_energy_strictly_decreasing = False\n                break\n    \n    # 5. Find k_energy: iteration to meet energy norm tolerance\n    tau_energy = 1e-8\n    try:\n        # Use a small tolerance to prevent issues with e0_A_norm being zero\n        if e0_A_norm > 1e-15:\n            k_energy = next(k for k, norm in enumerate(energy_norms) if norm = tau_energy * e0_A_norm)\n        else:\n            k_energy = 0\n    except StopIteration:\n        k_energy = -1 # Sentinel for not converged\n\n    # 3. Check if a naive stop would be premature\n    is_naive_stop_early = False\n    if naive_stop_iter != -1 and (k_energy == -1 or naive_stop_iter  k_energy):\n        is_naive_stop_early = True\n\n    # 4. Find k_res: iteration to meet residual norm tolerance\n    tau_res = 1e-8\n    try:\n        if r0_norm > 1e-15:\n            k_res = next(k for k, norm in enumerate(res_norms) if norm = tau_res * r0_norm)\n        else:\n            k_res = 0\n    except StopIteration:\n        k_res = -1 # Sentinel for not converged\n\n    return is_res_non_monotone, is_energy_strictly_decreasing, is_naive_stop_early, k_res, k_energy\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (50, 0.0, (1.0, 0.3)),\n        (80, 5.0, (1.0, 0.2)),\n        (60, 0.0, (1.0, 0.0)),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        # Unpack n, alpha, and the tuple (c1, c2)\n        n, alpha_const, c_coeffs = params\n        result_tuple = run_cg_test(n, alpha_const, c_coeffs)\n        all_results.append(list(result_tuple))\n    \n    # Format the final output string as specified in the problem statement.\n    # e.g., [[true,true,false,2,2],[...],[...]]\n    item_strs = []\n    for res in all_results:\n        # Convert booleans to lowercase strings \"true\" or \"false\"\n        str_res = [str(r).lower() if isinstance(r, bool) else str(r) for r in res]\n        item_strs.append(f\"[{','.join(str_res)}]\")\n    \n    final_output = f\"[{','.join(item_strs)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}