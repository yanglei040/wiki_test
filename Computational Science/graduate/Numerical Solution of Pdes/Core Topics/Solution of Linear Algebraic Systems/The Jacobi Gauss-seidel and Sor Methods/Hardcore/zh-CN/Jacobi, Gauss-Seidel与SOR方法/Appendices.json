{
    "hands_on_practices": [
        {
            "introduction": "在多重网格等算法中，理解迭代法作为平滑器的性能至关重要。本练习  提供了一种动手分析的经验，通过局部傅里叶分析（LFA）来寻找加权雅可比法的最优松弛因子，这是分析更复杂平滑器性能的一项基本技能。",
            "id": "3455516",
            "problem": "考虑在网格间距为 $h$ 的均匀 $N \\times N$ 周期性网格上，使用标准的 $5$ 点模板对二维泊松方程 $-\\Delta u = f$ 进行有限差分离散。得到的线性算子 $A$ 是循环的，并可被离散傅里叶模对角化。其符号由 $a(\\theta_1,\\theta_2) = 4 - 2\\cos\\theta_1 - 2\\cos\\theta_2$ 给出，且 $A$ 的对角部分为 $D = 4$。权重为 $\\omega \\in \\mathbb{R}$ 的加权雅可比法具有误差传播算子 $E = I - \\omega D^{-1}A$，其在频率为 $(\\theta_1,\\theta_2)$ 的傅里叶模上的符号为 $S(\\theta_1,\\theta_2) = 1 - \\omega\\,a(\\theta_1,\\theta_2)/4$。\n\n使用局部傅里叶分析 (LFA)，将相对于标准 $2h$-粗化的高频集 $\\mathcal{H}$ 定义为两个分量均为高频的模：\n$$\n\\mathcal{H} = \\left\\{ (\\theta_1,\\theta_2) : |\\theta_1| \\in \\left[\\frac{\\pi}{2},\\pi\\right],\\;|\\theta_2| \\in \\left[\\frac{\\pi}{2},\\pi\\right] \\right\\}.\n$$\n加权雅可比光滑因子是在 $\\mathcal{H}$ 上的最坏情况误差放大率：\n$$\n\\mu(\\omega) = \\max_{(\\theta_1,\\theta_2) \\in \\mathcal{H}} \\left| 1 - \\omega\\,\\frac{a(\\theta_1,\\theta_2)}{4} \\right|.\n$$\n\n从循环算子的符号演算的基本原理和加权雅可比误差传播符号的定义出发，通过刻画 $a(\\theta_1,\\theta_2)/4$ 在 $\\mathcal{H}$ 上的范围，将 $\\mu(\\omega)$ 显式地计算为 $\\omega$ 的函数。然后，确定在 $\\omega > 0$ 上使 $\\mu(\\omega)$ 最小化的 $\\omega$ 值。将你的最终答案表述为最小化权重 $\\omega$。",
            "solution": "目标是确定当加权雅可比法用作二维泊松方程的光滑子时，最优的松弛参数 $\\omega > 0$。最优参数记为 $\\omega_{\\text{opt}}$，它是使光滑因子 $\\mu(\\omega)$ 最小化的参数。\n\n问题给出了局部傅里叶分析（LFA）的必要定义。加权雅可比法的误差传播算子是 $E = I - \\omega D^{-1}A$。其符号，作为频率为 $(\\theta_1, \\theta_2)$ 的傅里叶模的放大因子，由 $S(\\theta_1, \\theta_2) = 1 - \\omega\\frac{a(\\theta_1,\\theta_2)}{4}$ 给出。\n离散算子 $A$ 的符号是 $a(\\theta_1, \\theta_2) = 4 - 2\\cos\\theta_1 - 2\\cos\\theta_2$。\n我们定义算子 $A$ 的缩放符号为 $\\lambda(\\theta_1, \\theta_2) = \\frac{a(\\theta_1, \\theta_2)}{4}$。代入 $a(\\theta_1, \\theta_2)$ 的表达式，我们得到：\n$$\n\\lambda(\\theta_1, \\theta_2) = \\frac{4 - 2\\cos\\theta_1 - 2\\cos\\theta_2}{4} = 1 - \\frac{1}{2}(\\cos\\theta_1 + \\cos\\theta_2)\n$$\n于是误差放大因子可以写成 $S(\\theta_1, \\theta_2) = 1 - \\omega\\lambda(\\theta_1, \\theta_2)$。\n\n光滑因子 $\\mu(\\omega)$ 定义为在高频集 $\\mathcal{H}$ 上的最大放大因子：\n$$\n\\mu(\\omega) = \\max_{(\\theta_1, \\theta_2) \\in \\mathcal{H}} |S(\\theta_1, \\theta_2)| = \\max_{(\\theta_1, \\theta_2) \\in \\mathcal{H}} |1 - \\omega\\lambda(\\theta_1, \\theta_2)|\n$$\n高频集由下式给出：\n$$\n\\mathcal{H} = \\left\\{ (\\theta_1,\\theta_2) : |\\theta_1| \\in \\left[\\frac{\\pi}{2},\\pi\\right],\\;|\\theta_2| \\in \\left[\\frac{\\pi}{2},\\pi\\right] \\right\\}\n$$\n为了计算 $\\mu(\\omega)$，我们必须首先找到对于所有 $(\\theta_1, \\theta_2) \\in \\mathcal{H}$，$\\lambda(\\theta_1, \\theta_2)$ 所取的值的范围。设该范围为 $\\Lambda_{\\mathcal{H}}$。\n$\\lambda(\\theta_1, \\theta_2)$ 的值取决于和 $\\cos\\theta_1 + \\cos\\theta_2$。由于 $\\cos(\\theta)$ 是偶函数，$\\lambda(\\theta_1, \\theta_2)$ 的值在构成 $\\mathcal{H}$ 的四个对称区域中是相同的。因此，我们可以将分析限制在 $\\theta_1 \\in [\\frac{\\pi}{2}, \\pi]$ 和 $\\theta_2 \\in [\\frac{\\pi}{2}, \\pi]$ 的区域。\n在区间 $[\\frac{\\pi}{2}, \\pi]$ 内，函数 $\\cos(\\theta)$ 是单调递减的，其值域从 $\\cos(\\frac{\\pi}{2}) = 0$ 到 $\\cos(\\pi) = -1$。\n在此区域上，和 $\\cos\\theta_1 + \\cos\\theta_2$ 的最小值在 $\\theta_1$ 和 $\\theta_2$ 均为 $\\pi$ 时取得，为 $\\cos(\\pi) + \\cos(\\pi) = -1 - 1 = -2$。\n最大值在 $\\theta_1$ 和 $\\theta_2$ 均为 $\\frac{\\pi}{2}$ 时取得，为 $\\cos(\\frac{\\pi}{2}) + \\cos(\\frac{\\pi}{2}) = 0 + 0 = 0$。\n因此，对于 $(\\theta_1, \\theta_2) \\in \\mathcal{H}$，$\\cos\\theta_1 + \\cos\\theta_2$ 的范围是 $[-2, 0]$。\n利用这一点，我们可以确定 $\\lambda(\\theta_1, \\theta_2) = 1 - \\frac{1}{2}(\\cos\\theta_1 + \\cos\\theta_2)$ 的范围。\n$\\lambda$ 的最小值对应于 $\\cos\\theta_1 + \\cos\\theta_2$ 的最大值：\n$$\n\\lambda_{\\min} = 1 - \\frac{1}{2}(0) = 1\n$$\n这发生在 $|\\theta_1|=|\\theta_2|=\\frac{\\pi}{2}$ 的模上。\n$\\lambda$ 的最大值对应于 $\\cos\\theta_1 + \\cos\\theta_2$ 的最小值：\n$$\n\\lambda_{\\max} = 1 - \\frac{1}{2}(-2) = 1 + 1 = 2\n$$\n这发生在 $|\\theta_1|=|\\theta_2|=\\pi$ 的模上。\n因此，值集 $\\Lambda_{\\mathcal{H}}$ 是闭区间 $[1, 2]$。\n\n光滑因子的表达式变成了一个在这个区间上的最大化问题：\n$$\n\\mu(\\omega) = \\max_{\\lambda \\in [1, 2]} |1 - \\omega \\lambda|\n$$\n对于固定的 $\\omega$，函数 $f(\\lambda) = 1 - \\omega\\lambda$ 是 $\\lambda$ 的线性函数。一个线性函数在闭区间上的最大绝对值必在区间的某个端点处取得。因此，我们计算 $|f(\\lambda)|$ 在 $\\lambda=1$ 和 $\\lambda=2$ 处的值：\n$$\n\\mu(\\omega) = \\max\\left\\{|1 - \\omega \\cdot 1|, |1 - \\omega \\cdot 2|\\right\\} = \\max\\left\\{|1 - \\omega|, |1 - 2\\omega|\\right\\}\n$$\n为了找到最优权重 $\\omega_{\\text{opt}}$，我们必须找到使 $\\mu(\\omega)$ 最小化的 $\\omega > 0$ 的值。这是一个经典的极小化极大问题。$\\max\\{|f_1(\\omega)|, |f_2(\\omega)|\\}$ 的最小值通常在 $|f_1(\\omega)| = |f_2(\\omega)|$ 的点上取得。我们令最大值函数中的各项的绝对值相等：\n$$\n|1 - \\omega| = |1 - 2\\omega|\n$$\n该方程产生两种可能性：\n1. $1 - \\omega = 1 - 2\\omega$，化简为 $\\omega = 0$。这不是一个有效的解，因为问题指定 $\\omega > 0$。\n2. $1 - \\omega = -(1 - 2\\omega)$，化简为 $1 - \\omega = 2\\omega - 1$。重新整理得到 $3\\omega = 2$，因此 $\\omega = \\frac{2}{3}$。\n\n为了确认 $\\omega = \\frac{2}{3}$ 确实是最小值点，我们可以分析 $\\mu(\\omega)$ 的行为。函数 $\\mu(\\omega)$ 由分段函数组成。$\\omega$ 的临界点是 $0$、$\\frac{1}{2}$ 和 $1$，在这些点上绝对值内的符号会发生变化。\n- 对于 $\\omega \\in (0, \\frac{2}{3})$，$\\mu(\\omega)$ 是 $\\omega$ 的递减函数。例如，在 $(0, \\frac{1}{2}]$ 中，$\\mu(\\omega) = 1-\\omega$。在 $(\\frac{1}{2}, \\frac{2}{3})$ 中，$\\mu(\\omega)=1-\\omega$ 也成立。\n- 对于 $\\omega \\in (\\frac{2}{3}, \\infty)$，$\\mu(\\omega)$ 是 $\\omega$ 的递增函数。例如，在 $(\\frac{2}{3}, 1)$ 中，$\\mu(\\omega) = 2\\omega-1$。对于 $\\omega \\ge 1$，$\\mu(\\omega)=2\\omega-1$ 也成立。\n\n由于当 $\\omega  \\frac{2}{3}$ 时 $\\mu(\\omega)$ 递减，而当 $\\omega > \\frac{2}{3}$ 时递增，因此对于 $\\omega>0$ 的全局最小值恰好在 $\\omega = \\frac{2}{3}$ 处取得。\n最小光滑因子是 $\\mu(\\frac{2}{3}) = |1 - \\frac{2}{3}| = \\frac{1}{3}$。\n问题要求使 $\\mu(\\omega)$ 最小化的 $\\omega$ 值。这个最优值是 $\\frac{2}{3}$。",
            "answer": "$$\n\\boxed{\\frac{2}{3}}\n$$"
        },
        {
            "introduction": "将理论转化为代码是数值计算的关键一步。本练习  将指导您构建标准的迭代求解器，并进一步实现一种先进的自适应SOR方法，展示如何动态调整参数以提高在具有挑战性的各向异性和非均匀问题上的收敛性能。",
            "id": "3455492",
            "problem": "考虑线性二阶椭圆偏微分方程 (PDE) $$-\\nabla \\cdot \\left(\\mathbf{K}(x,y)\\,\\nabla u(x,y)\\right) = f(x,y) \\quad \\text{for } (x,y)\\in(0,1)\\times(0,1),$$ 其齐次 Dirichlet 边界条件为 $$u(x,y)=0 \\quad \\text{for } (x,y)\\in\\partial\\left((0,1)\\times(0,1)\\right),$$ 其中对称正定的扩散张量为 $$\\mathbf{K}(x,y) = \\begin{pmatrix} k_x(x,y)  0 \\\\ 0  k_y(x,y) \\end{pmatrix}.$$ 使用标准的有限差分格式对该偏微分方程进行离散化，该格式建立在具有 $n\\times n$ 个内部点的均匀网格上，两个方向的网格间距均为 $h=1/(n+1)$，并对通量使用面心系数（在单元面进行算术平均）。设离散后的线性系统为 $$A u = b,$$ 其中 $A\\in\\mathbb{R}^{N\\times N}$，$u\\in\\mathbb{R}^N$，$b\\in\\mathbb{R}^N$，且 $N=n^2$。记 $D=\\mathrm{diag}(A)$ 为 $A$ 的对角部分，并将 $A$ 进行标准矩阵分裂 $A = D - L - U$，其中 $L$ 为严格下三角部分，$U$ 为严格上三角部分。\n\n实现以下三种定常迭代求解器来求解 $A u = b$：\n- Jacobi 方法（首次出现：Jacobi method），定义为 $$u^{(k+1)} = D^{-1}(L+U)u^{(k)} + D^{-1}b.$$\n- Gauss–Seidel 方法（首次出现：Gauss–Seidel method），通过使用分裂 $A = (D-L) - U$ 进行字典序前向替换来定义。\n- 逐次超松弛 (SOR) 方法（首次出现：Successive Over-Relaxation (SOR) method），定义为 $$u^{(k+1)} = (D - \\omega L)^{-1}\\left(\\omega b - (\\omega U + (\\omega - 1) D)u^{(k)}\\right),$$ 其中松弛参数 $\\omega\\in(0,2)$ 在每次迭代中使用根据当前残差构造的局部瑞利商估计自适应地选择。\n\n在第 $k$ 次迭代时，定义残差 $$r^{(k)} = b - A u^{(k)}.$$ 对于自适应 SOR 方法，计算局部瑞利商 $$\\rho_{\\text{loc}}^{(k)} = \\frac{\\left(r^{(k)}\\right)^{\\top} A\\, r^{(k)}}{\\left(r^{(k)}\\right)^{\\top} D\\, r^{(k)}}$$ 并通过一个有原则的映射，从 $\\rho_{\\text{loc}}^{(k)}$ 得到松弛参数 $\\omega^{(k)}$，以改善收敛性。该映射基于 Jacobi 迭代与 SOR 之间的关系。该映射必须在您的解中从第一性原理推导得出，并且得到的 $\\omega^{(k)}$ 必须满足 $0  \\omega^{(k)}  2$，同时为诸如残差消失等边缘情况提供适当的保障措施。\n\n离散化细节。对于内部索引 $(i,j)$，其中 $i=1,\\dots,n$ 且 $j=1,\\dots,n$，令 $x_i = i h$ 和 $y_j = j h$，并设 $$k_x^{i,j} = k_x(x_i,y_j),\\qquad k_y^{i,j} = k_y(x_i,y_j).$$ 通过算术平均定义面心扩散系数，\n$$k_{x}^{i+\\frac{1}{2},j} = \\begin{cases}\n\\frac{1}{2}\\left(k_x^{i,j} + k_x^{i+1,j}\\right),  i  n,\\\\\nk_x^{i,j},  i=n,\n\\end{cases}\n\\quad\nk_{x}^{i-\\frac{1}{2},j} = \\begin{cases}\n\\frac{1}{2}\\left(k_x^{i,j} + k_x^{i-1,j}\\right),  i > 1,\\\\\nk_x^{i,j},  i=1,\n\\end{cases}$$\n$$k_{y}^{i,j+\\frac{1}{2}} = \\begin{cases}\n\\frac{1}{2}\\left(k_y^{i,j} + k_y^{i,j+1}\\right),  j  n,\\\\\nk_y^{i,j},  j=n,\n\\end{cases}\n\\quad\nk_{y}^{i,j-\\frac{1}{2}} = \\begin{cases}\n\\frac{1}{2}\\left(k_y^{i,j} + k_y^{i,j-1}\\right),  j > 1,\\\\\nk_y^{i,j},  j=1.\n\\end{cases}$$\n那么在点 $(i,j)$ 处的离散算子为\n$$\n(Au)^{i,j} = \\left(c_x^{+\\,i,j} + c_x^{-\\,i,j} + c_y^{+\\,i,j} + c_y^{-\\,i,j}\\right) u^{i,j}\n- c_x^{+\\,i,j}\\, u^{i+1,j} - c_x^{-\\,i,j}\\, u^{i-1,j}\n- c_y^{+\\,i,j}\\, u^{i,j+1} - c_y^{-\\,i,j}\\, u^{i,j-1},\n$$\n其中 $$c_x^{+\\,i,j} = \\frac{k_{x}^{i+\\frac{1}{2},j}}{h^2},\\quad c_x^{-\\,i,j} = \\frac{k_{x}^{i-\\frac{1}{2},j}}{h^2},\\quad c_y^{+\\,i,j} = \\frac{k_{y}^{i,j+\\frac{1}{2}}}{h^2},\\quad c_y^{-\\,i,j} = \\frac{k_{y}^{i,j-\\frac{1}{2}}}{h^2},$$ 且内部区域外的边界值设为零（齐次 Dirichlet）。对角线元素为 $$D^{i,j} = c_x^{+\\,i,j} + c_x^{-\\,i,j} + c_y^{+\\,i,j} + c_y^{-\\,i,j}.$$\n\n使用源项 $$f(x,y) = 1$$，因此 $$b^{i,j} = f(x_i,y_j) = 1.$$\n\n停止准则与初始化。使用 $$u^{(0)} = 0$$ 进行初始化。在第 $k$ 次迭代时，计算残差范数 $$\\|r^{(k)}\\|_2 = \\left(\\sum_{i=1}^n\\sum_{j=1}^n \\left(\\left(r^{(k)}\\right)^{i,j}\\right)^2\\right)^{1/2}.$$ 当 $$\\|r^{(k)}\\|_2 \\le \\varepsilon,$$ 时停止，容差为 $$\\varepsilon = 10^{-6},$$ 或者当迭代次数达到 $$k_{\\max} = 5000$$ 时停止。所有量均为无量纲。\n\n测试套件。使用网格尺寸 $$n = 20.$$ 定义三个测试用例以评估在各向异性和非均匀性下的稳健性：\n- 测试 1（各向异性，$x$ 方向主导）：$$k_x(x,y) \\equiv 10,\\qquad k_y(x,y) \\equiv 1.$$\n- 测试 2（各向异性，$y$ 方向主导）：$$k_x(x,y) \\equiv 1,\\qquad k_y(x,y) \\equiv 10.$$\n- 测试 3（非均匀性，分片各向异性）：$$k_x(x,y) = 1 + 9\\,\\mathbf{1}_{\\{x > 0.5\\}},\\qquad k_y(x,y) = 1 + 9\\,\\mathbf{1}_{\\{y > 0.5\\}},$$ 其中 $\\mathbf{1}_{\\{\\cdot\\}}$ 是指示函数。\n\n对于每个测试用例，运行 Jacobi 方法、Gauss–Seidel 方法以及在每次迭代中从 $\\rho_{\\text{loc}}$ 选择 $\\omega$ 的自适应 SOR 方法。记录每种方法满足停止准则所需的迭代次数。如果在 $k_{\\max}$ 次迭代内未满足停止准则，则记录 $$k_{\\max}.$$ 最终输出格式要求为一行，包含一个由方括号括起来的九个整数的逗号分隔列表，顺序为\n$$[\\text{Jacobi}_1,\\text{GaussSeidel}_1,\\text{SOR}_1,\\text{Jacobi}_2,\\text{GaussSeidel}_2,\\text{SOR}_2,\\text{Jacobi}_3,\\text{GaussSeidel}_3,\\text{SOR}_3].$$",
            "solution": "用户提供了一个问题，要求实现和比较三种迭代求解器（Jacobi、Gauss-Seidel 和自适应逐次超松弛法），用于求解由二阶椭圆偏微分方程的有限差分法离散化得到的线性系统。\n\n### 步骤 1：问题验证\n\n在尝试求解之前，对问题进行了严格的验证过程。\n\n- **已知条件提取**：\n    - PDE：在单位正方形 $(0,1)\\times(0,1)$ 上求解 $-\\nabla \\cdot \\left(\\mathbf{K}(x,y)\\,\\nabla u(x,y)\\right) = f(x,y)$。\n    - 边界条件：齐次 Dirichlet 条件，$u(x,y)=0$ 在边界上成立。\n    - 扩散张量：$\\mathbf{K}(x,y) = \\mathrm{diag}(k_x(x,y), k_y(x,y))$，对称正定。\n    - 源项：$f(x,y) = 1$。\n    - 离散化：在具有 $n\\times n$ 个内部点的均匀网格上采用标准的七点有限差分格式，网格间距为 $h=1/(n+1)$。面心扩散系数使用算术平均计算。\n    - 线性系统：$A u = b$，其中 $N=n^2$。\n    - 迭代求解器：Jacobi、Gauss-Seidel 和自适应 SOR 方法。\n    - 自适应 SOR：松弛参数 $\\omega^{(k)}$ 在每次迭代 $k$ 时根据局部瑞利商 $\\rho_{\\text{loc}}^{(k)} = \\frac{\\left(r^{(k)}\\right)^{\\top} A\\, r^{(k)}}{\\left(r^{(k)}\\right)^{\\top} D\\, r^{(k)}}$ 确定，其中 $r^{(k)} = b - A u^{(k)}$ 且 $D = \\mathrm{diag}(A)$。\n    - 初始化：$u^{(0)} = 0$。\n    - 停止准则：$\\|r^{(k)}\\|_2 \\le \\varepsilon=10^{-6}$ 或迭代次数 $k \\ge k_{\\max}=5000$。\n    - 测试套件：$n=20$，并为扩散张量 $\\mathbf{K}(x,y)$ 指定了三个测试用例，以测试各向异性和非均匀性。\n    - 输出：一个包含九个整数的列表，表示每种方法在每个测试用例上的迭代次数。\n\n- **验证结论**：\n    1.  **科学基础扎实**：该问题是应用数值线性代数求解 PDE 的经典案例，是计算物理和工程学的基石。所涉及的方法和概念都是标准且成熟的。\n    2.  **适定的**：带有 Dirichlet 条件的椭圆型 PDE 是适定的。所述的有限差分法导出的线性系统 $Au=b$ 中，矩阵 $A$ 是对称正定的（SPD），保证了唯一解的存在。指定的迭代方法对于此类系统已知是收敛的。SOR 的自适应方案是一种公认的（尽管是高级的）技术。\n    3.  **客观的**：问题以数学精度进行规定，提供了所有必要的公式、参数和测试用例。它没有歧义或主观性陈述。\n    4.  **完整的**：提供了所有必需的信息。离散化方案，包括边界附近系数的处理，都有明确定义。\n\n问题被判定为**有效**。这是一个结构良好、可解的数值分析问题。\n\n### 步骤 2：原理性解决方案\n\n#### I. 离散化与线性系统\n偏微分方程 $-\\frac{\\partial}{\\partial x}(k_x \\frac{\\partial u}{\\partial x}) - \\frac{\\partial}{\\partial y}(k_y \\frac{\\partial u}{\\partial y}) = f$ 在间距为 $h=1/(n+1)$ 的均匀网格上进行离散化。对导数使用中心差分近似，并对面心系数 $k_x$ 和 $k_y$ 使用指定的算术平均，我们为每个内部网格点 $(i,j)$（其中 $i,j \\in \\{1, \\dots, n\\}$）获得一个线性方程。这个由 $N=n^2$ 个方程组成的系统可以写成矩阵形式 $Au = b$。\n\n矩阵 $A$ 对每个内部点都具有五点差分格式结构。对于一个点 $(i,j)$（映射到单个索引 $p$），矩阵 $A$ 的第 $p$ 行将有对应于该点本身及其四个邻点 $(i,j)$、$(i\\pm1, j)$ 和 $(i, j\\pm1)$ 的非零项。这些系数在问题描述中已定义。矩阵 $A$ 是对称的，因为 $k_{x}^{i+\\frac{1}{2},j}$ 是耦合节点 $(i,j)$ 和 $(i+1,j)$ 的系数，并且它也是耦合 $(i+1,j)$ 和 $(i,j)$ 的系数。由于扩散张量 $\\mathbf{K}$ 是正定的，得到的矩阵 $A$ 是对称正定（SPD）的。\n\n#### II. 迭代求解器\n我们的任务是使用三种定常迭代方法求解 $Au=b$。这些方法基于矩阵 $A$ 的分裂 $A = M - K$，这导致迭代格式 $M u^{(k+1)} = K u^{(k)} + b$。为保证收敛，迭代矩阵 $M^{-1}K$ 的谱半径必须小于 1。\n\n1.  **Jacobi 方法**：\n    矩阵 $A$ 分裂为 $A = D - L - U$，其中 $D$ 是 $A$ 的对角部分，$-L$ 是严格下三角部分，$-U$ 是严格上三角部分。Jacobi 方法使用 $M=D$ 和 $K=L+U$。迭代公式为：\n    $$u^{(k+1)} = D^{-1}(L+U)u^{(k)} + D^{-1}b$$\n    这可以使用残差 $r^{(k)} = b - Au^{(k)}$ 重写为：\n    $$u^{(k+1)} = D^{-1}(D-A)u^{(k)} + D^{-1}b = (I - D^{-1}A)u^{(k)} + D^{-1}b = u^{(k)} + D^{-1}(b - Au^{(k)}) = u^{(k)} + D^{-1}r^{(k)}$$\n    在计算上，每个分量 $u_i^{(k+1)}$ 仅使用 $u^{(k)}$ 的分量来计算。这意味着整个向量 $u^{(k+1)}$ 可以并行计算，并且需要存储前一个迭代的完整副本 $u^{(k)}$。\n\n2.  **Gauss-Seidel 方法**：\n    Gauss-Seidel 方法使用分裂 $M=D-L$ 和 $K=U$。迭代公式为：\n    $$(D-L)u^{(k+1)} = Uu^{(k)} + b$$\n    由于 $D-L$ 是下三角矩阵，可以通过前向替换求得 $u^{(k+1)}$。在实践中，这通过按字典序遍历网格点来实现，其中对 $u_{i,j}$ 的更新会立即使用在同一次迭代 $k+1$ 中新计算出的 $u_{i-1,j}$ 和 $u_{i,j-1}$ 的值。这通常比 Jacobi 方法收敛得更快。对于 SPD 矩阵 $A$，收敛性得到保证。\n\n3.  **自适应逐次超松弛 (SOR) 方法**：\n    SOR 方法是 Gauss-Seidel 方法的一种加速，引入了一个松弛参数 $\\omega$。矩阵分裂为 $M = \\frac{1}{\\omega}(D-\\omega L)$ 和 $K = \\frac{1}{\\omega}((1-\\omega)D + \\omega U)$。更新公式为：\n    $$u^{(k+1)} = (1-\\omega)u^{(k)} + \\omega u_{GS}^{(k+1)}$$\n    其中 $u_{GS}^{(k+1)}$ 是通过一个 Gauss-Seidel 步骤计算出的迭代值。对于 SPD 矩阵，对任何 $\\omega \\in (0,2)$，收敛性都得到保证。$\\omega$ 的最优选择，记为 $\\omega_{opt}$，可以极大地加速收敛。\n\n    **$\\omega$ 的自适应选择**：问题要求在每次迭代 $k$ 时采用自适应策略来选择 $\\omega$。对于一致有序矩阵，最优 $\\omega$ 的经典公式是：\n    $$\\omega_{opt} = \\frac{2}{1 + \\sqrt{1 - \\rho(T_J)^2}}$$\n    其中 $\\rho(T_J)$ 是 Jacobi 迭代矩阵 $T_J = D^{-1}(L+U) = I - D^{-1}A$ 的谱半径。自适应策略的核心是在每次迭代中估计 $\\rho(T_J)$。\n\n    $T_J$ 的特征值 $\\mu$ 与 $D^{-1}A$ 的特征值 $\\lambda$ 通过 $\\mu = 1-\\lambda$ 相关联。由于 $A$ 是 SPD 矩阵，特征值 $\\lambda$ 是实数且为正，并且为了使 Jacobi 收敛，必须有 $0  \\lambda  2$。谱半径为 $\\rho(T_J) = \\max_i|\\mu_i| = \\max_i|1-\\lambda_i|$。\n\n    问题提供了局部瑞利商来估计一个特征值：\n    $$\\rho_{\\text{loc}}^{(k)} = \\frac{\\left(r^{(k)}\\right)^{\\top} A\\, r^{(k)}}{\\left(r^{(k)}\\right)^{\\top} D\\, r^{(k)}} = \\frac{(D^{1/2}r^{(k)})^T (D^{-1/2} A D^{-1/2}) (D^{1/2}r^{(k)})}{(D^{1/2}r^{(k)})^T (D^{1/2}r^{(k)})}$$\n    这是矩阵 $D^{-1/2} A D^{-1/2}$ 的瑞利商，它与 $D^{-1}A$ 具有相同的特征值 $\\{\\lambda_i\\}$。因此，$\\rho_{\\text{loc}}^{(k)}$ 是 $D^{-1}A$ 的某个特征值 $\\lambda$ 的估计。经过几次迭代后，残差向量 $r^{(k)}$ 趋向于由那些最限制收敛的特征值所对应的特征向量主导。对于这类问题，这些是“最平滑的”特征向量，对应于 $D^{-1}A$ 的小特征值。\n\n    我们假设 $\\rho_{\\text{loc}}^{(k)}$ 是一个特征值 $\\lambda$ 的估计。一个常见的启发式方法是将 Jacobi 谱半径估计为 $\\hat{\\rho}_k = |1 - \\rho_{\\text{loc}}^{(k)}|$。将此代入 $\\omega_{opt}$ 的公式，得到 $\\omega^{(k)}$ 的自适应估计：\n    $$\\omega^{(k)} = \\frac{2}{1 + \\sqrt{1 - \\hat{\\rho}_k^2}} = \\frac{2}{1 + \\sqrt{1 - (1-\\rho_{\\text{loc}}^{(k)})^2}} = \\frac{2}{1 + \\sqrt{\\rho_{\\text{loc}}^{(k)}(2-\\rho_{\\text{loc}}^{(k)})}}$$\n    平方根下的项是非负的，因为 $0  \\lambda  2 \\implies 0  \\rho_{\\text{loc}}^{(k)}  2$。得到的 $\\omega^{(k)}$ 将在范围 $[1, 2)$ 内，提供超松弛。这个公式提供了从局部瑞利商到松弛参数的有原则的映射，正如所要求的那样。\n\n#### III. 实现\n解决方案将使用 Python 和 NumPy 库实现。一个主函数将协调三个测试用例。对于每个用例，它将：\n1.  设置网格和特定于问题的系数（$k_x, k_y$）。这涉及到为面心系数（$c_x^\\pm, c_y^\\pm$）和对角项 $D$ 创建二维数组。\n2.  定义一个辅助函数 `matvec(u)` 来高效地计算矩阵向量乘积 $Au$，这对于计算残差至关重要。\n3.  调用三个独立的函数，分别对应每个求解器（`jacobi_solver`, `gauss_seidel_solver`, `sor_solver`）。\n4.  每个求解器函数将初始化解向量 $u=0$，然后循环直到收敛或达到最大迭代次数。\n5.  在每个循环内，检查残差范数 $\\|r^{(k)}\\|_2$。Jacobi 求解器将使用一个辅助数组来存储前一个迭代值。Gauss-Seidel 和 SOR 求解器将执行原地更新。SOR 求解器还将根据推导的公式在每一步计算 $\\omega^{(k)}$。\n6.  记录每次运行的迭代次数。如果未达到收敛，则记录 $k_{\\max}$。\n最后收集结果并按指定格式打印。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the specified PDE using Jacobi, Gauss-Seidel, and adaptive SOR methods\n    for three different test cases and reports the number of iterations required for convergence.\n    \"\"\"\n    n = 20\n    epsilon = 1e-6\n    k_max = 5000\n\n    def kx1(x, y): return 10.0\n    def ky1(x, y): return 1.0\n\n    def kx2(x, y): return 1.0\n    def ky2(x, y): return 10.0\n\n    def kx3(x, y): return 10.0 if x > 0.5 else 1.0\n    def ky3(x, y): return 10.0 if y > 0.5 else 1.0\n\n    test_cases = [\n        (kx1, ky1),\n        (kx2, ky2),\n        (kx3, ky3),\n    ]\n\n    def setup_coefficients(n, kx_func, ky_func):\n        h = 1.0 / (n + 1)\n        h2 = h * h\n\n        kx_grid = np.zeros((n + 2, n + 2))\n        ky_grid = np.zeros((n + 2, n + 2))\n        for i in range(1, n + 1):\n            for j in range(1, n + 1):\n                kx_grid[i, j] = kx_func(i * h, j * h)\n                ky_grid[i, j] = ky_func(i * h, j * h)\n\n        cx_p = np.zeros((n, n)); cx_m = np.zeros((n, n))\n        cy_p = np.zeros((n, n)); cy_m = np.zeros((n, n))\n\n        for i in range(n):\n            for j in range(n):\n                i1, j1 = i + 1, j + 1\n\n                k_xp = kx_grid[i1, j1] if i1 == n else 0.5 * (kx_grid[i1, j1] + kx_grid[i1 + 1, j1])\n                k_xm = kx_grid[i1, j1] if i1 == 1 else 0.5 * (kx_grid[i1, j1] + kx_grid[i1 - 1, j1])\n                k_yp = ky_grid[i1, j1] if j1 == n else 0.5 * (ky_grid[i1, j1] + ky_grid[i1, j1 + 1])\n                k_ym = ky_grid[i1, j1] if j1 == 1 else 0.5 * (ky_grid[i1, j1] + ky_grid[i1, j1 - 1])\n                \n                cx_p[i, j] = k_xp / h2\n                cx_m[i, j] = k_xm / h2\n                cy_p[i, j] = k_yp / h2\n                cy_m[i, j] = k_ym / h2\n\n        D = cx_p + cx_m + cy_p + cy_m\n        return {'cx_p': cx_p, 'cx_m': cx_m, 'cy_p': cy_p, 'cy_m': cy_m, 'D': D}\n\n    def matvec(u, n, coeffs):\n        u_padded = np.pad(u, 1, 'constant')\n        u_ip1, u_im1 = u_padded[2:, 1:-1], u_padded[:-2, 1:-1]\n        u_jp1, u_jm1 = u_padded[1:-1, 2:], u_padded[1:-1, :-2]\n        \n        Au = (coeffs['D'] * u -\n              (coeffs['cx_p'] * u_ip1 + coeffs['cx_m'] * u_im1 +\n               coeffs['cy_p'] * u_jp1 + coeffs['cy_m'] * u_jm1))\n        return Au\n\n    def jacobi_solver(n, b, coeffs, k_max, epsilon):\n        u = np.zeros((n, n))\n        for k in range(k_max):\n            r = b - matvec(u, n, coeffs)\n            if np.linalg.norm(r) = epsilon:\n                return k\n            \n            u = u + r / coeffs['D'] # This is a parallel update, equivalent to Jacobi\n        return k_max\n\n    def gauss_seidel_solver(n, b, coeffs, k_max, epsilon):\n        u = np.zeros((n, n))\n        D = coeffs['D']\n        cx_p, cx_m = coeffs['cx_p'], coeffs['cx_m']\n        cy_p, cy_m = coeffs['cy_p'], coeffs['cy_m']\n\n        for k in range(k_max):\n            r = b - matvec(u, n, coeffs)\n            if np.linalg.norm(r) = epsilon:\n                return k\n            \n            u_padded = np.pad(u, 1, 'constant')\n            for i in range(n):\n                for j in range(n):\n                    i_p, j_p = i + 1, j + 1\n                    term_neighbors = (cx_p[i, j] * u_padded[i_p + 1, j_p] +\n                                      cx_m[i, j] * u_padded[i_p - 1, j_p] +\n                                      cy_p[i, j] * u_padded[i_p, j_p + 1] +\n                                      cy_m[i, j] * u_padded[i_p, j_p - 1])\n                    u_padded[i_p, j_p] = (b[i, j] + term_neighbors) / D[i, j]\n            u = u_padded[1:-1, 1:-1]\n        return k_max\n\n    def sor_solver(n, b, coeffs, k_max, epsilon):\n        u = np.zeros((n, n))\n        D = coeffs['D']\n        cx_p, cx_m = coeffs['cx_p'], coeffs['cx_m']\n        cy_p, cy_m = coeffs['cy_p'], coeffs['cy_m']\n\n        for k in range(k_max):\n            r = b - matvec(u, n, coeffs)\n            if np.linalg.norm(r) = epsilon:\n                return k\n\n            # Compute adaptive omega\n            Ar = matvec(r, n, coeffs)\n            r_Dr = np.sum(r * D * r)\n            r_Ar = np.sum(r * Ar)\n\n            if abs(r_Dr)  1e-14:\n                omega = 1.0 # Fallback to Gauss-Seidel\n            else:\n                rho_loc = r_Ar / r_Dr\n                rho_loc = max(1e-12, min(rho_loc, 2.0 - 1e-12))\n                omega = 2.0 / (1.0 + np.sqrt(rho_loc * (2.0 - rho_loc)))\n            \n            # SOR sweep\n            u_padded = np.pad(u, 1, 'constant')\n            for i in range(n):\n                for j in range(n):\n                    i_p, j_p = i + 1, j + 1\n                    term_neighbors = (cx_p[i, j] * u_padded[i_p + 1, j_p] +\n                                      cx_m[i, j] * u_padded[i_p - 1, j_p] +\n                                      cy_p[i, j] * u_padded[i_p, j_p + 1] +\n                                      cy_m[i, j] * u_padded[i_p, j_p - 1])\n                    \n                    u_gs = (b[i, j] + term_neighbors) / D[i, j]\n                    u_padded[i_p, j_p] += omega * (u_gs - u_padded[i_p, j_p])\n            u = u_padded[1:-1, 1:-1]\n        return k_max\n\n    results = []\n    b = np.ones((n, n))\n    \n    for kx_func, ky_func in test_cases:\n        coeffs = setup_coefficients(n, kx_func, ky_func)\n        \n        iter_jacobi = jacobi_solver(n, b, coeffs, k_max, epsilon)\n        iter_gs = gauss_seidel_solver(n, b, coeffs, k_max, epsilon)\n        iter_sor = sor_solver(n, b, coeffs, k_max, epsilon)\n        \n        results.extend([iter_jacobi, iter_gs, iter_sor])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "本练习  是一个高级应用，它将数值方法与复杂的物理现实相结合。它挑战您研究经典平滑器在存在尖锐、弯曲的材料界面时的行为，并探索计算顺序对此类问题收敛性的显著影响。",
            "id": "3455542",
            "problem": "考虑一个具有可变和不连续传导系数的扩散模型，该模型由椭圆偏微分方程 $-\\nabla \\cdot (k(x,y)\\nabla u(x,y)) = 0$ 定义在方形域 $\\Omega = (0,1)\\times(0,1)$ 上，并服从齐次Dirichlet边界条件 $u(x,y) = 0$ for $(x,y) \\in \\partial\\Omega$。传导系数 $k(x,y)$ 是分段常数，其不连续界面由一个正弦扰动的圆形定义。设该界面在相对于中心 $(x_c,y_c) = (1/2,1/2)$ 的极坐标中由 $r(\\theta) = r_0 + \\varepsilon \\sin(m\\theta)$ 给出，其中 $\\theta(x,y) = \\operatorname{atan2}(y-y_c, x-x_c)$，$\\rho(x,y) = \\sqrt{(x-x_c)^2 + (y-y_c)^2}$，且 $r_0 \\in (0,1/2)$，$\\varepsilon \\ge 0$，$m \\in \\mathbb{N}$。定义符号距离场 $\\varphi(x,y) = \\rho(x,y) - r(\\theta)$，以及传导系数为\n$$\nk(x,y) = \n\\begin{cases}\nk_{\\mathrm{in}},  \\text{if } \\varphi(x,y)  0, \\\\\nk_{\\mathrm{out}},  \\text{if } \\varphi(x,y) \\ge 0,\n\\end{cases}\n$$\n其中 $k_{\\mathrm{in}} > 0$ 和 $k_{\\mathrm{out}} > 0$ 为常数。\n\n使用具有 $N\\times N$ 个内部未知数和网格间距 $h = 1/(N+1)$ 的均匀单元中心网格对域 $\\Omega$ 进行离散化。对于每个内部单元 $(i,j)$，使用五点有限体积模板来近似算子 $-\\nabla \\cdot (k \\nabla u)$，并在面上对传导系数进行调和平均，以确保跨不连续面的通量连续性。具体来说，定义面传导系数\n$$\nk_{i+1/2,j} = \\frac{2\\,k_{i,j}\\,k_{i+1,j}}{k_{i,j} + k_{i+1,j}},\\quad\nk_{i-1/2,j} = \\frac{2\\,k_{i,j}\\,k_{i-1,j}}{k_{i,j} + k_{i-1,j}},\n$$\n$$\nk_{i,j+1/2} = \\frac{2\\,k_{i,j}\\,k_{i,j+1}}{k_{i,j} + k_{i,j+1}},\\quad\nk_{i,j-1/2} = \\frac{2\\,k_{i,j}\\,k_{i,j-1}}{k_{i,j} + k_{i,j-1}},\n$$\n并约定缺失的邻居（与边界相邻）使用内部值，且Dirichlet边界值为零。得到的离散算子具有以下系数\n$$\na_{i,j}^{\\mathrm{E}} = -\\frac{k_{i+1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{W}} = -\\frac{k_{i-1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{N}} = -\\frac{k_{i,j+1/2}}{h^2},\\quad\na_{i,j}^{\\mathrm{S}} = -\\frac{k_{i,j-1/2}}{h^2},\n$$\n$$\na_{i,j}^{\\mathrm{diag}} = -\\left(a_{i,j}^{\\mathrm{E}} + a_{i,j}^{\\mathrm{W}} + a_{i,j}^{\\mathrm{N}} + a_{i,j}^{\\mathrm{S}}\\right),\n$$\n因此作用在网格函数 $u$ 上的离散算子 $A$ 为\n$$\n(Au)_{i,j} = a_{i,j}^{\\mathrm{diag}}\\,u_{i,j}\n+ a_{i,j}^{\\mathrm{E}}\\,u_{i+1,j}\n+ a_{i,j}^{\\mathrm{W}}\\,u_{i-1,j}\n+ a_{i,j}^{\\mathrm{N}}\\,u_{i,j+1}\n+ a_{i,j}^{\\mathrm{S}}\\,u_{i,j-1}.\n$$\n\n与偏微分方程相关的线性系统的经典松弛方法有：\n- Jacobi：使用 $A$ 的对角部分 $D$ 进行更新\n$$\nu^{(n+1)} = u^{(n)} + \\omega_{\\mathrm{J}} D^{-1}\\left(f - A u^{(n)}\\right),\n$$\n其中标量权重 $0  \\omega_{\\mathrm{J}} \\le 1$ 用于保证稳定性，此处考虑 $f=0$。\n- Gauss-Seidel：将 $A$ 分解为严格下三角、对角和严格上三角部分，$A = L + D + U$，并更新\n$$\n(D+L)\\,u^{(n+1)} = f - U\\,u^{(n)},\n$$\n其中 $f=0$。未知数的排序决定了 $L$ 和 $U$ 的结构。\n- Successive Over-Relaxation (SOR)：通过以下方式将松弛参数 $\\omega_{\\mathrm{SOR}} \\in (0,2)$ 应用于 Gauss-Seidel 更新\n$$\nu^{(n+1)} = u^{(n)} + \\omega_{\\mathrm{SOR}}\\left(u^{(n+1)}_{\\mathrm{GS}} - u^{(n)}\\right),\n$$\n其中 $u^{(n+1)}_{\\mathrm{GS}}$ 表示在所选排序下当前节点的 Gauss-Seidel 更新。\n\n定义两种排序方式：\n- 自然字典序排序：按递增的行主序更新 $(i,j)$（例如，首先 $j=0$ 到 $N-1$，然后在每行内 $i=0$ 到 $N-1$）。\n- 界面对齐排序：计算网格点上的符号距离场 $\\varphi_{i,j}$，并按 $\\varphi_{i,j}$ 的升序对节点进行排序更新，即沿平行于界面的层进行扫描。\n\n为了量化“界面平行模式的光滑化”，构造一个初始误差模式，该模式局部化在界面附近并沿界面振荡：\n$$\ne_0(x,y) = \\exp\\!\\left(-\\left(\\frac{\\varphi(x,y)}{\\sigma}\\right)^2\\right)\\,\\sin\\!\\big(q\\,\\theta(x,y)\\big),\n$$\n其中 $\\sigma > 0$ 控制局部化带宽，$q \\in \\mathbb{N}$ 是角波数，选择为 $q = \\min\\{32, \\lfloor N/4 \\rfloor\\}$。设 $\\mathcal{M} = \\{(x,y) \\in \\Omega : |\\varphi(x,y)| \\le 2\\sigma\\}$ 为界面附近的测量带。对于从 $e_0$ 生成 $e_1$ 的单次松弛扫描，定义光滑因子\n$$\nS = \\frac{\\|e_1\\|_{2,\\mathcal{M}}}{\\|e_0\\|_{2,\\mathcal{M}}},\\quad\n\\|w\\|_{2,\\mathcal{M}} = \\left(\\sum_{(i,j)\\in \\mathcal{M}} w_{i,j}^2\\right)^{1/2},\n$$\n其中求和遍及坐标位于 $\\mathcal{M}$ 内的单元中心。\n\n实现离散算子和三种松弛方法（Jacobi、Gauss-Seidel 和 Successive Over-Relaxation），并计算五种配置下的光滑因子 $S$：\n1. 使用权重 $\\omega_{\\mathrm{J}} = 0.66$ 的 Jacobi 方法。\n2. 使用自然字典序排序的 Gauss-Seidel 方法（无松弛，即 $\\omega_{\\mathrm{SOR}} = 1$）。\n3. 使用界面对齐排序的 Gauss-Seidel 方法（无松弛）。\n4. 使用 $\\omega_{\\mathrm{SOR}} = 1.9$ 和自然字典序排序的 Successive Over-Relaxation 方法。\n5. 使用 $\\omega_{\\mathrm{SOR}} = 1.9$ 和界面对齐排序的 Successive Over-Relaxation 方法。\n\n您的程序必须为以下每个测试案例计算这五个光滑因子并返回结果：\n\n测试套件：\n- 案例 A (正常路径): $N=96$, $r_0=0.35$, $\\varepsilon=0.04$, $m=4$, $k_{\\mathrm{in}}=10$, $k_{\\mathrm{out}}=1$, $\\sigma=0.03$, $\\omega_{\\mathrm{J}}=0.66$, $\\omega_{\\mathrm{SOR}}=1.9$。\n- 案例 B (高对比度): $N=96$, $r_0=0.35$, $\\varepsilon=0.08$, $m=6$, $k_{\\mathrm{in}}=1000$, $k_{\\mathrm{out}}=1$, $\\sigma=0.03$, $\\omega_{\\mathrm{J}}=0.66$, $\\omega_{\\mathrm{SOR}}=1.9$。\n- 案例 C (边界和曲率变化): $N=48$, $r_0=0.35$, $\\varepsilon=0.12$, $m=8$, $k_{\\mathrm{in}}=50$, $k_{\\mathrm{out}}=1$, $\\sigma=0.04$, $\\omega_{\\mathrm{J}}=0.66$, $\\omega_{\\mathrm{SOR}}=1.9$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个以逗号分隔的浮点数列表的列表，结果的顺序为案例A、B、C，每个案例内部按上述五种配置的顺序排列。例如，输出形式应为\n$$\n\\big[\\,[S_{A,\\mathrm{J}}, S_{A,\\mathrm{GSlex}}, S_{A,\\mathrm{GS\\varphi}}, S_{A,\\mathrm{SORlex}}, S_{A,\\mathrm{SOR\\varphi}}],\\ [S_{B,\\mathrm{J}}, \\dots],\\ [S_{C,\\mathrm{J}}, \\dots]\\,\\big]\n$$\n作为单行Python列表字面量打印。",
            "solution": "我们从椭圆扩散模型 $-\\nabla \\cdot (k \\nabla u) = 0$ 开始，该模型定义在 $\\Omega = (0,1)^2$ 上，并具有齐次Dirichlet边界条件。传导系数 $k(x,y)$ 沿由水平集 $\\varphi(x,y) = 0$ 定义的弯曲界面 $\\Gamma$ 发生不连续，其中符号距离 $\\varphi(x,y) = \\rho(x,y) - r(\\theta)$ 使用中心 $(x_c,y_c)=(1/2,1/2)$、径向坐标 $\\rho(x,y) = \\sqrt{(x-x_c)^2 + (y-y_c)^2}$、角度 $\\theta(x,y) = \\operatorname{atan2}(y-y_c, x-x_c)$ 以及扰动半径 $r(\\theta) = r_0 + \\varepsilon \\sin(m\\theta)$。传导系数在内部（$\\varphi  0$）为 $k_{\\mathrm{in}}$，在外部（$\\varphi \\ge 0$）为 $k_{\\mathrm{out}}$。\n\n为了进行离散化，我们采用一个包含 $N \\times N$ 个未知量 $(i,j)$ 的单元中心网格，其中 $i,j \\in \\{0,\\dots,N-1\\}$。网格间距为 $h=1/(N+1)$，因此单元中心 $(i,j)$ 的位置是 $(x_i, y_j) = ((i+1)h, (j+1)h)$。我们以通量形式近似散度算子。$(i,j)$ 和 $(i+1,j)$ 之间垂直面上的法向通量近似为 $-k_{i+1/2,j}\\,(u_{i+1,j} - u_{i,j})/h$，其中 $k_{i+1/2,j}$ 是一个面传导系数。对于不连续系数，一个经过充分检验的近似是跨面的调和平均，\n$$\nk_{i+1/2,j} = \\frac{2\\,k_{i,j}\\,k_{i+1,j}}{k_{i,j} + k_{i+1,j}},\n$$\n它能强制保证通量的连续性，并与物理界面条件保持一致。类似的公式适用于所有面。在零Dirichlet边界条件下，内部网格外部的邻近值为零，我们取边界邻近面上的传导系数等于内部值以保持一致性（由于边界值为零，邻居的贡献消失）。\n\n将单元 $(i,j)$ 四个面上的通量求和再除以 $h$，得到离散算子 $A$ 的五点模板：\n$$\n(Au)_{i,j} =\n\\frac{k_{i+1/2,j}}{h^2}\\,(u_{i,j} - u_{i+1,j})\n+ \\frac{k_{i-1/2,j}}{h^2}\\,(u_{i,j} - u_{i-1,j})\n+ \\frac{k_{i,j+1/2}}{h^2}\\,(u_{i,j} - u_{i,j+1})\n+ \\frac{k_{i,j-1/2}}{h^2}\\,(u_{i,j} - u_{i,j-1}).\n$$\n重写后得到标准形式\n$$\n(Au)_{i,j} = a_{i,j}^{\\mathrm{diag}}\\,u_{i,j}\n+ a_{i,j}^{\\mathrm{E}}\\,u_{i+1,j}\n+ a_{i,j}^{\\mathrm{W}}\\,u_{i-1,j}\n+ a_{i,j}^{\\mathrm{N}}\\,u_{i,j+1}\n+ a_{i,j}^{\\mathrm{S}}\\,u_{i,j-1},\n$$\n其中非对角系数为负，\n$$\na_{i,j}^{\\mathrm{E}} = -\\frac{k_{i+1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{W}} = -\\frac{k_{i-1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{N}} = -\\frac{k_{i,j+1/2}}{h^2},\\quad\na_{i,j}^{\\mathrm{S}} = -\\frac{k_{i,j-1/2}}{h^2},\n$$\n对角系数是正的面权重之和\n$$\na_{i,j}^{\\mathrm{diag}} = -\\left(a_{i,j}^{\\mathrm{E}} + a_{i,j}^{\\mathrm{W}} + a_{i,j}^{\\mathrm{N}} + a_{i,j}^{\\mathrm{S}}\\right)\n= \\frac{k_{i+1/2,j} + k_{i-1/2,j} + k_{i,j+1/2} + k_{i,j-1/2}}{h^2}.\n$$\n\n松弛法作用于线性系统 $A u = f$；这里我们研究误差方程在 $f=0$ 时的光滑化，使得更新将误差 $e^{(n)}$ 转换为 $e^{(n+1)}$。Jacobi 方法仅使用 $A$ 的对角部分 $D$：\n$$\ne^{(n+1)} = e^{(n)} - \\omega_{\\mathrm{J}}\\,D^{-1} A e^{(n)}.\n$$\n在 $(i,j)$ 处的分量形式中，Jacobi 更新可以表示为\n$$\ne_{i,j}^{(n+1)} = (1-\\omega_{\\mathrm{J}})\\,e_{i,j}^{(n)} + \\omega_{\\mathrm{J}}\\,\\frac{-a_{i,j}^{\\mathrm{E}}\\,e_{i+1,j}^{(n)}\n- a_{i,j}^{\\mathrm{W}}\\,e_{i-1,j}^{(n)}\n- a_{i,j}^{\\mathrm{N}}\\,e_{i,j+1}^{(n)}\n- a_{i,j}^{\\mathrm{S}}\\,e_{i,j-1}^{(n)}}{a_{i,j}^{\\mathrm{diag}}},\n$$\n这是通过重排 $D^{-1} A e^{(n)}$ 并识别五点模板结构得出的。\n\nGauss-Seidel 方法在规定的排序中使用最新的可用邻居值。如果我们根据所选排序，用 $L$、$D$ 和 $U$ 表示 $A$ 的下三角、对角和上三角部分，则更新求解\n$$\n(D+L)\\,e^{(n+1)} = -U\\,e^{(n)}.\n$$\n在所选排序中访问的节点 $(i,j)$ 处，以分量形式表示，Gauss-Seidel 方法将当前扫描中已更新的邻居替换为 $e^{(n+1)}$，尚未访问的邻居替换为 $e^{(n)}$，从而得到\n$$\ne_{i,j}^{(n+1)} = \\frac{-a_{i,j}^{\\mathrm{E}}\\,\\tilde{e}_{i+1,j}\n- a_{i,j}^{\\mathrm{W}}\\,\\tilde{e}_{i-1,j}\n- a_{i,j}^{\\mathrm{N}}\\,\\tilde{e}_{i,j+1}\n- a_{i,j}^{\\mathrm{S}}\\,\\tilde{e}_{i,j-1}}{a_{i,j}^{\\mathrm{diag}}},\n$$\n其中 $\\tilde{e}$ 表示遵循排序的最新可用邻居值。Successive Over-Relaxation 方法在每个节点上将一个松弛因子应用于 Gauss-Seidel 更新：\n$$\ne_{i,j}^{(n+1)} = (1-\\omega_{\\mathrm{SOR}})\\,e_{i,j}^{(n)} + \\omega_{\\mathrm{SOR}}\\,\\frac{-a_{i,j}^{\\mathrm{E}}\\,\\tilde{e}_{i+1,j}\n- a_{i,j}^{\\mathrm{W}}\\,\\tilde{e}_{i-1,j}\n- a_{i,j}^{\\mathrm{N}}\\,\\tilde{e}_{i,j+1}\n- a_{i,j}^{\\mathrm{S}}\\,\\tilde{e}_{i,j-1}}{a_{i,j}^{\\mathrm{diag}}}.\n$$\n\n排序的选择对 Gauss-Seidel 和 Successive Over-Relaxation 方法至关重要，因为它决定了哪些邻居值被视为“已更新”，从而决定了该方法更积极地衰减哪些误差分量。自然字典序排序对应于逐行扫描网格，这与笛卡尔方向对齐。界面对齐排序按符号距离 $\\varphi$ 的升序对节点进行排序，从而有效地沿平行于界面的层进行扫描。对于沿界面振荡并局部化在其附近的模式，将扫描与常数 $\\varphi$ 的层对齐，会优先为后续节点使用界面法向上的新更新值，从而改善界面平行误差分量的衰减，否则当排序未对齐时，这些分量会沿着不连续面滑动。\n\n为了量化界面平行模式的光滑化，我们定义初始误差\n$$\ne_0(x,y) = \\exp\\!\\left(-\\left(\\frac{\\varphi(x,y)}{\\sigma}\\right)^2\\right)\\,\\sin\\!\\big(q\\,\\theta(x,y)\\big),\n$$\n其中 $q$ 相对于网格分辨率选择（$q = \\min\\{32, \\lfloor N/4 \\rfloor\\}$），$\\sigma$ 控制局部化。在给定方法和排序下进行一次松弛扫描产生 $e_1$ 后，我们计算光滑因子\n$$\nS = \\frac{\\|e_1\\|_{2,\\mathcal{M}}}{\\|e_0\\|_{2,\\mathcal{M}}},\n$$\n其中 $\\mathcal{M} = \\{(x,y): |\\varphi(x,y)| \\le 2\\sigma\\}$ 隔离了界面周围的一个带状区域。这个度量隔离了松弛法相对于界面平行振荡的行为，避免了边界条件和远离不连续面区域的影响。\n\n算法步骤：\n1. 构造网格坐标 $(x_i,y_j)$（$i,j=0,\\dots,N-1$）并计算 $\\theta_{i,j}$、$\\rho_{i,j}$、$r(\\theta_{i,j})$ 和 $\\varphi_{i,j}$。\n2. 根据 $\\varphi_{i,j}$ 的符号，将 $k_{i,j}$ 构建为 $k_{\\mathrm{in}}$ 或 $k_{\\mathrm{out}}$。\n3. 对所有内部面，通过调和平均计算面传导系数；对于边界上的面，使用内部传导系数。\n4. 组装五点模板系数 $a_{i,j}^{\\mathrm{E}}$、$a_{i,j}^{\\mathrm{W}}$、$a_{i,j}^{\\mathrm{N}}$、$a_{i,j}^{\\mathrm{S}}$ 和 $a_{i,j}^{\\mathrm{diag}}$。\n5. 使用 $\\sigma$ 和 $q$ 构造初始误差场 $e_0$，并确定测量掩码 $\\mathcal{M}$，其中 $|\\varphi| \\le 2\\sigma$。\n6. 执行一次松弛扫描：\n   - 对于 Jacobi 方法，仅使用旧的邻居值和权重 $\\omega_{\\mathrm{J}}$ 计算各处的更新。\n   - 对于 Gauss-Seidel 和 Successive Over-Relaxation 方法，按自然字典序或按 $\\varphi$ 排序遍历节点；在每个节点，使用最新的可用邻居值形成 Gauss-Seidel 原始更新，然后应用松弛参数 $\\omega_{\\mathrm{SOR}}$（对于 Gauss-Seidel，等于 $1$）。\n7. 计算光滑因子 $S$，即掩码 $\\mathcal{M}$ 上离散 $\\ell^2$ 范数的比值。\n8. 对所有测试案例重复以上步骤。\n\n科学真实性考虑：\n- 调和平均是处理不连续系数的适当选择，以尊重通量连续性，并被广泛用于偏微分方程的数值解法中。\n- 初始模式 $e_0$ 被设计用来探测松弛法在界面切线方向上的行为，并通过高斯包络在法线方向上进行局部化，这使其对不连续面附近的排序敏感。\n- Successive Over-Relaxation 参数被设置为 $\\omega_{\\mathrm{SOR}} = 1.9$ 以展示稳定界限内的典型超松弛行为，而 Jacobi 方法使用 $\\omega_{\\mathrm{J}} = 0.66$ 以实现阻尼而不失稳定。\n\n程序实现了所有步骤，并为每个测试案例输出五个浮点光滑因子，分别对应于 Jacobi、使用字典序的 Gauss-Seidel、使用界面对齐排序的 Gauss-Seidel、使用字典序的 Successive Over-Relaxation 和使用界面对齐排序的 Successive Over-Relaxation，并按规定格式将它们聚合为单行打印的列表的列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef harmonic(a, b):\n    s = a + b\n    if s == 0:\n        return 0.0\n    return 2.0 * a * b / s\n\ndef build_geometry(N, r0, eps, m):\n    h = 1.0 / (N + 1)\n    i = np.arange(N)\n    j = np.arange(N)\n    x = (i + 1) * h\n    y = (j + 1) * h\n    X, Y = np.meshgrid(x, y, indexing='ij')\n    xc, yc = 0.5, 0.5\n    theta = np.arctan2(Y - yc, X - xc)\n    rho = np.sqrt((X - xc) ** 2 + (Y - yc) ** 2)\n    r_theta = r0 + eps * np.sin(m * theta)\n    phi = rho - r_theta\n    return X, Y, theta, rho, r_theta, phi, h\n\ndef build_conductivity(phi, kin, kout):\n    k = np.where(phi  0.0, kin, kout)\n    return k\n\ndef assemble_stencil(k, h):\n    N = k.shape[0]\n    # Face conductivities with harmonic averaging; handle boundaries\n    kE = np.empty_like(k)\n    kW = np.empty_like(k)\n    kN = np.empty_like(k)\n    kS = np.empty_like(k)\n\n    # East faces\n    kE[:-1, :] = harmonic_array(k[:-1, :], k[1:, :])\n    kE[-1, :] = k[-1, :]  # boundary face uses interior value\n\n    # West faces\n    kW[1:, :] = harmonic_array(k[1:, :], k[:-1, :])\n    kW[0, :] = k[0, :]\n\n    # North faces\n    kN[:, :-1] = harmonic_array(k[:, :-1], k[:, 1:])\n    kN[:, -1] = k[:, -1]\n\n    # South faces\n    kS[:, 1:] = harmonic_array(k[:, 1:], k[:, :-1])\n    kS[:, 0] = k[:, 0]\n\n    aE = -kE / (h ** 2)\n    aW = -kW / (h ** 2)\n    aN = -kN / (h ** 2)\n    aS = -kS / (h ** 2)\n    aD = -(aE + aW + aN + aS)\n    return aD, aE, aW, aN, aS\n\ndef harmonic_array(a, b):\n    s = a + b\n    out = np.zeros_like(s)\n    mask = s > 0\n    out[mask] = 2.0 * a[mask] * b[mask] / s[mask]\n    # if s == 0 (should not in our positive kin,kout), keep zero\n    return out\n\ndef initial_error(theta, phi, sigma, N):\n    q = int(min(32, N // 4))\n    e0 = np.exp(-(phi / sigma) ** 2) * np.sin(q * theta)\n    return e0\n\ndef mask_band(phi, sigma):\n    return np.abs(phi) = (2.0 * sigma)\n\ndef jacobi_sweep(u_old, aD, aE, aW, aN, aS, omegaJ):\n    # Build neighbor arrays with zero at boundaries\n    N = u_old.shape[0]\n    uR = np.zeros_like(u_old)\n    uL = np.zeros_like(u_old)\n    uU = np.zeros_like(u_old)\n    uD = np.zeros_like(u_old)\n    uR[:-1, :] = u_old[1:, :]\n    uL[1:, :] = u_old[:-1, :]\n    uU[:, :-1] = u_old[:, 1:]\n    uD[:, 1:] = u_old[:, :-1]\n    s = aE * uR + aW * uL + aN * uU + aS * uD\n    raw = -s / aD\n    u_new = (1.0 - omegaJ) * u_old + omegaJ * raw\n    return u_new\n\ndef build_order_indices(N, phi, mode='lex'):\n    if mode == 'lex':\n        # row-major: j outer, i inner\n        order = [(i, j) for j in range(N) for i in range(N)]\n    elif mode == 'phi':\n        # sort by ascending phi\n        flat_indices = [(i, j) for i in range(N) for j in range(N)]\n        flat_phi = [phi[i, j] for i, j in flat_indices]\n        order = [x for _, x in sorted(zip(flat_phi, flat_indices), key=lambda t: t[0])]\n    else:\n        raise ValueError(\"Unknown ordering mode\")\n    return order\n\ndef gs_sweep(u_old, aD, aE, aW, aN, aS, order, omega=1.0):\n    N = u_old.shape[0]\n    u_new = u_old.copy()\n    visited = np.zeros_like(u_old, dtype=bool)\n    for i, j in order:\n        s = 0.0\n        # East neighbor (i+1, j)\n        if i + 1  N:\n            val = u_new[i + 1, j] if visited[i + 1, j] else u_old[i + 1, j]\n            s += aE[i, j] * val\n        # West neighbor (i-1, j)\n        if i - 1 >= 0:\n            val = u_new[i - 1, j] if visited[i - 1, j] else u_old[i - 1, j]\n            s += aW[i, j] * val\n        # North neighbor (i, j+1)\n        if j + 1  N:\n            val = u_new[i, j + 1] if visited[i, j + 1] else u_old[i, j + 1]\n            s += aN[i, j] * val\n        # South neighbor (i, j-1)\n        if j - 1 >= 0:\n            val = u_new[i, j - 1] if visited[i, j - 1] else u_old[i, j - 1]\n            s += aS[i, j] * val\n\n        raw = -s / aD[i, j]\n        u_new[i, j] = (1.0 - omega) * u_old[i, j] + omega * raw\n        visited[i, j] = True\n    return u_new\n\ndef smoothing_factor(e0, e1, mask):\n    # compute discrete L2 norm over mask; ratio cancels h\n    sel0 = e0[mask]\n    sel1 = e1[mask]\n    norm0 = np.sqrt(np.sum(sel0 * sel0))\n    norm1 = np.sqrt(np.sum(sel1 * sel1))\n    # To avoid division by zero, if norm0 is zero, define smoothing factor as 0.0\n    if norm0 == 0.0:\n        return 0.0\n    return float(norm1 / norm0)\n\ndef run_case(N, r0, eps, m, kin, kout, sigma, omegaJ, omegaSOR):\n    # Geometry and coefficients\n    X, Y, theta, rho, r_theta, phi, h = build_geometry(N, r0, eps, m)\n    k = build_conductivity(phi, kin, kout)\n    aD, aE, aW, aN, aS = assemble_stencil(k, h)\n    # Initial error and mask\n    e0 = initial_error(theta, phi, sigma, N)\n    mask = mask_band(phi, sigma)\n\n    # Jacobi\n    e1_jacobi = jacobi_sweep(e0, aD, aE, aW, aN, aS, omegaJ)\n    S_jacobi = smoothing_factor(e0, e1_jacobi, mask)\n\n    # GS lex\n    order_lex = build_order_indices(N, phi, mode='lex')\n    e1_gs_lex = gs_sweep(e0, aD, aE, aW, aN, aS, order_lex, omega=1.0)\n    S_gs_lex = smoothing_factor(e0, e1_gs_lex, mask)\n\n    # GS phi\n    order_phi = build_order_indices(N, phi, mode='phi')\n    e1_gs_phi = gs_sweep(e0, aD, aE, aW, aN, aS, order_phi, omega=1.0)\n    S_gs_phi = smoothing_factor(e0, e1_gs_phi, mask)\n\n    # SOR lex\n    e1_sor_lex = gs_sweep(e0, aD, aE, aW, aN, aS, order_lex, omega=omegaSOR)\n    S_sor_lex = smoothing_factor(e0, e1_sor_lex, mask)\n\n    # SOR phi\n    e1_sor_phi = gs_sweep(e0, aD, aE, aW, aN, aS, order_phi, omega=omegaSOR)\n    S_sor_phi = smoothing_factor(e0, e1_sor_phi, mask)\n\n    return [S_jacobi, S_gs_lex, S_gs_phi, S_sor_lex, S_sor_phi]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (happy path)\n        {\"N\": 96, \"r0\": 0.35, \"eps\": 0.04, \"m\": 4, \"kin\": 10.0, \"kout\": 1.0, \"sigma\": 0.03, \"omegaJ\": 0.66, \"omegaSOR\": 1.9},\n        # Case B (high contrast)\n        {\"N\": 96, \"r0\": 0.35, \"eps\": 0.08, \"m\": 6, \"kin\": 1000.0, \"kout\": 1.0, \"sigma\": 0.03, \"omegaJ\": 0.66, \"omegaSOR\": 1.9},\n        # Case C (boundary and curvature variation)\n        {\"N\": 48, \"r0\": 0.35, \"eps\": 0.12, \"m\": 8, \"kin\": 50.0, \"kout\": 1.0, \"sigma\": 0.04, \"omegaJ\": 0.66, \"omegaSOR\": 1.9},\n    ]\n\n    results = []\n    for case in test_cases:\n        res = run_case(\n            N=case[\"N\"],\n            r0=case[\"r0\"],\n            eps=case[\"eps\"],\n            m=case[\"m\"],\n            kin=case[\"kin\"],\n            kout=case[\"kout\"],\n            sigma=case[\"sigma\"],\n            omegaJ=case[\"omegaJ\"],\n            omegaSOR=case[\"omegaSOR\"],\n        )\n        # Round results to a reasonable number of decimals for readability\n        results.append([float(f\"{x:.6f}\") for x in res])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}