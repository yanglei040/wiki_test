## 引言
在计算科学与工程领域，求解由[偏微分方程](@entry_id:141332)（PDEs）离散化后产生的[大型稀疏线性系统](@entry_id:137968) $A\mathbf{x}=\mathbf{b}$ 是一项核心且无处不在的任务。虽然高斯消去等直接法在理论上可以得到精确解，但对于现代科学计算中动辄包含数百万甚至数十亿未知数的问题，直接法的计算成本（时间与内存）往往高得令人无法承受。这使得迭代法——通过从一个初始猜测出发，逐步逼近真实解——成为不可或缺的求解策略。

本文聚焦于一类最基础且最具启发性的迭代法：[定常迭代法](@entry_id:144014)，具体包括雅可比（Jacobi）、高斯-赛德尔（Gauss-Seidel, GS）和逐次超松弛（Successive Over-Relaxation, SOR）方法。尽管更先进的迭代技术（如[克雷洛夫子空间](@entry_id:751067)法）在许多方面超越了它们，但深刻理解这些经典方法的原理、性能和局限性，是掌握现代数值算法的基石。本文旨在填补从理论学习到实际应用与前沿认知之间的鸿沟，揭示这些“古老”方法的现代价值。

为实现这一目标，本文将分为三个核心部分。首先，在“原理与机制”一章中，我们将建立[定常迭代法](@entry_id:144014)的统一数学框架，通过矩阵分裂推导迭代格式，并深入探讨以[谱半径](@entry_id:138984)为核心的收敛性理论，最后通过对模型问题的分析来量化比较不同方法的[收敛速度](@entry_id:636873)。接着，在“应用与交叉学科联系”一章中，我们将视野拓宽至更复杂的应用场景，探讨如何调整这些方法以应对各向异性、[对流](@entry_id:141806)占优等物理挑战，并重点阐述它们在现代[多重网格](@entry_id:172017)算法中作为“光滑算子”的关键角色，以及它们与优化理论和机器学习等领域的深刻联系。最后，“动手实践”部分将提供一系列精心设计的问题，引导您将理论知识付诸实践，解决从基础实现到高级分析的挑战。

## 原理与机制

在[求解偏微分方程](@entry_id:138485)（PDEs）的[数值离散化](@entry_id:752782)所产生的[大型稀疏线性系统](@entry_id:137968) $A\mathbf{x}=\mathbf{b}$ 时，直接法（如[LU分解](@entry_id:144767)）的计算成本和内存需求可能高得令人望而却步。因此，[迭代法](@entry_id:194857)成为了一种不可或缺的工具。本章将深入探讨一类经典而基础的迭代法：[定常迭代法](@entry_id:144014)，主要包括雅可比（Jacobi）、高斯-赛德尔（Gauss-Seidel, GS）和逐次超松弛（Successive Over-Relaxation, SOR）方法。我们将从它们共同的数学框架出发，揭示其内在机制，分析其收敛特性，并最终阐明它们在现代高性能算法（如[多重网格法](@entry_id:146386)）中作为“光滑算子”的关键作用。

### [定常迭代法](@entry_id:144014)的数学框架

[定常迭代法](@entry_id:144014)的核心思想是将矩阵 $A$ 分裂为一个“易于求解”的矩阵 $M$ 和一个[余项](@entry_id:159839)矩阵 $N$ 的差，即 $A=M-N$。通过这种分裂，原方程 $A\mathbf{x}=\mathbf{b}$ 可以改写为 $M\mathbf{x}=N\mathbf{x}+\mathbf{b}$。这一形式自然地引出了一个迭代格式：

$$
M\mathbf{x}^{(k+1)} = N\mathbf{x}^{(k)} + \mathbf{b}
$$

其中，$\mathbf{x}^{(k)}$ 是第 $k$ 次迭代的解向量。由于我们选择的 $M$ 是“易于求解”的（通常是对角矩阵或[三角矩阵](@entry_id:636278)），因此从 $\mathbf{x}^{(k)}$ 计算 $\mathbf{x}^{(k+1)}$ 的成本很低。这个过程持续进行，直到解收敛到足够的精度。

#### [迭代矩阵](@entry_id:637346)与收敛性

为了分析迭代的收敛性，我们将上述迭代格式写成一个显式的[定点迭代](@entry_id:137769)形式。由于 $M$ 是非奇异的，我们可以写出：

$$
\mathbf{x}^{(k+1)} = M^{-1}N\mathbf{x}^{(k)} + M^{-1}\mathbf{b}
$$

这是一个形如 $\mathbf{x}^{(k+1)} = G\mathbf{x}^{(k)} + \mathbf{c}$ 的标准仿射迭代，其中 $G = M^{-1}N$ 被称为**[迭代矩阵](@entry_id:637346)**。

迭代的收敛性完全由[迭代矩阵](@entry_id:637346) $G$ 的谱半径 $\rho(G)$ 决定。[谱半径](@entry_id:138984)定义为[矩阵特征值](@entry_id:156365)[绝对值](@entry_id:147688)的最大值，即 $\rho(G) = \max_i |\lambda_i(G)|$。一个基本而重要的结论是：**对于任意初始猜测 $\mathbf{x}^{(0)}$，[定常迭代法](@entry_id:144014)收敛的充分必要条件是[迭代矩阵](@entry_id:637346)的谱半径严格小于1，即 $\rho(G)  1$**。

#### 误差与残差的传播

为了更深入地理解收敛过程，我们可以考察**误差**向量 $\mathbf{e}^{(k)} = \mathbf{x} - \mathbf{x}^{(k)}$（其中 $\mathbf{x}$ 是真实解）的演化。真实解满足 $M\mathbf{x}=N\mathbf{x}+\mathbf{b}$。将迭代格式 $M\mathbf{x}^{(k+1)} = N\mathbf{x}^{(k)} + \mathbf{b}$ 与此式相减，得到：

$$
M(\mathbf{x} - \mathbf{x}^{(k+1)}) = N(\mathbf{x} - \mathbf{x}^{(k)})
$$

这导出了**[误差传播](@entry_id:147381)关系** ：

$$
\mathbf{e}^{(k+1)} = M^{-1}N\mathbf{e}^{(k)} = G\mathbf{e}^{(k)}
$$

由此可见，每一次迭代都将误差向量左乘[迭代矩阵](@entry_id:637346) $G$。为了使误差最终趋于零，$\lim_{k\to\infty} G^k = 0$，这等价于 $\rho(G)  1$。

除了误差，我们还可以从**残差**（residual） $\mathbf{r}^{(k)} = \mathbf{b} - A\mathbf{x}^{(k)}$ 的角度来表述迭代。将矩阵分裂 $A=M-N$ 代入迭代格式 $M\mathbf{x}^{(k+1)} = (M-A)\mathbf{x}^{(k)} + \mathbf{b}$，我们可以得到一种等价的**[残差校正](@entry_id:754267)**形式  ：

$$
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + M^{-1}(\mathbf{b} - A\mathbf{x}^{(k)}) = \mathbf{x}^{(k)} + M^{-1}\mathbf{r}^{(k)}
$$

这个形式直观地表达了迭代过程：在当前解 $\mathbf{x}^{(k)}$ 的基础上，加上一个由当前残差 $\mathbf{r}^{(k)}$ 决定的校正量。

残差本身也遵循一个线性的传播规律。从其定义出发，我们可以推导出**残差传播关系** ：
$$
\mathbf{r}^{(k+1)} = \mathbf{b} - A\mathbf{x}^{(k+1)} = \mathbf{b} - A(\mathbf{x}^{(k)} + M^{-1}\mathbf{r}^{(k)}) = (\mathbf{b} - A\mathbf{x}^{(k)}) - AM^{-1}\mathbf{r}^{(k)}
$$
$$
\mathbf{r}^{(k+1)} = (I - AM^{-1})\mathbf{r}^{(k)}
$$
残差的[迭代矩阵](@entry_id:637346)是 $G_r = I - AM^{-1}$。值得注意的是，误差的[迭代矩阵](@entry_id:637346) $G_e = M^{-1}N = M^{-1}(M-A) = I - M^{-1}A$ 和残差的[迭代矩阵](@entry_id:637346) $G_r$ 拥有相同的谱半径，因为对于任何方阵 $X$ 和 $Y$，$\rho(XY)=\rho(YX)$。因此，无论是分析误差还是残差，收敛的判据和速率都是一致的。

### 经典分裂方法

不同的矩阵分裂方式 $A=M-N$ 定义了不同的迭代法。最经典的三种方法均源于将矩阵 $A$ 分解为其对角部分 $D$、严格下三角部分 $-L$ 和严格上三角部分 $-U$ 的和：$A=D-L-U$。

#### [雅可比](@entry_id:264467)（Jacobi）方法

[雅可比方法](@entry_id:270947)采用最简单的分裂方式，取 $M$ 为 $A$ 的对角部分，即 $M=D$。这种选择使得 $M^{-1}$ 的计算极为简便，因为它只是一个[对角矩阵](@entry_id:637782)的逆。相应的 $N$ 矩阵为 $N = D-A = L+U$。

- **分裂矩阵**: $M_J = D$
- **迭代格式**: $D\mathbf{x}^{(k+1)} = (L+U)\mathbf{x}^{(k)} + \mathbf{b}$
- **[迭代矩阵](@entry_id:637346)**: $B_J = D^{-1}(L+U) = I - D^{-1}A$ 

从分量的角度看，第 $i$ 个未知数在第 $k+1$ 次迭代的值为：
$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)
$$
这个表达式清晰地表明，[雅可比迭代](@entry_id:139235)在计算所有分量的新值时，完全依赖于上一步迭代的旧值 $\mathbf{x}^{(k)}$。因此，所有分量的更新可以[并行计算](@entry_id:139241)，这使得[雅可比方法](@entry_id:270947)在[并行计算](@entry_id:139241)环境中具有吸[引力](@entry_id:175476)。它也被称为**同时位移法**。

#### 高斯-赛德尔（Gauss-Seidel）方法

高斯-赛德尔方法对[雅可比方法](@entry_id:270947)做了一个看似微小但影响深远的改进。在计算第 $i$ 个分量 $x_i^{(k+1)}$ 时，它会立即使用在同一次迭代中已经计算出的新值 $x_j^{(k+1)}$ (其中 $j  i$)。这种“即算即用”的策略使得信息的传播更快，通常能带来更快的[收敛速度](@entry_id:636873)。

- **分裂矩阵**: $M_{GS} = D-L$
- **迭代格式**: $(D-L)\mathbf{x}^{(k+1)} = U\mathbf{x}^{(k)} + \mathbf{b}$
- **[迭代矩阵](@entry_id:637346)**: $B_{GS} = (D-L)^{-1}U$

从分量的角度看，更新公式为：
$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j  i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right)
$$
由于每个分量的计算都依赖于前一个分量的最新值，高斯-赛德尔方法本质上是串行的。它也被称为**连续位移法**。

#### 逐次超松弛（Successive Over-Relaxation, SOR）方法

SOR方法可以看作是高斯-赛德尔方法的一种加速。它首先计算一个高斯-赛德尔步，然后将当前值与这个新值进行加权平均。这个平均由一个**松弛因子** $\omega$ 控制。

令 $x_i^{GS}$ 表示用高斯-赛德尔方法计算出的第 $i$ 个分量的新值。SOR的更新步骤是：
$$
x_i^{(k+1)} = (1-\omega)x_i^{(k)} + \omega x_i^{GS}
$$
其中 $x_i^{GS} = \frac{1}{a_{ii}} \left( b_i - \sum_{j  i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right)$。
当 $\omega=1$ 时，SOR方法退化为高斯-赛德尔方法。当 $\omega \in (0,1)$ 时，该方法被称为**[欠松弛](@entry_id:756302)**；当 $\omega \in (1,2)$ 时，被称为**超松弛**。对于许多重要问题（如由泊松方程产生的系统），选择一个最优的 $\omega_{opt} > 1$ 可以极大地加速收敛。

- **分裂矩阵**: $M_\omega = \frac{1}{\omega}(D-\omega L)$
- **迭代格式**: $\frac{1}{\omega}(D-\omega L)\mathbf{x}^{(k+1)} = \left(\frac{1-\omega}{\omega}D + U\right)\mathbf{x}^{(k)} + \mathbf{b}$
- **[迭代矩阵](@entry_id:637346)**: $B_\omega = (D-\omega L)^{-1}((1-\omega)D + \omega U)$

对于[对称正定](@entry_id:145886)（SPD）矩阵 $A$，SOR方法对所有 $\omega \in (0,2)$ 都收敛。寻找最优的 $\omega$ 是SOR方法应用中的一个核心问题。