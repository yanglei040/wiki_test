{
    "hands_on_practices": [
        {
            "introduction": "Classical iterative methods are not just solvers in their own right; they are also critical components of modern, highly efficient techniques like multigrid, where they act as 'smoothers.' This exercise will give you hands-on experience with Local Fourier Analysis (LFA), a powerful tool for understanding this smoothing behavior. By analyzing the weighted Jacobi method for the model Poisson problem, you will determine the optimal relaxation parameter that most effectively damps high-frequency error components, a foundational skill for designing and analyzing advanced numerical solvers.",
            "id": "3455516",
            "problem": "Consider the finite-difference discretization of the two-dimensional ($2$-dimensional) Poisson equation $-\\Delta u = f$ on a uniform $N \\times N$ periodic grid with mesh spacing $h$, using the standard $5$-point stencil. The resulting linear operator $A$ is circulant and diagonalized by discrete Fourier modes. Its symbol is given by $a(\\theta_1,\\theta_2) = 4 - 2\\cos\\theta_1 - 2\\cos\\theta_2$, and the diagonal of $A$ is $D = 4$. The weighted Jacobi method with weight $\\omega \\in \\mathbb{R}$ has error-propagation operator $E = I - \\omega D^{-1}A$, whose symbol on a Fourier mode with frequency $(\\theta_1,\\theta_2)$ is $S(\\theta_1,\\theta_2) = 1 - \\omega\\,a(\\theta_1,\\theta_2)/4$.\n\nUsing Local Fourier Analysis (LFA), define the high-frequency set $\\mathcal{H}$ relative to standard $2h$-coarsening as those modes with both components high:\n$$\n\\mathcal{H} = \\left\\{ (\\theta_1,\\theta_2) : |\\theta_1| \\in \\left[\\frac{\\pi}{2},\\pi\\right],\\;|\\theta_2| \\in \\left[\\frac{\\pi}{2},\\pi\\right] \\right\\}.\n$$\nThe weighted Jacobi smoothing factor is the worst-case error-amplification over $\\mathcal{H}$:\n$$\n\\mu(\\omega) = \\max_{(\\theta_1,\\theta_2) \\in \\mathcal{H}} \\left| 1 - \\omega\\,\\frac{a(\\theta_1,\\theta_2)}{4} \\right|.\n$$\n\nStarting from first principles of the symbol calculus for circulant operators and the definition of the weighted Jacobi error-propagation symbol, compute $\\mu(\\omega)$ explicitly as a function of $\\omega$ by characterizing the range of $a(\\theta_1,\\theta_2)/4$ over $\\mathcal{H}$. Then, determine the value of $\\omega$ that minimizes $\\mu(\\omega)$ over $\\omega  0$. State your final answer as the minimizing weight $\\omega$.",
            "solution": "The objective is to determine the optimal relaxation parameter $\\omega  0$ for the weighted Jacobi method when used as a smoother for the $2$-dimensional Poisson equation. The optimal parameter, denoted $\\omega_{\\text{opt}}$, is the one that minimizes the smoothing factor $\\mu(\\omega)$.\n\nThe problem provides the necessary definitions from Local Fourier Analysis (LFA). The error-propagation operator for the weighted Jacobi method is $E = I - \\omega D^{-1}A$. Its symbol, which acts as the amplification factor for a Fourier mode with frequency $(\\theta_1, \\theta_2)$, is given by $S(\\theta_1, \\theta_2) = 1 - \\omega\\frac{a(\\theta_1,\\theta_2)}{4}$.\nThe symbol of the discrete operator $A$ is $a(\\theta_1, \\theta_2) = 4 - 2\\cos\\theta_1 - 2\\cos\\theta_2$.\nLet's define the scaled symbol of the operator $A$ as $\\lambda(\\theta_1, \\theta_2) = \\frac{a(\\theta_1, \\theta_2)}{4}$. Substituting the expression for $a(\\theta_1, \\theta_2)$, we get:\n$$\n\\lambda(\\theta_1, \\theta_2) = \\frac{4 - 2\\cos\\theta_1 - 2\\cos\\theta_2}{4} = 1 - \\frac{1}{2}(\\cos\\theta_1 + \\cos\\theta_2)\n$$\nThe error amplification factor can then be written as $S(\\theta_1, \\theta_2) = 1 - \\omega\\lambda(\\theta_1, \\theta_2)$.\n\nThe smoothing factor $\\mu(\\omega)$ is defined as the maximum amplification factor over the set of high frequencies $\\mathcal{H}$:\n$$\n\\mu(\\omega) = \\max_{(\\theta_1, \\theta_2) \\in \\mathcal{H}} |S(\\theta_1, \\theta_2)| = \\max_{(\\theta_1, \\theta_2) \\in \\mathcal{H}} |1 - \\omega\\lambda(\\theta_1, \\theta_2)|\n$$\nThe high-frequency set is given by:\n$$\n\\mathcal{H} = \\left\\{ (\\theta_1,\\theta_2) : |\\theta_1| \\in \\left[\\frac{\\pi}{2},\\pi\\right],\\;|\\theta_2| \\in \\left[\\frac{\\pi}{2},\\pi\\right] \\right\\}\n$$\nTo evaluate $\\mu(\\omega)$, we must first find the range of values that $\\lambda(\\theta_1, \\theta_2)$ takes for all $(\\theta_1, \\theta_2) \\in \\mathcal{H}$. Let this range be denoted by $\\Lambda_{\\mathcal{H}}$.\nThe value of $\\lambda(\\theta_1, \\theta_2)$ depends on the sum $\\cos\\theta_1 + \\cos\\theta_2$. Since $\\cos(\\theta)$ is an even function, the values of $\\lambda(\\theta_1, \\theta_2)$ are identical in the four symmetric regions that constitute $\\mathcal{H}$. We can therefore confine our analysis to the region where $\\theta_1 \\in [\\frac{\\pi}{2}, \\pi]$ and $\\theta_2 \\in [\\frac{\\pi}{2}, \\pi]$.\nIn the interval $[\\frac{\\pi}{2}, \\pi]$, the function $\\cos(\\theta)$ is monotonically decreasing, ranging from $\\cos(\\frac{\\pi}{2}) = 0$ to $\\cos(\\pi) = -1$.\nThe minimum value of the sum $\\cos\\theta_1 + \\cos\\theta_2$ over this region occurs when both $\\theta_1$ and $\\theta_2$ are $\\pi$, yielding $\\cos(\\pi) + \\cos(\\pi) = -1 - 1 = -2$.\nThe maximum value occurs when both $\\theta_1$ and $\\theta_2$ are $\\frac{\\pi}{2}$, yielding $\\cos(\\frac{\\pi}{2}) + \\cos(\\frac{\\pi}{2}) = 0 + 0 = 0$.\nThus, for $(\\theta_1, \\theta_2) \\in \\mathcal{H}$, the range of $\\cos\\theta_1 + \\cos\\theta_2$ is $[-2, 0]$.\nUsing this, we can determine the range of $\\lambda(\\theta_1, \\theta_2) = 1 - \\frac{1}{2}(\\cos\\theta_1 + \\cos\\theta_2)$.\nThe minimum value of $\\lambda$ corresponds to the maximum of $\\cos\\theta_1 + \\cos\\theta_2$:\n$$\n\\lambda_{\\min} = 1 - \\frac{1}{2}(0) = 1\n$$\nThis occurs for modes where $|\\theta_1|=|\\theta_2|=\\frac{\\pi}{2}$.\nThe maximum value of $\\lambda$ corresponds to the minimum of $\\cos\\theta_1 + \\cos\\theta_2$:\n$$\n\\lambda_{\\max} = 1 - \\frac{1}{2}(-2) = 1 + 1 = 2\n$$\nThis occurs for the mode where $|\\theta_1|=|\\theta_2|=\\pi$.\nTherefore, the set of values $\\Lambda_{\\mathcal{H}}$ is the closed interval $[1, 2]$.\n\nThe expression for the smoothing factor becomes a maximization problem over this interval:\n$$\n\\mu(\\omega) = \\max_{\\lambda \\in [1, 2]} |1 - \\omega \\lambda|\n$$\nFor a fixed $\\omega$, the function $f(\\lambda) = 1 - \\omega\\lambda$ is linear in $\\lambda$. The maximum absolute value of a linear function over a closed interval must be achieved at one of the endpoints of the interval. So, we evaluate $|f(\\lambda)|$ at $\\lambda=1$ and $\\lambda=2$:\n$$\n\\mu(\\omega) = \\max\\left\\{|1 - \\omega \\cdot 1|, |1 - \\omega \\cdot 2|\\right\\} = \\max\\left\\{|1 - \\omega|, |1 - 2\\omega|\\right\\}\n$$\nTo find the optimal weight $\\omega_{\\text{opt}}$, we must find the value of $\\omega  0$ that minimizes $\\mu(\\omega)$. This is a classic minimax problem. The minimum value of $\\max\\{|f_1(\\omega)|, |f_2(\\omega)|\\}$ is typically found at a point where $|f_1(\\omega)| = |f_2(\\omega)|$. We set the arguments of the maximum function to be equal in magnitude:\n$$\n|1 - \\omega| = |1 - 2\\omega|\n$$\nThis equation yields two possibilities:\n1. $1 - \\omega = 1 - 2\\omega$, which simplifies to $\\omega = 0$. This is not a valid solution as the problem specifies $\\omega  0$.\n2. $1 - \\omega = -(1 - 2\\omega)$, which simplifies to $1 - \\omega = 2\\omega - 1$. Rearranging gives $3\\omega = 2$, so $\\omega = \\frac{2}{3}$.\n\nTo confirm that $\\omega = \\frac{2}{3}$ is indeed the minimizer, we can analyze the behavior of $\\mu(\\omega)$. The function $\\mu(\\omega)$ is composed of piecewise functions. The critical points for $\\omega$ are $0$, $\\frac{1}{2}$, and $1$, where the signs inside the absolute values change.\n- For $\\omega \\in (0, \\frac{2}{3})$, $\\mu(\\omega)$ is a decreasing function of $\\omega$. For instance, in $(0, \\frac{1}{2}]$, $\\mu(\\omega) = 1-\\omega$. In $(\\frac{1}{2}, \\frac{2}{3})$, $\\mu(\\omega)=1-\\omega$ as well.\n- For $\\omega \\in (\\frac{2}{3}, \\infty)$, $\\mu(\\omega)$ is an increasing function of $\\omega$. For instance, in $(\\frac{2}{3}, 1)$, $\\mu(\\omega) = 2\\omega-1$. For $\\omega \\ge 1$, $\\mu(\\omega)=2\\omega-1$ as well.\n\nSince $\\mu(\\omega)$ is decreasing for $\\omega  \\frac{2}{3}$ and increasing for $\\omega  \\frac{2}{3}$, the global minimum for $\\omega0$ occurs precisely at $\\omega = \\frac{2}{3}$.\nThe minimal smoothing factor is $\\mu(\\frac{2}{3}) = |1 - \\frac{2}{3}| = \\frac{1}{3}$.\nThe problem asks for the value of $\\omega$ that minimizes $\\mu(\\omega)$. This optimal value is $\\frac{2}{3}$.",
            "answer": "$$\n\\boxed{\\frac{2}{3}}\n$$"
        },
        {
            "introduction": "While textbook examples often feature well-behaved, nonsingular matrices, real-world physical problems frequently lead to algebraic complications. This practice explores a canonical example: the singular linear system arising from the Poisson equation with pure Neumann boundary conditions. You will analyze several strategies for modifying standard iterative schemes to enforce a unique solution, gaining crucial insight into handling null spaces and ensuring your solver converges to the physically correct, minimum-norm solution.",
            "id": "3455549",
            "problem": "Consider the Poisson equation with homogeneous Neumann boundary conditions on a rectangular domain, discretized by the standard $5$-point second-order finite difference scheme on a uniform grid with $N$ interior unknowns collected in a vector $\\mathbf{x} \\in \\mathbb{R}^N$. The resulting linear system is\n$$\nA \\mathbf{x} = \\mathbf{b},\n$$\nwhere $A \\in \\mathbb{R}^{N \\times N}$ is the discrete Neumann Laplacian, which is symmetric positive semidefinite with $\\operatorname{null}(A) = \\operatorname{span}\\{\\mathbf{1}\\}$, where $\\mathbf{1} \\in \\mathbb{R}^N$ is the vector of all ones. Assume the compatibility condition $\\mathbf{1}^T \\mathbf{b} = 0$ holds, so that the system is consistent. Define the minimum-norm solution as the unique solution $\\mathbf{x}^\\star$ of $A \\mathbf{x} = \\mathbf{b}$ that minimizes $\\|\\mathbf{x}\\|_2$ over all solutions.\n\nConsider stationary iterations of the form\n$$\n\\mathbf{x}^{k+1} = \\mathbf{x}^k + M^{-1}(\\mathbf{b} - A \\mathbf{x}^k),\n$$\nwhere $M$ is the iteration preconditioner corresponding to the Jacobi method ($M = D$, the diagonal of $A$), the Gauss–Seidel method ($M = D - L$, with $A = D - L - U$), or the Successive Over-Relaxation method ($M = \\frac{1}{\\omega}(D - \\omega L)$ with relaxation parameter $\\omega \\in (0,2)$). Because $A$ is singular, the constant mode $\\mathbf{1}$ appears as a neutral error mode in the unmodified iteration, which can prevent convergence to a unique solution.\n\nLet $P := I - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^T$ denote the orthogonal projector onto the mean-zero subspace $\\{\\mathbf{v} \\in \\mathbb{R}^N : \\mathbf{1}^T \\mathbf{v} = 0\\}$. You may assume that on the mean-zero subspace, the restriction of $A$ is symmetric positive definite and that standard convergence results for Jacobi, Gauss–Seidel, and Successive Over-Relaxation apply to this restriction (e.g., for $\\omega \\in (0,2)$ in Successive Over-Relaxation).\n\nWhich of the following modifications, applied at every iteration and with an initial guess $\\mathbf{x}^0$ satisfying $\\mathbf{1}^T \\mathbf{x}^0 = 0$, guarantee convergence of the modified Jacobi, Gauss–Seidel, or Successive Over-Relaxation iteration to the minimum-norm solution $\\mathbf{x}^\\star$?\n\nA. Enforce mean-zero corrections by projecting the computed correction onto the mean-zero subspace before updating:\n$$\n\\mathbf{x}^{k+1} = \\mathbf{x}^k + P\\,M^{-1}(\\mathbf{b} - A \\mathbf{x}^k).\n$$\n\nB. Enforce a mean-zero iterate by subtracting its average after each update:\n$$\n\\mathbf{x}^{k+1} \\leftarrow \\mathbf{x}^{k+1} - \\frac{1}{N}(\\mathbf{1}^T \\mathbf{x}^{k+1})\\,\\mathbf{1}.\n$$\n\nC. Regularize the operator by adding a positive multiple of the identity and then apply the standard iteration without projection:\n$$\n(A + \\varepsilon I)\\,\\mathbf{x} = \\mathbf{b}, \\quad \\varepsilon  0 \\text{ fixed and small},\n$$\nupdated by $\\mathbf{x}^{k+1} = \\mathbf{x}^k + M^{-1}(\\mathbf{b} - (A+\\varepsilon I)\\mathbf{x}^k)$.\n\nD. Pin a single grid value by enforcing $x_{i_0} = 0$ at one node index $i_0$ to remove the nullspace and then apply the standard iteration to the reduced nonsingular system.\n\nE. Project only the residual onto the mean-zero subspace before forming the correction:\n$$\n\\mathbf{x}^{k+1} = \\mathbf{x}^k + M^{-1} P (\\mathbf{b} - A \\mathbf{x}^k).\n$$\n\nSelect all that apply. Your reasoning should start from the discrete operator properties and the definition of the minimum-norm solution and must justify whether and why each modification does or does not ensure convergence to $\\mathbf{x}^\\star$ for Jacobi, Gauss–Seidel, and Successive Over-Relaxation under the stated assumptions.",
            "solution": "### Step 1: Extract Givens\nThe problem provides the following information:\n1.  **Equation:** A linear system $A \\mathbf{x} = \\mathbf{b}$ resulting from a finite difference discretization of the Poisson equation with homogeneous Neumann boundary conditions on a rectangular domain.\n2.  **Matrix Properties:** $A \\in \\mathbb{R}^{N \\times N}$ is the discrete Neumann Laplacian. It is symmetric positive semidefinite.\n3.  **Nullspace of A:** $\\operatorname{null}(A) = \\operatorname{span}\\{\\mathbf{1}\\}$, where $\\mathbf{1} \\in \\mathbb{R}^N$ is the vector of all ones.\n4.  **Compatibility Condition:** The system is consistent, satisfying $\\mathbf{1}^T \\mathbf{b} = 0$.\n5.  **Target Solution:** The minimum-norm solution $\\mathbf{x}^\\star$, defined as the unique solution of $A \\mathbf{x} = \\mathbf{b}$ that minimizes $\\|\\mathbf{x}\\|_2$. This implies $\\mathbf{x}^\\star$ is orthogonal to $\\operatorname{null}(A)$, i.e., $\\mathbf{1}^T \\mathbf{x}^\\star = 0$.\n6.  **Stationary Iteration Form:** $\\mathbf{x}^{k+1} = \\mathbf{x}^k + M^{-1}(\\mathbf{b} - A \\mathbf{x}^k)$.\n7.  **Methods:** The preconditioner $M$ corresponds to Jacobi ($M=D$), Gauss-Seidel ($M=D-L$), or Successive Over-Relaxation ($M=\\frac{1}{\\omega}(D-\\omega L)$ with $\\omega \\in (0,2)$), where $A = D-L-U$.\n8.  **Projector:** $P := I - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^T$ is the orthogonal projector onto the mean-zero subspace $\\{\\mathbf{v} \\in \\mathbb{R}^N : \\mathbf{1}^T \\mathbf{v} = 0\\}$.\n9.  **Convergence Assumption:** On the mean-zero subspace, the restriction of $A$ is symmetric positive definite, and standard convergence results for Jacobi, Gauss-Seidel, and SOR (with $\\omega \\in (0,2)$) apply. This implies that the spectral radius of the standard iteration matrix $G=I-M^{-1}A$, when its action is projected onto the mean-zero subspace, is less than $1$.\n10. **Initial Condition:** The initial guess is mean-zero: $\\mathbf{1}^T \\mathbf{x}^0 = 0$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes a classic and well-understood topic in numerical linear algebra: the solution of singular linear systems arising from PDEs with pure Neumann conditions.\n\n-   **Scientifically Grounded:** The setup is based on standard, correct principles of numerical analysis and linear algebra. The properties of the discrete Neumann Laplacian, the compatibility condition, and the nature of the solution space are all accurately described.\n-   **Well-Posed:** The question is precise. It asks which of the given modifications to a standard iterative scheme guarantees convergence to a well-defined target solution, the minimum-norm solution $\\mathbf{x}^\\star$. The assumptions provided are sufficient to answer the question.\n-   **Objective:** The problem is stated in formal mathematical language, free from ambiguity or subjective content.\n-   **Completeness:** The problem is self-contained. All necessary definitions (e.g., $A$, $\\mathbf{x}^\\star$, $M$, $P$) and assumptions (compatibility, convergence on the subspace) are provided.\n-   **Consistency:** The provided information is internally consistent. For example, the singularity of $A$ and the corresponding compatibility condition $\\mathbf{1}^T\\mathbf{b}=0$ are correctly paired.\n\nThe problem is a valid, standard exercise in numerical analysis.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution derivation.\n\n### Derivation\nThe goal is to find an iterative method that converges to the minimum-norm solution $\\mathbf{x}^\\star$ of the consistent singular system $A\\mathbf{x} = \\mathbf{b}$. This solution is unique and characterized by being a solution ($A\\mathbf{x}^\\star = \\mathbf{b}$) and being mean-zero ($\\mathbf{1}^T \\mathbf{x}^\\star = 0$).\n\nThe standard stationary iteration is $\\mathbf{x}^{k+1} = (I - M^{-1}A)\\mathbf{x}^k + M^{-1}\\mathbf{b}$. The iteration matrix is $G = I - M^{-1}A$. Since $A\\mathbf{1} = \\mathbf{0}$, we have $G\\mathbf{1} = (I - M^{-1}A)\\mathbf{1} = \\mathbf{1} - M^{-1}\\mathbf{0} = \\mathbf{1}$. Thus, $1$ is an eigenvalue of $G$ with eigenvector $\\mathbf{1}$. This means the error component in the direction of $\\mathbf{1}$ does not decay, preventing convergence to a unique solution. The proposed modifications aim to remedy this. We analyze each one.\n\nWe are given $\\mathbf{1}^T \\mathbf{x}^0 = 0$. We must check if a given modification ensures that the iterates $\\mathbf{x}^k$ converge to $\\mathbf{x}^\\star$. This requires two conditions to be met for the sequence $\\{\\mathbf{x}^k\\}$:\n1.  The sequence converges to a limit, say $\\mathbf{x}_\\infty$.\n2.  The limit is the minimum norm solution, i.e., $\\mathbf{x}_\\infty = \\mathbf{x}^\\star$. This means $A\\mathbf{x}_\\infty = \\mathbf{b}$ and $\\mathbf{1}^T\\mathbf{x}_\\infty = 0$.\n\nLet's analyze each option.\n\n**A. Enforce mean-zero corrections by projecting the computed correction onto the mean-zero subspace before updating:**\nThe iteration is $\\mathbf{x}^{k+1} = \\mathbf{x}^k + P\\,M^{-1}(\\mathbf{b} - A \\mathbf{x}^k)$.\nFirst, we check if the mean-zero property is maintained. Assume $\\mathbf{1}^T \\mathbf{x}^k = 0$. Taking the inner product with $\\mathbf{1}^T$:\n$$ \\mathbf{1}^T \\mathbf{x}^{k+1} = \\mathbf{1}^T \\mathbf{x}^k + \\mathbf{1}^T P (M^{-1}(\\mathbf{b} - A \\mathbf{x}^k)) $$\nBy definition of the projector $P$, the vector $P\\mathbf{v}$ is in the mean-zero subspace for any vector $\\mathbf{v}$, so $\\mathbf{1}^T P \\mathbf{v} = 0$. Thus, $\\mathbf{1}^T\\mathbf{x}^{k+1} = \\mathbf{1}^T\\mathbf{x}^k$. With the initial condition $\\mathbf{1}^T\\mathbf{x}^0 = 0$, induction implies $\\mathbf{1}^T\\mathbf{x}^k = 0$ for all $k \\ge 0$. All iterates remain in the mean-zero subspace.\n\nA fixed point $\\mathbf{x}$ of this iteration must satisfy $P\\,M^{-1}(\\mathbf{b} - A \\mathbf{x}) = \\mathbf{0}$. This means $M^{-1}(\\mathbf{b} - A \\mathbf{x})$ is in the nullspace of $P$, so $M^{-1}(\\mathbf{b} - A \\mathbf{x}) = c\\mathbf{1}$ for some scalar $c$. This implies $\\mathbf{b} - A \\mathbf{x} = cM\\mathbf{1}$. Taking the inner product with $\\mathbf{1}^T$:\n$$ \\mathbf{1}^T(\\mathbf{b} - A\\mathbf{x}) = c \\mathbf{1}^T M \\mathbf{1} $$\nGiven $\\mathbf{1}^T \\mathbf{b} = 0$, and since $A$ is symmetric, $\\mathbf{1}^T A = (A^T \\mathbf{1})^T = (A \\mathbf{1})^T = \\mathbf{0}^T$, so $\\mathbf{1}^T A\\mathbf{x} = 0$. The left side is $0$. For the preconditioners $M$ considered (Jacobi, GS, SOR on this class of problems), $M$ is such that $\\mathbf{1}^T M \\mathbf{1} \\ne 0$. Thus, $c=0$, which implies $\\mathbf{b} - A \\mathbf{x} = \\mathbf{0}$. The fixed point $\\mathbf{x}$ is a solution to $A\\mathbf{x}=\\mathbf{b}$. Since all iterates are mean-zero, the limit (the fixed point) must also be mean-zero. A mean-zero solution is precisely $\\mathbf{x}^\\star$.\n\nFor convergence, consider the error $\\mathbf{e}^k = \\mathbf{x}^k - \\mathbf{x}^\\star$. Since both $\\mathbf{x}^k$ and $\\mathbf{x}^\\star$ are mean-zero, so is $\\mathbf{e}^k$. The error update is:\n$$ \\mathbf{e}^{k+1} = \\mathbf{x}^{k+1} - \\mathbf{x}^\\star = \\mathbf{x}^k + P M^{-1}(\\mathbf{b} - A\\mathbf{x}^k) - \\mathbf{x}^\\star = \\mathbf{e}^k - P M^{-1}A(\\mathbf{x}^k - \\mathbf{x}^\\star) = (I - P M^{-1}A)\\mathbf{e}^k $$\nThe iteration matrix for the error is $G_A = I - P M^{-1}A$. Since $\\mathbf{e}^k$ is in the mean-zero space, let's analyze the action of $G_A$ on this subspace. For any $\\mathbf{v} \\in \\operatorname{ran}(P)$, $G_A\\mathbf{v} = \\mathbf{v} - PM^{-1}A\\mathbf{v} = P(\\mathbf{v} - M^{-1}A\\mathbf{v}) = P(G\\mathbf{v})$. The assumption that standard convergence results apply means that the operator $P \\circ G$ restricted to $\\operatorname{ran}(P)$ is a contraction. Since the error $\\mathbf{e}^k$ is confined to this subspace and evolves according to this contractive operator, $\\mathbf{e}^k \\to \\mathbf{0}$. Therefore, $\\mathbf{x}^k \\to \\mathbf{x}^\\star$.\nVerdict: **Correct**\n\n**B. Enforce a mean-zero iterate by subtracting its average after each update:**\nThe process is: $\\tilde{\\mathbf{x}}^{k+1} = \\mathbf{x}^k + M^{-1}(\\mathbf{b} - A \\mathbf{x}^k)$, followed by $\\mathbf{x}^{k+1} = P \\tilde{\\mathbf{x}}^{k+1}$.\nBy construction, $\\mathbf{x}^{k+1}$ is in the range of $P$, so it is guaranteed to be mean-zero for all $k \\ge 1$. Given $\\mathbf{1}^T\\mathbf{x}^0=0$, all iterates $\\mathbf{x}^k$ are mean-zero.\nLet's write the one-step update: $\\mathbf{x}^{k+1} = P(\\mathbf{x}^k + M^{-1}(\\mathbf{b} - A\\mathbf{x}^k))$.\nSince $\\mathbf{x}^k$ is mean-zero, $P\\mathbf{x}^k = \\mathbf{x}^k$.\n$$ \\mathbf{x}^{k+1} = P\\mathbf{x}^k + P(M^{-1}(\\mathbf{b} - A\\mathbf{x}^k)) = \\mathbf{x}^k + P(M^{-1}(\\mathbf{b} - A\\mathbf{x}^k)) $$\nThis is the exact same update rule as in option A. Since we start with the same mean-zero $\\mathbf{x}^0$, both methods generate an identical sequence of iterates $\\{\\mathbf{x}^k\\}$. Therefore, the analysis and conclusion are the same as for option A.\nVerdict: **Correct**\n\n**C. Regularize the operator by adding a positive multiple of the identity:**\nThe iteration is designed to solve $(A + \\varepsilon I)\\mathbf{x} = \\mathbf{b}$ for a fixed $\\varepsilon  0$. The matrix $A_\\varepsilon = A + \\varepsilon I$ is symmetric positive definite, so it has a unique solution $\\mathbf{x}_\\varepsilon = (A + \\varepsilon I)^{-1}\\mathbf{b}$. Assuming the iteration converges, it will converge to $\\mathbf{x}_\\varepsilon$. The question is whether $\\mathbf{x}_\\varepsilon = \\mathbf{x}^\\star$.\nThe target solution is $\\mathbf{x}^\\star = A^\\dagger\\mathbf{b}$, where $A^\\dagger$ is the Moore-Penrose pseudoinverse of $A$. In general, for a fixed $\\varepsilon  0$, $(A + \\varepsilon I)^{-1} \\neq A^\\dagger$. For example, let $\\mathbf{x}^\\star \\neq \\mathbf{0}$. We know $A\\mathbf{x}^\\star = \\mathbf{b}$.\nThen $\\mathbf{x}_\\varepsilon = (A + \\varepsilon I)^{-1}A\\mathbf{x}^\\star$. If $\\mathbf{x}_\\varepsilon = \\mathbf{x}^\\star$, then $(A + \\varepsilon I)\\mathbf{x}^\\star = A\\mathbf{x}^\\star$, which simplifies to $\\varepsilon I \\mathbf{x}^\\star = \\mathbf{0}$. Since $\\varepsilon  0$, this requires $\\mathbf{x}^\\star = \\mathbf{0}$, which only happens for the trivial case $\\mathbf{b}=\\mathbf{0}$. For any non-trivial problem, the regularized solution is not the minimum-norm solution. It is an approximation that approaches $\\mathbf{x}^\\star$ only in the limit $\\varepsilon \\to 0$. Since $\\varepsilon$ is fixed, this method does not converge to $\\mathbf{x}^\\star$.\nVerdict: **Incorrect**\n\n**D. Pin a single grid value by enforcing $x_{i_0}=0$:**\nThis modification removes the nullspace by adding the constraint $x_{i_0}=0$. The resulting linear system is nonsingular and has a unique solution, let's call it $\\mathbf{x}_{\\text{pin}}$. The problem's iterative methods will converge to $\\mathbf{x}_{\\text{pin}}$.\nThe general solution to $A\\mathbf{x}=\\mathbf{b}$ is of the form $\\mathbf{x}^\\star + c\\mathbf{1}$ for any scalar $c$. The pinning condition determines $c$:\n$$ (\\mathbf{x}_{\\text{pin}})_{i_0} = (\\mathbf{x}^\\star)_{i_0} + c = 0 \\implies c = -(\\mathbf{x}^\\star)_{i_0} $$\nSo the solution found is $\\mathbf{x}_{\\text{pin}} = \\mathbf{x}^\\star - (\\mathbf{x}^\\star)_{i_0}\\mathbf{1}$.\nThis solution is equal to the minimum-norm solution $\\mathbf{x}^\\star$ only if $(\\mathbf{x}^\\star)_{i_0}\\mathbf{1} = \\mathbf{0}$, which means $(\\mathbf{x}^\\star)_{i_0}$ must be $0$. This is not true in general. The mean of $\\mathbf{x}_{\\text{pin}}$ is $\\mathbf{1}^T\\mathbf{x}_{\\text{pin}}/N = (0 - (\\mathbf{x}^\\star)_{i_0} \\mathbf{1}^T\\mathbf{1})/N = -(\\mathbf{x}^\\star)_{i_0}$, which is generally non-zero. Thus, $\\mathbf{x}_{\\text{pin}}$ is not the minimum-norm solution.\nVerdict: **Incorrect**\n\n**E. Project only the residual onto the mean-zero subspace:**\nThe iteration is $\\mathbf{x}^{k+1} = \\mathbf{x}^k + M^{-1} P (\\mathbf{b} - A \\mathbf{x}^k)$.\nLet's analyze the argument of the projection, the residual $\\mathbf{r}^k = \\mathbf{b} - A\\mathbf{x}^k$. Let's check its mean:\n$$ \\mathbf{1}^T \\mathbf{r}^k = \\mathbf{1}^T \\mathbf{b} - \\mathbf{1}^T A \\mathbf{x}^k $$\nWe are given the compatibility condition $\\mathbf{1}^T \\mathbf{b} = 0$. Using the symmetry of $A$, we have $\\mathbf{1}^T A = (A^T \\mathbf{1})^T = (A \\mathbf{1})^T$. Since $\\mathbf{1}$ spans the nullspace of $A$, $A\\mathbf{1} = \\mathbf{0}$. Therefore, $\\mathbf{1}^T A = \\mathbf{0}^T$, and $\\mathbf{1}^T A \\mathbf{x}^k = 0$ for any $\\mathbf{x}^k$.\nSo, $\\mathbf{1}^T \\mathbf{r}^k = 0 - 0 = 0$. The residual vector $\\mathbf{r}^k$ is always in the mean-zero subspace, for any iterate $\\mathbf{x}^k$.\nFor any vector $\\mathbf{v}$ in the mean-zero subspace, $P\\mathbf{v} = \\mathbf{v}$. Since $\\mathbf{r}^k$ is always mean-zero, we have $P\\mathbf{r}^k = \\mathbf{r}^k$.\nThe modified iteration is therefore:\n$$ \\mathbf{x}^{k+1} = \\mathbf{x}^k + M^{-1}(\\mathbf{b} - A \\mathbf{x}^k) $$\nThis is exactly the original, unmodified stationary iteration. As stated in the problem, this iteration has an eigenvalue of $1$ corresponding to the eigenvector $\\mathbf{1}$ and does not guarantee convergence to the unique minimum-norm solution. The mean of the iterates can drift.\nVerdict: **Incorrect**",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "The true test of a numerical method lies in its performance on problems with complex physics and geometry. This advanced coding exercise challenges you to solve a diffusion problem with large, discontinuous coefficient jumps across a curved interface, a scenario common in fields like porous media flow or composite materials. By implementing and comparing different update orderings for Gauss-Seidel and SOR, you will discover firsthand how tailoring the algorithm to the problem's physical structure is key to achieving efficient error smoothing.",
            "id": "3455542",
            "problem": "Consider the diffusion model with variable and discontinuous conductivity defined by the elliptic partial differential equation $-\\nabla \\cdot (k(x,y)\\nabla u(x,y)) = 0$ on the square domain $\\Omega = (0,1)\\times(0,1)$, subject to homogeneous Dirichlet boundary conditions $u(x,y) = 0$ for $(x,y) \\in \\partial\\Omega$. The conductivity $k(x,y)$ is piecewise constant with a curved discontinuity interface defined by a sinusoidally perturbed circle. Let the interface be given in polar coordinates relative to the center $(x_c,y_c) = (1/2,1/2)$ by $r(\\theta) = r_0 + \\varepsilon \\sin(m\\theta)$, where $\\theta(x,y) = \\operatorname{atan2}(y-y_c, x-x_c)$, $\\rho(x,y) = \\sqrt{(x-x_c)^2 + (y-y_c)^2}$, and $r_0 \\in (0,1/2)$, $\\varepsilon \\ge 0$, $m \\in \\mathbb{N}$. Define the signed distance field $\\varphi(x,y) = \\rho(x,y) - r(\\theta)$, and the conductivity as\n$$\nk(x,y) = \n\\begin{cases}\nk_{\\mathrm{in}},  \\text{if } \\varphi(x,y) lt; 0, \\\\\nk_{\\mathrm{out}},  \\text{if } \\varphi(x,y) \\ge 0,\n\\end{cases}\n$$\nwith constants $k_{\\mathrm{in}}  0$, $k_{\\mathrm{out}}  0$.\n\nDiscretize the domain $\\Omega$ using a uniform cell-centered grid with $N\\times N$ interior unknowns and mesh spacing $h = 1/(N+1)$. For each interior cell $(i,j)$, approximate the operator $-\\nabla \\cdot (k \\nabla u)$ by a five-point finite-volume stencil with harmonic averaging of conductivity on faces to ensure flux continuity across discontinuities. Specifically, define face conductivities\n$$\nk_{i+1/2,j} = \\frac{2\\,k_{i,j}\\,k_{i+1,j}}{k_{i,j} + k_{i+1,j}},\\quad\nk_{i-1/2,j} = \\frac{2\\,k_{i,j}\\,k_{i-1,j}}{k_{i,j} + k_{i-1,j}},\n$$\n$$\nk_{i,j+1/2} = \\frac{2\\,k_{i,j}\\,k_{i,j+1}}{k_{i,j} + k_{i,j+1}},\\quad\nk_{i,j-1/2} = \\frac{2\\,k_{i,j}\\,k_{i,j-1}}{k_{i,j} + k_{i,j-1}},\n$$\nwith the convention that a missing neighbor (adjacent to the boundary) uses the interior value, and the Dirichlet boundary values are zero. The resulting discrete operator has coefficients\n$$\na_{i,j}^{\\mathrm{E}} = -\\frac{k_{i+1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{W}} = -\\frac{k_{i-1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{N}} = -\\frac{k_{i,j+1/2}}{h^2},\\quad\na_{i,j}^{\\mathrm{S}} = -\\frac{k_{i,j-1/2}}{h^2},\n$$\n$$\na_{i,j}^{\\mathrm{diag}} = -\\left(a_{i,j}^{\\mathrm{E}} + a_{i,j}^{\\mathrm{W}} + a_{i,j}^{\\mathrm{N}} + a_{i,j}^{\\mathrm{S}}\\right),\n$$\nso that the discrete operator $A$ acting on the grid function $u$ is\n$$\n(Au)_{i,j} = a_{i,j}^{\\mathrm{diag}}\\,u_{i,j}\n+ a_{i,j}^{\\mathrm{E}}\\,u_{i+1,j}\n+ a_{i,j}^{\\mathrm{W}}\\,u_{i-1,j}\n+ a_{i,j}^{\\mathrm{N}}\\,u_{i,j+1}\n+ a_{i,j}^{\\mathrm{S}}\\,u_{i,j-1}.\n$$\n\nThe classical relaxation methods for linear systems associated with partial differential equations are:\n- Jacobi: Using the diagonal part $D$ of $A$, update\n$$\nu^{(n+1)} = u^{(n)} + \\omega_{\\mathrm{J}} D^{-1}\\left(f - A u^{(n)}\\right),\n$$\nwith a scalar weight $0  \\omega_{\\mathrm{J}} \\le 1$ for stability, and here consider $f=0$.\n- Gauss-Seidel: Split $A = L + D + U$ as strict lower, diagonal, and strict upper parts, and update\n$$\n(D+L)\\,u^{(n+1)} = f - U\\,u^{(n)},\n$$\nwith $f=0$. The ordering of unknowns determines the structure of $L$ and $U$.\n- Successive Over-Relaxation (SOR): Apply relaxation with parameter $\\omega_{\\mathrm{SOR}} \\in (0,2)$ to the Gauss-Seidel update by\n$$\nu^{(n+1)} = u^{(n)} + \\omega_{\\mathrm{SOR}}\\left(u^{(n+1)}_{\\mathrm{GS}} - u^{(n)}\\right),\n$$\nwhere $u^{(n+1)}_{\\mathrm{GS}}$ denotes the Gauss-Seidel update at the current node with the chosen ordering.\n\nDefine two orderings:\n- Natural lexicographic ordering: update $(i,j)$ in increasing row-major order (for example, first $j=0$ to $N-1$, and within each row $i=0$ to $N-1$).\n- Interface-aligned ordering: compute the signed distance field $\\varphi_{i,j}$ at grid points and update nodes sorted by ascending $\\varphi_{i,j}$, i.e., sweeping through layers parallel to the interface.\n\nTo quantify “smoothing of interface-parallel modes,” construct an initial error mode localized near the interface and oscillatory along it:\n$$\ne_0(x,y) = \\exp\\!\\left(-\\left(\\frac{\\varphi(x,y)}{\\sigma}\\right)^2\\right)\\,\\sin\\!\\big(q\\,\\theta(x,y)\\big),\n$$\nwhere $\\sigma  0$ controls the localization band width, and $q \\in \\mathbb{N}$ is the angular wavenumber chosen as $q = \\min\\{32, \\lfloor N/4 \\rfloor\\}$. Let $\\mathcal{M} = \\{(x,y) \\in \\Omega : |\\varphi(x,y)| \\le 2\\sigma\\}$ be the measurement band near the interface. For a single relaxation sweep producing $e_1$ from $e_0$, define the smoothing factor\n$$\nS = \\frac{\\|e_1\\|_{2,\\mathcal{M}}}{\\|e_0\\|_{2,\\mathcal{M}}},\\quad\n\\|w\\|_{2,\\mathcal{M}} = \\left(\\sum_{(i,j)\\in \\mathcal{M}} w_{i,j}^2\\right)^{1/2},\n$$\nwith the sum taken over cell centers whose coordinates lie in $\\mathcal{M}$.\n\nImplement the discrete operator and the three relaxation methods (Jacobi, Gauss-Seidel, and Successive Over-Relaxation), and compute the smoothing factor $S$ for the five configurations:\n1. Jacobi with weight $\\omega_{\\mathrm{J}} = 0.66$.\n2. Gauss-Seidel with natural lexicographic ordering (no relaxation, i.e., $\\omega_{\\mathrm{SOR}} = 1$).\n3. Gauss-Seidel with interface-aligned ordering (no relaxation).\n4. Successive Over-Relaxation with $\\omega_{\\mathrm{SOR}} = 1.9$ and natural lexicographic ordering.\n5. Successive Over-Relaxation with $\\omega_{\\mathrm{SOR}} = 1.9$ and interface-aligned ordering.\n\nYour program must compute these five smoothing factors for each of the following test cases and return the results:\n\nTest Suite:\n- Case A (happy path): $N=96$, $r_0=0.35$, $\\varepsilon=0.04$, $m=4$, $k_{\\mathrm{in}}=10$, $k_{\\mathrm{out}}=1$, $\\sigma=0.03$, $\\omega_{\\mathrm{J}}=0.66$, $\\omega_{\\mathrm{SOR}}=1.9$.\n- Case B (high contrast): $N=96$, $r_0=0.35$, $\\varepsilon=0.08$, $m=6$, $k_{\\mathrm{in}}=1000$, $k_{\\mathrm{out}}=1$, $\\sigma=0.03$, $\\omega_{\\mathrm{J}}=0.66$, $\\omega_{\\mathrm{SOR}}=1.9$.\n- Case C (boundary and curvature variation): $N=48$, $r_0=0.35$, $\\varepsilon=0.12$, $m=8$, $k_{\\mathrm{in}}=50$, $k_{\\mathrm{out}}=1$, $\\sigma=0.04$, $\\omega_{\\mathrm{J}}=0.66$, $\\omega_{\\mathrm{SOR}}=1.9$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists of floating-point numbers, in the order of cases A, B, C, and within each case in the order of the five configurations specified above. For example, the output should be of the form\n$$\n\\big[\\,[S_{A,\\mathrm{J}}, S_{A,\\mathrm{GSlex}}, S_{A,\\mathrm{GS\\varphi}}, S_{A,\\mathrm{SORlex}}, S_{A,\\mathrm{SOR\\varphi}}],\\ [S_{B,\\mathrm{J}}, \\dots],\\ [S_{C,\\mathrm{J}}, \\dots]\\,\\big]\n$$\nprinted as a single Python list literal on one line.",
            "solution": "We begin from the elliptic diffusion model $-\\nabla \\cdot (k \\nabla u) = 0$ on $\\Omega = (0,1)^2$ with homogeneous Dirichlet boundary conditions. The conductivity $k(x,y)$ is discontinuous along a curved interface $\\Gamma$ defined by the level set $\\varphi(x,y) = 0$, where the signed distance $\\varphi(x,y) = \\rho(x,y) - r(\\theta)$ uses the center $(x_c,y_c)=(1/2,1/2)$, radial coordinate $\\rho(x,y) = \\sqrt{(x-x_c)^2 + (y-y_c)^2}$, angle $\\theta(x,y) = \\operatorname{atan2}(y-y_c, x-x_c)$, and a perturbed radius $r(\\theta) = r_0 + \\varepsilon \\sin(m\\theta)$. The conductivity is $k_{\\mathrm{in}}$ inside ($\\varphi  0$) and $k_{\\mathrm{out}}$ outside ($\\varphi \\ge 0$).\n\nTo discretize, we adopt a cell-centered grid with $N \\times N$ unknowns $(i,j)$ for $i,j \\in \\{0,\\dots,N-1\\}$. The grid spacing is $h=1/(N+1)$, so the location of the cell center $(i,j)$ is $(x_i, y_j) = ((i+1)h, (j+1)h)$. We approximate the divergence operator in flux form. The normal flux across a vertical face between $(i,j)$ and $(i+1,j)$ is approximated by $-k_{i+1/2,j}\\,(u_{i+1,j} - u_{i,j})/h$, with $k_{i+1/2,j}$ a face conductivity. A well-tested approximation for discontinuous coefficients is the harmonic average across the face,\n$$\nk_{i+1/2,j} = \\frac{2\\,k_{i,j}\\,k_{i+1,j}}{k_{i,j} + k_{i+1,j}},\n$$\nwhich enforces continuity of flux and aligns with the physical interface conditions. Similar formulas apply for all faces. With zero Dirichlet boundary conditions, neighboring values outside the interior grid are zero, and we take the face conductivity at boundary-adjacent faces to equal the interior value to maintain consistency (the neighbor contribution vanishes due to zero boundary value).\n\nSumming the fluxes over the four faces of cell $(i,j)$ and dividing by $h$ yields a five-point stencil for the discrete operator $A$:\n$$\n(Au)_{i,j} =\n\\frac{k_{i+1/2,j}}{h^2}\\,(u_{i,j} - u_{i+1,j})\n+ \\frac{k_{i-1/2,j}}{h^2}\\,(u_{i,j} - u_{i-1,j})\n+ \\frac{k_{i,j+1/2}}{h^2}\\,(u_{i,j} - u_{i,j+1})\n+ \\frac{k_{i,j-1/2}}{h^2}\\,(u_{i,j} - u_{i,j-1}).\n$$\nRewriting gives the canonical form\n$$\n(Au)_{i,j} = a_{i,j}^{\\mathrm{diag}}\\,u_{i,j}\n+ a_{i,j}^{\\mathrm{E}}\\,u_{i+1,j}\n+ a_{i,j}^{\\mathrm{W}}\\,u_{i-1,j}\n+ a_{i,j}^{\\mathrm{N}}\\,u_{i,j+1}\n+ a_{i,j}^{\\mathrm{S}}\\,u_{i,j-1},\n$$\nwhere the off-diagonal coefficients are negative,\n$$\na_{i,j}^{\\mathrm{E}} = -\\frac{k_{i+1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{W}} = -\\frac{k_{i-1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{N}} = -\\frac{k_{i,j+1/2}}{h^2},\\quad\na_{i,j}^{\\mathrm{S}} = -\\frac{k_{i,j-1/2}}{h^2},\n$$\nand the diagonal coefficient is the sum of positive face weights\n$$\na_{i,j}^{\\mathrm{diag}} = -\\left(a_{i,j}^{\\mathrm{E}} + a_{i,j}^{\\mathrm{W}} + a_{i,j}^{\\mathrm{N}} + a_{i,j}^{\\mathrm{S}}\\right)\n= \\frac{k_{i+1/2,j} + k_{i-1/2,j} + k_{i,j+1/2} + k_{i,j-1/2}}{h^2}.\n$$\n\nRelaxation methods act on the linear system $A u = f$; here we study smoothing on the error equation with $f=0$ so that the update transforms the error $e^{(n)}$ into $e^{(n+1)}$. The Jacobi method uses only the diagonal $D$ of $A$:\n$$\ne^{(n+1)} = e^{(n)} - \\omega_{\\mathrm{J}}\\,D^{-1} A e^{(n)}.\n$$\nIn component form at $(i,j)$, the Jacobi update can be expressed as\n$$\ne_{i,j}^{(n+1)} = (1-\\omega_{\\mathrm{J}})\\,e_{i,j}^{(n)} + \\omega_{\\mathrm{J}}\\,\\frac{-a_{i,j}^{\\mathrm{E}}\\,e_{i+1,j}^{(n)}\n- a_{i,j}^{\\mathrm{W}}\\,e_{i-1,j}^{(n)}\n- a_{i,j}^{\\mathrm{N}}\\,e_{i,j+1}^{(n)}\n- a_{i,j}^{\\mathrm{S}}\\,e_{i,j-1}^{(n)}}{a_{i,j}^{\\mathrm{diag}}},\n$$\nwhich is derived by rearranging $D^{-1} A e^{(n)}$ and recognizing the five-point stencil structure.\n\nThe Gauss-Seidel method uses the latest available neighbor values in a prescribed ordering. If we denote by $L$, $D$, and $U$ the lower, diagonal, and upper parts of $A$ according to the chosen ordering, the update solves\n$$\n(D+L)\\,e^{(n+1)} = -U\\,e^{(n)}.\n$$\nIn component form at a node $(i,j)$ visited in the chosen ordering, Gauss-Seidel replaces neighbors that were already updated in the current sweep by $e^{(n+1)}$ and those not yet visited by $e^{(n)}$, yielding\n$$\ne_{i,j}^{(n+1)} = \\frac{-a_{i,j}^{\\mathrm{E}}\\,\\tilde{e}_{i+1,j}\n- a_{i,j}^{\\mathrm{W}}\\,\\tilde{e}_{i-1,j}\n- a_{i,j}^{\\mathrm{N}}\\,\\tilde{e}_{i,j+1}\n- a_{i,j}^{\\mathrm{S}}\\,\\tilde{e}_{i,j-1}}{a_{i,j}^{\\mathrm{diag}}},\n$$\nwhere $\\tilde{e}$ denotes the most recent available neighbor values following the ordering. The Successive Over-Relaxation method applies a relaxation factor to the Gauss-Seidel update at each node:\n$$\ne_{i,j}^{(n+1)} = (1-\\omega_{\\mathrm{SOR}})\\,e_{i,j}^{(n)} + \\omega_{\\mathrm{SOR}}\\,\\frac{-a_{i,j}^{\\mathrm{E}}\\,\\tilde{e}_{i+1,j}\n- a_{i,j}^{\\mathrm{W}}\\,\\tilde{e}_{i-1,j}\n- a_{i,j}^{\\mathrm{N}}\\,\\tilde{e}_{i,j+1}\n- a_{i,j}^{\\mathrm{S}}\\,\\tilde{e}_{i,j-1}}{a_{i,j}^{\\mathrm{diag}}}.\n$$\n\nThe choice of ordering crucially affects Gauss-Seidel and Successive Over-Relaxation because it determines which neighbor values are considered “updated” and, thus, which error components the method damps more aggressively. Natural lexicographic ordering corresponds to scanning the grid row by row, which aligns with Cartesian directions. Interface-aligned ordering sorts nodes by ascending signed distance $\\varphi$, effectively sweeping through layers parallel to the interface. For modes that are oscillatory along the interface and localized near it, aligning the sweep with layers of constant $\\varphi$ preferentially uses freshly updated values in the normal direction to the interface for subsequent nodes, thereby improving damping of interface-parallel error components that would otherwise slip along the discontinuity when the ordering is misaligned.\n\nTo quantify smoothing of interface-parallel modes, we define the initial error\n$$\ne_0(x,y) = \\exp\\!\\left(-\\left(\\frac{\\varphi(x,y)}{\\sigma}\\right)^2\\right)\\,\\sin\\!\\big(q\\,\\theta(x,y)\\big),\n$$\nwith $q$ chosen relative to grid resolution ($q = \\min\\{32, \\lfloor N/4 \\rfloor\\}$) and $\\sigma$ controlling localization. After one relaxation sweep of a given method and ordering producing $e_1$, we compute the smoothing factor\n$$\nS = \\frac{\\|e_1\\|_{2,\\mathcal{M}}}{\\|e_0\\|_{2,\\mathcal{M}}},\n$$\nwhere $\\mathcal{M} = \\{(x,y): |\\varphi(x,y)| \\le 2\\sigma\\}$ isolates a band around the interface. This measure isolates the behavior of the relaxation with respect to interface-parallel oscillations, avoiding the influence of boundary conditions and regions far from the discontinuity.\n\nAlgorithmic steps:\n1. Construct grid coordinates $(x_i,y_j)$ for $i,j=0,\\dots,N-1$ and compute $\\theta_{i,j}$, $\\rho_{i,j}$, $r(\\theta_{i,j})$, and $\\varphi_{i,j}$.\n2. Build $k_{i,j}$ as $k_{\\mathrm{in}}$ or $k_{\\mathrm{out}}$ according to the sign of $\\varphi_{i,j}$.\n3. Compute face conductivities by harmonic averaging for all interior faces; for faces at the boundary, use the interior conductivity.\n4. Assemble the five-point stencil coefficients $a_{i,j}^{\\mathrm{E}}$, $a_{i,j}^{\\mathrm{W}}$, $a_{i,j}^{\\mathrm{N}}$, $a_{i,j}^{\\mathrm{S}}$, and $a_{i,j}^{\\mathrm{diag}}$.\n5. Construct the initial error field $e_0$ using $\\sigma$ and $q$, and determine the measurement mask $\\mathcal{M}$ where $|\\varphi| \\le 2\\sigma$.\n6. Perform one relaxation sweep:\n   - For Jacobi, compute the update everywhere using only old neighbor values and weight $\\omega_{\\mathrm{J}}$.\n   - For Gauss-Seidel and Successive Over-Relaxation, traverse nodes in natural lexicographic order or sorted by $\\varphi$; at each node, form the Gauss-Seidel raw update using the latest available neighbor values, then apply relaxation with $\\omega_{\\mathrm{SOR}}$ (equal to $1$ for Gauss-Seidel).\n7. Compute smoothing factors $S$ as the ratio of discrete $\\ell^2$ norms over the mask $\\mathcal{M}$.\n8. Repeat for all test cases.\n\nScientific realism considerations:\n- Harmonic averaging is the appropriate choice for discontinuous coefficients to respect flux continuity and is widely used in the numerical solution of partial differential equations.\n- The initial mode $e_0$ is designed to probe relaxation behavior in the tangential direction to the interface and is localized by a Gaussian envelope in the normal direction, making it sensitive to ordering near the discontinuity.\n- The Successive Over-Relaxation parameter is set to $\\omega_{\\mathrm{SOR}} = 1.9$ to demonstrate typical over-relaxation behavior within stable bounds, while Jacobi uses $\\omega_{\\mathrm{J}} = 0.66$ for damping without instability.\n\nThe program implements all steps and outputs, for each test case, five floating-point smoothing factors corresponding to Jacobi, Gauss-Seidel with lexicographic ordering, Gauss-Seidel with interface-aligned ordering, Successive Over-Relaxation with lexicographic ordering, and Successive Over-Relaxation with interface-aligned ordering, respectively, aggregated in a single list of lists printed on one line as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef harmonic(a, b):\n    s = a + b\n    if s = 0:\n        return 0.0\n    return 2.0 * a * b / s\n\ndef build_geometry(N, r0, eps, m):\n    h = 1.0 / (N + 1)\n    i = np.arange(N)\n    j = np.arange(N)\n    x = (i + 1) * h\n    y = (j + 1) * h\n    X, Y = np.meshgrid(x, y, indexing='ij')\n    xc, yc = 0.5, 0.5\n    theta = np.arctan2(Y - yc, X - xc)\n    rho = np.sqrt((X - xc) ** 2 + (Y - yc) ** 2)\n    r_theta = r0 + eps * np.sin(m * theta)\n    phi = rho - r_theta\n    return X, Y, theta, rho, r_theta, phi, h\n\ndef build_conductivity(phi, kin, kout):\n    k = np.where(phi  0.0, kin, kout)\n    return k\n\ndef assemble_stencil(k, h):\n    N = k.shape[0]\n    # Face conductivities with harmonic averaging; handle boundaries\n    kE = np.empty_like(k)\n    kW = np.empty_like(k)\n    kN = np.empty_like(k)\n    kS = np.empty_like(k)\n\n    # East faces\n    kE[:-1, :] = harmonic_array(k[:-1, :], k[1:, :])\n    kE[-1, :] = k[-1, :]  # boundary face uses interior value\n\n    # West faces\n    kW[1:, :] = harmonic_array(k[1:, :], k[:-1, :])\n    kW[0, :] = k[0, :]\n\n    # North faces\n    kN[:, :-1] = harmonic_array(k[:, :-1], k[:, 1:])\n    kN[:, -1] = k[:, -1]\n\n    # South faces\n    kS[:, 1:] = harmonic_array(k[:, 1:], k[:, :-1])\n    kS[:, 0] = k[:, 0]\n\n    aE = -kE / (h ** 2)\n    aW = -kW / (h ** 2)\n    aN = -kN / (h ** 2)\n    aS = -kS / (h ** 2)\n    aD = -(aE + aW + aN + aS)\n    return aD, aE, aW, aN, aS\n\ndef harmonic_array(a, b):\n    s = a + b\n    out = np.zeros_like(s)\n    mask = s  0\n    out[mask] = 2.0 * a[mask] * b[mask] / s[mask]\n    # if s == 0 (should not in our positive kin,kout), keep zero\n    return out\n\ndef initial_error(theta, phi, sigma, N):\n    q = int(min(32, N // 4))\n    e0 = np.exp(-(phi / sigma) ** 2) * np.sin(q * theta)\n    return e0\n\ndef mask_band(phi, sigma):\n    return np.abs(phi) = (2.0 * sigma)\n\ndef jacobi_sweep(u_old, aD, aE, aW, aN, aS, omegaJ):\n    # Build neighbor arrays with zero at boundaries\n    N = u_old.shape[0]\n    uR = np.zeros_like(u_old)\n    uL = np.zeros_like(u_old)\n    uU = np.zeros_like(u_old)\n    uD = np.zeros_like(u_old)\n    uR[:-1, :] = u_old[1:, :]\n    uL[1:, :] = u_old[:-1, :]\n    uU[:, :-1] = u_old[:, 1:]\n    uD[:, 1:] = u_old[:, :-1]\n    s = aE * uR + aW * uL + aN * uU + aS * uD\n    raw = -s / aD\n    u_new = (1.0 - omegaJ) * u_old + omegaJ * raw\n    return u_new\n\ndef build_order_indices(N, phi, mode='lex'):\n    if mode == 'lex':\n        # row-major: j outer, i inner\n        order = [(i, j) for j in range(N) for i in range(N)]\n    elif mode == 'phi':\n        # sort by ascending phi\n        flat_indices = [(i, j) for i in range(N) for j in range(N)]\n        flat_phi = [phi[i, j] for i, j in flat_indices]\n        order = [x for _, x in sorted(zip(flat_phi, flat_indices), key=lambda t: t[0])]\n    else:\n        raise ValueError(\"Unknown ordering mode\")\n    return order\n\ndef gs_sweep(u_old, aD, aE, aW, aN, aS, order, omega=1.0):\n    N = u_old.shape[0]\n    u_new = u_old.copy()\n    visited = np.zeros_like(u_old, dtype=bool)\n    for i, j in order:\n        s = 0.0\n        # East neighbor (i+1, j)\n        if i + 1  N:\n            val = u_new[i + 1, j] if visited[i + 1, j] else u_old[i + 1, j]\n            s += aE[i, j] * val\n        # West neighbor (i-1, j)\n        if i - 1 = 0:\n            val = u_new[i - 1, j] if visited[i - 1, j] else u_old[i - 1, j]\n            s += aW[i, j] * val\n        # North neighbor (i, j+1)\n        if j + 1  N:\n            val = u_new[i, j + 1] if visited[i, j + 1] else u_old[i, j + 1]\n            s += aN[i, j] * val\n        # South neighbor (i, j-1)\n        if j - 1 = 0:\n            val = u_new[i, j - 1] if visited[i, j - 1] else u_old[i, j - 1]\n            s += aS[i, j] * val\n\n        raw = -s / aD[i, j]\n        u_new[i, j] = (1.0 - omega) * u_old[i, j] + omega * raw\n        visited[i, j] = True\n    return u_new\n\ndef smoothing_factor(e0, e1, mask):\n    # compute discrete L2 norm over mask; ratio cancels h\n    sel0 = e0[mask]\n    sel1 = e1[mask]\n    norm0 = np.sqrt(np.sum(sel0 * sel0))\n    norm1 = np.sqrt(np.sum(sel1 * sel1))\n    # To avoid division by zero, if norm0 is zero, define smoothing factor as 0.0\n    if norm0 == 0.0:\n        return 0.0\n    return float(norm1 / norm0)\n\ndef run_case(N, r0, eps, m, kin, kout, sigma, omegaJ, omegaSOR):\n    # Geometry and coefficients\n    X, Y, theta, rho, r_theta, phi, h = build_geometry(N, r0, eps, m)\n    k = build_conductivity(phi, kin, kout)\n    aD, aE, aW, aN, aS = assemble_stencil(k, h)\n    # Initial error and mask\n    e0 = initial_error(theta, phi, sigma, N)\n    mask = mask_band(phi, sigma)\n\n    # Jacobi\n    e1_jacobi = jacobi_sweep(e0, aD, aE, aW, aN, aS, omegaJ)\n    S_jacobi = smoothing_factor(e0, e1_jacobi, mask)\n\n    # GS lex\n    order_lex = build_order_indices(N, phi, mode='lex')\n    e1_gs_lex = gs_sweep(e0, aD, aE, aW, aN, aS, order_lex, omega=1.0)\n    S_gs_lex = smoothing_factor(e0, e1_gs_lex, mask)\n\n    # GS phi\n    order_phi = build_order_indices(N, phi, mode='phi')\n    e1_gs_phi = gs_sweep(e0, aD, aE, aW, aN, aS, order_phi, omega=1.0)\n    S_gs_phi = smoothing_factor(e0, e1_gs_phi, mask)\n\n    # SOR lex\n    e1_sor_lex = gs_sweep(e0, aD, aE, aW, aN, aS, order_lex, omega=omegaSOR)\n    S_sor_lex = smoothing_factor(e0, e1_sor_lex, mask)\n\n    # SOR phi\n    e1_sor_phi = gs_sweep(e0, aD, aE, aW, aN, aS, order_phi, omega=omegaSOR)\n    S_sor_phi = smoothing_factor(e0, e1_sor_phi, mask)\n\n    return [S_jacobi, S_gs_lex, S_gs_phi, S_sor_lex, S_sor_phi]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (happy path)\n        {\"N\": 96, \"r0\": 0.35, \"eps\": 0.04, \"m\": 4, \"kin\": 10.0, \"kout\": 1.0, \"sigma\": 0.03, \"omegaJ\": 0.66, \"omegaSOR\": 1.9},\n        # Case B (high contrast)\n        {\"N\": 96, \"r0\": 0.35, \"eps\": 0.08, \"m\": 6, \"kin\": 1000.0, \"kout\": 1.0, \"sigma\": 0.03, \"omegaJ\": 0.66, \"omegaSOR\": 1.9},\n        # Case C (boundary and curvature variation)\n        {\"N\": 48, \"r0\": 0.35, \"eps\": 0.12, \"m\": 8, \"kin\": 50.0, \"kout\": 1.0, \"sigma\": 0.04, \"omegaJ\": 0.66, \"omegaSOR\": 1.9},\n    ]\n\n    results = []\n    for case in test_cases:\n        res = run_case(\n            N=case[\"N\"],\n            r0=case[\"r0\"],\n            eps=case[\"eps\"],\n            m=case[\"m\"],\n            kin=case[\"kin\"],\n            kout=case[\"kout\"],\n            sigma=case[\"sigma\"],\n            omegaJ=case[\"omegaJ\"],\n            omegaSOR=case[\"omegaSOR\"],\n        )\n        # Round results to a reasonable number of decimals for readability\n        results.append([float(f\"{x:.6f}\") for x in res])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}