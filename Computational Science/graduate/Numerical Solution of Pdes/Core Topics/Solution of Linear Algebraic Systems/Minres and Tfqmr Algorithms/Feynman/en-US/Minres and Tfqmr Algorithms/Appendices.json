{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of the MINRES algorithm for symmetric systems is rooted in the Lanczos process, which cleverly constructs a small tridiagonal matrix that captures essential properties of the original large operator. This exercise provides a step-by-step walkthrough of the first few iterations of this process for a model problem arising from a PDE discretization. By performing these calculations by hand, you will gain a concrete understanding of how the orthonormal basis and the tridiagonal projection are built, demystifying the core mechanism that makes MINRES powerful and efficient.",
            "id": "3421772",
            "problem": "Consider the linear system arising from the centered finite-difference discretization of the one-dimensional Poisson equation $-u''(x)=f(x)$ on the unit interval with homogeneous Dirichlet boundary conditions, where the mesh parameter scaling is absorbed into the right-hand side for simplicity. The resulting symmetric positive definite matrix is\n$$\nA=\\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix},\n$$\nwhich is a standard model problem in Krylov subspace methods used for the numerical solution of partial differential equations. In the Minimum Residual method (MINRES), the Lanczos process generates an orthonormal basis for the Krylov subspace and a symmetric tridiagonal projection of $A$. Transpose-Free Quasi-Minimal Residual (TFQMR) is a related method for nonsymmetric problems, but here the focus is on the symmetric case relevant to MINRES.\n\nStarting from the initial guess $x_0=\\mathbf{0}$ and the right-hand side vector $b=\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$, define the initial residual $r_0=b-Ax_0$ and the first Lanczos basis vector $v_1=r_0/\\|r_0\\|$. Using the fundamental definitions of Krylov subspaces and the three-term recurrence that characterizes the Lanczos process on symmetric matrices, carry out exactly two Lanczos iterations to construct the orthonormal basis vectors $v_1$ and $v_2$, and assemble the $2\\times 2$ tridiagonal matrix $T_2$ whose entries are the projection coefficients generated by these iterations.\n\nAs a final scalar diagnostic related to the spectral compression that underpins MINRES, compute the determinant of the tridiagonal matrix $T_2$. Provide this determinant as a single exact real number. No rounding is required, and no units are involved. The answer must be given as a single real-valued number.",
            "solution": "The problem requires the computation of the determinant of a $2 \\times 2$ tridiagonal matrix $T_2$, which is generated by performing two iterations of the Lanczos process on a given symmetric matrix $A$. The process starts with a specific right-hand side vector $b$ and initial guess $x_0$.\n\nThe given matrix is:\n$$\nA=\\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}\n$$\nThe right-hand side vector is $b=\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$, and the initial guess is $x_0=\\mathbf{0}=\\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$.\n\nThe Lanczos process is an iterative algorithm that generates an orthonormal basis $\\{v_j\\}$ for the Krylov subspace $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$ and a symmetric tridiagonal matrix $T_k$ whose entries are the coefficients of a three-term recurrence relation. The standard algorithm proceeds as follows:\n\n1.  Initialize: Calculate the initial residual $r_0 = b - Ax_0$. Set $\\beta_1 = \\|r_0\\|_2$ and $v_1 = r_0 / \\beta_1$. Set $v_0 = \\mathbf{0}$.\n2.  Iterate for $j=1, 2, \\dots$:\n    a. Compute $w_j = A v_j$.\n    b. Compute the diagonal coefficient $\\alpha_j = v_j^T w_j$.\n    c. Compute the unnormalized next vector $\\tilde{v}_{j+1} = w_j - \\alpha_j v_j - \\beta_j v_{j-1}$.\n    d. Compute the off-diagonal coefficient $\\beta_{j+1} = \\|\\tilde{v}_{j+1}\\|_2$.\n    e. If $\\beta_{j+1} \\neq 0$, normalize to get the next basis vector $v_{j+1} = \\tilde{v}_{j+1} / \\beta_{j+1}$.\n\nWe will carry out this process for two iterations to find the coefficients $\\alpha_1$, $\\beta_2$, and $\\alpha_2$, which form the matrix $T_2$.\n\n**Initialization Step:**\n\nFirst, we compute the initial residual $r_0$:\n$$\nr_0 = b - Ax_0 = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} - \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}\n$$\nNext, we compute the Euclidean norm of $r_0$ to find $\\beta_1$ and then normalize $r_0$ to get $v_1$:\n$$\n\\beta_1 = \\|r_0\\|_2 = \\sqrt{1^2 + 0^2 + 0^2} = 1\n$$\n$$\nv_1 = \\frac{r_0}{\\beta_1} = \\frac{1}{1} \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}\n$$\nThe problem statement defines $v_1=r_0/\\|r_0\\|$, which is consistent with our initialization.\n\n**First Lanczos Iteration ($j=1$):**\n\nWe compute $w_1 = Av_1$:\n$$\nw_1 = Av_1 = \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}2 \\\\ -1 \\\\ 0\\end{pmatrix}\n$$\nThe first diagonal coefficient $\\alpha_1$ is the projection of $w_1$ onto $v_1$:\n$$\n\\alpha_1 = v_1^T w_1 = \\begin{pmatrix}1 & 0 & 0\\end{pmatrix} \\begin{pmatrix}2 \\\\ -1 \\\\ 0\\end{pmatrix} = (1)(2) + (0)(-1) + (0)(0) = 2\n$$\nNow we compute the unnormalized vector for the next step. For $j=1$, the recurrence is $\\tilde{v}_2 = w_1 - \\alpha_1 v_1 - \\beta_1 v_0$. Since $v_0 = \\mathbf{0}$, this simplifies:\n$$\n\\tilde{v}_2 = w_1 - \\alpha_1 v_1 = \\begin{pmatrix}2 \\\\ -1 \\\\ 0\\end{pmatrix} - 2 \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix}\n$$\nThe first off-diagonal coefficient $\\beta_2$ is the norm of this vector:\n$$\n\\beta_2 = \\|\\tilde{v}_2\\|_2 = \\sqrt{0^2 + (-1)^2 + 0^2} = 1\n$$\nSince $\\beta_2 \\neq 0$, we find the second orthonormal basis vector $v_2$:\n$$\nv_2 = \\frac{\\tilde{v}_2}{\\beta_2} = \\frac{1}{1} \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix}\n$$\n\n**Second Lanczos Iteration ($j=2$):**\n\nWe compute $w_2 = Av_2$:\n$$\nw_2 = Av_2 = \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -2 \\\\ 1\\end{pmatrix}\n$$\nThe second diagonal coefficient $\\alpha_2$ is the projection of $w_2$ onto $v_2$:\n$$\n\\alpha_2 = v_2^T w_2 = \\begin{pmatrix}0 & -1 & 0\\end{pmatrix} \\begin{pmatrix}1 \\\\ -2 \\\\ 1\\end{pmatrix} = (0)(1) + (-1)(-2) + (0)(1) = 2\n$$\nThe problem only requires the construction of $T_2$, so we do not need to compute $\\beta_3$ or $v_3$.\n\n**Constructing and Evaluating $T_2$:**\n\nThe $k \\times k$ tridiagonal matrix $T_k$ generated by the Lanczos process is given by:\n$$\nT_k = \\begin{pmatrix}\n\\alpha_1 & \\beta_2 & & \\\\\n\\beta_2 & \\alpha_2 & \\ddots & \\\\\n & \\ddots & \\ddots & \\beta_k \\\\\n & & \\beta_k & \\alpha_k\n\\end{pmatrix}\n$$\nFor $k=2$, using the coefficients we calculated ($\\alpha_1=2$, $\\beta_2=1$, $\\alpha_2=2$), the matrix $T_2$ is:\n$$\nT_2 = \\begin{pmatrix}\n\\alpha_1 & \\beta_2 \\\\\n\\beta_2 & \\alpha_2\n\\end{pmatrix} = \\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n$$\nThe final step is to compute the determinant of $T_2$:\n$$\n\\det(T_2) = (\\alpha_1)(\\alpha_2) - (\\beta_2)^2 = (2)(2) - (1)^2 = 4 - 1 = 3\n$$\nThe determinant of the tridiagonal matrix $T_2$ is $3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "While TFQMR extends the reach of residual-minimizing methods to nonsymmetric systems, its practical implementation with left preconditioning introduces a critical subtlety. The algorithm monitors the norm of the preconditioned residual, which can be a poor proxy for the true residual, potentially leading to premature termination and inaccurate solutions. This practice uses a concrete example from a convection-diffusion problem to demonstrate this dangerous discrepancy and then guides you through deriving a robust tolerance adaptation strategy to guarantee true convergence.",
            "id": "3421783",
            "problem": "Consider the one-dimensional, steady convection–diffusion boundary value problem on the interval $\\left[0,1\\right]$,\n$$\n-\\varepsilon\\,u''(x) + \\beta\\,u'(x) = f(x),\\quad u(0)=0,\\quad u(1)=0,\n$$\nwith positive parameters $\\varepsilon$ and $\\beta$. Discretize using second-order central differences for $u''(x)$ and first-order upwind for $u'(x)$ on a uniform grid with two interior points, so that the resulting linear system is $A\\,x=b$ with\n$$\nA=\\begin{pmatrix}\n2\\varepsilon/h^{2}+\\beta/h & -\\varepsilon/h^{2} \\\\\n-\\left(\\varepsilon/h^{2}+\\beta/h\\right) & 2\\varepsilon/h^{2}+\\beta/h\n\\end{pmatrix},\n$$\nwhere $h$ is the uniform grid spacing. Let $\\varepsilon=10^{-4}$, $\\beta=2$, and $h=1/3$, and choose the right-hand side $b=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$. Consider left preconditioning with the diagonal matrix\n$$\nM=\\begin{pmatrix}\n1 & 0 \\\\\n0 & 10^{5}\n\\end{pmatrix},\n$$\nso the left-preconditioned system is $M^{-1}A\\,x=M^{-1}b$. Let $r_{k}=b-A\\,x_{k}$ denote the true residual at an iterate $x_{k}$ and $\\hat{r}_{k}=M^{-1}r_{k}$ the left-preconditioned residual that is monitored by Transpose-Free Quasi-Minimal Residual (TFQMR).\n\n(a) Using only the core definitions of residuals and preconditioning, evaluate the initial residuals at $x_{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ and compute the ratio\n$$\nR=\\frac{\\|r_{0}\\|_{2}}{\\|\\hat{r}_{0}\\|_{2}},\n$$\nto demonstrate a case where the recursively monitored residual in left-preconditioned TFQMR significantly underestimates the true residual.\n\n(b) Starting from norm equivalence and the submultiplicativity of the operator norm, derive a tolerance adaptation strategy that guarantees $\\|r_{k}\\|_{2}\\le t_{\\mathrm{true}}$ whenever TFQMR’s recursive monitor satisfies $\\|\\hat{r}_{k}\\|_{2}\\le \\tau_{\\mathrm{TFQMR}}$. Express the adaptation in the form $\\tau_{\\mathrm{TFQMR}}=\\alpha\\,t_{\\mathrm{true}}$ and give $\\alpha$ in terms of $M$.\n\n(c) For the specific $M$ above, evaluate the scaling factor $\\alpha$ numerically, and provide its value rounded to four significant figures. No units are required for the final numerical value.",
            "solution": "The problem is divided into three parts: (a) a numerical demonstration, (b) a theoretical derivation, and (c) a final numerical evaluation. We will address each part in sequence.\n\n(a) Evaluation of the initial residuals and their norm ratio.\nWe are given the parameters $\\varepsilon = 10^{-4}$, $\\beta = 2$, and the grid spacing $h = 1/3$. We first evaluate the entries of the matrix $A$.\nThe required terms are:\n$$\n\\frac{\\varepsilon}{h^2} = \\frac{10^{-4}}{(1/3)^2} = \\frac{10^{-4}}{1/9} = 9 \\times 10^{-4} = 0.0009\n$$\n$$\n\\frac{\\beta}{h} = \\frac{2}{1/3} = 6\n$$\nUsing these values, we can construct the matrix $A$:\n$$\nA = \\begin{pmatrix}\n2\\varepsilon/h^{2}+\\beta/h & -\\varepsilon/h^{2} \\\\\n-\\left(\\varepsilon/h^{2}+\\beta/h\\right) & 2\\varepsilon/h^{2}+\\beta/h\n\\end{pmatrix}\n= \\begin{pmatrix}\n2(0.0009) + 6 & -0.0009 \\\\\n-(0.0009 + 6) & 2(0.0009) + 6\n\\end{pmatrix}\n= \\begin{pmatrix}\n6.0018 & -0.0009 \\\\\n-6.0009 & 6.0018\n\\end{pmatrix}\n$$\nThe initial guess for the solution is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. The initial true residual $r_0$ is defined as $r_0 = b - A x_0$. With $x_0$ being the zero vector, this simplifies to $r_0 = b$.\nGiven $b = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, we have $r_0 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe Euclidean norm (or $L_2$-norm) of the initial true residual is:\n$$\n\\|r_0\\|_2 = \\sqrt{0^2 + 1^2} = 1\n$$\nNext, we compute the initial left-preconditioned residual, $\\hat{r}_0 = M^{-1}r_0$. The preconditioning matrix is given as $M = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^5 \\end{pmatrix}$. Its inverse is:\n$$\nM^{-1} = \\begin{pmatrix} 1^{-1} & 0 \\\\ 0 & (10^5)^{-1} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{-5} \\end{pmatrix}\n$$\nNow we compute $\\hat{r}_0$:\n$$\n\\hat{r}_0 = M^{-1}r_0 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{-5} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (0)(1) \\\\ (0)(0) + (10^{-5})(1) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 10^{-5} \\end{pmatrix}\n$$\nThe Euclidean norm of the initial preconditioned residual is:\n$$\n\\|\\hat{r}_0\\|_2 = \\sqrt{0^2 + (10^{-5})^2} = 10^{-5}\n$$\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{\\|r_0\\|_2}{\\|\\hat{r}_0\\|_2} = \\frac{1}{10^{-5}} = 10^5\n$$\nThis result demonstrates that for this specific problem setup, the norm of the monitored residual $\\|\\hat{r}_0\\|_2$ is $5$ orders of magnitude smaller than the norm of the true residual $\\|r_0\\|_2$.\n\n(b) Derivation of the tolerance adaptation strategy.\nThe relationship between the true residual $r_k$ and the left-preconditioned residual $\\hat{r}_k$ is given by $\\hat{r}_k = M^{-1}r_k$.\nMultiplying from the left by $M$, we obtain an expression for the true residual in terms of the preconditioned one:\n$$\nr_k = M\\hat{r}_k\n$$\nWe are interested in the relationship between the norms of these vectors. Taking the vector $2$-norm of both sides, we get:\n$$\n\\|r_k\\|_2 = \\|M\\hat{r}_k\\|_2\n$$\nUsing the submultiplicative property of induced matrix norms (also known as consistency property), which states that for any compatible matrix $A$ and vector $v$, $\\|Av\\| \\le \\|A\\|\\|v\\|$, we can establish an inequality:\n$$\n\\|M\\hat{r}_k\\|_2 \\le \\|M\\|_2 \\|\\hatr_k\\|_2\n$$\nwhere $\\|M\\|_2$ is the matrix $2$-norm (operator norm) induced by the vector $2$-norm. This leads to the inequality:\n$$\n\\|r_k\\|_2 \\le \\|M\\|_2 \\|\\hat{r}_k\\|_2\n$$\nThe TFQMR algorithm monitors the preconditioned residual norm $\\|\\hat{r}_k\\|_2$ and typically stops when it falls below a user-defined tolerance, which we denote as $\\tau_{\\mathrm{TFQMR}}$. That is, the stopping criterion is $\\|\\hat{r}_k\\|_2 \\le \\tau_{\\mathrm{TFQMR}}$.\nSubstituting this into our inequality gives an upper bound on the true residual norm at termination:\n$$\n\\|r_k\\|_2 \\le \\|M\\|_2 \\tau_{\\mathrm{TFQMR}}\n$$\nOur goal is to ensure that the true residual norm is bounded by a desired tolerance $t_{\\mathrm{true}}$, i.e., $\\|r_k\\|_2 \\le t_{\\mathrm{true}}$. To guarantee this, we must choose $\\tau_{\\mathrm{TFQMR}}$ such that the upper bound on $\\|r_k\\|_2$ is at most $t_{\\mathrm{true}}$:\n$$\n\\|M\\|_2 \\tau_{\\mathrm{TFQMR}} \\le t_{\\mathrm{true}}\n$$\nTo obtain the least restrictive (largest) possible tolerance $\\tau_{\\mathrm{TFQMR}}$ that satisfies this condition, we set the terms to be equal:\n$$\n\\|M\\|_2 \\tau_{\\mathrm{TFQMR}} = t_{\\mathrm{true}}\n$$\nSolving for $\\tau_{\\mathrm{TFQMR}}$, we find the adaptive tolerance strategy:\n$$\n\\tau_{\\mathrm{TFQMR}} = \\frac{1}{\\|M\\|_2} t_{\\mathrm{true}}\n$$\nComparing this to the requested form $\\tau_{\\mathrm{TFQMR}} = \\alpha\\,t_{\\mathrm{true}}$, we identify the scaling factor $\\alpha$ as:\n$$\n\\alpha = \\frac{1}{\\|M\\|_2}\n$$\n\n(c) Numerical evaluation of the scaling factor $\\alpha$.\nFrom the result of part (b), we have $\\alpha = 1/\\|M\\|_2$. We need to calculate the $2$-norm of the given preconditioner $M$:\n$$\nM = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^5 \\end{pmatrix}\n$$\nThe matrix $2$-norm $\\|M\\|_2$ is defined as the maximum singular value of $M$. For a diagonal matrix $D = \\mathrm{diag}(d_1, d_2, \\dots, d_n)$, the singular values are simply the absolute values of the diagonal entries, $|d_i|$. The $2$-norm is the maximum of these values.\nIn this case, the diagonal entries of $M$ are $1$ and $10^5$. Their absolute values are $1$ and $10^5$.\nTherefore, the $2$-norm of $M$ is:\n$$\n\\|M\\|_2 = \\max(|1|, |10^5|) = 10^5\n$$\nNow we can compute the scaling factor $\\alpha$:\n$$\n\\alpha = \\frac{1}{\\|M\\|_2} = \\frac{1}{10^5} = 10^{-5}\n$$\nThe problem asks for this value to be rounded to four significant figures. The exact value is $0.00001$. To express this with four significant figures, we write it in scientific notation as $1.000 \\times 10^{-5}$.",
            "answer": "$$\n\\boxed{1.000 \\times 10^{-5}}\n$$"
        },
        {
            "introduction": "In practical scientific computing, we often work with operators in a 'matrix-free' context, where we can apply the operator but cannot inspect its entries directly. This presents a fundamental challenge: how do we choose the right tool for the job, such as MINRES or TFQMR, without knowing the matrix properties beforehand? This problem guides you through the process of designing a robust numerical testing strategy to probe an unknown operator for symmetry and definiteness, culminating in a clear decision-making flowchart for selecting the most appropriate solver.",
            "id": "3421838",
            "problem": "You are implementing a matrix-free Krylov solver for a linear system $A u = b$ arising from a finite element discretization of a linear partial differential equation. The operator $A$ is available only as a function that returns $A x$ for a given vector $x$. A left preconditioner $M^{-1}$ is available via a function that returns $M^{-1} r$ for any vector $r$, but the explicit matrices $A$ and $M$ are not assembled. You must decide between the Minimum Residual method (MINRES) and the Transpose-Free Quasi-Minimal Residual method (TFQMR) based on whether the operator is symmetric and on its definiteness, and you must ensure that any preconditioning assumptions required by the chosen solver hold.\n\nStarting from the definitions of self-adjointness and definiteness in an inner-product space, propose matrix-free numerical tests to assess:\n\n- Whether $A$ is symmetric (self-adjoint) with respect to the Euclidean inner product $\\langle x, y \\rangle = x^{\\top} y$, and\n- Whether $A$ is positive definite, indefinite, or positive semidefinite,\n\nand explain how to adapt the solver selection between MINRES and TFQMR based on the test results, taking into account the role of preconditioning in the required inner product. Consider that the partial differential equation may yield a null space (for example, pure Neumann boundary conditions in Poisson problems), and that the preconditioner may not be exactly symmetric in practice. Assume double precision arithmetic with machine unit roundoff $\\epsilon_{\\mathrm{mach}}$ on the order of $10^{-16}$.\n\nWhich option below gives a scientifically sound set of matrix-free tests and a correct solver-selection policy under these constraints?\n\nA. Use random probe tests for symmetry: draw $k$ random nonzero pairs $(x_i, y_i)$ and compute the normalized antisymmetry measures\n$\\eta_i = \\dfrac{\\left|\\langle A x_i, y_i \\rangle - \\langle x_i, A y_i \\rangle\\right|}{\\|x_i\\| \\, \\|A y_i\\| + \\|y_i\\| \\, \\|A x_i\\|}$, accept symmetry if $\\max_i \\eta_i \\le c \\, \\epsilon_{\\mathrm{mach}}$ for a modest constant $c$. For definiteness, run $m$ steps of matrix-free Lanczos on $A$ with a random start to estimate extremal Ritz values $\\lambda_{\\min}$ and $\\lambda_{\\max}$ of $A$ in the Euclidean inner product; classify as symmetric positive definite if $\\lambda_{\\min} > 0$, indefinite if $\\lambda_{\\min} < 0 < \\lambda_{\\max}$, and positive semidefinite if $\\lambda_{\\min} \\approx 0 \\le \\lambda_{\\max}$; if a known null space exists (e.g., constants), project it out before testing. Verify the preconditioner is symmetric positive definite by checking $\\langle M z, z \\rangle > 0$ for random $z \\ne 0$ and small antisymmetry in $\\langle M u, v \\rangle - \\langle u, M v \\rangle$. Choose MINRES if $A$ is symmetric (definiteness can be indefinite) and $M$ is symmetric positive definite, using the $M$-inner product $\\langle x, y \\rangle_M = \\langle M x, y \\rangle$ for preconditioned MINRES; otherwise, choose TFQMR.\n\nB. To test symmetry, approximate $\\|A x - A^{\\top} x\\|$ for random $x$ and accept symmetry if the norm is below $10^{-8}$; to test definiteness, check that $\\|A x\\|$ increases monotonically with $\\|x\\|$. Choose TFQMR only if $A$ is symmetric positive definite; otherwise, choose MINRES because it minimizes the residual for any $A$.\n\nC. Use a single-vector energy test: draw random nonzero $x$ and compute $\\rho(x) = \\langle A x, x \\rangle / \\langle x, x \\rangle$. If $\\rho(x) > 0$, conclude $A$ is symmetric positive definite; if $\\rho(x) < 0$, conclude $A$ is nonsymmetric. Choose Conjugate Gradient for symmetric indefinite systems because it is faster than MINRES; otherwise choose TFQMR.\n\nD. Use a single directional-derivative test: pick one random nonzero pair $(x, y)$, compute $\\left|\\langle A x, y \\rangle - \\langle x, A y \\rangle\\right|$, and accept symmetry if it is below $10^{-3}$. If a negative Rayleigh quotient $\\langle A x, x \\rangle / \\langle x, x \\rangle$ is ever observed, conclude $A$ is nonsymmetric. Always use MINRES on the left-preconditioned system $M^{-1} A u = M^{-1} b$ whenever $M$ is positive definite, because left preconditioning symmetrizes any $A$ in the Euclidean inner product; otherwise use TFQMR.",
            "solution": "This problem requires selecting the correct testing strategy and solver selection logic for a matrix-free environment. Let's analyze each option based on established principles of numerical linear algebra.\n\n**Option A is correct.** It describes a state-of-the-art, robust strategy:\n*   **Symmetry Test:** It proposes a statistical test for symmetry ($A=A^\\top \\iff \\langle Ax, y \\rangle = \\langle x, Ay \\rangle$). Using multiple random vector pairs and a properly normalized measure of deviation is crucial for reliability in floating-point arithmetic. The comparison to machine epsilon ($\\epsilon_{\\mathrm{mach}}$) is the correct way to distinguish numerical noise from true non-symmetry.\n*   **Definiteness Test:** It correctly identifies the matrix-free Lanczos iteration as the standard tool for estimating extremal eigenvalues (Ritz values) of a symmetric operator. The classification based on the signs of $\\lambda_{\\min}$ and $\\lambda_{\\max}$ is the definition of positive definite, indefinite, and semidefinite. Acknowledging the need to handle null spaces is a sign of a robust practical strategy.\n*   **Preconditioner Check & Solver Selection:** It correctly states the requirements for using preconditioned MINRES: the original matrix $A$ must be symmetric, and the preconditioner $M$ must be symmetric positive definite (SPD). This ensures the preconditioned operator $M^{-1}A$ is symmetric in the $M$-inner product, which is necessary for MINRES's short recurrence. The fallback to the general-purpose TFQMR solver when these conditions are not met is the appropriate choice.\n\n**Option B is incorrect.** It contains multiple fundamental errors:\n*   The proposed symmetry test, computing $\\|A x - A^{\\top} x\\|$, is impossible in a matrix-free context, as the action of the transpose operator $A^{\\top}$ is not assumed to be available.\n*   The definiteness test is not a valid measure of definiteness.\n*   The solver selection logic is reversed. Specialized solvers like CG or MINRES are used for symmetric systems, while general solvers like TFQMR or GMRES are used for non-symmetric ones. Furthermore, MINRES is designed for symmetric systems, not \"any A\".\n\n**Option C is incorrect.** It relies on unreliable tests and faulty logic:\n*   A test based on a single random vector is not robust; it can easily miss the relevant behavior of an operator.\n*   Its conclusions are based on logical fallacies. A non-symmetric matrix can happen to yield a positive Rayleigh quotient $\\langle Ax, x \\rangle > 0$. A symmetric indefinite matrix will, by definition, yield a negative Rayleigh quotient for some vectors, so observing this does not imply non-symmetry.\n*   It fatally suggests using the Conjugate Gradient (CG) method for indefinite systems. CG is only guaranteed to converge for symmetric positive-definite systems and will likely fail or diverge on an indefinite one.\n\n**Option D is incorrect.** It is flawed in its testing methodology and its core understanding of preconditioning:\n*   The single-pair symmetry test is not robust, and the proposed tolerance of $10^{-3}$ is arbitrary and far too loose.\n*   It repeats the error from Option C, incorrectly concluding that a negative Rayleigh quotient implies non-symmetry.\n*   Most importantly, it is based on a critical falsehood: that left preconditioning ($M^{-1}A$) symmetrizes a non-symmetric operator $A$. In general, the product of a symmetric matrix ($M^{-1}$) and a non-symmetric one ($A$) is not symmetric. Applying MINRES to this non-symmetric operator would be a severe error, leading to algorithmic breakdown.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}