{
    "hands_on_practices": [
        {
            "introduction": "We begin with a foundational exercise in convergence analysis. The convergence rate of a stationary iteration is governed by the spectral radius of its iteration matrix. This practice focuses on the weighted Jacobi method, where a relaxation parameter $\\omega$ can be tuned to optimize performance. By analyzing the eigenvalues of the iteration matrix for the classic 2D Poisson problem, you will derive the precise range of $\\omega$ that guarantees convergence and determine the optimal value that yields the fastest asymptotic convergence rate. ",
            "id": "3437780",
            "problem": "Consider the two-dimensional Poisson equation $-\\Delta u = f$ on $(0,1)^{2}$ with homogeneous Dirichlet boundary conditions. Discretize the problem using the standard five-point finite difference stencil on an $N \\times N$ interior grid with mesh spacing $h = \\frac{1}{N+1}$, producing a symmetric positive definite (SPD) linear system $A u = b$. Let $A = D - (L + U)$ denote the usual splitting into diagonal $D$ and strictly lower and upper triangular parts $L$ and $U$. The weighted Jacobi method with damping parameter $\\omega$ applies the iteration $x^{(k+1)} = x^{(k)} + \\omega D^{-1}(b - A x^{(k)})$, whose iteration matrix is $T_{\\omega} = I - \\omega D^{-1} A$.\n\nIt is known that $D = \\frac{4}{h^{2}} I$ and that the eigenvalues of $D^{-1} A$ are indexed by integer mode pairs $(p,q)$ with $1 \\leq p,q \\leq N$, and have the form\n$$\n\\mu_{p,q} = \\sin^{2}\\!\\left(\\alpha_{p}\\right) + \\sin^{2}\\!\\left(\\alpha_{q}\\right), \\quad \\text{where} \\quad \\alpha_{k} = \\frac{\\pi k}{2(N+1)}.\n$$\nFrom these, the extremal bounds on the spectrum are\n$$\n\\mu_{\\min} = 2 \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right), \\qquad \\mu_{\\max} = 2 \\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right).\n$$\n\nUsing only these bounds and fundamental properties of the spectral radius, do the following:\n- Derive a provable interval for the damping parameter $\\omega$ that guarantees the convergence of the weighted Jacobi iteration (i.e., $\\rho(T_{\\omega})  1$).\n- Under the optimal choice of $\\omega$ based on the given bounds (chosen to minimize the worst-case spectral radius over the interval $[\\mu_{\\min}, \\mu_{\\max}]$), estimate the worst-case asymptotic convergence factor of the iteration.\n\nExpress your final answer as the exact closed-form expression for the worst-case asymptotic convergence factor as a function of $N$. No rounding is required, and no physical units are involved.",
            "solution": "The user has provided a problem statement regarding the convergence analysis of the weighted Jacobi method for the discretized 2D Poisson equation.\n\n### Step 1: Extract Givens\n- **Governing Equation  Domain**: $-\\Delta u = f$ on the unit square $(0,1)^{2}$ with homogeneous Dirichlet boundary conditions.\n- **Discretization**: Standard five-point finite difference method on an $N \\times N$ interior grid.\n- **Mesh Spacing**: $h = \\frac{1}{N+1}$.\n- **Linear System**: $A u = b$, where $A$ is symmetric positive definite (SPD).\n- **Matrix Splitting**: $A = D - (L + U)$, where $D$ is the diagonal part of $A$, and $L, U$ are the strictly lower and upper triangular parts, respectively.\n- **Weighted Jacobi Iteration**: $x^{(k+1)} = x^{(k)} + \\omega D^{-1}(b - A x^{(k)})$.\n- **Iteration Matrix**: $T_{\\omega} = I - \\omega D^{-1} A$.\n- **Diagonal Matrix**: $D = \\frac{4}{h^{2}} I$, where $I$ is the identity matrix.\n- **Eigenvalues of $D^{-1} A$**: The eigenvalues are denoted $\\mu_{p,q}$ for $1 \\leq p,q \\leq N$, with the form $\\mu_{p,q} = \\sin^{2}\\!\\left(\\alpha_{p}\\right) + \\sin^{2}\\!\\left(\\alpha_{q}\\right)$, where $\\alpha_{k} = \\frac{\\pi k}{2(N+1)}$.\n- **Extremal Eigenvalues of $D^{-1} A$**: The minimum and maximum eigenvalues are given as:\n  $$ \\mu_{\\min} = 2 \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) $$\n  $$ \\mu_{\\max} = 2 \\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) $$\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is grounded in the standard theory of numerical analysis for partial differential equations. The discretization of the Poisson equation, the formulation of the weighted Jacobi method, and the spectral analysis of convergence are cornerstones of this field. The provided formulas for the eigenvalues of the preconditioned matrix $D^{-1}A$ and their bounds are correct and well-documented for this specific problem. No scientific or mathematical flaws are present.\n2.  **Non-Formalizable or Irrelevant**: The problem is a formal and standard exercise in the rate-of-convergence analysis for stationary iterative methods, which is precisely the topic it purports to be about.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained. It provides the iteration matrix, the full spectrum of the relevant matrix $D^{-1}A$ (via its extremal bounds), and a clear objective. The information is sufficient and consistent.\n4.  **Unrealistic or Infeasible**: All mathematical conditions and data are standard for this model problem. No unrealistic or infeasible requirements are made.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. It asks for the derivation of a convergence interval and an optimal convergence factor, both of which are uniquely determined by the provided spectral bounds. The terminology is precise and unambiguous.\n6.  **Pseudo-Profound, Trivial, or Tautological**: While the problem relies on provided eigenvalue bounds, the task of deriving the optimal parameter and the corresponding convergence rate requires a substantive chain of reasoning involving spectral radius properties and optimization. It is a non-trivial canonical problem.\n7.  **Outside Scientific Verifiability**: The claims and derivations are fully verifiable using the principles of linear algebra and numerical analysis.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe convergence of the weighted Jacobi iteration is determined by the spectral radius of the iteration matrix $T_{\\omega} = I - \\omega D^{-1} A$. The iteration converges if and only if $\\rho(T_{\\omega})  1$.\n\nLet $\\mu$ be an eigenvalue of the matrix $D^{-1}A$. Then the corresponding eigenvalue $\\lambda$ of $T_{\\omega}$ is given by:\n$$ \\lambda = 1 - \\omega \\mu $$\nThe spectral radius of $T_{\\omega}$ is the maximum absolute value of its eigenvalues. Since the eigenvalues of $D^{-1}A$ are real and lie in the interval $[\\mu_{\\min}, \\mu_{\\max}]$, the spectral radius is:\n$$ \\rho(T_{\\omega}) = \\max_{\\mu \\in [\\mu_{\\min}, \\mu_{\\max}]} |1 - \\omega \\mu| $$\nFor convergence, we require $\\rho(T_{\\omega})  1$, which translates to the condition:\n$$ -1  1 - \\omega \\mu  1 \\quad \\text{for all } \\mu \\in [\\mu_{\\min}, \\mu_{\\max}] $$\nThe right-hand side inequality, $1 - \\omega \\mu  1$, implies $-\\omega \\mu  0$. Since the matrix $A$ is SPD, its eigenvalues are all positive. The matrix $D$ is a positive scalar multiple of the identity, so $D^{-1}$ is also a positive scalar multiple of the identity. Therefore, the eigenvalues $\\mu$ of $D^{-1}A$ are all positive, meaning $\\mu_{\\min}  0$. The inequality $-\\omega \\mu  0$ thus requires $\\omega  0$.\n\nThe left-hand side inequality, $-1  1 - \\omega \\mu$, implies $\\omega \\mu  2$, or $\\omega  \\frac{2}{\\mu}$. This must hold for all eigenvalues $\\mu$, so it must hold for the largest one, $\\mu_{\\max}$. This gives the condition $\\omega  \\frac{2}{\\mu_{\\max}}$.\n\nCombining these, the interval for $\\omega$ that guarantees convergence is:\n$$ 0  \\omega  \\frac{2}{\\mu_{\\max}} $$\nSubstituting the given expression for $\\mu_{\\max}$:\n$$ 0  \\omega  \\frac{2}{2 \\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right)} = \\frac{1}{\\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right)} = \\sec^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) $$\nThis completes the first part of the task.\n\nFor the second part, we must find the optimal damping parameter $\\omega_{\\text{opt}}$ that minimizes the spectral radius $\\rho(T_{\\omega})$. The function to be minimized is:\n$$ \\rho(T_{\\omega}) = \\max(|1 - \\omega \\mu_{\\min}|, |1 - \\omega \\mu_{\\max}|) $$\nThe function $g(\\mu) = 1 - \\omega \\mu$ is linear in $\\mu$. The minimum of the maximum of $|g(\\mu)|$ over an interval is achieved when the values at the endpoints have equal magnitude, i.e., $|1 - \\omega_{\\text{opt}} \\mu_{\\min}| = |1 - \\omega_{\\text{opt}} \\mu_{\\max}|$. To achieve the smallest possible magnitude, the function $g(\\mu)$ should have opposite signs at the endpoints. Thus, we set:\n$$ 1 - \\omega_{\\text{opt}} \\mu_{\\min} = -(1 - \\omega_{\\text{opt}} \\mu_{\\max}) $$\n$$ 1 - \\omega_{\\text{opt}} \\mu_{\\min} = -1 + \\omega_{\\text{opt}} \\mu_{\\max} $$\n$$ 2 = \\omega_{\\text{opt}}(\\mu_{\\min} + \\mu_{\\max}) $$\nThis yields the optimal damping parameter:\n$$ \\omega_{\\text{opt}} = \\frac{2}{\\mu_{\\min} + \\mu_{\\max}} $$\nThe corresponding minimal spectral radius, which is the worst-case asymptotic convergence factor, is:\n$$ \\rho(T_{\\omega_{\\text{opt}}}) = |1 - \\omega_{\\text{opt}} \\mu_{\\min}| = \\left|1 - \\frac{2 \\mu_{\\min}}{\\mu_{\\min} + \\mu_{\\max}}\\right| = \\left|\\frac{\\mu_{\\min} + \\mu_{\\max} - 2 \\mu_{\\min}}{\\mu_{\\min} + \\mu_{\\max}}\\right| = \\frac{\\mu_{\\max} - \\mu_{\\min}}{\\mu_{\\max} + \\mu_{\\min}} $$\nThe last step holds because $\\mu_{\\max}  \\mu_{\\min}  0$.\n\nNow, we substitute the given expressions for $\\mu_{\\min}$ and $\\mu_{\\max}$ into this formula.\n$$ \\mu_{\\min} = 2 \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) $$\n$$ \\mu_{\\max} = 2 \\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) $$\nThe denominator is:\n$$ \\mu_{\\max} + \\mu_{\\min} = 2 \\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) + 2 \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) = 2\\left(\\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) + \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right)\\right) = 2 $$\nThe numerator is:\n$$ \\mu_{\\max} - \\mu_{\\min} = 2 \\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) - 2 \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) = 2\\left(\\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) - \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right)\\right) $$\nUsing the double-angle identity $\\cos(2\\theta) = \\cos^{2}(\\theta) - \\sin^{2}(\\theta)$, with $\\theta = \\frac{\\pi}{2(N+1)}$, we get:\n$$ \\mu_{\\max} - \\mu_{\\min} = 2 \\cos\\left(2 \\cdot \\frac{\\pi}{2(N+1)}\\right) = 2 \\cos\\left(\\frac{\\pi}{N+1}\\right) $$\nFinally, the worst-case asymptotic convergence factor is:\n$$ \\rho(T_{\\omega_{\\text{opt}}}) = \\frac{2 \\cos\\left(\\frac{\\pi}{N+1}\\right)}{2} = \\cos\\left(\\frac{\\pi}{N+1}\\right) $$\nThis is the final expression for the convergence factor.",
            "answer": "$$\n\\boxed{\\cos\\left(\\frac{\\pi}{N+1}\\right)}\n$$"
        },
        {
            "introduction": "Real-world problems often exhibit features that challenge simple iterative methods. This exercise explores how strong anisotropy in a diffusion problem can dramatically degrade the convergence of the standard Gauss-Seidel method. Using Local Fourier Analysis (LFA), you will quantify this effect by calculating the \"smoothing factor\" for different node orderings, discovering how a problem-aware strategy like line relaxation can restore robust and efficient convergence. This analysis is fundamental to designing effective multigrid solvers for complex PDEs. ",
            "id": "3437865",
            "problem": "Consider the two-dimensional constant-coefficient anisotropic diffusion operator discretized on a uniform Cartesian grid with mesh spacing $h$ in both directions, acting on a grid function $u_{i,j}$ as\n$$\nA u_{i,j} \\;=\\; \\alpha \\left(u_{i-1,j} - 2 u_{i,j} + u_{i+1,j}\\right) \\;+\\; \\beta \\left(u_{i,j-1} - 2 u_{i,j} + u_{i,j+1}\\right),\n$$\nwhere $\\alpha = a_{xx}/h^{2}$ and $\\beta = a_{yy}/h^{2}$ with $a_{xx} \\gg a_{yy}$, and define the anisotropy ratio $\\varepsilon = \\beta/\\alpha \\ll 1$. We study the error-propagation of Gauss–Seidel (GS) stationary iterations for $A u = f$ under three node orderings:\n- natural lexicographic row-wise ordering (left-to-right within a row, then row-by-row in the vertical direction),\n- red–black two-color ordering on the standard five-point stencil adjacency,\n- multicolor line relaxation that treats each horizontal line (fixed $j$) as one block with exact block solve and performs a block Gauss–Seidel sweep in the vertical direction.\n\nStarting from the iteration error propagation definition $e^{(k+1)} = G e^{(k)}$, the Gauss–Seidel splitting $A = L + D + U$ with $G = -\\left(D + L\\right)^{-1} U$, and Local Fourier Analysis (LFA) on the infinite grid using Fourier modes $e_{i,j}(\\boldsymbol{\\theta}) = \\exp\\!\\big(i(\\theta_{x} i + \\theta_{y} j)\\big)$ and their symbols, derive the corresponding error symbols $g(\\theta_{x},\\theta_{y})$ for each ordering. Define the high-frequency set\n$$\n\\Theta_{\\mathrm{H}} \\;=\\; \\left\\{ (\\theta_{x},\\theta_{y}) \\in (-\\pi,\\pi]^{2} \\;:\\; |\\theta_{x}| \\in \\left[\\tfrac{\\pi}{2},\\pi\\right] \\;\\text{or}\\; |\\theta_{y}| \\in \\left[\\tfrac{\\pi}{2},\\pi\\right] \\right\\},\n$$\nand the smoothing factor (the high-frequency spectral radius)\n$$\n\\rho_{\\mathrm{H}}(G) \\;=\\; \\sup_{(\\theta_{x},\\theta_{y}) \\in \\Theta_{\\mathrm{H}}} \\left| g(\\theta_{x},\\theta_{y}) \\right|.\n$$\nQuantify $\\rho_{\\mathrm{H}}(G)$ for each ordering as a function of $\\varepsilon$, identify the ordering that minimizes $\\rho_{\\mathrm{H}}(G)$ in the regime $\\varepsilon \\ll 1$, and compute the minimal value of $\\rho_{\\mathrm{H}}(G)$ as an exact closed-form quantity. Express your final answer as a single exact value (no rounding).",
            "solution": "The problem requires a validation and subsequent analysis of the smoothing properties of Gauss-Seidel iterations for an anisotropic diffusion equation under three different node orderings. The tool for this analysis is Local Fourier Analysis (LFA).\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Discretized operator: $A u_{i,j} = \\alpha (u_{i-1,j} - 2 u_{i,j} + u_{i+1,j}) + \\beta (u_{i,j-1} - 2 u_{i,j} + u_{i,j+1})$\n- Coefficients: $\\alpha = a_{xx}/h^{2}$, $\\beta = a_{yy}/h^{2}$\n- Anisotropy conditions: $a_{xx} \\gg a_{yy}$, and $\\varepsilon = \\beta/\\alpha \\ll 1$\n- Iteration definition: Error propagation $e^{(k+1)} = G e^{(k)}$, with $G = -(D+L)^{-1}U$ for the Gauss-Seidel splitting $A = L+D+U$.\n- LFA modes: $e_{i,j}(\\boldsymbol{\\theta}) = \\exp(i(\\theta_{x} i + \\theta_{y} j))$ for $\\boldsymbol{\\theta} = (\\theta_x, \\theta_y) \\in (-\\pi, \\pi]^2$.\n- Node orderings: (1) Natural lexicographic, (2) Red-black, (3) Horizontal line relaxation.\n- High-frequency set: $\\Theta_{\\mathrm{H}} = \\{ (\\theta_{x},\\theta_{y}) \\in (-\\pi,\\pi]^{2} : |\\theta_{x}| \\in [\\tfrac{\\pi}{2},\\pi] \\;\\text{or}\\; |\\theta_{y}| \\in [\\tfrac{\\pi}{2},\\pi] \\}$.\n- Smoothing factor: $\\rho_{\\mathrm{H}}(G) = \\sup_{(\\theta_{x},\\theta_{y}) \\in \\Theta_{\\mathrm{H}}} | g(\\theta_{x},\\theta_{y}) |$, where $g(\\boldsymbol{\\theta})$ is the symbol of the error propagation operator $G$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard topic in numerical analysis for partial differential equations, specifically multigrid methods. All terms are defined precisely, and the setup is self-contained and mathematically consistent. The task is well-posed: to derive and compare the smoothing factors for three classical iteration schemes. The problem does not violate any fundamental principles, is objective, and leads to a unique, verifiable solution.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of Smoothing Factors\n\nFirst, we find the symbol $a(\\boldsymbol{\\theta})$ of the operator $A$ by applying it to a Fourier mode $e_{i,j}(\\boldsymbol{\\theta})$:\n$$\nA e_{i,j}(\\boldsymbol{\\theta}) = \\left[ \\alpha(e^{-i\\theta_x} - 2 + e^{i\\theta_x}) + \\beta(e^{-i\\theta_y} - 2 + e^{i\\theta_y}) \\right] e_{i,j}(\\boldsymbol{\\theta})\n$$\n$$\na(\\theta_x, \\theta_y) = \\alpha(2\\cos\\theta_x - 2) + \\beta(2\\cos\\theta_y - 2) = -4\\alpha\\sin^2(\\tfrac{\\theta_x}{2}) - 4\\beta\\sin^2(\\tfrac{\\theta_y}{2})\n$$\nThe total diagonal element of the operator $A$ at point $(i,j)$ is $-2(\\alpha+\\beta)$. The error propagation equation for a stationary iteration $e^{(k+1)} = G e^{(k)}$ is analyzed by finding the symbol $g(\\boldsymbol{\\theta})$ of $G$, which is the amplification factor of the mode $\\boldsymbol{\\theta}$.\n\n**1. Lexicographic Gauss-Seidel (Lex-GS)**\nIn the lexicographic ordering (row-wise), the update for the error at node $(i,j)$ uses new values from \"past\" nodes $(i-1,j)$ and $(i,j-1)$:\n$$\n-2(\\alpha+\\beta) e_{i,j}^{(k+1)} + \\alpha e_{i-1,j}^{(k+1)} + \\beta e_{i,j-1}^{(k+1)} + \\alpha e_{i+1,j}^{(k)} + \\beta e_{i,j+1}^{(k)} = 0\n$$\nSubstituting $e_{m,n}^{(k)} = g^k \\exp(i(\\theta_x m + \\theta_y n))$ and dividing by common factors:\n$$\ng(-2(\\alpha+\\beta) + \\alpha e^{-i\\theta_x} + \\beta e^{-i\\theta_y}) + \\alpha e^{i\\theta_x} + \\beta e^{i\\theta_y} = 0\n$$\nSolving for the symbol $g_{\\text{Lex}}(\\boldsymbol{\\theta})$:\n$$\ng_{\\text{Lex}}(\\theta_x, \\theta_y) = -\\frac{\\alpha e^{i\\theta_x} + \\beta e^{i\\theta_y}}{-2(\\alpha+\\beta) + \\alpha e^{-i\\theta_x} + \\beta e^{-i\\theta_y}} = \\frac{\\alpha e^{i\\theta_x} + \\beta e^{i\\theta_y}}{2(\\alpha+\\beta) - \\alpha e^{-i\\theta_x} - \\beta e^{-i\\theta_y}}\n$$\nUsing $\\varepsilon = \\beta/\\alpha$:\n$$\ng_{\\text{Lex}}(\\theta_x, \\theta_y) = \\frac{e^{i\\theta_x} + \\varepsilon e^{i\\theta_y}}{2(1+\\varepsilon) - e^{-i\\theta_x} - \\varepsilon e^{-i\\theta_y}}\n$$\nTo find the smoothing factor, we consider the high-frequency modes that are smoothed the least. For strong anisotropy $\\varepsilon \\ll 1$, the coupling in the $x$-direction dominates. Modes that are smooth in $x$ but oscillatory in $y$ (e.g., $\\theta_x \\approx 0, \\theta_y \\approx \\pi$) are poorly damped. Let's evaluate the amplification factor at $(\\theta_x, \\theta_y) = (0, \\pi) \\in \\Theta_{\\mathrm{H}}$:\n$$\ng_{\\text{Lex}}(0, \\pi) = \\frac{1 + \\varepsilon e^{i\\pi}}{2(1+\\varepsilon) - 1 - \\varepsilon e^{-i\\pi}} = \\frac{1 - \\varepsilon}{2+2\\varepsilon-1+\\varepsilon} = \\frac{1-\\varepsilon}{1+3\\varepsilon}\n$$\nIn the limit $\\varepsilon \\to 0$, $|g_{\\text{Lex}}(0, \\pi)| \\to 1$. Thus, Lex-GS is not an effective smoother for this problem.\n$$\n\\rho_{\\mathrm{H}}(G_{\\text{Lex}}) = \\sup_{\\boldsymbol{\\theta} \\in \\Theta_{\\mathrm{H}}} |g_{\\text{Lex}}(\\boldsymbol{\\theta})| \\ge \\left|\\frac{1-\\varepsilon}{1+3\\varepsilon}\\right| \\xrightarrow{\\varepsilon \\to 0} 1\n$$\n\n**2. Red-Black Gauss-Seidel (RB-GS)**\nFor a two-color ordering with the 5-point stencil, the matrix $A$ is consistently ordered. A known result for consistently ordered matrices is that the eigenvalues of the Gauss-Seidel operator, $\\lambda(G_{\\text{GS}})$, are related to the eigenvalues of the Jacobi operator, $\\lambda(J)$, by $\\lambda(G_{\\text{GS}}) = \\lambda(J)^2$ (plus some zero eigenvalues). The symbol of the Jacobi operator $J = -D^{-1}(L+U)$ is:\n$$\nj(\\theta_x, \\theta_y) = -\\frac{\\alpha(e^{i\\theta_x}+e^{-i\\theta_x}) + \\beta(e^{i\\theta_y}+e^{-i\\theta_y})}{-2(\\alpha+\\beta)} = \\frac{\\alpha \\cos\\theta_x + \\beta \\cos\\theta_y}{\\alpha+\\beta} = \\frac{\\cos\\theta_x + \\varepsilon \\cos\\theta_y}{1+\\varepsilon}\n$$\nThe symbol for RB-GS is thus $g_{\\text{RB}}(\\theta_x, \\theta_y) = [j(\\theta_x, \\theta_y)]^2$:\n$$\ng_{\\text{RB}}(\\theta_x, \\theta_y) = \\left( \\frac{\\cos\\theta_x + \\varepsilon \\cos\\theta_y}{1+\\varepsilon} \\right)^2\n$$\nTo find the smoothing factor, we must maximize $|g_{\\text{RB}}|$ over $\\Theta_{\\mathrm{H}}$. Consider the high-frequency mode $(\\theta_x, \\theta_y) = (\\pi, \\pi) \\in \\Theta_{\\mathrm{H}}$:\n$$\ng_{\\text{RB}}(\\pi, \\pi) = \\left( \\frac{\\cos\\pi + \\varepsilon \\cos\\pi}{1+\\varepsilon} \\right)^2 = \\left( \\frac{-1 - \\varepsilon}{1+\\varepsilon} \\right)^2 = (-1)^2 = 1\n$$\nSince the amplification factor for a high-frequency mode is $1$, RB-GS fails to damp this mode entirely. Thus, it is not a smoother under this definition.\n$$\n\\rho_{\\mathrm{H}}(G_{\\text{RB}}) = 1\n$$\n\n**3. Horizontal Line Relaxation**\nThis is a block Gauss-Seidel method where each block is a horizontal line of nodes. The update for line $j$ uses new values from line $j-1$ and old values from line $j+1$. Within each line, the system is solved exactly. For the error equation, the system for line $j$ is:\n$$\n\\alpha(e_{i-1,j}^{(k+1)} - 2e_{i,j}^{(k+1)} + e_{i+1,j}^{(k+1)}) - 2\\beta e_{i,j}^{(k+1)} + \\beta e_{i,j-1}^{(k+1)} + \\beta e_{i,j+1}^{(k)} = 0\n$$\nWe perform a Fourier transform in the $i$-direction, with mode $\\theta_x$ and amplitude $\\hat{e}_j(\\theta_x)$:\n$$\n\\left( \\alpha(2\\cos\\theta_x-2) - 2\\beta \\right) \\hat{e}_j^{(k+1)}(\\theta_x) + \\beta \\hat{e}_{j-1}^{(k+1)}(\\theta_x) + \\beta \\hat{e}_{j+1}^{(k)}(\\theta_x) = 0\n$$\n$$\n\\left( -4\\alpha\\sin^2(\\tfrac{\\theta_x}{2}) - 2\\beta \\right) \\hat{e}_j^{(k+1)}(\\theta_x) = -\\beta \\hat{e}_{j-1}^{(k+1)}(\\theta_x) - \\beta \\hat{e}_{j+1}^{(k)}(\\theta_x)\n$$\nThis is a 1D vertical GS-like update for each $\\theta_x$ mode. Now, we perform a Fourier transform in the $j$-direction with mode $\\theta_y$ and amplification factor $g(\\theta_x, \\theta_y)$:\n$$\ng \\left( -4\\alpha\\sin^2(\\tfrac{\\theta_x}{2}) - 2\\beta \\right) = -g\\beta e^{-i\\theta_y} - \\beta e^{i\\theta_y}\n$$\nSolving for $g_{\\text{Line}}(\\theta_x, \\theta_y)$:\n$$\ng_{\\text{Line}}(\\theta_x, \\theta_y) = \\frac{\\beta e^{i\\theta_y}}{4\\alpha\\sin^2(\\tfrac{\\theta_x}{2}) + 2\\beta - \\beta e^{-i\\theta_y}} = \\frac{\\varepsilon e^{i\\theta_y}}{4\\sin^2(\\tfrac{\\theta_x}{2}) + 2\\varepsilon - \\varepsilon e^{-i\\theta_y}}\n$$\nWe must find the supremum of $|g_{\\text{Line}}|$ over $\\Theta_{\\mathrm{H}}$. Let's analyze its squared magnitude:\n$$\n|g_{\\text{Line}}|^2 = \\frac{\\varepsilon^2}{|4\\sin^2(\\tfrac{\\theta_x}{2}) + 2\\varepsilon - \\varepsilon e^{-i\\theta_y}|^2} = \\frac{\\varepsilon^2}{(4\\sin^2(\\tfrac{\\theta_x}{2}) + 2\\varepsilon - \\varepsilon\\cos\\theta_y)^2 + (\\varepsilon\\sin\\theta_y)^2}\n$$\nTo maximize this value, we need to minimize the denominator $D(\\theta_x, \\theta_y)$. The term $4\\sin^2(\\tfrac{\\theta_x}{2})$ is non-negative. The denominator is an increasing function of $\\sin^2(\\tfrac{\\theta_x}{2})$. Therefore, we must seek the minimum of the denominator for the smallest possible values of $|\\theta_x|$ allowed within $\\Theta_{\\mathrm{H}}$.\nThe set $\\Theta_{\\mathrm{H}}$ is $\\{ |\\theta_x| \\in [\\tfrac{\\pi}{2},\\pi] \\;\\text{or}\\; |\\theta_y| \\in [\\tfrac{\\pi}{2},\\pi] \\}$.\n- If $|\\theta_x| \\in [\\tfrac{\\pi}{2},\\pi]$, then $\\sin^2(\\tfrac{\\theta_x}{2}) \\ge \\sin^2(\\tfrac{\\pi}{4}) = \\frac{1}{2}$. The term $4\\sin^2(\\tfrac{\\theta_x}{2})$ is at least $2$. For $\\varepsilon \\ll 1$, $|g_{\\text{Line}}|$ will be small, of order $O(\\varepsilon)$.\n- If $|\\theta_y| \\in [\\tfrac{\\pi}{2},\\pi]$, we are free to choose $\\theta_x \\in [-\\pi,\\pi]$. To minimize the denominator, we choose $\\theta_x = 0$.\nSo the supremum will be found along the line $\\theta_x = 0$, with $|\\theta_y| \\in [\\tfrac{\\pi}{2},\\pi]$.\nFor $\\theta_x=0$:\n$$\ng_{\\text{Line}}(0, \\theta_y) = \\frac{\\varepsilon e^{i\\theta_y}}{2\\varepsilon - \\varepsilon e^{-i\\theta_y}} = \\frac{e^{i\\theta_y}}{2 - e^{-i\\theta_y}}\n$$\nThe squared magnitude is:\n$$\n|g_{\\text{Line}}(0, \\theta_y)|^2 = \\frac{1}{|2 - (\\cos\\theta_y + i\\sin\\theta_y)|^{-1}} = \\frac{1}{|2 - \\cos\\theta_y + i\\sin\\theta_y|^2} = \\frac{1}{(2-\\cos\\theta_y)^2 + \\sin^2\\theta_y} = \\frac{1}{5 - 4\\cos\\theta_y}\n$$\nWe need to maximize this function for $|\\theta_y| \\in [\\tfrac{\\pi}{2},\\pi]$, which corresponds to $\\cos\\theta_y \\in [-1, 0]$. The function $f(c) = 1/(5-4c)$ is increasing for $c \\in [-1, 0]$. The maximum is attained at the largest value of $c$, which is $c=\\cos\\theta_y = 0$. This occurs at $\\theta_y = \\pm \\frac{\\pi}{2}$.\nThe maximum value of $|g_{\\text{Line}}(0, \\theta_y)|^2$ is $\\frac{1}{5 - 4(0)} = \\frac{1}{5}$.\nThe smoothing factor is the square root of this value.\n$$\n\\rho_{\\mathrm{H}}(G_{\\text{Line}}) = \\sqrt{\\frac{1}{5}} = \\frac{1}{\\sqrt{5}}\n$$\n\n### Conclusion\nComparing the smoothing factors for $\\varepsilon \\ll 1$:\n- Lexicographic GS: $\\rho_{\\mathrm{H}}(G_{\\text{Lex}}) \\approx 1$\n- Red-Black GS: $\\rho_{\\mathrm{H}}(G_{\\text{RB}}) = 1$\n- Horizontal Line Relaxation: $\\rho_{\\mathrm{H}}(G_{\\text{Line}}) = \\frac{1}{\\sqrt{5}}$\n\nThe horizontal line relaxation scheme is the only effective smoother among the three for the given anisotropic problem. It is the ordering that minimizes the smoothing factor. The minimal value of $\\rho_{\\mathrm{H}}(G)$ is $\\frac{1}{\\sqrt{5}}$.",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{5}}}$$"
        },
        {
            "introduction": "This final practice bridges the gap between pen-and-paper theory and computational reality. While the exact spectral radius of an iteration matrix dictates its long-term behavior, it can be difficult to compute for large-scale systems. This coding exercise challenges you to implement the Gauss-Seidel method and compare its exact, numerically computed spectral radius against theoretical upper bounds derived from tools like the Gershgorin circle theorem. By testing these estimates on several benchmark problems, you will develop a practical intuition for the utility and limitations of these important analytical tools. ",
            "id": "3437823",
            "problem": "Consider linear systems arising from standard finite difference discretizations of second-order symmetric positive definite (SPD) elliptic partial differential equations on Cartesian grids. Let $A \\in \\mathbb{R}^{N \\times N}$ denote an SPD matrix obtained from either one-dimensional or two-dimensional five-point stencils with homogeneous Dirichlet boundary conditions. For a given right-hand side $f \\in \\mathbb{R}^{N}$, the Gauss–Seidel (GS) method is defined by splitting $A = D + L + U$, where $D$ is the diagonal of $A$, $L$ is the strict lower triangular part, and $U$ is the strict upper triangular part. The GS iteration can be written as $x^{(k+1)} = G x^{(k)} + c$, where the error-propagation (iteration) matrix is $G = -(D + L)^{-1} U$. The asymptotic convergence rate of the method in any norm is governed by the spectral radius $\\rho(G) = \\max \\{ |\\lambda| : \\lambda \\in \\sigma(G) \\}$, where $\\sigma(G)$ denotes the spectrum of $G$.\n\nYour task is to estimate $\\rho(G)$ using two classical tools from matrix analysis and to test the tightness of these estimates on benchmark elliptic problems.\n\nFoundational bases to use:\n- Gershgorin circle theorem: For any matrix $M = (m_{ij}) \\in \\mathbb{C}^{N \\times N}$, all eigenvalues lie in the union of Gershgorin discs $\\{ z \\in \\mathbb{C} : |z - m_{ii}| \\le \\sum_{j \\ne i} |m_{ij}| \\}$ for $i = 1, \\dots, N$.\n- Hoffman–Wielandt theorem (symmetric case): If $H, \\widetilde{H} \\in \\mathbb{R}^{N \\times N}$ are symmetric with eigenvalues $\\lambda_1(H), \\dots, \\lambda_N(H)$ and $\\lambda_1(\\widetilde{H}), \\dots, \\lambda_N(\\widetilde{H})$, then there exists a permutation $\\pi$ such that $\\sum_{i=1}^N \\left( \\lambda_i(H) - \\lambda_{\\pi(i)}(\\widetilde{H}) \\right)^2 \\le \\| H - \\widetilde{H} \\|_F^2$, where $\\| \\cdot \\|_F$ is the Frobenius norm.\n\nRequired estimation designs:\n- Gershgorin-based upper bound for $\\rho(G)$: Since all eigenvalues of $G$ lie within the union of Gershgorin discs, define the bound $B_{\\mathrm{Gersh}} := \\max_{1 \\le i \\le N} \\left( |g_{ii}| + \\sum_{j \\ne i} |g_{ij}| \\right )$, where $G = (g_{ij})$. Then $\\rho(G) \\le B_{\\mathrm{Gersh}}$.\n- Hoffman–Wielandt-based upper bound via a symmetric surrogate: For any matrix $G$, its spectral radius satisfies $\\rho(G) \\le \\|G\\|_2 = \\sqrt{\\lambda_{\\max}(G^{\\top} G)}$. Construct a periodic wrap-around surrogate $\\widetilde{A}$ by imposing periodic boundary couplings on the same stencil and coefficients as $A$ (this $\\widetilde{A}$ is symmetric but may be singular). Define $\\widetilde{G} = -(\\widetilde{D} + \\widetilde{L})^{-1} \\widetilde{U}$ from the splitting $\\widetilde{A} = \\widetilde{D} + \\widetilde{L} + \\widetilde{U}$. Let $H := G^{\\top} G$ and $\\widetilde{H} := \\widetilde{G}^{\\top} \\widetilde{G}$. Since $H$ and $\\widetilde{H}$ are symmetric, the Hoffman–Wielandt theorem implies there exists an ordering of eigenvalues such that $|\\lambda_{\\max}(H) - \\lambda_{j}(\\widetilde{H})| \\le \\|H - \\widetilde{H}\\|_F$ for some index $j$. Therefore, using $\\lambda_{\\max}(\\widetilde{H}) \\ge \\lambda_{j}(\\widetilde{H})$ and the inequality $\\|\\cdot\\|_2 \\le \\|\\cdot\\|_F$, we obtain the computable bound $\\lambda_{\\max}(H) \\le \\lambda_{\\max}(\\widetilde{H}) + \\|H - \\widetilde{H}\\|_F$. Define $B_{\\mathrm{HW}} := \\sqrt{ \\lambda_{\\max}(\\widetilde{H}) + \\|H - \\widetilde{H}\\|_F }$, which yields $\\rho(G) \\le B_{\\mathrm{HW}}$.\n\nBenchmark matrices $A$ to be used:\n- One-dimensional Poisson operator with homogeneous Dirichlet boundary conditions on a uniform grid of $n$ interior points, discrete stencil $[-1, 2, -1]$ (no scaling by mesh size is necessary).\n- Two-dimensional anisotropic diffusion operator on an $n_x \\times n_y$ grid with homogeneous Dirichlet boundary conditions and constant coefficients $a_x  0$ and $a_y  0$, realized by the Kronecker sum $A = a_x (I_{n_y} \\otimes T_{n_x}) + a_y (T_{n_y} \\otimes I_{n_x})$, where $T_{m}$ is the tridiagonal matrix with diagonal entries $2$ and off-diagonal entries $-1$.\n\nFor the periodic wrap-around surrogates $\\widetilde{A}$, use the same stencils but connect boundary points periodically in each spatial direction to create circulant couplings.\n\nImplement a program that, for each test case, constructs $A$, computes $G$, and returns the following three quantities:\n- the exact spectral radius $\\rho(G)$,\n- the Gershgorin upper bound $B_{\\mathrm{Gersh}}$,\n- the Hoffman–Wielandt-based upper bound $B_{\\mathrm{HW}}$.\n\nTest suite to cover diverse scenarios:\n- Test $1$: One-dimensional Poisson, Dirichlet, $n = 10$.\n- Test $2$: Two-dimensional Poisson, Dirichlet, $n_x = 8$, $n_y = 8$, $a_x = 1$, $a_y = 1$.\n- Test $3$: Two-dimensional anisotropic diffusion, Dirichlet, $n_x = 8$, $n_y = 8$, $a_x = 1$, $a_y = 0.01$.\n- Test $4$: Two-dimensional anisotropic diffusion, Dirichlet, $n_x = 8$, $n_y = 8$, $a_x = 0.1$, $a_y = 1$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Specifically, output the concatenated sequence $[\\rho(G)_1, B_{\\mathrm{Gersh},1}, B_{\\mathrm{HW},1}, \\rho(G)_2, B_{\\mathrm{Gersh},2}, B_{\\mathrm{HW},2}, \\rho(G)_3, B_{\\mathrm{Gersh},3}, B_{\\mathrm{HW},3}, \\rho(G)_4, B_{\\mathrm{Gersh},4}, B_{\\mathrm{HW},4}]$, where the subscript indicates the test case number. All values must be real numbers (floating-point).",
            "solution": "The problem is valid as it presents a well-defined task in numerical linear algebra, based on established mathematical principles and theorems. The problem statement is self-contained, scientifically grounded, and formalizable. The objective is to compute and compare the exact spectral radius of the Gauss-Seidel iteration matrix with two distinct upper bounds for a set of benchmark problems.\n\nThe core of the problem lies in analyzing the asymptotic convergence rate of the Gauss-Seidel method for a linear system $A\\mathbf{x} = \\mathbf{f}$, where $A$ is a symmetric positive definite (SPD) matrix. The convergence is governed by the spectral radius, $\\rho(G)$, of the iteration matrix $G = -(D+L)^{-1}U$, derived from the splitting $A = D+L+U$. Here, $D$ is the diagonal part of $A$, $L$ is the strict lower triangular part, and $U$ is the strict upper triangular part.\n\nFirst, we must construct the problem matrices $A$ and their periodic surrogates $\\widetilde{A}$ for both one-dimensional and two-dimensional cases.\n\n**Matrix Construction**\n\nFor the one-dimensional case with $n$ interior grid points, the matrix $A$ corresponds to the discrete Laplacian with homogeneous Dirichlet boundary conditions. It is an $n \\times n$ tridiagonal matrix, denoted $T_n$:\n$$\nT_n = \\begin{pmatrix}\n2  -1   \\\\\n-1  2  -1  \\\\\n \\ddots  \\ddots  \\ddots \\\\\n  -1  2  -1 \\\\\n   -1  2\n\\end{pmatrix}\n$$\nThe periodic surrogate, $\\widetilde{A} = \\widetilde{T}_n$, corresponds to the same stencil but on a periodic domain. This results in a circulant matrix. For $n  2$, its first row is $(2, -1, 0, \\dots, 0, -1)$. The cases $n=1$ and $n=2$ require special handling due to aliasing of neighbors, resulting in $\\widetilde{T}_1 = (0)$ and $\\widetilde{T}_2 = \\begin{pmatrix} 2  -2 \\\\ -2  2 \\end{pmatrix}$.\n\nFor the two-dimensional case on an $n_x \\times n_y$ grid, the matrix $A$ for the anisotropic diffusion operator is constructed using the Kronecker sum. Given diffusion coefficients $a_x  0$ and $a_y  0$, the matrix $A$ is of size $N \\times N$ with $N=n_x n_y$:\n$$\nA = a_x (I_{n_y} \\otimes T_{n_x}) + a_y (T_{n_y} \\otimes I_{n_x})\n$$\nwhere $I_m$ is the $m \\times m$ identity matrix and $\\otimes$ denotes the Kronecker product. This construction models homogeneous Dirichlet boundary conditions on the rectangular domain. The periodic surrogate $\\widetilde{A}$ is constructed similarly, but using the 1D periodic matrices $\\widetilde{T}_m$ to enforce periodicity in both spatial directions:\n$$\n\\widetilde{A} = a_x (I_{n_y} \\otimes \\widetilde{T}_{n_x}) + a_y (\\widetilde{T}_{n_y} \\otimes I_{n_x})\n$$\n\n**Computation of Spectral Radius and Bounds**\n\nFor each test case, we compute three quantities:\n\n1.  **Exact Spectral Radius $\\rho(G)$**: After constructing $A$, we form $D$, $L$, and $U$. The iteration matrix $G=-(D+L)^{-1}U$ is computed by solving the matrix equation $(D+L)G = -U$, which is a numerically stable approach. Then, the eigenvalues $\\lambda_i$ of $G$ are computed, and the spectral radius is found as $\\rho(G) = \\max_i |\\lambda_i|$.\n\n2.  **Gershgorin-based Bound $B_{\\mathrm{Gersh}}$**: This bound is derived from the Gershgorin circle theorem, which states that all eigenvalues of $G=(g_{ij})$ lie in the union of discs $D_i = \\{z \\in \\mathbb{C} : |z - g_{ii}| \\le \\sum_{j \\ne i} |g_{ij}|\\}$. A simple upper bound for $\\rho(G)$ is thus the maximum radius of these discs when centered at the origin, which is equivalent to the matrix infinity-norm. The problem defines the bound as:\n    $$\n    B_{\\mathrm{Gersh}} = \\max_{1 \\le i \\le N} \\left( |g_{ii}| + \\sum_{j \\ne i} |g_{ij}| \\right) = \\|G\\|_{\\infty}\n    $$\n    This is calculated by summing the absolute values of the elements in each row of $G$ and taking the maximum of these sums.\n\n3.  **Hoffman–Wielandt-based Bound $B_{\\mathrm{HW}}$**: This bound leverages a symmetric surrogate matrix. We define $H = G^\\top G$ and its periodic counterpart $\\widetilde{H} = \\widetilde{G}^\\top \\widetilde{G}$. The spectral radius of $G$ is related to $H$ by $\\rho(G) \\le \\|G\\|_2 = \\sqrt{\\lambda_{\\max}(H)}$. Using Weyl's inequality for the eigenvalues of symmetric matrices, we can bound $\\lambda_{\\max}(H)$:\n    $$\n    \\lambda_{\\max}(H) = \\lambda_{\\max}(\\widetilde{H} + (H - \\widetilde{H})) \\le \\lambda_{\\max}(\\widetilde{H}) + \\lambda_{\\max}(H - \\widetilde{H})\n    $$\n    Since $\\lambda_{\\max}(M) \\le \\|M\\|_2 \\le \\|M\\|_F$ for any matrix $M$, we have:\n    $$\n    \\lambda_{\\max}(H) \\le \\lambda_{\\max}(\\widetilde{H}) + \\|H - \\widetilde{H}\\|_F\n    $$\n    This leads to the defined bound on $\\rho(G)$:\n    $$\n    B_{\\mathrm{HW}} = \\sqrt{ \\lambda_{\\max}(\\widetilde{H}) + \\|H - \\widetilde{H}\\|_F }\n    $$\n    To compute this, we construct $\\widetilde{A}$ and subsequently $\\widetilde{G}$, form $H$ and $\\widetilde{H}$, compute the maximum eigenvalue of $\\widetilde{H}$ (which is symmetric, allowing for efficient algorithms), and find the Frobenius norm of the difference $H - \\widetilde{H}$.\n\nThe described procedure is applied systematically to each of the four test cases specified in the problem statement. The matrices involved are dense but of manageable size ($10 \\times 10$ and $64 \\times 64$), making direct computation feasible. The results provide a quantitative comparison of the true convergence rate against the estimates provided by the two theoretical bounds.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef construct_T(n):\n    \"\"\"Constructs the 1D Poisson matrix T_n of size n x n.\"\"\"\n    T = np.zeros((n, n), dtype=float)\n    np.fill_diagonal(T, 2.0)\n    if n > 1:\n        off_diag = np.ones(n - 1) * -1.0\n        np.fill_diagonal(T[1:, :], off_diag)\n        np.fill_diagonal(T[:, 1:], off_diag)\n    return T\n\ndef construct_T_tilde(n):\n    \"\"\"Constructs the 1D periodic Poisson matrix of size n x n.\"\"\"\n    if n == 1:\n        return np.array([[0.0]])\n    if n == 2:\n        return np.array([[2.0, -2.0], [-2.0, 2.0]])\n    \n    # For n > 2, the matrix is circulant. Its first column/row determines it.\n    first_col = np.zeros(n, dtype=float)\n    first_col[0] = 2.0\n    first_col[1] = -1.0\n    first_col[-1] = -1.0 # Index -1 corresponds to n-1\n    return linalg.circulant(first_col)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'type': '1D', 'n': 10},\n        {'type': '2D', 'nx': 8, 'ny': 8, 'ax': 1.0, 'ay': 1.0},\n        {'type': '2D', 'nx': 8, 'ny': 8, 'ax': 1.0, 'ay': 0.01},\n        {'type': '2D', 'nx': 8, 'ny': 8, 'ax': 0.1, 'ay': 1.0}\n    ]\n\n    results = []\n    for case in test_cases:\n        # Step 1: Construct matrices A (Dirichlet) and A_tilde (Periodic)\n        if case['type'] == '1D':\n            n = case['n']\n            A = construct_T(n)\n            A_tilde = construct_T_tilde(n)\n        else: # type == '2D'\n            nx, ny = case['nx'], case['ny']\n            ax, ay = case['ax'], case['ay']\n            \n            # Dirichlet matrix A\n            Tn_x = construct_T(nx)\n            Tn_y = construct_T(ny)\n            In_x = np.identity(nx)\n            In_y = np.identity(ny)\n            A = ax * np.kron(In_y, Tn_x) + ay * np.kron(Tn_y, In_x)\n\n            # Periodic surrogate A_tilde\n            T_tilde_nx = construct_T_tilde(nx)\n            T_tilde_ny = construct_T_tilde(ny)\n            # Fully periodic in both directions\n            A_tilde = ax * np.kron(In_y, T_tilde_nx) + ay * np.kron(T_tilde_ny, In_x)\n\n        # Step 2: Compute Gauss-Seidel iteration matrices G and G_tilde\n        # Split A = D + L + U\n        D = np.diag(np.diag(A))\n        L = np.tril(A, k=-1)\n        U = np.triu(A, k=1)\n        # G = -(D+L)^-1 * U. This is equivalent to solving (D+L)X = -U for X.\n        G = linalg.solve_triangular(D + L, -U, lower=True)\n\n        # Split A_tilde = D_tilde + L_tilde + U_tilde\n        D_tilde = np.diag(np.diag(A_tilde))\n        L_tilde = np.tril(A_tilde, k=-1)\n        U_tilde = np.triu(A_tilde, k=1)\n        # G_tilde = -(D_tilde + L_tilde)^-1 * U_tilde\n        G_tilde = linalg.solve_triangular(D_tilde + L_tilde, -U_tilde, lower=True)\n        \n        # Step 3: Calculate the three required quantities\n        # 1. Exact spectral radius rho(G)\n        eigenvalues_G = linalg.eigvals(G)\n        rho_G = np.max(np.abs(eigenvalues_G))\n        \n        # 2. Gershgorin bound B_Gersh\n        # B_Gersh = max_i sum_j |g_ij|, which is the infinity-norm of G.\n        B_Gersh = np.max(np.sum(np.abs(G), axis=1))\n\n        # 3. Hoffman-Wielandt-based bound B_HW\n        H = G.T @ G\n        H_tilde = G_tilde.T @ G_tilde\n        \n        # H and H_tilde are symmetric positive semi-definite. Use eigvalsh for accuracy/stability.\n        lambda_max_H_tilde = np.max(linalg.eigvalsh(H_tilde))\n        \n        fro_norm_diff = linalg.norm(H - H_tilde, 'fro')\n        \n        B_HW = np.sqrt(lambda_max_H_tilde + fro_norm_diff)\n        \n        results.extend([rho_G, B_Gersh, B_HW])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}