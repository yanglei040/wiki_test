{
    "hands_on_practices": [
        {
            "introduction": "Before analyzing the convergence of an iterative method, it's crucial to understand its computational cost. This practice grounds the abstract concept of the Successive Over-Relaxation (SOR) method in the practical realities of implementation by analyzing the number of floating-point operations (flops) required per iteration. By also exploring the memory access patterns of SOR's \"in-place\" updates compared to other methods like Jacobi, you will develop skills in algorithmic analysis and appreciate the fundamental trade-offs between computational cost, memory usage, and potential for parallelism .",
            "id": "3451653",
            "problem": "Consider a sparse linear system $A x = b$ arising from a finite-difference discretization of a linear elliptic partial differential equation on a structured grid, where $A \\in \\mathbb{R}^{n \\times n}$ is real symmetric positive definite and stored in Compressed Sparse Row (CSR) format. Let $m = \\operatorname{nnz}(A)$ denote the number of nonzero entries in $A$. Assume the Successive Over-Relaxation (SOR) method with relaxation parameter $\\omega \\in (0,2)$ is applied in-place, meaning that each entry $x_{i}$ is overwritten as soon as it is updated. Use the standard splitting $A = D + L + U$, where $D$ is the diagonal, $L$ the strict lower triangular part, and $U$ the strict upper triangular part.\n\nYou may assume the following implementation model:\n- The update of $x_{i}$ in a forward SOR sweep uses the sum of all off-diagonal contributions in row $i$, computed as a dot product $s_{i} = \\sum_{j \\neq i} a_{ij} x_{j}$, with the convention that $x_{j}$ is the most recently available value (new for $j  i$, old for $j  i$).\n- The new value is computed by $x_{i}^{\\text{new}} = (1 - \\omega) x_{i}^{\\text{old}} + \\alpha_{i} \\left( b_{i} - s_{i} \\right)$ with $\\alpha_{i} = \\omega / a_{ii}$.\n- The diagonal inverses $a_{ii}^{-1}$ and the scaled diagonal coefficients $\\alpha_{i}$ are precomputed prior to the iteration and available at no per-iteration cost.\n\nAdopt the following flop-counting conventions:\n- Count only floating-point additions and multiplications. Divisions are excluded by the precomputation of $\\alpha_{i}$.\n- For the accumulation of $s_{i}$, each off-diagonal nonzero $a_{ij}$ contributes one multiplication and one addition, both counted.\n- The operations to form $b_{i} - s_{i}$ and to combine the two terms in $x_{i}^{\\text{new}} = (1 - \\omega) x_{i}^{\\text{old}} + \\alpha_{i} \\left( b_{i} - s_{i} \\right)$ are counted explicitly as addition/multiplication flops.\n\nTasks:\n1. Derive from first principles the exact flop count per forward SOR sweep in terms of $m$ and $n$ under the above assumptions.\n2. Explain how a backward sweep yields the same flop count and conclude the cost per symmetric SOR (SSOR) iteration consisting of one forward and one backward sweep.\n3. Discuss, using the definition of the SOR update and the CSR storage model, how in-place updates during SOR/SSOR affect data access patterns compared to a Jacobi iteration, which uses two separate vectors $x^{(k)}$ and $x^{(k+1)}$ so that updates are out-of-place.\n\nExpress your final answer as a single closed-form analytic expression giving the total per-iteration flop cost of a single forward SOR sweep in terms of $m$ and $n$ only. No rounding is required, and no physical units apply. The discussion is required to justify the derivation but will not affect the final numerical expression requested.",
            "solution": "The problem is well-posed, scientifically grounded, and provides a clear set of assumptions and definitions for a rigorous analysis. We proceed with the derivation.\n\nThe primary task is to determine the exact number of floating-point operations (flops), defined as additions and multiplications, required for one complete forward sweep of the Successive Over-Relaxation (SOR) method.\n\nLet the linear system be $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a real symmetric positive definite matrix with $m = \\operatorname{nnz}(A)$ nonzero entries. The matrix is stored in Compressed Sparse Row (CSR) format. The SOR method updates each component $x_i$ of the solution vector $x$ for $i=1, 2, \\dots, n$ according to the provided formula:\n$$x_{i}^{\\text{new}} = (1 - \\omega) x_{i}^{\\text{old}} + \\alpha_{i} \\left( b_{i} - s_{i} \\right)$$\nwhere $\\omega \\in (0,2)$ is the relaxation parameter, $\\alpha_{i} = \\omega / a_{ii}$, and $s_{i} = \\sum_{j \\neq i} a_{ij} x_{j}$. The sum $s_i$ uses the most recently updated values of $x_j$, meaning $x_j^{\\text{new}}$ for $j  i$ and $x_j^{\\text{old}}$ for $j  i$.\n\nWe will determine the flop count for the update of a single component $x_i$ and then sum over all $n$ components to find the total for a full sweep.\n\n**1. Flop Count per Forward SOR Sweep**\n\nLet's analyze the computational cost for updating a single component $x_i$. The calculation proceeds in two main stages as defined by the formula.\n\n**Stage 1: Computation of the off-diagonal sum $s_i$.**\nThe sum is given by $s_{i} = \\sum_{j \\neq i} a_{ij} x_{j}$. Let $k_i$ be the number of off-diagonal nonzero entries in row $i$ of matrix $A$. The total number of nonzero entries in row $i$ is $k_i + 1$ (including the diagonal element $a_{ii}$).\nAccording to the problem's flop-counting convention, \"each off-diagonal nonzero $a_{ij}$ contributes one multiplication and one addition\".\nTherefore, computing $s_i$ requires:\n- $k_i$ multiplications (for each $a_{ij} x_j$ term).\n- $k_i$ additions (to accumulate these terms, assuming the sum is initialized to $0$).\nThe cost for computing $s_i$ is $k_i$ multiplications and $k_i$ additions.\n\n**Stage 2: Computation of the final value $x_i^{\\text{new}}$.**\nThe update formula is $x_{i}^{\\text{new}} = (1 - \\omega) x_{i}^{\\text{old}} + \\alpha_{i} ( b_{i} - s_{i} )$. We count the operations in sequence:\n- Subtraction: $b_i - s_i$. This is $1$ addition flop.\n- Multiplication: $\\alpha_i \\times (b_i - s_i)$. The parameter $\\alpha_i$ is precomputed, so this is $1$ multiplication flop.\n- Multiplication: $(1 - \\omega) \\times x_i^{\\text{old}}$. The term $(1 - \\omega)$ is a constant throughout the iteration and can be precomputed before the sweep begins. This is $1$ multiplication flop.\n- Addition: The final sum of the two terms $(1 - \\omega) x_i^{\\text{old}}$ and $\\alpha_i ( b_i - s_i )$. This is $1$ addition flop.\n\nIn total, Stage 2 requires $1+1=2$ multiplications and $1+1=2$ additions per component $x_i$.\n\n**Total cost per component $x_i$:**\nCombining both stages, the total number of flops to update $x_i$ is:\n- Total multiplications: $k_i (\\text{from } s_i) + 2 (\\text{from update}) = k_i + 2$.\n- Total additions: $k_i (\\text{from } s_i) + 2 (\\text{from update}) = k_i + 2$.\nThe total flop count for updating $x_i$ is $(k_i+2) + (k_i+2) = 2(k_i+2)$.\n\n**Total cost for a full forward sweep:**\nTo find the total cost for one full forward sweep (for $i=1, \\dots, n$), we sum the costs for each component:\n- Total multiplications = $\\sum_{i=1}^{n} (k_i + 2) = \\left(\\sum_{i=1}^{n} k_i\\right) + \\left(\\sum_{i=1}^{n} 2\\right)$.\n- Total additions = $\\sum_{i=1}^{n} (k_i + 2) = \\left(\\sum_{i=1}^{n} k_i\\right) + \\left(\\sum_{i=1}^{n} 2\\right)$.\n\nThe term $\\sum_{i=1}^{n} k_i$ represents the sum of the number of off-diagonal nonzero entries over all rows. This is precisely the total number of off-diagonal nonzero entries in the matrix $A$. Given that the total number of nonzeros is $m = \\operatorname{nnz}(A)$ and the matrix has $n$ diagonal entries (since $A$ is from a PDE discretization, $a_{ii} \\neq 0$), the total number of off-diagonal nonzeros is $m-n$.\nSo, $\\sum_{i=1}^{n} k_i = m - n$.\n\nThe term $\\sum_{i=1}^{n} 2$ is simply $2n$.\n\nSubstituting these results back:\n- Total multiplications = $(m - n) + 2n = m + n$.\n- Total additions = $(m - n) + 2n = m + n$.\nThe total flop count for a single forward SOR sweep is the sum of total multiplications and total additions:\n- Total Flops = $(m+n) + (m+n) = 2(m+n)$.\n\n**2. Backward Sweep and Symmetric SOR (SSOR) Cost**\n\nA backward SOR sweep performs the same set of calculations but for $i=n, n-1, \\dots, 1$. The update formula for each $x_i$ is identical. Although the values of $x_j$ used in the sum $s_i$ are different (newly computed values are for $ji$), the *number* of arithmetic operations to update a single component $x_i$ remains $2(k_i+2)$. Consequently, the total flop count for a full backward sweep is identical to the forward sweep: $2(m+n)$.\n\nA single iteration of the Symmetric SOR (SSOR) method consists of one full forward sweep followed by one full backward sweep. Therefore, the total per-iteration flop cost for SSOR is the sum of the costs of the two sweeps:\n- SSOR Flop Cost = (Forward Sweep Cost) + (Backward Sweep Cost) = $2(m+n) + 2(m+n) = 4(m+n)$.\n\n**3. Data Access Patterns: In-place SOR/SSOR vs. Out-of-place Jacobi**\n\nThe choice between an in-place update scheme (like SOR) and an out-of-place scheme (like Jacobi) has significant implications for data access patterns and parallelizability.\n\n**SOR/SSOR (In-place):**\n- The update for $x_i^{(k+1)}$ depends on values from the same iteration, $x_j^{(k+1)}$ for $ji$ (in a forward sweep), as well as values from the previous iteration, $x_j^{(k)}$ for $ji$. This introduces a loop-carried dependency.\n- All reads and writes for the solution vector are performed on a single memory buffer. For each $i$, the computation reads previously computed values from $x$ (both old and new) to calculate $s_i$, and then overwrites $x_i$ with its new value.\n- This sequential dependency makes standard SOR difficult to parallelize. The computation of $x_{i+1}$ cannot begin until $x_i$ is finalized. While techniques like wavefront parallelization (for structured grids) exist, they are more complex than the parallelism available in Jacobi.\n- In CSR format, the algorithm iterates through rows $1$ to $n$. For each row $i$, it accesses a contiguous block of memory in the CSR arrays for $a_{ij}$ and $col_ind$, but the corresponding accesses to the vector $x$ at indices $j$ are sparse and irregular. The key feature is the read-after-write hazards on the single $x$ vector.\n\n**Jacobi (Out-of-place):**\n- The Jacobi update is $x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j \\neq i} a_{ij} x_j^{(k)}\\right)$. Every component of the new vector $x^{(k+1)}$ is computed using only components from the old vector $x^{(k)}$.\n- This requires two separate storage locations for the solution vector, one for $x^{(k)}$ (read-only) and one for $x^{(k+1)}$ (write-only) within a single iteration. This doubles the memory requirement for the solution vector compared to SOR.\n- The major advantage is the absence of data dependencies between the computations for different components of $x^{(k+1)}$. All $n$ updates can be performed simultaneously and independently. This property is often called \"embarrassingly parallel\".\n- The data access pattern is simpler: it involves streaming reads from the matrix $A$ and the vector $x^{(k)}$, and streaming writes to the vector $x^{(k+1)}$. This is highly friendly to modern parallel architectures (multicore CPUs, GPUs) and vectorization, leading to high computational throughput despite Jacobi's typically slower mathematical convergence rate compared to SOR.\n\nIn summary, the in-place nature of SOR/SSOR is memory-efficient but creates sequential dependencies that hinder parallelism. The out-of-place nature of Jacobi requires more memory but allows for trivial and highly efficient parallelization.",
            "answer": "$$\n\\boxed{2(m+n)}\n$$"
        },
        {
            "introduction": "The convergence speed of an iterative method is governed by the spectral radius of its iteration matrix, but this quantity is often difficult to compute for general matrices. This exercise provides a valuable opportunity to analyze it exactly for an important model problem: the 1D discrete Laplacian with periodic boundary conditions. Using Fourier analysis and the properties of circulant matrices, you will derive the precise eigenvalues and spectral radius of the SOR iteration matrix, gaining a deeper intuition for how the relaxation parameter $\\omega$ and the problem size $n$ directly influence the convergence rate .",
            "id": "3451637",
            "problem": "Consider the one-dimensional Poisson equation with periodic boundary conditions on the unit interval, discretized on a uniform grid of $n \\geq 4$ points. Using the standard three-point stencil, the discrete periodic Laplacian yields the linear system $A u = f$, where $A \\in \\mathbb{R}^{n \\times n}$ is circulant with entries $2$ on the diagonal, $-1$ on the first sub- and super-diagonals, and $-1$ in the two corner positions. Let $P \\in \\mathbb{R}^{n \\times n}$ denote the cyclic downshift permutation matrix, defined by $(P x)_{i} = x_{i-1}$ with indices modulo $n$, so that $A = 2 I - P - P^{\\top}$.\n\nDefine the Successive Over-Relaxation (SOR) iteration matrix by the splitting $A = D - L - U$ with $D = 2 I$, $L = P$, and $U = P^{\\top}$:\n$$\nT_{\\mathrm{SOR}}(\\omega) \\;=\\; (D - \\omega L)^{-1}\\big((1 - \\omega) D + \\omega U\\big),\n$$\nfor a relaxation parameter $\\omega \\in (0,2)$.\n\n1. Starting from the properties of circulant matrices and the Discrete Fourier Transform (DFT), derive the exact eigenvectors and eigenvalues of $T_{\\mathrm{SOR}}(\\omega)$ for this periodic problem. Make explicit the eigenpairs indexed by the discrete Fourier modes.\n\n2. The constant vector is an eigenvector of $A$ associated with the eigenvalue $0$, and therefore $T_{\\mathrm{SOR}}(\\omega)$ has eigenvalue $1$ on this mode. Consequently, define the effective spectral radius $\\rho_{\\perp}(T_{\\mathrm{SOR}})$ to be the spectral radius of $T_{\\mathrm{SOR}}(\\omega)$ restricted to the subspace orthogonal to the constants (i.e., after excluding the zero-frequency Fourier mode). Using your eigenvalue formula, obtain a closed-form expression for $\\rho_{\\perp}(T_{\\mathrm{SOR}})$ as a function of $n$ and $\\omega$.\n\n3. Finally, evaluate $\\rho_{\\perp}(T_{\\mathrm{SOR}})$ for $n = 64$ and $\\omega = 1.5$, rounding your answer to four significant figures. Angles, if any appear in intermediate steps, must be treated in radians. Your final answer must be a single real number without units.",
            "solution": "The problem is first validated against the specified criteria.\n\n**Problem Validation**\n\n1.  **Extract Givens**:\n    -   System: 1D Poisson equation, periodic boundary conditions, unit interval, uniform grid of $n \\geq 4$ points.\n    -   Linear system: $A u = f$.\n    -   Matrix $A$: $A \\in \\mathbb{R}^{n \\times n}$ is circulant with diagonal entries $2$, first sub/super-diagonal entries $-1$, and corner entries $-1$.\n    -   Matrix representation: $A = 2 I - P - P^{\\top}$, where $P$ is the cyclic downshift permutation matrix.\n    -   SOR splitting: $A = D - L - U$ with $D = 2 I$, $L = P$, and $U = P^{\\top}$.\n    -   SOR iteration matrix: $T_{\\mathrm{SOR}}(\\omega) = (D - \\omega L)^{-1}\\big((1 - \\omega) D + \\omega U\\big)$ for $\\omega \\in (0,2)$.\n    -   Task 1: Derive eigenvectors and eigenvalues of $T_{\\mathrm{SOR}}(\\omega)$.\n    -   Task 2: Derive a closed-form expression for the effective spectral radius $\\rho_{\\perp}(T_{\\mathrm{SOR}})$, which excludes the zero-frequency mode.\n    -   Task 3: Evaluate $\\rho_{\\perp}(T_{\\mathrm{SOR}})$ for $n = 64$ and $\\omega = 1.5$ to four significant figures.\n\n2.  **Validate**: The problem is scientifically grounded in numerical linear algebra. All terms are standard and well-defined. The premise that the discrete periodic Laplacian is a circulant matrix is correct. The structure of $A$, the SOR splitting, and the definition of the iteration matrix are standard. The problem is self-contained, consistent, and well-posed. The tasks build upon each other logically. The problem is therefore deemed valid.\n\n**Solution**\n\n**1. Eigenvectors and Eigenvalues of $T_{\\mathrm{SOR}}(\\omega)$**\n\nThe matrix $A$ is an $n \\times n$ circulant matrix. Any $n \\times n$ circulant matrix is diagonalized by the Discrete Fourier Transform (DFT). The eigenvectors are the same for all $n \\times n$ circulant matrices and are given by the vectors $v_k$ for $k = 0, 1, \\dots, n-1$, whose components are:\n$$\n(v_k)_j = \\frac{1}{\\sqrt{n}} \\exp\\left(\\frac{2 \\pi i (j-1)k}{n}\\right), \\quad j = 1, \\dots, n\n$$\nThe matrices in the SOR splitting, $D = 2I$, $L = P$, and $U = P^{\\top}$, are also circulant. Thus, they share the same eigenvectors $v_k$. We find their corresponding eigenvalues.\n\nLet $\\lambda_M(k)$ denote the eigenvalue of a matrix $M$ corresponding to the eigenvector $v_k$.\n-   For $D=2I$, the eigenvalue is $\\lambda_D(k) = 2$ for all $k$.\n-   For $L=P$, the cyclic downshift matrix, acting on $v_k$:\n    $(P v_k)_j = (v_k)_{j-1} = \\frac{1}{\\sqrt{n}} \\exp\\left(\\frac{2 \\pi i (j-2)k}{n}\\right) = \\exp\\left(-\\frac{2 \\pi i k}{n}\\right) (v_k)_j$.\n    So, the eigenvalue is $\\lambda_L(k) = \\exp\\left(-\\frac{2 \\pi i k}{n}\\right)$.\n-   For $U=P^{\\top}$, which is the inverse of $P$, the eigenvalues are the inverse of those of $P$.\n    The eigenvalue is $\\lambda_U(k) = (\\lambda_L(k))^{-1} = \\exp\\left(\\frac{2 \\pi i k}{n}\\right)$.\n\nThe eigenvalue equation for $T_{\\mathrm{SOR}}(\\omega)$ is $T_{\\mathrm{SOR}}(\\omega) v_k = \\lambda_k(\\omega) v_k$. Substituting the definition of $T_{\\mathrm{SOR}}(\\omega)$:\n$$\n(D - \\omega L)^{-1}\\big((1 - \\omega) D + \\omega U\\big) v_k = \\lambda_k(\\omega) v_k\n$$\nApplying both sides to $v_k$ and replacing the matrices with their respective eigenvalues for mode $k$:\n$$\n\\big( \\lambda_D(k) - \\omega \\lambda_L(k) \\big)^{-1} \\big( (1-\\omega)\\lambda_D(k) + \\omega \\lambda_U(k) \\big) = \\lambda_k(\\omega)\n$$\nSubstituting the eigenvalues:\n$$\n\\lambda_k(\\omega) = \\frac{(1-\\omega) \\cdot 2 + \\omega \\exp\\left(\\frac{2 \\pi i k}{n}\\right)}{2 - \\omega \\exp\\left(-\\frac{2 \\pi i k}{n}\\right)}\n$$\nThis provides the eigenvalues of $T_{\\mathrm{SOR}}(\\omega)$ for $k=0, 1, \\dots, n-1$. The corresponding eigenvectors are the DFT vectors $v_k$.\n\n**2. Effective Spectral Radius $\\rho_{\\perp}(T_{\\mathrm{SOR}})$}\n\nFirst, consider the zero-frequency mode $k=0$. The eigenvector $v_0$ is a constant vector. The corresponding eigenvalue is:\n$$\n\\lambda_0(\\omega) = \\frac{2(1-\\omega) + \\omega \\exp(0)}{2 - \\omega \\exp(0)} = \\frac{2-2\\omega+\\omega}{2-\\omega} = \\frac{2-\\omega}{2-\\omega} = 1\n$$\nThis eigenvalue of $1$ indicates that the iteration does not converge on the null space of $A$. The effective spectral radius $\\rho_{\\perp}(T_{\\mathrm{SOR}})$ is the maximum modulus of eigenvalues for all other modes, $k=1, \\dots, n-1$.\n$$\n\\rho_{\\perp}(T_{\\mathrm{SOR}}) = \\max_{k \\in \\{1, \\dots, n-1\\}} |\\lambda_k(\\omega)|\n$$\nLet $\\theta_k = \\frac{2\\pi k}{n}$. The squared modulus of an eigenvalue $\\lambda_k(\\omega)$ is:\n$$\n|\\lambda_k(\\omega)|^2 = \\left| \\frac{2(1-\\omega) + \\omega \\exp(i\\theta_k)}{2 - \\omega \\exp(-i\\theta_k)} \\right|^2 = \\frac{|2(1-\\omega) + \\omega \\exp(i\\theta_k)|^2}{|2 - \\omega \\exp(-i\\theta_k)|^2}\n$$\nThe numerator's squared modulus is:\n$$\n|2(1-\\omega) + \\omega(\\cos\\theta_k + i\\sin\\theta_k)|^2 = (2-2\\omega+\\omega\\cos\\theta_k)^2 + (\\omega\\sin\\theta_k)^2\n= 4(1-\\omega)^2 + 4\\omega(1-\\omega)\\cos\\theta_k + \\omega^2 = 4-8\\omega+5\\omega^2 + 4\\omega(1-\\omega)\\cos\\theta_k\n$$\nThe denominator's squared modulus is:\n$$\n|2 - \\omega(\\cos\\theta_k - i\\sin\\theta_k)|^2 = (2-\\omega\\cos\\theta_k)^2 + (\\omega\\sin\\theta_k)^2 = 4 - 4\\omega\\cos\\theta_k + \\omega^2\n$$\nSo, the squared modulus is:\n$$\n|\\lambda_k(\\omega)|^2 = \\frac{4-8\\omega+5\\omega^2 + 4\\omega(1-\\omega)\\cos\\theta_k}{4+\\omega^2-4\\omega\\cos\\theta_k}\n$$\nLet $x = \\cos\\theta_k$. We analyze the function $g(x) = \\frac{a+bx}{c-dx}$ where $a=4-8\\omega+5\\omega^2$, $b=4\\omega(1-\\omega)$, $c=4+\\omega^2$, and $d=4\\omega$. To find the maximum of $g(x)$, we check its derivative:\n$$\ng'(x) = \\frac{b(c-dx) - (a+bx)(-d)}{(c-dx)^2} = \\frac{bc+ad}{(c-dx)^2}\n$$\nThe sign of $g'(x)$ is determined by $bc+ad$:\n$$\nbc+ad = 4\\omega(1-\\omega)(4+\\omega^2) + (4-8\\omega+5\\omega^2)(4\\omega) = 4\\omega \\left[ (1-\\omega)(4+\\omega^2) + (4-8\\omega+5\\omega^2) \\right]\n$$\n$$\n= 4\\omega [4+\\omega^2-4\\omega-\\omega^3 + 4-8\\omega+5\\omega^2] = 4\\omega [8 - 12\\omega + 6\\omega^2 - \\omega^3] = 4\\omega(2-\\omega)^3\n$$\nFor $\\omega \\in (0,2)$, both $4\\omega$ and $(2-\\omega)^3$ are positive, so $g'(x)0$. The function $g(x)$ is monotonically increasing with $x=\\cos\\theta_k$. Therefore, the maximum of $|\\lambda_k(\\omega)|^2$ occurs for the largest value of $\\cos\\theta_k$ over $k \\in \\{1, \\dots, n-1\\}$. This occurs when $k=1$ (and $k=n-1$), giving $x_{max} = \\cos(2\\pi/n)$.\n\nThe effective spectral radius is thus $|\\lambda_1(\\omega)|$:\n$$\n\\rho_{\\perp}(T_{\\mathrm{SOR}}) = \\sqrt{\\frac{4-8\\omega+5\\omega^2 + 4\\omega(1-\\omega)\\cos\\left(\\frac{2\\pi}{n}\\right)}{4+\\omega^2-4\\omega\\cos\\left(\\frac{2\\pi}{n}\\right)}}\n$$\n\n**3. Evaluation for $n=64$ and $\\omega=1.5$**\n\nWe substitute $n=64$ and $\\omega=1.5$ into the derived expression for $\\rho_{\\perp}(T_{\\mathrm{SOR}})$.\nThe angle term is $\\cos\\left(\\frac{2\\pi}{64}\\right) = \\cos\\left(\\frac{\\pi}{32}\\right)$.\nFirst, evaluate the coefficients for $\\omega=1.5$:\n-   $4-8\\omega+5\\omega^2 = 4-8(1.5)+5(1.5)^2 = 4-12+5(2.25) = -8+11.25 = 3.25$.\n-   $4\\omega(1-\\omega) = 4(1.5)(1-1.5) = 6(-0.5) = -3$.\n-   $4+\\omega^2 = 4+(1.5)^2 = 4+2.25 = 6.25$.\n-   $4\\omega = 4(1.5) = 6$.\n\nSubstituting these into the formula for the squared spectral radius:\n$$\n\\rho_{\\perp}^2 = \\frac{3.25 - 3\\cos\\left(\\frac{\\pi}{32}\\right)}{6.25 - 6\\cos\\left(\\frac{\\pi}{32}\\right)}\n$$\nNow, we calculate the numerical value. Angles are in radians.\n$$\n\\cos\\left(\\frac{\\pi}{32}\\right) \\approx 0.9951847267\n$$\nPlugging this value in:\n$$\n\\rho_{\\perp}^2 \\approx \\frac{3.25 - 3(0.9951847267)}{6.25 - 6(0.9951847267)} = \\frac{3.25 - 2.9855541801}{6.25 - 5.9711083602} = \\frac{0.2644458199}{0.2788916398} \\approx 0.94820128\n$$\nTaking the square root:\n$$\n\\rho_{\\perp} \\approx \\sqrt{0.94820128} \\approx 0.97375626\n$$\nRounding to four significant figures, we get $0.9738$.",
            "answer": "$$\n\\boxed{0.9738}\n$$"
        },
        {
            "introduction": "The performance of SOR and SSOR methods critically depends on choosing the right relaxation parameter, $\\omega$. This exercise addresses a key practical question: how is the optimal choice of $\\omega$ affected by common matrix pre-processing techniques like diagonal scaling? By exploring the invariance properties of the optimal $\\omega$ under a similarity transformation, you will uncover a fundamental theoretical property that simplifies parameter tuning and deepens your understanding of the algebraic structure of these methods .",
            "id": "3451659",
            "problem": "Consider a linear system arising from a finite-difference discretization of a uniformly elliptic partial differential equation on a structured grid. Let the coefficient matrix be real, symmetric positive definite (SPD) and written in the standard splitting as $A = D - L - U$, where $D$ is the diagonal of $A$ with strictly positive diagonal entries, $L$ is the strictly lower-triangular part, and $U$ is the strictly upper-triangular part. The Successive Over-Relaxation (SOR) iteration uses a relaxation parameter $\\omega \\in (0,2)$ and is defined in terms of the splitting of $A$ and the residuals in a forward-sweep update. The Symmetric Successive Over-Relaxation (SSOR) method corresponds to a forward SOR sweep followed by a backward SOR sweep with the same $\\omega$, and, when used as a preconditioner for the Conjugate Gradient method, defines a linear preconditioner built from the same splitting elements and the parameter $\\omega$.\n\nNow consider the diagonal equilibration (also called symmetric Jacobi scaling) of $A$ by introducing the change of variables $x = D^{-1/2} y$, which transforms the system $A x = b$ into $\\tilde{A} y = \\tilde{b}$ with $\\tilde{A} = D^{-1/2} A D^{-1/2}$ and $\\tilde{b} = D^{-1/2} b$. This scaling equilibrates the diagonal so that the diagonal of $\\tilde{A}$ is the identity matrix.\n\nAnswer the following question. Focus on the asymptotic convergence rate of stationary iterations (as measured by the spectral radius of the iteration matrix) and, for preconditioned Krylov methods, on the spectrum or condition number of the preconditioned operator. You may use standard and well-tested facts about SOR and SSOR iteration matrices and preconditioners, as well as basic spectral properties such as invariance of the spectrum under similarity transformations.\n\nWhich statement best describes the effect of the diagonal equilibration by $D^{-1/2}$ on the optimal relaxation parameter $\\omega$ for:\n(i) the SOR stationary iteration applied to $A x = b$ versus the SOR stationary iteration applied to the scaled system $\\tilde{A} y = \\tilde{b}$, and\n(ii) the SSOR preconditioner applied to $A$ versus the SSOR preconditioner applied to $\\tilde{A}$?\n\nA. The optimal $\\omega$ is unchanged by the scaling in both cases. The SOR iteration matrices for $A$ and $\\tilde{A}$ are similar for every fixed $\\omega$, hence they have identical spectra and spectral radii, implying the same $\\omega$ minimizes the spectral radius in either variable. Likewise, the preconditioned operators defined by SSOR for $A$ and for $\\tilde{A}$ are similar, hence their spectra and condition numbers coincide for each $\\omega$, implying the optimal $\\omega$ is invariant.\n\nB. The optimal $\\omega$ generally changes after scaling because the entries of $L$ and $U$ are altered. The SOR and SSOR constructions are not preserved by diagonal scaling, so the spectral radius and condition number minimizers for $\\omega$ typically differ before and after scaling.\n\nC. The optimal $\\omega$ rescales according to $\\tilde{\\omega} = \\omega / \\sqrt{\\kappa(D)}$, where $\\kappa(D)$ is the spectral condition number of $D$, because the scaling contracts the off-diagonal entries relative to the diagonal.\n\nD. For SSOR used as a preconditioner, the best $\\omega$ must be mapped to $\\tilde{\\omega} = 2 - \\omega$ after diagonal equilibration to preserve the preconditioned spectrum, while for stationary SOR the optimal $\\omega$ is unchanged.",
            "solution": "The user wants an analysis of the effect of symmetric diagonal scaling on the optimal relaxation parameter $\\omega$ for the SOR and SSOR methods.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Linear system: $A x = b$.\n-   $A$: real, symmetric positive definite (SPD).\n-   Splitting: $A = D - L - U$, where $D$ is the diagonal of $A$ with $D_{ii}  0$, $L$ is strictly lower-triangular, and $U$ is strictly upper-triangular. Since $A$ is symmetric, $U = L^T$.\n-   SOR iteration with parameter $\\omega \\in (0,2)$.\n-   SSOR method: forward SOR sweep followed by a backward SOR sweep. Used as a preconditioner for the Conjugate Gradient method.\n-   Diagonal equilibration: A change of variables $x = D^{-1/2} y$ transforms $A x = b$ to $\\tilde{A} y = \\tilde{b}$, where $\\tilde{A} = D^{-1/2} A D^{-1/2}$ and $\\tilde{b} = D^{-1/2} b$. The diagonal of $\\tilde{A}$ is the identity matrix $I$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is grounded in the well-established theory of iterative methods for numerical linear algebra, specifically for solving linear systems arising from PDEs. All concepts (SPD-ness, SOR, SSOR, preconditioning, diagonal scaling) are standard.\n-   **Well-Posed**: The problem is well-posed. It asks for the effect of a specific transformation on the optimal parameter for two standard methods. The metrics for optimality (spectral radius for stationary methods, condition number for preconditioned Krylov methods) are clearly defined.\n-   **Objective**: The problem is stated in precise, objective mathematical language.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the solution.\n\n### Derivation and Option Analysis\n\nThe core of the problem is to compare the optimal relaxation parameter $\\omega$ for the original system $A x = b$ and the diagonally scaled system $\\tilde{A} y = \\tilde{b}$.\n\nLet the splitting of $\\tilde{A}$ be $\\tilde{A} = \\tilde{D} - \\tilde{L} - \\tilde{U}$.\nFrom the definition $\\tilde{A} = D^{-1/2} A D^{-1/2}$, we can find the components of a splitting:\n$\\tilde{A} = D^{-1/2} (D - L - U) D^{-1/2} = D^{-1/2}D D^{-1/2} - D^{-1/2}L D^{-1/2} - D^{-1/2}U D^{-1/2}$.\nThe diagonal part of $\\tilde{A}$ is $\\tilde{D} = \\text{diag}(D^{-1/2} A D^{-1/2})$. The $(i,i)$-th element is $(D^{-1/2})_{ii} A_{ii} (D^{-1/2})_{ii} = (1/\\sqrt{D_{ii}}) D_{ii} (1/\\sqrt{D_{ii}}) = 1$. Thus, $\\tilde{D} = I$.\nThe strictly lower triangular part is $\\tilde{L} = D^{-1/2} L D^{-1/2}$.\nThe strictly upper triangular part is $\\tilde{U} = D^{-1/2} U D^{-1/2}$.\n\n**Part (i): SOR Stationary Iteration**\n\nThe SOR iteration matrix for $A x = b$ is given by\n$$T_{SOR}(A, \\omega) = (D - \\omega L)^{-1} ((1-\\omega)D + \\omega U)$$\nThe asymptotic rate of convergence is determined by the spectral radius $\\rho(T_{SOR}(A, \\omega))$, and the optimal $\\omega$ is the one that minimizes this radius.\n\nA more convenient form for analysis is $T_{SOR}(A, \\omega) = (I - \\omega D^{-1}L)^{-1}((1-\\omega)I + \\omega D^{-1}U)$.\nThe Jacobi iteration matrix for $A$ is $J_A = D^{-1}(L+U)$.\n\nFor the scaled system $\\tilde{A} y = \\tilde{b}$, the SOR iteration matrix is\n$$T_{SOR}(\\tilde{A}, \\omega) = (\\tilde{D} - \\omega \\tilde{L})^{-1} ((1-\\omega)\\tilde{D} + \\omega \\tilde{U}) = (I - \\omega \\tilde{L})^{-1} ((1-\\omega)I + \\omega \\tilde{U})$$\nThe Jacobi iteration matrix for $\\tilde{A}$ is $\\tilde{J}_{\\tilde{A}} = \\tilde{D}^{-1}(\\tilde{L}+\\tilde{U}) = I^{-1}(\\tilde{L}+\\tilde{U}) = \\tilde{L}+\\tilde{U} = D^{-1/2}(L+U)D^{-1/2}$.\n\nLet's examine the relationship between $J_A$ and $J_{\\tilde{A}}$.\nConsider the similarity transformation of $J_A$ by $D^{1/2}$:\n$$D^{1/2} J_A D^{-1/2} = D^{1/2} (D^{-1}(L+U)) D^{-1/2} = D^{-1/2}(L+U)D^{-1/2} = J_{\\tilde{A}}$$\nSince $J_A$ and $J_{\\tilde{A}}$ are similar, they have the same eigenvalues, and thus the same spectral radius, $\\rho(J_A) = \\rho(J_{\\tilde{A}})$.\n\nFor a matrix $A$ that is \"consistently ordered\", a property common for matrices arising from finite-difference discretizations on structured grids, a well-known result by D. Young relates the eigenvalues of $T_{SOR}$ to the eigenvalues of $J_A$. The optimal relaxation parameter is given by\n$$\\omega_{opt} = \\frac{2}{1 + \\sqrt{1 - \\rho(J_A)^2}}$$\nSince $\\rho(J_A) = \\rho(J_{\\tilde{A}})$, the optimal $\\omega$ for the SOR method is the same for both the original and the scaled systems, provided $A$ is consistently ordered.\n\nHowever, the SOR iteration matrices $T_{SOR}(A, \\omega)$ and $T_{SOR}(\\tilde{A}, \\omega)$ are not, in general, similar. Similarity would imply they have the same spectrum for all $\\omega$, but this is not a necessary condition for them to have the same optimal $\\omega$.\n\n**Part (ii): SSOR Preconditioner**\n\nThe SSOR preconditioner for $A$ is defined as\n$$M_{SSOR}(A, \\omega) = \\frac{1}{\\omega(2-\\omega)} (D-\\omega L) D^{-1} (D-\\omega U)$$\nFor a symmetric matrix $A$, this preconditioner $M_{SSOR}$ is symmetric and positive definite for $\\omega \\in (0,2)$. The convergence of SSOR-preconditioned CG depends on the condition number of the preconditioned matrix $K_A = M_{SSOR}(A, \\omega)^{-1} A$.\n\nThe preconditioner for the scaled system $\\tilde{A}$ is\n$$M_{SSOR}(\\tilde{A}, \\omega) = \\frac{1}{\\omega(2-\\omega)} (\\tilde{D}-\\omega \\tilde{L}) \\tilde{D}^{-1} (\\tilde{D}-\\omega \\tilde{U}) = \\frac{1}{\\omega(2-\\omega)} (I-\\omega \\tilde{L}) (I-\\omega \\tilde{U})$$\nSubstituting $\\tilde{L} = D^{-1/2} L D^{-1/2}$ and $\\tilde{U} = D^{-1/2} U D^{-1/2}$:\n$$M_{SSOR}(\\tilde{A}, \\omega) = \\frac{1}{\\omega(2-\\omega)} (I - \\omega D^{-1/2}LD^{-1/2})(I - \\omega D^{-1/2}UD^{-1/2})$$\nLet's analyze the relationship between $M_{SSOR}(A, \\omega)$ and $M_{SSOR}(\\tilde{A}, \\omega)$. Consider the transformation of $M_{SSOR}(A, \\omega)$ by $D^{-1/2}$:\n$$D^{-1/2} M_{SSOR}(A, \\omega) D^{-1/2} = \\frac{1}{\\omega(2-\\omega)} D^{-1/2} (D-\\omega L) D^{-1} (D-\\omega U) D^{-1/2}$$\n$$= \\frac{1}{\\omega(2-\\omega)} [D^{-1/2}(D-\\omega L)] D^{-1} [(D-\\omega U)D^{-1/2}]$$\n$$= \\frac{1}{\\omega(2-\\omega)} (D^{1/2} - \\omega D^{-1/2}L) D^{-1} (D^{1/2} - \\omega U D^{-1/2})$$\n$$= \\frac{1}{\\omega(2-\\omega)} (D^{1/2} - \\omega D^{-1/2}L) (D^{-1/2} - \\omega D^{-1} U D^{-1/2})$$\nExpanding this product:\n$$= \\frac{1}{\\omega(2-\\omega)} [I - \\omega D^{-1/2}UD^{-1/2} - \\omega D^{-1/2}LD^{-1/2} + \\omega^2 D^{-1/2}L D^{-1} U D^{-1/2}]$$\nNow, expanding $M_{SSOR}(\\tilde{A}, \\omega)$:\n$$M_{SSOR}(\\tilde{A}, \\omega) = \\frac{1}{\\omega(2-\\omega)} [I - \\omega D^{-1/2}UD^{-1/2} - \\omega D^{-1/2}LD^{-1/2} + \\omega^2 (D^{-1/2}LD^{-1/2})(D^{-1/2}UD^{-1/2})]$$\n$$= \\frac{1}{\\omega(2-\\omega)} [I - \\omega D^{-1/2}UD^{-1/2} - \\omega D^{-1/2}LD^{-1/2} + \\omega^2 D^{-1/2}L D^{-1} U D^{-1/2}]$$\nThe expressions are identical. Therefore, we have the crucial identity:\n$$M_{SSOR}(\\tilde{A}, \\omega) = D^{-1/2} M_{SSOR}(A, \\omega) D^{-1/2}$$\nNow we can relate the preconditioned operators $K_A = M_{SSOR}(A, \\omega)^{-1} A$ and $K_{\\tilde{A}} = M_{SSOR}(\\tilde{A}, \\omega)^{-1} \\tilde{A}$. Let's apply a similarity transformation to $K_A$ with the matrix $S = D^{1/2}$:\n$$S K_A S^{-1} = D^{1/2} K_A D^{-1/2} = D^{1/2} [M_{SSOR}(A, \\omega)^{-1} A] D^{-1/2}$$\nUsing the identity $(PQR)^{-1} = R^{-1}Q^{-1}P^{-1}$ and $(S M S^{-1})^{-1} = S M^{-1} S^{-1}$:\n$$= [D^{-1/2} M_{SSOR}(A, \\omega) D^{1/2}]^{-1} [D^{1/2} A D^{-1/2}]$$\nThis is not the correct path. Let's start from $K_{\\tilde{A}}$:\n$$K_{\\tilde{A}} = M_{SSOR}(\\tilde{A}, \\omega)^{-1} \\tilde{A} = [D^{-1/2} M_{SSOR}(A, \\omega) D^{-1/2}]^{-1} [D^{-1/2} A D^{-1/2}]$$\nUsing $(S M S)^{-1} = S^{-1} M^{-1} S^{-1}$ for $S=D^{-1/2}$ (which is symmetric):\n$$= [D^{1/2} M_{SSOR}(A, \\omega)^{-1} D^{1/2}] [D^{-1/2} A D^{-1/2}]$$\n$$= D^{1/2} M_{SSOR}(A, \\omega)^{-1} (D^{1/2} D^{-1/2}) A D^{-1/2} = D^{1/2} [M_{SSOR}(A, \\omega)^{-1} A] D^{-1/2}$$\n$$= D^{1/2} K_A D^{-1/2} $$\nThis proves that $K_{\\tilde{A}}$ is similar to $K_A$. Therefore, for any fixed $\\omega$, the preconditioned operators for the original and scaled systems have identical spectra. This means their condition numbers are also identical. The value of $\\omega$ that minimizes $\\kappa(K_A)$ must also be the value that minimizes $\\kappa(K_{\\tilde{A}})$. The optimal $\\omega$ for the SSOR preconditioner is invariant under diagonal scaling.\n\n**Conclusion Summary**\n-   (i) For SOR, the optimal $\\omega$ is unchanged for the important class of consistently ordered matrices, which is strongly implied by the problem context.\n-   (ii) For the SSOR preconditioner, the optimal $\\omega$ is unchanged in general.\n\nThis leads to the conclusion that the optimal $\\omega$ is unchanged in both cases.\n\n**Option-by-Option Analysis**\n\n**A. The optimal $\\omega$ is unchanged by the scaling in both cases. The SOR iteration matrices for $A$ and $\\tilde{A}$ are similar for every fixed $\\omega$, hence they have identical spectra and spectral radii, implying the same $\\omega$ minimizes the spectral radius in either variable. Likewise, the preconditioned operators defined by SSOR for $A$ and for $\\tilde{A}$ are similar, hence their spectra and condition numbers coincide for each $\\omega$, implying the optimal $\\omega$ is invariant.**\n-   The main conclusion that the optimal $\\omega$ is unchanged in both cases is correct under the standard assumptions for this problem type.\n-   The justification for SSOR is fully correct: the preconditioned operators are indeed similar.\n-   The justification for SOR is flawed. The iteration matrices $T_{SOR}(A, \\omega)$ and $T_{SOR}(\\tilde{A}, \\omega)$ are not similar in general. However, the conclusion that $\\omega_{opt}$ is the same is correct for consistently ordered matrices, which is the key case for SOR theory. Given that the overall conclusion is correct and the SSOR reasoning is perfect, this is the best available option.\n-   Verdict: **Correct**.\n\n**B. The optimal $\\omega$ generally changes after scaling because the entries of $L$ and $U$ are altered. The SOR and SSOR constructions are not preserved by diagonal scaling, so the spectral radius and condition number minimizers for $\\omega$ typically differ before and after scaling.**\n-   This is incorrect. As proven, the SSOR construction is preserved in a way that leads to similar preconditioned operators, so the optimal $\\omega$ does not change. For SOR, the optimal $\\omega$ is also unchanged in the canonical case of consistently ordered matrices.\n-   Verdict: **Incorrect**.\n\n**C. The optimal $\\omega$ rescales according to $\\tilde{\\omega} = \\omega / \\sqrt{\\kappa(D)}$, where $\\kappa(D)$ is the spectral condition number of $D$, because the scaling contracts the off-diagonal entries relative to the diagonal.**\n-   This provides a specific, but incorrect and unsubstantiated, scaling law. The relationship is not a simple rescaling of $\\omega$.\n-   Verdict: **Incorrect**.\n\n**D. For SSOR used as a preconditioner, the best $\\omega$ must be mapped to $\\tilde{\\omega} = 2 - \\omega$ after diagonal equilibration to preserve the preconditioned spectrum, while for stationary SOR the optimal $\\omega$ is unchanged.**\n-   The claim for SSOR is incorrect. The optimal $\\omega$ is unchanged, not mapped to $2-\\omega$. The claim for SOR is correct for consistently ordered matrices, but the SSOR part makes the entire statement false.\n-   Verdict: **Incorrect**.\n\nThe best description is offered by option A, despite a subtle flaw in its reasoning for the SOR case. Its conclusions are correct for the problem class described.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}