{
    "hands_on_practices": [
        {
            "introduction": "To effectively apply iterative methods in practice, we must first understand their computational cost. This exercise provides a foundational analysis of the work required per iteration for the Successive Over-Relaxation (SOR) method when applied to a general sparse linear system. By performing a careful flop count based on the widely-used Compressed Sparse Row (CSR) storage format, you will develop a concrete understanding of how the algorithm's cost scales with problem size and sparsity, and explore the crucial impact of in-place updates on data access patterns .",
            "id": "3451653",
            "problem": "Consider a sparse linear system $A x = b$ arising from a finite-difference discretization of a linear elliptic partial differential equation on a structured grid, where $A \\in \\mathbb{R}^{n \\times n}$ is real symmetric positive definite and stored in Compressed Sparse Row (CSR) format. Let $m = \\operatorname{nnz}(A)$ denote the number of nonzero entries in $A$. Assume the Successive Over-Relaxation (SOR) method with relaxation parameter $\\omega \\in (0,2)$ is applied in-place, meaning that each entry $x_{i}$ is overwritten as soon as it is updated. Use the standard splitting $A = D + L + U$, where $D$ is the diagonal, $L$ the strict lower triangular part, and $U$ the strict upper triangular part.\n\nYou may assume the following implementation model:\n- The update of $x_{i}$ in a forward SOR sweep uses the sum of all off-diagonal contributions in row $i$, computed as a dot product $s_{i} = \\sum_{j \\neq i} a_{ij} x_{j}$, with the convention that $x_{j}$ is the most recently available value (new for $j < i$, old for $j > i$).\n- The new value is computed by $x_{i}^{\\text{new}} = (1 - \\omega) x_{i}^{\\text{old}} + \\alpha_{i} \\left( b_{i} - s_{i} \\right)$ with $\\alpha_{i} = \\omega / a_{ii}$.\n- The diagonal inverses $a_{ii}^{-1}$ and the scaled diagonal coefficients $\\alpha_{i}$ are precomputed prior to the iteration and available at no per-iteration cost.\n\nAdopt the following flop-counting conventions:\n- Count only floating-point additions and multiplications. Divisions are excluded by the precomputation of $\\alpha_{i}$.\n- For the accumulation of $s_{i}$, each off-diagonal nonzero $a_{ij}$ contributes one multiplication and one addition, both counted.\n- The operations to form $b_{i} - s_{i}$ and to combine the two terms in $x_{i}^{\\text{new}} = (1 - \\omega) x_{i}^{\\text{old}} + \\alpha_{i} \\left( b_{i} - s_{i} \\right)$ are counted explicitly as addition/multiplication flops.\n\nTasks:\n1. Derive from first principles the exact flop count per forward SOR sweep in terms of $m$ and $n$ under the above assumptions.\n2. Explain how a backward sweep yields the same flop count and conclude the cost per symmetric SOR (SSOR) iteration consisting of one forward and one backward sweep.\n3. Discuss, using the definition of the SOR update and the CSR storage model, how in-place updates during SOR/SSOR affect data access patterns compared to a Jacobi iteration, which uses two separate vectors $x^{(k)}$ and $x^{(k+1)}$ so that updates are out-of-place.\n\nExpress your final answer as a single closed-form analytic expression giving the total per-iteration flop cost of a single forward SOR sweep in terms of $m$ and $n$ only. No rounding is required, and no physical units apply. The discussion is required to justify the derivation but will not affect the final numerical expression requested.",
            "solution": "The problem is well-posed, scientifically grounded, and provides a clear set of assumptions and definitions for a rigorous analysis. We proceed with the derivation.\n\nThe primary task is to determine the exact number of floating-point operations (flops), defined as additions and multiplications, required for one complete forward sweep of the Successive Over-Relaxation (SOR) method.\n\nLet the linear system be $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a real symmetric positive definite matrix with $m = \\operatorname{nnz}(A)$ nonzero entries. The matrix is stored in Compressed Sparse Row (CSR) format. The SOR method updates each component $x_i$ of the solution vector $x$ for $i=1, 2, \\dots, n$ according to the provided formula:\n$$x_{i}^{\\text{new}} = (1 - \\omega) x_{i}^{\\text{old}} + \\alpha_{i} \\left( b_{i} - s_{i} \\right)$$\nwhere $\\omega \\in (0,2)$ is the relaxation parameter, $\\alpha_{i} = \\omega / a_{ii}$, and $s_{i} = \\sum_{j \\neq i} a_{ij} x_{j}$. The sum $s_i$ uses the most recently updated values of $x_j$, meaning $x_j^{\\text{new}}$ for $j < i$ and $x_j^{\\text{old}}$ for $j > i$.\n\nWe will determine the flop count for the update of a single component $x_i$ and then sum over all $n$ components to find the total for a full sweep.\n\n**1. Flop Count per Forward SOR Sweep**\n\nLet's analyze the computational cost for updating a single component $x_i$. The calculation proceeds in two main stages as defined by the formula.\n\n**Stage 1: Computation of the off-diagonal sum $s_i$.**\nThe sum is given by $s_{i} = \\sum_{j \\neq i} a_{ij} x_{j}$. Let $k_i$ be the number of off-diagonal nonzero entries in row $i$ of matrix $A$. The total number of nonzero entries in row $i$ is $k_i + 1$ (including the diagonal element $a_{ii}$).\nAccording to the problem's flop-counting convention, \"each off-diagonal nonzero $a_{ij}$ contributes one multiplication and one addition\".\nTherefore, computing $s_i$ requires:\n- $k_i$ multiplications (for each $a_{ij} x_j$ term).\n- $k_i$ additions (to accumulate these terms, assuming the sum is initialized to $0$).\nThe cost for computing $s_i$ is $k_i$ multiplications and $k_i$ additions.\n\n**Stage 2: Computation of the final value $x_i^{\\text{new}}$.**\nThe update formula is $x_{i}^{\\text{new}} = (1 - \\omega) x_{i}^{\\text{old}} + \\alpha_{i} ( b_{i} - s_{i} )$. We count the operations in sequence:\n- Subtraction: $b_i - s_i$. This is $1$ addition flop.\n- Multiplication: $\\alpha_i \\times (b_i - s_i)$. The parameter $\\alpha_i$ is precomputed, so this is $1$ multiplication flop.\n- Multiplication: $(1 - \\omega) \\times x_i^{\\text{old}}$. The term $(1 - \\omega)$ is a constant throughout the iteration and can be precomputed before the sweep begins. This is $1$ multiplication flop.\n- Addition: The final sum of the two terms $(1 - \\omega) x_i^{\\text{old}}$ and $\\alpha_i ( b_i - s_i )$. This is $1$ addition flop.\n\nIn total, Stage 2 requires $1+1=2$ multiplications and $1+1=2$ additions per component $x_i$.\n\n**Total cost per component $x_i$:**\nCombining both stages, the total number of flops to update $x_i$ is:\n- Total multiplications: $k_i (\\text{from } s_i) + 2 (\\text{from update}) = k_i + 2$.\n- Total additions: $k_i (\\text{from } s_i) + 2 (\\text{from update}) = k_i + 2$.\nThe total flop count for updating $x_i$ is $2(k_i+2)$.\n\n**Total cost for a full forward sweep:**\nTo find the total cost for one full forward sweep (for $i=1, \\dots, n$), we sum the costs for each component:\n- Total multiplications = $\\sum_{i=1}^{n} (k_i + 2) = \\left(\\sum_{i=1}^{n} k_i\\right) + \\left(\\sum_{i=1}^{n} 2\\right)$.\n- Total additions = $\\sum_{i=1}^{n} (k_i + 2) = \\left(\\sum_{i=1}^{n} k_i\\right) + \\left(\\sum_{i=1}^{n} 2\\right)$.\n\nThe term $\\sum_{i=1}^{n} k_i$ represents the sum of the number of off-diagonal nonzero entries over all rows. This is precisely the total number of off-diagonal nonzero entries in the matrix $A$. Given that the total number of nonzeros is $m = \\operatorname{nnz}(A)$ and the matrix has $n$ diagonal entries (since $A$ is from a PDE discretization, $a_{ii} \\neq 0$), the total number of off-diagonal nonzeros is $m-n$.\nSo, $\\sum_{i=1}^{n} k_i = m - n$.\n\nThe term $\\sum_{i=1}^{n} 2$ is simply $2n$.\n\nSubstituting these results back:\n- Total multiplications = $(m - n) + 2n = m + n$.\n- Total additions = $(m - n) + 2n = m + n$.\nThe total flop count for a single forward SOR sweep is the sum of total multiplications and total additions:\n- Total Flops = $(m+n) + (m+n) = 2(m+n)$.\n\n**2. Backward Sweep and Symmetric SOR (SSOR) Cost**\n\nA backward SOR sweep performs the same set of calculations but for $i=n, n-1, \\dots, 1$. The update formula for each $x_i$ is identical. Although the values of $x_j$ used in the sum $s_i$ are different (newly computed values are for $j>i$), the *number* of arithmetic operations to update a single component $x_i$ remains $2(k_i+2)$. Consequently, the total flop count for a full backward sweep is identical to the forward sweep: $2(m+n)$.\n\nA single iteration of the Symmetric SOR (SSOR) method consists of one full forward sweep followed by one full backward sweep. Therefore, the total per-iteration flop cost for SSOR is the sum of the costs of the two sweeps:\n- SSOR Flop Cost = (Forward Sweep Cost) + (Backward Sweep Cost) = $2(m+n) + 2(m+n) = 4(m+n)$.\n\n**3. Data Access Patterns: In-place SOR/SSOR vs. Out-of-place Jacobi**\n\nThe choice between an in-place update scheme (like SOR) and an out-of-place scheme (like Jacobi) has significant implications for data access patterns and parallelizability.\n\n**SOR/SSOR (In-place):**\n- The update for $x_i^{(k+1)}$ depends on values from the same iteration, $x_j^{(k+1)}$ for $j<i$ (in a forward sweep), as well as values from the previous iteration, $x_j^{(k)}$ for $j>i$. This introduces a loop-carried dependency.\n- All reads and writes for the solution vector are performed on a single memory buffer. For each $i$, the computation reads previously computed values from $x$ (both old and new) to calculate $s_i$, and then overwrites $x_i$ with its new value.\n- This sequential dependency makes standard SOR difficult to parallelize. The computation of $x_{i+1}$ cannot begin until $x_i$ is finalized. While techniques like wavefront parallelization (for structured grids) exist, they are more complex than the parallelism available in Jacobi.\n- In CSR format, the algorithm iterates through rows $1$ to $n$. For each row $i$, it accesses a contiguous block of memory in the CSR arrays for $a_{ij}$ and $col_ind$, but the corresponding accesses to the vector $x$ at indices $j$ are sparse and irregular. The key feature is the read-after-write hazards on the single $x$ vector.\n\n**Jacobi (Out-of-place):**\n- The Jacobi update is $x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j \\neq i} a_{ij} x_j^{(k)}\\right)$. Every component of the new vector $x^{(k+1)}$ is computed using only components from the old vector $x^{(k)}$.\n- This requires two separate storage locations for the solution vector, one for $x^{(k)}$ (read-only) and one for $x^{(k+1)}$ (write-only) within a single iteration. This doubles the memory requirement for the solution vector compared to SOR.\n- The major advantage is the absence of data dependencies between the computations for different components of $x^{(k+1)}$. All $n$ updates can be performed simultaneously and independently. This property is often called \"embarrassingly parallel\".\n- The data access pattern is simpler: it involves streaming reads from the matrix $A$ and the vector $x^{(k)}$, and streaming writes to the vector $x^{(k+1)}$. This is highly friendly to modern parallel architectures (multicore CPUs, GPUs) and vectorization, leading to high computational throughput despite Jacobi's typically slower mathematical convergence rate compared to SOR.\n\nIn summary, the in-place nature of SOR/SSOR is memory-efficient but creates sequential dependencies that hinder parallelism. The out-of-place nature of Jacobi requires more memory but allows for trivial and highly efficient parallelization.",
            "answer": "$$\n\\boxed{2(m+n)}\n$$"
        },
        {
            "introduction": "While general cost analysis is essential, deep insights into an algorithm's convergence behavior often come from studying its performance on model problems where exact analysis is possible. This practice focuses on one such canonical case: the one-dimensional discrete Laplacian with periodic boundary conditions, which results in a circulant matrix. You will use the powerful tools of Fourier analysis to derive the exact eigenvalues of the SOR iteration matrix, allowing for a precise characterization of the method's convergence rate as a function of the relaxation parameter $\\omega$ .",
            "id": "3451637",
            "problem": "Consider the one-dimensional Poisson equation with periodic boundary conditions on the unit interval, discretized on a uniform grid of $n \\geq 4$ points. Using the standard three-point stencil, the discrete periodic Laplacian yields the linear system $A u = f$, where $A \\in \\mathbb{R}^{n \\times n}$ is circulant with entries $2$ on the diagonal, $-1$ on the first sub- and super-diagonals, and $-1$ in the two corner positions. Let $P \\in \\mathbb{R}^{n \\times n}$ denote the cyclic downshift permutation matrix, defined by $(P x)_{i} = x_{i-1}$ with indices modulo $n$, so that $A = 2 I - P - P^{\\top}$.\n\nDefine the Successive Over-Relaxation (SOR) iteration matrix by the splitting $A = D - L - U$ with $D = 2 I$, $L = P$, and $U = P^{\\top}$:\n$$\nT_{\\mathrm{SOR}}(\\omega) \\;=\\; (D - \\omega L)^{-1}\\big((1 - \\omega) D + \\omega U\\big),\n$$\nfor a relaxation parameter $\\omega \\in (0,2)$.\n\n1. Starting from the properties of circulant matrices and the Discrete Fourier Transform (DFT), derive the exact eigenvectors and eigenvalues of $T_{\\mathrm{SOR}}(\\omega)$ for this periodic problem. Make explicit the eigenpairs indexed by the discrete Fourier modes.\n\n2. The constant vector is an eigenvector of $A$ associated with the eigenvalue $0$, and therefore $T_{\\mathrm{SOR}}(\\omega)$ has eigenvalue $1$ on this mode. Consequently, define the effective spectral radius $\\rho_{\\perp}(T_{\\mathrm{SOR}})$ to be the spectral radius of $T_{\\mathrm{SOR}}(\\omega)$ restricted to the subspace orthogonal to the constants (i.e., after excluding the zero-frequency Fourier mode). Using your eigenvalue formula, obtain a closed-form expression for $\\rho_{\\perp}(T_{\\mathrm{SOR}})$ as a function of $n$ and $\\omega$.\n\n3. Finally, evaluate $\\rho_{\\perp}(T_{\\mathrm{SOR}})$ for $n = 64$ and $\\omega = 1.5$, rounding your answer to four significant figures. Angles, if any appear in intermediate steps, must be treated in radians. Your final answer must be a single real number without units.",
            "solution": "The problem is first validated against the specified criteria.\n\n**Problem Validation**\n\n1.  **Extract Givens**:\n    -   System: 1D Poisson equation, periodic boundary conditions, unit interval, uniform grid of $n \\geq 4$ points.\n    -   Linear system: $A u = f$.\n    -   Matrix $A$: $A \\in \\mathbb{R}^{n \\times n}$ is circulant with diagonal entries $2$, first sub/super-diagonal entries $-1$, and corner entries $-1$.\n    -   Matrix representation: $A = 2 I - P - P^{\\top}$, where $P$ is the cyclic downshift permutation matrix.\n    -   SOR splitting: $A = D - L - U$ with $D = 2 I$, $L = P$, and $U = P^{\\top}$.\n    -   SOR iteration matrix: $T_{\\mathrm{SOR}}(\\omega) = (D - \\omega L)^{-1}\\big((1 - \\omega) D + \\omega U\\big)$ for $\\omega \\in (0,2)$.\n    -   Task 1: Derive eigenvectors and eigenvalues of $T_{\\mathrm{SOR}}(\\omega)$.\n    -   Task 2: Derive a closed-form expression for the effective spectral radius $\\rho_{\\perp}(T_{\\mathrm{SOR}})$, which excludes the zero-frequency mode.\n    -   Task 3: Evaluate $\\rho_{\\perp}(T_{\\mathrm{SOR}})$ for $n = 64$ and $\\omega = 1.5$ to four significant figures.\n\n2.  **Validate**: The problem is scientifically grounded in numerical linear algebra. All terms are standard and well-defined. The premise that the discrete periodic Laplacian is a circulant matrix is correct. The structure of $A$, the SOR splitting, and the definition of the iteration matrix are standard. The problem is self-contained, consistent, and well-posed. The tasks build upon each other logically. The problem is therefore deemed valid.\n\n**Solution**\n\n**1. Eigenvectors and Eigenvalues of $T_{\\mathrm{SOR}}(\\omega)$**\n\nThe matrix $A$ is an $n \\times n$ circulant matrix. Any $n \\times n$ circulant matrix is diagonalized by the Discrete Fourier Transform (DFT). The eigenvectors are the same for all $n \\times n$ circulant matrices and are given by the vectors $v_k$ for $k = 0, 1, \\dots, n-1$, whose components are:\n$$\n(v_k)_j = \\frac{1}{\\sqrt{n}} \\exp\\left(\\frac{2 \\pi i (j-1)k}{n}\\right), \\quad j = 1, \\dots, n\n$$\nThe matrices in the SOR splitting, $D = 2I$, $L = P$, and $U = P^{\\top}$, are also circulant. Thus, they share the same eigenvectors $v_k$. We find their corresponding eigenvalues.\n\nLet $\\lambda_M(k)$ denote the eigenvalue of a matrix $M$ corresponding to the eigenvector $v_k$.\n-   For $D=2I$, the eigenvalue is $\\lambda_D(k) = 2$ for all $k$.\n-   For $L=P$, the cyclic downshift matrix, acting on $v_k$:\n    $(P v_k)_j = (v_k)_{j-1} = \\frac{1}{\\sqrt{n}} \\exp\\left(\\frac{2 \\pi i (j-2)k}{n}\\right) = \\exp\\left(-\\frac{2 \\pi i k}{n}\\right) (v_k)_j$.\n    So, the eigenvalue is $\\lambda_L(k) = \\exp\\left(-\\frac{2 \\pi i k}{n}\\right)$.\n-   For $U=P^{\\top}$, which is the inverse of $P$, the eigenvalues are the inverse of those of $P$.\n    The eigenvalue is $\\lambda_U(k) = (\\lambda_L(k))^{-1} = \\exp\\left(\\frac{2 \\pi i k}{n}\\right)$.\n\nThe eigenvalue equation for $T_{\\mathrm{SOR}}(\\omega)$ is $T_{\\mathrm{SOR}}(\\omega) v_k = \\lambda_k(\\omega) v_k$. Substituting the definition of $T_{\\mathrm{SOR}}(\\omega)$:\n$$\n(D - \\omega L)^{-1}\\big((1 - \\omega) D + \\omega U\\big) v_k = \\lambda_k(\\omega) v_k\n$$\nApplying both sides to $v_k$ and replacing the matrices with their respective eigenvalues for mode $k$:\n$$\n\\big( \\lambda_D(k) - \\omega \\lambda_L(k) \\big)^{-1} \\big( (1-\\omega)\\lambda_D(k) + \\omega \\lambda_U(k) \\big) = \\lambda_k(\\omega)\n$$\nSubstituting the eigenvalues:\n$$\n\\lambda_k(\\omega) = \\frac{(1-\\omega) \\cdot 2 + \\omega \\exp\\left(\\frac{2 \\pi i k}{n}\\right)}{2 - \\omega \\exp\\left(-\\frac{2 \\pi i k}{n}\\right)}\n$$\nThis provides the eigenvalues of $T_{\\mathrm{SOR}}(\\omega)$ for $k=0, 1, \\dots, n-1$. The corresponding eigenvectors are the DFT vectors $v_k$.\n\n**2. Effective Spectral Radius $\\rho_{\\perp}(T_{\\mathrm{SOR}})$}\n\nFirst, consider the zero-frequency mode $k=0$. The eigenvector $v_0$ is a constant vector. The corresponding eigenvalue is:\n$$\n\\lambda_0(\\omega) = \\frac{2(1-\\omega) + \\omega \\exp(0)}{2 - \\omega \\exp(0)} = \\frac{2-2\\omega+\\omega}{2-\\omega} = \\frac{2-\\omega}{2-\\omega} = 1\n$$\nThis eigenvalue of $1$ indicates that the iteration does not converge on the null space of $A$. The effective spectral radius $\\rho_{\\perp}(T_{\\mathrm{SOR}})$ is the maximum modulus of eigenvalues for all other modes, $k=1, \\dots, n-1$.\n$$\n\\rho_{\\perp}(T_{\\mathrm{SOR}}) = \\max_{k \\in \\{1, \\dots, n-1\\}} |\\lambda_k(\\omega)|\n$$\nLet $\\theta_k = \\frac{2\\pi k}{n}$. The squared modulus of an eigenvalue $\\lambda_k(\\omega)$ is:\n$$\n|\\lambda_k(\\omega)|^2 = \\left| \\frac{2(1-\\omega) + \\omega \\exp(i\\theta_k)}{2 - \\omega \\exp(-i\\theta_k)} \\right|^2 = \\frac{|2(1-\\omega) + \\omega \\exp(i\\theta_k)|^2}{|2 - \\omega \\exp(-i\\theta_k)|^2}\n$$\nThe numerator's squared modulus is:\n$$\n|2(1-\\omega) + \\omega(\\cos\\theta_k + i\\sin\\theta_k)|^2 = (2-2\\omega+\\omega\\cos\\theta_k)^2 + (\\omega\\sin\\theta_k)^2\n= 4(1-\\omega)^2 + 4\\omega(1-\\omega)\\cos\\theta_k + \\omega^2\\cos^2\\theta_k + \\omega^2\\sin^2\\theta_k\n= 4-8\\omega+4\\omega^2 + \\omega^2 + 4\\omega(1-\\omega)\\cos\\theta_k = 4-8\\omega+5\\omega^2 + 4\\omega(1-\\omega)\\cos\\theta_k\n$$\nThe denominator's squared modulus is:\n$$\n|2 - \\omega(\\cos\\theta_k - i\\sin\\theta_k)|^2 = (2-\\omega\\cos\\theta_k)^2 + (\\omega\\sin\\theta_k)^2 = 4 - 4\\omega\\cos\\theta_k + \\omega^2\\cos^2\\theta_k + \\omega^2\\sin^2\\theta_k = 4 - 4\\omega\\cos\\theta_k + \\omega^2\n$$\nSo, the squared modulus is:\n$$\n|\\lambda_k(\\omega)|^2 = \\frac{4-8\\omega+5\\omega^2 + 4\\omega(1-\\omega)\\cos\\theta_k}{4+\\omega^2-4\\omega\\cos\\theta_k}\n$$\nLet $x = \\cos\\theta_k$. We analyze the function $g(x) = \\frac{a+bx}{c-dx}$ where $a=4-8\\omega+5\\omega^2$, $b=4\\omega(1-\\omega)$, $c=4+\\omega^2$, and $d=4\\omega$. To find the maximum of $g(x)$, we check its derivative:\n$$\ng'(x) = \\frac{b(c-dx) - (a+bx)(-d)}{(c-dx)^2} = \\frac{bc+ad}{(c-dx)^2}\n$$\nThe sign of $g'(x)$ is determined by $bc+ad$:\n$$\nbc+ad = 4\\omega(1-\\omega)(4+\\omega^2) + (4-8\\omega+5\\omega^2)(4\\omega) = 4\\omega \\left[ (1-\\omega)(4+\\omega^2) + (4-8\\omega+5\\omega^2) \\right]\n$$\n$$\n= 4\\omega [4+\\omega^2-4\\omega-\\omega^3 + 4-8\\omega+5\\omega^2] = 4\\omega [8 - 12\\omega + 6\\omega^2 - \\omega^3] = 4\\omega(2-\\omega)^3\n$$\nFor $\\omega \\in (0,2)$, both $4\\omega$ and $(2-\\omega)^3$ are positive, so $g'(x)>0$. The function $g(x)$ is monotonically increasing with $x=\\cos\\theta_k$. Therefore, the maximum of $|\\lambda_k(\\omega)|^2$ occurs for the largest value of $\\cos\\theta_k$ over $k \\in \\{1, \\dots, n-1\\}$. This occurs when $k=1$ (and $k=n-1$), giving $x_{max} = \\cos(2\\pi/n)$.\n\nThe effective spectral radius is thus $|\\lambda_1(\\omega)|$:\n$$\n\\rho_{\\perp}(T_{\\mathrm{SOR}}) = \\sqrt{\\frac{4-8\\omega+5\\omega^2 + 4\\omega(1-\\omega)\\cos\\left(\\frac{2\\pi}{n}\\right)}{4+\\omega^2-4\\omega\\cos\\left(\\frac{2\\pi}{n}\\right)}}\n$$\n\n**3. Evaluation for $n=64$ and $\\omega=1.5$**\n\nWe substitute $n=64$ and $\\omega=1.5$ into the derived expression for $\\rho_{\\perp}(T_{\\mathrm{SOR}})$.\nThe angle term is $\\cos\\left(\\frac{2\\pi}{64}\\right) = \\cos\\left(\\frac{\\pi}{32}\\right)$.\nFirst, evaluate the coefficients for $\\omega=1.5$:\n-   $4-8\\omega+5\\omega^2 = 4-8(1.5)+5(1.5)^2 = 4-12+5(2.25) = -8+11.25 = 3.25$.\n-   $4\\omega(1-\\omega) = 4(1.5)(1-1.5) = 6(-0.5) = -3$.\n-   $4+\\omega^2 = 4+(1.5)^2 = 4+2.25 = 6.25$.\n-   $4\\omega = 4(1.5) = 6$.\n\nSubstituting these into the formula for the squared spectral radius:\n$$\n\\rho_{\\perp}^2 = \\frac{3.25 - 3\\cos\\left(\\frac{\\pi}{32}\\right)}{6.25 - 6\\cos\\left(\\frac{\\pi}{32}\\right)}\n$$\nNow, we calculate the numerical value. Angles are in radians.\n$$\n\\cos\\left(\\frac{\\pi}{32}\\right) \\approx 0.9951847267\n$$\nPlugging this value in:\n$$\n\\rho_{\\perp}^2 \\approx \\frac{3.25 - 3(0.9951847267)}{6.25 - 6(0.9951847267)} = \\frac{3.25 - 2.9855541801}{6.25 - 5.9711083602} = \\frac{0.2644458199}{0.2788916398} \\approx 0.94820128\n$$\nTaking the square root:\n$$\n\\rho_{\\perp} \\approx \\sqrt{0.94820128} \\approx 0.97375626\n$$\nRounding to four significant figures, we get $0.9738$.",
            "answer": "$$\n\\boxed{0.9738}\n$$"
        },
        {
            "introduction": "The standard SOR and SSOR methods are part of a much broader family of polynomial-based iterative methods. This advanced exercise recasts relaxation strategies in the powerful language of polynomial approximation theory to understand their ultimate performance limits. By using the properties of Chebyshev polynomials, you will construct an optimal two-step relaxation polynomial and rigorously compare its error-damping capabilities to a simple one-step scheme, revealing the fundamental principles behind more advanced and rapidly converging iterative techniques .",
            "id": "3451592",
            "problem": "Consider the boundary value problem for the Poisson equation $-u''(x) = f(x)$ on the interval $(0,1)$ with homogeneous Dirichlet boundary conditions $u(0) = 0$ and $u(1) = 0$. Using a standard second-order centered finite-difference scheme with $N$ interior grid points leads to a symmetric positive definite (SPD) linear system $A u = b$. For sufficiently large $N$, the spectrum of $A$ is contained in a closed interval $[\\alpha,\\beta]$ with $0 < \\alpha < \\beta$, which is assumed known a priori.\n\nA single forwardâ€“backward sweep of Symmetric Successive Over-Relaxation (SSOR) can be abstracted, in the error-propagation sense, as the application of a symmetric two-step relaxation polynomial in $A$ to the current error. Motivated by this perspective, consider the following two strategies for reducing the error:\n\n1. A two-step symmetric relaxation characterized by a degree-two polynomial $p_{2}(\\lambda)$ applied to the error, where $\\lambda$ denotes an eigenvalue in $[\\alpha,\\beta]$. The polynomial must satisfy $p_{2}(0) = 1$ to ensure consistency of the iteration. Using Chebyshev semi-iterative theory, construct the degree-two relaxation polynomial that minimizes the worst-case error-amplification over the spectral interval, i.e., minimizes $\\max_{\\lambda \\in [\\alpha,\\beta]} |p_{2}(\\lambda)|$ under the constraint $p_{2}(0) = 1$.\n\n2. A one-step relaxation that mimics optimal Successive Over-Relaxation (SOR) in the sense of a single linear relaxation of the form $p_{1}(\\lambda) = 1 - \\tau \\lambda$, where the scalar $\\tau$ is chosen to minimize the worst-case error-amplification $\\max_{\\lambda \\in [\\alpha,\\beta]} |1 - \\tau \\lambda|$.\n\nStarting from fundamental properties of Chebyshev polynomials and the minimax characterization of optimal scalar relaxation on an interval, derive the optimal worst-case amplification factors for both strategies on $[\\alpha,\\beta]$, and then compute the exact ratio\n$$\nR(\\alpha,\\beta) = \\frac{\\displaystyle \\min_{p_{2}} \\max_{\\lambda \\in [\\alpha,\\beta]} |p_{2}(\\lambda)|}{\\displaystyle \\min_{\\tau} \\max_{\\lambda \\in [\\alpha,\\beta]} |1 - \\tau \\lambda|}.\n$$\nExpress your final answer as a single closed-form analytic expression in terms of $\\alpha$ and $\\beta$. No numerical rounding is required.",
            "solution": "The user wants to find the ratio of the optimal worst-case error amplification factors for a two-step symmetric relaxation method and a one-step linear relaxation method for solving a linear system whose coefficient matrix $A$ has a spectrum contained in the interval $[\\alpha, \\beta]$, where $0 < \\alpha < \\beta$.\n\nLet's analyze each strategy separately.\n\n### Strategy 2: One-Step Relaxation (Denominator)\n\nThe one-step relaxation method corresponds to an error propagation polynomial of degree one, $p_1(\\lambda) = 1 - \\tau\\lambda$. The goal is to find the value of the relaxation parameter $\\tau$ that minimizes the maximum absolute value of $p_1(\\lambda)$ over the spectral interval $[\\alpha, \\beta]$. We need to solve the minimax problem:\n$$\n\\min_{\\tau} \\max_{\\lambda \\in [\\alpha,\\beta]} |p_{1}(\\lambda)| = \\min_{\\tau} \\max_{\\lambda \\in [\\alpha,\\beta]} |1 - \\tau \\lambda|\n$$\nThe function $p_1(\\lambda)$ is a linear function of $\\lambda$. For a fixed $\\tau$, the maximum of $|p_1(\\lambda)|$ on the interval $[\\alpha, \\beta]$ must be attained at one of the endpoints, $\\lambda = \\alpha$ or $\\lambda = \\beta$. Therefore, we want to minimize $\\max(|1-\\tau\\alpha|, |1-\\tau\\beta|)$.\n\nThe minimum of this maximum value is achieved when the magnitudes at the endpoints are equal (equioscillation principle):\n$$\n|1 - \\tau \\alpha| = |1 - \\tau \\beta|\n$$\nThis implies either $1 - \\tau \\alpha = 1 - \\tau \\beta$, which gives $\\tau(\\beta-\\alpha) = 0$ and thus $\\tau=0$ (since $\\beta \\neq \\alpha$), leading to a non-optimal amplification factor of $1$. The optimal solution comes from the other case:\n$$\n1 - \\tau \\alpha = -(1 - \\tau \\beta) = -1 + \\tau \\beta\n$$\nSolving for $\\tau$:\n$$\n2 = \\tau \\alpha + \\tau \\beta = \\tau(\\alpha + \\beta)\n$$\nThe optimal relaxation parameter is $\\tau_{opt} = \\frac{2}{\\alpha + \\beta}$.\n\nWith this optimal $\\tau$, the error-amplification factor at the endpoints is:\n$$\n|p_1(\\alpha)| = \\left|1 - \\frac{2\\alpha}{\\alpha + \\beta}\\right| = \\left|\\frac{\\alpha + \\beta - 2\\alpha}{\\alpha + \\beta}\\right| = \\left|\\frac{\\beta - \\alpha}{\\alpha + \\beta}\\right| = \\frac{\\beta - \\alpha}{\\alpha + \\beta}\n$$\n$$\n|p_1(\\beta)| = \\left|1 - \\frac{2\\beta}{\\alpha + \\beta}\\right| = \\left|\\frac{\\alpha + \\beta - 2\\beta}{\\alpha + \\beta}\\right| = \\left|\\frac{\\alpha - \\beta}{\\alpha + \\beta}\\right| = \\frac{\\beta - \\alpha}{\\alpha + \\beta}\n$$\nSince $p_1(\\lambda)$ is linear, the maximum value on the interval is indeed this value. So, the denominator of the required ratio is:\n$$\n\\min_{\\tau} \\max_{\\lambda \\in [\\alpha,\\beta]} |1 - \\tau \\lambda| = \\frac{\\beta - \\alpha}{\\alpha + \\beta}\n$$\n\n### Strategy 1: Two-Step Symmetric Relaxation (Numerator)\n\nThe two-step method corresponds to an error propagation polynomial $p_2(\\lambda)$ of degree two. The consistency condition implies that for an eigenvalue of $0$, the iteration matrix should be the identity, meaning the error is unchanged. This translates to the constraint $p_2(0) = 1$. The problem is to find the polynomial of degree at most $2$ satisfying $p_2(0)=1$ which has the minimum possible maximum absolute value on $[\\alpha, \\beta]$. This is a classic problem in approximation theory, whose solution involves Chebyshev polynomials.\n\nLet $T_n(x)$ be the Chebyshev polynomial of the first kind of degree $n$. The solution to the problem \"find the polynomial $p_n(x)$ of degree $n$ that minimizes $\\max_{x \\in [-1,1]}|p_n(x)|$ subject to a constraint $p_n(x_0)=1$ for a point $x_0 \\notin [-1,1]$\" is given by $p_n(x) = \\frac{T_n(x)}{T_n(x_0)}$. The minimum value of the maximum is $\\frac{1}{|T_n(x_0)|}$.\n\nFirst, we map the spectral interval $[\\alpha, \\beta]$ to the canonical interval $[-1, 1]$ using a linear transformation:\n$$\nx = \\frac{2\\lambda - (\\alpha + \\beta)}{\\beta - \\alpha}\n$$\nThe constraint is given at $\\lambda = 0$. We must find the corresponding point $x_0$ in the transformed coordinate system:\n$$\nx_0 = \\frac{2(0) - (\\alpha + \\beta)}{\\beta - \\alpha} = -\\frac{\\alpha + \\beta}{\\beta - \\alpha}\n$$\nSince $0 < \\alpha < \\beta$, we have $\\alpha+\\beta > \\beta-\\alpha > 0$, which implies $x_0 < -1$. The constraint point lies outside the interval $[-1, 1]$, as required.\n\nThe problem is now to find the polynomial $q_2(x)$ of degree $2$ that minimizes $\\max_{x \\in [-1,1]}|q_2(x)|$ subject to $q_2(x_0) = 1$. The solution is $q_2(x) = \\frac{T_2(x)}{T_2(x_0)}$, and the minimax value is $\\frac{1}{|T_2(x_0)|}$.\n\nWe use the formula for the Chebyshev polynomial of degree $2$: $T_2(x) = 2x^2 - 1$.\nNow, we evaluate $T_2(x_0)$:\n$$\nT_2(x_0) = 2x_0^2 - 1 = 2 \\left( -\\frac{\\alpha + \\beta}{\\beta - \\alpha} \\right)^2 - 1 = 2 \\frac{(\\alpha + \\beta)^2}{(\\beta - \\alpha)^2} - 1\n$$\n$$\nT_2(x_0) = \\frac{2(\\alpha + \\beta)^2 - (\\beta - \\alpha)^2}{(\\beta - \\alpha)^2} = \\frac{2(\\alpha^2 + 2\\alpha\\beta + \\beta^2) - (\\beta^2 - 2\\alpha\\beta + \\alpha^2)}{(\\beta - \\alpha)^2}\n$$\n$$\nT_2(x_0) = \\frac{(2\\alpha^2 - \\alpha^2) + (4\\alpha\\beta + 2\\alpha\\beta) + (2\\beta^2 - \\beta^2)}{(\\beta - \\alpha)^2} = \\frac{\\alpha^2 + 6\\alpha\\beta + \\beta^2}{(\\beta - \\alpha)^2}\n$$\nSince $\\alpha, \\beta > 0$, the numerator $\\alpha^2 + 6\\alpha\\beta + \\beta^2$ is positive. The denominator is also positive. Thus, $T_2(x_0) > 0$, and $|T_2(x_0)| = T_2(x_0)$.\n\nThe minimal worst-case amplification factor for the two-step method is:\n$$\n\\min_{p_2, p_2(0)=1} \\max_{\\lambda \\in [\\alpha,\\beta]} |p_2(\\lambda)| = \\frac{1}{T_2(x_0)} = \\frac{(\\beta - \\alpha)^2}{\\alpha^2 + 6\\alpha\\beta + \\beta^2}\n$$\n\n### The Ratio $R(\\alpha, \\beta)$\n\nFinally, we compute the ratio of the amplification factor of the two-step method to that of the one-step method:\n$$\nR(\\alpha, \\beta) = \\frac{\\displaystyle \\min_{p_{2}} \\max_{\\lambda \\in [\\alpha,\\beta]} |p_{2}(\\lambda)|}{\\displaystyle \\min_{\\tau} \\max_{\\lambda \\in [\\alpha,\\beta]} |1 - \\tau \\lambda|} = \\frac{\\frac{(\\beta - \\alpha)^2}{\\alpha^2 + 6\\alpha\\beta + \\beta^2}}{\\frac{\\beta - \\alpha}{\\alpha + \\beta}}\n$$\nWe can simplify this expression:\n$$\nR(\\alpha, \\beta) = \\frac{(\\beta - \\alpha)^2}{\\alpha^2 + 6\\alpha\\beta + \\beta^2} \\cdot \\frac{\\alpha + \\beta}{\\beta - \\alpha}\n$$\nAssuming $\\beta \\neq \\alpha$, we can cancel a factor of $(\\beta - \\alpha)$:\n$$\nR(\\alpha, \\beta) = \\frac{(\\beta - \\alpha)(\\alpha + \\beta)}{\\alpha^2 + 6\\alpha\\beta + \\beta^2}\n$$\nThe numerator is a difference of squares, $\\beta^2 - \\alpha^2$.\n$$\nR(\\alpha, \\beta) = \\frac{\\beta^2 - \\alpha^2}{\\alpha^2 + 6\\alpha\\beta + \\beta^2}\n$$\nThis is the final closed-form expression for the ratio in terms of $\\alpha$ and $\\beta$.",
            "answer": "$$\\boxed{\\frac{\\beta^2 - \\alpha^2}{\\alpha^2 + 6\\alpha\\beta + \\beta^2}}$$"
        }
    ]
}