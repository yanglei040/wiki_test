{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the mechanics of the steepest descent method, it is invaluable to perform a manual calculation on a small-scale system. This exercise  guides you through this process, starting from the discretization of a simple one-dimensional boundary value problem to create a $2 \\times 2$ linear system. By executing a single iteration, you will solidify your understanding of how the gradient of the energy functional defines the search direction and how an exact line search determines the optimal step length.",
            "id": "3421077",
            "problem": "Consider the one-dimensional, second-order elliptic boundary value problem $-u''(x)=f(x)$ on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$. Discretize the domain with a uniform grid of two interior points at locations $x_{1}=\\frac{1}{3}$ and $x_{2}=\\frac{2}{3}$, so that the grid spacing is $h=\\frac{1}{3}$. Use the standard second-order centered finite difference approximation for the second derivative, namely\n$$\n-\\frac{u_{i-1}-2u_{i}+u_{i+1}}{h^{2}}=f(x_{i}),\n$$\nwith the boundary values incorporated as $u_{0}=u_{3}=0$. This yields a linear system $A x=b$, where $A$ is a $2\\times 2$ Symmetric Positive Definite (SPD) matrix and $b$ is a $2$-vector. Let the source term be $f(x)=\\sin(\\pi x)$, and construct the corresponding $A$ and $b$ for this discretization.\n\nDefine the quadratic functional\n$$\nJ(x)=\\frac{1}{2} x^{\\top} A x - b^{\\top} x,\n$$\nassociated with the linear system. Starting from the initial guess $x_{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$, perform one iteration of the method of steepest descent: choose the search direction to be the negative gradient of $J$ at $x_{0}$, and choose the step length by minimizing $J$ along that direction. Compute the step length $\\alpha_{0}$, the updated iterate $x_{1}$, and the functional value $J(x_{1})$. Express all quantities exactly; no rounding is required. Provide your final answer as a single row matrix containing, in order, $\\alpha_{0}$, the two components of $x_{1}$, and $J(x_{1})$.",
            "solution": "The user wants to solve a numerical analysis problem involving the method of steepest descent.\n\n### Step 1: Extract Givens\n-   **Problem**: One-dimensional, second-order elliptic boundary value problem $-u''(x)=f(x)$ on the interval $[0,1]$.\n-   **Boundary conditions**: Homogeneous Dirichlet, $u(0)=u(1)=0$.\n-   **Discretization**: Uniform grid with two interior points $x_{1}=\\frac{1}{3}$ and $x_{2}=\\frac{2}{3}$.\n-   **Grid spacing**: $h=\\frac{1}{3}$.\n-   **Finite difference approximation**: $-\\frac{u_{i-1}-2u_{i}+u_{i+1}}{h^{2}}=f(x_{i})$.\n-   **Boundary values for discretization**: $u_{0}=0$, $u_{3}=0$.\n-   **Linear System**: The discretization yields $A x=b$, where $A$ is a $2\\times 2$ Symmetric Positive Definite (SPD) matrix, $x = \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix}$, and $b$ is a $2$-vector.\n-   **Source term**: $f(x)=\\sin(\\pi x)$.\n-   **Quadratic functional**: $J(x)=\\frac{1}{2} x^{\\top} A x - b^{\\top} x$.\n-   **Initial guess**: $x_{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\n-   **Task**: Perform one iteration of the method of steepest descent. The search direction is the negative gradient of $J$ at $x_{0}$, and the step length minimizes $J$ along that direction.\n-   **Required output**: The step length $\\alpha_{0}$, the updated iterate $x_{1}$, and the functional value $J(x_{1})$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It describes a standard application of the finite difference method to a simple boundary value problem (the Poisson equation in one dimension) and then asks to apply a classic iterative method, the method of steepest descent, to solve the resulting linear system. All the components (the differential equation, the discretization scheme, the definition of the quadratic functional, and the steepest descent algorithm) are fundamental concepts in numerical analysis and scientific computing.\n\nThe problem is well-posed. All necessary data and definitions are provided to construct the linear system $A x=b$ and to perform one iteration of the specified algorithm. The matrix $A$ resulting from this standard central difference discretization of the negative second derivative operator with Dirichlet boundary conditions is known to be symmetric and positive definite, which guarantees that the quadratic functional $J(x)$ has a unique minimum and the steepest descent method is well-defined. The problem is stated objectively and without ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full solution.\n\n### Solution\nFirst, we construct the linear system $A x=b$. The computational grid points are $x_{0}=0$, $x_{1}=\\frac{1}{3}$, $x_{2}=\\frac{2}{3}$, and $x_{3}=1$. The vector of unknowns is $x = \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix}$, where $u_i \\approx u(x_i)$. The grid spacing is $h=\\frac{1}{3}$, so $h^2 = \\frac{1}{9}$.\n\nWe apply the given finite difference formula at each interior point.\nFor $i=1$:\n$$-\\frac{u_{0}-2u_{1}+u_{2}}{h^{2}}=f(x_{1})$$\nUsing the boundary condition $u_{0}=0$ and $h^2=\\frac{1}{9}$:\n$$-\\frac{-2u_{1}+u_{2}}{1/9}=f(x_{1}) \\implies 9(2u_{1}-u_{2}) = f(x_{1})$$\nThis simplifies to $18u_1 - 9u_2 = f(x_1)$.\n\nFor $i=2$:\n$$-\\frac{u_{1}-2u_{2}+u_{3}}{h^{2}}=f(x_{2})$$\nUsing the boundary condition $u_{3}=0$:\n$$-\\frac{u_{1}-2u_{2}}{1/9}=f(x_{2}) \\implies 9(-u_{1}+2u_{2}) = f(x_2)$$\nThis simplifies to $-9u_1 + 18u_2 = f(x_2)$.\n\nFrom these two equations, we can identify the matrix $A$:\n$$A = \\begin{pmatrix} 18 & -9 \\\\ -9 & 18 \\end{pmatrix}$$\n\nNext, we construct the vector $b$ using the source term $f(x)=\\sin(\\pi x)$:\n$$b = \\begin{pmatrix} f(x_1) \\\\ f(x_2) \\end{pmatrix} = \\begin{pmatrix} f(1/3) \\\\ f(2/3) \\end{pmatrix} = \\begin{pmatrix} \\sin(\\pi/3) \\\\ \\sin(2\\pi/3) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix}$$\n\nNow we perform one iteration of the method of steepest descent. We start with the initial guess $x_{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\nThe search direction $p_k$ at step $k$ is the negative gradient of $J(x)$ at $x_k$. The gradient is $\\nabla J(x) = Ax - b$.\nSo, the search direction is $p_k = - \\nabla J(x_k) = b - Ax_k$. This vector is also known as the residual, $r_k$.\nFor the first iteration ($k=0$), the residual is:\n$$r_0 = b - Ax_{0} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} - \\begin{pmatrix} 18 & -9 \\\\ -9 & 18 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix}$$\nSo, $p_0 = r_0 = b$.\n\nThe step length $\\alpha_0$ is chosen to minimize $J(x_{0}+\\alpha p_{0})$. The well-known formula for the optimal step length for the residual $r_k$ as the search direction is:\n$$\\alpha_k = \\frac{r_k^{\\top} r_k}{r_k^{\\top} A r_k}$$\nFor our first iteration, since $r_0=b$:\n$$\\alpha_0 = \\frac{b^{\\top} b}{b^{\\top} A b}$$\nWe compute the required vector products:\n$$b^{\\top}b = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} & \\frac{\\sqrt{3}}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} = \\left(\\frac{\\sqrt{3}}{2}\\right)^{2} + \\left(\\frac{\\sqrt{3}}{2}\\right)^{2} = \\frac{3}{4} + \\frac{3}{4} = \\frac{6}{4} = \\frac{3}{2}$$\nNext, we compute $Ab$:\n$$Ab = \\begin{pmatrix} 18 & -9 \\\\ -9 & 18 \\end{pmatrix} \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} = \\begin{pmatrix} 18(\\frac{\\sqrt{3}}{2}) - 9(\\frac{\\sqrt{3}}{2}) \\\\ -9(\\frac{\\sqrt{3}}{2}) + 18(\\frac{\\sqrt{3}}{2}) \\end{pmatrix} = \\begin{pmatrix} 9\\frac{\\sqrt{3}}{2} \\\\ 9\\frac{\\sqrt{3}}{2} \\end{pmatrix} = 9 \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} = 9b$$\nThen, we compute $b^{\\top}Ab$:\n$$b^{\\top}Ab = b^{\\top}(9b) = 9(b^{\\top}b) = 9 \\left(\\frac{3}{2}\\right) = \\frac{27}{2}$$\nNow we can find $\\alpha_{0}$:\n$$\\alpha_0 = \\frac{3/2}{27/2} = \\frac{3}{27} = \\frac{1}{9}$$\n\nThe updated iterate $x_{1}$ is given by:\n$$x_{1} = x_{0} + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{9} \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{18} \\\\ \\frac{\\sqrt{3}}{18} \\end{pmatrix}$$\n\nFinally, we compute the functional value $J(x_1)$:\n$$J(x_1) = \\frac{1}{2} x_1^{\\top} A x_1 - b^{\\top} x_1$$\nWe first compute the terms $A x_1$ and $b^{\\top} x_1$:\n$$A x_1 = \\begin{pmatrix} 18 & -9 \\\\ -9 & 18 \\end{pmatrix} \\begin{pmatrix} \\frac{\\sqrt{3}}{18} \\\\ \\frac{\\sqrt{3}}{18} \\end{pmatrix} = \\frac{\\sqrt{3}}{18} \\begin{pmatrix} 18 - 9 \\\\ -9 + 18 \\end{pmatrix} = \\frac{\\sqrt{3}}{18} \\begin{pmatrix} 9 \\\\ 9 \\end{pmatrix} = \\begin{pmatrix} \\frac{9\\sqrt{3}}{18} \\\\ \\frac{9\\sqrt{3}}{18} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} = b$$\nSince $Ax_1 = b$, this means that $x_1$ is the exact solution to the linear system. This is a special case that occurs because the initial residual $r_0=b$ is an eigenvector of the matrix $A$.\nNow we compute $b^{\\top}x_1$:\n$$b^{\\top}x_1 = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} & \\frac{\\sqrt{3}}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{\\sqrt{3}}{18} \\\\ \\frac{\\sqrt{3}}{18} \\end{pmatrix} = \\left(\\frac{\\sqrt{3}}{2}\\right)\\left(\\frac{\\sqrt{3}}{18}\\right) + \\left(\\frac{\\sqrt{3}}{2}\\right)\\left(\\frac{\\sqrt{3}}{18}\\right) = 2 \\left(\\frac{3}{36}\\right) = \\frac{6}{36} = \\frac{1}{6}$$\nWe can now compute $J(x_1)$. Since $Ax_1 = b$, we can write $x_1^\\top A x_1 = x_1^\\top b$. But $A$ is symmetric, so $x_1^\\top A x_1 = (Ax_1)^\\top x_1 = b^\\top x_1$.\n$$J(x_1) = \\frac{1}{2} x_1^{\\top} (A x_1) - b^{\\top} x_1 = \\frac{1}{2} x_1^{\\top} b - b^{\\top} x_1 = \\frac{1}{2} b^{\\top} x_1 - b^{\\top} x_1 = -\\frac{1}{2} b^{\\top} x_1$$\nSubstituting the value of $b^{\\top}x_1$:\n$$J(x_1) = -\\frac{1}{2} \\left(\\frac{1}{6}\\right) = -\\frac{1}{12}$$\n\nThe required quantities are:\n- Step length: $\\alpha_0 = \\frac{1}{9}$\n- Updated iterate: $x_1 = \\begin{pmatrix} \\frac{\\sqrt{3}}{18} \\\\ \\frac{\\sqrt{3}}{18} \\end{pmatrix}$\n- Functional value: $J(x_1) = -\\frac{1}{12}$\n\nThe final answer is presented as a row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{9} & \\frac{\\sqrt{3}}{18} & \\frac{\\sqrt{3}}{18} & -\\frac{1}{12}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While iterative methods provide a sequence of approximations, a critical question is how to measure progress and when to stop. This practice  delves into the theory by asking you to derive the exact relationship between the error in the energy functional, $J(x_{k}) - J(x^{\\ast})$, and the norm of the residual vector, $r_k$. This connection is fundamental for creating robust stopping criteria, and the exercise culminates in developing a practical test based on the computable Euclidean norm of the residual.",
            "id": "3421083",
            "problem": "Consider a linear second-order elliptic Partial Differential Equation (PDE) posed on a bounded domain with homogeneous Dirichlet boundary conditions, discretized by a conforming finite element method to obtain a linear system with a symmetric positive definite (SPD) stiffness matrix $A \\in \\mathbb{R}^{n \\times n}$ and a load vector $b \\in \\mathbb{R}^{n}$. Define the quadratic energy functional $J(x)$ by $J(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$, whose unique minimizer $x^{\\ast}$ satisfies $A x^{\\ast} = b$. Let the steepest descent method be applied to minimize $J(x)$, yielding iterates $x_{k}$ with residual $r_{k} = b - A x_{k}$. Assume a known spectral lower bound $m > 0$ such that $A \\succeq m I$, where $I$ is the identity matrix.\n\nStarting from the variational characterization $x^{\\ast} = \\arg\\min_{x \\in \\mathbb{R}^{n}} J(x)$ and the definition of the residual $r_{k}$, derive an exact identity relating the energy error $J(x_{k}) - J(x^{\\ast})$ to the residual norm $\\| r_{k} \\|_{A^{-1}}$, where $\\| r \\|_{A^{-1}}^{2} := r^{\\top} A^{-1} r$. Then, propose a stopping rule for the steepest descent iteration that guarantees a prescribed tolerance $\\delta > 0$ in the energy error, i.e., $J(x_{k}) - J(x^{\\ast}) \\leq \\delta$. Finally, assuming only the spectral lower bound $m$ is available and that the Euclidean norm $\\| r_{k} \\|$ is the quantity that can be computed, determine the smallest explicit upper bound on $\\| r_{k} \\|$ as a function of $m$ and $\\delta$ that ensures $J(x_{k}) - J(x^{\\ast}) \\leq \\delta$. Express your final answer as a single closed-form analytic expression. No rounding is required.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the well-established theory of numerical linear algebra and the finite element method for solving partial differential equations. The premises are mathematically sound, consistent, and all necessary information for the derivation is provided. The terminology is precise and objective. There are no identifiable flaws.\n\nThe problem asks for three distinct results:\n1. An exact identity relating the energy error $J(x_{k}) - J(x^{\\ast})$ to the residual norm $\\| r_{k} \\|_{A^{-1}}$.\n2. A stopping rule for the steepest descent iteration based on this identity.\n3. A computable upper bound on the Euclidean norm of the residual, $\\| r_{k} \\|$, that guarantees the energy error is within a tolerance $\\delta$.\n\nWe shall address each in sequence.\n\nFirst, we derive the identity for the energy error. The energy functional is given by $J(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$. The unique minimizer $x^{\\ast}$ satisfies the normal equations $\\nabla J(x^{\\ast}) = A x^{\\ast} - b = 0$, which implies $A x^{\\ast} = b$. The energy error at the $k$-th iterate, $x_k$, is the difference $J(x_k) - J(x^{\\ast})$.\n\nLet us expand this expression:\n$$ J(x_{k}) - J(x^{\\ast}) = \\left(\\frac{1}{2} x_{k}^{\\top} A x_{k} - b^{\\top} x_{k}\\right) - \\left(\\frac{1}{2} (x^{\\ast})^{\\top} A x^{\\ast} - b^{\\top} x^{\\ast}\\right) $$\nSubstitute $b = A x^{\\ast}$ into the expression:\n$$ J(x_{k}) - J(x^{\\ast}) = \\frac{1}{2} x_{k}^{\\top} A x_{k} - (A x^{\\ast})^{\\top} x_{k} - \\frac{1}{2} (x^{\\ast})^{\\top} A x^{\\ast} + (A x^{\\ast})^{\\top} x^{\\ast} $$\nSince the matrix $A$ is symmetric, $(A x^{\\ast})^{\\top} = (x^{\\ast})^{\\top} A^{\\top} = (x^{\\ast})^{\\top} A$.\n$$ J(x_{k}) - J(x^{\\ast}) = \\frac{1}{2} x_{k}^{\\top} A x_{k} - (x^{\\ast})^{\\top} A x_{k} - \\frac{1}{2} (x^{\\ast})^{\\top} A x^{\\ast} + (x^{\\ast})^{\\top} A x^{\\ast} $$\n$$ J(x_{k}) - J(x^{\\ast}) = \\frac{1}{2} x_{k}^{\\top} A x_{k} - (x^{\\ast})^{\\top} A x_{k} + \\frac{1}{2} (x^{\\ast})^{\\top} A x^{\\ast} $$\nThis expression is a quadratic form that can be factored as:\n$$ J(x_{k}) - J(x^{\\ast}) = \\frac{1}{2} (x_{k} - x^{\\ast})^{\\top} A (x_{k} - x^{\\ast}) $$\nLet the error vector be $e_{k} = x_{k} - x^{\\ast}$. The energy error is thus $\\frac{1}{2} e_{k}^{\\top} A e_{k}$, which is half the squared $A$-norm of the error, $\\frac{1}{2} \\| e_{k} \\|_{A}^{2}$.\n\nNext, we relate the error $e_k$ to the residual $r_k = b - A x_k$.\n$$ r_{k} = b - A x_{k} = A x^{\\ast} - A x_{k} = A(x^{\\ast} - x_{k}) = -A(x_{k} - x^{\\ast}) = -A e_{k} $$\nSince $A$ is symmetric positive definite (SPD), it is invertible. Thus, we can express the error as $e_{k} = -A^{-1} r_{k}$.\n\nSubstituting this expression for $e_k$ into the equation for the energy error:\n$$ J(x_{k}) - J(x^{\\ast}) = \\frac{1}{2} (-A^{-1} r_{k})^{\\top} A (-A^{-1} r_{k}) $$\n$$ J(x_{k}) - J(x^{\\ast}) = \\frac{1}{2} (r_{k}^{\\top} (A^{-1})^{\\top}) A (A^{-1} r_{k}) $$\nSince $A$ is symmetric, its inverse $A^{-1}$ is also symmetric, so $(A^{-1})^{\\top} = A^{-1}$.\n$$ J(x_{k}) - J(x^{\\ast}) = \\frac{1}{2} r_{k}^{\\top} A^{-1} A A^{-1} r_{k} = \\frac{1}{2} r_{k}^{\\top} (A^{-1} A) A^{-1} r_{k} = \\frac{1}{2} r_{k}^{\\top} I A^{-1} r_{k} $$\n$$ J(x_{k}) - J(x^{\\ast}) = \\frac{1}{2} r_{k}^{\\top} A^{-1} r_{k} $$\nUsing the notation provided in the problem, $\\| r \\|_{A^{-1}}^{2} := r^{\\top} A^{-1} r$, we arrive at the desired identity:\n$$ J(x_{k}) - J(x^{\\ast}) = \\frac{1}{2} \\| r_{k} \\|_{A^{-1}}^{2} $$\n\nSecond, we propose a stopping rule. The objective is to terminate the iteration when $J(x_{k}) - J(x^{\\ast}) \\leq \\delta$ for a given tolerance $\\delta > 0$. Using the identity just derived, this condition is equivalent to:\n$$ \\frac{1}{2} \\| r_{k} \\|_{A^{-1}}^{2} \\leq \\delta \\quad \\iff \\quad \\| r_{k} \\|_{A^{-1}}^{2} \\leq 2\\delta $$\nA stopping rule is therefore to terminate the iteration at step $k$ if $\\| r_{k} \\|_{A^{-1}}^{2} \\leq 2\\delta$. It is important to note that computing $\\| r_{k} \\|_{A^{-1}}^{2} = r_{k}^{\\top} A^{-1} r_{k}$ requires applying the inverse of $A$ to a vector, which is computationally as expensive as solving the original linear system. Hence, this criterion is not practical, which motivates the final part of the problem.\n\nThird, we derive a practical stopping criterion based on the computable Euclidean norm $\\|r_k\\|$ and the given spectral lower bound $m > 0$. The condition $A \\succeq mI$ means that for any non-zero vector $v \\in \\mathbb{R}^n$, we have $v^{\\top} A v \\geq m \\|v\\|^2$. This implies that the smallest eigenvalue of $A$, $\\lambda_{\\min}(A)$, satisfies $\\lambda_{\\min}(A) \\geq m$.\n\nThe eigenvalues of the inverse matrix $A^{-1}$ are the reciprocals of the eigenvalues of $A$. Therefore, the largest eigenvalue of $A^{-1}$, $\\lambda_{\\max}(A^{-1})$, is bounded as follows:\n$$ \\lambda_{\\max}(A^{-1}) = \\frac{1}{\\lambda_{\\min}(A)} \\leq \\frac{1}{m} $$\nThe quantity $r_{k}^{\\top} A^{-1} r_{k}$ can be bounded above using the Rayleigh quotient for the matrix $A^{-1}$:\n$$ \\frac{r_{k}^{\\top} A^{-1} r_{k}}{r_{k}^{\\top} r_{k}} \\leq \\lambda_{\\max}(A^{-1}) \\implies r_{k}^{\\top} A^{-1} r_{k} \\leq \\lambda_{\\max}(A^{-1}) \\|r_{k}\\|^{2} $$\nCombining these inequalities, we obtain a bound on $r_{k}^{\\top} A^{-1} r_{k}$ in terms of the Euclidean norm $\\|r_k\\|$:\n$$ r_{k}^{\\top} A^{-1} r_{k} \\leq \\frac{1}{m} \\|r_{k}\\|^{2} $$\nNow, we substitute this inequality into the expression for the energy error:\n$$ J(x_{k}) - J(x^{\\ast}) = \\frac{1}{2} r_{k}^{\\top} A^{-1} r_{k} \\leq \\frac{1}{2m} \\|r_{k}\\|^{2} $$\nTo guarantee that the energy error does not exceed the tolerance $\\delta$, i.e., $J(x_{k}) - J(x^{\\ast}) \\leq \\delta$, it is sufficient to enforce the more stringent condition:\n$$ \\frac{\\|r_{k}\\|^{2}}{2m} \\leq \\delta $$\nSolving for $\\|r_{k}\\|$, we find:\n$$ \\|r_{k}\\|^{2} \\leq 2m\\delta \\implies \\|r_{k}\\| \\leq \\sqrt{2m\\delta} $$\nThis inequality provides a stopping criterion based on the Euclidean norm of the residual, which is readily computable at each step of an iterative method. The threshold value $\\sqrt{2m\\delta}$ represents an upper bound on $\\|r_k\\|$ that, if satisfied, ensures the energy error tolerance is met. This bound is the tightest possible given only the lower spectral bound $m$, as equality can be achieved if $A=mI$. Therefore, the smallest explicit upper bound on $\\|r_k\\|$ that ensures the condition is met is $\\sqrt{2m\\delta}$.",
            "answer": "$$\\boxed{\\sqrt{2m\\delta}}$$"
        },
        {
            "introduction": "The convergence rate of the steepest descent method is highly dependent on the conditioning of the system matrix $A$; preconditioning is a powerful technique to address this by reshaping the problem's geometry. This exercise  explores the crucial role of the preconditioner's properties by challenging you with a counterexample involving a non-symmetric preconditioner. By analyzing the degraded performance and deriving the necessary conditions on the preconditioner matrix $M$, you will gain a deeper appreciation for why it must be symmetric and positive definite to guarantee effective and reliable convergence.",
            "id": "3421050",
            "problem": "Consider the finite element discretization of a uniformly elliptic, self-adjoint second-order partial differential equation (PDE), which yields a symmetric positive definite (SPD) stiffness matrix $A \\in \\mathbb{R}^{n \\times n}$. The variational formulation leads to the quadratic functional $\\phi(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$, whose minimizer $x^{\\ast}$ solves $A x^{\\ast} = b$. The classical method of steepest descent, with an exact line search, updates $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$ along a descent direction $p_{k}$, and the step length $\\alpha_{k}$ minimizes $\\phi(x_{k} + \\alpha p_{k})$ over $\\alpha \\in \\mathbb{R}$. When one chooses an inner product induced by a matrix $M$, namely $(u,v)_{M} = u^{\\top} M v$, the steepest descent direction is found with respect to this inner product. A preconditioned steepest descent method uses a search direction $p_{k} = M^{-1} r_{k}$ with the residual $r_{k} = b - A x_{k}$. In practice, one often takes $M$ as a preconditioner approximating $A$.\n\nProvide a concrete counterexample showing that a naive choice of a non-symmetric $M$ breaks the symmetry of $A$ in the chosen $(\\cdot,\\cdot)_{M}$-inner product and worsens the steepest descent convergence. Use the following data:\n- $A = \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix}$,\n- the exact solution $x^{\\ast} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ so that $b = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$,\n- the initial iterate $x_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$,\n- the naive preconditioner $M = \\begin{pmatrix} 1 & 5 \\\\ 0 & 1 \\end{pmatrix}$.\n\nCarry out one exact-line-search step of the preconditioned steepest descent update $x_{1} = x_{0} + \\alpha_{0} p_{0}$ with $p_{0} = M^{-1} r_{0}$ and $r_{0} = b - A x_{0}$. Compute the precise reduction factor in the $A$-energy norm of the error, that is, the ratio\n$$\\frac{\\|x_{1} - x^{\\ast}\\|_{A}^{2}}{\\|x_{0} - x^{\\ast}\\|_{A}^{2}},$$\nwhere $\\|z\\|_{A}^{2} = z^{\\top} A z$. Express your final result as a simplified rational number.\n\nFinally, derive general conditions on $M$ under which the direction $p_{k} = M^{-1} r_{k}$ is the true steepest descent direction in the chosen inner product and the operator $M^{-1} A$ is self-adjoint with respect to $(\\cdot,\\cdot)_{M}$, ensuring effective descent and standard convergence guarantees. Do not use any shortcut formulas; begin from the definition of the gradient with respect to an inner product and the exact line search minimization of $\\phi$ along a given direction. No rounding is required for the reduction factor; express it exactly as a rational number.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n-   The system arises from a finite element discretization of a uniformly elliptic, self-adjoint second-order PDE.\n-   The stiffness matrix is $A = \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix}$, which is symmetric positive definite (SPD).\n-   The quadratic functional to minimize is $\\phi(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$.\n-   The minimizer $x^{\\ast}$ solves $A x^{\\ast} = b$.\n-   The exact solution is $x^{\\ast} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n-   The right-hand side vector is $b = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n-   The initial iterate is $x_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n-   The method is preconditioned steepest descent with update $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$.\n-   The step length $\\alpha_{k}$ is determined by an exact line search, minimizing $\\phi(x_{k} + \\alpha p_{k})$.\n-   The inner product is defined as $(u,v)_{M} = u^{\\top} M v$.\n-   The search direction is given by $p_{k} = M^{-1} r_{k}$, where the residual is $r_{k} = b - A x_{k}$.\n-   The preconditioner matrix is $M = \\begin{pmatrix} 1 & 5 \\\\ 0 & 1 \\end{pmatrix}$.\n-   The quantity to be computed is the reduction factor $\\frac{\\|x_{1} - x^{\\ast}\\|_{A}^{2}}{\\|x_{0} - x^{\\ast}\\|_{A}^{2}}$, where $\\|z\\|_{A}^{2} = z^{\\top} A z$.\n-   A derivation of general conditions on $M$ is also required.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is set in the context of numerical linear algebra and optimization, specifically the solution of linear systems derived from PDEs. All concepts, such as SPD matrices, quadratic functionals, steepest descent, preconditioning, and energy norms, are standard and well-established in this field.\n2.  **Well-Posed**: The problem requests a specific calculation based on a complete set of initial data and a derivation of a standard theoretical result. The calculation leads to a unique numerical value. The derivation is a standard exercise in the analysis of iterative methods.\n3.  **Objective**: The problem statement is precise and uses standard mathematical terminology without any subjective or ambiguous language.\n4.  **Completeness and Consistency**: All necessary matrices ($A$, $M$), vectors ($x_0$, $x^*$, $b$), and definitions are provided. The values are consistent; for instance, $x^*=0$ implies $b=Ax^*=0$, which is correctly given. The matrix $A$ is indeed SPD as its eigenvalues are $4$ and $1$.\n5.  **No other flaws**: The problem is neither trivial nor ill-posed. The choice of a non-symmetric $M$ is a valid pedagogical choice to illustrate the importance of the preconditioner's properties.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe solution consists of two parts. First, we perform the numerical calculation for one step of the described method. Second, we derive the general conditions on the matrix $M$.\n\n**Part 1: Calculation of the Reduction Factor**\n\n1.  **Initial State**:\n    The initial error is $e_{0} = x_{0} - x^{\\ast} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n    The initial residual is $r_{0} = b - A x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ -1 \\end{pmatrix}$.\n\n2.  **Initial Error in A-norm**:\n    The square of the initial error in the $A$-energy norm is:\n    $$ \\|x_{0} - x^{\\ast}\\|_{A}^{2} = e_{0}^{\\top} A e_{0} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix} = 4 + 1 = 5 $$\n\n3.  **Search Direction**:\n    The search direction is $p_{0} = M^{-1} r_{0}$. First, we compute the inverse of $M = \\begin{pmatrix} 1 & 5 \\\\ 0 & 1 \\end{pmatrix}$. The determinant is $\\det(M) = 1 \\cdot 1 - 5 \\cdot 0 = 1$.\n    $$ M^{-1} = \\frac{1}{1} \\begin{pmatrix} 1 & -5 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & -5 \\\\ 0 & 1 \\end{pmatrix} $$\n    Now, we compute $p_{0}$:\n    $$ p_{0} = \\begin{pmatrix} 1 & -5 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} -4 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -4 + 5 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n\n4.  **Exact Line Search Step Length $\\alpha_{0}$**:\n    We need to find $\\alpha_{0}$ that minimizes $g(\\alpha) = \\phi(x_{0} + \\alpha p_{0})$. The derivative with respect to $\\alpha$ is $g'(\\alpha) = \\nabla\\phi(x_0+\\alpha p_0)^\\top p_0 = (A(x_0+\\alpha p_0)-b)^\\top p_0 = (Ax_0-b+\\alpha Ap_0)^\\top p_0 = (-r_0+\\alpha Ap_0)^\\top p_0$. Setting to zero gives:\n    $$ \\alpha_{0} = \\frac{r_{0}^{\\top} p_{0}}{p_{0}^{\\top} A p_{0}} $$\n    Let's compute the terms:\n    $$ r_{0}^{\\top} p_{0} = \\begin{pmatrix} -4 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = -4 + 1 = -3 $$\n    $$ p_{0}^{\\top} A p_{0} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix} = 4 + 1 = 5 $$\n    So, the step length is:\n    $$ \\alpha_{0} = \\frac{-3}{5} $$\n\n5.  **New Iterate and Error**:\n    The new iterate is $x_{1} = x_{0} + \\alpha_{0} p_{0}$:\n    $$ x_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\frac{3}{5} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{3}{5} \\\\ 1 + \\frac{3}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{8}{5} \\end{pmatrix} $$\n    The new error is $e_{1} = x_{1} - x^{\\ast} = x_{1} = \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{8}{5} \\end{pmatrix}$.\n\n6.  **New Error in A-norm**:\n    The square of the new error in the $A$-energy norm is:\n    $$ \\|x_{1} - x^{\\ast}\\|_{A}^{2} = e_{1}^{\\top} A e_{1} = \\begin{pmatrix} \\frac{2}{5} & \\frac{8}{5} \\end{pmatrix} \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{8}{5} \\end{pmatrix} $$\n    $$ = 4 \\left(\\frac{2}{5}\\right)^{2} + 1 \\left(\\frac{8}{5}\\right)^{2} = 4 \\left(\\frac{4}{25}\\right) + \\frac{64}{25} = \\frac{16}{25} + \\frac{64}{25} = \\frac{80}{25} = \\frac{16}{5} $$\n\n7.  **Reduction Factor**:\n    The reduction factor is the ratio of the new and initial squared $A$-norms of the error:\n    $$ \\frac{\\|x_{1} - x^{\\ast}\\|_{A}^{2}}{\\|x_{0} - x^{\\ast}\\|_{A}^{2}} = \\frac{16/5}{5} = \\frac{16}{25} $$\n\n**Part 2: General Conditions on $M$**\n\n1.  **Steepest Descent Direction in the $M$-inner product**:\n    The gradient of the functional $\\phi(x)$ in the standard Euclidean inner product is $\\nabla \\phi(x) = Ax - b = -r$. The directional derivative of $\\phi$ at $x$ in a direction $v$ is $D\\phi(x)[v] = (\\nabla \\phi(x))^{\\top} v = -r^{\\top}v$.\n    The gradient with respect to the bilinear form $(u,v)_M = u^{\\top} M v$, denoted $\\nabla_{M} \\phi(x)$, is defined by the relation $D\\phi(x)[v] = (\\nabla_{M} \\phi(x), v)_{M}$ for all $v$.\n    Substituting the expressions, we get:\n    $$ -r^{\\top}v = (\\nabla_{M} \\phi(x))^{\\top} M v $$\n    This must hold for all vectors $v$, which implies equality of the vectors multiplying $v^{\\top}$ on the left and right:\n    $$ -r = M^{\\top} (\\nabla_{M} \\phi(x)) $$\n    Solving for the gradient $\\nabla_{M} \\phi(x)$ yields:\n    $$ \\nabla_{M} \\phi(x) = -(M^{\\top})^{-1} r $$\n    The direction of steepest descent is the negative of the gradient. Thus, the true steepest descent direction at step $k$ in the $(\\cdot, \\cdot)_M$ sense is $p_{k, \\text{true}} = -(-(M^{\\top})^{-1} r_k) = (M^{\\top})^{-1} r_k$.\n    The problem states the used direction is $p_k = M^{-1} r_k$. For this to be the true steepest descent direction, we must have:\n    $$ M^{-1} r_k = (M^{\\top})^{-1} r_k $$\n    This must hold for any possible residual vector $r_k$, so the matrices must be equal: $M^{-1} = (M^{\\top})^{-1}$. Taking the inverse of both sides gives $M = M^{\\top}$.\n    Thus, the matrix $M$ must be symmetric. For $(\\cdot, \\cdot)_{M}$ to be a valid inner product, $M$ must also be positive definite. The combined condition is that $M$ must be symmetric positive definite (SPD).\n\n2.  **Self-Adjointness of $M^{-1}A$ in the $M$-inner product**:\n    An operator $C$ is self-adjoint (symmetric) with respect to $(\\cdot, \\cdot)_M$ if $(Cu, v)_M = (u, Cv)_M$ for all $u, v$. Here, $C = M^{-1}A$.\n    The condition is $((M^{-1}A)u, v)_M = (u, (M^{-1}A)v)_M$.\n    Let's expand both sides:\n    LHS: $((M^{-1}A)u)^{\\top} M v = u^{\\top} A^{\\top} (M^{-1})^{\\top} M v$.\n    RHS: $u^{\\top} M (M^{-1}A v) = u^{\\top} I A v = u^{\\top} A v$.\n    Equating the two expressions, we get:\n    $$ u^{\\top} A^{\\top} (M^{-1})^{\\top} M v = u^{\\top} A v $$\n    Since this must hold for all $u,v$, the matrices must be equal:\n    $$ A^{\\top} (M^{-1})^{\\top} M = A $$\n    The problem specifies that $A$ is self-adjoint (symmetric), so $A^{\\top}=A$. The condition becomes:\n    $$ A (M^{\\top})^{-1} M = A $$\n    Since $A$ is SPD, it is invertible. We can multiply by $A^{-1}$ from the left:\n    $$ (M^{\\top})^{-1} M = I $$\n    Multiplying by $M^{\\top}$ from the left gives $M = M^{\\top}$.\n    Again, the condition for the preconditioned operator $M^{-1}A$ to be self-adjoint with respect to the $M$-inner product is that **$M$ must be symmetric**.\n\nIn summary, for the preconditioned steepest descent method to have the standard convergence properties associated with the classical method, which rely on the symmetry of the preconditioned operator ($M^{-1}A$) in the corresponding energy inner product ($(u,v)_M$), the preconditioner $M$ must be symmetric. The choice of a non-symmetric $M$ in this problem violates this fundamental requirement, leading to degraded performance, as demonstrated by the numerical result. For robustness and to guarantee convergence, $M$ should be SPD.",
            "answer": "$$\n\\boxed{\\frac{16}{25}}\n$$"
        }
    ]
}