{
    "hands_on_practices": [
        {
            "introduction": "This exercise explores the fundamental link between the geometric mapping of an element and the algebraic properties of the resulting discrete system. By analyzing a simple scaling transformation, you will derive how the mass and stiffness matrices behave as an element is uniformly compressed. This analysis is not merely academic; it directly reveals why meshes with highly distorted or small elements can impose severe restrictions on the time step size in explicit dynamic simulations.",
            "id": "3439531",
            "problem": "Consider the scalar diffusion equation in two spatial dimensions, posed on a single finite element, and its semi-discrete finite element method (FEM) formulation. Let the reference element be the square $\\hat{K} = [-1,1]^2$ with bilinear shape functions $\\{\\hat{N}_i\\}_{i=1}^4$ corresponding to the four corners. Use an isoparametric mapping $F_\\varepsilon : \\hat{K} \\to K_\\varepsilon$ defined by $\\boldsymbol{x} = F_\\varepsilon(\\hat{\\boldsymbol{x}}) = s(\\varepsilon) \\hat{\\boldsymbol{x}}$ with $s(\\varepsilon) = \\sqrt{\\varepsilon}$, where $\\varepsilon \\in (0,1]$ is a given parameter. This mapping yields a Jacobian matrix $J(\\hat{\\boldsymbol{x}}) = s(\\varepsilon) I$, where $I$ is the identity matrix, so that $\\det J(\\hat{\\boldsymbol{x}}) = \\varepsilon$ for all $\\hat{\\boldsymbol{x}} \\in \\hat{K}$, and therefore $\\min_{\\hat{K}} \\det J = \\varepsilon$. Although this mapping is affine, interpret it as a limiting case of a curved element that is uniformly compressed to achieve the same minimum Jacobian determinant.\n\nStarting from the standard definitions of the local mass matrix $M(\\varepsilon)$ and stiffness matrix $K(\\varepsilon)$ under an isoparametric mapping,\n$$\nM_{ij}(\\varepsilon) = \\int_{K_\\varepsilon} N_i(\\boldsymbol{x}) N_j(\\boldsymbol{x}) \\, d\\boldsymbol{x} = \\int_{\\hat{K}} \\hat{N}_i(\\hat{\\boldsymbol{x}}) \\hat{N}_j(\\hat{\\boldsymbol{x}})\\, \\det J(\\hat{\\boldsymbol{x}}) \\, d\\hat{\\boldsymbol{x}},\n$$\n$$\nK_{ij}(\\varepsilon) = \\int_{K_\\varepsilon} \\nabla N_i(\\boldsymbol{x}) \\cdot \\nabla N_j(\\boldsymbol{x}) \\, d\\boldsymbol{x} = \\int_{\\hat{K}} \\hat{\\nabla} \\hat{N}_i(\\hat{\\boldsymbol{x}})^{\\top} \\Big(J(\\hat{\\boldsymbol{x}})^{-1} J(\\hat{\\boldsymbol{x}})^{-T}\\Big) \\hat{\\nabla} \\hat{N}_j(\\hat{\\boldsymbol{x}}) \\, \\det J(\\hat{\\boldsymbol{x}})\\, d\\hat{\\boldsymbol{x}},\n$$\nderive from first principles how the spectra of $M(\\varepsilon)$ and $K(\\varepsilon)$ scale with $\\varepsilon$. In particular, use the gradient transformation $\\nabla N = J^{-T} \\hat{\\nabla} \\hat{N}$ and the change-of-variables formula to show the asymptotic behaviors $M(\\varepsilon) = \\mathcal{O}(\\varepsilon)$ and $K(\\varepsilon) = \\mathcal{O}(1)$ as $\\varepsilon \\to 0^+$.\n\nThen, consider the semi-discrete system for the diffusion equation with unit diffusivity,\n$$\nM(\\varepsilon)\\, \\frac{d u}{d t} + K(\\varepsilon)\\, u = 0,\n$$\nand explain the explicit Forward Euler (FE) time step constraint in terms of the spectral radius of $M(\\varepsilon)^{-1} K(\\varepsilon)$. Show how the largest stable time step $\\Delta t_{\\max}$ scales with $\\varepsilon$.\n\nYour program must:\n- Assemble $M(\\varepsilon)$ and $K(\\varepsilon)$ using Gaussian quadrature on $\\hat{K}$ with bilinear shape functions $\\{\\hat{N}_i\\}_{i=1}^4$ and the mapping $F_\\varepsilon(\\hat{\\boldsymbol{x}}) = s(\\varepsilon) \\hat{\\boldsymbol{x}}$.\n- Compute, for each test case, the following three quantities:\n  1. The ratio $\\lambda_{\\max}\\big(M(\\varepsilon)\\big)/\\varepsilon$, where $\\lambda_{\\max}$ denotes the largest eigenvalue.\n  2. The largest eigenvalue $\\lambda_{\\max}\\big(K(\\varepsilon)\\big)$.\n  3. The largest stable Forward Euler time step $\\Delta t_{\\max} = \\frac{2}{\\rho\\big(M(\\varepsilon)^{-1} K(\\varepsilon)\\big)}$, where $\\rho(\\cdot)$ denotes the spectral radius.\n- Use double-precision floating-point arithmetic and report all quantities as dimensionless floats.\n\nTest suite:\n- Evaluate the program for the parameter values $\\varepsilon \\in \\{10^{0}, 10^{-2}, 10^{-6}, 10^{-10}\\}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list $[\\lambda_{\\max}(M(\\varepsilon))/\\varepsilon,\\ \\lambda_{\\max}(K(\\varepsilon)),\\ \\Delta t_{\\max}]$ for the corresponding test case. For example, an output with two cases should look like $[[a_1,b_1,c_1],[a_2,b_2,c_2]]$, but with actual numeric values.",
            "solution": "The problem requires a theoretical analysis and numerical verification of the scaling properties of the local mass and stiffness matrices, and the corresponding Forward Euler time step stability limit, for a finite element under a specific parametric mapping.\n\n### **Problem Validation**\n\n**Step 1: Extracted Givens**\n-   **Governing Equation:** Scalar diffusion equation, semi-discrete form: $M(\\varepsilon)\\, \\frac{d u}{d t} + K(\\varepsilon)\\, u = 0$ with unit diffusivity.\n-   **Reference Element:** A square $\\hat{K} = [-1,1]^2$.\n-   **Shape Functions:** Bilinear shape functions on $\\hat{K}$, denoted $\\{\\hat{N}_i\\}_{i=1}^4$.\n-   **Isoparametric Mapping:** $F_\\varepsilon : \\hat{K} \\to K_\\varepsilon$ defined by $\\boldsymbol{x} = F_\\varepsilon(\\hat{\\boldsymbol{x}}) = s(\\varepsilon) \\hat{\\boldsymbol{x}}$, with $s(\\varepsilon) = \\sqrt{\\varepsilon}$ for $\\varepsilon \\in (0,1]$.\n-   **Jacobian Matrix:** $J(\\hat{\\boldsymbol{x}}) = \\frac{\\partial \\boldsymbol{x}}{\\partial \\hat{\\boldsymbol{x}}} = s(\\varepsilon) I = \\sqrt{\\varepsilon} I$, where $I$ is the $2 \\times 2$ identity matrix.\n-   **Jacobian Determinant:** $\\det J(\\hat{\\boldsymbol{x}}) = (s(\\varepsilon))^2 = \\varepsilon$.\n-   **Mass Matrix Definition:** $M_{ij}(\\varepsilon) = \\int_{K_\\varepsilon} N_i(\\boldsymbol{x}) N_j(\\boldsymbol{x}) \\, d\\boldsymbol{x} = \\int_{\\hat{K}} \\hat{N}_i(\\hat{\\boldsymbol{x}}) \\hat{N}_j(\\hat{\\boldsymbol{x}})\\, \\det J(\\hat{\\boldsymbol{x}}) \\, d\\hat{\\boldsymbol{x}}$.\n-   **Stiffness Matrix Definition:** $K_{ij}(\\varepsilon) = \\int_{K_\\varepsilon} \\nabla N_i(\\boldsymbol{x}) \\cdot \\nabla N_j(\\boldsymbol{x}) \\, d\\boldsymbol{x} = \\int_{\\hat{K}} \\hat{\\nabla} \\hat{N}_i(\\hat{\\boldsymbol{x}})^{\\top} \\Big(J(\\hat{\\boldsymbol{x}})^{-1} J(\\hat{\\boldsymbol{x}})^{-T}\\Big) \\hat{\\nabla} \\hat{N}_j(\\hat{\\boldsymbol{x}}) \\, \\det J(\\hat{\\boldsymbol{x}})\\, d\\hat{\\boldsymbol{x}}$.\n-   **Numerical Task:** For $\\varepsilon \\in \\{10^{0}, 10^{-2}, 10^{-6}, 10^{-10}\\}$, compute $\\lambda_{\\max}\\big(M(\\varepsilon)\\big)/\\varepsilon$, $\\lambda_{\\max}\\big(K(\\varepsilon)\\big)$, and $\\Delta t_{\\max} = \\frac{2}{\\rho\\big(M(\\varepsilon)^{-1} K(\\varepsilon)\\big)}$.\n\n**Step 2: Validation**\nThe problem is well-defined and scientifically sound. It is a standard exercise in the numerical analysis of partial differential equations, specifically within the finite element method. The provided definitions for the mass and stiffness matrices under an isoparametric mapping are correct. The chosen mapping is a simple affine transformation, which is a valid and common case to study. The stability analysis of the Forward Euler method for the resulting system of ordinary differential equations (ODEs) is a fundamental topic. The problem is self-contained, provides all necessary data, and asks for verifiable, objective quantities. The interpretation of the problem in a non-dimensional framework makes all requested outputs dimensionless and consistent.\n\n**Step 3: Verdict**\nThe problem is valid. We proceed with the solution.\n\n---\n\n### **Theoretical Derivation**\n\nThe core of the problem is to understand how the geometry of a finite element, parameterized by $\\varepsilon$, affects the algebraic properties of the discrete system.\n\n**1. Scaling of the Mass Matrix $M(\\varepsilon)$**\n\nThe local mass matrix $M(\\varepsilon)$ is defined by the integral over the reference element $\\hat{K}$:\n$$\nM_{ij}(\\varepsilon) = \\int_{\\hat{K}} \\hat{N}_i(\\hat{\\boldsymbol{x}}) \\hat{N}_j(\\hat{\\boldsymbol{x}})\\, \\det J(\\hat{\\boldsymbol{x}}) \\, d\\hat{\\boldsymbol{x}}\n$$\nFrom the problem statement, the Jacobian determinant is constant, $\\det J(\\hat{\\boldsymbol{x}}) = \\varepsilon$. We can factor this constant out of the integral:\n$$\nM_{ij}(\\varepsilon) = \\varepsilon \\int_{\\hat{K}} \\hat{N}_i(\\hat{\\boldsymbol{x}}) \\hat{N}_j(\\hat{\\boldsymbol{x}}) \\, d\\hat{\\boldsymbol{x}}\n$$\nThe remaining integral is independent of $\\varepsilon$. It defines the reference mass matrix, $\\hat{M}$:\n$$\n\\hat{M}_{ij} = \\int_{\\hat{K}} \\hat{N}_i(\\hat{\\boldsymbol{x}}) \\hat{N}_j(\\hat{\\boldsymbol{x}}) \\, d\\hat{\\boldsymbol{x}}\n$$\nThus, the relationship between the mass matrix on the element $K_\\varepsilon$ and the reference mass matrix is a simple scaling:\n$$\nM(\\varepsilon) = \\varepsilon \\hat{M}\n$$\nThe eigenvalues of $M(\\varepsilon)$ are directly related to the eigenvalues of $\\hat{M}$. Let $\\lambda_{\\hat{M}}$ be an eigenvalue of $\\hat{M}$. Then $M(\\varepsilon) v = (\\varepsilon \\hat{M}) v = \\varepsilon (\\lambda_{\\hat{M}} v) = (\\varepsilon \\lambda_{\\hat{M}}) v$. This shows that the eigenvalues of $M(\\varepsilon)$ are $\\varepsilon$ times the eigenvalues of $\\hat{M}$. Consequently, the spectrum of $M(\\varepsilon)$ scales linearly with $\\varepsilon$, and we can write $M(\\varepsilon) = \\mathcal{O}(\\varepsilon)$ as $\\varepsilon \\to 0^+$. The ratio $\\lambda_{\\max}(M(\\varepsilon)) / \\varepsilon$ is equal to the constant $\\lambda_{\\max}(\\hat{M})$.\n\n**2. Scaling of the Stiffness Matrix $K(\\varepsilon)$**\n\nThe local stiffness matrix $K(\\varepsilon)$ is given by:\n$$\nK_{ij}(\\varepsilon) = \\int_{\\hat{K}} \\hat{\\nabla} \\hat{N}_i(\\hat{\\boldsymbol{x}})^{\\top} \\Big(J(\\hat{\\boldsymbol{x}})^{-1} J(\\hat{\\boldsymbol{x}})^{-T}\\Big) \\hat{\\nabla} \\hat{N}_j(\\hat{\\boldsymbol{x}}) \\, \\det J(\\hat{\\boldsymbol{x}})\\, d\\hat{\\boldsymbol{x}}\n$$\nWe need to evaluate the geometric term $G(\\hat{\\boldsymbol{x}}) = J(\\hat{\\boldsymbol{x}})^{-1} J(\\hat{\\boldsymbol{x}})^{-T}$. The Jacobian matrix is $J(\\hat{\\boldsymbol{x}}) = \\sqrt{\\varepsilon}I$. Its inverse is $J(\\hat{\\boldsymbol{x}})^{-1} = \\frac{1}{\\sqrt{\\varepsilon}}I$, which is also its own transpose, $J(\\hat{\\boldsymbol{x}})^{-T} = \\frac{1}{\\sqrt{\\varepsilon}}I$.\nTherefore,\n$$\nG(\\hat{\\boldsymbol{x}}) = \\left(\\frac{1}{\\sqrt{\\varepsilon}}I\\right) \\left(\\frac{1}{\\sqrt{\\varepsilon}}I\\right) = \\frac{1}{\\varepsilon}I\n$$\nSubstituting this and $\\det J(\\hat{\\boldsymbol{x}}) = \\varepsilon$ into the integral for $K_{ij}(\\varepsilon)$:\n$$\nK_{ij}(\\varepsilon) = \\int_{\\hat{K}} \\hat{\\nabla} \\hat{N}_i(\\hat{\\boldsymbol{x}})^{\\top} \\left(\\frac{1}{\\varepsilon}I\\right) \\hat{\\nabla} \\hat{N}_j(\\hat{\\boldsymbol{x}}) \\, (\\varepsilon) \\, d\\hat{\\boldsymbol{x}}\n$$\nThe scalar factors $\\frac{1}{\\varepsilon}$ and $\\varepsilon$ cancel each other out:\n$$\nK_{ij}(\\varepsilon) = \\int_{\\hat{K}} \\hat{\\nabla} \\hat{N}_i(\\hat{\\boldsymbol{x}})^{\\top} I \\hat{\\nabla} \\hat{N}_j(\\hat{\\boldsymbol{x}}) \\, d\\hat{\\boldsymbol{x}} = \\int_{\\hat{K}} \\hat{\\nabla} \\hat{N}_i(\\hat{\\boldsymbol{x}}) \\cdot \\hat{\\nabla} \\hat{N}_j(\\hat{\\boldsymbol{x}}) \\, d\\hat{\\boldsymbol{x}}\n$$\nThis expression is the definition of the reference stiffness matrix, $\\hat{K}$, which is independent of $\\varepsilon$.\n$$\nK(\\varepsilon) = \\hat{K}\n$$\nTherefore, the stiffness matrix $K(\\varepsilon)$ and its spectrum do not change with $\\varepsilon$. This confirms the asymptotic behavior $K(\\varepsilon) = \\mathcal{O}(1)$ as $\\varepsilon \\to 0^+$. The quantity $\\lambda_{\\max}(K(\\varepsilon))$ is equal to the constant $\\lambda_{\\max}(\\hat{K})$.\n\n**3. Scaling of the Maximum Stable Time Step $\\Delta t_{\\max}$**\n\nThe semi-discrete system is given by $M(\\varepsilon)\\, \\frac{d u}{d t} + K(\\varepsilon)\\, u = 0$. Applying the Forward Euler time integration scheme, $u^{n+1} = u^n + \\Delta t \\frac{du}{dt}\\big|_{t=t_n}$, gives:\n$$\nM(\\varepsilon) \\left(\\frac{u^{n+1} - u^n}{\\Delta t}\\right) + K(\\varepsilon) u^n = 0\n$$\nSolving for the new state $u^{n+1}$:\n$$\nu^{n+1} = \\left(I - \\Delta t M(\\varepsilon)^{-1} K(\\varepsilon)\\right) u^n\n$$\nFor stability, the spectral radius of the amplification matrix $A = I - \\Delta t M(\\varepsilon)^{-1} K(\\varepsilon)$ must be no greater than $1$. The eigenvalues of $A$ are $1 - \\Delta t \\lambda_G$, where $\\lambda_G$ are the eigenvalues of the matrix $M(\\varepsilon)^{-1} K(\\varepsilon)$, which are the solutions to the generalized eigenvalue problem $K(\\varepsilon)v = \\lambda_G M(\\varepsilon)v$. Since $M$ and $K$ are symmetric and positive (semi-)definite, the eigenvalues $\\lambda_G$ are real and non-negative.\n\nThe stability condition $|1 - \\Delta t \\lambda_G| \\le 1$ for all $\\lambda_G$ implies $-1 \\le 1 - \\Delta t \\lambda_G \\le 1$, which simplifies to $\\Delta t \\lambda_G \\le 2$. This must hold for the largest eigenvalue, so the constraint is:\n$$\n\\Delta t \\le \\frac{2}{\\max(\\lambda_G)} = \\frac{2}{\\rho(M(\\varepsilon)^{-1} K(\\varepsilon))}\n$$\nThe maximum stable time step is thus $\\Delta t_{\\max} = \\frac{2}{\\rho(M(\\varepsilon)^{-1} K(\\varepsilon))}$.\n\nNow we analyze its scaling with $\\varepsilon$. Using our previous results, $M(\\varepsilon) = \\varepsilon \\hat{M}$ and $K(\\varepsilon) = \\hat{K}$. The generalized eigenvalue problem becomes:\n$$\n\\hat{K} v = \\lambda_G (\\varepsilon \\hat{M}) v\n$$\nRearranging gives:\n$$\n\\frac{1}{\\varepsilon} \\hat{K} v = \\lambda_G \\hat{M} v\n$$\nThis shows that the eigenvalues $\\lambda_G$ of $M(\\varepsilon)^{-1}K(\\varepsilon)$ are equal to $\\frac{1}{\\varepsilon}$ times the eigenvalues of the reference system $\\hat{M}^{-1}\\hat{K}$. The spectral radius scales accordingly:\n$$\n\\rho(M(\\varepsilon)^{-1} K(\\varepsilon)) = \\frac{1}{\\varepsilon} \\rho(\\hat{M}^{-1} \\hat{K})\n$$\nSubstituting this into the expression for $\\Delta t_{\\max}$:\n$$\n\\Delta t_{\\max}(\\varepsilon) = \\frac{2}{\\frac{1}{\\varepsilon} \\rho(\\hat{M}^{-1} \\hat{K})} = \\varepsilon \\left(\\frac{2}{\\rho(\\hat{M}^{-1} \\hat{K})}\\right)\n$$\nSince the term in parentheses is a constant, we have shown that $\\Delta t_{\\max}(\\varepsilon) = \\mathcal{O}(\\varepsilon)$. This critical result indicates that as the element becomes smaller (as $\\varepsilon \\to 0$), the stability constraint on the explicit time step becomes proportionally more severe.\n\n### **Numerical Implementation**\n\nThe program will implement the following steps:\n1.  Define the bilinear shape functions $\\{\\hat{N}_i(\\xi, \\eta)\\}_{i=1}^4$ and their gradients $\\{\\hat{\\nabla} \\hat{N}_i(\\xi, \\eta)\\}_{i=1}^4$ on the reference square $\\hat{K} = [-1,1]^2$.\n2.  For each given $\\varepsilon$, assemble the $4 \\times 4$ matrices $M(\\varepsilon)$ and $K(\\varepsilon)$ using a $2 \\times 2$ Gaussian quadrature rule, which is exact for the polynomial integrands encountered.\n3.  The integrands are evaluated using the derived formulas: the integrand for $M_{ij}$ is $\\varepsilon (\\hat{N}_i \\hat{N}_j)$ and the integrand for $K_{ij}$ is $(\\hat{\\nabla} \\hat{N}_i \\cdot \\hat{\\nabla} \\hat{N}_j)$.\n4.  For each assembled pair $(M(\\varepsilon), K(\\varepsilon))$, compute the required quantities:\n    a. The largest eigenvalue of $M(\\varepsilon)$, divided by $\\varepsilon$. This should be a constant equal to $\\lambda_{\\max}(\\hat{M})$.\n    b. The largest eigenvalue of $K(\\varepsilon)$. This should be a constant equal to $\\lambda_{\\max}(\\hat{K})$.\n    c. The largest stable time step, $\\Delta t_{\\max}$, by first solving the generalized eigenvalue problem $K(\\varepsilon)v = \\lambda_G M(\\varepsilon)v$ to find $\\rho(M(\\varepsilon)^{-1} K(\\varepsilon))$ and then applying the formula.\n5.  The results for each $\\varepsilon$ are collected and formatted into the specified string output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the FEM scaling problem for various values of epsilon.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [1.0, 1e-2, 1e-6, 1e-10]\n\n    def get_bilinear_basis(xi, eta):\n        \"\"\"\n        Evaluates bilinear shape functions and their gradients at a point (xi, eta)\n        in the reference element [-1, 1]^2.\n\n        Node ordering:\n        1: (-1, -1)\n        2: ( 1, -1)\n        3: ( 1,  1)\n        4: (-1,  1)\n\n        Returns:\n            - N: A (4,) array of shape function values.\n            - dN: A (4, 2) array of shape function gradients [d/dxi, d/deta].\n        \"\"\"\n        N = 0.25 * np.array([\n            (1 - xi) * (1 - eta),\n            (1 + xi) * (1 - eta),\n            (1 + xi) * (1 + eta),\n            (1 - xi) * (1 + eta)\n        ])\n\n        dN = 0.25 * np.array([\n            [-(1 - eta), -(1 - xi)],\n            [ (1 - eta), -(1 + xi)],\n            [ (1 + eta),  (1 + xi)],\n            [-(1 + eta),  (1 - xi)]\n        ])\n        return N, dN\n\n    def assemble_matrices(epsilon):\n        \"\"\"\n        Assembles the mass matrix M(epsilon) and stiffness matrix K(epsilon)\n        for a given epsilon using 2x2 Gaussian quadrature.\n        \"\"\"\n        M_eps = np.zeros((4, 4), dtype=np.float64)\n        K_eps = np.zeros((4, 4), dtype=np.float64)\n\n        # 2-point Gauss-Legendre quadrature points and weights\n        quad_points = [-1.0 / np.sqrt(3.0), 1.0 / np.sqrt(3.0)]\n        quad_weights = [1.0, 1.0]\n\n        # Get Jacobian related terms (constant over the element)\n        det_J = epsilon\n        # J_inv_J_invT is (J^-1)(J^-T). For J = sqrt(eps)*I, this is (1/eps)*I.\n        # The geometric factor in the stiffness integral is (J^-1)(J^-T) * det(J)\n        # which is ((1/eps)*I) * eps = I.\n        # This means K(eps) is independent of epsilon.\n\n        for i, weight_i in enumerate(quad_weights):\n            for j, weight_j in enumerate(quad_weights):\n                xi = quad_points[i]\n                eta = quad_points[j]\n                weight = weight_i * weight_j\n\n                N_vals, dN_vals = get_bilinear_basis(xi, eta)\n\n                # Assemble mass matrix M(epsilon)\n                # Integrand is N_i * N_j * det(J)\n                M_integrand = np.outer(N_vals, N_vals)\n                M_eps += M_integrand * det_J * weight\n\n                # Assemble stiffness matrix K(epsilon)\n                # Integrand is (grad_N_i^T * J_inv_J_invT * grad_N_j) * det(J)\n                # which simplifies to (grad_N_i . grad_N_j)\n                K_integrand = dN_vals @ dN_vals.T\n                K_eps += K_integrand * weight\n        \n        return M_eps, K_eps\n\n    results = []\n    for epsilon in test_cases:\n        M, K = assemble_matrices(epsilon)\n\n        # 1. Compute lambda_max(M(epsilon)) / epsilon\n        # Use eigvalsh for symmetric matrices\n        lambda_max_M = linalg.eigvalsh(M).max()\n        ratio_1 = lambda_max_M / epsilon\n\n        # 2. Compute lambda_max(K(epsilon))\n        lambda_max_K = linalg.eigvalsh(K).max()\n\n        # 3. Compute largest stable Forward Euler time step\n        # dt_max = 2 / rho(M^-1 * K)\n        # We solve the generalized eigenvalue problem K*v = lambda*M*v\n        # This is more stable than inverting M.\n        # The eigenvalues are real since K, M are symmetric and M is pos-def.\n        gen_eigenvalues = linalg.eigh(K, M, eigvals_only=True)\n        spectral_radius = np.max(gen_eigenvalues)\n        \n        # Handle the case where the spectral radius is zero (e.g., if K is all zeros)\n        if np.isclose(spectral_radius, 0):\n            dt_max = np.inf\n        else:\n            dt_max = 2.0 / spectral_radius\n        \n        results.append([ratio_1, lambda_max_K, dt_max])\n\n    # Format the final output string exactly as required.\n    formatted_results = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After transforming integrals from a physical element to the reference element, we must evaluate them, typically using numerical quadrature. The accuracy of the entire finite element solution hinges on choosing a quadrature rule that is sufficient for the integrand. This practice will guide you through the process of determining the minimal Gauss quadrature order required to integrate the element stiffness matrix exactly for a high-order ($Q_3$) element, a core skill for implementing and verifying finite element codes.",
            "id": "3598675",
            "problem": "Consider the three-dimensional small-strain linear elasticity formulation within the Finite Element Method (FEM). The element stiffness matrix entries are given by the domain integral of the product of the transpose of the strain–displacement matrix, the constitutive matrix, and the strain–displacement matrix, namely $K_{ab} = \\int_{\\Omega_e} B_a^{T} D B_b \\,\\mathrm{d}\\Omega$, where $B$ is the strain–displacement matrix constructed from spatial derivatives of the shape functions and $D$ is the constant constitutive matrix for a homogeneous isotropic material. Assume an isoparametric $Q_3$ hexahedral element (tensor-product Lagrange polynomials of degree $p=3$ in each reference coordinate) with an affine mapping from the reference element to the physical element, so that the Jacobian matrix and its determinant are constant within the element.\n\nUsing only the following base facts:\n- $Q_p$ tensor-product Lagrange shape functions on the reference hexahedron are degree $p$ polynomials in each reference coordinate, and their derivatives with respect to a given reference coordinate reduce the polynomial degree in that coordinate by $1$ while leaving the degrees in the other coordinates unchanged.\n- For an affine mapping, the spatial derivatives entering $B$ are linear combinations of reference-derivatives with constant coefficients, so the per-coordinate polynomial degrees of the derivatives are preserved up to those reductions.\n- A one-dimensional Gauss–Legendre quadrature with $n$ points integrates exactly any polynomial of degree up to $2n-1$.\n\nDetermine the smallest equal-per-direction tensor-product Gauss–Legendre quadrature rule, specified by the number of points $n$ per reference coordinate $(\\xi, \\eta, \\zeta)$, that integrates $B^T D B$ exactly over the element. In your derivation, identify the maximal polynomial degree that appears in each reference coordinate within the integrand $B^T D B$ for the $Q_3$ case, and justify your choice by applying the $2n-1$ exactness property per direction. Express your final answer as a row vector giving the number of Gauss points per direction in the order $(\\xi,\\eta,\\zeta)$. No rounding is required, and no units should be included in the final answer.",
            "solution": "The problem requires the determination of the minimum number of Gauss-Legendre quadrature points, $n$, required per reference coordinate direction for the exact integration of the element stiffness matrix for a $Q_3$ hexahedral element under an affine mapping.\n\nThe entries of the element stiffness matrix are given by the integral over the physical element domain $\\Omega_e$:\n$$K_{ab} = \\int_{\\Omega_e} B_a^{T} D B_b \\,\\mathrm{d}\\Omega$$\nwhere $B$ is the strain-displacement matrix and $D$ is the constitutive matrix. For numerical integration, this integral is transformed to the reference element domain $\\hat{\\Omega} = [-1, 1]^3$, which is spanned by the coordinates $(\\xi, \\eta, \\zeta)$. The transformation involves the Jacobian matrix $J$ of the mapping from reference to physical coordinates.\n$$K_{ab} = \\int_{\\hat{\\Omega}} (B_a^T D B_b) \\det(J) \\,\\mathrm{d}\\xi \\,\\mathrm{d}\\eta \\,\\mathrm{d}\\zeta$$\nThe problem states that the mapping is affine. A key consequence of an affine mapping is that the Jacobian matrix $J$ and its determinant $\\det(J)$ are constant throughout the element. Furthermore, the constitutive matrix $D$ is given as constant for a homogeneous material. Therefore, the task reduces to determining the polynomial nature of the integrand term $B_a^T D B_b$, which simplifies to analyzing the polynomial degree of products of entries from the $B$ matrix.\n\nThe strain-displacement matrix $B$ is constructed from the spatial derivatives of the shape functions $N_i$. For a 3D elasticity problem, the entries of $B$ are of the form $\\frac{\\partial N_i}{\\partial x}$, $\\frac{\\partial N_i}{\\partial y}$, and $\\frac{\\partial N_i}{\\partial z}$.\n\nThe shape functions $N_i(\\xi, \\eta, \\zeta)$ for an isoparametric $Q_p$ element are tensor products of one-dimensional Lagrange polynomials of degree $p$. For the given $Q_3$ element, we have $p=3$. This means that any shape function $N_i$ is a polynomial with a maximum degree of $p=3$ in each of the reference coordinates $\\xi$, $\\eta$, and $\\zeta$.\n\nTo find the polynomial degree of the spatial derivatives, we use the chain rule of differentiation. For a generic spatial coordinate $x_j$ (where $j=1,2,3$ corresponds to $x,y,z$) and a reference coordinate $\\xi_k$ (where $k=1,2,3$ corresponds to $\\xi,\\eta,\\zeta$), the relationship is:\n$$\\frac{\\partial N_i}{\\partial x_j} = \\sum_{k=1}^{3} \\frac{\\partial N_i}{\\partial \\xi_k} \\frac{\\partial \\xi_k}{\\partial x_j}$$\nThe terms $\\frac{\\partial \\xi_k}{\\partial x_j}$ are the entries of the inverse Jacobian matrix, $J^{-1}$. Since the mapping is affine, $J$ is constant, and thus $J^{-1}$ is also a constant matrix. Let the entries of $J^{-1}$ be denoted by $(J^{-1})_{kj}$. Then, the spatial derivatives are constant linear combinations of the reference derivatives.\n\nWe must now determine the polynomial degree of the reference derivatives. Given that $N_i$ is a polynomial of degree at most $p=3$ in each of $\\xi$, $\\eta$, and $\\zeta$:\n- Differentiating with respect to $\\xi$ reduces the polynomial degree in $\\xi$ by $1$ to $p-1=2$, while the degrees in $\\eta$ and $\\zeta$ remain $p=3$. So, $\\frac{\\partial N_i}{\\partial \\xi}$ is a polynomial of degree $(2, 3, 3)$ in $(\\xi, \\eta, \\zeta)$.\n- Similarly, $\\frac{\\partial N_i}{\\partial \\eta}$ is a polynomial of degree $(3, 2, 3)$ in $(\\xi, \\eta, \\zeta)$.\n- And $\\frac{\\partial N_i}{\\partial \\zeta}$ is a polynomial of degree $(3, 3, 2)$ in $(\\xi, \\eta, \\zeta)$.\n\nNow we analyze the degree of a spatial derivative, for instance $\\frac{\\partial N_i}{\\partial x}$, which is a linear combination of the three reference derivatives: $\\frac{\\partial N_i}{\\partial x} = (J^{-1})_{11}\\frac{\\partial N_i}{\\partial \\xi} + (J^{-1})_{12}\\frac{\\partial N_i}{\\partial \\eta} + (J^{-1})_{13}\\frac{\\partial N_i}{\\partial \\zeta}$. The polynomial degree of this sum in a given coordinate is the maximum of the degrees of its terms (assuming a general non-degenerate mapping where the coefficients are non-zero).\n- The degree in $\\xi$ of $\\frac{\\partial N_i}{\\partial x}$ is $\\max(\\text{deg}_\\xi(\\frac{\\partial N_i}{\\partial \\xi}), \\text{deg}_\\xi(\\frac{\\partial N_i}{\\partial \\eta}), \\text{deg}_\\xi(\\frac{\\partial N_i}{\\partial \\zeta})) = \\max(p-1, p, p) = p$.\n- The degree in $\\eta$ of $\\frac{\\partial N_i}{\\partial x}$ is $\\max(\\text{deg}_\\eta(\\frac{\\partial N_i}{\\partial \\xi}), \\text{deg}_\\eta(\\frac{\\partial N_i}{\\partial \\eta}), \\text{deg}_\\eta(\\frac{\\partial N_i}{\\partial \\zeta})) = \\max(p, p-1, p) = p$.\n- The degree in $\\zeta$ of $\\frac{\\partial N_i}{\\partial x}$ is $\\max(\\text{deg}_\\zeta(\\frac{\\partial N_i}{\\partial \\xi}), \\text{deg}_\\zeta(\\frac{\\partial N_i}{\\partial \\eta}), \\text{deg}_\\zeta(\\frac{\\partial N_i}{\\partial \\zeta})) = \\max(p, p, p-1) = p$.\n\nThus, for $p=3$, any entry of the strain-displacement matrix $B$ is a polynomial of degree at most $3$ in each of the reference variables $\\xi$, $\\eta$, and $\\zeta$.\n\nThe integrand under consideration, $B^T D B \\det(J)$, consists of terms that are products of two entries from the $B$ matrix, scaled by constants from $D$ and $\\det(J)$. When two polynomials are multiplied, their degrees add. Therefore, the maximal degree of any term in the integrand in a specific coordinate, say $\\xi$, is the sum of the maximal degrees in $\\xi$ of the two multiplying entries from $B$.\nMaximal degree in $\\xi = p + p = 2p$.\nMaximal degree in $\\eta = p + p = 2p$.\nMaximal degree in $\\zeta = p + p = 2p$.\n\nFor the $Q_3$ element, $p=3$. The maximal polynomial degree of the integrand in each direction is $2p = 2 \\times 3 = 6$.\n\nThe problem specifies the use of a tensor-product Gauss-Legendre quadrature rule. A one-dimensional Gauss-Legendre rule with $n$ points can integrate any polynomial of degree up to $2n-1$ exactly. To ensure exact integration of the stiffness matrix, we must choose $n$ such that this condition is met for the highest polynomial degree present in the integrand along each integration direction.\nWe require:\n$$2n - 1 \\ge 2p$$\nSubstituting $p=3$:\n$$2n - 1 \\ge 6$$\n$$2n \\ge 7$$\n$$n \\ge 3.5$$\nSince the number of quadrature points $n$ must be an integer, the smallest integer value for $n$ that satisfies this condition is $n=4$.\nThis requirement applies to each reference coordinate direction $(\\xi, \\eta, \\zeta)$ because the polynomial degree structure is the same for each. Thus, the smallest equal-per-direction quadrature rule that ensures exact integration is a $4 \\times 4 \\times 4$ rule.\n\nThe number of Gauss points required per direction is $4$. The final answer, expressed as a row vector for the $(\\xi,\\eta,\\zeta)$ directions, is therefore $(4, 4, 4)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 4  4  4 \\end{pmatrix}}$$"
        },
        {
            "introduction": "The elegance of the finite element method relies on its rigorous mathematical foundation, where properties like symmetry of the bilinear form translate to efficient and robust numerical solvers. This exercise serves as a powerful cautionary example, demonstrating how seemingly innocuous approximations within the coordinate transformation can violate these fundamental properties. By calculating the \"symmetry defect\" arising from an inconsistent Jacobian approximation, you will gain a deeper appreciation for the importance of preserving the mathematical structure of the weak form during discretization .",
            "id": "3439550",
            "problem": "Consider a single isoparametric element with reference domain $\\hat{K} = [0,1]^{2}$ and a $C^{1}$ isoparametric map $F_{K}:\\hat{K} \\to K$ with Jacobian matrix $J(\\hat{\\boldsymbol{x}})$, where $\\hat{\\boldsymbol{x}} = (\\xi,\\eta)$. For a scalar second-order elliptic partial differential equation (PDE), the element bilinear form is defined by $a_{K}(u,v) = \\int_{K} \\nabla_{\\boldsymbol{x}} u \\cdot \\nabla_{\\boldsymbol{x}} v \\, \\mathrm{d}\\boldsymbol{x}$ for sufficiently smooth functions $u$ and $v$ on $K$. Starting only from the chain rule and the change-of-variables formula, derive the expression of the pullback bilinear form on the reference element $\\hat{K}$ in terms of $J(\\hat{\\boldsymbol{x}})$ and gradients with respect to the reference coordinates.\n\nNext, define the specific isoparametric map $F_{K}(\\xi,\\eta) = \\big(\\xi + \\gamma\\,\\xi\\eta,\\;\\eta\\big)$ with parameter $\\gamma \\in \\mathbb{R}$ satisfying $1+\\gamma\\eta0$ for all $\\eta\\in[0,1]$ (to ensure orientation preservation). Let $\\hat{J} = J(\\xi_{0},\\eta_{0})$ be a constant approximate Jacobian formed by freezing $J$ at the element center $(\\xi_{0},\\eta_{0}) = \\left(\\tfrac{1}{2},\\tfrac{1}{2}\\right)$. Consider the inconsistent approximate pullback in which the metric tensor is formed by mixing the approximate inverse on the “trial” side with the exact inverse on the “test” side, namely\n$$\nG_{\\mathrm{appr}}(\\hat{\\boldsymbol{x}}) = \\det\\big(J(\\hat{\\boldsymbol{x}})\\big)\\,\\hat{J}^{-1}\\,J(\\hat{\\boldsymbol{x}})^{-T},\n$$\nand the resulting approximate bilinear form\n$$\na_{\\mathrm{appr}}(u,v) = \\int_{\\hat{K}} \\nabla_{\\hat{\\boldsymbol{x}}} u(\\hat{\\boldsymbol{x}})^{\\!T}\\,G_{\\mathrm{appr}}(\\hat{\\boldsymbol{x}})\\,\\nabla_{\\hat{\\boldsymbol{x}}} v(\\hat{\\boldsymbol{x}})\\,\\mathrm{d}\\hat{\\boldsymbol{x}}.\n$$\nUsing the test functions $u(\\xi,\\eta) = \\xi$ and $v(\\xi,\\eta) = \\eta^{2}$, compute the analytic value of the symmetry defect\n$$\na_{\\mathrm{appr}}(u,v)\\;-\\;a_{\\mathrm{appr}}(v,u)\n$$\nas a closed-form expression in terms of $\\gamma$. No numerical rounding is required; provide the exact expression as your final answer.",
            "solution": "The user has provided a two-part problem. The first part requires the derivation of the pullback of a standard elliptic bilinear form onto a reference element. The second part requires the computation of a \"symmetry defect\" for a specifically defined approximate bilinear form. We shall address both parts in sequence.\n\nFirst, the validation of the problem statement is required.\n\n### Step 1: Extract Givens\n- Reference domain: $\\hat{K} = [0,1]^{2}$.\n- Isoparametric map: $F_{K}:\\hat{K} \\to K$, a $C^{1}$ function mapping reference coordinates $\\hat{\\boldsymbol{x}} = (\\xi,\\eta)$ to physical coordinates $\\boldsymbol{x}$.\n- Jacobian matrix of the map: $J(\\hat{\\boldsymbol{x}})$.\n- Element bilinear form on the physical element $K$: $a_{K}(u,v) = \\int_{K} \\nabla_{\\boldsymbol{x}} u \\cdot \\nabla_{\\boldsymbol{x}} v \\, \\mathrm{d}\\boldsymbol{x}$.\n- Specific isoparametric map: $F_{K}(\\xi,\\eta) = \\big(\\xi + \\gamma\\,\\xi\\eta,\\;\\eta\\big)$.\n- Parameter constraint: $\\gamma \\in \\mathbb{R}$ such that $1+\\gamma\\eta0$ for all $\\eta\\in[0,1]$.\n- Constant approximate Jacobian: $\\hat{J} = J(\\xi_{0},\\eta_{0})$ evaluated at the element center $(\\xi_{0},\\eta_{0}) = \\left(\\frac{1}{2},\\frac{1}{2}\\right)$.\n- Approximate metric tensor: $G_{\\mathrm{appr}}(\\hat{\\boldsymbol{x}}) = \\det\\big(J(\\hat{\\boldsymbol{x}})\\big)\\,\\hat{J}^{-1}\\,J(\\hat{\\boldsymbol{x}})^{-T}$.\n- Approximate bilinear form: $a_{\\mathrm{appr}}(u,v) = \\int_{\\hat{K}} \\nabla_{\\hat{\\boldsymbol{x}}} u(\\hat{\\boldsymbol{x}})^{\\!T}\\,G_{\\mathrm{appr}}(\\hat{\\boldsymbol{x}})\\,\\nabla_{\\hat{\\boldsymbol{x}}} v(\\hat{\\boldsymbol{x}})\\,\\mathrm{d}\\hat{\\boldsymbol{x}}$.\n- Test functions: $u(\\xi,\\eta) = \\xi$ and $v(\\xi,\\eta) = \\eta^{2}$.\n- Quantity to compute: The symmetry defect $a_{\\mathrm{appr}}(u,v) - a_{\\mathrm{appr}}(v,u)$ as a closed-form expression in $\\gamma$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is situated within the standard mathematical framework of the finite element method (FEM) for solving partial differential equations. All concepts used—isoparametric mapping, Jacobians, change of variables, bilinear forms—are fundamental and rigorously defined in this field. The problem is scientifically grounded and objective. The first part is a standard textbook derivation. The second part involves a specific, well-defined calculation for a non-standard (or \"inconsistent\") bilinear form, which represents a valid theoretical exercise in numerical analysis to study the properties of approximation schemes. All provided information is self-contained and mathematically consistent. The constraint on $\\gamma$ ensures the map is non-degenerate. The calculation is feasible and leads to a unique, meaningful result. Therefore, the problem is well-posed.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\n**Part 1: Derivation of the Pullback Bilinear Form**\n\nLet $\\hat{u}(\\hat{\\boldsymbol{x}}) = u(F_K(\\hat{\\boldsymbol{x}}))$ and $\\hat{v}(\\hat{\\boldsymbol{x}}) = v(F_K(\\hat{\\boldsymbol{x}}))$ be the compositions of the functions $u$ and $v$ with the mapping $F_K$. The gradient of a function with respect to the physical coordinates $\\boldsymbol{x}$ is denoted $\\nabla_{\\boldsymbol{x}}$, and with respect to reference coordinates $\\hat{\\boldsymbol{x}}$ is denoted $\\nabla_{\\hat{\\boldsymbol{x}}}$. The chain rule relates these gradients:\n$$\n\\nabla_{\\hat{\\boldsymbol{x}}} \\hat{u} = J(\\hat{\\boldsymbol{x}})^T \\nabla_{\\boldsymbol{x}} u\n$$\nwhere $J(\\hat{\\boldsymbol{x}})$ is the Jacobian matrix of the transformation $F_K$. To express the physical gradient in terms of the reference gradient, we invert this relationship:\n$$\n\\nabla_{\\boldsymbol{x}} u = J(\\hat{\\boldsymbol{x}})^{-T} \\nabla_{\\hat{\\boldsymbol{x}}} \\hat{u}\n$$\nThe bilinear form on the physical element $K$ is given by $a_{K}(u,v) = \\int_{K} \\nabla_{\\boldsymbol{x}} u \\cdot \\nabla_{\\boldsymbol{x}} v \\, \\mathrm{d}\\boldsymbol{x}$. We can rewrite the dot product using matrix notation as $(\\nabla_{\\boldsymbol{x}} u)^T (\\nabla_{\\boldsymbol{x}} v)$. Substituting the expression for the physical gradients, we get:\n$$\n\\nabla_{\\boldsymbol{x}} u \\cdot \\nabla_{\\boldsymbol{x}} v = \\left( J^{-T} \\nabla_{\\hat{\\boldsymbol{x}}} \\hat{u} \\right)^T \\left( J^{-T} \\nabla_{\\hat{\\boldsymbol{x}}} \\hat{v} \\right) = (\\nabla_{\\hat{\\boldsymbol{x}}} \\hat{u})^T J^{-1} J^{-T} (\\nabla_{\\hat{\\boldsymbol{x}}} \\hat{v})\n$$\nNext, we apply the change-of-variables formula for integration, which relates the differential volume elements:\n$$\n\\mathrm{d}\\boldsymbol{x} = \\det(J(\\hat{\\boldsymbol{x}})) \\, \\mathrm{d}\\hat{\\boldsymbol{x}}\n$$\nSubstituting these into the definition of $a_K(u,v)$, we obtain the pullback bilinear form $\\hat{a}_K(\\hat{u},\\hat{v})$ on the reference element $\\hat{K}$:\n$$\n\\hat{a}_K(\\hat{u},\\hat{v}) = \\int_{\\hat{K}} (\\nabla_{\\hat{\\boldsymbol{x}}} \\hat{u})^T \\left( \\det(J) J^{-1} J^{-T} \\right) (\\nabla_{\\hat{\\boldsymbol{x}}} \\hat{v}) \\, \\mathrm{d}\\hat{\\boldsymbol{x}}\n$$\nThe matrix $G(\\hat{\\boldsymbol{x}}) = \\det(J(\\hat{\\boldsymbol{x}})) J(\\hat{\\boldsymbol{x}})^{-1} J(\\hat{\\boldsymbol{x}})^{-T}$ is often called the metric tensor transformed to reference coordinates. This completes the first part of the problem.\n\n**Part 2: Calculation of the Symmetry Defect**\n\nWe are given the specific map $F_{K}(\\xi,\\eta) = (x_1, x_2) = (\\xi + \\gamma\\xi\\eta, \\eta)$.\n\n1.  **Compute the Jacobian $J(\\hat{\\boldsymbol{x}})$ and related quantities.**\n    The Jacobian matrix is $J = \\begin{pmatrix} \\frac{\\partial x_1}{\\partial \\xi}  \\frac{\\partial x_1}{\\partial \\eta} \\\\ \\frac{\\partial x_2}{\\partial \\xi}  \\frac{\\partial x_2}{\\partial \\eta} \\end{pmatrix} = \\begin{pmatrix} 1+\\gamma\\eta  \\gamma\\xi \\\\ 0  1 \\end{pmatrix}$.\n    The determinant is $\\det(J) = 1+\\gamma\\eta$. The condition $1+\\gamma\\eta>0$ ensures the map is orientation-preserving.\n    The inverse transpose is $J^{-T} = \\left( \\frac{1}{1+\\gamma\\eta} \\begin{pmatrix} 1  -\\gamma\\xi \\\\ 0  1+\\gamma\\eta \\end{pmatrix} \\right)^T = \\frac{1}{1+\\gamma\\eta}\\begin{pmatrix} 1  0 \\\\ -\\gamma\\xi  1+\\gamma\\eta \\end{pmatrix}$.\n\n2.  **Compute the constant approximate Jacobian $\\hat{J}$ and its inverse.**\n    $\\hat{J}$ is $J$ evaluated at $(\\xi_0, \\eta_0) = (\\frac{1}{2}, \\frac{1}{2})$:\n    $\\hat{J} = J(\\tfrac{1}{2}, \\tfrac{1}{2}) = \\begin{pmatrix} 1+\\frac{\\gamma}{2}  \\frac{\\gamma}{2} \\\\ 0  1 \\end{pmatrix}$.\n    The inverse of $\\hat{J}$ is $\\hat{J}^{-1} = \\frac{1}{1+\\gamma/2} \\begin{pmatrix} 1  -\\frac{\\gamma}{2} \\\\ 0  1+\\frac{\\gamma}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1+\\gamma/2}  \\frac{-\\gamma/2}{1+\\gamma/2} \\\\ 0  1 \\end{pmatrix}$.\n\n3.  **Construct the approximate metric tensor $G_{\\mathrm{appr}}(\\hat{\\boldsymbol{x}})$.**\n    $G_{\\mathrm{appr}} = \\det(J) \\hat{J}^{-1} J^{-T}$.\n    $$\n    G_{\\mathrm{appr}} = (1+\\gamma\\eta) \\begin{pmatrix} \\frac{1}{1+\\gamma/2}  \\frac{-\\gamma/2}{1+\\gamma/2} \\\\ 0  1 \\end{pmatrix} \\frac{1}{1+\\gamma\\eta}\\begin{pmatrix} 1  0 \\\\ -\\gamma\\xi  1+\\gamma\\eta \\end{pmatrix}\n    $$\n    Multiplying the matrices:\n    $$\n    G_{\\mathrm{appr}} = \\frac{1}{1+\\gamma/2} \\begin{pmatrix} 1  -\\gamma/2 \\\\ 0  1+\\gamma/2 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ -\\gamma\\xi  1+\\gamma\\eta \\end{pmatrix} = \\frac{1}{1+\\gamma/2} \\begin{pmatrix} 1+\\gamma^2\\xi/2  -\\frac{\\gamma}{2}(1+\\gamma\\eta) \\\\ -(1+\\gamma/2)\\gamma\\xi  (1+\\gamma/2)(1+\\gamma\\eta) \\end{pmatrix}\n    $$\n    The matrix $G_{\\mathrm{appr}}$ is not symmetric, which is the origin of the defect.\n\n4.  **Compute the integrands for $a_{\\mathrm{appr}}(u,v)$ and $a_{\\mathrm{appr}}(v,u)$.**\n    The test functions are $u(\\xi, \\eta) = \\xi$ and $v(\\xi, \\eta) = \\eta^2$. Their gradients are:\n    $$\n    \\nabla_{\\hat{\\boldsymbol{x}}} u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\nabla_{\\hat{\\boldsymbol{x}}} v = \\begin{pmatrix} 0 \\\\ 2\\eta \\end{pmatrix}\n    $$\n    The integrand for $a_{\\mathrm{appr}}(u,v)$ is $(\\nabla_{\\hat{\\boldsymbol{x}}}u)^T G_{\\mathrm{appr}} (\\nabla_{\\hat{\\boldsymbol{x}}}v)$:\n    $$\n    \\begin{pmatrix} 1  0 \\end{pmatrix} G_{\\mathrm{appr}} \\begin{pmatrix} 0 \\\\ 2\\eta \\end{pmatrix} = 2\\eta \\cdot (G_{\\mathrm{appr}})_{12} = 2\\eta \\left( \\frac{-\\frac{\\gamma}{2}(1+\\gamma\\eta)}{1+\\gamma/2} \\right) = \\frac{-\\gamma\\eta(1+\\gamma\\eta)}{1+\\gamma/2}\n    $$\n    The integrand for $a_{\\mathrm{appr}}(v,u)$ is $(\\nabla_{\\hat{\\boldsymbol{x}}}v)^T G_{\\mathrm{appr}} (\\nabla_{\\hat{\\boldsymbol{x}}}u)$:\n    $$\n    \\begin{pmatrix} 0  2\\eta \\end{pmatrix} G_{\\mathrm{appr}} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 2\\eta \\cdot (G_{\\mathrm{appr}})_{21} = 2\\eta \\left( \\frac{-(1+\\gamma/2)\\gamma\\xi}{1+\\gamma/2} \\right) = -2\\gamma\\xi\\eta\n    $$\n\n5.  **Evaluate the integrals.**\n    The domain of integration is $\\hat{K} = [0,1]^2$.\n    $$\n    a_{\\mathrm{appr}}(u,v) = \\int_0^1 \\int_0^1 \\frac{-\\gamma(\\eta+\\gamma\\eta^2)}{1+\\gamma/2} \\,d\\xi d\\eta = \\frac{-\\gamma}{1+\\gamma/2} \\left( \\int_0^1 d\\xi \\right) \\left( \\int_0^1 (\\eta+\\gamma\\eta^2)d\\eta \\right)\n    $$\n    $$\n    a_{\\mathrm{appr}}(u,v) = \\frac{-\\gamma}{1+\\gamma/2} (1) \\left[ \\frac{\\eta^2}{2} + \\frac{\\gamma\\eta^3}{3} \\right]_0^1 = \\frac{-\\gamma}{1+\\gamma/2} \\left( \\frac{1}{2} + \\frac{\\gamma}{3} \\right)\n    $$\n    $$\n    a_{\\mathrm{appr}}(v,u) = \\int_0^1 \\int_0^1 -2\\gamma\\xi\\eta \\,d\\xi d\\eta = -2\\gamma \\left( \\int_0^1 \\xi d\\xi \\right) \\left( \\int_0^1 \\eta d\\eta \\right)\n    $$\n    $$\n    a_{\\mathrm{appr}}(v,u) = -2\\gamma \\left( \\frac{1}{2} \\right) \\left( \\frac{1}{2} \\right) = -\\frac{\\gamma}{2}\n    $$\n\n6.  **Compute the symmetry defect.**\n    The defect is $a_{\\mathrm{appr}}(u,v) - a_{\\mathrm{appr}}(v,u)$.\n    $$\n    \\text{Defect} = \\frac{-\\gamma}{1+\\gamma/2} \\left( \\frac{1}{2} + \\frac{\\gamma}{3} \\right) - \\left(-\\frac{\\gamma}{2}\\right) = \\frac{-\\gamma(\\frac{3+2\\gamma}{6})}{\\frac{2+\\gamma}{2}} + \\frac{\\gamma}{2}\n    $$\n    $$\n    \\text{Defect} = \\frac{-2\\gamma(3+2\\gamma)}{6(2+\\gamma)} + \\frac{\\gamma}{2} = \\frac{-\\gamma(3+2\\gamma)}{3(2+\\gamma)} + \\frac{\\gamma}{2}\n    $$\n    Bringing to a common denominator of $6(2+\\gamma)$:\n    $$\n    \\text{Defect} = \\frac{-2\\gamma(3+2\\gamma) + 3\\gamma(2+\\gamma)}{6(2+\\gamma)} = \\frac{-6\\gamma - 4\\gamma^2 + 6\\gamma + 3\\gamma^2}{6(2+\\gamma)}\n    $$\n    $$\n    \\text{Defect} = \\frac{-\\gamma^2}{6(2+\\gamma)}\n    $$\n    This is the final closed-form expression for the symmetry defect. As a check, if $\\gamma=0$, the defect is $0$, which is expected as the map becomes the identity and all Jacobians are the identity matrix, making $G_{\\mathrm{appr}}$ symmetric.",
            "answer": "$$\\boxed{\\frac{-\\gamma^2}{6(2+\\gamma)}}$$"
        }
    ]
}