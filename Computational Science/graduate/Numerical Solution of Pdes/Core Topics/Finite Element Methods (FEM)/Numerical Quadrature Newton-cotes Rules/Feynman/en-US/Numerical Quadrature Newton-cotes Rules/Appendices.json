{
    "hands_on_practices": [
        {
            "introduction": "Understanding numerical methods begins with constructing them from first principles. This initial practice guides you through the fundamental derivation of the simplest closed Newton-Cotes rule: the trapezoidal rule. By enforcing exactness for a basis of linear polynomials, you will derive the rule's weights and then rigorously analyze its error, laying the groundwork for all interpolatory quadrature methods .",
            "id": "3426583",
            "problem": "Let $f \\in C^{2}([a,b])$ with $a<b$. Consider the closed Newton-Cotes quadrature rule with $n=1$ subinterval on the interval $[a,b]$, namely the trapezoidal rule, which takes the form\n$$\nQ[f] \\equiv w_{0}\\,f(a)+w_{1}\\,f(b),\n$$\nfor weights $w_{0}$ and $w_{1}$ that depend only on $a$ and $b$. Define the error functional\n$$\nE(f) \\equiv \\int_{a}^{b} f(x)\\,dx - Q[f].\n$$\nTasks:\n- Determine $w_{0}$ and $w_{1}$ by enforcing exactness of $Q[\\cdot]$ for all polynomials of degree at most $1$.\n- Starting from the integral-remainder form of Taylor’s theorem and the linearity of $E(\\cdot)$, derive an identity of the form\n$$\nE(f)=\\int_{a}^{b} f''(t)\\,K(t)\\,dt,\n$$\nfor some kernel $K$ depending only on $a$ and $b$, and identify $K(t)$ explicitly.\n- Prove, using the fact that $K$ does not change sign on $(a,b)$ and the mean value theorem for integrals, that there exists $\\xi \\in (a,b)$ and a constant $C$ depending only on $a$ and $b$ such that\n$$\nE(f)=C\\,f''(\\xi).\n$$\nCompute $C$ in closed form.\n\nProvide your final answer as the row vector $\\bigl(w_{0},\\,w_{1},\\,C\\bigr)$. No numerical approximation is required; give exact expressions.",
            "solution": "The problem asks for the determination of the weights of the $n=1$ closed Newton-Cotes formula (the trapezoidal rule), the derivation of its integral error representation, and the calculation of the constant in the well-known error formula. The final answer is to be presented as a row vector containing the two weights and the error constant.\n\n**Part 1: Determination of weights $w_{0}$ and $w_{1}$**\n\nThe quadrature rule $Q[f] = w_{0}f(a)+w_{1}f(b)$ is required to be exact for all polynomials of degree at most $1$. Let's denote the space of such polynomials as $\\mathcal{P}_{1}$. By the linearity of both integration and the quadrature rule, it is sufficient to enforce exactness for a basis of $\\mathcal{P}_{1}$. A convenient basis is $\\{1, x\\}$.\n\nFor the basis function $p_{0}(x) = 1$:\nThe exact integral is $\\int_{a}^{b} 1 \\,dx = x \\big|_{a}^{b} = b-a$.\nThe quadrature formula gives $Q[p_{0}] = w_{0}p_{0}(a) + w_{1}p_{0}(b) = w_{0}(1) + w_{1}(1) = w_{0} + w_{1}$.\nFor the rule to be exact, we must have:\n$$\nw_{0} + w_{1} = b-a\n$$\n\nFor the basis function $p_{1}(x) = x$:\nThe exact integral is $\\int_{a}^{b} x \\,dx = \\frac{x^{2}}{2} \\bigg|_{a}^{b} = \\frac{b^{2}-a^{2}}{2}$.\nThe quadrature formula gives $Q[p_{1}] = w_{0}p_{1}(a) + w_{1}p_{1}(b) = w_{0}a + w_{1}b$.\nFor the rule to be exact, we must have:\n$$\naw_{0} + bw_{1} = \\frac{b^{2}-a^{2}}{2} = \\frac{(b-a)(b+a)}{2}\n$$\n\nWe now have a system of two linear equations for $w_{0}$ and $w_{1}$:\n1. $w_{0} + w_{1} = b-a$\n2. $aw_{0} + bw_{1} = \\frac{(b-a)(b+a)}{2}$\n\nFrom equation (1), we have $w_{1} = (b-a) - w_{0}$. Substituting this into equation (2):\n$$\naw_{0} + b((b-a) - w_{0}) = \\frac{(b-a)(b+a)}{2}\n$$\n$$\naw_{0} + b(b-a) - bw_{0} = \\frac{(b-a)(b+a)}{2}\n$$\n$$\nw_{0}(a-b) = \\frac{(b-a)(b+a)}{2} - b(b-a)\n$$\nFactoring out $(b-a)$ on the right-hand side:\n$$\n-w_{0}(b-a) = (b-a)\\left(\\frac{b+a}{2} - b\\right)\n$$\nSince $a < b$ is given, $b-a \\neq 0$, so we can divide both sides by $(b-a)$:\n$$\n-w_{0} = \\frac{a+b-2b}{2} = \\frac{a-b}{2}\n$$\n$$\nw_{0} = \\frac{b-a}{2}\n$$\nNow, we find $w_{1}$ using $w_{1} = (b-a) - w_{0}$:\n$$\nw_{1} = (b-a) - \\frac{b-a}{2} = \\frac{b-a}{2}\n$$\nThus, the weights are $w_{0} = \\frac{b-a}{2}$ and $w_{1} = \\frac{b-a}{2}$.\n\n**Part 2: Derivation of the error kernel $K(t)$**\n\nThe error functional is $E(f) = \\int_{a}^{b} f(x)\\,dx - Q[f]$. Since the rule is exact for polynomials of degree at most $1$, the Peano kernel theorem states that the error can be expressed as $E(f) = \\int_{a}^{b} f''(t)K(t)\\,dt$, where the kernel $K(t)$ is given by $K(t) = E_{x}[(x-t)_{+}]$. The subscript $x$ indicates that the functional $E$ acts on its argument as a function of the variable $x$.\n\nFor $t \\in [a,b]$, the kernel $K(t)$ is:\n$$\nK(t) = \\int_{a}^{b} (x-t)_{+}\\,dx - \\left(w_{0}(a-t)_{+} + w_{1}(b-t)_{+}\\right)\n$$\nwhere $(z)_{+} = \\max(z,0)$.\n\nLet's evaluate the integral term. Since $t \\in [a,b]$, the function $(x-t)_{+}$ is zero for $x \\le t$ and equal to $x-t$ for $x > t$.\n$$\n\\int_{a}^{b} (x-t)_{+}\\,dx = \\int_{t}^{b} (x-t)\\,dx = \\left[\\frac{(x-t)^{2}}{2}\\right]_{t}^{b} = \\frac{(b-t)^{2}}{2} - 0 = \\frac{(b-t)^{2}}{2}\n$$\nNext, let's evaluate the quadrature part of the functional. For $t \\in [a,b]$, we have $t \\ge a$, which implies $a-t \\le 0$, so $(a-t)_{+} = 0$. We also have $t \\le b$, which implies $b-t \\ge 0$, so $(b-t)_{+} = b-t$.\nUsing the weights we found, $w_{0} = w_{1} = \\frac{b-a}{2}$:\n$$\nw_{0}(a-t)_{+} + w_{1}(b-t)_{+} = \\frac{b-a}{2}(0) + \\frac{b-a}{2}(b-t) = \\frac{(b-a)(b-t)}{2}\n$$\nCombining these results gives the kernel:\n$$\nK(t) = \\frac{(b-t)^{2}}{2} - \\frac{(b-a)(b-t)}{2} = \\frac{b-t}{2}\\left((b-t) - (b-a)\\right) = \\frac{b-t}{2}(a-t)\n$$\nThus, the explicit form of the kernel is $K(t) = \\frac{1}{2}(t-a)(t-b)$.\n\n**Part 3: Derivation of the error constant $C$**\n\nWe have the error expression $E(f) = \\int_{a}^{b} f''(t)K(t)\\,dt$. The problem asks to prove that $E(f) = C f''(\\xi)$ for some $\\xi \\in (a,b)$ and to find the constant $C$.\nWe examine the sign of the kernel $K(t) = \\frac{1}{2}(t-a)(t-b)$ on the interval $(a,b)$. For any $t \\in (a,b)$, we have $t-a > 0$ and $t-b < 0$. Therefore, $K(t) < 0$ for all $t \\in (a,b)$.\nSince $f \\in C^{2}([a,b])$, its second derivative $f''(t)$ is continuous on $[a,b]$. The kernel $K(t)$ is an integrable function that does not change sign on $(a,b)$. We can therefore apply the Mean Value Theorem for Integrals, which states that there exists a value $\\xi \\in (a,b)$ such that:\n$$\n\\int_{a}^{b} f''(t)K(t)\\,dt = f''(\\xi)\\int_{a}^{b} K(t)\\,dt\n$$\nComparing this with the desired expression $E(f) = C f''(\\xi)$, we identify the constant $C$ as the definite integral of the kernel:\n$$\nC = \\int_{a}^{b} K(t)\\,dt = \\int_{a}^{b} \\frac{1}{2}(t-a)(t-b)\\,dt\n$$\nLet's compute this integral.\n$$\nC = \\frac{1}{2}\\int_{a}^{b} (t^{2} - (a+b)t + ab)\\,dt = \\frac{1}{2}\\left[\\frac{t^{3}}{3} - (a+b)\\frac{t^{2}}{2} + abt\\right]_{a}^{b}\n$$\n$$\nC = \\frac{1}{2}\\left[\\left(\\frac{b^{3}}{3} - \\frac{(a+b)b^{2}}{2} + ab^{2}\\right) - \\left(\\frac{a^{3}}{3} - \\frac{(a+b)a^{2}}{2} + a^{2}b\\right)\\right]\n$$\n$$\nC = \\frac{1}{2}\\left[\\frac{b^{3}-a^{3}}{3} - \\frac{a+b}{2}(b^{2}-a^{2}) + ab(b-a)\\right]\n$$\nFactor out the common term $(b-a)$:\n$$\nC = \\frac{b-a}{2}\\left[\\frac{b^{2}+ab+a^{2}}{3} - \\frac{(a+b)^{2}}{2} + ab\\right]\n$$\nPlace the terms over a common denominator of $6$:\n$$\nC = \\frac{b-a}{12}\\left[2(b^{2}+ab+a^{2}) - 3(a+b)^{2} + 6ab\\right]\n$$\n$$\nC = \\frac{b-a}{12}\\left[2b^{2}+2ab+2a^{2} - 3(a^{2}+2ab+b^{2}) + 6ab\\right]\n$$\n$$\nC = \\frac{b-a}{12}\\left[2b^{2}+2ab+2a^{2} - 3a^{2}-6ab-3b^{2} + 6ab\\right]\n$$\nCombine like terms inside the bracket:\n$$\nC = \\frac{b-a}{12}\\left[-a^{2} + 2ab - b^{2}\\right] = \\frac{b-a}{12}\\left[-(a^{2}-2ab+b^{2})\\right]\n$$\n$$\nC = \\frac{b-a}{12}\\left[-(a-b)^{2}\\right] = -\\frac{b-a}{12}(b-a)^{2} = -\\frac{(b-a)^{3}}{12}\n$$\nThe required components for the final answer are $w_{0} = \\frac{b-a}{2}$, $w_{1} = \\frac{b-a}{2}$, and $C = -\\frac{(b-a)^{3}}{12}$.\nThe final answer is the row vector $(w_0, w_1, C)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{b-a}{2} & \\frac{b-a}{2} & -\\frac{(b-a)^{3}}{12} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A hallmark of an expert numerical analyst is not just using a method, but knowing how to improve it. This exercise introduces the powerful technique of Richardson extrapolation, a method to systematically boost the accuracy of a numerical scheme . You will leverage the known error structure of the composite trapezoidal rule, as described by the Euler-Maclaurin formula, to combine two lower-accuracy estimates into a single, much more accurate result.",
            "id": "3426550",
            "problem": "Consider a smooth integrand $f \\in C^{5}([a,b])$ appearing in the weak formulation of a one-dimensional Partial Differential Equation (PDE). The task is to evaluate the bilinear form integral $I = \\int_{a}^{b} f(x)\\,dx$ using Newton-Cotes rules. Let the composite trapezoidal rule with uniform subinterval size $h = \\frac{b-a}{N}$ be denoted by\n$$\nT(h) = h \\left( \\frac{f(a) + f(b)}{2} + \\sum_{k=1}^{N-1} f(a + k h) \\right),\n$$\nand similarly $T(h/2)$ denote the composite trapezoidal rule computed on the same interval $[a,b]$ with step size $h/2$.\n\nDesign a single Richardson extrapolation step that combines $T(h)$ and $T(h/2)$ to produce an estimate $Q(h)$ of $I$ whose error is $O(h^{4})$. Your design must:\n- Choose constants $\\alpha$ and $\\beta$ to form a linear combination $Q(h) = \\alpha\\,T(h/2) + \\beta\\,T(h)$.\n- Justify, using the Euler-Maclaurin summation formula as the fundamental base, why your choice cancels the leading $O(h^{2})$ term in the error expansion of $T(h)$, thereby yielding $O(h^{4})$ accuracy.\n- Provide the final expression for $Q(h)$ in closed form in terms of $T(h/2)$ and $T(h)$.\n\nExpress your final answer as a single closed-form analytic expression. No rounding is required.",
            "solution": "The problem requires designing a single Richardson extrapolation step to improve the accuracy of the composite trapezoidal rule. The goal is to combine two estimates, $T(h)$ and $T(h/2)$, to produce a new estimate, $Q(h)$, with a higher order of accuracy. The theoretical justification for this procedure relies on the Euler-Maclaurin summation formula.\n\nFirst, we establish the asymptotic error expansion for the composite trapezoidal rule, $T(h)$. The Euler-Maclaurin formula provides a relationship between an integral and a corresponding sum. For a sufficiently smooth function $g$ on an interval $[m, n]$, the formula can be expressed as:\n$$\n\\sum_{k=m}^{n} g(k) = \\int_{m}^{n} g(t) dt + \\frac{g(m)+g(n)}{2} + \\sum_{j=1}^{p} \\frac{B_{2j}}{(2j)!} \\left( g^{(2j-1)}(n) - g^{(2j-1)}(m) \\right) + R_p\n$$\nwhere $B_{2j}$ are the Bernoulli numbers.\n\nTo apply this to our integral $I = \\int_{a}^{b} f(x)\\,dx$, we perform a change of variables $x = a + th$, where $h = \\frac{b-a}{N}$. As $t$ goes from $0$ to $N$, $x$ goes from $a$ to $b$. The integral becomes $I = h \\int_{0}^{N} f(a+th) dt$. Let $g(t) = f(a+th)$.\nThe composite trapezoidal rule is given by:\n$$\nT(h) = h \\left( \\frac{f(a) + f(b)}{2} + \\sum_{k=1}^{N-1} f(a + k h) \\right) = h \\left( \\frac{g(0) + g(N)}{2} + \\sum_{k=1}^{N-1} g(k) \\right)\n$$\nThis can be rewritten as $\\frac{T(h)}{h} = \\sum_{k=0}^{N} g(k) - \\frac{g(0) + g(N)}{2}$.\nRearranging the Euler-Maclaurin formula, we have:\n$$\n\\sum_{k=0}^{N} g(k) - \\frac{g(0) + g(N)}{2} = \\int_{0}^{N} g(t) dt + \\sum_{j=1}^{p} \\frac{B_{2j}}{(2j)!} \\left( g^{(2j-1)}(N) - g^{(2j-1)}(0) \\right) + R_p\n$$\nSubstituting our expressions for $\\frac{T(h)}{h}$ and $\\int_{0}^{N} g(t) dt = \\frac{I}{h}$:\n$$\n\\frac{T(h)}{h} = \\frac{I}{h} + \\sum_{j=1}^{p} \\frac{B_{2j}}{(2j)!} \\left( g^{(2j-1)}(N) - g^{(2j-1)}(0) \\right) + R_p'\n$$\nThe derivatives of $g(t)$ relate to derivatives of $f(x)$ by the chain rule: $g^{(k)}(t) = h^k f^{(k)}(a+th)$.\nThus, $g^{(2j-1)}(N) = h^{2j-1}f^{(2j-1)}(b)$ and $g^{(2j-1)}(0) = h^{2j-1}f^{(2j-1)}(a)$.\nSubstituting these back and multiplying the entire equation by $h$, we obtain:\n$$\nT(h) = I + \\sum_{j=1}^{p} \\frac{B_{2j}}{(2j)!} h^{2j} \\left( f^{(2j-1)}(b) - f^{(2j-1)}(a) \\right) + O(h^{2p+2})\n$$\nThe condition $f \\in C^{5}([a,b])$ ensures that we can write out the expansion to the $O(h^4)$ term with a well-defined remainder. Let us define constants $C_2$ and $C_4$ that are independent of $h$:\n$C_2 = \\frac{B_2}{2!} \\left( f'(b) - f'(a) \\right)$\n$C_4 = \\frac{B_4}{4!} \\left( f'''(b) - f'''(a) \\right)$\nThe error expansion for the trapezoidal rule is therefore:\n$$\nT(h) = I + C_2 h^2 + C_4 h^4 + O(h^6)\n$$\nThis expansion in even powers of $h$ is the foundation for Richardson extrapolation.\n\nNow, we write the expansion for step size $h$ and for step size $h/2$:\n$$\nT(h) = I + C_2 h^2 + C_4 h^4 + O(h^6) \\quad (1)\n$$\n$$\nT(h/2) = I + C_2 \\left(\\frac{h}{2}\\right)^2 + C_4 \\left(\\frac{h}{2}\\right)^4 + O(h^6) = I + \\frac{1}{4} C_2 h^2 + \\frac{1}{16} C_4 h^4 + O(h^6) \\quad (2)\n$$\nWe seek to form a linear combination $Q(h) = \\alpha\\,T(h/2) + \\beta\\,T(h)$ that provides a better estimate for $I$. Substituting the expansions $(1)$ and $(2)$ into this form:\n$$\nQ(h) = \\alpha \\left(I + \\frac{1}{4} C_2 h^2 + \\frac{1}{16} C_4 h^4 \\right) + \\beta \\left(I + C_2 h^2 + C_4 h^4 \\right) + O(h^6)\n$$\n$$\nQ(h) = (\\alpha + \\beta)I + \\left( \\frac{\\alpha}{4} + \\beta \\right) C_2 h^2 + \\left( \\frac{\\alpha}{16} + \\beta \\right) C_4 h^4 + O(h^6)\n$$\nTo make $Q(h)$ a more accurate approximation of $I$, we impose two conditions on the coefficients $\\alpha$ and $\\beta$:\n1. The coefficient of $I$ must be $1$ for the estimate to be consistent:\n   $$ \\alpha + \\beta = 1 $$\n2. The coefficient of the leading error term, $C_2 h^2$, must be $0$ to cancel this term:\n   $$ \\frac{\\alpha}{4} + \\beta = 0 $$\nWe now have a system of two linear equations for $\\alpha$ and $\\beta$. From the second equation, we find $\\beta = -\\frac{\\alpha}{4}$. Substituting this into the first equation:\n$$\n\\alpha - \\frac{\\alpha}{4} = 1 \\implies \\frac{3}{4}\\alpha = 1 \\implies \\alpha = \\frac{4}{3}\n$$\nThen, we find $\\beta$:\n$$\n\\beta = 1 - \\alpha = 1 - \\frac{4}{3} = -\\frac{1}{3}\n$$\nWith these values for $\\alpha$ and $\\beta$, the extrapolated estimate $Q(h)$ is:\n$$\nQ(h) = \\frac{4}{3}T(h/2) - \\frac{1}{3}T(h)\n$$\nThe error of this new estimate is determined by the next non-zero term in its expansion. The $C_2 h^2$ term is zero by construction. The $I$ term has a coefficient of $1$. The error is thus:\n$$\nQ(h) - I = \\left( \\frac{\\alpha}{16} + \\beta \\right) C_4 h^4 + O(h^6)\n$$\nLet's compute the new leading coefficient:\n$$\n\\frac{\\alpha}{16} + \\beta = \\frac{4/3}{16} - \\frac{1}{3} = \\frac{1}{12} - \\frac{1}{3} = \\frac{1-4}{12} = -\\frac{3}{12} = -\\frac{1}{4}\n$$\nSo the error for $Q(h)$ is:\n$$\nQ(h) - I = -\\frac{1}{4} C_4 h^4 + O(h^6)\n$$\nThis confirms that the error is of order $O(h^4)$, as required. The resulting formula is known as Simpson's rule, obtained here through Richardson extrapolation of the trapezoidal rule.\n\nThe final expression for the improved estimate $Q(h)$ in terms of $T(h)$ and $T(h/2)$ is:\n$$\nQ(h) = \\frac{4}{3}T(h/2) - \\frac{1}{3}T(h)\n$$\nThis can be written in a single closed form.",
            "answer": "$$\n\\boxed{\\frac{4}{3}T\\left(\\frac{h}{2}\\right) - \\frac{1}{3}T(h)}\n$$"
        },
        {
            "introduction": "Theoretical error bounds are essential, but their true power is realized when they inform practical, efficient algorithms. This final practice delves into adaptive quadrature, where the integration step size is dynamically adjusted to meet a desired error tolerance . You will use the asymptotic error formula for Simpson's rule to construct a local error estimator, forming the core of an intelligent algorithm that concentrates computational effort only where it is most needed.",
            "id": "3426571",
            "problem": "In assembling one-dimensional finite element matrices for a linear scalar partial differential equation (PDE), you must approximate integrals of the form $\\int_{x_0}^{x_2} f(x)\\,\\mathrm{d}x$ over a current panel $\\left[x_0,x_2\\right]\\subset [A,B]$ using a closed Newton-Cotes rule. Consider the composite Simpson rule on the panel $\\left[x_0,x_2\\right]$ with a coarse step $h:=x_2-x_0$, namely the single-panel Simpson approximation $S_h$ using the nodes $x_0$, $x_1:=\\tfrac{x_0+x_2}{2}$, and $x_2$. Now refine the panel by splitting $\\left[x_0,x_2\\right]$ into $2$ half-panels of length $h/2$ and apply the Simpson rule on each subpanel, yielding the refined composite approximation $S_{h/2}$ and the computable difference $D := S_{h/2}-S_h$.\n\nAssume $f$ is sufficiently smooth so that the Simpson rule error on an interval of length $h$ is proportional to $h^5$ times $f^{(4)}$ evaluated at some point in the interval. Construct a consistent local a posteriori error estimator for the refined approximation $S_{h/2}$ that depends only on $D$, and then derive an acceptance criterion based on a user-specified absolute tolerance $\\varepsilon>0$ for the entire integral over $[A,B]$. Adopt the standard local tolerance allocation $\\tau := \\varepsilon\\,\\frac{x_2-x_0}{B-A}$ for the current panel. Your acceptance criterion should take the form “accept the refined panel if $\\lvert D\\rvert \\leq \\text{threshold}(\\varepsilon,x_2-x_0,B-A)$”. Provide the explicit closed-form expression for this threshold as a function of $\\varepsilon$, $x_2-x_0$, and $B-A$.\n\nYour final answer must be a single analytic expression. No units are required, and no rounding is needed.",
            "solution": "The error for the single-panel Simpson's rule applied to an interval $[a,b]$ of total length $L=b-a$ is given by the formula:\n$$ E_L = \\int_a^b f(x)\\,\\mathrm{d}x - S_L = -\\frac{L^5}{2880}f^{(4)}(\\xi) $$\nfor some point $\\xi \\in [a,b]$. The key is that the error scales with the fifth power of the interval length.\n\nOn our panel $[x_0, x_2]$, the total length is $h := x_2 - x_0$. The error of the coarse Simpson approximation $S_h$ is therefore:\n$$ E_h := \\int_{x_0}^{x_2} f(x)\\,\\mathrm{d}x - S_h = -\\frac{h^5}{2880}f^{(4)}(\\xi_h) $$\nThe refined approximation $S_{h/2}$ applies Simpson's rule to two subpanels, $[x_0, x_1]$ and $[x_1, x_2]$, each of length $h/2$. The total error $E_{h/2}$ is the sum of the errors from these two applications:\n$$ E_{h/2} := \\int_{x_0}^{x_2} f(x)\\,\\mathrm{d}x - S_{h/2} = \\left(-\\frac{(h/2)^5}{2880}f^{(4)}(\\xi_1)\\right) + \\left(-\\frac{(h/2)^5}{2880}f^{(4)}(\\xi_2)\\right) $$\nAssuming $f^{(4)}(x)$ is approximately constant over the panel $[x_0, x_2]$, we can say $f^{(4)}(\\xi_1) \\approx f^{(4)}(\\xi_2) \\approx f^{(4)}(\\xi_h)$, which leads to:\n$$ E_{h/2} \\approx 2 \\cdot \\left(-\\frac{h^5/32}{2880}f^{(4)}(\\xi_h)\\right) = \\frac{1}{16}\\left(-\\frac{h^5}{2880}f^{(4)}(\\xi_h)\\right) = \\frac{1}{16}E_h $$\nHence, the leading-order relationship is $E_{h/2} \\approx \\frac{1}{16}E_h$.\n\nConsider now the computable difference\n$$\nD := S_{h/2}-S_h = \\bigl(\\int_{x_0}^{x_2} f(x)\\,\\mathrm{d}x - E_{h/2}\\bigr) - \\bigl(\\int_{x_0}^{x_2} f(x)\\,\\mathrm{d}x - E_h\\bigr) = E_h - E_{h/2}.\n$$\nUsing $E_{h/2}\\approx \\tfrac{1}{16}E_h$, we obtain to leading order\n$$\nD \\approx E_h - \\frac{1}{16}E_h = \\frac{15}{16}\\,E_h.\n$$\nThis allows us to estimate the unknown errors from the known difference $D$:\n$$\nE_h \\approx \\frac{16}{15}\\,D,\\qquad E_{h/2} \\approx \\frac{1}{16}\\,E_h \\approx \\frac{1}{15}\\,D.\n$$\nTherefore, a consistent local a posteriori error estimator for the refined composite Simpson approximation $S_{h/2}$ is\n$$\n\\eta := \\left|\\frac{1}{15}\\,D\\right| = \\frac{1}{15}\\,\\lvert S_{h/2}-S_h\\rvert.\n$$\n\nSuppose the user specifies an absolute tolerance $\\varepsilon>0$ for the entire integral over $[A,B]$. A standard tolerance allocation for adaptive quadrature assigns to the panel $\\left[x_0,x_2\\right]$ the local tolerance\n$$\n\\tau := \\varepsilon\\,\\frac{x_2-x_0}{B-A}.\n$$\nTo accept the refined panel, we require that the estimated refined error does not exceed its local allocation:\n$$\n\\eta \\le \\tau \\quad \\Longleftrightarrow \\quad \\frac{1}{15}\\,\\lvert S_{h/2}-S_h\\rvert \\le \\varepsilon\\,\\frac{x_2-x_0}{B-A}.\n$$\nEquivalently, this yields an acceptance criterion of the form\n$$\n\\lvert D\\rvert \\le \\underbrace{15\\,\\varepsilon\\,\\frac{x_2-x_0}{B-A}}_{\\text{threshold}(\\varepsilon,x_2-x_0,B-A)}.\n$$\n\nThus, the explicit threshold function requested is $15\\,\\varepsilon\\,\\dfrac{x_2-x_0}{B-A}$.",
            "answer": "$$\\boxed{15\\,\\varepsilon\\,\\frac{x_2 - x_0}{B - A}}$$"
        }
    ]
}